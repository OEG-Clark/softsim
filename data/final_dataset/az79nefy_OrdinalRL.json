{"home.repos.pwc.inspect_result.az79nefy_OrdinalRL.None.gym_wrappers.NoopResetEnv.__init__": [[13, 21], ["gym.Wrapper.__init__", "env.unwrapped.get_action_meanings"], "methods", ["home.repos.pwc.inspect_result.az79nefy_OrdinalRL.dqn.atari_dqn_single_net_agent.DQNAgent.__init__"], ["    ", "def", "__init__", "(", "self", ",", "env", "=", "None", ",", "noop_max", "=", "30", ")", ":", "\n", "        ", "\"\"\"Sample initial states by taking random number of no-ops on reset.\n        No-op is assumed to be action 0.\n        \"\"\"", "\n", "super", "(", "NoopResetEnv", ",", "self", ")", ".", "__init__", "(", "env", ")", "\n", "self", ".", "noop_max", "=", "noop_max", "\n", "self", ".", "override_num_noops", "=", "None", "\n", "assert", "env", ".", "unwrapped", ".", "get_action_meanings", "(", ")", "[", "0", "]", "==", "'NOOP'", "\n", "\n"]], "home.repos.pwc.inspect_result.az79nefy_OrdinalRL.None.gym_wrappers.NoopResetEnv.reset": [[22, 36], ["gym_wrappers.NoopResetEnv.env.reset", "range", "numpy.random.randint", "gym_wrappers.NoopResetEnv.env.step", "gym_wrappers.NoopResetEnv.env.reset"], "methods", ["home.repos.pwc.inspect_result.az79nefy_OrdinalRL.None.gym_wrappers.FrameStack.reset", "home.repos.pwc.inspect_result.az79nefy_OrdinalRL.None.gym_wrappers.FrameStack.step", "home.repos.pwc.inspect_result.az79nefy_OrdinalRL.None.gym_wrappers.FrameStack.reset"], ["", "def", "reset", "(", "self", ")", ":", "\n", "        ", "\"\"\" Do no-op action for a number of steps in [1, noop_max].\"\"\"", "\n", "self", ".", "env", ".", "reset", "(", ")", "\n", "if", "self", ".", "override_num_noops", "is", "not", "None", ":", "\n", "            ", "noops", "=", "self", ".", "override_num_noops", "\n", "", "else", ":", "\n", "            ", "noops", "=", "np", ".", "random", ".", "randint", "(", "1", ",", "self", ".", "noop_max", "+", "1", ")", "\n", "", "assert", "noops", ">", "0", "\n", "obs", "=", "None", "\n", "for", "_", "in", "range", "(", "noops", ")", ":", "\n", "            ", "obs", ",", "_", ",", "done", ",", "_", "=", "self", ".", "env", ".", "step", "(", "0", ")", "\n", "if", "done", ":", "\n", "                ", "obs", "=", "self", ".", "env", ".", "reset", "(", ")", "\n", "", "", "return", "obs", "\n", "\n"]], "home.repos.pwc.inspect_result.az79nefy_OrdinalRL.None.gym_wrappers.FireResetEnv.__init__": [[39, 44], ["gym.Wrapper.__init__", "len", "env.unwrapped.get_action_meanings", "env.unwrapped.get_action_meanings"], "methods", ["home.repos.pwc.inspect_result.az79nefy_OrdinalRL.dqn.atari_dqn_single_net_agent.DQNAgent.__init__"], ["    ", "def", "__init__", "(", "self", ",", "env", "=", "None", ")", ":", "\n", "        ", "\"\"\"For environments where the user need to press FIRE for the game to start.\"\"\"", "\n", "super", "(", "FireResetEnv", ",", "self", ")", ".", "__init__", "(", "env", ")", "\n", "assert", "env", ".", "unwrapped", ".", "get_action_meanings", "(", ")", "[", "1", "]", "==", "'FIRE'", "\n", "assert", "len", "(", "env", ".", "unwrapped", ".", "get_action_meanings", "(", ")", ")", ">=", "3", "\n", "\n"]], "home.repos.pwc.inspect_result.az79nefy_OrdinalRL.None.gym_wrappers.FireResetEnv.reset": [[45, 54], ["gym_wrappers.FireResetEnv.env.reset", "gym_wrappers.FireResetEnv.env.step", "gym_wrappers.FireResetEnv.env.step", "gym_wrappers.FireResetEnv.env.reset", "gym_wrappers.FireResetEnv.env.reset"], "methods", ["home.repos.pwc.inspect_result.az79nefy_OrdinalRL.None.gym_wrappers.FrameStack.reset", "home.repos.pwc.inspect_result.az79nefy_OrdinalRL.None.gym_wrappers.FrameStack.step", "home.repos.pwc.inspect_result.az79nefy_OrdinalRL.None.gym_wrappers.FrameStack.step", "home.repos.pwc.inspect_result.az79nefy_OrdinalRL.None.gym_wrappers.FrameStack.reset", "home.repos.pwc.inspect_result.az79nefy_OrdinalRL.None.gym_wrappers.FrameStack.reset"], ["", "def", "reset", "(", "self", ")", ":", "\n", "        ", "self", ".", "env", ".", "reset", "(", ")", "\n", "obs", ",", "_", ",", "done", ",", "_", "=", "self", ".", "env", ".", "step", "(", "1", ")", "\n", "if", "done", ":", "\n", "            ", "self", ".", "env", ".", "reset", "(", ")", "\n", "", "obs", ",", "_", ",", "done", ",", "_", "=", "self", ".", "env", ".", "step", "(", "2", ")", "\n", "if", "done", ":", "\n", "            ", "self", ".", "env", ".", "reset", "(", ")", "\n", "", "return", "obs", "\n", "\n"]], "home.repos.pwc.inspect_result.az79nefy_OrdinalRL.None.gym_wrappers.EpisodicLifeEnv.__init__": [[57, 65], ["gym.Wrapper.__init__"], "methods", ["home.repos.pwc.inspect_result.az79nefy_OrdinalRL.dqn.atari_dqn_single_net_agent.DQNAgent.__init__"], ["    ", "def", "__init__", "(", "self", ",", "env", "=", "None", ")", ":", "\n", "        ", "\"\"\"Make end-of-life == end-of-episode, but only reset on true game over.\n        Done by DeepMind for the DQN and co. since it helps value estimation.\n        \"\"\"", "\n", "super", "(", "EpisodicLifeEnv", ",", "self", ")", ".", "__init__", "(", "env", ")", "\n", "self", ".", "lives", "=", "0", "\n", "self", ".", "was_real_done", "=", "True", "\n", "self", ".", "was_real_reset", "=", "False", "\n", "\n"]], "home.repos.pwc.inspect_result.az79nefy_OrdinalRL.None.gym_wrappers.EpisodicLifeEnv.step": [[66, 79], ["gym_wrappers.EpisodicLifeEnv.env.step", "gym_wrappers.EpisodicLifeEnv.env.unwrapped.ale.lives"], "methods", ["home.repos.pwc.inspect_result.az79nefy_OrdinalRL.None.gym_wrappers.FrameStack.step"], ["", "def", "step", "(", "self", ",", "action", ")", ":", "\n", "        ", "obs", ",", "reward", ",", "done", ",", "info", "=", "self", ".", "env", ".", "step", "(", "action", ")", "\n", "self", ".", "was_real_done", "=", "done", "\n", "# check current lives, make loss of life terminal,", "\n", "# then update lives to handle bonus lives", "\n", "lives", "=", "self", ".", "env", ".", "unwrapped", ".", "ale", ".", "lives", "(", ")", "\n", "if", "lives", "<", "self", ".", "lives", "and", "lives", ">", "0", ":", "\n", "# for Qbert somtimes we stay in lives == 0 condtion for a few frames", "\n", "# so its important to keep lives > 0, so that we only reset once", "\n", "# the environment advertises done.", "\n", "            ", "done", "=", "True", "\n", "", "self", ".", "lives", "=", "lives", "\n", "return", "obs", ",", "reward", ",", "done", ",", "info", "\n", "\n"]], "home.repos.pwc.inspect_result.az79nefy_OrdinalRL.None.gym_wrappers.EpisodicLifeEnv.reset": [[80, 94], ["gym_wrappers.EpisodicLifeEnv.env.unwrapped.ale.lives", "gym_wrappers.EpisodicLifeEnv.env.reset", "gym_wrappers.EpisodicLifeEnv.env.step"], "methods", ["home.repos.pwc.inspect_result.az79nefy_OrdinalRL.None.gym_wrappers.FrameStack.reset", "home.repos.pwc.inspect_result.az79nefy_OrdinalRL.None.gym_wrappers.FrameStack.step"], ["", "def", "reset", "(", "self", ")", ":", "\n", "        ", "\"\"\"Reset only when lives are exhausted.\n        This way all states are still reachable even though lives are episodic,\n        and the learner need not know about any of this behind-the-scenes.\n        \"\"\"", "\n", "if", "self", ".", "was_real_done", ":", "\n", "            ", "obs", "=", "self", ".", "env", ".", "reset", "(", ")", "\n", "self", ".", "was_real_reset", "=", "True", "\n", "", "else", ":", "\n", "# no-op step to advance from terminal/lost life state", "\n", "            ", "obs", ",", "_", ",", "_", ",", "_", "=", "self", ".", "env", ".", "step", "(", "0", ")", "\n", "self", ".", "was_real_reset", "=", "False", "\n", "", "self", ".", "lives", "=", "self", ".", "env", ".", "unwrapped", ".", "ale", ".", "lives", "(", ")", "\n", "return", "obs", "\n", "\n"]], "home.repos.pwc.inspect_result.az79nefy_OrdinalRL.None.gym_wrappers.ProcessFrame84.__init__": [[97, 100], ["gym.ObservationWrapper.__init__", "gym.spaces.Box"], "methods", ["home.repos.pwc.inspect_result.az79nefy_OrdinalRL.dqn.atari_dqn_single_net_agent.DQNAgent.__init__"], ["    ", "def", "__init__", "(", "self", ",", "env", "=", "None", ")", ":", "\n", "        ", "super", "(", "ProcessFrame84", ",", "self", ")", ".", "__init__", "(", "env", ")", "\n", "self", ".", "observation_space", "=", "spaces", ".", "Box", "(", "low", "=", "0", ",", "high", "=", "255", ",", "shape", "=", "(", "84", ",", "84", ",", "1", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.az79nefy_OrdinalRL.None.gym_wrappers.ProcessFrame84.observation": [[101, 103], ["gym_wrappers.ProcessFrame84.process"], "methods", ["home.repos.pwc.inspect_result.az79nefy_OrdinalRL.None.gym_wrappers.ProcessFrame84.process"], ["", "def", "observation", "(", "self", ",", "obs", ")", ":", "\n", "        ", "return", "ProcessFrame84", ".", "process", "(", "obs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.az79nefy_OrdinalRL.None.gym_wrappers.ProcessFrame84.process": [[104, 117], ["cv2.resize", "numpy.reshape", "numpy.reshape.astype", "numpy.reshape().astype", "numpy.reshape().astype", "numpy.reshape", "numpy.reshape"], "methods", ["None"], ["", "@", "staticmethod", "\n", "def", "process", "(", "frame", ")", ":", "\n", "        ", "if", "frame", ".", "size", "==", "210", "*", "160", "*", "3", ":", "\n", "            ", "img", "=", "np", ".", "reshape", "(", "frame", ",", "[", "210", ",", "160", ",", "3", "]", ")", ".", "astype", "(", "np", ".", "float32", ")", "\n", "", "elif", "frame", ".", "size", "==", "250", "*", "160", "*", "3", ":", "\n", "            ", "img", "=", "np", ".", "reshape", "(", "frame", ",", "[", "250", ",", "160", ",", "3", "]", ")", ".", "astype", "(", "np", ".", "float32", ")", "\n", "", "else", ":", "\n", "            ", "assert", "False", ",", "\"Unknown resolution.\"", "\n", "", "img", "=", "img", "[", ":", ",", ":", ",", "0", "]", "*", "0.299", "+", "img", "[", ":", ",", ":", ",", "1", "]", "*", "0.587", "+", "img", "[", ":", ",", ":", ",", "2", "]", "*", "0.114", "\n", "resized_screen", "=", "cv2", ".", "resize", "(", "img", ",", "(", "84", ",", "110", ")", ",", "interpolation", "=", "cv2", ".", "INTER_AREA", ")", "\n", "x_t", "=", "resized_screen", "[", "18", ":", "102", ",", ":", "]", "\n", "x_t", "=", "np", ".", "reshape", "(", "x_t", ",", "[", "84", ",", "84", ",", "1", "]", ")", "\n", "return", "x_t", ".", "astype", "(", "np", ".", "uint8", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.az79nefy_OrdinalRL.None.gym_wrappers.ClippedRewardsWrapper.reward": [[120, 123], ["numpy.sign"], "methods", ["None"], ["    ", "def", "reward", "(", "self", ",", "reward", ")", ":", "\n", "        ", "\"\"\"Change all the positive rewards to 1, negative to -1 and keep zero.\"\"\"", "\n", "return", "np", ".", "sign", "(", "reward", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.az79nefy_OrdinalRL.None.gym_wrappers.LazyFrames.__init__": [[126, 133], ["None"], "methods", ["None"], ["    ", "def", "__init__", "(", "self", ",", "frames", ")", ":", "\n", "        ", "\"\"\"This object ensures that common frames between the observations are only stored once.\n        It exists purely to optimize memory usage which can be huge for DQN's 1M frames replay\n        buffers.\n        This object should only be converted to numpy array before being passed to the model.\n        You'd not belive how complex the previous solution was.\"\"\"", "\n", "self", ".", "_frames", "=", "frames", "\n", "\n"]], "home.repos.pwc.inspect_result.az79nefy_OrdinalRL.None.gym_wrappers.LazyFrames.__array__": [[134, 139], ["numpy.concatenate", "out.astype.astype.astype"], "methods", ["None"], ["", "def", "__array__", "(", "self", ",", "dtype", "=", "None", ")", ":", "\n", "        ", "out", "=", "np", ".", "concatenate", "(", "self", ".", "_frames", ",", "axis", "=", "0", ")", "\n", "if", "dtype", "is", "not", "None", ":", "\n", "            ", "out", "=", "out", ".", "astype", "(", "dtype", ")", "\n", "", "return", "out", "\n", "\n"]], "home.repos.pwc.inspect_result.az79nefy_OrdinalRL.None.gym_wrappers.FrameStack.__init__": [[142, 154], ["gym.Wrapper.__init__", "collections.deque", "gym.spaces.Box"], "methods", ["home.repos.pwc.inspect_result.az79nefy_OrdinalRL.dqn.atari_dqn_single_net_agent.DQNAgent.__init__"], ["    ", "def", "__init__", "(", "self", ",", "env", ",", "k", ")", ":", "\n", "        ", "\"\"\"Stack k last frames.\n        Returns lazy array, which is much more memory efficient.\n        See Also\n        --------\n        baselines.common.atari_wrappers.LazyFrames\n        \"\"\"", "\n", "gym", ".", "Wrapper", ".", "__init__", "(", "self", ",", "env", ")", "\n", "self", ".", "k", "=", "k", "\n", "self", ".", "frames", "=", "deque", "(", "[", "]", ",", "maxlen", "=", "k", ")", "\n", "shp", "=", "env", ".", "observation_space", ".", "shape", "\n", "self", ".", "observation_space", "=", "spaces", ".", "Box", "(", "low", "=", "0", ",", "high", "=", "255", ",", "shape", "=", "(", "shp", "[", "0", "]", "*", "k", ",", "shp", "[", "1", "]", ",", "shp", "[", "2", "]", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.az79nefy_OrdinalRL.None.gym_wrappers.FrameStack.reset": [[155, 160], ["gym_wrappers.FrameStack.env.reset", "range", "gym_wrappers.FrameStack._get_ob", "gym_wrappers.FrameStack.frames.append"], "methods", ["home.repos.pwc.inspect_result.az79nefy_OrdinalRL.None.gym_wrappers.FrameStack.reset", "home.repos.pwc.inspect_result.az79nefy_OrdinalRL.None.gym_wrappers.FrameStack._get_ob"], ["", "def", "reset", "(", "self", ")", ":", "\n", "        ", "ob", "=", "self", ".", "env", ".", "reset", "(", ")", "\n", "for", "_", "in", "range", "(", "self", ".", "k", ")", ":", "\n", "            ", "self", ".", "frames", ".", "append", "(", "ob", ")", "\n", "", "return", "self", ".", "_get_ob", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.az79nefy_OrdinalRL.None.gym_wrappers.FrameStack.step": [[161, 165], ["gym_wrappers.FrameStack.env.step", "gym_wrappers.FrameStack.frames.append", "gym_wrappers.FrameStack._get_ob"], "methods", ["home.repos.pwc.inspect_result.az79nefy_OrdinalRL.None.gym_wrappers.FrameStack.step", "home.repos.pwc.inspect_result.az79nefy_OrdinalRL.None.gym_wrappers.FrameStack._get_ob"], ["", "def", "step", "(", "self", ",", "action", ")", ":", "\n", "        ", "ob", ",", "reward", ",", "done", ",", "info", "=", "self", ".", "env", ".", "step", "(", "action", ")", "\n", "self", ".", "frames", ".", "append", "(", "ob", ")", "\n", "return", "self", ".", "_get_ob", "(", ")", ",", "reward", ",", "done", ",", "info", "\n", "\n"]], "home.repos.pwc.inspect_result.az79nefy_OrdinalRL.None.gym_wrappers.FrameStack._get_ob": [[166, 169], ["gym_wrappers.LazyFrames", "len", "list"], "methods", ["None"], ["", "def", "_get_ob", "(", "self", ")", ":", "\n", "        ", "assert", "len", "(", "self", ".", "frames", ")", "==", "self", ".", "k", "\n", "return", "LazyFrames", "(", "list", "(", "self", ".", "frames", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.az79nefy_OrdinalRL.None.gym_wrappers.ChannelsFirstImageShape.__init__": [[175, 179], ["gym.ObservationWrapper.__init__", "gym.spaces.Box"], "methods", ["home.repos.pwc.inspect_result.az79nefy_OrdinalRL.dqn.atari_dqn_single_net_agent.DQNAgent.__init__"], ["def", "__init__", "(", "self", ",", "env", ")", ":", "\n", "        ", "super", "(", "ChannelsFirstImageShape", ",", "self", ")", ".", "__init__", "(", "env", ")", "\n", "old_shape", "=", "self", ".", "observation_space", ".", "shape", "\n", "self", ".", "observation_space", "=", "gym", ".", "spaces", ".", "Box", "(", "low", "=", "0.0", ",", "high", "=", "1.0", ",", "shape", "=", "(", "old_shape", "[", "-", "1", "]", ",", "old_shape", "[", "0", "]", ",", "old_shape", "[", "1", "]", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.az79nefy_OrdinalRL.None.gym_wrappers.ChannelsFirstImageShape.observation": [[180, 182], ["numpy.swapaxes"], "methods", ["None"], ["", "def", "observation", "(", "self", ",", "observation", ")", ":", "\n", "        ", "return", "np", ".", "swapaxes", "(", "observation", ",", "2", ",", "0", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.az79nefy_OrdinalRL.None.gym_wrappers.MainGymWrapper.wrap": [[186, 197], ["gym_wrappers.EpisodicLifeEnv", "gym_wrappers.NoopResetEnv", "gym_wrappers.ProcessFrame84", "gym_wrappers.ChannelsFirstImageShape", "gym_wrappers.FrameStack", "gym_wrappers.ClippedRewardsWrapper", "FireResetEnv.unwrapped.get_action_meanings", "gym_wrappers.FireResetEnv"], "methods", ["None"], ["    ", "@", "staticmethod", "\n", "def", "wrap", "(", "env", ")", ":", "\n", "        ", "env", "=", "EpisodicLifeEnv", "(", "env", ")", "\n", "env", "=", "NoopResetEnv", "(", "env", ",", "noop_max", "=", "30", ")", "\n", "if", "'FIRE'", "in", "env", ".", "unwrapped", ".", "get_action_meanings", "(", ")", ":", "\n", "            ", "env", "=", "FireResetEnv", "(", "env", ")", "\n", "", "env", "=", "ProcessFrame84", "(", "env", ")", "\n", "env", "=", "ChannelsFirstImageShape", "(", "env", ")", "\n", "env", "=", "FrameStack", "(", "env", ",", "4", ")", "\n", "env", "=", "ClippedRewardsWrapper", "(", "env", ")", "\n", "return", "env", "\n", "", "", ""]], "home.repos.pwc.inspect_result.az79nefy_OrdinalRL.sarsa.ordinal_sarsa_agent.SarsaAgent.__init__": [[7, 20], ["numpy.full"], "methods", ["None"], ["    ", "def", "__init__", "(", "self", ",", "alpha", ",", "gamma", ",", "epsilon", ",", "epsilon_min", ",", "n_actions", ",", "n_ordinals", ",", "n_observations", ",", "randomize", ")", ":", "\n", "        ", "self", ".", "alpha", "=", "alpha", "\n", "self", ".", "gamma", "=", "gamma", "\n", "self", ".", "epsilon", "=", "epsilon", "\n", "self", ".", "epsilon_min", "=", "epsilon_min", "\n", "self", ".", "n_actions", "=", "n_actions", "\n", "self", ".", "n_ordinals", "=", "n_ordinals", "\n", "\n", "# Ordinal_Values (3-dimensional array with ordinal_value (array of floats) for each action in each observation)", "\n", "self", ".", "ordinal_values", "=", "np", ".", "full", "(", "(", "n_observations", ",", "n_actions", ",", "n_ordinals", ")", ",", "0.0", ")", "\n", "\n", "self", ".", "win_rates", "=", "[", "]", "\n", "self", ".", "average_rewards", "=", "[", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.az79nefy_OrdinalRL.sarsa.ordinal_sarsa_agent.SarsaAgent.update": [[21, 25], ["ordinal_sarsa_agent.SarsaAgent.reward_to_ordinal", "ordinal_sarsa_agent.SarsaAgent.update_ordinal_values"], "methods", ["home.repos.pwc.inspect_result.az79nefy_OrdinalRL.dqn.ordinal_dqn_discretized_agent.DQNAgent.reward_to_ordinal", "home.repos.pwc.inspect_result.az79nefy_OrdinalRL.q_learning.alternative_ordinal_q_discretized_agent.QAgent.update_ordinal_values"], ["", "def", "update", "(", "self", ",", "prev_obs", ",", "prev_act", ",", "obs", ",", "act", ",", "reward", ",", "episode_reward", ",", "done", ")", ":", "\n", "        ", "ordinal", "=", "self", ".", "reward_to_ordinal", "(", "reward", ",", "episode_reward", ",", "done", ")", "\n", "# update ordinal_values with received ordinal", "\n", "self", ".", "update_ordinal_values", "(", "prev_obs", ",", "prev_act", ",", "obs", ",", "act", ",", "ordinal", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.az79nefy_OrdinalRL.sarsa.ordinal_sarsa_agent.SarsaAgent.update_ordinal_values": [[27, 35], ["range"], "methods", ["None"], ["", "def", "update_ordinal_values", "(", "self", ",", "prev_obs", ",", "prev_act", ",", "obs", ",", "act", ",", "ordinal", ")", ":", "\n", "# reduce old data weight", "\n", "        ", "for", "i", "in", "range", "(", "self", ".", "n_ordinals", ")", ":", "\n", "            ", "self", ".", "ordinal_values", "[", "prev_obs", ",", "prev_act", ",", "i", "]", "*=", "(", "1", "-", "self", ".", "alpha", ")", "\n", "self", ".", "ordinal_values", "[", "prev_obs", ",", "prev_act", ",", "i", "]", "+=", "self", ".", "alpha", "*", "(", "self", ".", "gamma", "*", "self", ".", "ordinal_values", "[", "obs", ",", "act", ",", "i", "]", ")", "\n", "\n", "# add new data point", "\n", "", "self", ".", "ordinal_values", "[", "prev_obs", ",", "prev_act", ",", "ordinal", "]", "+=", "self", ".", "alpha", "\n", "\n"]], "home.repos.pwc.inspect_result.az79nefy_OrdinalRL.sarsa.ordinal_sarsa_agent.SarsaAgent.compute_borda_scores": [[37, 87], ["numpy.zeros", "range", "numpy.count_nonzero", "range", "borda_scores.append", "borda_scores.append", "range", "borda_scores.append", "range"], "methods", ["None"], ["", "def", "compute_borda_scores", "(", "self", ",", "obs", ")", ":", "\n", "# sum up all ordinal values per action for given observation", "\n", "        ", "ordinal_value_sum_per_action", "=", "np", ".", "zeros", "(", "self", ".", "n_actions", ")", "\n", "for", "action_a", "in", "range", "(", "self", ".", "n_actions", ")", ":", "\n", "            ", "for", "ordinal_value", "in", "self", ".", "ordinal_values", "[", "obs", ",", "action_a", "]", ":", "\n", "                ", "ordinal_value_sum_per_action", "[", "action_a", "]", "+=", "ordinal_value", "\n", "\n", "# count actions whose ordinal value sum is not zero (no comparision possible for actions without ordinal_value)", "\n", "", "", "non_zero_action_count", "=", "np", ".", "count_nonzero", "(", "ordinal_value_sum_per_action", ")", "\n", "actions_to_compare_count", "=", "non_zero_action_count", "-", "1", "\n", "\n", "borda_scores", "=", "[", "]", "\n", "# compute borda_values for action_a (probability that action_a wins against any other action)", "\n", "for", "action_a", "in", "range", "(", "self", ".", "n_actions", ")", ":", "\n", "# if action has not yet recorded any ordinal values, action has to be played (set borda_value to 1.0)", "\n", "            ", "if", "ordinal_value_sum_per_action", "[", "action_a", "]", "==", "0", ":", "\n", "                ", "borda_scores", ".", "append", "(", "1.0", ")", "\n", "continue", "\n", "\n", "", "if", "actions_to_compare_count", "<", "1", ":", "\n", "# set lower than 1.0 (borda_value for zero_actions is 1.0)", "\n", "                ", "borda_scores", ".", "append", "(", "0.5", ")", "\n", "", "else", ":", "\n", "# over all actions: sum up the probabilities that action_a wins against the given action", "\n", "                ", "winning_probability_a_sum", "=", "0", "\n", "# compare action_a to all other actions", "\n", "for", "action_b", "in", "range", "(", "self", ".", "n_actions", ")", ":", "\n", "                    ", "if", "action_a", "==", "action_b", ":", "\n", "                        ", "continue", "\n", "# not comparable if action_b has no ordinal_values", "\n", "", "if", "ordinal_value_sum_per_action", "[", "action_b", "]", "==", "0", ":", "\n", "                        ", "continue", "\n", "", "else", ":", "\n", "# probability that action_a wins against action_b", "\n", "                        ", "winning_probability_a", "=", "0", "\n", "# running ordinal probability that action_b is worse than current investigated ordinal", "\n", "worse_probability_b", "=", "0", "\n", "for", "ordinal_count", "in", "range", "(", "self", ".", "n_ordinals", ")", ":", "\n", "                            ", "ordinal_probability_a", "=", "self", ".", "ordinal_values", "[", "obs", ",", "action_a", ",", "ordinal_count", "]", "/", "ordinal_value_sum_per_action", "[", "action_a", "]", "\n", "# ordinal_probability_b is also the tie probability", "\n", "ordinal_probability_b", "=", "(", "self", ".", "ordinal_values", "[", "obs", ",", "action_b", ",", "ordinal_count", "]", "/", "\n", "ordinal_value_sum_per_action", "[", "action_b", "]", ")", "\n", "winning_probability_a", "+=", "ordinal_probability_a", "*", "(", "worse_probability_b", "+", "ordinal_probability_b", "/", "2.0", ")", "\n", "worse_probability_b", "+=", "ordinal_probability_b", "\n", "", "winning_probability_a_sum", "+=", "winning_probability_a", "\n", "# normalize summed up probabilities with number of actions that have been compared", "\n", "", "", "borda_scores", ".", "append", "(", "winning_probability_a_sum", "/", "actions_to_compare_count", ")", "\n", "", "", "return", "borda_scores", "\n", "\n"]], "home.repos.pwc.inspect_result.az79nefy_OrdinalRL.sarsa.ordinal_sarsa_agent.SarsaAgent.get_greedy_action": [[88, 90], ["numpy.argmax", "ordinal_sarsa_agent.SarsaAgent.compute_borda_scores"], "methods", ["home.repos.pwc.inspect_result.az79nefy_OrdinalRL.dqn.ordinal_dqn_discretized_agent.DQNAgent.compute_borda_scores"], ["", "def", "get_greedy_action", "(", "self", ",", "obs", ")", ":", "\n", "        ", "return", "np", ".", "argmax", "(", "self", ".", "compute_borda_scores", "(", "obs", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.az79nefy_OrdinalRL.sarsa.ordinal_sarsa_agent.SarsaAgent.choose_action": [[92, 100], ["ordinal_sarsa_agent.SarsaAgent.get_greedy_action", "random.randrange", "random.random"], "methods", ["home.repos.pwc.inspect_result.az79nefy_OrdinalRL.dqn.atari_dqn_single_net_agent.DQNAgent.get_greedy_action"], ["", "def", "choose_action", "(", "self", ",", "obs", ",", "greedy", ")", ":", "\n", "        ", "greedy_action", "=", "self", ".", "get_greedy_action", "(", "obs", ")", "\n", "# choose random action with probability epsilon", "\n", "if", "not", "greedy", "and", "random", ".", "random", "(", ")", "<", "self", ".", "epsilon", ":", "\n", "            ", "return", "random", ".", "randrange", "(", "self", ".", "n_actions", ")", "\n", "# greedy action is chosen with probability (1 - epsilon)", "\n", "", "else", ":", "\n", "            ", "return", "greedy_action", "\n", "\n"]], "home.repos.pwc.inspect_result.az79nefy_OrdinalRL.sarsa.ordinal_sarsa_agent.SarsaAgent.end_episode": [[101, 104], ["None"], "methods", ["None"], ["", "", "def", "end_episode", "(", "self", ",", "n_episodes", ")", ":", "\n", "# gradually reduce epsilon after every done episode", "\n", "        ", "self", ".", "epsilon", "=", "self", ".", "epsilon", "-", "2", "/", "n_episodes", "if", "self", ".", "epsilon", ">", "self", ".", "epsilon_min", "else", "self", ".", "epsilon_min", "\n", "\n"]], "home.repos.pwc.inspect_result.az79nefy_OrdinalRL.sarsa.ordinal_sarsa_agent.SarsaAgent.preprocess_observation": [[105, 107], ["None"], "methods", ["None"], ["", "def", "preprocess_observation", "(", "self", ",", "obs", ")", ":", "\n", "        ", "return", "obs", "\n", "\n"]], "home.repos.pwc.inspect_result.az79nefy_OrdinalRL.sarsa.ordinal_sarsa_agent.SarsaAgent.reward_to_ordinal": [[109, 116], ["None"], "methods", ["None"], ["", "def", "reward_to_ordinal", "(", "self", ",", "reward", ",", "episode_reward", ",", "done", ")", ":", "\n", "        ", "if", "reward", "==", "-", "10", ":", "\n", "            ", "return", "0", "\n", "", "if", "reward", "==", "-", "1", ":", "\n", "            ", "return", "1", "\n", "", "else", ":", "\n", "            ", "return", "2", "\n", "\n"]], "home.repos.pwc.inspect_result.az79nefy_OrdinalRL.sarsa.ordinal_sarsa_agent.SarsaAgent.check_win_condition": [[118, 124], ["None"], "methods", ["None"], ["", "", "@", "staticmethod", "\n", "def", "check_win_condition", "(", "reward", ",", "episode_reward", ",", "done", ")", ":", "\n", "        ", "if", "done", "and", "reward", "==", "20", ":", "\n", "            ", "return", "True", "\n", "", "else", ":", "\n", "            ", "return", "False", "\n", "\n"]], "home.repos.pwc.inspect_result.az79nefy_OrdinalRL.sarsa.ordinal_sarsa_agent.SarsaAgent.evaluate": [[125, 133], ["round", "round", "ordinal_sarsa_agent.SarsaAgent.average_rewards.append", "ordinal_sarsa_agent.SarsaAgent.win_rates.append", "print", "sum", "len", "sum", "len"], "methods", ["None"], ["", "", "def", "evaluate", "(", "self", ",", "i_episode", ",", "episode_rewards", ",", "episode_wins", ")", ":", "\n", "# compute average episode reward and win rate over last episodes", "\n", "        ", "average_reward", "=", "round", "(", "sum", "(", "episode_rewards", ")", "/", "len", "(", "episode_rewards", ")", ",", "2", ")", "\n", "win_rate", "=", "round", "(", "sum", "(", "episode_wins", ")", "/", "len", "(", "episode_wins", ")", ",", "2", ")", "\n", "# store average episode reward and win rate over last episodes for plotting purposes", "\n", "self", ".", "average_rewards", ".", "append", "(", "average_reward", ")", "\n", "self", ".", "win_rates", ".", "append", "(", "win_rate", ")", "\n", "print", "(", "\"{}\\t{}\\t{}\"", ".", "format", "(", "i_episode", "+", "1", ",", "average_reward", ",", "win_rate", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.az79nefy_OrdinalRL.sarsa.ordinal_sarsa_agent.SarsaAgent.plot": [[135, 149], ["matplotlib.figure", "matplotlib.plot", "matplotlib.xlabel", "matplotlib.ylabel", "matplotlib.figure", "matplotlib.plot", "matplotlib.xlabel", "matplotlib.ylabel", "matplotlib.show", "list", "list", "range", "range"], "methods", ["home.repos.pwc.inspect_result.az79nefy_OrdinalRL.dqn.atari_dqn_single_net_agent.DQNAgent.plot", "home.repos.pwc.inspect_result.az79nefy_OrdinalRL.dqn.atari_dqn_single_net_agent.DQNAgent.plot"], ["", "def", "plot", "(", "self", ",", "n_episodes", ",", "step_size", ")", ":", "\n", "# plot win rate", "\n", "        ", "plt", ".", "figure", "(", ")", "\n", "plt", ".", "plot", "(", "list", "(", "range", "(", "step_size", ",", "n_episodes", "+", "step_size", ",", "step_size", ")", ")", ",", "self", ".", "win_rates", ")", "\n", "plt", ".", "xlabel", "(", "'Number of episodes'", ")", "\n", "plt", ".", "ylabel", "(", "'Win rate'", ")", "\n", "\n", "# plot average score", "\n", "plt", ".", "figure", "(", ")", "\n", "plt", ".", "plot", "(", "list", "(", "range", "(", "step_size", ",", "n_episodes", "+", "step_size", ",", "step_size", ")", ")", ",", "self", ".", "average_rewards", ")", "\n", "plt", ".", "xlabel", "(", "'Number of episodes'", ")", "\n", "plt", ".", "ylabel", "(", "'Average score'", ")", "\n", "\n", "plt", ".", "show", "(", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.az79nefy_OrdinalRL.sarsa.sarsa_discretized_agent.SarsaAgent.__init__": [[8, 25], ["sarsa_discretized_agent.SarsaAgent.init_observation_space", "sarsa_discretized_agent.SarsaAgent.build_obs_dict", "numpy.full", "numpy.random.random_sample"], "methods", ["home.repos.pwc.inspect_result.az79nefy_OrdinalRL.q_learning.alternative_ordinal_q_discretized_agent.QAgent.init_observation_space", "home.repos.pwc.inspect_result.az79nefy_OrdinalRL.q_learning.alternative_ordinal_q_discretized_agent.QAgent.build_obs_dict"], ["    ", "def", "__init__", "(", "self", ",", "alpha", ",", "gamma", ",", "epsilon", ",", "epsilon_min", ",", "n_actions", ",", "n_ordinals", ",", "n_observations", ",", "randomize", ")", ":", "\n", "        ", "self", ".", "alpha", "=", "alpha", "\n", "self", ".", "gamma", "=", "gamma", "\n", "self", ".", "epsilon", "=", "epsilon", "\n", "self", ".", "epsilon_min", "=", "epsilon_min", "\n", "self", ".", "n_actions", "=", "n_actions", "\n", "self", ".", "observation_space", "=", "self", ".", "init_observation_space", "(", ")", "\n", "self", ".", "observation_to_index", "=", "self", ".", "build_obs_dict", "(", "self", ".", "observation_space", ")", "\n", "\n", "# Q_Values (2-dimensional array with float-value for each action (e.g. [Left, Down, Right, Up]) in each observation)", "\n", "if", "randomize", ":", "\n", "            ", "self", ".", "q_values", "=", "np", ".", "random", ".", "random_sample", "(", "(", "n_observations", ",", "n_actions", ")", ")", "*", "0.1", "\n", "", "else", ":", "\n", "            ", "self", ".", "q_values", "=", "np", ".", "full", "(", "(", "n_observations", ",", "n_actions", ")", ",", "0.0", ")", "\n", "\n", "", "self", ".", "win_rates", "=", "[", "]", "\n", "self", ".", "average_rewards", "=", "[", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.az79nefy_OrdinalRL.sarsa.sarsa_discretized_agent.SarsaAgent.init_observation_space": [[27, 34], ["numpy.linspace", "numpy.linspace", "numpy.linspace", "numpy.linspace"], "methods", ["None"], ["", "@", "staticmethod", "\n", "def", "init_observation_space", "(", ")", ":", "\n", "        ", "cart_pos_space", "=", "np", ".", "linspace", "(", "-", "2.4", ",", "2.4", ",", "10", ")", "\n", "cart_vel_space", "=", "np", ".", "linspace", "(", "-", "4", ",", "4", ",", "10", ")", "\n", "pole_theta_space", "=", "np", ".", "linspace", "(", "-", "0.20943951", ",", "0.20943951", ",", "10", ")", "\n", "pole_theta_vel_space", "=", "np", ".", "linspace", "(", "-", "4", ",", "4", ",", "10", ")", "\n", "return", "[", "cart_pos_space", ",", "cart_vel_space", ",", "pole_theta_space", ",", "pole_theta_vel_space", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.az79nefy_OrdinalRL.sarsa.sarsa_discretized_agent.SarsaAgent.build_obs_dict": [[35, 46], ["list", "range", "itertools.product", "len"], "methods", ["None"], ["", "@", "staticmethod", "\n", "def", "build_obs_dict", "(", "observation_space", ")", ":", "\n", "# List of all possible discrete observations", "\n", "        ", "observation_range", "=", "[", "range", "(", "len", "(", "i", ")", "+", "1", ")", "for", "i", "in", "observation_space", "]", "\n", "# Dictionary that maps discretized observations to array indices", "\n", "observation_to_index", "=", "{", "}", "\n", "index_counter", "=", "0", "\n", "for", "observation", "in", "list", "(", "itertools", ".", "product", "(", "*", "observation_range", ")", ")", ":", "\n", "            ", "observation_to_index", "[", "observation", "]", "=", "index_counter", "\n", "index_counter", "+=", "1", "\n", "", "return", "observation_to_index", "\n", "\n"]], "home.repos.pwc.inspect_result.az79nefy_OrdinalRL.sarsa.sarsa_discretized_agent.SarsaAgent.update": [[47, 50], ["sarsa_discretized_agent.SarsaAgent.remap_reward", "sarsa_discretized_agent.SarsaAgent.update_q_values"], "methods", ["home.repos.pwc.inspect_result.az79nefy_OrdinalRL.dqn.dqn_single_net_agent.DQNAgent.remap_reward", "home.repos.pwc.inspect_result.az79nefy_OrdinalRL.q_learning.q_discretized_agent.QAgent.update_q_values"], ["", "def", "update", "(", "self", ",", "prev_obs", ",", "prev_act", ",", "obs", ",", "act", ",", "reward", ",", "episode_reward", ",", "done", ")", ":", "\n", "        ", "reward", "=", "self", ".", "remap_reward", "(", "reward", ",", "episode_reward", ",", "done", ")", "\n", "self", ".", "update_q_values", "(", "prev_obs", ",", "prev_act", ",", "obs", ",", "act", ",", "reward", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.az79nefy_OrdinalRL.sarsa.sarsa_discretized_agent.SarsaAgent.update_q_values": [[52, 56], ["None"], "methods", ["None"], ["", "def", "update_q_values", "(", "self", ",", "prev_obs", ",", "prev_act", ",", "obs", ",", "act", ",", "rew", ")", ":", "\n", "        ", "q_old", "=", "self", ".", "q_values", "[", "prev_obs", ",", "prev_act", "]", "\n", "q_new", "=", "(", "1", "-", "self", ".", "alpha", ")", "*", "q_old", "+", "self", ".", "alpha", "*", "(", "rew", "+", "self", ".", "gamma", "*", "self", ".", "q_values", "[", "obs", ",", "act", "]", ")", "\n", "self", ".", "q_values", "[", "prev_obs", ",", "prev_act", "]", "=", "q_new", "\n", "\n"]], "home.repos.pwc.inspect_result.az79nefy_OrdinalRL.sarsa.sarsa_discretized_agent.SarsaAgent.get_greedy_action": [[57, 59], ["numpy.argmax"], "methods", ["None"], ["", "def", "get_greedy_action", "(", "self", ",", "obs", ")", ":", "\n", "        ", "return", "np", ".", "argmax", "(", "self", ".", "q_values", "[", "obs", "]", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.az79nefy_OrdinalRL.sarsa.sarsa_discretized_agent.SarsaAgent.choose_action": [[61, 69], ["sarsa_discretized_agent.SarsaAgent.get_greedy_action", "random.randrange", "random.random"], "methods", ["home.repos.pwc.inspect_result.az79nefy_OrdinalRL.dqn.atari_dqn_single_net_agent.DQNAgent.get_greedy_action"], ["", "def", "choose_action", "(", "self", ",", "obs", ",", "greedy", ")", ":", "\n", "        ", "greedy_action", "=", "self", ".", "get_greedy_action", "(", "obs", ")", "\n", "# choose random action with probability epsilon", "\n", "if", "not", "greedy", "and", "random", ".", "random", "(", ")", "<", "self", ".", "epsilon", ":", "\n", "            ", "return", "random", ".", "randrange", "(", "self", ".", "n_actions", ")", "\n", "# greedy action is chosen with probability (1 - epsilon)", "\n", "", "else", ":", "\n", "            ", "return", "greedy_action", "\n", "\n"]], "home.repos.pwc.inspect_result.az79nefy_OrdinalRL.sarsa.sarsa_discretized_agent.SarsaAgent.end_episode": [[70, 73], ["None"], "methods", ["None"], ["", "", "def", "end_episode", "(", "self", ",", "n_episodes", ")", ":", "\n", "# gradually reduce epsilon after every done episode", "\n", "        ", "self", ".", "epsilon", "=", "self", ".", "epsilon", "-", "2", "/", "n_episodes", "if", "self", ".", "epsilon", ">", "self", ".", "epsilon_min", "else", "self", ".", "epsilon_min", "\n", "\n"]], "home.repos.pwc.inspect_result.az79nefy_OrdinalRL.sarsa.sarsa_discretized_agent.SarsaAgent.preprocess_observation": [[74, 79], ["range", "len", "discrete_observation.append", "int", "tuple", "numpy.digitize"], "methods", ["None"], ["", "def", "preprocess_observation", "(", "self", ",", "obs", ")", ":", "\n", "        ", "discrete_observation", "=", "[", "]", "\n", "for", "obs_idx", "in", "range", "(", "len", "(", "obs", ")", ")", ":", "\n", "            ", "discrete_observation", ".", "append", "(", "int", "(", "np", ".", "digitize", "(", "obs", "[", "obs_idx", "]", ",", "self", ".", "observation_space", "[", "obs_idx", "]", ")", ")", ")", "\n", "", "return", "self", ".", "observation_to_index", "[", "tuple", "(", "discrete_observation", ")", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.az79nefy_OrdinalRL.sarsa.sarsa_discretized_agent.SarsaAgent.remap_reward": [[81, 86], ["sarsa_discretized_agent.SarsaAgent.check_win_condition"], "methods", ["home.repos.pwc.inspect_result.az79nefy_OrdinalRL.dqn.atari_dqn_single_net_agent.DQNAgent.check_win_condition"], ["", "def", "remap_reward", "(", "self", ",", "reward", ",", "episode_reward", ",", "done", ")", ":", "\n", "        ", "if", "done", "and", "not", "self", ".", "check_win_condition", "(", "reward", ",", "episode_reward", ",", "done", ")", ":", "\n", "            ", "return", "-", "1", "\n", "", "else", ":", "\n", "            ", "return", "0", "\n", "\n"]], "home.repos.pwc.inspect_result.az79nefy_OrdinalRL.sarsa.sarsa_discretized_agent.SarsaAgent.check_win_condition": [[88, 94], ["None"], "methods", ["None"], ["", "", "@", "staticmethod", "\n", "def", "check_win_condition", "(", "reward", ",", "episode_reward", ",", "done", ")", ":", "\n", "        ", "if", "done", "and", "episode_reward", "==", "200", ":", "\n", "            ", "return", "True", "\n", "", "else", ":", "\n", "            ", "return", "False", "\n", "\n"]], "home.repos.pwc.inspect_result.az79nefy_OrdinalRL.sarsa.sarsa_discretized_agent.SarsaAgent.evaluate": [[95, 103], ["round", "round", "sarsa_discretized_agent.SarsaAgent.average_rewards.append", "sarsa_discretized_agent.SarsaAgent.win_rates.append", "print", "sum", "len", "sum", "len"], "methods", ["None"], ["", "", "def", "evaluate", "(", "self", ",", "i_episode", ",", "episode_rewards", ",", "episode_wins", ")", ":", "\n", "# compute average episode reward and win rate over last episodes", "\n", "        ", "average_reward", "=", "round", "(", "sum", "(", "episode_rewards", ")", "/", "len", "(", "episode_rewards", ")", ",", "2", ")", "\n", "win_rate", "=", "round", "(", "sum", "(", "episode_wins", ")", "/", "len", "(", "episode_wins", ")", ",", "2", ")", "\n", "# store average episode reward and win rate over last episodes for plotting purposes", "\n", "self", ".", "average_rewards", ".", "append", "(", "average_reward", ")", "\n", "self", ".", "win_rates", ".", "append", "(", "win_rate", ")", "\n", "print", "(", "\"{}\\t{}\\t{}\"", ".", "format", "(", "i_episode", "+", "1", ",", "average_reward", ",", "win_rate", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.az79nefy_OrdinalRL.sarsa.sarsa_discretized_agent.SarsaAgent.plot": [[105, 119], ["matplotlib.figure", "matplotlib.plot", "matplotlib.xlabel", "matplotlib.ylabel", "matplotlib.figure", "matplotlib.plot", "matplotlib.xlabel", "matplotlib.ylabel", "matplotlib.show", "list", "list", "range", "range"], "methods", ["home.repos.pwc.inspect_result.az79nefy_OrdinalRL.dqn.atari_dqn_single_net_agent.DQNAgent.plot", "home.repos.pwc.inspect_result.az79nefy_OrdinalRL.dqn.atari_dqn_single_net_agent.DQNAgent.plot"], ["", "def", "plot", "(", "self", ",", "n_episodes", ",", "step_size", ")", ":", "\n", "# plot win rate", "\n", "        ", "plt", ".", "figure", "(", ")", "\n", "plt", ".", "plot", "(", "list", "(", "range", "(", "step_size", ",", "n_episodes", "+", "step_size", ",", "step_size", ")", ")", ",", "self", ".", "win_rates", ")", "\n", "plt", ".", "xlabel", "(", "'Number of episodes'", ")", "\n", "plt", ".", "ylabel", "(", "'Win rate'", ")", "\n", "\n", "# plot average score", "\n", "plt", ".", "figure", "(", ")", "\n", "plt", ".", "plot", "(", "list", "(", "range", "(", "step_size", ",", "n_episodes", "+", "step_size", ",", "step_size", ")", ")", ",", "self", ".", "average_rewards", ")", "\n", "plt", ".", "xlabel", "(", "'Number of episodes'", ")", "\n", "plt", ".", "ylabel", "(", "'Average score'", ")", "\n", "\n", "plt", ".", "show", "(", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.az79nefy_OrdinalRL.sarsa.sarsa_agent.SarsaAgent.__init__": [[7, 22], ["numpy.full", "numpy.random.random_sample"], "methods", ["None"], ["    ", "def", "__init__", "(", "self", ",", "alpha", ",", "gamma", ",", "epsilon", ",", "epsilon_min", ",", "n_actions", ",", "n_ordinals", ",", "n_observations", ",", "randomize", ")", ":", "\n", "        ", "self", ".", "alpha", "=", "alpha", "\n", "self", ".", "gamma", "=", "gamma", "\n", "self", ".", "epsilon", "=", "epsilon", "\n", "self", ".", "epsilon_min", "=", "epsilon_min", "\n", "self", ".", "n_actions", "=", "n_actions", "\n", "\n", "# Q_Values (2-dimensional array with float-value for each action (e.g. [Left, Down, Right, Up]) in each observation)", "\n", "if", "randomize", ":", "\n", "            ", "self", ".", "q_values", "=", "np", ".", "random", ".", "random_sample", "(", "(", "n_observations", ",", "n_actions", ")", ")", "*", "0.1", "\n", "", "else", ":", "\n", "            ", "self", ".", "q_values", "=", "np", ".", "full", "(", "(", "n_observations", ",", "n_actions", ")", ",", "0.0", ")", "\n", "\n", "", "self", ".", "win_rates", "=", "[", "]", "\n", "self", ".", "average_rewards", "=", "[", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.az79nefy_OrdinalRL.sarsa.sarsa_agent.SarsaAgent.update": [[23, 26], ["sarsa_agent.SarsaAgent.remap_reward", "sarsa_agent.SarsaAgent.update_q_values"], "methods", ["home.repos.pwc.inspect_result.az79nefy_OrdinalRL.dqn.dqn_single_net_agent.DQNAgent.remap_reward", "home.repos.pwc.inspect_result.az79nefy_OrdinalRL.q_learning.q_discretized_agent.QAgent.update_q_values"], ["", "def", "update", "(", "self", ",", "prev_obs", ",", "prev_act", ",", "obs", ",", "act", ",", "reward", ",", "episode_reward", ",", "done", ")", ":", "\n", "        ", "reward", "=", "self", ".", "remap_reward", "(", "reward", ",", "episode_reward", ",", "done", ")", "\n", "self", ".", "update_q_values", "(", "prev_obs", ",", "prev_act", ",", "obs", ",", "act", ",", "reward", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.az79nefy_OrdinalRL.sarsa.sarsa_agent.SarsaAgent.update_q_values": [[28, 32], ["None"], "methods", ["None"], ["", "def", "update_q_values", "(", "self", ",", "prev_obs", ",", "prev_act", ",", "obs", ",", "act", ",", "rew", ")", ":", "\n", "        ", "q_old", "=", "self", ".", "q_values", "[", "prev_obs", ",", "prev_act", "]", "\n", "q_new", "=", "(", "1", "-", "self", ".", "alpha", ")", "*", "q_old", "+", "self", ".", "alpha", "*", "(", "rew", "+", "self", ".", "gamma", "*", "self", ".", "q_values", "[", "obs", ",", "act", "]", ")", "\n", "self", ".", "q_values", "[", "prev_obs", ",", "prev_act", "]", "=", "q_new", "\n", "\n"]], "home.repos.pwc.inspect_result.az79nefy_OrdinalRL.sarsa.sarsa_agent.SarsaAgent.get_greedy_action": [[33, 35], ["numpy.argmax"], "methods", ["None"], ["", "def", "get_greedy_action", "(", "self", ",", "obs", ")", ":", "\n", "        ", "return", "np", ".", "argmax", "(", "self", ".", "q_values", "[", "obs", "]", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.az79nefy_OrdinalRL.sarsa.sarsa_agent.SarsaAgent.choose_action": [[37, 45], ["sarsa_agent.SarsaAgent.get_greedy_action", "random.randrange", "random.random"], "methods", ["home.repos.pwc.inspect_result.az79nefy_OrdinalRL.dqn.atari_dqn_single_net_agent.DQNAgent.get_greedy_action"], ["", "def", "choose_action", "(", "self", ",", "obs", ",", "greedy", ")", ":", "\n", "        ", "greedy_action", "=", "self", ".", "get_greedy_action", "(", "obs", ")", "\n", "# choose random action with probability epsilon", "\n", "if", "not", "greedy", "and", "random", ".", "random", "(", ")", "<", "self", ".", "epsilon", ":", "\n", "            ", "return", "random", ".", "randrange", "(", "self", ".", "n_actions", ")", "\n", "# greedy action is chosen with probability (1 - epsilon)", "\n", "", "else", ":", "\n", "            ", "return", "greedy_action", "\n", "\n"]], "home.repos.pwc.inspect_result.az79nefy_OrdinalRL.sarsa.sarsa_agent.SarsaAgent.end_episode": [[46, 49], ["None"], "methods", ["None"], ["", "", "def", "end_episode", "(", "self", ",", "n_episodes", ")", ":", "\n", "# gradually reduce epsilon after every done episode", "\n", "        ", "self", ".", "epsilon", "=", "self", ".", "epsilon", "-", "2", "/", "n_episodes", "if", "self", ".", "epsilon", ">", "self", ".", "epsilon_min", "else", "self", ".", "epsilon_min", "\n", "\n"]], "home.repos.pwc.inspect_result.az79nefy_OrdinalRL.sarsa.sarsa_agent.SarsaAgent.preprocess_observation": [[50, 52], ["None"], "methods", ["None"], ["", "def", "preprocess_observation", "(", "self", ",", "obs", ")", ":", "\n", "        ", "return", "obs", "\n", "\n"]], "home.repos.pwc.inspect_result.az79nefy_OrdinalRL.sarsa.sarsa_agent.SarsaAgent.remap_reward": [[54, 56], ["None"], "methods", ["None"], ["", "def", "remap_reward", "(", "self", ",", "reward", ",", "episode_reward", ",", "done", ")", ":", "\n", "        ", "return", "reward", "\n", "\n"]], "home.repos.pwc.inspect_result.az79nefy_OrdinalRL.sarsa.sarsa_agent.SarsaAgent.check_win_condition": [[58, 64], ["None"], "methods", ["None"], ["", "@", "staticmethod", "\n", "def", "check_win_condition", "(", "reward", ",", "episode_reward", ",", "done", ")", ":", "\n", "        ", "if", "done", "and", "reward", "==", "20", ":", "\n", "            ", "return", "True", "\n", "", "else", ":", "\n", "            ", "return", "False", "\n", "\n"]], "home.repos.pwc.inspect_result.az79nefy_OrdinalRL.sarsa.sarsa_agent.SarsaAgent.evaluate": [[65, 73], ["round", "round", "sarsa_agent.SarsaAgent.average_rewards.append", "sarsa_agent.SarsaAgent.win_rates.append", "print", "sum", "len", "sum", "len"], "methods", ["None"], ["", "", "def", "evaluate", "(", "self", ",", "i_episode", ",", "episode_rewards", ",", "episode_wins", ")", ":", "\n", "# compute average episode reward and win rate over last episodes", "\n", "        ", "average_reward", "=", "round", "(", "sum", "(", "episode_rewards", ")", "/", "len", "(", "episode_rewards", ")", ",", "2", ")", "\n", "win_rate", "=", "round", "(", "sum", "(", "episode_wins", ")", "/", "len", "(", "episode_wins", ")", ",", "2", ")", "\n", "# store average episode reward and win rate over last episodes for plotting purposes", "\n", "self", ".", "average_rewards", ".", "append", "(", "average_reward", ")", "\n", "self", ".", "win_rates", ".", "append", "(", "win_rate", ")", "\n", "print", "(", "\"{}\\t{}\\t{}\"", ".", "format", "(", "i_episode", "+", "1", ",", "average_reward", ",", "win_rate", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.az79nefy_OrdinalRL.sarsa.sarsa_agent.SarsaAgent.plot": [[75, 89], ["matplotlib.figure", "matplotlib.plot", "matplotlib.xlabel", "matplotlib.ylabel", "matplotlib.figure", "matplotlib.plot", "matplotlib.xlabel", "matplotlib.ylabel", "matplotlib.show", "list", "list", "range", "range"], "methods", ["home.repos.pwc.inspect_result.az79nefy_OrdinalRL.dqn.atari_dqn_single_net_agent.DQNAgent.plot", "home.repos.pwc.inspect_result.az79nefy_OrdinalRL.dqn.atari_dqn_single_net_agent.DQNAgent.plot"], ["", "def", "plot", "(", "self", ",", "n_episodes", ",", "step_size", ")", ":", "\n", "# plot win rate", "\n", "        ", "plt", ".", "figure", "(", ")", "\n", "plt", ".", "plot", "(", "list", "(", "range", "(", "step_size", ",", "n_episodes", "+", "step_size", ",", "step_size", ")", ")", ",", "self", ".", "win_rates", ")", "\n", "plt", ".", "xlabel", "(", "'Number of episodes'", ")", "\n", "plt", ".", "ylabel", "(", "'Win rate'", ")", "\n", "\n", "# plot average score", "\n", "plt", ".", "figure", "(", ")", "\n", "plt", ".", "plot", "(", "list", "(", "range", "(", "step_size", ",", "n_episodes", "+", "step_size", ",", "step_size", ")", ")", ",", "self", ".", "average_rewards", ")", "\n", "plt", ".", "xlabel", "(", "'Number of episodes'", ")", "\n", "plt", ".", "ylabel", "(", "'Average score'", ")", "\n", "\n", "plt", ".", "show", "(", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.az79nefy_OrdinalRL.sarsa.ordinal_sarsa_discretized_agent.SarsaAgent.__init__": [[8, 23], ["ordinal_sarsa_discretized_agent.SarsaAgent.init_observation_space", "ordinal_sarsa_discretized_agent.SarsaAgent.build_obs_dict", "numpy.full"], "methods", ["home.repos.pwc.inspect_result.az79nefy_OrdinalRL.q_learning.alternative_ordinal_q_discretized_agent.QAgent.init_observation_space", "home.repos.pwc.inspect_result.az79nefy_OrdinalRL.q_learning.alternative_ordinal_q_discretized_agent.QAgent.build_obs_dict"], ["    ", "def", "__init__", "(", "self", ",", "alpha", ",", "gamma", ",", "epsilon", ",", "epsilon_min", ",", "n_actions", ",", "n_ordinals", ",", "n_observations", ",", "randomize", ")", ":", "\n", "        ", "self", ".", "alpha", "=", "alpha", "\n", "self", ".", "gamma", "=", "gamma", "\n", "self", ".", "epsilon", "=", "epsilon", "\n", "self", ".", "epsilon_min", "=", "epsilon_min", "\n", "self", ".", "n_actions", "=", "n_actions", "\n", "self", ".", "n_ordinals", "=", "n_ordinals", "\n", "self", ".", "observation_space", "=", "self", ".", "init_observation_space", "(", ")", "\n", "self", ".", "observation_to_index", "=", "self", ".", "build_obs_dict", "(", "self", ".", "observation_space", ")", "\n", "\n", "# Ordinal_Values (3-dimensional array with ordinal_value (array of floats) for each action in each observation)", "\n", "self", ".", "ordinal_values", "=", "np", ".", "full", "(", "(", "n_observations", ",", "n_actions", ",", "n_ordinals", ")", ",", "0.0", ")", "\n", "\n", "self", ".", "win_rates", "=", "[", "]", "\n", "self", ".", "average_rewards", "=", "[", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.az79nefy_OrdinalRL.sarsa.ordinal_sarsa_discretized_agent.SarsaAgent.init_observation_space": [[25, 32], ["numpy.linspace", "numpy.linspace", "numpy.linspace", "numpy.linspace"], "methods", ["None"], ["", "@", "staticmethod", "\n", "def", "init_observation_space", "(", ")", ":", "\n", "        ", "cart_pos_space", "=", "np", ".", "linspace", "(", "-", "2.4", ",", "2.4", ",", "10", ")", "\n", "cart_vel_space", "=", "np", ".", "linspace", "(", "-", "4", ",", "4", ",", "10", ")", "\n", "pole_theta_space", "=", "np", ".", "linspace", "(", "-", "0.20943951", ",", "0.20943951", ",", "10", ")", "\n", "pole_theta_vel_space", "=", "np", ".", "linspace", "(", "-", "4", ",", "4", ",", "10", ")", "\n", "return", "[", "cart_pos_space", ",", "cart_vel_space", ",", "pole_theta_space", ",", "pole_theta_vel_space", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.az79nefy_OrdinalRL.sarsa.ordinal_sarsa_discretized_agent.SarsaAgent.build_obs_dict": [[33, 44], ["list", "range", "itertools.product", "len"], "methods", ["None"], ["", "@", "staticmethod", "\n", "def", "build_obs_dict", "(", "observation_space", ")", ":", "\n", "# List of all possible discrete observations", "\n", "        ", "observation_range", "=", "[", "range", "(", "len", "(", "i", ")", "+", "1", ")", "for", "i", "in", "observation_space", "]", "\n", "# Dictionary that maps discretized observations to array indices", "\n", "observation_to_index", "=", "{", "}", "\n", "index_counter", "=", "0", "\n", "for", "observation", "in", "list", "(", "itertools", ".", "product", "(", "*", "observation_range", ")", ")", ":", "\n", "            ", "observation_to_index", "[", "observation", "]", "=", "index_counter", "\n", "index_counter", "+=", "1", "\n", "", "return", "observation_to_index", "\n", "\n"]], "home.repos.pwc.inspect_result.az79nefy_OrdinalRL.sarsa.ordinal_sarsa_discretized_agent.SarsaAgent.update": [[45, 49], ["ordinal_sarsa_discretized_agent.SarsaAgent.reward_to_ordinal", "ordinal_sarsa_discretized_agent.SarsaAgent.update_ordinal_values"], "methods", ["home.repos.pwc.inspect_result.az79nefy_OrdinalRL.dqn.ordinal_dqn_discretized_agent.DQNAgent.reward_to_ordinal", "home.repos.pwc.inspect_result.az79nefy_OrdinalRL.q_learning.alternative_ordinal_q_discretized_agent.QAgent.update_ordinal_values"], ["", "def", "update", "(", "self", ",", "prev_obs", ",", "prev_act", ",", "obs", ",", "act", ",", "reward", ",", "episode_reward", ",", "done", ")", ":", "\n", "        ", "ordinal", "=", "self", ".", "reward_to_ordinal", "(", "reward", ",", "episode_reward", ",", "done", ")", "\n", "# update ordinal_values with received ordinal", "\n", "self", ".", "update_ordinal_values", "(", "prev_obs", ",", "prev_act", ",", "obs", ",", "act", ",", "ordinal", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.az79nefy_OrdinalRL.sarsa.ordinal_sarsa_discretized_agent.SarsaAgent.update_ordinal_values": [[51, 59], ["range"], "methods", ["None"], ["", "def", "update_ordinal_values", "(", "self", ",", "prev_obs", ",", "prev_act", ",", "obs", ",", "act", ",", "ordinal", ")", ":", "\n", "# reduce old data weight", "\n", "        ", "for", "i", "in", "range", "(", "self", ".", "n_ordinals", ")", ":", "\n", "            ", "self", ".", "ordinal_values", "[", "prev_obs", ",", "prev_act", ",", "i", "]", "*=", "(", "1", "-", "self", ".", "alpha", ")", "\n", "self", ".", "ordinal_values", "[", "prev_obs", ",", "prev_act", ",", "i", "]", "+=", "self", ".", "alpha", "*", "(", "self", ".", "gamma", "*", "self", ".", "ordinal_values", "[", "obs", ",", "act", ",", "i", "]", ")", "\n", "\n", "# add new data point", "\n", "", "self", ".", "ordinal_values", "[", "prev_obs", ",", "prev_act", ",", "ordinal", "]", "+=", "self", ".", "alpha", "\n", "\n"]], "home.repos.pwc.inspect_result.az79nefy_OrdinalRL.sarsa.ordinal_sarsa_discretized_agent.SarsaAgent.compute_borda_scores": [[61, 111], ["numpy.zeros", "range", "numpy.count_nonzero", "range", "borda_scores.append", "borda_scores.append", "range", "borda_scores.append", "range"], "methods", ["None"], ["", "def", "compute_borda_scores", "(", "self", ",", "obs", ")", ":", "\n", "# sum up all ordinal values per action for given observation", "\n", "        ", "ordinal_value_sum_per_action", "=", "np", ".", "zeros", "(", "self", ".", "n_actions", ")", "\n", "for", "action_a", "in", "range", "(", "self", ".", "n_actions", ")", ":", "\n", "            ", "for", "ordinal_value", "in", "self", ".", "ordinal_values", "[", "obs", ",", "action_a", "]", ":", "\n", "                ", "ordinal_value_sum_per_action", "[", "action_a", "]", "+=", "ordinal_value", "\n", "\n", "# count actions whose ordinal value sum is not zero (no comparision possible for actions without ordinal_value)", "\n", "", "", "non_zero_action_count", "=", "np", ".", "count_nonzero", "(", "ordinal_value_sum_per_action", ")", "\n", "actions_to_compare_count", "=", "non_zero_action_count", "-", "1", "\n", "\n", "borda_scores", "=", "[", "]", "\n", "# compute borda_values for action_a (probability that action_a wins against any other action)", "\n", "for", "action_a", "in", "range", "(", "self", ".", "n_actions", ")", ":", "\n", "# if action has not yet recorded any ordinal values, action has to be played (set borda_value to 1.0)", "\n", "            ", "if", "ordinal_value_sum_per_action", "[", "action_a", "]", "==", "0", ":", "\n", "                ", "borda_scores", ".", "append", "(", "1.0", ")", "\n", "continue", "\n", "\n", "", "if", "actions_to_compare_count", "<", "1", ":", "\n", "# set lower than 1.0 (borda_value for zero_actions is 1.0)", "\n", "                ", "borda_scores", ".", "append", "(", "0.5", ")", "\n", "", "else", ":", "\n", "# over all actions: sum up the probabilities that action_a wins against the given action", "\n", "                ", "winning_probability_a_sum", "=", "0", "\n", "# compare action_a to all other actions", "\n", "for", "action_b", "in", "range", "(", "self", ".", "n_actions", ")", ":", "\n", "                    ", "if", "action_a", "==", "action_b", ":", "\n", "                        ", "continue", "\n", "# not comparable if action_b has no ordinal_values", "\n", "", "if", "ordinal_value_sum_per_action", "[", "action_b", "]", "==", "0", ":", "\n", "                        ", "continue", "\n", "", "else", ":", "\n", "# probability that action_a wins against action_b", "\n", "                        ", "winning_probability_a", "=", "0", "\n", "# running ordinal probability that action_b is worse than current investigated ordinal", "\n", "worse_probability_b", "=", "0", "\n", "for", "ordinal_count", "in", "range", "(", "self", ".", "n_ordinals", ")", ":", "\n", "                            ", "ordinal_probability_a", "=", "self", ".", "ordinal_values", "[", "obs", ",", "action_a", ",", "ordinal_count", "]", "/", "ordinal_value_sum_per_action", "[", "action_a", "]", "\n", "# ordinal_probability_b is also the tie probability", "\n", "ordinal_probability_b", "=", "(", "self", ".", "ordinal_values", "[", "obs", ",", "action_b", ",", "ordinal_count", "]", "/", "\n", "ordinal_value_sum_per_action", "[", "action_b", "]", ")", "\n", "winning_probability_a", "+=", "ordinal_probability_a", "*", "(", "worse_probability_b", "+", "ordinal_probability_b", "/", "2.0", ")", "\n", "worse_probability_b", "+=", "ordinal_probability_b", "\n", "", "winning_probability_a_sum", "+=", "winning_probability_a", "\n", "# normalize summed up probabilities with number of actions that have been compared", "\n", "", "", "borda_scores", ".", "append", "(", "winning_probability_a_sum", "/", "actions_to_compare_count", ")", "\n", "", "", "return", "borda_scores", "\n", "\n"]], "home.repos.pwc.inspect_result.az79nefy_OrdinalRL.sarsa.ordinal_sarsa_discretized_agent.SarsaAgent.get_greedy_action": [[112, 114], ["numpy.argmax", "ordinal_sarsa_discretized_agent.SarsaAgent.compute_borda_scores"], "methods", ["home.repos.pwc.inspect_result.az79nefy_OrdinalRL.dqn.ordinal_dqn_discretized_agent.DQNAgent.compute_borda_scores"], ["", "def", "get_greedy_action", "(", "self", ",", "obs", ")", ":", "\n", "        ", "return", "np", ".", "argmax", "(", "self", ".", "compute_borda_scores", "(", "obs", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.az79nefy_OrdinalRL.sarsa.ordinal_sarsa_discretized_agent.SarsaAgent.choose_action": [[116, 124], ["ordinal_sarsa_discretized_agent.SarsaAgent.get_greedy_action", "random.randrange", "random.random"], "methods", ["home.repos.pwc.inspect_result.az79nefy_OrdinalRL.dqn.atari_dqn_single_net_agent.DQNAgent.get_greedy_action"], ["", "def", "choose_action", "(", "self", ",", "obs", ",", "greedy", ")", ":", "\n", "        ", "greedy_action", "=", "self", ".", "get_greedy_action", "(", "obs", ")", "\n", "# choose random action with probability epsilon", "\n", "if", "not", "greedy", "and", "random", ".", "random", "(", ")", "<", "self", ".", "epsilon", ":", "\n", "            ", "return", "random", ".", "randrange", "(", "self", ".", "n_actions", ")", "\n", "# greedy action is chosen with probability (1 - epsilon)", "\n", "", "else", ":", "\n", "            ", "return", "greedy_action", "\n", "\n"]], "home.repos.pwc.inspect_result.az79nefy_OrdinalRL.sarsa.ordinal_sarsa_discretized_agent.SarsaAgent.end_episode": [[125, 128], ["None"], "methods", ["None"], ["", "", "def", "end_episode", "(", "self", ",", "n_episodes", ")", ":", "\n", "# gradually reduce epsilon after every done episode", "\n", "        ", "self", ".", "epsilon", "=", "self", ".", "epsilon", "-", "2", "/", "n_episodes", "if", "self", ".", "epsilon", ">", "self", ".", "epsilon_min", "else", "self", ".", "epsilon_min", "\n", "\n"]], "home.repos.pwc.inspect_result.az79nefy_OrdinalRL.sarsa.ordinal_sarsa_discretized_agent.SarsaAgent.preprocess_observation": [[129, 134], ["range", "len", "discrete_observation.append", "int", "tuple", "numpy.digitize"], "methods", ["None"], ["", "def", "preprocess_observation", "(", "self", ",", "obs", ")", ":", "\n", "        ", "discrete_observation", "=", "[", "]", "\n", "for", "obs_idx", "in", "range", "(", "len", "(", "obs", ")", ")", ":", "\n", "            ", "discrete_observation", ".", "append", "(", "int", "(", "np", ".", "digitize", "(", "obs", "[", "obs_idx", "]", ",", "self", ".", "observation_space", "[", "obs_idx", "]", ")", ")", ")", "\n", "", "return", "self", ".", "observation_to_index", "[", "tuple", "(", "discrete_observation", ")", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.az79nefy_OrdinalRL.sarsa.ordinal_sarsa_discretized_agent.SarsaAgent.reward_to_ordinal": [[136, 141], ["ordinal_sarsa_discretized_agent.SarsaAgent.check_win_condition"], "methods", ["home.repos.pwc.inspect_result.az79nefy_OrdinalRL.dqn.atari_dqn_single_net_agent.DQNAgent.check_win_condition"], ["", "def", "reward_to_ordinal", "(", "self", ",", "reward", ",", "episode_reward", ",", "done", ")", ":", "\n", "        ", "if", "done", "and", "not", "self", ".", "check_win_condition", "(", "reward", ",", "episode_reward", ",", "done", ")", ":", "\n", "            ", "return", "0", "\n", "", "else", ":", "\n", "            ", "return", "1", "\n", "\n"]], "home.repos.pwc.inspect_result.az79nefy_OrdinalRL.sarsa.ordinal_sarsa_discretized_agent.SarsaAgent.check_win_condition": [[143, 149], ["None"], "methods", ["None"], ["", "", "@", "staticmethod", "\n", "def", "check_win_condition", "(", "reward", ",", "episode_reward", ",", "done", ")", ":", "\n", "        ", "if", "done", "and", "episode_reward", "==", "200", ":", "\n", "            ", "return", "True", "\n", "", "else", ":", "\n", "            ", "return", "False", "\n", "\n"]], "home.repos.pwc.inspect_result.az79nefy_OrdinalRL.sarsa.ordinal_sarsa_discretized_agent.SarsaAgent.evaluate": [[150, 158], ["round", "round", "ordinal_sarsa_discretized_agent.SarsaAgent.average_rewards.append", "ordinal_sarsa_discretized_agent.SarsaAgent.win_rates.append", "print", "sum", "len", "sum", "len"], "methods", ["None"], ["", "", "def", "evaluate", "(", "self", ",", "i_episode", ",", "episode_rewards", ",", "episode_wins", ")", ":", "\n", "# compute average episode reward and win rate over last episodes", "\n", "        ", "average_reward", "=", "round", "(", "sum", "(", "episode_rewards", ")", "/", "len", "(", "episode_rewards", ")", ",", "2", ")", "\n", "win_rate", "=", "round", "(", "sum", "(", "episode_wins", ")", "/", "len", "(", "episode_wins", ")", ",", "2", ")", "\n", "# store average episode reward and win rate over last episodes for plotting purposes", "\n", "self", ".", "average_rewards", ".", "append", "(", "average_reward", ")", "\n", "self", ".", "win_rates", ".", "append", "(", "win_rate", ")", "\n", "print", "(", "\"{}\\t{}\\t{}\"", ".", "format", "(", "i_episode", "+", "1", ",", "average_reward", ",", "win_rate", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.az79nefy_OrdinalRL.sarsa.ordinal_sarsa_discretized_agent.SarsaAgent.plot": [[160, 174], ["matplotlib.figure", "matplotlib.plot", "matplotlib.xlabel", "matplotlib.ylabel", "matplotlib.figure", "matplotlib.plot", "matplotlib.xlabel", "matplotlib.ylabel", "matplotlib.show", "list", "list", "range", "range"], "methods", ["home.repos.pwc.inspect_result.az79nefy_OrdinalRL.dqn.atari_dqn_single_net_agent.DQNAgent.plot", "home.repos.pwc.inspect_result.az79nefy_OrdinalRL.dqn.atari_dqn_single_net_agent.DQNAgent.plot"], ["", "def", "plot", "(", "self", ",", "n_episodes", ",", "step_size", ")", ":", "\n", "# plot win rate", "\n", "        ", "plt", ".", "figure", "(", ")", "\n", "plt", ".", "plot", "(", "list", "(", "range", "(", "step_size", ",", "n_episodes", "+", "step_size", ",", "step_size", ")", ")", ",", "self", ".", "win_rates", ")", "\n", "plt", ".", "xlabel", "(", "'Number of episodes'", ")", "\n", "plt", ".", "ylabel", "(", "'Win rate'", ")", "\n", "\n", "# plot average score", "\n", "plt", ".", "figure", "(", ")", "\n", "plt", ".", "plot", "(", "list", "(", "range", "(", "step_size", ",", "n_episodes", "+", "step_size", ",", "step_size", ")", ")", ",", "self", ".", "average_rewards", ")", "\n", "plt", ".", "xlabel", "(", "'Number of episodes'", ")", "\n", "plt", ".", "ylabel", "(", "'Average score'", ")", "\n", "\n", "plt", ".", "show", "(", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.az79nefy_OrdinalRL.sarsa_lambda.ordinal_sarsa_lambda_discretized_agent.SarsaLambdaAgent.__init__": [[9, 27], ["ordinal_sarsa_lambda_discretized_agent.SarsaLambdaAgent.init_observation_space", "ordinal_sarsa_lambda_discretized_agent.SarsaLambdaAgent.build_obs_dict", "numpy.full", "collections.deque"], "methods", ["home.repos.pwc.inspect_result.az79nefy_OrdinalRL.q_learning.alternative_ordinal_q_discretized_agent.QAgent.init_observation_space", "home.repos.pwc.inspect_result.az79nefy_OrdinalRL.q_learning.alternative_ordinal_q_discretized_agent.QAgent.build_obs_dict"], ["    ", "def", "__init__", "(", "self", ",", "alpha", ",", "gamma", ",", "epsilon", ",", "epsilon_min", ",", "lambda_", ",", "n_actions", ",", "n_ordinals", ",", "n_observations", ",", "randomize", ")", ":", "\n", "        ", "self", ".", "alpha", "=", "alpha", "\n", "self", ".", "gamma", "=", "gamma", "\n", "self", ".", "epsilon", "=", "epsilon", "\n", "self", ".", "epsilon_min", "=", "epsilon_min", "\n", "self", ".", "lambda_", "=", "lambda_", "\n", "self", ".", "n_actions", "=", "n_actions", "\n", "self", ".", "n_ordinals", "=", "n_ordinals", "\n", "self", ".", "observation_space", "=", "self", ".", "init_observation_space", "(", ")", "\n", "self", ".", "observation_to_index", "=", "self", ".", "build_obs_dict", "(", "self", ".", "observation_space", ")", "\n", "\n", "# Ordinal_Values (3-dimensional array with ordinal_value (array of floats) for each action in each observation)", "\n", "self", ".", "ordinal_values", "=", "np", ".", "full", "(", "(", "n_observations", ",", "n_actions", ",", "n_ordinals", ")", ",", "0.0", ")", "\n", "\n", "# History of visited state-action-pairs for eligibility trace computation", "\n", "self", ".", "history", "=", "deque", "(", "maxlen", "=", "20", ")", "\n", "self", ".", "win_rates", "=", "[", "]", "\n", "self", ".", "average_rewards", "=", "[", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.az79nefy_OrdinalRL.sarsa_lambda.ordinal_sarsa_lambda_discretized_agent.SarsaLambdaAgent.init_observation_space": [[29, 36], ["numpy.linspace", "numpy.linspace", "numpy.linspace", "numpy.linspace"], "methods", ["None"], ["", "@", "staticmethod", "\n", "def", "init_observation_space", "(", ")", ":", "\n", "        ", "cart_pos_space", "=", "np", ".", "linspace", "(", "-", "2.4", ",", "2.4", ",", "10", ")", "\n", "cart_vel_space", "=", "np", ".", "linspace", "(", "-", "4", ",", "4", ",", "10", ")", "\n", "pole_theta_space", "=", "np", ".", "linspace", "(", "-", "0.20943951", ",", "0.20943951", ",", "10", ")", "\n", "pole_theta_vel_space", "=", "np", ".", "linspace", "(", "-", "4", ",", "4", ",", "10", ")", "\n", "return", "[", "cart_pos_space", ",", "cart_vel_space", ",", "pole_theta_space", ",", "pole_theta_vel_space", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.az79nefy_OrdinalRL.sarsa_lambda.ordinal_sarsa_lambda_discretized_agent.SarsaLambdaAgent.build_obs_dict": [[37, 48], ["list", "range", "itertools.product", "len"], "methods", ["None"], ["", "@", "staticmethod", "\n", "def", "build_obs_dict", "(", "observation_space", ")", ":", "\n", "# List of all possible discrete observations", "\n", "        ", "observation_range", "=", "[", "range", "(", "len", "(", "i", ")", "+", "1", ")", "for", "i", "in", "observation_space", "]", "\n", "# Dictionary that maps discretized observations to array indices", "\n", "observation_to_index", "=", "{", "}", "\n", "index_counter", "=", "0", "\n", "for", "observation", "in", "list", "(", "itertools", ".", "product", "(", "*", "observation_range", ")", ")", ":", "\n", "            ", "observation_to_index", "[", "observation", "]", "=", "index_counter", "\n", "index_counter", "+=", "1", "\n", "", "return", "observation_to_index", "\n", "\n"]], "home.repos.pwc.inspect_result.az79nefy_OrdinalRL.sarsa_lambda.ordinal_sarsa_lambda_discretized_agent.SarsaLambdaAgent.update": [[49, 55], ["ordinal_sarsa_lambda_discretized_agent.SarsaLambdaAgent.reward_to_ordinal", "ordinal_sarsa_lambda_discretized_agent.SarsaLambdaAgent.history.append", "ordinal_sarsa_lambda_discretized_agent.SarsaLambdaAgent.update_ordinal_values"], "methods", ["home.repos.pwc.inspect_result.az79nefy_OrdinalRL.dqn.ordinal_dqn_discretized_agent.DQNAgent.reward_to_ordinal", "home.repos.pwc.inspect_result.az79nefy_OrdinalRL.q_learning.alternative_ordinal_q_discretized_agent.QAgent.update_ordinal_values"], ["", "def", "update", "(", "self", ",", "prev_obs", ",", "prev_act", ",", "obs", ",", "act", ",", "reward", ",", "episode_reward", ",", "done", ")", ":", "\n", "        ", "ordinal", "=", "self", ".", "reward_to_ordinal", "(", "reward", ",", "episode_reward", ",", "done", ")", "\n", "# add executed observation-action pair to history", "\n", "self", ".", "history", ".", "append", "(", "(", "prev_obs", ",", "prev_act", ")", ")", "\n", "# update ordinal_values with received ordinal", "\n", "self", ".", "update_ordinal_values", "(", "prev_obs", ",", "prev_act", ",", "obs", ",", "act", ",", "ordinal", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.az79nefy_OrdinalRL.sarsa_lambda.ordinal_sarsa_lambda_discretized_agent.SarsaLambdaAgent.update_ordinal_values": [[57, 67], ["range", "range", "len", "len"], "methods", ["None"], ["", "def", "update_ordinal_values", "(", "self", ",", "prev_obs", ",", "prev_act", ",", "obs", ",", "act", ",", "ordinal", ")", ":", "\n", "# reduce old data weight", "\n", "        ", "for", "i", "in", "range", "(", "self", ".", "n_ordinals", ")", ":", "\n", "            ", "rew", "=", "1", "if", "i", "==", "ordinal", "else", "0", "\n", "ordinal_value_old", "=", "self", ".", "ordinal_values", "[", "prev_obs", ",", "prev_act", ",", "i", "]", "\n", "ordinal_value_target", "=", "rew", "+", "self", ".", "gamma", "*", "self", ".", "ordinal_values", "[", "obs", ",", "act", ",", "i", "]", "\n", "for", "j", "in", "range", "(", "len", "(", "self", ".", "history", ")", ")", ":", "\n", "                ", "hist_obs", ",", "hist_act", "=", "self", ".", "history", "[", "j", "]", "\n", "eligibility_trace", "=", "(", "self", ".", "lambda_", "*", "self", ".", "gamma", ")", "**", "(", "len", "(", "self", ".", "history", ")", "-", "1", "-", "j", ")", "\n", "self", ".", "ordinal_values", "[", "hist_obs", ",", "hist_act", ",", "i", "]", "+=", "self", ".", "alpha", "*", "(", "ordinal_value_target", "-", "ordinal_value_old", ")", "*", "eligibility_trace", "\n", "\n"]], "home.repos.pwc.inspect_result.az79nefy_OrdinalRL.sarsa_lambda.ordinal_sarsa_lambda_discretized_agent.SarsaLambdaAgent.compute_borda_scores": [[69, 119], ["numpy.zeros", "range", "numpy.count_nonzero", "range", "borda_scores.append", "borda_scores.append", "range", "borda_scores.append", "range"], "methods", ["None"], ["", "", "", "def", "compute_borda_scores", "(", "self", ",", "obs", ")", ":", "\n", "# sum up all ordinal values per action for given observation", "\n", "        ", "ordinal_value_sum_per_action", "=", "np", ".", "zeros", "(", "self", ".", "n_actions", ")", "\n", "for", "action_a", "in", "range", "(", "self", ".", "n_actions", ")", ":", "\n", "            ", "for", "ordinal_value", "in", "self", ".", "ordinal_values", "[", "obs", ",", "action_a", "]", ":", "\n", "                ", "ordinal_value_sum_per_action", "[", "action_a", "]", "+=", "ordinal_value", "\n", "\n", "# count actions whose ordinal value sum is not zero (no comparision possible for actions without ordinal_value)", "\n", "", "", "non_zero_action_count", "=", "np", ".", "count_nonzero", "(", "ordinal_value_sum_per_action", ")", "\n", "actions_to_compare_count", "=", "non_zero_action_count", "-", "1", "\n", "\n", "borda_scores", "=", "[", "]", "\n", "# compute borda_values for action_a (probability that action_a wins against any other action)", "\n", "for", "action_a", "in", "range", "(", "self", ".", "n_actions", ")", ":", "\n", "# if action has not yet recorded any ordinal values, action has to be played (set borda_value to 1.0)", "\n", "            ", "if", "ordinal_value_sum_per_action", "[", "action_a", "]", "==", "0", ":", "\n", "                ", "borda_scores", ".", "append", "(", "1.0", ")", "\n", "continue", "\n", "\n", "", "if", "actions_to_compare_count", "<", "1", ":", "\n", "# set lower than 1.0 (borda_value for zero_actions is 1.0)", "\n", "                ", "borda_scores", ".", "append", "(", "0.5", ")", "\n", "", "else", ":", "\n", "# over all actions: sum up the probabilities that action_a wins against the given action", "\n", "                ", "winning_probability_a_sum", "=", "0", "\n", "# compare action_a to all other actions", "\n", "for", "action_b", "in", "range", "(", "self", ".", "n_actions", ")", ":", "\n", "                    ", "if", "action_a", "==", "action_b", ":", "\n", "                        ", "continue", "\n", "# not comparable if action_b has no ordinal_values", "\n", "", "if", "ordinal_value_sum_per_action", "[", "action_b", "]", "==", "0", ":", "\n", "                        ", "continue", "\n", "", "else", ":", "\n", "# probability that action_a wins against action_b", "\n", "                        ", "winning_probability_a", "=", "0", "\n", "# running ordinal probability that action_b is worse than current investigated ordinal", "\n", "worse_probability_b", "=", "0", "\n", "for", "ordinal_count", "in", "range", "(", "self", ".", "n_ordinals", ")", ":", "\n", "                            ", "ordinal_probability_a", "=", "self", ".", "ordinal_values", "[", "obs", ",", "action_a", ",", "ordinal_count", "]", "/", "ordinal_value_sum_per_action", "[", "action_a", "]", "\n", "# ordinal_probability_b is also the tie probability", "\n", "ordinal_probability_b", "=", "(", "self", ".", "ordinal_values", "[", "obs", ",", "action_b", ",", "ordinal_count", "]", "/", "\n", "ordinal_value_sum_per_action", "[", "action_b", "]", ")", "\n", "winning_probability_a", "+=", "ordinal_probability_a", "*", "(", "worse_probability_b", "+", "ordinal_probability_b", "/", "2.0", ")", "\n", "worse_probability_b", "+=", "ordinal_probability_b", "\n", "", "winning_probability_a_sum", "+=", "winning_probability_a", "\n", "# normalize summed up probabilities with number of actions that have been compared", "\n", "", "", "borda_scores", ".", "append", "(", "winning_probability_a_sum", "/", "actions_to_compare_count", ")", "\n", "", "", "return", "borda_scores", "\n", "\n"]], "home.repos.pwc.inspect_result.az79nefy_OrdinalRL.sarsa_lambda.ordinal_sarsa_lambda_discretized_agent.SarsaLambdaAgent.get_greedy_action": [[120, 122], ["numpy.argmax", "ordinal_sarsa_lambda_discretized_agent.SarsaLambdaAgent.compute_borda_scores"], "methods", ["home.repos.pwc.inspect_result.az79nefy_OrdinalRL.dqn.ordinal_dqn_discretized_agent.DQNAgent.compute_borda_scores"], ["", "def", "get_greedy_action", "(", "self", ",", "obs", ")", ":", "\n", "        ", "return", "np", ".", "argmax", "(", "self", ".", "compute_borda_scores", "(", "obs", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.az79nefy_OrdinalRL.sarsa_lambda.ordinal_sarsa_lambda_discretized_agent.SarsaLambdaAgent.choose_action": [[124, 132], ["ordinal_sarsa_lambda_discretized_agent.SarsaLambdaAgent.get_greedy_action", "random.randrange", "random.random"], "methods", ["home.repos.pwc.inspect_result.az79nefy_OrdinalRL.dqn.atari_dqn_single_net_agent.DQNAgent.get_greedy_action"], ["", "def", "choose_action", "(", "self", ",", "obs", ",", "greedy", ")", ":", "\n", "        ", "greedy_action", "=", "self", ".", "get_greedy_action", "(", "obs", ")", "\n", "# choose random action with probability epsilon", "\n", "if", "not", "greedy", "and", "random", ".", "random", "(", ")", "<", "self", ".", "epsilon", ":", "\n", "            ", "return", "random", ".", "randrange", "(", "self", ".", "n_actions", ")", "\n", "# greedy action is chosen with probability (1 - epsilon)", "\n", "", "else", ":", "\n", "            ", "return", "greedy_action", "\n", "\n"]], "home.repos.pwc.inspect_result.az79nefy_OrdinalRL.sarsa_lambda.ordinal_sarsa_lambda_discretized_agent.SarsaLambdaAgent.end_episode": [[133, 138], ["collections.deque"], "methods", ["None"], ["", "", "def", "end_episode", "(", "self", ",", "n_episodes", ")", ":", "\n", "# gradually reduce epsilon after every done episode", "\n", "        ", "self", ".", "epsilon", "=", "self", ".", "epsilon", "-", "2", "/", "n_episodes", "if", "self", ".", "epsilon", ">", "self", ".", "epsilon_min", "else", "self", ".", "epsilon_min", "\n", "# reset history after every episode", "\n", "self", ".", "history", "=", "deque", "(", "maxlen", "=", "20", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.az79nefy_OrdinalRL.sarsa_lambda.ordinal_sarsa_lambda_discretized_agent.SarsaLambdaAgent.preprocess_observation": [[139, 144], ["range", "len", "discrete_observation.append", "int", "tuple", "numpy.digitize"], "methods", ["None"], ["", "def", "preprocess_observation", "(", "self", ",", "obs", ")", ":", "\n", "        ", "discrete_observation", "=", "[", "]", "\n", "for", "obs_idx", "in", "range", "(", "len", "(", "obs", ")", ")", ":", "\n", "            ", "discrete_observation", ".", "append", "(", "int", "(", "np", ".", "digitize", "(", "obs", "[", "obs_idx", "]", ",", "self", ".", "observation_space", "[", "obs_idx", "]", ")", ")", ")", "\n", "", "return", "self", ".", "observation_to_index", "[", "tuple", "(", "discrete_observation", ")", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.az79nefy_OrdinalRL.sarsa_lambda.ordinal_sarsa_lambda_discretized_agent.SarsaLambdaAgent.reward_to_ordinal": [[146, 151], ["ordinal_sarsa_lambda_discretized_agent.SarsaLambdaAgent.check_win_condition"], "methods", ["home.repos.pwc.inspect_result.az79nefy_OrdinalRL.dqn.atari_dqn_single_net_agent.DQNAgent.check_win_condition"], ["", "def", "reward_to_ordinal", "(", "self", ",", "reward", ",", "episode_reward", ",", "done", ")", ":", "\n", "        ", "if", "done", "and", "not", "self", ".", "check_win_condition", "(", "reward", ",", "episode_reward", ",", "done", ")", ":", "\n", "            ", "return", "0", "\n", "", "else", ":", "\n", "            ", "return", "1", "\n", "\n"]], "home.repos.pwc.inspect_result.az79nefy_OrdinalRL.sarsa_lambda.ordinal_sarsa_lambda_discretized_agent.SarsaLambdaAgent.check_win_condition": [[153, 159], ["None"], "methods", ["None"], ["", "", "@", "staticmethod", "\n", "def", "check_win_condition", "(", "reward", ",", "episode_reward", ",", "done", ")", ":", "\n", "        ", "if", "done", "and", "episode_reward", "==", "200", ":", "\n", "            ", "return", "True", "\n", "", "else", ":", "\n", "            ", "return", "False", "\n", "\n"]], "home.repos.pwc.inspect_result.az79nefy_OrdinalRL.sarsa_lambda.ordinal_sarsa_lambda_discretized_agent.SarsaLambdaAgent.evaluate": [[160, 168], ["round", "round", "ordinal_sarsa_lambda_discretized_agent.SarsaLambdaAgent.average_rewards.append", "ordinal_sarsa_lambda_discretized_agent.SarsaLambdaAgent.win_rates.append", "print", "sum", "len", "sum", "len"], "methods", ["None"], ["", "", "def", "evaluate", "(", "self", ",", "i_episode", ",", "episode_rewards", ",", "episode_wins", ")", ":", "\n", "# compute average episode reward and win rate over last episodes", "\n", "        ", "average_reward", "=", "round", "(", "sum", "(", "episode_rewards", ")", "/", "len", "(", "episode_rewards", ")", ",", "2", ")", "\n", "win_rate", "=", "round", "(", "sum", "(", "episode_wins", ")", "/", "len", "(", "episode_wins", ")", ",", "2", ")", "\n", "# store average episode reward and win rate over last episodes for plotting purposes", "\n", "self", ".", "average_rewards", ".", "append", "(", "average_reward", ")", "\n", "self", ".", "win_rates", ".", "append", "(", "win_rate", ")", "\n", "print", "(", "\"{}\\t{}\\t{}\"", ".", "format", "(", "i_episode", "+", "1", ",", "average_reward", ",", "win_rate", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.az79nefy_OrdinalRL.sarsa_lambda.ordinal_sarsa_lambda_discretized_agent.SarsaLambdaAgent.plot": [[170, 184], ["matplotlib.figure", "matplotlib.plot", "matplotlib.xlabel", "matplotlib.ylabel", "matplotlib.figure", "matplotlib.plot", "matplotlib.xlabel", "matplotlib.ylabel", "matplotlib.show", "list", "list", "range", "range"], "methods", ["home.repos.pwc.inspect_result.az79nefy_OrdinalRL.dqn.atari_dqn_single_net_agent.DQNAgent.plot", "home.repos.pwc.inspect_result.az79nefy_OrdinalRL.dqn.atari_dqn_single_net_agent.DQNAgent.plot"], ["", "def", "plot", "(", "self", ",", "n_episodes", ",", "step_size", ")", ":", "\n", "# plot win rate", "\n", "        ", "plt", ".", "figure", "(", ")", "\n", "plt", ".", "plot", "(", "list", "(", "range", "(", "step_size", ",", "n_episodes", "+", "step_size", ",", "step_size", ")", ")", ",", "self", ".", "win_rates", ")", "\n", "plt", ".", "xlabel", "(", "'Number of episodes'", ")", "\n", "plt", ".", "ylabel", "(", "'Win rate'", ")", "\n", "\n", "# plot average score", "\n", "plt", ".", "figure", "(", ")", "\n", "plt", ".", "plot", "(", "list", "(", "range", "(", "step_size", ",", "n_episodes", "+", "step_size", ",", "step_size", ")", ")", ",", "self", ".", "average_rewards", ")", "\n", "plt", ".", "xlabel", "(", "'Number of episodes'", ")", "\n", "plt", ".", "ylabel", "(", "'Average score'", ")", "\n", "\n", "plt", ".", "show", "(", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.az79nefy_OrdinalRL.sarsa_lambda.sarsa_lambda_agent.SarsaLambdaAgent.__init__": [[8, 26], ["collections.deque", "numpy.full", "numpy.random.random_sample"], "methods", ["None"], ["    ", "def", "__init__", "(", "self", ",", "alpha", ",", "gamma", ",", "epsilon", ",", "epsilon_min", ",", "lambda_", ",", "n_actions", ",", "n_ordinals", ",", "n_observations", ",", "randomize", ")", ":", "\n", "        ", "self", ".", "alpha", "=", "alpha", "\n", "self", ".", "gamma", "=", "gamma", "\n", "self", ".", "epsilon", "=", "epsilon", "\n", "self", ".", "epsilon_min", "=", "epsilon_min", "\n", "self", ".", "lambda_", "=", "lambda_", "\n", "self", ".", "n_actions", "=", "n_actions", "\n", "\n", "# Q_Values (2-dimensional array with float-value for each action (e.g. [Left, Down, Right, Up]) in each observation)", "\n", "if", "randomize", ":", "\n", "            ", "self", ".", "q_values", "=", "np", ".", "random", ".", "random_sample", "(", "(", "n_observations", ",", "n_actions", ")", ")", "*", "0.1", "\n", "", "else", ":", "\n", "            ", "self", ".", "q_values", "=", "np", ".", "full", "(", "(", "n_observations", ",", "n_actions", ")", ",", "0.0", ")", "\n", "\n", "# History of visited state-action-pairs for eligibility trace computation", "\n", "", "self", ".", "history", "=", "deque", "(", "maxlen", "=", "20", ")", "\n", "self", ".", "win_rates", "=", "[", "]", "\n", "self", ".", "average_rewards", "=", "[", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.az79nefy_OrdinalRL.sarsa_lambda.sarsa_lambda_agent.SarsaLambdaAgent.update": [[27, 32], ["sarsa_lambda_agent.SarsaLambdaAgent.remap_reward", "sarsa_lambda_agent.SarsaLambdaAgent.history.append", "sarsa_lambda_agent.SarsaLambdaAgent.update_q_values"], "methods", ["home.repos.pwc.inspect_result.az79nefy_OrdinalRL.dqn.dqn_single_net_agent.DQNAgent.remap_reward", "home.repos.pwc.inspect_result.az79nefy_OrdinalRL.q_learning.q_discretized_agent.QAgent.update_q_values"], ["", "def", "update", "(", "self", ",", "prev_obs", ",", "prev_act", ",", "obs", ",", "act", ",", "reward", ",", "episode_reward", ",", "done", ")", ":", "\n", "        ", "reward", "=", "self", ".", "remap_reward", "(", "reward", ",", "episode_reward", ",", "done", ")", "\n", "# add executed observation-action pair to history", "\n", "self", ".", "history", ".", "append", "(", "(", "prev_obs", ",", "prev_act", ")", ")", "\n", "self", ".", "update_q_values", "(", "prev_obs", ",", "prev_act", ",", "obs", ",", "act", ",", "reward", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.az79nefy_OrdinalRL.sarsa_lambda.sarsa_lambda_agent.SarsaLambdaAgent.update_q_values": [[34, 41], ["range", "len", "len"], "methods", ["None"], ["", "def", "update_q_values", "(", "self", ",", "prev_obs", ",", "prev_act", ",", "obs", ",", "act", ",", "rew", ")", ":", "\n", "        ", "q_old", "=", "self", ".", "q_values", "[", "prev_obs", ",", "prev_act", "]", "\n", "q_target", "=", "rew", "+", "self", ".", "gamma", "*", "self", ".", "q_values", "[", "obs", ",", "act", "]", "\n", "for", "i", "in", "range", "(", "len", "(", "self", ".", "history", ")", ")", ":", "\n", "            ", "hist_obs", ",", "hist_act", "=", "self", ".", "history", "[", "i", "]", "\n", "eligibility_trace", "=", "(", "self", ".", "lambda_", "*", "self", ".", "gamma", ")", "**", "(", "len", "(", "self", ".", "history", ")", "-", "1", "-", "i", ")", "\n", "self", ".", "q_values", "[", "hist_obs", ",", "hist_act", "]", "+=", "self", ".", "alpha", "*", "(", "q_target", "-", "q_old", ")", "*", "eligibility_trace", "\n", "\n"]], "home.repos.pwc.inspect_result.az79nefy_OrdinalRL.sarsa_lambda.sarsa_lambda_agent.SarsaLambdaAgent.get_greedy_action": [[42, 44], ["numpy.argmax"], "methods", ["None"], ["", "", "def", "get_greedy_action", "(", "self", ",", "obs", ")", ":", "\n", "        ", "return", "np", ".", "argmax", "(", "self", ".", "q_values", "[", "obs", "]", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.az79nefy_OrdinalRL.sarsa_lambda.sarsa_lambda_agent.SarsaLambdaAgent.choose_action": [[46, 54], ["sarsa_lambda_agent.SarsaLambdaAgent.get_greedy_action", "random.randrange", "random.random"], "methods", ["home.repos.pwc.inspect_result.az79nefy_OrdinalRL.dqn.atari_dqn_single_net_agent.DQNAgent.get_greedy_action"], ["", "def", "choose_action", "(", "self", ",", "obs", ",", "greedy", ")", ":", "\n", "        ", "greedy_action", "=", "self", ".", "get_greedy_action", "(", "obs", ")", "\n", "# choose random action with probability epsilon", "\n", "if", "not", "greedy", "and", "random", ".", "random", "(", ")", "<", "self", ".", "epsilon", ":", "\n", "            ", "return", "random", ".", "randrange", "(", "self", ".", "n_actions", ")", "\n", "# greedy action is chosen with probability (1 - epsilon)", "\n", "", "else", ":", "\n", "            ", "return", "greedy_action", "\n", "\n"]], "home.repos.pwc.inspect_result.az79nefy_OrdinalRL.sarsa_lambda.sarsa_lambda_agent.SarsaLambdaAgent.end_episode": [[55, 60], ["collections.deque"], "methods", ["None"], ["", "", "def", "end_episode", "(", "self", ",", "n_episodes", ")", ":", "\n", "# gradually reduce epsilon after every done episode", "\n", "        ", "self", ".", "epsilon", "=", "self", ".", "epsilon", "-", "2", "/", "n_episodes", "if", "self", ".", "epsilon", ">", "self", ".", "epsilon_min", "else", "self", ".", "epsilon_min", "\n", "# reset history after every episode", "\n", "self", ".", "history", "=", "deque", "(", "maxlen", "=", "20", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.az79nefy_OrdinalRL.sarsa_lambda.sarsa_lambda_agent.SarsaLambdaAgent.preprocess_observation": [[61, 63], ["None"], "methods", ["None"], ["", "def", "preprocess_observation", "(", "self", ",", "obs", ")", ":", "\n", "        ", "return", "obs", "\n", "\n"]], "home.repos.pwc.inspect_result.az79nefy_OrdinalRL.sarsa_lambda.sarsa_lambda_agent.SarsaLambdaAgent.remap_reward": [[65, 67], ["None"], "methods", ["None"], ["", "def", "remap_reward", "(", "self", ",", "reward", ",", "episode_reward", ",", "done", ")", ":", "\n", "        ", "return", "reward", "\n", "\n"]], "home.repos.pwc.inspect_result.az79nefy_OrdinalRL.sarsa_lambda.sarsa_lambda_agent.SarsaLambdaAgent.check_win_condition": [[69, 75], ["None"], "methods", ["None"], ["", "@", "staticmethod", "\n", "def", "check_win_condition", "(", "reward", ",", "episode_reward", ",", "done", ")", ":", "\n", "        ", "if", "done", "and", "reward", "==", "20", ":", "\n", "            ", "return", "True", "\n", "", "else", ":", "\n", "            ", "return", "False", "\n", "\n"]], "home.repos.pwc.inspect_result.az79nefy_OrdinalRL.sarsa_lambda.sarsa_lambda_agent.SarsaLambdaAgent.evaluate": [[76, 84], ["round", "round", "sarsa_lambda_agent.SarsaLambdaAgent.average_rewards.append", "sarsa_lambda_agent.SarsaLambdaAgent.win_rates.append", "print", "sum", "len", "sum", "len"], "methods", ["None"], ["", "", "def", "evaluate", "(", "self", ",", "i_episode", ",", "episode_rewards", ",", "episode_wins", ")", ":", "\n", "# compute average episode reward and win rate over last episodes", "\n", "        ", "average_reward", "=", "round", "(", "sum", "(", "episode_rewards", ")", "/", "len", "(", "episode_rewards", ")", ",", "2", ")", "\n", "win_rate", "=", "round", "(", "sum", "(", "episode_wins", ")", "/", "len", "(", "episode_wins", ")", ",", "2", ")", "\n", "# store average episode reward and win rate over last episodes for plotting purposes", "\n", "self", ".", "average_rewards", ".", "append", "(", "average_reward", ")", "\n", "self", ".", "win_rates", ".", "append", "(", "win_rate", ")", "\n", "print", "(", "\"{}\\t{}\\t{}\"", ".", "format", "(", "i_episode", "+", "1", ",", "average_reward", ",", "win_rate", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.az79nefy_OrdinalRL.sarsa_lambda.sarsa_lambda_agent.SarsaLambdaAgent.plot": [[86, 100], ["matplotlib.figure", "matplotlib.plot", "matplotlib.xlabel", "matplotlib.ylabel", "matplotlib.figure", "matplotlib.plot", "matplotlib.xlabel", "matplotlib.ylabel", "matplotlib.show", "list", "list", "range", "range"], "methods", ["home.repos.pwc.inspect_result.az79nefy_OrdinalRL.dqn.atari_dqn_single_net_agent.DQNAgent.plot", "home.repos.pwc.inspect_result.az79nefy_OrdinalRL.dqn.atari_dqn_single_net_agent.DQNAgent.plot"], ["", "def", "plot", "(", "self", ",", "n_episodes", ",", "step_size", ")", ":", "\n", "# plot win rate", "\n", "        ", "plt", ".", "figure", "(", ")", "\n", "plt", ".", "plot", "(", "list", "(", "range", "(", "step_size", ",", "n_episodes", "+", "step_size", ",", "step_size", ")", ")", ",", "self", ".", "win_rates", ")", "\n", "plt", ".", "xlabel", "(", "'Number of episodes'", ")", "\n", "plt", ".", "ylabel", "(", "'Win rate'", ")", "\n", "\n", "# plot average score", "\n", "plt", ".", "figure", "(", ")", "\n", "plt", ".", "plot", "(", "list", "(", "range", "(", "step_size", ",", "n_episodes", "+", "step_size", ",", "step_size", ")", ")", ",", "self", ".", "average_rewards", ")", "\n", "plt", ".", "xlabel", "(", "'Number of episodes'", ")", "\n", "plt", ".", "ylabel", "(", "'Average score'", ")", "\n", "\n", "plt", ".", "show", "(", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.az79nefy_OrdinalRL.sarsa_lambda.ordinal_sarsa_lambda_agent.SarsaLambdaAgent.__init__": [[8, 24], ["numpy.full", "collections.deque"], "methods", ["None"], ["    ", "def", "__init__", "(", "self", ",", "alpha", ",", "gamma", ",", "epsilon", ",", "epsilon_min", ",", "lambda_", ",", "n_actions", ",", "n_ordinals", ",", "n_observations", ",", "randomize", ")", ":", "\n", "        ", "self", ".", "alpha", "=", "alpha", "\n", "self", ".", "gamma", "=", "gamma", "\n", "self", ".", "epsilon", "=", "epsilon", "\n", "self", ".", "epsilon_min", "=", "epsilon_min", "\n", "self", ".", "lambda_", "=", "lambda_", "\n", "self", ".", "n_actions", "=", "n_actions", "\n", "self", ".", "n_ordinals", "=", "n_ordinals", "\n", "\n", "# Ordinal_Values (3-dimensional array with ordinal_value (array of floats) for each action in each observation)", "\n", "self", ".", "ordinal_values", "=", "np", ".", "full", "(", "(", "n_observations", ",", "n_actions", ",", "n_ordinals", ")", ",", "0.0", ")", "\n", "\n", "# History of visited state-action-pairs for eligibility trace computation", "\n", "self", ".", "history", "=", "deque", "(", "maxlen", "=", "20", ")", "\n", "self", ".", "win_rates", "=", "[", "]", "\n", "self", ".", "average_rewards", "=", "[", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.az79nefy_OrdinalRL.sarsa_lambda.ordinal_sarsa_lambda_agent.SarsaLambdaAgent.update": [[25, 31], ["ordinal_sarsa_lambda_agent.SarsaLambdaAgent.reward_to_ordinal", "ordinal_sarsa_lambda_agent.SarsaLambdaAgent.history.append", "ordinal_sarsa_lambda_agent.SarsaLambdaAgent.update_ordinal_values"], "methods", ["home.repos.pwc.inspect_result.az79nefy_OrdinalRL.dqn.ordinal_dqn_discretized_agent.DQNAgent.reward_to_ordinal", "home.repos.pwc.inspect_result.az79nefy_OrdinalRL.q_learning.alternative_ordinal_q_discretized_agent.QAgent.update_ordinal_values"], ["", "def", "update", "(", "self", ",", "prev_obs", ",", "prev_act", ",", "obs", ",", "act", ",", "reward", ",", "episode_reward", ",", "done", ")", ":", "\n", "        ", "ordinal", "=", "self", ".", "reward_to_ordinal", "(", "reward", ",", "episode_reward", ",", "done", ")", "\n", "# add executed observation-action pair to history", "\n", "self", ".", "history", ".", "append", "(", "(", "prev_obs", ",", "prev_act", ")", ")", "\n", "# update ordinal_values with received ordinal", "\n", "self", ".", "update_ordinal_values", "(", "prev_obs", ",", "prev_act", ",", "obs", ",", "act", ",", "ordinal", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.az79nefy_OrdinalRL.sarsa_lambda.ordinal_sarsa_lambda_agent.SarsaLambdaAgent.update_ordinal_values": [[33, 43], ["range", "range", "len", "len"], "methods", ["None"], ["", "def", "update_ordinal_values", "(", "self", ",", "prev_obs", ",", "prev_act", ",", "obs", ",", "act", ",", "ordinal", ")", ":", "\n", "# reduce old data weight", "\n", "        ", "for", "i", "in", "range", "(", "self", ".", "n_ordinals", ")", ":", "\n", "            ", "rew", "=", "1", "if", "i", "==", "ordinal", "else", "0", "\n", "ordinal_value_old", "=", "self", ".", "ordinal_values", "[", "prev_obs", ",", "prev_act", ",", "i", "]", "\n", "ordinal_value_target", "=", "rew", "+", "self", ".", "gamma", "*", "self", ".", "ordinal_values", "[", "obs", ",", "act", ",", "i", "]", "\n", "for", "j", "in", "range", "(", "len", "(", "self", ".", "history", ")", ")", ":", "\n", "                ", "hist_obs", ",", "hist_act", "=", "self", ".", "history", "[", "j", "]", "\n", "eligibility_trace", "=", "(", "self", ".", "lambda_", "*", "self", ".", "gamma", ")", "**", "(", "len", "(", "self", ".", "history", ")", "-", "1", "-", "j", ")", "\n", "self", ".", "ordinal_values", "[", "hist_obs", ",", "hist_act", ",", "i", "]", "+=", "self", ".", "alpha", "*", "(", "ordinal_value_target", "-", "ordinal_value_old", ")", "*", "eligibility_trace", "\n", "\n"]], "home.repos.pwc.inspect_result.az79nefy_OrdinalRL.sarsa_lambda.ordinal_sarsa_lambda_agent.SarsaLambdaAgent.compute_borda_scores": [[45, 95], ["numpy.zeros", "range", "numpy.count_nonzero", "range", "borda_scores.append", "borda_scores.append", "range", "borda_scores.append", "range"], "methods", ["None"], ["", "", "", "def", "compute_borda_scores", "(", "self", ",", "obs", ")", ":", "\n", "# sum up all ordinal values per action for given observation", "\n", "        ", "ordinal_value_sum_per_action", "=", "np", ".", "zeros", "(", "self", ".", "n_actions", ")", "\n", "for", "action_a", "in", "range", "(", "self", ".", "n_actions", ")", ":", "\n", "            ", "for", "ordinal_value", "in", "self", ".", "ordinal_values", "[", "obs", ",", "action_a", "]", ":", "\n", "                ", "ordinal_value_sum_per_action", "[", "action_a", "]", "+=", "ordinal_value", "\n", "\n", "# count actions whose ordinal value sum is not zero (no comparision possible for actions without ordinal_value)", "\n", "", "", "non_zero_action_count", "=", "np", ".", "count_nonzero", "(", "ordinal_value_sum_per_action", ")", "\n", "actions_to_compare_count", "=", "non_zero_action_count", "-", "1", "\n", "\n", "borda_scores", "=", "[", "]", "\n", "# compute borda_values for action_a (probability that action_a wins against any other action)", "\n", "for", "action_a", "in", "range", "(", "self", ".", "n_actions", ")", ":", "\n", "# if action has not yet recorded any ordinal values, action has to be played (set borda_value to 1.0)", "\n", "            ", "if", "ordinal_value_sum_per_action", "[", "action_a", "]", "==", "0", ":", "\n", "                ", "borda_scores", ".", "append", "(", "1.0", ")", "\n", "continue", "\n", "\n", "", "if", "actions_to_compare_count", "<", "1", ":", "\n", "# set lower than 1.0 (borda_value for zero_actions is 1.0)", "\n", "                ", "borda_scores", ".", "append", "(", "0.5", ")", "\n", "", "else", ":", "\n", "# over all actions: sum up the probabilities that action_a wins against the given action", "\n", "                ", "winning_probability_a_sum", "=", "0", "\n", "# compare action_a to all other actions", "\n", "for", "action_b", "in", "range", "(", "self", ".", "n_actions", ")", ":", "\n", "                    ", "if", "action_a", "==", "action_b", ":", "\n", "                        ", "continue", "\n", "# not comparable if action_b has no ordinal_values", "\n", "", "if", "ordinal_value_sum_per_action", "[", "action_b", "]", "==", "0", ":", "\n", "                        ", "continue", "\n", "", "else", ":", "\n", "# probability that action_a wins against action_b", "\n", "                        ", "winning_probability_a", "=", "0", "\n", "# running ordinal probability that action_b is worse than current investigated ordinal", "\n", "worse_probability_b", "=", "0", "\n", "for", "ordinal_count", "in", "range", "(", "self", ".", "n_ordinals", ")", ":", "\n", "                            ", "ordinal_probability_a", "=", "self", ".", "ordinal_values", "[", "obs", ",", "action_a", ",", "ordinal_count", "]", "/", "ordinal_value_sum_per_action", "[", "action_a", "]", "\n", "# ordinal_probability_b is also the tie probability", "\n", "ordinal_probability_b", "=", "(", "self", ".", "ordinal_values", "[", "obs", ",", "action_b", ",", "ordinal_count", "]", "/", "\n", "ordinal_value_sum_per_action", "[", "action_b", "]", ")", "\n", "winning_probability_a", "+=", "ordinal_probability_a", "*", "(", "worse_probability_b", "+", "ordinal_probability_b", "/", "2.0", ")", "\n", "worse_probability_b", "+=", "ordinal_probability_b", "\n", "", "winning_probability_a_sum", "+=", "winning_probability_a", "\n", "# normalize summed up probabilities with number of actions that have been compared", "\n", "", "", "borda_scores", ".", "append", "(", "winning_probability_a_sum", "/", "actions_to_compare_count", ")", "\n", "", "", "return", "borda_scores", "\n", "\n"]], "home.repos.pwc.inspect_result.az79nefy_OrdinalRL.sarsa_lambda.ordinal_sarsa_lambda_agent.SarsaLambdaAgent.get_greedy_action": [[96, 98], ["numpy.argmax", "ordinal_sarsa_lambda_agent.SarsaLambdaAgent.compute_borda_scores"], "methods", ["home.repos.pwc.inspect_result.az79nefy_OrdinalRL.dqn.ordinal_dqn_discretized_agent.DQNAgent.compute_borda_scores"], ["", "def", "get_greedy_action", "(", "self", ",", "obs", ")", ":", "\n", "        ", "return", "np", ".", "argmax", "(", "self", ".", "compute_borda_scores", "(", "obs", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.az79nefy_OrdinalRL.sarsa_lambda.ordinal_sarsa_lambda_agent.SarsaLambdaAgent.choose_action": [[100, 108], ["ordinal_sarsa_lambda_agent.SarsaLambdaAgent.get_greedy_action", "random.randrange", "random.random"], "methods", ["home.repos.pwc.inspect_result.az79nefy_OrdinalRL.dqn.atari_dqn_single_net_agent.DQNAgent.get_greedy_action"], ["", "def", "choose_action", "(", "self", ",", "obs", ",", "greedy", ")", ":", "\n", "        ", "greedy_action", "=", "self", ".", "get_greedy_action", "(", "obs", ")", "\n", "# choose random action with probability epsilon", "\n", "if", "not", "greedy", "and", "random", ".", "random", "(", ")", "<", "self", ".", "epsilon", ":", "\n", "            ", "return", "random", ".", "randrange", "(", "self", ".", "n_actions", ")", "\n", "# greedy action is chosen with probability (1 - epsilon)", "\n", "", "else", ":", "\n", "            ", "return", "greedy_action", "\n", "\n"]], "home.repos.pwc.inspect_result.az79nefy_OrdinalRL.sarsa_lambda.ordinal_sarsa_lambda_agent.SarsaLambdaAgent.end_episode": [[109, 114], ["collections.deque"], "methods", ["None"], ["", "", "def", "end_episode", "(", "self", ",", "n_episodes", ")", ":", "\n", "# gradually reduce epsilon after every done episode", "\n", "        ", "self", ".", "epsilon", "=", "self", ".", "epsilon", "-", "2", "/", "n_episodes", "if", "self", ".", "epsilon", ">", "self", ".", "epsilon_min", "else", "self", ".", "epsilon_min", "\n", "# reset history after every episode", "\n", "self", ".", "history", "=", "deque", "(", "maxlen", "=", "20", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.az79nefy_OrdinalRL.sarsa_lambda.ordinal_sarsa_lambda_agent.SarsaLambdaAgent.preprocess_observation": [[115, 117], ["None"], "methods", ["None"], ["", "def", "preprocess_observation", "(", "self", ",", "obs", ")", ":", "\n", "        ", "return", "obs", "\n", "\n"]], "home.repos.pwc.inspect_result.az79nefy_OrdinalRL.sarsa_lambda.ordinal_sarsa_lambda_agent.SarsaLambdaAgent.reward_to_ordinal": [[119, 126], ["None"], "methods", ["None"], ["", "def", "reward_to_ordinal", "(", "self", ",", "reward", ",", "episode_reward", ",", "done", ")", ":", "\n", "        ", "if", "reward", "==", "-", "10", ":", "\n", "            ", "return", "0", "\n", "", "if", "reward", "==", "-", "1", ":", "\n", "            ", "return", "1", "\n", "", "else", ":", "\n", "            ", "return", "2", "\n", "\n"]], "home.repos.pwc.inspect_result.az79nefy_OrdinalRL.sarsa_lambda.ordinal_sarsa_lambda_agent.SarsaLambdaAgent.check_win_condition": [[128, 134], ["None"], "methods", ["None"], ["", "", "@", "staticmethod", "\n", "def", "check_win_condition", "(", "reward", ",", "episode_reward", ",", "done", ")", ":", "\n", "        ", "if", "done", "and", "reward", "==", "20", ":", "\n", "            ", "return", "True", "\n", "", "else", ":", "\n", "            ", "return", "False", "\n", "\n"]], "home.repos.pwc.inspect_result.az79nefy_OrdinalRL.sarsa_lambda.ordinal_sarsa_lambda_agent.SarsaLambdaAgent.evaluate": [[135, 143], ["round", "round", "ordinal_sarsa_lambda_agent.SarsaLambdaAgent.average_rewards.append", "ordinal_sarsa_lambda_agent.SarsaLambdaAgent.win_rates.append", "print", "sum", "len", "sum", "len"], "methods", ["None"], ["", "", "def", "evaluate", "(", "self", ",", "i_episode", ",", "episode_rewards", ",", "episode_wins", ")", ":", "\n", "# compute average episode reward and win rate over last episodes", "\n", "        ", "average_reward", "=", "round", "(", "sum", "(", "episode_rewards", ")", "/", "len", "(", "episode_rewards", ")", ",", "2", ")", "\n", "win_rate", "=", "round", "(", "sum", "(", "episode_wins", ")", "/", "len", "(", "episode_wins", ")", ",", "2", ")", "\n", "# store average episode reward and win rate over last episodes for plotting purposes", "\n", "self", ".", "average_rewards", ".", "append", "(", "average_reward", ")", "\n", "self", ".", "win_rates", ".", "append", "(", "win_rate", ")", "\n", "print", "(", "\"{}\\t{}\\t{}\"", ".", "format", "(", "i_episode", "+", "1", ",", "average_reward", ",", "win_rate", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.az79nefy_OrdinalRL.sarsa_lambda.ordinal_sarsa_lambda_agent.SarsaLambdaAgent.plot": [[145, 159], ["matplotlib.figure", "matplotlib.plot", "matplotlib.xlabel", "matplotlib.ylabel", "matplotlib.figure", "matplotlib.plot", "matplotlib.xlabel", "matplotlib.ylabel", "matplotlib.show", "list", "list", "range", "range"], "methods", ["home.repos.pwc.inspect_result.az79nefy_OrdinalRL.dqn.atari_dqn_single_net_agent.DQNAgent.plot", "home.repos.pwc.inspect_result.az79nefy_OrdinalRL.dqn.atari_dqn_single_net_agent.DQNAgent.plot"], ["", "def", "plot", "(", "self", ",", "n_episodes", ",", "step_size", ")", ":", "\n", "# plot win rate", "\n", "        ", "plt", ".", "figure", "(", ")", "\n", "plt", ".", "plot", "(", "list", "(", "range", "(", "step_size", ",", "n_episodes", "+", "step_size", ",", "step_size", ")", ")", ",", "self", ".", "win_rates", ")", "\n", "plt", ".", "xlabel", "(", "'Number of episodes'", ")", "\n", "plt", ".", "ylabel", "(", "'Win rate'", ")", "\n", "\n", "# plot average score", "\n", "plt", ".", "figure", "(", ")", "\n", "plt", ".", "plot", "(", "list", "(", "range", "(", "step_size", ",", "n_episodes", "+", "step_size", ",", "step_size", ")", ")", ",", "self", ".", "average_rewards", ")", "\n", "plt", ".", "xlabel", "(", "'Number of episodes'", ")", "\n", "plt", ".", "ylabel", "(", "'Average score'", ")", "\n", "\n", "plt", ".", "show", "(", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.az79nefy_OrdinalRL.sarsa_lambda.sarsa_lambda_discretized_agent.SarsaLambdaAgent.__init__": [[9, 29], ["sarsa_lambda_discretized_agent.SarsaLambdaAgent.init_observation_space", "sarsa_lambda_discretized_agent.SarsaLambdaAgent.build_obs_dict", "collections.deque", "numpy.full", "numpy.random.random_sample"], "methods", ["home.repos.pwc.inspect_result.az79nefy_OrdinalRL.q_learning.alternative_ordinal_q_discretized_agent.QAgent.init_observation_space", "home.repos.pwc.inspect_result.az79nefy_OrdinalRL.q_learning.alternative_ordinal_q_discretized_agent.QAgent.build_obs_dict"], ["    ", "def", "__init__", "(", "self", ",", "alpha", ",", "gamma", ",", "epsilon", ",", "epsilon_min", ",", "lambda_", ",", "n_actions", ",", "n_ordinals", ",", "n_observations", ",", "randomize", ")", ":", "\n", "        ", "self", ".", "alpha", "=", "alpha", "\n", "self", ".", "gamma", "=", "gamma", "\n", "self", ".", "epsilon", "=", "epsilon", "\n", "self", ".", "epsilon_min", "=", "epsilon_min", "\n", "self", ".", "lambda_", "=", "lambda_", "\n", "self", ".", "n_actions", "=", "n_actions", "\n", "self", ".", "observation_space", "=", "self", ".", "init_observation_space", "(", ")", "\n", "self", ".", "observation_to_index", "=", "self", ".", "build_obs_dict", "(", "self", ".", "observation_space", ")", "\n", "\n", "# Q_Values (2-dimensional array with float-value for each action (e.g. [Left, Down, Right, Up]) in each observation)", "\n", "if", "randomize", ":", "\n", "            ", "self", ".", "q_values", "=", "np", ".", "random", ".", "random_sample", "(", "(", "n_observations", ",", "n_actions", ")", ")", "*", "0.1", "\n", "", "else", ":", "\n", "            ", "self", ".", "q_values", "=", "np", ".", "full", "(", "(", "n_observations", ",", "n_actions", ")", ",", "0.0", ")", "\n", "\n", "# History of visited state-action-pairs for eligibility trace computation", "\n", "", "self", ".", "history", "=", "deque", "(", "maxlen", "=", "20", ")", "\n", "self", ".", "win_rates", "=", "[", "]", "\n", "self", ".", "average_rewards", "=", "[", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.az79nefy_OrdinalRL.sarsa_lambda.sarsa_lambda_discretized_agent.SarsaLambdaAgent.init_observation_space": [[31, 38], ["numpy.linspace", "numpy.linspace", "numpy.linspace", "numpy.linspace"], "methods", ["None"], ["", "@", "staticmethod", "\n", "def", "init_observation_space", "(", ")", ":", "\n", "        ", "cart_pos_space", "=", "np", ".", "linspace", "(", "-", "2.4", ",", "2.4", ",", "10", ")", "\n", "cart_vel_space", "=", "np", ".", "linspace", "(", "-", "4", ",", "4", ",", "10", ")", "\n", "pole_theta_space", "=", "np", ".", "linspace", "(", "-", "0.20943951", ",", "0.20943951", ",", "10", ")", "\n", "pole_theta_vel_space", "=", "np", ".", "linspace", "(", "-", "4", ",", "4", ",", "10", ")", "\n", "return", "[", "cart_pos_space", ",", "cart_vel_space", ",", "pole_theta_space", ",", "pole_theta_vel_space", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.az79nefy_OrdinalRL.sarsa_lambda.sarsa_lambda_discretized_agent.SarsaLambdaAgent.build_obs_dict": [[39, 50], ["list", "range", "itertools.product", "len"], "methods", ["None"], ["", "@", "staticmethod", "\n", "def", "build_obs_dict", "(", "observation_space", ")", ":", "\n", "# List of all possible discrete observations", "\n", "        ", "observation_range", "=", "[", "range", "(", "len", "(", "i", ")", "+", "1", ")", "for", "i", "in", "observation_space", "]", "\n", "# Dictionary that maps discretized observations to array indices", "\n", "observation_to_index", "=", "{", "}", "\n", "index_counter", "=", "0", "\n", "for", "observation", "in", "list", "(", "itertools", ".", "product", "(", "*", "observation_range", ")", ")", ":", "\n", "            ", "observation_to_index", "[", "observation", "]", "=", "index_counter", "\n", "index_counter", "+=", "1", "\n", "", "return", "observation_to_index", "\n", "\n"]], "home.repos.pwc.inspect_result.az79nefy_OrdinalRL.sarsa_lambda.sarsa_lambda_discretized_agent.SarsaLambdaAgent.update": [[51, 56], ["sarsa_lambda_discretized_agent.SarsaLambdaAgent.remap_reward", "sarsa_lambda_discretized_agent.SarsaLambdaAgent.history.append", "sarsa_lambda_discretized_agent.SarsaLambdaAgent.update_q_values"], "methods", ["home.repos.pwc.inspect_result.az79nefy_OrdinalRL.dqn.dqn_single_net_agent.DQNAgent.remap_reward", "home.repos.pwc.inspect_result.az79nefy_OrdinalRL.q_learning.q_discretized_agent.QAgent.update_q_values"], ["", "def", "update", "(", "self", ",", "prev_obs", ",", "prev_act", ",", "obs", ",", "act", ",", "reward", ",", "episode_reward", ",", "done", ")", ":", "\n", "        ", "reward", "=", "self", ".", "remap_reward", "(", "reward", ",", "episode_reward", ",", "done", ")", "\n", "# add executed observation-action pair to history", "\n", "self", ".", "history", ".", "append", "(", "(", "prev_obs", ",", "prev_act", ")", ")", "\n", "self", ".", "update_q_values", "(", "prev_obs", ",", "prev_act", ",", "obs", ",", "act", ",", "reward", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.az79nefy_OrdinalRL.sarsa_lambda.sarsa_lambda_discretized_agent.SarsaLambdaAgent.update_q_values": [[58, 65], ["range", "len", "len"], "methods", ["None"], ["", "def", "update_q_values", "(", "self", ",", "prev_obs", ",", "prev_act", ",", "obs", ",", "act", ",", "rew", ")", ":", "\n", "        ", "q_old", "=", "self", ".", "q_values", "[", "prev_obs", ",", "prev_act", "]", "\n", "q_target", "=", "rew", "+", "self", ".", "gamma", "*", "self", ".", "q_values", "[", "obs", ",", "act", "]", "\n", "for", "i", "in", "range", "(", "len", "(", "self", ".", "history", ")", ")", ":", "\n", "            ", "hist_obs", ",", "hist_act", "=", "self", ".", "history", "[", "i", "]", "\n", "eligibility_trace", "=", "(", "self", ".", "lambda_", "*", "self", ".", "gamma", ")", "**", "(", "len", "(", "self", ".", "history", ")", "-", "1", "-", "i", ")", "\n", "self", ".", "q_values", "[", "hist_obs", ",", "hist_act", "]", "+=", "self", ".", "alpha", "*", "(", "q_target", "-", "q_old", ")", "*", "eligibility_trace", "\n", "\n"]], "home.repos.pwc.inspect_result.az79nefy_OrdinalRL.sarsa_lambda.sarsa_lambda_discretized_agent.SarsaLambdaAgent.get_greedy_action": [[66, 68], ["numpy.argmax"], "methods", ["None"], ["", "", "def", "get_greedy_action", "(", "self", ",", "obs", ")", ":", "\n", "        ", "return", "np", ".", "argmax", "(", "self", ".", "q_values", "[", "obs", "]", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.az79nefy_OrdinalRL.sarsa_lambda.sarsa_lambda_discretized_agent.SarsaLambdaAgent.choose_action": [[70, 78], ["sarsa_lambda_discretized_agent.SarsaLambdaAgent.get_greedy_action", "random.randrange", "random.random"], "methods", ["home.repos.pwc.inspect_result.az79nefy_OrdinalRL.dqn.atari_dqn_single_net_agent.DQNAgent.get_greedy_action"], ["", "def", "choose_action", "(", "self", ",", "obs", ",", "greedy", ")", ":", "\n", "        ", "greedy_action", "=", "self", ".", "get_greedy_action", "(", "obs", ")", "\n", "# choose random action with probability epsilon", "\n", "if", "not", "greedy", "and", "random", ".", "random", "(", ")", "<", "self", ".", "epsilon", ":", "\n", "            ", "return", "random", ".", "randrange", "(", "self", ".", "n_actions", ")", "\n", "# greedy action is chosen with probability (1 - epsilon)", "\n", "", "else", ":", "\n", "            ", "return", "greedy_action", "\n", "\n"]], "home.repos.pwc.inspect_result.az79nefy_OrdinalRL.sarsa_lambda.sarsa_lambda_discretized_agent.SarsaLambdaAgent.end_episode": [[79, 84], ["collections.deque"], "methods", ["None"], ["", "", "def", "end_episode", "(", "self", ",", "n_episodes", ")", ":", "\n", "# gradually reduce epsilon after every done episode", "\n", "        ", "self", ".", "epsilon", "=", "self", ".", "epsilon", "-", "2", "/", "n_episodes", "if", "self", ".", "epsilon", ">", "self", ".", "epsilon_min", "else", "self", ".", "epsilon_min", "\n", "# reset history after every episode", "\n", "self", ".", "history", "=", "deque", "(", "maxlen", "=", "20", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.az79nefy_OrdinalRL.sarsa_lambda.sarsa_lambda_discretized_agent.SarsaLambdaAgent.preprocess_observation": [[85, 90], ["range", "len", "discrete_observation.append", "int", "tuple", "numpy.digitize"], "methods", ["None"], ["", "def", "preprocess_observation", "(", "self", ",", "obs", ")", ":", "\n", "        ", "discrete_observation", "=", "[", "]", "\n", "for", "obs_idx", "in", "range", "(", "len", "(", "obs", ")", ")", ":", "\n", "            ", "discrete_observation", ".", "append", "(", "int", "(", "np", ".", "digitize", "(", "obs", "[", "obs_idx", "]", ",", "self", ".", "observation_space", "[", "obs_idx", "]", ")", ")", ")", "\n", "", "return", "self", ".", "observation_to_index", "[", "tuple", "(", "discrete_observation", ")", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.az79nefy_OrdinalRL.sarsa_lambda.sarsa_lambda_discretized_agent.SarsaLambdaAgent.remap_reward": [[92, 97], ["sarsa_lambda_discretized_agent.SarsaLambdaAgent.check_win_condition"], "methods", ["home.repos.pwc.inspect_result.az79nefy_OrdinalRL.dqn.atari_dqn_single_net_agent.DQNAgent.check_win_condition"], ["", "def", "remap_reward", "(", "self", ",", "reward", ",", "episode_reward", ",", "done", ")", ":", "\n", "        ", "if", "done", "and", "not", "self", ".", "check_win_condition", "(", "reward", ",", "episode_reward", ",", "done", ")", ":", "\n", "            ", "return", "-", "1", "\n", "", "else", ":", "\n", "            ", "return", "0", "\n", "\n"]], "home.repos.pwc.inspect_result.az79nefy_OrdinalRL.sarsa_lambda.sarsa_lambda_discretized_agent.SarsaLambdaAgent.check_win_condition": [[99, 105], ["None"], "methods", ["None"], ["", "", "@", "staticmethod", "\n", "def", "check_win_condition", "(", "reward", ",", "episode_reward", ",", "done", ")", ":", "\n", "        ", "if", "done", "and", "episode_reward", "==", "200", ":", "\n", "            ", "return", "True", "\n", "", "else", ":", "\n", "            ", "return", "False", "\n", "\n"]], "home.repos.pwc.inspect_result.az79nefy_OrdinalRL.sarsa_lambda.sarsa_lambda_discretized_agent.SarsaLambdaAgent.evaluate": [[106, 114], ["round", "round", "sarsa_lambda_discretized_agent.SarsaLambdaAgent.average_rewards.append", "sarsa_lambda_discretized_agent.SarsaLambdaAgent.win_rates.append", "print", "sum", "len", "sum", "len"], "methods", ["None"], ["", "", "def", "evaluate", "(", "self", ",", "i_episode", ",", "episode_rewards", ",", "episode_wins", ")", ":", "\n", "# compute average episode reward and win rate over last episodes", "\n", "        ", "average_reward", "=", "round", "(", "sum", "(", "episode_rewards", ")", "/", "len", "(", "episode_rewards", ")", ",", "2", ")", "\n", "win_rate", "=", "round", "(", "sum", "(", "episode_wins", ")", "/", "len", "(", "episode_wins", ")", ",", "2", ")", "\n", "# store average episode reward and win rate over last episodes for plotting purposes", "\n", "self", ".", "average_rewards", ".", "append", "(", "average_reward", ")", "\n", "self", ".", "win_rates", ".", "append", "(", "win_rate", ")", "\n", "print", "(", "\"{}\\t{}\\t{}\"", ".", "format", "(", "i_episode", "+", "1", ",", "average_reward", ",", "win_rate", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.az79nefy_OrdinalRL.sarsa_lambda.sarsa_lambda_discretized_agent.SarsaLambdaAgent.plot": [[116, 130], ["matplotlib.figure", "matplotlib.plot", "matplotlib.xlabel", "matplotlib.ylabel", "matplotlib.figure", "matplotlib.plot", "matplotlib.xlabel", "matplotlib.ylabel", "matplotlib.show", "list", "list", "range", "range"], "methods", ["home.repos.pwc.inspect_result.az79nefy_OrdinalRL.dqn.atari_dqn_single_net_agent.DQNAgent.plot", "home.repos.pwc.inspect_result.az79nefy_OrdinalRL.dqn.atari_dqn_single_net_agent.DQNAgent.plot"], ["", "def", "plot", "(", "self", ",", "n_episodes", ",", "step_size", ")", ":", "\n", "# plot win rate", "\n", "        ", "plt", ".", "figure", "(", ")", "\n", "plt", ".", "plot", "(", "list", "(", "range", "(", "step_size", ",", "n_episodes", "+", "step_size", ",", "step_size", ")", ")", ",", "self", ".", "win_rates", ")", "\n", "plt", ".", "xlabel", "(", "'Number of episodes'", ")", "\n", "plt", ".", "ylabel", "(", "'Win rate'", ")", "\n", "\n", "# plot average score", "\n", "plt", ".", "figure", "(", ")", "\n", "plt", ".", "plot", "(", "list", "(", "range", "(", "step_size", ",", "n_episodes", "+", "step_size", ",", "step_size", ")", ")", ",", "self", ".", "average_rewards", ")", "\n", "plt", ".", "xlabel", "(", "'Number of episodes'", ")", "\n", "plt", ".", "ylabel", "(", "'Average score'", ")", "\n", "\n", "plt", ".", "show", "(", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.az79nefy_OrdinalRL.q_learning.q_agent.QAgent.__init__": [[7, 22], ["numpy.full", "numpy.random.random_sample"], "methods", ["None"], ["    ", "def", "__init__", "(", "self", ",", "alpha", ",", "gamma", ",", "epsilon", ",", "epsilon_min", ",", "n_actions", ",", "n_ordinals", ",", "n_observations", ",", "randomize", ")", ":", "\n", "        ", "self", ".", "alpha", "=", "alpha", "\n", "self", ".", "gamma", "=", "gamma", "\n", "self", ".", "epsilon", "=", "epsilon", "\n", "self", ".", "epsilon_min", "=", "epsilon_min", "\n", "self", ".", "n_actions", "=", "n_actions", "\n", "\n", "# Q_Values (2-dimensional array with float-value for each action (e.g. [Left, Down, Right, Up]) in each observation)", "\n", "if", "randomize", ":", "\n", "            ", "self", ".", "q_values", "=", "np", ".", "random", ".", "random_sample", "(", "(", "n_observations", ",", "n_actions", ")", ")", "*", "0.1", "\n", "", "else", ":", "\n", "            ", "self", ".", "q_values", "=", "np", ".", "full", "(", "(", "n_observations", ",", "n_actions", ")", ",", "0.0", ")", "\n", "\n", "", "self", ".", "win_rates", "=", "[", "]", "\n", "self", ".", "average_rewards", "=", "[", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.az79nefy_OrdinalRL.q_learning.q_agent.QAgent.update": [[23, 26], ["q_agent.QAgent.remap_reward", "q_agent.QAgent.update_q_values"], "methods", ["home.repos.pwc.inspect_result.az79nefy_OrdinalRL.dqn.dqn_single_net_agent.DQNAgent.remap_reward", "home.repos.pwc.inspect_result.az79nefy_OrdinalRL.q_learning.q_discretized_agent.QAgent.update_q_values"], ["", "def", "update", "(", "self", ",", "prev_obs", ",", "prev_act", ",", "obs", ",", "reward", ",", "episode_reward", ",", "done", ")", ":", "\n", "        ", "reward", "=", "self", ".", "remap_reward", "(", "reward", ",", "episode_reward", ",", "done", ")", "\n", "self", ".", "update_q_values", "(", "prev_obs", ",", "prev_act", ",", "obs", ",", "reward", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.az79nefy_OrdinalRL.q_learning.q_agent.QAgent.update_q_values": [[28, 32], ["numpy.max"], "methods", ["None"], ["", "def", "update_q_values", "(", "self", ",", "prev_obs", ",", "prev_act", ",", "obs", ",", "rew", ")", ":", "\n", "        ", "q_old", "=", "self", ".", "q_values", "[", "prev_obs", ",", "prev_act", "]", "\n", "q_new", "=", "(", "1", "-", "self", ".", "alpha", ")", "*", "q_old", "+", "self", ".", "alpha", "*", "(", "rew", "+", "self", ".", "gamma", "*", "np", ".", "max", "(", "self", ".", "q_values", "[", "obs", "]", ")", ")", "\n", "self", ".", "q_values", "[", "prev_obs", ",", "prev_act", "]", "=", "q_new", "\n", "\n"]], "home.repos.pwc.inspect_result.az79nefy_OrdinalRL.q_learning.q_agent.QAgent.get_greedy_action": [[33, 35], ["numpy.argmax"], "methods", ["None"], ["", "def", "get_greedy_action", "(", "self", ",", "obs", ")", ":", "\n", "        ", "return", "np", ".", "argmax", "(", "self", ".", "q_values", "[", "obs", "]", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.az79nefy_OrdinalRL.q_learning.q_agent.QAgent.choose_action": [[37, 45], ["q_agent.QAgent.get_greedy_action", "random.randrange", "random.random"], "methods", ["home.repos.pwc.inspect_result.az79nefy_OrdinalRL.dqn.atari_dqn_single_net_agent.DQNAgent.get_greedy_action"], ["", "def", "choose_action", "(", "self", ",", "obs", ",", "greedy", ")", ":", "\n", "        ", "greedy_action", "=", "self", ".", "get_greedy_action", "(", "obs", ")", "\n", "# choose random action with probability epsilon", "\n", "if", "not", "greedy", "and", "random", ".", "random", "(", ")", "<", "self", ".", "epsilon", ":", "\n", "            ", "return", "random", ".", "randrange", "(", "self", ".", "n_actions", ")", "\n", "# greedy action is chosen with probability (1 - epsilon)", "\n", "", "else", ":", "\n", "            ", "return", "greedy_action", "\n", "\n"]], "home.repos.pwc.inspect_result.az79nefy_OrdinalRL.q_learning.q_agent.QAgent.end_episode": [[46, 49], ["None"], "methods", ["None"], ["", "", "def", "end_episode", "(", "self", ",", "n_episodes", ")", ":", "\n", "# gradually reduce epsilon after every done episode", "\n", "        ", "self", ".", "epsilon", "=", "self", ".", "epsilon", "-", "2", "/", "n_episodes", "if", "self", ".", "epsilon", ">", "self", ".", "epsilon_min", "else", "self", ".", "epsilon_min", "\n", "\n"]], "home.repos.pwc.inspect_result.az79nefy_OrdinalRL.q_learning.q_agent.QAgent.preprocess_observation": [[50, 52], ["None"], "methods", ["None"], ["", "def", "preprocess_observation", "(", "self", ",", "obs", ")", ":", "\n", "        ", "return", "obs", "\n", "\n"]], "home.repos.pwc.inspect_result.az79nefy_OrdinalRL.q_learning.q_agent.QAgent.remap_reward": [[54, 56], ["None"], "methods", ["None"], ["", "def", "remap_reward", "(", "self", ",", "reward", ",", "episode_reward", ",", "done", ")", ":", "\n", "        ", "return", "reward", "\n", "\n"]], "home.repos.pwc.inspect_result.az79nefy_OrdinalRL.q_learning.q_agent.QAgent.check_win_condition": [[58, 64], ["None"], "methods", ["None"], ["", "@", "staticmethod", "\n", "def", "check_win_condition", "(", "reward", ",", "episode_reward", ",", "done", ")", ":", "\n", "        ", "if", "done", "and", "reward", "==", "20", ":", "\n", "            ", "return", "True", "\n", "", "else", ":", "\n", "            ", "return", "False", "\n", "\n"]], "home.repos.pwc.inspect_result.az79nefy_OrdinalRL.q_learning.q_agent.QAgent.evaluate": [[65, 73], ["round", "round", "q_agent.QAgent.average_rewards.append", "q_agent.QAgent.win_rates.append", "print", "sum", "len", "sum", "len"], "methods", ["None"], ["", "", "def", "evaluate", "(", "self", ",", "i_episode", ",", "episode_rewards", ",", "episode_wins", ")", ":", "\n", "# compute average episode reward and win rate over last episodes", "\n", "        ", "average_reward", "=", "round", "(", "sum", "(", "episode_rewards", ")", "/", "len", "(", "episode_rewards", ")", ",", "2", ")", "\n", "win_rate", "=", "round", "(", "sum", "(", "episode_wins", ")", "/", "len", "(", "episode_wins", ")", ",", "2", ")", "\n", "# store average episode reward and win rate over last episodes for plotting purposes", "\n", "self", ".", "average_rewards", ".", "append", "(", "average_reward", ")", "\n", "self", ".", "win_rates", ".", "append", "(", "win_rate", ")", "\n", "print", "(", "\"{}\\t{}\\t{}\"", ".", "format", "(", "i_episode", "+", "1", ",", "average_reward", ",", "win_rate", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.az79nefy_OrdinalRL.q_learning.q_agent.QAgent.plot": [[75, 89], ["matplotlib.figure", "matplotlib.plot", "matplotlib.xlabel", "matplotlib.ylabel", "matplotlib.figure", "matplotlib.plot", "matplotlib.xlabel", "matplotlib.ylabel", "matplotlib.show", "list", "list", "range", "range"], "methods", ["home.repos.pwc.inspect_result.az79nefy_OrdinalRL.dqn.atari_dqn_single_net_agent.DQNAgent.plot", "home.repos.pwc.inspect_result.az79nefy_OrdinalRL.dqn.atari_dqn_single_net_agent.DQNAgent.plot"], ["", "def", "plot", "(", "self", ",", "n_episodes", ",", "step_size", ")", ":", "\n", "# plot win rate", "\n", "        ", "plt", ".", "figure", "(", ")", "\n", "plt", ".", "plot", "(", "list", "(", "range", "(", "step_size", ",", "n_episodes", "+", "step_size", ",", "step_size", ")", ")", ",", "self", ".", "win_rates", ")", "\n", "plt", ".", "xlabel", "(", "'Number of episodes'", ")", "\n", "plt", ".", "ylabel", "(", "'Win rate'", ")", "\n", "\n", "# plot average score", "\n", "plt", ".", "figure", "(", ")", "\n", "plt", ".", "plot", "(", "list", "(", "range", "(", "step_size", ",", "n_episodes", "+", "step_size", ",", "step_size", ")", ")", ",", "self", ".", "average_rewards", ")", "\n", "plt", ".", "xlabel", "(", "'Number of episodes'", ")", "\n", "plt", ".", "ylabel", "(", "'Average score'", ")", "\n", "\n", "plt", ".", "show", "(", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.az79nefy_OrdinalRL.q_learning.ordinal_q_agent.QAgent.__init__": [[7, 20], ["numpy.full"], "methods", ["None"], ["    ", "def", "__init__", "(", "self", ",", "alpha", ",", "gamma", ",", "epsilon", ",", "epsilon_min", ",", "n_actions", ",", "n_ordinals", ",", "n_observations", ",", "randomize", ")", ":", "\n", "        ", "self", ".", "alpha", "=", "alpha", "\n", "self", ".", "gamma", "=", "gamma", "\n", "self", ".", "epsilon", "=", "epsilon", "\n", "self", ".", "epsilon_min", "=", "epsilon_min", "\n", "self", ".", "n_actions", "=", "n_actions", "\n", "self", ".", "n_ordinals", "=", "n_ordinals", "\n", "\n", "# Ordinal_Values (3-dimensional array with ordinal_value (array of floats) for each action in each observation)", "\n", "self", ".", "ordinal_values", "=", "np", ".", "full", "(", "(", "n_observations", ",", "n_actions", ",", "n_ordinals", ")", ",", "0.0", ")", "\n", "\n", "self", ".", "win_rates", "=", "[", "]", "\n", "self", ".", "average_rewards", "=", "[", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.az79nefy_OrdinalRL.q_learning.ordinal_q_agent.QAgent.update": [[21, 25], ["ordinal_q_agent.QAgent.reward_to_ordinal", "ordinal_q_agent.QAgent.update_ordinal_values"], "methods", ["home.repos.pwc.inspect_result.az79nefy_OrdinalRL.dqn.ordinal_dqn_discretized_agent.DQNAgent.reward_to_ordinal", "home.repos.pwc.inspect_result.az79nefy_OrdinalRL.q_learning.alternative_ordinal_q_discretized_agent.QAgent.update_ordinal_values"], ["", "def", "update", "(", "self", ",", "prev_obs", ",", "prev_act", ",", "obs", ",", "reward", ",", "episode_reward", ",", "done", ")", ":", "\n", "        ", "ordinal", "=", "self", ".", "reward_to_ordinal", "(", "reward", ",", "episode_reward", ",", "done", ")", "\n", "# update ordinal_values with received ordinal", "\n", "self", ".", "update_ordinal_values", "(", "prev_obs", ",", "prev_act", ",", "obs", ",", "ordinal", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.az79nefy_OrdinalRL.q_learning.ordinal_q_agent.QAgent.update_ordinal_values": [[27, 36], ["ordinal_q_agent.QAgent.get_greedy_action", "range"], "methods", ["home.repos.pwc.inspect_result.az79nefy_OrdinalRL.dqn.atari_dqn_single_net_agent.DQNAgent.get_greedy_action"], ["", "def", "update_ordinal_values", "(", "self", ",", "prev_obs", ",", "prev_act", ",", "obs", ",", "ordinal", ")", ":", "\n", "        ", "greedy_action", "=", "self", ".", "get_greedy_action", "(", "obs", ")", "\n", "# reduce old data weight", "\n", "for", "i", "in", "range", "(", "self", ".", "n_ordinals", ")", ":", "\n", "            ", "self", ".", "ordinal_values", "[", "prev_obs", ",", "prev_act", ",", "i", "]", "*=", "(", "1", "-", "self", ".", "alpha", ")", "\n", "self", ".", "ordinal_values", "[", "prev_obs", ",", "prev_act", ",", "i", "]", "+=", "self", ".", "alpha", "*", "(", "self", ".", "gamma", "*", "self", ".", "ordinal_values", "[", "obs", ",", "greedy_action", ",", "i", "]", ")", "\n", "\n", "# add new data point", "\n", "", "self", ".", "ordinal_values", "[", "prev_obs", ",", "prev_act", ",", "ordinal", "]", "+=", "self", ".", "alpha", "\n", "\n"]], "home.repos.pwc.inspect_result.az79nefy_OrdinalRL.q_learning.ordinal_q_agent.QAgent.compute_borda_scores": [[38, 88], ["numpy.zeros", "range", "numpy.count_nonzero", "range", "borda_scores.append", "borda_scores.append", "range", "borda_scores.append", "range"], "methods", ["None"], ["", "def", "compute_borda_scores", "(", "self", ",", "obs", ")", ":", "\n", "# sum up all ordinal values per action for given observation", "\n", "        ", "ordinal_value_sum_per_action", "=", "np", ".", "zeros", "(", "self", ".", "n_actions", ")", "\n", "for", "action_a", "in", "range", "(", "self", ".", "n_actions", ")", ":", "\n", "            ", "for", "ordinal_value", "in", "self", ".", "ordinal_values", "[", "obs", ",", "action_a", "]", ":", "\n", "                ", "ordinal_value_sum_per_action", "[", "action_a", "]", "+=", "ordinal_value", "\n", "\n", "# count actions whose ordinal value sum is not zero (no comparision possible for actions without ordinal_value)", "\n", "", "", "non_zero_action_count", "=", "np", ".", "count_nonzero", "(", "ordinal_value_sum_per_action", ")", "\n", "actions_to_compare_count", "=", "non_zero_action_count", "-", "1", "\n", "\n", "borda_scores", "=", "[", "]", "\n", "# compute borda_values for action_a (probability that action_a wins against any other action)", "\n", "for", "action_a", "in", "range", "(", "self", ".", "n_actions", ")", ":", "\n", "# if action has not yet recorded any ordinal values, action has to be played (set borda_value to 1.0)", "\n", "            ", "if", "ordinal_value_sum_per_action", "[", "action_a", "]", "==", "0", ":", "\n", "                ", "borda_scores", ".", "append", "(", "1.0", ")", "\n", "continue", "\n", "\n", "", "if", "actions_to_compare_count", "<", "1", ":", "\n", "# set lower than 1.0 (borda_value for zero_actions is 1.0)", "\n", "                ", "borda_scores", ".", "append", "(", "0.5", ")", "\n", "", "else", ":", "\n", "# over all actions: sum up the probabilities that action_a wins against the given action", "\n", "                ", "winning_probability_a_sum", "=", "0", "\n", "# compare action_a to all other actions", "\n", "for", "action_b", "in", "range", "(", "self", ".", "n_actions", ")", ":", "\n", "                    ", "if", "action_a", "==", "action_b", ":", "\n", "                        ", "continue", "\n", "# not comparable if action_b has no ordinal_values", "\n", "", "if", "ordinal_value_sum_per_action", "[", "action_b", "]", "==", "0", ":", "\n", "                        ", "continue", "\n", "", "else", ":", "\n", "# probability that action_a wins against action_b", "\n", "                        ", "winning_probability_a", "=", "0", "\n", "# running ordinal probability that action_b is worse than current investigated ordinal", "\n", "worse_probability_b", "=", "0", "\n", "for", "ordinal_count", "in", "range", "(", "self", ".", "n_ordinals", ")", ":", "\n", "                            ", "ordinal_probability_a", "=", "self", ".", "ordinal_values", "[", "obs", ",", "action_a", ",", "ordinal_count", "]", "/", "ordinal_value_sum_per_action", "[", "action_a", "]", "\n", "# ordinal_probability_b is also the tie probability", "\n", "ordinal_probability_b", "=", "(", "self", ".", "ordinal_values", "[", "obs", ",", "action_b", ",", "ordinal_count", "]", "/", "\n", "ordinal_value_sum_per_action", "[", "action_b", "]", ")", "\n", "winning_probability_a", "+=", "ordinal_probability_a", "*", "(", "worse_probability_b", "+", "ordinal_probability_b", "/", "2.0", ")", "\n", "worse_probability_b", "+=", "ordinal_probability_b", "\n", "", "winning_probability_a_sum", "+=", "winning_probability_a", "\n", "# normalize summed up probabilities with number of actions that have been compared", "\n", "", "", "borda_scores", ".", "append", "(", "winning_probability_a_sum", "/", "actions_to_compare_count", ")", "\n", "", "", "return", "borda_scores", "\n", "\n"]], "home.repos.pwc.inspect_result.az79nefy_OrdinalRL.q_learning.ordinal_q_agent.QAgent.get_greedy_action": [[89, 91], ["numpy.argmax", "ordinal_q_agent.QAgent.compute_borda_scores"], "methods", ["home.repos.pwc.inspect_result.az79nefy_OrdinalRL.dqn.ordinal_dqn_discretized_agent.DQNAgent.compute_borda_scores"], ["", "def", "get_greedy_action", "(", "self", ",", "obs", ")", ":", "\n", "        ", "return", "np", ".", "argmax", "(", "self", ".", "compute_borda_scores", "(", "obs", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.az79nefy_OrdinalRL.q_learning.ordinal_q_agent.QAgent.choose_action": [[93, 101], ["ordinal_q_agent.QAgent.get_greedy_action", "random.randrange", "random.random"], "methods", ["home.repos.pwc.inspect_result.az79nefy_OrdinalRL.dqn.atari_dqn_single_net_agent.DQNAgent.get_greedy_action"], ["", "def", "choose_action", "(", "self", ",", "obs", ",", "greedy", ")", ":", "\n", "        ", "greedy_action", "=", "self", ".", "get_greedy_action", "(", "obs", ")", "\n", "# choose random action with probability epsilon", "\n", "if", "not", "greedy", "and", "random", ".", "random", "(", ")", "<", "self", ".", "epsilon", ":", "\n", "            ", "return", "random", ".", "randrange", "(", "self", ".", "n_actions", ")", "\n", "# greedy action is chosen with probability (1 - epsilon)", "\n", "", "else", ":", "\n", "            ", "return", "greedy_action", "\n", "\n"]], "home.repos.pwc.inspect_result.az79nefy_OrdinalRL.q_learning.ordinal_q_agent.QAgent.end_episode": [[102, 105], ["None"], "methods", ["None"], ["", "", "def", "end_episode", "(", "self", ",", "n_episodes", ")", ":", "\n", "# gradually reduce epsilon after every done episode", "\n", "        ", "self", ".", "epsilon", "=", "self", ".", "epsilon", "-", "2", "/", "n_episodes", "if", "self", ".", "epsilon", ">", "self", ".", "epsilon_min", "else", "self", ".", "epsilon_min", "\n", "\n"]], "home.repos.pwc.inspect_result.az79nefy_OrdinalRL.q_learning.ordinal_q_agent.QAgent.preprocess_observation": [[106, 108], ["None"], "methods", ["None"], ["", "def", "preprocess_observation", "(", "self", ",", "obs", ")", ":", "\n", "        ", "return", "obs", "\n", "\n"]], "home.repos.pwc.inspect_result.az79nefy_OrdinalRL.q_learning.ordinal_q_agent.QAgent.reward_to_ordinal": [[110, 117], ["None"], "methods", ["None"], ["", "def", "reward_to_ordinal", "(", "self", ",", "reward", ",", "episode_reward", ",", "done", ")", ":", "\n", "        ", "if", "reward", "==", "-", "10", ":", "\n", "            ", "return", "0", "\n", "", "if", "reward", "==", "-", "1", ":", "\n", "            ", "return", "1", "\n", "", "else", ":", "\n", "            ", "return", "2", "\n", "\n"]], "home.repos.pwc.inspect_result.az79nefy_OrdinalRL.q_learning.ordinal_q_agent.QAgent.check_win_condition": [[119, 125], ["None"], "methods", ["None"], ["", "", "@", "staticmethod", "\n", "def", "check_win_condition", "(", "reward", ",", "episode_reward", ",", "done", ")", ":", "\n", "        ", "if", "done", "and", "reward", "==", "20", ":", "\n", "            ", "return", "True", "\n", "", "else", ":", "\n", "            ", "return", "False", "\n", "\n"]], "home.repos.pwc.inspect_result.az79nefy_OrdinalRL.q_learning.ordinal_q_agent.QAgent.evaluate": [[126, 134], ["round", "round", "ordinal_q_agent.QAgent.average_rewards.append", "ordinal_q_agent.QAgent.win_rates.append", "print", "sum", "len", "sum", "len"], "methods", ["None"], ["", "", "def", "evaluate", "(", "self", ",", "i_episode", ",", "episode_rewards", ",", "episode_wins", ")", ":", "\n", "# compute average episode reward and win rate over last episodes", "\n", "        ", "average_reward", "=", "round", "(", "sum", "(", "episode_rewards", ")", "/", "len", "(", "episode_rewards", ")", ",", "2", ")", "\n", "win_rate", "=", "round", "(", "sum", "(", "episode_wins", ")", "/", "len", "(", "episode_wins", ")", ",", "2", ")", "\n", "# store average episode reward and win rate over last episodes for plotting purposes", "\n", "self", ".", "average_rewards", ".", "append", "(", "average_reward", ")", "\n", "self", ".", "win_rates", ".", "append", "(", "win_rate", ")", "\n", "print", "(", "\"{}\\t{}\\t{}\"", ".", "format", "(", "i_episode", "+", "1", ",", "average_reward", ",", "win_rate", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.az79nefy_OrdinalRL.q_learning.ordinal_q_agent.QAgent.plot": [[136, 150], ["matplotlib.figure", "matplotlib.plot", "matplotlib.xlabel", "matplotlib.ylabel", "matplotlib.figure", "matplotlib.plot", "matplotlib.xlabel", "matplotlib.ylabel", "matplotlib.show", "list", "list", "range", "range"], "methods", ["home.repos.pwc.inspect_result.az79nefy_OrdinalRL.dqn.atari_dqn_single_net_agent.DQNAgent.plot", "home.repos.pwc.inspect_result.az79nefy_OrdinalRL.dqn.atari_dqn_single_net_agent.DQNAgent.plot"], ["", "def", "plot", "(", "self", ",", "n_episodes", ",", "step_size", ")", ":", "\n", "# plot win rate", "\n", "        ", "plt", ".", "figure", "(", ")", "\n", "plt", ".", "plot", "(", "list", "(", "range", "(", "step_size", ",", "n_episodes", "+", "step_size", ",", "step_size", ")", ")", ",", "self", ".", "win_rates", ")", "\n", "plt", ".", "xlabel", "(", "'Number of episodes'", ")", "\n", "plt", ".", "ylabel", "(", "'Win rate'", ")", "\n", "\n", "# plot average score", "\n", "plt", ".", "figure", "(", ")", "\n", "plt", ".", "plot", "(", "list", "(", "range", "(", "step_size", ",", "n_episodes", "+", "step_size", ",", "step_size", ")", ")", ",", "self", ".", "average_rewards", ")", "\n", "plt", ".", "xlabel", "(", "'Number of episodes'", ")", "\n", "plt", ".", "ylabel", "(", "'Average score'", ")", "\n", "\n", "plt", ".", "show", "(", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.az79nefy_OrdinalRL.q_learning.q_discretized_agent.QAgent.__init__": [[8, 25], ["q_discretized_agent.QAgent.init_observation_space", "q_discretized_agent.QAgent.build_obs_dict", "numpy.full", "numpy.random.random_sample"], "methods", ["home.repos.pwc.inspect_result.az79nefy_OrdinalRL.q_learning.alternative_ordinal_q_discretized_agent.QAgent.init_observation_space", "home.repos.pwc.inspect_result.az79nefy_OrdinalRL.q_learning.alternative_ordinal_q_discretized_agent.QAgent.build_obs_dict"], ["    ", "def", "__init__", "(", "self", ",", "alpha", ",", "gamma", ",", "epsilon", ",", "epsilon_min", ",", "n_actions", ",", "n_ordinals", ",", "n_observations", ",", "randomize", ")", ":", "\n", "        ", "self", ".", "alpha", "=", "alpha", "\n", "self", ".", "gamma", "=", "gamma", "\n", "self", ".", "epsilon", "=", "epsilon", "\n", "self", ".", "epsilon_min", "=", "epsilon_min", "\n", "self", ".", "n_actions", "=", "n_actions", "\n", "self", ".", "observation_space", "=", "self", ".", "init_observation_space", "(", ")", "\n", "self", ".", "observation_to_index", "=", "self", ".", "build_obs_dict", "(", "self", ".", "observation_space", ")", "\n", "\n", "# Q_Values (2-dimensional array with float-value for each action (e.g. [Left, Down, Right, Up]) in each observation)", "\n", "if", "randomize", ":", "\n", "            ", "self", ".", "q_values", "=", "np", ".", "random", ".", "random_sample", "(", "(", "n_observations", ",", "n_actions", ")", ")", "*", "0.1", "\n", "", "else", ":", "\n", "            ", "self", ".", "q_values", "=", "np", ".", "full", "(", "(", "n_observations", ",", "n_actions", ")", ",", "0.0", ")", "\n", "\n", "", "self", ".", "win_rates", "=", "[", "]", "\n", "self", ".", "average_rewards", "=", "[", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.az79nefy_OrdinalRL.q_learning.q_discretized_agent.QAgent.init_observation_space": [[27, 34], ["numpy.linspace", "numpy.linspace", "numpy.linspace", "numpy.linspace"], "methods", ["None"], ["", "@", "staticmethod", "\n", "def", "init_observation_space", "(", ")", ":", "\n", "        ", "cart_pos_space", "=", "np", ".", "linspace", "(", "-", "2.4", ",", "2.4", ",", "10", ")", "\n", "cart_vel_space", "=", "np", ".", "linspace", "(", "-", "4", ",", "4", ",", "10", ")", "\n", "pole_theta_space", "=", "np", ".", "linspace", "(", "-", "0.20943951", ",", "0.20943951", ",", "10", ")", "\n", "pole_theta_vel_space", "=", "np", ".", "linspace", "(", "-", "4", ",", "4", ",", "10", ")", "\n", "return", "[", "cart_pos_space", ",", "cart_vel_space", ",", "pole_theta_space", ",", "pole_theta_vel_space", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.az79nefy_OrdinalRL.q_learning.q_discretized_agent.QAgent.build_obs_dict": [[35, 46], ["list", "range", "itertools.product", "len"], "methods", ["None"], ["", "@", "staticmethod", "\n", "def", "build_obs_dict", "(", "observation_space", ")", ":", "\n", "# List of all possible discrete observations", "\n", "        ", "observation_range", "=", "[", "range", "(", "len", "(", "i", ")", "+", "1", ")", "for", "i", "in", "observation_space", "]", "\n", "# Dictionary that maps discretized observations to array indices", "\n", "observation_to_index", "=", "{", "}", "\n", "index_counter", "=", "0", "\n", "for", "observation", "in", "list", "(", "itertools", ".", "product", "(", "*", "observation_range", ")", ")", ":", "\n", "            ", "observation_to_index", "[", "observation", "]", "=", "index_counter", "\n", "index_counter", "+=", "1", "\n", "", "return", "observation_to_index", "\n", "\n"]], "home.repos.pwc.inspect_result.az79nefy_OrdinalRL.q_learning.q_discretized_agent.QAgent.update": [[47, 50], ["q_discretized_agent.QAgent.remap_reward", "q_discretized_agent.QAgent.update_q_values"], "methods", ["home.repos.pwc.inspect_result.az79nefy_OrdinalRL.dqn.dqn_single_net_agent.DQNAgent.remap_reward", "home.repos.pwc.inspect_result.az79nefy_OrdinalRL.q_learning.q_discretized_agent.QAgent.update_q_values"], ["", "def", "update", "(", "self", ",", "prev_obs", ",", "prev_act", ",", "obs", ",", "reward", ",", "episode_reward", ",", "done", ")", ":", "\n", "        ", "reward", "=", "self", ".", "remap_reward", "(", "reward", ",", "episode_reward", ",", "done", ")", "\n", "self", ".", "update_q_values", "(", "prev_obs", ",", "prev_act", ",", "obs", ",", "reward", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.az79nefy_OrdinalRL.q_learning.q_discretized_agent.QAgent.update_q_values": [[52, 56], ["numpy.max"], "methods", ["None"], ["", "def", "update_q_values", "(", "self", ",", "prev_obs", ",", "prev_act", ",", "obs", ",", "rew", ")", ":", "\n", "        ", "q_old", "=", "self", ".", "q_values", "[", "prev_obs", ",", "prev_act", "]", "\n", "q_new", "=", "(", "1", "-", "self", ".", "alpha", ")", "*", "q_old", "+", "self", ".", "alpha", "*", "(", "rew", "+", "self", ".", "gamma", "*", "np", ".", "max", "(", "self", ".", "q_values", "[", "obs", "]", ")", ")", "\n", "self", ".", "q_values", "[", "prev_obs", ",", "prev_act", "]", "=", "q_new", "\n", "\n"]], "home.repos.pwc.inspect_result.az79nefy_OrdinalRL.q_learning.q_discretized_agent.QAgent.get_greedy_action": [[57, 59], ["numpy.argmax"], "methods", ["None"], ["", "def", "get_greedy_action", "(", "self", ",", "obs", ")", ":", "\n", "        ", "return", "np", ".", "argmax", "(", "self", ".", "q_values", "[", "obs", "]", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.az79nefy_OrdinalRL.q_learning.q_discretized_agent.QAgent.choose_action": [[61, 69], ["q_discretized_agent.QAgent.get_greedy_action", "random.randrange", "random.random"], "methods", ["home.repos.pwc.inspect_result.az79nefy_OrdinalRL.dqn.atari_dqn_single_net_agent.DQNAgent.get_greedy_action"], ["", "def", "choose_action", "(", "self", ",", "obs", ",", "greedy", ")", ":", "\n", "        ", "greedy_action", "=", "self", ".", "get_greedy_action", "(", "obs", ")", "\n", "# choose random action with probability epsilon", "\n", "if", "not", "greedy", "and", "random", ".", "random", "(", ")", "<", "self", ".", "epsilon", ":", "\n", "            ", "return", "random", ".", "randrange", "(", "self", ".", "n_actions", ")", "\n", "# greedy action is chosen with probability (1 - epsilon)", "\n", "", "else", ":", "\n", "            ", "return", "greedy_action", "\n", "\n"]], "home.repos.pwc.inspect_result.az79nefy_OrdinalRL.q_learning.q_discretized_agent.QAgent.end_episode": [[70, 73], ["None"], "methods", ["None"], ["", "", "def", "end_episode", "(", "self", ",", "n_episodes", ")", ":", "\n", "# gradually reduce epsilon after every done episode", "\n", "        ", "self", ".", "epsilon", "=", "self", ".", "epsilon", "-", "2", "/", "n_episodes", "if", "self", ".", "epsilon", ">", "self", ".", "epsilon_min", "else", "self", ".", "epsilon_min", "\n", "\n"]], "home.repos.pwc.inspect_result.az79nefy_OrdinalRL.q_learning.q_discretized_agent.QAgent.preprocess_observation": [[74, 79], ["range", "len", "discrete_observation.append", "int", "tuple", "numpy.digitize"], "methods", ["None"], ["", "def", "preprocess_observation", "(", "self", ",", "obs", ")", ":", "\n", "        ", "discrete_observation", "=", "[", "]", "\n", "for", "obs_idx", "in", "range", "(", "len", "(", "obs", ")", ")", ":", "\n", "            ", "discrete_observation", ".", "append", "(", "int", "(", "np", ".", "digitize", "(", "obs", "[", "obs_idx", "]", ",", "self", ".", "observation_space", "[", "obs_idx", "]", ")", ")", ")", "\n", "", "return", "self", ".", "observation_to_index", "[", "tuple", "(", "discrete_observation", ")", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.az79nefy_OrdinalRL.q_learning.q_discretized_agent.QAgent.remap_reward": [[81, 86], ["q_discretized_agent.QAgent.check_win_condition"], "methods", ["home.repos.pwc.inspect_result.az79nefy_OrdinalRL.dqn.atari_dqn_single_net_agent.DQNAgent.check_win_condition"], ["", "def", "remap_reward", "(", "self", ",", "reward", ",", "episode_reward", ",", "done", ")", ":", "\n", "        ", "if", "done", "and", "not", "self", ".", "check_win_condition", "(", "reward", ",", "episode_reward", ",", "done", ")", ":", "\n", "            ", "return", "-", "1", "\n", "", "else", ":", "\n", "            ", "return", "0", "\n", "\n"]], "home.repos.pwc.inspect_result.az79nefy_OrdinalRL.q_learning.q_discretized_agent.QAgent.check_win_condition": [[88, 94], ["None"], "methods", ["None"], ["", "", "@", "staticmethod", "\n", "def", "check_win_condition", "(", "reward", ",", "episode_reward", ",", "done", ")", ":", "\n", "        ", "if", "done", "and", "episode_reward", "==", "200", ":", "\n", "            ", "return", "True", "\n", "", "else", ":", "\n", "            ", "return", "False", "\n", "\n"]], "home.repos.pwc.inspect_result.az79nefy_OrdinalRL.q_learning.q_discretized_agent.QAgent.evaluate": [[95, 103], ["round", "round", "q_discretized_agent.QAgent.average_rewards.append", "q_discretized_agent.QAgent.win_rates.append", "print", "sum", "len", "sum", "len"], "methods", ["None"], ["", "", "def", "evaluate", "(", "self", ",", "i_episode", ",", "episode_rewards", ",", "episode_wins", ")", ":", "\n", "# compute average episode reward and win rate over last episodes", "\n", "        ", "average_reward", "=", "round", "(", "sum", "(", "episode_rewards", ")", "/", "len", "(", "episode_rewards", ")", ",", "2", ")", "\n", "win_rate", "=", "round", "(", "sum", "(", "episode_wins", ")", "/", "len", "(", "episode_wins", ")", ",", "2", ")", "\n", "# store average episode reward and win rate over last episodes for plotting purposes", "\n", "self", ".", "average_rewards", ".", "append", "(", "average_reward", ")", "\n", "self", ".", "win_rates", ".", "append", "(", "win_rate", ")", "\n", "print", "(", "\"{}\\t{}\\t{}\"", ".", "format", "(", "i_episode", "+", "1", ",", "average_reward", ",", "win_rate", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.az79nefy_OrdinalRL.q_learning.q_discretized_agent.QAgent.plot": [[105, 119], ["matplotlib.figure", "matplotlib.plot", "matplotlib.xlabel", "matplotlib.ylabel", "matplotlib.figure", "matplotlib.plot", "matplotlib.xlabel", "matplotlib.ylabel", "matplotlib.show", "list", "list", "range", "range"], "methods", ["home.repos.pwc.inspect_result.az79nefy_OrdinalRL.dqn.atari_dqn_single_net_agent.DQNAgent.plot", "home.repos.pwc.inspect_result.az79nefy_OrdinalRL.dqn.atari_dqn_single_net_agent.DQNAgent.plot"], ["", "def", "plot", "(", "self", ",", "n_episodes", ",", "step_size", ")", ":", "\n", "# plot win rate", "\n", "        ", "plt", ".", "figure", "(", ")", "\n", "plt", ".", "plot", "(", "list", "(", "range", "(", "step_size", ",", "n_episodes", "+", "step_size", ",", "step_size", ")", ")", ",", "self", ".", "win_rates", ")", "\n", "plt", ".", "xlabel", "(", "'Number of episodes'", ")", "\n", "plt", ".", "ylabel", "(", "'Win rate'", ")", "\n", "\n", "# plot average score", "\n", "plt", ".", "figure", "(", ")", "\n", "plt", ".", "plot", "(", "list", "(", "range", "(", "step_size", ",", "n_episodes", "+", "step_size", ",", "step_size", ")", ")", ",", "self", ".", "average_rewards", ")", "\n", "plt", ".", "xlabel", "(", "'Number of episodes'", ")", "\n", "plt", ".", "ylabel", "(", "'Average score'", ")", "\n", "\n", "plt", ".", "show", "(", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.az79nefy_OrdinalRL.q_learning.ordinal_q_discretized_agent.QAgent.__init__": [[8, 23], ["ordinal_q_discretized_agent.QAgent.init_observation_space", "ordinal_q_discretized_agent.QAgent.build_obs_dict", "numpy.full"], "methods", ["home.repos.pwc.inspect_result.az79nefy_OrdinalRL.q_learning.alternative_ordinal_q_discretized_agent.QAgent.init_observation_space", "home.repos.pwc.inspect_result.az79nefy_OrdinalRL.q_learning.alternative_ordinal_q_discretized_agent.QAgent.build_obs_dict"], ["    ", "def", "__init__", "(", "self", ",", "alpha", ",", "gamma", ",", "epsilon", ",", "epsilon_min", ",", "n_actions", ",", "n_ordinals", ",", "n_observations", ",", "randomize", ")", ":", "\n", "        ", "self", ".", "alpha", "=", "alpha", "\n", "self", ".", "gamma", "=", "gamma", "\n", "self", ".", "epsilon", "=", "epsilon", "\n", "self", ".", "epsilon_min", "=", "epsilon_min", "\n", "self", ".", "n_actions", "=", "n_actions", "\n", "self", ".", "n_ordinals", "=", "n_ordinals", "\n", "self", ".", "observation_space", "=", "self", ".", "init_observation_space", "(", ")", "\n", "self", ".", "observation_to_index", "=", "self", ".", "build_obs_dict", "(", "self", ".", "observation_space", ")", "\n", "\n", "# Ordinal_Values (3-dimensional array with ordinal_value (array of floats) for each action in each observation)", "\n", "self", ".", "ordinal_values", "=", "np", ".", "full", "(", "(", "n_observations", ",", "n_actions", ",", "n_ordinals", ")", ",", "0.0", ")", "\n", "\n", "self", ".", "win_rates", "=", "[", "]", "\n", "self", ".", "average_rewards", "=", "[", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.az79nefy_OrdinalRL.q_learning.ordinal_q_discretized_agent.QAgent.init_observation_space": [[25, 32], ["numpy.linspace", "numpy.linspace", "numpy.linspace", "numpy.linspace"], "methods", ["None"], ["", "@", "staticmethod", "\n", "def", "init_observation_space", "(", ")", ":", "\n", "        ", "cart_pos_space", "=", "np", ".", "linspace", "(", "-", "2.4", ",", "2.4", ",", "10", ")", "\n", "cart_vel_space", "=", "np", ".", "linspace", "(", "-", "4", ",", "4", ",", "10", ")", "\n", "pole_theta_space", "=", "np", ".", "linspace", "(", "-", "0.20943951", ",", "0.20943951", ",", "10", ")", "\n", "pole_theta_vel_space", "=", "np", ".", "linspace", "(", "-", "4", ",", "4", ",", "10", ")", "\n", "return", "[", "cart_pos_space", ",", "cart_vel_space", ",", "pole_theta_space", ",", "pole_theta_vel_space", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.az79nefy_OrdinalRL.q_learning.ordinal_q_discretized_agent.QAgent.build_obs_dict": [[33, 44], ["list", "range", "itertools.product", "len"], "methods", ["None"], ["", "@", "staticmethod", "\n", "def", "build_obs_dict", "(", "observation_space", ")", ":", "\n", "# List of all possible discrete observations", "\n", "        ", "observation_range", "=", "[", "range", "(", "len", "(", "i", ")", "+", "1", ")", "for", "i", "in", "observation_space", "]", "\n", "# Dictionary that maps discretized observations to array indices", "\n", "observation_to_index", "=", "{", "}", "\n", "index_counter", "=", "0", "\n", "for", "observation", "in", "list", "(", "itertools", ".", "product", "(", "*", "observation_range", ")", ")", ":", "\n", "            ", "observation_to_index", "[", "observation", "]", "=", "index_counter", "\n", "index_counter", "+=", "1", "\n", "", "return", "observation_to_index", "\n", "\n"]], "home.repos.pwc.inspect_result.az79nefy_OrdinalRL.q_learning.ordinal_q_discretized_agent.QAgent.update": [[45, 49], ["ordinal_q_discretized_agent.QAgent.reward_to_ordinal", "ordinal_q_discretized_agent.QAgent.update_ordinal_values"], "methods", ["home.repos.pwc.inspect_result.az79nefy_OrdinalRL.dqn.ordinal_dqn_discretized_agent.DQNAgent.reward_to_ordinal", "home.repos.pwc.inspect_result.az79nefy_OrdinalRL.q_learning.alternative_ordinal_q_discretized_agent.QAgent.update_ordinal_values"], ["", "def", "update", "(", "self", ",", "prev_obs", ",", "prev_act", ",", "obs", ",", "reward", ",", "episode_reward", ",", "done", ")", ":", "\n", "        ", "ordinal", "=", "self", ".", "reward_to_ordinal", "(", "reward", ",", "episode_reward", ",", "done", ")", "\n", "# update ordinal_values with received ordinal", "\n", "self", ".", "update_ordinal_values", "(", "prev_obs", ",", "prev_act", ",", "obs", ",", "ordinal", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.az79nefy_OrdinalRL.q_learning.ordinal_q_discretized_agent.QAgent.update_ordinal_values": [[51, 60], ["ordinal_q_discretized_agent.QAgent.get_greedy_action", "range"], "methods", ["home.repos.pwc.inspect_result.az79nefy_OrdinalRL.dqn.atari_dqn_single_net_agent.DQNAgent.get_greedy_action"], ["", "def", "update_ordinal_values", "(", "self", ",", "prev_obs", ",", "prev_act", ",", "obs", ",", "ordinal", ")", ":", "\n", "        ", "greedy_action", "=", "self", ".", "get_greedy_action", "(", "obs", ")", "\n", "# reduce old data weight", "\n", "for", "i", "in", "range", "(", "self", ".", "n_ordinals", ")", ":", "\n", "            ", "self", ".", "ordinal_values", "[", "prev_obs", ",", "prev_act", ",", "i", "]", "*=", "(", "1", "-", "self", ".", "alpha", ")", "\n", "self", ".", "ordinal_values", "[", "prev_obs", ",", "prev_act", ",", "i", "]", "+=", "self", ".", "alpha", "*", "(", "self", ".", "gamma", "*", "self", ".", "ordinal_values", "[", "obs", ",", "greedy_action", ",", "i", "]", ")", "\n", "\n", "# add new data point", "\n", "", "self", ".", "ordinal_values", "[", "prev_obs", ",", "prev_act", ",", "ordinal", "]", "+=", "self", ".", "alpha", "\n", "\n"]], "home.repos.pwc.inspect_result.az79nefy_OrdinalRL.q_learning.ordinal_q_discretized_agent.QAgent.compute_borda_scores": [[62, 112], ["numpy.zeros", "range", "numpy.count_nonzero", "range", "borda_scores.append", "borda_scores.append", "range", "borda_scores.append", "range"], "methods", ["None"], ["", "def", "compute_borda_scores", "(", "self", ",", "obs", ")", ":", "\n", "# sum up all ordinal values per action for given observation", "\n", "        ", "ordinal_value_sum_per_action", "=", "np", ".", "zeros", "(", "self", ".", "n_actions", ")", "\n", "for", "action_a", "in", "range", "(", "self", ".", "n_actions", ")", ":", "\n", "            ", "for", "ordinal_value", "in", "self", ".", "ordinal_values", "[", "obs", ",", "action_a", "]", ":", "\n", "                ", "ordinal_value_sum_per_action", "[", "action_a", "]", "+=", "ordinal_value", "\n", "\n", "# count actions whose ordinal value sum is not zero (no comparision possible for actions without ordinal_value)", "\n", "", "", "non_zero_action_count", "=", "np", ".", "count_nonzero", "(", "ordinal_value_sum_per_action", ")", "\n", "actions_to_compare_count", "=", "non_zero_action_count", "-", "1", "\n", "\n", "borda_scores", "=", "[", "]", "\n", "# compute borda_values for action_a (probability that action_a wins against any other action)", "\n", "for", "action_a", "in", "range", "(", "self", ".", "n_actions", ")", ":", "\n", "# if action has not yet recorded any ordinal values, action has to be played (set borda_value to 1.0)", "\n", "            ", "if", "ordinal_value_sum_per_action", "[", "action_a", "]", "==", "0", ":", "\n", "                ", "borda_scores", ".", "append", "(", "1.0", ")", "\n", "continue", "\n", "\n", "", "if", "actions_to_compare_count", "<", "1", ":", "\n", "# set lower than 1.0 (borda_value for zero_actions is 1.0)", "\n", "                ", "borda_scores", ".", "append", "(", "0.5", ")", "\n", "", "else", ":", "\n", "# over all actions: sum up the probabilities that action_a wins against the given action", "\n", "                ", "winning_probability_a_sum", "=", "0", "\n", "# compare action_a to all other actions", "\n", "for", "action_b", "in", "range", "(", "self", ".", "n_actions", ")", ":", "\n", "                    ", "if", "action_a", "==", "action_b", ":", "\n", "                        ", "continue", "\n", "# not comparable if action_b has no ordinal_values", "\n", "", "if", "ordinal_value_sum_per_action", "[", "action_b", "]", "==", "0", ":", "\n", "                        ", "continue", "\n", "", "else", ":", "\n", "# probability that action_a wins against action_b", "\n", "                        ", "winning_probability_a", "=", "0", "\n", "# running ordinal probability that action_b is worse than current investigated ordinal", "\n", "worse_probability_b", "=", "0", "\n", "for", "ordinal_count", "in", "range", "(", "self", ".", "n_ordinals", ")", ":", "\n", "                            ", "ordinal_probability_a", "=", "self", ".", "ordinal_values", "[", "obs", ",", "action_a", ",", "ordinal_count", "]", "/", "ordinal_value_sum_per_action", "[", "action_a", "]", "\n", "# ordinal_probability_b is also the tie probability", "\n", "ordinal_probability_b", "=", "(", "self", ".", "ordinal_values", "[", "obs", ",", "action_b", ",", "ordinal_count", "]", "/", "\n", "ordinal_value_sum_per_action", "[", "action_b", "]", ")", "\n", "winning_probability_a", "+=", "ordinal_probability_a", "*", "(", "worse_probability_b", "+", "ordinal_probability_b", "/", "2.0", ")", "\n", "worse_probability_b", "+=", "ordinal_probability_b", "\n", "", "winning_probability_a_sum", "+=", "winning_probability_a", "\n", "# normalize summed up probabilities with number of actions that have been compared", "\n", "", "", "borda_scores", ".", "append", "(", "winning_probability_a_sum", "/", "actions_to_compare_count", ")", "\n", "", "", "return", "borda_scores", "\n", "\n"]], "home.repos.pwc.inspect_result.az79nefy_OrdinalRL.q_learning.ordinal_q_discretized_agent.QAgent.get_greedy_action": [[113, 115], ["numpy.argmax", "ordinal_q_discretized_agent.QAgent.compute_borda_scores"], "methods", ["home.repos.pwc.inspect_result.az79nefy_OrdinalRL.dqn.ordinal_dqn_discretized_agent.DQNAgent.compute_borda_scores"], ["", "def", "get_greedy_action", "(", "self", ",", "obs", ")", ":", "\n", "        ", "return", "np", ".", "argmax", "(", "self", ".", "compute_borda_scores", "(", "obs", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.az79nefy_OrdinalRL.q_learning.ordinal_q_discretized_agent.QAgent.choose_action": [[117, 125], ["ordinal_q_discretized_agent.QAgent.get_greedy_action", "random.randrange", "random.random"], "methods", ["home.repos.pwc.inspect_result.az79nefy_OrdinalRL.dqn.atari_dqn_single_net_agent.DQNAgent.get_greedy_action"], ["", "def", "choose_action", "(", "self", ",", "obs", ",", "greedy", ")", ":", "\n", "        ", "greedy_action", "=", "self", ".", "get_greedy_action", "(", "obs", ")", "\n", "# choose random action with probability epsilon", "\n", "if", "not", "greedy", "and", "random", ".", "random", "(", ")", "<", "self", ".", "epsilon", ":", "\n", "            ", "return", "random", ".", "randrange", "(", "self", ".", "n_actions", ")", "\n", "# greedy action is chosen with probability (1 - epsilon)", "\n", "", "else", ":", "\n", "            ", "return", "greedy_action", "\n", "\n"]], "home.repos.pwc.inspect_result.az79nefy_OrdinalRL.q_learning.ordinal_q_discretized_agent.QAgent.end_episode": [[126, 129], ["None"], "methods", ["None"], ["", "", "def", "end_episode", "(", "self", ",", "n_episodes", ")", ":", "\n", "# gradually reduce epsilon after every done episode", "\n", "        ", "self", ".", "epsilon", "=", "self", ".", "epsilon", "-", "2", "/", "n_episodes", "if", "self", ".", "epsilon", ">", "self", ".", "epsilon_min", "else", "self", ".", "epsilon_min", "\n", "\n"]], "home.repos.pwc.inspect_result.az79nefy_OrdinalRL.q_learning.ordinal_q_discretized_agent.QAgent.preprocess_observation": [[130, 135], ["range", "len", "discrete_observation.append", "int", "tuple", "numpy.digitize"], "methods", ["None"], ["", "def", "preprocess_observation", "(", "self", ",", "obs", ")", ":", "\n", "        ", "discrete_observation", "=", "[", "]", "\n", "for", "obs_idx", "in", "range", "(", "len", "(", "obs", ")", ")", ":", "\n", "            ", "discrete_observation", ".", "append", "(", "int", "(", "np", ".", "digitize", "(", "obs", "[", "obs_idx", "]", ",", "self", ".", "observation_space", "[", "obs_idx", "]", ")", ")", ")", "\n", "", "return", "self", ".", "observation_to_index", "[", "tuple", "(", "discrete_observation", ")", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.az79nefy_OrdinalRL.q_learning.ordinal_q_discretized_agent.QAgent.reward_to_ordinal": [[137, 142], ["ordinal_q_discretized_agent.QAgent.check_win_condition"], "methods", ["home.repos.pwc.inspect_result.az79nefy_OrdinalRL.dqn.atari_dqn_single_net_agent.DQNAgent.check_win_condition"], ["", "def", "reward_to_ordinal", "(", "self", ",", "reward", ",", "episode_reward", ",", "done", ")", ":", "\n", "        ", "if", "done", "and", "not", "self", ".", "check_win_condition", "(", "reward", ",", "episode_reward", ",", "done", ")", ":", "\n", "            ", "return", "0", "\n", "", "else", ":", "\n", "            ", "return", "1", "\n", "\n"]], "home.repos.pwc.inspect_result.az79nefy_OrdinalRL.q_learning.ordinal_q_discretized_agent.QAgent.check_win_condition": [[144, 150], ["None"], "methods", ["None"], ["", "", "@", "staticmethod", "\n", "def", "check_win_condition", "(", "reward", ",", "episode_reward", ",", "done", ")", ":", "\n", "        ", "if", "done", "and", "episode_reward", "==", "200", ":", "\n", "            ", "return", "True", "\n", "", "else", ":", "\n", "            ", "return", "False", "\n", "\n"]], "home.repos.pwc.inspect_result.az79nefy_OrdinalRL.q_learning.ordinal_q_discretized_agent.QAgent.evaluate": [[151, 159], ["round", "round", "ordinal_q_discretized_agent.QAgent.average_rewards.append", "ordinal_q_discretized_agent.QAgent.win_rates.append", "print", "sum", "len", "sum", "len"], "methods", ["None"], ["", "", "def", "evaluate", "(", "self", ",", "i_episode", ",", "episode_rewards", ",", "episode_wins", ")", ":", "\n", "# compute average episode reward and win rate over last episodes", "\n", "        ", "average_reward", "=", "round", "(", "sum", "(", "episode_rewards", ")", "/", "len", "(", "episode_rewards", ")", ",", "2", ")", "\n", "win_rate", "=", "round", "(", "sum", "(", "episode_wins", ")", "/", "len", "(", "episode_wins", ")", ",", "2", ")", "\n", "# store average episode reward and win rate over last episodes for plotting purposes", "\n", "self", ".", "average_rewards", ".", "append", "(", "average_reward", ")", "\n", "self", ".", "win_rates", ".", "append", "(", "win_rate", ")", "\n", "print", "(", "\"{}\\t{}\\t{}\"", ".", "format", "(", "i_episode", "+", "1", ",", "average_reward", ",", "win_rate", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.az79nefy_OrdinalRL.q_learning.ordinal_q_discretized_agent.QAgent.plot": [[161, 175], ["matplotlib.figure", "matplotlib.plot", "matplotlib.xlabel", "matplotlib.ylabel", "matplotlib.figure", "matplotlib.plot", "matplotlib.xlabel", "matplotlib.ylabel", "matplotlib.show", "list", "list", "range", "range"], "methods", ["home.repos.pwc.inspect_result.az79nefy_OrdinalRL.dqn.atari_dqn_single_net_agent.DQNAgent.plot", "home.repos.pwc.inspect_result.az79nefy_OrdinalRL.dqn.atari_dqn_single_net_agent.DQNAgent.plot"], ["", "def", "plot", "(", "self", ",", "n_episodes", ",", "step_size", ")", ":", "\n", "# plot win rate", "\n", "        ", "plt", ".", "figure", "(", ")", "\n", "plt", ".", "plot", "(", "list", "(", "range", "(", "step_size", ",", "n_episodes", "+", "step_size", ",", "step_size", ")", ")", ",", "self", ".", "win_rates", ")", "\n", "plt", ".", "xlabel", "(", "'Number of episodes'", ")", "\n", "plt", ".", "ylabel", "(", "'Win rate'", ")", "\n", "\n", "# plot average score", "\n", "plt", ".", "figure", "(", ")", "\n", "plt", ".", "plot", "(", "list", "(", "range", "(", "step_size", ",", "n_episodes", "+", "step_size", ",", "step_size", ")", ")", ",", "self", ".", "average_rewards", ")", "\n", "plt", ".", "xlabel", "(", "'Number of episodes'", ")", "\n", "plt", ".", "ylabel", "(", "'Average score'", ")", "\n", "\n", "plt", ".", "show", "(", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.az79nefy_OrdinalRL.q_learning.alternative_ordinal_q_agent.QAgent.__init__": [[7, 20], ["numpy.full"], "methods", ["None"], ["    ", "def", "__init__", "(", "self", ",", "alpha", ",", "gamma", ",", "epsilon", ",", "epsilon_min", ",", "n_actions", ",", "n_ordinals", ",", "n_observations", ",", "randomize", ")", ":", "\n", "        ", "self", ".", "alpha", "=", "alpha", "\n", "self", ".", "gamma", "=", "gamma", "\n", "self", ".", "epsilon", "=", "epsilon", "\n", "self", ".", "epsilon_min", "=", "epsilon_min", "\n", "self", ".", "n_actions", "=", "n_actions", "\n", "self", ".", "n_ordinals", "=", "n_ordinals", "\n", "\n", "# Ordinal_Values (3-dimensional array with ordinal_value (array of floats) for each action in each observation)", "\n", "self", ".", "ordinal_values", "=", "np", ".", "full", "(", "(", "n_observations", ",", "n_actions", ",", "n_ordinals", ")", ",", "0.0", ")", "\n", "\n", "self", ".", "win_rates", "=", "[", "]", "\n", "self", ".", "average_rewards", "=", "[", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.az79nefy_OrdinalRL.q_learning.alternative_ordinal_q_agent.QAgent.update": [[21, 25], ["alternative_ordinal_q_agent.QAgent.reward_to_ordinal", "alternative_ordinal_q_agent.QAgent.update_ordinal_values"], "methods", ["home.repos.pwc.inspect_result.az79nefy_OrdinalRL.dqn.ordinal_dqn_discretized_agent.DQNAgent.reward_to_ordinal", "home.repos.pwc.inspect_result.az79nefy_OrdinalRL.q_learning.alternative_ordinal_q_discretized_agent.QAgent.update_ordinal_values"], ["", "def", "update", "(", "self", ",", "prev_obs", ",", "prev_act", ",", "obs", ",", "reward", ",", "episode_reward", ",", "done", ")", ":", "\n", "        ", "ordinal", "=", "self", ".", "reward_to_ordinal", "(", "reward", ",", "episode_reward", ",", "done", ")", "\n", "# update ordinal_values with received ordinal", "\n", "self", ".", "update_ordinal_values", "(", "prev_obs", ",", "prev_act", ",", "obs", ",", "ordinal", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.az79nefy_OrdinalRL.q_learning.alternative_ordinal_q_agent.QAgent.update_ordinal_values": [[27, 36], ["alternative_ordinal_q_agent.QAgent.get_greedy_action", "range"], "methods", ["home.repos.pwc.inspect_result.az79nefy_OrdinalRL.dqn.atari_dqn_single_net_agent.DQNAgent.get_greedy_action"], ["", "def", "update_ordinal_values", "(", "self", ",", "prev_obs", ",", "prev_act", ",", "obs", ",", "ordinal", ")", ":", "\n", "        ", "greedy_action", "=", "self", ".", "get_greedy_action", "(", "obs", ")", "\n", "# reduce old data weight", "\n", "for", "i", "in", "range", "(", "self", ".", "n_ordinals", ")", ":", "\n", "            ", "self", ".", "ordinal_values", "[", "prev_obs", ",", "prev_act", ",", "i", "]", "*=", "(", "1", "-", "self", ".", "alpha", ")", "\n", "self", ".", "ordinal_values", "[", "prev_obs", ",", "prev_act", ",", "i", "]", "+=", "self", ".", "alpha", "*", "(", "self", ".", "gamma", "*", "self", ".", "ordinal_values", "[", "obs", ",", "greedy_action", ",", "i", "]", ")", "\n", "\n", "# add new data point", "\n", "", "self", ".", "ordinal_values", "[", "prev_obs", ",", "prev_act", ",", "ordinal", "]", "+=", "self", ".", "alpha", "\n", "\n"]], "home.repos.pwc.inspect_result.az79nefy_OrdinalRL.q_learning.alternative_ordinal_q_agent.QAgent.compute_borda_counts": [[38, 56], ["numpy.zeros", "range", "range", "reversed", "borda_counts.append"], "methods", ["None"], ["", "def", "compute_borda_counts", "(", "self", ",", "obs", ")", ":", "\n", "# sum up all ordinal values per action for given observation", "\n", "        ", "ordinal_value_sum_per_action", "=", "np", ".", "zeros", "(", "self", ".", "n_actions", ")", "\n", "for", "action_a", "in", "range", "(", "self", ".", "n_actions", ")", ":", "\n", "            ", "for", "ordinal_value", "in", "self", ".", "ordinal_values", "[", "obs", ",", "action_a", "]", ":", "\n", "                ", "ordinal_value_sum_per_action", "[", "action_a", "]", "+=", "ordinal_value", "\n", "\n", "", "", "borda_counts", "=", "[", "]", "\n", "for", "action_a", "in", "range", "(", "self", ".", "n_actions", ")", ":", "\n", "            ", "action_score", "=", "0", "\n", "ordinal_values", "=", "self", ".", "ordinal_values", "[", "obs", ",", "action_a", "]", "\n", "ordinal_worth", "=", "1.0", "\n", "for", "ordinal_value", "in", "reversed", "(", "ordinal_values", ")", ":", "\n", "                ", "ordinal_probability", "=", "ordinal_value", "/", "ordinal_value_sum_per_action", "[", "action_a", "]", "\n", "action_score", "+=", "ordinal_probability", "*", "ordinal_worth", "\n", "ordinal_worth", "=", "ordinal_worth", "/", "2", "\n", "", "borda_counts", ".", "append", "(", "action_score", ")", "\n", "", "return", "borda_counts", "\n", "\n"]], "home.repos.pwc.inspect_result.az79nefy_OrdinalRL.q_learning.alternative_ordinal_q_agent.QAgent.compute_borda_scores": [[58, 108], ["numpy.zeros", "range", "numpy.count_nonzero", "range", "borda_scores.append", "borda_scores.append", "range", "borda_scores.append", "range"], "methods", ["None"], ["", "def", "compute_borda_scores", "(", "self", ",", "obs", ")", ":", "\n", "# sum up all ordinal values per action for given observation", "\n", "        ", "ordinal_value_sum_per_action", "=", "np", ".", "zeros", "(", "self", ".", "n_actions", ")", "\n", "for", "action_a", "in", "range", "(", "self", ".", "n_actions", ")", ":", "\n", "            ", "for", "ordinal_value", "in", "self", ".", "ordinal_values", "[", "obs", ",", "action_a", "]", ":", "\n", "                ", "ordinal_value_sum_per_action", "[", "action_a", "]", "+=", "ordinal_value", "\n", "\n", "# count actions whose ordinal value sum is not zero (no comparision possible for actions without ordinal_value)", "\n", "", "", "non_zero_action_count", "=", "np", ".", "count_nonzero", "(", "ordinal_value_sum_per_action", ")", "\n", "actions_to_compare_count", "=", "non_zero_action_count", "-", "1", "\n", "\n", "borda_scores", "=", "[", "]", "\n", "# compute borda_values for action_a (probability that action_a wins against any other action)", "\n", "for", "action_a", "in", "range", "(", "self", ".", "n_actions", ")", ":", "\n", "# if action has not yet recorded any ordinal values, action has to be played (set borda_value to 1.0)", "\n", "            ", "if", "ordinal_value_sum_per_action", "[", "action_a", "]", "==", "0", ":", "\n", "                ", "borda_scores", ".", "append", "(", "1.0", ")", "\n", "continue", "\n", "\n", "", "if", "actions_to_compare_count", "<", "1", ":", "\n", "# set lower than 1.0 (borda_value for zero_actions is 1.0)", "\n", "                ", "borda_scores", ".", "append", "(", "0.5", ")", "\n", "", "else", ":", "\n", "# over all actions: sum up the probabilities that action_a wins against the given action", "\n", "                ", "winning_probability_a_sum", "=", "0", "\n", "# compare action_a to all other actions", "\n", "for", "action_b", "in", "range", "(", "self", ".", "n_actions", ")", ":", "\n", "                    ", "if", "action_a", "==", "action_b", ":", "\n", "                        ", "continue", "\n", "# not comparable if action_b has no ordinal_values", "\n", "", "if", "ordinal_value_sum_per_action", "[", "action_b", "]", "==", "0", ":", "\n", "                        ", "continue", "\n", "", "else", ":", "\n", "# probability that action_a wins against action_b", "\n", "                        ", "winning_probability_a", "=", "0", "\n", "# running ordinal probability that action_b is worse than current investigated ordinal", "\n", "worse_probability_b", "=", "0", "\n", "for", "ordinal_count", "in", "range", "(", "self", ".", "n_ordinals", ")", ":", "\n", "                            ", "ordinal_probability_a", "=", "self", ".", "ordinal_values", "[", "obs", ",", "action_a", ",", "ordinal_count", "]", "/", "ordinal_value_sum_per_action", "[", "action_a", "]", "\n", "# ordinal_probability_b is also the tie probability", "\n", "ordinal_probability_b", "=", "(", "self", ".", "ordinal_values", "[", "obs", ",", "action_b", ",", "ordinal_count", "]", "/", "\n", "ordinal_value_sum_per_action", "[", "action_b", "]", ")", "\n", "winning_probability_a", "+=", "ordinal_probability_a", "*", "(", "worse_probability_b", "+", "ordinal_probability_b", "/", "2.0", ")", "\n", "worse_probability_b", "+=", "ordinal_probability_b", "\n", "", "winning_probability_a_sum", "+=", "winning_probability_a", "\n", "# normalize summed up probabilities with number of actions that have been compared", "\n", "", "", "borda_scores", ".", "append", "(", "winning_probability_a_sum", "/", "actions_to_compare_count", ")", "\n", "", "", "return", "borda_scores", "\n", "\n"]], "home.repos.pwc.inspect_result.az79nefy_OrdinalRL.q_learning.alternative_ordinal_q_agent.QAgent.compute_copeland_win_rate": [[110, 163], ["numpy.zeros", "range", "numpy.count_nonzero", "range", "win_rates.append", "win_rates.append", "range", "win_rates.append", "range"], "methods", ["None"], ["", "def", "compute_copeland_win_rate", "(", "self", ",", "obs", ")", ":", "\n", "# sum up all ordinal values per action for given observation", "\n", "        ", "ordinal_value_sum_per_action", "=", "np", ".", "zeros", "(", "self", ".", "n_actions", ")", "\n", "for", "action_a", "in", "range", "(", "self", ".", "n_actions", ")", ":", "\n", "            ", "for", "ordinal_value", "in", "self", ".", "ordinal_values", "[", "obs", ",", "action_a", "]", ":", "\n", "                ", "ordinal_value_sum_per_action", "[", "action_a", "]", "+=", "ordinal_value", "\n", "\n", "# count actions whose ordinal value sum is not zero (no comparision possible for actions without ordinal_value)", "\n", "", "", "non_zero_action_count", "=", "np", ".", "count_nonzero", "(", "ordinal_value_sum_per_action", ")", "\n", "actions_to_compare_count", "=", "non_zero_action_count", "-", "1", "\n", "\n", "win_rates", "=", "[", "]", "\n", "# compute win_rate for action_a", "\n", "for", "action_a", "in", "range", "(", "self", ".", "n_actions", ")", ":", "\n", "# if action has not yet recorded any ordinal values, action has to be played (set win_rate to 1.0)", "\n", "            ", "if", "ordinal_value_sum_per_action", "[", "action_a", "]", "==", "0", ":", "\n", "                ", "win_rates", ".", "append", "(", "1.0", ")", "\n", "continue", "\n", "\n", "", "if", "actions_to_compare_count", "<", "1", ":", "\n", "# set lower than 1.0 (win_rate for zero_actions is 1.0)", "\n", "                ", "win_rates", ".", "append", "(", "0.5", ")", "\n", "", "else", ":", "\n", "# over all actions: count the number of times that action_a wins against the given action", "\n", "                ", "win_counter_a", "=", "0", "\n", "# compare action_a to all other actions", "\n", "for", "action_b", "in", "range", "(", "self", ".", "n_actions", ")", ":", "\n", "                    ", "if", "action_a", "==", "action_b", ":", "\n", "                        ", "continue", "\n", "# not comparable if action_b has no ordinal_values", "\n", "", "if", "ordinal_value_sum_per_action", "[", "action_b", "]", "==", "0", ":", "\n", "                        ", "continue", "\n", "", "else", ":", "\n", "# probability that action_a wins against action_b", "\n", "                        ", "winning_probability_a", "=", "0", "\n", "# running ordinal probability that action_b is worse than current investigated ordinal", "\n", "worse_probability_b", "=", "0", "\n", "for", "ordinal_count", "in", "range", "(", "self", ".", "n_ordinals", ")", ":", "\n", "                            ", "ordinal_probability_a", "=", "self", ".", "ordinal_values", "[", "obs", ",", "action_a", ",", "ordinal_count", "]", "/", "ordinal_value_sum_per_action", "[", "action_a", "]", "\n", "# ordinal_probability_b is also the tie probability", "\n", "ordinal_probability_b", "=", "(", "self", ".", "ordinal_values", "[", "obs", ",", "action_b", ",", "ordinal_count", "]", "/", "\n", "ordinal_value_sum_per_action", "[", "action_b", "]", ")", "\n", "winning_probability_a", "+=", "ordinal_probability_a", "*", "(", "worse_probability_b", "+", "ordinal_probability_b", "/", "2.0", ")", "\n", "worse_probability_b", "+=", "ordinal_probability_b", "\n", "", "if", "winning_probability_a", ">", "0.5", ":", "\n", "                            ", "win_counter_a", "+=", "1.0", "\n", "", "elif", "winning_probability_a", "==", "0.5", ":", "\n", "                            ", "win_counter_a", "+=", "0.5", "\n", "# normalize win_counter with number of actions that have been compared to compute win_rate", "\n", "", "", "", "win_rates", ".", "append", "(", "win_counter_a", "/", "actions_to_compare_count", ")", "\n", "", "", "return", "win_rates", "\n", "\n"]], "home.repos.pwc.inspect_result.az79nefy_OrdinalRL.q_learning.alternative_ordinal_q_agent.QAgent.compute_plurality_runoff_borda_winner": [[164, 223], ["numpy.zeros", "range", "numpy.count_nonzero", "list", "range", "range", "list.pop", "borda_scores.append", "borda_scores.append", "borda_scores.append", "len", "list.pop", "borda_scores.pop", "numpy.argmin", "numpy.argmin", "numpy.argmin", "range"], "methods", ["None"], ["", "def", "compute_plurality_runoff_borda_winner", "(", "self", ",", "obs", ")", ":", "\n", "# sum up all ordinal values per action for given observation", "\n", "        ", "ordinal_value_sum_per_action", "=", "np", ".", "zeros", "(", "self", ".", "n_actions", ")", "\n", "for", "action_a", "in", "range", "(", "self", ".", "n_actions", ")", ":", "\n", "            ", "for", "ordinal_value", "in", "self", ".", "ordinal_values", "[", "obs", ",", "action_a", "]", ":", "\n", "                ", "ordinal_value_sum_per_action", "[", "action_a", "]", "+=", "ordinal_value", "\n", "\n", "# count actions whose ordinal value sum is not zero (no comparision possible for actions without ordinal_value)", "\n", "", "", "non_zero_action_count", "=", "np", ".", "count_nonzero", "(", "ordinal_value_sum_per_action", ")", "\n", "actions_to_compare_count", "=", "non_zero_action_count", "-", "1", "\n", "\n", "possible_actions", "=", "list", "(", "range", "(", "self", ".", "n_actions", ")", ")", "\n", "# in first iteration all actions are compared; in the second iteration the two best actions are compared", "\n", "for", "i", "in", "range", "(", "2", ")", ":", "\n", "            ", "borda_scores", "=", "[", "]", "\n", "# compute borda_values for action_a (probability that action_a wins against any other action)", "\n", "for", "action_a", "in", "possible_actions", ":", "\n", "# if action has not yet recorded any ordinal values, action has to be played (set borda_value to 1.0)", "\n", "                ", "if", "ordinal_value_sum_per_action", "[", "action_a", "]", "==", "0", ":", "\n", "                    ", "borda_scores", ".", "append", "(", "1.0", ")", "\n", "continue", "\n", "\n", "", "if", "actions_to_compare_count", "<", "1", ":", "\n", "# set lower than 1.0 (borda_value for zero_actions is 1.0)", "\n", "                    ", "borda_scores", ".", "append", "(", "0.5", ")", "\n", "", "else", ":", "\n", "# over all actions: sum up the probabilities that action_a wins against the given action", "\n", "                    ", "winning_probability_a_sum", "=", "0", "\n", "# compare action_a to all other actions", "\n", "for", "action_b", "in", "possible_actions", ":", "\n", "                        ", "if", "action_a", "==", "action_b", ":", "\n", "                            ", "continue", "\n", "# not comparable if action_b has no ordinal_values", "\n", "", "if", "ordinal_value_sum_per_action", "[", "action_b", "]", "==", "0", ":", "\n", "                            ", "continue", "\n", "", "else", ":", "\n", "# probability that action_a wins against action_b", "\n", "                            ", "winning_probability_a", "=", "0", "\n", "# running ordinal probability that action_b is worse than current investigated ordinal", "\n", "worse_probability_b", "=", "0", "\n", "for", "ordinal_count", "in", "range", "(", "self", ".", "n_ordinals", ")", ":", "\n", "                                ", "ordinal_probability_a", "=", "self", ".", "ordinal_values", "[", "obs", ",", "action_a", ",", "ordinal_count", "]", "/", "ordinal_value_sum_per_action", "[", "action_a", "]", "\n", "# ordinal_probability_b is also the tie probability", "\n", "ordinal_probability_b", "=", "(", "self", ".", "ordinal_values", "[", "obs", ",", "action_b", ",", "ordinal_count", "]", "/", "\n", "ordinal_value_sum_per_action", "[", "action_b", "]", ")", "\n", "winning_probability_a", "+=", "ordinal_probability_a", "*", "(", "worse_probability_b", "+", "ordinal_probability_b", "/", "2.0", ")", "\n", "worse_probability_b", "+=", "ordinal_probability_b", "\n", "", "winning_probability_a_sum", "+=", "winning_probability_a", "\n", "# normalize summed up probabilities with number of actions that have been compared", "\n", "", "", "borda_scores", ".", "append", "(", "winning_probability_a_sum", "/", "actions_to_compare_count", ")", "\n", "", "", "if", "i", "==", "0", ":", "\n", "                ", "while", "len", "(", "possible_actions", ")", ">", "2", ":", "\n", "                    ", "possible_actions", ".", "pop", "(", "np", ".", "argmin", "(", "borda_scores", ")", ")", "\n", "borda_scores", ".", "pop", "(", "np", ".", "argmin", "(", "borda_scores", ")", ")", "\n", "", "", "if", "i", "==", "1", ":", "\n", "                ", "possible_actions", ".", "pop", "(", "np", ".", "argmin", "(", "borda_scores", ")", ")", "\n", "", "", "return", "possible_actions", "[", "0", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.az79nefy_OrdinalRL.q_learning.alternative_ordinal_q_agent.QAgent.compute_instant_runoff_borda_winner": [[224, 278], ["numpy.zeros", "range", "numpy.count_nonzero", "list", "range", "range", "list.pop", "numpy.argmin", "borda_scores.append", "borda_scores.append", "borda_scores.append", "range"], "methods", ["None"], ["", "def", "compute_instant_runoff_borda_winner", "(", "self", ",", "obs", ")", ":", "\n", "# sum up all ordinal values per action for given observation", "\n", "        ", "ordinal_value_sum_per_action", "=", "np", ".", "zeros", "(", "self", ".", "n_actions", ")", "\n", "for", "action_a", "in", "range", "(", "self", ".", "n_actions", ")", ":", "\n", "            ", "for", "ordinal_value", "in", "self", ".", "ordinal_values", "[", "obs", ",", "action_a", "]", ":", "\n", "                ", "ordinal_value_sum_per_action", "[", "action_a", "]", "+=", "ordinal_value", "\n", "\n", "# count actions whose ordinal value sum is not zero (no comparision possible for actions without ordinal_value)", "\n", "", "", "non_zero_action_count", "=", "np", ".", "count_nonzero", "(", "ordinal_value_sum_per_action", ")", "\n", "actions_to_compare_count", "=", "non_zero_action_count", "-", "1", "\n", "\n", "possible_actions", "=", "list", "(", "range", "(", "self", ".", "n_actions", ")", ")", "\n", "# in each iteration the worst action according to borda_scores is removed and borda_scores are recalculated", "\n", "for", "_", "in", "range", "(", "self", ".", "n_actions", "-", "1", ")", ":", "\n", "            ", "borda_scores", "=", "[", "]", "\n", "# compute borda_values for action_a (probability that action_a wins against any other action)", "\n", "for", "action_a", "in", "possible_actions", ":", "\n", "# if action has not yet recorded any ordinal values, action has to be played (set borda_value to 1.0)", "\n", "                ", "if", "ordinal_value_sum_per_action", "[", "action_a", "]", "==", "0", ":", "\n", "                    ", "borda_scores", ".", "append", "(", "1.0", ")", "\n", "continue", "\n", "\n", "", "if", "actions_to_compare_count", "<", "1", ":", "\n", "# set lower than 1.0 (borda_value for zero_actions is 1.0)", "\n", "                    ", "borda_scores", ".", "append", "(", "0.5", ")", "\n", "", "else", ":", "\n", "# over all actions: sum up the probabilities that action_a wins against the given action", "\n", "                    ", "winning_probability_a_sum", "=", "0", "\n", "# compare action_a to all other actions", "\n", "for", "action_b", "in", "possible_actions", ":", "\n", "                        ", "if", "action_a", "==", "action_b", ":", "\n", "                            ", "continue", "\n", "# not comparable if action_b has no ordinal_values", "\n", "", "if", "ordinal_value_sum_per_action", "[", "action_b", "]", "==", "0", ":", "\n", "                            ", "continue", "\n", "", "else", ":", "\n", "# probability that action_a wins against action_b", "\n", "                            ", "winning_probability_a", "=", "0", "\n", "# running ordinal probability that action_b is worse than current investigated ordinal", "\n", "worse_probability_b", "=", "0", "\n", "for", "ordinal_count", "in", "range", "(", "self", ".", "n_ordinals", ")", ":", "\n", "                                ", "ordinal_probability_a", "=", "self", ".", "ordinal_values", "[", "obs", ",", "action_a", ",", "ordinal_count", "]", "/", "ordinal_value_sum_per_action", "[", "action_a", "]", "\n", "# ordinal_probability_b is also the tie probability", "\n", "ordinal_probability_b", "=", "(", "self", ".", "ordinal_values", "[", "obs", ",", "action_b", ",", "ordinal_count", "]", "/", "\n", "ordinal_value_sum_per_action", "[", "action_b", "]", ")", "\n", "winning_probability_a", "+=", "ordinal_probability_a", "*", "(", "worse_probability_b", "+", "ordinal_probability_b", "/", "2.0", ")", "\n", "worse_probability_b", "+=", "ordinal_probability_b", "\n", "", "winning_probability_a_sum", "+=", "winning_probability_a", "\n", "# normalize summed up probabilities with number of actions that have been compared", "\n", "", "", "borda_scores", ".", "append", "(", "winning_probability_a_sum", "/", "actions_to_compare_count", ")", "\n", "", "", "possible_actions", ".", "pop", "(", "np", ".", "argmin", "(", "borda_scores", ")", ")", "\n", "", "return", "possible_actions", "[", "0", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.az79nefy_OrdinalRL.q_learning.alternative_ordinal_q_agent.QAgent.get_greedy_action": [[279, 283], ["numpy.argmax", "alternative_ordinal_q_agent.QAgent.compute_copeland_win_rate"], "methods", ["home.repos.pwc.inspect_result.az79nefy_OrdinalRL.q_learning.alternative_ordinal_q_discretized_agent.QAgent.compute_copeland_win_rate"], ["", "def", "get_greedy_action", "(", "self", ",", "obs", ")", ":", "\n", "# return np.argmax(self.compute_borda_counts(obs))", "\n", "# return np.argmax(self.compute_borda_scores(obs))", "\n", "        ", "return", "np", ".", "argmax", "(", "self", ".", "compute_copeland_win_rate", "(", "obs", ")", ")", "\n", "# return self.compute_plurality_runoff_borda_winner(obs)", "\n"]], "home.repos.pwc.inspect_result.az79nefy_OrdinalRL.q_learning.alternative_ordinal_q_agent.QAgent.choose_action": [[287, 295], ["alternative_ordinal_q_agent.QAgent.get_greedy_action", "random.randrange", "random.random"], "methods", ["home.repos.pwc.inspect_result.az79nefy_OrdinalRL.dqn.atari_dqn_single_net_agent.DQNAgent.get_greedy_action"], ["", "def", "choose_action", "(", "self", ",", "obs", ",", "greedy", ")", ":", "\n", "        ", "greedy_action", "=", "self", ".", "get_greedy_action", "(", "obs", ")", "\n", "# choose random action with probability epsilon", "\n", "if", "not", "greedy", "and", "random", ".", "random", "(", ")", "<", "self", ".", "epsilon", ":", "\n", "            ", "return", "random", ".", "randrange", "(", "self", ".", "n_actions", ")", "\n", "# greedy action is chosen with probability (1 - epsilon)", "\n", "", "else", ":", "\n", "            ", "return", "greedy_action", "\n", "\n"]], "home.repos.pwc.inspect_result.az79nefy_OrdinalRL.q_learning.alternative_ordinal_q_agent.QAgent.end_episode": [[296, 299], ["None"], "methods", ["None"], ["", "", "def", "end_episode", "(", "self", ",", "n_episodes", ")", ":", "\n", "# gradually reduce epsilon after every done episode", "\n", "        ", "self", ".", "epsilon", "=", "self", ".", "epsilon", "-", "2", "/", "n_episodes", "if", "self", ".", "epsilon", ">", "self", ".", "epsilon_min", "else", "self", ".", "epsilon_min", "\n", "\n"]], "home.repos.pwc.inspect_result.az79nefy_OrdinalRL.q_learning.alternative_ordinal_q_agent.QAgent.preprocess_observation": [[300, 302], ["None"], "methods", ["None"], ["", "def", "preprocess_observation", "(", "self", ",", "obs", ")", ":", "\n", "        ", "return", "obs", "\n", "\n"]], "home.repos.pwc.inspect_result.az79nefy_OrdinalRL.q_learning.alternative_ordinal_q_agent.QAgent.reward_to_ordinal": [[304, 311], ["None"], "methods", ["None"], ["", "def", "reward_to_ordinal", "(", "self", ",", "reward", ",", "episode_reward", ",", "done", ")", ":", "\n", "        ", "if", "reward", "==", "-", "10", ":", "\n", "            ", "return", "0", "\n", "", "if", "reward", "==", "-", "1", ":", "\n", "            ", "return", "1", "\n", "", "else", ":", "\n", "            ", "return", "2", "\n", "\n"]], "home.repos.pwc.inspect_result.az79nefy_OrdinalRL.q_learning.alternative_ordinal_q_agent.QAgent.check_win_condition": [[313, 319], ["None"], "methods", ["None"], ["", "", "@", "staticmethod", "\n", "def", "check_win_condition", "(", "reward", ",", "episode_reward", ",", "done", ")", ":", "\n", "        ", "if", "done", "and", "reward", "==", "20", ":", "\n", "            ", "return", "True", "\n", "", "else", ":", "\n", "            ", "return", "False", "\n", "\n"]], "home.repos.pwc.inspect_result.az79nefy_OrdinalRL.q_learning.alternative_ordinal_q_agent.QAgent.evaluate": [[320, 328], ["round", "round", "alternative_ordinal_q_agent.QAgent.average_rewards.append", "alternative_ordinal_q_agent.QAgent.win_rates.append", "print", "sum", "len", "sum", "len"], "methods", ["None"], ["", "", "def", "evaluate", "(", "self", ",", "i_episode", ",", "episode_rewards", ",", "episode_wins", ")", ":", "\n", "# compute average episode reward and win rate over last episodes", "\n", "        ", "average_reward", "=", "round", "(", "sum", "(", "episode_rewards", ")", "/", "len", "(", "episode_rewards", ")", ",", "2", ")", "\n", "win_rate", "=", "round", "(", "sum", "(", "episode_wins", ")", "/", "len", "(", "episode_wins", ")", ",", "2", ")", "\n", "# store average episode reward and win rate over last episodes for plotting purposes", "\n", "self", ".", "average_rewards", ".", "append", "(", "average_reward", ")", "\n", "self", ".", "win_rates", ".", "append", "(", "win_rate", ")", "\n", "print", "(", "\"{}\\t{}\\t{}\"", ".", "format", "(", "i_episode", "+", "1", ",", "average_reward", ",", "win_rate", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.az79nefy_OrdinalRL.q_learning.alternative_ordinal_q_agent.QAgent.plot": [[330, 344], ["matplotlib.figure", "matplotlib.plot", "matplotlib.xlabel", "matplotlib.ylabel", "matplotlib.figure", "matplotlib.plot", "matplotlib.xlabel", "matplotlib.ylabel", "matplotlib.show", "list", "list", "range", "range"], "methods", ["home.repos.pwc.inspect_result.az79nefy_OrdinalRL.dqn.atari_dqn_single_net_agent.DQNAgent.plot", "home.repos.pwc.inspect_result.az79nefy_OrdinalRL.dqn.atari_dqn_single_net_agent.DQNAgent.plot"], ["", "def", "plot", "(", "self", ",", "n_episodes", ",", "step_size", ")", ":", "\n", "# plot win rate", "\n", "        ", "plt", ".", "figure", "(", ")", "\n", "plt", ".", "plot", "(", "list", "(", "range", "(", "step_size", ",", "n_episodes", "+", "step_size", ",", "step_size", ")", ")", ",", "self", ".", "win_rates", ")", "\n", "plt", ".", "xlabel", "(", "'Number of episodes'", ")", "\n", "plt", ".", "ylabel", "(", "'Win rate'", ")", "\n", "\n", "# plot average score", "\n", "plt", ".", "figure", "(", ")", "\n", "plt", ".", "plot", "(", "list", "(", "range", "(", "step_size", ",", "n_episodes", "+", "step_size", ",", "step_size", ")", ")", ",", "self", ".", "average_rewards", ")", "\n", "plt", ".", "xlabel", "(", "'Number of episodes'", ")", "\n", "plt", ".", "ylabel", "(", "'Average score'", ")", "\n", "\n", "plt", ".", "show", "(", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.az79nefy_OrdinalRL.q_learning.alternative_ordinal_q_discretized_agent.QAgent.__init__": [[8, 23], ["alternative_ordinal_q_discretized_agent.QAgent.init_observation_space", "alternative_ordinal_q_discretized_agent.QAgent.build_obs_dict", "numpy.full"], "methods", ["home.repos.pwc.inspect_result.az79nefy_OrdinalRL.q_learning.alternative_ordinal_q_discretized_agent.QAgent.init_observation_space", "home.repos.pwc.inspect_result.az79nefy_OrdinalRL.q_learning.alternative_ordinal_q_discretized_agent.QAgent.build_obs_dict"], ["    ", "def", "__init__", "(", "self", ",", "alpha", ",", "gamma", ",", "epsilon", ",", "epsilon_min", ",", "n_actions", ",", "n_ordinals", ",", "n_observations", ",", "randomize", ")", ":", "\n", "        ", "self", ".", "alpha", "=", "alpha", "\n", "self", ".", "gamma", "=", "gamma", "\n", "self", ".", "epsilon", "=", "epsilon", "\n", "self", ".", "epsilon_min", "=", "epsilon_min", "\n", "self", ".", "n_actions", "=", "n_actions", "\n", "self", ".", "n_ordinals", "=", "n_ordinals", "\n", "self", ".", "observation_space", "=", "self", ".", "init_observation_space", "(", ")", "\n", "self", ".", "observation_to_index", "=", "self", ".", "build_obs_dict", "(", "self", ".", "observation_space", ")", "\n", "\n", "# Ordinal_Values (3-dimensional array with ordinal_value (array of floats) for each action in each observation)", "\n", "self", ".", "ordinal_values", "=", "np", ".", "full", "(", "(", "n_observations", ",", "n_actions", ",", "n_ordinals", ")", ",", "0.0", ")", "\n", "\n", "self", ".", "win_rates", "=", "[", "]", "\n", "self", ".", "average_rewards", "=", "[", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.az79nefy_OrdinalRL.q_learning.alternative_ordinal_q_discretized_agent.QAgent.init_observation_space": [[25, 32], ["numpy.linspace", "numpy.linspace", "numpy.linspace", "numpy.linspace"], "methods", ["None"], ["", "@", "staticmethod", "\n", "def", "init_observation_space", "(", ")", ":", "\n", "        ", "cart_pos_space", "=", "np", ".", "linspace", "(", "-", "2.4", ",", "2.4", ",", "10", ")", "\n", "cart_vel_space", "=", "np", ".", "linspace", "(", "-", "4", ",", "4", ",", "10", ")", "\n", "pole_theta_space", "=", "np", ".", "linspace", "(", "-", "0.20943951", ",", "0.20943951", ",", "10", ")", "\n", "pole_theta_vel_space", "=", "np", ".", "linspace", "(", "-", "4", ",", "4", ",", "10", ")", "\n", "return", "[", "cart_pos_space", ",", "cart_vel_space", ",", "pole_theta_space", ",", "pole_theta_vel_space", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.az79nefy_OrdinalRL.q_learning.alternative_ordinal_q_discretized_agent.QAgent.build_obs_dict": [[33, 44], ["list", "range", "itertools.product", "len"], "methods", ["None"], ["", "@", "staticmethod", "\n", "def", "build_obs_dict", "(", "observation_space", ")", ":", "\n", "# List of all possible discrete observations", "\n", "        ", "observation_range", "=", "[", "range", "(", "len", "(", "i", ")", "+", "1", ")", "for", "i", "in", "observation_space", "]", "\n", "# Dictionary that maps discretized observations to array indices", "\n", "observation_to_index", "=", "{", "}", "\n", "index_counter", "=", "0", "\n", "for", "observation", "in", "list", "(", "itertools", ".", "product", "(", "*", "observation_range", ")", ")", ":", "\n", "            ", "observation_to_index", "[", "observation", "]", "=", "index_counter", "\n", "index_counter", "+=", "1", "\n", "", "return", "observation_to_index", "\n", "\n"]], "home.repos.pwc.inspect_result.az79nefy_OrdinalRL.q_learning.alternative_ordinal_q_discretized_agent.QAgent.update": [[45, 49], ["alternative_ordinal_q_discretized_agent.QAgent.reward_to_ordinal", "alternative_ordinal_q_discretized_agent.QAgent.update_ordinal_values"], "methods", ["home.repos.pwc.inspect_result.az79nefy_OrdinalRL.dqn.ordinal_dqn_discretized_agent.DQNAgent.reward_to_ordinal", "home.repos.pwc.inspect_result.az79nefy_OrdinalRL.q_learning.alternative_ordinal_q_discretized_agent.QAgent.update_ordinal_values"], ["", "def", "update", "(", "self", ",", "prev_obs", ",", "prev_act", ",", "obs", ",", "reward", ",", "episode_reward", ",", "done", ")", ":", "\n", "        ", "ordinal", "=", "self", ".", "reward_to_ordinal", "(", "reward", ",", "episode_reward", ",", "done", ")", "\n", "# update ordinal_values with received ordinal", "\n", "self", ".", "update_ordinal_values", "(", "prev_obs", ",", "prev_act", ",", "obs", ",", "ordinal", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.az79nefy_OrdinalRL.q_learning.alternative_ordinal_q_discretized_agent.QAgent.update_ordinal_values": [[51, 60], ["alternative_ordinal_q_discretized_agent.QAgent.get_greedy_action", "range"], "methods", ["home.repos.pwc.inspect_result.az79nefy_OrdinalRL.dqn.atari_dqn_single_net_agent.DQNAgent.get_greedy_action"], ["", "def", "update_ordinal_values", "(", "self", ",", "prev_obs", ",", "prev_act", ",", "obs", ",", "ordinal", ")", ":", "\n", "        ", "greedy_action", "=", "self", ".", "get_greedy_action", "(", "obs", ")", "\n", "# reduce old data weight", "\n", "for", "i", "in", "range", "(", "self", ".", "n_ordinals", ")", ":", "\n", "            ", "self", ".", "ordinal_values", "[", "prev_obs", ",", "prev_act", ",", "i", "]", "*=", "(", "1", "-", "self", ".", "alpha", ")", "\n", "self", ".", "ordinal_values", "[", "prev_obs", ",", "prev_act", ",", "i", "]", "+=", "self", ".", "alpha", "*", "(", "self", ".", "gamma", "*", "self", ".", "ordinal_values", "[", "obs", ",", "greedy_action", ",", "i", "]", ")", "\n", "\n", "# add new data point", "\n", "", "self", ".", "ordinal_values", "[", "prev_obs", ",", "prev_act", ",", "ordinal", "]", "+=", "self", ".", "alpha", "\n", "\n"]], "home.repos.pwc.inspect_result.az79nefy_OrdinalRL.q_learning.alternative_ordinal_q_discretized_agent.QAgent.compute_borda_counts": [[62, 80], ["numpy.zeros", "range", "range", "reversed", "borda_counts.append"], "methods", ["None"], ["", "def", "compute_borda_counts", "(", "self", ",", "obs", ")", ":", "\n", "# sum up all ordinal values per action for given observation", "\n", "        ", "ordinal_value_sum_per_action", "=", "np", ".", "zeros", "(", "self", ".", "n_actions", ")", "\n", "for", "action_a", "in", "range", "(", "self", ".", "n_actions", ")", ":", "\n", "            ", "for", "ordinal_value", "in", "self", ".", "ordinal_values", "[", "obs", ",", "action_a", "]", ":", "\n", "                ", "ordinal_value_sum_per_action", "[", "action_a", "]", "+=", "ordinal_value", "\n", "\n", "", "", "borda_counts", "=", "[", "]", "\n", "for", "action_a", "in", "range", "(", "self", ".", "n_actions", ")", ":", "\n", "            ", "action_score", "=", "0", "\n", "ordinal_values", "=", "self", ".", "ordinal_values", "[", "obs", ",", "action_a", "]", "\n", "ordinal_worth", "=", "1.0", "\n", "for", "ordinal_value", "in", "reversed", "(", "ordinal_values", ")", ":", "\n", "                ", "ordinal_probability", "=", "ordinal_value", "/", "ordinal_value_sum_per_action", "[", "action_a", "]", "\n", "action_score", "+=", "ordinal_probability", "*", "ordinal_worth", "\n", "ordinal_worth", "=", "ordinal_worth", "/", "2", "\n", "", "borda_counts", ".", "append", "(", "action_score", ")", "\n", "", "return", "borda_counts", "\n", "\n"]], "home.repos.pwc.inspect_result.az79nefy_OrdinalRL.q_learning.alternative_ordinal_q_discretized_agent.QAgent.compute_borda_scores": [[82, 132], ["numpy.zeros", "range", "numpy.count_nonzero", "range", "borda_scores.append", "borda_scores.append", "range", "borda_scores.append", "range"], "methods", ["None"], ["", "def", "compute_borda_scores", "(", "self", ",", "obs", ")", ":", "\n", "# sum up all ordinal values per action for given observation", "\n", "        ", "ordinal_value_sum_per_action", "=", "np", ".", "zeros", "(", "self", ".", "n_actions", ")", "\n", "for", "action_a", "in", "range", "(", "self", ".", "n_actions", ")", ":", "\n", "            ", "for", "ordinal_value", "in", "self", ".", "ordinal_values", "[", "obs", ",", "action_a", "]", ":", "\n", "                ", "ordinal_value_sum_per_action", "[", "action_a", "]", "+=", "ordinal_value", "\n", "\n", "# count actions whose ordinal value sum is not zero (no comparision possible for actions without ordinal_value)", "\n", "", "", "non_zero_action_count", "=", "np", ".", "count_nonzero", "(", "ordinal_value_sum_per_action", ")", "\n", "actions_to_compare_count", "=", "non_zero_action_count", "-", "1", "\n", "\n", "borda_scores", "=", "[", "]", "\n", "# compute borda_values for action_a (probability that action_a wins against any other action)", "\n", "for", "action_a", "in", "range", "(", "self", ".", "n_actions", ")", ":", "\n", "# if action has not yet recorded any ordinal values, action has to be played (set borda_value to 1.0)", "\n", "            ", "if", "ordinal_value_sum_per_action", "[", "action_a", "]", "==", "0", ":", "\n", "                ", "borda_scores", ".", "append", "(", "1.0", ")", "\n", "continue", "\n", "\n", "", "if", "actions_to_compare_count", "<", "1", ":", "\n", "# set lower than 1.0 (borda_value for zero_actions is 1.0)", "\n", "                ", "borda_scores", ".", "append", "(", "0.5", ")", "\n", "", "else", ":", "\n", "# over all actions: sum up the probabilities that action_a wins against the given action", "\n", "                ", "winning_probability_a_sum", "=", "0", "\n", "# compare action_a to all other actions", "\n", "for", "action_b", "in", "range", "(", "self", ".", "n_actions", ")", ":", "\n", "                    ", "if", "action_a", "==", "action_b", ":", "\n", "                        ", "continue", "\n", "# not comparable if action_b has no ordinal_values", "\n", "", "if", "ordinal_value_sum_per_action", "[", "action_b", "]", "==", "0", ":", "\n", "                        ", "continue", "\n", "", "else", ":", "\n", "# probability that action_a wins against action_b", "\n", "                        ", "winning_probability_a", "=", "0", "\n", "# running ordinal probability that action_b is worse than current investigated ordinal", "\n", "worse_probability_b", "=", "0", "\n", "for", "ordinal_count", "in", "range", "(", "self", ".", "n_ordinals", ")", ":", "\n", "                            ", "ordinal_probability_a", "=", "self", ".", "ordinal_values", "[", "obs", ",", "action_a", ",", "ordinal_count", "]", "/", "ordinal_value_sum_per_action", "[", "action_a", "]", "\n", "# ordinal_probability_b is also the tie probability", "\n", "ordinal_probability_b", "=", "(", "self", ".", "ordinal_values", "[", "obs", ",", "action_b", ",", "ordinal_count", "]", "/", "\n", "ordinal_value_sum_per_action", "[", "action_b", "]", ")", "\n", "winning_probability_a", "+=", "ordinal_probability_a", "*", "(", "worse_probability_b", "+", "ordinal_probability_b", "/", "2.0", ")", "\n", "worse_probability_b", "+=", "ordinal_probability_b", "\n", "", "winning_probability_a_sum", "+=", "winning_probability_a", "\n", "# normalize summed up probabilities with number of actions that have been compared", "\n", "", "", "borda_scores", ".", "append", "(", "winning_probability_a_sum", "/", "actions_to_compare_count", ")", "\n", "", "", "return", "borda_scores", "\n", "\n"]], "home.repos.pwc.inspect_result.az79nefy_OrdinalRL.q_learning.alternative_ordinal_q_discretized_agent.QAgent.compute_copeland_win_rate": [[134, 187], ["numpy.zeros", "range", "numpy.count_nonzero", "range", "win_rates.append", "win_rates.append", "range", "win_rates.append", "range"], "methods", ["None"], ["", "def", "compute_copeland_win_rate", "(", "self", ",", "obs", ")", ":", "\n", "# sum up all ordinal values per action for given observation", "\n", "        ", "ordinal_value_sum_per_action", "=", "np", ".", "zeros", "(", "self", ".", "n_actions", ")", "\n", "for", "action_a", "in", "range", "(", "self", ".", "n_actions", ")", ":", "\n", "            ", "for", "ordinal_value", "in", "self", ".", "ordinal_values", "[", "obs", ",", "action_a", "]", ":", "\n", "                ", "ordinal_value_sum_per_action", "[", "action_a", "]", "+=", "ordinal_value", "\n", "\n", "# count actions whose ordinal value sum is not zero (no comparision possible for actions without ordinal_value)", "\n", "", "", "non_zero_action_count", "=", "np", ".", "count_nonzero", "(", "ordinal_value_sum_per_action", ")", "\n", "actions_to_compare_count", "=", "non_zero_action_count", "-", "1", "\n", "\n", "win_rates", "=", "[", "]", "\n", "# compute win_rate for action_a", "\n", "for", "action_a", "in", "range", "(", "self", ".", "n_actions", ")", ":", "\n", "# if action has not yet recorded any ordinal values, action has to be played (set win_rate to 1.0)", "\n", "            ", "if", "ordinal_value_sum_per_action", "[", "action_a", "]", "==", "0", ":", "\n", "                ", "win_rates", ".", "append", "(", "1.0", ")", "\n", "continue", "\n", "\n", "", "if", "actions_to_compare_count", "<", "1", ":", "\n", "# set lower than 1.0 (win_rate for zero_actions is 1.0)", "\n", "                ", "win_rates", ".", "append", "(", "0.5", ")", "\n", "", "else", ":", "\n", "# over all actions: count the number of times that action_a wins against the given action", "\n", "                ", "win_counter_a", "=", "0", "\n", "# compare action_a to all other actions", "\n", "for", "action_b", "in", "range", "(", "self", ".", "n_actions", ")", ":", "\n", "                    ", "if", "action_a", "==", "action_b", ":", "\n", "                        ", "continue", "\n", "# not comparable if action_b has no ordinal_values", "\n", "", "if", "ordinal_value_sum_per_action", "[", "action_b", "]", "==", "0", ":", "\n", "                        ", "continue", "\n", "", "else", ":", "\n", "# probability that action_a wins against action_b", "\n", "                        ", "winning_probability_a", "=", "0", "\n", "# running ordinal probability that action_b is worse than current investigated ordinal", "\n", "worse_probability_b", "=", "0", "\n", "for", "ordinal_count", "in", "range", "(", "self", ".", "n_ordinals", ")", ":", "\n", "                            ", "ordinal_probability_a", "=", "self", ".", "ordinal_values", "[", "obs", ",", "action_a", ",", "ordinal_count", "]", "/", "ordinal_value_sum_per_action", "[", "action_a", "]", "\n", "# ordinal_probability_b is also the tie probability", "\n", "ordinal_probability_b", "=", "(", "self", ".", "ordinal_values", "[", "obs", ",", "action_b", ",", "ordinal_count", "]", "/", "\n", "ordinal_value_sum_per_action", "[", "action_b", "]", ")", "\n", "winning_probability_a", "+=", "ordinal_probability_a", "*", "(", "worse_probability_b", "+", "ordinal_probability_b", "/", "2.0", ")", "\n", "worse_probability_b", "+=", "ordinal_probability_b", "\n", "", "if", "winning_probability_a", ">", "0.5", ":", "\n", "                            ", "win_counter_a", "+=", "1.0", "\n", "", "elif", "winning_probability_a", "==", "0.5", ":", "\n", "                            ", "win_counter_a", "+=", "0.5", "\n", "# normalize win_counter with number of actions that have been compared to compute win_rate", "\n", "", "", "", "win_rates", ".", "append", "(", "win_counter_a", "/", "actions_to_compare_count", ")", "\n", "", "", "return", "win_rates", "\n", "\n"]], "home.repos.pwc.inspect_result.az79nefy_OrdinalRL.q_learning.alternative_ordinal_q_discretized_agent.QAgent.compute_plurality_runoff_borda_winner": [[188, 247], ["numpy.zeros", "range", "numpy.count_nonzero", "list", "range", "range", "list.pop", "borda_scores.append", "borda_scores.append", "borda_scores.append", "len", "list.pop", "borda_scores.pop", "numpy.argmin", "numpy.argmin", "numpy.argmin", "range"], "methods", ["None"], ["", "def", "compute_plurality_runoff_borda_winner", "(", "self", ",", "obs", ")", ":", "\n", "# sum up all ordinal values per action for given observation", "\n", "        ", "ordinal_value_sum_per_action", "=", "np", ".", "zeros", "(", "self", ".", "n_actions", ")", "\n", "for", "action_a", "in", "range", "(", "self", ".", "n_actions", ")", ":", "\n", "            ", "for", "ordinal_value", "in", "self", ".", "ordinal_values", "[", "obs", ",", "action_a", "]", ":", "\n", "                ", "ordinal_value_sum_per_action", "[", "action_a", "]", "+=", "ordinal_value", "\n", "\n", "# count actions whose ordinal value sum is not zero (no comparision possible for actions without ordinal_value)", "\n", "", "", "non_zero_action_count", "=", "np", ".", "count_nonzero", "(", "ordinal_value_sum_per_action", ")", "\n", "actions_to_compare_count", "=", "non_zero_action_count", "-", "1", "\n", "\n", "possible_actions", "=", "list", "(", "range", "(", "self", ".", "n_actions", ")", ")", "\n", "# in first iteration all actions are compared; in the second iteration the two best actions are compared", "\n", "for", "i", "in", "range", "(", "2", ")", ":", "\n", "            ", "borda_scores", "=", "[", "]", "\n", "# compute borda_values for action_a (probability that action_a wins against any other action)", "\n", "for", "action_a", "in", "possible_actions", ":", "\n", "# if action has not yet recorded any ordinal values, action has to be played (set borda_value to 1.0)", "\n", "                ", "if", "ordinal_value_sum_per_action", "[", "action_a", "]", "==", "0", ":", "\n", "                    ", "borda_scores", ".", "append", "(", "1.0", ")", "\n", "continue", "\n", "\n", "", "if", "actions_to_compare_count", "<", "1", ":", "\n", "# set lower than 1.0 (borda_value for zero_actions is 1.0)", "\n", "                    ", "borda_scores", ".", "append", "(", "0.5", ")", "\n", "", "else", ":", "\n", "# over all actions: sum up the probabilities that action_a wins against the given action", "\n", "                    ", "winning_probability_a_sum", "=", "0", "\n", "# compare action_a to all other actions", "\n", "for", "action_b", "in", "possible_actions", ":", "\n", "                        ", "if", "action_a", "==", "action_b", ":", "\n", "                            ", "continue", "\n", "# not comparable if action_b has no ordinal_values", "\n", "", "if", "ordinal_value_sum_per_action", "[", "action_b", "]", "==", "0", ":", "\n", "                            ", "continue", "\n", "", "else", ":", "\n", "# probability that action_a wins against action_b", "\n", "                            ", "winning_probability_a", "=", "0", "\n", "# running ordinal probability that action_b is worse than current investigated ordinal", "\n", "worse_probability_b", "=", "0", "\n", "for", "ordinal_count", "in", "range", "(", "self", ".", "n_ordinals", ")", ":", "\n", "                                ", "ordinal_probability_a", "=", "self", ".", "ordinal_values", "[", "obs", ",", "action_a", ",", "ordinal_count", "]", "/", "ordinal_value_sum_per_action", "[", "action_a", "]", "\n", "# ordinal_probability_b is also the tie probability", "\n", "ordinal_probability_b", "=", "(", "self", ".", "ordinal_values", "[", "obs", ",", "action_b", ",", "ordinal_count", "]", "/", "\n", "ordinal_value_sum_per_action", "[", "action_b", "]", ")", "\n", "winning_probability_a", "+=", "ordinal_probability_a", "*", "(", "worse_probability_b", "+", "ordinal_probability_b", "/", "2.0", ")", "\n", "worse_probability_b", "+=", "ordinal_probability_b", "\n", "", "winning_probability_a_sum", "+=", "winning_probability_a", "\n", "# normalize summed up probabilities with number of actions that have been compared", "\n", "", "", "borda_scores", ".", "append", "(", "winning_probability_a_sum", "/", "actions_to_compare_count", ")", "\n", "", "", "if", "i", "==", "0", ":", "\n", "                ", "while", "len", "(", "possible_actions", ")", ">", "2", ":", "\n", "                    ", "possible_actions", ".", "pop", "(", "np", ".", "argmin", "(", "borda_scores", ")", ")", "\n", "borda_scores", ".", "pop", "(", "np", ".", "argmin", "(", "borda_scores", ")", ")", "\n", "", "", "if", "i", "==", "1", ":", "\n", "                ", "possible_actions", ".", "pop", "(", "np", ".", "argmin", "(", "borda_scores", ")", ")", "\n", "", "", "return", "possible_actions", "[", "0", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.az79nefy_OrdinalRL.q_learning.alternative_ordinal_q_discretized_agent.QAgent.compute_instant_runoff_borda_winner": [[248, 302], ["numpy.zeros", "range", "numpy.count_nonzero", "list", "range", "range", "list.pop", "numpy.argmin", "borda_scores.append", "borda_scores.append", "borda_scores.append", "range"], "methods", ["None"], ["", "def", "compute_instant_runoff_borda_winner", "(", "self", ",", "obs", ")", ":", "\n", "# sum up all ordinal values per action for given observation", "\n", "        ", "ordinal_value_sum_per_action", "=", "np", ".", "zeros", "(", "self", ".", "n_actions", ")", "\n", "for", "action_a", "in", "range", "(", "self", ".", "n_actions", ")", ":", "\n", "            ", "for", "ordinal_value", "in", "self", ".", "ordinal_values", "[", "obs", ",", "action_a", "]", ":", "\n", "                ", "ordinal_value_sum_per_action", "[", "action_a", "]", "+=", "ordinal_value", "\n", "\n", "# count actions whose ordinal value sum is not zero (no comparision possible for actions without ordinal_value)", "\n", "", "", "non_zero_action_count", "=", "np", ".", "count_nonzero", "(", "ordinal_value_sum_per_action", ")", "\n", "actions_to_compare_count", "=", "non_zero_action_count", "-", "1", "\n", "\n", "possible_actions", "=", "list", "(", "range", "(", "self", ".", "n_actions", ")", ")", "\n", "# in each iteration the worst action according to borda_scores is removed and borda_scores are recalculated", "\n", "for", "_", "in", "range", "(", "self", ".", "n_actions", "-", "1", ")", ":", "\n", "            ", "borda_scores", "=", "[", "]", "\n", "# compute borda_values for action_a (probability that action_a wins against any other action)", "\n", "for", "action_a", "in", "possible_actions", ":", "\n", "# if action has not yet recorded any ordinal values, action has to be played (set borda_value to 1.0)", "\n", "                ", "if", "ordinal_value_sum_per_action", "[", "action_a", "]", "==", "0", ":", "\n", "                    ", "borda_scores", ".", "append", "(", "1.0", ")", "\n", "continue", "\n", "\n", "", "if", "actions_to_compare_count", "<", "1", ":", "\n", "# set lower than 1.0 (borda_value for zero_actions is 1.0)", "\n", "                    ", "borda_scores", ".", "append", "(", "0.5", ")", "\n", "", "else", ":", "\n", "# over all actions: sum up the probabilities that action_a wins against the given action", "\n", "                    ", "winning_probability_a_sum", "=", "0", "\n", "# compare action_a to all other actions", "\n", "for", "action_b", "in", "possible_actions", ":", "\n", "                        ", "if", "action_a", "==", "action_b", ":", "\n", "                            ", "continue", "\n", "# not comparable if action_b has no ordinal_values", "\n", "", "if", "ordinal_value_sum_per_action", "[", "action_b", "]", "==", "0", ":", "\n", "                            ", "continue", "\n", "", "else", ":", "\n", "# probability that action_a wins against action_b", "\n", "                            ", "winning_probability_a", "=", "0", "\n", "# running ordinal probability that action_b is worse than current investigated ordinal", "\n", "worse_probability_b", "=", "0", "\n", "for", "ordinal_count", "in", "range", "(", "self", ".", "n_ordinals", ")", ":", "\n", "                                ", "ordinal_probability_a", "=", "self", ".", "ordinal_values", "[", "obs", ",", "action_a", ",", "ordinal_count", "]", "/", "ordinal_value_sum_per_action", "[", "action_a", "]", "\n", "# ordinal_probability_b is also the tie probability", "\n", "ordinal_probability_b", "=", "(", "self", ".", "ordinal_values", "[", "obs", ",", "action_b", ",", "ordinal_count", "]", "/", "\n", "ordinal_value_sum_per_action", "[", "action_b", "]", ")", "\n", "winning_probability_a", "+=", "ordinal_probability_a", "*", "(", "worse_probability_b", "+", "ordinal_probability_b", "/", "2.0", ")", "\n", "worse_probability_b", "+=", "ordinal_probability_b", "\n", "", "winning_probability_a_sum", "+=", "winning_probability_a", "\n", "# normalize summed up probabilities with number of actions that have been compared", "\n", "", "", "borda_scores", ".", "append", "(", "winning_probability_a_sum", "/", "actions_to_compare_count", ")", "\n", "", "", "possible_actions", ".", "pop", "(", "np", ".", "argmin", "(", "borda_scores", ")", ")", "\n", "", "return", "possible_actions", "[", "0", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.az79nefy_OrdinalRL.q_learning.alternative_ordinal_q_discretized_agent.QAgent.get_greedy_action": [[303, 307], ["numpy.argmax", "alternative_ordinal_q_discretized_agent.QAgent.compute_copeland_win_rate"], "methods", ["home.repos.pwc.inspect_result.az79nefy_OrdinalRL.q_learning.alternative_ordinal_q_discretized_agent.QAgent.compute_copeland_win_rate"], ["", "def", "get_greedy_action", "(", "self", ",", "obs", ")", ":", "\n", "# return np.argmax(self.compute_borda_counts(obs))", "\n", "# return np.argmax(self.compute_borda_scores(obs))", "\n", "        ", "return", "np", ".", "argmax", "(", "self", ".", "compute_copeland_win_rate", "(", "obs", ")", ")", "\n", "# return self.compute_plurality_runoff_borda_winner(obs)", "\n"]], "home.repos.pwc.inspect_result.az79nefy_OrdinalRL.q_learning.alternative_ordinal_q_discretized_agent.QAgent.choose_action": [[311, 319], ["alternative_ordinal_q_discretized_agent.QAgent.get_greedy_action", "random.randrange", "random.random"], "methods", ["home.repos.pwc.inspect_result.az79nefy_OrdinalRL.dqn.atari_dqn_single_net_agent.DQNAgent.get_greedy_action"], ["", "def", "choose_action", "(", "self", ",", "obs", ",", "greedy", ")", ":", "\n", "        ", "greedy_action", "=", "self", ".", "get_greedy_action", "(", "obs", ")", "\n", "# choose random action with probability epsilon", "\n", "if", "not", "greedy", "and", "random", ".", "random", "(", ")", "<", "self", ".", "epsilon", ":", "\n", "            ", "return", "random", ".", "randrange", "(", "self", ".", "n_actions", ")", "\n", "# greedy action is chosen with probability (1 - epsilon)", "\n", "", "else", ":", "\n", "            ", "return", "greedy_action", "\n", "\n"]], "home.repos.pwc.inspect_result.az79nefy_OrdinalRL.q_learning.alternative_ordinal_q_discretized_agent.QAgent.end_episode": [[320, 323], ["None"], "methods", ["None"], ["", "", "def", "end_episode", "(", "self", ",", "n_episodes", ")", ":", "\n", "# gradually reduce epsilon after every done episode", "\n", "        ", "self", ".", "epsilon", "=", "self", ".", "epsilon", "-", "2", "/", "n_episodes", "if", "self", ".", "epsilon", ">", "self", ".", "epsilon_min", "else", "self", ".", "epsilon_min", "\n", "\n"]], "home.repos.pwc.inspect_result.az79nefy_OrdinalRL.q_learning.alternative_ordinal_q_discretized_agent.QAgent.preprocess_observation": [[324, 329], ["range", "len", "discrete_observation.append", "int", "tuple", "numpy.digitize"], "methods", ["None"], ["", "def", "preprocess_observation", "(", "self", ",", "obs", ")", ":", "\n", "        ", "discrete_observation", "=", "[", "]", "\n", "for", "obs_idx", "in", "range", "(", "len", "(", "obs", ")", ")", ":", "\n", "            ", "discrete_observation", ".", "append", "(", "int", "(", "np", ".", "digitize", "(", "obs", "[", "obs_idx", "]", ",", "self", ".", "observation_space", "[", "obs_idx", "]", ")", ")", ")", "\n", "", "return", "self", ".", "observation_to_index", "[", "tuple", "(", "discrete_observation", ")", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.az79nefy_OrdinalRL.q_learning.alternative_ordinal_q_discretized_agent.QAgent.reward_to_ordinal": [[331, 336], ["alternative_ordinal_q_discretized_agent.QAgent.check_win_condition"], "methods", ["home.repos.pwc.inspect_result.az79nefy_OrdinalRL.dqn.atari_dqn_single_net_agent.DQNAgent.check_win_condition"], ["", "def", "reward_to_ordinal", "(", "self", ",", "reward", ",", "episode_reward", ",", "done", ")", ":", "\n", "        ", "if", "done", "and", "not", "self", ".", "check_win_condition", "(", "reward", ",", "episode_reward", ",", "done", ")", ":", "\n", "            ", "return", "0", "\n", "", "else", ":", "\n", "            ", "return", "1", "\n", "\n"]], "home.repos.pwc.inspect_result.az79nefy_OrdinalRL.q_learning.alternative_ordinal_q_discretized_agent.QAgent.check_win_condition": [[338, 344], ["None"], "methods", ["None"], ["", "", "@", "staticmethod", "\n", "def", "check_win_condition", "(", "reward", ",", "episode_reward", ",", "done", ")", ":", "\n", "        ", "if", "done", "and", "episode_reward", "==", "200", ":", "\n", "            ", "return", "True", "\n", "", "else", ":", "\n", "            ", "return", "False", "\n", "\n"]], "home.repos.pwc.inspect_result.az79nefy_OrdinalRL.q_learning.alternative_ordinal_q_discretized_agent.QAgent.evaluate": [[345, 353], ["round", "round", "alternative_ordinal_q_discretized_agent.QAgent.average_rewards.append", "alternative_ordinal_q_discretized_agent.QAgent.win_rates.append", "print", "sum", "len", "sum", "len"], "methods", ["None"], ["", "", "def", "evaluate", "(", "self", ",", "i_episode", ",", "episode_rewards", ",", "episode_wins", ")", ":", "\n", "# compute average episode reward and win rate over last episodes", "\n", "        ", "average_reward", "=", "round", "(", "sum", "(", "episode_rewards", ")", "/", "len", "(", "episode_rewards", ")", ",", "2", ")", "\n", "win_rate", "=", "round", "(", "sum", "(", "episode_wins", ")", "/", "len", "(", "episode_wins", ")", ",", "2", ")", "\n", "# store average episode reward and win rate over last episodes for plotting purposes", "\n", "self", ".", "average_rewards", ".", "append", "(", "average_reward", ")", "\n", "self", ".", "win_rates", ".", "append", "(", "win_rate", ")", "\n", "print", "(", "\"{}\\t{}\\t{}\"", ".", "format", "(", "i_episode", "+", "1", ",", "average_reward", ",", "win_rate", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.az79nefy_OrdinalRL.q_learning.alternative_ordinal_q_discretized_agent.QAgent.plot": [[355, 369], ["matplotlib.figure", "matplotlib.plot", "matplotlib.xlabel", "matplotlib.ylabel", "matplotlib.figure", "matplotlib.plot", "matplotlib.xlabel", "matplotlib.ylabel", "matplotlib.show", "list", "list", "range", "range"], "methods", ["home.repos.pwc.inspect_result.az79nefy_OrdinalRL.dqn.atari_dqn_single_net_agent.DQNAgent.plot", "home.repos.pwc.inspect_result.az79nefy_OrdinalRL.dqn.atari_dqn_single_net_agent.DQNAgent.plot"], ["", "def", "plot", "(", "self", ",", "n_episodes", ",", "step_size", ")", ":", "\n", "# plot win rate", "\n", "        ", "plt", ".", "figure", "(", ")", "\n", "plt", ".", "plot", "(", "list", "(", "range", "(", "step_size", ",", "n_episodes", "+", "step_size", ",", "step_size", ")", ")", ",", "self", ".", "win_rates", ")", "\n", "plt", ".", "xlabel", "(", "'Number of episodes'", ")", "\n", "plt", ".", "ylabel", "(", "'Win rate'", ")", "\n", "\n", "# plot average score", "\n", "plt", ".", "figure", "(", ")", "\n", "plt", ".", "plot", "(", "list", "(", "range", "(", "step_size", ",", "n_episodes", "+", "step_size", ",", "step_size", ")", ")", ",", "self", ".", "average_rewards", ")", "\n", "plt", ".", "xlabel", "(", "'Number of episodes'", ")", "\n", "plt", ".", "ylabel", "(", "'Average score'", ")", "\n", "\n", "plt", ".", "show", "(", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.az79nefy_OrdinalRL.dqn.dqn_multi_net_agent.DQNAgent.__init__": [[11, 29], ["collections.deque", "dqn_multi_net_agent.DQNAgent.build_model", "dqn_multi_net_agent.DQNAgent.build_model", "range", "range"], "methods", ["home.repos.pwc.inspect_result.az79nefy_OrdinalRL.dqn.atari_dqn_single_net_agent.DQNAgent.build_model", "home.repos.pwc.inspect_result.az79nefy_OrdinalRL.dqn.atari_dqn_single_net_agent.DQNAgent.build_model"], ["    ", "def", "__init__", "(", "self", ",", "alpha", ",", "gamma", ",", "epsilon", ",", "epsilon_min", ",", "n_actions", ",", "n_ordinals", ",", "observation_dim", ",", "batch_size", ",", "memory_len", ",", "replace_target_iter", ")", ":", "\n", "        ", "self", ".", "alpha", "=", "alpha", "\n", "self", ".", "gamma", "=", "gamma", "\n", "self", ".", "epsilon", "=", "epsilon", "\n", "self", ".", "epsilon_min", "=", "epsilon_min", "\n", "self", ".", "n_actions", "=", "n_actions", "\n", "self", ".", "n_inputs", "=", "observation_dim", "\n", "\n", "self", ".", "batch_size", "=", "batch_size", "\n", "self", ".", "memory", "=", "deque", "(", "maxlen", "=", "memory_len", ")", "\n", "# creation of a neural net for every action", "\n", "self", ".", "eval_action_nets", "=", "[", "self", ".", "build_model", "(", "self", ".", "n_inputs", ")", "for", "_", "in", "range", "(", "n_actions", ")", "]", "\n", "self", ".", "target_action_nets", "=", "[", "self", ".", "build_model", "(", "self", ".", "n_inputs", ")", "for", "_", "in", "range", "(", "n_actions", ")", "]", "\n", "self", ".", "replace_target_iter", "=", "replace_target_iter", "\n", "\n", "self", ".", "replay_counter", "=", "0", "\n", "self", ".", "win_rates", "=", "[", "]", "\n", "self", ".", "average_rewards", "=", "[", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.az79nefy_OrdinalRL.dqn.dqn_multi_net_agent.DQNAgent.build_model": [[31, 38], ["keras.models.Sequential", "keras.models.Sequential.add", "keras.models.Sequential.add", "keras.models.Sequential.add", "keras.models.Sequential.compile", "keras.layers.Dense", "keras.layers.Dense", "keras.layers.Dense", "keras.optimizers.Adam"], "methods", ["None"], ["", "def", "build_model", "(", "self", ",", "n_inputs", ")", ":", "\n", "        ", "neural_net", "=", "Sequential", "(", ")", "\n", "neural_net", ".", "add", "(", "Dense", "(", "6", ",", "input_dim", "=", "n_inputs", ",", "activation", "=", "'relu'", ")", ")", "\n", "neural_net", ".", "add", "(", "Dense", "(", "6", ",", "activation", "=", "'relu'", ")", ")", "\n", "neural_net", ".", "add", "(", "Dense", "(", "1", ",", "activation", "=", "'linear'", ")", ")", "\n", "neural_net", ".", "compile", "(", "loss", "=", "'mse'", ",", "optimizer", "=", "Adam", "(", "lr", "=", "self", ".", "alpha", ")", ")", "\n", "return", "neural_net", "\n", "\n"]], "home.repos.pwc.inspect_result.az79nefy_OrdinalRL.dqn.dqn_multi_net_agent.DQNAgent.update": [[39, 44], ["dqn_multi_net_agent.DQNAgent.remap_reward", "dqn_multi_net_agent.DQNAgent.remember", "len", "dqn_multi_net_agent.DQNAgent.replay"], "methods", ["home.repos.pwc.inspect_result.az79nefy_OrdinalRL.dqn.dqn_single_net_agent.DQNAgent.remap_reward", "home.repos.pwc.inspect_result.az79nefy_OrdinalRL.dqn.atari_dqn_single_net_agent.DQNAgent.remember", "home.repos.pwc.inspect_result.az79nefy_OrdinalRL.dqn.atari_dqn_single_net_agent.DQNAgent.replay"], ["", "def", "update", "(", "self", ",", "prev_obs", ",", "prev_act", ",", "obs", ",", "reward", ",", "episode_reward", ",", "done", ")", ":", "\n", "        ", "reward", "=", "self", ".", "remap_reward", "(", "reward", ",", "episode_reward", ",", "done", ")", "\n", "self", ".", "remember", "(", "prev_obs", ",", "prev_act", ",", "obs", ",", "reward", ",", "done", ")", "\n", "if", "len", "(", "self", ".", "memory", ")", ">", "self", ".", "batch_size", ":", "\n", "            ", "self", ".", "replay", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.az79nefy_OrdinalRL.dqn.dqn_multi_net_agent.DQNAgent.remember": [[45, 47], ["dqn_multi_net_agent.DQNAgent.memory.append"], "methods", ["None"], ["", "", "def", "remember", "(", "self", ",", "prev_obs", ",", "prev_act", ",", "obs", ",", "rew", ",", "d", ")", ":", "\n", "        ", "self", ".", "memory", ".", "append", "(", "(", "prev_obs", ",", "prev_act", ",", "obs", ",", "rew", ",", "d", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.az79nefy_OrdinalRL.dqn.dqn_multi_net_agent.DQNAgent.replay": [[48, 72], ["random.sample", "range", "range", "x_batch[].append", "y_batch[].append", "dqn_multi_net_agent.DQNAgent.target_action_nets[].set_weights", "numpy.argmax", "len", "dqn_multi_net_agent.DQNAgent.eval_action_nets[].fit", "dqn_multi_net_agent.DQNAgent.eval_action_nets[].get_weights", "range", "range", "action_predictions.append", "numpy.array", "numpy.array", "len", "act_net.predict", "dqn_multi_net_agent.DQNAgent.target_action_nets[].predict"], "methods", ["None"], ["", "def", "replay", "(", "self", ")", ":", "\n", "# copy evaluation action nets to target action nets at first replay and then every 200 replay steps", "\n", "        ", "if", "self", ".", "replay_counter", "%", "self", ".", "replace_target_iter", "==", "0", ":", "\n", "            ", "for", "a", "in", "range", "(", "self", ".", "n_actions", ")", ":", "\n", "                ", "self", ".", "target_action_nets", "[", "a", "]", ".", "set_weights", "(", "self", ".", "eval_action_nets", "[", "a", "]", ".", "get_weights", "(", ")", ")", "\n", "", "", "self", ".", "replay_counter", "+=", "1", "\n", "\n", "mini_batch", "=", "random", ".", "sample", "(", "self", ".", "memory", ",", "self", ".", "batch_size", ")", "\n", "x_batch", ",", "y_batch", "=", "[", "[", "]", "for", "_", "in", "range", "(", "self", ".", "n_actions", ")", "]", ",", "[", "[", "]", "for", "_", "in", "range", "(", "self", ".", "n_actions", ")", "]", "\n", "for", "prev_obs", ",", "prev_act", ",", "obs", ",", "rew", ",", "d", "in", "mini_batch", ":", "\n", "            ", "if", "not", "d", ":", "\n", "                ", "action_predictions", "=", "[", "]", "\n", "for", "act_net", "in", "self", ".", "eval_action_nets", ":", "\n", "                    ", "action_predictions", ".", "append", "(", "act_net", ".", "predict", "(", "obs", ")", "[", "0", "]", ")", "\n", "", "best_act", "=", "np", ".", "argmax", "(", "action_predictions", ")", "\n", "target", "=", "rew", "+", "self", ".", "gamma", "*", "self", ".", "target_action_nets", "[", "best_act", "]", ".", "predict", "(", "obs", ")", "[", "0", "]", "\n", "", "else", ":", "\n", "                ", "target", "=", "rew", "\n", "# fit predicted value of previous action in previous observation to target value of max_action", "\n", "", "x_batch", "[", "prev_act", "]", ".", "append", "(", "prev_obs", "[", "0", "]", ")", "\n", "y_batch", "[", "prev_act", "]", ".", "append", "(", "target", ")", "\n", "", "for", "a", "in", "range", "(", "self", ".", "n_actions", ")", ":", "\n", "            ", "if", "len", "(", "x_batch", "[", "a", "]", ")", "!=", "0", ":", "\n", "                ", "self", ".", "eval_action_nets", "[", "a", "]", ".", "fit", "(", "np", ".", "array", "(", "x_batch", "[", "a", "]", ")", ",", "np", ".", "array", "(", "y_batch", "[", "a", "]", ")", ",", "batch_size", "=", "len", "(", "x_batch", "[", "a", "]", ")", ",", "verbose", "=", "0", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.az79nefy_OrdinalRL.dqn.dqn_multi_net_agent.DQNAgent.get_greedy_action": [[73, 78], ["numpy.argmax", "action_predictions.append", "act_net.predict"], "methods", ["None"], ["", "", "", "def", "get_greedy_action", "(", "self", ",", "obs", ")", ":", "\n", "        ", "action_predictions", "=", "[", "]", "\n", "for", "act_net", "in", "self", ".", "eval_action_nets", ":", "\n", "            ", "action_predictions", ".", "append", "(", "act_net", ".", "predict", "(", "obs", ")", "[", "0", "]", ")", "\n", "", "return", "np", ".", "argmax", "(", "action_predictions", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.az79nefy_OrdinalRL.dqn.dqn_multi_net_agent.DQNAgent.choose_action": [[80, 88], ["dqn_multi_net_agent.DQNAgent.get_greedy_action", "random.randrange", "random.random"], "methods", ["home.repos.pwc.inspect_result.az79nefy_OrdinalRL.dqn.atari_dqn_single_net_agent.DQNAgent.get_greedy_action"], ["", "def", "choose_action", "(", "self", ",", "obs", ",", "greedy", ")", ":", "\n", "        ", "greedy_action", "=", "self", ".", "get_greedy_action", "(", "obs", ")", "\n", "# choose random action with probability epsilon", "\n", "if", "not", "greedy", "and", "random", ".", "random", "(", ")", "<", "self", ".", "epsilon", ":", "\n", "            ", "return", "random", ".", "randrange", "(", "self", ".", "n_actions", ")", "\n", "# greedy action is chosen with probability (1 - epsilon)", "\n", "", "else", ":", "\n", "            ", "return", "greedy_action", "\n", "\n"]], "home.repos.pwc.inspect_result.az79nefy_OrdinalRL.dqn.dqn_multi_net_agent.DQNAgent.end_episode": [[89, 92], ["None"], "methods", ["None"], ["", "", "def", "end_episode", "(", "self", ",", "n_episodes", ")", ":", "\n", "# gradually reduce epsilon after every done episode", "\n", "        ", "self", ".", "epsilon", "=", "self", ".", "epsilon", "-", "2", "/", "n_episodes", "if", "self", ".", "epsilon", ">", "self", ".", "epsilon_min", "else", "self", ".", "epsilon_min", "\n", "\n"]], "home.repos.pwc.inspect_result.az79nefy_OrdinalRL.dqn.dqn_multi_net_agent.DQNAgent.preprocess_observation": [[93, 96], ["numpy.expand_dims"], "methods", ["None"], ["", "@", "staticmethod", "\n", "def", "preprocess_observation", "(", "obs", ")", ":", "\n", "        ", "return", "np", ".", "expand_dims", "(", "obs", ",", "axis", "=", "0", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.az79nefy_OrdinalRL.dqn.dqn_multi_net_agent.DQNAgent.remap_reward": [[98, 100], ["None"], "methods", ["None"], ["", "def", "remap_reward", "(", "self", ",", "reward", ",", "episode_reward", ",", "done", ")", ":", "\n", "        ", "return", "reward", "\n", "\n"]], "home.repos.pwc.inspect_result.az79nefy_OrdinalRL.dqn.dqn_multi_net_agent.DQNAgent.check_win_condition": [[102, 108], ["None"], "methods", ["None"], ["", "@", "staticmethod", "\n", "def", "check_win_condition", "(", "reward", ",", "episode_reward", ",", "done", ")", ":", "\n", "        ", "if", "done", "and", "reward", "==", "20", ":", "\n", "            ", "return", "True", "\n", "", "else", ":", "\n", "            ", "return", "False", "\n", "\n"]], "home.repos.pwc.inspect_result.az79nefy_OrdinalRL.dqn.dqn_multi_net_agent.DQNAgent.evaluate": [[109, 117], ["round", "round", "dqn_multi_net_agent.DQNAgent.average_rewards.append", "dqn_multi_net_agent.DQNAgent.win_rates.append", "print", "sum", "len", "sum", "len"], "methods", ["None"], ["", "", "def", "evaluate", "(", "self", ",", "i_episode", ",", "episode_rewards", ",", "episode_wins", ")", ":", "\n", "# compute average episode reward and win rate over last episodes", "\n", "        ", "average_reward", "=", "round", "(", "sum", "(", "episode_rewards", ")", "/", "len", "(", "episode_rewards", ")", ",", "2", ")", "\n", "win_rate", "=", "round", "(", "sum", "(", "episode_wins", ")", "/", "len", "(", "episode_wins", ")", ",", "2", ")", "\n", "# store average episode reward and win rate over last episodes for plotting purposes", "\n", "self", ".", "average_rewards", ".", "append", "(", "average_reward", ")", "\n", "self", ".", "win_rates", ".", "append", "(", "win_rate", ")", "\n", "print", "(", "\"{}\\t{}\\t{}\"", ".", "format", "(", "i_episode", "+", "1", ",", "average_reward", ",", "win_rate", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.az79nefy_OrdinalRL.dqn.dqn_multi_net_agent.DQNAgent.plot": [[119, 133], ["matplotlib.figure", "matplotlib.plot", "matplotlib.xlabel", "matplotlib.ylabel", "matplotlib.figure", "matplotlib.plot", "matplotlib.xlabel", "matplotlib.ylabel", "matplotlib.show", "list", "list", "range", "range"], "methods", ["home.repos.pwc.inspect_result.az79nefy_OrdinalRL.dqn.atari_dqn_single_net_agent.DQNAgent.plot", "home.repos.pwc.inspect_result.az79nefy_OrdinalRL.dqn.atari_dqn_single_net_agent.DQNAgent.plot"], ["", "def", "plot", "(", "self", ",", "n_episodes", ",", "step_size", ")", ":", "\n", "# plot win rate", "\n", "        ", "plt", ".", "figure", "(", ")", "\n", "plt", ".", "plot", "(", "list", "(", "range", "(", "step_size", ",", "n_episodes", "+", "step_size", ",", "step_size", ")", ")", ",", "self", ".", "win_rates", ")", "\n", "plt", ".", "xlabel", "(", "'Number of episodes'", ")", "\n", "plt", ".", "ylabel", "(", "'Win rate'", ")", "\n", "\n", "# plot average score", "\n", "plt", ".", "figure", "(", ")", "\n", "plt", ".", "plot", "(", "list", "(", "range", "(", "step_size", ",", "n_episodes", "+", "step_size", ",", "step_size", ")", ")", ",", "self", ".", "average_rewards", ")", "\n", "plt", ".", "xlabel", "(", "'Number of episodes'", ")", "\n", "plt", ".", "ylabel", "(", "'Average score'", ")", "\n", "\n", "plt", ".", "show", "(", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.az79nefy_OrdinalRL.dqn.ordinal_dqn_agent.DQNAgent.__init__": [[11, 30], ["collections.deque", "ordinal_dqn_agent.DQNAgent.build_model", "ordinal_dqn_agent.DQNAgent.build_model", "range", "range"], "methods", ["home.repos.pwc.inspect_result.az79nefy_OrdinalRL.dqn.atari_dqn_single_net_agent.DQNAgent.build_model", "home.repos.pwc.inspect_result.az79nefy_OrdinalRL.dqn.atari_dqn_single_net_agent.DQNAgent.build_model"], ["    ", "def", "__init__", "(", "self", ",", "alpha", ",", "gamma", ",", "epsilon", ",", "epsilon_min", ",", "n_actions", ",", "n_ordinals", ",", "observation_dim", ",", "batch_size", ",", "memory_len", ",", "replace_target_iter", ")", ":", "\n", "        ", "self", ".", "alpha", "=", "alpha", "\n", "self", ".", "gamma", "=", "gamma", "\n", "self", ".", "epsilon", "=", "epsilon", "\n", "self", ".", "epsilon_min", "=", "epsilon_min", "\n", "self", ".", "n_actions", "=", "n_actions", "\n", "self", ".", "n_ordinals", "=", "n_ordinals", "\n", "self", ".", "n_inputs", "=", "observation_dim", "\n", "\n", "self", ".", "batch_size", "=", "batch_size", "\n", "self", ".", "memory", "=", "deque", "(", "maxlen", "=", "memory_len", ")", "\n", "# creation of a neural net for every action", "\n", "self", ".", "eval_action_nets", "=", "[", "self", ".", "build_model", "(", "self", ".", "n_inputs", ")", "for", "_", "in", "range", "(", "n_actions", ")", "]", "\n", "self", ".", "target_action_nets", "=", "[", "self", ".", "build_model", "(", "self", ".", "n_inputs", ")", "for", "_", "in", "range", "(", "n_actions", ")", "]", "\n", "self", ".", "replace_target_iter", "=", "replace_target_iter", "\n", "\n", "self", ".", "replay_counter", "=", "0", "\n", "self", ".", "win_rates", "=", "[", "]", "\n", "self", ".", "average_rewards", "=", "[", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.az79nefy_OrdinalRL.dqn.ordinal_dqn_agent.DQNAgent.build_model": [[32, 39], ["keras.models.Sequential", "keras.models.Sequential.add", "keras.models.Sequential.add", "keras.models.Sequential.add", "keras.models.Sequential.compile", "keras.layers.Dense", "keras.layers.Dense", "keras.layers.Dense", "keras.optimizers.Adam"], "methods", ["None"], ["", "def", "build_model", "(", "self", ",", "n_inputs", ")", ":", "\n", "        ", "neural_net", "=", "Sequential", "(", ")", "\n", "neural_net", ".", "add", "(", "Dense", "(", "6", ",", "input_dim", "=", "n_inputs", ",", "activation", "=", "'relu'", ")", ")", "\n", "neural_net", ".", "add", "(", "Dense", "(", "6", ",", "activation", "=", "'relu'", ")", ")", "\n", "neural_net", ".", "add", "(", "Dense", "(", "self", ".", "n_ordinals", ",", "activation", "=", "'linear'", ")", ")", "\n", "neural_net", ".", "compile", "(", "loss", "=", "'mse'", ",", "optimizer", "=", "Adam", "(", "lr", "=", "self", ".", "alpha", ")", ")", "\n", "return", "neural_net", "\n", "\n"]], "home.repos.pwc.inspect_result.az79nefy_OrdinalRL.dqn.ordinal_dqn_agent.DQNAgent.update": [[40, 45], ["ordinal_dqn_agent.DQNAgent.reward_to_ordinal", "ordinal_dqn_agent.DQNAgent.remember", "len", "ordinal_dqn_agent.DQNAgent.replay"], "methods", ["home.repos.pwc.inspect_result.az79nefy_OrdinalRL.dqn.ordinal_dqn_discretized_agent.DQNAgent.reward_to_ordinal", "home.repos.pwc.inspect_result.az79nefy_OrdinalRL.dqn.atari_dqn_single_net_agent.DQNAgent.remember", "home.repos.pwc.inspect_result.az79nefy_OrdinalRL.dqn.atari_dqn_single_net_agent.DQNAgent.replay"], ["", "def", "update", "(", "self", ",", "prev_obs", ",", "prev_act", ",", "obs", ",", "reward", ",", "episode_reward", ",", "done", ")", ":", "\n", "        ", "ordinal", "=", "self", ".", "reward_to_ordinal", "(", "reward", ",", "episode_reward", ",", "done", ")", "\n", "self", ".", "remember", "(", "prev_obs", ",", "prev_act", ",", "obs", ",", "ordinal", ",", "done", ")", "\n", "if", "len", "(", "self", ".", "memory", ")", ">", "self", ".", "batch_size", ":", "\n", "            ", "self", ".", "replay", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.az79nefy_OrdinalRL.dqn.ordinal_dqn_agent.DQNAgent.remember": [[46, 48], ["ordinal_dqn_agent.DQNAgent.memory.append"], "methods", ["None"], ["", "", "def", "remember", "(", "self", ",", "prev_obs", ",", "prev_act", ",", "obs", ",", "rew", ",", "d", ")", ":", "\n", "        ", "self", ".", "memory", ".", "append", "(", "(", "prev_obs", ",", "prev_act", ",", "obs", ",", "rew", ",", "d", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.az79nefy_OrdinalRL.dqn.ordinal_dqn_agent.DQNAgent.replay": [[49, 72], ["random.sample", "range", "range", "ordinal_dqn_agent.DQNAgent.get_greedy_action", "x_batch[].append", "y_batch[].append", "ordinal_dqn_agent.DQNAgent.target_action_nets[].set_weights", "numpy.zeros", "len", "ordinal_dqn_agent.DQNAgent.eval_action_nets[].fit", "ordinal_dqn_agent.DQNAgent.eval_action_nets[].get_weights", "range", "range", "numpy.array", "numpy.array", "ordinal_dqn_agent.DQNAgent.target_action_nets[].predict", "len"], "methods", ["home.repos.pwc.inspect_result.az79nefy_OrdinalRL.dqn.atari_dqn_single_net_agent.DQNAgent.get_greedy_action"], ["", "def", "replay", "(", "self", ")", ":", "\n", "# copy evaluation action nets to target action nets at first replay and then every 200 replay steps", "\n", "        ", "if", "self", ".", "replay_counter", "%", "self", ".", "replace_target_iter", "==", "0", ":", "\n", "            ", "for", "a", "in", "range", "(", "self", ".", "n_actions", ")", ":", "\n", "                ", "self", ".", "target_action_nets", "[", "a", "]", ".", "set_weights", "(", "self", ".", "eval_action_nets", "[", "a", "]", ".", "get_weights", "(", ")", ")", "\n", "", "", "self", ".", "replay_counter", "+=", "1", "\n", "\n", "mini_batch", "=", "random", ".", "sample", "(", "self", ".", "memory", ",", "self", ".", "batch_size", ")", "\n", "x_batch", ",", "y_batch", "=", "[", "[", "]", "for", "_", "in", "range", "(", "self", ".", "n_actions", ")", "]", ",", "[", "[", "]", "for", "_", "in", "range", "(", "self", ".", "n_actions", ")", "]", "\n", "for", "prev_obs", ",", "prev_act", ",", "obs", ",", "ordinal", ",", "d", "in", "mini_batch", ":", "\n", "            ", "greedy_action", "=", "self", ".", "get_greedy_action", "(", "obs", ")", "\n", "if", "not", "d", ":", "\n", "                ", "target", "=", "self", ".", "gamma", "*", "self", ".", "target_action_nets", "[", "greedy_action", "]", ".", "predict", "(", "obs", ")", "[", "0", "]", "\n", "target", "[", "ordinal", "]", "+=", "1", "\n", "", "else", ":", "\n", "                ", "target", "=", "np", ".", "zeros", "(", "self", ".", "n_ordinals", ")", "\n", "target", "[", "ordinal", "]", "+=", "1", "\n", "# fit predicted value of previous action in previous observation to target value of max_action", "\n", "", "x_batch", "[", "prev_act", "]", ".", "append", "(", "prev_obs", "[", "0", "]", ")", "\n", "y_batch", "[", "prev_act", "]", ".", "append", "(", "target", ")", "\n", "", "for", "a", "in", "range", "(", "self", ".", "n_actions", ")", ":", "\n", "            ", "if", "len", "(", "x_batch", "[", "a", "]", ")", "!=", "0", ":", "\n", "                ", "self", ".", "eval_action_nets", "[", "a", "]", ".", "fit", "(", "np", ".", "array", "(", "x_batch", "[", "a", "]", ")", ",", "np", ".", "array", "(", "y_batch", "[", "a", "]", ")", ",", "batch_size", "=", "len", "(", "x_batch", "[", "a", "]", ")", ",", "verbose", "=", "0", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.az79nefy_OrdinalRL.dqn.ordinal_dqn_agent.DQNAgent.compute_borda_scores": [[74, 130], ["numpy.zeros", "range", "numpy.array", "numpy.count_nonzero", "range", "range", "ordinal_dqn_agent.DQNAgent.eval_action_nets[].predict", "ordinal_values_per_action[].append", "borda_scores.append", "borda_scores.append", "range", "borda_scores.append", "range"], "methods", ["None"], ["", "", "", "def", "compute_borda_scores", "(", "self", ",", "obs", ")", ":", "\n", "# sum up all ordinal values per action for given observation", "\n", "        ", "ordinal_value_sum_per_action", "=", "np", ".", "zeros", "(", "self", ".", "n_actions", ")", "\n", "ordinal_values_per_action", "=", "[", "[", "]", "for", "_", "in", "range", "(", "self", ".", "n_actions", ")", "]", "\n", "for", "action_a", "in", "range", "(", "self", ".", "n_actions", ")", ":", "\n", "            ", "for", "ordinal_value", "in", "self", ".", "eval_action_nets", "[", "action_a", "]", ".", "predict", "(", "obs", ")", "[", "0", "]", ":", "\n", "                ", "ordinal_value_sum_per_action", "[", "action_a", "]", "+=", "ordinal_value", "\n", "ordinal_values_per_action", "[", "action_a", "]", ".", "append", "(", "ordinal_value", ")", "\n", "", "", "ordinal_values_per_action", "=", "np", ".", "array", "(", "ordinal_values_per_action", ")", "\n", "\n", "# count actions whose ordinal value sum is not zero (no comparision possible for actions without ordinal_value)", "\n", "non_zero_action_count", "=", "np", ".", "count_nonzero", "(", "ordinal_value_sum_per_action", ")", "\n", "actions_to_compare_count", "=", "non_zero_action_count", "-", "1", "\n", "\n", "borda_scores", "=", "[", "]", "\n", "# compute borda_values for action_a (probability that action_a wins against any other action)", "\n", "for", "action_a", "in", "range", "(", "self", ".", "n_actions", ")", ":", "\n", "# if action has not yet recorded any ordinal values, action has to be played (set borda_value to 1.0)", "\n", "            ", "if", "ordinal_value_sum_per_action", "[", "action_a", "]", "==", "0", ":", "\n", "                ", "borda_scores", ".", "append", "(", "1.0", ")", "\n", "continue", "\n", "\n", "", "if", "actions_to_compare_count", "<", "1", ":", "\n", "# set lower than 1.0 (borda_value for zero_actions is 1.0)", "\n", "                ", "borda_scores", ".", "append", "(", "0.5", ")", "\n", "", "else", ":", "\n", "# over all actions: sum up the probabilities that action_a wins against the given action", "\n", "                ", "winning_probability_a_sum", "=", "0", "\n", "# compare action_a to all other actions", "\n", "for", "action_b", "in", "range", "(", "self", ".", "n_actions", ")", ":", "\n", "                    ", "if", "action_a", "==", "action_b", ":", "\n", "                        ", "continue", "\n", "# not comparable if action_b has no ordinal_values", "\n", "", "if", "ordinal_value_sum_per_action", "[", "action_b", "]", "==", "0", ":", "\n", "                        ", "continue", "\n", "", "else", ":", "\n", "# probability that action_a wins against action_b", "\n", "                        ", "winning_probability_a", "=", "0", "\n", "# running ordinal probability that action_b is worse than current investigated ordinal", "\n", "worse_probability_b", "=", "0", "\n", "# predict ordinal values for action a and b", "\n", "ordinal_values_a", "=", "ordinal_values_per_action", "[", "action_a", "]", "\n", "ordinal_values_b", "=", "ordinal_values_per_action", "[", "action_b", "]", "\n", "for", "ordinal_count", "in", "range", "(", "self", ".", "n_ordinals", ")", ":", "\n", "                            ", "ordinal_probability_a", "=", "ordinal_values_a", "[", "ordinal_count", "]", "/", "ordinal_value_sum_per_action", "[", "action_a", "]", "\n", "# ordinal_probability_b is also the tie probability", "\n", "ordinal_probability_b", "=", "(", "ordinal_values_b", "[", "ordinal_count", "]", "/", "\n", "ordinal_value_sum_per_action", "[", "action_b", "]", ")", "\n", "winning_probability_a", "+=", "ordinal_probability_a", "*", "(", "worse_probability_b", "+", "ordinal_probability_b", "/", "2.0", ")", "\n", "worse_probability_b", "+=", "ordinal_probability_b", "\n", "", "winning_probability_a_sum", "+=", "winning_probability_a", "\n", "# normalize summed up probabilities with number of actions that have been compared", "\n", "", "", "borda_scores", ".", "append", "(", "winning_probability_a_sum", "/", "actions_to_compare_count", ")", "\n", "", "", "return", "borda_scores", "\n", "\n"]], "home.repos.pwc.inspect_result.az79nefy_OrdinalRL.dqn.ordinal_dqn_agent.DQNAgent.get_greedy_action": [[131, 133], ["numpy.argmax", "ordinal_dqn_agent.DQNAgent.compute_borda_scores"], "methods", ["home.repos.pwc.inspect_result.az79nefy_OrdinalRL.dqn.ordinal_dqn_discretized_agent.DQNAgent.compute_borda_scores"], ["", "def", "get_greedy_action", "(", "self", ",", "obs", ")", ":", "\n", "        ", "return", "np", ".", "argmax", "(", "self", ".", "compute_borda_scores", "(", "obs", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.az79nefy_OrdinalRL.dqn.ordinal_dqn_agent.DQNAgent.choose_action": [[135, 143], ["ordinal_dqn_agent.DQNAgent.get_greedy_action", "random.randrange", "random.random"], "methods", ["home.repos.pwc.inspect_result.az79nefy_OrdinalRL.dqn.atari_dqn_single_net_agent.DQNAgent.get_greedy_action"], ["", "def", "choose_action", "(", "self", ",", "obs", ",", "greedy", ")", ":", "\n", "        ", "greedy_action", "=", "self", ".", "get_greedy_action", "(", "obs", ")", "\n", "# choose random action with probability epsilon", "\n", "if", "not", "greedy", "and", "random", ".", "random", "(", ")", "<", "self", ".", "epsilon", ":", "\n", "            ", "return", "random", ".", "randrange", "(", "self", ".", "n_actions", ")", "\n", "# greedy action is chosen with probability (1 - epsilon)", "\n", "", "else", ":", "\n", "            ", "return", "greedy_action", "\n", "\n"]], "home.repos.pwc.inspect_result.az79nefy_OrdinalRL.dqn.ordinal_dqn_agent.DQNAgent.end_episode": [[144, 147], ["None"], "methods", ["None"], ["", "", "def", "end_episode", "(", "self", ",", "n_episodes", ")", ":", "\n", "# gradually reduce epsilon after every done episode", "\n", "        ", "self", ".", "epsilon", "=", "self", ".", "epsilon", "-", "2", "/", "n_episodes", "if", "self", ".", "epsilon", ">", "self", ".", "epsilon_min", "else", "self", ".", "epsilon_min", "\n", "\n"]], "home.repos.pwc.inspect_result.az79nefy_OrdinalRL.dqn.ordinal_dqn_agent.DQNAgent.preprocess_observation": [[148, 151], ["numpy.expand_dims"], "methods", ["None"], ["", "@", "staticmethod", "\n", "def", "preprocess_observation", "(", "obs", ")", ":", "\n", "        ", "return", "np", ".", "expand_dims", "(", "obs", ",", "axis", "=", "0", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.az79nefy_OrdinalRL.dqn.ordinal_dqn_agent.DQNAgent.reward_to_ordinal": [[153, 160], ["None"], "methods", ["None"], ["", "def", "reward_to_ordinal", "(", "self", ",", "reward", ",", "episode_reward", ",", "done", ")", ":", "\n", "        ", "if", "reward", "==", "-", "10", ":", "\n", "            ", "return", "0", "\n", "", "if", "reward", "==", "-", "1", ":", "\n", "            ", "return", "1", "\n", "", "else", ":", "\n", "            ", "return", "2", "\n", "\n"]], "home.repos.pwc.inspect_result.az79nefy_OrdinalRL.dqn.ordinal_dqn_agent.DQNAgent.check_win_condition": [[162, 168], ["None"], "methods", ["None"], ["", "", "@", "staticmethod", "\n", "def", "check_win_condition", "(", "reward", ",", "episode_reward", ",", "done", ")", ":", "\n", "        ", "if", "done", "and", "reward", "==", "20", ":", "\n", "            ", "return", "True", "\n", "", "else", ":", "\n", "            ", "return", "False", "\n", "\n"]], "home.repos.pwc.inspect_result.az79nefy_OrdinalRL.dqn.ordinal_dqn_agent.DQNAgent.evaluate": [[169, 177], ["round", "round", "ordinal_dqn_agent.DQNAgent.average_rewards.append", "ordinal_dqn_agent.DQNAgent.win_rates.append", "print", "sum", "len", "sum", "len"], "methods", ["None"], ["", "", "def", "evaluate", "(", "self", ",", "i_episode", ",", "episode_rewards", ",", "episode_wins", ")", ":", "\n", "# compute average episode reward and win rate over last episodes", "\n", "        ", "average_reward", "=", "round", "(", "sum", "(", "episode_rewards", ")", "/", "len", "(", "episode_rewards", ")", ",", "2", ")", "\n", "win_rate", "=", "round", "(", "sum", "(", "episode_wins", ")", "/", "len", "(", "episode_wins", ")", ",", "2", ")", "\n", "# store average episode reward and win rate over last episodes for plotting purposes", "\n", "self", ".", "average_rewards", ".", "append", "(", "average_reward", ")", "\n", "self", ".", "win_rates", ".", "append", "(", "win_rate", ")", "\n", "print", "(", "\"{}\\t{}\\t{}\"", ".", "format", "(", "i_episode", "+", "1", ",", "average_reward", ",", "win_rate", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.az79nefy_OrdinalRL.dqn.ordinal_dqn_agent.DQNAgent.plot": [[179, 193], ["matplotlib.figure", "matplotlib.plot", "matplotlib.xlabel", "matplotlib.ylabel", "matplotlib.figure", "matplotlib.plot", "matplotlib.xlabel", "matplotlib.ylabel", "matplotlib.show", "list", "list", "range", "range"], "methods", ["home.repos.pwc.inspect_result.az79nefy_OrdinalRL.dqn.atari_dqn_single_net_agent.DQNAgent.plot", "home.repos.pwc.inspect_result.az79nefy_OrdinalRL.dqn.atari_dqn_single_net_agent.DQNAgent.plot"], ["", "def", "plot", "(", "self", ",", "n_episodes", ",", "step_size", ")", ":", "\n", "# plot win rate", "\n", "        ", "plt", ".", "figure", "(", ")", "\n", "plt", ".", "plot", "(", "list", "(", "range", "(", "step_size", ",", "n_episodes", "+", "step_size", ",", "step_size", ")", ")", ",", "self", ".", "win_rates", ")", "\n", "plt", ".", "xlabel", "(", "'Number of episodes'", ")", "\n", "plt", ".", "ylabel", "(", "'Win rate'", ")", "\n", "\n", "# plot average score", "\n", "plt", ".", "figure", "(", ")", "\n", "plt", ".", "plot", "(", "list", "(", "range", "(", "step_size", ",", "n_episodes", "+", "step_size", ",", "step_size", ")", ")", ",", "self", ".", "average_rewards", ")", "\n", "plt", ".", "xlabel", "(", "'Number of episodes'", ")", "\n", "plt", ".", "ylabel", "(", "'Average score'", ")", "\n", "\n", "plt", ".", "show", "(", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.az79nefy_OrdinalRL.dqn.dqn_multi_net_discretized_agent.DQNAgent.__init__": [[11, 29], ["collections.deque", "dqn_multi_net_discretized_agent.DQNAgent.build_model", "dqn_multi_net_discretized_agent.DQNAgent.build_model", "range", "range"], "methods", ["home.repos.pwc.inspect_result.az79nefy_OrdinalRL.dqn.atari_dqn_single_net_agent.DQNAgent.build_model", "home.repos.pwc.inspect_result.az79nefy_OrdinalRL.dqn.atari_dqn_single_net_agent.DQNAgent.build_model"], ["    ", "def", "__init__", "(", "self", ",", "alpha", ",", "gamma", ",", "epsilon", ",", "epsilon_min", ",", "n_actions", ",", "n_ordinals", ",", "observation_dim", ",", "batch_size", ",", "memory_len", ",", "replace_target_iter", ")", ":", "\n", "        ", "self", ".", "alpha", "=", "alpha", "\n", "self", ".", "gamma", "=", "gamma", "\n", "self", ".", "epsilon", "=", "epsilon", "\n", "self", ".", "epsilon_min", "=", "epsilon_min", "\n", "self", ".", "n_actions", "=", "n_actions", "\n", "self", ".", "n_inputs", "=", "observation_dim", "\n", "\n", "self", ".", "batch_size", "=", "batch_size", "\n", "self", ".", "memory", "=", "deque", "(", "maxlen", "=", "memory_len", ")", "\n", "# creation of a neural net for every action", "\n", "self", ".", "eval_action_nets", "=", "[", "self", ".", "build_model", "(", "self", ".", "n_inputs", ")", "for", "_", "in", "range", "(", "n_actions", ")", "]", "\n", "self", ".", "target_action_nets", "=", "[", "self", ".", "build_model", "(", "self", ".", "n_inputs", ")", "for", "_", "in", "range", "(", "n_actions", ")", "]", "\n", "self", ".", "replace_target_iter", "=", "replace_target_iter", "\n", "\n", "self", ".", "replay_counter", "=", "0", "\n", "self", ".", "win_rates", "=", "[", "]", "\n", "self", ".", "average_rewards", "=", "[", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.az79nefy_OrdinalRL.dqn.dqn_multi_net_discretized_agent.DQNAgent.build_model": [[31, 38], ["keras.models.Sequential", "keras.models.Sequential.add", "keras.models.Sequential.add", "keras.models.Sequential.add", "keras.models.Sequential.compile", "keras.layers.Dense", "keras.layers.Dense", "keras.layers.Dense", "keras.optimizers.Adam"], "methods", ["None"], ["", "def", "build_model", "(", "self", ",", "n_inputs", ")", ":", "\n", "        ", "neural_net", "=", "Sequential", "(", ")", "\n", "neural_net", ".", "add", "(", "Dense", "(", "24", ",", "input_dim", "=", "n_inputs", ",", "activation", "=", "'relu'", ")", ")", "\n", "neural_net", ".", "add", "(", "Dense", "(", "24", ",", "activation", "=", "'relu'", ")", ")", "\n", "neural_net", ".", "add", "(", "Dense", "(", "1", ",", "activation", "=", "'linear'", ")", ")", "\n", "neural_net", ".", "compile", "(", "loss", "=", "'mse'", ",", "optimizer", "=", "Adam", "(", "lr", "=", "self", ".", "alpha", ")", ")", "\n", "return", "neural_net", "\n", "\n"]], "home.repos.pwc.inspect_result.az79nefy_OrdinalRL.dqn.dqn_multi_net_discretized_agent.DQNAgent.update": [[39, 44], ["dqn_multi_net_discretized_agent.DQNAgent.remap_reward", "dqn_multi_net_discretized_agent.DQNAgent.remember", "len", "dqn_multi_net_discretized_agent.DQNAgent.replay"], "methods", ["home.repos.pwc.inspect_result.az79nefy_OrdinalRL.dqn.dqn_single_net_agent.DQNAgent.remap_reward", "home.repos.pwc.inspect_result.az79nefy_OrdinalRL.dqn.atari_dqn_single_net_agent.DQNAgent.remember", "home.repos.pwc.inspect_result.az79nefy_OrdinalRL.dqn.atari_dqn_single_net_agent.DQNAgent.replay"], ["", "def", "update", "(", "self", ",", "prev_obs", ",", "prev_act", ",", "obs", ",", "reward", ",", "episode_reward", ",", "done", ")", ":", "\n", "        ", "reward", "=", "self", ".", "remap_reward", "(", "reward", ",", "episode_reward", ",", "done", ")", "\n", "self", ".", "remember", "(", "prev_obs", ",", "prev_act", ",", "obs", ",", "reward", ",", "done", ")", "\n", "if", "len", "(", "self", ".", "memory", ")", ">", "self", ".", "batch_size", ":", "\n", "            ", "self", ".", "replay", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.az79nefy_OrdinalRL.dqn.dqn_multi_net_discretized_agent.DQNAgent.remember": [[45, 47], ["dqn_multi_net_discretized_agent.DQNAgent.memory.append"], "methods", ["None"], ["", "", "def", "remember", "(", "self", ",", "prev_obs", ",", "prev_act", ",", "obs", ",", "rew", ",", "d", ")", ":", "\n", "        ", "self", ".", "memory", ".", "append", "(", "(", "prev_obs", ",", "prev_act", ",", "obs", ",", "rew", ",", "d", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.az79nefy_OrdinalRL.dqn.dqn_multi_net_discretized_agent.DQNAgent.replay": [[48, 72], ["random.sample", "range", "range", "x_batch[].append", "y_batch[].append", "dqn_multi_net_discretized_agent.DQNAgent.target_action_nets[].set_weights", "numpy.argmax", "len", "dqn_multi_net_discretized_agent.DQNAgent.eval_action_nets[].fit", "dqn_multi_net_discretized_agent.DQNAgent.eval_action_nets[].get_weights", "range", "range", "action_predictions.append", "numpy.array", "numpy.array", "len", "act_net.predict", "dqn_multi_net_discretized_agent.DQNAgent.target_action_nets[].predict"], "methods", ["None"], ["", "def", "replay", "(", "self", ")", ":", "\n", "# copy evaluation action nets to target action nets at first replay and then every 200 replay steps", "\n", "        ", "if", "self", ".", "replay_counter", "%", "self", ".", "replace_target_iter", "==", "0", ":", "\n", "            ", "for", "a", "in", "range", "(", "self", ".", "n_actions", ")", ":", "\n", "                ", "self", ".", "target_action_nets", "[", "a", "]", ".", "set_weights", "(", "self", ".", "eval_action_nets", "[", "a", "]", ".", "get_weights", "(", ")", ")", "\n", "", "", "self", ".", "replay_counter", "+=", "1", "\n", "\n", "mini_batch", "=", "random", ".", "sample", "(", "self", ".", "memory", ",", "self", ".", "batch_size", ")", "\n", "x_batch", ",", "y_batch", "=", "[", "[", "]", "for", "_", "in", "range", "(", "self", ".", "n_actions", ")", "]", ",", "[", "[", "]", "for", "_", "in", "range", "(", "self", ".", "n_actions", ")", "]", "\n", "for", "prev_obs", ",", "prev_act", ",", "obs", ",", "rew", ",", "d", "in", "mini_batch", ":", "\n", "            ", "if", "not", "d", ":", "\n", "                ", "action_predictions", "=", "[", "]", "\n", "for", "act_net", "in", "self", ".", "eval_action_nets", ":", "\n", "                    ", "action_predictions", ".", "append", "(", "act_net", ".", "predict", "(", "obs", ")", "[", "0", "]", ")", "\n", "", "best_act", "=", "np", ".", "argmax", "(", "action_predictions", ")", "\n", "target", "=", "rew", "+", "self", ".", "gamma", "*", "self", ".", "target_action_nets", "[", "best_act", "]", ".", "predict", "(", "obs", ")", "[", "0", "]", "\n", "", "else", ":", "\n", "                ", "target", "=", "rew", "\n", "# fit predicted value of previous action in previous observation to target value of max_action", "\n", "", "x_batch", "[", "prev_act", "]", ".", "append", "(", "prev_obs", "[", "0", "]", ")", "\n", "y_batch", "[", "prev_act", "]", ".", "append", "(", "target", ")", "\n", "", "for", "a", "in", "range", "(", "self", ".", "n_actions", ")", ":", "\n", "            ", "if", "len", "(", "x_batch", "[", "a", "]", ")", "!=", "0", ":", "\n", "                ", "self", ".", "eval_action_nets", "[", "a", "]", ".", "fit", "(", "np", ".", "array", "(", "x_batch", "[", "a", "]", ")", ",", "np", ".", "array", "(", "y_batch", "[", "a", "]", ")", ",", "batch_size", "=", "len", "(", "x_batch", "[", "a", "]", ")", ",", "verbose", "=", "0", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.az79nefy_OrdinalRL.dqn.dqn_multi_net_discretized_agent.DQNAgent.get_greedy_action": [[73, 78], ["numpy.argmax", "action_predictions.append", "act_net.predict"], "methods", ["None"], ["", "", "", "def", "get_greedy_action", "(", "self", ",", "obs", ")", ":", "\n", "        ", "action_predictions", "=", "[", "]", "\n", "for", "act_net", "in", "self", ".", "eval_action_nets", ":", "\n", "            ", "action_predictions", ".", "append", "(", "act_net", ".", "predict", "(", "obs", ")", "[", "0", "]", ")", "\n", "", "return", "np", ".", "argmax", "(", "action_predictions", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.az79nefy_OrdinalRL.dqn.dqn_multi_net_discretized_agent.DQNAgent.choose_action": [[80, 88], ["dqn_multi_net_discretized_agent.DQNAgent.get_greedy_action", "random.randrange", "random.random"], "methods", ["home.repos.pwc.inspect_result.az79nefy_OrdinalRL.dqn.atari_dqn_single_net_agent.DQNAgent.get_greedy_action"], ["", "def", "choose_action", "(", "self", ",", "obs", ",", "greedy", ")", ":", "\n", "        ", "greedy_action", "=", "self", ".", "get_greedy_action", "(", "obs", ")", "\n", "# choose random action with probability epsilon", "\n", "if", "not", "greedy", "and", "random", ".", "random", "(", ")", "<", "self", ".", "epsilon", ":", "\n", "            ", "return", "random", ".", "randrange", "(", "self", ".", "n_actions", ")", "\n", "# greedy action is chosen with probability (1 - epsilon)", "\n", "", "else", ":", "\n", "            ", "return", "greedy_action", "\n", "\n"]], "home.repos.pwc.inspect_result.az79nefy_OrdinalRL.dqn.dqn_multi_net_discretized_agent.DQNAgent.end_episode": [[89, 92], ["None"], "methods", ["None"], ["", "", "def", "end_episode", "(", "self", ",", "n_episodes", ")", ":", "\n", "# gradually reduce epsilon after every done episode", "\n", "        ", "self", ".", "epsilon", "=", "self", ".", "epsilon", "-", "2", "/", "n_episodes", "if", "self", ".", "epsilon", ">", "self", ".", "epsilon_min", "else", "self", ".", "epsilon_min", "\n", "\n"]], "home.repos.pwc.inspect_result.az79nefy_OrdinalRL.dqn.dqn_multi_net_discretized_agent.DQNAgent.preprocess_observation": [[93, 96], ["numpy.expand_dims"], "methods", ["None"], ["", "@", "staticmethod", "\n", "def", "preprocess_observation", "(", "obs", ")", ":", "\n", "        ", "return", "np", ".", "expand_dims", "(", "obs", ",", "axis", "=", "0", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.az79nefy_OrdinalRL.dqn.dqn_multi_net_discretized_agent.DQNAgent.remap_reward": [[98, 103], ["dqn_multi_net_discretized_agent.DQNAgent.check_win_condition"], "methods", ["home.repos.pwc.inspect_result.az79nefy_OrdinalRL.dqn.atari_dqn_single_net_agent.DQNAgent.check_win_condition"], ["", "def", "remap_reward", "(", "self", ",", "reward", ",", "episode_reward", ",", "done", ")", ":", "\n", "        ", "if", "done", "and", "not", "self", ".", "check_win_condition", "(", "reward", ",", "episode_reward", ",", "done", ")", ":", "\n", "            ", "return", "-", "1", "\n", "", "else", ":", "\n", "            ", "return", "0", "\n", "\n"]], "home.repos.pwc.inspect_result.az79nefy_OrdinalRL.dqn.dqn_multi_net_discretized_agent.DQNAgent.check_win_condition": [[105, 111], ["None"], "methods", ["None"], ["", "", "@", "staticmethod", "\n", "def", "check_win_condition", "(", "reward", ",", "episode_reward", ",", "done", ")", ":", "\n", "        ", "if", "done", "and", "episode_reward", "==", "200", ":", "\n", "            ", "return", "True", "\n", "", "else", ":", "\n", "            ", "return", "False", "\n", "\n"]], "home.repos.pwc.inspect_result.az79nefy_OrdinalRL.dqn.dqn_multi_net_discretized_agent.DQNAgent.evaluate": [[112, 120], ["round", "round", "dqn_multi_net_discretized_agent.DQNAgent.average_rewards.append", "dqn_multi_net_discretized_agent.DQNAgent.win_rates.append", "print", "sum", "len", "sum", "len"], "methods", ["None"], ["", "", "def", "evaluate", "(", "self", ",", "i_episode", ",", "episode_rewards", ",", "episode_wins", ")", ":", "\n", "# compute average episode reward and win rate over last episodes", "\n", "        ", "average_reward", "=", "round", "(", "sum", "(", "episode_rewards", ")", "/", "len", "(", "episode_rewards", ")", ",", "2", ")", "\n", "win_rate", "=", "round", "(", "sum", "(", "episode_wins", ")", "/", "len", "(", "episode_wins", ")", ",", "2", ")", "\n", "# store average episode reward and win rate over last episodes for plotting purposes", "\n", "self", ".", "average_rewards", ".", "append", "(", "average_reward", ")", "\n", "self", ".", "win_rates", ".", "append", "(", "win_rate", ")", "\n", "print", "(", "\"{}\\t{}\\t{}\"", ".", "format", "(", "i_episode", "+", "1", ",", "average_reward", ",", "win_rate", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.az79nefy_OrdinalRL.dqn.dqn_multi_net_discretized_agent.DQNAgent.plot": [[122, 136], ["matplotlib.figure", "matplotlib.plot", "matplotlib.xlabel", "matplotlib.ylabel", "matplotlib.figure", "matplotlib.plot", "matplotlib.xlabel", "matplotlib.ylabel", "matplotlib.show", "list", "list", "range", "range"], "methods", ["home.repos.pwc.inspect_result.az79nefy_OrdinalRL.dqn.atari_dqn_single_net_agent.DQNAgent.plot", "home.repos.pwc.inspect_result.az79nefy_OrdinalRL.dqn.atari_dqn_single_net_agent.DQNAgent.plot"], ["", "def", "plot", "(", "self", ",", "n_episodes", ",", "step_size", ")", ":", "\n", "# plot win rate", "\n", "        ", "plt", ".", "figure", "(", ")", "\n", "plt", ".", "plot", "(", "list", "(", "range", "(", "step_size", ",", "n_episodes", "+", "step_size", ",", "step_size", ")", ")", ",", "self", ".", "win_rates", ")", "\n", "plt", ".", "xlabel", "(", "'Number of episodes'", ")", "\n", "plt", ".", "ylabel", "(", "'Win rate'", ")", "\n", "\n", "# plot average score", "\n", "plt", ".", "figure", "(", ")", "\n", "plt", ".", "plot", "(", "list", "(", "range", "(", "step_size", ",", "n_episodes", "+", "step_size", ",", "step_size", ")", ")", ",", "self", ".", "average_rewards", ")", "\n", "plt", ".", "xlabel", "(", "'Number of episodes'", ")", "\n", "plt", ".", "ylabel", "(", "'Average score'", ")", "\n", "\n", "plt", ".", "show", "(", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.az79nefy_OrdinalRL.dqn.atari_dqn_multi_net_agent.DQNAgent.__init__": [[11, 30], ["collections.deque", "atari_dqn_multi_net_agent.DQNAgent.build_model", "atari_dqn_multi_net_agent.DQNAgent.build_model", "range", "range"], "methods", ["home.repos.pwc.inspect_result.az79nefy_OrdinalRL.dqn.atari_dqn_single_net_agent.DQNAgent.build_model", "home.repos.pwc.inspect_result.az79nefy_OrdinalRL.dqn.atari_dqn_single_net_agent.DQNAgent.build_model"], ["    ", "def", "__init__", "(", "self", ",", "alpha", ",", "gamma", ",", "epsilon", ",", "epsilon_min", ",", "n_actions", ",", "n_ordinals", ",", "observation_dim", ",", "batch_size", ",", "memory_len", ",", "replace_target_iter", ")", ":", "\n", "        ", "self", ".", "alpha", "=", "alpha", "\n", "self", ".", "gamma", "=", "gamma", "\n", "self", ".", "epsilon", "=", "epsilon", "\n", "self", ".", "epsilon_min", "=", "epsilon_min", "\n", "self", ".", "n_actions", "=", "n_actions", "\n", "self", ".", "n_inputs", "=", "observation_dim", "\n", "\n", "self", ".", "batch_size", "=", "batch_size", "\n", "self", ".", "memory", "=", "deque", "(", "maxlen", "=", "memory_len", ")", "\n", "# creation of a neural net for every action", "\n", "self", ".", "eval_action_nets", "=", "[", "self", ".", "build_model", "(", "self", ".", "n_inputs", ")", "for", "_", "in", "range", "(", "n_actions", ")", "]", "\n", "self", ".", "target_action_nets", "=", "[", "self", ".", "build_model", "(", "self", ".", "n_inputs", ")", "for", "_", "in", "range", "(", "n_actions", ")", "]", "\n", "self", ".", "replace_target_iter", "=", "replace_target_iter", "\n", "\n", "self", ".", "update_counter", "=", "0", "\n", "self", ".", "replay_counter", "=", "0", "\n", "self", ".", "win_rates", "=", "[", "]", "\n", "self", ".", "average_rewards", "=", "[", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.az79nefy_OrdinalRL.dqn.atari_dqn_multi_net_agent.DQNAgent.build_model": [[32, 45], ["keras.models.Sequential", "keras.models.Sequential.add", "keras.models.Sequential.add", "keras.models.Sequential.add", "keras.models.Sequential.add", "keras.models.Sequential.add", "keras.models.Sequential.add", "keras.models.Sequential.compile", "keras.layers.Conv2D", "keras.layers.Conv2D", "keras.layers.Conv2D", "keras.layers.Flatten", "keras.layers.Dense", "keras.layers.Dense", "keras.optimizers.RMSprop"], "methods", ["None"], ["", "def", "build_model", "(", "self", ",", "n_inputs", ")", ":", "\n", "        ", "neural_net", "=", "Sequential", "(", ")", "\n", "neural_net", ".", "add", "(", "Conv2D", "(", "32", ",", "8", ",", "strides", "=", "(", "4", ",", "4", ")", ",", "\n", "padding", "=", "'valid'", ",", "activation", "=", "'relu'", ",", "input_shape", "=", "n_inputs", ",", "data_format", "=", "'channels_first'", ")", ")", "\n", "neural_net", ".", "add", "(", "Conv2D", "(", "64", ",", "4", ",", "strides", "=", "(", "2", ",", "2", ")", ",", "\n", "padding", "=", "'valid'", ",", "activation", "=", "'relu'", ",", "input_shape", "=", "n_inputs", ",", "data_format", "=", "'channels_first'", ")", ")", "\n", "neural_net", ".", "add", "(", "Conv2D", "(", "64", ",", "3", ",", "strides", "=", "(", "1", ",", "1", ")", ",", "\n", "padding", "=", "'valid'", ",", "activation", "=", "'relu'", ",", "input_shape", "=", "n_inputs", ",", "data_format", "=", "'channels_first'", ")", ")", "\n", "neural_net", ".", "add", "(", "Flatten", "(", ")", ")", "\n", "neural_net", ".", "add", "(", "Dense", "(", "512", ",", "activation", "=", "'relu'", ")", ")", "\n", "neural_net", ".", "add", "(", "Dense", "(", "1", ")", ")", "\n", "neural_net", ".", "compile", "(", "loss", "=", "'mse'", ",", "optimizer", "=", "RMSprop", "(", "lr", "=", "self", ".", "alpha", ",", "rho", "=", "0.95", ",", "epsilon", "=", "0.01", ")", ",", "metrics", "=", "[", "'accuracy'", "]", ")", "\n", "return", "neural_net", "\n", "\n"]], "home.repos.pwc.inspect_result.az79nefy_OrdinalRL.dqn.atari_dqn_multi_net_agent.DQNAgent.update": [[46, 51], ["atari_dqn_multi_net_agent.DQNAgent.remember", "len", "atari_dqn_multi_net_agent.DQNAgent.replay"], "methods", ["home.repos.pwc.inspect_result.az79nefy_OrdinalRL.dqn.atari_dqn_single_net_agent.DQNAgent.remember", "home.repos.pwc.inspect_result.az79nefy_OrdinalRL.dqn.atari_dqn_single_net_agent.DQNAgent.replay"], ["", "def", "update", "(", "self", ",", "prev_obs", ",", "prev_act", ",", "obs", ",", "reward", ",", "episode_reward", ",", "done", ")", ":", "\n", "        ", "self", ".", "update_counter", "+=", "1", "\n", "self", ".", "remember", "(", "prev_obs", ",", "prev_act", ",", "obs", ",", "reward", ",", "done", ")", "\n", "if", "len", "(", "self", ".", "memory", ")", ">", "self", ".", "batch_size", ":", "\n", "            ", "self", ".", "replay", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.az79nefy_OrdinalRL.dqn.atari_dqn_multi_net_agent.DQNAgent.remember": [[52, 54], ["atari_dqn_multi_net_agent.DQNAgent.memory.append"], "methods", ["None"], ["", "", "def", "remember", "(", "self", ",", "prev_obs", ",", "prev_act", ",", "obs", ",", "rew", ",", "d", ")", ":", "\n", "        ", "self", ".", "memory", ".", "append", "(", "(", "prev_obs", ",", "prev_act", ",", "obs", ",", "rew", ",", "d", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.az79nefy_OrdinalRL.dqn.atari_dqn_multi_net_agent.DQNAgent.replay": [[55, 80], ["random.sample", "range", "range", "x_batch[].append", "y_batch[].append", "atari_dqn_multi_net_agent.DQNAgent.eval_action_nets[].fit", "atari_dqn_multi_net_agent.DQNAgent.target_action_nets[].set_weights", "numpy.argmax", "atari_dqn_multi_net_agent.DQNAgent.convert", "len", "atari_dqn_multi_net_agent.DQNAgent.eval_action_nets[].fit", "atari_dqn_multi_net_agent.DQNAgent.eval_action_nets[].get_weights", "range", "range", "action_predictions.append", "atari_dqn_multi_net_agent.DQNAgent.convert", "numpy.array", "numpy.array", "len", "act_net.predict", "atari_dqn_multi_net_agent.DQNAgent.target_action_nets[].predict", "atari_dqn_multi_net_agent.DQNAgent.convert", "atari_dqn_multi_net_agent.DQNAgent.convert"], "methods", ["home.repos.pwc.inspect_result.az79nefy_OrdinalRL.dqn.atari_dqn_single_net_agent.DQNAgent.convert", "home.repos.pwc.inspect_result.az79nefy_OrdinalRL.dqn.atari_dqn_single_net_agent.DQNAgent.convert", "home.repos.pwc.inspect_result.az79nefy_OrdinalRL.dqn.atari_dqn_single_net_agent.DQNAgent.convert", "home.repos.pwc.inspect_result.az79nefy_OrdinalRL.dqn.atari_dqn_single_net_agent.DQNAgent.convert"], ["", "def", "replay", "(", "self", ")", ":", "\n", "# copy evaluation action nets to target action nets at first replay and then every 200 replay steps", "\n", "        ", "if", "self", ".", "replay_counter", "%", "self", ".", "replace_target_iter", "==", "0", ":", "\n", "            ", "for", "a", "in", "range", "(", "self", ".", "n_actions", ")", ":", "\n", "                ", "self", ".", "target_action_nets", "[", "a", "]", ".", "set_weights", "(", "self", ".", "eval_action_nets", "[", "a", "]", ".", "get_weights", "(", ")", ")", "\n", "", "", "self", ".", "replay_counter", "+=", "1", "\n", "\n", "mini_batch", "=", "random", ".", "sample", "(", "self", ".", "memory", ",", "self", ".", "batch_size", ")", "\n", "x_batch", ",", "y_batch", "=", "[", "[", "]", "for", "_", "in", "range", "(", "self", ".", "n_actions", ")", "]", ",", "[", "[", "]", "for", "_", "in", "range", "(", "self", ".", "n_actions", ")", "]", "\n", "for", "prev_obs", ",", "prev_act", ",", "obs", ",", "rew", ",", "d", "in", "mini_batch", ":", "\n", "            ", "if", "not", "d", ":", "\n", "                ", "action_predictions", "=", "[", "]", "\n", "for", "act_net", "in", "self", ".", "eval_action_nets", ":", "\n", "                    ", "action_predictions", ".", "append", "(", "act_net", ".", "predict", "(", "self", ".", "convert", "(", "obs", ")", ")", "[", "0", "]", ")", "\n", "", "best_act", "=", "np", ".", "argmax", "(", "action_predictions", ")", "\n", "target", "=", "rew", "+", "self", ".", "gamma", "*", "self", ".", "target_action_nets", "[", "best_act", "]", ".", "predict", "(", "self", ".", "convert", "(", "obs", ")", ")", "[", "0", "]", "\n", "", "else", ":", "\n", "                ", "target", "=", "rew", "\n", "# fit predicted value of previous action in previous observation to target value of max_action", "\n", "", "x_batch", "[", "prev_act", "]", ".", "append", "(", "self", ".", "convert", "(", "prev_obs", ")", "[", "0", "]", ")", "\n", "y_batch", "[", "prev_act", "]", ".", "append", "(", "target", ")", "\n", "self", ".", "eval_action_nets", "[", "prev_act", "]", ".", "fit", "(", "self", ".", "convert", "(", "prev_obs", ")", ",", "[", "[", "target", "]", "]", ",", "verbose", "=", "0", ")", "\n", "", "for", "a", "in", "range", "(", "self", ".", "n_actions", ")", ":", "\n", "            ", "if", "len", "(", "x_batch", "[", "a", "]", ")", "!=", "0", ":", "\n", "                ", "self", ".", "eval_action_nets", "[", "a", "]", ".", "fit", "(", "np", ".", "array", "(", "x_batch", "[", "a", "]", ")", ",", "np", ".", "array", "(", "y_batch", "[", "a", "]", ")", ",", "batch_size", "=", "len", "(", "x_batch", "[", "a", "]", ")", ",", "verbose", "=", "0", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.az79nefy_OrdinalRL.dqn.atari_dqn_multi_net_agent.DQNAgent.get_greedy_action": [[81, 86], ["numpy.argmax", "action_predictions.append", "act_net.predict", "atari_dqn_multi_net_agent.DQNAgent.convert"], "methods", ["home.repos.pwc.inspect_result.az79nefy_OrdinalRL.dqn.atari_dqn_single_net_agent.DQNAgent.convert"], ["", "", "", "def", "get_greedy_action", "(", "self", ",", "obs", ")", ":", "\n", "        ", "action_predictions", "=", "[", "]", "\n", "for", "act_net", "in", "self", ".", "eval_action_nets", ":", "\n", "            ", "action_predictions", ".", "append", "(", "act_net", ".", "predict", "(", "self", ".", "convert", "(", "obs", ")", ")", "[", "0", "]", ")", "\n", "", "return", "np", ".", "argmax", "(", "action_predictions", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.az79nefy_OrdinalRL.dqn.atari_dqn_multi_net_agent.DQNAgent.choose_action": [[88, 96], ["atari_dqn_multi_net_agent.DQNAgent.get_greedy_action", "random.randrange", "random.random"], "methods", ["home.repos.pwc.inspect_result.az79nefy_OrdinalRL.dqn.atari_dqn_single_net_agent.DQNAgent.get_greedy_action"], ["", "def", "choose_action", "(", "self", ",", "obs", ",", "greedy", ")", ":", "\n", "        ", "greedy_action", "=", "self", ".", "get_greedy_action", "(", "obs", ")", "\n", "# choose random action with probability epsilon", "\n", "if", "not", "greedy", "and", "random", ".", "random", "(", ")", "<", "self", ".", "epsilon", ":", "\n", "            ", "return", "random", ".", "randrange", "(", "self", ".", "n_actions", ")", "\n", "# greedy action is chosen with probability (1 - epsilon)", "\n", "", "else", ":", "\n", "            ", "return", "greedy_action", "\n", "\n"]], "home.repos.pwc.inspect_result.az79nefy_OrdinalRL.dqn.atari_dqn_multi_net_agent.DQNAgent.end_episode": [[97, 100], ["None"], "methods", ["None"], ["", "", "def", "end_episode", "(", "self", ",", "n_episodes", ")", ":", "\n", "# gradually reduce epsilon after every done episode", "\n", "        ", "self", ".", "epsilon", "=", "self", ".", "epsilon", "-", "2", "/", "n_episodes", "if", "self", ".", "epsilon", ">", "self", ".", "epsilon_min", "else", "self", ".", "epsilon_min", "\n", "\n"]], "home.repos.pwc.inspect_result.az79nefy_OrdinalRL.dqn.atari_dqn_multi_net_agent.DQNAgent.preprocess_observation": [[101, 103], ["None"], "methods", ["None"], ["", "def", "preprocess_observation", "(", "self", ",", "obs", ")", ":", "\n", "        ", "return", "obs", "\n", "\n"]], "home.repos.pwc.inspect_result.az79nefy_OrdinalRL.dqn.atari_dqn_multi_net_agent.DQNAgent.convert": [[104, 107], ["numpy.expand_dims", "numpy.asarray().astype", "numpy.asarray"], "methods", ["None"], ["", "@", "staticmethod", "\n", "def", "convert", "(", "obs", ")", ":", "\n", "        ", "return", "np", ".", "expand_dims", "(", "np", ".", "asarray", "(", "obs", ")", ".", "astype", "(", "np", ".", "float64", ")", ",", "axis", "=", "0", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.az79nefy_OrdinalRL.dqn.atari_dqn_multi_net_agent.DQNAgent.check_win_condition": [[109, 115], ["None"], "methods", ["None"], ["", "@", "staticmethod", "\n", "def", "check_win_condition", "(", "reward", ",", "episode_reward", ",", "done", ")", ":", "\n", "        ", "if", "done", "and", "episode_reward", ">", "20", ":", "\n", "            ", "return", "True", "\n", "", "else", ":", "\n", "            ", "return", "False", "\n", "\n"]], "home.repos.pwc.inspect_result.az79nefy_OrdinalRL.dqn.atari_dqn_multi_net_agent.DQNAgent.evaluate": [[116, 124], ["round", "round", "atari_dqn_multi_net_agent.DQNAgent.average_rewards.append", "atari_dqn_multi_net_agent.DQNAgent.win_rates.append", "print", "sum", "len", "sum", "len"], "methods", ["None"], ["", "", "def", "evaluate", "(", "self", ",", "i_episode", ",", "episode_rewards", ",", "episode_wins", ")", ":", "\n", "# compute average episode reward and win rate over last episodes", "\n", "        ", "average_reward", "=", "round", "(", "sum", "(", "episode_rewards", ")", "/", "len", "(", "episode_rewards", ")", ",", "2", ")", "\n", "win_rate", "=", "round", "(", "sum", "(", "episode_wins", ")", "/", "len", "(", "episode_wins", ")", ",", "2", ")", "\n", "# store average episode reward and win rate over last episodes for plotting purposes", "\n", "self", ".", "average_rewards", ".", "append", "(", "average_reward", ")", "\n", "self", ".", "win_rates", ".", "append", "(", "win_rate", ")", "\n", "print", "(", "\"{}\\t{}\\t{}\"", ".", "format", "(", "i_episode", "+", "1", ",", "average_reward", ",", "win_rate", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.az79nefy_OrdinalRL.dqn.atari_dqn_multi_net_agent.DQNAgent.plot": [[126, 140], ["matplotlib.figure", "matplotlib.plot", "matplotlib.xlabel", "matplotlib.ylabel", "matplotlib.figure", "matplotlib.plot", "matplotlib.xlabel", "matplotlib.ylabel", "matplotlib.show", "list", "list", "range", "range"], "methods", ["home.repos.pwc.inspect_result.az79nefy_OrdinalRL.dqn.atari_dqn_single_net_agent.DQNAgent.plot", "home.repos.pwc.inspect_result.az79nefy_OrdinalRL.dqn.atari_dqn_single_net_agent.DQNAgent.plot"], ["", "def", "plot", "(", "self", ",", "n_episodes", ",", "step_size", ")", ":", "\n", "# plot win rate", "\n", "        ", "plt", ".", "figure", "(", ")", "\n", "plt", ".", "plot", "(", "list", "(", "range", "(", "step_size", ",", "n_episodes", "+", "step_size", ",", "step_size", ")", ")", ",", "self", ".", "win_rates", ")", "\n", "plt", ".", "xlabel", "(", "'Number of episodes'", ")", "\n", "plt", ".", "ylabel", "(", "'Win rate'", ")", "\n", "\n", "# plot average score", "\n", "plt", ".", "figure", "(", ")", "\n", "plt", ".", "plot", "(", "list", "(", "range", "(", "step_size", ",", "n_episodes", "+", "step_size", ",", "step_size", ")", ")", ",", "self", ".", "average_rewards", ")", "\n", "plt", ".", "xlabel", "(", "'Number of episodes'", ")", "\n", "plt", ".", "ylabel", "(", "'Average score'", ")", "\n", "\n", "plt", ".", "show", "(", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.az79nefy_OrdinalRL.dqn.atari_ordinal_dqn_agent.DQNAgent.__init__": [[11, 31], ["collections.deque", "atari_ordinal_dqn_agent.DQNAgent.build_model", "atari_ordinal_dqn_agent.DQNAgent.build_model", "range", "range"], "methods", ["home.repos.pwc.inspect_result.az79nefy_OrdinalRL.dqn.atari_dqn_single_net_agent.DQNAgent.build_model", "home.repos.pwc.inspect_result.az79nefy_OrdinalRL.dqn.atari_dqn_single_net_agent.DQNAgent.build_model"], ["    ", "def", "__init__", "(", "self", ",", "alpha", ",", "gamma", ",", "epsilon", ",", "epsilon_min", ",", "n_actions", ",", "n_ordinals", ",", "observation_dim", ",", "batch_size", ",", "memory_len", ",", "replace_target_iter", ")", ":", "\n", "        ", "self", ".", "alpha", "=", "alpha", "\n", "self", ".", "gamma", "=", "gamma", "\n", "self", ".", "epsilon", "=", "epsilon", "\n", "self", ".", "epsilon_min", "=", "epsilon_min", "\n", "self", ".", "n_actions", "=", "n_actions", "\n", "self", ".", "n_ordinals", "=", "n_ordinals", "\n", "self", ".", "n_inputs", "=", "observation_dim", "\n", "\n", "self", ".", "batch_size", "=", "batch_size", "\n", "self", ".", "memory", "=", "deque", "(", "maxlen", "=", "memory_len", ")", "\n", "# creation of a neural net for every action", "\n", "self", ".", "eval_action_nets", "=", "[", "self", ".", "build_model", "(", "self", ".", "n_inputs", ")", "for", "_", "in", "range", "(", "n_actions", ")", "]", "\n", "self", ".", "target_action_nets", "=", "[", "self", ".", "build_model", "(", "self", ".", "n_inputs", ")", "for", "_", "in", "range", "(", "n_actions", ")", "]", "\n", "self", ".", "replace_target_iter", "=", "replace_target_iter", "\n", "\n", "self", ".", "update_counter", "=", "0", "\n", "self", ".", "replay_counter", "=", "0", "\n", "self", ".", "win_rates", "=", "[", "]", "\n", "self", ".", "average_rewards", "=", "[", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.az79nefy_OrdinalRL.dqn.atari_ordinal_dqn_agent.DQNAgent.build_model": [[33, 46], ["keras.models.Sequential", "keras.models.Sequential.add", "keras.models.Sequential.add", "keras.models.Sequential.add", "keras.models.Sequential.add", "keras.models.Sequential.add", "keras.models.Sequential.add", "keras.models.Sequential.compile", "keras.layers.Conv2D", "keras.layers.Conv2D", "keras.layers.Conv2D", "keras.layers.Flatten", "keras.layers.Dense", "keras.layers.Dense", "keras.optimizers.RMSprop"], "methods", ["None"], ["", "def", "build_model", "(", "self", ",", "n_inputs", ")", ":", "\n", "        ", "neural_net", "=", "Sequential", "(", ")", "\n", "neural_net", ".", "add", "(", "Conv2D", "(", "32", ",", "8", ",", "strides", "=", "(", "4", ",", "4", ")", ",", "\n", "padding", "=", "'valid'", ",", "activation", "=", "'relu'", ",", "input_shape", "=", "n_inputs", ",", "data_format", "=", "'channels_first'", ")", ")", "\n", "neural_net", ".", "add", "(", "Conv2D", "(", "64", ",", "4", ",", "strides", "=", "(", "2", ",", "2", ")", ",", "\n", "padding", "=", "'valid'", ",", "activation", "=", "'relu'", ",", "input_shape", "=", "n_inputs", ",", "data_format", "=", "'channels_first'", ")", ")", "\n", "neural_net", ".", "add", "(", "Conv2D", "(", "64", ",", "3", ",", "strides", "=", "(", "1", ",", "1", ")", ",", "\n", "padding", "=", "'valid'", ",", "activation", "=", "'relu'", ",", "input_shape", "=", "n_inputs", ",", "data_format", "=", "'channels_first'", ")", ")", "\n", "neural_net", ".", "add", "(", "Flatten", "(", ")", ")", "\n", "neural_net", ".", "add", "(", "Dense", "(", "512", ",", "activation", "=", "'relu'", ")", ")", "\n", "neural_net", ".", "add", "(", "Dense", "(", "self", ".", "n_ordinals", ")", ")", "\n", "neural_net", ".", "compile", "(", "loss", "=", "'mse'", ",", "optimizer", "=", "RMSprop", "(", "lr", "=", "self", ".", "alpha", ",", "rho", "=", "0.95", ",", "epsilon", "=", "0.01", ")", ",", "metrics", "=", "[", "'accuracy'", "]", ")", "\n", "return", "neural_net", "\n", "\n"]], "home.repos.pwc.inspect_result.az79nefy_OrdinalRL.dqn.atari_ordinal_dqn_agent.DQNAgent.update": [[47, 53], ["atari_ordinal_dqn_agent.DQNAgent.reward_to_ordinal", "atari_ordinal_dqn_agent.DQNAgent.remember", "len", "atari_ordinal_dqn_agent.DQNAgent.replay"], "methods", ["home.repos.pwc.inspect_result.az79nefy_OrdinalRL.dqn.ordinal_dqn_discretized_agent.DQNAgent.reward_to_ordinal", "home.repos.pwc.inspect_result.az79nefy_OrdinalRL.dqn.atari_dqn_single_net_agent.DQNAgent.remember", "home.repos.pwc.inspect_result.az79nefy_OrdinalRL.dqn.atari_dqn_single_net_agent.DQNAgent.replay"], ["", "def", "update", "(", "self", ",", "prev_obs", ",", "prev_act", ",", "obs", ",", "reward", ",", "episode_reward", ",", "done", ")", ":", "\n", "        ", "ordinal", "=", "self", ".", "reward_to_ordinal", "(", "reward", ",", "episode_reward", ",", "done", ")", "\n", "self", ".", "update_counter", "+=", "1", "\n", "self", ".", "remember", "(", "prev_obs", ",", "prev_act", ",", "obs", ",", "ordinal", ",", "done", ")", "\n", "if", "len", "(", "self", ".", "memory", ")", ">", "self", ".", "batch_size", ":", "\n", "            ", "self", ".", "replay", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.az79nefy_OrdinalRL.dqn.atari_ordinal_dqn_agent.DQNAgent.remember": [[54, 56], ["atari_ordinal_dqn_agent.DQNAgent.memory.append"], "methods", ["None"], ["", "", "def", "remember", "(", "self", ",", "prev_obs", ",", "prev_act", ",", "obs", ",", "rew", ",", "d", ")", ":", "\n", "        ", "self", ".", "memory", ".", "append", "(", "(", "prev_obs", ",", "prev_act", ",", "obs", ",", "rew", ",", "d", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.az79nefy_OrdinalRL.dqn.atari_ordinal_dqn_agent.DQNAgent.replay": [[57, 80], ["random.sample", "range", "range", "atari_ordinal_dqn_agent.DQNAgent.get_greedy_action", "x_batch[].append", "y_batch[].append", "atari_ordinal_dqn_agent.DQNAgent.target_action_nets[].set_weights", "numpy.zeros", "len", "atari_ordinal_dqn_agent.DQNAgent.eval_action_nets[].fit", "atari_ordinal_dqn_agent.DQNAgent.eval_action_nets[].get_weights", "range", "range", "atari_ordinal_dqn_agent.DQNAgent.convert", "numpy.array", "numpy.array", "atari_ordinal_dqn_agent.DQNAgent.target_action_nets[].predict", "len", "atari_ordinal_dqn_agent.DQNAgent.convert"], "methods", ["home.repos.pwc.inspect_result.az79nefy_OrdinalRL.dqn.atari_dqn_single_net_agent.DQNAgent.get_greedy_action", "home.repos.pwc.inspect_result.az79nefy_OrdinalRL.dqn.atari_dqn_single_net_agent.DQNAgent.convert", "home.repos.pwc.inspect_result.az79nefy_OrdinalRL.dqn.atari_dqn_single_net_agent.DQNAgent.convert"], ["", "def", "replay", "(", "self", ")", ":", "\n", "# copy evaluation action nets to target action nets at first replay and then every 200 replay steps", "\n", "        ", "if", "self", ".", "replay_counter", "%", "self", ".", "replace_target_iter", "==", "0", ":", "\n", "            ", "for", "a", "in", "range", "(", "self", ".", "n_actions", ")", ":", "\n", "                ", "self", ".", "target_action_nets", "[", "a", "]", ".", "set_weights", "(", "self", ".", "eval_action_nets", "[", "a", "]", ".", "get_weights", "(", ")", ")", "\n", "", "", "self", ".", "replay_counter", "+=", "1", "\n", "\n", "mini_batch", "=", "random", ".", "sample", "(", "self", ".", "memory", ",", "self", ".", "batch_size", ")", "\n", "x_batch", ",", "y_batch", "=", "[", "[", "]", "for", "_", "in", "range", "(", "self", ".", "n_actions", ")", "]", ",", "[", "[", "]", "for", "_", "in", "range", "(", "self", ".", "n_actions", ")", "]", "\n", "for", "prev_obs", ",", "prev_act", ",", "obs", ",", "ordinal", ",", "d", "in", "mini_batch", ":", "\n", "            ", "greedy_action", "=", "self", ".", "get_greedy_action", "(", "obs", ")", "\n", "if", "not", "d", ":", "\n", "                ", "target", "=", "self", ".", "gamma", "*", "self", ".", "target_action_nets", "[", "greedy_action", "]", ".", "predict", "(", "self", ".", "convert", "(", "obs", ")", ")", "[", "0", "]", "\n", "target", "[", "ordinal", "]", "+=", "1", "\n", "", "else", ":", "\n", "                ", "target", "=", "np", ".", "zeros", "(", "self", ".", "n_ordinals", ")", "\n", "target", "[", "ordinal", "]", "+=", "1", "\n", "# fit predicted value of previous action in previous observation to target value of max_action", "\n", "", "x_batch", "[", "prev_act", "]", ".", "append", "(", "self", ".", "convert", "(", "prev_obs", ")", "[", "0", "]", ")", "\n", "y_batch", "[", "prev_act", "]", ".", "append", "(", "target", ")", "\n", "", "for", "a", "in", "range", "(", "self", ".", "n_actions", ")", ":", "\n", "            ", "if", "len", "(", "x_batch", "[", "a", "]", ")", "!=", "0", ":", "\n", "                ", "self", ".", "eval_action_nets", "[", "a", "]", ".", "fit", "(", "np", ".", "array", "(", "x_batch", "[", "a", "]", ")", ",", "np", ".", "array", "(", "y_batch", "[", "a", "]", ")", ",", "batch_size", "=", "len", "(", "x_batch", "[", "a", "]", ")", ",", "verbose", "=", "0", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.az79nefy_OrdinalRL.dqn.atari_ordinal_dqn_agent.DQNAgent.compute_borda_scores": [[82, 138], ["numpy.zeros", "range", "numpy.array", "numpy.count_nonzero", "range", "range", "atari_ordinal_dqn_agent.DQNAgent.eval_action_nets[].predict", "ordinal_values_per_action[].append", "borda_scores.append", "borda_scores.append", "range", "borda_scores.append", "atari_ordinal_dqn_agent.DQNAgent.convert", "range"], "methods", ["home.repos.pwc.inspect_result.az79nefy_OrdinalRL.dqn.atari_dqn_single_net_agent.DQNAgent.convert"], ["", "", "", "def", "compute_borda_scores", "(", "self", ",", "obs", ")", ":", "\n", "# sum up all ordinal values per action for given observation", "\n", "        ", "ordinal_value_sum_per_action", "=", "np", ".", "zeros", "(", "self", ".", "n_actions", ")", "\n", "ordinal_values_per_action", "=", "[", "[", "]", "for", "_", "in", "range", "(", "self", ".", "n_actions", ")", "]", "\n", "for", "action_a", "in", "range", "(", "self", ".", "n_actions", ")", ":", "\n", "            ", "for", "ordinal_value", "in", "self", ".", "eval_action_nets", "[", "action_a", "]", ".", "predict", "(", "self", ".", "convert", "(", "obs", ")", ")", "[", "0", "]", ":", "\n", "                ", "ordinal_value_sum_per_action", "[", "action_a", "]", "+=", "ordinal_value", "\n", "ordinal_values_per_action", "[", "action_a", "]", ".", "append", "(", "ordinal_value", ")", "\n", "", "", "ordinal_values_per_action", "=", "np", ".", "array", "(", "ordinal_values_per_action", ")", "\n", "\n", "# count actions whose ordinal value sum is not zero (no comparision possible for actions without ordinal_value)", "\n", "non_zero_action_count", "=", "np", ".", "count_nonzero", "(", "ordinal_value_sum_per_action", ")", "\n", "actions_to_compare_count", "=", "non_zero_action_count", "-", "1", "\n", "\n", "borda_scores", "=", "[", "]", "\n", "# compute borda_values for action_a (probability that action_a wins against any other action)", "\n", "for", "action_a", "in", "range", "(", "self", ".", "n_actions", ")", ":", "\n", "# if action has not yet recorded any ordinal values, action has to be played (set borda_value to 1.0)", "\n", "            ", "if", "ordinal_value_sum_per_action", "[", "action_a", "]", "==", "0", ":", "\n", "                ", "borda_scores", ".", "append", "(", "1.0", ")", "\n", "continue", "\n", "\n", "", "if", "actions_to_compare_count", "<", "1", ":", "\n", "# set lower than 1.0 (borda_value for zero_actions is 1.0)", "\n", "                ", "borda_scores", ".", "append", "(", "0.5", ")", "\n", "", "else", ":", "\n", "# over all actions: sum up the probabilities that action_a wins against the given action", "\n", "                ", "winning_probability_a_sum", "=", "0", "\n", "# compare action_a to all other actions", "\n", "for", "action_b", "in", "range", "(", "self", ".", "n_actions", ")", ":", "\n", "                    ", "if", "action_a", "==", "action_b", ":", "\n", "                        ", "continue", "\n", "# not comparable if action_b has no ordinal_values", "\n", "", "if", "ordinal_value_sum_per_action", "[", "action_b", "]", "==", "0", ":", "\n", "                        ", "continue", "\n", "", "else", ":", "\n", "# probability that action_a wins against action_b", "\n", "                        ", "winning_probability_a", "=", "0", "\n", "# running ordinal probability that action_b is worse than current investigated ordinal", "\n", "worse_probability_b", "=", "0", "\n", "# predict ordinal values for action a and b", "\n", "ordinal_values_a", "=", "ordinal_values_per_action", "[", "action_a", "]", "\n", "ordinal_values_b", "=", "ordinal_values_per_action", "[", "action_b", "]", "\n", "for", "ordinal_count", "in", "range", "(", "self", ".", "n_ordinals", ")", ":", "\n", "                            ", "ordinal_probability_a", "=", "ordinal_values_a", "[", "ordinal_count", "]", "/", "ordinal_value_sum_per_action", "[", "action_a", "]", "\n", "# ordinal_probability_b is also the tie probability", "\n", "ordinal_probability_b", "=", "(", "ordinal_values_b", "[", "ordinal_count", "]", "/", "\n", "ordinal_value_sum_per_action", "[", "action_b", "]", ")", "\n", "winning_probability_a", "+=", "ordinal_probability_a", "*", "(", "worse_probability_b", "+", "ordinal_probability_b", "/", "2.0", ")", "\n", "worse_probability_b", "+=", "ordinal_probability_b", "\n", "", "winning_probability_a_sum", "+=", "winning_probability_a", "\n", "# normalize summed up probabilities with number of actions that have been compared", "\n", "", "", "borda_scores", ".", "append", "(", "winning_probability_a_sum", "/", "actions_to_compare_count", ")", "\n", "", "", "return", "borda_scores", "\n", "\n"]], "home.repos.pwc.inspect_result.az79nefy_OrdinalRL.dqn.atari_ordinal_dqn_agent.DQNAgent.get_greedy_action": [[139, 141], ["numpy.argmax", "atari_ordinal_dqn_agent.DQNAgent.compute_borda_scores"], "methods", ["home.repos.pwc.inspect_result.az79nefy_OrdinalRL.dqn.ordinal_dqn_discretized_agent.DQNAgent.compute_borda_scores"], ["", "def", "get_greedy_action", "(", "self", ",", "obs", ")", ":", "\n", "        ", "return", "np", ".", "argmax", "(", "self", ".", "compute_borda_scores", "(", "obs", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.az79nefy_OrdinalRL.dqn.atari_ordinal_dqn_agent.DQNAgent.choose_action": [[143, 151], ["atari_ordinal_dqn_agent.DQNAgent.get_greedy_action", "random.randrange", "random.random"], "methods", ["home.repos.pwc.inspect_result.az79nefy_OrdinalRL.dqn.atari_dqn_single_net_agent.DQNAgent.get_greedy_action"], ["", "def", "choose_action", "(", "self", ",", "obs", ",", "greedy", ")", ":", "\n", "        ", "greedy_action", "=", "self", ".", "get_greedy_action", "(", "obs", ")", "\n", "# choose random action with probability epsilon", "\n", "if", "not", "greedy", "and", "random", ".", "random", "(", ")", "<", "self", ".", "epsilon", ":", "\n", "            ", "return", "random", ".", "randrange", "(", "self", ".", "n_actions", ")", "\n", "# greedy action is chosen with probability (1 - epsilon)", "\n", "", "else", ":", "\n", "            ", "return", "greedy_action", "\n", "\n"]], "home.repos.pwc.inspect_result.az79nefy_OrdinalRL.dqn.atari_ordinal_dqn_agent.DQNAgent.end_episode": [[152, 155], ["None"], "methods", ["None"], ["", "", "def", "end_episode", "(", "self", ",", "n_episodes", ")", ":", "\n", "# gradually reduce epsilon after every done episode", "\n", "        ", "self", ".", "epsilon", "=", "self", ".", "epsilon", "-", "2", "/", "n_episodes", "if", "self", ".", "epsilon", ">", "self", ".", "epsilon_min", "else", "self", ".", "epsilon_min", "\n", "\n"]], "home.repos.pwc.inspect_result.az79nefy_OrdinalRL.dqn.atari_ordinal_dqn_agent.DQNAgent.preprocess_observation": [[156, 158], ["None"], "methods", ["None"], ["", "def", "preprocess_observation", "(", "self", ",", "obs", ")", ":", "\n", "        ", "return", "obs", "\n", "\n"]], "home.repos.pwc.inspect_result.az79nefy_OrdinalRL.dqn.atari_ordinal_dqn_agent.DQNAgent.convert": [[159, 162], ["numpy.expand_dims", "numpy.asarray().astype", "numpy.asarray"], "methods", ["None"], ["", "@", "staticmethod", "\n", "def", "convert", "(", "obs", ")", ":", "\n", "        ", "return", "np", ".", "expand_dims", "(", "np", ".", "asarray", "(", "obs", ")", ".", "astype", "(", "np", ".", "float64", ")", ",", "axis", "=", "0", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.az79nefy_OrdinalRL.dqn.atari_ordinal_dqn_agent.DQNAgent.reward_to_ordinal": [[164, 171], ["None"], "methods", ["None"], ["", "def", "reward_to_ordinal", "(", "self", ",", "reward", ",", "episode_reward", ",", "done", ")", ":", "\n", "        ", "if", "reward", "==", "-", "1", ":", "\n", "            ", "return", "0", "\n", "", "elif", "reward", "==", "0", ":", "\n", "            ", "return", "0", "\n", "", "else", ":", "\n", "            ", "return", "1", "\n", "\n"]], "home.repos.pwc.inspect_result.az79nefy_OrdinalRL.dqn.atari_ordinal_dqn_agent.DQNAgent.check_win_condition": [[173, 179], ["None"], "methods", ["None"], ["", "", "@", "staticmethod", "\n", "def", "check_win_condition", "(", "reward", ",", "episode_reward", ",", "done", ")", ":", "\n", "        ", "if", "done", "and", "episode_reward", ">", "20", ":", "\n", "            ", "return", "True", "\n", "", "else", ":", "\n", "            ", "return", "False", "\n", "\n"]], "home.repos.pwc.inspect_result.az79nefy_OrdinalRL.dqn.atari_ordinal_dqn_agent.DQNAgent.evaluate": [[180, 188], ["round", "round", "atari_ordinal_dqn_agent.DQNAgent.average_rewards.append", "atari_ordinal_dqn_agent.DQNAgent.win_rates.append", "print", "sum", "len", "sum", "len"], "methods", ["None"], ["", "", "def", "evaluate", "(", "self", ",", "i_episode", ",", "episode_rewards", ",", "episode_wins", ")", ":", "\n", "# compute average episode reward and win rate over last episodes", "\n", "        ", "average_reward", "=", "round", "(", "sum", "(", "episode_rewards", ")", "/", "len", "(", "episode_rewards", ")", ",", "2", ")", "\n", "win_rate", "=", "round", "(", "sum", "(", "episode_wins", ")", "/", "len", "(", "episode_wins", ")", ",", "2", ")", "\n", "# store average episode reward and win rate over last episodes for plotting purposes", "\n", "self", ".", "average_rewards", ".", "append", "(", "average_reward", ")", "\n", "self", ".", "win_rates", ".", "append", "(", "win_rate", ")", "\n", "print", "(", "\"{}\\t{}\\t{}\"", ".", "format", "(", "i_episode", "+", "1", ",", "average_reward", ",", "win_rate", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.az79nefy_OrdinalRL.dqn.atari_ordinal_dqn_agent.DQNAgent.plot": [[190, 204], ["matplotlib.figure", "matplotlib.plot", "matplotlib.xlabel", "matplotlib.ylabel", "matplotlib.figure", "matplotlib.plot", "matplotlib.xlabel", "matplotlib.ylabel", "matplotlib.show", "list", "list", "range", "range"], "methods", ["home.repos.pwc.inspect_result.az79nefy_OrdinalRL.dqn.atari_dqn_single_net_agent.DQNAgent.plot", "home.repos.pwc.inspect_result.az79nefy_OrdinalRL.dqn.atari_dqn_single_net_agent.DQNAgent.plot"], ["", "def", "plot", "(", "self", ",", "n_episodes", ",", "step_size", ")", ":", "\n", "# plot win rate", "\n", "        ", "plt", ".", "figure", "(", ")", "\n", "plt", ".", "plot", "(", "list", "(", "range", "(", "step_size", ",", "n_episodes", "+", "step_size", ",", "step_size", ")", ")", ",", "self", ".", "win_rates", ")", "\n", "plt", ".", "xlabel", "(", "'Number of episodes'", ")", "\n", "plt", ".", "ylabel", "(", "'Win rate'", ")", "\n", "\n", "# plot average score", "\n", "plt", ".", "figure", "(", ")", "\n", "plt", ".", "plot", "(", "list", "(", "range", "(", "step_size", ",", "n_episodes", "+", "step_size", ",", "step_size", ")", ")", ",", "self", ".", "average_rewards", ")", "\n", "plt", ".", "xlabel", "(", "'Number of episodes'", ")", "\n", "plt", ".", "ylabel", "(", "'Average score'", ")", "\n", "\n", "plt", ".", "show", "(", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.az79nefy_OrdinalRL.dqn.dqn_single_net_discretized_agent.DQNAgent.__init__": [[11, 28], ["collections.deque", "dqn_single_net_discretized_agent.DQNAgent.build_model", "dqn_single_net_discretized_agent.DQNAgent.build_model"], "methods", ["home.repos.pwc.inspect_result.az79nefy_OrdinalRL.dqn.atari_dqn_single_net_agent.DQNAgent.build_model", "home.repos.pwc.inspect_result.az79nefy_OrdinalRL.dqn.atari_dqn_single_net_agent.DQNAgent.build_model"], ["    ", "def", "__init__", "(", "self", ",", "alpha", ",", "gamma", ",", "epsilon", ",", "epsilon_min", ",", "n_actions", ",", "n_ordinals", ",", "observation_dim", ",", "batch_size", ",", "memory_len", ",", "replace_target_iter", ")", ":", "\n", "        ", "self", ".", "alpha", "=", "alpha", "\n", "self", ".", "gamma", "=", "gamma", "\n", "self", ".", "epsilon", "=", "epsilon", "\n", "self", ".", "epsilon_min", "=", "epsilon_min", "\n", "self", ".", "n_actions", "=", "n_actions", "\n", "self", ".", "n_inputs", "=", "observation_dim", "\n", "\n", "self", ".", "batch_size", "=", "batch_size", "\n", "self", ".", "memory", "=", "deque", "(", "maxlen", "=", "memory_len", ")", "\n", "self", ".", "eval_model", "=", "self", ".", "build_model", "(", "self", ".", "n_inputs", ")", "\n", "self", ".", "target_model", "=", "self", ".", "build_model", "(", "self", ".", "n_inputs", ")", "\n", "self", ".", "replace_target_iter", "=", "replace_target_iter", "\n", "\n", "self", ".", "replay_counter", "=", "0", "\n", "self", ".", "win_rates", "=", "[", "]", "\n", "self", ".", "average_rewards", "=", "[", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.az79nefy_OrdinalRL.dqn.dqn_single_net_discretized_agent.DQNAgent.build_model": [[30, 37], ["keras.models.Sequential", "keras.models.Sequential.add", "keras.models.Sequential.add", "keras.models.Sequential.add", "keras.models.Sequential.compile", "keras.layers.Dense", "keras.layers.Dense", "keras.layers.Dense", "keras.optimizers.Adam"], "methods", ["None"], ["", "def", "build_model", "(", "self", ",", "n_inputs", ")", ":", "\n", "        ", "neural_net", "=", "Sequential", "(", ")", "\n", "neural_net", ".", "add", "(", "Dense", "(", "24", ",", "input_dim", "=", "n_inputs", ",", "activation", "=", "'relu'", ")", ")", "\n", "neural_net", ".", "add", "(", "Dense", "(", "24", ",", "activation", "=", "'relu'", ")", ")", "\n", "neural_net", ".", "add", "(", "Dense", "(", "self", ".", "n_actions", ",", "activation", "=", "'linear'", ")", ")", "\n", "neural_net", ".", "compile", "(", "loss", "=", "'mse'", ",", "optimizer", "=", "Adam", "(", "lr", "=", "self", ".", "alpha", ")", ")", "\n", "return", "neural_net", "\n", "\n"]], "home.repos.pwc.inspect_result.az79nefy_OrdinalRL.dqn.dqn_single_net_discretized_agent.DQNAgent.update": [[38, 43], ["dqn_single_net_discretized_agent.DQNAgent.remap_reward", "dqn_single_net_discretized_agent.DQNAgent.remember", "len", "dqn_single_net_discretized_agent.DQNAgent.replay"], "methods", ["home.repos.pwc.inspect_result.az79nefy_OrdinalRL.dqn.dqn_single_net_agent.DQNAgent.remap_reward", "home.repos.pwc.inspect_result.az79nefy_OrdinalRL.dqn.atari_dqn_single_net_agent.DQNAgent.remember", "home.repos.pwc.inspect_result.az79nefy_OrdinalRL.dqn.atari_dqn_single_net_agent.DQNAgent.replay"], ["", "def", "update", "(", "self", ",", "prev_obs", ",", "prev_act", ",", "obs", ",", "reward", ",", "episode_reward", ",", "done", ")", ":", "\n", "        ", "reward", "=", "self", ".", "remap_reward", "(", "reward", ",", "episode_reward", ",", "done", ")", "\n", "self", ".", "remember", "(", "prev_obs", ",", "prev_act", ",", "obs", ",", "reward", ",", "done", ")", "\n", "if", "len", "(", "self", ".", "memory", ")", ">", "self", ".", "batch_size", ":", "\n", "            ", "self", ".", "replay", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.az79nefy_OrdinalRL.dqn.dqn_single_net_discretized_agent.DQNAgent.remember": [[44, 46], ["dqn_single_net_discretized_agent.DQNAgent.memory.append"], "methods", ["None"], ["", "", "def", "remember", "(", "self", ",", "prev_obs", ",", "prev_act", ",", "obs", ",", "rew", ",", "d", ")", ":", "\n", "        ", "self", ".", "memory", ".", "append", "(", "(", "prev_obs", ",", "prev_act", ",", "obs", ",", "rew", ",", "d", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.az79nefy_OrdinalRL.dqn.dqn_single_net_discretized_agent.DQNAgent.replay": [[47, 67], ["random.sample", "dqn_single_net_discretized_agent.DQNAgent.eval_model.fit", "dqn_single_net_discretized_agent.DQNAgent.target_model.set_weights", "dqn_single_net_discretized_agent.DQNAgent.eval_model.predict", "x_batch.append", "y_batch.append", "numpy.array", "numpy.array", "dqn_single_net_discretized_agent.DQNAgent.eval_model.get_weights", "numpy.argmax", "len", "dqn_single_net_discretized_agent.DQNAgent.eval_model.predict", "dqn_single_net_discretized_agent.DQNAgent.target_model.predict"], "methods", ["None"], ["", "def", "replay", "(", "self", ")", ":", "\n", "# copy evaluation model to target model at first replay and then every 200 replay steps", "\n", "        ", "if", "self", ".", "replay_counter", "%", "self", ".", "replace_target_iter", "==", "0", ":", "\n", "            ", "self", ".", "target_model", ".", "set_weights", "(", "self", ".", "eval_model", ".", "get_weights", "(", ")", ")", "\n", "", "self", ".", "replay_counter", "+=", "1", "\n", "\n", "mini_batch", "=", "random", ".", "sample", "(", "self", ".", "memory", ",", "self", ".", "batch_size", ")", "\n", "x_batch", ",", "y_batch", "=", "[", "]", ",", "[", "]", "\n", "for", "prev_obs", ",", "prev_act", ",", "obs", ",", "rew", ",", "d", "in", "mini_batch", ":", "\n", "            ", "prediction", "=", "self", ".", "eval_model", ".", "predict", "(", "prev_obs", ")", "\n", "if", "not", "d", ":", "\n", "                ", "best_act", "=", "np", ".", "argmax", "(", "self", ".", "eval_model", ".", "predict", "(", "obs", ")", "[", "0", "]", ")", "\n", "target", "=", "rew", "+", "self", ".", "gamma", "*", "self", ".", "target_model", ".", "predict", "(", "obs", ")", "[", "0", ",", "best_act", "]", "\n", "", "else", ":", "\n", "                ", "target", "=", "rew", "\n", "# fit predicted value of previous action in previous observation to target value of max_action", "\n", "", "prediction", "[", "0", "]", "[", "prev_act", "]", "=", "target", "\n", "x_batch", ".", "append", "(", "prev_obs", "[", "0", "]", ")", "\n", "y_batch", ".", "append", "(", "prediction", "[", "0", "]", ")", "\n", "", "self", ".", "eval_model", ".", "fit", "(", "np", ".", "array", "(", "x_batch", ")", ",", "np", ".", "array", "(", "y_batch", ")", ",", "batch_size", "=", "len", "(", "x_batch", ")", ",", "verbose", "=", "0", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.az79nefy_OrdinalRL.dqn.dqn_single_net_discretized_agent.DQNAgent.get_greedy_action": [[68, 70], ["numpy.argmax", "dqn_single_net_discretized_agent.DQNAgent.eval_model.predict"], "methods", ["None"], ["", "def", "get_greedy_action", "(", "self", ",", "obs", ")", ":", "\n", "        ", "return", "np", ".", "argmax", "(", "self", ".", "eval_model", ".", "predict", "(", "obs", ")", "[", "0", "]", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.az79nefy_OrdinalRL.dqn.dqn_single_net_discretized_agent.DQNAgent.choose_action": [[72, 80], ["dqn_single_net_discretized_agent.DQNAgent.get_greedy_action", "random.randrange", "random.random"], "methods", ["home.repos.pwc.inspect_result.az79nefy_OrdinalRL.dqn.atari_dqn_single_net_agent.DQNAgent.get_greedy_action"], ["", "def", "choose_action", "(", "self", ",", "obs", ",", "greedy", ")", ":", "\n", "        ", "greedy_action", "=", "self", ".", "get_greedy_action", "(", "obs", ")", "\n", "# choose random action with probability epsilon", "\n", "if", "not", "greedy", "and", "random", ".", "random", "(", ")", "<", "self", ".", "epsilon", ":", "\n", "            ", "return", "random", ".", "randrange", "(", "self", ".", "n_actions", ")", "\n", "# greedy action is chosen with probability (1 - epsilon)", "\n", "", "else", ":", "\n", "            ", "return", "greedy_action", "\n", "\n"]], "home.repos.pwc.inspect_result.az79nefy_OrdinalRL.dqn.dqn_single_net_discretized_agent.DQNAgent.end_episode": [[81, 84], ["None"], "methods", ["None"], ["", "", "def", "end_episode", "(", "self", ",", "n_episodes", ")", ":", "\n", "# gradually reduce epsilon after every done episode", "\n", "        ", "self", ".", "epsilon", "=", "self", ".", "epsilon", "-", "2", "/", "n_episodes", "if", "self", ".", "epsilon", ">", "self", ".", "epsilon_min", "else", "self", ".", "epsilon_min", "\n", "\n"]], "home.repos.pwc.inspect_result.az79nefy_OrdinalRL.dqn.dqn_single_net_discretized_agent.DQNAgent.preprocess_observation": [[85, 88], ["numpy.expand_dims"], "methods", ["None"], ["", "@", "staticmethod", "\n", "def", "preprocess_observation", "(", "obs", ")", ":", "\n", "        ", "return", "np", ".", "expand_dims", "(", "obs", ",", "axis", "=", "0", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.az79nefy_OrdinalRL.dqn.dqn_single_net_discretized_agent.DQNAgent.remap_reward": [[90, 95], ["dqn_single_net_discretized_agent.DQNAgent.check_win_condition"], "methods", ["home.repos.pwc.inspect_result.az79nefy_OrdinalRL.dqn.atari_dqn_single_net_agent.DQNAgent.check_win_condition"], ["", "def", "remap_reward", "(", "self", ",", "reward", ",", "episode_reward", ",", "done", ")", ":", "\n", "        ", "if", "done", "and", "not", "self", ".", "check_win_condition", "(", "reward", ",", "episode_reward", ",", "done", ")", ":", "\n", "            ", "return", "-", "1", "\n", "", "else", ":", "\n", "            ", "return", "0", "\n", "\n"]], "home.repos.pwc.inspect_result.az79nefy_OrdinalRL.dqn.dqn_single_net_discretized_agent.DQNAgent.check_win_condition": [[97, 103], ["None"], "methods", ["None"], ["", "", "@", "staticmethod", "\n", "def", "check_win_condition", "(", "reward", ",", "episode_reward", ",", "done", ")", ":", "\n", "        ", "if", "done", "and", "episode_reward", "==", "200", ":", "\n", "            ", "return", "True", "\n", "", "else", ":", "\n", "            ", "return", "False", "\n", "\n"]], "home.repos.pwc.inspect_result.az79nefy_OrdinalRL.dqn.dqn_single_net_discretized_agent.DQNAgent.evaluate": [[104, 112], ["round", "round", "dqn_single_net_discretized_agent.DQNAgent.average_rewards.append", "dqn_single_net_discretized_agent.DQNAgent.win_rates.append", "print", "sum", "len", "sum", "len"], "methods", ["None"], ["", "", "def", "evaluate", "(", "self", ",", "i_episode", ",", "episode_rewards", ",", "episode_wins", ")", ":", "\n", "# compute average episode reward and win rate over last episodes", "\n", "        ", "average_reward", "=", "round", "(", "sum", "(", "episode_rewards", ")", "/", "len", "(", "episode_rewards", ")", ",", "2", ")", "\n", "win_rate", "=", "round", "(", "sum", "(", "episode_wins", ")", "/", "len", "(", "episode_wins", ")", ",", "2", ")", "\n", "# store average episode reward and win rate over last episodes for plotting purposes", "\n", "self", ".", "average_rewards", ".", "append", "(", "average_reward", ")", "\n", "self", ".", "win_rates", ".", "append", "(", "win_rate", ")", "\n", "print", "(", "\"{}\\t{}\\t{}\"", ".", "format", "(", "i_episode", "+", "1", ",", "average_reward", ",", "win_rate", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.az79nefy_OrdinalRL.dqn.dqn_single_net_discretized_agent.DQNAgent.plot": [[114, 128], ["matplotlib.figure", "matplotlib.plot", "matplotlib.xlabel", "matplotlib.ylabel", "matplotlib.figure", "matplotlib.plot", "matplotlib.xlabel", "matplotlib.ylabel", "matplotlib.show", "list", "list", "range", "range"], "methods", ["home.repos.pwc.inspect_result.az79nefy_OrdinalRL.dqn.atari_dqn_single_net_agent.DQNAgent.plot", "home.repos.pwc.inspect_result.az79nefy_OrdinalRL.dqn.atari_dqn_single_net_agent.DQNAgent.plot"], ["", "def", "plot", "(", "self", ",", "n_episodes", ",", "step_size", ")", ":", "\n", "# plot win rate", "\n", "        ", "plt", ".", "figure", "(", ")", "\n", "plt", ".", "plot", "(", "list", "(", "range", "(", "step_size", ",", "n_episodes", "+", "step_size", ",", "step_size", ")", ")", ",", "self", ".", "win_rates", ")", "\n", "plt", ".", "xlabel", "(", "'Number of episodes'", ")", "\n", "plt", ".", "ylabel", "(", "'Win rate'", ")", "\n", "\n", "# plot average score", "\n", "plt", ".", "figure", "(", ")", "\n", "plt", ".", "plot", "(", "list", "(", "range", "(", "step_size", ",", "n_episodes", "+", "step_size", ",", "step_size", ")", ")", ",", "self", ".", "average_rewards", ")", "\n", "plt", ".", "xlabel", "(", "'Number of episodes'", ")", "\n", "plt", ".", "ylabel", "(", "'Average score'", ")", "\n", "\n", "plt", ".", "show", "(", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.az79nefy_OrdinalRL.dqn.dqn_single_net_agent.DQNAgent.__init__": [[11, 28], ["collections.deque", "dqn_single_net_agent.DQNAgent.build_model", "dqn_single_net_agent.DQNAgent.build_model"], "methods", ["home.repos.pwc.inspect_result.az79nefy_OrdinalRL.dqn.atari_dqn_single_net_agent.DQNAgent.build_model", "home.repos.pwc.inspect_result.az79nefy_OrdinalRL.dqn.atari_dqn_single_net_agent.DQNAgent.build_model"], ["    ", "def", "__init__", "(", "self", ",", "alpha", ",", "gamma", ",", "epsilon", ",", "epsilon_min", ",", "n_actions", ",", "n_ordinals", ",", "observation_dim", ",", "batch_size", ",", "memory_len", ",", "replace_target_iter", ")", ":", "\n", "        ", "self", ".", "alpha", "=", "alpha", "\n", "self", ".", "gamma", "=", "gamma", "\n", "self", ".", "epsilon", "=", "epsilon", "\n", "self", ".", "epsilon_min", "=", "epsilon_min", "\n", "self", ".", "n_actions", "=", "n_actions", "\n", "self", ".", "n_inputs", "=", "observation_dim", "\n", "\n", "self", ".", "batch_size", "=", "batch_size", "\n", "self", ".", "memory", "=", "deque", "(", "maxlen", "=", "memory_len", ")", "\n", "self", ".", "eval_model", "=", "self", ".", "build_model", "(", "self", ".", "n_inputs", ")", "\n", "self", ".", "target_model", "=", "self", ".", "build_model", "(", "self", ".", "n_inputs", ")", "\n", "self", ".", "replace_target_iter", "=", "replace_target_iter", "\n", "\n", "self", ".", "replay_counter", "=", "0", "\n", "self", ".", "win_rates", "=", "[", "]", "\n", "self", ".", "average_rewards", "=", "[", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.az79nefy_OrdinalRL.dqn.dqn_single_net_agent.DQNAgent.build_model": [[30, 37], ["keras.models.Sequential", "keras.models.Sequential.add", "keras.models.Sequential.add", "keras.models.Sequential.add", "keras.models.Sequential.compile", "keras.layers.Dense", "keras.layers.Dense", "keras.layers.Dense", "keras.optimizers.Adam"], "methods", ["None"], ["", "def", "build_model", "(", "self", ",", "n_inputs", ")", ":", "\n", "        ", "neural_net", "=", "Sequential", "(", ")", "\n", "neural_net", ".", "add", "(", "Dense", "(", "6", ",", "input_dim", "=", "n_inputs", ",", "activation", "=", "'relu'", ")", ")", "\n", "neural_net", ".", "add", "(", "Dense", "(", "6", ",", "activation", "=", "'relu'", ")", ")", "\n", "neural_net", ".", "add", "(", "Dense", "(", "self", ".", "n_actions", ",", "activation", "=", "'linear'", ")", ")", "\n", "neural_net", ".", "compile", "(", "loss", "=", "'mse'", ",", "optimizer", "=", "Adam", "(", "lr", "=", "self", ".", "alpha", ")", ")", "\n", "return", "neural_net", "\n", "\n"]], "home.repos.pwc.inspect_result.az79nefy_OrdinalRL.dqn.dqn_single_net_agent.DQNAgent.update": [[38, 43], ["dqn_single_net_agent.DQNAgent.remap_reward", "dqn_single_net_agent.DQNAgent.remember", "len", "dqn_single_net_agent.DQNAgent.replay"], "methods", ["home.repos.pwc.inspect_result.az79nefy_OrdinalRL.dqn.dqn_single_net_agent.DQNAgent.remap_reward", "home.repos.pwc.inspect_result.az79nefy_OrdinalRL.dqn.atari_dqn_single_net_agent.DQNAgent.remember", "home.repos.pwc.inspect_result.az79nefy_OrdinalRL.dqn.atari_dqn_single_net_agent.DQNAgent.replay"], ["", "def", "update", "(", "self", ",", "prev_obs", ",", "prev_act", ",", "obs", ",", "reward", ",", "episode_reward", ",", "done", ")", ":", "\n", "        ", "reward", "=", "self", ".", "remap_reward", "(", "reward", ",", "episode_reward", ",", "done", ")", "\n", "self", ".", "remember", "(", "prev_obs", ",", "prev_act", ",", "obs", ",", "reward", ",", "done", ")", "\n", "if", "len", "(", "self", ".", "memory", ")", ">", "self", ".", "batch_size", ":", "\n", "            ", "self", ".", "replay", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.az79nefy_OrdinalRL.dqn.dqn_single_net_agent.DQNAgent.remember": [[44, 46], ["dqn_single_net_agent.DQNAgent.memory.append"], "methods", ["None"], ["", "", "def", "remember", "(", "self", ",", "prev_obs", ",", "prev_act", ",", "obs", ",", "rew", ",", "d", ")", ":", "\n", "        ", "self", ".", "memory", ".", "append", "(", "(", "prev_obs", ",", "prev_act", ",", "obs", ",", "rew", ",", "d", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.az79nefy_OrdinalRL.dqn.dqn_single_net_agent.DQNAgent.replay": [[47, 67], ["random.sample", "dqn_single_net_agent.DQNAgent.eval_model.fit", "dqn_single_net_agent.DQNAgent.target_model.set_weights", "dqn_single_net_agent.DQNAgent.eval_model.predict", "x_batch.append", "y_batch.append", "numpy.array", "numpy.array", "dqn_single_net_agent.DQNAgent.eval_model.get_weights", "numpy.argmax", "len", "dqn_single_net_agent.DQNAgent.eval_model.predict", "dqn_single_net_agent.DQNAgent.target_model.predict"], "methods", ["None"], ["", "def", "replay", "(", "self", ")", ":", "\n", "# copy evaluation model to target model at first replay and then every 200 replay steps", "\n", "        ", "if", "self", ".", "replay_counter", "%", "self", ".", "replace_target_iter", "==", "0", ":", "\n", "            ", "self", ".", "target_model", ".", "set_weights", "(", "self", ".", "eval_model", ".", "get_weights", "(", ")", ")", "\n", "", "self", ".", "replay_counter", "+=", "1", "\n", "\n", "mini_batch", "=", "random", ".", "sample", "(", "self", ".", "memory", ",", "self", ".", "batch_size", ")", "\n", "x_batch", ",", "y_batch", "=", "[", "]", ",", "[", "]", "\n", "for", "prev_obs", ",", "prev_act", ",", "obs", ",", "rew", ",", "d", "in", "mini_batch", ":", "\n", "            ", "prediction", "=", "self", ".", "eval_model", ".", "predict", "(", "prev_obs", ")", "\n", "if", "not", "d", ":", "\n", "                ", "best_act", "=", "np", ".", "argmax", "(", "self", ".", "eval_model", ".", "predict", "(", "obs", ")", "[", "0", "]", ")", "\n", "target", "=", "rew", "+", "self", ".", "gamma", "*", "self", ".", "target_model", ".", "predict", "(", "obs", ")", "[", "0", ",", "best_act", "]", "\n", "", "else", ":", "\n", "                ", "target", "=", "rew", "\n", "# fit predicted value of previous action in previous observation to target value of max_action", "\n", "", "prediction", "[", "0", "]", "[", "prev_act", "]", "=", "target", "\n", "x_batch", ".", "append", "(", "prev_obs", "[", "0", "]", ")", "\n", "y_batch", ".", "append", "(", "prediction", "[", "0", "]", ")", "\n", "", "self", ".", "eval_model", ".", "fit", "(", "np", ".", "array", "(", "x_batch", ")", ",", "np", ".", "array", "(", "y_batch", ")", ",", "batch_size", "=", "len", "(", "x_batch", ")", ",", "verbose", "=", "0", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.az79nefy_OrdinalRL.dqn.dqn_single_net_agent.DQNAgent.get_greedy_action": [[68, 70], ["numpy.argmax", "dqn_single_net_agent.DQNAgent.eval_model.predict"], "methods", ["None"], ["", "def", "get_greedy_action", "(", "self", ",", "obs", ")", ":", "\n", "        ", "return", "np", ".", "argmax", "(", "self", ".", "eval_model", ".", "predict", "(", "obs", ")", "[", "0", "]", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.az79nefy_OrdinalRL.dqn.dqn_single_net_agent.DQNAgent.choose_action": [[72, 80], ["dqn_single_net_agent.DQNAgent.get_greedy_action", "random.randrange", "random.random"], "methods", ["home.repos.pwc.inspect_result.az79nefy_OrdinalRL.dqn.atari_dqn_single_net_agent.DQNAgent.get_greedy_action"], ["", "def", "choose_action", "(", "self", ",", "obs", ",", "greedy", ")", ":", "\n", "        ", "greedy_action", "=", "self", ".", "get_greedy_action", "(", "obs", ")", "\n", "# choose random action with probability epsilon", "\n", "if", "not", "greedy", "and", "random", ".", "random", "(", ")", "<", "self", ".", "epsilon", ":", "\n", "            ", "return", "random", ".", "randrange", "(", "self", ".", "n_actions", ")", "\n", "# greedy action is chosen with probability (1 - epsilon)", "\n", "", "else", ":", "\n", "            ", "return", "greedy_action", "\n", "\n"]], "home.repos.pwc.inspect_result.az79nefy_OrdinalRL.dqn.dqn_single_net_agent.DQNAgent.end_episode": [[81, 84], ["None"], "methods", ["None"], ["", "", "def", "end_episode", "(", "self", ",", "n_episodes", ")", ":", "\n", "# gradually reduce epsilon after every done episode", "\n", "        ", "self", ".", "epsilon", "=", "self", ".", "epsilon", "-", "2", "/", "n_episodes", "if", "self", ".", "epsilon", ">", "self", ".", "epsilon_min", "else", "self", ".", "epsilon_min", "\n", "\n"]], "home.repos.pwc.inspect_result.az79nefy_OrdinalRL.dqn.dqn_single_net_agent.DQNAgent.preprocess_observation": [[85, 88], ["numpy.expand_dims"], "methods", ["None"], ["", "@", "staticmethod", "\n", "def", "preprocess_observation", "(", "obs", ")", ":", "\n", "        ", "return", "np", ".", "expand_dims", "(", "obs", ",", "axis", "=", "0", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.az79nefy_OrdinalRL.dqn.dqn_single_net_agent.DQNAgent.remap_reward": [[90, 92], ["None"], "methods", ["None"], ["", "def", "remap_reward", "(", "self", ",", "reward", ",", "episode_reward", ",", "done", ")", ":", "\n", "        ", "return", "reward", "\n", "\n"]], "home.repos.pwc.inspect_result.az79nefy_OrdinalRL.dqn.dqn_single_net_agent.DQNAgent.check_win_condition": [[94, 100], ["None"], "methods", ["None"], ["", "@", "staticmethod", "\n", "def", "check_win_condition", "(", "reward", ",", "episode_reward", ",", "done", ")", ":", "\n", "        ", "if", "done", "and", "reward", "==", "20", ":", "\n", "            ", "return", "True", "\n", "", "else", ":", "\n", "            ", "return", "False", "\n", "\n"]], "home.repos.pwc.inspect_result.az79nefy_OrdinalRL.dqn.dqn_single_net_agent.DQNAgent.evaluate": [[101, 109], ["round", "round", "dqn_single_net_agent.DQNAgent.average_rewards.append", "dqn_single_net_agent.DQNAgent.win_rates.append", "print", "sum", "len", "sum", "len"], "methods", ["None"], ["", "", "def", "evaluate", "(", "self", ",", "i_episode", ",", "episode_rewards", ",", "episode_wins", ")", ":", "\n", "# compute average episode reward and win rate over last episodes", "\n", "        ", "average_reward", "=", "round", "(", "sum", "(", "episode_rewards", ")", "/", "len", "(", "episode_rewards", ")", ",", "2", ")", "\n", "win_rate", "=", "round", "(", "sum", "(", "episode_wins", ")", "/", "len", "(", "episode_wins", ")", ",", "2", ")", "\n", "# store average episode reward and win rate over last episodes for plotting purposes", "\n", "self", ".", "average_rewards", ".", "append", "(", "average_reward", ")", "\n", "self", ".", "win_rates", ".", "append", "(", "win_rate", ")", "\n", "print", "(", "\"{}\\t{}\\t{}\"", ".", "format", "(", "i_episode", "+", "1", ",", "average_reward", ",", "win_rate", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.az79nefy_OrdinalRL.dqn.dqn_single_net_agent.DQNAgent.plot": [[111, 125], ["matplotlib.figure", "matplotlib.plot", "matplotlib.xlabel", "matplotlib.ylabel", "matplotlib.figure", "matplotlib.plot", "matplotlib.xlabel", "matplotlib.ylabel", "matplotlib.show", "list", "list", "range", "range"], "methods", ["home.repos.pwc.inspect_result.az79nefy_OrdinalRL.dqn.atari_dqn_single_net_agent.DQNAgent.plot", "home.repos.pwc.inspect_result.az79nefy_OrdinalRL.dqn.atari_dqn_single_net_agent.DQNAgent.plot"], ["", "def", "plot", "(", "self", ",", "n_episodes", ",", "step_size", ")", ":", "\n", "# plot win rate", "\n", "        ", "plt", ".", "figure", "(", ")", "\n", "plt", ".", "plot", "(", "list", "(", "range", "(", "step_size", ",", "n_episodes", "+", "step_size", ",", "step_size", ")", ")", ",", "self", ".", "win_rates", ")", "\n", "plt", ".", "xlabel", "(", "'Number of episodes'", ")", "\n", "plt", ".", "ylabel", "(", "'Win rate'", ")", "\n", "\n", "# plot average score", "\n", "plt", ".", "figure", "(", ")", "\n", "plt", ".", "plot", "(", "list", "(", "range", "(", "step_size", ",", "n_episodes", "+", "step_size", ",", "step_size", ")", ")", ",", "self", ".", "average_rewards", ")", "\n", "plt", ".", "xlabel", "(", "'Number of episodes'", ")", "\n", "plt", ".", "ylabel", "(", "'Average score'", ")", "\n", "\n", "plt", ".", "show", "(", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.az79nefy_OrdinalRL.dqn.ordinal_dqn_discretized_agent.DQNAgent.__init__": [[11, 30], ["collections.deque", "ordinal_dqn_discretized_agent.DQNAgent.build_model", "ordinal_dqn_discretized_agent.DQNAgent.build_model", "range", "range"], "methods", ["home.repos.pwc.inspect_result.az79nefy_OrdinalRL.dqn.atari_dqn_single_net_agent.DQNAgent.build_model", "home.repos.pwc.inspect_result.az79nefy_OrdinalRL.dqn.atari_dqn_single_net_agent.DQNAgent.build_model"], ["    ", "def", "__init__", "(", "self", ",", "alpha", ",", "gamma", ",", "epsilon", ",", "epsilon_min", ",", "n_actions", ",", "n_ordinals", ",", "observation_dim", ",", "batch_size", ",", "memory_len", ",", "replace_target_iter", ")", ":", "\n", "        ", "self", ".", "alpha", "=", "alpha", "\n", "self", ".", "gamma", "=", "gamma", "\n", "self", ".", "epsilon", "=", "epsilon", "\n", "self", ".", "epsilon_min", "=", "epsilon_min", "\n", "self", ".", "n_actions", "=", "n_actions", "\n", "self", ".", "n_ordinals", "=", "n_ordinals", "\n", "self", ".", "n_inputs", "=", "observation_dim", "\n", "\n", "self", ".", "batch_size", "=", "batch_size", "\n", "self", ".", "memory", "=", "deque", "(", "maxlen", "=", "memory_len", ")", "\n", "# creation of a neural net for every action", "\n", "self", ".", "eval_action_nets", "=", "[", "self", ".", "build_model", "(", "self", ".", "n_inputs", ")", "for", "_", "in", "range", "(", "n_actions", ")", "]", "\n", "self", ".", "target_action_nets", "=", "[", "self", ".", "build_model", "(", "self", ".", "n_inputs", ")", "for", "_", "in", "range", "(", "n_actions", ")", "]", "\n", "self", ".", "replace_target_iter", "=", "replace_target_iter", "\n", "\n", "self", ".", "replay_counter", "=", "0", "\n", "self", ".", "win_rates", "=", "[", "]", "\n", "self", ".", "average_rewards", "=", "[", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.az79nefy_OrdinalRL.dqn.ordinal_dqn_discretized_agent.DQNAgent.build_model": [[32, 39], ["keras.models.Sequential", "keras.models.Sequential.add", "keras.models.Sequential.add", "keras.models.Sequential.add", "keras.models.Sequential.compile", "keras.layers.Dense", "keras.layers.Dense", "keras.layers.Dense", "keras.optimizers.Adam"], "methods", ["None"], ["", "def", "build_model", "(", "self", ",", "n_inputs", ")", ":", "\n", "        ", "neural_net", "=", "Sequential", "(", ")", "\n", "neural_net", ".", "add", "(", "Dense", "(", "24", ",", "input_dim", "=", "n_inputs", ",", "activation", "=", "'relu'", ")", ")", "\n", "neural_net", ".", "add", "(", "Dense", "(", "24", ",", "activation", "=", "'relu'", ")", ")", "\n", "neural_net", ".", "add", "(", "Dense", "(", "self", ".", "n_ordinals", ",", "activation", "=", "'linear'", ")", ")", "\n", "neural_net", ".", "compile", "(", "loss", "=", "'mse'", ",", "optimizer", "=", "Adam", "(", "lr", "=", "self", ".", "alpha", ")", ")", "\n", "return", "neural_net", "\n", "\n"]], "home.repos.pwc.inspect_result.az79nefy_OrdinalRL.dqn.ordinal_dqn_discretized_agent.DQNAgent.update": [[40, 45], ["ordinal_dqn_discretized_agent.DQNAgent.reward_to_ordinal", "ordinal_dqn_discretized_agent.DQNAgent.remember", "len", "ordinal_dqn_discretized_agent.DQNAgent.replay"], "methods", ["home.repos.pwc.inspect_result.az79nefy_OrdinalRL.dqn.ordinal_dqn_discretized_agent.DQNAgent.reward_to_ordinal", "home.repos.pwc.inspect_result.az79nefy_OrdinalRL.dqn.atari_dqn_single_net_agent.DQNAgent.remember", "home.repos.pwc.inspect_result.az79nefy_OrdinalRL.dqn.atari_dqn_single_net_agent.DQNAgent.replay"], ["", "def", "update", "(", "self", ",", "prev_obs", ",", "prev_act", ",", "obs", ",", "reward", ",", "episode_reward", ",", "done", ")", ":", "\n", "        ", "ordinal", "=", "self", ".", "reward_to_ordinal", "(", "reward", ",", "episode_reward", ",", "done", ")", "\n", "self", ".", "remember", "(", "prev_obs", ",", "prev_act", ",", "obs", ",", "ordinal", ",", "done", ")", "\n", "if", "len", "(", "self", ".", "memory", ")", ">", "self", ".", "batch_size", ":", "\n", "            ", "self", ".", "replay", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.az79nefy_OrdinalRL.dqn.ordinal_dqn_discretized_agent.DQNAgent.remember": [[46, 48], ["ordinal_dqn_discretized_agent.DQNAgent.memory.append"], "methods", ["None"], ["", "", "def", "remember", "(", "self", ",", "prev_obs", ",", "prev_act", ",", "obs", ",", "rew", ",", "d", ")", ":", "\n", "        ", "self", ".", "memory", ".", "append", "(", "(", "prev_obs", ",", "prev_act", ",", "obs", ",", "rew", ",", "d", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.az79nefy_OrdinalRL.dqn.ordinal_dqn_discretized_agent.DQNAgent.replay": [[49, 72], ["random.sample", "range", "range", "ordinal_dqn_discretized_agent.DQNAgent.get_greedy_action", "x_batch[].append", "y_batch[].append", "ordinal_dqn_discretized_agent.DQNAgent.target_action_nets[].set_weights", "numpy.zeros", "len", "ordinal_dqn_discretized_agent.DQNAgent.eval_action_nets[].fit", "ordinal_dqn_discretized_agent.DQNAgent.eval_action_nets[].get_weights", "range", "range", "numpy.array", "numpy.array", "ordinal_dqn_discretized_agent.DQNAgent.target_action_nets[].predict", "len"], "methods", ["home.repos.pwc.inspect_result.az79nefy_OrdinalRL.dqn.atari_dqn_single_net_agent.DQNAgent.get_greedy_action"], ["", "def", "replay", "(", "self", ")", ":", "\n", "# copy evaluation action nets to target action nets at first replay and then every 200 replay steps", "\n", "        ", "if", "self", ".", "replay_counter", "%", "self", ".", "replace_target_iter", "==", "0", ":", "\n", "            ", "for", "a", "in", "range", "(", "self", ".", "n_actions", ")", ":", "\n", "                ", "self", ".", "target_action_nets", "[", "a", "]", ".", "set_weights", "(", "self", ".", "eval_action_nets", "[", "a", "]", ".", "get_weights", "(", ")", ")", "\n", "", "", "self", ".", "replay_counter", "+=", "1", "\n", "\n", "mini_batch", "=", "random", ".", "sample", "(", "self", ".", "memory", ",", "self", ".", "batch_size", ")", "\n", "x_batch", ",", "y_batch", "=", "[", "[", "]", "for", "_", "in", "range", "(", "self", ".", "n_actions", ")", "]", ",", "[", "[", "]", "for", "_", "in", "range", "(", "self", ".", "n_actions", ")", "]", "\n", "for", "prev_obs", ",", "prev_act", ",", "obs", ",", "ordinal", ",", "d", "in", "mini_batch", ":", "\n", "            ", "greedy_action", "=", "self", ".", "get_greedy_action", "(", "obs", ")", "\n", "if", "not", "d", ":", "\n", "                ", "target", "=", "self", ".", "gamma", "*", "self", ".", "target_action_nets", "[", "greedy_action", "]", ".", "predict", "(", "obs", ")", "[", "0", "]", "\n", "target", "[", "ordinal", "]", "+=", "1", "\n", "", "else", ":", "\n", "                ", "target", "=", "np", ".", "zeros", "(", "self", ".", "n_ordinals", ")", "\n", "target", "[", "ordinal", "]", "+=", "1", "\n", "# fit predicted value of previous action in previous observation to target value of max_action", "\n", "", "x_batch", "[", "prev_act", "]", ".", "append", "(", "prev_obs", "[", "0", "]", ")", "\n", "y_batch", "[", "prev_act", "]", ".", "append", "(", "target", ")", "\n", "", "for", "a", "in", "range", "(", "self", ".", "n_actions", ")", ":", "\n", "            ", "if", "len", "(", "x_batch", "[", "a", "]", ")", "!=", "0", ":", "\n", "                ", "self", ".", "eval_action_nets", "[", "a", "]", ".", "fit", "(", "np", ".", "array", "(", "x_batch", "[", "a", "]", ")", ",", "np", ".", "array", "(", "y_batch", "[", "a", "]", ")", ",", "batch_size", "=", "len", "(", "x_batch", "[", "a", "]", ")", ",", "verbose", "=", "0", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.az79nefy_OrdinalRL.dqn.ordinal_dqn_discretized_agent.DQNAgent.compute_borda_scores": [[74, 130], ["numpy.zeros", "range", "numpy.array", "numpy.count_nonzero", "range", "range", "ordinal_dqn_discretized_agent.DQNAgent.eval_action_nets[].predict", "ordinal_values_per_action[].append", "borda_scores.append", "borda_scores.append", "range", "borda_scores.append", "range"], "methods", ["None"], ["", "", "", "def", "compute_borda_scores", "(", "self", ",", "obs", ")", ":", "\n", "# sum up all ordinal values per action for given observation", "\n", "        ", "ordinal_value_sum_per_action", "=", "np", ".", "zeros", "(", "self", ".", "n_actions", ")", "\n", "ordinal_values_per_action", "=", "[", "[", "]", "for", "_", "in", "range", "(", "self", ".", "n_actions", ")", "]", "\n", "for", "action_a", "in", "range", "(", "self", ".", "n_actions", ")", ":", "\n", "            ", "for", "ordinal_value", "in", "self", ".", "eval_action_nets", "[", "action_a", "]", ".", "predict", "(", "obs", ")", "[", "0", "]", ":", "\n", "                ", "ordinal_value_sum_per_action", "[", "action_a", "]", "+=", "ordinal_value", "\n", "ordinal_values_per_action", "[", "action_a", "]", ".", "append", "(", "ordinal_value", ")", "\n", "", "", "ordinal_values_per_action", "=", "np", ".", "array", "(", "ordinal_values_per_action", ")", "\n", "\n", "# count actions whose ordinal value sum is not zero (no comparision possible for actions without ordinal_value)", "\n", "non_zero_action_count", "=", "np", ".", "count_nonzero", "(", "ordinal_value_sum_per_action", ")", "\n", "actions_to_compare_count", "=", "non_zero_action_count", "-", "1", "\n", "\n", "borda_scores", "=", "[", "]", "\n", "# compute borda_values for action_a (probability that action_a wins against any other action)", "\n", "for", "action_a", "in", "range", "(", "self", ".", "n_actions", ")", ":", "\n", "# if action has not yet recorded any ordinal values, action has to be played (set borda_value to 1.0)", "\n", "            ", "if", "ordinal_value_sum_per_action", "[", "action_a", "]", "==", "0", ":", "\n", "                ", "borda_scores", ".", "append", "(", "1.0", ")", "\n", "continue", "\n", "\n", "", "if", "actions_to_compare_count", "<", "1", ":", "\n", "# set lower than 1.0 (borda_value for zero_actions is 1.0)", "\n", "                ", "borda_scores", ".", "append", "(", "0.5", ")", "\n", "", "else", ":", "\n", "# over all actions: sum up the probabilities that action_a wins against the given action", "\n", "                ", "winning_probability_a_sum", "=", "0", "\n", "# compare action_a to all other actions", "\n", "for", "action_b", "in", "range", "(", "self", ".", "n_actions", ")", ":", "\n", "                    ", "if", "action_a", "==", "action_b", ":", "\n", "                        ", "continue", "\n", "# not comparable if action_b has no ordinal_values", "\n", "", "if", "ordinal_value_sum_per_action", "[", "action_b", "]", "==", "0", ":", "\n", "                        ", "continue", "\n", "", "else", ":", "\n", "# probability that action_a wins against action_b", "\n", "                        ", "winning_probability_a", "=", "0", "\n", "# running ordinal probability that action_b is worse than current investigated ordinal", "\n", "worse_probability_b", "=", "0", "\n", "# predict ordinal values for action a and b", "\n", "ordinal_values_a", "=", "ordinal_values_per_action", "[", "action_a", "]", "\n", "ordinal_values_b", "=", "ordinal_values_per_action", "[", "action_b", "]", "\n", "for", "ordinal_count", "in", "range", "(", "self", ".", "n_ordinals", ")", ":", "\n", "                            ", "ordinal_probability_a", "=", "ordinal_values_a", "[", "ordinal_count", "]", "/", "ordinal_value_sum_per_action", "[", "action_a", "]", "\n", "# ordinal_probability_b is also the tie probability", "\n", "ordinal_probability_b", "=", "(", "ordinal_values_b", "[", "ordinal_count", "]", "/", "\n", "ordinal_value_sum_per_action", "[", "action_b", "]", ")", "\n", "winning_probability_a", "+=", "ordinal_probability_a", "*", "(", "worse_probability_b", "+", "ordinal_probability_b", "/", "2.0", ")", "\n", "worse_probability_b", "+=", "ordinal_probability_b", "\n", "", "winning_probability_a_sum", "+=", "winning_probability_a", "\n", "# normalize summed up probabilities with number of actions that have been compared", "\n", "", "", "borda_scores", ".", "append", "(", "winning_probability_a_sum", "/", "actions_to_compare_count", ")", "\n", "", "", "return", "borda_scores", "\n", "\n"]], "home.repos.pwc.inspect_result.az79nefy_OrdinalRL.dqn.ordinal_dqn_discretized_agent.DQNAgent.get_greedy_action": [[131, 133], ["numpy.argmax", "ordinal_dqn_discretized_agent.DQNAgent.compute_borda_scores"], "methods", ["home.repos.pwc.inspect_result.az79nefy_OrdinalRL.dqn.ordinal_dqn_discretized_agent.DQNAgent.compute_borda_scores"], ["", "def", "get_greedy_action", "(", "self", ",", "obs", ")", ":", "\n", "        ", "return", "np", ".", "argmax", "(", "self", ".", "compute_borda_scores", "(", "obs", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.az79nefy_OrdinalRL.dqn.ordinal_dqn_discretized_agent.DQNAgent.choose_action": [[135, 143], ["ordinal_dqn_discretized_agent.DQNAgent.get_greedy_action", "random.randrange", "random.random"], "methods", ["home.repos.pwc.inspect_result.az79nefy_OrdinalRL.dqn.atari_dqn_single_net_agent.DQNAgent.get_greedy_action"], ["", "def", "choose_action", "(", "self", ",", "obs", ",", "greedy", ")", ":", "\n", "        ", "greedy_action", "=", "self", ".", "get_greedy_action", "(", "obs", ")", "\n", "# choose random action with probability epsilon", "\n", "if", "not", "greedy", "and", "random", ".", "random", "(", ")", "<", "self", ".", "epsilon", ":", "\n", "            ", "return", "random", ".", "randrange", "(", "self", ".", "n_actions", ")", "\n", "# greedy action is chosen with probability (1 - epsilon)", "\n", "", "else", ":", "\n", "            ", "return", "greedy_action", "\n", "\n"]], "home.repos.pwc.inspect_result.az79nefy_OrdinalRL.dqn.ordinal_dqn_discretized_agent.DQNAgent.end_episode": [[144, 147], ["None"], "methods", ["None"], ["", "", "def", "end_episode", "(", "self", ",", "n_episodes", ")", ":", "\n", "# gradually reduce epsilon after every done episode", "\n", "        ", "self", ".", "epsilon", "=", "self", ".", "epsilon", "-", "2", "/", "n_episodes", "if", "self", ".", "epsilon", ">", "self", ".", "epsilon_min", "else", "self", ".", "epsilon_min", "\n", "\n"]], "home.repos.pwc.inspect_result.az79nefy_OrdinalRL.dqn.ordinal_dqn_discretized_agent.DQNAgent.preprocess_observation": [[148, 151], ["numpy.expand_dims"], "methods", ["None"], ["", "@", "staticmethod", "\n", "def", "preprocess_observation", "(", "obs", ")", ":", "\n", "        ", "return", "np", ".", "expand_dims", "(", "obs", ",", "axis", "=", "0", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.az79nefy_OrdinalRL.dqn.ordinal_dqn_discretized_agent.DQNAgent.reward_to_ordinal": [[153, 158], ["ordinal_dqn_discretized_agent.DQNAgent.check_win_condition"], "methods", ["home.repos.pwc.inspect_result.az79nefy_OrdinalRL.dqn.atari_dqn_single_net_agent.DQNAgent.check_win_condition"], ["", "def", "reward_to_ordinal", "(", "self", ",", "reward", ",", "episode_reward", ",", "done", ")", ":", "\n", "        ", "if", "done", "and", "not", "self", ".", "check_win_condition", "(", "reward", ",", "episode_reward", ",", "done", ")", ":", "\n", "            ", "return", "0", "\n", "", "else", ":", "\n", "            ", "return", "1", "\n", "\n"]], "home.repos.pwc.inspect_result.az79nefy_OrdinalRL.dqn.ordinal_dqn_discretized_agent.DQNAgent.check_win_condition": [[160, 166], ["None"], "methods", ["None"], ["", "", "@", "staticmethod", "\n", "def", "check_win_condition", "(", "reward", ",", "episode_reward", ",", "done", ")", ":", "\n", "        ", "if", "done", "and", "episode_reward", "==", "200", ":", "\n", "            ", "return", "True", "\n", "", "else", ":", "\n", "            ", "return", "False", "\n", "\n"]], "home.repos.pwc.inspect_result.az79nefy_OrdinalRL.dqn.ordinal_dqn_discretized_agent.DQNAgent.evaluate": [[167, 175], ["round", "round", "ordinal_dqn_discretized_agent.DQNAgent.average_rewards.append", "ordinal_dqn_discretized_agent.DQNAgent.win_rates.append", "print", "sum", "len", "sum", "len"], "methods", ["None"], ["", "", "def", "evaluate", "(", "self", ",", "i_episode", ",", "episode_rewards", ",", "episode_wins", ")", ":", "\n", "# compute average episode reward and win rate over last episodes", "\n", "        ", "average_reward", "=", "round", "(", "sum", "(", "episode_rewards", ")", "/", "len", "(", "episode_rewards", ")", ",", "2", ")", "\n", "win_rate", "=", "round", "(", "sum", "(", "episode_wins", ")", "/", "len", "(", "episode_wins", ")", ",", "2", ")", "\n", "# store average episode reward and win rate over last episodes for plotting purposes", "\n", "self", ".", "average_rewards", ".", "append", "(", "average_reward", ")", "\n", "self", ".", "win_rates", ".", "append", "(", "win_rate", ")", "\n", "print", "(", "\"{}\\t{}\\t{}\"", ".", "format", "(", "i_episode", "+", "1", ",", "average_reward", ",", "win_rate", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.az79nefy_OrdinalRL.dqn.ordinal_dqn_discretized_agent.DQNAgent.plot": [[177, 191], ["matplotlib.figure", "matplotlib.plot", "matplotlib.xlabel", "matplotlib.ylabel", "matplotlib.figure", "matplotlib.plot", "matplotlib.xlabel", "matplotlib.ylabel", "matplotlib.show", "list", "list", "range", "range"], "methods", ["home.repos.pwc.inspect_result.az79nefy_OrdinalRL.dqn.atari_dqn_single_net_agent.DQNAgent.plot", "home.repos.pwc.inspect_result.az79nefy_OrdinalRL.dqn.atari_dqn_single_net_agent.DQNAgent.plot"], ["", "def", "plot", "(", "self", ",", "n_episodes", ",", "step_size", ")", ":", "\n", "# plot win rate", "\n", "        ", "plt", ".", "figure", "(", ")", "\n", "plt", ".", "plot", "(", "list", "(", "range", "(", "step_size", ",", "n_episodes", "+", "step_size", ",", "step_size", ")", ")", ",", "self", ".", "win_rates", ")", "\n", "plt", ".", "xlabel", "(", "'Number of episodes'", ")", "\n", "plt", ".", "ylabel", "(", "'Win rate'", ")", "\n", "\n", "# plot average score", "\n", "plt", ".", "figure", "(", ")", "\n", "plt", ".", "plot", "(", "list", "(", "range", "(", "step_size", ",", "n_episodes", "+", "step_size", ",", "step_size", ")", ")", ",", "self", ".", "average_rewards", ")", "\n", "plt", ".", "xlabel", "(", "'Number of episodes'", ")", "\n", "plt", ".", "ylabel", "(", "'Average score'", ")", "\n", "\n", "plt", ".", "show", "(", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.az79nefy_OrdinalRL.dqn.atari_dqn_single_net_agent.DQNAgent.__init__": [[11, 29], ["collections.deque", "atari_dqn_single_net_agent.DQNAgent.build_model", "atari_dqn_single_net_agent.DQNAgent.build_model"], "methods", ["home.repos.pwc.inspect_result.az79nefy_OrdinalRL.dqn.atari_dqn_single_net_agent.DQNAgent.build_model", "home.repos.pwc.inspect_result.az79nefy_OrdinalRL.dqn.atari_dqn_single_net_agent.DQNAgent.build_model"], ["    ", "def", "__init__", "(", "self", ",", "alpha", ",", "gamma", ",", "epsilon", ",", "epsilon_min", ",", "n_actions", ",", "n_ordinals", ",", "observation_dim", ",", "batch_size", ",", "memory_len", ",", "replace_target_iter", ")", ":", "\n", "        ", "self", ".", "alpha", "=", "alpha", "\n", "self", ".", "gamma", "=", "gamma", "\n", "self", ".", "epsilon", "=", "epsilon", "\n", "self", ".", "epsilon_min", "=", "epsilon_min", "\n", "self", ".", "n_actions", "=", "n_actions", "\n", "self", ".", "n_inputs", "=", "observation_dim", "\n", "\n", "self", ".", "batch_size", "=", "batch_size", "\n", "self", ".", "memory", "=", "deque", "(", "maxlen", "=", "memory_len", ")", "\n", "self", ".", "eval_model", "=", "self", ".", "build_model", "(", "self", ".", "n_inputs", ")", "\n", "self", ".", "target_model", "=", "self", ".", "build_model", "(", "self", ".", "n_inputs", ")", "\n", "self", ".", "replace_target_iter", "=", "replace_target_iter", "\n", "\n", "self", ".", "update_counter", "=", "0", "\n", "self", ".", "replay_counter", "=", "0", "\n", "self", ".", "win_rates", "=", "[", "]", "\n", "self", ".", "average_rewards", "=", "[", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.az79nefy_OrdinalRL.dqn.atari_dqn_single_net_agent.DQNAgent.build_model": [[31, 44], ["keras.models.Sequential", "keras.models.Sequential.add", "keras.models.Sequential.add", "keras.models.Sequential.add", "keras.models.Sequential.add", "keras.models.Sequential.add", "keras.models.Sequential.add", "keras.models.Sequential.compile", "keras.layers.Conv2D", "keras.layers.Conv2D", "keras.layers.Conv2D", "keras.layers.Flatten", "keras.layers.Dense", "keras.layers.Dense", "keras.optimizers.RMSprop"], "methods", ["None"], ["", "def", "build_model", "(", "self", ",", "n_inputs", ")", ":", "\n", "        ", "neural_net", "=", "Sequential", "(", ")", "\n", "neural_net", ".", "add", "(", "Conv2D", "(", "32", ",", "8", ",", "strides", "=", "(", "4", ",", "4", ")", ",", "\n", "padding", "=", "'valid'", ",", "activation", "=", "'relu'", ",", "input_shape", "=", "n_inputs", ",", "data_format", "=", "'channels_first'", ")", ")", "\n", "neural_net", ".", "add", "(", "Conv2D", "(", "64", ",", "4", ",", "strides", "=", "(", "2", ",", "2", ")", ",", "\n", "padding", "=", "'valid'", ",", "activation", "=", "'relu'", ",", "input_shape", "=", "n_inputs", ",", "data_format", "=", "'channels_first'", ")", ")", "\n", "neural_net", ".", "add", "(", "Conv2D", "(", "64", ",", "3", ",", "strides", "=", "(", "1", ",", "1", ")", ",", "\n", "padding", "=", "'valid'", ",", "activation", "=", "'relu'", ",", "input_shape", "=", "n_inputs", ",", "data_format", "=", "'channels_first'", ")", ")", "\n", "neural_net", ".", "add", "(", "Flatten", "(", ")", ")", "\n", "neural_net", ".", "add", "(", "Dense", "(", "512", ",", "activation", "=", "'relu'", ")", ")", "\n", "neural_net", ".", "add", "(", "Dense", "(", "self", ".", "n_actions", ")", ")", "\n", "neural_net", ".", "compile", "(", "loss", "=", "'mse'", ",", "optimizer", "=", "RMSprop", "(", "lr", "=", "self", ".", "alpha", ",", "rho", "=", "0.95", ",", "epsilon", "=", "0.01", ")", ",", "metrics", "=", "[", "'accuracy'", "]", ")", "\n", "return", "neural_net", "\n", "\n"]], "home.repos.pwc.inspect_result.az79nefy_OrdinalRL.dqn.atari_dqn_single_net_agent.DQNAgent.update": [[45, 50], ["atari_dqn_single_net_agent.DQNAgent.remember", "len", "atari_dqn_single_net_agent.DQNAgent.replay"], "methods", ["home.repos.pwc.inspect_result.az79nefy_OrdinalRL.dqn.atari_dqn_single_net_agent.DQNAgent.remember", "home.repos.pwc.inspect_result.az79nefy_OrdinalRL.dqn.atari_dqn_single_net_agent.DQNAgent.replay"], ["", "def", "update", "(", "self", ",", "prev_obs", ",", "prev_act", ",", "obs", ",", "reward", ",", "episode_reward", ",", "done", ")", ":", "\n", "        ", "self", ".", "update_counter", "+=", "1", "\n", "self", ".", "remember", "(", "prev_obs", ",", "prev_act", ",", "obs", ",", "reward", ",", "done", ")", "\n", "if", "len", "(", "self", ".", "memory", ")", ">", "self", ".", "batch_size", ":", "\n", "            ", "self", ".", "replay", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.az79nefy_OrdinalRL.dqn.atari_dqn_single_net_agent.DQNAgent.remember": [[51, 53], ["atari_dqn_single_net_agent.DQNAgent.memory.append"], "methods", ["None"], ["", "", "def", "remember", "(", "self", ",", "prev_obs", ",", "prev_act", ",", "obs", ",", "rew", ",", "d", ")", ":", "\n", "        ", "self", ".", "memory", ".", "append", "(", "(", "prev_obs", ",", "prev_act", ",", "obs", ",", "rew", ",", "d", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.az79nefy_OrdinalRL.dqn.atari_dqn_single_net_agent.DQNAgent.replay": [[54, 74], ["random.sample", "atari_dqn_single_net_agent.DQNAgent.eval_model.fit", "atari_dqn_single_net_agent.DQNAgent.target_model.set_weights", "atari_dqn_single_net_agent.DQNAgent.eval_model.predict", "x_batch.append", "y_batch.append", "numpy.array", "numpy.array", "atari_dqn_single_net_agent.DQNAgent.eval_model.get_weights", "atari_dqn_single_net_agent.DQNAgent.convert", "numpy.argmax", "len", "atari_dqn_single_net_agent.DQNAgent.convert", "atari_dqn_single_net_agent.DQNAgent.eval_model.predict", "atari_dqn_single_net_agent.DQNAgent.convert", "atari_dqn_single_net_agent.DQNAgent.target_model.predict", "atari_dqn_single_net_agent.DQNAgent.convert"], "methods", ["home.repos.pwc.inspect_result.az79nefy_OrdinalRL.dqn.atari_dqn_single_net_agent.DQNAgent.convert", "home.repos.pwc.inspect_result.az79nefy_OrdinalRL.dqn.atari_dqn_single_net_agent.DQNAgent.convert", "home.repos.pwc.inspect_result.az79nefy_OrdinalRL.dqn.atari_dqn_single_net_agent.DQNAgent.convert", "home.repos.pwc.inspect_result.az79nefy_OrdinalRL.dqn.atari_dqn_single_net_agent.DQNAgent.convert"], ["", "def", "replay", "(", "self", ")", ":", "\n", "# copy evaluation model to target model at first replay and then every 200 replay steps", "\n", "        ", "if", "self", ".", "replay_counter", "%", "self", ".", "replace_target_iter", "==", "0", ":", "\n", "            ", "self", ".", "target_model", ".", "set_weights", "(", "self", ".", "eval_model", ".", "get_weights", "(", ")", ")", "\n", "", "self", ".", "replay_counter", "+=", "1", "\n", "\n", "mini_batch", "=", "random", ".", "sample", "(", "self", ".", "memory", ",", "self", ".", "batch_size", ")", "\n", "x_batch", ",", "y_batch", "=", "[", "]", ",", "[", "]", "\n", "for", "prev_obs", ",", "prev_act", ",", "obs", ",", "rew", ",", "d", "in", "mini_batch", ":", "\n", "            ", "prediction", "=", "self", ".", "eval_model", ".", "predict", "(", "self", ".", "convert", "(", "prev_obs", ")", ")", "\n", "if", "not", "d", ":", "\n", "                ", "best_act", "=", "np", ".", "argmax", "(", "self", ".", "eval_model", ".", "predict", "(", "self", ".", "convert", "(", "obs", ")", ")", "[", "0", "]", ")", "\n", "target", "=", "rew", "+", "self", ".", "gamma", "*", "self", ".", "target_model", ".", "predict", "(", "self", ".", "convert", "(", "obs", ")", ")", "[", "0", ",", "best_act", "]", "\n", "", "else", ":", "\n", "                ", "target", "=", "rew", "\n", "# fit predicted value of previous action in previous observation to target value of max_action", "\n", "", "prediction", "[", "0", "]", "[", "prev_act", "]", "=", "target", "\n", "x_batch", ".", "append", "(", "self", ".", "convert", "(", "prev_obs", ")", "[", "0", "]", ")", "\n", "y_batch", ".", "append", "(", "prediction", "[", "0", "]", ")", "\n", "", "self", ".", "eval_model", ".", "fit", "(", "np", ".", "array", "(", "x_batch", ")", ",", "np", ".", "array", "(", "y_batch", ")", ",", "batch_size", "=", "len", "(", "x_batch", ")", ",", "verbose", "=", "0", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.az79nefy_OrdinalRL.dqn.atari_dqn_single_net_agent.DQNAgent.get_greedy_action": [[75, 77], ["numpy.argmax", "atari_dqn_single_net_agent.DQNAgent.eval_model.predict", "atari_dqn_single_net_agent.DQNAgent.convert"], "methods", ["home.repos.pwc.inspect_result.az79nefy_OrdinalRL.dqn.atari_dqn_single_net_agent.DQNAgent.convert"], ["", "def", "get_greedy_action", "(", "self", ",", "obs", ")", ":", "\n", "        ", "return", "np", ".", "argmax", "(", "self", ".", "eval_model", ".", "predict", "(", "self", ".", "convert", "(", "obs", ")", ")", "[", "0", "]", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.az79nefy_OrdinalRL.dqn.atari_dqn_single_net_agent.DQNAgent.choose_action": [[79, 87], ["atari_dqn_single_net_agent.DQNAgent.get_greedy_action", "random.randrange", "random.random"], "methods", ["home.repos.pwc.inspect_result.az79nefy_OrdinalRL.dqn.atari_dqn_single_net_agent.DQNAgent.get_greedy_action"], ["", "def", "choose_action", "(", "self", ",", "obs", ",", "greedy", ")", ":", "\n", "        ", "greedy_action", "=", "self", ".", "get_greedy_action", "(", "obs", ")", "\n", "# choose random action with probability epsilon", "\n", "if", "not", "greedy", "and", "random", ".", "random", "(", ")", "<", "self", ".", "epsilon", ":", "\n", "            ", "return", "random", ".", "randrange", "(", "self", ".", "n_actions", ")", "\n", "# greedy action is chosen with probability (1 - epsilon)", "\n", "", "else", ":", "\n", "            ", "return", "greedy_action", "\n", "\n"]], "home.repos.pwc.inspect_result.az79nefy_OrdinalRL.dqn.atari_dqn_single_net_agent.DQNAgent.end_episode": [[88, 91], ["None"], "methods", ["None"], ["", "", "def", "end_episode", "(", "self", ",", "n_episodes", ")", ":", "\n", "# gradually reduce epsilon after every done episode", "\n", "        ", "self", ".", "epsilon", "=", "self", ".", "epsilon", "-", "2", "/", "n_episodes", "if", "self", ".", "epsilon", ">", "self", ".", "epsilon_min", "else", "self", ".", "epsilon_min", "\n", "\n"]], "home.repos.pwc.inspect_result.az79nefy_OrdinalRL.dqn.atari_dqn_single_net_agent.DQNAgent.preprocess_observation": [[92, 94], ["None"], "methods", ["None"], ["", "def", "preprocess_observation", "(", "self", ",", "obs", ")", ":", "\n", "        ", "return", "obs", "\n", "\n"]], "home.repos.pwc.inspect_result.az79nefy_OrdinalRL.dqn.atari_dqn_single_net_agent.DQNAgent.convert": [[95, 98], ["numpy.expand_dims", "numpy.asarray().astype", "numpy.asarray"], "methods", ["None"], ["", "@", "staticmethod", "\n", "def", "convert", "(", "obs", ")", ":", "\n", "        ", "return", "np", ".", "expand_dims", "(", "np", ".", "asarray", "(", "obs", ")", ".", "astype", "(", "np", ".", "float64", ")", ",", "axis", "=", "0", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.az79nefy_OrdinalRL.dqn.atari_dqn_single_net_agent.DQNAgent.check_win_condition": [[100, 106], ["None"], "methods", ["None"], ["", "@", "staticmethod", "\n", "def", "check_win_condition", "(", "reward", ",", "episode_reward", ",", "done", ")", ":", "\n", "        ", "if", "done", "and", "episode_reward", ">", "20", ":", "\n", "            ", "return", "True", "\n", "", "else", ":", "\n", "            ", "return", "False", "\n", "\n"]], "home.repos.pwc.inspect_result.az79nefy_OrdinalRL.dqn.atari_dqn_single_net_agent.DQNAgent.evaluate": [[107, 115], ["round", "round", "atari_dqn_single_net_agent.DQNAgent.average_rewards.append", "atari_dqn_single_net_agent.DQNAgent.win_rates.append", "print", "sum", "len", "sum", "len"], "methods", ["None"], ["", "", "def", "evaluate", "(", "self", ",", "i_episode", ",", "episode_rewards", ",", "episode_wins", ")", ":", "\n", "# compute average episode reward and win rate over last episodes", "\n", "        ", "average_reward", "=", "round", "(", "sum", "(", "episode_rewards", ")", "/", "len", "(", "episode_rewards", ")", ",", "2", ")", "\n", "win_rate", "=", "round", "(", "sum", "(", "episode_wins", ")", "/", "len", "(", "episode_wins", ")", ",", "2", ")", "\n", "# store average episode reward and win rate over last episodes for plotting purposes", "\n", "self", ".", "average_rewards", ".", "append", "(", "average_reward", ")", "\n", "self", ".", "win_rates", ".", "append", "(", "win_rate", ")", "\n", "print", "(", "\"{}\\t{}\\t{}\"", ".", "format", "(", "i_episode", "+", "1", ",", "average_reward", ",", "win_rate", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.az79nefy_OrdinalRL.dqn.atari_dqn_single_net_agent.DQNAgent.plot": [[117, 131], ["matplotlib.figure", "matplotlib.plot", "matplotlib.xlabel", "matplotlib.ylabel", "matplotlib.figure", "matplotlib.plot", "matplotlib.xlabel", "matplotlib.ylabel", "matplotlib.show", "list", "list", "range", "range"], "methods", ["home.repos.pwc.inspect_result.az79nefy_OrdinalRL.dqn.atari_dqn_single_net_agent.DQNAgent.plot", "home.repos.pwc.inspect_result.az79nefy_OrdinalRL.dqn.atari_dqn_single_net_agent.DQNAgent.plot"], ["", "def", "plot", "(", "self", ",", "n_episodes", ",", "step_size", ")", ":", "\n", "# plot win rate", "\n", "        ", "plt", ".", "figure", "(", ")", "\n", "plt", ".", "plot", "(", "list", "(", "range", "(", "step_size", ",", "n_episodes", "+", "step_size", ",", "step_size", ")", ")", ",", "self", ".", "win_rates", ")", "\n", "plt", ".", "xlabel", "(", "'Number of episodes'", ")", "\n", "plt", ".", "ylabel", "(", "'Win rate'", ")", "\n", "\n", "# plot average score", "\n", "plt", ".", "figure", "(", ")", "\n", "plt", ".", "plot", "(", "list", "(", "range", "(", "step_size", ",", "n_episodes", "+", "step_size", ",", "step_size", ")", ")", ",", "self", ".", "average_rewards", ")", "\n", "plt", ".", "xlabel", "(", "'Number of episodes'", ")", "\n", "plt", ".", "ylabel", "(", "'Average score'", ")", "\n", "\n", "plt", ".", "show", "(", ")", "\n", "", "", ""]]}