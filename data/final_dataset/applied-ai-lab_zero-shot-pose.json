{"home.repos.pwc.inspect_result.applied-ai-lab_zero-shot-pose.utils.visuals.denorm_torch_to_pil": [[15, 19], ["PIL.Image.fromarray", "torch.Tensor", "torch.Tensor", "image.permute"], "function", ["None"], ["def", "denorm_torch_to_pil", "(", "image", ")", ":", "\n", "    ", "image", "=", "image", "*", "torch", ".", "Tensor", "(", "image_norm_std", ")", "[", ":", ",", "None", ",", "None", "]", "\n", "image", "=", "image", "+", "torch", ".", "Tensor", "(", "image_norm_mean", ")", "[", ":", ",", "None", ",", "None", "]", "\n", "return", "Image", ".", "fromarray", "(", "(", "image", ".", "permute", "(", "1", ",", "2", ",", "0", ")", "*", "255", ")", ".", "numpy", "(", ")", ".", "astype", "(", "np", ".", "uint8", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.applied-ai-lab_zero-shot-pose.utils.visuals.fig_to_pil": [[21, 28], ["tempfile.SpooledTemporaryFile", "fig.savefig", "PIL.Image.open", "Image.open.load", "tempfile.SpooledTemporaryFile.close"], "function", ["None"], ["", "def", "fig_to_pil", "(", "fig", ")", ":", "\n", "    ", "tmp_file", "=", "tempfile", ".", "SpooledTemporaryFile", "(", "max_size", "=", "10", "*", "1024", "*", "1024", ")", "# 10MB ", "\n", "fig", ".", "savefig", "(", "tmp_file", ",", "bbox_inches", "=", "'tight'", ",", "pad_inches", "=", "0", ")", "\n", "image", "=", "Image", ".", "open", "(", "tmp_file", ")", "\n", "image", ".", "load", "(", ")", "\n", "tmp_file", ".", "close", "(", ")", "\n", "return", "image", "\n", "\n"]], "home.repos.pwc.inspect_result.applied-ai-lab_zero-shot-pose.utils.visuals.fig_to_array": [[30, 32], ["numpy.array", "visuals.fig_to_pil"], "function", ["home.repos.pwc.inspect_result.applied-ai-lab_zero-shot-pose.utils.visuals.fig_to_pil"], ["", "def", "fig_to_array", "(", "fig", ")", ":", "\n", "    ", "return", "np", ".", "array", "(", "fig_to_pil", "(", "fig", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.applied-ai-lab_zero-shot-pose.utils.visuals.arrange": [[34, 40], ["numpy.concatenate", "numpy.concatenate"], "function", ["None"], ["", "def", "arrange", "(", "images", ")", ":", "\n", "    ", "rows", "=", "[", "]", "\n", "for", "row", "in", "images", ":", "\n", "        ", "rows", "+=", "[", "np", ".", "concatenate", "(", "row", ",", "axis", "=", "1", ")", "]", "\n", "", "image", "=", "np", ".", "concatenate", "(", "rows", ",", "axis", "=", "0", ")", "\n", "return", "image", "\n", "\n"]], "home.repos.pwc.inspect_result.applied-ai-lab_zero-shot-pose.utils.visuals.co3d_rgb_to_pil": [[42, 44], ["PIL.Image.fromarray", "image_rgb.permute"], "function", ["None"], ["", "def", "co3d_rgb_to_pil", "(", "image_rgb", ")", ":", "\n", "    ", "return", "Image", ".", "fromarray", "(", "(", "image_rgb", ".", "permute", "(", "1", ",", "2", ",", "0", ")", "*", "255", ")", ".", "numpy", "(", ")", ".", "astype", "(", "np", ".", "uint8", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.applied-ai-lab_zero-shot-pose.utils.visuals.tile_ims_horizontal_highlight_best": [[47, 64], ["max", "PIL.Image.new", "enumerate", "cumul_offsets.append", "Image.new.paste", "PIL.ImageDraw.Draw", "ImageDraw.Draw.rectangle"], "function", ["None"], ["", "def", "tile_ims_horizontal_highlight_best", "(", "ims", ",", "gap_px", "=", "20", ",", "highlight_idx", "=", "None", ")", ":", "\n", "    ", "cumul_offsets", "=", "[", "0", "]", "\n", "for", "im", "in", "ims", ":", "\n", "        ", "cumul_offsets", ".", "append", "(", "cumul_offsets", "[", "-", "1", "]", "+", "im", ".", "width", "+", "gap_px", ")", "\n", "", "max_h", "=", "max", "(", "[", "im", ".", "height", "for", "im", "in", "ims", "]", ")", "\n", "dst", "=", "Image", ".", "new", "(", "'RGB'", ",", "(", "cumul_offsets", "[", "-", "1", "]", ",", "max_h", ")", ",", "(", "255", ",", "255", ",", "255", ")", ")", "\n", "for", "i", ",", "im", "in", "enumerate", "(", "ims", ")", ":", "\n", "        ", "dst", ".", "paste", "(", "im", ",", "(", "cumul_offsets", "[", "i", "]", ",", "(", "max_h", "-", "im", ".", "height", ")", "//", "2", ")", ")", "\n", "\n", "if", "i", "==", "highlight_idx", ":", "\n", "            ", "img1", "=", "ImageDraw", ".", "Draw", "(", "dst", ")", "\n", "# shape is defined as [(x1,y1), (x2, y2)]", "\n", "shape", "=", "[", "(", "cumul_offsets", "[", "i", "]", ",", "(", "max_h", "-", "im", ".", "height", ")", "//", "2", ")", ",", "\n", "(", "cumul_offsets", "[", "i", "]", "+", "im", ".", "width", ",", "max_h", "-", "(", "max_h", "-", "im", ".", "height", ")", "//", "2", ")", "]", "\n", "img1", ".", "rectangle", "(", "shape", ",", "fill", "=", "None", ",", "outline", "=", "\"green\"", ",", "width", "=", "6", ")", "\n", "\n", "", "", "return", "dst", "\n", "\n"]], "home.repos.pwc.inspect_result.applied-ai-lab_zero-shot-pose.utils.visuals.plot_pcd_and_ims": [[66, 92], ["torch.sort", "matplotlib.subplots", "ax[].imshow", "ax[].set_title", "ax[].axis", "ax[].imshow", "ax[].set_title", "ax[].axis", "ax[].scatter", "ax[].axis", "ax[].set_title", "[].numpy", "len", "numpy.random.choice", "[].numpy", "numpy.arange", "len", "pcd1.features_list", "pcd2.features_list"], "function", ["None"], ["", "def", "plot_pcd_and_ims", "(", "im1", ",", "im2", ",", "pcd1", ",", "pcd2", ",", "P_im", ",", "error", ",", "axs", ",", "pcd_frame", "=", "1", ")", ":", "\n", "# Sort the resulting points by their (NDC) depth, from high to low depth", "\n", "    ", "_", ",", "indices", "=", "torch", ".", "sort", "(", "P_im", "[", ":", ",", "2", "]", ",", "0", ",", "descending", "=", "False", ")", "\n", "P_im", "=", "P_im", "[", "indices", "]", "\n", "if", "pcd_frame", "==", "1", ":", "\n", "        ", "colors", "=", "pcd1", ".", "features_list", "(", ")", "[", "0", "]", "[", "indices", "]", ".", "numpy", "(", ")", "\n", "", "elif", "pcd_frame", "==", "2", ":", "\n", "        ", "colors", "=", "pcd2", ".", "features_list", "(", ")", "[", "0", "]", "[", "indices", "]", ".", "numpy", "(", ")", "\n", "", "N_subsample", "=", "100000", "\n", "if", "len", "(", "P_im", ")", ">", "N_subsample", ":", "\n", "        ", "subsample", "=", "np", ".", "random", ".", "choice", "(", "np", ".", "arange", "(", "len", "(", "P_im", ")", ")", ",", "N_subsample", ",", "replace", "=", "False", ")", "\n", "colors", "=", "colors", "[", "subsample", "]", "\n", "P_im", "=", "P_im", "[", "subsample", "]", "\n", "\n", "", "fig", ",", "ax", "=", "plt", ".", "subplots", "(", "1", ",", "3", ",", "figsize", "=", "(", "10", ",", "3", ")", ")", "\n", "ax", "[", "0", "]", ".", "imshow", "(", "im1", ")", "\n", "ax", "[", "0", "]", ".", "set_title", "(", "'Original image1'", ")", "\n", "ax", "[", "0", "]", ".", "axis", "(", "'off'", ")", "\n", "ax", "[", "1", "]", ".", "imshow", "(", "im2", ")", "\n", "ax", "[", "1", "]", ".", "set_title", "(", "'Original image2'", ")", "\n", "ax", "[", "1", "]", ".", "axis", "(", "'off'", ")", "\n", "ax", "[", "2", "]", ".", "scatter", "(", "P_im", "[", ":", ",", "0", "]", ",", "-", "P_im", "[", ":", ",", "1", "]", ",", "\n", "c", "=", "colors", "[", ":", "]", ",", "s", "=", "3", ")", "\n", "ax", "[", "2", "]", ".", "axis", "(", "'equal'", ")", "\n", "# ax[2].axis('off')", "\n", "ax", "[", "2", "]", ".", "set_title", "(", "f'im1 pcd in im2 pose - {error} deg error'", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.applied-ai-lab_zero-shot-pose.utils.visuals.plot_pcd": [[94, 107], ["torch.sort", "[].numpy", "ax.scatter", "ax.axis", "len", "numpy.random.choice", "numpy.arange", "len", "pcd.features_list"], "function", ["None"], ["", "def", "plot_pcd", "(", "P_im", ",", "pcd", ",", "ax", ",", "N_subsample", "=", "100000", ",", "s", "=", "3", ")", ":", "\n", "# Sort the resulting points by their (NDC) depth, from high to low depth", "\n", "    ", "_", ",", "indices", "=", "torch", ".", "sort", "(", "P_im", "[", ":", ",", "2", "]", ",", "0", ",", "descending", "=", "False", ")", "\n", "P_im", "=", "P_im", "[", "indices", "]", "\n", "colors", "=", "pcd", ".", "features_list", "(", ")", "[", "0", "]", "[", "indices", "]", ".", "numpy", "(", ")", "\n", "if", "len", "(", "P_im", ")", ">", "N_subsample", ":", "\n", "        ", "subsample", "=", "np", ".", "random", ".", "choice", "(", "np", ".", "arange", "(", "len", "(", "P_im", ")", ")", ",", "N_subsample", ",", "replace", "=", "False", ")", "\n", "colors", "=", "colors", "[", "subsample", "]", "\n", "P_im", "=", "P_im", "[", "subsample", "]", "\n", "", "ax", ".", "scatter", "(", "P_im", "[", ":", ",", "0", "]", ",", "-", "P_im", "[", ":", ",", "1", "]", ",", "\n", "c", "=", "colors", "[", ":", "]", ",", "s", "=", "s", ")", "\n", "ax", ".", "axis", "(", "'equal'", ")", "\n", "return", "ax", "\n", "\n"]], "home.repos.pwc.inspect_result.applied-ai-lab_zero-shot-pose.utils.visuals.get_concat_h_cut_center": [[109, 114], ["PIL.Image.new", "Image.new.paste", "Image.new.paste", "min"], "function", ["None"], ["", "def", "get_concat_h_cut_center", "(", "im1", ",", "im2", ",", "gap_px", "=", "20", ")", ":", "\n", "    ", "dst", "=", "Image", ".", "new", "(", "'RGB'", ",", "(", "im1", ".", "width", "+", "im2", ".", "width", "+", "gap_px", ",", "min", "(", "im1", ".", "height", ",", "im2", ".", "height", ")", ")", ",", "(", "255", ",", "255", ",", "255", ")", ")", "\n", "dst", ".", "paste", "(", "im1", ",", "(", "0", ",", "0", ")", ")", "\n", "dst", ".", "paste", "(", "im2", ",", "(", "im1", ".", "width", "+", "gap_px", ",", "(", "im1", ".", "height", "-", "im2", ".", "height", ")", "//", "2", ")", ")", "\n", "return", "dst", "\n", "\n"]], "home.repos.pwc.inspect_result.applied-ai-lab_zero-shot-pose.utils.visuals.draw_correspondences_lines": [[116, 153], ["numpy.array", "ax.imshow", "ax.axis", "len", "matplotlib.colors.ListedColormap", "numpy.array", "numpy.array", "zip", "visuals.get_concat_h_cut_center", "len", "len", "matplotlib.Circle", "matplotlib.Circle", "ax.add_patch", "ax.add_patch", "matplotlib.Circle", "matplotlib.Circle", "ax.add_patch", "ax.add_patch", "matplotlib.Line2D", "ax.add_line", "ax.plot", "len", "len", "matplotlib.colors.ListedColormap.", "range"], "function", ["home.repos.pwc.inspect_result.applied-ai-lab_zero-shot-pose.utils.visuals.get_concat_h_cut_center"], ["", "def", "draw_correspondences_lines", "(", "points1", ",", "points2", ",", "image1", ",", "image2", ",", "ax", ")", ":", "\n", "    ", "\"\"\"\n    draw point correspondences on images.\n    :param points1: a list of (y, x) coordinates of image1, corresponding to points2.\n    :param points2: a list of (y, x) coordinates of image2, corresponding to points1.\n    :param image1: a PIL image.\n    :param image2: a PIL image.\n    :param ax: a matplotlib axis object\n    :return: the matplotlib axis.\n    \"\"\"", "\n", "gap", "=", "20", "\n", "\n", "im", "=", "np", ".", "array", "(", "get_concat_h_cut_center", "(", "image1", ",", "image2", ",", "gap", ")", ")", "\n", "ax", ".", "imshow", "(", "im", ")", "\n", "ax", ".", "axis", "(", "'off'", ")", "\n", "assert", "len", "(", "points1", ")", "==", "len", "(", "points2", ")", ",", "f\"points lengths are incompatible: {len(points1)} != {len(points2)}.\"", "\n", "num_points", "=", "len", "(", "points1", ")", "\n", "cmap", "=", "ListedColormap", "(", "[", "\"red\"", ",", "\"yellow\"", ",", "\"blue\"", ",", "\"lime\"", ",", "\"magenta\"", ",", "\"indigo\"", ",", "\"orange\"", ",", "\"cyan\"", ",", "\"darkgreen\"", ",", "\n", "\"maroon\"", ",", "\"black\"", ",", "\"white\"", ",", "\"chocolate\"", ",", "\"gray\"", ",", "\"blueviolet\"", "]", "*", "(", "1", "+", "num_points", "//", "15", ")", ")", "\n", "colors", "=", "np", ".", "array", "(", "[", "cmap", "(", "x", ")", "for", "x", "in", "range", "(", "num_points", ")", "]", ")", "\n", "radius1", ",", "radius2", "=", "6", ",", "2", "\n", "points2", "+=", "np", ".", "array", "(", "[", "0", ",", "gap", "+", "image1", ".", "width", "]", ")", "\n", "for", "point1", ",", "point2", ",", "color", "in", "zip", "(", "points1", ",", "points2", ",", "colors", ")", ":", "\n", "        ", "y1", ",", "x1", "=", "point1", "\n", "circ1_1", "=", "plt", ".", "Circle", "(", "(", "x1", ",", "y1", ")", ",", "radius1", ",", "facecolor", "=", "color", ",", "edgecolor", "=", "'white'", ",", "alpha", "=", "0.5", ")", "\n", "circ1_2", "=", "plt", ".", "Circle", "(", "(", "x1", ",", "y1", ")", ",", "radius2", ",", "facecolor", "=", "color", ",", "edgecolor", "=", "'white'", ")", "\n", "ax", ".", "add_patch", "(", "circ1_1", ")", "\n", "ax", ".", "add_patch", "(", "circ1_2", ")", "\n", "y2", ",", "x2", "=", "point2", "\n", "circ2_1", "=", "plt", ".", "Circle", "(", "(", "x2", ",", "y2", ")", ",", "radius1", ",", "facecolor", "=", "color", ",", "edgecolor", "=", "'white'", ",", "alpha", "=", "0.5", ")", "\n", "circ2_2", "=", "plt", ".", "Circle", "(", "(", "x2", ",", "y2", ")", ",", "radius2", ",", "facecolor", "=", "color", ",", "edgecolor", "=", "'white'", ")", "\n", "ax", ".", "add_patch", "(", "circ2_1", ")", "\n", "ax", ".", "add_patch", "(", "circ2_2", ")", "\n", "l", "=", "mlines", ".", "Line2D", "(", "[", "x1", ",", "x2", "]", ",", "[", "y1", ",", "y2", "]", ",", "c", "=", "color", ",", "linewidth", "=", "0.75", ")", "\n", "ax", ".", "add_line", "(", "l", ")", "\n", "ax", ".", "plot", "(", "x1", ",", "y1", ",", "x2", ",", "y2", ",", "linestyle", "=", "'-'", ",", "c", "=", "'w'", ")", "\n", "", "return", "ax", "\n", "\n"]], "home.repos.pwc.inspect_result.applied-ai-lab_zero-shot-pose.utils.pcd_proc._apply_similarity_transform": [[9, 23], ["torch.bmm"], "function", ["None"], ["def", "_apply_similarity_transform", "(", "\n", "X", ":", "torch", ".", "Tensor", ",", "R", ":", "torch", ".", "Tensor", ",", "T", ":", "torch", ".", "Tensor", ",", "s", ":", "torch", ".", "Tensor", "\n", ")", "->", "torch", ".", "Tensor", ":", "\n", "    ", "\"\"\"\n    Copied from \"https://pytorch3d.readthedocs.io/en/latest/_modules/pytorch3d/ops/points_alignment.html\n    \n    Applies a similarity transformation parametrized with a batch of orthonormal\n    matrices `R` of shape `(minibatch, d, d)`, a batch of translations `T`\n    of shape `(minibatch, d)` and a batch of scaling factors `s`\n    of shape `(minibatch,)` to a given `d`-dimensional cloud `X`\n    of shape `(minibatch, num_points, d)`\n    \"\"\"", "\n", "X", "=", "s", "[", ":", ",", "None", ",", "None", "]", "*", "torch", ".", "bmm", "(", "X", ",", "R", ")", "+", "T", "[", ":", ",", "None", ",", ":", "]", "\n", "return", "X", "\n", "\n"]], "home.repos.pwc.inspect_result.applied-ai-lab_zero-shot-pose.utils.pcd_proc.apply_similarity_transform": [[24, 42], ["pytorch3d.ops.utils.convert_pointclouds_to_tensor", "pcd_proc._apply_similarity_transform", "pytorch3d.ops.utils.is_pointclouds", "X.update_padded"], "function", ["home.repos.pwc.inspect_result.applied-ai-lab_zero-shot-pose.utils.pcd_proc._apply_similarity_transform"], ["", "def", "apply_similarity_transform", "(", "\n", "X", ":", "Union", "[", "torch", ".", "Tensor", ",", "Pointclouds", "]", ",", "R", ":", "torch", ".", "Tensor", ",", "T", ":", "torch", ".", "Tensor", ",", "s", ":", "torch", ".", "Tensor", "\n", ")", "->", "torch", ".", "Tensor", ":", "\n", "    ", "\"\"\"\n    Copied from \"https://pytorch3d.readthedocs.io/en/latest/_modules/pytorch3d/ops/points_alignment.html\n    \n    Wraps _apply_similarity_transform, handles Tensor/Pointcloud conversions\n    \"\"\"", "\n", "# make sure we convert input Pointclouds structures to", "\n", "# padded tensors of shape (N, P, 3)", "\n", "Xt", ",", "num_points_X", "=", "oputil", ".", "convert_pointclouds_to_tensor", "(", "X", ")", "\n", "\n", "# apply the init transform to the input point cloud", "\n", "Xt", "=", "_apply_similarity_transform", "(", "Xt", ",", "R", ",", "T", ",", "s", ")", "\n", "if", "oputil", ".", "is_pointclouds", "(", "X", ")", ":", "\n", "        ", "Xt", "=", "X", ".", "update_padded", "(", "Xt", "[", "0", "]", ")", "# type: ignore", "\n", "\n", "", "return", "Xt", "\n", "\n"]], "home.repos.pwc.inspect_result.applied-ai-lab_zero-shot-pose.utils.pcd_proc.apply_transform_to_pcd": [[43, 60], ["pytorch3d.ops.utils.convert_pointclouds_to_tensor", "tran.transform_points", "pytorch3d.ops.utils.is_pointclouds", "X.update_padded"], "function", ["None"], ["", "def", "apply_transform_to_pcd", "(", "\n", "X", ":", "Pointclouds", ",", "tran", ":", "Transform3d", "\n", ")", "->", "torch", ".", "Tensor", ":", "\n", "    ", "\"\"\"\n    Copied from \"https://pytorch3d.readthedocs.io/en/latest/_modules/pytorch3d/ops/points_alignment.html\n    \n    Wraps a transform, handles Tensor/Pointcloud conversions\n    \"\"\"", "\n", "# make sure we convert input Pointclouds structures to", "\n", "# padded tensors of shape (N, P, 3)", "\n", "Xt", ",", "num_points_X", "=", "oputil", ".", "convert_pointclouds_to_tensor", "(", "X", ")", "\n", "\n", "# apply the transform to the input point cloud", "\n", "Xt", "=", "tran", ".", "transform_points", "(", "Xt", ")", "\n", "if", "oputil", ".", "is_pointclouds", "(", "X", ")", ":", "\n", "        ", "Xt", "=", "X", ".", "update_padded", "(", "Xt", ")", "# type: ignore", "\n", "", "return", "Xt", "\n", "\n"]], "home.repos.pwc.inspect_result.applied-ai-lab_zero-shot-pose.utils.depthproc.cv2_depth_fillmask": [[7, 23], ["numpy.ones", "cv2.dilate", "numpy.abs().max", "cv2.inpaint", "cv2.inpaint.astype", "numpy.abs"], "function", ["None"], ["def", "cv2_depth_fillmask", "(", "depth_crop", ",", "depth_nan_mask", ")", ":", "\n", "    ", "kernel", "=", "np", ".", "ones", "(", "(", "3", ",", "3", ")", ",", "np", ".", "uint8", ")", "\n", "depth_nan_mask", "=", "cv2", ".", "dilate", "(", "depth_nan_mask", ",", "kernel", ",", "iterations", "=", "1", ")", "\n", "\n", "depth_crop", "[", "depth_nan_mask", "==", "1", "]", "=", "0", "\n", "\n", "# Scale to keep as float, but has to be in bounds -1:1 to keep opencv happy.", "\n", "depth_scale", "=", "np", ".", "abs", "(", "depth_crop", ")", ".", "max", "(", ")", "\n", "depth_crop", "=", "depth_crop", ".", "astype", "(", "np", ".", "float32", ")", "/", "depth_scale", "# Has to be float32, 64 not supported.", "\n", "\n", "depth_crop", "=", "cv2", ".", "inpaint", "(", "depth_crop", ",", "depth_nan_mask", ",", "1", ",", "cv2", ".", "INPAINT_NS", ")", "\n", "\n", "# Back to original size and value range.", "\n", "depth_crop", "=", "depth_crop", "[", "1", ":", "-", "1", ",", "1", ":", "-", "1", "]", "\n", "depth_crop", "=", "depth_crop", "*", "depth_scale", "\n", "return", "depth_crop", "\n", "\n"]], "home.repos.pwc.inspect_result.applied-ai-lab_zero-shot-pose.utils.depthproc.cv2_depth_fillzero": [[24, 30], ["cv2.copyMakeBorder", "depthproc.cv2_depth_fillmask"], "function", ["home.repos.pwc.inspect_result.applied-ai-lab_zero-shot-pose.utils.depthproc.cv2_depth_fillmask"], ["", "def", "cv2_depth_fillzero", "(", "depth_crop", ")", ":", "\n", "# OpenCV inpainting does weird things at the border.", "\n", "    ", "depth_crop", "=", "cv2", ".", "copyMakeBorder", "(", "depth_crop", ",", "1", ",", "1", ",", "1", ",", "1", ",", "cv2", ".", "BORDER_DEFAULT", ")", "\n", "depth_nan_mask", "=", "(", "depth_crop", "==", "0", ")", ".", "astype", "(", "np", ".", "uint8", ")", "\n", "depth_crop", "=", "cv2_depth_fillmask", "(", "depth_crop", ",", "depth_nan_mask", ")", "\n", "return", "depth_crop", "\n", "\n"]], "home.repos.pwc.inspect_result.applied-ai-lab_zero-shot-pose.utils.depthproc.cv2_depth_fillna": [[31, 37], ["cv2.copyMakeBorder", "numpy.isnan().astype", "depthproc.cv2_depth_fillmask", "numpy.isnan"], "function", ["home.repos.pwc.inspect_result.applied-ai-lab_zero-shot-pose.utils.depthproc.cv2_depth_fillmask"], ["", "def", "cv2_depth_fillna", "(", "depth_crop", ")", ":", "\n", "# OpenCV inpainting does weird things at the border.", "\n", "    ", "depth_crop", "=", "cv2", ".", "copyMakeBorder", "(", "depth_crop", ",", "1", ",", "1", ",", "1", ",", "1", ",", "cv2", ".", "BORDER_DEFAULT", ")", "\n", "depth_nan_mask", "=", "np", ".", "isnan", "(", "depth_crop", ")", ".", "astype", "(", "np", ".", "uint8", ")", "\n", "depth_crop", "=", "cv2_depth_fillmask", "(", "depth_crop", ",", "depth_nan_mask", ")", "\n", "return", "depth_crop", "\n", "\n"]], "home.repos.pwc.inspect_result.applied-ai-lab_zero-shot-pose.utils.depthproc.transform_cameraframe_to_screen": [[39, 63], ["cam.get_projection_transform", "view_to_ndc_transform.compose.transform_points", "kwargs.get", "pytorch3d.renderer.cameras.get_ndc_to_screen_transform().transform_points", "cam.in_ndc", "cam.get_ndc_camera_transform", "view_to_ndc_transform.compose.compose", "cam.get_image_size", "pytorch3d.renderer.cameras.get_ndc_to_screen_transform"], "function", ["None"], ["", "def", "transform_cameraframe_to_screen", "(", "cam", ",", "P_cam", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\"My version of Camera.transform_points_screen, goes from camera rather than world coords\n    \n    Assumes that P_cam is already in the camera coordinate frame (though not yet NDC).\n    This means that, given world coordinate points and a camera with a viewpoint given by\n    R and T, P_cam is the result of:\n        P_cam = cam.get_world_to_view_transform().transform_points(P_world)\n    \n    This function then takes all of the other steps required to use the camera intrinsics to\n    form an image (or more properly \"screen\" coordinates) of the pointcloud.\n    \"\"\"", "\n", "# Can't just call cam.transform_points_ndc as this assumes points in world", "\n", "# points_ndc = cam.transform_points_ndc(points, eps=eps, **kwargs)", "\n", "view_to_ndc_transform", "=", "cam", ".", "get_projection_transform", "(", ")", "\n", "if", "not", "cam", ".", "in_ndc", "(", ")", ":", "\n", "        ", "to_ndc_transform", "=", "cam", ".", "get_ndc_camera_transform", "(", ")", "\n", "view_to_ndc_transform", "=", "view_to_ndc_transform", ".", "compose", "(", "to_ndc_transform", ")", "\n", "\n", "", "points_ndc", "=", "view_to_ndc_transform", ".", "transform_points", "(", "P_cam", ")", "\n", "image_size", "=", "kwargs", ".", "get", "(", "\"image_size\"", ",", "cam", ".", "get_image_size", "(", ")", ")", "\n", "\n", "return", "get_ndc_to_screen_transform", "(", "\n", "cam", ",", "with_xyflip", "=", "True", ",", "image_size", "=", "image_size", "\n", ")", ".", "transform_points", "(", "points_ndc", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.applied-ai-lab_zero-shot-pose.utils.depthproc.get_structured_pcd": [[64, 105], ["pytorch3d.renderer.cameras.get_ndc_to_screen_transform().inverse().get_matrix().squeeze", "torch.arange", "torch.arange", "torch.meshgrid", "torch.stack", "frame.camera.unproject_points", "frame.camera.unproject_points.view", "torch.Tensor", "frame.depth_map.squeeze", "pytorch3d.renderer.cameras.get_ndc_to_screen_transform().inverse().get_matrix", "depthproc.cv2_depth_fillzero", "torch.ones_like", "torch.stack.float().view", "frame.depth_map.squeeze().numpy", "pytorch3d.renderer.cameras.get_ndc_to_screen_transform().inverse", "torch.stack.float", "frame.depth_map.squeeze", "pytorch3d.renderer.cameras.get_ndc_to_screen_transform"], "function", ["home.repos.pwc.inspect_result.applied-ai-lab_zero-shot-pose.utils.depthproc.cv2_depth_fillzero"], ["", "def", "get_structured_pcd", "(", "frame", ",", "inpaint", "=", "True", ",", "world_coordinates", "=", "False", ")", ":", "\n", "    ", "\"\"\"Takes a frame from CO3D dataset, returns a structured pointcloud\n\n    Pointcloud returned is in world-coordinates, with shape (H, W, 3), \n    with the 3 dimensions encoding X, Y and Z world coordinates.\n    Optionally infills the depth map from the CO3D dataset, which tends\n    to be ~50% zeros (the equivalent of NaNs in the .png encoding). This\n    leads to fewer NaNs in the unprojected structured pointcloud.\n\n    world_coordinates passed to cam.unproject_points: if it is false, the\n    points are unprojected to the *camera* frame, else to the world frame\n    \"\"\"", "\n", "# --- 1. Get transform from screen to NDC space ---", "\n", "# image_size in Pytorch3D camera is (H, W) ordering", "\n", "H", ",", "W", "=", "frame", ".", "image_rgb", ".", "shape", "[", "1", ":", "]", "\n", "# get the ndc_to_screen transform, invert it for screen to NDC", "\n", "ndc_to_screen_transform", "=", "get_ndc_to_screen_transform", "(", "\n", "frame", ".", "camera", ",", "with_xyflip", "=", "False", ",", "image_size", "=", "(", "H", ",", "W", ")", ")", ".", "inverse", "(", ")", ".", "get_matrix", "(", ")", ".", "squeeze", "(", ")", "\n", "\n", "# --- 2. Make a grid of the coordinates in Pytorch3D screen space ---", "\n", "x", "=", "torch", ".", "arange", "(", "W", ")", "\n", "y", "=", "torch", ".", "arange", "(", "H", ")", "\n", "grid_y", ",", "grid_x", "=", "torch", ".", "meshgrid", "(", "y", ",", "x", ")", "# both returns have shape (H, W)", "\n", "\n", "if", "inpaint", ":", "\n", "        ", "depth_proc", "=", "torch", ".", "Tensor", "(", "cv2_depth_fillzero", "(", "frame", ".", "depth_map", ".", "squeeze", "(", ")", ".", "numpy", "(", ")", ")", ")", "\n", "", "else", ":", "\n", "        ", "depth_proc", "=", "frame", ".", "depth_map", ".", "squeeze", "(", ")", "\n", "\n", "# Stack the xy coords with depth, which is in NDC/screen format (no difference in this case, I think)", "\n", "", "xy_grid", "=", "torch", ".", "stack", "(", "(", "grid_y", ",", "grid_x", ",", "depth_proc", ",", "torch", ".", "ones_like", "(", "grid_x", ")", ")", ",", "dim", "=", "-", "1", ")", "\n", "\n", "# --- 3. Convert the 4D vectors to NDC space ---", "\n", "# Transpose the transform to operate on column vectors (i.e. normal convention, but not in Pytorch3D)", "\n", "xy_grid_ndc", "=", "ndc_to_screen_transform", ".", "T", "@", "xy_grid", ".", "float", "(", ")", ".", "view", "(", "-", "1", ",", "4", ")", ".", "T", "# (4, H*W)", "\n", "# --- 4. Unproject ---", "\n", "unproj", "=", "frame", ".", "camera", ".", "unproject_points", "(", "xy_grid_ndc", ".", "T", "[", "...", ",", ":", "3", "]", ",", "world_coordinates", ")", "# (H*W, 3)", "\n", "# --- 5. Convert back to image shape, remove homogeneous dimension ---", "\n", "structured_pcd", "=", "unproj", ".", "view", "(", "H", ",", "W", ",", "3", ")", "\n", "\n", "return", "structured_pcd", "\n", "\n"]], "home.repos.pwc.inspect_result.applied-ai-lab_zero-shot-pose.utils.depthproc.get_structured_pcd2": [[106, 144], ["pytorch3d.renderer.cameras.get_ndc_to_screen_transform().inverse().get_matrix().squeeze", "pytorch3d.renderer.implicit.NDCGridRaysampler", "torch.cat", "xy_grid_ndc.view.view", "frame.camera.unproject_points", "frame.camera.unproject_points.view", "torch.Tensor", "frame.depth_map.squeeze", "pytorch3d.renderer.cameras.get_ndc_to_screen_transform().inverse().get_matrix", "depthproc.cv2_depth_fillzero", "frame.depth_map.squeeze.unsqueeze", "frame.depth_map.squeeze().numpy", "pytorch3d.renderer.cameras.get_ndc_to_screen_transform().inverse", "frame.depth_map.squeeze", "pytorch3d.renderer.cameras.get_ndc_to_screen_transform"], "function", ["home.repos.pwc.inspect_result.applied-ai-lab_zero-shot-pose.utils.depthproc.cv2_depth_fillzero"], ["", "def", "get_structured_pcd2", "(", "frame", ",", "inpaint", "=", "True", ",", "world_coordinates", "=", "False", ")", ":", "\n", "    ", "\"\"\"Takes a frame from CO3D dataset, returns a structured pointcloud\n\n    Pointcloud returned is in world-coordinates, with shape (H, W, 3), \n    with the 3 dimensions encoding X, Y and Z world coordinates.\n    Optionally infills the depth map from the CO3D dataset, which tends\n    to be ~50% zeros (the equivalent of NaNs in the .png encoding). This\n    leads to fewer NaNs in the unprojected structured pointcloud.\n\n    world_coordinates passed to cam.unproject_points: if it is false, the\n    points are unprojected to the *camera* frame, else to the world frame\n    \"\"\"", "\n", "# --- 1. Get transform from screen to NDC space ---", "\n", "# image_size in Pytorch3D camera is (H, W) ordering", "\n", "H", ",", "W", "=", "frame", ".", "image_rgb", ".", "shape", "[", "1", ":", "]", "\n", "# get the ndc_to_screen transform, invert it for screen to NDC", "\n", "ndc_to_screen_transform", "=", "get_ndc_to_screen_transform", "(", "\n", "frame", ".", "camera", ",", "with_xyflip", "=", "False", ",", "image_size", "=", "(", "H", ",", "W", ")", ")", ".", "inverse", "(", ")", ".", "get_matrix", "(", ")", ".", "squeeze", "(", ")", "\n", "\n", "# --- 2. Make a grid of the coordinates in Pytorch3D screen space ---", "\n", "gridsampler", "=", "NDCGridRaysampler", "(", "image_width", "=", "W", ",", "image_height", "=", "H", ",", "n_pts_per_ray", "=", "1", ",", "min_depth", "=", "0", ",", "max_depth", "=", "1", ")", "\n", "xy_grid", "=", "gridsampler", ".", "_xy_grid", "\n", "\n", "if", "inpaint", ":", "\n", "        ", "depth_proc", "=", "torch", ".", "Tensor", "(", "cv2_depth_fillzero", "(", "frame", ".", "depth_map", ".", "squeeze", "(", ")", ".", "numpy", "(", ")", ")", ")", "\n", "", "else", ":", "\n", "        ", "depth_proc", "=", "frame", ".", "depth_map", ".", "squeeze", "(", ")", "\n", "\n", "# Stack the xy coords with depth, which is in NDC/screen format (no difference in this case, I think)", "\n", "", "xy_grid_ndc", "=", "torch", ".", "cat", "(", "(", "xy_grid", ",", "\n", "depth_proc", ".", "unsqueeze", "(", "-", "1", ")", ")", ",", "dim", "=", "-", "1", ")", "\n", "xy_grid_ndc", "=", "xy_grid_ndc", ".", "view", "(", "-", "1", ",", "3", ")", "\n", "#     return xy_grid", "\n", "# --- 4. Unproject ---", "\n", "unproj", "=", "frame", ".", "camera", ".", "unproject_points", "(", "xy_grid_ndc", ",", "world_coordinates", ")", "# (H*W, 3)", "\n", "# --- 5. Convert back to image shape, remove homogeneous dimension ---", "\n", "structured_pcd", "=", "unproj", ".", "view", "(", "H", ",", "W", ",", "3", ")", "\n", "return", "structured_pcd", "", "", ""]], "home.repos.pwc.inspect_result.applied-ai-lab_zero-shot-pose.utils.project_utils.AverageMeter.__init__": [[14, 16], ["project_utils.AverageMeter.reset"], "methods", ["home.repos.pwc.inspect_result.applied-ai-lab_zero-shot-pose.utils.project_utils.AverageMeter.reset"], ["def", "__init__", "(", "self", ")", ":", "\n", "        ", "self", ".", "reset", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.applied-ai-lab_zero-shot-pose.utils.project_utils.AverageMeter.reset": [[17, 23], ["None"], "methods", ["None"], ["", "def", "reset", "(", "self", ")", ":", "\n", "\n", "        ", "self", ".", "val", "=", "0", "\n", "self", ".", "avg", "=", "0", "\n", "self", ".", "sum", "=", "0", "\n", "self", ".", "count", "=", "0", "\n", "\n"]], "home.repos.pwc.inspect_result.applied-ai-lab_zero-shot-pose.utils.project_utils.AverageMeter.update": [[24, 30], ["None"], "methods", ["None"], ["", "def", "update", "(", "self", ",", "val", ",", "n", "=", "1", ")", ":", "\n", "\n", "        ", "self", ".", "val", "=", "val", "\n", "self", ".", "sum", "+=", "val", "*", "n", "\n", "self", ".", "count", "+=", "n", "\n", "self", ".", "avg", "=", "self", ".", "sum", "/", "self", ".", "count", "\n", "\n"]], "home.repos.pwc.inspect_result.applied-ai-lab_zero-shot-pose.utils.project_utils.ClassificationPredSaver.__init__": [[118, 132], ["save_path.split"], "methods", ["None"], ["    ", "def", "__init__", "(", "self", ",", "length", ",", "save_path", "=", "None", ")", ":", "\n", "\n", "        ", "if", "save_path", "is", "not", "None", ":", "\n", "\n", "# Remove filetype from save_path", "\n", "            ", "save_path", "=", "save_path", ".", "split", "(", "'.'", ")", "[", "0", "]", "\n", "self", ".", "save_path", "=", "save_path", "\n", "\n", "", "self", ".", "length", "=", "length", "\n", "\n", "self", ".", "all_preds", "=", "None", "\n", "self", ".", "all_labels", "=", "None", "\n", "\n", "self", ".", "running_start_idx", "=", "0", "\n", "\n"]], "home.repos.pwc.inspect_result.applied-ai-lab_zero-shot-pose.utils.project_utils.ClassificationPredSaver.update": [[133, 158], ["torch.is_tensor", "preds.detach().cpu().numpy.detach().cpu().numpy.detach().cpu().numpy", "numpy.zeros", "torch.is_tensor", "labels.detach().cpu().numpy.detach().cpu().numpy.detach().cpu().numpy", "numpy.zeros", "preds.detach().cpu().numpy.detach().cpu().numpy.detach().cpu", "labels.detach().cpu().numpy.detach().cpu().numpy.detach().cpu", "preds.detach().cpu().numpy.detach().cpu().numpy.detach", "labels.detach().cpu().numpy.detach().cpu().numpy.detach"], "methods", ["home.repos.pwc.inspect_result.applied-ai-lab_zero-shot-pose.datasets.co3d_dataset.FrameData.cpu", "home.repos.pwc.inspect_result.applied-ai-lab_zero-shot-pose.datasets.co3d_dataset.FrameData.cpu"], ["", "def", "update", "(", "self", ",", "preds", ",", "labels", "=", "None", ")", ":", "\n", "\n", "# Expect preds in shape B x C", "\n", "\n", "        ", "if", "torch", ".", "is_tensor", "(", "preds", ")", ":", "\n", "            ", "preds", "=", "preds", ".", "detach", "(", ")", ".", "cpu", "(", ")", ".", "numpy", "(", ")", "\n", "\n", "", "b", ",", "c", "=", "preds", ".", "shape", "\n", "\n", "if", "self", ".", "all_preds", "is", "None", ":", "\n", "            ", "self", ".", "all_preds", "=", "np", ".", "zeros", "(", "(", "self", ".", "length", ",", "c", ")", ")", "\n", "\n", "", "self", ".", "all_preds", "[", "self", ".", "running_start_idx", ":", "self", ".", "running_start_idx", "+", "b", "]", "=", "preds", "\n", "\n", "if", "labels", "is", "not", "None", ":", "\n", "            ", "if", "torch", ".", "is_tensor", "(", "labels", ")", ":", "\n", "                ", "labels", "=", "labels", ".", "detach", "(", ")", ".", "cpu", "(", ")", ".", "numpy", "(", ")", "\n", "\n", "", "if", "self", ".", "all_labels", "is", "None", ":", "\n", "                ", "self", ".", "all_labels", "=", "np", ".", "zeros", "(", "(", "self", ".", "length", ",", ")", ")", "\n", "\n", "", "self", ".", "all_labels", "[", "self", ".", "running_start_idx", ":", "self", ".", "running_start_idx", "+", "b", "]", "=", "labels", "\n", "\n", "# Maintain running index on dataset being evaluated", "\n", "", "self", ".", "running_start_idx", "+=", "b", "\n", "\n"]], "home.repos.pwc.inspect_result.applied-ai-lab_zero-shot-pose.utils.project_utils.ClassificationPredSaver.save": [[159, 176], ["torch.from_numpy", "torch.from_numpy.numpy", "print", "torch.save", "torch.nn.Softmax", "project_utils.ClassificationPredSaver.evaluate", "torch.save"], "methods", ["home.repos.pwc.inspect_result.applied-ai-lab_zero-shot-pose.utils.project_utils.ClassificationPredSaver.save", "home.repos.pwc.inspect_result.applied-ai-lab_zero-shot-pose.utils.project_utils.ClassificationPredSaver.evaluate", "home.repos.pwc.inspect_result.applied-ai-lab_zero-shot-pose.utils.project_utils.ClassificationPredSaver.save"], ["", "def", "save", "(", "self", ")", ":", "\n", "\n", "# Softmax over preds", "\n", "        ", "preds", "=", "torch", ".", "from_numpy", "(", "self", ".", "all_preds", ")", "\n", "preds", "=", "torch", ".", "nn", ".", "Softmax", "(", "dim", "=", "-", "1", ")", "(", "preds", ")", "\n", "self", ".", "all_preds", "=", "preds", ".", "numpy", "(", ")", "\n", "\n", "pred_path", "=", "self", ".", "save_path", "+", "'.pth'", "\n", "print", "(", "f'Saving all predictions to {pred_path}'", ")", "\n", "\n", "torch", ".", "save", "(", "self", ".", "all_preds", ",", "pred_path", ")", "\n", "\n", "if", "self", ".", "all_labels", "is", "not", "None", ":", "\n", "\n", "# Evaluate", "\n", "            ", "self", ".", "evaluate", "(", ")", "\n", "torch", ".", "save", "(", "self", ".", "all_labels", ",", "self", ".", "save_path", "+", "'_labels.pth'", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.applied-ai-lab_zero-shot-pose.utils.project_utils.ClassificationPredSaver.evaluate": [[177, 185], ["project_utils.accuracy", "zip", "torch.from_numpy", "torch.from_numpy", "print", "a.item"], "methods", ["home.repos.pwc.inspect_result.applied-ai-lab_zero-shot-pose.utils.project_utils.accuracy"], ["", "", "def", "evaluate", "(", "self", ")", ":", "\n", "\n", "        ", "topk", "=", "[", "1", ",", "5", ",", "10", "]", "\n", "topk", "=", "[", "k", "for", "k", "in", "topk", "if", "k", "<", "self", ".", "all_preds", ".", "shape", "[", "-", "1", "]", "]", "\n", "acc", "=", "accuracy", "(", "torch", ".", "from_numpy", "(", "self", ".", "all_preds", ")", ",", "torch", ".", "from_numpy", "(", "self", ".", "all_labels", ")", ",", "topk", "=", "topk", ")", "\n", "\n", "for", "k", ",", "a", "in", "zip", "(", "topk", ",", "acc", ")", ":", "\n", "            ", "print", "(", "f'Top{k} Acc: {a.item()}'", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.applied-ai-lab_zero-shot-pose.utils.project_utils.IndicatePlateau.__init__": [[211, 228], ["project_utils.IndicatePlateau._init_is_better", "project_utils.IndicatePlateau._init_is_better", "project_utils.IndicatePlateau._reset"], "methods", ["home.repos.pwc.inspect_result.applied-ai-lab_zero-shot-pose.utils.project_utils.IndicatePlateau._init_is_better", "home.repos.pwc.inspect_result.applied-ai-lab_zero-shot-pose.utils.project_utils.IndicatePlateau._init_is_better", "home.repos.pwc.inspect_result.applied-ai-lab_zero-shot-pose.utils.project_utils.IndicatePlateau._reset"], ["    ", "def", "__init__", "(", "self", ",", "threshold", "=", "5e-4", ",", "patience_epochs", "=", "5", ",", "mode", "=", "'min'", ",", "threshold_mode", "=", "'rel'", ")", ":", "\n", "\n", "        ", "self", ".", "patience", "=", "patience_epochs", "\n", "self", ".", "cooldown_counter", "=", "0", "\n", "self", ".", "mode", "=", "mode", "\n", "self", ".", "threshold", "=", "threshold", "\n", "self", ".", "threshold_mode", "=", "threshold_mode", "\n", "self", ".", "best", "=", "None", "\n", "self", ".", "num_bad_epochs", "=", "None", "\n", "self", ".", "mode_worse", "=", "None", "# the worse value for the chosen mode", "\n", "self", ".", "last_epoch", "=", "0", "\n", "self", ".", "_init_is_better", "(", "mode", "=", "mode", ",", "threshold", "=", "threshold", ",", "\n", "threshold_mode", "=", "threshold_mode", ")", "\n", "\n", "self", ".", "_init_is_better", "(", "mode", "=", "mode", ",", "threshold", "=", "threshold", ",", "\n", "threshold_mode", "=", "threshold_mode", ")", "\n", "self", ".", "_reset", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.applied-ai-lab_zero-shot-pose.utils.project_utils.IndicatePlateau._reset": [[229, 234], ["None"], "methods", ["None"], ["", "def", "_reset", "(", "self", ")", ":", "\n", "        ", "\"\"\"Resets num_bad_epochs counter and cooldown counter.\"\"\"", "\n", "self", ".", "best", "=", "self", ".", "mode_worse", "\n", "self", ".", "cooldown_counter", "=", "0", "\n", "self", ".", "num_bad_epochs", "=", "0", "\n", "\n"]], "home.repos.pwc.inspect_result.applied-ai-lab_zero-shot-pose.utils.project_utils.IndicatePlateau.step": [[235, 252], ["float", "project_utils.IndicatePlateau.is_better", "print", "project_utils.IndicatePlateau._reset"], "methods", ["home.repos.pwc.inspect_result.applied-ai-lab_zero-shot-pose.utils.project_utils.IndicatePlateau.is_better", "home.repos.pwc.inspect_result.applied-ai-lab_zero-shot-pose.utils.project_utils.IndicatePlateau._reset"], ["", "def", "step", "(", "self", ",", "metrics", ",", "epoch", "=", "None", ")", ":", "\n", "# convert `metrics` to float, in case it's a zero-dim Tensor", "\n", "        ", "current", "=", "float", "(", "metrics", ")", "\n", "self", ".", "last_epoch", "=", "epoch", "\n", "\n", "if", "self", ".", "is_better", "(", "current", ",", "self", ".", "best", ")", ":", "\n", "            ", "self", ".", "best", "=", "current", "\n", "self", ".", "num_bad_epochs", "=", "0", "\n", "", "else", ":", "\n", "            ", "self", ".", "num_bad_epochs", "+=", "1", "\n", "\n", "", "if", "self", ".", "num_bad_epochs", ">", "self", ".", "patience", ":", "\n", "            ", "print", "(", "'Tracked metric has plateaud'", ")", "\n", "self", ".", "_reset", "(", ")", "\n", "return", "True", "\n", "", "else", ":", "\n", "            ", "return", "False", "\n", "\n"]], "home.repos.pwc.inspect_result.applied-ai-lab_zero-shot-pose.utils.project_utils.IndicatePlateau.is_better": [[253, 268], ["None"], "methods", ["None"], ["", "", "def", "is_better", "(", "self", ",", "a", ",", "best", ")", ":", "\n", "\n", "        ", "if", "self", ".", "mode", "==", "'min'", "and", "self", ".", "threshold_mode", "==", "'rel'", ":", "\n", "            ", "rel_epsilon", "=", "1.", "-", "self", ".", "threshold", "\n", "return", "a", "<", "best", "*", "rel_epsilon", "\n", "\n", "", "elif", "self", ".", "mode", "==", "'min'", "and", "self", ".", "threshold_mode", "==", "'abs'", ":", "\n", "            ", "return", "a", "<", "best", "-", "self", ".", "threshold", "\n", "\n", "", "elif", "self", ".", "mode", "==", "'max'", "and", "self", ".", "threshold_mode", "==", "'rel'", ":", "\n", "            ", "rel_epsilon", "=", "self", ".", "threshold", "+", "1.", "\n", "return", "a", ">", "best", "*", "rel_epsilon", "\n", "\n", "", "else", ":", "# mode == 'max' and epsilon_mode == 'abs':", "\n", "            ", "return", "a", ">", "best", "+", "self", ".", "threshold", "\n", "\n"]], "home.repos.pwc.inspect_result.applied-ai-lab_zero-shot-pose.utils.project_utils.IndicatePlateau._init_is_better": [[269, 284], ["ValueError", "ValueError", "float", "float"], "methods", ["None"], ["", "", "def", "_init_is_better", "(", "self", ",", "mode", ",", "threshold", ",", "threshold_mode", ")", ":", "\n", "\n", "        ", "if", "mode", "not", "in", "{", "'min'", ",", "'max'", "}", ":", "\n", "            ", "raise", "ValueError", "(", "'mode '", "+", "mode", "+", "' is unknown!'", ")", "\n", "", "if", "threshold_mode", "not", "in", "{", "'rel'", ",", "'abs'", "}", ":", "\n", "            ", "raise", "ValueError", "(", "'threshold mode '", "+", "threshold_mode", "+", "' is unknown!'", ")", "\n", "\n", "", "if", "mode", "==", "'min'", ":", "\n", "            ", "self", ".", "mode_worse", "=", "float", "(", "'inf'", ")", "\n", "", "else", ":", "# mode == 'max':", "\n", "            ", "self", ".", "mode_worse", "=", "-", "float", "(", "'inf'", ")", "\n", "\n", "", "self", ".", "mode", "=", "mode", "\n", "self", ".", "threshold", "=", "threshold", "\n", "self", ".", "threshold_mode", "=", "threshold_mode", "\n", "\n"]], "home.repos.pwc.inspect_result.applied-ai-lab_zero-shot-pose.utils.project_utils.seed_torch": [[32, 42], ["random.seed", "str", "numpy.random.seed", "torch.manual_seed", "torch.cuda.manual_seed", "torch.cuda.manual_seed_all"], "function", ["None"], ["", "", "def", "seed_torch", "(", "seed", "=", "1029", ")", ":", "\n", "\n", "    ", "random", ".", "seed", "(", "seed", ")", "\n", "os", ".", "environ", "[", "'PYTHONHASHSEED'", "]", "=", "str", "(", "seed", ")", "\n", "np", ".", "random", ".", "seed", "(", "seed", ")", "\n", "torch", ".", "manual_seed", "(", "seed", ")", "\n", "torch", ".", "cuda", ".", "manual_seed", "(", "seed", ")", "\n", "torch", ".", "cuda", ".", "manual_seed_all", "(", "seed", ")", "# if you are using multi-GPU.", "\n", "torch", ".", "backends", ".", "cudnn", ".", "benchmark", "=", "False", "\n", "torch", ".", "backends", ".", "cudnn", ".", "deterministic", "=", "True", "\n", "\n"]], "home.repos.pwc.inspect_result.applied-ai-lab_zero-shot-pose.utils.project_utils.strip_state_dict": [[44, 57], ["list", "state_dict.keys", "k.startswith", "len"], "function", ["home.repos.pwc.inspect_result.applied-ai-lab_zero-shot-pose.datasets.co3d_dataset.FrameData.keys"], ["", "def", "strip_state_dict", "(", "state_dict", ",", "strip_key", "=", "'module.'", ")", ":", "\n", "\n", "    ", "\"\"\"\n    Strip 'module' from start of state_dict keys\n    Useful if model has been trained as DataParallel model\n    \"\"\"", "\n", "\n", "for", "k", "in", "list", "(", "state_dict", ".", "keys", "(", ")", ")", ":", "\n", "        ", "if", "k", ".", "startswith", "(", "strip_key", ")", ":", "\n", "            ", "state_dict", "[", "k", "[", "len", "(", "strip_key", ")", ":", "]", "]", "=", "state_dict", "[", "k", "]", "\n", "del", "state_dict", "[", "k", "]", "\n", "\n", "", "", "return", "state_dict", "\n", "\n"]], "home.repos.pwc.inspect_result.applied-ai-lab_zero-shot-pose.utils.project_utils.transform_moco_state_dict": [[59, 87], ["obj.items", "k.replace.replace", "k.replace.startswith", "k.replace.startswith", "k.replace.startswith", "k.replace.replace", "torch.randn", "torch.randn", "torch.randn.size"], "function", ["None"], ["", "def", "transform_moco_state_dict", "(", "obj", ",", "num_classes", ")", ":", "\n", "\n", "    ", "\"\"\"\n    :param obj: Moco State Dict\n    :param args: argsparse object with training classes\n    :return: State dict compatable with standard resnet architecture\n    \"\"\"", "\n", "\n", "newmodel", "=", "{", "}", "\n", "for", "k", ",", "v", "in", "obj", ".", "items", "(", ")", ":", "\n", "        ", "if", "not", "k", ".", "startswith", "(", "\"module.encoder_q.\"", ")", ":", "\n", "            ", "continue", "\n", "", "old_k", "=", "k", "\n", "k", "=", "k", ".", "replace", "(", "\"module.encoder_q.\"", ",", "\"\"", ")", "\n", "\n", "if", "k", ".", "startswith", "(", "\"fc.2\"", ")", ":", "\n", "            ", "continue", "\n", "\n", "", "if", "k", ".", "startswith", "(", "\"fc.0\"", ")", ":", "\n", "            ", "k", "=", "k", ".", "replace", "(", "\"0.\"", ",", "\"\"", ")", "\n", "if", "\"weight\"", "in", "k", ":", "\n", "                ", "v", "=", "torch", ".", "randn", "(", "(", "num_classes", ",", "v", ".", "size", "(", "1", ")", ")", ")", "\n", "", "elif", "\"bias\"", "in", "k", ":", "\n", "                ", "v", "=", "torch", ".", "randn", "(", "(", "num_classes", ",", ")", ")", "\n", "\n", "", "", "newmodel", "[", "k", "]", "=", "v", "\n", "\n", "", "return", "newmodel", "\n", "\n"]], "home.repos.pwc.inspect_result.applied-ai-lab_zero-shot-pose.utils.project_utils.accuracy": [[90, 105], ["torch.no_grad", "max", "target.size", "output.topk", "pred.t.t", "pred.t.eq", "target.view().expand_as", "correct[].reshape().float().sum", "res.append", "correct[].reshape().float().sum.mul_", "target.view", "correct[].reshape().float", "correct[].reshape"], "function", ["None"], ["", "def", "accuracy", "(", "output", ",", "target", ",", "topk", "=", "(", "1", ",", ")", ")", ":", "\n", "    ", "\"\"\"Computes the accuracy over the k top predictions for the specified values of k\"\"\"", "\n", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "        ", "maxk", "=", "max", "(", "topk", ")", "\n", "batch_size", "=", "target", ".", "size", "(", "0", ")", "\n", "\n", "_", ",", "pred", "=", "output", ".", "topk", "(", "maxk", ",", "1", ",", "True", ",", "True", ")", "\n", "pred", "=", "pred", ".", "t", "(", ")", "\n", "correct", "=", "pred", ".", "eq", "(", "target", ".", "view", "(", "1", ",", "-", "1", ")", ".", "expand_as", "(", "pred", ")", ")", "\n", "\n", "res", "=", "[", "]", "\n", "for", "k", "in", "topk", ":", "\n", "            ", "correct_k", "=", "correct", "[", ":", "k", "]", ".", "reshape", "(", "-", "1", ")", ".", "float", "(", ")", ".", "sum", "(", "0", ",", "keepdim", "=", "True", ")", "\n", "res", ".", "append", "(", "correct_k", ".", "mul_", "(", "100.0", "/", "batch_size", ")", ")", "\n", "", "return", "res", "\n", "\n"]], "home.repos.pwc.inspect_result.applied-ai-lab_zero-shot-pose.utils.project_utils.str2bool": [[107, 114], ["v.lower", "v.lower", "argparse.ArgumentTypeError"], "function", ["None"], ["", "", "def", "str2bool", "(", "v", ")", ":", "\n", "    ", "if", "v", ".", "lower", "(", ")", "in", "(", "'yes'", ",", "'true'", ",", "'t'", ",", "'y'", ",", "'1'", ")", ":", "\n", "        ", "return", "True", "\n", "", "elif", "v", ".", "lower", "(", ")", "in", "(", "'no'", ",", "'false'", ",", "'f'", ",", "'n'", ",", "'0'", ")", ":", "\n", "        ", "return", "False", "\n", "", "else", ":", "\n", "        ", "raise", "argparse", ".", "ArgumentTypeError", "(", "'Boolean value expected.'", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.applied-ai-lab_zero-shot-pose.utils.project_utils.get_acc_auroc_curves": [[187, 204], ["EventAccumulator", "EventAccumulator.Reload", "EventAccumulator.Tags", "numpy.array"], "function", ["None"], ["", "", "", "def", "get_acc_auroc_curves", "(", "logdir", ")", ":", "\n", "\n", "    ", "\"\"\"\n    :param logdir: Path to logs: E.g '/work/me/logs/'\n    :return:\n    \"\"\"", "\n", "\n", "event_acc", "=", "EventAccumulator", "(", "logdir", ")", "\n", "event_acc", ".", "Reload", "(", ")", "\n", "\n", "# Only gets scalars", "\n", "log_info", "=", "{", "}", "\n", "for", "tag", "in", "event_acc", ".", "Tags", "(", ")", "[", "'scalars'", "]", ":", "\n", "\n", "        ", "log_info", "[", "tag", "]", "=", "np", ".", "array", "(", "[", "[", "x", ".", "step", ",", "x", ".", "value", "]", "for", "x", "in", "event_acc", ".", "scalars", ".", "_buckets", "[", "tag", "]", ".", "items", "]", ")", "\n", "\n", "", "return", "log_info", "\n", "\n"]], "home.repos.pwc.inspect_result.applied-ai-lab_zero-shot-pose.utils.project_utils.get_mean_lr": [[206, 208], ["torch.mean().item", "torch.mean", "torch.Tensor"], "function", ["None"], ["", "def", "get_mean_lr", "(", "optimizer", ")", ":", "\n", "    ", "return", "torch", ".", "mean", "(", "torch", ".", "Tensor", "(", "[", "param_group", "[", "'lr'", "]", "for", "param_group", "in", "optimizer", ".", "param_groups", "]", ")", ")", ".", "item", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.applied-ai-lab_zero-shot-pose.utils.project_utils.denorm_image": [[286, 290], ["None"], "function", ["None"], ["", "", "def", "denorm_image", "(", "image", ",", "mean", ",", "std", ")", ":", "\n", "    ", "image", "=", "image", "*", "std", "[", ":", ",", "None", ",", "None", "]", "\n", "image", "=", "image", "+", "mean", "[", ":", ",", "None", ",", "None", "]", "\n", "return", "image", "\n", "\n"]], "home.repos.pwc.inspect_result.applied-ai-lab_zero-shot-pose.utils.project_utils.square_bbox": [[292, 312], ["float", "int", "int", "int", "max", "round", "round", "round"], "function", ["None"], ["", "def", "square_bbox", "(", "bbox", ")", ":", "\n", "    ", "'''\n    Converts a bbox to have a square shape by increasing size along non-max dimension.\n\n    https://github.com/akanazawa/cmr\n    '''", "\n", "sq_bbox", "=", "[", "int", "(", "round", "(", "coord", ")", ")", "for", "coord", "in", "bbox", "]", "\n", "bwidth", "=", "sq_bbox", "[", "2", "]", "-", "sq_bbox", "[", "0", "]", "+", "1", "\n", "bheight", "=", "sq_bbox", "[", "3", "]", "-", "sq_bbox", "[", "1", "]", "+", "1", "\n", "maxdim", "=", "float", "(", "max", "(", "bwidth", ",", "bheight", ")", ")", "\n", "\n", "dw_b_2", "=", "int", "(", "round", "(", "(", "maxdim", "-", "bwidth", ")", "/", "2.0", ")", ")", "\n", "dh_b_2", "=", "int", "(", "round", "(", "(", "maxdim", "-", "bheight", ")", "/", "2.0", ")", ")", "\n", "\n", "sq_bbox", "[", "0", "]", "-=", "dw_b_2", "\n", "sq_bbox", "[", "1", "]", "-=", "dh_b_2", "\n", "sq_bbox", "[", "2", "]", "=", "sq_bbox", "[", "0", "]", "+", "maxdim", "-", "1", "\n", "sq_bbox", "[", "3", "]", "=", "sq_bbox", "[", "1", "]", "+", "maxdim", "-", "1", "\n", "\n", "return", "sq_bbox", "\n", "\n"]], "home.repos.pwc.inspect_result.applied-ai-lab_zero-shot-pose.utils.project_utils.get_results_length": [[327, 335], ["os.path.exists", "list", "len", "open", "list.remove"], "function", ["None"], ["", "", "", "def", "get_results_length", "(", "results_file", ")", ":", "\n", "    ", "if", "os", ".", "path", ".", "exists", "(", "results_file", ")", ":", "\n", "        ", "lines", "=", "list", "(", "open", "(", "results_file", ",", "'r'", ")", ")", "\n", "if", "'THIS CATEGORY BROKE'", "in", "lines", ":", "\n", "            ", "lines", ".", "remove", "(", "'THIS CATEGORY BROKE'", ")", "\n", "", "return", "len", "(", "lines", ")", "\n", "", "else", ":", "\n", "        ", "return", "0", "\n", "", "", ""]], "home.repos.pwc.inspect_result.applied-ai-lab_zero-shot-pose.datasets.co3d_dataset.FrameData.to": [[112, 121], ["dataclasses.fields", "getattr", "isinstance", "type", "getattr.to"], "methods", ["home.repos.pwc.inspect_result.applied-ai-lab_zero-shot-pose.datasets.co3d_dataset.FrameData.to"], ["def", "to", "(", "self", ",", "*", "args", ",", "**", "kwargs", ")", ":", "\n", "        ", "new_params", "=", "{", "}", "\n", "for", "f", "in", "fields", "(", "self", ")", ":", "\n", "            ", "value", "=", "getattr", "(", "self", ",", "f", ".", "name", ")", "\n", "if", "isinstance", "(", "value", ",", "(", "torch", ".", "Tensor", ",", "Pointclouds", ",", "CamerasBase", ")", ")", ":", "\n", "                ", "new_params", "[", "f", ".", "name", "]", "=", "value", ".", "to", "(", "*", "args", ",", "**", "kwargs", ")", "\n", "", "else", ":", "\n", "                ", "new_params", "[", "f", ".", "name", "]", "=", "value", "\n", "", "", "return", "type", "(", "self", ")", "(", "**", "new_params", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.applied-ai-lab_zero-shot-pose.datasets.co3d_dataset.FrameData.cpu": [[122, 124], ["co3d_dataset.FrameData.to", "torch.device"], "methods", ["home.repos.pwc.inspect_result.applied-ai-lab_zero-shot-pose.datasets.co3d_dataset.FrameData.to"], ["", "def", "cpu", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "to", "(", "device", "=", "torch", ".", "device", "(", "\"cpu\"", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.applied-ai-lab_zero-shot-pose.datasets.co3d_dataset.FrameData.cuda": [[125, 127], ["co3d_dataset.FrameData.to", "torch.device"], "methods", ["home.repos.pwc.inspect_result.applied-ai-lab_zero-shot-pose.datasets.co3d_dataset.FrameData.to"], ["", "def", "cuda", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "to", "(", "device", "=", "torch", ".", "device", "(", "\"cuda\"", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.applied-ai-lab_zero-shot-pose.datasets.co3d_dataset.FrameData.keys": [[129, 132], ["dataclasses.fields"], "methods", ["None"], ["", "def", "keys", "(", "self", ")", ":", "\n", "        ", "for", "f", "in", "fields", "(", "self", ")", ":", "\n", "            ", "yield", "f", ".", "name", "\n", "\n"]], "home.repos.pwc.inspect_result.applied-ai-lab_zero-shot-pose.datasets.co3d_dataset.FrameData.__getitem__": [[133, 135], ["getattr"], "methods", ["None"], ["", "", "def", "__getitem__", "(", "self", ",", "key", ")", ":", "\n", "        ", "return", "getattr", "(", "self", ",", "key", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.applied-ai-lab_zero-shot-pose.datasets.co3d_dataset.FrameData.collate": [[136, 197], ["isinstance", "collections.defaultdict", "enumerate", "enumerate", "dataclasses.fields", "cls", "isinstance", "id", "id_to_idx[].append", "numpy.ones", "collections.defaultdict.values", "sequence_point_cloud.append", "sequence_point_cloud_idx.tolist", "override_fields.get", "isinstance", "all", "cls.collate", "type", "torch.utils.data._utils.collate.default_collate", "len", "getattr", "type", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "p.points_padded", "p.normals_padded", "p.features_padded", "torch.cat"], "methods", ["home.repos.pwc.inspect_result.applied-ai-lab_zero-shot-pose.datasets.co3d_dataset.FrameData.collate"], ["", "@", "classmethod", "\n", "def", "collate", "(", "cls", ",", "batch", ")", ":", "\n", "        ", "\"\"\"\n        Given a list objects `batch` of class `cls`, collates them into a batched\n        representation suitable for processing with deep networks.\n        \"\"\"", "\n", "\n", "elem", "=", "batch", "[", "0", "]", "\n", "\n", "if", "isinstance", "(", "elem", ",", "cls", ")", ":", "\n", "            ", "pointcloud_ids", "=", "[", "id", "(", "el", ".", "sequence_point_cloud", ")", "for", "el", "in", "batch", "]", "\n", "id_to_idx", "=", "defaultdict", "(", "list", ")", "\n", "for", "i", ",", "pc_id", "in", "enumerate", "(", "pointcloud_ids", ")", ":", "\n", "                ", "id_to_idx", "[", "pc_id", "]", ".", "append", "(", "i", ")", "\n", "\n", "", "sequence_point_cloud", "=", "[", "]", "\n", "sequence_point_cloud_idx", "=", "-", "np", ".", "ones", "(", "(", "len", "(", "batch", ")", ",", ")", ")", "\n", "for", "i", ",", "ind", "in", "enumerate", "(", "id_to_idx", ".", "values", "(", ")", ")", ":", "\n", "                ", "sequence_point_cloud_idx", "[", "ind", "]", "=", "i", "\n", "sequence_point_cloud", ".", "append", "(", "batch", "[", "ind", "[", "0", "]", "]", ".", "sequence_point_cloud", ")", "\n", "", "assert", "(", "sequence_point_cloud_idx", ">=", "0", ")", ".", "all", "(", ")", "\n", "\n", "override_fields", "=", "{", "\n", "\"sequence_point_cloud\"", ":", "sequence_point_cloud", ",", "\n", "\"sequence_point_cloud_idx\"", ":", "sequence_point_cloud_idx", ".", "tolist", "(", ")", ",", "\n", "}", "\n", "# note that the pre-collate value of sequence_point_cloud_idx is unused", "\n", "\n", "collated", "=", "{", "}", "\n", "for", "f", "in", "fields", "(", "elem", ")", ":", "\n", "                ", "list_values", "=", "override_fields", ".", "get", "(", "\n", "f", ".", "name", ",", "[", "getattr", "(", "d", ",", "f", ".", "name", ")", "for", "d", "in", "batch", "]", "\n", ")", "\n", "collated", "[", "f", ".", "name", "]", "=", "(", "\n", "cls", ".", "collate", "(", "list_values", ")", "\n", "if", "all", "(", "l", "is", "not", "None", "for", "l", "in", "list_values", ")", "\n", "else", "None", "\n", ")", "\n", "", "return", "cls", "(", "**", "collated", ")", "\n", "", "elif", "isinstance", "(", "elem", ",", "Pointclouds", ")", ":", "\n", "# TODO: use concatenation", "\n", "            ", "pointclouds", "=", "type", "(", "elem", ")", "(", "\n", "points", "=", "[", "p", ".", "points_padded", "(", ")", "[", "0", "]", "for", "p", "in", "batch", "]", ",", "\n", "normals", "=", "[", "p", ".", "normals_padded", "(", ")", "[", "0", "]", "for", "p", "in", "batch", "]", ",", "\n", "features", "=", "[", "p", ".", "features_padded", "(", ")", "[", "0", "]", "for", "p", "in", "batch", "]", ",", "\n", ")", "\n", "return", "pointclouds", "\n", "", "elif", "isinstance", "(", "elem", ",", "CamerasBase", ")", ":", "\n", "# TODO: make a function for it", "\n", "# TODO: don't store K; enforce working in NDC space", "\n", "            ", "return", "type", "(", "elem", ")", "(", "\n", "R", "=", "torch", ".", "cat", "(", "[", "c", ".", "R", "for", "c", "in", "batch", "]", ",", "dim", "=", "0", ")", ",", "\n", "T", "=", "torch", ".", "cat", "(", "[", "c", ".", "T", "for", "c", "in", "batch", "]", ",", "dim", "=", "0", ")", ",", "\n", "K", "=", "torch", ".", "cat", "(", "[", "c", ".", "K", "for", "c", "in", "batch", "]", ",", "dim", "=", "0", ")", "\n", "if", "elem", ".", "K", "is", "not", "None", "\n", "else", "None", ",", "\n", "focal_length", "=", "torch", ".", "cat", "(", "[", "c", ".", "focal_length", "for", "c", "in", "batch", "]", ",", "dim", "=", "0", ")", ",", "\n", "principal_point", "=", "torch", ".", "cat", "(", "[", "c", ".", "principal_point", "for", "c", "in", "batch", "]", ",", "dim", "=", "0", ")", ",", "\n", ")", "\n", "", "else", ":", "\n", "            ", "return", "torch", ".", "utils", ".", "data", ".", "_utils", ".", "collate", ".", "default_collate", "(", "batch", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.applied-ai-lab_zero-shot-pose.datasets.co3d_dataset.Co3dDataset.__post_init__": [[288, 296], ["zsp.datasets.co3d_tools.camera_utils.assert_pytorch3d_has_new_ndc_convention", "co3d_dataset.Co3dDataset._load_frames", "co3d_dataset.Co3dDataset._load_sequences", "co3d_dataset.Co3dDataset._load_subset_lists", "co3d_dataset.Co3dDataset._filter_db", "print", "str"], "methods", ["home.repos.pwc.inspect_result.applied-ai-lab_zero-shot-pose.co3d_tools.camera_utils.assert_pytorch3d_has_new_ndc_convention", "home.repos.pwc.inspect_result.applied-ai-lab_zero-shot-pose.datasets.co3d_dataset.Co3dDataset._load_frames", "home.repos.pwc.inspect_result.applied-ai-lab_zero-shot-pose.datasets.co3d_dataset.Co3dDataset._load_sequences", "home.repos.pwc.inspect_result.applied-ai-lab_zero-shot-pose.datasets.co3d_dataset.Co3dDataset._load_subset_lists", "home.repos.pwc.inspect_result.applied-ai-lab_zero-shot-pose.datasets.co3d_dataset.Co3dDataset._filter_db"], ["def", "__post_init__", "(", "self", ")", ":", "\n", "        ", "assert_pytorch3d_has_new_ndc_convention", "(", ")", "\n", "self", ".", "subset_to_image_path", "=", "None", "\n", "self", ".", "_load_frames", "(", ")", "\n", "self", ".", "_load_sequences", "(", ")", "\n", "self", ".", "_load_subset_lists", "(", ")", "\n", "self", ".", "_filter_db", "(", ")", "# also computes sequence indices", "\n", "print", "(", "str", "(", "self", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.applied-ai-lab_zero-shot-pose.datasets.co3d_dataset.Co3dDataset.seq_frame_index_to_dataset_index": [[297, 329], ["co3d_dataset.Co3dDataset.seq_to_idx.items", "co3d_dataset.Co3dDataset.seq_frame_index_to_dataset_index._get_batch_idx"], "methods", ["None"], ["", "def", "seq_frame_index_to_dataset_index", "(", "\n", "self", ",", "\n", "seq_frame_index", ":", "Union", "[", "\n", "List", "[", "List", "[", "Union", "[", "Tuple", "[", "str", ",", "int", ",", "str", "]", ",", "Tuple", "[", "str", ",", "int", "]", "]", "]", "]", ",", "\n", "]", ",", "\n", ")", ":", "\n", "        ", "\"\"\"\n        Obtain indices into the dataset object given a list of frames specified as\n        `seq_frame_index = List[List[Tuple[sequence_name:str, frame_number:int]]]`.\n        \"\"\"", "\n", "_dataset_seq_frame_n_index", "=", "{", "\n", "seq", ":", "{", "\n", "self", ".", "frame_annots", "[", "idx", "]", "[", "\"frame_annotation\"", "]", ".", "frame_number", ":", "idx", "\n", "for", "idx", "in", "seq_idx", "\n", "}", "\n", "for", "seq", ",", "seq_idx", "in", "self", ".", "seq_to_idx", ".", "items", "(", ")", "\n", "}", "\n", "\n", "def", "_get_batch_idx", "(", "seq_name", ",", "frame_no", ",", "path", "=", "None", ")", ":", "\n", "            ", "idx", "=", "_dataset_seq_frame_n_index", "[", "seq_name", "]", "[", "frame_no", "]", "\n", "if", "path", "is", "not", "None", ":", "\n", "# Check that the loaded frame path is consistent", "\n", "# with the one stored in self.frame_annots.", "\n", "                ", "assert", "os", ".", "path", ".", "normpath", "(", "\n", "self", ".", "frame_annots", "[", "idx", "]", "[", "\"frame_annotation\"", "]", ".", "image", ".", "path", "\n", ")", "==", "os", ".", "path", ".", "normpath", "(", "\n", "path", "\n", ")", ",", "f\"Inconsistent batch {seq_name, frame_no, path}.\"", "\n", "", "return", "idx", "\n", "\n", "", "batches_idx", "=", "[", "[", "_get_batch_idx", "(", "*", "b", ")", "for", "b", "in", "batch", "]", "for", "batch", "in", "seq_frame_index", "]", "\n", "return", "batches_idx", "\n", "\n"]], "home.repos.pwc.inspect_result.applied-ai-lab_zero-shot-pose.datasets.co3d_dataset.Co3dDataset.__str__": [[330, 332], ["len"], "methods", ["None"], ["", "def", "__str__", "(", "self", ")", ":", "\n", "        ", "return", "f\"CO3D Dataset #frames={len(self.frame_annots)}\"", "\n", "\n"]], "home.repos.pwc.inspect_result.applied-ai-lab_zero-shot-pose.datasets.co3d_dataset.Co3dDataset.__len__": [[333, 335], ["len"], "methods", ["None"], ["", "def", "__len__", "(", "self", ")", ":", "\n", "        ", "return", "len", "(", "self", ".", "frame_annots", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.applied-ai-lab_zero-shot-pose.datasets.co3d_dataset.Co3dDataset._get_frame_type": [[336, 338], ["None"], "methods", ["None"], ["", "def", "_get_frame_type", "(", "self", ",", "entry", ")", ":", "\n", "        ", "return", "entry", "[", "\"subset\"", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.applied-ai-lab_zero-shot-pose.datasets.co3d_dataset.Co3dDataset.__getitem__": [[339, 412], ["co3d_dataset.FrameData", "co3d_dataset.Co3dDataset._get_frame_type", "co3d_dataset.Co3dDataset._load_crop_fg_probability", "co3d_dataset.Co3dDataset._get_pytorch3d_camera", "len", "co3d_dataset.Co3dDataset._load_crop_images", "co3d_dataset.Co3dDataset._load_mask_depth", "os.path.join", "len", "co3d_dataset._safe_as_tensor", "co3d_dataset._safe_as_tensor", "co3d_dataset._safe_as_tensor", "co3d_dataset._safe_as_tensor", "os.path.isfile", "co3d_dataset._load_pointcloud", "co3d_dataset._safe_as_tensor"], "methods", ["home.repos.pwc.inspect_result.applied-ai-lab_zero-shot-pose.datasets.co3d_dataset.Co3dDataset._get_frame_type", "home.repos.pwc.inspect_result.applied-ai-lab_zero-shot-pose.datasets.co3d_dataset.Co3dDataset._load_crop_fg_probability", "home.repos.pwc.inspect_result.applied-ai-lab_zero-shot-pose.datasets.co3d_dataset.Co3dDataset._get_pytorch3d_camera", "home.repos.pwc.inspect_result.applied-ai-lab_zero-shot-pose.datasets.co3d_dataset.Co3dDataset._load_crop_images", "home.repos.pwc.inspect_result.applied-ai-lab_zero-shot-pose.datasets.co3d_dataset.Co3dDataset._load_mask_depth", "home.repos.pwc.inspect_result.applied-ai-lab_zero-shot-pose.datasets.co3d_dataset._safe_as_tensor", "home.repos.pwc.inspect_result.applied-ai-lab_zero-shot-pose.datasets.co3d_dataset._safe_as_tensor", "home.repos.pwc.inspect_result.applied-ai-lab_zero-shot-pose.datasets.co3d_dataset._safe_as_tensor", "home.repos.pwc.inspect_result.applied-ai-lab_zero-shot-pose.datasets.co3d_dataset._safe_as_tensor", "home.repos.pwc.inspect_result.applied-ai-lab_zero-shot-pose.datasets.co3d_dataset._load_pointcloud", "home.repos.pwc.inspect_result.applied-ai-lab_zero-shot-pose.datasets.co3d_dataset._safe_as_tensor"], ["", "def", "__getitem__", "(", "self", ",", "index", ")", ":", "\n", "        ", "assert", "index", "<", "len", "(", "\n", "self", ".", "frame_annots", "\n", ")", ",", "f\"index {index} out of range {len(self.frame_annots)}\"", "\n", "\n", "entry", "=", "self", ".", "frame_annots", "[", "index", "]", "[", "\"frame_annotation\"", "]", "\n", "point_cloud", "=", "self", ".", "seq_annots", "[", "entry", ".", "sequence_name", "]", ".", "point_cloud", "\n", "frame_data", "=", "FrameData", "(", "\n", "frame_number", "=", "_safe_as_tensor", "(", "entry", ".", "frame_number", ",", "torch", ".", "long", ")", ",", "\n", "frame_timestamp", "=", "_safe_as_tensor", "(", "entry", ".", "frame_timestamp", ",", "torch", ".", "float", ")", ",", "\n", "sequence_name", "=", "entry", ".", "sequence_name", ",", "\n", "sequence_category", "=", "self", ".", "seq_annots", "[", "entry", ".", "sequence_name", "]", ".", "category", ",", "\n", "# original image size", "\n", "image_size_hw", "=", "_safe_as_tensor", "(", "entry", ".", "image", ".", "size", ",", "torch", ".", "long", ")", ",", "\n", "camera_quality_score", "=", "_safe_as_tensor", "(", "\n", "self", ".", "seq_annots", "[", "entry", ".", "sequence_name", "]", ".", "viewpoint_quality_score", ",", "\n", "torch", ".", "float", ",", "\n", ")", ",", "\n", "point_cloud_quality_score", "=", "_safe_as_tensor", "(", "\n", "point_cloud", ".", "quality_score", ",", "torch", ".", "float", "\n", ")", "\n", "if", "point_cloud", "is", "not", "None", "\n", "else", "None", ",", "\n", ")", "\n", "\n", "# The rest of the fields are optional", "\n", "frame_data", ".", "frame_type", "=", "self", ".", "_get_frame_type", "(", "self", ".", "frame_annots", "[", "index", "]", ")", "\n", "\n", "(", "\n", "frame_data", ".", "fg_probability", ",", "\n", "frame_data", ".", "mask_path", ",", "\n", "frame_data", ".", "bbox_xywh", ",", "\n", "clamp_bbox_xyxy", ",", "\n", ")", "=", "self", ".", "_load_crop_fg_probability", "(", "entry", ")", "\n", "\n", "scale", "=", "1.0", "\n", "if", "self", ".", "load_images", ":", "\n", "            ", "(", "\n", "frame_data", ".", "image_rgb", ",", "\n", "frame_data", ".", "image_path", ",", "\n", "frame_data", ".", "mask_crop", ",", "\n", "scale", ",", "\n", ")", "=", "self", ".", "_load_crop_images", "(", "\n", "entry", ",", "frame_data", ".", "fg_probability", ",", "clamp_bbox_xyxy", "\n", ")", "\n", "\n", "", "if", "self", ".", "load_depths", "and", "entry", ".", "depth", "is", "not", "None", ":", "\n", "            ", "(", "\n", "frame_data", ".", "depth_map", ",", "\n", "frame_data", ".", "depth_path", ",", "\n", "frame_data", ".", "depth_mask", ",", "\n", ")", "=", "self", ".", "_load_mask_depth", "(", "entry", ",", "clamp_bbox_xyxy", ",", "frame_data", ".", "fg_probability", ")", "\n", "\n", "", "frame_data", ".", "camera", "=", "self", ".", "_get_pytorch3d_camera", "(", "\n", "entry", ",", "\n", "scale", ",", "\n", "clamp_bbox_xyxy", ",", "\n", ")", "\n", "\n", "if", "point_cloud", "is", "not", "None", "and", "self", ".", "load_point_clouds", ":", "\n", "            ", "frame_data", ".", "sequence_point_cloud_path", "=", "os", ".", "path", ".", "join", "(", "\n", "self", ".", "dataset_root", ",", "point_cloud", ".", "path", "\n", ")", "\n", "frame_data", ".", "sequence_point_cloud", "=", "(", "\n", "_load_pointcloud", "(", "\n", "frame_data", ".", "sequence_point_cloud_path", ",", "\n", "max_points", "=", "self", ".", "max_points", ",", "\n", ")", "\n", "if", "os", ".", "path", ".", "isfile", "(", "frame_data", ".", "sequence_point_cloud_path", ")", "\n", "else", "None", "\n", ")", "\n", "\n", "", "return", "frame_data", "\n", "\n"]], "home.repos.pwc.inspect_result.applied-ai-lab_zero-shot-pose.datasets.co3d_dataset.Co3dDataset._load_crop_fg_probability": [[413, 432], ["os.path.join", "co3d_dataset._load_mask", "torch.tensor", "co3d_dataset.Co3dDataset._resize_image", "ValueError", "co3d_dataset._get_bbox_from_mask", "co3d_dataset._get_clamp_bbox", "co3d_dataset._crop_around_box"], "methods", ["home.repos.pwc.inspect_result.applied-ai-lab_zero-shot-pose.datasets.co3d_dataset._load_mask", "home.repos.pwc.inspect_result.applied-ai-lab_zero-shot-pose.datasets.co3d_dataset.Co3dDataset._resize_image", "home.repos.pwc.inspect_result.applied-ai-lab_zero-shot-pose.datasets.co3d_dataset._get_bbox_from_mask", "home.repos.pwc.inspect_result.applied-ai-lab_zero-shot-pose.datasets.co3d_dataset._get_clamp_bbox", "home.repos.pwc.inspect_result.applied-ai-lab_zero-shot-pose.datasets.co3d_dataset._crop_around_box"], ["", "def", "_load_crop_fg_probability", "(", "self", ",", "entry", ")", ":", "\n", "        ", "clamp_bbox_xyxy", "=", "None", "\n", "if", "(", "self", ".", "load_masks", "or", "self", ".", "box_crop", ")", "and", "entry", ".", "mask", "is", "not", "None", ":", "\n", "            ", "full_path", "=", "os", ".", "path", ".", "join", "(", "self", ".", "dataset_root", ",", "entry", ".", "mask", ".", "path", ")", "\n", "mask", "=", "_load_mask", "(", "full_path", ")", "\n", "\n", "if", "mask", ".", "shape", "[", "-", "2", ":", "]", "!=", "entry", ".", "image", ".", "size", ":", "\n", "                ", "raise", "ValueError", "(", "\n", "f\"bad mask size: {mask.shape[-2:]} vs {entry.image.size}!\"", "\n", ")", "\n", "\n", "", "bbox_xywh", "=", "torch", ".", "tensor", "(", "_get_bbox_from_mask", "(", "mask", ",", "self", ".", "box_crop_mask_thr", ")", ")", "\n", "\n", "if", "self", ".", "box_crop", ":", "\n", "                ", "clamp_bbox_xyxy", "=", "_get_clamp_bbox", "(", "bbox_xywh", ",", "self", ".", "box_crop_context", ")", "\n", "mask", "=", "_crop_around_box", "(", "mask", ",", "clamp_bbox_xyxy", ",", "full_path", ")", "\n", "\n", "", "fg_probability", ",", "_", ",", "_", "=", "self", ".", "_resize_image", "(", "mask", ",", "mode", "=", "\"nearest\"", ")", "\n", "", "return", "fg_probability", ",", "full_path", ",", "bbox_xywh", ",", "clamp_bbox_xyxy", "\n", "\n"]], "home.repos.pwc.inspect_result.applied-ai-lab_zero-shot-pose.datasets.co3d_dataset.Co3dDataset._load_crop_images": [[433, 454], ["os.path.join", "co3d_dataset._load_image", "co3d_dataset.Co3dDataset._resize_image", "ValueError", "co3d_dataset._crop_around_box"], "methods", ["home.repos.pwc.inspect_result.applied-ai-lab_zero-shot-pose.datasets.co3d_dataset._load_image", "home.repos.pwc.inspect_result.applied-ai-lab_zero-shot-pose.datasets.co3d_dataset.Co3dDataset._resize_image", "home.repos.pwc.inspect_result.applied-ai-lab_zero-shot-pose.datasets.co3d_dataset._crop_around_box"], ["", "def", "_load_crop_images", "(", "self", ",", "entry", ",", "fg_probability", ",", "clamp_bbox_xyxy", ")", ":", "\n", "        ", "assert", "self", ".", "dataset_root", "is", "not", "None", "and", "entry", ".", "image", "is", "not", "None", "\n", "path", "=", "os", ".", "path", ".", "join", "(", "self", ".", "dataset_root", ",", "entry", ".", "image", ".", "path", ")", "\n", "image_rgb", "=", "_load_image", "(", "path", ")", "\n", "\n", "if", "image_rgb", ".", "shape", "[", "-", "2", ":", "]", "!=", "entry", ".", "image", ".", "size", ":", "\n", "            ", "raise", "ValueError", "(", "\n", "f\"bad image size: {image_rgb.shape[-2:]} vs {entry.image.size}!\"", "\n", ")", "\n", "\n", "", "if", "self", ".", "box_crop", ":", "\n", "            ", "assert", "clamp_bbox_xyxy", "is", "not", "None", "\n", "image_rgb", "=", "_crop_around_box", "(", "image_rgb", ",", "clamp_bbox_xyxy", ",", "path", ")", "\n", "\n", "", "image_rgb", ",", "scale", ",", "mask_crop", "=", "self", ".", "_resize_image", "(", "image_rgb", ")", "\n", "\n", "if", "self", ".", "mask_images", ":", "\n", "            ", "assert", "fg_probability", "is", "not", "None", "\n", "image_rgb", "*=", "fg_probability", "\n", "\n", "", "return", "image_rgb", ",", "path", ",", "mask_crop", ",", "scale", "\n", "\n"]], "home.repos.pwc.inspect_result.applied-ai-lab_zero-shot-pose.datasets.co3d_dataset.Co3dDataset._load_mask_depth": [[455, 489], ["os.path.join", "co3d_dataset._load_depth", "co3d_dataset.Co3dDataset._resize_image", "co3d_dataset._rescale_bbox", "co3d_dataset._crop_around_box", "os.path.join", "co3d_dataset._load_depth_mask", "co3d_dataset.Co3dDataset._resize_image", "torch.ones_like", "co3d_dataset._rescale_bbox", "co3d_dataset._crop_around_box"], "methods", ["home.repos.pwc.inspect_result.applied-ai-lab_zero-shot-pose.datasets.co3d_dataset._load_depth", "home.repos.pwc.inspect_result.applied-ai-lab_zero-shot-pose.datasets.co3d_dataset.Co3dDataset._resize_image", "home.repos.pwc.inspect_result.applied-ai-lab_zero-shot-pose.datasets.co3d_dataset._rescale_bbox", "home.repos.pwc.inspect_result.applied-ai-lab_zero-shot-pose.datasets.co3d_dataset._crop_around_box", "home.repos.pwc.inspect_result.applied-ai-lab_zero-shot-pose.datasets.co3d_dataset._load_depth_mask", "home.repos.pwc.inspect_result.applied-ai-lab_zero-shot-pose.datasets.co3d_dataset.Co3dDataset._resize_image", "home.repos.pwc.inspect_result.applied-ai-lab_zero-shot-pose.datasets.co3d_dataset._rescale_bbox", "home.repos.pwc.inspect_result.applied-ai-lab_zero-shot-pose.datasets.co3d_dataset._crop_around_box"], ["", "def", "_load_mask_depth", "(", "self", ",", "entry", ",", "clamp_bbox_xyxy", ",", "fg_probability", ")", ":", "\n", "        ", "path", "=", "os", ".", "path", ".", "join", "(", "self", ".", "dataset_root", ",", "entry", ".", "depth", ".", "path", ")", "\n", "depth_map", "=", "_load_depth", "(", "path", ",", "entry", ".", "depth", ".", "scale_adjustment", ")", "\n", "\n", "if", "self", ".", "box_crop", ":", "\n", "            ", "depth_bbox_xyxy", "=", "_rescale_bbox", "(", "\n", "clamp_bbox_xyxy", ",", "entry", ".", "image", ".", "size", ",", "depth_map", ".", "shape", "[", "-", "2", ":", "]", "\n", ")", "\n", "depth_map", "=", "_crop_around_box", "(", "depth_map", ",", "depth_bbox_xyxy", ",", "path", ")", "\n", "\n", "", "depth_map", ",", "_", ",", "_", "=", "self", ".", "_resize_image", "(", "depth_map", ",", "mode", "=", "\"nearest\"", ")", "\n", "\n", "if", "self", ".", "mask_depths", ":", "\n", "            ", "assert", "fg_probability", "is", "not", "None", "\n", "depth_map", "*=", "fg_probability", "\n", "\n", "", "if", "self", ".", "load_depth_masks", ":", "\n", "            ", "assert", "entry", ".", "depth", ".", "mask_path", "is", "not", "None", "\n", "mask_path", "=", "os", ".", "path", ".", "join", "(", "self", ".", "dataset_root", ",", "entry", ".", "depth", ".", "mask_path", ")", "\n", "depth_mask", "=", "_load_depth_mask", "(", "mask_path", ")", "\n", "\n", "if", "self", ".", "box_crop", ":", "\n", "                ", "depth_mask_bbox_xyxy", "=", "_rescale_bbox", "(", "\n", "clamp_bbox_xyxy", ",", "entry", ".", "image", ".", "size", ",", "depth_mask", ".", "shape", "[", "-", "2", ":", "]", "\n", ")", "\n", "depth_mask", "=", "_crop_around_box", "(", "\n", "depth_mask", ",", "depth_mask_bbox_xyxy", ",", "mask_path", "\n", ")", "\n", "\n", "", "depth_mask", ",", "_", ",", "_", "=", "self", ".", "_resize_image", "(", "depth_mask", ",", "mode", "=", "\"nearest\"", ")", "\n", "", "else", ":", "\n", "            ", "depth_mask", "=", "torch", ".", "ones_like", "(", "depth_map", ")", "\n", "\n", "", "return", "depth_map", ",", "path", ",", "depth_mask", "\n", "\n"]], "home.repos.pwc.inspect_result.applied-ai-lab_zero-shot-pose.datasets.co3d_dataset.Co3dDataset._get_pytorch3d_camera": [[490, 535], ["torch.tensor", "torch.tensor", "half_image_size_output.min", "pytorch3d.renderer.cameras.PerspectiveCameras", "torch.tensor", "list", "torch.tensor", "list", "reversed", "reversed", "torch.tensor", "torch.tensor"], "methods", ["None"], ["", "def", "_get_pytorch3d_camera", "(", "self", ",", "entry", ",", "scale", ",", "clamp_bbox_xyxy", ")", ":", "\n", "# principal point and focal length", "\n", "        ", "principal_point", "=", "torch", ".", "tensor", "(", "\n", "entry", ".", "viewpoint", ".", "principal_point", ",", "dtype", "=", "torch", ".", "float", "\n", ")", "\n", "focal_length", "=", "torch", ".", "tensor", "(", "\n", "entry", ".", "viewpoint", ".", "focal_length", ",", "dtype", "=", "torch", ".", "float", "\n", ")", "\n", "\n", "# first, we convert from the legacy Pytorch3D NDC convention", "\n", "# (currently used in CO3D for storing intrinsics) to pixels", "\n", "half_image_size_wh_orig", "=", "(", "\n", "torch", ".", "tensor", "(", "list", "(", "reversed", "(", "entry", ".", "image", ".", "size", ")", ")", ",", "dtype", "=", "torch", ".", "float", ")", "\n", "/", "2.0", "\n", ")", "\n", "\n", "# principal point and focal length in pixels", "\n", "principal_point_px", "=", "(", "\n", "-", "1.0", "*", "(", "principal_point", "-", "1.0", ")", "*", "half_image_size_wh_orig", "\n", ")", "\n", "focal_length_px", "=", "focal_length", "*", "half_image_size_wh_orig", "\n", "if", "self", ".", "box_crop", ":", "\n", "            ", "assert", "clamp_bbox_xyxy", "is", "not", "None", "\n", "principal_point_px", "-=", "clamp_bbox_xyxy", "[", ":", "2", "]", "\n", "\n", "# now, convert from pixels to Pytorch3D v0.5+ NDC convention", "\n", "", "if", "self", ".", "image_height", "is", "None", "or", "self", ".", "image_width", "is", "None", ":", "\n", "            ", "out_size", "=", "list", "(", "reversed", "(", "entry", ".", "image", ".", "size", ")", ")", "\n", "", "else", ":", "\n", "            ", "out_size", "=", "[", "self", ".", "image_width", ",", "self", ".", "image_height", "]", "\n", "\n", "", "half_image_size_output", "=", "torch", ".", "tensor", "(", "out_size", ",", "dtype", "=", "torch", ".", "float", ")", "/", "2.0", "\n", "half_min_image_size_output", "=", "half_image_size_output", ".", "min", "(", ")", "\n", "\n", "# rescaled principal point and focal length in ndc", "\n", "principal_point", "=", "(", "\n", "half_image_size_output", "-", "principal_point_px", "*", "scale", "\n", ")", "/", "half_min_image_size_output", "\n", "focal_length", "=", "focal_length_px", "*", "scale", "/", "half_min_image_size_output", "\n", "\n", "return", "PerspectiveCameras", "(", "\n", "focal_length", "=", "focal_length", "[", "None", "]", ",", "\n", "principal_point", "=", "principal_point", "[", "None", "]", ",", "\n", "R", "=", "torch", ".", "tensor", "(", "entry", ".", "viewpoint", ".", "R", ",", "dtype", "=", "torch", ".", "float", ")", "[", "None", "]", ",", "\n", "T", "=", "torch", ".", "tensor", "(", "entry", ".", "viewpoint", ".", "T", ",", "dtype", "=", "torch", ".", "float", ")", "[", "None", "]", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.applied-ai-lab_zero-shot-pose.datasets.co3d_dataset.Co3dDataset._load_frames": [[538, 548], ["print", "gzip.open", "zsp.datasets.co3d_types.load_dataclass", "ValueError"], "methods", ["home.repos.pwc.inspect_result.applied-ai-lab_zero-shot-pose.datasets.co3d_types.load_dataclass"], ["", "def", "_load_frames", "(", "self", ")", ":", "\n", "        ", "print", "(", "f\"Loading Co3D frames from {self.frame_annotations_file}.\"", ")", "\n", "with", "gzip", ".", "open", "(", "self", ".", "frame_annotations_file", ",", "\"rt\"", ",", "encoding", "=", "\"utf8\"", ")", "as", "zipfile", ":", "\n", "            ", "frame_annots_list", "=", "types", ".", "load_dataclass", "(", "\n", "zipfile", ",", "List", "[", "types", ".", "FrameAnnotation", "]", "\n", ")", "\n", "", "if", "not", "frame_annots_list", ":", "\n", "            ", "raise", "ValueError", "(", "\"Empty dataset!\"", ")", "\n", "", "self", ".", "frame_annots", "=", "[", "\n", "{", "\"frame_annotation\"", ":", "a", ",", "\"subset\"", ":", "None", "}", "for", "a", "in", "frame_annots_list", "\n", "]", "\n"]], "home.repos.pwc.inspect_result.applied-ai-lab_zero-shot-pose.datasets.co3d_dataset.Co3dDataset._load_sequences": [[550, 559], ["print", "gzip.open", "zsp.datasets.co3d_types.load_dataclass", "ValueError"], "methods", ["home.repos.pwc.inspect_result.applied-ai-lab_zero-shot-pose.datasets.co3d_types.load_dataclass"], ["", "def", "_load_sequences", "(", "self", ")", ":", "\n", "        ", "print", "(", "f\"Loading Co3D sequences from {self.sequence_annotations_file}.\"", ")", "\n", "with", "gzip", ".", "open", "(", "\n", "self", ".", "sequence_annotations_file", ",", "\"rt\"", ",", "encoding", "=", "\"utf8\"", "\n", ")", "as", "zipfile", ":", "\n", "            ", "seq_annots", "=", "types", ".", "load_dataclass", "(", "zipfile", ",", "List", "[", "types", ".", "SequenceAnnotation", "]", ")", "\n", "", "if", "not", "seq_annots", ":", "\n", "            ", "raise", "ValueError", "(", "\"Empty sequences file!\"", ")", "\n", "", "self", ".", "seq_annots", "=", "{", "entry", ".", "sequence_name", ":", "entry", "for", "entry", "in", "seq_annots", "}", "\n", "\n"]], "home.repos.pwc.inspect_result.applied-ai-lab_zero-shot-pose.datasets.co3d_dataset.Co3dDataset._load_subset_lists": [[560, 582], ["print", "open", "json.load", "frame_path_to_subset.get", "json.load.items", "warnings.warn"], "methods", ["None"], ["", "def", "_load_subset_lists", "(", "self", ")", ":", "\n", "        ", "print", "(", "f\"Loading Co3D subset lists from {self.subset_lists_file}.\"", ")", "\n", "if", "not", "self", ".", "subset_lists_file", ":", "\n", "            ", "return", "\n", "\n", "", "with", "open", "(", "self", ".", "subset_lists_file", ",", "\"r\"", ")", "as", "f", ":", "\n", "            ", "subset_to_seq_frame", "=", "json", ".", "load", "(", "f", ")", "\n", "\n", "", "frame_path_to_subset", "=", "{", "\n", "path", ":", "subset", "\n", "for", "subset", ",", "frames", "in", "subset_to_seq_frame", ".", "items", "(", ")", "\n", "for", "_", ",", "_", ",", "path", "in", "frames", "\n", "}", "\n", "\n", "for", "frame", "in", "self", ".", "frame_annots", ":", "\n", "            ", "frame", "[", "\"subset\"", "]", "=", "frame_path_to_subset", ".", "get", "(", "\n", "frame", "[", "\"frame_annotation\"", "]", ".", "image", ".", "path", ",", "None", "\n", ")", "\n", "if", "frame", "[", "\"subset\"", "]", "is", "None", ":", "\n", "                ", "warnings", ".", "warn", "(", "\n", "\"Subset lists are given but don't include \"", "\n", "+", "frame", "[", "\"frame_annotation\"", "]", ".", "image", ".", "path", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.applied-ai-lab_zero-shot-pose.datasets.co3d_dataset.Co3dDataset._filter_db": [[584, 675], ["co3d_dataset.Co3dDataset._invalidate_indexes", "print", "len", "print", "print", "co3d_dataset.Co3dDataset._invalidate_indexes", "len", "print", "len", "getattr", "dict", "print", "co3d_dataset.Co3dDataset.seq_to_idx.items", "print", "co3d_dataset.Co3dDataset._invalidate_indexes", "print", "co3d_dataset.Co3dDataset._invalidate_indexes", "ValueError", "len", "ValueError", "len", "print", "print", "itertools.islice", "random.Random().sample", "keep_idx.extend", "len", "co3d_dataset.Co3dDataset.seq_annots.values", "co3d_dataset.Co3dDataset.seq_annots.items", "co3d_dataset._seq_name_to_seed", "sorted", "len", "len", "co3d_dataset.Co3dDataset.seq_annots.items", "cond", "random.Random", "len", "len", "len", "str", "len"], "methods", ["home.repos.pwc.inspect_result.applied-ai-lab_zero-shot-pose.datasets.co3d_dataset.Co3dDataset._invalidate_indexes", "home.repos.pwc.inspect_result.applied-ai-lab_zero-shot-pose.datasets.co3d_dataset.Co3dDataset._invalidate_indexes", "home.repos.pwc.inspect_result.applied-ai-lab_zero-shot-pose.datasets.co3d_dataset.Co3dDataset._invalidate_indexes", "home.repos.pwc.inspect_result.applied-ai-lab_zero-shot-pose.datasets.co3d_dataset.Co3dDataset._invalidate_indexes", "home.repos.pwc.inspect_result.applied-ai-lab_zero-shot-pose.datasets.co3d_dataset._seq_name_to_seed"], ["", "", "", "def", "_filter_db", "(", "self", ")", ":", "\n", "        ", "if", "self", ".", "remove_empty_masks", ":", "\n", "            ", "print", "(", "\"Removing images with empty masks.\"", ")", "\n", "old_len", "=", "len", "(", "self", ".", "frame_annots", ")", "\n", "self", ".", "frame_annots", "=", "[", "\n", "frame", "\n", "for", "frame", "in", "self", ".", "frame_annots", "\n", "if", "frame", "[", "\"frame_annotation\"", "]", ".", "mask", "is", "not", "None", "\n", "and", "frame", "[", "\"frame_annotation\"", "]", ".", "mask", ".", "mass", ">", "1", "\n", "]", "\n", "print", "(", "\"... filtered %d -> %d\"", "%", "(", "old_len", ",", "len", "(", "self", ".", "frame_annots", ")", ")", ")", "\n", "\n", "# this has to be called after joining with categories!!", "\n", "", "if", "self", ".", "subsets", ":", "\n", "            ", "if", "not", "self", ".", "subset_lists_file", ":", "\n", "                ", "raise", "ValueError", "(", "\n", "\"Subset filter is on but subset_lists_file was not given\"", "\n", ")", "\n", "\n", "", "print", "(", "f\"Limitting Co3D dataset to the '{self.subsets}' subsets.\"", ")", "\n", "# truncate the list of subsets to the valid one", "\n", "self", ".", "frame_annots", "=", "[", "\n", "entry", "for", "entry", "in", "self", ".", "frame_annots", "if", "entry", "[", "\"subset\"", "]", "in", "self", ".", "subsets", "\n", "]", "\n", "if", "len", "(", "self", ".", "frame_annots", ")", "==", "0", ":", "\n", "                ", "raise", "ValueError", "(", "\n", "f\"There are no frames in the '{self.subsets}' subsets!\"", "\n", ")", "\n", "\n", "", "self", ".", "_invalidate_indexes", "(", "filter_seq_annots", "=", "True", ")", "\n", "\n", "", "if", "len", "(", "self", ".", "limit_category_to", ")", ">", "0", ":", "\n", "            ", "print", "(", "f\"Limitting dataset to categories: {self.limit_category_to}\"", ")", "\n", "self", ".", "seq_annots", "=", "{", "\n", "name", ":", "entry", "\n", "for", "name", ",", "entry", "in", "self", ".", "seq_annots", ".", "values", "(", ")", "\n", "if", "entry", ".", "category", "in", "self", ".", "limit_category_to", "\n", "}", "\n", "\n", "# sequence filters", "\n", "", "for", "prefix", "in", "(", "\"pick\"", ",", "\"exclude\"", ")", ":", "\n", "            ", "orig_len", "=", "len", "(", "self", ".", "seq_annots", ")", "\n", "attr", "=", "f\"{prefix}_sequence\"", "\n", "arr", "=", "getattr", "(", "self", ",", "attr", ")", "\n", "if", "len", "(", "arr", ")", ">", "0", ":", "\n", "                ", "print", "(", "f\"{attr}: {str(arr)}\"", ")", "\n", "cond", "=", "lambda", "name", ",", "exclude", "=", "False", ":", "(", "name", "in", "arr", ")", "!=", "exclude", "\n", "self", ".", "seq_annots", "=", "{", "\n", "name", ":", "entry", "\n", "for", "name", ",", "entry", "in", "self", ".", "seq_annots", ".", "items", "(", ")", "\n", "if", "cond", "(", "name", ",", "exclude", "=", "prefix", "==", "\"exclude\"", ")", "\n", "}", "\n", "print", "(", "\"... filtered %d -> %d\"", "%", "(", "orig_len", ",", "len", "(", "self", ".", "seq_annots", ")", ")", ")", "\n", "\n", "", "", "if", "self", ".", "limit_sequences_to", ">", "0", ":", "\n", "            ", "self", ".", "seq_annots", "=", "dict", "(", "\n", "islice", "(", "self", ".", "seq_annots", ".", "items", "(", ")", ",", "self", ".", "limit_sequences_to", ")", "\n", ")", "\n", "\n", "# retain only frames from retained sequences", "\n", "", "self", ".", "frame_annots", "=", "[", "\n", "f", "\n", "for", "f", "in", "self", ".", "frame_annots", "\n", "if", "f", "[", "\"frame_annotation\"", "]", ".", "sequence_name", "in", "self", ".", "seq_annots", "\n", "]", "\n", "\n", "self", ".", "_invalidate_indexes", "(", ")", "\n", "\n", "if", "self", ".", "n_frames_per_sequence", ">", "0", ":", "\n", "            ", "print", "(", "f\"Taking max {self.n_frames_per_sequence} per sequence.\"", ")", "\n", "keep_idx", "=", "[", "]", "\n", "for", "seq", ",", "seq_indices", "in", "self", ".", "seq_to_idx", ".", "items", "(", ")", ":", "\n", "# infer the seed from the sequence name, this is reproducible", "\n", "# and makes the selection differ for different sequences", "\n", "                ", "seed", "=", "_seq_name_to_seed", "(", "seq", ")", "+", "self", ".", "seed", "\n", "seq_idx_shuffled", "=", "random", ".", "Random", "(", "seed", ")", ".", "sample", "(", "\n", "sorted", "(", "seq_indices", ")", ",", "len", "(", "seq_indices", ")", "\n", ")", "\n", "keep_idx", ".", "extend", "(", "seq_idx_shuffled", "[", ":", "self", ".", "n_frames_per_sequence", "]", ")", "\n", "\n", "", "print", "(", "\"... filtered %d -> %d\"", "%", "(", "len", "(", "self", ".", "frame_annots", ")", ",", "len", "(", "keep_idx", ")", ")", ")", "\n", "self", ".", "frame_annots", "=", "[", "self", ".", "frame_annots", "[", "i", "]", "for", "i", "in", "keep_idx", "]", "\n", "self", ".", "_invalidate_indexes", "(", "filter_seq_annots", "=", "False", ")", "\n", "# sequences are not decimated, so self.seq_annots is valid", "\n", "\n", "", "if", "self", ".", "limit_to", ">", "0", "and", "self", ".", "limit_to", "<", "len", "(", "self", ".", "frame_annots", ")", ":", "\n", "            ", "print", "(", "\n", "\"limit_to: filtered %d -> %d\"", "%", "(", "len", "(", "self", ".", "frame_annots", ")", ",", "self", ".", "limit_to", ")", "\n", ")", "\n", "self", ".", "frame_annots", "=", "self", ".", "frame_annots", "[", ":", "self", ".", "limit_to", "]", "\n", "self", ".", "_invalidate_indexes", "(", "filter_seq_annots", "=", "True", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.applied-ai-lab_zero-shot-pose.datasets.co3d_dataset.Co3dDataset._invalidate_indexes": [[676, 684], ["co3d_dataset.Co3dDataset._invalidate_seq_to_idx", "co3d_dataset.Co3dDataset.seq_annots.items"], "methods", ["home.repos.pwc.inspect_result.applied-ai-lab_zero-shot-pose.datasets.co3d_dataset.Co3dDataset._invalidate_seq_to_idx"], ["", "", "def", "_invalidate_indexes", "(", "self", ",", "filter_seq_annots", "=", "False", ")", ":", "\n", "# update seq_to_idx and filter seq_meta according to frame_annots change", "\n", "# if filter_seq_annots, also uldates seq_annots based on the changed seq_to_idx", "\n", "        ", "self", ".", "_invalidate_seq_to_idx", "(", ")", "\n", "\n", "if", "filter_seq_annots", ":", "\n", "            ", "self", ".", "seq_annots", "=", "{", "\n", "k", ":", "v", "for", "k", ",", "v", "in", "self", ".", "seq_annots", ".", "items", "(", ")", "if", "k", "in", "self", ".", "seq_to_idx", "\n", "}", "\n"]], "home.repos.pwc.inspect_result.applied-ai-lab_zero-shot-pose.datasets.co3d_dataset.Co3dDataset._invalidate_seq_to_idx": [[686, 691], ["collections.defaultdict", "enumerate", "seq_to_idx[].append"], "methods", ["None"], ["", "", "def", "_invalidate_seq_to_idx", "(", "self", ")", ":", "\n", "        ", "seq_to_idx", "=", "defaultdict", "(", "list", ")", "\n", "for", "idx", ",", "entry", "in", "enumerate", "(", "self", ".", "frame_annots", ")", ":", "\n", "            ", "seq_to_idx", "[", "entry", "[", "\"frame_annotation\"", "]", ".", "sequence_name", "]", ".", "append", "(", "idx", ")", "\n", "", "self", ".", "seq_to_idx", "=", "seq_to_idx", "\n", "\n"]], "home.repos.pwc.inspect_result.applied-ai-lab_zero-shot-pose.datasets.co3d_dataset.Co3dDataset._resize_image": [[692, 714], ["min", "torch.zeros", "torch.zeros", "torch.from_numpy", "torch.nn.functional.interpolate", "torch.ones_like", "torch.from_numpy"], "methods", ["None"], ["", "def", "_resize_image", "(", "self", ",", "image", ",", "mode", "=", "\"bilinear\"", ")", ":", "\n", "        ", "if", "self", ".", "image_height", "is", "None", "and", "self", ".", "image_width", "is", "None", ":", "\n", "# skip the resizing", "\n", "            ", "imre_", "=", "torch", ".", "from_numpy", "(", "image", ")", "\n", "return", "imre_", ",", "1.0", ",", "torch", ".", "ones_like", "(", "imre_", "[", ":", "1", "]", ")", "\n", "# takes numpy array, returns pytorch tensor", "\n", "", "minscale", "=", "min", "(", "\n", "self", ".", "image_height", "/", "image", ".", "shape", "[", "-", "2", "]", ",", "\n", "self", ".", "image_width", "/", "image", ".", "shape", "[", "-", "1", "]", ",", "\n", ")", "\n", "imre", "=", "torch", ".", "nn", ".", "functional", ".", "interpolate", "(", "\n", "torch", ".", "from_numpy", "(", "image", ")", "[", "None", "]", ",", "\n", "scale_factor", "=", "minscale", ",", "\n", "mode", "=", "mode", ",", "\n", "align_corners", "=", "False", "if", "mode", "==", "\"bilinear\"", "else", "None", ",", "\n", "recompute_scale_factor", "=", "True", ",", "\n", ")", "[", "0", "]", "\n", "imre_", "=", "torch", ".", "zeros", "(", "image", ".", "shape", "[", "0", "]", ",", "self", ".", "image_height", ",", "self", ".", "image_width", ")", "\n", "imre_", "[", ":", ",", "0", ":", "imre", ".", "shape", "[", "1", "]", ",", "0", ":", "imre", ".", "shape", "[", "2", "]", "]", "=", "imre", "\n", "mask", "=", "torch", ".", "zeros", "(", "1", ",", "self", ".", "image_height", ",", "self", ".", "image_width", ")", "\n", "mask", "[", ":", ",", "0", ":", "imre", ".", "shape", "[", "1", "]", "-", "1", ",", "0", ":", "imre", ".", "shape", "[", "2", "]", "-", "1", "]", "=", "1.0", "\n", "return", "imre_", ",", "minscale", ",", "mask", "\n", "\n"]], "home.repos.pwc.inspect_result.applied-ai-lab_zero-shot-pose.datasets.co3d_dataset._seq_name_to_seed": [[716, 718], ["int", "hashlib.sha1().hexdigest", "hashlib.sha1", "seq_name.encode"], "function", ["None"], ["", "", "def", "_seq_name_to_seed", "(", "seq_name", ")", ":", "\n", "    ", "return", "int", "(", "hashlib", ".", "sha1", "(", "seq_name", ".", "encode", "(", "\"utf-8\"", ")", ")", ".", "hexdigest", "(", ")", ",", "16", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.applied-ai-lab_zero-shot-pose.datasets.co3d_dataset._load_image": [[720, 726], ["np.array.transpose", "PIL.Image.open", "numpy.array", "np.array.astype", "pil_im.convert"], "function", ["None"], ["", "def", "_load_image", "(", "path", ")", ":", "\n", "    ", "with", "Image", ".", "open", "(", "path", ")", "as", "pil_im", ":", "\n", "        ", "im", "=", "np", ".", "array", "(", "pil_im", ".", "convert", "(", "\"RGB\"", ")", ")", "\n", "", "im", "=", "im", ".", "transpose", "(", "(", "2", ",", "0", ",", "1", ")", ")", "\n", "im", "=", "im", ".", "astype", "(", "np", ".", "float32", ")", "/", "255.0", "\n", "return", "im", "\n", "\n"]], "home.repos.pwc.inspect_result.applied-ai-lab_zero-shot-pose.datasets.co3d_dataset._load_16big_png_depth": [[728, 738], ["PIL.Image.open", "numpy.frombuffer().astype().reshape", "numpy.frombuffer().astype", "numpy.frombuffer", "numpy.array"], "function", ["None"], ["", "def", "_load_16big_png_depth", "(", "depth_png", ")", ":", "\n", "    ", "with", "Image", ".", "open", "(", "depth_png", ")", "as", "depth_pil", ":", "\n", "# the image is stored with 16-bit depth but PIL reads it as I (32 bit).", "\n", "# we cast it to uint16, then reinterpret as float16, then cast to float32", "\n", "        ", "depth", "=", "(", "\n", "np", ".", "frombuffer", "(", "np", ".", "array", "(", "depth_pil", ",", "dtype", "=", "np", ".", "uint16", ")", ",", "dtype", "=", "np", ".", "float16", ")", "\n", ".", "astype", "(", "np", ".", "float32", ")", "\n", ".", "reshape", "(", "(", "depth_pil", ".", "size", "[", "1", "]", ",", "depth_pil", ".", "size", "[", "0", "]", ")", ")", "\n", ")", "\n", "", "return", "depth", "\n", "\n"]], "home.repos.pwc.inspect_result.applied-ai-lab_zero-shot-pose.datasets.co3d_dataset._load_1bit_png_mask": [[740, 744], ["PIL.Image.open", "numpy.array", "pil_im.convert"], "function", ["None"], ["", "def", "_load_1bit_png_mask", "(", "file", ":", "str", ")", ":", "\n", "    ", "with", "Image", ".", "open", "(", "file", ")", "as", "pil_im", ":", "\n", "        ", "mask", "=", "(", "np", ".", "array", "(", "pil_im", ".", "convert", "(", "\"L\"", ")", ")", ">", "0.0", ")", ".", "astype", "(", "np", ".", "float32", ")", "\n", "", "return", "mask", "\n", "\n"]], "home.repos.pwc.inspect_result.applied-ai-lab_zero-shot-pose.datasets.co3d_dataset._load_depth_mask": [[746, 751], ["co3d_dataset._load_1bit_png_mask", "path.lower().endswith", "ValueError", "path.lower"], "function", ["home.repos.pwc.inspect_result.applied-ai-lab_zero-shot-pose.datasets.co3d_dataset._load_1bit_png_mask"], ["", "def", "_load_depth_mask", "(", "path", ")", ":", "\n", "    ", "if", "not", "path", ".", "lower", "(", ")", ".", "endswith", "(", "\".png\"", ")", ":", "\n", "        ", "raise", "ValueError", "(", "'unsupported depth mask file name \"%s\"'", "%", "path", ")", "\n", "", "m", "=", "_load_1bit_png_mask", "(", "path", ")", "\n", "return", "m", "[", "None", "]", "# fake feature channel", "\n", "\n"]], "home.repos.pwc.inspect_result.applied-ai-lab_zero-shot-pose.datasets.co3d_dataset._load_depth": [[753, 760], ["path.lower().endswith", "ValueError", "co3d_dataset._load_16big_png_depth", "path.lower", "numpy.isfinite"], "function", ["home.repos.pwc.inspect_result.applied-ai-lab_zero-shot-pose.datasets.co3d_dataset._load_16big_png_depth"], ["", "def", "_load_depth", "(", "path", ",", "scale_adjustment", ")", ":", "\n", "    ", "if", "not", "path", ".", "lower", "(", ")", ".", "endswith", "(", "\".jpg.geometric.png\"", ")", ":", "\n", "        ", "raise", "ValueError", "(", "'unsupported depth file name \"%s\"'", "%", "path", ")", "\n", "\n", "", "d", "=", "_load_16big_png_depth", "(", "path", ")", "*", "scale_adjustment", "\n", "d", "[", "~", "np", ".", "isfinite", "(", "d", ")", "]", "=", "0.0", "\n", "return", "d", "[", "None", "]", "# fake feature channel", "\n", "\n"]], "home.repos.pwc.inspect_result.applied-ai-lab_zero-shot-pose.datasets.co3d_dataset._load_mask": [[762, 767], ["PIL.Image.open", "numpy.array", "np.array.astype"], "function", ["None"], ["", "def", "_load_mask", "(", "path", ")", ":", "\n", "    ", "with", "Image", ".", "open", "(", "path", ")", "as", "pil_im", ":", "\n", "        ", "mask", "=", "np", ".", "array", "(", "pil_im", ")", "\n", "", "mask", "=", "mask", ".", "astype", "(", "np", ".", "float32", ")", "/", "255.0", "\n", "return", "mask", "[", "None", "]", "# fake feature channel", "\n", "\n"]], "home.repos.pwc.inspect_result.applied-ai-lab_zero-shot-pose.datasets.co3d_dataset._get_1d_bounds": [[769, 772], ["numpy.flatnonzero"], "function", ["None"], ["", "def", "_get_1d_bounds", "(", "arr", ")", ":", "\n", "    ", "nz", "=", "np", ".", "flatnonzero", "(", "arr", ")", "\n", "return", "nz", "[", "0", "]", ",", "nz", "[", "-", "1", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.applied-ai-lab_zero-shot-pose.datasets.co3d_dataset._get_bbox_from_mask": [[774, 787], ["numpy.zeros_like", "co3d_dataset._get_1d_bounds", "co3d_dataset._get_1d_bounds", "np.zeros_like.sum", "warnings.warn", "np.zeros_like.sum", "np.zeros_like.sum"], "function", ["home.repos.pwc.inspect_result.applied-ai-lab_zero-shot-pose.datasets.co3d_dataset._get_1d_bounds", "home.repos.pwc.inspect_result.applied-ai-lab_zero-shot-pose.datasets.co3d_dataset._get_1d_bounds"], ["", "def", "_get_bbox_from_mask", "(", "mask", ",", "thr", ",", "decrease_quant", "=", "0.05", ")", ":", "\n", "# bbox in xywh", "\n", "    ", "masks_for_box", "=", "np", ".", "zeros_like", "(", "mask", ")", "\n", "while", "masks_for_box", ".", "sum", "(", ")", "<=", "1.0", ":", "\n", "        ", "masks_for_box", "=", "(", "mask", ">", "thr", ")", ".", "astype", "(", "np", ".", "float32", ")", "\n", "thr", "-=", "decrease_quant", "\n", "", "if", "thr", "<=", "0.0", ":", "\n", "        ", "warnings", ".", "warn", "(", "f\"Empty masks_for_bbox (thr={thr}) => using full image.\"", ")", "\n", "\n", "", "x0", ",", "x1", "=", "_get_1d_bounds", "(", "masks_for_box", ".", "sum", "(", "axis", "=", "-", "2", ")", ")", "\n", "y0", ",", "y1", "=", "_get_1d_bounds", "(", "masks_for_box", ".", "sum", "(", "axis", "=", "-", "1", ")", ")", "\n", "\n", "return", "x0", ",", "y0", ",", "x1", "-", "x0", ",", "y1", "-", "y0", "\n", "\n"]], "home.repos.pwc.inspect_result.applied-ai-lab_zero-shot-pose.datasets.co3d_dataset._get_clamp_bbox": [[789, 811], ["torch.clamp", "bbox.float.float", "warnings.warn"], "function", ["None"], ["", "def", "_get_clamp_bbox", "(", "bbox", ",", "box_crop_context", "=", "0.0", ",", "impath", "=", "\"\"", ")", ":", "\n", "# box_crop_context: rate of expansion for bbox", "\n", "# returns possibly expanded bbox xyxy as float", "\n", "\n", "# increase box size", "\n", "    ", "if", "box_crop_context", ">", "0.0", ":", "\n", "        ", "c", "=", "box_crop_context", "\n", "bbox", "=", "bbox", ".", "float", "(", ")", "\n", "bbox", "[", "0", "]", "-=", "bbox", "[", "2", "]", "*", "c", "/", "2", "\n", "bbox", "[", "1", "]", "-=", "bbox", "[", "3", "]", "*", "c", "/", "2", "\n", "bbox", "[", "2", "]", "+=", "bbox", "[", "2", "]", "*", "c", "\n", "bbox", "[", "3", "]", "+=", "bbox", "[", "3", "]", "*", "c", "\n", "\n", "", "if", "(", "bbox", "[", "2", ":", "]", "<=", "1.0", ")", ".", "any", "(", ")", ":", "\n", "        ", "warnings", ".", "warn", "(", "f\"squashed image {impath}!!\"", ")", "\n", "return", "None", "\n", "\n", "", "bbox", "[", "2", ":", "]", "=", "torch", ".", "clamp", "(", "bbox", "[", "2", ":", "]", ",", "2", ")", "\n", "bbox", "[", "2", ":", "]", "+=", "bbox", "[", "0", ":", "2", "]", "+", "1", "# convert to [xmin, ymin, xmax, ymax]", "\n", "# +1 because upper bound is not inclusive", "\n", "\n", "return", "bbox", "\n", "\n"]], "home.repos.pwc.inspect_result.applied-ai-lab_zero-shot-pose.datasets.co3d_dataset._crop_around_box": [[813, 822], ["torch.clamp", "torch.clamp", "bbox.round().long.round().long", "all", "bbox.round().long.round"], "function", ["None"], ["", "def", "_crop_around_box", "(", "tensor", ",", "bbox", ",", "impath", "=", "\"\"", ")", ":", "\n", "# bbox is xyxy, where the upper bound is corrected with +1", "\n", "    ", "bbox", "[", "[", "0", ",", "2", "]", "]", "=", "torch", ".", "clamp", "(", "bbox", "[", "[", "0", ",", "2", "]", "]", ",", "0.0", ",", "tensor", ".", "shape", "[", "-", "1", "]", ")", "\n", "bbox", "[", "[", "1", ",", "3", "]", "]", "=", "torch", ".", "clamp", "(", "bbox", "[", "[", "1", ",", "3", "]", "]", ",", "0.0", ",", "tensor", ".", "shape", "[", "-", "2", "]", ")", "\n", "bbox", "=", "bbox", ".", "round", "(", ")", ".", "long", "(", ")", "\n", "tensor", "=", "tensor", "[", "...", ",", "bbox", "[", "1", "]", ":", "bbox", "[", "3", "]", ",", "bbox", "[", "0", "]", ":", "bbox", "[", "2", "]", "]", "\n", "assert", "all", "(", "c", ">", "0", "for", "c", "in", "tensor", ".", "shape", ")", ",", "f\"squashed image {impath}\"", "\n", "\n", "return", "tensor", "\n", "\n"]], "home.repos.pwc.inspect_result.applied-ai-lab_zero-shot-pose.datasets.co3d_dataset._rescale_bbox": [[824, 830], ["numpy.prod"], "function", ["None"], ["", "def", "_rescale_bbox", "(", "bbox", ",", "orig_res", ",", "new_res", ")", ":", "\n", "    ", "assert", "bbox", "is", "not", "None", "\n", "assert", "np", ".", "prod", "(", "orig_res", ")", ">", "1e-8", "\n", "# average ratio of dimensions", "\n", "rel_size", "=", "(", "new_res", "[", "0", "]", "/", "orig_res", "[", "0", "]", "+", "new_res", "[", "1", "]", "/", "orig_res", "[", "1", "]", ")", "/", "2.0", "\n", "return", "bbox", "*", "rel_size", "\n", "\n"]], "home.repos.pwc.inspect_result.applied-ai-lab_zero-shot-pose.datasets.co3d_dataset._safe_as_tensor": [[832, 836], ["torch.tensor", "torch.long", "torch.float", "torch.long", "torch.float", "torch.float"], "function", ["None"], ["", "def", "_safe_as_tensor", "(", "data", ",", "dtype", ")", ":", "\n", "    ", "if", "data", "is", "None", ":", "\n", "        ", "return", "None", "\n", "", "return", "torch", ".", "tensor", "(", "data", ",", "dtype", "=", "dtype", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.applied-ai-lab_zero-shot-pose.datasets.co3d_dataset._load_pointcloud": [[841, 861], ["functools.lru_cache", "torch.stack", "torch.stack.split", "pytorch3d.structures.pointclouds.Pointclouds", "open", "plyfile.PlyData.read", "torch.FloatTensor", "torch.randperm", "numpy.array().astype", "numpy.array"], "function", ["None"], ["", "@", "functools", ".", "lru_cache", "(", "maxsize", "=", "256", ")", "\n", "def", "_load_pointcloud", "(", "pcl_path", ",", "max_points", "=", "0", ")", ":", "\n", "    ", "with", "open", "(", "pcl_path", ",", "\"rb\"", ")", "as", "f", ":", "\n", "        ", "plydata", "=", "PlyData", ".", "read", "(", "f", ")", "\n", "\n", "", "pcl_data", "=", "torch", ".", "stack", "(", "\n", "[", "\n", "torch", ".", "FloatTensor", "(", "np", ".", "array", "(", "plydata", "[", "\"vertex\"", "]", "[", "c", "]", ")", ".", "astype", "(", "float", ")", ")", "\n", "for", "c", "in", "(", "\"x\"", ",", "\"y\"", ",", "\"z\"", ",", "\"red\"", ",", "\"green\"", ",", "\"blue\"", ",", "\"nx\"", ",", "\"ny\"", ",", "\"nz\"", ")", "\n", "]", ",", "\n", "dim", "=", "1", ",", "\n", ")", "\n", "\n", "if", "pcl_data", ".", "shape", "[", "0", "]", ">", "max_points", ">", "0", ":", "\n", "        ", "prm", "=", "torch", ".", "randperm", "(", "pcl_data", ".", "shape", "[", "0", "]", ")", "[", ":", "max_points", "]", "\n", "pcl_data", "=", "pcl_data", "[", "prm", "]", "\n", "\n", "", "points", ",", "points_rgb", ",", "normals", "=", "pcl_data", ".", "split", "(", "[", "3", ",", "3", ",", "3", "]", ",", "dim", "=", "1", ")", "\n", "\n", "return", "Pointclouds", "(", "points", "[", "None", "]", ",", "normals", "[", "None", "]", ",", "features", "=", "points_rgb", "[", "None", "]", "/", "255.0", ")", "\n", "", ""]], "home.repos.pwc.inspect_result.applied-ai-lab_zero-shot-pose.datasets.co3d_types.dump_dataclass": [[103, 114], ["f.write", "json.dump", "json.dumps().encode", "co3d_types._asdict_rec", "json.dumps", "co3d_types._asdict_rec"], "function", ["home.repos.pwc.inspect_result.applied-ai-lab_zero-shot-pose.datasets.co3d_types._asdict_rec", "home.repos.pwc.inspect_result.applied-ai-lab_zero-shot-pose.datasets.co3d_types._asdict_rec"], ["", "def", "dump_dataclass", "(", "obj", ":", "Any", ",", "f", ":", "Union", "[", "IO", ",", "str", "]", ",", "binary", ":", "bool", "=", "False", ")", ":", "\n", "    ", "\"\"\"\n    Args:\n        f: Either a path to a file, or a file opened for writing.\n        obj: A @dataclass or collection hiererchy including dataclasses.\n        binary: Set to True if `f` is a file handle, else False.\n    \"\"\"", "\n", "if", "binary", ":", "\n", "        ", "f", ".", "write", "(", "json", ".", "dumps", "(", "_asdict_rec", "(", "obj", ")", ")", ".", "encode", "(", "\"utf8\"", ")", ")", "\n", "", "else", ":", "\n", "        ", "json", ".", "dump", "(", "_asdict_rec", "(", "obj", ")", ",", "f", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.applied-ai-lab_zero-shot-pose.datasets.co3d_types.load_dataclass": [[116, 133], ["co3d_types._dataclass_from_dict", "json.loads", "json.load", "f.read().decode", "f.read"], "function", ["home.repos.pwc.inspect_result.applied-ai-lab_zero-shot-pose.datasets.co3d_types._dataclass_from_dict"], ["", "", "def", "load_dataclass", "(", "f", ":", "Union", "[", "IO", ",", "str", "]", ",", "cls", ":", "Any", ",", "binary", ":", "bool", "=", "False", ")", ":", "\n", "    ", "\"\"\"\n    Loads to a @dataclass or collection hiererchy including dataclasses\n    from a json recursively.\n    Call it like load_dataclass(f, typing.List[FrameAnnotationAnnotation]).\n    raises KeyError if json has keys not mapping to the dataclass fields.\n\n    Args:\n        f: Either a path to a file, or a file opened for writing.\n        cls: The class of the loaded dataclass.\n        binary: Set to True if `f` is a file handle, else False.\n    \"\"\"", "\n", "if", "binary", ":", "\n", "        ", "asdict", "=", "json", ".", "loads", "(", "f", ".", "read", "(", ")", ".", "decode", "(", "\"utf8\"", ")", ")", "\n", "", "else", ":", "\n", "        ", "asdict", "=", "json", ".", "load", "(", "f", ")", "\n", "", "return", "_dataclass_from_dict", "(", "asdict", ",", "cls", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.applied-ai-lab_zero-shot-pose.datasets.co3d_types._asdict_rec": [[135, 137], ["dataclasses._asdict_inner"], "function", ["None"], ["", "def", "_asdict_rec", "(", "obj", ")", ":", "\n", "    ", "return", "dataclasses", ".", "_asdict_inner", "(", "obj", ",", "dict", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.applied-ai-lab_zero-shot-pose.datasets.co3d_types._get_origin": [[140, 142], ["getattr"], "function", ["None"], ["", "def", "_get_origin", "(", "cls", ")", ":", "\n", "    ", "return", "getattr", "(", "cls", ",", "\"__origin__\"", ",", "None", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.applied-ai-lab_zero-shot-pose.datasets.co3d_types._dataclass_from_dict": [[144, 168], ["dataclasses.is_dataclass", "cls", "co3d_types._get_origin", "co3d_types._unwrap_type", "issubclass", "hasattr", "cls._field_types.values", "cls", "issubclass", "dataclasses.fields", "typing.get_args", "cls", "issubclass", "co3d_types._dataclass_from_dict", "len", "typing.get_args", "cls", "d.items", "co3d_types._dataclass_from_dict", "len", "co3d_types._dataclass_from_dict", "dataclasses.is_dataclass", "zip", "zip", "co3d_types._dataclass_from_dict", "co3d_types._dataclass_from_dict", "d.items"], "function", ["home.repos.pwc.inspect_result.applied-ai-lab_zero-shot-pose.datasets.co3d_types._get_origin", "home.repos.pwc.inspect_result.applied-ai-lab_zero-shot-pose.datasets.co3d_types._unwrap_type", "home.repos.pwc.inspect_result.applied-ai-lab_zero-shot-pose.datasets.co3d_types._dataclass_from_dict", "home.repos.pwc.inspect_result.applied-ai-lab_zero-shot-pose.datasets.co3d_types._dataclass_from_dict", "home.repos.pwc.inspect_result.applied-ai-lab_zero-shot-pose.datasets.co3d_types._dataclass_from_dict", "home.repos.pwc.inspect_result.applied-ai-lab_zero-shot-pose.datasets.co3d_types._dataclass_from_dict", "home.repos.pwc.inspect_result.applied-ai-lab_zero-shot-pose.datasets.co3d_types._dataclass_from_dict"], ["", "def", "_dataclass_from_dict", "(", "d", ",", "typeannot", ")", ":", "\n", "    ", "cls", "=", "_get_origin", "(", "typeannot", ")", "or", "typeannot", "\n", "if", "d", "is", "None", ":", "\n", "        ", "return", "d", "\n", "", "elif", "issubclass", "(", "cls", ",", "tuple", ")", "and", "hasattr", "(", "cls", ",", "\"_fields\"", ")", ":", "# namedtuple", "\n", "        ", "types", "=", "cls", ".", "_field_types", ".", "values", "(", ")", "\n", "return", "cls", "(", "*", "[", "_dataclass_from_dict", "(", "v", ",", "tp", ")", "for", "v", ",", "tp", "in", "zip", "(", "d", ",", "types", ")", "]", ")", "\n", "", "elif", "issubclass", "(", "cls", ",", "(", "list", ",", "tuple", ")", ")", ":", "\n", "        ", "types", "=", "typing", ".", "get_args", "(", "typeannot", ")", "\n", "if", "len", "(", "types", ")", "==", "1", ":", "# probably List; replicate for all items", "\n", "            ", "types", "=", "types", "*", "len", "(", "d", ")", "\n", "", "return", "cls", "(", "_dataclass_from_dict", "(", "v", ",", "tp", ")", "for", "v", ",", "tp", "in", "zip", "(", "d", ",", "types", ")", ")", "\n", "", "elif", "issubclass", "(", "cls", ",", "dict", ")", ":", "\n", "        ", "key_t", ",", "val_t", "=", "typing", ".", "get_args", "(", "typeannot", ")", "\n", "return", "cls", "(", "\n", "(", "_dataclass_from_dict", "(", "k", ",", "key_t", ")", ",", "_dataclass_from_dict", "(", "v", ",", "val_t", ")", ")", "\n", "for", "k", ",", "v", "in", "d", ".", "items", "(", ")", "\n", ")", "\n", "", "elif", "not", "dataclasses", ".", "is_dataclass", "(", "typeannot", ")", ":", "\n", "        ", "return", "d", "\n", "\n", "", "assert", "dataclasses", ".", "is_dataclass", "(", "cls", ")", "\n", "fieldtypes", "=", "{", "f", ".", "name", ":", "_unwrap_type", "(", "f", ".", "type", ")", "for", "f", "in", "dataclasses", ".", "fields", "(", "typeannot", ")", "}", "\n", "return", "cls", "(", "**", "{", "k", ":", "_dataclass_from_dict", "(", "v", ",", "fieldtypes", "[", "k", "]", ")", "for", "k", ",", "v", "in", "d", ".", "items", "(", ")", "}", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.applied-ai-lab_zero-shot-pose.datasets.co3d_types._unwrap_type": [[170, 178], ["co3d_types._get_origin", "typing.get_args", "any", "len", "type", "type"], "function", ["home.repos.pwc.inspect_result.applied-ai-lab_zero-shot-pose.datasets.co3d_types._get_origin"], ["", "def", "_unwrap_type", "(", "tp", ")", ":", "\n", "# strips Optional wrapper, if any", "\n", "    ", "if", "_get_origin", "(", "tp", ")", "is", "typing", ".", "Union", ":", "\n", "        ", "args", "=", "typing", ".", "get_args", "(", "tp", ")", "\n", "if", "len", "(", "args", ")", "==", "2", "and", "any", "(", "a", "is", "type", "(", "None", ")", "for", "a", "in", "args", ")", ":", "\n", "# this is typing.Optional", "\n", "            ", "return", "args", "[", "0", "]", "if", "args", "[", "1", "]", "is", "type", "(", "None", ")", "else", "args", "[", "1", "]", "\n", "", "", "return", "tp", "\n", "\n"]], "home.repos.pwc.inspect_result.applied-ai-lab_zero-shot-pose.datasets.co3d_types.dump_dataclass_jgzip": [[180, 190], ["gzip.GzipFile", "co3d_types.dump_dataclass"], "function", ["home.repos.pwc.inspect_result.applied-ai-lab_zero-shot-pose.datasets.co3d_types.dump_dataclass"], ["", "def", "dump_dataclass_jgzip", "(", "outfile", ":", "str", ",", "obj", ":", "Any", ")", ":", "\n", "    ", "\"\"\"\n    Dumps obj to a gzipped json outfile.\n\n    Args:\n        obj: A @dataclass or collection hiererchy including dataclasses.\n        outfile: The path to the output file.\n    \"\"\"", "\n", "with", "gzip", ".", "GzipFile", "(", "outfile", ",", "\"wb\"", ")", "as", "f", ":", "\n", "        ", "dump_dataclass", "(", "f", ",", "obj", ",", "binary", "=", "True", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.applied-ai-lab_zero-shot-pose.datasets.co3d_types.load_dataclass_jgzip": [[192, 205], ["gzip.GzipFile", "co3d_types.load_dataclass"], "function", ["home.repos.pwc.inspect_result.applied-ai-lab_zero-shot-pose.datasets.co3d_types.load_dataclass"], ["", "", "def", "load_dataclass_jgzip", "(", "outfile", ",", "cls", ")", ":", "\n", "    ", "\"\"\"\n    Loads a dataclass from a gzipped json outfile.\n\n    Args:\n        outfile: The path to the loaded file.\n        cls: The type annotation of the loaded dataclass.\n\n    Returns:\n        loaded_dataclass: The loaded dataclass.\n    \"\"\"", "\n", "with", "gzip", ".", "GzipFile", "(", "outfile", ",", "\"rb\"", ")", "as", "f", ":", "\n", "        ", "return", "load_dataclass", "(", "f", ",", "cls", ",", "binary", "=", "True", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.applied-ai-lab_zero-shot-pose.datasets.co3d_pose_dataset.Co3DPoseDataset.__init__": [[21, 87], ["co3d_pose_dataset.Co3DPoseDataset.load_seq_world_coords", "os.path.join", "os.listdir", "co3d_pose_dataset.Co3DPoseDataset.get_base_dataset", "co3d_pose_dataset.Co3DPoseDataset.construct_labelled_seq_dict", "os.path.join", "os.path.exists", "co3d_pose_dataset.Co3DPoseDataset.sample_instances", "candidate_seqs[].append", "len", "ValueError", "torch.load", "print", "co3d_pose_dataset.Co3DPoseDataset.sample_instances", "f.rstrip", "len", "ValueError", "len", "len"], "methods", ["home.repos.pwc.inspect_result.applied-ai-lab_zero-shot-pose.datasets.co3d_pose_dataset.Co3DPoseDataset.load_seq_world_coords", "home.repos.pwc.inspect_result.applied-ai-lab_zero-shot-pose.datasets.co3d_pose_dataset.PlottingCo3DPoseDataset.get_base_dataset", "home.repos.pwc.inspect_result.applied-ai-lab_zero-shot-pose.datasets.co3d_pose_dataset.Co3DPoseDataset.construct_labelled_seq_dict", "home.repos.pwc.inspect_result.applied-ai-lab_zero-shot-pose.datasets.co3d_pose_dataset.Co3DPoseDataset.sample_instances", "home.repos.pwc.inspect_result.applied-ai-lab_zero-shot-pose.datasets.co3d_pose_dataset.Co3DPoseDataset.sample_instances"], ["    ", "def", "__init__", "(", "self", ",", "dataset_root", "=", "co3d_root", ",", "\n", "categories", ":", "List", "[", "str", "]", "=", "[", "'bicycle'", "]", ",", "\n", "num_samples_per_class", ":", "int", "=", "5", ",", "\n", "target_frames_sampling_mode", ":", "str", "=", "'uniform'", ",", "\n", "num_frames_in_target_seq", ":", "int", "=", "5", ",", "\n", "label_dir", ":", "str", "=", "label_dir", ",", "\n", "determ_eval_root", ":", "str", "=", "None", ",", "\n", "image_transform", "=", "None", ")", ":", "\n", "\n", "# Get all candidate sequences for a class", "\n", "        ", "candidate_seqs", "=", "{", "}", "\n", "for", "cat", "in", "categories", ":", "\n", "\n", "            ", "candidate_seqs", "[", "cat", "]", "=", "[", "]", "\n", "cat_dir", "=", "os", ".", "path", ".", "join", "(", "label_dir", ",", "cat", ")", "\n", "\n", "for", "f", "in", "os", ".", "listdir", "(", "cat_dir", ")", ":", "\n", "                ", "candidate_seqs", "[", "cat", "]", ".", "append", "(", "f", ".", "rstrip", "(", "'.json'", ")", ")", "\n", "\n", "# Dict of datasets, one for each class", "\n", "", "", "self", ".", "classes", "=", "categories", "\n", "self", ".", "all_datasets", "=", "{", "\n", "cls", ":", "self", ".", "get_base_dataset", "(", "dataset_root", ",", "cls", ")", "\n", "for", "cls", "in", "self", ".", "classes", "\n", "}", "\n", "\n", "# Stores which sequences contain which Frame IDs", "\n", "self", ".", "labelled_seqs_to_frames", "=", "{", "cls", ":", "self", ".", "construct_labelled_seq_dict", "(", "cls", ",", "candidate_seqs", "[", "cls", "]", ")", "\n", "for", "cls", "in", "self", ".", "classes", "}", "\n", "\n", "# Stores all the sequences which have labels", "\n", "self", ".", "labelled_seqs", "=", "candidate_seqs", "\n", "\n", "# Computes and stores samples of (reference_frame, [target_frame_0...target_frame_N]) pairs", "\n", "# Stored in format", "\n", "# sample = {", "\n", "#         'class': class,", "\n", "#         'reference_seq_name': root_seq_name,", "\n", "#         'reference_frame_id': root_frame_id,", "\n", "#         'target_seq_name': target_seq_name,", "\n", "#         'all_target_seq_id': target_frame_ids", "\n", "#         }", "\n", "if", "determ_eval_root", ":", "\n", "            ", "if", "len", "(", "categories", ")", "!=", "1", ":", "\n", "                ", "raise", "ValueError", "(", "'Can only use determ_eval_path when a single category is being loaded'", ")", "\n", "", "determ_eval_path", "=", "os", ".", "path", ".", "join", "(", "determ_eval_root", ",", "f'{categories[0]}.pt'", ")", "\n", "if", "os", ".", "path", ".", "exists", "(", "determ_eval_path", ")", ":", "\n", "                ", "self", ".", "samples", "=", "torch", ".", "load", "(", "determ_eval_path", ")", "\n", "if", "len", "(", "self", ".", "samples", ")", "<", "num_samples_per_class", ":", "\n", "                    ", "raise", "ValueError", "(", "\n", "f'Only {len(self.samples)} samples in the determ set, but asked for {num_samples_per_class}'", ")", "\n", "", "self", ".", "samples", "=", "self", ".", "samples", "[", ":", "num_samples_per_class", "]", "\n", "print", "(", "f\"Successfully loaded determ eval set for category {categories[0]}, kept {len(self.samples)} samples\"", ")", "\n", "", "else", ":", "\n", "                ", "self", ".", "samples", "=", "self", ".", "sample_instances", "(", "num_samples_per_class", "=", "num_samples_per_class", ",", "\n", "num_frames_in_target_seq", "=", "num_frames_in_target_seq", ",", "\n", "target_frame_sampling_mode", "=", "target_frames_sampling_mode", ")", "\n", "", "", "else", ":", "\n", "            ", "self", ".", "samples", "=", "self", ".", "sample_instances", "(", "num_samples_per_class", "=", "num_samples_per_class", ",", "\n", "num_frames_in_target_seq", "=", "num_frames_in_target_seq", ",", "\n", "target_frame_sampling_mode", "=", "target_frames_sampling_mode", ")", "\n", "# Load ground truth coordinate frames", "\n", "", "self", ".", "seq_world_coords", "=", "self", ".", "load_seq_world_coords", "(", "label_dir", ")", "\n", "\n", "# Transform on image data", "\n", "self", ".", "image_transform", "=", "image_transform", "\n", "\n"]], "home.repos.pwc.inspect_result.applied-ai-lab_zero-shot-pose.datasets.co3d_pose_dataset.Co3DPoseDataset.get_base_dataset": [[88, 98], ["zsp.datasets.co3d_dataset_fix.Co3dDataset", "os.path.join", "os.path.join"], "methods", ["None"], ["", "def", "get_base_dataset", "(", "self", ",", "dataset_root", ",", "cls", ")", ":", "\n", "        ", "return", "Co3dDataset", "(", "frame_annotations_file", "=", "os", ".", "path", ".", "join", "(", "dataset_root", ",", "cls", ",", "'frame_annotations.jgz'", ")", ",", "\n", "sequence_annotations_file", "=", "os", ".", "path", ".", "join", "(", "dataset_root", ",", "cls", ",", "'sequence_annotations.jgz'", ")", ",", "\n", "# subset_lists_file=os.path.join(dataset_root, cls, 'set_lists.json'),", "\n", "dataset_root", "=", "dataset_root", ",", "\n", "box_crop", "=", "True", ",", "\n", "box_crop_context", "=", "0.1", ",", "\n", "image_height", "=", "None", ",", "# Doesn't resize", "\n", "image_width", "=", "None", ",", "# Doesn't resize", "\n", "load_point_clouds", "=", "True", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.applied-ai-lab_zero-shot-pose.datasets.co3d_pose_dataset.Co3DPoseDataset.sample_instances": [[100, 196], ["numpy.random.seed", "random.seed", "sorted", "len", "list", "list", "random.choice", "seq_pairs.append", "random.choice", "len", "all_samples.append", "range", "range", "co3d_pose_dataset.Co3DPoseDataset.sample_instances.custom_range"], "methods", ["None"], ["", "def", "sample_instances", "(", "self", ",", "num_samples_per_class", "=", "1", ",", "\n", "num_frames_in_target_seq", "=", "5", ",", "target_frame_sampling_mode", "=", "'uniform'", ")", ":", "\n", "\n", "        ", "\"\"\"\n        For all categories, compare 'num_samples_per_class' sequences\n        Choose sequence as the root and randomly sample a FrameID\n        Sample 'num_frames_in_target_seq' frame IDs from the other frame\n        Two sampling modes for the target frames:\n            * 'uniform': Randomly sample a first index then uniformly sample the rest\n            * 'random': Randomly sample all target frames\n        Store all of this in a list of dicts\n        \"\"\"", "\n", "\n", "def", "custom_range", "(", "n", ",", "end", ",", "start", "=", "0", ")", ":", "\n", "            ", "return", "list", "(", "range", "(", "start", ",", "n", ")", ")", "+", "list", "(", "range", "(", "n", "+", "1", ",", "end", ")", ")", "\n", "\n", "", "np", ".", "random", ".", "seed", "(", "0", ")", "\n", "random", ".", "seed", "(", "0", ")", "\n", "\n", "all_samples", "=", "[", "]", "\n", "\n", "for", "cls", "in", "self", ".", "classes", ":", "\n", "\n", "# First, sample which pairs of sequences we are going to compare", "\n", "            ", "labelled_seqs", "=", "self", ".", "labelled_seqs", "[", "cls", "]", "\n", "labelled_seqs", "=", "sorted", "(", "labelled_seqs", ")", "\n", "num_seqs_in_cls", "=", "len", "(", "labelled_seqs", ")", "\n", "seq_pairs", "=", "[", "]", "\n", "\n", "root_seq_id", "=", "0", "\n", "num_samples_in_this_class_so_far", "=", "0", "\n", "while", "num_samples_in_this_class_so_far", "<", "num_samples_per_class", ":", "\n", "\n", "                ", "root_seq_name", "=", "labelled_seqs", "[", "root_seq_id", "]", "\n", "target_seq_id", "=", "random", ".", "choice", "(", "custom_range", "(", "root_seq_id", ",", "num_seqs_in_cls", ",", "start", "=", "0", ")", ")", "\n", "target_seq_name", "=", "labelled_seqs", "[", "target_seq_id", "]", "\n", "seq_pairs", ".", "append", "(", "(", "root_seq_name", ",", "target_seq_name", ")", ")", "\n", "\n", "num_samples_in_this_class_so_far", "+=", "1", "\n", "root_seq_id", "+=", "1", "\n", "root_seq_id", "=", "root_seq_id", "%", "num_seqs_in_cls", "\n", "\n", "# Now sample frame IDs from each pair of sequences", "\n", "# One frame from the root, and 'num_frames_in_target_seq' in the target", "\n", "", "for", "root_seq_name", ",", "target_seq_name", "in", "seq_pairs", ":", "\n", "\n", "# Sample frame from root sequence", "\n", "                ", "root_frame_number", "=", "random", ".", "choice", "(", "\n", "range", "(", "len", "(", "self", ".", "labelled_seqs_to_frames", "[", "cls", "]", "[", "root_seq_name", "]", ")", ")", "\n", ")", "\n", "\n", "root_frame_id", "=", "self", ".", "labelled_seqs_to_frames", "[", "cls", "]", "[", "root_seq_name", "]", "[", "root_frame_number", "]", "\n", "\n", "# Sample frames from target sequence", "\n", "# Uniform sampling:", "\n", "len_current_target_sequence", "=", "len", "(", "self", ".", "labelled_seqs_to_frames", "[", "cls", "]", "[", "target_seq_name", "]", ")", "\n", "if", "target_frame_sampling_mode", "==", "'uniform'", ":", "\n", "                    ", "target_frame_number", "=", "random", ".", "choice", "(", "\n", "range", "(", "len", "(", "self", ".", "labelled_seqs_to_frames", "[", "cls", "]", "[", "target_seq_name", "]", ")", ")", "\n", ")", "\n", "target_frame_ids", "=", "[", "\n", "self", ".", "labelled_seqs_to_frames", "[", "cls", "]", "[", "target_seq_name", "]", "[", "target_frame_number", "]", "\n", "]", "\n", "while", "len", "(", "target_frame_ids", ")", "<", "num_frames_in_target_seq", ":", "\n", "\n", "                        ", "target_frame_number", "=", "(", "target_frame_number", "+", "len_current_target_sequence", "//", "num_frames_in_target_seq", ")", "%", "len_current_target_sequence", "\n", "target_frame_ids", ".", "append", "(", "\n", "self", ".", "labelled_seqs_to_frames", "[", "cls", "]", "[", "target_seq_name", "]", "[", "target_frame_number", "]", "\n", ")", "\n", "\n", "# Random sampling:", "\n", "", "", "elif", "target_frame_sampling_mode", "==", "'random'", ":", "\n", "\n", "                    ", "target_frame_numbers", "=", "random", ".", "sample", "(", "range", "(", "len_current_target_sequence", ")", ",", "\n", "k", "=", "num_frames_in_target_seq", ")", "\n", "target_frame_ids", "=", "[", "\n", "self", ".", "labelled_seqs_to_frames", "[", "cls", "]", "[", "target_seq_name", "]", "[", "i", "]", "\n", "for", "i", "in", "target_frame_numbers", "\n", "]", "\n", "\n", "", "else", ":", "\n", "\n", "                    ", "raise", "ValueError", "\n", "\n", "# Construct sample:", "\n", "", "sample", "=", "{", "\n", "'class'", ":", "cls", ",", "\n", "'reference_seq_name'", ":", "root_seq_name", ",", "\n", "'reference_frame_id'", ":", "root_frame_id", ",", "\n", "'target_seq_name'", ":", "target_seq_name", ",", "\n", "'all_target_id'", ":", "target_frame_ids", "\n", "}", "\n", "\n", "all_samples", ".", "append", "(", "sample", ")", "\n", "\n", "", "", "return", "all_samples", "\n", "\n"]], "home.repos.pwc.inspect_result.applied-ai-lab_zero-shot-pose.datasets.co3d_pose_dataset.Co3DPoseDataset.construct_labelled_seq_dict": [[197, 208], ["dict", "enumerate"], "methods", ["None"], ["", "def", "construct_labelled_seq_dict", "(", "self", ",", "cls", ",", "labelled_seq_names", ")", ":", "\n", "\n", "        ", "ds", "=", "self", ".", "all_datasets", "[", "cls", "]", "\n", "\n", "seqs_to_frames", "=", "dict", "(", "[", "(", "seq_name", ",", "{", "}", ")", "for", "seq_name", "in", "labelled_seq_names", "]", ")", "\n", "for", "i", ",", "frame_ann", "in", "enumerate", "(", "ds", ".", "frame_annots", ")", ":", "\n", "            ", "frame_ann", "=", "frame_ann", "[", "'frame_annotation'", "]", "\n", "if", "frame_ann", ".", "sequence_name", "in", "labelled_seq_names", ":", "\n", "                ", "seqs_to_frames", "[", "frame_ann", ".", "sequence_name", "]", "[", "frame_ann", ".", "frame_number", "]", "=", "i", "\n", "\n", "", "", "return", "seqs_to_frames", "\n", "\n"]], "home.repos.pwc.inspect_result.applied-ai-lab_zero-shot-pose.datasets.co3d_pose_dataset.Co3DPoseDataset.load_seq_world_coords": [[210, 226], ["os.listdir", "os.path.join", "os.listdir", "os.path.join", "numpy.array", "open", "json.load"], "methods", ["None"], ["", "def", "load_seq_world_coords", "(", "self", ",", "label_dir", ")", ":", "\n", "\n", "        ", "cat_labelled_dict", "=", "{", "}", "\n", "for", "cat", "in", "os", ".", "listdir", "(", "label_dir", ")", ":", "\n", "            ", "cat_labelled_dict", "[", "cat", "]", "=", "{", "}", "\n", "cat_dir", "=", "os", ".", "path", ".", "join", "(", "label_dir", ",", "cat", ")", "\n", "\n", "for", "f", "in", "os", ".", "listdir", "(", "cat_dir", ")", ":", "\n", "\n", "                ", "label_path", "=", "os", ".", "path", ".", "join", "(", "cat_dir", ",", "f", ")", "\n", "with", "open", "(", "label_path", ",", "'r'", ")", "as", "json_file", ":", "\n", "                    ", "seq_trans", "=", "json", ".", "load", "(", "json_file", ")", "\n", "\n", "", "cat_labelled_dict", "[", "cat", "]", "[", "seq_trans", "[", "'seq'", "]", "]", "=", "np", ".", "array", "(", "seq_trans", "[", "'trans'", "]", ")", "\n", "\n", "", "", "return", "cat_labelled_dict", "\n", "\n"]], "home.repos.pwc.inspect_result.applied-ai-lab_zero-shot-pose.datasets.co3d_pose_dataset.Co3DPoseDataset.process_list_of_frames": [[228, 262], ["co3d_pose_dataset.co3d_rgb_to_pil", "co3d_pose_dataset.Co3DPoseDataset.image_transform", "torch.Tensor", "numpy.array", "numpy.array", "zip"], "methods", ["home.repos.pwc.inspect_result.applied-ai-lab_zero-shot-pose.datasets.co3d_pose_dataset.co3d_rgb_to_pil"], ["", "def", "process_list_of_frames", "(", "self", ",", "frames", ")", ":", "\n", "\n", "        ", "\"\"\"\n        Take a list of frames and return list of:\n            Images (PIL or Tensor)\n            Scalings\n            Depth maps\n            Cameras\n        \"\"\"", "\n", "images", "=", "[", "co3d_rgb_to_pil", "(", "f", ".", "image_rgb", ")", "for", "f", "in", "frames", "]", "\n", "\n", "if", "self", ".", "image_transform", "is", "not", "None", ":", "\n", "\n", "            ", "images", "=", "[", "self", ".", "image_transform", "(", "im", ")", "for", "im", "in", "images", "]", "\n", "scalings", "=", "[", "\n", "np", ".", "array", "(", "f", ".", "image_rgb", ".", "shape", "[", "1", ":", "]", ")", "/", "np", ".", "array", "(", "im", ".", "shape", "[", "1", ":", "]", ")", "for", "im", ",", "f", "in", "zip", "(", "images", ",", "frames", ")", "\n", "]", "\n", "\n", "", "else", ":", "\n", "\n", "            ", "scalings", "=", "[", "\n", "torch", ".", "Tensor", "(", "[", "1", ",", "1", "]", ")", "for", "_", "in", "frames", "\n", "]", "\n", "\n", "# Get depth maps", "\n", "", "depth_map", "=", "[", "f", ".", "depth_map", "for", "f", "in", "frames", "]", "\n", "\n", "# Get cameras", "\n", "cameras", "=", "[", "f", ".", "camera", "for", "f", "in", "frames", "]", "\n", "\n", "# Pointclouds (just one needed per seq - same for every frame!)", "\n", "pcd", "=", "frames", "[", "0", "]", ".", "sequence_point_cloud", "\n", "\n", "return", "images", ",", "depth_map", ",", "scalings", ",", "cameras", ",", "pcd", "\n", "\n"]], "home.repos.pwc.inspect_result.applied-ai-lab_zero-shot-pose.datasets.co3d_pose_dataset.Co3DPoseDataset.__getitem__": [[264, 294], ["co3d_pose_dataset.Co3DPoseDataset.process_list_of_frames", "co3d_pose_dataset.Co3DPoseDataset.process_list_of_frames"], "methods", ["home.repos.pwc.inspect_result.applied-ai-lab_zero-shot-pose.datasets.co3d_pose_dataset.ICPCo3DPoseDataset.process_list_of_frames", "home.repos.pwc.inspect_result.applied-ai-lab_zero-shot-pose.datasets.co3d_pose_dataset.ICPCo3DPoseDataset.process_list_of_frames"], ["", "def", "__getitem__", "(", "self", ",", "item", ")", ":", "\n", "\n", "        ", "sample", "=", "self", ".", "samples", "[", "item", "]", "\n", "cls", "=", "sample", "[", "'class'", "]", "\n", "ref_seq_name", "=", "sample", "[", "'reference_seq_name'", "]", "\n", "target_seq_name", "=", "sample", "[", "'target_seq_name'", "]", "\n", "\n", "ref_frame_id", "=", "sample", "[", "'reference_frame_id'", "]", "\n", "all_target_id", "=", "sample", "[", "'all_target_id'", "]", "\n", "\n", "base_dataset", "=", "self", ".", "all_datasets", "[", "cls", "]", "\n", "\n", "# Get frames", "\n", "ref_frame", "=", "base_dataset", "[", "ref_frame_id", "]", "\n", "all_target_frames", "=", "[", "base_dataset", "[", "idx", "]", "for", "idx", "in", "all_target_id", "]", "\n", "\n", "# Get world coordinate transforms for each seq", "\n", "ref_transform", "=", "self", ".", "seq_world_coords", "[", "cls", "]", "[", "ref_seq_name", "]", "\n", "target_transform", "=", "self", ".", "seq_world_coords", "[", "cls", "]", "[", "target_seq_name", "]", "\n", "\n", "# Process ref frame", "\n", "ref_image", ",", "ref_depth_map", ",", "ref_scaling", ",", "ref_camera", ",", "ref_pcd", "=", "(", "x", "[", "0", "]", "for", "x", "in", "self", ".", "process_list_of_frames", "(", "[", "ref_frame", "]", ")", ")", "\n", "\n", "# Process target frames", "\n", "all_target_images", ",", "all_target_depth_maps", ",", "all_target_scalings", ",", "all_target_cameras", ",", "target_pcd", "=", "self", ".", "process_list_of_frames", "(", "all_target_frames", ")", "\n", "\n", "return", "ref_image", ",", "ref_transform", ",", "ref_depth_map", ",", "ref_scaling", ",", "ref_camera", ",", "all_target_images", ",", "target_transform", ",", "all_target_depth_maps", ",", "all_target_scalings", ",", "all_target_cameras", ",", "ref_pcd", ",", "target_pcd", ",", "item", "\n", "\n"]], "home.repos.pwc.inspect_result.applied-ai-lab_zero-shot-pose.datasets.co3d_pose_dataset.Co3DPoseDataset.__len__": [[296, 298], ["len"], "methods", ["None"], ["", "def", "__len__", "(", "self", ")", ":", "\n", "        ", "return", "len", "(", "self", ".", "samples", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.applied-ai-lab_zero-shot-pose.datasets.co3d_pose_dataset.ICPCo3DPoseDataset.__init__": [[302, 305], ["co3d_pose_dataset.Co3DPoseDataset.__init__"], "methods", ["home.repos.pwc.inspect_result.applied-ai-lab_zero-shot-pose.method.vision_transformer_flexible.DINOHead.__init__"], ["    ", "def", "__init__", "(", "self", ",", "*", "args", ",", "**", "kwargs", ")", ":", "\n", "\n", "        ", "super", "(", "ICPCo3DPoseDataset", ",", "self", ")", ".", "__init__", "(", "*", "args", ",", "**", "kwargs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.applied-ai-lab_zero-shot-pose.datasets.co3d_pose_dataset.ICPCo3DPoseDataset.get_base_dataset": [[306, 316], ["zsp.datasets.co3d_dataset_fix.Co3dDataset", "os.path.join", "os.path.join", "os.path.join"], "methods", ["None"], ["", "def", "get_base_dataset", "(", "self", ",", "dataset_root", ",", "cls", ")", ":", "\n", "        ", "return", "Co3dDataset", "(", "frame_annotations_file", "=", "os", ".", "path", ".", "join", "(", "dataset_root", ",", "cls", ",", "'frame_annotations.jgz'", ")", ",", "\n", "sequence_annotations_file", "=", "os", ".", "path", ".", "join", "(", "dataset_root", ",", "cls", ",", "'sequence_annotations.jgz'", ")", ",", "\n", "subset_lists_file", "=", "os", ".", "path", ".", "join", "(", "dataset_root", ",", "cls", ",", "'set_lists.json'", ")", ",", "\n", "dataset_root", "=", "dataset_root", ",", "\n", "box_crop", "=", "True", ",", "\n", "box_crop_context", "=", "0.1", ",", "\n", "image_height", "=", "224", ",", "# Doesn't resize", "\n", "image_width", "=", "224", ",", "# Doesn't resize", "\n", "load_point_clouds", "=", "True", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.applied-ai-lab_zero-shot-pose.datasets.co3d_pose_dataset.ICPCo3DPoseDataset.process_list_of_frames": [[318, 358], ["co3d_pose_dataset.co3d_rgb_to_pil", "co3d_pose_dataset.ICPCo3DPoseDataset.image_transform", "torch.Tensor", "numpy.array", "numpy.array", "zip"], "methods", ["home.repos.pwc.inspect_result.applied-ai-lab_zero-shot-pose.datasets.co3d_pose_dataset.co3d_rgb_to_pil"], ["", "def", "process_list_of_frames", "(", "self", ",", "frames", ")", ":", "\n", "\n", "        ", "\"\"\"\n        Take a list of frames and return list of:\n            Images (PIL or Tensor)\n            Scalings\n            Depth maps\n            Cameras\n        \"\"\"", "\n", "images", "=", "[", "co3d_rgb_to_pil", "(", "f", ".", "image_rgb", ")", "for", "f", "in", "frames", "]", "\n", "\n", "if", "self", ".", "image_transform", "is", "not", "None", ":", "\n", "\n", "            ", "images", "=", "[", "self", ".", "image_transform", "(", "im", ")", "for", "im", "in", "images", "]", "\n", "scalings", "=", "[", "\n", "np", ".", "array", "(", "f", ".", "image_rgb", ".", "shape", "[", "1", ":", "]", ")", "/", "np", ".", "array", "(", "im", ".", "shape", "[", "1", ":", "]", ")", "for", "im", ",", "f", "in", "zip", "(", "images", ",", "frames", ")", "\n", "]", "\n", "\n", "", "else", ":", "\n", "\n", "            ", "scalings", "=", "[", "\n", "torch", ".", "Tensor", "(", "[", "1", ",", "1", "]", ")", "for", "_", "in", "frames", "\n", "]", "\n", "\n", "# Get depth maps", "\n", "", "depth_map", "=", "[", "f", ".", "depth_map", "for", "f", "in", "frames", "]", "\n", "\n", "# Get cameras", "\n", "cameras", "=", "[", "f", ".", "camera", "for", "f", "in", "frames", "]", "\n", "\n", "# Pointclouds (just one needed per seq - same for every frame!)", "\n", "pcd", "=", "frames", "[", "0", "]", ".", "sequence_point_cloud", "\n", "\n", "# Foreground probability maps", "\n", "fg_probs", "=", "[", "f", ".", "fg_probability", "for", "f", "in", "frames", "]", "\n", "\n", "# Original image_rgb - for unprojecting single/few images to pointclouds", "\n", "image_rgbs", "=", "[", "f", ".", "image_rgb", "for", "f", "in", "frames", "]", "\n", "\n", "return", "images", ",", "depth_map", ",", "scalings", ",", "cameras", ",", "pcd", ",", "fg_probs", ",", "image_rgbs", "\n", "\n"]], "home.repos.pwc.inspect_result.applied-ai-lab_zero-shot-pose.datasets.co3d_pose_dataset.ICPCo3DPoseDataset.__getitem__": [[360, 404], ["co3d_pose_dataset.ICPCo3DPoseDataset.process_list_of_frames", "co3d_pose_dataset.ICPCo3DPoseDataset.process_list_of_frames"], "methods", ["home.repos.pwc.inspect_result.applied-ai-lab_zero-shot-pose.datasets.co3d_pose_dataset.ICPCo3DPoseDataset.process_list_of_frames", "home.repos.pwc.inspect_result.applied-ai-lab_zero-shot-pose.datasets.co3d_pose_dataset.ICPCo3DPoseDataset.process_list_of_frames"], ["", "def", "__getitem__", "(", "self", ",", "item", ")", ":", "\n", "\n", "        ", "sample", "=", "self", ".", "samples", "[", "item", "]", "\n", "cls", "=", "sample", "[", "'class'", "]", "\n", "ref_seq_name", "=", "sample", "[", "'reference_seq_name'", "]", "\n", "target_seq_name", "=", "sample", "[", "'target_seq_name'", "]", "\n", "\n", "ref_frame_id", "=", "sample", "[", "'reference_frame_id'", "]", "\n", "all_target_id", "=", "sample", "[", "'all_target_id'", "]", "\n", "\n", "base_dataset", "=", "self", ".", "all_datasets", "[", "cls", "]", "\n", "\n", "# Get frames", "\n", "ref_frame", "=", "base_dataset", "[", "ref_frame_id", "]", "\n", "all_target_frames", "=", "[", "base_dataset", "[", "idx", "]", "for", "idx", "in", "all_target_id", "]", "\n", "\n", "# Get world coordinate transforms for each seq", "\n", "ref_transform", "=", "self", ".", "seq_world_coords", "[", "cls", "]", "[", "ref_seq_name", "]", "\n", "target_transform", "=", "self", ".", "seq_world_coords", "[", "cls", "]", "[", "target_seq_name", "]", "\n", "\n", "# Process ref frame", "\n", "ref_image", ",", "ref_depth_map", ",", "ref_scaling", ",", "ref_camera", ",", "ref_pcd", ",", "ref_fgprob", ",", "ref_image_rgb", "=", "(", "\n", "x", "[", "0", "]", "for", "x", "in", "self", ".", "process_list_of_frames", "(", "[", "ref_frame", "]", ")", "\n", ")", "\n", "\n", "# Process target frames", "\n", "all_target_images", ",", "all_target_depth_maps", ",", "all_target_scalings", ",", "all_target_cameras", ",", "target_pcd", ",", "all_target_fgprobs", ",", "all_target_image_rgb", "=", "self", ".", "process_list_of_frames", "(", "\n", "all_target_frames", "\n", ")", "\n", "\n", "# Now also return additional cameras for the 0th frame from the reference and target sequences", "\n", "base_dataset", "=", "self", ".", "all_datasets", "[", "cls", "]", "\n", "ref_sequence_zero_id", "=", "self", ".", "labelled_seqs_to_frames", "[", "cls", "]", "[", "ref_seq_name", "]", "[", "0", "]", "\n", "target_sequence_zero_id", "=", "self", ".", "labelled_seqs_to_frames", "[", "cls", "]", "[", "target_seq_name", "]", "[", "0", "]", "\n", "\n", "ref_sequence_zero_camera", "=", "base_dataset", "[", "ref_sequence_zero_id", "]", ".", "camera", "\n", "target_sequence_zero_camera", "=", "base_dataset", "[", "target_sequence_zero_id", "]", ".", "camera", "\n", "\n", "return", "ref_image", ",", "ref_transform", ",", "ref_depth_map", ",", "ref_scaling", ",", "ref_camera", ",", "all_target_images", ",", "target_transform", ",", "all_target_depth_maps", ",", "all_target_scalings", ",", "all_target_cameras", ",", "ref_pcd", ",", "target_pcd", ",", "ref_fgprob", ",", "all_target_fgprobs", ",", "ref_sequence_zero_camera", ",", "target_sequence_zero_camera", ",", "ref_image_rgb", ",", "all_target_image_rgb", ",", "item", "\n", "\n"]], "home.repos.pwc.inspect_result.applied-ai-lab_zero-shot-pose.datasets.co3d_pose_dataset.PlottingCo3DPoseDataset.__init__": [[408, 411], ["co3d_pose_dataset.ICPCo3DPoseDataset.__init__"], "methods", ["home.repos.pwc.inspect_result.applied-ai-lab_zero-shot-pose.method.vision_transformer_flexible.DINOHead.__init__"], ["    ", "def", "__init__", "(", "self", ",", "*", "args", ",", "**", "kwargs", ")", ":", "\n", "\n", "        ", "super", "(", "PlottingCo3DPoseDataset", ",", "self", ")", ".", "__init__", "(", "*", "args", ",", "**", "kwargs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.applied-ai-lab_zero-shot-pose.datasets.co3d_pose_dataset.PlottingCo3DPoseDataset.get_base_dataset": [[412, 422], ["zsp.datasets.co3d_dataset_fix.Co3dDataset", "os.path.join", "os.path.join", "os.path.join"], "methods", ["None"], ["", "def", "get_base_dataset", "(", "self", ",", "dataset_root", ",", "cls", ")", ":", "\n", "        ", "return", "Co3dDataset", "(", "frame_annotations_file", "=", "os", ".", "path", ".", "join", "(", "dataset_root", ",", "cls", ",", "'frame_annotations.jgz'", ")", ",", "\n", "sequence_annotations_file", "=", "os", ".", "path", ".", "join", "(", "dataset_root", ",", "cls", ",", "'sequence_annotations.jgz'", ")", ",", "\n", "subset_lists_file", "=", "os", ".", "path", ".", "join", "(", "dataset_root", ",", "cls", ",", "'set_lists.json'", ")", ",", "\n", "dataset_root", "=", "dataset_root", ",", "\n", "box_crop", "=", "True", ",", "\n", "box_crop_context", "=", "0.1", ",", "\n", "image_height", "=", "None", ",", "# Doesn't resize", "\n", "image_width", "=", "None", ",", "# Doesn't resize", "\n", "load_point_clouds", "=", "True", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.applied-ai-lab_zero-shot-pose.datasets.co3d_pose_dataset.co3d_rgb_to_pil": [[425, 427], ["PIL.Image.fromarray", "image_rgb.permute"], "function", ["None"], ["", "", "def", "co3d_rgb_to_pil", "(", "image_rgb", ")", ":", "\n", "    ", "return", "Image", ".", "fromarray", "(", "(", "image_rgb", ".", "permute", "(", "1", ",", "2", ",", "0", ")", "*", "255", ")", ".", "numpy", "(", ")", ".", "astype", "(", "np", ".", "uint8", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.applied-ai-lab_zero-shot-pose.datasets.co3d_pose_dataset.co3d_pose_dataset_collate": [[429, 458], ["zip", "torch.stack", "torch.stack", "torch.as_tensor", "torch.as_tensor", "torch.as_tensor", "numpy.stack", "numpy.stack", "torch.as_tensor", "torch.as_tensor", "torch.stack", "numpy.stack", "numpy.stack"], "function", ["None"], ["", "def", "co3d_pose_dataset_collate", "(", "batch", ")", ":", "\n", "    ", "r\"\"\"Puts each data field into a tensor with outer dimension batch size\"\"\"", "\n", "\n", "ref_image", ",", "ref_transform", ",", "ref_depth_map", ",", "ref_scaling", ",", "ref_camera", ",", "all_target_images", ",", "target_transform", ",", "all_target_depth_maps", ",", "all_target_scalings", ",", "all_target_cameras", ",", "ref_pcd", ",", "target_pcd", ",", "item", "=", "zip", "(", "*", "batch", ")", "\n", "\n", "ref_image", "=", "torch", ".", "stack", "(", "ref_image", ")", "\n", "all_target_images", "=", "torch", ".", "stack", "(", "[", "torch", ".", "stack", "(", "x", ")", "for", "x", "in", "all_target_images", "]", ")", "\n", "ref_transform", "=", "torch", ".", "as_tensor", "(", "np", ".", "stack", "(", "ref_transform", ")", ")", "\n", "target_transform", "=", "torch", ".", "as_tensor", "(", "np", ".", "stack", "(", "target_transform", ")", ")", "\n", "\n", "idx_into_dataset", "=", "torch", ".", "as_tensor", "(", "item", ")", "\n", "\n", "ref_meta_data", "=", "{", "\n", "'depth_maps'", ":", "ref_depth_map", ",", "\n", "'scalings'", ":", "torch", ".", "as_tensor", "(", "np", ".", "stack", "(", "ref_scaling", ")", ")", ",", "\n", "'cameras'", ":", "ref_camera", ",", "\n", "'pcd'", ":", "ref_pcd", "\n", "}", "\n", "\n", "target_meta_data", "=", "{", "\n", "'depth_maps'", ":", "all_target_depth_maps", ",", "\n", "'scalings'", ":", "torch", ".", "as_tensor", "(", "np", ".", "stack", "(", "all_target_scalings", ")", ")", ",", "\n", "'cameras'", ":", "all_target_cameras", ",", "\n", "'pcd'", ":", "target_pcd", "\n", "}", "\n", "\n", "return", "(", "ref_image", ",", "all_target_images", ")", ",", "(", "ref_transform", ",", "target_transform", ")", ",", "(", "ref_meta_data", ",", "target_meta_data", ")", ",", "idx_into_dataset", "\n", "\n"]], "home.repos.pwc.inspect_result.applied-ai-lab_zero-shot-pose.datasets.co3d_pose_dataset.co3d_icp_pose_dataset_collate": [[461, 510], ["zip", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.as_tensor", "torch.as_tensor", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.as_tensor", "numpy.stack", "numpy.stack", "torch.as_tensor", "torch.as_tensor", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "numpy.stack", "numpy.stack"], "function", ["None"], ["", "def", "co3d_icp_pose_dataset_collate", "(", "batch", ")", ":", "\n", "    ", "r\"\"\"Puts each data field into a tensor with outer dimension batch size\"\"\"", "\n", "\n", "ref_image", ",", "ref_transform", ",", "ref_depth_map", ",", "ref_scaling", ",", "ref_camera", ",", "all_target_images", ",", "target_transform", ",", "all_target_depth_maps", ",", "all_target_scalings", ",", "all_target_cameras", ",", "ref_pcd", ",", "target_pcd", ",", "ref_fgprob", ",", "all_target_fgprobs", ",", "ref_sequence_zero_camera", ",", "target_sequence_zero_camera", ",", "ref_image_rgb", ",", "all_target_image_rgb", ",", "item", "=", "zip", "(", "*", "batch", ")", "\n", "\n", "# Stack images", "\n", "ref_image", "=", "torch", ".", "stack", "(", "ref_image", ")", "\n", "all_target_images", "=", "torch", ".", "stack", "(", "[", "torch", ".", "stack", "(", "x", ")", "for", "x", "in", "all_target_images", "]", ")", "\n", "# Stack fullsize, original images", "\n", "ref_image_rgb", "=", "torch", ".", "stack", "(", "ref_image_rgb", ")", "\n", "all_target_image_rgb", "=", "torch", ".", "stack", "(", "[", "torch", ".", "stack", "(", "x", ")", "for", "x", "in", "all_target_image_rgb", "]", ")", "\n", "# Stack transforms", "\n", "ref_transform", "=", "torch", ".", "as_tensor", "(", "np", ".", "stack", "(", "ref_transform", ")", ")", "\n", "target_transform", "=", "torch", ".", "as_tensor", "(", "np", ".", "stack", "(", "target_transform", ")", ")", "\n", "# Stack depth_map", "\n", "ref_depth_map", "=", "torch", ".", "stack", "(", "ref_depth_map", ")", "\n", "all_target_depth_maps", "=", "torch", ".", "stack", "(", "[", "torch", ".", "stack", "(", "x", ")", "for", "x", "in", "all_target_depth_maps", "]", ")", "\n", "# Stack fg_probability", "\n", "ref_fgprob", "=", "torch", ".", "stack", "(", "ref_fgprob", ")", "\n", "all_target_fgprobs", "=", "torch", ".", "stack", "(", "[", "torch", ".", "stack", "(", "x", ")", "for", "x", "in", "all_target_fgprobs", "]", ")", "\n", "\n", "idx_into_dataset", "=", "torch", ".", "as_tensor", "(", "item", ")", "\n", "\n", "ref_meta_data", "=", "{", "\n", "'image_rgb'", ":", "ref_image_rgb", ",", "\n", "'depth_maps'", ":", "ref_depth_map", ",", "\n", "'fg_probability'", ":", "ref_fgprob", ",", "\n", "'scalings'", ":", "torch", ".", "as_tensor", "(", "np", ".", "stack", "(", "ref_scaling", ")", ")", ",", "\n", "'cameras'", ":", "ref_camera", ",", "\n", "'pcd'", ":", "ref_pcd", ",", "\n", "'zero_camera'", ":", "ref_sequence_zero_camera", "\n", "}", "\n", "\n", "target_meta_data", "=", "{", "\n", "'image_rgb'", ":", "all_target_image_rgb", ",", "\n", "'depth_maps'", ":", "all_target_depth_maps", ",", "\n", "'fg_probability'", ":", "all_target_fgprobs", ",", "\n", "'scalings'", ":", "torch", ".", "as_tensor", "(", "np", ".", "stack", "(", "all_target_scalings", ")", ")", ",", "\n", "'cameras'", ":", "all_target_cameras", ",", "\n", "'pcd'", ":", "target_pcd", ",", "\n", "'zero_camera'", ":", "target_sequence_zero_camera", "\n", "}", "\n", "\n", "return", "(", "ref_image", ",", "all_target_images", ")", ",", "(", "ref_transform", ",", "target_transform", ")", ",", "(", "ref_meta_data", ",", "target_meta_data", ")", ",", "idx_into_dataset", "\n", "\n"]], "home.repos.pwc.inspect_result.applied-ai-lab_zero-shot-pose.datasets.co3d_pose_dataset.co3d_plotting_pose_dataset_collate": [[511, 553], ["zip", "torch.stack", "torch.stack", "torch.as_tensor", "torch.as_tensor", "torch.as_tensor", "numpy.stack", "numpy.stack", "torch.as_tensor", "torch.as_tensor", "torch.stack", "numpy.stack", "numpy.stack"], "function", ["None"], ["", "def", "co3d_plotting_pose_dataset_collate", "(", "batch", ")", ":", "\n", "    ", "r\"\"\"Puts each data field into a tensor with outer dimension batch size\"\"\"", "\n", "\n", "ref_image", ",", "ref_transform", ",", "ref_depth_map", ",", "ref_scaling", ",", "ref_camera", ",", "all_target_images", ",", "target_transform", ",", "all_target_depth_maps", ",", "all_target_scalings", ",", "all_target_cameras", ",", "ref_pcd", ",", "target_pcd", ",", "ref_fgprob", ",", "all_target_fgprobs", ",", "ref_sequence_zero_camera", ",", "target_sequence_zero_camera", ",", "ref_image_rgb", ",", "all_target_image_rgb", ",", "item", "=", "zip", "(", "*", "batch", ")", "\n", "\n", "# Stack images", "\n", "ref_image", "=", "torch", ".", "stack", "(", "ref_image", ")", "\n", "all_target_images", "=", "torch", ".", "stack", "(", "[", "torch", ".", "stack", "(", "x", ")", "for", "x", "in", "all_target_images", "]", ")", "\n", "# Stack transforms", "\n", "ref_transform", "=", "torch", ".", "as_tensor", "(", "np", ".", "stack", "(", "ref_transform", ")", ")", "\n", "target_transform", "=", "torch", ".", "as_tensor", "(", "np", ".", "stack", "(", "target_transform", ")", ")", "\n", "# NB: don't stack depth_map, fg_probability, image_rgb as these are not", "\n", "# all the same size in the PlottingCo3DPoseDataset", "\n", "\n", "idx_into_dataset", "=", "torch", ".", "as_tensor", "(", "item", ")", "\n", "\n", "ref_meta_data", "=", "{", "\n", "'image_rgb'", ":", "ref_image_rgb", ",", "\n", "'depth_maps'", ":", "ref_depth_map", ",", "\n", "'fg_probability'", ":", "ref_fgprob", ",", "\n", "'scalings'", ":", "torch", ".", "as_tensor", "(", "np", ".", "stack", "(", "ref_scaling", ")", ")", ",", "\n", "'cameras'", ":", "ref_camera", ",", "\n", "'pcd'", ":", "ref_pcd", ",", "\n", "'zero_camera'", ":", "ref_sequence_zero_camera", "\n", "}", "\n", "\n", "target_meta_data", "=", "{", "\n", "'image_rgb'", ":", "all_target_image_rgb", ",", "\n", "'depth_maps'", ":", "all_target_depth_maps", ",", "\n", "'fg_probability'", ":", "all_target_fgprobs", ",", "\n", "'scalings'", ":", "torch", ".", "as_tensor", "(", "np", ".", "stack", "(", "all_target_scalings", ")", ")", ",", "\n", "'cameras'", ":", "all_target_cameras", ",", "\n", "'pcd'", ":", "target_pcd", ",", "\n", "'zero_camera'", ":", "target_sequence_zero_camera", "\n", "}", "\n", "\n", "return", "(", "ref_image", ",", "all_target_images", ")", ",", "(", "ref_transform", ",", "target_transform", ")", ",", "(", "ref_meta_data", ",", "target_meta_data", ")", ",", "idx_into_dataset", "\n", "\n"]], "home.repos.pwc.inspect_result.applied-ai-lab_zero-shot-pose.co3d_tools.point_cloud_utils.get_rgbd_point_cloud": [[25, 66], ["pytorch3d.renderer.ray_bundle_to_ray_points", "pts_mask.reshape.reshape", "torch.nn.functional.interpolate", "torch.nn.functional.interpolate", "pytorch3d.structures.Pointclouds", "pytorch3d.renderer.ray_bundle_to_ray_points.reshape", "torch.nn.functional.interpolate.permute().reshape", "torch.nn.functional.interpolate.permute", "pytorch3d.renderer.NDCGridRaysampler"], "function", ["None"], ["def", "get_rgbd_point_cloud", "(", "\n", "camera", ":", "CamerasBase", ",", "\n", "image_rgb", ":", "torch", ".", "Tensor", ",", "\n", "depth_map", ":", "torch", ".", "Tensor", ",", "\n", "mask", ":", "Optional", "[", "torch", ".", "Tensor", "]", "=", "None", ",", "\n", "mask_thr", ":", "float", "=", "0.5", ",", "\n", ")", "->", "Pointclouds", ":", "\n", "    ", "\"\"\"\n    Given a batch of images, depths, masks and cameras, generate a colored\n    point cloud by unprojecting depth maps to the  and coloring with the source\n    pixel colors.\n    \"\"\"", "\n", "imh", ",", "imw", "=", "image_rgb", ".", "shape", "[", "2", ":", "]", "\n", "\n", "# convert the depth maps to point clouds using the grid ray sampler", "\n", "pts_3d", "=", "ray_bundle_to_ray_points", "(", "\n", "NDCGridRaysampler", "(", "\n", "image_width", "=", "imw", ",", "\n", "image_height", "=", "imh", ",", "\n", "n_pts_per_ray", "=", "1", ",", "\n", "min_depth", "=", "1.0", ",", "\n", "max_depth", "=", "1.0", ",", "\n", ")", "(", "camera", ")", ".", "_replace", "(", "lengths", "=", "depth_map", "[", ":", ",", "0", ",", "...", ",", "None", "]", ")", "\n", ")", "\n", "\n", "pts_mask", "=", "depth_map", ">", "0.0", "\n", "if", "mask", "is", "not", "None", ":", "\n", "        ", "pts_mask", "*=", "mask", ">", "mask_thr", "\n", "", "pts_mask", "=", "pts_mask", ".", "reshape", "(", "-", "1", ")", "\n", "\n", "pts_3d", "=", "pts_3d", ".", "reshape", "(", "-", "1", ",", "3", ")", "[", "pts_mask", "]", "\n", "\n", "pts_colors", "=", "torch", ".", "nn", ".", "functional", ".", "interpolate", "(", "\n", "image_rgb", ",", "\n", "size", "=", "[", "imh", ",", "imw", "]", ",", "\n", "mode", "=", "\"bilinear\"", ",", "\n", "align_corners", "=", "False", ",", "\n", ")", "\n", "pts_colors", "=", "pts_colors", ".", "permute", "(", "0", ",", "2", ",", "3", ",", "1", ")", ".", "reshape", "(", "-", "1", ",", "3", ")", "[", "pts_mask", "]", "\n", "\n", "return", "Pointclouds", "(", "points", "=", "pts_3d", "[", "None", "]", ",", "features", "=", "pts_colors", "[", "None", "]", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.applied-ai-lab_zero-shot-pose.co3d_tools.point_cloud_utils.render_point_cloud_pytorch3d": [[68, 132], ["point_cloud_utils._transform_points", "camera.clone", "torch.eye", "torch.eye", "pytorch3d.renderer.PointsRasterizer", "pytorch3d.renderer.PointsRasterizer.", "rasterizer.idx.long().permute", "weights.permute", "torch.cumprod", "torch.cumprod", "torch.cat", "torch.cat", "pytorch3d.renderer.AlphaCompositor", "_transform_points.features_packed().permute", "torch.prod", "torch.prod", "_transform_points.points_packed", "pytorch3d.renderer.PointsRasterizationSettings", "rasterizer.idx.long", "torch.ones_like", "torch.ones_like", "_transform_points.features_packed", "int", "max"], "function", ["home.repos.pwc.inspect_result.applied-ai-lab_zero-shot-pose.co3d_tools.point_cloud_utils._transform_points"], ["", "def", "render_point_cloud_pytorch3d", "(", "\n", "camera", ",", "\n", "point_cloud", ",", "\n", "render_size", ":", "Tuple", "[", "int", ",", "int", "]", ",", "\n", "point_radius", ":", "float", "=", "0.03", ",", "\n", "topk", "=", "10", ",", "\n", "eps", "=", "1e-2", ",", "\n", "bg_color", "=", "None", ",", "\n", "**", "kwargs", ",", "\n", ")", ":", "\n", "\n", "# feature dimension", "\n", "    ", "featdim", "=", "point_cloud", ".", "points_packed", "(", ")", ".", "shape", "[", "-", "1", "]", "\n", "\n", "# move to the camera coordinates; using identity cameras in the renderer", "\n", "point_cloud", "=", "_transform_points", "(", "camera", ",", "point_cloud", ",", "eps", ",", "**", "kwargs", ")", "\n", "camera_trivial", "=", "camera", ".", "clone", "(", ")", "\n", "camera_trivial", ".", "R", "[", ":", "]", "=", "torch", ".", "eye", "(", "3", ")", "\n", "camera_trivial", ".", "T", "*=", "0.0", "\n", "\n", "rasterizer", "=", "PointsRasterizer", "(", "\n", "cameras", "=", "camera_trivial", ",", "\n", "raster_settings", "=", "PointsRasterizationSettings", "(", "\n", "image_size", "=", "render_size", ",", "\n", "radius", "=", "point_radius", ",", "\n", "points_per_pixel", "=", "topk", ",", "\n", "bin_size", "=", "64", "if", "int", "(", "max", "(", "render_size", ")", ")", ">", "1024", "else", "None", ",", "\n", ")", ",", "\n", ")", "\n", "\n", "fragments", "=", "rasterizer", "(", "point_cloud", ",", "**", "kwargs", ")", "\n", "\n", "# Construct weights based on the distance of a point to the true point.", "\n", "# However, this could be done differently: e.g. predicted as opposed", "\n", "# to a function of the weights.", "\n", "r", "=", "rasterizer", ".", "raster_settings", ".", "radius", "\n", "\n", "# set up the blending weights", "\n", "dists2", "=", "fragments", ".", "dists", "\n", "weights", "=", "1", "-", "dists2", "/", "(", "r", "*", "r", ")", "\n", "ok", "=", "(", "fragments", ".", "idx", ">=", "0", ")", ".", "float", "(", ")", "\n", "\n", "weights", "=", "weights", "*", "ok", "\n", "\n", "fragments_prm", "=", "fragments", ".", "idx", ".", "long", "(", ")", ".", "permute", "(", "0", ",", "3", ",", "1", ",", "2", ")", "\n", "weights_prm", "=", "weights", ".", "permute", "(", "0", ",", "3", ",", "1", ",", "2", ")", "\n", "images", "=", "AlphaCompositor", "(", ")", "(", "\n", "fragments_prm", ",", "\n", "weights_prm", ",", "\n", "point_cloud", ".", "features_packed", "(", ")", ".", "permute", "(", "1", ",", "0", ")", ",", "\n", "background_color", "=", "bg_color", "if", "bg_color", "is", "not", "None", "else", "[", "0.0", "]", "*", "featdim", ",", "\n", "**", "kwargs", ",", "\n", ")", "\n", "\n", "# get the depths ...", "\n", "# weighted_fs[b,c,i,j] = sum_k cum_alpha_k * features[c,pointsidx[b,k,i,j]]", "\n", "# cum_alpha_k = alphas[b,k,i,j] * prod_l=0..k-1 (1 - alphas[b,l,i,j])", "\n", "cumprod", "=", "torch", ".", "cumprod", "(", "1", "-", "weights", ",", "dim", "=", "-", "1", ")", "\n", "cumprod", "=", "torch", ".", "cat", "(", "(", "torch", ".", "ones_like", "(", "cumprod", "[", "...", ",", ":", "1", "]", ")", ",", "cumprod", "[", "...", ",", ":", "-", "1", "]", ")", ",", "dim", "=", "-", "1", ")", "\n", "depths", "=", "(", "weights", "*", "cumprod", "*", "fragments", ".", "zbuf", ")", ".", "sum", "(", "dim", "=", "-", "1", ")", "\n", "# add the rendering mask", "\n", "render_mask", "=", "1.0", "-", "torch", ".", "prod", "(", "1.0", "-", "weights", ",", "dim", "=", "-", "1", ")", "\n", "\n", "return", "images", ",", "render_mask", "[", ":", ",", "None", "]", ",", "depths", "[", ":", ",", "None", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.applied-ai-lab_zero-shot-pose.co3d_tools.point_cloud_utils._signed_clamp": [[134, 138], ["x.sign", "torch.clamp", "torch.clamp", "x.abs"], "function", ["None"], ["", "def", "_signed_clamp", "(", "x", ",", "eps", ")", ":", "\n", "    ", "sign", "=", "x", ".", "sign", "(", ")", "+", "(", "x", "==", "0.0", ")", ".", "type_as", "(", "x", ")", "\n", "x_clamp", "=", "sign", "*", "torch", ".", "clamp", "(", "x", ".", "abs", "(", ")", ",", "eps", ")", "\n", "return", "x_clamp", "\n", "\n"]], "home.repos.pwc.inspect_result.applied-ai-lab_zero-shot-pose.co3d_tools.point_cloud_utils._transform_points": [[140, 151], ["point_clouds.update_padded.points_padded", "cameras.get_world_to_view_transform().transform_points", "torch.cat", "torch.cat", "point_clouds.update_padded.update_padded", "cameras.get_world_to_view_transform", "point_cloud_utils._signed_clamp"], "function", ["home.repos.pwc.inspect_result.applied-ai-lab_zero-shot-pose.co3d_tools.point_cloud_utils._signed_clamp"], ["", "def", "_transform_points", "(", "cameras", ",", "point_clouds", ",", "eps", ",", "**", "kwargs", ")", ":", "\n", "    ", "pts_world", "=", "point_clouds", ".", "points_padded", "(", ")", "\n", "pts_view", "=", "cameras", ".", "get_world_to_view_transform", "(", "**", "kwargs", ")", ".", "transform_points", "(", "\n", "pts_world", ",", "eps", "=", "eps", "\n", ")", "\n", "# it is crucial to actually clamp the points as well ...", "\n", "pts_view", "=", "torch", ".", "cat", "(", "\n", "(", "pts_view", "[", "...", ",", ":", "-", "1", "]", ",", "_signed_clamp", "(", "pts_view", "[", "...", ",", "-", "1", ":", "]", ",", "eps", ")", ")", ",", "dim", "=", "-", "1", "\n", ")", "\n", "point_clouds", "=", "point_clouds", ".", "update_padded", "(", "pts_view", ")", "\n", "return", "point_clouds", "\n", "", ""]], "home.repos.pwc.inspect_result.applied-ai-lab_zero-shot-pose.co3d_tools.metric_utils.eval_depth": [[13, 70], ["torch.clamp", "dmask.sum", "xy.mean", "torch.clamp", "xx.mean", "df.abs"], "function", ["None"], ["def", "eval_depth", "(", "\n", "pred", ":", "torch", ".", "Tensor", ",", "\n", "gt", ":", "torch", ".", "Tensor", ",", "\n", "crop", ":", "int", "=", "1", ",", "\n", "mask", ":", "Optional", "[", "torch", ".", "Tensor", "]", "=", "None", ",", "\n", "get_best_scale", ":", "bool", "=", "True", ",", "\n", "mask_thr", ":", "float", "=", "0.5", ",", "\n", ")", "->", "Tuple", "[", "torch", ".", "Tensor", ",", "torch", ".", "Tensor", "]", ":", "\n", "    ", "\"\"\"\n    Evaluate the depth error between the prediction `pred` and the ground\n    truth `gt`.\n\n    Args:\n        pred: A tensor of shape (N, 1, H, W) denoting the predicted depth maps.\n        gt: A tensor of shape (N, 1, H, W) denoting the ground truth depth maps.\n        crop: The number of pixels to crop from the border.\n        mask: A mask denoting the valid regions of the gt depth.\n        get_best_scale: If `True`, estimates a scaling factor of the predicted depth\n            that yields the best mean squared error between `pred` and `gt`.\n            This is typically enabled for cases where predicted reconstructions\n            are inherently defined up to an arbitrary scaling factor.\n        mask_thr: A constant used to threshold the `mask` to specify the valid\n            regions.\n\n    Returns:\n        mse_depth: Mean squared error between `pred` and `gt`.\n        abs_depth: Mean absolute difference between `pred` and `gt`.\n    \"\"\"", "\n", "\n", "# chuck out the border", "\n", "if", "crop", ">", "0", ":", "\n", "        ", "gt", "=", "gt", "[", ":", ",", ":", ",", "crop", ":", "-", "crop", ",", "crop", ":", "-", "crop", "]", "\n", "pred", "=", "pred", "[", ":", ",", ":", ",", "crop", ":", "-", "crop", ",", "crop", ":", "-", "crop", "]", "\n", "\n", "", "if", "mask", "is", "not", "None", ":", "\n", "# mult gt by mask", "\n", "        ", "if", "crop", ">", "0", ":", "\n", "            ", "mask", "=", "mask", "[", ":", ",", ":", ",", "crop", ":", "-", "crop", ",", "crop", ":", "-", "crop", "]", "\n", "", "gt", "=", "gt", "*", "(", "mask", ">", "mask_thr", ")", ".", "float", "(", ")", "\n", "\n", "", "dmask", "=", "(", "gt", ">", "0.0", ")", ".", "float", "(", ")", "\n", "dmask_mass", "=", "torch", ".", "clamp", "(", "dmask", ".", "sum", "(", "(", "1", ",", "2", ",", "3", ")", ")", ",", "1e-4", ")", "\n", "\n", "if", "get_best_scale", ":", "\n", "# mult preds by a scalar \"scale_best\"", "\n", "# \ts.t. we get best possible mse error", "\n", "        ", "xy", "=", "pred", "*", "gt", "*", "dmask", "\n", "xx", "=", "pred", "*", "pred", "*", "dmask", "\n", "scale_best", "=", "xy", ".", "mean", "(", "(", "1", ",", "2", ",", "3", ")", ")", "/", "torch", ".", "clamp", "(", "xx", ".", "mean", "(", "(", "1", ",", "2", ",", "3", ")", ")", ",", "1e-4", ")", "\n", "pred", "=", "pred", "*", "scale_best", "[", ":", ",", "None", ",", "None", ",", "None", "]", "\n", "\n", "", "df", "=", "gt", "-", "pred", "\n", "\n", "mse_depth", "=", "(", "dmask", "*", "(", "df", "**", "2", ")", ")", ".", "sum", "(", "(", "1", ",", "2", ",", "3", ")", ")", "/", "dmask_mass", "\n", "abs_depth", "=", "(", "dmask", "*", "df", ".", "abs", "(", ")", ")", ".", "sum", "(", "(", "1", ",", "2", ",", "3", ")", ")", "/", "dmask_mass", "\n", "\n", "return", "mse_depth", ",", "abs_depth", "\n", "\n"]], "home.repos.pwc.inspect_result.applied-ai-lab_zero-shot-pose.co3d_tools.metric_utils.calc_psnr": [[72, 83], ["metric_utils.calc_mse", "torch.log10"], "function", ["home.repos.pwc.inspect_result.applied-ai-lab_zero-shot-pose.co3d_tools.metric_utils.calc_mse"], ["", "def", "calc_psnr", "(", "\n", "x", ":", "torch", ".", "Tensor", ",", "\n", "y", ":", "torch", ".", "Tensor", ",", "\n", "mask", ":", "Optional", "[", "torch", ".", "Tensor", "]", "=", "None", ",", "\n", ")", "->", "torch", ".", "Tensor", ":", "\n", "    ", "\"\"\"\n    Calculates the Peak-signal-to-noise ratio between tensors `x` and `y`.\n    \"\"\"", "\n", "mse", "=", "calc_mse", "(", "x", ",", "y", ",", "mask", "=", "mask", ")", "\n", "psnr", "=", "-", "10.0", "*", "torch", ".", "log10", "(", "mse", ")", "\n", "return", "psnr", "\n", "\n"]], "home.repos.pwc.inspect_result.applied-ai-lab_zero-shot-pose.co3d_tools.metric_utils.calc_mse": [[85, 97], ["torch.mean", "mask.expand_as().sum().clamp", "mask.expand_as().sum", "mask.expand_as"], "function", ["None"], ["", "def", "calc_mse", "(", "\n", "x", ":", "torch", ".", "Tensor", ",", "\n", "y", ":", "torch", ".", "Tensor", ",", "\n", "mask", ":", "Optional", "[", "torch", ".", "Tensor", "]", "=", "None", ",", "\n", ")", "->", "torch", ".", "Tensor", ":", "\n", "    ", "\"\"\"\n    Calculates the mean square error between tensors `x` and `y`.\n    \"\"\"", "\n", "if", "mask", "is", "None", ":", "\n", "        ", "return", "torch", ".", "mean", "(", "(", "x", "-", "y", ")", "**", "2", ")", "\n", "", "else", ":", "\n", "        ", "return", "(", "(", "(", "x", "-", "y", ")", "**", "2", ")", "*", "mask", ")", ".", "sum", "(", ")", "/", "mask", ".", "expand_as", "(", "x", ")", ".", "sum", "(", ")", ".", "clamp", "(", "1e-5", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.applied-ai-lab_zero-shot-pose.co3d_tools.metric_utils.calc_bce": [[99, 121], ["torch.nn.functional.binary_cross_entropy", "torch.clamp", "torch.ones_like", "mask.sum().clamp", "torch.ones_like.numel", "torch.ones_like.sum().clamp", "mask.sum", "torch.ones_like.sum"], "function", ["None"], ["", "", "def", "calc_bce", "(", "\n", "pred", ":", "torch", ".", "Tensor", ",", "\n", "gt", ":", "torch", ".", "Tensor", ",", "\n", "equal_w", ":", "bool", "=", "True", ",", "\n", "pred_eps", ":", "float", "=", "0.01", ",", "\n", ")", "->", "torch", ".", "Tensor", ":", "\n", "    ", "\"\"\"\n    Calculates the binary cross entropy.\n    \"\"\"", "\n", "if", "pred_eps", ">", "0.0", ":", "\n", "# up/low bound the predictions", "\n", "        ", "pred", "=", "torch", ".", "clamp", "(", "pred", ",", "pred_eps", ",", "1.0", "-", "pred_eps", ")", "\n", "\n", "", "if", "equal_w", ":", "\n", "        ", "mask", "=", "(", "gt", ">", "0.5", ")", ".", "float", "(", ")", "\n", "weight", "=", "mask", "/", "mask", ".", "sum", "(", ")", ".", "clamp", "(", "1.0", ")", "+", "(", "1", "-", "mask", ")", "/", "(", "1", "-", "mask", ")", ".", "sum", "(", ")", ".", "clamp", "(", "1.0", ")", "\n", "# weight sum should be at this point ~2", "\n", "weight", "=", "weight", "*", "(", "weight", ".", "numel", "(", ")", "/", "weight", ".", "sum", "(", ")", ".", "clamp", "(", "1.0", ")", ")", "\n", "", "else", ":", "\n", "        ", "weight", "=", "torch", ".", "ones_like", "(", "gt", ")", "\n", "\n", "", "return", "F", ".", "binary_cross_entropy", "(", "pred", ",", "gt", ",", "reduction", "=", "\"mean\"", ",", "weight", "=", "weight", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.applied-ai-lab_zero-shot-pose.co3d_tools.metric_utils.rgb_l1": [[123, 135], ["torch.ones_like", "torch.ones_like.sum().clamp", "torch.ones_like.sum"], "function", ["None"], ["", "def", "rgb_l1", "(", "\n", "pred", ":", "torch", ".", "Tensor", ",", "target", ":", "torch", ".", "Tensor", ",", "mask", ":", "Optional", "[", "torch", ".", "Tensor", "]", "=", "None", "\n", ")", "->", "torch", ".", "Tensor", ":", "\n", "    ", "\"\"\"\n    Calculates the mean absolute error between the predicted colors `pred`\n    and ground truth colors `target`.\n    \"\"\"", "\n", "if", "mask", "is", "None", ":", "\n", "        ", "mask", "=", "torch", ".", "ones_like", "(", "pred", "[", ":", ",", ":", "1", "]", ")", "\n", "", "return", "(", "(", "pred", "-", "target", ")", ".", "abs", "(", ")", "*", "mask", ")", ".", "sum", "(", "dim", "=", "(", "1", ",", "2", ",", "3", ")", ")", "/", "mask", ".", "sum", "(", "\n", "dim", "=", "(", "1", ",", "2", ",", "3", ")", "\n", ")", ".", "clamp", "(", "1", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.applied-ai-lab_zero-shot-pose.co3d_tools.metric_utils.huber": [[137, 145], ["metric_utils.safe_sqrt"], "function", ["home.repos.pwc.inspect_result.applied-ai-lab_zero-shot-pose.co3d_tools.metric_utils.safe_sqrt"], ["", "def", "huber", "(", "dfsq", ":", "torch", ".", "Tensor", ",", "scaling", ":", "float", "=", "0.03", ")", "->", "torch", ".", "Tensor", ":", "\n", "    ", "\"\"\"\n    Calculates the huber function of the input squared error `dfsq`.\n    The function smoothly transitions from a region with unit gradient\n    to a hyperbolic function at `dfsq=scaling`.\n    \"\"\"", "\n", "loss", "=", "(", "safe_sqrt", "(", "1", "+", "dfsq", "/", "(", "scaling", "*", "scaling", ")", ",", "eps", "=", "1e-4", ")", "-", "1", ")", "*", "scaling", "\n", "return", "loss", "\n", "\n"]], "home.repos.pwc.inspect_result.applied-ai-lab_zero-shot-pose.co3d_tools.metric_utils.neg_iou_loss": [[147, 157], ["metric_utils.iou"], "function", ["home.repos.pwc.inspect_result.applied-ai-lab_zero-shot-pose.co3d_tools.metric_utils.iou"], ["", "def", "neg_iou_loss", "(", "\n", "predict", ":", "torch", ".", "Tensor", ",", "\n", "target", ":", "torch", ".", "Tensor", ",", "\n", "mask", ":", "Optional", "[", "torch", ".", "Tensor", "]", "=", "None", ",", "\n", ")", "->", "torch", ".", "Tensor", ":", "\n", "    ", "\"\"\"\n    This is a great loss because it emphasizes on the active\n    regions of the predict and targets\n    \"\"\"", "\n", "return", "1.0", "-", "iou", "(", "predict", ",", "target", ",", "mask", "=", "mask", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.applied-ai-lab_zero-shot-pose.co3d_tools.metric_utils.safe_sqrt": [[159, 164], ["float", "torch.clamp", "float"], "function", ["None"], ["", "def", "safe_sqrt", "(", "A", ":", "torch", ".", "Tensor", ",", "eps", ":", "float", "=", "float", "(", "1e-4", ")", ")", "->", "torch", ".", "Tensor", ":", "\n", "    ", "\"\"\"\n    performs safe differentiable sqrt\n    \"\"\"", "\n", "return", "(", "torch", ".", "clamp", "(", "A", ",", "float", "(", "0", ")", ")", "+", "eps", ")", ".", "sqrt", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.applied-ai-lab_zero-shot-pose.co3d_tools.metric_utils.iou": [[166, 182], ["tuple", "intersect.numel", "range", "predict.dim"], "function", ["None"], ["", "def", "iou", "(", "\n", "predict", ":", "torch", ".", "Tensor", ",", "\n", "target", ":", "torch", ".", "Tensor", ",", "\n", "mask", ":", "Optional", "[", "torch", ".", "Tensor", "]", "=", "None", ",", "\n", ")", "->", "torch", ".", "Tensor", ":", "\n", "    ", "\"\"\"\n    This is a great loss because it emphasizes on the active\n    regions of the predict and targets\n    \"\"\"", "\n", "dims", "=", "tuple", "(", "range", "(", "predict", ".", "dim", "(", ")", ")", "[", "1", ":", "]", ")", "\n", "if", "mask", "is", "not", "None", ":", "\n", "        ", "predict", "=", "predict", "*", "mask", "\n", "target", "=", "target", "*", "mask", "\n", "", "intersect", "=", "(", "predict", "*", "target", ")", ".", "sum", "(", "dims", ")", "\n", "union", "=", "(", "predict", "+", "target", "-", "predict", "*", "target", ")", ".", "sum", "(", "dims", ")", "+", "1e-4", "\n", "return", "(", "intersect", "/", "union", ")", ".", "sum", "(", ")", "/", "intersect", ".", "numel", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.applied-ai-lab_zero-shot-pose.co3d_tools.metric_utils.beta_prior": [[184, 190], ["ValueError", "math.log", "math.log", "torch.log", "torch.log"], "function", ["None"], ["", "def", "beta_prior", "(", "pred", ":", "torch", ".", "Tensor", ",", "cap", ":", "float", "=", "0.1", ")", "->", "torch", ".", "Tensor", ":", "\n", "    ", "if", "cap", "<=", "0.0", ":", "\n", "        ", "raise", "ValueError", "(", "\"capping should be positive to avoid unbound loss\"", ")", "\n", "\n", "", "min_value", "=", "math", ".", "log", "(", "cap", ")", "+", "math", ".", "log", "(", "cap", "+", "1.0", ")", "\n", "return", "(", "torch", ".", "log", "(", "pred", "+", "cap", ")", "+", "torch", ".", "log", "(", "1.0", "-", "pred", "+", "cap", ")", ")", ".", "mean", "(", ")", "-", "min_value", "\n", "", ""]], "home.repos.pwc.inspect_result.applied-ai-lab_zero-shot-pose.co3d_tools.camera_utils.select_cameras": [[21, 44], ["isinstance", "pytorch3d.renderer.PerspectiveCameras", "isinstance", "ValueError", "max", "len", "ValueError", "max", "getattr", "hasattr", "getattr"], "function", ["None"], ["def", "select_cameras", "(", "cameras", ":", "CamerasBase", ",", "idx", ":", "Union", "[", "int", ",", "List", "[", "int", "]", ",", "torch", ".", "LongTensor", "]", ")", ":", "\n", "    ", "\"\"\"\n    Make a new batch of cameras by indexing into the input PyTorch3D\n    camera batch `cameras`.\n    \"\"\"", "\n", "if", "not", "isinstance", "(", "cameras", ",", "pt3d", ".", "renderer", ".", "PerspectiveCameras", ")", ":", "\n", "        ", "raise", "ValueError", "(", "\"select_cameras works only for PerspectiveCameras!\"", ")", "\n", "\n", "", "if", "isinstance", "(", "idx", ",", "int", ")", ":", "\n", "        ", "idx", "=", "[", "idx", "]", "\n", "\n", "", "if", "max", "(", "idx", ")", ">=", "len", "(", "cameras", ")", ":", "\n", "        ", "raise", "ValueError", "(", "f\"Index {max(idx)} is out of bounds for select cameras\"", ")", "\n", "\n", "", "cameras", "=", "pt3d", ".", "renderer", ".", "PerspectiveCameras", "(", "\n", "**", "{", "\n", "k", ":", "getattr", "(", "cameras", ",", "k", ")", "[", "idx", "]", "\n", "for", "k", "in", "(", "\"focal_length\"", ",", "\"principal_point\"", ",", "\"R\"", ",", "\"T\"", ",", "\"K\"", ")", "\n", "if", "(", "hasattr", "(", "cameras", ",", "k", ")", "and", "(", "getattr", "(", "cameras", ",", "k", ")", "is", "not", "None", ")", ")", "\n", "}", ",", "\n", "device", "=", "cameras", ".", "device", ",", "\n", ")", "\n", "return", "cameras", "\n", "\n"]], "home.repos.pwc.inspect_result.applied-ai-lab_zero-shot-pose.co3d_tools.camera_utils.concatenate_cameras": [[46, 64], ["pytorch3d.renderer.PerspectiveCameras", "isinstance", "torch.cat", "all", "getattr", "hasattr", "getattr"], "function", ["None"], ["", "def", "concatenate_cameras", "(", "cameras_list", ":", "List", "[", "CamerasBase", "]", ")", ":", "\n", "    ", "\"\"\"\n    Make a new batch of cameras by concatenating a list of input\n    PyTorch3D camera batches `cameras_list`.\n    \"\"\"", "\n", "for", "c", "in", "cameras_list", ":", "\n", "        ", "assert", "isinstance", "(", "\n", "c", ",", "pt3d", ".", "renderer", ".", "PerspectiveCameras", "\n", ")", ",", "\"This only works for PerspectiveCameras!\"", "\n", "", "cameras_cat", "=", "pt3d", ".", "renderer", ".", "PerspectiveCameras", "(", "\n", "**", "{", "\n", "k", ":", "torch", ".", "cat", "(", "[", "getattr", "(", "c", ",", "k", ")", "for", "c", "in", "cameras_list", "]", ",", "dim", "=", "0", ")", "\n", "for", "k", "in", "(", "\"focal_length\"", ",", "\"principal_point\"", ",", "\"R\"", ",", "\"T\"", ",", "\"K\"", ")", "\n", "if", "all", "(", "hasattr", "(", "c", ",", "k", ")", "and", "(", "getattr", "(", "c", ",", "k", ")", "is", "not", "None", ")", "for", "c", "in", "cameras_list", ")", "\n", "}", ",", "\n", "device", "=", "cameras_list", "[", "0", "]", ".", "device", ",", "\n", ")", "\n", "return", "cameras_cat", "\n", "\n"]], "home.repos.pwc.inspect_result.applied-ai-lab_zero-shot-pose.co3d_tools.camera_utils.jitter_extrinsics": [[66, 92], ["all", "pytorch3d.transforms.random_rotations", "pytorch3d.transforms.so3_exponential_map", "pytorch3d.ops.eyes", "pt3d.transforms.so3_exponential_map.expand", "T_jit.expand", "torch.exp().expand", "camera_utils.apply_camera_alignment", "torch.randn_like", "pytorch3d.transforms.so3_log_map", "torch.exp", "torch.randn_like"], "function", ["home.repos.pwc.inspect_result.applied-ai-lab_zero-shot-pose.co3d_tools.camera_utils.apply_camera_alignment"], ["", "def", "jitter_extrinsics", "(", "\n", "R", ":", "torch", ".", "Tensor", ",", "\n", "T", ":", "torch", ".", "Tensor", ",", "\n", "max_angle", "=", "(", "math", ".", "pi", "*", "2.0", ")", ",", "\n", "translation_std", "=", "1.0", ",", "\n", "scale_std", "=", "0.3", ",", "\n", ")", ":", "\n", "    ", "\"\"\"\n    Jitter the extrinsic camera parameters `R` and `T` with a random similarity\n    transformation. The transformation rotates by a random angle between [0, max_angle];\n    scales by a random factor exp(N(0, scale_std)), where N(0, scale_std) is\n    a random sample from a normal distrubtion with zero mean and variance scale_std;\n    and translates by a 3D offset sampled from N(0, translation_std).\n    \"\"\"", "\n", "assert", "all", "(", "x", ">=", "0.0", "for", "x", "in", "(", "max_angle", ",", "translation_std", ",", "scale_std", ")", ")", "\n", "N", "=", "R", ".", "shape", "[", "0", "]", "\n", "R_jit", "=", "pt3d", ".", "transforms", ".", "random_rotations", "(", "1", ",", "device", "=", "R", ".", "device", ")", "\n", "R_jit", "=", "pt3d", ".", "transforms", ".", "so3_exponential_map", "(", "\n", "pt3d", ".", "transforms", ".", "so3_log_map", "(", "R_jit", ")", "*", "max_angle", "\n", ")", "\n", "T_jit", "=", "torch", ".", "randn_like", "(", "R_jit", "[", ":", "1", ",", ":", ",", "0", "]", ")", "*", "translation_std", "\n", "rigid_transform", "=", "pt3d", ".", "ops", ".", "eyes", "(", "dim", "=", "4", ",", "N", "=", "N", ",", "device", "=", "R", ".", "device", ")", "\n", "rigid_transform", "[", ":", ",", ":", "3", ",", ":", "3", "]", "=", "R_jit", ".", "expand", "(", "N", ",", "3", ",", "3", ")", "\n", "rigid_transform", "[", ":", ",", "3", ",", ":", "3", "]", "=", "T_jit", ".", "expand", "(", "N", ",", "3", ")", "\n", "scale_jit", "=", "torch", ".", "exp", "(", "torch", ".", "randn_like", "(", "T_jit", "[", ":", ",", "0", "]", ")", "*", "scale_std", ")", ".", "expand", "(", "N", ")", "\n", "return", "apply_camera_alignment", "(", "R", ",", "T", ",", "rigid_transform", ",", "scale_jit", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.applied-ai-lab_zero-shot-pose.co3d_tools.camera_utils.apply_camera_alignment": [[94, 118], ["R_rigid.permute().bmm", "R_rigid.permute"], "function", ["None"], ["", "def", "apply_camera_alignment", "(", "\n", "R", ":", "torch", ".", "Tensor", ",", "\n", "T", ":", "torch", ".", "Tensor", ",", "\n", "rigid_transform", ":", "torch", ".", "Tensor", ",", "\n", "scale", ":", "torch", ".", "Tensor", ",", "\n", ")", ":", "\n", "    ", "\"\"\"\n    Args:\n        R: Camera rotation matrix of shape (N, 3, 3).\n        T: Camera translation  of shape (N, 3).\n        rigid_transform: A tensor of shape (N, 4, 4) representing a batch of\n            N 4x4 tensors that map the scene pointcloud from misaligned coords\n            to the aligned space.\n        scale: A list of N scaling factors. A tensor of shape (N,)\n\n    Returns:\n        R_aligned: The aligned rotations R.\n        T_aligned: The aligned translations T.\n    \"\"\"", "\n", "R_rigid", "=", "rigid_transform", "[", ":", ",", ":", "3", ",", ":", "3", "]", "\n", "T_rigid", "=", "rigid_transform", "[", ":", ",", "3", ":", ",", ":", "3", "]", "\n", "R_aligned", "=", "R_rigid", ".", "permute", "(", "0", ",", "2", ",", "1", ")", ".", "bmm", "(", "R", ")", "\n", "T_aligned", "=", "scale", "[", ":", ",", "None", "]", "*", "(", "T", "-", "(", "T_rigid", "@", "R_aligned", ")", "[", ":", ",", "0", "]", ")", "\n", "return", "R_aligned", ",", "T_aligned", "\n", "\n"]], "home.repos.pwc.inspect_result.applied-ai-lab_zero-shot-pose.co3d_tools.camera_utils.get_min_max_depth_bounds": [[120, 137], ["cameras.get_camera_center", "center_dist.clamp.clamp", "scene_center.to"], "function", ["home.repos.pwc.inspect_result.applied-ai-lab_zero-shot-pose.datasets.co3d_dataset.FrameData.to"], ["", "def", "get_min_max_depth_bounds", "(", "cameras", ",", "scene_center", ",", "scene_extent", ")", ":", "\n", "    ", "\"\"\"\n    Estimate near/far depth plane as:\n    near = dist(cam_center, self.scene_center) - self.scene_extent\n    far  = dist(cam_center, self.scene_center) + self.scene_extent\n    \"\"\"", "\n", "cam_center", "=", "cameras", ".", "get_camera_center", "(", ")", "\n", "center_dist", "=", "(", "\n", "(", "(", "cam_center", "-", "scene_center", ".", "to", "(", "cameras", ".", "R", ")", "[", "None", "]", ")", "**", "2", ")", "\n", ".", "sum", "(", "dim", "=", "-", "1", ")", "\n", ".", "clamp", "(", "0.001", ")", "\n", ".", "sqrt", "(", ")", "\n", ")", "\n", "center_dist", "=", "center_dist", ".", "clamp", "(", "scene_extent", "+", "1e-3", ")", "\n", "min_depth", "=", "center_dist", "-", "scene_extent", "\n", "max_depth", "=", "center_dist", "+", "scene_extent", "\n", "return", "min_depth", ",", "max_depth", "\n", "\n"]], "home.repos.pwc.inspect_result.applied-ai-lab_zero-shot-pose.co3d_tools.camera_utils.volumetric_camera_overlaps": [[139, 191], ["int", "pytorch3d.structures.Volumes().get_coord_grid", "grid.view().expand.view().expand", "cameras.transform_points", "torch.diag", "torch.prod", "torch.nn.functional.normalize", "rays_masked.view.view", "pytorch3d.structures.Volumes", "grid.view().expand.view", "proj_in_camera.t", "gridp[].abs", "rays_masked.view.t", "torch.zeros", "cameras.get_camera_center", "[].to", "torch.FloatTensor"], "function", ["home.repos.pwc.inspect_result.applied-ai-lab_zero-shot-pose.datasets.co3d_dataset.FrameData.to"], ["", "def", "volumetric_camera_overlaps", "(", "\n", "cameras", ":", "CamerasBase", ",", "\n", "scene_extent", ":", "float", "=", "8.0", ",", "\n", "scene_center", ":", "Tuple", "[", "float", ",", "float", ",", "float", "]", "=", "[", "0.0", ",", "0.0", ",", "0.0", "]", ",", "\n", "resol", ":", "int", "=", "16", ",", "\n", "weigh_by_ray_angle", ":", "bool", "=", "True", ",", "\n", ")", ":", "\n", "    ", "\"\"\"\n    Compute the overlaps between viewing frustrums of all pairs of cameras\n    in `cameras`.\n    \"\"\"", "\n", "device", "=", "cameras", ".", "device", "\n", "ba", "=", "cameras", ".", "R", ".", "shape", "[", "0", "]", "\n", "n_vox", "=", "int", "(", "resol", "**", "3", ")", "\n", "grid", "=", "pt3d", ".", "structures", ".", "Volumes", "(", "\n", "densities", "=", "torch", ".", "zeros", "(", "[", "1", ",", "1", ",", "resol", ",", "resol", ",", "resol", "]", ",", "device", "=", "device", ")", ",", "\n", "volume_translation", "=", "-", "torch", ".", "FloatTensor", "(", "scene_center", ")", "[", "None", "]", ".", "to", "(", "device", ")", ",", "\n", "voxel_size", "=", "2.0", "*", "scene_extent", "/", "resol", ",", "\n", ")", ".", "get_coord_grid", "(", "world_coordinates", "=", "True", ")", "\n", "\n", "grid", "=", "grid", ".", "view", "(", "1", ",", "n_vox", ",", "3", ")", ".", "expand", "(", "ba", ",", "n_vox", ",", "3", ")", "\n", "gridp", "=", "cameras", ".", "transform_points", "(", "grid", ",", "eps", "=", "1e-2", ")", "\n", "proj_in_camera", "=", "(", "\n", "torch", ".", "prod", "(", "(", "gridp", "[", "...", ",", ":", "2", "]", ".", "abs", "(", ")", "<=", "1.0", ")", ",", "dim", "=", "-", "1", ")", "\n", "*", "(", "gridp", "[", "...", ",", "2", "]", ">", "0.0", ")", ".", "float", "(", ")", "\n", ")", "# ba x n_vox", "\n", "\n", "if", "weigh_by_ray_angle", ":", "\n", "        ", "rays", "=", "torch", ".", "nn", ".", "functional", ".", "normalize", "(", "\n", "grid", "-", "cameras", ".", "get_camera_center", "(", ")", "[", ":", ",", "None", "]", ",", "dim", "=", "-", "1", "\n", ")", "\n", "rays_masked", "=", "rays", "*", "proj_in_camera", "[", "...", ",", "None", "]", "\n", "\n", "# - slow and readable:", "\n", "# inter = torch.zeros(ba, ba)", "\n", "# for i1 in range(ba):", "\n", "#     for i2 in range(ba):", "\n", "#         inter[i1, i2] = (", "\n", "#             1 + (rays_masked[i1] * rays_masked[i2]", "\n", "#         ).sum(dim=-1)).sum()", "\n", "\n", "# - fast:", "\n", "rays_masked", "=", "rays_masked", ".", "view", "(", "ba", ",", "n_vox", "*", "3", ")", "\n", "inter", "=", "n_vox", "+", "(", "rays_masked", "@", "rays_masked", ".", "t", "(", ")", ")", "\n", "\n", "", "else", ":", "\n", "        ", "inter", "=", "proj_in_camera", "@", "proj_in_camera", ".", "t", "(", ")", "\n", "\n", "", "mass", "=", "torch", ".", "diag", "(", "inter", ")", "\n", "iou", "=", "inter", "/", "(", "mass", "[", ":", ",", "None", "]", "+", "mass", "[", "None", ",", ":", "]", "-", "inter", ")", ".", "clamp", "(", "0.1", ")", "\n", "\n", "return", "iou", "\n", "\n"]], "home.repos.pwc.inspect_result.applied-ai-lab_zero-shot-pose.co3d_tools.camera_utils.pytorch3d_has_old_ndc_convention": [[193, 207], ["pytorch3d.renderer.NDCGridRaysampler", "pytorch3d.renderer.PerspectiveCameras", "test_ray_bundle.xys.abs"], "function", ["None"], ["", "def", "pytorch3d_has_old_ndc_convention", "(", ")", "->", "bool", ":", "\n", "    ", "test_ray_bundle", "=", "pt3d", ".", "renderer", ".", "NDCGridRaysampler", "(", "\n", "image_width", "=", "4", ",", "\n", "image_height", "=", "2", ",", "\n", "n_pts_per_ray", "=", "1", ",", "\n", "min_depth", "=", "1.0", ",", "\n", "max_depth", "=", "1.0", ",", "\n", ")", "(", "pt3d", ".", "renderer", ".", "PerspectiveCameras", "(", "focal_length", "=", "[", "1.0", "]", ")", ")", "\n", "xy_range_above_1", "=", "(", "test_ray_bundle", ".", "xys", ".", "abs", "(", ")", ">", "1.001", ")", ".", "any", "(", ")", "\n", "if", "xy_range_above_1", ":", "\n", "# the new ndc convention has to contain xys > 1", "\n", "        ", "return", "False", "\n", "", "else", ":", "\n", "        ", "return", "True", "\n", "\n"]], "home.repos.pwc.inspect_result.applied-ai-lab_zero-shot-pose.co3d_tools.camera_utils.assert_pytorch3d_has_new_ndc_convention": [[209, 213], ["camera_utils.pytorch3d_has_old_ndc_convention", "EnvironmentError"], "function", ["home.repos.pwc.inspect_result.applied-ai-lab_zero-shot-pose.co3d_tools.camera_utils.pytorch3d_has_old_ndc_convention"], ["", "", "def", "assert_pytorch3d_has_new_ndc_convention", "(", ")", ":", "\n", "    ", "if", "pytorch3d_has_old_ndc_convention", "(", ")", ":", "\n", "        ", "raise", "EnvironmentError", "(", "\n", "\"This codebase uses the new Pytorch3D NDC convention.\"", "\n", "\" Please update Pytorch3D to the very latest version.\"", "\n"]], "home.repos.pwc.inspect_result.applied-ai-lab_zero-shot-pose.co3d_tools.vis_utils.make_depth_image": [[11, 42], ["zip", "torch.stack", "[].view", "max", "max", "torch.stack.append", "normfacs[].view", "normfacs[].view", "ok.sum", "print", "torch.stack.append", "int", "int", "torch.stack", "d.view", "m.view", "torch.zeros().type_as", "round", "round", "[].view.topk", "[].view.topk", "masks.float", "d.view", "torch.zeros", "[].view.numel", "[].view.numel"], "function", ["None"], ["def", "make_depth_image", "(", "\n", "depths", ",", "\n", "masks", ",", "\n", "max_quantile", "=", "0.98", ",", "\n", "min_quantile", "=", "0.02", ",", "\n", "min_out_depth", "=", "0.1", ",", "\n", "max_out_depth", "=", "0.9", ",", "\n", ")", ":", "\n", "    ", "normfacs", "=", "[", "]", "\n", "for", "d", ",", "m", "in", "zip", "(", "depths", ",", "masks", ")", ":", "\n", "        ", "ok", "=", "(", "d", ".", "view", "(", "-", "1", ")", ">", "1e-6", ")", "*", "(", "m", ".", "view", "(", "-", "1", ")", ">", "0.5", ")", "\n", "if", "ok", ".", "sum", "(", ")", "<=", "1", ":", "\n", "            ", "print", "(", "'empty depth!'", ")", "\n", "normfacs", ".", "append", "(", "torch", ".", "zeros", "(", "2", ")", ".", "type_as", "(", "depths", ")", ")", "\n", "continue", "\n", "", "dok", "=", "d", ".", "view", "(", "-", "1", ")", "[", "ok", "]", ".", "view", "(", "-", "1", ")", "\n", "_maxk", "=", "max", "(", "int", "(", "round", "(", "(", "1", "-", "max_quantile", ")", "*", "(", "dok", ".", "numel", "(", ")", ")", ")", ")", ",", "1", ")", "\n", "_mink", "=", "max", "(", "int", "(", "round", "(", "min_quantile", "*", "(", "dok", ".", "numel", "(", ")", ")", ")", ")", ",", "1", ")", "\n", "normfac_max", "=", "dok", ".", "topk", "(", "k", "=", "_maxk", ",", "dim", "=", "-", "1", ")", ".", "values", "[", "-", "1", "]", "\n", "normfac_min", "=", "dok", ".", "topk", "(", "k", "=", "_mink", ",", "dim", "=", "-", "1", ",", "largest", "=", "False", ")", ".", "values", "[", "-", "1", "]", "\n", "normfacs", ".", "append", "(", "torch", ".", "stack", "(", "[", "normfac_min", ",", "normfac_max", "]", ")", ")", "\n", "", "normfacs", "=", "torch", ".", "stack", "(", "normfacs", ")", "\n", "_min", ",", "_max", "=", "(", "\n", "normfacs", "[", ":", ",", "0", "]", ".", "view", "(", "-", "1", ",", "1", ",", "1", ",", "1", ")", ",", "normfacs", "[", ":", ",", "1", "]", ".", "view", "(", "-", "1", ",", "1", ",", "1", ",", "1", ")", "\n", ")", "\n", "depths", "=", "(", "depths", "-", "_min", ")", "/", "(", "_max", "-", "_min", ")", ".", "clamp", "(", "1e-4", ")", "\n", "depths", "=", "(", "\n", "(", "depths", "*", "(", "max_out_depth", "-", "min_out_depth", ")", "+", "min_out_depth", ")", "\n", "*", "masks", ".", "float", "(", ")", "\n", ")", ".", "clamp", "(", "0.0", ",", "1.0", ")", "\n", "return", "depths", "\n", "", ""]], "home.repos.pwc.inspect_result.applied-ai-lab_zero-shot-pose.co3d_tools.image_utils.mask_background": [[12, 46], ["torch.is_tensor", "mask_fg.type_as.type_as", "bg_color.view().clone().to", "isinstance", "torch.tensor().view", "isinstance", "bg_color.view().clone", "ValueError", "torch.tensor", "image_rgb.new_ones", "image_utils._invalid_color_error_msg", "bg_color.view", "image_rgb.new_zeros", "ValueError", "image_utils._invalid_color_error_msg"], "function", ["home.repos.pwc.inspect_result.applied-ai-lab_zero-shot-pose.datasets.co3d_dataset.FrameData.to", "home.repos.pwc.inspect_result.applied-ai-lab_zero-shot-pose.co3d_tools.image_utils._invalid_color_error_msg", "home.repos.pwc.inspect_result.applied-ai-lab_zero-shot-pose.co3d_tools.image_utils._invalid_color_error_msg"], ["def", "mask_background", "(", "\n", "image_rgb", ":", "torch", ".", "Tensor", ",", "\n", "mask_fg", ":", "torch", ".", "Tensor", ",", "\n", "dim_color", ":", "int", "=", "1", ",", "\n", "bg_color", ":", "Union", "[", "torch", ".", "Tensor", ",", "str", ",", "float", "]", "=", "0.0", ",", "\n", ")", "->", "torch", ".", "Tensor", ":", "\n", "    ", "\"\"\"\n    Mask the background input image tensor `image_rgb` with `bg_color`.\n    The background regions are obtained from the binary foreground segmentation\n    mask `mask_fg`.\n    \"\"\"", "\n", "tgt_view", "=", "[", "1", ",", "1", ",", "1", ",", "1", "]", "\n", "tgt_view", "[", "dim_color", "]", "=", "3", "\n", "# obtain the background color tensor", "\n", "if", "torch", ".", "is_tensor", "(", "bg_color", ")", ":", "\n", "        ", "bg_color_t", "=", "bg_color", ".", "view", "(", "1", ",", "3", ",", "1", ",", "1", ")", ".", "clone", "(", ")", ".", "to", "(", "image_rgb", ")", "\n", "", "elif", "isinstance", "(", "bg_color", ",", "float", ")", ":", "\n", "        ", "bg_color_t", "=", "torch", ".", "tensor", "(", "\n", "[", "bg_color", "]", "*", "3", ",", "device", "=", "image_rgb", ".", "device", ",", "dtype", "=", "image_rgb", ".", "dtype", "\n", ")", ".", "view", "(", "*", "tgt_view", ")", "\n", "", "elif", "isinstance", "(", "bg_color", ",", "str", ")", ":", "\n", "        ", "if", "bg_color", "==", "\"white\"", ":", "\n", "            ", "bg_color_t", "=", "image_rgb", ".", "new_ones", "(", "tgt_view", ")", "\n", "", "elif", "bg_color", "==", "\"black\"", ":", "\n", "            ", "bg_color_t", "=", "image_rgb", ".", "new_zeros", "(", "tgt_view", ")", "\n", "", "else", ":", "\n", "            ", "raise", "ValueError", "(", "_invalid_color_error_msg", "(", "bg_color", ")", ")", "\n", "", "", "else", ":", "\n", "        ", "raise", "ValueError", "(", "_invalid_color_error_msg", "(", "bg_color", ")", ")", "\n", "# cast to the image_rgb's type", "\n", "", "mask_fg", "=", "mask_fg", ".", "type_as", "(", "image_rgb", ")", "\n", "# mask the bg", "\n", "image_masked", "=", "mask_fg", "*", "image_rgb", "+", "(", "1", "-", "mask_fg", ")", "*", "bg_color_t", "\n", "return", "image_masked", "\n", "\n"]], "home.repos.pwc.inspect_result.applied-ai-lab_zero-shot-pose.co3d_tools.image_utils._invalid_color_error_msg": [[48, 52], ["None"], "function", ["None"], ["", "def", "_invalid_color_error_msg", "(", "bg_color", ")", "->", "str", ":", "\n", "    ", "return", "(", "\n", "f\"Invalid bg_color={bg_color}. Plese set bg_color to a 3-element\"", "\n", "+", "\" tensor. or a string (white | black), or a float.\"", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.applied-ai-lab_zero-shot-pose.co3d_tools.utils.Timer.__init__": [[74, 77], ["None"], "methods", ["None"], ["    ", "def", "__init__", "(", "self", ",", "name", "=", "\"timer\"", ",", "quiet", "=", "False", ")", ":", "\n", "        ", "self", ".", "name", "=", "name", "\n", "self", ".", "quiet", "=", "quiet", "\n", "\n"]], "home.repos.pwc.inspect_result.applied-ai-lab_zero-shot-pose.co3d_tools.utils.Timer.__enter__": [[78, 81], ["time.time"], "methods", ["None"], ["", "def", "__enter__", "(", "self", ")", ":", "\n", "        ", "self", ".", "start", "=", "time", ".", "time", "(", ")", "\n", "return", "self", "\n", "\n"]], "home.repos.pwc.inspect_result.applied-ai-lab_zero-shot-pose.co3d_tools.utils.Timer.__exit__": [[82, 87], ["time.time", "print"], "methods", ["None"], ["", "def", "__exit__", "(", "self", ",", "*", "args", ")", ":", "\n", "        ", "self", ".", "end", "=", "time", ".", "time", "(", ")", "\n", "self", ".", "interval", "=", "self", ".", "end", "-", "self", ".", "start", "\n", "if", "not", "self", ".", "quiet", ":", "\n", "            ", "print", "(", "\"%20s: %1.6f sec\"", "%", "(", "self", ".", "name", ",", "self", ".", "interval", ")", ")", "\n", "", "", "", ""]], "home.repos.pwc.inspect_result.applied-ai-lab_zero-shot-pose.co3d_tools.utils.evaluating": [[15, 25], ["net.eval", "net.train"], "function", ["None"], ["@", "contextmanager", "\n", "def", "evaluating", "(", "net", ")", ":", "\n", "    ", "\"\"\"Temporarily switch to evaluation mode.\"\"\"", "\n", "istrain", "=", "net", ".", "training", "\n", "try", ":", "\n", "        ", "net", ".", "eval", "(", ")", "\n", "yield", "net", "\n", "", "finally", ":", "\n", "        ", "if", "istrain", ":", "\n", "            ", "net", ".", "train", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.applied-ai-lab_zero-shot-pose.co3d_tools.utils.try_to_cuda": [[27, 33], ["t.cuda.cuda"], "function", ["home.repos.pwc.inspect_result.applied-ai-lab_zero-shot-pose.datasets.co3d_dataset.FrameData.cuda"], ["", "", "", "def", "try_to_cuda", "(", "t", ")", ":", "\n", "    ", "try", ":", "\n", "        ", "t", "=", "t", ".", "cuda", "(", ")", "\n", "", "except", "AttributeError", ":", "\n", "        ", "pass", "\n", "", "return", "t", "\n", "\n"]], "home.repos.pwc.inspect_result.applied-ai-lab_zero-shot-pose.co3d_tools.utils.dict_to_cuda": [[35, 37], ["utils.try_to_cuda", "batch.items"], "function", ["home.repos.pwc.inspect_result.applied-ai-lab_zero-shot-pose.co3d_tools.utils.try_to_cuda"], ["", "def", "dict_to_cuda", "(", "batch", ")", ":", "\n", "    ", "return", "{", "k", ":", "try_to_cuda", "(", "v", ")", "for", "k", ",", "v", "in", "batch", ".", "items", "(", ")", "}", "\n", "\n"]], "home.repos.pwc.inspect_result.applied-ai-lab_zero-shot-pose.co3d_tools.utils.dataclass_to_cuda_": [[39, 43], ["dataclasses.fields", "setattr", "utils.try_to_cuda", "getattr"], "function", ["home.repos.pwc.inspect_result.applied-ai-lab_zero-shot-pose.co3d_tools.utils.try_to_cuda"], ["", "def", "dataclass_to_cuda_", "(", "obj", ")", ":", "\n", "    ", "for", "f", "in", "dataclasses", ".", "fields", "(", "obj", ")", ":", "\n", "        ", "setattr", "(", "obj", ",", "f", ".", "name", ",", "try_to_cuda", "(", "getattr", "(", "obj", ",", "f", ".", "name", ")", ")", ")", "\n", "", "return", "obj", "\n", "\n"]], "home.repos.pwc.inspect_result.applied-ai-lab_zero-shot-pose.co3d_tools.utils.cat_dataclass": [[46, 71], ["dataclasses.fields", "getattr", "type", "torch.is_tensor", "tensor_collator", "dataclasses.is_dataclass", "utils.cat_dataclass", "isinstance", "getattr", "ValueError", "getattr", "tensor_collator", "getattr"], "function", ["home.repos.pwc.inspect_result.applied-ai-lab_zero-shot-pose.co3d_tools.utils.cat_dataclass"], ["", "def", "cat_dataclass", "(", "batch", ",", "tensor_collator", ")", ":", "\n", "    ", "elem", "=", "batch", "[", "0", "]", "\n", "collated", "=", "{", "}", "\n", "\n", "for", "f", "in", "dataclasses", ".", "fields", "(", "elem", ")", ":", "\n", "        ", "elem_f", "=", "getattr", "(", "elem", ",", "f", ".", "name", ")", "\n", "if", "elem_f", "is", "None", ":", "\n", "            ", "collated", "[", "f", ".", "name", "]", "=", "None", "\n", "", "elif", "torch", ".", "is_tensor", "(", "elem_f", ")", ":", "\n", "            ", "collated", "[", "f", ".", "name", "]", "=", "tensor_collator", "(", "[", "getattr", "(", "e", ",", "f", ".", "name", ")", "for", "e", "in", "batch", "]", ")", "\n", "", "elif", "dataclasses", ".", "is_dataclass", "(", "elem_f", ")", ":", "\n", "            ", "collated", "[", "f", ".", "name", "]", "=", "cat_dataclass", "(", "\n", "[", "getattr", "(", "e", ",", "f", ".", "name", ")", "for", "e", "in", "batch", "]", ",", "tensor_collator", "\n", ")", "\n", "", "elif", "isinstance", "(", "elem_f", ",", "collections", ".", "abc", ".", "Mapping", ")", ":", "\n", "            ", "collated", "[", "f", ".", "name", "]", "=", "{", "\n", "k", ":", "tensor_collator", "(", "[", "getattr", "(", "e", ",", "f", ".", "name", ")", "[", "k", "]", "for", "e", "in", "batch", "]", ")", "\n", "if", "elem_f", "[", "k", "]", "is", "not", "None", "\n", "else", "None", "\n", "for", "k", "in", "elem_f", "\n", "}", "\n", "", "else", ":", "\n", "            ", "raise", "ValueError", "(", "\"Unsupported field type for concatenation\"", ")", "\n", "\n", "", "", "return", "type", "(", "elem", ")", "(", "**", "collated", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.applied-ai-lab_zero-shot-pose.method.zero_shot_pose.DescriptorExtractor.__init__": [[34, 88], ["ValueError"], "methods", ["None"], ["    ", "def", "__init__", "(", "\n", "self", ",", "\n", "patch_size", "=", "8", ",", "\n", "feat_layer", "=", "9", ",", "\n", "high_res", "=", "False", ",", "\n", "binning", "=", "'none'", ",", "\n", "image_size", "=", "224", ",", "\n", "n_target", "=", "5", ",", "\n", "saliency_map_thresh", "=", "0.1", ",", "\n", "num_correspondences", "=", "50", ",", "\n", "kmeans", "=", "False", ",", "\n", "best_frame_mode", "=", "\"corresponding_feats_similarity\"", ",", "\n", ")", ":", "\n", "# if do_log_bin and do_gaussian_blur:", "\n", "#     print('Warning: both gaussian blur and log bin flags set to True')", "\n", "        ", "self", ".", "patch_size", "=", "patch_size", "\n", "self", ".", "feat_layer", "=", "feat_layer", "\n", "self", ".", "high_res", "=", "high_res", "\n", "self", ".", "binning", "=", "binning", "\n", "# self.do_log_bin = do_log_bin", "\n", "# self.do_gaussian_blur = do_gaussian_blur", "\n", "self", ".", "image_size", "=", "image_size", "\n", "self", ".", "n_target", "=", "n_target", "\n", "self", ".", "saliency_map_thresh", "=", "saliency_map_thresh", "\n", "self", ".", "num_correspondences", "=", "num_correspondences", "\n", "self", ".", "best_frame_mode", "=", "best_frame_mode", "\n", "\n", "if", "self", ".", "patch_size", "==", "16", ":", "\n", "            ", "self", ".", "model_name", "=", "'vit_base'", "\n", "self", ".", "stride", "=", "8", "\n", "self", ".", "num_patches", "=", "14", "\n", "self", ".", "padding", "=", "5", "\n", "", "elif", "self", ".", "patch_size", "==", "8", ":", "\n", "            ", "self", ".", "model_name", "=", "'vit_small'", "\n", "self", ".", "stride", "=", "4", "\n", "self", ".", "num_patches", "=", "28", "\n", "self", ".", "padding", "=", "2", "\n", "", "else", ":", "\n", "            ", "raise", "ValueError", "(", "'ViT models only supported with patch sizes 8 or 16'", ")", "\n", "\n", "", "if", "self", ".", "high_res", ":", "\n", "            ", "self", ".", "num_patches", "*=", "2", "\n", "\n", "", "if", "kmeans", ":", "\n", "            ", "self", ".", "correspondence_mode", "=", "'batch_knn'", "# ('batch', 'batch_knn', 'original')", "\n", "", "else", ":", "\n", "            ", "self", ".", "correspondence_mode", "=", "'batch'", "# ('batch', 'batch_knn', 'original')", "\n", "", "self", ".", "batched_correspond", "=", "True", "\n", "# Image processing", "\n", "self", ".", "image_norm_mean", "=", "(", "0.485", ",", "0.456", ",", "0.406", ")", "\n", "self", ".", "image_norm_std", "=", "(", "0.229", ",", "0.224", ",", "0.225", ")", "\n", "\n", "# Initialise None model - this gets loaded by call to load_model", "\n", "self", ".", "model", "=", "None", "\n", "\n"]], "home.repos.pwc.inspect_result.applied-ai-lab_zero-shot-pose.method.zero_shot_pose.DescriptorExtractor.load_model": [[90, 103], ["torch.load", "model.load_state_dict", "model.to", "model.eval"], "methods", ["home.repos.pwc.inspect_result.applied-ai-lab_zero-shot-pose.datasets.co3d_dataset.FrameData.to"], ["", "def", "load_model", "(", "self", ",", "pretrain_path", ",", "device", ")", ":", "\n", "        ", "model", "=", "vits", ".", "__dict__", "[", "self", ".", "model_name", "]", "(", "patch_size", "=", "self", ".", "patch_size", ")", "\n", "state_dict", "=", "torch", ".", "load", "(", "pretrain_path", ",", "map_location", "=", "'cpu'", ")", "\n", "model", ".", "load_state_dict", "(", "state_dict", ")", "\n", "model", ".", "to", "(", "device", ")", "\n", "model", ".", "eval", "(", ")", "\n", "\n", "if", "self", ".", "high_res", ":", "\n", "            ", "model", ".", "patch_embed", ".", "proj", ".", "stride", "=", "(", "self", ".", "stride", ",", "self", ".", "stride", ")", "\n", "model", ".", "num_patches", "=", "self", ".", "num_patches", "**", "2", "\n", "model", ".", "patch_embed", ".", "patch_size", "=", "self", ".", "stride", "\n", "model", ".", "patch_embed", ".", "proj", ".", "padding", "=", "self", ".", "padding", "\n", "", "self", ".", "model", "=", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.applied-ai-lab_zero-shot-pose.method.zero_shot_pose.DescriptorExtractor.extract_features_and_attn": [[105, 151], ["all_images.view.view.size", "all_images.view.view.view", "torch.no_grad", "torch.cuda.empty_cache", "zero_shot_pose.DescriptorExtractor.model.get_specific_tokens", "numpy.ceil().astype", "enumerate", "torch.cat", "torch.cat", "torch.cat", "all_images.view.view.chunk", "data_chunks.append", "numpy.ceil", "zero_shot_pose.DescriptorExtractor.model.get_specific_tokens"], "methods", ["home.repos.pwc.inspect_result.applied-ai-lab_zero-shot-pose.method.vision_transformer_flexible.VisionTransformer.get_specific_tokens", "home.repos.pwc.inspect_result.applied-ai-lab_zero-shot-pose.method.vision_transformer_flexible.VisionTransformer.get_specific_tokens"], ["", "def", "extract_features_and_attn", "(", "self", ",", "all_images", ")", ":", "\n", "        ", "\"\"\"\n        A definition of relevant dimensions {all_b, nh, t, d}:\n            image_size: Side length of input images (assumed square)\n            all_b: The first dimension size of the input tensor - not necessarily\n                the same as \"batch size\" in high-level script, as we assume that\n                reference and target images are all flattened-then-concatenated\n                along the batch dimension. With e.g. a batch size of 2, and 5 target\n                images, 1 reference image; all_b = 2 * (5+1) = 12\n            h: number of heads in ViT, e.g. 6\n            t: number of items in ViT keys/values/tokens, e.g. 785 (= 28*28 + 1)\n            d: feature dim in ViT, e.g. 64\n\n        Args:\n            all_images (torch.Tensor): shape (all_b, 3, image_size, image_size)\n        Returns:\n            features (torch.Tensor): shape (all_b, nh, t, d) e.g. (12, 6, 785, 64)\n            attn (torch.Tensor): shape (all_b, nh, t, t) e.g. (12, 6, 785, 785)\n            output_cls_tokens (torch.Tensor): shape (all_b, nh*d) e.g. (12, 384)\n        \"\"\"", "\n", "MAX_BATCH_SIZE", "=", "50", "\n", "all_images_batch_size", "=", "all_images", ".", "size", "(", "0", ")", "\n", "c", ",", "img_h", ",", "img_w", "=", "all_images", ".", "shape", "[", "-", "3", ":", "]", "\n", "all_images", "=", "all_images", ".", "view", "(", "-", "1", ",", "c", ",", "img_h", ",", "img_w", ")", "\n", "\n", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "            ", "torch", ".", "cuda", ".", "empty_cache", "(", ")", "\n", "\n", "if", "all_images_batch_size", "<=", "MAX_BATCH_SIZE", ":", "\n", "                ", "data", "=", "self", ".", "model", ".", "get_specific_tokens", "(", "all_images", ",", "layers_to_return", "=", "(", "9", ",", "11", ")", ")", "\n", "features", "=", "data", "[", "self", ".", "feat_layer", "]", "[", "'k'", "]", "\n", "attn", "=", "data", "[", "11", "]", "[", "'attn'", "]", "\n", "output_cls_tokens", "=", "data", "[", "11", "]", "[", "'t'", "]", "[", ":", ",", "0", ",", ":", "]", "\n", "\n", "# Process in chunks to avoid CUDA out-of-memory", "\n", "", "else", ":", "\n", "                ", "num_chunks", "=", "np", ".", "ceil", "(", "all_images_batch_size", "/", "MAX_BATCH_SIZE", ")", ".", "astype", "(", "'int'", ")", "\n", "data_chunks", "=", "[", "]", "\n", "for", "i", ",", "ims_", "in", "enumerate", "(", "all_images", ".", "chunk", "(", "num_chunks", ")", ")", ":", "\n", "                    ", "data_chunks", ".", "append", "(", "self", ".", "model", ".", "get_specific_tokens", "(", "ims_", ",", "layers_to_return", "=", "(", "9", ",", "11", ")", ")", ")", "\n", "\n", "", "features", "=", "torch", ".", "cat", "(", "[", "d", "[", "self", ".", "feat_layer", "]", "[", "'k'", "]", "for", "d", "in", "data_chunks", "]", ",", "dim", "=", "0", ")", "\n", "attn", "=", "torch", ".", "cat", "(", "[", "d", "[", "11", "]", "[", "'attn'", "]", "for", "d", "in", "data_chunks", "]", ",", "dim", "=", "0", ")", "\n", "output_cls_tokens", "=", "torch", ".", "cat", "(", "[", "d", "[", "11", "]", "[", "'t'", "]", "[", ":", ",", "0", ",", ":", "]", "for", "d", "in", "data_chunks", "]", ",", "dim", "=", "0", ")", "\n", "\n", "", "", "return", "features", ",", "attn", ",", "output_cls_tokens", "\n", "\n"]], "home.repos.pwc.inspect_result.applied-ai-lab_zero-shot-pose.method.zero_shot_pose.DescriptorExtractor.create_reshape_descriptors": [[152, 200], ["zsp.method.dense_descriptor_utils.gaussian_blurring.size", "zsp.method.dense_descriptor_utils.gaussian_blurring.size", "zsp.method.dense_descriptor_utils.gaussian_blurring.view", "attn.to.to.view", "zsp.method.dense_descriptor_utils.gaussian_blurring.to", "attn.to.to.to", "zsp.method.dense_descriptor_utils.gaussian_blurring.permute().reshape", "zsp.method.dense_descriptor_utils._log_bin", "zsp.method.dense_descriptor_utils.gaussian_blurring", "zsp.method.dense_descriptor_utils.gaussian_blurring.permute", "zsp.method.dense_descriptor_utils.gaussian_blurring.squeeze", "ValueError"], "methods", ["home.repos.pwc.inspect_result.applied-ai-lab_zero-shot-pose.datasets.co3d_dataset.FrameData.to", "home.repos.pwc.inspect_result.applied-ai-lab_zero-shot-pose.datasets.co3d_dataset.FrameData.to", "home.repos.pwc.inspect_result.applied-ai-lab_zero-shot-pose.method.dense_descriptor_utils._log_bin", "home.repos.pwc.inspect_result.applied-ai-lab_zero-shot-pose.method.dense_descriptor_utils.gaussian_blurring"], ["", "def", "create_reshape_descriptors", "(", "self", ",", "features", ",", "attn", ",", "batch_size", ",", "device", ")", ":", "\n", "        ", "\"\"\"\n        Relevant dimensions are defined as for extract_features_and_attn method above\n        \n        3 new dimension params here are:\n            B: This is the batch size used in the dataloader/calling script\n            n_tgt: This is equal to self.n_target\n            feat_dim: This is the dimensionality of the descriptors - while related\n                to the ViT feature dimension, it may have undergone further binning\n                procedures that will increase its dimension, or dimensionality reduction\n                approaches to *decrease* the dimension\n\n        Args:\n            features (torch.Tensor): shape (all_b, nh, t, d) e.g. (12, 6, 785, 64)\n            attn (torch.Tensor): shape (all_b, nh, t, t) e.g. (12, 6, 785, 785)\n            output_cls_tokens (torch.Tensor): shape (all_b, nh*d) e.g. (12, 384)\n\n        Returns:\n            features (torch.Tensor): shape Bx(n_tgt+1)x1x(t-1)xfeat_dim, this is\n                a descriptor tensor, rather than raw features from the ViT. \n            attn (torch.Tensor): shape Bx(n_tgt+1)xhxtxt, this is the spatial\n                self-attention maps\n        \"\"\"", "\n", "all_b", ",", "h", ",", "t", ",", "d", "=", "features", ".", "size", "(", ")", "\n", "# Remove cls output (first 'patch') from features", "\n", "features", "=", "features", "[", ":", ",", ":", ",", "1", ":", ",", ":", "]", "# (all_b) x h x (t-1) x d  e.g. (12, 6, 784, 64)", "\n", "# Roll multiple ViT heads into a single feature, re-add head dimension", "\n", "features", "=", "features", ".", "permute", "(", "0", ",", "2", ",", "1", ",", "3", ")", ".", "reshape", "(", "all_b", ",", "t", "-", "1", ",", "h", "*", "d", ")", "[", ":", ",", "None", ",", ":", ",", ":", "]", "# all_b x 1 x (t-1) x (d*h)", "\n", "if", "self", ".", "binning", "==", "'log'", ":", "\n", "            ", "features", "=", "_log_bin", "(", "features", ",", "device", "=", "device", ")", "# all_b x 1 x (t - 1) x (17*d*h), with 17 from the log binning", "\n", "", "elif", "self", ".", "binning", "==", "'gaussian'", ":", "\n", "            ", "features", "=", "gaussian_blurring", "(", "features", ".", "squeeze", "(", "1", ")", ",", "kernel_size", "=", "7", ",", "sigma", "=", "2", ")", "\n", "features", "=", "features", "[", ":", ",", "None", ",", ":", ",", ":", "]", "\n", "", "elif", "self", ".", "binning", "==", "'none'", ":", "\n", "            ", "pass", "\n", "", "else", ":", "\n", "            ", "raise", "ValueError", "(", "f\"{self.binning} is not a valid choice for the 'binning' parameter of the DescriptorExtractor\"", ")", "\n", "\n", "# Reshape back to batched view", "\n", "", "_", ",", "_", ",", "_", ",", "feat_dim_after_binning", "=", "features", ".", "size", "(", ")", "\n", "features", "=", "features", ".", "view", "(", "batch_size", ",", "-", "1", ",", "1", ",", "t", "-", "1", ",", "feat_dim_after_binning", ")", "# Bx(n_tgt+1)x1x(t-1)xfeat_dim", "\n", "\n", "attn", "=", "attn", ".", "view", "(", "batch_size", ",", "-", "1", ",", "h", ",", "t", ",", "t", ")", "# B x (n_tgt+1) x h x t x t", "\n", "\n", "# Ensure descriptors & attn are on the correct device", "\n", "features", "=", "features", ".", "to", "(", "device", ")", "\n", "attn", "=", "attn", ".", "to", "(", "device", ")", "\n", "return", "features", ",", "attn", "\n", "\n"]], "home.repos.pwc.inspect_result.applied-ai-lab_zero-shot-pose.method.zero_shot_pose.DescriptorExtractor.split_ref_target": [[201, 243], ["attn.size", "features.split", "attn.split", "ref_feats.view.view.repeat", "ref_attn.view.view.repeat", "ref_feats.view.view.view", "ref_attn.view.view.view", "target_feats.reshape.reshape.reshape", "target_attn.reshape.reshape.reshape", "features.size"], "methods", ["None"], ["", "def", "split_ref_target", "(", "self", ",", "features", ",", "attn", ")", ":", "\n", "        ", "\"\"\"\n        Reshapes, repeats and splits features and attention into ref/tgt\n\n        Specifically, this function splits out the reference and target descriptors/attn,\n        repeats the reference image n_tgt times, and flattens the n_tgt dimension\n        into the batch dimension.\n\n        Dimensions as for extract_features_and_attn, create_reshape_descriptors\n        Args:\n            features (torch.Tensor): shape Bx(n_tgt+1)x1x(t-1)xfeat_dim, this is\n                a descriptor tensor, rather than raw features from the ViT. \n            attn (torch.Tensor): shape Bx(n_tgt+1)xhxtxt, this is the spatial\n                self-attention maps\n\n        Returns:\n            ref_feats (torch.Tensor): shape (B*n_tgt)x1x(t-1)xfeat_dim, this is\n                a descriptor tensor, repeated n_tgt times to match target_feats. \n            target_feats (torch.Tensor): shape (B*n_tgt)x1x(t-1)xfeat_dim, this is\n                the tensor of descriptors for the target images\n            ref_attn (torch.Tensor): shape (B*n_tgt)xhxtxt, the reference im's spatial\n                self-attention map, repeated n_tgt times to match target_feats\n            target_attn (torch.Tensor): shape (B*n_tgt)xhxtxt, the spatial\n                self-attention maps for the target images    \n        \"\"\"", "\n", "batch_size", ",", "_", ",", "h", ",", "t", ",", "t", "=", "attn", ".", "size", "(", ")", "\n", "feat_dim_after_binning", "=", "features", ".", "size", "(", ")", "[", "-", "1", "]", "\n", "\n", "# Split descriptors, attn back to reference image & target images", "\n", "ref_feats", ",", "target_feats", "=", "features", ".", "split", "(", "(", "1", ",", "self", ".", "n_target", ")", ",", "dim", "=", "1", ")", "\n", "ref_attn", ",", "target_attn", "=", "attn", ".", "split", "(", "(", "1", ",", "self", ".", "n_target", ")", ",", "dim", "=", "1", ")", "\n", "\n", "ref_feats", "=", "ref_feats", ".", "repeat", "(", "1", ",", "self", ".", "n_target", ",", "1", ",", "1", ",", "1", ")", "\n", "ref_attn", "=", "ref_attn", ".", "repeat", "(", "1", ",", "self", ".", "n_target", ",", "1", ",", "1", ",", "1", ")", "\n", "\n", "# Flatten first 2 dims again:", "\n", "ref_feats", "=", "ref_feats", ".", "view", "(", "batch_size", "*", "self", ".", "n_target", ",", "1", ",", "t", "-", "1", ",", "feat_dim_after_binning", ")", "\n", "ref_attn", "=", "ref_attn", ".", "view", "(", "batch_size", "*", "self", ".", "n_target", ",", "h", ",", "t", ",", "t", ")", "\n", "\n", "target_feats", "=", "target_feats", ".", "reshape", "(", "batch_size", "*", "self", ".", "n_target", ",", "1", ",", "t", "-", "1", ",", "feat_dim_after_binning", ")", "\n", "target_attn", "=", "target_attn", ".", "reshape", "(", "batch_size", "*", "self", ".", "n_target", ",", "h", ",", "t", ",", "t", ")", "\n", "return", "ref_feats", ",", "target_feats", ",", "ref_attn", ",", "target_attn", "\n", "\n"]], "home.repos.pwc.inspect_result.applied-ai-lab_zero-shot-pose.method.zero_shot_pose.DescriptorExtractor.get_correspondences": [[245, 313], ["zsp.method.correspondence_functions.find_correspondences_batch", "NotImplementedError", "range", "torch.stack", "torch.stack", "torch.stack", "len", "zsp.method.correspondence_functions.find_correspondences_original", "selected_points_image_1_batch.append", "selected_points_image_2_batch.append", "sim_selected_12_batch.append", "zsp.method.correspondence_functions.find_correspondences_batch_with_knn"], "methods", ["home.repos.pwc.inspect_result.applied-ai-lab_zero-shot-pose.method.correspondence_functions.find_correspondences_batch", "home.repos.pwc.inspect_result.applied-ai-lab_zero-shot-pose.method.correspondence_functions.find_correspondences_original", "home.repos.pwc.inspect_result.applied-ai-lab_zero-shot-pose.method.correspondence_functions.find_correspondences_batch_with_knn"], ["", "def", "get_correspondences", "(", "self", ",", "ref_feats", ",", "target_feats", ",", "ref_attn", ",", "target_attn", ",", "device", ")", ":", "\n", "        ", "\"\"\"\n        Args:\n            ref_feats (torch.Tensor): shape (B*n_tgt)x1x(t-1)xfeat_dim, this is\n                a descriptor tensor, repeated n_tgt times to match target_feats. \n            target_feats (torch.Tensor): shape (B*n_tgt)x1x(t-1)xfeat_dim, this is\n                the tensor of descriptors for the target images\n            ref_attn (torch.Tensor): shape (B*n_tgt)xhxtxt, the reference im's spatial\n                self-attention map, repeated n_tgt times to match target_feats\n            target_attn (torch.Tensor): shape (B*n_tgt)xhxtxt, the spatial\n                self-attention maps for the target images \n\n        Returns:\n            selected_points_image_2 (torch.Tensor): Shape (Bxn_tgt)xself.num_correspondencesx2, \n                this is a tensor giving the \n            selected_points_image_1 (torch.Tensor):\n            cyclical_dists (torch.Tensor):\n            sim_selected_12 (torch.Tensor):\n            \n        \"\"\"", "\n", "\n", "# Note flipped way in which features and attention maps are passed to find correspondence function", "\n", "if", "self", ".", "correspondence_mode", "==", "'batch'", ":", "\n", "\n", "            ", "selected_points_image_2", ",", "selected_points_image_1", ",", "cyclical_dists", ",", "sim_selected_12", "=", "find_correspondences_batch", "(", "\n", "descriptors1", "=", "target_feats", ",", "\n", "descriptors2", "=", "ref_feats", ",", "\n", "attn1", "=", "target_attn", ",", "\n", "attn2", "=", "ref_attn", ",", "\n", "device", "=", "device", ",", "\n", "num_pairs", "=", "self", ".", "num_correspondences", ")", "\n", "\n", "", "elif", "self", ".", "correspondence_mode", "==", "'original'", ":", "\n", "            ", "raise", "NotImplementedError", "(", "\"'original' (non-batched) correspondence mode not currently supported\"", ")", "\n", "selected_points_image_1_batch", "=", "[", "]", "\n", "selected_points_image_2_batch", "=", "[", "]", "\n", "sim_selected_12_batch", "=", "[", "]", "\n", "for", "i", "in", "range", "(", "len", "(", "target_feats", ")", ")", ":", "\n", "                ", "selected_points_image_2", ",", "selected_points_image_1", ",", "sim_selected_12", "=", "find_correspondences_original", "(", "\n", "descriptors1", "=", "target_feats", "[", "i", ":", "i", "+", "1", "]", ",", "\n", "descriptors2", "=", "ref_feats", "[", "i", ":", "i", "+", "1", "]", ",", "\n", "attn1", "=", "target_attn", "[", "i", ":", "i", "+", "1", "]", ",", "\n", "attn2", "=", "ref_attn", "[", "i", ":", "i", "+", "1", "]", ",", "\n", "device", "=", "device", ",", "\n", "num_pairs", "=", "self", ".", "num_correspondences", ")", "\n", "selected_points_image_1_batch", ".", "append", "(", "selected_points_image_1", ")", "\n", "selected_points_image_2_batch", ".", "append", "(", "selected_points_image_2", ")", "\n", "sim_selected_12_batch", ".", "append", "(", "sim_selected_12", ")", "\n", "", "selected_points_image_1", "=", "torch", ".", "stack", "(", "selected_points_image_1_batch", ",", "dim", "=", "0", ")", "\n", "selected_points_image_2", "=", "torch", ".", "stack", "(", "selected_points_image_2_batch", ",", "dim", "=", "0", ")", "\n", "sim_selected_12", "=", "torch", ".", "stack", "(", "sim_selected_12_batch", ",", "dim", "=", "0", ")", "\n", "\n", "", "elif", "self", ".", "correspondence_mode", "==", "'batch_knn'", ":", "\n", "            ", "if", "self", ".", "high_res", ":", "\n", "                ", "num_pairs_for_topk", "=", "8", "*", "self", ".", "num_correspondences", "\n", "", "else", ":", "\n", "                ", "num_pairs_for_topk", "=", "2", "*", "self", ".", "num_correspondences", "\n", "\n", "", "selected_points_image_2", ",", "selected_points_image_1", ",", "cyclical_dists", ",", "sim_selected_12", "=", "find_correspondences_batch_with_knn", "(", "\n", "descriptors1", "=", "target_feats", ",", "\n", "descriptors2", "=", "ref_feats", ",", "\n", "attn1", "=", "target_attn", ",", "\n", "attn2", "=", "ref_attn", ",", "\n", "device", "=", "device", ",", "\n", "num_pairs_for_topk", "=", "num_pairs_for_topk", ")", "\n", "\n", "", "return", "(", "selected_points_image_2", ",", "selected_points_image_1", ",", "cyclical_dists", ",", "sim_selected_12", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.applied-ai-lab_zero-shot-pose.method.zero_shot_pose.DescriptorExtractor.find_closest_match": [[314, 359], ["attn.size", "int", "attn.split", "ref_attn.view.view.repeat", "ref_attn.view.view.view", "target_attn.reshape.reshape.reshape", "numpy.sqrt", "output_cls_tokens.view.view.view", "output_cls_tokens.view.view.split", "zsp.method.zero_shot_pose_utils.rank_target_images_by_global_sim", "similarities.view.view.argmax", "similarities.view.argmax.squeeze", "zsp.method.dense_descriptor_utils.extract_saliency_maps", "zsp.method.dense_descriptor_utils.extract_saliency_maps", "zsp.method.zero_shot_pose_utils.batch_intersection_over_union", "similarities.view.view.view", "similarities.view.view.argmax", "x.view", "sim_selected_12.view.view.view", "sim_selected_12.view.view.sum", "similarities.view.view.argmax", "zsp.method.dense_descriptor_utils.extract_saliency_maps", "saliency_map_target.view.view.view", "zsp.method.zero_shot_pose_utils.normalize_cyclical_dists", "zsp.method.zero_shot_pose_utils.batch_intersection_over_union", "similarities.view.view.view", "similarities.view.view.argmax", "ValueError"], "methods", ["home.repos.pwc.inspect_result.applied-ai-lab_zero-shot-pose.method.zero_shot_pose_utils.rank_target_images_by_global_sim", "home.repos.pwc.inspect_result.applied-ai-lab_zero-shot-pose.method.dense_descriptor_utils.extract_saliency_maps", "home.repos.pwc.inspect_result.applied-ai-lab_zero-shot-pose.method.dense_descriptor_utils.extract_saliency_maps", "home.repos.pwc.inspect_result.applied-ai-lab_zero-shot-pose.method.zero_shot_pose_utils.batch_intersection_over_union", "home.repos.pwc.inspect_result.applied-ai-lab_zero-shot-pose.method.dense_descriptor_utils.extract_saliency_maps", "home.repos.pwc.inspect_result.applied-ai-lab_zero-shot-pose.method.zero_shot_pose_utils.normalize_cyclical_dists", "home.repos.pwc.inspect_result.applied-ai-lab_zero-shot-pose.method.zero_shot_pose_utils.batch_intersection_over_union"], ["", "def", "find_closest_match", "(", "self", ",", "attn", ",", "output_cls_tokens", ",", "sim_selected_12", ",", "batch_size", ")", ":", "\n", "        ", "batch_size", ",", "_", ",", "h", ",", "t", ",", "t", "=", "attn", ".", "size", "(", ")", "\n", "# N is the height or width of the feature map", "\n", "N", "=", "int", "(", "np", ".", "sqrt", "(", "t", "-", "1", ")", ")", "\n", "\n", "ref_attn", ",", "target_attn", "=", "attn", ".", "split", "(", "(", "1", ",", "self", ".", "n_target", ")", ",", "dim", "=", "1", ")", "\n", "ref_attn", "=", "ref_attn", ".", "repeat", "(", "1", ",", "self", ".", "n_target", ",", "1", ",", "1", ",", "1", ")", "\n", "# Flatten first 2 dims again:", "\n", "ref_attn", "=", "ref_attn", ".", "view", "(", "batch_size", "*", "self", ".", "n_target", ",", "h", ",", "t", ",", "t", ")", "\n", "target_attn", "=", "target_attn", ".", "reshape", "(", "batch_size", "*", "self", ".", "n_target", ",", "h", ",", "t", ",", "t", ")", "\n", "\n", "if", "self", ".", "best_frame_mode", "==", "'global_similarity'", ":", "\n", "            ", "output_cls_tokens", "=", "output_cls_tokens", ".", "view", "(", "batch_size", ",", "self", ".", "n_target", "+", "1", ",", "-", "1", ")", "\n", "ref_global_feats", ",", "target_global_feats", "=", "output_cls_tokens", ".", "split", "(", "(", "1", ",", "self", ".", "n_target", ")", ",", "dim", "=", "1", ")", "\n", "similarities", "=", "rank_target_images_by_global_sim", "(", "ref_global_feats", ",", "target_global_feats", ")", "\n", "best_idxs", "=", "similarities", ".", "argmax", "(", "dim", "=", "-", "1", ")", "\n", "best_idxs", "=", "best_idxs", ".", "squeeze", "(", ")", "\n", "", "elif", "self", ".", "best_frame_mode", "==", "'ref_to_target_saliency_map_iou'", ":", "\n", "# TODO: Can make the below quicker by calling on just one copy of ref_attn, *then* 'repeat' ", "\n", "            ", "saliency_map_ref", "=", "extract_saliency_maps", "(", "ref_attn", ")", "# (B*n_tgt)x(t-1), with (t-1)=(N**2)", "\n", "saliency_map_target", "=", "extract_saliency_maps", "(", "target_attn", ")", "# (B*n_tgt)x(t-1), with (t-1)=(N**2)", "\n", "# Reshape saliency maps to (B*n_tgt)xNxN (N is height and width of feature map)", "\n", "saliency_map_ref", ",", "saliency_map_target", "=", "(", "x", ".", "view", "(", "batch_size", "*", "self", ".", "n_target", ",", "N", ",", "N", ")", "\n", "for", "x", "in", "(", "saliency_map_ref", ",", "saliency_map_target", ")", ")", "\n", "# Compute IoUs", "\n", "similarities", ",", "intersection_map", "=", "batch_intersection_over_union", "(", "\n", "saliency_map_ref", ",", "saliency_map_target", ",", "threshold", "=", "self", ".", "saliency_map_thresh", ")", "\n", "similarities", "=", "similarities", ".", "view", "(", "batch_size", ",", "self", ".", "n_target", ")", "\n", "best_idxs", "=", "similarities", ".", "argmax", "(", "dim", "=", "-", "1", ")", "\n", "", "elif", "self", ".", "best_frame_mode", "==", "'corresponding_feats_similarity'", ":", "\n", "            ", "sim_selected_12", "=", "sim_selected_12", ".", "view", "(", "batch_size", ",", "self", ".", "n_target", ",", "self", ".", "num_correspondences", ")", "\n", "similarities", "=", "sim_selected_12", ".", "sum", "(", "dim", "=", "-", "1", ")", "\n", "best_idxs", "=", "similarities", ".", "argmax", "(", "dim", "=", "-", "1", ")", "\n", "", "elif", "self", ".", "best_frame_mode", "==", "'cylical_dists_to_saliency_map_iou'", ":", "\n", "            ", "saliency_map_target", "=", "extract_saliency_maps", "(", "target_attn", ")", "\n", "saliency_map_target", "=", "saliency_map_target", ".", "view", "(", "batch_size", "*", "self", ".", "n_target", ",", "N", ",", "N", ")", "\n", "\n", "cyclical_dists", "=", "normalize_cyclical_dists", "(", "cyclical_dists", ")", "\n", "similarities", ",", "intersection_map", "=", "batch_intersection_over_union", "(", "\n", "cyclical_dists", ",", "saliency_map_target", ",", "threshold", "=", "self", ".", "saliency_map_thresh", ")", "\n", "similarities", "=", "similarities", ".", "view", "(", "batch_size", ",", "self", ".", "n_target", ")", "\n", "best_idxs", "=", "similarities", ".", "argmax", "(", "dim", "=", "-", "1", ")", "\n", "", "else", ":", "\n", "            ", "raise", "ValueError", "(", "f'Method of picking best frame not implemented: {self.best_frame_mode}'", ")", "\n", "", "return", "similarities", ",", "best_idxs", "\n", "\n"]], "home.repos.pwc.inspect_result.applied-ai-lab_zero-shot-pose.method.zero_shot_pose.DescriptorExtractor.scale_patch_to_pix": [[361, 376], ["zsp.method.zero_shot_pose_utils.scale_points_from_patch"], "methods", ["home.repos.pwc.inspect_result.applied-ai-lab_zero-shot-pose.method.zero_shot_pose_utils.scale_points_from_patch"], ["", "def", "scale_patch_to_pix", "(", "self", ",", "points1", ",", "points2", ",", "N", ")", ":", "\n", "        ", "\"\"\"\n        Args:\n            points1 (torch.Tensor): shape num_correspondencesx2, the *patch* coordinates\n                of correspondence points in image 1 (the reference image)\n            points2 (torch.Tensor): shape num_correspondencesx2, the *patch* coordinates\n                of correspondence points in image 2 (the best target image)\n            N (int): N is the height or width of the feature map\n        \"\"\"", "\n", "if", "self", ".", "batched_correspond", ":", "\n", "            ", "points1_rescaled", ",", "points2_rescaled", "=", "(", "scale_points_from_patch", "(", "\n", "p", ",", "vit_image_size", "=", "self", ".", "image_size", ",", "num_patches", "=", "N", ")", "for", "p", "in", "(", "points1", ",", "points2", ")", ")", "\n", "", "else", ":", "# earlier descriptor extractor functions for ViT features scaled before return", "\n", "            ", "points1_rescaled", ",", "points2_rescaled", "=", "(", "points1", ",", "points2", ")", "\n", "", "return", "points1_rescaled", ",", "points2_rescaled", "\n", "\n"]], "home.repos.pwc.inspect_result.applied-ai-lab_zero-shot-pose.method.zero_shot_pose.DescriptorExtractor.get_transform": [[378, 385], ["torchvision.transforms.Compose", "torchvision.transforms.Resize", "torchvision.transforms.ToTensor", "torchvision.transforms.Normalize"], "methods", ["None"], ["", "def", "get_transform", "(", "self", ")", ":", "\n", "        ", "image_transform", "=", "transforms", ".", "Compose", "(", "[", "\n", "transforms", ".", "Resize", "(", "(", "self", ".", "image_size", ",", "self", ".", "image_size", ")", ")", ",", "\n", "transforms", ".", "ToTensor", "(", ")", ",", "\n", "transforms", ".", "Normalize", "(", "mean", "=", "self", ".", "image_norm_mean", ",", "std", "=", "self", ".", "image_norm_std", ")", "\n", "]", ")", "\n", "return", "image_transform", "\n", "\n"]], "home.repos.pwc.inspect_result.applied-ai-lab_zero-shot-pose.method.zero_shot_pose.DescriptorExtractor.denorm_torch_to_pil": [[387, 391], ["PIL.Image.fromarray", "torch.Tensor", "torch.Tensor", "image.permute"], "methods", ["None"], ["", "def", "denorm_torch_to_pil", "(", "self", ",", "image", ")", ":", "\n", "        ", "image", "=", "image", "*", "torch", ".", "Tensor", "(", "self", ".", "image_norm_std", ")", "[", ":", ",", "None", ",", "None", "]", "\n", "image", "=", "image", "+", "torch", ".", "Tensor", "(", "self", ".", "image_norm_mean", ")", "[", ":", ",", "None", ",", "None", "]", "\n", "return", "Image", ".", "fromarray", "(", "(", "image", ".", "permute", "(", "1", ",", "2", ",", "0", ")", "*", "255", ")", ".", "numpy", "(", ")", ".", "astype", "(", "np", ".", "uint8", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.applied-ai-lab_zero-shot-pose.method.zero_shot_pose.ZeroShotPoseMethod.__init__": [[394, 417], ["None"], "methods", ["None"], ["    ", "def", "__init__", "(", "\n", "self", ",", "\n", "num_samples_per_class", "=", "100", ",", "\n", "batched_correspond", "=", "True", ",", "\n", "num_plot_examples_per_batch", "=", "1", ",", "\n", "saliency_map_thresh", "=", "0.1", ",", "\n", "ransac_thresh", "=", "0.2", ",", "\n", "n_target", "=", "5", ",", "\n", "num_correspondences", "=", "50", ",", "\n", "take_best_view", "=", "False", ",", "# if True, simply use the best view as the pose estimate", "\n", "ransac_min_samples", "=", "4", ",", "\n", "ransac_max_trials", "=", "10000", ",", "\n", ")", ":", "\n", "        ", "self", ".", "num_samples_per_class", "=", "num_samples_per_class", "\n", "self", ".", "batched_correspond", "=", "batched_correspond", "\n", "self", ".", "num_plot_examples_per_batch", "=", "num_plot_examples_per_batch", "\n", "self", ".", "saliency_map_thresh", "=", "saliency_map_thresh", "\n", "self", ".", "ransac_thresh", "=", "ransac_thresh", "\n", "self", ".", "n_target", "=", "n_target", "\n", "self", ".", "num_correspondences", "=", "num_correspondences", "\n", "self", ".", "take_best_view", "=", "take_best_view", "\n", "self", ".", "ransac_min_samples", "=", "ransac_min_samples", "\n", "self", ".", "ransac_max_trials", "=", "ransac_max_trials", "\n", "\n"]], "home.repos.pwc.inspect_result.applied-ai-lab_zero-shot-pose.method.zero_shot_pose.ZeroShotPoseMethod.make_log_dirs": [[418, 434], ["os.path.join", "os.path.exists", "os.path.join", "os.path.join", "zsp.utils.project_utils.get_results_length", "os.path.exists", "os.makedirs", "print", "print", "os.path.exists", "os.remove"], "methods", ["home.repos.pwc.inspect_result.applied-ai-lab_zero-shot-pose.utils.project_utils.get_results_length"], ["", "def", "make_log_dirs", "(", "self", ",", "log_dir", ",", "category", ")", ":", "\n", "        ", "cat_log_dir", "=", "os", ".", "path", ".", "join", "(", "log_dir", ",", "category", ")", "\n", "if", "os", ".", "path", ".", "exists", "(", "cat_log_dir", ")", ":", "\n", "            ", "results_file", "=", "os", ".", "path", ".", "join", "(", "cat_log_dir", ",", "f\"results.txt\"", ")", "\n", "num_run", "=", "get_results_length", "(", "results_file", ")", "\n", "if", "num_run", ">=", "self", ".", "num_samples_per_class", ":", "\n", "                ", "print", "(", "f\"CATEGORY {category} ALREADY RUN -- SKIPPING!\"", ")", "\n", "return", "0", "\n", "", "else", ":", "\n", "                ", "print", "(", "F\"CATEGORY {category} RUN FOR {num_run} ITEMS - REMOVING, RE-RUNNING\"", ")", "\n", "if", "os", ".", "path", ".", "exists", "(", "results_file", ")", ":", "\n", "                    ", "os", ".", "remove", "(", "results_file", ")", "\n", "", "", "", "fig_dir", "=", "os", ".", "path", ".", "join", "(", "cat_log_dir", ",", "'plots'", ")", "\n", "if", "not", "os", ".", "path", ".", "exists", "(", "fig_dir", ")", ":", "\n", "            ", "os", ".", "makedirs", "(", "fig_dir", ",", "exist_ok", "=", "True", ")", "\n", "", "return", "cat_log_dir", ",", "fig_dir", "\n", "\n"]], "home.repos.pwc.inspect_result.applied-ai-lab_zero-shot-pose.method.zero_shot_pose.ZeroShotPoseMethod.solve_umeyama_ransac": [[435, 455], ["skimage.measure.ransac", "pytorch3d.transforms.Rotate", "pytorch3d.transforms.Translate", "pytorch3d.transforms.Scale", "pytorch3d.transforms.Scale.compose", "torch.Tensor().unsqueeze", "torch.tensor", "pytorch3d.transforms.Rotate.compose", "torch.Tensor"], "methods", ["None"], ["", "def", "solve_umeyama_ransac", "(", "self", ",", "world_corr1", ",", "world_corr2", ")", ":", "\n", "        ", "rbt_model", ",", "inliers", "=", "ransac", "(", "data", "=", "(", "world_corr1", ",", "world_corr2", ")", ",", "\n", "model_class", "=", "RigidBodyUmeyama", ",", "\n", "min_samples", "=", "4", ",", "\n", "residual_threshold", "=", "self", ".", "ransac_thresh", ",", "\n", "# max_trials=10000)", "\n", "max_trials", "=", "1000", ")", "\n", "# print(f\"{sum(inliers)} inliers from {self.num_correspondences} points\")", "\n", "# if rbt_model.lam == 1:", "\n", "#     print(\"UMEYAMA RETURNED IDENTITY AS FALLBACK\")", "\n", "R", "=", "rbt_model", ".", "T", "[", ":", "3", ",", ":", "3", "]", "/", "rbt_model", ".", "lam", "\n", "t", "=", "rbt_model", ".", "T", "[", ":", "3", ",", "3", ":", "]", "\n", "scale", "=", "rbt_model", ".", "lam", "\n", "\n", "R_", "=", "Rotate", "(", "torch", ".", "Tensor", "(", "R", ".", "T", ")", ".", "unsqueeze", "(", "0", ")", ")", "\n", "T_", "=", "Translate", "(", "torch", ".", "tensor", "(", "t", ".", "T", ")", ")", "\n", "S_", "=", "Scale", "(", "scale", ")", "\n", "# trans21 = get_world_to_view_transform(torch.Tensor(R.T).unsqueeze(0), torch.tensor(t.T))", "\n", "trans21", "=", "S_", ".", "compose", "(", "R_", ".", "compose", "(", "T_", ")", ")", "\n", "return", "trans21", "", "", "", ""]], "home.repos.pwc.inspect_result.applied-ai-lab_zero-shot-pose.method.vision_transformer_flexible.DropPath.__init__": [[46, 49], ["torch.Module.__init__"], "methods", ["home.repos.pwc.inspect_result.applied-ai-lab_zero-shot-pose.method.vision_transformer_flexible.DINOHead.__init__"], ["def", "__init__", "(", "self", ",", "drop_prob", "=", "None", ")", ":", "\n", "        ", "super", "(", "DropPath", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "drop_prob", "=", "drop_prob", "\n", "\n"]], "home.repos.pwc.inspect_result.applied-ai-lab_zero-shot-pose.method.vision_transformer_flexible.DropPath.forward": [[50, 52], ["vision_transformer_flexible.drop_path"], "methods", ["home.repos.pwc.inspect_result.applied-ai-lab_zero-shot-pose.method.vision_transformer_flexible.drop_path"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "return", "drop_path", "(", "x", ",", "self", ".", "drop_prob", ",", "self", ".", "training", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.applied-ai-lab_zero-shot-pose.method.vision_transformer_flexible.Mlp.__init__": [[55, 63], ["torch.Module.__init__", "torch.Linear", "torch.Linear", "act_layer", "torch.Linear", "torch.Linear", "torch.Dropout", "torch.Dropout"], "methods", ["home.repos.pwc.inspect_result.applied-ai-lab_zero-shot-pose.method.vision_transformer_flexible.DINOHead.__init__"], ["    ", "def", "__init__", "(", "self", ",", "in_features", ",", "hidden_features", "=", "None", ",", "out_features", "=", "None", ",", "act_layer", "=", "nn", ".", "GELU", ",", "drop", "=", "0.", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "out_features", "=", "out_features", "or", "in_features", "\n", "hidden_features", "=", "hidden_features", "or", "in_features", "\n", "self", ".", "fc1", "=", "nn", ".", "Linear", "(", "in_features", ",", "hidden_features", ")", "\n", "self", ".", "act", "=", "act_layer", "(", ")", "\n", "self", ".", "fc2", "=", "nn", ".", "Linear", "(", "hidden_features", ",", "out_features", ")", "\n", "self", ".", "drop", "=", "nn", ".", "Dropout", "(", "drop", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.applied-ai-lab_zero-shot-pose.method.vision_transformer_flexible.Mlp.forward": [[64, 71], ["vision_transformer_flexible.Mlp.fc1", "vision_transformer_flexible.Mlp.act", "vision_transformer_flexible.Mlp.drop", "vision_transformer_flexible.Mlp.fc2", "vision_transformer_flexible.Mlp.drop"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "x", "=", "self", ".", "fc1", "(", "x", ")", "\n", "x", "=", "self", ".", "act", "(", "x", ")", "\n", "x", "=", "self", ".", "drop", "(", "x", ")", "\n", "x", "=", "self", ".", "fc2", "(", "x", ")", "\n", "x", "=", "self", ".", "drop", "(", "x", ")", "\n", "return", "x", "\n", "\n"]], "home.repos.pwc.inspect_result.applied-ai-lab_zero-shot-pose.method.vision_transformer_flexible.Attention.__init__": [[74, 84], ["torch.Module.__init__", "torch.Linear", "torch.Linear", "torch.Dropout", "torch.Dropout", "torch.Linear", "torch.Linear", "torch.Dropout", "torch.Dropout"], "methods", ["home.repos.pwc.inspect_result.applied-ai-lab_zero-shot-pose.method.vision_transformer_flexible.DINOHead.__init__"], ["    ", "def", "__init__", "(", "self", ",", "dim", ",", "num_heads", "=", "8", ",", "qkv_bias", "=", "False", ",", "qk_scale", "=", "None", ",", "attn_drop", "=", "0.", ",", "proj_drop", "=", "0.", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "num_heads", "=", "num_heads", "\n", "head_dim", "=", "dim", "//", "num_heads", "\n", "self", ".", "scale", "=", "qk_scale", "or", "head_dim", "**", "-", "0.5", "\n", "\n", "self", ".", "qkv", "=", "nn", ".", "Linear", "(", "dim", ",", "dim", "*", "3", ",", "bias", "=", "qkv_bias", ")", "\n", "self", ".", "attn_drop", "=", "nn", ".", "Dropout", "(", "attn_drop", ")", "\n", "self", ".", "proj", "=", "nn", ".", "Linear", "(", "dim", ",", "dim", ")", "\n", "self", ".", "proj_drop", "=", "nn", ".", "Dropout", "(", "proj_drop", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.applied-ai-lab_zero-shot-pose.method.vision_transformer_flexible.Attention.forward": [[85, 102], ["vision_transformer_flexible.Attention.qkv().reshape().permute", "vision_transformer_flexible.Attention.softmax", "vision_transformer_flexible.Attention.attn_drop", "vision_transformer_flexible.Attention.proj", "vision_transformer_flexible.Attention.proj_drop", "vision_transformer_flexible.Attention.qkv().reshape", "k.transpose", "vision_transformer_flexible.Attention.qkv"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "x", ",", "return_qkv", "=", "False", ")", ":", "\n", "        ", "B", ",", "N", ",", "C", "=", "x", ".", "shape", "\n", "qkv", "=", "self", ".", "qkv", "(", "x", ")", ".", "reshape", "(", "B", ",", "N", ",", "3", ",", "self", ".", "num_heads", ",", "C", "//", "self", ".", "num_heads", ")", ".", "permute", "(", "2", ",", "0", ",", "3", ",", "1", ",", "4", ")", "\n", "q", ",", "k", ",", "v", "=", "qkv", "[", "0", "]", ",", "qkv", "[", "1", "]", ",", "qkv", "[", "2", "]", "\n", "\n", "attn", "=", "(", "q", "@", "k", ".", "transpose", "(", "-", "2", ",", "-", "1", ")", ")", "*", "self", ".", "scale", "\n", "attn", "=", "attn", ".", "softmax", "(", "dim", "=", "-", "1", ")", "\n", "attn", "=", "self", ".", "attn_drop", "(", "attn", ")", "\n", "\n", "x", "=", "(", "attn", "@", "v", ")", ".", "transpose", "(", "1", ",", "2", ")", ".", "reshape", "(", "B", ",", "N", ",", "C", ")", "\n", "x", "=", "self", ".", "proj", "(", "x", ")", "\n", "x", "=", "self", ".", "proj_drop", "(", "x", ")", "\n", "\n", "if", "return_qkv", ":", "\n", "            ", "return", "x", ",", "attn", ",", "(", "q", ",", "k", ",", "v", ")", "\n", "", "else", ":", "\n", "            ", "return", "x", ",", "attn", "\n", "\n"]], "home.repos.pwc.inspect_result.applied-ai-lab_zero-shot-pose.method.vision_transformer_flexible.Block.__init__": [[105, 115], ["torch.Module.__init__", "norm_layer", "vision_transformer_flexible.Attention", "norm_layer", "int", "vision_transformer_flexible.Mlp", "vision_transformer_flexible.DropPath", "torch.Identity", "torch.Identity"], "methods", ["home.repos.pwc.inspect_result.applied-ai-lab_zero-shot-pose.method.vision_transformer_flexible.DINOHead.__init__"], ["    ", "def", "__init__", "(", "self", ",", "dim", ",", "num_heads", ",", "mlp_ratio", "=", "4.", ",", "qkv_bias", "=", "False", ",", "qk_scale", "=", "None", ",", "drop", "=", "0.", ",", "attn_drop", "=", "0.", ",", "\n", "drop_path", "=", "0.", ",", "act_layer", "=", "nn", ".", "GELU", ",", "norm_layer", "=", "nn", ".", "LayerNorm", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "norm1", "=", "norm_layer", "(", "dim", ")", "\n", "self", ".", "attn", "=", "Attention", "(", "\n", "dim", ",", "num_heads", "=", "num_heads", ",", "qkv_bias", "=", "qkv_bias", ",", "qk_scale", "=", "qk_scale", ",", "attn_drop", "=", "attn_drop", ",", "proj_drop", "=", "drop", ")", "\n", "self", ".", "drop_path", "=", "DropPath", "(", "drop_path", ")", "if", "drop_path", ">", "0.", "else", "nn", ".", "Identity", "(", ")", "\n", "self", ".", "norm2", "=", "norm_layer", "(", "dim", ")", "\n", "mlp_hidden_dim", "=", "int", "(", "dim", "*", "mlp_ratio", ")", "\n", "self", ".", "mlp", "=", "Mlp", "(", "in_features", "=", "dim", ",", "hidden_features", "=", "mlp_hidden_dim", ",", "act_layer", "=", "act_layer", ",", "drop", "=", "drop", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.applied-ai-lab_zero-shot-pose.method.vision_transformer_flexible.Block.forward": [[116, 125], ["vision_transformer_flexible.Block.attn", "vision_transformer_flexible.Block.norm1", "vision_transformer_flexible.Block.drop_path", "vision_transformer_flexible.Block.drop_path", "vision_transformer_flexible.Block.mlp", "vision_transformer_flexible.Block.norm2"], "methods", ["home.repos.pwc.inspect_result.applied-ai-lab_zero-shot-pose.method.vision_transformer_flexible.drop_path", "home.repos.pwc.inspect_result.applied-ai-lab_zero-shot-pose.method.vision_transformer_flexible.drop_path"], ["", "def", "forward", "(", "self", ",", "x", ",", "return_attention", "=", "False", ")", ":", "\n", "        ", "y", ",", "attn", "=", "self", ".", "attn", "(", "self", ".", "norm1", "(", "x", ")", ")", "\n", "x", "=", "x", "+", "self", ".", "drop_path", "(", "y", ")", "\n", "x", "=", "x", "+", "self", ".", "drop_path", "(", "self", ".", "mlp", "(", "self", ".", "norm2", "(", "x", ")", ")", ")", "\n", "\n", "if", "return_attention", ":", "\n", "            ", "return", "x", ",", "attn", "\n", "", "else", ":", "\n", "            ", "return", "x", "\n", "\n"]], "home.repos.pwc.inspect_result.applied-ai-lab_zero-shot-pose.method.vision_transformer_flexible.Block.return_qkv": [[126, 133], ["vision_transformer_flexible.Block.attn", "vision_transformer_flexible.Block.norm1", "vision_transformer_flexible.Block.drop_path", "vision_transformer_flexible.Block.drop_path", "vision_transformer_flexible.Block.mlp", "vision_transformer_flexible.Block.norm2"], "methods", ["home.repos.pwc.inspect_result.applied-ai-lab_zero-shot-pose.method.vision_transformer_flexible.drop_path", "home.repos.pwc.inspect_result.applied-ai-lab_zero-shot-pose.method.vision_transformer_flexible.drop_path"], ["", "", "def", "return_qkv", "(", "self", ",", "x", ")", ":", "\n", "\n", "        ", "y", ",", "attn", ",", "(", "q", ",", "k", ",", "v", ")", "=", "self", ".", "attn", "(", "self", ".", "norm1", "(", "x", ")", ",", "return_qkv", "=", "True", ")", "\n", "x", "=", "x", "+", "self", ".", "drop_path", "(", "y", ")", "\n", "x", "=", "x", "+", "self", ".", "drop_path", "(", "self", ".", "mlp", "(", "self", ".", "norm2", "(", "x", ")", ")", ")", "\n", "\n", "return", "x", ",", "attn", ",", "(", "q", ",", "k", ",", "v", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.applied-ai-lab_zero-shot-pose.method.vision_transformer_flexible.PatchEmbed.__init__": [[137, 145], ["torch.Module.__init__", "torch.Conv2d", "torch.Conv2d"], "methods", ["home.repos.pwc.inspect_result.applied-ai-lab_zero-shot-pose.method.vision_transformer_flexible.DINOHead.__init__"], ["def", "__init__", "(", "self", ",", "img_size", "=", "224", ",", "patch_size", "=", "16", ",", "in_chans", "=", "3", ",", "embed_dim", "=", "768", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "num_patches", "=", "(", "img_size", "//", "patch_size", ")", "*", "(", "img_size", "//", "patch_size", ")", "\n", "self", ".", "img_size", "=", "img_size", "\n", "self", ".", "patch_size", "=", "patch_size", "\n", "self", ".", "num_patches", "=", "num_patches", "\n", "\n", "self", ".", "proj", "=", "nn", ".", "Conv2d", "(", "in_chans", ",", "embed_dim", ",", "kernel_size", "=", "patch_size", ",", "stride", "=", "patch_size", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.applied-ai-lab_zero-shot-pose.method.vision_transformer_flexible.PatchEmbed.forward": [[146, 150], ["vision_transformer_flexible.PatchEmbed.proj().flatten().transpose", "vision_transformer_flexible.PatchEmbed.proj().flatten", "vision_transformer_flexible.PatchEmbed.proj"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "B", ",", "C", ",", "H", ",", "W", "=", "x", ".", "shape", "\n", "x", "=", "self", ".", "proj", "(", "x", ")", ".", "flatten", "(", "2", ")", ".", "transpose", "(", "1", ",", "2", ")", "\n", "return", "x", "\n", "\n"]], "home.repos.pwc.inspect_result.applied-ai-lab_zero-shot-pose.method.vision_transformer_flexible.VisionTransformer.__init__": [[154, 182], ["torch.Module.__init__", "vision_transformer_flexible.PatchEmbed", "torch.Parameter", "torch.Parameter", "torch.Parameter", "torch.Parameter", "torch.Dropout", "torch.Dropout", "torch.ModuleList", "torch.ModuleList", "norm_layer", "torch.nn.init.trunc_normal_", "torch.nn.init.trunc_normal_", "torch.nn.init.trunc_normal_", "torch.nn.init.trunc_normal_", "vision_transformer_flexible.VisionTransformer.apply", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "x.item", "torch.Linear", "torch.Linear", "torch.Identity", "torch.Identity", "torch.linspace", "torch.linspace", "torch.linspace", "torch.linspace", "vision_transformer_flexible.Block", "range"], "methods", ["home.repos.pwc.inspect_result.applied-ai-lab_zero-shot-pose.method.vision_transformer_flexible.DINOHead.__init__"], ["def", "__init__", "(", "self", ",", "img_size", "=", "[", "224", "]", ",", "patch_size", "=", "16", ",", "in_chans", "=", "3", ",", "num_classes", "=", "0", ",", "embed_dim", "=", "768", ",", "depth", "=", "12", ",", "\n", "num_heads", "=", "12", ",", "mlp_ratio", "=", "4.", ",", "qkv_bias", "=", "False", ",", "qk_scale", "=", "None", ",", "drop_rate", "=", "0.", ",", "attn_drop_rate", "=", "0.", ",", "\n", "drop_path_rate", "=", "0.", ",", "norm_layer", "=", "nn", ".", "LayerNorm", ",", "**", "kwargs", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "num_features", "=", "self", ".", "embed_dim", "=", "embed_dim", "\n", "\n", "self", ".", "patch_embed", "=", "PatchEmbed", "(", "\n", "img_size", "=", "img_size", "[", "0", "]", ",", "patch_size", "=", "patch_size", ",", "in_chans", "=", "in_chans", ",", "embed_dim", "=", "embed_dim", ")", "\n", "num_patches", "=", "self", ".", "patch_embed", ".", "num_patches", "\n", "\n", "self", ".", "cls_token", "=", "nn", ".", "Parameter", "(", "torch", ".", "zeros", "(", "1", ",", "1", ",", "embed_dim", ")", ")", "\n", "self", ".", "pos_embed", "=", "nn", ".", "Parameter", "(", "torch", ".", "zeros", "(", "1", ",", "num_patches", "+", "1", ",", "embed_dim", ")", ")", "\n", "self", ".", "pos_drop", "=", "nn", ".", "Dropout", "(", "p", "=", "drop_rate", ")", "\n", "\n", "dpr", "=", "[", "x", ".", "item", "(", ")", "for", "x", "in", "torch", ".", "linspace", "(", "0", ",", "drop_path_rate", ",", "depth", ")", "]", "# stochastic depth decay rule", "\n", "self", ".", "blocks", "=", "nn", ".", "ModuleList", "(", "[", "\n", "Block", "(", "\n", "dim", "=", "embed_dim", ",", "num_heads", "=", "num_heads", ",", "mlp_ratio", "=", "mlp_ratio", ",", "qkv_bias", "=", "qkv_bias", ",", "qk_scale", "=", "qk_scale", ",", "\n", "drop", "=", "drop_rate", ",", "attn_drop", "=", "attn_drop_rate", ",", "drop_path", "=", "dpr", "[", "i", "]", ",", "norm_layer", "=", "norm_layer", ")", "\n", "for", "i", "in", "range", "(", "depth", ")", "]", ")", "\n", "self", ".", "norm", "=", "norm_layer", "(", "embed_dim", ")", "\n", "\n", "# Classifier head", "\n", "self", ".", "head", "=", "nn", ".", "Linear", "(", "embed_dim", ",", "num_classes", ")", "if", "num_classes", ">", "0", "else", "nn", ".", "Identity", "(", ")", "\n", "\n", "trunc_normal_", "(", "self", ".", "pos_embed", ",", "std", "=", ".02", ")", "\n", "trunc_normal_", "(", "self", ".", "cls_token", ",", "std", "=", ".02", ")", "\n", "self", ".", "apply", "(", "self", ".", "_init_weights", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.applied-ai-lab_zero-shot-pose.method.vision_transformer_flexible.VisionTransformer._init_weights": [[183, 191], ["isinstance", "torch.nn.init.trunc_normal_", "torch.nn.init.trunc_normal_", "isinstance", "isinstance", "torch.init.constant_", "torch.init.constant_", "torch.init.constant_", "torch.init.constant_", "torch.init.constant_", "torch.init.constant_"], "methods", ["None"], ["", "def", "_init_weights", "(", "self", ",", "m", ")", ":", "\n", "        ", "if", "isinstance", "(", "m", ",", "nn", ".", "Linear", ")", ":", "\n", "            ", "trunc_normal_", "(", "m", ".", "weight", ",", "std", "=", ".02", ")", "\n", "if", "isinstance", "(", "m", ",", "nn", ".", "Linear", ")", "and", "m", ".", "bias", "is", "not", "None", ":", "\n", "                ", "nn", ".", "init", ".", "constant_", "(", "m", ".", "bias", ",", "0", ")", "\n", "", "", "elif", "isinstance", "(", "m", ",", "nn", ".", "LayerNorm", ")", ":", "\n", "            ", "nn", ".", "init", ".", "constant_", "(", "m", ".", "bias", ",", "0", ")", "\n", "nn", ".", "init", ".", "constant_", "(", "m", ".", "weight", ",", "1.0", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.applied-ai-lab_zero-shot-pose.method.vision_transformer_flexible.VisionTransformer.interpolate_pos_encoding": [[192, 213], ["torch.functional.interpolate", "torch.functional.interpolate", "patch_pos_embed.permute().view.permute().view.permute().view", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "patch_pos_embed.permute().view.permute().view.reshape().permute", "int", "int", "patch_pos_embed.permute().view.permute().view.permute", "class_pos_embed.unsqueeze", "patch_pos_embed.permute().view.permute().view.reshape", "int", "int", "math.sqrt", "math.sqrt", "math.sqrt", "math.sqrt"], "methods", ["None"], ["", "", "def", "interpolate_pos_encoding", "(", "self", ",", "x", ",", "w", ",", "h", ")", ":", "\n", "        ", "npatch", "=", "x", ".", "shape", "[", "1", "]", "-", "1", "\n", "N", "=", "self", ".", "pos_embed", ".", "shape", "[", "1", "]", "-", "1", "\n", "if", "npatch", "==", "N", "and", "w", "==", "h", ":", "\n", "            ", "return", "self", ".", "pos_embed", "\n", "", "class_pos_embed", "=", "self", ".", "pos_embed", "[", ":", ",", "0", "]", "\n", "patch_pos_embed", "=", "self", ".", "pos_embed", "[", ":", ",", "1", ":", "]", "\n", "dim", "=", "x", ".", "shape", "[", "-", "1", "]", "\n", "w0", "=", "w", "//", "self", ".", "patch_embed", ".", "patch_size", "\n", "h0", "=", "h", "//", "self", ".", "patch_embed", ".", "patch_size", "\n", "# we add a small number to avoid floating point error in the interpolation", "\n", "# see discussion at https://github.com/facebookresearch/dino/issues/8", "\n", "w0", ",", "h0", "=", "w0", "+", "0.1", ",", "h0", "+", "0.1", "\n", "patch_pos_embed", "=", "nn", ".", "functional", ".", "interpolate", "(", "\n", "patch_pos_embed", ".", "reshape", "(", "1", ",", "int", "(", "math", ".", "sqrt", "(", "N", ")", ")", ",", "int", "(", "math", ".", "sqrt", "(", "N", ")", ")", ",", "dim", ")", ".", "permute", "(", "0", ",", "3", ",", "1", ",", "2", ")", ",", "\n", "scale_factor", "=", "(", "w0", "/", "math", ".", "sqrt", "(", "N", ")", ",", "h0", "/", "math", ".", "sqrt", "(", "N", ")", ")", ",", "\n", "mode", "=", "'bicubic'", ",", "\n", ")", "\n", "assert", "int", "(", "w0", ")", "==", "patch_pos_embed", ".", "shape", "[", "-", "2", "]", "and", "int", "(", "h0", ")", "==", "patch_pos_embed", ".", "shape", "[", "-", "1", "]", "\n", "patch_pos_embed", "=", "patch_pos_embed", ".", "permute", "(", "0", ",", "2", ",", "3", ",", "1", ")", ".", "view", "(", "1", ",", "-", "1", ",", "dim", ")", "\n", "return", "torch", ".", "cat", "(", "(", "class_pos_embed", ".", "unsqueeze", "(", "0", ")", ",", "patch_pos_embed", ")", ",", "dim", "=", "1", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.applied-ai-lab_zero-shot-pose.method.vision_transformer_flexible.VisionTransformer.prepare_tokens": [[214, 226], ["vision_transformer_flexible.VisionTransformer.patch_embed", "vision_transformer_flexible.VisionTransformer.cls_token.expand", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "vision_transformer_flexible.VisionTransformer.pos_drop", "vision_transformer_flexible.VisionTransformer.interpolate_pos_encoding"], "methods", ["home.repos.pwc.inspect_result.applied-ai-lab_zero-shot-pose.method.vision_transformer_flexible.VisionTransformer.interpolate_pos_encoding"], ["", "def", "prepare_tokens", "(", "self", ",", "x", ")", ":", "\n", "        ", "B", ",", "nc", ",", "w", ",", "h", "=", "x", ".", "shape", "\n", "x", "=", "self", ".", "patch_embed", "(", "x", ")", "# patch linear embedding", "\n", "\n", "# add the [CLS] token to the embed patch tokens", "\n", "cls_tokens", "=", "self", ".", "cls_token", ".", "expand", "(", "B", ",", "-", "1", ",", "-", "1", ")", "\n", "x", "=", "torch", ".", "cat", "(", "(", "cls_tokens", ",", "x", ")", ",", "dim", "=", "1", ")", "\n", "\n", "# add positional encoding to each token", "\n", "x", "=", "x", "+", "self", ".", "interpolate_pos_encoding", "(", "x", ",", "w", ",", "h", ")", "\n", "\n", "return", "self", ".", "pos_drop", "(", "x", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.applied-ai-lab_zero-shot-pose.method.vision_transformer_flexible.VisionTransformer.forward": [[227, 237], ["vision_transformer_flexible.VisionTransformer.prepare_tokens", "vision_transformer_flexible.VisionTransformer.norm", "blk"], "methods", ["home.repos.pwc.inspect_result.applied-ai-lab_zero-shot-pose.method.vision_transformer_flexible.VisionTransformer.prepare_tokens"], ["", "def", "forward", "(", "self", ",", "x", ",", "return_all_patches", "=", "False", ")", ":", "\n", "        ", "x", "=", "self", ".", "prepare_tokens", "(", "x", ")", "\n", "for", "blk", "in", "self", ".", "blocks", ":", "\n", "            ", "x", "=", "blk", "(", "x", ")", "\n", "", "x", "=", "self", ".", "norm", "(", "x", ")", "\n", "\n", "if", "return_all_patches", ":", "\n", "            ", "return", "x", "\n", "", "else", ":", "\n", "            ", "return", "x", "[", ":", ",", "0", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.applied-ai-lab_zero-shot-pose.method.vision_transformer_flexible.VisionTransformer.get_last_selfattention": [[238, 248], ["vision_transformer_flexible.VisionTransformer.prepare_tokens", "enumerate", "blk", "blk", "vision_transformer_flexible.VisionTransformer.norm", "len"], "methods", ["home.repos.pwc.inspect_result.applied-ai-lab_zero-shot-pose.method.vision_transformer_flexible.VisionTransformer.prepare_tokens"], ["", "", "def", "get_last_selfattention", "(", "self", ",", "x", ")", ":", "\n", "        ", "x", "=", "self", ".", "prepare_tokens", "(", "x", ")", "\n", "for", "i", ",", "blk", "in", "enumerate", "(", "self", ".", "blocks", ")", ":", "\n", "            ", "if", "i", "<", "len", "(", "self", ".", "blocks", ")", "-", "1", ":", "\n", "                ", "x", "=", "blk", "(", "x", ")", "\n", "", "else", ":", "\n", "# return attention of the last block", "\n", "                ", "x", ",", "attn", "=", "blk", "(", "x", ",", "return_attention", "=", "True", ")", "\n", "x", "=", "self", ".", "norm", "(", "x", ")", "\n", "return", "x", ",", "attn", "\n", "\n"]], "home.repos.pwc.inspect_result.applied-ai-lab_zero-shot-pose.method.vision_transformer_flexible.VisionTransformer.get_specific_tokens": [[249, 263], ["vision_transformer_flexible.VisionTransformer.prepare_tokens", "enumerate", "blk.return_qkv"], "methods", ["home.repos.pwc.inspect_result.applied-ai-lab_zero-shot-pose.method.vision_transformer_flexible.VisionTransformer.prepare_tokens", "home.repos.pwc.inspect_result.applied-ai-lab_zero-shot-pose.method.vision_transformer_flexible.Block.return_qkv"], ["", "", "", "def", "get_specific_tokens", "(", "self", ",", "x", ",", "layers_to_return", "=", "(", "11", ",", ")", ")", ":", "\n", "\n", "        ", "x", "=", "self", ".", "prepare_tokens", "(", "x", ")", "\n", "to_return", "=", "{", "}", "\n", "\n", "for", "i", ",", "blk", "in", "enumerate", "(", "self", ".", "blocks", ")", ":", "\n", "            ", "x", ",", "attn", ",", "(", "q", ",", "k", ",", "v", ")", "=", "blk", ".", "return_qkv", "(", "x", ")", "\n", "\n", "if", "i", "in", "layers_to_return", ":", "\n", "                ", "to_return", "[", "i", "]", "=", "{", "\n", "'t'", ":", "x", ",", "'q'", ":", "q", ",", "'k'", ":", "k", ",", "'v'", ":", "v", ",", "'attn'", ":", "attn", "\n", "}", "\n", "\n", "", "", "return", "to_return", "\n", "\n"]], "home.repos.pwc.inspect_result.applied-ai-lab_zero-shot-pose.method.vision_transformer_flexible.VisionTransformer.get_intermediate_layers": [[265, 274], ["vision_transformer_flexible.VisionTransformer.prepare_tokens", "enumerate", "blk", "output.append", "len", "vision_transformer_flexible.VisionTransformer.norm"], "methods", ["home.repos.pwc.inspect_result.applied-ai-lab_zero-shot-pose.method.vision_transformer_flexible.VisionTransformer.prepare_tokens"], ["", "def", "get_intermediate_layers", "(", "self", ",", "x", ",", "n", "=", "1", ")", ":", "\n", "        ", "x", "=", "self", ".", "prepare_tokens", "(", "x", ")", "\n", "# we return the output tokens from the `n` last blocks", "\n", "output", "=", "[", "]", "\n", "for", "i", ",", "blk", "in", "enumerate", "(", "self", ".", "blocks", ")", ":", "\n", "            ", "x", "=", "blk", "(", "x", ")", "\n", "if", "len", "(", "self", ".", "blocks", ")", "-", "i", "<=", "n", ":", "\n", "                ", "output", ".", "append", "(", "self", ".", "norm", "(", "x", ")", ")", "\n", "", "", "return", "output", "\n", "\n"]], "home.repos.pwc.inspect_result.applied-ai-lab_zero-shot-pose.method.vision_transformer_flexible.DINOHead.__init__": [[298, 320], ["torch.Module.__init__", "max", "vision_transformer_flexible.DINOHead.apply", "torch.utils.weight_norm", "torch.utils.weight_norm", "vision_transformer_flexible.DINOHead.last_layer.weight_g.data.fill_", "torch.Linear", "torch.Linear", "layers.append", "range", "layers.append", "torch.Sequential", "torch.Sequential", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "layers.append", "torch.GELU", "torch.GELU", "layers.append", "layers.append", "torch.Linear", "torch.Linear", "torch.BatchNorm1d", "torch.BatchNorm1d", "torch.Linear", "torch.Linear", "layers.append", "torch.GELU", "torch.GELU", "torch.BatchNorm1d", "torch.BatchNorm1d"], "methods", ["home.repos.pwc.inspect_result.applied-ai-lab_zero-shot-pose.method.vision_transformer_flexible.DINOHead.__init__"], ["    ", "def", "__init__", "(", "self", ",", "in_dim", ",", "out_dim", ",", "use_bn", "=", "False", ",", "norm_last_layer", "=", "True", ",", "nlayers", "=", "3", ",", "hidden_dim", "=", "2048", ",", "bottleneck_dim", "=", "256", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "nlayers", "=", "max", "(", "nlayers", ",", "1", ")", "\n", "if", "nlayers", "==", "1", ":", "\n", "            ", "self", ".", "mlp", "=", "nn", ".", "Linear", "(", "in_dim", ",", "bottleneck_dim", ")", "\n", "", "else", ":", "\n", "            ", "layers", "=", "[", "nn", ".", "Linear", "(", "in_dim", ",", "hidden_dim", ")", "]", "\n", "if", "use_bn", ":", "\n", "                ", "layers", ".", "append", "(", "nn", ".", "BatchNorm1d", "(", "hidden_dim", ")", ")", "\n", "", "layers", ".", "append", "(", "nn", ".", "GELU", "(", ")", ")", "\n", "for", "_", "in", "range", "(", "nlayers", "-", "2", ")", ":", "\n", "                ", "layers", ".", "append", "(", "nn", ".", "Linear", "(", "hidden_dim", ",", "hidden_dim", ")", ")", "\n", "if", "use_bn", ":", "\n", "                    ", "layers", ".", "append", "(", "nn", ".", "BatchNorm1d", "(", "hidden_dim", ")", ")", "\n", "", "layers", ".", "append", "(", "nn", ".", "GELU", "(", ")", ")", "\n", "", "layers", ".", "append", "(", "nn", ".", "Linear", "(", "hidden_dim", ",", "bottleneck_dim", ")", ")", "\n", "self", ".", "mlp", "=", "nn", ".", "Sequential", "(", "*", "layers", ")", "\n", "", "self", ".", "apply", "(", "self", ".", "_init_weights", ")", "\n", "self", ".", "last_layer", "=", "nn", ".", "utils", ".", "weight_norm", "(", "nn", ".", "Linear", "(", "bottleneck_dim", ",", "out_dim", ",", "bias", "=", "False", ")", ")", "\n", "self", ".", "last_layer", ".", "weight_g", ".", "data", ".", "fill_", "(", "1", ")", "\n", "if", "norm_last_layer", ":", "\n", "            ", "self", ".", "last_layer", ".", "weight_g", ".", "requires_grad", "=", "False", "\n", "\n"]], "home.repos.pwc.inspect_result.applied-ai-lab_zero-shot-pose.method.vision_transformer_flexible.DINOHead._init_weights": [[321, 326], ["isinstance", "torch.nn.init.trunc_normal_", "torch.nn.init.trunc_normal_", "isinstance", "torch.init.constant_", "torch.init.constant_"], "methods", ["None"], ["", "", "def", "_init_weights", "(", "self", ",", "m", ")", ":", "\n", "        ", "if", "isinstance", "(", "m", ",", "nn", ".", "Linear", ")", ":", "\n", "            ", "trunc_normal_", "(", "m", ".", "weight", ",", "std", "=", ".02", ")", "\n", "if", "isinstance", "(", "m", ",", "nn", ".", "Linear", ")", "and", "m", ".", "bias", "is", "not", "None", ":", "\n", "                ", "nn", ".", "init", ".", "constant_", "(", "m", ".", "bias", ",", "0", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.applied-ai-lab_zero-shot-pose.method.vision_transformer_flexible.DINOHead.forward": [[327, 332], ["vision_transformer_flexible.DINOHead.mlp", "torch.functional.normalize", "torch.functional.normalize", "vision_transformer_flexible.DINOHead.last_layer"], "methods", ["None"], ["", "", "", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "x", "=", "self", ".", "mlp", "(", "x", ")", "\n", "x", "=", "nn", ".", "functional", ".", "normalize", "(", "x", ",", "dim", "=", "-", "1", ",", "p", "=", "2", ")", "\n", "x", "=", "self", ".", "last_layer", "(", "x", ")", "\n", "return", "x", "\n", "\n"]], "home.repos.pwc.inspect_result.applied-ai-lab_zero-shot-pose.method.vision_transformer_flexible.drop_path": [[32, 41], ["random_tensor.floor_", "torch.rand", "torch.rand", "x.div"], "function", ["None"], ["def", "drop_path", "(", "x", ",", "drop_prob", ":", "float", "=", "0.", ",", "training", ":", "bool", "=", "False", ")", ":", "\n", "    ", "if", "drop_prob", "==", "0.", "or", "not", "training", ":", "\n", "        ", "return", "x", "\n", "", "keep_prob", "=", "1", "-", "drop_prob", "\n", "shape", "=", "(", "x", ".", "shape", "[", "0", "]", ",", ")", "+", "(", "1", ",", ")", "*", "(", "x", ".", "ndim", "-", "1", ")", "# work with diff dim tensors, not just 2D ConvNets", "\n", "random_tensor", "=", "keep_prob", "+", "torch", ".", "rand", "(", "shape", ",", "dtype", "=", "x", ".", "dtype", ",", "device", "=", "x", ".", "device", ")", "\n", "random_tensor", ".", "floor_", "(", ")", "# binarize", "\n", "output", "=", "x", ".", "div", "(", "keep_prob", ")", "*", "random_tensor", "\n", "return", "output", "\n", "\n"]], "home.repos.pwc.inspect_result.applied-ai-lab_zero-shot-pose.method.vision_transformer_flexible.vit_tiny": [[276, 281], ["vision_transformer_flexible.VisionTransformer", "functools.partial"], "function", ["None"], ["", "", "def", "vit_tiny", "(", "patch_size", "=", "16", ",", "**", "kwargs", ")", ":", "\n", "    ", "model", "=", "VisionTransformer", "(", "\n", "patch_size", "=", "patch_size", ",", "embed_dim", "=", "192", ",", "depth", "=", "12", ",", "num_heads", "=", "3", ",", "mlp_ratio", "=", "4", ",", "\n", "qkv_bias", "=", "True", ",", "norm_layer", "=", "partial", "(", "nn", ".", "LayerNorm", ",", "eps", "=", "1e-6", ")", ",", "**", "kwargs", ")", "\n", "return", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.applied-ai-lab_zero-shot-pose.method.vision_transformer_flexible.vit_small": [[283, 288], ["vision_transformer_flexible.VisionTransformer", "functools.partial"], "function", ["None"], ["", "def", "vit_small", "(", "patch_size", "=", "16", ",", "**", "kwargs", ")", ":", "\n", "    ", "model", "=", "VisionTransformer", "(", "\n", "patch_size", "=", "patch_size", ",", "embed_dim", "=", "384", ",", "depth", "=", "12", ",", "num_heads", "=", "6", ",", "mlp_ratio", "=", "4", ",", "\n", "qkv_bias", "=", "True", ",", "norm_layer", "=", "partial", "(", "nn", ".", "LayerNorm", ",", "eps", "=", "1e-6", ")", ",", "**", "kwargs", ")", "\n", "return", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.applied-ai-lab_zero-shot-pose.method.vision_transformer_flexible.vit_base": [[290, 295], ["vision_transformer_flexible.VisionTransformer", "functools.partial"], "function", ["None"], ["", "def", "vit_base", "(", "patch_size", "=", "16", ",", "**", "kwargs", ")", ":", "\n", "    ", "model", "=", "VisionTransformer", "(", "\n", "patch_size", "=", "patch_size", ",", "embed_dim", "=", "768", ",", "depth", "=", "12", ",", "num_heads", "=", "12", ",", "mlp_ratio", "=", "4", ",", "\n", "qkv_bias", "=", "True", ",", "norm_layer", "=", "partial", "(", "nn", ".", "LayerNorm", ",", "eps", "=", "1e-6", ")", ",", "**", "kwargs", ")", "\n", "return", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.applied-ai-lab_zero-shot-pose.method.vision_transformer_flexible.preprocess": [[333, 361], ["torchvision.transforms.Compose", "PIL.Image.open().convert", "transforms.Compose.", "type", "torchvision.transforms.ToTensor", "torchvision.transforms.Normalize", "PIL.Image.open", "torchvision.transforms.Resize", "torchvision.transforms.Resize"], "function", ["None"], ["", "", "def", "preprocess", "(", "image_path", ":", "Union", "[", "str", ",", "Path", "]", "=", "None", ",", "\n", "pil_image", ":", "Image", ".", "Image", "=", "None", ",", "\n", "load_size", ":", "Union", "[", "int", ",", "Tuple", "[", "int", ",", "int", "]", "]", "=", "None", ")", "->", "Tuple", "[", "torch", ".", "Tensor", ",", "Image", ".", "Image", "]", ":", "\n", "    ", "\"\"\"\n    Preprocesses an image before extraction.\n    :param image_path: path to image to be extracted.\n    :param load_size: optional. Size to resize image before the rest of preprocessing.\n    :return: a tuple containing:\n                (1) the preprocessed image as a tensor to insert the model of shape BxCxHxW.\n                (2) the pil image in relevant dimensions\n    \"\"\"", "\n", "mean", "=", "(", "0.485", ",", "0.456", ",", "0.406", ")", "\n", "std", "=", "(", "0.229", ",", "0.224", ",", "0.225", ")", "\n", "\n", "assert", "not", "(", "image_path", "and", "pil_image", ")", "\n", "if", "image_path", ":", "\n", "        ", "pil_image", "=", "Image", ".", "open", "(", "image_path", ")", ".", "convert", "(", "'RGB'", ")", "\n", "", "if", "load_size", "is", "not", "None", ":", "\n", "        ", "if", "type", "(", "load_size", ")", "==", "int", ":", "\n", "            ", "pil_image", "=", "transforms", ".", "Resize", "(", "load_size", ",", "max_size", "=", "300", ",", "interpolation", "=", "transforms", ".", "InterpolationMode", ".", "LANCZOS", ")", "(", "pil_image", ")", "\n", "", "else", ":", "\n", "            ", "pil_image", "=", "transforms", ".", "Resize", "(", "load_size", ",", "interpolation", "=", "transforms", ".", "InterpolationMode", ".", "LANCZOS", ")", "(", "pil_image", ")", "\n", "", "", "prep", "=", "transforms", ".", "Compose", "(", "[", "\n", "transforms", ".", "ToTensor", "(", ")", ",", "\n", "transforms", ".", "Normalize", "(", "mean", "=", "mean", ",", "std", "=", "std", ")", "\n", "]", ")", "\n", "prep_img", "=", "prep", "(", "pil_image", ")", "[", "None", ",", "...", "]", "\n", "return", "prep_img", ",", "pil_image", "\n", "", ""]], "home.repos.pwc.inspect_result.applied-ai-lab_zero-shot-pose.method.zero_shot_pose_utils.RigidBodyTransform.estimate": [[153, 155], ["zsp.method.rigid_body.least_squares_solution"], "methods", ["home.repos.pwc.inspect_result.applied-ai-lab_zero-shot-pose.method.rigid_body.least_squares_solution"], ["    ", "def", "estimate", "(", "self", ",", "world_corr1", ",", "world_corr2", ")", ":", "\n", "        ", "self", ".", "R", ",", "self", ".", "t", ",", "self", ".", "lam", "=", "least_squares_solution", "(", "world_corr1", ".", "T", ",", "world_corr2", ".", "T", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.applied-ai-lab_zero-shot-pose.method.zero_shot_pose_utils.RigidBodyTransform.residuals": [[156, 162], ["zero_shot_pose_utils.RigidBodyTransform.transform", "res.numpy", "torch.nn.PairwiseDistance", "torch.nn.PairwiseDistance", "torch.nn.PairwiseDistance", "torch.nn.PairwiseDistance", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor"], "methods", ["home.repos.pwc.inspect_result.applied-ai-lab_zero-shot-pose.method.zero_shot_pose_utils.RigidBodyUmeyama.transform"], ["", "def", "residuals", "(", "self", ",", "world_corr1", ",", "world_corr2", ")", ":", "\n", "# E(x2_i) = \u03bb*R*x1_i+t, {i = 1, ..., I}", "\n", "        ", "world_corr2_est", "=", "self", ".", "transform", "(", "world_corr1", ")", "\n", "res", "=", "torch", ".", "nn", ".", "PairwiseDistance", "(", "p", "=", "2", ")", "(", "torch", ".", "Tensor", "(", "world_corr2_est", ")", ",", "\n", "torch", ".", "Tensor", "(", "world_corr2", ")", ")", "\n", "return", "res", ".", "numpy", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.applied-ai-lab_zero-shot-pose.method.zero_shot_pose_utils.RigidBodyTransform.transform": [[163, 165], ["None"], "methods", ["None"], ["", "def", "transform", "(", "self", ",", "world_corr1", ")", ":", "\n", "        ", "return", "(", "self", ".", "lam", "*", "self", ".", "R", "@", "world_corr1", ".", "T", "+", "self", ".", "t", ")", ".", "T", "\n", "\n"]], "home.repos.pwc.inspect_result.applied-ai-lab_zero-shot-pose.method.zero_shot_pose_utils.RigidBodyUmeyama.estimate": [[169, 171], ["zsp.method.rigid_body.umeyama"], "methods", ["home.repos.pwc.inspect_result.applied-ai-lab_zero-shot-pose.method.rigid_body.umeyama"], ["    ", "def", "estimate", "(", "self", ",", "world_corr1", ",", "world_corr2", ")", ":", "\n", "        ", "self", ".", "T", ",", "self", ".", "lam", "=", "umeyama", "(", "world_corr1", ",", "world_corr2", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.applied-ai-lab_zero-shot-pose.method.zero_shot_pose_utils.RigidBodyUmeyama.residuals": [[172, 177], ["zero_shot_pose_utils.RigidBodyUmeyama.transform", "res.numpy", "torch.nn.PairwiseDistance", "torch.nn.PairwiseDistance", "torch.nn.PairwiseDistance", "torch.nn.PairwiseDistance", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor"], "methods", ["home.repos.pwc.inspect_result.applied-ai-lab_zero-shot-pose.method.zero_shot_pose_utils.RigidBodyUmeyama.transform"], ["", "def", "residuals", "(", "self", ",", "world_corr1", ",", "world_corr2", ")", ":", "\n", "        ", "world_corr2_est", "=", "self", ".", "transform", "(", "world_corr1", ")", "\n", "res", "=", "torch", ".", "nn", ".", "PairwiseDistance", "(", "p", "=", "2", ")", "(", "torch", ".", "Tensor", "(", "world_corr2_est", ")", ",", "\n", "torch", ".", "Tensor", "(", "world_corr2", ")", ")", "\n", "return", "res", ".", "numpy", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.applied-ai-lab_zero-shot-pose.method.zero_shot_pose_utils.RigidBodyUmeyama.transform": [[178, 182], ["numpy.vstack", "numpy.ones", "len"], "methods", ["None"], ["", "def", "transform", "(", "self", ",", "world_corr1", ")", ":", "\n", "        ", "w1_homo", "=", "np", ".", "vstack", "(", "(", "world_corr1", ".", "T", ",", "np", ".", "ones", "(", "(", "1", ",", "(", "len", "(", "world_corr1", ")", ")", ")", ")", ")", ")", "\n", "transformed", "=", "self", ".", "T", "@", "w1_homo", "\n", "return", "(", "transformed", "[", ":", "3", ",", ":", "]", ")", ".", "T", "\n", "\n"]], "home.repos.pwc.inspect_result.applied-ai-lab_zero-shot-pose.method.zero_shot_pose_utils.normalize_cyclical_dists": [[15, 37], ["dists.view.size", "dists.view.view", "dists.view.view", "dists.view.min", "dists.view.max", "dists.view.min", "dists.view.max"], "function", ["None"], ["def", "normalize_cyclical_dists", "(", "dists", ")", ":", "\n", "\n", "# Assume dists are in the format B x H x W", "\n", "    ", "b", ",", "h", ",", "w", "=", "dists", ".", "size", "(", ")", "\n", "dists", "=", "dists", ".", "view", "(", "b", ",", "h", "*", "w", ")", "\n", "\n", "# Normalize to [0, 1]", "\n", "dists", "-=", "dists", ".", "min", "(", "dim", "=", "-", "1", ")", "[", "0", "]", "[", ":", ",", "None", "]", "\n", "dists", "/=", "dists", ".", "max", "(", "dim", "=", "-", "1", ")", "[", "0", "]", "[", ":", ",", "None", "]", "\n", "\n", "# Hack to find the mininimum non-negligible value", "\n", "dists", "-=", "0.05", "\n", "dists", "[", "dists", "<", "0", "]", "=", "3", "\n", "\n", "# Re-Normalize to [0, 1]", "\n", "dists", "-=", "dists", ".", "min", "(", "dim", "=", "-", "1", ")", "[", "0", "]", "[", ":", ",", "None", "]", "\n", "dists", "[", "dists", ">", "1", "]", "=", "0", "\n", "dists", "/=", "dists", ".", "max", "(", "dim", "=", "-", "1", ")", "[", "0", "]", "[", ":", ",", "None", "]", "\n", "\n", "dists", "=", "dists", ".", "view", "(", "b", ",", "h", ",", "w", ")", "\n", "\n", "return", "dists", "\n", "\n"]], "home.repos.pwc.inspect_result.applied-ai-lab_zero-shot-pose.method.zero_shot_pose_utils.rank_target_images_by_global_sim": [[42, 54], ["F.normalize.size", "torch.normalize", "torch.normalize", "torch.matmul", "torch.matmul", "F.normalize.view", "F.normalize.permute"], "function", ["None"], ["", "def", "rank_target_images_by_global_sim", "(", "ref_global_feats", ",", "target_global_feats", ")", ":", "\n", "    ", "\"\"\"\n    Similarity from source image to target images with ViT global feature\n    \"\"\"", "\n", "b", ",", "n_tgt", ",", "d", "=", "target_global_feats", ".", "size", "(", ")", "\n", "\n", "ref_global_feats", "=", "F", ".", "normalize", "(", "ref_global_feats", ",", "dim", "=", "-", "1", ")", "\n", "target_global_feats", "=", "F", ".", "normalize", "(", "target_global_feats", ",", "dim", "=", "-", "1", ")", "\n", "\n", "sim", "=", "torch", ".", "matmul", "(", "ref_global_feats", ".", "view", "(", "b", ",", "1", ",", "d", ")", ",", "target_global_feats", ".", "permute", "(", "0", ",", "2", ",", "1", ")", ")", "\n", "\n", "return", "sim", "\n", "\n"]], "home.repos.pwc.inspect_result.applied-ai-lab_zero-shot-pose.method.zero_shot_pose_utils.batch_intersection_over_union": [[59, 85], ["None"], "function", ["None"], ["", "def", "batch_intersection_over_union", "(", "tensor_a", ",", "tensor_b", ",", "threshold", "=", "None", ")", ":", "\n", "\n", "    ", "\"\"\"\n    a is B x H x W\n    b is B x H x W\n    \"\"\"", "\n", "\n", "intersection", "=", "(", "tensor_a", "*", "tensor_b", ")", ".", "sum", "(", "dim", "=", "(", "-", "1", ",", "-", "2", ")", ")", "\n", "union", "=", "(", "tensor_a", "+", "tensor_b", ")", ".", "sum", "(", "dim", "=", "(", "-", "1", ",", "-", "2", ")", ")", "\n", "iou", "=", "2", "*", "(", "intersection", "/", "union", ")", "\n", "\n", "if", "threshold", "is", "None", ":", "\n", "\n", "        ", "return", "iou", "# Return shape (B,)", "\n", "\n", "", "else", ":", "\n", "\n", "        ", "tensor_a", "[", "tensor_a", "<", "threshold", "]", "=", "0", "\n", "tensor_b", "[", "tensor_b", "<", "threshold", "]", "=", "0", "\n", "\n", "tensor_a", "[", "tensor_a", ">=", "threshold", "]", "=", "1", "\n", "tensor_b", "[", "tensor_b", ">=", "threshold", "]", "=", "1", "\n", "\n", "intersection_map", "=", "tensor_a", "*", "tensor_b", "\n", "\n", "return", "iou", ",", "intersection_map", "\n", "\n"]], "home.repos.pwc.inspect_result.applied-ai-lab_zero-shot-pose.method.zero_shot_pose_utils.scale_points_from_patch": [[90, 94], ["None"], "function", ["None"], ["", "", "def", "scale_points_from_patch", "(", "points", ",", "vit_image_size", "=", "224", ",", "num_patches", "=", "28", ")", ":", "\n", "    ", "points", "=", "(", "points", "+", "0.5", ")", "/", "num_patches", "*", "vit_image_size", "\n", "\n", "return", "points", "\n", "\n"]], "home.repos.pwc.inspect_result.applied-ai-lab_zero-shot-pose.method.zero_shot_pose_utils.scale_points_to_orig": [[96, 100], ["points.int().long", "points.int"], "function", ["None"], ["", "def", "scale_points_to_orig", "(", "points", ",", "image_scaling", ")", ":", "\n", "    ", "points", "*=", "image_scaling", "\n", "\n", "return", "points", ".", "int", "(", ")", ".", "long", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.applied-ai-lab_zero-shot-pose.method.zero_shot_pose_utils.get_structured_pcd": [[105, 146], ["zsp.utils.depthproc.NDCGridRaysampler", "torch.cat", "torch.cat", "xy_grid_ndc.view.view", "camera.unproject_points", "camera.unproject_points.view", "torch.Tensor", "torch.Tensor", "depth_map.squeeze", "zsp.utils.depthproc.cv2_depth_fillzero", "depth_map.squeeze.unsqueeze", "depth_map.squeeze().numpy", "depth_map.squeeze"], "function", ["home.repos.pwc.inspect_result.applied-ai-lab_zero-shot-pose.utils.depthproc.cv2_depth_fillzero"], ["", "def", "get_structured_pcd", "(", "frame", ",", "inpaint", "=", "True", ",", "world_coordinates", "=", "False", ")", ":", "\n", "\n", "    ", "\"\"\"Takes a frame from CO3D dataset\n    Frame is not a frame object, but a dict with keys 'shape', 'camera', 'depth_map'\n\n\n    Pointcloud returned is in world-coordinates, with shape (H, W, 3),\n    with the 3 dimensions encoding X, Y and Z world coordinates.\n    Optionally infills the depth map from the CO3D dataset, which tends\n    to be ~50% zeros (the equivalent of NaNs in the .png encoding). This\n    leads to fewer NaNs in the unprojected structured pointcloud.\n\n    world_coordinates passed to cam.unproject_points: if it is false, the\n    points are unprojected to the *camera* frame, else to the world frame\n    \"\"\"", "\n", "H", ",", "W", "=", "frame", "[", "'shape'", "]", "\n", "camera", "=", "frame", "[", "'camera'", "]", "\n", "depth_map", "=", "frame", "[", "'depth_map'", "]", "\n", "\n", "# --- 1. Make a grid of the coordinates in Pytorch3D NDC space ---", "\n", "gridsampler", "=", "NDCGridRaysampler", "(", "image_width", "=", "W", ",", "image_height", "=", "H", ",", "n_pts_per_ray", "=", "1", ",", "min_depth", "=", "0", ",", "max_depth", "=", "1", ")", "\n", "xy_grid", "=", "gridsampler", ".", "_xy_grid", "\n", "\n", "# --- 2. In paint depth map ---", "\n", "if", "inpaint", ":", "\n", "        ", "depth_proc", "=", "torch", ".", "Tensor", "(", "cv2_depth_fillzero", "(", "depth_map", ".", "squeeze", "(", ")", ".", "numpy", "(", ")", ")", ")", "\n", "", "else", ":", "\n", "        ", "depth_proc", "=", "depth_map", ".", "squeeze", "(", ")", "\n", "\n", "# --- 3. Stack the xy coords with depth, which is in NDC/screen format (no difference in this case, I think)", "\n", "", "xy_grid_ndc", "=", "torch", ".", "cat", "(", "(", "xy_grid", ",", "\n", "depth_proc", ".", "unsqueeze", "(", "-", "1", ")", ")", ",", "dim", "=", "-", "1", ")", "\n", "xy_grid_ndc", "=", "xy_grid_ndc", ".", "view", "(", "-", "1", ",", "3", ")", "\n", "\n", "# --- 4. Unproject ---", "\n", "unproj", "=", "camera", ".", "unproject_points", "(", "xy_grid_ndc", ",", "world_coordinates", ")", "# (H*W, 3)", "\n", "\n", "# --- 5. Convert back to image shape, remove homogeneous dimension ---", "\n", "structured_pcd", "=", "unproj", ".", "view", "(", "H", ",", "W", ",", "3", ")", "\n", "\n", "return", "structured_pcd", "\n", "\n"]], "home.repos.pwc.inspect_result.applied-ai-lab_zero-shot-pose.method.zero_shot_pose_utils.rotation_err": [[186, 192], ["torch.acos", "torch.acos", "torch.sum().clamp", "torch.sum().clamp", "torch.sum", "torch.sum"], "function", ["None"], ["", "", "def", "rotation_err", "(", "R_pred", ",", "R_gt", ")", ":", "\n", "    ", "\"\"\"compute rotation error for viewpoint estimation\"\"\"", "\n", "# compute the angle distance between rotation matrix in degrees", "\n", "R_err", "=", "torch", ".", "acos", "(", "(", "(", "torch", ".", "sum", "(", "R_pred", "*", "R_gt", ",", "1", ")", ")", ".", "clamp", "(", "-", "1.", ",", "3.", ")", "-", "1.", ")", "/", "2", ")", "\n", "R_err", "=", "R_err", "*", "180.", "/", "np", ".", "pi", "\n", "return", "R_err", "\n", "\n"]], "home.repos.pwc.inspect_result.applied-ai-lab_zero-shot-pose.method.zero_shot_pose_utils.rotation_acc": [[194, 196], ["torch.mean", "torch.mean"], "function", ["None"], ["", "def", "rotation_acc", "(", "R_err", ",", "th", "=", "30.", ")", ":", "\n", "    ", "return", "100.", "*", "torch", ".", "mean", "(", "(", "R_err", "<=", "th", ")", ".", "float", "(", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.applied-ai-lab_zero-shot-pose.method.zero_shot_pose_utils.trans21_error": [[198, 239], ["pytorch3d.renderer.cameras.get_world_to_view_transform", "pytorch3d.renderer.cameras.get_world_to_view_transform", "pytorch3d.renderer.cameras.get_world_to_view_transform.inverse().compose", "cam1.get_world_to_view_transform", "cam2.get_world_to_view_transform", "cam1.get_world_to_view_transform.inverse().compose().compose", "torch.linalg.norm", "torch.linalg.norm", "trans1_gt[].t().unsqueeze", "trans1_gt[].t", "trans2_gt[].t().unsqueeze", "trans2_gt[].t", "trans21.get_matrix().permute", "w2v_cam1.inverse().compose().compose.get_matrix().permute", "pytorch3d.renderer.cameras.get_world_to_view_transform.inverse", "cam1.get_world_to_view_transform.inverse().compose", "pytorch3d.transforms.so3_relative_angle", "trans1_gt[].t", "trans2_gt[].t", "trans21.get_matrix", "w2v_cam1.inverse().compose().compose.get_matrix", "cam1.get_world_to_view_transform.inverse"], "function", ["None"], ["", "def", "trans21_error", "(", "trans21", ",", "trans1_gt", ",", "trans2_gt", ",", "cam1", ",", "cam2", ")", ":", "\n", "    ", "\"\"\"Returns geodesic rotation error (degrees)\n\n    Args:\n        trans21: The camera-frame transform between im1 and im2\n        trans1_gt: The world-frame transform between the pointcloud of im1\n            and the pointcloud of im2\n        trans2_gt: The world-frame transform between the pointcloud of im1\n            and the pointcloud of im2\n        cam1 (pytorch3d.renderer.cameras.PerspectiveCameras): the camera for\n            im1, including the viewpoint (extrinsics) and intrinsics\n        cam2 (pytorch3d.renderer.cameras.PerspectiveCameras): the camera for\n            im2, including the viewpoint (extrinsics) and intrinsics\n    \"\"\"", "\n", "trans1", "=", "get_world_to_view_transform", "(", "trans1_gt", "[", ":", "3", ",", ":", "3", "]", ".", "t", "(", ")", ".", "unsqueeze", "(", "0", ")", ",", "\n", "trans1_gt", "[", ":", "3", ",", "3", ":", "]", ".", "t", "(", ")", ")", "\n", "trans2", "=", "get_world_to_view_transform", "(", "trans2_gt", "[", ":", "3", ",", ":", "3", "]", ".", "t", "(", ")", ".", "unsqueeze", "(", "0", ")", ",", "\n", "trans2_gt", "[", ":", "3", ",", "3", ":", "]", ".", "t", "(", ")", ")", "\n", "\n", "trans_gt", "=", "trans1", ".", "inverse", "(", ")", ".", "compose", "(", "trans2", ")", "\n", "w2v_cam1", "=", "cam1", ".", "get_world_to_view_transform", "(", ")", "\n", "w2v_cam2", "=", "cam2", ".", "get_world_to_view_transform", "(", ")", "\n", "\n", "trans21_gt", "=", "w2v_cam1", ".", "inverse", "(", ")", ".", "compose", "(", "trans_gt", ")", ".", "compose", "(", "w2v_cam2", ")", "\n", "R21_pred", "=", "trans21", ".", "get_matrix", "(", ")", ".", "permute", "(", "0", ",", "2", ",", "1", ")", "[", ":", ",", ":", "3", ",", ":", "3", "]", "\n", "R21_gt", "=", "trans21_gt", ".", "get_matrix", "(", ")", ".", "permute", "(", "0", ",", "2", ",", "1", ")", "[", ":", ",", ":", "3", ",", ":", "3", "]", "\n", "umeyama_scale", "=", "torch", ".", "linalg", ".", "norm", "(", "R21_pred", "[", ":", ",", "0", ",", ":", "]", ",", "2", ",", "dim", "=", "-", "1", ")", "\n", "R21_pred", "/=", "umeyama_scale", "[", ":", ",", "None", ",", "None", "]", "\n", "# print(f\"Scaled R21_pred by {umeyama_scale}, now testing R21_pred for valid rotation matrix\")", "\n", "# _check_valid_rotation_matrix(R21_pred)", "\n", "# print(\"testing R21_gt for valid rotation matrix\")", "\n", "# _check_valid_rotation_matrix(R21_gt)", "\n", "\n", "# if so3_relative_angle(R21_pred, R21_gt) * 180 / np.pi < 25:", "\n", "#     t21_pred = trans21.get_matrix().permute(0, 2, 1)[:, :3, 3:]", "\n", "#     t21_gt = trans21_gt.get_matrix().permute(0, 2, 1)[:, :3, 3:]", "\n", "#     import pdb", "\n", "#     pdb.set_trace()", "\n", "#     print(torch.nn.PairwiseDistance(p=2)(t21_pred.squeeze(), t21_gt.squeeze()))", "\n", "\n", "return", "so3_relative_angle", "(", "R21_pred", ",", "R21_gt", ")", "*", "180", "/", "np", ".", "pi", "\n", "\n"]], "home.repos.pwc.inspect_result.applied-ai-lab_zero-shot-pose.method.zero_shot_pose_utils.trans_gt_error": [[241, 266], ["pytorch3d.renderer.cameras.get_world_to_view_transform", "pytorch3d.renderer.cameras.get_world_to_view_transform", "pytorch3d.renderer.cameras.get_world_to_view_transform.inverse().compose", "torch.linalg.norm", "torch.linalg.norm", "trans1_gt[].t().unsqueeze", "trans1_gt[].t", "trans2_gt[].t().unsqueeze", "trans2_gt[].t", "trans21w_hat.get_matrix().permute", "trans1_gt.inverse().compose.get_matrix().permute", "pytorch3d.renderer.cameras.get_world_to_view_transform.inverse", "pytorch3d.transforms.so3_relative_angle", "trans1_gt[].t", "trans2_gt[].t", "trans21w_hat.get_matrix", "trans1_gt.inverse().compose.get_matrix"], "function", ["None"], ["", "def", "trans_gt_error", "(", "trans21w_hat", ",", "trans1_gt", ",", "trans2_gt", ")", ":", "\n", "    ", "\"\"\"Returns geodesic rotation error (degrees)\n\n    Args:\n        trans21w_hat: The estimated world-frame transform between pcd1 and pcd2\n        trans1_gt: The world-frame transform between the pointcloud of im1\n            and the pointcloud of im2\n        trans2_gt: The world-frame transform between the pointcloud of im1\n            and the pointcloud of im2\n    \"\"\"", "\n", "\n", "trans1_gt", "=", "get_world_to_view_transform", "(", "trans1_gt", "[", ":", "3", ",", ":", "3", "]", ".", "t", "(", ")", ".", "unsqueeze", "(", "0", ")", ",", "\n", "trans1_gt", "[", ":", "3", ",", "3", ":", "]", ".", "t", "(", ")", ")", "\n", "trans2_gt", "=", "get_world_to_view_transform", "(", "trans2_gt", "[", ":", "3", ",", ":", "3", "]", ".", "t", "(", ")", ".", "unsqueeze", "(", "0", ")", ",", "\n", "trans2_gt", "[", ":", "3", ",", "3", ":", "]", ".", "t", "(", ")", ")", "\n", "\n", "trans21w_gt", "=", "trans1_gt", ".", "inverse", "(", ")", ".", "compose", "(", "trans2_gt", ")", "\n", "\n", "R21_pred", "=", "trans21w_hat", ".", "get_matrix", "(", ")", ".", "permute", "(", "0", ",", "2", ",", "1", ")", "[", ":", ",", ":", "3", ",", ":", "3", "]", "\n", "R21_gt", "=", "trans21w_gt", ".", "get_matrix", "(", ")", ".", "permute", "(", "0", ",", "2", ",", "1", ")", "[", ":", ",", ":", "3", ",", ":", "3", "]", "\n", "\n", "umeyama_scale", "=", "torch", ".", "linalg", ".", "norm", "(", "R21_pred", "[", ":", ",", "0", ",", ":", "]", ",", "2", ",", "dim", "=", "-", "1", ")", "\n", "R21_pred", "/=", "umeyama_scale", "[", ":", ",", "None", ",", "None", "]", "\n", "\n", "return", "so3_relative_angle", "(", "R21_pred", ",", "R21_gt", ")", "*", "180", "/", "np", ".", "pi", "\n", "", ""]], "home.repos.pwc.inspect_result.applied-ai-lab_zero-shot-pose.method.icp_functions.subsample_random_pcd": [[13, 25], ["pytorch3d.ops.utils.convert_pointclouds_to_tensor", "torch.randperm", "pytorch3d.structures.pointclouds.Pointclouds", "pcd.features_list", "X1.size", "len"], "function", ["None"], ["def", "subsample_random_pcd", "(", "pcd", ",", "K", "=", "4000", ")", ":", "\n", "    ", "X1", ",", "numpoints1", "=", "oputil", ".", "convert_pointclouds_to_tensor", "(", "pcd", ")", "\n", "col1", "=", "pcd", ".", "features_list", "(", ")", "[", "0", "]", "\n", "perm", "=", "torch", ".", "randperm", "(", "X1", ".", "size", "(", "1", ")", ")", "\n", "if", "len", "(", "perm", ")", "<=", "K", ":", "\n", "        ", "idx", "=", "perm", "\n", "", "else", ":", "\n", "        ", "idx", "=", "perm", "[", ":", "K", "]", "\n", "", "X1_sub", "=", "X1", "[", ":", ",", "idx", ",", ":", "]", "\n", "col1_sub", "=", "col1", "[", "idx", "]", "[", "None", ",", ":", ",", ":", "]", "\n", "pcd1_sub", "=", "Pointclouds", "(", "X1_sub", ",", "col1_sub", ")", "\n", "return", "pcd1_sub", "\n", "\n"]], "home.repos.pwc.inspect_result.applied-ai-lab_zero-shot-pose.method.icp_functions.subsample_and_ICP": [[27, 40], ["icp_functions.subsample_random_pcd", "icp_functions.subsample_random_pcd", "pytorch3d.ops.points_alignment.iterative_closest_point", "pytorch3d.renderer.cameras.get_world_to_view_transform", "R_icp.permute"], "function", ["home.repos.pwc.inspect_result.applied-ai-lab_zero-shot-pose.method.icp_functions.subsample_random_pcd", "home.repos.pwc.inspect_result.applied-ai-lab_zero-shot-pose.method.icp_functions.subsample_random_pcd"], ["", "def", "subsample_and_ICP", "(", "pcd_ref", ",", "pcd_query", ",", "K", "=", "4000", ")", ":", "\n", "    ", "pcd1_sub", "=", "subsample_random_pcd", "(", "pcd_ref", ",", "K", "=", "K", ")", "\n", "pcd2_sub", "=", "subsample_random_pcd", "(", "pcd_query", ",", "K", "=", "K", ")", "\n", "\n", "ICPSolution", "=", "iterative_closest_point", "(", "pcd1_sub", ",", "pcd2_sub", ",", "\n", "estimate_scale", "=", "True", ",", "\n", "max_iterations", "=", "500", ",", "\n", "verbose", "=", "False", "\n", ")", "\n", "R_icp", "=", "ICPSolution", ".", "RTs", ".", "R", "# (B, 3, 3)", "\n", "t_icp", "=", "ICPSolution", ".", "RTs", ".", "T", "# (B, 3)", "\n", "T21w_icp", "=", "get_world_to_view_transform", "(", "R_icp", ".", "permute", "(", "0", ",", "2", ",", "1", ")", ",", "t_icp", ")", "\n", "return", "T21w_icp", "\n", "\n"]], "home.repos.pwc.inspect_result.applied-ai-lab_zero-shot-pose.method.icp_functions.transform_query_pcds": [[42, 63], ["query_camera0.get_world_to_view_transform", "cam1.get_world_to_view_transform", "query_camera0.get_world_to_view_transform.compose", "query_pcds_transforms.append", "zsp.utils.pcd_proc.apply_transform_to_pcd", "query_pcds_transformed.append", "cam1.get_world_to_view_transform.inverse", "query_pcd.clone"], "function", ["home.repos.pwc.inspect_result.applied-ai-lab_zero-shot-pose.utils.pcd_proc.apply_transform_to_pcd"], ["", "def", "transform_query_pcds", "(", "query_cameras", ",", "query_camera0", ",", "query_pcd", ")", ":", "\n", "    ", "\"\"\"Transforms pcds to make the view in cam0 match original view in original cam\n    \"\"\"", "\n", "# Initialise the list of transforms: ", "\n", "query_pcds_transforms", "=", "[", "]", "\n", "T_c0", "=", "query_camera0", ".", "get_world_to_view_transform", "(", ")", "\n", "for", "cam1", "in", "query_cameras", ":", "\n", "        ", "T_c1", "=", "cam1", ".", "get_world_to_view_transform", "(", ")", "\n", "# Rotation to apply to pcd ", "\n", "T10w_hat", "=", "T_c0", ".", "compose", "(", "T_c1", ".", "inverse", "(", ")", ")", "\n", "query_pcds_transforms", ".", "append", "(", "T10w_hat", ")", "\n", "\n", "", "query_pcds_transformed", "=", "[", "]", "\n", "for", "T10w_hat", "in", "query_pcds_transforms", ":", "\n", "        ", "query_pcd_cam_i", "=", "apply_transform_to_pcd", "(", "\n", "query_pcd", ".", "clone", "(", ")", ",", "\n", "T10w_hat", "\n", ")", "\n", "query_pcds_transformed", ".", "append", "(", "query_pcd_cam_i", ")", "\n", "\n", "", "return", "query_pcds_transforms", ",", "query_pcds_transformed", "\n", "\n"]], "home.repos.pwc.inspect_result.applied-ai-lab_zero-shot-pose.method.icp_functions.transform_ref_pcd": [[65, 70], ["icp_functions.transform_query_pcds"], "function", ["home.repos.pwc.inspect_result.applied-ai-lab_zero-shot-pose.method.icp_functions.transform_query_pcds"], ["", "def", "transform_ref_pcd", "(", "ref_camera", ",", "ref_camera0", ",", "ref_pcd", ")", ":", "\n", "    ", "ref_pcd_transforms", ",", "ref_pcd_transformed", "=", "transform_query_pcds", "(", "\n", "[", "ref_camera", "]", ",", "ref_camera0", ",", "ref_pcd", ")", "\n", "\n", "return", "ref_pcd_transforms", "[", "0", "]", ",", "ref_pcd_transformed", "[", "0", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.applied-ai-lab_zero-shot-pose.method.icp_functions.icp_ref2query": [[72, 98], ["print", "icp_functions.transform_query_pcds", "icp_functions.transform_ref_pcd", "icp_functions.subsample_and_ICP", "query_pcd_transform.compose().compose", "ref_pcd_transform.inverse", "query_pcd_transform.compose"], "function", ["home.repos.pwc.inspect_result.applied-ai-lab_zero-shot-pose.method.icp_functions.transform_query_pcds", "home.repos.pwc.inspect_result.applied-ai-lab_zero-shot-pose.method.icp_functions.transform_ref_pcd", "home.repos.pwc.inspect_result.applied-ai-lab_zero-shot-pose.method.icp_functions.subsample_and_ICP"], ["", "def", "icp_ref2query", "(", "ref_camera", ",", "query_cameras", ",", "\n", "ref_camera0", ",", "query_camera0", ",", "\n", "ref_pcd", ",", "query_pcd", ",", "\n", "best_view_idx", "=", "None", ")", ":", "\n", "    ", "\"\"\" Returns ICP as the average estimate over mu\n\n    \"\"\"", "\n", "print", "(", "\"Calling ICP\"", ")", "\n", "# Make a copy of the query pcd for each query image, transformed such that its image", "\n", "# in query_camera0 is the original image as seen through the original camera. ", "\n", "if", "not", "best_view_idx", ":", "\n", "        ", "best_view_idx", "=", "0", "\n", "", "query_pcd_transform", ",", "query_pcd_transformed", "=", "transform_query_pcds", "(", "\n", "[", "query_cameras", "[", "best_view_idx", "]", "]", ",", "query_camera0", ",", "query_pcd", ")", "\n", "query_pcd_transform", "=", "query_pcd_transform", "[", "0", "]", "\n", "query_pcd_transformed", "=", "query_pcd_transformed", "[", "0", "]", "\n", "# Make a copy of the reference pcd transformed such that its image", "\n", "# in ref_camera0 is the original image as seen through the original camera. ", "\n", "ref_pcd_transform", ",", "ref_pcd_transformed", "=", "transform_ref_pcd", "(", "\n", "ref_camera", ",", "ref_camera0", ",", "ref_pcd", ")", "\n", "\n", "Tref2query_icp", "=", "subsample_and_ICP", "(", "ref_pcd_transformed", ",", "query_pcd_transformed", ")", "\n", "# Transform the predicted transform so it can be compared to the ground-truth", "\n", "# (that is, undo the effects of transforming the ref and query pcds before ICP)", "\n", "T_hat_GT", "=", "query_pcd_transform", ".", "compose", "(", "Tref2query_icp", ")", ".", "compose", "(", "ref_pcd_transform", ".", "inverse", "(", ")", ")", "\n", "return", "T_hat_GT", "\n", "\n"]], "home.repos.pwc.inspect_result.applied-ai-lab_zero-shot-pose.method.icp_functions.icp_ref2query_bestframe": [[100, 104], ["None"], "function", ["None"], ["", "def", "icp_ref2query_bestframe", "(", "ref_camera", ",", "query_cameras", ",", "\n", "ref_camera0", ",", "query_camera0", ",", "\n", "ref_pcd", ",", "query_pcd", ")", ":", "\n", "    ", "return", "\n", "", ""]], "home.repos.pwc.inspect_result.applied-ai-lab_zero-shot-pose.method.rigid_body.rotation_err_matrix_torch": [[4, 9], ["torch.acos", "torch.sum().clamp", "torch.sum"], "function", ["None"], ["def", "rotation_err_matrix_torch", "(", "R_pred", ",", "R_gt", ")", ":", "\n", "# compute the angle distance between rotation matrix in degrees", "\n", "    ", "R_err", "=", "torch", ".", "acos", "(", "(", "(", "torch", ".", "sum", "(", "R_pred", "*", "R_gt", ",", "1", ")", ")", ".", "clamp", "(", "-", "1.", ",", "3.", ")", "-", "1.", ")", "/", "2", ")", "\n", "R_err", "=", "R_err", "*", "180.", "/", "np", ".", "pi", "\n", "return", "R_err", "\n", "\n"]], "home.repos.pwc.inspect_result.applied-ai-lab_zero-shot-pose.method.rigid_body.rotation_err_matrix_np": [[10, 15], ["numpy.arccos", "numpy.sum().clip", "numpy.sum"], "function", ["None"], ["", "def", "rotation_err_matrix_np", "(", "R_pred", ",", "R_gt", ")", ":", "\n", "# compute the angle distance between rotation matrix in degrees", "\n", "    ", "R_err", "=", "np", ".", "arccos", "(", "(", "(", "np", ".", "sum", "(", "R_pred", "*", "R_gt", ",", "1", ")", ")", ".", "clip", "(", "-", "1.", ",", "3.", ")", "-", "1.", ")", "/", "2", ")", "\n", "R_err", "=", "R_err", "*", "180.", "/", "np", ".", "pi", "\n", "return", "R_err", "\n", "\n"]], "home.repos.pwc.inspect_result.applied-ai-lab_zero-shot-pose.method.rigid_body.rotation_acc_matrix_torch": [[16, 19], ["rigid_body.rotation_err_matrix_torch", "torch.mean"], "function", ["home.repos.pwc.inspect_result.applied-ai-lab_zero-shot-pose.method.rigid_body.rotation_err_matrix_torch"], ["", "def", "rotation_acc_matrix_torch", "(", "preds", ",", "targets", ",", "th", "=", "30.", ")", ":", "\n", "    ", "R_err", "=", "rotation_err_matrix_torch", "(", "preds", ",", "targets", ")", "\n", "return", "100.", "*", "torch", ".", "mean", "(", "(", "R_err", "<=", "th", ")", ".", "float", "(", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.applied-ai-lab_zero-shot-pose.method.rigid_body.rotation_acc_matrix_np": [[20, 23], ["rigid_body.rotation_err_matrix_np", "numpy.mean"], "function", ["home.repos.pwc.inspect_result.applied-ai-lab_zero-shot-pose.method.rigid_body.rotation_err_matrix_np"], ["", "def", "rotation_acc_matrix_np", "(", "preds", ",", "targets", ",", "th", "=", "30.", ")", ":", "\n", "    ", "R_err", "=", "rotation_err_matrix_np", "(", "preds", ",", "targets", ")", "\n", "return", "100.", "*", "np", ".", "mean", "(", "R_err", "<=", "th", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.applied-ai-lab_zero-shot-pose.method.rigid_body.quaternion_distance_np": [[24, 31], ["numpy.sum", "numpy.arccos", "numpy.arccos", "numpy.abs", "numpy.abs", "p.dot"], "function", ["None"], ["", "def", "quaternion_distance_np", "(", "p", ",", "q", ")", ":", "\n", "    ", "\"\"\"Quaternion distance (angle, in radians) in numpy\"\"\"", "\n", "if", "p", ".", "ndim", ">", "1", ":", "\n", "        ", "batch_dot", "=", "np", ".", "sum", "(", "p", "*", "q", ",", "axis", "=", "-", "1", ")", "\n", "return", "2", "*", "np", ".", "arccos", "(", "np", ".", "abs", "(", "batch_dot", ")", ")", "\n", "", "else", ":", "\n", "        ", "return", "2", "*", "np", ".", "arccos", "(", "np", ".", "abs", "(", "p", ".", "dot", "(", "q", ")", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.applied-ai-lab_zero-shot-pose.method.rigid_body.bdot": [[32, 37], ["torch.bmm().reshape", "torch.bmm", "a.view", "b.view"], "function", ["None"], ["", "", "def", "bdot", "(", "a", ",", "b", ")", ":", "\n", "    ", "\"\"\"Batched vector dot product in pytorch\"\"\"", "\n", "B", "=", "a", ".", "shape", "[", "0", "]", "\n", "S", "=", "a", ".", "shape", "[", "1", "]", "\n", "return", "torch", ".", "bmm", "(", "a", ".", "view", "(", "B", ",", "1", ",", "S", ")", ",", "b", ".", "view", "(", "B", ",", "S", ",", "1", ")", ")", ".", "reshape", "(", "-", "1", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.applied-ai-lab_zero-shot-pose.method.rigid_body.quaternion_distance_torch": [[38, 49], ["torch.acos", "torch.acos", "bdot().clamp", "torch.dot().clamp", "rigid_body.bdot", "torch.dot"], "function", ["home.repos.pwc.inspect_result.applied-ai-lab_zero-shot-pose.method.rigid_body.bdot"], ["", "def", "quaternion_distance_torch", "(", "p", ",", "q", ")", ":", "\n", "    ", "\"\"\"Quaternion distance (angle, in radians) in pytorch\n    \n    Args:\n        p (torch.Tensor, size=(b, 4)): Predicted quaternion (normed to S3 sphere)\n        q (torch.Tensor, size=(b, 4)): Target quaternion (on S3 sphere)\n    \"\"\"", "\n", "if", "p", ".", "ndim", ">", "1", ":", "\n", "        ", "return", "2", "*", "torch", ".", "acos", "(", "bdot", "(", "p", ",", "q", ")", ".", "clamp", "(", "-", "1.", ",", "1.", ")", ")", "\n", "", "else", ":", "\n", "        ", "return", "2", "*", "torch", ".", "acos", "(", "torch", ".", "dot", "(", "p", ",", "q", ")", ".", "clamp", "(", "-", "1.", ",", "1.", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.applied-ai-lab_zero-shot-pose.method.rigid_body.least_squares_solution": [[50, 108], ["numpy.sqrt", "numpy.linalg.svd", "numpy.ones", "numpy.ones", "numpy.sum", "rigid_body.least_squares_solution.get_sums"], "function", ["None"], ["", "", "def", "least_squares_solution", "(", "x1", ",", "x2", ")", ":", "\n", "    ", "\"\"\"\n    x1, x2 both define i=1,...,I 3-dimensional points, with known correspondence\n    https://www.youtube.com/watch?v=re08BUepRxo\n    Assumption: points x1, x2 related by a similarity:\n      E(x2_i) = \u03bb*R*x1_i+t, {i = 1, ..., I}\n    Task: estimate the parameters, provide uncertainty\n\n    Args:\n        x1 (array): Shape (3, N)\n        x2 (array): Shape (3, N)\n\n    Returns:\n        rot (array): Shape (3, 3), a rotation matrix\n        t (array):   Shape (3, 1), a translation vector\n        lam (float): Scaling factor\n\n    \"\"\"", "\n", "w1", "=", "(", "1", "/", "0.1", "**", "2", ")", "*", "np", ".", "ones", "(", "x1", ".", "shape", "[", "1", "]", ")", "# 'weights' are (1/sig^2) - fix for now", "\n", "w2", "=", "(", "1", "/", "0.1", "**", "2", ")", "*", "np", ".", "ones", "(", "x2", ".", "shape", "[", "1", "]", ")", "# 'weights' are (1/sig^2) - fix for now", "\n", "# Find centroid (weighted) of the 'observed' (x2) points", "\n", "x2_C", "=", "(", "np", ".", "sum", "(", "x2", "*", "w2", ",", "axis", "=", "1", ")", "/", "np", ".", "sum", "(", "w2", ")", ")", ".", "reshape", "(", "3", ",", "1", ")", "\n", "x1_C", "=", "(", "np", ".", "sum", "(", "x1", "*", "w1", ",", "axis", "=", "1", ")", "/", "np", ".", "sum", "(", "w1", ")", ")", ".", "reshape", "(", "3", ",", "1", ")", "\n", "\n", "# Unknown params: rotation R, scale \u03bb, modified translation vector u, residuals v_x2_i", "\n", "# t = x2_C - \u03bb*R*u", "\n", "\n", "# Minimising the weighted sum of the residuals, we arrive at", "\n", "u", "=", "x1_C", "\n", "\n", "# Approximate solution for lambda, holds for small noise - analytic soln is dependent on R!", "\n", "def", "get_sums", "(", "x", ",", "xc", ",", "w", ")", ":", "\n", "# total = 0", "\n", "# for x_i, x_ci, w_i in zip(x, xc, w):", "\n", "#     total += w_i * (x_i-x_ci).T @ (x_i-x_ci)", "\n", "# return total", "\n", "# Or, parallel implementation!", "\n", "        ", "return", "np", ".", "sum", "(", "np", ".", "sum", "(", "(", "x", "-", "xc", ")", "*", "(", "x", "-", "xc", ")", ",", "axis", "=", "0", ")", "*", "w", ")", "\n", "\n", "", "lam_sq", "=", "get_sums", "(", "x2", ",", "x2_C", ",", "w2", ")", "/", "get_sums", "(", "x1", ",", "x1_C", ",", "w2", ")", "\n", "lam", "=", "np", ".", "sqrt", "(", "lam_sq", ")", "\n", "# lam = 1", "\n", "\n", "# Estimation of rotation", "\n", "# 1. Centre coordinates", "\n", "c_x1", "=", "x1", "-", "x1_C", "\n", "c_x2", "=", "x2", "-", "x2_C", "\n", "# 2. Create H matrix (3x3)", "\n", "H", "=", "c_x1", "@", "np", ".", "diag", "(", "w2", ")", "@", "c_x2", ".", "T", "\n", "# 3. Use SVD to find estimated rotation R", "\n", "U", ",", "S", ",", "Vh", "=", "np", ".", "linalg", ".", "svd", "(", "H", ")", "\n", "R", "=", "Vh", ".", "T", "@", "U", ".", "T", "\n", "\n", "# Get translation parameter using t = x2_C - \u03bb*R*u", "\n", "# import pdb", "\n", "# pdb.set_trace()", "\n", "t", "=", "x2_C", "-", "lam", "*", "R", "@", "u", "\n", "return", "R", ",", "t", ",", "lam", "\n", "\n"]], "home.repos.pwc.inspect_result.applied-ai-lab_zero-shot-pose.method.rigid_body.rigid_transform_3D": [[110, 160], ["numpy.mean", "numpy.mean", "centroid_A.reshape.reshape", "centroid_B.reshape.reshape", "numpy.linalg.svd", "print", "print", "Exception", "Exception", "numpy.transpose", "numpy.linalg.det", "print"], "function", ["None"], ["", "def", "rigid_transform_3D", "(", "A", ",", "B", ")", ":", "\n", "    ", "\"\"\"\n    Un-weighted version, from https://github.com/nghiaho12/rigid_transform_3D\n\n    Implementation of \"Least-Squares Fitting of Two 3-D Point Sets\", Arun, K. S. \n    and Huang, T. S. and Blostein, S. D, [1987]\n    \"\"\"", "\n", "assert", "A", ".", "shape", "==", "B", ".", "shape", "\n", "\n", "num_rows", ",", "num_cols", "=", "A", ".", "shape", "\n", "if", "num_rows", "!=", "3", ":", "\n", "        ", "raise", "Exception", "(", "f\"matrix A is not 3xN, it is {num_rows}x{num_cols}\"", ")", "\n", "\n", "", "num_rows", ",", "num_cols", "=", "B", ".", "shape", "\n", "if", "num_rows", "!=", "3", ":", "\n", "        ", "raise", "Exception", "(", "f\"matrix B is not 3xN, it is {num_rows}x{num_cols}\"", ")", "\n", "\n", "# find mean column wise", "\n", "", "centroid_A", "=", "np", ".", "mean", "(", "A", ",", "axis", "=", "1", ")", "\n", "centroid_B", "=", "np", ".", "mean", "(", "B", ",", "axis", "=", "1", ")", "\n", "\n", "# ensure centroids are 3x1", "\n", "centroid_A", "=", "centroid_A", ".", "reshape", "(", "-", "1", ",", "1", ")", "\n", "centroid_B", "=", "centroid_B", ".", "reshape", "(", "-", "1", ",", "1", ")", "\n", "\n", "# subtract mean", "\n", "Am", "=", "A", "-", "centroid_A", "\n", "Bm", "=", "B", "-", "centroid_B", "\n", "\n", "H", "=", "Am", "@", "np", ".", "transpose", "(", "Bm", ")", "\n", "\n", "# sanity check", "\n", "#if linalg.matrix_rank(H) < 3:", "\n", "#    raise ValueError(\"rank of H = {}, expecting 3\".format(linalg.matrix_rank(H)))", "\n", "\n", "# find rotation", "\n", "U", ",", "S", ",", "Vt", "=", "np", ".", "linalg", ".", "svd", "(", "H", ")", "\n", "R", "=", "Vt", ".", "T", "@", "U", ".", "T", "\n", "\n", "# special reflection case", "\n", "print", "(", "\"R before correcting for reflection:\"", ")", "\n", "print", "(", "R", ")", "\n", "if", "np", ".", "linalg", ".", "det", "(", "R", ")", "<", "0", ":", "\n", "        ", "print", "(", "\"det(R) < R, reflection detected!, correcting for it ...\"", ")", "\n", "Vt", "[", "2", ",", ":", "]", "*=", "-", "1", "\n", "R", "=", "Vt", ".", "T", "@", "U", ".", "T", "\n", "\n", "", "t", "=", "-", "R", "@", "centroid_A", "+", "centroid_B", "\n", "\n", "return", "R", ",", "t", "\n", "\n"]], "home.repos.pwc.inspect_result.applied-ai-lab_zero-shot-pose.method.rigid_body.umeyama": [[161, 237], ["src.mean", "dst.mean", "numpy.ones", "numpy.eye", "numpy.linalg.svd", "numpy.linalg.matrix_rank", "numpy.linalg.det", "numpy.eye", "src_demean.var().sum", "numpy.linalg.det", "numpy.linalg.det", "numpy.diag", "numpy.diag", "src_demean.var"], "function", ["None"], ["", "def", "umeyama", "(", "src", ",", "dst", ",", "estimate_scale", "=", "True", ")", ":", "\n", "    ", "\"\"\"Estimate N-D similarity transformation with or without scaling.\n    Taken from skimage!\n\n    homo_src = np.hstack((src, np.ones((len(src), 1))))\n    homo_dst = np.hstack((src, np.ones((len(src), 1))))\n\n    homo_dst = T @ homo_src, where T is the returned transformation\n\n    Parameters\n    ----------\n    src : (M, N) array\n        Source coordinates.\n    dst : (M, N) array\n        Destination coordinates.\n    estimate_scale : bool\n        Whether to estimate scaling factor.\n    Returns\n    -------\n    T : (N + 1, N + 1)\n        The homogeneous similarity transformation matrix. The matrix contains\n        NaN values only if the problem is not well-conditioned.\n    References\n    ----------\n    .. [1] \"Least-squares estimation of transformation parameters between two\n            point patterns\", Shinji Umeyama, PAMI 1991, :DOI:`10.1109/34.88573`\n    \"\"\"", "\n", "\n", "num", "=", "src", ".", "shape", "[", "0", "]", "\n", "dim", "=", "src", ".", "shape", "[", "1", "]", "\n", "\n", "# Compute mean of src and dst.", "\n", "src_mean", "=", "src", ".", "mean", "(", "axis", "=", "0", ")", "\n", "dst_mean", "=", "dst", ".", "mean", "(", "axis", "=", "0", ")", "\n", "\n", "# Subtract mean from src and dst.", "\n", "src_demean", "=", "src", "-", "src_mean", "\n", "dst_demean", "=", "dst", "-", "dst_mean", "\n", "\n", "# Eq. (38).", "\n", "A", "=", "dst_demean", ".", "T", "@", "src_demean", "/", "num", "\n", "\n", "# Eq. (39).", "\n", "d", "=", "np", ".", "ones", "(", "(", "dim", ",", ")", ",", "dtype", "=", "np", ".", "double", ")", "\n", "if", "np", ".", "linalg", ".", "det", "(", "A", ")", "<", "0", ":", "\n", "        ", "d", "[", "dim", "-", "1", "]", "=", "-", "1", "\n", "\n", "", "T", "=", "np", ".", "eye", "(", "dim", "+", "1", ",", "dtype", "=", "np", ".", "double", ")", "\n", "\n", "U", ",", "S", ",", "V", "=", "np", ".", "linalg", ".", "svd", "(", "A", ")", "\n", "\n", "# Eq. (40) and (43).", "\n", "rank", "=", "np", ".", "linalg", ".", "matrix_rank", "(", "A", ")", "\n", "if", "rank", "==", "0", ":", "\n", "        ", "return", "np", ".", "eye", "(", "4", ")", ",", "1", "\n", "", "elif", "rank", "==", "dim", "-", "1", ":", "\n", "        ", "if", "np", ".", "linalg", ".", "det", "(", "U", ")", "*", "np", ".", "linalg", ".", "det", "(", "V", ")", ">", "0", ":", "\n", "            ", "T", "[", ":", "dim", ",", ":", "dim", "]", "=", "U", "@", "V", "\n", "", "else", ":", "\n", "            ", "s", "=", "d", "[", "dim", "-", "1", "]", "\n", "d", "[", "dim", "-", "1", "]", "=", "-", "1", "\n", "T", "[", ":", "dim", ",", ":", "dim", "]", "=", "U", "@", "np", ".", "diag", "(", "d", ")", "@", "V", "\n", "d", "[", "dim", "-", "1", "]", "=", "s", "\n", "", "", "else", ":", "\n", "        ", "T", "[", ":", "dim", ",", ":", "dim", "]", "=", "U", "@", "np", ".", "diag", "(", "d", ")", "@", "V", "\n", "\n", "", "if", "estimate_scale", ":", "\n", "# Eq. (41) and (42).", "\n", "        ", "scale", "=", "1.0", "/", "src_demean", ".", "var", "(", "axis", "=", "0", ")", ".", "sum", "(", ")", "*", "(", "S", "@", "d", ")", "\n", "", "else", ":", "\n", "        ", "scale", "=", "1.0", "\n", "\n", "", "T", "[", ":", "dim", ",", "dim", "]", "=", "dst_mean", "-", "scale", "*", "(", "T", "[", ":", "dim", ",", ":", "dim", "]", "@", "src_mean", ".", "T", ")", "\n", "T", "[", ":", "dim", ",", ":", "dim", "]", "*=", "scale", "\n", "\n", "return", "T", ",", "scale", "\n", "\n"]], "home.repos.pwc.inspect_result.applied-ai-lab_zero-shot-pose.method.dense_descriptor_utils._to_cartesian": [[17, 28], ["torch.stack", "torch.from_numpy", "numpy.unravel_index", "coords.cpu"], "function", ["home.repos.pwc.inspect_result.applied-ai-lab_zero-shot-pose.datasets.co3d_dataset.FrameData.cpu"], ["def", "_to_cartesian", "(", "coords", ":", "torch", ".", "Tensor", ",", "shape", ":", "Tuple", ")", ":", "\n", "\n", "    ", "\"\"\"\n    Takes raveled coordinates and returns them in a cartesian coordinate frame\n    coords: B x D\n    shape: tuple of cartesian dimensions\n    return: B x D x 2\n    \"\"\"", "\n", "\n", "i", ",", "j", "=", "(", "torch", ".", "from_numpy", "(", "inds", ")", "for", "inds", "in", "np", ".", "unravel_index", "(", "coords", ".", "cpu", "(", ")", ",", "shape", "=", "shape", ")", ")", "\n", "return", "torch", ".", "stack", "(", "[", "i", ",", "j", "]", ",", "dim", "=", "-", "1", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.applied-ai-lab_zero-shot-pose.method.dense_descriptor_utils.draw_correspondences": [[30, 67], ["len", "matplotlib.subplots", "ax1.axis", "matplotlib.subplots", "ax2.axis", "ax1.imshow", "ax2.imshow", "numpy.array", "zip", "len", "len", "matplotlib.get_cmap", "matplotlib.colors.ListedColormap", "matplotlib.Circle", "matplotlib.Circle", "ax1.add_patch", "ax1.add_patch", "matplotlib.Circle", "matplotlib.Circle", "ax2.add_patch", "ax2.add_patch", "len", "len", "matplotlib.colors.ListedColormap.", "range"], "function", ["None"], ["", "def", "draw_correspondences", "(", "points1", ":", "List", "[", "Tuple", "[", "float", ",", "float", "]", "]", ",", "points2", ":", "List", "[", "Tuple", "[", "float", ",", "float", "]", "]", ",", "\n", "image1", ":", "Image", ".", "Image", ",", "image2", ":", "Image", ".", "Image", ")", "->", "Tuple", "[", "plt", ".", "Figure", ",", "plt", ".", "Figure", "]", ":", "\n", "    ", "\"\"\"\n    draw point correspondences on images.\n    :param points1: a list of (y, x) coordinates of image1, corresponding to points2.\n    :param points2: a list of (y, x) coordinates of image2, corresponding to points1.\n    :param image1: a PIL image.\n    :param image2: a PIL image.\n    :return: two figures of images with marked points.\n    \"\"\"", "\n", "assert", "len", "(", "points1", ")", "==", "len", "(", "points2", ")", ",", "f\"points lengths are incompatible: {len(points1)} != {len(points2)}.\"", "\n", "num_points", "=", "len", "(", "points1", ")", "\n", "fig1", ",", "ax1", "=", "plt", ".", "subplots", "(", ")", "\n", "ax1", ".", "axis", "(", "'off'", ")", "\n", "fig2", ",", "ax2", "=", "plt", ".", "subplots", "(", ")", "\n", "ax2", ".", "axis", "(", "'off'", ")", "\n", "ax1", ".", "imshow", "(", "image1", ")", "\n", "ax2", ".", "imshow", "(", "image2", ")", "\n", "if", "num_points", ">", "15", ":", "\n", "        ", "cmap", "=", "plt", ".", "get_cmap", "(", "'tab10'", ")", "\n", "", "else", ":", "\n", "        ", "cmap", "=", "ListedColormap", "(", "[", "\"red\"", ",", "\"yellow\"", ",", "\"blue\"", ",", "\"lime\"", ",", "\"magenta\"", ",", "\"indigo\"", ",", "\"orange\"", ",", "\"cyan\"", ",", "\"darkgreen\"", ",", "\n", "\"maroon\"", ",", "\"black\"", ",", "\"white\"", ",", "\"chocolate\"", ",", "\"gray\"", ",", "\"blueviolet\"", "]", ")", "\n", "", "colors", "=", "np", ".", "array", "(", "[", "cmap", "(", "x", ")", "for", "x", "in", "range", "(", "num_points", ")", "]", ")", "\n", "radius1", ",", "radius2", "=", "8", ",", "1", "\n", "for", "point1", ",", "point2", ",", "color", "in", "zip", "(", "points1", ",", "points2", ",", "colors", ")", ":", "\n", "        ", "y1", ",", "x1", "=", "point1", "\n", "circ1_1", "=", "plt", ".", "Circle", "(", "(", "x1", ",", "y1", ")", ",", "radius1", ",", "facecolor", "=", "color", ",", "edgecolor", "=", "'white'", ",", "alpha", "=", "0.5", ")", "\n", "circ1_2", "=", "plt", ".", "Circle", "(", "(", "x1", ",", "y1", ")", ",", "radius2", ",", "facecolor", "=", "color", ",", "edgecolor", "=", "'white'", ")", "\n", "ax1", ".", "add_patch", "(", "circ1_1", ")", "\n", "ax1", ".", "add_patch", "(", "circ1_2", ")", "\n", "y2", ",", "x2", "=", "point2", "\n", "circ2_1", "=", "plt", ".", "Circle", "(", "(", "x2", ",", "y2", ")", ",", "radius1", ",", "facecolor", "=", "color", ",", "edgecolor", "=", "'white'", ",", "alpha", "=", "0.5", ")", "\n", "circ2_2", "=", "plt", ".", "Circle", "(", "(", "x2", ",", "y2", ")", ",", "radius2", ",", "facecolor", "=", "color", ",", "edgecolor", "=", "'white'", ")", "\n", "ax2", ".", "add_patch", "(", "circ2_1", ")", "\n", "ax2", ".", "add_patch", "(", "circ2_2", ")", "\n", "", "return", "fig1", ",", "fig2", "\n", "\n"]], "home.repos.pwc.inspect_result.applied-ai-lab_zero-shot-pose.method.dense_descriptor_utils.chunk_cosine_sim": [[69, 83], ["range", "torch.stack", "x[].unsqueeze", "result_list.append", "torch.nn.CosineSimilarity"], "function", ["None"], ["", "def", "chunk_cosine_sim", "(", "x", ":", "torch", ".", "Tensor", ",", "y", ":", "torch", ".", "Tensor", ")", "->", "torch", ".", "Tensor", ":", "\n", "    ", "\"\"\" Computes cosine similarity between all possible pairs in two sets of vectors.\n    Operates on chunks so no large amount of GPU RAM is required.\n    :param x: an tensor of descriptors of shape Bx1x(t_x)xd' where d' is the dimensionality of the descriptors and t_x\n    is the number of tokens in x.\n    :param y: a tensor of descriptors of shape Bx1x(t_y)xd' where d' is the dimensionality of the descriptors and t_y\n    is the number of tokens in y.\n    :return: cosine similarity between all descriptors in x and all descriptors in y. Has shape of Bx1x(t_x)x(t_y) \"\"\"", "\n", "result_list", "=", "[", "]", "\n", "num_token_x", "=", "x", ".", "shape", "[", "2", "]", "\n", "for", "token_idx", "in", "range", "(", "num_token_x", ")", ":", "\n", "        ", "token", "=", "x", "[", ":", ",", ":", ",", "token_idx", ",", ":", "]", ".", "unsqueeze", "(", "dim", "=", "2", ")", "# Bx1x1xd'", "\n", "result_list", ".", "append", "(", "torch", ".", "nn", ".", "CosineSimilarity", "(", "dim", "=", "3", ")", "(", "token", ",", "y", ")", ")", "# Bx1xt", "\n", "", "return", "torch", ".", "stack", "(", "result_list", ",", "dim", "=", "2", ")", "# Bx1x(t_x)x(t_y)", "\n", "\n"]], "home.repos.pwc.inspect_result.applied-ai-lab_zero-shot-pose.method.dense_descriptor_utils.extract_saliency_maps": [[85, 97], ["curr_feats[].mean", "curr_feats[].mean.min", "curr_feats[].mean.max"], "function", ["None"], ["", "def", "extract_saliency_maps", "(", "attn_maps", ":", "torch", ".", "Tensor", ",", "head_idxs", "=", "[", "0", ",", "2", ",", "4", ",", "5", "]", ")", "->", "torch", ".", "Tensor", ":", "\n", "    ", "\"\"\"\n    extract saliency maps. The saliency maps are extracted by averaging several attention heads from the last layer\n    in of the CLS token. All values are then normalized to range between 0 and 1.\n    :param attn_maps: attention maps of the last layer. B x H x T x T\n    :return: a tensor of saliency maps. has shape Bxt-1\n    \"\"\"", "\n", "curr_feats", "=", "attn_maps", "#B x h x t x t", "\n", "cls_attn_map", "=", "curr_feats", "[", ":", ",", "head_idxs", ",", "0", ",", "1", ":", "]", ".", "mean", "(", "dim", "=", "1", ")", "#B x (t-1)", "\n", "temp_mins", ",", "temp_maxs", "=", "cls_attn_map", ".", "min", "(", "dim", "=", "1", ")", "[", "0", "]", "[", ":", ",", "None", "]", ",", "cls_attn_map", ".", "max", "(", "dim", "=", "1", ")", "[", "0", "]", "[", ":", ",", "None", "]", "\n", "cls_attn_maps", "=", "(", "cls_attn_map", "-", "temp_mins", ")", "/", "(", "temp_maxs", "-", "temp_mins", ")", "# normalize to range [0,1]", "\n", "return", "cls_attn_maps", "\n", "\n"]], "home.repos.pwc.inspect_result.applied-ai-lab_zero-shot-pose.method.dense_descriptor_utils.gaussian_blurring": [[99, 115], ["features.permute().reshape.size", "numpy.sqrt().astype", "torchvision.transforms.GaussianBlur", "features.permute().reshape.reshape().permute", "torchvision.transforms.GaussianBlur.", "features.permute().reshape.permute().reshape", "numpy.sqrt", "features.permute().reshape.reshape", "features.permute().reshape.permute"], "function", ["None"], ["", "def", "gaussian_blurring", "(", "features", ",", "kernel_size", "=", "9", ",", "sigma", "=", "3", ")", ":", "\n", "\n", "    ", "\"\"\"\n    Assume features in shape B x N x D\n    \"\"\"", "\n", "\n", "b", ",", "n_patches", ",", "d", "=", "features", ".", "size", "(", ")", "\n", "n_patches_h", "=", "np", ".", "sqrt", "(", "n_patches", ")", ".", "astype", "(", "'int'", ")", "\n", "\n", "blur_kernel", "=", "torchvision", ".", "transforms", ".", "GaussianBlur", "(", "kernel_size", ",", "sigma", "=", "(", "sigma", ",", "sigma", ")", ")", "\n", "\n", "features", "=", "features", ".", "reshape", "(", "b", ",", "n_patches_h", ",", "n_patches_h", ",", "d", ")", ".", "permute", "(", "0", ",", "3", ",", "1", ",", "2", ")", "\n", "features", "=", "blur_kernel", "(", "features", ")", "\n", "features", "=", "features", ".", "permute", "(", "0", ",", "2", ",", "3", ",", "1", ")", ".", "reshape", "(", "b", ",", "n_patches_h", "*", "n_patches_h", ",", "d", ")", "\n", "\n", "return", "features", "\n", "\n"]], "home.repos.pwc.inspect_result.applied-ai-lab_zero-shot-pose.method.dense_descriptor_utils._log_bin": [[117, 168], ["int", "x.squeeze", "bin_x.flatten().permute().unsqueeze.permute", "bin_x.flatten().permute().unsqueeze.reshape", "range", "torch.zeros().to", "range", "bin_x.flatten().permute().unsqueeze.flatten().permute().unsqueeze", "ValueError", "numpy.sqrt", "torch.nn.AvgPool2d", "avg_pools.append", "range", "torch.nn.AvgPool2d.", "torch.zeros", "range", "bin_x.flatten().permute().unsqueeze.flatten().permute", "range", "range", "bin_x.flatten().permute().unsqueeze.flatten", "max", "max", "min", "min"], "function", ["home.repos.pwc.inspect_result.applied-ai-lab_zero-shot-pose.datasets.co3d_dataset.FrameData.to"], ["", "def", "_log_bin", "(", "x", ":", "torch", ".", "Tensor", ",", "hierarchy", ":", "int", "=", "2", ",", "device", ":", "str", "=", "'cpu'", ")", "->", "torch", ".", "Tensor", ":", "\n", "    ", "\"\"\"\n    create a log-binned descriptor.\n    :param x: tensor of features. Has shape Bx1xtx(dxh).\n    :param hierarchy: how many bin hierarchies to use.\n    \"\"\"", "\n", "B", "=", "x", ".", "shape", "[", "0", "]", "\n", "if", "x", ".", "shape", "[", "1", "]", "!=", "1", ":", "\n", "        ", "raise", "ValueError", "(", "'log_bin function now expects features reshaped to Bx1xtx(dxh), not Bxhxtxd'", ")", "\n", "", "num_patches", "=", "int", "(", "np", ".", "sqrt", "(", "x", ".", "shape", "[", "2", "]", ")", ")", "\n", "num_bins", "=", "1", "+", "8", "*", "hierarchy", "\n", "\n", "# bin_x = x.permute(0, 2, 3, 1).flatten(start_dim=-2, end_dim=-1)  # Bx(t-1)x(dxh)", "\n", "bin_x", "=", "x", ".", "squeeze", "(", "1", ")", "# Bx(t-1)x(dxh)", "\n", "bin_x", "=", "bin_x", ".", "permute", "(", "0", ",", "2", ",", "1", ")", "# Bx(dxh)x(t-1)", "\n", "bin_x", "=", "bin_x", ".", "reshape", "(", "B", ",", "bin_x", ".", "shape", "[", "1", "]", ",", "num_patches", ",", "num_patches", ")", "\n", "# Bx(dxh)xnum_patches[0]xnum_patches[1]", "\n", "sub_desc_dim", "=", "bin_x", ".", "shape", "[", "1", "]", "\n", "\n", "avg_pools", "=", "[", "]", "\n", "# compute bins of all sizes for all spatial locations.", "\n", "for", "k", "in", "range", "(", "0", ",", "hierarchy", ")", ":", "\n", "# avg pooling with kernel 3**kx3**k", "\n", "        ", "win_size", "=", "3", "**", "k", "\n", "avg_pool", "=", "torch", ".", "nn", ".", "AvgPool2d", "(", "win_size", ",", "stride", "=", "1", ",", "padding", "=", "win_size", "//", "2", ",", "count_include_pad", "=", "False", ")", "\n", "avg_pools", ".", "append", "(", "avg_pool", "(", "bin_x", ")", ")", "\n", "\n", "", "bin_x", "=", "torch", ".", "zeros", "(", "(", "B", ",", "sub_desc_dim", "*", "num_bins", ",", "num_patches", ",", "num_patches", ")", ")", ".", "to", "(", "device", ")", "\n", "for", "y", "in", "range", "(", "num_patches", ")", ":", "\n", "        ", "for", "x", "in", "range", "(", "num_patches", ")", ":", "\n", "            ", "part_idx", "=", "0", "\n", "# fill all bins for a spatial location (y, x)", "\n", "for", "k", "in", "range", "(", "0", ",", "hierarchy", ")", ":", "\n", "                ", "kernel_size", "=", "3", "**", "k", "\n", "for", "i", "in", "range", "(", "y", "-", "kernel_size", ",", "y", "+", "kernel_size", "+", "1", ",", "kernel_size", ")", ":", "\n", "                    ", "for", "j", "in", "range", "(", "x", "-", "kernel_size", ",", "x", "+", "kernel_size", "+", "1", ",", "kernel_size", ")", ":", "\n", "                        ", "if", "i", "==", "y", "and", "j", "==", "x", "and", "k", "!=", "0", ":", "\n", "                            ", "continue", "\n", "", "if", "0", "<=", "i", "<", "num_patches", "and", "0", "<=", "j", "<", "num_patches", ":", "\n", "                            ", "bin_x", "[", ":", ",", "part_idx", "*", "sub_desc_dim", ":", "(", "part_idx", "+", "1", ")", "*", "sub_desc_dim", ",", "y", ",", "x", "]", "=", "avg_pools", "[", "k", "]", "[", "\n", ":", ",", ":", ",", "i", ",", "j", "]", "\n", "", "else", ":", "# handle padding in a more delicate way than zero padding", "\n", "                            ", "temp_i", "=", "max", "(", "0", ",", "min", "(", "i", ",", "num_patches", "-", "1", ")", ")", "\n", "temp_j", "=", "max", "(", "0", ",", "min", "(", "j", ",", "num_patches", "-", "1", ")", ")", "\n", "bin_x", "[", ":", ",", "part_idx", "*", "sub_desc_dim", ":", "(", "part_idx", "+", "1", ")", "*", "sub_desc_dim", ",", "y", ",", "x", "]", "=", "avg_pools", "[", "k", "]", "[", "\n", ":", ",", ":", ",", "temp_i", ",", "\n", "temp_j", "]", "\n", "", "part_idx", "+=", "1", "\n", "", "", "", "", "", "bin_x", "=", "bin_x", ".", "flatten", "(", "start_dim", "=", "-", "2", ",", "end_dim", "=", "-", "1", ")", ".", "permute", "(", "0", ",", "2", ",", "1", ")", ".", "unsqueeze", "(", "dim", "=", "1", ")", "\n", "# Bx1x(t-1)x(dxh)", "\n", "return", "bin_x", "\n", "", ""]], "home.repos.pwc.inspect_result.applied-ai-lab_zero-shot-pose.method.correspondence_functions.find_correspondences_batch": [[13, 117], ["torch.device", "torch.device", "descriptors1.size", "int", "zsp.method.dense_descriptor_utils.extract_saliency_maps", "zsp.method.dense_descriptor_utils.extract_saliency_maps", "time.time", "zsp.method.dense_descriptor_utils.chunk_cosine_sim", "torch.max", "torch.max", "torch.max", "torch.max", "torch.gather", "torch.gather", "[].repeat", "zsp.method.dense_descriptor_utils._to_cartesian().to", "zsp.method.dense_descriptor_utils._to_cartesian().to", "_to_cartesian().to.size", "cyclical_dists.reshape.view", "fg_mask1.float", "cyclical_dists_norm.sort", "torch.gather", "torch.gather", "torch.gather", "torch.gather", "cyclical_dists.reshape.reshape", "cyclical_dists_norm.max", "zsp.method.dense_descriptor_utils._to_cartesian", "int", "int", "zsp.method.dense_descriptor_utils._to_cartesian", "zsp.method.dense_descriptor_utils._to_cartesian", "[].to", "torch.nn.PairwiseDistance", "torch.nn.PairwiseDistance", "_to_cartesian().to.view", "_to_cartesian().to.view", "cyclical_dists.reshape.min", "selected_points_image_1.to", "numpy.sqrt", "numpy.sqrt", "torch.arange", "torch.arange", "torch.Tensor", "torch.Tensor"], "function", ["home.repos.pwc.inspect_result.applied-ai-lab_zero-shot-pose.method.dense_descriptor_utils.extract_saliency_maps", "home.repos.pwc.inspect_result.applied-ai-lab_zero-shot-pose.method.dense_descriptor_utils.extract_saliency_maps", "home.repos.pwc.inspect_result.applied-ai-lab_zero-shot-pose.method.dense_descriptor_utils.chunk_cosine_sim", "home.repos.pwc.inspect_result.applied-ai-lab_zero-shot-pose.datasets.co3d_dataset.FrameData.to", "home.repos.pwc.inspect_result.applied-ai-lab_zero-shot-pose.datasets.co3d_dataset.FrameData.to", "home.repos.pwc.inspect_result.applied-ai-lab_zero-shot-pose.method.dense_descriptor_utils._to_cartesian", "home.repos.pwc.inspect_result.applied-ai-lab_zero-shot-pose.method.dense_descriptor_utils._to_cartesian", "home.repos.pwc.inspect_result.applied-ai-lab_zero-shot-pose.method.dense_descriptor_utils._to_cartesian", "home.repos.pwc.inspect_result.applied-ai-lab_zero-shot-pose.datasets.co3d_dataset.FrameData.to", "home.repos.pwc.inspect_result.applied-ai-lab_zero-shot-pose.datasets.co3d_dataset.FrameData.to"], ["def", "find_correspondences_batch", "(", "descriptors1", ",", "descriptors2", ",", "attn1", ",", "attn2", ",", "\n", "num_pairs", ":", "int", "=", "10", ",", "thresh", ":", "float", "=", "0.05", ",", "device", ":", "torch", ".", "device", "=", "torch", ".", "device", "(", "'cpu'", ")", ")", ":", "\n", "    ", "\"\"\"\n    Finding point correspondences between two images.\n    Legend: B: batch, T: total tokens (num_patches ** 2 + 1), D: Descriptor dim per head, H: Num attention heads\n\n    Method: Compute similarity between all pairs of pixel descriptors\n            Find nearest neighbours from Image1 --> Image2, and Image2 --> Image1\n            Use nearest neighbours to define a cycle from Image1 --> Image2 --> Image1\n            Take points in Image1 (and corresponding points in Image2) which have smallest 'cycle distance'\n            Also, filter examples which aren't part of the foreground in both images, as determined by ViT attention maps\n\n    :param descriptors1: ViT features of shape B x 1 x (T - 1) x D * H (i.e, no CLS token)\n    :param descriptors2: ViT features of shape B x 1 x (T - 1) x D * H (i.e, no CLS token)\n    :param attn1: ViT attention maps from final layer of shape B x H x T x T\n    :param attn2: ViT attention maps from final layer of shape B x H x T x T\n    :param num_pairs: number of outputted corresponding pairs.\n    :param thresh: threshold of saliency maps to distinguish fg and bg.\n    \"\"\"", "\n", "# extracting descriptors for each image", "\n", "B", ",", "_", ",", "t_m_1", ",", "d_h", "=", "descriptors1", ".", "size", "(", ")", "\n", "\n", "# Hard code", "\n", "num_patches1", ",", "load_size1", "=", "(", "int", "(", "np", ".", "sqrt", "(", "t_m_1", ")", ")", ",", "int", "(", "np", ".", "sqrt", "(", "t_m_1", ")", ")", ")", ",", "224", "\n", "inf_idx", "=", "int", "(", "t_m_1", ")", "\n", "\n", "# -----------------", "\n", "# EXTRACT SALIENCE MAPS", "\n", "# -----------------", "\n", "saliency_map1", "=", "extract_saliency_maps", "(", "attn1", ")", "# B x T - 1", "\n", "saliency_map2", "=", "extract_saliency_maps", "(", "attn2", ")", "\n", "# threshold saliency maps to get fg / bg masks", "\n", "fg_mask1", "=", "saliency_map1", ">", "thresh", "\n", "fg_mask2", "=", "saliency_map2", ">", "thresh", "\n", "\n", "# -----------------", "\n", "# COMPUTE SIMILARITIES", "\n", "# calculate similarity between image1 and image2 descriptors", "\n", "# -----------------", "\n", "t1", "=", "time", ".", "time", "(", ")", "\n", "similarities", "=", "chunk_cosine_sim", "(", "descriptors1", ",", "descriptors2", ")", "\n", "\n", "# -----------------", "\n", "# COMPUTE MUTUAL NEAREST NEIGHBOURS", "\n", "# -----------------", "\n", "sim_1", ",", "nn_1", "=", "torch", ".", "max", "(", "similarities", ",", "dim", "=", "-", "1", ",", "keepdim", "=", "False", ")", "# nn_1 - indices of block2 closest to block1. B x T - 1", "\n", "sim_2", ",", "nn_2", "=", "torch", ".", "max", "(", "similarities", ",", "dim", "=", "-", "2", ",", "keepdim", "=", "False", ")", "# nn_2 - indices of block1 closest to block2. B x T - 1", "\n", "nn_1", ",", "nn_2", "=", "nn_1", "[", ":", ",", "0", ",", ":", "]", ",", "nn_2", "[", ":", ",", "0", ",", ":", "]", "\n", "\n", "# Map nn_2 points which are not highlighed by fg_mask to 0", "\n", "nn_2", "[", "~", "fg_mask2", "]", "=", "0", "# TODO: Note, this assumes top left pixel is never a point of interest", "\n", "cyclical_idxs", "=", "torch", ".", "gather", "(", "nn_2", ",", "dim", "=", "-", "1", ",", "index", "=", "nn_1", ")", "# Intuitively, nn_2[nn_1]", "\n", "\n", "# -----------------", "\n", "# COMPUTE SIMILARITIES", "\n", "# Find distance between cyclical point and original point in Image1", "\n", "# -----------------", "\n", "image_idxs", "=", "torch", ".", "arange", "(", "num_patches1", "[", "0", "]", "*", "num_patches1", "[", "1", "]", ")", "[", "None", ",", ":", "]", ".", "repeat", "(", "B", ",", "1", ")", "\n", "cyclical_idxs_ij", "=", "_to_cartesian", "(", "cyclical_idxs", ",", "shape", "=", "num_patches1", ")", ".", "to", "(", "device", ")", "\n", "image_idxs_ij", "=", "_to_cartesian", "(", "image_idxs", ",", "shape", "=", "num_patches1", ")", ".", "to", "(", "device", ")", "\n", "\n", "# Find which points are mapped to 0, artificially map them to a high value", "\n", "# TODO: tom: why the subtraction?", "\n", "zero_mask", "=", "(", "cyclical_idxs_ij", "-", "torch", ".", "Tensor", "(", "[", "0", ",", "0", "]", ")", "[", "None", ",", "None", ",", ":", "]", ".", "to", "(", "device", ")", ")", "==", "0", "\n", "\n", "# TODO: tom: is the inf_idx value correct?", "\n", "cyclical_idxs_ij", "[", "zero_mask", "]", "=", "inf_idx", "\n", "\n", "# Find negative of distance between cyclical point and original point", "\n", "# View to make sure PairwiseDistance behaviour is consistent across torch versions", "\n", "b", ",", "hw", ",", "ij_dim", "=", "cyclical_idxs_ij", ".", "size", "(", ")", "\n", "cyclical_dists", "=", "-", "torch", ".", "nn", ".", "PairwiseDistance", "(", "p", "=", "2", ")", "(", "cyclical_idxs_ij", ".", "view", "(", "-", "1", ",", "ij_dim", ")", ",", "image_idxs_ij", ".", "view", "(", "-", "1", ",", "ij_dim", ")", ")", "\n", "cyclical_dists", "=", "cyclical_dists", ".", "view", "(", "b", ",", "hw", ")", "\n", "\n", "# TODO: tom: why to normalize?", "\n", "cyclical_dists_norm", "=", "cyclical_dists", "-", "cyclical_dists", ".", "min", "(", "1", ",", "keepdim", "=", "True", ")", "[", "0", "]", "# Normalize to [0, 1]", "\n", "cyclical_dists_norm", "/=", "cyclical_dists_norm", ".", "max", "(", "1", ",", "keepdim", "=", "True", ")", "[", "0", "]", "\n", "\n", "# -----------------", "\n", "# Further mask pixel locations in Image1 which are not highlighted by FG mask", "\n", "# -----------------", "\n", "cyclical_dists_norm", "*=", "fg_mask1", ".", "float", "(", ")", "\n", "\n", "# -----------------", "\n", "# Find the TopK points in Image1 and their correspondences in Image2", "\n", "# -----------------", "\n", "sorted_vals", ",", "selected_points_image_1", "=", "cyclical_dists_norm", ".", "sort", "(", "dim", "=", "-", "1", ",", "descending", "=", "True", ")", "\n", "selected_points_image_1", "=", "selected_points_image_1", "[", ":", ",", ":", "num_pairs", "]", "\n", "\n", "# Get corresponding points in image 2", "\n", "selected_points_image_2", "=", "torch", ".", "gather", "(", "nn_1", ",", "dim", "=", "-", "1", ",", "index", "=", "selected_points_image_1", ")", "\n", "\n", "# -----------------", "\n", "# Compute the distances of the selected points", "\n", "# -----------------", "\n", "sim_selected_12", "=", "torch", ".", "gather", "(", "sim_1", "[", ":", ",", "0", ",", ":", "]", ",", "dim", "=", "-", "1", ",", "index", "=", "selected_points_image_1", ".", "to", "(", "device", ")", ")", "\n", "\n", "# Convert to cartesian coordinates", "\n", "selected_points_image_1", ",", "selected_points_image_2", "=", "(", "_to_cartesian", "(", "inds", ",", "shape", "=", "num_patches1", ")", "for", "inds", "in", "\n", "(", "selected_points_image_1", ",", "selected_points_image_2", ")", ")", "\n", "\n", "cyclical_dists", "=", "cyclical_dists", ".", "reshape", "(", "-", "1", ",", "num_patches1", "[", "0", "]", ",", "num_patches1", "[", "1", "]", ")", "\n", "\n", "return", "selected_points_image_1", ",", "selected_points_image_2", ",", "cyclical_dists", ",", "sim_selected_12", "\n", "\n"]], "home.repos.pwc.inspect_result.applied-ai-lab_zero-shot-pose.method.correspondence_functions.find_correspondences_original": [[122, 228], ["torch.device", "torch.device", "descriptors1.size", "zsp.method.dense_descriptor_utils.chunk_cosine_sim", "torch.arange", "torch.arange", "torch.max", "torch.max", "torch.max", "torch.max", "torch.zeros", "torch.zeros", "torch.bitwise_and", "torch.bitwise_and", "torch.bitwise_and", "torch.bitwise_and", "descriptors1[].cpu().numpy", "descriptors2[].cpu().numpy", "numpy.concatenate", "min", "sklearn.cluster.KMeans().fit", "numpy.full", "numpy.full", "range", "zip", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "zsp.method.dense_descriptor_utils.extract_saliency_maps", "zsp.method.dense_descriptor_utils.extract_saliency_maps", "len", "numpy.sqrt", "len", "enumerate", "torch.nonzero().squeeze", "torch.nonzero().squeeze", "torch.arange", "torch.arange", "torch.Tensor.append", "torch.Tensor.append", "int", "int", "int", "int", "descriptors1[].cpu", "descriptors2[].cpu", "sklearn.cluster.KMeans", "zip", "numpy.sqrt", "numpy.sqrt", "numpy.sqrt", "numpy.sqrt", "torch.nonzero", "torch.nonzero", "int", "int", "int", "int"], "function", ["home.repos.pwc.inspect_result.applied-ai-lab_zero-shot-pose.method.dense_descriptor_utils.chunk_cosine_sim", "home.repos.pwc.inspect_result.applied-ai-lab_zero-shot-pose.method.dense_descriptor_utils.extract_saliency_maps", "home.repos.pwc.inspect_result.applied-ai-lab_zero-shot-pose.method.dense_descriptor_utils.extract_saliency_maps", "home.repos.pwc.inspect_result.applied-ai-lab_zero-shot-pose.datasets.co3d_dataset.FrameData.cpu", "home.repos.pwc.inspect_result.applied-ai-lab_zero-shot-pose.datasets.co3d_dataset.FrameData.cpu"], ["", "def", "find_correspondences_original", "(", "descriptors1", ",", "descriptors2", ",", "attn1", ",", "attn2", ",", "\n", "num_pairs", ":", "int", "=", "10", ",", "thresh", ":", "float", "=", "0.05", ",", "patch_size", ":", "int", "=", "16", ",", "\n", "stride", ":", "int", "=", "8", ",", "device", ":", "torch", ".", "device", "=", "torch", ".", "device", "(", "'cpu'", ")", ")", ":", "\n", "    ", "\"\"\"\n    finding point correspondences between two images.\n    Legend: B: batch, T: total tokens (num_patches ** 2 + 1), D: Descriptor dim per head, H: Num attention heads\n    :param descriptors1: ViT features of shape B x 1 x (T - 1) x D * H (i.e, no CLS token)\n    :param descriptors2: ViT features of shape B x 1 x (T - 1) x D * H (i.e, no CLS token)\n    :param attn1: ViT attention maps from final layer of shape B x H x T x T\n    :param attn2: ViT attention maps from final layer of shape B x H x T x T\n    :param num_pairs: number of outputted corresponding pairs.\n    :param load_size: size of the smaller edge of loaded images. If None, does not resize.\n    :param layer: layer to extract descriptors from.\n    :param facet: facet to extract descriptors from.\n    :param bin: if True use a log-binning descriptor.\n    :param thresh: threshold of saliency maps to distinguish fg and bg.\n    :param model_type: type of model to extract descriptors from.\n    :param stride: stride of the model.\n    :return: list of points from image_path1, list of corresponding points from image_path2, the processed pil image of\n    image_path1, and the processed pil image of image_path2.\n    \"\"\"", "\n", "# extracting descriptors for each image", "\n", "B", ",", "_", ",", "t_m_1", ",", "d_h", "=", "descriptors1", ".", "size", "(", ")", "\n", "\n", "# Hard code", "\n", "num_patches1", ",", "load_size1", "=", "(", "int", "(", "np", ".", "sqrt", "(", "t_m_1", ")", ")", ",", "int", "(", "np", ".", "sqrt", "(", "t_m_1", ")", ")", ")", ",", "224", "\n", "num_patches2", ",", "load_size2", "=", "(", "int", "(", "np", ".", "sqrt", "(", "t_m_1", ")", ")", ",", "int", "(", "np", ".", "sqrt", "(", "t_m_1", ")", ")", ")", ",", "224", "\n", "\n", "# extracting saliency maps for each image", "\n", "saliency_map1", "=", "extract_saliency_maps", "(", "attn1", ")", "[", "0", "]", "\n", "saliency_map2", "=", "extract_saliency_maps", "(", "attn2", ")", "[", "0", "]", "\n", "\n", "# threshold saliency maps to get fg / bg masks", "\n", "fg_mask1", "=", "saliency_map1", ">", "thresh", "\n", "fg_mask2", "=", "saliency_map2", ">", "thresh", "\n", "\n", "# calculate similarity between image1 and image2 descriptors", "\n", "similarities", "=", "chunk_cosine_sim", "(", "descriptors1", ",", "descriptors2", ")", "\n", "\n", "# calculate best buddies", "\n", "image_idxs", "=", "torch", ".", "arange", "(", "num_patches1", "[", "0", "]", "*", "num_patches1", "[", "1", "]", ",", "device", "=", "device", ")", "\n", "sim_1", ",", "nn_1", "=", "torch", ".", "max", "(", "similarities", ",", "dim", "=", "-", "1", ")", "# nn_1 - indices of block2 closest to block1", "\n", "sim_2", ",", "nn_2", "=", "torch", ".", "max", "(", "similarities", ",", "dim", "=", "-", "2", ")", "# nn_2 - indices of block1 closest to block2", "\n", "sim_1", ",", "nn_1", "=", "sim_1", "[", "0", ",", "0", "]", ",", "nn_1", "[", "0", ",", "0", "]", "\n", "sim_2", ",", "nn_2", "=", "sim_2", "[", "0", ",", "0", "]", ",", "nn_2", "[", "0", ",", "0", "]", "\n", "bbs_mask", "=", "nn_2", "[", "nn_1", "]", "==", "image_idxs", "\n", "\n", "# remove best buddies where at least one descriptor is marked bg by saliency mask.", "\n", "fg_mask2_new_coors", "=", "nn_2", "[", "fg_mask2", "]", "\n", "fg_mask2_mask_new_coors", "=", "torch", ".", "zeros", "(", "num_patches1", "[", "0", "]", "*", "num_patches1", "[", "1", "]", ",", "dtype", "=", "torch", ".", "bool", ",", "device", "=", "device", ")", "\n", "fg_mask2_mask_new_coors", "[", "fg_mask2_new_coors", "]", "=", "True", "\n", "bbs_mask", "=", "torch", ".", "bitwise_and", "(", "bbs_mask", ",", "fg_mask1", ")", "\n", "bbs_mask", "=", "torch", ".", "bitwise_and", "(", "bbs_mask", ",", "fg_mask2_mask_new_coors", ")", "\n", "\n", "# applying k-means to extract k high quality well distributed correspondence pairs", "\n", "bb_descs1", "=", "descriptors1", "[", "0", ",", "0", ",", "bbs_mask", ",", ":", "]", ".", "cpu", "(", ")", ".", "numpy", "(", ")", "\n", "bb_descs2", "=", "descriptors2", "[", "0", ",", "0", ",", "nn_1", "[", "bbs_mask", "]", ",", ":", "]", ".", "cpu", "(", ")", ".", "numpy", "(", ")", "\n", "# apply k-means on a concatenation of a pairs descriptors.", "\n", "all_keys_together", "=", "np", ".", "concatenate", "(", "(", "bb_descs1", ",", "bb_descs2", ")", ",", "axis", "=", "1", ")", "\n", "n_clusters", "=", "min", "(", "num_pairs", ",", "len", "(", "all_keys_together", ")", ")", "# if not enough pairs, show all found pairs.", "\n", "length", "=", "np", ".", "sqrt", "(", "(", "all_keys_together", "**", "2", ")", ".", "sum", "(", "axis", "=", "1", ")", ")", "[", ":", ",", "None", "]", "\n", "normalized", "=", "all_keys_together", "/", "length", "\n", "\n", "if", "len", "(", "normalized", ")", "==", "0", ":", "\n", "        ", "return", "[", "]", ",", "[", "]", "\n", "\n", "", "kmeans", "=", "KMeans", "(", "n_clusters", "=", "n_clusters", ",", "random_state", "=", "0", ")", ".", "fit", "(", "normalized", ")", "\n", "bb_topk_sims", "=", "np", ".", "full", "(", "(", "n_clusters", ")", ",", "-", "np", ".", "inf", ")", "\n", "bb_indices_to_show", "=", "np", ".", "full", "(", "(", "n_clusters", ")", ",", "-", "np", ".", "inf", ")", "\n", "\n", "# rank pairs by their mean saliency value", "\n", "bb_cls_attn1", "=", "saliency_map1", "[", "bbs_mask", "]", "\n", "bb_cls_attn2", "=", "saliency_map2", "[", "nn_1", "[", "bbs_mask", "]", "]", "\n", "bb_cls_attn", "=", "(", "bb_cls_attn1", "+", "bb_cls_attn2", ")", "/", "2", "\n", "ranks", "=", "bb_cls_attn", "\n", "\n", "for", "k", "in", "range", "(", "n_clusters", ")", ":", "\n", "        ", "for", "i", ",", "(", "label", ",", "rank", ")", "in", "enumerate", "(", "zip", "(", "kmeans", ".", "labels_", ",", "ranks", ")", ")", ":", "\n", "            ", "if", "rank", ">", "bb_topk_sims", "[", "label", "]", ":", "\n", "                ", "bb_topk_sims", "[", "label", "]", "=", "rank", "\n", "bb_indices_to_show", "[", "label", "]", "=", "i", "\n", "\n", "# get coordinates to show", "\n", "", "", "", "indices_to_show", "=", "torch", ".", "nonzero", "(", "bbs_mask", ",", "as_tuple", "=", "False", ")", ".", "squeeze", "(", "dim", "=", "1", ")", "[", "\n", "bb_indices_to_show", "]", "# close bbs", "\n", "img1_indices_to_show", "=", "torch", ".", "arange", "(", "num_patches1", "[", "0", "]", "*", "num_patches1", "[", "1", "]", ",", "device", "=", "device", ")", "[", "indices_to_show", "]", "\n", "sim_selected_12", "=", "sim_1", "[", "img1_indices_to_show", "]", "\n", "\n", "img2_indices_to_show", "=", "nn_1", "[", "indices_to_show", "]", "\n", "# coordinates in descriptor map's dimensions", "\n", "img1_y_to_show", "=", "(", "img1_indices_to_show", "/", "num_patches1", "[", "1", "]", ")", ".", "cpu", "(", ")", ".", "numpy", "(", ")", "\n", "img1_x_to_show", "=", "(", "img1_indices_to_show", "%", "num_patches1", "[", "1", "]", ")", ".", "cpu", "(", ")", ".", "numpy", "(", ")", "\n", "img2_y_to_show", "=", "(", "img2_indices_to_show", "/", "num_patches2", "[", "1", "]", ")", ".", "cpu", "(", ")", ".", "numpy", "(", ")", "\n", "img2_x_to_show", "=", "(", "img2_indices_to_show", "%", "num_patches2", "[", "1", "]", ")", ".", "cpu", "(", ")", ".", "numpy", "(", ")", "\n", "points1", ",", "points2", "=", "[", "]", ",", "[", "]", "\n", "for", "y1", ",", "x1", ",", "y2", ",", "x2", "in", "zip", "(", "img1_y_to_show", ",", "img1_x_to_show", ",", "img2_y_to_show", ",", "img2_x_to_show", ")", ":", "\n", "        ", "x1_show", "=", "(", "int", "(", "x1", ")", "-", "1", ")", "*", "stride", "+", "stride", "+", "patch_size", "//", "2", "\n", "y1_show", "=", "(", "int", "(", "y1", ")", "-", "1", ")", "*", "stride", "+", "stride", "+", "patch_size", "//", "2", "\n", "x2_show", "=", "(", "int", "(", "x2", ")", "-", "1", ")", "*", "stride", "+", "stride", "+", "patch_size", "//", "2", "\n", "y2_show", "=", "(", "int", "(", "y2", ")", "-", "1", ")", "*", "stride", "+", "stride", "+", "patch_size", "//", "2", "\n", "points1", ".", "append", "(", "(", "y1_show", ",", "x1_show", ")", ")", "\n", "points2", ".", "append", "(", "(", "y2_show", ",", "x2_show", ")", ")", "\n", "", "points1", "=", "torch", ".", "Tensor", "(", "points1", ")", "\n", "points2", "=", "torch", ".", "Tensor", "(", "points2", ")", "\n", "\n", "return", "points1", ",", "points2", ",", "sim_selected_12", "\n", "\n"]], "home.repos.pwc.inspect_result.applied-ai-lab_zero-shot-pose.method.correspondence_functions.find_correspondences_batch_with_knn": [[234, 366], ["torch.device", "torch.device", "descriptors1.size", "int", "zsp.method.dense_descriptor_utils.extract_saliency_maps", "zsp.method.dense_descriptor_utils.extract_saliency_maps", "zsp.method.dense_descriptor_utils.chunk_cosine_sim", "torch.max", "torch.max", "torch.max", "torch.max", "torch.gather", "torch.gather", "[].repeat", "zsp.method.dense_descriptor_utils._to_cartesian().to", "zsp.method.dense_descriptor_utils._to_cartesian().to", "_to_cartesian().to.size", "cyclical_dists.reshape.view", "fg_mask1.float", "cyclical_dists_norm.sort", "range", "torch.stack", "torch.stack", "torch.gather", "torch.gather", "torch.gather", "torch.gather", "cyclical_dists.reshape.reshape", "cyclical_dists_norm.max", "torch.normalize().cpu().numpy", "sklearn.cluster.KMeans().fit", "torch.as_tensor().to", "torch.as_tensor().to", "range", "torch.stack", "torch.stack", "torch.stack.append", "zsp.method.dense_descriptor_utils._to_cartesian", "int", "int", "zsp.method.dense_descriptor_utils._to_cartesian", "zsp.method.dense_descriptor_utils._to_cartesian", "[].to", "torch.nn.PairwiseDistance", "torch.nn.PairwiseDistance", "_to_cartesian().to.view", "_to_cartesian().to.view", "cyclical_dists.reshape.min", "saliencies_at_k.argmax", "torch.stack.append", "torch.stack.to", "numpy.sqrt", "numpy.sqrt", "torch.arange", "torch.arange", "torch.normalize().cpu", "sklearn.cluster.KMeans", "torch.as_tensor", "torch.as_tensor", "torch.where", "torch.where", "torch.Tensor", "torch.Tensor", "torch.normalize"], "function", ["home.repos.pwc.inspect_result.applied-ai-lab_zero-shot-pose.method.dense_descriptor_utils.extract_saliency_maps", "home.repos.pwc.inspect_result.applied-ai-lab_zero-shot-pose.method.dense_descriptor_utils.extract_saliency_maps", "home.repos.pwc.inspect_result.applied-ai-lab_zero-shot-pose.method.dense_descriptor_utils.chunk_cosine_sim", "home.repos.pwc.inspect_result.applied-ai-lab_zero-shot-pose.datasets.co3d_dataset.FrameData.to", "home.repos.pwc.inspect_result.applied-ai-lab_zero-shot-pose.datasets.co3d_dataset.FrameData.to", "home.repos.pwc.inspect_result.applied-ai-lab_zero-shot-pose.datasets.co3d_dataset.FrameData.to", "home.repos.pwc.inspect_result.applied-ai-lab_zero-shot-pose.datasets.co3d_dataset.FrameData.to", "home.repos.pwc.inspect_result.applied-ai-lab_zero-shot-pose.method.dense_descriptor_utils._to_cartesian", "home.repos.pwc.inspect_result.applied-ai-lab_zero-shot-pose.method.dense_descriptor_utils._to_cartesian", "home.repos.pwc.inspect_result.applied-ai-lab_zero-shot-pose.method.dense_descriptor_utils._to_cartesian", "home.repos.pwc.inspect_result.applied-ai-lab_zero-shot-pose.datasets.co3d_dataset.FrameData.to", "home.repos.pwc.inspect_result.applied-ai-lab_zero-shot-pose.datasets.co3d_dataset.FrameData.to", "home.repos.pwc.inspect_result.applied-ai-lab_zero-shot-pose.datasets.co3d_dataset.FrameData.cpu"], ["", "def", "find_correspondences_batch_with_knn", "(", "descriptors1", ",", "descriptors2", ",", "attn1", ",", "attn2", ",", "\n", "num_pairs_for_topk", ":", "int", "=", "10", ",", "thresh", ":", "float", "=", "0.05", ",", "high_res", "=", "False", ",", "\n", "device", ":", "torch", ".", "device", "=", "torch", ".", "device", "(", "'cpu'", ")", ")", ":", "\n", "    ", "\"\"\"\n    Finding point correspondences between two images.\n    Legend: B: batch, T: total tokens (num_patches ** 2 + 1), D: Descriptor dim per head, H: Num attention heads\n\n    Method: Compute similarity between all pairs of pixel descriptors\n            Find nearest neighbours from Image1 --> Image2, and Image2 --> Image1\n            Use nearest neighbours to define a cycle from Image1 --> Image2 --> Image1\n            Take points in Image1 (and corresponding points in Image2) which have smallest 'cycle distance'\n            Also, filter examples which aren't part of the foreground in both images, as determined by ViT attention maps\n\n    :param descriptors1: ViT features of shape B x 1 x (T - 1) x D * H (i.e, no CLS token)\n    :param descriptors2: ViT features of shape B x 1 x (T - 1) x D * H (i.e, no CLS token)\n    :param attn1: ViT attention maps from final layer of shape B x H x T x T\n    :param attn2: ViT attention maps from final layer of shape B x H x T x T\n    :param num_pairs: number of outputted corresponding pairs.\n    :param thresh: threshold of saliency maps to distinguish fg and bg.\n    \"\"\"", "\n", "# extracting descriptors for each image", "\n", "B", ",", "_", ",", "t_m_1", ",", "d_h", "=", "descriptors1", ".", "size", "(", ")", "\n", "\n", "# Hard code", "\n", "num_patches1", ",", "load_size1", "=", "(", "int", "(", "np", ".", "sqrt", "(", "t_m_1", ")", ")", ",", "int", "(", "np", ".", "sqrt", "(", "t_m_1", ")", ")", ")", ",", "224", "\n", "inf_idx", "=", "int", "(", "t_m_1", ")", "\n", "\n", "# -----------------", "\n", "# EXTRACT SALIENCE MAPS", "\n", "# -----------------", "\n", "saliency_map1", "=", "extract_saliency_maps", "(", "attn1", ")", "# B x T - 1", "\n", "saliency_map2", "=", "extract_saliency_maps", "(", "attn2", ")", "\n", "\n", "# threshold saliency maps to get fg / bg masks", "\n", "fg_mask1", "=", "saliency_map1", ">", "thresh", "\n", "fg_mask2", "=", "saliency_map2", ">", "thresh", "\n", "\n", "# -----------------", "\n", "# COMPUTE SIMILARITIES", "\n", "# calculate similarity between image1 and image2 descriptors", "\n", "# -----------------", "\n", "similarities", "=", "chunk_cosine_sim", "(", "descriptors1", ",", "descriptors2", ")", "\n", "\n", "# -----------------", "\n", "# COMPUTE MUTUAL NEAREST NEIGHBOURS", "\n", "# -----------------", "\n", "sim_1", ",", "nn_1", "=", "torch", ".", "max", "(", "similarities", ",", "dim", "=", "-", "1", ",", "keepdim", "=", "False", ")", "# nn_1 - indices of block2 closest to block1. B x T - 1", "\n", "sim_2", ",", "nn_2", "=", "torch", ".", "max", "(", "similarities", ",", "dim", "=", "-", "2", ",", "keepdim", "=", "False", ")", "# nn_2 - indices of block1 closest to block2. B x T - 1", "\n", "nn_1", ",", "nn_2", "=", "nn_1", "[", ":", ",", "0", ",", ":", "]", ",", "nn_2", "[", ":", ",", "0", ",", ":", "]", "\n", "\n", "# Map nn_2 points which are not highlighed by fg_mask to 0", "\n", "nn_2", "[", "~", "fg_mask2", "]", "=", "0", "# TODO: Note, this assumes top left pixel is never a point of interest", "\n", "cyclical_idxs", "=", "torch", ".", "gather", "(", "nn_2", ",", "dim", "=", "-", "1", ",", "index", "=", "nn_1", ")", "# Intuitively, nn_2[nn_1]", "\n", "\n", "# -----------------", "\n", "# COMPUTE SIMILARITIES", "\n", "# Find distance between cyclical point and original point in Image1", "\n", "# -----------------", "\n", "image_idxs", "=", "torch", ".", "arange", "(", "num_patches1", "[", "0", "]", "*", "num_patches1", "[", "1", "]", ")", "[", "None", ",", ":", "]", ".", "repeat", "(", "B", ",", "1", ")", "\n", "cyclical_idxs_ij", "=", "_to_cartesian", "(", "cyclical_idxs", ",", "shape", "=", "num_patches1", ")", ".", "to", "(", "device", ")", "\n", "image_idxs_ij", "=", "_to_cartesian", "(", "image_idxs", ",", "shape", "=", "num_patches1", ")", ".", "to", "(", "device", ")", "\n", "\n", "# Find which points are mapped to 0, artificially map them to a high value", "\n", "zero_mask", "=", "(", "cyclical_idxs_ij", "-", "torch", ".", "Tensor", "(", "[", "0", ",", "0", "]", ")", "[", "None", ",", "None", ",", ":", "]", ".", "to", "(", "device", ")", ")", "==", "0", "\n", "cyclical_idxs_ij", "[", "zero_mask", "]", "=", "inf_idx", "\n", "\n", "# Find negative of distance between cyclical point and original point", "\n", "# View to make sure PairwiseDistance behaviour is consistent across torch versions", "\n", "b", ",", "hw", ",", "ij_dim", "=", "cyclical_idxs_ij", ".", "size", "(", ")", "\n", "cyclical_dists", "=", "-", "torch", ".", "nn", ".", "PairwiseDistance", "(", "p", "=", "2", ")", "(", "cyclical_idxs_ij", ".", "view", "(", "-", "1", ",", "ij_dim", ")", ",", "image_idxs_ij", ".", "view", "(", "-", "1", ",", "ij_dim", ")", ")", "\n", "cyclical_dists", "=", "cyclical_dists", ".", "view", "(", "b", ",", "hw", ")", "\n", "\n", "cyclical_dists_norm", "=", "cyclical_dists", "-", "cyclical_dists", ".", "min", "(", "1", ",", "keepdim", "=", "True", ")", "[", "0", "]", "# Normalize to [0, 1]", "\n", "cyclical_dists_norm", "/=", "cyclical_dists_norm", ".", "max", "(", "1", ",", "keepdim", "=", "True", ")", "[", "0", "]", "\n", "\n", "# -----------------", "\n", "# Further mask pixel locations in Image1 which are not highlighted by FG mask", "\n", "# -----------------", "\n", "cyclical_dists_norm", "*=", "fg_mask1", ".", "float", "(", ")", "\n", "\n", "# -----------------", "\n", "# Find the TopK points in Image1 and their correspondences in Image2", "\n", "# -----------------", "\n", "sorted_vals", ",", "topk_candidate_points_image_1", "=", "cyclical_dists_norm", ".", "sort", "(", "dim", "=", "-", "1", ",", "descending", "=", "True", ")", "\n", "topk_candidate_points_image_1", "=", "topk_candidate_points_image_1", "[", ":", ",", ":", "num_pairs_for_topk", "]", "\n", "\n", "# -----------------", "\n", "# Now do K-Means clustering on the descriptors in image 1 to choose well distributed features", "\n", "# -----------------", "\n", "if", "high_res", ":", "\n", "        ", "num_pairs_to_return", "=", "num_pairs_for_topk", "//", "8", "\n", "", "else", ":", "\n", "        ", "num_pairs_to_return", "=", "num_pairs_for_topk", "//", "2", "\n", "", "selected_points_image_1", "=", "[", "]", "\n", "for", "b", "in", "range", "(", "B", ")", ":", "\n", "\n", "        ", "idxs_b", "=", "topk_candidate_points_image_1", "[", "b", "]", "\n", "feats_b", "=", "descriptors1", "[", "b", "]", "[", "0", ",", ":", ",", ":", "]", "[", "idxs_b", "]", "# num_pairs_for_topk x D * H", "\n", "feats_b", "=", "F", ".", "normalize", "(", "feats_b", ",", "dim", "=", "-", "1", ")", ".", "cpu", "(", ")", ".", "numpy", "(", ")", "\n", "salience_b", "=", "saliency_map1", "[", "b", "]", "[", "idxs_b", "]", "# num_pairs_for_topk", "\n", "\n", "kmeans", "=", "KMeans", "(", "n_clusters", "=", "num_pairs_to_return", ",", "random_state", "=", "0", ")", ".", "fit", "(", "feats_b", ")", "\n", "kmeans_labels", "=", "torch", ".", "as_tensor", "(", "kmeans", ".", "labels_", ")", ".", "to", "(", "device", ")", "\n", "\n", "final_idxs_chosen_from_image_1_b", "=", "[", "]", "\n", "for", "k", "in", "range", "(", "num_pairs_to_return", ")", ":", "\n", "\n", "            ", "locations_in_cluster_k", "=", "torch", ".", "where", "(", "kmeans_labels", "==", "k", ")", "[", "0", "]", "\n", "saliencies_at_k", "=", "salience_b", "[", "locations_in_cluster_k", "]", "\n", "point_chosen_from_cluster_k", "=", "saliencies_at_k", ".", "argmax", "(", ")", "\n", "final_idxs_chosen_from_image_1_b", ".", "append", "(", "idxs_b", "[", "locations_in_cluster_k", "]", "[", "point_chosen_from_cluster_k", "]", ")", "\n", "\n", "", "final_idxs_chosen_from_image_1_b", "=", "torch", ".", "stack", "(", "final_idxs_chosen_from_image_1_b", ")", "\n", "selected_points_image_1", ".", "append", "(", "final_idxs_chosen_from_image_1_b", ")", "\n", "\n", "", "selected_points_image_1", "=", "torch", ".", "stack", "(", "selected_points_image_1", ")", "\n", "\n", "# Get corresponding points in image 2", "\n", "selected_points_image_2", "=", "torch", ".", "gather", "(", "nn_1", ",", "dim", "=", "-", "1", ",", "index", "=", "selected_points_image_1", ")", "\n", "\n", "# -----------------", "\n", "# Compute the distances of the selected points", "\n", "# -----------------", "\n", "sim_selected_12", "=", "torch", ".", "gather", "(", "sim_1", "[", ":", ",", "0", ",", ":", "]", ",", "dim", "=", "-", "1", ",", "index", "=", "selected_points_image_1", ".", "to", "(", "device", ")", ")", "\n", "\n", "# Convert to cartesian coordinates", "\n", "selected_points_image_1", ",", "selected_points_image_2", "=", "(", "_to_cartesian", "(", "inds", ",", "shape", "=", "num_patches1", ")", "for", "inds", "in", "\n", "(", "selected_points_image_1", ",", "selected_points_image_2", ")", ")", "\n", "\n", "cyclical_dists", "=", "cyclical_dists", ".", "reshape", "(", "-", "1", ",", "num_patches1", "[", "0", "]", ",", "num_patches1", "[", "1", "]", ")", "\n", "\n", "return", "selected_points_image_1", ",", "selected_points_image_2", ",", "cyclical_dists", ",", "sim_selected_12", "", "", ""]]}