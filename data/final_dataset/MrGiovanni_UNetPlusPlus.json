{"home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.tests.test_steps_for_sliding_window_prediction.TestSlidingWindow.setUp": [[22, 24], ["None"], "methods", ["None"], ["    ", "def", "setUp", "(", "self", ")", "->", "None", ":", "\n", "        ", "pass", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.tests.test_steps_for_sliding_window_prediction.TestSlidingWindow._verify_steps": [[25, 59], ["test_steps_for_sliding_window_prediction.TestSlidingWindow.assertTrue", "range", "all", "len", "test_steps_for_sliding_window_prediction.TestSlidingWindow.assertTrue", "test_steps_for_sliding_window_prediction.TestSlidingWindow.assertTrue", "test_steps_for_sliding_window_prediction.TestSlidingWindow.assertTrue", "test_steps_for_sliding_window_prediction.TestSlidingWindow.assertTrue", "str", "str", "str", "int", "zip", "all", "all", "numpy.ceil", "str", "len", "enumerate", "str", "str", "str", "str", "range", "range", "numpy.ceil"], "methods", ["None"], ["", "def", "_verify_steps", "(", "self", ",", "steps", ",", "patch_size", ",", "image_size", ",", "step_size", ")", ":", "\n", "        ", "debug_information", "=", "'steps= %s\\nimage_size= %s\\npatch_size= %s\\nstep_size= %0.4f'", "%", "(", "str", "(", "steps", ")", ",", "\n", "str", "(", "image_size", ")", ",", "\n", "str", "(", "patch_size", ")", ",", "step_size", ")", "\n", "target_step_sizes_in_voxels", "=", "[", "i", "*", "step_size", "for", "i", "in", "patch_size", "]", "\n", "\n", "# this code is copied form the current implementation. Not ideal, but I don't know hoe else to the the", "\n", "# expected num_steps", "\n", "num_steps", "=", "[", "int", "(", "np", ".", "ceil", "(", "(", "i", "-", "k", ")", "/", "j", ")", ")", "+", "1", "for", "i", ",", "j", ",", "k", "in", "zip", "(", "image_size", ",", "target_step_sizes_in_voxels", ",", "\n", "patch_size", ")", "]", "\n", "\n", "self", ".", "assertTrue", "(", "all", "(", "[", "len", "(", "i", ")", "==", "num_steps", "[", "j", "]", "for", "j", ",", "i", "in", "enumerate", "(", "steps", ")", "]", ")", ",", "\n", "'steps do not match expected num_steps %s. \\nDebug: %s'", "%", "(", "str", "(", "num_steps", ")", ",", "debug_information", ")", ")", "\n", "\n", "for", "dim", "in", "range", "(", "len", "(", "steps", ")", ")", ":", "\n", "# first step must start at 0", "\n", "            ", "self", ".", "assertTrue", "(", "steps", "[", "dim", "]", "[", "0", "]", "==", "0", ")", "\n", "\n", "# last step + patch size must equal to image size", "\n", "self", ".", "assertTrue", "(", "steps", "[", "dim", "]", "[", "-", "1", "]", "+", "patch_size", "[", "dim", "]", "==", "image_size", "[", "dim", "]", ",", "'not the whole image is covered. '", "\n", "'\\nDebug: %s'", "%", "debug_information", ")", "\n", "\n", "# there cannot be gaps between adjacent predictions", "\n", "self", ".", "assertTrue", "(", "all", "(", "[", "steps", "[", "dim", "]", "[", "i", "+", "1", "]", "<=", "steps", "[", "dim", "]", "[", "i", "]", "+", "patch_size", "[", "dim", "]", "for", "i", "in", "\n", "range", "(", "num_steps", "[", "dim", "]", "-", "1", ")", "]", ")", ",", "'steps are not overlapping or touching. dim: %d, steps:'", "\n", "' %s, image_size: %s, patch_size: %s, step_size: '", "\n", "'%0.4f'", "%", "(", "\n", "dim", ",", "str", "(", "steps", "[", "dim", "]", ")", ",", "str", "(", "image_size", "[", "dim", "]", ")", ",", "str", "(", "patch_size", "[", "dim", "]", ")", ",", "step_size", ")", ")", "\n", "\n", "# two successive steps cannot be further apart than target_step_sizes_in_voxels", "\n", "self", ".", "assertTrue", "(", "all", "(", "[", "steps", "[", "dim", "]", "[", "i", "]", "+", "np", ".", "ceil", "(", "target_step_sizes_in_voxels", "[", "dim", "]", ")", ">=", "steps", "[", "dim", "]", "[", "i", "+", "1", "]", "for", "i", "\n", "in", "range", "(", "num_steps", "[", "dim", "]", "-", "1", ")", "]", ")", ",", "\n", "'consecutive steps are too far apart. Steps: %s, dim: %d. \\nDebug: %s'", "%", "\n", "(", "str", "(", "steps", "[", "dim", "]", ")", ",", "dim", ",", "debug_information", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.tests.test_steps_for_sliding_window_prediction.TestSlidingWindow.test_same_image_and_patch_size_3d": [[60, 77], ["nnunet.network_architecture.neural_network.SegmentationNetwork._compute_steps_for_sliding_window", "test_steps_for_sliding_window_prediction.TestSlidingWindow.assertTrue", "nnunet.network_architecture.neural_network.SegmentationNetwork._compute_steps_for_sliding_window", "test_steps_for_sliding_window_prediction.TestSlidingWindow.assertTrue", "nnunet.network_architecture.neural_network.SegmentationNetwork._compute_steps_for_sliding_window", "test_steps_for_sliding_window_prediction.TestSlidingWindow.assertTrue"], "methods", ["home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.network_architecture.neural_network.SegmentationNetwork._compute_steps_for_sliding_window", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.network_architecture.neural_network.SegmentationNetwork._compute_steps_for_sliding_window", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.network_architecture.neural_network.SegmentationNetwork._compute_steps_for_sliding_window"], ["", "", "def", "test_same_image_and_patch_size_3d", "(", "self", ")", ":", "\n", "        ", "image_size", "=", "(", "24", ",", "845", ",", "321", ")", "\n", "patch_size", "=", "(", "24", ",", "845", ",", "321", ")", "\n", "\n", "# this should always return steps=[[0],[0],[0]] no matter what step_size we choose", "\n", "expected_result", "=", "[", "[", "0", "]", ",", "[", "0", "]", ",", "[", "0", "]", "]", "\n", "step_size", "=", "1", "\n", "steps", "=", "SegmentationNetwork", ".", "_compute_steps_for_sliding_window", "(", "patch_size", ",", "image_size", ",", "step_size", ")", "\n", "self", ".", "assertTrue", "(", "steps", "==", "expected_result", ")", "\n", "\n", "step_size", "=", "0.125", "\n", "steps", "=", "SegmentationNetwork", ".", "_compute_steps_for_sliding_window", "(", "patch_size", ",", "image_size", ",", "step_size", ")", "\n", "self", ".", "assertTrue", "(", "steps", "==", "expected_result", ")", "\n", "\n", "step_size", "=", "0.5", "\n", "steps", "=", "SegmentationNetwork", ".", "_compute_steps_for_sliding_window", "(", "patch_size", ",", "image_size", ",", "step_size", ")", "\n", "self", ".", "assertTrue", "(", "steps", "==", "expected_result", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.tests.test_steps_for_sliding_window_prediction.TestSlidingWindow.test_same_image_and_patch_size_2d": [[78, 95], ["nnunet.network_architecture.neural_network.SegmentationNetwork._compute_steps_for_sliding_window", "test_steps_for_sliding_window_prediction.TestSlidingWindow.assertTrue", "nnunet.network_architecture.neural_network.SegmentationNetwork._compute_steps_for_sliding_window", "test_steps_for_sliding_window_prediction.TestSlidingWindow.assertTrue", "nnunet.network_architecture.neural_network.SegmentationNetwork._compute_steps_for_sliding_window", "test_steps_for_sliding_window_prediction.TestSlidingWindow.assertTrue"], "methods", ["home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.network_architecture.neural_network.SegmentationNetwork._compute_steps_for_sliding_window", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.network_architecture.neural_network.SegmentationNetwork._compute_steps_for_sliding_window", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.network_architecture.neural_network.SegmentationNetwork._compute_steps_for_sliding_window"], ["", "def", "test_same_image_and_patch_size_2d", "(", "self", ")", ":", "\n", "        ", "image_size", "=", "(", "123", ",", "143", ")", "\n", "patch_size", "=", "(", "123", ",", "143", ")", "\n", "\n", "# this should always return steps=[[0],[0]] no matter what step_size we choose", "\n", "expected_result", "=", "[", "[", "0", "]", ",", "[", "0", "]", "]", "\n", "step_size", "=", "1", "\n", "steps", "=", "SegmentationNetwork", ".", "_compute_steps_for_sliding_window", "(", "patch_size", ",", "image_size", ",", "step_size", ")", "\n", "self", ".", "assertTrue", "(", "steps", "==", "expected_result", ")", "\n", "\n", "step_size", "=", "0.125", "\n", "steps", "=", "SegmentationNetwork", ".", "_compute_steps_for_sliding_window", "(", "patch_size", ",", "image_size", ",", "step_size", ")", "\n", "self", ".", "assertTrue", "(", "steps", "==", "expected_result", ")", "\n", "\n", "step_size", "=", "0.5", "\n", "steps", "=", "SegmentationNetwork", ".", "_compute_steps_for_sliding_window", "(", "patch_size", ",", "image_size", ",", "step_size", ")", "\n", "self", ".", "assertTrue", "(", "steps", "==", "expected_result", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.tests.test_steps_for_sliding_window_prediction.TestSlidingWindow.test_some_manually_verified_combinations": [[96, 163], ["nnunet.network_architecture.neural_network.SegmentationNetwork._compute_steps_for_sliding_window", "test_steps_for_sliding_window_prediction.TestSlidingWindow.assertTrue", "nnunet.network_architecture.neural_network.SegmentationNetwork._compute_steps_for_sliding_window", "test_steps_for_sliding_window_prediction.TestSlidingWindow.assertTrue", "nnunet.network_architecture.neural_network.SegmentationNetwork._compute_steps_for_sliding_window", "test_steps_for_sliding_window_prediction.TestSlidingWindow.assertTrue", "nnunet.network_architecture.neural_network.SegmentationNetwork._compute_steps_for_sliding_window", "test_steps_for_sliding_window_prediction.TestSlidingWindow.assertTrue", "nnunet.network_architecture.neural_network.SegmentationNetwork._compute_steps_for_sliding_window", "test_steps_for_sliding_window_prediction.TestSlidingWindow.assertTrue", "nnunet.network_architecture.neural_network.SegmentationNetwork._compute_steps_for_sliding_window", "test_steps_for_sliding_window_prediction.TestSlidingWindow.assertTrue", "nnunet.network_architecture.neural_network.SegmentationNetwork._compute_steps_for_sliding_window", "test_steps_for_sliding_window_prediction.TestSlidingWindow.assertTrue", "nnunet.network_architecture.neural_network.SegmentationNetwork._compute_steps_for_sliding_window", "test_steps_for_sliding_window_prediction.TestSlidingWindow.assertTrue", "nnunet.network_architecture.neural_network.SegmentationNetwork._compute_steps_for_sliding_window", "test_steps_for_sliding_window_prediction.TestSlidingWindow.assertTrue"], "methods", ["home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.network_architecture.neural_network.SegmentationNetwork._compute_steps_for_sliding_window", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.network_architecture.neural_network.SegmentationNetwork._compute_steps_for_sliding_window", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.network_architecture.neural_network.SegmentationNetwork._compute_steps_for_sliding_window", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.network_architecture.neural_network.SegmentationNetwork._compute_steps_for_sliding_window", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.network_architecture.neural_network.SegmentationNetwork._compute_steps_for_sliding_window", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.network_architecture.neural_network.SegmentationNetwork._compute_steps_for_sliding_window", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.network_architecture.neural_network.SegmentationNetwork._compute_steps_for_sliding_window", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.network_architecture.neural_network.SegmentationNetwork._compute_steps_for_sliding_window", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.network_architecture.neural_network.SegmentationNetwork._compute_steps_for_sliding_window"], ["", "def", "test_some_manually_verified_combinations", "(", "self", ")", ":", "\n", "        ", "image_size", "=", "(", "128", ",", "260", ")", "\n", "patch_size", "=", "(", "64", ",", "130", ")", "\n", "step_size", "=", "0.5", "\n", "\n", "steps", "=", "SegmentationNetwork", ".", "_compute_steps_for_sliding_window", "(", "patch_size", ",", "image_size", ",", "step_size", ")", "\n", "self", ".", "assertTrue", "(", "steps", "==", "[", "[", "0", ",", "32", ",", "64", "]", ",", "[", "0", ",", "65", ",", "130", "]", "]", ")", "\n", "\n", "step_size", "=", "0.85", "\n", "steps", "=", "SegmentationNetwork", ".", "_compute_steps_for_sliding_window", "(", "patch_size", ",", "image_size", ",", "step_size", ")", "\n", "self", ".", "assertTrue", "(", "steps", "==", "[", "[", "0", ",", "32", ",", "64", "]", ",", "[", "0", ",", "65", ",", "130", "]", "]", ")", "\n", "\n", "step_size", "=", "1", "\n", "steps", "=", "SegmentationNetwork", ".", "_compute_steps_for_sliding_window", "(", "patch_size", ",", "image_size", ",", "step_size", ")", "\n", "self", ".", "assertTrue", "(", "steps", "==", "[", "[", "0", ",", "64", "]", ",", "[", "0", ",", "130", "]", "]", ")", "\n", "\n", "# an example from task02", "\n", "image_size", "=", "(", "146", ",", "176", ",", "148", ")", "\n", "patch_size", "=", "(", "128", ",", "128", ",", "128", ")", "\n", "step_size", "=", "0.5", "\n", "\n", "steps", "=", "SegmentationNetwork", ".", "_compute_steps_for_sliding_window", "(", "patch_size", ",", "image_size", ",", "step_size", ")", "\n", "self", ".", "assertTrue", "(", "steps", "==", "[", "[", "0", ",", "18", "]", ",", "[", "0", ",", "48", "]", ",", "[", "0", ",", "20", "]", "]", ")", "\n", "\n", "# heart", "\n", "image_size", "=", "(", "130", ",", "320", ",", "244", ")", "\n", "patch_size", "=", "(", "80", ",", "192", ",", "160", ")", "\n", "step_size", "=", "0.5", "\n", "\n", "steps", "=", "SegmentationNetwork", ".", "_compute_steps_for_sliding_window", "(", "patch_size", ",", "image_size", ",", "step_size", ")", "\n", "self", ".", "assertTrue", "(", "steps", "==", "[", "[", "0", ",", "25", ",", "50", "]", ",", "[", "0", ",", "64", ",", "128", "]", ",", "[", "0", ",", "42", ",", "84", "]", "]", ")", "\n", "\n", "step_size", "=", "0.75", "\n", "steps", "=", "SegmentationNetwork", ".", "_compute_steps_for_sliding_window", "(", "patch_size", ",", "image_size", ",", "step_size", ")", "\n", "self", ".", "assertTrue", "(", "steps", "==", "[", "[", "0", ",", "50", "]", ",", "[", "0", ",", "128", "]", ",", "[", "0", ",", "84", "]", "]", ")", "\n", "\n", "# liver", "\n", "image_size", "=", "(", "424", ",", "456", ",", "456", ")", "\n", "patch_size", "=", "(", "128", ",", "128", ",", "128", ")", "\n", "step_size", "=", "0.5", "\n", "\n", "steps", "=", "SegmentationNetwork", ".", "_compute_steps_for_sliding_window", "(", "patch_size", ",", "image_size", ",", "step_size", ")", "\n", "self", ".", "assertTrue", "(", "steps", "==", "[", "[", "0", ",", "59", ",", "118", ",", "178", ",", "237", ",", "296", "]", ",", "\n", "[", "0", ",", "55", ",", "109", ",", "164", ",", "219", ",", "273", ",", "328", "]", ",", "\n", "[", "0", ",", "55", ",", "109", ",", "164", ",", "219", ",", "273", ",", "328", "]", "]", "\n", ")", "\n", "\n", "# hippo", "\n", "image_size", "=", "(", "40", ",", "56", ",", "40", ")", "\n", "patch_size", "=", "(", "40", ",", "56", ",", "40", ")", "\n", "step_size", "=", "0.5", "\n", "\n", "steps", "=", "SegmentationNetwork", ".", "_compute_steps_for_sliding_window", "(", "patch_size", ",", "image_size", ",", "step_size", ")", "\n", "self", ".", "assertTrue", "(", "steps", "==", "[", "[", "0", "]", ",", "\n", "[", "0", "]", ",", "\n", "[", "0", "]", "]", "\n", ")", "\n", "\n", "# hepaticvessel", "\n", "image_size", "=", "(", "94", ",", "308", ",", "308", ")", "\n", "patch_size", "=", "(", "64", ",", "192", ",", "192", ")", "\n", "step_size", "=", "0.5", "\n", "\n", "steps", "=", "SegmentationNetwork", ".", "_compute_steps_for_sliding_window", "(", "patch_size", ",", "image_size", ",", "step_size", ")", "\n", "self", ".", "assertTrue", "(", "steps", "==", "[", "[", "0", ",", "30", "]", ",", "\n", "[", "0", ",", "58", ",", "116", "]", ",", "\n", "[", "0", ",", "58", ",", "116", "]", "]", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.tests.test_steps_for_sliding_window_prediction.TestSlidingWindow.test_loads_of_combinations": [[165, 182], ["range", "numpy.random.choice", "tuple", "tuple", "tuple", "numpy.random.uniform", "nnunet.network_architecture.neural_network.SegmentationNetwork._compute_steps_for_sliding_window", "test_steps_for_sliding_window_prediction.TestSlidingWindow._verify_steps", "numpy.random.randint", "numpy.random.randint", "max", "range", "len"], "methods", ["home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.network_architecture.neural_network.SegmentationNetwork._compute_steps_for_sliding_window", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.tests.test_steps_for_sliding_window_prediction.TestSlidingWindow._verify_steps"], ["", "def", "test_loads_of_combinations", "(", "self", ")", ":", "\n", "        ", "\"\"\"\n        We now take a large number of random combinations and perform sanity checks\n        :return:\n        \"\"\"", "\n", "for", "_", "in", "range", "(", "5000", ")", ":", "\n", "            ", "dim", "=", "np", ".", "random", ".", "choice", "(", "(", "2", ",", "3", ")", ")", "\n", "\n", "patch_size", "=", "tuple", "(", "np", ".", "random", ".", "randint", "(", "16", ",", "1024", ",", "dim", ")", ")", "\n", "image_size", "=", "tuple", "(", "np", ".", "random", ".", "randint", "(", "i", "/", "2", ",", "i", "*", "10", ")", "for", "i", "in", "patch_size", ")", "\n", "image_size", "=", "tuple", "(", "max", "(", "image_size", "[", "i", "]", ",", "patch_size", "[", "i", "]", ")", "for", "i", "in", "range", "(", "len", "(", "image_size", ")", ")", ")", "\n", "step_size", "=", "np", ".", "random", ".", "uniform", "(", "0.01", ",", "1", ")", "\n", "\n", "#print(image_size, patch_size, step_size)", "\n", "\n", "steps", "=", "SegmentationNetwork", ".", "_compute_steps_for_sliding_window", "(", "patch_size", ",", "image_size", ",", "step_size", ")", "\n", "self", ".", "_verify_steps", "(", "steps", ",", "patch_size", ",", "image_size", ",", "step_size", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.tests.test_imagenet.get_top": [[101, 107], ["y.squeeze.squeeze", "list", "y.squeeze.argsort", "zip"], "function", ["None"], ["def", "get_top", "(", "y", ",", "top", "=", "5", ")", ":", "\n", "    ", "y", "=", "y", ".", "squeeze", "(", ")", "\n", "idx", "=", "y", ".", "argsort", "(", ")", "[", ":", ":", "-", "1", "]", "\n", "top_idx", "=", "idx", "[", ":", "top", "]", "\n", "top_pred", "=", "y", "[", "top_idx", "]", "\n", "return", "list", "(", "zip", "(", "top_idx", ",", "top_pred", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.tests.test_imagenet.is_equal": [[109, 121], ["range", "len", "numpy.allclose"], "function", ["None"], ["", "def", "is_equal", "(", "gt", ",", "pr", ",", "eps", "=", "10e-5", ")", ":", "\n", "\n", "    ", "for", "i", "in", "range", "(", "len", "(", "gt", ")", ")", ":", "\n", "        ", "idx_gt", ",", "prob_gt", "=", "gt", "[", "i", "]", "\n", "idx_pr", ",", "prob_pr", "=", "pr", "[", "i", "]", "\n", "\n", "if", "idx_gt", "!=", "idx_pr", ":", "\n", "            ", "return", "False", "\n", "\n", "", "if", "not", "np", ".", "allclose", "(", "prob_gt", ",", "prob_pr", ",", "atol", "=", "eps", ")", ":", "\n", "            ", "return", "False", "\n", "", "", "return", "True", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.tests.test_imagenet.test_model": [[123, 139], ["preprocessing_func", "numpy.expand_dims", "model.predict", "print", "test_imagenet.get_top", "test_imagenet.is_equal", "keras.applications.imagenet_utils.decode_predictions", "print", "print", "print", "print", "print"], "function", ["home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.tests.test_imagenet.get_top", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.tests.test_imagenet.is_equal"], ["", "def", "test_model", "(", "model", ",", "preprocessing_func", ",", "sample", ",", "ground_truth", ")", ":", "\n", "\n", "    ", "x", "=", "preprocessing_func", "(", "sample", ")", "\n", "x", "=", "np", ".", "expand_dims", "(", "x", ",", "0", ")", "\n", "y", "=", "model", ".", "predict", "(", "x", ")", "\n", "\n", "print", "(", "'[INFO]'", ",", "decode_predictions", "(", "y", ")", ")", "\n", "\n", "pred", "=", "get_top", "(", "y", ")", "\n", "if", "is_equal", "(", "pred", ",", "ground_truth", ")", ":", "\n", "        ", "print", "(", "'[INFO] Test passed...\\n'", ")", "\n", "", "else", ":", "\n", "        ", "print", "(", "'[WARN] TEST FAILED...'", ")", "\n", "print", "(", "'[WARN] PREDICTION'", ",", "pred", ")", "\n", "print", "(", "'[WARN] GROUND TRUTH'", ",", "ground_truth", ")", "\n", "print", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.tests.test_imagenet.main": [[141, 158], ["skimage.io.imread", "print", "model.", "test_imagenet.test_model"], "function", ["home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.tests.test_imagenet.test_model"], ["", "", "def", "main", "(", ")", ":", "\n", "\n", "    ", "path", "=", "(", "'../imgs/tests/seagull.jpg'", ")", "\n", "img", "=", "imread", "(", "path", ")", "\n", "for", "model_type", "in", "models_zoo", ":", "\n", "        ", "for", "params", "in", "models_zoo", "[", "model_type", "]", "[", "'params'", "]", ":", "\n", "\n", "            ", "input_shape", "=", "params", "[", "'input_shape'", "]", "\n", "dataset", "=", "params", "[", "'dataset'", "]", "\n", "preprocessing_function", "=", "params", "[", "'preprocessing_function'", "]", "\n", "groud_truth", "=", "params", "[", "'ground_truth'", "]", "\n", "\n", "print", "(", "'[INFO] Loading model {} with weights {}....'", ".", "format", "(", "model_type", ",", "dataset", ")", ")", "\n", "model", "=", "models_zoo", "[", "model_type", "]", "[", "'model'", "]", "\n", "model", "=", "model", "(", "input_shape", ",", "weights", "=", "dataset", ",", "classes", "=", "1000", ")", "\n", "\n", "test_model", "(", "model", ",", "preprocessing_function", ",", "img", ",", "groud_truth", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.experiment_planning.experiment_planner_baseline_3DUNet.ExperimentPlanner.__init__": [[33, 65], ["subfiles", "isfile", "load_pickle", "collections.OrderedDict", "collections.OrderedDict", "join", "join", "join"], "methods", ["None"], ["    ", "def", "__init__", "(", "self", ",", "folder_with_cropped_data", ",", "preprocessed_output_folder", ")", ":", "\n", "        ", "self", ".", "folder_with_cropped_data", "=", "folder_with_cropped_data", "\n", "self", ".", "preprocessed_output_folder", "=", "preprocessed_output_folder", "\n", "self", ".", "list_of_cropped_npz_files", "=", "subfiles", "(", "self", ".", "folder_with_cropped_data", ",", "True", ",", "None", ",", "\".npz\"", ",", "True", ")", "\n", "\n", "self", ".", "preprocessor_name", "=", "\"GenericPreprocessor\"", "\n", "\n", "assert", "isfile", "(", "join", "(", "self", ".", "folder_with_cropped_data", ",", "\"dataset_properties.pkl\"", ")", ")", ",", "\"folder_with_cropped_data must contain dataset_properties.pkl\"", "\n", "self", ".", "dataset_properties", "=", "load_pickle", "(", "join", "(", "self", ".", "folder_with_cropped_data", ",", "\"dataset_properties.pkl\"", ")", ")", "\n", "\n", "self", ".", "plans_per_stage", "=", "OrderedDict", "(", ")", "\n", "self", ".", "plans", "=", "OrderedDict", "(", ")", "\n", "self", ".", "plans_fname", "=", "join", "(", "self", ".", "preprocessed_output_folder", ",", "\"nnUNetPlans\"", "+", "\"fixed_plans_3D.pkl\"", ")", "\n", "self", ".", "data_identifier", "=", "default_data_identifier", "\n", "\n", "self", ".", "transpose_forward", "=", "[", "0", ",", "1", ",", "2", "]", "\n", "self", ".", "transpose_backward", "=", "[", "0", ",", "1", ",", "2", "]", "\n", "\n", "self", ".", "unet_base_num_features", "=", "Generic_UNet", ".", "BASE_NUM_FEATURES_3D", "\n", "self", ".", "unet_max_num_filters", "=", "320", "\n", "self", ".", "unet_max_numpool", "=", "999", "\n", "self", ".", "unet_min_batch_size", "=", "2", "\n", "self", ".", "unet_featuremap_min_edge_length", "=", "4", "\n", "\n", "self", ".", "target_spacing_percentile", "=", "50", "\n", "self", ".", "anisotropy_threshold", "=", "3", "\n", "self", ".", "how_much_of_a_patient_must_the_network_see_at_stage0", "=", "4", "# 1/4 of a patient", "\n", "self", ".", "batch_size_covers_max_percent_of_dataset", "=", "0.05", "# all samples in the batch together cannot cover more", "\n", "# than 5% of the entire dataset", "\n", "\n", "self", ".", "conv_per_stage", "=", "2", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.experiment_planning.experiment_planner_baseline_3DUNet.ExperimentPlanner.get_target_spacing": [[66, 80], ["numpy.percentile", "numpy.vstack"], "methods", ["None"], ["", "def", "get_target_spacing", "(", "self", ")", ":", "\n", "        ", "spacings", "=", "self", ".", "dataset_properties", "[", "'all_spacings'", "]", "\n", "\n", "# target = np.median(np.vstack(spacings), 0)", "\n", "# if target spacing is very anisotropic we may want to not downsample the axis with the worst spacing", "\n", "# uncomment after mystery task submission", "\n", "\"\"\"worst_spacing_axis = np.argmax(target)\n        if max(target) > (2.5 * min(target)):\n            spacings_of_that_axis = np.vstack(spacings)[:, worst_spacing_axis]\n            target_spacing_of_that_axis = np.percentile(spacings_of_that_axis, 5)\n            target[worst_spacing_axis] = target_spacing_of_that_axis\"\"\"", "\n", "\n", "target", "=", "np", ".", "percentile", "(", "np", ".", "vstack", "(", "spacings", ")", ",", "self", ".", "target_spacing_percentile", ",", "0", ")", "\n", "return", "target", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.experiment_planning.experiment_planner_baseline_3DUNet.ExperimentPlanner.save_my_plans": [[81, 84], ["open", "pickle.dump"], "methods", ["None"], ["", "def", "save_my_plans", "(", "self", ")", ":", "\n", "        ", "with", "open", "(", "self", ".", "plans_fname", ",", "'wb'", ")", "as", "f", ":", "\n", "            ", "pickle", ".", "dump", "(", "self", ".", "plans", ",", "f", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.experiment_planning.experiment_planner_baseline_3DUNet.ExperimentPlanner.load_my_plans": [[85, 93], ["load_pickle"], "methods", ["None"], ["", "", "def", "load_my_plans", "(", "self", ")", ":", "\n", "        ", "self", ".", "plans", "=", "load_pickle", "(", "self", ".", "plans_fname", ")", "\n", "\n", "self", ".", "plans_per_stage", "=", "self", ".", "plans", "[", "'plans_per_stage'", "]", "\n", "self", ".", "dataset_properties", "=", "self", ".", "plans", "[", "'dataset_properties'", "]", "\n", "\n", "self", ".", "transpose_forward", "=", "self", ".", "plans", "[", "'transpose_forward'", "]", "\n", "self", ".", "transpose_backward", "=", "self", ".", "plans", "[", "'transpose_backward'", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.experiment_planning.experiment_planner_baseline_3DUNet.ExperimentPlanner.determine_postprocessing": [[94, 143], ["None"], "methods", ["None"], ["", "def", "determine_postprocessing", "(", "self", ")", ":", "\n", "        ", "pass", "\n", "\"\"\"\n        Spoiler: This is unused, postprocessing was removed. Ignore it.\n        :return:\n        print(\"determining postprocessing...\")\n\n        props_per_patient = self.dataset_properties['segmentation_props_per_patient']\n\n        all_region_keys = [i for k in props_per_patient.keys() for i in props_per_patient[k]['only_one_region'].keys()]\n        all_region_keys = list(set(all_region_keys))\n\n        only_keep_largest_connected_component = OrderedDict()\n\n        for r in all_region_keys:\n            all_results = [props_per_patient[k]['only_one_region'][r] for k in props_per_patient.keys()]\n            only_keep_largest_connected_component[tuple(r)] = all(all_results)\n\n        print(\"Postprocessing: only_keep_largest_connected_component\", only_keep_largest_connected_component)\n\n        all_classes = self.dataset_properties['all_classes']\n        classes = [i for i in all_classes if i > 0]\n\n        props_per_patient = self.dataset_properties['segmentation_props_per_patient']\n\n        min_size_per_class = OrderedDict()\n        for c in classes:\n            all_num_voxels = []\n            for k in props_per_patient.keys():\n                all_num_voxels.append(props_per_patient[k]['volume_per_class'][c])\n            if len(all_num_voxels) > 0:\n                min_size_per_class[c] = np.percentile(all_num_voxels, 1) * MIN_SIZE_PER_CLASS_FACTOR\n            else:\n                min_size_per_class[c] = np.inf\n\n        min_region_size_per_class = OrderedDict()\n        for c in classes:\n            region_sizes = [l for k in props_per_patient for l in props_per_patient[k]['region_volume_per_class'][c]]\n            if len(region_sizes) > 0:\n                min_region_size_per_class[c] = min(region_sizes)\n                # we don't need that line but better safe than sorry, right?\n                min_region_size_per_class[c] = min(min_region_size_per_class[c], min_size_per_class[c])\n            else:\n                min_region_size_per_class[c] = 0\n\n        print(\"Postprocessing: min_size_per_class\", min_size_per_class)\n        print(\"Postprocessing: min_region_size_per_class\", min_region_size_per_class)\n        return only_keep_largest_connected_component, min_size_per_class, min_region_size_per_class\n        \"\"\"", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.experiment_planning.experiment_planner_baseline_3DUNet.ExperimentPlanner.get_properties_for_stage": [[144, 246], ["numpy.round().astype", "numpy.round().astype.mean", "numpy.round().astype", "nnunet.experiment_planning.common_utils.get_pool_and_conv_props_poolLateV2", "nnunet.network_architecture.generic_UNet.Generic_UNet.compute_approx_vram_consumption", "int", "numpy.round().astype", "max", "min", "numpy.prod", "numpy.array", "min", "copy.deepcopy", "nnunet.experiment_planning.common_utils.get_pool_and_conv_props_poolLateV2", "nnunet.experiment_planning.common_utils.get_pool_and_conv_props_poolLateV2", "nnunet.network_architecture.generic_UNet.Generic_UNet.compute_approx_vram_consumption", "numpy.floor", "numpy.round", "min", "numpy.round", "zip", "numpy.argsort", "numpy.round", "max", "max", "numpy.prod"], "methods", ["home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.experiment_planning.common_utils.get_pool_and_conv_props_poolLateV2", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.network_architecture.generic_modular_UNet.PlainConvUNet.compute_approx_vram_consumption", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.experiment_planning.common_utils.get_pool_and_conv_props_poolLateV2", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.experiment_planning.common_utils.get_pool_and_conv_props_poolLateV2", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.network_architecture.generic_modular_UNet.PlainConvUNet.compute_approx_vram_consumption"], ["", "def", "get_properties_for_stage", "(", "self", ",", "current_spacing", ",", "original_spacing", ",", "original_shape", ",", "num_cases", ",", "\n", "num_modalities", ",", "num_classes", ")", ":", "\n", "        ", "\"\"\"\n        Computation of input patch size starts out with the new median shape (in voxels) of a dataset. This is\n        opposed to prior experiments where I based it on the median size in mm. The rationale behind this is that\n        for some organ of interest the acquisition method will most likely be chosen such that the field of view and\n        voxel resolution go hand in hand to show the doctor what they need to see. This assumption may be violated\n        for some modalities with anisotropy (cine MRI) but we will have t live with that. In future experiments I\n        will try to 1) base input patch size match aspect ratio of input size in mm (instead of voxels) and 2) to\n        try to enforce that we see the same 'distance' in all directions (try to maintain equal size in mm of patch)\n\n        The patches created here attempt keep the aspect ratio of the new_median_shape\n\n        :param current_spacing:\n        :param original_spacing:\n        :param original_shape:\n        :param num_cases:\n        :return:\n        \"\"\"", "\n", "new_median_shape", "=", "np", ".", "round", "(", "original_spacing", "/", "current_spacing", "*", "original_shape", ")", ".", "astype", "(", "int", ")", "\n", "dataset_num_voxels", "=", "np", ".", "prod", "(", "new_median_shape", ")", "*", "num_cases", "\n", "\n", "# the next line is what we had before as a default. The patch size had the same aspect ratio as the median shape of a patient. We swapped t", "\n", "# input_patch_size = new_median_shape", "\n", "\n", "# compute how many voxels are one mm", "\n", "input_patch_size", "=", "1", "/", "np", ".", "array", "(", "current_spacing", ")", "\n", "\n", "# normalize voxels per mm", "\n", "input_patch_size", "/=", "input_patch_size", ".", "mean", "(", ")", "\n", "\n", "# create an isotropic patch of size 512x512x512mm", "\n", "input_patch_size", "*=", "1", "/", "min", "(", "input_patch_size", ")", "*", "512", "# to get a starting value", "\n", "input_patch_size", "=", "np", ".", "round", "(", "input_patch_size", ")", ".", "astype", "(", "int", ")", "\n", "\n", "# clip it to the median shape of the dataset because patches larger then that make not much sense", "\n", "input_patch_size", "=", "[", "min", "(", "i", ",", "j", ")", "for", "i", ",", "j", "in", "zip", "(", "input_patch_size", ",", "new_median_shape", ")", "]", "\n", "\n", "network_num_pool_per_axis", ",", "pool_op_kernel_sizes", ",", "conv_kernel_sizes", ",", "new_shp", ",", "shape_must_be_divisible_by", "=", "get_pool_and_conv_props_poolLateV2", "(", "input_patch_size", ",", "\n", "self", ".", "unet_featuremap_min_edge_length", ",", "\n", "self", ".", "unet_max_numpool", ",", "\n", "current_spacing", ")", "\n", "\n", "ref", "=", "Generic_UNet", ".", "use_this_for_batch_size_computation_3D", "\n", "here", "=", "Generic_UNet", ".", "compute_approx_vram_consumption", "(", "new_shp", ",", "network_num_pool_per_axis", ",", "\n", "self", ".", "unet_base_num_features", ",", "\n", "self", ".", "unet_max_num_filters", ",", "num_modalities", ",", "\n", "num_classes", ",", "\n", "pool_op_kernel_sizes", ",", "conv_per_stage", "=", "self", ".", "conv_per_stage", ")", "\n", "while", "here", ">", "ref", ":", "\n", "            ", "axis_to_be_reduced", "=", "np", ".", "argsort", "(", "new_shp", "/", "new_median_shape", ")", "[", "-", "1", "]", "\n", "\n", "tmp", "=", "deepcopy", "(", "new_shp", ")", "\n", "tmp", "[", "axis_to_be_reduced", "]", "-=", "shape_must_be_divisible_by", "[", "axis_to_be_reduced", "]", "\n", "_", ",", "_", ",", "_", ",", "_", ",", "shape_must_be_divisible_by_new", "=", "get_pool_and_conv_props_poolLateV2", "(", "tmp", ",", "\n", "self", ".", "unet_featuremap_min_edge_length", ",", "\n", "self", ".", "unet_max_numpool", ",", "\n", "current_spacing", ")", "\n", "new_shp", "[", "axis_to_be_reduced", "]", "-=", "shape_must_be_divisible_by_new", "[", "axis_to_be_reduced", "]", "\n", "\n", "# we have to recompute numpool now:", "\n", "network_num_pool_per_axis", ",", "pool_op_kernel_sizes", ",", "conv_kernel_sizes", ",", "new_shp", ",", "shape_must_be_divisible_by", "=", "get_pool_and_conv_props_poolLateV2", "(", "new_shp", ",", "\n", "self", ".", "unet_featuremap_min_edge_length", ",", "\n", "self", ".", "unet_max_numpool", ",", "\n", "current_spacing", ")", "\n", "\n", "here", "=", "Generic_UNet", ".", "compute_approx_vram_consumption", "(", "new_shp", ",", "network_num_pool_per_axis", ",", "\n", "self", ".", "unet_base_num_features", ",", "\n", "self", ".", "unet_max_num_filters", ",", "num_modalities", ",", "\n", "num_classes", ",", "pool_op_kernel_sizes", ",", "\n", "conv_per_stage", "=", "self", ".", "conv_per_stage", ")", "\n", "# print(new_shp)", "\n", "\n", "", "input_patch_size", "=", "new_shp", "\n", "\n", "batch_size", "=", "Generic_UNet", ".", "DEFAULT_BATCH_SIZE_3D", "# This is what works with 128**3", "\n", "batch_size", "=", "int", "(", "np", ".", "floor", "(", "max", "(", "ref", "/", "here", ",", "1", ")", "*", "batch_size", ")", ")", "\n", "\n", "# check if batch size is too large", "\n", "max_batch_size", "=", "np", ".", "round", "(", "self", ".", "batch_size_covers_max_percent_of_dataset", "*", "dataset_num_voxels", "/", "\n", "np", ".", "prod", "(", "input_patch_size", ",", "dtype", "=", "np", ".", "int64", ")", ")", ".", "astype", "(", "int", ")", "\n", "max_batch_size", "=", "max", "(", "max_batch_size", ",", "self", ".", "unet_min_batch_size", ")", "\n", "batch_size", "=", "min", "(", "batch_size", ",", "max_batch_size", ")", "\n", "\n", "do_dummy_2D_data_aug", "=", "(", "max", "(", "input_patch_size", ")", "/", "input_patch_size", "[", "\n", "0", "]", ")", ">", "self", ".", "anisotropy_threshold", "\n", "\n", "plan", "=", "{", "\n", "'batch_size'", ":", "batch_size", ",", "\n", "'num_pool_per_axis'", ":", "network_num_pool_per_axis", ",", "\n", "'patch_size'", ":", "input_patch_size", ",", "\n", "'median_patient_size_in_voxels'", ":", "new_median_shape", ",", "\n", "'current_spacing'", ":", "current_spacing", ",", "\n", "'original_spacing'", ":", "original_spacing", ",", "\n", "'do_dummy_2D_data_aug'", ":", "do_dummy_2D_data_aug", ",", "\n", "'pool_op_kernel_sizes'", ":", "pool_op_kernel_sizes", ",", "\n", "'conv_kernel_sizes'", ":", "conv_kernel_sizes", ",", "\n", "}", "\n", "return", "plan", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.experiment_planning.experiment_planner_baseline_3DUNet.ExperimentPlanner.plan_experiment": [[247, 358], ["experiment_planner_baseline_3DUNet.ExperimentPlanner.determine_whether_to_use_mask_for_norm", "print", "len", "experiment_planner_baseline_3DUNet.ExperimentPlanner.get_target_spacing", "numpy.argmax", "numpy.median", "print", "numpy.max", "print", "numpy.min", "print", "print", "list", "print", "print", "experiment_planner_baseline_3DUNet.ExperimentPlanner.plans_per_stage.append", "numpy.prod", "print", "print", "print", "experiment_planner_baseline_3DUNet.ExperimentPlanner.determine_normalization_scheme", "experiment_planner_baseline_3DUNet.ExperimentPlanner.save_my_plans", "list", "numpy.vstack", "numpy.vstack", "numpy.vstack", "numpy.array", "numpy.array", "experiment_planner_baseline_3DUNet.ExperimentPlanner.get_properties_for_stage", "print", "copy.deepcopy", "numpy.prod", "len", "len", "modalities.keys", "numpy.array", "zip", "list", "range", "len", "numpy.prod", "max", "numpy.any", "numpy.prod", "experiment_planner_baseline_3DUNet.ExperimentPlanner.get_properties_for_stage", "numpy.prod", "numpy.prod", "experiment_planner_baseline_3DUNet.ExperimentPlanner.plans_per_stage.append", "range", "list", "numpy.array", "range", "numpy.argwhere", "len", "numpy.array", "len", "numpy.prod", "len", "experiment_planner_baseline_3DUNet.ExperimentPlanner.plans_per_stage.keys", "len", "numpy.array"], "methods", ["home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.experiment_planning.experiment_planner_baseline_3DUNet.ExperimentPlanner.determine_whether_to_use_mask_for_norm", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.target_spacing.experiment_planner_baseline_3DUNet_targetSpacingForAnisoAxis.ExperimentPlannerTargetSpacingForAnisoAxis.get_target_spacing", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.normalization.experiment_planner_3DUNet_nonCT.ExperimentPlannernonCT.determine_normalization_scheme", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.experiment_planning.experiment_planner_baseline_3DUNet.ExperimentPlanner.save_my_plans", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.patch_size.experiment_planner_3DUNet_isotropic_in_voxels.ExperimentPlanner3D_IsoPatchesInVoxels.get_properties_for_stage", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.patch_size.experiment_planner_3DUNet_isotropic_in_voxels.ExperimentPlanner3D_IsoPatchesInVoxels.get_properties_for_stage"], ["", "def", "plan_experiment", "(", "self", ")", ":", "\n", "        ", "use_nonzero_mask_for_normalization", "=", "self", ".", "determine_whether_to_use_mask_for_norm", "(", ")", "\n", "print", "(", "\"Are we using the nonzero mask for normalizaion?\"", ",", "use_nonzero_mask_for_normalization", ")", "\n", "spacings", "=", "self", ".", "dataset_properties", "[", "'all_spacings'", "]", "\n", "sizes", "=", "self", ".", "dataset_properties", "[", "'all_sizes'", "]", "\n", "\n", "all_classes", "=", "self", ".", "dataset_properties", "[", "'all_classes'", "]", "\n", "modalities", "=", "self", ".", "dataset_properties", "[", "'modalities'", "]", "\n", "num_modalities", "=", "len", "(", "list", "(", "modalities", ".", "keys", "(", ")", ")", ")", "\n", "\n", "target_spacing", "=", "self", ".", "get_target_spacing", "(", ")", "\n", "new_shapes", "=", "[", "np", ".", "array", "(", "i", ")", "/", "target_spacing", "*", "np", ".", "array", "(", "j", ")", "for", "i", ",", "j", "in", "zip", "(", "spacings", ",", "sizes", ")", "]", "\n", "\n", "max_spacing_axis", "=", "np", ".", "argmax", "(", "target_spacing", ")", "\n", "remaining_axes", "=", "[", "i", "for", "i", "in", "list", "(", "range", "(", "3", ")", ")", "if", "i", "!=", "max_spacing_axis", "]", "\n", "self", ".", "transpose_forward", "=", "[", "max_spacing_axis", "]", "+", "remaining_axes", "\n", "self", ".", "transpose_backward", "=", "[", "np", ".", "argwhere", "(", "np", ".", "array", "(", "self", ".", "transpose_forward", ")", "==", "i", ")", "[", "0", "]", "[", "0", "]", "for", "i", "in", "range", "(", "3", ")", "]", "\n", "\n", "# we base our calculations on the median shape of the datasets", "\n", "median_shape", "=", "np", ".", "median", "(", "np", ".", "vstack", "(", "new_shapes", ")", ",", "0", ")", "\n", "print", "(", "\"the median shape of the dataset is \"", ",", "median_shape", ")", "\n", "\n", "max_shape", "=", "np", ".", "max", "(", "np", ".", "vstack", "(", "new_shapes", ")", ",", "0", ")", "\n", "print", "(", "\"the max shape in the dataset is \"", ",", "max_shape", ")", "\n", "min_shape", "=", "np", ".", "min", "(", "np", ".", "vstack", "(", "new_shapes", ")", ",", "0", ")", "\n", "print", "(", "\"the min shape in the dataset is \"", ",", "min_shape", ")", "\n", "\n", "print", "(", "\"we don't want feature maps smaller than \"", ",", "self", ".", "unet_featuremap_min_edge_length", ",", "\" in the bottleneck\"", ")", "\n", "\n", "# how many stages will the image pyramid have?", "\n", "self", ".", "plans_per_stage", "=", "list", "(", ")", "\n", "\n", "target_spacing_transposed", "=", "np", ".", "array", "(", "target_spacing", ")", "[", "self", ".", "transpose_forward", "]", "\n", "median_shape_transposed", "=", "np", ".", "array", "(", "median_shape", ")", "[", "self", ".", "transpose_forward", "]", "\n", "print", "(", "\"the transposed median shape of the dataset is \"", ",", "median_shape_transposed", ")", "\n", "\n", "print", "(", "\"generating configuration for 3d_fullres\"", ")", "\n", "self", ".", "plans_per_stage", ".", "append", "(", "self", ".", "get_properties_for_stage", "(", "target_spacing_transposed", ",", "target_spacing_transposed", ",", "\n", "median_shape_transposed", ",", "\n", "len", "(", "self", ".", "list_of_cropped_npz_files", ")", ",", "\n", "num_modalities", ",", "len", "(", "all_classes", ")", "+", "1", ")", ")", "\n", "\n", "# thanks Zakiyi (https://github.com/MIC-DKFZ/nnUNet/issues/61) for spotting this bug :-)", "\n", "# if np.prod(self.plans_per_stage[-1]['median_patient_size_in_voxels'], dtype=np.int64) / \\", "\n", "#        architecture_input_voxels < HOW_MUCH_OF_A_PATIENT_MUST_THE_NETWORK_SEE_AT_STAGE0:", "\n", "architecture_input_voxels_here", "=", "np", ".", "prod", "(", "self", ".", "plans_per_stage", "[", "-", "1", "]", "[", "'patch_size'", "]", ",", "dtype", "=", "np", ".", "int64", ")", "\n", "if", "np", ".", "prod", "(", "self", ".", "plans_per_stage", "[", "-", "1", "]", "[", "'median_patient_size_in_voxels'", "]", ",", "dtype", "=", "np", ".", "int64", ")", "/", "architecture_input_voxels_here", "<", "self", ".", "how_much_of_a_patient_must_the_network_see_at_stage0", ":", "\n", "            ", "more", "=", "False", "\n", "", "else", ":", "\n", "            ", "more", "=", "True", "\n", "\n", "", "if", "more", ":", "\n", "            ", "print", "(", "\"generating configuration for 3d_lowres\"", ")", "\n", "# if we are doing more than one stage then we want the lowest stage to have exactly", "\n", "# HOW_MUCH_OF_A_PATIENT_MUST_THE_NETWORK_SEE_AT_STAGE0 (this is 4 by default so the number of voxels in the", "\n", "# median shape of the lowest stage must be 4 times as much as the network can process at once (128x128x128 by", "\n", "# default). Problem is that we are downsampling higher resolution axes before we start downsampling the", "\n", "# out-of-plane axis. We could probably/maybe do this analytically but I am lazy, so here", "\n", "# we do it the dumb way", "\n", "\n", "lowres_stage_spacing", "=", "deepcopy", "(", "target_spacing", ")", "\n", "num_voxels", "=", "np", ".", "prod", "(", "median_shape", ",", "dtype", "=", "np", ".", "int64", ")", "\n", "while", "num_voxels", ">", "self", ".", "how_much_of_a_patient_must_the_network_see_at_stage0", "*", "architecture_input_voxels_here", ":", "\n", "                ", "max_spacing", "=", "max", "(", "lowres_stage_spacing", ")", "\n", "if", "np", ".", "any", "(", "(", "max_spacing", "/", "lowres_stage_spacing", ")", ">", "2", ")", ":", "\n", "                    ", "lowres_stage_spacing", "[", "(", "max_spacing", "/", "lowres_stage_spacing", ")", ">", "2", "]", "*=", "1.01", "\n", "", "else", ":", "\n", "                    ", "lowres_stage_spacing", "*=", "1.01", "\n", "", "num_voxels", "=", "np", ".", "prod", "(", "target_spacing", "/", "lowres_stage_spacing", "*", "median_shape", ",", "dtype", "=", "np", ".", "int64", ")", "\n", "\n", "lowres_stage_spacing_transposed", "=", "np", ".", "array", "(", "lowres_stage_spacing", ")", "[", "self", ".", "transpose_forward", "]", "\n", "new", "=", "self", ".", "get_properties_for_stage", "(", "lowres_stage_spacing_transposed", ",", "target_spacing_transposed", ",", "\n", "median_shape_transposed", ",", "\n", "len", "(", "self", ".", "list_of_cropped_npz_files", ")", ",", "\n", "num_modalities", ",", "len", "(", "all_classes", ")", "+", "1", ")", "\n", "architecture_input_voxels_here", "=", "np", ".", "prod", "(", "new", "[", "'patch_size'", "]", ",", "dtype", "=", "np", ".", "int64", ")", "\n", "", "if", "2", "*", "np", ".", "prod", "(", "new", "[", "'median_patient_size_in_voxels'", "]", ",", "dtype", "=", "np", ".", "int64", ")", "<", "np", ".", "prod", "(", "\n", "self", ".", "plans_per_stage", "[", "0", "]", "[", "'median_patient_size_in_voxels'", "]", ",", "dtype", "=", "np", ".", "int64", ")", ":", "\n", "                ", "self", ".", "plans_per_stage", ".", "append", "(", "new", ")", "\n", "\n", "", "", "self", ".", "plans_per_stage", "=", "self", ".", "plans_per_stage", "[", ":", ":", "-", "1", "]", "\n", "self", ".", "plans_per_stage", "=", "{", "i", ":", "self", ".", "plans_per_stage", "[", "i", "]", "for", "i", "in", "range", "(", "len", "(", "self", ".", "plans_per_stage", ")", ")", "}", "# convert to dict", "\n", "\n", "print", "(", "self", ".", "plans_per_stage", ")", "\n", "print", "(", "\"transpose forward\"", ",", "self", ".", "transpose_forward", ")", "\n", "print", "(", "\"transpose backward\"", ",", "self", ".", "transpose_backward", ")", "\n", "\n", "normalization_schemes", "=", "self", ".", "determine_normalization_scheme", "(", ")", "\n", "only_keep_largest_connected_component", ",", "min_size_per_class", ",", "min_region_size_per_class", "=", "None", ",", "None", ",", "None", "\n", "# removed training data based postprocessing. This is deprecated", "\n", "\n", "# these are independent of the stage", "\n", "plans", "=", "{", "'num_stages'", ":", "len", "(", "list", "(", "self", ".", "plans_per_stage", ".", "keys", "(", ")", ")", ")", ",", "'num_modalities'", ":", "num_modalities", ",", "\n", "'modalities'", ":", "modalities", ",", "'normalization_schemes'", ":", "normalization_schemes", ",", "\n", "'dataset_properties'", ":", "self", ".", "dataset_properties", ",", "'list_of_npz_files'", ":", "self", ".", "list_of_cropped_npz_files", ",", "\n", "'original_spacings'", ":", "spacings", ",", "'original_sizes'", ":", "sizes", ",", "\n", "'preprocessed_data_folder'", ":", "self", ".", "preprocessed_output_folder", ",", "'num_classes'", ":", "len", "(", "all_classes", ")", ",", "\n", "'all_classes'", ":", "all_classes", ",", "'base_num_features'", ":", "self", ".", "unet_base_num_features", ",", "\n", "'use_mask_for_norm'", ":", "use_nonzero_mask_for_normalization", ",", "\n", "'keep_only_largest_region'", ":", "only_keep_largest_connected_component", ",", "\n", "'min_region_size_per_class'", ":", "min_region_size_per_class", ",", "'min_size_per_class'", ":", "min_size_per_class", ",", "\n", "'transpose_forward'", ":", "self", ".", "transpose_forward", ",", "'transpose_backward'", ":", "self", ".", "transpose_backward", ",", "\n", "'data_identifier'", ":", "self", ".", "data_identifier", ",", "'plans_per_stage'", ":", "self", ".", "plans_per_stage", ",", "\n", "'preprocessor_name'", ":", "self", ".", "preprocessor_name", ",", "\n", "'conv_per_stage'", ":", "self", ".", "conv_per_stage", ",", "\n", "}", "\n", "\n", "self", ".", "plans", "=", "plans", "\n", "self", ".", "save_my_plans", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.experiment_planning.experiment_planner_baseline_3DUNet.ExperimentPlanner.determine_normalization_scheme": [[359, 370], ["collections.OrderedDict", "len", "range", "list", "modalities.keys"], "methods", ["None"], ["", "def", "determine_normalization_scheme", "(", "self", ")", ":", "\n", "        ", "schemes", "=", "OrderedDict", "(", ")", "\n", "modalities", "=", "self", ".", "dataset_properties", "[", "'modalities'", "]", "\n", "num_modalities", "=", "len", "(", "list", "(", "modalities", ".", "keys", "(", ")", ")", ")", "\n", "\n", "for", "i", "in", "range", "(", "num_modalities", ")", ":", "\n", "            ", "if", "modalities", "[", "i", "]", "==", "\"CT\"", "or", "modalities", "[", "i", "]", "==", "'ct'", ":", "\n", "                ", "schemes", "[", "i", "]", "=", "\"CT\"", "\n", "", "else", ":", "\n", "                ", "schemes", "[", "i", "]", "=", "\"nonCT\"", "\n", "", "", "return", "schemes", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.experiment_planning.experiment_planner_baseline_3DUNet.ExperimentPlanner.save_properties_of_cropped": [[371, 374], ["open", "pickle.dump", "join"], "methods", ["None"], ["", "def", "save_properties_of_cropped", "(", "self", ",", "case_identifier", ",", "properties", ")", ":", "\n", "        ", "with", "open", "(", "join", "(", "self", ".", "folder_with_cropped_data", ",", "\"%s.pkl\"", "%", "case_identifier", ")", ",", "'wb'", ")", "as", "f", ":", "\n", "            ", "pickle", ".", "dump", "(", "properties", ",", "f", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.experiment_planning.experiment_planner_baseline_3DUNet.ExperimentPlanner.load_properties_of_cropped": [[375, 379], ["open", "pickle.load", "join"], "methods", ["None"], ["", "", "def", "load_properties_of_cropped", "(", "self", ",", "case_identifier", ")", ":", "\n", "        ", "with", "open", "(", "join", "(", "self", ".", "folder_with_cropped_data", ",", "\"%s.pkl\"", "%", "case_identifier", ")", ",", "'rb'", ")", "as", "f", ":", "\n", "            ", "properties", "=", "pickle", ".", "load", "(", "f", ")", "\n", "", "return", "properties", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.experiment_planning.experiment_planner_baseline_3DUNet.ExperimentPlanner.determine_whether_to_use_mask_for_norm": [[380, 410], ["len", "collections.OrderedDict", "range", "list", "nnunet.preprocessing.cropping.get_case_identifier_from_npz", "experiment_planner_baseline_3DUNet.ExperimentPlanner.load_properties_of_cropped", "experiment_planner_baseline_3DUNet.ExperimentPlanner.save_properties_of_cropped", "modalities.keys", "experiment_planner_baseline_3DUNet.ExperimentPlanner.dataset_properties[].keys", "all_size_reductions.append", "numpy.median", "print", "print"], "methods", ["home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.preprocessing.cropping.get_case_identifier_from_npz", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.experiment_planning.DatasetAnalyzer.DatasetAnalyzer.load_properties_of_cropped", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.experiment_planning.experiment_planner_baseline_3DUNet.ExperimentPlanner.save_properties_of_cropped"], ["", "def", "determine_whether_to_use_mask_for_norm", "(", "self", ")", ":", "\n", "# only use the nonzero mask for normalization of the cropping based on it resulted in a decrease in", "\n", "# image size (this is an indication that the data is something like brats/isles and then we want to", "\n", "# normalize in the brain region only)", "\n", "        ", "modalities", "=", "self", ".", "dataset_properties", "[", "'modalities'", "]", "\n", "num_modalities", "=", "len", "(", "list", "(", "modalities", ".", "keys", "(", ")", ")", ")", "\n", "use_nonzero_mask_for_norm", "=", "OrderedDict", "(", ")", "\n", "\n", "for", "i", "in", "range", "(", "num_modalities", ")", ":", "\n", "            ", "if", "\"CT\"", "in", "modalities", "[", "i", "]", ":", "\n", "                ", "use_nonzero_mask_for_norm", "[", "i", "]", "=", "False", "\n", "", "else", ":", "\n", "                ", "all_size_reductions", "=", "[", "]", "\n", "for", "k", "in", "self", ".", "dataset_properties", "[", "'size_reductions'", "]", ".", "keys", "(", ")", ":", "\n", "                    ", "all_size_reductions", ".", "append", "(", "self", ".", "dataset_properties", "[", "'size_reductions'", "]", "[", "k", "]", ")", "\n", "\n", "", "if", "np", ".", "median", "(", "all_size_reductions", ")", "<", "3", "/", "4.", ":", "\n", "                    ", "print", "(", "\"using nonzero mask for normalization\"", ")", "\n", "use_nonzero_mask_for_norm", "[", "i", "]", "=", "True", "\n", "", "else", ":", "\n", "                    ", "print", "(", "\"not using nonzero mask for normalization\"", ")", "\n", "use_nonzero_mask_for_norm", "[", "i", "]", "=", "False", "\n", "\n", "", "", "", "for", "c", "in", "self", ".", "list_of_cropped_npz_files", ":", "\n", "            ", "case_identifier", "=", "get_case_identifier_from_npz", "(", "c", ")", "\n", "properties", "=", "self", ".", "load_properties_of_cropped", "(", "case_identifier", ")", "\n", "properties", "[", "'use_nonzero_mask_for_norm'", "]", "=", "use_nonzero_mask_for_norm", "\n", "self", ".", "save_properties_of_cropped", "(", "case_identifier", ",", "properties", ")", "\n", "", "use_nonzero_mask_for_normalization", "=", "use_nonzero_mask_for_norm", "\n", "return", "use_nonzero_mask_for_normalization", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.experiment_planning.experiment_planner_baseline_3DUNet.ExperimentPlanner.write_normalization_scheme_to_patients": [[411, 421], ["nnunet.preprocessing.cropping.get_case_identifier_from_npz", "experiment_planner_baseline_3DUNet.ExperimentPlanner.load_properties_of_cropped", "experiment_planner_baseline_3DUNet.ExperimentPlanner.save_properties_of_cropped"], "methods", ["home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.preprocessing.cropping.get_case_identifier_from_npz", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.experiment_planning.DatasetAnalyzer.DatasetAnalyzer.load_properties_of_cropped", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.experiment_planning.experiment_planner_baseline_3DUNet.ExperimentPlanner.save_properties_of_cropped"], ["", "def", "write_normalization_scheme_to_patients", "(", "self", ")", ":", "\n", "        ", "\"\"\"\n        This is used for test set preprocessing\n        :return: \n        \"\"\"", "\n", "for", "c", "in", "self", ".", "list_of_cropped_npz_files", ":", "\n", "            ", "case_identifier", "=", "get_case_identifier_from_npz", "(", "c", ")", "\n", "properties", "=", "self", ".", "load_properties_of_cropped", "(", "case_identifier", ")", "\n", "properties", "[", "'use_nonzero_mask_for_norm'", "]", "=", "self", ".", "plans", "[", "'use_mask_for_norm'", "]", "\n", "self", ".", "save_properties_of_cropped", "(", "case_identifier", ",", "properties", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.experiment_planning.experiment_planner_baseline_3DUNet.ExperimentPlanner.run_preprocessing": [[422, 443], ["os.path.isdir", "shutil.copytree", "nnunet.training.model_restore.recursive_find_python_class", "nnunet.training.model_restore.recursive_find_python_class.", "nnunet.training.model_restore.recursive_find_python_class.run", "join", "shutil.rmtree", "join", "join", "join", "join", "experiment_planner_baseline_3DUNet.ExperimentPlanner.plans_per_stage.values", "isinstance", "isinstance"], "methods", ["home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.training.model_restore.recursive_find_python_class", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.preprocessing.preprocessing.PreprocessorFor2D.run"], ["", "", "def", "run_preprocessing", "(", "self", ",", "num_threads", ")", ":", "\n", "        ", "if", "os", ".", "path", ".", "isdir", "(", "join", "(", "self", ".", "preprocessed_output_folder", ",", "\"gt_segmentations\"", ")", ")", ":", "\n", "            ", "shutil", ".", "rmtree", "(", "join", "(", "self", ".", "preprocessed_output_folder", ",", "\"gt_segmentations\"", ")", ")", "\n", "", "shutil", ".", "copytree", "(", "join", "(", "self", ".", "folder_with_cropped_data", ",", "\"gt_segmentations\"", ")", ",", "join", "(", "self", ".", "preprocessed_output_folder", ",", "\n", "\"gt_segmentations\"", ")", ")", "\n", "normalization_schemes", "=", "self", ".", "plans", "[", "'normalization_schemes'", "]", "\n", "use_nonzero_mask_for_normalization", "=", "self", ".", "plans", "[", "'use_mask_for_norm'", "]", "\n", "intensityproperties", "=", "self", ".", "plans", "[", "'dataset_properties'", "]", "[", "'intensityproperties'", "]", "\n", "preprocessor_class", "=", "recursive_find_python_class", "(", "[", "join", "(", "nnunet", ".", "__path__", "[", "0", "]", ",", "\"preprocessing\"", ")", "]", ",", "\n", "self", ".", "preprocessor_name", ",", "current_module", "=", "\"nnunet.preprocessing\"", ")", "\n", "assert", "preprocessor_class", "is", "not", "None", "\n", "preprocessor", "=", "preprocessor_class", "(", "normalization_schemes", ",", "use_nonzero_mask_for_normalization", ",", "\n", "self", ".", "transpose_forward", ",", "\n", "intensityproperties", ")", "\n", "target_spacings", "=", "[", "i", "[", "\"current_spacing\"", "]", "for", "i", "in", "self", ".", "plans_per_stage", ".", "values", "(", ")", "]", "\n", "if", "self", ".", "plans", "[", "'num_stages'", "]", ">", "1", "and", "not", "isinstance", "(", "num_threads", ",", "(", "list", ",", "tuple", ")", ")", ":", "\n", "            ", "num_threads", "=", "(", "default_num_threads", ",", "num_threads", ")", "\n", "", "elif", "self", ".", "plans", "[", "'num_stages'", "]", "==", "1", "and", "isinstance", "(", "num_threads", ",", "(", "list", ",", "tuple", ")", ")", ":", "\n", "            ", "num_threads", "=", "num_threads", "[", "-", "1", "]", "\n", "", "preprocessor", ".", "run", "(", "target_spacings", ",", "self", ".", "folder_with_cropped_data", ",", "self", ".", "preprocessed_output_folder", ",", "\n", "self", ".", "plans", "[", "'data_identifier'", "]", ",", "num_threads", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.experiment_planning.nnUNet_convert_decathlon_task.crawl_and_remove_hidden_from_decathlon": [[20, 39], ["nnunet.utilities.file_endings.remove_trailing_slash", "[].startswith", "subfolders", "os.remove", "os.remove", "os.remove", "os.remove", "subfiles", "subfiles", "subfiles", "subfiles", "nnunet.utilities.file_endings.remove_trailing_slash.split", "join", "join", "join"], "function", ["home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.utilities.file_endings.remove_trailing_slash"], ["def", "crawl_and_remove_hidden_from_decathlon", "(", "folder", ")", ":", "\n", "    ", "folder", "=", "remove_trailing_slash", "(", "folder", ")", "\n", "assert", "folder", ".", "split", "(", "'/'", ")", "[", "-", "1", "]", ".", "startswith", "(", "\"Task\"", ")", ",", "\"This does not seem to be a decathlon folder. Please give me a \"", "\"folder that starts with TaskXX and has the subfolders imagesTr, \"", "\"labelsTr and imagesTs\"", "\n", "subf", "=", "subfolders", "(", "folder", ",", "join", "=", "False", ")", "\n", "assert", "'imagesTr'", "in", "subf", ",", "\"This does not seem to be a decathlon folder. Please give me a \"", "\"folder that starts with TaskXX and has the subfolders imagesTr, \"", "\"labelsTr and imagesTs\"", "\n", "assert", "'imagesTs'", "in", "subf", ",", "\"This does not seem to be a decathlon folder. Please give me a \"", "\"folder that starts with TaskXX and has the subfolders imagesTr, \"", "\"labelsTr and imagesTs\"", "\n", "assert", "'labelsTr'", "in", "subf", ",", "\"This does not seem to be a decathlon folder. Please give me a \"", "\"folder that starts with TaskXX and has the subfolders imagesTr, \"", "\"labelsTr and imagesTs\"", "\n", "_", "=", "[", "os", ".", "remove", "(", "i", ")", "for", "i", "in", "subfiles", "(", "folder", ",", "prefix", "=", "\".\"", ")", "]", "\n", "_", "=", "[", "os", ".", "remove", "(", "i", ")", "for", "i", "in", "subfiles", "(", "join", "(", "folder", ",", "'imagesTr'", ")", ",", "prefix", "=", "\".\"", ")", "]", "\n", "_", "=", "[", "os", ".", "remove", "(", "i", ")", "for", "i", "in", "subfiles", "(", "join", "(", "folder", ",", "'labelsTr'", ")", ",", "prefix", "=", "\".\"", ")", "]", "\n", "_", "=", "[", "os", ".", "remove", "(", "i", ")", "for", "i", "in", "subfiles", "(", "join", "(", "folder", ",", "'imagesTs'", ")", ",", "prefix", "=", "\".\"", ")", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.experiment_planning.nnUNet_convert_decathlon_task.main": [[41, 61], ["argparse.ArgumentParser", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.parse_args", "nnUNet_convert_decathlon_task.crawl_and_remove_hidden_from_decathlon", "nnunet.experiment_planning.utils.split_4d"], "function", ["home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.experiment_planning.nnUNet_convert_decathlon_task.crawl_and_remove_hidden_from_decathlon", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.experiment_planning.utils.split_4d"], ["", "def", "main", "(", ")", ":", "\n", "    ", "import", "argparse", "\n", "parser", "=", "argparse", ".", "ArgumentParser", "(", "description", "=", "\"The MSD provides data as 4D Niftis with the modality being the first\"", "\n", "\" dimension. We think this may be cumbersome for some users and \"", "\n", "\"therefore expect 3D niftixs instead, with one file per modality. \"", "\n", "\"This utility will convert 4D MSD data into the format nnU-Net \"", "\n", "\"expects\"", ")", "\n", "parser", ".", "add_argument", "(", "\"-i\"", ",", "help", "=", "\"Input folder. Must point to a TaskXX_TASKNAME folder as downloaded from the MSD \"", "\n", "\"website\"", ",", "required", "=", "True", ")", "\n", "parser", ".", "add_argument", "(", "\"-p\"", ",", "required", "=", "False", ",", "default", "=", "default_num_threads", ",", "type", "=", "int", ",", "\n", "help", "=", "\"Use this to specify how many processes are used to run the script. \"", "\n", "\"Default is %d\"", "%", "default_num_threads", ")", "\n", "parser", ".", "add_argument", "(", "\"-output_task_id\"", ",", "required", "=", "False", ",", "default", "=", "None", ",", "type", "=", "int", ",", "\n", "help", "=", "\"If specified, this will overwrite the task id in the output folder. If unspecified, the \"", "\n", "\"task id of the input folder will be used.\"", ")", "\n", "args", "=", "parser", ".", "parse_args", "(", ")", "\n", "\n", "crawl_and_remove_hidden_from_decathlon", "(", "args", ".", "i", ")", "\n", "\n", "split_4d", "(", "args", ".", "i", ",", "args", ".", "p", ",", "args", ".", "output_task_id", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.experiment_planning.experiment_planner_baseline_2DUNet_v21.ExperimentPlanner2D_v21.__init__": [[24, 30], ["nnunet.experiment_planning.experiment_planner_baseline_2DUNet.ExperimentPlanner2D.__init__", "join"], "methods", ["home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.common.layers.ResizeImage.__init__"], ["    ", "def", "__init__", "(", "self", ",", "folder_with_cropped_data", ",", "preprocessed_output_folder", ")", ":", "\n", "        ", "super", "(", "ExperimentPlanner2D_v21", ",", "self", ")", ".", "__init__", "(", "folder_with_cropped_data", ",", "preprocessed_output_folder", ")", "\n", "self", ".", "data_identifier", "=", "\"nnUNetData_plans_v2.1_2D\"", "\n", "self", ".", "plans_fname", "=", "join", "(", "self", ".", "preprocessed_output_folder", ",", "\n", "\"nnUNetPlansv2.1_plans_2D.pkl\"", ")", "\n", "self", ".", "unet_base_num_features", "=", "32", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.experiment_planning.experiment_planner_baseline_2DUNet_v21.ExperimentPlanner2D_v21.get_properties_for_stage": [[31, 77], ["numpy.round().astype", "nnunet.experiment_planning.common_utils.get_pool_and_conv_props", "nnunet.network_architecture.generic_UNet.Generic_UNet.compute_approx_vram_consumption", "int", "numpy.round().astype", "min", "numpy.prod", "numpy.floor", "RuntimeError", "numpy.round", "numpy.round", "numpy.prod"], "methods", ["home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.experiment_planning.common_utils.get_pool_and_conv_props", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.network_architecture.generic_modular_UNet.PlainConvUNet.compute_approx_vram_consumption"], ["", "def", "get_properties_for_stage", "(", "self", ",", "current_spacing", ",", "original_spacing", ",", "original_shape", ",", "num_cases", ",", "\n", "num_modalities", ",", "num_classes", ")", ":", "\n", "\n", "        ", "new_median_shape", "=", "np", ".", "round", "(", "original_spacing", "/", "current_spacing", "*", "original_shape", ")", ".", "astype", "(", "int", ")", "\n", "\n", "dataset_num_voxels", "=", "np", ".", "prod", "(", "new_median_shape", ",", "dtype", "=", "np", ".", "int64", ")", "*", "num_cases", "\n", "input_patch_size", "=", "new_median_shape", "[", "1", ":", "]", "\n", "\n", "network_numpool", ",", "net_pool_kernel_sizes", ",", "net_conv_kernel_sizes", ",", "input_patch_size", ",", "shape_must_be_divisible_by", "=", "get_pool_and_conv_props", "(", "current_spacing", "[", "1", ":", "]", ",", "input_patch_size", ",", "\n", "self", ".", "unet_featuremap_min_edge_length", ",", "\n", "self", ".", "unet_max_numpool", ")", "\n", "\n", "# we pretend to use 30 feature maps. This will yield the same configuration as in V1. The larger memory", "\n", "# footpring of 32 vs 30 is mor ethan offset by the fp16 training. We make fp16 training default", "\n", "# Reason for 32 vs 30 feature maps is that 32 is faster in fp16 training (because multiple of 8)", "\n", "estimated_gpu_ram_consumption", "=", "Generic_UNet", ".", "compute_approx_vram_consumption", "(", "input_patch_size", ",", "\n", "network_numpool", ",", "\n", "30", ",", "\n", "self", ".", "unet_max_num_filters", ",", "\n", "num_modalities", ",", "num_classes", ",", "\n", "net_pool_kernel_sizes", ",", "\n", "conv_per_stage", "=", "self", ".", "conv_per_stage", ")", "\n", "batch_size", "=", "int", "(", "np", ".", "floor", "(", "Generic_UNet", ".", "use_this_for_batch_size_computation_2D", "/", "\n", "estimated_gpu_ram_consumption", "*", "Generic_UNet", ".", "DEFAULT_BATCH_SIZE_2D", ")", ")", "\n", "if", "batch_size", "<", "self", ".", "unet_min_batch_size", ":", "\n", "            ", "raise", "RuntimeError", "(", "\"This framework is not made to process patches this large. We will add patch-based \"", "\n", "\"2D networks later. Sorry for the inconvenience\"", ")", "\n", "\n", "# check if batch size is too large (more than 5 % of dataset)", "\n", "", "max_batch_size", "=", "np", ".", "round", "(", "self", ".", "batch_size_covers_max_percent_of_dataset", "*", "dataset_num_voxels", "/", "\n", "np", ".", "prod", "(", "input_patch_size", ",", "dtype", "=", "np", ".", "int64", ")", ")", ".", "astype", "(", "int", ")", "\n", "batch_size", "=", "min", "(", "batch_size", ",", "max_batch_size", ")", "\n", "\n", "plan", "=", "{", "\n", "'batch_size'", ":", "batch_size", ",", "\n", "'num_pool_per_axis'", ":", "network_numpool", ",", "\n", "'patch_size'", ":", "input_patch_size", ",", "\n", "'median_patient_size_in_voxels'", ":", "new_median_shape", ",", "\n", "'current_spacing'", ":", "current_spacing", ",", "\n", "'original_spacing'", ":", "original_spacing", ",", "\n", "'pool_op_kernel_sizes'", ":", "net_pool_kernel_sizes", ",", "\n", "'conv_kernel_sizes'", ":", "net_conv_kernel_sizes", ",", "\n", "'do_dummy_2D_data_aug'", ":", "False", "\n", "}", "\n", "return", "plan", "\n", "", "", ""]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.experiment_planning.common_utils.split_4d_nifti": [[23, 48], ["SimpleITK.ReadImage", "sitk.ReadImage.GetDimension", "filename.split", "shutil.copy", "batchgenerators.utilities.file_and_folder_operations.join", "RuntimeError", "SimpleITK.GetArrayFromImage", "sitk.ReadImage.GetSpacing", "sitk.ReadImage.GetOrigin", "numpy.array().reshape", "tuple", "tuple", "tuple", "enumerate", "list", "list", "direction[].reshape", "range", "SimpleITK.GetImageFromArray", "sitk.GetImageFromArray.SetSpacing", "sitk.GetImageFromArray.SetOrigin", "sitk.GetImageFromArray.SetDirection", "SimpleITK.WriteImage", "numpy.array", "batchgenerators.utilities.file_and_folder_operations.join", "sitk.ReadImage.GetDirection"], "function", ["None"], ["def", "split_4d_nifti", "(", "filename", ",", "output_folder", ")", ":", "\n", "    ", "img_itk", "=", "sitk", ".", "ReadImage", "(", "filename", ")", "\n", "dim", "=", "img_itk", ".", "GetDimension", "(", ")", "\n", "file_base", "=", "filename", ".", "split", "(", "\"/\"", ")", "[", "-", "1", "]", "\n", "if", "dim", "==", "3", ":", "\n", "        ", "shutil", ".", "copy", "(", "filename", ",", "join", "(", "output_folder", ",", "file_base", "[", ":", "-", "7", "]", "+", "\"_0000.nii.gz\"", ")", ")", "\n", "return", "\n", "", "elif", "dim", "!=", "4", ":", "\n", "        ", "raise", "RuntimeError", "(", "\"Unexpected dimensionality: %d of file %s, cannot split\"", "%", "(", "dim", ",", "filename", ")", ")", "\n", "", "else", ":", "\n", "        ", "img_npy", "=", "sitk", ".", "GetArrayFromImage", "(", "img_itk", ")", "\n", "spacing", "=", "img_itk", ".", "GetSpacing", "(", ")", "\n", "origin", "=", "img_itk", ".", "GetOrigin", "(", ")", "\n", "direction", "=", "np", ".", "array", "(", "img_itk", ".", "GetDirection", "(", ")", ")", ".", "reshape", "(", "4", ",", "4", ")", "\n", "# now modify these to remove the fourth dimension", "\n", "spacing", "=", "tuple", "(", "list", "(", "spacing", "[", ":", "-", "1", "]", ")", ")", "\n", "origin", "=", "tuple", "(", "list", "(", "origin", "[", ":", "-", "1", "]", ")", ")", "\n", "direction", "=", "tuple", "(", "direction", "[", ":", "-", "1", ",", ":", "-", "1", "]", ".", "reshape", "(", "-", "1", ")", ")", "\n", "for", "i", ",", "t", "in", "enumerate", "(", "range", "(", "img_npy", ".", "shape", "[", "0", "]", ")", ")", ":", "\n", "            ", "img", "=", "img_npy", "[", "t", "]", "\n", "img_itk_new", "=", "sitk", ".", "GetImageFromArray", "(", "img", ")", "\n", "img_itk_new", ".", "SetSpacing", "(", "spacing", ")", "\n", "img_itk_new", ".", "SetOrigin", "(", "origin", ")", "\n", "img_itk_new", ".", "SetDirection", "(", "direction", ")", "\n", "sitk", ".", "WriteImage", "(", "img_itk_new", ",", "join", "(", "output_folder", ",", "file_base", "[", ":", "-", "7", "]", "+", "\"_%04.0d.nii.gz\"", "%", "i", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.experiment_planning.common_utils.get_pool_and_conv_props_poolLateV2": [[50, 87], ["copy.deepcopy", "max", "len", "common_utils.get_network_numpool", "max", "range", "net_conv_kernel_sizes.append", "common_utils.get_shape_must_be_divisible_by", "common_utils.pad_shape", "all", "net_num_pool_op_kernel_sizes.append", "net_conv_kernel_sizes.append", "range", "range", "zip", "range"], "function", ["home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.experiment_planning.common_utils.get_network_numpool", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.experiment_planning.common_utils.get_shape_must_be_divisible_by", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.experiment_planning.common_utils.pad_shape"], ["", "", "", "def", "get_pool_and_conv_props_poolLateV2", "(", "patch_size", ",", "min_feature_map_size", ",", "max_numpool", ",", "spacing", ")", ":", "\n", "    ", "\"\"\"\n\n    :param spacing:\n    :param patch_size:\n    :param min_feature_map_size: min edge length of feature maps in bottleneck\n    :return:\n    \"\"\"", "\n", "initial_spacing", "=", "deepcopy", "(", "spacing", ")", "\n", "reach", "=", "max", "(", "initial_spacing", ")", "\n", "dim", "=", "len", "(", "patch_size", ")", "\n", "\n", "num_pool_per_axis", "=", "get_network_numpool", "(", "patch_size", ",", "max_numpool", ",", "min_feature_map_size", ")", "\n", "\n", "net_num_pool_op_kernel_sizes", "=", "[", "]", "\n", "net_conv_kernel_sizes", "=", "[", "]", "\n", "net_numpool", "=", "max", "(", "num_pool_per_axis", ")", "\n", "\n", "current_spacing", "=", "spacing", "\n", "for", "p", "in", "range", "(", "net_numpool", ")", ":", "\n", "        ", "reached", "=", "[", "current_spacing", "[", "i", "]", "/", "reach", ">", "0.5", "for", "i", "in", "range", "(", "dim", ")", "]", "\n", "pool", "=", "[", "2", "if", "num_pool_per_axis", "[", "i", "]", "+", "p", ">=", "net_numpool", "else", "1", "for", "i", "in", "range", "(", "dim", ")", "]", "\n", "if", "all", "(", "reached", ")", ":", "\n", "            ", "conv", "=", "[", "3", "]", "*", "dim", "\n", "", "else", ":", "\n", "            ", "conv", "=", "[", "3", "if", "not", "reached", "[", "i", "]", "else", "1", "for", "i", "in", "range", "(", "dim", ")", "]", "\n", "", "net_num_pool_op_kernel_sizes", ".", "append", "(", "pool", ")", "\n", "net_conv_kernel_sizes", ".", "append", "(", "conv", ")", "\n", "current_spacing", "=", "[", "i", "*", "j", "for", "i", ",", "j", "in", "zip", "(", "current_spacing", ",", "pool", ")", "]", "\n", "\n", "", "net_conv_kernel_sizes", ".", "append", "(", "[", "3", "]", "*", "dim", ")", "\n", "\n", "must_be_divisible_by", "=", "get_shape_must_be_divisible_by", "(", "num_pool_per_axis", ")", "\n", "patch_size", "=", "pad_shape", "(", "patch_size", ",", "must_be_divisible_by", ")", "\n", "\n", "# we need to add one more conv_kernel_size for the bottleneck. We always use 3x3(x3) conv here", "\n", "return", "num_pool_per_axis", ",", "net_num_pool_op_kernel_sizes", ",", "net_conv_kernel_sizes", ",", "patch_size", ",", "must_be_divisible_by", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.experiment_planning.common_utils.get_pool_and_conv_props": [[89, 155], ["len", "copy.deepcopy", "copy.deepcopy", "common_utils.get_shape_must_be_divisible_by", "common_utils.pad_shape", "conv_kernel_sizes.append", "list", "list", "min", "range", "pool_op_kernel_sizes.append", "conv_kernel_sizes.append", "len", "numpy.ceil", "range", "len", "len", "range", "range", "range"], "function", ["home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.experiment_planning.common_utils.get_shape_must_be_divisible_by", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.experiment_planning.common_utils.pad_shape"], ["", "def", "get_pool_and_conv_props", "(", "spacing", ",", "patch_size", ",", "min_feature_map_size", ",", "max_numpool", ")", ":", "\n", "    ", "\"\"\"\n\n    :param spacing:\n    :param patch_size:\n    :param min_feature_map_size: min edge length of feature maps in bottleneck\n    :return:\n    \"\"\"", "\n", "dim", "=", "len", "(", "spacing", ")", "\n", "\n", "current_spacing", "=", "deepcopy", "(", "list", "(", "spacing", ")", ")", "\n", "current_size", "=", "deepcopy", "(", "list", "(", "patch_size", ")", ")", "\n", "\n", "pool_op_kernel_sizes", "=", "[", "]", "\n", "conv_kernel_sizes", "=", "[", "]", "\n", "\n", "num_pool_per_axis", "=", "[", "0", "]", "*", "dim", "\n", "\n", "while", "True", ":", "\n", "# This is a problem because sometimes we have spacing 20, 50, 50 and we want to still keep pooling.", "\n", "# Here we would stop however. This is not what we want! Fixed in get_pool_and_conv_propsv2", "\n", "        ", "min_spacing", "=", "min", "(", "current_spacing", ")", "\n", "valid_axes_for_pool", "=", "[", "i", "for", "i", "in", "range", "(", "dim", ")", "if", "current_spacing", "[", "i", "]", "/", "min_spacing", "<", "2", "]", "\n", "axes", "=", "[", "]", "\n", "for", "a", "in", "range", "(", "dim", ")", ":", "\n", "            ", "my_spacing", "=", "current_spacing", "[", "a", "]", "\n", "partners", "=", "[", "i", "for", "i", "in", "range", "(", "dim", ")", "if", "current_spacing", "[", "i", "]", "/", "my_spacing", "<", "2", "and", "my_spacing", "/", "current_spacing", "[", "i", "]", "<", "2", "]", "\n", "if", "len", "(", "partners", ")", ">", "len", "(", "axes", ")", ":", "\n", "                ", "axes", "=", "partners", "\n", "", "", "conv_kernel_size", "=", "[", "3", "if", "i", "in", "axes", "else", "1", "for", "i", "in", "range", "(", "dim", ")", "]", "\n", "\n", "# exclude axes that we cannot pool further because of min_feature_map_size constraint", "\n", "#before = len(valid_axes_for_pool)", "\n", "valid_axes_for_pool", "=", "[", "i", "for", "i", "in", "valid_axes_for_pool", "if", "current_size", "[", "i", "]", ">=", "2", "*", "min_feature_map_size", "]", "\n", "#after = len(valid_axes_for_pool)", "\n", "#if after == 1 and before > 1:", "\n", "#    break", "\n", "\n", "valid_axes_for_pool", "=", "[", "i", "for", "i", "in", "valid_axes_for_pool", "if", "num_pool_per_axis", "[", "i", "]", "<", "max_numpool", "]", "\n", "\n", "if", "len", "(", "valid_axes_for_pool", ")", "==", "0", ":", "\n", "            ", "break", "\n", "\n", "#print(current_spacing, current_size)", "\n", "\n", "", "other_axes", "=", "[", "i", "for", "i", "in", "range", "(", "dim", ")", "if", "i", "not", "in", "valid_axes_for_pool", "]", "\n", "\n", "pool_kernel_sizes", "=", "[", "0", "]", "*", "dim", "\n", "for", "v", "in", "valid_axes_for_pool", ":", "\n", "            ", "pool_kernel_sizes", "[", "v", "]", "=", "2", "\n", "num_pool_per_axis", "[", "v", "]", "+=", "1", "\n", "current_spacing", "[", "v", "]", "*=", "2", "\n", "current_size", "[", "v", "]", "=", "np", ".", "ceil", "(", "current_size", "[", "v", "]", "/", "2", ")", "\n", "", "for", "nv", "in", "other_axes", ":", "\n", "            ", "pool_kernel_sizes", "[", "nv", "]", "=", "1", "\n", "\n", "", "pool_op_kernel_sizes", ".", "append", "(", "pool_kernel_sizes", ")", "\n", "conv_kernel_sizes", ".", "append", "(", "conv_kernel_size", ")", "\n", "#print(conv_kernel_sizes)", "\n", "\n", "", "must_be_divisible_by", "=", "get_shape_must_be_divisible_by", "(", "num_pool_per_axis", ")", "\n", "patch_size", "=", "pad_shape", "(", "patch_size", ",", "must_be_divisible_by", ")", "\n", "\n", "# we need to add one more conv_kernel_size for the bottleneck. We always use 3x3(x3) conv here", "\n", "conv_kernel_sizes", ".", "append", "(", "[", "3", "]", "*", "dim", ")", "\n", "return", "num_pool_per_axis", ",", "pool_op_kernel_sizes", ",", "conv_kernel_sizes", ",", "patch_size", ",", "must_be_divisible_by", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.experiment_planning.common_utils.get_pool_and_conv_props_v2": [[157, 230], ["len", "copy.deepcopy", "copy.deepcopy", "common_utils.get_shape_must_be_divisible_by", "common_utils.pad_shape", "conv_kernel_sizes.append", "list", "list", "min", "range", "pool_op_kernel_sizes.append", "conv_kernel_sizes.append", "len", "len", "len", "numpy.ceil", "copy.deepcopy", "range", "range", "min"], "function", ["home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.experiment_planning.common_utils.get_shape_must_be_divisible_by", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.experiment_planning.common_utils.pad_shape"], ["", "def", "get_pool_and_conv_props_v2", "(", "spacing", ",", "patch_size", ",", "min_feature_map_size", ",", "max_numpool", ")", ":", "\n", "    ", "\"\"\"\n\n    :param spacing:\n    :param patch_size:\n    :param min_feature_map_size: min edge length of feature maps in bottleneck\n    :return:\n    \"\"\"", "\n", "dim", "=", "len", "(", "spacing", ")", "\n", "\n", "current_spacing", "=", "deepcopy", "(", "list", "(", "spacing", ")", ")", "\n", "current_size", "=", "deepcopy", "(", "list", "(", "patch_size", ")", ")", "\n", "\n", "pool_op_kernel_sizes", "=", "[", "]", "\n", "conv_kernel_sizes", "=", "[", "]", "\n", "\n", "num_pool_per_axis", "=", "[", "0", "]", "*", "dim", "\n", "kernel_size", "=", "[", "1", "]", "*", "dim", "\n", "\n", "while", "True", ":", "\n", "# exclude axes that we cannot pool further because of min_feature_map_size constraint", "\n", "        ", "valid_axes_for_pool", "=", "[", "i", "for", "i", "in", "range", "(", "dim", ")", "if", "current_size", "[", "i", "]", ">=", "2", "*", "min_feature_map_size", "]", "\n", "if", "len", "(", "valid_axes_for_pool", ")", "<", "1", ":", "\n", "            ", "break", "\n", "\n", "", "spacings_of_axes", "=", "[", "current_spacing", "[", "i", "]", "for", "i", "in", "valid_axes_for_pool", "]", "\n", "\n", "# find axis that are within factor of 2 within smallest spacing", "\n", "min_spacing_of_valid", "=", "min", "(", "spacings_of_axes", ")", "\n", "valid_axes_for_pool", "=", "[", "i", "for", "i", "in", "valid_axes_for_pool", "if", "current_spacing", "[", "i", "]", "/", "min_spacing_of_valid", "<", "2", "]", "\n", "\n", "# max_numpool constraint", "\n", "valid_axes_for_pool", "=", "[", "i", "for", "i", "in", "valid_axes_for_pool", "if", "num_pool_per_axis", "[", "i", "]", "<", "max_numpool", "]", "\n", "\n", "if", "len", "(", "valid_axes_for_pool", ")", "==", "1", ":", "\n", "            ", "if", "current_size", "[", "valid_axes_for_pool", "[", "0", "]", "]", ">=", "3", "*", "min_feature_map_size", ":", "\n", "                ", "pass", "\n", "", "else", ":", "\n", "                ", "break", "\n", "", "", "if", "len", "(", "valid_axes_for_pool", ")", "<", "1", ":", "\n", "            ", "break", "\n", "\n", "# now we need to find kernel sizes", "\n", "# kernel sizes are initialized to 1. They are successively set to 3 when their associated axis becomes within", "\n", "# factor 2 of min_spacing. Once they are 3 they remain 3", "\n", "", "for", "d", "in", "range", "(", "dim", ")", ":", "\n", "            ", "if", "kernel_size", "[", "d", "]", "==", "3", ":", "\n", "                ", "continue", "\n", "", "else", ":", "\n", "                ", "if", "spacings_of_axes", "[", "d", "]", "/", "min", "(", "current_spacing", ")", "<", "2", ":", "\n", "                    ", "kernel_size", "[", "d", "]", "=", "3", "\n", "\n", "", "", "", "other_axes", "=", "[", "i", "for", "i", "in", "range", "(", "dim", ")", "if", "i", "not", "in", "valid_axes_for_pool", "]", "\n", "\n", "pool_kernel_sizes", "=", "[", "0", "]", "*", "dim", "\n", "for", "v", "in", "valid_axes_for_pool", ":", "\n", "            ", "pool_kernel_sizes", "[", "v", "]", "=", "2", "\n", "num_pool_per_axis", "[", "v", "]", "+=", "1", "\n", "current_spacing", "[", "v", "]", "*=", "2", "\n", "current_size", "[", "v", "]", "=", "np", ".", "ceil", "(", "current_size", "[", "v", "]", "/", "2", ")", "\n", "", "for", "nv", "in", "other_axes", ":", "\n", "            ", "pool_kernel_sizes", "[", "nv", "]", "=", "1", "\n", "\n", "", "pool_op_kernel_sizes", ".", "append", "(", "pool_kernel_sizes", ")", "\n", "conv_kernel_sizes", ".", "append", "(", "deepcopy", "(", "kernel_size", ")", ")", "\n", "#print(conv_kernel_sizes)", "\n", "\n", "", "must_be_divisible_by", "=", "get_shape_must_be_divisible_by", "(", "num_pool_per_axis", ")", "\n", "patch_size", "=", "pad_shape", "(", "patch_size", ",", "must_be_divisible_by", ")", "\n", "\n", "# we need to add one more conv_kernel_size for the bottleneck. We always use 3x3(x3) conv here", "\n", "conv_kernel_sizes", ".", "append", "(", "[", "3", "]", "*", "dim", ")", "\n", "return", "num_pool_per_axis", ",", "pool_op_kernel_sizes", ",", "conv_kernel_sizes", ",", "patch_size", ",", "must_be_divisible_by", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.experiment_planning.common_utils.get_shape_must_be_divisible_by": [[232, 234], ["numpy.array"], "function", ["None"], ["", "def", "get_shape_must_be_divisible_by", "(", "net_numpool_per_axis", ")", ":", "\n", "    ", "return", "2", "**", "np", ".", "array", "(", "net_numpool_per_axis", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.experiment_planning.common_utils.pad_shape": [[236, 255], ["range", "numpy.array().astype", "isinstance", "len", "len", "len", "len", "range", "numpy.array", "len"], "function", ["None"], ["", "def", "pad_shape", "(", "shape", ",", "must_be_divisible_by", ")", ":", "\n", "    ", "\"\"\"\n    pads shape so that it is divisibly by must_be_divisible_by\n    :param shape:\n    :param must_be_divisible_by:\n    :return:\n    \"\"\"", "\n", "if", "not", "isinstance", "(", "must_be_divisible_by", ",", "(", "tuple", ",", "list", ",", "np", ".", "ndarray", ")", ")", ":", "\n", "        ", "must_be_divisible_by", "=", "[", "must_be_divisible_by", "]", "*", "len", "(", "shape", ")", "\n", "", "else", ":", "\n", "        ", "assert", "len", "(", "must_be_divisible_by", ")", "==", "len", "(", "shape", ")", "\n", "\n", "", "new_shp", "=", "[", "shape", "[", "i", "]", "+", "must_be_divisible_by", "[", "i", "]", "-", "shape", "[", "i", "]", "%", "must_be_divisible_by", "[", "i", "]", "for", "i", "in", "range", "(", "len", "(", "shape", ")", ")", "]", "\n", "\n", "for", "i", "in", "range", "(", "len", "(", "shape", ")", ")", ":", "\n", "        ", "if", "shape", "[", "i", "]", "%", "must_be_divisible_by", "[", "i", "]", "==", "0", ":", "\n", "            ", "new_shp", "[", "i", "]", "-=", "must_be_divisible_by", "[", "i", "]", "\n", "", "", "new_shp", "=", "np", ".", "array", "(", "new_shp", ")", ".", "astype", "(", "int", ")", "\n", "return", "new_shp", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.experiment_planning.common_utils.get_network_numpool": [[257, 261], ["numpy.floor().astype", "min", "numpy.floor", "numpy.log", "numpy.log"], "function", ["None"], ["", "def", "get_network_numpool", "(", "patch_size", ",", "maxpool_cap", "=", "999", ",", "min_feature_map_size", "=", "4", ")", ":", "\n", "    ", "network_numpool_per_axis", "=", "np", ".", "floor", "(", "[", "np", ".", "log", "(", "i", "/", "min_feature_map_size", ")", "/", "np", ".", "log", "(", "2", ")", "for", "i", "in", "patch_size", "]", ")", ".", "astype", "(", "int", ")", "\n", "network_numpool_per_axis", "=", "[", "min", "(", "i", ",", "maxpool_cap", ")", "for", "i", "in", "network_numpool_per_axis", "]", "\n", "return", "network_numpool_per_axis", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.experiment_planning.DatasetAnalyzer.DatasetAnalyzer.__init__": [[28, 44], ["nnunet.preprocessing.cropping.get_patient_identifiers_from_cropped_files", "isfile", "join", "join", "join"], "methods", ["home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.preprocessing.cropping.get_patient_identifiers_from_cropped_files"], ["    ", "def", "__init__", "(", "self", ",", "folder_with_cropped_data", ",", "overwrite", "=", "True", ",", "num_processes", "=", "default_num_threads", ")", ":", "\n", "        ", "\"\"\"\n        :param folder_with_cropped_data:\n        :param overwrite: If True then precomputed values will not be used and instead recomputed from the data.\n        False will allow loading of precomputed values. This may be dangerous though if some of the code of this class\n        was changed, therefore the default is True.\n        \"\"\"", "\n", "self", ".", "num_processes", "=", "num_processes", "\n", "self", ".", "overwrite", "=", "overwrite", "\n", "self", ".", "folder_with_cropped_data", "=", "folder_with_cropped_data", "\n", "self", ".", "sizes", "=", "self", ".", "spacings", "=", "None", "\n", "self", ".", "patient_identifiers", "=", "get_patient_identifiers_from_cropped_files", "(", "self", ".", "folder_with_cropped_data", ")", "\n", "assert", "isfile", "(", "join", "(", "self", ".", "folder_with_cropped_data", ",", "\"dataset.json\"", ")", ")", ",", "\"dataset.json needs to be in folder_with_cropped_data\"", "\n", "self", ".", "props_per_case_file", "=", "join", "(", "self", ".", "folder_with_cropped_data", ",", "\"props_per_case.pkl\"", ")", "\n", "self", ".", "intensityproperties_file", "=", "join", "(", "self", ".", "folder_with_cropped_data", ",", "\"intensityproperties.pkl\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.experiment_planning.DatasetAnalyzer.DatasetAnalyzer.load_properties_of_cropped": [[45, 49], ["open", "pickle.load", "join"], "methods", ["None"], ["", "def", "load_properties_of_cropped", "(", "self", ",", "case_identifier", ")", ":", "\n", "        ", "with", "open", "(", "join", "(", "self", ".", "folder_with_cropped_data", ",", "\"%s.pkl\"", "%", "case_identifier", ")", ",", "'rb'", ")", "as", "f", ":", "\n", "            ", "properties", "=", "pickle", ".", "load", "(", "f", ")", "\n", "", "return", "properties", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.experiment_planning.DatasetAnalyzer.DatasetAnalyzer._check_if_all_in_one_region": [[50, 63], ["collections.OrderedDict", "numpy.zeros", "skimage.morphology.label", "tuple", "tuple"], "methods", ["None"], ["", "@", "staticmethod", "\n", "def", "_check_if_all_in_one_region", "(", "seg", ",", "regions", ")", ":", "\n", "        ", "res", "=", "OrderedDict", "(", ")", "\n", "for", "r", "in", "regions", ":", "\n", "            ", "new_seg", "=", "np", ".", "zeros", "(", "seg", ".", "shape", ")", "\n", "for", "c", "in", "r", ":", "\n", "                ", "new_seg", "[", "seg", "==", "c", "]", "=", "1", "\n", "", "labelmap", ",", "numlabels", "=", "label", "(", "new_seg", ",", "return_num", "=", "True", ")", "\n", "if", "numlabels", "!=", "1", ":", "\n", "                ", "res", "[", "tuple", "(", "r", ")", "]", "=", "False", "\n", "", "else", ":", "\n", "                ", "res", "[", "tuple", "(", "r", ")", "]", "=", "True", "\n", "", "", "return", "res", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.experiment_planning.DatasetAnalyzer.DatasetAnalyzer._collect_class_and_region_sizes": [[64, 75], ["collections.OrderedDict", "collections.OrderedDict", "skimage.morphology.label", "range", "numpy.sum", "region_volume_per_class[].append", "numpy.sum"], "methods", ["None"], ["", "@", "staticmethod", "\n", "def", "_collect_class_and_region_sizes", "(", "seg", ",", "all_classes", ",", "vol_per_voxel", ")", ":", "\n", "        ", "volume_per_class", "=", "OrderedDict", "(", ")", "\n", "region_volume_per_class", "=", "OrderedDict", "(", ")", "\n", "for", "c", "in", "all_classes", ":", "\n", "            ", "region_volume_per_class", "[", "c", "]", "=", "[", "]", "\n", "volume_per_class", "[", "c", "]", "=", "np", ".", "sum", "(", "seg", "==", "c", ")", "*", "vol_per_voxel", "\n", "labelmap", ",", "numregions", "=", "label", "(", "seg", "==", "c", ",", "return_num", "=", "True", ")", "\n", "for", "l", "in", "range", "(", "1", ",", "numregions", "+", "1", ")", ":", "\n", "                ", "region_volume_per_class", "[", "c", "]", ".", "append", "(", "np", ".", "sum", "(", "labelmap", "==", "l", ")", "*", "vol_per_voxel", ")", "\n", "", "", "return", "volume_per_class", ",", "region_volume_per_class", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.experiment_planning.DatasetAnalyzer.DatasetAnalyzer._get_unique_labels": [[76, 80], ["numpy.unique", "numpy.load", "join"], "methods", ["None"], ["", "def", "_get_unique_labels", "(", "self", ",", "patient_identifier", ")", ":", "\n", "        ", "seg", "=", "np", ".", "load", "(", "join", "(", "self", ".", "folder_with_cropped_data", ",", "patient_identifier", ")", "+", "\".npz\"", ")", "[", "'data'", "]", "[", "-", "1", "]", "\n", "unique_classes", "=", "np", ".", "unique", "(", "seg", ")", "\n", "return", "unique_classes", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.experiment_planning.DatasetAnalyzer.DatasetAnalyzer._load_seg_analyze_classes": [[81, 108], ["load_pickle", "numpy.prod", "numpy.unique", "list", "list.append", "DatasetAnalyzer.DatasetAnalyzer._check_if_all_in_one_region", "DatasetAnalyzer.DatasetAnalyzer._collect_class_and_region_sizes", "list", "list.append", "numpy.load", "join", "join"], "methods", ["home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.experiment_planning.DatasetAnalyzer.DatasetAnalyzer._check_if_all_in_one_region", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.experiment_planning.DatasetAnalyzer.DatasetAnalyzer._collect_class_and_region_sizes"], ["", "def", "_load_seg_analyze_classes", "(", "self", ",", "patient_identifier", ",", "all_classes", ")", ":", "\n", "        ", "\"\"\"\n        1) what class is in this training case?\n        2) what is the size distribution for each class?\n        3) what is the region size of each class?\n        4) check if all in one region\n        :return:\n        \"\"\"", "\n", "seg", "=", "np", ".", "load", "(", "join", "(", "self", ".", "folder_with_cropped_data", ",", "patient_identifier", ")", "+", "\".npz\"", ")", "[", "'data'", "]", "[", "-", "1", "]", "\n", "pkl", "=", "load_pickle", "(", "join", "(", "self", ".", "folder_with_cropped_data", ",", "patient_identifier", ")", "+", "\".pkl\"", ")", "\n", "vol_per_voxel", "=", "np", ".", "prod", "(", "pkl", "[", "'itk_spacing'", "]", ")", "\n", "\n", "# ad 1)", "\n", "unique_classes", "=", "np", ".", "unique", "(", "seg", ")", "\n", "\n", "# 4) check if all in one region", "\n", "regions", "=", "list", "(", ")", "\n", "regions", ".", "append", "(", "list", "(", "all_classes", ")", ")", "\n", "for", "c", "in", "all_classes", ":", "\n", "            ", "regions", ".", "append", "(", "(", "c", ",", ")", ")", "\n", "\n", "", "all_in_one_region", "=", "self", ".", "_check_if_all_in_one_region", "(", "seg", ",", "regions", ")", "\n", "\n", "# 2 & 3) region sizes", "\n", "volume_per_class", ",", "region_sizes", "=", "self", ".", "_collect_class_and_region_sizes", "(", "seg", ",", "all_classes", ",", "vol_per_voxel", ")", "\n", "\n", "return", "unique_classes", ",", "all_in_one_region", ",", "volume_per_class", ",", "region_sizes", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.experiment_planning.DatasetAnalyzer.DatasetAnalyzer.get_classes": [[109, 112], ["load_json", "join"], "methods", ["None"], ["", "def", "get_classes", "(", "self", ")", ":", "\n", "        ", "datasetjson", "=", "load_json", "(", "join", "(", "self", ".", "folder_with_cropped_data", ",", "\"dataset.json\"", ")", ")", "\n", "return", "datasetjson", "[", "'labels'", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.experiment_planning.DatasetAnalyzer.DatasetAnalyzer.analyse_segmentations": [[113, 141], ["DatasetAnalyzer.DatasetAnalyzer.get_classes", "numpy.array", "multiprocessing.Pool", "multiprocessing.Pool.map", "multiprocessing.Pool.close", "multiprocessing.Pool.join", "collections.OrderedDict", "zip", "save_pickle", "load_pickle", "int", "isfile", "dict", "DatasetAnalyzer.DatasetAnalyzer.keys"], "methods", ["home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.experiment_planning.DatasetAnalyzer.DatasetAnalyzer.get_classes"], ["", "def", "analyse_segmentations", "(", "self", ")", ":", "\n", "        ", "class_dct", "=", "self", ".", "get_classes", "(", ")", "\n", "all_classes", "=", "np", ".", "array", "(", "[", "int", "(", "i", ")", "for", "i", "in", "class_dct", ".", "keys", "(", ")", "]", ")", "\n", "all_classes", "=", "all_classes", "[", "all_classes", ">", "0", "]", "# remove background", "\n", "\n", "if", "self", ".", "overwrite", "or", "not", "isfile", "(", "self", ".", "props_per_case_file", ")", ":", "\n", "            ", "p", "=", "Pool", "(", "self", ".", "num_processes", ")", "\n", "#res = p.starmap(self._load_seg_analyze_classes, zip(self.patient_identifiers,", "\n", "#                                                [all_classes] * len(self.patient_identifiers)))", "\n", "res", "=", "p", ".", "map", "(", "self", ".", "_get_unique_labels", ",", "self", ".", "patient_identifiers", ")", "\n", "p", ".", "close", "(", ")", "\n", "p", ".", "join", "(", ")", "\n", "\n", "props_per_patient", "=", "OrderedDict", "(", ")", "\n", "#for p, (unique_classes, all_in_one_region, voxels_per_class, region_volume_per_class) in \\", "\n", "for", "p", ",", "unique_classes", "in", "zip", "(", "self", ".", "patient_identifiers", ",", "res", ")", ":", "\n", "                ", "props", "=", "dict", "(", ")", "\n", "props", "[", "'has_classes'", "]", "=", "unique_classes", "\n", "#props['only_one_region'] = all_in_one_region", "\n", "#props['volume_per_class'] = voxels_per_class", "\n", "#props['region_volume_per_class'] = region_volume_per_class", "\n", "props_per_patient", "[", "p", "]", "=", "props", "\n", "\n", "", "save_pickle", "(", "props_per_patient", ",", "self", ".", "props_per_case_file", ")", "\n", "", "else", ":", "\n", "            ", "props_per_patient", "=", "load_pickle", "(", "self", ".", "props_per_case_file", ")", "\n", "", "return", "class_dct", ",", "props_per_patient", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.experiment_planning.DatasetAnalyzer.DatasetAnalyzer.get_sizes_and_spacings_after_cropping": [[142, 152], ["nnunet.preprocessing.cropping.get_patient_identifiers_from_cropped_files", "DatasetAnalyzer.DatasetAnalyzer.load_properties_of_cropped", "sizes.append", "spacings.append"], "methods", ["home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.preprocessing.cropping.get_patient_identifiers_from_cropped_files", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.experiment_planning.DatasetAnalyzer.DatasetAnalyzer.load_properties_of_cropped"], ["", "def", "get_sizes_and_spacings_after_cropping", "(", "self", ")", ":", "\n", "        ", "case_identifiers", "=", "get_patient_identifiers_from_cropped_files", "(", "self", ".", "folder_with_cropped_data", ")", "\n", "sizes", "=", "[", "]", "\n", "spacings", "=", "[", "]", "\n", "for", "c", "in", "case_identifiers", ":", "\n", "            ", "properties", "=", "self", ".", "load_properties_of_cropped", "(", "c", ")", "\n", "sizes", ".", "append", "(", "properties", "[", "\"size_after_cropping\"", "]", ")", "\n", "spacings", ".", "append", "(", "properties", "[", "\"original_spacing\"", "]", ")", "\n", "\n", "", "return", "sizes", ",", "spacings", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.experiment_planning.DatasetAnalyzer.DatasetAnalyzer.get_modalities": [[153, 158], ["load_json", "join", "int", "modalities.keys"], "methods", ["None"], ["", "def", "get_modalities", "(", "self", ")", ":", "\n", "        ", "datasetjson", "=", "load_json", "(", "join", "(", "self", ".", "folder_with_cropped_data", ",", "\"dataset.json\"", ")", ")", "\n", "modalities", "=", "datasetjson", "[", "\"modality\"", "]", "\n", "modalities", "=", "{", "int", "(", "k", ")", ":", "modalities", "[", "k", "]", "for", "k", "in", "modalities", ".", "keys", "(", ")", "}", "\n", "return", "modalities", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.experiment_planning.DatasetAnalyzer.DatasetAnalyzer.get_size_reduction_by_cropping": [[159, 168], ["collections.OrderedDict", "DatasetAnalyzer.DatasetAnalyzer.load_properties_of_cropped", "numpy.prod", "numpy.prod"], "methods", ["home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.experiment_planning.DatasetAnalyzer.DatasetAnalyzer.load_properties_of_cropped"], ["", "def", "get_size_reduction_by_cropping", "(", "self", ")", ":", "\n", "        ", "size_reduction", "=", "OrderedDict", "(", ")", "\n", "for", "p", "in", "self", ".", "patient_identifiers", ":", "\n", "            ", "props", "=", "self", ".", "load_properties_of_cropped", "(", "p", ")", "\n", "shape_before_crop", "=", "props", "[", "\"original_size_of_raw_data\"", "]", "\n", "shape_after_crop", "=", "props", "[", "'size_after_cropping'", "]", "\n", "size_red", "=", "np", ".", "prod", "(", "shape_after_crop", ")", "/", "np", ".", "prod", "(", "shape_before_crop", ")", "\n", "size_reduction", "[", "p", "]", "=", "size_red", "\n", "", "return", "size_reduction", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.experiment_planning.DatasetAnalyzer.DatasetAnalyzer._get_voxels_in_foreground": [[169, 175], ["list", "numpy.load", "join"], "methods", ["None"], ["", "def", "_get_voxels_in_foreground", "(", "self", ",", "patient_identifier", ",", "modality_id", ")", ":", "\n", "        ", "all_data", "=", "np", ".", "load", "(", "join", "(", "self", ".", "folder_with_cropped_data", ",", "patient_identifier", ")", "+", "\".npz\"", ")", "[", "'data'", "]", "\n", "modality", "=", "all_data", "[", "modality_id", "]", "\n", "mask", "=", "all_data", "[", "-", "1", "]", ">", "0", "\n", "voxels", "=", "list", "(", "modality", "[", "mask", "]", "[", ":", ":", "10", "]", ")", "# no need to take every voxel", "\n", "return", "voxels", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.experiment_planning.DatasetAnalyzer.DatasetAnalyzer._compute_stats": [[176, 188], ["numpy.median", "numpy.mean", "numpy.std", "numpy.min", "numpy.max", "numpy.percentile", "numpy.percentile", "len"], "methods", ["None"], ["", "@", "staticmethod", "\n", "def", "_compute_stats", "(", "voxels", ")", ":", "\n", "        ", "if", "len", "(", "voxels", ")", "==", "0", ":", "\n", "            ", "return", "np", ".", "nan", ",", "np", ".", "nan", ",", "np", ".", "nan", ",", "np", ".", "nan", ",", "np", ".", "nan", ",", "np", ".", "nan", ",", "np", ".", "nan", "\n", "", "median", "=", "np", ".", "median", "(", "voxels", ")", "\n", "mean", "=", "np", ".", "mean", "(", "voxels", ")", "\n", "sd", "=", "np", ".", "std", "(", "voxels", ")", "\n", "mn", "=", "np", ".", "min", "(", "voxels", ")", "\n", "mx", "=", "np", ".", "max", "(", "voxels", ")", "\n", "percentile_99_5", "=", "np", ".", "percentile", "(", "voxels", ",", "99.5", ")", "\n", "percentile_00_5", "=", "np", ".", "percentile", "(", "voxels", ",", "00.5", ")", "\n", "return", "median", ",", "mean", ",", "sd", ",", "mn", ",", "mx", ",", "percentile_99_5", ",", "percentile_00_5", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.experiment_planning.DatasetAnalyzer.DatasetAnalyzer.collect_intensity_properties": [[189, 232], ["multiprocessing.Pool", "collections.OrderedDict", "range", "multiprocessing.Pool.close", "multiprocessing.Pool.join", "save_pickle", "load_pickle", "isfile", "collections.OrderedDict", "multiprocessing.Pool.starmap", "DatasetAnalyzer.DatasetAnalyzer._compute_stats", "multiprocessing.Pool.map", "collections.OrderedDict", "enumerate", "zip", "collections.OrderedDict", "len"], "methods", ["home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.experiment_planning.DatasetAnalyzer.DatasetAnalyzer._compute_stats"], ["", "def", "collect_intensity_properties", "(", "self", ",", "num_modalities", ")", ":", "\n", "        ", "if", "self", ".", "overwrite", "or", "not", "isfile", "(", "self", ".", "intensityproperties_file", ")", ":", "\n", "            ", "p", "=", "Pool", "(", "self", ".", "num_processes", ")", "\n", "\n", "results", "=", "OrderedDict", "(", ")", "\n", "for", "mod_id", "in", "range", "(", "num_modalities", ")", ":", "\n", "                ", "results", "[", "mod_id", "]", "=", "OrderedDict", "(", ")", "\n", "v", "=", "p", ".", "starmap", "(", "self", ".", "_get_voxels_in_foreground", ",", "zip", "(", "self", ".", "patient_identifiers", ",", "\n", "[", "mod_id", "]", "*", "len", "(", "self", ".", "patient_identifiers", ")", ")", ")", "\n", "\n", "w", "=", "[", "]", "\n", "for", "iv", "in", "v", ":", "\n", "                    ", "w", "+=", "iv", "\n", "\n", "", "median", ",", "mean", ",", "sd", ",", "mn", ",", "mx", ",", "percentile_99_5", ",", "percentile_00_5", "=", "self", ".", "_compute_stats", "(", "w", ")", "\n", "\n", "local_props", "=", "p", ".", "map", "(", "self", ".", "_compute_stats", ",", "v", ")", "\n", "props_per_case", "=", "OrderedDict", "(", ")", "\n", "for", "i", ",", "pat", "in", "enumerate", "(", "self", ".", "patient_identifiers", ")", ":", "\n", "                    ", "props_per_case", "[", "pat", "]", "=", "OrderedDict", "(", ")", "\n", "props_per_case", "[", "pat", "]", "[", "'median'", "]", "=", "local_props", "[", "i", "]", "[", "0", "]", "\n", "props_per_case", "[", "pat", "]", "[", "'mean'", "]", "=", "local_props", "[", "i", "]", "[", "1", "]", "\n", "props_per_case", "[", "pat", "]", "[", "'sd'", "]", "=", "local_props", "[", "i", "]", "[", "2", "]", "\n", "props_per_case", "[", "pat", "]", "[", "'mn'", "]", "=", "local_props", "[", "i", "]", "[", "3", "]", "\n", "props_per_case", "[", "pat", "]", "[", "'mx'", "]", "=", "local_props", "[", "i", "]", "[", "4", "]", "\n", "props_per_case", "[", "pat", "]", "[", "'percentile_99_5'", "]", "=", "local_props", "[", "i", "]", "[", "5", "]", "\n", "props_per_case", "[", "pat", "]", "[", "'percentile_00_5'", "]", "=", "local_props", "[", "i", "]", "[", "6", "]", "\n", "\n", "", "results", "[", "mod_id", "]", "[", "'local_props'", "]", "=", "props_per_case", "\n", "results", "[", "mod_id", "]", "[", "'median'", "]", "=", "median", "\n", "results", "[", "mod_id", "]", "[", "'mean'", "]", "=", "mean", "\n", "results", "[", "mod_id", "]", "[", "'sd'", "]", "=", "sd", "\n", "results", "[", "mod_id", "]", "[", "'mn'", "]", "=", "mn", "\n", "results", "[", "mod_id", "]", "[", "'mx'", "]", "=", "mx", "\n", "results", "[", "mod_id", "]", "[", "'percentile_99_5'", "]", "=", "percentile_99_5", "\n", "results", "[", "mod_id", "]", "[", "'percentile_00_5'", "]", "=", "percentile_00_5", "\n", "\n", "", "p", ".", "close", "(", ")", "\n", "p", ".", "join", "(", ")", "\n", "save_pickle", "(", "results", ",", "self", ".", "intensityproperties_file", ")", "\n", "", "else", ":", "\n", "            ", "results", "=", "load_pickle", "(", "self", ".", "intensityproperties_file", ")", "\n", "", "return", "results", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.experiment_planning.DatasetAnalyzer.DatasetAnalyzer.analyze_dataset": [[233, 270], ["DatasetAnalyzer.DatasetAnalyzer.get_sizes_and_spacings_after_cropping", "DatasetAnalyzer.DatasetAnalyzer.get_classes", "DatasetAnalyzer.DatasetAnalyzer.get_modalities", "DatasetAnalyzer.DatasetAnalyzer.get_size_reduction_by_cropping", "dict", "save_pickle", "int", "DatasetAnalyzer.DatasetAnalyzer.collect_intensity_properties", "join", "DatasetAnalyzer.DatasetAnalyzer.keys", "len", "int"], "methods", ["home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.experiment_planning.DatasetAnalyzer.DatasetAnalyzer.get_sizes_and_spacings_after_cropping", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.experiment_planning.DatasetAnalyzer.DatasetAnalyzer.get_classes", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.experiment_planning.DatasetAnalyzer.DatasetAnalyzer.get_modalities", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.experiment_planning.DatasetAnalyzer.DatasetAnalyzer.get_size_reduction_by_cropping", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.experiment_planning.DatasetAnalyzer.DatasetAnalyzer.collect_intensity_properties"], ["", "def", "analyze_dataset", "(", "self", ",", "collect_intensityproperties", "=", "True", ")", ":", "\n", "# get all spacings and sizes", "\n", "        ", "sizes", ",", "spacings", "=", "self", ".", "get_sizes_and_spacings_after_cropping", "(", ")", "\n", "\n", "# get all classes and what classes are in what patients", "\n", "# class min size", "\n", "# region size per class", "\n", "#class_dct, segmentation_props_per_patient = self.analyse_segmentations()", "\n", "#all_classes = np.array([int(i) for i in class_dct.keys()])", "\n", "#all_classes = all_classes[all_classes > 0]", "\n", "classes", "=", "self", ".", "get_classes", "(", ")", "\n", "all_classes", "=", "[", "int", "(", "i", ")", "for", "i", "in", "classes", ".", "keys", "(", ")", "if", "int", "(", "i", ")", ">", "0", "]", "\n", "\n", "# modalities", "\n", "modalities", "=", "self", ".", "get_modalities", "(", ")", "\n", "\n", "# collect intensity information", "\n", "if", "collect_intensityproperties", ":", "\n", "            ", "intensityproperties", "=", "self", ".", "collect_intensity_properties", "(", "len", "(", "modalities", ")", ")", "\n", "", "else", ":", "\n", "            ", "intensityproperties", "=", "None", "\n", "\n", "# size reduction by cropping", "\n", "", "size_reductions", "=", "self", ".", "get_size_reduction_by_cropping", "(", ")", "\n", "\n", "dataset_properties", "=", "dict", "(", ")", "\n", "dataset_properties", "[", "'all_sizes'", "]", "=", "sizes", "\n", "dataset_properties", "[", "'all_spacings'", "]", "=", "spacings", "\n", "#dataset_properties['segmentation_props_per_patient'] = segmentation_props_per_patient", "\n", "#dataset_properties['class_dct'] = class_dct  # {int: class name}", "\n", "dataset_properties", "[", "'all_classes'", "]", "=", "all_classes", "\n", "dataset_properties", "[", "'modalities'", "]", "=", "modalities", "# {idx: modality name}", "\n", "dataset_properties", "[", "'intensityproperties'", "]", "=", "intensityproperties", "\n", "dataset_properties", "[", "'size_reductions'", "]", "=", "size_reductions", "# {patient_id: size_reduction}", "\n", "\n", "save_pickle", "(", "dataset_properties", ",", "join", "(", "self", ".", "folder_with_cropped_data", ",", "\"dataset_properties.pkl\"", ")", ")", "\n", "return", "dataset_properties", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.experiment_planning.experiment_planner_baseline_3DUNet_v21.ExperimentPlanner3D_v21.__init__": [[31, 37], ["nnunet.experiment_planning.experiment_planner_baseline_3DUNet.ExperimentPlanner.__init__", "join"], "methods", ["home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.common.layers.ResizeImage.__init__"], ["def", "__init__", "(", "self", ",", "folder_with_cropped_data", ",", "preprocessed_output_folder", ")", ":", "\n", "        ", "super", "(", "ExperimentPlanner3D_v21", ",", "self", ")", ".", "__init__", "(", "folder_with_cropped_data", ",", "preprocessed_output_folder", ")", "\n", "self", ".", "data_identifier", "=", "\"nnUNetData_plans_v2.1\"", "\n", "self", ".", "plans_fname", "=", "join", "(", "self", ".", "preprocessed_output_folder", ",", "\n", "\"nnUNetPlansv2.1_plans_3D.pkl\"", ")", "\n", "self", ".", "unet_base_num_features", "=", "32", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.experiment_planning.experiment_planner_baseline_3DUNet_v21.ExperimentPlanner3D_v21.get_target_spacing": [[38, 82], ["numpy.percentile", "numpy.percentile", "numpy.argmax", "numpy.vstack", "numpy.vstack", "numpy.array", "numpy.array", "min", "numpy.percentile", "range", "min", "numpy.vstack", "min", "len", "max", "min"], "methods", ["None"], ["", "def", "get_target_spacing", "(", "self", ")", ":", "\n", "        ", "\"\"\"\n        per default we use the 50th percentile=median for the target spacing. Higher spacing results in smaller data\n        and thus faster and easier training. Smaller spacing results in larger data and thus longer and harder training\n\n        For some datasets the median is not a good choice. Those are the datasets where the spacing is very anisotropic\n        (for example ACDC with (10, 1.5, 1.5)). These datasets still have examples with a spacing of 5 or 6 mm in the low\n        resolution axis. Choosing the median here will result in bad interpolation artifacts that can substantially\n        impact performance (due to the low number of slices).\n        \"\"\"", "\n", "spacings", "=", "self", ".", "dataset_properties", "[", "'all_spacings'", "]", "\n", "sizes", "=", "self", ".", "dataset_properties", "[", "'all_sizes'", "]", "\n", "\n", "target", "=", "np", ".", "percentile", "(", "np", ".", "vstack", "(", "spacings", ")", ",", "self", ".", "target_spacing_percentile", ",", "0", ")", "\n", "\n", "# This should be used to determine the new median shape. The old implementation is not 100% correct.", "\n", "# Fixed in 2.4", "\n", "# sizes = [np.array(i) / target * np.array(j) for i, j in zip(spacings, sizes)]", "\n", "\n", "target_size", "=", "np", ".", "percentile", "(", "np", ".", "vstack", "(", "sizes", ")", ",", "self", ".", "target_spacing_percentile", ",", "0", ")", "\n", "target_size_mm", "=", "np", ".", "array", "(", "target", ")", "*", "np", ".", "array", "(", "target_size", ")", "\n", "# we need to identify datasets for which a different target spacing could be beneficial. These datasets have", "\n", "# the following properties:", "\n", "# - one axis which much lower resolution than the others", "\n", "# - the lowres axis has much less voxels than the others", "\n", "# - (the size in mm of the lowres axis is also reduced)", "\n", "worst_spacing_axis", "=", "np", ".", "argmax", "(", "target", ")", "\n", "other_axes", "=", "[", "i", "for", "i", "in", "range", "(", "len", "(", "target", ")", ")", "if", "i", "!=", "worst_spacing_axis", "]", "\n", "other_spacings", "=", "[", "target", "[", "i", "]", "for", "i", "in", "other_axes", "]", "\n", "other_sizes", "=", "[", "target_size", "[", "i", "]", "for", "i", "in", "other_axes", "]", "\n", "\n", "has_aniso_spacing", "=", "target", "[", "worst_spacing_axis", "]", ">", "(", "self", ".", "anisotropy_threshold", "*", "min", "(", "other_spacings", ")", ")", "\n", "has_aniso_voxels", "=", "target_size", "[", "worst_spacing_axis", "]", "*", "self", ".", "anisotropy_threshold", "<", "min", "(", "other_sizes", ")", "\n", "# we don't use the last one for now", "\n", "#median_size_in_mm = target[target_size_mm] * RESAMPLING_SEPARATE_Z_ANISOTROPY_THRESHOLD < max(target_size_mm)", "\n", "\n", "if", "has_aniso_spacing", "and", "has_aniso_voxels", ":", "\n", "            ", "spacings_of_that_axis", "=", "np", ".", "vstack", "(", "spacings", ")", "[", ":", ",", "worst_spacing_axis", "]", "\n", "target_spacing_of_that_axis", "=", "np", ".", "percentile", "(", "spacings_of_that_axis", ",", "10", ")", "\n", "# don't let the spacing of that axis get higher than the other axes", "\n", "if", "target_spacing_of_that_axis", "<", "min", "(", "other_spacings", ")", ":", "\n", "                ", "target_spacing_of_that_axis", "=", "max", "(", "min", "(", "other_spacings", ")", ",", "target_spacing_of_that_axis", ")", "+", "1e-5", "\n", "", "target", "[", "worst_spacing_axis", "]", "=", "target_spacing_of_that_axis", "\n", "", "return", "target", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.experiment_planning.experiment_planner_baseline_3DUNet_v21.ExperimentPlanner3D_v21.get_properties_for_stage": [[83, 180], ["numpy.round().astype", "numpy.round().astype.mean", "numpy.round().astype", "nnunet.experiment_planning.common_utils.get_pool_and_conv_props", "nnunet.network_architecture.generic_UNet.Generic_UNet.compute_approx_vram_consumption", "int", "numpy.round().astype", "max", "min", "numpy.prod", "numpy.array", "min", "copy.deepcopy", "nnunet.experiment_planning.common_utils.get_pool_and_conv_props", "nnunet.experiment_planning.common_utils.get_pool_and_conv_props", "nnunet.network_architecture.generic_UNet.Generic_UNet.compute_approx_vram_consumption", "numpy.floor", "numpy.round", "min", "numpy.round", "zip", "numpy.argsort", "numpy.round", "max", "max", "numpy.prod"], "methods", ["home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.experiment_planning.common_utils.get_pool_and_conv_props", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.network_architecture.generic_modular_UNet.PlainConvUNet.compute_approx_vram_consumption", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.experiment_planning.common_utils.get_pool_and_conv_props", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.experiment_planning.common_utils.get_pool_and_conv_props", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.network_architecture.generic_modular_UNet.PlainConvUNet.compute_approx_vram_consumption"], ["", "def", "get_properties_for_stage", "(", "self", ",", "current_spacing", ",", "original_spacing", ",", "original_shape", ",", "num_cases", ",", "\n", "num_modalities", ",", "num_classes", ")", ":", "\n", "        ", "\"\"\"\n        ExperimentPlanner configures pooling so that we pool late. Meaning that if the number of pooling per axis is\n        (2, 3, 3), then the first pooling operation will always pool axes 1 and 2 and not 0, irrespective of spacing.\n        This can cause a larger memory footprint, so it can be beneficial to revise this.\n\n        Here we are pooling based on the spacing of the data.\n\n        \"\"\"", "\n", "new_median_shape", "=", "np", ".", "round", "(", "original_spacing", "/", "current_spacing", "*", "original_shape", ")", ".", "astype", "(", "int", ")", "\n", "dataset_num_voxels", "=", "np", ".", "prod", "(", "new_median_shape", ")", "*", "num_cases", "\n", "\n", "# the next line is what we had before as a default. The patch size had the same aspect ratio as the median shape of a patient. We swapped t", "\n", "# input_patch_size = new_median_shape", "\n", "\n", "# compute how many voxels are one mm", "\n", "input_patch_size", "=", "1", "/", "np", ".", "array", "(", "current_spacing", ")", "\n", "\n", "# normalize voxels per mm", "\n", "input_patch_size", "/=", "input_patch_size", ".", "mean", "(", ")", "\n", "\n", "# create an isotropic patch of size 512x512x512mm", "\n", "input_patch_size", "*=", "1", "/", "min", "(", "input_patch_size", ")", "*", "512", "# to get a starting value", "\n", "input_patch_size", "=", "np", ".", "round", "(", "input_patch_size", ")", ".", "astype", "(", "int", ")", "\n", "\n", "# clip it to the median shape of the dataset because patches larger then that make not much sense", "\n", "input_patch_size", "=", "[", "min", "(", "i", ",", "j", ")", "for", "i", ",", "j", "in", "zip", "(", "input_patch_size", ",", "new_median_shape", ")", "]", "\n", "\n", "network_num_pool_per_axis", ",", "pool_op_kernel_sizes", ",", "conv_kernel_sizes", ",", "new_shp", ",", "shape_must_be_divisible_by", "=", "get_pool_and_conv_props", "(", "current_spacing", ",", "input_patch_size", ",", "\n", "self", ".", "unet_featuremap_min_edge_length", ",", "\n", "self", ".", "unet_max_numpool", ")", "\n", "\n", "# we compute as if we were using only 30 feature maps. We can do that because fp16 training is the standard", "\n", "# now. That frees up some space. The decision to go with 32 is solely due to the speedup we get (non-multiples", "\n", "# of 8 are not supported in nvidia amp)", "\n", "ref", "=", "Generic_UNet", ".", "use_this_for_batch_size_computation_3D", "*", "self", ".", "unet_base_num_features", "/", "Generic_UNet", ".", "BASE_NUM_FEATURES_3D", "\n", "here", "=", "Generic_UNet", ".", "compute_approx_vram_consumption", "(", "new_shp", ",", "network_num_pool_per_axis", ",", "\n", "self", ".", "unet_base_num_features", ",", "\n", "self", ".", "unet_max_num_filters", ",", "num_modalities", ",", "\n", "num_classes", ",", "\n", "pool_op_kernel_sizes", ",", "conv_per_stage", "=", "self", ".", "conv_per_stage", ")", "\n", "while", "here", ">", "ref", ":", "\n", "            ", "axis_to_be_reduced", "=", "np", ".", "argsort", "(", "new_shp", "/", "new_median_shape", ")", "[", "-", "1", "]", "\n", "\n", "tmp", "=", "deepcopy", "(", "new_shp", ")", "\n", "tmp", "[", "axis_to_be_reduced", "]", "-=", "shape_must_be_divisible_by", "[", "axis_to_be_reduced", "]", "\n", "_", ",", "_", ",", "_", ",", "_", ",", "shape_must_be_divisible_by_new", "=", "get_pool_and_conv_props", "(", "current_spacing", ",", "tmp", ",", "\n", "self", ".", "unet_featuremap_min_edge_length", ",", "\n", "self", ".", "unet_max_numpool", ",", "\n", ")", "\n", "new_shp", "[", "axis_to_be_reduced", "]", "-=", "shape_must_be_divisible_by_new", "[", "axis_to_be_reduced", "]", "\n", "\n", "# we have to recompute numpool now:", "\n", "network_num_pool_per_axis", ",", "pool_op_kernel_sizes", ",", "conv_kernel_sizes", ",", "new_shp", ",", "shape_must_be_divisible_by", "=", "get_pool_and_conv_props", "(", "current_spacing", ",", "new_shp", ",", "\n", "self", ".", "unet_featuremap_min_edge_length", ",", "\n", "self", ".", "unet_max_numpool", ",", "\n", ")", "\n", "\n", "here", "=", "Generic_UNet", ".", "compute_approx_vram_consumption", "(", "new_shp", ",", "network_num_pool_per_axis", ",", "\n", "self", ".", "unet_base_num_features", ",", "\n", "self", ".", "unet_max_num_filters", ",", "num_modalities", ",", "\n", "num_classes", ",", "pool_op_kernel_sizes", ",", "\n", "conv_per_stage", "=", "self", ".", "conv_per_stage", ")", "\n", "#print(new_shp)", "\n", "#print(here, ref)", "\n", "\n", "", "input_patch_size", "=", "new_shp", "\n", "\n", "batch_size", "=", "Generic_UNet", ".", "DEFAULT_BATCH_SIZE_3D", "# This is what wirks with 128**3", "\n", "batch_size", "=", "int", "(", "np", ".", "floor", "(", "max", "(", "ref", "/", "here", ",", "1", ")", "*", "batch_size", ")", ")", "\n", "\n", "# check if batch size is too large", "\n", "max_batch_size", "=", "np", ".", "round", "(", "self", ".", "batch_size_covers_max_percent_of_dataset", "*", "dataset_num_voxels", "/", "\n", "np", ".", "prod", "(", "input_patch_size", ",", "dtype", "=", "np", ".", "int64", ")", ")", ".", "astype", "(", "int", ")", "\n", "max_batch_size", "=", "max", "(", "max_batch_size", ",", "self", ".", "unet_min_batch_size", ")", "\n", "batch_size", "=", "min", "(", "batch_size", ",", "max_batch_size", ")", "\n", "\n", "do_dummy_2D_data_aug", "=", "(", "max", "(", "input_patch_size", ")", "/", "input_patch_size", "[", "\n", "0", "]", ")", ">", "self", ".", "anisotropy_threshold", "\n", "\n", "plan", "=", "{", "\n", "'batch_size'", ":", "batch_size", ",", "\n", "'num_pool_per_axis'", ":", "network_num_pool_per_axis", ",", "\n", "'patch_size'", ":", "input_patch_size", ",", "\n", "'median_patient_size_in_voxels'", ":", "new_median_shape", ",", "\n", "'current_spacing'", ":", "current_spacing", ",", "\n", "'original_spacing'", ":", "original_spacing", ",", "\n", "'do_dummy_2D_data_aug'", ":", "do_dummy_2D_data_aug", ",", "\n", "'pool_op_kernel_sizes'", ":", "pool_op_kernel_sizes", ",", "\n", "'conv_kernel_sizes'", ":", "conv_kernel_sizes", ",", "\n", "}", "\n", "return", "plan", "\n", "", "", ""]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.experiment_planning.summarize_plans.summarize_plans": [[20, 35], ["load_pickle", "print", "print", "print", "print", "print", "print", "print", "print", "range", "len", "print", "print", "print"], "function", ["None"], ["def", "summarize_plans", "(", "file", ")", ":", "\n", "    ", "plans", "=", "load_pickle", "(", "file", ")", "\n", "print", "(", "\"num_classes: \"", ",", "plans", "[", "'num_classes'", "]", ")", "\n", "print", "(", "\"modalities: \"", ",", "plans", "[", "'modalities'", "]", ")", "\n", "print", "(", "\"use_mask_for_norm\"", ",", "plans", "[", "'use_mask_for_norm'", "]", ")", "\n", "print", "(", "\"keep_only_largest_region\"", ",", "plans", "[", "'keep_only_largest_region'", "]", ")", "\n", "print", "(", "\"min_region_size_per_class\"", ",", "plans", "[", "'min_region_size_per_class'", "]", ")", "\n", "print", "(", "\"min_size_per_class\"", ",", "plans", "[", "'min_size_per_class'", "]", ")", "\n", "print", "(", "\"normalization_schemes\"", ",", "plans", "[", "'normalization_schemes'", "]", ")", "\n", "print", "(", "\"stages...\\n\"", ")", "\n", "\n", "for", "i", "in", "range", "(", "len", "(", "plans", "[", "'plans_per_stage'", "]", ")", ")", ":", "\n", "        ", "print", "(", "\"stage: \"", ",", "i", ")", "\n", "print", "(", "plans", "[", "'plans_per_stage'", "]", "[", "i", "]", ")", "\n", "print", "(", "\"\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.experiment_planning.summarize_plans.write_plans_to_file": [[37, 62], ["print", "load_pickle", "list", "list.sort", "a[].keys", "f.write", "f.write", "f.write", "f.write", "f.write", "f.write", "f.write", "f.write", "f.write", "f.write", "f.write", "f.write", "f.write", "f.write", "f.write", "zip", "zip", "plans_file.split", "str", "str", "str", "str", "str", "str", "str", "str", "str", "str", "str", "plans_file.split", "str", "str", "str", "str"], "function", ["None"], ["", "", "def", "write_plans_to_file", "(", "f", ",", "plans_file", ")", ":", "\n", "    ", "print", "(", "plans_file", ")", "\n", "a", "=", "load_pickle", "(", "plans_file", ")", "\n", "stages", "=", "list", "(", "a", "[", "'plans_per_stage'", "]", ".", "keys", "(", ")", ")", "\n", "stages", ".", "sort", "(", ")", "\n", "for", "stage", "in", "stages", ":", "\n", "        ", "patch_size_in_mm", "=", "[", "i", "*", "j", "for", "i", ",", "j", "in", "zip", "(", "a", "[", "'plans_per_stage'", "]", "[", "stages", "[", "stage", "]", "]", "[", "'patch_size'", "]", ",", "\n", "a", "[", "'plans_per_stage'", "]", "[", "stages", "[", "stage", "]", "]", "[", "'current_spacing'", "]", ")", "]", "\n", "median_patient_size_in_mm", "=", "[", "i", "*", "j", "for", "i", ",", "j", "in", "zip", "(", "a", "[", "'plans_per_stage'", "]", "[", "stages", "[", "stage", "]", "]", "[", "'median_patient_size_in_voxels'", "]", ",", "\n", "a", "[", "'plans_per_stage'", "]", "[", "stages", "[", "stage", "]", "]", "[", "'current_spacing'", "]", ")", "]", "\n", "f", ".", "write", "(", "plans_file", ".", "split", "(", "\"/\"", ")", "[", "-", "2", "]", ")", "\n", "f", ".", "write", "(", "\";%s\"", "%", "plans_file", ".", "split", "(", "\"/\"", ")", "[", "-", "1", "]", ")", "\n", "f", ".", "write", "(", "\";%d\"", "%", "stage", ")", "\n", "f", ".", "write", "(", "\";%s\"", "%", "str", "(", "a", "[", "'plans_per_stage'", "]", "[", "stages", "[", "stage", "]", "]", "[", "'batch_size'", "]", ")", ")", "\n", "f", ".", "write", "(", "\";%s\"", "%", "str", "(", "a", "[", "'plans_per_stage'", "]", "[", "stages", "[", "stage", "]", "]", "[", "'num_pool_per_axis'", "]", ")", ")", "\n", "f", ".", "write", "(", "\";%s\"", "%", "str", "(", "a", "[", "'plans_per_stage'", "]", "[", "stages", "[", "stage", "]", "]", "[", "'patch_size'", "]", ")", ")", "\n", "f", ".", "write", "(", "\";%s\"", "%", "str", "(", "[", "str", "(", "\"%03.2f\"", "%", "i", ")", "for", "i", "in", "patch_size_in_mm", "]", ")", ")", "\n", "f", ".", "write", "(", "\";%s\"", "%", "str", "(", "a", "[", "'plans_per_stage'", "]", "[", "stages", "[", "stage", "]", "]", "[", "'median_patient_size_in_voxels'", "]", ")", ")", "\n", "f", ".", "write", "(", "\";%s\"", "%", "str", "(", "[", "str", "(", "\"%03.2f\"", "%", "i", ")", "for", "i", "in", "median_patient_size_in_mm", "]", ")", ")", "\n", "f", ".", "write", "(", "\";%s\"", "%", "str", "(", "[", "str", "(", "\"%03.2f\"", "%", "i", ")", "for", "i", "in", "a", "[", "'plans_per_stage'", "]", "[", "stages", "[", "stage", "]", "]", "[", "'current_spacing'", "]", "]", ")", ")", "\n", "f", ".", "write", "(", "\";%s\"", "%", "str", "(", "[", "str", "(", "\"%03.2f\"", "%", "i", ")", "for", "i", "in", "a", "[", "'plans_per_stage'", "]", "[", "stages", "[", "stage", "]", "]", "[", "'original_spacing'", "]", "]", ")", ")", "\n", "f", ".", "write", "(", "\";%s\"", "%", "str", "(", "a", "[", "'plans_per_stage'", "]", "[", "stages", "[", "stage", "]", "]", "[", "'pool_op_kernel_sizes'", "]", ")", ")", "\n", "f", ".", "write", "(", "\";%s\"", "%", "str", "(", "a", "[", "'plans_per_stage'", "]", "[", "stages", "[", "stage", "]", "]", "[", "'conv_kernel_sizes'", "]", ")", ")", "\n", "f", ".", "write", "(", "\";%s\"", "%", "str", "(", "a", "[", "'data_identifier'", "]", ")", ")", "\n", "f", ".", "write", "(", "\"\\n\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.experiment_planning.nnUNet_plan_and_preprocess.main": [[27, 134], ["argparse.ArgumentParser", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.parse_args", "join", "int", "nnunet.utilities.task_name_id_conversion.convert_id_to_task_name", "nnunet.experiment_planning.utils.crop", "tasks.append", "nnunet.training.model_restore.recursive_find_python_class", "nnunet.training.model_restore.recursive_find_python_class", "print", "os.path.join", "os.path.join", "load_json", "list", "nnunet.experiment_planning.DatasetAnalyzer.DatasetAnalyzer", "nnunet.experiment_planning.DatasetAnalyzer.DatasetAnalyzer.analyze_dataset", "maybe_mkdir_p", "shutil.copy", "shutil.copy", "print", "nnunet.preprocessing.sanity_checks.verify_dataset_integrity", "RuntimeError", "RuntimeError", "join", "dataset_json[].values", "join", "join", "nnunet.training.model_restore.recursive_find_python_class.", "planner_2d.plan_experiment", "nnunet.training.model_restore.recursive_find_python_class.", "planner_2d.plan_experiment", "join", "planner_2d.run_preprocessing", "planner_2d.run_preprocessing"], "function", ["home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.utilities.task_name_id_conversion.convert_id_to_task_name", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.preprocessing.cropping.ImageCropper.crop", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.training.model_restore.recursive_find_python_class", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.training.model_restore.recursive_find_python_class", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.experiment_planning.utils.analyze_dataset", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.preprocessing.sanity_checks.verify_dataset_integrity", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.experiment_planning.experiment_planner_baseline_2DUNet.ExperimentPlanner2D.plan_experiment", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.experiment_planning.experiment_planner_baseline_2DUNet.ExperimentPlanner2D.plan_experiment", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.pooling_and_convs.experiment_planner_baseline_3DUNet_allConv3x3.ExperimentPlannerAllConv3x3.run_preprocessing", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.pooling_and_convs.experiment_planner_baseline_3DUNet_allConv3x3.ExperimentPlannerAllConv3x3.run_preprocessing"], ["def", "main", "(", ")", ":", "\n", "    ", "import", "argparse", "\n", "\n", "parser", "=", "argparse", ".", "ArgumentParser", "(", ")", "\n", "parser", ".", "add_argument", "(", "\"-t\"", ",", "\"--task_ids\"", ",", "nargs", "=", "\"+\"", ",", "help", "=", "\"List of integers belonging to the task ids you wish to run\"", "\n", "\" experiment planning and preprocessing for. Each of these \"", "\n", "\"ids must, have a matching folder 'TaskXXX_' in the raw \"", "\n", "\"data folder\"", ")", "\n", "parser", ".", "add_argument", "(", "\"-pl3d\"", ",", "\"--planner3d\"", ",", "type", "=", "str", ",", "default", "=", "\"ExperimentPlanner3D_v21\"", ",", "\n", "help", "=", "\"Name of the ExperimentPlanner class for the full resolution 3D U-Net and U-Net cascade. \"", "\n", "\"Default is ExperimentPlanner3D_v21. Can be 'None', in which case these U-Nets will not be \"", "\n", "\"configured\"", ")", "\n", "parser", ".", "add_argument", "(", "\"-pl2d\"", ",", "\"--planner2d\"", ",", "type", "=", "str", ",", "default", "=", "\"ExperimentPlanner2D_v21\"", ",", "\n", "help", "=", "\"Name of the ExperimentPlanner class for the 2D U-Net. Default is ExperimentPlanner2D_v21. \"", "\n", "\"Can be 'None', in which case this U-Net will not be configured\"", ")", "\n", "parser", ".", "add_argument", "(", "\"-no_pp\"", ",", "action", "=", "\"store_true\"", ",", "\n", "help", "=", "\"Set this flag if you dont want to run the preprocessing. If this is set then this script \"", "\n", "\"will only run the experiment planning and create the plans file\"", ")", "\n", "parser", ".", "add_argument", "(", "\"-tl\"", ",", "type", "=", "int", ",", "required", "=", "False", ",", "default", "=", "8", ",", "\n", "help", "=", "\"Number of processes used for preprocessing the low resolution data for the 3D low \"", "\n", "\"resolution U-Net. This can be larger than -tf. Don't overdo it or you will run out of \"", "\n", "\"RAM\"", ")", "\n", "parser", ".", "add_argument", "(", "\"-tf\"", ",", "type", "=", "int", ",", "required", "=", "False", ",", "default", "=", "8", ",", "\n", "help", "=", "\"Number of processes used for preprocessing the full resolution data of the 2D U-Net and \"", "\n", "\"3D U-Net. Don't overdo it or you will run out of RAM\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--verify_dataset_integrity\"", ",", "required", "=", "False", ",", "default", "=", "False", ",", "action", "=", "\"store_true\"", ",", "\n", "help", "=", "\"set this flag to check the dataset integrity. This is useful and should be done once for \"", "\n", "\"each dataset!\"", ")", "\n", "\n", "args", "=", "parser", ".", "parse_args", "(", ")", "\n", "task_ids", "=", "args", ".", "task_ids", "\n", "dont_run_preprocessing", "=", "args", ".", "no_pp", "\n", "tl", "=", "args", ".", "tl", "\n", "tf", "=", "args", ".", "tf", "\n", "planner_name3d", "=", "args", ".", "planner3d", "\n", "planner_name2d", "=", "args", ".", "planner2d", "\n", "\n", "if", "planner_name3d", "==", "\"None\"", ":", "\n", "        ", "planner_name3d", "=", "None", "\n", "", "if", "planner_name2d", "==", "\"None\"", ":", "\n", "        ", "planner_name2d", "=", "None", "\n", "\n", "# we need raw data", "\n", "", "tasks", "=", "[", "]", "\n", "for", "i", "in", "task_ids", ":", "\n", "        ", "i", "=", "int", "(", "i", ")", "\n", "\n", "task_name", "=", "convert_id_to_task_name", "(", "i", ")", "\n", "\n", "if", "args", ".", "verify_dataset_integrity", ":", "\n", "            ", "verify_dataset_integrity", "(", "join", "(", "nnUNet_raw_data", ",", "task_name", ")", ")", "\n", "\n", "", "crop", "(", "task_name", ",", "False", ",", "tf", ")", "\n", "\n", "tasks", ".", "append", "(", "task_name", ")", "\n", "\n", "", "search_in", "=", "join", "(", "nnunet", ".", "__path__", "[", "0", "]", ",", "\"experiment_planning\"", ")", "\n", "\n", "if", "planner_name3d", "is", "not", "None", ":", "\n", "        ", "planner_3d", "=", "recursive_find_python_class", "(", "[", "search_in", "]", ",", "planner_name3d", ",", "current_module", "=", "\"nnunet.experiment_planning\"", ")", "\n", "if", "planner_3d", "is", "None", ":", "\n", "            ", "raise", "RuntimeError", "(", "\"Could not find the Planner class %s. Make sure it is located somewhere in \"", "\n", "\"nnunet.experiment_planning\"", "%", "planner_name3d", ")", "\n", "", "", "else", ":", "\n", "        ", "planner_3d", "=", "None", "\n", "\n", "", "if", "planner_name2d", "is", "not", "None", ":", "\n", "        ", "planner_2d", "=", "recursive_find_python_class", "(", "[", "search_in", "]", ",", "planner_name2d", ",", "current_module", "=", "\"nnunet.experiment_planning\"", ")", "\n", "if", "planner_2d", "is", "None", ":", "\n", "            ", "raise", "RuntimeError", "(", "\"Could not find the Planner class %s. Make sure it is located somewhere in \"", "\n", "\"nnunet.experiment_planning\"", "%", "planner_name2d", ")", "\n", "", "", "else", ":", "\n", "        ", "planner_2d", "=", "None", "\n", "\n", "", "for", "t", "in", "tasks", ":", "\n", "        ", "print", "(", "\"\\n\\n\\n\"", ",", "t", ")", "\n", "cropped_out_dir", "=", "os", ".", "path", ".", "join", "(", "nnUNet_cropped_data", ",", "t", ")", "\n", "preprocessing_output_dir_this_task", "=", "os", ".", "path", ".", "join", "(", "preprocessing_output_dir", ",", "t", ")", "\n", "#splitted_4d_output_dir_task = os.path.join(nnUNet_raw_data, t)", "\n", "#lists, modalities = create_lists_from_splitted_dataset(splitted_4d_output_dir_task)", "\n", "\n", "# we need to figure out if we need the intensity propoerties. We collect them only if one of the modalities is CT", "\n", "dataset_json", "=", "load_json", "(", "join", "(", "cropped_out_dir", ",", "'dataset.json'", ")", ")", "\n", "modalities", "=", "list", "(", "dataset_json", "[", "\"modality\"", "]", ".", "values", "(", ")", ")", "\n", "collect_intensityproperties", "=", "True", "if", "(", "(", "\"CT\"", "in", "modalities", ")", "or", "(", "\"ct\"", "in", "modalities", ")", ")", "else", "False", "\n", "dataset_analyzer", "=", "DatasetAnalyzer", "(", "cropped_out_dir", ",", "overwrite", "=", "False", ",", "num_processes", "=", "tf", ")", "# this class creates the fingerprint", "\n", "_", "=", "dataset_analyzer", ".", "analyze_dataset", "(", "collect_intensityproperties", ")", "# this will write output files that will be used by the ExperimentPlanner", "\n", "\n", "\n", "maybe_mkdir_p", "(", "preprocessing_output_dir_this_task", ")", "\n", "shutil", ".", "copy", "(", "join", "(", "cropped_out_dir", ",", "\"dataset_properties.pkl\"", ")", ",", "preprocessing_output_dir_this_task", ")", "\n", "shutil", ".", "copy", "(", "join", "(", "nnUNet_raw_data", ",", "t", ",", "\"dataset.json\"", ")", ",", "preprocessing_output_dir_this_task", ")", "\n", "\n", "threads", "=", "(", "tl", ",", "tf", ")", "\n", "\n", "print", "(", "\"number of threads: \"", ",", "threads", ",", "\"\\n\"", ")", "\n", "\n", "if", "planner_3d", "is", "not", "None", ":", "\n", "            ", "exp_planner", "=", "planner_3d", "(", "cropped_out_dir", ",", "preprocessing_output_dir_this_task", ")", "\n", "exp_planner", ".", "plan_experiment", "(", ")", "\n", "if", "not", "dont_run_preprocessing", ":", "# double negative, yooo", "\n", "                ", "exp_planner", ".", "run_preprocessing", "(", "threads", ")", "\n", "", "", "if", "planner_2d", "is", "not", "None", ":", "\n", "            ", "exp_planner", "=", "planner_2d", "(", "cropped_out_dir", ",", "preprocessing_output_dir_this_task", ")", "\n", "exp_planner", ".", "plan_experiment", "(", ")", "\n", "if", "not", "dont_run_preprocessing", ":", "# double negative, yooo", "\n", "                ", "exp_planner", ".", "run_preprocessing", "(", "threads", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.experiment_planning.experiment_planner_baseline_2DUNet.ExperimentPlanner2D.__init__": [[33, 44], ["nnunet.experiment_planning.experiment_planner_baseline_3DUNet.ExperimentPlanner.__init__", "join"], "methods", ["home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.common.layers.ResizeImage.__init__"], ["    ", "def", "__init__", "(", "self", ",", "folder_with_cropped_data", ",", "preprocessed_output_folder", ")", ":", "\n", "        ", "super", "(", "ExperimentPlanner2D", ",", "self", ")", ".", "__init__", "(", "folder_with_cropped_data", ",", "\n", "preprocessed_output_folder", ")", "\n", "self", ".", "data_identifier", "=", "default_data_identifier", "+", "\"_2D\"", "\n", "self", ".", "plans_fname", "=", "join", "(", "self", ".", "preprocessed_output_folder", ",", "\"nnUNetPlans\"", "+", "\"_plans_2D.pkl\"", ")", "\n", "\n", "self", ".", "unet_base_num_features", "=", "30", "\n", "self", ".", "unet_max_num_filters", "=", "512", "\n", "self", ".", "unet_max_numpool", "=", "999", "\n", "\n", "self", ".", "preprocessor_name", "=", "\"PreprocessorFor2D\"", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.experiment_planning.experiment_planner_baseline_2DUNet.ExperimentPlanner2D.get_properties_for_stage": [[45, 89], ["numpy.round().astype", "nnunet.experiment_planning.common_utils.get_pool_and_conv_props", "nnunet.network_architecture.generic_UNet.Generic_UNet.compute_approx_vram_consumption", "int", "numpy.round().astype", "min", "numpy.prod", "numpy.floor", "RuntimeError", "numpy.round", "numpy.round", "numpy.prod"], "methods", ["home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.experiment_planning.common_utils.get_pool_and_conv_props", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.network_architecture.generic_modular_UNet.PlainConvUNet.compute_approx_vram_consumption"], ["", "def", "get_properties_for_stage", "(", "self", ",", "current_spacing", ",", "original_spacing", ",", "original_shape", ",", "num_cases", ",", "\n", "num_modalities", ",", "num_classes", ")", ":", "\n", "\n", "        ", "new_median_shape", "=", "np", ".", "round", "(", "original_spacing", "/", "current_spacing", "*", "original_shape", ")", ".", "astype", "(", "int", ")", "\n", "\n", "dataset_num_voxels", "=", "np", ".", "prod", "(", "new_median_shape", ",", "dtype", "=", "np", ".", "int64", ")", "*", "num_cases", "\n", "input_patch_size", "=", "new_median_shape", "[", "1", ":", "]", "\n", "\n", "network_numpool", ",", "net_pool_kernel_sizes", ",", "net_conv_kernel_sizes", ",", "input_patch_size", ",", "shape_must_be_divisible_by", "=", "get_pool_and_conv_props", "(", "current_spacing", "[", "1", ":", "]", ",", "input_patch_size", ",", "\n", "self", ".", "unet_featuremap_min_edge_length", ",", "\n", "self", ".", "unet_max_numpool", ")", "\n", "\n", "estimated_gpu_ram_consumption", "=", "Generic_UNet", ".", "compute_approx_vram_consumption", "(", "input_patch_size", ",", "\n", "network_numpool", ",", "\n", "self", ".", "unet_base_num_features", ",", "\n", "self", ".", "unet_max_num_filters", ",", "\n", "num_modalities", ",", "num_classes", ",", "\n", "net_pool_kernel_sizes", ",", "\n", "conv_per_stage", "=", "self", ".", "conv_per_stage", ")", "\n", "\n", "batch_size", "=", "int", "(", "np", ".", "floor", "(", "Generic_UNet", ".", "use_this_for_batch_size_computation_2D", "/", "\n", "estimated_gpu_ram_consumption", "*", "Generic_UNet", ".", "DEFAULT_BATCH_SIZE_2D", ")", ")", "\n", "if", "batch_size", "<", "self", ".", "unet_min_batch_size", ":", "\n", "            ", "raise", "RuntimeError", "(", "\"This framework is not made to process patches this large. We will add patch-based \"", "\n", "\"2D networks later. Sorry for the inconvenience\"", ")", "\n", "\n", "# check if batch size is too large (more than 5 % of dataset)", "\n", "", "max_batch_size", "=", "np", ".", "round", "(", "self", ".", "batch_size_covers_max_percent_of_dataset", "*", "dataset_num_voxels", "/", "\n", "np", ".", "prod", "(", "input_patch_size", ",", "dtype", "=", "np", ".", "int64", ")", ")", ".", "astype", "(", "int", ")", "\n", "batch_size", "=", "min", "(", "batch_size", ",", "max_batch_size", ")", "\n", "\n", "plan", "=", "{", "\n", "'batch_size'", ":", "batch_size", ",", "\n", "'num_pool_per_axis'", ":", "network_numpool", ",", "\n", "'patch_size'", ":", "input_patch_size", ",", "\n", "'median_patient_size_in_voxels'", ":", "new_median_shape", ",", "\n", "'current_spacing'", ":", "current_spacing", ",", "\n", "'original_spacing'", ":", "original_spacing", ",", "\n", "'pool_op_kernel_sizes'", ":", "net_pool_kernel_sizes", ",", "\n", "'conv_kernel_sizes'", ":", "net_conv_kernel_sizes", ",", "\n", "'do_dummy_2D_data_aug'", ":", "False", "\n", "}", "\n", "return", "plan", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.experiment_planning.experiment_planner_baseline_2DUNet.ExperimentPlanner2D.plan_experiment": [[90, 159], ["experiment_planner_baseline_2DUNet.ExperimentPlanner2D.determine_whether_to_use_mask_for_norm", "print", "len", "experiment_planner_baseline_2DUNet.ExperimentPlanner2D.get_target_spacing", "numpy.array", "numpy.argmax", "numpy.median", "print", "numpy.max", "print", "numpy.min", "print", "print", "print", "experiment_planner_baseline_2DUNet.ExperimentPlanner2D.plans_per_stage.append", "print", "experiment_planner_baseline_2DUNet.ExperimentPlanner2D.determine_normalization_scheme", "experiment_planner_baseline_2DUNet.ExperimentPlanner2D.save_my_plans", "list", "numpy.vstack", "numpy.vstack", "numpy.vstack", "numpy.array", "numpy.array", "experiment_planner_baseline_2DUNet.ExperimentPlanner2D.get_properties_for_stage", "len", "len", "modalities.keys", "list", "range", "range", "list", "numpy.array", "zip", "range", "numpy.argwhere", "len", "len", "experiment_planner_baseline_2DUNet.ExperimentPlanner2D.plans_per_stage.keys", "numpy.array", "len", "numpy.array"], "methods", ["home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.experiment_planning.experiment_planner_baseline_3DUNet.ExperimentPlanner.determine_whether_to_use_mask_for_norm", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.target_spacing.experiment_planner_baseline_3DUNet_targetSpacingForAnisoAxis.ExperimentPlannerTargetSpacingForAnisoAxis.get_target_spacing", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.normalization.experiment_planner_3DUNet_nonCT.ExperimentPlannernonCT.determine_normalization_scheme", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.experiment_planning.experiment_planner_baseline_3DUNet.ExperimentPlanner.save_my_plans", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.patch_size.experiment_planner_3DUNet_isotropic_in_voxels.ExperimentPlanner3D_IsoPatchesInVoxels.get_properties_for_stage"], ["", "def", "plan_experiment", "(", "self", ")", ":", "\n", "        ", "use_nonzero_mask_for_normalization", "=", "self", ".", "determine_whether_to_use_mask_for_norm", "(", ")", "\n", "print", "(", "\"Are we using the nonzero maks for normalizaion?\"", ",", "use_nonzero_mask_for_normalization", ")", "\n", "\n", "spacings", "=", "self", ".", "dataset_properties", "[", "'all_spacings'", "]", "\n", "sizes", "=", "self", ".", "dataset_properties", "[", "'all_sizes'", "]", "\n", "all_classes", "=", "self", ".", "dataset_properties", "[", "'all_classes'", "]", "\n", "modalities", "=", "self", ".", "dataset_properties", "[", "'modalities'", "]", "\n", "num_modalities", "=", "len", "(", "list", "(", "modalities", ".", "keys", "(", ")", ")", ")", "\n", "\n", "target_spacing", "=", "self", ".", "get_target_spacing", "(", ")", "\n", "new_shapes", "=", "np", ".", "array", "(", "[", "np", ".", "array", "(", "i", ")", "/", "target_spacing", "*", "np", ".", "array", "(", "j", ")", "for", "i", ",", "j", "in", "zip", "(", "spacings", ",", "sizes", ")", "]", ")", "\n", "\n", "max_spacing_axis", "=", "np", ".", "argmax", "(", "target_spacing", ")", "\n", "remaining_axes", "=", "[", "i", "for", "i", "in", "list", "(", "range", "(", "3", ")", ")", "if", "i", "!=", "max_spacing_axis", "]", "\n", "self", ".", "transpose_forward", "=", "[", "max_spacing_axis", "]", "+", "remaining_axes", "\n", "self", ".", "transpose_backward", "=", "[", "np", ".", "argwhere", "(", "np", ".", "array", "(", "self", ".", "transpose_forward", ")", "==", "i", ")", "[", "0", "]", "[", "0", "]", "for", "i", "in", "range", "(", "3", ")", "]", "\n", "\n", "# we base our calculations on the median shape of the datasets", "\n", "median_shape", "=", "np", ".", "median", "(", "np", ".", "vstack", "(", "new_shapes", ")", ",", "0", ")", "\n", "print", "(", "\"the median shape of the dataset is \"", ",", "median_shape", ")", "\n", "\n", "max_shape", "=", "np", ".", "max", "(", "np", ".", "vstack", "(", "new_shapes", ")", ",", "0", ")", "\n", "print", "(", "\"the max shape in the dataset is \"", ",", "max_shape", ")", "\n", "min_shape", "=", "np", ".", "min", "(", "np", ".", "vstack", "(", "new_shapes", ")", ",", "0", ")", "\n", "print", "(", "\"the min shape in the dataset is \"", ",", "min_shape", ")", "\n", "\n", "print", "(", "\"we don't want feature maps smaller than \"", ",", "self", ".", "unet_featuremap_min_edge_length", ",", "\" in the bottleneck\"", ")", "\n", "\n", "# how many stages will the image pyramid have?", "\n", "self", ".", "plans_per_stage", "=", "[", "]", "\n", "\n", "target_spacing_transposed", "=", "np", ".", "array", "(", "target_spacing", ")", "[", "self", ".", "transpose_forward", "]", "\n", "median_shape_transposed", "=", "np", ".", "array", "(", "median_shape", ")", "[", "self", ".", "transpose_forward", "]", "\n", "print", "(", "\"the transposed median shape of the dataset is \"", ",", "median_shape_transposed", ")", "\n", "\n", "self", ".", "plans_per_stage", ".", "append", "(", "\n", "self", ".", "get_properties_for_stage", "(", "target_spacing_transposed", ",", "target_spacing_transposed", ",", "median_shape_transposed", ",", "\n", "num_cases", "=", "len", "(", "self", ".", "list_of_cropped_npz_files", ")", ",", "\n", "num_modalities", "=", "num_modalities", ",", "\n", "num_classes", "=", "len", "(", "all_classes", ")", "+", "1", ")", ",", "\n", ")", "\n", "\n", "print", "(", "self", ".", "plans_per_stage", ")", "\n", "\n", "self", ".", "plans_per_stage", "=", "self", ".", "plans_per_stage", "[", ":", ":", "-", "1", "]", "\n", "self", ".", "plans_per_stage", "=", "{", "i", ":", "self", ".", "plans_per_stage", "[", "i", "]", "for", "i", "in", "range", "(", "len", "(", "self", ".", "plans_per_stage", ")", ")", "}", "# convert to dict", "\n", "\n", "normalization_schemes", "=", "self", ".", "determine_normalization_scheme", "(", ")", "\n", "# deprecated", "\n", "only_keep_largest_connected_component", ",", "min_size_per_class", ",", "min_region_size_per_class", "=", "None", ",", "None", ",", "None", "\n", "\n", "# these are independent of the stage", "\n", "plans", "=", "{", "'num_stages'", ":", "len", "(", "list", "(", "self", ".", "plans_per_stage", ".", "keys", "(", ")", ")", ")", ",", "'num_modalities'", ":", "num_modalities", ",", "\n", "'modalities'", ":", "modalities", ",", "'normalization_schemes'", ":", "normalization_schemes", ",", "\n", "'dataset_properties'", ":", "self", ".", "dataset_properties", ",", "'list_of_npz_files'", ":", "self", ".", "list_of_cropped_npz_files", ",", "\n", "'original_spacings'", ":", "spacings", ",", "'original_sizes'", ":", "sizes", ",", "\n", "'preprocessed_data_folder'", ":", "self", ".", "preprocessed_output_folder", ",", "'num_classes'", ":", "len", "(", "all_classes", ")", ",", "\n", "'all_classes'", ":", "all_classes", ",", "'base_num_features'", ":", "self", ".", "unet_base_num_features", ",", "\n", "'use_mask_for_norm'", ":", "use_nonzero_mask_for_normalization", ",", "\n", "'keep_only_largest_region'", ":", "only_keep_largest_connected_component", ",", "\n", "'min_region_size_per_class'", ":", "min_region_size_per_class", ",", "'min_size_per_class'", ":", "min_size_per_class", ",", "\n", "'transpose_forward'", ":", "self", ".", "transpose_forward", ",", "'transpose_backward'", ":", "self", ".", "transpose_backward", ",", "\n", "'data_identifier'", ":", "self", ".", "data_identifier", ",", "'plans_per_stage'", ":", "self", ".", "plans_per_stage", ",", "\n", "'preprocessor_name'", ":", "self", ".", "preprocessor_name", ",", "\n", "}", "\n", "\n", "self", ".", "plans", "=", "plans", "\n", "self", ".", "save_my_plans", "(", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.experiment_planning.utils.split_4d": [[31, 80], ["input_folder.endswith", "full_task_name.startswith", "full_task_name.find", "int", "batchgenerators.utilities.file_and_folder_operations.join", "batchgenerators.utilities.file_and_folder_operations.isdir", "batchgenerators.utilities.file_and_folder_operations.maybe_mkdir_p", "shutil.copytree", "multiprocessing.Pool", "multiprocessing.Pool.starmap", "multiprocessing.Pool.close", "multiprocessing.Pool.join", "shutil.copy", "batchgenerators.utilities.file_and_folder_operations.isdir", "batchgenerators.utilities.file_and_folder_operations.isdir", "batchgenerators.utilities.file_and_folder_operations.isfile", "input_folder.split", "shutil.rmtree", "batchgenerators.utilities.file_and_folder_operations.join", "batchgenerators.utilities.file_and_folder_operations.join", "nii_files.sort", "batchgenerators.utilities.file_and_folder_operations.join", "batchgenerators.utilities.file_and_folder_operations.join", "zip", "batchgenerators.utilities.file_and_folder_operations.join", "batchgenerators.utilities.file_and_folder_operations.join", "batchgenerators.utilities.file_and_folder_operations.join", "batchgenerators.utilities.file_and_folder_operations.join", "batchgenerators.utilities.file_and_folder_operations.isdir", "os.mkdir", "batchgenerators.utilities.file_and_folder_operations.join", "files.append", "output_dirs.append", "os.listdir", "i.endswith"], "function", ["None"], ["def", "split_4d", "(", "input_folder", ",", "num_processes", "=", "default_num_threads", ",", "overwrite_task_output_id", "=", "None", ")", ":", "\n", "    ", "assert", "isdir", "(", "join", "(", "input_folder", ",", "\"imagesTr\"", ")", ")", "and", "isdir", "(", "join", "(", "input_folder", ",", "\"labelsTr\"", ")", ")", "and", "isfile", "(", "join", "(", "input_folder", ",", "\"dataset.json\"", ")", ")", ",", "\"The input folder must be a valid Task folder from the Medical Segmentation Decathlon with at least the \"", "\"imagesTr and labelsTr subfolders and the dataset.json file\"", "\n", "\n", "while", "input_folder", ".", "endswith", "(", "\"/\"", ")", ":", "\n", "        ", "input_folder", "=", "input_folder", "[", ":", "-", "1", "]", "\n", "\n", "", "full_task_name", "=", "input_folder", ".", "split", "(", "\"/\"", ")", "[", "-", "1", "]", "\n", "\n", "assert", "full_task_name", ".", "startswith", "(", "\"Task\"", ")", ",", "\"The input folder must point to a folder that starts with TaskXX_\"", "\n", "\n", "first_underscore", "=", "full_task_name", ".", "find", "(", "\"_\"", ")", "\n", "assert", "first_underscore", "==", "6", ",", "\"Input folder start with TaskXX with XX being a 3-digit id: 00, 01, 02 etc\"", "\n", "\n", "input_task_id", "=", "int", "(", "full_task_name", "[", "4", ":", "6", "]", ")", "\n", "if", "overwrite_task_output_id", "is", "None", ":", "\n", "        ", "overwrite_task_output_id", "=", "input_task_id", "\n", "\n", "", "task_name", "=", "full_task_name", "[", "7", ":", "]", "\n", "\n", "output_folder", "=", "join", "(", "nnUNet_raw_data", ",", "\"Task%03.0d_\"", "%", "overwrite_task_output_id", "+", "task_name", ")", "\n", "\n", "if", "isdir", "(", "output_folder", ")", ":", "\n", "        ", "shutil", ".", "rmtree", "(", "output_folder", ")", "\n", "\n", "", "files", "=", "[", "]", "\n", "output_dirs", "=", "[", "]", "\n", "\n", "maybe_mkdir_p", "(", "output_folder", ")", "\n", "for", "subdir", "in", "[", "\"imagesTr\"", ",", "\"imagesTs\"", "]", ":", "\n", "        ", "curr_out_dir", "=", "join", "(", "output_folder", ",", "subdir", ")", "\n", "if", "not", "isdir", "(", "curr_out_dir", ")", ":", "\n", "            ", "os", ".", "mkdir", "(", "curr_out_dir", ")", "\n", "", "curr_dir", "=", "join", "(", "input_folder", ",", "subdir", ")", "\n", "nii_files", "=", "[", "join", "(", "curr_dir", ",", "i", ")", "for", "i", "in", "os", ".", "listdir", "(", "curr_dir", ")", "if", "i", ".", "endswith", "(", "\".nii.gz\"", ")", "]", "\n", "nii_files", ".", "sort", "(", ")", "\n", "for", "n", "in", "nii_files", ":", "\n", "            ", "files", ".", "append", "(", "n", ")", "\n", "output_dirs", ".", "append", "(", "curr_out_dir", ")", "\n", "\n", "", "", "shutil", ".", "copytree", "(", "join", "(", "input_folder", ",", "\"labelsTr\"", ")", ",", "join", "(", "output_folder", ",", "\"labelsTr\"", ")", ")", "\n", "\n", "p", "=", "Pool", "(", "num_processes", ")", "\n", "p", ".", "starmap", "(", "split_4d_nifti", ",", "zip", "(", "files", ",", "output_dirs", ")", ")", "\n", "p", ".", "close", "(", ")", "\n", "p", ".", "join", "(", ")", "\n", "shutil", ".", "copy", "(", "join", "(", "input_folder", ",", "\"dataset.json\"", ")", ",", "output_folder", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.experiment_planning.utils.create_lists_from_splitted_dataset": [[82, 98], ["batchgenerators.utilities.file_and_folder_operations.join", "len", "open", "json.load", "d[].keys", "range", "cur_pat.append", "lists.append", "cur_pat.append", "batchgenerators.utilities.file_and_folder_operations.join", "int", "batchgenerators.utilities.file_and_folder_operations.join", "d[].keys", "tr[].split", "str", "tr[].split"], "function", ["None"], ["", "def", "create_lists_from_splitted_dataset", "(", "base_folder_splitted", ")", ":", "\n", "    ", "lists", "=", "[", "]", "\n", "\n", "json_file", "=", "join", "(", "base_folder_splitted", ",", "\"dataset.json\"", ")", "\n", "with", "open", "(", "json_file", ")", "as", "jsn", ":", "\n", "        ", "d", "=", "json", ".", "load", "(", "jsn", ")", "\n", "training_files", "=", "d", "[", "'training'", "]", "\n", "", "num_modalities", "=", "len", "(", "d", "[", "'modality'", "]", ".", "keys", "(", ")", ")", "\n", "for", "tr", "in", "training_files", ":", "\n", "        ", "cur_pat", "=", "[", "]", "\n", "for", "mod", "in", "range", "(", "num_modalities", ")", ":", "\n", "            ", "cur_pat", ".", "append", "(", "join", "(", "base_folder_splitted", ",", "\"imagesTr\"", ",", "tr", "[", "'image'", "]", ".", "split", "(", "\"/\"", ")", "[", "-", "1", "]", "[", ":", "-", "7", "]", "+", "\n", "\"_%04.0d.nii.gz\"", "%", "mod", ")", ")", "\n", "", "cur_pat", ".", "append", "(", "join", "(", "base_folder_splitted", ",", "\"labelsTr\"", ",", "tr", "[", "'label'", "]", ".", "split", "(", "\"/\"", ")", "[", "-", "1", "]", ")", ")", "\n", "lists", ".", "append", "(", "cur_pat", ")", "\n", "", "return", "lists", ",", "{", "int", "(", "i", ")", ":", "d", "[", "'modality'", "]", "[", "str", "(", "i", ")", "]", "for", "i", "in", "d", "[", "'modality'", "]", ".", "keys", "(", ")", "}", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.experiment_planning.utils.create_lists_from_splitted_dataset_folder": [[100, 111], ["utils.get_caseIDs_from_splitted_dataset_folder", "list_of_lists.append", "batchgenerators.utilities.file_and_folder_operations.subfiles"], "function", ["home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.experiment_planning.utils.get_caseIDs_from_splitted_dataset_folder"], ["", "def", "create_lists_from_splitted_dataset_folder", "(", "folder", ")", ":", "\n", "    ", "\"\"\"\n    does not rely on dataset.json\n    :param folder:\n    :return:\n    \"\"\"", "\n", "caseIDs", "=", "get_caseIDs_from_splitted_dataset_folder", "(", "folder", ")", "\n", "list_of_lists", "=", "[", "]", "\n", "for", "f", "in", "caseIDs", ":", "\n", "        ", "list_of_lists", ".", "append", "(", "subfiles", "(", "folder", ",", "prefix", "=", "f", ",", "suffix", "=", "\".nii.gz\"", ",", "join", "=", "True", ",", "sort", "=", "True", ")", ")", "\n", "", "return", "list_of_lists", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.experiment_planning.utils.get_caseIDs_from_splitted_dataset_folder": [[113, 120], ["batchgenerators.utilities.file_and_folder_operations.subfiles", "numpy.unique"], "function", ["None"], ["", "def", "get_caseIDs_from_splitted_dataset_folder", "(", "folder", ")", ":", "\n", "    ", "files", "=", "subfiles", "(", "folder", ",", "suffix", "=", "\".nii.gz\"", ",", "join", "=", "False", ")", "\n", "# all files must be .nii.gz and have 4 digit modality index", "\n", "files", "=", "[", "i", "[", ":", "-", "12", "]", "for", "i", "in", "files", "]", "\n", "# only unique patient ids", "\n", "files", "=", "np", ".", "unique", "(", "files", ")", "\n", "return", "files", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.experiment_planning.utils.crop": [[122, 136], ["batchgenerators.utilities.file_and_folder_operations.join", "batchgenerators.utilities.file_and_folder_operations.maybe_mkdir_p", "batchgenerators.utilities.file_and_folder_operations.join", "utils.create_lists_from_splitted_dataset", "nnunet.preprocessing.cropping.ImageCropper", "nnunet.preprocessing.cropping.ImageCropper.run_cropping", "shutil.copy", "batchgenerators.utilities.file_and_folder_operations.isdir", "shutil.rmtree", "batchgenerators.utilities.file_and_folder_operations.maybe_mkdir_p", "batchgenerators.utilities.file_and_folder_operations.join"], "function", ["home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.experiment_planning.utils.create_lists_from_splitted_dataset", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.preprocessing.cropping.ImageCropper.run_cropping"], ["", "def", "crop", "(", "task_string", ",", "override", "=", "False", ",", "num_threads", "=", "default_num_threads", ")", ":", "\n", "    ", "cropped_out_dir", "=", "join", "(", "nnUNet_cropped_data", ",", "task_string", ")", "\n", "maybe_mkdir_p", "(", "cropped_out_dir", ")", "\n", "\n", "if", "override", "and", "isdir", "(", "cropped_out_dir", ")", ":", "\n", "        ", "shutil", ".", "rmtree", "(", "cropped_out_dir", ")", "\n", "maybe_mkdir_p", "(", "cropped_out_dir", ")", "\n", "\n", "", "splitted_4d_output_dir_task", "=", "join", "(", "nnUNet_raw_data", ",", "task_string", ")", "\n", "lists", ",", "_", "=", "create_lists_from_splitted_dataset", "(", "splitted_4d_output_dir_task", ")", "\n", "\n", "imgcrop", "=", "ImageCropper", "(", "num_threads", ",", "cropped_out_dir", ")", "\n", "imgcrop", ".", "run_cropping", "(", "lists", ",", "overwrite_existing", "=", "override", ")", "\n", "shutil", ".", "copy", "(", "join", "(", "nnUNet_raw_data", ",", "task_string", ",", "\"dataset.json\"", ")", ",", "cropped_out_dir", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.experiment_planning.utils.analyze_dataset": [[138, 142], ["batchgenerators.utilities.file_and_folder_operations.join", "nnunet.experiment_planning.DatasetAnalyzer.DatasetAnalyzer", "nnunet.experiment_planning.DatasetAnalyzer.DatasetAnalyzer.analyze_dataset"], "function", ["home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.experiment_planning.utils.analyze_dataset"], ["", "def", "analyze_dataset", "(", "task_string", ",", "override", "=", "False", ",", "collect_intensityproperties", "=", "True", ",", "num_processes", "=", "default_num_threads", ")", ":", "\n", "    ", "cropped_out_dir", "=", "join", "(", "nnUNet_cropped_data", ",", "task_string", ")", "\n", "dataset_analyzer", "=", "DatasetAnalyzer", "(", "cropped_out_dir", ",", "overwrite", "=", "override", ",", "num_processes", "=", "num_processes", ")", "\n", "_", "=", "dataset_analyzer", ".", "analyze_dataset", "(", "collect_intensityproperties", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.experiment_planning.utils.plan_and_preprocess": [[144, 188], ["batchgenerators.utilities.file_and_folder_operations.join", "batchgenerators.utilities.file_and_folder_operations.join", "batchgenerators.utilities.file_and_folder_operations.maybe_mkdir_p", "shutil.copy", "shutil.copy", "ExperimentPlanner", "ExperimentPlanner2D.plan_experiment", "ExperimentPlanner2D", "ExperimentPlanner2D.plan_experiment", "batchgenerators.utilities.file_and_folder_operations.join", "batchgenerators.utilities.file_and_folder_operations.join", "ExperimentPlanner2D.run_preprocessing", "ExperimentPlanner2D.run_preprocessing", "multiprocessing.Pool", "multiprocessing.Pool.close", "multiprocessing.Pool.join", "print", "batchgenerators.utilities.file_and_folder_operations.subfiles", "multiprocessing.Pool.map", "batchgenerators.utilities.file_and_folder_operations.subdirs", "numpy.array", "all_classes.append", "zip", "[].find", "s.split", "open", "pickle.load", "i.split"], "function", ["home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.experiment_planning.experiment_planner_baseline_2DUNet.ExperimentPlanner2D.plan_experiment", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.experiment_planning.experiment_planner_baseline_2DUNet.ExperimentPlanner2D.plan_experiment", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.pooling_and_convs.experiment_planner_baseline_3DUNet_allConv3x3.ExperimentPlannerAllConv3x3.run_preprocessing", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.pooling_and_convs.experiment_planner_baseline_3DUNet_allConv3x3.ExperimentPlannerAllConv3x3.run_preprocessing"], ["", "def", "plan_and_preprocess", "(", "task_string", ",", "processes_lowres", "=", "default_num_threads", ",", "processes_fullres", "=", "3", ",", "no_preprocessing", "=", "False", ")", ":", "\n", "    ", "from", "nnunet", ".", "experiment_planning", ".", "experiment_planner_baseline_2DUNet", "import", "ExperimentPlanner2D", "\n", "from", "nnunet", ".", "experiment_planning", ".", "experiment_planner_baseline_3DUNet", "import", "ExperimentPlanner", "\n", "\n", "preprocessing_output_dir_this_task_train", "=", "join", "(", "preprocessing_output_dir", ",", "task_string", ")", "\n", "cropped_out_dir", "=", "join", "(", "nnUNet_cropped_data", ",", "task_string", ")", "\n", "maybe_mkdir_p", "(", "preprocessing_output_dir_this_task_train", ")", "\n", "\n", "shutil", ".", "copy", "(", "join", "(", "cropped_out_dir", ",", "\"dataset_properties.pkl\"", ")", ",", "preprocessing_output_dir_this_task_train", ")", "\n", "shutil", ".", "copy", "(", "join", "(", "nnUNet_raw_data", ",", "task_string", ",", "\"dataset.json\"", ")", ",", "preprocessing_output_dir_this_task_train", ")", "\n", "\n", "exp_planner", "=", "ExperimentPlanner", "(", "cropped_out_dir", ",", "preprocessing_output_dir_this_task_train", ")", "\n", "exp_planner", ".", "plan_experiment", "(", ")", "\n", "if", "not", "no_preprocessing", ":", "\n", "        ", "exp_planner", ".", "run_preprocessing", "(", "(", "processes_lowres", ",", "processes_fullres", ")", ")", "\n", "\n", "", "exp_planner", "=", "ExperimentPlanner2D", "(", "cropped_out_dir", ",", "preprocessing_output_dir_this_task_train", ")", "\n", "exp_planner", ".", "plan_experiment", "(", ")", "\n", "if", "not", "no_preprocessing", ":", "\n", "        ", "exp_planner", ".", "run_preprocessing", "(", "processes_fullres", ")", "\n", "\n", "# write which class is in which slice to all training cases (required to speed up 2D Dataloader)", "\n", "# This is done for all data so that if we wanted to use them with 2D we could do so", "\n", "\n", "", "if", "not", "no_preprocessing", ":", "\n", "        ", "p", "=", "Pool", "(", "default_num_threads", ")", "\n", "\n", "# if there is more than one my_data_identifier (different brnaches) then this code will run for all of them if", "\n", "# they start with the same string. not problematic, but not pretty", "\n", "stages", "=", "[", "i", "for", "i", "in", "subdirs", "(", "preprocessing_output_dir_this_task_train", ",", "join", "=", "True", ",", "sort", "=", "True", ")", "\n", "if", "i", ".", "split", "(", "\"/\"", ")", "[", "-", "1", "]", ".", "find", "(", "\"stage\"", ")", "!=", "-", "1", "]", "\n", "for", "s", "in", "stages", ":", "\n", "            ", "print", "(", "s", ".", "split", "(", "\"/\"", ")", "[", "-", "1", "]", ")", "\n", "list_of_npz_files", "=", "subfiles", "(", "s", ",", "True", ",", "None", ",", "\".npz\"", ",", "True", ")", "\n", "list_of_pkl_files", "=", "[", "i", "[", ":", "-", "4", "]", "+", "\".pkl\"", "for", "i", "in", "list_of_npz_files", "]", "\n", "all_classes", "=", "[", "]", "\n", "for", "pk", "in", "list_of_pkl_files", ":", "\n", "                ", "with", "open", "(", "pk", ",", "'rb'", ")", "as", "f", ":", "\n", "                    ", "props", "=", "pickle", ".", "load", "(", "f", ")", "\n", "", "all_classes_tmp", "=", "np", ".", "array", "(", "props", "[", "'classes'", "]", ")", "\n", "all_classes", ".", "append", "(", "all_classes_tmp", "[", "all_classes_tmp", ">=", "0", "]", ")", "\n", "", "p", ".", "map", "(", "add_classes_in_slice_info", ",", "zip", "(", "list_of_npz_files", ",", "list_of_pkl_files", ",", "all_classes", ")", ")", "\n", "", "p", ".", "close", "(", ")", "\n", "p", ".", "join", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.experiment_planning.utils.add_classes_in_slice_info": [[190, 223], ["print", "collections.OrderedDict", "range", "collections.OrderedDict", "open", "pickle.load", "tuple", "collections.OrderedDict", "numpy.sum", "open", "pickle.dump", "numpy.load", "numpy.where", "range", "numpy.sum"], "function", ["None"], ["", "", "def", "add_classes_in_slice_info", "(", "args", ")", ":", "\n", "    ", "\"\"\"\n    We need this for 2D dataloader with oversampling. As of now it will detect slices that contain specific classes\n    at run time, meaning it needs to iterate over an entire patient just to extract one slice. That is obviously bad,\n    so we are doing this once beforehand and just give the dataloader the info it needs in the patients pkl file.\n\n    \"\"\"", "\n", "npz_file", ",", "pkl_file", ",", "all_classes", "=", "args", "\n", "seg_map", "=", "np", ".", "load", "(", "npz_file", ")", "[", "'data'", "]", "[", "-", "1", "]", "\n", "with", "open", "(", "pkl_file", ",", "'rb'", ")", "as", "f", ":", "\n", "        ", "props", "=", "pickle", ".", "load", "(", "f", ")", "\n", "#if props.get('classes_in_slice_per_axis') is not None:", "\n", "", "print", "(", "pkl_file", ")", "\n", "# this will be a dict of dict where the first dict encodes the axis along which a slice is extracted in its keys.", "\n", "# The second dict (value of first dict) will have all classes as key and as values a list of all slice ids that", "\n", "# contain this class", "\n", "classes_in_slice", "=", "OrderedDict", "(", ")", "\n", "for", "axis", "in", "range", "(", "3", ")", ":", "\n", "        ", "other_axes", "=", "tuple", "(", "[", "i", "for", "i", "in", "range", "(", "3", ")", "if", "i", "!=", "axis", "]", ")", "\n", "classes_in_slice", "[", "axis", "]", "=", "OrderedDict", "(", ")", "\n", "for", "c", "in", "all_classes", ":", "\n", "            ", "valid_slices", "=", "np", ".", "where", "(", "np", ".", "sum", "(", "seg_map", "==", "c", ",", "axis", "=", "other_axes", ")", ">", "0", ")", "[", "0", "]", "\n", "classes_in_slice", "[", "axis", "]", "[", "c", "]", "=", "valid_slices", "\n", "\n", "", "", "number_of_voxels_per_class", "=", "OrderedDict", "(", ")", "\n", "for", "c", "in", "all_classes", ":", "\n", "        ", "number_of_voxels_per_class", "[", "c", "]", "=", "np", ".", "sum", "(", "seg_map", "==", "c", ")", "\n", "\n", "", "props", "[", "'classes_in_slice_per_axis'", "]", "=", "classes_in_slice", "\n", "props", "[", "'number_of_voxels_per_class'", "]", "=", "number_of_voxels_per_class", "\n", "\n", "with", "open", "(", "pkl_file", ",", "'wb'", ")", "as", "f", ":", "\n", "        ", "pickle", ".", "dump", "(", "props", ",", "f", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.alternative_experiment_planning.experiment_planner_residual_3DUNet_v21.ExperimentPlanner3DFabiansResUNet_v21.__init__": [[27, 32], ["nnunet.experiment_planning.experiment_planner_baseline_3DUNet_v21.ExperimentPlanner3D_v21.__init__", "join"], "methods", ["home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.common.layers.ResizeImage.__init__"], ["    ", "def", "__init__", "(", "self", ",", "folder_with_cropped_data", ",", "preprocessed_output_folder", ")", ":", "\n", "        ", "super", "(", "ExperimentPlanner3DFabiansResUNet_v21", ",", "self", ")", ".", "__init__", "(", "folder_with_cropped_data", ",", "preprocessed_output_folder", ")", "\n", "self", ".", "data_identifier", "=", "\"nnUNetData_plans_v2.1\"", "# \"nnUNetData_FabiansResUNet_v2.1\"", "\n", "self", ".", "plans_fname", "=", "join", "(", "self", ".", "preprocessed_output_folder", ",", "\n", "\"nnUNetPlans_FabiansResUNet_v2.1_plans_3D.pkl\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.alternative_experiment_planning.experiment_planner_residual_3DUNet_v21.ExperimentPlanner3DFabiansResUNet_v21.get_properties_for_stage": [[33, 123], ["numpy.round().astype", "numpy.round().astype.mean", "numpy.round().astype", "nnunet.experiment_planning.common_utils.get_pool_and_conv_props", "nnunet.network_architecture.generic_modular_residual_UNet.FabiansUNet.compute_approx_vram_consumption", "int", "numpy.round().astype", "max", "min", "numpy.prod", "numpy.array", "min", "copy.deepcopy", "nnunet.experiment_planning.common_utils.get_pool_and_conv_props", "nnunet.experiment_planning.common_utils.get_pool_and_conv_props", "nnunet.network_architecture.generic_modular_residual_UNet.FabiansUNet.compute_approx_vram_consumption", "numpy.floor", "numpy.round", "min", "numpy.round", "zip", "len", "numpy.argsort", "numpy.round", "max", "len", "len", "max", "len", "numpy.prod"], "methods", ["home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.experiment_planning.common_utils.get_pool_and_conv_props", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.network_architecture.generic_modular_UNet.PlainConvUNet.compute_approx_vram_consumption", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.experiment_planning.common_utils.get_pool_and_conv_props", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.experiment_planning.common_utils.get_pool_and_conv_props", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.network_architecture.generic_modular_UNet.PlainConvUNet.compute_approx_vram_consumption"], ["", "def", "get_properties_for_stage", "(", "self", ",", "current_spacing", ",", "original_spacing", ",", "original_shape", ",", "num_cases", ",", "\n", "num_modalities", ",", "num_classes", ")", ":", "\n", "        ", "\"\"\"\n        We use FabiansUNet instead of Generic_UNet\n        \"\"\"", "\n", "new_median_shape", "=", "np", ".", "round", "(", "original_spacing", "/", "current_spacing", "*", "original_shape", ")", ".", "astype", "(", "int", ")", "\n", "dataset_num_voxels", "=", "np", ".", "prod", "(", "new_median_shape", ")", "*", "num_cases", "\n", "\n", "# the next line is what we had before as a default. The patch size had the same aspect ratio as the median shape of a patient. We swapped t", "\n", "# input_patch_size = new_median_shape", "\n", "\n", "# compute how many voxels are one mm", "\n", "input_patch_size", "=", "1", "/", "np", ".", "array", "(", "current_spacing", ")", "\n", "\n", "# normalize voxels per mm", "\n", "input_patch_size", "/=", "input_patch_size", ".", "mean", "(", ")", "\n", "\n", "# create an isotropic patch of size 512x512x512mm", "\n", "input_patch_size", "*=", "1", "/", "min", "(", "input_patch_size", ")", "*", "512", "# to get a starting value", "\n", "input_patch_size", "=", "np", ".", "round", "(", "input_patch_size", ")", ".", "astype", "(", "int", ")", "\n", "\n", "# clip it to the median shape of the dataset because patches larger then that make not much sense", "\n", "input_patch_size", "=", "[", "min", "(", "i", ",", "j", ")", "for", "i", ",", "j", "in", "zip", "(", "input_patch_size", ",", "new_median_shape", ")", "]", "\n", "\n", "network_num_pool_per_axis", ",", "pool_op_kernel_sizes", ",", "conv_kernel_sizes", ",", "new_shp", ",", "shape_must_be_divisible_by", "=", "get_pool_and_conv_props", "(", "current_spacing", ",", "input_patch_size", ",", "\n", "self", ".", "unet_featuremap_min_edge_length", ",", "\n", "self", ".", "unet_max_numpool", ")", "\n", "pool_op_kernel_sizes", "=", "[", "[", "1", ",", "1", ",", "1", "]", "]", "+", "pool_op_kernel_sizes", "\n", "blocks_per_stage_encoder", "=", "FabiansUNet", ".", "default_blocks_per_stage_encoder", "[", ":", "len", "(", "pool_op_kernel_sizes", ")", "]", "\n", "blocks_per_stage_decoder", "=", "FabiansUNet", ".", "default_blocks_per_stage_decoder", "[", ":", "len", "(", "pool_op_kernel_sizes", ")", "-", "1", "]", "\n", "\n", "ref", "=", "FabiansUNet", ".", "use_this_for_3D_configuration", "\n", "here", "=", "FabiansUNet", ".", "compute_approx_vram_consumption", "(", "input_patch_size", ",", "self", ".", "unet_base_num_features", ",", "\n", "self", ".", "unet_max_num_filters", ",", "num_modalities", ",", "num_classes", ",", "\n", "pool_op_kernel_sizes", ",", "blocks_per_stage_encoder", ",", "\n", "blocks_per_stage_decoder", ",", "2", ",", "self", ".", "unet_min_batch_size", ",", ")", "\n", "while", "here", ">", "ref", ":", "\n", "            ", "axis_to_be_reduced", "=", "np", ".", "argsort", "(", "new_shp", "/", "new_median_shape", ")", "[", "-", "1", "]", "\n", "\n", "tmp", "=", "deepcopy", "(", "new_shp", ")", "\n", "tmp", "[", "axis_to_be_reduced", "]", "-=", "shape_must_be_divisible_by", "[", "axis_to_be_reduced", "]", "\n", "_", ",", "_", ",", "_", ",", "_", ",", "shape_must_be_divisible_by_new", "=", "get_pool_and_conv_props", "(", "current_spacing", ",", "tmp", ",", "\n", "self", ".", "unet_featuremap_min_edge_length", ",", "\n", "self", ".", "unet_max_numpool", ",", "\n", ")", "\n", "new_shp", "[", "axis_to_be_reduced", "]", "-=", "shape_must_be_divisible_by_new", "[", "axis_to_be_reduced", "]", "\n", "\n", "# we have to recompute numpool now:", "\n", "network_num_pool_per_axis", ",", "pool_op_kernel_sizes", ",", "conv_kernel_sizes", ",", "new_shp", ",", "shape_must_be_divisible_by", "=", "get_pool_and_conv_props", "(", "current_spacing", ",", "new_shp", ",", "\n", "self", ".", "unet_featuremap_min_edge_length", ",", "\n", "self", ".", "unet_max_numpool", ",", "\n", ")", "\n", "pool_op_kernel_sizes", "=", "[", "[", "1", ",", "1", ",", "1", "]", "]", "+", "pool_op_kernel_sizes", "\n", "blocks_per_stage_encoder", "=", "FabiansUNet", ".", "default_blocks_per_stage_encoder", "[", ":", "len", "(", "pool_op_kernel_sizes", ")", "]", "\n", "blocks_per_stage_decoder", "=", "FabiansUNet", ".", "default_blocks_per_stage_decoder", "[", ":", "len", "(", "pool_op_kernel_sizes", ")", "-", "1", "]", "\n", "here", "=", "FabiansUNet", ".", "compute_approx_vram_consumption", "(", "new_shp", ",", "self", ".", "unet_base_num_features", ",", "\n", "self", ".", "unet_max_num_filters", ",", "num_modalities", ",", "num_classes", ",", "\n", "pool_op_kernel_sizes", ",", "blocks_per_stage_encoder", ",", "\n", "blocks_per_stage_decoder", ",", "2", ",", "self", ".", "unet_min_batch_size", ")", "\n", "", "input_patch_size", "=", "new_shp", "\n", "\n", "batch_size", "=", "FabiansUNet", ".", "default_min_batch_size", "\n", "batch_size", "=", "int", "(", "np", ".", "floor", "(", "max", "(", "ref", "/", "here", ",", "1", ")", "*", "batch_size", ")", ")", "\n", "\n", "# check if batch size is too large", "\n", "max_batch_size", "=", "np", ".", "round", "(", "self", ".", "batch_size_covers_max_percent_of_dataset", "*", "dataset_num_voxels", "/", "\n", "np", ".", "prod", "(", "input_patch_size", ",", "dtype", "=", "np", ".", "int64", ")", ")", ".", "astype", "(", "int", ")", "\n", "max_batch_size", "=", "max", "(", "max_batch_size", ",", "self", ".", "unet_min_batch_size", ")", "\n", "batch_size", "=", "min", "(", "batch_size", ",", "max_batch_size", ")", "\n", "\n", "do_dummy_2D_data_aug", "=", "(", "max", "(", "input_patch_size", ")", "/", "input_patch_size", "[", "\n", "0", "]", ")", ">", "self", ".", "anisotropy_threshold", "\n", "\n", "plan", "=", "{", "\n", "'batch_size'", ":", "batch_size", ",", "\n", "'num_pool_per_axis'", ":", "network_num_pool_per_axis", ",", "\n", "'patch_size'", ":", "input_patch_size", ",", "\n", "'median_patient_size_in_voxels'", ":", "new_median_shape", ",", "\n", "'current_spacing'", ":", "current_spacing", ",", "\n", "'original_spacing'", ":", "original_spacing", ",", "\n", "'do_dummy_2D_data_aug'", ":", "do_dummy_2D_data_aug", ",", "\n", "'pool_op_kernel_sizes'", ":", "pool_op_kernel_sizes", ",", "\n", "'conv_kernel_sizes'", ":", "conv_kernel_sizes", ",", "\n", "'num_blocks_encoder'", ":", "blocks_per_stage_encoder", ",", "\n", "'num_blocks_decoder'", ":", "blocks_per_stage_decoder", "\n", "}", "\n", "return", "plan", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.alternative_experiment_planning.experiment_planner_residual_3DUNet_v21.ExperimentPlanner3DFabiansResUNet_v21.run_preprocessing": [[124, 133], ["None"], "methods", ["None"], ["", "def", "run_preprocessing", "(", "self", ",", "num_threads", ")", ":", "\n", "        ", "\"\"\"\n        On all datasets except 3d fullres on spleen the preprocessed data would look identical to\n        ExperimentPlanner3D_v21 (I tested decathlon data only). Therefore we just reuse the preprocessed data of\n        that other planner\n        :param num_threads:\n        :return:\n        \"\"\"", "\n", "pass", "\n", "", "", ""]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.alternative_experiment_planning.experiment_planner_baseline_3DUNet_v22.ExperimentPlanner3D_v22.__init__": [[24, 29], ["nnunet.experiment_planning.experiment_planner_baseline_3DUNet_v21.ExperimentPlanner3D_v21.__init__", "join"], "methods", ["home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.common.layers.ResizeImage.__init__"], ["def", "__init__", "(", "self", ",", "folder_with_cropped_data", ",", "preprocessed_output_folder", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "folder_with_cropped_data", ",", "preprocessed_output_folder", ")", "\n", "self", ".", "data_identifier", "=", "\"nnUNetData_plans_v2.2\"", "\n", "self", ".", "plans_fname", "=", "join", "(", "self", ".", "preprocessed_output_folder", ",", "\n", "\"nnUNetPlansv2.2_plans_3D.pkl\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.alternative_experiment_planning.experiment_planner_baseline_3DUNet_v22.ExperimentPlanner3D_v22.get_target_spacing": [[30, 59], ["numpy.percentile", "numpy.percentile", "numpy.argmax", "numpy.vstack", "numpy.vstack", "numpy.array", "numpy.array", "min", "numpy.percentile", "max", "range", "min", "numpy.vstack", "len", "min"], "methods", ["None"], ["", "def", "get_target_spacing", "(", "self", ")", ":", "\n", "        ", "spacings", "=", "self", ".", "dataset_properties", "[", "'all_spacings'", "]", "\n", "sizes", "=", "self", ".", "dataset_properties", "[", "'all_sizes'", "]", "\n", "\n", "target", "=", "np", ".", "percentile", "(", "np", ".", "vstack", "(", "spacings", ")", ",", "self", ".", "target_spacing_percentile", ",", "0", ")", "\n", "target_size", "=", "np", ".", "percentile", "(", "np", ".", "vstack", "(", "sizes", ")", ",", "self", ".", "target_spacing_percentile", ",", "0", ")", "\n", "target_size_mm", "=", "np", ".", "array", "(", "target", ")", "*", "np", ".", "array", "(", "target_size", ")", "\n", "# we need to identify datasets for which a different target spacing could be beneficial. These datasets have", "\n", "# the following properties:", "\n", "# - one axis which much lower resolution than the others", "\n", "# - the lowres axis has much less voxels than the others", "\n", "# - (the size in mm of the lowres axis is also reduced)", "\n", "worst_spacing_axis", "=", "np", ".", "argmax", "(", "target", ")", "\n", "other_axes", "=", "[", "i", "for", "i", "in", "range", "(", "len", "(", "target", ")", ")", "if", "i", "!=", "worst_spacing_axis", "]", "\n", "other_spacings", "=", "[", "target", "[", "i", "]", "for", "i", "in", "other_axes", "]", "\n", "other_sizes", "=", "[", "target_size", "[", "i", "]", "for", "i", "in", "other_axes", "]", "\n", "\n", "has_aniso_spacing", "=", "target", "[", "worst_spacing_axis", "]", ">", "(", "self", ".", "anisotropy_threshold", "*", "min", "(", "other_spacings", ")", ")", "\n", "has_aniso_voxels", "=", "target_size", "[", "worst_spacing_axis", "]", "*", "self", ".", "anisotropy_threshold", "<", "min", "(", "other_sizes", ")", "\n", "# we don't use the last one for now", "\n", "#median_size_in_mm = target[target_size_mm] * RESAMPLING_SEPARATE_Z_ANISOTROPY_THRESHOLD < max(target_size_mm)", "\n", "\n", "if", "has_aniso_spacing", "and", "has_aniso_voxels", ":", "\n", "            ", "spacings_of_that_axis", "=", "np", ".", "vstack", "(", "spacings", ")", "[", ":", ",", "worst_spacing_axis", "]", "\n", "target_spacing_of_that_axis", "=", "np", ".", "percentile", "(", "spacings_of_that_axis", ",", "10", ")", "\n", "# don't let the spacing of that axis get higher than self.anisotropy_thresholdxthe_other_axes", "\n", "target_spacing_of_that_axis", "=", "max", "(", "min", "(", "other_spacings", ")", "*", "self", ".", "anisotropy_threshold", ",", "target_spacing_of_that_axis", ")", "\n", "target", "[", "worst_spacing_axis", "]", "=", "target_spacing_of_that_axis", "\n", "", "return", "target", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.alternative_experiment_planning.experiment_planner_baseline_3DUNet_v21_3convperstage.ExperimentPlanner3D_v21_3cps.__init__": [[32, 38], ["nnunet.experiment_planning.experiment_planner_baseline_3DUNet_v21.ExperimentPlanner3D_v21.__init__", "join"], "methods", ["home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.common.layers.ResizeImage.__init__"], ["def", "__init__", "(", "self", ",", "folder_with_cropped_data", ",", "preprocessed_output_folder", ")", ":", "\n", "        ", "super", "(", "ExperimentPlanner3D_v21_3cps", ",", "self", ")", ".", "__init__", "(", "folder_with_cropped_data", ",", "preprocessed_output_folder", ")", "\n", "self", ".", "plans_fname", "=", "join", "(", "self", ".", "preprocessed_output_folder", ",", "\n", "\"nnUNetPlansv2.1_3cps_plans_3D.pkl\"", ")", "\n", "self", ".", "unet_base_num_features", "=", "32", "\n", "self", ".", "conv_per_stage", "=", "3", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.alternative_experiment_planning.experiment_planner_baseline_3DUNet_v21_3convperstage.ExperimentPlanner3D_v21_3cps.run_preprocessing": [[39, 41], ["None"], "methods", ["None"], ["", "def", "run_preprocessing", "(", "self", ",", "num_threads", ")", ":", "\n", "        ", "pass", "\n", "", "", ""]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.alternative_experiment_planning.experiment_planner_baseline_3DUNet_v21_32GB.ExperimentPlanner3D_v21_32GB.__init__": [[29, 34], ["nnunet.experiment_planning.experiment_planner_baseline_3DUNet_v21.ExperimentPlanner3D_v21.__init__", "join"], "methods", ["home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.common.layers.ResizeImage.__init__"], ["def", "__init__", "(", "self", ",", "folder_with_cropped_data", ",", "preprocessed_output_folder", ")", ":", "\n", "        ", "super", "(", "ExperimentPlanner3D_v21_32GB", ",", "self", ")", ".", "__init__", "(", "folder_with_cropped_data", ",", "preprocessed_output_folder", ")", "\n", "self", ".", "data_identifier", "=", "\"nnUNetData_plans_v2.1_verybig\"", "\n", "self", ".", "plans_fname", "=", "join", "(", "self", ".", "preprocessed_output_folder", ",", "\n", "\"nnUNetPlansv2.1_verybig_plans_3D.pkl\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.alternative_experiment_planning.experiment_planner_baseline_3DUNet_v21_32GB.ExperimentPlanner3D_v21_32GB.get_properties_for_stage": [[35, 123], ["numpy.round().astype", "numpy.round().astype.mean", "numpy.round().astype", "nnunet.experiment_planning.common_utils.get_pool_and_conv_props", "nnunet.network_architecture.generic_UNet.Generic_UNet.compute_approx_vram_consumption", "int", "numpy.round().astype", "max", "min", "numpy.prod", "numpy.array", "min", "copy.deepcopy", "nnunet.experiment_planning.common_utils.get_pool_and_conv_props", "nnunet.experiment_planning.common_utils.get_pool_and_conv_props", "nnunet.network_architecture.generic_UNet.Generic_UNet.compute_approx_vram_consumption", "numpy.floor", "numpy.round", "min", "numpy.round", "zip", "numpy.argsort", "numpy.round", "max", "max", "numpy.prod"], "methods", ["home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.experiment_planning.common_utils.get_pool_and_conv_props", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.network_architecture.generic_modular_UNet.PlainConvUNet.compute_approx_vram_consumption", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.experiment_planning.common_utils.get_pool_and_conv_props", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.experiment_planning.common_utils.get_pool_and_conv_props", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.network_architecture.generic_modular_UNet.PlainConvUNet.compute_approx_vram_consumption"], ["", "def", "get_properties_for_stage", "(", "self", ",", "current_spacing", ",", "original_spacing", ",", "original_shape", ",", "num_cases", ",", "\n", "num_modalities", ",", "num_classes", ")", ":", "\n", "        ", "\"\"\"\n        We need to adapt ref\n        \"\"\"", "\n", "new_median_shape", "=", "np", ".", "round", "(", "original_spacing", "/", "current_spacing", "*", "original_shape", ")", ".", "astype", "(", "int", ")", "\n", "dataset_num_voxels", "=", "np", ".", "prod", "(", "new_median_shape", ")", "*", "num_cases", "\n", "\n", "# the next line is what we had before as a default. The patch size had the same aspect ratio as the median shape of a patient. We swapped t", "\n", "# input_patch_size = new_median_shape", "\n", "\n", "# compute how many voxels are one mm", "\n", "input_patch_size", "=", "1", "/", "np", ".", "array", "(", "current_spacing", ")", "\n", "\n", "# normalize voxels per mm", "\n", "input_patch_size", "/=", "input_patch_size", ".", "mean", "(", ")", "\n", "\n", "# create an isotropic patch of size 512x512x512mm", "\n", "input_patch_size", "*=", "1", "/", "min", "(", "input_patch_size", ")", "*", "512", "# to get a starting value", "\n", "input_patch_size", "=", "np", ".", "round", "(", "input_patch_size", ")", ".", "astype", "(", "int", ")", "\n", "\n", "# clip it to the median shape of the dataset because patches larger then that make not much sense", "\n", "input_patch_size", "=", "[", "min", "(", "i", ",", "j", ")", "for", "i", ",", "j", "in", "zip", "(", "input_patch_size", ",", "new_median_shape", ")", "]", "\n", "\n", "network_num_pool_per_axis", ",", "pool_op_kernel_sizes", ",", "conv_kernel_sizes", ",", "new_shp", ",", "shape_must_be_divisible_by", "=", "get_pool_and_conv_props", "(", "current_spacing", ",", "input_patch_size", ",", "\n", "self", ".", "unet_featuremap_min_edge_length", ",", "\n", "self", ".", "unet_max_numpool", ")", "\n", "#     use_this_for_batch_size_computation_3D = 520000000 # 505789440", "\n", "# typical ExperimentPlanner3D_v21 configurations use 7.5GB, but on a V100 we have 32. Allow for more space", "\n", "# to be used", "\n", "ref", "=", "Generic_UNet", ".", "use_this_for_batch_size_computation_3D", "*", "32", "/", "8", "\n", "here", "=", "Generic_UNet", ".", "compute_approx_vram_consumption", "(", "new_shp", ",", "network_num_pool_per_axis", ",", "\n", "self", ".", "unet_base_num_features", ",", "\n", "self", ".", "unet_max_num_filters", ",", "num_modalities", ",", "\n", "num_classes", ",", "\n", "pool_op_kernel_sizes", ",", "conv_per_stage", "=", "self", ".", "conv_per_stage", ")", "\n", "while", "here", ">", "ref", ":", "\n", "            ", "axis_to_be_reduced", "=", "np", ".", "argsort", "(", "new_shp", "/", "new_median_shape", ")", "[", "-", "1", "]", "\n", "\n", "tmp", "=", "deepcopy", "(", "new_shp", ")", "\n", "tmp", "[", "axis_to_be_reduced", "]", "-=", "shape_must_be_divisible_by", "[", "axis_to_be_reduced", "]", "\n", "_", ",", "_", ",", "_", ",", "_", ",", "shape_must_be_divisible_by_new", "=", "get_pool_and_conv_props", "(", "current_spacing", ",", "tmp", ",", "\n", "self", ".", "unet_featuremap_min_edge_length", ",", "\n", "self", ".", "unet_max_numpool", ",", "\n", ")", "\n", "new_shp", "[", "axis_to_be_reduced", "]", "-=", "shape_must_be_divisible_by_new", "[", "axis_to_be_reduced", "]", "\n", "\n", "# we have to recompute numpool now:", "\n", "network_num_pool_per_axis", ",", "pool_op_kernel_sizes", ",", "conv_kernel_sizes", ",", "new_shp", ",", "shape_must_be_divisible_by", "=", "get_pool_and_conv_props", "(", "current_spacing", ",", "new_shp", ",", "\n", "self", ".", "unet_featuremap_min_edge_length", ",", "\n", "self", ".", "unet_max_numpool", ",", "\n", ")", "\n", "\n", "here", "=", "Generic_UNet", ".", "compute_approx_vram_consumption", "(", "new_shp", ",", "network_num_pool_per_axis", ",", "\n", "self", ".", "unet_base_num_features", ",", "\n", "self", ".", "unet_max_num_filters", ",", "num_modalities", ",", "\n", "num_classes", ",", "pool_op_kernel_sizes", ",", "\n", "conv_per_stage", "=", "self", ".", "conv_per_stage", ")", "\n", "# print(new_shp)", "\n", "", "input_patch_size", "=", "new_shp", "\n", "\n", "batch_size", "=", "Generic_UNet", ".", "DEFAULT_BATCH_SIZE_3D", "# This is what wirks with 128**3", "\n", "batch_size", "=", "int", "(", "np", ".", "floor", "(", "max", "(", "ref", "/", "here", ",", "1", ")", "*", "batch_size", ")", ")", "\n", "\n", "# check if batch size is too large", "\n", "max_batch_size", "=", "np", ".", "round", "(", "self", ".", "batch_size_covers_max_percent_of_dataset", "*", "dataset_num_voxels", "/", "\n", "np", ".", "prod", "(", "input_patch_size", ",", "dtype", "=", "np", ".", "int64", ")", ")", ".", "astype", "(", "int", ")", "\n", "max_batch_size", "=", "max", "(", "max_batch_size", ",", "self", ".", "unet_min_batch_size", ")", "\n", "batch_size", "=", "min", "(", "batch_size", ",", "max_batch_size", ")", "\n", "\n", "do_dummy_2D_data_aug", "=", "(", "max", "(", "input_patch_size", ")", "/", "input_patch_size", "[", "\n", "0", "]", ")", ">", "self", ".", "anisotropy_threshold", "\n", "\n", "plan", "=", "{", "\n", "'batch_size'", ":", "batch_size", ",", "\n", "'num_pool_per_axis'", ":", "network_num_pool_per_axis", ",", "\n", "'patch_size'", ":", "input_patch_size", ",", "\n", "'median_patient_size_in_voxels'", ":", "new_median_shape", ",", "\n", "'current_spacing'", ":", "current_spacing", ",", "\n", "'original_spacing'", ":", "original_spacing", ",", "\n", "'do_dummy_2D_data_aug'", ":", "do_dummy_2D_data_aug", ",", "\n", "'pool_op_kernel_sizes'", ":", "pool_op_kernel_sizes", ",", "\n", "'conv_kernel_sizes'", ":", "conv_kernel_sizes", ",", "\n", "}", "\n", "return", "plan", "\n", "", "", ""]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.alternative_experiment_planning.experiment_planner_baseline_3DUNet_v21_11GB.ExperimentPlanner3D_v21_11GB.__init__": [[29, 34], ["nnunet.experiment_planning.experiment_planner_baseline_3DUNet_v21.ExperimentPlanner3D_v21.__init__", "join"], "methods", ["home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.common.layers.ResizeImage.__init__"], ["def", "__init__", "(", "self", ",", "folder_with_cropped_data", ",", "preprocessed_output_folder", ")", ":", "\n", "        ", "super", "(", "ExperimentPlanner3D_v21_11GB", ",", "self", ")", ".", "__init__", "(", "folder_with_cropped_data", ",", "preprocessed_output_folder", ")", "\n", "self", ".", "data_identifier", "=", "\"nnUNetData_plans_v2.1_big\"", "\n", "self", ".", "plans_fname", "=", "join", "(", "self", ".", "preprocessed_output_folder", ",", "\n", "\"nnUNetPlansv2.1_big_plans_3D.pkl\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.alternative_experiment_planning.experiment_planner_baseline_3DUNet_v21_11GB.ExperimentPlanner3D_v21_11GB.get_properties_for_stage": [[35, 124], ["numpy.round().astype", "numpy.round().astype.mean", "numpy.round().astype", "nnunet.experiment_planning.common_utils.get_pool_and_conv_props", "nnunet.network_architecture.generic_UNet.Generic_UNet.compute_approx_vram_consumption", "int", "numpy.round().astype", "max", "min", "numpy.prod", "numpy.array", "min", "copy.deepcopy", "nnunet.experiment_planning.common_utils.get_pool_and_conv_props", "nnunet.experiment_planning.common_utils.get_pool_and_conv_props", "nnunet.network_architecture.generic_UNet.Generic_UNet.compute_approx_vram_consumption", "numpy.floor", "numpy.round", "min", "numpy.round", "zip", "numpy.argsort", "numpy.round", "max", "max", "numpy.prod"], "methods", ["home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.experiment_planning.common_utils.get_pool_and_conv_props", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.network_architecture.generic_modular_UNet.PlainConvUNet.compute_approx_vram_consumption", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.experiment_planning.common_utils.get_pool_and_conv_props", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.experiment_planning.common_utils.get_pool_and_conv_props", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.network_architecture.generic_modular_UNet.PlainConvUNet.compute_approx_vram_consumption"], ["", "def", "get_properties_for_stage", "(", "self", ",", "current_spacing", ",", "original_spacing", ",", "original_shape", ",", "num_cases", ",", "\n", "num_modalities", ",", "num_classes", ")", ":", "\n", "        ", "\"\"\"\n        We need to adapt ref\n        \"\"\"", "\n", "new_median_shape", "=", "np", ".", "round", "(", "original_spacing", "/", "current_spacing", "*", "original_shape", ")", ".", "astype", "(", "int", ")", "\n", "dataset_num_voxels", "=", "np", ".", "prod", "(", "new_median_shape", ")", "*", "num_cases", "\n", "\n", "# the next line is what we had before as a default. The patch size had the same aspect ratio as the median shape of a patient. We swapped t", "\n", "# input_patch_size = new_median_shape", "\n", "\n", "# compute how many voxels are one mm", "\n", "input_patch_size", "=", "1", "/", "np", ".", "array", "(", "current_spacing", ")", "\n", "\n", "# normalize voxels per mm", "\n", "input_patch_size", "/=", "input_patch_size", ".", "mean", "(", ")", "\n", "\n", "# create an isotropic patch of size 512x512x512mm", "\n", "input_patch_size", "*=", "1", "/", "min", "(", "input_patch_size", ")", "*", "512", "# to get a starting value", "\n", "input_patch_size", "=", "np", ".", "round", "(", "input_patch_size", ")", ".", "astype", "(", "int", ")", "\n", "\n", "# clip it to the median shape of the dataset because patches larger then that make not much sense", "\n", "input_patch_size", "=", "[", "min", "(", "i", ",", "j", ")", "for", "i", ",", "j", "in", "zip", "(", "input_patch_size", ",", "new_median_shape", ")", "]", "\n", "\n", "network_num_pool_per_axis", ",", "pool_op_kernel_sizes", ",", "conv_kernel_sizes", ",", "new_shp", ",", "shape_must_be_divisible_by", "=", "get_pool_and_conv_props", "(", "current_spacing", ",", "input_patch_size", ",", "\n", "self", ".", "unet_featuremap_min_edge_length", ",", "\n", "self", ".", "unet_max_numpool", ")", "\n", "#     use_this_for_batch_size_computation_3D = 520000000 # 505789440", "\n", "# typical ExperimentPlanner3D_v21 configurations use 7.5GB, but on a 2080ti we have 11. Allow for more space", "\n", "# to be used", "\n", "ref", "=", "Generic_UNet", ".", "use_this_for_batch_size_computation_3D", "*", "11", "/", "8", "\n", "here", "=", "Generic_UNet", ".", "compute_approx_vram_consumption", "(", "new_shp", ",", "network_num_pool_per_axis", ",", "\n", "self", ".", "unet_base_num_features", ",", "\n", "self", ".", "unet_max_num_filters", ",", "num_modalities", ",", "\n", "num_classes", ",", "\n", "pool_op_kernel_sizes", ",", "conv_per_stage", "=", "self", ".", "conv_per_stage", ")", "\n", "while", "here", ">", "ref", ":", "\n", "            ", "axis_to_be_reduced", "=", "np", ".", "argsort", "(", "new_shp", "/", "new_median_shape", ")", "[", "-", "1", "]", "\n", "\n", "tmp", "=", "deepcopy", "(", "new_shp", ")", "\n", "tmp", "[", "axis_to_be_reduced", "]", "-=", "shape_must_be_divisible_by", "[", "axis_to_be_reduced", "]", "\n", "_", ",", "_", ",", "_", ",", "_", ",", "shape_must_be_divisible_by_new", "=", "get_pool_and_conv_props", "(", "current_spacing", ",", "tmp", ",", "\n", "self", ".", "unet_featuremap_min_edge_length", ",", "\n", "self", ".", "unet_max_numpool", ",", "\n", ")", "\n", "new_shp", "[", "axis_to_be_reduced", "]", "-=", "shape_must_be_divisible_by_new", "[", "axis_to_be_reduced", "]", "\n", "\n", "# we have to recompute numpool now:", "\n", "network_num_pool_per_axis", ",", "pool_op_kernel_sizes", ",", "conv_kernel_sizes", ",", "new_shp", ",", "shape_must_be_divisible_by", "=", "get_pool_and_conv_props", "(", "current_spacing", ",", "new_shp", ",", "\n", "self", ".", "unet_featuremap_min_edge_length", ",", "\n", "self", ".", "unet_max_numpool", ",", "\n", ")", "\n", "\n", "here", "=", "Generic_UNet", ".", "compute_approx_vram_consumption", "(", "new_shp", ",", "network_num_pool_per_axis", ",", "\n", "self", ".", "unet_base_num_features", ",", "\n", "self", ".", "unet_max_num_filters", ",", "num_modalities", ",", "\n", "num_classes", ",", "pool_op_kernel_sizes", ",", "\n", "conv_per_stage", "=", "self", ".", "conv_per_stage", ")", "\n", "# print(new_shp)", "\n", "\n", "", "input_patch_size", "=", "new_shp", "\n", "\n", "batch_size", "=", "Generic_UNet", ".", "DEFAULT_BATCH_SIZE_3D", "# This is what wirks with 128**3", "\n", "batch_size", "=", "int", "(", "np", ".", "floor", "(", "max", "(", "ref", "/", "here", ",", "1", ")", "*", "batch_size", ")", ")", "\n", "\n", "# check if batch size is too large", "\n", "max_batch_size", "=", "np", ".", "round", "(", "self", ".", "batch_size_covers_max_percent_of_dataset", "*", "dataset_num_voxels", "/", "\n", "np", ".", "prod", "(", "input_patch_size", ",", "dtype", "=", "np", ".", "int64", ")", ")", ".", "astype", "(", "int", ")", "\n", "max_batch_size", "=", "max", "(", "max_batch_size", ",", "self", ".", "unet_min_batch_size", ")", "\n", "batch_size", "=", "min", "(", "batch_size", ",", "max_batch_size", ")", "\n", "\n", "do_dummy_2D_data_aug", "=", "(", "max", "(", "input_patch_size", ")", "/", "input_patch_size", "[", "\n", "0", "]", ")", ">", "self", ".", "anisotropy_threshold", "\n", "\n", "plan", "=", "{", "\n", "'batch_size'", ":", "batch_size", ",", "\n", "'num_pool_per_axis'", ":", "network_num_pool_per_axis", ",", "\n", "'patch_size'", ":", "input_patch_size", ",", "\n", "'median_patient_size_in_voxels'", ":", "new_median_shape", ",", "\n", "'current_spacing'", ":", "current_spacing", ",", "\n", "'original_spacing'", ":", "original_spacing", ",", "\n", "'do_dummy_2D_data_aug'", ":", "do_dummy_2D_data_aug", ",", "\n", "'pool_op_kernel_sizes'", ":", "pool_op_kernel_sizes", ",", "\n", "'conv_kernel_sizes'", ":", "conv_kernel_sizes", ",", "\n", "}", "\n", "return", "plan", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.alternative_experiment_planning.experiment_planner_baseline_3DUNet_v23.ExperimentPlanner3D_v23.__init__": [[23, 29], ["nnunet.experiment_planning.experiment_planner_baseline_3DUNet_v21.ExperimentPlanner3D_v21.__init__", "join"], "methods", ["home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.common.layers.ResizeImage.__init__"], ["def", "__init__", "(", "self", ",", "folder_with_cropped_data", ",", "preprocessed_output_folder", ")", ":", "\n", "        ", "super", "(", "ExperimentPlanner3D_v23", ",", "self", ")", ".", "__init__", "(", "folder_with_cropped_data", ",", "preprocessed_output_folder", ")", "\n", "self", ".", "data_identifier", "=", "\"nnUNetData_plans_v2.3\"", "\n", "self", ".", "plans_fname", "=", "join", "(", "self", ".", "preprocessed_output_folder", ",", "\n", "\"nnUNetPlansv2.3_plans_3D.pkl\"", ")", "\n", "self", ".", "preprocessor_name", "=", "\"Preprocessor3DDifferentResampling\"", "\n", "", "", ""]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.normalization.experiment_planner_3DUNet_CT2.ExperimentPlannerCT2.__init__": [[30, 34], ["nnunet.experiment_planning.experiment_planner_baseline_3DUNet.ExperimentPlanner.__init__", "join"], "methods", ["home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.common.layers.ResizeImage.__init__"], ["def", "__init__", "(", "self", ",", "folder_with_cropped_data", ",", "preprocessed_output_folder", ")", ":", "\n", "        ", "super", "(", "ExperimentPlannerCT2", ",", "self", ")", ".", "__init__", "(", "folder_with_cropped_data", ",", "preprocessed_output_folder", ")", "\n", "self", ".", "data_identifier", "=", "\"nnUNet_CT2\"", "\n", "self", ".", "plans_fname", "=", "join", "(", "self", ".", "preprocessed_output_folder", ",", "\"nnUNetPlans\"", "+", "\"CT2_plans_3D.pkl\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.normalization.experiment_planner_3DUNet_CT2.ExperimentPlannerCT2.determine_normalization_scheme": [[35, 46], ["collections.OrderedDict", "len", "range", "list", "modalities.keys"], "methods", ["None"], ["", "def", "determine_normalization_scheme", "(", "self", ")", ":", "\n", "        ", "schemes", "=", "OrderedDict", "(", ")", "\n", "modalities", "=", "self", ".", "dataset_properties", "[", "'modalities'", "]", "\n", "num_modalities", "=", "len", "(", "list", "(", "modalities", ".", "keys", "(", ")", ")", ")", "\n", "\n", "for", "i", "in", "range", "(", "num_modalities", ")", ":", "\n", "            ", "if", "modalities", "[", "i", "]", "==", "\"CT\"", ":", "\n", "                ", "schemes", "[", "i", "]", "=", "\"CT2\"", "\n", "", "else", ":", "\n", "                ", "schemes", "[", "i", "]", "=", "\"nonCT\"", "\n", "", "", "return", "schemes", "\n", "", "", ""]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.normalization.experiment_planner_3DUNet_nonCT.ExperimentPlannernonCT.__init__": [[27, 31], ["nnunet.experiment_planning.experiment_planner_baseline_3DUNet.ExperimentPlanner.__init__", "join"], "methods", ["home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.common.layers.ResizeImage.__init__"], ["def", "__init__", "(", "self", ",", "folder_with_cropped_data", ",", "preprocessed_output_folder", ")", ":", "\n", "        ", "super", "(", "ExperimentPlannernonCT", ",", "self", ")", ".", "__init__", "(", "folder_with_cropped_data", ",", "preprocessed_output_folder", ")", "\n", "self", ".", "data_identifier", "=", "\"nnUNet_nonCT\"", "\n", "self", ".", "plans_fname", "=", "join", "(", "self", ".", "preprocessed_output_folder", ",", "\"nnUNetPlans\"", "+", "\"nonCT_plans_3D.pkl\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.normalization.experiment_planner_3DUNet_nonCT.ExperimentPlannernonCT.determine_normalization_scheme": [[32, 43], ["collections.OrderedDict", "len", "range", "list", "modalities.keys"], "methods", ["None"], ["", "def", "determine_normalization_scheme", "(", "self", ")", ":", "\n", "        ", "schemes", "=", "OrderedDict", "(", ")", "\n", "modalities", "=", "self", ".", "dataset_properties", "[", "'modalities'", "]", "\n", "num_modalities", "=", "len", "(", "list", "(", "modalities", ".", "keys", "(", ")", ")", ")", "\n", "\n", "for", "i", "in", "range", "(", "num_modalities", ")", ":", "\n", "            ", "if", "modalities", "[", "i", "]", "==", "\"CT\"", ":", "\n", "                ", "schemes", "[", "i", "]", "=", "\"nonCT\"", "\n", "", "else", ":", "\n", "                ", "schemes", "[", "i", "]", "=", "\"nonCT\"", "\n", "", "", "return", "schemes", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.target_spacing.experiment_planner_baseline_3DUNet_targetSpacingForAnisoAxis.ExperimentPlannerTargetSpacingForAnisoAxis.__init__": [[21, 26], ["nnunet.experiment_planning.experiment_planner_baseline_3DUNet.ExperimentPlanner.__init__", "join"], "methods", ["home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.common.layers.ResizeImage.__init__"], ["    ", "def", "__init__", "(", "self", ",", "folder_with_cropped_data", ",", "preprocessed_output_folder", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "folder_with_cropped_data", ",", "preprocessed_output_folder", ")", "\n", "self", ".", "data_identifier", "=", "\"nnUNetData_targetSpacingForAnisoAxis\"", "\n", "self", ".", "plans_fname", "=", "join", "(", "self", ".", "preprocessed_output_folder", ",", "\n", "\"nnUNetPlans\"", "+", "\"targetSpacingForAnisoAxis_plans_3D.pkl\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.target_spacing.experiment_planner_baseline_3DUNet_targetSpacingForAnisoAxis.ExperimentPlannerTargetSpacingForAnisoAxis.get_target_spacing": [[27, 63], ["numpy.percentile", "numpy.percentile", "numpy.argmax", "numpy.vstack", "numpy.vstack", "numpy.array", "numpy.array", "max", "numpy.percentile", "range", "max", "numpy.vstack", "len"], "methods", ["None"], ["", "def", "get_target_spacing", "(", "self", ")", ":", "\n", "        ", "\"\"\"\n        per default we use the 50th percentile=median for the target spacing. Higher spacing results in smaller data\n        and thus faster and easier training. Smaller spacing results in larger data and thus longer and harder training\n\n        For some datasets the median is not a good choice. Those are the datasets where the spacing is very anisotropic\n        (for example ACDC with (10, 1.5, 1.5)). These datasets still have examples with a pacing of 5 or 6 mm in the low\n        resolution axis. Choosing the median here will result in bad interpolation artifacts that can substantially\n        impact performance (due to the low number of slices).\n        \"\"\"", "\n", "spacings", "=", "self", ".", "dataset_properties", "[", "'all_spacings'", "]", "\n", "sizes", "=", "self", ".", "dataset_properties", "[", "'all_sizes'", "]", "\n", "\n", "target", "=", "np", ".", "percentile", "(", "np", ".", "vstack", "(", "spacings", ")", ",", "self", ".", "target_spacing_percentile", ",", "0", ")", "\n", "target_size", "=", "np", ".", "percentile", "(", "np", ".", "vstack", "(", "sizes", ")", ",", "self", ".", "target_spacing_percentile", ",", "0", ")", "\n", "target_size_mm", "=", "np", ".", "array", "(", "target", ")", "*", "np", ".", "array", "(", "target_size", ")", "\n", "# we need to identify datasets for which a different target spacing could be beneficial. These datasets have", "\n", "# the following properties:", "\n", "# - one axis which much lower resolution than the others", "\n", "# - the lowres axis has much less voxels than the others", "\n", "# - (the size in mm of the lowres axis is also reduced)", "\n", "worst_spacing_axis", "=", "np", ".", "argmax", "(", "target", ")", "\n", "other_axes", "=", "[", "i", "for", "i", "in", "range", "(", "len", "(", "target", ")", ")", "if", "i", "!=", "worst_spacing_axis", "]", "\n", "other_spacings", "=", "[", "target", "[", "i", "]", "for", "i", "in", "other_axes", "]", "\n", "other_sizes", "=", "[", "target_size", "[", "i", "]", "for", "i", "in", "other_axes", "]", "\n", "\n", "has_aniso_spacing", "=", "target", "[", "worst_spacing_axis", "]", ">", "(", "self", ".", "anisotropy_threshold", "*", "max", "(", "other_spacings", ")", ")", "\n", "has_aniso_voxels", "=", "target_size", "[", "worst_spacing_axis", "]", "*", "self", ".", "anisotropy_threshold", "<", "max", "(", "other_sizes", ")", "\n", "# we don't use the last one for now", "\n", "#median_size_in_mm = target[target_size_mm] * RESAMPLING_SEPARATE_Z_ANISOTROPY_THRESHOLD < max(target_size_mm)", "\n", "\n", "if", "has_aniso_spacing", "and", "has_aniso_voxels", ":", "\n", "            ", "spacings_of_that_axis", "=", "np", ".", "vstack", "(", "spacings", ")", "[", ":", ",", "worst_spacing_axis", "]", "\n", "target_spacing_of_that_axis", "=", "np", ".", "percentile", "(", "spacings_of_that_axis", ",", "10", ")", "\n", "target", "[", "worst_spacing_axis", "]", "=", "target_spacing_of_that_axis", "\n", "", "return", "target", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.pooling_and_convs.experiment_planner_baseline_3DUNet_poolBasedOnSpacing.ExperimentPlannerPoolBasedOnSpacing.__init__": [[25, 30], ["nnunet.experiment_planning.experiment_planner_baseline_3DUNet.ExperimentPlanner.__init__", "join"], "methods", ["home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.common.layers.ResizeImage.__init__"], ["    ", "def", "__init__", "(", "self", ",", "folder_with_cropped_data", ",", "preprocessed_output_folder", ")", ":", "\n", "        ", "super", "(", "ExperimentPlannerPoolBasedOnSpacing", ",", "self", ")", ".", "__init__", "(", "folder_with_cropped_data", ",", "preprocessed_output_folder", ")", "\n", "self", ".", "data_identifier", "=", "\"nnUNetData_poolBasedOnSpacing\"", "\n", "self", ".", "plans_fname", "=", "join", "(", "self", ".", "preprocessed_output_folder", ",", "\n", "\"nnUNetPlans\"", "+", "\"poolBasedOnSpacing_plans_3D.pkl\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.pooling_and_convs.experiment_planner_baseline_3DUNet_poolBasedOnSpacing.ExperimentPlannerPoolBasedOnSpacing.get_properties_for_stage": [[31, 123], ["numpy.round().astype", "numpy.round().astype.mean", "numpy.round().astype", "nnunet.experiment_planning.common_utils.get_pool_and_conv_props", "nnunet.network_architecture.generic_UNet.Generic_UNet.compute_approx_vram_consumption", "int", "numpy.round().astype", "max", "min", "numpy.prod", "numpy.array", "min", "copy.deepcopy", "nnunet.experiment_planning.common_utils.get_pool_and_conv_props", "nnunet.experiment_planning.common_utils.get_pool_and_conv_props", "nnunet.network_architecture.generic_UNet.Generic_UNet.compute_approx_vram_consumption", "print", "numpy.floor", "numpy.round", "min", "numpy.round", "zip", "numpy.argsort", "numpy.round", "max", "max", "numpy.prod"], "methods", ["home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.experiment_planning.common_utils.get_pool_and_conv_props", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.network_architecture.generic_modular_UNet.PlainConvUNet.compute_approx_vram_consumption", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.experiment_planning.common_utils.get_pool_and_conv_props", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.experiment_planning.common_utils.get_pool_and_conv_props", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.network_architecture.generic_modular_UNet.PlainConvUNet.compute_approx_vram_consumption"], ["", "def", "get_properties_for_stage", "(", "self", ",", "current_spacing", ",", "original_spacing", ",", "original_shape", ",", "num_cases", ",", "\n", "num_modalities", ",", "num_classes", ")", ":", "\n", "        ", "\"\"\"\n        ExperimentPlanner configures pooling so that we pool late. Meaning that if the number of pooling per axis is\n        (2, 3, 3), then the first pooling operation will always pool axes 1 and 2 and not 0, irrespective of spacing.\n        This can cause a larger memory footprint, so it can be beneficial to revise this.\n\n        Here we are pooling based on the spacing of the data.\n\n        \"\"\"", "\n", "new_median_shape", "=", "np", ".", "round", "(", "original_spacing", "/", "current_spacing", "*", "original_shape", ")", ".", "astype", "(", "int", ")", "\n", "dataset_num_voxels", "=", "np", ".", "prod", "(", "new_median_shape", ")", "*", "num_cases", "\n", "\n", "# the next line is what we had before as a default. The patch size had the same aspect ratio as the median shape of a patient. We swapped t", "\n", "# input_patch_size = new_median_shape", "\n", "\n", "# compute how many voxels are one mm", "\n", "input_patch_size", "=", "1", "/", "np", ".", "array", "(", "current_spacing", ")", "\n", "\n", "# normalize voxels per mm", "\n", "input_patch_size", "/=", "input_patch_size", ".", "mean", "(", ")", "\n", "\n", "# create an isotropic patch of size 512x512x512mm", "\n", "input_patch_size", "*=", "1", "/", "min", "(", "input_patch_size", ")", "*", "512", "# to get a starting value", "\n", "input_patch_size", "=", "np", ".", "round", "(", "input_patch_size", ")", ".", "astype", "(", "int", ")", "\n", "\n", "# clip it to the median shape of the dataset because patches larger then that make not much sense", "\n", "input_patch_size", "=", "[", "min", "(", "i", ",", "j", ")", "for", "i", ",", "j", "in", "zip", "(", "input_patch_size", ",", "new_median_shape", ")", "]", "\n", "\n", "network_num_pool_per_axis", ",", "pool_op_kernel_sizes", ",", "conv_kernel_sizes", ",", "new_shp", ",", "shape_must_be_divisible_by", "=", "get_pool_and_conv_props", "(", "current_spacing", ",", "input_patch_size", ",", "\n", "self", ".", "unet_featuremap_min_edge_length", ",", "\n", "self", ".", "unet_max_numpool", ")", "\n", "\n", "ref", "=", "Generic_UNet", ".", "use_this_for_batch_size_computation_3D", "\n", "here", "=", "Generic_UNet", ".", "compute_approx_vram_consumption", "(", "new_shp", ",", "network_num_pool_per_axis", ",", "\n", "self", ".", "unet_base_num_features", ",", "\n", "self", ".", "unet_max_num_filters", ",", "num_modalities", ",", "\n", "num_classes", ",", "\n", "pool_op_kernel_sizes", ",", "conv_per_stage", "=", "self", ".", "conv_per_stage", ")", "\n", "while", "here", ">", "ref", ":", "\n", "            ", "axis_to_be_reduced", "=", "np", ".", "argsort", "(", "new_shp", "/", "new_median_shape", ")", "[", "-", "1", "]", "\n", "\n", "tmp", "=", "deepcopy", "(", "new_shp", ")", "\n", "tmp", "[", "axis_to_be_reduced", "]", "-=", "shape_must_be_divisible_by", "[", "axis_to_be_reduced", "]", "\n", "_", ",", "_", ",", "_", ",", "_", ",", "shape_must_be_divisible_by_new", "=", "get_pool_and_conv_props", "(", "current_spacing", ",", "tmp", ",", "\n", "self", ".", "unet_featuremap_min_edge_length", ",", "\n", "self", ".", "unet_max_numpool", ",", "\n", ")", "\n", "new_shp", "[", "axis_to_be_reduced", "]", "-=", "shape_must_be_divisible_by_new", "[", "axis_to_be_reduced", "]", "\n", "\n", "# we have to recompute numpool now:", "\n", "network_num_pool_per_axis", ",", "pool_op_kernel_sizes", ",", "conv_kernel_sizes", ",", "new_shp", ",", "shape_must_be_divisible_by", "=", "get_pool_and_conv_props", "(", "current_spacing", ",", "new_shp", ",", "\n", "self", ".", "unet_featuremap_min_edge_length", ",", "\n", "self", ".", "unet_max_numpool", ",", "\n", ")", "\n", "\n", "here", "=", "Generic_UNet", ".", "compute_approx_vram_consumption", "(", "new_shp", ",", "network_num_pool_per_axis", ",", "\n", "self", ".", "unet_base_num_features", ",", "\n", "self", ".", "unet_max_num_filters", ",", "num_modalities", ",", "\n", "num_classes", ",", "pool_op_kernel_sizes", ",", "\n", "conv_per_stage", "=", "self", ".", "conv_per_stage", ")", "\n", "print", "(", "new_shp", ")", "\n", "\n", "", "input_patch_size", "=", "new_shp", "\n", "\n", "batch_size", "=", "Generic_UNet", ".", "DEFAULT_BATCH_SIZE_3D", "# This is what wirks with 128**3", "\n", "batch_size", "=", "int", "(", "np", ".", "floor", "(", "max", "(", "ref", "/", "here", ",", "1", ")", "*", "batch_size", ")", ")", "\n", "\n", "# check if batch size is too large", "\n", "max_batch_size", "=", "np", ".", "round", "(", "self", ".", "batch_size_covers_max_percent_of_dataset", "*", "dataset_num_voxels", "/", "\n", "np", ".", "prod", "(", "input_patch_size", ",", "dtype", "=", "np", ".", "int64", ")", ")", ".", "astype", "(", "int", ")", "\n", "max_batch_size", "=", "max", "(", "max_batch_size", ",", "self", ".", "unet_min_batch_size", ")", "\n", "batch_size", "=", "min", "(", "batch_size", ",", "max_batch_size", ")", "\n", "\n", "do_dummy_2D_data_aug", "=", "(", "max", "(", "input_patch_size", ")", "/", "input_patch_size", "[", "\n", "0", "]", ")", ">", "self", ".", "anisotropy_threshold", "\n", "\n", "plan", "=", "{", "\n", "'batch_size'", ":", "batch_size", ",", "\n", "'num_pool_per_axis'", ":", "network_num_pool_per_axis", ",", "\n", "'patch_size'", ":", "input_patch_size", ",", "\n", "'median_patient_size_in_voxels'", ":", "new_median_shape", ",", "\n", "'current_spacing'", ":", "current_spacing", ",", "\n", "'original_spacing'", ":", "original_spacing", ",", "\n", "'do_dummy_2D_data_aug'", ":", "do_dummy_2D_data_aug", ",", "\n", "'pool_op_kernel_sizes'", ":", "pool_op_kernel_sizes", ",", "\n", "'conv_kernel_sizes'", ":", "conv_kernel_sizes", ",", "\n", "}", "\n", "return", "plan", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.pooling_and_convs.experiment_planner_baseline_3DUNet_allConv3x3.ExperimentPlannerAllConv3x3.__init__": [[25, 29], ["nnunet.experiment_planning.experiment_planner_baseline_3DUNet.ExperimentPlanner.__init__", "join"], "methods", ["home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.common.layers.ResizeImage.__init__"], ["    ", "def", "__init__", "(", "self", ",", "folder_with_cropped_data", ",", "preprocessed_output_folder", ")", ":", "\n", "        ", "super", "(", "ExperimentPlannerAllConv3x3", ",", "self", ")", ".", "__init__", "(", "folder_with_cropped_data", ",", "preprocessed_output_folder", ")", "\n", "self", ".", "plans_fname", "=", "join", "(", "self", ".", "preprocessed_output_folder", ",", "\n", "\"nnUNetPlans\"", "+", "\"allConv3x3_plans_3D.pkl\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.pooling_and_convs.experiment_planner_baseline_3DUNet_allConv3x3.ExperimentPlannerAllConv3x3.get_properties_for_stage": [[30, 135], ["numpy.round().astype", "numpy.round().astype.mean", "numpy.round().astype", "nnunet.experiment_planning.common_utils.get_pool_and_conv_props_poolLateV2", "nnunet.network_architecture.generic_UNet.Generic_UNet.compute_approx_vram_consumption", "int", "numpy.round().astype", "max", "min", "range", "numpy.prod", "numpy.array", "min", "copy.deepcopy", "nnunet.experiment_planning.common_utils.get_pool_and_conv_props_poolLateV2", "nnunet.experiment_planning.common_utils.get_pool_and_conv_props_poolLateV2", "nnunet.network_architecture.generic_UNet.Generic_UNet.compute_approx_vram_consumption", "print", "numpy.floor", "len", "numpy.round", "min", "numpy.round", "zip", "numpy.argsort", "numpy.round", "max", "max", "numpy.prod"], "methods", ["home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.experiment_planning.common_utils.get_pool_and_conv_props_poolLateV2", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.network_architecture.generic_modular_UNet.PlainConvUNet.compute_approx_vram_consumption", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.experiment_planning.common_utils.get_pool_and_conv_props_poolLateV2", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.experiment_planning.common_utils.get_pool_and_conv_props_poolLateV2", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.network_architecture.generic_modular_UNet.PlainConvUNet.compute_approx_vram_consumption"], ["", "def", "get_properties_for_stage", "(", "self", ",", "current_spacing", ",", "original_spacing", ",", "original_shape", ",", "num_cases", ",", "\n", "num_modalities", ",", "num_classes", ")", ":", "\n", "        ", "\"\"\"\n        Computation of input patch size starts out with the new median shape (in voxels) of a dataset. This is\n        opposed to prior experiments where I based it on the median size in mm. The rationale behind this is that\n        for some organ of interest the acquisition method will most likely be chosen such that the field of view and\n        voxel resolution go hand in hand to show the doctor what they need to see. This assumption may be violated\n        for some modalities with anisotropy (cine MRI) but we will have t live with that. In future experiments I\n        will try to 1) base input patch size match aspect ratio of input size in mm (instead of voxels) and 2) to\n        try to enforce that we see the same 'distance' in all directions (try to maintain equal size in mm of patch)\n\n        The patches created here attempt keep the aspect ratio of the new_median_shape\n\n        :param current_spacing:\n        :param original_spacing:\n        :param original_shape:\n        :param num_cases:\n        :return:\n        \"\"\"", "\n", "new_median_shape", "=", "np", ".", "round", "(", "original_spacing", "/", "current_spacing", "*", "original_shape", ")", ".", "astype", "(", "int", ")", "\n", "dataset_num_voxels", "=", "np", ".", "prod", "(", "new_median_shape", ")", "*", "num_cases", "\n", "\n", "# the next line is what we had before as a default. The patch size had the same aspect ratio as the median shape of a patient. We swapped t", "\n", "# input_patch_size = new_median_shape", "\n", "\n", "# compute how many voxels are one mm", "\n", "input_patch_size", "=", "1", "/", "np", ".", "array", "(", "current_spacing", ")", "\n", "\n", "# normalize voxels per mm", "\n", "input_patch_size", "/=", "input_patch_size", ".", "mean", "(", ")", "\n", "\n", "# create an isotropic patch of size 512x512x512mm", "\n", "input_patch_size", "*=", "1", "/", "min", "(", "input_patch_size", ")", "*", "512", "# to get a starting value", "\n", "input_patch_size", "=", "np", ".", "round", "(", "input_patch_size", ")", ".", "astype", "(", "int", ")", "\n", "\n", "# clip it to the median shape of the dataset because patches larger then that make not much sense", "\n", "input_patch_size", "=", "[", "min", "(", "i", ",", "j", ")", "for", "i", ",", "j", "in", "zip", "(", "input_patch_size", ",", "new_median_shape", ")", "]", "\n", "\n", "network_num_pool_per_axis", ",", "pool_op_kernel_sizes", ",", "conv_kernel_sizes", ",", "new_shp", ",", "shape_must_be_divisible_by", "=", "get_pool_and_conv_props_poolLateV2", "(", "input_patch_size", ",", "\n", "self", ".", "unet_featuremap_min_edge_length", ",", "\n", "self", ".", "unet_max_numpool", ",", "\n", "current_spacing", ")", "\n", "\n", "ref", "=", "Generic_UNet", ".", "use_this_for_batch_size_computation_3D", "\n", "here", "=", "Generic_UNet", ".", "compute_approx_vram_consumption", "(", "new_shp", ",", "network_num_pool_per_axis", ",", "\n", "self", ".", "unet_base_num_features", ",", "\n", "self", ".", "unet_max_num_filters", ",", "num_modalities", ",", "\n", "num_classes", ",", "\n", "pool_op_kernel_sizes", ",", "conv_per_stage", "=", "self", ".", "conv_per_stage", ")", "\n", "while", "here", ">", "ref", ":", "\n", "            ", "axis_to_be_reduced", "=", "np", ".", "argsort", "(", "new_shp", "/", "new_median_shape", ")", "[", "-", "1", "]", "\n", "\n", "tmp", "=", "deepcopy", "(", "new_shp", ")", "\n", "tmp", "[", "axis_to_be_reduced", "]", "-=", "shape_must_be_divisible_by", "[", "axis_to_be_reduced", "]", "\n", "_", ",", "_", ",", "_", ",", "_", ",", "shape_must_be_divisible_by_new", "=", "get_pool_and_conv_props_poolLateV2", "(", "tmp", ",", "\n", "self", ".", "unet_featuremap_min_edge_length", ",", "\n", "self", ".", "unet_max_numpool", ",", "\n", "current_spacing", ")", "\n", "new_shp", "[", "axis_to_be_reduced", "]", "-=", "shape_must_be_divisible_by_new", "[", "axis_to_be_reduced", "]", "\n", "\n", "# we have to recompute numpool now:", "\n", "network_num_pool_per_axis", ",", "pool_op_kernel_sizes", ",", "conv_kernel_sizes", ",", "new_shp", ",", "shape_must_be_divisible_by", "=", "get_pool_and_conv_props_poolLateV2", "(", "new_shp", ",", "\n", "self", ".", "unet_featuremap_min_edge_length", ",", "\n", "self", ".", "unet_max_numpool", ",", "\n", "current_spacing", ")", "\n", "\n", "here", "=", "Generic_UNet", ".", "compute_approx_vram_consumption", "(", "new_shp", ",", "network_num_pool_per_axis", ",", "\n", "self", ".", "unet_base_num_features", ",", "\n", "self", ".", "unet_max_num_filters", ",", "num_modalities", ",", "\n", "num_classes", ",", "pool_op_kernel_sizes", ",", "\n", "conv_per_stage", "=", "self", ".", "conv_per_stage", ")", "\n", "print", "(", "new_shp", ")", "\n", "\n", "", "input_patch_size", "=", "new_shp", "\n", "\n", "batch_size", "=", "Generic_UNet", ".", "DEFAULT_BATCH_SIZE_3D", "# This is what works with 128**3", "\n", "batch_size", "=", "int", "(", "np", ".", "floor", "(", "max", "(", "ref", "/", "here", ",", "1", ")", "*", "batch_size", ")", ")", "\n", "\n", "# check if batch size is too large", "\n", "max_batch_size", "=", "np", ".", "round", "(", "self", ".", "batch_size_covers_max_percent_of_dataset", "*", "dataset_num_voxels", "/", "\n", "np", ".", "prod", "(", "input_patch_size", ",", "dtype", "=", "np", ".", "int64", ")", ")", ".", "astype", "(", "int", ")", "\n", "max_batch_size", "=", "max", "(", "max_batch_size", ",", "self", ".", "unet_min_batch_size", ")", "\n", "batch_size", "=", "min", "(", "batch_size", ",", "max_batch_size", ")", "\n", "\n", "do_dummy_2D_data_aug", "=", "(", "max", "(", "input_patch_size", ")", "/", "input_patch_size", "[", "\n", "0", "]", ")", ">", "self", ".", "anisotropy_threshold", "\n", "\n", "for", "s", "in", "range", "(", "len", "(", "conv_kernel_sizes", ")", ")", ":", "\n", "            ", "conv_kernel_sizes", "[", "s", "]", "=", "[", "3", "for", "_", "in", "conv_kernel_sizes", "[", "s", "]", "]", "\n", "\n", "", "plan", "=", "{", "\n", "'batch_size'", ":", "batch_size", ",", "\n", "'num_pool_per_axis'", ":", "network_num_pool_per_axis", ",", "\n", "'patch_size'", ":", "input_patch_size", ",", "\n", "'median_patient_size_in_voxels'", ":", "new_median_shape", ",", "\n", "'current_spacing'", ":", "current_spacing", ",", "\n", "'original_spacing'", ":", "original_spacing", ",", "\n", "'do_dummy_2D_data_aug'", ":", "do_dummy_2D_data_aug", ",", "\n", "'pool_op_kernel_sizes'", ":", "pool_op_kernel_sizes", ",", "\n", "'conv_kernel_sizes'", ":", "conv_kernel_sizes", ",", "\n", "}", "\n", "return", "plan", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.pooling_and_convs.experiment_planner_baseline_3DUNet_allConv3x3.ExperimentPlannerAllConv3x3.run_preprocessing": [[136, 138], ["None"], "methods", ["None"], ["", "def", "run_preprocessing", "(", "self", ",", "num_threads", ")", ":", "\n", "        ", "pass", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.patch_size.experiment_planner_3DUNet_isotropic_in_mm.ExperimentPlannerIso.__init__": [[32, 36], ["nnunet.experiment_planning.experiment_planner_baseline_3DUNet.ExperimentPlanner.__init__", "join"], "methods", ["home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.common.layers.ResizeImage.__init__"], ["def", "__init__", "(", "self", ",", "folder_with_cropped_data", ",", "preprocessed_output_folder", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "folder_with_cropped_data", ",", "preprocessed_output_folder", ")", "\n", "self", ".", "plans_fname", "=", "join", "(", "self", ".", "preprocessed_output_folder", ",", "\"nnUNetPlans\"", "+", "\"fixedisoPatchesInmm_plans_3D.pkl\"", ")", "\n", "self", ".", "data_identifier", "=", "\"nnUNet_isoPatchesInmm\"", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.patch_size.experiment_planner_3DUNet_isotropic_in_mm.ExperimentPlannerIso.get_properties_for_stage": [[37, 129], ["numpy.round().astype", "numpy.round().astype.mean", "numpy.round().astype", "nnunet.experiment_planning.common_utils.get_pool_and_conv_props_poolLateV2", "nnunet.network_architecture.generic_UNet.Generic_UNet.compute_approx_vram_consumption", "int", "numpy.round().astype", "max", "min", "numpy.prod", "numpy.array", "min", "copy.deepcopy", "nnunet.experiment_planning.common_utils.get_pool_and_conv_props_poolLateV2", "nnunet.experiment_planning.common_utils.get_pool_and_conv_props_poolLateV2", "nnunet.network_architecture.generic_UNet.Generic_UNet.compute_approx_vram_consumption", "print", "numpy.floor", "numpy.round", "min", "numpy.round", "zip", "numpy.argsort", "numpy.round", "max", "max", "numpy.prod"], "methods", ["home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.experiment_planning.common_utils.get_pool_and_conv_props_poolLateV2", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.network_architecture.generic_modular_UNet.PlainConvUNet.compute_approx_vram_consumption", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.experiment_planning.common_utils.get_pool_and_conv_props_poolLateV2", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.experiment_planning.common_utils.get_pool_and_conv_props_poolLateV2", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.network_architecture.generic_modular_UNet.PlainConvUNet.compute_approx_vram_consumption"], ["", "def", "get_properties_for_stage", "(", "self", ",", "current_spacing", ",", "original_spacing", ",", "original_shape", ",", "num_cases", ",", "\n", "num_modalities", ",", "num_classes", ")", ":", "\n", "        ", "\"\"\"\n        \"\"\"", "\n", "new_median_shape", "=", "np", ".", "round", "(", "original_spacing", "/", "current_spacing", "*", "original_shape", ")", ".", "astype", "(", "int", ")", "\n", "dataset_num_voxels", "=", "np", ".", "prod", "(", "new_median_shape", ")", "*", "num_cases", "\n", "\n", "# the next line is what we had before as a default. The patch size had the same aspect ratio as the median shape of a patient. We swapped t", "\n", "# input_patch_size = new_median_shape", "\n", "\n", "# compute how many voxels are one mm", "\n", "input_patch_size", "=", "1", "/", "np", ".", "array", "(", "current_spacing", ")", "\n", "\n", "# normalize voxels per mm", "\n", "input_patch_size", "/=", "input_patch_size", ".", "mean", "(", ")", "\n", "\n", "# create an isotropic patch of size 512x512x512mm", "\n", "input_patch_size", "*=", "1", "/", "min", "(", "input_patch_size", ")", "*", "512", "# to get a starting value", "\n", "input_patch_size", "=", "np", ".", "round", "(", "input_patch_size", ")", ".", "astype", "(", "int", ")", "\n", "\n", "# clip it to the median shape of the dataset because patches larger then that make not much sense", "\n", "input_patch_size", "=", "[", "min", "(", "i", ",", "j", ")", "for", "i", ",", "j", "in", "zip", "(", "input_patch_size", ",", "new_median_shape", ")", "]", "\n", "\n", "network_num_pool_per_axis", ",", "pool_op_kernel_sizes", ",", "conv_kernel_sizes", ",", "new_shp", ",", "shape_must_be_divisible_by", "=", "get_pool_and_conv_props_poolLateV2", "(", "input_patch_size", ",", "\n", "self", ".", "unet_featuremap_min_edge_length", ",", "\n", "self", ".", "unet_max_numpool", ",", "\n", "current_spacing", ")", "\n", "\n", "ref", "=", "Generic_UNet", ".", "use_this_for_batch_size_computation_3D", "\n", "here", "=", "Generic_UNet", ".", "compute_approx_vram_consumption", "(", "new_shp", ",", "network_num_pool_per_axis", ",", "\n", "self", ".", "unet_base_num_features", ",", "\n", "self", ".", "unet_max_num_filters", ",", "num_modalities", ",", "\n", "num_classes", ",", "\n", "pool_op_kernel_sizes", ",", "conv_per_stage", "=", "self", ".", "conv_per_stage", ")", "\n", "while", "here", ">", "ref", ":", "\n", "# here is the difference to ExperimentPlanner. In the old version we made the aspect ratio match", "\n", "# between patch and new_median_shape, regardless of spacing. It could be better to enforce isotropy", "\n", "# (in mm) instead", "\n", "            ", "current_patch_in_mm", "=", "new_shp", "*", "current_spacing", "\n", "axis_to_be_reduced", "=", "np", ".", "argsort", "(", "current_patch_in_mm", ")", "[", "-", "1", "]", "\n", "\n", "# from here on it's the same as before", "\n", "tmp", "=", "deepcopy", "(", "new_shp", ")", "\n", "tmp", "[", "axis_to_be_reduced", "]", "-=", "shape_must_be_divisible_by", "[", "axis_to_be_reduced", "]", "\n", "_", ",", "_", ",", "_", ",", "_", ",", "shape_must_be_divisible_by_new", "=", "get_pool_and_conv_props_poolLateV2", "(", "tmp", ",", "\n", "self", ".", "unet_featuremap_min_edge_length", ",", "\n", "self", ".", "unet_max_numpool", ",", "\n", "current_spacing", ")", "\n", "new_shp", "[", "axis_to_be_reduced", "]", "-=", "shape_must_be_divisible_by_new", "[", "axis_to_be_reduced", "]", "\n", "\n", "# we have to recompute numpool now:", "\n", "network_num_pool_per_axis", ",", "pool_op_kernel_sizes", ",", "conv_kernel_sizes", ",", "new_shp", ",", "shape_must_be_divisible_by", "=", "get_pool_and_conv_props_poolLateV2", "(", "new_shp", ",", "\n", "self", ".", "unet_featuremap_min_edge_length", ",", "\n", "self", ".", "unet_max_numpool", ",", "\n", "current_spacing", ")", "\n", "\n", "here", "=", "Generic_UNet", ".", "compute_approx_vram_consumption", "(", "new_shp", ",", "network_num_pool_per_axis", ",", "\n", "self", ".", "unet_base_num_features", ",", "\n", "self", ".", "unet_max_num_filters", ",", "num_modalities", ",", "\n", "num_classes", ",", "pool_op_kernel_sizes", ",", "\n", "conv_per_stage", "=", "self", ".", "conv_per_stage", ")", "\n", "print", "(", "new_shp", ")", "\n", "\n", "", "input_patch_size", "=", "new_shp", "\n", "\n", "batch_size", "=", "Generic_UNet", ".", "DEFAULT_BATCH_SIZE_3D", "# This is what works with 128**3", "\n", "batch_size", "=", "int", "(", "np", ".", "floor", "(", "max", "(", "ref", "/", "here", ",", "1", ")", "*", "batch_size", ")", ")", "\n", "\n", "# check if batch size is too large", "\n", "max_batch_size", "=", "np", ".", "round", "(", "self", ".", "batch_size_covers_max_percent_of_dataset", "*", "dataset_num_voxels", "/", "\n", "np", ".", "prod", "(", "input_patch_size", ",", "dtype", "=", "np", ".", "int64", ")", ")", ".", "astype", "(", "int", ")", "\n", "max_batch_size", "=", "max", "(", "max_batch_size", ",", "self", ".", "unet_min_batch_size", ")", "\n", "batch_size", "=", "min", "(", "batch_size", ",", "max_batch_size", ")", "\n", "\n", "do_dummy_2D_data_aug", "=", "(", "max", "(", "input_patch_size", ")", "/", "input_patch_size", "[", "\n", "0", "]", ")", ">", "self", ".", "anisotropy_threshold", "\n", "\n", "plan", "=", "{", "\n", "'batch_size'", ":", "batch_size", ",", "\n", "'num_pool_per_axis'", ":", "network_num_pool_per_axis", ",", "\n", "'patch_size'", ":", "input_patch_size", ",", "\n", "'median_patient_size_in_voxels'", ":", "new_median_shape", ",", "\n", "'current_spacing'", ":", "current_spacing", ",", "\n", "'original_spacing'", ":", "original_spacing", ",", "\n", "'do_dummy_2D_data_aug'", ":", "do_dummy_2D_data_aug", ",", "\n", "'pool_op_kernel_sizes'", ":", "pool_op_kernel_sizes", ",", "\n", "'conv_kernel_sizes'", ":", "conv_kernel_sizes", ",", "\n", "}", "\n", "return", "plan", "\n", "", "", ""]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.patch_size.experiment_planner_3DUNet_isotropic_in_voxels.ExperimentPlanner3D_IsoPatchesInVoxels.__init__": [[33, 37], ["nnunet.experiment_planning.experiment_planner_baseline_3DUNet.ExperimentPlanner.__init__", "join"], "methods", ["home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.common.layers.ResizeImage.__init__"], ["def", "__init__", "(", "self", ",", "folder_with_cropped_data", ",", "preprocessed_output_folder", ")", ":", "\n", "        ", "super", "(", "ExperimentPlanner3D_IsoPatchesInVoxels", ",", "self", ")", ".", "__init__", "(", "folder_with_cropped_data", ",", "preprocessed_output_folder", ")", "\n", "self", ".", "data_identifier", "=", "\"nnUNetData_isoPatchesInVoxels\"", "\n", "self", ".", "plans_fname", "=", "join", "(", "self", ".", "preprocessed_output_folder", ",", "\"nnUNetPlans\"", "+", "\"fixedisoPatchesInVoxels_plans_3D.pkl\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.patch_size.experiment_planner_3DUNet_isotropic_in_voxels.ExperimentPlanner3D_IsoPatchesInVoxels.get_properties_for_stage": [[38, 115], ["numpy.round().astype", "nnunet.experiment_planning.common_utils.get_pool_and_conv_props_poolLateV2", "nnunet.network_architecture.generic_UNet.Generic_UNet.compute_approx_vram_consumption", "int", "numpy.round().astype", "max", "min", "numpy.prod", "copy.deepcopy", "nnunet.experiment_planning.common_utils.get_pool_and_conv_props_poolLateV2", "nnunet.experiment_planning.common_utils.get_pool_and_conv_props_poolLateV2", "nnunet.network_architecture.generic_UNet.Generic_UNet.compute_approx_vram_consumption", "print", "numpy.floor", "numpy.round", "len", "numpy.round", "max", "numpy.unique", "numpy.argsort", "numpy.argsort", "max", "numpy.prod"], "methods", ["home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.experiment_planning.common_utils.get_pool_and_conv_props_poolLateV2", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.network_architecture.generic_modular_UNet.PlainConvUNet.compute_approx_vram_consumption", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.experiment_planning.common_utils.get_pool_and_conv_props_poolLateV2", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.experiment_planning.common_utils.get_pool_and_conv_props_poolLateV2", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.network_architecture.generic_modular_UNet.PlainConvUNet.compute_approx_vram_consumption"], ["", "def", "get_properties_for_stage", "(", "self", ",", "current_spacing", ",", "original_spacing", ",", "original_shape", ",", "num_cases", ",", "\n", "num_modalities", ",", "num_classes", ")", ":", "\n", "        ", "\"\"\"\n        \"\"\"", "\n", "new_median_shape", "=", "np", ".", "round", "(", "original_spacing", "/", "current_spacing", "*", "original_shape", ")", ".", "astype", "(", "int", ")", "\n", "dataset_num_voxels", "=", "np", ".", "prod", "(", "new_median_shape", ")", "*", "num_cases", "\n", "\n", "input_patch_size", "=", "new_median_shape", "\n", "\n", "network_num_pool_per_axis", ",", "pool_op_kernel_sizes", ",", "conv_kernel_sizes", ",", "new_shp", ",", "shape_must_be_divisible_by", "=", "get_pool_and_conv_props_poolLateV2", "(", "input_patch_size", ",", "\n", "self", ".", "unet_featuremap_min_edge_length", ",", "\n", "self", ".", "unet_max_numpool", ",", "\n", "current_spacing", ")", "\n", "\n", "ref", "=", "Generic_UNet", ".", "use_this_for_batch_size_computation_3D", "\n", "here", "=", "Generic_UNet", ".", "compute_approx_vram_consumption", "(", "new_shp", ",", "network_num_pool_per_axis", ",", "\n", "self", ".", "unet_base_num_features", ",", "\n", "self", ".", "unet_max_num_filters", ",", "num_modalities", ",", "\n", "num_classes", ",", "\n", "pool_op_kernel_sizes", ",", "conv_per_stage", "=", "self", ".", "conv_per_stage", ")", "\n", "while", "here", ">", "ref", ":", "\n", "# find the largest axis. If patch is isotropic, pick the axis with the largest spacing", "\n", "            ", "if", "len", "(", "np", ".", "unique", "(", "new_shp", ")", ")", "==", "1", ":", "\n", "                ", "axis_to_be_reduced", "=", "np", ".", "argsort", "(", "current_spacing", ")", "[", "-", "1", "]", "\n", "", "else", ":", "\n", "                ", "axis_to_be_reduced", "=", "np", ".", "argsort", "(", "new_shp", ")", "[", "-", "1", "]", "\n", "\n", "", "tmp", "=", "deepcopy", "(", "new_shp", ")", "\n", "tmp", "[", "axis_to_be_reduced", "]", "-=", "shape_must_be_divisible_by", "[", "axis_to_be_reduced", "]", "\n", "_", ",", "_", ",", "_", ",", "_", ",", "shape_must_be_divisible_by_new", "=", "get_pool_and_conv_props_poolLateV2", "(", "tmp", ",", "\n", "self", ".", "unet_featuremap_min_edge_length", ",", "\n", "self", ".", "unet_max_numpool", ",", "\n", "current_spacing", ")", "\n", "new_shp", "[", "axis_to_be_reduced", "]", "-=", "shape_must_be_divisible_by_new", "[", "axis_to_be_reduced", "]", "\n", "\n", "# we have to recompute numpool now:", "\n", "network_num_pool_per_axis", ",", "pool_op_kernel_sizes", ",", "conv_kernel_sizes", ",", "new_shp", ",", "shape_must_be_divisible_by", "=", "get_pool_and_conv_props_poolLateV2", "(", "new_shp", ",", "\n", "self", ".", "unet_featuremap_min_edge_length", ",", "\n", "self", ".", "unet_max_numpool", ",", "\n", "current_spacing", ")", "\n", "\n", "here", "=", "Generic_UNet", ".", "compute_approx_vram_consumption", "(", "new_shp", ",", "network_num_pool_per_axis", ",", "\n", "self", ".", "unet_base_num_features", ",", "\n", "self", ".", "unet_max_num_filters", ",", "num_modalities", ",", "\n", "num_classes", ",", "pool_op_kernel_sizes", ",", "\n", "conv_per_stage", "=", "self", ".", "conv_per_stage", ")", "\n", "print", "(", "new_shp", ")", "\n", "\n", "", "input_patch_size", "=", "new_shp", "\n", "\n", "batch_size", "=", "Generic_UNet", ".", "DEFAULT_BATCH_SIZE_3D", "# This is what works with 128**3", "\n", "batch_size", "=", "int", "(", "np", ".", "floor", "(", "max", "(", "ref", "/", "here", ",", "1", ")", "*", "batch_size", ")", ")", "\n", "\n", "# check if batch size is too large", "\n", "max_batch_size", "=", "np", ".", "round", "(", "self", ".", "batch_size_covers_max_percent_of_dataset", "*", "dataset_num_voxels", "/", "\n", "np", ".", "prod", "(", "input_patch_size", ",", "dtype", "=", "np", ".", "int64", ")", ")", ".", "astype", "(", "int", ")", "\n", "max_batch_size", "=", "max", "(", "max_batch_size", ",", "self", ".", "unet_min_batch_size", ")", "\n", "batch_size", "=", "min", "(", "batch_size", ",", "max_batch_size", ")", "\n", "\n", "do_dummy_2D_data_aug", "=", "(", "max", "(", "input_patch_size", ")", "/", "input_patch_size", "[", "\n", "0", "]", ")", ">", "self", ".", "anisotropy_threshold", "\n", "\n", "plan", "=", "{", "\n", "'batch_size'", ":", "batch_size", ",", "\n", "'num_pool_per_axis'", ":", "network_num_pool_per_axis", ",", "\n", "'patch_size'", ":", "input_patch_size", ",", "\n", "'median_patient_size_in_voxels'", ":", "new_median_shape", ",", "\n", "'current_spacing'", ":", "current_spacing", ",", "\n", "'original_spacing'", ":", "original_spacing", ",", "\n", "'do_dummy_2D_data_aug'", ":", "do_dummy_2D_data_aug", ",", "\n", "'pool_op_kernel_sizes'", ":", "pool_op_kernel_sizes", ",", "\n", "'conv_kernel_sizes'", ":", "conv_kernel_sizes", ",", "\n", "}", "\n", "return", "plan", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.preprocessing.sanity_checks.verify_all_same_orientation": [[25, 43], ["subfiles", "numpy.array", "numpy.unique", "nibabel.load", "nibabel.aff2axcodes", "np.array.append", "len"], "function", ["None"], ["def", "verify_all_same_orientation", "(", "folder", ")", ":", "\n", "    ", "\"\"\"\n    This should run after cropping\n    :param folder:\n    :return:\n    \"\"\"", "\n", "nii_files", "=", "subfiles", "(", "folder", ",", "suffix", "=", "\".nii.gz\"", ",", "join", "=", "True", ")", "\n", "orientations", "=", "[", "]", "\n", "for", "n", "in", "nii_files", ":", "\n", "        ", "img", "=", "nib", ".", "load", "(", "n", ")", "\n", "affine", "=", "img", ".", "affine", "\n", "orientation", "=", "nib", ".", "aff2axcodes", "(", "affine", ")", "\n", "orientations", ".", "append", "(", "orientation", ")", "\n", "# now we need to check whether they are all the same", "\n", "", "orientations", "=", "np", ".", "array", "(", "orientations", ")", "\n", "unique_orientations", "=", "np", ".", "unique", "(", "orientations", ",", "axis", "=", "0", ")", "\n", "all_same", "=", "len", "(", "unique_orientations", ")", "==", "1", "\n", "return", "all_same", ",", "unique_orientations", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.preprocessing.sanity_checks.verify_same_geometry": [[45, 77], ["numpy.all", "numpy.all", "numpy.all", "numpy.all", "img_1.GetOrigin", "img_1.GetSpacing", "img_1.GetDirection", "img_1.GetSize", "img_2.GetOrigin", "img_2.GetSpacing", "img_2.GetDirection", "img_2.GetSize", "numpy.isclose", "print", "print", "print", "numpy.isclose", "print", "print", "print", "numpy.isclose", "print", "print", "print", "numpy.isclose", "print", "print", "print"], "function", ["None"], ["", "def", "verify_same_geometry", "(", "img_1", ":", "sitk", ".", "Image", ",", "img_2", ":", "sitk", ".", "Image", ")", ":", "\n", "    ", "ori1", ",", "spacing1", ",", "direction1", ",", "size1", "=", "img_1", ".", "GetOrigin", "(", ")", ",", "img_1", ".", "GetSpacing", "(", ")", ",", "img_1", ".", "GetDirection", "(", ")", ",", "img_1", ".", "GetSize", "(", ")", "\n", "ori2", ",", "spacing2", ",", "direction2", ",", "size2", "=", "img_2", ".", "GetOrigin", "(", ")", ",", "img_2", ".", "GetSpacing", "(", ")", ",", "img_2", ".", "GetDirection", "(", ")", ",", "img_2", ".", "GetSize", "(", ")", "\n", "\n", "same_ori", "=", "np", ".", "all", "(", "np", ".", "isclose", "(", "ori1", ",", "ori2", ")", ")", "\n", "if", "not", "same_ori", ":", "\n", "        ", "print", "(", "\"the origin does not match between the images:\"", ")", "\n", "print", "(", "ori1", ")", "\n", "print", "(", "ori2", ")", "\n", "\n", "", "same_spac", "=", "np", ".", "all", "(", "np", ".", "isclose", "(", "spacing1", ",", "spacing2", ")", ")", "\n", "if", "not", "same_spac", ":", "\n", "        ", "print", "(", "\"the spacing does not match between the images\"", ")", "\n", "print", "(", "spacing1", ")", "\n", "print", "(", "spacing2", ")", "\n", "\n", "", "same_dir", "=", "np", ".", "all", "(", "np", ".", "isclose", "(", "direction1", ",", "direction2", ")", ")", "\n", "if", "not", "same_dir", ":", "\n", "        ", "print", "(", "\"the direction does not match between the images\"", ")", "\n", "print", "(", "direction1", ")", "\n", "print", "(", "direction2", ")", "\n", "\n", "", "same_size", "=", "np", ".", "all", "(", "np", ".", "isclose", "(", "size1", ",", "size2", ")", ")", "\n", "if", "not", "same_size", ":", "\n", "        ", "print", "(", "\"the size does not match between the images\"", ")", "\n", "print", "(", "size1", ")", "\n", "print", "(", "size2", ")", "\n", "\n", "", "if", "same_ori", "and", "same_spac", "and", "same_dir", "and", "same_size", ":", "\n", "        ", "return", "True", "\n", "", "else", ":", "\n", "        ", "return", "False", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.preprocessing.sanity_checks.verify_contains_only_expected_labels": [[79, 88], ["SimpleITK.GetArrayFromImage", "numpy.unique", "SimpleITK.ReadImage", "len"], "function", ["None"], ["", "", "def", "verify_contains_only_expected_labels", "(", "itk_img", ":", "str", ",", "valid_labels", ":", "(", "tuple", ",", "list", ")", ")", ":", "\n", "    ", "img_npy", "=", "sitk", ".", "GetArrayFromImage", "(", "sitk", ".", "ReadImage", "(", "itk_img", ")", ")", "\n", "uniques", "=", "np", ".", "unique", "(", "img_npy", ")", "\n", "invalid_uniques", "=", "[", "i", "for", "i", "in", "uniques", "if", "i", "not", "in", "valid_labels", "]", "\n", "if", "len", "(", "invalid_uniques", ")", "==", "0", ":", "\n", "        ", "r", "=", "True", "\n", "", "else", ":", "\n", "        ", "r", "=", "False", "\n", "", "return", "r", ",", "invalid_uniques", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.preprocessing.sanity_checks.verify_dataset_integrity": [[90, 229], ["isfile", "isdir", "isdir", "load_json", "len", "subfiles", "subfiles", "print", "print", "list", "multiprocessing.Pool", "multiprocessing.Pool.starmap", "multiprocessing.Pool.close", "multiprocessing.Pool.join", "print", "enumerate", "sanity_checks.verify_all_same_orientation", "join", "join", "join", "join", "dataset[].keys", "join", "join", "len", "len", "RuntimeError", "print", "join", "label_files.append", "isfile", "all", "SimpleITK.ReadImage", "numpy.any", "enumerate", "subfiles.remove", "len", "len", "zip", "AssertionError", "print", "len", "print", "subfiles", "join", "print", "Warning", "print", "RuntimeError", "numpy.unique", "join", "numpy.isnan", "print", "SimpleITK.ReadImage", "numpy.any", "sanity_checks.verify_same_geometry", "subfiles.remove", "os.path.basename", "int", "print", "join", "all", "len", "i[].split", "i.split", "range", "isfile", "SimpleITK.GetArrayFromImage", "numpy.isnan", "print", "print", "os.path.basename", "dataset[].keys", "len", "join", "enumerate", "subfiles.remove", "SimpleITK.GetArrayFromImage", "range", "isfile", "SimpleITK.ReadImage", "sanity_checks.verify_same_geometry", "os.path.basename"], "function", ["home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.preprocessing.sanity_checks.verify_all_same_orientation", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.preprocessing.sanity_checks.verify_same_geometry", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.preprocessing.sanity_checks.verify_same_geometry"], ["", "def", "verify_dataset_integrity", "(", "folder", ")", ":", "\n", "    ", "\"\"\"\n    folder needs the imagesTr, imagesTs and labelsTr subfolders. There also needs to be a dataset.json\n    checks if all training cases and labels are present\n    checks if all test cases (if any) are present\n    for each case, checks whether all modalities apre present\n    for each case, checks whether the pixel grids are aligned\n    checks whether the labels really only contain values they should\n    :param folder:\n    :return:\n    \"\"\"", "\n", "assert", "isfile", "(", "join", "(", "folder", ",", "\"dataset.json\"", ")", ")", ",", "\"There needs to be a dataset.json file in folder, folder=%s\"", "%", "folder", "\n", "assert", "isdir", "(", "join", "(", "folder", ",", "\"imagesTr\"", ")", ")", ",", "\"There needs to be a imagesTr subfolder in folder, folder=%s\"", "%", "folder", "\n", "assert", "isdir", "(", "join", "(", "folder", ",", "\"labelsTr\"", ")", ")", ",", "\"There needs to be a labelsTr subfolder in folder, folder=%s\"", "%", "folder", "\n", "dataset", "=", "load_json", "(", "join", "(", "folder", ",", "\"dataset.json\"", ")", ")", "\n", "training_cases", "=", "dataset", "[", "'training'", "]", "\n", "num_modalities", "=", "len", "(", "dataset", "[", "'modality'", "]", ".", "keys", "(", ")", ")", "\n", "test_cases", "=", "dataset", "[", "'test'", "]", "\n", "expected_train_identifiers", "=", "[", "i", "[", "'image'", "]", ".", "split", "(", "\"/\"", ")", "[", "-", "1", "]", "[", ":", "-", "7", "]", "for", "i", "in", "training_cases", "]", "\n", "expected_test_identifiers", "=", "[", "i", ".", "split", "(", "\"/\"", ")", "[", "-", "1", "]", "[", ":", "-", "7", "]", "for", "i", "in", "test_cases", "]", "\n", "\n", "## check training set", "\n", "nii_files_in_imagesTr", "=", "subfiles", "(", "(", "join", "(", "folder", ",", "\"imagesTr\"", ")", ")", ",", "suffix", "=", "\".nii.gz\"", ",", "join", "=", "False", ")", "\n", "nii_files_in_labelsTr", "=", "subfiles", "(", "(", "join", "(", "folder", ",", "\"labelsTr\"", ")", ")", ",", "suffix", "=", "\".nii.gz\"", ",", "join", "=", "False", ")", "\n", "\n", "label_files", "=", "[", "]", "\n", "geometries_OK", "=", "True", "\n", "has_nan", "=", "False", "\n", "\n", "# check all cases", "\n", "if", "len", "(", "expected_train_identifiers", ")", "!=", "len", "(", "np", ".", "unique", "(", "expected_train_identifiers", ")", ")", ":", "raise", "RuntimeError", "(", "\"found duplicate training cases in dataset.json\"", ")", "\n", "\n", "print", "(", "\"Verifying training set\"", ")", "\n", "for", "c", "in", "expected_train_identifiers", ":", "\n", "        ", "print", "(", "\"checking case\"", ",", "c", ")", "\n", "# check if all files are present", "\n", "expected_label_file", "=", "join", "(", "folder", ",", "\"labelsTr\"", ",", "c", "+", "\".nii.gz\"", ")", "\n", "label_files", ".", "append", "(", "expected_label_file", ")", "\n", "expected_image_files", "=", "[", "join", "(", "folder", ",", "\"imagesTr\"", ",", "c", "+", "\"_%04.0d.nii.gz\"", "%", "i", ")", "for", "i", "in", "range", "(", "num_modalities", ")", "]", "\n", "assert", "isfile", "(", "expected_label_file", ")", ",", "\"could not find label file for case %s. Expected file: \\n%s\"", "%", "(", "\n", "c", ",", "expected_label_file", ")", "\n", "assert", "all", "(", "[", "isfile", "(", "i", ")", "for", "i", "in", "\n", "expected_image_files", "]", ")", ",", "\"some image files are missing for case %s. Expected files:\\n %s\"", "%", "(", "\n", "c", ",", "expected_image_files", ")", "\n", "\n", "# verify that all modalities and the label have the same shape and geometry.", "\n", "label_itk", "=", "sitk", ".", "ReadImage", "(", "expected_label_file", ")", "\n", "\n", "nans_in_seg", "=", "np", ".", "any", "(", "np", ".", "isnan", "(", "sitk", ".", "GetArrayFromImage", "(", "label_itk", ")", ")", ")", "\n", "has_nan", "=", "has_nan", "|", "nans_in_seg", "\n", "if", "nans_in_seg", ":", "\n", "            ", "print", "(", "\"There are NAN values in segmentation %s\"", "%", "expected_label_file", ")", "\n", "\n", "", "images_itk", "=", "[", "sitk", ".", "ReadImage", "(", "i", ")", "for", "i", "in", "expected_image_files", "]", "\n", "for", "i", ",", "img", "in", "enumerate", "(", "images_itk", ")", ":", "\n", "            ", "nans_in_image", "=", "np", ".", "any", "(", "np", ".", "isnan", "(", "sitk", ".", "GetArrayFromImage", "(", "img", ")", ")", ")", "\n", "has_nan", "=", "has_nan", "|", "nans_in_image", "\n", "same_geometry", "=", "verify_same_geometry", "(", "img", ",", "label_itk", ")", "\n", "if", "not", "same_geometry", ":", "\n", "                ", "geometries_OK", "=", "False", "\n", "print", "(", "\"The geometry of the image %s does not match the geometry of the label file. The pixel arrays \"", "\n", "\"will not be aligned and nnU-Net cannot use this data. Please make sure your image modalities \"", "\n", "\"are coregistered and have the same geometry as the label\"", "%", "expected_image_files", "[", "0", "]", "[", ":", "-", "12", "]", ")", "\n", "", "if", "nans_in_image", ":", "\n", "                ", "print", "(", "\"There are NAN values in image %s\"", "%", "expected_image_files", "[", "i", "]", ")", "\n", "\n", "# now remove checked files from the lists nii_files_in_imagesTr and nii_files_in_labelsTr", "\n", "", "", "for", "i", "in", "expected_image_files", ":", "\n", "            ", "nii_files_in_imagesTr", ".", "remove", "(", "os", ".", "path", ".", "basename", "(", "i", ")", ")", "\n", "", "nii_files_in_labelsTr", ".", "remove", "(", "os", ".", "path", ".", "basename", "(", "expected_label_file", ")", ")", "\n", "\n", "# check for stragglers", "\n", "", "assert", "len", "(", "\n", "nii_files_in_imagesTr", ")", "==", "0", ",", "\"there are training cases in imagesTr that are not listed in dataset.json: %s\"", "%", "nii_files_in_imagesTr", "\n", "assert", "len", "(", "\n", "nii_files_in_labelsTr", ")", "==", "0", ",", "\"there are training cases in labelsTr that are not listed in dataset.json: %s\"", "%", "nii_files_in_labelsTr", "\n", "\n", "# verify that only properly declared values are present in the labels", "\n", "print", "(", "\"Verifying label values\"", ")", "\n", "expected_labels", "=", "list", "(", "int", "(", "i", ")", "for", "i", "in", "dataset", "[", "'labels'", "]", ".", "keys", "(", ")", ")", "\n", "p", "=", "Pool", "(", "default_num_threads", ")", "\n", "results", "=", "p", ".", "starmap", "(", "verify_contains_only_expected_labels", ",", "zip", "(", "label_files", ",", "[", "expected_labels", "]", "*", "len", "(", "label_files", ")", ")", ")", "\n", "p", ".", "close", "(", ")", "\n", "p", ".", "join", "(", ")", "\n", "\n", "fail", "=", "False", "\n", "print", "(", "\"Expected label values are\"", ",", "expected_labels", ")", "\n", "for", "i", ",", "r", "in", "enumerate", "(", "results", ")", ":", "\n", "        ", "if", "not", "r", "[", "0", "]", ":", "\n", "            ", "print", "(", "\"Unexpected labels found in file %s. Found these unexpected values (they should not be there) %s\"", "%", "(", "\n", "label_files", "[", "i", "]", ",", "r", "[", "1", "]", ")", ")", "\n", "fail", "=", "True", "\n", "\n", "", "", "if", "fail", ":", "\n", "        ", "raise", "AssertionError", "(", "\n", "\"Found unexpected labels in the training dataset. Please correct that or adjust your dataset.json accordingly\"", ")", "\n", "", "else", ":", "\n", "        ", "print", "(", "\"Labels OK\"", ")", "\n", "\n", "# check test set, but only if there actually is a test set", "\n", "", "if", "len", "(", "expected_test_identifiers", ")", ">", "0", ":", "\n", "        ", "print", "(", "\"Verifying test set\"", ")", "\n", "nii_files_in_imagesTs", "=", "subfiles", "(", "(", "join", "(", "folder", ",", "\"imagesTs\"", ")", ")", ",", "suffix", "=", "\".nii.gz\"", ",", "join", "=", "False", ")", "\n", "\n", "for", "c", "in", "expected_test_identifiers", ":", "\n", "# check if all files are present", "\n", "            ", "expected_image_files", "=", "[", "join", "(", "folder", ",", "\"imagesTs\"", ",", "c", "+", "\"_%04.0d.nii.gz\"", "%", "i", ")", "for", "i", "in", "range", "(", "num_modalities", ")", "]", "\n", "assert", "all", "(", "[", "isfile", "(", "i", ")", "for", "i", "in", "\n", "expected_image_files", "]", ")", ",", "\"some image files are missing for case %s. Expected files:\\n %s\"", "%", "(", "\n", "c", ",", "expected_image_files", ")", "\n", "\n", "# verify that all modalities and the label have the same geometry. We use the affine for this", "\n", "if", "num_modalities", ">", "1", ":", "\n", "                ", "images_itk", "=", "[", "sitk", ".", "ReadImage", "(", "i", ")", "for", "i", "in", "expected_image_files", "]", "\n", "reference_img", "=", "images_itk", "[", "0", "]", "\n", "\n", "for", "i", ",", "img", "in", "enumerate", "(", "images_itk", "[", "1", ":", "]", ")", ":", "\n", "                    ", "assert", "verify_same_geometry", "(", "img", ",", "reference_img", ")", ",", "\"The modalities of the image %s do not seem to be \"", "\"registered. Please coregister your modalities.\"", "%", "(", "\n", "expected_image_files", "[", "i", "]", ")", "\n", "\n", "# now remove checked files from the lists nii_files_in_imagesTr and nii_files_in_labelsTr", "\n", "", "", "for", "i", "in", "expected_image_files", ":", "\n", "                ", "nii_files_in_imagesTs", ".", "remove", "(", "os", ".", "path", ".", "basename", "(", "i", ")", ")", "\n", "", "", "assert", "len", "(", "\n", "nii_files_in_imagesTs", ")", "==", "0", ",", "\"there are training cases in imagesTs that are not listed in dataset.json: %s\"", "%", "nii_files_in_imagesTr", "\n", "\n", "", "all_same", ",", "unique_orientations", "=", "verify_all_same_orientation", "(", "join", "(", "folder", ",", "\"imagesTr\"", ")", ")", "\n", "if", "not", "all_same", ":", "\n", "        ", "print", "(", "\n", "\"WARNING: Not all images in the dataset have the same axis ordering. We very strongly recommend you correct that by reorienting the data. fslreorient2std should do the trick\"", ")", "\n", "# save unique orientations to dataset.json", "\n", "", "if", "not", "geometries_OK", ":", "\n", "        ", "raise", "Warning", "(", "\"GEOMETRY MISMATCH FOUND! CHECK THE TEXT OUTPUT! This does not cause an error at this point  but you should definitely check whether your geometries are alright!\"", ")", "\n", "", "else", ":", "\n", "        ", "print", "(", "\"Dataset OK\"", ")", "\n", "\n", "", "if", "has_nan", ":", "\n", "        ", "raise", "RuntimeError", "(", "\"Some images have nan values in them. This will break the training. See text output above to see which ones\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.preprocessing.sanity_checks.reorient_to_RAS": [[231, 237], ["nibabel.load", "nibabel.as_closest_canonical", "nibabel.save"], "function", ["None"], ["", "", "def", "reorient_to_RAS", "(", "img_fname", ":", "str", ",", "output_fname", ":", "str", "=", "None", ")", ":", "\n", "    ", "img", "=", "nib", ".", "load", "(", "img_fname", ")", "\n", "canonical_img", "=", "nib", ".", "as_closest_canonical", "(", "img", ")", "\n", "if", "output_fname", "is", "None", ":", "\n", "        ", "output_fname", "=", "img_fname", "\n", "", "nib", ".", "save", "(", "canonical_img", ",", "output_fname", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.preprocessing.preprocessing.GenericPreprocessor.__init__": [[202, 215], ["None"], "methods", ["None"], ["    ", "def", "__init__", "(", "self", ",", "normalization_scheme_per_modality", ",", "use_nonzero_mask", ",", "transpose_forward", ":", "(", "tuple", ",", "list", ")", ",", "intensityproperties", "=", "None", ")", ":", "\n", "        ", "\"\"\"\n\n        :param normalization_scheme_per_modality: dict {0:'nonCT'}\n        :param use_nonzero_mask: {0:False}\n        :param intensityproperties:\n        \"\"\"", "\n", "self", ".", "transpose_forward", "=", "transpose_forward", "\n", "self", ".", "intensityproperties", "=", "intensityproperties", "\n", "self", ".", "normalization_scheme_per_modality", "=", "normalization_scheme_per_modality", "\n", "self", ".", "use_nonzero_mask", "=", "use_nonzero_mask", "\n", "\n", "self", ".", "resample_separate_z_anisotropy_threshold", "=", "RESAMPLING_SEPARATE_Z_ANISO_THRESHOLD", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.preprocessing.preprocessing.GenericPreprocessor.load_cropped": [[216, 224], ["all_data[].astype", "numpy.load", "open", "pickle.load", "os.path.join", "os.path.join"], "methods", ["None"], ["", "@", "staticmethod", "\n", "def", "load_cropped", "(", "cropped_output_dir", ",", "case_identifier", ")", ":", "\n", "        ", "all_data", "=", "np", ".", "load", "(", "os", ".", "path", ".", "join", "(", "cropped_output_dir", ",", "\"%s.npz\"", "%", "case_identifier", ")", ")", "[", "'data'", "]", "\n", "data", "=", "all_data", "[", ":", "-", "1", "]", ".", "astype", "(", "np", ".", "float32", ")", "\n", "seg", "=", "all_data", "[", "-", "1", ":", "]", "\n", "with", "open", "(", "os", ".", "path", ".", "join", "(", "cropped_output_dir", ",", "\"%s.pkl\"", "%", "case_identifier", ")", ",", "'rb'", ")", "as", "f", ":", "\n", "            ", "properties", "=", "pickle", ".", "load", "(", "f", ")", "\n", "", "return", "data", ",", "seg", ",", "properties", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.preprocessing.preprocessing.GenericPreprocessor.resample_and_normalize": [[225, 304], ["preprocessing.resample_patient", "print", "range", "numpy.array", "numpy.array", "len", "len", "len", "len", "len", "numpy.isnan", "numpy.clip", "numpy.clip", "[].mean", "[].std", "numpy.ones", "[].mean", "[].std"], "methods", ["home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.preprocessing.preprocessing.resample_patient"], ["", "def", "resample_and_normalize", "(", "self", ",", "data", ",", "target_spacing", ",", "properties", ",", "seg", "=", "None", ",", "force_separate_z", "=", "None", ")", ":", "\n", "        ", "\"\"\"\n        data and seg must already have been transposed by transpose_forward. properties are the un-transposed values\n        (spacing etc)\n        :param data:\n        :param target_spacing:\n        :param properties:\n        :param seg:\n        :param force_separate_z:\n        :return:\n        \"\"\"", "\n", "\n", "# target_spacing is already transposed, properties[\"original_spacing\"] is not so we need to transpose it!", "\n", "# data, seg are already transposed. Double check this using the properties", "\n", "original_spacing_transposed", "=", "np", ".", "array", "(", "properties", "[", "\"original_spacing\"", "]", ")", "[", "self", ".", "transpose_forward", "]", "\n", "before", "=", "{", "\n", "'spacing'", ":", "properties", "[", "\"original_spacing\"", "]", ",", "\n", "'spacing_transposed'", ":", "original_spacing_transposed", ",", "\n", "'data.shape (data is transposed)'", ":", "data", ".", "shape", "\n", "}", "\n", "\n", "# remove nans", "\n", "data", "[", "np", ".", "isnan", "(", "data", ")", "]", "=", "0", "\n", "\n", "data", ",", "seg", "=", "resample_patient", "(", "data", ",", "seg", ",", "np", ".", "array", "(", "original_spacing_transposed", ")", ",", "target_spacing", ",", "3", ",", "1", ",", "\n", "force_separate_z", "=", "force_separate_z", ",", "order_z_data", "=", "0", ",", "order_z_seg", "=", "0", ",", "\n", "separate_z_anisotropy_threshold", "=", "self", ".", "resample_separate_z_anisotropy_threshold", ")", "\n", "after", "=", "{", "\n", "'spacing'", ":", "target_spacing", ",", "\n", "'data.shape (data is resampled)'", ":", "data", ".", "shape", "\n", "}", "\n", "print", "(", "\"before:\"", ",", "before", ",", "\"\\nafter: \"", ",", "after", ",", "\"\\n\"", ")", "\n", "\n", "if", "seg", "is", "not", "None", ":", "# hippocampus 243 has one voxel with -2 as label. wtf?", "\n", "            ", "seg", "[", "seg", "<", "-", "1", "]", "=", "0", "\n", "\n", "", "properties", "[", "\"size_after_resampling\"", "]", "=", "data", "[", "0", "]", ".", "shape", "\n", "properties", "[", "\"spacing_after_resampling\"", "]", "=", "target_spacing", "\n", "use_nonzero_mask", "=", "self", ".", "use_nonzero_mask", "\n", "\n", "assert", "len", "(", "self", ".", "normalization_scheme_per_modality", ")", "==", "len", "(", "data", ")", ",", "\"self.normalization_scheme_per_modality \"", "\"must have as many entries as data has \"", "\"modalities\"", "\n", "assert", "len", "(", "self", ".", "use_nonzero_mask", ")", "==", "len", "(", "data", ")", ",", "\"self.use_nonzero_mask must have as many entries as data\"", "\" has modalities\"", "\n", "\n", "for", "c", "in", "range", "(", "len", "(", "data", ")", ")", ":", "\n", "            ", "scheme", "=", "self", ".", "normalization_scheme_per_modality", "[", "c", "]", "\n", "if", "scheme", "==", "\"CT\"", ":", "\n", "# clip to lb and ub from train data foreground and use foreground mn and sd from training data", "\n", "                ", "assert", "self", ".", "intensityproperties", "is", "not", "None", ",", "\"ERROR: if there is a CT then we need intensity properties\"", "\n", "mean_intensity", "=", "self", ".", "intensityproperties", "[", "c", "]", "[", "'mean'", "]", "\n", "std_intensity", "=", "self", ".", "intensityproperties", "[", "c", "]", "[", "'sd'", "]", "\n", "lower_bound", "=", "self", ".", "intensityproperties", "[", "c", "]", "[", "'percentile_00_5'", "]", "\n", "upper_bound", "=", "self", ".", "intensityproperties", "[", "c", "]", "[", "'percentile_99_5'", "]", "\n", "data", "[", "c", "]", "=", "np", ".", "clip", "(", "data", "[", "c", "]", ",", "lower_bound", ",", "upper_bound", ")", "\n", "data", "[", "c", "]", "=", "(", "data", "[", "c", "]", "-", "mean_intensity", ")", "/", "std_intensity", "\n", "if", "use_nonzero_mask", "[", "c", "]", ":", "\n", "                    ", "data", "[", "c", "]", "[", "seg", "[", "-", "1", "]", "<", "0", "]", "=", "0", "\n", "", "", "elif", "scheme", "==", "\"CT2\"", ":", "\n", "# clip to lb and ub from train data foreground, use mn and sd form each case for normalization", "\n", "                ", "assert", "self", ".", "intensityproperties", "is", "not", "None", ",", "\"ERROR: if there is a CT then we need intensity properties\"", "\n", "lower_bound", "=", "self", ".", "intensityproperties", "[", "c", "]", "[", "'percentile_00_5'", "]", "\n", "upper_bound", "=", "self", ".", "intensityproperties", "[", "c", "]", "[", "'percentile_99_5'", "]", "\n", "mask", "=", "(", "data", "[", "c", "]", ">", "lower_bound", ")", "&", "(", "data", "[", "c", "]", "<", "upper_bound", ")", "\n", "data", "[", "c", "]", "=", "np", ".", "clip", "(", "data", "[", "c", "]", ",", "lower_bound", ",", "upper_bound", ")", "\n", "mn", "=", "data", "[", "c", "]", "[", "mask", "]", ".", "mean", "(", ")", "\n", "sd", "=", "data", "[", "c", "]", "[", "mask", "]", ".", "std", "(", ")", "\n", "data", "[", "c", "]", "=", "(", "data", "[", "c", "]", "-", "mn", ")", "/", "sd", "\n", "if", "use_nonzero_mask", "[", "c", "]", ":", "\n", "                    ", "data", "[", "c", "]", "[", "seg", "[", "-", "1", "]", "<", "0", "]", "=", "0", "\n", "", "", "else", ":", "\n", "                ", "if", "use_nonzero_mask", "[", "c", "]", ":", "\n", "                    ", "mask", "=", "seg", "[", "-", "1", "]", ">=", "0", "\n", "", "else", ":", "\n", "                    ", "mask", "=", "np", ".", "ones", "(", "seg", ".", "shape", "[", "1", ":", "]", ",", "dtype", "=", "bool", ")", "\n", "", "data", "[", "c", "]", "[", "mask", "]", "=", "(", "data", "[", "c", "]", "[", "mask", "]", "-", "data", "[", "c", "]", "[", "mask", "]", ".", "mean", "(", ")", ")", "/", "(", "data", "[", "c", "]", "[", "mask", "]", ".", "std", "(", ")", "+", "1e-8", ")", "\n", "data", "[", "c", "]", "[", "mask", "==", "0", "]", "=", "0", "\n", "", "", "return", "data", ",", "seg", ",", "properties", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.preprocessing.preprocessing.GenericPreprocessor.preprocess_test_case": [[305, 314], ["nnunet.preprocessing.cropping.ImageCropper.crop_from_list_of_files", "data.transpose.transpose.transpose", "seg.transpose.transpose.transpose", "preprocessing.GenericPreprocessor.resample_and_normalize", "data.transpose.transpose.astype"], "methods", ["home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.preprocessing.cropping.ImageCropper.crop_from_list_of_files", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.preprocessing.preprocessing.PreprocessorFor2D_noNormalization.resample_and_normalize"], ["", "def", "preprocess_test_case", "(", "self", ",", "data_files", ",", "target_spacing", ",", "seg_file", "=", "None", ",", "force_separate_z", "=", "None", ")", ":", "\n", "        ", "data", ",", "seg", ",", "properties", "=", "ImageCropper", ".", "crop_from_list_of_files", "(", "data_files", ",", "seg_file", ")", "\n", "\n", "data", "=", "data", ".", "transpose", "(", "(", "0", ",", "*", "[", "i", "+", "1", "for", "i", "in", "self", ".", "transpose_forward", "]", ")", ")", "\n", "seg", "=", "seg", ".", "transpose", "(", "(", "0", ",", "*", "[", "i", "+", "1", "for", "i", "in", "self", ".", "transpose_forward", "]", ")", ")", "\n", "\n", "data", ",", "seg", ",", "properties", "=", "self", ".", "resample_and_normalize", "(", "data", ",", "target_spacing", ",", "properties", ",", "seg", ",", "\n", "force_separate_z", "=", "force_separate_z", ")", "\n", "return", "data", ".", "astype", "(", "np", ".", "float32", ")", ",", "seg", ",", "properties", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.preprocessing.preprocessing.GenericPreprocessor._run_internal": [[315, 352], ["preprocessing.GenericPreprocessor.load_cropped", "data.transpose.transpose.transpose", "seg.transpose.transpose.transpose", "preprocessing.GenericPreprocessor.resample_and_normalize", "numpy.vstack().astype", "numpy.random.RandomState", "print", "numpy.savez_compressed", "numpy.argwhere", "min", "max", "print", "os.path.join", "os.path.join", "open", "pickle.dump", "numpy.vstack", "len", "len", "int", "numpy.vstack().astype.astype", "os.path.join", "numpy.ceil", "numpy.random.RandomState.choice", "len", "len"], "methods", ["home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.preprocessing.preprocessing.GenericPreprocessor.load_cropped", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.preprocessing.preprocessing.PreprocessorFor2D_noNormalization.resample_and_normalize"], ["", "def", "_run_internal", "(", "self", ",", "target_spacing", ",", "case_identifier", ",", "output_folder_stage", ",", "cropped_output_dir", ",", "force_separate_z", ",", "\n", "all_classes", ")", ":", "\n", "        ", "data", ",", "seg", ",", "properties", "=", "self", ".", "load_cropped", "(", "cropped_output_dir", ",", "case_identifier", ")", "\n", "\n", "data", "=", "data", ".", "transpose", "(", "(", "0", ",", "*", "[", "i", "+", "1", "for", "i", "in", "self", ".", "transpose_forward", "]", ")", ")", "\n", "seg", "=", "seg", ".", "transpose", "(", "(", "0", ",", "*", "[", "i", "+", "1", "for", "i", "in", "self", ".", "transpose_forward", "]", ")", ")", "\n", "\n", "data", ",", "seg", ",", "properties", "=", "self", ".", "resample_and_normalize", "(", "data", ",", "target_spacing", ",", "\n", "properties", ",", "seg", ",", "force_separate_z", ")", "\n", "\n", "all_data", "=", "np", ".", "vstack", "(", "(", "data", ",", "seg", ")", ")", ".", "astype", "(", "np", ".", "float32", ")", "\n", "\n", "# we need to find out where the classes are and sample some random locations", "\n", "# let's do 10.000 samples per class", "\n", "# seed this for reproducibility!", "\n", "num_samples", "=", "10000", "\n", "min_percent_coverage", "=", "0.01", "# at least 1% of the class voxels need to be selected, otherwise it may be too sparse", "\n", "rndst", "=", "np", ".", "random", ".", "RandomState", "(", "1234", ")", "\n", "class_locs", "=", "{", "}", "\n", "for", "c", "in", "all_classes", ":", "\n", "            ", "all_locs", "=", "np", ".", "argwhere", "(", "all_data", "[", "-", "1", "]", "==", "c", ")", "\n", "if", "len", "(", "all_locs", ")", "==", "0", ":", "\n", "                ", "class_locs", "[", "c", "]", "=", "[", "]", "\n", "continue", "\n", "", "target_num_samples", "=", "min", "(", "num_samples", ",", "len", "(", "all_locs", ")", ")", "\n", "target_num_samples", "=", "max", "(", "target_num_samples", ",", "int", "(", "np", ".", "ceil", "(", "len", "(", "all_locs", ")", "*", "min_percent_coverage", ")", ")", ")", "\n", "\n", "selected", "=", "all_locs", "[", "rndst", ".", "choice", "(", "len", "(", "all_locs", ")", ",", "target_num_samples", ",", "replace", "=", "False", ")", "]", "\n", "class_locs", "[", "c", "]", "=", "selected", "\n", "print", "(", "c", ",", "target_num_samples", ")", "\n", "", "properties", "[", "'class_locations'", "]", "=", "class_locs", "\n", "\n", "print", "(", "\"saving: \"", ",", "os", ".", "path", ".", "join", "(", "output_folder_stage", ",", "\"%s.npz\"", "%", "case_identifier", ")", ")", "\n", "np", ".", "savez_compressed", "(", "os", ".", "path", ".", "join", "(", "output_folder_stage", ",", "\"%s.npz\"", "%", "case_identifier", ")", ",", "\n", "data", "=", "all_data", ".", "astype", "(", "np", ".", "float32", ")", ")", "\n", "with", "open", "(", "os", ".", "path", ".", "join", "(", "output_folder_stage", ",", "\"%s.pkl\"", "%", "case_identifier", ")", ",", "'wb'", ")", "as", "f", ":", "\n", "            ", "pickle", ".", "dump", "(", "properties", ",", "f", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.preprocessing.preprocessing.GenericPreprocessor.run": [[353, 392], ["print", "print", "print", "subfiles", "maybe_mkdir_p", "len", "range", "isinstance", "len", "load_pickle", "os.path.join", "maybe_mkdir_p", "enumerate", "multiprocessing.pool.Pool", "multiprocessing.pool.Pool.starmap", "multiprocessing.pool.Pool.close", "multiprocessing.pool.Pool.join", "join", "nnunet.preprocessing.cropping.get_case_identifier_from_npz", "all_args.append"], "methods", ["home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.preprocessing.cropping.get_case_identifier_from_npz"], ["", "", "def", "run", "(", "self", ",", "target_spacings", ",", "input_folder_with_cropped_npz", ",", "output_folder", ",", "data_identifier", ",", "\n", "num_threads", "=", "default_num_threads", ",", "force_separate_z", "=", "None", ")", ":", "\n", "        ", "\"\"\"\n\n        :param target_spacings: list of lists [[1.25, 1.25, 5]]\n        :param input_folder_with_cropped_npz: dim: c, x, y, z | npz_file['data'] np.savez_compressed(fname.npz, data=arr)\n        :param output_folder:\n        :param num_threads:\n        :param force_separate_z: None\n        :return:\n        \"\"\"", "\n", "print", "(", "\"Initializing to run preprocessing\"", ")", "\n", "print", "(", "\"npz folder:\"", ",", "input_folder_with_cropped_npz", ")", "\n", "print", "(", "\"output_folder:\"", ",", "output_folder", ")", "\n", "list_of_cropped_npz_files", "=", "subfiles", "(", "input_folder_with_cropped_npz", ",", "True", ",", "None", ",", "\".npz\"", ",", "True", ")", "\n", "maybe_mkdir_p", "(", "output_folder", ")", "\n", "num_stages", "=", "len", "(", "target_spacings", ")", "\n", "if", "not", "isinstance", "(", "num_threads", ",", "(", "list", ",", "tuple", ",", "np", ".", "ndarray", ")", ")", ":", "\n", "            ", "num_threads", "=", "[", "num_threads", "]", "*", "num_stages", "\n", "\n", "", "assert", "len", "(", "num_threads", ")", "==", "num_stages", "\n", "\n", "# we need to know which classes are present in this dataset so that we can precompute where these classes are", "\n", "# located. This is needed for oversampling foreground", "\n", "all_classes", "=", "load_pickle", "(", "join", "(", "input_folder_with_cropped_npz", ",", "'dataset_properties.pkl'", ")", ")", "[", "'all_classes'", "]", "\n", "\n", "for", "i", "in", "range", "(", "num_stages", ")", ":", "\n", "            ", "all_args", "=", "[", "]", "\n", "output_folder_stage", "=", "os", ".", "path", ".", "join", "(", "output_folder", ",", "data_identifier", "+", "\"_stage%d\"", "%", "i", ")", "\n", "maybe_mkdir_p", "(", "output_folder_stage", ")", "\n", "spacing", "=", "target_spacings", "[", "i", "]", "\n", "for", "j", ",", "case", "in", "enumerate", "(", "list_of_cropped_npz_files", ")", ":", "\n", "                ", "case_identifier", "=", "get_case_identifier_from_npz", "(", "case", ")", "\n", "args", "=", "spacing", ",", "case_identifier", ",", "output_folder_stage", ",", "input_folder_with_cropped_npz", ",", "force_separate_z", ",", "all_classes", "\n", "all_args", ".", "append", "(", "args", ")", "\n", "", "p", "=", "Pool", "(", "num_threads", "[", "i", "]", ")", "\n", "p", ".", "starmap", "(", "self", ".", "_run_internal", ",", "all_args", ")", "\n", "p", ".", "close", "(", ")", "\n", "p", ".", "join", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.preprocessing.preprocessing.Preprocessor3DDifferentResampling.resample_and_normalize": [[395, 474], ["preprocessing.resample_patient", "print", "range", "numpy.array", "numpy.array", "len", "len", "len", "len", "len", "numpy.isnan", "numpy.clip", "numpy.clip", "[].mean", "[].std", "numpy.ones", "[].mean", "[].std"], "methods", ["home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.preprocessing.preprocessing.resample_patient"], ["    ", "def", "resample_and_normalize", "(", "self", ",", "data", ",", "target_spacing", ",", "properties", ",", "seg", "=", "None", ",", "force_separate_z", "=", "None", ")", ":", "\n", "        ", "\"\"\"\n        data and seg must already have been transposed by transpose_forward. properties are the un-transposed values\n        (spacing etc)\n        :param data:\n        :param target_spacing:\n        :param properties:\n        :param seg:\n        :param force_separate_z:\n        :return:\n        \"\"\"", "\n", "\n", "# target_spacing is already transposed, properties[\"original_spacing\"] is not so we need to transpose it!", "\n", "# data, seg are already transposed. Double check this using the properties", "\n", "original_spacing_transposed", "=", "np", ".", "array", "(", "properties", "[", "\"original_spacing\"", "]", ")", "[", "self", ".", "transpose_forward", "]", "\n", "before", "=", "{", "\n", "'spacing'", ":", "properties", "[", "\"original_spacing\"", "]", ",", "\n", "'spacing_transposed'", ":", "original_spacing_transposed", ",", "\n", "'data.shape (data is transposed)'", ":", "data", ".", "shape", "\n", "}", "\n", "\n", "# remove nans", "\n", "data", "[", "np", ".", "isnan", "(", "data", ")", "]", "=", "0", "\n", "\n", "data", ",", "seg", "=", "resample_patient", "(", "data", ",", "seg", ",", "np", ".", "array", "(", "original_spacing_transposed", ")", ",", "target_spacing", ",", "3", ",", "1", ",", "\n", "force_separate_z", "=", "force_separate_z", ",", "order_z_data", "=", "3", ",", "order_z_seg", "=", "1", ",", "\n", "separate_z_anisotropy_threshold", "=", "self", ".", "resample_separate_z_anisotropy_threshold", ")", "\n", "after", "=", "{", "\n", "'spacing'", ":", "target_spacing", ",", "\n", "'data.shape (data is resampled)'", ":", "data", ".", "shape", "\n", "}", "\n", "print", "(", "\"before:\"", ",", "before", ",", "\"\\nafter: \"", ",", "after", ",", "\"\\n\"", ")", "\n", "\n", "if", "seg", "is", "not", "None", ":", "# hippocampus 243 has one voxel with -2 as label. wtf?", "\n", "            ", "seg", "[", "seg", "<", "-", "1", "]", "=", "0", "\n", "\n", "", "properties", "[", "\"size_after_resampling\"", "]", "=", "data", "[", "0", "]", ".", "shape", "\n", "properties", "[", "\"spacing_after_resampling\"", "]", "=", "target_spacing", "\n", "use_nonzero_mask", "=", "self", ".", "use_nonzero_mask", "\n", "\n", "assert", "len", "(", "self", ".", "normalization_scheme_per_modality", ")", "==", "len", "(", "data", ")", ",", "\"self.normalization_scheme_per_modality \"", "\"must have as many entries as data has \"", "\"modalities\"", "\n", "assert", "len", "(", "self", ".", "use_nonzero_mask", ")", "==", "len", "(", "data", ")", ",", "\"self.use_nonzero_mask must have as many entries as data\"", "\" has modalities\"", "\n", "\n", "for", "c", "in", "range", "(", "len", "(", "data", ")", ")", ":", "\n", "            ", "scheme", "=", "self", ".", "normalization_scheme_per_modality", "[", "c", "]", "\n", "if", "scheme", "==", "\"CT\"", ":", "\n", "# clip to lb and ub from train data foreground and use foreground mn and sd from training data", "\n", "                ", "assert", "self", ".", "intensityproperties", "is", "not", "None", ",", "\"ERROR: if there is a CT then we need intensity properties\"", "\n", "mean_intensity", "=", "self", ".", "intensityproperties", "[", "c", "]", "[", "'mean'", "]", "\n", "std_intensity", "=", "self", ".", "intensityproperties", "[", "c", "]", "[", "'sd'", "]", "\n", "lower_bound", "=", "self", ".", "intensityproperties", "[", "c", "]", "[", "'percentile_00_5'", "]", "\n", "upper_bound", "=", "self", ".", "intensityproperties", "[", "c", "]", "[", "'percentile_99_5'", "]", "\n", "data", "[", "c", "]", "=", "np", ".", "clip", "(", "data", "[", "c", "]", ",", "lower_bound", ",", "upper_bound", ")", "\n", "data", "[", "c", "]", "=", "(", "data", "[", "c", "]", "-", "mean_intensity", ")", "/", "std_intensity", "\n", "if", "use_nonzero_mask", "[", "c", "]", ":", "\n", "                    ", "data", "[", "c", "]", "[", "seg", "[", "-", "1", "]", "<", "0", "]", "=", "0", "\n", "", "", "elif", "scheme", "==", "\"CT2\"", ":", "\n", "# clip to lb and ub from train data foreground, use mn and sd form each case for normalization", "\n", "                ", "assert", "self", ".", "intensityproperties", "is", "not", "None", ",", "\"ERROR: if there is a CT then we need intensity properties\"", "\n", "lower_bound", "=", "self", ".", "intensityproperties", "[", "c", "]", "[", "'percentile_00_5'", "]", "\n", "upper_bound", "=", "self", ".", "intensityproperties", "[", "c", "]", "[", "'percentile_99_5'", "]", "\n", "mask", "=", "(", "data", "[", "c", "]", ">", "lower_bound", ")", "&", "(", "data", "[", "c", "]", "<", "upper_bound", ")", "\n", "data", "[", "c", "]", "=", "np", ".", "clip", "(", "data", "[", "c", "]", ",", "lower_bound", ",", "upper_bound", ")", "\n", "mn", "=", "data", "[", "c", "]", "[", "mask", "]", ".", "mean", "(", ")", "\n", "sd", "=", "data", "[", "c", "]", "[", "mask", "]", ".", "std", "(", ")", "\n", "data", "[", "c", "]", "=", "(", "data", "[", "c", "]", "-", "mn", ")", "/", "sd", "\n", "if", "use_nonzero_mask", "[", "c", "]", ":", "\n", "                    ", "data", "[", "c", "]", "[", "seg", "[", "-", "1", "]", "<", "0", "]", "=", "0", "\n", "", "", "else", ":", "\n", "                ", "if", "use_nonzero_mask", "[", "c", "]", ":", "\n", "                    ", "mask", "=", "seg", "[", "-", "1", "]", ">=", "0", "\n", "", "else", ":", "\n", "                    ", "mask", "=", "np", ".", "ones", "(", "seg", ".", "shape", "[", "1", ":", "]", ",", "dtype", "=", "bool", ")", "\n", "", "data", "[", "c", "]", "[", "mask", "]", "=", "(", "data", "[", "c", "]", "[", "mask", "]", "-", "data", "[", "c", "]", "[", "mask", "]", ".", "mean", "(", ")", ")", "/", "(", "data", "[", "c", "]", "[", "mask", "]", ".", "std", "(", ")", "+", "1e-8", ")", "\n", "data", "[", "c", "]", "[", "mask", "==", "0", "]", "=", "0", "\n", "", "", "return", "data", ",", "seg", ",", "properties", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.preprocessing.preprocessing.Preprocessor3DBetterResampling.resample_and_normalize": [[482, 568], ["preprocessing.resample_patient", "print", "range", "print", "numpy.array", "numpy.array", "len", "len", "len", "len", "len", "numpy.isnan", "numpy.clip", "str", "numpy.clip", "[].mean", "[].std", "numpy.ones", "[].mean", "[].std"], "methods", ["home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.preprocessing.preprocessing.resample_patient"], ["def", "resample_and_normalize", "(", "self", ",", "data", ",", "target_spacing", ",", "properties", ",", "seg", "=", "None", ",", "force_separate_z", "=", "False", ")", ":", "\n", "        ", "\"\"\"\n        data and seg must already have been transposed by transpose_forward. properties are the un-transposed values\n        (spacing etc)\n        :param data:\n        :param target_spacing:\n        :param properties:\n        :param seg:\n        :param force_separate_z:\n        :return:\n        \"\"\"", "\n", "if", "force_separate_z", "is", "not", "False", ":", "\n", "            ", "print", "(", "\"WARNING: Preprocessor3DBetterResampling always uses force_separate_z=False. \"", "\n", "\"You specified %s. Your choice is overwritten\"", "%", "str", "(", "force_separate_z", ")", ")", "\n", "force_separate_z", "=", "False", "\n", "\n", "# be safe", "\n", "", "assert", "force_separate_z", "is", "False", "\n", "\n", "# target_spacing is already transposed, properties[\"original_spacing\"] is not so we need to transpose it!", "\n", "# data, seg are already transposed. Double check this using the properties", "\n", "original_spacing_transposed", "=", "np", ".", "array", "(", "properties", "[", "\"original_spacing\"", "]", ")", "[", "self", ".", "transpose_forward", "]", "\n", "before", "=", "{", "\n", "'spacing'", ":", "properties", "[", "\"original_spacing\"", "]", ",", "\n", "'spacing_transposed'", ":", "original_spacing_transposed", ",", "\n", "'data.shape (data is transposed)'", ":", "data", ".", "shape", "\n", "}", "\n", "\n", "# remove nans", "\n", "data", "[", "np", ".", "isnan", "(", "data", ")", "]", "=", "0", "\n", "\n", "data", ",", "seg", "=", "resample_patient", "(", "data", ",", "seg", ",", "np", ".", "array", "(", "original_spacing_transposed", ")", ",", "target_spacing", ",", "3", ",", "3", ",", "\n", "force_separate_z", "=", "force_separate_z", ",", "order_z_data", "=", "99999", ",", "order_z_seg", "=", "99999", ",", "\n", "separate_z_anisotropy_threshold", "=", "self", ".", "resample_separate_z_anisotropy_threshold", ")", "\n", "after", "=", "{", "\n", "'spacing'", ":", "target_spacing", ",", "\n", "'data.shape (data is resampled)'", ":", "data", ".", "shape", "\n", "}", "\n", "print", "(", "\"before:\"", ",", "before", ",", "\"\\nafter: \"", ",", "after", ",", "\"\\n\"", ")", "\n", "\n", "if", "seg", "is", "not", "None", ":", "# hippocampus 243 has one voxel with -2 as label. wtf?", "\n", "            ", "seg", "[", "seg", "<", "-", "1", "]", "=", "0", "\n", "\n", "", "properties", "[", "\"size_after_resampling\"", "]", "=", "data", "[", "0", "]", ".", "shape", "\n", "properties", "[", "\"spacing_after_resampling\"", "]", "=", "target_spacing", "\n", "use_nonzero_mask", "=", "self", ".", "use_nonzero_mask", "\n", "\n", "assert", "len", "(", "self", ".", "normalization_scheme_per_modality", ")", "==", "len", "(", "data", ")", ",", "\"self.normalization_scheme_per_modality \"", "\"must have as many entries as data has \"", "\"modalities\"", "\n", "assert", "len", "(", "self", ".", "use_nonzero_mask", ")", "==", "len", "(", "data", ")", ",", "\"self.use_nonzero_mask must have as many entries as data\"", "\" has modalities\"", "\n", "\n", "for", "c", "in", "range", "(", "len", "(", "data", ")", ")", ":", "\n", "            ", "scheme", "=", "self", ".", "normalization_scheme_per_modality", "[", "c", "]", "\n", "if", "scheme", "==", "\"CT\"", ":", "\n", "# clip to lb and ub from train data foreground and use foreground mn and sd from training data", "\n", "                ", "assert", "self", ".", "intensityproperties", "is", "not", "None", ",", "\"ERROR: if there is a CT then we need intensity properties\"", "\n", "mean_intensity", "=", "self", ".", "intensityproperties", "[", "c", "]", "[", "'mean'", "]", "\n", "std_intensity", "=", "self", ".", "intensityproperties", "[", "c", "]", "[", "'sd'", "]", "\n", "lower_bound", "=", "self", ".", "intensityproperties", "[", "c", "]", "[", "'percentile_00_5'", "]", "\n", "upper_bound", "=", "self", ".", "intensityproperties", "[", "c", "]", "[", "'percentile_99_5'", "]", "\n", "data", "[", "c", "]", "=", "np", ".", "clip", "(", "data", "[", "c", "]", ",", "lower_bound", ",", "upper_bound", ")", "\n", "data", "[", "c", "]", "=", "(", "data", "[", "c", "]", "-", "mean_intensity", ")", "/", "std_intensity", "\n", "if", "use_nonzero_mask", "[", "c", "]", ":", "\n", "                    ", "data", "[", "c", "]", "[", "seg", "[", "-", "1", "]", "<", "0", "]", "=", "0", "\n", "", "", "elif", "scheme", "==", "\"CT2\"", ":", "\n", "# clip to lb and ub from train data foreground, use mn and sd form each case for normalization", "\n", "                ", "assert", "self", ".", "intensityproperties", "is", "not", "None", ",", "\"ERROR: if there is a CT then we need intensity properties\"", "\n", "lower_bound", "=", "self", ".", "intensityproperties", "[", "c", "]", "[", "'percentile_00_5'", "]", "\n", "upper_bound", "=", "self", ".", "intensityproperties", "[", "c", "]", "[", "'percentile_99_5'", "]", "\n", "mask", "=", "(", "data", "[", "c", "]", ">", "lower_bound", ")", "&", "(", "data", "[", "c", "]", "<", "upper_bound", ")", "\n", "data", "[", "c", "]", "=", "np", ".", "clip", "(", "data", "[", "c", "]", ",", "lower_bound", ",", "upper_bound", ")", "\n", "mn", "=", "data", "[", "c", "]", "[", "mask", "]", ".", "mean", "(", ")", "\n", "sd", "=", "data", "[", "c", "]", "[", "mask", "]", ".", "std", "(", ")", "\n", "data", "[", "c", "]", "=", "(", "data", "[", "c", "]", "-", "mn", ")", "/", "sd", "\n", "if", "use_nonzero_mask", "[", "c", "]", ":", "\n", "                    ", "data", "[", "c", "]", "[", "seg", "[", "-", "1", "]", "<", "0", "]", "=", "0", "\n", "", "", "else", ":", "\n", "                ", "if", "use_nonzero_mask", "[", "c", "]", ":", "\n", "                    ", "mask", "=", "seg", "[", "-", "1", "]", ">=", "0", "\n", "", "else", ":", "\n", "                    ", "mask", "=", "np", ".", "ones", "(", "seg", ".", "shape", "[", "1", ":", "]", ",", "dtype", "=", "bool", ")", "\n", "", "data", "[", "c", "]", "[", "mask", "]", "=", "(", "data", "[", "c", "]", "[", "mask", "]", "-", "data", "[", "c", "]", "[", "mask", "]", ".", "mean", "(", ")", ")", "/", "(", "data", "[", "c", "]", "[", "mask", "]", ".", "std", "(", ")", "+", "1e-8", ")", "\n", "data", "[", "c", "]", "[", "mask", "==", "0", "]", "=", "0", "\n", "", "", "return", "data", ",", "seg", ",", "properties", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.preprocessing.preprocessing.PreprocessorFor2D.__init__": [[571, 574], ["preprocessing.GenericPreprocessor.__init__"], "methods", ["home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.common.layers.ResizeImage.__init__"], ["    ", "def", "__init__", "(", "self", ",", "normalization_scheme_per_modality", ",", "use_nonzero_mask", ",", "transpose_forward", ":", "(", "tuple", ",", "list", ")", ",", "intensityproperties", "=", "None", ")", ":", "\n", "        ", "super", "(", "PreprocessorFor2D", ",", "self", ")", ".", "__init__", "(", "normalization_scheme_per_modality", ",", "use_nonzero_mask", ",", "\n", "transpose_forward", ",", "intensityproperties", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.preprocessing.preprocessing.PreprocessorFor2D.run": [[575, 602], ["print", "print", "print", "subfiles", "maybe_mkdir_p", "len", "range", "multiprocessing.pool.Pool", "multiprocessing.pool.Pool.starmap", "multiprocessing.pool.Pool.close", "multiprocessing.pool.Pool.join", "len", "load_pickle", "os.path.join", "maybe_mkdir_p", "enumerate", "join", "nnunet.preprocessing.cropping.get_case_identifier_from_npz", "all_args.append"], "methods", ["home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.preprocessing.cropping.get_case_identifier_from_npz"], ["", "def", "run", "(", "self", ",", "target_spacings", ",", "input_folder_with_cropped_npz", ",", "output_folder", ",", "data_identifier", ",", "\n", "num_threads", "=", "default_num_threads", ",", "force_separate_z", "=", "None", ")", ":", "\n", "        ", "print", "(", "\"Initializing to run preprocessing\"", ")", "\n", "print", "(", "\"npz folder:\"", ",", "input_folder_with_cropped_npz", ")", "\n", "print", "(", "\"output_folder:\"", ",", "output_folder", ")", "\n", "list_of_cropped_npz_files", "=", "subfiles", "(", "input_folder_with_cropped_npz", ",", "True", ",", "None", ",", "\".npz\"", ",", "True", ")", "\n", "assert", "len", "(", "list_of_cropped_npz_files", ")", "!=", "0", ",", "\"set list of files first\"", "\n", "maybe_mkdir_p", "(", "output_folder", ")", "\n", "all_args", "=", "[", "]", "\n", "num_stages", "=", "len", "(", "target_spacings", ")", "\n", "\n", "# we need to know which classes are present in this dataset so that we can precompute where these classes are", "\n", "# located. This is needed for oversampling foreground", "\n", "all_classes", "=", "load_pickle", "(", "join", "(", "input_folder_with_cropped_npz", ",", "'dataset_properties.pkl'", ")", ")", "[", "'all_classes'", "]", "\n", "\n", "for", "i", "in", "range", "(", "num_stages", ")", ":", "\n", "            ", "output_folder_stage", "=", "os", ".", "path", ".", "join", "(", "output_folder", ",", "data_identifier", "+", "\"_stage%d\"", "%", "i", ")", "\n", "maybe_mkdir_p", "(", "output_folder_stage", ")", "\n", "spacing", "=", "target_spacings", "[", "i", "]", "\n", "for", "j", ",", "case", "in", "enumerate", "(", "list_of_cropped_npz_files", ")", ":", "\n", "                ", "case_identifier", "=", "get_case_identifier_from_npz", "(", "case", ")", "\n", "args", "=", "spacing", ",", "case_identifier", ",", "output_folder_stage", ",", "input_folder_with_cropped_npz", ",", "force_separate_z", ",", "all_classes", "\n", "all_args", ".", "append", "(", "args", ")", "\n", "", "", "p", "=", "Pool", "(", "num_threads", ")", "\n", "p", ".", "starmap", "(", "self", ".", "_run_internal", ",", "all_args", ")", "\n", "p", ".", "close", "(", ")", "\n", "p", ".", "join", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.preprocessing.preprocessing.PreprocessorFor2D.resample_and_normalize": [[603, 669], ["preprocessing.resample_patient", "print", "print", "range", "print", "numpy.array", "numpy.array", "len", "len", "len", "len", "len", "numpy.clip", "numpy.clip", "[].mean", "[].std", "numpy.ones", "[].mean", "[].std"], "methods", ["home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.preprocessing.preprocessing.resample_patient"], ["", "def", "resample_and_normalize", "(", "self", ",", "data", ",", "target_spacing", ",", "properties", ",", "seg", "=", "None", ",", "force_separate_z", "=", "None", ")", ":", "\n", "        ", "original_spacing_transposed", "=", "np", ".", "array", "(", "properties", "[", "\"original_spacing\"", "]", ")", "[", "self", ".", "transpose_forward", "]", "\n", "before", "=", "{", "\n", "'spacing'", ":", "properties", "[", "\"original_spacing\"", "]", ",", "\n", "'spacing_transposed'", ":", "original_spacing_transposed", ",", "\n", "'data.shape (data is transposed)'", ":", "data", ".", "shape", "\n", "}", "\n", "target_spacing", "[", "0", "]", "=", "original_spacing_transposed", "[", "0", "]", "\n", "data", ",", "seg", "=", "resample_patient", "(", "data", ",", "seg", ",", "np", ".", "array", "(", "original_spacing_transposed", ")", ",", "target_spacing", ",", "3", ",", "1", ",", "\n", "force_separate_z", "=", "force_separate_z", ",", "order_z_data", "=", "0", ",", "order_z_seg", "=", "0", ",", "\n", "separate_z_anisotropy_threshold", "=", "self", ".", "resample_separate_z_anisotropy_threshold", ")", "\n", "after", "=", "{", "\n", "'spacing'", ":", "target_spacing", ",", "\n", "'data.shape (data is resampled)'", ":", "data", ".", "shape", "\n", "}", "\n", "print", "(", "\"before:\"", ",", "before", ",", "\"\\nafter: \"", ",", "after", ",", "\"\\n\"", ")", "\n", "\n", "if", "seg", "is", "not", "None", ":", "# hippocampus 243 has one voxel with -2 as label. wtf?", "\n", "            ", "seg", "[", "seg", "<", "-", "1", "]", "=", "0", "\n", "\n", "", "properties", "[", "\"size_after_resampling\"", "]", "=", "data", "[", "0", "]", ".", "shape", "\n", "properties", "[", "\"spacing_after_resampling\"", "]", "=", "target_spacing", "\n", "use_nonzero_mask", "=", "self", ".", "use_nonzero_mask", "\n", "\n", "assert", "len", "(", "self", ".", "normalization_scheme_per_modality", ")", "==", "len", "(", "data", ")", ",", "\"self.normalization_scheme_per_modality \"", "\"must have as many entries as data has \"", "\"modalities\"", "\n", "assert", "len", "(", "self", ".", "use_nonzero_mask", ")", "==", "len", "(", "data", ")", ",", "\"self.use_nonzero_mask must have as many entries as data\"", "\" has modalities\"", "\n", "\n", "print", "(", "\"normalization...\"", ")", "\n", "\n", "for", "c", "in", "range", "(", "len", "(", "data", ")", ")", ":", "\n", "            ", "scheme", "=", "self", ".", "normalization_scheme_per_modality", "[", "c", "]", "\n", "if", "scheme", "==", "\"CT\"", ":", "\n", "# clip to lb and ub from train data foreground and use foreground mn and sd from training data", "\n", "                ", "assert", "self", ".", "intensityproperties", "is", "not", "None", ",", "\"ERROR: if there is a CT then we need intensity properties\"", "\n", "mean_intensity", "=", "self", ".", "intensityproperties", "[", "c", "]", "[", "'mean'", "]", "\n", "std_intensity", "=", "self", ".", "intensityproperties", "[", "c", "]", "[", "'sd'", "]", "\n", "lower_bound", "=", "self", ".", "intensityproperties", "[", "c", "]", "[", "'percentile_00_5'", "]", "\n", "upper_bound", "=", "self", ".", "intensityproperties", "[", "c", "]", "[", "'percentile_99_5'", "]", "\n", "data", "[", "c", "]", "=", "np", ".", "clip", "(", "data", "[", "c", "]", ",", "lower_bound", ",", "upper_bound", ")", "\n", "data", "[", "c", "]", "=", "(", "data", "[", "c", "]", "-", "mean_intensity", ")", "/", "std_intensity", "\n", "if", "use_nonzero_mask", "[", "c", "]", ":", "\n", "                    ", "data", "[", "c", "]", "[", "seg", "[", "-", "1", "]", "<", "0", "]", "=", "0", "\n", "", "", "elif", "scheme", "==", "\"CT2\"", ":", "\n", "# clip to lb and ub from train data foreground, use mn and sd form each case for normalization", "\n", "                ", "assert", "self", ".", "intensityproperties", "is", "not", "None", ",", "\"ERROR: if there is a CT then we need intensity properties\"", "\n", "lower_bound", "=", "self", ".", "intensityproperties", "[", "c", "]", "[", "'percentile_00_5'", "]", "\n", "upper_bound", "=", "self", ".", "intensityproperties", "[", "c", "]", "[", "'percentile_99_5'", "]", "\n", "mask", "=", "(", "data", "[", "c", "]", ">", "lower_bound", ")", "&", "(", "data", "[", "c", "]", "<", "upper_bound", ")", "\n", "data", "[", "c", "]", "=", "np", ".", "clip", "(", "data", "[", "c", "]", ",", "lower_bound", ",", "upper_bound", ")", "\n", "mn", "=", "data", "[", "c", "]", "[", "mask", "]", ".", "mean", "(", ")", "\n", "sd", "=", "data", "[", "c", "]", "[", "mask", "]", ".", "std", "(", ")", "\n", "data", "[", "c", "]", "=", "(", "data", "[", "c", "]", "-", "mn", ")", "/", "sd", "\n", "if", "use_nonzero_mask", "[", "c", "]", ":", "\n", "                    ", "data", "[", "c", "]", "[", "seg", "[", "-", "1", "]", "<", "0", "]", "=", "0", "\n", "", "", "else", ":", "\n", "                ", "if", "use_nonzero_mask", "[", "c", "]", ":", "\n", "                    ", "mask", "=", "seg", "[", "-", "1", "]", ">=", "0", "\n", "", "else", ":", "\n", "                    ", "mask", "=", "np", ".", "ones", "(", "seg", ".", "shape", "[", "1", ":", "]", ",", "dtype", "=", "bool", ")", "\n", "", "data", "[", "c", "]", "[", "mask", "]", "=", "(", "data", "[", "c", "]", "[", "mask", "]", "-", "data", "[", "c", "]", "[", "mask", "]", ".", "mean", "(", ")", ")", "/", "(", "data", "[", "c", "]", "[", "mask", "]", ".", "std", "(", ")", "+", "1e-8", ")", "\n", "data", "[", "c", "]", "[", "mask", "==", "0", "]", "=", "0", "\n", "", "", "print", "(", "\"normalization done\"", ")", "\n", "return", "data", ",", "seg", ",", "properties", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.preprocessing.preprocessing.PreprocessorFor2D_noNormalization.resample_and_normalize": [[672, 702], ["preprocessing.resample_patient", "print", "numpy.array", "numpy.array", "len", "len", "len", "len"], "methods", ["home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.preprocessing.preprocessing.resample_patient"], ["    ", "def", "resample_and_normalize", "(", "self", ",", "data", ",", "target_spacing", ",", "properties", ",", "seg", "=", "None", ",", "force_separate_z", "=", "None", ")", ":", "\n", "        ", "original_spacing_transposed", "=", "np", ".", "array", "(", "properties", "[", "\"original_spacing\"", "]", ")", "[", "self", ".", "transpose_forward", "]", "\n", "before", "=", "{", "\n", "'spacing'", ":", "properties", "[", "\"original_spacing\"", "]", ",", "\n", "'spacing_transposed'", ":", "original_spacing_transposed", ",", "\n", "'data.shape (data is transposed)'", ":", "data", ".", "shape", "\n", "}", "\n", "target_spacing", "[", "0", "]", "=", "original_spacing_transposed", "[", "0", "]", "\n", "data", ",", "seg", "=", "resample_patient", "(", "data", ",", "seg", ",", "np", ".", "array", "(", "original_spacing_transposed", ")", ",", "target_spacing", ",", "3", ",", "1", ",", "\n", "force_separate_z", "=", "force_separate_z", ",", "order_z_data", "=", "0", ",", "order_z_seg", "=", "0", ",", "\n", "separate_z_anisotropy_threshold", "=", "self", ".", "resample_separate_z_anisotropy_threshold", ")", "\n", "after", "=", "{", "\n", "'spacing'", ":", "target_spacing", ",", "\n", "'data.shape (data is resampled)'", ":", "data", ".", "shape", "\n", "}", "\n", "print", "(", "\"before:\"", ",", "before", ",", "\"\\nafter: \"", ",", "after", ",", "\"\\n\"", ")", "\n", "\n", "if", "seg", "is", "not", "None", ":", "# hippocampus 243 has one voxel with -2 as label. wtf?", "\n", "            ", "seg", "[", "seg", "<", "-", "1", "]", "=", "0", "\n", "\n", "", "properties", "[", "\"size_after_resampling\"", "]", "=", "data", "[", "0", "]", ".", "shape", "\n", "properties", "[", "\"spacing_after_resampling\"", "]", "=", "target_spacing", "\n", "use_nonzero_mask", "=", "self", ".", "use_nonzero_mask", "\n", "\n", "assert", "len", "(", "self", ".", "normalization_scheme_per_modality", ")", "==", "len", "(", "data", ")", ",", "\"self.normalization_scheme_per_modality \"", "\"must have as many entries as data has \"", "\"modalities\"", "\n", "assert", "len", "(", "self", ".", "use_nonzero_mask", ")", "==", "len", "(", "data", ")", ",", "\"self.use_nonzero_mask must have as many entries as data\"", "\" has modalities\"", "\n", "return", "data", ",", "seg", ",", "properties", "", "", "", ""]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.preprocessing.preprocessing.get_do_separate_z": [[26, 29], ["numpy.max", "numpy.min"], "function", ["None"], ["def", "get_do_separate_z", "(", "spacing", ",", "anisotropy_threshold", "=", "RESAMPLING_SEPARATE_Z_ANISO_THRESHOLD", ")", ":", "\n", "    ", "do_separate_z", "=", "(", "np", ".", "max", "(", "spacing", ")", "/", "np", ".", "min", "(", "spacing", ")", ")", ">", "anisotropy_threshold", "\n", "return", "do_separate_z", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.preprocessing.preprocessing.get_lowres_axis": [[31, 34], ["numpy.where", "max", "numpy.array"], "function", ["None"], ["", "def", "get_lowres_axis", "(", "new_spacing", ")", ":", "\n", "    ", "axis", "=", "np", ".", "where", "(", "max", "(", "new_spacing", ")", "/", "np", ".", "array", "(", "new_spacing", ")", "==", "1", ")", "[", "0", "]", "# find which axis is anisotropic", "\n", "return", "axis", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.preprocessing.preprocessing.resample_patient": [[36, 107], ["numpy.round().astype", "numpy.array", "numpy.array", "preprocessing.get_do_separate_z", "preprocessing.resample_data_or_seg", "preprocessing.resample_data_or_seg", "len", "len", "numpy.round", "preprocessing.get_lowres_axis", "preprocessing.get_lowres_axis", "preprocessing.get_do_separate_z", "len", "preprocessing.get_lowres_axis", "len", "print", "str", "numpy.array", "numpy.array"], "function", ["home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.preprocessing.preprocessing.get_do_separate_z", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.preprocessing.preprocessing.resample_data_or_seg", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.preprocessing.preprocessing.resample_data_or_seg", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.preprocessing.preprocessing.get_lowres_axis", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.preprocessing.preprocessing.get_lowres_axis", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.preprocessing.preprocessing.get_do_separate_z", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.preprocessing.preprocessing.get_lowres_axis"], ["", "def", "resample_patient", "(", "data", ",", "seg", ",", "original_spacing", ",", "target_spacing", ",", "order_data", "=", "3", ",", "order_seg", "=", "0", ",", "force_separate_z", "=", "False", ",", "\n", "cval_data", "=", "0", ",", "cval_seg", "=", "-", "1", ",", "order_z_data", "=", "0", ",", "order_z_seg", "=", "0", ",", "\n", "separate_z_anisotropy_threshold", "=", "RESAMPLING_SEPARATE_Z_ANISO_THRESHOLD", ")", ":", "\n", "    ", "\"\"\"\n    :param cval_seg:\n    :param cval_data:\n    :param data:\n    :param seg:\n    :param original_spacing:\n    :param target_spacing:\n    :param order_data:\n    :param order_seg:\n    :param force_separate_z: if None then we dynamically decide how to resample along z, if True/False then always\n    /never resample along z separately\n    :param order_z_seg: only applies if do_separate_z is True\n    :param order_z_data: only applies if do_separate_z is True\n    :param separate_z_anisotropy_threshold: if max_spacing > separate_z_anisotropy_threshold * min_spacing (per axis)\n    then resample along lowres axis with order_z_data/order_z_seg instead of order_data/order_seg\n\n    :return:\n    \"\"\"", "\n", "assert", "not", "(", "(", "data", "is", "None", ")", "and", "(", "seg", "is", "None", ")", ")", "\n", "if", "data", "is", "not", "None", ":", "\n", "        ", "assert", "len", "(", "data", ".", "shape", ")", "==", "4", ",", "\"data must be c x y z\"", "\n", "", "if", "seg", "is", "not", "None", ":", "\n", "        ", "assert", "len", "(", "seg", ".", "shape", ")", "==", "4", ",", "\"seg must be c x y z\"", "\n", "\n", "", "if", "data", "is", "not", "None", ":", "\n", "        ", "shape", "=", "np", ".", "array", "(", "data", "[", "0", "]", ".", "shape", ")", "\n", "", "else", ":", "\n", "        ", "shape", "=", "np", ".", "array", "(", "seg", "[", "0", "]", ".", "shape", ")", "\n", "", "new_shape", "=", "np", ".", "round", "(", "(", "(", "np", ".", "array", "(", "original_spacing", ")", "/", "np", ".", "array", "(", "target_spacing", ")", ")", ".", "astype", "(", "float", ")", "*", "shape", ")", ")", ".", "astype", "(", "int", ")", "\n", "\n", "if", "force_separate_z", "is", "not", "None", ":", "\n", "        ", "do_separate_z", "=", "force_separate_z", "\n", "if", "force_separate_z", ":", "\n", "            ", "axis", "=", "get_lowres_axis", "(", "original_spacing", ")", "\n", "", "else", ":", "\n", "            ", "axis", "=", "None", "\n", "", "", "else", ":", "\n", "        ", "if", "get_do_separate_z", "(", "original_spacing", ",", "separate_z_anisotropy_threshold", ")", ":", "\n", "            ", "do_separate_z", "=", "True", "\n", "axis", "=", "get_lowres_axis", "(", "original_spacing", ")", "\n", "", "elif", "get_do_separate_z", "(", "target_spacing", ",", "separate_z_anisotropy_threshold", ")", ":", "\n", "            ", "do_separate_z", "=", "True", "\n", "axis", "=", "get_lowres_axis", "(", "target_spacing", ")", "\n", "", "else", ":", "\n", "            ", "do_separate_z", "=", "False", "\n", "axis", "=", "None", "\n", "\n", "", "", "if", "axis", "is", "not", "None", ":", "\n", "        ", "if", "len", "(", "axis", ")", "==", "3", ":", "\n", "# every axis has the spacing", "\n", "            ", "axis", "=", "(", "0", ",", ")", "\n", "", "elif", "len", "(", "axis", ")", "==", "2", ":", "\n", "            ", "print", "(", "\"WARNING: axis has len 2, axis: %s, spacing: %s, target_spacing: %s\"", "%", "(", "str", "(", "axis", ")", ",", "original_spacing", ",", "target_spacing", ")", ")", "\n", "do_separate_z", "=", "False", "\n", "", "else", ":", "\n", "            ", "pass", "\n", "\n", "", "", "if", "data", "is", "not", "None", ":", "\n", "        ", "data_reshaped", "=", "resample_data_or_seg", "(", "data", ",", "new_shape", ",", "False", ",", "axis", ",", "order_data", ",", "do_separate_z", ",", "cval", "=", "cval_data", ",", "\n", "order_z", "=", "order_z_data", ")", "\n", "", "else", ":", "\n", "        ", "data_reshaped", "=", "None", "\n", "", "if", "seg", "is", "not", "None", ":", "\n", "        ", "seg_reshaped", "=", "resample_data_or_seg", "(", "seg", ",", "new_shape", ",", "True", ",", "axis", ",", "order_seg", ",", "do_separate_z", ",", "cval", "=", "cval_seg", ",", "\n", "order_z", "=", "order_z_seg", ")", "\n", "", "else", ":", "\n", "        ", "seg_reshaped", "=", "None", "\n", "", "return", "data_reshaped", ",", "seg_reshaped", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.preprocessing.preprocessing.resample_data_or_seg": [[109, 199], ["data.astype.astype", "numpy.array", "numpy.array", "numpy.any", "len", "collections.OrderedDict", "np.vstack.astype", "print", "print", "range", "numpy.vstack", "print", "range", "numpy.vstack", "len", "range", "numpy.stack", "np.zeros.append", "numpy.array", "np.vstack.append", "np.stack.append", "float", "float", "float", "np.vstack.append", "numpy.unique", "numpy.zeros", "enumerate", "np.vstack.append", "resize_fn", "resize_fn", "np.stack.append", "np.stack.append", "numpy.round", "resize_fn", "resize_fn", "scipy.ndimage.interpolation.map_coordinates", "scipy.ndimage.interpolation.map_coordinates"], "function", ["None"], ["", "def", "resample_data_or_seg", "(", "data", ",", "new_shape", ",", "is_seg", ",", "axis", "=", "None", ",", "order", "=", "3", ",", "do_separate_z", "=", "False", ",", "cval", "=", "0", ",", "order_z", "=", "0", ")", ":", "\n", "    ", "\"\"\"\n    separate_z=True will resample with order 0 along z\n    :param data:\n    :param new_shape:\n    :param is_seg:\n    :param axis:\n    :param order:\n    :param do_separate_z:\n    :param cval:\n    :param order_z: only applies if do_separate_z is True\n    :return:\n    \"\"\"", "\n", "assert", "len", "(", "data", ".", "shape", ")", "==", "4", ",", "\"data must be (c, x, y, z)\"", "\n", "if", "is_seg", ":", "\n", "        ", "resize_fn", "=", "resize_segmentation", "\n", "kwargs", "=", "OrderedDict", "(", ")", "\n", "", "else", ":", "\n", "        ", "resize_fn", "=", "resize", "\n", "kwargs", "=", "{", "'mode'", ":", "'edge'", ",", "'anti_aliasing'", ":", "False", "}", "\n", "", "dtype_data", "=", "data", ".", "dtype", "\n", "data", "=", "data", ".", "astype", "(", "float", ")", "\n", "shape", "=", "np", ".", "array", "(", "data", "[", "0", "]", ".", "shape", ")", "\n", "new_shape", "=", "np", ".", "array", "(", "new_shape", ")", "\n", "if", "np", ".", "any", "(", "shape", "!=", "new_shape", ")", ":", "\n", "        ", "if", "do_separate_z", ":", "\n", "            ", "print", "(", "\"separate z, order in z is\"", ",", "order_z", ",", "\"order inplane is\"", ",", "order", ")", "\n", "assert", "len", "(", "axis", ")", "==", "1", ",", "\"only one anisotropic axis supported\"", "\n", "axis", "=", "axis", "[", "0", "]", "\n", "if", "axis", "==", "0", ":", "\n", "                ", "new_shape_2d", "=", "new_shape", "[", "1", ":", "]", "\n", "", "elif", "axis", "==", "1", ":", "\n", "                ", "new_shape_2d", "=", "new_shape", "[", "[", "0", ",", "2", "]", "]", "\n", "", "else", ":", "\n", "                ", "new_shape_2d", "=", "new_shape", "[", ":", "-", "1", "]", "\n", "\n", "", "reshaped_final_data", "=", "[", "]", "\n", "for", "c", "in", "range", "(", "data", ".", "shape", "[", "0", "]", ")", ":", "\n", "                ", "reshaped_data", "=", "[", "]", "\n", "for", "slice_id", "in", "range", "(", "shape", "[", "axis", "]", ")", ":", "\n", "                    ", "if", "axis", "==", "0", ":", "\n", "                        ", "reshaped_data", ".", "append", "(", "resize_fn", "(", "data", "[", "c", ",", "slice_id", "]", ",", "new_shape_2d", ",", "order", ",", "cval", "=", "cval", ",", "**", "kwargs", ")", ")", "\n", "", "elif", "axis", "==", "1", ":", "\n", "                        ", "reshaped_data", ".", "append", "(", "resize_fn", "(", "data", "[", "c", ",", ":", ",", "slice_id", "]", ",", "new_shape_2d", ",", "order", ",", "cval", "=", "cval", ",", "**", "kwargs", ")", ")", "\n", "", "else", ":", "\n", "                        ", "reshaped_data", ".", "append", "(", "resize_fn", "(", "data", "[", "c", ",", ":", ",", ":", ",", "slice_id", "]", ",", "new_shape_2d", ",", "order", ",", "cval", "=", "cval", ",", "\n", "**", "kwargs", ")", ")", "\n", "", "", "reshaped_data", "=", "np", ".", "stack", "(", "reshaped_data", ",", "axis", ")", "\n", "if", "shape", "[", "axis", "]", "!=", "new_shape", "[", "axis", "]", ":", "\n", "\n", "# The following few lines are blatantly copied and modified from sklearn's resize()", "\n", "                    ", "rows", ",", "cols", ",", "dim", "=", "new_shape", "[", "0", "]", ",", "new_shape", "[", "1", "]", ",", "new_shape", "[", "2", "]", "\n", "orig_rows", ",", "orig_cols", ",", "orig_dim", "=", "reshaped_data", ".", "shape", "\n", "\n", "row_scale", "=", "float", "(", "orig_rows", ")", "/", "rows", "\n", "col_scale", "=", "float", "(", "orig_cols", ")", "/", "cols", "\n", "dim_scale", "=", "float", "(", "orig_dim", ")", "/", "dim", "\n", "\n", "map_rows", ",", "map_cols", ",", "map_dims", "=", "np", ".", "mgrid", "[", ":", "rows", ",", ":", "cols", ",", ":", "dim", "]", "\n", "map_rows", "=", "row_scale", "*", "(", "map_rows", "+", "0.5", ")", "-", "0.5", "\n", "map_cols", "=", "col_scale", "*", "(", "map_cols", "+", "0.5", ")", "-", "0.5", "\n", "map_dims", "=", "dim_scale", "*", "(", "map_dims", "+", "0.5", ")", "-", "0.5", "\n", "\n", "coord_map", "=", "np", ".", "array", "(", "[", "map_rows", ",", "map_cols", ",", "map_dims", "]", ")", "\n", "if", "not", "is_seg", "or", "order_z", "==", "0", ":", "\n", "                        ", "reshaped_final_data", ".", "append", "(", "map_coordinates", "(", "reshaped_data", ",", "coord_map", ",", "order", "=", "order_z", ",", "cval", "=", "cval", ",", "\n", "mode", "=", "'nearest'", ")", "[", "None", "]", ")", "\n", "", "else", ":", "\n", "                        ", "unique_labels", "=", "np", ".", "unique", "(", "reshaped_data", ")", "\n", "reshaped", "=", "np", ".", "zeros", "(", "new_shape", ",", "dtype", "=", "dtype_data", ")", "\n", "\n", "for", "i", ",", "cl", "in", "enumerate", "(", "unique_labels", ")", ":", "\n", "                            ", "reshaped_multihot", "=", "np", ".", "round", "(", "\n", "map_coordinates", "(", "(", "reshaped_data", "==", "cl", ")", ".", "astype", "(", "float", ")", ",", "coord_map", ",", "order", "=", "order_z", ",", "\n", "cval", "=", "cval", ",", "mode", "=", "'nearest'", ")", ")", "\n", "reshaped", "[", "reshaped_multihot", ">", "0.5", "]", "=", "cl", "\n", "", "reshaped_final_data", ".", "append", "(", "reshaped", "[", "None", "]", ")", "\n", "", "", "else", ":", "\n", "                    ", "reshaped_final_data", ".", "append", "(", "reshaped_data", "[", "None", "]", ")", "\n", "", "", "reshaped_final_data", "=", "np", ".", "vstack", "(", "reshaped_final_data", ")", "\n", "", "else", ":", "\n", "            ", "print", "(", "\"no separate z, order\"", ",", "order", ")", "\n", "reshaped", "=", "[", "]", "\n", "for", "c", "in", "range", "(", "data", ".", "shape", "[", "0", "]", ")", ":", "\n", "                ", "reshaped", ".", "append", "(", "resize_fn", "(", "data", "[", "c", "]", ",", "new_shape", ",", "order", ",", "cval", "=", "cval", ",", "**", "kwargs", ")", "[", "None", "]", ")", "\n", "", "reshaped_final_data", "=", "np", ".", "vstack", "(", "reshaped", ")", "\n", "", "return", "reshaped_final_data", ".", "astype", "(", "dtype_data", ")", "\n", "", "else", ":", "\n", "        ", "print", "(", "\"no resampling necessary\"", ")", "\n", "return", "data", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.preprocessing.cropping.ImageCropper.__init__": [[124, 137], ["maybe_mkdir_p"], "methods", ["None"], ["    ", "def", "__init__", "(", "self", ",", "num_threads", ",", "output_folder", "=", "None", ")", ":", "\n", "        ", "\"\"\"\n        This one finds a mask of nonzero elements (must be nonzero in all modalities) and crops the image to that mask.\n        In the case of BRaTS and ISLES data this results in a significant reduction in image size\n        :param num_threads:\n        :param output_folder: whete to store the cropped data\n        :param list_of_files:\n        \"\"\"", "\n", "self", ".", "output_folder", "=", "output_folder", "\n", "self", ".", "num_threads", "=", "num_threads", "\n", "\n", "if", "self", ".", "output_folder", "is", "not", "None", ":", "\n", "            ", "maybe_mkdir_p", "(", "self", ".", "output_folder", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.preprocessing.cropping.ImageCropper.crop": [[138, 151], ["cropping.crop_to_nonzero", "print", "numpy.unique", "numpy.array"], "methods", ["home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.preprocessing.cropping.crop_to_nonzero"], ["", "", "@", "staticmethod", "\n", "def", "crop", "(", "data", ",", "properties", ",", "seg", "=", "None", ")", ":", "\n", "        ", "shape_before", "=", "data", ".", "shape", "\n", "data", ",", "seg", ",", "bbox", "=", "crop_to_nonzero", "(", "data", ",", "seg", ",", "nonzero_label", "=", "-", "1", ")", "\n", "shape_after", "=", "data", ".", "shape", "\n", "print", "(", "\"before crop:\"", ",", "shape_before", ",", "\"after crop:\"", ",", "shape_after", ",", "\"spacing:\"", ",", "\n", "np", ".", "array", "(", "properties", "[", "\"original_spacing\"", "]", ")", ",", "\"\\n\"", ")", "\n", "\n", "properties", "[", "\"crop_bbox\"", "]", "=", "bbox", "\n", "properties", "[", "'classes'", "]", "=", "np", ".", "unique", "(", "seg", ")", "\n", "seg", "[", "seg", "<", "-", "1", "]", "=", "0", "\n", "properties", "[", "\"size_after_cropping\"", "]", "=", "data", "[", "0", "]", ".", "shape", "\n", "return", "data", ",", "seg", ",", "properties", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.preprocessing.cropping.ImageCropper.crop_from_list_of_files": [[152, 156], ["cropping.load_case_from_list_of_files", "cropping.ImageCropper.crop"], "methods", ["home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.preprocessing.cropping.load_case_from_list_of_files", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.preprocessing.cropping.ImageCropper.crop"], ["", "@", "staticmethod", "\n", "def", "crop_from_list_of_files", "(", "data_files", ",", "seg_file", "=", "None", ")", ":", "\n", "        ", "data", ",", "seg", ",", "properties", "=", "load_case_from_list_of_files", "(", "data_files", ",", "seg_file", ")", "\n", "return", "ImageCropper", ".", "crop", "(", "data", ",", "properties", ",", "seg", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.preprocessing.cropping.ImageCropper.load_crop_save": [[157, 174], ["print", "cropping.ImageCropper.crop_from_list_of_files", "numpy.vstack", "numpy.savez_compressed", "print", "print", "os.path.join", "open", "pickle.dump", "os.path.isfile", "os.path.isfile", "os.path.join", "os.path.join", "os.path.join"], "methods", ["home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.preprocessing.cropping.ImageCropper.crop_from_list_of_files"], ["", "def", "load_crop_save", "(", "self", ",", "case", ",", "case_identifier", ",", "overwrite_existing", "=", "False", ")", ":", "\n", "        ", "try", ":", "\n", "            ", "print", "(", "case_identifier", ")", "\n", "if", "overwrite_existing", "or", "(", "not", "os", ".", "path", ".", "isfile", "(", "os", ".", "path", ".", "join", "(", "self", ".", "output_folder", ",", "\"%s.npz\"", "%", "case_identifier", ")", ")", "\n", "or", "not", "os", ".", "path", ".", "isfile", "(", "os", ".", "path", ".", "join", "(", "self", ".", "output_folder", ",", "\"%s.pkl\"", "%", "case_identifier", ")", ")", ")", ":", "\n", "\n", "                ", "data", ",", "seg", ",", "properties", "=", "self", ".", "crop_from_list_of_files", "(", "case", "[", ":", "-", "1", "]", ",", "case", "[", "-", "1", "]", ")", "\n", "\n", "all_data", "=", "np", ".", "vstack", "(", "(", "data", ",", "seg", ")", ")", "\n", "np", ".", "savez_compressed", "(", "os", ".", "path", ".", "join", "(", "self", ".", "output_folder", ",", "\"%s.npz\"", "%", "case_identifier", ")", ",", "data", "=", "all_data", ")", "\n", "with", "open", "(", "os", ".", "path", ".", "join", "(", "self", ".", "output_folder", ",", "\"%s.pkl\"", "%", "case_identifier", ")", ",", "'wb'", ")", "as", "f", ":", "\n", "                    ", "pickle", ".", "dump", "(", "properties", ",", "f", ")", "\n", "", "", "", "except", "Exception", "as", "e", ":", "\n", "            ", "print", "(", "\"Exception in\"", ",", "case_identifier", ",", "\":\"", ")", "\n", "print", "(", "e", ")", "\n", "raise", "e", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.preprocessing.cropping.ImageCropper.get_list_of_cropped_files": [[175, 177], ["subfiles"], "methods", ["None"], ["", "", "def", "get_list_of_cropped_files", "(", "self", ")", ":", "\n", "        ", "return", "subfiles", "(", "self", ".", "output_folder", ",", "join", "=", "True", ",", "suffix", "=", "\".npz\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.preprocessing.cropping.ImageCropper.get_patient_identifiers_from_cropped_files": [[178, 180], ["cropping.ImageCropper.get_list_of_cropped_files", "i.split"], "methods", ["home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.preprocessing.cropping.ImageCropper.get_list_of_cropped_files"], ["", "def", "get_patient_identifiers_from_cropped_files", "(", "self", ")", ":", "\n", "        ", "return", "[", "i", ".", "split", "(", "\"/\"", ")", "[", "-", "1", "]", "[", ":", "-", "4", "]", "for", "i", "in", "self", ".", "get_list_of_cropped_files", "(", ")", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.preprocessing.cropping.ImageCropper.run_cropping": [[181, 208], ["os.path.join", "maybe_mkdir_p", "enumerate", "enumerate", "multiprocessing.Pool", "multiprocessing.Pool.starmap", "multiprocessing.Pool.close", "multiprocessing.Pool.join", "cropping.get_case_identifier", "list_of_args.append", "shutil.copy"], "methods", ["home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.preprocessing.cropping.get_case_identifier"], ["", "def", "run_cropping", "(", "self", ",", "list_of_files", ",", "overwrite_existing", "=", "False", ",", "output_folder", "=", "None", ")", ":", "\n", "        ", "\"\"\"\n        also copied ground truth nifti segmentation into the preprocessed folder so that we can use them for evaluation\n        on the cluster\n        :param list_of_files: list of list of files [[PATIENTID_TIMESTEP_0000.nii.gz], [PATIENTID_TIMESTEP_0000.nii.gz]]\n        :param overwrite_existing:\n        :param output_folder:\n        :return:\n        \"\"\"", "\n", "if", "output_folder", "is", "not", "None", ":", "\n", "            ", "self", ".", "output_folder", "=", "output_folder", "\n", "\n", "", "output_folder_gt", "=", "os", ".", "path", ".", "join", "(", "self", ".", "output_folder", ",", "\"gt_segmentations\"", ")", "\n", "maybe_mkdir_p", "(", "output_folder_gt", ")", "\n", "for", "j", ",", "case", "in", "enumerate", "(", "list_of_files", ")", ":", "\n", "            ", "if", "case", "[", "-", "1", "]", "is", "not", "None", ":", "\n", "                ", "shutil", ".", "copy", "(", "case", "[", "-", "1", "]", ",", "output_folder_gt", ")", "\n", "\n", "", "", "list_of_args", "=", "[", "]", "\n", "for", "j", ",", "case", "in", "enumerate", "(", "list_of_files", ")", ":", "\n", "            ", "case_identifier", "=", "get_case_identifier", "(", "case", ")", "\n", "list_of_args", ".", "append", "(", "(", "case", ",", "case_identifier", ",", "overwrite_existing", ")", ")", "\n", "\n", "", "p", "=", "Pool", "(", "self", ".", "num_threads", ")", "\n", "p", ".", "starmap", "(", "self", ".", "load_crop_save", ",", "list_of_args", ")", "\n", "p", ".", "close", "(", ")", "\n", "p", ".", "join", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.preprocessing.cropping.ImageCropper.load_properties": [[209, 213], ["open", "pickle.load", "os.path.join"], "methods", ["None"], ["", "def", "load_properties", "(", "self", ",", "case_identifier", ")", ":", "\n", "        ", "with", "open", "(", "os", ".", "path", ".", "join", "(", "self", ".", "output_folder", ",", "\"%s.pkl\"", "%", "case_identifier", ")", ",", "'rb'", ")", "as", "f", ":", "\n", "            ", "properties", "=", "pickle", ".", "load", "(", "f", ")", "\n", "", "return", "properties", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.preprocessing.cropping.ImageCropper.save_properties": [[214, 217], ["open", "pickle.dump", "os.path.join"], "methods", ["None"], ["", "def", "save_properties", "(", "self", ",", "case_identifier", ",", "properties", ")", ":", "\n", "        ", "with", "open", "(", "os", ".", "path", ".", "join", "(", "self", ".", "output_folder", ",", "\"%s.pkl\"", "%", "case_identifier", ")", ",", "'wb'", ")", "as", "f", ":", "\n", "            ", "pickle", ".", "dump", "(", "properties", ",", "f", ")", "\n", "", "", "", ""]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.preprocessing.cropping.create_nonzero_mask": [[23, 32], ["numpy.zeros", "range", "binary_fill_holes", "len", "len"], "function", ["None"], ["def", "create_nonzero_mask", "(", "data", ")", ":", "\n", "    ", "from", "scipy", ".", "ndimage", "import", "binary_fill_holes", "\n", "assert", "len", "(", "data", ".", "shape", ")", "==", "4", "or", "len", "(", "data", ".", "shape", ")", "==", "3", ",", "\"data must have shape (C, X, Y, Z) or shape (C, X, Y)\"", "\n", "nonzero_mask", "=", "np", ".", "zeros", "(", "data", ".", "shape", "[", "1", ":", "]", ",", "dtype", "=", "bool", ")", "\n", "for", "c", "in", "range", "(", "data", ".", "shape", "[", "0", "]", ")", ":", "\n", "        ", "this_mask", "=", "data", "[", "c", "]", "!=", "0", "\n", "nonzero_mask", "=", "nonzero_mask", "|", "this_mask", "\n", "", "nonzero_mask", "=", "binary_fill_holes", "(", "nonzero_mask", ")", "\n", "return", "nonzero_mask", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.preprocessing.cropping.get_bbox_from_mask": [[34, 43], ["numpy.where", "int", "int", "int", "numpy.min", "int", "numpy.min", "int", "numpy.min", "int", "numpy.max", "numpy.max", "numpy.max"], "function", ["None"], ["", "def", "get_bbox_from_mask", "(", "mask", ",", "outside_value", "=", "0", ")", ":", "\n", "    ", "mask_voxel_coords", "=", "np", ".", "where", "(", "mask", "!=", "outside_value", ")", "\n", "minzidx", "=", "int", "(", "np", ".", "min", "(", "mask_voxel_coords", "[", "0", "]", ")", ")", "\n", "maxzidx", "=", "int", "(", "np", ".", "max", "(", "mask_voxel_coords", "[", "0", "]", ")", ")", "+", "1", "\n", "minxidx", "=", "int", "(", "np", ".", "min", "(", "mask_voxel_coords", "[", "1", "]", ")", ")", "\n", "maxxidx", "=", "int", "(", "np", ".", "max", "(", "mask_voxel_coords", "[", "1", "]", ")", ")", "+", "1", "\n", "minyidx", "=", "int", "(", "np", ".", "min", "(", "mask_voxel_coords", "[", "2", "]", ")", ")", "\n", "maxyidx", "=", "int", "(", "np", ".", "max", "(", "mask_voxel_coords", "[", "2", "]", ")", ")", "+", "1", "\n", "return", "[", "[", "minzidx", ",", "maxzidx", "]", ",", "[", "minxidx", ",", "maxxidx", "]", ",", "[", "minyidx", ",", "maxyidx", "]", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.preprocessing.cropping.crop_to_bbox": [[45, 49], ["len", "slice", "slice", "slice"], "function", ["None"], ["", "def", "crop_to_bbox", "(", "image", ",", "bbox", ")", ":", "\n", "    ", "assert", "len", "(", "image", ".", "shape", ")", "==", "3", ",", "\"only supports 3d images\"", "\n", "resizer", "=", "(", "slice", "(", "bbox", "[", "0", "]", "[", "0", "]", ",", "bbox", "[", "0", "]", "[", "1", "]", ")", ",", "slice", "(", "bbox", "[", "1", "]", "[", "0", "]", ",", "bbox", "[", "1", "]", "[", "1", "]", ")", ",", "slice", "(", "bbox", "[", "2", "]", "[", "0", "]", ",", "bbox", "[", "2", "]", "[", "1", "]", ")", ")", "\n", "return", "image", "[", "resizer", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.preprocessing.cropping.get_case_identifier": [[51, 54], ["[].split", "case[].split"], "function", ["None"], ["", "def", "get_case_identifier", "(", "case", ")", ":", "\n", "    ", "case_identifier", "=", "case", "[", "0", "]", ".", "split", "(", "\"/\"", ")", "[", "-", "1", "]", ".", "split", "(", "\".nii.gz\"", ")", "[", "0", "]", "[", ":", "-", "5", "]", "\n", "return", "case_identifier", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.preprocessing.cropping.get_case_identifier_from_npz": [[56, 59], ["case.split"], "function", ["None"], ["", "def", "get_case_identifier_from_npz", "(", "case", ")", ":", "\n", "    ", "case_identifier", "=", "case", ".", "split", "(", "\"/\"", ")", "[", "-", "1", "]", "[", ":", "-", "4", "]", "\n", "return", "case_identifier", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.preprocessing.cropping.load_case_from_list_of_files": [[61, 82], ["collections.OrderedDict", "data_itk[].GetOrigin", "data_itk[].GetSpacing", "data_itk[].GetDirection", "numpy.vstack", "isinstance", "isinstance", "SimpleITK.ReadImage", "numpy.array", "numpy.array", "SimpleITK.ReadImage", "[].astype", "np.vstack.astype", "data_itk[].GetSize", "data_itk[].GetSpacing", "SimpleITK.GetArrayFromImage", "SimpleITK.GetArrayFromImage"], "function", ["None"], ["", "def", "load_case_from_list_of_files", "(", "data_files", ",", "seg_file", "=", "None", ")", ":", "\n", "    ", "assert", "isinstance", "(", "data_files", ",", "list", ")", "or", "isinstance", "(", "data_files", ",", "tuple", ")", ",", "\"case must be either a list or a tuple\"", "\n", "properties", "=", "OrderedDict", "(", ")", "\n", "data_itk", "=", "[", "sitk", ".", "ReadImage", "(", "f", ")", "for", "f", "in", "data_files", "]", "\n", "\n", "properties", "[", "\"original_size_of_raw_data\"", "]", "=", "np", ".", "array", "(", "data_itk", "[", "0", "]", ".", "GetSize", "(", ")", ")", "[", "[", "2", ",", "1", ",", "0", "]", "]", "\n", "properties", "[", "\"original_spacing\"", "]", "=", "np", ".", "array", "(", "data_itk", "[", "0", "]", ".", "GetSpacing", "(", ")", ")", "[", "[", "2", ",", "1", ",", "0", "]", "]", "\n", "properties", "[", "\"list_of_data_files\"", "]", "=", "data_files", "\n", "properties", "[", "\"seg_file\"", "]", "=", "seg_file", "\n", "\n", "properties", "[", "\"itk_origin\"", "]", "=", "data_itk", "[", "0", "]", ".", "GetOrigin", "(", ")", "\n", "properties", "[", "\"itk_spacing\"", "]", "=", "data_itk", "[", "0", "]", ".", "GetSpacing", "(", ")", "\n", "properties", "[", "\"itk_direction\"", "]", "=", "data_itk", "[", "0", "]", ".", "GetDirection", "(", ")", "\n", "\n", "data_npy", "=", "np", ".", "vstack", "(", "[", "sitk", ".", "GetArrayFromImage", "(", "d", ")", "[", "None", "]", "for", "d", "in", "data_itk", "]", ")", "\n", "if", "seg_file", "is", "not", "None", ":", "\n", "        ", "seg_itk", "=", "sitk", ".", "ReadImage", "(", "seg_file", ")", "\n", "seg_npy", "=", "sitk", ".", "GetArrayFromImage", "(", "seg_itk", ")", "[", "None", "]", ".", "astype", "(", "np", ".", "float32", ")", "\n", "", "else", ":", "\n", "        ", "seg_npy", "=", "None", "\n", "", "return", "data_npy", ".", "astype", "(", "np", ".", "float32", ")", ",", "seg_npy", ",", "properties", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.preprocessing.cropping.crop_to_nonzero": [[84, 117], ["cropping.create_nonzero_mask", "cropping.get_bbox_from_mask", "range", "numpy.vstack", "cropping.crop_to_bbox", "cropped_data.append", "range", "numpy.vstack", "cropping.crop_to_bbox", "nonzero_mask.astype.astype", "cropping.crop_to_bbox", "cropped_seg.append"], "function", ["home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.preprocessing.cropping.create_nonzero_mask", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.preprocessing.cropping.get_bbox_from_mask", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.preprocessing.cropping.crop_to_bbox", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.preprocessing.cropping.crop_to_bbox", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.preprocessing.cropping.crop_to_bbox"], ["", "def", "crop_to_nonzero", "(", "data", ",", "seg", "=", "None", ",", "nonzero_label", "=", "-", "1", ")", ":", "\n", "    ", "\"\"\"\n\n    :param data:\n    :param seg:\n    :param nonzero_label: this will be written into the segmentation map\n    :return:\n    \"\"\"", "\n", "nonzero_mask", "=", "create_nonzero_mask", "(", "data", ")", "\n", "bbox", "=", "get_bbox_from_mask", "(", "nonzero_mask", ",", "0", ")", "\n", "\n", "cropped_data", "=", "[", "]", "\n", "for", "c", "in", "range", "(", "data", ".", "shape", "[", "0", "]", ")", ":", "\n", "        ", "cropped", "=", "crop_to_bbox", "(", "data", "[", "c", "]", ",", "bbox", ")", "\n", "cropped_data", ".", "append", "(", "cropped", "[", "None", "]", ")", "\n", "", "data", "=", "np", ".", "vstack", "(", "cropped_data", ")", "\n", "\n", "if", "seg", "is", "not", "None", ":", "\n", "        ", "cropped_seg", "=", "[", "]", "\n", "for", "c", "in", "range", "(", "seg", ".", "shape", "[", "0", "]", ")", ":", "\n", "            ", "cropped", "=", "crop_to_bbox", "(", "seg", "[", "c", "]", ",", "bbox", ")", "\n", "cropped_seg", ".", "append", "(", "cropped", "[", "None", "]", ")", "\n", "", "seg", "=", "np", ".", "vstack", "(", "cropped_seg", ")", "\n", "\n", "", "nonzero_mask", "=", "crop_to_bbox", "(", "nonzero_mask", ",", "bbox", ")", "[", "None", "]", "\n", "if", "seg", "is", "not", "None", ":", "\n", "        ", "seg", "[", "(", "seg", "==", "0", ")", "&", "(", "nonzero_mask", "==", "0", ")", "]", "=", "nonzero_label", "\n", "", "else", ":", "\n", "        ", "nonzero_mask", "=", "nonzero_mask", ".", "astype", "(", "int", ")", "\n", "nonzero_mask", "[", "nonzero_mask", "==", "0", "]", "=", "nonzero_label", "\n", "nonzero_mask", "[", "nonzero_mask", ">", "0", "]", "=", "0", "\n", "seg", "=", "nonzero_mask", "\n", "", "return", "data", ",", "seg", ",", "bbox", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.preprocessing.cropping.get_patient_identifiers_from_cropped_files": [[119, 121], ["subfiles", "i.split"], "function", ["None"], ["", "def", "get_patient_identifiers_from_cropped_files", "(", "folder", ")", ":", "\n", "    ", "return", "[", "i", ".", "split", "(", "\"/\"", ")", "[", "-", "1", "]", "[", ":", "-", "4", "]", "for", "i", "in", "subfiles", "(", "folder", ",", "join", "=", "True", ",", "suffix", "=", "\".npz\"", ")", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.network_architecture.generic_UNet_DP.Generic_UNet_DP.__init__": [[27, 60], ["nnunet.network_architecture.initialization.InitWeights_He", "nnunet.network_architecture.generic_UNet.Generic_UNet.__init__", "nnunet.training.loss_functions.crossentropy.RobustCrossEntropyLoss"], "methods", ["home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.common.layers.ResizeImage.__init__"], ["    ", "def", "__init__", "(", "self", ",", "input_channels", ",", "base_num_features", ",", "num_classes", ",", "num_pool", ",", "num_conv_per_stage", "=", "2", ",", "\n", "feat_map_mul_on_downscale", "=", "2", ",", "conv_op", "=", "nn", ".", "Conv2d", ",", "\n", "norm_op", "=", "nn", ".", "BatchNorm2d", ",", "norm_op_kwargs", "=", "None", ",", "\n", "dropout_op", "=", "nn", ".", "Dropout2d", ",", "dropout_op_kwargs", "=", "None", ",", "\n", "nonlin", "=", "nn", ".", "LeakyReLU", ",", "nonlin_kwargs", "=", "None", ",", "deep_supervision", "=", "True", ",", "dropout_in_localization", "=", "False", ",", "\n", "weightInitializer", "=", "InitWeights_He", "(", "1e-2", ")", ",", "pool_op_kernel_sizes", "=", "None", ",", "\n", "conv_kernel_sizes", "=", "None", ",", "\n", "upscale_logits", "=", "False", ",", "convolutional_pooling", "=", "False", ",", "convolutional_upsampling", "=", "False", ",", "\n", "max_num_features", "=", "None", ")", ":", "\n", "        ", "\"\"\"\n        As opposed to the Generic_UNet, this class will compute parts of the loss function in the forward pass. This is\n        useful for GPU parallelization. The batch DICE loss, if used, must be computed over the whole batch. Therefore, in a\n        naive implementation, all softmax outputs must be copied to a single GPU which will then\n        do the loss computation all by itself. In the context of 3D Segmentation, this results in a lot of overhead AND\n        is inefficient because the DICE computation is also kinda expensive (Think 8 GPUs with a result of shape\n        2x4x128x128x128 each.). The DICE is a global metric, but its parts can be computed locally (TP, FP, FN). Thus,\n        this implementation will compute all the parts of the loss function in the forward pass (and thus in a\n        parallelized way). The results are very small (batch_size x num_classes for TP, FN and FP, respectively; scalar for CE) and\n        copied easily. Also the final steps of the loss function (computing batch dice and average CE values) are easy\n        and very quick on the one GPU they need to run on. BAM.\n        final_nonlin is lambda x:x here!\n        \"\"\"", "\n", "super", "(", "Generic_UNet_DP", ",", "self", ")", ".", "__init__", "(", "input_channels", ",", "base_num_features", ",", "num_classes", ",", "num_pool", ",", "\n", "num_conv_per_stage", ",", "\n", "feat_map_mul_on_downscale", ",", "conv_op", ",", "\n", "norm_op", ",", "norm_op_kwargs", ",", "\n", "dropout_op", ",", "dropout_op_kwargs", ",", "\n", "nonlin", ",", "nonlin_kwargs", ",", "deep_supervision", ",", "dropout_in_localization", ",", "\n", "lambda", "x", ":", "x", ",", "weightInitializer", ",", "pool_op_kernel_sizes", ",", "\n", "conv_kernel_sizes", ",", "\n", "upscale_logits", ",", "convolutional_pooling", ",", "convolutional_upsampling", ",", "\n", "max_num_features", ")", "\n", "self", ".", "ce_loss", "=", "RobustCrossEntropyLoss", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.network_architecture.generic_UNet_DP.Generic_UNet_DP.forward": [[61, 125], ["super().forward", "nnunet.utilities.nd_softmax.softmax_helper", "nnunet.training.loss_functions.dice_loss.get_tp_fp_fn_tn", "tps.append", "fps.append", "fns.append", "range", "generic_UNet_DP.Generic_UNet_DP.ce_loss().unsqueeze", "nnunet.utilities.nd_softmax.softmax_helper", "nnunet.training.loss_functions.dice_loss.get_tp_fp_fn_tn", "generic_UNet_DP.Generic_UNet_DP.ce_loss().unsqueeze", "len", "ce_losses.append", "nnunet.utilities.nd_softmax.softmax_helper", "nnunet.training.loss_functions.dice_loss.get_tp_fp_fn_tn", "tps.append", "fps.append", "fns.append", "torch.no_grad", "nnunet.utilities.nd_softmax.softmax_helper", "nnunet.utilities.nd_softmax.softmax_helper.argmax", "tuple", "torch.zeros().to", "torch.zeros().to", "torch.zeros().to", "range", "generic_UNet_DP.Generic_UNet_DP.ce_loss().unsqueeze", "generic_UNet_DP.Generic_UNet_DP.ce_loss", "range", "nnunet.utilities.tensor_utilities.sum_tensor", "nnunet.utilities.tensor_utilities.sum_tensor", "nnunet.utilities.tensor_utilities.sum_tensor", "torch.zeros().to.sum", "torch.zeros().to.sum", "torch.zeros().to.sum", "generic_UNet_DP.Generic_UNet_DP.ce_loss", "len", "torch.zeros", "torch.zeros", "torch.zeros", "generic_UNet_DP.Generic_UNet_DP.ce_loss"], "methods", ["home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.utilities.distributed.awesome_allgather_function.forward", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.loss_functions.dice_loss.get_tp_fp_fn_tn", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.loss_functions.dice_loss.get_tp_fp_fn_tn", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.loss_functions.dice_loss.get_tp_fp_fn_tn", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.utilities.tensor_utilities.sum_tensor", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.utilities.tensor_utilities.sum_tensor", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.utilities.tensor_utilities.sum_tensor"], ["", "def", "forward", "(", "self", ",", "x", ",", "y", "=", "None", ",", "return_hard_tp_fp_fn", "=", "False", ")", ":", "\n", "        ", "res", "=", "super", "(", "Generic_UNet_DP", ",", "self", ")", ".", "forward", "(", "x", ")", "# regular Generic_UNet forward pass", "\n", "\n", "if", "y", "is", "None", ":", "\n", "            ", "return", "res", "\n", "", "else", ":", "\n", "# compute ce loss", "\n", "            ", "if", "self", ".", "_deep_supervision", "and", "self", ".", "do_ds", ":", "\n", "                ", "ce_losses", "=", "[", "self", ".", "ce_loss", "(", "res", "[", "0", "]", ",", "y", "[", "0", "]", ")", ".", "unsqueeze", "(", "0", ")", "]", "\n", "tps", "=", "[", "]", "\n", "fps", "=", "[", "]", "\n", "fns", "=", "[", "]", "\n", "\n", "res_softmax", "=", "softmax_helper", "(", "res", "[", "0", "]", ")", "\n", "tp", ",", "fp", ",", "fn", ",", "_", "=", "get_tp_fp_fn_tn", "(", "res_softmax", ",", "y", "[", "0", "]", ")", "\n", "tps", ".", "append", "(", "tp", ")", "\n", "fps", ".", "append", "(", "fp", ")", "\n", "fns", ".", "append", "(", "fn", ")", "\n", "for", "i", "in", "range", "(", "1", ",", "len", "(", "y", ")", ")", ":", "\n", "                    ", "ce_losses", ".", "append", "(", "self", ".", "ce_loss", "(", "res", "[", "i", "]", ",", "y", "[", "i", "]", ")", ".", "unsqueeze", "(", "0", ")", ")", "\n", "res_softmax", "=", "softmax_helper", "(", "res", "[", "i", "]", ")", "\n", "tp", ",", "fp", ",", "fn", ",", "_", "=", "get_tp_fp_fn_tn", "(", "res_softmax", ",", "y", "[", "i", "]", ")", "\n", "tps", ".", "append", "(", "tp", ")", "\n", "fps", ".", "append", "(", "fp", ")", "\n", "fns", ".", "append", "(", "fn", ")", "\n", "", "ret", "=", "ce_losses", ",", "tps", ",", "fps", ",", "fns", "\n", "", "else", ":", "\n", "                ", "ce_loss", "=", "self", ".", "ce_loss", "(", "res", ",", "y", ")", ".", "unsqueeze", "(", "0", ")", "\n", "\n", "# tp fp and fn need the output to be softmax", "\n", "res_softmax", "=", "softmax_helper", "(", "res", ")", "\n", "\n", "tp", ",", "fp", ",", "fn", ",", "_", "=", "get_tp_fp_fn_tn", "(", "res_softmax", ",", "y", ")", "\n", "\n", "ret", "=", "ce_loss", ",", "tp", ",", "fp", ",", "fn", "\n", "\n", "", "if", "return_hard_tp_fp_fn", ":", "\n", "                ", "if", "self", ".", "_deep_supervision", "and", "self", ".", "do_ds", ":", "\n", "                    ", "output", "=", "res", "[", "0", "]", "\n", "target", "=", "y", "[", "0", "]", "\n", "", "else", ":", "\n", "                    ", "target", "=", "y", "\n", "output", "=", "res", "\n", "\n", "", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "                    ", "num_classes", "=", "output", ".", "shape", "[", "1", "]", "\n", "output_softmax", "=", "softmax_helper", "(", "output", ")", "\n", "output_seg", "=", "output_softmax", ".", "argmax", "(", "1", ")", "\n", "target", "=", "target", "[", ":", ",", "0", "]", "\n", "axes", "=", "tuple", "(", "range", "(", "1", ",", "len", "(", "target", ".", "shape", ")", ")", ")", "\n", "tp_hard", "=", "torch", ".", "zeros", "(", "(", "target", ".", "shape", "[", "0", "]", ",", "num_classes", "-", "1", ")", ")", ".", "to", "(", "output_seg", ".", "device", ".", "index", ")", "\n", "fp_hard", "=", "torch", ".", "zeros", "(", "(", "target", ".", "shape", "[", "0", "]", ",", "num_classes", "-", "1", ")", ")", ".", "to", "(", "output_seg", ".", "device", ".", "index", ")", "\n", "fn_hard", "=", "torch", ".", "zeros", "(", "(", "target", ".", "shape", "[", "0", "]", ",", "num_classes", "-", "1", ")", ")", ".", "to", "(", "output_seg", ".", "device", ".", "index", ")", "\n", "for", "c", "in", "range", "(", "1", ",", "num_classes", ")", ":", "\n", "                        ", "tp_hard", "[", ":", ",", "c", "-", "1", "]", "=", "sum_tensor", "(", "(", "output_seg", "==", "c", ")", ".", "float", "(", ")", "*", "(", "target", "==", "c", ")", ".", "float", "(", ")", ",", "axes", "=", "axes", ")", "\n", "fp_hard", "[", ":", ",", "c", "-", "1", "]", "=", "sum_tensor", "(", "(", "output_seg", "==", "c", ")", ".", "float", "(", ")", "*", "(", "target", "!=", "c", ")", ".", "float", "(", ")", ",", "axes", "=", "axes", ")", "\n", "fn_hard", "[", ":", ",", "c", "-", "1", "]", "=", "sum_tensor", "(", "(", "output_seg", "!=", "c", ")", ".", "float", "(", ")", "*", "(", "target", "==", "c", ")", ".", "float", "(", ")", ",", "axes", "=", "axes", ")", "\n", "\n", "", "tp_hard", "=", "tp_hard", ".", "sum", "(", "0", ",", "keepdim", "=", "False", ")", "[", "None", "]", "\n", "fp_hard", "=", "fp_hard", ".", "sum", "(", "0", ",", "keepdim", "=", "False", ")", "[", "None", "]", "\n", "fn_hard", "=", "fn_hard", ".", "sum", "(", "0", ",", "keepdim", "=", "False", ")", "[", "None", "]", "\n", "\n", "ret", "=", "*", "ret", ",", "tp_hard", ",", "fp_hard", ",", "fn_hard", "\n", "", "", "return", "ret", "\n", "", "", "", ""]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.network_architecture.generic_UNet.ConvDropoutNormNonlin.__init__": [[31, 63], ["torch.nn.Module.__init__", "generic_UNet.ConvDropoutNormNonlin.conv_op", "generic_UNet.ConvDropoutNormNonlin.norm_op", "generic_UNet.ConvDropoutNormNonlin.nonlin", "generic_UNet.ConvDropoutNormNonlin.dropout_op"], "methods", ["home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.common.layers.ResizeImage.__init__"], ["def", "__init__", "(", "self", ",", "input_channels", ",", "output_channels", ",", "\n", "conv_op", "=", "nn", ".", "Conv2d", ",", "conv_kwargs", "=", "None", ",", "\n", "norm_op", "=", "nn", ".", "BatchNorm2d", ",", "norm_op_kwargs", "=", "None", ",", "\n", "dropout_op", "=", "nn", ".", "Dropout2d", ",", "dropout_op_kwargs", "=", "None", ",", "\n", "nonlin", "=", "nn", ".", "LeakyReLU", ",", "nonlin_kwargs", "=", "None", ")", ":", "\n", "        ", "super", "(", "ConvDropoutNormNonlin", ",", "self", ")", ".", "__init__", "(", ")", "\n", "if", "nonlin_kwargs", "is", "None", ":", "\n", "            ", "nonlin_kwargs", "=", "{", "'negative_slope'", ":", "1e-2", ",", "'inplace'", ":", "True", "}", "\n", "", "if", "dropout_op_kwargs", "is", "None", ":", "\n", "            ", "dropout_op_kwargs", "=", "{", "'p'", ":", "0.5", ",", "'inplace'", ":", "True", "}", "\n", "", "if", "norm_op_kwargs", "is", "None", ":", "\n", "            ", "norm_op_kwargs", "=", "{", "'eps'", ":", "1e-5", ",", "'affine'", ":", "True", ",", "'momentum'", ":", "0.1", "}", "\n", "", "if", "conv_kwargs", "is", "None", ":", "\n", "            ", "conv_kwargs", "=", "{", "'kernel_size'", ":", "3", ",", "'stride'", ":", "1", ",", "'padding'", ":", "1", ",", "'dilation'", ":", "1", ",", "'bias'", ":", "True", "}", "\n", "\n", "", "self", ".", "nonlin_kwargs", "=", "nonlin_kwargs", "\n", "self", ".", "nonlin", "=", "nonlin", "\n", "self", ".", "dropout_op", "=", "dropout_op", "\n", "self", ".", "dropout_op_kwargs", "=", "dropout_op_kwargs", "\n", "self", ".", "norm_op_kwargs", "=", "norm_op_kwargs", "\n", "self", ".", "conv_kwargs", "=", "conv_kwargs", "\n", "self", ".", "conv_op", "=", "conv_op", "\n", "self", ".", "norm_op", "=", "norm_op", "\n", "\n", "self", ".", "conv", "=", "self", ".", "conv_op", "(", "input_channels", ",", "output_channels", ",", "**", "self", ".", "conv_kwargs", ")", "\n", "if", "self", ".", "dropout_op", "is", "not", "None", "and", "self", ".", "dropout_op_kwargs", "[", "'p'", "]", "is", "not", "None", "and", "self", ".", "dropout_op_kwargs", "[", "\n", "'p'", "]", ">", "0", ":", "\n", "            ", "self", ".", "dropout", "=", "self", ".", "dropout_op", "(", "**", "self", ".", "dropout_op_kwargs", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "dropout", "=", "None", "\n", "", "self", ".", "instnorm", "=", "self", ".", "norm_op", "(", "output_channels", ",", "**", "self", ".", "norm_op_kwargs", ")", "\n", "self", ".", "lrelu", "=", "self", ".", "nonlin", "(", "**", "self", ".", "nonlin_kwargs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.network_architecture.generic_UNet.ConvDropoutNormNonlin.forward": [[64, 69], ["generic_UNet.ConvDropoutNormNonlin.conv", "generic_UNet.ConvDropoutNormNonlin.lrelu", "generic_UNet.ConvDropoutNormNonlin.dropout", "generic_UNet.ConvDropoutNormNonlin.instnorm"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "x", "=", "self", ".", "conv", "(", "x", ")", "\n", "if", "self", ".", "dropout", "is", "not", "None", ":", "\n", "            ", "x", "=", "self", ".", "dropout", "(", "x", ")", "\n", "", "return", "self", ".", "lrelu", "(", "self", ".", "instnorm", "(", "x", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.network_architecture.generic_UNet.ConvDropoutNonlinNorm.forward": [[72, 77], ["generic_UNet.ConvDropoutNonlinNorm.conv", "generic_UNet.ConvDropoutNonlinNorm.instnorm", "generic_UNet.ConvDropoutNonlinNorm.dropout", "generic_UNet.ConvDropoutNonlinNorm.lrelu"], "methods", ["None"], ["    ", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "x", "=", "self", ".", "conv", "(", "x", ")", "\n", "if", "self", ".", "dropout", "is", "not", "None", ":", "\n", "            ", "x", "=", "self", ".", "dropout", "(", "x", ")", "\n", "", "return", "self", ".", "instnorm", "(", "self", ".", "lrelu", "(", "x", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.network_architecture.generic_UNet.StackedConvLayers.__init__": [[80, 140], ["torch.nn.Module.__init__", "torch.nn.Sequential", "torch.nn.Sequential", "copy.deepcopy", "basic_block", "basic_block", "range"], "methods", ["home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.common.layers.ResizeImage.__init__"], ["    ", "def", "__init__", "(", "self", ",", "input_feature_channels", ",", "output_feature_channels", ",", "num_convs", ",", "\n", "conv_op", "=", "nn", ".", "Conv2d", ",", "conv_kwargs", "=", "None", ",", "\n", "norm_op", "=", "nn", ".", "BatchNorm2d", ",", "norm_op_kwargs", "=", "None", ",", "\n", "dropout_op", "=", "nn", ".", "Dropout2d", ",", "dropout_op_kwargs", "=", "None", ",", "\n", "nonlin", "=", "nn", ".", "LeakyReLU", ",", "nonlin_kwargs", "=", "None", ",", "first_stride", "=", "None", ",", "basic_block", "=", "ConvDropoutNormNonlin", ")", ":", "\n", "        ", "'''\n        stacks ConvDropoutNormLReLU layers. initial_stride will only be applied to first layer in the stack. The other parameters affect all layers\n        :param input_feature_channels:\n        :param output_feature_channels:\n        :param num_convs:\n        :param dilation:\n        :param kernel_size:\n        :param padding:\n        :param dropout:\n        :param initial_stride:\n        :param conv_op:\n        :param norm_op:\n        :param dropout_op:\n        :param inplace:\n        :param neg_slope:\n        :param norm_affine:\n        :param conv_bias:\n        '''", "\n", "self", ".", "input_channels", "=", "input_feature_channels", "\n", "self", ".", "output_channels", "=", "output_feature_channels", "\n", "\n", "if", "nonlin_kwargs", "is", "None", ":", "\n", "            ", "nonlin_kwargs", "=", "{", "'negative_slope'", ":", "1e-2", ",", "'inplace'", ":", "True", "}", "\n", "", "if", "dropout_op_kwargs", "is", "None", ":", "\n", "            ", "dropout_op_kwargs", "=", "{", "'p'", ":", "0.5", ",", "'inplace'", ":", "True", "}", "\n", "", "if", "norm_op_kwargs", "is", "None", ":", "\n", "            ", "norm_op_kwargs", "=", "{", "'eps'", ":", "1e-5", ",", "'affine'", ":", "True", ",", "'momentum'", ":", "0.1", "}", "\n", "", "if", "conv_kwargs", "is", "None", ":", "\n", "            ", "conv_kwargs", "=", "{", "'kernel_size'", ":", "3", ",", "'stride'", ":", "1", ",", "'padding'", ":", "1", ",", "'dilation'", ":", "1", ",", "'bias'", ":", "True", "}", "\n", "\n", "", "self", ".", "nonlin_kwargs", "=", "nonlin_kwargs", "\n", "self", ".", "nonlin", "=", "nonlin", "\n", "self", ".", "dropout_op", "=", "dropout_op", "\n", "self", ".", "dropout_op_kwargs", "=", "dropout_op_kwargs", "\n", "self", ".", "norm_op_kwargs", "=", "norm_op_kwargs", "\n", "self", ".", "conv_kwargs", "=", "conv_kwargs", "\n", "self", ".", "conv_op", "=", "conv_op", "\n", "self", ".", "norm_op", "=", "norm_op", "\n", "\n", "if", "first_stride", "is", "not", "None", ":", "\n", "            ", "self", ".", "conv_kwargs_first_conv", "=", "deepcopy", "(", "conv_kwargs", ")", "\n", "self", ".", "conv_kwargs_first_conv", "[", "'stride'", "]", "=", "first_stride", "\n", "", "else", ":", "\n", "            ", "self", ".", "conv_kwargs_first_conv", "=", "conv_kwargs", "\n", "\n", "", "super", "(", "StackedConvLayers", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "blocks", "=", "nn", ".", "Sequential", "(", "\n", "*", "(", "[", "basic_block", "(", "input_feature_channels", ",", "output_feature_channels", ",", "self", ".", "conv_op", ",", "\n", "self", ".", "conv_kwargs_first_conv", ",", "\n", "self", ".", "norm_op", ",", "self", ".", "norm_op_kwargs", ",", "self", ".", "dropout_op", ",", "self", ".", "dropout_op_kwargs", ",", "\n", "self", ".", "nonlin", ",", "self", ".", "nonlin_kwargs", ")", "]", "+", "\n", "[", "basic_block", "(", "output_feature_channels", ",", "output_feature_channels", ",", "self", ".", "conv_op", ",", "\n", "self", ".", "conv_kwargs", ",", "\n", "self", ".", "norm_op", ",", "self", ".", "norm_op_kwargs", ",", "self", ".", "dropout_op", ",", "self", ".", "dropout_op_kwargs", ",", "\n", "self", ".", "nonlin", ",", "self", ".", "nonlin_kwargs", ")", "for", "_", "in", "range", "(", "num_convs", "-", "1", ")", "]", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.network_architecture.generic_UNet.StackedConvLayers.forward": [[141, 143], ["generic_UNet.StackedConvLayers.blocks"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "return", "self", ".", "blocks", "(", "x", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.network_architecture.generic_UNet.Upsample.__init__": [[155, 161], ["torch.nn.Module.__init__"], "methods", ["home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.common.layers.ResizeImage.__init__"], ["    ", "def", "__init__", "(", "self", ",", "size", "=", "None", ",", "scale_factor", "=", "None", ",", "mode", "=", "'nearest'", ",", "align_corners", "=", "False", ")", ":", "\n", "        ", "super", "(", "Upsample", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "align_corners", "=", "align_corners", "\n", "self", ".", "mode", "=", "mode", "\n", "self", ".", "scale_factor", "=", "scale_factor", "\n", "self", ".", "size", "=", "size", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.network_architecture.generic_UNet.Upsample.forward": [[162, 165], ["torch.nn.functional.interpolate", "torch.nn.functional.interpolate"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "return", "nn", ".", "functional", ".", "interpolate", "(", "x", ",", "size", "=", "self", ".", "size", ",", "scale_factor", "=", "self", ".", "scale_factor", ",", "mode", "=", "self", ".", "mode", ",", "\n", "align_corners", "=", "self", ".", "align_corners", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.network_architecture.generic_UNet.Generic_UNet.__init__": [[184, 385], ["nnunet.network_architecture.initialization.InitWeights_He", "nnunet.network_architecture.neural_network.SegmentationNetwork.__init__", "numpy.prod", "range", "generic_UNet.Generic_UNet.conv_blocks_context.append", "range", "range", "range", "torch.nn.ModuleList", "torch.nn.ModuleList", "torch.nn.ModuleList", "torch.nn.ModuleList", "torch.nn.ModuleList", "torch.nn.ModuleList", "torch.nn.ModuleList", "torch.nn.ModuleList", "torch.nn.ModuleList", "torch.nn.ModuleList", "generic_UNet.Generic_UNet.conv_pad_sizes.append", "generic_UNet.Generic_UNet.conv_blocks_context.append", "int", "min", "torch.nn.Sequential", "torch.nn.Sequential", "generic_UNet.Generic_UNet.conv_blocks_localization.append", "len", "generic_UNet.Generic_UNet.seg_outputs.append", "numpy.cumprod", "torch.nn.ModuleList", "torch.nn.ModuleList", "generic_UNet.Generic_UNet.apply", "ValueError", "generic_UNet.StackedConvLayers", "generic_UNet.Generic_UNet.td.append", "numpy.round", "generic_UNet.StackedConvLayers", "generic_UNet.StackedConvLayers", "generic_UNet.Generic_UNet.tu.append", "generic_UNet.Generic_UNet.tu.append", "torch.nn.Sequential", "torch.nn.Sequential", "conv_op", "numpy.vstack", "generic_UNet.Generic_UNet.upscale_logits_ops.append", "generic_UNet.Generic_UNet.upscale_logits_ops.append", "pool_op", "generic_UNet.Upsample", "transpconv", "generic_UNet.StackedConvLayers", "generic_UNet.StackedConvLayers", "generic_UNet.Upsample", "str", "tuple", "int"], "methods", ["home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.common.layers.ResizeImage.__init__"], ["def", "__init__", "(", "self", ",", "input_channels", ",", "base_num_features", ",", "num_classes", ",", "num_pool", ",", "num_conv_per_stage", "=", "2", ",", "\n", "feat_map_mul_on_downscale", "=", "2", ",", "conv_op", "=", "nn", ".", "Conv2d", ",", "\n", "norm_op", "=", "nn", ".", "BatchNorm2d", ",", "norm_op_kwargs", "=", "None", ",", "\n", "dropout_op", "=", "nn", ".", "Dropout2d", ",", "dropout_op_kwargs", "=", "None", ",", "\n", "nonlin", "=", "nn", ".", "LeakyReLU", ",", "nonlin_kwargs", "=", "None", ",", "deep_supervision", "=", "True", ",", "dropout_in_localization", "=", "False", ",", "\n", "final_nonlin", "=", "softmax_helper", ",", "weightInitializer", "=", "InitWeights_He", "(", "1e-2", ")", ",", "pool_op_kernel_sizes", "=", "None", ",", "\n", "conv_kernel_sizes", "=", "None", ",", "\n", "upscale_logits", "=", "False", ",", "convolutional_pooling", "=", "False", ",", "convolutional_upsampling", "=", "False", ",", "\n", "max_num_features", "=", "None", ",", "basic_block", "=", "ConvDropoutNormNonlin", ",", "\n", "seg_output_use_bias", "=", "False", ")", ":", "\n", "        ", "\"\"\"\n        basically more flexible than v1, architecture is the same\n\n        Does this look complicated? Nah bro. Functionality > usability\n\n        This does everything you need, including world peace.\n\n        Questions? -> f.isensee@dkfz.de\n        \"\"\"", "\n", "super", "(", "Generic_UNet", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "convolutional_upsampling", "=", "convolutional_upsampling", "\n", "self", ".", "convolutional_pooling", "=", "convolutional_pooling", "\n", "self", ".", "upscale_logits", "=", "upscale_logits", "\n", "if", "nonlin_kwargs", "is", "None", ":", "\n", "            ", "nonlin_kwargs", "=", "{", "'negative_slope'", ":", "1e-2", ",", "'inplace'", ":", "True", "}", "\n", "", "if", "dropout_op_kwargs", "is", "None", ":", "\n", "            ", "dropout_op_kwargs", "=", "{", "'p'", ":", "0.5", ",", "'inplace'", ":", "True", "}", "\n", "", "if", "norm_op_kwargs", "is", "None", ":", "\n", "            ", "norm_op_kwargs", "=", "{", "'eps'", ":", "1e-5", ",", "'affine'", ":", "True", ",", "'momentum'", ":", "0.1", "}", "\n", "\n", "", "self", ".", "conv_kwargs", "=", "{", "'stride'", ":", "1", ",", "'dilation'", ":", "1", ",", "'bias'", ":", "True", "}", "\n", "\n", "self", ".", "nonlin", "=", "nonlin", "\n", "self", ".", "nonlin_kwargs", "=", "nonlin_kwargs", "\n", "self", ".", "dropout_op_kwargs", "=", "dropout_op_kwargs", "\n", "self", ".", "norm_op_kwargs", "=", "norm_op_kwargs", "\n", "self", ".", "weightInitializer", "=", "weightInitializer", "\n", "self", ".", "conv_op", "=", "conv_op", "\n", "self", ".", "norm_op", "=", "norm_op", "\n", "self", ".", "dropout_op", "=", "dropout_op", "\n", "self", ".", "num_classes", "=", "num_classes", "\n", "self", ".", "final_nonlin", "=", "final_nonlin", "\n", "self", ".", "_deep_supervision", "=", "deep_supervision", "\n", "self", ".", "do_ds", "=", "deep_supervision", "\n", "\n", "if", "conv_op", "==", "nn", ".", "Conv2d", ":", "\n", "            ", "upsample_mode", "=", "'bilinear'", "\n", "pool_op", "=", "nn", ".", "MaxPool2d", "\n", "transpconv", "=", "nn", ".", "ConvTranspose2d", "\n", "if", "pool_op_kernel_sizes", "is", "None", ":", "\n", "                ", "pool_op_kernel_sizes", "=", "[", "(", "2", ",", "2", ")", "]", "*", "num_pool", "\n", "", "if", "conv_kernel_sizes", "is", "None", ":", "\n", "                ", "conv_kernel_sizes", "=", "[", "(", "3", ",", "3", ")", "]", "*", "(", "num_pool", "+", "1", ")", "\n", "", "", "elif", "conv_op", "==", "nn", ".", "Conv3d", ":", "\n", "            ", "upsample_mode", "=", "'trilinear'", "\n", "pool_op", "=", "nn", ".", "MaxPool3d", "\n", "transpconv", "=", "nn", ".", "ConvTranspose3d", "\n", "if", "pool_op_kernel_sizes", "is", "None", ":", "\n", "                ", "pool_op_kernel_sizes", "=", "[", "(", "2", ",", "2", ",", "2", ")", "]", "*", "num_pool", "\n", "", "if", "conv_kernel_sizes", "is", "None", ":", "\n", "                ", "conv_kernel_sizes", "=", "[", "(", "3", ",", "3", ",", "3", ")", "]", "*", "(", "num_pool", "+", "1", ")", "\n", "", "", "else", ":", "\n", "            ", "raise", "ValueError", "(", "\"unknown convolution dimensionality, conv op: %s\"", "%", "str", "(", "conv_op", ")", ")", "\n", "\n", "", "self", ".", "input_shape_must_be_divisible_by", "=", "np", ".", "prod", "(", "pool_op_kernel_sizes", ",", "0", ",", "dtype", "=", "np", ".", "int64", ")", "\n", "self", ".", "pool_op_kernel_sizes", "=", "pool_op_kernel_sizes", "\n", "self", ".", "conv_kernel_sizes", "=", "conv_kernel_sizes", "\n", "\n", "self", ".", "conv_pad_sizes", "=", "[", "]", "\n", "for", "krnl", "in", "self", ".", "conv_kernel_sizes", ":", "\n", "            ", "self", ".", "conv_pad_sizes", ".", "append", "(", "[", "1", "if", "i", "==", "3", "else", "0", "for", "i", "in", "krnl", "]", ")", "\n", "\n", "", "if", "max_num_features", "is", "None", ":", "\n", "            ", "if", "self", ".", "conv_op", "==", "nn", ".", "Conv3d", ":", "\n", "                ", "self", ".", "max_num_features", "=", "self", ".", "MAX_NUM_FILTERS_3D", "\n", "", "else", ":", "\n", "                ", "self", ".", "max_num_features", "=", "self", ".", "MAX_FILTERS_2D", "\n", "", "", "else", ":", "\n", "            ", "self", ".", "max_num_features", "=", "max_num_features", "\n", "\n", "", "self", ".", "conv_blocks_context", "=", "[", "]", "\n", "self", ".", "conv_blocks_localization", "=", "[", "]", "\n", "self", ".", "td", "=", "[", "]", "\n", "self", ".", "tu", "=", "[", "]", "\n", "self", ".", "seg_outputs", "=", "[", "]", "\n", "\n", "output_features", "=", "base_num_features", "\n", "input_features", "=", "input_channels", "\n", "\n", "for", "d", "in", "range", "(", "num_pool", ")", ":", "\n", "# determine the first stride", "\n", "            ", "if", "d", "!=", "0", "and", "self", ".", "convolutional_pooling", ":", "\n", "                ", "first_stride", "=", "pool_op_kernel_sizes", "[", "d", "-", "1", "]", "\n", "", "else", ":", "\n", "                ", "first_stride", "=", "None", "\n", "\n", "", "self", ".", "conv_kwargs", "[", "'kernel_size'", "]", "=", "self", ".", "conv_kernel_sizes", "[", "d", "]", "\n", "self", ".", "conv_kwargs", "[", "'padding'", "]", "=", "self", ".", "conv_pad_sizes", "[", "d", "]", "\n", "# add convolutions", "\n", "self", ".", "conv_blocks_context", ".", "append", "(", "StackedConvLayers", "(", "input_features", ",", "output_features", ",", "num_conv_per_stage", ",", "\n", "self", ".", "conv_op", ",", "self", ".", "conv_kwargs", ",", "self", ".", "norm_op", ",", "\n", "self", ".", "norm_op_kwargs", ",", "self", ".", "dropout_op", ",", "\n", "self", ".", "dropout_op_kwargs", ",", "self", ".", "nonlin", ",", "self", ".", "nonlin_kwargs", ",", "\n", "first_stride", ",", "basic_block", "=", "basic_block", ")", ")", "\n", "if", "not", "self", ".", "convolutional_pooling", ":", "\n", "                ", "self", ".", "td", ".", "append", "(", "pool_op", "(", "pool_op_kernel_sizes", "[", "d", "]", ")", ")", "\n", "", "input_features", "=", "output_features", "\n", "output_features", "=", "int", "(", "np", ".", "round", "(", "output_features", "*", "feat_map_mul_on_downscale", ")", ")", "\n", "\n", "output_features", "=", "min", "(", "output_features", ",", "self", ".", "max_num_features", ")", "\n", "\n", "# now the bottleneck.", "\n", "# determine the first stride", "\n", "", "if", "self", ".", "convolutional_pooling", ":", "\n", "            ", "first_stride", "=", "pool_op_kernel_sizes", "[", "-", "1", "]", "\n", "", "else", ":", "\n", "            ", "first_stride", "=", "None", "\n", "\n", "# the output of the last conv must match the number of features from the skip connection if we are not using", "\n", "# convolutional upsampling. If we use convolutional upsampling then the reduction in feature maps will be", "\n", "# done by the transposed conv", "\n", "", "if", "self", ".", "convolutional_upsampling", ":", "\n", "            ", "final_num_features", "=", "output_features", "\n", "", "else", ":", "\n", "            ", "final_num_features", "=", "self", ".", "conv_blocks_context", "[", "-", "1", "]", ".", "output_channels", "\n", "\n", "", "self", ".", "conv_kwargs", "[", "'kernel_size'", "]", "=", "self", ".", "conv_kernel_sizes", "[", "num_pool", "]", "\n", "self", ".", "conv_kwargs", "[", "'padding'", "]", "=", "self", ".", "conv_pad_sizes", "[", "num_pool", "]", "\n", "self", ".", "conv_blocks_context", ".", "append", "(", "nn", ".", "Sequential", "(", "\n", "StackedConvLayers", "(", "input_features", ",", "output_features", ",", "num_conv_per_stage", "-", "1", ",", "self", ".", "conv_op", ",", "self", ".", "conv_kwargs", ",", "\n", "self", ".", "norm_op", ",", "self", ".", "norm_op_kwargs", ",", "self", ".", "dropout_op", ",", "self", ".", "dropout_op_kwargs", ",", "self", ".", "nonlin", ",", "\n", "self", ".", "nonlin_kwargs", ",", "first_stride", ",", "basic_block", "=", "basic_block", ")", ",", "\n", "StackedConvLayers", "(", "output_features", ",", "final_num_features", ",", "1", ",", "self", ".", "conv_op", ",", "self", ".", "conv_kwargs", ",", "\n", "self", ".", "norm_op", ",", "self", ".", "norm_op_kwargs", ",", "self", ".", "dropout_op", ",", "self", ".", "dropout_op_kwargs", ",", "self", ".", "nonlin", ",", "\n", "self", ".", "nonlin_kwargs", ",", "basic_block", "=", "basic_block", ")", ")", ")", "\n", "\n", "# if we don't want to do dropout in the localization pathway then we set the dropout prob to zero here", "\n", "if", "not", "dropout_in_localization", ":", "\n", "            ", "old_dropout_p", "=", "self", ".", "dropout_op_kwargs", "[", "'p'", "]", "\n", "self", ".", "dropout_op_kwargs", "[", "'p'", "]", "=", "0.0", "\n", "\n", "# now lets build the localization pathway", "\n", "", "for", "u", "in", "range", "(", "num_pool", ")", ":", "\n", "            ", "nfeatures_from_down", "=", "final_num_features", "\n", "nfeatures_from_skip", "=", "self", ".", "conv_blocks_context", "[", "\n", "-", "(", "2", "+", "u", ")", "]", ".", "output_channels", "# self.conv_blocks_context[-1] is bottleneck, so start with -2", "\n", "n_features_after_tu_and_concat", "=", "nfeatures_from_skip", "*", "2", "\n", "\n", "# the first conv reduces the number of features to match those of skip", "\n", "# the following convs work on that number of features", "\n", "# if not convolutional upsampling then the final conv reduces the num of features again", "\n", "if", "u", "!=", "num_pool", "-", "1", "and", "not", "self", ".", "convolutional_upsampling", ":", "\n", "                ", "final_num_features", "=", "self", ".", "conv_blocks_context", "[", "-", "(", "3", "+", "u", ")", "]", ".", "output_channels", "\n", "", "else", ":", "\n", "                ", "final_num_features", "=", "nfeatures_from_skip", "\n", "\n", "", "if", "not", "self", ".", "convolutional_upsampling", ":", "\n", "                ", "self", ".", "tu", ".", "append", "(", "Upsample", "(", "scale_factor", "=", "pool_op_kernel_sizes", "[", "-", "(", "u", "+", "1", ")", "]", ",", "mode", "=", "upsample_mode", ")", ")", "\n", "", "else", ":", "\n", "                ", "self", ".", "tu", ".", "append", "(", "transpconv", "(", "nfeatures_from_down", ",", "nfeatures_from_skip", ",", "pool_op_kernel_sizes", "[", "-", "(", "u", "+", "1", ")", "]", ",", "\n", "pool_op_kernel_sizes", "[", "-", "(", "u", "+", "1", ")", "]", ",", "bias", "=", "False", ")", ")", "\n", "\n", "", "self", ".", "conv_kwargs", "[", "'kernel_size'", "]", "=", "self", ".", "conv_kernel_sizes", "[", "-", "(", "u", "+", "1", ")", "]", "\n", "self", ".", "conv_kwargs", "[", "'padding'", "]", "=", "self", ".", "conv_pad_sizes", "[", "-", "(", "u", "+", "1", ")", "]", "\n", "self", ".", "conv_blocks_localization", ".", "append", "(", "nn", ".", "Sequential", "(", "\n", "StackedConvLayers", "(", "n_features_after_tu_and_concat", ",", "nfeatures_from_skip", ",", "num_conv_per_stage", "-", "1", ",", "\n", "self", ".", "conv_op", ",", "self", ".", "conv_kwargs", ",", "self", ".", "norm_op", ",", "self", ".", "norm_op_kwargs", ",", "self", ".", "dropout_op", ",", "\n", "self", ".", "dropout_op_kwargs", ",", "self", ".", "nonlin", ",", "self", ".", "nonlin_kwargs", ",", "basic_block", "=", "basic_block", ")", ",", "\n", "StackedConvLayers", "(", "nfeatures_from_skip", ",", "final_num_features", ",", "1", ",", "self", ".", "conv_op", ",", "self", ".", "conv_kwargs", ",", "\n", "self", ".", "norm_op", ",", "self", ".", "norm_op_kwargs", ",", "self", ".", "dropout_op", ",", "self", ".", "dropout_op_kwargs", ",", "\n", "self", ".", "nonlin", ",", "self", ".", "nonlin_kwargs", ",", "basic_block", "=", "basic_block", ")", "\n", ")", ")", "\n", "\n", "", "for", "ds", "in", "range", "(", "len", "(", "self", ".", "conv_blocks_localization", ")", ")", ":", "\n", "            ", "self", ".", "seg_outputs", ".", "append", "(", "conv_op", "(", "self", ".", "conv_blocks_localization", "[", "ds", "]", "[", "-", "1", "]", ".", "output_channels", ",", "num_classes", ",", "\n", "1", ",", "1", ",", "0", ",", "1", ",", "1", ",", "seg_output_use_bias", ")", ")", "\n", "\n", "", "self", ".", "upscale_logits_ops", "=", "[", "]", "\n", "cum_upsample", "=", "np", ".", "cumprod", "(", "np", ".", "vstack", "(", "pool_op_kernel_sizes", ")", ",", "axis", "=", "0", ")", "[", ":", ":", "-", "1", "]", "\n", "for", "usl", "in", "range", "(", "num_pool", "-", "1", ")", ":", "\n", "            ", "if", "self", ".", "upscale_logits", ":", "\n", "                ", "self", ".", "upscale_logits_ops", ".", "append", "(", "Upsample", "(", "scale_factor", "=", "tuple", "(", "[", "int", "(", "i", ")", "for", "i", "in", "cum_upsample", "[", "usl", "+", "1", "]", "]", ")", ",", "\n", "mode", "=", "upsample_mode", ")", ")", "\n", "", "else", ":", "\n", "                ", "self", ".", "upscale_logits_ops", ".", "append", "(", "lambda", "x", ":", "x", ")", "\n", "\n", "", "", "if", "not", "dropout_in_localization", ":", "\n", "            ", "self", ".", "dropout_op_kwargs", "[", "'p'", "]", "=", "old_dropout_p", "\n", "\n", "# register all modules properly", "\n", "", "self", ".", "conv_blocks_localization", "=", "nn", ".", "ModuleList", "(", "self", ".", "conv_blocks_localization", ")", "\n", "self", ".", "conv_blocks_context", "=", "nn", ".", "ModuleList", "(", "self", ".", "conv_blocks_context", ")", "\n", "self", ".", "td", "=", "nn", ".", "ModuleList", "(", "self", ".", "td", ")", "\n", "self", ".", "tu", "=", "nn", ".", "ModuleList", "(", "self", ".", "tu", ")", "\n", "self", ".", "seg_outputs", "=", "nn", ".", "ModuleList", "(", "self", ".", "seg_outputs", ")", "\n", "if", "self", ".", "upscale_logits", ":", "\n", "            ", "self", ".", "upscale_logits_ops", "=", "nn", ".", "ModuleList", "(", "\n", "self", ".", "upscale_logits_ops", ")", "# lambda x:x is not a Module so we need to distinguish here", "\n", "\n", "", "if", "self", ".", "weightInitializer", "is", "not", "None", ":", "\n", "            ", "self", ".", "apply", "(", "self", ".", "weightInitializer", ")", "\n", "# self.apply(print_module_training_status)", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.network_architecture.generic_UNet.Generic_UNet.forward": [[387, 409], ["range", "range", "skips.append", "len", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "seg_outputs.append", "tuple", "len", "generic_UNet.Generic_UNet.final_nonlin", "i", "zip", "list"], "methods", ["None"], ["", "", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "skips", "=", "[", "]", "\n", "seg_outputs", "=", "[", "]", "\n", "for", "d", "in", "range", "(", "len", "(", "self", ".", "conv_blocks_context", ")", "-", "1", ")", ":", "\n", "            ", "x", "=", "self", ".", "conv_blocks_context", "[", "d", "]", "(", "x", ")", "\n", "skips", ".", "append", "(", "x", ")", "\n", "if", "not", "self", ".", "convolutional_pooling", ":", "\n", "                ", "x", "=", "self", ".", "td", "[", "d", "]", "(", "x", ")", "\n", "\n", "", "", "x", "=", "self", ".", "conv_blocks_context", "[", "-", "1", "]", "(", "x", ")", "\n", "\n", "for", "u", "in", "range", "(", "len", "(", "self", ".", "tu", ")", ")", ":", "\n", "            ", "x", "=", "self", ".", "tu", "[", "u", "]", "(", "x", ")", "\n", "x", "=", "torch", ".", "cat", "(", "(", "x", ",", "skips", "[", "-", "(", "u", "+", "1", ")", "]", ")", ",", "dim", "=", "1", ")", "\n", "x", "=", "self", ".", "conv_blocks_localization", "[", "u", "]", "(", "x", ")", "\n", "seg_outputs", ".", "append", "(", "self", ".", "final_nonlin", "(", "self", ".", "seg_outputs", "[", "u", "]", "(", "x", ")", ")", ")", "\n", "\n", "", "if", "self", ".", "_deep_supervision", "and", "self", ".", "do_ds", ":", "\n", "            ", "return", "tuple", "(", "[", "seg_outputs", "[", "-", "1", "]", "]", "+", "[", "i", "(", "j", ")", "for", "i", ",", "j", "in", "\n", "zip", "(", "list", "(", "self", ".", "upscale_logits_ops", ")", "[", ":", ":", "-", "1", "]", ",", "seg_outputs", "[", ":", "-", "1", "]", "[", ":", ":", "-", "1", "]", ")", "]", ")", "\n", "", "else", ":", "\n", "            ", "return", "seg_outputs", "[", "-", "1", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.network_architecture.generic_UNet.Generic_UNet.compute_approx_vram_consumption": [[410, 450], ["len", "numpy.array", "numpy.int64", "range", "isinstance", "numpy.array", "range", "min", "len", "numpy.prod", "numpy.prod", "numpy.prod", "numpy.prod", "numpy.prod"], "methods", ["None"], ["", "", "@", "staticmethod", "\n", "def", "compute_approx_vram_consumption", "(", "patch_size", ",", "num_pool_per_axis", ",", "base_num_features", ",", "max_num_features", ",", "\n", "num_modalities", ",", "num_classes", ",", "pool_op_kernel_sizes", ",", "deep_supervision", "=", "False", ",", "\n", "conv_per_stage", "=", "2", ")", ":", "\n", "        ", "\"\"\"\n        This only applies for num_conv_per_stage and convolutional_upsampling=True\n        not real vram consumption. just a constant term to which the vram consumption will be approx proportional\n        (+ offset for parameter storage)\n        :param deep_supervision:\n        :param patch_size:\n        :param num_pool_per_axis:\n        :param base_num_features:\n        :param max_num_features:\n        :param num_modalities:\n        :param num_classes:\n        :param pool_op_kernel_sizes:\n        :return:\n        \"\"\"", "\n", "if", "not", "isinstance", "(", "num_pool_per_axis", ",", "np", ".", "ndarray", ")", ":", "\n", "            ", "num_pool_per_axis", "=", "np", ".", "array", "(", "num_pool_per_axis", ")", "\n", "\n", "", "npool", "=", "len", "(", "pool_op_kernel_sizes", ")", "\n", "\n", "map_size", "=", "np", ".", "array", "(", "patch_size", ")", "\n", "tmp", "=", "np", ".", "int64", "(", "(", "conv_per_stage", "*", "2", "+", "1", ")", "*", "np", ".", "prod", "(", "map_size", ",", "dtype", "=", "np", ".", "int64", ")", "*", "base_num_features", "+", "\n", "num_modalities", "*", "np", ".", "prod", "(", "map_size", ",", "dtype", "=", "np", ".", "int64", ")", "+", "\n", "num_classes", "*", "np", ".", "prod", "(", "map_size", ",", "dtype", "=", "np", ".", "int64", ")", ")", "\n", "\n", "num_feat", "=", "base_num_features", "\n", "\n", "for", "p", "in", "range", "(", "npool", ")", ":", "\n", "            ", "for", "pi", "in", "range", "(", "len", "(", "num_pool_per_axis", ")", ")", ":", "\n", "                ", "map_size", "[", "pi", "]", "/=", "pool_op_kernel_sizes", "[", "p", "]", "[", "pi", "]", "\n", "", "num_feat", "=", "min", "(", "num_feat", "*", "2", ",", "max_num_features", ")", "\n", "num_blocks", "=", "(", "conv_per_stage", "*", "2", "+", "1", ")", "if", "p", "<", "(", "npool", "-", "1", ")", "else", "conv_per_stage", "# conv_per_stage + conv_per_stage for the convs of encode/decode and 1 for transposed conv", "\n", "tmp", "+=", "num_blocks", "*", "np", ".", "prod", "(", "map_size", ",", "dtype", "=", "np", ".", "int64", ")", "*", "num_feat", "\n", "if", "deep_supervision", "and", "p", "<", "(", "npool", "-", "2", ")", ":", "\n", "                ", "tmp", "+=", "np", ".", "prod", "(", "map_size", ",", "dtype", "=", "np", ".", "int64", ")", "*", "num_classes", "\n", "# print(p, map_size, num_feat, tmp)", "\n", "", "", "return", "tmp", "\n", "", "", ""]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.network_architecture.generic_UNet.print_module_training_status": [[145, 152], ["isinstance", "isinstance", "isinstance", "isinstance", "isinstance", "isinstance", "isinstance", "isinstance", "isinstance", "isinstance", "isinstance", "print", "str"], "function", ["None"], ["", "", "def", "print_module_training_status", "(", "module", ")", ":", "\n", "    ", "if", "isinstance", "(", "module", ",", "nn", ".", "Conv2d", ")", "or", "isinstance", "(", "module", ",", "nn", ".", "Conv3d", ")", "or", "isinstance", "(", "module", ",", "nn", ".", "Dropout3d", ")", "or", "isinstance", "(", "module", ",", "nn", ".", "Dropout2d", ")", "or", "isinstance", "(", "module", ",", "nn", ".", "Dropout", ")", "or", "isinstance", "(", "module", ",", "nn", ".", "InstanceNorm3d", ")", "or", "isinstance", "(", "module", ",", "nn", ".", "InstanceNorm2d", ")", "or", "isinstance", "(", "module", ",", "nn", ".", "InstanceNorm1d", ")", "or", "isinstance", "(", "module", ",", "nn", ".", "BatchNorm2d", ")", "or", "isinstance", "(", "module", ",", "nn", ".", "BatchNorm3d", ")", "or", "isinstance", "(", "module", ",", "\n", "nn", ".", "BatchNorm1d", ")", ":", "\n", "        ", "print", "(", "str", "(", "module", ")", ",", "module", ".", "training", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.network_architecture.generic_XNet.ConvDropoutNormNonlin.__init__": [[31, 63], ["torch.nn.Module.__init__", "generic_XNet.ConvDropoutNormNonlin.conv_op", "generic_XNet.ConvDropoutNormNonlin.norm_op", "generic_XNet.ConvDropoutNormNonlin.nonlin", "generic_XNet.ConvDropoutNormNonlin.dropout_op"], "methods", ["home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.common.layers.ResizeImage.__init__"], ["def", "__init__", "(", "self", ",", "input_channels", ",", "output_channels", ",", "\n", "conv_op", "=", "nn", ".", "Conv2d", ",", "conv_kwargs", "=", "None", ",", "\n", "norm_op", "=", "nn", ".", "BatchNorm2d", ",", "norm_op_kwargs", "=", "None", ",", "\n", "dropout_op", "=", "nn", ".", "Dropout2d", ",", "dropout_op_kwargs", "=", "None", ",", "\n", "nonlin", "=", "nn", ".", "LeakyReLU", ",", "nonlin_kwargs", "=", "None", ")", ":", "\n", "        ", "super", "(", "ConvDropoutNormNonlin", ",", "self", ")", ".", "__init__", "(", ")", "\n", "if", "nonlin_kwargs", "is", "None", ":", "\n", "            ", "nonlin_kwargs", "=", "{", "'negative_slope'", ":", "1e-2", ",", "'inplace'", ":", "True", "}", "\n", "", "if", "dropout_op_kwargs", "is", "None", ":", "\n", "            ", "dropout_op_kwargs", "=", "{", "'p'", ":", "0.5", ",", "'inplace'", ":", "True", "}", "\n", "", "if", "norm_op_kwargs", "is", "None", ":", "\n", "            ", "norm_op_kwargs", "=", "{", "'eps'", ":", "1e-5", ",", "'affine'", ":", "True", ",", "'momentum'", ":", "0.1", "}", "\n", "", "if", "conv_kwargs", "is", "None", ":", "\n", "            ", "conv_kwargs", "=", "{", "'kernel_size'", ":", "3", ",", "'stride'", ":", "1", ",", "'padding'", ":", "1", ",", "'dilation'", ":", "1", ",", "'bias'", ":", "True", "}", "\n", "\n", "", "self", ".", "nonlin_kwargs", "=", "nonlin_kwargs", "\n", "self", ".", "nonlin", "=", "nonlin", "\n", "self", ".", "dropout_op", "=", "dropout_op", "\n", "self", ".", "dropout_op_kwargs", "=", "dropout_op_kwargs", "\n", "self", ".", "norm_op_kwargs", "=", "norm_op_kwargs", "\n", "self", ".", "conv_kwargs", "=", "conv_kwargs", "\n", "self", ".", "conv_op", "=", "conv_op", "\n", "self", ".", "norm_op", "=", "norm_op", "\n", "\n", "self", ".", "conv", "=", "self", ".", "conv_op", "(", "input_channels", ",", "output_channels", ",", "**", "self", ".", "conv_kwargs", ")", "\n", "if", "self", ".", "dropout_op", "is", "not", "None", "and", "self", ".", "dropout_op_kwargs", "[", "'p'", "]", "is", "not", "None", "and", "self", ".", "dropout_op_kwargs", "[", "\n", "'p'", "]", ">", "0", ":", "\n", "            ", "self", ".", "dropout", "=", "self", ".", "dropout_op", "(", "**", "self", ".", "dropout_op_kwargs", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "dropout", "=", "None", "\n", "", "self", ".", "instnorm", "=", "self", ".", "norm_op", "(", "output_channels", ",", "**", "self", ".", "norm_op_kwargs", ")", "\n", "self", ".", "lrelu", "=", "self", ".", "nonlin", "(", "**", "self", ".", "nonlin_kwargs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.network_architecture.generic_XNet.ConvDropoutNormNonlin.forward": [[64, 69], ["generic_XNet.ConvDropoutNormNonlin.conv", "generic_XNet.ConvDropoutNormNonlin.lrelu", "generic_XNet.ConvDropoutNormNonlin.dropout", "generic_XNet.ConvDropoutNormNonlin.instnorm"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "x", "=", "self", ".", "conv", "(", "x", ")", "\n", "if", "self", ".", "dropout", "is", "not", "None", ":", "\n", "            ", "x", "=", "self", ".", "dropout", "(", "x", ")", "\n", "", "return", "self", ".", "lrelu", "(", "self", ".", "instnorm", "(", "x", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.network_architecture.generic_XNet.ConvDropoutNonlinNorm.forward": [[72, 77], ["generic_XNet.ConvDropoutNonlinNorm.conv", "generic_XNet.ConvDropoutNonlinNorm.instnorm", "generic_XNet.ConvDropoutNonlinNorm.dropout", "generic_XNet.ConvDropoutNonlinNorm.lrelu"], "methods", ["None"], ["    ", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "x", "=", "self", ".", "conv", "(", "x", ")", "\n", "if", "self", ".", "dropout", "is", "not", "None", ":", "\n", "            ", "x", "=", "self", ".", "dropout", "(", "x", ")", "\n", "", "return", "self", ".", "instnorm", "(", "self", ".", "lrelu", "(", "x", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.network_architecture.generic_XNet.StackedConvLayers.__init__": [[80, 140], ["torch.nn.Module.__init__", "torch.nn.Sequential", "torch.nn.Sequential", "copy.deepcopy", "basic_block", "basic_block", "range"], "methods", ["home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.common.layers.ResizeImage.__init__"], ["    ", "def", "__init__", "(", "self", ",", "input_feature_channels", ",", "output_feature_channels", ",", "num_convs", ",", "\n", "conv_op", "=", "nn", ".", "Conv2d", ",", "conv_kwargs", "=", "None", ",", "\n", "norm_op", "=", "nn", ".", "BatchNorm2d", ",", "norm_op_kwargs", "=", "None", ",", "\n", "dropout_op", "=", "nn", ".", "Dropout2d", ",", "dropout_op_kwargs", "=", "None", ",", "\n", "nonlin", "=", "nn", ".", "LeakyReLU", ",", "nonlin_kwargs", "=", "None", ",", "first_stride", "=", "None", ",", "basic_block", "=", "ConvDropoutNormNonlin", ")", ":", "\n", "        ", "'''\n        stacks ConvDropoutNormLReLU layers. initial_stride will only be applied to first layer in the stack. The other parameters affect all layers\n        :param input_feature_channels:\n        :param output_feature_channels:\n        :param num_convs:\n        :param dilation:\n        :param kernel_size:\n        :param padding:\n        :param dropout:\n        :param initial_stride:\n        :param conv_op:\n        :param norm_op:\n        :param dropout_op:\n        :param inplace:\n        :param neg_slope:\n        :param norm_affine:\n        :param conv_bias:\n        '''", "\n", "self", ".", "input_channels", "=", "input_feature_channels", "\n", "self", ".", "output_channels", "=", "output_feature_channels", "\n", "\n", "if", "nonlin_kwargs", "is", "None", ":", "\n", "            ", "nonlin_kwargs", "=", "{", "'negative_slope'", ":", "1e-2", ",", "'inplace'", ":", "True", "}", "\n", "", "if", "dropout_op_kwargs", "is", "None", ":", "\n", "            ", "dropout_op_kwargs", "=", "{", "'p'", ":", "0.5", ",", "'inplace'", ":", "True", "}", "\n", "", "if", "norm_op_kwargs", "is", "None", ":", "\n", "            ", "norm_op_kwargs", "=", "{", "'eps'", ":", "1e-5", ",", "'affine'", ":", "True", ",", "'momentum'", ":", "0.1", "}", "\n", "", "if", "conv_kwargs", "is", "None", ":", "\n", "            ", "conv_kwargs", "=", "{", "'kernel_size'", ":", "3", ",", "'stride'", ":", "1", ",", "'padding'", ":", "1", ",", "'dilation'", ":", "1", ",", "'bias'", ":", "True", "}", "\n", "\n", "", "self", ".", "nonlin_kwargs", "=", "nonlin_kwargs", "\n", "self", ".", "nonlin", "=", "nonlin", "\n", "self", ".", "dropout_op", "=", "dropout_op", "\n", "self", ".", "dropout_op_kwargs", "=", "dropout_op_kwargs", "\n", "self", ".", "norm_op_kwargs", "=", "norm_op_kwargs", "\n", "self", ".", "conv_kwargs", "=", "conv_kwargs", "\n", "self", ".", "conv_op", "=", "conv_op", "\n", "self", ".", "norm_op", "=", "norm_op", "\n", "\n", "if", "first_stride", "is", "not", "None", ":", "\n", "            ", "self", ".", "conv_kwargs_first_conv", "=", "deepcopy", "(", "conv_kwargs", ")", "\n", "self", ".", "conv_kwargs_first_conv", "[", "'stride'", "]", "=", "first_stride", "\n", "", "else", ":", "\n", "            ", "self", ".", "conv_kwargs_first_conv", "=", "conv_kwargs", "\n", "\n", "", "super", "(", "StackedConvLayers", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "blocks", "=", "nn", ".", "Sequential", "(", "\n", "*", "(", "[", "basic_block", "(", "input_feature_channels", ",", "output_feature_channels", ",", "self", ".", "conv_op", ",", "\n", "self", ".", "conv_kwargs_first_conv", ",", "\n", "self", ".", "norm_op", ",", "self", ".", "norm_op_kwargs", ",", "self", ".", "dropout_op", ",", "self", ".", "dropout_op_kwargs", ",", "\n", "self", ".", "nonlin", ",", "self", ".", "nonlin_kwargs", ")", "]", "+", "\n", "[", "basic_block", "(", "output_feature_channels", ",", "output_feature_channels", ",", "self", ".", "conv_op", ",", "\n", "self", ".", "conv_kwargs", ",", "\n", "self", ".", "norm_op", ",", "self", ".", "norm_op_kwargs", ",", "self", ".", "dropout_op", ",", "self", ".", "dropout_op_kwargs", ",", "\n", "self", ".", "nonlin", ",", "self", ".", "nonlin_kwargs", ")", "for", "_", "in", "range", "(", "num_convs", "-", "1", ")", "]", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.network_architecture.generic_XNet.StackedConvLayers.forward": [[141, 143], ["generic_XNet.StackedConvLayers.blocks"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "return", "self", ".", "blocks", "(", "x", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.network_architecture.generic_XNet.Upsample.__init__": [[155, 161], ["torch.nn.Module.__init__"], "methods", ["home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.common.layers.ResizeImage.__init__"], ["    ", "def", "__init__", "(", "self", ",", "size", "=", "None", ",", "scale_factor", "=", "None", ",", "mode", "=", "'nearest'", ",", "align_corners", "=", "False", ")", ":", "\n", "        ", "super", "(", "Upsample", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "align_corners", "=", "align_corners", "\n", "self", ".", "mode", "=", "mode", "\n", "self", ".", "scale_factor", "=", "scale_factor", "\n", "self", ".", "size", "=", "size", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.network_architecture.generic_XNet.Upsample.forward": [[162, 165], ["torch.nn.functional.interpolate", "torch.nn.functional.interpolate"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "return", "nn", ".", "functional", ".", "interpolate", "(", "x", ",", "size", "=", "self", ".", "size", ",", "scale_factor", "=", "self", ".", "scale_factor", ",", "mode", "=", "self", ".", "mode", ",", "\n", "align_corners", "=", "self", ".", "align_corners", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.network_architecture.generic_XNet.Generic_XNet.__init__": [[184, 392], ["nnunet.network_architecture.initialization.InitWeights_He", "nnunet.network_architecture.neural_network.SegmentationNetwork.__init__", "numpy.prod", "range", "generic_XNet.Generic_XNet.conv_blocks_context.append", "generic_XNet.Generic_XNet.create_nest", "generic_XNet.Generic_XNet.create_nest", "generic_XNet.Generic_XNet.create_nest", "generic_XNet.Generic_XNet.create_nest", "generic_XNet.Generic_XNet.create_nest", "generic_XNet.Generic_XNet.seg_outputs.append", "generic_XNet.Generic_XNet.seg_outputs.append", "generic_XNet.Generic_XNet.seg_outputs.append", "generic_XNet.Generic_XNet.seg_outputs.append", "generic_XNet.Generic_XNet.seg_outputs.append", "range", "torch.nn.ModuleList", "torch.nn.ModuleList", "torch.nn.ModuleList", "torch.nn.ModuleList", "torch.nn.ModuleList", "torch.nn.ModuleList", "torch.nn.ModuleList", "torch.nn.ModuleList", "torch.nn.ModuleList", "torch.nn.ModuleList", "torch.nn.ModuleList", "torch.nn.ModuleList", "torch.nn.ModuleList", "torch.nn.ModuleList", "torch.nn.ModuleList", "torch.nn.ModuleList", "torch.nn.ModuleList", "torch.nn.ModuleList", "torch.nn.ModuleList", "torch.nn.ModuleList", "torch.nn.ModuleList", "torch.nn.ModuleList", "torch.nn.ModuleList", "torch.nn.ModuleList", "torch.nn.ModuleList", "torch.nn.ModuleList", "generic_XNet.Generic_XNet.conv_pad_sizes.append", "generic_XNet.Generic_XNet.conv_blocks_context.append", "int", "min", "torch.nn.Sequential", "torch.nn.Sequential", "conv_op", "conv_op", "conv_op", "conv_op", "conv_op", "numpy.cumprod", "torch.nn.ModuleList", "torch.nn.ModuleList", "generic_XNet.Generic_XNet.apply", "ValueError", "generic_XNet.StackedConvLayers", "generic_XNet.Generic_XNet.td.append", "numpy.round", "generic_XNet.StackedConvLayers", "generic_XNet.StackedConvLayers", "numpy.vstack", "generic_XNet.Generic_XNet.upscale_logits_ops.append", "generic_XNet.Generic_XNet.upscale_logits_ops.append", "pool_op", "generic_XNet.Upsample", "str", "tuple", "int"], "methods", ["home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.common.layers.ResizeImage.__init__", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.network_architecture.generic_hipp_XNet.Generic_XNet.create_nest", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.network_architecture.generic_hipp_XNet.Generic_XNet.create_nest", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.network_architecture.generic_hipp_XNet.Generic_XNet.create_nest", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.network_architecture.generic_hipp_XNet.Generic_XNet.create_nest", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.network_architecture.generic_hipp_XNet.Generic_XNet.create_nest"], ["def", "__init__", "(", "self", ",", "input_channels", ",", "base_num_features", ",", "num_classes", ",", "num_pool", ",", "num_conv_per_stage", "=", "2", ",", "\n", "feat_map_mul_on_downscale", "=", "2", ",", "conv_op", "=", "nn", ".", "Conv2d", ",", "\n", "norm_op", "=", "nn", ".", "BatchNorm2d", ",", "norm_op_kwargs", "=", "None", ",", "\n", "dropout_op", "=", "nn", ".", "Dropout2d", ",", "dropout_op_kwargs", "=", "None", ",", "\n", "nonlin", "=", "nn", ".", "LeakyReLU", ",", "nonlin_kwargs", "=", "None", ",", "deep_supervision", "=", "True", ",", "dropout_in_localization", "=", "False", ",", "\n", "final_nonlin", "=", "softmax_helper", ",", "weightInitializer", "=", "InitWeights_He", "(", "1e-2", ")", ",", "pool_op_kernel_sizes", "=", "None", ",", "\n", "conv_kernel_sizes", "=", "None", ",", "\n", "upscale_logits", "=", "False", ",", "convolutional_pooling", "=", "False", ",", "convolutional_upsampling", "=", "False", ",", "\n", "max_num_features", "=", "None", ",", "basic_block", "=", "ConvDropoutNormNonlin", ",", "\n", "seg_output_use_bias", "=", "False", ")", ":", "\n", "        ", "\"\"\"\n        basically more flexible than v1, architecture is the same\n\n        Does this look complicated? Nah bro. Functionality > usability\n\n        This does everything you need, including world peace.\n\n        Questions? -> f.isensee@dkfz.de\n        \"\"\"", "\n", "super", "(", "Generic_XNet", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "convolutional_upsampling", "=", "convolutional_upsampling", "\n", "self", ".", "convolutional_pooling", "=", "convolutional_pooling", "\n", "self", ".", "upscale_logits", "=", "upscale_logits", "\n", "if", "nonlin_kwargs", "is", "None", ":", "\n", "            ", "nonlin_kwargs", "=", "{", "'negative_slope'", ":", "1e-2", ",", "'inplace'", ":", "True", "}", "\n", "", "if", "dropout_op_kwargs", "is", "None", ":", "\n", "            ", "dropout_op_kwargs", "=", "{", "'p'", ":", "0.5", ",", "'inplace'", ":", "True", "}", "\n", "", "if", "norm_op_kwargs", "is", "None", ":", "\n", "            ", "norm_op_kwargs", "=", "{", "'eps'", ":", "1e-5", ",", "'affine'", ":", "True", ",", "'momentum'", ":", "0.1", "}", "\n", "\n", "", "self", ".", "conv_kwargs", "=", "{", "'stride'", ":", "1", ",", "'dilation'", ":", "1", ",", "'bias'", ":", "True", "}", "\n", "\n", "self", ".", "nonlin", "=", "nonlin", "\n", "self", ".", "nonlin_kwargs", "=", "nonlin_kwargs", "\n", "self", ".", "dropout_op_kwargs", "=", "dropout_op_kwargs", "\n", "self", ".", "norm_op_kwargs", "=", "norm_op_kwargs", "\n", "self", ".", "weightInitializer", "=", "weightInitializer", "\n", "self", ".", "conv_op", "=", "conv_op", "\n", "self", ".", "norm_op", "=", "norm_op", "\n", "self", ".", "dropout_op", "=", "dropout_op", "\n", "self", ".", "num_classes", "=", "num_classes", "\n", "self", ".", "final_nonlin", "=", "final_nonlin", "\n", "self", ".", "_deep_supervision", "=", "deep_supervision", "\n", "self", ".", "do_ds", "=", "deep_supervision", "\n", "\n", "if", "conv_op", "==", "nn", ".", "Conv2d", ":", "\n", "            ", "upsample_mode", "=", "'bilinear'", "\n", "pool_op", "=", "nn", ".", "MaxPool2d", "\n", "transpconv", "=", "nn", ".", "ConvTranspose2d", "\n", "if", "pool_op_kernel_sizes", "is", "None", ":", "\n", "                ", "pool_op_kernel_sizes", "=", "[", "(", "2", ",", "2", ")", "]", "*", "num_pool", "\n", "", "if", "conv_kernel_sizes", "is", "None", ":", "\n", "                ", "conv_kernel_sizes", "=", "[", "(", "3", ",", "3", ")", "]", "*", "(", "num_pool", "+", "1", ")", "\n", "", "", "elif", "conv_op", "==", "nn", ".", "Conv3d", ":", "\n", "            ", "upsample_mode", "=", "'trilinear'", "\n", "pool_op", "=", "nn", ".", "MaxPool3d", "\n", "transpconv", "=", "nn", ".", "ConvTranspose3d", "\n", "if", "pool_op_kernel_sizes", "is", "None", ":", "\n", "                ", "pool_op_kernel_sizes", "=", "[", "(", "2", ",", "2", ",", "2", ")", "]", "*", "num_pool", "\n", "", "if", "conv_kernel_sizes", "is", "None", ":", "\n", "                ", "conv_kernel_sizes", "=", "[", "(", "3", ",", "3", ",", "3", ")", "]", "*", "(", "num_pool", "+", "1", ")", "\n", "", "", "else", ":", "\n", "            ", "raise", "ValueError", "(", "\"unknown convolution dimensionality, conv op: %s\"", "%", "str", "(", "conv_op", ")", ")", "\n", "\n", "", "self", ".", "input_shape_must_be_divisible_by", "=", "np", ".", "prod", "(", "pool_op_kernel_sizes", ",", "0", ",", "dtype", "=", "np", ".", "int64", ")", "\n", "self", ".", "pool_op_kernel_sizes", "=", "pool_op_kernel_sizes", "\n", "self", ".", "conv_kernel_sizes", "=", "conv_kernel_sizes", "\n", "\n", "self", ".", "conv_pad_sizes", "=", "[", "]", "\n", "for", "krnl", "in", "self", ".", "conv_kernel_sizes", ":", "\n", "            ", "self", ".", "conv_pad_sizes", ".", "append", "(", "[", "1", "if", "i", "==", "3", "else", "0", "for", "i", "in", "krnl", "]", ")", "\n", "\n", "", "if", "max_num_features", "is", "None", ":", "\n", "            ", "if", "self", ".", "conv_op", "==", "nn", ".", "Conv3d", ":", "\n", "                ", "self", ".", "max_num_features", "=", "self", ".", "MAX_NUM_FILTERS_3D", "\n", "", "else", ":", "\n", "                ", "self", ".", "max_num_features", "=", "self", ".", "MAX_FILTERS_2D", "\n", "", "", "else", ":", "\n", "            ", "self", ".", "max_num_features", "=", "max_num_features", "\n", "\n", "", "self", ".", "conv_blocks_context", "=", "[", "]", "\n", "# self.conv_blocks_localization = []", "\n", "self", ".", "loc0", "=", "[", "]", "\n", "self", ".", "loc1", "=", "[", "]", "\n", "self", ".", "loc2", "=", "[", "]", "\n", "self", ".", "loc3", "=", "[", "]", "\n", "self", ".", "loc4", "=", "[", "]", "\n", "self", ".", "td", "=", "[", "]", "\n", "self", ".", "up0", "=", "[", "]", "\n", "self", ".", "up1", "=", "[", "]", "\n", "self", ".", "up2", "=", "[", "]", "\n", "self", ".", "up3", "=", "[", "]", "\n", "self", ".", "up4", "=", "[", "]", "\n", "# self.tu = []", "\n", "self", ".", "seg_outputs", "=", "[", "]", "\n", "\n", "output_features", "=", "base_num_features", "\n", "input_features", "=", "input_channels", "\n", "\n", "for", "d", "in", "range", "(", "num_pool", ")", ":", "\n", "# determine the first stride", "\n", "            ", "if", "d", "!=", "0", "and", "self", ".", "convolutional_pooling", ":", "\n", "                ", "first_stride", "=", "pool_op_kernel_sizes", "[", "d", "-", "1", "]", "\n", "", "else", ":", "\n", "                ", "first_stride", "=", "None", "\n", "\n", "", "self", ".", "conv_kwargs", "[", "'kernel_size'", "]", "=", "self", ".", "conv_kernel_sizes", "[", "d", "]", "\n", "self", ".", "conv_kwargs", "[", "'padding'", "]", "=", "self", ".", "conv_pad_sizes", "[", "d", "]", "\n", "# add convolutions", "\n", "self", ".", "conv_blocks_context", ".", "append", "(", "StackedConvLayers", "(", "input_features", ",", "output_features", ",", "num_conv_per_stage", ",", "\n", "self", ".", "conv_op", ",", "self", ".", "conv_kwargs", ",", "self", ".", "norm_op", ",", "\n", "self", ".", "norm_op_kwargs", ",", "self", ".", "dropout_op", ",", "\n", "self", ".", "dropout_op_kwargs", ",", "self", ".", "nonlin", ",", "self", ".", "nonlin_kwargs", ",", "\n", "first_stride", ",", "basic_block", "=", "basic_block", ")", ")", "\n", "if", "not", "self", ".", "convolutional_pooling", ":", "\n", "                ", "self", ".", "td", ".", "append", "(", "pool_op", "(", "pool_op_kernel_sizes", "[", "d", "]", ")", ")", "\n", "", "input_features", "=", "output_features", "\n", "output_features", "=", "int", "(", "np", ".", "round", "(", "output_features", "*", "feat_map_mul_on_downscale", ")", ")", "\n", "\n", "output_features", "=", "min", "(", "output_features", ",", "self", ".", "max_num_features", ")", "\n", "\n", "# now the bottleneck.", "\n", "# determine the first stride", "\n", "", "if", "self", ".", "convolutional_pooling", ":", "\n", "            ", "first_stride", "=", "pool_op_kernel_sizes", "[", "-", "1", "]", "\n", "", "else", ":", "\n", "            ", "first_stride", "=", "None", "\n", "\n", "# the output of the last conv must match the number of features from the skip connection if we are not using", "\n", "# convolutional upsampling. If we use convolutional upsampling then the reduction in feature maps will be", "\n", "# done by the transposed conv", "\n", "", "if", "self", ".", "convolutional_upsampling", ":", "\n", "            ", "final_num_features", "=", "output_features", "\n", "", "else", ":", "\n", "            ", "final_num_features", "=", "self", ".", "conv_blocks_context", "[", "-", "1", "]", ".", "output_channels", "\n", "\n", "", "self", ".", "conv_kwargs", "[", "'kernel_size'", "]", "=", "self", ".", "conv_kernel_sizes", "[", "num_pool", "]", "\n", "self", ".", "conv_kwargs", "[", "'padding'", "]", "=", "self", ".", "conv_pad_sizes", "[", "num_pool", "]", "\n", "self", ".", "conv_blocks_context", ".", "append", "(", "nn", ".", "Sequential", "(", "\n", "StackedConvLayers", "(", "input_features", ",", "output_features", ",", "num_conv_per_stage", "-", "1", ",", "self", ".", "conv_op", ",", "self", ".", "conv_kwargs", ",", "\n", "self", ".", "norm_op", ",", "self", ".", "norm_op_kwargs", ",", "self", ".", "dropout_op", ",", "self", ".", "dropout_op_kwargs", ",", "self", ".", "nonlin", ",", "\n", "self", ".", "nonlin_kwargs", ",", "first_stride", ",", "basic_block", "=", "basic_block", ")", ",", "\n", "StackedConvLayers", "(", "output_features", ",", "final_num_features", ",", "1", ",", "self", ".", "conv_op", ",", "self", ".", "conv_kwargs", ",", "\n", "self", ".", "norm_op", ",", "self", ".", "norm_op_kwargs", ",", "self", ".", "dropout_op", ",", "self", ".", "dropout_op_kwargs", ",", "self", ".", "nonlin", ",", "\n", "self", ".", "nonlin_kwargs", ",", "basic_block", "=", "basic_block", ")", ")", ")", "\n", "\n", "# if we don't want to do dropout in the localization pathway then we set the dropout prob to zero here", "\n", "if", "not", "dropout_in_localization", ":", "\n", "            ", "old_dropout_p", "=", "self", ".", "dropout_op_kwargs", "[", "'p'", "]", "\n", "self", ".", "dropout_op_kwargs", "[", "'p'", "]", "=", "0.0", "\n", "\n", "# now lets build the localization pathway", "\n", "", "encoder_features", "=", "final_num_features", "\n", "self", ".", "loc0", ",", "self", ".", "up0", ",", "encoder_features", "=", "self", ".", "create_nest", "(", "0", ",", "num_pool", ",", "final_num_features", ",", "num_conv_per_stage", ",", "\n", "basic_block", ",", "transpconv", ")", "\n", "self", ".", "loc1", ",", "self", ".", "up1", ",", "encoder_features1", "=", "self", ".", "create_nest", "(", "1", ",", "num_pool", ",", "encoder_features", ",", "num_conv_per_stage", ",", "\n", "basic_block", ",", "transpconv", ")", "\n", "self", ".", "loc2", ",", "self", ".", "up2", ",", "encoder_features2", "=", "self", ".", "create_nest", "(", "2", ",", "num_pool", ",", "encoder_features1", ",", "num_conv_per_stage", ",", "\n", "basic_block", ",", "transpconv", ")", "\n", "self", ".", "loc3", ",", "self", ".", "up3", ",", "encoder_features3", "=", "self", ".", "create_nest", "(", "3", ",", "num_pool", ",", "encoder_features2", ",", "num_conv_per_stage", ",", "\n", "basic_block", ",", "transpconv", ")", "\n", "self", ".", "loc4", ",", "self", ".", "up4", ",", "encoder_features4", "=", "self", ".", "create_nest", "(", "4", ",", "num_pool", ",", "encoder_features3", ",", "num_conv_per_stage", ",", "\n", "basic_block", ",", "transpconv", ")", "\n", "\n", "self", ".", "seg_outputs", ".", "append", "(", "conv_op", "(", "self", ".", "loc0", "[", "-", "1", "]", "[", "-", "1", "]", ".", "output_channels", ",", "num_classes", ",", "\n", "1", ",", "1", ",", "0", ",", "1", ",", "1", ",", "seg_output_use_bias", ")", ")", "\n", "self", ".", "seg_outputs", ".", "append", "(", "conv_op", "(", "self", ".", "loc1", "[", "-", "1", "]", "[", "-", "1", "]", ".", "output_channels", ",", "num_classes", ",", "\n", "1", ",", "1", ",", "0", ",", "1", ",", "1", ",", "seg_output_use_bias", ")", ")", "\n", "self", ".", "seg_outputs", ".", "append", "(", "conv_op", "(", "self", ".", "loc2", "[", "-", "1", "]", "[", "-", "1", "]", ".", "output_channels", ",", "num_classes", ",", "\n", "1", ",", "1", ",", "0", ",", "1", ",", "1", ",", "seg_output_use_bias", ")", ")", "\n", "self", ".", "seg_outputs", ".", "append", "(", "conv_op", "(", "self", ".", "loc3", "[", "-", "1", "]", "[", "-", "1", "]", ".", "output_channels", ",", "num_classes", ",", "\n", "1", ",", "1", ",", "0", ",", "1", ",", "1", ",", "seg_output_use_bias", ")", ")", "\n", "self", ".", "seg_outputs", ".", "append", "(", "conv_op", "(", "self", ".", "loc4", "[", "-", "1", "]", "[", "-", "1", "]", ".", "output_channels", ",", "num_classes", ",", "\n", "1", ",", "1", ",", "0", ",", "1", ",", "1", ",", "seg_output_use_bias", ")", ")", "\n", "\n", "self", ".", "upscale_logits_ops", "=", "[", "]", "\n", "cum_upsample", "=", "np", ".", "cumprod", "(", "np", ".", "vstack", "(", "pool_op_kernel_sizes", ")", ",", "axis", "=", "0", ")", "[", ":", ":", "-", "1", "]", "\n", "for", "usl", "in", "range", "(", "num_pool", ")", ":", "\n", "            ", "if", "self", ".", "upscale_logits", ":", "\n", "                ", "self", ".", "upscale_logits_ops", ".", "append", "(", "Upsample", "(", "scale_factor", "=", "tuple", "(", "[", "int", "(", "i", ")", "for", "i", "in", "cum_upsample", "[", "usl", "+", "1", "]", "]", ")", ",", "\n", "mode", "=", "upsample_mode", ")", ")", "\n", "", "else", ":", "\n", "                ", "self", ".", "upscale_logits_ops", ".", "append", "(", "lambda", "x", ":", "x", ")", "\n", "\n", "", "", "if", "not", "dropout_in_localization", ":", "\n", "            ", "self", ".", "dropout_op_kwargs", "[", "'p'", "]", "=", "old_dropout_p", "\n", "\n", "# register all modules properly", "\n", "# self.conv_blocks_localization = nn.ModuleList(self.conv_blocks_localization)", "\n", "", "self", ".", "loc0", "=", "nn", ".", "ModuleList", "(", "self", ".", "loc0", ")", "\n", "self", ".", "loc1", "=", "nn", ".", "ModuleList", "(", "self", ".", "loc1", ")", "\n", "self", ".", "loc2", "=", "nn", ".", "ModuleList", "(", "self", ".", "loc2", ")", "\n", "self", ".", "loc3", "=", "nn", ".", "ModuleList", "(", "self", ".", "loc3", ")", "\n", "self", ".", "loc4", "=", "nn", ".", "ModuleList", "(", "self", ".", "loc4", ")", "\n", "self", ".", "conv_blocks_context", "=", "nn", ".", "ModuleList", "(", "self", ".", "conv_blocks_context", ")", "\n", "self", ".", "td", "=", "nn", ".", "ModuleList", "(", "self", ".", "td", ")", "\n", "self", ".", "up0", "=", "nn", ".", "ModuleList", "(", "self", ".", "up0", ")", "\n", "self", ".", "up1", "=", "nn", ".", "ModuleList", "(", "self", ".", "up1", ")", "\n", "self", ".", "up2", "=", "nn", ".", "ModuleList", "(", "self", ".", "up2", ")", "\n", "self", ".", "up3", "=", "nn", ".", "ModuleList", "(", "self", ".", "up3", ")", "\n", "self", ".", "up4", "=", "nn", ".", "ModuleList", "(", "self", ".", "up4", ")", "\n", "self", ".", "seg_outputs", "=", "nn", ".", "ModuleList", "(", "self", ".", "seg_outputs", ")", "\n", "if", "self", ".", "upscale_logits", ":", "\n", "            ", "self", ".", "upscale_logits_ops", "=", "nn", ".", "ModuleList", "(", "\n", "self", ".", "upscale_logits_ops", ")", "# lambda x:x is not a Module so we need to distinguish here", "\n", "\n", "", "if", "self", ".", "weightInitializer", "is", "not", "None", ":", "\n", "            ", "self", ".", "apply", "(", "self", ".", "weightInitializer", ")", "\n", "# self.apply(print_module_training_status)", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.network_architecture.generic_XNet.Generic_XNet.forward": [[394, 433], ["seg_outputs.append", "seg_outputs.append", "seg_outputs.append", "seg_outputs.append", "seg_outputs.append", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "generic_XNet.Generic_XNet.final_nonlin", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "generic_XNet.Generic_XNet.final_nonlin", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "generic_XNet.Generic_XNet.final_nonlin", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "generic_XNet.Generic_XNet.final_nonlin", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "generic_XNet.Generic_XNet.final_nonlin", "tuple", "i", "zip", "list"], "methods", ["None"], ["", "", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "# skips = []", "\n", "        ", "seg_outputs", "=", "[", "]", "\n", "x0_0", "=", "self", ".", "conv_blocks_context", "[", "0", "]", "(", "x", ")", "\n", "x1_0", "=", "self", ".", "conv_blocks_context", "[", "1", "]", "(", "x0_0", ")", "\n", "x0_1", "=", "self", ".", "loc4", "[", "0", "]", "(", "torch", ".", "cat", "(", "[", "x0_0", ",", "self", ".", "up4", "[", "0", "]", "(", "x1_0", ")", "]", ",", "1", ")", ")", "\n", "seg_outputs", ".", "append", "(", "self", ".", "final_nonlin", "(", "self", ".", "seg_outputs", "[", "-", "1", "]", "(", "x0_1", ")", ")", ")", "\n", "\n", "x2_0", "=", "self", ".", "conv_blocks_context", "[", "2", "]", "(", "x1_0", ")", "\n", "x1_1", "=", "self", ".", "loc3", "[", "0", "]", "(", "torch", ".", "cat", "(", "[", "x1_0", ",", "self", ".", "up3", "[", "0", "]", "(", "x2_0", ")", "]", ",", "1", ")", ")", "\n", "x0_2", "=", "self", ".", "loc3", "[", "1", "]", "(", "torch", ".", "cat", "(", "[", "x0_0", ",", "x0_1", ",", "self", ".", "up3", "[", "1", "]", "(", "x1_1", ")", "]", ",", "1", ")", ")", "\n", "seg_outputs", ".", "append", "(", "self", ".", "final_nonlin", "(", "self", ".", "seg_outputs", "[", "-", "2", "]", "(", "x0_2", ")", ")", ")", "\n", "\n", "x3_0", "=", "self", ".", "conv_blocks_context", "[", "3", "]", "(", "x2_0", ")", "\n", "x2_1", "=", "self", ".", "loc2", "[", "0", "]", "(", "torch", ".", "cat", "(", "[", "x2_0", ",", "self", ".", "up2", "[", "0", "]", "(", "x3_0", ")", "]", ",", "1", ")", ")", "\n", "x1_2", "=", "self", ".", "loc2", "[", "1", "]", "(", "torch", ".", "cat", "(", "[", "x1_0", ",", "x1_1", ",", "self", ".", "up2", "[", "1", "]", "(", "x2_1", ")", "]", ",", "1", ")", ")", "\n", "x0_3", "=", "self", ".", "loc2", "[", "2", "]", "(", "torch", ".", "cat", "(", "[", "x0_0", ",", "x0_1", ",", "x0_2", ",", "self", ".", "up2", "[", "2", "]", "(", "x1_2", ")", "]", ",", "1", ")", ")", "\n", "seg_outputs", ".", "append", "(", "self", ".", "final_nonlin", "(", "self", ".", "seg_outputs", "[", "-", "3", "]", "(", "x0_3", ")", ")", ")", "\n", "\n", "x4_0", "=", "self", ".", "conv_blocks_context", "[", "4", "]", "(", "x3_0", ")", "\n", "x3_1", "=", "self", ".", "loc1", "[", "0", "]", "(", "torch", ".", "cat", "(", "[", "x3_0", ",", "self", ".", "up1", "[", "0", "]", "(", "x4_0", ")", "]", ",", "1", ")", ")", "\n", "x2_2", "=", "self", ".", "loc1", "[", "1", "]", "(", "torch", ".", "cat", "(", "[", "x2_0", ",", "x2_1", ",", "self", ".", "up1", "[", "1", "]", "(", "x3_1", ")", "]", ",", "1", ")", ")", "\n", "x1_3", "=", "self", ".", "loc1", "[", "2", "]", "(", "torch", ".", "cat", "(", "[", "x1_0", ",", "x1_1", ",", "x1_2", ",", "self", ".", "up1", "[", "2", "]", "(", "x2_2", ")", "]", ",", "1", ")", ")", "\n", "x0_4", "=", "self", ".", "loc1", "[", "3", "]", "(", "torch", ".", "cat", "(", "[", "x0_0", ",", "x0_1", ",", "x0_2", ",", "x0_3", ",", "self", ".", "up1", "[", "3", "]", "(", "x1_3", ")", "]", ",", "1", ")", ")", "\n", "seg_outputs", ".", "append", "(", "self", ".", "final_nonlin", "(", "self", ".", "seg_outputs", "[", "-", "4", "]", "(", "x0_4", ")", ")", ")", "\n", "\n", "x5_0", "=", "self", ".", "conv_blocks_context", "[", "5", "]", "(", "x4_0", ")", "\n", "x4_1", "=", "self", ".", "loc0", "[", "0", "]", "(", "torch", ".", "cat", "(", "[", "x4_0", ",", "self", ".", "up0", "[", "0", "]", "(", "x5_0", ")", "]", ",", "1", ")", ")", "\n", "x3_2", "=", "self", ".", "loc0", "[", "1", "]", "(", "torch", ".", "cat", "(", "[", "x3_0", ",", "x3_1", ",", "self", ".", "up0", "[", "1", "]", "(", "x4_1", ")", "]", ",", "1", ")", ")", "\n", "x2_3", "=", "self", ".", "loc0", "[", "2", "]", "(", "torch", ".", "cat", "(", "[", "x2_0", ",", "x2_1", ",", "x2_2", ",", "self", ".", "up0", "[", "2", "]", "(", "x3_2", ")", "]", ",", "1", ")", ")", "\n", "x1_4", "=", "self", ".", "loc0", "[", "3", "]", "(", "torch", ".", "cat", "(", "[", "x1_0", ",", "x1_1", ",", "x1_2", ",", "x1_3", ",", "self", ".", "up0", "[", "3", "]", "(", "x2_3", ")", "]", ",", "1", ")", ")", "\n", "x0_5", "=", "self", ".", "loc0", "[", "4", "]", "(", "torch", ".", "cat", "(", "[", "x0_0", ",", "x0_1", ",", "x0_2", ",", "x0_3", ",", "x0_4", ",", "self", ".", "up0", "[", "4", "]", "(", "x1_4", ")", "]", ",", "1", ")", ")", "\n", "seg_outputs", ".", "append", "(", "self", ".", "final_nonlin", "(", "self", ".", "seg_outputs", "[", "-", "5", "]", "(", "x0_5", ")", ")", ")", "\n", "\n", "if", "self", ".", "_deep_supervision", "and", "self", ".", "do_ds", ":", "\n", "            ", "return", "tuple", "(", "[", "seg_outputs", "[", "-", "1", "]", "]", "+", "[", "i", "(", "j", ")", "for", "i", ",", "j", "in", "\n", "zip", "(", "list", "(", "self", ".", "upscale_logits_ops", ")", "[", ":", ":", "-", "1", "]", ",", "seg_outputs", "[", ":", "-", "1", "]", "[", ":", ":", "-", "1", "]", ")", "]", ")", "\n", "", "else", ":", "\n", "            ", "return", "seg_outputs", "[", "-", "1", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.network_architecture.generic_XNet.Generic_XNet.create_nest": [[435, 476], ["range", "conv_blocks_localization.append", "tu.append", "tu.append", "torch.nn.Sequential", "torch.nn.Sequential", "generic_XNet.Upsample", "transpconv", "generic_XNet.StackedConvLayers", "generic_XNet.StackedConvLayers"], "methods", ["None"], ["", "", "def", "create_nest", "(", "self", ",", "z", ",", "num_pool", ",", "final_num_features", ",", "num_conv_per_stage", ",", "basic_block", ",", "transpconv", ")", ":", "\n", "# print(final_num_features)", "\n", "        ", "conv_blocks_localization", "=", "[", "]", "\n", "tu", "=", "[", "]", "\n", "i", "=", "0", "\n", "# seg_outputs = []", "\n", "for", "u", "in", "range", "(", "z", ",", "num_pool", ")", ":", "\n", "            ", "nfeatures_from_down", "=", "final_num_features", "\n", "nfeatures_from_skip", "=", "self", ".", "conv_blocks_context", "[", "\n", "-", "(", "2", "+", "u", ")", "]", ".", "output_channels", "# self.conv_blocks_context[-1] is bottleneck, so start with -2", "\n", "n_features_after_tu_and_concat", "=", "nfeatures_from_skip", "*", "(", "2", "+", "u", "-", "z", ")", "\n", "if", "i", "==", "0", ":", "\n", "                ", "unet_final_features", "=", "nfeatures_from_skip", "\n", "i", "+=", "1", "\n", "# the first conv reduces the number of features to match those of skip", "\n", "# the following convs work on that number of features", "\n", "# if not convolutional upsampling then the final conv reduces the num of features again", "\n", "", "if", "u", "!=", "num_pool", "-", "1", "and", "not", "self", ".", "convolutional_upsampling", ":", "\n", "                ", "final_num_features", "=", "self", ".", "conv_blocks_context", "[", "-", "(", "3", "+", "u", ")", "]", ".", "output_channels", "\n", "", "else", ":", "\n", "                ", "final_num_features", "=", "nfeatures_from_skip", "\n", "\n", "", "if", "not", "self", ".", "convolutional_upsampling", ":", "\n", "                ", "tu", ".", "append", "(", "Upsample", "(", "scale_factor", "=", "self", ".", "pool_op_kernel_sizes", "[", "-", "(", "u", "+", "1", ")", "]", ",", "mode", "=", "self", ".", "upsample_mode", ")", ")", "\n", "", "else", ":", "\n", "                ", "tu", ".", "append", "(", "transpconv", "(", "nfeatures_from_down", ",", "nfeatures_from_skip", ",", "self", ".", "pool_op_kernel_sizes", "[", "-", "(", "u", "+", "1", ")", "]", ",", "\n", "self", ".", "pool_op_kernel_sizes", "[", "-", "(", "u", "+", "1", ")", "]", ",", "bias", "=", "False", ")", ")", "\n", "\n", "", "self", ".", "conv_kwargs", "[", "'kernel_size'", "]", "=", "self", ".", "conv_kernel_sizes", "[", "-", "(", "u", "+", "1", ")", "]", "\n", "self", ".", "conv_kwargs", "[", "'padding'", "]", "=", "self", ".", "conv_pad_sizes", "[", "-", "(", "u", "+", "1", ")", "]", "\n", "conv_blocks_localization", ".", "append", "(", "nn", ".", "Sequential", "(", "\n", "StackedConvLayers", "(", "n_features_after_tu_and_concat", ",", "nfeatures_from_skip", ",", "num_conv_per_stage", "-", "1", ",", "\n", "self", ".", "conv_op", ",", "self", ".", "conv_kwargs", ",", "self", ".", "norm_op", ",", "self", ".", "norm_op_kwargs", ",", "self", ".", "dropout_op", ",", "\n", "self", ".", "dropout_op_kwargs", ",", "self", ".", "nonlin", ",", "self", ".", "nonlin_kwargs", ",", "basic_block", "=", "basic_block", ")", ",", "\n", "StackedConvLayers", "(", "nfeatures_from_skip", ",", "final_num_features", ",", "1", ",", "self", ".", "conv_op", ",", "self", ".", "conv_kwargs", ",", "\n", "self", ".", "norm_op", ",", "self", ".", "norm_op_kwargs", ",", "self", ".", "dropout_op", ",", "self", ".", "dropout_op_kwargs", ",", "\n", "self", ".", "nonlin", ",", "self", ".", "nonlin_kwargs", ",", "basic_block", "=", "basic_block", ")", "\n", ")", ")", "\n", "# print(final_num_features)", "\n", "# print('hello')", "\n", "", "return", "conv_blocks_localization", ",", "tu", ",", "unet_final_features", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.network_architecture.generic_XNet.Generic_XNet.compute_approx_vram_consumption": [[477, 517], ["len", "numpy.array", "numpy.int64", "range", "isinstance", "numpy.array", "range", "min", "len", "numpy.prod", "numpy.prod", "numpy.prod", "numpy.prod", "numpy.prod"], "methods", ["None"], ["", "@", "staticmethod", "\n", "def", "compute_approx_vram_consumption", "(", "patch_size", ",", "num_pool_per_axis", ",", "base_num_features", ",", "max_num_features", ",", "\n", "num_modalities", ",", "num_classes", ",", "pool_op_kernel_sizes", ",", "deep_supervision", "=", "False", ",", "\n", "conv_per_stage", "=", "2", ")", ":", "\n", "        ", "\"\"\"\n        This only applies for num_conv_per_stage and convolutional_upsampling=True\n        not real vram consumption. just a constant term to which the vram consumption will be approx proportional\n        (+ offset for parameter storage)\n        :param deep_supervision:\n        :param patch_size:\n        :param num_pool_per_axis:\n        :param base_num_features:\n        :param max_num_features:\n        :param num_modalities:\n        :param num_classes:\n        :param pool_op_kernel_sizes:\n        :return:\n        \"\"\"", "\n", "if", "not", "isinstance", "(", "num_pool_per_axis", ",", "np", ".", "ndarray", ")", ":", "\n", "            ", "num_pool_per_axis", "=", "np", ".", "array", "(", "num_pool_per_axis", ")", "\n", "\n", "", "npool", "=", "len", "(", "pool_op_kernel_sizes", ")", "\n", "\n", "map_size", "=", "np", ".", "array", "(", "patch_size", ")", "\n", "tmp", "=", "np", ".", "int64", "(", "(", "conv_per_stage", "*", "2", "+", "1", ")", "*", "np", ".", "prod", "(", "map_size", ",", "dtype", "=", "np", ".", "int64", ")", "*", "base_num_features", "+", "\n", "num_modalities", "*", "np", ".", "prod", "(", "map_size", ",", "dtype", "=", "np", ".", "int64", ")", "+", "\n", "num_classes", "*", "np", ".", "prod", "(", "map_size", ",", "dtype", "=", "np", ".", "int64", ")", ")", "\n", "\n", "num_feat", "=", "base_num_features", "\n", "\n", "for", "p", "in", "range", "(", "npool", ")", ":", "\n", "            ", "for", "pi", "in", "range", "(", "len", "(", "num_pool_per_axis", ")", ")", ":", "\n", "                ", "map_size", "[", "pi", "]", "/=", "pool_op_kernel_sizes", "[", "p", "]", "[", "pi", "]", "\n", "", "num_feat", "=", "min", "(", "num_feat", "*", "2", ",", "max_num_features", ")", "\n", "num_blocks", "=", "(", "conv_per_stage", "*", "2", "+", "1", ")", "if", "p", "<", "(", "npool", "-", "1", ")", "else", "conv_per_stage", "# conv_per_stage + conv_per_stage for the convs of encode/decode and 1 for transposed conv", "\n", "tmp", "+=", "num_blocks", "*", "np", ".", "prod", "(", "map_size", ",", "dtype", "=", "np", ".", "int64", ")", "*", "num_feat", "\n", "if", "deep_supervision", "and", "p", "<", "(", "npool", "-", "2", ")", ":", "\n", "                ", "tmp", "+=", "np", ".", "prod", "(", "map_size", ",", "dtype", "=", "np", ".", "int64", ")", "*", "num_classes", "\n", "# print(p, map_size, num_feat, tmp)", "\n", "", "", "return", "tmp", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.network_architecture.generic_XNet.print_module_training_status": [[145, 152], ["isinstance", "isinstance", "isinstance", "isinstance", "isinstance", "isinstance", "isinstance", "isinstance", "isinstance", "isinstance", "isinstance", "print", "str"], "function", ["None"], ["", "", "def", "print_module_training_status", "(", "module", ")", ":", "\n", "    ", "if", "isinstance", "(", "module", ",", "nn", ".", "Conv2d", ")", "or", "isinstance", "(", "module", ",", "nn", ".", "Conv3d", ")", "or", "isinstance", "(", "module", ",", "nn", ".", "Dropout3d", ")", "or", "isinstance", "(", "module", ",", "nn", ".", "Dropout2d", ")", "or", "isinstance", "(", "module", ",", "nn", ".", "Dropout", ")", "or", "isinstance", "(", "module", ",", "nn", ".", "InstanceNorm3d", ")", "or", "isinstance", "(", "module", ",", "nn", ".", "InstanceNorm2d", ")", "or", "isinstance", "(", "module", ",", "nn", ".", "InstanceNorm1d", ")", "or", "isinstance", "(", "module", ",", "nn", ".", "BatchNorm2d", ")", "or", "isinstance", "(", "module", ",", "nn", ".", "BatchNorm3d", ")", "or", "isinstance", "(", "module", ",", "\n", "nn", ".", "BatchNorm1d", ")", ":", "\n", "        ", "print", "(", "str", "(", "module", ")", ",", "module", ".", "training", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.network_architecture.initialization.InitWeights_He.__init__": [[20, 22], ["None"], "methods", ["None"], ["    ", "def", "__init__", "(", "self", ",", "neg_slope", "=", "1e-2", ")", ":", "\n", "        ", "self", ".", "neg_slope", "=", "neg_slope", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.network_architecture.initialization.InitWeights_He.__call__": [[23, 28], ["isinstance", "isinstance", "isinstance", "isinstance", "torch.nn.init.kaiming_normal_", "torch.nn.init.constant_"], "methods", ["None"], ["", "def", "__call__", "(", "self", ",", "module", ")", ":", "\n", "        ", "if", "isinstance", "(", "module", ",", "nn", ".", "Conv3d", ")", "or", "isinstance", "(", "module", ",", "nn", ".", "Conv2d", ")", "or", "isinstance", "(", "module", ",", "nn", ".", "ConvTranspose2d", ")", "or", "isinstance", "(", "module", ",", "nn", ".", "ConvTranspose3d", ")", ":", "\n", "            ", "module", ".", "weight", "=", "nn", ".", "init", ".", "kaiming_normal_", "(", "module", ".", "weight", ",", "a", "=", "self", ".", "neg_slope", ")", "\n", "if", "module", ".", "bias", "is", "not", "None", ":", "\n", "                ", "module", ".", "bias", "=", "nn", ".", "init", ".", "constant_", "(", "module", ".", "bias", ",", "0", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.network_architecture.initialization.InitWeights_XavierUniform.__init__": [[31, 33], ["None"], "methods", ["None"], ["    ", "def", "__init__", "(", "self", ",", "gain", "=", "1", ")", ":", "\n", "        ", "self", ".", "gain", "=", "gain", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.network_architecture.initialization.InitWeights_XavierUniform.__call__": [[34, 39], ["isinstance", "isinstance", "isinstance", "isinstance", "torch.nn.init.xavier_uniform_", "torch.nn.init.constant_"], "methods", ["None"], ["", "def", "__call__", "(", "self", ",", "module", ")", ":", "\n", "        ", "if", "isinstance", "(", "module", ",", "nn", ".", "Conv3d", ")", "or", "isinstance", "(", "module", ",", "nn", ".", "Conv2d", ")", "or", "isinstance", "(", "module", ",", "nn", ".", "ConvTranspose2d", ")", "or", "isinstance", "(", "module", ",", "nn", ".", "ConvTranspose3d", ")", ":", "\n", "            ", "module", ".", "weight", "=", "nn", ".", "init", ".", "xavier_uniform_", "(", "module", ".", "weight", ",", "self", ".", "gain", ")", "\n", "if", "module", ".", "bias", "is", "not", "None", ":", "\n", "                ", "module", ".", "bias", "=", "nn", ".", "init", ".", "constant_", "(", "module", ".", "bias", ",", "0", ")", "\n", "", "", "", "", ""]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.network_architecture.generic_UNetPlusPlus.ConvDropoutNormNonlin.__init__": [[31, 63], ["torch.nn.Module.__init__", "generic_UNetPlusPlus.ConvDropoutNormNonlin.conv_op", "generic_UNetPlusPlus.ConvDropoutNormNonlin.norm_op", "generic_UNetPlusPlus.ConvDropoutNormNonlin.nonlin", "generic_UNetPlusPlus.ConvDropoutNormNonlin.dropout_op"], "methods", ["home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.common.layers.ResizeImage.__init__"], ["def", "__init__", "(", "self", ",", "input_channels", ",", "output_channels", ",", "\n", "conv_op", "=", "nn", ".", "Conv2d", ",", "conv_kwargs", "=", "None", ",", "\n", "norm_op", "=", "nn", ".", "BatchNorm2d", ",", "norm_op_kwargs", "=", "None", ",", "\n", "dropout_op", "=", "nn", ".", "Dropout2d", ",", "dropout_op_kwargs", "=", "None", ",", "\n", "nonlin", "=", "nn", ".", "LeakyReLU", ",", "nonlin_kwargs", "=", "None", ")", ":", "\n", "        ", "super", "(", "ConvDropoutNormNonlin", ",", "self", ")", ".", "__init__", "(", ")", "\n", "if", "nonlin_kwargs", "is", "None", ":", "\n", "            ", "nonlin_kwargs", "=", "{", "'negative_slope'", ":", "1e-2", ",", "'inplace'", ":", "True", "}", "\n", "", "if", "dropout_op_kwargs", "is", "None", ":", "\n", "            ", "dropout_op_kwargs", "=", "{", "'p'", ":", "0.5", ",", "'inplace'", ":", "True", "}", "\n", "", "if", "norm_op_kwargs", "is", "None", ":", "\n", "            ", "norm_op_kwargs", "=", "{", "'eps'", ":", "1e-5", ",", "'affine'", ":", "True", ",", "'momentum'", ":", "0.1", "}", "\n", "", "if", "conv_kwargs", "is", "None", ":", "\n", "            ", "conv_kwargs", "=", "{", "'kernel_size'", ":", "3", ",", "'stride'", ":", "1", ",", "'padding'", ":", "1", ",", "'dilation'", ":", "1", ",", "'bias'", ":", "True", "}", "\n", "\n", "", "self", ".", "nonlin_kwargs", "=", "nonlin_kwargs", "\n", "self", ".", "nonlin", "=", "nonlin", "\n", "self", ".", "dropout_op", "=", "dropout_op", "\n", "self", ".", "dropout_op_kwargs", "=", "dropout_op_kwargs", "\n", "self", ".", "norm_op_kwargs", "=", "norm_op_kwargs", "\n", "self", ".", "conv_kwargs", "=", "conv_kwargs", "\n", "self", ".", "conv_op", "=", "conv_op", "\n", "self", ".", "norm_op", "=", "norm_op", "\n", "\n", "self", ".", "conv", "=", "self", ".", "conv_op", "(", "input_channels", ",", "output_channels", ",", "**", "self", ".", "conv_kwargs", ")", "\n", "if", "self", ".", "dropout_op", "is", "not", "None", "and", "self", ".", "dropout_op_kwargs", "[", "'p'", "]", "is", "not", "None", "and", "self", ".", "dropout_op_kwargs", "[", "\n", "'p'", "]", ">", "0", ":", "\n", "            ", "self", ".", "dropout", "=", "self", ".", "dropout_op", "(", "**", "self", ".", "dropout_op_kwargs", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "dropout", "=", "None", "\n", "", "self", ".", "instnorm", "=", "self", ".", "norm_op", "(", "output_channels", ",", "**", "self", ".", "norm_op_kwargs", ")", "\n", "self", ".", "lrelu", "=", "self", ".", "nonlin", "(", "**", "self", ".", "nonlin_kwargs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.network_architecture.generic_UNetPlusPlus.ConvDropoutNormNonlin.forward": [[64, 69], ["generic_UNetPlusPlus.ConvDropoutNormNonlin.conv", "generic_UNetPlusPlus.ConvDropoutNormNonlin.lrelu", "generic_UNetPlusPlus.ConvDropoutNormNonlin.dropout", "generic_UNetPlusPlus.ConvDropoutNormNonlin.instnorm"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "x", "=", "self", ".", "conv", "(", "x", ")", "\n", "if", "self", ".", "dropout", "is", "not", "None", ":", "\n", "            ", "x", "=", "self", ".", "dropout", "(", "x", ")", "\n", "", "return", "self", ".", "lrelu", "(", "self", ".", "instnorm", "(", "x", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.network_architecture.generic_UNetPlusPlus.ConvDropoutNonlinNorm.forward": [[72, 77], ["generic_UNetPlusPlus.ConvDropoutNonlinNorm.conv", "generic_UNetPlusPlus.ConvDropoutNonlinNorm.instnorm", "generic_UNetPlusPlus.ConvDropoutNonlinNorm.dropout", "generic_UNetPlusPlus.ConvDropoutNonlinNorm.lrelu"], "methods", ["None"], ["    ", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "x", "=", "self", ".", "conv", "(", "x", ")", "\n", "if", "self", ".", "dropout", "is", "not", "None", ":", "\n", "            ", "x", "=", "self", ".", "dropout", "(", "x", ")", "\n", "", "return", "self", ".", "instnorm", "(", "self", ".", "lrelu", "(", "x", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.network_architecture.generic_UNetPlusPlus.StackedConvLayers.__init__": [[80, 140], ["torch.nn.Module.__init__", "torch.nn.Sequential", "torch.nn.Sequential", "copy.deepcopy", "basic_block", "basic_block", "range"], "methods", ["home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.common.layers.ResizeImage.__init__"], ["    ", "def", "__init__", "(", "self", ",", "input_feature_channels", ",", "output_feature_channels", ",", "num_convs", ",", "\n", "conv_op", "=", "nn", ".", "Conv2d", ",", "conv_kwargs", "=", "None", ",", "\n", "norm_op", "=", "nn", ".", "BatchNorm2d", ",", "norm_op_kwargs", "=", "None", ",", "\n", "dropout_op", "=", "nn", ".", "Dropout2d", ",", "dropout_op_kwargs", "=", "None", ",", "\n", "nonlin", "=", "nn", ".", "LeakyReLU", ",", "nonlin_kwargs", "=", "None", ",", "first_stride", "=", "None", ",", "basic_block", "=", "ConvDropoutNormNonlin", ")", ":", "\n", "        ", "'''\n        stacks ConvDropoutNormLReLU layers. initial_stride will only be applied to first layer in the stack. The other parameters affect all layers\n        :param input_feature_channels:\n        :param output_feature_channels:\n        :param num_convs:\n        :param dilation:\n        :param kernel_size:\n        :param padding:\n        :param dropout:\n        :param initial_stride:\n        :param conv_op:\n        :param norm_op:\n        :param dropout_op:\n        :param inplace:\n        :param neg_slope:\n        :param norm_affine:\n        :param conv_bias:\n        '''", "\n", "self", ".", "input_channels", "=", "input_feature_channels", "\n", "self", ".", "output_channels", "=", "output_feature_channels", "\n", "\n", "if", "nonlin_kwargs", "is", "None", ":", "\n", "            ", "nonlin_kwargs", "=", "{", "'negative_slope'", ":", "1e-2", ",", "'inplace'", ":", "True", "}", "\n", "", "if", "dropout_op_kwargs", "is", "None", ":", "\n", "            ", "dropout_op_kwargs", "=", "{", "'p'", ":", "0.5", ",", "'inplace'", ":", "True", "}", "\n", "", "if", "norm_op_kwargs", "is", "None", ":", "\n", "            ", "norm_op_kwargs", "=", "{", "'eps'", ":", "1e-5", ",", "'affine'", ":", "True", ",", "'momentum'", ":", "0.1", "}", "\n", "", "if", "conv_kwargs", "is", "None", ":", "\n", "            ", "conv_kwargs", "=", "{", "'kernel_size'", ":", "3", ",", "'stride'", ":", "1", ",", "'padding'", ":", "1", ",", "'dilation'", ":", "1", ",", "'bias'", ":", "True", "}", "\n", "\n", "", "self", ".", "nonlin_kwargs", "=", "nonlin_kwargs", "\n", "self", ".", "nonlin", "=", "nonlin", "\n", "self", ".", "dropout_op", "=", "dropout_op", "\n", "self", ".", "dropout_op_kwargs", "=", "dropout_op_kwargs", "\n", "self", ".", "norm_op_kwargs", "=", "norm_op_kwargs", "\n", "self", ".", "conv_kwargs", "=", "conv_kwargs", "\n", "self", ".", "conv_op", "=", "conv_op", "\n", "self", ".", "norm_op", "=", "norm_op", "\n", "\n", "if", "first_stride", "is", "not", "None", ":", "\n", "            ", "self", ".", "conv_kwargs_first_conv", "=", "deepcopy", "(", "conv_kwargs", ")", "\n", "self", ".", "conv_kwargs_first_conv", "[", "'stride'", "]", "=", "first_stride", "\n", "", "else", ":", "\n", "            ", "self", ".", "conv_kwargs_first_conv", "=", "conv_kwargs", "\n", "\n", "", "super", "(", "StackedConvLayers", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "blocks", "=", "nn", ".", "Sequential", "(", "\n", "*", "(", "[", "basic_block", "(", "input_feature_channels", ",", "output_feature_channels", ",", "self", ".", "conv_op", ",", "\n", "self", ".", "conv_kwargs_first_conv", ",", "\n", "self", ".", "norm_op", ",", "self", ".", "norm_op_kwargs", ",", "self", ".", "dropout_op", ",", "self", ".", "dropout_op_kwargs", ",", "\n", "self", ".", "nonlin", ",", "self", ".", "nonlin_kwargs", ")", "]", "+", "\n", "[", "basic_block", "(", "output_feature_channels", ",", "output_feature_channels", ",", "self", ".", "conv_op", ",", "\n", "self", ".", "conv_kwargs", ",", "\n", "self", ".", "norm_op", ",", "self", ".", "norm_op_kwargs", ",", "self", ".", "dropout_op", ",", "self", ".", "dropout_op_kwargs", ",", "\n", "self", ".", "nonlin", ",", "self", ".", "nonlin_kwargs", ")", "for", "_", "in", "range", "(", "num_convs", "-", "1", ")", "]", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.network_architecture.generic_UNetPlusPlus.StackedConvLayers.forward": [[141, 143], ["generic_UNetPlusPlus.StackedConvLayers.blocks"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "return", "self", ".", "blocks", "(", "x", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.network_architecture.generic_UNetPlusPlus.Upsample.__init__": [[155, 161], ["torch.nn.Module.__init__"], "methods", ["home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.common.layers.ResizeImage.__init__"], ["    ", "def", "__init__", "(", "self", ",", "size", "=", "None", ",", "scale_factor", "=", "None", ",", "mode", "=", "'nearest'", ",", "align_corners", "=", "False", ")", ":", "\n", "        ", "super", "(", "Upsample", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "align_corners", "=", "align_corners", "\n", "self", ".", "mode", "=", "mode", "\n", "self", ".", "scale_factor", "=", "scale_factor", "\n", "self", ".", "size", "=", "size", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.network_architecture.generic_UNetPlusPlus.Upsample.forward": [[162, 165], ["torch.nn.functional.interpolate", "torch.nn.functional.interpolate"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "return", "nn", ".", "functional", ".", "interpolate", "(", "x", ",", "size", "=", "self", ".", "size", ",", "scale_factor", "=", "self", ".", "scale_factor", ",", "mode", "=", "self", ".", "mode", ",", "\n", "align_corners", "=", "self", ".", "align_corners", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.network_architecture.generic_UNetPlusPlus.Generic_UNetPlusPlus.__init__": [[184, 392], ["nnunet.network_architecture.initialization.InitWeights_He", "nnunet.network_architecture.neural_network.SegmentationNetwork.__init__", "numpy.prod", "range", "generic_UNetPlusPlus.Generic_UNetPlusPlus.conv_blocks_context.append", "generic_UNetPlusPlus.Generic_UNetPlusPlus.create_nest", "generic_UNetPlusPlus.Generic_UNetPlusPlus.create_nest", "generic_UNetPlusPlus.Generic_UNetPlusPlus.create_nest", "generic_UNetPlusPlus.Generic_UNetPlusPlus.create_nest", "generic_UNetPlusPlus.Generic_UNetPlusPlus.create_nest", "generic_UNetPlusPlus.Generic_UNetPlusPlus.seg_outputs.append", "generic_UNetPlusPlus.Generic_UNetPlusPlus.seg_outputs.append", "generic_UNetPlusPlus.Generic_UNetPlusPlus.seg_outputs.append", "generic_UNetPlusPlus.Generic_UNetPlusPlus.seg_outputs.append", "generic_UNetPlusPlus.Generic_UNetPlusPlus.seg_outputs.append", "range", "torch.nn.ModuleList", "torch.nn.ModuleList", "torch.nn.ModuleList", "torch.nn.ModuleList", "torch.nn.ModuleList", "torch.nn.ModuleList", "torch.nn.ModuleList", "torch.nn.ModuleList", "torch.nn.ModuleList", "torch.nn.ModuleList", "torch.nn.ModuleList", "torch.nn.ModuleList", "torch.nn.ModuleList", "torch.nn.ModuleList", "torch.nn.ModuleList", "torch.nn.ModuleList", "torch.nn.ModuleList", "torch.nn.ModuleList", "torch.nn.ModuleList", "torch.nn.ModuleList", "torch.nn.ModuleList", "torch.nn.ModuleList", "torch.nn.ModuleList", "torch.nn.ModuleList", "torch.nn.ModuleList", "torch.nn.ModuleList", "generic_UNetPlusPlus.Generic_UNetPlusPlus.conv_pad_sizes.append", "generic_UNetPlusPlus.Generic_UNetPlusPlus.conv_blocks_context.append", "int", "min", "torch.nn.Sequential", "torch.nn.Sequential", "conv_op", "conv_op", "conv_op", "conv_op", "conv_op", "numpy.cumprod", "torch.nn.ModuleList", "torch.nn.ModuleList", "generic_UNetPlusPlus.Generic_UNetPlusPlus.apply", "ValueError", "generic_UNetPlusPlus.StackedConvLayers", "generic_UNetPlusPlus.Generic_UNetPlusPlus.td.append", "numpy.round", "generic_UNetPlusPlus.StackedConvLayers", "generic_UNetPlusPlus.StackedConvLayers", "numpy.vstack", "generic_UNetPlusPlus.Generic_UNetPlusPlus.upscale_logits_ops.append", "generic_UNetPlusPlus.Generic_UNetPlusPlus.upscale_logits_ops.append", "pool_op", "generic_UNetPlusPlus.Upsample", "str", "tuple", "int"], "methods", ["home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.common.layers.ResizeImage.__init__", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.network_architecture.generic_hipp_XNet.Generic_XNet.create_nest", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.network_architecture.generic_hipp_XNet.Generic_XNet.create_nest", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.network_architecture.generic_hipp_XNet.Generic_XNet.create_nest", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.network_architecture.generic_hipp_XNet.Generic_XNet.create_nest", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.network_architecture.generic_hipp_XNet.Generic_XNet.create_nest"], ["def", "__init__", "(", "self", ",", "input_channels", ",", "base_num_features", ",", "num_classes", ",", "num_pool", ",", "num_conv_per_stage", "=", "2", ",", "\n", "feat_map_mul_on_downscale", "=", "2", ",", "conv_op", "=", "nn", ".", "Conv2d", ",", "\n", "norm_op", "=", "nn", ".", "BatchNorm2d", ",", "norm_op_kwargs", "=", "None", ",", "\n", "dropout_op", "=", "nn", ".", "Dropout2d", ",", "dropout_op_kwargs", "=", "None", ",", "\n", "nonlin", "=", "nn", ".", "LeakyReLU", ",", "nonlin_kwargs", "=", "None", ",", "deep_supervision", "=", "True", ",", "dropout_in_localization", "=", "False", ",", "\n", "final_nonlin", "=", "softmax_helper", ",", "weightInitializer", "=", "InitWeights_He", "(", "1e-2", ")", ",", "pool_op_kernel_sizes", "=", "None", ",", "\n", "conv_kernel_sizes", "=", "None", ",", "\n", "upscale_logits", "=", "False", ",", "convolutional_pooling", "=", "False", ",", "convolutional_upsampling", "=", "False", ",", "\n", "max_num_features", "=", "None", ",", "basic_block", "=", "ConvDropoutNormNonlin", ",", "\n", "seg_output_use_bias", "=", "False", ")", ":", "\n", "        ", "\"\"\"\n        basically more flexible than v1, architecture is the same\n\n        Does this look complicated? Nah bro. Functionality > usability\n\n        This does everything you need, including world peace.\n\n        Questions? -> f.isensee@dkfz.de\n        \"\"\"", "\n", "super", "(", "Generic_UNetPlusPlus", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "convolutional_upsampling", "=", "convolutional_upsampling", "\n", "self", ".", "convolutional_pooling", "=", "convolutional_pooling", "\n", "self", ".", "upscale_logits", "=", "upscale_logits", "\n", "if", "nonlin_kwargs", "is", "None", ":", "\n", "            ", "nonlin_kwargs", "=", "{", "'negative_slope'", ":", "1e-2", ",", "'inplace'", ":", "True", "}", "\n", "", "if", "dropout_op_kwargs", "is", "None", ":", "\n", "            ", "dropout_op_kwargs", "=", "{", "'p'", ":", "0.5", ",", "'inplace'", ":", "True", "}", "\n", "", "if", "norm_op_kwargs", "is", "None", ":", "\n", "            ", "norm_op_kwargs", "=", "{", "'eps'", ":", "1e-5", ",", "'affine'", ":", "True", ",", "'momentum'", ":", "0.1", "}", "\n", "\n", "", "self", ".", "conv_kwargs", "=", "{", "'stride'", ":", "1", ",", "'dilation'", ":", "1", ",", "'bias'", ":", "True", "}", "\n", "\n", "self", ".", "nonlin", "=", "nonlin", "\n", "self", ".", "nonlin_kwargs", "=", "nonlin_kwargs", "\n", "self", ".", "dropout_op_kwargs", "=", "dropout_op_kwargs", "\n", "self", ".", "norm_op_kwargs", "=", "norm_op_kwargs", "\n", "self", ".", "weightInitializer", "=", "weightInitializer", "\n", "self", ".", "conv_op", "=", "conv_op", "\n", "self", ".", "norm_op", "=", "norm_op", "\n", "self", ".", "dropout_op", "=", "dropout_op", "\n", "self", ".", "num_classes", "=", "num_classes", "\n", "self", ".", "final_nonlin", "=", "final_nonlin", "\n", "self", ".", "_deep_supervision", "=", "deep_supervision", "\n", "self", ".", "do_ds", "=", "deep_supervision", "\n", "\n", "if", "conv_op", "==", "nn", ".", "Conv2d", ":", "\n", "            ", "upsample_mode", "=", "'bilinear'", "\n", "pool_op", "=", "nn", ".", "MaxPool2d", "\n", "transpconv", "=", "nn", ".", "ConvTranspose2d", "\n", "if", "pool_op_kernel_sizes", "is", "None", ":", "\n", "                ", "pool_op_kernel_sizes", "=", "[", "(", "2", ",", "2", ")", "]", "*", "num_pool", "\n", "", "if", "conv_kernel_sizes", "is", "None", ":", "\n", "                ", "conv_kernel_sizes", "=", "[", "(", "3", ",", "3", ")", "]", "*", "(", "num_pool", "+", "1", ")", "\n", "", "", "elif", "conv_op", "==", "nn", ".", "Conv3d", ":", "\n", "            ", "upsample_mode", "=", "'trilinear'", "\n", "pool_op", "=", "nn", ".", "MaxPool3d", "\n", "transpconv", "=", "nn", ".", "ConvTranspose3d", "\n", "if", "pool_op_kernel_sizes", "is", "None", ":", "\n", "                ", "pool_op_kernel_sizes", "=", "[", "(", "2", ",", "2", ",", "2", ")", "]", "*", "num_pool", "\n", "", "if", "conv_kernel_sizes", "is", "None", ":", "\n", "                ", "conv_kernel_sizes", "=", "[", "(", "3", ",", "3", ",", "3", ")", "]", "*", "(", "num_pool", "+", "1", ")", "\n", "", "", "else", ":", "\n", "            ", "raise", "ValueError", "(", "\"unknown convolution dimensionality, conv op: %s\"", "%", "str", "(", "conv_op", ")", ")", "\n", "\n", "", "self", ".", "input_shape_must_be_divisible_by", "=", "np", ".", "prod", "(", "pool_op_kernel_sizes", ",", "0", ",", "dtype", "=", "np", ".", "int64", ")", "\n", "self", ".", "pool_op_kernel_sizes", "=", "pool_op_kernel_sizes", "\n", "self", ".", "conv_kernel_sizes", "=", "conv_kernel_sizes", "\n", "\n", "self", ".", "conv_pad_sizes", "=", "[", "]", "\n", "for", "krnl", "in", "self", ".", "conv_kernel_sizes", ":", "\n", "            ", "self", ".", "conv_pad_sizes", ".", "append", "(", "[", "1", "if", "i", "==", "3", "else", "0", "for", "i", "in", "krnl", "]", ")", "\n", "\n", "", "if", "max_num_features", "is", "None", ":", "\n", "            ", "if", "self", ".", "conv_op", "==", "nn", ".", "Conv3d", ":", "\n", "                ", "self", ".", "max_num_features", "=", "self", ".", "MAX_NUM_FILTERS_3D", "\n", "", "else", ":", "\n", "                ", "self", ".", "max_num_features", "=", "self", ".", "MAX_FILTERS_2D", "\n", "", "", "else", ":", "\n", "            ", "self", ".", "max_num_features", "=", "max_num_features", "\n", "\n", "", "self", ".", "conv_blocks_context", "=", "[", "]", "\n", "# self.conv_blocks_localization = []", "\n", "self", ".", "loc0", "=", "[", "]", "\n", "self", ".", "loc1", "=", "[", "]", "\n", "self", ".", "loc2", "=", "[", "]", "\n", "self", ".", "loc3", "=", "[", "]", "\n", "self", ".", "loc4", "=", "[", "]", "\n", "self", ".", "td", "=", "[", "]", "\n", "self", ".", "up0", "=", "[", "]", "\n", "self", ".", "up1", "=", "[", "]", "\n", "self", ".", "up2", "=", "[", "]", "\n", "self", ".", "up3", "=", "[", "]", "\n", "self", ".", "up4", "=", "[", "]", "\n", "# self.tu = []", "\n", "self", ".", "seg_outputs", "=", "[", "]", "\n", "\n", "output_features", "=", "base_num_features", "\n", "input_features", "=", "input_channels", "\n", "\n", "for", "d", "in", "range", "(", "num_pool", ")", ":", "\n", "# determine the first stride", "\n", "            ", "if", "d", "!=", "0", "and", "self", ".", "convolutional_pooling", ":", "\n", "                ", "first_stride", "=", "pool_op_kernel_sizes", "[", "d", "-", "1", "]", "\n", "", "else", ":", "\n", "                ", "first_stride", "=", "None", "\n", "\n", "", "self", ".", "conv_kwargs", "[", "'kernel_size'", "]", "=", "self", ".", "conv_kernel_sizes", "[", "d", "]", "\n", "self", ".", "conv_kwargs", "[", "'padding'", "]", "=", "self", ".", "conv_pad_sizes", "[", "d", "]", "\n", "# add convolutions", "\n", "self", ".", "conv_blocks_context", ".", "append", "(", "StackedConvLayers", "(", "input_features", ",", "output_features", ",", "num_conv_per_stage", ",", "\n", "self", ".", "conv_op", ",", "self", ".", "conv_kwargs", ",", "self", ".", "norm_op", ",", "\n", "self", ".", "norm_op_kwargs", ",", "self", ".", "dropout_op", ",", "\n", "self", ".", "dropout_op_kwargs", ",", "self", ".", "nonlin", ",", "self", ".", "nonlin_kwargs", ",", "\n", "first_stride", ",", "basic_block", "=", "basic_block", ")", ")", "\n", "if", "not", "self", ".", "convolutional_pooling", ":", "\n", "                ", "self", ".", "td", ".", "append", "(", "pool_op", "(", "pool_op_kernel_sizes", "[", "d", "]", ")", ")", "\n", "", "input_features", "=", "output_features", "\n", "output_features", "=", "int", "(", "np", ".", "round", "(", "output_features", "*", "feat_map_mul_on_downscale", ")", ")", "\n", "\n", "output_features", "=", "min", "(", "output_features", ",", "self", ".", "max_num_features", ")", "\n", "\n", "# now the bottleneck.", "\n", "# determine the first stride", "\n", "", "if", "self", ".", "convolutional_pooling", ":", "\n", "            ", "first_stride", "=", "pool_op_kernel_sizes", "[", "-", "1", "]", "\n", "", "else", ":", "\n", "            ", "first_stride", "=", "None", "\n", "\n", "# the output of the last conv must match the number of features from the skip connection if we are not using", "\n", "# convolutional upsampling. If we use convolutional upsampling then the reduction in feature maps will be", "\n", "# done by the transposed conv", "\n", "", "if", "self", ".", "convolutional_upsampling", ":", "\n", "            ", "final_num_features", "=", "output_features", "\n", "", "else", ":", "\n", "            ", "final_num_features", "=", "self", ".", "conv_blocks_context", "[", "-", "1", "]", ".", "output_channels", "\n", "\n", "", "self", ".", "conv_kwargs", "[", "'kernel_size'", "]", "=", "self", ".", "conv_kernel_sizes", "[", "num_pool", "]", "\n", "self", ".", "conv_kwargs", "[", "'padding'", "]", "=", "self", ".", "conv_pad_sizes", "[", "num_pool", "]", "\n", "self", ".", "conv_blocks_context", ".", "append", "(", "nn", ".", "Sequential", "(", "\n", "StackedConvLayers", "(", "input_features", ",", "output_features", ",", "num_conv_per_stage", "-", "1", ",", "self", ".", "conv_op", ",", "self", ".", "conv_kwargs", ",", "\n", "self", ".", "norm_op", ",", "self", ".", "norm_op_kwargs", ",", "self", ".", "dropout_op", ",", "self", ".", "dropout_op_kwargs", ",", "self", ".", "nonlin", ",", "\n", "self", ".", "nonlin_kwargs", ",", "first_stride", ",", "basic_block", "=", "basic_block", ")", ",", "\n", "StackedConvLayers", "(", "output_features", ",", "final_num_features", ",", "1", ",", "self", ".", "conv_op", ",", "self", ".", "conv_kwargs", ",", "\n", "self", ".", "norm_op", ",", "self", ".", "norm_op_kwargs", ",", "self", ".", "dropout_op", ",", "self", ".", "dropout_op_kwargs", ",", "self", ".", "nonlin", ",", "\n", "self", ".", "nonlin_kwargs", ",", "basic_block", "=", "basic_block", ")", ")", ")", "\n", "\n", "# if we don't want to do dropout in the localization pathway then we set the dropout prob to zero here", "\n", "if", "not", "dropout_in_localization", ":", "\n", "            ", "old_dropout_p", "=", "self", ".", "dropout_op_kwargs", "[", "'p'", "]", "\n", "self", ".", "dropout_op_kwargs", "[", "'p'", "]", "=", "0.0", "\n", "\n", "# now lets build the localization pathway", "\n", "", "encoder_features", "=", "final_num_features", "\n", "self", ".", "loc0", ",", "self", ".", "up0", ",", "encoder_features", "=", "self", ".", "create_nest", "(", "0", ",", "num_pool", ",", "final_num_features", ",", "num_conv_per_stage", ",", "\n", "basic_block", ",", "transpconv", ")", "\n", "self", ".", "loc1", ",", "self", ".", "up1", ",", "encoder_features1", "=", "self", ".", "create_nest", "(", "1", ",", "num_pool", ",", "encoder_features", ",", "num_conv_per_stage", ",", "\n", "basic_block", ",", "transpconv", ")", "\n", "self", ".", "loc2", ",", "self", ".", "up2", ",", "encoder_features2", "=", "self", ".", "create_nest", "(", "2", ",", "num_pool", ",", "encoder_features1", ",", "num_conv_per_stage", ",", "\n", "basic_block", ",", "transpconv", ")", "\n", "self", ".", "loc3", ",", "self", ".", "up3", ",", "encoder_features3", "=", "self", ".", "create_nest", "(", "3", ",", "num_pool", ",", "encoder_features2", ",", "num_conv_per_stage", ",", "\n", "basic_block", ",", "transpconv", ")", "\n", "self", ".", "loc4", ",", "self", ".", "up4", ",", "encoder_features4", "=", "self", ".", "create_nest", "(", "4", ",", "num_pool", ",", "encoder_features3", ",", "num_conv_per_stage", ",", "\n", "basic_block", ",", "transpconv", ")", "\n", "\n", "self", ".", "seg_outputs", ".", "append", "(", "conv_op", "(", "self", ".", "loc0", "[", "-", "1", "]", "[", "-", "1", "]", ".", "output_channels", ",", "num_classes", ",", "\n", "1", ",", "1", ",", "0", ",", "1", ",", "1", ",", "seg_output_use_bias", ")", ")", "\n", "self", ".", "seg_outputs", ".", "append", "(", "conv_op", "(", "self", ".", "loc1", "[", "-", "1", "]", "[", "-", "1", "]", ".", "output_channels", ",", "num_classes", ",", "\n", "1", ",", "1", ",", "0", ",", "1", ",", "1", ",", "seg_output_use_bias", ")", ")", "\n", "self", ".", "seg_outputs", ".", "append", "(", "conv_op", "(", "self", ".", "loc2", "[", "-", "1", "]", "[", "-", "1", "]", ".", "output_channels", ",", "num_classes", ",", "\n", "1", ",", "1", ",", "0", ",", "1", ",", "1", ",", "seg_output_use_bias", ")", ")", "\n", "self", ".", "seg_outputs", ".", "append", "(", "conv_op", "(", "self", ".", "loc3", "[", "-", "1", "]", "[", "-", "1", "]", ".", "output_channels", ",", "num_classes", ",", "\n", "1", ",", "1", ",", "0", ",", "1", ",", "1", ",", "seg_output_use_bias", ")", ")", "\n", "self", ".", "seg_outputs", ".", "append", "(", "conv_op", "(", "self", ".", "loc4", "[", "-", "1", "]", "[", "-", "1", "]", ".", "output_channels", ",", "num_classes", ",", "\n", "1", ",", "1", ",", "0", ",", "1", ",", "1", ",", "seg_output_use_bias", ")", ")", "\n", "\n", "self", ".", "upscale_logits_ops", "=", "[", "]", "\n", "cum_upsample", "=", "np", ".", "cumprod", "(", "np", ".", "vstack", "(", "pool_op_kernel_sizes", ")", ",", "axis", "=", "0", ")", "[", ":", ":", "-", "1", "]", "\n", "for", "usl", "in", "range", "(", "num_pool", ")", ":", "\n", "            ", "if", "self", ".", "upscale_logits", ":", "\n", "                ", "self", ".", "upscale_logits_ops", ".", "append", "(", "Upsample", "(", "scale_factor", "=", "tuple", "(", "[", "int", "(", "i", ")", "for", "i", "in", "cum_upsample", "[", "usl", "+", "1", "]", "]", ")", ",", "\n", "mode", "=", "upsample_mode", ")", ")", "\n", "", "else", ":", "\n", "                ", "self", ".", "upscale_logits_ops", ".", "append", "(", "lambda", "x", ":", "x", ")", "\n", "\n", "", "", "if", "not", "dropout_in_localization", ":", "\n", "            ", "self", ".", "dropout_op_kwargs", "[", "'p'", "]", "=", "old_dropout_p", "\n", "\n", "# register all modules properly", "\n", "# self.conv_blocks_localization = nn.ModuleList(self.conv_blocks_localization)", "\n", "", "self", ".", "loc0", "=", "nn", ".", "ModuleList", "(", "self", ".", "loc0", ")", "\n", "self", ".", "loc1", "=", "nn", ".", "ModuleList", "(", "self", ".", "loc1", ")", "\n", "self", ".", "loc2", "=", "nn", ".", "ModuleList", "(", "self", ".", "loc2", ")", "\n", "self", ".", "loc3", "=", "nn", ".", "ModuleList", "(", "self", ".", "loc3", ")", "\n", "self", ".", "loc4", "=", "nn", ".", "ModuleList", "(", "self", ".", "loc4", ")", "\n", "self", ".", "conv_blocks_context", "=", "nn", ".", "ModuleList", "(", "self", ".", "conv_blocks_context", ")", "\n", "self", ".", "td", "=", "nn", ".", "ModuleList", "(", "self", ".", "td", ")", "\n", "self", ".", "up0", "=", "nn", ".", "ModuleList", "(", "self", ".", "up0", ")", "\n", "self", ".", "up1", "=", "nn", ".", "ModuleList", "(", "self", ".", "up1", ")", "\n", "self", ".", "up2", "=", "nn", ".", "ModuleList", "(", "self", ".", "up2", ")", "\n", "self", ".", "up3", "=", "nn", ".", "ModuleList", "(", "self", ".", "up3", ")", "\n", "self", ".", "up4", "=", "nn", ".", "ModuleList", "(", "self", ".", "up4", ")", "\n", "self", ".", "seg_outputs", "=", "nn", ".", "ModuleList", "(", "self", ".", "seg_outputs", ")", "\n", "if", "self", ".", "upscale_logits", ":", "\n", "            ", "self", ".", "upscale_logits_ops", "=", "nn", ".", "ModuleList", "(", "\n", "self", ".", "upscale_logits_ops", ")", "# lambda x:x is not a Module so we need to distinguish here", "\n", "\n", "", "if", "self", ".", "weightInitializer", "is", "not", "None", ":", "\n", "            ", "self", ".", "apply", "(", "self", ".", "weightInitializer", ")", "\n", "# self.apply(print_module_training_status)", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.network_architecture.generic_UNetPlusPlus.Generic_UNetPlusPlus.forward": [[394, 433], ["seg_outputs.append", "seg_outputs.append", "seg_outputs.append", "seg_outputs.append", "seg_outputs.append", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "generic_UNetPlusPlus.Generic_UNetPlusPlus.final_nonlin", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "generic_UNetPlusPlus.Generic_UNetPlusPlus.final_nonlin", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "generic_UNetPlusPlus.Generic_UNetPlusPlus.final_nonlin", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "generic_UNetPlusPlus.Generic_UNetPlusPlus.final_nonlin", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "generic_UNetPlusPlus.Generic_UNetPlusPlus.final_nonlin", "tuple", "i", "zip", "list"], "methods", ["None"], ["", "", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "# skips = []", "\n", "        ", "seg_outputs", "=", "[", "]", "\n", "x0_0", "=", "self", ".", "conv_blocks_context", "[", "0", "]", "(", "x", ")", "\n", "x1_0", "=", "self", ".", "conv_blocks_context", "[", "1", "]", "(", "x0_0", ")", "\n", "x0_1", "=", "self", ".", "loc4", "[", "0", "]", "(", "torch", ".", "cat", "(", "[", "x0_0", ",", "self", ".", "up4", "[", "0", "]", "(", "x1_0", ")", "]", ",", "1", ")", ")", "\n", "seg_outputs", ".", "append", "(", "self", ".", "final_nonlin", "(", "self", ".", "seg_outputs", "[", "-", "1", "]", "(", "x0_1", ")", ")", ")", "\n", "\n", "x2_0", "=", "self", ".", "conv_blocks_context", "[", "2", "]", "(", "x1_0", ")", "\n", "x1_1", "=", "self", ".", "loc3", "[", "0", "]", "(", "torch", ".", "cat", "(", "[", "x1_0", ",", "self", ".", "up3", "[", "0", "]", "(", "x2_0", ")", "]", ",", "1", ")", ")", "\n", "x0_2", "=", "self", ".", "loc3", "[", "1", "]", "(", "torch", ".", "cat", "(", "[", "x0_0", ",", "x0_1", ",", "self", ".", "up3", "[", "1", "]", "(", "x1_1", ")", "]", ",", "1", ")", ")", "\n", "seg_outputs", ".", "append", "(", "self", ".", "final_nonlin", "(", "self", ".", "seg_outputs", "[", "-", "2", "]", "(", "x0_2", ")", ")", ")", "\n", "\n", "x3_0", "=", "self", ".", "conv_blocks_context", "[", "3", "]", "(", "x2_0", ")", "\n", "x2_1", "=", "self", ".", "loc2", "[", "0", "]", "(", "torch", ".", "cat", "(", "[", "x2_0", ",", "self", ".", "up2", "[", "0", "]", "(", "x3_0", ")", "]", ",", "1", ")", ")", "\n", "x1_2", "=", "self", ".", "loc2", "[", "1", "]", "(", "torch", ".", "cat", "(", "[", "x1_0", ",", "x1_1", ",", "self", ".", "up2", "[", "1", "]", "(", "x2_1", ")", "]", ",", "1", ")", ")", "\n", "x0_3", "=", "self", ".", "loc2", "[", "2", "]", "(", "torch", ".", "cat", "(", "[", "x0_0", ",", "x0_1", ",", "x0_2", ",", "self", ".", "up2", "[", "2", "]", "(", "x1_2", ")", "]", ",", "1", ")", ")", "\n", "seg_outputs", ".", "append", "(", "self", ".", "final_nonlin", "(", "self", ".", "seg_outputs", "[", "-", "3", "]", "(", "x0_3", ")", ")", ")", "\n", "\n", "x4_0", "=", "self", ".", "conv_blocks_context", "[", "4", "]", "(", "x3_0", ")", "\n", "x3_1", "=", "self", ".", "loc1", "[", "0", "]", "(", "torch", ".", "cat", "(", "[", "x3_0", ",", "self", ".", "up1", "[", "0", "]", "(", "x4_0", ")", "]", ",", "1", ")", ")", "\n", "x2_2", "=", "self", ".", "loc1", "[", "1", "]", "(", "torch", ".", "cat", "(", "[", "x2_0", ",", "x2_1", ",", "self", ".", "up1", "[", "1", "]", "(", "x3_1", ")", "]", ",", "1", ")", ")", "\n", "x1_3", "=", "self", ".", "loc1", "[", "2", "]", "(", "torch", ".", "cat", "(", "[", "x1_0", ",", "x1_1", ",", "x1_2", ",", "self", ".", "up1", "[", "2", "]", "(", "x2_2", ")", "]", ",", "1", ")", ")", "\n", "x0_4", "=", "self", ".", "loc1", "[", "3", "]", "(", "torch", ".", "cat", "(", "[", "x0_0", ",", "x0_1", ",", "x0_2", ",", "x0_3", ",", "self", ".", "up1", "[", "3", "]", "(", "x1_3", ")", "]", ",", "1", ")", ")", "\n", "seg_outputs", ".", "append", "(", "self", ".", "final_nonlin", "(", "self", ".", "seg_outputs", "[", "-", "4", "]", "(", "x0_4", ")", ")", ")", "\n", "\n", "x5_0", "=", "self", ".", "conv_blocks_context", "[", "5", "]", "(", "x4_0", ")", "\n", "x4_1", "=", "self", ".", "loc0", "[", "0", "]", "(", "torch", ".", "cat", "(", "[", "x4_0", ",", "self", ".", "up0", "[", "0", "]", "(", "x5_0", ")", "]", ",", "1", ")", ")", "\n", "x3_2", "=", "self", ".", "loc0", "[", "1", "]", "(", "torch", ".", "cat", "(", "[", "x3_0", ",", "x3_1", ",", "self", ".", "up0", "[", "1", "]", "(", "x4_1", ")", "]", ",", "1", ")", ")", "\n", "x2_3", "=", "self", ".", "loc0", "[", "2", "]", "(", "torch", ".", "cat", "(", "[", "x2_0", ",", "x2_1", ",", "x2_2", ",", "self", ".", "up0", "[", "2", "]", "(", "x3_2", ")", "]", ",", "1", ")", ")", "\n", "x1_4", "=", "self", ".", "loc0", "[", "3", "]", "(", "torch", ".", "cat", "(", "[", "x1_0", ",", "x1_1", ",", "x1_2", ",", "x1_3", ",", "self", ".", "up0", "[", "3", "]", "(", "x2_3", ")", "]", ",", "1", ")", ")", "\n", "x0_5", "=", "self", ".", "loc0", "[", "4", "]", "(", "torch", ".", "cat", "(", "[", "x0_0", ",", "x0_1", ",", "x0_2", ",", "x0_3", ",", "x0_4", ",", "self", ".", "up0", "[", "4", "]", "(", "x1_4", ")", "]", ",", "1", ")", ")", "\n", "seg_outputs", ".", "append", "(", "self", ".", "final_nonlin", "(", "self", ".", "seg_outputs", "[", "-", "5", "]", "(", "x0_5", ")", ")", ")", "\n", "\n", "if", "self", ".", "_deep_supervision", "and", "self", ".", "do_ds", ":", "\n", "            ", "return", "tuple", "(", "[", "seg_outputs", "[", "-", "1", "]", "]", "+", "[", "i", "(", "j", ")", "for", "i", ",", "j", "in", "\n", "zip", "(", "list", "(", "self", ".", "upscale_logits_ops", ")", "[", ":", ":", "-", "1", "]", ",", "seg_outputs", "[", ":", "-", "1", "]", "[", ":", ":", "-", "1", "]", ")", "]", ")", "\n", "", "else", ":", "\n", "            ", "return", "seg_outputs", "[", "-", "1", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.network_architecture.generic_UNetPlusPlus.Generic_UNetPlusPlus.create_nest": [[435, 476], ["range", "conv_blocks_localization.append", "tu.append", "tu.append", "torch.nn.Sequential", "torch.nn.Sequential", "generic_UNetPlusPlus.Upsample", "transpconv", "generic_UNetPlusPlus.StackedConvLayers", "generic_UNetPlusPlus.StackedConvLayers"], "methods", ["None"], ["", "", "def", "create_nest", "(", "self", ",", "z", ",", "num_pool", ",", "final_num_features", ",", "num_conv_per_stage", ",", "basic_block", ",", "transpconv", ")", ":", "\n", "# print(final_num_features)", "\n", "        ", "conv_blocks_localization", "=", "[", "]", "\n", "tu", "=", "[", "]", "\n", "i", "=", "0", "\n", "# seg_outputs = []", "\n", "for", "u", "in", "range", "(", "z", ",", "num_pool", ")", ":", "\n", "            ", "nfeatures_from_down", "=", "final_num_features", "\n", "nfeatures_from_skip", "=", "self", ".", "conv_blocks_context", "[", "\n", "-", "(", "2", "+", "u", ")", "]", ".", "output_channels", "# self.conv_blocks_context[-1] is bottleneck, so start with -2", "\n", "n_features_after_tu_and_concat", "=", "nfeatures_from_skip", "*", "(", "2", "+", "u", "-", "z", ")", "\n", "if", "i", "==", "0", ":", "\n", "                ", "unet_final_features", "=", "nfeatures_from_skip", "\n", "i", "+=", "1", "\n", "# the first conv reduces the number of features to match those of skip", "\n", "# the following convs work on that number of features", "\n", "# if not convolutional upsampling then the final conv reduces the num of features again", "\n", "", "if", "u", "!=", "num_pool", "-", "1", "and", "not", "self", ".", "convolutional_upsampling", ":", "\n", "                ", "final_num_features", "=", "self", ".", "conv_blocks_context", "[", "-", "(", "3", "+", "u", ")", "]", ".", "output_channels", "\n", "", "else", ":", "\n", "                ", "final_num_features", "=", "nfeatures_from_skip", "\n", "\n", "", "if", "not", "self", ".", "convolutional_upsampling", ":", "\n", "                ", "tu", ".", "append", "(", "Upsample", "(", "scale_factor", "=", "self", ".", "pool_op_kernel_sizes", "[", "-", "(", "u", "+", "1", ")", "]", ",", "mode", "=", "self", ".", "upsample_mode", ")", ")", "\n", "", "else", ":", "\n", "                ", "tu", ".", "append", "(", "transpconv", "(", "nfeatures_from_down", ",", "nfeatures_from_skip", ",", "self", ".", "pool_op_kernel_sizes", "[", "-", "(", "u", "+", "1", ")", "]", ",", "\n", "self", ".", "pool_op_kernel_sizes", "[", "-", "(", "u", "+", "1", ")", "]", ",", "bias", "=", "False", ")", ")", "\n", "\n", "", "self", ".", "conv_kwargs", "[", "'kernel_size'", "]", "=", "self", ".", "conv_kernel_sizes", "[", "-", "(", "u", "+", "1", ")", "]", "\n", "self", ".", "conv_kwargs", "[", "'padding'", "]", "=", "self", ".", "conv_pad_sizes", "[", "-", "(", "u", "+", "1", ")", "]", "\n", "conv_blocks_localization", ".", "append", "(", "nn", ".", "Sequential", "(", "\n", "StackedConvLayers", "(", "n_features_after_tu_and_concat", ",", "nfeatures_from_skip", ",", "num_conv_per_stage", "-", "1", ",", "\n", "self", ".", "conv_op", ",", "self", ".", "conv_kwargs", ",", "self", ".", "norm_op", ",", "self", ".", "norm_op_kwargs", ",", "self", ".", "dropout_op", ",", "\n", "self", ".", "dropout_op_kwargs", ",", "self", ".", "nonlin", ",", "self", ".", "nonlin_kwargs", ",", "basic_block", "=", "basic_block", ")", ",", "\n", "StackedConvLayers", "(", "nfeatures_from_skip", ",", "final_num_features", ",", "1", ",", "self", ".", "conv_op", ",", "self", ".", "conv_kwargs", ",", "\n", "self", ".", "norm_op", ",", "self", ".", "norm_op_kwargs", ",", "self", ".", "dropout_op", ",", "self", ".", "dropout_op_kwargs", ",", "\n", "self", ".", "nonlin", ",", "self", ".", "nonlin_kwargs", ",", "basic_block", "=", "basic_block", ")", "\n", ")", ")", "\n", "# print(final_num_features)", "\n", "# print('hello')", "\n", "", "return", "conv_blocks_localization", ",", "tu", ",", "unet_final_features", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.network_architecture.generic_UNetPlusPlus.Generic_UNetPlusPlus.compute_approx_vram_consumption": [[477, 517], ["len", "numpy.array", "numpy.int64", "range", "isinstance", "numpy.array", "range", "min", "len", "numpy.prod", "numpy.prod", "numpy.prod", "numpy.prod", "numpy.prod"], "methods", ["None"], ["", "@", "staticmethod", "\n", "def", "compute_approx_vram_consumption", "(", "patch_size", ",", "num_pool_per_axis", ",", "base_num_features", ",", "max_num_features", ",", "\n", "num_modalities", ",", "num_classes", ",", "pool_op_kernel_sizes", ",", "deep_supervision", "=", "False", ",", "\n", "conv_per_stage", "=", "2", ")", ":", "\n", "        ", "\"\"\"\n        This only applies for num_conv_per_stage and convolutional_upsampling=True\n        not real vram consumption. just a constant term to which the vram consumption will be approx proportional\n        (+ offset for parameter storage)\n        :param deep_supervision:\n        :param patch_size:\n        :param num_pool_per_axis:\n        :param base_num_features:\n        :param max_num_features:\n        :param num_modalities:\n        :param num_classes:\n        :param pool_op_kernel_sizes:\n        :return:\n        \"\"\"", "\n", "if", "not", "isinstance", "(", "num_pool_per_axis", ",", "np", ".", "ndarray", ")", ":", "\n", "            ", "num_pool_per_axis", "=", "np", ".", "array", "(", "num_pool_per_axis", ")", "\n", "\n", "", "npool", "=", "len", "(", "pool_op_kernel_sizes", ")", "\n", "\n", "map_size", "=", "np", ".", "array", "(", "patch_size", ")", "\n", "tmp", "=", "np", ".", "int64", "(", "(", "conv_per_stage", "*", "2", "+", "1", ")", "*", "np", ".", "prod", "(", "map_size", ",", "dtype", "=", "np", ".", "int64", ")", "*", "base_num_features", "+", "\n", "num_modalities", "*", "np", ".", "prod", "(", "map_size", ",", "dtype", "=", "np", ".", "int64", ")", "+", "\n", "num_classes", "*", "np", ".", "prod", "(", "map_size", ",", "dtype", "=", "np", ".", "int64", ")", ")", "\n", "\n", "num_feat", "=", "base_num_features", "\n", "\n", "for", "p", "in", "range", "(", "npool", ")", ":", "\n", "            ", "for", "pi", "in", "range", "(", "len", "(", "num_pool_per_axis", ")", ")", ":", "\n", "                ", "map_size", "[", "pi", "]", "/=", "pool_op_kernel_sizes", "[", "p", "]", "[", "pi", "]", "\n", "", "num_feat", "=", "min", "(", "num_feat", "*", "2", ",", "max_num_features", ")", "\n", "num_blocks", "=", "(", "conv_per_stage", "*", "2", "+", "1", ")", "if", "p", "<", "(", "npool", "-", "1", ")", "else", "conv_per_stage", "# conv_per_stage + conv_per_stage for the convs of encode/decode and 1 for transposed conv", "\n", "tmp", "+=", "num_blocks", "*", "np", ".", "prod", "(", "map_size", ",", "dtype", "=", "np", ".", "int64", ")", "*", "num_feat", "\n", "if", "deep_supervision", "and", "p", "<", "(", "npool", "-", "2", ")", ":", "\n", "                ", "tmp", "+=", "np", ".", "prod", "(", "map_size", ",", "dtype", "=", "np", ".", "int64", ")", "*", "num_classes", "\n", "# print(p, map_size, num_feat, tmp)", "\n", "", "", "return", "tmp", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.network_architecture.generic_UNetPlusPlus.print_module_training_status": [[145, 152], ["isinstance", "isinstance", "isinstance", "isinstance", "isinstance", "isinstance", "isinstance", "isinstance", "isinstance", "isinstance", "isinstance", "print", "str"], "function", ["None"], ["", "", "def", "print_module_training_status", "(", "module", ")", ":", "\n", "    ", "if", "isinstance", "(", "module", ",", "nn", ".", "Conv2d", ")", "or", "isinstance", "(", "module", ",", "nn", ".", "Conv3d", ")", "or", "isinstance", "(", "module", ",", "nn", ".", "Dropout3d", ")", "or", "isinstance", "(", "module", ",", "nn", ".", "Dropout2d", ")", "or", "isinstance", "(", "module", ",", "nn", ".", "Dropout", ")", "or", "isinstance", "(", "module", ",", "nn", ".", "InstanceNorm3d", ")", "or", "isinstance", "(", "module", ",", "nn", ".", "InstanceNorm2d", ")", "or", "isinstance", "(", "module", ",", "nn", ".", "InstanceNorm1d", ")", "or", "isinstance", "(", "module", ",", "nn", ".", "BatchNorm2d", ")", "or", "isinstance", "(", "module", ",", "nn", ".", "BatchNorm3d", ")", "or", "isinstance", "(", "module", ",", "\n", "nn", ".", "BatchNorm1d", ")", ":", "\n", "        ", "print", "(", "str", "(", "module", ")", ",", "module", ".", "training", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.network_architecture.generic_modular_residual_UNet.ResidualUNetEncoder.__init__": [[29, 88], ["torch.nn.Module.__init__", "len", "range", "torch.nn.ModuleList", "len", "len", "isinstance", "min", "nnunet.network_architecture.custom_modules.conv_blocks.ResidualLayer", "generic_modular_residual_UNet.ResidualUNetEncoder.stages.append", "generic_modular_residual_UNet.ResidualUNetEncoder.stage_output_features.append", "generic_modular_residual_UNet.ResidualUNetEncoder.stage_conv_op_kernel_size.append", "generic_modular_residual_UNet.ResidualUNetEncoder.stage_pool_kernel_size.append", "len"], "methods", ["home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.common.layers.ResizeImage.__init__"], ["    ", "def", "__init__", "(", "self", ",", "input_channels", ",", "base_num_features", ",", "num_blocks_per_stage", ",", "feat_map_mul_on_downscale", ",", "\n", "pool_op_kernel_sizes", ",", "conv_kernel_sizes", ",", "props", ",", "default_return_skips", "=", "True", ",", "\n", "max_num_features", "=", "480", ",", "block", "=", "BasicResidualBlock", ")", ":", "\n", "        ", "\"\"\"\n        Following UNet building blocks can be added by utilizing the properties this class exposes (TODO)\n\n        this one includes the bottleneck layer!\n\n        :param input_channels:\n        :param base_num_features:\n        :param num_blocks_per_stage:\n        :param feat_map_mul_on_downscale:\n        :param pool_op_kernel_sizes:\n        :param conv_kernel_sizes:\n        :param props:\n        \"\"\"", "\n", "super", "(", "ResidualUNetEncoder", ",", "self", ")", ".", "__init__", "(", ")", "\n", "\n", "self", ".", "default_return_skips", "=", "default_return_skips", "\n", "self", ".", "props", "=", "props", "\n", "\n", "self", ".", "stages", "=", "[", "]", "\n", "self", ".", "stage_output_features", "=", "[", "]", "\n", "self", ".", "stage_pool_kernel_size", "=", "[", "]", "\n", "self", ".", "stage_conv_op_kernel_size", "=", "[", "]", "\n", "\n", "assert", "len", "(", "pool_op_kernel_sizes", ")", "==", "len", "(", "conv_kernel_sizes", ")", "\n", "\n", "num_stages", "=", "len", "(", "conv_kernel_sizes", ")", "\n", "\n", "if", "not", "isinstance", "(", "num_blocks_per_stage", ",", "(", "list", ",", "tuple", ")", ")", ":", "\n", "            ", "num_blocks_per_stage", "=", "[", "num_blocks_per_stage", "]", "*", "num_stages", "\n", "", "else", ":", "\n", "            ", "assert", "len", "(", "num_blocks_per_stage", ")", "==", "num_stages", "\n", "\n", "", "self", ".", "num_blocks_per_stage", "=", "num_blocks_per_stage", "# decoder may need this", "\n", "\n", "self", ".", "initial_conv", "=", "props", "[", "'conv_op'", "]", "(", "input_channels", ",", "base_num_features", ",", "3", ",", "padding", "=", "1", ",", "**", "props", "[", "'conv_op_kwargs'", "]", ")", "\n", "self", ".", "initial_norm", "=", "props", "[", "'norm_op'", "]", "(", "base_num_features", ",", "**", "props", "[", "'norm_op_kwargs'", "]", ")", "\n", "self", ".", "initial_nonlin", "=", "props", "[", "'nonlin'", "]", "(", "**", "props", "[", "'nonlin_kwargs'", "]", ")", "\n", "\n", "current_input_features", "=", "base_num_features", "\n", "for", "stage", "in", "range", "(", "num_stages", ")", ":", "\n", "            ", "current_output_features", "=", "min", "(", "base_num_features", "*", "feat_map_mul_on_downscale", "**", "stage", ",", "max_num_features", ")", "\n", "current_kernel_size", "=", "conv_kernel_sizes", "[", "stage", "]", "\n", "current_pool_kernel_size", "=", "pool_op_kernel_sizes", "[", "stage", "]", "\n", "\n", "current_stage", "=", "ResidualLayer", "(", "current_input_features", ",", "current_output_features", ",", "current_kernel_size", ",", "props", ",", "\n", "self", ".", "num_blocks_per_stage", "[", "stage", "]", ",", "current_pool_kernel_size", ",", "block", ")", "\n", "\n", "self", ".", "stages", ".", "append", "(", "current_stage", ")", "\n", "self", ".", "stage_output_features", ".", "append", "(", "current_output_features", ")", "\n", "self", ".", "stage_conv_op_kernel_size", ".", "append", "(", "current_kernel_size", ")", "\n", "self", ".", "stage_pool_kernel_size", ".", "append", "(", "current_pool_kernel_size", ")", "\n", "\n", "# update current_input_features", "\n", "current_input_features", "=", "current_output_features", "\n", "\n", "", "self", ".", "stages", "=", "nn", ".", "ModuleList", "(", "self", ".", "stages", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.network_architecture.generic_modular_residual_UNet.ResidualUNetEncoder.forward": [[89, 111], ["generic_modular_residual_UNet.ResidualUNetEncoder.initial_nonlin", "generic_modular_residual_UNet.ResidualUNetEncoder.initial_norm", "s", "generic_modular_residual_UNet.ResidualUNetEncoder.initial_conv", "skips.append"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "x", ",", "return_skips", "=", "None", ")", ":", "\n", "        ", "\"\"\"\n\n        :param x:\n        :param return_skips: if none then self.default_return_skips is used\n        :return:\n        \"\"\"", "\n", "skips", "=", "[", "]", "\n", "\n", "x", "=", "self", ".", "initial_nonlin", "(", "self", ".", "initial_norm", "(", "self", ".", "initial_conv", "(", "x", ")", ")", ")", "\n", "for", "s", "in", "self", ".", "stages", ":", "\n", "            ", "x", "=", "s", "(", "x", ")", "\n", "if", "self", ".", "default_return_skips", ":", "\n", "                ", "skips", ".", "append", "(", "x", ")", "\n", "\n", "", "", "if", "return_skips", "is", "None", ":", "\n", "            ", "return_skips", "=", "self", ".", "default_return_skips", "\n", "\n", "", "if", "return_skips", ":", "\n", "            ", "return", "skips", "\n", "", "else", ":", "\n", "            ", "return", "x", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.network_architecture.generic_modular_residual_UNet.ResidualUNetEncoder.compute_approx_vram_consumption": [[112, 132], ["numpy.array", "range", "len", "min", "print", "numpy.prod", "numpy.array", "numpy.prod", "numpy.prod"], "methods", ["None"], ["", "", "@", "staticmethod", "\n", "def", "compute_approx_vram_consumption", "(", "patch_size", ",", "base_num_features", ",", "max_num_features", ",", "\n", "num_modalities", ",", "pool_op_kernel_sizes", ",", "num_conv_per_stage_encoder", ",", "\n", "feat_map_mul_on_downscale", ",", "batch_size", ")", ":", "\n", "        ", "npool", "=", "len", "(", "pool_op_kernel_sizes", ")", "-", "1", "\n", "\n", "current_shape", "=", "np", ".", "array", "(", "patch_size", ")", "\n", "\n", "tmp", "=", "(", "num_conv_per_stage_encoder", "[", "0", "]", "*", "2", "+", "1", ")", "*", "np", ".", "prod", "(", "current_shape", ")", "*", "base_num_features", "+", "num_modalities", "*", "np", ".", "prod", "(", "current_shape", ")", "\n", "\n", "num_feat", "=", "base_num_features", "\n", "\n", "for", "p", "in", "range", "(", "1", ",", "npool", "+", "1", ")", ":", "\n", "            ", "current_shape", "=", "current_shape", "/", "np", ".", "array", "(", "pool_op_kernel_sizes", "[", "p", "]", ")", "\n", "num_feat", "=", "min", "(", "num_feat", "*", "feat_map_mul_on_downscale", ",", "max_num_features", ")", "\n", "num_convs", "=", "num_conv_per_stage_encoder", "[", "p", "]", "*", "2", "+", "1", "# + 1 for conv in skip in first block", "\n", "print", "(", "p", ",", "num_feat", ",", "num_convs", ",", "current_shape", ")", "\n", "tmp", "+=", "num_convs", "*", "np", ".", "prod", "(", "current_shape", ")", "*", "num_feat", "\n", "", "return", "tmp", "*", "batch_size", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.network_architecture.generic_modular_residual_UNet.ResidualUNetDecoder.__init__": [[135, 204], ["torch.nn.Module.__init__", "numpy.cumprod().astype", "enumerate", "torch.nn.ModuleList", "torch.nn.ModuleList", "torch.nn.ModuleList", "len", "len", "generic_modular_residual_UNet.ResidualUNetDecoder.tus.append", "generic_modular_residual_UNet.ResidualUNetDecoder.stages.append", "ValueError", "len", "numpy.cumprod", "numpy.arange", "transpconv", "nnunet.network_architecture.custom_modules.conv_blocks.ResidualLayer", "numpy.vstack", "nnunet.network_architecture.generic_UNet.Upsample", "generic_modular_residual_UNet.ResidualUNetDecoder.deep_supervision_outputs.append", "generic_modular_residual_UNet.ResidualUNetDecoder.deep_supervision_outputs.append", "str", "torch.nn.Sequential"], "methods", ["home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.common.layers.ResizeImage.__init__"], ["    ", "def", "__init__", "(", "self", ",", "previous", ",", "num_classes", ",", "num_blocks_per_stage", "=", "None", ",", "network_props", "=", "None", ",", "deep_supervision", "=", "False", ",", "\n", "upscale_logits", "=", "False", ",", "block", "=", "BasicResidualBlock", ")", ":", "\n", "        ", "super", "(", "ResidualUNetDecoder", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "num_classes", "=", "num_classes", "\n", "self", ".", "deep_supervision", "=", "deep_supervision", "\n", "\"\"\"\n        We assume the bottleneck is part of the encoder, so we can start with upsample -> concat here\n        \"\"\"", "\n", "previous_stages", "=", "previous", ".", "stages", "\n", "previous_stage_output_features", "=", "previous", ".", "stage_output_features", "\n", "previous_stage_pool_kernel_size", "=", "previous", ".", "stage_pool_kernel_size", "\n", "previous_stage_conv_op_kernel_size", "=", "previous", ".", "stage_conv_op_kernel_size", "\n", "\n", "if", "network_props", "is", "None", ":", "\n", "            ", "self", ".", "props", "=", "previous", ".", "props", "\n", "", "else", ":", "\n", "            ", "self", ".", "props", "=", "network_props", "\n", "\n", "", "if", "self", ".", "props", "[", "'conv_op'", "]", "==", "nn", ".", "Conv2d", ":", "\n", "            ", "transpconv", "=", "nn", ".", "ConvTranspose2d", "\n", "upsample_mode", "=", "\"bilinear\"", "\n", "", "elif", "self", ".", "props", "[", "'conv_op'", "]", "==", "nn", ".", "Conv3d", ":", "\n", "            ", "transpconv", "=", "nn", ".", "ConvTranspose3d", "\n", "upsample_mode", "=", "\"trilinear\"", "\n", "", "else", ":", "\n", "            ", "raise", "ValueError", "(", "\"unknown convolution dimensionality, conv op: %s\"", "%", "str", "(", "self", ".", "props", "[", "'conv_op'", "]", ")", ")", "\n", "\n", "", "if", "num_blocks_per_stage", "is", "None", ":", "\n", "            ", "num_blocks_per_stage", "=", "previous", ".", "num_blocks_per_stage", "[", ":", "-", "1", "]", "[", ":", ":", "-", "1", "]", "\n", "\n", "", "assert", "len", "(", "num_blocks_per_stage", ")", "==", "len", "(", "previous", ".", "num_blocks_per_stage", ")", "-", "1", "\n", "\n", "self", ".", "stage_pool_kernel_size", "=", "previous_stage_pool_kernel_size", "\n", "self", ".", "stage_output_features", "=", "previous_stage_output_features", "\n", "self", ".", "stage_conv_op_kernel_size", "=", "previous_stage_conv_op_kernel_size", "\n", "\n", "num_stages", "=", "len", "(", "previous_stages", ")", "-", "1", "# we have one less as the first stage here is what comes after the", "\n", "# bottleneck", "\n", "\n", "self", ".", "tus", "=", "[", "]", "\n", "self", ".", "stages", "=", "[", "]", "\n", "self", ".", "deep_supervision_outputs", "=", "[", "]", "\n", "\n", "# only used for upsample_logits", "\n", "cum_upsample", "=", "np", ".", "cumprod", "(", "np", ".", "vstack", "(", "self", ".", "stage_pool_kernel_size", ")", ",", "axis", "=", "0", ")", ".", "astype", "(", "int", ")", "\n", "\n", "for", "i", ",", "s", "in", "enumerate", "(", "np", ".", "arange", "(", "num_stages", ")", "[", ":", ":", "-", "1", "]", ")", ":", "\n", "            ", "features_below", "=", "previous_stage_output_features", "[", "s", "+", "1", "]", "\n", "features_skip", "=", "previous_stage_output_features", "[", "s", "]", "\n", "\n", "self", ".", "tus", ".", "append", "(", "transpconv", "(", "features_below", ",", "features_skip", ",", "previous_stage_pool_kernel_size", "[", "s", "+", "1", "]", ",", "\n", "previous_stage_pool_kernel_size", "[", "s", "+", "1", "]", ",", "bias", "=", "False", ")", ")", "\n", "# after we tu we concat features so now we have 2xfeatures_skip", "\n", "self", ".", "stages", ".", "append", "(", "ResidualLayer", "(", "2", "*", "features_skip", ",", "features_skip", ",", "previous_stage_conv_op_kernel_size", "[", "s", "]", ",", "\n", "self", ".", "props", ",", "num_blocks_per_stage", "[", "i", "]", ",", "None", ",", "block", ")", ")", "\n", "\n", "if", "deep_supervision", "and", "s", "!=", "0", ":", "\n", "                ", "seg_layer", "=", "self", ".", "props", "[", "'conv_op'", "]", "(", "features_skip", ",", "num_classes", ",", "1", ",", "1", ",", "0", ",", "1", ",", "1", ",", "False", ")", "\n", "if", "upscale_logits", ":", "\n", "                    ", "upsample", "=", "Upsample", "(", "scale_factor", "=", "cum_upsample", "[", "s", "]", ",", "mode", "=", "upsample_mode", ")", "\n", "self", ".", "deep_supervision_outputs", ".", "append", "(", "nn", ".", "Sequential", "(", "seg_layer", ",", "upsample", ")", ")", "\n", "", "else", ":", "\n", "                    ", "self", ".", "deep_supervision_outputs", ".", "append", "(", "seg_layer", ")", "\n", "\n", "", "", "", "self", ".", "segmentation_output", "=", "self", ".", "props", "[", "'conv_op'", "]", "(", "features_skip", ",", "num_classes", ",", "1", ",", "1", ",", "0", ",", "1", ",", "1", ",", "False", ")", "\n", "\n", "self", ".", "tus", "=", "nn", ".", "ModuleList", "(", "self", ".", "tus", ")", "\n", "self", ".", "stages", "=", "nn", ".", "ModuleList", "(", "self", ".", "stages", ")", "\n", "self", ".", "deep_supervision_outputs", "=", "nn", ".", "ModuleList", "(", "self", ".", "deep_supervision_outputs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.network_architecture.generic_modular_residual_UNet.ResidualUNetDecoder.forward": [[205, 230], ["range", "generic_modular_residual_UNet.ResidualUNetDecoder.segmentation_output", "len", "torch.cat", "seg_outputs.append", "seg_outputs.append", "len"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "skips", ")", ":", "\n", "# skips come from the encoder. They are sorted so that the bottleneck is last in the list", "\n", "# what is maybe not perfect is that the TUs and stages here are sorted the other way around", "\n", "# so let's just reverse the order of skips", "\n", "        ", "skips", "=", "skips", "[", ":", ":", "-", "1", "]", "\n", "seg_outputs", "=", "[", "]", "\n", "\n", "x", "=", "skips", "[", "0", "]", "# this is the bottleneck", "\n", "\n", "for", "i", "in", "range", "(", "len", "(", "self", ".", "tus", ")", ")", ":", "\n", "            ", "x", "=", "self", ".", "tus", "[", "i", "]", "(", "x", ")", "\n", "x", "=", "torch", ".", "cat", "(", "(", "x", ",", "skips", "[", "i", "+", "1", "]", ")", ",", "dim", "=", "1", ")", "\n", "x", "=", "self", ".", "stages", "[", "i", "]", "(", "x", ")", "\n", "if", "self", ".", "deep_supervision", "and", "(", "i", "!=", "len", "(", "self", ".", "tus", ")", "-", "1", ")", ":", "\n", "                ", "seg_outputs", ".", "append", "(", "self", ".", "deep_supervision_outputs", "[", "i", "]", "(", "x", ")", ")", "\n", "\n", "", "", "segmentation", "=", "self", ".", "segmentation_output", "(", "x", ")", "\n", "\n", "if", "self", ".", "deep_supervision", ":", "\n", "            ", "seg_outputs", ".", "append", "(", "segmentation", ")", "\n", "return", "seg_outputs", "[", "\n", ":", ":", "-", "1", "]", "# seg_outputs are ordered so that the seg from the highest layer is first, the seg from", "\n", "# the bottleneck of the UNet last", "\n", "", "else", ":", "\n", "            ", "return", "segmentation", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.network_architecture.generic_modular_residual_UNet.ResidualUNetDecoder.compute_approx_vram_consumption": [[231, 261], ["numpy.array", "range", "len", "min", "print", "numpy.prod", "numpy.array", "numpy.prod", "numpy.prod"], "methods", ["None"], ["", "", "@", "staticmethod", "\n", "def", "compute_approx_vram_consumption", "(", "patch_size", ",", "base_num_features", ",", "max_num_features", ",", "\n", "num_classes", ",", "pool_op_kernel_sizes", ",", "num_blocks_per_stage_decoder", ",", "\n", "feat_map_mul_on_downscale", ",", "batch_size", ")", ":", "\n", "        ", "\"\"\"\n        This only applies for num_conv_per_stage and convolutional_upsampling=True\n        not real vram consumption. just a constant term to which the vram consumption will be approx proportional\n        (+ offset for parameter storage)\n        :param patch_size:\n        :param num_pool_per_axis:\n        :param base_num_features:\n        :param max_num_features:\n        :return:\n        \"\"\"", "\n", "npool", "=", "len", "(", "pool_op_kernel_sizes", ")", "-", "1", "\n", "\n", "current_shape", "=", "np", ".", "array", "(", "patch_size", ")", "\n", "tmp", "=", "(", "num_blocks_per_stage_decoder", "[", "-", "1", "]", "*", "2", "+", "1", ")", "*", "np", ".", "prod", "(", "\n", "current_shape", ")", "*", "base_num_features", "+", "num_classes", "*", "np", ".", "prod", "(", "current_shape", ")", "\n", "\n", "num_feat", "=", "base_num_features", "\n", "\n", "for", "p", "in", "range", "(", "1", ",", "npool", ")", ":", "\n", "            ", "current_shape", "=", "current_shape", "/", "np", ".", "array", "(", "pool_op_kernel_sizes", "[", "p", "]", ")", "\n", "num_feat", "=", "min", "(", "num_feat", "*", "feat_map_mul_on_downscale", ",", "max_num_features", ")", "\n", "num_convs", "=", "num_blocks_per_stage_decoder", "[", "-", "(", "p", "+", "1", ")", "]", "*", "2", "+", "1", "+", "1", "# +1 for transpconv and +1 for conv in skip", "\n", "print", "(", "p", ",", "num_feat", ",", "num_convs", ",", "current_shape", ")", "\n", "tmp", "+=", "num_convs", "*", "np", ".", "prod", "(", "current_shape", ")", "*", "num_feat", "\n", "\n", "", "return", "tmp", "*", "batch_size", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.network_architecture.generic_modular_residual_UNet.ResidualUNet.__init__": [[269, 284], ["nnunet.network_architecture.neural_network.SegmentationNetwork.__init__", "generic_modular_residual_UNet.ResidualUNetEncoder", "generic_modular_residual_UNet.ResidualUNetDecoder", "generic_modular_residual_UNet.ResidualUNet.apply"], "methods", ["home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.common.layers.ResizeImage.__init__"], ["def", "__init__", "(", "self", ",", "input_channels", ",", "base_num_features", ",", "num_blocks_per_stage_encoder", ",", "feat_map_mul_on_downscale", ",", "\n", "pool_op_kernel_sizes", ",", "conv_kernel_sizes", ",", "props", ",", "num_classes", ",", "num_blocks_per_stage_decoder", ",", "\n", "deep_supervision", "=", "False", ",", "upscale_logits", "=", "False", ",", "max_features", "=", "512", ",", "initializer", "=", "None", ",", "\n", "block", "=", "BasicResidualBlock", ")", ":", "\n", "        ", "super", "(", "ResidualUNet", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "conv_op", "=", "props", "[", "'conv_op'", "]", "\n", "self", ".", "num_classes", "=", "num_classes", "\n", "\n", "self", ".", "encoder", "=", "ResidualUNetEncoder", "(", "input_channels", ",", "base_num_features", ",", "num_blocks_per_stage_encoder", ",", "\n", "feat_map_mul_on_downscale", ",", "pool_op_kernel_sizes", ",", "conv_kernel_sizes", ",", "\n", "props", ",", "default_return_skips", "=", "True", ",", "max_num_features", "=", "max_features", ",", "block", "=", "block", ")", "\n", "self", ".", "decoder", "=", "ResidualUNetDecoder", "(", "self", ".", "encoder", ",", "num_classes", ",", "num_blocks_per_stage_decoder", ",", "props", ",", "\n", "deep_supervision", ",", "upscale_logits", ",", "block", "=", "block", ")", "\n", "if", "initializer", "is", "not", "None", ":", "\n", "            ", "self", ".", "apply", "(", "initializer", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.network_architecture.generic_modular_residual_UNet.ResidualUNet.forward": [[285, 288], ["generic_modular_residual_UNet.ResidualUNet.encoder", "generic_modular_residual_UNet.ResidualUNet.decoder"], "methods", ["None"], ["", "", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "skips", "=", "self", ".", "encoder", "(", "x", ")", "\n", "return", "self", ".", "decoder", "(", "skips", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.network_architecture.generic_modular_residual_UNet.ResidualUNet.compute_approx_vram_consumption": [[289, 303], ["generic_modular_residual_UNet.ResidualUNetEncoder.compute_approx_vram_consumption", "generic_modular_residual_UNet.ResidualUNetDecoder.compute_approx_vram_consumption"], "methods", ["home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.network_architecture.generic_modular_UNet.PlainConvUNet.compute_approx_vram_consumption", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.network_architecture.generic_modular_UNet.PlainConvUNet.compute_approx_vram_consumption"], ["", "@", "staticmethod", "\n", "def", "compute_approx_vram_consumption", "(", "patch_size", ",", "base_num_features", ",", "max_num_features", ",", "\n", "num_modalities", ",", "num_classes", ",", "pool_op_kernel_sizes", ",", "num_conv_per_stage_encoder", ",", "\n", "num_conv_per_stage_decoder", ",", "feat_map_mul_on_downscale", ",", "batch_size", ")", ":", "\n", "        ", "enc", "=", "ResidualUNetEncoder", ".", "compute_approx_vram_consumption", "(", "patch_size", ",", "base_num_features", ",", "max_num_features", ",", "\n", "num_modalities", ",", "pool_op_kernel_sizes", ",", "\n", "num_conv_per_stage_encoder", ",", "\n", "feat_map_mul_on_downscale", ",", "batch_size", ")", "\n", "dec", "=", "ResidualUNetDecoder", ".", "compute_approx_vram_consumption", "(", "patch_size", ",", "base_num_features", ",", "max_num_features", ",", "\n", "num_classes", ",", "pool_op_kernel_sizes", ",", "\n", "num_conv_per_stage_decoder", ",", "\n", "feat_map_mul_on_downscale", ",", "batch_size", ")", "\n", "\n", "return", "enc", "+", "dec", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.network_architecture.generic_modular_residual_UNet.FabiansUNet.__init__": [[315, 334], ["nnunet.network_architecture.neural_network.SegmentationNetwork.__init__", "generic_modular_residual_UNet.ResidualUNetEncoder", "nnunet.network_architecture.generic_modular_UNet.PlainConvUNetDecoder", "generic_modular_residual_UNet.FabiansUNet.apply"], "methods", ["home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.common.layers.ResizeImage.__init__"], ["def", "__init__", "(", "self", ",", "input_channels", ",", "base_num_features", ",", "num_blocks_per_stage_encoder", ",", "feat_map_mul_on_downscale", ",", "\n", "pool_op_kernel_sizes", ",", "conv_kernel_sizes", ",", "props", ",", "num_classes", ",", "num_blocks_per_stage_decoder", ",", "\n", "deep_supervision", "=", "False", ",", "upscale_logits", "=", "False", ",", "max_features", "=", "512", ",", "initializer", "=", "None", ",", "\n", "block", "=", "BasicResidualBlock", ",", "\n", "props_decoder", "=", "None", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "conv_op", "=", "props", "[", "'conv_op'", "]", "\n", "self", ".", "num_classes", "=", "num_classes", "\n", "\n", "self", ".", "encoder", "=", "ResidualUNetEncoder", "(", "input_channels", ",", "base_num_features", ",", "num_blocks_per_stage_encoder", ",", "\n", "feat_map_mul_on_downscale", ",", "pool_op_kernel_sizes", ",", "conv_kernel_sizes", ",", "\n", "props", ",", "default_return_skips", "=", "True", ",", "max_num_features", "=", "max_features", ",", "block", "=", "block", ")", "\n", "props", "[", "'dropout_op_kwargs'", "]", "[", "'p'", "]", "=", "0", "\n", "if", "props_decoder", "is", "None", ":", "\n", "            ", "props_decoder", "=", "props", "\n", "", "self", ".", "decoder", "=", "PlainConvUNetDecoder", "(", "self", ".", "encoder", ",", "num_classes", ",", "num_blocks_per_stage_decoder", ",", "props_decoder", ",", "\n", "deep_supervision", ",", "upscale_logits", ")", "\n", "if", "initializer", "is", "not", "None", ":", "\n", "            ", "self", ".", "apply", "(", "initializer", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.network_architecture.generic_modular_residual_UNet.FabiansUNet.forward": [[335, 338], ["generic_modular_residual_UNet.FabiansUNet.encoder", "generic_modular_residual_UNet.FabiansUNet.decoder"], "methods", ["None"], ["", "", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "skips", "=", "self", ".", "encoder", "(", "x", ")", "\n", "return", "self", ".", "decoder", "(", "skips", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.network_architecture.generic_modular_residual_UNet.FabiansUNet.compute_approx_vram_consumption": [[339, 353], ["generic_modular_residual_UNet.ResidualUNetEncoder.compute_approx_vram_consumption", "nnunet.network_architecture.generic_modular_UNet.PlainConvUNetDecoder.compute_approx_vram_consumption"], "methods", ["home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.network_architecture.generic_modular_UNet.PlainConvUNet.compute_approx_vram_consumption", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.network_architecture.generic_modular_UNet.PlainConvUNet.compute_approx_vram_consumption"], ["", "@", "staticmethod", "\n", "def", "compute_approx_vram_consumption", "(", "patch_size", ",", "base_num_features", ",", "max_num_features", ",", "\n", "num_modalities", ",", "num_classes", ",", "pool_op_kernel_sizes", ",", "num_conv_per_stage_encoder", ",", "\n", "num_conv_per_stage_decoder", ",", "feat_map_mul_on_downscale", ",", "batch_size", ")", ":", "\n", "        ", "enc", "=", "ResidualUNetEncoder", ".", "compute_approx_vram_consumption", "(", "patch_size", ",", "base_num_features", ",", "max_num_features", ",", "\n", "num_modalities", ",", "pool_op_kernel_sizes", ",", "\n", "num_conv_per_stage_encoder", ",", "\n", "feat_map_mul_on_downscale", ",", "batch_size", ")", "\n", "dec", "=", "PlainConvUNetDecoder", ".", "compute_approx_vram_consumption", "(", "patch_size", ",", "base_num_features", ",", "max_num_features", ",", "\n", "num_classes", ",", "pool_op_kernel_sizes", ",", "\n", "num_conv_per_stage_decoder", ",", "\n", "feat_map_mul_on_downscale", ",", "batch_size", ")", "\n", "\n", "return", "enc", "+", "dec", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.network_architecture.generic_modular_residual_UNet.find_3d_configuration": [[355, 425], ["FabiansUNet().cuda", "torch.optim.SGD", "nnunet.training.loss_functions.dice_loss.DC_and_CE_loss", "torch.rand().cuda", "range", "print", "FabiansUNet().cuda.parameters", "torch.optim.SGD.zero_grad", "FabiansUNet().cuda.encoder", "print", "FabiansUNet().cuda.decoder", "nnunet.training.loss_functions.dice_loss.DC_and_CE_loss.", "loss.backward", "torch.optim.SGD.step", "generic_modular_residual_UNet.FabiansUNet.compute_approx_vram_consumption", "generic_modular_residual_UNet.FabiansUNet", "torch.rand", "torch.cuda.empty_cache", "nnunet.network_architecture.generic_modular_UNet.get_default_network_config", "len", "len", "len", "len", "torch.rand"], "function", ["home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.utilities.distributed.awesome_allgather_function.backward", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.optimizer.ranger.Ranger.step", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.network_architecture.generic_modular_UNet.PlainConvUNet.compute_approx_vram_consumption", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.network_architecture.generic_modular_UNet.get_default_network_config"], ["", "", "def", "find_3d_configuration", "(", ")", ":", "\n", "# lets compute a reference for 3D", "\n", "# we select hyperparameters here so that we get approximately the same patch size as we would get with the", "\n", "# regular unet. This is just my choice. You can do whatever you want", "\n", "# These default hyperparemeters will then be used by the experiment planner", "\n", "\n", "# since this is more parameter intensive than the UNet, we will test a configuration that has a lot of parameters", "\n", "# herefore we copy the UNet configuration for Task005_Prostate", "\n", "    ", "cudnn", ".", "deterministic", "=", "False", "\n", "cudnn", ".", "benchmark", "=", "True", "\n", "\n", "patch_size", "=", "(", "20", ",", "320", ",", "256", ")", "\n", "max_num_features", "=", "320", "\n", "num_modalities", "=", "2", "\n", "num_classes", "=", "3", "\n", "batch_size", "=", "2", "\n", "\n", "# now we fiddle with the network specific hyperparameters until everything just barely fits into a titanx", "\n", "blocks_per_stage_encoder", "=", "FabiansUNet", ".", "default_blocks_per_stage_encoder", "\n", "blocks_per_stage_decoder", "=", "FabiansUNet", ".", "default_blocks_per_stage_decoder", "\n", "initial_num_features", "=", "32", "\n", "\n", "# we neeed to add a [1, 1, 1] for the res unet because in this implementation all stages of the encoder can have a stride", "\n", "pool_op_kernel_sizes", "=", "[", "[", "1", ",", "1", ",", "1", "]", ",", "\n", "[", "1", ",", "2", ",", "2", "]", ",", "\n", "[", "1", ",", "2", ",", "2", "]", ",", "\n", "[", "2", ",", "2", ",", "2", "]", ",", "\n", "[", "2", ",", "2", ",", "2", "]", ",", "\n", "[", "1", ",", "2", ",", "2", "]", ",", "\n", "[", "1", ",", "2", ",", "2", "]", "]", "\n", "\n", "conv_op_kernel_sizes", "=", "[", "[", "1", ",", "3", ",", "3", "]", ",", "\n", "[", "1", ",", "3", ",", "3", "]", ",", "\n", "[", "3", ",", "3", ",", "3", "]", ",", "\n", "[", "3", ",", "3", ",", "3", "]", ",", "\n", "[", "3", ",", "3", ",", "3", "]", ",", "\n", "[", "3", ",", "3", ",", "3", "]", ",", "\n", "[", "3", ",", "3", ",", "3", "]", "]", "\n", "\n", "unet", "=", "FabiansUNet", "(", "num_modalities", ",", "initial_num_features", ",", "blocks_per_stage_encoder", "[", ":", "len", "(", "conv_op_kernel_sizes", ")", "]", ",", "2", ",", "\n", "pool_op_kernel_sizes", ",", "conv_op_kernel_sizes", ",", "\n", "get_default_network_config", "(", "3", ",", "dropout_p", "=", "None", ")", ",", "num_classes", ",", "\n", "blocks_per_stage_decoder", "[", ":", "len", "(", "conv_op_kernel_sizes", ")", "-", "1", "]", ",", "False", ",", "False", ",", "\n", "max_features", "=", "max_num_features", ")", ".", "cuda", "(", ")", "\n", "\n", "optimizer", "=", "SGD", "(", "unet", ".", "parameters", "(", ")", ",", "lr", "=", "0.1", ",", "momentum", "=", "0.95", ")", "\n", "loss", "=", "DC_and_CE_loss", "(", "{", "'batch_dice'", ":", "True", ",", "'smooth'", ":", "1e-5", ",", "'do_bg'", ":", "False", "}", ",", "{", "}", ")", "\n", "\n", "dummy_input", "=", "torch", ".", "rand", "(", "(", "batch_size", ",", "num_modalities", ",", "*", "patch_size", ")", ")", ".", "cuda", "(", ")", "\n", "dummy_gt", "=", "(", "torch", ".", "rand", "(", "(", "batch_size", ",", "1", ",", "*", "patch_size", ")", ")", "*", "num_classes", ")", ".", "round", "(", ")", ".", "clamp_", "(", "0", ",", "2", ")", ".", "cuda", "(", ")", ".", "long", "(", ")", "\n", "\n", "for", "_", "in", "range", "(", "20", ")", ":", "\n", "        ", "optimizer", ".", "zero_grad", "(", ")", "\n", "skips", "=", "unet", ".", "encoder", "(", "dummy_input", ")", "\n", "print", "(", "[", "i", ".", "shape", "for", "i", "in", "skips", "]", ")", "\n", "output", "=", "unet", ".", "decoder", "(", "skips", ")", "\n", "\n", "l", "=", "loss", "(", "output", ",", "dummy_gt", ")", "\n", "l", ".", "backward", "(", ")", "\n", "\n", "optimizer", ".", "step", "(", ")", "\n", "if", "_", "==", "0", ":", "\n", "            ", "torch", ".", "cuda", ".", "empty_cache", "(", ")", "\n", "\n", "# that should do. Now take the network hyperparameters and insert them in FabiansUNet.compute_approx_vram_consumption", "\n", "# whatever number this spits out, save it to FabiansUNet.use_this_for_batch_size_computation_3D", "\n", "", "", "print", "(", "FabiansUNet", ".", "compute_approx_vram_consumption", "(", "patch_size", ",", "initial_num_features", ",", "max_num_features", ",", "num_modalities", ",", "\n", "num_classes", ",", "pool_op_kernel_sizes", ",", "\n", "blocks_per_stage_encoder", "[", ":", "len", "(", "conv_op_kernel_sizes", ")", "]", ",", "\n", "blocks_per_stage_decoder", "[", ":", "len", "(", "conv_op_kernel_sizes", ")", "-", "1", "]", ",", "2", ",", "batch_size", ")", ")", "\n", "# the output is 1230348800.0 for me", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.network_architecture.generic_modular_residual_UNet.find_2d_configuration": [[429, 501], ["FabiansUNet().cuda", "torch.optim.SGD", "nnunet.training.loss_functions.dice_loss.DC_and_CE_loss", "torch.rand().cuda", "range", "print", "FabiansUNet().cuda.parameters", "torch.optim.SGD.zero_grad", "FabiansUNet().cuda.encoder", "print", "FabiansUNet().cuda.decoder", "nnunet.training.loss_functions.dice_loss.DC_and_CE_loss.", "loss.backward", "torch.optim.SGD.step", "generic_modular_residual_UNet.FabiansUNet.compute_approx_vram_consumption", "generic_modular_residual_UNet.FabiansUNet", "torch.rand", "torch.cuda.empty_cache", "nnunet.network_architecture.generic_modular_UNet.get_default_network_config", "len", "len", "len", "len", "torch.rand"], "function", ["home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.utilities.distributed.awesome_allgather_function.backward", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.optimizer.ranger.Ranger.step", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.network_architecture.generic_modular_UNet.PlainConvUNet.compute_approx_vram_consumption", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.network_architecture.generic_modular_UNet.get_default_network_config"], ["", "def", "find_2d_configuration", "(", ")", ":", "\n", "# lets compute a reference for 3D", "\n", "# we select hyperparameters here so that we get approximately the same patch size as we would get with the", "\n", "# regular unet. This is just my choice. You can do whatever you want", "\n", "# These default hyperparemeters will then be used by the experiment planner", "\n", "\n", "# since this is more parameter intensive than the UNet, we will test a configuration that has a lot of parameters", "\n", "# herefore we copy the UNet configuration for Task003_Liver", "\n", "    ", "cudnn", ".", "deterministic", "=", "False", "\n", "cudnn", ".", "benchmark", "=", "True", "\n", "\n", "patch_size", "=", "(", "512", ",", "512", ")", "\n", "max_num_features", "=", "512", "\n", "num_modalities", "=", "1", "\n", "num_classes", "=", "3", "\n", "batch_size", "=", "12", "\n", "\n", "# now we fiddle with the network specific hyperparameters until everything just barely fits into a titanx", "\n", "blocks_per_stage_encoder", "=", "FabiansUNet", ".", "default_blocks_per_stage_encoder", "\n", "blocks_per_stage_decoder", "=", "FabiansUNet", ".", "default_blocks_per_stage_decoder", "\n", "initial_num_features", "=", "30", "\n", "\n", "# we neeed to add a [1, 1, 1] for the res unet because in this implementation all stages of the encoder can have a stride", "\n", "pool_op_kernel_sizes", "=", "[", "[", "1", ",", "1", "]", ",", "\n", "[", "2", ",", "2", "]", ",", "\n", "[", "2", ",", "2", "]", ",", "\n", "[", "2", ",", "2", "]", ",", "\n", "[", "2", ",", "2", "]", ",", "\n", "[", "2", ",", "2", "]", ",", "\n", "[", "2", ",", "2", "]", ",", "\n", "[", "2", ",", "2", "]", "]", "\n", "\n", "conv_op_kernel_sizes", "=", "[", "[", "3", ",", "3", "]", ",", "\n", "[", "3", ",", "3", "]", ",", "\n", "[", "3", ",", "3", "]", ",", "\n", "[", "3", ",", "3", "]", ",", "\n", "[", "3", ",", "3", "]", ",", "\n", "[", "3", ",", "3", "]", ",", "\n", "[", "3", ",", "3", "]", ",", "\n", "[", "3", ",", "3", "]", "]", "\n", "\n", "unet", "=", "FabiansUNet", "(", "num_modalities", ",", "initial_num_features", ",", "blocks_per_stage_encoder", "[", ":", "len", "(", "conv_op_kernel_sizes", ")", "]", ",", "2", ",", "\n", "pool_op_kernel_sizes", ",", "conv_op_kernel_sizes", ",", "\n", "get_default_network_config", "(", "2", ",", "dropout_p", "=", "None", ")", ",", "num_classes", ",", "\n", "blocks_per_stage_decoder", "[", ":", "len", "(", "conv_op_kernel_sizes", ")", "-", "1", "]", ",", "False", ",", "False", ",", "\n", "max_features", "=", "max_num_features", ")", ".", "cuda", "(", ")", "\n", "\n", "optimizer", "=", "SGD", "(", "unet", ".", "parameters", "(", ")", ",", "lr", "=", "0.1", ",", "momentum", "=", "0.95", ")", "\n", "loss", "=", "DC_and_CE_loss", "(", "{", "'batch_dice'", ":", "True", ",", "'smooth'", ":", "1e-5", ",", "'do_bg'", ":", "False", "}", ",", "{", "}", ")", "\n", "\n", "dummy_input", "=", "torch", ".", "rand", "(", "(", "batch_size", ",", "num_modalities", ",", "*", "patch_size", ")", ")", ".", "cuda", "(", ")", "\n", "dummy_gt", "=", "(", "torch", ".", "rand", "(", "(", "batch_size", ",", "1", ",", "*", "patch_size", ")", ")", "*", "num_classes", ")", ".", "round", "(", ")", ".", "clamp_", "(", "0", ",", "2", ")", ".", "cuda", "(", ")", ".", "long", "(", ")", "\n", "\n", "for", "_", "in", "range", "(", "20", ")", ":", "\n", "        ", "optimizer", ".", "zero_grad", "(", ")", "\n", "skips", "=", "unet", ".", "encoder", "(", "dummy_input", ")", "\n", "print", "(", "[", "i", ".", "shape", "for", "i", "in", "skips", "]", ")", "\n", "output", "=", "unet", ".", "decoder", "(", "skips", ")", "\n", "\n", "l", "=", "loss", "(", "output", ",", "dummy_gt", ")", "\n", "l", ".", "backward", "(", ")", "\n", "\n", "optimizer", ".", "step", "(", ")", "\n", "if", "_", "==", "0", ":", "\n", "            ", "torch", ".", "cuda", ".", "empty_cache", "(", ")", "\n", "\n", "# that should do. Now take the network hyperparameters and insert them in FabiansUNet.compute_approx_vram_consumption", "\n", "# whatever number this spits out, save it to FabiansUNet.use_this_for_batch_size_computation_2D", "\n", "", "", "print", "(", "FabiansUNet", ".", "compute_approx_vram_consumption", "(", "patch_size", ",", "initial_num_features", ",", "max_num_features", ",", "num_modalities", ",", "\n", "num_classes", ",", "pool_op_kernel_sizes", ",", "\n", "blocks_per_stage_encoder", "[", ":", "len", "(", "conv_op_kernel_sizes", ")", "]", ",", "\n", "blocks_per_stage_decoder", "[", ":", "len", "(", "conv_op_kernel_sizes", ")", "-", "1", "]", ",", "2", ",", "batch_size", ")", ")", "\n", "# the output is 1244233728.0 for me", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.network_architecture.generic_hipp_XNet.ConvDropoutNormNonlin.__init__": [[31, 63], ["torch.nn.Module.__init__", "generic_hipp_XNet.ConvDropoutNormNonlin.conv_op", "generic_hipp_XNet.ConvDropoutNormNonlin.norm_op", "generic_hipp_XNet.ConvDropoutNormNonlin.nonlin", "generic_hipp_XNet.ConvDropoutNormNonlin.dropout_op"], "methods", ["home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.common.layers.ResizeImage.__init__"], ["def", "__init__", "(", "self", ",", "input_channels", ",", "output_channels", ",", "\n", "conv_op", "=", "nn", ".", "Conv2d", ",", "conv_kwargs", "=", "None", ",", "\n", "norm_op", "=", "nn", ".", "BatchNorm2d", ",", "norm_op_kwargs", "=", "None", ",", "\n", "dropout_op", "=", "nn", ".", "Dropout2d", ",", "dropout_op_kwargs", "=", "None", ",", "\n", "nonlin", "=", "nn", ".", "LeakyReLU", ",", "nonlin_kwargs", "=", "None", ")", ":", "\n", "        ", "super", "(", "ConvDropoutNormNonlin", ",", "self", ")", ".", "__init__", "(", ")", "\n", "if", "nonlin_kwargs", "is", "None", ":", "\n", "            ", "nonlin_kwargs", "=", "{", "'negative_slope'", ":", "1e-2", ",", "'inplace'", ":", "True", "}", "\n", "", "if", "dropout_op_kwargs", "is", "None", ":", "\n", "            ", "dropout_op_kwargs", "=", "{", "'p'", ":", "0.5", ",", "'inplace'", ":", "True", "}", "\n", "", "if", "norm_op_kwargs", "is", "None", ":", "\n", "            ", "norm_op_kwargs", "=", "{", "'eps'", ":", "1e-5", ",", "'affine'", ":", "True", ",", "'momentum'", ":", "0.1", "}", "\n", "", "if", "conv_kwargs", "is", "None", ":", "\n", "            ", "conv_kwargs", "=", "{", "'kernel_size'", ":", "3", ",", "'stride'", ":", "1", ",", "'padding'", ":", "1", ",", "'dilation'", ":", "1", ",", "'bias'", ":", "True", "}", "\n", "\n", "", "self", ".", "nonlin_kwargs", "=", "nonlin_kwargs", "\n", "self", ".", "nonlin", "=", "nonlin", "\n", "self", ".", "dropout_op", "=", "dropout_op", "\n", "self", ".", "dropout_op_kwargs", "=", "dropout_op_kwargs", "\n", "self", ".", "norm_op_kwargs", "=", "norm_op_kwargs", "\n", "self", ".", "conv_kwargs", "=", "conv_kwargs", "\n", "self", ".", "conv_op", "=", "conv_op", "\n", "self", ".", "norm_op", "=", "norm_op", "\n", "\n", "self", ".", "conv", "=", "self", ".", "conv_op", "(", "input_channels", ",", "output_channels", ",", "**", "self", ".", "conv_kwargs", ")", "\n", "if", "self", ".", "dropout_op", "is", "not", "None", "and", "self", ".", "dropout_op_kwargs", "[", "'p'", "]", "is", "not", "None", "and", "self", ".", "dropout_op_kwargs", "[", "\n", "'p'", "]", ">", "0", ":", "\n", "            ", "self", ".", "dropout", "=", "self", ".", "dropout_op", "(", "**", "self", ".", "dropout_op_kwargs", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "dropout", "=", "None", "\n", "", "self", ".", "instnorm", "=", "self", ".", "norm_op", "(", "output_channels", ",", "**", "self", ".", "norm_op_kwargs", ")", "\n", "self", ".", "lrelu", "=", "self", ".", "nonlin", "(", "**", "self", ".", "nonlin_kwargs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.network_architecture.generic_hipp_XNet.ConvDropoutNormNonlin.forward": [[64, 69], ["generic_hipp_XNet.ConvDropoutNormNonlin.conv", "generic_hipp_XNet.ConvDropoutNormNonlin.lrelu", "generic_hipp_XNet.ConvDropoutNormNonlin.dropout", "generic_hipp_XNet.ConvDropoutNormNonlin.instnorm"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "x", "=", "self", ".", "conv", "(", "x", ")", "\n", "if", "self", ".", "dropout", "is", "not", "None", ":", "\n", "            ", "x", "=", "self", ".", "dropout", "(", "x", ")", "\n", "", "return", "self", ".", "lrelu", "(", "self", ".", "instnorm", "(", "x", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.network_architecture.generic_hipp_XNet.ConvDropoutNonlinNorm.forward": [[72, 77], ["generic_hipp_XNet.ConvDropoutNonlinNorm.conv", "generic_hipp_XNet.ConvDropoutNonlinNorm.instnorm", "generic_hipp_XNet.ConvDropoutNonlinNorm.dropout", "generic_hipp_XNet.ConvDropoutNonlinNorm.lrelu"], "methods", ["None"], ["    ", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "x", "=", "self", ".", "conv", "(", "x", ")", "\n", "if", "self", ".", "dropout", "is", "not", "None", ":", "\n", "            ", "x", "=", "self", ".", "dropout", "(", "x", ")", "\n", "", "return", "self", ".", "instnorm", "(", "self", ".", "lrelu", "(", "x", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.network_architecture.generic_hipp_XNet.StackedConvLayers.__init__": [[80, 140], ["torch.nn.Module.__init__", "torch.nn.Sequential", "torch.nn.Sequential", "copy.deepcopy", "basic_block", "basic_block", "range"], "methods", ["home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.common.layers.ResizeImage.__init__"], ["    ", "def", "__init__", "(", "self", ",", "input_feature_channels", ",", "output_feature_channels", ",", "num_convs", ",", "\n", "conv_op", "=", "nn", ".", "Conv2d", ",", "conv_kwargs", "=", "None", ",", "\n", "norm_op", "=", "nn", ".", "BatchNorm2d", ",", "norm_op_kwargs", "=", "None", ",", "\n", "dropout_op", "=", "nn", ".", "Dropout2d", ",", "dropout_op_kwargs", "=", "None", ",", "\n", "nonlin", "=", "nn", ".", "LeakyReLU", ",", "nonlin_kwargs", "=", "None", ",", "first_stride", "=", "None", ",", "basic_block", "=", "ConvDropoutNormNonlin", ")", ":", "\n", "        ", "'''\n        stacks ConvDropoutNormLReLU layers. initial_stride will only be applied to first layer in the stack. The other parameters affect all layers\n        :param input_feature_channels:\n        :param output_feature_channels:\n        :param num_convs:\n        :param dilation:\n        :param kernel_size:\n        :param padding:\n        :param dropout:\n        :param initial_stride:\n        :param conv_op:\n        :param norm_op:\n        :param dropout_op:\n        :param inplace:\n        :param neg_slope:\n        :param norm_affine:\n        :param conv_bias:\n        '''", "\n", "self", ".", "input_channels", "=", "input_feature_channels", "\n", "self", ".", "output_channels", "=", "output_feature_channels", "\n", "\n", "if", "nonlin_kwargs", "is", "None", ":", "\n", "            ", "nonlin_kwargs", "=", "{", "'negative_slope'", ":", "1e-2", ",", "'inplace'", ":", "True", "}", "\n", "", "if", "dropout_op_kwargs", "is", "None", ":", "\n", "            ", "dropout_op_kwargs", "=", "{", "'p'", ":", "0.5", ",", "'inplace'", ":", "True", "}", "\n", "", "if", "norm_op_kwargs", "is", "None", ":", "\n", "            ", "norm_op_kwargs", "=", "{", "'eps'", ":", "1e-5", ",", "'affine'", ":", "True", ",", "'momentum'", ":", "0.1", "}", "\n", "", "if", "conv_kwargs", "is", "None", ":", "\n", "            ", "conv_kwargs", "=", "{", "'kernel_size'", ":", "3", ",", "'stride'", ":", "1", ",", "'padding'", ":", "1", ",", "'dilation'", ":", "1", ",", "'bias'", ":", "True", "}", "\n", "\n", "", "self", ".", "nonlin_kwargs", "=", "nonlin_kwargs", "\n", "self", ".", "nonlin", "=", "nonlin", "\n", "self", ".", "dropout_op", "=", "dropout_op", "\n", "self", ".", "dropout_op_kwargs", "=", "dropout_op_kwargs", "\n", "self", ".", "norm_op_kwargs", "=", "norm_op_kwargs", "\n", "self", ".", "conv_kwargs", "=", "conv_kwargs", "\n", "self", ".", "conv_op", "=", "conv_op", "\n", "self", ".", "norm_op", "=", "norm_op", "\n", "\n", "if", "first_stride", "is", "not", "None", ":", "\n", "            ", "self", ".", "conv_kwargs_first_conv", "=", "deepcopy", "(", "conv_kwargs", ")", "\n", "self", ".", "conv_kwargs_first_conv", "[", "'stride'", "]", "=", "first_stride", "\n", "", "else", ":", "\n", "            ", "self", ".", "conv_kwargs_first_conv", "=", "conv_kwargs", "\n", "\n", "", "super", "(", "StackedConvLayers", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "blocks", "=", "nn", ".", "Sequential", "(", "\n", "*", "(", "[", "basic_block", "(", "input_feature_channels", ",", "output_feature_channels", ",", "self", ".", "conv_op", ",", "\n", "self", ".", "conv_kwargs_first_conv", ",", "\n", "self", ".", "norm_op", ",", "self", ".", "norm_op_kwargs", ",", "self", ".", "dropout_op", ",", "self", ".", "dropout_op_kwargs", ",", "\n", "self", ".", "nonlin", ",", "self", ".", "nonlin_kwargs", ")", "]", "+", "\n", "[", "basic_block", "(", "output_feature_channels", ",", "output_feature_channels", ",", "self", ".", "conv_op", ",", "\n", "self", ".", "conv_kwargs", ",", "\n", "self", ".", "norm_op", ",", "self", ".", "norm_op_kwargs", ",", "self", ".", "dropout_op", ",", "self", ".", "dropout_op_kwargs", ",", "\n", "self", ".", "nonlin", ",", "self", ".", "nonlin_kwargs", ")", "for", "_", "in", "range", "(", "num_convs", "-", "1", ")", "]", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.network_architecture.generic_hipp_XNet.StackedConvLayers.forward": [[141, 143], ["generic_hipp_XNet.StackedConvLayers.blocks"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "return", "self", ".", "blocks", "(", "x", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.network_architecture.generic_hipp_XNet.Upsample.__init__": [[155, 161], ["torch.nn.Module.__init__"], "methods", ["home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.common.layers.ResizeImage.__init__"], ["    ", "def", "__init__", "(", "self", ",", "size", "=", "None", ",", "scale_factor", "=", "None", ",", "mode", "=", "'nearest'", ",", "align_corners", "=", "False", ")", ":", "\n", "        ", "super", "(", "Upsample", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "align_corners", "=", "align_corners", "\n", "self", ".", "mode", "=", "mode", "\n", "self", ".", "scale_factor", "=", "scale_factor", "\n", "self", ".", "size", "=", "size", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.network_architecture.generic_hipp_XNet.Upsample.forward": [[162, 165], ["torch.nn.functional.interpolate", "torch.nn.functional.interpolate"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "return", "nn", ".", "functional", ".", "interpolate", "(", "x", ",", "size", "=", "self", ".", "size", ",", "scale_factor", "=", "self", ".", "scale_factor", ",", "mode", "=", "self", ".", "mode", ",", "\n", "align_corners", "=", "self", ".", "align_corners", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.network_architecture.generic_hipp_XNet.Generic_XNet.__init__": [[184, 376], ["nnunet.network_architecture.initialization.InitWeights_He", "nnunet.network_architecture.neural_network.SegmentationNetwork.__init__", "numpy.prod", "range", "generic_hipp_XNet.Generic_XNet.conv_blocks_context.append", "generic_hipp_XNet.Generic_XNet.create_nest", "generic_hipp_XNet.Generic_XNet.create_nest", "generic_hipp_XNet.Generic_XNet.create_nest", "generic_hipp_XNet.Generic_XNet.seg_outputs.append", "generic_hipp_XNet.Generic_XNet.seg_outputs.append", "generic_hipp_XNet.Generic_XNet.seg_outputs.append", "range", "torch.nn.ModuleList", "torch.nn.ModuleList", "torch.nn.ModuleList", "torch.nn.ModuleList", "torch.nn.ModuleList", "torch.nn.ModuleList", "torch.nn.ModuleList", "torch.nn.ModuleList", "torch.nn.ModuleList", "torch.nn.ModuleList", "torch.nn.ModuleList", "torch.nn.ModuleList", "torch.nn.ModuleList", "torch.nn.ModuleList", "torch.nn.ModuleList", "torch.nn.ModuleList", "torch.nn.ModuleList", "torch.nn.ModuleList", "generic_hipp_XNet.Generic_XNet.conv_pad_sizes.append", "generic_hipp_XNet.Generic_XNet.conv_blocks_context.append", "int", "min", "torch.nn.Sequential", "torch.nn.Sequential", "conv_op", "conv_op", "conv_op", "numpy.cumprod", "torch.nn.ModuleList", "torch.nn.ModuleList", "generic_hipp_XNet.Generic_XNet.apply", "ValueError", "generic_hipp_XNet.StackedConvLayers", "generic_hipp_XNet.Generic_XNet.td.append", "numpy.round", "generic_hipp_XNet.StackedConvLayers", "generic_hipp_XNet.StackedConvLayers", "numpy.vstack", "generic_hipp_XNet.Generic_XNet.upscale_logits_ops.append", "generic_hipp_XNet.Generic_XNet.upscale_logits_ops.append", "pool_op", "generic_hipp_XNet.Upsample", "str", "tuple", "int"], "methods", ["home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.common.layers.ResizeImage.__init__", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.network_architecture.generic_hipp_XNet.Generic_XNet.create_nest", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.network_architecture.generic_hipp_XNet.Generic_XNet.create_nest", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.network_architecture.generic_hipp_XNet.Generic_XNet.create_nest"], ["def", "__init__", "(", "self", ",", "input_channels", ",", "base_num_features", ",", "num_classes", ",", "num_pool", ",", "num_conv_per_stage", "=", "2", ",", "\n", "feat_map_mul_on_downscale", "=", "2", ",", "conv_op", "=", "nn", ".", "Conv2d", ",", "\n", "norm_op", "=", "nn", ".", "BatchNorm2d", ",", "norm_op_kwargs", "=", "None", ",", "\n", "dropout_op", "=", "nn", ".", "Dropout2d", ",", "dropout_op_kwargs", "=", "None", ",", "\n", "nonlin", "=", "nn", ".", "LeakyReLU", ",", "nonlin_kwargs", "=", "None", ",", "deep_supervision", "=", "True", ",", "dropout_in_localization", "=", "False", ",", "\n", "final_nonlin", "=", "softmax_helper", ",", "weightInitializer", "=", "InitWeights_He", "(", "1e-2", ")", ",", "pool_op_kernel_sizes", "=", "None", ",", "\n", "conv_kernel_sizes", "=", "None", ",", "\n", "upscale_logits", "=", "False", ",", "convolutional_pooling", "=", "False", ",", "convolutional_upsampling", "=", "False", ",", "\n", "max_num_features", "=", "None", ",", "basic_block", "=", "ConvDropoutNormNonlin", ",", "\n", "seg_output_use_bias", "=", "False", ")", ":", "\n", "        ", "\"\"\"\n        basically more flexible than v1, architecture is the same\n\n        Does this look complicated? Nah bro. Functionality > usability\n\n        This does everything you need, including world peace.\n\n        Questions? -> f.isensee@dkfz.de\n        \"\"\"", "\n", "super", "(", "Generic_XNet", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "convolutional_upsampling", "=", "convolutional_upsampling", "\n", "self", ".", "convolutional_pooling", "=", "convolutional_pooling", "\n", "self", ".", "upscale_logits", "=", "upscale_logits", "\n", "if", "nonlin_kwargs", "is", "None", ":", "\n", "            ", "nonlin_kwargs", "=", "{", "'negative_slope'", ":", "1e-2", ",", "'inplace'", ":", "True", "}", "\n", "", "if", "dropout_op_kwargs", "is", "None", ":", "\n", "            ", "dropout_op_kwargs", "=", "{", "'p'", ":", "0.5", ",", "'inplace'", ":", "True", "}", "\n", "", "if", "norm_op_kwargs", "is", "None", ":", "\n", "            ", "norm_op_kwargs", "=", "{", "'eps'", ":", "1e-5", ",", "'affine'", ":", "True", ",", "'momentum'", ":", "0.1", "}", "\n", "\n", "", "self", ".", "conv_kwargs", "=", "{", "'stride'", ":", "1", ",", "'dilation'", ":", "1", ",", "'bias'", ":", "True", "}", "\n", "\n", "self", ".", "nonlin", "=", "nonlin", "\n", "self", ".", "nonlin_kwargs", "=", "nonlin_kwargs", "\n", "self", ".", "dropout_op_kwargs", "=", "dropout_op_kwargs", "\n", "self", ".", "norm_op_kwargs", "=", "norm_op_kwargs", "\n", "self", ".", "weightInitializer", "=", "weightInitializer", "\n", "self", ".", "conv_op", "=", "conv_op", "\n", "self", ".", "norm_op", "=", "norm_op", "\n", "self", ".", "dropout_op", "=", "dropout_op", "\n", "self", ".", "num_classes", "=", "num_classes", "\n", "self", ".", "final_nonlin", "=", "final_nonlin", "\n", "self", ".", "_deep_supervision", "=", "deep_supervision", "\n", "self", ".", "do_ds", "=", "deep_supervision", "\n", "\n", "if", "conv_op", "==", "nn", ".", "Conv2d", ":", "\n", "            ", "upsample_mode", "=", "'bilinear'", "\n", "pool_op", "=", "nn", ".", "MaxPool2d", "\n", "transpconv", "=", "nn", ".", "ConvTranspose2d", "\n", "if", "pool_op_kernel_sizes", "is", "None", ":", "\n", "                ", "pool_op_kernel_sizes", "=", "[", "(", "2", ",", "2", ")", "]", "*", "num_pool", "\n", "", "if", "conv_kernel_sizes", "is", "None", ":", "\n", "                ", "conv_kernel_sizes", "=", "[", "(", "3", ",", "3", ")", "]", "*", "(", "num_pool", "+", "1", ")", "\n", "", "", "elif", "conv_op", "==", "nn", ".", "Conv3d", ":", "\n", "            ", "upsample_mode", "=", "'trilinear'", "\n", "pool_op", "=", "nn", ".", "MaxPool3d", "\n", "transpconv", "=", "nn", ".", "ConvTranspose3d", "\n", "if", "pool_op_kernel_sizes", "is", "None", ":", "\n", "                ", "pool_op_kernel_sizes", "=", "[", "(", "2", ",", "2", ",", "2", ")", "]", "*", "num_pool", "\n", "", "if", "conv_kernel_sizes", "is", "None", ":", "\n", "                ", "conv_kernel_sizes", "=", "[", "(", "3", ",", "3", ",", "3", ")", "]", "*", "(", "num_pool", "+", "1", ")", "\n", "", "", "else", ":", "\n", "            ", "raise", "ValueError", "(", "\"unknown convolution dimensionality, conv op: %s\"", "%", "str", "(", "conv_op", ")", ")", "\n", "\n", "", "self", ".", "input_shape_must_be_divisible_by", "=", "np", ".", "prod", "(", "pool_op_kernel_sizes", ",", "0", ",", "dtype", "=", "np", ".", "int64", ")", "\n", "self", ".", "pool_op_kernel_sizes", "=", "pool_op_kernel_sizes", "\n", "self", ".", "conv_kernel_sizes", "=", "conv_kernel_sizes", "\n", "\n", "self", ".", "conv_pad_sizes", "=", "[", "]", "\n", "for", "krnl", "in", "self", ".", "conv_kernel_sizes", ":", "\n", "            ", "self", ".", "conv_pad_sizes", ".", "append", "(", "[", "1", "if", "i", "==", "3", "else", "0", "for", "i", "in", "krnl", "]", ")", "\n", "\n", "", "if", "max_num_features", "is", "None", ":", "\n", "            ", "if", "self", ".", "conv_op", "==", "nn", ".", "Conv3d", ":", "\n", "                ", "self", ".", "max_num_features", "=", "self", ".", "MAX_NUM_FILTERS_3D", "\n", "", "else", ":", "\n", "                ", "self", ".", "max_num_features", "=", "self", ".", "MAX_FILTERS_2D", "\n", "", "", "else", ":", "\n", "            ", "self", ".", "max_num_features", "=", "max_num_features", "\n", "\n", "", "self", ".", "conv_blocks_context", "=", "[", "]", "\n", "# self.conv_blocks_localization = []", "\n", "self", ".", "loc0", "=", "[", "]", "\n", "self", ".", "loc1", "=", "[", "]", "\n", "self", ".", "loc2", "=", "[", "]", "\n", "self", ".", "td", "=", "[", "]", "\n", "self", ".", "up0", "=", "[", "]", "\n", "self", ".", "up1", "=", "[", "]", "\n", "self", ".", "up2", "=", "[", "]", "\n", "# self.tu = []", "\n", "self", ".", "seg_outputs", "=", "[", "]", "\n", "\n", "output_features", "=", "base_num_features", "\n", "input_features", "=", "input_channels", "\n", "\n", "for", "d", "in", "range", "(", "num_pool", ")", ":", "\n", "# determine the first stride", "\n", "            ", "if", "d", "!=", "0", "and", "self", ".", "convolutional_pooling", ":", "\n", "                ", "first_stride", "=", "pool_op_kernel_sizes", "[", "d", "-", "1", "]", "\n", "", "else", ":", "\n", "                ", "first_stride", "=", "None", "\n", "\n", "", "self", ".", "conv_kwargs", "[", "'kernel_size'", "]", "=", "self", ".", "conv_kernel_sizes", "[", "d", "]", "\n", "self", ".", "conv_kwargs", "[", "'padding'", "]", "=", "self", ".", "conv_pad_sizes", "[", "d", "]", "\n", "# add convolutions", "\n", "self", ".", "conv_blocks_context", ".", "append", "(", "StackedConvLayers", "(", "input_features", ",", "output_features", ",", "num_conv_per_stage", ",", "\n", "self", ".", "conv_op", ",", "self", ".", "conv_kwargs", ",", "self", ".", "norm_op", ",", "\n", "self", ".", "norm_op_kwargs", ",", "self", ".", "dropout_op", ",", "\n", "self", ".", "dropout_op_kwargs", ",", "self", ".", "nonlin", ",", "self", ".", "nonlin_kwargs", ",", "\n", "first_stride", ",", "basic_block", "=", "basic_block", ")", ")", "\n", "if", "not", "self", ".", "convolutional_pooling", ":", "\n", "                ", "self", ".", "td", ".", "append", "(", "pool_op", "(", "pool_op_kernel_sizes", "[", "d", "]", ")", ")", "\n", "", "input_features", "=", "output_features", "\n", "output_features", "=", "int", "(", "np", ".", "round", "(", "output_features", "*", "feat_map_mul_on_downscale", ")", ")", "\n", "\n", "output_features", "=", "min", "(", "output_features", ",", "self", ".", "max_num_features", ")", "\n", "\n", "# now the bottleneck.", "\n", "# determine the first stride", "\n", "", "if", "self", ".", "convolutional_pooling", ":", "\n", "            ", "first_stride", "=", "pool_op_kernel_sizes", "[", "-", "1", "]", "\n", "", "else", ":", "\n", "            ", "first_stride", "=", "None", "\n", "\n", "# the output of the last conv must match the number of features from the skip connection if we are not using", "\n", "# convolutional upsampling. If we use convolutional upsampling then the reduction in feature maps will be", "\n", "# done by the transposed conv", "\n", "", "if", "self", ".", "convolutional_upsampling", ":", "\n", "            ", "final_num_features", "=", "output_features", "\n", "", "else", ":", "\n", "            ", "final_num_features", "=", "self", ".", "conv_blocks_context", "[", "-", "1", "]", ".", "output_channels", "\n", "\n", "", "self", ".", "conv_kwargs", "[", "'kernel_size'", "]", "=", "self", ".", "conv_kernel_sizes", "[", "num_pool", "]", "\n", "self", ".", "conv_kwargs", "[", "'padding'", "]", "=", "self", ".", "conv_pad_sizes", "[", "num_pool", "]", "\n", "self", ".", "conv_blocks_context", ".", "append", "(", "nn", ".", "Sequential", "(", "\n", "StackedConvLayers", "(", "input_features", ",", "output_features", ",", "num_conv_per_stage", "-", "1", ",", "self", ".", "conv_op", ",", "self", ".", "conv_kwargs", ",", "\n", "self", ".", "norm_op", ",", "self", ".", "norm_op_kwargs", ",", "self", ".", "dropout_op", ",", "self", ".", "dropout_op_kwargs", ",", "self", ".", "nonlin", ",", "\n", "self", ".", "nonlin_kwargs", ",", "first_stride", ",", "basic_block", "=", "basic_block", ")", ",", "\n", "StackedConvLayers", "(", "output_features", ",", "final_num_features", ",", "1", ",", "self", ".", "conv_op", ",", "self", ".", "conv_kwargs", ",", "\n", "self", ".", "norm_op", ",", "self", ".", "norm_op_kwargs", ",", "self", ".", "dropout_op", ",", "self", ".", "dropout_op_kwargs", ",", "self", ".", "nonlin", ",", "\n", "self", ".", "nonlin_kwargs", ",", "basic_block", "=", "basic_block", ")", ")", ")", "\n", "\n", "# if we don't want to do dropout in the localization pathway then we set the dropout prob to zero here", "\n", "if", "not", "dropout_in_localization", ":", "\n", "            ", "old_dropout_p", "=", "self", ".", "dropout_op_kwargs", "[", "'p'", "]", "\n", "self", ".", "dropout_op_kwargs", "[", "'p'", "]", "=", "0.0", "\n", "\n", "# now lets build the localization pathway", "\n", "", "encoder_features", "=", "final_num_features", "\n", "self", ".", "loc0", ",", "self", ".", "up0", ",", "encoder_features", "=", "self", ".", "create_nest", "(", "0", ",", "num_pool", ",", "final_num_features", ",", "num_conv_per_stage", ",", "\n", "basic_block", ",", "transpconv", ")", "\n", "self", ".", "loc1", ",", "self", ".", "up1", ",", "encoder_features1", "=", "self", ".", "create_nest", "(", "1", ",", "num_pool", ",", "encoder_features", ",", "num_conv_per_stage", ",", "\n", "basic_block", ",", "transpconv", ")", "\n", "self", ".", "loc2", ",", "self", ".", "up2", ",", "encoder_features2", "=", "self", ".", "create_nest", "(", "2", ",", "num_pool", ",", "encoder_features1", ",", "num_conv_per_stage", ",", "\n", "basic_block", ",", "transpconv", ")", "\n", "\n", "self", ".", "seg_outputs", ".", "append", "(", "conv_op", "(", "self", ".", "loc0", "[", "-", "1", "]", "[", "-", "1", "]", ".", "output_channels", ",", "num_classes", ",", "\n", "1", ",", "1", ",", "0", ",", "1", ",", "1", ",", "seg_output_use_bias", ")", ")", "\n", "self", ".", "seg_outputs", ".", "append", "(", "conv_op", "(", "self", ".", "loc1", "[", "-", "1", "]", "[", "-", "1", "]", ".", "output_channels", ",", "num_classes", ",", "\n", "1", ",", "1", ",", "0", ",", "1", ",", "1", ",", "seg_output_use_bias", ")", ")", "\n", "self", ".", "seg_outputs", ".", "append", "(", "conv_op", "(", "self", ".", "loc2", "[", "-", "1", "]", "[", "-", "1", "]", ".", "output_channels", ",", "num_classes", ",", "\n", "1", ",", "1", ",", "0", ",", "1", ",", "1", ",", "seg_output_use_bias", ")", ")", "\n", "\n", "self", ".", "upscale_logits_ops", "=", "[", "]", "\n", "cum_upsample", "=", "np", ".", "cumprod", "(", "np", ".", "vstack", "(", "pool_op_kernel_sizes", ")", ",", "axis", "=", "0", ")", "[", ":", ":", "-", "1", "]", "\n", "for", "usl", "in", "range", "(", "num_pool", ")", ":", "\n", "            ", "if", "self", ".", "upscale_logits", ":", "\n", "                ", "self", ".", "upscale_logits_ops", ".", "append", "(", "Upsample", "(", "scale_factor", "=", "tuple", "(", "[", "int", "(", "i", ")", "for", "i", "in", "cum_upsample", "[", "usl", "+", "1", "]", "]", ")", ",", "\n", "mode", "=", "upsample_mode", ")", ")", "\n", "", "else", ":", "\n", "                ", "self", ".", "upscale_logits_ops", ".", "append", "(", "lambda", "x", ":", "x", ")", "\n", "\n", "", "", "if", "not", "dropout_in_localization", ":", "\n", "            ", "self", ".", "dropout_op_kwargs", "[", "'p'", "]", "=", "old_dropout_p", "\n", "\n", "# register all modules properly", "\n", "# self.conv_blocks_localization = nn.ModuleList(self.conv_blocks_localization)", "\n", "", "self", ".", "loc0", "=", "nn", ".", "ModuleList", "(", "self", ".", "loc0", ")", "\n", "self", ".", "loc1", "=", "nn", ".", "ModuleList", "(", "self", ".", "loc1", ")", "\n", "self", ".", "loc2", "=", "nn", ".", "ModuleList", "(", "self", ".", "loc2", ")", "\n", "self", ".", "conv_blocks_context", "=", "nn", ".", "ModuleList", "(", "self", ".", "conv_blocks_context", ")", "\n", "self", ".", "td", "=", "nn", ".", "ModuleList", "(", "self", ".", "td", ")", "\n", "self", ".", "up0", "=", "nn", ".", "ModuleList", "(", "self", ".", "up0", ")", "\n", "self", ".", "up1", "=", "nn", ".", "ModuleList", "(", "self", ".", "up1", ")", "\n", "self", ".", "up2", "=", "nn", ".", "ModuleList", "(", "self", ".", "up2", ")", "\n", "self", ".", "seg_outputs", "=", "nn", ".", "ModuleList", "(", "self", ".", "seg_outputs", ")", "\n", "if", "self", ".", "upscale_logits", ":", "\n", "            ", "self", ".", "upscale_logits_ops", "=", "nn", ".", "ModuleList", "(", "\n", "self", ".", "upscale_logits_ops", ")", "# lambda x:x is not a Module so we need to distinguish here", "\n", "\n", "", "if", "self", ".", "weightInitializer", "is", "not", "None", ":", "\n", "            ", "self", ".", "apply", "(", "self", ".", "weightInitializer", ")", "\n", "# self.apply(print_module_training_status)", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.network_architecture.generic_hipp_XNet.Generic_XNet.forward": [[378, 402], ["seg_outputs.append", "seg_outputs.append", "seg_outputs.append", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "generic_hipp_XNet.Generic_XNet.final_nonlin", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "generic_hipp_XNet.Generic_XNet.final_nonlin", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "generic_hipp_XNet.Generic_XNet.final_nonlin", "tuple", "i", "zip", "list"], "methods", ["None"], ["", "", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "# skips = []", "\n", "        ", "seg_outputs", "=", "[", "]", "\n", "x0_0", "=", "self", ".", "conv_blocks_context", "[", "0", "]", "(", "x", ")", "\n", "x1_0", "=", "self", ".", "conv_blocks_context", "[", "1", "]", "(", "x0_0", ")", "\n", "x0_1", "=", "self", ".", "loc2", "[", "0", "]", "(", "torch", ".", "cat", "(", "[", "x0_0", ",", "self", ".", "up2", "[", "0", "]", "(", "x1_0", ")", "]", ",", "1", ")", ")", "\n", "seg_outputs", ".", "append", "(", "self", ".", "final_nonlin", "(", "self", ".", "seg_outputs", "[", "-", "1", "]", "(", "x0_1", ")", ")", ")", "\n", "\n", "x2_0", "=", "self", ".", "conv_blocks_context", "[", "2", "]", "(", "x1_0", ")", "\n", "x1_1", "=", "self", ".", "loc1", "[", "0", "]", "(", "torch", ".", "cat", "(", "[", "x1_0", ",", "self", ".", "up1", "[", "0", "]", "(", "x2_0", ")", "]", ",", "1", ")", ")", "\n", "x0_2", "=", "self", ".", "loc1", "[", "1", "]", "(", "torch", ".", "cat", "(", "[", "x0_0", ",", "x0_1", ",", "self", ".", "up1", "[", "1", "]", "(", "x1_1", ")", "]", ",", "1", ")", ")", "\n", "seg_outputs", ".", "append", "(", "self", ".", "final_nonlin", "(", "self", ".", "seg_outputs", "[", "-", "2", "]", "(", "x0_2", ")", ")", ")", "\n", "\n", "x3_0", "=", "self", ".", "conv_blocks_context", "[", "3", "]", "(", "x2_0", ")", "\n", "x2_1", "=", "self", ".", "loc0", "[", "0", "]", "(", "torch", ".", "cat", "(", "[", "x2_0", ",", "self", ".", "up0", "[", "0", "]", "(", "x3_0", ")", "]", ",", "1", ")", ")", "\n", "x1_2", "=", "self", ".", "loc0", "[", "1", "]", "(", "torch", ".", "cat", "(", "[", "x1_0", ",", "x1_1", ",", "self", ".", "up0", "[", "1", "]", "(", "x2_1", ")", "]", ",", "1", ")", ")", "\n", "x0_3", "=", "self", ".", "loc0", "[", "2", "]", "(", "torch", ".", "cat", "(", "[", "x0_0", ",", "x0_1", ",", "x0_2", ",", "self", ".", "up0", "[", "2", "]", "(", "x1_2", ")", "]", ",", "1", ")", ")", "\n", "seg_outputs", ".", "append", "(", "self", ".", "final_nonlin", "(", "self", ".", "seg_outputs", "[", "-", "3", "]", "(", "x0_3", ")", ")", ")", "\n", "\n", "if", "self", ".", "_deep_supervision", "and", "self", ".", "do_ds", ":", "\n", "            ", "return", "tuple", "(", "[", "seg_outputs", "[", "-", "1", "]", "]", "+", "[", "i", "(", "j", ")", "for", "i", ",", "j", "in", "\n", "zip", "(", "list", "(", "self", ".", "upscale_logits_ops", ")", "[", ":", ":", "-", "1", "]", ",", "seg_outputs", "[", ":", "-", "1", "]", "[", ":", ":", "-", "1", "]", ")", "]", ")", "\n", "", "else", ":", "\n", "            ", "return", "seg_outputs", "[", "-", "1", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.network_architecture.generic_hipp_XNet.Generic_XNet.create_nest": [[404, 445], ["range", "conv_blocks_localization.append", "tu.append", "tu.append", "torch.nn.Sequential", "torch.nn.Sequential", "generic_hipp_XNet.Upsample", "transpconv", "generic_hipp_XNet.StackedConvLayers", "generic_hipp_XNet.StackedConvLayers"], "methods", ["None"], ["", "", "def", "create_nest", "(", "self", ",", "z", ",", "num_pool", ",", "final_num_features", ",", "num_conv_per_stage", ",", "basic_block", ",", "transpconv", ")", ":", "\n", "# print(final_num_features)", "\n", "        ", "conv_blocks_localization", "=", "[", "]", "\n", "tu", "=", "[", "]", "\n", "i", "=", "0", "\n", "# seg_outputs = []", "\n", "for", "u", "in", "range", "(", "z", ",", "num_pool", ")", ":", "\n", "            ", "nfeatures_from_down", "=", "final_num_features", "\n", "nfeatures_from_skip", "=", "self", ".", "conv_blocks_context", "[", "\n", "-", "(", "2", "+", "u", ")", "]", ".", "output_channels", "# self.conv_blocks_context[-1] is bottleneck, so start with -2", "\n", "n_features_after_tu_and_concat", "=", "nfeatures_from_skip", "*", "(", "2", "+", "u", "-", "z", ")", "\n", "if", "i", "==", "0", ":", "\n", "                ", "unet_final_features", "=", "nfeatures_from_skip", "\n", "i", "+=", "1", "\n", "# the first conv reduces the number of features to match those of skip", "\n", "# the following convs work on that number of features", "\n", "# if not convolutional upsampling then the final conv reduces the num of features again", "\n", "", "if", "u", "!=", "num_pool", "-", "1", "and", "not", "self", ".", "convolutional_upsampling", ":", "\n", "                ", "final_num_features", "=", "self", ".", "conv_blocks_context", "[", "-", "(", "3", "+", "u", ")", "]", ".", "output_channels", "\n", "", "else", ":", "\n", "                ", "final_num_features", "=", "nfeatures_from_skip", "\n", "\n", "", "if", "not", "self", ".", "convolutional_upsampling", ":", "\n", "                ", "tu", ".", "append", "(", "Upsample", "(", "scale_factor", "=", "self", ".", "pool_op_kernel_sizes", "[", "-", "(", "u", "+", "1", ")", "]", ",", "mode", "=", "self", ".", "upsample_mode", ")", ")", "\n", "", "else", ":", "\n", "                ", "tu", ".", "append", "(", "transpconv", "(", "nfeatures_from_down", ",", "nfeatures_from_skip", ",", "self", ".", "pool_op_kernel_sizes", "[", "-", "(", "u", "+", "1", ")", "]", ",", "\n", "self", ".", "pool_op_kernel_sizes", "[", "-", "(", "u", "+", "1", ")", "]", ",", "bias", "=", "False", ")", ")", "\n", "\n", "", "self", ".", "conv_kwargs", "[", "'kernel_size'", "]", "=", "self", ".", "conv_kernel_sizes", "[", "-", "(", "u", "+", "1", ")", "]", "\n", "self", ".", "conv_kwargs", "[", "'padding'", "]", "=", "self", ".", "conv_pad_sizes", "[", "-", "(", "u", "+", "1", ")", "]", "\n", "conv_blocks_localization", ".", "append", "(", "nn", ".", "Sequential", "(", "\n", "StackedConvLayers", "(", "n_features_after_tu_and_concat", ",", "nfeatures_from_skip", ",", "num_conv_per_stage", "-", "1", ",", "\n", "self", ".", "conv_op", ",", "self", ".", "conv_kwargs", ",", "self", ".", "norm_op", ",", "self", ".", "norm_op_kwargs", ",", "self", ".", "dropout_op", ",", "\n", "self", ".", "dropout_op_kwargs", ",", "self", ".", "nonlin", ",", "self", ".", "nonlin_kwargs", ",", "basic_block", "=", "basic_block", ")", ",", "\n", "StackedConvLayers", "(", "nfeatures_from_skip", ",", "final_num_features", ",", "1", ",", "self", ".", "conv_op", ",", "self", ".", "conv_kwargs", ",", "\n", "self", ".", "norm_op", ",", "self", ".", "norm_op_kwargs", ",", "self", ".", "dropout_op", ",", "self", ".", "dropout_op_kwargs", ",", "\n", "self", ".", "nonlin", ",", "self", ".", "nonlin_kwargs", ",", "basic_block", "=", "basic_block", ")", "\n", ")", ")", "\n", "# print(final_num_features)", "\n", "# print('hello')", "\n", "", "return", "conv_blocks_localization", ",", "tu", ",", "unet_final_features", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.network_architecture.generic_hipp_XNet.Generic_XNet.compute_approx_vram_consumption": [[446, 486], ["len", "numpy.array", "numpy.int64", "range", "isinstance", "numpy.array", "range", "min", "len", "numpy.prod", "numpy.prod", "numpy.prod", "numpy.prod", "numpy.prod"], "methods", ["None"], ["", "@", "staticmethod", "\n", "def", "compute_approx_vram_consumption", "(", "patch_size", ",", "num_pool_per_axis", ",", "base_num_features", ",", "max_num_features", ",", "\n", "num_modalities", ",", "num_classes", ",", "pool_op_kernel_sizes", ",", "deep_supervision", "=", "False", ",", "\n", "conv_per_stage", "=", "2", ")", ":", "\n", "        ", "\"\"\"\n        This only applies for num_conv_per_stage and convolutional_upsampling=True\n        not real vram consumption. just a constant term to which the vram consumption will be approx proportional\n        (+ offset for parameter storage)\n        :param deep_supervision:\n        :param patch_size:\n        :param num_pool_per_axis:\n        :param base_num_features:\n        :param max_num_features:\n        :param num_modalities:\n        :param num_classes:\n        :param pool_op_kernel_sizes:\n        :return:\n        \"\"\"", "\n", "if", "not", "isinstance", "(", "num_pool_per_axis", ",", "np", ".", "ndarray", ")", ":", "\n", "            ", "num_pool_per_axis", "=", "np", ".", "array", "(", "num_pool_per_axis", ")", "\n", "\n", "", "npool", "=", "len", "(", "pool_op_kernel_sizes", ")", "\n", "\n", "map_size", "=", "np", ".", "array", "(", "patch_size", ")", "\n", "tmp", "=", "np", ".", "int64", "(", "(", "conv_per_stage", "*", "2", "+", "1", ")", "*", "np", ".", "prod", "(", "map_size", ",", "dtype", "=", "np", ".", "int64", ")", "*", "base_num_features", "+", "\n", "num_modalities", "*", "np", ".", "prod", "(", "map_size", ",", "dtype", "=", "np", ".", "int64", ")", "+", "\n", "num_classes", "*", "np", ".", "prod", "(", "map_size", ",", "dtype", "=", "np", ".", "int64", ")", ")", "\n", "\n", "num_feat", "=", "base_num_features", "\n", "\n", "for", "p", "in", "range", "(", "npool", ")", ":", "\n", "            ", "for", "pi", "in", "range", "(", "len", "(", "num_pool_per_axis", ")", ")", ":", "\n", "                ", "map_size", "[", "pi", "]", "/=", "pool_op_kernel_sizes", "[", "p", "]", "[", "pi", "]", "\n", "", "num_feat", "=", "min", "(", "num_feat", "*", "2", ",", "max_num_features", ")", "\n", "num_blocks", "=", "(", "conv_per_stage", "*", "2", "+", "1", ")", "if", "p", "<", "(", "npool", "-", "1", ")", "else", "conv_per_stage", "# conv_per_stage + conv_per_stage for the convs of encode/decode and 1 for transposed conv", "\n", "tmp", "+=", "num_blocks", "*", "np", ".", "prod", "(", "map_size", ",", "dtype", "=", "np", ".", "int64", ")", "*", "num_feat", "\n", "if", "deep_supervision", "and", "p", "<", "(", "npool", "-", "2", ")", ":", "\n", "                ", "tmp", "+=", "np", ".", "prod", "(", "map_size", ",", "dtype", "=", "np", ".", "int64", ")", "*", "num_classes", "\n", "# print(p, map_size, num_feat, tmp)", "\n", "", "", "return", "tmp", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.network_architecture.generic_hipp_XNet.print_module_training_status": [[145, 152], ["isinstance", "isinstance", "isinstance", "isinstance", "isinstance", "isinstance", "isinstance", "isinstance", "isinstance", "isinstance", "isinstance", "print", "str"], "function", ["None"], ["", "", "def", "print_module_training_status", "(", "module", ")", ":", "\n", "    ", "if", "isinstance", "(", "module", ",", "nn", ".", "Conv2d", ")", "or", "isinstance", "(", "module", ",", "nn", ".", "Conv3d", ")", "or", "isinstance", "(", "module", ",", "nn", ".", "Dropout3d", ")", "or", "isinstance", "(", "module", ",", "nn", ".", "Dropout2d", ")", "or", "isinstance", "(", "module", ",", "nn", ".", "Dropout", ")", "or", "isinstance", "(", "module", ",", "nn", ".", "InstanceNorm3d", ")", "or", "isinstance", "(", "module", ",", "nn", ".", "InstanceNorm2d", ")", "or", "isinstance", "(", "module", ",", "nn", ".", "InstanceNorm1d", ")", "or", "isinstance", "(", "module", ",", "nn", ".", "BatchNorm2d", ")", "or", "isinstance", "(", "module", ",", "nn", ".", "BatchNorm3d", ")", "or", "isinstance", "(", "module", ",", "\n", "nn", ".", "BatchNorm1d", ")", ":", "\n", "        ", "print", "(", "str", "(", "module", ")", ",", "module", ".", "training", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.network_architecture.neural_network.NeuralNetwork.__init__": [[29, 31], ["torch.nn.Module.__init__"], "methods", ["home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.common.layers.ResizeImage.__init__"], ["    ", "def", "__init__", "(", "self", ")", ":", "\n", "        ", "super", "(", "NeuralNetwork", ",", "self", ")", ".", "__init__", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.network_architecture.neural_network.NeuralNetwork.get_device": [[32, 37], ["next", "neural_network.NeuralNetwork.parameters", "next", "neural_network.NeuralNetwork.parameters"], "methods", ["None"], ["", "def", "get_device", "(", "self", ")", ":", "\n", "        ", "if", "next", "(", "self", ".", "parameters", "(", ")", ")", ".", "device", "==", "\"cpu\"", ":", "\n", "            ", "return", "\"cpu\"", "\n", "", "else", ":", "\n", "            ", "return", "next", "(", "self", ".", "parameters", "(", ")", ")", ".", "device", ".", "index", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.network_architecture.neural_network.NeuralNetwork.set_device": [[38, 43], ["neural_network.NeuralNetwork.cpu", "neural_network.NeuralNetwork.cuda"], "methods", ["None"], ["", "", "def", "set_device", "(", "self", ",", "device", ")", ":", "\n", "        ", "if", "device", "==", "\"cpu\"", ":", "\n", "            ", "self", ".", "cpu", "(", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "cuda", "(", "device", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.network_architecture.neural_network.NeuralNetwork.forward": [[44, 46], ["None"], "methods", ["None"], ["", "", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "raise", "NotImplementedError", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.network_architecture.neural_network.SegmentationNetwork.__init__": [[49, 72], ["neural_network.NeuralNetwork.__init__"], "methods", ["home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.common.layers.ResizeImage.__init__"], ["    ", "def", "__init__", "(", "self", ")", ":", "\n", "        ", "super", "(", "NeuralNetwork", ",", "self", ")", ".", "__init__", "(", ")", "\n", "\n", "# if we have 5 pooling then our patch size must be divisible by 2**5", "\n", "self", ".", "input_shape_must_be_divisible_by", "=", "None", "# for example in a 2d network that does 5 pool in x and 6 pool", "\n", "# in y this would be (32, 64)", "\n", "\n", "# we need to know this because we need to know if we are a 2d or a 3d netowrk", "\n", "self", ".", "conv_op", "=", "None", "# nn.Conv2d or nn.Conv3d", "\n", "\n", "# this tells us how many channely we have in the output. Important for preallocation in inference", "\n", "self", ".", "num_classes", "=", "None", "# number of channels in the output", "\n", "\n", "# depending on the loss, we do not hard code a nonlinearity into the architecture. To aggregate predictions", "\n", "# during inference, we need to apply the nonlinearity, however. So it is important to let the newtork know what", "\n", "# to apply in inference. For the most part this will be softmax", "\n", "self", ".", "inference_apply_nonlin", "=", "lambda", "x", ":", "x", "# softmax_helper", "\n", "\n", "# This is for saving a gaussian importance map for inference. It weights voxels higher that are closer to the", "\n", "# center. Prediction at the borders are often less accurate and are thus downweighted. Creating these Gaussians", "\n", "# can be expensive, so it makes sense to save and reuse them.", "\n", "self", ".", "_gaussian_3d", "=", "self", ".", "_patch_size_for_gaussian_3d", "=", "None", "\n", "self", ".", "_gaussian_2d", "=", "self", ".", "_patch_size_for_gaussian_2d", "=", "None", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.network_architecture.neural_network.SegmentationNetwork.predict_3D": [[73, 164], ["len", "print", "neural_network.SegmentationNetwork.get_device", "print", "len", "context", "torch.no_grad", "max", "ValueError", "max", "ValueError", "neural_network.SegmentationNetwork._internal_predict_3D_3Dconv_tiled", "neural_network.SegmentationNetwork._internal_predict_3D_3Dconv", "RuntimeError", "neural_network.SegmentationNetwork._internal_predict_3D_2Dconv_tiled", "neural_network.SegmentationNetwork._internal_predict_3D_2Dconv"], "methods", ["home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.network_architecture.neural_network.NeuralNetwork.get_device", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.network_architecture.neural_network.SegmentationNetwork._internal_predict_3D_3Dconv_tiled", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.network_architecture.neural_network.SegmentationNetwork._internal_predict_3D_3Dconv", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.network_architecture.neural_network.SegmentationNetwork._internal_predict_3D_2Dconv_tiled", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.network_architecture.neural_network.SegmentationNetwork._internal_predict_3D_2Dconv"], ["", "def", "predict_3D", "(", "self", ",", "x", ":", "np", ".", "ndarray", ",", "do_mirroring", ":", "bool", ",", "mirror_axes", ":", "Tuple", "[", "int", ",", "...", "]", "=", "(", "0", ",", "1", ",", "2", ")", ",", "\n", "use_sliding_window", ":", "bool", "=", "False", ",", "\n", "step_size", ":", "float", "=", "0.5", ",", "patch_size", ":", "Tuple", "[", "int", ",", "...", "]", "=", "None", ",", "regions_class_order", ":", "Tuple", "[", "int", ",", "...", "]", "=", "None", ",", "\n", "use_gaussian", ":", "bool", "=", "False", ",", "pad_border_mode", ":", "str", "=", "\"constant\"", ",", "\n", "pad_kwargs", ":", "dict", "=", "None", ",", "all_in_gpu", ":", "bool", "=", "False", ",", "\n", "verbose", ":", "bool", "=", "True", ",", "mixed_precision", ":", "bool", "=", "True", ")", "->", "Tuple", "[", "np", ".", "ndarray", ",", "np", ".", "ndarray", "]", ":", "\n", "        ", "\"\"\"\n        Use this function to predict a 3D image. It does not matter whether the network is a 2D or 3D U-Net, it will\n        detect that automatically and run the appropriate code.\n\n        When running predictions, you need to specify whether you want to run fully convolutional of sliding window\n        based inference. We very strongly recommend you use sliding window with the default settings.\n\n        It is the responsibility of the user to make sure the network is in the proper mode (eval for inference!). If\n        the network is not in eval mode it will print a warning.\n\n        :param x: Your input data. Must be a nd.ndarray of shape (c, x, y, z).\n        :param do_mirroring: If True, use test time data augmentation in the form of mirroring\n        :param mirror_axes: Determines which axes to use for mirroing. Per default, mirroring is done along all three\n        axes\n        :param use_sliding_window: if True, run sliding window prediction. Heavily recommended! This is also the default\n        :param step_size: When running sliding window prediction, the step size determines the distance between adjacent\n        predictions. The smaller the step size, the denser the predictions (and the longer it takes!). Step size is given\n        as a fraction of the patch_size. 0.5 is the default and means that wen advance by patch_size * 0.5 between\n        predictions. step_size cannot be larger than 1!\n        :param patch_size: The patch size that was used for training the network. Do not use different patch sizes here,\n        this will either crash or give potentially less accurate segmentations\n        :param regions_class_order: Fabian only\n        :param use_gaussian: (Only applies to sliding window prediction) If True, uses a Gaussian importance weighting\n         to weigh predictions closer to the center of the current patch higher than those at the borders. The reason\n         behind this is that the segmentation accuracy decreases towards the borders. Default (and recommended): True\n        :param pad_border_mode: leave this alone\n        :param pad_kwargs: leave this alone\n        :param all_in_gpu: experimental. You probably want to leave this as is it\n        :param verbose: Do you want a wall of text? If yes then set this to True\n        :param mixed_precision: if True, will run inference in mixed precision with autocast()\n        :return:\n        \"\"\"", "\n", "assert", "step_size", "<=", "1", ",", "'step_size must be smaller than 1. Otherwise there will be a gap between consecutive '", "'predictions'", "\n", "\n", "if", "verbose", ":", "print", "(", "\"debug: mirroring\"", ",", "do_mirroring", ",", "\"mirror_axes\"", ",", "mirror_axes", ")", "\n", "\n", "assert", "self", ".", "get_device", "(", ")", "!=", "\"cpu\"", ",", "\"CPU not implemented\"", "\n", "\n", "if", "pad_kwargs", "is", "None", ":", "\n", "            ", "pad_kwargs", "=", "{", "'constant_values'", ":", "0", "}", "\n", "\n", "# A very long time ago the mirror axes were (2, 3, 4) for a 3d network. This is just to intercept any old", "\n", "# code that uses this convention", "\n", "", "if", "len", "(", "mirror_axes", ")", ":", "\n", "            ", "if", "self", ".", "conv_op", "==", "nn", ".", "Conv2d", ":", "\n", "                ", "if", "max", "(", "mirror_axes", ")", ">", "1", ":", "\n", "                    ", "raise", "ValueError", "(", "\"mirror axes. duh\"", ")", "\n", "", "", "if", "self", ".", "conv_op", "==", "nn", ".", "Conv3d", ":", "\n", "                ", "if", "max", "(", "mirror_axes", ")", ">", "2", ":", "\n", "                    ", "raise", "ValueError", "(", "\"mirror axes. duh\"", ")", "\n", "\n", "", "", "", "if", "self", ".", "training", ":", "\n", "            ", "print", "(", "'WARNING! Network is in train mode during inference. This may be intended, or not...'", ")", "\n", "\n", "", "assert", "len", "(", "x", ".", "shape", ")", "==", "4", ",", "\"data must have shape (c,x,y,z)\"", "\n", "\n", "if", "mixed_precision", ":", "\n", "            ", "context", "=", "autocast", "\n", "", "else", ":", "\n", "            ", "context", "=", "no_op", "\n", "\n", "", "with", "context", "(", ")", ":", "\n", "            ", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "                ", "if", "self", ".", "conv_op", "==", "nn", ".", "Conv3d", ":", "\n", "                    ", "if", "use_sliding_window", ":", "\n", "                        ", "res", "=", "self", ".", "_internal_predict_3D_3Dconv_tiled", "(", "x", ",", "step_size", ",", "do_mirroring", ",", "mirror_axes", ",", "patch_size", ",", "\n", "regions_class_order", ",", "use_gaussian", ",", "pad_border_mode", ",", "\n", "pad_kwargs", "=", "pad_kwargs", ",", "all_in_gpu", "=", "all_in_gpu", ",", "\n", "verbose", "=", "verbose", ")", "\n", "", "else", ":", "\n", "                        ", "res", "=", "self", ".", "_internal_predict_3D_3Dconv", "(", "x", ",", "patch_size", ",", "do_mirroring", ",", "mirror_axes", ",", "regions_class_order", ",", "\n", "pad_border_mode", ",", "pad_kwargs", "=", "pad_kwargs", ",", "verbose", "=", "verbose", ")", "\n", "", "", "elif", "self", ".", "conv_op", "==", "nn", ".", "Conv2d", ":", "\n", "                    ", "if", "use_sliding_window", ":", "\n", "                        ", "res", "=", "self", ".", "_internal_predict_3D_2Dconv_tiled", "(", "x", ",", "patch_size", ",", "do_mirroring", ",", "mirror_axes", ",", "step_size", ",", "\n", "regions_class_order", ",", "use_gaussian", ",", "pad_border_mode", ",", "\n", "pad_kwargs", ",", "all_in_gpu", ",", "False", ")", "\n", "", "else", ":", "\n", "                        ", "res", "=", "self", ".", "_internal_predict_3D_2Dconv", "(", "x", ",", "patch_size", ",", "do_mirroring", ",", "mirror_axes", ",", "regions_class_order", ",", "\n", "pad_border_mode", ",", "pad_kwargs", ",", "all_in_gpu", ",", "False", ")", "\n", "", "", "else", ":", "\n", "                    ", "raise", "RuntimeError", "(", "\"Invalid conv op, cannot determine what dimensionality (2d/3d) the network is\"", ")", "\n", "\n", "", "", "", "return", "res", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.network_architecture.neural_network.SegmentationNetwork.predict_2D": [[165, 244], ["len", "RuntimeError", "print", "neural_network.SegmentationNetwork.get_device", "print", "len", "context", "max", "ValueError", "torch.no_grad", "RuntimeError", "neural_network.SegmentationNetwork._internal_predict_2D_2Dconv_tiled", "neural_network.SegmentationNetwork._internal_predict_2D_2Dconv"], "methods", ["home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.network_architecture.neural_network.NeuralNetwork.get_device", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.network_architecture.neural_network.SegmentationNetwork._internal_predict_2D_2Dconv_tiled", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.network_architecture.neural_network.SegmentationNetwork._internal_predict_2D_2Dconv"], ["", "def", "predict_2D", "(", "self", ",", "x", ",", "do_mirroring", ":", "bool", ",", "mirror_axes", ":", "tuple", "=", "(", "0", ",", "1", ",", "2", ")", ",", "use_sliding_window", ":", "bool", "=", "False", ",", "\n", "step_size", ":", "float", "=", "0.5", ",", "patch_size", ":", "tuple", "=", "None", ",", "regions_class_order", ":", "tuple", "=", "None", ",", "\n", "use_gaussian", ":", "bool", "=", "False", ",", "pad_border_mode", ":", "str", "=", "\"constant\"", ",", "\n", "pad_kwargs", ":", "dict", "=", "None", ",", "all_in_gpu", ":", "bool", "=", "False", ",", "\n", "verbose", ":", "bool", "=", "True", ",", "mixed_precision", ":", "bool", "=", "True", ")", "->", "Tuple", "[", "np", ".", "ndarray", ",", "np", ".", "ndarray", "]", ":", "\n", "        ", "\"\"\"\n        Use this function to predict a 2D image. If this is a 3D U-Net it will crash because you cannot predict a 2D\n        image with that (you dummy).\n\n        When running predictions, you need to specify whether you want to run fully convolutional of sliding window\n        based inference. We very strongly recommend you use sliding window with the default settings.\n\n        It is the responsibility of the user to make sure the network is in the proper mode (eval for inference!). If\n        the network is not in eval mode it will print a warning.\n\n        :param x: Your input data. Must be a nd.ndarray of shape (c, x, y).\n        :param do_mirroring: If True, use test time data augmentation in the form of mirroring\n        :param mirror_axes: Determines which axes to use for mirroing. Per default, mirroring is done along all three\n        axes\n        :param use_sliding_window: if True, run sliding window prediction. Heavily recommended! This is also the default\n        :param step_size: When running sliding window prediction, the step size determines the distance between adjacent\n        predictions. The smaller the step size, the denser the predictions (and the longer it takes!). Step size is given\n        as a fraction of the patch_size. 0.5 is the default and means that wen advance by patch_size * 0.5 between\n        predictions. step_size cannot be larger than 1!\n        :param patch_size: The patch size that was used for training the network. Do not use different patch sizes here,\n        this will either crash or give potentially less accurate segmentations\n        :param regions_class_order: Fabian only\n        :param use_gaussian: (Only applies to sliding window prediction) If True, uses a Gaussian importance weighting\n         to weigh predictions closer to the center of the current patch higher than those at the borders. The reason\n         behind this is that the segmentation accuracy decreases towards the borders. Default (and recommended): True\n        :param pad_border_mode: leave this alone\n        :param pad_kwargs: leave this alone\n        :param all_in_gpu: experimental. You probably want to leave this as is it\n        :param verbose: Do you want a wall of text? If yes then set this to True\n        :return:\n        \"\"\"", "\n", "assert", "step_size", "<=", "1", ",", "'step_size must be smaler than 1. Otherwise there will be a gap between consecutive '", "'predictions'", "\n", "\n", "if", "self", ".", "conv_op", "==", "nn", ".", "Conv3d", ":", "\n", "            ", "raise", "RuntimeError", "(", "\"Cannot predict 2d if the network is 3d. Dummy.\"", ")", "\n", "\n", "", "if", "verbose", ":", "print", "(", "\"debug: mirroring\"", ",", "do_mirroring", ",", "\"mirror_axes\"", ",", "mirror_axes", ")", "\n", "\n", "assert", "self", ".", "get_device", "(", ")", "!=", "\"cpu\"", ",", "\"CPU not implemented\"", "\n", "\n", "if", "pad_kwargs", "is", "None", ":", "\n", "            ", "pad_kwargs", "=", "{", "'constant_values'", ":", "0", "}", "\n", "\n", "# A very long time ago the mirror axes were (2, 3) for a 2d network. This is just to intercept any old", "\n", "# code that uses this convention", "\n", "", "if", "len", "(", "mirror_axes", ")", ":", "\n", "            ", "if", "max", "(", "mirror_axes", ")", ">", "1", ":", "\n", "                ", "raise", "ValueError", "(", "\"mirror axes. duh\"", ")", "\n", "\n", "", "", "if", "self", ".", "training", ":", "\n", "            ", "print", "(", "'WARNING! Network is in train mode during inference. This may be intended, or not...'", ")", "\n", "\n", "", "assert", "len", "(", "x", ".", "shape", ")", "==", "3", ",", "\"data must have shape (c,x,y)\"", "\n", "\n", "if", "mixed_precision", ":", "\n", "            ", "context", "=", "autocast", "\n", "", "else", ":", "\n", "            ", "context", "=", "no_op", "\n", "\n", "", "with", "context", "(", ")", ":", "\n", "            ", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "                ", "if", "self", ".", "conv_op", "==", "nn", ".", "Conv2d", ":", "\n", "                    ", "if", "use_sliding_window", ":", "\n", "                        ", "res", "=", "self", ".", "_internal_predict_2D_2Dconv_tiled", "(", "x", ",", "step_size", ",", "do_mirroring", ",", "mirror_axes", ",", "patch_size", ",", "\n", "regions_class_order", ",", "use_gaussian", ",", "pad_border_mode", ",", "\n", "pad_kwargs", ",", "all_in_gpu", ",", "verbose", ")", "\n", "", "else", ":", "\n", "                        ", "res", "=", "self", ".", "_internal_predict_2D_2Dconv", "(", "x", ",", "patch_size", ",", "do_mirroring", ",", "mirror_axes", ",", "regions_class_order", ",", "\n", "pad_border_mode", ",", "pad_kwargs", ",", "verbose", ")", "\n", "", "", "else", ":", "\n", "                    ", "raise", "RuntimeError", "(", "\"Invalid conv op, cannot determine what dimensionality (2d/3d) the network is\"", ")", "\n", "\n", "", "", "", "return", "res", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.network_architecture.neural_network.SegmentationNetwork._get_gaussian": [[245, 260], ["numpy.zeros", "scipy.ndimage.filters.gaussian_filter", "gaussian_importance_map.astype.astype.astype", "numpy.min", "tuple", "numpy.max"], "methods", ["None"], ["", "@", "staticmethod", "\n", "def", "_get_gaussian", "(", "patch_size", ",", "sigma_scale", "=", "1.", "/", "8", ")", "->", "np", ".", "ndarray", ":", "\n", "        ", "tmp", "=", "np", ".", "zeros", "(", "patch_size", ")", "\n", "center_coords", "=", "[", "i", "//", "2", "for", "i", "in", "patch_size", "]", "\n", "sigmas", "=", "[", "i", "*", "sigma_scale", "for", "i", "in", "patch_size", "]", "\n", "tmp", "[", "tuple", "(", "center_coords", ")", "]", "=", "1", "\n", "gaussian_importance_map", "=", "gaussian_filter", "(", "tmp", ",", "sigmas", ",", "0", ",", "mode", "=", "'constant'", ",", "cval", "=", "0", ")", "\n", "gaussian_importance_map", "=", "gaussian_importance_map", "/", "np", ".", "max", "(", "gaussian_importance_map", ")", "*", "1", "\n", "gaussian_importance_map", "=", "gaussian_importance_map", ".", "astype", "(", "np", ".", "float32", ")", "\n", "\n", "# gaussian_importance_map cannot be 0, otherwise we may end up with nans!", "\n", "gaussian_importance_map", "[", "gaussian_importance_map", "==", "0", "]", "=", "np", ".", "min", "(", "\n", "gaussian_importance_map", "[", "gaussian_importance_map", "!=", "0", "]", ")", "\n", "\n", "return", "gaussian_importance_map", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.network_architecture.neural_network.SegmentationNetwork._compute_steps_for_sliding_window": [[261, 286], ["range", "len", "steps.append", "zip", "int", "zip", "int", "numpy.ceil", "numpy.round", "range"], "methods", ["None"], ["", "@", "staticmethod", "\n", "def", "_compute_steps_for_sliding_window", "(", "patch_size", ":", "Tuple", "[", "int", ",", "...", "]", ",", "image_size", ":", "Tuple", "[", "int", ",", "...", "]", ",", "step_size", ":", "float", ")", "->", "List", "[", "List", "[", "int", "]", "]", ":", "\n", "        ", "assert", "[", "i", ">=", "j", "for", "i", ",", "j", "in", "zip", "(", "image_size", ",", "patch_size", ")", "]", ",", "\"image size must be as large or larger than patch_size\"", "\n", "assert", "0", "<", "step_size", "<=", "1", ",", "'step_size must be larger than 0 and smaller or equal to 1'", "\n", "\n", "# our step width is patch_size*step_size at most, but can be narrower. For example if we have image size of", "\n", "# 110, patch size of 32 and step_size of 0.5, then we want to make 4 steps starting at coordinate 0, 27, 55, 78", "\n", "target_step_sizes_in_voxels", "=", "[", "i", "*", "step_size", "for", "i", "in", "patch_size", "]", "\n", "\n", "num_steps", "=", "[", "int", "(", "np", ".", "ceil", "(", "(", "i", "-", "k", ")", "/", "j", ")", ")", "+", "1", "for", "i", ",", "j", ",", "k", "in", "zip", "(", "image_size", ",", "target_step_sizes_in_voxels", ",", "patch_size", ")", "]", "\n", "\n", "steps", "=", "[", "]", "\n", "for", "dim", "in", "range", "(", "len", "(", "patch_size", ")", ")", ":", "\n", "# the highest step value for this dimension is", "\n", "            ", "max_step_value", "=", "image_size", "[", "dim", "]", "-", "patch_size", "[", "dim", "]", "\n", "if", "num_steps", "[", "dim", "]", ">", "1", ":", "\n", "                ", "actual_step_size", "=", "max_step_value", "/", "(", "num_steps", "[", "dim", "]", "-", "1", ")", "\n", "", "else", ":", "\n", "                ", "actual_step_size", "=", "99999999999", "# does not matter because there is only one step at 0", "\n", "\n", "", "steps_here", "=", "[", "int", "(", "np", ".", "round", "(", "actual_step_size", "*", "i", ")", ")", "for", "i", "in", "range", "(", "num_steps", "[", "dim", "]", ")", "]", "\n", "\n", "steps", ".", "append", "(", "steps_here", ")", "\n", "\n", "", "return", "steps", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.network_architecture.neural_network.SegmentationNetwork._internal_predict_3D_3Dconv_tiled": [[287, 424], ["torch.cuda.empty_cache", "batchgenerators.augmentations.utils.pad_nd_image", "neural_network.SegmentationNetwork._compute_steps_for_sliding_window", "tuple", "len", "neural_network.SegmentationNetwork.get_device", "print", "print", "len", "print", "print", "print", "print", "torch.from_numpy().cuda", "torch.zeros", "torch.from_numpy().cuda", "torch.zeros", "numpy.zeros", "numpy.zeros", "class_probabilities.detach().cpu().numpy.detach().cpu().numpy.argmax", "numpy.zeros", "enumerate", "class_probabilities.detach().cpu().numpy.detach().cpu().numpy.detach().cpu().numpy", "print", "len", "len", "neural_network.SegmentationNetwork._get_gaussian", "neural_network.SegmentationNetwork.get_device", "gaussian_importance_map.half.half.half", "gaussian_importance_map[].min", "torch.ones", "print", "print", "neural_network.SegmentationNetwork.get_device", "print", "numpy.ones", "class_probabilities.detach().cpu().numpy.detach().cpu().numpy.detach().cpu().numpy", "print", "predicted_segmentation.detach().cpu().numpy.detach().cpu().numpy.detach().cpu().numpy", "all", "print", "print", "torch.from_numpy", "list", "neural_network.SegmentationNetwork.get_device", "torch.from_numpy", "list", "neural_network.SegmentationNetwork.get_device", "list", "list", "slice", "class_probabilities.detach().cpu().numpy.detach().cpu().numpy.detach().cpu", "neural_network.SegmentationNetwork.get_device", "neural_network.SegmentationNetwork._internal_maybe_mirror_and_pred_3D", "predicted_patch.cpu().numpy.cpu().numpy.half", "predicted_patch.cpu().numpy.cpu().numpy.cpu().numpy", "range", "class_probabilities.detach().cpu().numpy.detach().cpu().numpy.detach().cpu", "predicted_segmentation.detach().cpu().numpy.detach().cpu().numpy.detach().cpu", "class_probabilities.detach().cpu().numpy.detach().cpu().numpy.detach", "zip", "predicted_patch.cpu().numpy.cpu().numpy.cpu", "len", "class_probabilities.detach().cpu().numpy.detach().cpu().numpy.detach", "predicted_segmentation.detach().cpu().numpy.detach().cpu().numpy.detach", "len"], "methods", ["home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.network_architecture.neural_network.SegmentationNetwork._compute_steps_for_sliding_window", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.network_architecture.neural_network.NeuralNetwork.get_device", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.network_architecture.neural_network.SegmentationNetwork._get_gaussian", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.network_architecture.neural_network.NeuralNetwork.get_device", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.network_architecture.neural_network.NeuralNetwork.get_device", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.network_architecture.neural_network.NeuralNetwork.get_device", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.network_architecture.neural_network.NeuralNetwork.get_device", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.network_architecture.neural_network.NeuralNetwork.get_device", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.network_architecture.neural_network.SegmentationNetwork._internal_maybe_mirror_and_pred_3D"], ["", "def", "_internal_predict_3D_3Dconv_tiled", "(", "self", ",", "x", ":", "np", ".", "ndarray", ",", "step_size", ":", "float", ",", "do_mirroring", ":", "bool", ",", "mirror_axes", ":", "tuple", ",", "\n", "patch_size", ":", "tuple", ",", "regions_class_order", ":", "tuple", ",", "use_gaussian", ":", "bool", ",", "\n", "pad_border_mode", ":", "str", ",", "pad_kwargs", ":", "dict", ",", "all_in_gpu", ":", "bool", ",", "\n", "verbose", ":", "bool", ")", "->", "Tuple", "[", "np", ".", "ndarray", ",", "np", ".", "ndarray", "]", ":", "\n", "# better safe than sorry", "\n", "        ", "assert", "len", "(", "x", ".", "shape", ")", "==", "4", ",", "\"x must be (c, x, y, z)\"", "\n", "assert", "self", ".", "get_device", "(", ")", "!=", "\"cpu\"", "\n", "if", "verbose", ":", "print", "(", "\"step_size:\"", ",", "step_size", ")", "\n", "if", "verbose", ":", "print", "(", "\"do mirror:\"", ",", "do_mirroring", ")", "\n", "\n", "torch", ".", "cuda", ".", "empty_cache", "(", ")", "\n", "\n", "assert", "patch_size", "is", "not", "None", ",", "\"patch_size cannot be None for tiled prediction\"", "\n", "\n", "# for sliding window inference the image must at least be as large as the patch size. It does not matter", "\n", "# whether the shape is divisible by 2**num_pool as long as the patch size is", "\n", "data", ",", "slicer", "=", "pad_nd_image", "(", "x", ",", "patch_size", ",", "pad_border_mode", ",", "pad_kwargs", ",", "True", ",", "None", ")", "\n", "data_shape", "=", "data", ".", "shape", "# still c, x, y, z", "\n", "\n", "# compute the steps for sliding window", "\n", "steps", "=", "self", ".", "_compute_steps_for_sliding_window", "(", "patch_size", ",", "data_shape", "[", "1", ":", "]", ",", "step_size", ")", "\n", "num_tiles", "=", "len", "(", "steps", "[", "0", "]", ")", "*", "len", "(", "steps", "[", "1", "]", ")", "*", "len", "(", "steps", "[", "2", "]", ")", "\n", "\n", "if", "verbose", ":", "\n", "            ", "print", "(", "\"data shape:\"", ",", "data_shape", ")", "\n", "print", "(", "\"patch size:\"", ",", "patch_size", ")", "\n", "print", "(", "\"steps (x, y, and z):\"", ",", "steps", ")", "\n", "print", "(", "\"number of tiles:\"", ",", "num_tiles", ")", "\n", "\n", "# we only need to compute that once. It can take a while to compute this due to the large sigma in", "\n", "# gaussian_filter", "\n", "", "if", "use_gaussian", "and", "num_tiles", ">", "1", ":", "\n", "            ", "if", "self", ".", "_gaussian_3d", "is", "None", "or", "not", "all", "(", "\n", "[", "i", "==", "j", "for", "i", ",", "j", "in", "zip", "(", "patch_size", ",", "self", ".", "_patch_size_for_gaussian_3d", ")", "]", ")", ":", "\n", "                ", "if", "verbose", ":", "print", "(", "'computing Gaussian'", ")", "\n", "gaussian_importance_map", "=", "self", ".", "_get_gaussian", "(", "patch_size", ",", "sigma_scale", "=", "1.", "/", "8", ")", "\n", "\n", "self", ".", "_gaussian_3d", "=", "gaussian_importance_map", "\n", "self", ".", "_patch_size_for_gaussian_3d", "=", "patch_size", "\n", "", "else", ":", "\n", "                ", "if", "verbose", ":", "print", "(", "\"using precomputed Gaussian\"", ")", "\n", "gaussian_importance_map", "=", "self", ".", "_gaussian_3d", "\n", "\n", "", "gaussian_importance_map", "=", "torch", ".", "from_numpy", "(", "gaussian_importance_map", ")", ".", "cuda", "(", "self", ".", "get_device", "(", ")", ",", "\n", "non_blocking", "=", "True", ")", "\n", "\n", "", "else", ":", "\n", "            ", "gaussian_importance_map", "=", "None", "\n", "\n", "", "if", "all_in_gpu", ":", "\n", "# If we run the inference in GPU only (meaning all tensors are allocated on the GPU, this reduces", "\n", "# CPU-GPU communication but required more GPU memory) we need to preallocate a few things on GPU", "\n", "\n", "            ", "if", "use_gaussian", "and", "num_tiles", ">", "1", ":", "\n", "# half precision for the outputs should be good enough. If the outputs here are half, the", "\n", "# gaussian_importance_map should be as well", "\n", "                ", "gaussian_importance_map", "=", "gaussian_importance_map", ".", "half", "(", ")", "\n", "\n", "# make sure we did not round anything to 0", "\n", "gaussian_importance_map", "[", "gaussian_importance_map", "==", "0", "]", "=", "gaussian_importance_map", "[", "\n", "gaussian_importance_map", "!=", "0", "]", ".", "min", "(", ")", "\n", "\n", "add_for_nb_of_preds", "=", "gaussian_importance_map", "\n", "", "else", ":", "\n", "                ", "add_for_nb_of_preds", "=", "torch", ".", "ones", "(", "data", ".", "shape", "[", "1", ":", "]", ",", "device", "=", "self", ".", "get_device", "(", ")", ")", "\n", "\n", "", "if", "verbose", ":", "print", "(", "\"initializing result array (on GPU)\"", ")", "\n", "aggregated_results", "=", "torch", ".", "zeros", "(", "[", "self", ".", "num_classes", "]", "+", "list", "(", "data", ".", "shape", "[", "1", ":", "]", ")", ",", "dtype", "=", "torch", ".", "half", ",", "\n", "device", "=", "self", ".", "get_device", "(", ")", ")", "\n", "\n", "if", "verbose", ":", "print", "(", "\"moving data to GPU\"", ")", "\n", "data", "=", "torch", ".", "from_numpy", "(", "data", ")", ".", "cuda", "(", "self", ".", "get_device", "(", ")", ",", "non_blocking", "=", "True", ")", "\n", "\n", "if", "verbose", ":", "print", "(", "\"initializing result_numsamples (on GPU)\"", ")", "\n", "aggregated_nb_of_predictions", "=", "torch", ".", "zeros", "(", "[", "self", ".", "num_classes", "]", "+", "list", "(", "data", ".", "shape", "[", "1", ":", "]", ")", ",", "dtype", "=", "torch", ".", "half", ",", "\n", "device", "=", "self", ".", "get_device", "(", ")", ")", "\n", "", "else", ":", "\n", "            ", "if", "use_gaussian", "and", "num_tiles", ">", "1", ":", "\n", "                ", "add_for_nb_of_preds", "=", "self", ".", "_gaussian_3d", "\n", "", "else", ":", "\n", "                ", "add_for_nb_of_preds", "=", "np", ".", "ones", "(", "data", ".", "shape", "[", "1", ":", "]", ",", "dtype", "=", "np", ".", "float32", ")", "\n", "", "aggregated_results", "=", "np", ".", "zeros", "(", "[", "self", ".", "num_classes", "]", "+", "list", "(", "data", ".", "shape", "[", "1", ":", "]", ")", ",", "dtype", "=", "np", ".", "float32", ")", "\n", "aggregated_nb_of_predictions", "=", "np", ".", "zeros", "(", "[", "self", ".", "num_classes", "]", "+", "list", "(", "data", ".", "shape", "[", "1", ":", "]", ")", ",", "dtype", "=", "np", ".", "float32", ")", "\n", "\n", "", "for", "x", "in", "steps", "[", "0", "]", ":", "\n", "            ", "lb_x", "=", "x", "\n", "ub_x", "=", "x", "+", "patch_size", "[", "0", "]", "\n", "for", "y", "in", "steps", "[", "1", "]", ":", "\n", "                ", "lb_y", "=", "y", "\n", "ub_y", "=", "y", "+", "patch_size", "[", "1", "]", "\n", "for", "z", "in", "steps", "[", "2", "]", ":", "\n", "                    ", "lb_z", "=", "z", "\n", "ub_z", "=", "z", "+", "patch_size", "[", "2", "]", "\n", "\n", "predicted_patch", "=", "self", ".", "_internal_maybe_mirror_and_pred_3D", "(", "\n", "data", "[", "None", ",", ":", ",", "lb_x", ":", "ub_x", ",", "lb_y", ":", "ub_y", ",", "lb_z", ":", "ub_z", "]", ",", "mirror_axes", ",", "do_mirroring", ",", "\n", "gaussian_importance_map", ")", "[", "0", "]", "\n", "\n", "if", "all_in_gpu", ":", "\n", "                        ", "predicted_patch", "=", "predicted_patch", ".", "half", "(", ")", "\n", "", "else", ":", "\n", "                        ", "predicted_patch", "=", "predicted_patch", ".", "cpu", "(", ")", ".", "numpy", "(", ")", "\n", "\n", "", "aggregated_results", "[", ":", ",", "lb_x", ":", "ub_x", ",", "lb_y", ":", "ub_y", ",", "lb_z", ":", "ub_z", "]", "+=", "predicted_patch", "\n", "aggregated_nb_of_predictions", "[", ":", ",", "lb_x", ":", "ub_x", ",", "lb_y", ":", "ub_y", ",", "lb_z", ":", "ub_z", "]", "+=", "add_for_nb_of_preds", "\n", "\n", "# we reverse the padding here (remeber that we padded the input to be at least as large as the patch size", "\n", "", "", "", "slicer", "=", "tuple", "(", "\n", "[", "slice", "(", "0", ",", "aggregated_results", ".", "shape", "[", "i", "]", ")", "for", "i", "in", "\n", "range", "(", "len", "(", "aggregated_results", ".", "shape", ")", "-", "(", "len", "(", "slicer", ")", "-", "1", ")", ")", "]", "+", "slicer", "[", "1", ":", "]", ")", "\n", "aggregated_results", "=", "aggregated_results", "[", "slicer", "]", "\n", "aggregated_nb_of_predictions", "=", "aggregated_nb_of_predictions", "[", "slicer", "]", "\n", "\n", "# computing the class_probabilities by dividing the aggregated result with result_numsamples", "\n", "class_probabilities", "=", "aggregated_results", "/", "aggregated_nb_of_predictions", "\n", "\n", "if", "regions_class_order", "is", "None", ":", "\n", "            ", "predicted_segmentation", "=", "class_probabilities", ".", "argmax", "(", "0", ")", "\n", "", "else", ":", "\n", "            ", "if", "all_in_gpu", ":", "\n", "                ", "class_probabilities_here", "=", "class_probabilities", ".", "detach", "(", ")", ".", "cpu", "(", ")", ".", "numpy", "(", ")", "\n", "", "else", ":", "\n", "                ", "class_probabilities_here", "=", "class_probabilities", "\n", "", "predicted_segmentation", "=", "np", ".", "zeros", "(", "class_probabilities_here", ".", "shape", "[", "1", ":", "]", ",", "dtype", "=", "np", ".", "float32", ")", "\n", "for", "i", ",", "c", "in", "enumerate", "(", "regions_class_order", ")", ":", "\n", "                ", "predicted_segmentation", "[", "class_probabilities_here", "[", "i", "]", ">", "0.5", "]", "=", "c", "\n", "\n", "", "", "if", "all_in_gpu", ":", "\n", "            ", "if", "verbose", ":", "print", "(", "\"copying results to CPU\"", ")", "\n", "\n", "if", "regions_class_order", "is", "None", ":", "\n", "                ", "predicted_segmentation", "=", "predicted_segmentation", ".", "detach", "(", ")", ".", "cpu", "(", ")", ".", "numpy", "(", ")", "\n", "\n", "", "class_probabilities", "=", "class_probabilities", ".", "detach", "(", ")", ".", "cpu", "(", ")", ".", "numpy", "(", ")", "\n", "\n", "", "if", "verbose", ":", "print", "(", "\"prediction done\"", ")", "\n", "return", "predicted_segmentation", ",", "class_probabilities", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.network_architecture.neural_network.SegmentationNetwork._internal_predict_2D_2Dconv": [[425, 462], ["torch.cuda.empty_cache", "batchgenerators.augmentations.utils.pad_nd_image", "tuple", "len", "neural_network.SegmentationNetwork.get_device", "print", "neural_network.SegmentationNetwork._internal_maybe_mirror_and_pred_2D", "predicted_probabilities.detach().cpu().numpy.detach().cpu().numpy.argmax", "numpy.zeros.detach().cpu().numpy", "predicted_probabilities.detach().cpu().numpy.detach().cpu().numpy.detach().cpu().numpy", "predicted_probabilities.detach().cpu().numpy.detach().cpu().numpy.detach().cpu().numpy", "numpy.zeros", "enumerate", "slice", "numpy.zeros.detach().cpu", "predicted_probabilities.detach().cpu().numpy.detach().cpu().numpy.detach().cpu", "predicted_probabilities.detach().cpu().numpy.detach().cpu().numpy.detach().cpu", "range", "numpy.zeros.detach", "predicted_probabilities.detach().cpu().numpy.detach().cpu().numpy.detach", "predicted_probabilities.detach().cpu().numpy.detach().cpu().numpy.detach", "len", "len"], "methods", ["home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.network_architecture.neural_network.NeuralNetwork.get_device", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.network_architecture.neural_network.SegmentationNetwork._internal_maybe_mirror_and_pred_2D"], ["", "def", "_internal_predict_2D_2Dconv", "(", "self", ",", "x", ":", "np", ".", "ndarray", ",", "min_size", ":", "Tuple", "[", "int", ",", "int", "]", ",", "do_mirroring", ":", "bool", ",", "\n", "mirror_axes", ":", "tuple", "=", "(", "0", ",", "1", ",", "2", ")", ",", "regions_class_order", ":", "tuple", "=", "None", ",", "\n", "pad_border_mode", ":", "str", "=", "\"constant\"", ",", "pad_kwargs", ":", "dict", "=", "None", ",", "\n", "verbose", ":", "bool", "=", "True", ")", "->", "Tuple", "[", "np", ".", "ndarray", ",", "np", ".", "ndarray", "]", ":", "\n", "        ", "\"\"\"\n        This one does fully convolutional inference. No sliding window\n        \"\"\"", "\n", "assert", "len", "(", "x", ".", "shape", ")", "==", "3", ",", "\"x must be (c, x, y)\"", "\n", "assert", "self", ".", "get_device", "(", ")", "!=", "\"cpu\"", "\n", "assert", "self", ".", "input_shape_must_be_divisible_by", "is", "not", "None", ",", "'input_shape_must_be_divisible_by must be set to '", "'run _internal_predict_2D_2Dconv'", "\n", "if", "verbose", ":", "print", "(", "\"do mirror:\"", ",", "do_mirroring", ")", "\n", "\n", "torch", ".", "cuda", ".", "empty_cache", "(", ")", "\n", "\n", "data", ",", "slicer", "=", "pad_nd_image", "(", "x", ",", "min_size", ",", "pad_border_mode", ",", "pad_kwargs", ",", "True", ",", "\n", "self", ".", "input_shape_must_be_divisible_by", ")", "\n", "\n", "predicted_probabilities", "=", "self", ".", "_internal_maybe_mirror_and_pred_2D", "(", "data", "[", "None", "]", ",", "mirror_axes", ",", "do_mirroring", ",", "\n", "None", ")", "[", "0", "]", "\n", "\n", "slicer", "=", "tuple", "(", "\n", "[", "slice", "(", "0", ",", "predicted_probabilities", ".", "shape", "[", "i", "]", ")", "for", "i", "in", "range", "(", "len", "(", "predicted_probabilities", ".", "shape", ")", "-", "\n", "(", "len", "(", "slicer", ")", "-", "1", ")", ")", "]", "+", "slicer", "[", "1", ":", "]", ")", "\n", "predicted_probabilities", "=", "predicted_probabilities", "[", "slicer", "]", "\n", "\n", "if", "regions_class_order", "is", "None", ":", "\n", "            ", "predicted_segmentation", "=", "predicted_probabilities", ".", "argmax", "(", "0", ")", "\n", "predicted_segmentation", "=", "predicted_segmentation", ".", "detach", "(", ")", ".", "cpu", "(", ")", ".", "numpy", "(", ")", "\n", "predicted_probabilities", "=", "predicted_probabilities", ".", "detach", "(", ")", ".", "cpu", "(", ")", ".", "numpy", "(", ")", "\n", "", "else", ":", "\n", "            ", "predicted_probabilities", "=", "predicted_probabilities", ".", "detach", "(", ")", ".", "cpu", "(", ")", ".", "numpy", "(", ")", "\n", "predicted_segmentation", "=", "np", ".", "zeros", "(", "predicted_probabilities", ".", "shape", "[", "1", ":", "]", ",", "dtype", "=", "np", ".", "float32", ")", "\n", "for", "i", ",", "c", "in", "enumerate", "(", "regions_class_order", ")", ":", "\n", "                ", "predicted_segmentation", "[", "predicted_probabilities", "[", "i", "]", ">", "0.5", "]", "=", "c", "\n", "\n", "", "", "return", "predicted_segmentation", ",", "predicted_probabilities", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.network_architecture.neural_network.SegmentationNetwork._internal_predict_3D_3Dconv": [[463, 500], ["torch.cuda.empty_cache", "batchgenerators.augmentations.utils.pad_nd_image", "tuple", "len", "neural_network.SegmentationNetwork.get_device", "print", "neural_network.SegmentationNetwork._internal_maybe_mirror_and_pred_3D", "predicted_probabilities.detach().cpu().numpy.detach().cpu().numpy.argmax", "numpy.zeros.detach().cpu().numpy", "predicted_probabilities.detach().cpu().numpy.detach().cpu().numpy.detach().cpu().numpy", "predicted_probabilities.detach().cpu().numpy.detach().cpu().numpy.detach().cpu().numpy", "numpy.zeros", "enumerate", "slice", "numpy.zeros.detach().cpu", "predicted_probabilities.detach().cpu().numpy.detach().cpu().numpy.detach().cpu", "predicted_probabilities.detach().cpu().numpy.detach().cpu().numpy.detach().cpu", "range", "numpy.zeros.detach", "predicted_probabilities.detach().cpu().numpy.detach().cpu().numpy.detach", "predicted_probabilities.detach().cpu().numpy.detach().cpu().numpy.detach", "len", "len"], "methods", ["home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.network_architecture.neural_network.NeuralNetwork.get_device", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.network_architecture.neural_network.SegmentationNetwork._internal_maybe_mirror_and_pred_3D"], ["", "def", "_internal_predict_3D_3Dconv", "(", "self", ",", "x", ":", "np", ".", "ndarray", ",", "min_size", ":", "Tuple", "[", "int", ",", "...", "]", ",", "do_mirroring", ":", "bool", ",", "\n", "mirror_axes", ":", "tuple", "=", "(", "0", ",", "1", ",", "2", ")", ",", "regions_class_order", ":", "tuple", "=", "None", ",", "\n", "pad_border_mode", ":", "str", "=", "\"constant\"", ",", "pad_kwargs", ":", "dict", "=", "None", ",", "\n", "verbose", ":", "bool", "=", "True", ")", "->", "Tuple", "[", "np", ".", "ndarray", ",", "np", ".", "ndarray", "]", ":", "\n", "        ", "\"\"\"\n        This one does fully convolutional inference. No sliding window\n        \"\"\"", "\n", "assert", "len", "(", "x", ".", "shape", ")", "==", "4", ",", "\"x must be (c, x, y, z)\"", "\n", "assert", "self", ".", "get_device", "(", ")", "!=", "\"cpu\"", "\n", "assert", "self", ".", "input_shape_must_be_divisible_by", "is", "not", "None", ",", "'input_shape_must_be_divisible_by must be set to '", "'run _internal_predict_3D_3Dconv'", "\n", "if", "verbose", ":", "print", "(", "\"do mirror:\"", ",", "do_mirroring", ")", "\n", "\n", "torch", ".", "cuda", ".", "empty_cache", "(", ")", "\n", "\n", "data", ",", "slicer", "=", "pad_nd_image", "(", "x", ",", "min_size", ",", "pad_border_mode", ",", "pad_kwargs", ",", "True", ",", "\n", "self", ".", "input_shape_must_be_divisible_by", ")", "\n", "\n", "predicted_probabilities", "=", "self", ".", "_internal_maybe_mirror_and_pred_3D", "(", "data", "[", "None", "]", ",", "mirror_axes", ",", "do_mirroring", ",", "\n", "None", ")", "[", "0", "]", "\n", "\n", "slicer", "=", "tuple", "(", "\n", "[", "slice", "(", "0", ",", "predicted_probabilities", ".", "shape", "[", "i", "]", ")", "for", "i", "in", "range", "(", "len", "(", "predicted_probabilities", ".", "shape", ")", "-", "\n", "(", "len", "(", "slicer", ")", "-", "1", ")", ")", "]", "+", "slicer", "[", "1", ":", "]", ")", "\n", "predicted_probabilities", "=", "predicted_probabilities", "[", "slicer", "]", "\n", "\n", "if", "regions_class_order", "is", "None", ":", "\n", "            ", "predicted_segmentation", "=", "predicted_probabilities", ".", "argmax", "(", "0", ")", "\n", "predicted_segmentation", "=", "predicted_segmentation", ".", "detach", "(", ")", ".", "cpu", "(", ")", ".", "numpy", "(", ")", "\n", "predicted_probabilities", "=", "predicted_probabilities", ".", "detach", "(", ")", ".", "cpu", "(", ")", ".", "numpy", "(", ")", "\n", "", "else", ":", "\n", "            ", "predicted_probabilities", "=", "predicted_probabilities", ".", "detach", "(", ")", ".", "cpu", "(", ")", ".", "numpy", "(", ")", "\n", "predicted_segmentation", "=", "np", ".", "zeros", "(", "predicted_probabilities", ".", "shape", "[", "1", ":", "]", ",", "dtype", "=", "np", ".", "float32", ")", "\n", "for", "i", ",", "c", "in", "enumerate", "(", "regions_class_order", ")", ":", "\n", "                ", "predicted_segmentation", "[", "predicted_probabilities", "[", "i", "]", ">", "0.5", "]", "=", "c", "\n", "\n", "", "", "return", "predicted_segmentation", ",", "predicted_probabilities", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.network_architecture.neural_network.SegmentationNetwork._internal_maybe_mirror_and_pred_3D": [[501, 558], ["nnunet.utilities.to_torch.to_cuda", "torch.zeros().cuda", "range", "len", "nnunet.utilities.to_torch.maybe_to_torch", "neural_network.SegmentationNetwork.get_device", "nnunet.utilities.to_torch.to_cuda", "neural_network.SegmentationNetwork.get_device", "torch.zeros", "nnunet.utilities.to_torch.maybe_to_torch", "len", "neural_network.SegmentationNetwork.inference_apply_nonlin", "neural_network.SegmentationNetwork.inference_apply_nonlin", "neural_network.SegmentationNetwork.inference_apply_nonlin", "neural_network.SegmentationNetwork.inference_apply_nonlin", "neural_network.SegmentationNetwork.inference_apply_nonlin", "neural_network.SegmentationNetwork.inference_apply_nonlin", "neural_network.SegmentationNetwork.inference_apply_nonlin", "neural_network.SegmentationNetwork.inference_apply_nonlin", "neural_network.SegmentationNetwork.get_device", "neural_network.SegmentationNetwork.", "neural_network.SegmentationNetwork.", "torch.flip", "neural_network.SegmentationNetwork.", "torch.flip", "neural_network.SegmentationNetwork.", "torch.flip", "neural_network.SegmentationNetwork.", "torch.flip", "neural_network.SegmentationNetwork.", "torch.flip", "neural_network.SegmentationNetwork.", "torch.flip", "neural_network.SegmentationNetwork.", "torch.flip", "list", "torch.flip", "torch.flip", "torch.flip", "torch.flip", "torch.flip", "torch.flip", "torch.flip"], "methods", ["home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.utilities.to_torch.to_cuda", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.utilities.to_torch.maybe_to_torch", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.network_architecture.neural_network.NeuralNetwork.get_device", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.utilities.to_torch.to_cuda", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.network_architecture.neural_network.NeuralNetwork.get_device", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.utilities.to_torch.maybe_to_torch", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.network_architecture.neural_network.NeuralNetwork.get_device", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.utilities.tensor_utilities.flip", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.utilities.tensor_utilities.flip", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.utilities.tensor_utilities.flip", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.utilities.tensor_utilities.flip", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.utilities.tensor_utilities.flip", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.utilities.tensor_utilities.flip", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.utilities.tensor_utilities.flip", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.utilities.tensor_utilities.flip", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.utilities.tensor_utilities.flip", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.utilities.tensor_utilities.flip", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.utilities.tensor_utilities.flip", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.utilities.tensor_utilities.flip", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.utilities.tensor_utilities.flip", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.utilities.tensor_utilities.flip"], ["", "def", "_internal_maybe_mirror_and_pred_3D", "(", "self", ",", "x", ":", "Union", "[", "np", ".", "ndarray", ",", "torch", ".", "tensor", "]", ",", "mirror_axes", ":", "tuple", ",", "\n", "do_mirroring", ":", "bool", "=", "True", ",", "\n", "mult", ":", "np", ".", "ndarray", "or", "torch", ".", "tensor", "=", "None", ")", "->", "torch", ".", "tensor", ":", "\n", "        ", "assert", "len", "(", "x", ".", "shape", ")", "==", "5", ",", "'x must be (b, c, x, y, z)'", "\n", "# everything in here takes place on the GPU. If x and mult are not yet on GPU this will be taken care of here", "\n", "# we now return a cuda tensor! Not numpy array!", "\n", "\n", "x", "=", "to_cuda", "(", "maybe_to_torch", "(", "x", ")", ",", "gpu_id", "=", "self", ".", "get_device", "(", ")", ")", "\n", "result_torch", "=", "torch", ".", "zeros", "(", "[", "1", ",", "self", ".", "num_classes", "]", "+", "list", "(", "x", ".", "shape", "[", "2", ":", "]", ")", ",", "\n", "dtype", "=", "torch", ".", "float", ")", ".", "cuda", "(", "self", ".", "get_device", "(", ")", ",", "non_blocking", "=", "True", ")", "\n", "\n", "if", "mult", "is", "not", "None", ":", "\n", "            ", "mult", "=", "to_cuda", "(", "maybe_to_torch", "(", "mult", ")", ",", "gpu_id", "=", "self", ".", "get_device", "(", ")", ")", "\n", "\n", "", "if", "do_mirroring", ":", "\n", "            ", "mirror_idx", "=", "8", "\n", "num_results", "=", "2", "**", "len", "(", "mirror_axes", ")", "\n", "", "else", ":", "\n", "            ", "mirror_idx", "=", "1", "\n", "num_results", "=", "1", "\n", "", "for", "m", "in", "range", "(", "mirror_idx", ")", ":", "\n", "            ", "if", "m", "==", "0", ":", "\n", "                ", "pred", "=", "self", ".", "inference_apply_nonlin", "(", "self", "(", "x", ")", ")", "\n", "result_torch", "+=", "1", "/", "num_results", "*", "pred", "\n", "\n", "", "if", "m", "==", "1", "and", "(", "2", "in", "mirror_axes", ")", ":", "\n", "                ", "pred", "=", "self", ".", "inference_apply_nonlin", "(", "self", "(", "torch", ".", "flip", "(", "x", ",", "(", "4", ",", ")", ")", ")", ")", "\n", "result_torch", "+=", "1", "/", "num_results", "*", "torch", ".", "flip", "(", "pred", ",", "(", "4", ",", ")", ")", "\n", "\n", "", "if", "m", "==", "2", "and", "(", "1", "in", "mirror_axes", ")", ":", "\n", "                ", "pred", "=", "self", ".", "inference_apply_nonlin", "(", "self", "(", "torch", ".", "flip", "(", "x", ",", "(", "3", ",", ")", ")", ")", ")", "\n", "result_torch", "+=", "1", "/", "num_results", "*", "torch", ".", "flip", "(", "pred", ",", "(", "3", ",", ")", ")", "\n", "\n", "", "if", "m", "==", "3", "and", "(", "2", "in", "mirror_axes", ")", "and", "(", "1", "in", "mirror_axes", ")", ":", "\n", "                ", "pred", "=", "self", ".", "inference_apply_nonlin", "(", "self", "(", "torch", ".", "flip", "(", "x", ",", "(", "4", ",", "3", ")", ")", ")", ")", "\n", "result_torch", "+=", "1", "/", "num_results", "*", "torch", ".", "flip", "(", "pred", ",", "(", "4", ",", "3", ")", ")", "\n", "\n", "", "if", "m", "==", "4", "and", "(", "0", "in", "mirror_axes", ")", ":", "\n", "                ", "pred", "=", "self", ".", "inference_apply_nonlin", "(", "self", "(", "torch", ".", "flip", "(", "x", ",", "(", "2", ",", ")", ")", ")", ")", "\n", "result_torch", "+=", "1", "/", "num_results", "*", "torch", ".", "flip", "(", "pred", ",", "(", "2", ",", ")", ")", "\n", "\n", "", "if", "m", "==", "5", "and", "(", "0", "in", "mirror_axes", ")", "and", "(", "2", "in", "mirror_axes", ")", ":", "\n", "                ", "pred", "=", "self", ".", "inference_apply_nonlin", "(", "self", "(", "torch", ".", "flip", "(", "x", ",", "(", "4", ",", "2", ")", ")", ")", ")", "\n", "result_torch", "+=", "1", "/", "num_results", "*", "torch", ".", "flip", "(", "pred", ",", "(", "4", ",", "2", ")", ")", "\n", "\n", "", "if", "m", "==", "6", "and", "(", "0", "in", "mirror_axes", ")", "and", "(", "1", "in", "mirror_axes", ")", ":", "\n", "                ", "pred", "=", "self", ".", "inference_apply_nonlin", "(", "self", "(", "torch", ".", "flip", "(", "x", ",", "(", "3", ",", "2", ")", ")", ")", ")", "\n", "result_torch", "+=", "1", "/", "num_results", "*", "torch", ".", "flip", "(", "pred", ",", "(", "3", ",", "2", ")", ")", "\n", "\n", "", "if", "m", "==", "7", "and", "(", "0", "in", "mirror_axes", ")", "and", "(", "1", "in", "mirror_axes", ")", "and", "(", "2", "in", "mirror_axes", ")", ":", "\n", "                ", "pred", "=", "self", ".", "inference_apply_nonlin", "(", "self", "(", "torch", ".", "flip", "(", "x", ",", "(", "4", ",", "3", ",", "2", ")", ")", ")", ")", "\n", "result_torch", "+=", "1", "/", "num_results", "*", "torch", ".", "flip", "(", "pred", ",", "(", "4", ",", "3", ",", "2", ")", ")", "\n", "\n", "", "", "if", "mult", "is", "not", "None", ":", "\n", "            ", "result_torch", "[", ":", ",", ":", "]", "*=", "mult", "\n", "\n", "", "return", "result_torch", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.network_architecture.neural_network.SegmentationNetwork._internal_maybe_mirror_and_pred_2D": [[559, 601], ["nnunet.utilities.to_torch.to_cuda", "torch.zeros().cuda", "range", "len", "nnunet.utilities.to_torch.maybe_to_torch", "neural_network.SegmentationNetwork.get_device", "nnunet.utilities.to_torch.to_cuda", "neural_network.SegmentationNetwork.get_device", "torch.zeros", "nnunet.utilities.to_torch.maybe_to_torch", "len", "neural_network.SegmentationNetwork.inference_apply_nonlin", "neural_network.SegmentationNetwork.inference_apply_nonlin", "neural_network.SegmentationNetwork.inference_apply_nonlin", "neural_network.SegmentationNetwork.inference_apply_nonlin", "neural_network.SegmentationNetwork.get_device", "neural_network.SegmentationNetwork.", "neural_network.SegmentationNetwork.", "torch.flip", "neural_network.SegmentationNetwork.", "torch.flip", "neural_network.SegmentationNetwork.", "torch.flip", "list", "torch.flip", "torch.flip", "torch.flip"], "methods", ["home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.utilities.to_torch.to_cuda", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.utilities.to_torch.maybe_to_torch", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.network_architecture.neural_network.NeuralNetwork.get_device", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.utilities.to_torch.to_cuda", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.network_architecture.neural_network.NeuralNetwork.get_device", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.utilities.to_torch.maybe_to_torch", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.network_architecture.neural_network.NeuralNetwork.get_device", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.utilities.tensor_utilities.flip", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.utilities.tensor_utilities.flip", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.utilities.tensor_utilities.flip", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.utilities.tensor_utilities.flip", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.utilities.tensor_utilities.flip", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.utilities.tensor_utilities.flip"], ["", "def", "_internal_maybe_mirror_and_pred_2D", "(", "self", ",", "x", ":", "Union", "[", "np", ".", "ndarray", ",", "torch", ".", "tensor", "]", ",", "mirror_axes", ":", "tuple", ",", "\n", "do_mirroring", ":", "bool", "=", "True", ",", "\n", "mult", ":", "np", ".", "ndarray", "or", "torch", ".", "tensor", "=", "None", ")", "->", "torch", ".", "tensor", ":", "\n", "# everything in here takes place on the GPU. If x and mult are not yet on GPU this will be taken care of here", "\n", "# we now return a cuda tensor! Not numpy array!", "\n", "        ", "assert", "len", "(", "x", ".", "shape", ")", "==", "4", ",", "'x must be (b, c, x, y)'", "\n", "\n", "x", "=", "to_cuda", "(", "maybe_to_torch", "(", "x", ")", ",", "gpu_id", "=", "self", ".", "get_device", "(", ")", ")", "\n", "result_torch", "=", "torch", ".", "zeros", "(", "[", "x", ".", "shape", "[", "0", "]", ",", "self", ".", "num_classes", "]", "+", "list", "(", "x", ".", "shape", "[", "2", ":", "]", ")", ",", "\n", "dtype", "=", "torch", ".", "float", ")", ".", "cuda", "(", "self", ".", "get_device", "(", ")", ",", "non_blocking", "=", "True", ")", "\n", "\n", "if", "mult", "is", "not", "None", ":", "\n", "            ", "mult", "=", "to_cuda", "(", "maybe_to_torch", "(", "mult", ")", ",", "gpu_id", "=", "self", ".", "get_device", "(", ")", ")", "\n", "\n", "", "if", "do_mirroring", ":", "\n", "            ", "mirror_idx", "=", "4", "\n", "num_results", "=", "2", "**", "len", "(", "mirror_axes", ")", "\n", "", "else", ":", "\n", "            ", "mirror_idx", "=", "1", "\n", "num_results", "=", "1", "\n", "\n", "", "for", "m", "in", "range", "(", "mirror_idx", ")", ":", "\n", "            ", "if", "m", "==", "0", ":", "\n", "                ", "pred", "=", "self", ".", "inference_apply_nonlin", "(", "self", "(", "x", ")", ")", "\n", "result_torch", "+=", "1", "/", "num_results", "*", "pred", "\n", "\n", "", "if", "m", "==", "1", "and", "(", "1", "in", "mirror_axes", ")", ":", "\n", "                ", "pred", "=", "self", ".", "inference_apply_nonlin", "(", "self", "(", "torch", ".", "flip", "(", "x", ",", "(", "3", ",", ")", ")", ")", ")", "\n", "result_torch", "+=", "1", "/", "num_results", "*", "torch", ".", "flip", "(", "pred", ",", "(", "3", ",", ")", ")", "\n", "\n", "", "if", "m", "==", "2", "and", "(", "0", "in", "mirror_axes", ")", ":", "\n", "                ", "pred", "=", "self", ".", "inference_apply_nonlin", "(", "self", "(", "torch", ".", "flip", "(", "x", ",", "(", "2", ",", ")", ")", ")", ")", "\n", "result_torch", "+=", "1", "/", "num_results", "*", "torch", ".", "flip", "(", "pred", ",", "(", "2", ",", ")", ")", "\n", "\n", "", "if", "m", "==", "3", "and", "(", "0", "in", "mirror_axes", ")", "and", "(", "1", "in", "mirror_axes", ")", ":", "\n", "                ", "pred", "=", "self", ".", "inference_apply_nonlin", "(", "self", "(", "torch", ".", "flip", "(", "x", ",", "(", "3", ",", "2", ")", ")", ")", ")", "\n", "result_torch", "+=", "1", "/", "num_results", "*", "torch", ".", "flip", "(", "pred", ",", "(", "3", ",", "2", ")", ")", "\n", "\n", "", "", "if", "mult", "is", "not", "None", ":", "\n", "            ", "result_torch", "[", ":", ",", ":", "]", "*=", "mult", "\n", "\n", "", "return", "result_torch", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.network_architecture.neural_network.SegmentationNetwork._internal_predict_2D_2Dconv_tiled": [[602, 735], ["torch.cuda.empty_cache", "batchgenerators.augmentations.utils.pad_nd_image", "neural_network.SegmentationNetwork._compute_steps_for_sliding_window", "tuple", "len", "neural_network.SegmentationNetwork.get_device", "print", "print", "len", "len", "print", "print", "print", "print", "torch.from_numpy().cuda", "torch.zeros", "torch.from_numpy().cuda", "torch.zeros", "numpy.zeros", "numpy.zeros", "class_probabilities.detach().cpu().numpy.detach().cpu().numpy.argmax", "numpy.zeros", "enumerate", "class_probabilities.detach().cpu().numpy.detach().cpu().numpy.detach().cpu().numpy", "print", "neural_network.SegmentationNetwork._get_gaussian", "neural_network.SegmentationNetwork.get_device", "gaussian_importance_map.half.half.half", "gaussian_importance_map[].min", "torch.ones", "print", "print", "neural_network.SegmentationNetwork.get_device", "print", "numpy.ones", "class_probabilities.detach().cpu().numpy.detach().cpu().numpy.detach().cpu().numpy", "print", "predicted_segmentation.detach().cpu().numpy.detach().cpu().numpy.detach().cpu().numpy", "all", "print", "print", "torch.from_numpy", "list", "neural_network.SegmentationNetwork.get_device", "torch.from_numpy", "list", "neural_network.SegmentationNetwork.get_device", "list", "list", "neural_network.SegmentationNetwork._internal_maybe_mirror_and_pred_2D", "predicted_patch.cpu().numpy.cpu().numpy.half", "predicted_patch.cpu().numpy.cpu().numpy.cpu().numpy", "slice", "class_probabilities.detach().cpu().numpy.detach().cpu().numpy.detach().cpu", "neural_network.SegmentationNetwork.get_device", "range", "class_probabilities.detach().cpu().numpy.detach().cpu().numpy.detach().cpu", "predicted_segmentation.detach().cpu().numpy.detach().cpu().numpy.detach().cpu", "predicted_patch.cpu().numpy.cpu().numpy.cpu", "class_probabilities.detach().cpu().numpy.detach().cpu().numpy.detach", "zip", "len", "class_probabilities.detach().cpu().numpy.detach().cpu().numpy.detach", "predicted_segmentation.detach().cpu().numpy.detach().cpu().numpy.detach", "len"], "methods", ["home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.network_architecture.neural_network.SegmentationNetwork._compute_steps_for_sliding_window", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.network_architecture.neural_network.NeuralNetwork.get_device", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.network_architecture.neural_network.SegmentationNetwork._get_gaussian", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.network_architecture.neural_network.NeuralNetwork.get_device", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.network_architecture.neural_network.NeuralNetwork.get_device", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.network_architecture.neural_network.NeuralNetwork.get_device", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.network_architecture.neural_network.NeuralNetwork.get_device", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.network_architecture.neural_network.SegmentationNetwork._internal_maybe_mirror_and_pred_2D", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.network_architecture.neural_network.NeuralNetwork.get_device"], ["", "def", "_internal_predict_2D_2Dconv_tiled", "(", "self", ",", "x", ":", "np", ".", "ndarray", ",", "step_size", ":", "float", ",", "do_mirroring", ":", "bool", ",", "mirror_axes", ":", "tuple", ",", "\n", "patch_size", ":", "tuple", ",", "regions_class_order", ":", "tuple", ",", "use_gaussian", ":", "bool", ",", "\n", "pad_border_mode", ":", "str", ",", "pad_kwargs", ":", "dict", ",", "all_in_gpu", ":", "bool", ",", "\n", "verbose", ":", "bool", ")", "->", "Tuple", "[", "np", ".", "ndarray", ",", "np", ".", "ndarray", "]", ":", "\n", "# better safe than sorry", "\n", "        ", "assert", "len", "(", "x", ".", "shape", ")", "==", "3", ",", "\"x must be (c, x, y)\"", "\n", "assert", "self", ".", "get_device", "(", ")", "!=", "\"cpu\"", "\n", "if", "verbose", ":", "print", "(", "\"step_size:\"", ",", "step_size", ")", "\n", "if", "verbose", ":", "print", "(", "\"do mirror:\"", ",", "do_mirroring", ")", "\n", "\n", "torch", ".", "cuda", ".", "empty_cache", "(", ")", "\n", "\n", "assert", "patch_size", "is", "not", "None", ",", "\"patch_size cannot be None for tiled prediction\"", "\n", "\n", "# for sliding window inference the image must at least be as large as the patch size. It does not matter", "\n", "# whether the shape is divisible by 2**num_pool as long as the patch size is", "\n", "data", ",", "slicer", "=", "pad_nd_image", "(", "x", ",", "patch_size", ",", "pad_border_mode", ",", "pad_kwargs", ",", "True", ",", "None", ")", "\n", "data_shape", "=", "data", ".", "shape", "# still c, x, y", "\n", "\n", "# compute the steps for sliding window", "\n", "steps", "=", "self", ".", "_compute_steps_for_sliding_window", "(", "patch_size", ",", "data_shape", "[", "1", ":", "]", ",", "step_size", ")", "\n", "num_tiles", "=", "len", "(", "steps", "[", "0", "]", ")", "*", "len", "(", "steps", "[", "1", "]", ")", "\n", "\n", "if", "verbose", ":", "\n", "            ", "print", "(", "\"data shape:\"", ",", "data_shape", ")", "\n", "print", "(", "\"patch size:\"", ",", "patch_size", ")", "\n", "print", "(", "\"steps (x, y, and z):\"", ",", "steps", ")", "\n", "print", "(", "\"number of tiles:\"", ",", "num_tiles", ")", "\n", "\n", "# we only need to compute that once. It can take a while to compute this due to the large sigma in", "\n", "# gaussian_filter", "\n", "", "if", "use_gaussian", "and", "num_tiles", ">", "1", ":", "\n", "            ", "if", "self", ".", "_gaussian_2d", "is", "None", "or", "not", "all", "(", "\n", "[", "i", "==", "j", "for", "i", ",", "j", "in", "zip", "(", "patch_size", ",", "self", ".", "_patch_size_for_gaussian_2d", ")", "]", ")", ":", "\n", "                ", "if", "verbose", ":", "print", "(", "'computing Gaussian'", ")", "\n", "gaussian_importance_map", "=", "self", ".", "_get_gaussian", "(", "patch_size", ",", "sigma_scale", "=", "1.", "/", "8", ")", "\n", "\n", "self", ".", "_gaussian_2d", "=", "gaussian_importance_map", "\n", "self", ".", "_patch_size_for_gaussian_2d", "=", "patch_size", "\n", "", "else", ":", "\n", "                ", "if", "verbose", ":", "print", "(", "\"using precomputed Gaussian\"", ")", "\n", "gaussian_importance_map", "=", "self", ".", "_gaussian_2d", "\n", "\n", "", "gaussian_importance_map", "=", "torch", ".", "from_numpy", "(", "gaussian_importance_map", ")", ".", "cuda", "(", "self", ".", "get_device", "(", ")", ",", "\n", "non_blocking", "=", "True", ")", "\n", "", "else", ":", "\n", "            ", "gaussian_importance_map", "=", "None", "\n", "\n", "", "if", "all_in_gpu", ":", "\n", "# If we run the inference in GPU only (meaning all tensors are allocated on the GPU, this reduces", "\n", "# CPU-GPU communication but required more GPU memory) we need to preallocate a few things on GPU", "\n", "\n", "            ", "if", "use_gaussian", "and", "num_tiles", ">", "1", ":", "\n", "# half precision for the outputs should be good enough. If the outputs here are half, the", "\n", "# gaussian_importance_map should be as well", "\n", "                ", "gaussian_importance_map", "=", "gaussian_importance_map", ".", "half", "(", ")", "\n", "\n", "# make sure we did not round anything to 0", "\n", "gaussian_importance_map", "[", "gaussian_importance_map", "==", "0", "]", "=", "gaussian_importance_map", "[", "\n", "gaussian_importance_map", "!=", "0", "]", ".", "min", "(", ")", "\n", "\n", "add_for_nb_of_preds", "=", "gaussian_importance_map", "\n", "", "else", ":", "\n", "                ", "add_for_nb_of_preds", "=", "torch", ".", "ones", "(", "data", ".", "shape", "[", "1", ":", "]", ",", "device", "=", "self", ".", "get_device", "(", ")", ")", "\n", "\n", "", "if", "verbose", ":", "print", "(", "\"initializing result array (on GPU)\"", ")", "\n", "aggregated_results", "=", "torch", ".", "zeros", "(", "[", "self", ".", "num_classes", "]", "+", "list", "(", "data", ".", "shape", "[", "1", ":", "]", ")", ",", "dtype", "=", "torch", ".", "half", ",", "\n", "device", "=", "self", ".", "get_device", "(", ")", ")", "\n", "\n", "if", "verbose", ":", "print", "(", "\"moving data to GPU\"", ")", "\n", "data", "=", "torch", ".", "from_numpy", "(", "data", ")", ".", "cuda", "(", "self", ".", "get_device", "(", ")", ",", "non_blocking", "=", "True", ")", "\n", "\n", "if", "verbose", ":", "print", "(", "\"initializing result_numsamples (on GPU)\"", ")", "\n", "aggregated_nb_of_predictions", "=", "torch", ".", "zeros", "(", "[", "self", ".", "num_classes", "]", "+", "list", "(", "data", ".", "shape", "[", "1", ":", "]", ")", ",", "dtype", "=", "torch", ".", "half", ",", "\n", "device", "=", "self", ".", "get_device", "(", ")", ")", "\n", "", "else", ":", "\n", "            ", "if", "use_gaussian", "and", "num_tiles", ">", "1", ":", "\n", "                ", "add_for_nb_of_preds", "=", "self", ".", "_gaussian_2d", "\n", "", "else", ":", "\n", "                ", "add_for_nb_of_preds", "=", "np", ".", "ones", "(", "data", ".", "shape", "[", "1", ":", "]", ",", "dtype", "=", "np", ".", "float32", ")", "\n", "", "aggregated_results", "=", "np", ".", "zeros", "(", "[", "self", ".", "num_classes", "]", "+", "list", "(", "data", ".", "shape", "[", "1", ":", "]", ")", ",", "dtype", "=", "np", ".", "float32", ")", "\n", "aggregated_nb_of_predictions", "=", "np", ".", "zeros", "(", "[", "self", ".", "num_classes", "]", "+", "list", "(", "data", ".", "shape", "[", "1", ":", "]", ")", ",", "dtype", "=", "np", ".", "float32", ")", "\n", "\n", "", "for", "x", "in", "steps", "[", "0", "]", ":", "\n", "            ", "lb_x", "=", "x", "\n", "ub_x", "=", "x", "+", "patch_size", "[", "0", "]", "\n", "for", "y", "in", "steps", "[", "1", "]", ":", "\n", "                ", "lb_y", "=", "y", "\n", "ub_y", "=", "y", "+", "patch_size", "[", "1", "]", "\n", "\n", "predicted_patch", "=", "self", ".", "_internal_maybe_mirror_and_pred_2D", "(", "\n", "data", "[", "None", ",", ":", ",", "lb_x", ":", "ub_x", ",", "lb_y", ":", "ub_y", "]", ",", "mirror_axes", ",", "do_mirroring", ",", "\n", "gaussian_importance_map", ")", "[", "0", "]", "\n", "\n", "if", "all_in_gpu", ":", "\n", "                    ", "predicted_patch", "=", "predicted_patch", ".", "half", "(", ")", "\n", "", "else", ":", "\n", "                    ", "predicted_patch", "=", "predicted_patch", ".", "cpu", "(", ")", ".", "numpy", "(", ")", "\n", "\n", "", "aggregated_results", "[", ":", ",", "lb_x", ":", "ub_x", ",", "lb_y", ":", "ub_y", "]", "+=", "predicted_patch", "\n", "aggregated_nb_of_predictions", "[", ":", ",", "lb_x", ":", "ub_x", ",", "lb_y", ":", "ub_y", "]", "+=", "add_for_nb_of_preds", "\n", "\n", "# we reverse the padding here (remeber that we padded the input to be at least as large as the patch size", "\n", "", "", "slicer", "=", "tuple", "(", "\n", "[", "slice", "(", "0", ",", "aggregated_results", ".", "shape", "[", "i", "]", ")", "for", "i", "in", "\n", "range", "(", "len", "(", "aggregated_results", ".", "shape", ")", "-", "(", "len", "(", "slicer", ")", "-", "1", ")", ")", "]", "+", "slicer", "[", "1", ":", "]", ")", "\n", "aggregated_results", "=", "aggregated_results", "[", "slicer", "]", "\n", "aggregated_nb_of_predictions", "=", "aggregated_nb_of_predictions", "[", "slicer", "]", "\n", "\n", "# computing the class_probabilities by dividing the aggregated result with result_numsamples", "\n", "class_probabilities", "=", "aggregated_results", "/", "aggregated_nb_of_predictions", "\n", "\n", "if", "regions_class_order", "is", "None", ":", "\n", "            ", "predicted_segmentation", "=", "class_probabilities", ".", "argmax", "(", "0", ")", "\n", "", "else", ":", "\n", "            ", "if", "all_in_gpu", ":", "\n", "                ", "class_probabilities_here", "=", "class_probabilities", ".", "detach", "(", ")", ".", "cpu", "(", ")", ".", "numpy", "(", ")", "\n", "", "else", ":", "\n", "                ", "class_probabilities_here", "=", "class_probabilities", "\n", "", "predicted_segmentation", "=", "np", ".", "zeros", "(", "class_probabilities_here", ".", "shape", "[", "1", ":", "]", ",", "dtype", "=", "np", ".", "float32", ")", "\n", "for", "i", ",", "c", "in", "enumerate", "(", "regions_class_order", ")", ":", "\n", "                ", "predicted_segmentation", "[", "class_probabilities_here", "[", "i", "]", ">", "0.5", "]", "=", "c", "\n", "\n", "", "", "if", "all_in_gpu", ":", "\n", "            ", "if", "verbose", ":", "print", "(", "\"copying results to CPU\"", ")", "\n", "\n", "if", "regions_class_order", "is", "None", ":", "\n", "                ", "predicted_segmentation", "=", "predicted_segmentation", ".", "detach", "(", ")", ".", "cpu", "(", ")", ".", "numpy", "(", ")", "\n", "\n", "", "class_probabilities", "=", "class_probabilities", ".", "detach", "(", ")", ".", "cpu", "(", ")", ".", "numpy", "(", ")", "\n", "\n", "", "if", "verbose", ":", "print", "(", "\"prediction done\"", ")", "\n", "return", "predicted_segmentation", ",", "class_probabilities", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.network_architecture.neural_network.SegmentationNetwork._internal_predict_3D_2Dconv": [[736, 753], ["range", "numpy.vstack", "numpy.vstack().transpose", "len", "neural_network.SegmentationNetwork._internal_predict_2D_2Dconv", "numpy.vstack.append", "numpy.vstack().transpose.append", "numpy.vstack"], "methods", ["home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.network_architecture.neural_network.SegmentationNetwork._internal_predict_2D_2Dconv"], ["", "def", "_internal_predict_3D_2Dconv", "(", "self", ",", "x", ":", "np", ".", "ndarray", ",", "min_size", ":", "Tuple", "[", "int", ",", "int", "]", ",", "do_mirroring", ":", "bool", ",", "\n", "mirror_axes", ":", "tuple", "=", "(", "0", ",", "1", ")", ",", "regions_class_order", ":", "tuple", "=", "None", ",", "\n", "pad_border_mode", ":", "str", "=", "\"constant\"", ",", "pad_kwargs", ":", "dict", "=", "None", ",", "\n", "all_in_gpu", ":", "bool", "=", "False", ",", "verbose", ":", "bool", "=", "True", ")", "->", "Tuple", "[", "np", ".", "ndarray", ",", "np", ".", "ndarray", "]", ":", "\n", "        ", "if", "all_in_gpu", ":", "\n", "            ", "raise", "NotImplementedError", "\n", "", "assert", "len", "(", "x", ".", "shape", ")", "==", "4", ",", "\"data must be c, x, y, z\"", "\n", "predicted_segmentation", "=", "[", "]", "\n", "softmax_pred", "=", "[", "]", "\n", "for", "s", "in", "range", "(", "x", ".", "shape", "[", "1", "]", ")", ":", "\n", "            ", "pred_seg", ",", "softmax_pres", "=", "self", ".", "_internal_predict_2D_2Dconv", "(", "\n", "x", "[", ":", ",", "s", "]", ",", "min_size", ",", "do_mirroring", ",", "mirror_axes", ",", "regions_class_order", ",", "pad_border_mode", ",", "pad_kwargs", ",", "verbose", ")", "\n", "predicted_segmentation", ".", "append", "(", "pred_seg", "[", "None", "]", ")", "\n", "softmax_pred", ".", "append", "(", "softmax_pres", "[", "None", "]", ")", "\n", "", "predicted_segmentation", "=", "np", ".", "vstack", "(", "predicted_segmentation", ")", "\n", "softmax_pred", "=", "np", ".", "vstack", "(", "softmax_pred", ")", ".", "transpose", "(", "(", "1", ",", "0", ",", "2", ",", "3", ")", ")", "\n", "return", "predicted_segmentation", ",", "softmax_pred", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.network_architecture.neural_network.SegmentationNetwork.predict_3D_pseudo3D_2Dconv": [[754, 785], ["numpy.array", "numpy.zeros", "numpy.concatenate", "range", "numpy.vstack", "numpy.vstack().transpose", "len", "d.reshape.reshape.reshape", "neural_network.SegmentationNetwork._internal_predict_2D_2Dconv", "numpy.vstack.append", "numpy.vstack().transpose.append", "numpy.vstack"], "methods", ["home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.network_architecture.neural_network.SegmentationNetwork._internal_predict_2D_2Dconv"], ["", "def", "predict_3D_pseudo3D_2Dconv", "(", "self", ",", "x", ":", "np", ".", "ndarray", ",", "min_size", ":", "Tuple", "[", "int", ",", "int", "]", ",", "do_mirroring", ":", "bool", ",", "\n", "mirror_axes", ":", "tuple", "=", "(", "0", ",", "1", ")", ",", "regions_class_order", ":", "tuple", "=", "None", ",", "\n", "pseudo3D_slices", ":", "int", "=", "5", ",", "all_in_gpu", ":", "bool", "=", "False", ",", "\n", "pad_border_mode", ":", "str", "=", "\"constant\"", ",", "pad_kwargs", ":", "dict", "=", "None", ",", "\n", "verbose", ":", "bool", "=", "True", ")", "->", "Tuple", "[", "np", ".", "ndarray", ",", "np", ".", "ndarray", "]", ":", "\n", "        ", "if", "all_in_gpu", ":", "\n", "            ", "raise", "NotImplementedError", "\n", "", "assert", "len", "(", "x", ".", "shape", ")", "==", "4", ",", "\"data must be c, x, y, z\"", "\n", "assert", "pseudo3D_slices", "%", "2", "==", "1", ",", "\"pseudo3D_slices must be odd\"", "\n", "extra_slices", "=", "(", "pseudo3D_slices", "-", "1", ")", "//", "2", "\n", "\n", "shp_for_pad", "=", "np", ".", "array", "(", "x", ".", "shape", ")", "\n", "shp_for_pad", "[", "1", "]", "=", "extra_slices", "\n", "\n", "pad", "=", "np", ".", "zeros", "(", "shp_for_pad", ",", "dtype", "=", "np", ".", "float32", ")", "\n", "data", "=", "np", ".", "concatenate", "(", "(", "pad", ",", "x", ",", "pad", ")", ",", "1", ")", "\n", "\n", "predicted_segmentation", "=", "[", "]", "\n", "softmax_pred", "=", "[", "]", "\n", "for", "s", "in", "range", "(", "extra_slices", ",", "data", ".", "shape", "[", "1", "]", "-", "extra_slices", ")", ":", "\n", "            ", "d", "=", "data", "[", ":", ",", "(", "s", "-", "extra_slices", ")", ":", "(", "s", "+", "extra_slices", "+", "1", ")", "]", "\n", "d", "=", "d", ".", "reshape", "(", "(", "-", "1", ",", "d", ".", "shape", "[", "-", "2", "]", ",", "d", ".", "shape", "[", "-", "1", "]", ")", ")", "\n", "pred_seg", ",", "softmax_pres", "=", "self", ".", "_internal_predict_2D_2Dconv", "(", "d", ",", "min_size", ",", "do_mirroring", ",", "mirror_axes", ",", "\n", "regions_class_order", ",", "pad_border_mode", ",", "pad_kwargs", ",", "verbose", ")", "\n", "predicted_segmentation", ".", "append", "(", "pred_seg", "[", "None", "]", ")", "\n", "softmax_pred", ".", "append", "(", "softmax_pres", "[", "None", "]", ")", "\n", "", "predicted_segmentation", "=", "np", ".", "vstack", "(", "predicted_segmentation", ")", "\n", "softmax_pred", "=", "np", ".", "vstack", "(", "softmax_pred", ")", ".", "transpose", "(", "(", "1", ",", "0", ",", "2", ",", "3", ")", ")", "\n", "\n", "return", "predicted_segmentation", ",", "softmax_pred", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.network_architecture.neural_network.SegmentationNetwork._internal_predict_3D_2Dconv_tiled": [[786, 812], ["range", "numpy.vstack", "numpy.vstack().transpose", "len", "neural_network.SegmentationNetwork._internal_predict_2D_2Dconv_tiled", "numpy.vstack.append", "numpy.vstack().transpose.append", "numpy.vstack"], "methods", ["home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.network_architecture.neural_network.SegmentationNetwork._internal_predict_2D_2Dconv_tiled"], ["", "def", "_internal_predict_3D_2Dconv_tiled", "(", "self", ",", "x", ":", "np", ".", "ndarray", ",", "patch_size", ":", "Tuple", "[", "int", ",", "int", "]", ",", "do_mirroring", ":", "bool", ",", "\n", "mirror_axes", ":", "tuple", "=", "(", "0", ",", "1", ")", ",", "step_size", ":", "float", "=", "0.5", ",", "\n", "regions_class_order", ":", "tuple", "=", "None", ",", "use_gaussian", ":", "bool", "=", "False", ",", "\n", "pad_border_mode", ":", "str", "=", "\"edge\"", ",", "pad_kwargs", ":", "dict", "=", "None", ",", "\n", "all_in_gpu", ":", "bool", "=", "False", ",", "\n", "verbose", ":", "bool", "=", "True", ")", "->", "Tuple", "[", "np", ".", "ndarray", ",", "np", ".", "ndarray", "]", ":", "\n", "        ", "if", "all_in_gpu", ":", "\n", "            ", "raise", "NotImplementedError", "\n", "\n", "", "assert", "len", "(", "x", ".", "shape", ")", "==", "4", ",", "\"data must be c, x, y, z\"", "\n", "\n", "predicted_segmentation", "=", "[", "]", "\n", "softmax_pred", "=", "[", "]", "\n", "\n", "for", "s", "in", "range", "(", "x", ".", "shape", "[", "1", "]", ")", ":", "\n", "            ", "pred_seg", ",", "softmax_pres", "=", "self", ".", "_internal_predict_2D_2Dconv_tiled", "(", "\n", "x", "[", ":", ",", "s", "]", ",", "step_size", ",", "do_mirroring", ",", "mirror_axes", ",", "patch_size", ",", "regions_class_order", ",", "use_gaussian", ",", "\n", "pad_border_mode", ",", "pad_kwargs", ",", "all_in_gpu", ",", "verbose", ")", "\n", "\n", "predicted_segmentation", ".", "append", "(", "pred_seg", "[", "None", "]", ")", "\n", "softmax_pred", ".", "append", "(", "softmax_pres", "[", "None", "]", ")", "\n", "\n", "", "predicted_segmentation", "=", "np", ".", "vstack", "(", "predicted_segmentation", ")", "\n", "softmax_pred", "=", "np", ".", "vstack", "(", "softmax_pred", ")", ".", "transpose", "(", "(", "1", ",", "0", ",", "2", ",", "3", ")", ")", "\n", "\n", "return", "predicted_segmentation", ",", "softmax_pred", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.network_architecture.generic_modular_UNet.PlainConvUNetEncoder.__init__": [[83, 138], ["torch.nn.Module.__init__", "len", "range", "torch.nn.ModuleList", "len", "len", "isinstance", "min", "nnunet.network_architecture.custom_modules.conv_blocks.StackedConvLayers", "generic_modular_UNet.PlainConvUNetEncoder.stages.append", "generic_modular_UNet.PlainConvUNetEncoder.stage_output_features.append", "generic_modular_UNet.PlainConvUNetEncoder.stage_conv_op_kernel_size.append", "generic_modular_UNet.PlainConvUNetEncoder.stage_pool_kernel_size.append", "len", "int"], "methods", ["home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.common.layers.ResizeImage.__init__"], ["    ", "def", "__init__", "(", "self", ",", "input_channels", ",", "base_num_features", ",", "num_blocks_per_stage", ",", "feat_map_mul_on_downscale", ",", "\n", "pool_op_kernel_sizes", ",", "conv_kernel_sizes", ",", "props", ",", "default_return_skips", "=", "True", ",", "\n", "max_num_features", "=", "480", ")", ":", "\n", "        ", "\"\"\"\n        Following UNet building blocks can be added by utilizing the properties this class exposes (TODO)\n\n        this one includes the bottleneck layer!\n\n        :param input_channels:\n        :param base_num_features:\n        :param num_blocks_per_stage:\n        :param feat_map_mul_on_downscale:\n        :param pool_op_kernel_sizes:\n        :param conv_kernel_sizes:\n        :param props:\n        \"\"\"", "\n", "super", "(", "PlainConvUNetEncoder", ",", "self", ")", ".", "__init__", "(", ")", "\n", "\n", "self", ".", "default_return_skips", "=", "default_return_skips", "\n", "self", ".", "props", "=", "props", "\n", "\n", "self", ".", "stages", "=", "[", "]", "\n", "self", ".", "stage_output_features", "=", "[", "]", "\n", "self", ".", "stage_pool_kernel_size", "=", "[", "]", "\n", "self", ".", "stage_conv_op_kernel_size", "=", "[", "]", "\n", "\n", "assert", "len", "(", "pool_op_kernel_sizes", ")", "==", "len", "(", "conv_kernel_sizes", ")", "\n", "\n", "num_stages", "=", "len", "(", "conv_kernel_sizes", ")", "\n", "\n", "if", "not", "isinstance", "(", "num_blocks_per_stage", ",", "(", "list", ",", "tuple", ")", ")", ":", "\n", "            ", "num_blocks_per_stage", "=", "[", "num_blocks_per_stage", "]", "*", "num_stages", "\n", "", "else", ":", "\n", "            ", "assert", "len", "(", "num_blocks_per_stage", ")", "==", "num_stages", "\n", "\n", "", "self", ".", "num_blocks_per_stage", "=", "num_blocks_per_stage", "# decoder may need this", "\n", "\n", "current_input_features", "=", "input_channels", "\n", "for", "stage", "in", "range", "(", "num_stages", ")", ":", "\n", "            ", "current_output_features", "=", "min", "(", "int", "(", "base_num_features", "*", "feat_map_mul_on_downscale", "**", "stage", ")", ",", "max_num_features", ")", "\n", "current_kernel_size", "=", "conv_kernel_sizes", "[", "stage", "]", "\n", "current_pool_kernel_size", "=", "pool_op_kernel_sizes", "[", "stage", "]", "\n", "\n", "current_stage", "=", "StackedConvLayers", "(", "current_input_features", ",", "current_output_features", ",", "current_kernel_size", ",", "\n", "props", ",", "num_blocks_per_stage", "[", "stage", "]", ",", "current_pool_kernel_size", ")", "\n", "\n", "self", ".", "stages", ".", "append", "(", "current_stage", ")", "\n", "self", ".", "stage_output_features", ".", "append", "(", "current_output_features", ")", "\n", "self", ".", "stage_conv_op_kernel_size", ".", "append", "(", "current_kernel_size", ")", "\n", "self", ".", "stage_pool_kernel_size", ".", "append", "(", "current_pool_kernel_size", ")", "\n", "\n", "# update current_input_features", "\n", "current_input_features", "=", "current_output_features", "\n", "\n", "", "self", ".", "stages", "=", "nn", ".", "ModuleList", "(", "self", ".", "stages", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.network_architecture.generic_modular_UNet.PlainConvUNetEncoder.forward": [[139, 160], ["s", "skips.append"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "x", ",", "return_skips", "=", "None", ")", ":", "\n", "        ", "\"\"\"\n\n        :param x:\n        :param return_skips: if none then self.default_return_skips is used\n        :return:\n        \"\"\"", "\n", "skips", "=", "[", "]", "\n", "\n", "for", "s", "in", "self", ".", "stages", ":", "\n", "            ", "x", "=", "s", "(", "x", ")", "\n", "if", "self", ".", "default_return_skips", ":", "\n", "                ", "skips", ".", "append", "(", "x", ")", "\n", "\n", "", "", "if", "return_skips", "is", "None", ":", "\n", "            ", "return_skips", "=", "self", ".", "default_return_skips", "\n", "\n", "", "if", "return_skips", ":", "\n", "            ", "return", "skips", "\n", "", "else", ":", "\n", "            ", "return", "x", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.network_architecture.generic_modular_UNet.PlainConvUNetEncoder.compute_approx_vram_consumption": [[161, 181], ["numpy.array", "range", "len", "min", "print", "numpy.prod", "numpy.array", "numpy.prod", "numpy.prod"], "methods", ["None"], ["", "", "@", "staticmethod", "\n", "def", "compute_approx_vram_consumption", "(", "patch_size", ",", "base_num_features", ",", "max_num_features", ",", "\n", "num_modalities", ",", "pool_op_kernel_sizes", ",", "num_blocks_per_stage_encoder", ",", "\n", "feat_map_mul_on_downscale", ",", "batch_size", ")", ":", "\n", "        ", "npool", "=", "len", "(", "pool_op_kernel_sizes", ")", "-", "1", "\n", "\n", "current_shape", "=", "np", ".", "array", "(", "patch_size", ")", "\n", "\n", "tmp", "=", "num_blocks_per_stage_encoder", "[", "0", "]", "*", "np", ".", "prod", "(", "current_shape", ")", "*", "base_num_features", "+", "num_modalities", "*", "np", ".", "prod", "(", "current_shape", ")", "\n", "\n", "num_feat", "=", "base_num_features", "\n", "\n", "for", "p", "in", "range", "(", "1", ",", "npool", "+", "1", ")", ":", "\n", "            ", "current_shape", "=", "current_shape", "/", "np", ".", "array", "(", "pool_op_kernel_sizes", "[", "p", "]", ")", "\n", "num_feat", "=", "min", "(", "num_feat", "*", "feat_map_mul_on_downscale", ",", "max_num_features", ")", "\n", "num_convs", "=", "num_blocks_per_stage_encoder", "[", "p", "]", "\n", "print", "(", "p", ",", "num_feat", ",", "num_convs", ",", "current_shape", ")", "\n", "tmp", "+=", "num_convs", "*", "np", ".", "prod", "(", "current_shape", ")", "*", "num_feat", "\n", "", "return", "tmp", "*", "batch_size", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.network_architecture.generic_modular_UNet.PlainConvUNetDecoder.__init__": [[184, 254], ["torch.nn.Module.__init__", "numpy.cumprod().astype", "enumerate", "torch.nn.ModuleList", "torch.nn.ModuleList", "torch.nn.ModuleList", "len", "len", "generic_modular_UNet.PlainConvUNetDecoder.tus.append", "generic_modular_UNet.PlainConvUNetDecoder.stages.append", "ValueError", "len", "numpy.cumprod", "numpy.arange", "transpconv", "nnunet.network_architecture.custom_modules.conv_blocks.StackedConvLayers", "numpy.vstack", "nnunet.network_architecture.generic_UNet.Upsample", "generic_modular_UNet.PlainConvUNetDecoder.deep_supervision_outputs.append", "generic_modular_UNet.PlainConvUNetDecoder.deep_supervision_outputs.append", "str", "torch.nn.Sequential"], "methods", ["home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.common.layers.ResizeImage.__init__"], ["    ", "def", "__init__", "(", "self", ",", "previous", ",", "num_classes", ",", "num_blocks_per_stage", "=", "None", ",", "network_props", "=", "None", ",", "deep_supervision", "=", "False", ",", "\n", "upscale_logits", "=", "False", ")", ":", "\n", "        ", "super", "(", "PlainConvUNetDecoder", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "num_classes", "=", "num_classes", "\n", "self", ".", "deep_supervision", "=", "deep_supervision", "\n", "\"\"\"\n        We assume the bottleneck is part of the encoder, so we can start with upsample -> concat here\n        \"\"\"", "\n", "previous_stages", "=", "previous", ".", "stages", "\n", "previous_stage_output_features", "=", "previous", ".", "stage_output_features", "\n", "previous_stage_pool_kernel_size", "=", "previous", ".", "stage_pool_kernel_size", "\n", "previous_stage_conv_op_kernel_size", "=", "previous", ".", "stage_conv_op_kernel_size", "\n", "\n", "if", "network_props", "is", "None", ":", "\n", "            ", "self", ".", "props", "=", "previous", ".", "props", "\n", "", "else", ":", "\n", "            ", "self", ".", "props", "=", "network_props", "\n", "\n", "", "if", "self", ".", "props", "[", "'conv_op'", "]", "==", "nn", ".", "Conv2d", ":", "\n", "            ", "transpconv", "=", "nn", ".", "ConvTranspose2d", "\n", "upsample_mode", "=", "\"bilinear\"", "\n", "", "elif", "self", ".", "props", "[", "'conv_op'", "]", "==", "nn", ".", "Conv3d", ":", "\n", "            ", "transpconv", "=", "nn", ".", "ConvTranspose3d", "\n", "upsample_mode", "=", "\"trilinear\"", "\n", "", "else", ":", "\n", "            ", "raise", "ValueError", "(", "\"unknown convolution dimensionality, conv op: %s\"", "%", "str", "(", "self", ".", "props", "[", "'conv_op'", "]", ")", ")", "\n", "\n", "", "if", "num_blocks_per_stage", "is", "None", ":", "\n", "            ", "num_blocks_per_stage", "=", "previous", ".", "num_blocks_per_stage", "[", ":", "-", "1", "]", "[", ":", ":", "-", "1", "]", "\n", "\n", "", "assert", "len", "(", "num_blocks_per_stage", ")", "==", "len", "(", "previous", ".", "num_blocks_per_stage", ")", "-", "1", "\n", "\n", "self", ".", "stage_pool_kernel_size", "=", "previous_stage_pool_kernel_size", "\n", "self", ".", "stage_output_features", "=", "previous_stage_output_features", "\n", "self", ".", "stage_conv_op_kernel_size", "=", "previous_stage_conv_op_kernel_size", "\n", "\n", "num_stages", "=", "len", "(", "previous_stages", ")", "-", "1", "# we have one less as the first stage here is what comes after the", "\n", "# bottleneck", "\n", "\n", "self", ".", "tus", "=", "[", "]", "\n", "self", ".", "stages", "=", "[", "]", "\n", "self", ".", "deep_supervision_outputs", "=", "[", "]", "\n", "\n", "# only used for upsample_logits", "\n", "cum_upsample", "=", "np", ".", "cumprod", "(", "np", ".", "vstack", "(", "self", ".", "stage_pool_kernel_size", ")", ",", "axis", "=", "0", ")", ".", "astype", "(", "int", ")", "\n", "\n", "for", "i", ",", "s", "in", "enumerate", "(", "np", ".", "arange", "(", "num_stages", ")", "[", ":", ":", "-", "1", "]", ")", ":", "\n", "            ", "features_below", "=", "previous_stage_output_features", "[", "s", "+", "1", "]", "\n", "features_skip", "=", "previous_stage_output_features", "[", "s", "]", "\n", "\n", "self", ".", "tus", ".", "append", "(", "transpconv", "(", "features_below", ",", "features_skip", ",", "previous_stage_pool_kernel_size", "[", "s", "+", "1", "]", ",", "\n", "previous_stage_pool_kernel_size", "[", "s", "+", "1", "]", ",", "bias", "=", "False", ")", ")", "\n", "# after we tu we concat features so now we have 2xfeatures_skip", "\n", "self", ".", "stages", ".", "append", "(", "StackedConvLayers", "(", "2", "*", "features_skip", ",", "features_skip", ",", "\n", "previous_stage_conv_op_kernel_size", "[", "s", "]", ",", "self", ".", "props", ",", "\n", "num_blocks_per_stage", "[", "i", "]", ")", ")", "\n", "\n", "if", "deep_supervision", "and", "s", "!=", "0", ":", "\n", "                ", "seg_layer", "=", "self", ".", "props", "[", "'conv_op'", "]", "(", "features_skip", ",", "num_classes", ",", "1", ",", "1", ",", "0", ",", "1", ",", "1", ",", "False", ")", "\n", "if", "upscale_logits", ":", "\n", "                    ", "upsample", "=", "Upsample", "(", "scale_factor", "=", "cum_upsample", "[", "s", "]", ",", "mode", "=", "upsample_mode", ")", "\n", "self", ".", "deep_supervision_outputs", ".", "append", "(", "nn", ".", "Sequential", "(", "seg_layer", ",", "upsample", ")", ")", "\n", "", "else", ":", "\n", "                    ", "self", ".", "deep_supervision_outputs", ".", "append", "(", "seg_layer", ")", "\n", "\n", "", "", "", "self", ".", "segmentation_output", "=", "self", ".", "props", "[", "'conv_op'", "]", "(", "features_skip", ",", "num_classes", ",", "1", ",", "1", ",", "0", ",", "1", ",", "1", ",", "False", ")", "\n", "\n", "self", ".", "tus", "=", "nn", ".", "ModuleList", "(", "self", ".", "tus", ")", "\n", "self", ".", "stages", "=", "nn", ".", "ModuleList", "(", "self", ".", "stages", ")", "\n", "self", ".", "deep_supervision_outputs", "=", "nn", ".", "ModuleList", "(", "self", ".", "deep_supervision_outputs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.network_architecture.generic_modular_UNet.PlainConvUNetDecoder.forward": [[255, 285], ["range", "generic_modular_UNet.PlainConvUNetDecoder.segmentation_output", "len", "torch.cat", "seg_outputs.append", "seg_outputs.append", "loss", "loss", "len"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "skips", ",", "gt", "=", "None", ",", "loss", "=", "None", ")", ":", "\n", "# skips come from the encoder. They are sorted so that the bottleneck is last in the list", "\n", "# what is maybe not perfect is that the TUs and stages here are sorted the other way around", "\n", "# so let's just reverse the order of skips", "\n", "        ", "skips", "=", "skips", "[", ":", ":", "-", "1", "]", "\n", "seg_outputs", "=", "[", "]", "\n", "\n", "x", "=", "skips", "[", "0", "]", "# this is the bottleneck", "\n", "\n", "for", "i", "in", "range", "(", "len", "(", "self", ".", "tus", ")", ")", ":", "\n", "            ", "x", "=", "self", ".", "tus", "[", "i", "]", "(", "x", ")", "\n", "x", "=", "torch", ".", "cat", "(", "(", "x", ",", "skips", "[", "i", "+", "1", "]", ")", ",", "dim", "=", "1", ")", "\n", "x", "=", "self", ".", "stages", "[", "i", "]", "(", "x", ")", "\n", "if", "self", ".", "deep_supervision", "and", "(", "i", "!=", "len", "(", "self", ".", "tus", ")", "-", "1", ")", ":", "\n", "                ", "tmp", "=", "self", ".", "deep_supervision_outputs", "[", "i", "]", "(", "x", ")", "\n", "if", "gt", "is", "not", "None", ":", "\n", "                    ", "tmp", "=", "loss", "(", "tmp", ",", "gt", ")", "\n", "", "seg_outputs", ".", "append", "(", "tmp", ")", "\n", "\n", "", "", "segmentation", "=", "self", ".", "segmentation_output", "(", "x", ")", "\n", "\n", "if", "self", ".", "deep_supervision", ":", "\n", "            ", "tmp", "=", "segmentation", "\n", "if", "gt", "is", "not", "None", ":", "\n", "                ", "tmp", "=", "loss", "(", "tmp", ",", "gt", ")", "\n", "", "seg_outputs", ".", "append", "(", "tmp", ")", "\n", "return", "seg_outputs", "[", ":", ":", "-", "1", "]", "# seg_outputs are ordered so that the seg from the highest layer is first, the seg from", "\n", "# the bottleneck of the UNet last", "\n", "", "else", ":", "\n", "            ", "return", "segmentation", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.network_architecture.generic_modular_UNet.PlainConvUNetDecoder.compute_approx_vram_consumption": [[286, 315], ["numpy.array", "range", "len", "min", "print", "numpy.prod", "numpy.array", "numpy.prod", "numpy.prod"], "methods", ["None"], ["", "", "@", "staticmethod", "\n", "def", "compute_approx_vram_consumption", "(", "patch_size", ",", "base_num_features", ",", "max_num_features", ",", "\n", "num_classes", ",", "pool_op_kernel_sizes", ",", "num_blocks_per_stage_decoder", ",", "\n", "feat_map_mul_on_downscale", ",", "batch_size", ")", ":", "\n", "        ", "\"\"\"\n        This only applies for num_blocks_per_stage and convolutional_upsampling=True\n        not real vram consumption. just a constant term to which the vram consumption will be approx proportional\n        (+ offset for parameter storage)\n        :param patch_size:\n        :param num_pool_per_axis:\n        :param base_num_features:\n        :param max_num_features:\n        :return:\n        \"\"\"", "\n", "npool", "=", "len", "(", "pool_op_kernel_sizes", ")", "-", "1", "\n", "\n", "current_shape", "=", "np", ".", "array", "(", "patch_size", ")", "\n", "tmp", "=", "(", "num_blocks_per_stage_decoder", "[", "-", "1", "]", "+", "1", ")", "*", "np", ".", "prod", "(", "current_shape", ")", "*", "base_num_features", "+", "num_classes", "*", "np", ".", "prod", "(", "current_shape", ")", "\n", "\n", "num_feat", "=", "base_num_features", "\n", "\n", "for", "p", "in", "range", "(", "1", ",", "npool", ")", ":", "\n", "            ", "current_shape", "=", "current_shape", "/", "np", ".", "array", "(", "pool_op_kernel_sizes", "[", "p", "]", ")", "\n", "num_feat", "=", "min", "(", "num_feat", "*", "feat_map_mul_on_downscale", ",", "max_num_features", ")", "\n", "num_convs", "=", "num_blocks_per_stage_decoder", "[", "-", "(", "p", "+", "1", ")", "]", "+", "1", "\n", "print", "(", "p", ",", "num_feat", ",", "num_convs", ",", "current_shape", ")", "\n", "tmp", "+=", "num_convs", "*", "np", ".", "prod", "(", "current_shape", ")", "*", "num_feat", "\n", "\n", "", "return", "tmp", "*", "batch_size", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.network_architecture.generic_modular_UNet.PlainConvUNet.__init__": [[321, 335], ["nnunet.network_architecture.neural_network.SegmentationNetwork.__init__", "generic_modular_UNet.PlainConvUNetEncoder", "generic_modular_UNet.PlainConvUNetDecoder", "generic_modular_UNet.PlainConvUNet.apply"], "methods", ["home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.common.layers.ResizeImage.__init__"], ["def", "__init__", "(", "self", ",", "input_channels", ",", "base_num_features", ",", "num_blocks_per_stage_encoder", ",", "feat_map_mul_on_downscale", ",", "\n", "pool_op_kernel_sizes", ",", "conv_kernel_sizes", ",", "props", ",", "num_classes", ",", "num_blocks_per_stage_decoder", ",", "\n", "deep_supervision", "=", "False", ",", "upscale_logits", "=", "False", ",", "max_features", "=", "512", ",", "initializer", "=", "None", ")", ":", "\n", "        ", "super", "(", "PlainConvUNet", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "conv_op", "=", "props", "[", "'conv_op'", "]", "\n", "self", ".", "num_classes", "=", "num_classes", "\n", "\n", "self", ".", "encoder", "=", "PlainConvUNetEncoder", "(", "input_channels", ",", "base_num_features", ",", "num_blocks_per_stage_encoder", ",", "\n", "feat_map_mul_on_downscale", ",", "pool_op_kernel_sizes", ",", "conv_kernel_sizes", ",", "\n", "props", ",", "default_return_skips", "=", "True", ",", "max_num_features", "=", "max_features", ")", "\n", "self", ".", "decoder", "=", "PlainConvUNetDecoder", "(", "self", ".", "encoder", ",", "num_classes", ",", "num_blocks_per_stage_decoder", ",", "props", ",", "\n", "deep_supervision", ",", "upscale_logits", ")", "\n", "if", "initializer", "is", "not", "None", ":", "\n", "            ", "self", ".", "apply", "(", "initializer", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.network_architecture.generic_modular_UNet.PlainConvUNet.forward": [[336, 339], ["generic_modular_UNet.PlainConvUNet.encoder", "generic_modular_UNet.PlainConvUNet.decoder"], "methods", ["None"], ["", "", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "skips", "=", "self", ".", "encoder", "(", "x", ")", "\n", "return", "self", ".", "decoder", "(", "skips", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.network_architecture.generic_modular_UNet.PlainConvUNet.compute_approx_vram_consumption": [[340, 354], ["generic_modular_UNet.PlainConvUNetEncoder.compute_approx_vram_consumption", "generic_modular_UNet.PlainConvUNetDecoder.compute_approx_vram_consumption"], "methods", ["home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.network_architecture.generic_modular_UNet.PlainConvUNet.compute_approx_vram_consumption", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.network_architecture.generic_modular_UNet.PlainConvUNet.compute_approx_vram_consumption"], ["", "@", "staticmethod", "\n", "def", "compute_approx_vram_consumption", "(", "patch_size", ",", "base_num_features", ",", "max_num_features", ",", "\n", "num_modalities", ",", "num_classes", ",", "pool_op_kernel_sizes", ",", "num_blocks_per_stage_encoder", ",", "\n", "num_blocks_per_stage_decoder", ",", "feat_map_mul_on_downscale", ",", "batch_size", ")", ":", "\n", "        ", "enc", "=", "PlainConvUNetEncoder", ".", "compute_approx_vram_consumption", "(", "patch_size", ",", "base_num_features", ",", "max_num_features", ",", "\n", "num_modalities", ",", "pool_op_kernel_sizes", ",", "\n", "num_blocks_per_stage_encoder", ",", "\n", "feat_map_mul_on_downscale", ",", "batch_size", ")", "\n", "dec", "=", "PlainConvUNetDecoder", ".", "compute_approx_vram_consumption", "(", "patch_size", ",", "base_num_features", ",", "max_num_features", ",", "\n", "num_classes", ",", "pool_op_kernel_sizes", ",", "\n", "num_blocks_per_stage_decoder", ",", "\n", "feat_map_mul_on_downscale", ",", "batch_size", ")", "\n", "\n", "return", "enc", "+", "dec", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.network_architecture.generic_modular_UNet.PlainConvUNet.compute_reference_for_vram_consumption_3d": [[355, 369], ["generic_modular_UNet.PlainConvUNet.compute_approx_vram_consumption"], "methods", ["home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.network_architecture.generic_modular_UNet.PlainConvUNet.compute_approx_vram_consumption"], ["", "@", "staticmethod", "\n", "def", "compute_reference_for_vram_consumption_3d", "(", ")", ":", "\n", "        ", "patch_size", "=", "(", "160", ",", "128", ",", "128", ")", "\n", "pool_op_kernel_sizes", "=", "(", "(", "1", ",", "1", ",", "1", ")", ",", "\n", "(", "2", ",", "2", ",", "2", ")", ",", "\n", "(", "2", ",", "2", ",", "2", ")", ",", "\n", "(", "2", ",", "2", ",", "2", ")", ",", "\n", "(", "2", ",", "2", ",", "2", ")", ",", "\n", "(", "2", ",", "2", ",", "2", ")", ")", "\n", "conv_per_stage_encoder", "=", "(", "2", ",", "2", ",", "2", ",", "2", ",", "2", ",", "2", ")", "\n", "conv_per_stage_decoder", "=", "(", "2", ",", "2", ",", "2", ",", "2", ",", "2", ")", "\n", "\n", "return", "PlainConvUNet", ".", "compute_approx_vram_consumption", "(", "patch_size", ",", "32", ",", "512", ",", "4", ",", "3", ",", "pool_op_kernel_sizes", ",", "\n", "conv_per_stage_encoder", ",", "conv_per_stage_decoder", ",", "2", ",", "2", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.network_architecture.generic_modular_UNet.PlainConvUNet.compute_reference_for_vram_consumption_2d": [[370, 387], ["generic_modular_UNet.PlainConvUNet.compute_approx_vram_consumption"], "methods", ["home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.network_architecture.generic_modular_UNet.PlainConvUNet.compute_approx_vram_consumption"], ["", "@", "staticmethod", "\n", "def", "compute_reference_for_vram_consumption_2d", "(", ")", ":", "\n", "        ", "patch_size", "=", "(", "256", ",", "256", ")", "\n", "pool_op_kernel_sizes", "=", "(", "\n", "(", "1", ",", "1", ")", ",", "# (256, 256)", "\n", "(", "2", ",", "2", ")", ",", "# (128, 128)", "\n", "(", "2", ",", "2", ")", ",", "# (64, 64)", "\n", "(", "2", ",", "2", ")", ",", "# (32, 32)", "\n", "(", "2", ",", "2", ")", ",", "# (16, 16)", "\n", "(", "2", ",", "2", ")", ",", "# (8, 8)", "\n", "(", "2", ",", "2", ")", "# (4, 4)", "\n", ")", "\n", "conv_per_stage_encoder", "=", "(", "2", ",", "2", ",", "2", ",", "2", ",", "2", ",", "2", ",", "2", ")", "\n", "conv_per_stage_decoder", "=", "(", "2", ",", "2", ",", "2", ",", "2", ",", "2", ",", "2", ")", "\n", "\n", "return", "PlainConvUNet", ".", "compute_approx_vram_consumption", "(", "patch_size", ",", "32", ",", "512", ",", "4", ",", "3", ",", "pool_op_kernel_sizes", ",", "\n", "conv_per_stage_encoder", ",", "conv_per_stage_decoder", ",", "2", ",", "56", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.network_architecture.generic_modular_UNet.get_default_network_config": [[31, 79], ["None"], "function", ["None"], ["def", "get_default_network_config", "(", "dim", "=", "2", ",", "dropout_p", "=", "None", ",", "nonlin", "=", "\"LeakyReLU\"", ",", "norm_type", "=", "\"bn\"", ")", ":", "\n", "    ", "\"\"\"\n    returns a dictionary that contains pointers to conv, nonlin and norm ops and the default kwargs I like to use\n    :return:\n    \"\"\"", "\n", "props", "=", "{", "}", "\n", "if", "dim", "==", "2", ":", "\n", "        ", "props", "[", "'conv_op'", "]", "=", "nn", ".", "Conv2d", "\n", "props", "[", "'dropout_op'", "]", "=", "nn", ".", "Dropout2d", "\n", "", "elif", "dim", "==", "3", ":", "\n", "        ", "props", "[", "'conv_op'", "]", "=", "nn", ".", "Conv3d", "\n", "props", "[", "'dropout_op'", "]", "=", "nn", ".", "Dropout3d", "\n", "", "else", ":", "\n", "        ", "raise", "NotImplementedError", "\n", "\n", "", "if", "norm_type", "==", "\"bn\"", ":", "\n", "        ", "if", "dim", "==", "2", ":", "\n", "            ", "props", "[", "'norm_op'", "]", "=", "nn", ".", "BatchNorm2d", "\n", "", "elif", "dim", "==", "3", ":", "\n", "            ", "props", "[", "'norm_op'", "]", "=", "nn", ".", "BatchNorm3d", "\n", "", "props", "[", "'norm_op_kwargs'", "]", "=", "{", "'eps'", ":", "1e-5", ",", "'affine'", ":", "True", "}", "\n", "", "elif", "norm_type", "==", "\"in\"", ":", "\n", "        ", "if", "dim", "==", "2", ":", "\n", "            ", "props", "[", "'norm_op'", "]", "=", "nn", ".", "InstanceNorm2d", "\n", "", "elif", "dim", "==", "3", ":", "\n", "            ", "props", "[", "'norm_op'", "]", "=", "nn", ".", "InstanceNorm3d", "\n", "", "props", "[", "'norm_op_kwargs'", "]", "=", "{", "'eps'", ":", "1e-5", ",", "'affine'", ":", "True", "}", "\n", "", "else", ":", "\n", "        ", "raise", "NotImplementedError", "\n", "\n", "", "if", "dropout_p", "is", "None", ":", "\n", "        ", "props", "[", "'dropout_op'", "]", "=", "None", "\n", "props", "[", "'dropout_op_kwargs'", "]", "=", "{", "'p'", ":", "0", ",", "'inplace'", ":", "True", "}", "\n", "", "else", ":", "\n", "        ", "props", "[", "'dropout_op_kwargs'", "]", "=", "{", "'p'", ":", "dropout_p", ",", "'inplace'", ":", "True", "}", "\n", "\n", "", "props", "[", "'conv_op_kwargs'", "]", "=", "{", "'stride'", ":", "1", ",", "'dilation'", ":", "1", ",", "'bias'", ":", "True", "}", "# kernel size will be set by network!", "\n", "\n", "if", "nonlin", "==", "\"LeakyReLU\"", ":", "\n", "        ", "props", "[", "'nonlin'", "]", "=", "nn", ".", "LeakyReLU", "\n", "props", "[", "'nonlin_kwargs'", "]", "=", "{", "'negative_slope'", ":", "1e-2", ",", "'inplace'", ":", "True", "}", "\n", "", "elif", "nonlin", "==", "\"ReLU\"", ":", "\n", "        ", "props", "[", "'nonlin'", "]", "=", "nn", ".", "ReLU", "\n", "props", "[", "'nonlin_kwargs'", "]", "=", "{", "'inplace'", ":", "True", "}", "\n", "", "else", ":", "\n", "        ", "raise", "ValueError", "\n", "\n", "", "return", "props", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.custom_modules.conv_blocks.ConvDropoutNormReLU.__init__": [[22, 53], ["torch.nn.Module.__init__", "copy.deepcopy", "torch.nn.Sequential", "nnunet.network_architecture.custom_modules.helperModules.Identity", "nnunet.network_architecture.custom_modules.helperModules.Identity"], "methods", ["home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.common.layers.ResizeImage.__init__"], ["    ", "def", "__init__", "(", "self", ",", "input_channels", ",", "output_channels", ",", "kernel_size", ",", "network_props", ")", ":", "\n", "        ", "\"\"\"\n        if network_props['dropout_op'] is None then no dropout\n        if network_props['norm_op'] is None then no norm\n        :param input_channels:\n        :param output_channels:\n        :param kernel_size:\n        :param network_props:\n        \"\"\"", "\n", "super", "(", "ConvDropoutNormReLU", ",", "self", ")", ".", "__init__", "(", ")", "\n", "\n", "network_props", "=", "deepcopy", "(", "network_props", ")", "# network_props is a dict and mutable, so we deepcopy to be safe.", "\n", "\n", "self", ".", "conv", "=", "network_props", "[", "'conv_op'", "]", "(", "input_channels", ",", "output_channels", ",", "kernel_size", ",", "\n", "padding", "=", "[", "(", "i", "-", "1", ")", "//", "2", "for", "i", "in", "kernel_size", "]", ",", "\n", "**", "network_props", "[", "'conv_op_kwargs'", "]", ")", "\n", "\n", "# maybe dropout", "\n", "if", "network_props", "[", "'dropout_op'", "]", "is", "not", "None", ":", "\n", "            ", "self", ".", "do", "=", "network_props", "[", "'dropout_op'", "]", "(", "**", "network_props", "[", "'dropout_op_kwargs'", "]", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "do", "=", "Identity", "(", ")", "\n", "\n", "", "if", "network_props", "[", "'norm_op'", "]", "is", "not", "None", ":", "\n", "            ", "self", ".", "norm", "=", "network_props", "[", "'norm_op'", "]", "(", "output_channels", ",", "**", "network_props", "[", "'norm_op_kwargs'", "]", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "norm", "=", "Identity", "(", ")", "\n", "\n", "", "self", ".", "nonlin", "=", "network_props", "[", "'nonlin'", "]", "(", "**", "network_props", "[", "'nonlin_kwargs'", "]", ")", "\n", "\n", "self", ".", "all", "=", "nn", ".", "Sequential", "(", "self", ".", "conv", ",", "self", ".", "do", ",", "self", ".", "norm", ",", "self", ".", "nonlin", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.custom_modules.conv_blocks.ConvDropoutNormReLU.forward": [[54, 56], ["conv_blocks.ConvDropoutNormReLU.all"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "return", "self", ".", "all", "(", "x", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.custom_modules.conv_blocks.StackedConvLayers.__init__": [[59, 80], ["torch.nn.Module.__init__", "copy.deepcopy", "copy.deepcopy", "torch.nn.Sequential", "conv_blocks.ConvDropoutNormReLU", "conv_blocks.ConvDropoutNormReLU", "range"], "methods", ["home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.common.layers.ResizeImage.__init__"], ["    ", "def", "__init__", "(", "self", ",", "input_channels", ",", "output_channels", ",", "kernel_size", ",", "network_props", ",", "num_convs", ",", "first_stride", "=", "None", ")", ":", "\n", "        ", "\"\"\"\n        if network_props['dropout_op'] is None then no dropout\n        if network_props['norm_op'] is None then no norm\n        :param input_channels:\n        :param output_channels:\n        :param kernel_size:\n        :param network_props:\n        \"\"\"", "\n", "super", "(", "StackedConvLayers", ",", "self", ")", ".", "__init__", "(", ")", "\n", "\n", "network_props", "=", "deepcopy", "(", "network_props", ")", "# network_props is a dict and mutable, so we deepcopy to be safe.", "\n", "network_props_first", "=", "deepcopy", "(", "network_props", ")", "\n", "\n", "if", "first_stride", "is", "not", "None", ":", "\n", "            ", "network_props_first", "[", "'conv_op_kwargs'", "]", "[", "'stride'", "]", "=", "first_stride", "\n", "\n", "", "self", ".", "convs", "=", "nn", ".", "Sequential", "(", "\n", "ConvDropoutNormReLU", "(", "input_channels", ",", "output_channels", ",", "kernel_size", ",", "network_props_first", ")", ",", "\n", "*", "[", "ConvDropoutNormReLU", "(", "output_channels", ",", "output_channels", ",", "kernel_size", ",", "network_props", ")", "for", "_", "in", "\n", "range", "(", "num_convs", "-", "1", ")", "]", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.custom_modules.conv_blocks.StackedConvLayers.forward": [[82, 84], ["conv_blocks.StackedConvLayers.convs"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "return", "self", ".", "convs", "(", "x", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.custom_modules.conv_blocks.BasicResidualBlock.__init__": [[87, 132], ["torch.nn.Module.__init__", "copy.deepcopy", "nnunet.network_architecture.custom_modules.helperModules.Identity", "torch.nn.Sequential", "any"], "methods", ["home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.common.layers.ResizeImage.__init__"], ["    ", "def", "__init__", "(", "self", ",", "in_planes", ",", "out_planes", ",", "kernel_size", ",", "props", ",", "stride", "=", "None", ")", ":", "\n", "        ", "\"\"\"\n        This is the conv bn nonlin conv bn nonlin kind of block\n        :param in_planes:\n        :param out_planes:\n        :param props:\n        :param override_stride:\n        \"\"\"", "\n", "super", "(", ")", ".", "__init__", "(", ")", "\n", "\n", "self", ".", "kernel_size", "=", "kernel_size", "\n", "props", "[", "'conv_op_kwargs'", "]", "[", "'stride'", "]", "=", "1", "\n", "\n", "self", ".", "stride", "=", "stride", "\n", "self", ".", "props", "=", "props", "\n", "self", ".", "out_planes", "=", "out_planes", "\n", "self", ".", "in_planes", "=", "in_planes", "\n", "\n", "if", "stride", "is", "not", "None", ":", "\n", "            ", "kwargs_conv1", "=", "deepcopy", "(", "props", "[", "'conv_op_kwargs'", "]", ")", "\n", "kwargs_conv1", "[", "'stride'", "]", "=", "stride", "\n", "", "else", ":", "\n", "            ", "kwargs_conv1", "=", "props", "[", "'conv_op_kwargs'", "]", "\n", "\n", "", "self", ".", "conv1", "=", "props", "[", "'conv_op'", "]", "(", "in_planes", ",", "out_planes", ",", "kernel_size", ",", "padding", "=", "[", "(", "i", "-", "1", ")", "//", "2", "for", "i", "in", "kernel_size", "]", ",", "\n", "**", "kwargs_conv1", ")", "\n", "self", ".", "norm1", "=", "props", "[", "'norm_op'", "]", "(", "out_planes", ",", "**", "props", "[", "'norm_op_kwargs'", "]", ")", "\n", "self", ".", "nonlin1", "=", "props", "[", "'nonlin'", "]", "(", "**", "props", "[", "'nonlin_kwargs'", "]", ")", "\n", "\n", "if", "props", "[", "'dropout_op_kwargs'", "]", "[", "'p'", "]", "!=", "0", ":", "\n", "            ", "self", ".", "dropout", "=", "props", "[", "'dropout_op'", "]", "(", "**", "props", "[", "'dropout_op_kwargs'", "]", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "dropout", "=", "Identity", "(", ")", "\n", "\n", "", "self", ".", "conv2", "=", "props", "[", "'conv_op'", "]", "(", "out_planes", ",", "out_planes", ",", "kernel_size", ",", "padding", "=", "[", "(", "i", "-", "1", ")", "//", "2", "for", "i", "in", "kernel_size", "]", ",", "\n", "**", "props", "[", "'conv_op_kwargs'", "]", ")", "\n", "self", ".", "norm2", "=", "props", "[", "'norm_op'", "]", "(", "out_planes", ",", "**", "props", "[", "'norm_op_kwargs'", "]", ")", "\n", "self", ".", "nonlin2", "=", "props", "[", "'nonlin'", "]", "(", "**", "props", "[", "'nonlin_kwargs'", "]", ")", "\n", "\n", "if", "(", "self", ".", "stride", "is", "not", "None", "and", "any", "(", "(", "i", "!=", "1", "for", "i", "in", "self", ".", "stride", ")", ")", ")", "or", "(", "in_planes", "!=", "out_planes", ")", ":", "\n", "            ", "stride_here", "=", "stride", "if", "stride", "is", "not", "None", "else", "1", "\n", "self", ".", "downsample_skip", "=", "nn", ".", "Sequential", "(", "props", "[", "'conv_op'", "]", "(", "in_planes", ",", "out_planes", ",", "1", ",", "stride_here", ",", "bias", "=", "False", ")", ",", "\n", "props", "[", "'norm_op'", "]", "(", "out_planes", ",", "**", "props", "[", "'norm_op_kwargs'", "]", ")", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "downsample_skip", "=", "lambda", "x", ":", "x", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.custom_modules.conv_blocks.BasicResidualBlock.forward": [[133, 146], ["conv_blocks.BasicResidualBlock.dropout", "conv_blocks.BasicResidualBlock.nonlin1", "conv_blocks.BasicResidualBlock.norm2", "conv_blocks.BasicResidualBlock.downsample_skip", "conv_blocks.BasicResidualBlock.nonlin2", "conv_blocks.BasicResidualBlock.conv1", "conv_blocks.BasicResidualBlock.norm1", "conv_blocks.BasicResidualBlock.conv2"], "methods", ["None"], ["", "", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "residual", "=", "x", "\n", "\n", "out", "=", "self", ".", "dropout", "(", "self", ".", "conv1", "(", "x", ")", ")", "\n", "out", "=", "self", ".", "nonlin1", "(", "self", ".", "norm1", "(", "out", ")", ")", "\n", "\n", "out", "=", "self", ".", "norm2", "(", "self", ".", "conv2", "(", "out", ")", ")", "\n", "\n", "residual", "=", "self", ".", "downsample_skip", "(", "residual", ")", "\n", "\n", "out", "+=", "residual", "\n", "\n", "return", "self", ".", "nonlin2", "(", "out", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.custom_modules.conv_blocks.ResidualBottleneckBlock.__init__": [[149, 198], ["torch.nn.Module.__init__", "NotImplementedError", "copy.deepcopy", "torch.nn.Sequential", "any"], "methods", ["home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.common.layers.ResizeImage.__init__"], ["    ", "def", "__init__", "(", "self", ",", "in_planes", ",", "out_planes", ",", "kernel_size", ",", "props", ",", "stride", "=", "None", ")", ":", "\n", "        ", "\"\"\"\n        This is the conv bn nonlin conv bn nonlin kind of block\n        :param in_planes:\n        :param out_planes:\n        :param props:\n        :param override_stride:\n        \"\"\"", "\n", "super", "(", ")", ".", "__init__", "(", ")", "\n", "\n", "if", "props", "[", "'dropout_op_kwargs'", "]", "is", "None", "and", "props", "[", "'dropout_op_kwargs'", "]", ">", "0", ":", "\n", "            ", "raise", "NotImplementedError", "(", "\"ResidualBottleneckBlock does not yet support dropout!\"", ")", "\n", "\n", "", "self", ".", "kernel_size", "=", "kernel_size", "\n", "props", "[", "'conv_op_kwargs'", "]", "[", "'stride'", "]", "=", "1", "\n", "\n", "self", ".", "stride", "=", "stride", "\n", "self", ".", "props", "=", "props", "\n", "self", ".", "out_planes", "=", "out_planes", "\n", "self", ".", "in_planes", "=", "in_planes", "\n", "self", ".", "bottleneck_planes", "=", "out_planes", "//", "4", "\n", "\n", "if", "stride", "is", "not", "None", ":", "\n", "            ", "kwargs_conv1", "=", "deepcopy", "(", "props", "[", "'conv_op_kwargs'", "]", ")", "\n", "kwargs_conv1", "[", "'stride'", "]", "=", "stride", "\n", "", "else", ":", "\n", "            ", "kwargs_conv1", "=", "props", "[", "'conv_op_kwargs'", "]", "\n", "\n", "", "self", ".", "conv1", "=", "props", "[", "'conv_op'", "]", "(", "in_planes", ",", "self", ".", "bottleneck_planes", ",", "[", "1", "for", "_", "in", "kernel_size", "]", ",", "padding", "=", "[", "0", "for", "i", "in", "kernel_size", "]", ",", "\n", "**", "kwargs_conv1", ")", "\n", "self", ".", "norm1", "=", "props", "[", "'norm_op'", "]", "(", "self", ".", "bottleneck_planes", ",", "**", "props", "[", "'norm_op_kwargs'", "]", ")", "\n", "self", ".", "nonlin1", "=", "props", "[", "'nonlin'", "]", "(", "**", "props", "[", "'nonlin_kwargs'", "]", ")", "\n", "\n", "self", ".", "conv2", "=", "props", "[", "'conv_op'", "]", "(", "self", ".", "bottleneck_planes", ",", "self", ".", "bottleneck_planes", ",", "kernel_size", ",", "padding", "=", "[", "(", "i", "-", "1", ")", "//", "2", "for", "i", "in", "kernel_size", "]", ",", "\n", "**", "props", "[", "'conv_op_kwargs'", "]", ")", "\n", "self", ".", "norm2", "=", "props", "[", "'norm_op'", "]", "(", "self", ".", "bottleneck_planes", ",", "**", "props", "[", "'norm_op_kwargs'", "]", ")", "\n", "self", ".", "nonlin2", "=", "props", "[", "'nonlin'", "]", "(", "**", "props", "[", "'nonlin_kwargs'", "]", ")", "\n", "\n", "self", ".", "conv3", "=", "props", "[", "'conv_op'", "]", "(", "self", ".", "bottleneck_planes", ",", "out_planes", ",", "[", "1", "for", "_", "in", "kernel_size", "]", ",", "padding", "=", "[", "0", "for", "i", "in", "kernel_size", "]", ",", "\n", "**", "props", "[", "'conv_op_kwargs'", "]", ")", "\n", "self", ".", "norm3", "=", "props", "[", "'norm_op'", "]", "(", "out_planes", ",", "**", "props", "[", "'norm_op_kwargs'", "]", ")", "\n", "self", ".", "nonlin3", "=", "props", "[", "'nonlin'", "]", "(", "**", "props", "[", "'nonlin_kwargs'", "]", ")", "\n", "\n", "if", "(", "self", ".", "stride", "is", "not", "None", "and", "any", "(", "(", "i", "!=", "1", "for", "i", "in", "self", ".", "stride", ")", ")", ")", "or", "(", "in_planes", "!=", "out_planes", ")", ":", "\n", "            ", "stride_here", "=", "stride", "if", "stride", "is", "not", "None", "else", "1", "\n", "self", ".", "downsample_skip", "=", "nn", ".", "Sequential", "(", "props", "[", "'conv_op'", "]", "(", "in_planes", ",", "out_planes", ",", "1", ",", "stride_here", ",", "bias", "=", "False", ")", ",", "\n", "props", "[", "'norm_op'", "]", "(", "out_planes", ",", "**", "props", "[", "'norm_op_kwargs'", "]", ")", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "downsample_skip", "=", "lambda", "x", ":", "x", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.custom_modules.conv_blocks.ResidualBottleneckBlock.forward": [[199, 212], ["conv_blocks.ResidualBottleneckBlock.nonlin1", "conv_blocks.ResidualBottleneckBlock.nonlin2", "conv_blocks.ResidualBottleneckBlock.norm3", "conv_blocks.ResidualBottleneckBlock.downsample_skip", "conv_blocks.ResidualBottleneckBlock.nonlin3", "conv_blocks.ResidualBottleneckBlock.norm1", "conv_blocks.ResidualBottleneckBlock.norm2", "conv_blocks.ResidualBottleneckBlock.conv3", "conv_blocks.ResidualBottleneckBlock.conv1", "conv_blocks.ResidualBottleneckBlock.conv2"], "methods", ["None"], ["", "", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "residual", "=", "x", "\n", "\n", "out", "=", "self", ".", "nonlin1", "(", "self", ".", "norm1", "(", "self", ".", "conv1", "(", "x", ")", ")", ")", "\n", "out", "=", "self", ".", "nonlin2", "(", "self", ".", "norm2", "(", "self", ".", "conv2", "(", "out", ")", ")", ")", "\n", "\n", "out", "=", "self", ".", "norm3", "(", "self", ".", "conv3", "(", "out", ")", ")", "\n", "\n", "residual", "=", "self", ".", "downsample_skip", "(", "residual", ")", "\n", "\n", "out", "+=", "residual", "\n", "\n", "return", "self", ".", "nonlin3", "(", "out", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.custom_modules.conv_blocks.ResidualLayer.__init__": [[215, 224], ["torch.nn.Module.__init__", "copy.deepcopy", "torch.nn.Sequential", "block", "block", "range"], "methods", ["home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.common.layers.ResizeImage.__init__"], ["    ", "def", "__init__", "(", "self", ",", "input_channels", ",", "output_channels", ",", "kernel_size", ",", "network_props", ",", "num_blocks", ",", "first_stride", "=", "None", ",", "block", "=", "BasicResidualBlock", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "\n", "network_props", "=", "deepcopy", "(", "network_props", ")", "# network_props is a dict and mutable, so we deepcopy to be safe.", "\n", "\n", "self", ".", "convs", "=", "nn", ".", "Sequential", "(", "\n", "block", "(", "input_channels", ",", "output_channels", ",", "kernel_size", ",", "network_props", ",", "first_stride", ")", ",", "\n", "*", "[", "block", "(", "output_channels", ",", "output_channels", ",", "kernel_size", ",", "network_props", ")", "for", "_", "in", "\n", "range", "(", "num_blocks", "-", "1", ")", "]", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.custom_modules.conv_blocks.ResidualLayer.forward": [[226, 228], ["conv_blocks.ResidualLayer.convs"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "return", "self", ".", "convs", "(", "x", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.custom_modules.helperModules.Identity.__init__": [[20, 22], ["torch.nn.Module.__init__"], "methods", ["home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.common.layers.ResizeImage.__init__"], ["    ", "def", "__init__", "(", "self", ",", "*", "args", ",", "**", "kwargs", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.custom_modules.helperModules.Identity.forward": [[23, 25], ["None"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "input", ")", ":", "\n", "        ", "return", "input", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.custom_modules.helperModules.MyGroupNorm.__init__": [[28, 30], ["torch.nn.GroupNorm.__init__"], "methods", ["home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.common.layers.ResizeImage.__init__"], ["    ", "def", "__init__", "(", "self", ",", "num_channels", ",", "eps", "=", "1e-5", ",", "affine", "=", "True", ",", "num_groups", "=", "8", ")", ":", "\n", "        ", "super", "(", "MyGroupNorm", ",", "self", ")", ".", "__init__", "(", "num_groups", ",", "num_channels", ",", "eps", ",", "affine", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.custom_modules.mish.Mish.__init__": [[18, 20], ["torch.Module.__init__"], "methods", ["home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.common.layers.ResizeImage.__init__"], ["    ", "def", "__init__", "(", "self", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.custom_modules.mish.Mish.forward": [[21, 24], ["torch.tanh", "torch.tanh", "torch.tanh", "torch.tanh", "torch.tanh", "torch.tanh", "torch.tanh", "torch.tanh", "torch.tanh", "torch.softplus", "torch.softplus", "torch.softplus"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "# inlining this saves 1 second per epoch (V100 GPU) vs having a temp x and then returning x(!)", "\n", "        ", "return", "x", "*", "(", "torch", ".", "tanh", "(", "F", ".", "softplus", "(", "x", ")", ")", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.custom_modules.feature_response_normalization.FRN3D.__init__": [[24, 31], ["torch.nn.Module.__init__", "torch.nn.parameter.Parameter", "torch.nn.parameter.Parameter", "torch.nn.parameter.Parameter", "torch.nn.parameter.Parameter", "torch.nn.parameter.Parameter", "torch.nn.parameter.Parameter", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros"], "methods", ["home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.common.layers.ResizeImage.__init__"], ["    ", "def", "__init__", "(", "self", ",", "num_features", ":", "int", ",", "eps", "=", "1e-6", ",", "**", "kwargs", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "eps", "=", "eps", "\n", "self", ".", "num_features", "=", "num_features", "\n", "self", ".", "weight", "=", "Parameter", "(", "torch", ".", "ones", "(", "1", ",", "num_features", ",", "1", ",", "1", ",", "1", ")", ",", "True", ")", "\n", "self", ".", "bias", "=", "Parameter", "(", "torch", ".", "zeros", "(", "1", ",", "num_features", ",", "1", ",", "1", ",", "1", ")", ",", "True", ")", "\n", "self", ".", "tau", "=", "Parameter", "(", "torch", ".", "zeros", "(", "1", ",", "num_features", ",", "1", ",", "1", ",", "1", ")", ",", "True", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.custom_modules.feature_response_normalization.FRN3D.forward": [[32, 36], ["torch.max", "torch.max", "torch.max", "torch.max", "torch.rsqrt", "torch.rsqrt", "torch.rsqrt", "torch.rsqrt", "nnunet.utilities.tensor_utilities.mean_tensor"], "methods", ["home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.utilities.tensor_utilities.mean_tensor"], ["", "def", "forward", "(", "self", ",", "x", ":", "torch", ".", "Tensor", ")", ":", "\n", "        ", "x", "=", "x", "*", "torch", ".", "rsqrt", "(", "mean_tensor", "(", "x", "*", "x", ",", "[", "2", ",", "3", ",", "4", "]", ",", "keepdim", "=", "True", ")", "+", "self", ".", "eps", ")", "\n", "\n", "return", "torch", ".", "max", "(", "self", ".", "weight", "*", "x", "+", "self", ".", "bias", ",", "self", ".", "tau", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.postprocessing.connected_components.load_remove_save": [[30, 46], ["SimpleITK.ReadImage", "SimpleITK.GetArrayFromImage", "float", "connected_components.remove_all_but_the_largest_connected_component", "SimpleITK.GetImageFromArray", "nnunet.utilities.sitk_stuff.copy_geometry", "SimpleITK.WriteImage", "numpy.prod", "sitk.ReadImage.GetSpacing"], "function", ["home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.postprocessing.connected_components.remove_all_but_the_largest_connected_component", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.utilities.sitk_stuff.copy_geometry"], ["def", "load_remove_save", "(", "input_file", ":", "str", ",", "output_file", ":", "str", ",", "for_which_classes", ":", "list", ",", "\n", "minimum_valid_object_size", ":", "dict", "=", "None", ")", ":", "\n", "# Only objects larger than minimum_valid_object_size will be removed. Keys in minimum_valid_object_size must", "\n", "# match entries in for_which_classes", "\n", "    ", "img_in", "=", "sitk", ".", "ReadImage", "(", "input_file", ")", "\n", "img_npy", "=", "sitk", ".", "GetArrayFromImage", "(", "img_in", ")", "\n", "volume_per_voxel", "=", "float", "(", "np", ".", "prod", "(", "img_in", ".", "GetSpacing", "(", ")", ",", "dtype", "=", "np", ".", "float64", ")", ")", "\n", "\n", "image", ",", "largest_removed", ",", "kept_size", "=", "remove_all_but_the_largest_connected_component", "(", "img_npy", ",", "for_which_classes", ",", "\n", "volume_per_voxel", ",", "\n", "minimum_valid_object_size", ")", "\n", "# print(input_file, \"kept:\", kept_size)", "\n", "img_out_itk", "=", "sitk", ".", "GetImageFromArray", "(", "image", ")", "\n", "img_out_itk", "=", "copy_geometry", "(", "img_out_itk", ",", "img_in", ")", "\n", "sitk", ".", "WriteImage", "(", "img_out_itk", ",", "output_file", ")", "\n", "return", "largest_removed", ",", "kept_size", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.postprocessing.connected_components.remove_all_but_the_largest_connected_component": [[48, 106], ["numpy.unique", "isinstance", "scipy.ndimage.label", "range", "tuple", "numpy.zeros_like", "np.zeros_like.astype", "max", "range", "object_sizes.values", "max"], "function", ["None"], ["", "def", "remove_all_but_the_largest_connected_component", "(", "image", ":", "np", ".", "ndarray", ",", "for_which_classes", ":", "list", ",", "volume_per_voxel", ":", "float", ",", "\n", "minimum_valid_object_size", ":", "dict", "=", "None", ")", ":", "\n", "    ", "\"\"\"\n    removes all but the largest connected component, individually for each class\n    :param image:\n    :param for_which_classes: can be None. Should be list of int. Can also be something like [(1, 2), 2, 4].\n    Here (1, 2) will be treated as a joint region, not individual classes (example LiTS here we can use (1, 2)\n    to use all foreground classes together)\n    :param minimum_valid_object_size: Only objects larger than minimum_valid_object_size will be removed. Keys in\n    minimum_valid_object_size must match entries in for_which_classes\n    :return:\n    \"\"\"", "\n", "if", "for_which_classes", "is", "None", ":", "\n", "        ", "for_which_classes", "=", "np", ".", "unique", "(", "image", ")", "\n", "for_which_classes", "=", "for_which_classes", "[", "for_which_classes", ">", "0", "]", "\n", "\n", "", "assert", "0", "not", "in", "for_which_classes", ",", "\"cannot remove background\"", "\n", "largest_removed", "=", "{", "}", "\n", "kept_size", "=", "{", "}", "\n", "for", "c", "in", "for_which_classes", ":", "\n", "        ", "if", "isinstance", "(", "c", ",", "(", "list", ",", "tuple", ")", ")", ":", "\n", "            ", "c", "=", "tuple", "(", "c", ")", "# otherwise it cant be used as key in the dict", "\n", "mask", "=", "np", ".", "zeros_like", "(", "image", ",", "dtype", "=", "bool", ")", "\n", "for", "cl", "in", "c", ":", "\n", "                ", "mask", "[", "image", "==", "cl", "]", "=", "True", "\n", "", "", "else", ":", "\n", "            ", "mask", "=", "image", "==", "c", "\n", "# get labelmap and number of objects", "\n", "", "lmap", ",", "num_objects", "=", "label", "(", "mask", ".", "astype", "(", "int", ")", ")", "\n", "\n", "# collect object sizes", "\n", "object_sizes", "=", "{", "}", "\n", "for", "object_id", "in", "range", "(", "1", ",", "num_objects", "+", "1", ")", ":", "\n", "            ", "object_sizes", "[", "object_id", "]", "=", "(", "lmap", "==", "object_id", ")", ".", "sum", "(", ")", "*", "volume_per_voxel", "\n", "\n", "", "largest_removed", "[", "c", "]", "=", "None", "\n", "kept_size", "[", "c", "]", "=", "None", "\n", "\n", "if", "num_objects", ">", "0", ":", "\n", "# we always keep the largest object. We could also consider removing the largest object if it is smaller", "\n", "# than minimum_valid_object_size in the future but we don't do that now.", "\n", "            ", "maximum_size", "=", "max", "(", "object_sizes", ".", "values", "(", ")", ")", "\n", "kept_size", "[", "c", "]", "=", "maximum_size", "\n", "\n", "for", "object_id", "in", "range", "(", "1", ",", "num_objects", "+", "1", ")", ":", "\n", "# we only remove objects that are not the largest", "\n", "                ", "if", "object_sizes", "[", "object_id", "]", "!=", "maximum_size", ":", "\n", "# we only remove objects that are smaller than minimum_valid_object_size", "\n", "                    ", "remove", "=", "True", "\n", "if", "minimum_valid_object_size", "is", "not", "None", ":", "\n", "                        ", "remove", "=", "object_sizes", "[", "object_id", "]", "<", "minimum_valid_object_size", "[", "c", "]", "\n", "", "if", "remove", ":", "\n", "                        ", "image", "[", "(", "lmap", "==", "object_id", ")", "&", "mask", "]", "=", "0", "\n", "if", "largest_removed", "[", "c", "]", "is", "None", ":", "\n", "                            ", "largest_removed", "[", "c", "]", "=", "object_sizes", "[", "object_id", "]", "\n", "", "else", ":", "\n", "                            ", "largest_removed", "[", "c", "]", "=", "max", "(", "largest_removed", "[", "c", "]", ",", "object_sizes", "[", "object_id", "]", ")", "\n", "", "", "", "", "", "", "return", "image", ",", "largest_removed", ",", "kept_size", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.postprocessing.connected_components.load_postprocessing": [[108, 120], ["load_json", "load_json.keys", "ast.literal_eval"], "function", ["None"], ["", "def", "load_postprocessing", "(", "json_file", ")", ":", "\n", "    ", "'''\n    loads the relevant part of the pkl file that is needed for applying postprocessing\n    :param pkl_file:\n    :return:\n    '''", "\n", "a", "=", "load_json", "(", "json_file", ")", "\n", "if", "'min_valid_object_sizes'", "in", "a", ".", "keys", "(", ")", ":", "\n", "        ", "min_valid_object_sizes", "=", "ast", ".", "literal_eval", "(", "a", "[", "'min_valid_object_sizes'", "]", ")", "\n", "", "else", ":", "\n", "        ", "min_valid_object_sizes", "=", "None", "\n", "", "return", "a", "[", "'for_which_classes'", "]", ",", "min_valid_object_sizes", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.postprocessing.connected_components.determine_postprocessing": [[122, 398], ["join", "join", "isdir", "isdir", "multiprocessing.pool.Pool", "isfile", "subfiles", "maybe_mkdir_p", "maybe_mkdir_p", "maybe_mkdir_p", "len", "nnunet.evaluation.evaluator.aggregate_scores", "numpy.mean", "numpy.mean", "print", "print", "print", "any", "print", "print", "print", "print", "print", "nnunet.evaluation.evaluator.aggregate_scores", "str", "save_json", "multiprocessing.pool.Pool.close", "multiprocessing.pool.Pool.join", "print", "int", "shutil.rmtree", "shutil.rmtree", "join", "join", "join", "load_json", "print", "print", "join", "join", "results.append", "pred_gt_tuples.append", "i.get", "any", "len", "nnunet.evaluation.evaluator.aggregate_scores", "print", "join", "join", "results.append", "pred_gt_tuples.append", "i.get", "join", "shutil.rmtree", "shutil.rmtree", "[].keys", "join", "join", "join", "results.append", "i.get", "multiprocessing.pool.Pool.starmap_async", "join", "load_json", "pp_results[].append", "print", "print", "print", "join", "print", "print", "print", "join", "join", "results.append", "pred_gt_tuples.append", "i.get", "copy.deepcopy", "print", "print", "print", "multiprocessing.pool.Pool.starmap_async", "join", "int", "multiprocessing.pool.Pool.starmap_async", "join", "join", "str", "str", "pp_results[].update", "join", "join", "results.append", "i.get", "multiprocessing.pool.Pool.starmap_async", "join", "load_json", "pp_results[].append", "print", "print", "join", "tuple", "str", "str", "str", "str", "str", "copy.deepcopy", "multiprocessing.pool.Pool.starmap_async", "join", "join", "str", "int", "pp_results[].update", "max_size_removed.get", "max", "min_size_kept.get", "min", "str", "str", "str", "load_json", "str", "str", "max_size_removed.get", "max", "min_size_kept.get", "min", "join"], "function", ["home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.evaluation.evaluator.aggregate_scores", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.evaluation.evaluator.aggregate_scores", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.evaluation.evaluator.aggregate_scores"], ["", "def", "determine_postprocessing", "(", "base", ",", "gt_labels_folder", ",", "raw_subfolder_name", "=", "\"validation_raw\"", ",", "\n", "temp_folder", "=", "\"temp\"", ",", "\n", "final_subf_name", "=", "\"validation_final\"", ",", "processes", "=", "default_num_threads", ",", "\n", "dice_threshold", "=", "0", ",", "debug", "=", "False", ",", "\n", "advanced_postprocessing", "=", "False", ",", "\n", "pp_filename", "=", "\"postprocessing.json\"", ")", ":", "\n", "    ", "\"\"\"\n    :param base:\n    :param gt_labels_folder: subfolder of base with niftis of ground truth labels\n    :param raw_subfolder_name: subfolder of base with niftis of predicted (non-postprocessed) segmentations\n    :param temp_folder: used to store temporary data, will be deleted after we are done here undless debug=True\n    :param final_subf_name: final results will be stored here (subfolder of base)\n    :param processes:\n    :param dice_threshold: only apply postprocessing if results is better than old_result+dice_threshold (can be used as eps)\n    :param debug: if True then the temporary files will not be deleted\n    :return:\n    \"\"\"", "\n", "# lets see what classes are in the dataset", "\n", "classes", "=", "[", "int", "(", "i", ")", "for", "i", "in", "load_json", "(", "join", "(", "base", ",", "raw_subfolder_name", ",", "\"summary.json\"", ")", ")", "[", "'results'", "]", "[", "'mean'", "]", ".", "keys", "(", ")", "if", "\n", "int", "(", "i", ")", "!=", "0", "]", "\n", "\n", "folder_all_classes_as_fg", "=", "join", "(", "base", ",", "temp_folder", "+", "\"_allClasses\"", ")", "\n", "folder_per_class", "=", "join", "(", "base", ",", "temp_folder", "+", "\"_perClass\"", ")", "\n", "\n", "if", "isdir", "(", "folder_all_classes_as_fg", ")", ":", "\n", "        ", "shutil", ".", "rmtree", "(", "folder_all_classes_as_fg", ")", "\n", "", "if", "isdir", "(", "folder_per_class", ")", ":", "\n", "        ", "shutil", ".", "rmtree", "(", "folder_per_class", ")", "\n", "\n", "# multiprocessing rules", "\n", "", "p", "=", "Pool", "(", "processes", ")", "\n", "\n", "assert", "isfile", "(", "join", "(", "base", ",", "raw_subfolder_name", ",", "\"summary.json\"", ")", ")", ",", "\"join(base, raw_subfolder_name) does not \"", "\"contain a summary.json\"", "\n", "\n", "# these are all the files we will be dealing with", "\n", "fnames", "=", "subfiles", "(", "join", "(", "base", ",", "raw_subfolder_name", ")", ",", "suffix", "=", "\".nii.gz\"", ",", "join", "=", "False", ")", "\n", "\n", "# make output and temp dir", "\n", "maybe_mkdir_p", "(", "folder_all_classes_as_fg", ")", "\n", "maybe_mkdir_p", "(", "folder_per_class", ")", "\n", "maybe_mkdir_p", "(", "join", "(", "base", ",", "final_subf_name", ")", ")", "\n", "\n", "pp_results", "=", "{", "}", "\n", "pp_results", "[", "'dc_per_class_raw'", "]", "=", "{", "}", "\n", "pp_results", "[", "'dc_per_class_pp_all'", "]", "=", "{", "}", "# dice scores after treating all foreground classes as one", "\n", "pp_results", "[", "'dc_per_class_pp_per_class'", "]", "=", "{", "}", "# dice scores after removing everything except larges cc", "\n", "# independently for each class after we already did dc_per_class_pp_all", "\n", "pp_results", "[", "'for_which_classes'", "]", "=", "[", "]", "\n", "pp_results", "[", "'min_valid_object_sizes'", "]", "=", "{", "}", "\n", "\n", "\n", "validation_result_raw", "=", "load_json", "(", "join", "(", "base", ",", "raw_subfolder_name", ",", "\"summary.json\"", ")", ")", "[", "'results'", "]", "\n", "pp_results", "[", "'num_samples'", "]", "=", "len", "(", "validation_result_raw", "[", "'all'", "]", ")", "\n", "validation_result_raw", "=", "validation_result_raw", "[", "'mean'", "]", "\n", "\n", "if", "advanced_postprocessing", ":", "\n", "# first treat all foreground classes as one and remove all but the largest foreground connected component", "\n", "        ", "results", "=", "[", "]", "\n", "for", "f", "in", "fnames", ":", "\n", "            ", "predicted_segmentation", "=", "join", "(", "base", ",", "raw_subfolder_name", ",", "f", ")", "\n", "# now remove all but the largest connected component for each class", "\n", "output_file", "=", "join", "(", "folder_all_classes_as_fg", ",", "f", ")", "\n", "results", ".", "append", "(", "p", ".", "starmap_async", "(", "load_remove_save", ",", "(", "(", "predicted_segmentation", ",", "output_file", ",", "(", "classes", ",", ")", ")", ",", ")", ")", ")", "\n", "\n", "", "results", "=", "[", "i", ".", "get", "(", ")", "for", "i", "in", "results", "]", "\n", "\n", "# aggregate max_size_removed and min_size_kept", "\n", "max_size_removed", "=", "{", "}", "\n", "min_size_kept", "=", "{", "}", "\n", "for", "tmp", "in", "results", ":", "\n", "            ", "mx_rem", ",", "min_kept", "=", "tmp", "[", "0", "]", "\n", "for", "k", "in", "mx_rem", ":", "\n", "                ", "if", "mx_rem", "[", "k", "]", "is", "not", "None", ":", "\n", "                    ", "if", "max_size_removed", ".", "get", "(", "k", ")", "is", "None", ":", "\n", "                        ", "max_size_removed", "[", "k", "]", "=", "mx_rem", "[", "k", "]", "\n", "", "else", ":", "\n", "                        ", "max_size_removed", "[", "k", "]", "=", "max", "(", "max_size_removed", "[", "k", "]", ",", "mx_rem", "[", "k", "]", ")", "\n", "", "", "", "for", "k", "in", "min_kept", ":", "\n", "                ", "if", "min_kept", "[", "k", "]", "is", "not", "None", ":", "\n", "                    ", "if", "min_size_kept", ".", "get", "(", "k", ")", "is", "None", ":", "\n", "                        ", "min_size_kept", "[", "k", "]", "=", "min_kept", "[", "k", "]", "\n", "", "else", ":", "\n", "                        ", "min_size_kept", "[", "k", "]", "=", "min", "(", "min_size_kept", "[", "k", "]", ",", "min_kept", "[", "k", "]", ")", "\n", "\n", "", "", "", "", "print", "(", "\"foreground vs background, smallest valid object size was\"", ",", "min_size_kept", "[", "tuple", "(", "classes", ")", "]", ")", "\n", "print", "(", "\"removing only objects smaller than that...\"", ")", "\n", "\n", "", "else", ":", "\n", "        ", "min_size_kept", "=", "None", "\n", "\n", "# we need to rerun the step from above, now with the size constraint", "\n", "", "pred_gt_tuples", "=", "[", "]", "\n", "results", "=", "[", "]", "\n", "# first treat all foreground classes as one and remove all but the largest foreground connected component", "\n", "for", "f", "in", "fnames", ":", "\n", "        ", "predicted_segmentation", "=", "join", "(", "base", ",", "raw_subfolder_name", ",", "f", ")", "\n", "# now remove all but the largest connected component for each class", "\n", "output_file", "=", "join", "(", "folder_all_classes_as_fg", ",", "f", ")", "\n", "results", ".", "append", "(", "\n", "p", ".", "starmap_async", "(", "load_remove_save", ",", "(", "(", "predicted_segmentation", ",", "output_file", ",", "(", "classes", ",", ")", ",", "min_size_kept", ")", ",", ")", ")", ")", "\n", "pred_gt_tuples", ".", "append", "(", "[", "output_file", ",", "join", "(", "gt_labels_folder", ",", "f", ")", "]", ")", "\n", "\n", "", "_", "=", "[", "i", ".", "get", "(", ")", "for", "i", "in", "results", "]", "\n", "\n", "# evaluate postprocessed predictions", "\n", "_", "=", "aggregate_scores", "(", "pred_gt_tuples", ",", "labels", "=", "classes", ",", "\n", "json_output_file", "=", "join", "(", "folder_all_classes_as_fg", ",", "\"summary.json\"", ")", ",", "\n", "json_author", "=", "\"Fabian\"", ",", "num_threads", "=", "processes", ")", "\n", "\n", "# now we need to figure out if doing this improved the dice scores. We will implement that defensively in so far", "\n", "# that if a single class got worse as a result we won't do this. We can change this in the future but right now I", "\n", "# prefer to do it this way", "\n", "validation_result_PP_test", "=", "load_json", "(", "join", "(", "folder_all_classes_as_fg", ",", "\"summary.json\"", ")", ")", "[", "'results'", "]", "[", "'mean'", "]", "\n", "\n", "for", "c", "in", "classes", ":", "\n", "        ", "dc_raw", "=", "validation_result_raw", "[", "str", "(", "c", ")", "]", "[", "'Dice'", "]", "\n", "dc_pp", "=", "validation_result_PP_test", "[", "str", "(", "c", ")", "]", "[", "'Dice'", "]", "\n", "pp_results", "[", "'dc_per_class_raw'", "]", "[", "str", "(", "c", ")", "]", "=", "dc_raw", "\n", "pp_results", "[", "'dc_per_class_pp_all'", "]", "[", "str", "(", "c", ")", "]", "=", "dc_pp", "\n", "\n", "# true if new is better", "\n", "", "do_fg_cc", "=", "False", "\n", "comp", "=", "[", "pp_results", "[", "'dc_per_class_pp_all'", "]", "[", "str", "(", "cl", ")", "]", ">", "(", "pp_results", "[", "'dc_per_class_raw'", "]", "[", "str", "(", "cl", ")", "]", "+", "dice_threshold", ")", "for", "\n", "cl", "in", "classes", "]", "\n", "before", "=", "np", ".", "mean", "(", "[", "pp_results", "[", "'dc_per_class_raw'", "]", "[", "str", "(", "cl", ")", "]", "for", "cl", "in", "classes", "]", ")", "\n", "after", "=", "np", ".", "mean", "(", "[", "pp_results", "[", "'dc_per_class_pp_all'", "]", "[", "str", "(", "cl", ")", "]", "for", "cl", "in", "classes", "]", ")", "\n", "print", "(", "\"Foreground vs background\"", ")", "\n", "print", "(", "\"before:\"", ",", "before", ")", "\n", "print", "(", "\"after: \"", ",", "after", ")", "\n", "if", "any", "(", "comp", ")", ":", "\n", "# at least one class improved - yay!", "\n", "# now check if another got worse", "\n", "# true if new is worse", "\n", "        ", "any_worse", "=", "any", "(", "\n", "[", "pp_results", "[", "'dc_per_class_pp_all'", "]", "[", "str", "(", "cl", ")", "]", "<", "pp_results", "[", "'dc_per_class_raw'", "]", "[", "str", "(", "cl", ")", "]", "for", "cl", "in", "classes", "]", ")", "\n", "if", "not", "any_worse", ":", "\n", "            ", "pp_results", "[", "'for_which_classes'", "]", ".", "append", "(", "classes", ")", "\n", "if", "min_size_kept", "is", "not", "None", ":", "\n", "                ", "pp_results", "[", "'min_valid_object_sizes'", "]", ".", "update", "(", "deepcopy", "(", "min_size_kept", ")", ")", "\n", "", "do_fg_cc", "=", "True", "\n", "print", "(", "\"Removing all but the largest foreground region improved results!\"", ")", "\n", "print", "(", "'for_which_classes'", ",", "classes", ")", "\n", "print", "(", "'min_valid_object_sizes'", ",", "min_size_kept", ")", "\n", "", "", "else", ":", "\n", "# did not improve things - don't do it", "\n", "        ", "pass", "\n", "\n", "", "if", "len", "(", "classes", ")", ">", "1", ":", "\n", "# now depending on whether we do remove all but the largest foreground connected component we define the source dir", "\n", "# for the next one to be the raw or the temp dir", "\n", "        ", "if", "do_fg_cc", ":", "\n", "            ", "source", "=", "folder_all_classes_as_fg", "\n", "", "else", ":", "\n", "            ", "source", "=", "join", "(", "base", ",", "raw_subfolder_name", ")", "\n", "\n", "", "if", "advanced_postprocessing", ":", "\n", "# now run this for each class separately", "\n", "            ", "results", "=", "[", "]", "\n", "for", "f", "in", "fnames", ":", "\n", "                ", "predicted_segmentation", "=", "join", "(", "source", ",", "f", ")", "\n", "output_file", "=", "join", "(", "folder_per_class", ",", "f", ")", "\n", "results", ".", "append", "(", "p", ".", "starmap_async", "(", "load_remove_save", ",", "(", "(", "predicted_segmentation", ",", "output_file", ",", "classes", ")", ",", ")", ")", ")", "\n", "\n", "", "results", "=", "[", "i", ".", "get", "(", ")", "for", "i", "in", "results", "]", "\n", "\n", "# aggregate max_size_removed and min_size_kept", "\n", "max_size_removed", "=", "{", "}", "\n", "min_size_kept", "=", "{", "}", "\n", "for", "tmp", "in", "results", ":", "\n", "                ", "mx_rem", ",", "min_kept", "=", "tmp", "[", "0", "]", "\n", "for", "k", "in", "mx_rem", ":", "\n", "                    ", "if", "mx_rem", "[", "k", "]", "is", "not", "None", ":", "\n", "                        ", "if", "max_size_removed", ".", "get", "(", "k", ")", "is", "None", ":", "\n", "                            ", "max_size_removed", "[", "k", "]", "=", "mx_rem", "[", "k", "]", "\n", "", "else", ":", "\n", "                            ", "max_size_removed", "[", "k", "]", "=", "max", "(", "max_size_removed", "[", "k", "]", ",", "mx_rem", "[", "k", "]", ")", "\n", "", "", "", "for", "k", "in", "min_kept", ":", "\n", "                    ", "if", "min_kept", "[", "k", "]", "is", "not", "None", ":", "\n", "                        ", "if", "min_size_kept", ".", "get", "(", "k", ")", "is", "None", ":", "\n", "                            ", "min_size_kept", "[", "k", "]", "=", "min_kept", "[", "k", "]", "\n", "", "else", ":", "\n", "                            ", "min_size_kept", "[", "k", "]", "=", "min", "(", "min_size_kept", "[", "k", "]", ",", "min_kept", "[", "k", "]", ")", "\n", "\n", "", "", "", "", "print", "(", "\"classes treated separately, smallest valid object sizes are\"", ")", "\n", "print", "(", "min_size_kept", ")", "\n", "print", "(", "\"removing only objects smaller than that\"", ")", "\n", "", "else", ":", "\n", "            ", "min_size_kept", "=", "None", "\n", "\n", "# rerun with the size thresholds from above", "\n", "", "pred_gt_tuples", "=", "[", "]", "\n", "results", "=", "[", "]", "\n", "for", "f", "in", "fnames", ":", "\n", "            ", "predicted_segmentation", "=", "join", "(", "source", ",", "f", ")", "\n", "output_file", "=", "join", "(", "folder_per_class", ",", "f", ")", "\n", "results", ".", "append", "(", "p", ".", "starmap_async", "(", "load_remove_save", ",", "(", "(", "predicted_segmentation", ",", "output_file", ",", "classes", ",", "min_size_kept", ")", ",", ")", ")", ")", "\n", "pred_gt_tuples", ".", "append", "(", "[", "output_file", ",", "join", "(", "gt_labels_folder", ",", "f", ")", "]", ")", "\n", "\n", "", "_", "=", "[", "i", ".", "get", "(", ")", "for", "i", "in", "results", "]", "\n", "\n", "# evaluate postprocessed predictions", "\n", "_", "=", "aggregate_scores", "(", "pred_gt_tuples", ",", "labels", "=", "classes", ",", "\n", "json_output_file", "=", "join", "(", "folder_per_class", ",", "\"summary.json\"", ")", ",", "\n", "json_author", "=", "\"Fabian\"", ",", "num_threads", "=", "processes", ")", "\n", "\n", "if", "do_fg_cc", ":", "\n", "            ", "old_res", "=", "deepcopy", "(", "validation_result_PP_test", ")", "\n", "", "else", ":", "\n", "            ", "old_res", "=", "validation_result_raw", "\n", "\n", "# these are the new dice scores", "\n", "", "validation_result_PP_test", "=", "load_json", "(", "join", "(", "folder_per_class", ",", "\"summary.json\"", ")", ")", "[", "'results'", "]", "[", "'mean'", "]", "\n", "\n", "for", "c", "in", "classes", ":", "\n", "            ", "dc_raw", "=", "old_res", "[", "str", "(", "c", ")", "]", "[", "'Dice'", "]", "\n", "dc_pp", "=", "validation_result_PP_test", "[", "str", "(", "c", ")", "]", "[", "'Dice'", "]", "\n", "pp_results", "[", "'dc_per_class_pp_per_class'", "]", "[", "str", "(", "c", ")", "]", "=", "dc_pp", "\n", "print", "(", "c", ")", "\n", "print", "(", "\"before:\"", ",", "dc_raw", ")", "\n", "print", "(", "\"after: \"", ",", "dc_pp", ")", "\n", "\n", "if", "dc_pp", ">", "(", "dc_raw", "+", "dice_threshold", ")", ":", "\n", "                ", "pp_results", "[", "'for_which_classes'", "]", ".", "append", "(", "int", "(", "c", ")", ")", "\n", "if", "min_size_kept", "is", "not", "None", ":", "\n", "                    ", "pp_results", "[", "'min_valid_object_sizes'", "]", ".", "update", "(", "{", "c", ":", "min_size_kept", "[", "c", "]", "}", ")", "\n", "", "print", "(", "\"Removing all but the largest region for class %d improved results!\"", "%", "c", ")", "\n", "print", "(", "'min_valid_object_sizes'", ",", "min_size_kept", ")", "\n", "", "", "", "else", ":", "\n", "        ", "print", "(", "\"Only one class present, no need to do each class separately as this is covered in fg vs bg\"", ")", "\n", "\n", "", "if", "not", "advanced_postprocessing", ":", "\n", "        ", "pp_results", "[", "'min_valid_object_sizes'", "]", "=", "None", "\n", "\n", "", "print", "(", "\"done\"", ")", "\n", "print", "(", "\"for which classes:\"", ")", "\n", "print", "(", "pp_results", "[", "'for_which_classes'", "]", ")", "\n", "print", "(", "\"min_object_sizes\"", ")", "\n", "print", "(", "pp_results", "[", "'min_valid_object_sizes'", "]", ")", "\n", "\n", "pp_results", "[", "'validation_raw'", "]", "=", "raw_subfolder_name", "\n", "pp_results", "[", "'validation_final'", "]", "=", "final_subf_name", "\n", "\n", "# now that we have a proper for_which_classes, apply that", "\n", "pred_gt_tuples", "=", "[", "]", "\n", "results", "=", "[", "]", "\n", "for", "f", "in", "fnames", ":", "\n", "        ", "predicted_segmentation", "=", "join", "(", "base", ",", "raw_subfolder_name", ",", "f", ")", "\n", "\n", "# now remove all but the largest connected component for each class", "\n", "output_file", "=", "join", "(", "base", ",", "final_subf_name", ",", "f", ")", "\n", "results", ".", "append", "(", "p", ".", "starmap_async", "(", "load_remove_save", ",", "(", "\n", "(", "predicted_segmentation", ",", "output_file", ",", "pp_results", "[", "'for_which_classes'", "]", ",", "\n", "pp_results", "[", "'min_valid_object_sizes'", "]", ")", ",", ")", ")", ")", "\n", "\n", "pred_gt_tuples", ".", "append", "(", "[", "output_file", ",", "\n", "join", "(", "gt_labels_folder", ",", "f", ")", "]", ")", "\n", "\n", "", "_", "=", "[", "i", ".", "get", "(", ")", "for", "i", "in", "results", "]", "\n", "# evaluate postprocessed predictions", "\n", "_", "=", "aggregate_scores", "(", "pred_gt_tuples", ",", "labels", "=", "classes", ",", "\n", "json_output_file", "=", "join", "(", "base", ",", "final_subf_name", ",", "\"summary.json\"", ")", ",", "\n", "json_author", "=", "\"Fabian\"", ",", "num_threads", "=", "processes", ")", "\n", "\n", "pp_results", "[", "'min_valid_object_sizes'", "]", "=", "str", "(", "pp_results", "[", "'min_valid_object_sizes'", "]", ")", "\n", "\n", "save_json", "(", "pp_results", ",", "join", "(", "base", ",", "pp_filename", ")", ")", "\n", "\n", "# delete temp", "\n", "if", "not", "debug", ":", "\n", "        ", "shutil", ".", "rmtree", "(", "folder_per_class", ")", "\n", "shutil", ".", "rmtree", "(", "folder_all_classes_as_fg", ")", "\n", "\n", "", "p", ".", "close", "(", ")", "\n", "p", ".", "join", "(", ")", "\n", "print", "(", "\"done\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.postprocessing.connected_components.apply_postprocessing_to_folder": [[400, 422], ["maybe_mkdir_p", "multiprocessing.pool.Pool", "subfiles", "multiprocessing.pool.Pool.starmap_async", "p.starmap_async.get", "multiprocessing.pool.Pool.close", "multiprocessing.pool.Pool.join", "join", "join", "zip", "len", "len"], "function", ["None"], ["", "def", "apply_postprocessing_to_folder", "(", "input_folder", ":", "str", ",", "output_folder", ":", "str", ",", "for_which_classes", ":", "list", ",", "\n", "min_valid_object_size", ":", "dict", "=", "None", ",", "num_processes", "=", "8", ")", ":", "\n", "    ", "\"\"\"\n    applies removing of all but the largest connected component to all niftis in a folder\n    :param min_valid_object_size:\n    :param min_valid_object_size:\n    :param input_folder:\n    :param output_folder:\n    :param for_which_classes:\n    :param num_processes:\n    :return:\n    \"\"\"", "\n", "maybe_mkdir_p", "(", "output_folder", ")", "\n", "p", "=", "Pool", "(", "num_processes", ")", "\n", "nii_files", "=", "subfiles", "(", "input_folder", ",", "suffix", "=", "\".nii.gz\"", ",", "join", "=", "False", ")", "\n", "input_files", "=", "[", "join", "(", "input_folder", ",", "i", ")", "for", "i", "in", "nii_files", "]", "\n", "out_files", "=", "[", "join", "(", "output_folder", ",", "i", ")", "for", "i", "in", "nii_files", "]", "\n", "results", "=", "p", ".", "starmap_async", "(", "load_remove_save", ",", "zip", "(", "input_files", ",", "out_files", ",", "[", "for_which_classes", "]", "*", "len", "(", "input_files", ")", ",", "\n", "[", "min_valid_object_size", "]", "*", "len", "(", "input_files", ")", ")", ")", "\n", "res", "=", "results", ".", "get", "(", ")", "\n", "p", ".", "close", "(", ")", "\n", "p", ".", "join", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.postprocessing.consolidate_all_for_paper.get_datasets": [[19, 42], ["None"], "function", ["None"], ["def", "get_datasets", "(", ")", ":", "\n", "    ", "configurations_all", "=", "{", "\n", "\"Task01_BrainTumour\"", ":", "(", "\"3d_fullres\"", ",", "\"2d\"", ")", ",", "\n", "\"Task02_Heart\"", ":", "(", "\"3d_fullres\"", ",", "\"2d\"", ",", ")", ",", "\n", "\"Task03_Liver\"", ":", "(", "\"3d_cascade_fullres\"", ",", "\"3d_fullres\"", ",", "\"3d_lowres\"", ",", "\"2d\"", ")", ",", "\n", "\"Task04_Hippocampus\"", ":", "(", "\"3d_fullres\"", ",", "\"2d\"", ",", ")", ",", "\n", "\"Task05_Prostate\"", ":", "(", "\"3d_fullres\"", ",", "\"2d\"", ",", ")", ",", "\n", "\"Task06_Lung\"", ":", "(", "\"3d_cascade_fullres\"", ",", "\"3d_fullres\"", ",", "\"3d_lowres\"", ",", "\"2d\"", ")", ",", "\n", "\"Task07_Pancreas\"", ":", "(", "\"3d_cascade_fullres\"", ",", "\"3d_fullres\"", ",", "\"3d_lowres\"", ",", "\"2d\"", ")", ",", "\n", "\"Task08_HepaticVessel\"", ":", "(", "\"3d_cascade_fullres\"", ",", "\"3d_fullres\"", ",", "\"3d_lowres\"", ",", "\"2d\"", ")", ",", "\n", "\"Task09_Spleen\"", ":", "(", "\"3d_cascade_fullres\"", ",", "\"3d_fullres\"", ",", "\"3d_lowres\"", ",", "\"2d\"", ")", ",", "\n", "\"Task10_Colon\"", ":", "(", "\"3d_cascade_fullres\"", ",", "\"3d_fullres\"", ",", "\"3d_lowres\"", ",", "\"2d\"", ")", ",", "\n", "\"Task48_KiTS_clean\"", ":", "(", "\"3d_cascade_fullres\"", ",", "\"3d_lowres\"", ",", "\"3d_fullres\"", ",", "\"2d\"", ")", ",", "\n", "\"Task27_ACDC\"", ":", "(", "\"3d_fullres\"", ",", "\"2d\"", ",", ")", ",", "\n", "\"Task24_Promise\"", ":", "(", "\"3d_fullres\"", ",", "\"2d\"", ",", ")", ",", "\n", "\"Task35_ISBILesionSegmentation\"", ":", "(", "\"3d_fullres\"", ",", "\"2d\"", ",", ")", ",", "\n", "\"Task38_CHAOS_Task_3_5_Variant2\"", ":", "(", "\"3d_fullres\"", ",", "\"2d\"", ",", ")", ",", "\n", "\"Task29_LITS\"", ":", "(", "\"3d_cascade_fullres\"", ",", "\"3d_lowres\"", ",", "\"2d\"", ",", "\"3d_fullres\"", ",", ")", ",", "\n", "\"Task17_AbdominalOrganSegmentation\"", ":", "(", "\"3d_cascade_fullres\"", ",", "\"3d_lowres\"", ",", "\"2d\"", ",", "\"3d_fullres\"", ",", ")", ",", "\n", "\"Task55_SegTHOR\"", ":", "(", "\"3d_cascade_fullres\"", ",", "\"3d_lowres\"", ",", "\"3d_fullres\"", ",", "\"2d\"", ",", ")", ",", "\n", "\"Task56_VerSe\"", ":", "(", "\"3d_cascade_fullres\"", ",", "\"3d_lowres\"", ",", "\"3d_fullres\"", ",", "\"2d\"", ",", ")", ",", "\n", "}", "\n", "return", "configurations_all", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.postprocessing.consolidate_all_for_paper.get_commands": [[44, 62], ["nnunet.utilities.folder_names.get_output_folder_name", "print", "range", "range", "range", "len"], "function", ["home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.utilities.folder_names.get_output_folder_name"], ["", "def", "get_commands", "(", "configurations", ",", "regular_trainer", "=", "\"nnUNetTrainerV2\"", ",", "cascade_trainer", "=", "\"nnUNetTrainerV2CascadeFullRes\"", ",", "\n", "plans", "=", "\"nnUNetPlansv2.1\"", ")", ":", "\n", "\n", "    ", "node_pool", "=", "[", "\"hdf18-gpu%02.0d\"", "%", "i", "for", "i", "in", "range", "(", "1", ",", "21", ")", "]", "+", "[", "\"hdf19-gpu%02.0d\"", "%", "i", "for", "i", "in", "range", "(", "1", ",", "8", ")", "]", "+", "[", "\"hdf19-gpu%02.0d\"", "%", "i", "for", "i", "in", "range", "(", "11", ",", "16", ")", "]", "\n", "ctr", "=", "0", "\n", "for", "task", "in", "configurations", ":", "\n", "        ", "models", "=", "configurations", "[", "task", "]", "\n", "for", "m", "in", "models", ":", "\n", "            ", "if", "m", "==", "\"3d_cascade_fullres\"", ":", "\n", "                ", "trainer", "=", "cascade_trainer", "\n", "", "else", ":", "\n", "                ", "trainer", "=", "regular_trainer", "\n", "\n", "", "folder", "=", "get_output_folder_name", "(", "m", ",", "task", ",", "trainer", ",", "plans", ",", "overwrite_training_output_dir", "=", "\"/datasets/datasets_fabian/results/nnUNet\"", ")", "\n", "node", "=", "node_pool", "[", "ctr", "%", "len", "(", "node_pool", ")", "]", "\n", "print", "(", "\"bsub -m %s -q gputest -L /bin/bash \\\"source ~/.bashrc && python postprocessing/\"", "\n", "\"consolidate_postprocessing.py -f\"", "%", "node", ",", "folder", ",", "\"\\\"\"", ")", "\n", "ctr", "+=", "1", "\n", "", "", "", ""]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.postprocessing.consolidate_postprocessing.collect_cv_niftis": [[25, 40], ["all", "maybe_mkdir_p", "join", "join", "subfiles", "isdir", "shutil.copy", "join"], "function", ["None"], ["def", "collect_cv_niftis", "(", "cv_folder", ":", "str", ",", "output_folder", ":", "str", ",", "validation_folder_name", ":", "str", "=", "'validation_raw'", ",", "\n", "folds", ":", "tuple", "=", "(", "0", ",", "1", ",", "2", ",", "3", ",", "4", ")", ")", ":", "\n", "    ", "folders_folds", "=", "[", "join", "(", "cv_folder", ",", "\"fold_%d\"", "%", "i", ")", "for", "i", "in", "folds", "]", "\n", "\n", "assert", "all", "(", "[", "isdir", "(", "i", ")", "for", "i", "in", "folders_folds", "]", ")", ",", "\"some folds are missing\"", "\n", "\n", "# now for each fold, read the postprocessing json. this will tell us what the name of the validation folder is", "\n", "validation_raw_folders", "=", "[", "join", "(", "cv_folder", ",", "\"fold_%d\"", "%", "i", ",", "validation_folder_name", ")", "for", "i", "in", "folds", "]", "\n", "\n", "# now copy all raw niftis into cv_niftis_raw", "\n", "maybe_mkdir_p", "(", "output_folder", ")", "\n", "for", "f", "in", "folds", ":", "\n", "        ", "niftis", "=", "subfiles", "(", "validation_raw_folders", "[", "f", "]", ",", "suffix", "=", "\".nii.gz\"", ")", "\n", "for", "n", "in", "niftis", ":", "\n", "            ", "shutil", ".", "copy", "(", "n", ",", "join", "(", "output_folder", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.postprocessing.consolidate_postprocessing.consolidate_folds": [[42, 82], ["join", "join", "consolidate_postprocessing.collect_cv_niftis", "len", "len", "subfiles", "nnunet.evaluation.evaluator.aggregate_scores", "nnunet.postprocessing.connected_components.determine_postprocessing", "subfiles", "subfiles", "shutil.rmtree", "AssertionError", "int", "join", "load_json", "summary_fold0.keys", "join", "join", "join", "join"], "function", ["home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.postprocessing.consolidate_postprocessing.collect_cv_niftis", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.evaluation.evaluator.aggregate_scores", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.postprocessing.connected_components.determine_postprocessing"], ["", "", "", "def", "consolidate_folds", "(", "output_folder_base", ",", "validation_folder_name", ":", "str", "=", "'validation_raw'", ",", "\n", "advanced_postprocessing", ":", "bool", "=", "False", ",", "folds", ":", "Tuple", "[", "int", "]", "=", "(", "0", ",", "1", ",", "2", ",", "3", ",", "4", ")", ")", ":", "\n", "    ", "\"\"\"\n    Used to determine the postprocessing for an experiment after all five folds have been completed. In the validation of\n    each fold, the postprocessing can only be determined on the cases within that fold. This can result in different\n    postprocessing decisions for different folds. In the end, we can only decide for one postprocessing per experiment,\n    so we have to rerun it\n    :param folds:\n    :param advanced_postprocessing:\n    :param output_folder_base:experiment output folder (fold_0, fold_1, etc must be subfolders of the given folder)\n    :param validation_folder_name: dont use this\n    :return:\n    \"\"\"", "\n", "output_folder_raw", "=", "join", "(", "output_folder_base", ",", "\"cv_niftis_raw\"", ")", "\n", "output_folder_gt", "=", "join", "(", "output_folder_base", ",", "\"gt_niftis\"", ")", "\n", "collect_cv_niftis", "(", "output_folder_base", ",", "output_folder_raw", ",", "validation_folder_name", ",", "\n", "folds", ")", "\n", "\n", "num_niftis_gt", "=", "len", "(", "subfiles", "(", "join", "(", "output_folder_base", ",", "\"gt_niftis\"", ")", ")", ")", "\n", "# count niftis in there", "\n", "num_niftis", "=", "len", "(", "subfiles", "(", "output_folder_raw", ")", ")", "\n", "if", "num_niftis", "!=", "num_niftis_gt", ":", "\n", "        ", "shutil", ".", "rmtree", "(", "output_folder_raw", ")", "\n", "raise", "AssertionError", "(", "\"If does not seem like you trained all the folds! Train all folds first!\"", ")", "\n", "\n", "# load a summary file so that we can know what class labels to expect", "\n", "", "summary_fold0", "=", "load_json", "(", "join", "(", "output_folder_base", ",", "\"fold_0\"", ",", "validation_folder_name", ",", "\"summary.json\"", ")", ")", "[", "'results'", "]", "[", "\n", "'mean'", "]", "\n", "classes", "=", "[", "int", "(", "i", ")", "for", "i", "in", "summary_fold0", ".", "keys", "(", ")", "]", "\n", "niftis", "=", "subfiles", "(", "output_folder_raw", ",", "join", "=", "False", ",", "suffix", "=", "\".nii.gz\"", ")", "\n", "test_pred_pairs", "=", "[", "(", "join", "(", "output_folder_gt", ",", "i", ")", ",", "join", "(", "output_folder_raw", ",", "i", ")", ")", "for", "i", "in", "niftis", "]", "\n", "\n", "# determine_postprocessing needs a summary.json file in the folder where the raw predictions are. We could compute", "\n", "# that from the summary files of the five folds but I am feeling lazy today", "\n", "aggregate_scores", "(", "test_pred_pairs", ",", "labels", "=", "classes", ",", "json_output_file", "=", "join", "(", "output_folder_raw", ",", "\"summary.json\"", ")", ",", "\n", "num_threads", "=", "default_num_threads", ")", "\n", "\n", "determine_postprocessing", "(", "output_folder_base", ",", "output_folder_gt", ",", "'cv_niftis_raw'", ",", "\n", "final_subf_name", "=", "\"cv_niftis_postprocessed\"", ",", "processes", "=", "default_num_threads", ",", "\n", "advanced_postprocessing", "=", "advanced_postprocessing", ")", "\n", "# determine_postprocessing will create a postprocessing.json file that can be used for inference", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.postprocessing.consolidate_postprocessing_simple.main": [[23, 57], ["argparse.ArgumentParser", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.parse_args", "nnunet.utilities.folder_names.get_output_folder_name", "nnunet.postprocessing.consolidate_postprocessing.consolidate_folds", "nnunet.utilities.task_name_id_conversion.convert_id_to_task_name.startswith", "int", "nnunet.utilities.task_name_id_conversion.convert_id_to_task_name"], "function", ["home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.utilities.folder_names.get_output_folder_name", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.postprocessing.consolidate_postprocessing.consolidate_folds", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.utilities.task_name_id_conversion.convert_id_to_task_name"], ["def", "main", "(", ")", ":", "\n", "    ", "argparser", "=", "argparse", ".", "ArgumentParser", "(", "usage", "=", "\"Used to determine the postprocessing for a trained model. Useful for \"", "\n", "\"when the best configuration (2d, 3d_fullres etc) as selected manually.\"", ")", "\n", "argparser", ".", "add_argument", "(", "\"-m\"", ",", "type", "=", "str", ",", "required", "=", "True", ",", "help", "=", "\"U-Net model (2d, 3d_lowres, 3d_fullres or \"", "\n", "\"3d_cascade_fullres)\"", ")", "\n", "argparser", ".", "add_argument", "(", "\"-t\"", ",", "type", "=", "str", ",", "required", "=", "True", ",", "help", "=", "\"Task name or id\"", ")", "\n", "argparser", ".", "add_argument", "(", "\"-tr\"", ",", "type", "=", "str", ",", "required", "=", "False", ",", "default", "=", "None", ",", "\n", "help", "=", "\"nnUNetTrainer class. Default: %s, unless 3d_cascade_fullres \"", "\n", "\"(then it's %s)\"", "%", "(", "default_trainer", ",", "default_cascade_trainer", ")", ")", "\n", "argparser", ".", "add_argument", "(", "\"-pl\"", ",", "type", "=", "str", ",", "required", "=", "False", ",", "default", "=", "default_plans_identifier", ",", "\n", "help", "=", "\"Plans name, Default=%s\"", "%", "default_plans_identifier", ")", "\n", "argparser", ".", "add_argument", "(", "\"-val\"", ",", "type", "=", "str", ",", "required", "=", "False", ",", "default", "=", "\"validation_raw\"", ",", "\n", "help", "=", "\"Validation folder name. Default: validation_raw\"", ")", "\n", "\n", "args", "=", "argparser", ".", "parse_args", "(", ")", "\n", "model", "=", "args", ".", "m", "\n", "task", "=", "args", ".", "t", "\n", "trainer", "=", "args", ".", "tr", "\n", "plans", "=", "args", ".", "pl", "\n", "val", "=", "args", ".", "val", "\n", "\n", "if", "not", "task", ".", "startswith", "(", "\"Task\"", ")", ":", "\n", "        ", "task_id", "=", "int", "(", "task", ")", "\n", "task", "=", "convert_id_to_task_name", "(", "task_id", ")", "\n", "\n", "", "if", "trainer", "is", "None", ":", "\n", "        ", "if", "model", "==", "\"3d_cascade_fullres\"", ":", "\n", "            ", "trainer", "=", "\"nnUNetTrainerV2CascadeFullRes\"", "\n", "", "else", ":", "\n", "            ", "trainer", "=", "\"nnUNetTrainerV2\"", "\n", "\n", "", "", "folder", "=", "get_output_folder_name", "(", "model", ",", "task", ",", "trainer", ",", "plans", ",", "None", ")", "\n", "\n", "consolidate_folds", "(", "folder", ",", "val", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.dataset_conversion.Task040_KiTS.compute_dice_scores": [[28, 47], ["SimpleITK.GetArrayFromImage", "SimpleITK.GetArrayFromImage", "numpy.mean", "SimpleITK.ReadImage", "SimpleITK.ReadImage", "medpy.metric.dc", "medpy.metric.dc", "numpy.sum", "kidney_mask_ref.sum", "numpy.sum", "tumor_mask_pred.sum"], "function", ["None"], ["def", "compute_dice_scores", "(", "ref", ":", "str", ",", "pred", ":", "str", ")", ":", "\n", "    ", "ref", "=", "sitk", ".", "GetArrayFromImage", "(", "sitk", ".", "ReadImage", "(", "ref", ")", ")", "\n", "pred", "=", "sitk", ".", "GetArrayFromImage", "(", "sitk", ".", "ReadImage", "(", "pred", ")", ")", "\n", "kidney_mask_ref", "=", "ref", ">", "0", "\n", "kidney_mask_pred", "=", "pred", ">", "0", "\n", "if", "np", ".", "sum", "(", "kidney_mask_pred", ")", "==", "0", "and", "kidney_mask_ref", ".", "sum", "(", ")", "==", "0", ":", "\n", "        ", "kidney_dice", "=", "np", ".", "nan", "\n", "", "else", ":", "\n", "        ", "kidney_dice", "=", "dc", "(", "kidney_mask_pred", ",", "kidney_mask_ref", ")", "\n", "\n", "", "tumor_mask_ref", "=", "ref", "==", "2", "\n", "tumor_mask_pred", "=", "pred", "==", "2", "\n", "if", "np", ".", "sum", "(", "tumor_mask_ref", ")", "==", "0", "and", "tumor_mask_pred", ".", "sum", "(", ")", "==", "0", ":", "\n", "        ", "tumor_dice", "=", "np", ".", "nan", "\n", "", "else", ":", "\n", "        ", "tumor_dice", "=", "dc", "(", "tumor_mask_ref", ",", "tumor_mask_pred", ")", "\n", "\n", "", "geometric_mean", "=", "np", ".", "mean", "(", "(", "kidney_dice", ",", "tumor_dice", ")", ")", "\n", "return", "kidney_dice", ",", "tumor_dice", ",", "geometric_mean", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.dataset_conversion.Task040_KiTS.evaluate_folder": [[49, 61], ["multiprocessing.Pool", "subfiles", "multiprocessing.Pool.starmap", "multiprocessing.Pool.close", "multiprocessing.Pool.join", "join", "join", "zip", "open", "enumerate", "join", "f.write"], "function", ["None"], ["", "def", "evaluate_folder", "(", "folder_gt", ":", "str", ",", "folder_pred", ":", "str", ")", ":", "\n", "    ", "p", "=", "Pool", "(", "8", ")", "\n", "niftis", "=", "subfiles", "(", "folder_gt", ",", "suffix", "=", "\".nii.gz\"", ",", "join", "=", "False", ")", "\n", "images_gt", "=", "[", "join", "(", "folder_gt", ",", "i", ")", "for", "i", "in", "niftis", "]", "\n", "images_pred", "=", "[", "join", "(", "folder_pred", ",", "i", ")", "for", "i", "in", "niftis", "]", "\n", "results", "=", "p", ".", "starmap", "(", "compute_dice_scores", ",", "zip", "(", "images_gt", ",", "images_pred", ")", ")", "\n", "p", ".", "close", "(", ")", "\n", "p", ".", "join", "(", ")", "\n", "\n", "with", "open", "(", "join", "(", "folder_pred", ",", "\"results.csv\"", ")", ",", "'w'", ")", "as", "f", ":", "\n", "        ", "for", "i", ",", "ni", "in", "enumerate", "(", "niftis", ")", ":", "\n", "            ", "f", ".", "write", "(", "\"%s,%0.4f,%0.4f,%0.4f\\n\"", "%", "(", "ni", ",", "*", "results", "[", "i", "]", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.dataset_conversion.Task040_KiTS.remove_all_but_the_two_largest_conn_comp": [[63, 85], ["SimpleITK.ReadImage", "SimpleITK.GetArrayFromImage", "scipy.ndimage.label", "range", "SimpleITK.GetImageFromArray", "sitk.GetImageFromArray.CopyInformation", "SimpleITK.WriteImage", "print", "shutil.copy", "label_sizes.append", "numpy.argsort", "os.path.basename", "numpy.sum"], "function", ["None"], ["", "", "", "def", "remove_all_but_the_two_largest_conn_comp", "(", "img_itk_file", ":", "str", ",", "file_out", ":", "str", ")", ":", "\n", "    ", "\"\"\"\n    This was not used. I was just curious because others used this. Turns out this is not necessary for my networks\n    \"\"\"", "\n", "img_itk", "=", "sitk", ".", "ReadImage", "(", "img_itk_file", ")", "\n", "img_npy", "=", "sitk", ".", "GetArrayFromImage", "(", "img_itk", ")", "\n", "\n", "labelmap", ",", "num_labels", "=", "label", "(", "(", "img_npy", ">", "0", ")", ".", "astype", "(", "int", ")", ")", "\n", "\n", "if", "num_labels", ">", "2", ":", "\n", "        ", "label_sizes", "=", "[", "]", "\n", "for", "i", "in", "range", "(", "1", ",", "num_labels", "+", "1", ")", ":", "\n", "            ", "label_sizes", ".", "append", "(", "np", ".", "sum", "(", "labelmap", "==", "i", ")", ")", "\n", "", "argsrt", "=", "np", ".", "argsort", "(", "label_sizes", ")", "[", ":", ":", "-", "1", "]", "# two largest are now argsrt[0] and argsrt[1]", "\n", "keep_mask", "=", "(", "labelmap", "==", "argsrt", "[", "0", "]", "+", "1", ")", "|", "(", "labelmap", "==", "argsrt", "[", "1", "]", "+", "1", ")", "\n", "img_npy", "[", "~", "keep_mask", "]", "=", "0", "\n", "new", "=", "sitk", ".", "GetImageFromArray", "(", "img_npy", ")", "\n", "new", ".", "CopyInformation", "(", "img_itk", ")", "\n", "sitk", ".", "WriteImage", "(", "new", ",", "file_out", ")", "\n", "print", "(", "os", ".", "path", ".", "basename", "(", "img_itk_file", ")", ",", "num_labels", ",", "label_sizes", ")", "\n", "", "else", ":", "\n", "        ", "shutil", ".", "copy", "(", "img_itk_file", ",", "file_out", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.dataset_conversion.Task040_KiTS.manual_postprocess": [[87, 103], ["maybe_mkdir_p", "subfiles", "multiprocessing.Pool", "multiprocessing.Pool.starmap_async", "_.get.get", "multiprocessing.Pool.close", "multiprocessing.Pool.join", "join", "join", "zip"], "function", ["None"], ["", "", "def", "manual_postprocess", "(", "folder_in", ",", "\n", "folder_out", ")", ":", "\n", "    ", "\"\"\"\n    This was not used. I was just curious because others used this. Turns out this is not necessary for my networks\n    \"\"\"", "\n", "maybe_mkdir_p", "(", "folder_out", ")", "\n", "infiles", "=", "subfiles", "(", "folder_in", ",", "suffix", "=", "\".nii.gz\"", ",", "join", "=", "False", ")", "\n", "\n", "outfiles", "=", "[", "join", "(", "folder_out", ",", "i", ")", "for", "i", "in", "infiles", "]", "\n", "infiles", "=", "[", "join", "(", "folder_in", ",", "i", ")", "for", "i", "in", "infiles", "]", "\n", "\n", "p", "=", "Pool", "(", "8", ")", "\n", "_", "=", "p", ".", "starmap_async", "(", "remove_all_but_the_two_largest_conn_comp", ",", "zip", "(", "infiles", ",", "outfiles", ")", ")", "\n", "_", "=", "_", ".", "get", "(", ")", "\n", "p", ".", "close", "(", ")", "\n", "p", ".", "join", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.dataset_conversion.Task040_KiTS.copy_npz_fom_valsets": [[107, 129], ["join", "join", "maybe_mkdir_p", "shutil.copy", "range", "join", "join", "subfiles", "all", "shutil.copy", "shutil.copy", "isfile", "join", "join", "join"], "function", ["None"], ["", "def", "copy_npz_fom_valsets", "(", ")", ":", "\n", "    ", "'''\n    this is preparation for ensembling\n    :return:\n    '''", "\n", "base", "=", "join", "(", "network_training_output_dir", ",", "\"3d_lowres/Task048_KiTS_clean\"", ")", "\n", "folders", "=", "[", "'nnUNetTrainerNewCandidate23_FabiansPreActResNet__nnUNetPlans'", ",", "\n", "'nnUNetTrainerNewCandidate23_FabiansResNet__nnUNetPlans'", ",", "\n", "'nnUNetTrainerNewCandidate23__nnUNetPlans'", "]", "\n", "for", "f", "in", "folders", ":", "\n", "        ", "out", "=", "join", "(", "base", ",", "f", ",", "'crossval_npz'", ")", "\n", "maybe_mkdir_p", "(", "out", ")", "\n", "shutil", ".", "copy", "(", "join", "(", "base", ",", "f", ",", "'plans.pkl'", ")", ",", "out", ")", "\n", "for", "fold", "in", "range", "(", "5", ")", ":", "\n", "            ", "cur", "=", "join", "(", "base", ",", "f", ",", "'fold_%d'", "%", "fold", ",", "'validation_raw'", ")", "\n", "npz_files", "=", "subfiles", "(", "cur", ",", "suffix", "=", "'.npz'", ",", "join", "=", "False", ")", "\n", "pkl_files", "=", "[", "i", "[", ":", "-", "3", "]", "+", "'pkl'", "for", "i", "in", "npz_files", "]", "\n", "assert", "all", "(", "[", "isfile", "(", "join", "(", "cur", ",", "i", ")", ")", "for", "i", "in", "pkl_files", "]", ")", "\n", "for", "n", "in", "npz_files", ":", "\n", "                ", "corresponding_pkl", "=", "n", "[", ":", "-", "3", "]", "+", "'pkl'", "\n", "shutil", ".", "copy", "(", "join", "(", "cur", ",", "n", ")", ",", "out", ")", "\n", "shutil", ".", "copy", "(", "join", "(", "cur", ",", "corresponding_pkl", ")", ",", "out", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.dataset_conversion.Task040_KiTS.ensemble": [[131, 136], ["merge", "join"], "function", ["home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.model_selection.ensemble.merge"], ["", "", "", "", "def", "ensemble", "(", "experiments", "=", "(", "'nnUNetTrainerNewCandidate23_FabiansPreActResNet__nnUNetPlans'", ",", "\n", "'nnUNetTrainerNewCandidate23_FabiansResNet__nnUNetPlans'", ")", ",", "out_dir", "=", "\"/media/fabian/Results/nnUNet/3d_lowres/Task048_KiTS_clean/ensemble_preactres_and_res\"", ")", ":", "\n", "    ", "from", "nnunet", ".", "inference", ".", "ensemble_predictions", "import", "merge", "\n", "folders", "=", "[", "join", "(", "network_training_output_dir", ",", "\"3d_lowres/Task048_KiTS_clean\"", ",", "i", ",", "'crossval_npz'", ")", "for", "i", "in", "experiments", "]", "\n", "merge", "(", "folders", ",", "out_dir", ",", "8", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.dataset_conversion.Task040_KiTS.prepare_submission": [[138, 145], ["subfiles", "maybe_mkdir_p", "n.replace", "shutil.copy", "join", "join"], "function", ["None"], ["", "def", "prepare_submission", "(", "fld", "=", "\"/home/fabian/drives/datasets/results/nnUNet/test_sets/Task048_KiTS_clean/predicted_ens_3d_fullres_3d_cascade_fullres_postprocessed\"", ",", "# '/home/fabian/datasets_fabian/predicted_KiTS_nnUNetTrainerNewCandidate23_FabiansResNet',", "\n", "out", "=", "'/home/fabian/drives/datasets/results/nnUNet/test_sets/Task048_KiTS_clean/submission'", ")", ":", "\n", "    ", "nii", "=", "subfiles", "(", "fld", ",", "join", "=", "False", ",", "suffix", "=", "'.nii.gz'", ")", "\n", "maybe_mkdir_p", "(", "out", ")", "\n", "for", "n", "in", "nii", ":", "\n", "        ", "outfname", "=", "n", ".", "replace", "(", "'case'", ",", "'prediction'", ")", "\n", "shutil", ".", "copy", "(", "join", "(", "fld", ",", "n", ")", ",", "join", "(", "out", ",", "outfname", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.dataset_conversion.Task040_KiTS.pretent_to_be_nnUNetTrainer": [[147, 160], ["join", "join", "load_pickle", "copy.deepcopy", "save_pickle"], "function", ["None"], ["", "", "def", "pretent_to_be_nnUNetTrainer", "(", "base", ",", "folds", "=", "(", "0", ",", "1", ",", "2", ",", "3", ",", "4", ")", ")", ":", "\n", "    ", "\"\"\"\n    changes best checkpoint pickle nnunettrainer class name to nnUNetTrainer\n    :param experiments:\n    :return:\n    \"\"\"", "\n", "for", "fold", "in", "folds", ":", "\n", "        ", "cur", "=", "join", "(", "base", ",", "\"fold_%d\"", "%", "fold", ")", "\n", "pkl_file", "=", "join", "(", "cur", ",", "'model_best.model.pkl'", ")", "\n", "a", "=", "load_pickle", "(", "pkl_file", ")", "\n", "a", "[", "'name_old'", "]", "=", "deepcopy", "(", "a", "[", "'name'", "]", ")", "\n", "a", "[", "'name'", "]", "=", "'nnUNetTrainer'", "\n", "save_pickle", "(", "a", ",", "pkl_file", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.dataset_conversion.Task040_KiTS.reset_trainerName": [[162, 170], ["join", "join", "load_pickle", "save_pickle"], "function", ["None"], ["", "", "def", "reset_trainerName", "(", "base", ",", "folds", "=", "(", "0", ",", "1", ",", "2", ",", "3", ",", "4", ")", ")", ":", "\n", "    ", "for", "fold", "in", "folds", ":", "\n", "        ", "cur", "=", "join", "(", "base", ",", "\"fold_%d\"", "%", "fold", ")", "\n", "pkl_file", "=", "join", "(", "cur", ",", "'model_best.model.pkl'", ")", "\n", "a", "=", "load_pickle", "(", "pkl_file", ")", "\n", "a", "[", "'name'", "]", "=", "a", "[", "'name_old'", "]", "\n", "del", "a", "[", "'name_old'", "]", "\n", "save_pickle", "(", "a", ",", "pkl_file", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.dataset_conversion.Task040_KiTS.nnUNetTrainer_these": [[172, 184], ["join", "join", "Task040_KiTS.pretent_to_be_nnUNetTrainer"], "function", ["home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.dataset_conversion.Task040_KiTS.pretent_to_be_nnUNetTrainer"], ["", "", "def", "nnUNetTrainer_these", "(", "experiments", "=", "(", "'nnUNetTrainerNewCandidate23_FabiansPreActResNet__nnUNetPlans'", ",", "\n", "'nnUNetTrainerNewCandidate23_FabiansResNet__nnUNetPlans'", ",", "\n", "'nnUNetTrainerNewCandidate23__nnUNetPlans'", ")", ")", ":", "\n", "    ", "\"\"\"\n    changes best checkpoint pickle nnunettrainer class name to nnUNetTrainer\n    :param experiments:\n    :return:\n    \"\"\"", "\n", "base", "=", "join", "(", "network_training_output_dir", ",", "\"3d_lowres/Task048_KiTS_clean\"", ")", "\n", "for", "exp", "in", "experiments", ":", "\n", "        ", "cur", "=", "join", "(", "base", ",", "exp", ")", "\n", "pretent_to_be_nnUNetTrainer", "(", "cur", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.dataset_conversion.Task040_KiTS.reset_trainerName_these": [[186, 198], ["join", "join", "Task040_KiTS.reset_trainerName"], "function", ["home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.dataset_conversion.Task040_KiTS.reset_trainerName"], ["", "", "def", "reset_trainerName_these", "(", "experiments", "=", "(", "'nnUNetTrainerNewCandidate23_FabiansPreActResNet__nnUNetPlans'", ",", "\n", "'nnUNetTrainerNewCandidate23_FabiansResNet__nnUNetPlans'", ",", "\n", "'nnUNetTrainerNewCandidate23__nnUNetPlans'", ")", ")", ":", "\n", "    ", "\"\"\"\n    changes best checkpoint pickle nnunettrainer class name to nnUNetTrainer\n    :param experiments:\n    :return:\n    \"\"\"", "\n", "base", "=", "join", "(", "network_training_output_dir", ",", "\"3d_lowres/Task048_KiTS_clean\"", ")", "\n", "for", "exp", "in", "experiments", ":", "\n", "        ", "cur", "=", "join", "(", "base", ",", "exp", ")", "\n", "reset_trainerName", "(", "cur", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.dataset_conversion.Task082_BraTS_2020.apply_brats_threshold": [[37, 47], ["SimpleITK.ReadImage", "SimpleITK.GetArrayFromImage", "numpy.sum", "SimpleITK.GetImageFromArray", "sitk.GetImageFromArray.CopyInformation", "SimpleITK.WriteImage", "print", "join", "fname.split"], "function", ["None"], ["def", "apply_brats_threshold", "(", "fname", ",", "out_dir", ",", "threshold", ",", "replace_with", ")", ":", "\n", "    ", "img_itk", "=", "sitk", ".", "ReadImage", "(", "fname", ")", "\n", "img_npy", "=", "sitk", ".", "GetArrayFromImage", "(", "img_itk", ")", "\n", "s", "=", "np", ".", "sum", "(", "img_npy", "==", "3", ")", "\n", "if", "s", "<", "threshold", ":", "\n", "        ", "print", "(", "s", ",", "fname", ")", "\n", "img_npy", "[", "img_npy", "==", "3", "]", "=", "replace_with", "\n", "", "img_itk_postprocessed", "=", "sitk", ".", "GetImageFromArray", "(", "img_npy", ")", "\n", "img_itk_postprocessed", ".", "CopyInformation", "(", "img_itk", ")", "\n", "sitk", ".", "WriteImage", "(", "img_itk_postprocessed", ",", "join", "(", "out_dir", ",", "fname", ".", "split", "(", "\"/\"", ")", "[", "-", "1", "]", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.dataset_conversion.Task082_BraTS_2020.load_niftis_threshold_compute_dice": [[49, 78], ["SimpleITK.GetArrayFromImage", "SimpleITK.GetArrayFromImage", "numpy.sum", "numpy.sum", "medpy.metric.dc", "SimpleITK.ReadImage", "SimpleITK.ReadImage", "copy.deepcopy"], "function", ["None"], ["", "def", "load_niftis_threshold_compute_dice", "(", "gt_file", ",", "pred_file", ",", "thresholds", ":", "Tuple", "[", "list", ",", "tuple", "]", ")", ":", "\n", "    ", "gt", "=", "sitk", ".", "GetArrayFromImage", "(", "sitk", ".", "ReadImage", "(", "gt_file", ")", ")", "\n", "pred", "=", "sitk", ".", "GetArrayFromImage", "(", "sitk", ".", "ReadImage", "(", "pred_file", ")", ")", "\n", "mask_pred", "=", "pred", "==", "3", "\n", "mask_gt", "=", "gt", "==", "3", "\n", "num_pred", "=", "np", ".", "sum", "(", "mask_pred", ")", "\n", "\n", "num_gt", "=", "np", ".", "sum", "(", "mask_gt", ")", "\n", "dice", "=", "dc", "(", "mask_pred", ",", "mask_gt", ")", "\n", "\n", "res_dice", "=", "{", "}", "\n", "res_was_smaller", "=", "{", "}", "\n", "\n", "for", "t", "in", "thresholds", ":", "\n", "        ", "was_smaller", "=", "False", "\n", "\n", "if", "num_pred", "<", "t", ":", "\n", "            ", "was_smaller", "=", "True", "\n", "if", "num_gt", "==", "0", ":", "\n", "                ", "dice_here", "=", "1.", "\n", "", "else", ":", "\n", "                ", "dice_here", "=", "0.", "\n", "", "", "else", ":", "\n", "            ", "dice_here", "=", "deepcopy", "(", "dice", ")", "\n", "\n", "", "res_dice", "[", "t", "]", "=", "dice_here", "\n", "res_was_smaller", "[", "t", "]", "=", "was_smaller", "\n", "\n", "", "return", "res_was_smaller", ",", "res_dice", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.dataset_conversion.Task082_BraTS_2020.apply_threshold_to_folder": [[80, 89], ["maybe_mkdir_p", "subfiles", "multiprocessing.pool.Pool", "multiprocessing.pool.Pool.starmap", "multiprocessing.pool.Pool.close", "multiprocessing.pool.Pool.join", "zip", "len", "len", "len"], "function", ["None"], ["", "def", "apply_threshold_to_folder", "(", "folder_in", ",", "folder_out", ",", "threshold", ",", "replace_with", ",", "processes", "=", "24", ")", ":", "\n", "    ", "maybe_mkdir_p", "(", "folder_out", ")", "\n", "niftis", "=", "subfiles", "(", "folder_in", ",", "suffix", "=", "'.nii.gz'", ",", "join", "=", "True", ")", "\n", "\n", "p", "=", "Pool", "(", "processes", ")", "\n", "p", ".", "starmap", "(", "apply_brats_threshold", ",", "zip", "(", "niftis", ",", "[", "folder_out", "]", "*", "len", "(", "niftis", ")", ",", "[", "threshold", "]", "*", "len", "(", "niftis", ")", ",", "[", "replace_with", "]", "*", "len", "(", "niftis", ")", ")", ")", "\n", "\n", "p", ".", "close", "(", ")", "\n", "p", ".", "join", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.dataset_conversion.Task082_BraTS_2020.determine_brats_postprocessing": [[91, 120], ["subfiles", "multiprocessing.pool.Pool", "subfiles", "multiprocessing.pool.Pool.starmap_async", "results.get.get", "print", "maybe_mkdir_p", "multiprocessing.pool.Pool.starmap", "multiprocessing.pool.Pool.close", "multiprocessing.pool.Pool.join", "save_pickle", "zip", "numpy.array", "print", "numpy.mean", "zip", "join", "numpy.mean", "numpy.argmax", "len", "numpy.argmax", "len", "len", "len"], "function", ["None"], ["", "def", "determine_brats_postprocessing", "(", "folder_with_preds", ",", "folder_with_gt", ",", "postprocessed_output_dir", ",", "processes", "=", "8", ",", "\n", "thresholds", "=", "(", "0", ",", "10", ",", "50", ",", "100", ",", "200", ",", "500", ",", "750", ",", "1000", ",", "1500", ",", "2500", ",", "10000", ")", ",", "replace_with", "=", "2", ")", ":", "\n", "# find pairs", "\n", "    ", "nifti_gt", "=", "subfiles", "(", "folder_with_gt", ",", "suffix", "=", "\".nii.gz\"", ",", "sort", "=", "True", ")", "\n", "\n", "p", "=", "Pool", "(", "processes", ")", "\n", "\n", "nifti_pred", "=", "subfiles", "(", "folder_with_preds", ",", "suffix", "=", "'.nii.gz'", ",", "sort", "=", "True", ")", "\n", "\n", "results", "=", "p", ".", "starmap_async", "(", "load_niftis_threshold_compute_dice", ",", "zip", "(", "nifti_gt", ",", "nifti_pred", ",", "[", "thresholds", "]", "*", "len", "(", "nifti_pred", ")", ")", ")", "\n", "results", "=", "results", ".", "get", "(", ")", "\n", "\n", "all_dc_per_threshold", "=", "{", "}", "\n", "for", "t", "in", "thresholds", ":", "\n", "        ", "all_dc_per_threshold", "[", "t", "]", "=", "np", ".", "array", "(", "[", "i", "[", "1", "]", "[", "t", "]", "for", "i", "in", "results", "]", ")", "\n", "print", "(", "t", ",", "np", ".", "mean", "(", "all_dc_per_threshold", "[", "t", "]", ")", ")", "\n", "\n", "", "means", "=", "[", "np", ".", "mean", "(", "all_dc_per_threshold", "[", "t", "]", ")", "for", "t", "in", "thresholds", "]", "\n", "best_threshold", "=", "thresholds", "[", "np", ".", "argmax", "(", "means", ")", "]", "\n", "print", "(", "'best'", ",", "best_threshold", ",", "means", "[", "np", ".", "argmax", "(", "means", ")", "]", ")", "\n", "\n", "maybe_mkdir_p", "(", "postprocessed_output_dir", ")", "\n", "\n", "p", ".", "starmap", "(", "apply_brats_threshold", ",", "zip", "(", "nifti_pred", ",", "[", "postprocessed_output_dir", "]", "*", "len", "(", "nifti_pred", ")", ",", "[", "best_threshold", "]", "*", "len", "(", "nifti_pred", ")", ",", "[", "replace_with", "]", "*", "len", "(", "nifti_pred", ")", ")", ")", "\n", "\n", "p", ".", "close", "(", ")", "\n", "p", ".", "join", "(", ")", "\n", "\n", "save_pickle", "(", "(", "thresholds", ",", "means", ",", "best_threshold", ",", "all_dc_per_threshold", ")", ",", "join", "(", "postprocessed_output_dir", ",", "\"threshold.pkl\"", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.dataset_conversion.Task082_BraTS_2020.collect_and_prepare": [[122, 274], ["join", "join", "subfolders", "nnunet.evaluation.region_based_evaluation.get_brats_regions", "join", "join", "join", "maybe_mkdir_p", "Task082_BraTS_2020.apply_threshold_to_folder", "join", "join", "maybe_mkdir_p", "Task082_BraTS_2020.apply_threshold_to_folder", "join", "Task082_BraTS_2020.summarize_validation_set_predictions", "print", "open", "f.write", "open", "f.write", "isdir", "load_pickle", "load_pickle", "join", "join", "maybe_mkdir_p", "maybe_mkdir_p", "nnunet.postprocessing.consolidate_postprocessing.collect_cv_niftis", "successful.append", "join", "join", "join", "isfile", "isfile", "join", "subdirs", "join", "join", "isfile", "isfile", "join", "join", "subfiles", "join", "maybe_mkdir_p", "Task082_BraTS_2020.apply_threshold_to_folder", "has_val_pred.append", "print", "missing_valset.append", "join", "join", "join", "subfiles", "join", "maybe_mkdir_p", "nnunet.dataset_conversion.Task032_BraTS_2018.convert_labels_back_to_BraTS_2018_2019_convention", "join", "nnunet.evaluation.region_based_evaluation.evaluate_regions", "Task082_BraTS_2020.determine_brats_postprocessing", "nnunet.evaluation.region_based_evaluation.evaluate_regions", "print", "failed.append", "f.write", "f.write", "f.write", "f.write", "f.write", "f.write", "f.write", "f.write", "f.write", "f.write", "join", "print", "join", "isfile", "f.write", "f.write", "f.write", "f.write", "f.write", "f.write", "f.write", "f.write", "f.write", "f.write", "len", "print", "load_pickle", "isdir", "print", "RuntimeError", "len", "print", "isfile", "isfile", "isfile", "numpy.loadtxt", "float", "numpy.loadtxt", "float", "nnunet.evaluation.region_based_evaluation.evaluate_regions", "f.write", "f.write", "f.write", "f.write", "f.write", "numpy.loadtxt", "float", "numpy.loadtxt", "float", "join", "join", "join", "list", "join", "numpy.mean", "numpy.mean", "isfile", "join", "numpy.loadtxt", "float", "numpy.mean", "numpy.mean", "numpy.arange", "numpy.mean", "len", "len"], "function", ["home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.evaluation.region_based_evaluation.get_brats_regions", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.dataset_conversion.Task082_BraTS_2020.apply_threshold_to_folder", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.dataset_conversion.Task082_BraTS_2020.apply_threshold_to_folder", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.dataset_conversion.Task082_BraTS_2020.summarize_validation_set_predictions", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.postprocessing.consolidate_postprocessing.collect_cv_niftis", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.dataset_conversion.Task082_BraTS_2020.apply_threshold_to_folder", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.dataset_conversion.Task032_BraTS_2018.convert_labels_back_to_BraTS_2018_2019_convention", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.evaluation.region_based_evaluation.evaluate_regions", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.dataset_conversion.Task082_BraTS_2020.determine_brats_postprocessing", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.evaluation.region_based_evaluation.evaluate_regions", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.evaluation.region_based_evaluation.evaluate_regions"], ["", "def", "collect_and_prepare", "(", "base_dir", ",", "num_processes", "=", "12", ",", "clean", "=", "False", ")", ":", "\n", "    ", "\"\"\"\n    collect all cv_niftis, compute brats metrics, compute enh tumor thresholds and summarize in csv\n    :param base_dir:\n    :return:\n    \"\"\"", "\n", "out", "=", "join", "(", "base_dir", ",", "'cv_results'", ")", "\n", "out_pp", "=", "join", "(", "base_dir", ",", "'cv_results_pp'", ")", "\n", "experiments", "=", "subfolders", "(", "base_dir", ",", "join", "=", "False", ",", "prefix", "=", "'nnUNetTrainer'", ")", "\n", "regions", "=", "get_brats_regions", "(", ")", "\n", "gt_dir", "=", "join", "(", "base_dir", ",", "'gt_niftis'", ")", "\n", "replace_with", "=", "2", "\n", "\n", "failed", "=", "[", "]", "\n", "successful", "=", "[", "]", "\n", "for", "e", "in", "experiments", ":", "\n", "        ", "print", "(", "e", ")", "\n", "try", ":", "\n", "            ", "o", "=", "join", "(", "out", ",", "e", ")", "\n", "o_p", "=", "join", "(", "out_pp", ",", "e", ")", "\n", "maybe_mkdir_p", "(", "o", ")", "\n", "maybe_mkdir_p", "(", "o_p", ")", "\n", "collect_cv_niftis", "(", "join", "(", "base_dir", ",", "e", ")", ",", "o", ")", "\n", "if", "clean", "or", "not", "isfile", "(", "join", "(", "o", ",", "'summary.csv'", ")", ")", ":", "\n", "                ", "evaluate_regions", "(", "o", ",", "gt_dir", ",", "regions", ",", "num_processes", ")", "\n", "", "if", "clean", "or", "not", "isfile", "(", "join", "(", "o_p", ",", "'threshold.pkl'", ")", ")", ":", "\n", "                ", "determine_brats_postprocessing", "(", "o", ",", "gt_dir", ",", "o_p", ",", "num_processes", ",", "thresholds", "=", "list", "(", "np", ".", "arange", "(", "0", ",", "760", ",", "10", ")", ")", ",", "replace_with", "=", "replace_with", ")", "\n", "", "if", "clean", "or", "not", "isfile", "(", "join", "(", "o_p", ",", "'summary.csv'", ")", ")", ":", "\n", "                ", "evaluate_regions", "(", "o_p", ",", "gt_dir", ",", "regions", ",", "num_processes", ")", "\n", "", "successful", ".", "append", "(", "e", ")", "\n", "", "except", "Exception", "as", "ex", ":", "\n", "            ", "print", "(", "\"\\nERROR\\n\"", ",", "e", ",", "ex", ",", "\"\\n\"", ")", "\n", "failed", ".", "append", "(", "e", ")", "\n", "\n", "# we are interested in the mean (nan is 1) column", "\n", "", "", "with", "open", "(", "join", "(", "base_dir", ",", "'cv_summary.csv'", ")", ",", "'w'", ")", "as", "f", ":", "\n", "        ", "f", ".", "write", "(", "'name,whole,core,enh,mean\\n'", ")", "\n", "for", "e", "in", "successful", ":", "\n", "            ", "expected_nopp", "=", "join", "(", "out", ",", "e", ",", "'summary.csv'", ")", "\n", "expected_pp", "=", "join", "(", "out", ",", "out_pp", ",", "e", ",", "'summary.csv'", ")", "\n", "if", "isfile", "(", "expected_nopp", ")", ":", "\n", "                ", "res", "=", "np", ".", "loadtxt", "(", "expected_nopp", ",", "dtype", "=", "str", ",", "skiprows", "=", "0", ",", "delimiter", "=", "','", ")", "[", "-", "2", "]", "\n", "as_numeric", "=", "[", "float", "(", "i", ")", "for", "i", "in", "res", "[", "1", ":", "]", "]", "\n", "f", ".", "write", "(", "e", "+", "'_noPP,'", ")", "\n", "f", ".", "write", "(", "\"%0.4f,\"", "%", "as_numeric", "[", "0", "]", ")", "\n", "f", ".", "write", "(", "\"%0.4f,\"", "%", "as_numeric", "[", "1", "]", ")", "\n", "f", ".", "write", "(", "\"%0.4f,\"", "%", "as_numeric", "[", "2", "]", ")", "\n", "f", ".", "write", "(", "\"%0.4f\\n\"", "%", "np", ".", "mean", "(", "as_numeric", ")", ")", "\n", "", "if", "isfile", "(", "expected_pp", ")", ":", "\n", "                ", "res", "=", "np", ".", "loadtxt", "(", "expected_pp", ",", "dtype", "=", "str", ",", "skiprows", "=", "0", ",", "delimiter", "=", "','", ")", "[", "-", "2", "]", "\n", "as_numeric", "=", "[", "float", "(", "i", ")", "for", "i", "in", "res", "[", "1", ":", "]", "]", "\n", "f", ".", "write", "(", "e", "+", "'_PP,'", ")", "\n", "f", ".", "write", "(", "\"%0.4f,\"", "%", "as_numeric", "[", "0", "]", ")", "\n", "f", ".", "write", "(", "\"%0.4f,\"", "%", "as_numeric", "[", "1", "]", ")", "\n", "f", ".", "write", "(", "\"%0.4f,\"", "%", "as_numeric", "[", "2", "]", ")", "\n", "f", ".", "write", "(", "\"%0.4f\\n\"", "%", "np", ".", "mean", "(", "as_numeric", ")", ")", "\n", "\n", "# this just crawls the folders and evaluates what it finds", "\n", "", "", "", "with", "open", "(", "join", "(", "base_dir", ",", "'cv_summary2.csv'", ")", ",", "'w'", ")", "as", "f", ":", "\n", "        ", "for", "folder", "in", "[", "'cv_results'", ",", "'cv_results_pp'", "]", ":", "\n", "            ", "for", "ex", "in", "subdirs", "(", "join", "(", "base_dir", ",", "folder", ")", ",", "join", "=", "False", ")", ":", "\n", "                ", "print", "(", "folder", ",", "ex", ")", "\n", "expected", "=", "join", "(", "base_dir", ",", "folder", ",", "ex", ",", "'summary.csv'", ")", "\n", "if", "clean", "or", "not", "isfile", "(", "expected", ")", ":", "\n", "                    ", "evaluate_regions", "(", "join", "(", "base_dir", ",", "folder", ",", "ex", ")", ",", "gt_dir", ",", "regions", ",", "num_processes", ")", "\n", "", "if", "isfile", "(", "expected", ")", ":", "\n", "                    ", "res", "=", "np", ".", "loadtxt", "(", "expected", ",", "dtype", "=", "str", ",", "skiprows", "=", "0", ",", "delimiter", "=", "','", ")", "[", "-", "2", "]", "\n", "as_numeric", "=", "[", "float", "(", "i", ")", "for", "i", "in", "res", "[", "1", ":", "]", "]", "\n", "f", ".", "write", "(", "'%s__%s,'", "%", "(", "folder", ",", "ex", ")", ")", "\n", "f", ".", "write", "(", "\"%0.4f,\"", "%", "as_numeric", "[", "0", "]", ")", "\n", "f", ".", "write", "(", "\"%0.4f,\"", "%", "as_numeric", "[", "1", "]", ")", "\n", "f", ".", "write", "(", "\"%0.4f,\"", "%", "as_numeric", "[", "2", "]", ")", "\n", "f", ".", "write", "(", "\"%0.4f\\n\"", "%", "np", ".", "mean", "(", "as_numeric", ")", ")", "\n", "\n", "", "", "", "f", ".", "write", "(", "'name,whole,core,enh,mean\\n'", ")", "\n", "for", "e", "in", "successful", ":", "\n", "            ", "expected_nopp", "=", "join", "(", "out", ",", "e", ",", "'summary.csv'", ")", "\n", "expected_pp", "=", "join", "(", "out", ",", "out_pp", ",", "e", ",", "'summary.csv'", ")", "\n", "if", "isfile", "(", "expected_nopp", ")", ":", "\n", "                ", "res", "=", "np", ".", "loadtxt", "(", "expected_nopp", ",", "dtype", "=", "str", ",", "skiprows", "=", "0", ",", "delimiter", "=", "','", ")", "[", "-", "2", "]", "\n", "as_numeric", "=", "[", "float", "(", "i", ")", "for", "i", "in", "res", "[", "1", ":", "]", "]", "\n", "f", ".", "write", "(", "e", "+", "'_noPP,'", ")", "\n", "f", ".", "write", "(", "\"%0.4f,\"", "%", "as_numeric", "[", "0", "]", ")", "\n", "f", ".", "write", "(", "\"%0.4f,\"", "%", "as_numeric", "[", "1", "]", ")", "\n", "f", ".", "write", "(", "\"%0.4f,\"", "%", "as_numeric", "[", "2", "]", ")", "\n", "f", ".", "write", "(", "\"%0.4f\\n\"", "%", "np", ".", "mean", "(", "as_numeric", ")", ")", "\n", "", "if", "isfile", "(", "expected_pp", ")", ":", "\n", "                ", "res", "=", "np", ".", "loadtxt", "(", "expected_pp", ",", "dtype", "=", "str", ",", "skiprows", "=", "0", ",", "delimiter", "=", "','", ")", "[", "-", "2", "]", "\n", "as_numeric", "=", "[", "float", "(", "i", ")", "for", "i", "in", "res", "[", "1", ":", "]", "]", "\n", "f", ".", "write", "(", "e", "+", "'_PP,'", ")", "\n", "f", ".", "write", "(", "\"%0.4f,\"", "%", "as_numeric", "[", "0", "]", ")", "\n", "f", ".", "write", "(", "\"%0.4f,\"", "%", "as_numeric", "[", "1", "]", ")", "\n", "f", ".", "write", "(", "\"%0.4f,\"", "%", "as_numeric", "[", "2", "]", ")", "\n", "f", ".", "write", "(", "\"%0.4f\\n\"", "%", "np", ".", "mean", "(", "as_numeric", ")", ")", "\n", "\n", "# apply threshold to val set", "\n", "", "", "", "expected_num_cases", "=", "125", "\n", "missing_valset", "=", "[", "]", "\n", "has_val_pred", "=", "[", "]", "\n", "for", "e", "in", "successful", ":", "\n", "        ", "if", "isdir", "(", "join", "(", "base_dir", ",", "'predVal'", ",", "e", ")", ")", ":", "\n", "            ", "currdir", "=", "join", "(", "base_dir", ",", "'predVal'", ",", "e", ")", "\n", "files", "=", "subfiles", "(", "currdir", ",", "suffix", "=", "'.nii.gz'", ",", "join", "=", "False", ")", "\n", "if", "len", "(", "files", ")", "!=", "expected_num_cases", ":", "\n", "                ", "print", "(", "e", ",", "'prediction not done, found %d files, expected %s'", "%", "(", "len", "(", "files", ")", ",", "expected_num_cases", ")", ")", "\n", "continue", "\n", "", "output_folder", "=", "join", "(", "base_dir", ",", "'predVal_PP'", ",", "e", ")", "\n", "maybe_mkdir_p", "(", "output_folder", ")", "\n", "threshold", "=", "load_pickle", "(", "join", "(", "out_pp", ",", "e", ",", "'threshold.pkl'", ")", ")", "[", "2", "]", "\n", "if", "threshold", ">", "1000", ":", "threshold", "=", "750", "# don't make it too big!", "\n", "apply_threshold_to_folder", "(", "currdir", ",", "output_folder", ",", "threshold", ",", "replace_with", ",", "num_processes", ")", "\n", "has_val_pred", ".", "append", "(", "e", ")", "\n", "", "else", ":", "\n", "            ", "print", "(", "e", ",", "'has no valset predictions'", ")", "\n", "missing_valset", ".", "append", "(", "e", ")", "\n", "\n", "# 'nnUNetTrainerV2BraTSRegions_DA3_BN__nnUNetPlansv2.1_bs5_15fold' needs special treatment", "\n", "", "", "e", "=", "'nnUNetTrainerV2BraTSRegions_DA3_BN__nnUNetPlansv2.1_bs5'", "\n", "currdir", "=", "join", "(", "base_dir", ",", "'predVal'", ",", "'nnUNetTrainerV2BraTSRegions_DA3_BN__nnUNetPlansv2.1_bs5_15fold'", ")", "\n", "output_folder", "=", "join", "(", "base_dir", ",", "'predVal_PP'", ",", "'nnUNetTrainerV2BraTSRegions_DA3_BN__nnUNetPlansv2.1_bs5_15fold'", ")", "\n", "maybe_mkdir_p", "(", "output_folder", ")", "\n", "threshold", "=", "load_pickle", "(", "join", "(", "out_pp", ",", "e", ",", "'threshold.pkl'", ")", ")", "[", "2", "]", "\n", "if", "threshold", ">", "1000", ":", "threshold", "=", "750", "# don't make it too big!", "\n", "apply_threshold_to_folder", "(", "currdir", ",", "output_folder", ",", "threshold", ",", "replace_with", ",", "num_processes", ")", "\n", "\n", "# 'nnUNetTrainerV2BraTSRegions_DA3_BN__nnUNetPlansv2.1_bs5_15fold' needs special treatment", "\n", "e", "=", "'nnUNetTrainerV2BraTSRegions_DA4_BN__nnUNetPlansv2.1_bs5'", "\n", "currdir", "=", "join", "(", "base_dir", ",", "'predVal'", ",", "'nnUNetTrainerV2BraTSRegions_DA4_BN__nnUNetPlansv2.1_bs5_15fold'", ")", "\n", "output_folder", "=", "join", "(", "base_dir", ",", "'predVal_PP'", ",", "'nnUNetTrainerV2BraTSRegions_DA4_BN__nnUNetPlansv2.1_bs5_15fold'", ")", "\n", "maybe_mkdir_p", "(", "output_folder", ")", "\n", "threshold", "=", "load_pickle", "(", "join", "(", "out_pp", ",", "e", ",", "'threshold.pkl'", ")", ")", "[", "2", "]", "\n", "if", "threshold", ">", "1000", ":", "threshold", "=", "750", "# don't make it too big!", "\n", "apply_threshold_to_folder", "(", "currdir", ",", "output_folder", ",", "threshold", ",", "replace_with", ",", "num_processes", ")", "\n", "\n", "# convert val set to brats labels for submission", "\n", "output_converted", "=", "join", "(", "base_dir", ",", "'converted_valSet'", ")", "\n", "\n", "for", "source", "in", "[", "'predVal'", ",", "'predVal_PP'", "]", ":", "\n", "        ", "for", "e", "in", "has_val_pred", "+", "[", "'nnUNetTrainerV2BraTSRegions_DA3_BN__nnUNetPlansv2.1_bs5_15fold'", ",", "'nnUNetTrainerV2BraTSRegions_DA4_BN__nnUNetPlansv2.1_bs5_15fold'", "]", ":", "\n", "            ", "expected_source_folder", "=", "join", "(", "base_dir", ",", "source", ",", "e", ")", "\n", "if", "not", "isdir", "(", "expected_source_folder", ")", ":", "\n", "                ", "print", "(", "e", ",", "'has no'", ",", "source", ")", "\n", "raise", "RuntimeError", "(", ")", "\n", "", "files", "=", "subfiles", "(", "expected_source_folder", ",", "suffix", "=", "'.nii.gz'", ",", "join", "=", "False", ")", "\n", "if", "len", "(", "files", ")", "!=", "expected_num_cases", ":", "\n", "                ", "print", "(", "e", ",", "'prediction not done, found %d files, expected %s'", "%", "(", "len", "(", "files", ")", ",", "expected_num_cases", ")", ")", "\n", "continue", "\n", "", "target_folder", "=", "join", "(", "output_converted", ",", "source", ",", "e", ")", "\n", "maybe_mkdir_p", "(", "target_folder", ")", "\n", "convert_labels_back_to_BraTS_2018_2019_convention", "(", "expected_source_folder", ",", "target_folder", ")", "\n", "\n", "", "", "summarize_validation_set_predictions", "(", "output_converted", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.dataset_conversion.Task082_BraTS_2020.summarize_validation_set_predictions": [[276, 299], ["open", "f.write", "subfolders", "join", "subfolders", "join", "join", "numpy.loadtxt", "f.write", "f.write", "f.write", "f.write", "f.write", "f.write", "f.write", "f.write", "f.write", "isfile", "print", "float", "float", "numpy.mean", "numpy.mean"], "function", ["None"], ["", "def", "summarize_validation_set_predictions", "(", "base", ")", ":", "\n", "    ", "with", "open", "(", "join", "(", "base", ",", "'summary.csv'", ")", ",", "'w'", ")", "as", "f", ":", "\n", "        ", "f", ".", "write", "(", "'name,whole,core,enh,mean,whole,core,enh,mean\\n'", ")", "\n", "for", "subf", "in", "subfolders", "(", "base", ",", "join", "=", "False", ")", ":", "\n", "            ", "for", "e", "in", "subfolders", "(", "join", "(", "base", ",", "subf", ")", ",", "join", "=", "False", ")", ":", "\n", "                ", "expected", "=", "join", "(", "base", ",", "subf", ",", "e", ",", "'Stats_Validation_final.csv'", ")", "\n", "if", "not", "isfile", "(", "expected", ")", ":", "\n", "                    ", "print", "(", "subf", ",", "e", ",", "'has missing csv'", ")", "\n", "continue", "\n", "", "a", "=", "np", ".", "loadtxt", "(", "expected", ",", "delimiter", "=", "','", ",", "dtype", "=", "str", ")", "\n", "assert", "a", ".", "shape", "[", "0", "]", "==", "131", ",", "'did not evaluate all 125 cases!'", "\n", "selected_row", "=", "a", "[", "-", "5", "]", "\n", "values", "=", "[", "float", "(", "i", ")", "for", "i", "in", "selected_row", "[", "1", ":", "4", "]", "]", "\n", "f", ".", "write", "(", "e", "+", "\"_\"", "+", "subf", "+", "','", ")", "\n", "f", ".", "write", "(", "\"%0.4f,\"", "%", "values", "[", "1", "]", ")", "\n", "f", ".", "write", "(", "\"%0.4f,\"", "%", "values", "[", "2", "]", ")", "\n", "f", ".", "write", "(", "\"%0.4f,\"", "%", "values", "[", "0", "]", ")", "\n", "f", ".", "write", "(", "\"%0.4f,\"", "%", "np", ".", "mean", "(", "values", ")", ")", "\n", "values", "=", "[", "float", "(", "i", ")", "for", "i", "in", "selected_row", "[", "-", "3", ":", "]", "]", "\n", "f", ".", "write", "(", "\"%0.4f,\"", "%", "values", "[", "1", "]", ")", "\n", "f", ".", "write", "(", "\"%0.4f,\"", "%", "values", "[", "2", "]", ")", "\n", "f", ".", "write", "(", "\"%0.4f,\"", "%", "values", "[", "0", "]", ")", "\n", "f", ".", "write", "(", "\"%0.4f\\n\"", "%", "np", ".", "mean", "(", "values", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.dataset_conversion.Task082_BraTS_2020.compute_BraTS_dice": [[301, 318], ["numpy.sum", "numpy.sum", "medpy.metric.dc"], "function", ["None"], ["", "", "", "", "def", "compute_BraTS_dice", "(", "ref", ",", "pred", ")", ":", "\n", "    ", "\"\"\"\n    ref and gt are binary integer numpy.ndarray s\n    :param ref:\n    :param gt:\n    :return:\n    \"\"\"", "\n", "num_ref", "=", "np", ".", "sum", "(", "ref", ")", "\n", "num_pred", "=", "np", ".", "sum", "(", "pred", ")", "\n", "\n", "if", "num_ref", "==", "0", ":", "\n", "        ", "if", "num_pred", "==", "0", ":", "\n", "            ", "return", "1", "\n", "", "else", ":", "\n", "            ", "return", "0", "\n", "", "", "else", ":", "\n", "        ", "return", "dc", "(", "pred", ",", "ref", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.dataset_conversion.Task082_BraTS_2020.convert_all_to_BraTS": [[319, 327], ["subdirs", "subfiles", "join", "len", "print", "join", "nnunet.dataset_conversion.Task032_BraTS_2018.convert_labels_back_to_BraTS_2018_2019_convention", "join"], "function", ["home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.dataset_conversion.Task032_BraTS_2018.convert_labels_back_to_BraTS_2018_2019_convention"], ["", "", "def", "convert_all_to_BraTS", "(", "input_folder", ",", "output_folder", ",", "expected_num_cases", "=", "125", ")", ":", "\n", "    ", "for", "s", "in", "subdirs", "(", "input_folder", ",", "join", "=", "False", ")", ":", "\n", "        ", "nii", "=", "subfiles", "(", "join", "(", "input_folder", ",", "s", ")", ",", "suffix", "=", "'.nii.gz'", ",", "join", "=", "False", ")", "\n", "if", "len", "(", "nii", ")", "!=", "expected_num_cases", ":", "\n", "            ", "print", "(", "s", ")", "\n", "", "else", ":", "\n", "            ", "target_dir", "=", "join", "(", "output_folder", ",", "s", ")", "\n", "convert_labels_back_to_BraTS_2018_2019_convention", "(", "join", "(", "input_folder", ",", "s", ")", ",", "target_dir", ",", "num_processes", "=", "6", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.dataset_conversion.Task082_BraTS_2020.compute_BraTS_HD95": [[329, 349], ["numpy.sum", "numpy.sum", "medpy.metric.hd95"], "function", ["None"], ["", "", "", "def", "compute_BraTS_HD95", "(", "ref", ",", "pred", ")", ":", "\n", "    ", "\"\"\"\n    ref and gt are binary integer numpy.ndarray s\n    spacing is assumed to be (1, 1, 1)\n    :param ref:\n    :param pred:\n    :return:\n    \"\"\"", "\n", "num_ref", "=", "np", ".", "sum", "(", "ref", ")", "\n", "num_pred", "=", "np", ".", "sum", "(", "pred", ")", "\n", "\n", "if", "num_ref", "==", "0", ":", "\n", "        ", "if", "num_pred", "==", "0", ":", "\n", "            ", "return", "0", "\n", "", "else", ":", "\n", "            ", "return", "373.12866", "\n", "", "", "elif", "num_pred", "==", "0", "and", "num_ref", "!=", "0", ":", "\n", "        ", "return", "373.12866", "\n", "", "else", ":", "\n", "        ", "return", "hd95", "(", "pred", ",", "ref", ",", "(", "1", ",", "1", ",", "1", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.dataset_conversion.Task082_BraTS_2020.evaluate_BraTS_case": [[351, 381], ["Task082_BraTS_2020.compute_BraTS_dice", "Task082_BraTS_2020.compute_BraTS_HD95", "Task082_BraTS_2020.compute_BraTS_dice", "Task082_BraTS_2020.compute_BraTS_HD95", "Task082_BraTS_2020.compute_BraTS_dice", "Task082_BraTS_2020.compute_BraTS_HD95"], "function", ["home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.dataset_conversion.Task082_BraTS_2020.compute_BraTS_dice", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.dataset_conversion.Task082_BraTS_2020.compute_BraTS_HD95", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.dataset_conversion.Task082_BraTS_2020.compute_BraTS_dice", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.dataset_conversion.Task082_BraTS_2020.compute_BraTS_HD95", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.dataset_conversion.Task082_BraTS_2020.compute_BraTS_dice", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.dataset_conversion.Task082_BraTS_2020.compute_BraTS_HD95"], ["", "", "def", "evaluate_BraTS_case", "(", "arr", ":", "np", ".", "ndarray", ",", "arr_gt", ":", "np", ".", "ndarray", ")", ":", "\n", "    ", "\"\"\"\n    attempting to reimplement the brats evaluation scheme\n    assumes edema=1, non_enh=2, enh=3\n    :param arr:\n    :param arr_gt:\n    :return:\n    \"\"\"", "\n", "# whole tumor", "\n", "mask_gt", "=", "(", "arr_gt", "!=", "0", ")", ".", "astype", "(", "int", ")", "\n", "mask_pred", "=", "(", "arr", "!=", "0", ")", ".", "astype", "(", "int", ")", "\n", "dc_whole", "=", "compute_BraTS_dice", "(", "mask_gt", ",", "mask_pred", ")", "\n", "hd95_whole", "=", "compute_BraTS_HD95", "(", "mask_gt", ",", "mask_pred", ")", "\n", "del", "mask_gt", ",", "mask_pred", "\n", "\n", "# tumor core", "\n", "mask_gt", "=", "(", "arr_gt", ">", "1", ")", ".", "astype", "(", "int", ")", "\n", "mask_pred", "=", "(", "arr", ">", "1", ")", ".", "astype", "(", "int", ")", "\n", "dc_core", "=", "compute_BraTS_dice", "(", "mask_gt", ",", "mask_pred", ")", "\n", "hd95_core", "=", "compute_BraTS_HD95", "(", "mask_gt", ",", "mask_pred", ")", "\n", "del", "mask_gt", ",", "mask_pred", "\n", "\n", "# enhancing", "\n", "mask_gt", "=", "(", "arr_gt", "==", "3", ")", ".", "astype", "(", "int", ")", "\n", "mask_pred", "=", "(", "arr", "==", "3", ")", ".", "astype", "(", "int", ")", "\n", "dc_enh", "=", "compute_BraTS_dice", "(", "mask_gt", ",", "mask_pred", ")", "\n", "hd95_enh", "=", "compute_BraTS_HD95", "(", "mask_gt", ",", "mask_pred", ")", "\n", "del", "mask_gt", ",", "mask_pred", "\n", "\n", "return", "dc_whole", ",", "dc_core", ",", "dc_enh", ",", "hd95_whole", ",", "hd95_core", ",", "hd95_enh", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.dataset_conversion.Task082_BraTS_2020.load_evaluate": [[383, 387], ["SimpleITK.GetArrayFromImage", "SimpleITK.GetArrayFromImage", "Task082_BraTS_2020.evaluate_BraTS_case", "SimpleITK.ReadImage", "SimpleITK.ReadImage"], "function", ["home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.dataset_conversion.Task082_BraTS_2020.evaluate_BraTS_case"], ["", "def", "load_evaluate", "(", "filename_gt", ":", "str", ",", "filename_pred", ":", "str", ")", ":", "\n", "    ", "arr_pred", "=", "sitk", ".", "GetArrayFromImage", "(", "sitk", ".", "ReadImage", "(", "filename_pred", ")", ")", "\n", "arr_gt", "=", "sitk", ".", "GetArrayFromImage", "(", "sitk", ".", "ReadImage", "(", "filename_gt", ")", ")", "\n", "return", "evaluate_BraTS_case", "(", "arr_pred", ",", "arr_gt", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.dataset_conversion.Task082_BraTS_2020.evaluate_BraTS_folder": [[389, 407], ["subfiles", "subfiles", "all", "multiprocessing.pool.Pool", "multiprocessing.pool.Pool.starmap", "len", "all", "join", "join", "zip", "open", "f.write", "zip", "join", "f.write", "f.write"], "function", ["None"], ["", "def", "evaluate_BraTS_folder", "(", "folder_pred", ",", "folder_gt", ",", "num_processes", ":", "int", "=", "24", ",", "strict", "=", "False", ")", ":", "\n", "    ", "nii_pred", "=", "subfiles", "(", "folder_pred", ",", "suffix", "=", "'.nii.gz'", ",", "join", "=", "False", ")", "\n", "if", "len", "(", "nii_pred", ")", "==", "0", ":", "\n", "        ", "return", "\n", "", "nii_gt", "=", "subfiles", "(", "folder_gt", ",", "suffix", "=", "'.nii.gz'", ",", "join", "=", "False", ")", "\n", "assert", "all", "(", "[", "i", "in", "nii_gt", "for", "i", "in", "nii_pred", "]", ")", ",", "'not all predicted niftis have a reference file!'", "\n", "if", "strict", ":", "\n", "        ", "assert", "all", "(", "[", "i", "in", "nii_pred", "for", "i", "in", "nii_gt", "]", ")", ",", "'not all gt niftis have a predicted file!'", "\n", "", "p", "=", "Pool", "(", "num_processes", ")", "\n", "nii_pred_fullpath", "=", "[", "join", "(", "folder_pred", ",", "i", ")", "for", "i", "in", "nii_pred", "]", "\n", "nii_gt_fullpath", "=", "[", "join", "(", "folder_gt", ",", "i", ")", "for", "i", "in", "nii_pred", "]", "\n", "results", "=", "p", ".", "starmap", "(", "load_evaluate", ",", "zip", "(", "nii_gt_fullpath", ",", "nii_pred_fullpath", ")", ")", "\n", "# now write to output file", "\n", "with", "open", "(", "join", "(", "folder_pred", ",", "'results.csv'", ")", ",", "'w'", ")", "as", "f", ":", "\n", "        ", "f", ".", "write", "(", "\"name,dc_whole,dc_core,dc_enh,hd95_whole,hd95_core,hd95_enh\\n\"", ")", "\n", "for", "fname", ",", "r", "in", "zip", "(", "nii_pred", ",", "results", ")", ":", "\n", "            ", "f", ".", "write", "(", "fname", ")", "\n", "f", ".", "write", "(", "\",%0.4f,%0.4f,%0.4f,%3.3f,%3.3f,%3.3f\\n\"", "%", "r", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.dataset_conversion.Task082_BraTS_2020.load_csv_for_ranking": [[409, 417], ["numpy.loadtxt", "res[].astype", "numpy.all", "numpy.all"], "function", ["None"], ["", "", "", "def", "load_csv_for_ranking", "(", "csv_file", ":", "str", ")", ":", "\n", "    ", "res", "=", "np", ".", "loadtxt", "(", "csv_file", ",", "dtype", "=", "'str'", ",", "delimiter", "=", "','", ")", "\n", "scores", "=", "res", "[", "1", ":", ",", "[", "1", ",", "2", ",", "3", ",", "-", "3", ",", "-", "2", ",", "-", "1", "]", "]", ".", "astype", "(", "float", ")", "\n", "scores", "[", ":", ",", "-", "3", ":", "]", "*=", "-", "1", "\n", "scores", "[", ":", ",", "-", "3", ":", "]", "+=", "373.129", "\n", "assert", "np", ".", "all", "(", "scores", "<=", "373.129", ")", "\n", "assert", "np", ".", "all", "(", "scores", ">=", "0", ")", "\n", "return", "scores", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.dataset_conversion.Task082_BraTS_2020.rank_algorithms": [[419, 433], ["numpy.zeros", "range", "numpy.mean", "scipy.rankdata", "numpy.apply_along_axis", "np.apply_along_axis.mean"], "function", ["None"], ["", "def", "rank_algorithms", "(", "data", ":", "np", ".", "ndarray", ")", ":", "\n", "    ", "\"\"\"\n    data is (metrics x experiments x cases)\n    :param data:\n    :return:\n    \"\"\"", "\n", "num_metrics", ",", "num_experiments", ",", "num_cases", "=", "data", ".", "shape", "\n", "ranks", "=", "np", ".", "zeros", "(", "(", "num_metrics", ",", "num_experiments", ")", ")", "\n", "for", "m", "in", "range", "(", "6", ")", ":", "\n", "        ", "r", "=", "np", ".", "apply_along_axis", "(", "ss", ".", "rankdata", ",", "0", ",", "-", "data", "[", "m", "]", ",", "'min'", ")", "\n", "ranks", "[", "m", "]", "=", "r", ".", "mean", "(", "1", ")", "\n", "", "average_rank", "=", "np", ".", "mean", "(", "ranks", ",", "0", ")", "\n", "final_ranks", "=", "ss", ".", "rankdata", "(", "average_rank", ",", "'min'", ")", "\n", "return", "final_ranks", ",", "average_rank", ",", "ranks", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.dataset_conversion.Task082_BraTS_2020.score_and_postprocess_model_based_on_rank_then_aggregate": [[435, 623], ["join", "maybe_mkdir_p", "join", "subfolders", "join", "numpy.loadtxt", "numpy.zeros", "enumerate", "Task082_BraTS_2020.rank_algorithms", "numpy.argsort", "numpy.arange", "join", "numpy.concatenate().transpose", "numpy.concatenate", "Task082_BraTS_2020.rank_algorithms", "numpy.argsort", "join", "numpy.argmin", "int", "join", "Task082_BraTS_2020.apply_threshold_to_folder", "has_val_pred.append", "numpy.argmin", "int", "join", "Task082_BraTS_2020.apply_threshold_to_folder", "has_val_pred.append", "join", "subdirs", "numpy.concatenate", "results_valset.transpose.transpose", "Task082_BraTS_2020.rank_algorithms", "numpy.argsort", "print", "join", "maybe_mkdir_p", "join", "len", "Task082_BraTS_2020.load_csv_for_ranking", "range", "print", "join", "print", "print", "join", "join", "join", "join", "subfiles", "join", "maybe_mkdir_p", "nnunet.dataset_conversion.Task032_BraTS_2018.convert_labels_back_to_BraTS_2018_2019_convention", "join", "subdirs", "print", "nnunet.postprocessing.consolidate_postprocessing.collect_cv_niftis", "experiments_with_full_cv.append", "len", "join", "join", "maybe_mkdir_p", "join", "join", "results.append", "experiment_names.append", "numpy.concatenate", "len", "numpy.argmin", "print", "print", "int", "print", "enumerate", "best.split", "enumerate", "best.split", "isdir", "print", "RuntimeError", "len", "print", "join", "join", "join", "isfile", "Task082_BraTS_2020.evaluate_BraTS_folder", "print", "isfile", "str", "isfile", "Task082_BraTS_2020.apply_threshold_to_folder", "Task082_BraTS_2020.evaluate_BraTS_folder", "str", "isfile", "print", "Task082_BraTS_2020.load_csv_for_ranking", "experiment_names.index", "experiment_names.index", "enumerate", "min", "isdir", "print", "subfiles", "isfile", "print", "results_valset.transpose.append", "names_valset.append", "join", "join", "os.remove", "join", "best.split", "len", "print", "Task082_BraTS_2020.apply_threshold_to_folder", "has_val_pred.append", "Task082_BraTS_2020.load_csv_for_ranking", "join", "join", "i.split", "i.split", "len", "i.split", "len"], "function", ["home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.dataset_conversion.Task082_BraTS_2020.rank_algorithms", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.dataset_conversion.Task082_BraTS_2020.rank_algorithms", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.dataset_conversion.Task082_BraTS_2020.apply_threshold_to_folder", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.dataset_conversion.Task082_BraTS_2020.apply_threshold_to_folder", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.dataset_conversion.Task082_BraTS_2020.rank_algorithms", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.dataset_conversion.Task082_BraTS_2020.load_csv_for_ranking", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.dataset_conversion.Task032_BraTS_2018.convert_labels_back_to_BraTS_2018_2019_convention", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.postprocessing.consolidate_postprocessing.collect_cv_niftis", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.dataset_conversion.Task082_BraTS_2020.evaluate_BraTS_folder", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.dataset_conversion.Task082_BraTS_2020.apply_threshold_to_folder", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.dataset_conversion.Task082_BraTS_2020.evaluate_BraTS_folder", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.dataset_conversion.Task082_BraTS_2020.load_csv_for_ranking", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.dataset_conversion.Task082_BraTS_2020.apply_threshold_to_folder", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.dataset_conversion.Task082_BraTS_2020.load_csv_for_ranking"], ["", "def", "score_and_postprocess_model_based_on_rank_then_aggregate", "(", ")", ":", "\n", "    ", "\"\"\"\n    Similarly to BraTS 2017 - BraTS 2019, each participant will be ranked for each of the X test cases. Each case\n    includes 3 regions of evaluation, and the metrics used to produce the rankings will be the Dice Similarity\n    Coefficient and the 95% Hausdorff distance. Thus, for X number of cases included in the BraTS 2020, each\n    participant ends up having X*3*2 rankings. The final ranking score is the average of all these rankings normalized\n    by the number of teams.\n    https://zenodo.org/record/3718904\n\n    -> let's optimize for this.\n\n    Important: the outcome very much depends on the competing models. We need some references. We only got our own,\n    so let's hope this still works\n    :return:\n    \"\"\"", "\n", "base", "=", "\"/media/fabian/Results/nnUNet/3d_fullres/Task082_BraTS2020\"", "\n", "replace_with", "=", "2", "\n", "num_processes", "=", "24", "\n", "expected_num_cases_val", "=", "125", "\n", "\n", "# use a separate output folder from the previous experiments to ensure we are not messing things up", "\n", "output_base_here", "=", "join", "(", "base", ",", "'use_brats_ranking'", ")", "\n", "maybe_mkdir_p", "(", "output_base_here", ")", "\n", "\n", "# collect cv niftis and compute metrics with evaluate_BraTS_folder to ensure we work with the same metrics as brats", "\n", "out", "=", "join", "(", "output_base_here", ",", "'cv_results'", ")", "\n", "experiments", "=", "subfolders", "(", "base", ",", "join", "=", "False", ",", "prefix", "=", "'nnUNetTrainer'", ")", "\n", "gt_dir", "=", "join", "(", "base", ",", "'gt_niftis'", ")", "\n", "\n", "experiments_with_full_cv", "=", "[", "]", "\n", "for", "e", "in", "experiments", ":", "\n", "        ", "print", "(", "e", ")", "\n", "o", "=", "join", "(", "out", ",", "e", ")", "\n", "maybe_mkdir_p", "(", "o", ")", "\n", "try", ":", "\n", "            ", "collect_cv_niftis", "(", "join", "(", "base", ",", "e", ")", ",", "o", ")", "\n", "if", "not", "isfile", "(", "join", "(", "o", ",", "'results.csv'", ")", ")", ":", "\n", "                ", "evaluate_BraTS_folder", "(", "o", ",", "gt_dir", ",", "num_processes", ",", "strict", "=", "True", ")", "\n", "", "experiments_with_full_cv", ".", "append", "(", "e", ")", "\n", "", "except", "Exception", "as", "ex", ":", "\n", "            ", "print", "(", "\"\\nERROR\\n\"", ",", "e", ",", "ex", ",", "\"\\n\"", ")", "\n", "if", "isfile", "(", "join", "(", "o", ",", "'results.csv'", ")", ")", ":", "\n", "                ", "os", ".", "remove", "(", "join", "(", "o", ",", "'results.csv'", ")", ")", "\n", "\n", "# rank the non-postprocessed models", "\n", "", "", "", "tmp", "=", "np", ".", "loadtxt", "(", "join", "(", "out", ",", "experiments_with_full_cv", "[", "0", "]", ",", "'results.csv'", ")", ",", "dtype", "=", "'str'", ",", "delimiter", "=", "','", ")", "\n", "num_cases", "=", "len", "(", "tmp", ")", "-", "1", "\n", "data_for_ranking", "=", "np", ".", "zeros", "(", "(", "6", ",", "len", "(", "experiments_with_full_cv", ")", ",", "num_cases", ")", ")", "\n", "for", "i", ",", "e", "in", "enumerate", "(", "experiments_with_full_cv", ")", ":", "\n", "        ", "scores", "=", "load_csv_for_ranking", "(", "join", "(", "out", ",", "e", ",", "'results.csv'", ")", ")", "\n", "for", "metric", "in", "range", "(", "6", ")", ":", "\n", "            ", "data_for_ranking", "[", "metric", ",", "i", "]", "=", "scores", "[", ":", ",", "metric", "]", "\n", "\n", "", "", "final_ranks", ",", "average_rank", ",", "ranks", "=", "rank_algorithms", "(", "data_for_ranking", ")", "\n", "\n", "for", "t", "in", "np", ".", "argsort", "(", "final_ranks", ")", ":", "\n", "        ", "print", "(", "final_ranks", "[", "t", "]", ",", "average_rank", "[", "t", "]", ",", "experiments_with_full_cv", "[", "t", "]", ")", "\n", "\n", "# for each model, create output directories with different thresholds. evaluate ALL OF THEM (might take a while lol)", "\n", "", "thresholds", "=", "np", ".", "arange", "(", "25", ",", "751", ",", "25", ")", "\n", "output_pp_tmp", "=", "join", "(", "output_base_here", ",", "'cv_determine_pp_thresholds'", ")", "\n", "for", "e", "in", "experiments_with_full_cv", ":", "\n", "        ", "input_folder", "=", "join", "(", "out", ",", "e", ")", "\n", "for", "t", "in", "thresholds", ":", "\n", "            ", "output_directory", "=", "join", "(", "output_pp_tmp", ",", "e", ",", "str", "(", "t", ")", ")", "\n", "maybe_mkdir_p", "(", "output_directory", ")", "\n", "if", "not", "isfile", "(", "join", "(", "output_directory", ",", "'results.csv'", ")", ")", ":", "\n", "                ", "apply_threshold_to_folder", "(", "input_folder", ",", "output_directory", ",", "t", ",", "replace_with", ",", "processes", "=", "16", ")", "\n", "evaluate_BraTS_folder", "(", "output_directory", ",", "gt_dir", ",", "num_processes", ")", "\n", "\n", "# load ALL the results!", "\n", "", "", "", "results", "=", "[", "]", "\n", "experiment_names", "=", "[", "]", "\n", "for", "e", "in", "experiments_with_full_cv", ":", "\n", "        ", "for", "t", "in", "thresholds", ":", "\n", "            ", "output_directory", "=", "join", "(", "output_pp_tmp", ",", "e", ",", "str", "(", "t", ")", ")", "\n", "expected_file", "=", "join", "(", "output_directory", ",", "'results.csv'", ")", "\n", "if", "not", "isfile", "(", "expected_file", ")", ":", "\n", "                ", "print", "(", "e", ",", "'does not have a results file for threshold'", ",", "t", ")", "\n", "continue", "\n", "", "results", ".", "append", "(", "load_csv_for_ranking", "(", "expected_file", ")", ")", "\n", "experiment_names", ".", "append", "(", "\"%s___%d\"", "%", "(", "e", ",", "t", ")", ")", "\n", "", "", "all_results", "=", "np", ".", "concatenate", "(", "[", "i", "[", "None", "]", "for", "i", "in", "results", "]", ",", "0", ")", ".", "transpose", "(", "(", "2", ",", "0", ",", "1", ")", ")", "\n", "\n", "# concatenate with non postprocessed models", "\n", "all_results", "=", "np", ".", "concatenate", "(", "(", "data_for_ranking", ",", "all_results", ")", ",", "1", ")", "\n", "experiment_names", "+=", "experiments_with_full_cv", "\n", "\n", "final_ranks", ",", "average_rank", ",", "ranks", "=", "rank_algorithms", "(", "all_results", ")", "\n", "\n", "for", "t", "in", "np", ".", "argsort", "(", "final_ranks", ")", ":", "\n", "        ", "print", "(", "final_ranks", "[", "t", "]", ",", "average_rank", "[", "t", "]", ",", "experiment_names", "[", "t", "]", ")", "\n", "\n", "# for each model, print the non postprocessed model as well as the best postprocessed model. If there are", "\n", "# validation set predictions, apply the best threshold to the validation set", "\n", "", "pred_val_base", "=", "join", "(", "base", ",", "'predVal_PP_rank'", ")", "\n", "has_val_pred", "=", "[", "]", "\n", "for", "e", "in", "experiments_with_full_cv", ":", "\n", "        ", "rank_nonpp", "=", "final_ranks", "[", "experiment_names", ".", "index", "(", "e", ")", "]", "\n", "avg_rank_nonpp", "=", "average_rank", "[", "experiment_names", ".", "index", "(", "e", ")", "]", "\n", "print", "(", "e", ",", "avg_rank_nonpp", ",", "rank_nonpp", ")", "\n", "predicted_val", "=", "join", "(", "base", ",", "'predVal'", ",", "e", ")", "\n", "\n", "pp_models", "=", "[", "j", "for", "j", ",", "i", "in", "enumerate", "(", "experiment_names", ")", "if", "i", ".", "split", "(", "\"___\"", ")", "[", "0", "]", "==", "e", "and", "i", "!=", "e", "]", "\n", "if", "len", "(", "pp_models", ")", ">", "0", ":", "\n", "            ", "ranks", "=", "[", "final_ranks", "[", "i", "]", "for", "i", "in", "pp_models", "]", "\n", "best_idx", "=", "np", ".", "argmin", "(", "ranks", ")", "\n", "best", "=", "experiment_names", "[", "pp_models", "[", "best_idx", "]", "]", "\n", "best_avg_rank", "=", "average_rank", "[", "pp_models", "[", "best_idx", "]", "]", "\n", "print", "(", "best", ",", "best_avg_rank", ",", "min", "(", "ranks", ")", ")", "\n", "print", "(", "''", ")", "\n", "# apply threshold to validation set", "\n", "best_threshold", "=", "int", "(", "best", ".", "split", "(", "'___'", ")", "[", "-", "1", "]", ")", "\n", "if", "not", "isdir", "(", "predicted_val", ")", ":", "\n", "                ", "print", "(", "e", ",", "'has not valset predictions'", ")", "\n", "", "else", ":", "\n", "                ", "files", "=", "subfiles", "(", "predicted_val", ",", "suffix", "=", "'.nii.gz'", ")", "\n", "if", "len", "(", "files", ")", "!=", "expected_num_cases_val", ":", "\n", "                    ", "print", "(", "e", ",", "'has missing val cases. found: %d expected: %d'", "%", "(", "len", "(", "files", ")", ",", "expected_num_cases_val", ")", ")", "\n", "", "else", ":", "\n", "                    ", "apply_threshold_to_folder", "(", "predicted_val", ",", "join", "(", "pred_val_base", ",", "e", ")", ",", "best_threshold", ",", "replace_with", ",", "num_processes", ")", "\n", "has_val_pred", ".", "append", "(", "e", ")", "\n", "", "", "", "else", ":", "\n", "            ", "print", "(", "e", ",", "'not found in ranking'", ")", "\n", "\n", "# apply nnUNetTrainerV2BraTSRegions_DA3_BN__nnUNetPlansv2.1_bs5 to nnUNetTrainerV2BraTSRegions_DA3_BN__nnUNetPlansv2.1_bs5_15fold", "\n", "", "", "e", "=", "'nnUNetTrainerV2BraTSRegions_DA3_BN__nnUNetPlansv2.1_bs5'", "\n", "pp_models", "=", "[", "j", "for", "j", ",", "i", "in", "enumerate", "(", "experiment_names", ")", "if", "i", ".", "split", "(", "\"___\"", ")", "[", "0", "]", "==", "e", "and", "i", "!=", "e", "]", "\n", "ranks", "=", "[", "final_ranks", "[", "i", "]", "for", "i", "in", "pp_models", "]", "\n", "best_idx", "=", "np", ".", "argmin", "(", "ranks", ")", "\n", "best", "=", "experiment_names", "[", "pp_models", "[", "best_idx", "]", "]", "\n", "best_avg_rank", "=", "average_rank", "[", "pp_models", "[", "best_idx", "]", "]", "\n", "best_threshold", "=", "int", "(", "best", ".", "split", "(", "'___'", ")", "[", "-", "1", "]", ")", "\n", "predicted_val", "=", "join", "(", "base", ",", "'predVal'", ",", "'nnUNetTrainerV2BraTSRegions_DA3_BN__nnUNetPlansv2.1_bs5_15fold'", ")", "\n", "apply_threshold_to_folder", "(", "predicted_val", ",", "join", "(", "pred_val_base", ",", "'nnUNetTrainerV2BraTSRegions_DA3_BN__nnUNetPlansv2.1_bs5_15fold'", ")", ",", "best_threshold", ",", "replace_with", ",", "num_processes", ")", "\n", "has_val_pred", ".", "append", "(", "'nnUNetTrainerV2BraTSRegions_DA3_BN__nnUNetPlansv2.1_bs5_15fold'", ")", "\n", "\n", "# apply nnUNetTrainerV2BraTSRegions_DA4_BN__nnUNetPlansv2.1_bs5 to nnUNetTrainerV2BraTSRegions_DA4_BN__nnUNetPlansv2.1_bs5_15fold", "\n", "e", "=", "'nnUNetTrainerV2BraTSRegions_DA4_BN__nnUNetPlansv2.1_bs5'", "\n", "pp_models", "=", "[", "j", "for", "j", ",", "i", "in", "enumerate", "(", "experiment_names", ")", "if", "i", ".", "split", "(", "\"___\"", ")", "[", "0", "]", "==", "e", "and", "i", "!=", "e", "]", "\n", "ranks", "=", "[", "final_ranks", "[", "i", "]", "for", "i", "in", "pp_models", "]", "\n", "best_idx", "=", "np", ".", "argmin", "(", "ranks", ")", "\n", "best", "=", "experiment_names", "[", "pp_models", "[", "best_idx", "]", "]", "\n", "best_avg_rank", "=", "average_rank", "[", "pp_models", "[", "best_idx", "]", "]", "\n", "best_threshold", "=", "int", "(", "best", ".", "split", "(", "'___'", ")", "[", "-", "1", "]", ")", "\n", "predicted_val", "=", "join", "(", "base", ",", "'predVal'", ",", "'nnUNetTrainerV2BraTSRegions_DA4_BN__nnUNetPlansv2.1_bs5_15fold'", ")", "\n", "apply_threshold_to_folder", "(", "predicted_val", ",", "join", "(", "pred_val_base", ",", "'nnUNetTrainerV2BraTSRegions_DA4_BN__nnUNetPlansv2.1_bs5_15fold'", ")", ",", "best_threshold", ",", "replace_with", ",", "num_processes", ")", "\n", "has_val_pred", ".", "append", "(", "'nnUNetTrainerV2BraTSRegions_DA4_BN__nnUNetPlansv2.1_bs5_15fold'", ")", "\n", "\n", "# convert valsets", "\n", "output_converted", "=", "join", "(", "base", ",", "'converted_valSet'", ")", "\n", "for", "e", "in", "has_val_pred", ":", "\n", "        ", "expected_source_folder", "=", "join", "(", "base", ",", "'predVal_PP_rank'", ",", "e", ")", "\n", "if", "not", "isdir", "(", "expected_source_folder", ")", ":", "\n", "            ", "print", "(", "e", ",", "'has no predVal_PP_rank'", ")", "\n", "raise", "RuntimeError", "(", ")", "\n", "", "files", "=", "subfiles", "(", "expected_source_folder", ",", "suffix", "=", "'.nii.gz'", ",", "join", "=", "False", ")", "\n", "if", "len", "(", "files", ")", "!=", "expected_num_cases_val", ":", "\n", "            ", "print", "(", "e", ",", "'prediction not done, found %d files, expected %s'", "%", "(", "len", "(", "files", ")", ",", "expected_num_cases_val", ")", ")", "\n", "continue", "\n", "", "target_folder", "=", "join", "(", "output_converted", ",", "'predVal_PP_rank'", ",", "e", ")", "\n", "maybe_mkdir_p", "(", "target_folder", ")", "\n", "convert_labels_back_to_BraTS_2018_2019_convention", "(", "expected_source_folder", ",", "target_folder", ")", "\n", "\n", "# now load all the csvs for the validation set (obtained from evaluation platform) and rank our models on the", "\n", "# validation set", "\n", "", "flds", "=", "subdirs", "(", "output_converted", ",", "join", "=", "False", ")", "\n", "results_valset", "=", "[", "]", "\n", "names_valset", "=", "[", "]", "\n", "for", "f", "in", "flds", ":", "\n", "        ", "curr", "=", "join", "(", "output_converted", ",", "f", ")", "\n", "experiments", "=", "subdirs", "(", "curr", ",", "join", "=", "False", ")", "\n", "for", "e", "in", "experiments", ":", "\n", "            ", "currr", "=", "join", "(", "curr", ",", "e", ")", "\n", "expected_file", "=", "join", "(", "currr", ",", "'Stats_Validation_final.csv'", ")", "\n", "if", "not", "isfile", "(", "expected_file", ")", ":", "\n", "                ", "print", "(", "f", ",", "e", ",", "\"has not been evaluated yet!\"", ")", "\n", "", "else", ":", "\n", "                ", "res", "=", "load_csv_for_ranking", "(", "expected_file", ")", "[", ":", "-", "5", "]", "\n", "assert", "res", ".", "shape", "[", "0", "]", "==", "expected_num_cases_val", "\n", "results_valset", ".", "append", "(", "res", "[", "None", "]", ")", "\n", "names_valset", ".", "append", "(", "\"%s___%s\"", "%", "(", "f", ",", "e", ")", ")", "\n", "", "", "", "results_valset", "=", "np", ".", "concatenate", "(", "results_valset", ",", "0", ")", "# experiments x cases x metrics", "\n", "# convert to metrics x experiments x cases", "\n", "results_valset", "=", "results_valset", ".", "transpose", "(", "(", "2", ",", "0", ",", "1", ")", ")", "\n", "final_ranks", ",", "average_rank", ",", "ranks", "=", "rank_algorithms", "(", "results_valset", ")", "\n", "for", "t", "in", "np", ".", "argsort", "(", "final_ranks", ")", ":", "\n", "        ", "print", "(", "final_ranks", "[", "t", "]", ",", "average_rank", "[", "t", "]", ",", "names_valset", "[", "t", "]", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.dataset_conversion.Task075_Fluo_C3DH_A549_ManAndSim.load_tiff_convert_to_nifti": [[24, 36], ["skimage.io.imread", "SimpleITK.GetImageFromArray", "sitk.GetImageFromArray.SetSpacing", "SimpleITK.WriteImage", "skimage.io.imread.astype", "join", "skimage.io.imread", "SimpleITK.GetImageFromArray", "sitk.GetImageFromArray.SetSpacing", "SimpleITK.WriteImage", "numpy.array", "skimage.io.imread.astype", "numpy.array"], "function", ["None"], ["def", "load_tiff_convert_to_nifti", "(", "img_file", ",", "lab_file", ",", "img_out_base", ",", "anno_out", ",", "spacing", ")", ":", "\n", "    ", "img", "=", "imread", "(", "img_file", ")", "\n", "img_itk", "=", "sitk", ".", "GetImageFromArray", "(", "img", ".", "astype", "(", "np", ".", "float32", ")", ")", "\n", "img_itk", ".", "SetSpacing", "(", "np", ".", "array", "(", "spacing", ")", "[", ":", ":", "-", "1", "]", ")", "\n", "sitk", ".", "WriteImage", "(", "img_itk", ",", "join", "(", "img_out_base", "+", "\"_0000.nii.gz\"", ")", ")", "\n", "\n", "if", "lab_file", "is", "not", "None", ":", "\n", "        ", "l", "=", "imread", "(", "lab_file", ")", "\n", "l", "[", "l", ">", "0", "]", "=", "1", "\n", "l_itk", "=", "sitk", ".", "GetImageFromArray", "(", "l", ".", "astype", "(", "np", ".", "uint8", ")", ")", "\n", "l_itk", ".", "SetSpacing", "(", "np", ".", "array", "(", "spacing", ")", "[", ":", ":", "-", "1", "]", ")", "\n", "sitk", ".", "WriteImage", "(", "l_itk", ",", "anno_out", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.dataset_conversion.Task075_Fluo_C3DH_A549_ManAndSim.prepare_task": [[38, 107], ["multiprocessing.Pool", "join", "join", "join", "join", "maybe_mkdir_p", "maybe_mkdir_p", "maybe_mkdir_p", "len", "len", "save_json", "multiprocessing.Pool.close", "multiprocessing.Pool.join", "subfiles", "subfiles", "i.get", "os.path.join", "subfolders", "join", "join", "join", "join", "join", "res.append", "train_patient_names.append", "subfolders", "join", "join", "join", "res.append", "test_patient_names.append", "i.endswith", "isfile", "multiprocessing.Pool.starmap_async", "i.endswith", "multiprocessing.Pool.starmap_async"], "function", ["None"], ["", "", "def", "prepare_task", "(", "base", ",", "task_id", ",", "task_name", ",", "spacing", ")", ":", "\n", "    ", "p", "=", "Pool", "(", "16", ")", "\n", "\n", "foldername", "=", "\"Task%03.0d_%s\"", "%", "(", "task_id", ",", "task_name", ")", "\n", "\n", "out_base", "=", "join", "(", "nnUNet_raw_data", ",", "foldername", ")", "\n", "imagestr", "=", "join", "(", "out_base", ",", "\"imagesTr\"", ")", "\n", "imagests", "=", "join", "(", "out_base", ",", "\"imagesTs\"", ")", "\n", "labelstr", "=", "join", "(", "out_base", ",", "\"labelsTr\"", ")", "\n", "maybe_mkdir_p", "(", "imagestr", ")", "\n", "maybe_mkdir_p", "(", "imagests", ")", "\n", "maybe_mkdir_p", "(", "labelstr", ")", "\n", "\n", "train_patient_names", "=", "[", "]", "\n", "test_patient_names", "=", "[", "]", "\n", "res", "=", "[", "]", "\n", "\n", "for", "train_sequence", "in", "[", "i", "for", "i", "in", "subfolders", "(", "base", "+", "\"_train\"", ",", "join", "=", "False", ")", "if", "not", "i", ".", "endswith", "(", "\"_GT\"", ")", "]", ":", "\n", "        ", "train_cases", "=", "subfiles", "(", "join", "(", "base", "+", "'_train'", ",", "train_sequence", ")", ",", "suffix", "=", "\".tif\"", ",", "join", "=", "False", ")", "\n", "for", "t", "in", "train_cases", ":", "\n", "            ", "casename", "=", "train_sequence", "+", "\"_\"", "+", "t", "[", ":", "-", "4", "]", "\n", "img_file", "=", "join", "(", "base", "+", "'_train'", ",", "train_sequence", ",", "t", ")", "\n", "lab_file", "=", "join", "(", "base", "+", "'_train'", ",", "train_sequence", "+", "\"_GT\"", ",", "\"SEG\"", ",", "\"man_seg\"", "+", "t", "[", "1", ":", "]", ")", "\n", "if", "not", "isfile", "(", "lab_file", ")", ":", "\n", "                ", "continue", "\n", "", "img_out_base", "=", "join", "(", "imagestr", ",", "casename", ")", "\n", "anno_out", "=", "join", "(", "labelstr", ",", "casename", "+", "\".nii.gz\"", ")", "\n", "res", ".", "append", "(", "\n", "p", ".", "starmap_async", "(", "load_tiff_convert_to_nifti", ",", "(", "(", "img_file", ",", "lab_file", ",", "img_out_base", ",", "anno_out", ",", "spacing", ")", ",", ")", ")", ")", "\n", "train_patient_names", ".", "append", "(", "casename", ")", "\n", "\n", "", "", "for", "test_sequence", "in", "[", "i", "for", "i", "in", "subfolders", "(", "base", "+", "\"_test\"", ",", "join", "=", "False", ")", "if", "not", "i", ".", "endswith", "(", "\"_GT\"", ")", "]", ":", "\n", "        ", "test_cases", "=", "subfiles", "(", "join", "(", "base", "+", "'_test'", ",", "test_sequence", ")", ",", "suffix", "=", "\".tif\"", ",", "join", "=", "False", ")", "\n", "for", "t", "in", "test_cases", ":", "\n", "            ", "casename", "=", "test_sequence", "+", "\"_\"", "+", "t", "[", ":", "-", "4", "]", "\n", "img_file", "=", "join", "(", "base", "+", "'_test'", ",", "test_sequence", ",", "t", ")", "\n", "lab_file", "=", "None", "\n", "img_out_base", "=", "join", "(", "imagests", ",", "casename", ")", "\n", "anno_out", "=", "None", "\n", "res", ".", "append", "(", "\n", "p", ".", "starmap_async", "(", "load_tiff_convert_to_nifti", ",", "(", "(", "img_file", ",", "lab_file", ",", "img_out_base", ",", "anno_out", ",", "spacing", ")", ",", ")", ")", ")", "\n", "test_patient_names", ".", "append", "(", "casename", ")", "\n", "\n", "", "", "_", "=", "[", "i", ".", "get", "(", ")", "for", "i", "in", "res", "]", "\n", "\n", "json_dict", "=", "{", "}", "\n", "json_dict", "[", "'name'", "]", "=", "task_name", "\n", "json_dict", "[", "'description'", "]", "=", "\"\"", "\n", "json_dict", "[", "'tensorImageSize'", "]", "=", "\"4D\"", "\n", "json_dict", "[", "'reference'", "]", "=", "\"\"", "\n", "json_dict", "[", "'licence'", "]", "=", "\"\"", "\n", "json_dict", "[", "'release'", "]", "=", "\"0.0\"", "\n", "json_dict", "[", "'modality'", "]", "=", "{", "\n", "\"0\"", ":", "\"BF\"", ",", "\n", "}", "\n", "json_dict", "[", "'labels'", "]", "=", "{", "\n", "\"0\"", ":", "\"background\"", ",", "\n", "\"1\"", ":", "\"cell\"", ",", "\n", "}", "\n", "\n", "json_dict", "[", "'numTraining'", "]", "=", "len", "(", "train_patient_names", ")", "\n", "json_dict", "[", "'numTest'", "]", "=", "len", "(", "test_patient_names", ")", "\n", "json_dict", "[", "'training'", "]", "=", "[", "{", "'image'", ":", "\"./imagesTr/%s.nii.gz\"", "%", "i", ",", "\"label\"", ":", "\"./labelsTr/%s.nii.gz\"", "%", "i", "}", "for", "i", "in", "\n", "train_patient_names", "]", "\n", "json_dict", "[", "'test'", "]", "=", "[", "\"./imagesTs/%s.nii.gz\"", "%", "i", "for", "i", "in", "test_patient_names", "]", "\n", "\n", "save_json", "(", "json_dict", ",", "os", ".", "path", ".", "join", "(", "out_base", ",", "\"dataset.json\"", ")", ")", "\n", "p", ".", "close", "(", ")", "\n", "p", ".", "join", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.dataset_conversion.Task056_Verse_normalize_orientation.reverse_axes": [[25, 27], ["numpy.transpose", "tuple", "reversed", "range"], "function", ["None"], ["def", "reverse_axes", "(", "image", ")", ":", "\n", "    ", "return", "np", ".", "transpose", "(", "image", ",", "tuple", "(", "reversed", "(", "range", "(", "image", ".", "ndim", ")", ")", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.dataset_conversion.Task056_Verse_normalize_orientation.read_image": [[29, 38], ["SimpleITK.ReadImage", "Task056_Verse_normalize_orientation.reverse_axes", "SimpleITK.GetArrayFromImage", "sitk.ReadImage.GetSpacing", "sitk.ReadImage.GetOrigin", "sitk.ReadImage.GetDirection"], "function", ["home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.dataset_conversion.Task056_Verse_normalize_orientation.reverse_axes"], ["", "def", "read_image", "(", "imagefile", ")", ":", "\n", "    ", "image", "=", "sitk", ".", "ReadImage", "(", "imagefile", ")", "\n", "data", "=", "reverse_axes", "(", "sitk", ".", "GetArrayFromImage", "(", "image", ")", ")", "# switch from zyx to xyz", "\n", "header", "=", "{", "\n", "'spacing'", ":", "image", ".", "GetSpacing", "(", ")", ",", "\n", "'origin'", ":", "image", ".", "GetOrigin", "(", ")", ",", "\n", "'direction'", ":", "image", ".", "GetDirection", "(", ")", "\n", "}", "\n", "return", "data", ",", "header", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.dataset_conversion.Task056_Verse_normalize_orientation.save_image": [[40, 58], ["Task056_Verse_normalize_orientation.reverse_axes", "SimpleITK.GetImageFromArray", "sitk.GetImageFromArray.SetSpacing", "sitk.GetImageFromArray.SetOrigin", "SimpleITK.WriteImage", "isinstance", "sitk.GetImageFromArray.SetDirection", "sitk.GetImageFromArray.SetDirection", "header[].flatten"], "function", ["home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.dataset_conversion.Task056_Verse_normalize_orientation.reverse_axes"], ["", "def", "save_image", "(", "img", ":", "np", ".", "ndarray", ",", "header", ":", "dict", ",", "output_file", ":", "str", ")", ":", "\n", "    ", "\"\"\"\n    CAREFUL you need to restore_original_slice_orientation before saving!\n    :param img:\n    :param header:\n    :return:\n    \"\"\"", "\n", "# reverse back", "\n", "img", "=", "reverse_axes", "(", "img", ")", "# switch from zyx to xyz", "\n", "img_itk", "=", "sitk", ".", "GetImageFromArray", "(", "img", ")", "\n", "img_itk", ".", "SetSpacing", "(", "header", "[", "'spacing'", "]", ")", "\n", "img_itk", ".", "SetOrigin", "(", "header", "[", "'origin'", "]", ")", "\n", "if", "not", "isinstance", "(", "header", "[", "'direction'", "]", ",", "tuple", ")", ":", "\n", "        ", "img_itk", ".", "SetDirection", "(", "header", "[", "'direction'", "]", ".", "flatten", "(", ")", ")", "\n", "", "else", ":", "\n", "        ", "img_itk", ".", "SetDirection", "(", "header", "[", "'direction'", "]", ")", "\n", "\n", "", "sitk", ".", "WriteImage", "(", "img_itk", ",", "output_file", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.dataset_conversion.Task056_Verse_normalize_orientation.swap_flip_dimensions": [[60, 77], ["numpy.argmax", "numpy.sum", "numpy.transpose", "tuple", "numpy.eye", "abs", "tuple", "tuple", "slice", "int"], "function", ["None"], ["", "def", "swap_flip_dimensions", "(", "cosine_matrix", ",", "image", ",", "header", "=", "None", ")", ":", "\n", "# Compute swaps and flips", "\n", "    ", "swap", "=", "np", ".", "argmax", "(", "abs", "(", "cosine_matrix", ")", ",", "axis", "=", "0", ")", "\n", "flip", "=", "np", ".", "sum", "(", "cosine_matrix", ",", "axis", "=", "0", ")", "\n", "\n", "# Apply transformation to image volume", "\n", "image", "=", "np", ".", "transpose", "(", "image", ",", "tuple", "(", "swap", ")", ")", "\n", "image", "=", "image", "[", "tuple", "(", "slice", "(", "None", ",", "None", ",", "int", "(", "f", ")", ")", "for", "f", "in", "flip", ")", "]", "\n", "\n", "if", "header", "is", "None", ":", "\n", "        ", "return", "image", "\n", "\n", "# Apply transformation to header", "\n", "", "header", "[", "'spacing'", "]", "=", "tuple", "(", "header", "[", "'spacing'", "]", "[", "s", "]", "for", "s", "in", "swap", ")", "\n", "header", "[", "'direction'", "]", "=", "np", ".", "eye", "(", "3", ")", "\n", "\n", "return", "image", ",", "header", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.dataset_conversion.Task056_Verse_normalize_orientation.normalize_slice_orientation": [[79, 89], ["header.copy", "numpy.asarray().reshape", "numpy.linalg.inv", "Task056_Verse_normalize_orientation.swap_flip_dimensions", "numpy.round", "numpy.asarray"], "function", ["home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.dataset_conversion.Task056_Verse_normalize_orientation.swap_flip_dimensions"], ["", "def", "normalize_slice_orientation", "(", "image", ",", "header", ")", ":", "\n", "# Preserve original header so that we can easily transform back", "\n", "    ", "header", "[", "'original'", "]", "=", "header", ".", "copy", "(", ")", "\n", "\n", "# Compute inverse of cosine (round first because we assume 0/1 values only)", "\n", "# to determine how the image has to be transposed and flipped for cosine = identity", "\n", "cosine", "=", "np", ".", "asarray", "(", "header", "[", "'direction'", "]", ")", ".", "reshape", "(", "3", ",", "3", ")", "\n", "cosine_inv", "=", "np", ".", "linalg", ".", "inv", "(", "np", ".", "round", "(", "cosine", ")", ")", "\n", "\n", "return", "swap_flip_dimensions", "(", "cosine_inv", ",", "image", ",", "header", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.dataset_conversion.Task056_Verse_normalize_orientation.restore_original_slice_orientation": [[91, 99], ["numpy.asarray().reshape", "numpy.round", "Task056_Verse_normalize_orientation.swap_flip_dimensions", "numpy.asarray"], "function", ["home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.dataset_conversion.Task056_Verse_normalize_orientation.swap_flip_dimensions"], ["", "def", "restore_original_slice_orientation", "(", "mask", ",", "header", ")", ":", "\n", "# Use original orientation for transformation because we assume the image to be in", "\n", "# normalized orientation, i.e., identity cosine)", "\n", "    ", "cosine", "=", "np", ".", "asarray", "(", "header", "[", "'original'", "]", "[", "'direction'", "]", ")", ".", "reshape", "(", "3", ",", "3", ")", "\n", "cosine_rnd", "=", "np", ".", "round", "(", "cosine", ")", "\n", "\n", "# Apply transformations to both the image and the mask", "\n", "return", "swap_flip_dimensions", "(", "cosine_rnd", ",", "mask", ")", ",", "header", "[", "'original'", "]", "\n", "", ""]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.dataset_conversion.Task061_CREMI.load_sample": [[30, 42], ["h5py.File", "numpy.array", "f[].keys", "numpy.array"], "function", ["None"], ["", "def", "load_sample", "(", "filename", ")", ":", "\n", "# we need raw data and seg", "\n", "    ", "f", "=", "h5py", ".", "File", "(", "filename", ",", "'r'", ")", "\n", "data", "=", "np", ".", "array", "(", "f", "[", "'volumes'", "]", "[", "'raw'", "]", ")", "\n", "\n", "if", "'labels'", "in", "f", "[", "'volumes'", "]", ".", "keys", "(", ")", ":", "\n", "        ", "labels", "=", "np", ".", "array", "(", "f", "[", "'volumes'", "]", "[", "'labels'", "]", "[", "'clefts'", "]", ")", "\n", "# clefts are low values, background is high", "\n", "labels", "=", "(", "labels", "<", "100000", ")", ".", "astype", "(", "np", ".", "uint8", ")", "\n", "", "else", ":", "\n", "        ", "labels", "=", "None", "\n", "", "return", "data", ",", "labels", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.dataset_conversion.Task061_CREMI.save_as_nifti": [[44, 48], ["SimpleITK.GetImageFromArray", "sitk.GetImageFromArray.SetSpacing", "SimpleITK.WriteImage"], "function", ["None"], ["", "def", "save_as_nifti", "(", "arr", ",", "filename", ",", "spacing", ")", ":", "\n", "    ", "itk_img", "=", "sitk", ".", "GetImageFromArray", "(", "arr", ")", "\n", "itk_img", ".", "SetSpacing", "(", "spacing", ")", "\n", "sitk", ".", "WriteImage", "(", "itk_img", ",", "filename", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.dataset_conversion.Task061_CREMI.prepare_submission": [[50, 76], ["SimpleITK.GetArrayFromImage().astype", "CremiFile", "Volume", "CremiFile.write_clefts", "CremiFile.close", "SimpleITK.GetArrayFromImage().astype", "CremiFile", "Volume", "CremiFile.write_clefts", "CremiFile.close", "SimpleITK.GetArrayFromImage().astype", "CremiFile", "Volume", "CremiFile.write_clefts", "CremiFile.close", "join", "join", "join", "SimpleITK.GetArrayFromImage", "SimpleITK.GetArrayFromImage", "SimpleITK.GetArrayFromImage", "SimpleITK.ReadImage", "SimpleITK.ReadImage", "SimpleITK.ReadImage", "join", "join", "join"], "function", ["None"], ["", "def", "prepare_submission", "(", ")", ":", "\n", "    ", "from", "cremi", ".", "io", "import", "CremiFile", "\n", "from", "cremi", ".", "Volume", "import", "Volume", "\n", "\n", "base", "=", "\"/home/fabian/drives/datasets/results/nnUNet/test_sets/Task061_CREMI/\"", "\n", "# a+", "\n", "pred", "=", "sitk", ".", "GetArrayFromImage", "(", "sitk", ".", "ReadImage", "(", "join", "(", "base", ",", "'results_3d_fullres'", ",", "\"sample_a+.nii.gz\"", ")", ")", ")", ".", "astype", "(", "np", ".", "uint64", ")", "\n", "pred", "[", "pred", "==", "0", "]", "=", "0xffffffffffffffff", "\n", "out_a", "=", "CremiFile", "(", "join", "(", "base", ",", "'sample_A+_20160601.hdf'", ")", ",", "'w'", ")", "\n", "clefts", "=", "Volume", "(", "pred", ",", "(", "40.", ",", "4.", ",", "4.", ")", ")", "\n", "out_a", ".", "write_clefts", "(", "clefts", ")", "\n", "out_a", ".", "close", "(", ")", "\n", "\n", "pred", "=", "sitk", ".", "GetArrayFromImage", "(", "sitk", ".", "ReadImage", "(", "join", "(", "base", ",", "'results_3d_fullres'", ",", "\"sample_b+.nii.gz\"", ")", ")", ")", ".", "astype", "(", "np", ".", "uint64", ")", "\n", "pred", "[", "pred", "==", "0", "]", "=", "0xffffffffffffffff", "\n", "out_b", "=", "CremiFile", "(", "join", "(", "base", ",", "'sample_B+_20160601.hdf'", ")", ",", "'w'", ")", "\n", "clefts", "=", "Volume", "(", "pred", ",", "(", "40.", ",", "4.", ",", "4.", ")", ")", "\n", "out_b", ".", "write_clefts", "(", "clefts", ")", "\n", "out_b", ".", "close", "(", ")", "\n", "\n", "pred", "=", "sitk", ".", "GetArrayFromImage", "(", "sitk", ".", "ReadImage", "(", "join", "(", "base", ",", "'results_3d_fullres'", ",", "\"sample_c+.nii.gz\"", ")", ")", ")", ".", "astype", "(", "np", ".", "uint64", ")", "\n", "pred", "[", "pred", "==", "0", "]", "=", "0xffffffffffffffff", "\n", "out_c", "=", "CremiFile", "(", "join", "(", "base", ",", "'sample_C+_20160601.hdf'", ")", ",", "'w'", ")", "\n", "clefts", "=", "Volume", "(", "pred", ",", "(", "40.", ",", "4.", ",", "4.", ")", ")", "\n", "out_c", ".", "write_clefts", "(", "clefts", ")", "\n", "out_c", ".", "close", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.dataset_conversion.Task055_SegTHOR.convert_for_submission": [[23, 36], ["subfiles", "maybe_mkdir_p", "SimpleITK.ReadImage", "join", "SimpleITK.WriteImage", "join"], "function", ["None"], ["def", "convert_for_submission", "(", "source_dir", ",", "target_dir", ")", ":", "\n", "    ", "\"\"\"\n    I believe they want .nii, not .nii.gz\n    :param source_dir:\n    :param target_dir:\n    :return:\n    \"\"\"", "\n", "files", "=", "subfiles", "(", "source_dir", ",", "suffix", "=", "\".nii.gz\"", ",", "join", "=", "False", ")", "\n", "maybe_mkdir_p", "(", "target_dir", ")", "\n", "for", "f", "in", "files", ":", "\n", "        ", "img", "=", "sitk", ".", "ReadImage", "(", "join", "(", "source_dir", ",", "f", ")", ")", "\n", "out_file", "=", "join", "(", "target_dir", ",", "f", "[", ":", "-", "7", "]", "+", "\".nii\"", ")", "\n", "sitk", ".", "WriteImage", "(", "img", ",", "out_file", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.dataset_conversion.Task076_Fluo_N3DH_SIM.load_bmp_convert_to_nifti_borders": [[32, 46], ["skimage.io.imread", "SimpleITK.GetImageFromArray", "sitk.GetImageFromArray.SetSpacing", "SimpleITK.WriteImage", "skimage.io.imread.astype", "join", "skimage.io.imread", "Task076_Fluo_N3DH_SIM.generate_border_as_suggested_by_twollmann", "SimpleITK.GetImageFromArray", "sitk.GetImageFromArray.SetSpacing", "SimpleITK.WriteImage", "numpy.array", "skimage.io.imread.astype", "numpy.array"], "function", ["home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.dataset_conversion.Task076_Fluo_N3DH_SIM.generate_border_as_suggested_by_twollmann"], ["def", "load_bmp_convert_to_nifti_borders", "(", "img_file", ",", "lab_file", ",", "img_out_base", ",", "anno_out", ",", "spacing", ",", "border_thickness", "=", "0.7", ")", ":", "\n", "    ", "img", "=", "imread", "(", "img_file", ")", "\n", "img_itk", "=", "sitk", ".", "GetImageFromArray", "(", "img", ".", "astype", "(", "np", ".", "float32", ")", ")", "\n", "img_itk", ".", "SetSpacing", "(", "np", ".", "array", "(", "spacing", ")", "[", ":", ":", "-", "1", "]", ")", "\n", "sitk", ".", "WriteImage", "(", "img_itk", ",", "join", "(", "img_out_base", "+", "\"_0000.nii.gz\"", ")", ")", "\n", "\n", "if", "lab_file", "is", "not", "None", ":", "\n", "        ", "l", "=", "imread", "(", "lab_file", ")", "\n", "borders", "=", "generate_border_as_suggested_by_twollmann", "(", "l", ",", "spacing", ",", "border_thickness", ")", "\n", "l", "[", "l", ">", "0", "]", "=", "1", "\n", "l", "[", "borders", "==", "1", "]", "=", "2", "\n", "l_itk", "=", "sitk", ".", "GetImageFromArray", "(", "l", ".", "astype", "(", "np", ".", "uint8", ")", ")", "\n", "l_itk", ".", "SetSpacing", "(", "np", ".", "array", "(", "spacing", ")", "[", ":", ":", "-", "1", "]", ")", "\n", "sitk", ".", "WriteImage", "(", "l_itk", ",", "anno_out", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.dataset_conversion.Task076_Fluo_N3DH_SIM.generate_ball": [[48, 56], ["numpy.round().astype", "skimage.morphology.ball", "skimage.transform.resize", "skimage.transform.resize.astype", "numpy.round", "max", "numpy.array"], "function", ["None"], ["", "", "def", "generate_ball", "(", "spacing", ",", "radius", ",", "dtype", "=", "int", ")", ":", "\n", "    ", "radius_in_voxels", "=", "np", ".", "round", "(", "radius", "/", "np", ".", "array", "(", "spacing", ")", ")", ".", "astype", "(", "int", ")", "\n", "n", "=", "2", "*", "radius_in_voxels", "+", "1", "\n", "ball_iso", "=", "ball", "(", "max", "(", "n", ")", "*", "2", ",", "dtype", "=", "np", ".", "float64", ")", "\n", "ball_resampled", "=", "resize", "(", "ball_iso", ",", "n", ",", "1", ",", "'constant'", ",", "0", ",", "clip", "=", "True", ",", "anti_aliasing", "=", "False", ",", "preserve_range", "=", "True", ")", "\n", "ball_resampled", "[", "ball_resampled", ">", "0.5", "]", "=", "1", "\n", "ball_resampled", "[", "ball_resampled", "<=", "0.5", "]", "=", "0", "\n", "return", "ball_resampled", ".", "astype", "(", "dtype", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.dataset_conversion.Task076_Fluo_N3DH_SIM.generate_border_as_suggested_by_twollmann": [[58, 67], ["numpy.zeros_like", "Task076_Fluo_N3DH_SIM.generate_ball", "numpy.unique", "skimage.morphology.erosion"], "function", ["home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.dataset_conversion.Task076_Fluo_N3DH_SIM.generate_ball"], ["", "def", "generate_border_as_suggested_by_twollmann", "(", "label_img", ":", "np", ".", "ndarray", ",", "spacing", ",", "border_thickness", ":", "float", "=", "2", ")", "->", "np", ".", "ndarray", ":", "\n", "    ", "border", "=", "np", ".", "zeros_like", "(", "label_img", ")", "\n", "selem", "=", "generate_ball", "(", "spacing", ",", "border_thickness", ")", "\n", "for", "l", "in", "np", ".", "unique", "(", "label_img", ")", ":", "\n", "        ", "if", "l", "==", "0", ":", "continue", "\n", "mask", "=", "(", "label_img", "==", "l", ")", ".", "astype", "(", "int", ")", "\n", "eroded", "=", "erosion", "(", "mask", ",", "selem", ")", "\n", "border", "[", "(", "eroded", "==", "0", ")", "&", "(", "mask", "!=", "0", ")", "]", "=", "1", "\n", "", "return", "border", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.dataset_conversion.Task076_Fluo_N3DH_SIM.find_differences": [[69, 74], ["subfiles", "SimpleITK.GetArrayFromImage", "SimpleITK.GetArrayFromImage", "print", "SimpleITK.ReadImage", "SimpleITK.ReadImage", "numpy.sum", "join", "join"], "function", ["None"], ["", "def", "find_differences", "(", "labelstr1", ",", "labelstr2", ")", ":", "\n", "    ", "for", "n", "in", "subfiles", "(", "labelstr1", ",", "suffix", "=", "'.nii.gz'", ",", "join", "=", "False", ")", ":", "\n", "        ", "a", "=", "sitk", ".", "GetArrayFromImage", "(", "sitk", ".", "ReadImage", "(", "join", "(", "labelstr1", ",", "n", ")", ")", ")", "\n", "b", "=", "sitk", ".", "GetArrayFromImage", "(", "sitk", ".", "ReadImage", "(", "join", "(", "labelstr2", ",", "n", ")", ")", ")", "\n", "print", "(", "n", ",", "np", ".", "sum", "(", "a", "!=", "b", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.dataset_conversion.Task076_Fluo_N3DH_SIM.prepare_task": [[76, 146], ["multiprocessing.Pool", "join", "join", "join", "join", "maybe_mkdir_p", "maybe_mkdir_p", "maybe_mkdir_p", "len", "len", "save_json", "multiprocessing.Pool.close", "multiprocessing.Pool.join", "subfiles", "subfiles", "i.get", "os.path.join", "subfolders", "join", "join", "join", "join", "join", "res.append", "train_patient_names.append", "subfolders", "join", "join", "join", "res.append", "test_patient_names.append", "i.endswith", "isfile", "multiprocessing.Pool.starmap_async", "i.endswith", "multiprocessing.Pool.starmap_async"], "function", ["None"], ["", "", "def", "prepare_task", "(", "base", ",", "task_id", ",", "task_name", ",", "spacing", ",", "border_thickness", ":", "float", "=", "15", ",", "processes", ":", "int", "=", "16", ")", ":", "\n", "    ", "p", "=", "Pool", "(", "processes", ")", "\n", "\n", "foldername", "=", "\"Task%03.0d_%s\"", "%", "(", "task_id", ",", "task_name", ")", "\n", "\n", "out_base", "=", "join", "(", "nnUNet_raw_data", ",", "foldername", ")", "\n", "imagestr", "=", "join", "(", "out_base", ",", "\"imagesTr\"", ")", "\n", "imagests", "=", "join", "(", "out_base", ",", "\"imagesTs\"", ")", "\n", "labelstr", "=", "join", "(", "out_base", ",", "\"labelsTr\"", ")", "\n", "maybe_mkdir_p", "(", "imagestr", ")", "\n", "maybe_mkdir_p", "(", "imagests", ")", "\n", "maybe_mkdir_p", "(", "labelstr", ")", "\n", "\n", "train_patient_names", "=", "[", "]", "\n", "test_patient_names", "=", "[", "]", "\n", "res", "=", "[", "]", "\n", "\n", "for", "train_sequence", "in", "[", "i", "for", "i", "in", "subfolders", "(", "base", "+", "\"_train\"", ",", "join", "=", "False", ")", "if", "not", "i", ".", "endswith", "(", "\"_GT\"", ")", "]", ":", "\n", "        ", "train_cases", "=", "subfiles", "(", "join", "(", "base", "+", "'_train'", ",", "train_sequence", ")", ",", "suffix", "=", "\".tif\"", ",", "join", "=", "False", ")", "\n", "for", "t", "in", "train_cases", ":", "\n", "            ", "casename", "=", "train_sequence", "+", "\"_\"", "+", "t", "[", ":", "-", "4", "]", "\n", "img_file", "=", "join", "(", "base", "+", "'_train'", ",", "train_sequence", ",", "t", ")", "\n", "lab_file", "=", "join", "(", "base", "+", "'_train'", ",", "train_sequence", "+", "\"_GT\"", ",", "\"SEG\"", ",", "\"man_seg\"", "+", "t", "[", "1", ":", "]", ")", "\n", "if", "not", "isfile", "(", "lab_file", ")", ":", "\n", "                ", "continue", "\n", "", "img_out_base", "=", "join", "(", "imagestr", ",", "casename", ")", "\n", "anno_out", "=", "join", "(", "labelstr", ",", "casename", "+", "\".nii.gz\"", ")", "\n", "res", ".", "append", "(", "\n", "p", ".", "starmap_async", "(", "load_bmp_convert_to_nifti_borders", ",", "(", "(", "img_file", ",", "lab_file", ",", "img_out_base", ",", "anno_out", ",", "spacing", ",", "border_thickness", ")", ",", ")", ")", ")", "\n", "train_patient_names", ".", "append", "(", "casename", ")", "\n", "\n", "", "", "for", "test_sequence", "in", "[", "i", "for", "i", "in", "subfolders", "(", "base", "+", "\"_test\"", ",", "join", "=", "False", ")", "if", "not", "i", ".", "endswith", "(", "\"_GT\"", ")", "]", ":", "\n", "        ", "test_cases", "=", "subfiles", "(", "join", "(", "base", "+", "'_test'", ",", "test_sequence", ")", ",", "suffix", "=", "\".tif\"", ",", "join", "=", "False", ")", "\n", "for", "t", "in", "test_cases", ":", "\n", "            ", "casename", "=", "test_sequence", "+", "\"_\"", "+", "t", "[", ":", "-", "4", "]", "\n", "img_file", "=", "join", "(", "base", "+", "'_test'", ",", "test_sequence", ",", "t", ")", "\n", "lab_file", "=", "None", "\n", "img_out_base", "=", "join", "(", "imagests", ",", "casename", ")", "\n", "anno_out", "=", "None", "\n", "res", ".", "append", "(", "\n", "p", ".", "starmap_async", "(", "load_bmp_convert_to_nifti_borders", ",", "(", "(", "img_file", ",", "lab_file", ",", "img_out_base", ",", "anno_out", ",", "spacing", ",", "border_thickness", ")", ",", ")", ")", ")", "\n", "test_patient_names", ".", "append", "(", "casename", ")", "\n", "\n", "", "", "_", "=", "[", "i", ".", "get", "(", ")", "for", "i", "in", "res", "]", "\n", "\n", "json_dict", "=", "{", "}", "\n", "json_dict", "[", "'name'", "]", "=", "task_name", "\n", "json_dict", "[", "'description'", "]", "=", "\"\"", "\n", "json_dict", "[", "'tensorImageSize'", "]", "=", "\"4D\"", "\n", "json_dict", "[", "'reference'", "]", "=", "\"\"", "\n", "json_dict", "[", "'licence'", "]", "=", "\"\"", "\n", "json_dict", "[", "'release'", "]", "=", "\"0.0\"", "\n", "json_dict", "[", "'modality'", "]", "=", "{", "\n", "\"0\"", ":", "\"BF\"", ",", "\n", "}", "\n", "json_dict", "[", "'labels'", "]", "=", "{", "\n", "\"0\"", ":", "\"background\"", ",", "\n", "\"1\"", ":", "\"cell\"", ",", "\n", "\"2\"", ":", "\"border\"", ",", "\n", "}", "\n", "\n", "json_dict", "[", "'numTraining'", "]", "=", "len", "(", "train_patient_names", ")", "\n", "json_dict", "[", "'numTest'", "]", "=", "len", "(", "test_patient_names", ")", "\n", "json_dict", "[", "'training'", "]", "=", "[", "{", "'image'", ":", "\"./imagesTr/%s.nii.gz\"", "%", "i", ",", "\"label\"", ":", "\"./labelsTr/%s.nii.gz\"", "%", "i", "}", "for", "i", "in", "\n", "train_patient_names", "]", "\n", "json_dict", "[", "'test'", "]", "=", "[", "\"./imagesTs/%s.nii.gz\"", "%", "i", "for", "i", "in", "test_patient_names", "]", "\n", "\n", "save_json", "(", "json_dict", ",", "os", ".", "path", ".", "join", "(", "out_base", ",", "\"dataset.json\"", ")", ")", "\n", "p", ".", "close", "(", ")", "\n", "p", ".", "join", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.dataset_conversion.Task076_Fluo_N3DH_SIM.plot_images": [[148, 155], ["maybe_mkdir_p", "subfiles", "SimpleITK.GetArrayFromImage", "plt.imsave", "SimpleITK.ReadImage", "join", "join"], "function", ["None"], ["", "def", "plot_images", "(", "folder", ",", "output_folder", ")", ":", "\n", "    ", "maybe_mkdir_p", "(", "output_folder", ")", "\n", "import", "matplotlib", ".", "pyplot", "as", "plt", "\n", "for", "i", "in", "subfiles", "(", "folder", ",", "suffix", "=", "'.nii.gz'", ",", "join", "=", "False", ")", ":", "\n", "        ", "img", "=", "sitk", ".", "GetArrayFromImage", "(", "sitk", ".", "ReadImage", "(", "join", "(", "folder", ",", "i", ")", ")", ")", "\n", "center_slice", "=", "img", "[", "img", ".", "shape", "[", "0", "]", "//", "2", "]", "\n", "plt", ".", "imsave", "(", "join", "(", "output_folder", ",", "i", "[", ":", "-", "7", "]", "+", "'.png'", ")", ",", "center_slice", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.dataset_conversion.Task076_Fluo_N3DH_SIM.convert_to_tiff": [[157, 160], ["SimpleITK.GetArrayFromImage", "skimage.io.imsave", "SimpleITK.ReadImage", "sitk.GetArrayFromImage.astype"], "function", ["None"], ["", "", "def", "convert_to_tiff", "(", "nifti_image", ":", "str", ",", "output_name", ":", "str", ")", ":", "\n", "    ", "npy", "=", "sitk", ".", "GetArrayFromImage", "(", "sitk", ".", "ReadImage", "(", "nifti_image", ")", ")", "\n", "imsave", "(", "output_name", ",", "npy", ".", "astype", "(", "np", ".", "uint16", ")", ",", "compress", "=", "6", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.dataset_conversion.Task076_Fluo_N3DH_SIM.convert_to_instance_seg": [[162, 195], ["label", "numpy.copy", "numpy.copy", "numpy.array", "numpy.array", "np.copy.astype", "numpy.sum", "max", "range", "skimage.morphology.ball", "dilation", "min", "range"], "function", ["None"], ["", "def", "convert_to_instance_seg", "(", "arr", ":", "np", ".", "ndarray", ",", "spacing", ":", "tuple", "=", "(", "0.2", ",", "0.125", ",", "0.125", ")", ")", ":", "\n", "    ", "from", "skimage", ".", "morphology", "import", "label", ",", "dilation", "\n", "# 1 is core, 2 is border", "\n", "objects", "=", "label", "(", "(", "arr", "==", "1", ")", ".", "astype", "(", "int", ")", ")", "\n", "final", "=", "np", ".", "copy", "(", "objects", ")", "\n", "remaining_border", "=", "arr", "==", "2", "\n", "current", "=", "np", ".", "copy", "(", "objects", ")", "\n", "dilated_mm", "=", "np", ".", "array", "(", "(", "0", ",", "0", ",", "0", ")", ")", "\n", "spacing", "=", "np", ".", "array", "(", "spacing", ")", "\n", "\n", "while", "np", ".", "sum", "(", "remaining_border", ")", ">", "0", ":", "\n", "        ", "strel_size", "=", "[", "0", ",", "0", ",", "0", "]", "\n", "maximum_dilation", "=", "max", "(", "dilated_mm", ")", "\n", "for", "i", "in", "range", "(", "3", ")", ":", "\n", "            ", "if", "spacing", "[", "i", "]", "==", "min", "(", "spacing", ")", ":", "\n", "                ", "strel_size", "[", "i", "]", "=", "1", "\n", "continue", "\n", "", "if", "dilated_mm", "[", "i", "]", "+", "spacing", "[", "i", "]", "/", "2", "<", "maximum_dilation", ":", "\n", "                ", "strel_size", "[", "i", "]", "=", "1", "\n", "", "", "ball_here", "=", "ball", "(", "1", ")", "\n", "\n", "if", "strel_size", "[", "0", "]", "==", "0", ":", "ball_here", "=", "ball_here", "[", "1", ":", "2", "]", "\n", "if", "strel_size", "[", "1", "]", "==", "0", ":", "ball_here", "=", "ball_here", "[", ":", ",", "1", ":", "2", "]", "\n", "if", "strel_size", "[", "2", "]", "==", "0", ":", "ball_here", "=", "ball_here", "[", ":", ",", ":", ",", "1", ":", "2", "]", "\n", "\n", "#print(1)", "\n", "dilated", "=", "dilation", "(", "current", ",", "ball_here", ")", "\n", "diff", "=", "(", "current", "==", "0", ")", "&", "(", "dilated", "!=", "current", ")", "\n", "final", "[", "diff", "&", "remaining_border", "]", "=", "dilated", "[", "diff", "&", "remaining_border", "]", "\n", "remaining_border", "[", "diff", "]", "=", "0", "\n", "current", "=", "dilated", "\n", "dilated_mm", "=", "[", "dilated_mm", "[", "i", "]", "+", "spacing", "[", "i", "]", "if", "strel_size", "[", "i", "]", "==", "1", "else", "dilated_mm", "[", "i", "]", "for", "i", "in", "range", "(", "3", ")", "]", "\n", "", "return", "final", ".", "astype", "(", "np", ".", "uint32", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.dataset_conversion.Task076_Fluo_N3DH_SIM.convert_to_instance_seg2": [[197, 254], ["label", "numpy.unique", "label", "numpy.copy", "numpy.copy", "numpy.array", "numpy.array", "numpy.max", "label", "numpy.unique", "np.copy.astype", "numpy.sum", "max", "range", "skimage.morphology.ball", "dilation", "numpy.sum", "numpy.sum", "min", "range", "numpy.unique"], "function", ["None"], ["", "def", "convert_to_instance_seg2", "(", "arr", ":", "np", ".", "ndarray", ",", "spacing", ":", "tuple", "=", "(", "0.2", ",", "0.125", ",", "0.125", ")", ",", "small_center_threshold", "=", "30", ",", "\n", "isolated_border_as_separate_instance_threshold", ":", "int", "=", "15", ")", ":", "\n", "    ", "from", "skimage", ".", "morphology", "import", "label", ",", "dilation", "\n", "# we first identify centers that are too small and set them to be border. This should remove false positive instances", "\n", "objects", "=", "label", "(", "(", "arr", "==", "1", ")", ".", "astype", "(", "int", ")", ")", "\n", "for", "o", "in", "np", ".", "unique", "(", "objects", ")", ":", "\n", "        ", "if", "o", ">", "0", "and", "np", ".", "sum", "(", "objects", "==", "o", ")", "<=", "small_center_threshold", ":", "\n", "            ", "arr", "[", "objects", "==", "o", "]", "=", "2", "\n", "\n", "# 1 is core, 2 is border", "\n", "", "", "objects", "=", "label", "(", "(", "arr", "==", "1", ")", ".", "astype", "(", "int", ")", ")", "\n", "final", "=", "np", ".", "copy", "(", "objects", ")", "\n", "remaining_border", "=", "arr", "==", "2", "\n", "current", "=", "np", ".", "copy", "(", "objects", ")", "\n", "dilated_mm", "=", "np", ".", "array", "(", "(", "0", ",", "0", ",", "0", ")", ")", "\n", "spacing", "=", "np", ".", "array", "(", "spacing", ")", "\n", "\n", "while", "np", ".", "sum", "(", "remaining_border", ")", ">", "0", ":", "\n", "        ", "strel_size", "=", "[", "0", ",", "0", ",", "0", "]", "\n", "maximum_dilation", "=", "max", "(", "dilated_mm", ")", "\n", "for", "i", "in", "range", "(", "3", ")", ":", "\n", "            ", "if", "spacing", "[", "i", "]", "==", "min", "(", "spacing", ")", ":", "\n", "                ", "strel_size", "[", "i", "]", "=", "1", "\n", "continue", "\n", "", "if", "dilated_mm", "[", "i", "]", "+", "spacing", "[", "i", "]", "/", "2", "<", "maximum_dilation", ":", "\n", "                ", "strel_size", "[", "i", "]", "=", "1", "\n", "", "", "ball_here", "=", "ball", "(", "1", ")", "\n", "\n", "if", "strel_size", "[", "0", "]", "==", "0", ":", "ball_here", "=", "ball_here", "[", "1", ":", "2", "]", "\n", "if", "strel_size", "[", "1", "]", "==", "0", ":", "ball_here", "=", "ball_here", "[", ":", ",", "1", ":", "2", "]", "\n", "if", "strel_size", "[", "2", "]", "==", "0", ":", "ball_here", "=", "ball_here", "[", ":", ",", ":", ",", "1", ":", "2", "]", "\n", "\n", "#print(1)", "\n", "dilated", "=", "dilation", "(", "current", ",", "ball_here", ")", "\n", "diff", "=", "(", "current", "==", "0", ")", "&", "(", "dilated", "!=", "current", ")", "\n", "final", "[", "diff", "&", "remaining_border", "]", "=", "dilated", "[", "diff", "&", "remaining_border", "]", "\n", "remaining_border", "[", "diff", "]", "=", "0", "\n", "current", "=", "dilated", "\n", "dilated_mm", "=", "[", "dilated_mm", "[", "i", "]", "+", "spacing", "[", "i", "]", "if", "strel_size", "[", "i", "]", "==", "1", "else", "dilated_mm", "[", "i", "]", "for", "i", "in", "range", "(", "3", ")", "]", "\n", "\n", "# what can happen is that a cell is so small that the network only predicted border and no core. This cell will be", "\n", "# fused with the nearest other instance, which we don't want. Therefore we identify isolated border predictions and", "\n", "# give them a separate instance id", "\n", "# we identify isolated border predictions by checking each foreground object in arr and see whether this object", "\n", "# also contains label 1", "\n", "", "max_label", "=", "np", ".", "max", "(", "final", ")", "\n", "\n", "foreground_objects", "=", "label", "(", "(", "arr", "!=", "0", ")", ".", "astype", "(", "int", ")", ")", "\n", "for", "i", "in", "np", ".", "unique", "(", "foreground_objects", ")", ":", "\n", "        ", "if", "i", ">", "0", "and", "(", "1", "not", "in", "np", ".", "unique", "(", "arr", "[", "foreground_objects", "==", "i", "]", ")", ")", ":", "\n", "            ", "size_of_object", "=", "np", ".", "sum", "(", "foreground_objects", "==", "i", ")", "\n", "if", "size_of_object", ">=", "isolated_border_as_separate_instance_threshold", ":", "\n", "                ", "final", "[", "foreground_objects", "==", "i", "]", "=", "max_label", "+", "1", "\n", "max_label", "+=", "1", "\n", "#print('yeah boi')", "\n", "\n", "", "", "", "return", "final", ".", "astype", "(", "np", ".", "uint32", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.dataset_conversion.Task076_Fluo_N3DH_SIM.load_instanceseg_save": [[256, 265], ["SimpleITK.ReadImage", "SimpleITK.GetImageFromArray", "sitk.GetImageFromArray.CopyInformation", "SimpleITK.WriteImage", "Task076_Fluo_N3DH_SIM.convert_to_instance_seg", "Task076_Fluo_N3DH_SIM.convert_to_instance_seg2", "SimpleITK.GetArrayFromImage", "SimpleITK.GetArrayFromImage"], "function", ["home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.dataset_conversion.Task089_Fluo-N2DH-SIM.convert_to_instance_seg", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.dataset_conversion.Task076_Fluo_N3DH_SIM.convert_to_instance_seg2"], ["", "def", "load_instanceseg_save", "(", "in_file", ":", "str", ",", "out_file", ":", "str", ",", "better", ":", "bool", ")", ":", "\n", "    ", "itk_img", "=", "sitk", ".", "ReadImage", "(", "in_file", ")", "\n", "if", "not", "better", ":", "\n", "        ", "instanceseg", "=", "convert_to_instance_seg", "(", "sitk", ".", "GetArrayFromImage", "(", "itk_img", ")", ")", "\n", "", "else", ":", "\n", "        ", "instanceseg", "=", "convert_to_instance_seg2", "(", "sitk", ".", "GetArrayFromImage", "(", "itk_img", ")", ")", "\n", "", "itk_out", "=", "sitk", ".", "GetImageFromArray", "(", "instanceseg", ")", "\n", "itk_out", ".", "CopyInformation", "(", "itk_img", ")", "\n", "sitk", ".", "WriteImage", "(", "itk_out", ",", "out_file", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.dataset_conversion.Task076_Fluo_N3DH_SIM.convert_all_to_instance": [[267, 278], ["maybe_mkdir_p", "multiprocessing.Pool", "subfiles", "multiprocessing.Pool.starmap_async", "p.starmap_async.get", "multiprocessing.Pool.close", "multiprocessing.Pool.join", "join", "join", "len", "zip"], "function", ["None"], ["", "def", "convert_all_to_instance", "(", "input_folder", ":", "str", ",", "output_folder", ":", "str", ",", "processes", ":", "int", "=", "24", ",", "better", ":", "bool", "=", "False", ")", ":", "\n", "    ", "maybe_mkdir_p", "(", "output_folder", ")", "\n", "p", "=", "Pool", "(", "processes", ")", "\n", "files", "=", "subfiles", "(", "input_folder", ",", "suffix", "=", "'.nii.gz'", ",", "join", "=", "False", ")", "\n", "output_files", "=", "[", "join", "(", "output_folder", ",", "i", ")", "for", "i", "in", "files", "]", "\n", "input_files", "=", "[", "join", "(", "input_folder", ",", "i", ")", "for", "i", "in", "files", "]", "\n", "better", "=", "[", "better", "]", "*", "len", "(", "files", ")", "\n", "r", "=", "p", ".", "starmap_async", "(", "load_instanceseg_save", ",", "zip", "(", "input_files", ",", "output_files", ",", "better", ")", ")", "\n", "_", "=", "r", ".", "get", "(", ")", "\n", "p", ".", "close", "(", ")", "\n", "p", ".", "join", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.dataset_conversion.Task032_BraTS_2018.convert_labels_back_to_BraTS": [[26, 32], ["numpy.zeros_like"], "function", ["None"], ["def", "convert_labels_back_to_BraTS", "(", "seg", ":", "np", ".", "ndarray", ")", ":", "\n", "    ", "new_seg", "=", "np", ".", "zeros_like", "(", "seg", ")", "\n", "new_seg", "[", "seg", "==", "1", "]", "=", "2", "\n", "new_seg", "[", "seg", "==", "3", "]", "=", "4", "\n", "new_seg", "[", "seg", "==", "2", "]", "=", "1", "\n", "return", "new_seg", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.dataset_conversion.Task032_BraTS_2018.convert_labels_back_to_BraTS_2018_2019_convention": [[34, 51], ["maybe_mkdir_p", "subfiles", "SimpleITK.ReadImage", "SimpleITK.GetArrayFromImage", "Task032_BraTS_2018.convert_labels_back_to_BraTS", "SimpleITK.GetImageFromArray", "sitk.GetImageFromArray.CopyInformation", "SimpleITK.WriteImage", "join", "join"], "function", ["home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.dataset_conversion.Task032_BraTS_2018.convert_labels_back_to_BraTS"], ["", "def", "convert_labels_back_to_BraTS_2018_2019_convention", "(", "input_folder", ":", "str", ",", "output_folder", ":", "str", ")", ":", "\n", "    ", "\"\"\"\n    reads all prediction files (nifti) in the input folder, converts the labels back to BraTS convention and saves the\n    result in output_folder\n    :param input_folder:\n    :param output_folder:\n    :return:\n    \"\"\"", "\n", "maybe_mkdir_p", "(", "output_folder", ")", "\n", "nii", "=", "subfiles", "(", "input_folder", ",", "suffix", "=", "'.nii.gz'", ",", "join", "=", "False", ")", "\n", "for", "n", "in", "nii", ":", "\n", "        ", "a", "=", "sitk", ".", "ReadImage", "(", "join", "(", "input_folder", ",", "n", ")", ")", "\n", "b", "=", "sitk", ".", "GetArrayFromImage", "(", "a", ")", "\n", "c", "=", "convert_labels_back_to_BraTS", "(", "b", ")", "\n", "d", "=", "sitk", ".", "GetImageFromArray", "(", "c", ")", "\n", "d", ".", "CopyInformation", "(", "a", ")", "\n", "sitk", ".", "WriteImage", "(", "d", ",", "join", "(", "output_folder", ",", "n", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.dataset_conversion.Task027_AutomaticCardiacDetectionChallenge.convert_to_submission": [[22, 33], ["subfiles", "numpy.unique", "maybe_mkdir_p", "subfiles", "len", "subfiles.sort", "shutil.copy", "shutil.copy", "join", "join", "join", "join"], "function", ["None"], ["def", "convert_to_submission", "(", "source_dir", ",", "target_dir", ")", ":", "\n", "    ", "niftis", "=", "subfiles", "(", "source_dir", ",", "join", "=", "False", ",", "suffix", "=", "\".nii.gz\"", ")", "\n", "patientids", "=", "np", ".", "unique", "(", "[", "i", "[", ":", "10", "]", "for", "i", "in", "niftis", "]", ")", "\n", "maybe_mkdir_p", "(", "target_dir", ")", "\n", "for", "p", "in", "patientids", ":", "\n", "        ", "files_of_that_patient", "=", "subfiles", "(", "source_dir", ",", "prefix", "=", "p", ",", "suffix", "=", "\".nii.gz\"", ",", "join", "=", "False", ")", "\n", "assert", "len", "(", "files_of_that_patient", ")", "\n", "files_of_that_patient", ".", "sort", "(", ")", "\n", "# first is ED, second is ES", "\n", "shutil", ".", "copy", "(", "join", "(", "source_dir", ",", "files_of_that_patient", "[", "0", "]", ")", ",", "join", "(", "target_dir", ",", "p", "+", "\"_ED.nii.gz\"", ")", ")", "\n", "shutil", ".", "copy", "(", "join", "(", "source_dir", ",", "files_of_that_patient", "[", "1", "]", ")", ",", "join", "(", "target_dir", ",", "p", "+", "\"_ES.nii.gz\"", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.dataset_conversion.Task062_NIHPancreas.reorient": [[24, 28], ["nibabel.load", "nibabel.as_closest_canonical", "nibabel.save"], "function", ["None"], ["def", "reorient", "(", "filename", ")", ":", "\n", "    ", "img", "=", "nibabel", ".", "load", "(", "filename", ")", "\n", "img", "=", "nibabel", ".", "as_closest_canonical", "(", "img", ")", "\n", "nibabel", ".", "save", "(", "img", ",", "filename", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.dataset_conversion.Task089_Fluo-N2DH-SIM.load_bmp_convert_to_nifti_borders_2d": [[30, 44], ["skimage.io.imread", "SimpleITK.GetImageFromArray", "sitk.GetImageFromArray.SetSpacing", "SimpleITK.WriteImage", "join", "skimage.io.imread", "Task089_Fluo-N2DH-SIM.generate_border_as_suggested_by_twollmann_2d", "SimpleITK.GetImageFromArray", "sitk.GetImageFromArray.SetSpacing", "SimpleITK.WriteImage", "skimage.io.imread.astype", "list", "skimage.io.imread.astype", "list"], "function", ["home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.dataset_conversion.Task089_Fluo-N2DH-SIM.generate_border_as_suggested_by_twollmann_2d"], ["def", "load_bmp_convert_to_nifti_borders_2d", "(", "img_file", ",", "lab_file", ",", "img_out_base", ",", "anno_out", ",", "spacing", ",", "border_thickness", "=", "0.7", ")", ":", "\n", "    ", "img", "=", "imread", "(", "img_file", ")", "\n", "img_itk", "=", "sitk", ".", "GetImageFromArray", "(", "img", ".", "astype", "(", "np", ".", "float32", ")", "[", "None", "]", ")", "\n", "img_itk", ".", "SetSpacing", "(", "list", "(", "spacing", ")", "[", ":", ":", "-", "1", "]", "+", "[", "999", "]", ")", "\n", "sitk", ".", "WriteImage", "(", "img_itk", ",", "join", "(", "img_out_base", "+", "\"_0000.nii.gz\"", ")", ")", "\n", "\n", "if", "lab_file", "is", "not", "None", ":", "\n", "        ", "l", "=", "imread", "(", "lab_file", ")", "\n", "borders", "=", "generate_border_as_suggested_by_twollmann_2d", "(", "l", ",", "spacing", ",", "border_thickness", ")", "\n", "l", "[", "l", ">", "0", "]", "=", "1", "\n", "l", "[", "borders", "==", "1", "]", "=", "2", "\n", "l_itk", "=", "sitk", ".", "GetImageFromArray", "(", "l", ".", "astype", "(", "np", ".", "uint8", ")", "[", "None", "]", ")", "\n", "l_itk", ".", "SetSpacing", "(", "list", "(", "spacing", ")", "[", ":", ":", "-", "1", "]", "+", "[", "999", "]", ")", "\n", "sitk", ".", "WriteImage", "(", "l_itk", ",", "anno_out", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.dataset_conversion.Task089_Fluo-N2DH-SIM.generate_disk": [[46, 54], ["numpy.round().astype", "skimage.morphology.disk", "skimage.transform.resize", "skimage.transform.resize.astype", "numpy.round", "max", "numpy.array"], "function", ["None"], ["", "", "def", "generate_disk", "(", "spacing", ",", "radius", ",", "dtype", "=", "int", ")", ":", "\n", "    ", "radius_in_voxels", "=", "np", ".", "round", "(", "radius", "/", "np", ".", "array", "(", "spacing", ")", ")", ".", "astype", "(", "int", ")", "\n", "n", "=", "2", "*", "radius_in_voxels", "+", "1", "\n", "disk_iso", "=", "disk", "(", "max", "(", "n", ")", "*", "2", ",", "dtype", "=", "np", ".", "float64", ")", "\n", "disk_resampled", "=", "resize", "(", "disk_iso", ",", "n", ",", "1", ",", "'constant'", ",", "0", ",", "clip", "=", "True", ",", "anti_aliasing", "=", "False", ",", "preserve_range", "=", "True", ")", "\n", "disk_resampled", "[", "disk_resampled", ">", "0.5", "]", "=", "1", "\n", "disk_resampled", "[", "disk_resampled", "<=", "0.5", "]", "=", "0", "\n", "return", "disk_resampled", ".", "astype", "(", "dtype", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.dataset_conversion.Task089_Fluo-N2DH-SIM.generate_border_as_suggested_by_twollmann_2d": [[56, 66], ["numpy.zeros_like", "Task089_Fluo-N2DH-SIM.generate_disk", "numpy.unique", "skimage.morphology.erosion"], "function", ["home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.dataset_conversion.Task089_Fluo-N2DH-SIM.generate_disk"], ["", "def", "generate_border_as_suggested_by_twollmann_2d", "(", "label_img", ":", "np", ".", "ndarray", ",", "spacing", ",", "\n", "border_thickness", ":", "float", "=", "2", ")", "->", "np", ".", "ndarray", ":", "\n", "    ", "border", "=", "np", ".", "zeros_like", "(", "label_img", ")", "\n", "selem", "=", "generate_disk", "(", "spacing", ",", "border_thickness", ")", "\n", "for", "l", "in", "np", ".", "unique", "(", "label_img", ")", ":", "\n", "        ", "if", "l", "==", "0", ":", "continue", "\n", "mask", "=", "(", "label_img", "==", "l", ")", ".", "astype", "(", "int", ")", "\n", "eroded", "=", "erosion", "(", "mask", ",", "selem", ")", "\n", "border", "[", "(", "eroded", "==", "0", ")", "&", "(", "mask", "!=", "0", ")", "]", "=", "1", "\n", "", "return", "border", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.dataset_conversion.Task089_Fluo-N2DH-SIM.prepare_task": [[68, 140], ["multiprocessing.Pool", "join", "join", "join", "join", "maybe_mkdir_p", "maybe_mkdir_p", "maybe_mkdir_p", "len", "len", "save_json", "multiprocessing.Pool.close", "multiprocessing.Pool.join", "subfiles", "subfiles", "i.get", "os.path.join", "subfolders", "join", "join", "join", "join", "join", "res.append", "train_patient_names.append", "subfolders", "join", "join", "join", "res.append", "test_patient_names.append", "i.endswith", "isfile", "multiprocessing.Pool.starmap_async", "i.endswith", "multiprocessing.Pool.starmap_async"], "function", ["None"], ["", "def", "prepare_task", "(", "base", ",", "task_id", ",", "task_name", ",", "spacing", ",", "border_thickness", ":", "float", "=", "15", ")", ":", "\n", "    ", "p", "=", "Pool", "(", "16", ")", "\n", "\n", "foldername", "=", "\"Task%03.0d_%s\"", "%", "(", "task_id", ",", "task_name", ")", "\n", "\n", "out_base", "=", "join", "(", "nnUNet_raw_data", ",", "foldername", ")", "\n", "imagestr", "=", "join", "(", "out_base", ",", "\"imagesTr\"", ")", "\n", "imagests", "=", "join", "(", "out_base", ",", "\"imagesTs\"", ")", "\n", "labelstr", "=", "join", "(", "out_base", ",", "\"labelsTr\"", ")", "\n", "maybe_mkdir_p", "(", "imagestr", ")", "\n", "maybe_mkdir_p", "(", "imagests", ")", "\n", "maybe_mkdir_p", "(", "labelstr", ")", "\n", "\n", "train_patient_names", "=", "[", "]", "\n", "test_patient_names", "=", "[", "]", "\n", "res", "=", "[", "]", "\n", "\n", "for", "train_sequence", "in", "[", "i", "for", "i", "in", "subfolders", "(", "base", "+", "\"_train\"", ",", "join", "=", "False", ")", "if", "not", "i", ".", "endswith", "(", "\"_GT\"", ")", "]", ":", "\n", "        ", "train_cases", "=", "subfiles", "(", "join", "(", "base", "+", "'_train'", ",", "train_sequence", ")", ",", "suffix", "=", "\".tif\"", ",", "join", "=", "False", ")", "\n", "for", "t", "in", "train_cases", ":", "\n", "            ", "casename", "=", "train_sequence", "+", "\"_\"", "+", "t", "[", ":", "-", "4", "]", "\n", "img_file", "=", "join", "(", "base", "+", "'_train'", ",", "train_sequence", ",", "t", ")", "\n", "lab_file", "=", "join", "(", "base", "+", "'_train'", ",", "train_sequence", "+", "\"_GT\"", ",", "\"SEG\"", ",", "\"man_seg\"", "+", "t", "[", "1", ":", "]", ")", "\n", "if", "not", "isfile", "(", "lab_file", ")", ":", "\n", "                ", "continue", "\n", "", "img_out_base", "=", "join", "(", "imagestr", ",", "casename", ")", "\n", "anno_out", "=", "join", "(", "labelstr", ",", "casename", "+", "\".nii.gz\"", ")", "\n", "res", ".", "append", "(", "\n", "p", ".", "starmap_async", "(", "load_bmp_convert_to_nifti_borders_2d", ",", "\n", "(", "(", "img_file", ",", "lab_file", ",", "img_out_base", ",", "anno_out", ",", "spacing", ",", "border_thickness", ")", ",", ")", ")", ")", "\n", "train_patient_names", ".", "append", "(", "casename", ")", "\n", "\n", "", "", "for", "test_sequence", "in", "[", "i", "for", "i", "in", "subfolders", "(", "base", "+", "\"_test\"", ",", "join", "=", "False", ")", "if", "not", "i", ".", "endswith", "(", "\"_GT\"", ")", "]", ":", "\n", "        ", "test_cases", "=", "subfiles", "(", "join", "(", "base", "+", "'_test'", ",", "test_sequence", ")", ",", "suffix", "=", "\".tif\"", ",", "join", "=", "False", ")", "\n", "for", "t", "in", "test_cases", ":", "\n", "            ", "casename", "=", "test_sequence", "+", "\"_\"", "+", "t", "[", ":", "-", "4", "]", "\n", "img_file", "=", "join", "(", "base", "+", "'_test'", ",", "test_sequence", ",", "t", ")", "\n", "lab_file", "=", "None", "\n", "img_out_base", "=", "join", "(", "imagests", ",", "casename", ")", "\n", "anno_out", "=", "None", "\n", "res", ".", "append", "(", "\n", "p", ".", "starmap_async", "(", "load_bmp_convert_to_nifti_borders_2d", ",", "\n", "(", "(", "img_file", ",", "lab_file", ",", "img_out_base", ",", "anno_out", ",", "spacing", ",", "border_thickness", ")", ",", ")", ")", ")", "\n", "test_patient_names", ".", "append", "(", "casename", ")", "\n", "\n", "", "", "_", "=", "[", "i", ".", "get", "(", ")", "for", "i", "in", "res", "]", "\n", "\n", "json_dict", "=", "{", "}", "\n", "json_dict", "[", "'name'", "]", "=", "task_name", "\n", "json_dict", "[", "'description'", "]", "=", "\"\"", "\n", "json_dict", "[", "'tensorImageSize'", "]", "=", "\"4D\"", "\n", "json_dict", "[", "'reference'", "]", "=", "\"\"", "\n", "json_dict", "[", "'licence'", "]", "=", "\"\"", "\n", "json_dict", "[", "'release'", "]", "=", "\"0.0\"", "\n", "json_dict", "[", "'modality'", "]", "=", "{", "\n", "\"0\"", ":", "\"BF\"", ",", "\n", "}", "\n", "json_dict", "[", "'labels'", "]", "=", "{", "\n", "\"0\"", ":", "\"background\"", ",", "\n", "\"1\"", ":", "\"cell\"", ",", "\n", "\"2\"", ":", "\"border\"", ",", "\n", "}", "\n", "\n", "json_dict", "[", "'numTraining'", "]", "=", "len", "(", "train_patient_names", ")", "\n", "json_dict", "[", "'numTest'", "]", "=", "len", "(", "test_patient_names", ")", "\n", "json_dict", "[", "'training'", "]", "=", "[", "{", "'image'", ":", "\"./imagesTr/%s.nii.gz\"", "%", "i", ",", "\"label\"", ":", "\"./labelsTr/%s.nii.gz\"", "%", "i", "}", "for", "i", "in", "\n", "train_patient_names", "]", "\n", "json_dict", "[", "'test'", "]", "=", "[", "\"./imagesTs/%s.nii.gz\"", "%", "i", "for", "i", "in", "test_patient_names", "]", "\n", "\n", "save_json", "(", "json_dict", ",", "os", ".", "path", ".", "join", "(", "out_base", ",", "\"dataset.json\"", ")", ")", "\n", "p", ".", "close", "(", ")", "\n", "p", ".", "join", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.dataset_conversion.Task089_Fluo-N2DH-SIM.convert_to_instance_seg": [[142, 199], ["label", "numpy.unique", "label", "numpy.copy", "numpy.copy", "numpy.array", "numpy.array", "numpy.max", "label", "numpy.unique", "np.copy.astype", "numpy.sum", "max", "range", "skimage.morphology.disk", "dilation", "numpy.sum", "numpy.sum", "min", "range", "numpy.unique"], "function", ["None"], ["", "def", "convert_to_instance_seg", "(", "arr", ":", "np", ".", "ndarray", ",", "spacing", ":", "tuple", "=", "(", "0.125", ",", "0.125", ")", ",", "small_center_threshold", ":", "int", "=", "30", ",", "\n", "isolated_border_as_separate_instance_threshold", "=", "15", ")", ":", "\n", "    ", "from", "skimage", ".", "morphology", "import", "label", ",", "dilation", "\n", "\n", "# we first identify centers that are too small and set them to be border. This should remove false positive instances", "\n", "objects", "=", "label", "(", "(", "arr", "==", "1", ")", ".", "astype", "(", "int", ")", ")", "\n", "for", "o", "in", "np", ".", "unique", "(", "objects", ")", ":", "\n", "        ", "if", "o", ">", "0", "and", "np", ".", "sum", "(", "objects", "==", "o", ")", "<=", "small_center_threshold", ":", "\n", "            ", "arr", "[", "objects", "==", "o", "]", "=", "2", "\n", "\n", "# 1 is core, 2 is border", "\n", "", "", "objects", "=", "label", "(", "(", "arr", "==", "1", ")", ".", "astype", "(", "int", ")", ")", "\n", "final", "=", "np", ".", "copy", "(", "objects", ")", "\n", "remaining_border", "=", "arr", "==", "2", "\n", "current", "=", "np", ".", "copy", "(", "objects", ")", "\n", "dilated_mm", "=", "np", ".", "array", "(", "(", "0", ",", "0", ")", ")", "\n", "spacing", "=", "np", ".", "array", "(", "spacing", ")", "\n", "\n", "while", "np", ".", "sum", "(", "remaining_border", ")", ">", "0", ":", "\n", "        ", "strel_size", "=", "[", "0", ",", "0", "]", "\n", "maximum_dilation", "=", "max", "(", "dilated_mm", ")", "\n", "for", "i", "in", "range", "(", "2", ")", ":", "\n", "            ", "if", "spacing", "[", "i", "]", "==", "min", "(", "spacing", ")", ":", "\n", "                ", "strel_size", "[", "i", "]", "=", "1", "\n", "continue", "\n", "", "if", "dilated_mm", "[", "i", "]", "+", "spacing", "[", "i", "]", "/", "2", "<", "maximum_dilation", ":", "\n", "                ", "strel_size", "[", "i", "]", "=", "1", "\n", "", "", "ball_here", "=", "disk", "(", "1", ")", "\n", "\n", "if", "strel_size", "[", "0", "]", "==", "0", ":", "ball_here", "=", "ball_here", "[", "1", ":", "2", "]", "\n", "if", "strel_size", "[", "1", "]", "==", "0", ":", "ball_here", "=", "ball_here", "[", ":", ",", "1", ":", "2", "]", "\n", "\n", "#print(1)", "\n", "dilated", "=", "dilation", "(", "current", ",", "ball_here", ")", "\n", "diff", "=", "(", "current", "==", "0", ")", "&", "(", "dilated", "!=", "current", ")", "\n", "final", "[", "diff", "&", "remaining_border", "]", "=", "dilated", "[", "diff", "&", "remaining_border", "]", "\n", "remaining_border", "[", "diff", "]", "=", "0", "\n", "current", "=", "dilated", "\n", "dilated_mm", "=", "[", "dilated_mm", "[", "i", "]", "+", "spacing", "[", "i", "]", "if", "strel_size", "[", "i", "]", "==", "1", "else", "dilated_mm", "[", "i", "]", "for", "i", "in", "range", "(", "2", ")", "]", "\n", "\n", "# what can happen is that a cell is so small that the network only predicted border and no core. This cell will be", "\n", "# fused with the nearest other instance, which we don't want. Therefore we identify isolated border predictions and", "\n", "# give them a separate instance id", "\n", "# we identify isolated border predictions by checking each foreground object in arr and see whether this object", "\n", "# also contains label 1", "\n", "", "max_label", "=", "np", ".", "max", "(", "final", ")", "\n", "\n", "foreground_objects", "=", "label", "(", "(", "arr", "!=", "0", ")", ".", "astype", "(", "int", ")", ")", "\n", "for", "i", "in", "np", ".", "unique", "(", "foreground_objects", ")", ":", "\n", "        ", "if", "i", ">", "0", "and", "(", "1", "not", "in", "np", ".", "unique", "(", "arr", "[", "foreground_objects", "==", "i", "]", ")", ")", ":", "\n", "            ", "size_of_object", "=", "np", ".", "sum", "(", "foreground_objects", "==", "i", ")", "\n", "if", "size_of_object", ">=", "isolated_border_as_separate_instance_threshold", ":", "\n", "                ", "final", "[", "foreground_objects", "==", "i", "]", "=", "max_label", "+", "1", "\n", "max_label", "+=", "1", "\n", "#print('yeah boi')", "\n", "\n", "", "", "", "return", "final", ".", "astype", "(", "np", ".", "uint32", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.dataset_conversion.Task089_Fluo-N2DH-SIM.load_convert_to_instance_save": [[201, 208], ["SimpleITK.ReadImage", "SimpleITK.GetArrayFromImage", "SimpleITK.GetImageFromArray", "sitk.GetImageFromArray.CopyInformation", "SimpleITK.WriteImage", "Task089_Fluo-N2DH-SIM.convert_to_instance_seg", "out.astype"], "function", ["home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.dataset_conversion.Task089_Fluo-N2DH-SIM.convert_to_instance_seg"], ["", "def", "load_convert_to_instance_save", "(", "file_in", ":", "str", ",", "file_out", ":", "str", ",", "spacing", ")", ":", "\n", "    ", "img", "=", "sitk", ".", "ReadImage", "(", "file_in", ")", "\n", "img_npy", "=", "sitk", ".", "GetArrayFromImage", "(", "img", ")", "\n", "out", "=", "convert_to_instance_seg", "(", "img_npy", "[", "0", "]", ",", "spacing", ")", "[", "None", "]", "\n", "out_itk", "=", "sitk", ".", "GetImageFromArray", "(", "out", ".", "astype", "(", "np", ".", "int16", ")", ")", "\n", "out_itk", ".", "CopyInformation", "(", "img", ")", "\n", "sitk", ".", "WriteImage", "(", "out_itk", ",", "file_out", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.dataset_conversion.Task089_Fluo-N2DH-SIM.convert_folder_to_instanceseg": [[210, 227], ["subfiles", "maybe_mkdir_p", "multiprocessing.Pool", "zip", "multiprocessing.Pool.close", "multiprocessing.Pool.join", "join", "join", "r.append", "i.get", "multiprocessing.Pool.starmap_async"], "function", ["None"], ["", "def", "convert_folder_to_instanceseg", "(", "folder_in", ":", "str", ",", "folder_out", ":", "str", ",", "spacing", ",", "processes", ":", "int", "=", "12", ")", ":", "\n", "    ", "input_files", "=", "subfiles", "(", "folder_in", ",", "suffix", "=", "\".nii.gz\"", ",", "join", "=", "False", ")", "\n", "maybe_mkdir_p", "(", "folder_out", ")", "\n", "output_files", "=", "[", "join", "(", "folder_out", ",", "i", ")", "for", "i", "in", "input_files", "]", "\n", "input_files", "=", "[", "join", "(", "folder_in", ",", "i", ")", "for", "i", "in", "input_files", "]", "\n", "p", "=", "Pool", "(", "processes", ")", "\n", "r", "=", "[", "]", "\n", "for", "i", ",", "o", "in", "zip", "(", "input_files", ",", "output_files", ")", ":", "\n", "        ", "r", ".", "append", "(", "\n", "p", ".", "starmap_async", "(", "\n", "load_convert_to_instance_save", ",", "\n", "(", "(", "i", ",", "o", ",", "spacing", ")", ",", ")", "\n", ")", "\n", ")", "\n", "", "_", "=", "[", "i", ".", "get", "(", ")", "for", "i", "in", "r", "]", "\n", "p", ".", "close", "(", ")", "\n", "p", ".", "join", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.dataset_conversion.Task089_Fluo-N2DH-SIM.convert_to_tiff": [[229, 232], ["SimpleITK.GetArrayFromImage", "skimage.io.imsave", "SimpleITK.ReadImage", "npy[].astype"], "function", ["None"], ["", "def", "convert_to_tiff", "(", "nifti_image", ":", "str", ",", "output_name", ":", "str", ")", ":", "\n", "    ", "npy", "=", "sitk", ".", "GetArrayFromImage", "(", "sitk", ".", "ReadImage", "(", "nifti_image", ")", ")", "\n", "imsave", "(", "output_name", ",", "npy", "[", "0", "]", ".", "astype", "(", "np", ".", "uint16", ")", ",", "compress", "=", "6", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.dataset_conversion.Task056_VerSe2019.load_corr_save": [[30, 38], ["filename.endswith", "maybe_mkdir_p", "nnunet.dataset_conversion.Task056_Verse_normalize_orientation.read_image", "nnunet.dataset_conversion.Task056_Verse_normalize_orientation.normalize_slice_orientation", "nnunet.dataset_conversion.Task056_Verse_normalize_orientation.save_image", "save_pickle", "join", "join", "join"], "function", ["home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.dataset_conversion.Task056_Verse_normalize_orientation.read_image", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.dataset_conversion.Task056_Verse_normalize_orientation.normalize_slice_orientation", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.dataset_conversion.Task056_Verse_normalize_orientation.save_image"], ["def", "load_corr_save", "(", "in_folder", ":", "str", ",", "out_folder", ":", "str", ",", "filename", ":", "str", ")", ":", "\n", "    ", "assert", "filename", ".", "endswith", "(", "\".nii.gz\"", ")", "\n", "maybe_mkdir_p", "(", "out_folder", ")", "\n", "img", ",", "header", "=", "read_image", "(", "join", "(", "in_folder", ",", "filename", ")", ")", "\n", "img_corr", ",", "header_corr", "=", "normalize_slice_orientation", "(", "img", ",", "header", ")", "\n", "# now we save without restoring original slice orientation. We pickle the header for later", "\n", "save_image", "(", "img_corr", ",", "header_corr", ",", "join", "(", "out_folder", ",", "filename", ")", ")", "\n", "save_pickle", "(", "header", ",", "join", "(", "out_folder", ",", "filename", "[", ":", "-", "7", "]", "+", "\".pkl\"", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.dataset_conversion.Task056_VerSe2019.evaluate_verse_case": [[45, 64], ["SimpleITK.GetArrayFromImage", "SimpleITK.GetArrayFromImage", "range", "SimpleITK.ReadImage", "SimpleITK.ReadImage", "dice_scores.append", "numpy.sum", "medpy.metric.dc"], "function", ["None"], ["", "def", "evaluate_verse_case", "(", "sitk_file_ref", ":", "str", ",", "sitk_file_test", ":", "str", ")", ":", "\n", "    ", "\"\"\"\n    Only vertebra that are present in the reference will be evaluated\n    :param sitk_file_ref:\n    :param sitk_file_test:\n    :return:\n    \"\"\"", "\n", "gt_npy", "=", "sitk", ".", "GetArrayFromImage", "(", "sitk", ".", "ReadImage", "(", "sitk_file_ref", ")", ")", "\n", "pred_npy", "=", "sitk", ".", "GetArrayFromImage", "(", "sitk", ".", "ReadImage", "(", "sitk_file_test", ")", ")", "\n", "dice_scores", "=", "[", "]", "\n", "for", "label", "in", "range", "(", "1", ",", "26", ")", ":", "\n", "        ", "mask_gt", "=", "gt_npy", "==", "label", "\n", "if", "np", ".", "sum", "(", "mask_gt", ")", ">", "0", ":", "\n", "            ", "mask_pred", "=", "pred_npy", "==", "label", "\n", "dc", "=", "metric", ".", "dc", "(", "mask_pred", ",", "mask_gt", ")", "\n", "", "else", ":", "\n", "            ", "dc", "=", "np", ".", "nan", "\n", "", "dice_scores", ".", "append", "(", "dc", ")", "\n", "", "return", "dice_scores", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.dataset_conversion.Task056_VerSe2019.evaluate_verse_folder": [[66, 84], ["multiprocessing.pool.Pool", "subfiles", "all", "multiprocessing.pool.Pool.starmap_async", "results.get.get", "numpy.vstack", "numpy.nanmean", "numpy.nanmean", "save_json", "join", "join", "zip", "isfile", "zip", "list", "join"], "function", ["None"], ["", "def", "evaluate_verse_folder", "(", "folder_pred", ",", "folder_gt", ",", "out_json", "=", "\"/home/fabian/verse.json\"", ")", ":", "\n", "    ", "p", "=", "Pool", "(", "default_num_threads", ")", "\n", "files_gt_bare", "=", "subfiles", "(", "folder_gt", ",", "join", "=", "False", ")", "\n", "assert", "all", "(", "[", "isfile", "(", "join", "(", "folder_pred", ",", "i", ")", ")", "for", "i", "in", "files_gt_bare", "]", ")", ",", "\"some files are missing in the predicted folder\"", "\n", "files_pred", "=", "[", "join", "(", "folder_pred", ",", "i", ")", "for", "i", "in", "files_gt_bare", "]", "\n", "files_gt", "=", "[", "join", "(", "folder_gt", ",", "i", ")", "for", "i", "in", "files_gt_bare", "]", "\n", "\n", "results", "=", "p", ".", "starmap_async", "(", "evaluate_verse_case", ",", "zip", "(", "files_gt", ",", "files_pred", ")", ")", "\n", "\n", "results", "=", "results", ".", "get", "(", ")", "\n", "\n", "dct", "=", "{", "i", ":", "j", "for", "i", ",", "j", "in", "zip", "(", "files_gt_bare", ",", "results", ")", "}", "\n", "\n", "results_stacked", "=", "np", ".", "vstack", "(", "results", ")", "\n", "results_mean", "=", "np", ".", "nanmean", "(", "results_stacked", ",", "0", ")", "\n", "overall_mean", "=", "np", ".", "nanmean", "(", "results_mean", ")", "\n", "\n", "save_json", "(", "(", "dct", ",", "list", "(", "results_mean", ")", ",", "overall_mean", ")", ",", "out_json", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.dataset_conversion.Task029_LiverTumorSegmentationChallenge.export_segmentations": [[24, 31], ["subfiles", "str", "join", "SimpleITK.ReadImage", "SimpleITK.WriteImage", "join", "n.split"], "function", ["None"], ["def", "export_segmentations", "(", "indir", ",", "outdir", ")", ":", "\n", "    ", "niftis", "=", "subfiles", "(", "indir", ",", "suffix", "=", "'nii.gz'", ",", "join", "=", "False", ")", "\n", "for", "n", "in", "niftis", ":", "\n", "        ", "identifier", "=", "str", "(", "n", ".", "split", "(", "\"_\"", ")", "[", "-", "1", "]", "[", ":", "-", "7", "]", ")", "\n", "outfname", "=", "join", "(", "outdir", ",", "\"test-segmentation-%s.nii\"", "%", "identifier", ")", "\n", "img", "=", "sitk", ".", "ReadImage", "(", "join", "(", "indir", ",", "n", ")", ")", "\n", "sitk", ".", "WriteImage", "(", "img", ",", "outfname", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.dataset_conversion.Task029_LiverTumorSegmentationChallenge.export_segmentations_postprocess": [[33, 52], ["maybe_mkdir_p", "subfiles", "print", "str", "join", "SimpleITK.ReadImage", "SimpleITK.GetArrayFromImage", "scipy.ndimage.label", "range", "print", "SimpleITK.GetImageFromArray", "sitk.GetImageFromArray.CopyInformation", "SimpleITK.WriteImage", "join", "sizes.append", "numpy.argmax", "n.split"], "function", ["None"], ["", "", "def", "export_segmentations_postprocess", "(", "indir", ",", "outdir", ")", ":", "\n", "    ", "maybe_mkdir_p", "(", "outdir", ")", "\n", "niftis", "=", "subfiles", "(", "indir", ",", "suffix", "=", "'nii.gz'", ",", "join", "=", "False", ")", "\n", "for", "n", "in", "niftis", ":", "\n", "        ", "print", "(", "\"\\n\"", ",", "n", ")", "\n", "identifier", "=", "str", "(", "n", ".", "split", "(", "\"_\"", ")", "[", "-", "1", "]", "[", ":", "-", "7", "]", ")", "\n", "outfname", "=", "join", "(", "outdir", ",", "\"test-segmentation-%s.nii\"", "%", "identifier", ")", "\n", "img", "=", "sitk", ".", "ReadImage", "(", "join", "(", "indir", ",", "n", ")", ")", "\n", "img_npy", "=", "sitk", ".", "GetArrayFromImage", "(", "img", ")", "\n", "lmap", ",", "num_objects", "=", "label", "(", "(", "img_npy", ">", "0", ")", ".", "astype", "(", "int", ")", ")", "\n", "sizes", "=", "[", "]", "\n", "for", "o", "in", "range", "(", "1", ",", "num_objects", "+", "1", ")", ":", "\n", "            ", "sizes", ".", "append", "(", "(", "lmap", "==", "o", ")", ".", "sum", "(", ")", ")", "\n", "", "mx", "=", "np", ".", "argmax", "(", "sizes", ")", "+", "1", "\n", "print", "(", "sizes", ")", "\n", "img_npy", "[", "lmap", "!=", "mx", "]", "=", "0", "\n", "img_new", "=", "sitk", ".", "GetImageFromArray", "(", "img_npy", ")", "\n", "img_new", ".", "CopyInformation", "(", "img", ")", "\n", "sitk", ".", "WriteImage", "(", "img_new", ",", "outfname", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.dataset_conversion.Task058_ISBI_EM_SEG.export_for_submission": [[25, 37], ["out_file.endswith", "skimage.io.imsave", "numpy.load", "nonmembr_prob.astype", "a.sum"], "function", ["None"], ["def", "export_for_submission", "(", "predicted_npz", ",", "out_file", ")", ":", "\n", "    ", "\"\"\"\n    they expect us to submit a 32 bit 3d tif image with values between 0 (100% membrane certainty) and 1\n    (100% non-membrane certainty). We use the softmax output for that\n    :return:\n    \"\"\"", "\n", "a", "=", "np", ".", "load", "(", "predicted_npz", ")", "[", "'softmax'", "]", "\n", "a", "=", "a", "/", "a", ".", "sum", "(", "0", ")", "[", "None", "]", "\n", "# channel 0 is non-membrane prob", "\n", "nonmembr_prob", "=", "a", "[", "0", "]", "\n", "assert", "out_file", ".", "endswith", "(", "\".tif\"", ")", "\n", "io", ".", "imsave", "(", "out_file", ",", "nonmembr_prob", ".", "astype", "(", "np", ".", "float32", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.dataset_conversion.Task037_038_Chaos_Challenge.load_png_stack": [[31, 39], ["subfiles", "subfiles.sort", "loaded.append", "numpy.stack", "numpy.array", "PIL.Image.open"], "function", ["None"], ["def", "load_png_stack", "(", "folder", ")", ":", "\n", "    ", "pngs", "=", "subfiles", "(", "folder", ",", "suffix", "=", "\"png\"", ")", "\n", "pngs", ".", "sort", "(", ")", "\n", "loaded", "=", "[", "]", "\n", "for", "p", "in", "pngs", ":", "\n", "        ", "loaded", ".", "append", "(", "np", ".", "array", "(", "Image", ".", "open", "(", "p", ")", ")", ")", "\n", "", "loaded", "=", "np", ".", "stack", "(", "loaded", ",", "0", ")", "[", ":", ":", "-", "1", "]", "\n", "return", "loaded", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.dataset_conversion.Task037_038_Chaos_Challenge.convert_CT_seg": [[41, 43], ["loaded_png.astype"], "function", ["None"], ["", "def", "convert_CT_seg", "(", "loaded_png", ")", ":", "\n", "    ", "return", "loaded_png", ".", "astype", "(", "np", ".", "uint16", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.dataset_conversion.Task037_038_Chaos_Challenge.convert_MR_seg": [[45, 52], ["numpy.zeros"], "function", ["None"], ["", "def", "convert_MR_seg", "(", "loaded_png", ")", ":", "\n", "    ", "result", "=", "np", ".", "zeros", "(", "loaded_png", ".", "shape", ")", "\n", "result", "[", "(", "loaded_png", ">", "55", ")", "&", "(", "loaded_png", "<=", "70", ")", "]", "=", "1", "# liver", "\n", "result", "[", "(", "loaded_png", ">", "110", ")", "&", "(", "loaded_png", "<=", "135", ")", "]", "=", "2", "# right kidney", "\n", "result", "[", "(", "loaded_png", ">", "175", ")", "&", "(", "loaded_png", "<=", "200", ")", "]", "=", "3", "# left kidney", "\n", "result", "[", "(", "loaded_png", ">", "240", ")", "&", "(", "loaded_png", "<=", "255", ")", "]", "=", "4", "# spleen", "\n", "return", "result", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.dataset_conversion.Task037_038_Chaos_Challenge.convert_seg_to_intensity_task5": [[54, 61], ["numpy.zeros"], "function", ["None"], ["", "def", "convert_seg_to_intensity_task5", "(", "seg", ")", ":", "\n", "    ", "seg_new", "=", "np", ".", "zeros", "(", "seg", ".", "shape", ",", "dtype", "=", "np", ".", "uint8", ")", "\n", "seg_new", "[", "seg", "==", "1", "]", "=", "63", "\n", "seg_new", "[", "seg", "==", "2", "]", "=", "126", "\n", "seg_new", "[", "seg", "==", "3", "]", "=", "189", "\n", "seg_new", "[", "seg", "==", "4", "]", "=", "252", "\n", "return", "seg_new", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.dataset_conversion.Task037_038_Chaos_Challenge.convert_seg_to_intensity_task3": [[63, 67], ["numpy.zeros"], "function", ["None"], ["", "def", "convert_seg_to_intensity_task3", "(", "seg", ")", ":", "\n", "    ", "seg_new", "=", "np", ".", "zeros", "(", "seg", ".", "shape", ",", "dtype", "=", "np", ".", "uint8", ")", "\n", "seg_new", "[", "seg", "==", "1", "]", "=", "63", "\n", "return", "seg_new", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.dataset_conversion.Task037_038_Chaos_Challenge.write_pngs_from_nifti": [[69, 74], ["SimpleITK.GetArrayFromImage", "converter", "range", "SimpleITK.ReadImage", "len", "PIL.Image.fromarray().save", "join", "PIL.Image.fromarray"], "function", ["None"], ["", "def", "write_pngs_from_nifti", "(", "nifti", ",", "output_folder", ",", "converter", "=", "convert_seg_to_intensity_task3", ")", ":", "\n", "    ", "npy", "=", "sitk", ".", "GetArrayFromImage", "(", "sitk", ".", "ReadImage", "(", "nifti", ")", ")", "\n", "seg_new", "=", "converter", "(", "npy", ")", "\n", "for", "z", "in", "range", "(", "len", "(", "npy", ")", ")", ":", "\n", "        ", "Image", ".", "fromarray", "(", "seg_new", "[", "z", "]", ")", ".", "save", "(", "join", "(", "output_folder", ",", "\"img%03.0d.png\"", "%", "z", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.dataset_conversion.Task037_038_Chaos_Challenge.convert_variant2_predicted_test_to_submission_format": [[76, 153], ["join", "maybe_mkdir_p", "join", "join", "maybe_mkdir_p", "maybe_mkdir_p", "shutil.copy", "shutil.copy", "numpy.savez_compressed", "shutil.copy", "nnunet.inference.ensemble_predictions.merge", "subfiles", "join", "load_postprocessing", "apply_postprocessing_to_folder", "join", "join", "shutil.copy", "shutil.copy", "shutil.copy", "shutil.copy", "join", "join", "join", "join", "numpy.load", "join", "join", "join", "shutil.copy", "join", "join", "Task037_038_Chaos_Challenge.write_pngs_from_nifti", "join", "join", "Task037_038_Chaos_Challenge.write_pngs_from_nifti", "join", "join", "Task037_038_Chaos_Challenge.write_pngs_from_nifti", "join", "join", "Task037_038_Chaos_Challenge.write_pngs_from_nifti", "subfiles", "join", "join", "join", "join", "join", "join", "join", "join", "join", "join", "join", "i.split", "t2.split", "t2.split"], "function", ["home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.model_selection.ensemble.merge", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.postprocessing.connected_components.load_postprocessing", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.postprocessing.connected_components.apply_postprocessing_to_folder", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.dataset_conversion.Task037_038_Chaos_Challenge.write_pngs_from_nifti", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.dataset_conversion.Task037_038_Chaos_Challenge.write_pngs_from_nifti", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.dataset_conversion.Task037_038_Chaos_Challenge.write_pngs_from_nifti", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.dataset_conversion.Task037_038_Chaos_Challenge.write_pngs_from_nifti"], ["", "", "def", "convert_variant2_predicted_test_to_submission_format", "(", "folder_with_predictions", ",", "\n", "output_folder", "=", "\"/home/fabian/drives/datasets/results/nnUNet/test_sets/Task038_CHAOS_Task_3_5_Variant2/ready_to_submit\"", ",", "\n", "postprocessing_file", "=", "\"/home/fabian/drives/datasets/results/nnUNet/ensembles/Task038_CHAOS_Task_3_5_Variant2/ensemble_2d__nnUNetTrainerV2__nnUNetPlansv2.1--3d_fullres__nnUNetTrainerV2__nnUNetPlansv2.1/postprocessing.json\"", ")", ":", "\n", "    ", "\"\"\"\n    output_folder is where the extracted template is\n    :param folder_with_predictions:\n    :param output_folder:\n    :return:\n    \"\"\"", "\n", "postprocessing_file", "=", "\"/media/fabian/Results/nnUNet/3d_fullres/Task039_CHAOS_Task_3_5_Variant2_highres/\"", "\"nnUNetTrainerV2__nnUNetPlansfixed/postprocessing.json\"", "\n", "\n", "# variant 2 treats in and out phase as two training examples, so we need to ensemble these two again", "\n", "final_predictions_folder", "=", "join", "(", "output_folder", ",", "\"final\"", ")", "\n", "maybe_mkdir_p", "(", "final_predictions_folder", ")", "\n", "t1_patient_names", "=", "[", "i", ".", "split", "(", "\"_\"", ")", "[", "-", "1", "]", "[", ":", "-", "7", "]", "for", "i", "in", "subfiles", "(", "folder_with_predictions", ",", "prefix", "=", "\"T1\"", ",", "suffix", "=", "\".nii.gz\"", ",", "join", "=", "False", ")", "]", "\n", "folder_for_ensembing0", "=", "join", "(", "output_folder", ",", "\"ens0\"", ")", "\n", "folder_for_ensembing1", "=", "join", "(", "output_folder", ",", "\"ens1\"", ")", "\n", "maybe_mkdir_p", "(", "folder_for_ensembing0", ")", "\n", "maybe_mkdir_p", "(", "folder_for_ensembing1", ")", "\n", "# now copy all t1 out phases in ens0 and all in phases in ens1. Name them the same.", "\n", "for", "t1", "in", "t1_patient_names", ":", "\n", "        ", "shutil", ".", "copy", "(", "join", "(", "folder_with_predictions", ",", "\"T1_in_%s.npz\"", "%", "t1", ")", ",", "join", "(", "folder_for_ensembing1", ",", "\"T1_%s.npz\"", "%", "t1", ")", ")", "\n", "shutil", ".", "copy", "(", "join", "(", "folder_with_predictions", ",", "\"T1_in_%s.pkl\"", "%", "t1", ")", ",", "join", "(", "folder_for_ensembing1", ",", "\"T1_%s.pkl\"", "%", "t1", ")", ")", "\n", "shutil", ".", "copy", "(", "join", "(", "folder_with_predictions", ",", "\"T1_out_%s.npz\"", "%", "t1", ")", ",", "join", "(", "folder_for_ensembing0", ",", "\"T1_%s.npz\"", "%", "t1", ")", ")", "\n", "shutil", ".", "copy", "(", "join", "(", "folder_with_predictions", ",", "\"T1_out_%s.pkl\"", "%", "t1", ")", ",", "join", "(", "folder_for_ensembing0", ",", "\"T1_%s.pkl\"", "%", "t1", ")", ")", "\n", "", "shutil", ".", "copy", "(", "join", "(", "folder_with_predictions", ",", "\"plans.pkl\"", ")", ",", "join", "(", "folder_for_ensembing0", ",", "\"plans.pkl\"", ")", ")", "\n", "shutil", ".", "copy", "(", "join", "(", "folder_with_predictions", ",", "\"plans.pkl\"", ")", ",", "join", "(", "folder_for_ensembing1", ",", "\"plans.pkl\"", ")", ")", "\n", "\n", "# there is a problem with T1_35 that I need to correct manually (different crop size, will not negatively impact results)", "\n", "#ens0_softmax = np.load(join(folder_for_ensembing0, \"T1_35.npz\"))['softmax']", "\n", "ens1_softmax", "=", "np", ".", "load", "(", "join", "(", "folder_for_ensembing1", ",", "\"T1_35.npz\"", ")", ")", "[", "'softmax'", "]", "\n", "#ens0_props = load_pickle(join(folder_for_ensembing0, \"T1_35.pkl\"))", "\n", "#ens1_props = load_pickle(join(folder_for_ensembing1, \"T1_35.pkl\"))", "\n", "ens1_softmax", "=", "ens1_softmax", "[", ":", ",", ":", ",", ":", "-", "1", ",", ":", "]", "\n", "np", ".", "savez_compressed", "(", "join", "(", "folder_for_ensembing1", ",", "\"T1_35.npz\"", ")", ",", "softmax", "=", "ens1_softmax", ")", "\n", "shutil", ".", "copy", "(", "join", "(", "folder_for_ensembing0", ",", "\"T1_35.pkl\"", ")", ",", "join", "(", "folder_for_ensembing1", ",", "\"T1_35.pkl\"", ")", ")", "\n", "\n", "# now call my ensemble function", "\n", "merge", "(", "(", "folder_for_ensembing0", ",", "folder_for_ensembing1", ")", ",", "final_predictions_folder", ",", "8", ",", "True", ",", "\n", "postprocessing_file", "=", "postprocessing_file", ")", "\n", "# copy t2 files to final_predictions_folder as well", "\n", "t2_files", "=", "subfiles", "(", "folder_with_predictions", ",", "prefix", "=", "\"T2\"", ",", "suffix", "=", "\".nii.gz\"", ",", "join", "=", "False", ")", "\n", "for", "t2", "in", "t2_files", ":", "\n", "        ", "shutil", ".", "copy", "(", "join", "(", "folder_with_predictions", ",", "t2", ")", ",", "join", "(", "final_predictions_folder", ",", "t2", ")", ")", "\n", "\n", "# apply postprocessing", "\n", "", "from", "nnunet", ".", "postprocessing", ".", "connected_components", "import", "apply_postprocessing_to_folder", ",", "load_postprocessing", "\n", "postprocessed_folder", "=", "join", "(", "output_folder", ",", "\"final_postprocessed\"", ")", "\n", "for_which_classes", ",", "min_valid_obj_size", "=", "load_postprocessing", "(", "postprocessing_file", ")", "\n", "apply_postprocessing_to_folder", "(", "final_predictions_folder", ",", "postprocessed_folder", ",", "\n", "for_which_classes", ",", "min_valid_obj_size", ",", "8", ")", "\n", "\n", "# now export the niftis in the weird png format", "\n", "# task 3", "\n", "output_dir", "=", "join", "(", "output_folder", ",", "\"CHAOS_submission_template_new\"", ",", "\"Task3\"", ",", "\"MR\"", ")", "\n", "for", "t1", "in", "t1_patient_names", ":", "\n", "        ", "output_folder_here", "=", "join", "(", "output_dir", ",", "t1", ",", "\"T1DUAL\"", ",", "\"Results\"", ")", "\n", "nifti_file", "=", "join", "(", "postprocessed_folder", ",", "\"T1_%s.nii.gz\"", "%", "t1", ")", "\n", "write_pngs_from_nifti", "(", "nifti_file", ",", "output_folder_here", ",", "converter", "=", "convert_seg_to_intensity_task3", ")", "\n", "", "for", "t2", "in", "t2_files", ":", "\n", "        ", "patname", "=", "t2", ".", "split", "(", "\"_\"", ")", "[", "-", "1", "]", "[", ":", "-", "7", "]", "\n", "output_folder_here", "=", "join", "(", "output_dir", ",", "patname", ",", "\"T2SPIR\"", ",", "\"Results\"", ")", "\n", "nifti_file", "=", "join", "(", "postprocessed_folder", ",", "\"T2_%s.nii.gz\"", "%", "patname", ")", "\n", "write_pngs_from_nifti", "(", "nifti_file", ",", "output_folder_here", ",", "converter", "=", "convert_seg_to_intensity_task3", ")", "\n", "\n", "# task 5", "\n", "", "output_dir", "=", "join", "(", "output_folder", ",", "\"CHAOS_submission_template_new\"", ",", "\"Task5\"", ",", "\"MR\"", ")", "\n", "for", "t1", "in", "t1_patient_names", ":", "\n", "        ", "output_folder_here", "=", "join", "(", "output_dir", ",", "t1", ",", "\"T1DUAL\"", ",", "\"Results\"", ")", "\n", "nifti_file", "=", "join", "(", "postprocessed_folder", ",", "\"T1_%s.nii.gz\"", "%", "t1", ")", "\n", "write_pngs_from_nifti", "(", "nifti_file", ",", "output_folder_here", ",", "converter", "=", "convert_seg_to_intensity_task5", ")", "\n", "", "for", "t2", "in", "t2_files", ":", "\n", "        ", "patname", "=", "t2", ".", "split", "(", "\"_\"", ")", "[", "-", "1", "]", "[", ":", "-", "7", "]", "\n", "output_folder_here", "=", "join", "(", "output_dir", ",", "patname", ",", "\"T2SPIR\"", ",", "\"Results\"", ")", "\n", "nifti_file", "=", "join", "(", "postprocessed_folder", ",", "\"T2_%s.nii.gz\"", "%", "patname", ")", "\n", "write_pngs_from_nifti", "(", "nifti_file", ",", "output_folder_here", ",", "converter", "=", "convert_seg_to_intensity_task5", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.dataset_conversion.Task043_BraTS_2019.copy_BraTS_segmentation_and_convert_labels": [[25, 43], ["SimpleITK.ReadImage", "SimpleITK.GetArrayFromImage", "numpy.unique", "numpy.zeros_like", "SimpleITK.GetImageFromArray", "sitk.GetImageFromArray.CopyInformation", "SimpleITK.WriteImage", "RuntimeError"], "function", ["None"], ["def", "copy_BraTS_segmentation_and_convert_labels", "(", "in_file", ",", "out_file", ")", ":", "\n", "# use this for segmentation only!!!", "\n", "# nnUNet wants the labels to be continuous. BraTS is 0, 1, 2, 4 -> we make that into 0, 1, 2, 3", "\n", "    ", "img", "=", "sitk", ".", "ReadImage", "(", "in_file", ")", "\n", "img_npy", "=", "sitk", ".", "GetArrayFromImage", "(", "img", ")", "\n", "\n", "uniques", "=", "np", ".", "unique", "(", "img_npy", ")", "\n", "for", "u", "in", "uniques", ":", "\n", "        ", "if", "u", "not", "in", "[", "0", ",", "1", ",", "2", ",", "4", "]", ":", "\n", "            ", "raise", "RuntimeError", "(", "'unexpected label'", ")", "\n", "\n", "", "", "seg_new", "=", "np", ".", "zeros_like", "(", "img_npy", ")", "\n", "seg_new", "[", "img_npy", "==", "4", "]", "=", "3", "\n", "seg_new", "[", "img_npy", "==", "2", "]", "=", "1", "\n", "seg_new", "[", "img_npy", "==", "1", "]", "=", "2", "\n", "img_corr", "=", "sitk", ".", "GetImageFromArray", "(", "seg_new", ")", "\n", "img_corr", ".", "CopyInformation", "(", "img", ")", "\n", "sitk", ".", "WriteImage", "(", "img_corr", ",", "out_file", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.dataset_conversion.Task035_ISBI_MSLesionSegmentationChallenge.convert_to_nii_gz": [[23, 27], ["SimpleITK.ReadImage", "SimpleITK.WriteImage", "os.remove", "os.path.splitext"], "function", ["None"], ["def", "convert_to_nii_gz", "(", "filename", ")", ":", "\n", "    ", "f", "=", "sitk", ".", "ReadImage", "(", "filename", ")", "\n", "sitk", ".", "WriteImage", "(", "f", ",", "os", ".", "path", ".", "splitext", "(", "filename", ")", "[", "0", "]", "+", "\".nii.gz\"", ")", "\n", "os", ".", "remove", "(", "filename", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.dataset_conversion.Task035_ISBI_MSLesionSegmentationChallenge.convert_for_submission": [[29, 39], ["subfiles", "maybe_mkdir_p", "f.split", "int", "int", "join", "SimpleITK.ReadImage", "SimpleITK.WriteImage", "join"], "function", ["None"], ["", "def", "convert_for_submission", "(", "source_dir", ",", "target_dir", ")", ":", "\n", "    ", "files", "=", "subfiles", "(", "source_dir", ",", "suffix", "=", "\".nii.gz\"", ",", "join", "=", "False", ")", "\n", "maybe_mkdir_p", "(", "target_dir", ")", "\n", "for", "f", "in", "files", ":", "\n", "        ", "splitted", "=", "f", ".", "split", "(", "\"__\"", ")", "\n", "case_id", "=", "int", "(", "splitted", "[", "1", "]", ")", "\n", "timestep", "=", "int", "(", "splitted", "[", "2", "]", "[", ":", "-", "7", "]", ")", "\n", "t", "=", "join", "(", "target_dir", ",", "\"test%02d_%02d_nnUNet.nii\"", "%", "(", "case_id", ",", "timestep", ")", ")", "\n", "img", "=", "sitk", ".", "ReadImage", "(", "join", "(", "source_dir", ",", "f", ")", ")", "\n", "sitk", ".", "WriteImage", "(", "img", ",", "t", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.dataset_conversion.Task024_Promise2012.export_for_submission": [[19, 32], ["subfiles", "maybe_mkdir_p", "zip", "join", "SimpleITK.ReadImage", "SimpleITK.WriteImage", "join"], "function", ["None"], ["def", "export_for_submission", "(", "source_dir", ",", "target_dir", ")", ":", "\n", "    ", "\"\"\"\n    promise wants mhd :-/\n    :param source_dir:\n    :param target_dir:\n    :return:\n    \"\"\"", "\n", "files", "=", "subfiles", "(", "source_dir", ",", "suffix", "=", "\".nii.gz\"", ",", "join", "=", "False", ")", "\n", "target_files", "=", "[", "join", "(", "target_dir", ",", "i", "[", ":", "-", "7", "]", "+", "\".mhd\"", ")", "for", "i", "in", "files", "]", "\n", "maybe_mkdir_p", "(", "target_dir", ")", "\n", "for", "f", ",", "t", "in", "zip", "(", "files", ",", "target_files", ")", ":", "\n", "        ", "img", "=", "sitk", ".", "ReadImage", "(", "join", "(", "source_dir", ",", "f", ")", ")", "\n", "sitk", ".", "WriteImage", "(", "img", ",", "t", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.run.run_training_DDP.main": [[28, 161], ["argparse.ArgumentParser", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.parse_args", "nnunet.run.default_configuration.get_default_configuration", "trainer_class", "trainer_class.initialize", "nnunet.utilities.task_name_id_conversion.convert_id_to_task_name.startswith", "int", "nnunet.utilities.task_name_id_conversion.convert_id_to_task_name", "int", "RuntimeError", "issubclass", "issubclass", "trainer_class.find_lr", "trainer_class.network.eval", "trainer_class.validate", "trainer_class.run_training", "trainer_class.load_best_checkpoint", "print", "nnunet.training.cascade_stuff.predict_next_stage.predict_next_stage", "trainer_class.load_latest_checkpoint", "trainer_class.load_best_checkpoint", "trainer_class.load_latest_checkpoint", "join"], "function", ["home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.run.default_configuration.get_default_configuration", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.BraTS_trainer.nnUNetTrainerV2BraTSRegions.nnUNetTrainerV2BraTSRegions_DA3_BN.initialize", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.utilities.task_name_id_conversion.convert_id_to_task_name", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.network_training.network_trainer.NetworkTrainer.find_lr", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.BraTS_trainer.nnUNetTrainerV2BraTSRegions.nnUNetTrainerV2BraTSRegions_DA3_BN.validate", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.architectural_variants.nnUNetTrainerV2_ResencUNet.nnUNetTrainerV2_ResencUNet.run_training", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.network_training.network_trainer.NetworkTrainer.load_best_checkpoint", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.cascade_stuff.predict_next_stage.predict_next_stage", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.network_training.network_trainer.NetworkTrainer.load_latest_checkpoint", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.network_training.network_trainer.NetworkTrainer.load_best_checkpoint", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.network_training.network_trainer.NetworkTrainer.load_latest_checkpoint"], ["def", "main", "(", ")", ":", "\n", "    ", "parser", "=", "argparse", ".", "ArgumentParser", "(", ")", "\n", "parser", ".", "add_argument", "(", "\"network\"", ")", "\n", "parser", ".", "add_argument", "(", "\"network_trainer\"", ")", "\n", "parser", ".", "add_argument", "(", "\"task\"", ",", "help", "=", "\"can be task name or task id\"", ")", "\n", "parser", ".", "add_argument", "(", "\"fold\"", ",", "help", "=", "'0, 1, ..., 5 or \\'all\\''", ")", "\n", "parser", ".", "add_argument", "(", "\"-val\"", ",", "\"--validation_only\"", ",", "help", "=", "\"use this if you want to only run the validation\"", ",", "\n", "action", "=", "\"store_true\"", ")", "\n", "parser", ".", "add_argument", "(", "\"-c\"", ",", "\"--continue_training\"", ",", "help", "=", "\"use this if you want to continue a training\"", ",", "\n", "action", "=", "\"store_true\"", ")", "\n", "parser", ".", "add_argument", "(", "\"-p\"", ",", "help", "=", "\"plans identifier. Only change this if you created a custom experiment planner\"", ",", "\n", "default", "=", "default_plans_identifier", ",", "required", "=", "False", ")", "\n", "parser", ".", "add_argument", "(", "\"--use_compressed_data\"", ",", "default", "=", "False", ",", "action", "=", "\"store_true\"", ",", "\n", "help", "=", "\"If you set use_compressed_data, the training cases will not be decompressed. Reading compressed data \"", "\n", "\"is much more CPU and RAM intensive and should only be used if you know what you are \"", "\n", "\"doing\"", ",", "required", "=", "False", ")", "\n", "parser", ".", "add_argument", "(", "\"--deterministic\"", ",", "\n", "help", "=", "\"Makes training deterministic, but reduces training speed substantially. I (Fabian) think \"", "\n", "\"this is not necessary. Deterministic training will make you overfit to some random seed. \"", "\n", "\"Don't use that.\"", ",", "\n", "required", "=", "False", ",", "default", "=", "False", ",", "action", "=", "\"store_true\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--local_rank\"", ",", "default", "=", "0", ",", "type", "=", "int", ")", "\n", "parser", ".", "add_argument", "(", "\"--fp32\"", ",", "required", "=", "False", ",", "default", "=", "False", ",", "action", "=", "\"store_true\"", ",", "\n", "help", "=", "\"disable mixed precision training and run old school fp32\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--dbs\"", ",", "required", "=", "False", ",", "default", "=", "False", ",", "action", "=", "\"store_true\"", ",", "help", "=", "\"distribute batch size. If \"", "\n", "\"True then whatever \"", "\n", "\"batch_size is in plans will \"", "\n", "\"be distributed over DDP \"", "\n", "\"models, if False then each \"", "\n", "\"model will have batch_size \"", "\n", "\"for a total of \"", "\n", "\"GPUs*batch_size\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--npz\"", ",", "required", "=", "False", ",", "default", "=", "False", ",", "action", "=", "\"store_true\"", ",", "help", "=", "\"if set then nnUNet will \"", "\n", "\"export npz files of \"", "\n", "\"predicted segmentations \"", "\n", "\"in the vlaidation as well. \"", "\n", "\"This is needed to run the \"", "\n", "\"ensembling step so unless \"", "\n", "\"you are developing nnUNet \"", "\n", "\"you should enable this\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--valbest\"", ",", "required", "=", "False", ",", "default", "=", "False", ",", "action", "=", "\"store_true\"", ",", "help", "=", "\"\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--find_lr\"", ",", "required", "=", "False", ",", "default", "=", "False", ",", "action", "=", "\"store_true\"", ",", "help", "=", "\"\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--val_folder\"", ",", "required", "=", "False", ",", "default", "=", "\"validation_raw\"", ",", "\n", "help", "=", "\"name of the validation folder. No need to use this for most people\"", ")", "\n", "# parser.add_argument(\"--interp_order\", required=False, default=3, type=int,", "\n", "#                     help=\"order of interpolation for segmentations. Testing purpose only. Hands off\")", "\n", "# parser.add_argument(\"--interp_order_z\", required=False, default=0, type=int,", "\n", "#                     help=\"order of interpolation along z if z is resampled separately. Testing purpose only. \"", "\n", "#                          \"Hands off\")", "\n", "# parser.add_argument(\"--force_separate_z\", required=False, default=\"None\", type=str,", "\n", "#                     help=\"force_separate_z resampling. Can be None, True or False. Testing purpose only. Hands off\")", "\n", "\n", "args", "=", "parser", ".", "parse_args", "(", ")", "\n", "\n", "task", "=", "args", ".", "task", "\n", "fold", "=", "args", ".", "fold", "\n", "network", "=", "args", ".", "network", "\n", "network_trainer", "=", "args", ".", "network_trainer", "\n", "validation_only", "=", "args", ".", "validation_only", "\n", "plans_identifier", "=", "args", ".", "p", "\n", "use_compressed_data", "=", "args", ".", "use_compressed_data", "\n", "decompress_data", "=", "not", "use_compressed_data", "\n", "deterministic", "=", "args", ".", "deterministic", "\n", "valbest", "=", "args", ".", "valbest", "\n", "find_lr", "=", "args", ".", "find_lr", "\n", "val_folder", "=", "args", ".", "val_folder", "\n", "# interp_order = args.interp_order", "\n", "# interp_order_z = args.interp_order_z", "\n", "# force_separate_z = args.force_separate_z", "\n", "fp32", "=", "args", ".", "fp32", "\n", "\n", "if", "not", "task", ".", "startswith", "(", "\"Task\"", ")", ":", "\n", "        ", "task_id", "=", "int", "(", "task", ")", "\n", "task", "=", "convert_id_to_task_name", "(", "task_id", ")", "\n", "\n", "", "if", "fold", "==", "'all'", ":", "\n", "        ", "pass", "\n", "", "else", ":", "\n", "        ", "fold", "=", "int", "(", "fold", ")", "\n", "#", "\n", "# if force_separate_z == \"None\":", "\n", "#     force_separate_z = None", "\n", "# elif force_separate_z == \"False\":", "\n", "#     force_separate_z = False", "\n", "# elif force_separate_z == \"True\":", "\n", "#     force_separate_z = True", "\n", "# else:", "\n", "#     raise ValueError(\"force_separate_z must be None, True or False. Given: %s\" % force_separate_z)", "\n", "\n", "", "plans_file", ",", "output_folder_name", ",", "dataset_directory", ",", "batch_dice", ",", "stage", ",", "trainer_class", "=", "get_default_configuration", "(", "network", ",", "task", ",", "network_trainer", ",", "plans_identifier", ")", "\n", "\n", "if", "trainer_class", "is", "None", ":", "\n", "        ", "raise", "RuntimeError", "(", "\"Could not find trainer class in meddec.model_training\"", ")", "\n", "\n", "", "if", "network", "==", "\"3d_cascade_fullres\"", ":", "\n", "        ", "assert", "issubclass", "(", "trainer_class", ",", "(", "nnUNetTrainerCascadeFullRes", ",", "nnUNetTrainerV2CascadeFullRes", ")", ")", ",", "\"If running 3d_cascade_fullres then your \"", "\"trainer class must be derived from \"", "\"nnUNetTrainerCascadeFullRes\"", "\n", "", "else", ":", "\n", "        ", "assert", "issubclass", "(", "trainer_class", ",", "\n", "nnUNetTrainer", ")", ",", "\"network_trainer was found but is not derived from nnUNetTrainer\"", "\n", "\n", "", "trainer", "=", "trainer_class", "(", "plans_file", ",", "fold", ",", "local_rank", "=", "args", ".", "local_rank", ",", "output_folder", "=", "output_folder_name", ",", "\n", "dataset_directory", "=", "dataset_directory", ",", "batch_dice", "=", "batch_dice", ",", "stage", "=", "stage", ",", "\n", "unpack_data", "=", "decompress_data", ",", "deterministic", "=", "deterministic", ",", "fp16", "=", "not", "fp32", ",", "\n", "distribute_batch_size", "=", "args", ".", "dbs", ")", "\n", "\n", "trainer", ".", "initialize", "(", "not", "validation_only", ")", "\n", "\n", "if", "find_lr", ":", "\n", "        ", "trainer", ".", "find_lr", "(", ")", "\n", "", "else", ":", "\n", "        ", "if", "not", "validation_only", ":", "\n", "            ", "if", "args", ".", "continue_training", ":", "\n", "                ", "trainer", ".", "load_latest_checkpoint", "(", ")", "\n", "", "trainer", ".", "run_training", "(", ")", "\n", "", "else", ":", "\n", "            ", "if", "valbest", ":", "\n", "                ", "trainer", ".", "load_best_checkpoint", "(", "train", "=", "False", ")", "\n", "", "else", ":", "\n", "                ", "trainer", ".", "load_latest_checkpoint", "(", "train", "=", "False", ")", "\n", "\n", "", "", "trainer", ".", "network", ".", "eval", "(", ")", "\n", "\n", "# predict validation", "\n", "trainer", ".", "validate", "(", "save_softmax", "=", "args", ".", "npz", ",", "validation_folder_name", "=", "val_folder", ")", "\n", "\n", "if", "network", "==", "'3d_lowres'", ":", "\n", "            ", "trainer", ".", "load_best_checkpoint", "(", "False", ")", "\n", "print", "(", "\"predicting segmentations for the next stage of the cascade\"", ")", "\n", "predict_next_stage", "(", "trainer", ",", "join", "(", "dataset_directory", ",", "trainer", ".", "plans", "[", "'data_identifier'", "]", "+", "\"_stage%d\"", "%", "1", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.run.default_configuration.get_configuration_from_output_folder": [[23, 32], ["folder.startswith", "folder.split", "trainer_and_plans_identifier.split", "len"], "function", ["None"], ["def", "get_configuration_from_output_folder", "(", "folder", ")", ":", "\n", "# split off network_training_output_dir", "\n", "    ", "folder", "=", "folder", "[", "len", "(", "network_training_output_dir", ")", ":", "]", "\n", "if", "folder", ".", "startswith", "(", "\"/\"", ")", ":", "\n", "        ", "folder", "=", "folder", "[", "1", ":", "]", "\n", "\n", "", "configuration", ",", "task", ",", "trainer_and_plans_identifier", "=", "folder", ".", "split", "(", "\"/\"", ")", "\n", "trainer", ",", "plans_identifier", "=", "trainer_and_plans_identifier", ".", "split", "(", "\"__\"", ")", "\n", "return", "configuration", ",", "task", ",", "trainer", ",", "plans_identifier", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.run.default_configuration.get_default_configuration": [[34, 81], ["join", "load_pickle", "list", "nnunet.training.model_restore.recursive_find_python_class", "join", "print", "print", "print", "print", "nnunet.experiment_planning.summarize_plans.summarize_plans", "print", "print", "print", "join", "join", "plans[].keys", "RuntimeError", "print", "print", "join", "len", "join", "len"], "function", ["home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.training.model_restore.recursive_find_python_class", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.experiment_planning.summarize_plans.summarize_plans"], ["", "def", "get_default_configuration", "(", "network", ",", "task", ",", "network_trainer", ",", "plans_identifier", "=", "default_plans_identifier", ",", "\n", "search_in", "=", "(", "nnunet", ".", "__path__", "[", "0", "]", ",", "\"training\"", ",", "\"network_training\"", ")", ",", "\n", "base_module", "=", "'nnunet.training.network_training'", ")", ":", "\n", "    ", "assert", "network", "in", "[", "'2d'", ",", "'3d_lowres'", ",", "'3d_fullres'", ",", "'3d_cascade_fullres'", "]", ",", "\"network can only be one of the following: \\'3d\\', \\'3d_lowres\\', \\'3d_fullres\\', \\'3d_cascade_fullres\\'\"", "\n", "\n", "dataset_directory", "=", "join", "(", "preprocessing_output_dir", ",", "task", ")", "\n", "\n", "if", "network", "==", "'2d'", ":", "\n", "        ", "plans_file", "=", "join", "(", "preprocessing_output_dir", ",", "task", ",", "plans_identifier", "+", "\"_plans_2D.pkl\"", ")", "\n", "", "else", ":", "\n", "        ", "plans_file", "=", "join", "(", "preprocessing_output_dir", ",", "task", ",", "plans_identifier", "+", "\"_plans_3D.pkl\"", ")", "\n", "\n", "", "plans", "=", "load_pickle", "(", "plans_file", ")", "\n", "possible_stages", "=", "list", "(", "plans", "[", "'plans_per_stage'", "]", ".", "keys", "(", ")", ")", "\n", "domain", "=", "plans", "[", "'modalities'", "]", "[", "0", "]", "\n", "if", "(", "network", "==", "'3d_cascade_fullres'", "or", "network", "==", "\"3d_lowres\"", ")", "and", "len", "(", "possible_stages", ")", "==", "1", ":", "\n", "        ", "raise", "RuntimeError", "(", "\"3d_lowres/3d_cascade_fullres only applies if there is more than one stage. This task does \"", "\n", "\"not require the cascade. Run 3d_fullres instead\"", ")", "\n", "\n", "", "if", "network", "==", "'2d'", "or", "network", "==", "\"3d_lowres\"", ":", "\n", "        ", "stage", "=", "0", "\n", "", "else", ":", "\n", "        ", "stage", "=", "possible_stages", "[", "-", "1", "]", "\n", "\n", "", "trainer_class", "=", "recursive_find_python_class", "(", "[", "join", "(", "*", "search_in", ")", "]", ",", "network_trainer", ",", "\n", "current_module", "=", "base_module", ")", "\n", "\n", "output_folder_name", "=", "join", "(", "network_training_output_dir", ",", "network", ",", "task", ",", "network_trainer", "+", "\"__\"", "+", "plans_identifier", ")", "\n", "\n", "print", "(", "\"###############################################\"", ")", "\n", "print", "(", "\"I am running the following nnUNet: %s\"", "%", "network", ")", "\n", "print", "(", "\"My trainer class is: \"", ",", "trainer_class", ")", "\n", "print", "(", "\"For that I will be using the following configuration:\"", ")", "\n", "summarize_plans", "(", "plans_file", ")", "\n", "print", "(", "\"I am using stage %d from these plans\"", "%", "stage", ")", "\n", "\n", "if", "(", "network", "==", "'2d'", "or", "len", "(", "possible_stages", ")", ">", "1", ")", "and", "not", "network", "==", "'3d_lowres'", ":", "\n", "        ", "batch_dice", "=", "True", "\n", "print", "(", "\"I am using batch dice + CE loss\"", ")", "\n", "", "else", ":", "\n", "        ", "batch_dice", "=", "False", "\n", "print", "(", "\"I am using sample dice + CE loss\"", ")", "\n", "\n", "", "print", "(", "\"\\nI am using data from this folder: \"", ",", "join", "(", "dataset_directory", ",", "plans", "[", "'data_identifier'", "]", ")", ")", "\n", "print", "(", "\"###############################################\"", ")", "\n", "return", "plans_file", ",", "output_folder_name", ",", "dataset_directory", ",", "batch_dice", ",", "stage", ",", "trainer_class", ",", "domain", "\n", "", ""]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.run.run_training_DP.main": [[26, 161], ["argparse.ArgumentParser", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.parse_args", "nnunet.run.default_configuration.get_default_configuration", "trainer_class", "trainer_class.initialize", "nnunet.utilities.task_name_id_conversion.convert_id_to_task_name.startswith", "int", "nnunet.utilities.task_name_id_conversion.convert_id_to_task_name", "int", "RuntimeError", "issubclass", "issubclass", "trainer_class.find_lr", "trainer_class.network.eval", "trainer_class.validate", "trainer_class.run_training", "trainer_class.load_best_checkpoint", "print", "nnunet.training.cascade_stuff.predict_next_stage.predict_next_stage", "trainer_class.load_latest_checkpoint", "trainer_class.load_best_checkpoint", "trainer_class.load_latest_checkpoint", "join"], "function", ["home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.run.default_configuration.get_default_configuration", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.BraTS_trainer.nnUNetTrainerV2BraTSRegions.nnUNetTrainerV2BraTSRegions_DA3_BN.initialize", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.utilities.task_name_id_conversion.convert_id_to_task_name", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.network_training.network_trainer.NetworkTrainer.find_lr", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.BraTS_trainer.nnUNetTrainerV2BraTSRegions.nnUNetTrainerV2BraTSRegions_DA3_BN.validate", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.architectural_variants.nnUNetTrainerV2_ResencUNet.nnUNetTrainerV2_ResencUNet.run_training", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.network_training.network_trainer.NetworkTrainer.load_best_checkpoint", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.cascade_stuff.predict_next_stage.predict_next_stage", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.network_training.network_trainer.NetworkTrainer.load_latest_checkpoint", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.network_training.network_trainer.NetworkTrainer.load_best_checkpoint", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.network_training.network_trainer.NetworkTrainer.load_latest_checkpoint"], ["def", "main", "(", ")", ":", "\n", "    ", "parser", "=", "argparse", ".", "ArgumentParser", "(", ")", "\n", "parser", ".", "add_argument", "(", "\"network\"", ")", "\n", "parser", ".", "add_argument", "(", "\"network_trainer\"", ")", "\n", "parser", ".", "add_argument", "(", "\"task\"", ",", "help", "=", "\"can be task name or task id\"", ")", "\n", "parser", ".", "add_argument", "(", "\"fold\"", ",", "help", "=", "'0, 1, ..., 5 or \\'all\\''", ")", "\n", "parser", ".", "add_argument", "(", "\"-val\"", ",", "\"--validation_only\"", ",", "help", "=", "\"use this if you want to only run the validation\"", ",", "\n", "action", "=", "\"store_true\"", ")", "\n", "parser", ".", "add_argument", "(", "\"-c\"", ",", "\"--continue_training\"", ",", "help", "=", "\"use this if you want to continue a training\"", ",", "\n", "action", "=", "\"store_true\"", ")", "\n", "parser", ".", "add_argument", "(", "\"-p\"", ",", "help", "=", "\"plans identifier. Only change this if you created a custom experiment planner\"", ",", "\n", "default", "=", "default_plans_identifier", ",", "required", "=", "False", ")", "\n", "parser", ".", "add_argument", "(", "\"--use_compressed_data\"", ",", "default", "=", "False", ",", "action", "=", "\"store_true\"", ",", "\n", "help", "=", "\"If you set use_compressed_data, the training cases will not be decompressed. Reading compressed data \"", "\n", "\"is much more CPU and RAM intensive and should only be used if you know what you are \"", "\n", "\"doing\"", ",", "required", "=", "False", ")", "\n", "parser", ".", "add_argument", "(", "\"--deterministic\"", ",", "\n", "help", "=", "\"Makes training deterministic, but reduces training speed substantially. I (Fabian) think \"", "\n", "\"this is not necessary. Deterministic training will make you overfit to some random seed. \"", "\n", "\"Don't use that.\"", ",", "\n", "required", "=", "False", ",", "default", "=", "False", ",", "action", "=", "\"store_true\"", ")", "\n", "parser", ".", "add_argument", "(", "\"-gpus\"", ",", "help", "=", "\"number of gpus\"", ",", "required", "=", "True", ",", "type", "=", "int", ")", "\n", "parser", ".", "add_argument", "(", "\"--dbs\"", ",", "required", "=", "False", ",", "default", "=", "False", ",", "action", "=", "\"store_true\"", ",", "help", "=", "\"distribute batch size. If \"", "\n", "\"True then whatever \"", "\n", "\"batch_size is in plans will \"", "\n", "\"be distributed over DDP \"", "\n", "\"models, if False then each \"", "\n", "\"model will have batch_size \"", "\n", "\"for a total of \"", "\n", "\"GPUs*batch_size\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--npz\"", ",", "required", "=", "False", ",", "default", "=", "False", ",", "action", "=", "\"store_true\"", ",", "help", "=", "\"if set then nnUNet will \"", "\n", "\"export npz files of \"", "\n", "\"predicted segmentations \"", "\n", "\"in the vlaidation as well. \"", "\n", "\"This is needed to run the \"", "\n", "\"ensembling step so unless \"", "\n", "\"you are developing nnUNet \"", "\n", "\"you should enable this\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--valbest\"", ",", "required", "=", "False", ",", "default", "=", "False", ",", "action", "=", "\"store_true\"", ",", "help", "=", "\"\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--find_lr\"", ",", "required", "=", "False", ",", "default", "=", "False", ",", "action", "=", "\"store_true\"", ",", "help", "=", "\"\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--fp32\"", ",", "required", "=", "False", ",", "default", "=", "False", ",", "action", "=", "\"store_true\"", ",", "\n", "help", "=", "\"disable mixed precision training and run old school fp32\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--val_folder\"", ",", "required", "=", "False", ",", "default", "=", "\"validation_raw\"", ",", "\n", "help", "=", "\"name of the validation folder. No need to use this for most people\"", ")", "\n", "# parser.add_argument(\"--interp_order\", required=False, default=3, type=int,", "\n", "#                     help=\"order of interpolation for segmentations. Testing purpose only. Hands off\")", "\n", "# parser.add_argument(\"--interp_order_z\", required=False, default=0, type=int,", "\n", "#                     help=\"order of interpolation along z if z is resampled separately. Testing purpose only. \"", "\n", "#                          \"Hands off\")", "\n", "# parser.add_argument(\"--force_separate_z\", required=False, default=\"None\", type=str,", "\n", "#                     help=\"force_separate_z resampling. Can be None, True or False. Testing purpose only. Hands off\")", "\n", "\n", "args", "=", "parser", ".", "parse_args", "(", ")", "\n", "\n", "task", "=", "args", ".", "task", "\n", "fold", "=", "args", ".", "fold", "\n", "network", "=", "args", ".", "network", "\n", "network_trainer", "=", "args", ".", "network_trainer", "\n", "validation_only", "=", "args", ".", "validation_only", "\n", "plans_identifier", "=", "args", ".", "p", "\n", "\n", "use_compressed_data", "=", "args", ".", "use_compressed_data", "\n", "decompress_data", "=", "not", "use_compressed_data", "\n", "\n", "deterministic", "=", "args", ".", "deterministic", "\n", "valbest", "=", "args", ".", "valbest", "\n", "find_lr", "=", "args", ".", "find_lr", "\n", "num_gpus", "=", "args", ".", "gpus", "\n", "fp32", "=", "args", ".", "fp32", "\n", "val_folder", "=", "args", ".", "val_folder", "\n", "# interp_order = args.interp_order", "\n", "# interp_order_z = args.interp_order_z", "\n", "# force_separate_z = args.force_separate_z", "\n", "\n", "if", "not", "task", ".", "startswith", "(", "\"Task\"", ")", ":", "\n", "        ", "task_id", "=", "int", "(", "task", ")", "\n", "task", "=", "convert_id_to_task_name", "(", "task_id", ")", "\n", "\n", "", "if", "fold", "==", "'all'", ":", "\n", "        ", "pass", "\n", "", "else", ":", "\n", "        ", "fold", "=", "int", "(", "fold", ")", "\n", "\n", "# if force_separate_z == \"None\":", "\n", "#     force_separate_z = None", "\n", "# elif force_separate_z == \"False\":", "\n", "#     force_separate_z = False", "\n", "# elif force_separate_z == \"True\":", "\n", "#     force_separate_z = True", "\n", "# else:", "\n", "#     raise ValueError(\"force_separate_z must be None, True or False. Given: %s\" % force_separate_z)", "\n", "\n", "", "plans_file", ",", "output_folder_name", ",", "dataset_directory", ",", "batch_dice", ",", "stage", ",", "trainer_class", "=", "get_default_configuration", "(", "network", ",", "task", ",", "network_trainer", ",", "plans_identifier", ")", "\n", "\n", "if", "trainer_class", "is", "None", ":", "\n", "        ", "raise", "RuntimeError", "(", "\"Could not find trainer class\"", ")", "\n", "\n", "", "if", "network", "==", "\"3d_cascade_fullres\"", ":", "\n", "        ", "assert", "issubclass", "(", "trainer_class", ",", "nnUNetTrainerCascadeFullRes", ")", ",", "\"If running 3d_cascade_fullres then your \"", "\"trainer class must be derived from \"", "\"nnUNetTrainerCascadeFullRes\"", "\n", "", "else", ":", "\n", "        ", "assert", "issubclass", "(", "trainer_class", ",", "nnUNetTrainer", ")", ",", "\"network_trainer was found but is not derived from \"", "\"nnUNetTrainer\"", "\n", "\n", "", "trainer", "=", "trainer_class", "(", "plans_file", ",", "fold", ",", "output_folder", "=", "output_folder_name", ",", "\n", "dataset_directory", "=", "dataset_directory", ",", "batch_dice", "=", "batch_dice", ",", "stage", "=", "stage", ",", "\n", "unpack_data", "=", "decompress_data", ",", "deterministic", "=", "deterministic", ",", "\n", "distribute_batch_size", "=", "args", ".", "dbs", ",", "num_gpus", "=", "num_gpus", ",", "fp16", "=", "not", "fp32", ")", "\n", "\n", "trainer", ".", "initialize", "(", "not", "validation_only", ")", "\n", "\n", "if", "find_lr", ":", "\n", "        ", "trainer", ".", "find_lr", "(", ")", "\n", "", "else", ":", "\n", "        ", "if", "not", "validation_only", ":", "\n", "            ", "if", "args", ".", "continue_training", ":", "\n", "                ", "trainer", ".", "load_latest_checkpoint", "(", ")", "\n", "", "trainer", ".", "run_training", "(", ")", "\n", "", "else", ":", "\n", "            ", "if", "valbest", ":", "\n", "                ", "trainer", ".", "load_best_checkpoint", "(", "train", "=", "False", ")", "\n", "", "else", ":", "\n", "                ", "trainer", ".", "load_latest_checkpoint", "(", "train", "=", "False", ")", "\n", "\n", "", "", "trainer", ".", "network", ".", "eval", "(", ")", "\n", "\n", "# predict validation", "\n", "trainer", ".", "validate", "(", "save_softmax", "=", "args", ".", "npz", ",", "validation_folder_name", "=", "val_folder", ")", "\n", "\n", "if", "network", "==", "'3d_lowres'", ":", "\n", "            ", "trainer", ".", "load_best_checkpoint", "(", "False", ")", "\n", "print", "(", "\"predicting segmentations for the next stage of the cascade\"", ")", "\n", "predict_next_stage", "(", "trainer", ",", "join", "(", "dataset_directory", ",", "trainer", ".", "plans", "[", "'data_identifier'", "]", "+", "\"_stage%d\"", "%", "1", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.run.run_training.main": [[27, 164], ["argparse.ArgumentParser", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.parse_args", "nnunet.run.default_configuration.get_default_configuration", "trainer_class", "trainer_class.initialize", "sys.stdout.flush", "nnunet.utilities.task_name_id_conversion.convert_id_to_task_name.startswith", "int", "nnunet.utilities.task_name_id_conversion.convert_id_to_task_name", "int", "RuntimeError", "issubclass", "issubclass", "trainer_class.load_pretrained_encoder_weights", "trainer_class.find_lr", "trainer_class.network.eval", "trainer_class.validate", "trainer_class.run_training", "trainer_class.load_best_checkpoint", "print", "nnunet.training.cascade_stuff.predict_next_stage.predict_next_stage", "trainer_class.load_latest_checkpoint", "trainer_class.load_best_checkpoint", "trainer_class.load_latest_checkpoint", "join"], "function", ["home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.run.default_configuration.get_default_configuration", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.BraTS_trainer.nnUNetTrainerV2BraTSRegions.nnUNetTrainerV2BraTSRegions_DA3_BN.initialize", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.utilities.task_name_id_conversion.convert_id_to_task_name", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.network_training.network_trainer.NetworkTrainer.load_pretrained_encoder_weights", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.network_training.network_trainer.NetworkTrainer.find_lr", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.BraTS_trainer.nnUNetTrainerV2BraTSRegions.nnUNetTrainerV2BraTSRegions_DA3_BN.validate", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.architectural_variants.nnUNetTrainerV2_ResencUNet.nnUNetTrainerV2_ResencUNet.run_training", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.network_training.network_trainer.NetworkTrainer.load_best_checkpoint", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.cascade_stuff.predict_next_stage.predict_next_stage", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.network_training.network_trainer.NetworkTrainer.load_latest_checkpoint", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.network_training.network_trainer.NetworkTrainer.load_best_checkpoint", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.network_training.network_trainer.NetworkTrainer.load_latest_checkpoint"], ["def", "main", "(", ")", ":", "\n", "    ", "parser", "=", "argparse", ".", "ArgumentParser", "(", ")", "\n", "parser", ".", "add_argument", "(", "\"network\"", ")", "\n", "parser", ".", "add_argument", "(", "\"network_trainer\"", ")", "\n", "parser", ".", "add_argument", "(", "\"task\"", ",", "help", "=", "\"can be task name or task id\"", ")", "\n", "parser", ".", "add_argument", "(", "\"fold\"", ",", "help", "=", "'0, 1, ..., 5 or \\'all\\''", ")", "\n", "parser", ".", "add_argument", "(", "\"-val\"", ",", "\"--validation_only\"", ",", "help", "=", "\"use this if you want to only run the validation\"", ",", "\n", "action", "=", "\"store_true\"", ")", "\n", "parser", ".", "add_argument", "(", "\"-w\"", ",", "required", "=", "False", ",", "default", "=", "None", ",", "help", "=", "\"Load pre-trained Models Genesis\"", ")", "\n", "parser", ".", "add_argument", "(", "\"-c\"", ",", "\"--continue_training\"", ",", "help", "=", "\"use this if you want to continue a training\"", ",", "\n", "action", "=", "\"store_true\"", ")", "\n", "parser", ".", "add_argument", "(", "\"-p\"", ",", "help", "=", "\"plans identifier. Only change this if you created a custom experiment planner\"", ",", "\n", "default", "=", "default_plans_identifier", ",", "required", "=", "False", ")", "\n", "parser", ".", "add_argument", "(", "\"--use_compressed_data\"", ",", "default", "=", "False", ",", "action", "=", "\"store_true\"", ",", "\n", "help", "=", "\"If you set use_compressed_data, the training cases will not be decompressed. Reading compressed data \"", "\n", "\"is much more CPU and RAM intensive and should only be used if you know what you are \"", "\n", "\"doing\"", ",", "required", "=", "False", ")", "\n", "parser", ".", "add_argument", "(", "\"--deterministic\"", ",", "\n", "help", "=", "\"Makes training deterministic, but reduces training speed substantially. I (Fabian) think \"", "\n", "\"this is not necessary. Deterministic training will make you overfit to some random seed. \"", "\n", "\"Don't use that.\"", ",", "\n", "required", "=", "False", ",", "default", "=", "False", ",", "action", "=", "\"store_true\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--npz\"", ",", "required", "=", "False", ",", "default", "=", "False", ",", "action", "=", "\"store_true\"", ",", "help", "=", "\"if set then nnUNet will \"", "\n", "\"export npz files of \"", "\n", "\"predicted segmentations \"", "\n", "\"in the validation as well. \"", "\n", "\"This is needed to run the \"", "\n", "\"ensembling step so unless \"", "\n", "\"you are developing nnUNet \"", "\n", "\"you should enable this\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--find_lr\"", ",", "required", "=", "False", ",", "default", "=", "False", ",", "action", "=", "\"store_true\"", ",", "\n", "help", "=", "\"not used here, just for fun\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--valbest\"", ",", "required", "=", "False", ",", "default", "=", "False", ",", "action", "=", "\"store_true\"", ",", "\n", "help", "=", "\"hands off. This is not intended to be used\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--fp32\"", ",", "required", "=", "False", ",", "default", "=", "False", ",", "action", "=", "\"store_true\"", ",", "\n", "help", "=", "\"disable mixed precision training and run old school fp32\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--val_folder\"", ",", "required", "=", "False", ",", "default", "=", "\"validation_raw\"", ",", "\n", "help", "=", "\"name of the validation folder. No need to use this for most people\"", ")", "\n", "# parser.add_argument(\"--interp_order\", required=False, default=3, type=int,", "\n", "#                     help=\"order of interpolation for segmentations. Testing purpose only. Hands off\")", "\n", "# parser.add_argument(\"--interp_order_z\", required=False, default=0, type=int,", "\n", "#                     help=\"order of interpolation along z if z is resampled separately. Testing purpose only. \"", "\n", "#                          \"Hands off\")", "\n", "# parser.add_argument(\"--force_separate_z\", required=False, default=\"None\", type=str,", "\n", "#                     help=\"force_separate_z resampling. Can be None, True or False. Testing purpose only. Hands off\")", "\n", "\n", "args", "=", "parser", ".", "parse_args", "(", ")", "\n", "\n", "task", "=", "args", ".", "task", "\n", "fold", "=", "args", ".", "fold", "\n", "network", "=", "args", ".", "network", "\n", "network_trainer", "=", "args", ".", "network_trainer", "\n", "weights", "=", "args", ".", "w", "\n", "validation_only", "=", "args", ".", "validation_only", "\n", "plans_identifier", "=", "args", ".", "p", "\n", "find_lr", "=", "args", ".", "find_lr", "\n", "\n", "use_compressed_data", "=", "args", ".", "use_compressed_data", "\n", "decompress_data", "=", "not", "use_compressed_data", "\n", "\n", "deterministic", "=", "args", ".", "deterministic", "\n", "valbest", "=", "args", ".", "valbest", "\n", "\n", "fp32", "=", "args", ".", "fp32", "\n", "run_mixed_precision", "=", "not", "fp32", "\n", "\n", "val_folder", "=", "args", ".", "val_folder", "\n", "# interp_order = args.interp_order", "\n", "# interp_order_z = args.interp_order_z", "\n", "# force_separate_z = args.force_separate_z", "\n", "\n", "if", "not", "task", ".", "startswith", "(", "\"Task\"", ")", ":", "\n", "        ", "task_id", "=", "int", "(", "task", ")", "\n", "task", "=", "convert_id_to_task_name", "(", "task_id", ")", "\n", "\n", "", "if", "fold", "==", "'all'", ":", "\n", "        ", "pass", "\n", "", "else", ":", "\n", "        ", "fold", "=", "int", "(", "fold", ")", "\n", "\n", "# if force_separate_z == \"None\":", "\n", "#     force_separate_z = None", "\n", "# elif force_separate_z == \"False\":", "\n", "#     force_separate_z = False", "\n", "# elif force_separate_z == \"True\":", "\n", "#     force_separate_z = True", "\n", "# else:", "\n", "#     raise ValueError(\"force_separate_z must be None, True or False. Given: %s\" % force_separate_z)", "\n", "\n", "", "plans_file", ",", "output_folder_name", ",", "dataset_directory", ",", "batch_dice", ",", "stage", ",", "trainer_class", ",", "domain", "=", "get_default_configuration", "(", "network", ",", "task", ",", "network_trainer", ",", "plans_identifier", ")", "\n", "\n", "if", "trainer_class", "is", "None", ":", "\n", "        ", "raise", "RuntimeError", "(", "\"Could not find trainer class in nnunet.training.network_training\"", ")", "\n", "\n", "", "if", "network", "==", "\"3d_cascade_fullres\"", ":", "\n", "        ", "assert", "issubclass", "(", "trainer_class", ",", "(", "nnUNetTrainerCascadeFullRes", ",", "nnUNetTrainerV2CascadeFullRes", ")", ")", ",", "\"If running 3d_cascade_fullres then your \"", "\"trainer class must be derived from \"", "\"nnUNetTrainerCascadeFullRes\"", "\n", "", "else", ":", "\n", "        ", "assert", "issubclass", "(", "trainer_class", ",", "\n", "nnUNetTrainer", ")", ",", "\"network_trainer was found but is not derived from nnUNetTrainer\"", "\n", "\n", "", "trainer", "=", "trainer_class", "(", "plans_file", ",", "fold", ",", "output_folder", "=", "output_folder_name", ",", "dataset_directory", "=", "dataset_directory", ",", "\n", "batch_dice", "=", "batch_dice", ",", "stage", "=", "stage", ",", "unpack_data", "=", "decompress_data", ",", "\n", "deterministic", "=", "deterministic", ",", "\n", "fp16", "=", "run_mixed_precision", ")", "\n", "\n", "trainer", ".", "initialize", "(", "not", "validation_only", ")", "\n", "\n", "if", "weights", "!=", "None", ":", "\n", "        ", "trainer", ".", "load_pretrained_encoder_weights", "(", "weights", ")", "\n", "", "sys", ".", "stdout", ".", "flush", "(", ")", "\n", "\n", "if", "find_lr", ":", "\n", "        ", "trainer", ".", "find_lr", "(", ")", "\n", "", "else", ":", "\n", "        ", "if", "not", "validation_only", ":", "\n", "            ", "if", "args", ".", "continue_training", ":", "\n", "                ", "trainer", ".", "load_latest_checkpoint", "(", ")", "\n", "", "trainer", ".", "run_training", "(", ")", "\n", "", "else", ":", "\n", "            ", "if", "valbest", ":", "\n", "                ", "trainer", ".", "load_best_checkpoint", "(", "train", "=", "False", ")", "\n", "", "else", ":", "\n", "                ", "trainer", ".", "load_latest_checkpoint", "(", "train", "=", "False", ")", "\n", "\n", "", "", "trainer", ".", "network", ".", "eval", "(", ")", "\n", "\n", "# predict validation", "\n", "trainer", ".", "validate", "(", "save_softmax", "=", "args", ".", "npz", ",", "validation_folder_name", "=", "val_folder", ")", "\n", "\n", "if", "network", "==", "'3d_lowres'", ":", "\n", "            ", "trainer", ".", "load_best_checkpoint", "(", "False", ")", "\n", "print", "(", "\"predicting segmentations for the next stage of the cascade\"", ")", "\n", "predict_next_stage", "(", "trainer", ",", "join", "(", "dataset_directory", ",", "trainer", ".", "plans", "[", "'data_identifier'", "]", "+", "\"_stage%d\"", "%", "1", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.training.model_restore.recursive_find_python_class": [[23, 42], ["pkgutil.iter_modules", "pkgutil.iter_modules", "importlib.import_module", "hasattr", "getattr", "model_restore.recursive_find_python_class", "join"], "function", ["home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.training.model_restore.recursive_find_python_class"], ["def", "recursive_find_python_class", "(", "folder", ",", "trainer_name", ",", "current_module", ")", ":", "\n", "    ", "tr", "=", "None", "\n", "for", "importer", ",", "modname", ",", "ispkg", "in", "pkgutil", ".", "iter_modules", "(", "folder", ")", ":", "\n", "# print(modname, ispkg)", "\n", "        ", "if", "not", "ispkg", ":", "\n", "            ", "m", "=", "importlib", ".", "import_module", "(", "current_module", "+", "\".\"", "+", "modname", ")", "\n", "if", "hasattr", "(", "m", ",", "trainer_name", ")", ":", "\n", "                ", "tr", "=", "getattr", "(", "m", ",", "trainer_name", ")", "\n", "break", "\n", "\n", "", "", "", "if", "tr", "is", "None", ":", "\n", "        ", "for", "importer", ",", "modname", ",", "ispkg", "in", "pkgutil", ".", "iter_modules", "(", "folder", ")", ":", "\n", "            ", "if", "ispkg", ":", "\n", "                ", "next_current_module", "=", "current_module", "+", "\".\"", "+", "modname", "\n", "tr", "=", "recursive_find_python_class", "(", "[", "join", "(", "folder", "[", "0", "]", ",", "modname", ")", "]", ",", "trainer_name", ",", "current_module", "=", "next_current_module", ")", "\n", "", "if", "tr", "is", "not", "None", ":", "\n", "                ", "break", "\n", "\n", "", "", "", "return", "tr", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.training.model_restore.restore_model": [[44, 101], ["load_pickle", "join", "model_restore.recursive_find_python_class", "issubclass", "recursive_find_python_class.", "tr.process_plans", "RuntimeError", "tr.load_checkpoint", "join", "model_restore.recursive_find_python_class"], "function", ["home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.training.model_restore.recursive_find_python_class", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.BraTS_trainer.nnUNetTrainerV2BraTSRegions.nnUNetTrainerV2BraTSRegions_DA3_BN.process_plans", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.network_training.network_trainer.NetworkTrainer.load_checkpoint", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.training.model_restore.recursive_find_python_class"], ["", "def", "restore_model", "(", "pkl_file", ",", "checkpoint", "=", "None", ",", "train", "=", "False", ",", "fp16", "=", "None", ")", ":", "\n", "    ", "\"\"\"\n    This is a utility function to load any nnUNet trainer from a pkl. It will recursively search\n    nnunet.trainig.network_training for the file that contains the trainer and instantiate it with the arguments saved in the pkl file. If checkpoint\n    is specified, it will furthermore load the checkpoint file in train/test mode (as specified by train).\n    The pkl file required here is the one that will be saved automatically when calling nnUNetTrainer.save_checkpoint.\n    :param pkl_file:\n    :param checkpoint:\n    :param train:\n    :param fp16: if None then we take no action. If True/False we overwrite what the model has in its init\n    :return:\n    \"\"\"", "\n", "info", "=", "load_pickle", "(", "pkl_file", ")", "\n", "init", "=", "info", "[", "'init'", "]", "\n", "name", "=", "info", "[", "'name'", "]", "\n", "search_in", "=", "join", "(", "nnunet", ".", "__path__", "[", "0", "]", ",", "\"training\"", ",", "\"network_training\"", ")", "\n", "tr", "=", "recursive_find_python_class", "(", "[", "search_in", "]", ",", "name", ",", "current_module", "=", "\"nnunet.training.network_training\"", ")", "\n", "\n", "if", "tr", "is", "None", ":", "\n", "        ", "\"\"\"\n        Fabian only. This will trigger searching for trainer classes in other repositories as well\n        \"\"\"", "\n", "try", ":", "\n", "            ", "import", "meddec", "\n", "search_in", "=", "join", "(", "meddec", ".", "__path__", "[", "0", "]", ",", "\"model_training\"", ")", "\n", "tr", "=", "recursive_find_python_class", "(", "[", "search_in", "]", ",", "name", ",", "current_module", "=", "\"meddec.model_training\"", ")", "\n", "", "except", "ImportError", ":", "\n", "            ", "pass", "\n", "\n", "", "", "if", "tr", "is", "None", ":", "\n", "        ", "raise", "RuntimeError", "(", "\"Could not find the model trainer specified in checkpoint in nnunet.trainig.network_training. If it \"", "\n", "\"is not located there, please move it or change the code of restore_model. Your model \"", "\n", "\"trainer can be located in any directory within nnunet.trainig.network_training (search is recursive).\"", "\n", "\"\\nDebug info: \\ncheckpoint file: %s\\nName of trainer: %s \"", "%", "(", "checkpoint", ",", "name", ")", ")", "\n", "", "assert", "issubclass", "(", "tr", ",", "nnUNetTrainer", ")", ",", "\"The network trainer was found but is not a subclass of nnUNetTrainer. \"", "\"Please make it so!\"", "\n", "\n", "# this is now deprecated", "\n", "\"\"\"if len(init) == 7:\n        print(\"warning: this model seems to have been saved with a previous version of nnUNet. Attempting to load it \"\n              \"anyways. Expect the unexpected.\")\n        print(\"manually editing init args...\")\n        init = [init[i] for i in range(len(init)) if i != 2]\"\"\"", "\n", "\n", "# ToDo Fabian make saves use kwargs, please...", "\n", "\n", "trainer", "=", "tr", "(", "*", "init", ")", "\n", "\n", "# We can hack fp16 overwriting into the trainer without changing the init arguments because nothing happens with", "\n", "# fp16 in the init, it just saves it to a member variable", "\n", "if", "fp16", "is", "not", "None", ":", "\n", "        ", "trainer", ".", "fp16", "=", "fp16", "\n", "\n", "", "trainer", ".", "process_plans", "(", "info", "[", "'plans'", "]", ")", "\n", "if", "checkpoint", "is", "not", "None", ":", "\n", "        ", "trainer", ".", "load_checkpoint", "(", "checkpoint", ",", "train", ")", "\n", "", "return", "trainer", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.training.model_restore.load_best_model_for_inference": [[103, 107], ["join", "model_restore.restore_model"], "function", ["home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.training.model_restore.restore_model"], ["", "def", "load_best_model_for_inference", "(", "folder", ")", ":", "\n", "    ", "checkpoint", "=", "join", "(", "folder", ",", "\"model_best.model\"", ")", "\n", "pkl_file", "=", "checkpoint", "+", "\".pkl\"", "\n", "return", "restore_model", "(", "pkl_file", ",", "checkpoint", ",", "False", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.training.model_restore.load_model_and_checkpoint_files": [[109, 149], ["isinstance", "model_restore.restore_model", "restore_model.update_fold", "restore_model.initialize", "print", "isdir", "isinstance", "join", "join", "torch.load", "join", "all", "isinstance", "all", "torch.device", "len", "join", "join", "isdir", "join", "print", "subfolders", "print", "ValueError", "isdir", "str", "type"], "function", ["home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.training.model_restore.restore_model", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.network_training.nnUNetTrainer.nnUNetTrainer.update_fold", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.BraTS_trainer.nnUNetTrainerV2BraTSRegions.nnUNetTrainerV2BraTSRegions_DA3_BN.initialize"], ["", "def", "load_model_and_checkpoint_files", "(", "folder", ",", "folds", "=", "None", ",", "mixed_precision", "=", "None", ",", "checkpoint_name", "=", "\"model_best\"", ")", ":", "\n", "    ", "\"\"\"\n    used for if you need to ensemble the five models of a cross-validation. This will restore the model from the\n    checkpoint in fold 0, load all parameters of the five folds in ram and return both. This will allow for fast\n    switching between parameters (as opposed to loading them form disk each time).\n\n    This is best used for inference and test prediction\n    :param folder:\n    :param folds:\n    :param mixed_precision: if None then we take no action. If True/False we overwrite what the model has in its init\n    :return:\n    \"\"\"", "\n", "if", "isinstance", "(", "folds", ",", "str", ")", ":", "\n", "        ", "folds", "=", "[", "join", "(", "folder", ",", "\"all\"", ")", "]", "\n", "assert", "isdir", "(", "folds", "[", "0", "]", ")", ",", "\"no output folder for fold %s found\"", "%", "folds", "\n", "", "elif", "isinstance", "(", "folds", ",", "(", "list", ",", "tuple", ")", ")", ":", "\n", "        ", "if", "len", "(", "folds", ")", "==", "1", "and", "folds", "[", "0", "]", "==", "\"all\"", ":", "\n", "            ", "folds", "=", "[", "join", "(", "folder", ",", "\"all\"", ")", "]", "\n", "", "else", ":", "\n", "            ", "folds", "=", "[", "join", "(", "folder", ",", "\"fold_%d\"", "%", "i", ")", "for", "i", "in", "folds", "]", "\n", "", "assert", "all", "(", "[", "isdir", "(", "i", ")", "for", "i", "in", "folds", "]", ")", ",", "\"list of folds specified but not all output folders are present\"", "\n", "", "elif", "isinstance", "(", "folds", ",", "int", ")", ":", "\n", "        ", "folds", "=", "[", "join", "(", "folder", ",", "\"fold_%d\"", "%", "folds", ")", "]", "\n", "assert", "all", "(", "[", "isdir", "(", "i", ")", "for", "i", "in", "folds", "]", ")", ",", "\"output folder missing for fold %d\"", "%", "folds", "\n", "", "elif", "folds", "is", "None", ":", "\n", "        ", "print", "(", "\"folds is None so we will automatically look for output folders (not using \\'all\\'!)\"", ")", "\n", "folds", "=", "subfolders", "(", "folder", ",", "prefix", "=", "\"fold\"", ")", "\n", "print", "(", "\"found the following folds: \"", ",", "folds", ")", "\n", "", "else", ":", "\n", "        ", "raise", "ValueError", "(", "\"Unknown value for folds. Type: %s. Expected: list of int, int, str or None\"", ",", "str", "(", "type", "(", "folds", ")", ")", ")", "\n", "\n", "", "trainer", "=", "restore_model", "(", "join", "(", "folds", "[", "0", "]", ",", "\"%s.model.pkl\"", "%", "checkpoint_name", ")", ",", "fp16", "=", "mixed_precision", ")", "\n", "trainer", ".", "output_folder", "=", "folder", "\n", "trainer", ".", "output_folder_base", "=", "folder", "\n", "trainer", ".", "update_fold", "(", "0", ")", "\n", "trainer", ".", "initialize", "(", "False", ")", "\n", "all_best_model_files", "=", "[", "join", "(", "i", ",", "\"%s.model\"", "%", "checkpoint_name", ")", "for", "i", "in", "folds", "]", "\n", "print", "(", "\"using the following model files: \"", ",", "all_best_model_files", ")", "\n", "all_params", "=", "[", "torch", ".", "load", "(", "i", ",", "map_location", "=", "torch", ".", "device", "(", "'cpu'", ")", ")", "for", "i", "in", "all_best_model_files", "]", "\n", "return", "trainer", ",", "all_params", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.cascade_stuff.predict_next_stage.resample_and_save": [[31, 44], ["isinstance", "nnunet.preprocessing.preprocessing.resample_data_or_seg", "nnunet.preprocessing.preprocessing.resample_data_or_seg.argmax", "numpy.savez_compressed", "isfile", "copy.deepcopy", "numpy.load", "os.remove", "predicted_new_shape.argmax.astype"], "function", ["home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.preprocessing.preprocessing.resample_data_or_seg"], ["def", "resample_and_save", "(", "predicted", ",", "target_shape", ",", "output_file", ",", "force_separate_z", "=", "False", ",", "\n", "interpolation_order", "=", "1", ",", "interpolation_order_z", "=", "0", ")", ":", "\n", "    ", "if", "isinstance", "(", "predicted", ",", "str", ")", ":", "\n", "        ", "assert", "isfile", "(", "predicted", ")", ",", "\"If isinstance(segmentation_softmax, str) then \"", "\"isfile(segmentation_softmax) must be True\"", "\n", "del_file", "=", "deepcopy", "(", "predicted", ")", "\n", "predicted", "=", "np", ".", "load", "(", "predicted", ")", "\n", "os", ".", "remove", "(", "del_file", ")", "\n", "\n", "", "predicted_new_shape", "=", "resample_data_or_seg", "(", "predicted", ",", "target_shape", ",", "False", ",", "order", "=", "interpolation_order", ",", "\n", "do_separate_z", "=", "force_separate_z", ",", "cval", "=", "0", ",", "order_z", "=", "interpolation_order_z", ")", "\n", "seg_new_shape", "=", "predicted_new_shape", ".", "argmax", "(", "0", ")", "\n", "np", ".", "savez_compressed", "(", "output_file", ",", "data", "=", "seg_new_shape", ".", "astype", "(", "np", ".", "uint8", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.cascade_stuff.predict_next_stage.predict_next_stage": [[46, 90], ["join", "batchgenerators.utilities.file_and_folder_operations.maybe_mkdir_p", "multiprocessing.Pool", "trainer.dataset_val.keys", "multiprocessing.Pool.close", "multiprocessing.Pool.join", "pardir", "trainer.plans.keys", "print", "join", "join", "results.append", "i.get", "trainer.predict_preprocessed_data_return_seg_and_softmax", "data_file.split", "numpy.load", "numpy.prod", "numpy.save", "multiprocessing.Pool.starmap_async", "numpy.load", "join.split"], "function", ["home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.architectural_variants.nnUNetTrainerV2_ResencUNet.nnUNetTrainerV2_ResencUNet.predict_preprocessed_data_return_seg_and_softmax"], ["", "def", "predict_next_stage", "(", "trainer", ",", "stage_to_be_predicted_folder", ")", ":", "\n", "    ", "output_folder", "=", "join", "(", "pardir", "(", "trainer", ".", "output_folder", ")", ",", "\"pred_next_stage\"", ")", "\n", "maybe_mkdir_p", "(", "output_folder", ")", "\n", "\n", "if", "'segmentation_export_params'", "in", "trainer", ".", "plans", ".", "keys", "(", ")", ":", "\n", "        ", "force_separate_z", "=", "trainer", ".", "plans", "[", "'segmentation_export_params'", "]", "[", "'force_separate_z'", "]", "\n", "interpolation_order", "=", "trainer", ".", "plans", "[", "'segmentation_export_params'", "]", "[", "'interpolation_order'", "]", "\n", "interpolation_order_z", "=", "trainer", ".", "plans", "[", "'segmentation_export_params'", "]", "[", "'interpolation_order_z'", "]", "\n", "", "else", ":", "\n", "        ", "force_separate_z", "=", "None", "\n", "interpolation_order", "=", "1", "\n", "interpolation_order_z", "=", "0", "\n", "\n", "", "export_pool", "=", "Pool", "(", "2", ")", "\n", "results", "=", "[", "]", "\n", "\n", "for", "pat", "in", "trainer", ".", "dataset_val", ".", "keys", "(", ")", ":", "\n", "        ", "print", "(", "pat", ")", "\n", "data_file", "=", "trainer", ".", "dataset_val", "[", "pat", "]", "[", "'data_file'", "]", "\n", "data_preprocessed", "=", "np", ".", "load", "(", "data_file", ")", "[", "'data'", "]", "[", ":", "-", "1", "]", "\n", "\n", "predicted_probabilities", "=", "trainer", ".", "predict_preprocessed_data_return_seg_and_softmax", "(", "data_preprocessed", ",", "\n", "trainer", ".", "data_aug_params", "[", "\n", "\"do_mirror\"", "]", ",", "\n", "trainer", ".", "data_aug_params", "[", "\n", "'mirror_axes'", "]", ",", "mixed_precision", "=", "trainer", ".", "fp16", ")", "[", "1", "]", "\n", "\n", "data_file_nofolder", "=", "data_file", ".", "split", "(", "\"/\"", ")", "[", "-", "1", "]", "\n", "data_file_nextstage", "=", "join", "(", "stage_to_be_predicted_folder", ",", "data_file_nofolder", ")", "\n", "data_nextstage", "=", "np", ".", "load", "(", "data_file_nextstage", ")", "[", "'data'", "]", "\n", "target_shp", "=", "data_nextstage", ".", "shape", "[", "1", ":", "]", "\n", "output_file", "=", "join", "(", "output_folder", ",", "data_file_nextstage", ".", "split", "(", "\"/\"", ")", "[", "-", "1", "]", "[", ":", "-", "4", "]", "+", "\"_segFromPrevStage.npz\"", ")", "\n", "\n", "if", "np", ".", "prod", "(", "predicted_probabilities", ".", "shape", ")", ">", "(", "2e9", "/", "4", "*", "0.85", ")", ":", "# *0.85 just to be save", "\n", "            ", "np", ".", "save", "(", "output_file", "[", ":", "-", "4", "]", "+", "\".npy\"", ",", "predicted_probabilities", ")", "\n", "predicted_probabilities", "=", "output_file", "[", ":", "-", "4", "]", "+", "\".npy\"", "\n", "\n", "", "results", ".", "append", "(", "export_pool", ".", "starmap_async", "(", "resample_and_save", ",", "[", "(", "predicted_probabilities", ",", "target_shp", ",", "output_file", ",", "\n", "force_separate_z", ",", "interpolation_order", ",", "\n", "interpolation_order_z", ")", "]", ")", ")", "\n", "\n", "", "_", "=", "[", "i", ".", "get", "(", ")", "for", "i", "in", "results", "]", "\n", "export_pool", ".", "close", "(", ")", "\n", "export_pool", ".", "join", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.data_augmentation.downsampling.DownsampleSegForDSTransform3.__init__": [[34, 39], ["None"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "ds_scales", "=", "(", "1", ",", "0.5", ",", "0.25", ")", ",", "input_key", "=", "\"seg\"", ",", "output_key", "=", "\"seg\"", ",", "classes", "=", "None", ")", ":", "\n", "        ", "self", ".", "classes", "=", "classes", "\n", "self", ".", "output_key", "=", "output_key", "\n", "self", ".", "input_key", "=", "input_key", "\n", "self", ".", "ds_scales", "=", "ds_scales", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.data_augmentation.downsampling.DownsampleSegForDSTransform3.__call__": [[40, 43], ["downsampling.downsample_seg_for_ds_transform3"], "methods", ["home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.data_augmentation.downsampling.downsample_seg_for_ds_transform3"], ["", "def", "__call__", "(", "self", ",", "**", "data_dict", ")", ":", "\n", "        ", "data_dict", "[", "self", ".", "output_key", "]", "=", "downsample_seg_for_ds_transform3", "(", "data_dict", "[", "self", ".", "input_key", "]", "[", ":", ",", "0", "]", ",", "self", ".", "ds_scales", ",", "self", ".", "classes", ")", "\n", "return", "data_dict", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.data_augmentation.downsampling.DownsampleSegForDSTransform2.__init__": [[74, 81], ["None"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "ds_scales", "=", "(", "1", ",", "0.5", ",", "0.25", ")", ",", "order", "=", "0", ",", "cval", "=", "0", ",", "input_key", "=", "\"seg\"", ",", "output_key", "=", "\"seg\"", ",", "axes", "=", "None", ")", ":", "\n", "        ", "self", ".", "axes", "=", "axes", "\n", "self", ".", "output_key", "=", "output_key", "\n", "self", ".", "input_key", "=", "input_key", "\n", "self", ".", "cval", "=", "cval", "\n", "self", ".", "order", "=", "order", "\n", "self", ".", "ds_scales", "=", "ds_scales", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.data_augmentation.downsampling.DownsampleSegForDSTransform2.__call__": [[82, 86], ["downsampling.downsample_seg_for_ds_transform2"], "methods", ["home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.data_augmentation.downsampling.downsample_seg_for_ds_transform2"], ["", "def", "__call__", "(", "self", ",", "**", "data_dict", ")", ":", "\n", "        ", "data_dict", "[", "self", ".", "output_key", "]", "=", "downsample_seg_for_ds_transform2", "(", "data_dict", "[", "self", ".", "input_key", "]", ",", "self", ".", "ds_scales", ",", "self", ".", "order", ",", "\n", "self", ".", "cval", ",", "self", ".", "axes", ")", "\n", "return", "data_dict", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.data_augmentation.downsampling.downsample_seg_for_ds_transform3": [[45, 68], ["torch.from_numpy", "batchgenerators.augmentations.utils.convert_seg_image_to_one_hot_encoding_batched", "all", "output.append", "tuple", "tuple", "pool_op", "output.append", "torch.from_numpy", "len", "int", "len", "RuntimeError"], "function", ["None"], ["", "", "def", "downsample_seg_for_ds_transform3", "(", "seg", ",", "ds_scales", "=", "(", "(", "1", ",", "1", ",", "1", ")", ",", "(", "0.5", ",", "0.5", ",", "0.5", ")", ",", "(", "0.25", ",", "0.25", ",", "0.25", ")", ")", ",", "classes", "=", "None", ")", ":", "\n", "    ", "output", "=", "[", "]", "\n", "one_hot", "=", "torch", ".", "from_numpy", "(", "convert_seg_image_to_one_hot_encoding_batched", "(", "seg", ",", "classes", ")", ")", "# b, c,", "\n", "\n", "for", "s", "in", "ds_scales", ":", "\n", "        ", "if", "all", "(", "[", "i", "==", "1", "for", "i", "in", "s", "]", ")", ":", "\n", "            ", "output", ".", "append", "(", "torch", ".", "from_numpy", "(", "seg", ")", ")", "\n", "", "else", ":", "\n", "            ", "kernel_size", "=", "tuple", "(", "int", "(", "1", "/", "i", ")", "for", "i", "in", "s", ")", "\n", "stride", "=", "kernel_size", "\n", "pad", "=", "tuple", "(", "(", "i", "-", "1", ")", "//", "2", "for", "i", "in", "kernel_size", ")", "\n", "\n", "if", "len", "(", "s", ")", "==", "2", ":", "\n", "                ", "pool_op", "=", "avg_pool2d", "\n", "", "elif", "len", "(", "s", ")", "==", "3", ":", "\n", "                ", "pool_op", "=", "avg_pool3d", "\n", "", "else", ":", "\n", "                ", "raise", "RuntimeError", "(", ")", "\n", "\n", "", "pooled", "=", "pool_op", "(", "one_hot", ",", "kernel_size", ",", "stride", ",", "pad", ",", "count_include_pad", "=", "False", ",", "ceil_mode", "=", "False", ")", "\n", "\n", "output", ".", "append", "(", "pooled", ")", "\n", "", "", "return", "output", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.data_augmentation.downsampling.downsample_seg_for_ds_transform2": [[88, 106], ["list", "all", "range", "output.append", "numpy.array().astype", "enumerate", "numpy.round().astype", "numpy.zeros", "range", "output.append", "len", "range", "numpy.array", "numpy.round", "batchgenerators.augmentations.utils.resize_segmentation"], "function", ["None"], ["", "", "def", "downsample_seg_for_ds_transform2", "(", "seg", ",", "ds_scales", "=", "(", "(", "1", ",", "1", ",", "1", ")", ",", "(", "0.5", ",", "0.5", ",", "0.5", ")", ",", "(", "0.25", ",", "0.25", ",", "0.25", ")", ")", ",", "order", "=", "0", ",", "cval", "=", "0", ",", "axes", "=", "None", ")", ":", "\n", "    ", "if", "axes", "is", "None", ":", "\n", "        ", "axes", "=", "list", "(", "range", "(", "2", ",", "len", "(", "seg", ".", "shape", ")", ")", ")", "\n", "", "output", "=", "[", "]", "\n", "for", "s", "in", "ds_scales", ":", "\n", "        ", "if", "all", "(", "[", "i", "==", "1", "for", "i", "in", "s", "]", ")", ":", "\n", "            ", "output", ".", "append", "(", "seg", ")", "\n", "", "else", ":", "\n", "            ", "new_shape", "=", "np", ".", "array", "(", "seg", ".", "shape", ")", ".", "astype", "(", "float", ")", "\n", "for", "i", ",", "a", "in", "enumerate", "(", "axes", ")", ":", "\n", "                ", "new_shape", "[", "a", "]", "*=", "s", "[", "i", "]", "\n", "", "new_shape", "=", "np", ".", "round", "(", "new_shape", ")", ".", "astype", "(", "int", ")", "\n", "out_seg", "=", "np", ".", "zeros", "(", "new_shape", ",", "dtype", "=", "seg", ".", "dtype", ")", "\n", "for", "b", "in", "range", "(", "seg", ".", "shape", "[", "0", "]", ")", ":", "\n", "                ", "for", "c", "in", "range", "(", "seg", ".", "shape", "[", "1", "]", ")", ":", "\n", "                    ", "out_seg", "[", "b", ",", "c", "]", "=", "resize_segmentation", "(", "seg", "[", "b", ",", "c", "]", ",", "new_shape", "[", "2", ":", "]", ",", "order", ",", "cval", ")", "\n", "", "", "output", ".", "append", "(", "out_seg", ")", "\n", "", "", "return", "output", "\n", "", ""]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.data_augmentation.pyramid_augmentations.RemoveRandomConnectedComponentFromOneHotEncodingTransform.__init__": [[23, 38], ["isinstance"], "methods", ["None"], ["    ", "def", "__init__", "(", "self", ",", "channel_idx", ",", "key", "=", "\"data\"", ",", "p_per_sample", "=", "0.2", ",", "fill_with_other_class_p", "=", "0.25", ",", "\n", "dont_do_if_covers_more_than_X_percent", "=", "0.25", ",", "p_per_label", "=", "1", ")", ":", "\n", "        ", "\"\"\"\n        :param dont_do_if_covers_more_than_X_percent: dont_do_if_covers_more_than_X_percent=0.25 is 25\\%!\n        :param channel_idx: can be list or int\n        :param key:\n        \"\"\"", "\n", "self", ".", "p_per_label", "=", "p_per_label", "\n", "self", ".", "dont_do_if_covers_more_than_X_percent", "=", "dont_do_if_covers_more_than_X_percent", "\n", "self", ".", "fill_with_other_class_p", "=", "fill_with_other_class_p", "\n", "self", ".", "p_per_sample", "=", "p_per_sample", "\n", "self", ".", "key", "=", "key", "\n", "if", "not", "isinstance", "(", "channel_idx", ",", "(", "list", ",", "tuple", ")", ")", ":", "\n", "            ", "channel_idx", "=", "[", "channel_idx", "]", "\n", "", "self", ".", "channel_idx", "=", "channel_idx", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.data_augmentation.pyramid_augmentations.RemoveRandomConnectedComponentFromOneHotEncodingTransform.__call__": [[39, 68], ["data_dict.get", "range", "numpy.random.uniform", "numpy.random.uniform", "numpy.copy", "numpy.prod", "skimage.morphology.label", "range", "component_ids.append", "component_sizes.append", "len", "numpy.random.choice", "numpy.sum", "zip", "numpy.random.uniform", "len", "numpy.random.choice"], "methods", ["None"], ["", "def", "__call__", "(", "self", ",", "**", "data_dict", ")", ":", "\n", "        ", "data", "=", "data_dict", ".", "get", "(", "self", ".", "key", ")", "\n", "for", "b", "in", "range", "(", "data", ".", "shape", "[", "0", "]", ")", ":", "\n", "            ", "if", "np", ".", "random", ".", "uniform", "(", ")", "<", "self", ".", "p_per_sample", ":", "\n", "                ", "for", "c", "in", "self", ".", "channel_idx", ":", "\n", "                    ", "if", "np", ".", "random", ".", "uniform", "(", ")", "<", "self", ".", "p_per_label", ":", "\n", "                        ", "workon", "=", "np", ".", "copy", "(", "data", "[", "b", ",", "c", "]", ")", "\n", "num_voxels", "=", "np", ".", "prod", "(", "workon", ".", "shape", ",", "dtype", "=", "np", ".", "uint64", ")", "\n", "lab", ",", "num_comp", "=", "label", "(", "workon", ",", "return_num", "=", "True", ")", "\n", "if", "num_comp", ">", "0", ":", "\n", "                            ", "component_ids", "=", "[", "]", "\n", "component_sizes", "=", "[", "]", "\n", "for", "i", "in", "range", "(", "1", ",", "num_comp", "+", "1", ")", ":", "\n", "                                ", "component_ids", ".", "append", "(", "i", ")", "\n", "component_sizes", ".", "append", "(", "np", ".", "sum", "(", "lab", "==", "i", ")", ")", "\n", "", "component_ids", "=", "[", "i", "for", "i", ",", "j", "in", "zip", "(", "component_ids", ",", "component_sizes", ")", "if", "j", "<", "num_voxels", "*", "self", ".", "dont_do_if_covers_more_than_X_percent", "]", "\n", "#_ = component_ids.pop(np.argmax(component_sizes))", "\n", "#else:", "\n", "#    component_ids = list(range(1, num_comp + 1))", "\n", "if", "len", "(", "component_ids", ")", ">", "0", ":", "\n", "                                ", "random_component", "=", "np", ".", "random", ".", "choice", "(", "component_ids", ")", "\n", "data", "[", "b", ",", "c", "]", "[", "lab", "==", "random_component", "]", "=", "0", "\n", "if", "np", ".", "random", ".", "uniform", "(", ")", "<", "self", ".", "fill_with_other_class_p", ":", "\n", "                                    ", "other_ch", "=", "[", "i", "for", "i", "in", "self", ".", "channel_idx", "if", "i", "!=", "c", "]", "\n", "if", "len", "(", "other_ch", ")", ">", "0", ":", "\n", "                                        ", "other_class", "=", "np", ".", "random", ".", "choice", "(", "other_ch", ")", "\n", "data", "[", "b", ",", "other_class", "]", "[", "lab", "==", "random_component", "]", "=", "1", "\n", "", "", "", "", "", "", "", "", "data_dict", "[", "self", ".", "key", "]", "=", "data", "\n", "return", "data_dict", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.data_augmentation.pyramid_augmentations.MoveSegAsOneHotToData.__init__": [[71, 77], ["None"], "methods", ["None"], ["    ", "def", "__init__", "(", "self", ",", "channel_id", ",", "all_seg_labels", ",", "key_origin", "=", "\"seg\"", ",", "key_target", "=", "\"data\"", ",", "remove_from_origin", "=", "True", ")", ":", "\n", "        ", "self", ".", "remove_from_origin", "=", "remove_from_origin", "\n", "self", ".", "all_seg_labels", "=", "all_seg_labels", "\n", "self", ".", "key_target", "=", "key_target", "\n", "self", ".", "key_origin", "=", "key_origin", "\n", "self", ".", "channel_id", "=", "channel_id", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.data_augmentation.pyramid_augmentations.MoveSegAsOneHotToData.__call__": [[78, 93], ["data_dict.get", "data_dict.get", "numpy.zeros", "enumerate", "numpy.concatenate", "len", "range"], "methods", ["None"], ["", "def", "__call__", "(", "self", ",", "**", "data_dict", ")", ":", "\n", "        ", "origin", "=", "data_dict", ".", "get", "(", "self", ".", "key_origin", ")", "\n", "target", "=", "data_dict", ".", "get", "(", "self", ".", "key_target", ")", "\n", "seg", "=", "origin", "[", ":", ",", "self", ".", "channel_id", ":", "self", ".", "channel_id", "+", "1", "]", "\n", "seg_onehot", "=", "np", ".", "zeros", "(", "(", "seg", ".", "shape", "[", "0", "]", ",", "len", "(", "self", ".", "all_seg_labels", ")", ",", "*", "seg", ".", "shape", "[", "2", ":", "]", ")", ",", "dtype", "=", "seg", ".", "dtype", ")", "\n", "for", "i", ",", "l", "in", "enumerate", "(", "self", ".", "all_seg_labels", ")", ":", "\n", "            ", "seg_onehot", "[", ":", ",", "i", "]", "[", "seg", "[", ":", ",", "0", "]", "==", "l", "]", "=", "1", "\n", "", "target", "=", "np", ".", "concatenate", "(", "(", "target", ",", "seg_onehot", ")", ",", "1", ")", "\n", "data_dict", "[", "self", ".", "key_target", "]", "=", "target", "\n", "\n", "if", "self", ".", "remove_from_origin", ":", "\n", "            ", "remaining_channels", "=", "[", "i", "for", "i", "in", "range", "(", "origin", ".", "shape", "[", "1", "]", ")", "if", "i", "!=", "self", ".", "channel_id", "]", "\n", "origin", "=", "origin", "[", ":", ",", "remaining_channels", "]", "\n", "data_dict", "[", "self", ".", "key_origin", "]", "=", "origin", "\n", "", "return", "data_dict", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.data_augmentation.pyramid_augmentations.ApplyRandomBinaryOperatorTransform.__init__": [[96, 110], ["isinstance", "isinstance"], "methods", ["None"], ["    ", "def", "__init__", "(", "self", ",", "channel_idx", ",", "p_per_sample", "=", "0.3", ",", "any_of_these", "=", "(", "binary_dilation", ",", "binary_erosion", ",", "binary_closing", ",", "\n", "binary_opening", ")", ",", "\n", "key", "=", "\"data\"", ",", "strel_size", "=", "(", "1", ",", "10", ")", ",", "p_per_label", "=", "1", ")", ":", "\n", "        ", "self", ".", "p_per_label", "=", "p_per_label", "\n", "self", ".", "strel_size", "=", "strel_size", "\n", "self", ".", "key", "=", "key", "\n", "self", ".", "any_of_these", "=", "any_of_these", "\n", "self", ".", "p_per_sample", "=", "p_per_sample", "\n", "\n", "assert", "not", "isinstance", "(", "channel_idx", ",", "tuple", ")", ",", "\"b\u00e4h\"", "\n", "\n", "if", "not", "isinstance", "(", "channel_idx", ",", "list", ")", ":", "\n", "            ", "channel_idx", "=", "[", "channel_idx", "]", "\n", "", "self", ".", "channel_idx", "=", "channel_idx", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.data_augmentation.pyramid_augmentations.ApplyRandomBinaryOperatorTransform.__call__": [[111, 136], ["data_dict.get", "range", "numpy.random.uniform", "copy.deepcopy", "numpy.random.shuffle", "numpy.random.uniform", "numpy.random.choice", "skimage.morphology.ball", "numpy.copy().astype", "numpy.random.choice.astype", "numpy.random.uniform", "len", "numpy.copy", "numpy.random.choice."], "methods", ["None"], ["", "def", "__call__", "(", "self", ",", "**", "data_dict", ")", ":", "\n", "        ", "data", "=", "data_dict", ".", "get", "(", "self", ".", "key", ")", "\n", "for", "b", "in", "range", "(", "data", ".", "shape", "[", "0", "]", ")", ":", "\n", "            ", "if", "np", ".", "random", ".", "uniform", "(", ")", "<", "self", ".", "p_per_sample", ":", "\n", "                ", "ch", "=", "deepcopy", "(", "self", ".", "channel_idx", ")", "\n", "np", ".", "random", ".", "shuffle", "(", "ch", ")", "\n", "for", "c", "in", "ch", ":", "\n", "                    ", "if", "np", ".", "random", ".", "uniform", "(", ")", "<", "self", ".", "p_per_label", ":", "\n", "                        ", "operation", "=", "np", ".", "random", ".", "choice", "(", "self", ".", "any_of_these", ")", "\n", "selem", "=", "ball", "(", "np", ".", "random", ".", "uniform", "(", "*", "self", ".", "strel_size", ")", ")", "\n", "workon", "=", "np", ".", "copy", "(", "data", "[", "b", ",", "c", "]", ")", ".", "astype", "(", "int", ")", "\n", "res", "=", "operation", "(", "workon", ",", "selem", ")", ".", "astype", "(", "workon", ".", "dtype", ")", "\n", "data", "[", "b", ",", "c", "]", "=", "res", "\n", "\n", "# if class was added, we need to remove it in ALL other channels to keep one hot encoding", "\n", "# properties", "\n", "# we modify data", "\n", "other_ch", "=", "[", "i", "for", "i", "in", "ch", "if", "i", "!=", "c", "]", "\n", "if", "len", "(", "other_ch", ")", ">", "0", ":", "\n", "                            ", "was_added_mask", "=", "(", "res", "-", "workon", ")", ">", "0", "\n", "for", "oc", "in", "other_ch", ":", "\n", "                                ", "data", "[", "b", ",", "oc", "]", "[", "was_added_mask", "]", "=", "0", "\n", "# if class was removed, leave it at background", "\n", "", "", "", "", "", "", "data_dict", "[", "self", ".", "key", "]", "=", "data", "\n", "return", "data_dict", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.data_augmentation.pyramid_augmentations.ApplyRandomBinaryOperatorTransform2.__init__": [[139, 163], ["isinstance", "isinstance"], "methods", ["None"], ["    ", "def", "__init__", "(", "self", ",", "channel_idx", ",", "p_per_sample", "=", "0.3", ",", "p_per_label", "=", "0.3", ",", "any_of_these", "=", "(", "binary_dilation", ",", "binary_closing", ")", ",", "\n", "key", "=", "\"data\"", ",", "strel_size", "=", "(", "1", ",", "10", ")", ")", ":", "\n", "        ", "\"\"\"\n        2019_11_22: I have no idea what the purpose of this was...\n\n        the same as above but here we should use only expanding operations. Expansions will replace other labels\n        :param channel_idx: can be list or int\n        :param p_per_sample:\n        :param any_of_these:\n        :param fill_diff_with_other_class:\n        :param key:\n        :param strel_size:\n        \"\"\"", "\n", "self", ".", "strel_size", "=", "strel_size", "\n", "self", ".", "key", "=", "key", "\n", "self", ".", "any_of_these", "=", "any_of_these", "\n", "self", ".", "p_per_sample", "=", "p_per_sample", "\n", "self", ".", "p_per_label", "=", "p_per_label", "\n", "\n", "assert", "not", "isinstance", "(", "channel_idx", ",", "tuple", ")", ",", "\"b\u00e4h\"", "\n", "\n", "if", "not", "isinstance", "(", "channel_idx", ",", "list", ")", ":", "\n", "            ", "channel_idx", "=", "[", "channel_idx", "]", "\n", "", "self", ".", "channel_idx", "=", "channel_idx", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.data_augmentation.pyramid_augmentations.ApplyRandomBinaryOperatorTransform2.__call__": [[164, 189], ["data_dict.get", "range", "numpy.random.uniform", "copy.deepcopy", "numpy.random.shuffle", "numpy.random.uniform", "numpy.random.choice", "skimage.morphology.ball", "numpy.copy().astype", "numpy.random.choice.astype", "numpy.random.uniform", "len", "numpy.copy", "numpy.random.choice."], "methods", ["None"], ["", "def", "__call__", "(", "self", ",", "**", "data_dict", ")", ":", "\n", "        ", "data", "=", "data_dict", ".", "get", "(", "self", ".", "key", ")", "\n", "for", "b", "in", "range", "(", "data", ".", "shape", "[", "0", "]", ")", ":", "\n", "            ", "if", "np", ".", "random", ".", "uniform", "(", ")", "<", "self", ".", "p_per_sample", ":", "\n", "                ", "ch", "=", "deepcopy", "(", "self", ".", "channel_idx", ")", "\n", "np", ".", "random", ".", "shuffle", "(", "ch", ")", "\n", "for", "c", "in", "ch", ":", "\n", "                    ", "if", "np", ".", "random", ".", "uniform", "(", ")", "<", "self", ".", "p_per_label", ":", "\n", "                        ", "operation", "=", "np", ".", "random", ".", "choice", "(", "self", ".", "any_of_these", ")", "\n", "selem", "=", "ball", "(", "np", ".", "random", ".", "uniform", "(", "*", "self", ".", "strel_size", ")", ")", "\n", "workon", "=", "np", ".", "copy", "(", "data", "[", "b", ",", "c", "]", ")", ".", "astype", "(", "int", ")", "\n", "res", "=", "operation", "(", "workon", ",", "selem", ")", ".", "astype", "(", "workon", ".", "dtype", ")", "\n", "data", "[", "b", ",", "c", "]", "=", "res", "\n", "\n", "# if class was added, we need to remove it in ALL other channels to keep one hot encoding", "\n", "# properties", "\n", "# we modify data", "\n", "other_ch", "=", "[", "i", "for", "i", "in", "ch", "if", "i", "!=", "c", "]", "\n", "if", "len", "(", "other_ch", ")", ">", "0", ":", "\n", "                            ", "was_added_mask", "=", "(", "res", "-", "workon", ")", ">", "0", "\n", "for", "oc", "in", "other_ch", ":", "\n", "                                ", "data", "[", "b", ",", "oc", "]", "[", "was_added_mask", "]", "=", "0", "\n", "# if class was removed, leave it at backgound", "\n", "", "", "", "", "", "", "data_dict", "[", "self", ".", "key", "]", "=", "data", "\n", "return", "data_dict", "\n", "", "", ""]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.data_augmentation.default_data_augmentation.get_patch_size": [[107, 128], ["isinstance", "isinstance", "isinstance", "min", "min", "min", "numpy.array", "numpy.copy", "min", "np.max.astype", "max", "max", "max", "len", "numpy.max", "numpy.max", "numpy.max", "numpy.abs", "numpy.abs", "numpy.abs", "numpy.vstack", "numpy.vstack", "numpy.vstack", "len", "numpy.max", "numpy.vstack", "numpy.abs", "numpy.abs", "numpy.abs", "rotate_coords_3d", "rotate_coords_3d", "rotate_coords_3d", "numpy.abs", "rotate_coords_2d"], "function", ["None"], ["def", "get_patch_size", "(", "final_patch_size", ",", "rot_x", ",", "rot_y", ",", "rot_z", ",", "scale_range", ")", ":", "\n", "    ", "if", "isinstance", "(", "rot_x", ",", "(", "tuple", ",", "list", ")", ")", ":", "\n", "        ", "rot_x", "=", "max", "(", "np", ".", "abs", "(", "rot_x", ")", ")", "\n", "", "if", "isinstance", "(", "rot_y", ",", "(", "tuple", ",", "list", ")", ")", ":", "\n", "        ", "rot_y", "=", "max", "(", "np", ".", "abs", "(", "rot_y", ")", ")", "\n", "", "if", "isinstance", "(", "rot_z", ",", "(", "tuple", ",", "list", ")", ")", ":", "\n", "        ", "rot_z", "=", "max", "(", "np", ".", "abs", "(", "rot_z", ")", ")", "\n", "", "rot_x", "=", "min", "(", "90", "/", "360", "*", "2.", "*", "np", ".", "pi", ",", "rot_x", ")", "\n", "rot_y", "=", "min", "(", "90", "/", "360", "*", "2.", "*", "np", ".", "pi", ",", "rot_y", ")", "\n", "rot_z", "=", "min", "(", "90", "/", "360", "*", "2.", "*", "np", ".", "pi", ",", "rot_z", ")", "\n", "from", "batchgenerators", ".", "augmentations", ".", "utils", "import", "rotate_coords_3d", ",", "rotate_coords_2d", "\n", "coords", "=", "np", ".", "array", "(", "final_patch_size", ")", "\n", "final_shape", "=", "np", ".", "copy", "(", "coords", ")", "\n", "if", "len", "(", "coords", ")", "==", "3", ":", "\n", "        ", "final_shape", "=", "np", ".", "max", "(", "np", ".", "vstack", "(", "(", "np", ".", "abs", "(", "rotate_coords_3d", "(", "coords", ",", "rot_x", ",", "0", ",", "0", ")", ")", ",", "final_shape", ")", ")", ",", "0", ")", "\n", "final_shape", "=", "np", ".", "max", "(", "np", ".", "vstack", "(", "(", "np", ".", "abs", "(", "rotate_coords_3d", "(", "coords", ",", "0", ",", "rot_y", ",", "0", ")", ")", ",", "final_shape", ")", ")", ",", "0", ")", "\n", "final_shape", "=", "np", ".", "max", "(", "np", ".", "vstack", "(", "(", "np", ".", "abs", "(", "rotate_coords_3d", "(", "coords", ",", "0", ",", "0", ",", "rot_z", ")", ")", ",", "final_shape", ")", ")", ",", "0", ")", "\n", "", "elif", "len", "(", "coords", ")", "==", "2", ":", "\n", "        ", "final_shape", "=", "np", ".", "max", "(", "np", ".", "vstack", "(", "(", "np", ".", "abs", "(", "rotate_coords_2d", "(", "coords", ",", "rot_x", ")", ")", ",", "final_shape", ")", ")", ",", "0", ")", "\n", "", "final_shape", "/=", "min", "(", "scale_range", ")", "\n", "return", "final_shape", ".", "astype", "(", "int", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.data_augmentation.default_data_augmentation.get_default_augmentation": [[130, 229], ["batchgenerators.transforms.Compose.append", "params.get", "params.get", "batchgenerators.transforms.Compose.append", "batchgenerators.transforms.Compose.append", "batchgenerators.transforms.Compose.append", "batchgenerators.transforms.Compose", "batchgenerators.dataloading.MultiThreadedAugmenter", "batchgenerators.transforms.Compose.append", "batchgenerators.transforms.Compose.append", "batchgenerators.transforms.Compose.append", "batchgenerators.transforms.Compose", "batchgenerators.dataloading.MultiThreadedAugmenter", "params.get", "params.get", "batchgenerators.transforms.Compose.append", "params.get", "batchgenerators.transforms.Compose.append", "params.get", "batchgenerators.transforms.Compose.append", "batchgenerators.transforms.SpatialTransform", "params.get", "batchgenerators.transforms.Compose.append", "batchgenerators.transforms.Compose.append", "batchgenerators.transforms.Compose.append", "params.get", "params.get", "batchgenerators.transforms.Compose.append", "batchgenerators.transforms.utility_transforms.RemoveLabelTransform", "params.get", "batchgenerators.transforms.Compose.append", "batchgenerators.transforms.utility_transforms.RenameTransform", "batchgenerators.transforms.Compose.append", "batchgenerators.transforms.utility_transforms.NumpyToTensor", "params.get", "params.get", "batchgenerators.transforms.utility_transforms.RemoveLabelTransform", "params.get", "batchgenerators.transforms.Compose.append", "params.get", "batchgenerators.transforms.Compose.append", "params.get", "batchgenerators.transforms.Compose.append", "batchgenerators.transforms.utility_transforms.RenameTransform", "batchgenerators.transforms.Compose.append", "batchgenerators.transforms.utility_transforms.NumpyToTensor", "max", "params.get", "batchgenerators.transforms.DataChannelSelectionTransform", "batchgenerators.transforms.SegChannelSelectionTransform", "params.get", "nnunet.training.data_augmentation.custom_transforms.Convert3DTo2DTransform", "params.get", "nnunet.training.data_augmentation.custom_transforms.Convert2DTo3DTransform", "batchgenerators.transforms.GammaTransform", "batchgenerators.transforms.MirrorTransform", "nnunet.training.data_augmentation.custom_transforms.MaskTransform", "params.get", "nnunet.training.data_augmentation.pyramid_augmentations.MoveSegAsOneHotToData", "params.get", "params.get", "batchgenerators.transforms.Compose.append", "batchgenerators.transforms.Compose.append", "nnunet.training.data_augmentation.custom_transforms.ConvertSegmentationToRegionsTransform", "batchgenerators.transforms.DataChannelSelectionTransform", "batchgenerators.transforms.SegChannelSelectionTransform", "params.get", "nnunet.training.data_augmentation.pyramid_augmentations.MoveSegAsOneHotToData", "nnunet.training.data_augmentation.custom_transforms.ConvertSegmentationToRegionsTransform", "params.get", "params.get", "params.get", "params.get", "params.get", "params.get", "params.get", "params.get", "params.get", "params.get", "params.get", "params.get", "params.get", "params.get", "params.get", "params.get", "params.get", "params.get", "params.get", "params.get", "nnunet.training.data_augmentation.pyramid_augmentations.ApplyRandomBinaryOperatorTransform", "nnunet.training.data_augmentation.pyramid_augmentations.RemoveRandomConnectedComponentFromOneHotEncodingTransform", "params.get", "params.get", "params.get", "params.get", "params.get", "list", "params.get", "params.get", "list", "params.get", "params.get", "params.get", "range", "range", "len", "len", "params.get", "params.get"], "function", ["None"], ["", "def", "get_default_augmentation", "(", "dataloader_train", ",", "dataloader_val", ",", "patch_size", ",", "params", "=", "default_3D_augmentation_params", ",", "\n", "border_val_seg", "=", "-", "1", ",", "pin_memory", "=", "True", ",", "\n", "seeds_train", "=", "None", ",", "seeds_val", "=", "None", ",", "regions", "=", "None", ")", ":", "\n", "    ", "assert", "params", ".", "get", "(", "'mirror'", ")", "is", "None", ",", "\"old version of params, use new keyword do_mirror\"", "\n", "tr_transforms", "=", "[", "]", "\n", "\n", "if", "params", ".", "get", "(", "\"selected_data_channels\"", ")", "is", "not", "None", ":", "\n", "        ", "tr_transforms", ".", "append", "(", "DataChannelSelectionTransform", "(", "params", ".", "get", "(", "\"selected_data_channels\"", ")", ")", ")", "\n", "\n", "", "if", "params", ".", "get", "(", "\"selected_seg_channels\"", ")", "is", "not", "None", ":", "\n", "        ", "tr_transforms", ".", "append", "(", "SegChannelSelectionTransform", "(", "params", ".", "get", "(", "\"selected_seg_channels\"", ")", ")", ")", "\n", "\n", "# don't do color augmentations while in 2d mode with 3d data because the color channel is overloaded!!", "\n", "", "if", "params", ".", "get", "(", "\"dummy_2D\"", ")", "is", "not", "None", "and", "params", ".", "get", "(", "\"dummy_2D\"", ")", ":", "\n", "        ", "tr_transforms", ".", "append", "(", "Convert3DTo2DTransform", "(", ")", ")", "\n", "\n", "", "tr_transforms", ".", "append", "(", "SpatialTransform", "(", "\n", "patch_size", ",", "patch_center_dist_from_border", "=", "None", ",", "do_elastic_deform", "=", "params", ".", "get", "(", "\"do_elastic\"", ")", ",", "\n", "alpha", "=", "params", ".", "get", "(", "\"elastic_deform_alpha\"", ")", ",", "sigma", "=", "params", ".", "get", "(", "\"elastic_deform_sigma\"", ")", ",", "\n", "do_rotation", "=", "params", ".", "get", "(", "\"do_rotation\"", ")", ",", "angle_x", "=", "params", ".", "get", "(", "\"rotation_x\"", ")", ",", "angle_y", "=", "params", ".", "get", "(", "\"rotation_y\"", ")", ",", "\n", "angle_z", "=", "params", ".", "get", "(", "\"rotation_z\"", ")", ",", "do_scale", "=", "params", ".", "get", "(", "\"do_scaling\"", ")", ",", "scale", "=", "params", ".", "get", "(", "\"scale_range\"", ")", ",", "\n", "border_mode_data", "=", "params", ".", "get", "(", "\"border_mode_data\"", ")", ",", "border_cval_data", "=", "0", ",", "order_data", "=", "3", ",", "border_mode_seg", "=", "\"constant\"", ",", "\n", "border_cval_seg", "=", "border_val_seg", ",", "\n", "order_seg", "=", "1", ",", "random_crop", "=", "params", ".", "get", "(", "\"random_crop\"", ")", ",", "p_el_per_sample", "=", "params", ".", "get", "(", "\"p_eldef\"", ")", ",", "\n", "p_scale_per_sample", "=", "params", ".", "get", "(", "\"p_scale\"", ")", ",", "p_rot_per_sample", "=", "params", ".", "get", "(", "\"p_rot\"", ")", ",", "\n", "independent_scale_for_each_axis", "=", "params", ".", "get", "(", "\"independent_scale_factor_for_each_axis\"", ")", "\n", ")", ")", "\n", "if", "params", ".", "get", "(", "\"dummy_2D\"", ")", "is", "not", "None", "and", "params", ".", "get", "(", "\"dummy_2D\"", ")", ":", "\n", "        ", "tr_transforms", ".", "append", "(", "Convert2DTo3DTransform", "(", ")", ")", "\n", "\n", "", "if", "params", ".", "get", "(", "\"do_gamma\"", ")", ":", "\n", "        ", "tr_transforms", ".", "append", "(", "\n", "GammaTransform", "(", "params", ".", "get", "(", "\"gamma_range\"", ")", ",", "False", ",", "True", ",", "retain_stats", "=", "params", ".", "get", "(", "\"gamma_retain_stats\"", ")", ",", "\n", "p_per_sample", "=", "params", "[", "\"p_gamma\"", "]", ")", ")", "\n", "\n", "", "if", "params", ".", "get", "(", "\"do_mirror\"", ")", ":", "\n", "        ", "tr_transforms", ".", "append", "(", "MirrorTransform", "(", "params", ".", "get", "(", "\"mirror_axes\"", ")", ")", ")", "\n", "\n", "", "if", "params", ".", "get", "(", "\"mask_was_used_for_normalization\"", ")", "is", "not", "None", ":", "\n", "        ", "mask_was_used_for_normalization", "=", "params", ".", "get", "(", "\"mask_was_used_for_normalization\"", ")", "\n", "tr_transforms", ".", "append", "(", "MaskTransform", "(", "mask_was_used_for_normalization", ",", "mask_idx_in_seg", "=", "0", ",", "set_outside_to", "=", "0", ")", ")", "\n", "\n", "", "tr_transforms", ".", "append", "(", "RemoveLabelTransform", "(", "-", "1", ",", "0", ")", ")", "\n", "\n", "if", "params", ".", "get", "(", "\"move_last_seg_chanel_to_data\"", ")", "is", "not", "None", "and", "params", ".", "get", "(", "\"move_last_seg_chanel_to_data\"", ")", ":", "\n", "        ", "tr_transforms", ".", "append", "(", "MoveSegAsOneHotToData", "(", "1", ",", "params", ".", "get", "(", "\"all_segmentation_labels\"", ")", ",", "'seg'", ",", "'data'", ")", ")", "\n", "if", "params", ".", "get", "(", "\"cascade_do_cascade_augmentations\"", ")", "and", "not", "None", "and", "params", ".", "get", "(", "\n", "\"cascade_do_cascade_augmentations\"", ")", ":", "\n", "            ", "tr_transforms", ".", "append", "(", "ApplyRandomBinaryOperatorTransform", "(", "\n", "channel_idx", "=", "list", "(", "range", "(", "-", "len", "(", "params", ".", "get", "(", "\"all_segmentation_labels\"", ")", ")", ",", "0", ")", ")", ",", "\n", "p_per_sample", "=", "params", ".", "get", "(", "\"cascade_random_binary_transform_p\"", ")", ",", "\n", "key", "=", "\"data\"", ",", "\n", "strel_size", "=", "params", ".", "get", "(", "\"cascade_random_binary_transform_size\"", ")", ")", ")", "\n", "tr_transforms", ".", "append", "(", "RemoveRandomConnectedComponentFromOneHotEncodingTransform", "(", "\n", "channel_idx", "=", "list", "(", "range", "(", "-", "len", "(", "params", ".", "get", "(", "\"all_segmentation_labels\"", ")", ")", ",", "0", ")", ")", ",", "\n", "key", "=", "\"data\"", ",", "\n", "p_per_sample", "=", "params", ".", "get", "(", "\"cascade_remove_conn_comp_p\"", ")", ",", "\n", "fill_with_other_class_p", "=", "params", ".", "get", "(", "\"cascade_remove_conn_comp_max_size_percent_threshold\"", ")", ",", "\n", "dont_do_if_covers_more_than_X_percent", "=", "params", ".", "get", "(", "\"cascade_remove_conn_comp_fill_with_other_class_p\"", ")", ")", ")", "\n", "\n", "", "", "tr_transforms", ".", "append", "(", "RenameTransform", "(", "'seg'", ",", "'target'", ",", "True", ")", ")", "\n", "\n", "if", "regions", "is", "not", "None", ":", "\n", "        ", "tr_transforms", ".", "append", "(", "ConvertSegmentationToRegionsTransform", "(", "regions", ",", "'target'", ",", "'target'", ")", ")", "\n", "\n", "", "tr_transforms", ".", "append", "(", "NumpyToTensor", "(", "[", "'data'", ",", "'target'", "]", ",", "'float'", ")", ")", "\n", "\n", "tr_transforms", "=", "Compose", "(", "tr_transforms", ")", "\n", "# from batchgenerators.dataloading import SingleThreadedAugmenter", "\n", "# batchgenerator_train = SingleThreadedAugmenter(dataloader_train, tr_transforms)", "\n", "# import IPython;IPython.embed()", "\n", "\n", "batchgenerator_train", "=", "MultiThreadedAugmenter", "(", "dataloader_train", ",", "tr_transforms", ",", "params", ".", "get", "(", "'num_threads'", ")", ",", "\n", "params", ".", "get", "(", "\"num_cached_per_thread\"", ")", ",", "seeds", "=", "seeds_train", ",", "\n", "pin_memory", "=", "pin_memory", ")", "\n", "\n", "val_transforms", "=", "[", "]", "\n", "val_transforms", ".", "append", "(", "RemoveLabelTransform", "(", "-", "1", ",", "0", ")", ")", "\n", "if", "params", ".", "get", "(", "\"selected_data_channels\"", ")", "is", "not", "None", ":", "\n", "        ", "val_transforms", ".", "append", "(", "DataChannelSelectionTransform", "(", "params", ".", "get", "(", "\"selected_data_channels\"", ")", ")", ")", "\n", "", "if", "params", ".", "get", "(", "\"selected_seg_channels\"", ")", "is", "not", "None", ":", "\n", "        ", "val_transforms", ".", "append", "(", "SegChannelSelectionTransform", "(", "params", ".", "get", "(", "\"selected_seg_channels\"", ")", ")", ")", "\n", "\n", "", "if", "params", ".", "get", "(", "\"move_last_seg_chanel_to_data\"", ")", "is", "not", "None", "and", "params", ".", "get", "(", "\"move_last_seg_chanel_to_data\"", ")", ":", "\n", "        ", "val_transforms", ".", "append", "(", "MoveSegAsOneHotToData", "(", "1", ",", "params", ".", "get", "(", "\"all_segmentation_labels\"", ")", ",", "'seg'", ",", "'data'", ")", ")", "\n", "\n", "", "val_transforms", ".", "append", "(", "RenameTransform", "(", "'seg'", ",", "'target'", ",", "True", ")", ")", "\n", "\n", "if", "regions", "is", "not", "None", ":", "\n", "        ", "val_transforms", ".", "append", "(", "ConvertSegmentationToRegionsTransform", "(", "regions", ",", "'target'", ",", "'target'", ")", ")", "\n", "\n", "", "val_transforms", ".", "append", "(", "NumpyToTensor", "(", "[", "'data'", ",", "'target'", "]", ",", "'float'", ")", ")", "\n", "val_transforms", "=", "Compose", "(", "val_transforms", ")", "\n", "\n", "# batchgenerator_val = SingleThreadedAugmenter(dataloader_val, val_transforms)", "\n", "batchgenerator_val", "=", "MultiThreadedAugmenter", "(", "dataloader_val", ",", "val_transforms", ",", "max", "(", "params", ".", "get", "(", "'num_threads'", ")", "//", "2", ",", "1", ")", ",", "\n", "params", ".", "get", "(", "\"num_cached_per_thread\"", ")", ",", "seeds", "=", "seeds_val", ",", "\n", "pin_memory", "=", "pin_memory", ")", "\n", "return", "batchgenerator_train", ",", "batchgenerator_val", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.data_augmentation.default_data_augmentation.get_no_augmentation": [[231, 305], ["batchgenerators.transforms.Compose.append", "batchgenerators.transforms.Compose.append", "batchgenerators.transforms.Compose.append", "batchgenerators.transforms.Compose", "batchgenerators.dataloading.MultiThreadedAugmenter", "batchgenerators.dataloading.MultiThreadedAugmenter.restart", "batchgenerators.transforms.Compose.append", "batchgenerators.transforms.Compose.append", "batchgenerators.transforms.Compose.append", "batchgenerators.transforms.Compose", "batchgenerators.dataloading.MultiThreadedAugmenter", "batchgenerators.dataloading.MultiThreadedAugmenter.restart", "params.get", "batchgenerators.transforms.Compose.append", "params.get", "batchgenerators.transforms.Compose.append", "batchgenerators.transforms.utility_transforms.RemoveLabelTransform", "batchgenerators.transforms.utility_transforms.RenameTransform", "batchgenerators.transforms.Compose.append", "batchgenerators.transforms.utility_transforms.NumpyToTensor", "params.get", "params.get", "batchgenerators.transforms.utility_transforms.RemoveLabelTransform", "params.get", "batchgenerators.transforms.Compose.append", "params.get", "batchgenerators.transforms.Compose.append", "batchgenerators.transforms.utility_transforms.RenameTransform", "batchgenerators.transforms.Compose.append", "batchgenerators.transforms.utility_transforms.NumpyToTensor", "max", "params.get", "batchgenerators.transforms.DataChannelSelectionTransform", "batchgenerators.transforms.SegChannelSelectionTransform", "nnunet.training.data_augmentation.custom_transforms.ConvertSegmentationToRegionsTransform", "batchgenerators.transforms.Compose.append", "batchgenerators.transforms.Compose.append", "range", "batchgenerators.transforms.DataChannelSelectionTransform", "batchgenerators.transforms.SegChannelSelectionTransform", "nnunet.training.data_augmentation.custom_transforms.ConvertSegmentationToRegionsTransform", "batchgenerators.transforms.Compose.append", "batchgenerators.transforms.Compose.append", "range", "params.get", "params.get", "nnunet.training.data_augmentation.downsampling.DownsampleSegForDSTransform3", "nnunet.training.data_augmentation.downsampling.DownsampleSegForDSTransform2", "params.get", "params.get", "params.get", "nnunet.training.data_augmentation.downsampling.DownsampleSegForDSTransform3", "nnunet.training.data_augmentation.downsampling.DownsampleSegForDSTransform2", "params.get", "max", "params.get"], "function", ["None"], ["", "def", "get_no_augmentation", "(", "dataloader_train", ",", "dataloader_val", ",", "patch_size", ",", "params", "=", "default_3D_augmentation_params", ",", "\n", "border_val_seg", "=", "-", "1", ",", "\n", "seeds_train", "=", "None", ",", "seeds_val", "=", "None", ",", "order_seg", "=", "1", ",", "order_data", "=", "3", ",", "deep_supervision_scales", "=", "None", ",", "\n", "soft_ds", "=", "False", ",", "\n", "classes", "=", "None", ",", "pin_memory", "=", "True", ",", "regions", "=", "None", ")", ":", "\n", "    ", "\"\"\"\n    use this instead of get_default_augmentation (drop in replacement) to turn off all data augmentation\n    :param dataloader_train:\n    :param dataloader_val:\n    :param patch_size:\n    :param params:\n    :param border_val_seg:\n    :return:\n    \"\"\"", "\n", "tr_transforms", "=", "[", "]", "\n", "\n", "if", "params", ".", "get", "(", "\"selected_data_channels\"", ")", "is", "not", "None", ":", "\n", "        ", "tr_transforms", ".", "append", "(", "DataChannelSelectionTransform", "(", "params", ".", "get", "(", "\"selected_data_channels\"", ")", ")", ")", "\n", "\n", "", "if", "params", ".", "get", "(", "\"selected_seg_channels\"", ")", "is", "not", "None", ":", "\n", "        ", "tr_transforms", ".", "append", "(", "SegChannelSelectionTransform", "(", "params", ".", "get", "(", "\"selected_seg_channels\"", ")", ")", ")", "\n", "\n", "", "tr_transforms", ".", "append", "(", "RemoveLabelTransform", "(", "-", "1", ",", "0", ")", ")", "\n", "\n", "tr_transforms", ".", "append", "(", "RenameTransform", "(", "'seg'", ",", "'target'", ",", "True", ")", ")", "\n", "\n", "if", "regions", "is", "not", "None", ":", "\n", "        ", "tr_transforms", ".", "append", "(", "ConvertSegmentationToRegionsTransform", "(", "regions", ",", "'target'", ",", "'target'", ")", ")", "\n", "\n", "", "if", "deep_supervision_scales", "is", "not", "None", ":", "\n", "        ", "if", "soft_ds", ":", "\n", "            ", "assert", "classes", "is", "not", "None", "\n", "tr_transforms", ".", "append", "(", "DownsampleSegForDSTransform3", "(", "deep_supervision_scales", ",", "'target'", ",", "'target'", ",", "classes", ")", ")", "\n", "", "else", ":", "\n", "            ", "tr_transforms", ".", "append", "(", "DownsampleSegForDSTransform2", "(", "deep_supervision_scales", ",", "0", ",", "0", ",", "input_key", "=", "'target'", ",", "\n", "output_key", "=", "'target'", ")", ")", "\n", "\n", "", "", "tr_transforms", ".", "append", "(", "NumpyToTensor", "(", "[", "'data'", ",", "'target'", "]", ",", "'float'", ")", ")", "\n", "\n", "tr_transforms", "=", "Compose", "(", "tr_transforms", ")", "\n", "\n", "batchgenerator_train", "=", "MultiThreadedAugmenter", "(", "dataloader_train", ",", "tr_transforms", ",", "params", ".", "get", "(", "'num_threads'", ")", ",", "\n", "params", ".", "get", "(", "\"num_cached_per_thread\"", ")", ",", "\n", "seeds", "=", "range", "(", "params", ".", "get", "(", "'num_threads'", ")", ")", ",", "pin_memory", "=", "True", ")", "\n", "batchgenerator_train", ".", "restart", "(", ")", "\n", "\n", "val_transforms", "=", "[", "]", "\n", "val_transforms", ".", "append", "(", "RemoveLabelTransform", "(", "-", "1", ",", "0", ")", ")", "\n", "if", "params", ".", "get", "(", "\"selected_data_channels\"", ")", "is", "not", "None", ":", "\n", "        ", "val_transforms", ".", "append", "(", "DataChannelSelectionTransform", "(", "params", ".", "get", "(", "\"selected_data_channels\"", ")", ")", ")", "\n", "", "if", "params", ".", "get", "(", "\"selected_seg_channels\"", ")", "is", "not", "None", ":", "\n", "        ", "val_transforms", ".", "append", "(", "SegChannelSelectionTransform", "(", "params", ".", "get", "(", "\"selected_seg_channels\"", ")", ")", ")", "\n", "\n", "", "val_transforms", ".", "append", "(", "RenameTransform", "(", "'seg'", ",", "'target'", ",", "True", ")", ")", "\n", "\n", "if", "regions", "is", "not", "None", ":", "\n", "        ", "val_transforms", ".", "append", "(", "ConvertSegmentationToRegionsTransform", "(", "regions", ",", "'target'", ",", "'target'", ")", ")", "\n", "\n", "", "if", "deep_supervision_scales", "is", "not", "None", ":", "\n", "        ", "if", "soft_ds", ":", "\n", "            ", "assert", "classes", "is", "not", "None", "\n", "val_transforms", ".", "append", "(", "DownsampleSegForDSTransform3", "(", "deep_supervision_scales", ",", "'target'", ",", "'target'", ",", "classes", ")", ")", "\n", "", "else", ":", "\n", "            ", "val_transforms", ".", "append", "(", "DownsampleSegForDSTransform2", "(", "deep_supervision_scales", ",", "0", ",", "0", ",", "input_key", "=", "'target'", ",", "\n", "output_key", "=", "'target'", ")", ")", "\n", "\n", "", "", "val_transforms", ".", "append", "(", "NumpyToTensor", "(", "[", "'data'", ",", "'target'", "]", ",", "'float'", ")", ")", "\n", "val_transforms", "=", "Compose", "(", "val_transforms", ")", "\n", "\n", "batchgenerator_val", "=", "MultiThreadedAugmenter", "(", "dataloader_val", ",", "val_transforms", ",", "max", "(", "params", ".", "get", "(", "'num_threads'", ")", "//", "2", ",", "1", ")", ",", "\n", "params", ".", "get", "(", "\"num_cached_per_thread\"", ")", ",", "\n", "seeds", "=", "range", "(", "max", "(", "params", ".", "get", "(", "'num_threads'", ")", "//", "2", ",", "1", ")", ")", ",", "pin_memory", "=", "True", ")", "\n", "batchgenerator_val", ".", "restart", "(", ")", "\n", "return", "batchgenerator_train", ",", "batchgenerator_val", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.data_augmentation.default_data_augmentation.get_moreDA_augmentation": [[307, 457], ["batchgenerators.transforms.Compose.append", "params.get", "batchgenerators.transforms.Compose.append", "batchgenerators.transforms.Compose.append", "batchgenerators.transforms.Compose.append", "params.get", "batchgenerators.transforms.Compose.append", "batchgenerators.transforms.Compose.append", "batchgenerators.transforms.Compose.append", "params.get", "batchgenerators.transforms.Compose.append", "batchgenerators.transforms.Compose.append", "batchgenerators.transforms.Compose.append", "batchgenerators.transforms.Compose", "batchgenerators.dataloading.MultiThreadedAugmenter", "batchgenerators.transforms.Compose.append", "batchgenerators.transforms.Compose.append", "batchgenerators.transforms.Compose.append", "batchgenerators.transforms.Compose", "batchgenerators.dataloading.MultiThreadedAugmenter", "params.get", "params.get", "batchgenerators.transforms.Compose.append", "params.get", "batchgenerators.transforms.Compose.append", "params.get", "batchgenerators.transforms.Compose.append", "batchgenerators.transforms.SpatialTransform", "batchgenerators.transforms.Compose.append", "batchgenerators.transforms.noise_transforms.GaussianNoiseTransform", "batchgenerators.transforms.noise_transforms.GaussianBlurTransform", "batchgenerators.transforms.color_transforms.BrightnessMultiplicativeTransform", "batchgenerators.transforms.Compose.append", "batchgenerators.transforms.color_transforms.ContrastAugmentationTransform", "batchgenerators.transforms.resample_transforms.SimulateLowResolutionTransform", "batchgenerators.transforms.GammaTransform", "batchgenerators.transforms.Compose.append", "params.get", "params.get", "batchgenerators.transforms.Compose.append", "params.get", "params.get", "batchgenerators.transforms.Compose.append", "batchgenerators.transforms.utility_transforms.RemoveLabelTransform", "params.get", "batchgenerators.transforms.Compose.append", "batchgenerators.transforms.utility_transforms.RenameTransform", "batchgenerators.transforms.Compose.append", "batchgenerators.transforms.utility_transforms.NumpyToTensor", "params.get", "params.get", "batchgenerators.transforms.utility_transforms.RemoveLabelTransform", "params.get", "batchgenerators.transforms.Compose.append", "params.get", "batchgenerators.transforms.Compose.append", "params.get", "batchgenerators.transforms.Compose.append", "batchgenerators.transforms.utility_transforms.RenameTransform", "batchgenerators.transforms.Compose.append", "batchgenerators.transforms.utility_transforms.NumpyToTensor", "max", "params.get", "batchgenerators.transforms.DataChannelSelectionTransform", "batchgenerators.transforms.SegChannelSelectionTransform", "params.get", "nnunet.training.data_augmentation.custom_transforms.Convert3DTo2DTransform", "nnunet.training.data_augmentation.custom_transforms.Convert2DTo3DTransform", "batchgenerators.transforms.color_transforms.BrightnessTransform", "params.get", "batchgenerators.transforms.GammaTransform", "batchgenerators.transforms.MirrorTransform", "nnunet.training.data_augmentation.custom_transforms.MaskTransform", "params.get", "nnunet.training.data_augmentation.pyramid_augmentations.MoveSegAsOneHotToData", "params.get", "nnunet.training.data_augmentation.custom_transforms.ConvertSegmentationToRegionsTransform", "batchgenerators.transforms.Compose.append", "batchgenerators.transforms.Compose.append", "batchgenerators.transforms.DataChannelSelectionTransform", "batchgenerators.transforms.SegChannelSelectionTransform", "params.get", "nnunet.training.data_augmentation.pyramid_augmentations.MoveSegAsOneHotToData", "nnunet.training.data_augmentation.custom_transforms.ConvertSegmentationToRegionsTransform", "batchgenerators.transforms.Compose.append", "batchgenerators.transforms.Compose.append", "params.get", "params.get", "params.get", "params.get", "params.get", "params.get", "params.get", "params.get", "params.get", "params.get", "params.get", "params.get", "params.get", "params.get", "params.get", "params.get", "params.get", "params.get", "params.get", "params.get", "params.get", "params.get", "params.get", "params.get", "params.get", "params.get", "batchgenerators.transforms.Compose.append", "params.get", "batchgenerators.transforms.Compose.append", "nnunet.training.data_augmentation.downsampling.DownsampleSegForDSTransform3", "nnunet.training.data_augmentation.downsampling.DownsampleSegForDSTransform2", "params.get", "params.get", "params.get", "nnunet.training.data_augmentation.downsampling.DownsampleSegForDSTransform3", "nnunet.training.data_augmentation.downsampling.DownsampleSegForDSTransform2", "params.get", "params.get", "params.get", "params.get", "nnunet.training.data_augmentation.pyramid_augmentations.ApplyRandomBinaryOperatorTransform", "nnunet.training.data_augmentation.pyramid_augmentations.RemoveRandomConnectedComponentFromOneHotEncodingTransform", "list", "params.get", "params.get", "params.get", "list", "params.get", "params.get", "params.get", "range", "range", "len", "len", "params.get", "params.get"], "function", ["None"], ["", "def", "get_moreDA_augmentation", "(", "dataloader_train", ",", "dataloader_val", ",", "patch_size", ",", "params", "=", "default_3D_augmentation_params", ",", "\n", "border_val_seg", "=", "-", "1", ",", "\n", "seeds_train", "=", "None", ",", "seeds_val", "=", "None", ",", "order_seg", "=", "1", ",", "order_data", "=", "3", ",", "deep_supervision_scales", "=", "None", ",", "\n", "soft_ds", "=", "False", ",", "\n", "classes", "=", "None", ",", "pin_memory", "=", "True", ",", "regions", "=", "None", ")", ":", "\n", "    ", "assert", "params", ".", "get", "(", "'mirror'", ")", "is", "None", ",", "\"old version of params, use new keyword do_mirror\"", "\n", "\n", "tr_transforms", "=", "[", "]", "\n", "\n", "if", "params", ".", "get", "(", "\"selected_data_channels\"", ")", "is", "not", "None", ":", "\n", "        ", "tr_transforms", ".", "append", "(", "DataChannelSelectionTransform", "(", "params", ".", "get", "(", "\"selected_data_channels\"", ")", ")", ")", "\n", "\n", "", "if", "params", ".", "get", "(", "\"selected_seg_channels\"", ")", "is", "not", "None", ":", "\n", "        ", "tr_transforms", ".", "append", "(", "SegChannelSelectionTransform", "(", "params", ".", "get", "(", "\"selected_seg_channels\"", ")", ")", ")", "\n", "\n", "# don't do color augmentations while in 2d mode with 3d data because the color channel is overloaded!!", "\n", "", "if", "params", ".", "get", "(", "\"dummy_2D\"", ")", "is", "not", "None", "and", "params", ".", "get", "(", "\"dummy_2D\"", ")", ":", "\n", "        ", "ignore_axes", "=", "(", "0", ",", ")", "\n", "tr_transforms", ".", "append", "(", "Convert3DTo2DTransform", "(", ")", ")", "\n", "", "else", ":", "\n", "        ", "ignore_axes", "=", "None", "\n", "\n", "", "tr_transforms", ".", "append", "(", "SpatialTransform", "(", "\n", "patch_size", ",", "patch_center_dist_from_border", "=", "None", ",", "\n", "do_elastic_deform", "=", "params", ".", "get", "(", "\"do_elastic\"", ")", ",", "alpha", "=", "params", ".", "get", "(", "\"elastic_deform_alpha\"", ")", ",", "\n", "sigma", "=", "params", ".", "get", "(", "\"elastic_deform_sigma\"", ")", ",", "\n", "do_rotation", "=", "params", ".", "get", "(", "\"do_rotation\"", ")", ",", "angle_x", "=", "params", ".", "get", "(", "\"rotation_x\"", ")", ",", "angle_y", "=", "params", ".", "get", "(", "\"rotation_y\"", ")", ",", "\n", "angle_z", "=", "params", ".", "get", "(", "\"rotation_z\"", ")", ",", "p_rot_per_axis", "=", "params", ".", "get", "(", "\"rotation_p_per_axis\"", ")", ",", "\n", "do_scale", "=", "params", ".", "get", "(", "\"do_scaling\"", ")", ",", "scale", "=", "params", ".", "get", "(", "\"scale_range\"", ")", ",", "\n", "border_mode_data", "=", "params", ".", "get", "(", "\"border_mode_data\"", ")", ",", "border_cval_data", "=", "0", ",", "order_data", "=", "order_data", ",", "\n", "border_mode_seg", "=", "\"constant\"", ",", "border_cval_seg", "=", "border_val_seg", ",", "\n", "order_seg", "=", "order_seg", ",", "random_crop", "=", "params", ".", "get", "(", "\"random_crop\"", ")", ",", "p_el_per_sample", "=", "params", ".", "get", "(", "\"p_eldef\"", ")", ",", "\n", "p_scale_per_sample", "=", "params", ".", "get", "(", "\"p_scale\"", ")", ",", "p_rot_per_sample", "=", "params", ".", "get", "(", "\"p_rot\"", ")", ",", "\n", "independent_scale_for_each_axis", "=", "params", ".", "get", "(", "\"independent_scale_factor_for_each_axis\"", ")", "\n", ")", ")", "\n", "\n", "if", "params", ".", "get", "(", "\"dummy_2D\"", ")", ":", "\n", "        ", "tr_transforms", ".", "append", "(", "Convert2DTo3DTransform", "(", ")", ")", "\n", "\n", "# we need to put the color augmentations after the dummy 2d part (if applicable). Otherwise the overloaded color", "\n", "# channel gets in the way", "\n", "", "tr_transforms", ".", "append", "(", "GaussianNoiseTransform", "(", "p_per_sample", "=", "0.1", ")", ")", "\n", "tr_transforms", ".", "append", "(", "GaussianBlurTransform", "(", "(", "0.5", ",", "1.", ")", ",", "different_sigma_per_channel", "=", "True", ",", "p_per_sample", "=", "0.2", ",", "\n", "p_per_channel", "=", "0.5", ")", ")", "\n", "tr_transforms", ".", "append", "(", "BrightnessMultiplicativeTransform", "(", "multiplier_range", "=", "(", "0.75", ",", "1.25", ")", ",", "p_per_sample", "=", "0.15", ")", ")", "\n", "\n", "if", "params", ".", "get", "(", "\"do_additive_brightness\"", ")", ":", "\n", "        ", "tr_transforms", ".", "append", "(", "BrightnessTransform", "(", "params", ".", "get", "(", "\"additive_brightness_mu\"", ")", ",", "\n", "params", ".", "get", "(", "\"additive_brightness_sigma\"", ")", ",", "\n", "True", ",", "p_per_sample", "=", "params", ".", "get", "(", "\"additive_brightness_p_per_sample\"", ")", ",", "\n", "p_per_channel", "=", "params", ".", "get", "(", "\"additive_brightness_p_per_channel\"", ")", ")", ")", "\n", "\n", "", "tr_transforms", ".", "append", "(", "ContrastAugmentationTransform", "(", "p_per_sample", "=", "0.15", ")", ")", "\n", "tr_transforms", ".", "append", "(", "SimulateLowResolutionTransform", "(", "zoom_range", "=", "(", "0.5", ",", "1", ")", ",", "per_channel", "=", "True", ",", "\n", "p_per_channel", "=", "0.5", ",", "\n", "order_downsample", "=", "0", ",", "order_upsample", "=", "3", ",", "p_per_sample", "=", "0.25", ",", "\n", "ignore_axes", "=", "ignore_axes", ")", ")", "\n", "tr_transforms", ".", "append", "(", "\n", "GammaTransform", "(", "params", ".", "get", "(", "\"gamma_range\"", ")", ",", "True", ",", "True", ",", "retain_stats", "=", "params", ".", "get", "(", "\"gamma_retain_stats\"", ")", ",", "\n", "p_per_sample", "=", "0.1", ")", ")", "# inverted gamma", "\n", "\n", "if", "params", ".", "get", "(", "\"do_gamma\"", ")", ":", "\n", "        ", "tr_transforms", ".", "append", "(", "\n", "GammaTransform", "(", "params", ".", "get", "(", "\"gamma_range\"", ")", ",", "False", ",", "True", ",", "retain_stats", "=", "params", ".", "get", "(", "\"gamma_retain_stats\"", ")", ",", "\n", "p_per_sample", "=", "params", "[", "\"p_gamma\"", "]", ")", ")", "\n", "\n", "", "if", "params", ".", "get", "(", "\"do_mirror\"", ")", "or", "params", ".", "get", "(", "\"mirror\"", ")", ":", "\n", "        ", "tr_transforms", ".", "append", "(", "MirrorTransform", "(", "params", ".", "get", "(", "\"mirror_axes\"", ")", ")", ")", "\n", "\n", "", "if", "params", ".", "get", "(", "\"mask_was_used_for_normalization\"", ")", "is", "not", "None", ":", "\n", "        ", "mask_was_used_for_normalization", "=", "params", ".", "get", "(", "\"mask_was_used_for_normalization\"", ")", "\n", "tr_transforms", ".", "append", "(", "MaskTransform", "(", "mask_was_used_for_normalization", ",", "mask_idx_in_seg", "=", "0", ",", "set_outside_to", "=", "0", ")", ")", "\n", "\n", "", "tr_transforms", ".", "append", "(", "RemoveLabelTransform", "(", "-", "1", ",", "0", ")", ")", "\n", "\n", "if", "params", ".", "get", "(", "\"move_last_seg_chanel_to_data\"", ")", "is", "not", "None", "and", "params", ".", "get", "(", "\"move_last_seg_chanel_to_data\"", ")", ":", "\n", "        ", "tr_transforms", ".", "append", "(", "MoveSegAsOneHotToData", "(", "1", ",", "params", ".", "get", "(", "\"all_segmentation_labels\"", ")", ",", "'seg'", ",", "'data'", ")", ")", "\n", "if", "params", ".", "get", "(", "\"cascade_do_cascade_augmentations\"", ")", "is", "not", "None", "and", "params", ".", "get", "(", "\n", "\"cascade_do_cascade_augmentations\"", ")", ":", "\n", "            ", "if", "params", ".", "get", "(", "\"cascade_random_binary_transform_p\"", ")", ">", "0", ":", "\n", "                ", "tr_transforms", ".", "append", "(", "ApplyRandomBinaryOperatorTransform", "(", "\n", "channel_idx", "=", "list", "(", "range", "(", "-", "len", "(", "params", ".", "get", "(", "\"all_segmentation_labels\"", ")", ")", ",", "0", ")", ")", ",", "\n", "p_per_sample", "=", "params", ".", "get", "(", "\"cascade_random_binary_transform_p\"", ")", ",", "\n", "key", "=", "\"data\"", ",", "\n", "strel_size", "=", "params", ".", "get", "(", "\"cascade_random_binary_transform_size\"", ")", ",", "\n", "p_per_label", "=", "params", ".", "get", "(", "\"cascade_random_binary_transform_p_per_label\"", ")", ")", ")", "\n", "", "if", "params", ".", "get", "(", "\"cascade_remove_conn_comp_p\"", ")", ">", "0", ":", "\n", "                ", "tr_transforms", ".", "append", "(", "\n", "RemoveRandomConnectedComponentFromOneHotEncodingTransform", "(", "\n", "channel_idx", "=", "list", "(", "range", "(", "-", "len", "(", "params", ".", "get", "(", "\"all_segmentation_labels\"", ")", ")", ",", "0", ")", ")", ",", "\n", "key", "=", "\"data\"", ",", "\n", "p_per_sample", "=", "params", ".", "get", "(", "\"cascade_remove_conn_comp_p\"", ")", ",", "\n", "fill_with_other_class_p", "=", "params", ".", "get", "(", "\"cascade_remove_conn_comp_max_size_percent_threshold\"", ")", ",", "\n", "dont_do_if_covers_more_than_X_percent", "=", "params", ".", "get", "(", "\n", "\"cascade_remove_conn_comp_fill_with_other_class_p\"", ")", ")", ")", "\n", "\n", "", "", "", "tr_transforms", ".", "append", "(", "RenameTransform", "(", "'seg'", ",", "'target'", ",", "True", ")", ")", "\n", "\n", "if", "regions", "is", "not", "None", ":", "\n", "        ", "tr_transforms", ".", "append", "(", "ConvertSegmentationToRegionsTransform", "(", "regions", ",", "'target'", ",", "'target'", ")", ")", "\n", "\n", "", "if", "deep_supervision_scales", "is", "not", "None", ":", "\n", "        ", "if", "soft_ds", ":", "\n", "            ", "assert", "classes", "is", "not", "None", "\n", "tr_transforms", ".", "append", "(", "DownsampleSegForDSTransform3", "(", "deep_supervision_scales", ",", "'target'", ",", "'target'", ",", "classes", ")", ")", "\n", "", "else", ":", "\n", "            ", "tr_transforms", ".", "append", "(", "DownsampleSegForDSTransform2", "(", "deep_supervision_scales", ",", "0", ",", "0", ",", "input_key", "=", "'target'", ",", "\n", "output_key", "=", "'target'", ")", ")", "\n", "\n", "\n", "", "", "tr_transforms", ".", "append", "(", "NumpyToTensor", "(", "[", "'data'", ",", "'target'", "]", ",", "'float'", ")", ")", "\n", "tr_transforms", "=", "Compose", "(", "tr_transforms", ")", "\n", "\n", "batchgenerator_train", "=", "MultiThreadedAugmenter", "(", "dataloader_train", ",", "tr_transforms", ",", "params", ".", "get", "(", "'num_threads'", ")", ",", "\n", "params", ".", "get", "(", "\"num_cached_per_thread\"", ")", ",", "\n", "seeds", "=", "seeds_train", ",", "pin_memory", "=", "pin_memory", ")", "\n", "#batchgenerator_train = SingleThreadedAugmenter(dataloader_train, tr_transforms)", "\n", "\n", "val_transforms", "=", "[", "]", "\n", "val_transforms", ".", "append", "(", "RemoveLabelTransform", "(", "-", "1", ",", "0", ")", ")", "\n", "if", "params", ".", "get", "(", "\"selected_data_channels\"", ")", "is", "not", "None", ":", "\n", "        ", "val_transforms", ".", "append", "(", "DataChannelSelectionTransform", "(", "params", ".", "get", "(", "\"selected_data_channels\"", ")", ")", ")", "\n", "", "if", "params", ".", "get", "(", "\"selected_seg_channels\"", ")", "is", "not", "None", ":", "\n", "        ", "val_transforms", ".", "append", "(", "SegChannelSelectionTransform", "(", "params", ".", "get", "(", "\"selected_seg_channels\"", ")", ")", ")", "\n", "\n", "", "if", "params", ".", "get", "(", "\"move_last_seg_chanel_to_data\"", ")", "is", "not", "None", "and", "params", ".", "get", "(", "\"move_last_seg_chanel_to_data\"", ")", ":", "\n", "        ", "val_transforms", ".", "append", "(", "MoveSegAsOneHotToData", "(", "1", ",", "params", ".", "get", "(", "\"all_segmentation_labels\"", ")", ",", "'seg'", ",", "'data'", ")", ")", "\n", "\n", "", "val_transforms", ".", "append", "(", "RenameTransform", "(", "'seg'", ",", "'target'", ",", "True", ")", ")", "\n", "\n", "if", "regions", "is", "not", "None", ":", "\n", "        ", "val_transforms", ".", "append", "(", "ConvertSegmentationToRegionsTransform", "(", "regions", ",", "'target'", ",", "'target'", ")", ")", "\n", "\n", "", "if", "deep_supervision_scales", "is", "not", "None", ":", "\n", "        ", "if", "soft_ds", ":", "\n", "            ", "assert", "classes", "is", "not", "None", "\n", "val_transforms", ".", "append", "(", "DownsampleSegForDSTransform3", "(", "deep_supervision_scales", ",", "'target'", ",", "'target'", ",", "classes", ")", ")", "\n", "", "else", ":", "\n", "            ", "val_transforms", ".", "append", "(", "DownsampleSegForDSTransform2", "(", "deep_supervision_scales", ",", "0", ",", "0", ",", "input_key", "=", "'target'", ",", "\n", "output_key", "=", "'target'", ")", ")", "\n", "\n", "", "", "val_transforms", ".", "append", "(", "NumpyToTensor", "(", "[", "'data'", ",", "'target'", "]", ",", "'float'", ")", ")", "\n", "val_transforms", "=", "Compose", "(", "val_transforms", ")", "\n", "\n", "batchgenerator_val", "=", "MultiThreadedAugmenter", "(", "dataloader_val", ",", "val_transforms", ",", "max", "(", "params", ".", "get", "(", "'num_threads'", ")", "//", "2", ",", "1", ")", ",", "\n", "params", ".", "get", "(", "\"num_cached_per_thread\"", ")", ",", "\n", "seeds", "=", "seeds_val", ",", "pin_memory", "=", "pin_memory", ")", "\n", "#batchgenerator_val = SingleThreadedAugmenter(dataloader_val, val_transforms)", "\n", "\n", "return", "batchgenerator_train", ",", "batchgenerator_val", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.data_augmentation.default_data_augmentation.get_insaneDA_augmentation": [[459, 602], ["batchgenerators.transforms.Compose.append", "params.get", "batchgenerators.transforms.Compose.append", "batchgenerators.transforms.Compose.append", "batchgenerators.transforms.Compose.append", "batchgenerators.transforms.Compose.append", "batchgenerators.transforms.Compose.append", "batchgenerators.transforms.Compose.append", "params.get", "params.get", "batchgenerators.transforms.Compose.append", "batchgenerators.transforms.Compose.append", "batchgenerators.transforms.Compose.append", "batchgenerators.transforms.Compose", "batchgenerators.dataloading.MultiThreadedAugmenter", "batchgenerators.transforms.Compose.append", "batchgenerators.transforms.Compose.append", "batchgenerators.transforms.Compose.append", "batchgenerators.transforms.Compose", "batchgenerators.dataloading.MultiThreadedAugmenter", "params.get", "params.get", "batchgenerators.transforms.Compose.append", "params.get", "batchgenerators.transforms.Compose.append", "params.get", "batchgenerators.transforms.Compose.append", "batchgenerators.transforms.SpatialTransform", "batchgenerators.transforms.Compose.append", "batchgenerators.transforms.noise_transforms.GaussianNoiseTransform", "batchgenerators.transforms.noise_transforms.GaussianBlurTransform", "batchgenerators.transforms.color_transforms.BrightnessMultiplicativeTransform", "batchgenerators.transforms.color_transforms.ContrastAugmentationTransform", "batchgenerators.transforms.resample_transforms.SimulateLowResolutionTransform", "batchgenerators.transforms.GammaTransform", "batchgenerators.transforms.Compose.append", "batchgenerators.transforms.Compose.append", "params.get", "params.get", "batchgenerators.transforms.Compose.append", "params.get", "params.get", "batchgenerators.transforms.Compose.append", "batchgenerators.transforms.utility_transforms.RemoveLabelTransform", "params.get", "batchgenerators.transforms.Compose.append", "batchgenerators.transforms.utility_transforms.RenameTransform", "batchgenerators.transforms.Compose.append", "batchgenerators.transforms.utility_transforms.NumpyToTensor", "params.get", "params.get", "batchgenerators.transforms.utility_transforms.RemoveLabelTransform", "params.get", "batchgenerators.transforms.Compose.append", "params.get", "batchgenerators.transforms.Compose.append", "params.get", "batchgenerators.transforms.Compose.append", "batchgenerators.transforms.utility_transforms.RenameTransform", "batchgenerators.transforms.Compose.append", "batchgenerators.transforms.utility_transforms.NumpyToTensor", "max", "params.get", "batchgenerators.transforms.DataChannelSelectionTransform", "batchgenerators.transforms.SegChannelSelectionTransform", "params.get", "nnunet.training.data_augmentation.custom_transforms.Convert3DTo2DTransform", "nnunet.training.data_augmentation.custom_transforms.Convert2DTo3DTransform", "params.get", "batchgenerators.transforms.color_transforms.BrightnessTransform", "batchgenerators.transforms.GammaTransform", "batchgenerators.transforms.MirrorTransform", "nnunet.training.data_augmentation.custom_transforms.MaskTransform", "params.get", "nnunet.training.data_augmentation.pyramid_augmentations.MoveSegAsOneHotToData", "params.get", "params.get", "nnunet.training.data_augmentation.custom_transforms.ConvertSegmentationToRegionsTransform", "batchgenerators.transforms.Compose.append", "batchgenerators.transforms.Compose.append", "batchgenerators.transforms.DataChannelSelectionTransform", "batchgenerators.transforms.SegChannelSelectionTransform", "params.get", "nnunet.training.data_augmentation.pyramid_augmentations.MoveSegAsOneHotToData", "nnunet.training.data_augmentation.custom_transforms.ConvertSegmentationToRegionsTransform", "batchgenerators.transforms.Compose.append", "batchgenerators.transforms.Compose.append", "params.get", "params.get", "params.get", "params.get", "params.get", "params.get", "params.get", "params.get", "params.get", "params.get", "params.get", "params.get", "params.get", "params.get", "params.get", "params.get", "params.get", "params.get", "params.get", "params.get", "params.get", "params.get", "params.get", "params.get", "params.get", "batchgenerators.transforms.Compose.append", "params.get", "batchgenerators.transforms.Compose.append", "nnunet.training.data_augmentation.downsampling.DownsampleSegForDSTransform3", "nnunet.training.data_augmentation.downsampling.DownsampleSegForDSTransform2", "params.get", "params.get", "params.get", "nnunet.training.data_augmentation.downsampling.DownsampleSegForDSTransform3", "nnunet.training.data_augmentation.downsampling.DownsampleSegForDSTransform2", "params.get", "params.get", "params.get", "params.get", "nnunet.training.data_augmentation.pyramid_augmentations.ApplyRandomBinaryOperatorTransform", "nnunet.training.data_augmentation.pyramid_augmentations.RemoveRandomConnectedComponentFromOneHotEncodingTransform", "list", "params.get", "params.get", "list", "params.get", "params.get", "params.get", "range", "range", "len", "len", "params.get", "params.get"], "function", ["None"], ["", "def", "get_insaneDA_augmentation", "(", "dataloader_train", ",", "dataloader_val", ",", "patch_size", ",", "params", "=", "default_3D_augmentation_params", ",", "\n", "border_val_seg", "=", "-", "1", ",", "\n", "seeds_train", "=", "None", ",", "seeds_val", "=", "None", ",", "order_seg", "=", "1", ",", "order_data", "=", "3", ",", "deep_supervision_scales", "=", "None", ",", "\n", "soft_ds", "=", "False", ",", "\n", "classes", "=", "None", ",", "pin_memory", "=", "True", ",", "regions", "=", "None", ")", ":", "\n", "    ", "assert", "params", ".", "get", "(", "'mirror'", ")", "is", "None", ",", "\"old version of params, use new keyword do_mirror\"", "\n", "\n", "tr_transforms", "=", "[", "]", "\n", "\n", "if", "params", ".", "get", "(", "\"selected_data_channels\"", ")", "is", "not", "None", ":", "\n", "        ", "tr_transforms", ".", "append", "(", "DataChannelSelectionTransform", "(", "params", ".", "get", "(", "\"selected_data_channels\"", ")", ")", ")", "\n", "\n", "", "if", "params", ".", "get", "(", "\"selected_seg_channels\"", ")", "is", "not", "None", ":", "\n", "        ", "tr_transforms", ".", "append", "(", "SegChannelSelectionTransform", "(", "params", ".", "get", "(", "\"selected_seg_channels\"", ")", ")", ")", "\n", "\n", "# don't do color augmentations while in 2d mode with 3d data because the color channel is overloaded!!", "\n", "", "if", "params", ".", "get", "(", "\"dummy_2D\"", ")", "is", "not", "None", "and", "params", ".", "get", "(", "\"dummy_2D\"", ")", ":", "\n", "        ", "ignore_axes", "=", "(", "0", ",", ")", "\n", "tr_transforms", ".", "append", "(", "Convert3DTo2DTransform", "(", ")", ")", "\n", "", "else", ":", "\n", "        ", "ignore_axes", "=", "None", "\n", "\n", "", "tr_transforms", ".", "append", "(", "SpatialTransform", "(", "\n", "patch_size", ",", "patch_center_dist_from_border", "=", "None", ",", "do_elastic_deform", "=", "params", ".", "get", "(", "\"do_elastic\"", ")", ",", "\n", "alpha", "=", "params", ".", "get", "(", "\"elastic_deform_alpha\"", ")", ",", "sigma", "=", "params", ".", "get", "(", "\"elastic_deform_sigma\"", ")", ",", "\n", "do_rotation", "=", "params", ".", "get", "(", "\"do_rotation\"", ")", ",", "angle_x", "=", "params", ".", "get", "(", "\"rotation_x\"", ")", ",", "angle_y", "=", "params", ".", "get", "(", "\"rotation_y\"", ")", ",", "\n", "angle_z", "=", "params", ".", "get", "(", "\"rotation_z\"", ")", ",", "do_scale", "=", "params", ".", "get", "(", "\"do_scaling\"", ")", ",", "scale", "=", "params", ".", "get", "(", "\"scale_range\"", ")", ",", "\n", "border_mode_data", "=", "params", ".", "get", "(", "\"border_mode_data\"", ")", ",", "border_cval_data", "=", "0", ",", "order_data", "=", "order_data", ",", "\n", "border_mode_seg", "=", "\"constant\"", ",", "border_cval_seg", "=", "border_val_seg", ",", "\n", "order_seg", "=", "order_seg", ",", "random_crop", "=", "params", ".", "get", "(", "\"random_crop\"", ")", ",", "p_el_per_sample", "=", "params", ".", "get", "(", "\"p_eldef\"", ")", ",", "\n", "p_scale_per_sample", "=", "params", ".", "get", "(", "\"p_scale\"", ")", ",", "p_rot_per_sample", "=", "params", ".", "get", "(", "\"p_rot\"", ")", ",", "\n", "independent_scale_for_each_axis", "=", "params", ".", "get", "(", "\"independent_scale_factor_for_each_axis\"", ")", ",", "\n", "p_independent_scale_per_axis", "=", "params", ".", "get", "(", "\"p_independent_scale_per_axis\"", ")", "\n", ")", ")", "\n", "\n", "if", "params", ".", "get", "(", "\"dummy_2D\"", ")", ":", "\n", "        ", "tr_transforms", ".", "append", "(", "Convert2DTo3DTransform", "(", ")", ")", "\n", "\n", "# we need to put the color augmentations after the dummy 2d part (if applicable). Otherwise the overloaded color", "\n", "# channel gets in the way", "\n", "", "tr_transforms", ".", "append", "(", "GaussianNoiseTransform", "(", "p_per_sample", "=", "0.15", ")", ")", "\n", "tr_transforms", ".", "append", "(", "GaussianBlurTransform", "(", "(", "0.5", ",", "1.5", ")", ",", "different_sigma_per_channel", "=", "True", ",", "p_per_sample", "=", "0.2", ",", "\n", "p_per_channel", "=", "0.5", ")", ")", "\n", "tr_transforms", ".", "append", "(", "BrightnessMultiplicativeTransform", "(", "multiplier_range", "=", "(", "0.70", ",", "1.3", ")", ",", "p_per_sample", "=", "0.15", ")", ")", "\n", "tr_transforms", ".", "append", "(", "ContrastAugmentationTransform", "(", "contrast_range", "=", "(", "0.65", ",", "1.5", ")", ",", "p_per_sample", "=", "0.15", ")", ")", "\n", "tr_transforms", ".", "append", "(", "SimulateLowResolutionTransform", "(", "zoom_range", "=", "(", "0.5", ",", "1", ")", ",", "per_channel", "=", "True", ",", "\n", "p_per_channel", "=", "0.5", ",", "\n", "order_downsample", "=", "0", ",", "order_upsample", "=", "3", ",", "p_per_sample", "=", "0.25", ",", "\n", "ignore_axes", "=", "ignore_axes", ")", ")", "\n", "tr_transforms", ".", "append", "(", "\n", "GammaTransform", "(", "params", ".", "get", "(", "\"gamma_range\"", ")", ",", "True", ",", "True", ",", "retain_stats", "=", "params", ".", "get", "(", "\"gamma_retain_stats\"", ")", ",", "\n", "p_per_sample", "=", "0.15", ")", ")", "# inverted gamma", "\n", "\n", "if", "params", ".", "get", "(", "\"do_additive_brightness\"", ")", ":", "\n", "        ", "tr_transforms", ".", "append", "(", "BrightnessTransform", "(", "params", ".", "get", "(", "\"additive_brightness_mu\"", ")", ",", "\n", "params", ".", "get", "(", "\"additive_brightness_sigma\"", ")", ",", "\n", "True", ",", "p_per_sample", "=", "params", ".", "get", "(", "\"additive_brightness_p_per_sample\"", ")", ",", "\n", "p_per_channel", "=", "params", ".", "get", "(", "\"additive_brightness_p_per_channel\"", ")", ")", ")", "\n", "\n", "", "if", "params", ".", "get", "(", "\"do_gamma\"", ")", ":", "\n", "        ", "tr_transforms", ".", "append", "(", "\n", "GammaTransform", "(", "params", ".", "get", "(", "\"gamma_range\"", ")", ",", "False", ",", "True", ",", "retain_stats", "=", "params", ".", "get", "(", "\"gamma_retain_stats\"", ")", ",", "\n", "p_per_sample", "=", "params", "[", "\"p_gamma\"", "]", ")", ")", "\n", "\n", "", "if", "params", ".", "get", "(", "\"do_mirror\"", ")", "or", "params", ".", "get", "(", "\"mirror\"", ")", ":", "\n", "        ", "tr_transforms", ".", "append", "(", "MirrorTransform", "(", "params", ".", "get", "(", "\"mirror_axes\"", ")", ")", ")", "\n", "\n", "", "if", "params", ".", "get", "(", "\"mask_was_used_for_normalization\"", ")", "is", "not", "None", ":", "\n", "        ", "mask_was_used_for_normalization", "=", "params", ".", "get", "(", "\"mask_was_used_for_normalization\"", ")", "\n", "tr_transforms", ".", "append", "(", "MaskTransform", "(", "mask_was_used_for_normalization", ",", "mask_idx_in_seg", "=", "0", ",", "set_outside_to", "=", "0", ")", ")", "\n", "\n", "", "tr_transforms", ".", "append", "(", "RemoveLabelTransform", "(", "-", "1", ",", "0", ")", ")", "\n", "\n", "if", "params", ".", "get", "(", "\"move_last_seg_chanel_to_data\"", ")", "is", "not", "None", "and", "params", ".", "get", "(", "\"move_last_seg_chanel_to_data\"", ")", ":", "\n", "        ", "tr_transforms", ".", "append", "(", "MoveSegAsOneHotToData", "(", "1", ",", "params", ".", "get", "(", "\"all_segmentation_labels\"", ")", ",", "'seg'", ",", "'data'", ")", ")", "\n", "if", "params", ".", "get", "(", "\"cascade_do_cascade_augmentations\"", ")", "and", "not", "None", "and", "params", ".", "get", "(", "\n", "\"cascade_do_cascade_augmentations\"", ")", ":", "\n", "            ", "if", "params", ".", "get", "(", "\"cascade_random_binary_transform_p\"", ")", ">", "0", ":", "\n", "                ", "tr_transforms", ".", "append", "(", "ApplyRandomBinaryOperatorTransform", "(", "\n", "channel_idx", "=", "list", "(", "range", "(", "-", "len", "(", "params", ".", "get", "(", "\"all_segmentation_labels\"", ")", ")", ",", "0", ")", ")", ",", "\n", "p_per_sample", "=", "params", ".", "get", "(", "\"cascade_random_binary_transform_p\"", ")", ",", "\n", "key", "=", "\"data\"", ",", "\n", "strel_size", "=", "params", ".", "get", "(", "\"cascade_random_binary_transform_size\"", ")", ")", ")", "\n", "", "if", "params", ".", "get", "(", "\"cascade_remove_conn_comp_p\"", ")", ">", "0", ":", "\n", "                ", "tr_transforms", ".", "append", "(", "\n", "RemoveRandomConnectedComponentFromOneHotEncodingTransform", "(", "\n", "channel_idx", "=", "list", "(", "range", "(", "-", "len", "(", "params", ".", "get", "(", "\"all_segmentation_labels\"", ")", ")", ",", "0", ")", ")", ",", "\n", "key", "=", "\"data\"", ",", "\n", "p_per_sample", "=", "params", ".", "get", "(", "\"cascade_remove_conn_comp_p\"", ")", ",", "\n", "fill_with_other_class_p", "=", "params", ".", "get", "(", "\"cascade_remove_conn_comp_max_size_percent_threshold\"", ")", ",", "\n", "dont_do_if_covers_more_than_X_percent", "=", "params", ".", "get", "(", "\n", "\"cascade_remove_conn_comp_fill_with_other_class_p\"", ")", ")", ")", "\n", "\n", "", "", "", "tr_transforms", ".", "append", "(", "RenameTransform", "(", "'seg'", ",", "'target'", ",", "True", ")", ")", "\n", "\n", "if", "regions", "is", "not", "None", ":", "\n", "        ", "tr_transforms", ".", "append", "(", "ConvertSegmentationToRegionsTransform", "(", "regions", ",", "'target'", ",", "'target'", ")", ")", "\n", "\n", "", "if", "deep_supervision_scales", "is", "not", "None", ":", "\n", "        ", "if", "soft_ds", ":", "\n", "            ", "assert", "classes", "is", "not", "None", "\n", "tr_transforms", ".", "append", "(", "DownsampleSegForDSTransform3", "(", "deep_supervision_scales", ",", "'target'", ",", "'target'", ",", "classes", ")", ")", "\n", "", "else", ":", "\n", "            ", "tr_transforms", ".", "append", "(", "DownsampleSegForDSTransform2", "(", "deep_supervision_scales", ",", "0", ",", "0", ",", "input_key", "=", "'target'", ",", "\n", "output_key", "=", "'target'", ")", ")", "\n", "\n", "", "", "tr_transforms", ".", "append", "(", "NumpyToTensor", "(", "[", "'data'", ",", "'target'", "]", ",", "'float'", ")", ")", "\n", "tr_transforms", "=", "Compose", "(", "tr_transforms", ")", "\n", "\n", "batchgenerator_train", "=", "MultiThreadedAugmenter", "(", "dataloader_train", ",", "tr_transforms", ",", "params", ".", "get", "(", "'num_threads'", ")", ",", "\n", "params", ".", "get", "(", "\"num_cached_per_thread\"", ")", ",", "\n", "seeds", "=", "seeds_train", ",", "pin_memory", "=", "pin_memory", ")", "\n", "\n", "val_transforms", "=", "[", "]", "\n", "val_transforms", ".", "append", "(", "RemoveLabelTransform", "(", "-", "1", ",", "0", ")", ")", "\n", "if", "params", ".", "get", "(", "\"selected_data_channels\"", ")", "is", "not", "None", ":", "\n", "        ", "val_transforms", ".", "append", "(", "DataChannelSelectionTransform", "(", "params", ".", "get", "(", "\"selected_data_channels\"", ")", ")", ")", "\n", "", "if", "params", ".", "get", "(", "\"selected_seg_channels\"", ")", "is", "not", "None", ":", "\n", "        ", "val_transforms", ".", "append", "(", "SegChannelSelectionTransform", "(", "params", ".", "get", "(", "\"selected_seg_channels\"", ")", ")", ")", "\n", "\n", "", "if", "params", ".", "get", "(", "\"move_last_seg_chanel_to_data\"", ")", "is", "not", "None", "and", "params", ".", "get", "(", "\"move_last_seg_chanel_to_data\"", ")", ":", "\n", "        ", "val_transforms", ".", "append", "(", "MoveSegAsOneHotToData", "(", "1", ",", "params", ".", "get", "(", "\"all_segmentation_labels\"", ")", ",", "'seg'", ",", "'data'", ")", ")", "\n", "\n", "", "val_transforms", ".", "append", "(", "RenameTransform", "(", "'seg'", ",", "'target'", ",", "True", ")", ")", "\n", "\n", "if", "regions", "is", "not", "None", ":", "\n", "        ", "val_transforms", ".", "append", "(", "ConvertSegmentationToRegionsTransform", "(", "regions", ",", "'target'", ",", "'target'", ")", ")", "\n", "\n", "", "if", "deep_supervision_scales", "is", "not", "None", ":", "\n", "        ", "if", "soft_ds", ":", "\n", "            ", "assert", "classes", "is", "not", "None", "\n", "val_transforms", ".", "append", "(", "DownsampleSegForDSTransform3", "(", "deep_supervision_scales", ",", "'target'", ",", "'target'", ",", "classes", ")", ")", "\n", "", "else", ":", "\n", "            ", "val_transforms", ".", "append", "(", "DownsampleSegForDSTransform2", "(", "deep_supervision_scales", ",", "0", ",", "0", ",", "input_key", "=", "'target'", ",", "\n", "output_key", "=", "'target'", ")", ")", "\n", "\n", "", "", "val_transforms", ".", "append", "(", "NumpyToTensor", "(", "[", "'data'", ",", "'target'", "]", ",", "'float'", ")", ")", "\n", "val_transforms", "=", "Compose", "(", "val_transforms", ")", "\n", "\n", "batchgenerator_val", "=", "MultiThreadedAugmenter", "(", "dataloader_val", ",", "val_transforms", ",", "max", "(", "params", ".", "get", "(", "'num_threads'", ")", "//", "2", ",", "1", ")", ",", "\n", "params", ".", "get", "(", "\"num_cached_per_thread\"", ")", ",", "\n", "seeds", "=", "seeds_val", ",", "pin_memory", "=", "pin_memory", ")", "\n", "return", "batchgenerator_train", ",", "batchgenerator_val", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.data_augmentation.custom_transforms.RemoveKeyTransform.__init__": [[20, 22], ["None"], "methods", ["None"], ["    ", "def", "__init__", "(", "self", ",", "key_to_remove", ")", ":", "\n", "        ", "self", ".", "key_to_remove", "=", "key_to_remove", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.data_augmentation.custom_transforms.RemoveKeyTransform.__call__": [[23, 26], ["data_dict.pop"], "methods", ["None"], ["", "def", "__call__", "(", "self", ",", "**", "data_dict", ")", ":", "\n", "        ", "_", "=", "data_dict", ".", "pop", "(", "self", ".", "key_to_remove", ",", "None", ")", "\n", "return", "data_dict", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.data_augmentation.custom_transforms.MaskTransform.__init__": [[29, 45], ["None"], "methods", ["None"], ["    ", "def", "__init__", "(", "self", ",", "dct_for_where_it_was_used", ",", "mask_idx_in_seg", "=", "1", ",", "set_outside_to", "=", "0", ",", "data_key", "=", "\"data\"", ",", "seg_key", "=", "\"seg\"", ")", ":", "\n", "        ", "\"\"\"\n        data[mask < 0] = 0\n        Sets everything outside the mask to 0. CAREFUL! outside is defined as < 0, not =0 (in the Mask)!!!\n\n        :param dct_for_where_it_was_used:\n        :param mask_idx_in_seg:\n        :param set_outside_to:\n        :param data_key:\n        :param seg_key:\n        \"\"\"", "\n", "self", ".", "dct_for_where_it_was_used", "=", "dct_for_where_it_was_used", "\n", "self", ".", "seg_key", "=", "seg_key", "\n", "self", ".", "data_key", "=", "data_key", "\n", "self", ".", "set_outside_to", "=", "set_outside_to", "\n", "self", ".", "mask_idx_in_seg", "=", "mask_idx_in_seg", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.data_augmentation.custom_transforms.MaskTransform.__call__": [[46, 58], ["data_dict.get", "data_dict.get", "range", "Warning", "range"], "methods", ["None"], ["", "def", "__call__", "(", "self", ",", "**", "data_dict", ")", ":", "\n", "        ", "seg", "=", "data_dict", ".", "get", "(", "self", ".", "seg_key", ")", "\n", "if", "seg", "is", "None", "or", "seg", ".", "shape", "[", "1", "]", "<", "self", ".", "mask_idx_in_seg", ":", "\n", "            ", "raise", "Warning", "(", "\"mask not found, seg may be missing or seg[:, mask_idx_in_seg] may not exist\"", ")", "\n", "", "data", "=", "data_dict", ".", "get", "(", "self", ".", "data_key", ")", "\n", "for", "b", "in", "range", "(", "data", ".", "shape", "[", "0", "]", ")", ":", "\n", "            ", "mask", "=", "seg", "[", "b", ",", "self", ".", "mask_idx_in_seg", "]", "\n", "for", "c", "in", "range", "(", "data", ".", "shape", "[", "1", "]", ")", ":", "\n", "                ", "if", "self", ".", "dct_for_where_it_was_used", "[", "c", "]", ":", "\n", "                    ", "data", "[", "b", ",", "c", "]", "[", "mask", "<", "0", "]", "=", "self", ".", "set_outside_to", "\n", "", "", "", "data_dict", "[", "self", ".", "data_key", "]", "=", "data", "\n", "return", "data_dict", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.data_augmentation.custom_transforms.Convert3DTo2DTransform.__init__": [[81, 83], ["None"], "methods", ["None"], ["    ", "def", "__init__", "(", "self", ")", ":", "\n", "        ", "pass", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.data_augmentation.custom_transforms.Convert3DTo2DTransform.__call__": [[84, 86], ["custom_transforms.convert_3d_to_2d_generator"], "methods", ["home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.data_augmentation.custom_transforms.convert_3d_to_2d_generator"], ["", "def", "__call__", "(", "self", ",", "**", "data_dict", ")", ":", "\n", "        ", "return", "convert_3d_to_2d_generator", "(", "data_dict", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.data_augmentation.custom_transforms.Convert2DTo3DTransform.__init__": [[89, 91], ["None"], "methods", ["None"], ["    ", "def", "__init__", "(", "self", ")", ":", "\n", "        ", "pass", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.data_augmentation.custom_transforms.Convert2DTo3DTransform.__call__": [[92, 94], ["custom_transforms.convert_2d_to_3d_generator"], "methods", ["home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.data_augmentation.custom_transforms.convert_2d_to_3d_generator"], ["", "def", "__call__", "(", "self", ",", "**", "data_dict", ")", ":", "\n", "        ", "return", "convert_2d_to_3d_generator", "(", "data_dict", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.data_augmentation.custom_transforms.ConvertSegmentationToRegionsTransform.__init__": [[97, 109], ["None"], "methods", ["None"], ["    ", "def", "__init__", "(", "self", ",", "regions", ":", "dict", ",", "seg_key", ":", "str", "=", "\"seg\"", ",", "output_key", ":", "str", "=", "\"seg\"", ",", "seg_channel", ":", "int", "=", "0", ")", ":", "\n", "        ", "\"\"\"\n        regions are tuple of tuples where each inner tuple holds the class indices that are merged into one region, example:\n        regions= ((1, 2), (2, )) will result in 2 regions: one covering the region of labels 1&2 and the other just 2\n        :param regions:\n        :param seg_key:\n        :param output_key:\n        \"\"\"", "\n", "self", ".", "seg_channel", "=", "seg_channel", "\n", "self", ".", "output_key", "=", "output_key", "\n", "self", ".", "seg_key", "=", "seg_key", "\n", "self", ".", "regions", "=", "regions", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.data_augmentation.custom_transforms.ConvertSegmentationToRegionsTransform.__call__": [[110, 124], ["data_dict.get", "len", "list", "numpy.zeros", "range", "enumerate", "custom_transforms.ConvertSegmentationToRegionsTransform.regions.keys"], "methods", ["None"], ["", "def", "__call__", "(", "self", ",", "**", "data_dict", ")", ":", "\n", "        ", "seg", "=", "data_dict", ".", "get", "(", "self", ".", "seg_key", ")", "\n", "num_regions", "=", "len", "(", "self", ".", "regions", ")", "\n", "if", "seg", "is", "not", "None", ":", "\n", "            ", "seg_shp", "=", "seg", ".", "shape", "\n", "output_shape", "=", "list", "(", "seg_shp", ")", "\n", "output_shape", "[", "1", "]", "=", "num_regions", "\n", "region_output", "=", "np", ".", "zeros", "(", "output_shape", ",", "dtype", "=", "seg", ".", "dtype", ")", "\n", "for", "b", "in", "range", "(", "seg_shp", "[", "0", "]", ")", ":", "\n", "                ", "for", "r", ",", "k", "in", "enumerate", "(", "self", ".", "regions", ".", "keys", "(", ")", ")", ":", "\n", "                    ", "for", "l", "in", "self", ".", "regions", "[", "k", "]", ":", "\n", "                        ", "region_output", "[", "b", ",", "r", "]", "[", "seg", "[", "b", ",", "self", ".", "seg_channel", "]", "==", "l", "]", "=", "1", "\n", "", "", "", "data_dict", "[", "self", ".", "output_key", "]", "=", "region_output", "\n", "", "return", "data_dict", "\n", "", "", ""]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.data_augmentation.custom_transforms.convert_3d_to_2d_generator": [[60, 68], ["data_dict[].reshape", "data_dict[].reshape"], "function", ["None"], ["", "", "def", "convert_3d_to_2d_generator", "(", "data_dict", ")", ":", "\n", "    ", "shp", "=", "data_dict", "[", "'data'", "]", ".", "shape", "\n", "data_dict", "[", "'data'", "]", "=", "data_dict", "[", "'data'", "]", ".", "reshape", "(", "(", "shp", "[", "0", "]", ",", "shp", "[", "1", "]", "*", "shp", "[", "2", "]", ",", "shp", "[", "3", "]", ",", "shp", "[", "4", "]", ")", ")", "\n", "data_dict", "[", "'orig_shape_data'", "]", "=", "shp", "\n", "shp", "=", "data_dict", "[", "'seg'", "]", ".", "shape", "\n", "data_dict", "[", "'seg'", "]", "=", "data_dict", "[", "'seg'", "]", ".", "reshape", "(", "(", "shp", "[", "0", "]", ",", "shp", "[", "1", "]", "*", "shp", "[", "2", "]", ",", "shp", "[", "3", "]", ",", "shp", "[", "4", "]", ")", ")", "\n", "data_dict", "[", "'orig_shape_seg'", "]", "=", "shp", "\n", "return", "data_dict", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.data_augmentation.custom_transforms.convert_2d_to_3d_generator": [[70, 78], ["data_dict[].reshape", "data_dict[].reshape"], "function", ["None"], ["", "def", "convert_2d_to_3d_generator", "(", "data_dict", ")", ":", "\n", "    ", "shp", "=", "data_dict", "[", "'orig_shape_data'", "]", "\n", "current_shape", "=", "data_dict", "[", "'data'", "]", ".", "shape", "\n", "data_dict", "[", "'data'", "]", "=", "data_dict", "[", "'data'", "]", ".", "reshape", "(", "(", "shp", "[", "0", "]", ",", "shp", "[", "1", "]", ",", "shp", "[", "2", "]", ",", "current_shape", "[", "-", "2", "]", ",", "current_shape", "[", "-", "1", "]", ")", ")", "\n", "shp", "=", "data_dict", "[", "'orig_shape_seg'", "]", "\n", "current_shape_seg", "=", "data_dict", "[", "'seg'", "]", ".", "shape", "\n", "data_dict", "[", "'seg'", "]", "=", "data_dict", "[", "'seg'", "]", ".", "reshape", "(", "(", "shp", "[", "0", "]", ",", "shp", "[", "1", "]", ",", "shp", "[", "2", "]", ",", "current_shape_seg", "[", "-", "2", "]", ",", "current_shape_seg", "[", "-", "1", "]", ")", ")", "\n", "return", "data_dict", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.data_augmentation.nnUNetTrainerV2_insaneDA.nnUNetTrainerV2_insaneDA.setup_DA_params": [[28, 79], ["nnunet.training.data_augmentation.default_data_augmentation.get_patch_size", "numpy.array", "nnunet.training.data_augmentation.default_data_augmentation.get_patch_size", "list", "nnUNetTrainerV2_insaneDA.nnUNetTrainerV2_insaneDA.print_to_log_file", "max", "min", "list", "list", "numpy.cumprod", "numpy.vstack"], "methods", ["home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.data_augmentation.default_data_augmentation.get_patch_size", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.data_augmentation.default_data_augmentation.get_patch_size", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.network_training.nnUNetTrainerV2_DDP.nnUNetTrainerV2_DDP.print_to_log_file"], ["    ", "def", "setup_DA_params", "(", "self", ")", ":", "\n", "        ", "self", ".", "deep_supervision_scales", "=", "[", "[", "1", ",", "1", ",", "1", "]", "]", "+", "list", "(", "list", "(", "i", ")", "for", "i", "in", "1", "/", "np", ".", "cumprod", "(", "\n", "np", ".", "vstack", "(", "self", ".", "net_num_pool_op_kernel_sizes", ")", ",", "axis", "=", "0", ")", ")", "[", ":", "-", "1", "]", "\n", "\n", "if", "self", ".", "threeD", ":", "\n", "            ", "self", ".", "data_aug_params", "=", "default_3D_augmentation_params", "\n", "self", ".", "data_aug_params", "[", "'rotation_x'", "]", "=", "(", "-", "30.", "/", "360", "*", "2.", "*", "np", ".", "pi", ",", "30.", "/", "360", "*", "2.", "*", "np", ".", "pi", ")", "\n", "self", ".", "data_aug_params", "[", "'rotation_y'", "]", "=", "(", "-", "30.", "/", "360", "*", "2.", "*", "np", ".", "pi", ",", "30.", "/", "360", "*", "2.", "*", "np", ".", "pi", ")", "\n", "self", ".", "data_aug_params", "[", "'rotation_z'", "]", "=", "(", "-", "30.", "/", "360", "*", "2.", "*", "np", ".", "pi", ",", "30.", "/", "360", "*", "2.", "*", "np", ".", "pi", ")", "\n", "if", "self", ".", "do_dummy_2D_aug", ":", "\n", "                ", "self", ".", "data_aug_params", "[", "\"dummy_2D\"", "]", "=", "True", "\n", "self", ".", "print_to_log_file", "(", "\"Using dummy2d data augmentation\"", ")", "\n", "self", ".", "data_aug_params", "[", "\"elastic_deform_alpha\"", "]", "=", "default_2D_augmentation_params", "[", "\"elastic_deform_alpha\"", "]", "\n", "self", ".", "data_aug_params", "[", "\"elastic_deform_sigma\"", "]", "=", "default_2D_augmentation_params", "[", "\"elastic_deform_sigma\"", "]", "\n", "self", ".", "data_aug_params", "[", "\"rotation_x\"", "]", "=", "default_2D_augmentation_params", "[", "\"rotation_x\"", "]", "\n", "", "", "else", ":", "\n", "            ", "self", ".", "do_dummy_2D_aug", "=", "False", "\n", "if", "max", "(", "self", ".", "patch_size", ")", "/", "min", "(", "self", ".", "patch_size", ")", ">", "1.5", ":", "\n", "                ", "default_2D_augmentation_params", "[", "'rotation_x'", "]", "=", "(", "-", "180.", "/", "360", "*", "2.", "*", "np", ".", "pi", ",", "180.", "/", "360", "*", "2.", "*", "np", ".", "pi", ")", "\n", "", "self", ".", "data_aug_params", "=", "default_2D_augmentation_params", "\n", "", "self", ".", "data_aug_params", "[", "\"mask_was_used_for_normalization\"", "]", "=", "self", ".", "use_mask_for_norm", "\n", "\n", "if", "self", ".", "do_dummy_2D_aug", ":", "\n", "            ", "self", ".", "basic_generator_patch_size", "=", "get_patch_size", "(", "self", ".", "patch_size", "[", "1", ":", "]", ",", "\n", "self", ".", "data_aug_params", "[", "'rotation_x'", "]", ",", "\n", "self", ".", "data_aug_params", "[", "'rotation_y'", "]", ",", "\n", "self", ".", "data_aug_params", "[", "'rotation_z'", "]", ",", "\n", "self", ".", "data_aug_params", "[", "'scale_range'", "]", ")", "\n", "self", ".", "basic_generator_patch_size", "=", "np", ".", "array", "(", "[", "self", ".", "patch_size", "[", "0", "]", "]", "+", "list", "(", "self", ".", "basic_generator_patch_size", ")", ")", "\n", "patch_size_for_spatialtransform", "=", "self", ".", "patch_size", "[", "1", ":", "]", "\n", "", "else", ":", "\n", "            ", "self", ".", "basic_generator_patch_size", "=", "get_patch_size", "(", "self", ".", "patch_size", ",", "self", ".", "data_aug_params", "[", "'rotation_x'", "]", ",", "\n", "self", ".", "data_aug_params", "[", "'rotation_y'", "]", ",", "\n", "self", ".", "data_aug_params", "[", "'rotation_z'", "]", ",", "\n", "self", ".", "data_aug_params", "[", "'scale_range'", "]", ")", "\n", "patch_size_for_spatialtransform", "=", "self", ".", "patch_size", "\n", "\n", "", "self", ".", "data_aug_params", "[", "\"scale_range\"", "]", "=", "(", "0.65", ",", "1.6", ")", "\n", "\n", "self", ".", "data_aug_params", "[", "\"do_elastic\"", "]", "=", "True", "\n", "self", ".", "data_aug_params", "[", "\"elastic_deform_alpha\"", "]", "=", "(", "0.", ",", "1300.", ")", "\n", "self", ".", "data_aug_params", "[", "\"elastic_deform_sigma\"", "]", "=", "(", "9.", ",", "15.", ")", "\n", "self", ".", "data_aug_params", "[", "\"p_eldef\"", "]", "=", "0.2", "\n", "\n", "self", ".", "data_aug_params", "[", "'selected_seg_channels'", "]", "=", "[", "0", "]", "\n", "\n", "self", ".", "data_aug_params", "[", "'gamma_range'", "]", "=", "(", "0.6", ",", "2", ")", "\n", "\n", "self", ".", "data_aug_params", "[", "'patch_size_for_spatialtransform'", "]", "=", "patch_size_for_spatialtransform", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.data_augmentation.nnUNetTrainerV2_insaneDA.nnUNetTrainerV2_insaneDA.initialize": [[80, 141], ["batchgenerators.utilities.file_and_folder_operations.maybe_mkdir_p", "nnUNetTrainerV2_insaneDA.nnUNetTrainerV2_insaneDA.process_plans", "nnUNetTrainerV2_insaneDA.nnUNetTrainerV2_insaneDA.setup_DA_params", "len", "numpy.array", "numpy.array", "nnunet.training.loss_functions.deep_supervision.MultipleOutputLoss2", "batchgenerators.utilities.file_and_folder_operations.join", "nnUNetTrainerV2_insaneDA.nnUNetTrainerV2_insaneDA.initialize_network", "nnUNetTrainerV2_insaneDA.nnUNetTrainerV2_insaneDA.initialize_optimizer_and_scheduler", "isinstance", "nnUNetTrainerV2_insaneDA.nnUNetTrainerV2_insaneDA.print_to_log_file", "nnUNetTrainerV2_insaneDA.nnUNetTrainerV2_insaneDA.load_plans_file", "numpy.array.sum", "nnUNetTrainerV2_insaneDA.nnUNetTrainerV2_insaneDA.get_basic_generators", "nnunet.training.data_augmentation.default_data_augmentation.get_insaneDA_augmentation", "nnUNetTrainerV2_insaneDA.nnUNetTrainerV2_insaneDA.print_to_log_file", "nnUNetTrainerV2_insaneDA.nnUNetTrainerV2_insaneDA.print_to_log_file", "print", "nnunet.training.dataloading.dataset_loading.unpack_dataset", "print", "print", "range", "range", "str", "str", "nnUNetTrainerV2_insaneDA.nnUNetTrainerV2_insaneDA.dataset_tr.keys", "nnUNetTrainerV2_insaneDA.nnUNetTrainerV2_insaneDA.dataset_val.keys"], "methods", ["home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.BraTS_trainer.nnUNetTrainerV2BraTSRegions.nnUNetTrainerV2BraTSRegions_DA3_BN.process_plans", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.BraTS_trainer.nnUNetTrainerV2BraTSRegions.nnUNetTrainerV2BraTSRegions_DA4_BN.setup_DA_params", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.BraTS_trainer.nnUNetTrainerV2BraTSRegions.nnUNetTrainerV2BraTSRegions_DA3.initialize_network", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.optimizer_and_lr.nnUNetTrainerV2_momentum09in2D.nnUNetTrainerV2_momentum09in2D.initialize_optimizer_and_scheduler", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.network_training.nnUNetTrainerV2_DDP.nnUNetTrainerV2_DDP.print_to_log_file", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.network_training.nnUNetTrainer.nnUNetTrainer.load_plans_file", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.nnUNet_variants.nnUNetTrainerNoDA.nnUNetTrainerNoDA.get_basic_generators", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.data_augmentation.default_data_augmentation.get_insaneDA_augmentation", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.network_training.nnUNetTrainerV2_DDP.nnUNetTrainerV2_DDP.print_to_log_file", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.network_training.nnUNetTrainerV2_DDP.nnUNetTrainerV2_DDP.print_to_log_file", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.dataloading.dataset_loading.unpack_dataset"], ["", "def", "initialize", "(", "self", ",", "training", "=", "True", ",", "force_load_plans", "=", "False", ")", ":", "\n", "        ", "if", "not", "self", ".", "was_initialized", ":", "\n", "            ", "maybe_mkdir_p", "(", "self", ".", "output_folder", ")", "\n", "\n", "if", "force_load_plans", "or", "(", "self", ".", "plans", "is", "None", ")", ":", "\n", "                ", "self", ".", "load_plans_file", "(", ")", "\n", "\n", "", "self", ".", "process_plans", "(", "self", ".", "plans", ")", "\n", "\n", "self", ".", "setup_DA_params", "(", ")", "\n", "\n", "################# Here we wrap the loss for deep supervision ############", "\n", "# we need to know the number of outputs of the network", "\n", "net_numpool", "=", "len", "(", "self", ".", "net_num_pool_op_kernel_sizes", ")", "\n", "\n", "# we give each output a weight which decreases exponentially (division by 2) as the resolution decreases", "\n", "# this gives higher resolution outputs more weight in the loss", "\n", "weights", "=", "np", ".", "array", "(", "[", "1", "/", "(", "2", "**", "i", ")", "for", "i", "in", "range", "(", "net_numpool", ")", "]", ")", "\n", "\n", "# we don't use the lowest 2 outputs. Normalize weights so that they sum to 1", "\n", "mask", "=", "np", ".", "array", "(", "[", "True", "if", "i", "<", "net_numpool", "-", "1", "else", "False", "for", "i", "in", "range", "(", "net_numpool", ")", "]", ")", "\n", "weights", "[", "~", "mask", "]", "=", "0", "\n", "weights", "=", "weights", "/", "weights", ".", "sum", "(", ")", "\n", "\n", "# now wrap the loss", "\n", "self", ".", "loss", "=", "MultipleOutputLoss2", "(", "self", ".", "loss", ",", "weights", ")", "\n", "################# END ###################", "\n", "\n", "self", ".", "folder_with_preprocessed_data", "=", "join", "(", "self", ".", "dataset_directory", ",", "self", ".", "plans", "[", "'data_identifier'", "]", "+", "\n", "\"_stage%d\"", "%", "self", ".", "stage", ")", "\n", "if", "training", ":", "\n", "                ", "self", ".", "dl_tr", ",", "self", ".", "dl_val", "=", "self", ".", "get_basic_generators", "(", ")", "\n", "if", "self", ".", "unpack_data", ":", "\n", "                    ", "print", "(", "\"unpacking dataset\"", ")", "\n", "unpack_dataset", "(", "self", ".", "folder_with_preprocessed_data", ")", "\n", "print", "(", "\"done\"", ")", "\n", "", "else", ":", "\n", "                    ", "print", "(", "\n", "\"INFO: Not unpacking data! Training may be slow due to that. Pray you are not using 2d or you \"", "\n", "\"will wait all winter for your model to finish!\"", ")", "\n", "\n", "", "self", ".", "tr_gen", ",", "self", ".", "val_gen", "=", "get_insaneDA_augmentation", "(", "self", ".", "dl_tr", ",", "self", ".", "dl_val", ",", "\n", "self", ".", "data_aug_params", "[", "\n", "'patch_size_for_spatialtransform'", "]", ",", "\n", "self", ".", "data_aug_params", ",", "\n", "deep_supervision_scales", "=", "self", ".", "deep_supervision_scales", ",", "\n", "pin_memory", "=", "self", ".", "pin_memory", ")", "\n", "self", ".", "print_to_log_file", "(", "\"TRAINING KEYS:\\n %s\"", "%", "(", "str", "(", "self", ".", "dataset_tr", ".", "keys", "(", ")", ")", ")", ",", "\n", "also_print_to_console", "=", "False", ")", "\n", "self", ".", "print_to_log_file", "(", "\"VALIDATION KEYS:\\n %s\"", "%", "(", "str", "(", "self", ".", "dataset_val", ".", "keys", "(", ")", ")", ")", ",", "\n", "also_print_to_console", "=", "False", ")", "\n", "", "else", ":", "\n", "                ", "pass", "\n", "\n", "", "self", ".", "initialize_network", "(", ")", "\n", "self", ".", "initialize_optimizer_and_scheduler", "(", ")", "\n", "\n", "assert", "isinstance", "(", "self", ".", "network", ",", "(", "SegmentationNetwork", ",", "nn", ".", "DataParallel", ")", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "print_to_log_file", "(", "'self.was_initialized is True, not running self.initialize again'", ")", "\n", "", "self", ".", "was_initialized", "=", "True", "\n", "", "", ""]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.data_augmentation.nnUNetTrainerV2_DA2.nnUNetTrainerV2_DA2.setup_DA_params": [[20, 31], ["super().setup_DA_params"], "methods", ["home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.BraTS_trainer.nnUNetTrainerV2BraTSRegions.nnUNetTrainerV2BraTSRegions_DA4_BN.setup_DA_params"], ["    ", "def", "setup_DA_params", "(", "self", ")", ":", "\n", "        ", "super", "(", ")", ".", "setup_DA_params", "(", ")", "\n", "\n", "self", ".", "data_aug_params", "[", "\"independent_scale_factor_for_each_axis\"", "]", "=", "True", "\n", "\n", "if", "self", ".", "threeD", ":", "\n", "            ", "self", ".", "data_aug_params", "[", "\"rotation_p_per_axis\"", "]", "=", "0.5", "\n", "", "else", ":", "\n", "            ", "self", ".", "data_aug_params", "[", "\"rotation_p_per_axis\"", "]", "=", "1", "\n", "\n", "", "self", ".", "data_aug_params", "[", "\"do_additive_brightness\"", "]", "=", "True", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.data_augmentation.nnUNetTrainerV2_independentScalePerAxis.nnUNetTrainerV2_independentScalePerAxis.setup_DA_params": [[20, 23], ["super().setup_DA_params"], "methods", ["home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.BraTS_trainer.nnUNetTrainerV2BraTSRegions.nnUNetTrainerV2BraTSRegions_DA4_BN.setup_DA_params"], ["    ", "def", "setup_DA_params", "(", "self", ")", ":", "\n", "        ", "super", "(", ")", ".", "setup_DA_params", "(", ")", "\n", "self", ".", "data_aug_params", "[", "\"independent_scale_factor_for_each_axis\"", "]", "=", "True", "\n", "", "", ""]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.data_augmentation.nnUNetTrainerV2_DA3.nnUNetTrainerV2_DA3.setup_DA_params": [[188, 251], ["super().setup_DA_params", "nnunet.training.data_augmentation.default_data_augmentation.get_patch_size", "numpy.array", "nnunet.training.data_augmentation.default_data_augmentation.get_patch_size", "list", "nnUNetTrainerV2_DA3.nnUNetTrainerV2_DA3.print_to_log_file", "max", "min", "list", "list", "numpy.cumprod", "numpy.vstack"], "methods", ["home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.BraTS_trainer.nnUNetTrainerV2BraTSRegions.nnUNetTrainerV2BraTSRegions_DA4_BN.setup_DA_params", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.data_augmentation.default_data_augmentation.get_patch_size", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.data_augmentation.default_data_augmentation.get_patch_size", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.network_training.nnUNetTrainerV2_DDP.nnUNetTrainerV2_DDP.print_to_log_file"], ["    ", "def", "setup_DA_params", "(", "self", ")", ":", "\n", "        ", "super", "(", ")", ".", "setup_DA_params", "(", ")", "\n", "self", ".", "deep_supervision_scales", "=", "[", "[", "1", ",", "1", ",", "1", "]", "]", "+", "list", "(", "list", "(", "i", ")", "for", "i", "in", "1", "/", "np", ".", "cumprod", "(", "\n", "np", ".", "vstack", "(", "self", ".", "net_num_pool_op_kernel_sizes", ")", ",", "axis", "=", "0", ")", ")", "[", ":", "-", "1", "]", "\n", "\n", "if", "self", ".", "threeD", ":", "\n", "            ", "self", ".", "data_aug_params", "=", "default_3D_augmentation_params", "\n", "self", ".", "data_aug_params", "[", "'rotation_x'", "]", "=", "(", "-", "30.", "/", "360", "*", "2.", "*", "np", ".", "pi", ",", "30.", "/", "360", "*", "2.", "*", "np", ".", "pi", ")", "\n", "self", ".", "data_aug_params", "[", "'rotation_y'", "]", "=", "(", "-", "30.", "/", "360", "*", "2.", "*", "np", ".", "pi", ",", "30.", "/", "360", "*", "2.", "*", "np", ".", "pi", ")", "\n", "self", ".", "data_aug_params", "[", "'rotation_z'", "]", "=", "(", "-", "30.", "/", "360", "*", "2.", "*", "np", ".", "pi", ",", "30.", "/", "360", "*", "2.", "*", "np", ".", "pi", ")", "\n", "if", "self", ".", "do_dummy_2D_aug", ":", "\n", "                ", "self", ".", "data_aug_params", "[", "\"dummy_2D\"", "]", "=", "True", "\n", "self", ".", "print_to_log_file", "(", "\"Using dummy2d data augmentation\"", ")", "\n", "self", ".", "data_aug_params", "[", "\"elastic_deform_alpha\"", "]", "=", "default_2D_augmentation_params", "[", "\"elastic_deform_alpha\"", "]", "\n", "self", ".", "data_aug_params", "[", "\"elastic_deform_sigma\"", "]", "=", "default_2D_augmentation_params", "[", "\"elastic_deform_sigma\"", "]", "\n", "self", ".", "data_aug_params", "[", "\"rotation_x\"", "]", "=", "default_2D_augmentation_params", "[", "\"rotation_x\"", "]", "\n", "", "", "else", ":", "\n", "            ", "self", ".", "do_dummy_2D_aug", "=", "False", "\n", "if", "max", "(", "self", ".", "patch_size", ")", "/", "min", "(", "self", ".", "patch_size", ")", ">", "1.5", ":", "\n", "                ", "default_2D_augmentation_params", "[", "'rotation_x'", "]", "=", "(", "-", "180.", "/", "360", "*", "2.", "*", "np", ".", "pi", ",", "180.", "/", "360", "*", "2.", "*", "np", ".", "pi", ")", "\n", "", "self", ".", "data_aug_params", "=", "default_2D_augmentation_params", "\n", "", "self", ".", "data_aug_params", "[", "\"mask_was_used_for_normalization\"", "]", "=", "self", ".", "use_mask_for_norm", "\n", "\n", "if", "self", ".", "do_dummy_2D_aug", ":", "\n", "            ", "self", ".", "basic_generator_patch_size", "=", "get_patch_size", "(", "self", ".", "patch_size", "[", "1", ":", "]", ",", "\n", "self", ".", "data_aug_params", "[", "'rotation_x'", "]", ",", "\n", "self", ".", "data_aug_params", "[", "'rotation_y'", "]", ",", "\n", "self", ".", "data_aug_params", "[", "'rotation_z'", "]", ",", "\n", "self", ".", "data_aug_params", "[", "'scale_range'", "]", ")", "\n", "self", ".", "basic_generator_patch_size", "=", "np", ".", "array", "(", "[", "self", ".", "patch_size", "[", "0", "]", "]", "+", "list", "(", "self", ".", "basic_generator_patch_size", ")", ")", "\n", "patch_size_for_spatialtransform", "=", "self", ".", "patch_size", "[", "1", ":", "]", "\n", "", "else", ":", "\n", "            ", "self", ".", "basic_generator_patch_size", "=", "get_patch_size", "(", "self", ".", "patch_size", ",", "self", ".", "data_aug_params", "[", "'rotation_x'", "]", ",", "\n", "self", ".", "data_aug_params", "[", "'rotation_y'", "]", ",", "\n", "self", ".", "data_aug_params", "[", "'rotation_z'", "]", ",", "\n", "self", ".", "data_aug_params", "[", "'scale_range'", "]", ")", "\n", "patch_size_for_spatialtransform", "=", "self", ".", "patch_size", "\n", "\n", "", "self", ".", "data_aug_params", "[", "'selected_seg_channels'", "]", "=", "[", "0", "]", "\n", "self", ".", "data_aug_params", "[", "'patch_size_for_spatialtransform'", "]", "=", "patch_size_for_spatialtransform", "\n", "\n", "self", ".", "data_aug_params", "[", "\"p_rot\"", "]", "=", "0.3", "\n", "\n", "self", ".", "data_aug_params", "[", "\"scale_range\"", "]", "=", "(", "0.65", ",", "1.6", ")", "\n", "self", ".", "data_aug_params", "[", "\"p_scale\"", "]", "=", "0.3", "\n", "self", ".", "data_aug_params", "[", "\"independent_scale_factor_for_each_axis\"", "]", "=", "True", "\n", "self", ".", "data_aug_params", "[", "\"p_independent_scale_per_axis\"", "]", "=", "0.3", "\n", "\n", "self", ".", "data_aug_params", "[", "\"do_elastic\"", "]", "=", "True", "\n", "self", ".", "data_aug_params", "[", "\"p_eldef\"", "]", "=", "0.3", "\n", "self", ".", "data_aug_params", "[", "\"eldef_deformation_scale\"", "]", "=", "(", "0", ",", "0.25", ")", "\n", "\n", "self", ".", "data_aug_params", "[", "\"do_additive_brightness\"", "]", "=", "True", "\n", "self", ".", "data_aug_params", "[", "\"additive_brightness_mu\"", "]", "=", "0", "\n", "self", ".", "data_aug_params", "[", "\"additive_brightness_sigma\"", "]", "=", "0.2", "\n", "self", ".", "data_aug_params", "[", "\"additive_brightness_p_per_sample\"", "]", "=", "0.3", "\n", "self", ".", "data_aug_params", "[", "\"additive_brightness_p_per_channel\"", "]", "=", "1", "\n", "\n", "self", ".", "data_aug_params", "[", "'gamma_range'", "]", "=", "(", "0.5", ",", "1.6", ")", "\n", "\n", "self", ".", "data_aug_params", "[", "'num_cached_per_thread'", "]", "=", "4", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.data_augmentation.nnUNetTrainerV2_DA3.nnUNetTrainerV2_DA3.initialize": [[252, 315], ["nnunet.training.network_training.nnUNetTrainerV2.maybe_mkdir_p", "nnUNetTrainerV2_DA3.nnUNetTrainerV2_DA3.process_plans", "nnUNetTrainerV2_DA3.nnUNetTrainerV2_DA3.setup_DA_params", "len", "numpy.array", "numpy.array", "nnunet.training.loss_functions.deep_supervision.MultipleOutputLoss2", "batchgenerators.utilities.file_and_folder_operations.join", "nnUNetTrainerV2_DA3.nnUNetTrainerV2_DA3.initialize_network", "nnUNetTrainerV2_DA3.nnUNetTrainerV2_DA3.initialize_optimizer_and_scheduler", "isinstance", "nnUNetTrainerV2_DA3.nnUNetTrainerV2_DA3.print_to_log_file", "nnUNetTrainerV2_DA3.nnUNetTrainerV2_DA3.load_plans_file", "numpy.array.sum", "nnUNetTrainerV2_DA3.nnUNetTrainerV2_DA3.get_basic_generators", "nnUNetTrainerV2_DA3.get_insaneDA_augmentation2", "nnUNetTrainerV2_DA3.nnUNetTrainerV2_DA3.print_to_log_file", "nnUNetTrainerV2_DA3.nnUNetTrainerV2_DA3.print_to_log_file", "print", "nnunet.training.dataloading.dataset_loading.unpack_dataset", "print", "print", "range", "str", "str", "range", "nnUNetTrainerV2_DA3.nnUNetTrainerV2_DA3.dataset_tr.keys", "nnUNetTrainerV2_DA3.nnUNetTrainerV2_DA3.dataset_val.keys"], "methods", ["home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.BraTS_trainer.nnUNetTrainerV2BraTSRegions.nnUNetTrainerV2BraTSRegions_DA3_BN.process_plans", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.BraTS_trainer.nnUNetTrainerV2BraTSRegions.nnUNetTrainerV2BraTSRegions_DA4_BN.setup_DA_params", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.BraTS_trainer.nnUNetTrainerV2BraTSRegions.nnUNetTrainerV2BraTSRegions_DA3.initialize_network", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.optimizer_and_lr.nnUNetTrainerV2_momentum09in2D.nnUNetTrainerV2_momentum09in2D.initialize_optimizer_and_scheduler", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.network_training.nnUNetTrainerV2_DDP.nnUNetTrainerV2_DDP.print_to_log_file", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.network_training.nnUNetTrainer.nnUNetTrainer.load_plans_file", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.nnUNet_variants.nnUNetTrainerNoDA.nnUNetTrainerNoDA.get_basic_generators", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.data_augmentation.nnUNetTrainerV2_DA3.get_insaneDA_augmentation2", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.network_training.nnUNetTrainerV2_DDP.nnUNetTrainerV2_DDP.print_to_log_file", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.network_training.nnUNetTrainerV2_DDP.nnUNetTrainerV2_DDP.print_to_log_file", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.dataloading.dataset_loading.unpack_dataset"], ["", "def", "initialize", "(", "self", ",", "training", "=", "True", ",", "force_load_plans", "=", "False", ")", ":", "\n", "        ", "if", "not", "self", ".", "was_initialized", ":", "\n", "            ", "maybe_mkdir_p", "(", "self", ".", "output_folder", ")", "\n", "\n", "if", "force_load_plans", "or", "(", "self", ".", "plans", "is", "None", ")", ":", "\n", "                ", "self", ".", "load_plans_file", "(", ")", "\n", "\n", "", "self", ".", "process_plans", "(", "self", ".", "plans", ")", "\n", "\n", "self", ".", "setup_DA_params", "(", ")", "\n", "\n", "################# Here we wrap the loss for deep supervision ############", "\n", "# we need to know the number of outputs of the network", "\n", "net_numpool", "=", "len", "(", "self", ".", "net_num_pool_op_kernel_sizes", ")", "\n", "\n", "# we give each output a weight which decreases exponentially (division by 2) as the resolution decreases", "\n", "# this gives higher resolution outputs more weight in the loss", "\n", "weights", "=", "np", ".", "array", "(", "[", "1", "/", "(", "2", "**", "i", ")", "for", "i", "in", "range", "(", "net_numpool", ")", "]", ")", "\n", "\n", "# we don't use the lowest 2 outputs. Normalize weights so that they sum to 1", "\n", "mask", "=", "np", ".", "array", "(", "[", "True", "]", "+", "[", "True", "if", "i", "<", "net_numpool", "-", "1", "else", "False", "for", "i", "in", "range", "(", "1", ",", "net_numpool", ")", "]", ")", "\n", "weights", "[", "~", "mask", "]", "=", "0", "\n", "weights", "=", "weights", "/", "weights", ".", "sum", "(", ")", "\n", "self", ".", "ds_loss_weights", "=", "weights", "\n", "# now wrap the loss", "\n", "self", ".", "loss", "=", "MultipleOutputLoss2", "(", "self", ".", "loss", ",", "self", ".", "ds_loss_weights", ")", "\n", "################# END ###################", "\n", "\n", "self", ".", "folder_with_preprocessed_data", "=", "join", "(", "self", ".", "dataset_directory", ",", "self", ".", "plans", "[", "'data_identifier'", "]", "+", "\n", "\"_stage%d\"", "%", "self", ".", "stage", ")", "\n", "if", "training", ":", "\n", "                ", "self", ".", "dl_tr", ",", "self", ".", "dl_val", "=", "self", ".", "get_basic_generators", "(", ")", "\n", "if", "self", ".", "unpack_data", ":", "\n", "                    ", "print", "(", "\"unpacking dataset\"", ")", "\n", "unpack_dataset", "(", "self", ".", "folder_with_preprocessed_data", ")", "\n", "print", "(", "\"done\"", ")", "\n", "", "else", ":", "\n", "                    ", "print", "(", "\n", "\"INFO: Not unpacking data! Training may be slow due to that. Pray you are not using 2d or you \"", "\n", "\"will wait all winter for your model to finish!\"", ")", "\n", "\n", "", "self", ".", "tr_gen", ",", "self", ".", "val_gen", "=", "get_insaneDA_augmentation2", "(", "\n", "self", ".", "dl_tr", ",", "self", ".", "dl_val", ",", "\n", "self", ".", "data_aug_params", "[", "\n", "'patch_size_for_spatialtransform'", "]", ",", "\n", "self", ".", "data_aug_params", ",", "\n", "deep_supervision_scales", "=", "self", ".", "deep_supervision_scales", ",", "\n", "pin_memory", "=", "self", ".", "pin_memory", "\n", ")", "\n", "self", ".", "print_to_log_file", "(", "\"TRAINING KEYS:\\n %s\"", "%", "(", "str", "(", "self", ".", "dataset_tr", ".", "keys", "(", ")", ")", ")", ",", "\n", "also_print_to_console", "=", "False", ")", "\n", "self", ".", "print_to_log_file", "(", "\"VALIDATION KEYS:\\n %s\"", "%", "(", "str", "(", "self", ".", "dataset_val", ".", "keys", "(", ")", ")", ")", ",", "\n", "also_print_to_console", "=", "False", ")", "\n", "", "else", ":", "\n", "                ", "pass", "\n", "\n", "", "self", ".", "initialize_network", "(", ")", "\n", "self", ".", "initialize_optimizer_and_scheduler", "(", ")", "\n", "\n", "assert", "isinstance", "(", "self", ".", "network", ",", "(", "SegmentationNetwork", ",", "nn", ".", "DataParallel", ")", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "print_to_log_file", "(", "'self.was_initialized is True, not running self.initialize again'", ")", "\n", "", "self", ".", "was_initialized", "=", "True", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.data_augmentation.nnUNetTrainerV2_DA3.nnUNetTrainerV2_DA3_BN.initialize_network": [[326, 350], ["nnunet.network_architecture.generic_UNet.Generic_UNet", "torch.cuda.is_available", "len", "nnunet.network_architecture.initialization.InitWeights_He", "nnUNetTrainerV2_DA3.nnUNetTrainerV2_DA3_BN.network.cuda"], "methods", ["None"], ["    ", "def", "initialize_network", "(", "self", ")", ":", "\n", "        ", "if", "self", ".", "threeD", ":", "\n", "            ", "conv_op", "=", "nn", ".", "Conv3d", "\n", "dropout_op", "=", "nn", ".", "Dropout3d", "\n", "norm_op", "=", "nn", ".", "BatchNorm3d", "\n", "\n", "", "else", ":", "\n", "            ", "conv_op", "=", "nn", ".", "Conv2d", "\n", "dropout_op", "=", "nn", ".", "Dropout2d", "\n", "norm_op", "=", "nn", ".", "BatchNorm2d", "\n", "\n", "", "norm_op_kwargs", "=", "{", "'eps'", ":", "1e-5", ",", "'affine'", ":", "True", "}", "\n", "dropout_op_kwargs", "=", "{", "'p'", ":", "0", ",", "'inplace'", ":", "True", "}", "\n", "net_nonlin", "=", "nn", ".", "LeakyReLU", "\n", "net_nonlin_kwargs", "=", "{", "'negative_slope'", ":", "1e-2", ",", "'inplace'", ":", "True", "}", "\n", "self", ".", "network", "=", "Generic_UNet", "(", "self", ".", "num_input_channels", ",", "self", ".", "base_num_features", ",", "self", ".", "num_classes", ",", "\n", "len", "(", "self", ".", "net_num_pool_op_kernel_sizes", ")", ",", "\n", "self", ".", "conv_per_stage", ",", "2", ",", "conv_op", ",", "norm_op", ",", "norm_op_kwargs", ",", "dropout_op", ",", "\n", "dropout_op_kwargs", ",", "\n", "net_nonlin", ",", "net_nonlin_kwargs", ",", "True", ",", "False", ",", "lambda", "x", ":", "x", ",", "InitWeights_He", "(", "1e-2", ")", ",", "\n", "self", ".", "net_num_pool_op_kernel_sizes", ",", "self", ".", "net_conv_kernel_sizes", ",", "False", ",", "True", ",", "True", ")", "\n", "if", "torch", ".", "cuda", ".", "is_available", "(", ")", ":", "\n", "            ", "self", ".", "network", ".", "cuda", "(", ")", "\n", "", "self", ".", "network", ".", "inference_apply_nonlin", "=", "softmax_helper", "\n", "", "", ""]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.data_augmentation.nnUNetTrainerV2_DA3.get_insaneDA_augmentation2": [[41, 185], ["batchgenerators.transforms.Compose.append", "params.get", "batchgenerators.transforms.Compose.append", "batchgenerators.transforms.Compose.append", "batchgenerators.transforms.Compose.append", "batchgenerators.transforms.Compose.append", "batchgenerators.transforms.Compose.append", "batchgenerators.transforms.Compose.append", "params.get", "params.get", "batchgenerators.transforms.Compose.append", "batchgenerators.transforms.Compose.append", "batchgenerators.transforms.Compose.append", "batchgenerators.transforms.Compose", "batchgenerators.dataloading.MultiThreadedAugmenter", "batchgenerators.transforms.Compose.append", "batchgenerators.transforms.Compose.append", "batchgenerators.transforms.Compose.append", "batchgenerators.transforms.Compose", "batchgenerators.dataloading.MultiThreadedAugmenter", "params.get", "params.get", "batchgenerators.transforms.Compose.append", "params.get", "batchgenerators.transforms.Compose.append", "params.get", "batchgenerators.transforms.Compose.append", "batchgenerators.transforms.spatial_transforms.SpatialTransform_2", "batchgenerators.transforms.Compose.append", "batchgenerators.transforms.GaussianNoiseTransform", "batchgenerators.transforms.GaussianBlurTransform", "batchgenerators.transforms.BrightnessMultiplicativeTransform", "batchgenerators.transforms.ContrastAugmentationTransform", "batchgenerators.transforms.SimulateLowResolutionTransform", "batchgenerators.transforms.GammaTransform", "batchgenerators.transforms.Compose.append", "batchgenerators.transforms.Compose.append", "params.get", "params.get", "batchgenerators.transforms.Compose.append", "params.get", "params.get", "batchgenerators.transforms.Compose.append", "batchgenerators.transforms.utility_transforms.RemoveLabelTransform", "params.get", "batchgenerators.transforms.Compose.append", "batchgenerators.transforms.utility_transforms.RenameTransform", "batchgenerators.transforms.Compose.append", "batchgenerators.transforms.utility_transforms.NumpyToTensor", "params.get", "params.get", "batchgenerators.transforms.utility_transforms.RemoveLabelTransform", "params.get", "batchgenerators.transforms.Compose.append", "params.get", "batchgenerators.transforms.Compose.append", "params.get", "batchgenerators.transforms.Compose.append", "batchgenerators.transforms.utility_transforms.RenameTransform", "batchgenerators.transforms.Compose.append", "batchgenerators.transforms.utility_transforms.NumpyToTensor", "max", "params.get", "batchgenerators.transforms.DataChannelSelectionTransform", "batchgenerators.transforms.SegChannelSelectionTransform", "params.get", "nnunet.training.data_augmentation.custom_transforms.Convert3DTo2DTransform", "nnunet.training.data_augmentation.custom_transforms.Convert2DTo3DTransform", "params.get", "batchgenerators.transforms.BrightnessTransform", "batchgenerators.transforms.GammaTransform", "batchgenerators.transforms.MirrorTransform", "nnunet.training.data_augmentation.custom_transforms.MaskTransform", "params.get", "nnunet.training.data_augmentation.pyramid_augmentations.MoveSegAsOneHotToData", "params.get", "params.get", "nnunet.training.data_augmentation.custom_transforms.ConvertSegmentationToRegionsTransform", "batchgenerators.transforms.Compose.append", "batchgenerators.transforms.Compose.append", "batchgenerators.transforms.DataChannelSelectionTransform", "batchgenerators.transforms.SegChannelSelectionTransform", "params.get", "nnunet.training.data_augmentation.pyramid_augmentations.MoveSegAsOneHotToData", "nnunet.training.data_augmentation.custom_transforms.ConvertSegmentationToRegionsTransform", "batchgenerators.transforms.Compose.append", "batchgenerators.transforms.Compose.append", "params.get", "params.get", "params.get", "params.get", "params.get", "params.get", "params.get", "params.get", "params.get", "params.get", "params.get", "params.get", "params.get", "params.get", "params.get", "params.get", "params.get", "params.get", "params.get", "params.get", "params.get", "params.get", "params.get", "params.get", "batchgenerators.transforms.Compose.append", "params.get", "batchgenerators.transforms.Compose.append", "nnunet.training.data_augmentation.downsampling.DownsampleSegForDSTransform3", "nnunet.training.data_augmentation.downsampling.DownsampleSegForDSTransform2", "params.get", "params.get", "params.get", "nnunet.training.data_augmentation.downsampling.DownsampleSegForDSTransform3", "nnunet.training.data_augmentation.downsampling.DownsampleSegForDSTransform2", "params.get", "params.get", "params.get", "params.get", "nnunet.training.data_augmentation.pyramid_augmentations.ApplyRandomBinaryOperatorTransform", "nnunet.training.data_augmentation.pyramid_augmentations.RemoveRandomConnectedComponentFromOneHotEncodingTransform", "list", "params.get", "params.get", "list", "params.get", "params.get", "params.get", "range", "range", "len", "len", "params.get", "params.get"], "function", ["None"], ["def", "get_insaneDA_augmentation2", "(", "dataloader_train", ",", "dataloader_val", ",", "patch_size", ",", "params", "=", "default_3D_augmentation_params", ",", "\n", "border_val_seg", "=", "-", "1", ",", "\n", "seeds_train", "=", "None", ",", "seeds_val", "=", "None", ",", "order_seg", "=", "1", ",", "order_data", "=", "3", ",", "deep_supervision_scales", "=", "None", ",", "\n", "soft_ds", "=", "False", ",", "\n", "classes", "=", "None", ",", "pin_memory", "=", "True", ",", "regions", "=", "None", ")", ":", "\n", "    ", "assert", "params", ".", "get", "(", "'mirror'", ")", "is", "None", ",", "\"old version of params, use new keyword do_mirror\"", "\n", "\n", "tr_transforms", "=", "[", "]", "\n", "\n", "if", "params", ".", "get", "(", "\"selected_data_channels\"", ")", "is", "not", "None", ":", "\n", "        ", "tr_transforms", ".", "append", "(", "DataChannelSelectionTransform", "(", "params", ".", "get", "(", "\"selected_data_channels\"", ")", ")", ")", "\n", "\n", "", "if", "params", ".", "get", "(", "\"selected_seg_channels\"", ")", "is", "not", "None", ":", "\n", "        ", "tr_transforms", ".", "append", "(", "SegChannelSelectionTransform", "(", "params", ".", "get", "(", "\"selected_seg_channels\"", ")", ")", ")", "\n", "\n", "# don't do color augmentations while in 2d mode with 3d data because the color channel is overloaded!!", "\n", "", "if", "params", ".", "get", "(", "\"dummy_2D\"", ")", "is", "not", "None", "and", "params", ".", "get", "(", "\"dummy_2D\"", ")", ":", "\n", "        ", "ignore_axes", "=", "(", "0", ",", ")", "\n", "tr_transforms", ".", "append", "(", "Convert3DTo2DTransform", "(", ")", ")", "\n", "", "else", ":", "\n", "        ", "ignore_axes", "=", "None", "\n", "\n", "", "tr_transforms", ".", "append", "(", "SpatialTransform_2", "(", "\n", "patch_size", ",", "patch_center_dist_from_border", "=", "None", ",", "do_elastic_deform", "=", "params", ".", "get", "(", "\"do_elastic\"", ")", ",", "\n", "deformation_scale", "=", "params", ".", "get", "(", "\"eldef_deformation_scale\"", ")", ",", "\n", "do_rotation", "=", "params", ".", "get", "(", "\"do_rotation\"", ")", ",", "angle_x", "=", "params", ".", "get", "(", "\"rotation_x\"", ")", ",", "angle_y", "=", "params", ".", "get", "(", "\"rotation_y\"", ")", ",", "\n", "angle_z", "=", "params", ".", "get", "(", "\"rotation_z\"", ")", ",", "do_scale", "=", "params", ".", "get", "(", "\"do_scaling\"", ")", ",", "scale", "=", "params", ".", "get", "(", "\"scale_range\"", ")", ",", "\n", "border_mode_data", "=", "params", ".", "get", "(", "\"border_mode_data\"", ")", ",", "border_cval_data", "=", "0", ",", "order_data", "=", "order_data", ",", "\n", "border_mode_seg", "=", "\"constant\"", ",", "border_cval_seg", "=", "border_val_seg", ",", "\n", "order_seg", "=", "order_seg", ",", "random_crop", "=", "params", ".", "get", "(", "\"random_crop\"", ")", ",", "p_el_per_sample", "=", "params", ".", "get", "(", "\"p_eldef\"", ")", ",", "\n", "p_scale_per_sample", "=", "params", ".", "get", "(", "\"p_scale\"", ")", ",", "p_rot_per_sample", "=", "params", ".", "get", "(", "\"p_rot\"", ")", ",", "\n", "independent_scale_for_each_axis", "=", "params", ".", "get", "(", "\"independent_scale_factor_for_each_axis\"", ")", ",", "\n", "p_independent_scale_per_axis", "=", "params", ".", "get", "(", "\"p_independent_scale_per_axis\"", ")", "\n", ")", ")", "\n", "\n", "if", "params", ".", "get", "(", "\"dummy_2D\"", ")", ":", "\n", "        ", "tr_transforms", ".", "append", "(", "Convert2DTo3DTransform", "(", ")", ")", "\n", "\n", "# we need to put the color augmentations after the dummy 2d part (if applicable). Otherwise the overloaded color", "\n", "# channel gets in the way", "\n", "", "tr_transforms", ".", "append", "(", "GaussianNoiseTransform", "(", "p_per_sample", "=", "0.15", ")", ")", "\n", "tr_transforms", ".", "append", "(", "GaussianBlurTransform", "(", "(", "0.5", ",", "1.5", ")", ",", "different_sigma_per_channel", "=", "True", ",", "p_per_sample", "=", "0.2", ",", "\n", "p_per_channel", "=", "0.5", ")", ")", "\n", "tr_transforms", ".", "append", "(", "BrightnessMultiplicativeTransform", "(", "multiplier_range", "=", "(", "0.70", ",", "1.3", ")", ",", "p_per_sample", "=", "0.15", ")", ")", "\n", "tr_transforms", ".", "append", "(", "ContrastAugmentationTransform", "(", "contrast_range", "=", "(", "0.65", ",", "1.5", ")", ",", "p_per_sample", "=", "0.15", ")", ")", "\n", "tr_transforms", ".", "append", "(", "SimulateLowResolutionTransform", "(", "zoom_range", "=", "(", "0.5", ",", "1", ")", ",", "per_channel", "=", "True", ",", "\n", "p_per_channel", "=", "0.5", ",", "\n", "order_downsample", "=", "0", ",", "order_upsample", "=", "3", ",", "p_per_sample", "=", "0.25", ",", "\n", "ignore_axes", "=", "ignore_axes", ")", ")", "\n", "tr_transforms", ".", "append", "(", "\n", "GammaTransform", "(", "params", ".", "get", "(", "\"gamma_range\"", ")", ",", "True", ",", "True", ",", "retain_stats", "=", "params", ".", "get", "(", "\"gamma_retain_stats\"", ")", ",", "\n", "p_per_sample", "=", "0.15", ")", ")", "# inverted gamma", "\n", "\n", "if", "params", ".", "get", "(", "\"do_additive_brightness\"", ")", ":", "\n", "        ", "tr_transforms", ".", "append", "(", "BrightnessTransform", "(", "params", ".", "get", "(", "\"additive_brightness_mu\"", ")", ",", "\n", "params", ".", "get", "(", "\"additive_brightness_sigma\"", ")", ",", "\n", "True", ",", "p_per_sample", "=", "params", ".", "get", "(", "\"additive_brightness_p_per_sample\"", ")", ",", "\n", "p_per_channel", "=", "params", ".", "get", "(", "\"additive_brightness_p_per_channel\"", ")", ")", ")", "\n", "\n", "", "if", "params", ".", "get", "(", "\"do_gamma\"", ")", ":", "\n", "        ", "tr_transforms", ".", "append", "(", "\n", "GammaTransform", "(", "params", ".", "get", "(", "\"gamma_range\"", ")", ",", "False", ",", "True", ",", "retain_stats", "=", "params", ".", "get", "(", "\"gamma_retain_stats\"", ")", ",", "\n", "p_per_sample", "=", "params", "[", "\"p_gamma\"", "]", ")", ")", "\n", "\n", "", "if", "params", ".", "get", "(", "\"do_mirror\"", ")", "or", "params", ".", "get", "(", "\"mirror\"", ")", ":", "\n", "        ", "tr_transforms", ".", "append", "(", "MirrorTransform", "(", "params", ".", "get", "(", "\"mirror_axes\"", ")", ")", ")", "\n", "\n", "", "if", "params", ".", "get", "(", "\"mask_was_used_for_normalization\"", ")", "is", "not", "None", ":", "\n", "        ", "mask_was_used_for_normalization", "=", "params", ".", "get", "(", "\"mask_was_used_for_normalization\"", ")", "\n", "tr_transforms", ".", "append", "(", "MaskTransform", "(", "mask_was_used_for_normalization", ",", "mask_idx_in_seg", "=", "0", ",", "set_outside_to", "=", "0", ")", ")", "\n", "\n", "", "tr_transforms", ".", "append", "(", "RemoveLabelTransform", "(", "-", "1", ",", "0", ")", ")", "\n", "\n", "if", "params", ".", "get", "(", "\"move_last_seg_chanel_to_data\"", ")", "is", "not", "None", "and", "params", ".", "get", "(", "\"move_last_seg_chanel_to_data\"", ")", ":", "\n", "        ", "tr_transforms", ".", "append", "(", "MoveSegAsOneHotToData", "(", "1", ",", "params", ".", "get", "(", "\"all_segmentation_labels\"", ")", ",", "'seg'", ",", "'data'", ")", ")", "\n", "if", "params", ".", "get", "(", "\"cascade_do_cascade_augmentations\"", ")", "and", "not", "None", "and", "params", ".", "get", "(", "\n", "\"cascade_do_cascade_augmentations\"", ")", ":", "\n", "            ", "if", "params", ".", "get", "(", "\"cascade_random_binary_transform_p\"", ")", ">", "0", ":", "\n", "                ", "tr_transforms", ".", "append", "(", "ApplyRandomBinaryOperatorTransform", "(", "\n", "channel_idx", "=", "list", "(", "range", "(", "-", "len", "(", "params", ".", "get", "(", "\"all_segmentation_labels\"", ")", ")", ",", "0", ")", ")", ",", "\n", "p_per_sample", "=", "params", ".", "get", "(", "\"cascade_random_binary_transform_p\"", ")", ",", "\n", "key", "=", "\"data\"", ",", "\n", "strel_size", "=", "params", ".", "get", "(", "\"cascade_random_binary_transform_size\"", ")", ")", ")", "\n", "", "if", "params", ".", "get", "(", "\"cascade_remove_conn_comp_p\"", ")", ">", "0", ":", "\n", "                ", "tr_transforms", ".", "append", "(", "\n", "RemoveRandomConnectedComponentFromOneHotEncodingTransform", "(", "\n", "channel_idx", "=", "list", "(", "range", "(", "-", "len", "(", "params", ".", "get", "(", "\"all_segmentation_labels\"", ")", ")", ",", "0", ")", ")", ",", "\n", "key", "=", "\"data\"", ",", "\n", "p_per_sample", "=", "params", ".", "get", "(", "\"cascade_remove_conn_comp_p\"", ")", ",", "\n", "fill_with_other_class_p", "=", "params", ".", "get", "(", "\"cascade_remove_conn_comp_max_size_percent_threshold\"", ")", ",", "\n", "dont_do_if_covers_more_than_X_percent", "=", "params", ".", "get", "(", "\n", "\"cascade_remove_conn_comp_fill_with_other_class_p\"", ")", ")", ")", "\n", "\n", "", "", "", "tr_transforms", ".", "append", "(", "RenameTransform", "(", "'seg'", ",", "'target'", ",", "True", ")", ")", "\n", "\n", "if", "regions", "is", "not", "None", ":", "\n", "        ", "tr_transforms", ".", "append", "(", "ConvertSegmentationToRegionsTransform", "(", "regions", ",", "'target'", ",", "'target'", ")", ")", "\n", "\n", "", "if", "deep_supervision_scales", "is", "not", "None", ":", "\n", "        ", "if", "soft_ds", ":", "\n", "            ", "assert", "classes", "is", "not", "None", "\n", "tr_transforms", ".", "append", "(", "DownsampleSegForDSTransform3", "(", "deep_supervision_scales", ",", "'target'", ",", "'target'", ",", "classes", ")", ")", "\n", "", "else", ":", "\n", "            ", "tr_transforms", ".", "append", "(", "DownsampleSegForDSTransform2", "(", "deep_supervision_scales", ",", "0", ",", "0", ",", "input_key", "=", "'target'", ",", "\n", "output_key", "=", "'target'", ")", ")", "\n", "\n", "", "", "tr_transforms", ".", "append", "(", "NumpyToTensor", "(", "[", "'data'", ",", "'target'", "]", ",", "'float'", ")", ")", "\n", "tr_transforms", "=", "Compose", "(", "tr_transforms", ")", "\n", "\n", "batchgenerator_train", "=", "MultiThreadedAugmenter", "(", "dataloader_train", ",", "tr_transforms", ",", "params", ".", "get", "(", "'num_threads'", ")", ",", "\n", "params", ".", "get", "(", "\"num_cached_per_thread\"", ")", ",", "\n", "seeds", "=", "seeds_train", ",", "pin_memory", "=", "pin_memory", ")", "\n", "#batchgenerator_train = SingleThreadedAugmenter(dataloader_train, tr_transforms)", "\n", "\n", "val_transforms", "=", "[", "]", "\n", "val_transforms", ".", "append", "(", "RemoveLabelTransform", "(", "-", "1", ",", "0", ")", ")", "\n", "if", "params", ".", "get", "(", "\"selected_data_channels\"", ")", "is", "not", "None", ":", "\n", "        ", "val_transforms", ".", "append", "(", "DataChannelSelectionTransform", "(", "params", ".", "get", "(", "\"selected_data_channels\"", ")", ")", ")", "\n", "", "if", "params", ".", "get", "(", "\"selected_seg_channels\"", ")", "is", "not", "None", ":", "\n", "        ", "val_transforms", ".", "append", "(", "SegChannelSelectionTransform", "(", "params", ".", "get", "(", "\"selected_seg_channels\"", ")", ")", ")", "\n", "\n", "", "if", "params", ".", "get", "(", "\"move_last_seg_chanel_to_data\"", ")", "is", "not", "None", "and", "params", ".", "get", "(", "\"move_last_seg_chanel_to_data\"", ")", ":", "\n", "        ", "val_transforms", ".", "append", "(", "MoveSegAsOneHotToData", "(", "1", ",", "params", ".", "get", "(", "\"all_segmentation_labels\"", ")", ",", "'seg'", ",", "'data'", ")", ")", "\n", "\n", "", "val_transforms", ".", "append", "(", "RenameTransform", "(", "'seg'", ",", "'target'", ",", "True", ")", ")", "\n", "\n", "if", "regions", "is", "not", "None", ":", "\n", "        ", "val_transforms", ".", "append", "(", "ConvertSegmentationToRegionsTransform", "(", "regions", ",", "'target'", ",", "'target'", ")", ")", "\n", "\n", "", "if", "deep_supervision_scales", "is", "not", "None", ":", "\n", "        ", "if", "soft_ds", ":", "\n", "            ", "assert", "classes", "is", "not", "None", "\n", "val_transforms", ".", "append", "(", "DownsampleSegForDSTransform3", "(", "deep_supervision_scales", ",", "'target'", ",", "'target'", ",", "classes", ")", ")", "\n", "", "else", ":", "\n", "            ", "val_transforms", ".", "append", "(", "DownsampleSegForDSTransform2", "(", "deep_supervision_scales", ",", "0", ",", "0", ",", "input_key", "=", "'target'", ",", "\n", "output_key", "=", "'target'", ")", ")", "\n", "\n", "", "", "val_transforms", ".", "append", "(", "NumpyToTensor", "(", "[", "'data'", ",", "'target'", "]", ",", "'float'", ")", ")", "\n", "val_transforms", "=", "Compose", "(", "val_transforms", ")", "\n", "\n", "batchgenerator_val", "=", "MultiThreadedAugmenter", "(", "dataloader_val", ",", "val_transforms", ",", "max", "(", "params", ".", "get", "(", "'num_threads'", ")", "//", "2", ",", "1", ")", ",", "\n", "params", ".", "get", "(", "\"num_cached_per_thread\"", ")", ",", "\n", "seeds", "=", "seeds_val", ",", "pin_memory", "=", "pin_memory", ")", "\n", "return", "batchgenerator_train", ",", "batchgenerator_val", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.data_augmentation.nnUNetTrainerV2_noMirroring.nnUNetTrainerV2_noMirroring.validate": [[20, 50], ["super().validate", "print"], "methods", ["home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.BraTS_trainer.nnUNetTrainerV2BraTSRegions.nnUNetTrainerV2BraTSRegions_DA3_BN.validate"], ["    ", "def", "validate", "(", "self", ",", "do_mirroring", ":", "bool", "=", "True", ",", "use_sliding_window", ":", "bool", "=", "True", ",", "\n", "step_size", ":", "float", "=", "0.5", ",", "save_softmax", ":", "bool", "=", "True", ",", "use_gaussian", ":", "bool", "=", "True", ",", "overwrite", ":", "bool", "=", "True", ",", "\n", "validation_folder_name", ":", "str", "=", "'validation_raw'", ",", "debug", ":", "bool", "=", "False", ",", "all_in_gpu", ":", "bool", "=", "False", ",", "\n", "segmentation_export_kwargs", ":", "dict", "=", "None", ")", ":", "\n", "        ", "\"\"\"\n        We need to wrap this because we need to enforce self.network.do_ds = False for prediction\n\n        :param do_mirroring:\n        :param use_train_mode:\n        :param use_sliding_window:\n        :param step_size:\n        :param save_softmax:\n        :param use_gaussian:\n        :param compute_global_dice:\n        :param overwrite:\n        :param validation_folder_name:\n        :return:\n        \"\"\"", "\n", "ds", "=", "self", ".", "network", ".", "do_ds", "\n", "if", "do_mirroring", ":", "\n", "            ", "print", "(", "\"WARNING! do_mirroring was True but we cannot do that because we trained without mirroring. \"", "\n", "\"do_mirroring was set to False\"", ")", "\n", "", "do_mirroring", "=", "False", "\n", "self", ".", "network", ".", "do_ds", "=", "False", "\n", "ret", "=", "super", "(", ")", ".", "validate", "(", "do_mirroring", ",", "use_sliding_window", ",", "\n", "step_size", ",", "save_softmax", ",", "use_gaussian", ",", "overwrite", ",", "\n", "validation_folder_name", ",", "debug", ",", "all_in_gpu", ",", "\n", "segmentation_export_kwargs", ")", "\n", "self", ".", "network", ".", "do_ds", "=", "ds", "\n", "return", "ret", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.data_augmentation.nnUNetTrainerV2_noMirroring.nnUNetTrainerV2_noMirroring.setup_DA_params": [[51, 54], ["super().setup_DA_params"], "methods", ["home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.BraTS_trainer.nnUNetTrainerV2BraTSRegions.nnUNetTrainerV2BraTSRegions_DA4_BN.setup_DA_params"], ["", "def", "setup_DA_params", "(", "self", ")", ":", "\n", "        ", "super", "(", ")", ".", "setup_DA_params", "(", ")", "\n", "self", ".", "data_aug_params", "[", "\"do_mirror\"", "]", "=", "False", "\n", "", "", ""]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.data_augmentation.nnUNetTrainerV2_noDA.nnUNetTrainerV2_noDataAugmentation.setup_DA_params": [[27, 32], ["super().setup_DA_params", "tuple"], "methods", ["home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.BraTS_trainer.nnUNetTrainerV2BraTSRegions.nnUNetTrainerV2BraTSRegions_DA4_BN.setup_DA_params"], ["    ", "def", "setup_DA_params", "(", "self", ")", ":", "\n", "        ", "super", "(", ")", ".", "setup_DA_params", "(", ")", "\n", "# important because we need to know in validation and inference that we did not mirror in training", "\n", "self", ".", "data_aug_params", "[", "\"do_mirror\"", "]", "=", "False", "\n", "self", ".", "data_aug_params", "[", "\"mirror_axes\"", "]", "=", "tuple", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.data_augmentation.nnUNetTrainerV2_noDA.nnUNetTrainerV2_noDataAugmentation.get_basic_generators": [[33, 54], ["nnUNetTrainerV2_noDA.nnUNetTrainerV2_noDataAugmentation.load_dataset", "nnUNetTrainerV2_noDA.nnUNetTrainerV2_noDataAugmentation.do_split", "nnunet.training.dataloading.dataset_loading.DataLoader3D", "nnunet.training.dataloading.dataset_loading.DataLoader3D", "nnunet.training.dataloading.dataset_loading.DataLoader2D", "nnunet.training.dataloading.dataset_loading.DataLoader2D", "nnUNetTrainerV2_noDA.nnUNetTrainerV2_noDataAugmentation.plans.get", "nnUNetTrainerV2_noDA.nnUNetTrainerV2_noDataAugmentation.plans.get"], "methods", ["home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.dataloading.dataset_loading.load_dataset", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.network_training.nnUNetTrainerV2.nnUNetTrainerV2.do_split"], ["", "def", "get_basic_generators", "(", "self", ")", ":", "\n", "        ", "self", ".", "load_dataset", "(", ")", "\n", "self", ".", "do_split", "(", ")", "\n", "\n", "if", "self", ".", "threeD", ":", "\n", "            ", "dl_tr", "=", "DataLoader3D", "(", "self", ".", "dataset_tr", ",", "self", ".", "patch_size", ",", "self", ".", "patch_size", ",", "self", ".", "batch_size", ",", "\n", "False", ",", "oversample_foreground_percent", "=", "self", ".", "oversample_foreground_percent", "\n", ",", "pad_mode", "=", "\"constant\"", ",", "pad_sides", "=", "self", ".", "pad_all_sides", ")", "\n", "dl_val", "=", "DataLoader3D", "(", "self", ".", "dataset_val", ",", "self", ".", "patch_size", ",", "self", ".", "patch_size", ",", "self", ".", "batch_size", ",", "False", ",", "\n", "oversample_foreground_percent", "=", "self", ".", "oversample_foreground_percent", ",", "\n", "pad_mode", "=", "\"constant\"", ",", "pad_sides", "=", "self", ".", "pad_all_sides", ")", "\n", "", "else", ":", "\n", "            ", "dl_tr", "=", "DataLoader2D", "(", "self", ".", "dataset_tr", ",", "self", ".", "patch_size", ",", "self", ".", "patch_size", ",", "self", ".", "batch_size", ",", "\n", "transpose", "=", "self", ".", "plans", ".", "get", "(", "'transpose_forward'", ")", ",", "\n", "oversample_foreground_percent", "=", "self", ".", "oversample_foreground_percent", "\n", ",", "pad_mode", "=", "\"constant\"", ",", "pad_sides", "=", "self", ".", "pad_all_sides", ")", "\n", "dl_val", "=", "DataLoader2D", "(", "self", ".", "dataset_val", ",", "self", ".", "patch_size", ",", "self", ".", "patch_size", ",", "self", ".", "batch_size", ",", "\n", "transpose", "=", "self", ".", "plans", ".", "get", "(", "'transpose_forward'", ")", ",", "\n", "oversample_foreground_percent", "=", "self", ".", "oversample_foreground_percent", ",", "\n", "pad_mode", "=", "\"constant\"", ",", "pad_sides", "=", "self", ".", "pad_all_sides", ")", "\n", "", "return", "dl_tr", ",", "dl_val", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.data_augmentation.nnUNetTrainerV2_noDA.nnUNetTrainerV2_noDataAugmentation.initialize": [[55, 116], ["batchgenerators.utilities.file_and_folder_operations.maybe_mkdir_p", "nnUNetTrainerV2_noDA.nnUNetTrainerV2_noDataAugmentation.process_plans", "nnUNetTrainerV2_noDA.nnUNetTrainerV2_noDataAugmentation.setup_DA_params", "len", "numpy.array", "numpy.array", "nnunet.training.loss_functions.deep_supervision.MultipleOutputLoss2", "batchgenerators.utilities.file_and_folder_operations.join", "nnUNetTrainerV2_noDA.nnUNetTrainerV2_noDataAugmentation.initialize_network", "nnUNetTrainerV2_noDA.nnUNetTrainerV2_noDataAugmentation.initialize_optimizer_and_scheduler", "isinstance", "nnUNetTrainerV2_noDA.nnUNetTrainerV2_noDataAugmentation.print_to_log_file", "nnUNetTrainerV2_noDA.nnUNetTrainerV2_noDataAugmentation.load_plans_file", "numpy.array.sum", "nnUNetTrainerV2_noDA.nnUNetTrainerV2_noDataAugmentation.get_basic_generators", "nnunet.training.data_augmentation.default_data_augmentation.get_no_augmentation", "nnUNetTrainerV2_noDA.nnUNetTrainerV2_noDataAugmentation.print_to_log_file", "nnUNetTrainerV2_noDA.nnUNetTrainerV2_noDataAugmentation.print_to_log_file", "print", "nnunet.training.dataloading.dataset_loading.unpack_dataset", "print", "print", "range", "range", "str", "str", "nnUNetTrainerV2_noDA.nnUNetTrainerV2_noDataAugmentation.dataset_tr.keys", "nnUNetTrainerV2_noDA.nnUNetTrainerV2_noDataAugmentation.dataset_val.keys"], "methods", ["home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.BraTS_trainer.nnUNetTrainerV2BraTSRegions.nnUNetTrainerV2BraTSRegions_DA3_BN.process_plans", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.BraTS_trainer.nnUNetTrainerV2BraTSRegions.nnUNetTrainerV2BraTSRegions_DA4_BN.setup_DA_params", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.BraTS_trainer.nnUNetTrainerV2BraTSRegions.nnUNetTrainerV2BraTSRegions_DA3.initialize_network", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.optimizer_and_lr.nnUNetTrainerV2_momentum09in2D.nnUNetTrainerV2_momentum09in2D.initialize_optimizer_and_scheduler", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.network_training.nnUNetTrainerV2_DDP.nnUNetTrainerV2_DDP.print_to_log_file", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.network_training.nnUNetTrainer.nnUNetTrainer.load_plans_file", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.nnUNet_variants.nnUNetTrainerNoDA.nnUNetTrainerNoDA.get_basic_generators", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.data_augmentation.default_data_augmentation.get_no_augmentation", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.network_training.nnUNetTrainerV2_DDP.nnUNetTrainerV2_DDP.print_to_log_file", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.network_training.nnUNetTrainerV2_DDP.nnUNetTrainerV2_DDP.print_to_log_file", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.dataloading.dataset_loading.unpack_dataset"], ["", "def", "initialize", "(", "self", ",", "training", "=", "True", ",", "force_load_plans", "=", "False", ")", ":", "\n", "        ", "if", "not", "self", ".", "was_initialized", ":", "\n", "            ", "maybe_mkdir_p", "(", "self", ".", "output_folder", ")", "\n", "\n", "if", "force_load_plans", "or", "(", "self", ".", "plans", "is", "None", ")", ":", "\n", "                ", "self", ".", "load_plans_file", "(", ")", "\n", "\n", "", "self", ".", "process_plans", "(", "self", ".", "plans", ")", "\n", "\n", "self", ".", "setup_DA_params", "(", ")", "\n", "\n", "################# Here we wrap the loss for deep supervision ############", "\n", "# we need to know the number of outputs of the network", "\n", "net_numpool", "=", "len", "(", "self", ".", "net_num_pool_op_kernel_sizes", ")", "\n", "\n", "# we give each output a weight which decreases exponentially (division by 2) as the resolution decreases", "\n", "# this gives higher resolution outputs more weight in the loss", "\n", "weights", "=", "np", ".", "array", "(", "[", "1", "/", "(", "2", "**", "i", ")", "for", "i", "in", "range", "(", "net_numpool", ")", "]", ")", "\n", "\n", "# we don't use the lowest 2 outputs. Normalize weights so that they sum to 1", "\n", "mask", "=", "np", ".", "array", "(", "[", "True", "if", "i", "<", "net_numpool", "-", "1", "else", "False", "for", "i", "in", "range", "(", "net_numpool", ")", "]", ")", "\n", "weights", "[", "~", "mask", "]", "=", "0", "\n", "weights", "=", "weights", "/", "weights", ".", "sum", "(", ")", "\n", "\n", "# now wrap the loss", "\n", "self", ".", "loss", "=", "MultipleOutputLoss2", "(", "self", ".", "loss", ",", "weights", ")", "\n", "################# END ###################", "\n", "\n", "self", ".", "folder_with_preprocessed_data", "=", "join", "(", "self", ".", "dataset_directory", ",", "self", ".", "plans", "[", "'data_identifier'", "]", "+", "\n", "\"_stage%d\"", "%", "self", ".", "stage", ")", "\n", "if", "training", ":", "\n", "                ", "self", ".", "dl_tr", ",", "self", ".", "dl_val", "=", "self", ".", "get_basic_generators", "(", ")", "\n", "if", "self", ".", "unpack_data", ":", "\n", "                    ", "print", "(", "\"unpacking dataset\"", ")", "\n", "unpack_dataset", "(", "self", ".", "folder_with_preprocessed_data", ")", "\n", "print", "(", "\"done\"", ")", "\n", "", "else", ":", "\n", "                    ", "print", "(", "\n", "\"INFO: Not unpacking data! Training may be slow due to that. Pray you are not using 2d or you \"", "\n", "\"will wait all winter for your model to finish!\"", ")", "\n", "\n", "", "self", ".", "tr_gen", ",", "self", ".", "val_gen", "=", "get_no_augmentation", "(", "self", ".", "dl_tr", ",", "self", ".", "dl_val", ",", "\n", "self", ".", "data_aug_params", "[", "\n", "'patch_size_for_spatialtransform'", "]", ",", "\n", "self", ".", "data_aug_params", ",", "\n", "deep_supervision_scales", "=", "self", ".", "deep_supervision_scales", ",", "\n", "pin_memory", "=", "self", ".", "pin_memory", ")", "\n", "self", ".", "print_to_log_file", "(", "\"TRAINING KEYS:\\n %s\"", "%", "(", "str", "(", "self", ".", "dataset_tr", ".", "keys", "(", ")", ")", ")", ",", "\n", "also_print_to_console", "=", "False", ")", "\n", "self", ".", "print_to_log_file", "(", "\"VALIDATION KEYS:\\n %s\"", "%", "(", "str", "(", "self", ".", "dataset_val", ".", "keys", "(", ")", ")", ")", ",", "\n", "also_print_to_console", "=", "False", ")", "\n", "", "else", ":", "\n", "                ", "pass", "\n", "\n", "", "self", ".", "initialize_network", "(", ")", "\n", "self", ".", "initialize_optimizer_and_scheduler", "(", ")", "\n", "\n", "assert", "isinstance", "(", "self", ".", "network", ",", "(", "SegmentationNetwork", ",", "nn", ".", "DataParallel", ")", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "print_to_log_file", "(", "'self.was_initialized is True, not running self.initialize again'", ")", "\n", "", "self", ".", "was_initialized", "=", "True", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.data_augmentation.nnUNetTrainerV2_noDA.nnUNetTrainerV2_noDataAugmentation.validate": [[117, 135], ["super().validate", "print"], "methods", ["home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.BraTS_trainer.nnUNetTrainerV2BraTSRegions.nnUNetTrainerV2BraTSRegions_DA3_BN.validate"], ["", "def", "validate", "(", "self", ",", "do_mirroring", ":", "bool", "=", "True", ",", "use_sliding_window", ":", "bool", "=", "True", ",", "\n", "step_size", ":", "float", "=", "0.5", ",", "save_softmax", ":", "bool", "=", "True", ",", "use_gaussian", ":", "bool", "=", "True", ",", "overwrite", ":", "bool", "=", "True", ",", "\n", "validation_folder_name", ":", "str", "=", "'validation_raw'", ",", "debug", ":", "bool", "=", "False", ",", "all_in_gpu", ":", "bool", "=", "False", ",", "\n", "segmentation_export_kwargs", ":", "dict", "=", "None", ")", ":", "\n", "        ", "\"\"\"\n        We need to wrap this because we need to enforce self.network.do_ds = False for prediction\n\n        \"\"\"", "\n", "ds", "=", "self", ".", "network", ".", "do_ds", "\n", "if", "do_mirroring", ":", "\n", "            ", "print", "(", "\"WARNING! do_mirroring was True but we cannot do that because we trained without mirroring. \"", "\n", "\"do_mirroring was set to False\"", ")", "\n", "", "do_mirroring", "=", "False", "\n", "self", ".", "network", ".", "do_ds", "=", "False", "\n", "ret", "=", "super", "(", ")", ".", "validate", "(", "do_mirroring", ",", "use_sliding_window", ",", "step_size", ",", "save_softmax", ",", "use_gaussian", ",", "\n", "overwrite", ",", "validation_folder_name", ",", "debug", ",", "all_in_gpu", ",", "segmentation_export_kwargs", ")", "\n", "self", ".", "network", ".", "do_ds", "=", "ds", "\n", "return", "ret", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.network_training.network_trainer.NetworkTrainer.__init__": [[43, 128], ["numpy.random.seed", "torch.manual_seed", "torch.manual_seed", "torch.manual_seed", "torch.manual_seed", "torch.cuda.is_available", "torch.cuda.is_available", "torch.cuda.is_available", "torch.cuda.is_available", "os.environ.keys", "bool", "torch.cuda.manual_seed_all", "torch.cuda.manual_seed_all", "torch.cuda.manual_seed_all", "torch.cuda.manual_seed_all", "int"], "methods", ["None"], ["    ", "def", "__init__", "(", "self", ",", "deterministic", "=", "True", ",", "fp16", "=", "False", ")", ":", "\n", "        ", "\"\"\"\n        A generic class that can train almost any neural network (RNNs excluded). It provides basic functionality such\n        as the training loop, tracking of training and validation losses (and the target metric if you implement it)\n        Training can be terminated early if the validation loss (or the target metric if implemented) do not improve\n        anymore. This is based on a moving average (MA) of the loss/metric instead of the raw values to get more smooth\n        results.\n\n        What you need to override:\n        - __init__\n        - initialize\n        - run_online_evaluation (optional)\n        - finish_online_evaluation (optional)\n        - validate\n        - predict_test_case\n        \"\"\"", "\n", "self", ".", "fp16", "=", "fp16", "\n", "self", ".", "amp_grad_scaler", "=", "None", "\n", "\n", "if", "deterministic", ":", "\n", "            ", "np", ".", "random", ".", "seed", "(", "12345", ")", "\n", "torch", ".", "manual_seed", "(", "12345", ")", "\n", "if", "torch", ".", "cuda", ".", "is_available", "(", ")", ":", "\n", "                ", "torch", ".", "cuda", ".", "manual_seed_all", "(", "12345", ")", "\n", "", "cudnn", ".", "deterministic", "=", "True", "\n", "torch", ".", "backends", ".", "cudnn", ".", "benchmark", "=", "False", "\n", "", "else", ":", "\n", "            ", "cudnn", ".", "deterministic", "=", "False", "\n", "torch", ".", "backends", ".", "cudnn", ".", "benchmark", "=", "True", "\n", "\n", "################# SET THESE IN self.initialize() ###################################", "\n", "", "self", ".", "network", ":", "Tuple", "[", "SegmentationNetwork", ",", "nn", ".", "DataParallel", "]", "=", "None", "\n", "self", ".", "optimizer", "=", "None", "\n", "self", ".", "lr_scheduler", "=", "None", "\n", "self", ".", "tr_gen", "=", "self", ".", "val_gen", "=", "None", "\n", "self", ".", "was_initialized", "=", "False", "\n", "\n", "################# SET THESE IN INIT ################################################", "\n", "self", ".", "output_folder", "=", "None", "\n", "self", ".", "fold", "=", "None", "\n", "self", ".", "loss", "=", "None", "\n", "self", ".", "dataset_directory", "=", "None", "\n", "\n", "################# SET THESE IN LOAD_DATASET OR DO_SPLIT ############################", "\n", "self", ".", "dataset", "=", "None", "# these can be None for inference mode", "\n", "self", ".", "dataset_tr", "=", "self", ".", "dataset_val", "=", "None", "# do not need to be used, they just appear if you are using the suggested load_dataset_and_do_split", "\n", "\n", "################# THESE DO NOT NECESSARILY NEED TO BE MODIFIED #####################", "\n", "self", ".", "patience", "=", "50", "\n", "self", ".", "val_eval_criterion_alpha", "=", "0.9", "# alpha * old + (1-alpha) * new", "\n", "# if this is too low then the moving average will be too noisy and the training may terminate early. If it is", "\n", "# too high the training will take forever", "\n", "self", ".", "train_loss_MA_alpha", "=", "0.93", "# alpha * old + (1-alpha) * new", "\n", "self", ".", "train_loss_MA_eps", "=", "5e-4", "# new MA must be at least this much better (smaller)", "\n", "self", ".", "max_num_epochs", "=", "1000", "\n", "self", ".", "num_batches_per_epoch", "=", "250", "\n", "self", ".", "num_val_batches_per_epoch", "=", "50", "\n", "self", ".", "also_val_in_tr_mode", "=", "False", "\n", "self", ".", "lr_threshold", "=", "1e-6", "# the network will not terminate training if the lr is still above this threshold", "\n", "\n", "################# LEAVE THESE ALONE ################################################", "\n", "self", ".", "val_eval_criterion_MA", "=", "None", "\n", "self", ".", "train_loss_MA", "=", "None", "\n", "self", ".", "best_val_eval_criterion_MA", "=", "None", "\n", "self", ".", "best_MA_tr_loss_for_patience", "=", "None", "\n", "self", ".", "best_epoch_based_on_MA_tr_loss", "=", "None", "\n", "self", ".", "all_tr_losses", "=", "[", "]", "\n", "self", ".", "all_val_losses", "=", "[", "]", "\n", "self", ".", "all_val_losses_tr_mode", "=", "[", "]", "\n", "self", ".", "all_val_eval_metrics", "=", "[", "]", "# does not have to be used", "\n", "self", ".", "epoch", "=", "0", "\n", "self", ".", "log_file", "=", "None", "\n", "self", ".", "deterministic", "=", "deterministic", "\n", "\n", "self", ".", "use_progress_bar", "=", "False", "\n", "if", "'nnunet_use_progress_bar'", "in", "os", ".", "environ", ".", "keys", "(", ")", ":", "\n", "            ", "self", ".", "use_progress_bar", "=", "bool", "(", "int", "(", "os", ".", "environ", "[", "'nnunet_use_progress_bar'", "]", ")", ")", "\n", "\n", "################# Settings for saving checkpoints ##################################", "\n", "", "self", ".", "save_every", "=", "50", "\n", "self", ".", "save_latest_only", "=", "True", "# if false it will not store/overwrite _latest but separate files each", "\n", "# time an intermediate checkpoint is created", "\n", "self", ".", "save_intermediate_checkpoints", "=", "True", "# whether or not to save checkpoint_latest", "\n", "self", ".", "save_best_checkpoint", "=", "True", "# whether or not to save the best checkpoint according to self.best_val_eval_criterion_MA", "\n", "self", ".", "save_final_checkpoint", "=", "True", "# whether or not to save the final checkpoint", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.network_training.network_trainer.NetworkTrainer.initialize": [[129, 144], ["None"], "methods", ["None"], ["", "@", "abstractmethod", "\n", "def", "initialize", "(", "self", ",", "training", "=", "True", ")", ":", "\n", "        ", "\"\"\"\n        create self.output_folder\n\n        modify self.output_folder if you are doing cross-validation (one folder per fold)\n\n        set self.tr_gen and self.val_gen\n\n        call self.initialize_network and self.initialize_optimizer_and_scheduler (important!)\n\n        finally set self.was_initialized to True\n        :param training:\n        :return:\n        \"\"\"", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.network_training.network_trainer.NetworkTrainer.load_dataset": [[145, 148], ["None"], "methods", ["None"], ["", "@", "abstractmethod", "\n", "def", "load_dataset", "(", "self", ")", ":", "\n", "        ", "pass", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.network_training.network_trainer.NetworkTrainer.do_split": [[149, 186], ["join", "load_pickle", "list.sort", "list.sort", "collections.OrderedDict", "collections.OrderedDict", "isfile", "network_trainer.NetworkTrainer.print_to_log_file", "numpy.sort", "sklearn.model_selection.KFold", "enumerate", "save_pickle", "list", "list", "sklearn.model_selection.KFold.split", "load_pickle.append", "network_trainer.NetworkTrainer.dataset.keys", "network_trainer.NetworkTrainer.dataset.keys", "numpy.array", "numpy.array", "collections.OrderedDict"], "methods", ["home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.network_training.nnUNetTrainerV2_DDP.nnUNetTrainerV2_DDP.print_to_log_file"], ["", "def", "do_split", "(", "self", ")", ":", "\n", "        ", "\"\"\"\n        This is a suggestion for if your dataset is a dictionary (my personal standard)\n        :return:\n        \"\"\"", "\n", "splits_file", "=", "join", "(", "self", ".", "dataset_directory", ",", "\"splits_final.pkl\"", ")", "\n", "if", "not", "isfile", "(", "splits_file", ")", ":", "\n", "            ", "self", ".", "print_to_log_file", "(", "\"Creating new split...\"", ")", "\n", "splits", "=", "[", "]", "\n", "all_keys_sorted", "=", "np", ".", "sort", "(", "list", "(", "self", ".", "dataset", ".", "keys", "(", ")", ")", ")", "\n", "kfold", "=", "KFold", "(", "n_splits", "=", "5", ",", "shuffle", "=", "True", ",", "random_state", "=", "12345", ")", "\n", "for", "i", ",", "(", "train_idx", ",", "test_idx", ")", "in", "enumerate", "(", "kfold", ".", "split", "(", "all_keys_sorted", ")", ")", ":", "\n", "                ", "train_keys", "=", "np", ".", "array", "(", "all_keys_sorted", ")", "[", "train_idx", "]", "\n", "test_keys", "=", "np", ".", "array", "(", "all_keys_sorted", ")", "[", "test_idx", "]", "\n", "splits", ".", "append", "(", "OrderedDict", "(", ")", ")", "\n", "splits", "[", "-", "1", "]", "[", "'train'", "]", "=", "train_keys", "\n", "splits", "[", "-", "1", "]", "[", "'val'", "]", "=", "test_keys", "\n", "", "save_pickle", "(", "splits", ",", "splits_file", ")", "\n", "\n", "", "splits", "=", "load_pickle", "(", "splits_file", ")", "\n", "\n", "if", "self", ".", "fold", "==", "\"all\"", ":", "\n", "            ", "tr_keys", "=", "val_keys", "=", "list", "(", "self", ".", "dataset", ".", "keys", "(", ")", ")", "\n", "", "else", ":", "\n", "            ", "tr_keys", "=", "splits", "[", "self", ".", "fold", "]", "[", "'train'", "]", "\n", "val_keys", "=", "splits", "[", "self", ".", "fold", "]", "[", "'val'", "]", "\n", "\n", "", "tr_keys", ".", "sort", "(", ")", "\n", "val_keys", ".", "sort", "(", ")", "\n", "\n", "self", ".", "dataset_tr", "=", "OrderedDict", "(", ")", "\n", "for", "i", "in", "tr_keys", ":", "\n", "            ", "self", ".", "dataset_tr", "[", "i", "]", "=", "self", ".", "dataset", "[", "i", "]", "\n", "\n", "", "self", ".", "dataset_val", "=", "OrderedDict", "(", ")", "\n", "for", "i", "in", "val_keys", ":", "\n", "            ", "self", ".", "dataset_val", "[", "i", "]", "=", "self", ".", "dataset", "[", "i", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.network_training.network_trainer.NetworkTrainer.plot_progress": [[187, 223], ["matplotlib.rc", "matplotlib.rc", "matplotlib.rc", "matplotlib.rc", "matplotlib.figure", "matplotlib.figure", "matplotlib.figure.add_subplot", "plt.figure.add_subplot.twinx", "list", "plt.figure.add_subplot.plot", "plt.figure.add_subplot.plot", "plt.figure.add_subplot.set_xlabel", "plt.figure.add_subplot.set_ylabel", "fig.add_subplot.twinx.set_ylabel", "plt.figure.add_subplot.legend", "fig.add_subplot.twinx.legend", "matplotlib.figure.savefig", "matplotlib.close", "matplotlib.close", "range", "len", "plt.figure.add_subplot.plot", "len", "len", "fig.add_subplot.twinx.plot", "join", "network_trainer.NetworkTrainer.print_to_log_file", "sys.exc_info"], "methods", ["home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.network_training.nnUNetTrainerV2_DDP.nnUNetTrainerV2_DDP.print_to_log_file"], ["", "", "def", "plot_progress", "(", "self", ")", ":", "\n", "        ", "\"\"\"\n        Should probably by improved\n        :return:\n        \"\"\"", "\n", "try", ":", "\n", "            ", "font", "=", "{", "'weight'", ":", "'normal'", ",", "\n", "'size'", ":", "18", "}", "\n", "\n", "matplotlib", ".", "rc", "(", "'font'", ",", "**", "font", ")", "\n", "\n", "fig", "=", "plt", ".", "figure", "(", "figsize", "=", "(", "30", ",", "24", ")", ")", "\n", "ax", "=", "fig", ".", "add_subplot", "(", "111", ")", "\n", "ax2", "=", "ax", ".", "twinx", "(", ")", "\n", "\n", "x_values", "=", "list", "(", "range", "(", "self", ".", "epoch", "+", "1", ")", ")", "\n", "\n", "ax", ".", "plot", "(", "x_values", ",", "self", ".", "all_tr_losses", ",", "color", "=", "'b'", ",", "ls", "=", "'-'", ",", "label", "=", "\"loss_tr\"", ")", "\n", "\n", "ax", ".", "plot", "(", "x_values", ",", "self", ".", "all_val_losses", ",", "color", "=", "'r'", ",", "ls", "=", "'-'", ",", "label", "=", "\"loss_val, train=False\"", ")", "\n", "\n", "if", "len", "(", "self", ".", "all_val_losses_tr_mode", ")", ">", "0", ":", "\n", "                ", "ax", ".", "plot", "(", "x_values", ",", "self", ".", "all_val_losses_tr_mode", ",", "color", "=", "'g'", ",", "ls", "=", "'-'", ",", "label", "=", "\"loss_val, train=True\"", ")", "\n", "", "if", "len", "(", "self", ".", "all_val_eval_metrics", ")", "==", "len", "(", "x_values", ")", ":", "\n", "                ", "ax2", ".", "plot", "(", "x_values", ",", "self", ".", "all_val_eval_metrics", ",", "color", "=", "'g'", ",", "ls", "=", "'--'", ",", "label", "=", "\"evaluation metric\"", ")", "\n", "\n", "", "ax", ".", "set_xlabel", "(", "\"epoch\"", ")", "\n", "ax", ".", "set_ylabel", "(", "\"loss\"", ")", "\n", "ax2", ".", "set_ylabel", "(", "\"evaluation metric\"", ")", "\n", "ax", ".", "legend", "(", ")", "\n", "ax2", ".", "legend", "(", "loc", "=", "9", ")", "\n", "\n", "fig", ".", "savefig", "(", "join", "(", "self", ".", "output_folder", ",", "\"progress.png\"", ")", ")", "\n", "plt", ".", "close", "(", ")", "\n", "", "except", "IOError", ":", "\n", "            ", "self", ".", "print_to_log_file", "(", "\"failed to plot: \"", ",", "sys", ".", "exc_info", "(", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.network_training.network_trainer.NetworkTrainer.print_to_log_file": [[224, 257], ["time.time.time", "datetime.datetime.datetime.fromtimestamp", "maybe_mkdir_p", "datetime.datetime.datetime.now", "join", "print", "open", "f.write", "open", "f.write", "print", "time.time.sleep", "f.write", "f.write", "sys.exc_info", "str", "datetime.datetime.datetime.fromtimestamp"], "methods", ["None"], ["", "", "def", "print_to_log_file", "(", "self", ",", "*", "args", ",", "also_print_to_console", "=", "True", ",", "add_timestamp", "=", "True", ")", ":", "\n", "\n", "        ", "timestamp", "=", "time", "(", ")", "\n", "dt_object", "=", "datetime", ".", "fromtimestamp", "(", "timestamp", ")", "\n", "\n", "if", "add_timestamp", ":", "\n", "            ", "args", "=", "(", "\"%s:\"", "%", "dt_object", ",", "*", "args", ")", "\n", "\n", "", "if", "self", ".", "log_file", "is", "None", ":", "\n", "            ", "maybe_mkdir_p", "(", "self", ".", "output_folder", ")", "\n", "timestamp", "=", "datetime", ".", "now", "(", ")", "\n", "self", ".", "log_file", "=", "join", "(", "self", ".", "output_folder", ",", "\"training_log_%d_%d_%d_%02.0d_%02.0d_%02.0d.txt\"", "%", "\n", "(", "timestamp", ".", "year", ",", "timestamp", ".", "month", ",", "timestamp", ".", "day", ",", "timestamp", ".", "hour", ",", "timestamp", ".", "minute", ",", "\n", "timestamp", ".", "second", ")", ")", "\n", "with", "open", "(", "self", ".", "log_file", ",", "'w'", ")", "as", "f", ":", "\n", "                ", "f", ".", "write", "(", "\"Starting... \\n\"", ")", "\n", "", "", "successful", "=", "False", "\n", "max_attempts", "=", "5", "\n", "ctr", "=", "0", "\n", "while", "not", "successful", "and", "ctr", "<", "max_attempts", ":", "\n", "            ", "try", ":", "\n", "                ", "with", "open", "(", "self", ".", "log_file", ",", "'a+'", ")", "as", "f", ":", "\n", "                    ", "for", "a", "in", "args", ":", "\n", "                        ", "f", ".", "write", "(", "str", "(", "a", ")", ")", "\n", "f", ".", "write", "(", "\" \"", ")", "\n", "", "f", ".", "write", "(", "\"\\n\"", ")", "\n", "", "successful", "=", "True", "\n", "", "except", "IOError", ":", "\n", "                ", "print", "(", "\"%s: failed to log: \"", "%", "datetime", ".", "fromtimestamp", "(", "timestamp", ")", ",", "sys", ".", "exc_info", "(", ")", ")", "\n", "sleep", "(", "0.5", ")", "\n", "ctr", "+=", "1", "\n", "", "", "if", "also_print_to_console", ":", "\n", "            ", "print", "(", "*", "args", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.network_training.network_trainer.NetworkTrainer.save_checkpoint": [[258, 288], ["time.time.time", "network_trainer.NetworkTrainer.network.state_dict", "network_trainer.NetworkTrainer.keys", "network_trainer.NetworkTrainer.print_to_log_file", "torch.save", "torch.save", "torch.save", "torch.save", "network_trainer.NetworkTrainer.print_to_log_file", "state_dict[].cpu", "hasattr", "network_trainer.NetworkTrainer.lr_scheduler.state_dict", "network_trainer.NetworkTrainer.optimizer.state_dict", "network_trainer.NetworkTrainer.amp_grad_scaler.state_dict", "time.time.time"], "methods", ["home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.network_training.nnUNetTrainerV2_DDP.nnUNetTrainerV2_DDP.print_to_log_file", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.network_training.nnUNetTrainerV2_DDP.nnUNetTrainerV2_DDP.print_to_log_file"], ["", "", "def", "save_checkpoint", "(", "self", ",", "fname", ",", "save_optimizer", "=", "True", ")", ":", "\n", "        ", "start_time", "=", "time", "(", ")", "\n", "state_dict", "=", "self", ".", "network", ".", "state_dict", "(", ")", "\n", "for", "key", "in", "state_dict", ".", "keys", "(", ")", ":", "\n", "            ", "state_dict", "[", "key", "]", "=", "state_dict", "[", "key", "]", ".", "cpu", "(", ")", "\n", "", "lr_sched_state_dct", "=", "None", "\n", "if", "self", ".", "lr_scheduler", "is", "not", "None", "and", "hasattr", "(", "self", ".", "lr_scheduler", ",", "\n", "'state_dict'", ")", ":", "# not isinstance(self.lr_scheduler, lr_scheduler.ReduceLROnPlateau):", "\n", "            ", "lr_sched_state_dct", "=", "self", ".", "lr_scheduler", ".", "state_dict", "(", ")", "\n", "# WTF is this!?", "\n", "# for key in lr_sched_state_dct.keys():", "\n", "#    lr_sched_state_dct[key] = lr_sched_state_dct[key]", "\n", "", "if", "save_optimizer", ":", "\n", "            ", "optimizer_state_dict", "=", "self", ".", "optimizer", ".", "state_dict", "(", ")", "\n", "", "else", ":", "\n", "            ", "optimizer_state_dict", "=", "None", "\n", "\n", "", "self", ".", "print_to_log_file", "(", "\"saving checkpoint...\"", ")", "\n", "save_this", "=", "{", "\n", "'epoch'", ":", "self", ".", "epoch", "+", "1", ",", "\n", "'state_dict'", ":", "state_dict", ",", "\n", "'optimizer_state_dict'", ":", "optimizer_state_dict", ",", "\n", "'lr_scheduler_state_dict'", ":", "lr_sched_state_dct", ",", "\n", "'plot_stuff'", ":", "(", "self", ".", "all_tr_losses", ",", "self", ".", "all_val_losses", ",", "self", ".", "all_val_losses_tr_mode", ",", "\n", "self", ".", "all_val_eval_metrics", ")", "}", "\n", "if", "self", ".", "amp_grad_scaler", "is", "not", "None", ":", "\n", "            ", "save_this", "[", "'amp_grad_scaler'", "]", "=", "self", ".", "amp_grad_scaler", ".", "state_dict", "(", ")", "\n", "\n", "", "torch", ".", "save", "(", "save_this", ",", "fname", ")", "\n", "self", ".", "print_to_log_file", "(", "\"done, saving took %.2f seconds\"", "%", "(", "time", "(", ")", "-", "start_time", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.network_training.network_trainer.NetworkTrainer.load_best_checkpoint": [[289, 298], ["isfile", "RuntimeError", "join", "network_trainer.NetworkTrainer.load_checkpoint", "network_trainer.NetworkTrainer.print_to_log_file", "network_trainer.NetworkTrainer.load_latest_checkpoint", "join"], "methods", ["home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.network_training.network_trainer.NetworkTrainer.load_checkpoint", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.network_training.nnUNetTrainerV2_DDP.nnUNetTrainerV2_DDP.print_to_log_file", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.network_training.network_trainer.NetworkTrainer.load_latest_checkpoint"], ["", "def", "load_best_checkpoint", "(", "self", ",", "train", "=", "True", ")", ":", "\n", "        ", "if", "self", ".", "fold", "is", "None", ":", "\n", "            ", "raise", "RuntimeError", "(", "\"Cannot load best checkpoint if self.fold is None\"", ")", "\n", "", "if", "isfile", "(", "join", "(", "self", ".", "output_folder", ",", "\"model_best.model\"", ")", ")", ":", "\n", "            ", "self", ".", "load_checkpoint", "(", "join", "(", "self", ".", "output_folder", ",", "\"model_best.model\"", ")", ",", "train", "=", "train", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "print_to_log_file", "(", "\"WARNING! model_best.model does not exist! Cannot load best checkpoint. Falling \"", "\n", "\"back to load_latest_checkpoint\"", ")", "\n", "self", ".", "load_latest_checkpoint", "(", "train", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.network_training.network_trainer.NetworkTrainer.load_latest_checkpoint": [[299, 307], ["isfile", "isfile", "isfile", "RuntimeError", "join", "network_trainer.NetworkTrainer.load_checkpoint", "join", "network_trainer.NetworkTrainer.load_checkpoint", "join", "network_trainer.NetworkTrainer.load_best_checkpoint", "join", "join"], "methods", ["home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.network_training.network_trainer.NetworkTrainer.load_checkpoint", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.network_training.network_trainer.NetworkTrainer.load_checkpoint", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.network_training.network_trainer.NetworkTrainer.load_best_checkpoint"], ["", "", "def", "load_latest_checkpoint", "(", "self", ",", "train", "=", "True", ")", ":", "\n", "        ", "if", "isfile", "(", "join", "(", "self", ".", "output_folder", ",", "\"model_final_checkpoint.model\"", ")", ")", ":", "\n", "            ", "return", "self", ".", "load_checkpoint", "(", "join", "(", "self", ".", "output_folder", ",", "\"model_final_checkpoint.model\"", ")", ",", "train", "=", "train", ")", "\n", "", "if", "isfile", "(", "join", "(", "self", ".", "output_folder", ",", "\"model_latest.model\"", ")", ")", ":", "\n", "            ", "return", "self", ".", "load_checkpoint", "(", "join", "(", "self", ".", "output_folder", ",", "\"model_latest.model\"", ")", ",", "train", "=", "train", ")", "\n", "", "if", "isfile", "(", "join", "(", "self", ".", "output_folder", ",", "\"model_best.model\"", ")", ")", ":", "\n", "            ", "return", "self", ".", "load_best_checkpoint", "(", "train", ")", "\n", "", "raise", "RuntimeError", "(", "\"No checkpoint found\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.network_training.network_trainer.NetworkTrainer.load_checkpoint": [[308, 315], ["network_trainer.NetworkTrainer.print_to_log_file", "torch.load", "torch.load", "torch.load", "torch.load", "network_trainer.NetworkTrainer.load_checkpoint_ram", "network_trainer.NetworkTrainer.initialize", "torch.device", "torch.device", "torch.device", "torch.device"], "methods", ["home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.network_training.nnUNetTrainerV2_DDP.nnUNetTrainerV2_DDP.print_to_log_file", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.loss_function.nnUNetTrainerV2_graduallyTransitionFromCEToDice.nnUNetTrainerV2_graduallyTransitionFromCEToDice.load_checkpoint_ram", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.BraTS_trainer.nnUNetTrainerV2BraTSRegions.nnUNetTrainerV2BraTSRegions_DA3_BN.initialize"], ["", "def", "load_checkpoint", "(", "self", ",", "fname", ",", "train", "=", "True", ")", ":", "\n", "        ", "self", ".", "print_to_log_file", "(", "\"loading checkpoint\"", ",", "fname", ",", "\"train=\"", ",", "train", ")", "\n", "if", "not", "self", ".", "was_initialized", ":", "\n", "            ", "self", ".", "initialize", "(", "train", ")", "\n", "# saved_model = torch.load(fname, map_location=torch.device('cuda', torch.cuda.current_device()))", "\n", "", "saved_model", "=", "torch", ".", "load", "(", "fname", ",", "map_location", "=", "torch", ".", "device", "(", "'cpu'", ")", ")", "\n", "self", ".", "load_checkpoint_ram", "(", "saved_model", ",", "train", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.network_training.network_trainer.NetworkTrainer.load_pretrained_weights": [[316, 345], ["torch.load", "torch.load", "torch.load", "torch.load", "network_trainer.NetworkTrainer.network.state_dict", "network_trainer.NetworkTrainer.items", "network_trainer.NetworkTrainer.update", "print", "print", "pretrained_dict.items", "print", "network_trainer.NetworkTrainer.items", "network_trainer.NetworkTrainer.network.load_state_dict", "print", "print", "print", "pretrained_dict.items"], "methods", ["None"], ["", "def", "load_pretrained_weights", "(", "self", ",", "fname", ")", ":", "\n", "        ", "saved_model", "=", "torch", ".", "load", "(", "fname", ")", "\n", "pretrained_dict", "=", "saved_model", "[", "'state_dict'", "]", "\n", "model_dict", "=", "self", ".", "network", ".", "state_dict", "(", ")", "\n", "fine_tune", "=", "True", "\n", "for", "key", ",", "_", "in", "model_dict", ".", "items", "(", ")", ":", "\n", "           ", "if", "(", "'conv_blocks'", "in", "key", ")", ":", "\n", "               ", "if", "(", "key", "in", "pretrained_dict", ")", "and", "(", "model_dict", "[", "key", "]", ".", "shape", "==", "pretrained_dict", "[", "key", "]", ".", "shape", ")", ":", "\n", "                   ", "continue", "\n", "", "else", ":", "\n", "                   ", "fine_tune", "=", "False", "\n", "break", "\n", "# filter unnecessary keys", "\n", "", "", "", "if", "fine_tune", ":", "\n", "            ", "pretrained_dict", "=", "{", "k", ":", "v", "for", "k", ",", "v", "in", "pretrained_dict", ".", "items", "(", ")", "if", "\n", "(", "k", "in", "model_dict", ")", "and", "(", "model_dict", "[", "k", "]", ".", "shape", "==", "pretrained_dict", "[", "k", "]", ".", "shape", ")", "}", "\n", "# 2. overwrite entries in the existing state dict                       ", "\n", "model_dict", ".", "update", "(", "pretrained_dict", ")", "\n", "# print(model_dict)                                                     ", "\n", "print", "(", "\"############################################### Loading pre-trained Models Genesis from \"", ",", "fname", ")", "\n", "print", "(", "\"Below is the list of overlapping blocks in pre-trained Models Genesis and nnUNet architecture:\"", ")", "\n", "for", "key", ",", "_", "in", "pretrained_dict", ".", "items", "(", ")", ":", "\n", "                ", "print", "(", "key", ")", "\n", "", "print", "(", "\"############################################### Done\"", ")", "\n", "for", "key", ",", "_", "in", "model_dict", ".", "items", "(", ")", ":", "\n", "                ", "print", "(", "key", ")", "\n", "", "self", ".", "network", ".", "load_state_dict", "(", "model_dict", ")", "\n", "", "else", ":", "\n", "            ", "print", "(", "'############################################### Training from scratch'", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.network_training.network_trainer.NetworkTrainer.load_pretrained_encoder_weights": [[346, 365], ["torch.load", "torch.load", "torch.load", "torch.load", "network_trainer.NetworkTrainer.network.state_dict", "network_trainer.NetworkTrainer.update", "print", "print", "pretrained_dict.items", "print", "network_trainer.NetworkTrainer.items", "network_trainer.NetworkTrainer.network.load_state_dict", "print", "print", "pretrained_dict.items"], "methods", ["None"], ["", "", "def", "load_pretrained_encoder_weights", "(", "self", ",", "fname", ")", ":", "\n", "        ", "saved_model", "=", "torch", ".", "load", "(", "fname", ")", "\n", "pretrained_dict", "=", "saved_model", "[", "'state_dict'", "]", "\n", "model_dict", "=", "self", ".", "network", ".", "state_dict", "(", ")", "\n", "# filter unnecessary keys                                               ", "\n", "pretrained_dict", "=", "{", "k", ":", "v", "for", "k", ",", "v", "in", "pretrained_dict", ".", "items", "(", ")", "if", "\n", "(", "k", "in", "model_dict", ")", "and", "(", "model_dict", "[", "k", "]", ".", "shape", "==", "pretrained_dict", "[", "k", "]", ".", "shape", ")", "}", "\n", "# 2. overwrite entries in the existing state dict                       ", "\n", "model_dict", ".", "update", "(", "pretrained_dict", ")", "\n", "\n", "# print(model_dict)                                                     ", "\n", "print", "(", "\"############################################### Loading pre-trained Models Genesis encoder from \"", ",", "fname", ")", "\n", "print", "(", "\"Below is the list of overlapping blocks in pre-trained Models Genesis (encoder) and nnUNet architecture (encoder):\"", ")", "\n", "for", "key", ",", "_", "in", "pretrained_dict", ".", "items", "(", ")", ":", "\n", "            ", "print", "(", "key", ")", "\n", "", "print", "(", "\"############################################### Done\"", ")", "\n", "for", "key", ",", "_", "in", "model_dict", ".", "items", "(", ")", ":", "\n", "            ", "print", "(", "key", ")", "\n", "", "self", ".", "network", ".", "load_state_dict", "(", "model_dict", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.network_training.network_trainer.NetworkTrainer.initialize_network": [[367, 374], ["None"], "methods", ["None"], ["", "@", "abstractmethod", "\n", "def", "initialize_network", "(", "self", ")", ":", "\n", "        ", "\"\"\"\n        initialize self.network here\n        :return:\n        \"\"\"", "\n", "pass", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.network_training.network_trainer.NetworkTrainer.initialize_optimizer_and_scheduler": [[375, 382], ["None"], "methods", ["None"], ["", "@", "abstractmethod", "\n", "def", "initialize_optimizer_and_scheduler", "(", "self", ")", ":", "\n", "        ", "\"\"\"\n        initialize self.optimizer and self.lr_scheduler (if applicable) here\n        :return:\n        \"\"\"", "\n", "pass", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.network_training.network_trainer.NetworkTrainer.load_checkpoint_ram": [[383, 439], ["collections.OrderedDict", "list", "checkpoint[].items", "network_trainer.NetworkTrainer.network.load_state_dict", "network_trainer.NetworkTrainer._maybe_init_amp", "network_trainer.NetworkTrainer.initialize", "network_trainer.NetworkTrainer.network.state_dict().keys", "network_trainer.NetworkTrainer._maybe_init_amp", "issubclass", "len", "network_trainer.NetworkTrainer.print_to_log_file", "len", "key.startswith", "checkpoint.keys", "network_trainer.NetworkTrainer.amp_grad_scaler.load_state_dict", "network_trainer.NetworkTrainer.optimizer.load_state_dict", "hasattr", "network_trainer.NetworkTrainer.lr_scheduler.load_state_dict", "network_trainer.NetworkTrainer.lr_scheduler.step", "network_trainer.NetworkTrainer.network.state_dict"], "methods", ["home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.network_training.network_trainer.NetworkTrainer._maybe_init_amp", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.BraTS_trainer.nnUNetTrainerV2BraTSRegions.nnUNetTrainerV2BraTSRegions_DA3_BN.initialize", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.network_training.network_trainer.NetworkTrainer._maybe_init_amp", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.network_training.nnUNetTrainerV2_DDP.nnUNetTrainerV2_DDP.print_to_log_file", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.optimizer.ranger.Ranger.step"], ["", "def", "load_checkpoint_ram", "(", "self", ",", "checkpoint", ",", "train", "=", "True", ")", ":", "\n", "        ", "\"\"\"\n        used for if the checkpoint is already in ram\n        :param checkpoint:\n        :param train:\n        :return:\n        \"\"\"", "\n", "if", "not", "self", ".", "was_initialized", ":", "\n", "            ", "self", ".", "initialize", "(", "train", ")", "\n", "\n", "", "new_state_dict", "=", "OrderedDict", "(", ")", "\n", "curr_state_dict_keys", "=", "list", "(", "self", ".", "network", ".", "state_dict", "(", ")", ".", "keys", "(", ")", ")", "\n", "# if state dict comes form nn.DataParallel but we use non-parallel model here then the state dict keys do not", "\n", "# match. Use heuristic to make it match", "\n", "for", "k", ",", "value", "in", "checkpoint", "[", "'state_dict'", "]", ".", "items", "(", ")", ":", "\n", "            ", "key", "=", "k", "\n", "if", "key", "not", "in", "curr_state_dict_keys", "and", "key", ".", "startswith", "(", "'module.'", ")", ":", "\n", "                ", "key", "=", "key", "[", "7", ":", "]", "\n", "", "new_state_dict", "[", "key", "]", "=", "value", "\n", "\n", "", "if", "self", ".", "fp16", ":", "\n", "            ", "self", ".", "_maybe_init_amp", "(", ")", "\n", "if", "'amp_grad_scaler'", "in", "checkpoint", ".", "keys", "(", ")", ":", "\n", "                ", "self", ".", "amp_grad_scaler", ".", "load_state_dict", "(", "checkpoint", "[", "'amp_grad_scaler'", "]", ")", "\n", "\n", "", "", "self", ".", "network", ".", "load_state_dict", "(", "new_state_dict", ")", "\n", "self", ".", "epoch", "=", "checkpoint", "[", "'epoch'", "]", "\n", "if", "train", ":", "\n", "            ", "optimizer_state_dict", "=", "checkpoint", "[", "'optimizer_state_dict'", "]", "\n", "if", "optimizer_state_dict", "is", "not", "None", ":", "\n", "                ", "self", ".", "optimizer", ".", "load_state_dict", "(", "optimizer_state_dict", ")", "\n", "\n", "", "if", "self", ".", "lr_scheduler", "is", "not", "None", "and", "hasattr", "(", "self", ".", "lr_scheduler", ",", "'load_state_dict'", ")", "and", "checkpoint", "[", "\n", "'lr_scheduler_state_dict'", "]", "is", "not", "None", ":", "\n", "                ", "self", ".", "lr_scheduler", ".", "load_state_dict", "(", "checkpoint", "[", "'lr_scheduler_state_dict'", "]", ")", "\n", "\n", "", "if", "issubclass", "(", "self", ".", "lr_scheduler", ".", "__class__", ",", "_LRScheduler", ")", ":", "\n", "                ", "self", ".", "lr_scheduler", ".", "step", "(", "self", ".", "epoch", ")", "\n", "\n", "", "", "self", ".", "all_tr_losses", ",", "self", ".", "all_val_losses", ",", "self", ".", "all_val_losses_tr_mode", ",", "self", ".", "all_val_eval_metrics", "=", "checkpoint", "[", "\n", "'plot_stuff'", "]", "\n", "\n", "# after the training is done, the epoch is incremented one more time in my old code. This results in", "\n", "# self.epoch = 1001 for old trained models when the epoch is actually 1000. This causes issues because", "\n", "# len(self.all_tr_losses) = 1000 and the plot function will fail. We can easily detect and correct that here", "\n", "if", "self", ".", "epoch", "!=", "len", "(", "self", ".", "all_tr_losses", ")", ":", "\n", "            ", "self", ".", "print_to_log_file", "(", "\"WARNING in loading checkpoint: self.epoch != len(self.all_tr_losses). This is \"", "\n", "\"due to an old bug and should only appear when you are loading old models. New \"", "\n", "\"models should have this fixed! self.epoch is now set to len(self.all_tr_losses)\"", ")", "\n", "self", ".", "epoch", "=", "len", "(", "self", ".", "all_tr_losses", ")", "\n", "self", ".", "all_tr_losses", "=", "self", ".", "all_tr_losses", "[", ":", "self", ".", "epoch", "]", "\n", "self", ".", "all_val_losses", "=", "self", ".", "all_val_losses", "[", ":", "self", ".", "epoch", "]", "\n", "self", ".", "all_val_losses_tr_mode", "=", "self", ".", "all_val_losses_tr_mode", "[", ":", "self", ".", "epoch", "]", "\n", "self", ".", "all_val_eval_metrics", "=", "self", ".", "all_val_eval_metrics", "[", ":", "self", ".", "epoch", "]", "\n", "\n", "", "self", ".", "_maybe_init_amp", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.network_training.network_trainer.NetworkTrainer._maybe_init_amp": [[440, 443], ["torch.cuda.is_available", "torch.cuda.is_available", "torch.cuda.is_available", "torch.cuda.is_available", "torch.cuda.amp.GradScaler", "torch.cuda.amp.GradScaler"], "methods", ["None"], ["", "def", "_maybe_init_amp", "(", "self", ")", ":", "\n", "        ", "if", "self", ".", "fp16", "and", "self", ".", "amp_grad_scaler", "is", "None", "and", "torch", ".", "cuda", ".", "is_available", "(", ")", ":", "\n", "                ", "self", ".", "amp_grad_scaler", "=", "GradScaler", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.network_training.network_trainer.NetworkTrainer.plot_network_architecture": [[444, 451], ["None"], "methods", ["None"], ["", "", "def", "plot_network_architecture", "(", "self", ")", ":", "\n", "        ", "\"\"\"\n        can be implemented (see nnUNetTrainer) but does not have to. Not implemented here because it imposes stronger\n        assumptions on the presence of class variables\n        :return:\n        \"\"\"", "\n", "pass", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.network_training.network_trainer.NetworkTrainer.run_training": [[452, 538], ["network_trainer.NetworkTrainer.tr_gen.next", "network_trainer.NetworkTrainer.val_gen.next", "torch.cuda.is_available", "torch.cuda.is_available", "torch.cuda.is_available", "torch.cuda.is_available", "network_trainer.NetworkTrainer._maybe_init_amp", "maybe_mkdir_p", "network_trainer.NetworkTrainer.plot_network_architecture", "isfile", "isfile", "torch.cuda.empty_cache", "torch.cuda.empty_cache", "torch.cuda.empty_cache", "torch.cuda.empty_cache", "_warnings.warn", "network_trainer.NetworkTrainer.initialize", "network_trainer.NetworkTrainer.print_to_log_file", "time.time.time", "network_trainer.NetworkTrainer.network.train", "network_trainer.NetworkTrainer.all_tr_losses.append", "network_trainer.NetworkTrainer.print_to_log_file", "network_trainer.NetworkTrainer.update_train_loss_MA", "network_trainer.NetworkTrainer.on_epoch_end", "time.time.time", "network_trainer.NetworkTrainer.print_to_log_file", "network_trainer.NetworkTrainer.save_checkpoint", "join", "os.remove", "join", "os.remove", "range", "numpy.mean", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "network_trainer.NetworkTrainer.network.eval", "range", "network_trainer.NetworkTrainer.all_val_losses.append", "network_trainer.NetworkTrainer.print_to_log_file", "join", "join", "join", "tqdm.trange", "network_trainer.NetworkTrainer.run_iteration", "train_losses_epoch.append", "network_trainer.NetworkTrainer.run_iteration", "val_losses.append", "numpy.mean", "network_trainer.NetworkTrainer.network.train", "range", "network_trainer.NetworkTrainer.all_val_losses_tr_mode.append", "network_trainer.NetworkTrainer.print_to_log_file", "tbar.set_description", "network_trainer.NetworkTrainer.run_iteration", "tbar.set_postfix", "train_losses_epoch.append", "network_trainer.NetworkTrainer.run_iteration", "val_losses.append", "numpy.mean"], "methods", ["home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.network_training.network_trainer.NetworkTrainer._maybe_init_amp", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.network_training.nnUNetTrainer.nnUNetTrainer.plot_network_architecture", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.BraTS_trainer.nnUNetTrainerV2BraTSRegions.nnUNetTrainerV2BraTSRegions_DA3_BN.initialize", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.network_training.nnUNetTrainerV2_DDP.nnUNetTrainerV2_DDP.print_to_log_file", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.network_training.nnUNetTrainerV2_DDP.nnUNetTrainerV2_DDP.print_to_log_file", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.network_training.network_trainer.NetworkTrainer.update_train_loss_MA", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.optimizer_and_lr.nnUNetTrainerV2_reduceMomentumDuringTraining.nnUNetTrainerV2_reduceMomentumDuringTraining.on_epoch_end", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.network_training.nnUNetTrainerV2_DDP.nnUNetTrainerV2_DDP.print_to_log_file", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.profiling.nnUNetTrainerV2_2epochs.nnUNetTrainerV2_DDP_5epochs.save_checkpoint", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.network_training.nnUNetTrainerV2_DDP.nnUNetTrainerV2_DDP.print_to_log_file", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.BraTS_trainer.nnUNetTrainerV2BraTSRegions.nnUNetTrainerV2BraTSRegions_DDP.run_iteration", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.BraTS_trainer.nnUNetTrainerV2BraTSRegions.nnUNetTrainerV2BraTSRegions_DDP.run_iteration", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.network_training.nnUNetTrainerV2_DDP.nnUNetTrainerV2_DDP.print_to_log_file", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.BraTS_trainer.nnUNetTrainerV2BraTSRegions.nnUNetTrainerV2BraTSRegions_DDP.run_iteration", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.BraTS_trainer.nnUNetTrainerV2BraTSRegions.nnUNetTrainerV2BraTSRegions_DDP.run_iteration"], ["", "def", "run_training", "(", "self", ")", ":", "\n", "        ", "_", "=", "self", ".", "tr_gen", ".", "next", "(", ")", "\n", "_", "=", "self", ".", "val_gen", ".", "next", "(", ")", "\n", "\n", "if", "torch", ".", "cuda", ".", "is_available", "(", ")", ":", "\n", "            ", "torch", ".", "cuda", ".", "empty_cache", "(", ")", "\n", "\n", "", "self", ".", "_maybe_init_amp", "(", ")", "\n", "\n", "maybe_mkdir_p", "(", "self", ".", "output_folder", ")", "\n", "self", ".", "plot_network_architecture", "(", ")", "\n", "\n", "if", "cudnn", ".", "benchmark", "and", "cudnn", ".", "deterministic", ":", "\n", "            ", "warn", "(", "\"torch.backends.cudnn.deterministic is True indicating a deterministic training is desired. \"", "\n", "\"But torch.backends.cudnn.benchmark is True as well and this will prevent deterministic training! \"", "\n", "\"If you want deterministic then set benchmark=False\"", ")", "\n", "\n", "", "if", "not", "self", ".", "was_initialized", ":", "\n", "            ", "self", ".", "initialize", "(", "True", ")", "\n", "\n", "", "while", "self", ".", "epoch", "<", "self", ".", "max_num_epochs", ":", "\n", "            ", "self", ".", "print_to_log_file", "(", "\"\\nepoch: \"", ",", "self", ".", "epoch", ")", "\n", "epoch_start_time", "=", "time", "(", ")", "\n", "train_losses_epoch", "=", "[", "]", "\n", "\n", "# train one epoch", "\n", "self", ".", "network", ".", "train", "(", ")", "\n", "\n", "if", "self", ".", "use_progress_bar", ":", "\n", "                ", "with", "trange", "(", "self", ".", "num_batches_per_epoch", ")", "as", "tbar", ":", "\n", "                    ", "for", "b", "in", "tbar", ":", "\n", "                        ", "tbar", ".", "set_description", "(", "\"Epoch {}/{}\"", ".", "format", "(", "self", ".", "epoch", "+", "1", ",", "self", ".", "max_num_epochs", ")", ")", "\n", "\n", "l", "=", "self", ".", "run_iteration", "(", "self", ".", "tr_gen", ",", "True", ")", "\n", "\n", "tbar", ".", "set_postfix", "(", "loss", "=", "l", ")", "\n", "train_losses_epoch", ".", "append", "(", "l", ")", "\n", "", "", "", "else", ":", "\n", "                ", "for", "_", "in", "range", "(", "self", ".", "num_batches_per_epoch", ")", ":", "\n", "                    ", "l", "=", "self", ".", "run_iteration", "(", "self", ".", "tr_gen", ",", "True", ")", "\n", "train_losses_epoch", ".", "append", "(", "l", ")", "\n", "\n", "", "", "self", ".", "all_tr_losses", ".", "append", "(", "np", ".", "mean", "(", "train_losses_epoch", ")", ")", "\n", "self", ".", "print_to_log_file", "(", "\"train loss : %.4f\"", "%", "self", ".", "all_tr_losses", "[", "-", "1", "]", ")", "\n", "\n", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "# validation with train=False", "\n", "                ", "self", ".", "network", ".", "eval", "(", ")", "\n", "val_losses", "=", "[", "]", "\n", "for", "b", "in", "range", "(", "self", ".", "num_val_batches_per_epoch", ")", ":", "\n", "                    ", "l", "=", "self", ".", "run_iteration", "(", "self", ".", "val_gen", ",", "False", ",", "True", ")", "\n", "val_losses", ".", "append", "(", "l", ")", "\n", "", "self", ".", "all_val_losses", ".", "append", "(", "np", ".", "mean", "(", "val_losses", ")", ")", "\n", "self", ".", "print_to_log_file", "(", "\"validation loss: %.4f\"", "%", "self", ".", "all_val_losses", "[", "-", "1", "]", ")", "\n", "\n", "if", "self", ".", "also_val_in_tr_mode", ":", "\n", "                    ", "self", ".", "network", ".", "train", "(", ")", "\n", "# validation with train=True", "\n", "val_losses", "=", "[", "]", "\n", "for", "b", "in", "range", "(", "self", ".", "num_val_batches_per_epoch", ")", ":", "\n", "                        ", "l", "=", "self", ".", "run_iteration", "(", "self", ".", "val_gen", ",", "False", ")", "\n", "val_losses", ".", "append", "(", "l", ")", "\n", "", "self", ".", "all_val_losses_tr_mode", ".", "append", "(", "np", ".", "mean", "(", "val_losses", ")", ")", "\n", "self", ".", "print_to_log_file", "(", "\"validation loss (train=True): %.4f\"", "%", "self", ".", "all_val_losses_tr_mode", "[", "-", "1", "]", ")", "\n", "\n", "", "", "self", ".", "update_train_loss_MA", "(", ")", "# needed for lr scheduler and stopping of training", "\n", "\n", "continue_training", "=", "self", ".", "on_epoch_end", "(", ")", "\n", "\n", "epoch_end_time", "=", "time", "(", ")", "\n", "\n", "if", "not", "continue_training", ":", "\n", "# allows for early stopping", "\n", "                ", "break", "\n", "\n", "", "self", ".", "epoch", "+=", "1", "\n", "self", ".", "print_to_log_file", "(", "\"This epoch took %f s\\n\"", "%", "(", "epoch_end_time", "-", "epoch_start_time", ")", ")", "\n", "\n", "", "self", ".", "epoch", "-=", "1", "# if we don't do this we can get a problem with loading model_final_checkpoint.", "\n", "\n", "if", "self", ".", "save_final_checkpoint", ":", "self", ".", "save_checkpoint", "(", "join", "(", "self", ".", "output_folder", ",", "\"model_final_checkpoint.model\"", ")", ")", "\n", "# now we can delete latest as it will be identical with final", "\n", "if", "isfile", "(", "join", "(", "self", ".", "output_folder", ",", "\"model_latest.model\"", ")", ")", ":", "\n", "            ", "os", ".", "remove", "(", "join", "(", "self", ".", "output_folder", ",", "\"model_latest.model\"", ")", ")", "\n", "", "if", "isfile", "(", "join", "(", "self", ".", "output_folder", ",", "\"model_latest.model.pkl\"", ")", ")", ":", "\n", "            ", "os", ".", "remove", "(", "join", "(", "self", ".", "output_folder", ",", "\"model_latest.model.pkl\"", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.network_training.network_trainer.NetworkTrainer.maybe_update_lr": [[539, 550], ["network_trainer.NetworkTrainer.print_to_log_file", "isinstance", "isinstance", "network_trainer.NetworkTrainer.lr_scheduler.step", "network_trainer.NetworkTrainer.lr_scheduler.step", "str"], "methods", ["home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.network_training.nnUNetTrainerV2_DDP.nnUNetTrainerV2_DDP.print_to_log_file", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.optimizer.ranger.Ranger.step", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.optimizer.ranger.Ranger.step"], ["", "", "def", "maybe_update_lr", "(", "self", ")", ":", "\n", "# maybe update learning rate", "\n", "        ", "if", "self", ".", "lr_scheduler", "is", "not", "None", ":", "\n", "            ", "assert", "isinstance", "(", "self", ".", "lr_scheduler", ",", "(", "lr_scheduler", ".", "ReduceLROnPlateau", ",", "lr_scheduler", ".", "_LRScheduler", ")", ")", "\n", "\n", "if", "isinstance", "(", "self", ".", "lr_scheduler", ",", "lr_scheduler", ".", "ReduceLROnPlateau", ")", ":", "\n", "# lr scheduler is updated with moving average val loss. should be more robust", "\n", "                ", "self", ".", "lr_scheduler", ".", "step", "(", "self", ".", "train_loss_MA", ")", "\n", "", "else", ":", "\n", "                ", "self", ".", "lr_scheduler", ".", "step", "(", "self", ".", "epoch", "+", "1", ")", "\n", "", "", "self", ".", "print_to_log_file", "(", "\"lr is now (scheduler) %s\"", "%", "str", "(", "self", ".", "optimizer", ".", "param_groups", "[", "0", "]", "[", "'lr'", "]", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.network_training.network_trainer.NetworkTrainer.maybe_save_checkpoint": [[551, 562], ["network_trainer.NetworkTrainer.print_to_log_file", "network_trainer.NetworkTrainer.save_checkpoint", "network_trainer.NetworkTrainer.print_to_log_file", "network_trainer.NetworkTrainer.save_checkpoint", "join", "join"], "methods", ["home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.network_training.nnUNetTrainerV2_DDP.nnUNetTrainerV2_DDP.print_to_log_file", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.profiling.nnUNetTrainerV2_2epochs.nnUNetTrainerV2_DDP_5epochs.save_checkpoint", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.network_training.nnUNetTrainerV2_DDP.nnUNetTrainerV2_DDP.print_to_log_file", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.profiling.nnUNetTrainerV2_2epochs.nnUNetTrainerV2_DDP_5epochs.save_checkpoint"], ["", "def", "maybe_save_checkpoint", "(", "self", ")", ":", "\n", "        ", "\"\"\"\n        Saves a checkpoint every save_ever epochs.\n        :return:\n        \"\"\"", "\n", "if", "self", ".", "save_intermediate_checkpoints", "and", "(", "self", ".", "epoch", "%", "self", ".", "save_every", "==", "(", "self", ".", "save_every", "-", "1", ")", ")", ":", "\n", "            ", "self", ".", "print_to_log_file", "(", "\"saving scheduled checkpoint file...\"", ")", "\n", "if", "not", "self", ".", "save_latest_only", ":", "\n", "                ", "self", ".", "save_checkpoint", "(", "join", "(", "self", ".", "output_folder", ",", "\"model_ep_%03.0d.model\"", "%", "(", "self", ".", "epoch", "+", "1", ")", ")", ")", "\n", "", "self", ".", "save_checkpoint", "(", "join", "(", "self", ".", "output_folder", ",", "\"model_latest.model\"", ")", ")", "\n", "self", ".", "print_to_log_file", "(", "\"done\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.network_training.network_trainer.NetworkTrainer.update_eval_criterion_MA": [[563, 587], ["len", "len"], "methods", ["None"], ["", "", "def", "update_eval_criterion_MA", "(", "self", ")", ":", "\n", "        ", "\"\"\"\n        If self.all_val_eval_metrics is unused (len=0) then we fall back to using -self.all_val_losses for the MA to determine early stopping\n        (not a minimization, but a maximization of a metric and therefore the - in the latter case)\n        :return:\n        \"\"\"", "\n", "if", "self", ".", "val_eval_criterion_MA", "is", "None", ":", "\n", "            ", "if", "len", "(", "self", ".", "all_val_eval_metrics", ")", "==", "0", ":", "\n", "                ", "self", ".", "val_eval_criterion_MA", "=", "-", "self", ".", "all_val_losses", "[", "-", "1", "]", "\n", "", "else", ":", "\n", "                ", "self", ".", "val_eval_criterion_MA", "=", "self", ".", "all_val_eval_metrics", "[", "-", "1", "]", "\n", "", "", "else", ":", "\n", "            ", "if", "len", "(", "self", ".", "all_val_eval_metrics", ")", "==", "0", ":", "\n", "                ", "\"\"\"\n                We here use alpha * old - (1 - alpha) * new because new in this case is the vlaidation loss and lower\n                is better, so we need to negate it.\n                \"\"\"", "\n", "self", ".", "val_eval_criterion_MA", "=", "self", ".", "val_eval_criterion_alpha", "*", "self", ".", "val_eval_criterion_MA", "-", "(", "\n", "1", "-", "self", ".", "val_eval_criterion_alpha", ")", "*", "self", ".", "all_val_losses", "[", "-", "1", "]", "\n", "", "else", ":", "\n", "                ", "self", ".", "val_eval_criterion_MA", "=", "self", ".", "val_eval_criterion_alpha", "*", "self", ".", "val_eval_criterion_MA", "+", "(", "\n", "1", "-", "self", ".", "val_eval_criterion_alpha", ")", "*", "self", ".", "all_val_eval_metrics", "[", "-", "1", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.network_training.network_trainer.NetworkTrainer.manage_patience": [[588, 639], ["network_trainer.NetworkTrainer.save_checkpoint", "join"], "methods", ["home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.profiling.nnUNetTrainerV2_2epochs.nnUNetTrainerV2_DDP_5epochs.save_checkpoint"], ["", "", "", "def", "manage_patience", "(", "self", ")", ":", "\n", "# update patience", "\n", "        ", "continue_training", "=", "True", "\n", "if", "self", ".", "patience", "is", "not", "None", ":", "\n", "# if best_MA_tr_loss_for_patience and best_epoch_based_on_MA_tr_loss were not yet initialized,", "\n", "# initialize them", "\n", "            ", "if", "self", ".", "best_MA_tr_loss_for_patience", "is", "None", ":", "\n", "                ", "self", ".", "best_MA_tr_loss_for_patience", "=", "self", ".", "train_loss_MA", "\n", "\n", "", "if", "self", ".", "best_epoch_based_on_MA_tr_loss", "is", "None", ":", "\n", "                ", "self", ".", "best_epoch_based_on_MA_tr_loss", "=", "self", ".", "epoch", "\n", "\n", "", "if", "self", ".", "best_val_eval_criterion_MA", "is", "None", ":", "\n", "                ", "self", ".", "best_val_eval_criterion_MA", "=", "self", ".", "val_eval_criterion_MA", "\n", "\n", "# check if the current epoch is the best one according to moving average of validation criterion. If so", "\n", "# then save 'best' model", "\n", "# Do not use this for validation. This is intended for test set prediction only.", "\n", "#self.print_to_log_file(\"current best_val_eval_criterion_MA is %.4f0\" % self.best_val_eval_criterion_MA)", "\n", "#self.print_to_log_file(\"current val_eval_criterion_MA is %.4f\" % self.val_eval_criterion_MA)", "\n", "\n", "", "if", "self", ".", "val_eval_criterion_MA", ">", "self", ".", "best_val_eval_criterion_MA", ":", "\n", "                ", "self", ".", "best_val_eval_criterion_MA", "=", "self", ".", "val_eval_criterion_MA", "\n", "#self.print_to_log_file(\"saving best epoch checkpoint...\")", "\n", "if", "self", ".", "save_best_checkpoint", ":", "self", ".", "save_checkpoint", "(", "join", "(", "self", ".", "output_folder", ",", "\"model_best.model\"", ")", ")", "\n", "\n", "# Now see if the moving average of the train loss has improved. If yes then reset patience, else", "\n", "# increase patience", "\n", "", "if", "self", ".", "train_loss_MA", "+", "self", ".", "train_loss_MA_eps", "<", "self", ".", "best_MA_tr_loss_for_patience", ":", "\n", "                ", "self", ".", "best_MA_tr_loss_for_patience", "=", "self", ".", "train_loss_MA", "\n", "self", ".", "best_epoch_based_on_MA_tr_loss", "=", "self", ".", "epoch", "\n", "#self.print_to_log_file(\"New best epoch (train loss MA): %03.4f\" % self.best_MA_tr_loss_for_patience)", "\n", "", "else", ":", "\n", "                ", "pass", "\n", "#self.print_to_log_file(\"No improvement: current train MA %03.4f, best: %03.4f, eps is %03.4f\" %", "\n", "#                       (self.train_loss_MA, self.best_MA_tr_loss_for_patience, self.train_loss_MA_eps))", "\n", "\n", "# if patience has reached its maximum then finish training (provided lr is low enough)", "\n", "", "if", "self", ".", "epoch", "-", "self", ".", "best_epoch_based_on_MA_tr_loss", ">", "self", ".", "patience", ":", "\n", "                ", "if", "self", ".", "optimizer", ".", "param_groups", "[", "0", "]", "[", "'lr'", "]", ">", "self", ".", "lr_threshold", ":", "\n", "#self.print_to_log_file(\"My patience ended, but I believe I need more time (lr > 1e-6)\")", "\n", "                    ", "self", ".", "best_epoch_based_on_MA_tr_loss", "=", "self", ".", "epoch", "-", "self", ".", "patience", "//", "2", "\n", "", "else", ":", "\n", "#self.print_to_log_file(\"My patience ended\")", "\n", "                    ", "continue_training", "=", "False", "\n", "", "", "else", ":", "\n", "                ", "pass", "\n", "#self.print_to_log_file(", "\n", "#    \"Patience: %d/%d\" % (self.epoch - self.best_epoch_based_on_MA_tr_loss, self.patience))", "\n", "\n", "", "", "return", "continue_training", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.network_training.network_trainer.NetworkTrainer.on_epoch_end": [[640, 654], ["network_trainer.NetworkTrainer.finish_online_evaluation", "network_trainer.NetworkTrainer.plot_progress", "network_trainer.NetworkTrainer.maybe_update_lr", "network_trainer.NetworkTrainer.maybe_save_checkpoint", "network_trainer.NetworkTrainer.update_eval_criterion_MA", "network_trainer.NetworkTrainer.manage_patience"], "methods", ["home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.miscellaneous.nnUNetTrainerV2_fullEvals.nnUNetTrainerV2_fullEvals.finish_online_evaluation", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.network_training.nnUNetTrainerV2_DDP.nnUNetTrainerV2_DDP.plot_progress", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.optimizer_and_lr.nnUNetTrainerV2_warmup.nnUNetTrainerV2_warmup.maybe_update_lr", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.network_training.network_trainer.NetworkTrainer.maybe_save_checkpoint", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.network_training.network_trainer.NetworkTrainer.update_eval_criterion_MA", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.network_training.network_trainer.NetworkTrainer.manage_patience"], ["", "def", "on_epoch_end", "(", "self", ")", ":", "\n", "        ", "self", ".", "finish_online_evaluation", "(", ")", "# does not have to do anything, but can be used to update self.all_val_eval_", "\n", "# metrics", "\n", "\n", "self", ".", "plot_progress", "(", ")", "\n", "\n", "self", ".", "maybe_update_lr", "(", ")", "\n", "\n", "self", ".", "maybe_save_checkpoint", "(", ")", "\n", "\n", "self", ".", "update_eval_criterion_MA", "(", ")", "\n", "\n", "continue_training", "=", "self", ".", "manage_patience", "(", ")", "\n", "return", "continue_training", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.network_training.network_trainer.NetworkTrainer.update_train_loss_MA": [[655, 661], ["None"], "methods", ["None"], ["", "def", "update_train_loss_MA", "(", "self", ")", ":", "\n", "        ", "if", "self", ".", "train_loss_MA", "is", "None", ":", "\n", "            ", "self", ".", "train_loss_MA", "=", "self", ".", "all_tr_losses", "[", "-", "1", "]", "\n", "", "else", ":", "\n", "            ", "self", ".", "train_loss_MA", "=", "self", ".", "train_loss_MA_alpha", "*", "self", ".", "train_loss_MA", "+", "(", "1", "-", "self", ".", "train_loss_MA_alpha", ")", "*", "self", ".", "all_tr_losses", "[", "-", "1", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.network_training.network_trainer.NetworkTrainer.run_iteration": [[662, 701], ["next", "nnunet.utilities.to_torch.maybe_to_torch", "nnunet.utilities.to_torch.maybe_to_torch", "torch.cuda.is_available", "torch.cuda.is_available", "torch.cuda.is_available", "torch.cuda.is_available", "network_trainer.NetworkTrainer.optimizer.zero_grad", "network_trainer.NetworkTrainer.detach().cpu().numpy", "nnunet.utilities.to_torch.to_cuda", "nnunet.utilities.to_torch.to_cuda", "network_trainer.NetworkTrainer.network", "network_trainer.NetworkTrainer.loss", "network_trainer.NetworkTrainer.run_online_evaluation", "torch.cuda.amp.autocast", "torch.cuda.amp.autocast", "network_trainer.NetworkTrainer.network", "network_trainer.NetworkTrainer.loss", "network_trainer.NetworkTrainer.amp_grad_scaler.scale().backward", "network_trainer.NetworkTrainer.amp_grad_scaler.step", "network_trainer.NetworkTrainer.amp_grad_scaler.update", "network_trainer.NetworkTrainer.backward", "network_trainer.NetworkTrainer.optimizer.step", "network_trainer.NetworkTrainer.detach().cpu", "network_trainer.NetworkTrainer.amp_grad_scaler.scale", "network_trainer.NetworkTrainer.detach"], "methods", ["home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.utilities.to_torch.maybe_to_torch", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.utilities.to_torch.maybe_to_torch", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.utilities.to_torch.to_cuda", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.utilities.to_torch.to_cuda", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.BraTS_trainer.nnUNetTrainerV2BraTSRegions.nnUNetTrainerV2BraTSRegions_DA3_BN.run_online_evaluation", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.utilities.distributed.awesome_allgather_function.backward", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.optimizer.ranger.Ranger.step", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.utilities.distributed.awesome_allgather_function.backward", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.optimizer.ranger.Ranger.step"], ["", "", "def", "run_iteration", "(", "self", ",", "data_generator", ",", "do_backprop", "=", "True", ",", "run_online_evaluation", "=", "False", ")", ":", "\n", "        ", "data_dict", "=", "next", "(", "data_generator", ")", "\n", "data", "=", "data_dict", "[", "'data'", "]", "\n", "target", "=", "data_dict", "[", "'target'", "]", "\n", "\n", "data", "=", "maybe_to_torch", "(", "data", ")", "\n", "target", "=", "maybe_to_torch", "(", "target", ")", "\n", "\n", "if", "torch", ".", "cuda", ".", "is_available", "(", ")", ":", "\n", "            ", "data", "=", "to_cuda", "(", "data", ")", "\n", "target", "=", "to_cuda", "(", "target", ")", "\n", "\n", "", "self", ".", "optimizer", ".", "zero_grad", "(", ")", "\n", "\n", "if", "self", ".", "fp16", ":", "\n", "            ", "with", "autocast", "(", ")", ":", "\n", "                ", "output", "=", "self", ".", "network", "(", "data", ")", "\n", "del", "data", "\n", "l", "=", "self", ".", "loss", "(", "output", ",", "target", ")", "\n", "\n", "", "if", "do_backprop", ":", "\n", "                ", "self", ".", "amp_grad_scaler", ".", "scale", "(", "l", ")", ".", "backward", "(", ")", "\n", "self", ".", "amp_grad_scaler", ".", "step", "(", "self", ".", "optimizer", ")", "\n", "self", ".", "amp_grad_scaler", ".", "update", "(", ")", "\n", "", "", "else", ":", "\n", "            ", "output", "=", "self", ".", "network", "(", "data", ")", "\n", "del", "data", "\n", "l", "=", "self", ".", "loss", "(", "output", ",", "target", ")", "\n", "\n", "if", "do_backprop", ":", "\n", "                ", "l", ".", "backward", "(", ")", "\n", "self", ".", "optimizer", ".", "step", "(", ")", "\n", "\n", "", "", "if", "run_online_evaluation", ":", "\n", "            ", "self", ".", "run_online_evaluation", "(", "output", ",", "target", ")", "\n", "\n", "", "del", "target", "\n", "\n", "return", "l", ".", "detach", "(", ")", ".", "cpu", "(", ")", ".", "numpy", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.network_training.network_trainer.NetworkTrainer.run_online_evaluation": [[702, 710], ["None"], "methods", ["None"], ["", "def", "run_online_evaluation", "(", "self", ",", "*", "args", ",", "**", "kwargs", ")", ":", "\n", "        ", "\"\"\"\n        Can be implemented, does not have to\n        :param output_torch:\n        :param target_npy:\n        :return:\n        \"\"\"", "\n", "pass", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.network_training.network_trainer.NetworkTrainer.finish_online_evaluation": [[711, 717], ["None"], "methods", ["None"], ["", "def", "finish_online_evaluation", "(", "self", ")", ":", "\n", "        ", "\"\"\"\n        Can be implemented, does not have to\n        :return:\n        \"\"\"", "\n", "pass", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.network_training.network_trainer.NetworkTrainer.validate": [[718, 721], ["None"], "methods", ["None"], ["", "@", "abstractmethod", "\n", "def", "validate", "(", "self", ",", "*", "args", ",", "**", "kwargs", ")", ":", "\n", "        ", "pass", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.network_training.network_trainer.NetworkTrainer.find_lr": [[722, 773], ["network_trainer.NetworkTrainer._maybe_init_amp", "range", "matplotlib.figure", "matplotlib.figure", "matplotlib.xscale", "matplotlib.xscale", "matplotlib.plot", "matplotlib.plot", "matplotlib.savefig", "matplotlib.savefig", "matplotlib.close", "matplotlib.close", "losses.append", "log_lrs.append", "join", "network_trainer.NetworkTrainer.run_iteration", "math.log10"], "methods", ["home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.network_training.network_trainer.NetworkTrainer._maybe_init_amp", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.BraTS_trainer.nnUNetTrainerV2BraTSRegions.nnUNetTrainerV2BraTSRegions_DDP.run_iteration"], ["", "def", "find_lr", "(", "self", ",", "num_iters", "=", "1000", ",", "init_value", "=", "1e-6", ",", "final_value", "=", "10.", ",", "beta", "=", "0.98", ")", ":", "\n", "        ", "\"\"\"\n        stolen and adapted from here: https://sgugger.github.io/how-do-you-find-a-good-learning-rate.html\n        :param num_iters:\n        :param init_value:\n        :param final_value:\n        :param beta:\n        :return:\n        \"\"\"", "\n", "import", "math", "\n", "self", ".", "_maybe_init_amp", "(", ")", "\n", "mult", "=", "(", "final_value", "/", "init_value", ")", "**", "(", "1", "/", "num_iters", ")", "\n", "lr", "=", "init_value", "\n", "self", ".", "optimizer", ".", "param_groups", "[", "0", "]", "[", "'lr'", "]", "=", "lr", "\n", "avg_loss", "=", "0.", "\n", "best_loss", "=", "0.", "\n", "losses", "=", "[", "]", "\n", "log_lrs", "=", "[", "]", "\n", "\n", "for", "batch_num", "in", "range", "(", "1", ",", "num_iters", "+", "1", ")", ":", "\n", "# +1 because this one here is not designed to have negative loss...", "\n", "            ", "loss", "=", "self", ".", "run_iteration", "(", "self", ".", "tr_gen", ",", "do_backprop", "=", "True", ",", "run_online_evaluation", "=", "False", ")", "+", "1", "\n", "\n", "# Compute the smoothed loss", "\n", "avg_loss", "=", "beta", "*", "avg_loss", "+", "(", "1", "-", "beta", ")", "*", "loss", "\n", "smoothed_loss", "=", "avg_loss", "/", "(", "1", "-", "beta", "**", "batch_num", ")", "\n", "\n", "# Stop if the loss is exploding", "\n", "if", "batch_num", ">", "1", "and", "smoothed_loss", ">", "4", "*", "best_loss", ":", "\n", "                ", "break", "\n", "\n", "# Record the best loss", "\n", "", "if", "smoothed_loss", "<", "best_loss", "or", "batch_num", "==", "1", ":", "\n", "                ", "best_loss", "=", "smoothed_loss", "\n", "\n", "# Store the values", "\n", "", "losses", ".", "append", "(", "smoothed_loss", ")", "\n", "log_lrs", ".", "append", "(", "math", ".", "log10", "(", "lr", ")", ")", "\n", "\n", "# Update the lr for the next step", "\n", "lr", "*=", "mult", "\n", "self", ".", "optimizer", ".", "param_groups", "[", "0", "]", "[", "'lr'", "]", "=", "lr", "\n", "\n", "", "import", "matplotlib", ".", "pyplot", "as", "plt", "\n", "lrs", "=", "[", "10", "**", "i", "for", "i", "in", "log_lrs", "]", "\n", "fig", "=", "plt", ".", "figure", "(", ")", "\n", "plt", ".", "xscale", "(", "'log'", ")", "\n", "plt", ".", "plot", "(", "lrs", "[", "10", ":", "-", "5", "]", ",", "losses", "[", "10", ":", "-", "5", "]", ")", "\n", "plt", ".", "savefig", "(", "join", "(", "self", ".", "output_folder", ",", "\"lr_finder.png\"", ")", ")", "\n", "plt", ".", "close", "(", ")", "\n", "return", "log_lrs", ",", "losses", "\n", "", "", ""]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.network_training.nnUNetTrainer.nnUNetTrainer.__init__": [[49, 133], ["nnunet.training.network_training.network_trainer.NetworkTrainer.__init__", "nnunet.training.loss_functions.dice_loss.DC_and_CE_loss", "nnUNetTrainer.nnUNetTrainer.update_fold", "isdir", "join"], "methods", ["home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.common.layers.ResizeImage.__init__", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.network_training.nnUNetTrainer.nnUNetTrainer.update_fold"], ["    ", "def", "__init__", "(", "self", ",", "plans_file", ",", "fold", ",", "output_folder", "=", "None", ",", "dataset_directory", "=", "None", ",", "batch_dice", "=", "True", ",", "stage", "=", "None", ",", "\n", "unpack_data", "=", "True", ",", "deterministic", "=", "True", ",", "fp16", "=", "False", ")", ":", "\n", "        ", "\"\"\"\n        :param deterministic:\n        :param fold: can be either [0 ... 5) for cross-validation, 'all' to train on all available training data or\n        None if you wish to load some checkpoint and do inference only\n        :param plans_file: the pkl file generated by preprocessing. This file will determine all design choices\n        :param subfolder_with_preprocessed_data: must be a subfolder of dataset_directory (just the name of the folder,\n        not the entire path). This is where the preprocessed data lies that will be used for network training. We made\n        this explicitly available so that differently preprocessed data can coexist and the user can choose what to use.\n        Can be None if you are doing inference only.\n        :param output_folder: where to store parameters, plot progress and to the validation\n        :param dataset_directory: the parent directory in which the preprocessed Task data is stored. This is required\n        because the split information is stored in this directory. For running prediction only this input is not\n        required and may be set to None\n        :param batch_dice: compute dice loss for each sample and average over all samples in the batch or pretend the\n        batch is a pseudo volume?\n        :param stage: The plans file may contain several stages (used for lowres / highres / pyramid). Stage must be\n        specified for training:\n        if stage 1 exists then stage 1 is the high resolution stage, otherwise it's 0\n        :param unpack_data: if False, npz preprocessed data will not be unpacked to npy. This consumes less space but\n        is considerably slower! Running unpack_data=False with 2d should never be done!\n\n        IMPORTANT: If you inherit from nnUNetTrainer and the init args change then you need to redefine self.init_args\n        in your init accordingly. Otherwise checkpoints won't load properly!\n        \"\"\"", "\n", "super", "(", "nnUNetTrainer", ",", "self", ")", ".", "__init__", "(", "deterministic", ",", "fp16", ")", "\n", "self", ".", "unpack_data", "=", "unpack_data", "\n", "self", ".", "init_args", "=", "(", "plans_file", ",", "fold", ",", "output_folder", ",", "dataset_directory", ",", "batch_dice", ",", "stage", ",", "unpack_data", ",", "\n", "deterministic", ",", "fp16", ")", "\n", "# set through arguments from init", "\n", "self", ".", "stage", "=", "stage", "\n", "self", ".", "experiment_name", "=", "self", ".", "__class__", ".", "__name__", "\n", "self", ".", "plans_file", "=", "plans_file", "\n", "self", ".", "output_folder", "=", "output_folder", "\n", "self", ".", "dataset_directory", "=", "dataset_directory", "\n", "self", ".", "output_folder_base", "=", "self", ".", "output_folder", "\n", "self", ".", "fold", "=", "fold", "\n", "\n", "self", ".", "plans", "=", "None", "\n", "\n", "# if we are running inference only then the self.dataset_directory is set (due to checkpoint loading) but it", "\n", "# irrelevant", "\n", "if", "self", ".", "dataset_directory", "is", "not", "None", "and", "isdir", "(", "self", ".", "dataset_directory", ")", ":", "\n", "            ", "self", ".", "gt_niftis_folder", "=", "join", "(", "self", ".", "dataset_directory", ",", "\"gt_segmentations\"", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "gt_niftis_folder", "=", "None", "\n", "\n", "", "self", ".", "folder_with_preprocessed_data", "=", "None", "\n", "\n", "# set in self.initialize()", "\n", "\n", "self", ".", "dl_tr", "=", "self", ".", "dl_val", "=", "None", "\n", "self", ".", "num_input_channels", "=", "self", ".", "num_classes", "=", "self", ".", "net_pool_per_axis", "=", "self", ".", "patch_size", "=", "self", ".", "batch_size", "=", "self", ".", "threeD", "=", "self", ".", "base_num_features", "=", "self", ".", "intensity_properties", "=", "self", ".", "normalization_schemes", "=", "self", ".", "net_num_pool_op_kernel_sizes", "=", "self", ".", "net_conv_kernel_sizes", "=", "None", "# loaded automatically from plans_file", "\n", "self", ".", "basic_generator_patch_size", "=", "self", ".", "data_aug_params", "=", "self", ".", "transpose_forward", "=", "self", ".", "transpose_backward", "=", "None", "\n", "\n", "self", ".", "batch_dice", "=", "batch_dice", "\n", "self", ".", "loss", "=", "DC_and_CE_loss", "(", "{", "'batch_dice'", ":", "self", ".", "batch_dice", ",", "'smooth'", ":", "1e-5", ",", "'do_bg'", ":", "False", "}", ",", "{", "}", ")", "\n", "\n", "self", ".", "online_eval_foreground_dc", "=", "[", "]", "\n", "self", ".", "online_eval_tp", "=", "[", "]", "\n", "self", ".", "online_eval_fp", "=", "[", "]", "\n", "self", ".", "online_eval_fn", "=", "[", "]", "\n", "\n", "self", ".", "classes", "=", "self", ".", "do_dummy_2D_aug", "=", "self", ".", "use_mask_for_norm", "=", "self", ".", "only_keep_largest_connected_component", "=", "self", ".", "min_region_size_per_class", "=", "self", ".", "min_size_per_class", "=", "None", "\n", "\n", "self", ".", "inference_pad_border_mode", "=", "\"constant\"", "\n", "self", ".", "inference_pad_kwargs", "=", "{", "'constant_values'", ":", "0", "}", "\n", "\n", "self", ".", "update_fold", "(", "fold", ")", "\n", "self", ".", "pad_all_sides", "=", "None", "\n", "\n", "self", ".", "lr_scheduler_eps", "=", "1e-3", "\n", "self", ".", "lr_scheduler_patience", "=", "30", "\n", "self", ".", "initial_lr", "=", "3e-4", "\n", "self", ".", "weight_decay", "=", "3e-5", "\n", "\n", "self", ".", "oversample_foreground_percent", "=", "0.33", "\n", "\n", "self", ".", "conv_per_stage", "=", "None", "\n", "self", ".", "regions_class_order", "=", "None", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.network_training.nnUNetTrainer.nnUNetTrainer.update_fold": [[134, 152], ["isinstance", "nnUNetTrainer.nnUNetTrainer.output_folder.endswith", "join", "nnUNetTrainer.nnUNetTrainer.output_folder.endswith", "join", "str", "str", "str", "str"], "methods", ["None"], ["", "def", "update_fold", "(", "self", ",", "fold", ")", ":", "\n", "        ", "\"\"\"\n        used to swap between folds for inference (ensemble of models from cross-validation)\n        DO NOT USE DURING TRAINING AS THIS WILL NOT UPDATE THE DATASET SPLIT AND THE DATA AUGMENTATION GENERATORS\n        :param fold:\n        :return:\n        \"\"\"", "\n", "if", "fold", "is", "not", "None", ":", "\n", "            ", "if", "isinstance", "(", "fold", ",", "str", ")", ":", "\n", "                ", "assert", "fold", "==", "\"all\"", ",", "\"if self.fold is a string then it must be \\'all\\'\"", "\n", "if", "self", ".", "output_folder", ".", "endswith", "(", "\"%s\"", "%", "str", "(", "self", ".", "fold", ")", ")", ":", "\n", "                    ", "self", ".", "output_folder", "=", "self", ".", "output_folder_base", "\n", "", "self", ".", "output_folder", "=", "join", "(", "self", ".", "output_folder", ",", "\"%s\"", "%", "str", "(", "fold", ")", ")", "\n", "", "else", ":", "\n", "                ", "if", "self", ".", "output_folder", ".", "endswith", "(", "\"fold_%s\"", "%", "str", "(", "self", ".", "fold", ")", ")", ":", "\n", "                    ", "self", ".", "output_folder", "=", "self", ".", "output_folder_base", "\n", "", "self", ".", "output_folder", "=", "join", "(", "self", ".", "output_folder", ",", "\"fold_%s\"", "%", "str", "(", "fold", ")", ")", "\n", "", "self", ".", "fold", "=", "fold", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.network_training.nnUNetTrainer.nnUNetTrainer.setup_DA_params": [[153, 188], ["nnunet.training.data_augmentation.default_data_augmentation.get_patch_size", "numpy.array", "nnunet.training.data_augmentation.default_data_augmentation.get_patch_size", "nnUNetTrainer.nnUNetTrainer.print_to_log_file", "max", "min", "list"], "methods", ["home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.data_augmentation.default_data_augmentation.get_patch_size", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.data_augmentation.default_data_augmentation.get_patch_size", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.network_training.nnUNetTrainerV2_DDP.nnUNetTrainerV2_DDP.print_to_log_file"], ["", "", "def", "setup_DA_params", "(", "self", ")", ":", "\n", "        ", "if", "self", ".", "threeD", ":", "\n", "            ", "self", ".", "data_aug_params", "=", "default_3D_augmentation_params", "\n", "if", "self", ".", "do_dummy_2D_aug", ":", "\n", "                ", "self", ".", "data_aug_params", "[", "\"dummy_2D\"", "]", "=", "True", "\n", "self", ".", "print_to_log_file", "(", "\"Using dummy2d data augmentation\"", ")", "\n", "self", ".", "data_aug_params", "[", "\"elastic_deform_alpha\"", "]", "=", "default_2D_augmentation_params", "[", "\"elastic_deform_alpha\"", "]", "\n", "self", ".", "data_aug_params", "[", "\"elastic_deform_sigma\"", "]", "=", "default_2D_augmentation_params", "[", "\"elastic_deform_sigma\"", "]", "\n", "self", ".", "data_aug_params", "[", "\"rotation_x\"", "]", "=", "default_2D_augmentation_params", "[", "\"rotation_x\"", "]", "\n", "", "", "else", ":", "\n", "            ", "self", ".", "do_dummy_2D_aug", "=", "False", "\n", "if", "max", "(", "self", ".", "patch_size", ")", "/", "min", "(", "self", ".", "patch_size", ")", ">", "1.5", ":", "\n", "                ", "default_2D_augmentation_params", "[", "'rotation_x'", "]", "=", "(", "-", "15.", "/", "360", "*", "2.", "*", "np", ".", "pi", ",", "15.", "/", "360", "*", "2.", "*", "np", ".", "pi", ")", "\n", "", "self", ".", "data_aug_params", "=", "default_2D_augmentation_params", "\n", "", "self", ".", "data_aug_params", "[", "\"mask_was_used_for_normalization\"", "]", "=", "self", ".", "use_mask_for_norm", "\n", "\n", "if", "self", ".", "do_dummy_2D_aug", ":", "\n", "            ", "self", ".", "basic_generator_patch_size", "=", "get_patch_size", "(", "self", ".", "patch_size", "[", "1", ":", "]", ",", "\n", "self", ".", "data_aug_params", "[", "'rotation_x'", "]", ",", "\n", "self", ".", "data_aug_params", "[", "'rotation_y'", "]", ",", "\n", "self", ".", "data_aug_params", "[", "'rotation_z'", "]", ",", "\n", "self", ".", "data_aug_params", "[", "'scale_range'", "]", ")", "\n", "self", ".", "basic_generator_patch_size", "=", "np", ".", "array", "(", "[", "self", ".", "patch_size", "[", "0", "]", "]", "+", "list", "(", "self", ".", "basic_generator_patch_size", ")", ")", "\n", "patch_size_for_spatialtransform", "=", "self", ".", "patch_size", "[", "1", ":", "]", "\n", "", "else", ":", "\n", "            ", "self", ".", "basic_generator_patch_size", "=", "get_patch_size", "(", "self", ".", "patch_size", ",", "self", ".", "data_aug_params", "[", "'rotation_x'", "]", ",", "\n", "self", ".", "data_aug_params", "[", "'rotation_y'", "]", ",", "\n", "self", ".", "data_aug_params", "[", "'rotation_z'", "]", ",", "\n", "self", ".", "data_aug_params", "[", "'scale_range'", "]", ")", "\n", "patch_size_for_spatialtransform", "=", "self", ".", "patch_size", "\n", "\n", "", "self", ".", "data_aug_params", "[", "'selected_seg_channels'", "]", "=", "[", "0", "]", "\n", "self", ".", "data_aug_params", "[", "'patch_size_for_spatialtransform'", "]", "=", "patch_size_for_spatialtransform", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.network_training.nnUNetTrainer.nnUNetTrainer.initialize": [[189, 233], ["maybe_mkdir_p", "nnUNetTrainer.nnUNetTrainer.process_plans", "nnUNetTrainer.nnUNetTrainer.setup_DA_params", "nnUNetTrainer.nnUNetTrainer.initialize_network", "nnUNetTrainer.nnUNetTrainer.initialize_optimizer_and_scheduler", "nnUNetTrainer.nnUNetTrainer.load_plans_file", "join", "nnUNetTrainer.nnUNetTrainer.get_basic_generators", "nnunet.training.data_augmentation.default_data_augmentation.get_default_augmentation", "nnUNetTrainer.nnUNetTrainer.print_to_log_file", "nnUNetTrainer.nnUNetTrainer.print_to_log_file", "nnUNetTrainer.nnUNetTrainer.print_to_log_file", "nnunet.training.dataloading.dataset_loading.unpack_dataset", "nnUNetTrainer.nnUNetTrainer.print_to_log_file", "nnUNetTrainer.nnUNetTrainer.print_to_log_file", "str", "str", "nnUNetTrainer.nnUNetTrainer.dataset_tr.keys", "nnUNetTrainer.nnUNetTrainer.dataset_val.keys"], "methods", ["home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.BraTS_trainer.nnUNetTrainerV2BraTSRegions.nnUNetTrainerV2BraTSRegions_DA3_BN.process_plans", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.BraTS_trainer.nnUNetTrainerV2BraTSRegions.nnUNetTrainerV2BraTSRegions_DA4_BN.setup_DA_params", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.BraTS_trainer.nnUNetTrainerV2BraTSRegions.nnUNetTrainerV2BraTSRegions_DA3.initialize_network", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.optimizer_and_lr.nnUNetTrainerV2_momentum09in2D.nnUNetTrainerV2_momentum09in2D.initialize_optimizer_and_scheduler", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.network_training.nnUNetTrainer.nnUNetTrainer.load_plans_file", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.nnUNet_variants.nnUNetTrainerNoDA.nnUNetTrainerNoDA.get_basic_generators", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.data_augmentation.default_data_augmentation.get_default_augmentation", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.network_training.nnUNetTrainerV2_DDP.nnUNetTrainerV2_DDP.print_to_log_file", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.network_training.nnUNetTrainerV2_DDP.nnUNetTrainerV2_DDP.print_to_log_file", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.network_training.nnUNetTrainerV2_DDP.nnUNetTrainerV2_DDP.print_to_log_file", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.dataloading.dataset_loading.unpack_dataset", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.network_training.nnUNetTrainerV2_DDP.nnUNetTrainerV2_DDP.print_to_log_file", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.network_training.nnUNetTrainerV2_DDP.nnUNetTrainerV2_DDP.print_to_log_file"], ["", "def", "initialize", "(", "self", ",", "training", "=", "True", ",", "force_load_plans", "=", "False", ")", ":", "\n", "        ", "\"\"\"\n        For prediction of test cases just set training=False, this will prevent loading of training data and\n        training batchgenerator initialization\n        :param training:\n        :return:\n        \"\"\"", "\n", "\n", "maybe_mkdir_p", "(", "self", ".", "output_folder", ")", "\n", "\n", "if", "force_load_plans", "or", "(", "self", ".", "plans", "is", "None", ")", ":", "\n", "            ", "self", ".", "load_plans_file", "(", ")", "\n", "\n", "", "self", ".", "process_plans", "(", "self", ".", "plans", ")", "\n", "\n", "self", ".", "setup_DA_params", "(", ")", "\n", "\n", "if", "training", ":", "\n", "            ", "self", ".", "folder_with_preprocessed_data", "=", "join", "(", "self", ".", "dataset_directory", ",", "self", ".", "plans", "[", "'data_identifier'", "]", "+", "\n", "\"_stage%d\"", "%", "self", ".", "stage", ")", "\n", "\n", "self", ".", "dl_tr", ",", "self", ".", "dl_val", "=", "self", ".", "get_basic_generators", "(", ")", "\n", "if", "self", ".", "unpack_data", ":", "\n", "                ", "self", ".", "print_to_log_file", "(", "\"unpacking dataset\"", ")", "\n", "unpack_dataset", "(", "self", ".", "folder_with_preprocessed_data", ")", "\n", "self", ".", "print_to_log_file", "(", "\"done\"", ")", "\n", "", "else", ":", "\n", "                ", "self", ".", "print_to_log_file", "(", "\n", "\"INFO: Not unpacking data! Training may be slow due to that. Pray you are not using 2d or you \"", "\n", "\"will wait all winter for your model to finish!\"", ")", "\n", "", "self", ".", "tr_gen", ",", "self", ".", "val_gen", "=", "get_default_augmentation", "(", "self", ".", "dl_tr", ",", "self", ".", "dl_val", ",", "\n", "self", ".", "data_aug_params", "[", "\n", "'patch_size_for_spatialtransform'", "]", ",", "\n", "self", ".", "data_aug_params", ")", "\n", "self", ".", "print_to_log_file", "(", "\"TRAINING KEYS:\\n %s\"", "%", "(", "str", "(", "self", ".", "dataset_tr", ".", "keys", "(", ")", ")", ")", ",", "\n", "also_print_to_console", "=", "False", ")", "\n", "self", ".", "print_to_log_file", "(", "\"VALIDATION KEYS:\\n %s\"", "%", "(", "str", "(", "self", ".", "dataset_val", ".", "keys", "(", ")", ")", ")", ",", "\n", "also_print_to_console", "=", "False", ")", "\n", "", "else", ":", "\n", "            ", "pass", "\n", "", "self", ".", "initialize_network", "(", ")", "\n", "self", ".", "initialize_optimizer_and_scheduler", "(", ")", "\n", "# assert isinstance(self.network, (SegmentationNetwork, nn.DataParallel))", "\n", "self", ".", "was_initialized", "=", "True", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.network_training.nnUNetTrainer.nnUNetTrainer.initialize_network": [[234, 266], ["len", "nnunet.network_architecture.generic_UNet.Generic_UNet", "torch.cuda.is_available", "nnunet.network_architecture.initialization.InitWeights_He", "nnUNetTrainer.nnUNetTrainer.network.cuda"], "methods", ["None"], ["", "def", "initialize_network", "(", "self", ")", ":", "\n", "        ", "\"\"\"\n        This is specific to the U-Net and must be adapted for other network architectures\n        :return:\n        \"\"\"", "\n", "# self.print_to_log_file(self.net_num_pool_op_kernel_sizes)", "\n", "# self.print_to_log_file(self.net_conv_kernel_sizes)", "\n", "\n", "net_numpool", "=", "len", "(", "self", ".", "net_num_pool_op_kernel_sizes", ")", "\n", "\n", "if", "self", ".", "threeD", ":", "\n", "            ", "conv_op", "=", "nn", ".", "Conv3d", "\n", "dropout_op", "=", "nn", ".", "Dropout3d", "\n", "norm_op", "=", "nn", ".", "InstanceNorm3d", "\n", "", "else", ":", "\n", "            ", "conv_op", "=", "nn", ".", "Conv2d", "\n", "dropout_op", "=", "nn", ".", "Dropout2d", "\n", "norm_op", "=", "nn", ".", "InstanceNorm2d", "\n", "\n", "", "norm_op_kwargs", "=", "{", "'eps'", ":", "1e-5", ",", "'affine'", ":", "True", "}", "\n", "dropout_op_kwargs", "=", "{", "'p'", ":", "0", ",", "'inplace'", ":", "True", "}", "\n", "net_nonlin", "=", "nn", ".", "LeakyReLU", "\n", "net_nonlin_kwargs", "=", "{", "'negative_slope'", ":", "1e-2", ",", "'inplace'", ":", "True", "}", "\n", "self", ".", "network", "=", "Generic_UNet", "(", "self", ".", "num_input_channels", ",", "self", ".", "base_num_features", ",", "self", ".", "num_classes", ",", "net_numpool", ",", "\n", "self", ".", "conv_per_stage", ",", "2", ",", "conv_op", ",", "norm_op", ",", "norm_op_kwargs", ",", "dropout_op", ",", "\n", "dropout_op_kwargs", ",", "\n", "net_nonlin", ",", "net_nonlin_kwargs", ",", "False", ",", "False", ",", "lambda", "x", ":", "x", ",", "InitWeights_He", "(", "1e-2", ")", ",", "\n", "self", ".", "net_num_pool_op_kernel_sizes", ",", "self", ".", "net_conv_kernel_sizes", ",", "False", ",", "True", ",", "True", ")", "\n", "self", ".", "network", ".", "inference_apply_nonlin", "=", "softmax_helper", "\n", "\n", "if", "torch", ".", "cuda", ".", "is_available", "(", ")", ":", "\n", "            ", "self", ".", "network", ".", "cuda", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.network_training.nnUNetTrainer.nnUNetTrainer.initialize_optimizer_and_scheduler": [[267, 275], ["torch.optim.Adam", "torch.optim.lr_scheduler.ReduceLROnPlateau", "nnUNetTrainer.nnUNetTrainer.network.parameters"], "methods", ["None"], ["", "", "def", "initialize_optimizer_and_scheduler", "(", "self", ")", ":", "\n", "        ", "assert", "self", ".", "network", "is", "not", "None", ",", "\"self.initialize_network must be called first\"", "\n", "self", ".", "optimizer", "=", "torch", ".", "optim", ".", "Adam", "(", "self", ".", "network", ".", "parameters", "(", ")", ",", "self", ".", "initial_lr", ",", "weight_decay", "=", "self", ".", "weight_decay", ",", "\n", "amsgrad", "=", "True", ")", "\n", "self", ".", "lr_scheduler", "=", "lr_scheduler", ".", "ReduceLROnPlateau", "(", "self", ".", "optimizer", ",", "mode", "=", "'min'", ",", "factor", "=", "0.2", ",", "\n", "patience", "=", "self", ".", "lr_scheduler_patience", ",", "\n", "verbose", "=", "True", ",", "threshold", "=", "self", ".", "lr_scheduler_eps", ",", "\n", "threshold_mode", "=", "\"abs\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.network_training.nnUNetTrainer.nnUNetTrainer.plot_network_architecture": [[276, 298], ["torch.cuda.is_available", "hl.build_graph.save", "torch.cuda.is_available", "hl.build_graph", "hl.build_graph", "join", "nnUNetTrainer.nnUNetTrainer.print_to_log_file", "nnUNetTrainer.nnUNetTrainer.print_to_log_file", "nnUNetTrainer.nnUNetTrainer.print_to_log_file", "nnUNetTrainer.nnUNetTrainer.print_to_log_file", "nnUNetTrainer.nnUNetTrainer.print_to_log_file", "torch.cuda.empty_cache", "torch.rand().cuda", "torch.rand", "torch.rand"], "methods", ["home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.network_training.nnUNetTrainerV2_DDP.nnUNetTrainerV2_DDP.print_to_log_file", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.network_training.nnUNetTrainerV2_DDP.nnUNetTrainerV2_DDP.print_to_log_file", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.network_training.nnUNetTrainerV2_DDP.nnUNetTrainerV2_DDP.print_to_log_file", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.network_training.nnUNetTrainerV2_DDP.nnUNetTrainerV2_DDP.print_to_log_file", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.network_training.nnUNetTrainerV2_DDP.nnUNetTrainerV2_DDP.print_to_log_file"], ["", "def", "plot_network_architecture", "(", "self", ")", ":", "\n", "        ", "try", ":", "\n", "            ", "from", "batchgenerators", ".", "utilities", ".", "file_and_folder_operations", "import", "join", "\n", "import", "hiddenlayer", "as", "hl", "\n", "if", "torch", ".", "cuda", ".", "is_available", "(", ")", ":", "\n", "                ", "g", "=", "hl", ".", "build_graph", "(", "self", ".", "network", ",", "torch", ".", "rand", "(", "(", "1", ",", "self", ".", "num_input_channels", ",", "*", "self", ".", "patch_size", ")", ")", ".", "cuda", "(", ")", ",", "\n", "transforms", "=", "None", ")", "\n", "", "else", ":", "\n", "                ", "g", "=", "hl", ".", "build_graph", "(", "self", ".", "network", ",", "torch", ".", "rand", "(", "(", "1", ",", "self", ".", "num_input_channels", ",", "*", "self", ".", "patch_size", ")", ")", ",", "\n", "transforms", "=", "None", ")", "\n", "", "g", ".", "save", "(", "join", "(", "self", ".", "output_folder", ",", "\"network_architecture.pdf\"", ")", ")", "\n", "del", "g", "\n", "", "except", "Exception", "as", "e", ":", "\n", "            ", "self", ".", "print_to_log_file", "(", "\"Unable to plot network architecture:\"", ")", "\n", "self", ".", "print_to_log_file", "(", "e", ")", "\n", "\n", "self", ".", "print_to_log_file", "(", "\"\\nprinting the network instead:\\n\"", ")", "\n", "self", ".", "print_to_log_file", "(", "self", ".", "network", ")", "\n", "self", ".", "print_to_log_file", "(", "\"\\n\"", ")", "\n", "", "finally", ":", "\n", "            ", "if", "torch", ".", "cuda", ".", "is_available", "(", ")", ":", "\n", "                ", "torch", ".", "cuda", ".", "empty_cache", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.network_training.nnUNetTrainer.nnUNetTrainer.run_training": [[299, 317], ["collections.OrderedDict", "nnUNetTrainer.nnUNetTrainer.__dir__", "save_json", "shutil.copy", "super().run_training", "join", "join", "k.startswith", "callable", "str", "getattr", "getattr"], "methods", ["home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.architectural_variants.nnUNetTrainerV2_ResencUNet.nnUNetTrainerV2_ResencUNet.run_training"], ["", "", "", "def", "run_training", "(", "self", ")", ":", "\n", "        ", "dct", "=", "OrderedDict", "(", ")", "\n", "for", "k", "in", "self", ".", "__dir__", "(", ")", ":", "\n", "            ", "if", "not", "k", ".", "startswith", "(", "\"__\"", ")", ":", "\n", "                ", "if", "not", "callable", "(", "getattr", "(", "self", ",", "k", ")", ")", ":", "\n", "                    ", "dct", "[", "k", "]", "=", "str", "(", "getattr", "(", "self", ",", "k", ")", ")", "\n", "", "", "", "del", "dct", "[", "'plans'", "]", "\n", "del", "dct", "[", "'intensity_properties'", "]", "\n", "del", "dct", "[", "'dataset'", "]", "\n", "del", "dct", "[", "'dataset_tr'", "]", "\n", "del", "dct", "[", "'dataset_val'", "]", "\n", "save_json", "(", "dct", ",", "join", "(", "self", ".", "output_folder", ",", "\"debug.json\"", ")", ")", "\n", "\n", "import", "shutil", "\n", "\n", "shutil", ".", "copy", "(", "self", ".", "plans_file", ",", "join", "(", "self", ".", "output_folder_base", ",", "\"plans.pkl\"", ")", ")", "\n", "\n", "super", "(", "nnUNetTrainer", ",", "self", ")", ".", "run_training", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.network_training.nnUNetTrainer.nnUNetTrainer.load_plans_file": [[318, 324], ["load_pickle"], "methods", ["None"], ["", "def", "load_plans_file", "(", "self", ")", ":", "\n", "        ", "\"\"\"\n        This is what actually configures the entire experiment. The plans file is generated by experiment planning\n        :return:\n        \"\"\"", "\n", "self", ".", "plans", "=", "load_pickle", "(", "self", ".", "plans_file", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.network_training.nnUNetTrainer.nnUNetTrainer.process_plans": [[325, 392], ["numpy.array().astype", "stage_plans.keys", "nnUNetTrainer.nnUNetTrainer.print_to_log_file", "range", "stage_plans.keys", "nnUNetTrainer.nnUNetTrainer.print_to_log_file", "print", "len", "plans.keys", "len", "list", "numpy.array", "stage_plans.keys", "max", "nnUNetTrainer.nnUNetTrainer.net_num_pool_op_kernel_sizes.append", "plans.get", "plans.get", "len", "RuntimeError", "list", "plans[].keys", "max", "plans[].keys", "curr.append", "curr.append", "len", "str", "max"], "methods", ["home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.network_training.nnUNetTrainerV2_DDP.nnUNetTrainerV2_DDP.print_to_log_file", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.network_training.nnUNetTrainerV2_DDP.nnUNetTrainerV2_DDP.print_to_log_file"], ["", "def", "process_plans", "(", "self", ",", "plans", ")", ":", "\n", "        ", "if", "self", ".", "stage", "is", "None", ":", "\n", "            ", "assert", "len", "(", "list", "(", "plans", "[", "'plans_per_stage'", "]", ".", "keys", "(", ")", ")", ")", "==", "1", ",", "\"If self.stage is None then there can be only one stage in the plans file. That seems to not be the \"", "\"case. Please specify which stage of the cascade must be trained\"", "\n", "self", ".", "stage", "=", "list", "(", "plans", "[", "'plans_per_stage'", "]", ".", "keys", "(", ")", ")", "[", "0", "]", "\n", "", "self", ".", "plans", "=", "plans", "\n", "\n", "stage_plans", "=", "self", ".", "plans", "[", "'plans_per_stage'", "]", "[", "self", ".", "stage", "]", "\n", "self", ".", "batch_size", "=", "stage_plans", "[", "'batch_size'", "]", "\n", "self", ".", "net_pool_per_axis", "=", "stage_plans", "[", "'num_pool_per_axis'", "]", "\n", "self", ".", "patch_size", "=", "np", ".", "array", "(", "stage_plans", "[", "'patch_size'", "]", ")", ".", "astype", "(", "int", ")", "\n", "self", ".", "do_dummy_2D_aug", "=", "stage_plans", "[", "'do_dummy_2D_data_aug'", "]", "\n", "\n", "if", "'pool_op_kernel_sizes'", "not", "in", "stage_plans", ".", "keys", "(", ")", ":", "\n", "            ", "assert", "'num_pool_per_axis'", "in", "stage_plans", ".", "keys", "(", ")", "\n", "self", ".", "print_to_log_file", "(", "\"WARNING! old plans file with missing pool_op_kernel_sizes. Attempting to fix it...\"", ")", "\n", "self", ".", "net_num_pool_op_kernel_sizes", "=", "[", "]", "\n", "for", "i", "in", "range", "(", "max", "(", "self", ".", "net_pool_per_axis", ")", ")", ":", "\n", "                ", "curr", "=", "[", "]", "\n", "for", "j", "in", "self", ".", "net_pool_per_axis", ":", "\n", "                    ", "if", "(", "max", "(", "self", ".", "net_pool_per_axis", ")", "-", "j", ")", "<=", "i", ":", "\n", "                        ", "curr", ".", "append", "(", "2", ")", "\n", "", "else", ":", "\n", "                        ", "curr", ".", "append", "(", "1", ")", "\n", "", "", "self", ".", "net_num_pool_op_kernel_sizes", ".", "append", "(", "curr", ")", "\n", "", "", "else", ":", "\n", "            ", "self", ".", "net_num_pool_op_kernel_sizes", "=", "stage_plans", "[", "'pool_op_kernel_sizes'", "]", "\n", "\n", "", "if", "'conv_kernel_sizes'", "not", "in", "stage_plans", ".", "keys", "(", ")", ":", "\n", "            ", "self", ".", "print_to_log_file", "(", "\"WARNING! old plans file with missing conv_kernel_sizes. Attempting to fix it...\"", ")", "\n", "self", ".", "net_conv_kernel_sizes", "=", "[", "[", "3", "]", "*", "len", "(", "self", ".", "net_pool_per_axis", ")", "]", "*", "(", "max", "(", "self", ".", "net_pool_per_axis", ")", "+", "1", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "net_conv_kernel_sizes", "=", "stage_plans", "[", "'conv_kernel_sizes'", "]", "\n", "\n", "", "self", ".", "pad_all_sides", "=", "None", "# self.patch_size", "\n", "self", ".", "intensity_properties", "=", "plans", "[", "'dataset_properties'", "]", "[", "'intensityproperties'", "]", "\n", "self", ".", "normalization_schemes", "=", "plans", "[", "'normalization_schemes'", "]", "\n", "self", ".", "base_num_features", "=", "plans", "[", "'base_num_features'", "]", "\n", "self", ".", "num_input_channels", "=", "plans", "[", "'num_modalities'", "]", "\n", "self", ".", "num_classes", "=", "plans", "[", "'num_classes'", "]", "+", "1", "# background is no longer in num_classes", "\n", "self", ".", "classes", "=", "plans", "[", "'all_classes'", "]", "\n", "self", ".", "use_mask_for_norm", "=", "plans", "[", "'use_mask_for_norm'", "]", "\n", "self", ".", "only_keep_largest_connected_component", "=", "plans", "[", "'keep_only_largest_region'", "]", "\n", "self", ".", "min_region_size_per_class", "=", "plans", "[", "'min_region_size_per_class'", "]", "\n", "self", ".", "min_size_per_class", "=", "None", "# DONT USE THIS. plans['min_size_per_class']", "\n", "\n", "if", "plans", ".", "get", "(", "'transpose_forward'", ")", "is", "None", "or", "plans", ".", "get", "(", "'transpose_backward'", ")", "is", "None", ":", "\n", "            ", "print", "(", "\"WARNING! You seem to have data that was preprocessed with a previous version of nnU-Net. \"", "\n", "\"You should rerun preprocessing. We will proceed and assume that both transpose_foward \"", "\n", "\"and transpose_backward are [0, 1, 2]. If that is not correct then weird things will happen!\"", ")", "\n", "plans", "[", "'transpose_forward'", "]", "=", "[", "0", ",", "1", ",", "2", "]", "\n", "plans", "[", "'transpose_backward'", "]", "=", "[", "0", ",", "1", ",", "2", "]", "\n", "", "self", ".", "transpose_forward", "=", "plans", "[", "'transpose_forward'", "]", "\n", "self", ".", "transpose_backward", "=", "plans", "[", "'transpose_backward'", "]", "\n", "\n", "if", "len", "(", "self", ".", "patch_size", ")", "==", "2", ":", "\n", "            ", "self", ".", "threeD", "=", "False", "\n", "", "elif", "len", "(", "self", ".", "patch_size", ")", "==", "3", ":", "\n", "            ", "self", ".", "threeD", "=", "True", "\n", "", "else", ":", "\n", "            ", "raise", "RuntimeError", "(", "\"invalid patch size in plans file: %s\"", "%", "str", "(", "self", ".", "patch_size", ")", ")", "\n", "\n", "", "if", "\"conv_per_stage\"", "in", "plans", ".", "keys", "(", ")", ":", "# this ha sbeen added to the plans only recently", "\n", "            ", "self", ".", "conv_per_stage", "=", "plans", "[", "'conv_per_stage'", "]", "\n", "", "else", ":", "\n", "            ", "self", ".", "conv_per_stage", "=", "2", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.network_training.nnUNetTrainer.nnUNetTrainer.load_dataset": [[393, 395], ["nnunet.training.dataloading.dataset_loading.load_dataset"], "methods", ["home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.dataloading.dataset_loading.load_dataset"], ["", "", "def", "load_dataset", "(", "self", ")", ":", "\n", "        ", "self", ".", "dataset", "=", "load_dataset", "(", "self", ".", "folder_with_preprocessed_data", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.network_training.nnUNetTrainer.nnUNetTrainer.get_basic_generators": [[396, 415], ["nnUNetTrainer.nnUNetTrainer.load_dataset", "nnUNetTrainer.nnUNetTrainer.do_split", "nnunet.training.dataloading.dataset_loading.DataLoader3D", "nnunet.training.dataloading.dataset_loading.DataLoader3D", "nnunet.training.dataloading.dataset_loading.DataLoader2D", "nnunet.training.dataloading.dataset_loading.DataLoader2D"], "methods", ["home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.dataloading.dataset_loading.load_dataset", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.network_training.nnUNetTrainerV2.nnUNetTrainerV2.do_split"], ["", "def", "get_basic_generators", "(", "self", ")", ":", "\n", "        ", "self", ".", "load_dataset", "(", ")", "\n", "self", ".", "do_split", "(", ")", "\n", "\n", "if", "self", ".", "threeD", ":", "\n", "            ", "dl_tr", "=", "DataLoader3D", "(", "self", ".", "dataset_tr", ",", "self", ".", "basic_generator_patch_size", ",", "self", ".", "patch_size", ",", "self", ".", "batch_size", ",", "\n", "False", ",", "oversample_foreground_percent", "=", "self", ".", "oversample_foreground_percent", ",", "\n", "pad_mode", "=", "\"constant\"", ",", "pad_sides", "=", "self", ".", "pad_all_sides", ",", "memmap_mode", "=", "'r'", ")", "\n", "dl_val", "=", "DataLoader3D", "(", "self", ".", "dataset_val", ",", "self", ".", "patch_size", ",", "self", ".", "patch_size", ",", "self", ".", "batch_size", ",", "False", ",", "\n", "oversample_foreground_percent", "=", "self", ".", "oversample_foreground_percent", ",", "\n", "pad_mode", "=", "\"constant\"", ",", "pad_sides", "=", "self", ".", "pad_all_sides", ",", "memmap_mode", "=", "'r'", ")", "\n", "", "else", ":", "\n", "            ", "dl_tr", "=", "DataLoader2D", "(", "self", ".", "dataset_tr", ",", "self", ".", "basic_generator_patch_size", ",", "self", ".", "patch_size", ",", "self", ".", "batch_size", ",", "\n", "oversample_foreground_percent", "=", "self", ".", "oversample_foreground_percent", ",", "\n", "pad_mode", "=", "\"constant\"", ",", "pad_sides", "=", "self", ".", "pad_all_sides", ",", "memmap_mode", "=", "'r'", ")", "\n", "dl_val", "=", "DataLoader2D", "(", "self", ".", "dataset_val", ",", "self", ".", "patch_size", ",", "self", ".", "patch_size", ",", "self", ".", "batch_size", ",", "\n", "oversample_foreground_percent", "=", "self", ".", "oversample_foreground_percent", ",", "\n", "pad_mode", "=", "\"constant\"", ",", "pad_sides", "=", "self", ".", "pad_all_sides", ",", "memmap_mode", "=", "'r'", ")", "\n", "", "return", "dl_tr", ",", "dl_val", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.network_training.nnUNetTrainer.nnUNetTrainer.preprocess_patient": [[416, 443], ["nnUNetTrainer.nnUNetTrainer.plans.get", "print", "recursive_find_python_class", "recursive_find_python_class.", "recursive_find_python_class.preprocess_test_case", "join"], "methods", ["home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.training.model_restore.recursive_find_python_class", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.preprocessing.preprocessing.GenericPreprocessor.preprocess_test_case"], ["", "def", "preprocess_patient", "(", "self", ",", "input_files", ")", ":", "\n", "        ", "\"\"\"\n        Used to predict new unseen data. Not used for the preprocessing of the training/test data\n        :param input_files:\n        :return:\n        \"\"\"", "\n", "from", "nnunet", ".", "training", ".", "model_restore", "import", "recursive_find_python_class", "\n", "preprocessor_name", "=", "self", ".", "plans", ".", "get", "(", "'preprocessor_name'", ")", "\n", "if", "preprocessor_name", "is", "None", ":", "\n", "            ", "if", "self", ".", "threeD", ":", "\n", "                ", "preprocessor_name", "=", "\"GenericPreprocessor\"", "\n", "", "else", ":", "\n", "                ", "preprocessor_name", "=", "\"PreprocessorFor2D\"", "\n", "\n", "", "", "print", "(", "\"using preprocessor\"", ",", "preprocessor_name", ")", "\n", "preprocessor_class", "=", "recursive_find_python_class", "(", "[", "join", "(", "nnunet", ".", "__path__", "[", "0", "]", ",", "\"preprocessing\"", ")", "]", ",", "\n", "preprocessor_name", ",", "\n", "current_module", "=", "\"nnunet.preprocessing\"", ")", "\n", "assert", "preprocessor_class", "is", "not", "None", ",", "\"Could not find preprocessor %s in nnunet.preprocessing\"", "%", "preprocessor_name", "\n", "preprocessor", "=", "preprocessor_class", "(", "self", ".", "normalization_schemes", ",", "self", ".", "use_mask_for_norm", ",", "\n", "self", ".", "transpose_forward", ",", "self", ".", "intensity_properties", ")", "\n", "\n", "d", ",", "s", ",", "properties", "=", "preprocessor", ".", "preprocess_test_case", "(", "input_files", ",", "\n", "self", ".", "plans", "[", "'plans_per_stage'", "]", "[", "self", ".", "stage", "]", "[", "\n", "'current_spacing'", "]", ")", "\n", "return", "d", ",", "s", ",", "properties", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.network_training.nnUNetTrainer.nnUNetTrainer.preprocess_predict_nifti": [[444, 479], ["print", "nnUNetTrainer.nnUNetTrainer.preprocess_patient", "print", "pred.transpose.transpose.transpose", "print", "nnunet.inference.segmentation_export.save_segmentation_nifti_from_softmax", "print", "nnUNetTrainer.nnUNetTrainer.predict_preprocessed_data_return_seg_and_softmax", "nnUNetTrainer.nnUNetTrainer.plans.keys"], "methods", ["home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.network_training.nnUNetTrainer.nnUNetTrainer.preprocess_patient", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.inference.segmentation_export.save_segmentation_nifti_from_softmax", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.architectural_variants.nnUNetTrainerV2_ResencUNet.nnUNetTrainerV2_ResencUNet.predict_preprocessed_data_return_seg_and_softmax"], ["", "def", "preprocess_predict_nifti", "(", "self", ",", "input_files", ":", "List", "[", "str", "]", ",", "output_file", ":", "str", "=", "None", ",", "\n", "softmax_ouput_file", ":", "str", "=", "None", ",", "mixed_precision", ":", "bool", "=", "True", ")", "->", "None", ":", "\n", "        ", "\"\"\"\n        Use this to predict new data\n        :param input_files:\n        :param output_file:\n        :param softmax_ouput_file:\n        :param mixed_precision:\n        :return:\n        \"\"\"", "\n", "print", "(", "\"preprocessing...\"", ")", "\n", "d", ",", "s", ",", "properties", "=", "self", ".", "preprocess_patient", "(", "input_files", ")", "\n", "print", "(", "\"predicting...\"", ")", "\n", "pred", "=", "self", ".", "predict_preprocessed_data_return_seg_and_softmax", "(", "d", ",", "self", ".", "data_aug_params", "[", "\"do_mirror\"", "]", ",", "\n", "self", ".", "data_aug_params", "[", "'mirror_axes'", "]", ",", "True", ",", "0.5", ",", "\n", "True", ",", "'constant'", ",", "{", "'constant_values'", ":", "0", "}", ",", "\n", "self", ".", "patch_size", ",", "True", ",", "\n", "mixed_precision", "=", "mixed_precision", ")", "[", "1", "]", "\n", "pred", "=", "pred", ".", "transpose", "(", "[", "0", "]", "+", "[", "i", "+", "1", "for", "i", "in", "self", ".", "transpose_backward", "]", ")", "\n", "\n", "if", "'segmentation_export_params'", "in", "self", ".", "plans", ".", "keys", "(", ")", ":", "\n", "            ", "force_separate_z", "=", "self", ".", "plans", "[", "'segmentation_export_params'", "]", "[", "'force_separate_z'", "]", "\n", "interpolation_order", "=", "self", ".", "plans", "[", "'segmentation_export_params'", "]", "[", "'interpolation_order'", "]", "\n", "interpolation_order_z", "=", "self", ".", "plans", "[", "'segmentation_export_params'", "]", "[", "'interpolation_order_z'", "]", "\n", "", "else", ":", "\n", "            ", "force_separate_z", "=", "None", "\n", "interpolation_order", "=", "1", "\n", "interpolation_order_z", "=", "0", "\n", "\n", "", "print", "(", "\"resampling to original spacing and nifti export...\"", ")", "\n", "save_segmentation_nifti_from_softmax", "(", "pred", ",", "output_file", ",", "properties", ",", "interpolation_order", ",", "\n", "self", ".", "regions_class_order", ",", "None", ",", "None", ",", "softmax_ouput_file", ",", "\n", "None", ",", "force_separate_z", "=", "force_separate_z", ",", "\n", "interpolation_order_z", "=", "interpolation_order_z", ")", "\n", "print", "(", "\"done\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.network_training.nnUNetTrainer.nnUNetTrainer.predict_preprocessed_data_return_seg_and_softmax": [[480, 519], ["list", "isinstance", "nnUNetTrainer.nnUNetTrainer.network.eval", "nnUNetTrainer.nnUNetTrainer.network.predict_3D", "nnUNetTrainer.nnUNetTrainer.network.train", "tuple"], "methods", ["home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.network_architecture.neural_network.SegmentationNetwork.predict_3D"], ["", "def", "predict_preprocessed_data_return_seg_and_softmax", "(", "self", ",", "data", ":", "np", ".", "ndarray", ",", "do_mirroring", ":", "bool", "=", "True", ",", "\n", "mirror_axes", ":", "Tuple", "[", "int", "]", "=", "None", ",", "\n", "use_sliding_window", ":", "bool", "=", "True", ",", "step_size", ":", "float", "=", "0.5", ",", "\n", "use_gaussian", ":", "bool", "=", "True", ",", "pad_border_mode", ":", "str", "=", "'constant'", ",", "\n", "pad_kwargs", ":", "dict", "=", "None", ",", "all_in_gpu", ":", "bool", "=", "True", ",", "\n", "verbose", ":", "bool", "=", "True", ",", "mixed_precision", ":", "bool", "=", "True", ")", "->", "Tuple", "[", "np", ".", "ndarray", ",", "np", ".", "ndarray", "]", ":", "\n", "        ", "\"\"\"\n        :param data:\n        :param do_mirroring:\n        :param mirror_axes:\n        :param use_sliding_window:\n        :param step_size:\n        :param use_gaussian:\n        :param pad_border_mode:\n        :param pad_kwargs:\n        :param all_in_gpu:\n        :param verbose:\n        :return:\n        \"\"\"", "\n", "if", "pad_border_mode", "==", "'constant'", "and", "pad_kwargs", "is", "None", ":", "\n", "            ", "pad_kwargs", "=", "{", "'constant_values'", ":", "0", "}", "\n", "\n", "", "if", "do_mirroring", "and", "mirror_axes", "is", "None", ":", "\n", "            ", "mirror_axes", "=", "self", ".", "data_aug_params", "[", "'mirror_axes'", "]", "\n", "\n", "", "if", "do_mirroring", ":", "\n", "            ", "assert", "self", ".", "data_aug_params", "[", "\"do_mirror\"", "]", ",", "\"Cannot do mirroring as test time augmentation when training \"", "\"was done without mirroring\"", "\n", "\n", "", "valid", "=", "list", "(", "(", "SegmentationNetwork", ",", "nn", ".", "DataParallel", ")", ")", "\n", "assert", "isinstance", "(", "self", ".", "network", ",", "tuple", "(", "valid", ")", ")", "\n", "\n", "current_mode", "=", "self", ".", "network", ".", "training", "\n", "self", ".", "network", ".", "eval", "(", ")", "\n", "ret", "=", "self", ".", "network", ".", "predict_3D", "(", "data", ",", "do_mirroring", ",", "mirror_axes", ",", "use_sliding_window", ",", "step_size", ",", "self", ".", "patch_size", ",", "\n", "self", ".", "regions_class_order", ",", "use_gaussian", ",", "pad_border_mode", ",", "pad_kwargs", ",", "\n", "all_in_gpu", ",", "verbose", ",", "mixed_precision", "=", "mixed_precision", ")", "\n", "self", ".", "network", ".", "train", "(", "current_mode", ")", "\n", "return", "ret", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.network_training.nnUNetTrainer.nnUNetTrainer.validate": [[520, 672], ["nnUNetTrainer.nnUNetTrainer.network.eval", "join", "maybe_mkdir_p", "save_json", "multiprocessing.Pool", "nnUNetTrainer.nnUNetTrainer.dataset_val.keys", "nnUNetTrainer.nnUNetTrainer.print_to_log_file", "nnUNetTrainer.nnUNetTrainer.print_to_log_file", "nnunet.evaluation.evaluator.aggregate_scores", "nnUNetTrainer.nnUNetTrainer.print_to_log_file", "nnunet.postprocessing.connected_components.determine_postprocessing", "join", "maybe_mkdir_p", "subfiles", "nnUNetTrainer.nnUNetTrainer.network.train", "nnUNetTrainer.nnUNetTrainer.load_dataset", "nnUNetTrainer.nnUNetTrainer.do_split", "join", "load_pickle", "pred_gt_tuples.append", "i.get", "nnUNetTrainer.nnUNetTrainer.dataset_directory.split", "nnUNetTrainer.nnUNetTrainer.plans.keys", "RuntimeError", "print", "join.transpose", "results.append", "list", "join", "print", "[].split", "isfile", "numpy.load", "nnUNetTrainer.nnUNetTrainer.predict_preprocessed_data_return_seg_and_softmax", "join", "numpy.prod", "numpy.save", "join", "multiprocessing.Pool.starmap_async", "join", "join", "range", "shutil.copy", "join", "isfile", "join", "str", "time.sleep", "join", "join"], "methods", ["home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.network_training.nnUNetTrainerV2_DDP.nnUNetTrainerV2_DDP.print_to_log_file", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.network_training.nnUNetTrainerV2_DDP.nnUNetTrainerV2_DDP.print_to_log_file", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.evaluation.evaluator.aggregate_scores", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.network_training.nnUNetTrainerV2_DDP.nnUNetTrainerV2_DDP.print_to_log_file", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.postprocessing.connected_components.determine_postprocessing", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.dataloading.dataset_loading.load_dataset", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.network_training.nnUNetTrainerV2.nnUNetTrainerV2.do_split", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.architectural_variants.nnUNetTrainerV2_ResencUNet.nnUNetTrainerV2_ResencUNet.predict_preprocessed_data_return_seg_and_softmax"], ["", "def", "validate", "(", "self", ",", "do_mirroring", ":", "bool", "=", "True", ",", "use_sliding_window", ":", "bool", "=", "True", ",", "step_size", ":", "float", "=", "0.5", ",", "\n", "save_softmax", ":", "bool", "=", "True", ",", "use_gaussian", ":", "bool", "=", "True", ",", "overwrite", ":", "bool", "=", "True", ",", "\n", "validation_folder_name", ":", "str", "=", "'validation_raw'", ",", "debug", ":", "bool", "=", "False", ",", "all_in_gpu", ":", "bool", "=", "False", ",", "\n", "segmentation_export_kwargs", ":", "dict", "=", "None", ")", ":", "\n", "        ", "\"\"\"\n        if debug=True then the temporary files generated for postprocessing determination will be kept\n        \"\"\"", "\n", "\n", "current_mode", "=", "self", ".", "network", ".", "training", "\n", "self", ".", "network", ".", "eval", "(", ")", "\n", "\n", "assert", "self", ".", "was_initialized", ",", "\"must initialize, ideally with checkpoint (or train first)\"", "\n", "if", "self", ".", "dataset_val", "is", "None", ":", "\n", "            ", "self", ".", "load_dataset", "(", ")", "\n", "self", ".", "do_split", "(", ")", "\n", "\n", "", "if", "segmentation_export_kwargs", "is", "None", ":", "\n", "            ", "if", "'segmentation_export_params'", "in", "self", ".", "plans", ".", "keys", "(", ")", ":", "\n", "                ", "force_separate_z", "=", "self", ".", "plans", "[", "'segmentation_export_params'", "]", "[", "'force_separate_z'", "]", "\n", "interpolation_order", "=", "self", ".", "plans", "[", "'segmentation_export_params'", "]", "[", "'interpolation_order'", "]", "\n", "interpolation_order_z", "=", "self", ".", "plans", "[", "'segmentation_export_params'", "]", "[", "'interpolation_order_z'", "]", "\n", "", "else", ":", "\n", "                ", "force_separate_z", "=", "None", "\n", "interpolation_order", "=", "1", "\n", "interpolation_order_z", "=", "0", "\n", "", "", "else", ":", "\n", "            ", "force_separate_z", "=", "segmentation_export_kwargs", "[", "'force_separate_z'", "]", "\n", "interpolation_order", "=", "segmentation_export_kwargs", "[", "'interpolation_order'", "]", "\n", "interpolation_order_z", "=", "segmentation_export_kwargs", "[", "'interpolation_order_z'", "]", "\n", "\n", "# predictions as they come from the network go here", "\n", "", "output_folder", "=", "join", "(", "self", ".", "output_folder", ",", "validation_folder_name", ")", "\n", "maybe_mkdir_p", "(", "output_folder", ")", "\n", "# this is for debug purposes", "\n", "my_input_args", "=", "{", "'do_mirroring'", ":", "do_mirroring", ",", "\n", "'use_sliding_window'", ":", "use_sliding_window", ",", "\n", "'step_size'", ":", "step_size", ",", "\n", "'save_softmax'", ":", "save_softmax", ",", "\n", "'use_gaussian'", ":", "use_gaussian", ",", "\n", "'overwrite'", ":", "overwrite", ",", "\n", "'validation_folder_name'", ":", "validation_folder_name", ",", "\n", "'debug'", ":", "debug", ",", "\n", "'all_in_gpu'", ":", "all_in_gpu", ",", "\n", "'segmentation_export_kwargs'", ":", "segmentation_export_kwargs", ",", "\n", "}", "\n", "save_json", "(", "my_input_args", ",", "join", "(", "output_folder", ",", "\"validation_args.json\"", ")", ")", "\n", "\n", "if", "do_mirroring", ":", "\n", "            ", "if", "not", "self", ".", "data_aug_params", "[", "'do_mirror'", "]", ":", "\n", "                ", "raise", "RuntimeError", "(", "\"We did not train with mirroring so you cannot do inference with mirroring enabled\"", ")", "\n", "", "mirror_axes", "=", "self", ".", "data_aug_params", "[", "'mirror_axes'", "]", "\n", "", "else", ":", "\n", "            ", "mirror_axes", "=", "(", ")", "\n", "\n", "", "pred_gt_tuples", "=", "[", "]", "\n", "\n", "export_pool", "=", "Pool", "(", "default_num_threads", ")", "\n", "results", "=", "[", "]", "\n", "\n", "for", "k", "in", "self", ".", "dataset_val", ".", "keys", "(", ")", ":", "\n", "            ", "properties", "=", "load_pickle", "(", "self", ".", "dataset", "[", "k", "]", "[", "'properties_file'", "]", ")", "\n", "fname", "=", "properties", "[", "'list_of_data_files'", "]", "[", "0", "]", ".", "split", "(", "\"/\"", ")", "[", "-", "1", "]", "[", ":", "-", "12", "]", "\n", "if", "overwrite", "or", "(", "not", "isfile", "(", "join", "(", "output_folder", ",", "fname", "+", "\".nii.gz\"", ")", ")", ")", "or", "(", "save_softmax", "and", "not", "isfile", "(", "join", "(", "output_folder", ",", "fname", "+", "\".npz\"", ")", ")", ")", ":", "\n", "                ", "data", "=", "np", ".", "load", "(", "self", ".", "dataset", "[", "k", "]", "[", "'data_file'", "]", ")", "[", "'data'", "]", "\n", "\n", "print", "(", "k", ",", "data", ".", "shape", ")", "\n", "data", "[", "-", "1", "]", "[", "data", "[", "-", "1", "]", "==", "-", "1", "]", "=", "0", "\n", "\n", "softmax_pred", "=", "self", ".", "predict_preprocessed_data_return_seg_and_softmax", "(", "data", "[", ":", "-", "1", "]", ",", "do_mirroring", ",", "\n", "mirror_axes", ",", "use_sliding_window", ",", "\n", "step_size", ",", "use_gaussian", ",", "\n", "all_in_gpu", "=", "all_in_gpu", ",", "\n", "mixed_precision", "=", "self", ".", "fp16", ")", "[", "1", "]", "\n", "\n", "softmax_pred", "=", "softmax_pred", ".", "transpose", "(", "[", "0", "]", "+", "[", "i", "+", "1", "for", "i", "in", "self", ".", "transpose_backward", "]", ")", "\n", "\n", "if", "save_softmax", ":", "\n", "                    ", "softmax_fname", "=", "join", "(", "output_folder", ",", "fname", "+", "\".npz\"", ")", "\n", "", "else", ":", "\n", "                    ", "softmax_fname", "=", "None", "\n", "\n", "", "\"\"\"There is a problem with python process communication that prevents us from communicating obejcts\n                larger than 2 GB between processes (basically when the length of the pickle string that will be sent is\n                communicated by the multiprocessing.Pipe object then the placeholder (\\%i I think) does not allow for long\n                enough strings (lol). This could be fixed by changing i to l (for long) but that would require manually\n                patching system python code. We circumvent that problem here by saving softmax_pred to a npy file that will\n                then be read (and finally deleted) by the Process. save_segmentation_nifti_from_softmax can take either\n                filename or np.ndarray and will handle this automatically\"\"\"", "\n", "if", "np", ".", "prod", "(", "softmax_pred", ".", "shape", ")", ">", "(", "2e9", "/", "4", "*", "0.85", ")", ":", "# *0.85 just to be save", "\n", "                    ", "np", ".", "save", "(", "join", "(", "output_folder", ",", "fname", "+", "\".npy\"", ")", ",", "softmax_pred", ")", "\n", "softmax_pred", "=", "join", "(", "output_folder", ",", "fname", "+", "\".npy\"", ")", "\n", "\n", "", "results", ".", "append", "(", "export_pool", ".", "starmap_async", "(", "save_segmentation_nifti_from_softmax", ",", "\n", "(", "(", "softmax_pred", ",", "join", "(", "output_folder", ",", "fname", "+", "\".nii.gz\"", ")", ",", "\n", "properties", ",", "interpolation_order", ",", "self", ".", "regions_class_order", ",", "\n", "None", ",", "None", ",", "\n", "softmax_fname", ",", "None", ",", "force_separate_z", ",", "\n", "interpolation_order_z", ")", ",", "\n", ")", "\n", ")", "\n", ")", "\n", "\n", "", "pred_gt_tuples", ".", "append", "(", "[", "join", "(", "output_folder", ",", "fname", "+", "\".nii.gz\"", ")", ",", "\n", "join", "(", "self", ".", "gt_niftis_folder", ",", "fname", "+", "\".nii.gz\"", ")", "]", ")", "\n", "\n", "", "_", "=", "[", "i", ".", "get", "(", ")", "for", "i", "in", "results", "]", "\n", "self", ".", "print_to_log_file", "(", "\"finished prediction\"", ")", "\n", "\n", "# evaluate raw predictions", "\n", "self", ".", "print_to_log_file", "(", "\"evaluation of raw predictions\"", ")", "\n", "task", "=", "self", ".", "dataset_directory", ".", "split", "(", "\"/\"", ")", "[", "-", "1", "]", "\n", "job_name", "=", "self", ".", "experiment_name", "\n", "_", "=", "aggregate_scores", "(", "pred_gt_tuples", ",", "labels", "=", "list", "(", "range", "(", "self", ".", "num_classes", ")", ")", ",", "\n", "json_output_file", "=", "join", "(", "output_folder", ",", "\"summary.json\"", ")", ",", "\n", "json_name", "=", "job_name", "+", "\" val tiled %s\"", "%", "(", "str", "(", "use_sliding_window", ")", ")", ",", "\n", "json_author", "=", "\"Fabian\"", ",", "\n", "json_task", "=", "task", ",", "num_threads", "=", "default_num_threads", ")", "\n", "\n", "# in the old nnunet we would stop here. Now we add a postprocessing. This postprocessing can remove everything", "\n", "# except the largest connected component for each class. To see if this improves results, we do this for all", "\n", "# classes and then rerun the evaluation. Those classes for which this resulted in an improved dice score will", "\n", "# have this applied during inference as well", "\n", "self", ".", "print_to_log_file", "(", "\"determining postprocessing\"", ")", "\n", "determine_postprocessing", "(", "self", ".", "output_folder", ",", "self", ".", "gt_niftis_folder", ",", "validation_folder_name", ",", "\n", "final_subf_name", "=", "validation_folder_name", "+", "\"_postprocessed\"", ",", "debug", "=", "debug", ")", "\n", "# after this the final predictions for the vlaidation set can be found in validation_folder_name_base + \"_postprocessed\"", "\n", "# They are always in that folder, even if no postprocessing as applied!", "\n", "\n", "# detemining postprocesing on a per-fold basis may be OK for this fold but what if another fold finds another", "\n", "# postprocesing to be better? In this case we need to consolidate. At the time the consolidation is going to be", "\n", "# done we won't know what self.gt_niftis_folder was, so now we copy all the niftis into a separate folder to", "\n", "# be used later", "\n", "gt_nifti_folder", "=", "join", "(", "self", ".", "output_folder_base", ",", "\"gt_niftis\"", ")", "\n", "maybe_mkdir_p", "(", "gt_nifti_folder", ")", "\n", "for", "f", "in", "subfiles", "(", "self", ".", "gt_niftis_folder", ",", "suffix", "=", "\".nii.gz\"", ")", ":", "\n", "            ", "success", "=", "False", "\n", "attempts", "=", "0", "\n", "e", "=", "None", "\n", "while", "not", "success", "and", "attempts", "<", "10", ":", "\n", "                ", "try", ":", "\n", "                    ", "shutil", ".", "copy", "(", "f", ",", "gt_nifti_folder", ")", "\n", "success", "=", "True", "\n", "", "except", "OSError", "as", "e", ":", "\n", "                    ", "attempts", "+=", "1", "\n", "sleep", "(", "1", ")", "\n", "", "", "if", "not", "success", ":", "\n", "                ", "print", "(", "\"Could not copy gt nifti file %s into folder %s\"", "%", "(", "f", ",", "gt_nifti_folder", ")", ")", "\n", "if", "e", "is", "not", "None", ":", "\n", "                    ", "raise", "e", "\n", "\n", "", "", "", "self", ".", "network", ".", "train", "(", "current_mode", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.network_training.nnUNetTrainer.nnUNetTrainer.run_online_evaluation": [[673, 696], ["torch.no_grad", "nnunet.utilities.nd_softmax.softmax_helper", "nnunet.utilities.nd_softmax.softmax_helper.argmax", "tuple", "torch.zeros().to", "torch.zeros().to", "torch.zeros().to", "range", "tp_hard.sum().detach().cpu().numpy.sum().detach().cpu().numpy.sum().detach().cpu().numpy", "fp_hard.sum().detach().cpu().numpy.sum().detach().cpu().numpy.sum().detach().cpu().numpy", "fn_hard.sum().detach().cpu().numpy.sum().detach().cpu().numpy.sum().detach().cpu().numpy", "nnUNetTrainer.nnUNetTrainer.online_eval_foreground_dc.append", "nnUNetTrainer.nnUNetTrainer.online_eval_tp.append", "nnUNetTrainer.nnUNetTrainer.online_eval_fp.append", "nnUNetTrainer.nnUNetTrainer.online_eval_fn.append", "range", "nnunet.utilities.tensor_utilities.sum_tensor", "nnunet.utilities.tensor_utilities.sum_tensor", "nnunet.utilities.tensor_utilities.sum_tensor", "list", "list", "list", "list", "len", "torch.zeros", "torch.zeros", "torch.zeros", "tp_hard.sum().detach().cpu().numpy.sum().detach().cpu().numpy.sum().detach().cpu", "fp_hard.sum().detach().cpu().numpy.sum().detach().cpu().numpy.sum().detach().cpu", "fn_hard.sum().detach().cpu().numpy.sum().detach().cpu().numpy.sum().detach().cpu", "tp_hard.sum().detach().cpu().numpy.sum().detach().cpu().numpy.sum().detach", "fp_hard.sum().detach().cpu().numpy.sum().detach().cpu().numpy.sum().detach", "fn_hard.sum().detach().cpu().numpy.sum().detach().cpu().numpy.sum().detach", "tp_hard.sum().detach().cpu().numpy.sum().detach().cpu().numpy.sum", "fp_hard.sum().detach().cpu().numpy.sum().detach().cpu().numpy.sum", "fn_hard.sum().detach().cpu().numpy.sum().detach().cpu().numpy.sum"], "methods", ["home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.utilities.tensor_utilities.sum_tensor", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.utilities.tensor_utilities.sum_tensor", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.utilities.tensor_utilities.sum_tensor"], ["", "def", "run_online_evaluation", "(", "self", ",", "output", ",", "target", ")", ":", "\n", "        ", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "            ", "num_classes", "=", "output", ".", "shape", "[", "1", "]", "\n", "output_softmax", "=", "softmax_helper", "(", "output", ")", "\n", "output_seg", "=", "output_softmax", ".", "argmax", "(", "1", ")", "\n", "target", "=", "target", "[", ":", ",", "0", "]", "\n", "axes", "=", "tuple", "(", "range", "(", "1", ",", "len", "(", "target", ".", "shape", ")", ")", ")", "\n", "tp_hard", "=", "torch", ".", "zeros", "(", "(", "target", ".", "shape", "[", "0", "]", ",", "num_classes", "-", "1", ")", ")", ".", "to", "(", "output_seg", ".", "device", ".", "index", ")", "\n", "fp_hard", "=", "torch", ".", "zeros", "(", "(", "target", ".", "shape", "[", "0", "]", ",", "num_classes", "-", "1", ")", ")", ".", "to", "(", "output_seg", ".", "device", ".", "index", ")", "\n", "fn_hard", "=", "torch", ".", "zeros", "(", "(", "target", ".", "shape", "[", "0", "]", ",", "num_classes", "-", "1", ")", ")", ".", "to", "(", "output_seg", ".", "device", ".", "index", ")", "\n", "for", "c", "in", "range", "(", "1", ",", "num_classes", ")", ":", "\n", "                ", "tp_hard", "[", ":", ",", "c", "-", "1", "]", "=", "sum_tensor", "(", "(", "output_seg", "==", "c", ")", ".", "float", "(", ")", "*", "(", "target", "==", "c", ")", ".", "float", "(", ")", ",", "axes", "=", "axes", ")", "\n", "fp_hard", "[", ":", ",", "c", "-", "1", "]", "=", "sum_tensor", "(", "(", "output_seg", "==", "c", ")", ".", "float", "(", ")", "*", "(", "target", "!=", "c", ")", ".", "float", "(", ")", ",", "axes", "=", "axes", ")", "\n", "fn_hard", "[", ":", ",", "c", "-", "1", "]", "=", "sum_tensor", "(", "(", "output_seg", "!=", "c", ")", ".", "float", "(", ")", "*", "(", "target", "==", "c", ")", ".", "float", "(", ")", ",", "axes", "=", "axes", ")", "\n", "\n", "", "tp_hard", "=", "tp_hard", ".", "sum", "(", "0", ",", "keepdim", "=", "False", ")", ".", "detach", "(", ")", ".", "cpu", "(", ")", ".", "numpy", "(", ")", "\n", "fp_hard", "=", "fp_hard", ".", "sum", "(", "0", ",", "keepdim", "=", "False", ")", ".", "detach", "(", ")", ".", "cpu", "(", ")", ".", "numpy", "(", ")", "\n", "fn_hard", "=", "fn_hard", ".", "sum", "(", "0", ",", "keepdim", "=", "False", ")", ".", "detach", "(", ")", ".", "cpu", "(", ")", ".", "numpy", "(", ")", "\n", "\n", "self", ".", "online_eval_foreground_dc", ".", "append", "(", "list", "(", "(", "2", "*", "tp_hard", ")", "/", "(", "2", "*", "tp_hard", "+", "fp_hard", "+", "fn_hard", "+", "1e-8", ")", ")", ")", "\n", "self", ".", "online_eval_tp", ".", "append", "(", "list", "(", "tp_hard", ")", ")", "\n", "self", ".", "online_eval_fp", ".", "append", "(", "list", "(", "fp_hard", ")", ")", "\n", "self", ".", "online_eval_fn", ".", "append", "(", "list", "(", "fn_hard", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.network_training.nnUNetTrainer.nnUNetTrainer.finish_online_evaluation": [[697, 715], ["numpy.sum", "numpy.sum", "numpy.sum", "nnUNetTrainer.nnUNetTrainer.all_val_eval_metrics.append", "nnUNetTrainer.nnUNetTrainer.print_to_log_file", "nnUNetTrainer.nnUNetTrainer.print_to_log_file", "numpy.mean", "str", "numpy.isnan", "zip"], "methods", ["home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.network_training.nnUNetTrainerV2_DDP.nnUNetTrainerV2_DDP.print_to_log_file", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.network_training.nnUNetTrainerV2_DDP.nnUNetTrainerV2_DDP.print_to_log_file"], ["", "", "def", "finish_online_evaluation", "(", "self", ")", ":", "\n", "        ", "self", ".", "online_eval_tp", "=", "np", ".", "sum", "(", "self", ".", "online_eval_tp", ",", "0", ")", "\n", "self", ".", "online_eval_fp", "=", "np", ".", "sum", "(", "self", ".", "online_eval_fp", ",", "0", ")", "\n", "self", ".", "online_eval_fn", "=", "np", ".", "sum", "(", "self", ".", "online_eval_fn", ",", "0", ")", "\n", "\n", "global_dc_per_class", "=", "[", "i", "for", "i", "in", "[", "2", "*", "i", "/", "(", "2", "*", "i", "+", "j", "+", "k", ")", "for", "i", ",", "j", ",", "k", "in", "\n", "zip", "(", "self", ".", "online_eval_tp", ",", "self", ".", "online_eval_fp", ",", "self", ".", "online_eval_fn", ")", "]", "\n", "if", "not", "np", ".", "isnan", "(", "i", ")", "]", "\n", "self", ".", "all_val_eval_metrics", ".", "append", "(", "np", ".", "mean", "(", "global_dc_per_class", ")", ")", "\n", "\n", "self", ".", "print_to_log_file", "(", "\"Average global foreground Dice:\"", ",", "str", "(", "global_dc_per_class", ")", ")", "\n", "self", ".", "print_to_log_file", "(", "\"(interpret this as an estimate for the Dice of the different classes. This is not \"", "\n", "\"exact.)\"", ")", "\n", "\n", "self", ".", "online_eval_foreground_dc", "=", "[", "]", "\n", "self", ".", "online_eval_tp", "=", "[", "]", "\n", "self", ".", "online_eval_fp", "=", "[", "]", "\n", "self", ".", "online_eval_fn", "=", "[", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.network_training.nnUNetTrainer.nnUNetTrainer.save_checkpoint": [[716, 725], ["super().save_checkpoint", "collections.OrderedDict", "str", "write_pickle"], "methods", ["home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.profiling.nnUNetTrainerV2_2epochs.nnUNetTrainerV2_DDP_5epochs.save_checkpoint"], ["", "def", "save_checkpoint", "(", "self", ",", "fname", ",", "save_optimizer", "=", "True", ")", ":", "\n", "        ", "super", "(", "nnUNetTrainer", ",", "self", ")", ".", "save_checkpoint", "(", "fname", ",", "save_optimizer", ")", "\n", "info", "=", "OrderedDict", "(", ")", "\n", "info", "[", "'init'", "]", "=", "self", ".", "init_args", "\n", "info", "[", "'name'", "]", "=", "self", ".", "__class__", ".", "__name__", "\n", "info", "[", "'class'", "]", "=", "str", "(", "self", ".", "__class__", ")", "\n", "info", "[", "'plans'", "]", "=", "self", ".", "plans", "\n", "\n", "write_pickle", "(", "info", ",", "fname", "+", "\".pkl\"", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.network_training.nnUNetPlusPlusTrainerV2.nnUNetPlusPlusTrainerV2.__init__": [[44, 54], ["nnunet.training.network_training.nnUNetTrainer.nnUNetTrainer.__init__"], "methods", ["home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.common.layers.ResizeImage.__init__"], ["def", "__init__", "(", "self", ",", "plans_file", ",", "fold", ",", "output_folder", "=", "None", ",", "dataset_directory", "=", "None", ",", "batch_dice", "=", "True", ",", "stage", "=", "None", ",", "\n", "unpack_data", "=", "True", ",", "deterministic", "=", "True", ",", "fp16", "=", "False", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "plans_file", ",", "fold", ",", "output_folder", ",", "dataset_directory", ",", "batch_dice", ",", "stage", ",", "unpack_data", ",", "\n", "deterministic", ",", "fp16", ")", "\n", "self", ".", "max_num_epochs", "=", "1000", "\n", "self", ".", "initial_lr", "=", "1e-2", "\n", "self", ".", "deep_supervision_scales", "=", "None", "\n", "self", ".", "ds_loss_weights", "=", "None", "\n", "\n", "self", ".", "pin_memory", "=", "True", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.network_training.nnUNetPlusPlusTrainerV2.nnUNetPlusPlusTrainerV2.initialize": [[55, 128], ["maybe_mkdir_p", "nnUNetPlusPlusTrainerV2.nnUNetPlusPlusTrainerV2.process_plans", "nnUNetPlusPlusTrainerV2.nnUNetPlusPlusTrainerV2.setup_DA_params", "len", "numpy.array", "numpy.array", "nnunet.training.loss_functions.deep_supervision.MultipleOutputLoss2", "join", "nnUNetPlusPlusTrainerV2.nnUNetPlusPlusTrainerV2.initialize_network", "nnUNetPlusPlusTrainerV2.nnUNetPlusPlusTrainerV2.initialize_optimizer_and_scheduler", "isinstance", "nnUNetPlusPlusTrainerV2.nnUNetPlusPlusTrainerV2.print_to_log_file", "nnUNetPlusPlusTrainerV2.nnUNetPlusPlusTrainerV2.load_plans_file", "numpy.array.sum", "nnUNetPlusPlusTrainerV2.nnUNetPlusPlusTrainerV2.get_basic_generators", "nnunet.training.data_augmentation.default_data_augmentation.get_moreDA_augmentation", "nnUNetPlusPlusTrainerV2.nnUNetPlusPlusTrainerV2.print_to_log_file", "nnUNetPlusPlusTrainerV2.nnUNetPlusPlusTrainerV2.print_to_log_file", "print", "nnunet.training.dataloading.dataset_loading.unpack_dataset", "print", "print", "range", "str", "str", "range", "nnUNetPlusPlusTrainerV2.nnUNetPlusPlusTrainerV2.dataset_tr.keys", "nnUNetPlusPlusTrainerV2.nnUNetPlusPlusTrainerV2.dataset_val.keys"], "methods", ["home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.BraTS_trainer.nnUNetTrainerV2BraTSRegions.nnUNetTrainerV2BraTSRegions_DA3_BN.process_plans", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.BraTS_trainer.nnUNetTrainerV2BraTSRegions.nnUNetTrainerV2BraTSRegions_DA4_BN.setup_DA_params", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.BraTS_trainer.nnUNetTrainerV2BraTSRegions.nnUNetTrainerV2BraTSRegions_DA3.initialize_network", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.optimizer_and_lr.nnUNetTrainerV2_momentum09in2D.nnUNetTrainerV2_momentum09in2D.initialize_optimizer_and_scheduler", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.network_training.nnUNetTrainerV2_DDP.nnUNetTrainerV2_DDP.print_to_log_file", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.network_training.nnUNetTrainer.nnUNetTrainer.load_plans_file", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.nnUNet_variants.nnUNetTrainerNoDA.nnUNetTrainerNoDA.get_basic_generators", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.data_augmentation.default_data_augmentation.get_moreDA_augmentation", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.network_training.nnUNetTrainerV2_DDP.nnUNetTrainerV2_DDP.print_to_log_file", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.network_training.nnUNetTrainerV2_DDP.nnUNetTrainerV2_DDP.print_to_log_file", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.dataloading.dataset_loading.unpack_dataset"], ["", "def", "initialize", "(", "self", ",", "training", "=", "True", ",", "force_load_plans", "=", "False", ")", ":", "\n", "        ", "\"\"\"\n        - replaced get_default_augmentation with get_moreDA_augmentation\n        - enforce to only run this code once\n        - loss function wrapper for deep supervision\n\n        :param training:\n        :param force_load_plans:\n        :return:\n        \"\"\"", "\n", "if", "not", "self", ".", "was_initialized", ":", "\n", "            ", "maybe_mkdir_p", "(", "self", ".", "output_folder", ")", "\n", "\n", "if", "force_load_plans", "or", "(", "self", ".", "plans", "is", "None", ")", ":", "\n", "                ", "self", ".", "load_plans_file", "(", ")", "\n", "\n", "", "self", ".", "process_plans", "(", "self", ".", "plans", ")", "\n", "\n", "self", ".", "setup_DA_params", "(", ")", "\n", "\n", "################# Here we wrap the loss for deep supervision ############", "\n", "# we need to know the number of outputs of the network", "\n", "net_numpool", "=", "len", "(", "self", ".", "net_num_pool_op_kernel_sizes", ")", "\n", "\n", "# we give each output a weight which decreases exponentially (division by 2) as the resolution decreases", "\n", "# this gives higher resolution outputs more weight in the loss", "\n", "weights", "=", "np", ".", "array", "(", "[", "1", "/", "(", "2", "**", "i", ")", "for", "i", "in", "range", "(", "net_numpool", ")", "]", ")", "\n", "\n", "# we don't use the lowest 2 outputs. Normalize weights so that they sum to 1", "\n", "mask", "=", "np", ".", "array", "(", "[", "True", "]", "+", "[", "True", "if", "i", "<", "net_numpool", "-", "1", "else", "False", "for", "i", "in", "range", "(", "1", ",", "net_numpool", ")", "]", ")", "\n", "weights", "[", "~", "mask", "]", "=", "0", "\n", "weights", "=", "weights", "/", "weights", ".", "sum", "(", ")", "\n", "#self.ds_loss_weights = weights", "\n", "self", ".", "ds_loss_weights", "=", "None", "\n", "# now wrap the loss", "\n", "self", ".", "loss", "=", "MultipleOutputLoss2", "(", "self", ".", "loss", ",", "self", ".", "ds_loss_weights", ")", "\n", "################# END ###################", "\n", "\n", "self", ".", "folder_with_preprocessed_data", "=", "join", "(", "self", ".", "dataset_directory", ",", "self", ".", "plans", "[", "'data_identifier'", "]", "+", "\n", "\"_stage%d\"", "%", "self", ".", "stage", ")", "\n", "if", "training", ":", "\n", "                ", "self", ".", "dl_tr", ",", "self", ".", "dl_val", "=", "self", ".", "get_basic_generators", "(", ")", "\n", "if", "self", ".", "unpack_data", ":", "\n", "                    ", "print", "(", "\"unpacking dataset\"", ")", "\n", "unpack_dataset", "(", "self", ".", "folder_with_preprocessed_data", ")", "\n", "print", "(", "\"done\"", ")", "\n", "", "else", ":", "\n", "                    ", "print", "(", "\n", "\"INFO: Not unpacking data! Training may be slow due to that. Pray you are not using 2d or you \"", "\n", "\"will wait all winter for your model to finish!\"", ")", "\n", "\n", "", "self", ".", "tr_gen", ",", "self", ".", "val_gen", "=", "get_moreDA_augmentation", "(", "\n", "self", ".", "dl_tr", ",", "self", ".", "dl_val", ",", "\n", "self", ".", "data_aug_params", "[", "\n", "'patch_size_for_spatialtransform'", "]", ",", "\n", "self", ".", "data_aug_params", ",", "\n", "deep_supervision_scales", "=", "self", ".", "deep_supervision_scales", ",", "\n", "pin_memory", "=", "self", ".", "pin_memory", "\n", ")", "\n", "self", ".", "print_to_log_file", "(", "\"TRAINING KEYS:\\n %s\"", "%", "(", "str", "(", "self", ".", "dataset_tr", ".", "keys", "(", ")", ")", ")", ",", "\n", "also_print_to_console", "=", "False", ")", "\n", "self", ".", "print_to_log_file", "(", "\"VALIDATION KEYS:\\n %s\"", "%", "(", "str", "(", "self", ".", "dataset_val", ".", "keys", "(", ")", ")", ")", ",", "\n", "also_print_to_console", "=", "False", ")", "\n", "", "else", ":", "\n", "                ", "pass", "\n", "\n", "", "self", ".", "initialize_network", "(", ")", "\n", "self", ".", "initialize_optimizer_and_scheduler", "(", ")", "\n", "\n", "assert", "isinstance", "(", "self", ".", "network", ",", "(", "SegmentationNetwork", ",", "nn", ".", "DataParallel", ")", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "print_to_log_file", "(", "'self.was_initialized is True, not running self.initialize again'", ")", "\n", "", "self", ".", "was_initialized", "=", "True", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.network_training.nnUNetPlusPlusTrainerV2.nnUNetPlusPlusTrainerV2.initialize_network": [[129, 162], ["nnunet.network_architecture.generic_UNetPlusPlus.Generic_UNetPlusPlus", "torch.cuda.is_available", "len", "nnunet.network_architecture.initialization.InitWeights_He", "nnUNetPlusPlusTrainerV2.nnUNetPlusPlusTrainerV2.network.cuda"], "methods", ["None"], ["", "def", "initialize_network", "(", "self", ")", ":", "\n", "        ", "\"\"\"\n        - momentum 0.99\n        - SGD instead of Adam\n        - self.lr_scheduler = None because we do poly_lr\n        - deep supervision = True\n        - i am sure I forgot something here\n\n        Known issue: forgot to set neg_slope=0 in InitWeights_He; should not make a difference though\n        :return:\n        \"\"\"", "\n", "if", "self", ".", "threeD", ":", "\n", "            ", "conv_op", "=", "nn", ".", "Conv3d", "\n", "dropout_op", "=", "nn", ".", "Dropout3d", "\n", "norm_op", "=", "nn", ".", "InstanceNorm3d", "\n", "\n", "", "else", ":", "\n", "            ", "conv_op", "=", "nn", ".", "Conv2d", "\n", "dropout_op", "=", "nn", ".", "Dropout2d", "\n", "norm_op", "=", "nn", ".", "InstanceNorm2d", "\n", "", "norm_op_kwargs", "=", "{", "'eps'", ":", "1e-5", ",", "'affine'", ":", "True", "}", "\n", "dropout_op_kwargs", "=", "{", "'p'", ":", "0", ",", "'inplace'", ":", "True", "}", "\n", "net_nonlin", "=", "nn", ".", "LeakyReLU", "\n", "net_nonlin_kwargs", "=", "{", "'negative_slope'", ":", "1e-2", ",", "'inplace'", ":", "True", "}", "\n", "self", ".", "network", "=", "Generic_UNetPlusPlus", "(", "self", ".", "num_input_channels", ",", "self", ".", "base_num_features", ",", "self", ".", "num_classes", ",", "\n", "len", "(", "self", ".", "net_num_pool_op_kernel_sizes", ")", ",", "\n", "self", ".", "conv_per_stage", ",", "2", ",", "conv_op", ",", "norm_op", ",", "norm_op_kwargs", ",", "dropout_op", ",", "\n", "dropout_op_kwargs", ",", "\n", "net_nonlin", ",", "net_nonlin_kwargs", ",", "True", ",", "False", ",", "lambda", "x", ":", "x", ",", "InitWeights_He", "(", "1e-2", ")", ",", "\n", "self", ".", "net_num_pool_op_kernel_sizes", ",", "self", ".", "net_conv_kernel_sizes", ",", "False", ",", "True", ",", "True", ")", "\n", "if", "torch", ".", "cuda", ".", "is_available", "(", ")", ":", "\n", "            ", "self", ".", "network", ".", "cuda", "(", ")", "\n", "", "self", ".", "network", ".", "inference_apply_nonlin", "=", "softmax_helper", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.network_training.nnUNetPlusPlusTrainerV2.nnUNetPlusPlusTrainerV2.initialize_optimizer_and_scheduler": [[163, 170], ["print", "sys.stdout.flush", "torch.optim.SGD", "nnUNetPlusPlusTrainerV2.nnUNetPlusPlusTrainerV2.network.parameters"], "methods", ["None"], ["", "def", "initialize_optimizer_and_scheduler", "(", "self", ")", ":", "\n", "        ", "assert", "self", ".", "network", "is", "not", "None", ",", "\"self.initialize_network must be called first\"", "\n", "print", "(", "'weight_decay: '", ",", "self", ".", "weight_decay", ")", "\n", "sys", ".", "stdout", ".", "flush", "(", ")", "\n", "self", ".", "optimizer", "=", "torch", ".", "optim", ".", "SGD", "(", "self", ".", "network", ".", "parameters", "(", ")", ",", "self", ".", "initial_lr", ",", "weight_decay", "=", "self", ".", "weight_decay", ",", "\n", "momentum", "=", "0.99", ",", "nesterov", "=", "True", ")", "\n", "self", ".", "lr_scheduler", "=", "None", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.network_training.nnUNetPlusPlusTrainerV2.nnUNetPlusPlusTrainerV2.run_online_evaluation": [[171, 182], ["super().run_online_evaluation"], "methods", ["home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.BraTS_trainer.nnUNetTrainerV2BraTSRegions.nnUNetTrainerV2BraTSRegions_DA3_BN.run_online_evaluation"], ["", "def", "run_online_evaluation", "(", "self", ",", "output", ",", "target", ")", ":", "\n", "        ", "\"\"\"\n        due to deep supervision the return value and the reference are now lists of tensors. We only need the full\n        resolution output because this is what we are interested in in the end. The others are ignored\n        :param output:\n        :param target:\n        :return:\n        \"\"\"", "\n", "target", "=", "target", "[", "0", "]", "\n", "output", "=", "output", "[", "0", "]", "\n", "return", "super", "(", ")", ".", "run_online_evaluation", "(", "output", ",", "target", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.network_training.nnUNetPlusPlusTrainerV2.nnUNetPlusPlusTrainerV2.validate": [[183, 197], ["super().validate"], "methods", ["home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.BraTS_trainer.nnUNetTrainerV2BraTSRegions.nnUNetTrainerV2BraTSRegions_DA3_BN.validate"], ["", "def", "validate", "(", "self", ",", "do_mirroring", ":", "bool", "=", "True", ",", "use_sliding_window", ":", "bool", "=", "True", ",", "\n", "step_size", ":", "float", "=", "0.5", ",", "save_softmax", ":", "bool", "=", "True", ",", "use_gaussian", ":", "bool", "=", "True", ",", "overwrite", ":", "bool", "=", "True", ",", "\n", "validation_folder_name", ":", "str", "=", "'validation_raw'", ",", "debug", ":", "bool", "=", "False", ",", "all_in_gpu", ":", "bool", "=", "False", ",", "\n", "segmentation_export_kwargs", ":", "dict", "=", "None", ")", ":", "\n", "        ", "\"\"\"\n        We need to wrap this because we need to enforce self.network.do_ds = False for prediction\n        \"\"\"", "\n", "ds", "=", "self", ".", "network", ".", "do_ds", "\n", "self", ".", "network", ".", "do_ds", "=", "False", "\n", "ret", "=", "super", "(", ")", ".", "validate", "(", "do_mirroring", ",", "use_sliding_window", ",", "step_size", ",", "save_softmax", ",", "use_gaussian", ",", "\n", "overwrite", ",", "validation_folder_name", ",", "debug", ",", "all_in_gpu", ",", "segmentation_export_kwargs", ")", "\n", "\n", "self", ".", "network", ".", "do_ds", "=", "ds", "\n", "return", "ret", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.network_training.nnUNetPlusPlusTrainerV2.nnUNetPlusPlusTrainerV2.predict_preprocessed_data_return_seg_and_softmax": [[198, 215], ["super().predict_preprocessed_data_return_seg_and_softmax"], "methods", ["home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.architectural_variants.nnUNetTrainerV2_ResencUNet.nnUNetTrainerV2_ResencUNet.predict_preprocessed_data_return_seg_and_softmax"], ["", "def", "predict_preprocessed_data_return_seg_and_softmax", "(", "self", ",", "data", ":", "np", ".", "ndarray", ",", "do_mirroring", ":", "bool", "=", "True", ",", "\n", "mirror_axes", ":", "Tuple", "[", "int", "]", "=", "None", ",", "\n", "use_sliding_window", ":", "bool", "=", "True", ",", "step_size", ":", "float", "=", "0.5", ",", "\n", "use_gaussian", ":", "bool", "=", "True", ",", "pad_border_mode", ":", "str", "=", "'constant'", ",", "\n", "pad_kwargs", ":", "dict", "=", "None", ",", "all_in_gpu", ":", "bool", "=", "True", ",", "\n", "verbose", ":", "bool", "=", "True", ",", "mixed_precision", "=", "True", ")", "->", "Tuple", "[", "np", ".", "ndarray", ",", "np", ".", "ndarray", "]", ":", "\n", "        ", "\"\"\"\n        We need to wrap this because we need to enforce self.network.do_ds = False for prediction\n        \"\"\"", "\n", "ds", "=", "self", ".", "network", ".", "do_ds", "\n", "self", ".", "network", ".", "do_ds", "=", "False", "\n", "ret", "=", "super", "(", ")", ".", "predict_preprocessed_data_return_seg_and_softmax", "(", "data", ",", "do_mirroring", ",", "mirror_axes", ",", "\n", "use_sliding_window", ",", "step_size", ",", "use_gaussian", ",", "\n", "pad_border_mode", ",", "pad_kwargs", ",", "all_in_gpu", ",", "verbose", ",", "\n", "mixed_precision", "=", "mixed_precision", ")", "\n", "self", ".", "network", ".", "do_ds", "=", "ds", "\n", "return", "ret", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.network_training.nnUNetPlusPlusTrainerV2.nnUNetPlusPlusTrainerV2.run_iteration": [[216, 266], ["next", "nnunet.utilities.to_torch.maybe_to_torch", "nnunet.utilities.to_torch.maybe_to_torch", "torch.cuda.is_available", "nnUNetPlusPlusTrainerV2.nnUNetPlusPlusTrainerV2.optimizer.zero_grad", "nnUNetPlusPlusTrainerV2.nnUNetPlusPlusTrainerV2.detach().cpu().numpy", "nnunet.utilities.to_torch.to_cuda", "nnunet.utilities.to_torch.to_cuda", "nnUNetPlusPlusTrainerV2.nnUNetPlusPlusTrainerV2.network", "nnUNetPlusPlusTrainerV2.nnUNetPlusPlusTrainerV2.loss", "nnUNetPlusPlusTrainerV2.nnUNetPlusPlusTrainerV2.run_online_evaluation", "torch.cuda.amp.autocast", "nnUNetPlusPlusTrainerV2.nnUNetPlusPlusTrainerV2.network", "nnUNetPlusPlusTrainerV2.nnUNetPlusPlusTrainerV2.loss", "nnUNetPlusPlusTrainerV2.nnUNetPlusPlusTrainerV2.amp_grad_scaler.scale().backward", "nnUNetPlusPlusTrainerV2.nnUNetPlusPlusTrainerV2.amp_grad_scaler.unscale_", "torch.nn.utils.clip_grad_norm_", "nnUNetPlusPlusTrainerV2.nnUNetPlusPlusTrainerV2.amp_grad_scaler.step", "nnUNetPlusPlusTrainerV2.nnUNetPlusPlusTrainerV2.amp_grad_scaler.update", "nnUNetPlusPlusTrainerV2.nnUNetPlusPlusTrainerV2.backward", "torch.nn.utils.clip_grad_norm_", "nnUNetPlusPlusTrainerV2.nnUNetPlusPlusTrainerV2.optimizer.step", "nnUNetPlusPlusTrainerV2.nnUNetPlusPlusTrainerV2.detach().cpu", "nnUNetPlusPlusTrainerV2.nnUNetPlusPlusTrainerV2.network.parameters", "nnUNetPlusPlusTrainerV2.nnUNetPlusPlusTrainerV2.network.parameters", "nnUNetPlusPlusTrainerV2.nnUNetPlusPlusTrainerV2.amp_grad_scaler.scale", "nnUNetPlusPlusTrainerV2.nnUNetPlusPlusTrainerV2.detach"], "methods", ["home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.utilities.to_torch.maybe_to_torch", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.utilities.to_torch.maybe_to_torch", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.utilities.to_torch.to_cuda", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.utilities.to_torch.to_cuda", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.BraTS_trainer.nnUNetTrainerV2BraTSRegions.nnUNetTrainerV2BraTSRegions_DA3_BN.run_online_evaluation", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.utilities.distributed.awesome_allgather_function.backward", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.optimizer.ranger.Ranger.step", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.utilities.distributed.awesome_allgather_function.backward", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.optimizer.ranger.Ranger.step"], ["", "def", "run_iteration", "(", "self", ",", "data_generator", ",", "do_backprop", "=", "True", ",", "run_online_evaluation", "=", "False", ")", ":", "\n", "        ", "\"\"\"\n        gradient clipping improves training stability\n\n        :param data_generator:\n        :param do_backprop:\n        :param run_online_evaluation:\n        :return:\n        \"\"\"", "\n", "data_dict", "=", "next", "(", "data_generator", ")", "\n", "data", "=", "data_dict", "[", "'data'", "]", "\n", "target", "=", "data_dict", "[", "'target'", "]", "\n", "\n", "data", "=", "maybe_to_torch", "(", "data", ")", "\n", "target", "=", "maybe_to_torch", "(", "target", ")", "\n", "\n", "if", "torch", ".", "cuda", ".", "is_available", "(", ")", ":", "\n", "            ", "data", "=", "to_cuda", "(", "data", ")", "\n", "target", "=", "to_cuda", "(", "target", ")", "\n", "\n", "", "self", ".", "optimizer", ".", "zero_grad", "(", ")", "\n", "\n", "if", "self", ".", "fp16", ":", "\n", "            ", "with", "autocast", "(", ")", ":", "\n", "                ", "output", "=", "self", ".", "network", "(", "data", ")", "\n", "del", "data", "\n", "l", "=", "self", ".", "loss", "(", "output", ",", "target", ")", "\n", "\n", "", "if", "do_backprop", ":", "\n", "                ", "self", ".", "amp_grad_scaler", ".", "scale", "(", "l", ")", ".", "backward", "(", ")", "\n", "self", ".", "amp_grad_scaler", ".", "unscale_", "(", "self", ".", "optimizer", ")", "\n", "torch", ".", "nn", ".", "utils", ".", "clip_grad_norm_", "(", "self", ".", "network", ".", "parameters", "(", ")", ",", "12", ")", "\n", "self", ".", "amp_grad_scaler", ".", "step", "(", "self", ".", "optimizer", ")", "\n", "self", ".", "amp_grad_scaler", ".", "update", "(", ")", "\n", "", "", "else", ":", "\n", "            ", "output", "=", "self", ".", "network", "(", "data", ")", "\n", "del", "data", "\n", "l", "=", "self", ".", "loss", "(", "output", ",", "target", ")", "\n", "\n", "if", "do_backprop", ":", "\n", "                ", "l", ".", "backward", "(", ")", "\n", "torch", ".", "nn", ".", "utils", ".", "clip_grad_norm_", "(", "self", ".", "network", ".", "parameters", "(", ")", ",", "12", ")", "\n", "self", ".", "optimizer", ".", "step", "(", ")", "\n", "\n", "", "", "if", "run_online_evaluation", ":", "\n", "            ", "self", ".", "run_online_evaluation", "(", "output", ",", "target", ")", "\n", "\n", "", "del", "target", "\n", "\n", "return", "l", ".", "detach", "(", ")", ".", "cpu", "(", ")", ".", "numpy", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.network_training.nnUNetPlusPlusTrainerV2.nnUNetPlusPlusTrainerV2.do_split": [[267, 318], ["list.sort", "list.sort", "collections.OrderedDict", "collections.OrderedDict", "list", "join", "load_pickle", "nnUNetPlusPlusTrainerV2.nnUNetPlusPlusTrainerV2.dataset.keys", "isfile", "nnUNetPlusPlusTrainerV2.nnUNetPlusPlusTrainerV2.print_to_log_file", "numpy.sort", "sklearn.model_selection.KFold", "enumerate", "save_pickle", "len", "nnUNetPlusPlusTrainerV2.nnUNetPlusPlusTrainerV2.print_to_log_file", "numpy.random.RandomState", "numpy.sort", "numpy.random.RandomState.choice", "list", "sklearn.model_selection.KFold.split", "load_pickle.append", "list", "len", "int", "nnUNetPlusPlusTrainerV2.nnUNetPlusPlusTrainerV2.dataset.keys", "numpy.array", "numpy.array", "collections.OrderedDict", "nnUNetPlusPlusTrainerV2.nnUNetPlusPlusTrainerV2.dataset.keys", "range", "len", "len", "len"], "methods", ["home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.network_training.nnUNetTrainerV2_DDP.nnUNetTrainerV2_DDP.print_to_log_file", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.network_training.nnUNetTrainerV2_DDP.nnUNetTrainerV2_DDP.print_to_log_file"], ["", "def", "do_split", "(", "self", ")", ":", "\n", "        ", "\"\"\"\n        we now allow more than 5 splits. IMPORTANT: and fold > 4 will not be a real split but just another random\n        80:20 split of the data. You cannot run X-fold cross-validation with this code. It will always be a 5-fold CV.\n        Folds > 4 will be independent from each other\n        :return:\n        \"\"\"", "\n", "if", "self", ".", "fold", "==", "\"all\"", ":", "\n", "# if fold==all then we use all images for training and validation", "\n", "            ", "tr_keys", "=", "val_keys", "=", "list", "(", "self", ".", "dataset", ".", "keys", "(", ")", ")", "\n", "", "else", ":", "\n", "            ", "splits_file", "=", "join", "(", "self", ".", "dataset_directory", ",", "\"splits_final.pkl\"", ")", "\n", "\n", "# if the split file does not exist we need to create it", "\n", "if", "not", "isfile", "(", "splits_file", ")", ":", "\n", "                ", "self", ".", "print_to_log_file", "(", "\"Creating new split...\"", ")", "\n", "splits", "=", "[", "]", "\n", "all_keys_sorted", "=", "np", ".", "sort", "(", "list", "(", "self", ".", "dataset", ".", "keys", "(", ")", ")", ")", "\n", "kfold", "=", "KFold", "(", "n_splits", "=", "5", ",", "shuffle", "=", "True", ",", "random_state", "=", "12345", ")", "\n", "for", "i", ",", "(", "train_idx", ",", "test_idx", ")", "in", "enumerate", "(", "kfold", ".", "split", "(", "all_keys_sorted", ")", ")", ":", "\n", "                    ", "train_keys", "=", "np", ".", "array", "(", "all_keys_sorted", ")", "[", "train_idx", "]", "\n", "test_keys", "=", "np", ".", "array", "(", "all_keys_sorted", ")", "[", "test_idx", "]", "\n", "splits", ".", "append", "(", "OrderedDict", "(", ")", ")", "\n", "splits", "[", "-", "1", "]", "[", "'train'", "]", "=", "train_keys", "\n", "splits", "[", "-", "1", "]", "[", "'val'", "]", "=", "test_keys", "\n", "", "save_pickle", "(", "splits", ",", "splits_file", ")", "\n", "\n", "", "splits", "=", "load_pickle", "(", "splits_file", ")", "\n", "\n", "if", "self", ".", "fold", "<", "len", "(", "splits", ")", ":", "\n", "                ", "tr_keys", "=", "splits", "[", "self", ".", "fold", "]", "[", "'train'", "]", "\n", "val_keys", "=", "splits", "[", "self", ".", "fold", "]", "[", "'val'", "]", "\n", "", "else", ":", "\n", "                ", "self", ".", "print_to_log_file", "(", "\"INFO: Requested fold %d but split file only has %d folds. I am now creating a \"", "\n", "\"random 80:20 split!\"", "%", "(", "self", ".", "fold", ",", "len", "(", "splits", ")", ")", ")", "\n", "# if we request a fold that is not in the split file, create a random 80:20 split", "\n", "rnd", "=", "np", ".", "random", ".", "RandomState", "(", "seed", "=", "12345", "+", "self", ".", "fold", ")", "\n", "keys", "=", "np", ".", "sort", "(", "list", "(", "self", ".", "dataset", ".", "keys", "(", ")", ")", ")", "\n", "idx_tr", "=", "rnd", ".", "choice", "(", "len", "(", "keys", ")", ",", "int", "(", "len", "(", "keys", ")", "*", "0.8", ")", ",", "replace", "=", "False", ")", "\n", "idx_val", "=", "[", "i", "for", "i", "in", "range", "(", "len", "(", "keys", ")", ")", "if", "i", "not", "in", "idx_tr", "]", "\n", "tr_keys", "=", "[", "keys", "[", "i", "]", "for", "i", "in", "idx_tr", "]", "\n", "val_keys", "=", "[", "keys", "[", "i", "]", "for", "i", "in", "idx_val", "]", "\n", "\n", "", "", "tr_keys", ".", "sort", "(", ")", "\n", "val_keys", ".", "sort", "(", ")", "\n", "self", ".", "dataset_tr", "=", "OrderedDict", "(", ")", "\n", "for", "i", "in", "tr_keys", ":", "\n", "            ", "self", ".", "dataset_tr", "[", "i", "]", "=", "self", ".", "dataset", "[", "i", "]", "\n", "", "self", ".", "dataset_val", "=", "OrderedDict", "(", ")", "\n", "for", "i", "in", "val_keys", ":", "\n", "            ", "self", ".", "dataset_val", "[", "i", "]", "=", "self", ".", "dataset", "[", "i", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.network_training.nnUNetPlusPlusTrainerV2.nnUNetPlusPlusTrainerV2.setup_DA_params": [[319, 372], ["nnunet.training.data_augmentation.default_data_augmentation.get_patch_size", "numpy.array", "nnunet.training.data_augmentation.default_data_augmentation.get_patch_size", "list", "nnUNetPlusPlusTrainerV2.nnUNetPlusPlusTrainerV2.print_to_log_file", "max", "min", "list", "list", "numpy.cumprod", "numpy.vstack"], "methods", ["home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.data_augmentation.default_data_augmentation.get_patch_size", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.data_augmentation.default_data_augmentation.get_patch_size", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.network_training.nnUNetTrainerV2_DDP.nnUNetTrainerV2_DDP.print_to_log_file"], ["", "", "def", "setup_DA_params", "(", "self", ")", ":", "\n", "        ", "\"\"\"\n        - we increase roation angle from [-15, 15] to [-30, 30]\n        - scale range is now (0.7, 1.4), was (0.85, 1.25)\n        - we don't do elastic deformation anymore\n\n        :return:\n        \"\"\"", "\n", "\n", "self", ".", "deep_supervision_scales", "=", "[", "[", "1", ",", "1", ",", "1", "]", "]", "+", "list", "(", "list", "(", "i", ")", "for", "i", "in", "1", "/", "np", ".", "cumprod", "(", "\n", "np", ".", "vstack", "(", "self", ".", "net_num_pool_op_kernel_sizes", ")", ",", "axis", "=", "0", ")", ")", "[", ":", "-", "1", "]", "\n", "\n", "if", "self", ".", "threeD", ":", "\n", "            ", "self", ".", "data_aug_params", "=", "default_3D_augmentation_params", "\n", "self", ".", "data_aug_params", "[", "'rotation_x'", "]", "=", "(", "-", "30.", "/", "360", "*", "2.", "*", "np", ".", "pi", ",", "30.", "/", "360", "*", "2.", "*", "np", ".", "pi", ")", "\n", "self", ".", "data_aug_params", "[", "'rotation_y'", "]", "=", "(", "-", "30.", "/", "360", "*", "2.", "*", "np", ".", "pi", ",", "30.", "/", "360", "*", "2.", "*", "np", ".", "pi", ")", "\n", "self", ".", "data_aug_params", "[", "'rotation_z'", "]", "=", "(", "-", "30.", "/", "360", "*", "2.", "*", "np", ".", "pi", ",", "30.", "/", "360", "*", "2.", "*", "np", ".", "pi", ")", "\n", "if", "self", ".", "do_dummy_2D_aug", ":", "\n", "                ", "self", ".", "data_aug_params", "[", "\"dummy_2D\"", "]", "=", "True", "\n", "self", ".", "print_to_log_file", "(", "\"Using dummy2d data augmentation\"", ")", "\n", "self", ".", "data_aug_params", "[", "\"elastic_deform_alpha\"", "]", "=", "default_2D_augmentation_params", "[", "\"elastic_deform_alpha\"", "]", "\n", "self", ".", "data_aug_params", "[", "\"elastic_deform_sigma\"", "]", "=", "default_2D_augmentation_params", "[", "\"elastic_deform_sigma\"", "]", "\n", "self", ".", "data_aug_params", "[", "\"rotation_x\"", "]", "=", "default_2D_augmentation_params", "[", "\"rotation_x\"", "]", "\n", "", "", "else", ":", "\n", "            ", "self", ".", "do_dummy_2D_aug", "=", "False", "\n", "if", "max", "(", "self", ".", "patch_size", ")", "/", "min", "(", "self", ".", "patch_size", ")", ">", "1.5", ":", "\n", "                ", "default_2D_augmentation_params", "[", "'rotation_x'", "]", "=", "(", "-", "15.", "/", "360", "*", "2.", "*", "np", ".", "pi", ",", "15.", "/", "360", "*", "2.", "*", "np", ".", "pi", ")", "\n", "", "self", ".", "data_aug_params", "=", "default_2D_augmentation_params", "\n", "", "self", ".", "data_aug_params", "[", "\"mask_was_used_for_normalization\"", "]", "=", "self", ".", "use_mask_for_norm", "\n", "\n", "if", "self", ".", "do_dummy_2D_aug", ":", "\n", "            ", "self", ".", "basic_generator_patch_size", "=", "get_patch_size", "(", "self", ".", "patch_size", "[", "1", ":", "]", ",", "\n", "self", ".", "data_aug_params", "[", "'rotation_x'", "]", ",", "\n", "self", ".", "data_aug_params", "[", "'rotation_y'", "]", ",", "\n", "self", ".", "data_aug_params", "[", "'rotation_z'", "]", ",", "\n", "self", ".", "data_aug_params", "[", "'scale_range'", "]", ")", "\n", "self", ".", "basic_generator_patch_size", "=", "np", ".", "array", "(", "[", "self", ".", "patch_size", "[", "0", "]", "]", "+", "list", "(", "self", ".", "basic_generator_patch_size", ")", ")", "\n", "patch_size_for_spatialtransform", "=", "self", ".", "patch_size", "[", "1", ":", "]", "\n", "", "else", ":", "\n", "            ", "self", ".", "basic_generator_patch_size", "=", "get_patch_size", "(", "self", ".", "patch_size", ",", "self", ".", "data_aug_params", "[", "'rotation_x'", "]", ",", "\n", "self", ".", "data_aug_params", "[", "'rotation_y'", "]", ",", "\n", "self", ".", "data_aug_params", "[", "'rotation_z'", "]", ",", "\n", "self", ".", "data_aug_params", "[", "'scale_range'", "]", ")", "\n", "patch_size_for_spatialtransform", "=", "self", ".", "patch_size", "\n", "\n", "", "self", ".", "data_aug_params", "[", "\"scale_range\"", "]", "=", "(", "0.7", ",", "1.4", ")", "\n", "self", ".", "data_aug_params", "[", "\"do_elastic\"", "]", "=", "False", "\n", "self", ".", "data_aug_params", "[", "'selected_seg_channels'", "]", "=", "[", "0", "]", "\n", "self", ".", "data_aug_params", "[", "'patch_size_for_spatialtransform'", "]", "=", "patch_size_for_spatialtransform", "\n", "\n", "self", ".", "data_aug_params", "[", "\"num_cached_per_thread\"", "]", "=", "2", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.network_training.nnUNetPlusPlusTrainerV2.nnUNetPlusPlusTrainerV2.maybe_update_lr": [[373, 389], ["nnunet.training.learning_rate.poly_lr.poly_lr", "nnUNetPlusPlusTrainerV2.nnUNetPlusPlusTrainerV2.print_to_log_file", "numpy.round"], "methods", ["home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.learning_rate.poly_lr.poly_lr", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.network_training.nnUNetTrainerV2_DDP.nnUNetTrainerV2_DDP.print_to_log_file"], ["", "def", "maybe_update_lr", "(", "self", ",", "epoch", "=", "None", ")", ":", "\n", "        ", "\"\"\"\n        if epoch is not None we overwrite epoch. Else we use epoch = self.epoch + 1\n\n        (maybe_update_lr is called in on_epoch_end which is called before epoch is incremented.\n        herefore we need to do +1 here)\n\n        :param epoch:\n        :return:\n        \"\"\"", "\n", "if", "epoch", "is", "None", ":", "\n", "            ", "ep", "=", "self", ".", "epoch", "+", "1", "\n", "", "else", ":", "\n", "            ", "ep", "=", "epoch", "\n", "", "self", ".", "optimizer", ".", "param_groups", "[", "0", "]", "[", "'lr'", "]", "=", "poly_lr", "(", "ep", ",", "self", ".", "max_num_epochs", ",", "self", ".", "initial_lr", ",", "0.9", ")", "\n", "self", ".", "print_to_log_file", "(", "\"lr:\"", ",", "np", ".", "round", "(", "self", ".", "optimizer", ".", "param_groups", "[", "0", "]", "[", "'lr'", "]", ",", "decimals", "=", "6", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.network_training.nnUNetPlusPlusTrainerV2.nnUNetPlusPlusTrainerV2.on_epoch_end": [[390, 409], ["super().on_epoch_end", "nnUNetPlusPlusTrainerV2.nnUNetPlusPlusTrainerV2.network.apply", "nnUNetPlusPlusTrainerV2.nnUNetPlusPlusTrainerV2.print_to_log_file", "nnunet.network_architecture.initialization.InitWeights_He"], "methods", ["home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.optimizer_and_lr.nnUNetTrainerV2_reduceMomentumDuringTraining.nnUNetTrainerV2_reduceMomentumDuringTraining.on_epoch_end", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.network_training.nnUNetTrainerV2_DDP.nnUNetTrainerV2_DDP.print_to_log_file"], ["", "def", "on_epoch_end", "(", "self", ")", ":", "\n", "        ", "\"\"\"\n        overwrite patient-based early stopping. Always run to 1000 epochs\n        :return:\n        \"\"\"", "\n", "super", "(", ")", ".", "on_epoch_end", "(", ")", "\n", "continue_training", "=", "self", ".", "epoch", "<", "self", ".", "max_num_epochs", "\n", "\n", "# it can rarely happen that the momentum of nnUNetTrainerV2_plus is too high for some dataset. If at epoch 100 the", "\n", "# estimated validation Dice is still 0 then we reduce the momentum from 0.99 to 0.95", "\n", "if", "self", ".", "epoch", "==", "100", ":", "\n", "            ", "if", "self", ".", "all_val_eval_metrics", "[", "-", "1", "]", "==", "0", ":", "\n", "                ", "self", ".", "optimizer", ".", "param_groups", "[", "0", "]", "[", "\"momentum\"", "]", "=", "0.95", "\n", "self", ".", "network", ".", "apply", "(", "InitWeights_He", "(", "1e-2", ")", ")", "\n", "self", ".", "print_to_log_file", "(", "\"At epoch 100, the mean foreground Dice was 0. This can be caused by a too \"", "\n", "\"high momentum. High momentum (0.99) is good for datasets where it works, but \"", "\n", "\"sometimes causes issues such as this one. Momentum has now been reduced to \"", "\n", "\"0.95 and network weights have been reinitialized\"", ")", "\n", "", "", "return", "continue_training", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.network_training.nnUNetPlusPlusTrainerV2.nnUNetPlusPlusTrainerV2.run_training": [[410, 425], ["nnUNetPlusPlusTrainerV2.nnUNetPlusPlusTrainerV2.maybe_update_lr", "super().run_training"], "methods", ["home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.optimizer_and_lr.nnUNetTrainerV2_warmup.nnUNetTrainerV2_warmup.maybe_update_lr", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.architectural_variants.nnUNetTrainerV2_ResencUNet.nnUNetTrainerV2_ResencUNet.run_training"], ["", "def", "run_training", "(", "self", ")", ":", "\n", "        ", "\"\"\"\n        if we run with -c then we need to set the correct lr for the first epoch, otherwise it will run the first\n        continued epoch with self.initial_lr\n\n        we also need to make sure deep supervision in the network is enabled for training, thus the wrapper\n        :return:\n        \"\"\"", "\n", "self", ".", "maybe_update_lr", "(", "self", ".", "epoch", ")", "# if we dont overwrite epoch then self.epoch+1 is used which is not what we", "\n", "# want at the start of the training", "\n", "ds", "=", "self", ".", "network", ".", "do_ds", "\n", "self", ".", "network", ".", "do_ds", "=", "True", "\n", "ret", "=", "super", "(", ")", ".", "run_training", "(", ")", "\n", "self", ".", "network", ".", "do_ds", "=", "ds", "\n", "return", "ret", "\n", "", "", ""]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.network_training.nnUNetTrainerCascadeFullRes.nnUNetTrainerCascadeFullRes.__init__": [[37, 59], ["nnunet.training.network_training.nnUNetTrainer.nnUNetTrainer.__init__", "join", "nnUNetTrainerCascadeFullRes.nnUNetTrainerCascadeFullRes.output_folder.split", "[].split", "isdir", "RuntimeError", "nnUNetTrainerCascadeFullRes.nnUNetTrainerCascadeFullRes.output_folder.split"], "methods", ["home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.common.layers.ResizeImage.__init__"], ["    ", "def", "__init__", "(", "self", ",", "plans_file", ",", "fold", ",", "output_folder", "=", "None", ",", "dataset_directory", "=", "None", ",", "batch_dice", "=", "True", ",", "stage", "=", "None", ",", "\n", "unpack_data", "=", "True", ",", "deterministic", "=", "True", ",", "previous_trainer", "=", "\"nnUNetTrainer\"", ",", "fp16", "=", "False", ")", ":", "\n", "        ", "super", "(", "nnUNetTrainerCascadeFullRes", ",", "self", ")", ".", "__init__", "(", "plans_file", ",", "fold", ",", "output_folder", ",", "dataset_directory", ",", "\n", "batch_dice", ",", "stage", ",", "unpack_data", ",", "deterministic", ",", "fp16", ")", "\n", "self", ".", "init_args", "=", "(", "plans_file", ",", "fold", ",", "output_folder", ",", "dataset_directory", ",", "batch_dice", ",", "stage", ",", "unpack_data", ",", "\n", "deterministic", ",", "previous_trainer", ",", "fp16", ")", "\n", "\n", "if", "self", ".", "output_folder", "is", "not", "None", ":", "\n", "            ", "task", "=", "self", ".", "output_folder", ".", "split", "(", "\"/\"", ")", "[", "-", "3", "]", "\n", "plans_identifier", "=", "self", ".", "output_folder", ".", "split", "(", "\"/\"", ")", "[", "-", "2", "]", ".", "split", "(", "\"__\"", ")", "[", "-", "1", "]", "\n", "\n", "folder_with_segs_prev_stage", "=", "join", "(", "network_training_output_dir", ",", "\"3d_lowres\"", ",", "\n", "task", ",", "previous_trainer", "+", "\"__\"", "+", "plans_identifier", ",", "\"pred_next_stage\"", ")", "\n", "if", "not", "isdir", "(", "folder_with_segs_prev_stage", ")", ":", "\n", "                ", "raise", "RuntimeError", "(", "\n", "\"Cannot run final stage of cascade. Run corresponding 3d_lowres first and predict the \"", "\n", "\"segmentations for the next stage\"", ")", "\n", "", "self", ".", "folder_with_segs_from_prev_stage", "=", "folder_with_segs_prev_stage", "\n", "# Do not put segs_prev_stage into self.output_folder as we need to unpack them for performance and we", "\n", "# don't want to do that in self.output_folder because that one is located on some network drive.", "\n", "", "else", ":", "\n", "            ", "self", ".", "folder_with_segs_from_prev_stage", "=", "None", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.network_training.nnUNetTrainerCascadeFullRes.nnUNetTrainerCascadeFullRes.do_split": [[60, 73], ["super().do_split", "join", "isfile", "join", "join"], "methods", ["home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.network_training.nnUNetTrainerV2.nnUNetTrainerV2.do_split"], ["", "", "def", "do_split", "(", "self", ")", ":", "\n", "        ", "super", "(", "nnUNetTrainerCascadeFullRes", ",", "self", ")", ".", "do_split", "(", ")", "\n", "for", "k", "in", "self", ".", "dataset", ":", "\n", "            ", "self", ".", "dataset", "[", "k", "]", "[", "'seg_from_prev_stage_file'", "]", "=", "join", "(", "self", ".", "folder_with_segs_from_prev_stage", ",", "\n", "k", "+", "\"_segFromPrevStage.npz\"", ")", "\n", "assert", "isfile", "(", "self", ".", "dataset", "[", "k", "]", "[", "'seg_from_prev_stage_file'", "]", ")", ",", "\"seg from prev stage missing: %s\"", "%", "(", "self", ".", "dataset", "[", "k", "]", "[", "'seg_from_prev_stage_file'", "]", ")", "\n", "", "for", "k", "in", "self", ".", "dataset_val", ":", "\n", "            ", "self", ".", "dataset_val", "[", "k", "]", "[", "'seg_from_prev_stage_file'", "]", "=", "join", "(", "self", ".", "folder_with_segs_from_prev_stage", ",", "\n", "k", "+", "\"_segFromPrevStage.npz\"", ")", "\n", "", "for", "k", "in", "self", ".", "dataset_tr", ":", "\n", "            ", "self", ".", "dataset_tr", "[", "k", "]", "[", "'seg_from_prev_stage_file'", "]", "=", "join", "(", "self", ".", "folder_with_segs_from_prev_stage", ",", "\n", "k", "+", "\"_segFromPrevStage.npz\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.network_training.nnUNetTrainerCascadeFullRes.nnUNetTrainerCascadeFullRes.get_basic_generators": [[74, 85], ["nnUNetTrainerCascadeFullRes.nnUNetTrainerCascadeFullRes.load_dataset", "nnUNetTrainerCascadeFullRes.nnUNetTrainerCascadeFullRes.do_split", "nnunet.training.dataloading.dataset_loading.DataLoader3D", "nnunet.training.dataloading.dataset_loading.DataLoader3D"], "methods", ["home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.dataloading.dataset_loading.load_dataset", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.network_training.nnUNetTrainerV2.nnUNetTrainerV2.do_split"], ["", "", "def", "get_basic_generators", "(", "self", ")", ":", "\n", "        ", "self", ".", "load_dataset", "(", ")", "\n", "self", ".", "do_split", "(", ")", "\n", "if", "self", ".", "threeD", ":", "\n", "            ", "dl_tr", "=", "DataLoader3D", "(", "self", ".", "dataset_tr", ",", "self", ".", "basic_generator_patch_size", ",", "self", ".", "patch_size", ",", "self", ".", "batch_size", ",", "\n", "True", ",", "oversample_foreground_percent", "=", "self", ".", "oversample_foreground_percent", ")", "\n", "dl_val", "=", "DataLoader3D", "(", "self", ".", "dataset_val", ",", "self", ".", "patch_size", ",", "self", ".", "patch_size", ",", "self", ".", "batch_size", ",", "True", ",", "\n", "oversample_foreground_percent", "=", "self", ".", "oversample_foreground_percent", ")", "\n", "", "else", ":", "\n", "            ", "raise", "NotImplementedError", "\n", "", "return", "dl_tr", ",", "dl_val", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.network_training.nnUNetTrainerCascadeFullRes.nnUNetTrainerCascadeFullRes.process_plans": [[86, 89], ["super().process_plans"], "methods", ["home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.BraTS_trainer.nnUNetTrainerV2BraTSRegions.nnUNetTrainerV2BraTSRegions_DA3_BN.process_plans"], ["", "def", "process_plans", "(", "self", ",", "plans", ")", ":", "\n", "        ", "super", "(", "nnUNetTrainerCascadeFullRes", ",", "self", ")", ".", "process_plans", "(", "plans", ")", "\n", "self", ".", "num_input_channels", "+=", "(", "self", ".", "num_classes", "-", "1", ")", "# for seg from prev stage", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.network_training.nnUNetTrainerCascadeFullRes.nnUNetTrainerCascadeFullRes.setup_DA_params": [[90, 108], ["super().setup_DA_params", "list", "range"], "methods", ["home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.BraTS_trainer.nnUNetTrainerV2BraTSRegions.nnUNetTrainerV2BraTSRegions_DA4_BN.setup_DA_params"], ["", "def", "setup_DA_params", "(", "self", ")", ":", "\n", "        ", "super", "(", ")", ".", "setup_DA_params", "(", ")", "\n", "self", ".", "data_aug_params", "[", "'move_last_seg_chanel_to_data'", "]", "=", "True", "\n", "self", ".", "data_aug_params", "[", "'cascade_do_cascade_augmentations'", "]", "=", "True", "\n", "\n", "self", ".", "data_aug_params", "[", "'cascade_random_binary_transform_p'", "]", "=", "0.4", "\n", "self", ".", "data_aug_params", "[", "'cascade_random_binary_transform_p_per_label'", "]", "=", "1", "\n", "self", ".", "data_aug_params", "[", "'cascade_random_binary_transform_size'", "]", "=", "(", "1", ",", "8", ")", "\n", "\n", "self", ".", "data_aug_params", "[", "'cascade_remove_conn_comp_p'", "]", "=", "0.2", "\n", "self", ".", "data_aug_params", "[", "'cascade_remove_conn_comp_max_size_percent_threshold'", "]", "=", "0.15", "\n", "self", ".", "data_aug_params", "[", "'cascade_remove_conn_comp_fill_with_other_class_p'", "]", "=", "0.0", "\n", "\n", "# we have 2 channels now because the segmentation from the previous stage is stored in 'seg' as well until it", "\n", "# is moved to 'data' at the end", "\n", "self", ".", "data_aug_params", "[", "'selected_seg_channels'", "]", "=", "[", "0", ",", "1", "]", "\n", "# needed for converting the segmentation from the previous stage to one hot", "\n", "self", ".", "data_aug_params", "[", "'all_segmentation_labels'", "]", "=", "list", "(", "range", "(", "1", ",", "self", ".", "num_classes", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.network_training.nnUNetTrainerCascadeFullRes.nnUNetTrainerCascadeFullRes.initialize": [[109, 151], ["nnUNetTrainerCascadeFullRes.nnUNetTrainerCascadeFullRes.process_plans", "nnUNetTrainerCascadeFullRes.nnUNetTrainerCascadeFullRes.setup_DA_params", "join", "nnUNetTrainerCascadeFullRes.nnUNetTrainerCascadeFullRes.initialize_network", "isinstance", "nnUNetTrainerCascadeFullRes.nnUNetTrainerCascadeFullRes.load_plans_file", "nnUNetTrainerCascadeFullRes.nnUNetTrainerCascadeFullRes.setup_DA_params", "nnUNetTrainerCascadeFullRes.nnUNetTrainerCascadeFullRes.get_basic_generators", "nnunet.training.data_augmentation.default_data_augmentation.get_default_augmentation", "nnUNetTrainerCascadeFullRes.nnUNetTrainerCascadeFullRes.print_to_log_file", "nnUNetTrainerCascadeFullRes.nnUNetTrainerCascadeFullRes.print_to_log_file", "print", "nnunet.training.dataloading.dataset_loading.unpack_dataset", "print", "print", "str", "str", "nnUNetTrainerCascadeFullRes.nnUNetTrainerCascadeFullRes.dataset_tr.keys", "nnUNetTrainerCascadeFullRes.nnUNetTrainerCascadeFullRes.dataset_val.keys"], "methods", ["home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.BraTS_trainer.nnUNetTrainerV2BraTSRegions.nnUNetTrainerV2BraTSRegions_DA3_BN.process_plans", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.BraTS_trainer.nnUNetTrainerV2BraTSRegions.nnUNetTrainerV2BraTSRegions_DA4_BN.setup_DA_params", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.BraTS_trainer.nnUNetTrainerV2BraTSRegions.nnUNetTrainerV2BraTSRegions_DA3.initialize_network", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.network_training.nnUNetTrainer.nnUNetTrainer.load_plans_file", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.BraTS_trainer.nnUNetTrainerV2BraTSRegions.nnUNetTrainerV2BraTSRegions_DA4_BN.setup_DA_params", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.nnUNet_variants.nnUNetTrainerNoDA.nnUNetTrainerNoDA.get_basic_generators", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.data_augmentation.default_data_augmentation.get_default_augmentation", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.network_training.nnUNetTrainerV2_DDP.nnUNetTrainerV2_DDP.print_to_log_file", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.network_training.nnUNetTrainerV2_DDP.nnUNetTrainerV2_DDP.print_to_log_file", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.dataloading.dataset_loading.unpack_dataset"], ["", "def", "initialize", "(", "self", ",", "training", "=", "True", ",", "force_load_plans", "=", "False", ")", ":", "\n", "        ", "\"\"\"\n        For prediction of test cases just set training=False, this will prevent loading of training data and\n        training batchgenerator initialization\n        :param training:\n        :return:\n        \"\"\"", "\n", "if", "force_load_plans", "or", "(", "self", ".", "plans", "is", "None", ")", ":", "\n", "            ", "self", ".", "load_plans_file", "(", ")", "\n", "\n", "", "self", ".", "process_plans", "(", "self", ".", "plans", ")", "\n", "\n", "self", ".", "setup_DA_params", "(", ")", "\n", "\n", "self", ".", "folder_with_preprocessed_data", "=", "join", "(", "self", ".", "dataset_directory", ",", "self", ".", "plans", "[", "'data_identifier'", "]", "+", "\n", "\"_stage%d\"", "%", "self", ".", "stage", ")", "\n", "if", "training", ":", "\n", "            ", "self", ".", "setup_DA_params", "(", ")", "\n", "\n", "if", "self", ".", "folder_with_preprocessed_data", "is", "not", "None", ":", "\n", "                ", "self", ".", "dl_tr", ",", "self", ".", "dl_val", "=", "self", ".", "get_basic_generators", "(", ")", "\n", "\n", "if", "self", ".", "unpack_data", ":", "\n", "                    ", "print", "(", "\"unpacking dataset\"", ")", "\n", "unpack_dataset", "(", "self", ".", "folder_with_preprocessed_data", ")", "\n", "print", "(", "\"done\"", ")", "\n", "", "else", ":", "\n", "                    ", "print", "(", "\n", "\"INFO: Not unpacking data! Training may be slow due to that. Pray you are not using 2d or you \"", "\n", "\"will wait all winter for your model to finish!\"", ")", "\n", "\n", "", "self", ".", "tr_gen", ",", "self", ".", "val_gen", "=", "get_default_augmentation", "(", "self", ".", "dl_tr", ",", "self", ".", "dl_val", ",", "\n", "self", ".", "data_aug_params", "[", "\n", "'patch_size_for_spatialtransform'", "]", ",", "\n", "self", ".", "data_aug_params", ")", "\n", "self", ".", "print_to_log_file", "(", "\"TRAINING KEYS:\\n %s\"", "%", "(", "str", "(", "self", ".", "dataset_tr", ".", "keys", "(", ")", ")", ")", ")", "\n", "self", ".", "print_to_log_file", "(", "\"VALIDATION KEYS:\\n %s\"", "%", "(", "str", "(", "self", ".", "dataset_val", ".", "keys", "(", ")", ")", ")", ")", "\n", "", "", "else", ":", "\n", "            ", "pass", "\n", "", "self", ".", "initialize_network", "(", ")", "\n", "assert", "isinstance", "(", "self", ".", "network", ",", "SegmentationNetwork", ")", "\n", "self", ".", "was_initialized", "=", "True", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.network_training.nnUNetTrainerCascadeFullRes.nnUNetTrainerCascadeFullRes.validate": [[152, 287], ["nnUNetTrainerCascadeFullRes.nnUNetTrainerCascadeFullRes.network.eval", "join", "maybe_mkdir_p", "multiprocessing.pool.Pool", "nnUNetTrainerCascadeFullRes.nnUNetTrainerCascadeFullRes.plans.get", "nnUNetTrainerCascadeFullRes.nnUNetTrainerCascadeFullRes.dataset_val.keys", "nnunet.evaluation.evaluator.aggregate_scores", "nnUNetTrainerCascadeFullRes.nnUNetTrainerCascadeFullRes.print_to_log_file", "nnunet.postprocessing.connected_components.determine_postprocessing", "join", "maybe_mkdir_p", "subfiles", "nnUNetTrainerCascadeFullRes.nnUNetTrainerCascadeFullRes.network.train", "multiprocessing.pool.Pool.close", "multiprocessing.pool.Pool.join", "nnUNetTrainerCascadeFullRes.nnUNetTrainerCascadeFullRes.load_dataset", "nnUNetTrainerCascadeFullRes.nnUNetTrainerCascadeFullRes.do_split", "load_pickle", "print", "numpy.concatenate", "results.append", "pred_gt_tuples.append", "i.get", "nnUNetTrainerCascadeFullRes.nnUNetTrainerCascadeFullRes.dataset_directory.split", "nnUNetTrainerCascadeFullRes.nnUNetTrainerCascadeFullRes.plans.keys", "numpy.load", "nnUNetTrainerCascadeFullRes.nnUNetTrainerCascadeFullRes.predict_preprocessed_data_return_seg_and_softmax", "nnUNetTrainerCascadeFullRes.nnUNetTrainerCascadeFullRes.plans.get", "softmax_pred.transpose.transpose.transpose", "join", "numpy.prod", "numpy.save", "multiprocessing.pool.Pool.starmap_async", "list", "join", "numpy.load", "nnunet.utilities.one_hot_encoding.to_one_hot", "[].split", "join", "join", "range", "shutil.copy", "join", "range", "time.sleep", "join"], "methods", ["home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.evaluation.evaluator.aggregate_scores", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.network_training.nnUNetTrainerV2_DDP.nnUNetTrainerV2_DDP.print_to_log_file", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.postprocessing.connected_components.determine_postprocessing", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.dataloading.dataset_loading.load_dataset", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.network_training.nnUNetTrainerV2.nnUNetTrainerV2.do_split", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.architectural_variants.nnUNetTrainerV2_ResencUNet.nnUNetTrainerV2_ResencUNet.predict_preprocessed_data_return_seg_and_softmax", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.utilities.one_hot_encoding.to_one_hot"], ["", "def", "validate", "(", "self", ",", "do_mirroring", ":", "bool", "=", "True", ",", "use_sliding_window", ":", "bool", "=", "True", ",", "\n", "step_size", ":", "float", "=", "0.5", ",", "\n", "save_softmax", ":", "bool", "=", "True", ",", "use_gaussian", ":", "bool", "=", "True", ",", "overwrite", ":", "bool", "=", "True", ",", "\n", "validation_folder_name", ":", "str", "=", "'validation_raw'", ",", "debug", ":", "bool", "=", "False", ",", "all_in_gpu", ":", "bool", "=", "False", ",", "\n", "segmentation_export_kwargs", ":", "dict", "=", "None", ")", ":", "\n", "\n", "        ", "current_mode", "=", "self", ".", "network", ".", "training", "\n", "self", ".", "network", ".", "eval", "(", ")", "\n", "\n", "assert", "self", ".", "was_initialized", ",", "\"must initialize, ideally with checkpoint (or train first)\"", "\n", "if", "self", ".", "dataset_val", "is", "None", ":", "\n", "            ", "self", ".", "load_dataset", "(", ")", "\n", "self", ".", "do_split", "(", ")", "\n", "\n", "", "if", "segmentation_export_kwargs", "is", "None", ":", "\n", "            ", "if", "'segmentation_export_params'", "in", "self", ".", "plans", ".", "keys", "(", ")", ":", "\n", "                ", "force_separate_z", "=", "self", ".", "plans", "[", "'segmentation_export_params'", "]", "[", "'force_separate_z'", "]", "\n", "interpolation_order", "=", "self", ".", "plans", "[", "'segmentation_export_params'", "]", "[", "'interpolation_order'", "]", "\n", "interpolation_order_z", "=", "self", ".", "plans", "[", "'segmentation_export_params'", "]", "[", "'interpolation_order_z'", "]", "\n", "", "else", ":", "\n", "                ", "force_separate_z", "=", "None", "\n", "interpolation_order", "=", "1", "\n", "interpolation_order_z", "=", "0", "\n", "", "", "else", ":", "\n", "            ", "force_separate_z", "=", "segmentation_export_kwargs", "[", "'force_separate_z'", "]", "\n", "interpolation_order", "=", "segmentation_export_kwargs", "[", "'interpolation_order'", "]", "\n", "interpolation_order_z", "=", "segmentation_export_kwargs", "[", "'interpolation_order_z'", "]", "\n", "\n", "", "output_folder", "=", "join", "(", "self", ".", "output_folder", ",", "validation_folder_name", ")", "\n", "maybe_mkdir_p", "(", "output_folder", ")", "\n", "\n", "if", "do_mirroring", ":", "\n", "            ", "mirror_axes", "=", "self", ".", "data_aug_params", "[", "'mirror_axes'", "]", "\n", "", "else", ":", "\n", "            ", "mirror_axes", "=", "(", ")", "\n", "\n", "", "pred_gt_tuples", "=", "[", "]", "\n", "\n", "export_pool", "=", "Pool", "(", "2", ")", "\n", "results", "=", "[", "]", "\n", "\n", "transpose_backward", "=", "self", ".", "plans", ".", "get", "(", "'transpose_backward'", ")", "\n", "\n", "for", "k", "in", "self", ".", "dataset_val", ".", "keys", "(", ")", ":", "\n", "            ", "properties", "=", "load_pickle", "(", "self", ".", "dataset", "[", "k", "]", "[", "'properties_file'", "]", ")", "\n", "data", "=", "np", ".", "load", "(", "self", ".", "dataset", "[", "k", "]", "[", "'data_file'", "]", ")", "[", "'data'", "]", "\n", "\n", "# concat segmentation of previous step", "\n", "seg_from_prev_stage", "=", "np", ".", "load", "(", "join", "(", "self", ".", "folder_with_segs_from_prev_stage", ",", "\n", "k", "+", "\"_segFromPrevStage.npz\"", ")", ")", "[", "'data'", "]", "[", "None", "]", "\n", "\n", "print", "(", "data", ".", "shape", ")", "\n", "data", "[", "-", "1", "]", "[", "data", "[", "-", "1", "]", "==", "-", "1", "]", "=", "0", "\n", "data_for_net", "=", "np", ".", "concatenate", "(", "(", "data", "[", ":", "-", "1", "]", ",", "to_one_hot", "(", "seg_from_prev_stage", "[", "0", "]", ",", "range", "(", "1", ",", "self", ".", "num_classes", ")", ")", ")", ")", "\n", "\n", "softmax_pred", "=", "self", ".", "predict_preprocessed_data_return_seg_and_softmax", "(", "data_for_net", ",", "do_mirroring", ",", "\n", "mirror_axes", ",", "use_sliding_window", ",", "\n", "step_size", ",", "use_gaussian", ",", "\n", "all_in_gpu", "=", "all_in_gpu", ",", "\n", "mixed_precision", "=", "self", ".", "fp16", ")", "[", "1", "]", "\n", "\n", "if", "transpose_backward", "is", "not", "None", ":", "\n", "                ", "transpose_backward", "=", "self", ".", "plans", ".", "get", "(", "'transpose_backward'", ")", "\n", "softmax_pred", "=", "softmax_pred", ".", "transpose", "(", "[", "0", "]", "+", "[", "i", "+", "1", "for", "i", "in", "transpose_backward", "]", ")", "\n", "\n", "", "fname", "=", "properties", "[", "'list_of_data_files'", "]", "[", "0", "]", ".", "split", "(", "\"/\"", ")", "[", "-", "1", "]", "[", ":", "-", "12", "]", "\n", "\n", "if", "save_softmax", ":", "\n", "                ", "softmax_fname", "=", "join", "(", "output_folder", ",", "fname", "+", "\".npz\"", ")", "\n", "", "else", ":", "\n", "                ", "softmax_fname", "=", "None", "\n", "\n", "", "\"\"\"There is a problem with python process communication that prevents us from communicating obejcts \n            larger than 2 GB between processes (basically when the length of the pickle string that will be sent is \n            communicated by the multiprocessing.Pipe object then the placeholder (\\%i I think) does not allow for long \n            enough strings (lol). This could be fixed by changing i to l (for long) but that would require manually \n            patching system python code. We circumvent that problem here by saving softmax_pred to a npy file that will \n            then be read (and finally deleted) by the Process. save_segmentation_nifti_from_softmax can take either \n            filename or np.ndarray and will handle this automatically\"\"\"", "\n", "if", "np", ".", "prod", "(", "softmax_pred", ".", "shape", ")", ">", "(", "2e9", "/", "4", "*", "0.85", ")", ":", "# *0.85 just to be save", "\n", "                ", "np", ".", "save", "(", "fname", "+", "\".npy\"", ",", "softmax_pred", ")", "\n", "softmax_pred", "=", "fname", "+", "\".npy\"", "\n", "\n", "", "results", ".", "append", "(", "export_pool", ".", "starmap_async", "(", "save_segmentation_nifti_from_softmax", ",", "\n", "(", "(", "softmax_pred", ",", "join", "(", "output_folder", ",", "fname", "+", "\".nii.gz\"", ")", ",", "\n", "properties", ",", "interpolation_order", ",", "self", ".", "regions_class_order", ",", "\n", "None", ",", "None", ",", "\n", "softmax_fname", ",", "None", ",", "force_separate_z", ",", "\n", "interpolation_order_z", ")", ",", "\n", ")", "\n", ")", "\n", ")", "\n", "\n", "pred_gt_tuples", ".", "append", "(", "[", "join", "(", "output_folder", ",", "fname", "+", "\".nii.gz\"", ")", ",", "\n", "join", "(", "self", ".", "gt_niftis_folder", ",", "fname", "+", "\".nii.gz\"", ")", "]", ")", "\n", "\n", "", "_", "=", "[", "i", ".", "get", "(", ")", "for", "i", "in", "results", "]", "\n", "\n", "task", "=", "self", ".", "dataset_directory", ".", "split", "(", "\"/\"", ")", "[", "-", "1", "]", "\n", "job_name", "=", "self", ".", "experiment_name", "\n", "_", "=", "aggregate_scores", "(", "pred_gt_tuples", ",", "labels", "=", "list", "(", "range", "(", "self", ".", "num_classes", ")", ")", ",", "\n", "json_output_file", "=", "join", "(", "output_folder", ",", "\"summary.json\"", ")", ",", "json_name", "=", "job_name", ",", "\n", "json_author", "=", "\"Fabian\"", ",", "json_description", "=", "\"\"", ",", "\n", "json_task", "=", "task", ")", "\n", "\n", "# in the old nnunet we would stop here. Now we add a postprocessing. This postprocessing can remove everything", "\n", "# except the largest connected component for each class. To see if this improves results, we do this for all", "\n", "# classes and then rerun the evaluation. Those classes for which this resulted in an improved dice score will", "\n", "# have this applied during inference as well", "\n", "self", ".", "print_to_log_file", "(", "\"determining postprocessing\"", ")", "\n", "determine_postprocessing", "(", "self", ".", "output_folder", ",", "self", ".", "gt_niftis_folder", ",", "validation_folder_name", ",", "\n", "final_subf_name", "=", "validation_folder_name", "+", "\"_postprocessed\"", ",", "debug", "=", "debug", ")", "\n", "# after this the final predictions for the vlaidation set can be found in validation_folder_name_base + \"_postprocessed\"", "\n", "# They are always in that folder, even if no postprocessing as applied!", "\n", "\n", "# detemining postprocesing on a per-fold basis may be OK for this fold but what if another fold finds another", "\n", "# postprocesing to be better? In this case we need to consolidate. At the time the consolidation is going to be", "\n", "# done we won't know what self.gt_niftis_folder was, so now we copy all the niftis into a separate folder to", "\n", "# be used later", "\n", "gt_nifti_folder", "=", "join", "(", "self", ".", "output_folder_base", ",", "\"gt_niftis\"", ")", "\n", "maybe_mkdir_p", "(", "gt_nifti_folder", ")", "\n", "for", "f", "in", "subfiles", "(", "self", ".", "gt_niftis_folder", ",", "suffix", "=", "\".nii.gz\"", ")", ":", "\n", "            ", "success", "=", "False", "\n", "attempts", "=", "0", "\n", "while", "not", "success", "and", "attempts", "<", "10", ":", "\n", "                ", "try", ":", "\n", "                    ", "shutil", ".", "copy", "(", "f", ",", "gt_nifti_folder", ")", "\n", "success", "=", "True", "\n", "", "except", "OSError", ":", "\n", "                    ", "attempts", "+=", "1", "\n", "sleep", "(", "1", ")", "\n", "\n", "", "", "", "self", ".", "network", ".", "train", "(", "current_mode", ")", "\n", "export_pool", ".", "close", "(", ")", "\n", "export_pool", ".", "join", "(", ")", "", "", "", ""]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.network_training.nnUNetTrainerV2_DP.nnUNetTrainerV2_DP.__init__": [[35, 47], ["nnunet.training.network_training.nnUNetTrainerV2.nnUNetTrainerV2.__init__"], "methods", ["home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.common.layers.ResizeImage.__init__"], ["    ", "def", "__init__", "(", "self", ",", "plans_file", ",", "fold", ",", "output_folder", "=", "None", ",", "dataset_directory", "=", "None", ",", "batch_dice", "=", "True", ",", "stage", "=", "None", ",", "\n", "unpack_data", "=", "True", ",", "deterministic", "=", "True", ",", "num_gpus", "=", "1", ",", "distribute_batch_size", "=", "False", ",", "fp16", "=", "False", ")", ":", "\n", "        ", "super", "(", "nnUNetTrainerV2_DP", ",", "self", ")", ".", "__init__", "(", "plans_file", ",", "fold", ",", "output_folder", ",", "dataset_directory", ",", "batch_dice", ",", "stage", ",", "\n", "unpack_data", ",", "deterministic", ",", "fp16", ")", "\n", "self", ".", "init_args", "=", "(", "plans_file", ",", "fold", ",", "output_folder", ",", "dataset_directory", ",", "batch_dice", ",", "stage", ",", "unpack_data", ",", "\n", "deterministic", ",", "num_gpus", ",", "distribute_batch_size", ",", "fp16", ")", "\n", "self", ".", "num_gpus", "=", "num_gpus", "\n", "self", ".", "distribute_batch_size", "=", "distribute_batch_size", "\n", "self", ".", "dice_smooth", "=", "1e-5", "\n", "self", ".", "dice_do_BG", "=", "False", "\n", "self", ".", "loss", "=", "None", "\n", "self", ".", "loss_weights", "=", "None", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.network_training.nnUNetTrainerV2_DP.nnUNetTrainerV2_DP.setup_DA_params": [[48, 51], ["super().setup_DA_params"], "methods", ["home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.BraTS_trainer.nnUNetTrainerV2BraTSRegions.nnUNetTrainerV2BraTSRegions_DA4_BN.setup_DA_params"], ["", "def", "setup_DA_params", "(", "self", ")", ":", "\n", "        ", "super", "(", "nnUNetTrainerV2_DP", ",", "self", ")", ".", "setup_DA_params", "(", ")", "\n", "self", ".", "data_aug_params", "[", "'num_threads'", "]", "=", "8", "*", "self", ".", "num_gpus", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.network_training.nnUNetTrainerV2_DP.nnUNetTrainerV2_DP.process_plans": [[52, 61], ["super().process_plans", "print", "print"], "methods", ["home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.BraTS_trainer.nnUNetTrainerV2BraTSRegions.nnUNetTrainerV2BraTSRegions_DA3_BN.process_plans"], ["", "def", "process_plans", "(", "self", ",", "plans", ")", ":", "\n", "        ", "super", "(", "nnUNetTrainerV2_DP", ",", "self", ")", ".", "process_plans", "(", "plans", ")", "\n", "if", "not", "self", ".", "distribute_batch_size", ":", "\n", "            ", "self", ".", "batch_size", "=", "self", ".", "num_gpus", "*", "self", ".", "plans", "[", "'plans_per_stage'", "]", "[", "self", ".", "stage", "]", "[", "'batch_size'", "]", "\n", "", "else", ":", "\n", "            ", "if", "self", ".", "batch_size", "<", "self", ".", "num_gpus", ":", "\n", "                ", "print", "(", "\"WARNING: self.batch_size < self.num_gpus. Will not be able to use the GPUs well\"", ")", "\n", "", "elif", "self", ".", "batch_size", "%", "self", ".", "num_gpus", "!=", "0", ":", "\n", "                ", "print", "(", "\"WARNING: self.batch_size % self.num_gpus != 0. Will not be able to use the GPUs well\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.network_training.nnUNetTrainerV2_DP.nnUNetTrainerV2_DP.initialize": [[62, 124], ["maybe_mkdir_p", "nnUNetTrainerV2_DP.nnUNetTrainerV2_DP.process_plans", "nnUNetTrainerV2_DP.nnUNetTrainerV2_DP.setup_DA_params", "len", "numpy.array", "numpy.array", "join", "nnUNetTrainerV2_DP.nnUNetTrainerV2_DP.initialize_network", "nnUNetTrainerV2_DP.nnUNetTrainerV2_DP.initialize_optimizer_and_scheduler", "isinstance", "nnUNetTrainerV2_DP.nnUNetTrainerV2_DP.print_to_log_file", "nnUNetTrainerV2_DP.nnUNetTrainerV2_DP.load_plans_file", "numpy.array.sum", "nnUNetTrainerV2_DP.nnUNetTrainerV2_DP.get_basic_generators", "nnunet.training.data_augmentation.default_data_augmentation.get_moreDA_augmentation", "nnUNetTrainerV2_DP.nnUNetTrainerV2_DP.print_to_log_file", "nnUNetTrainerV2_DP.nnUNetTrainerV2_DP.print_to_log_file", "print", "nnunet.training.dataloading.dataset_loading.unpack_dataset", "print", "print", "range", "range", "str", "str", "nnUNetTrainerV2_DP.nnUNetTrainerV2_DP.dataset_tr.keys", "nnUNetTrainerV2_DP.nnUNetTrainerV2_DP.dataset_val.keys"], "methods", ["home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.BraTS_trainer.nnUNetTrainerV2BraTSRegions.nnUNetTrainerV2BraTSRegions_DA3_BN.process_plans", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.BraTS_trainer.nnUNetTrainerV2BraTSRegions.nnUNetTrainerV2BraTSRegions_DA4_BN.setup_DA_params", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.BraTS_trainer.nnUNetTrainerV2BraTSRegions.nnUNetTrainerV2BraTSRegions_DA3.initialize_network", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.optimizer_and_lr.nnUNetTrainerV2_momentum09in2D.nnUNetTrainerV2_momentum09in2D.initialize_optimizer_and_scheduler", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.network_training.nnUNetTrainerV2_DDP.nnUNetTrainerV2_DDP.print_to_log_file", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.network_training.nnUNetTrainer.nnUNetTrainer.load_plans_file", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.nnUNet_variants.nnUNetTrainerNoDA.nnUNetTrainerNoDA.get_basic_generators", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.data_augmentation.default_data_augmentation.get_moreDA_augmentation", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.network_training.nnUNetTrainerV2_DDP.nnUNetTrainerV2_DDP.print_to_log_file", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.network_training.nnUNetTrainerV2_DDP.nnUNetTrainerV2_DDP.print_to_log_file", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.dataloading.dataset_loading.unpack_dataset"], ["", "", "", "def", "initialize", "(", "self", ",", "training", "=", "True", ",", "force_load_plans", "=", "False", ")", ":", "\n", "        ", "\"\"\"\n        - replaced get_default_augmentation with get_moreDA_augmentation\n        - only run this code once\n        - loss function wrapper for deep supervision\n\n        :param training:\n        :param force_load_plans:\n        :return:\n        \"\"\"", "\n", "if", "not", "self", ".", "was_initialized", ":", "\n", "            ", "maybe_mkdir_p", "(", "self", ".", "output_folder", ")", "\n", "\n", "if", "force_load_plans", "or", "(", "self", ".", "plans", "is", "None", ")", ":", "\n", "                ", "self", ".", "load_plans_file", "(", ")", "\n", "\n", "", "self", ".", "process_plans", "(", "self", ".", "plans", ")", "\n", "\n", "self", ".", "setup_DA_params", "(", ")", "\n", "\n", "################# Here configure the loss for deep supervision ############", "\n", "net_numpool", "=", "len", "(", "self", ".", "net_num_pool_op_kernel_sizes", ")", "\n", "weights", "=", "np", ".", "array", "(", "[", "1", "/", "(", "2", "**", "i", ")", "for", "i", "in", "range", "(", "net_numpool", ")", "]", ")", "\n", "mask", "=", "np", ".", "array", "(", "[", "True", "if", "i", "<", "net_numpool", "-", "1", "else", "False", "for", "i", "in", "range", "(", "net_numpool", ")", "]", ")", "\n", "weights", "[", "~", "mask", "]", "=", "0", "\n", "weights", "=", "weights", "/", "weights", ".", "sum", "(", ")", "\n", "self", ".", "loss_weights", "=", "weights", "\n", "################# END ###################", "\n", "\n", "self", ".", "folder_with_preprocessed_data", "=", "join", "(", "self", ".", "dataset_directory", ",", "self", ".", "plans", "[", "'data_identifier'", "]", "+", "\n", "\"_stage%d\"", "%", "self", ".", "stage", ")", "\n", "if", "training", ":", "\n", "                ", "self", ".", "dl_tr", ",", "self", ".", "dl_val", "=", "self", ".", "get_basic_generators", "(", ")", "\n", "if", "self", ".", "unpack_data", ":", "\n", "                    ", "print", "(", "\"unpacking dataset\"", ")", "\n", "unpack_dataset", "(", "self", ".", "folder_with_preprocessed_data", ")", "\n", "print", "(", "\"done\"", ")", "\n", "", "else", ":", "\n", "                    ", "print", "(", "\n", "\"INFO: Not unpacking data! Training may be slow due to that. Pray you are not using 2d or you \"", "\n", "\"will wait all winter for your model to finish!\"", ")", "\n", "\n", "", "self", ".", "tr_gen", ",", "self", ".", "val_gen", "=", "get_moreDA_augmentation", "(", "self", ".", "dl_tr", ",", "self", ".", "dl_val", ",", "\n", "self", ".", "data_aug_params", "[", "\n", "'patch_size_for_spatialtransform'", "]", ",", "\n", "self", ".", "data_aug_params", ",", "\n", "deep_supervision_scales", "=", "self", ".", "deep_supervision_scales", ",", "\n", "pin_memory", "=", "self", ".", "pin_memory", ")", "\n", "self", ".", "print_to_log_file", "(", "\"TRAINING KEYS:\\n %s\"", "%", "(", "str", "(", "self", ".", "dataset_tr", ".", "keys", "(", ")", ")", ")", ",", "\n", "also_print_to_console", "=", "False", ")", "\n", "self", ".", "print_to_log_file", "(", "\"VALIDATION KEYS:\\n %s\"", "%", "(", "str", "(", "self", ".", "dataset_val", ".", "keys", "(", ")", ")", ")", ",", "\n", "also_print_to_console", "=", "False", ")", "\n", "", "else", ":", "\n", "                ", "pass", "\n", "\n", "", "self", ".", "initialize_network", "(", ")", "\n", "self", ".", "initialize_optimizer_and_scheduler", "(", ")", "\n", "\n", "assert", "isinstance", "(", "self", ".", "network", ",", "(", "SegmentationNetwork", ",", "DataParallel", ")", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "print_to_log_file", "(", "'self.was_initialized is True, not running self.initialize again'", ")", "\n", "", "self", ".", "was_initialized", "=", "True", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.network_training.nnUNetTrainerV2_DP.nnUNetTrainerV2_DP.initialize_network": [[125, 151], ["nnunet.network_architecture.generic_UNet_DP.Generic_UNet_DP", "torch.cuda.is_available", "len", "nnunet.network_architecture.initialization.InitWeights_He", "nnUNetTrainerV2_DP.nnUNetTrainerV2_DP.network.cuda"], "methods", ["None"], ["", "def", "initialize_network", "(", "self", ")", ":", "\n", "        ", "\"\"\"\n        replace genericUNet with the implementation of above for super speeds\n        \"\"\"", "\n", "if", "self", ".", "threeD", ":", "\n", "            ", "conv_op", "=", "nn", ".", "Conv3d", "\n", "dropout_op", "=", "nn", ".", "Dropout3d", "\n", "norm_op", "=", "nn", ".", "InstanceNorm3d", "\n", "\n", "", "else", ":", "\n", "            ", "conv_op", "=", "nn", ".", "Conv2d", "\n", "dropout_op", "=", "nn", ".", "Dropout2d", "\n", "norm_op", "=", "nn", ".", "InstanceNorm2d", "\n", "\n", "", "norm_op_kwargs", "=", "{", "'eps'", ":", "1e-5", ",", "'affine'", ":", "True", "}", "\n", "dropout_op_kwargs", "=", "{", "'p'", ":", "0", ",", "'inplace'", ":", "True", "}", "\n", "net_nonlin", "=", "nn", ".", "LeakyReLU", "\n", "net_nonlin_kwargs", "=", "{", "'negative_slope'", ":", "1e-2", ",", "'inplace'", ":", "True", "}", "\n", "self", ".", "network", "=", "Generic_UNet_DP", "(", "self", ".", "num_input_channels", ",", "self", ".", "base_num_features", ",", "self", ".", "num_classes", ",", "\n", "len", "(", "self", ".", "net_num_pool_op_kernel_sizes", ")", ",", "\n", "self", ".", "conv_per_stage", ",", "2", ",", "conv_op", ",", "norm_op", ",", "norm_op_kwargs", ",", "dropout_op", ",", "dropout_op_kwargs", ",", "\n", "net_nonlin", ",", "net_nonlin_kwargs", ",", "True", ",", "False", ",", "InitWeights_He", "(", "1e-2", ")", ",", "\n", "self", ".", "net_num_pool_op_kernel_sizes", ",", "self", ".", "net_conv_kernel_sizes", ",", "False", ",", "True", ",", "True", ")", "\n", "if", "torch", ".", "cuda", ".", "is_available", "(", ")", ":", "\n", "            ", "self", ".", "network", ".", "cuda", "(", ")", "\n", "", "self", ".", "network", ".", "inference_apply_nonlin", "=", "softmax_helper", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.network_training.nnUNetTrainerV2_DP.nnUNetTrainerV2_DP.initialize_optimizer_and_scheduler": [[152, 157], ["torch.optim.SGD", "nnUNetTrainerV2_DP.nnUNetTrainerV2_DP.network.parameters"], "methods", ["None"], ["", "def", "initialize_optimizer_and_scheduler", "(", "self", ")", ":", "\n", "        ", "assert", "self", ".", "network", "is", "not", "None", ",", "\"self.initialize_network must be called first\"", "\n", "self", ".", "optimizer", "=", "torch", ".", "optim", ".", "SGD", "(", "self", ".", "network", ".", "parameters", "(", ")", ",", "self", ".", "initial_lr", ",", "weight_decay", "=", "self", ".", "weight_decay", ",", "\n", "momentum", "=", "0.99", ",", "nesterov", "=", "True", ")", "\n", "self", ".", "lr_scheduler", "=", "None", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.network_training.nnUNetTrainerV2_DP.nnUNetTrainerV2_DP.run_training": [[158, 170], ["nnUNetTrainerV2_DP.nnUNetTrainerV2_DP.maybe_update_lr", "torch.nn.parallel.data_parallel.DataParallel", "nnunet.training.network_training.nnUNetTrainer.nnUNetTrainer.run_training", "tuple", "range"], "methods", ["home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.optimizer_and_lr.nnUNetTrainerV2_warmup.nnUNetTrainerV2_warmup.maybe_update_lr", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.architectural_variants.nnUNetTrainerV2_ResencUNet.nnUNetTrainerV2_ResencUNet.run_training"], ["", "def", "run_training", "(", "self", ")", ":", "\n", "        ", "self", ".", "maybe_update_lr", "(", "self", ".", "epoch", ")", "\n", "\n", "# amp must be initialized before DP", "\n", "\n", "ds", "=", "self", ".", "network", ".", "do_ds", "\n", "self", ".", "network", ".", "do_ds", "=", "True", "\n", "self", ".", "network", "=", "DataParallel", "(", "self", ".", "network", ",", "tuple", "(", "range", "(", "self", ".", "num_gpus", ")", ")", ",", ")", "\n", "ret", "=", "nnUNetTrainer", ".", "run_training", "(", "self", ")", "\n", "self", ".", "network", "=", "self", ".", "network", ".", "module", "\n", "self", ".", "network", ".", "do_ds", "=", "ds", "\n", "return", "ret", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.network_training.nnUNetTrainerV2_DP.nnUNetTrainerV2_DP.run_iteration": [[171, 218], ["next", "nnunet.utilities.to_torch.maybe_to_torch", "nnunet.utilities.to_torch.maybe_to_torch", "torch.cuda.is_available", "nnUNetTrainerV2_DP.nnUNetTrainerV2_DP.optimizer.zero_grad", "nnUNetTrainerV2_DP.nnUNetTrainerV2_DP.detach().cpu().numpy", "nnunet.utilities.to_torch.to_cuda", "nnunet.utilities.to_torch.to_cuda", "nnUNetTrainerV2_DP.nnUNetTrainerV2_DP.network", "nnUNetTrainerV2_DP.nnUNetTrainerV2_DP.compute_loss", "torch.cuda.amp.autocast", "nnUNetTrainerV2_DP.nnUNetTrainerV2_DP.network", "nnUNetTrainerV2_DP.nnUNetTrainerV2_DP.compute_loss", "nnUNetTrainerV2_DP.nnUNetTrainerV2_DP.amp_grad_scaler.scale().backward", "nnUNetTrainerV2_DP.nnUNetTrainerV2_DP.amp_grad_scaler.unscale_", "torch.nn.utils.clip_grad_norm_", "nnUNetTrainerV2_DP.nnUNetTrainerV2_DP.amp_grad_scaler.step", "nnUNetTrainerV2_DP.nnUNetTrainerV2_DP.amp_grad_scaler.update", "nnUNetTrainerV2_DP.nnUNetTrainerV2_DP.run_online_evaluation", "nnUNetTrainerV2_DP.nnUNetTrainerV2_DP.backward", "torch.nn.utils.clip_grad_norm_", "nnUNetTrainerV2_DP.nnUNetTrainerV2_DP.optimizer.step", "nnUNetTrainerV2_DP.nnUNetTrainerV2_DP.detach().cpu", "nnUNetTrainerV2_DP.nnUNetTrainerV2_DP.run_online_evaluation", "nnUNetTrainerV2_DP.nnUNetTrainerV2_DP.network.parameters", "nnUNetTrainerV2_DP.nnUNetTrainerV2_DP.network.parameters", "nnUNetTrainerV2_DP.nnUNetTrainerV2_DP.amp_grad_scaler.scale", "nnUNetTrainerV2_DP.nnUNetTrainerV2_DP.detach"], "methods", ["home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.utilities.to_torch.maybe_to_torch", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.utilities.to_torch.maybe_to_torch", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.utilities.to_torch.to_cuda", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.utilities.to_torch.to_cuda", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.network_training.nnUNetTrainerV2_DDP.nnUNetTrainerV2_DDP.compute_loss", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.network_training.nnUNetTrainerV2_DDP.nnUNetTrainerV2_DDP.compute_loss", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.utilities.distributed.awesome_allgather_function.backward", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.optimizer.ranger.Ranger.step", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.BraTS_trainer.nnUNetTrainerV2BraTSRegions.nnUNetTrainerV2BraTSRegions_DA3_BN.run_online_evaluation", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.utilities.distributed.awesome_allgather_function.backward", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.optimizer.ranger.Ranger.step", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.BraTS_trainer.nnUNetTrainerV2BraTSRegions.nnUNetTrainerV2BraTSRegions_DA3_BN.run_online_evaluation"], ["", "def", "run_iteration", "(", "self", ",", "data_generator", ",", "do_backprop", "=", "True", ",", "run_online_evaluation", "=", "False", ")", ":", "\n", "        ", "data_dict", "=", "next", "(", "data_generator", ")", "\n", "data", "=", "data_dict", "[", "'data'", "]", "\n", "target", "=", "data_dict", "[", "'target'", "]", "\n", "\n", "data", "=", "maybe_to_torch", "(", "data", ")", "\n", "target", "=", "maybe_to_torch", "(", "target", ")", "\n", "\n", "if", "torch", ".", "cuda", ".", "is_available", "(", ")", ":", "\n", "            ", "data", "=", "to_cuda", "(", "data", ")", "\n", "target", "=", "to_cuda", "(", "target", ")", "\n", "\n", "", "self", ".", "optimizer", ".", "zero_grad", "(", ")", "\n", "\n", "if", "self", ".", "fp16", ":", "\n", "            ", "with", "autocast", "(", ")", ":", "\n", "                ", "ret", "=", "self", ".", "network", "(", "data", ",", "target", ",", "return_hard_tp_fp_fn", "=", "run_online_evaluation", ")", "\n", "if", "run_online_evaluation", ":", "\n", "                    ", "ces", ",", "tps", ",", "fps", ",", "fns", ",", "tp_hard", ",", "fp_hard", ",", "fn_hard", "=", "ret", "\n", "self", ".", "run_online_evaluation", "(", "tp_hard", ",", "fp_hard", ",", "fn_hard", ")", "\n", "", "else", ":", "\n", "                    ", "ces", ",", "tps", ",", "fps", ",", "fns", "=", "ret", "\n", "", "del", "data", ",", "target", "\n", "l", "=", "self", ".", "compute_loss", "(", "ces", ",", "tps", ",", "fps", ",", "fns", ")", "\n", "\n", "", "if", "do_backprop", ":", "\n", "                ", "self", ".", "amp_grad_scaler", ".", "scale", "(", "l", ")", ".", "backward", "(", ")", "\n", "self", ".", "amp_grad_scaler", ".", "unscale_", "(", "self", ".", "optimizer", ")", "\n", "torch", ".", "nn", ".", "utils", ".", "clip_grad_norm_", "(", "self", ".", "network", ".", "parameters", "(", ")", ",", "12", ")", "\n", "self", ".", "amp_grad_scaler", ".", "step", "(", "self", ".", "optimizer", ")", "\n", "self", ".", "amp_grad_scaler", ".", "update", "(", ")", "\n", "", "", "else", ":", "\n", "            ", "ret", "=", "self", ".", "network", "(", "data", ",", "target", ",", "return_hard_tp_fp_fn", "=", "run_online_evaluation", ")", "\n", "if", "run_online_evaluation", ":", "\n", "                ", "ces", ",", "tps", ",", "fps", ",", "fns", ",", "tp_hard", ",", "fp_hard", ",", "fn_hard", "=", "ret", "\n", "self", ".", "run_online_evaluation", "(", "tp_hard", ",", "fp_hard", ",", "fn_hard", ")", "\n", "", "else", ":", "\n", "                ", "ces", ",", "tps", ",", "fps", ",", "fns", "=", "ret", "\n", "", "del", "data", ",", "target", "\n", "l", "=", "self", ".", "compute_loss", "(", "ces", ",", "tps", ",", "fps", ",", "fns", ")", "\n", "\n", "if", "do_backprop", ":", "\n", "                ", "l", ".", "backward", "(", ")", "\n", "torch", ".", "nn", ".", "utils", ".", "clip_grad_norm_", "(", "self", ".", "network", ".", "parameters", "(", ")", ",", "12", ")", "\n", "self", ".", "optimizer", ".", "step", "(", ")", "\n", "\n", "", "", "return", "l", ".", "detach", "(", ")", ".", "cpu", "(", ")", ".", "numpy", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.network_training.nnUNetTrainerV2_DP.nnUNetTrainerV2_DP.run_online_evaluation": [[219, 227], ["tp_hard.detach().cpu().numpy().mean.detach().cpu().numpy().mean.detach().cpu().numpy().mean", "fp_hard.detach().cpu().numpy().mean.detach().cpu().numpy().mean.detach().cpu().numpy().mean", "fn_hard.detach().cpu().numpy().mean.detach().cpu().numpy().mean.detach().cpu().numpy().mean", "nnUNetTrainerV2_DP.nnUNetTrainerV2_DP.online_eval_foreground_dc.append", "nnUNetTrainerV2_DP.nnUNetTrainerV2_DP.online_eval_tp.append", "nnUNetTrainerV2_DP.nnUNetTrainerV2_DP.online_eval_fp.append", "nnUNetTrainerV2_DP.nnUNetTrainerV2_DP.online_eval_fn.append", "list", "list", "list", "list", "tp_hard.detach().cpu().numpy().mean.detach().cpu().numpy().mean.detach().cpu().numpy", "fp_hard.detach().cpu().numpy().mean.detach().cpu().numpy().mean.detach().cpu().numpy", "fn_hard.detach().cpu().numpy().mean.detach().cpu().numpy().mean.detach().cpu().numpy", "tp_hard.detach().cpu().numpy().mean.detach().cpu().numpy().mean.detach().cpu", "fp_hard.detach().cpu().numpy().mean.detach().cpu().numpy().mean.detach().cpu", "fn_hard.detach().cpu().numpy().mean.detach().cpu().numpy().mean.detach().cpu", "tp_hard.detach().cpu().numpy().mean.detach().cpu().numpy().mean.detach", "fp_hard.detach().cpu().numpy().mean.detach().cpu().numpy().mean.detach", "fn_hard.detach().cpu().numpy().mean.detach().cpu().numpy().mean.detach"], "methods", ["None"], ["", "def", "run_online_evaluation", "(", "self", ",", "tp_hard", ",", "fp_hard", ",", "fn_hard", ")", ":", "\n", "        ", "tp_hard", "=", "tp_hard", ".", "detach", "(", ")", ".", "cpu", "(", ")", ".", "numpy", "(", ")", ".", "mean", "(", "0", ")", "\n", "fp_hard", "=", "fp_hard", ".", "detach", "(", ")", ".", "cpu", "(", ")", ".", "numpy", "(", ")", ".", "mean", "(", "0", ")", "\n", "fn_hard", "=", "fn_hard", ".", "detach", "(", ")", ".", "cpu", "(", ")", ".", "numpy", "(", ")", ".", "mean", "(", "0", ")", "\n", "self", ".", "online_eval_foreground_dc", ".", "append", "(", "list", "(", "(", "2", "*", "tp_hard", ")", "/", "(", "2", "*", "tp_hard", "+", "fp_hard", "+", "fn_hard", "+", "1e-8", ")", ")", ")", "\n", "self", ".", "online_eval_tp", ".", "append", "(", "list", "(", "tp_hard", ")", ")", "\n", "self", ".", "online_eval_fp", ".", "append", "(", "list", "(", "fp_hard", ")", ")", "\n", "self", ".", "online_eval_fn", ".", "append", "(", "list", "(", "fn_hard", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.network_training.nnUNetTrainerV2_DP.nnUNetTrainerV2_DP.compute_loss": [[228, 258], ["range", "len", "tp.sum.sum.sum", "fp.sum.sum.sum", "fn.sum.sum.sum", "ces[].mean", "ces[].mean"], "methods", ["None"], ["", "def", "compute_loss", "(", "self", ",", "ces", ",", "tps", ",", "fps", ",", "fns", ")", ":", "\n", "# we now need to effectively reimplement the loss", "\n", "        ", "loss", "=", "None", "\n", "for", "i", "in", "range", "(", "len", "(", "ces", ")", ")", ":", "\n", "            ", "if", "not", "self", ".", "dice_do_BG", ":", "\n", "                ", "tp", "=", "tps", "[", "i", "]", "[", ":", ",", "1", ":", "]", "\n", "fp", "=", "fps", "[", "i", "]", "[", ":", ",", "1", ":", "]", "\n", "fn", "=", "fns", "[", "i", "]", "[", ":", ",", "1", ":", "]", "\n", "", "else", ":", "\n", "                ", "tp", "=", "tps", "[", "i", "]", "\n", "fp", "=", "fps", "[", "i", "]", "\n", "fn", "=", "fns", "[", "i", "]", "\n", "\n", "", "if", "self", ".", "batch_dice", ":", "\n", "                ", "tp", "=", "tp", ".", "sum", "(", "0", ")", "\n", "fp", "=", "fp", ".", "sum", "(", "0", ")", "\n", "fn", "=", "fn", ".", "sum", "(", "0", ")", "\n", "", "else", ":", "\n", "                ", "pass", "\n", "\n", "", "nominator", "=", "2", "*", "tp", "+", "self", ".", "dice_smooth", "\n", "denominator", "=", "2", "*", "tp", "+", "fp", "+", "fn", "+", "self", ".", "dice_smooth", "\n", "\n", "dice_loss", "=", "(", "-", "nominator", "/", "denominator", ")", ".", "mean", "(", ")", "\n", "if", "loss", "is", "None", ":", "\n", "                ", "loss", "=", "self", ".", "loss_weights", "[", "i", "]", "*", "(", "ces", "[", "i", "]", ".", "mean", "(", ")", "+", "dice_loss", ")", "\n", "", "else", ":", "\n", "                ", "loss", "+=", "self", ".", "loss_weights", "[", "i", "]", "*", "(", "ces", "[", "i", "]", ".", "mean", "(", ")", "+", "dice_loss", ")", "\n", "###########", "\n", "", "", "return", "loss", "", "", "", ""]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.network_training.nnUNetTrainerV2_fp32.nnUNetTrainerV2_fp32.__init__": [[24, 28], ["nnunet.training.network_training.nnUNetTrainerV2.nnUNetTrainerV2.__init__"], "methods", ["home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.common.layers.ResizeImage.__init__"], ["def", "__init__", "(", "self", ",", "plans_file", ",", "fold", ",", "output_folder", "=", "None", ",", "dataset_directory", "=", "None", ",", "batch_dice", "=", "True", ",", "stage", "=", "None", ",", "\n", "unpack_data", "=", "True", ",", "deterministic", "=", "True", ",", "fp16", "=", "False", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "plans_file", ",", "fold", ",", "output_folder", ",", "dataset_directory", ",", "batch_dice", ",", "stage", ",", "unpack_data", ",", "\n", "deterministic", ",", "False", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.network_training.nnUNetTrainerV2_DDP.nnUNetTrainerV2_DDP.__init__": [[42, 68], ["nnunet.training.network_training.nnUNetTrainerV2.nnUNetTrainerV2.__init__", "numpy.random.seed", "torch.manual_seed", "torch.manual_seed", "torch.manual_seed", "torch.manual_seed", "torch.cuda.is_available", "torch.cuda.is_available", "torch.cuda.is_available", "torch.cuda.is_available", "torch.cuda.is_available", "torch.cuda.is_available", "torch.cuda.is_available", "torch.cuda.is_available", "torch.init_process_group", "torch.init_process_group", "nnunet.training.loss_functions.crossentropy.RobustCrossEntropyLoss", "torch.cuda.manual_seed_all", "torch.cuda.manual_seed_all", "torch.cuda.manual_seed_all", "torch.cuda.manual_seed_all", "torch.cuda.set_device", "torch.cuda.set_device", "torch.cuda.set_device", "torch.cuda.set_device"], "methods", ["home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.common.layers.ResizeImage.__init__", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.network_architecture.neural_network.NeuralNetwork.set_device", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.network_architecture.neural_network.NeuralNetwork.set_device", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.network_architecture.neural_network.NeuralNetwork.set_device", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.network_architecture.neural_network.NeuralNetwork.set_device"], ["    ", "def", "__init__", "(", "self", ",", "plans_file", ",", "fold", ",", "local_rank", ",", "output_folder", "=", "None", ",", "dataset_directory", "=", "None", ",", "batch_dice", "=", "True", ",", "\n", "stage", "=", "None", ",", "\n", "unpack_data", "=", "True", ",", "deterministic", "=", "True", ",", "distribute_batch_size", "=", "False", ",", "fp16", "=", "False", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "plans_file", ",", "fold", ",", "output_folder", ",", "dataset_directory", ",", "batch_dice", ",", "stage", ",", "\n", "unpack_data", ",", "deterministic", ",", "fp16", ")", "\n", "self", ".", "init_args", "=", "(", "\n", "plans_file", ",", "fold", ",", "local_rank", ",", "output_folder", ",", "dataset_directory", ",", "batch_dice", ",", "stage", ",", "unpack_data", ",", "\n", "deterministic", ",", "distribute_batch_size", ",", "fp16", ")", "\n", "self", ".", "distribute_batch_size", "=", "distribute_batch_size", "\n", "np", ".", "random", ".", "seed", "(", "local_rank", ")", "\n", "torch", ".", "manual_seed", "(", "local_rank", ")", "\n", "if", "torch", ".", "cuda", ".", "is_available", "(", ")", ":", "\n", "            ", "torch", ".", "cuda", ".", "manual_seed_all", "(", "local_rank", ")", "\n", "", "self", ".", "local_rank", "=", "local_rank", "\n", "\n", "if", "torch", ".", "cuda", ".", "is_available", "(", ")", ":", "\n", "            ", "torch", ".", "cuda", ".", "set_device", "(", "local_rank", ")", "\n", "", "dist", ".", "init_process_group", "(", "backend", "=", "'nccl'", ",", "init_method", "=", "'env://'", ")", "\n", "\n", "self", ".", "val_loss_ma_alpha", "=", "0.95", "\n", "self", ".", "val_loss_MA", "=", "None", "\n", "\n", "self", ".", "loss", "=", "None", "\n", "self", ".", "ce_loss", "=", "RobustCrossEntropyLoss", "(", ")", "\n", "\n", "self", ".", "global_batch_size", "=", "None", "# we need to know this to properly steer oversample", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.network_training.nnUNetTrainerV2_DDP.nnUNetTrainerV2_DDP.set_batch_size_and_oversample": [[69, 112], ["torch.get_world_size", "torch.get_world_size", "torch.get_rank", "torch.get_rank", "numpy.ceil().astype", "range", "print", "print", "batch_sizes.append", "numpy.sum", "numpy.ceil", "numpy.sum", "oversample_percents.append", "len", "oversample_percents.append", "oversample_percents.append"], "methods", ["None"], ["", "def", "set_batch_size_and_oversample", "(", "self", ")", ":", "\n", "        ", "batch_sizes", "=", "[", "]", "\n", "oversample_percents", "=", "[", "]", "\n", "\n", "world_size", "=", "dist", ".", "get_world_size", "(", ")", "\n", "my_rank", "=", "dist", ".", "get_rank", "(", ")", "\n", "\n", "if", "self", ".", "distribute_batch_size", ":", "\n", "            ", "self", ".", "global_batch_size", "=", "self", ".", "batch_size", "\n", "", "else", ":", "\n", "            ", "self", ".", "global_batch_size", "=", "self", ".", "batch_size", "*", "world_size", "\n", "\n", "", "batch_size_per_GPU", "=", "np", ".", "ceil", "(", "self", ".", "batch_size", "/", "world_size", ")", ".", "astype", "(", "int", ")", "\n", "\n", "for", "rank", "in", "range", "(", "world_size", ")", ":", "\n", "            ", "if", "self", ".", "distribute_batch_size", ":", "\n", "                ", "if", "(", "rank", "+", "1", ")", "*", "batch_size_per_GPU", ">", "self", ".", "batch_size", ":", "\n", "                    ", "batch_size", "=", "batch_size_per_GPU", "-", "(", "(", "rank", "+", "1", ")", "*", "batch_size_per_GPU", "-", "self", ".", "batch_size", ")", "\n", "", "else", ":", "\n", "                    ", "batch_size", "=", "batch_size_per_GPU", "\n", "", "", "else", ":", "\n", "                ", "batch_size", "=", "self", ".", "batch_size", "\n", "\n", "", "batch_sizes", ".", "append", "(", "batch_size", ")", "\n", "\n", "sample_id_low", "=", "0", "if", "len", "(", "batch_sizes", ")", "==", "0", "else", "np", ".", "sum", "(", "batch_sizes", "[", ":", "-", "1", "]", ")", "\n", "sample_id_high", "=", "np", ".", "sum", "(", "batch_sizes", ")", "\n", "\n", "if", "sample_id_high", "/", "self", ".", "global_batch_size", "<", "(", "1", "-", "self", ".", "oversample_foreground_percent", ")", ":", "\n", "                ", "oversample_percents", ".", "append", "(", "0.0", ")", "\n", "", "elif", "sample_id_low", "/", "self", ".", "global_batch_size", ">", "(", "1", "-", "self", ".", "oversample_foreground_percent", ")", ":", "\n", "                ", "oversample_percents", ".", "append", "(", "1.0", ")", "\n", "", "else", ":", "\n", "                ", "percent_covered_by_this_rank", "=", "sample_id_high", "/", "self", ".", "global_batch_size", "-", "sample_id_low", "/", "self", ".", "global_batch_size", "\n", "oversample_percent_here", "=", "1", "-", "(", "(", "(", "1", "-", "self", ".", "oversample_foreground_percent", ")", "-", "\n", "sample_id_low", "/", "self", ".", "global_batch_size", ")", "/", "percent_covered_by_this_rank", ")", "\n", "oversample_percents", ".", "append", "(", "oversample_percent_here", ")", "\n", "\n", "", "", "print", "(", "\"worker\"", ",", "my_rank", ",", "\"oversample\"", ",", "oversample_percents", "[", "my_rank", "]", ")", "\n", "print", "(", "\"worker\"", ",", "my_rank", ",", "\"batch_size\"", ",", "batch_sizes", "[", "my_rank", "]", ")", "\n", "\n", "self", ".", "batch_size", "=", "batch_sizes", "[", "my_rank", "]", "\n", "self", ".", "oversample_foreground_percent", "=", "oversample_percents", "[", "my_rank", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.network_training.nnUNetTrainerV2_DDP.nnUNetTrainerV2_DDP.save_checkpoint": [[113, 116], ["super().save_checkpoint"], "methods", ["home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.profiling.nnUNetTrainerV2_2epochs.nnUNetTrainerV2_DDP_5epochs.save_checkpoint"], ["", "def", "save_checkpoint", "(", "self", ",", "fname", ",", "save_optimizer", "=", "True", ")", ":", "\n", "        ", "if", "self", ".", "local_rank", "==", "0", ":", "\n", "            ", "super", "(", ")", ".", "save_checkpoint", "(", "fname", ",", "save_optimizer", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.network_training.nnUNetTrainerV2_DDP.nnUNetTrainerV2_DDP.plot_progress": [[117, 120], ["super().plot_progress"], "methods", ["home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.network_training.nnUNetTrainerV2_DDP.nnUNetTrainerV2_DDP.plot_progress"], ["", "", "def", "plot_progress", "(", "self", ")", ":", "\n", "        ", "if", "self", ".", "local_rank", "==", "0", ":", "\n", "            ", "super", "(", ")", ".", "plot_progress", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.network_training.nnUNetTrainerV2_DDP.nnUNetTrainerV2_DDP.print_to_log_file": [[121, 124], ["super().print_to_log_file"], "methods", ["home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.network_training.nnUNetTrainerV2_DDP.nnUNetTrainerV2_DDP.print_to_log_file"], ["", "", "def", "print_to_log_file", "(", "self", ",", "*", "args", ",", "also_print_to_console", "=", "True", ")", ":", "\n", "        ", "if", "self", ".", "local_rank", "==", "0", ":", "\n", "            ", "super", "(", ")", ".", "print_to_log_file", "(", "*", "args", ",", "also_print_to_console", "=", "also_print_to_console", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.network_training.nnUNetTrainerV2_DDP.nnUNetTrainerV2_DDP.process_plans": [[125, 128], ["super().process_plans", "nnUNetTrainerV2_DDP.nnUNetTrainerV2_DDP.set_batch_size_and_oversample"], "methods", ["home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.BraTS_trainer.nnUNetTrainerV2BraTSRegions.nnUNetTrainerV2BraTSRegions_DA3_BN.process_plans", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.network_training.nnUNetTrainerV2_DDP.nnUNetTrainerV2_DDP.set_batch_size_and_oversample"], ["", "", "def", "process_plans", "(", "self", ",", "plans", ")", ":", "\n", "        ", "super", "(", ")", ".", "process_plans", "(", "plans", ")", "\n", "self", ".", "set_batch_size_and_oversample", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.network_training.nnUNetTrainerV2_DDP.nnUNetTrainerV2_DDP.initialize": [[129, 215], ["batchgenerators.utilities.file_and_folder_operations.maybe_mkdir_p", "nnUNetTrainerV2_DDP.nnUNetTrainerV2_DDP.process_plans", "nnUNetTrainerV2_DDP.nnUNetTrainerV2_DDP.setup_DA_params", "batchgenerators.utilities.file_and_folder_operations.join", "nnUNetTrainerV2_DDP.nnUNetTrainerV2_DDP.initialize_network", "nnUNetTrainerV2_DDP.nnUNetTrainerV2_DDP.initialize_optimizer_and_scheduler", "torch.nn.parallel.DistributedDataParallel", "torch.nn.parallel.DistributedDataParallel", "nnUNetTrainerV2_DDP.nnUNetTrainerV2_DDP.print_to_log_file", "nnUNetTrainerV2_DDP.nnUNetTrainerV2_DDP.load_plans_file", "nnUNetTrainerV2_DDP.nnUNetTrainerV2_DDP.get_basic_generators", "len", "numpy.array", "numpy.array", "numpy.random.random_integers", "numpy.random.random_integers", "print", "print", "nnunet.training.data_augmentation.default_data_augmentation.get_moreDA_augmentation", "nnUNetTrainerV2_DDP.nnUNetTrainerV2_DDP.print_to_log_file", "nnUNetTrainerV2_DDP.nnUNetTrainerV2_DDP.print_to_log_file", "print", "numpy.array.sum", "nnUNetTrainerV2_DDP.nnUNetTrainerV2_DDP.data_aug_params.get", "max", "print", "nnunet.training.dataloading.dataset_loading.unpack_dataset", "print", "batchgenerators.utilities.file_and_folder_operations.subfiles", "all", "str", "str", "print", "time.sleep", "all", "range", "range", "nnUNetTrainerV2_DDP.nnUNetTrainerV2_DDP.data_aug_params.get", "nnUNetTrainerV2_DDP.nnUNetTrainerV2_DDP.dataset_tr.keys", "nnUNetTrainerV2_DDP.nnUNetTrainerV2_DDP.dataset_val.keys", "batchgenerators.utilities.file_and_folder_operations.isfile", "batchgenerators.utilities.file_and_folder_operations.join", "batchgenerators.utilities.file_and_folder_operations.isfile", "batchgenerators.utilities.file_and_folder_operations.join"], "methods", ["home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.BraTS_trainer.nnUNetTrainerV2BraTSRegions.nnUNetTrainerV2BraTSRegions_DA3_BN.process_plans", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.BraTS_trainer.nnUNetTrainerV2BraTSRegions.nnUNetTrainerV2BraTSRegions_DA4_BN.setup_DA_params", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.BraTS_trainer.nnUNetTrainerV2BraTSRegions.nnUNetTrainerV2BraTSRegions_DA3.initialize_network", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.optimizer_and_lr.nnUNetTrainerV2_momentum09in2D.nnUNetTrainerV2_momentum09in2D.initialize_optimizer_and_scheduler", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.network_training.nnUNetTrainerV2_DDP.nnUNetTrainerV2_DDP.print_to_log_file", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.network_training.nnUNetTrainer.nnUNetTrainer.load_plans_file", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.nnUNet_variants.nnUNetTrainerNoDA.nnUNetTrainerNoDA.get_basic_generators", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.data_augmentation.default_data_augmentation.get_moreDA_augmentation", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.network_training.nnUNetTrainerV2_DDP.nnUNetTrainerV2_DDP.print_to_log_file", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.network_training.nnUNetTrainerV2_DDP.nnUNetTrainerV2_DDP.print_to_log_file", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.dataloading.dataset_loading.unpack_dataset"], ["", "def", "initialize", "(", "self", ",", "training", "=", "True", ",", "force_load_plans", "=", "False", ")", ":", "\n", "        ", "\"\"\"\n        For prediction of test cases just set training=False, this will prevent loading of training data and\n        training batchgenerator initialization\n        :param training:\n        :return:\n        \"\"\"", "\n", "if", "not", "self", ".", "was_initialized", ":", "\n", "            ", "maybe_mkdir_p", "(", "self", ".", "output_folder", ")", "\n", "\n", "if", "force_load_plans", "or", "(", "self", ".", "plans", "is", "None", ")", ":", "\n", "                ", "self", ".", "load_plans_file", "(", ")", "\n", "\n", "", "self", ".", "process_plans", "(", "self", ".", "plans", ")", "\n", "\n", "self", ".", "setup_DA_params", "(", ")", "\n", "\n", "self", ".", "folder_with_preprocessed_data", "=", "join", "(", "self", ".", "dataset_directory", ",", "self", ".", "plans", "[", "'data_identifier'", "]", "+", "\n", "\"_stage%d\"", "%", "self", ".", "stage", ")", "\n", "if", "training", ":", "\n", "                ", "self", ".", "dl_tr", ",", "self", ".", "dl_val", "=", "self", ".", "get_basic_generators", "(", ")", "\n", "if", "self", ".", "unpack_data", ":", "\n", "                    ", "if", "self", ".", "local_rank", "==", "0", ":", "\n", "                        ", "print", "(", "\"unpacking dataset\"", ")", "\n", "unpack_dataset", "(", "self", ".", "folder_with_preprocessed_data", ")", "\n", "print", "(", "\"done\"", ")", "\n", "", "else", ":", "\n", "# we need to wait until worker 0 has finished unpacking", "\n", "                        ", "npz_files", "=", "subfiles", "(", "self", ".", "folder_with_preprocessed_data", ",", "suffix", "=", "\".npz\"", ",", "join", "=", "False", ")", "\n", "case_ids", "=", "[", "i", "[", ":", "-", "4", "]", "for", "i", "in", "npz_files", "]", "\n", "all_present", "=", "all", "(", "\n", "[", "isfile", "(", "join", "(", "self", ".", "folder_with_preprocessed_data", ",", "i", "+", "\".npy\"", ")", ")", "for", "i", "in", "case_ids", "]", ")", "\n", "while", "not", "all_present", ":", "\n", "                            ", "print", "(", "\"worker\"", ",", "self", ".", "local_rank", ",", "\"is waiting for unpacking\"", ")", "\n", "sleep", "(", "3", ")", "\n", "all_present", "=", "all", "(", "\n", "[", "isfile", "(", "join", "(", "self", ".", "folder_with_preprocessed_data", ",", "i", "+", "\".npy\"", ")", ")", "for", "i", "in", "case_ids", "]", ")", "\n", "# there is some slight chance that there may arise some error because dataloader are loading a file", "\n", "# that is still being written by worker 0. We ignore this for now an address it only if it becomes", "\n", "# relevant", "\n", "# (this can occur because while worker 0 writes the file is technically present so the other workers", "\n", "# will proceed and eventually try to read it)", "\n", "", "", "", "else", ":", "\n", "                    ", "print", "(", "\n", "\"INFO: Not unpacking data! Training may be slow due to that. Pray you are not using 2d or you \"", "\n", "\"will wait all winter for your model to finish!\"", ")", "\n", "\n", "# setting weights for deep supervision losses", "\n", "", "net_numpool", "=", "len", "(", "self", ".", "net_num_pool_op_kernel_sizes", ")", "\n", "\n", "# we give each output a weight which decreases exponentially (division by 2) as the resolution decreases", "\n", "# this gives higher resolution outputs more weight in the loss", "\n", "weights", "=", "np", ".", "array", "(", "[", "1", "/", "(", "2", "**", "i", ")", "for", "i", "in", "range", "(", "net_numpool", ")", "]", ")", "\n", "\n", "# we don't use the lowest 2 outputs. Normalize weights so that they sum to 1", "\n", "mask", "=", "np", ".", "array", "(", "[", "True", "if", "i", "<", "net_numpool", "-", "1", "else", "False", "for", "i", "in", "range", "(", "net_numpool", ")", "]", ")", "\n", "weights", "[", "~", "mask", "]", "=", "0", "\n", "weights", "=", "weights", "/", "weights", ".", "sum", "(", ")", "\n", "self", ".", "ds_loss_weights", "=", "weights", "\n", "\n", "seeds_train", "=", "np", ".", "random", ".", "random_integers", "(", "0", ",", "99999", ",", "self", ".", "data_aug_params", ".", "get", "(", "'num_threads'", ")", ")", "\n", "seeds_val", "=", "np", ".", "random", ".", "random_integers", "(", "0", ",", "99999", ",", "max", "(", "self", ".", "data_aug_params", ".", "get", "(", "'num_threads'", ")", "//", "2", ",", "1", ")", ")", "\n", "print", "(", "\"seeds train\"", ",", "seeds_train", ")", "\n", "print", "(", "\"seeds_val\"", ",", "seeds_val", ")", "\n", "self", ".", "tr_gen", ",", "self", ".", "val_gen", "=", "get_moreDA_augmentation", "(", "self", ".", "dl_tr", ",", "self", ".", "dl_val", ",", "\n", "self", ".", "data_aug_params", "[", "\n", "'patch_size_for_spatialtransform'", "]", ",", "\n", "self", ".", "data_aug_params", ",", "\n", "deep_supervision_scales", "=", "self", ".", "deep_supervision_scales", ",", "\n", "seeds_train", "=", "seeds_train", ",", "\n", "seeds_val", "=", "seeds_val", ",", "\n", "pin_memory", "=", "self", ".", "pin_memory", ")", "\n", "self", ".", "print_to_log_file", "(", "\"TRAINING KEYS:\\n %s\"", "%", "(", "str", "(", "self", ".", "dataset_tr", ".", "keys", "(", ")", ")", ")", ",", "\n", "also_print_to_console", "=", "False", ")", "\n", "self", ".", "print_to_log_file", "(", "\"VALIDATION KEYS:\\n %s\"", "%", "(", "str", "(", "self", ".", "dataset_val", ".", "keys", "(", ")", ")", ")", ",", "\n", "also_print_to_console", "=", "False", ")", "\n", "", "else", ":", "\n", "                ", "pass", "\n", "\n", "", "self", ".", "initialize_network", "(", ")", "\n", "self", ".", "initialize_optimizer_and_scheduler", "(", ")", "\n", "self", ".", "network", "=", "DDP", "(", "self", ".", "network", ",", "device_ids", "=", "[", "self", ".", "local_rank", "]", ")", "\n", "\n", "", "else", ":", "\n", "            ", "self", ".", "print_to_log_file", "(", "'self.was_initialized is True, not running self.initialize again'", ")", "\n", "", "self", ".", "was_initialized", "=", "True", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.network_training.nnUNetTrainerV2_DDP.nnUNetTrainerV2_DDP.run_iteration": [[216, 258], ["next", "nnunet.utilities.to_torch.maybe_to_torch", "nnunet.utilities.to_torch.maybe_to_torch", "torch.cuda.is_available", "torch.cuda.is_available", "torch.cuda.is_available", "torch.cuda.is_available", "nnUNetTrainerV2_DDP.nnUNetTrainerV2_DDP.optimizer.zero_grad", "nnUNetTrainerV2_DDP.nnUNetTrainerV2_DDP.detach().cpu().numpy", "nnunet.utilities.to_torch.to_cuda", "nnunet.utilities.to_torch.to_cuda", "nnUNetTrainerV2_DDP.nnUNetTrainerV2_DDP.network", "nnUNetTrainerV2_DDP.nnUNetTrainerV2_DDP.compute_loss", "nnUNetTrainerV2_DDP.nnUNetTrainerV2_DDP.run_online_evaluation", "torch.cuda.amp.autocast", "torch.cuda.amp.autocast", "nnUNetTrainerV2_DDP.nnUNetTrainerV2_DDP.network", "nnUNetTrainerV2_DDP.nnUNetTrainerV2_DDP.compute_loss", "nnUNetTrainerV2_DDP.nnUNetTrainerV2_DDP.amp_grad_scaler.scale().backward", "nnUNetTrainerV2_DDP.nnUNetTrainerV2_DDP.amp_grad_scaler.unscale_", "torch.nn.utils.clip_grad_norm_", "torch.nn.utils.clip_grad_norm_", "torch.nn.utils.clip_grad_norm_", "torch.nn.utils.clip_grad_norm_", "nnUNetTrainerV2_DDP.nnUNetTrainerV2_DDP.amp_grad_scaler.step", "nnUNetTrainerV2_DDP.nnUNetTrainerV2_DDP.amp_grad_scaler.update", "nnUNetTrainerV2_DDP.nnUNetTrainerV2_DDP.backward", "torch.nn.utils.clip_grad_norm_", "torch.nn.utils.clip_grad_norm_", "torch.nn.utils.clip_grad_norm_", "torch.nn.utils.clip_grad_norm_", "nnUNetTrainerV2_DDP.nnUNetTrainerV2_DDP.optimizer.step", "nnUNetTrainerV2_DDP.nnUNetTrainerV2_DDP.detach().cpu", "nnUNetTrainerV2_DDP.nnUNetTrainerV2_DDP.network.parameters", "nnUNetTrainerV2_DDP.nnUNetTrainerV2_DDP.network.parameters", "nnUNetTrainerV2_DDP.nnUNetTrainerV2_DDP.amp_grad_scaler.scale", "nnUNetTrainerV2_DDP.nnUNetTrainerV2_DDP.detach"], "methods", ["home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.utilities.to_torch.maybe_to_torch", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.utilities.to_torch.maybe_to_torch", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.utilities.to_torch.to_cuda", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.utilities.to_torch.to_cuda", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.network_training.nnUNetTrainerV2_DDP.nnUNetTrainerV2_DDP.compute_loss", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.BraTS_trainer.nnUNetTrainerV2BraTSRegions.nnUNetTrainerV2BraTSRegions_DA3_BN.run_online_evaluation", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.network_training.nnUNetTrainerV2_DDP.nnUNetTrainerV2_DDP.compute_loss", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.utilities.distributed.awesome_allgather_function.backward", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.optimizer.ranger.Ranger.step", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.utilities.distributed.awesome_allgather_function.backward", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.optimizer.ranger.Ranger.step"], ["", "def", "run_iteration", "(", "self", ",", "data_generator", ",", "do_backprop", "=", "True", ",", "run_online_evaluation", "=", "False", ")", ":", "\n", "        ", "data_dict", "=", "next", "(", "data_generator", ")", "\n", "data", "=", "data_dict", "[", "'data'", "]", "\n", "target", "=", "data_dict", "[", "'target'", "]", "\n", "\n", "data", "=", "maybe_to_torch", "(", "data", ")", "\n", "target", "=", "maybe_to_torch", "(", "target", ")", "\n", "\n", "if", "torch", ".", "cuda", ".", "is_available", "(", ")", ":", "\n", "            ", "data", "=", "to_cuda", "(", "data", ",", "gpu_id", "=", "None", ")", "\n", "target", "=", "to_cuda", "(", "target", ",", "gpu_id", "=", "None", ")", "\n", "\n", "", "self", ".", "optimizer", ".", "zero_grad", "(", ")", "\n", "\n", "if", "self", ".", "fp16", ":", "\n", "            ", "with", "autocast", "(", ")", ":", "\n", "                ", "output", "=", "self", ".", "network", "(", "data", ")", "\n", "del", "data", "\n", "l", "=", "self", ".", "compute_loss", "(", "output", ",", "target", ")", "\n", "\n", "", "if", "do_backprop", ":", "\n", "                ", "self", ".", "amp_grad_scaler", ".", "scale", "(", "l", ")", ".", "backward", "(", ")", "\n", "self", ".", "amp_grad_scaler", ".", "unscale_", "(", "self", ".", "optimizer", ")", "\n", "torch", ".", "nn", ".", "utils", ".", "clip_grad_norm_", "(", "self", ".", "network", ".", "parameters", "(", ")", ",", "12", ")", "\n", "self", ".", "amp_grad_scaler", ".", "step", "(", "self", ".", "optimizer", ")", "\n", "self", ".", "amp_grad_scaler", ".", "update", "(", ")", "\n", "", "", "else", ":", "\n", "            ", "output", "=", "self", ".", "network", "(", "data", ")", "\n", "del", "data", "\n", "l", "=", "self", ".", "compute_loss", "(", "output", ",", "target", ")", "\n", "\n", "if", "do_backprop", ":", "\n", "                ", "l", ".", "backward", "(", ")", "\n", "torch", ".", "nn", ".", "utils", ".", "clip_grad_norm_", "(", "self", ".", "network", ".", "parameters", "(", ")", ",", "12", ")", "\n", "self", ".", "optimizer", ".", "step", "(", ")", "\n", "\n", "", "", "if", "run_online_evaluation", ":", "\n", "            ", "self", ".", "run_online_evaluation", "(", "output", ",", "target", ")", "\n", "\n", "", "del", "target", "\n", "\n", "return", "l", ".", "detach", "(", ")", ".", "cpu", "(", ")", ".", "numpy", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.network_training.nnUNetTrainerV2_DDP.nnUNetTrainerV2_DDP.compute_loss": [[259, 293], ["range", "len", "tuple", "nnunet.utilities.nd_softmax.softmax_helper", "nnunet.training.loss_functions.dice_loss.get_tp_fp_fn_tn", "nnUNetTrainerV2_DDP.nnUNetTrainerV2_DDP.ce_loss", "range", "nnunet.utilities.distributed.awesome_allgather_function.apply", "nnunet.utilities.distributed.awesome_allgather_function.apply", "nominator.sum.sum.sum", "denominator.sum.sum.sum", "[].long", "len", "output[].size"], "methods", ["home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.loss_functions.dice_loss.get_tp_fp_fn_tn"], ["", "def", "compute_loss", "(", "self", ",", "output", ",", "target", ")", ":", "\n", "        ", "total_loss", "=", "None", "\n", "for", "i", "in", "range", "(", "len", "(", "output", ")", ")", ":", "\n", "# Starting here it gets spicy!", "\n", "            ", "axes", "=", "tuple", "(", "range", "(", "2", ",", "len", "(", "output", "[", "i", "]", ".", "size", "(", ")", ")", ")", ")", "\n", "\n", "# network does not do softmax. We need to do softmax for dice", "\n", "output_softmax", "=", "softmax_helper", "(", "output", "[", "i", "]", ")", "\n", "\n", "# get the tp, fp and fn terms we need", "\n", "tp", ",", "fp", ",", "fn", ",", "_", "=", "get_tp_fp_fn_tn", "(", "output_softmax", ",", "target", "[", "i", "]", ",", "axes", ",", "mask", "=", "None", ")", "\n", "# for dice, compute nominator and denominator so that we have to accumulate only 2 instead of 3 variables", "\n", "# do_bg=False in nnUNetTrainer -> [:, 1:]", "\n", "nominator", "=", "2", "*", "tp", "[", ":", ",", "1", ":", "]", "\n", "denominator", "=", "2", "*", "tp", "[", ":", ",", "1", ":", "]", "+", "fp", "[", ":", ",", "1", ":", "]", "+", "fn", "[", ":", ",", "1", ":", "]", "\n", "\n", "if", "self", ".", "batch_dice", ":", "\n", "# for DDP we need to gather all nominator and denominator terms from all GPUS to do proper batch dice", "\n", "                ", "nominator", "=", "awesome_allgather_function", ".", "apply", "(", "nominator", ")", "\n", "denominator", "=", "awesome_allgather_function", ".", "apply", "(", "denominator", ")", "\n", "nominator", "=", "nominator", ".", "sum", "(", "0", ")", "\n", "denominator", "=", "denominator", ".", "sum", "(", "0", ")", "\n", "", "else", ":", "\n", "                ", "pass", "\n", "\n", "", "ce_loss", "=", "self", ".", "ce_loss", "(", "output", "[", "i", "]", ",", "target", "[", "i", "]", "[", ":", ",", "0", "]", ".", "long", "(", ")", ")", "\n", "\n", "# we smooth by 1e-5 to penalize false positives if tp is 0", "\n", "dice_loss", "=", "(", "-", "(", "nominator", "+", "1e-5", ")", "/", "(", "denominator", "+", "1e-5", ")", ")", ".", "mean", "(", ")", "\n", "if", "total_loss", "is", "None", ":", "\n", "                ", "total_loss", "=", "self", ".", "ds_loss_weights", "[", "i", "]", "*", "(", "ce_loss", "+", "dice_loss", ")", "\n", "", "else", ":", "\n", "                ", "total_loss", "+=", "self", ".", "ds_loss_weights", "[", "i", "]", "*", "(", "ce_loss", "+", "dice_loss", ")", "\n", "", "", "return", "total_loss", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.network_training.nnUNetTrainerV2_DDP.nnUNetTrainerV2_DDP.run_online_evaluation": [[294, 326], ["nnunet.utilities.distributed.awesome_allgather_function.apply.detach().cpu().numpy().sum", "nnunet.utilities.distributed.awesome_allgather_function.apply.detach().cpu().numpy().sum", "nnunet.utilities.distributed.awesome_allgather_function.apply.detach().cpu().numpy().sum", "nnUNetTrainerV2_DDP.nnUNetTrainerV2_DDP.online_eval_foreground_dc.append", "nnUNetTrainerV2_DDP.nnUNetTrainerV2_DDP.online_eval_tp.append", "nnUNetTrainerV2_DDP.nnUNetTrainerV2_DDP.online_eval_fp.append", "nnUNetTrainerV2_DDP.nnUNetTrainerV2_DDP.online_eval_fn.append", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "output[].argmax", "tuple", "torch.zeros().to", "torch.zeros().to", "torch.zeros().to", "torch.zeros().to", "torch.zeros().to", "torch.zeros().to", "torch.zeros().to", "torch.zeros().to", "torch.zeros().to", "torch.zeros().to", "torch.zeros().to", "torch.zeros().to", "range", "nnunet.utilities.distributed.awesome_allgather_function.apply", "nnunet.utilities.distributed.awesome_allgather_function.apply", "nnunet.utilities.distributed.awesome_allgather_function.apply", "list", "list", "list", "list", "range", "nnunet.utilities.tensor_utilities.sum_tensor", "nnunet.utilities.tensor_utilities.sum_tensor", "nnunet.utilities.tensor_utilities.sum_tensor", "nnunet.utilities.distributed.awesome_allgather_function.apply.sum", "nnunet.utilities.distributed.awesome_allgather_function.apply.sum", "nnunet.utilities.distributed.awesome_allgather_function.apply.sum", "nnunet.utilities.distributed.awesome_allgather_function.apply.detach().cpu().numpy", "nnunet.utilities.distributed.awesome_allgather_function.apply.detach().cpu().numpy", "nnunet.utilities.distributed.awesome_allgather_function.apply.detach().cpu().numpy", "len", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "nnunet.utilities.distributed.awesome_allgather_function.apply.detach().cpu", "nnunet.utilities.distributed.awesome_allgather_function.apply.detach().cpu", "nnunet.utilities.distributed.awesome_allgather_function.apply.detach().cpu", "nnunet.utilities.distributed.awesome_allgather_function.apply.detach", "nnunet.utilities.distributed.awesome_allgather_function.apply.detach", "nnunet.utilities.distributed.awesome_allgather_function.apply.detach"], "methods", ["home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.utilities.tensor_utilities.sum_tensor", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.utilities.tensor_utilities.sum_tensor", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.utilities.tensor_utilities.sum_tensor"], ["", "def", "run_online_evaluation", "(", "self", ",", "output", ",", "target", ")", ":", "\n", "        ", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "            ", "num_classes", "=", "output", "[", "0", "]", ".", "shape", "[", "1", "]", "\n", "output_seg", "=", "output", "[", "0", "]", ".", "argmax", "(", "1", ")", "\n", "target", "=", "target", "[", "0", "]", "[", ":", ",", "0", "]", "\n", "axes", "=", "tuple", "(", "range", "(", "1", ",", "len", "(", "target", ".", "shape", ")", ")", ")", "\n", "tp_hard", "=", "torch", ".", "zeros", "(", "(", "target", ".", "shape", "[", "0", "]", ",", "num_classes", "-", "1", ")", ")", ".", "to", "(", "output_seg", ".", "device", ".", "index", ")", "\n", "fp_hard", "=", "torch", ".", "zeros", "(", "(", "target", ".", "shape", "[", "0", "]", ",", "num_classes", "-", "1", ")", ")", ".", "to", "(", "output_seg", ".", "device", ".", "index", ")", "\n", "fn_hard", "=", "torch", ".", "zeros", "(", "(", "target", ".", "shape", "[", "0", "]", ",", "num_classes", "-", "1", ")", ")", ".", "to", "(", "output_seg", ".", "device", ".", "index", ")", "\n", "for", "c", "in", "range", "(", "1", ",", "num_classes", ")", ":", "\n", "                ", "tp_hard", "[", ":", ",", "c", "-", "1", "]", "=", "sum_tensor", "(", "(", "output_seg", "==", "c", ")", ".", "float", "(", ")", "*", "(", "target", "==", "c", ")", ".", "float", "(", ")", ",", "axes", "=", "axes", ")", "\n", "fp_hard", "[", ":", ",", "c", "-", "1", "]", "=", "sum_tensor", "(", "(", "output_seg", "==", "c", ")", ".", "float", "(", ")", "*", "(", "target", "!=", "c", ")", ".", "float", "(", ")", ",", "axes", "=", "axes", ")", "\n", "fn_hard", "[", ":", ",", "c", "-", "1", "]", "=", "sum_tensor", "(", "(", "output_seg", "!=", "c", ")", ".", "float", "(", ")", "*", "(", "target", "==", "c", ")", ".", "float", "(", ")", ",", "axes", "=", "axes", ")", "\n", "\n", "# tp_hard, fp_hard, fn_hard = get_tp_fp_fn((output_softmax > (1 / num_classes)).float(), target,", "\n", "#                                         axes, None)", "\n", "# print_if_rank0(\"before allgather\", tp_hard.shape)", "\n", "", "tp_hard", "=", "tp_hard", ".", "sum", "(", "0", ",", "keepdim", "=", "False", ")", "[", "None", "]", "\n", "fp_hard", "=", "fp_hard", ".", "sum", "(", "0", ",", "keepdim", "=", "False", ")", "[", "None", "]", "\n", "fn_hard", "=", "fn_hard", ".", "sum", "(", "0", ",", "keepdim", "=", "False", ")", "[", "None", "]", "\n", "\n", "tp_hard", "=", "awesome_allgather_function", ".", "apply", "(", "tp_hard", ")", "\n", "fp_hard", "=", "awesome_allgather_function", ".", "apply", "(", "fp_hard", ")", "\n", "fn_hard", "=", "awesome_allgather_function", ".", "apply", "(", "fn_hard", ")", "\n", "\n", "", "tp_hard", "=", "tp_hard", ".", "detach", "(", ")", ".", "cpu", "(", ")", ".", "numpy", "(", ")", ".", "sum", "(", "0", ")", "\n", "fp_hard", "=", "fp_hard", ".", "detach", "(", ")", ".", "cpu", "(", ")", ".", "numpy", "(", ")", ".", "sum", "(", "0", ")", "\n", "fn_hard", "=", "fn_hard", ".", "detach", "(", ")", ".", "cpu", "(", ")", ".", "numpy", "(", ")", ".", "sum", "(", "0", ")", "\n", "self", ".", "online_eval_foreground_dc", ".", "append", "(", "list", "(", "(", "2", "*", "tp_hard", ")", "/", "(", "2", "*", "tp_hard", "+", "fp_hard", "+", "fn_hard", "+", "1e-8", ")", ")", ")", "\n", "self", ".", "online_eval_tp", ".", "append", "(", "list", "(", "tp_hard", ")", ")", "\n", "self", ".", "online_eval_fp", ".", "append", "(", "list", "(", "fp_hard", ")", ")", "\n", "self", ".", "online_eval_fn", ".", "append", "(", "list", "(", "fn_hard", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.network_training.nnUNetTrainerV2_DDP.nnUNetTrainerV2_DDP.run_training": [[327, 346], ["nnUNetTrainerV2_DDP.nnUNetTrainerV2_DDP.maybe_update_lr", "isinstance", "nnunet.training.network_training.nnUNetTrainer.nnUNetTrainer.run_training"], "methods", ["home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.optimizer_and_lr.nnUNetTrainerV2_warmup.nnUNetTrainerV2_warmup.maybe_update_lr", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.architectural_variants.nnUNetTrainerV2_ResencUNet.nnUNetTrainerV2_ResencUNet.run_training"], ["", "def", "run_training", "(", "self", ")", ":", "\n", "        ", "\"\"\"\n        if we run with -c then we need to set the correct lr for the first epoch, otherwise it will run the first\n        continued epoch with self.initial_lr\n\n        we also need to make sure deep supervision in the network is enabled for training, thus the wrapper\n        :return:\n        \"\"\"", "\n", "self", ".", "maybe_update_lr", "(", "self", ".", "epoch", ")", "# if we dont overwrite epoch then self.epoch+1 is used which is not what we", "\n", "# want at the start of the training", "\n", "if", "isinstance", "(", "self", ".", "network", ",", "DDP", ")", ":", "\n", "            ", "net", "=", "self", ".", "network", ".", "module", "\n", "", "else", ":", "\n", "            ", "net", "=", "self", ".", "network", "\n", "", "ds", "=", "net", ".", "do_ds", "\n", "net", ".", "do_ds", "=", "True", "\n", "ret", "=", "nnUNetTrainer", ".", "run_training", "(", "self", ")", "\n", "net", ".", "do_ds", "=", "ds", "\n", "return", "ret", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.network_training.nnUNetTrainerV2_DDP.nnUNetTrainerV2_DDP.validate": [[347, 364], ["isinstance", "nnunet.training.network_training.nnUNetTrainer.nnUNetTrainer.validate"], "methods", ["home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.BraTS_trainer.nnUNetTrainerV2BraTSRegions.nnUNetTrainerV2BraTSRegions_DA3_BN.validate"], ["", "def", "validate", "(", "self", ",", "do_mirroring", ":", "bool", "=", "True", ",", "use_sliding_window", ":", "bool", "=", "True", ",", "\n", "step_size", ":", "float", "=", "0.5", ",", "save_softmax", ":", "bool", "=", "True", ",", "use_gaussian", ":", "bool", "=", "True", ",", "overwrite", ":", "bool", "=", "True", ",", "\n", "validation_folder_name", ":", "str", "=", "'validation_raw'", ",", "debug", ":", "bool", "=", "False", ",", "all_in_gpu", ":", "bool", "=", "False", ",", "\n", "segmentation_export_kwargs", ":", "dict", "=", "None", ")", ":", "\n", "        ", "if", "self", ".", "local_rank", "==", "0", ":", "\n", "            ", "if", "isinstance", "(", "self", ".", "network", ",", "DDP", ")", ":", "\n", "                ", "net", "=", "self", ".", "network", ".", "module", "\n", "", "else", ":", "\n", "                ", "net", "=", "self", ".", "network", "\n", "", "ds", "=", "net", ".", "do_ds", "\n", "net", ".", "do_ds", "=", "False", "\n", "\n", "ret", "=", "nnUNetTrainer", ".", "validate", "(", "self", ",", "do_mirroring", ",", "use_sliding_window", ",", "step_size", ",", "save_softmax", ",", "\n", "use_gaussian", ",", "overwrite", ",", "validation_folder_name", ",", "debug", ",", "all_in_gpu", ",", "\n", "segmentation_export_kwargs", ")", "\n", "net", ".", "do_ds", "=", "ds", "\n", "return", "ret", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.network_training.nnUNetTrainerV2_DDP.nnUNetTrainerV2_DDP.predict_preprocessed_data_return_seg_and_softmax": [[365, 394], ["list", "isinstance", "isinstance", "net.predict_3D", "tuple"], "methods", ["home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.network_architecture.neural_network.SegmentationNetwork.predict_3D"], ["", "", "def", "predict_preprocessed_data_return_seg_and_softmax", "(", "self", ",", "data", ":", "np", ".", "ndarray", ",", "do_mirroring", ":", "bool", "=", "True", ",", "\n", "mirror_axes", ":", "Tuple", "[", "int", "]", "=", "None", ",", "\n", "use_sliding_window", ":", "bool", "=", "True", ",", "step_size", ":", "float", "=", "0.5", ",", "\n", "use_gaussian", ":", "bool", "=", "True", ",", "pad_border_mode", ":", "str", "=", "'constant'", ",", "\n", "pad_kwargs", ":", "dict", "=", "None", ",", "all_in_gpu", ":", "bool", "=", "True", ",", "\n", "verbose", ":", "bool", "=", "True", ",", "mixed_precision", "=", "True", ")", "->", "Tuple", "[", "np", ".", "ndarray", ",", "np", ".", "ndarray", "]", ":", "\n", "        ", "if", "pad_border_mode", "==", "'constant'", "and", "pad_kwargs", "is", "None", ":", "\n", "            ", "pad_kwargs", "=", "{", "'constant_values'", ":", "0", "}", "\n", "\n", "", "if", "do_mirroring", "and", "mirror_axes", "is", "None", ":", "\n", "            ", "mirror_axes", "=", "self", ".", "data_aug_params", "[", "'mirror_axes'", "]", "\n", "\n", "", "if", "do_mirroring", ":", "\n", "            ", "assert", "self", ".", "data_aug_params", "[", "\"do_mirror\"", "]", ",", "\"Cannot do mirroring as test time augmentation when training \"", "\"was done without mirroring\"", "\n", "\n", "", "valid", "=", "list", "(", "(", "SegmentationNetwork", ",", "nn", ".", "DataParallel", ",", "DDP", ")", ")", "\n", "assert", "isinstance", "(", "self", ".", "network", ",", "tuple", "(", "valid", ")", ")", "\n", "if", "isinstance", "(", "self", ".", "network", ",", "DDP", ")", ":", "\n", "            ", "net", "=", "self", ".", "network", ".", "module", "\n", "", "else", ":", "\n", "            ", "net", "=", "self", ".", "network", "\n", "", "ds", "=", "net", ".", "do_ds", "\n", "net", ".", "do_ds", "=", "False", "\n", "ret", "=", "net", ".", "predict_3D", "(", "data", ",", "do_mirroring", ",", "mirror_axes", ",", "use_sliding_window", ",", "step_size", ",", "self", ".", "patch_size", ",", "\n", "self", ".", "regions_class_order", ",", "use_gaussian", ",", "pad_border_mode", ",", "pad_kwargs", ",", "\n", "all_in_gpu", ",", "verbose", ",", "mixed_precision", "=", "mixed_precision", ")", "\n", "net", ".", "do_ds", "=", "ds", "\n", "return", "ret", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.network_training.nnUNetTrainerV2_DDP.nnUNetTrainerV2_DDP.load_checkpoint_ram": [[395, 450], ["collections.OrderedDict", "list", "checkpoint[].items", "nnUNetTrainerV2_DDP.nnUNetTrainerV2_DDP.network.load_state_dict", "nnUNetTrainerV2_DDP.nnUNetTrainerV2_DDP.initialize", "nnUNetTrainerV2_DDP.nnUNetTrainerV2_DDP.network.state_dict().keys", "nnUNetTrainerV2_DDP.nnUNetTrainerV2_DDP._maybe_init_amp", "issubclass", "len", "nnUNetTrainerV2_DDP.nnUNetTrainerV2_DDP.print_to_log_file", "len", "print", "checkpoint.keys", "nnUNetTrainerV2_DDP.nnUNetTrainerV2_DDP.amp_grad_scaler.load_state_dict", "nnUNetTrainerV2_DDP.nnUNetTrainerV2_DDP.optimizer.load_state_dict", "hasattr", "nnUNetTrainerV2_DDP.nnUNetTrainerV2_DDP.lr_scheduler.load_state_dict", "nnUNetTrainerV2_DDP.nnUNetTrainerV2_DDP.lr_scheduler.step", "nnUNetTrainerV2_DDP.nnUNetTrainerV2_DDP.network.state_dict"], "methods", ["home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.BraTS_trainer.nnUNetTrainerV2BraTSRegions.nnUNetTrainerV2BraTSRegions_DA3_BN.initialize", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.network_training.network_trainer.NetworkTrainer._maybe_init_amp", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.network_training.nnUNetTrainerV2_DDP.nnUNetTrainerV2_DDP.print_to_log_file", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.optimizer.ranger.Ranger.step"], ["", "def", "load_checkpoint_ram", "(", "self", ",", "checkpoint", ",", "train", "=", "True", ")", ":", "\n", "        ", "\"\"\"\n        used for if the checkpoint is already in ram\n        :param checkpoint:\n        :param train:\n        :return:\n        \"\"\"", "\n", "if", "not", "self", ".", "was_initialized", ":", "\n", "            ", "self", ".", "initialize", "(", "train", ")", "\n", "\n", "", "new_state_dict", "=", "OrderedDict", "(", ")", "\n", "curr_state_dict_keys", "=", "list", "(", "self", ".", "network", ".", "state_dict", "(", ")", ".", "keys", "(", ")", ")", "\n", "# if state dict comes form nn.DataParallel but we use non-parallel model here then the state dict keys do not", "\n", "# match. Use heuristic to make it match", "\n", "for", "k", ",", "value", "in", "checkpoint", "[", "'state_dict'", "]", ".", "items", "(", ")", ":", "\n", "            ", "key", "=", "k", "\n", "if", "key", "not", "in", "curr_state_dict_keys", ":", "\n", "                ", "print", "(", "\"duh\"", ")", "\n", "key", "=", "key", "[", "7", ":", "]", "\n", "", "new_state_dict", "[", "key", "]", "=", "value", "\n", "\n", "", "if", "self", ".", "fp16", ":", "\n", "            ", "self", ".", "_maybe_init_amp", "(", ")", "\n", "if", "'amp_grad_scaler'", "in", "checkpoint", ".", "keys", "(", ")", ":", "\n", "                ", "self", ".", "amp_grad_scaler", ".", "load_state_dict", "(", "checkpoint", "[", "'amp_grad_scaler'", "]", ")", "\n", "\n", "", "", "self", ".", "network", ".", "load_state_dict", "(", "new_state_dict", ")", "\n", "self", ".", "epoch", "=", "checkpoint", "[", "'epoch'", "]", "\n", "if", "train", ":", "\n", "            ", "optimizer_state_dict", "=", "checkpoint", "[", "'optimizer_state_dict'", "]", "\n", "if", "optimizer_state_dict", "is", "not", "None", ":", "\n", "                ", "self", ".", "optimizer", ".", "load_state_dict", "(", "optimizer_state_dict", ")", "\n", "\n", "", "if", "self", ".", "lr_scheduler", "is", "not", "None", "and", "hasattr", "(", "self", ".", "lr_scheduler", ",", "'load_state_dict'", ")", "and", "checkpoint", "[", "\n", "'lr_scheduler_state_dict'", "]", "is", "not", "None", ":", "\n", "                ", "self", ".", "lr_scheduler", ".", "load_state_dict", "(", "checkpoint", "[", "'lr_scheduler_state_dict'", "]", ")", "\n", "\n", "", "if", "issubclass", "(", "self", ".", "lr_scheduler", ".", "__class__", ",", "_LRScheduler", ")", ":", "\n", "                ", "self", ".", "lr_scheduler", ".", "step", "(", "self", ".", "epoch", ")", "\n", "\n", "", "", "self", ".", "all_tr_losses", ",", "self", ".", "all_val_losses", ",", "self", ".", "all_val_losses_tr_mode", ",", "self", ".", "all_val_eval_metrics", "=", "checkpoint", "[", "\n", "'plot_stuff'", "]", "\n", "\n", "# after the training is done, the epoch is incremented one more time in my old code. This results in", "\n", "# self.epoch = 1001 for old trained models when the epoch is actually 1000. This causes issues because", "\n", "# len(self.all_tr_losses) = 1000 and the plot function will fail. We can easily detect and correct that here", "\n", "if", "self", ".", "epoch", "!=", "len", "(", "self", ".", "all_tr_losses", ")", ":", "\n", "            ", "self", ".", "print_to_log_file", "(", "\"WARNING in loading checkpoint: self.epoch != len(self.all_tr_losses). This is \"", "\n", "\"due to an old bug and should only appear when you are loading old models. New \"", "\n", "\"models should have this fixed! self.epoch is now set to len(self.all_tr_losses)\"", ")", "\n", "self", ".", "epoch", "=", "len", "(", "self", ".", "all_tr_losses", ")", "\n", "self", ".", "all_tr_losses", "=", "self", ".", "all_tr_losses", "[", ":", "self", ".", "epoch", "]", "\n", "self", ".", "all_val_losses", "=", "self", ".", "all_val_losses", "[", ":", "self", ".", "epoch", "]", "\n", "self", ".", "all_val_losses_tr_mode", "=", "self", ".", "all_val_losses_tr_mode", "[", ":", "self", ".", "epoch", "]", "\n", "self", ".", "all_val_eval_metrics", "=", "self", ".", "all_val_eval_metrics", "[", ":", "self", ".", "epoch", "]", "\n", "", "", "", ""]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.network_training.nnUNetTrainerV2_CascadeFullRes.nnUNetTrainerV2CascadeFullRes.__init__": [[41, 59], ["nnunet.training.network_training.nnUNetTrainerV2.nnUNetTrainerV2.__init__", "join", "nnUNetTrainerV2_CascadeFullRes.nnUNetTrainerV2CascadeFullRes.output_folder.split", "[].split", "nnUNetTrainerV2_CascadeFullRes.nnUNetTrainerV2CascadeFullRes.output_folder.split"], "methods", ["home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.common.layers.ResizeImage.__init__"], ["    ", "def", "__init__", "(", "self", ",", "plans_file", ",", "fold", ",", "output_folder", "=", "None", ",", "dataset_directory", "=", "None", ",", "batch_dice", "=", "True", ",", "stage", "=", "None", ",", "\n", "unpack_data", "=", "True", ",", "deterministic", "=", "True", ",", "previous_trainer", "=", "\"nnUNetTrainerV2\"", ",", "fp16", "=", "False", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "plans_file", ",", "fold", ",", "output_folder", ",", "dataset_directory", ",", "\n", "batch_dice", ",", "stage", ",", "unpack_data", ",", "deterministic", ",", "fp16", ")", "\n", "self", ".", "init_args", "=", "(", "plans_file", ",", "fold", ",", "output_folder", ",", "dataset_directory", ",", "batch_dice", ",", "stage", ",", "unpack_data", ",", "\n", "deterministic", ",", "previous_trainer", ",", "fp16", ")", "\n", "\n", "if", "self", ".", "output_folder", "is", "not", "None", ":", "\n", "            ", "task", "=", "self", ".", "output_folder", ".", "split", "(", "\"/\"", ")", "[", "-", "3", "]", "\n", "plans_identifier", "=", "self", ".", "output_folder", ".", "split", "(", "\"/\"", ")", "[", "-", "2", "]", ".", "split", "(", "\"__\"", ")", "[", "-", "1", "]", "\n", "\n", "folder_with_segs_prev_stage", "=", "join", "(", "network_training_output_dir", ",", "\"3d_lowres\"", ",", "\n", "task", ",", "previous_trainer", "+", "\"__\"", "+", "plans_identifier", ",", "\"pred_next_stage\"", ")", "\n", "self", ".", "folder_with_segs_from_prev_stage", "=", "folder_with_segs_prev_stage", "\n", "# Do not put segs_prev_stage into self.output_folder as we need to unpack them for performance and we", "\n", "# don't want to do that in self.output_folder because that one is located on some network drive.", "\n", "", "else", ":", "\n", "            ", "self", ".", "folder_with_segs_from_prev_stage", "=", "None", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.network_training.nnUNetTrainerV2_CascadeFullRes.nnUNetTrainerV2CascadeFullRes.do_split": [[60, 73], ["super().do_split", "join", "isfile", "join", "join"], "methods", ["home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.network_training.nnUNetTrainerV2.nnUNetTrainerV2.do_split"], ["", "", "def", "do_split", "(", "self", ")", ":", "\n", "        ", "super", "(", ")", ".", "do_split", "(", ")", "\n", "for", "k", "in", "self", ".", "dataset", ":", "\n", "            ", "self", ".", "dataset", "[", "k", "]", "[", "'seg_from_prev_stage_file'", "]", "=", "join", "(", "self", ".", "folder_with_segs_from_prev_stage", ",", "\n", "k", "+", "\"_segFromPrevStage.npz\"", ")", "\n", "assert", "isfile", "(", "self", ".", "dataset", "[", "k", "]", "[", "'seg_from_prev_stage_file'", "]", ")", ",", "\"seg from prev stage missing: %s\"", "%", "(", "self", ".", "dataset", "[", "k", "]", "[", "'seg_from_prev_stage_file'", "]", ")", "\n", "", "for", "k", "in", "self", ".", "dataset_val", ":", "\n", "            ", "self", ".", "dataset_val", "[", "k", "]", "[", "'seg_from_prev_stage_file'", "]", "=", "join", "(", "self", ".", "folder_with_segs_from_prev_stage", ",", "\n", "k", "+", "\"_segFromPrevStage.npz\"", ")", "\n", "", "for", "k", "in", "self", ".", "dataset_tr", ":", "\n", "            ", "self", ".", "dataset_tr", "[", "k", "]", "[", "'seg_from_prev_stage_file'", "]", "=", "join", "(", "self", ".", "folder_with_segs_from_prev_stage", ",", "\n", "k", "+", "\"_segFromPrevStage.npz\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.network_training.nnUNetTrainerV2_CascadeFullRes.nnUNetTrainerV2CascadeFullRes.get_basic_generators": [[74, 89], ["nnUNetTrainerV2_CascadeFullRes.nnUNetTrainerV2CascadeFullRes.load_dataset", "nnUNetTrainerV2_CascadeFullRes.nnUNetTrainerV2CascadeFullRes.do_split", "nnunet.training.dataloading.dataset_loading.DataLoader3D", "nnunet.training.dataloading.dataset_loading.DataLoader3D", "NotImplementedError"], "methods", ["home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.dataloading.dataset_loading.load_dataset", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.network_training.nnUNetTrainerV2.nnUNetTrainerV2.do_split"], ["", "", "def", "get_basic_generators", "(", "self", ")", ":", "\n", "        ", "self", ".", "load_dataset", "(", ")", "\n", "self", ".", "do_split", "(", ")", "\n", "\n", "if", "self", ".", "threeD", ":", "\n", "            ", "dl_tr", "=", "DataLoader3D", "(", "self", ".", "dataset_tr", ",", "self", ".", "basic_generator_patch_size", ",", "self", ".", "patch_size", ",", "self", ".", "batch_size", ",", "\n", "True", ",", "oversample_foreground_percent", "=", "self", ".", "oversample_foreground_percent", ",", "\n", "pad_mode", "=", "\"constant\"", ",", "pad_sides", "=", "self", ".", "pad_all_sides", ")", "\n", "dl_val", "=", "DataLoader3D", "(", "self", ".", "dataset_val", ",", "self", ".", "patch_size", ",", "self", ".", "patch_size", ",", "self", ".", "batch_size", ",", "True", ",", "\n", "oversample_foreground_percent", "=", "self", ".", "oversample_foreground_percent", ",", "\n", "pad_mode", "=", "\"constant\"", ",", "pad_sides", "=", "self", ".", "pad_all_sides", ")", "\n", "", "else", ":", "\n", "            ", "raise", "NotImplementedError", "(", "\"2D has no cascade\"", ")", "\n", "\n", "", "return", "dl_tr", ",", "dl_val", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.network_training.nnUNetTrainerV2_CascadeFullRes.nnUNetTrainerV2CascadeFullRes.process_plans": [[90, 93], ["super().process_plans"], "methods", ["home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.BraTS_trainer.nnUNetTrainerV2BraTSRegions.nnUNetTrainerV2BraTSRegions_DA3_BN.process_plans"], ["", "def", "process_plans", "(", "self", ",", "plans", ")", ":", "\n", "        ", "super", "(", ")", ".", "process_plans", "(", "plans", ")", "\n", "self", ".", "num_input_channels", "+=", "(", "self", ".", "num_classes", "-", "1", ")", "# for seg from prev stage", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.network_training.nnUNetTrainerV2_CascadeFullRes.nnUNetTrainerV2CascadeFullRes.setup_DA_params": [[94, 115], ["super().setup_DA_params", "list", "range"], "methods", ["home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.BraTS_trainer.nnUNetTrainerV2BraTSRegions.nnUNetTrainerV2BraTSRegions_DA4_BN.setup_DA_params"], ["", "def", "setup_DA_params", "(", "self", ")", ":", "\n", "        ", "super", "(", ")", ".", "setup_DA_params", "(", ")", "\n", "\n", "self", ".", "data_aug_params", "[", "\"num_cached_per_thread\"", "]", "=", "2", "\n", "\n", "self", ".", "data_aug_params", "[", "'move_last_seg_chanel_to_data'", "]", "=", "True", "\n", "self", ".", "data_aug_params", "[", "'cascade_do_cascade_augmentations'", "]", "=", "True", "\n", "\n", "self", ".", "data_aug_params", "[", "'cascade_random_binary_transform_p'", "]", "=", "0.4", "\n", "self", ".", "data_aug_params", "[", "'cascade_random_binary_transform_p_per_label'", "]", "=", "1", "\n", "self", ".", "data_aug_params", "[", "'cascade_random_binary_transform_size'", "]", "=", "(", "1", ",", "8", ")", "\n", "\n", "self", ".", "data_aug_params", "[", "'cascade_remove_conn_comp_p'", "]", "=", "0.2", "\n", "self", ".", "data_aug_params", "[", "'cascade_remove_conn_comp_max_size_percent_threshold'", "]", "=", "0.15", "\n", "self", ".", "data_aug_params", "[", "'cascade_remove_conn_comp_fill_with_other_class_p'", "]", "=", "0.0", "\n", "\n", "# we have 2 channels now because the segmentation from the previous stage is stored in 'seg' as well until it", "\n", "# is moved to 'data' at the end", "\n", "self", ".", "data_aug_params", "[", "'selected_seg_channels'", "]", "=", "[", "0", ",", "1", "]", "\n", "# needed for converting the segmentation from the previous stage to one hot", "\n", "self", ".", "data_aug_params", "[", "'all_segmentation_labels'", "]", "=", "list", "(", "range", "(", "1", ",", "self", ".", "num_classes", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.network_training.nnUNetTrainerV2_CascadeFullRes.nnUNetTrainerV2CascadeFullRes.initialize": [[116, 188], ["nnUNetTrainerV2_CascadeFullRes.nnUNetTrainerV2CascadeFullRes.process_plans", "nnUNetTrainerV2_CascadeFullRes.nnUNetTrainerV2CascadeFullRes.setup_DA_params", "len", "numpy.array", "numpy.array", "nnunet.training.loss_functions.deep_supervision.MultipleOutputLoss2", "join", "nnUNetTrainerV2_CascadeFullRes.nnUNetTrainerV2CascadeFullRes.initialize_network", "nnUNetTrainerV2_CascadeFullRes.nnUNetTrainerV2CascadeFullRes.initialize_optimizer_and_scheduler", "isinstance", "nnUNetTrainerV2_CascadeFullRes.nnUNetTrainerV2CascadeFullRes.print_to_log_file", "nnUNetTrainerV2_CascadeFullRes.nnUNetTrainerV2CascadeFullRes.load_plans_file", "numpy.array.sum", "nnUNetTrainerV2_CascadeFullRes.nnUNetTrainerV2CascadeFullRes.get_basic_generators", "nnunet.training.data_augmentation.default_data_augmentation.get_moreDA_augmentation", "nnUNetTrainerV2_CascadeFullRes.nnUNetTrainerV2CascadeFullRes.print_to_log_file", "nnUNetTrainerV2_CascadeFullRes.nnUNetTrainerV2CascadeFullRes.print_to_log_file", "isdir", "RuntimeError", "print", "nnunet.training.dataloading.dataset_loading.unpack_dataset", "print", "print", "range", "range", "str", "str", "nnUNetTrainerV2_CascadeFullRes.nnUNetTrainerV2CascadeFullRes.dataset_tr.keys", "nnUNetTrainerV2_CascadeFullRes.nnUNetTrainerV2CascadeFullRes.dataset_val.keys"], "methods", ["home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.BraTS_trainer.nnUNetTrainerV2BraTSRegions.nnUNetTrainerV2BraTSRegions_DA3_BN.process_plans", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.BraTS_trainer.nnUNetTrainerV2BraTSRegions.nnUNetTrainerV2BraTSRegions_DA4_BN.setup_DA_params", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.BraTS_trainer.nnUNetTrainerV2BraTSRegions.nnUNetTrainerV2BraTSRegions_DA3.initialize_network", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.optimizer_and_lr.nnUNetTrainerV2_momentum09in2D.nnUNetTrainerV2_momentum09in2D.initialize_optimizer_and_scheduler", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.network_training.nnUNetTrainerV2_DDP.nnUNetTrainerV2_DDP.print_to_log_file", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.network_training.nnUNetTrainer.nnUNetTrainer.load_plans_file", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.nnUNet_variants.nnUNetTrainerNoDA.nnUNetTrainerNoDA.get_basic_generators", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.data_augmentation.default_data_augmentation.get_moreDA_augmentation", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.network_training.nnUNetTrainerV2_DDP.nnUNetTrainerV2_DDP.print_to_log_file", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.network_training.nnUNetTrainerV2_DDP.nnUNetTrainerV2_DDP.print_to_log_file", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.dataloading.dataset_loading.unpack_dataset"], ["", "def", "initialize", "(", "self", ",", "training", "=", "True", ",", "force_load_plans", "=", "False", ")", ":", "\n", "        ", "\"\"\"\n        For prediction of test cases just set training=False, this will prevent loading of training data and\n        training batchgenerator initialization\n        :param training:\n        :return:\n        \"\"\"", "\n", "if", "not", "self", ".", "was_initialized", ":", "\n", "            ", "if", "force_load_plans", "or", "(", "self", ".", "plans", "is", "None", ")", ":", "\n", "                ", "self", ".", "load_plans_file", "(", ")", "\n", "\n", "", "self", ".", "process_plans", "(", "self", ".", "plans", ")", "\n", "\n", "self", ".", "setup_DA_params", "(", ")", "\n", "\n", "################# Here we wrap the loss for deep supervision ############", "\n", "# we need to know the number of outputs of the network", "\n", "net_numpool", "=", "len", "(", "self", ".", "net_num_pool_op_kernel_sizes", ")", "\n", "\n", "# we give each output a weight which decreases exponentially (division by 2) as the resolution decreases", "\n", "# this gives higher resolution outputs more weight in the loss", "\n", "weights", "=", "np", ".", "array", "(", "[", "1", "/", "(", "2", "**", "i", ")", "for", "i", "in", "range", "(", "net_numpool", ")", "]", ")", "\n", "\n", "# we don't use the lowest 2 outputs. Normalize weights so that they sum to 1", "\n", "mask", "=", "np", ".", "array", "(", "[", "True", "if", "i", "<", "net_numpool", "-", "1", "else", "False", "for", "i", "in", "range", "(", "net_numpool", ")", "]", ")", "\n", "weights", "[", "~", "mask", "]", "=", "0", "\n", "weights", "=", "weights", "/", "weights", ".", "sum", "(", ")", "\n", "self", ".", "ds_loss_weights", "=", "weights", "\n", "# now wrap the loss", "\n", "self", ".", "loss", "=", "MultipleOutputLoss2", "(", "self", ".", "loss", ",", "self", ".", "ds_loss_weights", ")", "\n", "################# END ###################", "\n", "\n", "self", ".", "folder_with_preprocessed_data", "=", "join", "(", "self", ".", "dataset_directory", ",", "self", ".", "plans", "[", "'data_identifier'", "]", "+", "\n", "\"_stage%d\"", "%", "self", ".", "stage", ")", "\n", "\n", "if", "training", ":", "\n", "                ", "if", "not", "isdir", "(", "self", ".", "folder_with_segs_from_prev_stage", ")", ":", "\n", "                    ", "raise", "RuntimeError", "(", "\n", "\"Cannot run final stage of cascade. Run corresponding 3d_lowres first and predict the \"", "\n", "\"segmentations for the next stage\"", ")", "\n", "\n", "", "self", ".", "dl_tr", ",", "self", ".", "dl_val", "=", "self", ".", "get_basic_generators", "(", ")", "\n", "if", "self", ".", "unpack_data", ":", "\n", "                    ", "print", "(", "\"unpacking dataset\"", ")", "\n", "unpack_dataset", "(", "self", ".", "folder_with_preprocessed_data", ")", "\n", "print", "(", "\"done\"", ")", "\n", "", "else", ":", "\n", "                    ", "print", "(", "\n", "\"INFO: Not unpacking data! Training may be slow due to that. Pray you are not using 2d or you \"", "\n", "\"will wait all winter for your model to finish!\"", ")", "\n", "\n", "", "self", ".", "tr_gen", ",", "self", ".", "val_gen", "=", "get_moreDA_augmentation", "(", "self", ".", "dl_tr", ",", "self", ".", "dl_val", ",", "\n", "self", ".", "data_aug_params", "[", "\n", "'patch_size_for_spatialtransform'", "]", ",", "\n", "self", ".", "data_aug_params", ",", "\n", "deep_supervision_scales", "=", "self", ".", "deep_supervision_scales", ",", "\n", "pin_memory", "=", "self", ".", "pin_memory", ")", "\n", "self", ".", "print_to_log_file", "(", "\"TRAINING KEYS:\\n %s\"", "%", "(", "str", "(", "self", ".", "dataset_tr", ".", "keys", "(", ")", ")", ")", ",", "\n", "also_print_to_console", "=", "False", ")", "\n", "self", ".", "print_to_log_file", "(", "\"VALIDATION KEYS:\\n %s\"", "%", "(", "str", "(", "self", ".", "dataset_val", ".", "keys", "(", ")", ")", ")", ",", "\n", "also_print_to_console", "=", "False", ")", "\n", "", "else", ":", "\n", "                ", "pass", "\n", "\n", "", "self", ".", "initialize_network", "(", ")", "\n", "self", ".", "initialize_optimizer_and_scheduler", "(", ")", "\n", "\n", "assert", "isinstance", "(", "self", ".", "network", ",", "(", "SegmentationNetwork", ",", "nn", ".", "DataParallel", ")", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "print_to_log_file", "(", "'self.was_initialized is True, not running self.initialize again'", ")", "\n", "\n", "", "self", ".", "was_initialized", "=", "True", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.network_training.nnUNetTrainerV2_CascadeFullRes.nnUNetTrainerV2CascadeFullRes.validate": [[189, 349], ["nnUNetTrainerV2_CascadeFullRes.nnUNetTrainerV2CascadeFullRes.network.eval", "join", "maybe_mkdir_p", "save_json", "multiprocessing.pool.Pool", "nnUNetTrainerV2_CascadeFullRes.nnUNetTrainerV2CascadeFullRes.dataset_val.keys", "nnUNetTrainerV2_CascadeFullRes.nnUNetTrainerV2CascadeFullRes.print_to_log_file", "nnUNetTrainerV2_CascadeFullRes.nnUNetTrainerV2CascadeFullRes.print_to_log_file", "nnunet.evaluation.evaluator.aggregate_scores", "nnUNetTrainerV2_CascadeFullRes.nnUNetTrainerV2CascadeFullRes.print_to_log_file", "nnunet.postprocessing.connected_components.determine_postprocessing", "join", "maybe_mkdir_p", "subfiles", "nnUNetTrainerV2_CascadeFullRes.nnUNetTrainerV2CascadeFullRes.network.train", "nnUNetTrainerV2_CascadeFullRes.nnUNetTrainerV2CascadeFullRes.load_dataset", "nnUNetTrainerV2_CascadeFullRes.nnUNetTrainerV2CascadeFullRes.do_split", "join", "load_pickle", "pred_gt_tuples.append", "i.get", "nnUNetTrainerV2_CascadeFullRes.nnUNetTrainerV2CascadeFullRes.dataset_directory.split", "nnUNetTrainerV2_CascadeFullRes.nnUNetTrainerV2CascadeFullRes.plans.keys", "RuntimeError", "print", "numpy.concatenate", "join.transpose", "results.append", "list", "join", "print", "[].split", "isfile", "numpy.load", "nnUNetTrainerV2_CascadeFullRes.nnUNetTrainerV2CascadeFullRes.predict_preprocessed_data_return_seg_and_softmax", "join", "numpy.prod", "numpy.save", "join", "multiprocessing.pool.Pool.starmap_async", "join", "join", "range", "shutil.copy", "join", "isfile", "numpy.load", "nnunet.utilities.one_hot_encoding.to_one_hot", "join", "str", "time.sleep", "join", "join", "range", "join"], "methods", ["home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.network_training.nnUNetTrainerV2_DDP.nnUNetTrainerV2_DDP.print_to_log_file", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.network_training.nnUNetTrainerV2_DDP.nnUNetTrainerV2_DDP.print_to_log_file", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.evaluation.evaluator.aggregate_scores", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.network_training.nnUNetTrainerV2_DDP.nnUNetTrainerV2_DDP.print_to_log_file", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.postprocessing.connected_components.determine_postprocessing", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.dataloading.dataset_loading.load_dataset", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.network_training.nnUNetTrainerV2.nnUNetTrainerV2.do_split", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.architectural_variants.nnUNetTrainerV2_ResencUNet.nnUNetTrainerV2_ResencUNet.predict_preprocessed_data_return_seg_and_softmax", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.utilities.one_hot_encoding.to_one_hot"], ["", "def", "validate", "(", "self", ",", "do_mirroring", ":", "bool", "=", "True", ",", "use_sliding_window", ":", "bool", "=", "True", ",", "step_size", ":", "float", "=", "0.5", ",", "\n", "save_softmax", ":", "bool", "=", "True", ",", "use_gaussian", ":", "bool", "=", "True", ",", "overwrite", ":", "bool", "=", "True", ",", "\n", "validation_folder_name", ":", "str", "=", "'validation_raw'", ",", "debug", ":", "bool", "=", "False", ",", "all_in_gpu", ":", "bool", "=", "False", ",", "\n", "segmentation_export_kwargs", ":", "dict", "=", "None", ")", ":", "\n", "        ", "assert", "self", ".", "was_initialized", ",", "\"must initialize, ideally with checkpoint (or train first)\"", "\n", "\n", "current_mode", "=", "self", ".", "network", ".", "training", "\n", "self", ".", "network", ".", "eval", "(", ")", "\n", "# save whether network is in deep supervision mode or not", "\n", "ds", "=", "self", ".", "network", ".", "do_ds", "\n", "# disable deep supervision", "\n", "self", ".", "network", ".", "do_ds", "=", "False", "\n", "\n", "if", "segmentation_export_kwargs", "is", "None", ":", "\n", "            ", "if", "'segmentation_export_params'", "in", "self", ".", "plans", ".", "keys", "(", ")", ":", "\n", "                ", "force_separate_z", "=", "self", ".", "plans", "[", "'segmentation_export_params'", "]", "[", "'force_separate_z'", "]", "\n", "interpolation_order", "=", "self", ".", "plans", "[", "'segmentation_export_params'", "]", "[", "'interpolation_order'", "]", "\n", "interpolation_order_z", "=", "self", ".", "plans", "[", "'segmentation_export_params'", "]", "[", "'interpolation_order_z'", "]", "\n", "", "else", ":", "\n", "                ", "force_separate_z", "=", "None", "\n", "interpolation_order", "=", "1", "\n", "interpolation_order_z", "=", "0", "\n", "", "", "else", ":", "\n", "            ", "force_separate_z", "=", "segmentation_export_kwargs", "[", "'force_separate_z'", "]", "\n", "interpolation_order", "=", "segmentation_export_kwargs", "[", "'interpolation_order'", "]", "\n", "interpolation_order_z", "=", "segmentation_export_kwargs", "[", "'interpolation_order_z'", "]", "\n", "\n", "", "if", "self", ".", "dataset_val", "is", "None", ":", "\n", "            ", "self", ".", "load_dataset", "(", ")", "\n", "self", ".", "do_split", "(", ")", "\n", "\n", "", "output_folder", "=", "join", "(", "self", ".", "output_folder", ",", "validation_folder_name", ")", "\n", "maybe_mkdir_p", "(", "output_folder", ")", "\n", "# this is for debug purposes", "\n", "my_input_args", "=", "{", "'do_mirroring'", ":", "do_mirroring", ",", "\n", "'use_sliding_window'", ":", "use_sliding_window", ",", "\n", "'step'", ":", "step_size", ",", "\n", "'save_softmax'", ":", "save_softmax", ",", "\n", "'use_gaussian'", ":", "use_gaussian", ",", "\n", "'overwrite'", ":", "overwrite", ",", "\n", "'validation_folder_name'", ":", "validation_folder_name", ",", "\n", "'debug'", ":", "debug", ",", "\n", "'all_in_gpu'", ":", "all_in_gpu", ",", "\n", "'segmentation_export_kwargs'", ":", "segmentation_export_kwargs", ",", "\n", "}", "\n", "save_json", "(", "my_input_args", ",", "join", "(", "output_folder", ",", "\"validation_args.json\"", ")", ")", "\n", "\n", "if", "do_mirroring", ":", "\n", "            ", "if", "not", "self", ".", "data_aug_params", "[", "'do_mirror'", "]", ":", "\n", "                ", "raise", "RuntimeError", "(", "\"We did not train with mirroring so you cannot do inference with mirroring enabled\"", ")", "\n", "", "mirror_axes", "=", "self", ".", "data_aug_params", "[", "'mirror_axes'", "]", "\n", "", "else", ":", "\n", "            ", "mirror_axes", "=", "(", ")", "\n", "\n", "", "pred_gt_tuples", "=", "[", "]", "\n", "\n", "export_pool", "=", "Pool", "(", "default_num_threads", ")", "\n", "results", "=", "[", "]", "\n", "\n", "for", "k", "in", "self", ".", "dataset_val", ".", "keys", "(", ")", ":", "\n", "            ", "properties", "=", "load_pickle", "(", "self", ".", "dataset", "[", "k", "]", "[", "'properties_file'", "]", ")", "\n", "fname", "=", "properties", "[", "'list_of_data_files'", "]", "[", "0", "]", ".", "split", "(", "\"/\"", ")", "[", "-", "1", "]", "[", ":", "-", "12", "]", "\n", "\n", "if", "overwrite", "or", "(", "not", "isfile", "(", "join", "(", "output_folder", ",", "fname", "+", "\".nii.gz\"", ")", ")", ")", "or", "(", "save_softmax", "and", "not", "isfile", "(", "join", "(", "output_folder", ",", "fname", "+", "\".npz\"", ")", ")", ")", ":", "\n", "                ", "data", "=", "np", ".", "load", "(", "self", ".", "dataset", "[", "k", "]", "[", "'data_file'", "]", ")", "[", "'data'", "]", "\n", "\n", "# concat segmentation of previous step", "\n", "seg_from_prev_stage", "=", "np", ".", "load", "(", "join", "(", "self", ".", "folder_with_segs_from_prev_stage", ",", "\n", "k", "+", "\"_segFromPrevStage.npz\"", ")", ")", "[", "'data'", "]", "[", "None", "]", "\n", "\n", "print", "(", "k", ",", "data", ".", "shape", ")", "\n", "data", "[", "-", "1", "]", "[", "data", "[", "-", "1", "]", "==", "-", "1", "]", "=", "0", "\n", "\n", "data_for_net", "=", "np", ".", "concatenate", "(", "(", "data", "[", ":", "-", "1", "]", ",", "to_one_hot", "(", "seg_from_prev_stage", "[", "0", "]", ",", "range", "(", "1", ",", "self", ".", "num_classes", ")", ")", ")", ")", "\n", "\n", "softmax_pred", "=", "self", ".", "predict_preprocessed_data_return_seg_and_softmax", "(", "data_for_net", ",", "do_mirroring", ",", "\n", "mirror_axes", ",", "use_sliding_window", ",", "\n", "step_size", ",", "use_gaussian", ",", "\n", "all_in_gpu", "=", "all_in_gpu", ",", "\n", "mixed_precision", "=", "self", ".", "fp16", ")", "[", "1", "]", "\n", "\n", "softmax_pred", "=", "softmax_pred", ".", "transpose", "(", "[", "0", "]", "+", "[", "i", "+", "1", "for", "i", "in", "self", ".", "transpose_backward", "]", ")", "\n", "\n", "if", "save_softmax", ":", "\n", "                    ", "softmax_fname", "=", "join", "(", "output_folder", ",", "fname", "+", "\".npz\"", ")", "\n", "", "else", ":", "\n", "                    ", "softmax_fname", "=", "None", "\n", "\n", "", "\"\"\"There is a problem with python process communication that prevents us from communicating obejcts \n                larger than 2 GB between processes (basically when the length of the pickle string that will be sent is \n                communicated by the multiprocessing.Pipe object then the placeholder (\\%i I think) does not allow for long \n                enough strings (lol). This could be fixed by changing i to l (for long) but that would require manually \n                patching system python code. We circumvent that problem here by saving softmax_pred to a npy file that will \n                then be read (and finally deleted) by the Process. save_segmentation_nifti_from_softmax can take either \n                filename or np.ndarray and will handle this automatically\"\"\"", "\n", "if", "np", ".", "prod", "(", "softmax_pred", ".", "shape", ")", ">", "(", "2e9", "/", "4", "*", "0.85", ")", ":", "# *0.85 just to be save", "\n", "                    ", "np", ".", "save", "(", "join", "(", "output_folder", ",", "fname", "+", "\".npy\"", ")", ",", "softmax_pred", ")", "\n", "softmax_pred", "=", "join", "(", "output_folder", ",", "fname", "+", "\".npy\"", ")", "\n", "\n", "", "results", ".", "append", "(", "export_pool", ".", "starmap_async", "(", "save_segmentation_nifti_from_softmax", ",", "\n", "(", "(", "softmax_pred", ",", "join", "(", "output_folder", ",", "fname", "+", "\".nii.gz\"", ")", ",", "\n", "properties", ",", "interpolation_order", ",", "None", ",", "None", ",", "None", ",", "\n", "softmax_fname", ",", "None", ",", "force_separate_z", ",", "\n", "interpolation_order_z", ")", ",", "\n", ")", "\n", ")", "\n", ")", "\n", "\n", "", "pred_gt_tuples", ".", "append", "(", "[", "join", "(", "output_folder", ",", "fname", "+", "\".nii.gz\"", ")", ",", "\n", "join", "(", "self", ".", "gt_niftis_folder", ",", "fname", "+", "\".nii.gz\"", ")", "]", ")", "\n", "\n", "", "_", "=", "[", "i", ".", "get", "(", ")", "for", "i", "in", "results", "]", "\n", "self", ".", "print_to_log_file", "(", "\"finished prediction\"", ")", "\n", "\n", "# evaluate raw predictions", "\n", "self", ".", "print_to_log_file", "(", "\"evaluation of raw predictions\"", ")", "\n", "task", "=", "self", ".", "dataset_directory", ".", "split", "(", "\"/\"", ")", "[", "-", "1", "]", "\n", "job_name", "=", "self", ".", "experiment_name", "\n", "_", "=", "aggregate_scores", "(", "pred_gt_tuples", ",", "labels", "=", "list", "(", "range", "(", "self", ".", "num_classes", ")", ")", ",", "\n", "json_output_file", "=", "join", "(", "output_folder", ",", "\"summary.json\"", ")", ",", "\n", "json_name", "=", "job_name", "+", "\" val tiled %s\"", "%", "(", "str", "(", "use_sliding_window", ")", ")", ",", "\n", "json_author", "=", "\"Fabian\"", ",", "\n", "json_task", "=", "task", ",", "num_threads", "=", "default_num_threads", ")", "\n", "\n", "# in the old nnunet we would stop here. Now we add a postprocessing. This postprocessing can remove everything", "\n", "# except the largest connected component for each class. To see if this improves results, we do this for all", "\n", "# classes and then rerun the evaluation. Those classes for which this resulted in an improved dice score will", "\n", "# have this applied during inference as well", "\n", "self", ".", "print_to_log_file", "(", "\"determining postprocessing\"", ")", "\n", "determine_postprocessing", "(", "self", ".", "output_folder", ",", "self", ".", "gt_niftis_folder", ",", "validation_folder_name", ",", "\n", "final_subf_name", "=", "validation_folder_name", "+", "\"_postprocessed\"", ",", "debug", "=", "debug", ")", "\n", "# after this the final predictions for the vlaidation set can be found in validation_folder_name_base + \"_postprocessed\"", "\n", "# They are always in that folder, even if no postprocessing as applied!", "\n", "\n", "# detemining postprocesing on a per-fold basis may be OK for this fold but what if another fold finds another", "\n", "# postprocesing to be better? In this case we need to consolidate. At the time the consolidation is going to be", "\n", "# done we won't know what self.gt_niftis_folder was, so now we copy all the niftis into a separate folder to", "\n", "# be used later", "\n", "gt_nifti_folder", "=", "join", "(", "self", ".", "output_folder_base", ",", "\"gt_niftis\"", ")", "\n", "maybe_mkdir_p", "(", "gt_nifti_folder", ")", "\n", "for", "f", "in", "subfiles", "(", "self", ".", "gt_niftis_folder", ",", "suffix", "=", "\".nii.gz\"", ")", ":", "\n", "            ", "success", "=", "False", "\n", "attempts", "=", "0", "\n", "e", "=", "None", "\n", "while", "not", "success", "and", "attempts", "<", "10", ":", "\n", "                ", "try", ":", "\n", "                    ", "shutil", ".", "copy", "(", "f", ",", "gt_nifti_folder", ")", "\n", "success", "=", "True", "\n", "", "except", "OSError", "as", "e", ":", "\n", "                    ", "attempts", "+=", "1", "\n", "sleep", "(", "1", ")", "\n", "", "", "if", "not", "success", ":", "\n", "                ", "print", "(", "\"Could not copy gt nifti file %s into folder %s\"", "%", "(", "f", ",", "gt_nifti_folder", ")", ")", "\n", "if", "e", "is", "not", "None", ":", "\n", "                    ", "raise", "e", "\n", "\n", "# restore network deep supervision mode", "\n", "", "", "", "self", ".", "network", ".", "train", "(", "current_mode", ")", "\n", "self", ".", "network", ".", "do_ds", "=", "ds", "\n", "", "", ""]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.network_training.nnUNetTrainerV2.nnUNetTrainerV2.__init__": [[44, 54], ["nnunet.training.network_training.nnUNetTrainer.nnUNetTrainer.__init__"], "methods", ["home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.common.layers.ResizeImage.__init__"], ["def", "__init__", "(", "self", ",", "plans_file", ",", "fold", ",", "output_folder", "=", "None", ",", "dataset_directory", "=", "None", ",", "batch_dice", "=", "True", ",", "stage", "=", "None", ",", "\n", "unpack_data", "=", "True", ",", "deterministic", "=", "True", ",", "fp16", "=", "False", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "plans_file", ",", "fold", ",", "output_folder", ",", "dataset_directory", ",", "batch_dice", ",", "stage", ",", "unpack_data", ",", "\n", "deterministic", ",", "fp16", ")", "\n", "self", ".", "max_num_epochs", "=", "1000", "\n", "self", ".", "initial_lr", "=", "1e-2", "\n", "self", ".", "deep_supervision_scales", "=", "None", "\n", "self", ".", "ds_loss_weights", "=", "None", "\n", "\n", "self", ".", "pin_memory", "=", "True", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.network_training.nnUNetTrainerV2.nnUNetTrainerV2.initialize": [[55, 128], ["maybe_mkdir_p", "nnUNetTrainerV2.nnUNetTrainerV2.process_plans", "nnUNetTrainerV2.nnUNetTrainerV2.setup_DA_params", "len", "numpy.array", "numpy.array", "nnunet.training.loss_functions.deep_supervision.MultipleOutputLoss2", "join", "nnUNetTrainerV2.nnUNetTrainerV2.initialize_network", "nnUNetTrainerV2.nnUNetTrainerV2.initialize_optimizer_and_scheduler", "isinstance", "nnUNetTrainerV2.nnUNetTrainerV2.print_to_log_file", "nnUNetTrainerV2.nnUNetTrainerV2.load_plans_file", "numpy.array.sum", "nnUNetTrainerV2.nnUNetTrainerV2.get_basic_generators", "nnunet.training.data_augmentation.default_data_augmentation.get_moreDA_augmentation", "nnUNetTrainerV2.nnUNetTrainerV2.print_to_log_file", "nnUNetTrainerV2.nnUNetTrainerV2.print_to_log_file", "print", "nnunet.training.dataloading.dataset_loading.unpack_dataset", "print", "print", "range", "str", "str", "range", "nnUNetTrainerV2.nnUNetTrainerV2.dataset_tr.keys", "nnUNetTrainerV2.nnUNetTrainerV2.dataset_val.keys"], "methods", ["home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.BraTS_trainer.nnUNetTrainerV2BraTSRegions.nnUNetTrainerV2BraTSRegions_DA3_BN.process_plans", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.BraTS_trainer.nnUNetTrainerV2BraTSRegions.nnUNetTrainerV2BraTSRegions_DA4_BN.setup_DA_params", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.BraTS_trainer.nnUNetTrainerV2BraTSRegions.nnUNetTrainerV2BraTSRegions_DA3.initialize_network", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.optimizer_and_lr.nnUNetTrainerV2_momentum09in2D.nnUNetTrainerV2_momentum09in2D.initialize_optimizer_and_scheduler", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.network_training.nnUNetTrainerV2_DDP.nnUNetTrainerV2_DDP.print_to_log_file", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.network_training.nnUNetTrainer.nnUNetTrainer.load_plans_file", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.nnUNet_variants.nnUNetTrainerNoDA.nnUNetTrainerNoDA.get_basic_generators", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.data_augmentation.default_data_augmentation.get_moreDA_augmentation", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.network_training.nnUNetTrainerV2_DDP.nnUNetTrainerV2_DDP.print_to_log_file", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.network_training.nnUNetTrainerV2_DDP.nnUNetTrainerV2_DDP.print_to_log_file", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.dataloading.dataset_loading.unpack_dataset"], ["", "def", "initialize", "(", "self", ",", "training", "=", "True", ",", "force_load_plans", "=", "False", ")", ":", "\n", "        ", "\"\"\"\n        - replaced get_default_augmentation with get_moreDA_augmentation\n        - enforce to only run this code once\n        - loss function wrapper for deep supervision\n\n        :param training:\n        :param force_load_plans:\n        :return:\n        \"\"\"", "\n", "if", "not", "self", ".", "was_initialized", ":", "\n", "            ", "maybe_mkdir_p", "(", "self", ".", "output_folder", ")", "\n", "\n", "if", "force_load_plans", "or", "(", "self", ".", "plans", "is", "None", ")", ":", "\n", "                ", "self", ".", "load_plans_file", "(", ")", "\n", "\n", "", "self", ".", "process_plans", "(", "self", ".", "plans", ")", "\n", "\n", "self", ".", "setup_DA_params", "(", ")", "\n", "\n", "################# Here we wrap the loss for deep supervision ############", "\n", "# we need to know the number of outputs of the network", "\n", "net_numpool", "=", "len", "(", "self", ".", "net_num_pool_op_kernel_sizes", ")", "\n", "\n", "# we give each output a weight which decreases exponentially (division by 2) as the resolution decreases", "\n", "# this gives higher resolution outputs more weight in the loss", "\n", "weights", "=", "np", ".", "array", "(", "[", "1", "/", "(", "2", "**", "i", ")", "for", "i", "in", "range", "(", "net_numpool", ")", "]", ")", "\n", "\n", "# we don't use the lowest 2 outputs. Normalize weights so that they sum to 1", "\n", "mask", "=", "np", ".", "array", "(", "[", "True", "]", "+", "[", "True", "if", "i", "<", "net_numpool", "-", "1", "else", "False", "for", "i", "in", "range", "(", "1", ",", "net_numpool", ")", "]", ")", "\n", "weights", "[", "~", "mask", "]", "=", "0", "\n", "weights", "=", "weights", "/", "weights", ".", "sum", "(", ")", "\n", "#self.ds_loss_weights = weights", "\n", "self", ".", "ds_loss_weights", "=", "None", "\n", "# now wrap the loss", "\n", "self", ".", "loss", "=", "MultipleOutputLoss2", "(", "self", ".", "loss", ",", "self", ".", "ds_loss_weights", ")", "\n", "################# END ###################", "\n", "\n", "self", ".", "folder_with_preprocessed_data", "=", "join", "(", "self", ".", "dataset_directory", ",", "self", ".", "plans", "[", "'data_identifier'", "]", "+", "\n", "\"_stage%d\"", "%", "self", ".", "stage", ")", "\n", "if", "training", ":", "\n", "                ", "self", ".", "dl_tr", ",", "self", ".", "dl_val", "=", "self", ".", "get_basic_generators", "(", ")", "\n", "if", "self", ".", "unpack_data", ":", "\n", "                    ", "print", "(", "\"unpacking dataset\"", ")", "\n", "unpack_dataset", "(", "self", ".", "folder_with_preprocessed_data", ")", "\n", "print", "(", "\"done\"", ")", "\n", "", "else", ":", "\n", "                    ", "print", "(", "\n", "\"INFO: Not unpacking data! Training may be slow due to that. Pray you are not using 2d or you \"", "\n", "\"will wait all winter for your model to finish!\"", ")", "\n", "\n", "", "self", ".", "tr_gen", ",", "self", ".", "val_gen", "=", "get_moreDA_augmentation", "(", "\n", "self", ".", "dl_tr", ",", "self", ".", "dl_val", ",", "\n", "self", ".", "data_aug_params", "[", "\n", "'patch_size_for_spatialtransform'", "]", ",", "\n", "self", ".", "data_aug_params", ",", "\n", "deep_supervision_scales", "=", "self", ".", "deep_supervision_scales", ",", "\n", "pin_memory", "=", "self", ".", "pin_memory", "\n", ")", "\n", "self", ".", "print_to_log_file", "(", "\"TRAINING KEYS:\\n %s\"", "%", "(", "str", "(", "self", ".", "dataset_tr", ".", "keys", "(", ")", ")", ")", ",", "\n", "also_print_to_console", "=", "False", ")", "\n", "self", ".", "print_to_log_file", "(", "\"VALIDATION KEYS:\\n %s\"", "%", "(", "str", "(", "self", ".", "dataset_val", ".", "keys", "(", ")", ")", ")", ",", "\n", "also_print_to_console", "=", "False", ")", "\n", "", "else", ":", "\n", "                ", "pass", "\n", "\n", "", "self", ".", "initialize_network", "(", ")", "\n", "self", ".", "initialize_optimizer_and_scheduler", "(", ")", "\n", "\n", "assert", "isinstance", "(", "self", ".", "network", ",", "(", "SegmentationNetwork", ",", "nn", ".", "DataParallel", ")", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "print_to_log_file", "(", "'self.was_initialized is True, not running self.initialize again'", ")", "\n", "", "self", ".", "was_initialized", "=", "True", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.network_training.nnUNetTrainerV2.nnUNetTrainerV2.initialize_network": [[129, 162], ["nnunet.network_architecture.generic_UNet.Generic_UNet", "torch.cuda.is_available", "len", "nnunet.network_architecture.initialization.InitWeights_He", "nnUNetTrainerV2.nnUNetTrainerV2.network.cuda"], "methods", ["None"], ["", "def", "initialize_network", "(", "self", ")", ":", "\n", "        ", "\"\"\"\n        - momentum 0.99\n        - SGD instead of Adam\n        - self.lr_scheduler = None because we do poly_lr\n        - deep supervision = True\n        - i am sure I forgot something here\n\n        Known issue: forgot to set neg_slope=0 in InitWeights_He; should not make a difference though\n        :return:\n        \"\"\"", "\n", "if", "self", ".", "threeD", ":", "\n", "            ", "conv_op", "=", "nn", ".", "Conv3d", "\n", "dropout_op", "=", "nn", ".", "Dropout3d", "\n", "norm_op", "=", "nn", ".", "InstanceNorm3d", "\n", "\n", "", "else", ":", "\n", "            ", "conv_op", "=", "nn", ".", "Conv2d", "\n", "dropout_op", "=", "nn", ".", "Dropout2d", "\n", "norm_op", "=", "nn", ".", "InstanceNorm2d", "\n", "", "norm_op_kwargs", "=", "{", "'eps'", ":", "1e-5", ",", "'affine'", ":", "True", "}", "\n", "dropout_op_kwargs", "=", "{", "'p'", ":", "0", ",", "'inplace'", ":", "True", "}", "\n", "net_nonlin", "=", "nn", ".", "LeakyReLU", "\n", "net_nonlin_kwargs", "=", "{", "'negative_slope'", ":", "1e-2", ",", "'inplace'", ":", "True", "}", "\n", "self", ".", "network", "=", "Generic_UNet", "(", "self", ".", "num_input_channels", ",", "self", ".", "base_num_features", ",", "self", ".", "num_classes", ",", "\n", "len", "(", "self", ".", "net_num_pool_op_kernel_sizes", ")", ",", "\n", "self", ".", "conv_per_stage", ",", "2", ",", "conv_op", ",", "norm_op", ",", "norm_op_kwargs", ",", "dropout_op", ",", "\n", "dropout_op_kwargs", ",", "\n", "net_nonlin", ",", "net_nonlin_kwargs", ",", "True", ",", "False", ",", "lambda", "x", ":", "x", ",", "InitWeights_He", "(", "1e-2", ")", ",", "\n", "self", ".", "net_num_pool_op_kernel_sizes", ",", "self", ".", "net_conv_kernel_sizes", ",", "False", ",", "True", ",", "True", ")", "\n", "if", "torch", ".", "cuda", ".", "is_available", "(", ")", ":", "\n", "            ", "self", ".", "network", ".", "cuda", "(", ")", "\n", "", "self", ".", "network", ".", "inference_apply_nonlin", "=", "softmax_helper", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.network_training.nnUNetTrainerV2.nnUNetTrainerV2.initialize_optimizer_and_scheduler": [[163, 170], ["print", "sys.stdout.flush", "torch.optim.SGD", "nnUNetTrainerV2.nnUNetTrainerV2.network.parameters"], "methods", ["None"], ["", "def", "initialize_optimizer_and_scheduler", "(", "self", ")", ":", "\n", "        ", "assert", "self", ".", "network", "is", "not", "None", ",", "\"self.initialize_network must be called first\"", "\n", "print", "(", "'weight_decay: '", ",", "self", ".", "weight_decay", ")", "\n", "sys", ".", "stdout", ".", "flush", "(", ")", "\n", "self", ".", "optimizer", "=", "torch", ".", "optim", ".", "SGD", "(", "self", ".", "network", ".", "parameters", "(", ")", ",", "self", ".", "initial_lr", ",", "weight_decay", "=", "self", ".", "weight_decay", ",", "\n", "momentum", "=", "0.99", ",", "nesterov", "=", "True", ")", "\n", "self", ".", "lr_scheduler", "=", "None", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.network_training.nnUNetTrainerV2.nnUNetTrainerV2.run_online_evaluation": [[171, 182], ["super().run_online_evaluation"], "methods", ["home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.BraTS_trainer.nnUNetTrainerV2BraTSRegions.nnUNetTrainerV2BraTSRegions_DA3_BN.run_online_evaluation"], ["", "def", "run_online_evaluation", "(", "self", ",", "output", ",", "target", ")", ":", "\n", "        ", "\"\"\"\n        due to deep supervision the return value and the reference are now lists of tensors. We only need the full\n        resolution output because this is what we are interested in in the end. The others are ignored\n        :param output:\n        :param target:\n        :return:\n        \"\"\"", "\n", "target", "=", "target", "[", "0", "]", "\n", "output", "=", "output", "[", "-", "1", "]", "\n", "return", "super", "(", ")", ".", "run_online_evaluation", "(", "output", ",", "target", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.network_training.nnUNetTrainerV2.nnUNetTrainerV2.validate": [[183, 197], ["super().validate"], "methods", ["home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.BraTS_trainer.nnUNetTrainerV2BraTSRegions.nnUNetTrainerV2BraTSRegions_DA3_BN.validate"], ["", "def", "validate", "(", "self", ",", "do_mirroring", ":", "bool", "=", "True", ",", "use_sliding_window", ":", "bool", "=", "True", ",", "\n", "step_size", ":", "float", "=", "0.5", ",", "save_softmax", ":", "bool", "=", "True", ",", "use_gaussian", ":", "bool", "=", "True", ",", "overwrite", ":", "bool", "=", "True", ",", "\n", "validation_folder_name", ":", "str", "=", "'validation_raw'", ",", "debug", ":", "bool", "=", "False", ",", "all_in_gpu", ":", "bool", "=", "False", ",", "\n", "segmentation_export_kwargs", ":", "dict", "=", "None", ")", ":", "\n", "        ", "\"\"\"\n        We need to wrap this because we need to enforce self.network.do_ds = False for prediction\n        \"\"\"", "\n", "ds", "=", "self", ".", "network", ".", "do_ds", "\n", "self", ".", "network", ".", "do_ds", "=", "False", "\n", "ret", "=", "super", "(", ")", ".", "validate", "(", "do_mirroring", ",", "use_sliding_window", ",", "step_size", ",", "save_softmax", ",", "use_gaussian", ",", "\n", "overwrite", ",", "validation_folder_name", ",", "debug", ",", "all_in_gpu", ",", "segmentation_export_kwargs", ")", "\n", "\n", "self", ".", "network", ".", "do_ds", "=", "ds", "\n", "return", "ret", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.network_training.nnUNetTrainerV2.nnUNetTrainerV2.predict_preprocessed_data_return_seg_and_softmax": [[198, 215], ["super().predict_preprocessed_data_return_seg_and_softmax"], "methods", ["home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.architectural_variants.nnUNetTrainerV2_ResencUNet.nnUNetTrainerV2_ResencUNet.predict_preprocessed_data_return_seg_and_softmax"], ["", "def", "predict_preprocessed_data_return_seg_and_softmax", "(", "self", ",", "data", ":", "np", ".", "ndarray", ",", "do_mirroring", ":", "bool", "=", "True", ",", "\n", "mirror_axes", ":", "Tuple", "[", "int", "]", "=", "None", ",", "\n", "use_sliding_window", ":", "bool", "=", "True", ",", "step_size", ":", "float", "=", "0.5", ",", "\n", "use_gaussian", ":", "bool", "=", "True", ",", "pad_border_mode", ":", "str", "=", "'constant'", ",", "\n", "pad_kwargs", ":", "dict", "=", "None", ",", "all_in_gpu", ":", "bool", "=", "True", ",", "\n", "verbose", ":", "bool", "=", "True", ",", "mixed_precision", "=", "True", ")", "->", "Tuple", "[", "np", ".", "ndarray", ",", "np", ".", "ndarray", "]", ":", "\n", "        ", "\"\"\"\n        We need to wrap this because we need to enforce self.network.do_ds = False for prediction\n        \"\"\"", "\n", "ds", "=", "self", ".", "network", ".", "do_ds", "\n", "self", ".", "network", ".", "do_ds", "=", "False", "\n", "ret", "=", "super", "(", ")", ".", "predict_preprocessed_data_return_seg_and_softmax", "(", "data", ",", "do_mirroring", ",", "mirror_axes", ",", "\n", "use_sliding_window", ",", "step_size", ",", "use_gaussian", ",", "\n", "pad_border_mode", ",", "pad_kwargs", ",", "all_in_gpu", ",", "verbose", ",", "\n", "mixed_precision", "=", "mixed_precision", ")", "\n", "self", ".", "network", ".", "do_ds", "=", "ds", "\n", "return", "ret", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.network_training.nnUNetTrainerV2.nnUNetTrainerV2.run_iteration": [[216, 266], ["next", "nnunet.utilities.to_torch.maybe_to_torch", "nnunet.utilities.to_torch.maybe_to_torch", "torch.cuda.is_available", "nnUNetTrainerV2.nnUNetTrainerV2.optimizer.zero_grad", "nnUNetTrainerV2.nnUNetTrainerV2.detach().cpu().numpy", "nnunet.utilities.to_torch.to_cuda", "nnunet.utilities.to_torch.to_cuda", "nnUNetTrainerV2.nnUNetTrainerV2.network", "nnUNetTrainerV2.nnUNetTrainerV2.loss", "nnUNetTrainerV2.nnUNetTrainerV2.run_online_evaluation", "torch.cuda.amp.autocast", "nnUNetTrainerV2.nnUNetTrainerV2.network", "nnUNetTrainerV2.nnUNetTrainerV2.loss", "nnUNetTrainerV2.nnUNetTrainerV2.amp_grad_scaler.scale().backward", "nnUNetTrainerV2.nnUNetTrainerV2.amp_grad_scaler.unscale_", "torch.nn.utils.clip_grad_norm_", "nnUNetTrainerV2.nnUNetTrainerV2.amp_grad_scaler.step", "nnUNetTrainerV2.nnUNetTrainerV2.amp_grad_scaler.update", "nnUNetTrainerV2.nnUNetTrainerV2.backward", "torch.nn.utils.clip_grad_norm_", "nnUNetTrainerV2.nnUNetTrainerV2.optimizer.step", "nnUNetTrainerV2.nnUNetTrainerV2.detach().cpu", "nnUNetTrainerV2.nnUNetTrainerV2.network.parameters", "nnUNetTrainerV2.nnUNetTrainerV2.network.parameters", "nnUNetTrainerV2.nnUNetTrainerV2.amp_grad_scaler.scale", "nnUNetTrainerV2.nnUNetTrainerV2.detach"], "methods", ["home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.utilities.to_torch.maybe_to_torch", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.utilities.to_torch.maybe_to_torch", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.utilities.to_torch.to_cuda", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.utilities.to_torch.to_cuda", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.BraTS_trainer.nnUNetTrainerV2BraTSRegions.nnUNetTrainerV2BraTSRegions_DA3_BN.run_online_evaluation", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.utilities.distributed.awesome_allgather_function.backward", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.optimizer.ranger.Ranger.step", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.utilities.distributed.awesome_allgather_function.backward", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.optimizer.ranger.Ranger.step"], ["", "def", "run_iteration", "(", "self", ",", "data_generator", ",", "do_backprop", "=", "True", ",", "run_online_evaluation", "=", "False", ")", ":", "\n", "        ", "\"\"\"\n        gradient clipping improves training stability\n\n        :param data_generator:\n        :param do_backprop:\n        :param run_online_evaluation:\n        :return:\n        \"\"\"", "\n", "data_dict", "=", "next", "(", "data_generator", ")", "\n", "data", "=", "data_dict", "[", "'data'", "]", "\n", "target", "=", "data_dict", "[", "'target'", "]", "\n", "\n", "data", "=", "maybe_to_torch", "(", "data", ")", "\n", "target", "=", "maybe_to_torch", "(", "target", ")", "\n", "\n", "if", "torch", ".", "cuda", ".", "is_available", "(", ")", ":", "\n", "            ", "data", "=", "to_cuda", "(", "data", ")", "\n", "target", "=", "to_cuda", "(", "target", ")", "\n", "\n", "", "self", ".", "optimizer", ".", "zero_grad", "(", ")", "\n", "\n", "if", "self", ".", "fp16", ":", "\n", "            ", "with", "autocast", "(", ")", ":", "\n", "                ", "output", "=", "self", ".", "network", "(", "data", ")", "\n", "del", "data", "\n", "l", "=", "self", ".", "loss", "(", "output", ",", "target", ")", "\n", "\n", "", "if", "do_backprop", ":", "\n", "                ", "self", ".", "amp_grad_scaler", ".", "scale", "(", "l", ")", ".", "backward", "(", ")", "\n", "self", ".", "amp_grad_scaler", ".", "unscale_", "(", "self", ".", "optimizer", ")", "\n", "torch", ".", "nn", ".", "utils", ".", "clip_grad_norm_", "(", "self", ".", "network", ".", "parameters", "(", ")", ",", "12", ")", "\n", "self", ".", "amp_grad_scaler", ".", "step", "(", "self", ".", "optimizer", ")", "\n", "self", ".", "amp_grad_scaler", ".", "update", "(", ")", "\n", "", "", "else", ":", "\n", "            ", "output", "=", "self", ".", "network", "(", "data", ")", "\n", "del", "data", "\n", "l", "=", "self", ".", "loss", "(", "output", ",", "target", ")", "\n", "\n", "if", "do_backprop", ":", "\n", "                ", "l", ".", "backward", "(", ")", "\n", "torch", ".", "nn", ".", "utils", ".", "clip_grad_norm_", "(", "self", ".", "network", ".", "parameters", "(", ")", ",", "12", ")", "\n", "self", ".", "optimizer", ".", "step", "(", ")", "\n", "\n", "", "", "if", "run_online_evaluation", ":", "\n", "            ", "self", ".", "run_online_evaluation", "(", "output", ",", "target", ")", "\n", "\n", "", "del", "target", "\n", "\n", "return", "l", ".", "detach", "(", ")", ".", "cpu", "(", ")", ".", "numpy", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.network_training.nnUNetTrainerV2.nnUNetTrainerV2.do_split": [[267, 318], ["list.sort", "list.sort", "collections.OrderedDict", "collections.OrderedDict", "list", "join", "load_pickle", "nnUNetTrainerV2.nnUNetTrainerV2.dataset.keys", "isfile", "nnUNetTrainerV2.nnUNetTrainerV2.print_to_log_file", "numpy.sort", "sklearn.model_selection.KFold", "enumerate", "save_pickle", "len", "nnUNetTrainerV2.nnUNetTrainerV2.print_to_log_file", "numpy.random.RandomState", "numpy.sort", "numpy.random.RandomState.choice", "list", "sklearn.model_selection.KFold.split", "load_pickle.append", "list", "len", "int", "nnUNetTrainerV2.nnUNetTrainerV2.dataset.keys", "numpy.array", "numpy.array", "collections.OrderedDict", "nnUNetTrainerV2.nnUNetTrainerV2.dataset.keys", "range", "len", "len", "len"], "methods", ["home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.network_training.nnUNetTrainerV2_DDP.nnUNetTrainerV2_DDP.print_to_log_file", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.network_training.nnUNetTrainerV2_DDP.nnUNetTrainerV2_DDP.print_to_log_file"], ["", "def", "do_split", "(", "self", ")", ":", "\n", "        ", "\"\"\"\n        we now allow more than 5 splits. IMPORTANT: and fold > 4 will not be a real split but just another random\n        80:20 split of the data. You cannot run X-fold cross-validation with this code. It will always be a 5-fold CV.\n        Folds > 4 will be independent from each other\n        :return:\n        \"\"\"", "\n", "if", "self", ".", "fold", "==", "\"all\"", ":", "\n", "# if fold==all then we use all images for training and validation", "\n", "            ", "tr_keys", "=", "val_keys", "=", "list", "(", "self", ".", "dataset", ".", "keys", "(", ")", ")", "\n", "", "else", ":", "\n", "            ", "splits_file", "=", "join", "(", "self", ".", "dataset_directory", ",", "\"splits_final.pkl\"", ")", "\n", "\n", "# if the split file does not exist we need to create it", "\n", "if", "not", "isfile", "(", "splits_file", ")", ":", "\n", "                ", "self", ".", "print_to_log_file", "(", "\"Creating new split...\"", ")", "\n", "splits", "=", "[", "]", "\n", "all_keys_sorted", "=", "np", ".", "sort", "(", "list", "(", "self", ".", "dataset", ".", "keys", "(", ")", ")", ")", "\n", "kfold", "=", "KFold", "(", "n_splits", "=", "5", ",", "shuffle", "=", "True", ",", "random_state", "=", "12345", ")", "\n", "for", "i", ",", "(", "train_idx", ",", "test_idx", ")", "in", "enumerate", "(", "kfold", ".", "split", "(", "all_keys_sorted", ")", ")", ":", "\n", "                    ", "train_keys", "=", "np", ".", "array", "(", "all_keys_sorted", ")", "[", "train_idx", "]", "\n", "test_keys", "=", "np", ".", "array", "(", "all_keys_sorted", ")", "[", "test_idx", "]", "\n", "splits", ".", "append", "(", "OrderedDict", "(", ")", ")", "\n", "splits", "[", "-", "1", "]", "[", "'train'", "]", "=", "train_keys", "\n", "splits", "[", "-", "1", "]", "[", "'val'", "]", "=", "test_keys", "\n", "", "save_pickle", "(", "splits", ",", "splits_file", ")", "\n", "\n", "", "splits", "=", "load_pickle", "(", "splits_file", ")", "\n", "\n", "if", "self", ".", "fold", "<", "len", "(", "splits", ")", ":", "\n", "                ", "tr_keys", "=", "splits", "[", "self", ".", "fold", "]", "[", "'train'", "]", "\n", "val_keys", "=", "splits", "[", "self", ".", "fold", "]", "[", "'val'", "]", "\n", "", "else", ":", "\n", "                ", "self", ".", "print_to_log_file", "(", "\"INFO: Requested fold %d but split file only has %d folds. I am now creating a \"", "\n", "\"random 80:20 split!\"", "%", "(", "self", ".", "fold", ",", "len", "(", "splits", ")", ")", ")", "\n", "# if we request a fold that is not in the split file, create a random 80:20 split", "\n", "rnd", "=", "np", ".", "random", ".", "RandomState", "(", "seed", "=", "12345", "+", "self", ".", "fold", ")", "\n", "keys", "=", "np", ".", "sort", "(", "list", "(", "self", ".", "dataset", ".", "keys", "(", ")", ")", ")", "\n", "idx_tr", "=", "rnd", ".", "choice", "(", "len", "(", "keys", ")", ",", "int", "(", "len", "(", "keys", ")", "*", "0.8", ")", ",", "replace", "=", "False", ")", "\n", "idx_val", "=", "[", "i", "for", "i", "in", "range", "(", "len", "(", "keys", ")", ")", "if", "i", "not", "in", "idx_tr", "]", "\n", "tr_keys", "=", "[", "keys", "[", "i", "]", "for", "i", "in", "idx_tr", "]", "\n", "val_keys", "=", "[", "keys", "[", "i", "]", "for", "i", "in", "idx_val", "]", "\n", "\n", "", "", "tr_keys", ".", "sort", "(", ")", "\n", "val_keys", ".", "sort", "(", ")", "\n", "self", ".", "dataset_tr", "=", "OrderedDict", "(", ")", "\n", "for", "i", "in", "tr_keys", ":", "\n", "            ", "self", ".", "dataset_tr", "[", "i", "]", "=", "self", ".", "dataset", "[", "i", "]", "\n", "", "self", ".", "dataset_val", "=", "OrderedDict", "(", ")", "\n", "for", "i", "in", "val_keys", ":", "\n", "            ", "self", ".", "dataset_val", "[", "i", "]", "=", "self", ".", "dataset", "[", "i", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.network_training.nnUNetTrainerV2.nnUNetTrainerV2.setup_DA_params": [[319, 372], ["nnunet.training.data_augmentation.default_data_augmentation.get_patch_size", "numpy.array", "nnunet.training.data_augmentation.default_data_augmentation.get_patch_size", "list", "nnUNetTrainerV2.nnUNetTrainerV2.print_to_log_file", "max", "min", "list", "list", "numpy.cumprod", "numpy.vstack"], "methods", ["home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.data_augmentation.default_data_augmentation.get_patch_size", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.data_augmentation.default_data_augmentation.get_patch_size", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.network_training.nnUNetTrainerV2_DDP.nnUNetTrainerV2_DDP.print_to_log_file"], ["", "", "def", "setup_DA_params", "(", "self", ")", ":", "\n", "        ", "\"\"\"\n        - we increase roation angle from [-15, 15] to [-30, 30]\n        - scale range is now (0.7, 1.4), was (0.85, 1.25)\n        - we don't do elastic deformation anymore\n\n        :return:\n        \"\"\"", "\n", "\n", "self", ".", "deep_supervision_scales", "=", "[", "[", "1", ",", "1", ",", "1", "]", "]", "+", "list", "(", "list", "(", "i", ")", "for", "i", "in", "1", "/", "np", ".", "cumprod", "(", "\n", "np", ".", "vstack", "(", "self", ".", "net_num_pool_op_kernel_sizes", ")", ",", "axis", "=", "0", ")", ")", "[", ":", "-", "1", "]", "\n", "\n", "if", "self", ".", "threeD", ":", "\n", "            ", "self", ".", "data_aug_params", "=", "default_3D_augmentation_params", "\n", "self", ".", "data_aug_params", "[", "'rotation_x'", "]", "=", "(", "-", "30.", "/", "360", "*", "2.", "*", "np", ".", "pi", ",", "30.", "/", "360", "*", "2.", "*", "np", ".", "pi", ")", "\n", "self", ".", "data_aug_params", "[", "'rotation_y'", "]", "=", "(", "-", "30.", "/", "360", "*", "2.", "*", "np", ".", "pi", ",", "30.", "/", "360", "*", "2.", "*", "np", ".", "pi", ")", "\n", "self", ".", "data_aug_params", "[", "'rotation_z'", "]", "=", "(", "-", "30.", "/", "360", "*", "2.", "*", "np", ".", "pi", ",", "30.", "/", "360", "*", "2.", "*", "np", ".", "pi", ")", "\n", "if", "self", ".", "do_dummy_2D_aug", ":", "\n", "                ", "self", ".", "data_aug_params", "[", "\"dummy_2D\"", "]", "=", "True", "\n", "self", ".", "print_to_log_file", "(", "\"Using dummy2d data augmentation\"", ")", "\n", "self", ".", "data_aug_params", "[", "\"elastic_deform_alpha\"", "]", "=", "default_2D_augmentation_params", "[", "\"elastic_deform_alpha\"", "]", "\n", "self", ".", "data_aug_params", "[", "\"elastic_deform_sigma\"", "]", "=", "default_2D_augmentation_params", "[", "\"elastic_deform_sigma\"", "]", "\n", "self", ".", "data_aug_params", "[", "\"rotation_x\"", "]", "=", "default_2D_augmentation_params", "[", "\"rotation_x\"", "]", "\n", "", "", "else", ":", "\n", "            ", "self", ".", "do_dummy_2D_aug", "=", "False", "\n", "if", "max", "(", "self", ".", "patch_size", ")", "/", "min", "(", "self", ".", "patch_size", ")", ">", "1.5", ":", "\n", "                ", "default_2D_augmentation_params", "[", "'rotation_x'", "]", "=", "(", "-", "15.", "/", "360", "*", "2.", "*", "np", ".", "pi", ",", "15.", "/", "360", "*", "2.", "*", "np", ".", "pi", ")", "\n", "", "self", ".", "data_aug_params", "=", "default_2D_augmentation_params", "\n", "", "self", ".", "data_aug_params", "[", "\"mask_was_used_for_normalization\"", "]", "=", "self", ".", "use_mask_for_norm", "\n", "\n", "if", "self", ".", "do_dummy_2D_aug", ":", "\n", "            ", "self", ".", "basic_generator_patch_size", "=", "get_patch_size", "(", "self", ".", "patch_size", "[", "1", ":", "]", ",", "\n", "self", ".", "data_aug_params", "[", "'rotation_x'", "]", ",", "\n", "self", ".", "data_aug_params", "[", "'rotation_y'", "]", ",", "\n", "self", ".", "data_aug_params", "[", "'rotation_z'", "]", ",", "\n", "self", ".", "data_aug_params", "[", "'scale_range'", "]", ")", "\n", "self", ".", "basic_generator_patch_size", "=", "np", ".", "array", "(", "[", "self", ".", "patch_size", "[", "0", "]", "]", "+", "list", "(", "self", ".", "basic_generator_patch_size", ")", ")", "\n", "patch_size_for_spatialtransform", "=", "self", ".", "patch_size", "[", "1", ":", "]", "\n", "", "else", ":", "\n", "            ", "self", ".", "basic_generator_patch_size", "=", "get_patch_size", "(", "self", ".", "patch_size", ",", "self", ".", "data_aug_params", "[", "'rotation_x'", "]", ",", "\n", "self", ".", "data_aug_params", "[", "'rotation_y'", "]", ",", "\n", "self", ".", "data_aug_params", "[", "'rotation_z'", "]", ",", "\n", "self", ".", "data_aug_params", "[", "'scale_range'", "]", ")", "\n", "patch_size_for_spatialtransform", "=", "self", ".", "patch_size", "\n", "\n", "", "self", ".", "data_aug_params", "[", "\"scale_range\"", "]", "=", "(", "0.7", ",", "1.4", ")", "\n", "self", ".", "data_aug_params", "[", "\"do_elastic\"", "]", "=", "False", "\n", "self", ".", "data_aug_params", "[", "'selected_seg_channels'", "]", "=", "[", "0", "]", "\n", "self", ".", "data_aug_params", "[", "'patch_size_for_spatialtransform'", "]", "=", "patch_size_for_spatialtransform", "\n", "\n", "self", ".", "data_aug_params", "[", "\"num_cached_per_thread\"", "]", "=", "2", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.network_training.nnUNetTrainerV2.nnUNetTrainerV2.maybe_update_lr": [[373, 389], ["nnunet.training.learning_rate.poly_lr.poly_lr", "nnUNetTrainerV2.nnUNetTrainerV2.print_to_log_file", "numpy.round"], "methods", ["home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.learning_rate.poly_lr.poly_lr", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.network_training.nnUNetTrainerV2_DDP.nnUNetTrainerV2_DDP.print_to_log_file"], ["", "def", "maybe_update_lr", "(", "self", ",", "epoch", "=", "None", ")", ":", "\n", "        ", "\"\"\"\n        if epoch is not None we overwrite epoch. Else we use epoch = self.epoch + 1\n\n        (maybe_update_lr is called in on_epoch_end which is called before epoch is incremented.\n        herefore we need to do +1 here)\n\n        :param epoch:\n        :return:\n        \"\"\"", "\n", "if", "epoch", "is", "None", ":", "\n", "            ", "ep", "=", "self", ".", "epoch", "+", "1", "\n", "", "else", ":", "\n", "            ", "ep", "=", "epoch", "\n", "", "self", ".", "optimizer", ".", "param_groups", "[", "0", "]", "[", "'lr'", "]", "=", "poly_lr", "(", "ep", ",", "self", ".", "max_num_epochs", ",", "self", ".", "initial_lr", ",", "0.9", ")", "\n", "self", ".", "print_to_log_file", "(", "\"lr:\"", ",", "np", ".", "round", "(", "self", ".", "optimizer", ".", "param_groups", "[", "0", "]", "[", "'lr'", "]", ",", "decimals", "=", "6", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.network_training.nnUNetTrainerV2.nnUNetTrainerV2.on_epoch_end": [[390, 409], ["super().on_epoch_end", "nnUNetTrainerV2.nnUNetTrainerV2.network.apply", "nnUNetTrainerV2.nnUNetTrainerV2.print_to_log_file", "nnunet.network_architecture.initialization.InitWeights_He"], "methods", ["home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.optimizer_and_lr.nnUNetTrainerV2_reduceMomentumDuringTraining.nnUNetTrainerV2_reduceMomentumDuringTraining.on_epoch_end", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.network_training.nnUNetTrainerV2_DDP.nnUNetTrainerV2_DDP.print_to_log_file"], ["", "def", "on_epoch_end", "(", "self", ")", ":", "\n", "        ", "\"\"\"\n        overwrite patient-based early stopping. Always run to 1000 epochs\n        :return:\n        \"\"\"", "\n", "super", "(", ")", ".", "on_epoch_end", "(", ")", "\n", "continue_training", "=", "self", ".", "epoch", "<", "self", ".", "max_num_epochs", "\n", "\n", "# it can rarely happen that the momentum of nnUNetTrainerV2 is too high for some dataset. If at epoch 100 the", "\n", "# estimated validation Dice is still 0 then we reduce the momentum from 0.99 to 0.95", "\n", "if", "self", ".", "epoch", "==", "100", ":", "\n", "            ", "if", "self", ".", "all_val_eval_metrics", "[", "-", "1", "]", "==", "0", ":", "\n", "                ", "self", ".", "optimizer", ".", "param_groups", "[", "0", "]", "[", "\"momentum\"", "]", "=", "0.95", "\n", "self", ".", "network", ".", "apply", "(", "InitWeights_He", "(", "1e-2", ")", ")", "\n", "self", ".", "print_to_log_file", "(", "\"At epoch 100, the mean foreground Dice was 0. This can be caused by a too \"", "\n", "\"high momentum. High momentum (0.99) is good for datasets where it works, but \"", "\n", "\"sometimes causes issues such as this one. Momentum has now been reduced to \"", "\n", "\"0.95 and network weights have been reinitialized\"", ")", "\n", "", "", "return", "continue_training", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.network_training.nnUNetTrainerV2.nnUNetTrainerV2.run_training": [[410, 425], ["nnUNetTrainerV2.nnUNetTrainerV2.maybe_update_lr", "super().run_training"], "methods", ["home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.optimizer_and_lr.nnUNetTrainerV2_warmup.nnUNetTrainerV2_warmup.maybe_update_lr", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.architectural_variants.nnUNetTrainerV2_ResencUNet.nnUNetTrainerV2_ResencUNet.run_training"], ["", "def", "run_training", "(", "self", ")", ":", "\n", "        ", "\"\"\"\n        if we run with -c then we need to set the correct lr for the first epoch, otherwise it will run the first\n        continued epoch with self.initial_lr\n\n        we also need to make sure deep supervision in the network is enabled for training, thus the wrapper\n        :return:\n        \"\"\"", "\n", "self", ".", "maybe_update_lr", "(", "self", ".", "epoch", ")", "# if we dont overwrite epoch then self.epoch+1 is used which is not what we", "\n", "# want at the start of the training", "\n", "ds", "=", "self", ".", "network", ".", "do_ds", "\n", "self", ".", "network", ".", "do_ds", "=", "True", "\n", "ret", "=", "super", "(", ")", ".", "run_training", "(", ")", "\n", "self", ".", "network", ".", "do_ds", "=", "ds", "\n", "return", "ret", "\n", "", "", ""]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.nnUNet_variants.nnUNetTrainerNoDA.nnUNetTrainerNoDA.get_basic_generators": [[28, 49], ["nnUNetTrainerNoDA.nnUNetTrainerNoDA.load_dataset", "nnUNetTrainerNoDA.nnUNetTrainerNoDA.do_split", "nnunet.training.dataloading.dataset_loading.DataLoader3D", "nnunet.training.dataloading.dataset_loading.DataLoader3D", "nnunet.training.dataloading.dataset_loading.DataLoader2D", "nnunet.training.dataloading.dataset_loading.DataLoader2D", "nnUNetTrainerNoDA.nnUNetTrainerNoDA.plans.get", "nnUNetTrainerNoDA.nnUNetTrainerNoDA.plans.get"], "methods", ["home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.dataloading.dataset_loading.load_dataset", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.network_training.nnUNetTrainerV2.nnUNetTrainerV2.do_split"], ["    ", "def", "get_basic_generators", "(", "self", ")", ":", "\n", "        ", "self", ".", "load_dataset", "(", ")", "\n", "self", ".", "do_split", "(", ")", "\n", "\n", "if", "self", ".", "threeD", ":", "\n", "            ", "dl_tr", "=", "DataLoader3D", "(", "self", ".", "dataset_tr", ",", "self", ".", "patch_size", ",", "self", ".", "patch_size", ",", "self", ".", "batch_size", ",", "\n", "False", ",", "oversample_foreground_percent", "=", "self", ".", "oversample_foreground_percent", "\n", ",", "pad_mode", "=", "\"constant\"", ",", "pad_sides", "=", "self", ".", "pad_all_sides", ")", "\n", "dl_val", "=", "DataLoader3D", "(", "self", ".", "dataset_val", ",", "self", ".", "patch_size", ",", "self", ".", "patch_size", ",", "self", ".", "batch_size", ",", "False", ",", "\n", "oversample_foreground_percent", "=", "self", ".", "oversample_foreground_percent", ",", "\n", "pad_mode", "=", "\"constant\"", ",", "pad_sides", "=", "self", ".", "pad_all_sides", ")", "\n", "", "else", ":", "\n", "            ", "dl_tr", "=", "DataLoader2D", "(", "self", ".", "dataset_tr", ",", "self", ".", "patch_size", ",", "self", ".", "patch_size", ",", "self", ".", "batch_size", ",", "\n", "transpose", "=", "self", ".", "plans", ".", "get", "(", "'transpose_forward'", ")", ",", "\n", "oversample_foreground_percent", "=", "self", ".", "oversample_foreground_percent", "\n", ",", "pad_mode", "=", "\"constant\"", ",", "pad_sides", "=", "self", ".", "pad_all_sides", ")", "\n", "dl_val", "=", "DataLoader2D", "(", "self", ".", "dataset_val", ",", "self", ".", "patch_size", ",", "self", ".", "patch_size", ",", "self", ".", "batch_size", ",", "\n", "transpose", "=", "self", ".", "plans", ".", "get", "(", "'transpose_forward'", ")", ",", "\n", "oversample_foreground_percent", "=", "self", ".", "oversample_foreground_percent", ",", "\n", "pad_mode", "=", "\"constant\"", ",", "pad_sides", "=", "self", ".", "pad_all_sides", ")", "\n", "", "return", "dl_tr", ",", "dl_val", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.nnUNet_variants.nnUNetTrainerNoDA.nnUNetTrainerNoDA.initialize": [[50, 92], ["batchgenerators.utilities.file_and_folder_operations.maybe_mkdir_p", "nnUNetTrainerNoDA.nnUNetTrainerNoDA.process_plans", "nnUNetTrainerNoDA.nnUNetTrainerNoDA.setup_DA_params", "batchgenerators.utilities.file_and_folder_operations.join", "nnUNetTrainerNoDA.nnUNetTrainerNoDA.initialize_network", "isinstance", "nnUNetTrainerNoDA.nnUNetTrainerNoDA.load_plans_file", "nnUNetTrainerNoDA.nnUNetTrainerNoDA.get_basic_generators", "nnunet.training.data_augmentation.default_data_augmentation.get_no_augmentation", "nnUNetTrainerNoDA.nnUNetTrainerNoDA.print_to_log_file", "nnUNetTrainerNoDA.nnUNetTrainerNoDA.print_to_log_file", "print", "nnunet.training.dataloading.dataset_loading.unpack_dataset", "print", "print", "str", "str", "nnUNetTrainerNoDA.nnUNetTrainerNoDA.dataset_tr.keys", "nnUNetTrainerNoDA.nnUNetTrainerNoDA.dataset_val.keys"], "methods", ["home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.BraTS_trainer.nnUNetTrainerV2BraTSRegions.nnUNetTrainerV2BraTSRegions_DA3_BN.process_plans", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.BraTS_trainer.nnUNetTrainerV2BraTSRegions.nnUNetTrainerV2BraTSRegions_DA4_BN.setup_DA_params", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.BraTS_trainer.nnUNetTrainerV2BraTSRegions.nnUNetTrainerV2BraTSRegions_DA3.initialize_network", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.network_training.nnUNetTrainer.nnUNetTrainer.load_plans_file", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.nnUNet_variants.nnUNetTrainerNoDA.nnUNetTrainerNoDA.get_basic_generators", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.data_augmentation.default_data_augmentation.get_no_augmentation", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.network_training.nnUNetTrainerV2_DDP.nnUNetTrainerV2_DDP.print_to_log_file", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.network_training.nnUNetTrainerV2_DDP.nnUNetTrainerV2_DDP.print_to_log_file", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.dataloading.dataset_loading.unpack_dataset"], ["", "def", "initialize", "(", "self", ",", "training", "=", "True", ",", "force_load_plans", "=", "False", ")", ":", "\n", "        ", "\"\"\"\n        For prediction of test cases just set training=False, this will prevent loading of training data and\n        training batchgenerator initialization\n        :param training:\n        :return:\n        \"\"\"", "\n", "\n", "maybe_mkdir_p", "(", "self", ".", "output_folder", ")", "\n", "\n", "if", "force_load_plans", "or", "(", "self", ".", "plans", "is", "None", ")", ":", "\n", "            ", "self", ".", "load_plans_file", "(", ")", "\n", "\n", "", "self", ".", "process_plans", "(", "self", ".", "plans", ")", "\n", "\n", "self", ".", "setup_DA_params", "(", ")", "\n", "\n", "self", ".", "folder_with_preprocessed_data", "=", "join", "(", "self", ".", "dataset_directory", ",", "self", ".", "plans", "[", "'data_identifier'", "]", "+", "\n", "\"_stage%d\"", "%", "self", ".", "stage", ")", "\n", "if", "training", ":", "\n", "            ", "self", ".", "dl_tr", ",", "self", ".", "dl_val", "=", "self", ".", "get_basic_generators", "(", ")", "\n", "if", "self", ".", "unpack_data", ":", "\n", "                ", "print", "(", "\"unpacking dataset\"", ")", "\n", "unpack_dataset", "(", "self", ".", "folder_with_preprocessed_data", ")", "\n", "print", "(", "\"done\"", ")", "\n", "", "else", ":", "\n", "                ", "print", "(", "\"INFO: Not unpacking data! Training may be slow due to that. Pray you are not using 2d or you \"", "\n", "\"will wait all winter for your model to finish!\"", ")", "\n", "", "self", ".", "tr_gen", ",", "self", ".", "val_gen", "=", "get_no_augmentation", "(", "self", ".", "dl_tr", ",", "self", ".", "dl_val", ",", "\n", "self", ".", "data_aug_params", "[", "\n", "'patch_size_for_spatialtransform'", "]", ",", "\n", "self", ".", "data_aug_params", ")", "\n", "self", ".", "print_to_log_file", "(", "\"TRAINING KEYS:\\n %s\"", "%", "(", "str", "(", "self", ".", "dataset_tr", ".", "keys", "(", ")", ")", ")", ",", "\n", "also_print_to_console", "=", "False", ")", "\n", "self", ".", "print_to_log_file", "(", "\"VALIDATION KEYS:\\n %s\"", "%", "(", "str", "(", "self", ".", "dataset_val", ".", "keys", "(", ")", ")", ")", ",", "\n", "also_print_to_console", "=", "False", ")", "\n", "", "else", ":", "\n", "            ", "pass", "\n", "", "self", ".", "initialize_network", "(", ")", "\n", "assert", "isinstance", "(", "self", ".", "network", ",", "(", "SegmentationNetwork", ",", "nn", ".", "DataParallel", ")", ")", "\n", "self", ".", "was_initialized", "=", "True", "\n", "self", ".", "data_aug_params", "[", "'mirror_axes'", "]", "=", "(", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.nnUNet_variants.nnUNetTrainerCE.nnUNetTrainerCE.__init__": [[19, 24], ["nnunet.training.network_training.nnUNetTrainer.nnUNetTrainer.__init__", "nnunet.training.loss_functions.crossentropy.RobustCrossEntropyLoss"], "methods", ["home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.common.layers.ResizeImage.__init__"], ["    ", "def", "__init__", "(", "self", ",", "plans_file", ",", "fold", ",", "output_folder", "=", "None", ",", "dataset_directory", "=", "None", ",", "batch_dice", "=", "True", ",", "stage", "=", "None", ",", "\n", "unpack_data", "=", "True", ",", "deterministic", "=", "True", ",", "fp16", "=", "False", ")", ":", "\n", "        ", "super", "(", "nnUNetTrainerCE", ",", "self", ")", ".", "__init__", "(", "plans_file", ",", "fold", ",", "output_folder", ",", "dataset_directory", ",", "batch_dice", ",", "stage", ",", "\n", "unpack_data", ",", "deterministic", ",", "fp16", ")", "\n", "self", ".", "loss", "=", "RobustCrossEntropyLoss", "(", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.loss_function.nnUNetTrainerV2_ForceSD.nnUNetTrainerV2_ForceSD.__init__": [[20, 25], ["nnunet.training.network_training.nnUNetTrainerV2.nnUNetTrainerV2.__init__"], "methods", ["home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.common.layers.ResizeImage.__init__"], ["    ", "def", "__init__", "(", "self", ",", "plans_file", ",", "fold", ",", "output_folder", "=", "None", ",", "dataset_directory", "=", "None", ",", "batch_dice", "=", "True", ",", "stage", "=", "None", ",", "\n", "unpack_data", "=", "True", ",", "deterministic", "=", "True", ",", "fp16", "=", "False", ")", ":", "\n", "        ", "batch_dice", "=", "False", "\n", "super", "(", ")", ".", "__init__", "(", "plans_file", ",", "fold", ",", "output_folder", ",", "dataset_directory", ",", "batch_dice", ",", "stage", ",", "unpack_data", ",", "\n", "deterministic", ",", "fp16", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.loss_function.nnUNetTrainerV2_Loss_MCC.nnUNetTrainerV2_Loss_MCC.__init__": [[22, 28], ["nnunet.training.network_training.nnUNetTrainerV2.nnUNetTrainerV2.__init__", "nnunet.training.loss_functions.dice_loss.MCCLoss"], "methods", ["home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.common.layers.ResizeImage.__init__"], ["    ", "def", "__init__", "(", "self", ",", "plans_file", ",", "fold", ",", "output_folder", "=", "None", ",", "dataset_directory", "=", "None", ",", "batch_dice", "=", "True", ",", "stage", "=", "None", ",", "\n", "unpack_data", "=", "True", ",", "deterministic", "=", "True", ",", "fp16", "=", "False", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "plans_file", ",", "fold", ",", "output_folder", ",", "dataset_directory", ",", "batch_dice", ",", "stage", ",", "unpack_data", ",", "\n", "deterministic", ",", "fp16", ")", "\n", "self", ".", "initial_lr", "=", "1e-3", "\n", "self", ".", "loss", "=", "MCCLoss", "(", "apply_nonlin", "=", "softmax_helper", ",", "batch_mcc", "=", "self", ".", "batch_dice", ",", "do_bg", "=", "True", ",", "smooth", "=", "0.0", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.loss_function.nnUNetTrainerV2_Loss_MCC.nnUNetTrainerV2_Loss_MCCnoBG.__init__": [[31, 37], ["nnunet.training.network_training.nnUNetTrainerV2.nnUNetTrainerV2.__init__", "nnunet.training.loss_functions.dice_loss.MCCLoss"], "methods", ["home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.common.layers.ResizeImage.__init__"], ["    ", "def", "__init__", "(", "self", ",", "plans_file", ",", "fold", ",", "output_folder", "=", "None", ",", "dataset_directory", "=", "None", ",", "batch_dice", "=", "True", ",", "stage", "=", "None", ",", "\n", "unpack_data", "=", "True", ",", "deterministic", "=", "True", ",", "fp16", "=", "False", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "plans_file", ",", "fold", ",", "output_folder", ",", "dataset_directory", ",", "batch_dice", ",", "stage", ",", "unpack_data", ",", "\n", "deterministic", ",", "fp16", ")", "\n", "self", ".", "initial_lr", "=", "1e-3", "\n", "self", ".", "loss", "=", "MCCLoss", "(", "apply_nonlin", "=", "softmax_helper", ",", "batch_mcc", "=", "self", ".", "batch_dice", ",", "do_bg", "=", "False", ",", "smooth", "=", "0.0", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.loss_function.nnUNetTrainerV2_Loss_TopK10.nnUNetTrainerV2_Loss_TopK10.__init__": [[21, 26], ["nnunet.training.network_training.nnUNetTrainerV2.nnUNetTrainerV2.__init__", "nnunet.training.loss_functions.TopK_loss.TopKLoss"], "methods", ["home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.common.layers.ResizeImage.__init__"], ["    ", "def", "__init__", "(", "self", ",", "plans_file", ",", "fold", ",", "output_folder", "=", "None", ",", "dataset_directory", "=", "None", ",", "batch_dice", "=", "True", ",", "stage", "=", "None", ",", "\n", "unpack_data", "=", "True", ",", "deterministic", "=", "True", ",", "fp16", "=", "False", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "plans_file", ",", "fold", ",", "output_folder", ",", "dataset_directory", ",", "batch_dice", ",", "stage", ",", "unpack_data", ",", "\n", "deterministic", ",", "fp16", ")", "\n", "self", ".", "loss", "=", "TopKLoss", "(", "k", "=", "10", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.loss_function.nnUNetTrainerV2_Loss_DiceTopK10.nnUNetTrainerV2_Loss_DiceTopK10.__init__": [[21, 27], ["nnunet.training.network_training.nnUNetTrainerV2.nnUNetTrainerV2.__init__", "nnunet.training.loss_functions.dice_loss.DC_and_topk_loss"], "methods", ["home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.common.layers.ResizeImage.__init__"], ["    ", "def", "__init__", "(", "self", ",", "plans_file", ",", "fold", ",", "output_folder", "=", "None", ",", "dataset_directory", "=", "None", ",", "batch_dice", "=", "True", ",", "stage", "=", "None", ",", "\n", "unpack_data", "=", "True", ",", "deterministic", "=", "True", ",", "fp16", "=", "False", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "plans_file", ",", "fold", ",", "output_folder", ",", "dataset_directory", ",", "batch_dice", ",", "stage", ",", "unpack_data", ",", "\n", "deterministic", ",", "fp16", ")", "\n", "self", ".", "loss", "=", "DC_and_topk_loss", "(", "{", "'batch_dice'", ":", "self", ".", "batch_dice", ",", "'smooth'", ":", "1e-5", ",", "'do_bg'", ":", "False", "}", ",", "\n", "{", "'k'", ":", "10", "}", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.loss_function.nnUNetTrainerV2_Loss_CE.nnUNetTrainerV2_Loss_CE.__init__": [[19, 24], ["nnunet.training.network_training.nnUNetTrainerV2.nnUNetTrainerV2.__init__", "nnunet.training.loss_functions.crossentropy.RobustCrossEntropyLoss"], "methods", ["home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.common.layers.ResizeImage.__init__"], ["    ", "def", "__init__", "(", "self", ",", "plans_file", ",", "fold", ",", "output_folder", "=", "None", ",", "dataset_directory", "=", "None", ",", "batch_dice", "=", "True", ",", "stage", "=", "None", ",", "\n", "unpack_data", "=", "True", ",", "deterministic", "=", "True", ",", "fp16", "=", "False", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "plans_file", ",", "fold", ",", "output_folder", ",", "dataset_directory", ",", "batch_dice", ",", "stage", ",", "unpack_data", ",", "\n", "deterministic", ",", "fp16", ")", "\n", "self", ".", "loss", "=", "RobustCrossEntropyLoss", "(", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.loss_function.nnUNetTrainerV2_ForceBD.nnUNetTrainerV2_ForceBD.__init__": [[20, 25], ["nnunet.training.network_training.nnUNetTrainerV2.nnUNetTrainerV2.__init__"], "methods", ["home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.common.layers.ResizeImage.__init__"], ["    ", "def", "__init__", "(", "self", ",", "plans_file", ",", "fold", ",", "output_folder", "=", "None", ",", "dataset_directory", "=", "None", ",", "batch_dice", "=", "True", ",", "stage", "=", "None", ",", "\n", "unpack_data", "=", "True", ",", "deterministic", "=", "True", ",", "fp16", "=", "False", ")", ":", "\n", "        ", "batch_dice", "=", "True", "\n", "super", "(", ")", ".", "__init__", "(", "plans_file", ",", "fold", ",", "output_folder", ",", "dataset_directory", ",", "batch_dice", ",", "stage", ",", "unpack_data", ",", "\n", "deterministic", ",", "fp16", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.loss_function.nnUNetTrainerV2_focalLoss.FocalLossBinary.__init__": [[122, 150], ["torch.nn.modules.loss._Loss.__init__", "functools.partial", "functools.partial"], "methods", ["home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.common.layers.ResizeImage.__init__"], ["    ", "def", "__init__", "(", "\n", "self", ",", "\n", "ignore", ":", "int", "=", "None", ",", "\n", "reduced", ":", "bool", "=", "False", ",", "\n", "gamma", ":", "float", "=", "2.0", ",", "\n", "alpha", ":", "float", "=", "0.25", ",", "\n", "threshold", ":", "float", "=", "0.5", ",", "\n", "reduction", ":", "str", "=", "\"mean\"", ",", "\n", ")", ":", "\n", "        ", "\"\"\"\n        Compute focal loss for binary classification problem.\n        \"\"\"", "\n", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "ignore", "=", "ignore", "\n", "\n", "if", "reduced", ":", "\n", "            ", "self", ".", "loss_fn", "=", "partial", "(", "\n", "reduced_focal_loss", ",", "\n", "gamma", "=", "gamma", ",", "\n", "threshold", "=", "threshold", ",", "\n", "reduction", "=", "reduction", "\n", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "loss_fn", "=", "partial", "(", "\n", "sigmoid_focal_loss", ",", "\n", "gamma", "=", "gamma", ",", "\n", "alpha", "=", "alpha", ",", "\n", "reduction", "=", "reduction", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.loss_function.nnUNetTrainerV2_focalLoss.FocalLossBinary.forward": [[152, 170], ["targets.view.view.view", "logits.view.view.view", "nnUNetTrainerV2_focalLoss.FocalLossBinary.loss_fn"], "methods", ["None"], ["", "", "def", "forward", "(", "self", ",", "logits", ",", "targets", ")", ":", "\n", "        ", "\"\"\"\n        Args:\n            logits: [bs; ...]\n            targets: [bs; ...]\n        \"\"\"", "\n", "targets", "=", "targets", ".", "view", "(", "-", "1", ")", "\n", "logits", "=", "logits", ".", "view", "(", "-", "1", ")", "\n", "\n", "if", "self", ".", "ignore", "is", "not", "None", ":", "\n", "# Filter predictions with ignore label from loss computation", "\n", "            ", "not_ignored", "=", "targets", "!=", "self", ".", "ignore", "\n", "logits", "=", "logits", "[", "not_ignored", "]", "\n", "targets", "=", "targets", "[", "not_ignored", "]", "\n", "\n", "", "loss", "=", "self", ".", "loss_fn", "(", "logits", ",", "targets", ")", "\n", "\n", "return", "loss", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.loss_function.nnUNetTrainerV2_focalLoss.FocalLossMultiClass.forward": [[178, 204], ["logits.view.view.size", "targets.view.view.view", "logits.view.view.view", "range", "nnUNetTrainerV2_focalLoss.FocalLossMultiClass.loss_fn"], "methods", ["None"], ["def", "forward", "(", "self", ",", "logits", ",", "targets", ")", ":", "\n", "        ", "\"\"\"\n        Args:\n            logits: [bs; num_classes; ...]\n            targets: [bs; ...]\n        \"\"\"", "\n", "num_classes", "=", "logits", ".", "size", "(", "1", ")", "\n", "loss", "=", "0", "\n", "targets", "=", "targets", ".", "view", "(", "-", "1", ")", "\n", "logits", "=", "logits", ".", "view", "(", "-", "1", ",", "num_classes", ")", "\n", "\n", "# Filter anchors with -1 label from loss computation", "\n", "if", "self", ".", "ignore", "is", "not", "None", ":", "\n", "            ", "not_ignored", "=", "targets", "!=", "self", ".", "ignore", "\n", "\n", "", "for", "cls", "in", "range", "(", "num_classes", ")", ":", "\n", "            ", "cls_label_target", "=", "(", "targets", "==", "(", "cls", "+", "0", ")", ")", ".", "long", "(", ")", "\n", "cls_label_input", "=", "logits", "[", "...", ",", "cls", "]", "\n", "\n", "if", "self", ".", "ignore", "is", "not", "None", ":", "\n", "                ", "cls_label_target", "=", "cls_label_target", "[", "not_ignored", "]", "\n", "cls_label_input", "=", "cls_label_input", "[", "not_ignored", "]", "\n", "\n", "", "loss", "+=", "self", ".", "loss_fn", "(", "cls_label_input", ",", "cls_label_target", ")", "\n", "\n", "", "return", "loss", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.loss_function.nnUNetTrainerV2_focalLoss.nnUNetTrainerV2_focalLoss.__init__": [[207, 212], ["nnunet.training.network_training.nnUNetTrainerV2.nnUNetTrainerV2.__init__", "nnUNetTrainerV2_focalLoss.FocalLossMultiClass"], "methods", ["home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.common.layers.ResizeImage.__init__"], ["    ", "def", "__init__", "(", "self", ",", "plans_file", ",", "fold", ",", "output_folder", "=", "None", ",", "dataset_directory", "=", "None", ",", "batch_dice", "=", "True", ",", "stage", "=", "None", ",", "\n", "unpack_data", "=", "True", ",", "deterministic", "=", "True", ",", "fp16", "=", "False", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "plans_file", ",", "fold", ",", "output_folder", ",", "dataset_directory", ",", "batch_dice", ",", "stage", ",", "unpack_data", ",", "\n", "deterministic", ",", "fp16", ")", "\n", "self", ".", "loss", "=", "FocalLossMultiClass", "(", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.loss_function.nnUNetTrainerV2_focalLoss.sigmoid_focal_loss": [[23, 67], ["targets.type.type", "torch.exp", "torch.exp", "outputs.type", "torch.binary_cross_entropy_with_logits", "loss.sum.mean", "loss.sum.sum", "loss.sum.sum"], "function", ["None"], ["def", "sigmoid_focal_loss", "(", "\n", "outputs", ":", "torch", ".", "Tensor", ",", "\n", "targets", ":", "torch", ".", "Tensor", ",", "\n", "gamma", ":", "float", "=", "2.0", ",", "\n", "alpha", ":", "float", "=", "0.25", ",", "\n", "reduction", ":", "str", "=", "\"mean\"", "\n", ")", ":", "\n", "    ", "\"\"\"\n    Compute binary focal loss between target and output logits.\n    Source https://github.com/BloodAxe/pytorch-toolbelt\n    See :class:`~pytorch_toolbelt.losses` for details.\n    Args:\n        outputs: Tensor of arbitrary shape\n        targets: Tensor of the same shape as input\n        reduction (string, optional):\n            Specifies the reduction to apply to the output:\n            \"none\" | \"mean\" | \"sum\" | \"batchwise_mean\".\n            \"none\": no reduction will be applied,\n            \"mean\": the sum of the output will be divided by the number of\n            elements in the output,\n            \"sum\": the output will be summed.\n    See https://github.com/open-mmlab/mmdetection/blob/master/mmdet/core/loss/losses.py  # noqa: E501\n    \"\"\"", "\n", "targets", "=", "targets", ".", "type", "(", "outputs", ".", "type", "(", ")", ")", "\n", "\n", "logpt", "=", "-", "F", ".", "binary_cross_entropy_with_logits", "(", "\n", "outputs", ",", "targets", ",", "reduction", "=", "\"none\"", "\n", ")", "\n", "pt", "=", "torch", ".", "exp", "(", "logpt", ")", "\n", "\n", "# compute the loss", "\n", "loss", "=", "-", "(", "(", "1", "-", "pt", ")", ".", "pow", "(", "gamma", ")", ")", "*", "logpt", "\n", "\n", "if", "alpha", "is", "not", "None", ":", "\n", "        ", "loss", "=", "loss", "*", "(", "alpha", "*", "targets", "+", "(", "1", "-", "alpha", ")", "*", "(", "1", "-", "targets", ")", ")", "\n", "\n", "", "if", "reduction", "==", "\"mean\"", ":", "\n", "        ", "loss", "=", "loss", ".", "mean", "(", ")", "\n", "", "if", "reduction", "==", "\"sum\"", ":", "\n", "        ", "loss", "=", "loss", ".", "sum", "(", ")", "\n", "", "if", "reduction", "==", "\"batchwise_mean\"", ":", "\n", "        ", "loss", "=", "loss", ".", "sum", "(", "0", ")", "\n", "\n", "", "return", "loss", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.loss_function.nnUNetTrainerV2_focalLoss.reduced_focal_loss": [[69, 119], ["targets.type.type", "torch.exp", "torch.exp", "outputs.type", "torch.binary_cross_entropy_with_logits", "loss.sum.mean", "loss.sum.sum", "loss.sum.sum"], "function", ["None"], ["", "def", "reduced_focal_loss", "(", "\n", "outputs", ":", "torch", ".", "Tensor", ",", "\n", "targets", ":", "torch", ".", "Tensor", ",", "\n", "threshold", ":", "float", "=", "0.5", ",", "\n", "gamma", ":", "float", "=", "2.0", ",", "\n", "reduction", "=", "\"mean\"", "\n", ")", ":", "\n", "    ", "\"\"\"\n    Compute reduced focal loss between target and output logits.\n    Source https://github.com/BloodAxe/pytorch-toolbelt\n    See :class:`~pytorch_toolbelt.losses` for details.\n    Args:\n        outputs: Tensor of arbitrary shape\n        targets: Tensor of the same shape as input\n        reduction (string, optional):\n            Specifies the reduction to apply to the output:\n            \"none\" | \"mean\" | \"sum\" | \"batchwise_mean\".\n            \"none\": no reduction will be applied,\n            \"mean\": the sum of the output will be divided by the number of\n            elements in the output,\n            \"sum\": the output will be summed.\n            Note: :attr:`size_average` and :attr:`reduce`\n            are in the process of being deprecated,\n            and in the meantime, specifying either of those two args\n            will override :attr:`reduction`.\n            \"batchwise_mean\" computes mean loss per sample in batch.\n            Default: \"mean\"\n    See https://arxiv.org/abs/1903.01347\n    \"\"\"", "\n", "targets", "=", "targets", ".", "type", "(", "outputs", ".", "type", "(", ")", ")", "\n", "\n", "logpt", "=", "-", "F", ".", "binary_cross_entropy_with_logits", "(", "\n", "outputs", ",", "targets", ",", "reduction", "=", "\"none\"", "\n", ")", "\n", "pt", "=", "torch", ".", "exp", "(", "logpt", ")", "\n", "\n", "# compute the loss", "\n", "focal_reduction", "=", "(", "(", "1.", "-", "pt", ")", "/", "threshold", ")", ".", "pow", "(", "gamma", ")", "\n", "focal_reduction", "[", "pt", "<", "threshold", "]", "=", "1", "\n", "\n", "loss", "=", "-", "focal_reduction", "*", "logpt", "\n", "\n", "if", "reduction", "==", "\"mean\"", ":", "\n", "        ", "loss", "=", "loss", ".", "mean", "(", ")", "\n", "", "if", "reduction", "==", "\"sum\"", ":", "\n", "        ", "loss", "=", "loss", ".", "sum", "(", ")", "\n", "", "if", "reduction", "==", "\"batchwise_mean\"", ":", "\n", "        ", "loss", "=", "loss", ".", "sum", "(", "0", ")", "\n", "\n", "", "return", "loss", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.loss_function.nnUNetTrainerV2_Loss_Dice_lr1en3.nnUNetTrainerV2_Loss_Dice_LR1en3.__init__": [[21, 26], ["nnunet.training.network_training.nnUNet_variants.loss_function.nnUNetTrainerV2_Loss_Dice.nnUNetTrainerV2_Loss_Dice.__init__"], "methods", ["home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.common.layers.ResizeImage.__init__"], ["    ", "def", "__init__", "(", "self", ",", "plans_file", ",", "fold", ",", "output_folder", "=", "None", ",", "dataset_directory", "=", "None", ",", "batch_dice", "=", "True", ",", "stage", "=", "None", ",", "\n", "unpack_data", "=", "True", ",", "deterministic", "=", "True", ",", "fp16", "=", "False", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "plans_file", ",", "fold", ",", "output_folder", ",", "dataset_directory", ",", "batch_dice", ",", "stage", ",", "unpack_data", ",", "\n", "deterministic", ",", "fp16", ")", "\n", "self", ".", "initial_lr", "=", "1e-3", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.loss_function.nnUNetTrainerV2_Loss_Dice_lr1en3.nnUNetTrainerV2_Loss_DicewithBG_LR1en3.__init__": [[29, 34], ["nnunet.training.network_training.nnUNet_variants.loss_function.nnUNetTrainerV2_Loss_Dice.nnUNetTrainerV2_Loss_DicewithBG.__init__"], "methods", ["home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.common.layers.ResizeImage.__init__"], ["    ", "def", "__init__", "(", "self", ",", "plans_file", ",", "fold", ",", "output_folder", "=", "None", ",", "dataset_directory", "=", "None", ",", "batch_dice", "=", "True", ",", "stage", "=", "None", ",", "\n", "unpack_data", "=", "True", ",", "deterministic", "=", "True", ",", "fp16", "=", "False", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "plans_file", ",", "fold", ",", "output_folder", ",", "dataset_directory", ",", "batch_dice", ",", "stage", ",", "unpack_data", ",", "\n", "deterministic", ",", "fp16", ")", "\n", "self", ".", "initial_lr", "=", "1e-3", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.loss_function.nnUNetTrainerV2_Loss_Dice_squared.nnUNetTrainerV2_Loss_Dice_squared.__init__": [[22, 28], ["nnunet.training.network_training.nnUNetTrainerV2.nnUNetTrainerV2.__init__", "nnunet.training.loss_functions.dice_loss.SoftDiceLossSquared"], "methods", ["home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.common.layers.ResizeImage.__init__"], ["    ", "def", "__init__", "(", "self", ",", "plans_file", ",", "fold", ",", "output_folder", "=", "None", ",", "dataset_directory", "=", "None", ",", "batch_dice", "=", "True", ",", "stage", "=", "None", ",", "\n", "unpack_data", "=", "True", ",", "deterministic", "=", "True", ",", "fp16", "=", "False", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "plans_file", ",", "fold", ",", "output_folder", ",", "dataset_directory", ",", "batch_dice", ",", "stage", ",", "unpack_data", ",", "\n", "deterministic", ",", "fp16", ")", "\n", "self", ".", "initial_lr", "=", "1e-3", "\n", "self", ".", "loss", "=", "SoftDiceLossSquared", "(", "**", "{", "'apply_nonlin'", ":", "softmax_helper", ",", "'batch_dice'", ":", "self", ".", "batch_dice", ",", "'smooth'", ":", "1e-5", ",", "'do_bg'", ":", "False", "}", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.loss_function.nnUNetTrainerV2_graduallyTransitionFromCEToDice.nnUNetTrainerV2_graduallyTransitionFromCEToDice.__init__": [[22, 27], ["nnunet.training.network_training.nnUNetTrainerV2.nnUNetTrainerV2.__init__", "nnunet.training.loss_functions.dice_loss.DC_and_CE_loss"], "methods", ["home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.common.layers.ResizeImage.__init__"], ["    ", "def", "__init__", "(", "self", ",", "plans_file", ",", "fold", ",", "output_folder", "=", "None", ",", "dataset_directory", "=", "None", ",", "batch_dice", "=", "True", ",", "stage", "=", "None", ",", "\n", "unpack_data", "=", "True", ",", "deterministic", "=", "True", ",", "fp16", "=", "False", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "plans_file", ",", "fold", ",", "output_folder", ",", "dataset_directory", ",", "batch_dice", ",", "stage", ",", "unpack_data", ",", "\n", "deterministic", ",", "fp16", ")", "\n", "self", ".", "loss", "=", "DC_and_CE_loss", "(", "{", "'batch_dice'", ":", "self", ".", "batch_dice", ",", "'smooth'", ":", "1e-5", ",", "'do_bg'", ":", "False", "}", ",", "{", "}", ",", "weight_ce", "=", "2", ",", "weight_dice", "=", "0", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.loss_function.nnUNetTrainerV2_graduallyTransitionFromCEToDice.nnUNetTrainerV2_graduallyTransitionFromCEToDice.update_loss": [[28, 49], ["nnUNetTrainerV2_graduallyTransitionFromCEToDice.nnUNetTrainerV2_graduallyTransitionFromCEToDice.print_to_log_file", "nnunet.training.loss_functions.dice_loss.DC_and_CE_loss", "nnunet.training.loss_functions.deep_supervision.MultipleOutputLoss2", "RuntimeError"], "methods", ["home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.network_training.nnUNetTrainerV2_DDP.nnUNetTrainerV2_DDP.print_to_log_file"], ["", "def", "update_loss", "(", "self", ")", ":", "\n", "# we train the first 500 epochs with CE, then transition to Dice between 500 and 750. The last 250 epochs will be Dice only", "\n", "\n", "        ", "if", "self", ".", "epoch", "<=", "500", ":", "\n", "            ", "weight_ce", "=", "2", "\n", "weight_dice", "=", "0", "\n", "", "elif", "500", "<", "self", ".", "epoch", "<=", "750", ":", "\n", "            ", "weight_ce", "=", "2", "-", "2", "/", "250", "*", "(", "self", ".", "epoch", "-", "500", ")", "\n", "weight_dice", "=", "0", "+", "2", "/", "250", "*", "(", "self", ".", "epoch", "-", "500", ")", "\n", "", "elif", "750", "<", "self", ".", "epoch", "<=", "self", ".", "max_num_epochs", ":", "\n", "            ", "weight_ce", "=", "0", "\n", "weight_dice", "=", "2", "\n", "", "else", ":", "\n", "            ", "raise", "RuntimeError", "(", "\"Invalid epoch: %d\"", "%", "self", ".", "epoch", ")", "\n", "\n", "", "self", ".", "print_to_log_file", "(", "\"weight ce\"", ",", "weight_ce", ",", "\"weight dice\"", ",", "weight_dice", ")", "\n", "\n", "self", ".", "loss", "=", "DC_and_CE_loss", "(", "{", "'batch_dice'", ":", "self", ".", "batch_dice", ",", "'smooth'", ":", "1e-5", ",", "'do_bg'", ":", "False", "}", ",", "{", "}", ",", "weight_ce", "=", "weight_ce", ",", "\n", "weight_dice", "=", "weight_dice", ")", "\n", "\n", "self", ".", "loss", "=", "MultipleOutputLoss2", "(", "self", ".", "loss", ",", "self", ".", "ds_loss_weights", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.loss_function.nnUNetTrainerV2_graduallyTransitionFromCEToDice.nnUNetTrainerV2_graduallyTransitionFromCEToDice.on_epoch_end": [[50, 54], ["super().on_epoch_end", "nnUNetTrainerV2_graduallyTransitionFromCEToDice.nnUNetTrainerV2_graduallyTransitionFromCEToDice.update_loss"], "methods", ["home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.optimizer_and_lr.nnUNetTrainerV2_reduceMomentumDuringTraining.nnUNetTrainerV2_reduceMomentumDuringTraining.on_epoch_end", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.loss_function.nnUNetTrainerV2_graduallyTransitionFromCEToDice.nnUNetTrainerV2_graduallyTransitionFromCEToDice.update_loss"], ["", "def", "on_epoch_end", "(", "self", ")", ":", "\n", "        ", "ret", "=", "super", "(", ")", ".", "on_epoch_end", "(", ")", "\n", "self", ".", "update_loss", "(", ")", "\n", "return", "ret", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.loss_function.nnUNetTrainerV2_graduallyTransitionFromCEToDice.nnUNetTrainerV2_graduallyTransitionFromCEToDice.load_checkpoint_ram": [[55, 59], ["super().load_checkpoint_ram", "nnUNetTrainerV2_graduallyTransitionFromCEToDice.nnUNetTrainerV2_graduallyTransitionFromCEToDice.update_loss"], "methods", ["home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.loss_function.nnUNetTrainerV2_graduallyTransitionFromCEToDice.nnUNetTrainerV2_graduallyTransitionFromCEToDice.load_checkpoint_ram", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.loss_function.nnUNetTrainerV2_graduallyTransitionFromCEToDice.nnUNetTrainerV2_graduallyTransitionFromCEToDice.update_loss"], ["", "def", "load_checkpoint_ram", "(", "self", ",", "checkpoint", ",", "train", "=", "True", ")", ":", "\n", "        ", "ret", "=", "super", "(", ")", ".", "load_checkpoint_ram", "(", "checkpoint", ",", "train", ")", "\n", "self", ".", "update_loss", "(", ")", "\n", "return", "ret", "\n", "", "", ""]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.loss_function.nnUNetTrainerV2_Loss_Dice.nnUNetTrainerV2_Loss_Dice.__init__": [[22, 27], ["nnunet.training.network_training.nnUNetTrainerV2.nnUNetTrainerV2.__init__", "nnunet.training.loss_functions.dice_loss.SoftDiceLoss"], "methods", ["home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.common.layers.ResizeImage.__init__"], ["    ", "def", "__init__", "(", "self", ",", "plans_file", ",", "fold", ",", "output_folder", "=", "None", ",", "dataset_directory", "=", "None", ",", "batch_dice", "=", "True", ",", "stage", "=", "None", ",", "\n", "unpack_data", "=", "True", ",", "deterministic", "=", "True", ",", "fp16", "=", "False", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "plans_file", ",", "fold", ",", "output_folder", ",", "dataset_directory", ",", "batch_dice", ",", "stage", ",", "unpack_data", ",", "\n", "deterministic", ",", "fp16", ")", "\n", "self", ".", "loss", "=", "SoftDiceLoss", "(", "**", "{", "'apply_nonlin'", ":", "softmax_helper", ",", "'batch_dice'", ":", "self", ".", "batch_dice", ",", "'smooth'", ":", "1e-5", ",", "'do_bg'", ":", "False", "}", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.loss_function.nnUNetTrainerV2_Loss_Dice.nnUNetTrainerV2_Loss_DicewithBG.__init__": [[30, 35], ["nnunet.training.network_training.nnUNetTrainerV2.nnUNetTrainerV2.__init__", "nnunet.training.loss_functions.dice_loss.SoftDiceLoss"], "methods", ["home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.common.layers.ResizeImage.__init__"], ["    ", "def", "__init__", "(", "self", ",", "plans_file", ",", "fold", ",", "output_folder", "=", "None", ",", "dataset_directory", "=", "None", ",", "batch_dice", "=", "True", ",", "stage", "=", "None", ",", "\n", "unpack_data", "=", "True", ",", "deterministic", "=", "True", ",", "fp16", "=", "False", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "plans_file", ",", "fold", ",", "output_folder", ",", "dataset_directory", ",", "batch_dice", ",", "stage", ",", "unpack_data", ",", "\n", "deterministic", ",", "fp16", ")", "\n", "self", ".", "loss", "=", "SoftDiceLoss", "(", "**", "{", "'apply_nonlin'", ":", "softmax_helper", ",", "'batch_dice'", ":", "self", ".", "batch_dice", ",", "'smooth'", ":", "1e-5", ",", "'do_bg'", ":", "True", "}", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.loss_function.nnUNetTrainerV2_Loss_CEGDL.nnUNetTrainerV2_Loss_CEGDL.__init__": [[21, 26], ["nnunet.training.network_training.nnUNetTrainerV2.nnUNetTrainerV2.__init__", "nnunet.training.loss_functions.dice_loss.GDL_and_CE_loss"], "methods", ["home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.common.layers.ResizeImage.__init__"], ["    ", "def", "__init__", "(", "self", ",", "plans_file", ",", "fold", ",", "output_folder", "=", "None", ",", "dataset_directory", "=", "None", ",", "batch_dice", "=", "True", ",", "stage", "=", "None", ",", "\n", "unpack_data", "=", "True", ",", "deterministic", "=", "True", ",", "fp16", "=", "False", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "plans_file", ",", "fold", ",", "output_folder", ",", "dataset_directory", ",", "batch_dice", ",", "stage", ",", "unpack_data", ",", "\n", "deterministic", ",", "fp16", ")", "\n", "self", ".", "loss", "=", "GDL_and_CE_loss", "(", "{", "'batch_dice'", ":", "self", ".", "batch_dice", ",", "'smooth'", ":", "1e-5", ",", "'do_bg'", ":", "False", "}", ",", "{", "}", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.profiling.nnUNetTrainerV2_dummyLoad.nnUNetTrainerV2_5epochs_dummyLoad.initialize": [[29, 34], ["super().initialize", "torch.rand().float().cuda", "torch.round().float().cuda", "torch.rand().float", "torch.round().float", "torch.rand", "torch.round", "torch.rand", "int", "zip"], "methods", ["home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.BraTS_trainer.nnUNetTrainerV2BraTSRegions.nnUNetTrainerV2BraTSRegions_DA3_BN.initialize"], ["    ", "def", "initialize", "(", "self", ",", "training", "=", "True", ",", "force_load_plans", "=", "False", ")", ":", "\n", "        ", "super", "(", ")", ".", "initialize", "(", "training", ",", "force_load_plans", ")", "\n", "self", ".", "some_batch", "=", "torch", ".", "rand", "(", "(", "self", ".", "batch_size", ",", "self", ".", "num_input_channels", ",", "*", "self", ".", "patch_size", ")", ")", ".", "float", "(", ")", ".", "cuda", "(", ")", "\n", "\n", "self", ".", "some_gt", "=", "[", "torch", ".", "round", "(", "torch", ".", "rand", "(", "(", "self", ".", "batch_size", ",", "1", ",", "*", "[", "int", "(", "i", "*", "j", ")", "for", "i", ",", "j", "in", "zip", "(", "self", ".", "patch_size", ",", "k", ")", "]", ")", ")", "*", "(", "self", ".", "num_classes", "-", "1", ")", ")", ".", "float", "(", ")", ".", "cuda", "(", ")", "for", "k", "in", "self", ".", "deep_supervision_scales", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.profiling.nnUNetTrainerV2_dummyLoad.nnUNetTrainerV2_5epochs_dummyLoad.run_iteration": [[35, 69], ["nnUNetTrainerV2_dummyLoad.nnUNetTrainerV2_5epochs_dummyLoad.optimizer.zero_grad", "nnUNetTrainerV2_dummyLoad.nnUNetTrainerV2_5epochs_dummyLoad.detach().cpu().numpy", "nnUNetTrainerV2_dummyLoad.nnUNetTrainerV2_5epochs_dummyLoad.network", "nnUNetTrainerV2_dummyLoad.nnUNetTrainerV2_5epochs_dummyLoad.loss", "nnUNetTrainerV2_dummyLoad.nnUNetTrainerV2_5epochs_dummyLoad.run_online_evaluation", "torch.cuda.amp.autocast", "nnUNetTrainerV2_dummyLoad.nnUNetTrainerV2_5epochs_dummyLoad.network", "nnUNetTrainerV2_dummyLoad.nnUNetTrainerV2_5epochs_dummyLoad.loss", "nnUNetTrainerV2_dummyLoad.nnUNetTrainerV2_5epochs_dummyLoad.amp_grad_scaler.scale().backward", "nnUNetTrainerV2_dummyLoad.nnUNetTrainerV2_5epochs_dummyLoad.amp_grad_scaler.unscale_", "torch.nn.utils.clip_grad_norm_", "nnUNetTrainerV2_dummyLoad.nnUNetTrainerV2_5epochs_dummyLoad.amp_grad_scaler.step", "nnUNetTrainerV2_dummyLoad.nnUNetTrainerV2_5epochs_dummyLoad.amp_grad_scaler.update", "nnUNetTrainerV2_dummyLoad.nnUNetTrainerV2_5epochs_dummyLoad.backward", "torch.nn.utils.clip_grad_norm_", "nnUNetTrainerV2_dummyLoad.nnUNetTrainerV2_5epochs_dummyLoad.optimizer.step", "nnUNetTrainerV2_dummyLoad.nnUNetTrainerV2_5epochs_dummyLoad.detach().cpu", "nnUNetTrainerV2_dummyLoad.nnUNetTrainerV2_5epochs_dummyLoad.network.parameters", "nnUNetTrainerV2_dummyLoad.nnUNetTrainerV2_5epochs_dummyLoad.network.parameters", "nnUNetTrainerV2_dummyLoad.nnUNetTrainerV2_5epochs_dummyLoad.amp_grad_scaler.scale", "nnUNetTrainerV2_dummyLoad.nnUNetTrainerV2_5epochs_dummyLoad.detach"], "methods", ["home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.BraTS_trainer.nnUNetTrainerV2BraTSRegions.nnUNetTrainerV2BraTSRegions_DA3_BN.run_online_evaluation", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.utilities.distributed.awesome_allgather_function.backward", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.optimizer.ranger.Ranger.step", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.utilities.distributed.awesome_allgather_function.backward", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.optimizer.ranger.Ranger.step"], ["", "def", "run_iteration", "(", "self", ",", "data_generator", ",", "do_backprop", "=", "True", ",", "run_online_evaluation", "=", "False", ")", ":", "\n", "        ", "data", "=", "self", ".", "some_batch", "\n", "target", "=", "self", ".", "some_gt", "\n", "\n", "self", ".", "optimizer", ".", "zero_grad", "(", ")", "\n", "\n", "if", "self", ".", "fp16", ":", "\n", "            ", "with", "autocast", "(", ")", ":", "\n", "                ", "output", "=", "self", ".", "network", "(", "data", ")", "\n", "del", "data", "\n", "l", "=", "self", ".", "loss", "(", "output", ",", "target", ")", "\n", "\n", "", "if", "do_backprop", ":", "\n", "                ", "self", ".", "amp_grad_scaler", ".", "scale", "(", "l", ")", ".", "backward", "(", ")", "\n", "self", ".", "amp_grad_scaler", ".", "unscale_", "(", "self", ".", "optimizer", ")", "\n", "torch", ".", "nn", ".", "utils", ".", "clip_grad_norm_", "(", "self", ".", "network", ".", "parameters", "(", ")", ",", "12", ")", "\n", "self", ".", "amp_grad_scaler", ".", "step", "(", "self", ".", "optimizer", ")", "\n", "self", ".", "amp_grad_scaler", ".", "update", "(", ")", "\n", "", "", "else", ":", "\n", "            ", "output", "=", "self", ".", "network", "(", "data", ")", "\n", "del", "data", "\n", "l", "=", "self", ".", "loss", "(", "output", ",", "target", ")", "\n", "\n", "if", "do_backprop", ":", "\n", "                ", "l", ".", "backward", "(", ")", "\n", "torch", ".", "nn", ".", "utils", ".", "clip_grad_norm_", "(", "self", ".", "network", ".", "parameters", "(", ")", ",", "12", ")", "\n", "self", ".", "optimizer", ".", "step", "(", ")", "\n", "\n", "", "", "if", "run_online_evaluation", ":", "\n", "            ", "self", ".", "run_online_evaluation", "(", "output", ",", "target", ")", "\n", "\n", "", "del", "target", "\n", "\n", "return", "l", ".", "detach", "(", ")", ".", "cpu", "(", ")", ".", "numpy", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.profiling.nnUNetTrainerV2_dummyLoad.nnUNetTrainerV2_5epochs_dummyLoadCEnoDS.__init__": [[72, 78], ["nnunet.training.network_training.nnUNet_variants.architectural_variants.nnUNetTrainerV2_noDeepSupervision.nnUNetTrainerV2_noDeepSupervision.__init__", "nnunet.training.loss_functions.crossentropy.RobustCrossEntropyLoss"], "methods", ["home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.common.layers.ResizeImage.__init__"], ["    ", "def", "__init__", "(", "self", ",", "plans_file", ",", "fold", ",", "output_folder", "=", "None", ",", "dataset_directory", "=", "None", ",", "batch_dice", "=", "True", ",", "stage", "=", "None", ",", "\n", "unpack_data", "=", "True", ",", "deterministic", "=", "True", ",", "fp16", "=", "False", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "plans_file", ",", "fold", ",", "output_folder", ",", "dataset_directory", ",", "batch_dice", ",", "stage", ",", "unpack_data", ",", "\n", "deterministic", ",", "fp16", ")", "\n", "self", ".", "max_num_epochs", "=", "5", "\n", "self", ".", "loss", "=", "RobustCrossEntropyLoss", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.profiling.nnUNetTrainerV2_dummyLoad.nnUNetTrainerV2_5epochs_dummyLoadCEnoDS.validate": [[79, 84], ["None"], "methods", ["None"], ["", "def", "validate", "(", "self", ",", "do_mirroring", ":", "bool", "=", "True", ",", "use_sliding_window", ":", "bool", "=", "True", ",", "step_size", ":", "float", "=", "0.5", ",", "\n", "save_softmax", ":", "bool", "=", "True", ",", "use_gaussian", ":", "bool", "=", "True", ",", "overwrite", ":", "bool", "=", "True", ",", "\n", "validation_folder_name", ":", "str", "=", "'validation_raw'", ",", "debug", ":", "bool", "=", "False", ",", "all_in_gpu", ":", "bool", "=", "False", ",", "\n", "segmentation_export_kwargs", "=", "None", ")", ":", "\n", "        ", "pass", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.profiling.nnUNetTrainerV2_dummyLoad.nnUNetTrainerV2_5epochs_dummyLoadCEnoDS.predict_preprocessed_data_return_seg_and_softmax": [[85, 92], ["None"], "methods", ["None"], ["", "def", "predict_preprocessed_data_return_seg_and_softmax", "(", "self", ",", "data", ":", "np", ".", "ndarray", ",", "do_mirroring", ":", "bool", "=", "True", ",", "\n", "mirror_axes", ":", "Tuple", "[", "int", "]", "=", "None", ",", "\n", "use_sliding_window", ":", "bool", "=", "True", ",", "step_size", ":", "float", "=", "0.5", ",", "\n", "use_gaussian", ":", "bool", "=", "True", ",", "pad_border_mode", ":", "str", "=", "'constant'", ",", "\n", "pad_kwargs", ":", "dict", "=", "None", ",", "all_in_gpu", ":", "bool", "=", "True", ",", "\n", "verbose", ":", "bool", "=", "True", ",", "mixed_precision", "=", "True", ")", "->", "Tuple", "[", "np", ".", "ndarray", ",", "np", ".", "ndarray", "]", ":", "\n", "        ", "pass", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.profiling.nnUNetTrainerV2_dummyLoad.nnUNetTrainerV2_5epochs_dummyLoadCEnoDS.save_checkpoint": [[93, 95], ["None"], "methods", ["None"], ["", "def", "save_checkpoint", "(", "self", ",", "fname", ",", "save_optimizer", "=", "True", ")", ":", "\n", "        ", "pass", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.profiling.nnUNetTrainerV2_dummyLoad.nnUNetTrainerV2_5epochs_dummyLoadCEnoDS.initialize": [[96, 101], ["super().initialize", "torch.rand().float().cuda", "torch.round().long().cuda", "torch.rand().float", "torch.round().long", "torch.rand", "torch.round", "torch.rand"], "methods", ["home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.BraTS_trainer.nnUNetTrainerV2BraTSRegions.nnUNetTrainerV2BraTSRegions_DA3_BN.initialize"], ["", "def", "initialize", "(", "self", ",", "training", "=", "True", ",", "force_load_plans", "=", "False", ")", ":", "\n", "        ", "super", "(", ")", ".", "initialize", "(", "training", ",", "force_load_plans", ")", "\n", "self", ".", "some_batch", "=", "torch", ".", "rand", "(", "(", "self", ".", "batch_size", ",", "self", ".", "num_input_channels", ",", "*", "self", ".", "patch_size", ")", ")", ".", "float", "(", ")", ".", "cuda", "(", ")", "\n", "\n", "self", ".", "some_gt", "=", "torch", ".", "round", "(", "torch", ".", "rand", "(", "(", "self", ".", "batch_size", ",", "*", "self", ".", "patch_size", ")", ")", "*", "(", "self", ".", "num_classes", "-", "1", ")", ")", ".", "long", "(", ")", ".", "cuda", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.profiling.nnUNetTrainerV2_dummyLoad.nnUNetTrainerV2_5epochs_dummyLoadCEnoDS.run_iteration": [[102, 127], ["nnUNetTrainerV2_dummyLoad.nnUNetTrainerV2_5epochs_dummyLoadCEnoDS.optimizer.zero_grad", "nnUNetTrainerV2_dummyLoad.nnUNetTrainerV2_5epochs_dummyLoadCEnoDS.network", "nnUNetTrainerV2_dummyLoad.nnUNetTrainerV2_5epochs_dummyLoadCEnoDS.loss", "nnUNetTrainerV2_dummyLoad.nnUNetTrainerV2_5epochs_dummyLoadCEnoDS.detach().cpu().numpy", "nnUNetTrainerV2_dummyLoad.nnUNetTrainerV2_5epochs_dummyLoadCEnoDS.run_online_evaluation", "torch.nn.utils.clip_grad_norm_", "nnUNetTrainerV2_dummyLoad.nnUNetTrainerV2_5epochs_dummyLoadCEnoDS.optimizer.step", "nnUNetTrainerV2_dummyLoad.nnUNetTrainerV2_5epochs_dummyLoadCEnoDS.backward", "nnUNetTrainerV2_dummyLoad.nnUNetTrainerV2_5epochs_dummyLoadCEnoDS.network.parameters", "nnUNetTrainerV2_dummyLoad.nnUNetTrainerV2_5epochs_dummyLoadCEnoDS.detach().cpu", "torch.cuda.is_available", "amp.scale_loss", "scaled_loss.backward", "nnUNetTrainerV2_dummyLoad.nnUNetTrainerV2_5epochs_dummyLoadCEnoDS.detach"], "methods", ["home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.BraTS_trainer.nnUNetTrainerV2BraTSRegions.nnUNetTrainerV2BraTSRegions_DA3_BN.run_online_evaluation", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.optimizer.ranger.Ranger.step", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.utilities.distributed.awesome_allgather_function.backward", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.utilities.distributed.awesome_allgather_function.backward"], ["", "def", "run_iteration", "(", "self", ",", "data_generator", ",", "do_backprop", "=", "True", ",", "run_online_evaluation", "=", "False", ")", ":", "\n", "        ", "data", "=", "self", ".", "some_batch", "\n", "target", "=", "self", ".", "some_gt", "\n", "\n", "self", ".", "optimizer", ".", "zero_grad", "(", ")", "\n", "\n", "output", "=", "self", ".", "network", "(", "data", ")", "\n", "\n", "del", "data", "\n", "loss", "=", "self", ".", "loss", "(", "output", ",", "target", ")", "\n", "\n", "if", "run_online_evaluation", ":", "\n", "            ", "self", ".", "run_online_evaluation", "(", "output", ",", "target", ")", "\n", "", "del", "target", "\n", "\n", "if", "do_backprop", ":", "\n", "            ", "if", "not", "self", ".", "fp16", "or", "amp", "is", "None", "or", "not", "torch", ".", "cuda", ".", "is_available", "(", ")", ":", "\n", "                ", "loss", ".", "backward", "(", ")", "\n", "", "else", ":", "\n", "                ", "with", "amp", ".", "scale_loss", "(", "loss", ",", "self", ".", "optimizer", ")", "as", "scaled_loss", ":", "\n", "                    ", "scaled_loss", ".", "backward", "(", ")", "\n", "", "", "_", "=", "clip_grad_norm_", "(", "self", ".", "network", ".", "parameters", "(", ")", ",", "12", ")", "\n", "self", ".", "optimizer", ".", "step", "(", ")", "\n", "\n", "", "return", "loss", ".", "detach", "(", ")", ".", "cpu", "(", ")", ".", "numpy", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.profiling.nnUNetTrainerV2_dummyLoad.nnUNetTrainerV2_5epochs_dummyLoadCEnoDS.run_online_evaluation": [[128, 130], ["None"], "methods", ["None"], ["", "def", "run_online_evaluation", "(", "self", ",", "output", ",", "target", ")", ":", "\n", "        ", "pass", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.profiling.nnUNetTrainerV2_dummyLoad.nnUNetTrainerV2_5epochs_dummyLoadCEnoDS.finish_online_evaluation": [[131, 133], ["None"], "methods", ["None"], ["", "def", "finish_online_evaluation", "(", "self", ")", ":", "\n", "        ", "pass", "\n", "", "", ""]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.profiling.nnUNetTrainerV2_2epochs.nnUNetTrainerV2_2epochs.__init__": [[28, 33], ["nnunet.training.network_training.nnUNetTrainerV2.nnUNetTrainerV2.__init__"], "methods", ["home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.common.layers.ResizeImage.__init__"], ["    ", "def", "__init__", "(", "self", ",", "plans_file", ",", "fold", ",", "output_folder", "=", "None", ",", "dataset_directory", "=", "None", ",", "batch_dice", "=", "True", ",", "stage", "=", "None", ",", "\n", "unpack_data", "=", "True", ",", "deterministic", "=", "True", ",", "fp16", "=", "False", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "plans_file", ",", "fold", ",", "output_folder", ",", "dataset_directory", ",", "batch_dice", ",", "stage", ",", "unpack_data", ",", "\n", "deterministic", ",", "fp16", ")", "\n", "self", ".", "max_num_epochs", "=", "2", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.profiling.nnUNetTrainerV2_2epochs.nnUNetTrainerV2_2epochs.validate": [[34, 39], ["None"], "methods", ["None"], ["", "def", "validate", "(", "self", ",", "do_mirroring", ":", "bool", "=", "True", ",", "use_sliding_window", ":", "bool", "=", "True", ",", "step_size", ":", "float", "=", "0.5", ",", "\n", "save_softmax", ":", "bool", "=", "True", ",", "use_gaussian", ":", "bool", "=", "True", ",", "overwrite", ":", "bool", "=", "True", ",", "\n", "validation_folder_name", ":", "str", "=", "'validation_raw'", ",", "debug", ":", "bool", "=", "False", ",", "all_in_gpu", ":", "bool", "=", "False", ",", "\n", "segmentation_export_kwargs", "=", "None", ")", ":", "\n", "        ", "pass", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.profiling.nnUNetTrainerV2_2epochs.nnUNetTrainerV2_2epochs.predict_preprocessed_data_return_seg_and_softmax": [[40, 47], ["None"], "methods", ["None"], ["", "def", "predict_preprocessed_data_return_seg_and_softmax", "(", "self", ",", "data", ":", "np", ".", "ndarray", ",", "do_mirroring", ":", "bool", "=", "True", ",", "\n", "mirror_axes", ":", "Tuple", "[", "int", "]", "=", "None", ",", "\n", "use_sliding_window", ":", "bool", "=", "True", ",", "step_size", ":", "float", "=", "0.5", ",", "\n", "use_gaussian", ":", "bool", "=", "True", ",", "pad_border_mode", ":", "str", "=", "'constant'", ",", "\n", "pad_kwargs", ":", "dict", "=", "None", ",", "all_in_gpu", ":", "bool", "=", "True", ",", "\n", "verbose", ":", "bool", "=", "True", ",", "mixed_precision", "=", "True", ")", "->", "Tuple", "[", "np", ".", "ndarray", ",", "np", ".", "ndarray", "]", ":", "\n", "        ", "pass", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.profiling.nnUNetTrainerV2_2epochs.nnUNetTrainerV2_2epochs.save_checkpoint": [[48, 50], ["None"], "methods", ["None"], ["", "def", "save_checkpoint", "(", "self", ",", "fname", ",", "save_optimizer", "=", "True", ")", ":", "\n", "        ", "pass", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.profiling.nnUNetTrainerV2_2epochs.nnUNetTrainerV2_5epochs.__init__": [[53, 58], ["nnunet.training.network_training.nnUNetTrainerV2.nnUNetTrainerV2.__init__"], "methods", ["home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.common.layers.ResizeImage.__init__"], ["    ", "def", "__init__", "(", "self", ",", "plans_file", ",", "fold", ",", "output_folder", "=", "None", ",", "dataset_directory", "=", "None", ",", "batch_dice", "=", "True", ",", "stage", "=", "None", ",", "\n", "unpack_data", "=", "True", ",", "deterministic", "=", "True", ",", "fp16", "=", "False", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "plans_file", ",", "fold", ",", "output_folder", ",", "dataset_directory", ",", "batch_dice", ",", "stage", ",", "unpack_data", ",", "\n", "deterministic", ",", "fp16", ")", "\n", "self", ".", "max_num_epochs", "=", "5", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.profiling.nnUNetTrainerV2_2epochs.nnUNetTrainerV2_5epochs.validate": [[59, 64], ["None"], "methods", ["None"], ["", "def", "validate", "(", "self", ",", "do_mirroring", ":", "bool", "=", "True", ",", "use_sliding_window", ":", "bool", "=", "True", ",", "step_size", ":", "float", "=", "0.5", ",", "\n", "save_softmax", ":", "bool", "=", "True", ",", "use_gaussian", ":", "bool", "=", "True", ",", "overwrite", ":", "bool", "=", "True", ",", "\n", "validation_folder_name", ":", "str", "=", "'validation_raw'", ",", "debug", ":", "bool", "=", "False", ",", "all_in_gpu", ":", "bool", "=", "False", ",", "\n", "segmentation_export_kwargs", "=", "None", ")", ":", "\n", "        ", "pass", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.profiling.nnUNetTrainerV2_2epochs.nnUNetTrainerV2_5epochs.predict_preprocessed_data_return_seg_and_softmax": [[65, 72], ["None"], "methods", ["None"], ["", "def", "predict_preprocessed_data_return_seg_and_softmax", "(", "self", ",", "data", ":", "np", ".", "ndarray", ",", "do_mirroring", ":", "bool", "=", "True", ",", "\n", "mirror_axes", ":", "Tuple", "[", "int", "]", "=", "None", ",", "\n", "use_sliding_window", ":", "bool", "=", "True", ",", "step_size", ":", "float", "=", "0.5", ",", "\n", "use_gaussian", ":", "bool", "=", "True", ",", "pad_border_mode", ":", "str", "=", "'constant'", ",", "\n", "pad_kwargs", ":", "dict", "=", "None", ",", "all_in_gpu", ":", "bool", "=", "True", ",", "\n", "verbose", ":", "bool", "=", "True", ",", "mixed_precision", "=", "True", ")", "->", "Tuple", "[", "np", ".", "ndarray", ",", "np", ".", "ndarray", "]", ":", "\n", "        ", "pass", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.profiling.nnUNetTrainerV2_2epochs.nnUNetTrainerV2_5epochs.save_checkpoint": [[73, 75], ["None"], "methods", ["None"], ["", "def", "save_checkpoint", "(", "self", ",", "fname", ",", "save_optimizer", "=", "True", ")", ":", "\n", "        ", "pass", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.profiling.nnUNetTrainerV2_2epochs.nnUNetTrainerV2_5epochs_CEnoDS.__init__": [[78, 84], ["nnunet.training.network_training.nnUNet_variants.architectural_variants.nnUNetTrainerV2_noDeepSupervision.nnUNetTrainerV2_noDeepSupervision.__init__", "nnunet.training.loss_functions.crossentropy.RobustCrossEntropyLoss"], "methods", ["home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.common.layers.ResizeImage.__init__"], ["    ", "def", "__init__", "(", "self", ",", "plans_file", ",", "fold", ",", "output_folder", "=", "None", ",", "dataset_directory", "=", "None", ",", "batch_dice", "=", "True", ",", "stage", "=", "None", ",", "\n", "unpack_data", "=", "True", ",", "deterministic", "=", "True", ",", "fp16", "=", "False", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "plans_file", ",", "fold", ",", "output_folder", ",", "dataset_directory", ",", "batch_dice", ",", "stage", ",", "unpack_data", ",", "\n", "deterministic", ",", "fp16", ")", "\n", "self", ".", "max_num_epochs", "=", "5", "\n", "self", ".", "loss", "=", "RobustCrossEntropyLoss", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.profiling.nnUNetTrainerV2_2epochs.nnUNetTrainerV2_5epochs_CEnoDS.validate": [[85, 90], ["None"], "methods", ["None"], ["", "def", "validate", "(", "self", ",", "do_mirroring", ":", "bool", "=", "True", ",", "use_sliding_window", ":", "bool", "=", "True", ",", "step_size", ":", "float", "=", "0.5", ",", "\n", "save_softmax", ":", "bool", "=", "True", ",", "use_gaussian", ":", "bool", "=", "True", ",", "overwrite", ":", "bool", "=", "True", ",", "\n", "validation_folder_name", ":", "str", "=", "'validation_raw'", ",", "debug", ":", "bool", "=", "False", ",", "all_in_gpu", ":", "bool", "=", "False", ",", "\n", "segmentation_export_kwargs", "=", "None", ")", ":", "\n", "        ", "pass", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.profiling.nnUNetTrainerV2_2epochs.nnUNetTrainerV2_5epochs_CEnoDS.predict_preprocessed_data_return_seg_and_softmax": [[91, 98], ["None"], "methods", ["None"], ["", "def", "predict_preprocessed_data_return_seg_and_softmax", "(", "self", ",", "data", ":", "np", ".", "ndarray", ",", "do_mirroring", ":", "bool", "=", "True", ",", "\n", "mirror_axes", ":", "Tuple", "[", "int", "]", "=", "None", ",", "\n", "use_sliding_window", ":", "bool", "=", "True", ",", "step_size", ":", "float", "=", "0.5", ",", "\n", "use_gaussian", ":", "bool", "=", "True", ",", "pad_border_mode", ":", "str", "=", "'constant'", ",", "\n", "pad_kwargs", ":", "dict", "=", "None", ",", "all_in_gpu", ":", "bool", "=", "True", ",", "\n", "verbose", ":", "bool", "=", "True", ",", "mixed_precision", "=", "True", ")", "->", "Tuple", "[", "np", ".", "ndarray", ",", "np", ".", "ndarray", "]", ":", "\n", "        ", "pass", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.profiling.nnUNetTrainerV2_2epochs.nnUNetTrainerV2_5epochs_CEnoDS.save_checkpoint": [[99, 101], ["None"], "methods", ["None"], ["", "def", "save_checkpoint", "(", "self", ",", "fname", ",", "save_optimizer", "=", "True", ")", ":", "\n", "        ", "pass", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.profiling.nnUNetTrainerV2_2epochs.nnUNetTrainerV2_5epochs_CEnoDS.run_iteration": [[102, 144], ["next", "nnunet.utilities.to_torch.maybe_to_torch", "torch.cuda.is_available", "nnUNetTrainerV2_2epochs.nnUNetTrainerV2_5epochs_CEnoDS.optimizer.zero_grad", "nnUNetTrainerV2_2epochs.nnUNetTrainerV2_5epochs_CEnoDS.detach().cpu().numpy", "nnunet.utilities.to_torch.maybe_to_torch().long", "nnunet.utilities.to_torch.to_cuda", "nnunet.utilities.to_torch.to_cuda", "nnUNetTrainerV2_2epochs.nnUNetTrainerV2_5epochs_CEnoDS.network", "nnUNetTrainerV2_2epochs.nnUNetTrainerV2_5epochs_CEnoDS.loss", "nnUNetTrainerV2_2epochs.nnUNetTrainerV2_5epochs_CEnoDS.run_online_evaluation", "torch.cuda.amp.autocast", "nnUNetTrainerV2_2epochs.nnUNetTrainerV2_5epochs_CEnoDS.network", "nnUNetTrainerV2_2epochs.nnUNetTrainerV2_5epochs_CEnoDS.loss", "nnUNetTrainerV2_2epochs.nnUNetTrainerV2_5epochs_CEnoDS.amp_grad_scaler.scale().backward", "nnUNetTrainerV2_2epochs.nnUNetTrainerV2_5epochs_CEnoDS.amp_grad_scaler.unscale_", "torch.nn.utils.clip_grad_norm_", "nnUNetTrainerV2_2epochs.nnUNetTrainerV2_5epochs_CEnoDS.amp_grad_scaler.step", "nnUNetTrainerV2_2epochs.nnUNetTrainerV2_5epochs_CEnoDS.amp_grad_scaler.update", "nnUNetTrainerV2_2epochs.nnUNetTrainerV2_5epochs_CEnoDS.backward", "torch.nn.utils.clip_grad_norm_", "nnUNetTrainerV2_2epochs.nnUNetTrainerV2_5epochs_CEnoDS.optimizer.step", "nnUNetTrainerV2_2epochs.nnUNetTrainerV2_5epochs_CEnoDS.detach().cpu", "nnunet.utilities.to_torch.maybe_to_torch", "nnUNetTrainerV2_2epochs.nnUNetTrainerV2_5epochs_CEnoDS.network.parameters", "nnUNetTrainerV2_2epochs.nnUNetTrainerV2_5epochs_CEnoDS.network.parameters", "nnUNetTrainerV2_2epochs.nnUNetTrainerV2_5epochs_CEnoDS.amp_grad_scaler.scale", "nnUNetTrainerV2_2epochs.nnUNetTrainerV2_5epochs_CEnoDS.detach"], "methods", ["home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.utilities.to_torch.maybe_to_torch", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.utilities.to_torch.to_cuda", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.utilities.to_torch.to_cuda", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.BraTS_trainer.nnUNetTrainerV2BraTSRegions.nnUNetTrainerV2BraTSRegions_DA3_BN.run_online_evaluation", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.utilities.distributed.awesome_allgather_function.backward", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.optimizer.ranger.Ranger.step", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.utilities.distributed.awesome_allgather_function.backward", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.optimizer.ranger.Ranger.step", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.utilities.to_torch.maybe_to_torch"], ["", "def", "run_iteration", "(", "self", ",", "data_generator", ",", "do_backprop", "=", "True", ",", "run_online_evaluation", "=", "False", ")", ":", "\n", "        ", "data_dict", "=", "next", "(", "data_generator", ")", "\n", "data", "=", "data_dict", "[", "'data'", "]", "\n", "target", "=", "data_dict", "[", "'target'", "]", "\n", "\n", "data", "=", "maybe_to_torch", "(", "data", ")", "\n", "target", "=", "maybe_to_torch", "(", "target", ")", ".", "long", "(", ")", "[", ":", ",", "0", "]", "\n", "\n", "if", "torch", ".", "cuda", ".", "is_available", "(", ")", ":", "\n", "            ", "data", "=", "to_cuda", "(", "data", ")", "\n", "target", "=", "to_cuda", "(", "target", ")", "\n", "\n", "", "self", ".", "optimizer", ".", "zero_grad", "(", ")", "\n", "\n", "if", "self", ".", "fp16", ":", "\n", "            ", "with", "autocast", "(", ")", ":", "\n", "                ", "output", "=", "self", ".", "network", "(", "data", ")", "\n", "del", "data", "\n", "l", "=", "self", ".", "loss", "(", "output", ",", "target", ")", "\n", "\n", "", "if", "do_backprop", ":", "\n", "                ", "self", ".", "amp_grad_scaler", ".", "scale", "(", "l", ")", ".", "backward", "(", ")", "\n", "self", ".", "amp_grad_scaler", ".", "unscale_", "(", "self", ".", "optimizer", ")", "\n", "torch", ".", "nn", ".", "utils", ".", "clip_grad_norm_", "(", "self", ".", "network", ".", "parameters", "(", ")", ",", "12", ")", "\n", "self", ".", "amp_grad_scaler", ".", "step", "(", "self", ".", "optimizer", ")", "\n", "self", ".", "amp_grad_scaler", ".", "update", "(", ")", "\n", "", "", "else", ":", "\n", "            ", "output", "=", "self", ".", "network", "(", "data", ")", "\n", "del", "data", "\n", "l", "=", "self", ".", "loss", "(", "output", ",", "target", ")", "\n", "\n", "if", "do_backprop", ":", "\n", "                ", "l", ".", "backward", "(", ")", "\n", "torch", ".", "nn", ".", "utils", ".", "clip_grad_norm_", "(", "self", ".", "network", ".", "parameters", "(", ")", ",", "12", ")", "\n", "self", ".", "optimizer", ".", "step", "(", ")", "\n", "\n", "", "", "if", "run_online_evaluation", ":", "\n", "            ", "self", ".", "run_online_evaluation", "(", "output", ",", "target", ")", "\n", "\n", "", "del", "target", "\n", "\n", "return", "l", ".", "detach", "(", ")", ".", "cpu", "(", ")", ".", "numpy", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.profiling.nnUNetTrainerV2_2epochs.nnUNetTrainerV2_5epochs_CEnoDS.run_online_evaluation": [[145, 147], ["None"], "methods", ["None"], ["", "def", "run_online_evaluation", "(", "self", ",", "output", ",", "target", ")", ":", "\n", "        ", "pass", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.profiling.nnUNetTrainerV2_2epochs.nnUNetTrainerV2_5epochs_CEnoDS.finish_online_evaluation": [[148, 150], ["None"], "methods", ["None"], ["", "def", "finish_online_evaluation", "(", "self", ")", ":", "\n", "        ", "pass", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.profiling.nnUNetTrainerV2_2epochs.nnUNetTrainerV2_5epochs_noDS.__init__": [[153, 158], ["nnunet.training.network_training.nnUNet_variants.architectural_variants.nnUNetTrainerV2_noDeepSupervision.nnUNetTrainerV2_noDeepSupervision.__init__"], "methods", ["home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.common.layers.ResizeImage.__init__"], ["    ", "def", "__init__", "(", "self", ",", "plans_file", ",", "fold", ",", "output_folder", "=", "None", ",", "dataset_directory", "=", "None", ",", "batch_dice", "=", "True", ",", "stage", "=", "None", ",", "\n", "unpack_data", "=", "True", ",", "deterministic", "=", "True", ",", "fp16", "=", "False", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "plans_file", ",", "fold", ",", "output_folder", ",", "dataset_directory", ",", "batch_dice", ",", "stage", ",", "unpack_data", ",", "\n", "deterministic", ",", "fp16", ")", "\n", "self", ".", "max_num_epochs", "=", "5", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.profiling.nnUNetTrainerV2_2epochs.nnUNetTrainerV2_5epochs_noDS.validate": [[159, 164], ["None"], "methods", ["None"], ["", "def", "validate", "(", "self", ",", "do_mirroring", ":", "bool", "=", "True", ",", "use_sliding_window", ":", "bool", "=", "True", ",", "step_size", ":", "float", "=", "0.5", ",", "\n", "save_softmax", ":", "bool", "=", "True", ",", "use_gaussian", ":", "bool", "=", "True", ",", "overwrite", ":", "bool", "=", "True", ",", "\n", "validation_folder_name", ":", "str", "=", "'validation_raw'", ",", "debug", ":", "bool", "=", "False", ",", "all_in_gpu", ":", "bool", "=", "False", ",", "\n", "segmentation_export_kwargs", "=", "None", ")", ":", "\n", "        ", "pass", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.profiling.nnUNetTrainerV2_2epochs.nnUNetTrainerV2_5epochs_noDS.predict_preprocessed_data_return_seg_and_softmax": [[165, 172], ["None"], "methods", ["None"], ["", "def", "predict_preprocessed_data_return_seg_and_softmax", "(", "self", ",", "data", ":", "np", ".", "ndarray", ",", "do_mirroring", ":", "bool", "=", "True", ",", "\n", "mirror_axes", ":", "Tuple", "[", "int", "]", "=", "None", ",", "\n", "use_sliding_window", ":", "bool", "=", "True", ",", "step_size", ":", "float", "=", "0.5", ",", "\n", "use_gaussian", ":", "bool", "=", "True", ",", "pad_border_mode", ":", "str", "=", "'constant'", ",", "\n", "pad_kwargs", ":", "dict", "=", "None", ",", "all_in_gpu", ":", "bool", "=", "True", ",", "\n", "verbose", ":", "bool", "=", "True", ",", "mixed_precision", "=", "True", ")", "->", "Tuple", "[", "np", ".", "ndarray", ",", "np", ".", "ndarray", "]", ":", "\n", "        ", "pass", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.profiling.nnUNetTrainerV2_2epochs.nnUNetTrainerV2_5epochs_noDS.save_checkpoint": [[173, 175], ["None"], "methods", ["None"], ["", "def", "save_checkpoint", "(", "self", ",", "fname", ",", "save_optimizer", "=", "True", ")", ":", "\n", "        ", "pass", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.profiling.nnUNetTrainerV2_2epochs.nnUNetTrainerV2_5epochs_noDS.run_iteration": [[176, 218], ["next", "nnunet.utilities.to_torch.maybe_to_torch", "nnunet.utilities.to_torch.maybe_to_torch", "torch.cuda.is_available", "nnUNetTrainerV2_2epochs.nnUNetTrainerV2_5epochs_noDS.optimizer.zero_grad", "nnUNetTrainerV2_2epochs.nnUNetTrainerV2_5epochs_noDS.detach().cpu().numpy", "nnunet.utilities.to_torch.to_cuda", "nnunet.utilities.to_torch.to_cuda", "nnUNetTrainerV2_2epochs.nnUNetTrainerV2_5epochs_noDS.network", "nnUNetTrainerV2_2epochs.nnUNetTrainerV2_5epochs_noDS.loss", "nnUNetTrainerV2_2epochs.nnUNetTrainerV2_5epochs_noDS.run_online_evaluation", "torch.cuda.amp.autocast", "nnUNetTrainerV2_2epochs.nnUNetTrainerV2_5epochs_noDS.network", "nnUNetTrainerV2_2epochs.nnUNetTrainerV2_5epochs_noDS.loss", "nnUNetTrainerV2_2epochs.nnUNetTrainerV2_5epochs_noDS.amp_grad_scaler.scale().backward", "nnUNetTrainerV2_2epochs.nnUNetTrainerV2_5epochs_noDS.amp_grad_scaler.unscale_", "torch.nn.utils.clip_grad_norm_", "nnUNetTrainerV2_2epochs.nnUNetTrainerV2_5epochs_noDS.amp_grad_scaler.step", "nnUNetTrainerV2_2epochs.nnUNetTrainerV2_5epochs_noDS.amp_grad_scaler.update", "nnUNetTrainerV2_2epochs.nnUNetTrainerV2_5epochs_noDS.backward", "torch.nn.utils.clip_grad_norm_", "nnUNetTrainerV2_2epochs.nnUNetTrainerV2_5epochs_noDS.optimizer.step", "nnUNetTrainerV2_2epochs.nnUNetTrainerV2_5epochs_noDS.detach().cpu", "nnUNetTrainerV2_2epochs.nnUNetTrainerV2_5epochs_noDS.network.parameters", "nnUNetTrainerV2_2epochs.nnUNetTrainerV2_5epochs_noDS.network.parameters", "nnUNetTrainerV2_2epochs.nnUNetTrainerV2_5epochs_noDS.amp_grad_scaler.scale", "nnUNetTrainerV2_2epochs.nnUNetTrainerV2_5epochs_noDS.detach"], "methods", ["home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.utilities.to_torch.maybe_to_torch", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.utilities.to_torch.maybe_to_torch", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.utilities.to_torch.to_cuda", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.utilities.to_torch.to_cuda", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.BraTS_trainer.nnUNetTrainerV2BraTSRegions.nnUNetTrainerV2BraTSRegions_DA3_BN.run_online_evaluation", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.utilities.distributed.awesome_allgather_function.backward", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.optimizer.ranger.Ranger.step", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.utilities.distributed.awesome_allgather_function.backward", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.optimizer.ranger.Ranger.step"], ["", "def", "run_iteration", "(", "self", ",", "data_generator", ",", "do_backprop", "=", "True", ",", "run_online_evaluation", "=", "False", ")", ":", "\n", "        ", "data_dict", "=", "next", "(", "data_generator", ")", "\n", "data", "=", "data_dict", "[", "'data'", "]", "\n", "target", "=", "data_dict", "[", "'target'", "]", "\n", "\n", "data", "=", "maybe_to_torch", "(", "data", ")", "\n", "target", "=", "maybe_to_torch", "(", "target", ")", "\n", "\n", "if", "torch", ".", "cuda", ".", "is_available", "(", ")", ":", "\n", "            ", "data", "=", "to_cuda", "(", "data", ")", "\n", "target", "=", "to_cuda", "(", "target", ")", "\n", "\n", "", "self", ".", "optimizer", ".", "zero_grad", "(", ")", "\n", "\n", "if", "self", ".", "fp16", ":", "\n", "            ", "with", "autocast", "(", ")", ":", "\n", "                ", "output", "=", "self", ".", "network", "(", "data", ")", "\n", "del", "data", "\n", "l", "=", "self", ".", "loss", "(", "output", ",", "target", ")", "\n", "\n", "", "if", "do_backprop", ":", "\n", "                ", "self", ".", "amp_grad_scaler", ".", "scale", "(", "l", ")", ".", "backward", "(", ")", "\n", "self", ".", "amp_grad_scaler", ".", "unscale_", "(", "self", ".", "optimizer", ")", "\n", "torch", ".", "nn", ".", "utils", ".", "clip_grad_norm_", "(", "self", ".", "network", ".", "parameters", "(", ")", ",", "12", ")", "\n", "self", ".", "amp_grad_scaler", ".", "step", "(", "self", ".", "optimizer", ")", "\n", "self", ".", "amp_grad_scaler", ".", "update", "(", ")", "\n", "", "", "else", ":", "\n", "            ", "output", "=", "self", ".", "network", "(", "data", ")", "\n", "del", "data", "\n", "l", "=", "self", ".", "loss", "(", "output", ",", "target", ")", "\n", "\n", "if", "do_backprop", ":", "\n", "                ", "l", ".", "backward", "(", ")", "\n", "torch", ".", "nn", ".", "utils", ".", "clip_grad_norm_", "(", "self", ".", "network", ".", "parameters", "(", ")", ",", "12", ")", "\n", "self", ".", "optimizer", ".", "step", "(", ")", "\n", "\n", "", "", "if", "run_online_evaluation", ":", "\n", "            ", "self", ".", "run_online_evaluation", "(", "output", ",", "target", ")", "\n", "\n", "", "del", "target", "\n", "\n", "return", "l", ".", "detach", "(", ")", ".", "cpu", "(", ")", ".", "numpy", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.profiling.nnUNetTrainerV2_2epochs.nnUNetTrainerV2_5epochs_noDS.run_online_evaluation": [[219, 221], ["None"], "methods", ["None"], ["", "def", "run_online_evaluation", "(", "self", ",", "output", ",", "target", ")", ":", "\n", "        ", "pass", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.profiling.nnUNetTrainerV2_2epochs.nnUNetTrainerV2_5epochs_noDS.finish_online_evaluation": [[222, 224], ["None"], "methods", ["None"], ["", "def", "finish_online_evaluation", "(", "self", ")", ":", "\n", "        ", "pass", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.profiling.nnUNetTrainerV2_2epochs.nnUNetTrainerV2_DDP_5epochs.__init__": [[228, 234], ["nnunet.training.network_training.nnUNetTrainerV2_DDP.nnUNetTrainerV2_DDP.__init__"], "methods", ["home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.common.layers.ResizeImage.__init__"], ["    ", "def", "__init__", "(", "self", ",", "plans_file", ",", "fold", ",", "local_rank", ",", "output_folder", "=", "None", ",", "dataset_directory", "=", "None", ",", "batch_dice", "=", "True", ",", "\n", "stage", "=", "None", ",", "\n", "unpack_data", "=", "True", ",", "deterministic", "=", "True", ",", "distribute_batch_size", "=", "False", ",", "fp16", "=", "False", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "plans_file", ",", "fold", ",", "local_rank", ",", "output_folder", ",", "dataset_directory", ",", "batch_dice", ",", "stage", ",", "unpack_data", ",", "\n", "deterministic", ",", "distribute_batch_size", ",", "fp16", ")", "\n", "self", ".", "max_num_epochs", "=", "5", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.profiling.nnUNetTrainerV2_2epochs.nnUNetTrainerV2_DDP_5epochs.validate": [[235, 240], ["None"], "methods", ["None"], ["", "def", "validate", "(", "self", ",", "do_mirroring", ":", "bool", "=", "True", ",", "use_sliding_window", ":", "bool", "=", "True", ",", "step_size", ":", "float", "=", "0.5", ",", "\n", "save_softmax", ":", "bool", "=", "True", ",", "use_gaussian", ":", "bool", "=", "True", ",", "overwrite", ":", "bool", "=", "True", ",", "\n", "validation_folder_name", ":", "str", "=", "'validation_raw'", ",", "debug", ":", "bool", "=", "False", ",", "all_in_gpu", ":", "bool", "=", "False", ",", "\n", "segmentation_export_kwargs", "=", "None", ")", ":", "\n", "        ", "pass", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.profiling.nnUNetTrainerV2_2epochs.nnUNetTrainerV2_DDP_5epochs.predict_preprocessed_data_return_seg_and_softmax": [[241, 248], ["None"], "methods", ["None"], ["", "def", "predict_preprocessed_data_return_seg_and_softmax", "(", "self", ",", "data", ":", "np", ".", "ndarray", ",", "do_mirroring", ":", "bool", "=", "True", ",", "\n", "mirror_axes", ":", "Tuple", "[", "int", "]", "=", "None", ",", "\n", "use_sliding_window", ":", "bool", "=", "True", ",", "step_size", ":", "float", "=", "0.5", ",", "\n", "use_gaussian", ":", "bool", "=", "True", ",", "pad_border_mode", ":", "str", "=", "'constant'", ",", "\n", "pad_kwargs", ":", "dict", "=", "None", ",", "all_in_gpu", ":", "bool", "=", "True", ",", "\n", "verbose", ":", "bool", "=", "True", ",", "mixed_precision", "=", "True", ")", "->", "Tuple", "[", "np", ".", "ndarray", ",", "np", ".", "ndarray", "]", ":", "\n", "        ", "pass", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.profiling.nnUNetTrainerV2_2epochs.nnUNetTrainerV2_DDP_5epochs.save_checkpoint": [[249, 251], ["None"], "methods", ["None"], ["", "def", "save_checkpoint", "(", "self", ",", "fname", ",", "save_optimizer", "=", "True", ")", ":", "\n", "        ", "pass", "", "", "", ""]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.resampling.nnUNetTrainerV2_resample33.nnUNetTrainerV2_resample33.validate": [[21, 27], ["super().validate"], "methods", ["home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.BraTS_trainer.nnUNetTrainerV2BraTSRegions.nnUNetTrainerV2BraTSRegions_DA3_BN.validate"], ["    ", "def", "validate", "(", "self", ",", "do_mirroring", ":", "bool", "=", "True", ",", "use_sliding_window", ":", "bool", "=", "True", ",", "\n", "step_size", ":", "float", "=", "0.5", ",", "save_softmax", ":", "bool", "=", "True", ",", "use_gaussian", ":", "bool", "=", "True", ",", "overwrite", ":", "bool", "=", "True", ",", "\n", "validation_folder_name", ":", "str", "=", "'validation_raw'", ",", "debug", ":", "bool", "=", "False", ",", "all_in_gpu", ":", "bool", "=", "False", ",", "\n", "segmentation_export_kwargs", ":", "dict", "=", "None", ")", ":", "\n", "        ", "return", "super", "(", ")", ".", "validate", "(", "do_mirroring", ",", "use_sliding_window", ",", "step_size", ",", "save_softmax", ",", "use_gaussian", ",", "\n", "overwrite", ",", "validation_folder_name", ",", "debug", ",", "all_in_gpu", ",", "segmentation_export_kwargs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.resampling.nnUNetTrainerV2_resample33.nnUNetTrainerV2_resample33.preprocess_predict_nifti": [[28, 52], ["print", "nnUNetTrainerV2_resample33.nnUNetTrainerV2_resample33.preprocess_patient", "print", "pred.transpose.transpose.transpose", "print", "nnunet.inference.segmentation_export.save_segmentation_nifti_from_softmax", "print", "nnUNetTrainerV2_resample33.nnUNetTrainerV2_resample33.predict_preprocessed_data_return_seg_and_softmax"], "methods", ["home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.network_training.nnUNetTrainer.nnUNetTrainer.preprocess_patient", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.inference.segmentation_export.save_segmentation_nifti_from_softmax", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.architectural_variants.nnUNetTrainerV2_ResencUNet.nnUNetTrainerV2_ResencUNet.predict_preprocessed_data_return_seg_and_softmax"], ["", "def", "preprocess_predict_nifti", "(", "self", ",", "input_files", ",", "output_file", "=", "None", ",", "softmax_ouput_file", "=", "None", ",", "\n", "mixed_precision", ":", "bool", "=", "True", ")", ":", "\n", "        ", "\"\"\"\n        Use this to predict new data\n        :param input_files:\n        :param output_file:\n        :param softmax_ouput_file:\n        :param mixed_precision:\n        :return:\n        \"\"\"", "\n", "print", "(", "\"preprocessing...\"", ")", "\n", "d", ",", "s", ",", "properties", "=", "self", ".", "preprocess_patient", "(", "input_files", ")", "\n", "print", "(", "\"predicting...\"", ")", "\n", "pred", "=", "self", ".", "predict_preprocessed_data_return_seg_and_softmax", "(", "d", ",", "self", ".", "data_aug_params", "[", "\"do_mirror\"", "]", ",", "\n", "self", ".", "data_aug_params", "[", "'mirror_axes'", "]", ",", "True", ",", "0.5", ",", "\n", "True", ",", "'constant'", ",", "{", "'constant_values'", ":", "0", "}", ",", "\n", "self", ".", "patch_size", ",", "True", ",", "\n", "mixed_precision", "=", "mixed_precision", ")", "[", "1", "]", "\n", "pred", "=", "pred", ".", "transpose", "(", "[", "0", "]", "+", "[", "i", "+", "1", "for", "i", "in", "self", ".", "transpose_backward", "]", ")", "\n", "\n", "print", "(", "\"resampling to original spacing and nifti export...\"", ")", "\n", "save_segmentation_nifti_from_softmax", "(", "pred", ",", "output_file", ",", "properties", ",", "3", ",", "None", ",", "None", ",", "None", ",", "softmax_ouput_file", ",", "\n", "None", ",", "force_separate_z", "=", "False", ",", "interpolation_order_z", "=", "3", ")", "\n", "print", "(", "\"done\"", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.architectural_variants.nnUNetTrainerV2_ReLU_biasInSegOutput.nnUNetTrainerV2_ReLU_biasInSegOutput.initialize_network": [[23, 47], ["nnunet.network_architecture.generic_UNet.Generic_UNet", "torch.cuda.is_available", "len", "nnunet.network_architecture.initialization.InitWeights_He", "nnUNetTrainerV2_ReLU_biasInSegOutput.nnUNetTrainerV2_ReLU_biasInSegOutput.network.cuda"], "methods", ["None"], ["    ", "def", "initialize_network", "(", "self", ")", ":", "\n", "        ", "if", "self", ".", "threeD", ":", "\n", "            ", "conv_op", "=", "nn", ".", "Conv3d", "\n", "dropout_op", "=", "nn", ".", "Dropout3d", "\n", "norm_op", "=", "nn", ".", "InstanceNorm3d", "\n", "\n", "", "else", ":", "\n", "            ", "conv_op", "=", "nn", ".", "Conv2d", "\n", "dropout_op", "=", "nn", ".", "Dropout2d", "\n", "norm_op", "=", "nn", ".", "InstanceNorm2d", "\n", "\n", "", "norm_op_kwargs", "=", "{", "'eps'", ":", "1e-5", ",", "'affine'", ":", "True", "}", "\n", "dropout_op_kwargs", "=", "{", "'p'", ":", "0", ",", "'inplace'", ":", "True", "}", "\n", "net_nonlin", "=", "nn", ".", "ReLU", "\n", "net_nonlin_kwargs", "=", "{", "'inplace'", ":", "True", "}", "\n", "self", ".", "network", "=", "Generic_UNet", "(", "self", ".", "num_input_channels", ",", "self", ".", "base_num_features", ",", "self", ".", "num_classes", ",", "\n", "len", "(", "self", ".", "net_num_pool_op_kernel_sizes", ")", ",", "\n", "self", ".", "conv_per_stage", ",", "2", ",", "conv_op", ",", "norm_op", ",", "norm_op_kwargs", ",", "dropout_op", ",", "dropout_op_kwargs", ",", "\n", "net_nonlin", ",", "net_nonlin_kwargs", ",", "True", ",", "False", ",", "lambda", "x", ":", "x", ",", "InitWeights_He", "(", "0", ")", ",", "\n", "self", ".", "net_num_pool_op_kernel_sizes", ",", "self", ".", "net_conv_kernel_sizes", ",", "False", ",", "True", ",", "True", ",", "\n", "seg_output_use_bias", "=", "True", ")", "\n", "if", "torch", ".", "cuda", ".", "is_available", "(", ")", ":", "\n", "            ", "self", ".", "network", ".", "cuda", "(", ")", "\n", "", "self", ".", "network", ".", "inference_apply_nonlin", "=", "softmax_helper", "\n", "", "", ""]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.architectural_variants.nnUNetTrainerV2_ReLU.nnUNetTrainerV2_ReLU.initialize_network": [[23, 46], ["nnunet.network_architecture.generic_UNet.Generic_UNet", "torch.cuda.is_available", "len", "nnunet.network_architecture.initialization.InitWeights_He", "nnUNetTrainerV2_ReLU.nnUNetTrainerV2_ReLU.network.cuda"], "methods", ["None"], ["    ", "def", "initialize_network", "(", "self", ")", ":", "\n", "        ", "if", "self", ".", "threeD", ":", "\n", "            ", "conv_op", "=", "nn", ".", "Conv3d", "\n", "dropout_op", "=", "nn", ".", "Dropout3d", "\n", "norm_op", "=", "nn", ".", "InstanceNorm3d", "\n", "\n", "", "else", ":", "\n", "            ", "conv_op", "=", "nn", ".", "Conv2d", "\n", "dropout_op", "=", "nn", ".", "Dropout2d", "\n", "norm_op", "=", "nn", ".", "InstanceNorm2d", "\n", "\n", "", "norm_op_kwargs", "=", "{", "'eps'", ":", "1e-5", ",", "'affine'", ":", "True", "}", "\n", "dropout_op_kwargs", "=", "{", "'p'", ":", "0", ",", "'inplace'", ":", "True", "}", "\n", "net_nonlin", "=", "nn", ".", "ReLU", "\n", "net_nonlin_kwargs", "=", "{", "'inplace'", ":", "True", "}", "\n", "self", ".", "network", "=", "Generic_UNet", "(", "self", ".", "num_input_channels", ",", "self", ".", "base_num_features", ",", "self", ".", "num_classes", ",", "\n", "len", "(", "self", ".", "net_num_pool_op_kernel_sizes", ")", ",", "\n", "self", ".", "conv_per_stage", ",", "2", ",", "conv_op", ",", "norm_op", ",", "norm_op_kwargs", ",", "dropout_op", ",", "dropout_op_kwargs", ",", "\n", "net_nonlin", ",", "net_nonlin_kwargs", ",", "True", ",", "False", ",", "lambda", "x", ":", "x", ",", "InitWeights_He", "(", "0", ")", ",", "\n", "self", ".", "net_num_pool_op_kernel_sizes", ",", "self", ".", "net_conv_kernel_sizes", ",", "False", ",", "True", ",", "True", ")", "\n", "if", "torch", ".", "cuda", ".", "is_available", "(", ")", ":", "\n", "            ", "self", ".", "network", ".", "cuda", "(", ")", "\n", "", "self", ".", "network", ".", "inference_apply_nonlin", "=", "softmax_helper", "\n", "", "", ""]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.architectural_variants.nnUNetTrainerV2_softDeepSupervision.nnUNetTrainerV2_softDeepSupervision.__init__": [[34, 39], ["nnunet.training.network_training.nnUNetTrainerV2.nnUNetTrainerV2.__init__"], "methods", ["home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.common.layers.ResizeImage.__init__"], ["    ", "def", "__init__", "(", "self", ",", "plans_file", ",", "fold", ",", "output_folder", "=", "None", ",", "dataset_directory", "=", "None", ",", "batch_dice", "=", "True", ",", "stage", "=", "None", ",", "\n", "unpack_data", "=", "True", ",", "deterministic", "=", "True", ",", "fp16", "=", "False", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "plans_file", ",", "fold", ",", "output_folder", ",", "dataset_directory", ",", "batch_dice", ",", "stage", ",", "unpack_data", ",", "\n", "deterministic", ",", "fp16", ")", "\n", "self", ".", "loss", "=", "None", "# we take care of that later", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.architectural_variants.nnUNetTrainerV2_softDeepSupervision.nnUNetTrainerV2_softDeepSupervision.initialize": [[40, 115], ["batchgenerators.utilities.file_and_folder_operations.maybe_mkdir_p", "nnUNetTrainerV2_softDeepSupervision.nnUNetTrainerV2_softDeepSupervision.process_plans", "nnUNetTrainerV2_softDeepSupervision.nnUNetTrainerV2_softDeepSupervision.setup_DA_params", "len", "numpy.array", "numpy.array", "MyDSLoss4", "batchgenerators.utilities.file_and_folder_operations.join", "nnUNetTrainerV2_softDeepSupervision.nnUNetTrainerV2_softDeepSupervision.initialize_network", "nnUNetTrainerV2_softDeepSupervision.nnUNetTrainerV2_softDeepSupervision.initialize_optimizer_and_scheduler", "isinstance", "nnUNetTrainerV2_softDeepSupervision.nnUNetTrainerV2_softDeepSupervision.print_to_log_file", "nnUNetTrainerV2_softDeepSupervision.nnUNetTrainerV2_softDeepSupervision.load_plans_file", "numpy.array.sum", "RuntimeError", "nnUNetTrainerV2_softDeepSupervision.nnUNetTrainerV2_softDeepSupervision.get_basic_generators", "nnunet.training.data_augmentation.default_data_augmentation.get_moreDA_augmentation", "nnUNetTrainerV2_softDeepSupervision.nnUNetTrainerV2_softDeepSupervision.print_to_log_file", "nnUNetTrainerV2_softDeepSupervision.nnUNetTrainerV2_softDeepSupervision.print_to_log_file", "print", "nnunet.training.dataloading.dataset_loading.unpack_dataset", "print", "print", "range", "range", "str", "str", "list", "nnUNetTrainerV2_softDeepSupervision.nnUNetTrainerV2_softDeepSupervision.dataset_tr.keys", "nnUNetTrainerV2_softDeepSupervision.nnUNetTrainerV2_softDeepSupervision.dataset_val.keys"], "methods", ["home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.BraTS_trainer.nnUNetTrainerV2BraTSRegions.nnUNetTrainerV2BraTSRegions_DA3_BN.process_plans", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.BraTS_trainer.nnUNetTrainerV2BraTSRegions.nnUNetTrainerV2BraTSRegions_DA4_BN.setup_DA_params", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.BraTS_trainer.nnUNetTrainerV2BraTSRegions.nnUNetTrainerV2BraTSRegions_DA3.initialize_network", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.optimizer_and_lr.nnUNetTrainerV2_momentum09in2D.nnUNetTrainerV2_momentum09in2D.initialize_optimizer_and_scheduler", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.network_training.nnUNetTrainerV2_DDP.nnUNetTrainerV2_DDP.print_to_log_file", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.network_training.nnUNetTrainer.nnUNetTrainer.load_plans_file", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.nnUNet_variants.nnUNetTrainerNoDA.nnUNetTrainerNoDA.get_basic_generators", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.data_augmentation.default_data_augmentation.get_moreDA_augmentation", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.network_training.nnUNetTrainerV2_DDP.nnUNetTrainerV2_DDP.print_to_log_file", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.network_training.nnUNetTrainerV2_DDP.nnUNetTrainerV2_DDP.print_to_log_file", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.dataloading.dataset_loading.unpack_dataset"], ["", "def", "initialize", "(", "self", ",", "training", "=", "True", ",", "force_load_plans", "=", "False", ")", ":", "\n", "        ", "\"\"\"\n        - replaced get_default_augmentation with get_moreDA_augmentation\n        - only run this code once\n        - loss function wrapper for deep supervision\n\n        :param training:\n        :param force_load_plans:\n        :return:\n        \"\"\"", "\n", "if", "not", "self", ".", "was_initialized", ":", "\n", "            ", "maybe_mkdir_p", "(", "self", ".", "output_folder", ")", "\n", "\n", "if", "force_load_plans", "or", "(", "self", ".", "plans", "is", "None", ")", ":", "\n", "                ", "self", ".", "load_plans_file", "(", ")", "\n", "\n", "", "self", ".", "process_plans", "(", "self", ".", "plans", ")", "\n", "\n", "self", ".", "setup_DA_params", "(", ")", "\n", "\n", "################# Here we wrap the loss for deep supervision ############", "\n", "# we need to know the number of outputs of the network", "\n", "net_numpool", "=", "len", "(", "self", ".", "net_num_pool_op_kernel_sizes", ")", "\n", "\n", "# we give each output a weight which decreases exponentially (division by 2) as the resolution decreases", "\n", "# this gives higher resolution outputs more weight in the loss", "\n", "weights", "=", "np", ".", "array", "(", "[", "1", "/", "(", "2", "**", "i", ")", "for", "i", "in", "range", "(", "net_numpool", ")", "]", ")", "\n", "\n", "# we don't use the lowest 2 outputs. Normalize weights so that they sum to 1", "\n", "mask", "=", "np", ".", "array", "(", "[", "True", "if", "i", "<", "net_numpool", "-", "1", "else", "False", "for", "i", "in", "range", "(", "net_numpool", ")", "]", ")", "\n", "weights", "[", "~", "mask", "]", "=", "0", "\n", "weights", "=", "weights", "/", "weights", ".", "sum", "(", ")", "\n", "\n", "# now wrap the loss", "\n", "if", "MyDSLoss4", "is", "None", ":", "\n", "                ", "raise", "RuntimeError", "(", "\"This aint ready for prime time yet\"", ")", "\n", "\n", "", "self", ".", "loss", "=", "MyDSLoss4", "(", "self", ".", "batch_dice", ",", "weights", ")", "\n", "#self.loss = MultipleOutputLoss2(self.loss, weights)", "\n", "################# END ###################", "\n", "\n", "self", ".", "folder_with_preprocessed_data", "=", "join", "(", "self", ".", "dataset_directory", ",", "self", ".", "plans", "[", "'data_identifier'", "]", "+", "\n", "\"_stage%d\"", "%", "self", ".", "stage", ")", "\n", "if", "training", ":", "\n", "                ", "self", ".", "dl_tr", ",", "self", ".", "dl_val", "=", "self", ".", "get_basic_generators", "(", ")", "\n", "if", "self", ".", "unpack_data", ":", "\n", "                    ", "print", "(", "\"unpacking dataset\"", ")", "\n", "unpack_dataset", "(", "self", ".", "folder_with_preprocessed_data", ")", "\n", "print", "(", "\"done\"", ")", "\n", "", "else", ":", "\n", "                    ", "print", "(", "\n", "\"INFO: Not unpacking data! Training may be slow due to that. Pray you are not using 2d or you \"", "\n", "\"will wait all winter for your model to finish!\"", ")", "\n", "\n", "", "self", ".", "tr_gen", ",", "self", ".", "val_gen", "=", "get_moreDA_augmentation", "(", "self", ".", "dl_tr", ",", "self", ".", "dl_val", ",", "\n", "self", ".", "data_aug_params", "[", "\n", "'patch_size_for_spatialtransform'", "]", ",", "\n", "self", ".", "data_aug_params", ",", "\n", "deep_supervision_scales", "=", "self", ".", "deep_supervision_scales", ",", "\n", "soft_ds", "=", "True", ",", "classes", "=", "[", "0", "]", "+", "list", "(", "self", ".", "classes", ")", ",", "\n", "pin_memory", "=", "self", ".", "pin_memory", ")", "\n", "self", ".", "print_to_log_file", "(", "\"TRAINING KEYS:\\n %s\"", "%", "(", "str", "(", "self", ".", "dataset_tr", ".", "keys", "(", ")", ")", ")", ",", "\n", "also_print_to_console", "=", "False", ")", "\n", "self", ".", "print_to_log_file", "(", "\"VALIDATION KEYS:\\n %s\"", "%", "(", "str", "(", "self", ".", "dataset_val", ".", "keys", "(", ")", ")", ")", ",", "\n", "also_print_to_console", "=", "False", ")", "\n", "", "else", ":", "\n", "                ", "pass", "\n", "\n", "", "self", ".", "initialize_network", "(", ")", "\n", "self", ".", "initialize_optimizer_and_scheduler", "(", ")", "\n", "\n", "assert", "isinstance", "(", "self", ".", "network", ",", "(", "SegmentationNetwork", ",", "nn", ".", "DataParallel", ")", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "print_to_log_file", "(", "'self.was_initialized is True, not running self.initialize again'", ")", "\n", "", "self", ".", "was_initialized", "=", "True", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.architectural_variants.nnUNetTrainerV2_softDeepSupervision.nnUNetTrainerV2_softDeepSupervision.run_online_evaluation": [[116, 128], ["nnunet.training.network_training.nnUNetTrainer.nnUNetTrainer.run_online_evaluation"], "methods", ["home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.BraTS_trainer.nnUNetTrainerV2BraTSRegions.nnUNetTrainerV2BraTSRegions_DA3_BN.run_online_evaluation"], ["", "def", "run_online_evaluation", "(", "self", ",", "output", ",", "target", ")", ":", "\n", "        ", "\"\"\"\n        due to deep supervision the return value and the reference are now lists of tensors. We only need the full\n        resolution output because this is what we are interested in in the end. The others are ignored\n        :param output:\n        :param target:\n        :return:\n        \"\"\"", "\n", "target", "=", "target", "[", "0", "]", "[", ":", ",", "\n", "None", "]", "# we need to restore color channel dimension here to be compatible with previous code", "\n", "output", "=", "output", "[", "0", "]", "\n", "return", "nnUNetTrainer", ".", "run_online_evaluation", "(", "self", ",", "output", ",", "target", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.architectural_variants.nnUNetTrainerV2_3ConvPerStage.nnUNetTrainerV2_3ConvPerStage.initialize_network": [[23, 47], ["nnunet.network_architecture.generic_UNet.Generic_UNet", "torch.cuda.is_available", "len", "nnunet.network_architecture.initialization.InitWeights_He", "nnUNetTrainerV2_3ConvPerStage.nnUNetTrainerV2_3ConvPerStage.network.cuda"], "methods", ["None"], ["    ", "def", "initialize_network", "(", "self", ")", ":", "\n", "        ", "self", ".", "base_num_features", "=", "24", "# otherwise we run out of VRAM", "\n", "if", "self", ".", "threeD", ":", "\n", "            ", "conv_op", "=", "nn", ".", "Conv3d", "\n", "dropout_op", "=", "nn", ".", "Dropout3d", "\n", "norm_op", "=", "nn", ".", "InstanceNorm3d", "\n", "\n", "", "else", ":", "\n", "            ", "conv_op", "=", "nn", ".", "Conv2d", "\n", "dropout_op", "=", "nn", ".", "Dropout2d", "\n", "norm_op", "=", "nn", ".", "InstanceNorm2d", "\n", "\n", "", "norm_op_kwargs", "=", "{", "'eps'", ":", "1e-5", ",", "'affine'", ":", "True", "}", "\n", "dropout_op_kwargs", "=", "{", "'p'", ":", "0", ",", "'inplace'", ":", "True", "}", "\n", "net_nonlin", "=", "nn", ".", "LeakyReLU", "\n", "net_nonlin_kwargs", "=", "{", "'negative_slope'", ":", "1e-2", ",", "'inplace'", ":", "True", "}", "\n", "self", ".", "network", "=", "Generic_UNet", "(", "self", ".", "num_input_channels", ",", "self", ".", "base_num_features", ",", "self", ".", "num_classes", ",", "\n", "len", "(", "self", ".", "net_num_pool_op_kernel_sizes", ")", ",", "\n", "3", ",", "2", ",", "conv_op", ",", "norm_op", ",", "norm_op_kwargs", ",", "dropout_op", ",", "dropout_op_kwargs", ",", "\n", "net_nonlin", ",", "net_nonlin_kwargs", ",", "True", ",", "False", ",", "lambda", "x", ":", "x", ",", "InitWeights_He", "(", "1e-2", ")", ",", "\n", "self", ".", "net_num_pool_op_kernel_sizes", ",", "self", ".", "net_conv_kernel_sizes", ",", "False", ",", "True", ",", "True", ")", "\n", "if", "torch", ".", "cuda", ".", "is_available", "(", ")", ":", "\n", "            ", "self", ".", "network", ".", "cuda", "(", ")", "\n", "", "self", ".", "network", ".", "inference_apply_nonlin", "=", "softmax_helper", "\n", "", "", ""]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.architectural_variants.nnUNetTrainerV2_lReLU_convlReLUIN.nnUNetTrainerV2_lReLU_convReLUIN.initialize_network": [[23, 47], ["nnunet.network_architecture.generic_UNet.Generic_UNet", "torch.cuda.is_available", "len", "nnunet.network_architecture.initialization.InitWeights_He", "nnUNetTrainerV2_lReLU_convlReLUIN.nnUNetTrainerV2_lReLU_convReLUIN.network.cuda"], "methods", ["None"], ["    ", "def", "initialize_network", "(", "self", ")", ":", "\n", "        ", "if", "self", ".", "threeD", ":", "\n", "            ", "conv_op", "=", "nn", ".", "Conv3d", "\n", "dropout_op", "=", "nn", ".", "Dropout3d", "\n", "norm_op", "=", "nn", ".", "InstanceNorm3d", "\n", "\n", "", "else", ":", "\n", "            ", "conv_op", "=", "nn", ".", "Conv2d", "\n", "dropout_op", "=", "nn", ".", "Dropout2d", "\n", "norm_op", "=", "nn", ".", "InstanceNorm2d", "\n", "\n", "", "norm_op_kwargs", "=", "{", "'eps'", ":", "1e-5", ",", "'affine'", ":", "True", "}", "\n", "dropout_op_kwargs", "=", "{", "'p'", ":", "0", ",", "'inplace'", ":", "True", "}", "\n", "net_nonlin", "=", "nn", ".", "LeakyReLU", "\n", "net_nonlin_kwargs", "=", "{", "'inplace'", ":", "True", ",", "'negative_slope'", ":", "1e-2", "}", "\n", "self", ".", "network", "=", "Generic_UNet", "(", "self", ".", "num_input_channels", ",", "self", ".", "base_num_features", ",", "self", ".", "num_classes", ",", "\n", "len", "(", "self", ".", "net_num_pool_op_kernel_sizes", ")", ",", "\n", "self", ".", "conv_per_stage", ",", "2", ",", "conv_op", ",", "norm_op", ",", "norm_op_kwargs", ",", "dropout_op", ",", "dropout_op_kwargs", ",", "\n", "net_nonlin", ",", "net_nonlin_kwargs", ",", "True", ",", "False", ",", "lambda", "x", ":", "x", ",", "InitWeights_He", "(", "1e-2", ")", ",", "\n", "self", ".", "net_num_pool_op_kernel_sizes", ",", "self", ".", "net_conv_kernel_sizes", ",", "False", ",", "True", ",", "True", ",", "\n", "basic_block", "=", "ConvDropoutNonlinNorm", ")", "\n", "if", "torch", ".", "cuda", ".", "is_available", "(", ")", ":", "\n", "            ", "self", ".", "network", ".", "cuda", "(", ")", "\n", "", "self", ".", "network", ".", "inference_apply_nonlin", "=", "softmax_helper", "\n", "", "", ""]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.architectural_variants.nnUNetTrainerV2_lReLU_biasInSegOutput.nnUNetTrainerV2_lReLU_biasInSegOutput.initialize_network": [[23, 47], ["nnunet.network_architecture.generic_UNet.Generic_UNet", "torch.cuda.is_available", "len", "nnunet.network_architecture.initialization.InitWeights_He", "nnUNetTrainerV2_lReLU_biasInSegOutput.nnUNetTrainerV2_lReLU_biasInSegOutput.network.cuda"], "methods", ["None"], ["    ", "def", "initialize_network", "(", "self", ")", ":", "\n", "        ", "if", "self", ".", "threeD", ":", "\n", "            ", "conv_op", "=", "nn", ".", "Conv3d", "\n", "dropout_op", "=", "nn", ".", "Dropout3d", "\n", "norm_op", "=", "nn", ".", "InstanceNorm3d", "\n", "\n", "", "else", ":", "\n", "            ", "conv_op", "=", "nn", ".", "Conv2d", "\n", "dropout_op", "=", "nn", ".", "Dropout2d", "\n", "norm_op", "=", "nn", ".", "InstanceNorm2d", "\n", "\n", "", "norm_op_kwargs", "=", "{", "'eps'", ":", "1e-5", ",", "'affine'", ":", "True", "}", "\n", "dropout_op_kwargs", "=", "{", "'p'", ":", "0", ",", "'inplace'", ":", "True", "}", "\n", "net_nonlin", "=", "nn", ".", "LeakyReLU", "\n", "net_nonlin_kwargs", "=", "{", "'negative_slope'", ":", "1e-2", ",", "'inplace'", ":", "True", "}", "\n", "self", ".", "network", "=", "Generic_UNet", "(", "self", ".", "num_input_channels", ",", "self", ".", "base_num_features", ",", "self", ".", "num_classes", ",", "\n", "len", "(", "self", ".", "net_num_pool_op_kernel_sizes", ")", ",", "\n", "self", ".", "conv_per_stage", ",", "2", ",", "conv_op", ",", "norm_op", ",", "norm_op_kwargs", ",", "dropout_op", ",", "dropout_op_kwargs", ",", "\n", "net_nonlin", ",", "net_nonlin_kwargs", ",", "True", ",", "False", ",", "lambda", "x", ":", "x", ",", "InitWeights_He", "(", "0", ")", ",", "\n", "self", ".", "net_num_pool_op_kernel_sizes", ",", "self", ".", "net_conv_kernel_sizes", ",", "False", ",", "True", ",", "True", ",", "\n", "seg_output_use_bias", "=", "True", ")", "\n", "if", "torch", ".", "cuda", ".", "is_available", "(", ")", ":", "\n", "            ", "self", ".", "network", ".", "cuda", "(", ")", "\n", "", "self", ".", "network", ".", "inference_apply_nonlin", "=", "softmax_helper", "\n", "", "", ""]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.architectural_variants.nnUNetTrainerV2_LReLU_slope_2en1.nnUNetTrainerV2_LReLU_slope_2en1.initialize_network": [[23, 46], ["nnunet.network_architecture.generic_UNet.Generic_UNet", "torch.cuda.is_available", "len", "nnunet.network_architecture.initialization.InitWeights_He", "nnUNetTrainerV2_LReLU_slope_2en1.nnUNetTrainerV2_LReLU_slope_2en1.network.cuda"], "methods", ["None"], ["    ", "def", "initialize_network", "(", "self", ")", ":", "\n", "        ", "if", "self", ".", "threeD", ":", "\n", "            ", "conv_op", "=", "nn", ".", "Conv3d", "\n", "dropout_op", "=", "nn", ".", "Dropout3d", "\n", "norm_op", "=", "nn", ".", "InstanceNorm3d", "\n", "\n", "", "else", ":", "\n", "            ", "conv_op", "=", "nn", ".", "Conv2d", "\n", "dropout_op", "=", "nn", ".", "Dropout2d", "\n", "norm_op", "=", "nn", ".", "InstanceNorm2d", "\n", "\n", "", "norm_op_kwargs", "=", "{", "'eps'", ":", "1e-5", ",", "'affine'", ":", "True", "}", "\n", "dropout_op_kwargs", "=", "{", "'p'", ":", "0", ",", "'inplace'", ":", "True", "}", "\n", "net_nonlin", "=", "nn", ".", "LeakyReLU", "\n", "net_nonlin_kwargs", "=", "{", "'inplace'", ":", "True", ",", "'negative_slope'", ":", "2e-1", "}", "\n", "self", ".", "network", "=", "Generic_UNet", "(", "self", ".", "num_input_channels", ",", "self", ".", "base_num_features", ",", "self", ".", "num_classes", ",", "\n", "len", "(", "self", ".", "net_num_pool_op_kernel_sizes", ")", ",", "\n", "self", ".", "conv_per_stage", ",", "2", ",", "conv_op", ",", "norm_op", ",", "norm_op_kwargs", ",", "dropout_op", ",", "dropout_op_kwargs", ",", "\n", "net_nonlin", ",", "net_nonlin_kwargs", ",", "True", ",", "False", ",", "lambda", "x", ":", "x", ",", "InitWeights_He", "(", "0", ")", ",", "\n", "self", ".", "net_num_pool_op_kernel_sizes", ",", "self", ".", "net_conv_kernel_sizes", ",", "False", ",", "True", ",", "True", ")", "\n", "if", "torch", ".", "cuda", ".", "is_available", "(", ")", ":", "\n", "            ", "self", ".", "network", ".", "cuda", "(", ")", "\n", "", "self", ".", "network", ".", "inference_apply_nonlin", "=", "softmax_helper", "\n", "", "", ""]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.architectural_variants.nnUNetTrainerV2_NoNormalization_lr1en3.nnUNetTrainerV2_NoNormalization_lr1en3.__init__": [[21, 26], ["nnunet.training.network_training.nnUNet_variants.architectural_variants.nnUNetTrainerV2_NoNormalization.nnUNetTrainerV2_NoNormalization.__init__"], "methods", ["home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.common.layers.ResizeImage.__init__"], ["    ", "def", "__init__", "(", "self", ",", "plans_file", ",", "fold", ",", "output_folder", "=", "None", ",", "dataset_directory", "=", "None", ",", "batch_dice", "=", "True", ",", "stage", "=", "None", ",", "\n", "unpack_data", "=", "True", ",", "deterministic", "=", "True", ",", "fp16", "=", "False", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "plans_file", ",", "fold", ",", "output_folder", ",", "dataset_directory", ",", "batch_dice", ",", "stage", ",", "unpack_data", ",", "\n", "deterministic", ",", "fp16", ")", "\n", "self", ".", "initial_lr", "=", "1e-3", "\n", "", "", ""]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.architectural_variants.nnUNetTrainerV2_3ConvPerStage_samefilters.nnUNetTrainerV2_3ConvPerStageSameFilters.initialize_network": [[23, 46], ["nnunet.network_architecture.generic_UNet.Generic_UNet", "torch.cuda.is_available", "len", "nnunet.network_architecture.initialization.InitWeights_He", "nnUNetTrainerV2_3ConvPerStage_samefilters.nnUNetTrainerV2_3ConvPerStageSameFilters.network.cuda"], "methods", ["None"], ["    ", "def", "initialize_network", "(", "self", ")", ":", "\n", "        ", "if", "self", ".", "threeD", ":", "\n", "            ", "conv_op", "=", "nn", ".", "Conv3d", "\n", "dropout_op", "=", "nn", ".", "Dropout3d", "\n", "norm_op", "=", "nn", ".", "InstanceNorm3d", "\n", "\n", "", "else", ":", "\n", "            ", "conv_op", "=", "nn", ".", "Conv2d", "\n", "dropout_op", "=", "nn", ".", "Dropout2d", "\n", "norm_op", "=", "nn", ".", "InstanceNorm2d", "\n", "\n", "", "norm_op_kwargs", "=", "{", "'eps'", ":", "1e-5", ",", "'affine'", ":", "True", "}", "\n", "dropout_op_kwargs", "=", "{", "'p'", ":", "0", ",", "'inplace'", ":", "True", "}", "\n", "net_nonlin", "=", "nn", ".", "LeakyReLU", "\n", "net_nonlin_kwargs", "=", "{", "'negative_slope'", ":", "1e-2", ",", "'inplace'", ":", "True", "}", "\n", "self", ".", "network", "=", "Generic_UNet", "(", "self", ".", "num_input_channels", ",", "self", ".", "base_num_features", ",", "self", ".", "num_classes", ",", "\n", "len", "(", "self", ".", "net_num_pool_op_kernel_sizes", ")", ",", "\n", "3", ",", "2", ",", "conv_op", ",", "norm_op", ",", "norm_op_kwargs", ",", "dropout_op", ",", "dropout_op_kwargs", ",", "\n", "net_nonlin", ",", "net_nonlin_kwargs", ",", "True", ",", "False", ",", "lambda", "x", ":", "x", ",", "InitWeights_He", "(", "1e-2", ")", ",", "\n", "self", ".", "net_num_pool_op_kernel_sizes", ",", "self", ".", "net_conv_kernel_sizes", ",", "False", ",", "True", ",", "True", ")", "\n", "if", "torch", ".", "cuda", ".", "is_available", "(", ")", ":", "\n", "            ", "self", ".", "network", ".", "cuda", "(", ")", "\n", "", "self", ".", "network", ".", "inference_apply_nonlin", "=", "softmax_helper", "\n", "", "", ""]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.architectural_variants.nnUNetTrainerV2_noDeepSupervision.nnUNetTrainerV2_noDeepSupervision.__init__": [[33, 38], ["nnunet.training.network_training.nnUNetTrainerV2.nnUNetTrainerV2.__init__", "nnunet.training.loss_functions.dice_loss.DC_and_CE_loss"], "methods", ["home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.common.layers.ResizeImage.__init__"], ["    ", "def", "__init__", "(", "self", ",", "plans_file", ",", "fold", ",", "output_folder", "=", "None", ",", "dataset_directory", "=", "None", ",", "batch_dice", "=", "True", ",", "stage", "=", "None", ",", "\n", "unpack_data", "=", "True", ",", "deterministic", "=", "True", ",", "fp16", "=", "False", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "plans_file", ",", "fold", ",", "output_folder", ",", "dataset_directory", ",", "batch_dice", ",", "stage", ",", "unpack_data", ",", "\n", "deterministic", ",", "fp16", ")", "\n", "self", ".", "loss", "=", "DC_and_CE_loss", "(", "{", "'batch_dice'", ":", "self", ".", "batch_dice", ",", "'smooth'", ":", "1e-5", ",", "'do_bg'", ":", "False", "}", ",", "{", "}", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.architectural_variants.nnUNetTrainerV2_noDeepSupervision.nnUNetTrainerV2_noDeepSupervision.setup_DA_params": [[39, 83], ["nnunet.training.data_augmentation.default_data_augmentation.get_patch_size", "numpy.array", "nnunet.training.data_augmentation.default_data_augmentation.get_patch_size", "nnUNetTrainerV2_noDeepSupervision.nnUNetTrainerV2_noDeepSupervision.print_to_log_file", "max", "min", "list"], "methods", ["home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.data_augmentation.default_data_augmentation.get_patch_size", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.data_augmentation.default_data_augmentation.get_patch_size", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.network_training.nnUNetTrainerV2_DDP.nnUNetTrainerV2_DDP.print_to_log_file"], ["", "def", "setup_DA_params", "(", "self", ")", ":", "\n", "        ", "\"\"\"\n        we leave out the creation of self.deep_supervision_scales, so it remains None\n        :return:\n        \"\"\"", "\n", "if", "self", ".", "threeD", ":", "\n", "            ", "self", ".", "data_aug_params", "=", "default_3D_augmentation_params", "\n", "self", ".", "data_aug_params", "[", "'rotation_x'", "]", "=", "(", "-", "30.", "/", "360", "*", "2.", "*", "np", ".", "pi", ",", "30.", "/", "360", "*", "2.", "*", "np", ".", "pi", ")", "\n", "self", ".", "data_aug_params", "[", "'rotation_y'", "]", "=", "(", "-", "30.", "/", "360", "*", "2.", "*", "np", ".", "pi", ",", "30.", "/", "360", "*", "2.", "*", "np", ".", "pi", ")", "\n", "self", ".", "data_aug_params", "[", "'rotation_z'", "]", "=", "(", "-", "30.", "/", "360", "*", "2.", "*", "np", ".", "pi", ",", "30.", "/", "360", "*", "2.", "*", "np", ".", "pi", ")", "\n", "if", "self", ".", "do_dummy_2D_aug", ":", "\n", "                ", "self", ".", "data_aug_params", "[", "\"dummy_2D\"", "]", "=", "True", "\n", "self", ".", "print_to_log_file", "(", "\"Using dummy2d data augmentation\"", ")", "\n", "self", ".", "data_aug_params", "[", "\"elastic_deform_alpha\"", "]", "=", "default_2D_augmentation_params", "[", "\"elastic_deform_alpha\"", "]", "\n", "self", ".", "data_aug_params", "[", "\"elastic_deform_sigma\"", "]", "=", "default_2D_augmentation_params", "[", "\"elastic_deform_sigma\"", "]", "\n", "self", ".", "data_aug_params", "[", "\"rotation_x\"", "]", "=", "default_2D_augmentation_params", "[", "\"rotation_x\"", "]", "\n", "", "", "else", ":", "\n", "            ", "self", ".", "do_dummy_2D_aug", "=", "False", "\n", "if", "max", "(", "self", ".", "patch_size", ")", "/", "min", "(", "self", ".", "patch_size", ")", ">", "1.5", ":", "\n", "                ", "default_2D_augmentation_params", "[", "'rotation_x'", "]", "=", "(", "-", "15.", "/", "360", "*", "2.", "*", "np", ".", "pi", ",", "15.", "/", "360", "*", "2.", "*", "np", ".", "pi", ")", "\n", "", "self", ".", "data_aug_params", "=", "default_2D_augmentation_params", "\n", "", "self", ".", "data_aug_params", "[", "\"mask_was_used_for_normalization\"", "]", "=", "self", ".", "use_mask_for_norm", "\n", "\n", "if", "self", ".", "do_dummy_2D_aug", ":", "\n", "            ", "self", ".", "basic_generator_patch_size", "=", "get_patch_size", "(", "self", ".", "patch_size", "[", "1", ":", "]", ",", "\n", "self", ".", "data_aug_params", "[", "'rotation_x'", "]", ",", "\n", "self", ".", "data_aug_params", "[", "'rotation_y'", "]", ",", "\n", "self", ".", "data_aug_params", "[", "'rotation_z'", "]", ",", "\n", "self", ".", "data_aug_params", "[", "'scale_range'", "]", ")", "\n", "self", ".", "basic_generator_patch_size", "=", "np", ".", "array", "(", "[", "self", ".", "patch_size", "[", "0", "]", "]", "+", "list", "(", "self", ".", "basic_generator_patch_size", ")", ")", "\n", "patch_size_for_spatialtransform", "=", "self", ".", "patch_size", "[", "1", ":", "]", "\n", "", "else", ":", "\n", "            ", "self", ".", "basic_generator_patch_size", "=", "get_patch_size", "(", "self", ".", "patch_size", ",", "self", ".", "data_aug_params", "[", "'rotation_x'", "]", ",", "\n", "self", ".", "data_aug_params", "[", "'rotation_y'", "]", ",", "\n", "self", ".", "data_aug_params", "[", "'rotation_z'", "]", ",", "\n", "self", ".", "data_aug_params", "[", "'scale_range'", "]", ")", "\n", "patch_size_for_spatialtransform", "=", "self", ".", "patch_size", "\n", "\n", "", "self", ".", "data_aug_params", "[", "\"scale_range\"", "]", "=", "(", "0.7", ",", "1.4", ")", "\n", "self", ".", "data_aug_params", "[", "\"do_elastic\"", "]", "=", "False", "\n", "self", ".", "data_aug_params", "[", "'selected_seg_channels'", "]", "=", "[", "0", "]", "\n", "self", ".", "data_aug_params", "[", "'patch_size_for_spatialtransform'", "]", "=", "patch_size_for_spatialtransform", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.architectural_variants.nnUNetTrainerV2_noDeepSupervision.nnUNetTrainerV2_noDeepSupervision.initialize": [[84, 135], ["maybe_mkdir_p", "nnUNetTrainerV2_noDeepSupervision.nnUNetTrainerV2_noDeepSupervision.process_plans", "nnUNetTrainerV2_noDeepSupervision.nnUNetTrainerV2_noDeepSupervision.setup_DA_params", "join", "nnUNetTrainerV2_noDeepSupervision.nnUNetTrainerV2_noDeepSupervision.initialize_network", "nnUNetTrainerV2_noDeepSupervision.nnUNetTrainerV2_noDeepSupervision.initialize_optimizer_and_scheduler", "isinstance", "nnUNetTrainerV2_noDeepSupervision.nnUNetTrainerV2_noDeepSupervision.print_to_log_file", "nnUNetTrainerV2_noDeepSupervision.nnUNetTrainerV2_noDeepSupervision.load_plans_file", "nnUNetTrainerV2_noDeepSupervision.nnUNetTrainerV2_noDeepSupervision.get_basic_generators", "nnunet.training.data_augmentation.default_data_augmentation.get_moreDA_augmentation", "nnUNetTrainerV2_noDeepSupervision.nnUNetTrainerV2_noDeepSupervision.print_to_log_file", "nnUNetTrainerV2_noDeepSupervision.nnUNetTrainerV2_noDeepSupervision.print_to_log_file", "print", "nnunet.training.dataloading.dataset_loading.unpack_dataset", "print", "print", "str", "str", "nnUNetTrainerV2_noDeepSupervision.nnUNetTrainerV2_noDeepSupervision.dataset_tr.keys", "nnUNetTrainerV2_noDeepSupervision.nnUNetTrainerV2_noDeepSupervision.dataset_val.keys"], "methods", ["home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.BraTS_trainer.nnUNetTrainerV2BraTSRegions.nnUNetTrainerV2BraTSRegions_DA3_BN.process_plans", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.BraTS_trainer.nnUNetTrainerV2BraTSRegions.nnUNetTrainerV2BraTSRegions_DA4_BN.setup_DA_params", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.BraTS_trainer.nnUNetTrainerV2BraTSRegions.nnUNetTrainerV2BraTSRegions_DA3.initialize_network", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.optimizer_and_lr.nnUNetTrainerV2_momentum09in2D.nnUNetTrainerV2_momentum09in2D.initialize_optimizer_and_scheduler", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.network_training.nnUNetTrainerV2_DDP.nnUNetTrainerV2_DDP.print_to_log_file", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.network_training.nnUNetTrainer.nnUNetTrainer.load_plans_file", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.nnUNet_variants.nnUNetTrainerNoDA.nnUNetTrainerNoDA.get_basic_generators", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.data_augmentation.default_data_augmentation.get_moreDA_augmentation", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.network_training.nnUNetTrainerV2_DDP.nnUNetTrainerV2_DDP.print_to_log_file", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.network_training.nnUNetTrainerV2_DDP.nnUNetTrainerV2_DDP.print_to_log_file", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.dataloading.dataset_loading.unpack_dataset"], ["", "def", "initialize", "(", "self", ",", "training", "=", "True", ",", "force_load_plans", "=", "False", ")", ":", "\n", "        ", "\"\"\"\n        removed deep supervision\n        :return:\n        \"\"\"", "\n", "if", "not", "self", ".", "was_initialized", ":", "\n", "            ", "maybe_mkdir_p", "(", "self", ".", "output_folder", ")", "\n", "\n", "if", "force_load_plans", "or", "(", "self", ".", "plans", "is", "None", ")", ":", "\n", "                ", "self", ".", "load_plans_file", "(", ")", "\n", "\n", "", "self", ".", "process_plans", "(", "self", ".", "plans", ")", "\n", "\n", "self", ".", "setup_DA_params", "(", ")", "\n", "\n", "self", ".", "folder_with_preprocessed_data", "=", "join", "(", "self", ".", "dataset_directory", ",", "self", ".", "plans", "[", "'data_identifier'", "]", "+", "\n", "\"_stage%d\"", "%", "self", ".", "stage", ")", "\n", "if", "training", ":", "\n", "                ", "self", ".", "dl_tr", ",", "self", ".", "dl_val", "=", "self", ".", "get_basic_generators", "(", ")", "\n", "if", "self", ".", "unpack_data", ":", "\n", "                    ", "print", "(", "\"unpacking dataset\"", ")", "\n", "unpack_dataset", "(", "self", ".", "folder_with_preprocessed_data", ")", "\n", "print", "(", "\"done\"", ")", "\n", "", "else", ":", "\n", "                    ", "print", "(", "\n", "\"INFO: Not unpacking data! Training may be slow due to that. Pray you are not using 2d or you \"", "\n", "\"will wait all winter for your model to finish!\"", ")", "\n", "\n", "", "assert", "self", ".", "deep_supervision_scales", "is", "None", "\n", "self", ".", "tr_gen", ",", "self", ".", "val_gen", "=", "get_moreDA_augmentation", "(", "self", ".", "dl_tr", ",", "self", ".", "dl_val", ",", "\n", "self", ".", "data_aug_params", "[", "\n", "'patch_size_for_spatialtransform'", "]", ",", "\n", "self", ".", "data_aug_params", ",", "\n", "deep_supervision_scales", "=", "self", ".", "deep_supervision_scales", ",", "\n", "classes", "=", "None", ",", "\n", "pin_memory", "=", "self", ".", "pin_memory", ")", "\n", "\n", "self", ".", "print_to_log_file", "(", "\"TRAINING KEYS:\\n %s\"", "%", "(", "str", "(", "self", ".", "dataset_tr", ".", "keys", "(", ")", ")", ")", ",", "\n", "also_print_to_console", "=", "False", ")", "\n", "self", ".", "print_to_log_file", "(", "\"VALIDATION KEYS:\\n %s\"", "%", "(", "str", "(", "self", ".", "dataset_val", ".", "keys", "(", ")", ")", ")", ",", "\n", "also_print_to_console", "=", "False", ")", "\n", "", "else", ":", "\n", "                ", "pass", "\n", "\n", "", "self", ".", "initialize_network", "(", ")", "\n", "self", ".", "initialize_optimizer_and_scheduler", "(", ")", "\n", "\n", "assert", "isinstance", "(", "self", ".", "network", ",", "(", "SegmentationNetwork", ",", "nn", ".", "DataParallel", ")", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "print_to_log_file", "(", "'self.was_initialized is True, not running self.initialize again'", ")", "\n", "", "self", ".", "was_initialized", "=", "True", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.architectural_variants.nnUNetTrainerV2_noDeepSupervision.nnUNetTrainerV2_noDeepSupervision.initialize_network": [[136, 163], ["nnunet.network_architecture.generic_UNet.Generic_UNet", "torch.cuda.is_available", "len", "nnunet.network_architecture.initialization.InitWeights_He", "nnUNetTrainerV2_noDeepSupervision.nnUNetTrainerV2_noDeepSupervision.network.cuda"], "methods", ["None"], ["", "def", "initialize_network", "(", "self", ")", ":", "\n", "        ", "\"\"\"\n        changed deep supervision to False\n        :return:\n        \"\"\"", "\n", "if", "self", ".", "threeD", ":", "\n", "            ", "conv_op", "=", "nn", ".", "Conv3d", "\n", "dropout_op", "=", "nn", ".", "Dropout3d", "\n", "norm_op", "=", "nn", ".", "InstanceNorm3d", "\n", "\n", "", "else", ":", "\n", "            ", "conv_op", "=", "nn", ".", "Conv2d", "\n", "dropout_op", "=", "nn", ".", "Dropout2d", "\n", "norm_op", "=", "nn", ".", "InstanceNorm2d", "\n", "\n", "", "norm_op_kwargs", "=", "{", "'eps'", ":", "1e-5", ",", "'affine'", ":", "True", "}", "\n", "dropout_op_kwargs", "=", "{", "'p'", ":", "0", ",", "'inplace'", ":", "True", "}", "\n", "net_nonlin", "=", "nn", ".", "LeakyReLU", "\n", "net_nonlin_kwargs", "=", "{", "'negative_slope'", ":", "1e-2", ",", "'inplace'", ":", "True", "}", "\n", "self", ".", "network", "=", "Generic_UNet", "(", "self", ".", "num_input_channels", ",", "self", ".", "base_num_features", ",", "self", ".", "num_classes", ",", "\n", "len", "(", "self", ".", "net_num_pool_op_kernel_sizes", ")", ",", "\n", "self", ".", "conv_per_stage", ",", "2", ",", "conv_op", ",", "norm_op", ",", "norm_op_kwargs", ",", "dropout_op", ",", "dropout_op_kwargs", ",", "\n", "net_nonlin", ",", "net_nonlin_kwargs", ",", "False", ",", "False", ",", "lambda", "x", ":", "x", ",", "InitWeights_He", "(", "1e-2", ")", ",", "\n", "self", ".", "net_num_pool_op_kernel_sizes", ",", "self", ".", "net_conv_kernel_sizes", ",", "False", ",", "True", ",", "True", ")", "\n", "if", "torch", ".", "cuda", ".", "is_available", "(", ")", ":", "\n", "            ", "self", ".", "network", ".", "cuda", "(", ")", "\n", "", "self", ".", "network", ".", "inference_apply_nonlin", "=", "softmax_helper", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.architectural_variants.nnUNetTrainerV2_noDeepSupervision.nnUNetTrainerV2_noDeepSupervision.run_online_evaluation": [[164, 166], ["nnunet.training.network_training.nnUNetTrainer.nnUNetTrainer.run_online_evaluation"], "methods", ["home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.BraTS_trainer.nnUNetTrainerV2BraTSRegions.nnUNetTrainerV2BraTSRegions_DA3_BN.run_online_evaluation"], ["", "def", "run_online_evaluation", "(", "self", ",", "output", ",", "target", ")", ":", "\n", "        ", "return", "nnUNetTrainer", ".", "run_online_evaluation", "(", "self", ",", "output", ",", "target", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.architectural_variants.nnUNetTrainerV2_GN.nnUNetTrainerV2_GN.initialize_network": [[24, 51], ["nnunet.network_architecture.generic_UNet.Generic_UNet", "torch.cuda.is_available", "len", "nnunet.network_architecture.initialization.InitWeights_He", "nnUNetTrainerV2_GN.nnUNetTrainerV2_GN.network.cuda"], "methods", ["None"], ["    ", "def", "initialize_network", "(", "self", ")", ":", "\n", "        ", "\"\"\"\n        changed deep supervision to False\n        :return:\n        \"\"\"", "\n", "if", "self", ".", "threeD", ":", "\n", "            ", "conv_op", "=", "nn", ".", "Conv3d", "\n", "dropout_op", "=", "nn", ".", "Dropout3d", "\n", "norm_op", "=", "MyGroupNorm", "\n", "\n", "", "else", ":", "\n", "            ", "conv_op", "=", "nn", ".", "Conv2d", "\n", "dropout_op", "=", "nn", ".", "Dropout2d", "\n", "norm_op", "=", "MyGroupNorm", "\n", "\n", "", "norm_op_kwargs", "=", "{", "'eps'", ":", "1e-5", ",", "'affine'", ":", "True", ",", "'num_groups'", ":", "8", "}", "\n", "dropout_op_kwargs", "=", "{", "'p'", ":", "0", ",", "'inplace'", ":", "True", "}", "\n", "net_nonlin", "=", "nn", ".", "LeakyReLU", "\n", "net_nonlin_kwargs", "=", "{", "'negative_slope'", ":", "1e-2", ",", "'inplace'", ":", "True", "}", "\n", "self", ".", "network", "=", "Generic_UNet", "(", "self", ".", "num_input_channels", ",", "self", ".", "base_num_features", ",", "self", ".", "num_classes", ",", "\n", "len", "(", "self", ".", "net_num_pool_op_kernel_sizes", ")", ",", "\n", "self", ".", "conv_per_stage", ",", "2", ",", "conv_op", ",", "norm_op", ",", "norm_op_kwargs", ",", "dropout_op", ",", "dropout_op_kwargs", ",", "\n", "net_nonlin", ",", "net_nonlin_kwargs", ",", "True", ",", "False", ",", "lambda", "x", ":", "x", ",", "InitWeights_He", "(", "1e-2", ")", ",", "\n", "self", ".", "net_num_pool_op_kernel_sizes", ",", "self", ".", "net_conv_kernel_sizes", ",", "False", ",", "True", ",", "True", ")", "\n", "if", "torch", ".", "cuda", ".", "is_available", "(", ")", ":", "\n", "            ", "self", ".", "network", ".", "cuda", "(", ")", "\n", "", "self", ".", "network", ".", "inference_apply_nonlin", "=", "softmax_helper", "\n", "", "", ""]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.architectural_variants.nnUNetTrainerV2_Mish.nnUNetTrainerV2_Mish.initialize_network": [[24, 47], ["nnunet.network_architecture.generic_UNet.Generic_UNet", "torch.cuda.is_available", "len", "nnunet.network_architecture.initialization.InitWeights_He", "nnUNetTrainerV2_Mish.nnUNetTrainerV2_Mish.network.cuda"], "methods", ["None"], ["    ", "def", "initialize_network", "(", "self", ")", ":", "\n", "        ", "if", "self", ".", "threeD", ":", "\n", "            ", "conv_op", "=", "nn", ".", "Conv3d", "\n", "dropout_op", "=", "nn", ".", "Dropout3d", "\n", "norm_op", "=", "nn", ".", "InstanceNorm3d", "\n", "\n", "", "else", ":", "\n", "            ", "conv_op", "=", "nn", ".", "Conv2d", "\n", "dropout_op", "=", "nn", ".", "Dropout2d", "\n", "norm_op", "=", "nn", ".", "InstanceNorm2d", "\n", "\n", "", "norm_op_kwargs", "=", "{", "'eps'", ":", "1e-5", ",", "'affine'", ":", "True", "}", "\n", "dropout_op_kwargs", "=", "{", "'p'", ":", "0", ",", "'inplace'", ":", "True", "}", "\n", "net_nonlin", "=", "Mish", "\n", "net_nonlin_kwargs", "=", "{", "}", "\n", "self", ".", "network", "=", "Generic_UNet", "(", "self", ".", "num_input_channels", ",", "self", ".", "base_num_features", ",", "self", ".", "num_classes", ",", "\n", "len", "(", "self", ".", "net_num_pool_op_kernel_sizes", ")", ",", "\n", "self", ".", "conv_per_stage", ",", "2", ",", "conv_op", ",", "norm_op", ",", "norm_op_kwargs", ",", "dropout_op", ",", "dropout_op_kwargs", ",", "\n", "net_nonlin", ",", "net_nonlin_kwargs", ",", "True", ",", "False", ",", "lambda", "x", ":", "x", ",", "InitWeights_He", "(", "0", ")", ",", "\n", "self", ".", "net_num_pool_op_kernel_sizes", ",", "self", ".", "net_conv_kernel_sizes", ",", "False", ",", "True", ",", "True", ")", "\n", "if", "torch", ".", "cuda", ".", "is_available", "(", ")", ":", "\n", "            ", "self", ".", "network", ".", "cuda", "(", ")", "\n", "", "self", ".", "network", ".", "inference_apply_nonlin", "=", "softmax_helper", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.architectural_variants.nnUNetTrainerV2_allConv3x3.nnUNetTrainerV2_allConv3x3.initialize_network": [[23, 61], ["range", "nnunet.network_architecture.generic_UNet.Generic_UNet", "torch.cuda.is_available", "len", "range", "len", "nnunet.network_architecture.initialization.InitWeights_He", "nnUNetTrainerV2_allConv3x3.nnUNetTrainerV2_allConv3x3.network.cuda", "len"], "methods", ["None"], ["    ", "def", "initialize_network", "(", "self", ")", ":", "\n", "        ", "\"\"\"\n        - momentum 0.99\n        - SGD instead of Adam\n        - self.lr_scheduler = None because we do poly_lr\n        - deep supervision = True\n        - i am sure I forgot something here\n\n        Known issue: forgot to set neg_slope=0 in InitWeights_He; should not make a difference though\n        :return:\n        \"\"\"", "\n", "if", "self", ".", "threeD", ":", "\n", "            ", "conv_op", "=", "nn", ".", "Conv3d", "\n", "dropout_op", "=", "nn", ".", "Dropout3d", "\n", "norm_op", "=", "nn", ".", "InstanceNorm3d", "\n", "\n", "", "else", ":", "\n", "            ", "conv_op", "=", "nn", ".", "Conv2d", "\n", "dropout_op", "=", "nn", ".", "Dropout2d", "\n", "norm_op", "=", "nn", ".", "InstanceNorm2d", "\n", "\n", "", "for", "s", "in", "range", "(", "len", "(", "self", ".", "net_conv_kernel_sizes", ")", ")", ":", "\n", "            ", "for", "i", "in", "range", "(", "len", "(", "self", ".", "net_conv_kernel_sizes", "[", "s", "]", ")", ")", ":", "\n", "                ", "self", ".", "net_conv_kernel_sizes", "[", "s", "]", "[", "i", "]", "=", "3", "\n", "\n", "", "", "norm_op_kwargs", "=", "{", "'eps'", ":", "1e-5", ",", "'affine'", ":", "True", "}", "\n", "dropout_op_kwargs", "=", "{", "'p'", ":", "0", ",", "'inplace'", ":", "True", "}", "\n", "net_nonlin", "=", "nn", ".", "LeakyReLU", "\n", "net_nonlin_kwargs", "=", "{", "'negative_slope'", ":", "1e-2", ",", "'inplace'", ":", "True", "}", "\n", "\n", "self", ".", "network", "=", "Generic_UNet", "(", "self", ".", "num_input_channels", ",", "self", ".", "base_num_features", ",", "self", ".", "num_classes", ",", "\n", "len", "(", "self", ".", "net_num_pool_op_kernel_sizes", ")", ",", "\n", "self", ".", "conv_per_stage", ",", "2", ",", "conv_op", ",", "norm_op", ",", "norm_op_kwargs", ",", "dropout_op", ",", "dropout_op_kwargs", ",", "\n", "net_nonlin", ",", "net_nonlin_kwargs", ",", "True", ",", "False", ",", "lambda", "x", ":", "x", ",", "InitWeights_He", "(", "1e-2", ")", ",", "\n", "self", ".", "net_num_pool_op_kernel_sizes", ",", "self", ".", "net_conv_kernel_sizes", ",", "False", ",", "True", ",", "True", ")", "\n", "if", "torch", ".", "cuda", ".", "is_available", "(", ")", ":", "\n", "            ", "self", ".", "network", ".", "cuda", "(", ")", "\n", "", "self", ".", "network", ".", "inference_apply_nonlin", "=", "softmax_helper", "\n", "", "", ""]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.architectural_variants.nnUNetTrainerV2_BN.nnUNetTrainerV2_BN.initialize_network": [[23, 50], ["nnunet.network_architecture.generic_UNet.Generic_UNet", "torch.cuda.is_available", "len", "nnunet.network_architecture.initialization.InitWeights_He", "nnUNetTrainerV2_BN.nnUNetTrainerV2_BN.network.cuda"], "methods", ["None"], ["    ", "def", "initialize_network", "(", "self", ")", ":", "\n", "        ", "\"\"\"\n        changed deep supervision to False\n        :return:\n        \"\"\"", "\n", "if", "self", ".", "threeD", ":", "\n", "            ", "conv_op", "=", "nn", ".", "Conv3d", "\n", "dropout_op", "=", "nn", ".", "Dropout3d", "\n", "norm_op", "=", "nn", ".", "BatchNorm3d", "\n", "\n", "", "else", ":", "\n", "            ", "conv_op", "=", "nn", ".", "Conv2d", "\n", "dropout_op", "=", "nn", ".", "Dropout2d", "\n", "norm_op", "=", "nn", ".", "BatchNorm2d", "\n", "\n", "", "norm_op_kwargs", "=", "{", "'eps'", ":", "1e-5", ",", "'affine'", ":", "True", "}", "\n", "dropout_op_kwargs", "=", "{", "'p'", ":", "0", ",", "'inplace'", ":", "True", "}", "\n", "net_nonlin", "=", "nn", ".", "LeakyReLU", "\n", "net_nonlin_kwargs", "=", "{", "'negative_slope'", ":", "1e-2", ",", "'inplace'", ":", "True", "}", "\n", "self", ".", "network", "=", "Generic_UNet", "(", "self", ".", "num_input_channels", ",", "self", ".", "base_num_features", ",", "self", ".", "num_classes", ",", "\n", "len", "(", "self", ".", "net_num_pool_op_kernel_sizes", ")", ",", "\n", "self", ".", "conv_per_stage", ",", "2", ",", "conv_op", ",", "norm_op", ",", "norm_op_kwargs", ",", "dropout_op", ",", "dropout_op_kwargs", ",", "\n", "net_nonlin", ",", "net_nonlin_kwargs", ",", "True", ",", "False", ",", "lambda", "x", ":", "x", ",", "InitWeights_He", "(", "1e-2", ")", ",", "\n", "self", ".", "net_num_pool_op_kernel_sizes", ",", "self", ".", "net_conv_kernel_sizes", ",", "False", ",", "True", ",", "True", ")", "\n", "if", "torch", ".", "cuda", ".", "is_available", "(", ")", ":", "\n", "            ", "self", ".", "network", ".", "cuda", "(", ")", "\n", "", "self", ".", "network", ".", "inference_apply_nonlin", "=", "softmax_helper", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.architectural_variants.nnUNetTrainerV2_GeLU.GeLU.forward": [[25, 27], ["torch.nn.functional.gelu"], "methods", ["None"], ["    ", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "return", "gelu", "(", "x", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.architectural_variants.nnUNetTrainerV2_GeLU.nnUNetTrainerV2_GeLU.initialize_network": [[30, 64], ["nnunet.network_architecture.generic_UNet.Generic_UNet", "torch.cuda.is_available", "len", "nnunet.network_architecture.initialization.InitWeights_He", "nnUNetTrainerV2_GeLU.nnUNetTrainerV2_GeLU.network.cuda"], "methods", ["None"], ["    ", "def", "initialize_network", "(", "self", ")", ":", "\n", "        ", "\"\"\"\n        - momentum 0.99\n        - SGD instead of Adam\n        - self.lr_scheduler = None because we do poly_lr\n        - deep supervision = True\n        - ReLU\n        - i am sure I forgot something here\n\n        Known issue: forgot to set neg_slope=0 in InitWeights_He; should not make a difference though\n        :return:\n        \"\"\"", "\n", "if", "self", ".", "threeD", ":", "\n", "            ", "conv_op", "=", "nn", ".", "Conv3d", "\n", "dropout_op", "=", "nn", ".", "Dropout3d", "\n", "norm_op", "=", "nn", ".", "InstanceNorm3d", "\n", "\n", "", "else", ":", "\n", "            ", "conv_op", "=", "nn", ".", "Conv2d", "\n", "dropout_op", "=", "nn", ".", "Dropout2d", "\n", "norm_op", "=", "nn", ".", "InstanceNorm2d", "\n", "\n", "", "norm_op_kwargs", "=", "{", "'eps'", ":", "1e-5", ",", "'affine'", ":", "True", "}", "\n", "dropout_op_kwargs", "=", "{", "'p'", ":", "0", ",", "'inplace'", ":", "True", "}", "\n", "net_nonlin", "=", "GeLU", "\n", "net_nonlin_kwargs", "=", "{", "}", "\n", "self", ".", "network", "=", "Generic_UNet", "(", "self", ".", "num_input_channels", ",", "self", ".", "base_num_features", ",", "self", ".", "num_classes", ",", "\n", "len", "(", "self", ".", "net_num_pool_op_kernel_sizes", ")", ",", "\n", "self", ".", "conv_per_stage", ",", "2", ",", "conv_op", ",", "norm_op", ",", "norm_op_kwargs", ",", "dropout_op", ",", "dropout_op_kwargs", ",", "\n", "net_nonlin", ",", "net_nonlin_kwargs", ",", "True", ",", "False", ",", "lambda", "x", ":", "x", ",", "InitWeights_He", "(", ")", ",", "\n", "self", ".", "net_num_pool_op_kernel_sizes", ",", "self", ".", "net_conv_kernel_sizes", ",", "False", ",", "True", ",", "True", ")", "\n", "if", "torch", ".", "cuda", ".", "is_available", "(", ")", ":", "\n", "            ", "self", ".", "network", ".", "cuda", "(", ")", "\n", "", "self", ".", "network", ".", "inference_apply_nonlin", "=", "softmax_helper", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.architectural_variants.nnUNetTrainerV2_ResencUNet.nnUNetTrainerV2_ResencUNet.initialize_network": [[26, 46], ["nnunet.network_architecture.generic_modular_residual_UNet.FabiansUNet", "torch.cuda.is_available", "nnunet.network_architecture.generic_modular_residual_UNet.get_default_network_config", "nnunet.network_architecture.generic_modular_residual_UNet.get_default_network_config", "nnunet.network_architecture.initialization.InitWeights_He", "nnUNetTrainerV2_ResencUNet.nnUNetTrainerV2_ResencUNet.network.cuda"], "methods", ["home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.network_architecture.generic_modular_UNet.get_default_network_config", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.network_architecture.generic_modular_UNet.get_default_network_config"], ["    ", "def", "initialize_network", "(", "self", ")", ":", "\n", "        ", "if", "self", ".", "threeD", ":", "\n", "            ", "cfg", "=", "get_default_network_config", "(", "3", ",", "None", ",", "norm_type", "=", "\"in\"", ")", "\n", "\n", "", "else", ":", "\n", "            ", "cfg", "=", "get_default_network_config", "(", "1", ",", "None", ",", "norm_type", "=", "\"in\"", ")", "\n", "\n", "", "stage_plans", "=", "self", ".", "plans", "[", "'plans_per_stage'", "]", "[", "self", ".", "stage", "]", "\n", "conv_kernel_sizes", "=", "stage_plans", "[", "'conv_kernel_sizes'", "]", "\n", "blocks_per_stage_encoder", "=", "stage_plans", "[", "'num_blocks_encoder'", "]", "\n", "blocks_per_stage_decoder", "=", "stage_plans", "[", "'num_blocks_decoder'", "]", "\n", "pool_op_kernel_sizes", "=", "stage_plans", "[", "'pool_op_kernel_sizes'", "]", "\n", "\n", "self", ".", "network", "=", "FabiansUNet", "(", "self", ".", "num_input_channels", ",", "self", ".", "base_num_features", ",", "blocks_per_stage_encoder", ",", "2", ",", "\n", "pool_op_kernel_sizes", ",", "conv_kernel_sizes", ",", "cfg", ",", "self", ".", "num_classes", ",", "\n", "blocks_per_stage_decoder", ",", "True", ",", "False", ",", "320", ",", "InitWeights_He", "(", "1e-2", ")", ")", "\n", "\n", "if", "torch", ".", "cuda", ".", "is_available", "(", ")", ":", "\n", "            ", "self", ".", "network", ".", "cuda", "(", ")", "\n", "", "self", ".", "network", ".", "inference_apply_nonlin", "=", "softmax_helper", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.architectural_variants.nnUNetTrainerV2_ResencUNet.nnUNetTrainerV2_ResencUNet.setup_DA_params": [[47, 54], ["super().setup_DA_params", "list", "list", "numpy.cumprod", "numpy.vstack"], "methods", ["home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.BraTS_trainer.nnUNetTrainerV2BraTSRegions.nnUNetTrainerV2BraTSRegions_DA4_BN.setup_DA_params"], ["", "def", "setup_DA_params", "(", "self", ")", ":", "\n", "        ", "\"\"\"\n        net_num_pool_op_kernel_sizes is different in resunet\n        \"\"\"", "\n", "super", "(", ")", ".", "setup_DA_params", "(", ")", "\n", "self", ".", "deep_supervision_scales", "=", "[", "[", "1", ",", "1", ",", "1", "]", "]", "+", "list", "(", "list", "(", "i", ")", "for", "i", "in", "1", "/", "np", ".", "cumprod", "(", "\n", "np", ".", "vstack", "(", "self", ".", "net_num_pool_op_kernel_sizes", "[", "1", ":", "]", ")", ",", "axis", "=", "0", ")", ")", "[", ":", "-", "1", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.architectural_variants.nnUNetTrainerV2_ResencUNet.nnUNetTrainerV2_ResencUNet.validate": [[55, 67], ["nnunet.training.network_training.nnUNetTrainer.nnUNetTrainer.validate"], "methods", ["home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.BraTS_trainer.nnUNetTrainerV2BraTSRegions.nnUNetTrainerV2BraTSRegions_DA3_BN.validate"], ["", "def", "validate", "(", "self", ",", "do_mirroring", ":", "bool", "=", "True", ",", "use_sliding_window", ":", "bool", "=", "True", ",", "step_size", ":", "float", "=", "0.5", ",", "\n", "save_softmax", ":", "bool", "=", "True", ",", "use_gaussian", ":", "bool", "=", "True", ",", "overwrite", ":", "bool", "=", "True", ",", "\n", "validation_folder_name", ":", "str", "=", "'validation_raw'", ",", "debug", ":", "bool", "=", "False", ",", "all_in_gpu", ":", "bool", "=", "False", ",", "\n", "force_separate_z", ":", "bool", "=", "None", ",", "interpolation_order", ":", "int", "=", "3", ",", "interpolation_order_z", "=", "0", ",", "\n", "segmentation_export_kwargs", ":", "dict", "=", "None", ")", ":", "\n", "        ", "ds", "=", "self", ".", "network", ".", "decoder", ".", "deep_supervision", "\n", "self", ".", "network", ".", "decoder", ".", "deep_supervision", "=", "False", "\n", "ret", "=", "nnUNetTrainer", ".", "validate", "(", "self", ",", "do_mirroring", ",", "use_sliding_window", ",", "step_size", ",", "save_softmax", ",", "use_gaussian", ",", "\n", "overwrite", ",", "validation_folder_name", ",", "debug", ",", "all_in_gpu", ",", "\n", "segmentation_export_kwargs", ")", "\n", "self", ".", "network", ".", "decoder", ".", "deep_supervision", "=", "ds", "\n", "return", "ret", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.architectural_variants.nnUNetTrainerV2_ResencUNet.nnUNetTrainerV2_ResencUNet.predict_preprocessed_data_return_seg_and_softmax": [[68, 83], ["nnunet.training.network_training.nnUNetTrainer.nnUNetTrainer.predict_preprocessed_data_return_seg_and_softmax"], "methods", ["home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.architectural_variants.nnUNetTrainerV2_ResencUNet.nnUNetTrainerV2_ResencUNet.predict_preprocessed_data_return_seg_and_softmax"], ["", "def", "predict_preprocessed_data_return_seg_and_softmax", "(", "self", ",", "data", ":", "np", ".", "ndarray", ",", "do_mirroring", ":", "bool", "=", "True", ",", "\n", "mirror_axes", ":", "Tuple", "[", "int", "]", "=", "None", ",", "\n", "use_sliding_window", ":", "bool", "=", "True", ",", "step_size", ":", "float", "=", "0.5", ",", "\n", "use_gaussian", ":", "bool", "=", "True", ",", "pad_border_mode", ":", "str", "=", "'constant'", ",", "\n", "pad_kwargs", ":", "dict", "=", "None", ",", "all_in_gpu", ":", "bool", "=", "True", ",", "\n", "verbose", ":", "bool", "=", "True", ",", "mixed_precision", "=", "True", ")", "->", "Tuple", "[", "np", ".", "ndarray", ",", "np", ".", "ndarray", "]", ":", "\n", "        ", "ds", "=", "self", ".", "network", ".", "decoder", ".", "deep_supervision", "\n", "self", ".", "network", ".", "decoder", ".", "deep_supervision", "=", "False", "\n", "ret", "=", "nnUNetTrainer", ".", "predict_preprocessed_data_return_seg_and_softmax", "(", "self", ",", "data", ",", "do_mirroring", ",", "mirror_axes", ",", "\n", "use_sliding_window", ",", "step_size", ",", "\n", "use_gaussian", ",", "pad_border_mode", ",", "pad_kwargs", ",", "\n", "all_in_gpu", ",", "verbose", ",", "\n", "mixed_precision", "=", "mixed_precision", ")", "\n", "self", ".", "network", ".", "decoder", ".", "deep_supervision", "=", "ds", "\n", "return", "ret", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.architectural_variants.nnUNetTrainerV2_ResencUNet.nnUNetTrainerV2_ResencUNet.run_training": [[84, 92], ["nnUNetTrainerV2_ResencUNet.nnUNetTrainerV2_ResencUNet.maybe_update_lr", "nnunet.training.network_training.nnUNetTrainer.nnUNetTrainer.run_training"], "methods", ["home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.optimizer_and_lr.nnUNetTrainerV2_warmup.nnUNetTrainerV2_warmup.maybe_update_lr", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.architectural_variants.nnUNetTrainerV2_ResencUNet.nnUNetTrainerV2_ResencUNet.run_training"], ["", "def", "run_training", "(", "self", ")", ":", "\n", "        ", "self", ".", "maybe_update_lr", "(", "self", ".", "epoch", ")", "# if we dont overwrite epoch then self.epoch+1 is used which is not what we", "\n", "# want at the start of the training", "\n", "ds", "=", "self", ".", "network", ".", "decoder", ".", "deep_supervision", "\n", "self", ".", "network", ".", "decoder", ".", "deep_supervision", "=", "True", "\n", "ret", "=", "nnUNetTrainer", ".", "run_training", "(", "self", ")", "\n", "self", ".", "network", ".", "decoder", ".", "deep_supervision", "=", "ds", "\n", "return", "ret", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.architectural_variants.nnUNetTrainerV2_NoNormalization.nnUNetTrainerV2_NoNormalization.initialize_network": [[24, 47], ["nnunet.network_architecture.generic_UNet.Generic_UNet", "torch.cuda.is_available", "len", "nnunet.network_architecture.initialization.InitWeights_He", "nnUNetTrainerV2_NoNormalization.nnUNetTrainerV2_NoNormalization.network.cuda"], "methods", ["None"], ["    ", "def", "initialize_network", "(", "self", ")", ":", "\n", "        ", "if", "self", ".", "threeD", ":", "\n", "            ", "conv_op", "=", "nn", ".", "Conv3d", "\n", "dropout_op", "=", "nn", ".", "Dropout3d", "\n", "norm_op", "=", "Identity", "\n", "\n", "", "else", ":", "\n", "            ", "conv_op", "=", "nn", ".", "Conv2d", "\n", "dropout_op", "=", "nn", ".", "Dropout2d", "\n", "norm_op", "=", "Identity", "\n", "\n", "", "norm_op_kwargs", "=", "{", "}", "\n", "dropout_op_kwargs", "=", "{", "'p'", ":", "0", ",", "'inplace'", ":", "True", "}", "\n", "net_nonlin", "=", "nn", ".", "LeakyReLU", "\n", "net_nonlin_kwargs", "=", "{", "'negative_slope'", ":", "1e-2", ",", "'inplace'", ":", "True", "}", "\n", "self", ".", "network", "=", "Generic_UNet", "(", "self", ".", "num_input_channels", ",", "self", ".", "base_num_features", ",", "self", ".", "num_classes", ",", "\n", "len", "(", "self", ".", "net_num_pool_op_kernel_sizes", ")", ",", "\n", "self", ".", "conv_per_stage", ",", "2", ",", "conv_op", ",", "norm_op", ",", "norm_op_kwargs", ",", "dropout_op", ",", "dropout_op_kwargs", ",", "\n", "net_nonlin", ",", "net_nonlin_kwargs", ",", "True", ",", "False", ",", "lambda", "x", ":", "x", ",", "InitWeights_He", "(", "1e-2", ")", ",", "\n", "self", ".", "net_num_pool_op_kernel_sizes", ",", "self", ".", "net_conv_kernel_sizes", ",", "False", ",", "True", ",", "True", ")", "\n", "if", "torch", ".", "cuda", ".", "is_available", "(", ")", ":", "\n", "            ", "self", ".", "network", ".", "cuda", "(", ")", "\n", "", "self", ".", "network", ".", "inference_apply_nonlin", "=", "softmax_helper", "\n", "", "", ""]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.architectural_variants.nnUNetTrainerV2_FRN.nnUNetTrainerV2_FRN.initialize_network": [[27, 55], ["nnunet.network_architecture.generic_UNet.Generic_UNet", "torch.cuda.is_available", "len", "nnunet.network_architecture.initialization.InitWeights_He", "nnUNetTrainerV2_FRN.nnUNetTrainerV2_FRN.network.cuda"], "methods", ["None"], ["    ", "def", "initialize_network", "(", "self", ")", ":", "\n", "        ", "\"\"\"\n        changed deep supervision to False\n        :return:\n        \"\"\"", "\n", "if", "self", ".", "threeD", ":", "\n", "            ", "conv_op", "=", "nn", ".", "Conv3d", "\n", "dropout_op", "=", "nn", ".", "Dropout3d", "\n", "norm_op", "=", "FRN3D", "\n", "\n", "", "else", ":", "\n", "            ", "conv_op", "=", "nn", ".", "Conv2d", "\n", "dropout_op", "=", "nn", ".", "Dropout2d", "\n", "raise", "NotImplementedError", "\n", "norm_op", "=", "nn", ".", "BatchNorm2d", "\n", "\n", "", "norm_op_kwargs", "=", "{", "'eps'", ":", "1e-6", "}", "\n", "dropout_op_kwargs", "=", "{", "'p'", ":", "0", ",", "'inplace'", ":", "True", "}", "\n", "net_nonlin", "=", "Identity", "\n", "net_nonlin_kwargs", "=", "{", "}", "\n", "self", ".", "network", "=", "Generic_UNet", "(", "self", ".", "num_input_channels", ",", "self", ".", "base_num_features", ",", "self", ".", "num_classes", ",", "\n", "len", "(", "self", ".", "net_num_pool_op_kernel_sizes", ")", ",", "\n", "self", ".", "conv_per_stage", ",", "2", ",", "conv_op", ",", "norm_op", ",", "norm_op_kwargs", ",", "dropout_op", ",", "dropout_op_kwargs", ",", "\n", "net_nonlin", ",", "net_nonlin_kwargs", ",", "True", ",", "False", ",", "lambda", "x", ":", "x", ",", "InitWeights_He", "(", "1e-2", ")", ",", "\n", "self", ".", "net_num_pool_op_kernel_sizes", ",", "self", ".", "net_conv_kernel_sizes", ",", "False", ",", "True", ",", "True", ")", "\n", "if", "torch", ".", "cuda", ".", "is_available", "(", ")", ":", "\n", "            ", "self", ".", "network", ".", "cuda", "(", ")", "\n", "", "self", ".", "network", ".", "inference_apply_nonlin", "=", "softmax_helper", "\n", "", "", ""]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.architectural_variants.nnUNetTrainerV2_ReLU_convReLUIN.nnUNetTrainerV2_ReLU_convReLUIN.initialize_network": [[23, 47], ["nnunet.network_architecture.generic_UNet.Generic_UNet", "torch.cuda.is_available", "len", "nnunet.network_architecture.initialization.InitWeights_He", "nnUNetTrainerV2_ReLU_convReLUIN.nnUNetTrainerV2_ReLU_convReLUIN.network.cuda"], "methods", ["None"], ["    ", "def", "initialize_network", "(", "self", ")", ":", "\n", "        ", "if", "self", ".", "threeD", ":", "\n", "            ", "conv_op", "=", "nn", ".", "Conv3d", "\n", "dropout_op", "=", "nn", ".", "Dropout3d", "\n", "norm_op", "=", "nn", ".", "InstanceNorm3d", "\n", "\n", "", "else", ":", "\n", "            ", "conv_op", "=", "nn", ".", "Conv2d", "\n", "dropout_op", "=", "nn", ".", "Dropout2d", "\n", "norm_op", "=", "nn", ".", "InstanceNorm2d", "\n", "\n", "", "norm_op_kwargs", "=", "{", "'eps'", ":", "1e-5", ",", "'affine'", ":", "True", "}", "\n", "dropout_op_kwargs", "=", "{", "'p'", ":", "0", ",", "'inplace'", ":", "True", "}", "\n", "net_nonlin", "=", "nn", ".", "ReLU", "\n", "net_nonlin_kwargs", "=", "{", "'inplace'", ":", "True", "}", "\n", "self", ".", "network", "=", "Generic_UNet", "(", "self", ".", "num_input_channels", ",", "self", ".", "base_num_features", ",", "self", ".", "num_classes", ",", "\n", "len", "(", "self", ".", "net_num_pool_op_kernel_sizes", ")", ",", "\n", "self", ".", "conv_per_stage", ",", "2", ",", "conv_op", ",", "norm_op", ",", "norm_op_kwargs", ",", "dropout_op", ",", "dropout_op_kwargs", ",", "\n", "net_nonlin", ",", "net_nonlin_kwargs", ",", "True", ",", "False", ",", "lambda", "x", ":", "x", ",", "InitWeights_He", "(", "0", ")", ",", "\n", "self", ".", "net_num_pool_op_kernel_sizes", ",", "self", ".", "net_conv_kernel_sizes", ",", "False", ",", "True", ",", "True", ",", "\n", "basic_block", "=", "ConvDropoutNonlinNorm", ")", "\n", "if", "torch", ".", "cuda", ".", "is_available", "(", ")", ":", "\n", "            ", "self", ".", "network", ".", "cuda", "(", ")", "\n", "", "self", ".", "network", ".", "inference_apply_nonlin", "=", "softmax_helper", "\n", "", "", ""]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.miscellaneous.nnUNetTrainerV2_fullEvals.nnUNetTrainerV2_fullEvals.__init__": [[32, 39], ["nnunet.training.network_training.nnUNetTrainerV2.nnUNetTrainerV2.__init__", "nnunet.evaluation.region_based_evaluation.get_brats_regions"], "methods", ["home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.common.layers.ResizeImage.__init__", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.evaluation.region_based_evaluation.get_brats_regions"], ["def", "__init__", "(", "self", ",", "plans_file", ",", "fold", ",", "output_folder", "=", "None", ",", "dataset_directory", "=", "None", ",", "batch_dice", "=", "True", ",", "stage", "=", "None", ",", "\n", "unpack_data", "=", "True", ",", "deterministic", "=", "True", ",", "fp16", "=", "False", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "plans_file", ",", "fold", ",", "output_folder", ",", "dataset_directory", ",", "batch_dice", ",", "stage", ",", "unpack_data", ",", "\n", "deterministic", ",", "fp16", ")", "\n", "self", ".", "validate_every", "=", "1", "\n", "self", ".", "evaluation_regions", "=", "get_brats_regions", "(", ")", "\n", "self", ".", "num_val_batches_per_epoch", "=", "0", "# we dont need this because this does not evaluate on full images", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.miscellaneous.nnUNetTrainerV2_fullEvals.nnUNetTrainerV2_fullEvals.finish_online_evaluation": [[40, 42], ["None"], "methods", ["None"], ["", "def", "finish_online_evaluation", "(", "self", ")", ":", "\n", "        ", "pass", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.miscellaneous.nnUNetTrainerV2_fullEvals.nnUNetTrainerV2_fullEvals.validate": [[43, 154], ["time.time.time", "nnUNetTrainerV2_fullEvals.nnUNetTrainerV2_fullEvals.network.eval", "join", "maybe_mkdir_p", "save_json", "multiprocessing.pool.Pool", "nnUNetTrainerV2_fullEvals.nnUNetTrainerV2_fullEvals.dataset_val.keys", "nnUNetTrainerV2_fullEvals.nnUNetTrainerV2_fullEvals.print_to_log_file", "nnUNetTrainerV2_fullEvals.nnUNetTrainerV2_fullEvals.print_to_log_file", "nnunet.evaluation.region_based_evaluation.evaluate_regions", "csv_file[].astype", "torch.cuda.is_available", "nnUNetTrainerV2_fullEvals.nnUNetTrainerV2_fullEvals.network.train", "time.time.time", "nnUNetTrainerV2_fullEvals.nnUNetTrainerV2_fullEvals.print_to_log_file", "nnUNetTrainerV2_fullEvals.nnUNetTrainerV2_fullEvals.print_to_log_file", "nnUNetTrainerV2_fullEvals.nnUNetTrainerV2_fullEvals.load_dataset", "nnUNetTrainerV2_fullEvals.nnUNetTrainerV2_fullEvals.do_split", "join", "load_pickle", "i.get", "numpy.loadtxt", "torch.cuda.empty_cache", "RuntimeError", "results.append", "join", "[].split", "isfile", "numpy.load", "nnUNetTrainerV2_fullEvals.nnUNetTrainerV2_fullEvals.predict_preprocessed_data_return_seg_and_softmax", "join", "multiprocessing.pool.Pool.starmap_async", "join", "isfile", "join", "join"], "methods", ["home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.network_training.nnUNetTrainerV2_DDP.nnUNetTrainerV2_DDP.print_to_log_file", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.network_training.nnUNetTrainerV2_DDP.nnUNetTrainerV2_DDP.print_to_log_file", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.evaluation.region_based_evaluation.evaluate_regions", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.network_training.nnUNetTrainerV2_DDP.nnUNetTrainerV2_DDP.print_to_log_file", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.network_training.nnUNetTrainerV2_DDP.nnUNetTrainerV2_DDP.print_to_log_file", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.dataloading.dataset_loading.load_dataset", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.network_training.nnUNetTrainerV2.nnUNetTrainerV2.do_split", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.architectural_variants.nnUNetTrainerV2_ResencUNet.nnUNetTrainerV2_ResencUNet.predict_preprocessed_data_return_seg_and_softmax"], ["", "def", "validate", "(", "self", ",", "do_mirroring", ":", "bool", "=", "True", ",", "use_sliding_window", ":", "bool", "=", "True", ",", "\n", "step_size", ":", "float", "=", "0.5", ",", "save_softmax", ":", "bool", "=", "True", ",", "use_gaussian", ":", "bool", "=", "True", ",", "overwrite", ":", "bool", "=", "True", ",", "\n", "validation_folder_name", ":", "str", "=", "'validation_raw'", ",", "debug", ":", "bool", "=", "False", ",", "all_in_gpu", ":", "bool", "=", "False", ",", "\n", "force_separate_z", ":", "bool", "=", "None", ",", "interpolation_order", ":", "int", "=", "3", ",", "interpolation_order_z", "=", "0", ")", ":", "\n", "        ", "\"\"\"\n        disable nnunet postprocessing. this would just waste computation time and does not benefit brats\n\n        !!!We run this with use_sliding_window=False per default (see on_epoch_end). This triggers fully convolutional\n        inference. THIS ONLY MAKES SENSE WHEN TRAINING ON FULL IMAGES! Make sure use_sliding_window=True when running\n        with default patch size (128x128x128)!!!\n\n        per default this does not use test time data augmentation (mirroring). The reference implementation, however,\n        does. I disabled it here because this eats up a lot of computation time\n\n        \"\"\"", "\n", "validation_start", "=", "time", "(", ")", "\n", "\n", "current_mode", "=", "self", ".", "network", ".", "training", "\n", "self", ".", "network", ".", "eval", "(", ")", "\n", "\n", "assert", "self", ".", "was_initialized", ",", "\"must initialize, ideally with checkpoint (or train first)\"", "\n", "if", "self", ".", "dataset_val", "is", "None", ":", "\n", "            ", "self", ".", "load_dataset", "(", ")", "\n", "self", ".", "do_split", "(", ")", "\n", "\n", "# predictions as they come from the network go here", "\n", "", "output_folder", "=", "join", "(", "self", ".", "output_folder", ",", "validation_folder_name", ")", "\n", "maybe_mkdir_p", "(", "output_folder", ")", "\n", "\n", "# this is for debug purposes", "\n", "my_input_args", "=", "{", "'do_mirroring'", ":", "do_mirroring", ",", "\n", "'use_sliding_window'", ":", "use_sliding_window", ",", "\n", "'step_size'", ":", "step_size", ",", "\n", "'save_softmax'", ":", "save_softmax", ",", "\n", "'use_gaussian'", ":", "use_gaussian", ",", "\n", "'overwrite'", ":", "overwrite", ",", "\n", "'validation_folder_name'", ":", "validation_folder_name", ",", "\n", "'debug'", ":", "debug", ",", "\n", "'all_in_gpu'", ":", "all_in_gpu", ",", "\n", "'force_separate_z'", ":", "force_separate_z", ",", "\n", "'interpolation_order'", ":", "interpolation_order", ",", "\n", "'interpolation_order_z'", ":", "interpolation_order_z", ",", "\n", "}", "\n", "save_json", "(", "my_input_args", ",", "join", "(", "output_folder", ",", "\"validation_args.json\"", ")", ")", "\n", "\n", "if", "do_mirroring", ":", "\n", "            ", "if", "not", "self", ".", "data_aug_params", "[", "'do_mirror'", "]", ":", "\n", "                ", "raise", "RuntimeError", "(", "\"We did not train with mirroring so you cannot do inference with mirroring enabled\"", ")", "\n", "", "mirror_axes", "=", "self", ".", "data_aug_params", "[", "'mirror_axes'", "]", "\n", "", "else", ":", "\n", "            ", "mirror_axes", "=", "(", ")", "\n", "\n", "", "export_pool", "=", "Pool", "(", "default_num_threads", ")", "\n", "results", "=", "[", "]", "\n", "\n", "for", "k", "in", "self", ".", "dataset_val", ".", "keys", "(", ")", ":", "\n", "            ", "properties", "=", "load_pickle", "(", "self", ".", "dataset", "[", "k", "]", "[", "'properties_file'", "]", ")", "\n", "fname", "=", "properties", "[", "'list_of_data_files'", "]", "[", "0", "]", ".", "split", "(", "\"/\"", ")", "[", "-", "1", "]", "[", ":", "-", "12", "]", "\n", "if", "overwrite", "or", "(", "not", "isfile", "(", "join", "(", "output_folder", ",", "fname", "+", "\".nii.gz\"", ")", ")", ")", "or", "(", "save_softmax", "and", "not", "isfile", "(", "join", "(", "output_folder", ",", "fname", "+", "\".npz\"", ")", ")", ")", ":", "\n", "                ", "data", "=", "np", ".", "load", "(", "self", ".", "dataset", "[", "k", "]", "[", "'data_file'", "]", ")", "[", "'data'", "]", "\n", "\n", "#print(k, data.shape)", "\n", "\n", "softmax_pred", "=", "self", ".", "predict_preprocessed_data_return_seg_and_softmax", "(", "data", "[", ":", "-", "1", "]", ",", "do_mirroring", ",", "\n", "mirror_axes", ",", "use_sliding_window", ",", "\n", "step_size", ",", "use_gaussian", ",", "\n", "all_in_gpu", "=", "all_in_gpu", ",", "\n", "verbose", "=", "False", ",", "\n", "mixed_precision", "=", "self", ".", "fp16", ")", "[", "1", "]", "\n", "\n", "# this does not do anything in brats -> remove this line", "\n", "# softmax_pred = softmax_pred.transpose([0] + [i + 1 for i in self.transpose_backward])", "\n", "\n", "if", "save_softmax", ":", "\n", "                    ", "softmax_fname", "=", "join", "(", "output_folder", ",", "fname", "+", "\".npz\"", ")", "\n", "", "else", ":", "\n", "                    ", "softmax_fname", "=", "None", "\n", "\n", "", "results", ".", "append", "(", "export_pool", ".", "starmap_async", "(", "save_segmentation_nifti_from_softmax", ",", "\n", "(", "(", "softmax_pred", ",", "join", "(", "output_folder", ",", "fname", "+", "\".nii.gz\"", ")", ",", "\n", "properties", ",", "interpolation_order", ",", "None", ",", "None", ",", "None", ",", "\n", "softmax_fname", ",", "None", ",", "force_separate_z", ",", "\n", "interpolation_order_z", ",", "False", ")", ",", "\n", ")", "\n", ")", "\n", ")", "\n", "\n", "", "", "_", "=", "[", "i", ".", "get", "(", ")", "for", "i", "in", "results", "]", "\n", "self", ".", "print_to_log_file", "(", "\"finished prediction\"", ")", "\n", "\n", "# evaluate raw predictions", "\n", "self", ".", "print_to_log_file", "(", "\"evaluation of raw predictions\"", ")", "\n", "\n", "# this writes a csv file into output_folder", "\n", "evaluate_regions", "(", "output_folder", ",", "self", ".", "gt_niftis_folder", ",", "self", ".", "evaluation_regions", ")", "\n", "csv_file", "=", "np", ".", "loadtxt", "(", "join", "(", "output_folder", ",", "'summary.csv'", ")", ",", "skiprows", "=", "1", ",", "dtype", "=", "str", ",", "delimiter", "=", "','", ")", "[", ":", ",", "1", ":", "]", "\n", "\n", "# these are the values that are compute with np.nanmean aggregation", "\n", "whole", ",", "core", ",", "enhancing", "=", "csv_file", "[", "-", "4", ",", ":", "]", ".", "astype", "(", "float", ")", "\n", "\n", "# do some cleanup", "\n", "if", "torch", ".", "cuda", ".", "is_available", "(", ")", ":", "\n", "            ", "torch", ".", "cuda", ".", "empty_cache", "(", ")", "\n", "\n", "", "self", ".", "network", ".", "train", "(", "current_mode", ")", "\n", "validation_end", "=", "time", "(", ")", "\n", "self", ".", "print_to_log_file", "(", "'Running the validation took %f seconds'", "%", "(", "validation_end", "-", "validation_start", ")", ")", "\n", "self", ".", "print_to_log_file", "(", "'(the time needed for validation is included in the total epoch time!)'", ")", "\n", "\n", "return", "whole", ",", "core", ",", "enhancing", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.miscellaneous.nnUNetTrainerV2_fullEvals.nnUNetTrainerV2_fullEvals.on_epoch_end": [[155, 193], ["super().on_epoch_end", "nnUNetTrainerV2_fullEvals.nnUNetTrainerV2_fullEvals.validate", "numpy.mean", "nnUNetTrainerV2_fullEvals.nnUNetTrainerV2_fullEvals.print_to_log_file", "nnUNetTrainerV2_fullEvals.nnUNetTrainerV2_fullEvals.print_to_log_file", "numpy.mean", "nnUNetTrainerV2_fullEvals.nnUNetTrainerV2_fullEvals.all_val_eval_metrics.append", "nnUNetTrainerV2_fullEvals.nnUNetTrainerV2_fullEvals.print_to_log_file", "nnUNetTrainerV2_fullEvals.nnUNetTrainerV2_fullEvals.print_to_log_file", "nnUNetTrainerV2_fullEvals.nnUNetTrainerV2_fullEvals.save_checkpoint", "join"], "methods", ["home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.optimizer_and_lr.nnUNetTrainerV2_reduceMomentumDuringTraining.nnUNetTrainerV2_reduceMomentumDuringTraining.on_epoch_end", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.BraTS_trainer.nnUNetTrainerV2BraTSRegions.nnUNetTrainerV2BraTSRegions_DA3_BN.validate", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.network_training.nnUNetTrainerV2_DDP.nnUNetTrainerV2_DDP.print_to_log_file", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.network_training.nnUNetTrainerV2_DDP.nnUNetTrainerV2_DDP.print_to_log_file", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.network_training.nnUNetTrainerV2_DDP.nnUNetTrainerV2_DDP.print_to_log_file", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.network_training.nnUNetTrainerV2_DDP.nnUNetTrainerV2_DDP.print_to_log_file", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.profiling.nnUNetTrainerV2_2epochs.nnUNetTrainerV2_DDP_5epochs.save_checkpoint"], ["", "def", "on_epoch_end", "(", "self", ")", ":", "\n", "        ", "return_value", "=", "True", "\n", "\n", "# on epoch end is called before the epoch counter is incremented, so we need to do that here to get the correct epoch number", "\n", "if", "(", "self", ".", "epoch", "+", "1", ")", "%", "self", ".", "validate_every", "==", "0", ":", "\n", "            ", "whole", ",", "core", ",", "enhancing", "=", "self", ".", "validate", "(", "do_mirroring", "=", "False", ",", "use_sliding_window", "=", "True", ",", "\n", "step_size", "=", "0.5", ",", "\n", "save_softmax", "=", "False", ",", "\n", "use_gaussian", "=", "True", ",", "overwrite", "=", "True", ",", "\n", "validation_folder_name", "=", "'validation_after_ep_%04.0d'", "%", "self", ".", "epoch", ",", "\n", "debug", "=", "False", ",", "all_in_gpu", "=", "True", ")", "\n", "\n", "here", "=", "np", ".", "mean", "(", "(", "whole", ",", "core", ",", "enhancing", ")", ")", "\n", "\n", "self", ".", "print_to_log_file", "(", "\"After epoch %d: whole %0.4f core %0.4f enhancing: %0.4f\"", "%", "\n", "(", "self", ".", "epoch", ",", "whole", ",", "core", ",", "enhancing", ")", ")", "\n", "self", ".", "print_to_log_file", "(", "\"Mean: %0.4f\"", "%", "here", ")", "\n", "\n", "# now we need to figure out if we are done", "\n", "fully_trained_nnunet", "=", "(", "0.911", ",", "0.8739", ",", "0.7848", ")", "\n", "mean_dice", "=", "np", ".", "mean", "(", "fully_trained_nnunet", ")", "\n", "target", "=", "0.97", "*", "mean_dice", "\n", "\n", "self", ".", "all_val_eval_metrics", ".", "append", "(", "here", ")", "\n", "self", ".", "print_to_log_file", "(", "\"Target mean: %0.4f\"", "%", "target", ")", "\n", "\n", "if", "here", ">=", "target", ":", "\n", "                ", "self", ".", "print_to_log_file", "(", "\"I am done!\"", ")", "\n", "self", ".", "save_checkpoint", "(", "join", "(", "self", ".", "output_folder", ",", "\"model_final_checkpoint.model\"", ")", ")", "\n", "return_value", "=", "False", "# this triggers early stopping", "\n", "\n", "", "", "ret_old", "=", "super", "(", ")", ".", "on_epoch_end", "(", ")", "\n", "# if we do not achieve the target accuracy in 1000 epochs then we need to stop the training. This is not built", "\n", "# to run longer than 1000 epochs", "\n", "if", "not", "ret_old", ":", "\n", "            ", "return_value", "=", "ret_old", "\n", "\n", "", "return", "return_value", "\n", "", "", ""]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.cascade.nnUNetTrainerV2CascadeFullRes_shorter_lowerLR.nnUNetTrainerV2CascadeFullRes_shorter_lowerLR.__init__": [[20, 27], ["nnunet.training.network_training.nnUNetTrainerV2_CascadeFullRes.nnUNetTrainerV2CascadeFullRes.__init__"], "methods", ["home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.common.layers.ResizeImage.__init__"], ["    ", "def", "__init__", "(", "self", ",", "plans_file", ",", "fold", ",", "output_folder", "=", "None", ",", "dataset_directory", "=", "None", ",", "batch_dice", "=", "True", ",", "stage", "=", "None", ",", "\n", "unpack_data", "=", "True", ",", "deterministic", "=", "True", ",", "previous_trainer", "=", "\"nnUNetTrainerV2\"", ",", "fp16", "=", "False", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "plans_file", ",", "fold", ",", "output_folder", ",", "dataset_directory", ",", "\n", "batch_dice", ",", "stage", ",", "unpack_data", ",", "deterministic", ",", "\n", "previous_trainer", ",", "fp16", ")", "\n", "self", ".", "max_num_epochs", "=", "500", "\n", "self", ".", "initial_lr", "=", "1e-3", "\n", "", "", ""]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.cascade.nnUNetTrainerV2CascadeFullRes_shorter.nnUNetTrainerV2CascadeFullRes_shorter.__init__": [[20, 26], ["nnunet.training.network_training.nnUNetTrainerV2_CascadeFullRes.nnUNetTrainerV2CascadeFullRes.__init__"], "methods", ["home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.common.layers.ResizeImage.__init__"], ["    ", "def", "__init__", "(", "self", ",", "plans_file", ",", "fold", ",", "output_folder", "=", "None", ",", "dataset_directory", "=", "None", ",", "batch_dice", "=", "True", ",", "stage", "=", "None", ",", "\n", "unpack_data", "=", "True", ",", "deterministic", "=", "True", ",", "previous_trainer", "=", "\"nnUNetTrainerV2\"", ",", "fp16", "=", "False", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "plans_file", ",", "fold", ",", "output_folder", ",", "dataset_directory", ",", "\n", "batch_dice", ",", "stage", ",", "unpack_data", ",", "deterministic", ",", "\n", "previous_trainer", ",", "fp16", ")", "\n", "self", ".", "max_num_epochs", "=", "500", "\n", "", "", ""]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.cascade.nnUNetTrainerV2CascadeFullRes_lowerLR.nnUNetTrainerV2CascadeFullRes_lowerLR.__init__": [[20, 26], ["nnunet.training.network_training.nnUNetTrainerV2_CascadeFullRes.nnUNetTrainerV2CascadeFullRes.__init__"], "methods", ["home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.common.layers.ResizeImage.__init__"], ["    ", "def", "__init__", "(", "self", ",", "plans_file", ",", "fold", ",", "output_folder", "=", "None", ",", "dataset_directory", "=", "None", ",", "batch_dice", "=", "True", ",", "stage", "=", "None", ",", "\n", "unpack_data", "=", "True", ",", "deterministic", "=", "True", ",", "previous_trainer", "=", "\"nnUNetTrainerV2\"", ",", "fp16", "=", "False", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "plans_file", ",", "fold", ",", "output_folder", ",", "dataset_directory", ",", "\n", "batch_dice", ",", "stage", ",", "unpack_data", ",", "deterministic", ",", "\n", "previous_trainer", ",", "fp16", ")", "\n", "self", ".", "initial_lr", "=", "1e-3", "\n", "", "", ""]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.cascade.nnUNetTrainerV2CascadeFullRes_DAVariants.nnUNetTrainerV2CascadeFullRes_noConnComp.setup_DA_params": [[20, 31], ["super().setup_DA_params"], "methods", ["home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.BraTS_trainer.nnUNetTrainerV2BraTSRegions.nnUNetTrainerV2BraTSRegions_DA4_BN.setup_DA_params"], ["    ", "def", "setup_DA_params", "(", "self", ")", ":", "\n", "        ", "super", "(", ")", ".", "setup_DA_params", "(", ")", "\n", "self", ".", "data_aug_params", "[", "'cascade_do_cascade_augmentations'", "]", "=", "True", "\n", "\n", "self", ".", "data_aug_params", "[", "'cascade_random_binary_transform_p'", "]", "=", "0.4", "\n", "self", ".", "data_aug_params", "[", "'cascade_random_binary_transform_p_per_label'", "]", "=", "1", "\n", "self", ".", "data_aug_params", "[", "'cascade_random_binary_transform_size'", "]", "=", "(", "1", ",", "8", ")", "\n", "\n", "self", ".", "data_aug_params", "[", "'cascade_remove_conn_comp_p'", "]", "=", "0.0", "\n", "self", ".", "data_aug_params", "[", "'cascade_remove_conn_comp_max_size_percent_threshold'", "]", "=", "0.15", "\n", "self", ".", "data_aug_params", "[", "'cascade_remove_conn_comp_fill_with_other_class_p'", "]", "=", "0.0", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.cascade.nnUNetTrainerV2CascadeFullRes_DAVariants.nnUNetTrainerV2CascadeFullRes_smallerBinStrel.setup_DA_params": [[34, 45], ["super().setup_DA_params"], "methods", ["home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.BraTS_trainer.nnUNetTrainerV2BraTSRegions.nnUNetTrainerV2BraTSRegions_DA4_BN.setup_DA_params"], ["    ", "def", "setup_DA_params", "(", "self", ")", ":", "\n", "        ", "super", "(", ")", ".", "setup_DA_params", "(", ")", "\n", "self", ".", "data_aug_params", "[", "'cascade_do_cascade_augmentations'", "]", "=", "True", "\n", "\n", "self", ".", "data_aug_params", "[", "'cascade_random_binary_transform_p'", "]", "=", "0.4", "\n", "self", ".", "data_aug_params", "[", "'cascade_random_binary_transform_p_per_label'", "]", "=", "1", "\n", "self", ".", "data_aug_params", "[", "'cascade_random_binary_transform_size'", "]", "=", "(", "1", ",", "5", ")", "\n", "\n", "self", ".", "data_aug_params", "[", "'cascade_remove_conn_comp_p'", "]", "=", "0.2", "\n", "self", ".", "data_aug_params", "[", "'cascade_remove_conn_comp_max_size_percent_threshold'", "]", "=", "0.15", "\n", "self", ".", "data_aug_params", "[", "'cascade_remove_conn_comp_fill_with_other_class_p'", "]", "=", "0.0", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.cascade.nnUNetTrainerV2CascadeFullRes_DAVariants.nnUNetTrainerV2CascadeFullRes_EducatedGuess.setup_DA_params": [[48, 59], ["super().setup_DA_params"], "methods", ["home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.BraTS_trainer.nnUNetTrainerV2BraTSRegions.nnUNetTrainerV2BraTSRegions_DA4_BN.setup_DA_params"], ["    ", "def", "setup_DA_params", "(", "self", ")", ":", "\n", "        ", "super", "(", ")", ".", "setup_DA_params", "(", ")", "\n", "self", ".", "data_aug_params", "[", "'cascade_do_cascade_augmentations'", "]", "=", "True", "\n", "\n", "self", ".", "data_aug_params", "[", "'cascade_random_binary_transform_p'", "]", "=", "0.5", "\n", "self", ".", "data_aug_params", "[", "'cascade_random_binary_transform_p_per_label'", "]", "=", "0.5", "\n", "self", ".", "data_aug_params", "[", "'cascade_random_binary_transform_size'", "]", "=", "(", "1", ",", "5", ")", "\n", "\n", "self", ".", "data_aug_params", "[", "'cascade_remove_conn_comp_p'", "]", "=", "0.2", "\n", "self", ".", "data_aug_params", "[", "'cascade_remove_conn_comp_max_size_percent_threshold'", "]", "=", "0.10", "\n", "self", ".", "data_aug_params", "[", "'cascade_remove_conn_comp_fill_with_other_class_p'", "]", "=", "0.0", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.cascade.nnUNetTrainerV2CascadeFullRes_DAVariants.nnUNetTrainerV2CascadeFullRes_EducatedGuess2.setup_DA_params": [[62, 73], ["super().setup_DA_params"], "methods", ["home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.BraTS_trainer.nnUNetTrainerV2BraTSRegions.nnUNetTrainerV2BraTSRegions_DA4_BN.setup_DA_params"], ["    ", "def", "setup_DA_params", "(", "self", ")", ":", "\n", "        ", "super", "(", ")", ".", "setup_DA_params", "(", ")", "\n", "self", ".", "data_aug_params", "[", "'cascade_do_cascade_augmentations'", "]", "=", "True", "\n", "\n", "self", ".", "data_aug_params", "[", "'cascade_random_binary_transform_p'", "]", "=", "0.5", "\n", "self", ".", "data_aug_params", "[", "'cascade_random_binary_transform_p_per_label'", "]", "=", "0.5", "\n", "self", ".", "data_aug_params", "[", "'cascade_random_binary_transform_size'", "]", "=", "(", "1", ",", "5", ")", "\n", "\n", "self", ".", "data_aug_params", "[", "'cascade_remove_conn_comp_p'", "]", "=", "0.0", "\n", "self", ".", "data_aug_params", "[", "'cascade_remove_conn_comp_max_size_percent_threshold'", "]", "=", "0.10", "\n", "self", ".", "data_aug_params", "[", "'cascade_remove_conn_comp_fill_with_other_class_p'", "]", "=", "0.0", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.cascade.nnUNetTrainerV2CascadeFullRes_DAVariants.nnUNetTrainerV2CascadeFullRes_EducatedGuess3.setup_DA_params": [[76, 87], ["super().setup_DA_params"], "methods", ["home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.BraTS_trainer.nnUNetTrainerV2BraTSRegions.nnUNetTrainerV2BraTSRegions_DA4_BN.setup_DA_params"], ["    ", "def", "setup_DA_params", "(", "self", ")", ":", "\n", "        ", "super", "(", ")", ".", "setup_DA_params", "(", ")", "\n", "self", ".", "data_aug_params", "[", "'cascade_do_cascade_augmentations'", "]", "=", "True", "\n", "\n", "self", ".", "data_aug_params", "[", "'cascade_random_binary_transform_p'", "]", "=", "1", "\n", "self", ".", "data_aug_params", "[", "'cascade_random_binary_transform_p_per_label'", "]", "=", "0.33", "\n", "self", ".", "data_aug_params", "[", "'cascade_random_binary_transform_size'", "]", "=", "(", "1", ",", "5", ")", "\n", "\n", "self", ".", "data_aug_params", "[", "'cascade_remove_conn_comp_p'", "]", "=", "0.0", "\n", "self", ".", "data_aug_params", "[", "'cascade_remove_conn_comp_max_size_percent_threshold'", "]", "=", "0.10", "\n", "self", ".", "data_aug_params", "[", "'cascade_remove_conn_comp_fill_with_other_class_p'", "]", "=", "0.0", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.copies.nnUNetTrainerV2_copies.nnUNetTrainerV2_copy1.__init__": [[24, 28], ["nnunet.training.network_training.nnUNetTrainerV2.nnUNetTrainerV2.__init__"], "methods", ["home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.common.layers.ResizeImage.__init__"], ["    ", "def", "__init__", "(", "self", ",", "plans_file", ",", "fold", ",", "output_folder", "=", "None", ",", "dataset_directory", "=", "None", ",", "batch_dice", "=", "True", ",", "stage", "=", "None", ",", "\n", "unpack_data", "=", "True", ",", "deterministic", "=", "True", ",", "fp16", "=", "False", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "plans_file", ",", "fold", ",", "output_folder", ",", "dataset_directory", ",", "batch_dice", ",", "stage", ",", "unpack_data", ",", "\n", "deterministic", ",", "fp16", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.copies.nnUNetTrainerV2_copies.nnUNetTrainerV2_copy2.__init__": [[31, 35], ["nnunet.training.network_training.nnUNetTrainerV2.nnUNetTrainerV2.__init__"], "methods", ["home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.common.layers.ResizeImage.__init__"], ["    ", "def", "__init__", "(", "self", ",", "plans_file", ",", "fold", ",", "output_folder", "=", "None", ",", "dataset_directory", "=", "None", ",", "batch_dice", "=", "True", ",", "stage", "=", "None", ",", "\n", "unpack_data", "=", "True", ",", "deterministic", "=", "True", ",", "fp16", "=", "False", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "plans_file", ",", "fold", ",", "output_folder", ",", "dataset_directory", ",", "batch_dice", ",", "stage", ",", "unpack_data", ",", "\n", "deterministic", ",", "fp16", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.copies.nnUNetTrainerV2_copies.nnUNetTrainerV2_copy3.__init__": [[38, 42], ["nnunet.training.network_training.nnUNetTrainerV2.nnUNetTrainerV2.__init__"], "methods", ["home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.common.layers.ResizeImage.__init__"], ["    ", "def", "__init__", "(", "self", ",", "plans_file", ",", "fold", ",", "output_folder", "=", "None", ",", "dataset_directory", "=", "None", ",", "batch_dice", "=", "True", ",", "stage", "=", "None", ",", "\n", "unpack_data", "=", "True", ",", "deterministic", "=", "True", ",", "fp16", "=", "False", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "plans_file", ",", "fold", ",", "output_folder", ",", "dataset_directory", ",", "batch_dice", ",", "stage", ",", "unpack_data", ",", "\n", "deterministic", ",", "fp16", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.copies.nnUNetTrainerV2_copies.nnUNetTrainerV2_copy4.__init__": [[45, 49], ["nnunet.training.network_training.nnUNetTrainerV2.nnUNetTrainerV2.__init__"], "methods", ["home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.common.layers.ResizeImage.__init__"], ["    ", "def", "__init__", "(", "self", ",", "plans_file", ",", "fold", ",", "output_folder", "=", "None", ",", "dataset_directory", "=", "None", ",", "batch_dice", "=", "True", ",", "stage", "=", "None", ",", "\n", "unpack_data", "=", "True", ",", "deterministic", "=", "True", ",", "fp16", "=", "False", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "plans_file", ",", "fold", ",", "output_folder", ",", "dataset_directory", ",", "batch_dice", ",", "stage", ",", "unpack_data", ",", "\n", "deterministic", ",", "fp16", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.optimizer_and_lr.nnUNetTrainerV2_Ranger_lr3en3.nnUNetTrainerV2_Ranger_lr3en3.__init__": [[21, 26], ["nnunet.training.network_training.nnUNetTrainerV2.nnUNetTrainerV2.__init__"], "methods", ["home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.common.layers.ResizeImage.__init__"], ["    ", "def", "__init__", "(", "self", ",", "plans_file", ",", "fold", ",", "output_folder", "=", "None", ",", "dataset_directory", "=", "None", ",", "batch_dice", "=", "True", ",", "stage", "=", "None", ",", "\n", "unpack_data", "=", "True", ",", "deterministic", "=", "True", ",", "fp16", "=", "False", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "plans_file", ",", "fold", ",", "output_folder", ",", "dataset_directory", ",", "batch_dice", ",", "stage", ",", "unpack_data", ",", "\n", "deterministic", ",", "fp16", ")", "\n", "self", ".", "initial_lr", "=", "3e-3", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.optimizer_and_lr.nnUNetTrainerV2_Ranger_lr3en3.nnUNetTrainerV2_Ranger_lr3en3.initialize_optimizer_and_scheduler": [[27, 31], ["nnunet.training.optimizer.ranger.Ranger", "nnUNetTrainerV2_Ranger_lr3en3.nnUNetTrainerV2_Ranger_lr3en3.network.parameters"], "methods", ["None"], ["", "def", "initialize_optimizer_and_scheduler", "(", "self", ")", ":", "\n", "        ", "self", ".", "optimizer", "=", "Ranger", "(", "self", ".", "network", ".", "parameters", "(", ")", ",", "self", ".", "initial_lr", ",", "k", "=", "6", ",", "N_sma_threshhold", "=", "5", ",", "\n", "weight_decay", "=", "self", ".", "weight_decay", ")", "\n", "self", ".", "lr_scheduler", "=", "None", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.optimizer_and_lr.nnUNetTrainerV2_Ranger_lr3en4.nnUNetTrainerV2_Ranger_lr3en4.__init__": [[21, 26], ["nnunet.training.network_training.nnUNetTrainerV2.nnUNetTrainerV2.__init__"], "methods", ["home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.common.layers.ResizeImage.__init__"], ["    ", "def", "__init__", "(", "self", ",", "plans_file", ",", "fold", ",", "output_folder", "=", "None", ",", "dataset_directory", "=", "None", ",", "batch_dice", "=", "True", ",", "stage", "=", "None", ",", "\n", "unpack_data", "=", "True", ",", "deterministic", "=", "True", ",", "fp16", "=", "False", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "plans_file", ",", "fold", ",", "output_folder", ",", "dataset_directory", ",", "batch_dice", ",", "stage", ",", "unpack_data", ",", "\n", "deterministic", ",", "fp16", ")", "\n", "self", ".", "initial_lr", "=", "3e-4", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.optimizer_and_lr.nnUNetTrainerV2_Ranger_lr3en4.nnUNetTrainerV2_Ranger_lr3en4.initialize_optimizer_and_scheduler": [[27, 31], ["nnunet.training.optimizer.ranger.Ranger", "nnUNetTrainerV2_Ranger_lr3en4.nnUNetTrainerV2_Ranger_lr3en4.network.parameters"], "methods", ["None"], ["", "def", "initialize_optimizer_and_scheduler", "(", "self", ")", ":", "\n", "        ", "self", ".", "optimizer", "=", "Ranger", "(", "self", ".", "network", ".", "parameters", "(", ")", ",", "self", ".", "initial_lr", ",", "k", "=", "6", ",", "N_sma_threshhold", "=", "5", ",", "\n", "weight_decay", "=", "self", ".", "weight_decay", ")", "\n", "self", ".", "lr_scheduler", "=", "None", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.optimizer_and_lr.nnUNetTrainerV2_SGD_ReduceOnPlateau.nnUNetTrainerV2_SGD_ReduceOnPlateau.__init__": [[23, 27], ["nnunet.training.network_training.nnUNetTrainerV2.nnUNetTrainerV2.__init__"], "methods", ["home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.common.layers.ResizeImage.__init__"], ["    ", "def", "__init__", "(", "self", ",", "plans_file", ",", "fold", ",", "output_folder", "=", "None", ",", "dataset_directory", "=", "None", ",", "batch_dice", "=", "True", ",", "stage", "=", "None", ",", "\n", "unpack_data", "=", "True", ",", "deterministic", "=", "True", ",", "fp16", "=", "False", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "plans_file", ",", "fold", ",", "output_folder", ",", "dataset_directory", ",", "batch_dice", ",", "stage", ",", "unpack_data", ",", "\n", "deterministic", ",", "fp16", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.optimizer_and_lr.nnUNetTrainerV2_SGD_ReduceOnPlateau.nnUNetTrainerV2_SGD_ReduceOnPlateau.initialize_optimizer_and_scheduler": [[28, 35], ["torch.optim.SGD", "torch.optim.lr_scheduler.ReduceLROnPlateau", "nnUNetTrainerV2_SGD_ReduceOnPlateau.nnUNetTrainerV2_SGD_ReduceOnPlateau.network.parameters"], "methods", ["None"], ["", "def", "initialize_optimizer_and_scheduler", "(", "self", ")", ":", "\n", "        ", "self", ".", "optimizer", "=", "torch", ".", "optim", ".", "SGD", "(", "self", ".", "network", ".", "parameters", "(", ")", ",", "self", ".", "initial_lr", ",", "weight_decay", "=", "self", ".", "weight_decay", ",", "\n", "momentum", "=", "0.99", ",", "nesterov", "=", "True", ")", "\n", "self", ".", "lr_scheduler", "=", "lr_scheduler", ".", "ReduceLROnPlateau", "(", "self", ".", "optimizer", ",", "mode", "=", "'min'", ",", "factor", "=", "0.2", ",", "\n", "patience", "=", "self", ".", "lr_scheduler_patience", ",", "\n", "verbose", "=", "True", ",", "threshold", "=", "self", ".", "lr_scheduler_eps", ",", "\n", "threshold_mode", "=", "\"abs\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.optimizer_and_lr.nnUNetTrainerV2_SGD_ReduceOnPlateau.nnUNetTrainerV2_SGD_ReduceOnPlateau.maybe_update_lr": [[36, 48], ["nnUNetTrainerV2_SGD_ReduceOnPlateau.nnUNetTrainerV2_SGD_ReduceOnPlateau.print_to_log_file", "isinstance", "isinstance", "nnUNetTrainerV2_SGD_ReduceOnPlateau.nnUNetTrainerV2_SGD_ReduceOnPlateau.lr_scheduler.step", "str", "nnUNetTrainerV2_SGD_ReduceOnPlateau.nnUNetTrainerV2_SGD_ReduceOnPlateau.lr_scheduler.step"], "methods", ["home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.network_training.nnUNetTrainerV2_DDP.nnUNetTrainerV2_DDP.print_to_log_file", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.optimizer.ranger.Ranger.step", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.optimizer.ranger.Ranger.step"], ["", "def", "maybe_update_lr", "(", "self", ",", "epoch", "=", "None", ")", ":", "\n", "# maybe update learning rate", "\n", "        ", "if", "self", ".", "lr_scheduler", "is", "not", "None", ":", "\n", "            ", "assert", "isinstance", "(", "self", ".", "lr_scheduler", ",", "(", "lr_scheduler", ".", "ReduceLROnPlateau", ",", "lr_scheduler", ".", "_LRScheduler", ")", ")", "\n", "\n", "if", "isinstance", "(", "self", ".", "lr_scheduler", ",", "lr_scheduler", ".", "ReduceLROnPlateau", ")", ":", "\n", "# lr scheduler is updated with moving average val loss. should be more robust", "\n", "                ", "if", "self", ".", "epoch", ">", "0", ":", "# otherwise self.train_loss_MA is None", "\n", "                    ", "self", ".", "lr_scheduler", ".", "step", "(", "self", ".", "train_loss_MA", ")", "\n", "", "", "else", ":", "\n", "                ", "self", ".", "lr_scheduler", ".", "step", "(", "self", ".", "epoch", "+", "1", ")", "\n", "", "", "self", ".", "print_to_log_file", "(", "\"lr is now (scheduler) %s\"", "%", "str", "(", "self", ".", "optimizer", ".", "param_groups", "[", "0", "]", "[", "'lr'", "]", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.optimizer_and_lr.nnUNetTrainerV2_SGD_ReduceOnPlateau.nnUNetTrainerV2_SGD_ReduceOnPlateau.on_epoch_end": [[49, 51], ["nnunet.training.network_training.nnUNetTrainer.nnUNetTrainer.on_epoch_end"], "methods", ["home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.optimizer_and_lr.nnUNetTrainerV2_reduceMomentumDuringTraining.nnUNetTrainerV2_reduceMomentumDuringTraining.on_epoch_end"], ["", "def", "on_epoch_end", "(", "self", ")", ":", "\n", "        ", "return", "nnUNetTrainer", ".", "on_epoch_end", "(", "self", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.optimizer_and_lr.nnUNetTrainerV2_momentum098.nnUNetTrainerV2_momentum098.initialize_optimizer_and_scheduler": [[22, 27], ["torch.optim.SGD", "nnUNetTrainerV2_momentum098.nnUNetTrainerV2_momentum098.network.parameters"], "methods", ["None"], ["    ", "def", "initialize_optimizer_and_scheduler", "(", "self", ")", ":", "\n", "        ", "assert", "self", ".", "network", "is", "not", "None", ",", "\"self.initialize_network must be called first\"", "\n", "self", ".", "optimizer", "=", "torch", ".", "optim", ".", "SGD", "(", "self", ".", "network", ".", "parameters", "(", ")", ",", "self", ".", "initial_lr", ",", "weight_decay", "=", "self", ".", "weight_decay", ",", "\n", "momentum", "=", "0.98", ",", "nesterov", "=", "True", ")", "\n", "self", ".", "lr_scheduler", "=", "None", "\n", "", "", ""]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.optimizer_and_lr.nnUNetTrainerV2_SGD_fixedSchedule2.nnUNetTrainerV2_SGD_fixedSchedule2.__init__": [[21, 25], ["nnunet.training.network_training.nnUNetTrainerV2.nnUNetTrainerV2.__init__"], "methods", ["home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.common.layers.ResizeImage.__init__"], ["    ", "def", "__init__", "(", "self", ",", "plans_file", ",", "fold", ",", "output_folder", "=", "None", ",", "dataset_directory", "=", "None", ",", "batch_dice", "=", "True", ",", "stage", "=", "None", ",", "\n", "unpack_data", "=", "True", ",", "deterministic", "=", "True", ",", "fp16", "=", "False", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "plans_file", ",", "fold", ",", "output_folder", ",", "dataset_directory", ",", "batch_dice", ",", "stage", ",", "unpack_data", ",", "\n", "deterministic", ",", "fp16", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.optimizer_and_lr.nnUNetTrainerV2_SGD_fixedSchedule2.nnUNetTrainerV2_SGD_fixedSchedule2.maybe_update_lr": [[26, 48], ["nnUNetTrainerV2_SGD_fixedSchedule2.nnUNetTrainerV2_SGD_fixedSchedule2.print_to_log_file", "nnunet.training.learning_rate.poly_lr.poly_lr", "RuntimeError"], "methods", ["home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.network_training.nnUNetTrainerV2_DDP.nnUNetTrainerV2_DDP.print_to_log_file", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.learning_rate.poly_lr.poly_lr"], ["", "def", "maybe_update_lr", "(", "self", ",", "epoch", "=", "None", ")", ":", "\n", "        ", "\"\"\"\n        here we go one step, then use polyLR\n        :param epoch:\n        :return:\n        \"\"\"", "\n", "if", "epoch", "is", "None", ":", "\n", "            ", "ep", "=", "self", ".", "epoch", "+", "1", "\n", "", "else", ":", "\n", "            ", "ep", "=", "epoch", "\n", "\n", "", "if", "0", "<=", "ep", "<", "500", ":", "\n", "            ", "new_lr", "=", "self", ".", "initial_lr", "\n", "", "elif", "500", "<=", "ep", "<", "675", ":", "\n", "            ", "new_lr", "=", "self", ".", "initial_lr", "*", "0.1", "\n", "", "elif", "ep", ">=", "675", ":", "\n", "            ", "new_lr", "=", "poly_lr", "(", "ep", "-", "675", ",", "self", ".", "max_num_epochs", "-", "675", ",", "self", ".", "initial_lr", "*", "0.1", ",", "0.9", ")", "\n", "", "else", ":", "\n", "            ", "raise", "RuntimeError", "(", "\"Really unexpected things happened, ep=%d\"", "%", "ep", ")", "\n", "\n", "", "self", ".", "optimizer", ".", "param_groups", "[", "0", "]", "[", "'lr'", "]", "=", "new_lr", "\n", "self", ".", "print_to_log_file", "(", "\"lr:\"", ",", "self", ".", "optimizer", ".", "param_groups", "[", "0", "]", "[", "'lr'", "]", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.optimizer_and_lr.nnUNetTrainerV2_Adam_ReduceOnPlateau.nnUNetTrainerV2_Adam_ReduceOnPlateau.__init__": [[26, 31], ["nnunet.training.network_training.nnUNetTrainerV2.nnUNetTrainerV2.__init__"], "methods", ["home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.common.layers.ResizeImage.__init__"], ["def", "__init__", "(", "self", ",", "plans_file", ",", "fold", ",", "output_folder", "=", "None", ",", "dataset_directory", "=", "None", ",", "batch_dice", "=", "True", ",", "stage", "=", "None", ",", "\n", "unpack_data", "=", "True", ",", "deterministic", "=", "True", ",", "fp16", "=", "False", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "plans_file", ",", "fold", ",", "output_folder", ",", "dataset_directory", ",", "batch_dice", ",", "stage", ",", "unpack_data", ",", "\n", "deterministic", ",", "fp16", ")", "\n", "self", ".", "initial_lr", "=", "3e-4", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.optimizer_and_lr.nnUNetTrainerV2_Adam_ReduceOnPlateau.nnUNetTrainerV2_Adam_ReduceOnPlateau.initialize_optimizer_and_scheduler": [[32, 40], ["torch.optim.Adam", "torch.optim.lr_scheduler.ReduceLROnPlateau", "nnUNetTrainerV2_Adam_ReduceOnPlateau.nnUNetTrainerV2_Adam_ReduceOnPlateau.network.parameters"], "methods", ["None"], ["", "def", "initialize_optimizer_and_scheduler", "(", "self", ")", ":", "\n", "        ", "assert", "self", ".", "network", "is", "not", "None", ",", "\"self.initialize_network must be called first\"", "\n", "self", ".", "optimizer", "=", "torch", ".", "optim", ".", "Adam", "(", "self", ".", "network", ".", "parameters", "(", ")", ",", "self", ".", "initial_lr", ",", "weight_decay", "=", "self", ".", "weight_decay", ",", "\n", "amsgrad", "=", "True", ")", "\n", "self", ".", "lr_scheduler", "=", "lr_scheduler", ".", "ReduceLROnPlateau", "(", "self", ".", "optimizer", ",", "mode", "=", "'min'", ",", "factor", "=", "0.2", ",", "\n", "patience", "=", "self", ".", "lr_scheduler_patience", ",", "\n", "verbose", "=", "True", ",", "threshold", "=", "self", ".", "lr_scheduler_eps", ",", "\n", "threshold_mode", "=", "\"abs\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.optimizer_and_lr.nnUNetTrainerV2_Adam_ReduceOnPlateau.nnUNetTrainerV2_Adam_ReduceOnPlateau.maybe_update_lr": [[41, 53], ["nnUNetTrainerV2_Adam_ReduceOnPlateau.nnUNetTrainerV2_Adam_ReduceOnPlateau.print_to_log_file", "isinstance", "isinstance", "nnUNetTrainerV2_Adam_ReduceOnPlateau.nnUNetTrainerV2_Adam_ReduceOnPlateau.lr_scheduler.step", "str", "nnUNetTrainerV2_Adam_ReduceOnPlateau.nnUNetTrainerV2_Adam_ReduceOnPlateau.lr_scheduler.step"], "methods", ["home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.network_training.nnUNetTrainerV2_DDP.nnUNetTrainerV2_DDP.print_to_log_file", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.optimizer.ranger.Ranger.step", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.optimizer.ranger.Ranger.step"], ["", "def", "maybe_update_lr", "(", "self", ",", "epoch", "=", "None", ")", ":", "\n", "# maybe update learning rate", "\n", "        ", "if", "self", ".", "lr_scheduler", "is", "not", "None", ":", "\n", "            ", "assert", "isinstance", "(", "self", ".", "lr_scheduler", ",", "(", "lr_scheduler", ".", "ReduceLROnPlateau", ",", "lr_scheduler", ".", "_LRScheduler", ")", ")", "\n", "\n", "if", "isinstance", "(", "self", ".", "lr_scheduler", ",", "lr_scheduler", ".", "ReduceLROnPlateau", ")", ":", "\n", "# lr scheduler is updated with moving average val loss. should be more robust", "\n", "                ", "if", "self", ".", "epoch", ">", "0", "and", "self", ".", "train_loss_MA", "is", "not", "None", ":", "# otherwise self.train_loss_MA is None", "\n", "                    ", "self", ".", "lr_scheduler", ".", "step", "(", "self", ".", "train_loss_MA", ")", "\n", "", "", "else", ":", "\n", "                ", "self", ".", "lr_scheduler", ".", "step", "(", "self", ".", "epoch", "+", "1", ")", "\n", "", "", "self", ".", "print_to_log_file", "(", "\"lr is now (scheduler) %s\"", "%", "str", "(", "self", ".", "optimizer", ".", "param_groups", "[", "0", "]", "[", "'lr'", "]", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.optimizer_and_lr.nnUNetTrainerV2_Adam_ReduceOnPlateau.nnUNetTrainerV2_Adam_ReduceOnPlateau.on_epoch_end": [[54, 56], ["nnunet.training.network_training.nnUNetTrainer.nnUNetTrainer.on_epoch_end"], "methods", ["home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.optimizer_and_lr.nnUNetTrainerV2_reduceMomentumDuringTraining.nnUNetTrainerV2_reduceMomentumDuringTraining.on_epoch_end"], ["", "def", "on_epoch_end", "(", "self", ")", ":", "\n", "        ", "return", "nnUNetTrainer", ".", "on_epoch_end", "(", "self", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.optimizer_and_lr.nnUNetTrainerV2_SGD_fixedSchedule.nnUNetTrainerV2_SGD_fixedSchedule.__init__": [[20, 24], ["nnunet.training.network_training.nnUNetTrainerV2.nnUNetTrainerV2.__init__"], "methods", ["home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.common.layers.ResizeImage.__init__"], ["    ", "def", "__init__", "(", "self", ",", "plans_file", ",", "fold", ",", "output_folder", "=", "None", ",", "dataset_directory", "=", "None", ",", "batch_dice", "=", "True", ",", "stage", "=", "None", ",", "\n", "unpack_data", "=", "True", ",", "deterministic", "=", "True", ",", "fp16", "=", "False", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "plans_file", ",", "fold", ",", "output_folder", ",", "dataset_directory", ",", "batch_dice", ",", "stage", ",", "unpack_data", ",", "\n", "deterministic", ",", "fp16", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.optimizer_and_lr.nnUNetTrainerV2_SGD_fixedSchedule.nnUNetTrainerV2_SGD_fixedSchedule.maybe_update_lr": [[25, 44], ["nnUNetTrainerV2_SGD_fixedSchedule.nnUNetTrainerV2_SGD_fixedSchedule.print_to_log_file", "RuntimeError"], "methods", ["home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.network_training.nnUNetTrainerV2_DDP.nnUNetTrainerV2_DDP.print_to_log_file"], ["", "def", "maybe_update_lr", "(", "self", ",", "epoch", "=", "None", ")", ":", "\n", "        ", "if", "epoch", "is", "None", ":", "\n", "            ", "ep", "=", "self", ".", "epoch", "+", "1", "\n", "", "else", ":", "\n", "            ", "ep", "=", "epoch", "\n", "\n", "", "if", "0", "<=", "ep", "<", "500", ":", "\n", "            ", "new_lr", "=", "self", ".", "initial_lr", "\n", "", "elif", "500", "<=", "ep", "<", "675", ":", "\n", "            ", "new_lr", "=", "self", ".", "initial_lr", "*", "0.1", "\n", "", "elif", "675", "<=", "ep", "<", "850", ":", "\n", "            ", "new_lr", "=", "self", ".", "initial_lr", "*", "0.01", "\n", "", "elif", "ep", ">=", "850", ":", "\n", "            ", "new_lr", "=", "self", ".", "initial_lr", "*", "0.001", "\n", "", "else", ":", "\n", "            ", "raise", "RuntimeError", "(", "\"Really unexpected things happened, ep=%d\"", "%", "ep", ")", "\n", "\n", "", "self", ".", "optimizer", ".", "param_groups", "[", "0", "]", "[", "'lr'", "]", "=", "new_lr", "\n", "self", ".", "print_to_log_file", "(", "\"lr:\"", ",", "self", ".", "optimizer", ".", "param_groups", "[", "0", "]", "[", "'lr'", "]", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.optimizer_and_lr.nnUNetTrainerV2_momentum095.nnUNetTrainerV2_momentum095.initialize_optimizer_and_scheduler": [[22, 27], ["torch.optim.SGD", "nnUNetTrainerV2_momentum095.nnUNetTrainerV2_momentum095.network.parameters"], "methods", ["None"], ["    ", "def", "initialize_optimizer_and_scheduler", "(", "self", ")", ":", "\n", "        ", "assert", "self", ".", "network", "is", "not", "None", ",", "\"self.initialize_network must be called first\"", "\n", "self", ".", "optimizer", "=", "torch", ".", "optim", ".", "SGD", "(", "self", ".", "network", ".", "parameters", "(", ")", ",", "self", ".", "initial_lr", ",", "weight_decay", "=", "self", ".", "weight_decay", ",", "\n", "momentum", "=", "0.95", ",", "nesterov", "=", "True", ")", "\n", "self", ".", "lr_scheduler", "=", "None", "\n", "", "", ""]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.optimizer_and_lr.nnUNetTrainerV2_SGD_lrs.nnUNetTrainerV2_SGD_lr1en1.__init__": [[20, 25], ["nnunet.training.network_training.nnUNetTrainerV2.nnUNetTrainerV2.__init__"], "methods", ["home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.common.layers.ResizeImage.__init__"], ["    ", "def", "__init__", "(", "self", ",", "plans_file", ",", "fold", ",", "output_folder", "=", "None", ",", "dataset_directory", "=", "None", ",", "batch_dice", "=", "True", ",", "stage", "=", "None", ",", "\n", "unpack_data", "=", "True", ",", "deterministic", "=", "True", ",", "fp16", "=", "False", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "plans_file", ",", "fold", ",", "output_folder", ",", "dataset_directory", ",", "batch_dice", ",", "stage", ",", "unpack_data", ",", "\n", "deterministic", ",", "fp16", ")", "\n", "self", ".", "initial_lr", "=", "1e-1", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.optimizer_and_lr.nnUNetTrainerV2_SGD_lrs.nnUNetTrainerV2_SGD_lr1en3.__init__": [[28, 33], ["nnunet.training.network_training.nnUNetTrainerV2.nnUNetTrainerV2.__init__"], "methods", ["home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.common.layers.ResizeImage.__init__"], ["    ", "def", "__init__", "(", "self", ",", "plans_file", ",", "fold", ",", "output_folder", "=", "None", ",", "dataset_directory", "=", "None", ",", "batch_dice", "=", "True", ",", "stage", "=", "None", ",", "\n", "unpack_data", "=", "True", ",", "deterministic", "=", "True", ",", "fp16", "=", "False", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "plans_file", ",", "fold", ",", "output_folder", ",", "dataset_directory", ",", "batch_dice", ",", "stage", ",", "unpack_data", ",", "\n", "deterministic", ",", "fp16", ")", "\n", "self", ".", "initial_lr", "=", "1e-3", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.optimizer_and_lr.nnUNetTrainerV2_reduceMomentumDuringTraining.nnUNetTrainerV2_reduceMomentumDuringTraining.initialize_optimizer_and_scheduler": [[27, 43], ["nnUNetTrainerV2_reduceMomentumDuringTraining.nnUNetTrainerV2_reduceMomentumDuringTraining.print_to_log_file", "torch.optim.SGD", "nnUNetTrainerV2_reduceMomentumDuringTraining.nnUNetTrainerV2_reduceMomentumDuringTraining.network.parameters"], "methods", ["home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.network_training.nnUNetTrainerV2_DDP.nnUNetTrainerV2_DDP.print_to_log_file"], ["def", "initialize_optimizer_and_scheduler", "(", "self", ")", ":", "\n", "        ", "current_momentum", "=", "0.99", "\n", "min_momentum", "=", "0.9", "\n", "\n", "if", "self", ".", "epoch", ">", "800", ":", "\n", "            ", "current_momentum", "=", "current_momentum", "-", "(", "current_momentum", "-", "min_momentum", ")", "/", "200", "*", "(", "self", ".", "epoch", "-", "800", ")", "\n", "\n", "", "self", ".", "print_to_log_file", "(", "\"current momentum\"", ",", "current_momentum", ")", "\n", "assert", "self", ".", "network", "is", "not", "None", ",", "\"self.initialize_network must be called first\"", "\n", "if", "self", ".", "optimizer", "is", "None", ":", "\n", "            ", "self", ".", "optimizer", "=", "torch", ".", "optim", ".", "SGD", "(", "self", ".", "network", ".", "parameters", "(", ")", ",", "self", ".", "initial_lr", ",", "weight_decay", "=", "self", ".", "weight_decay", ",", "\n", "momentum", "=", "0.99", ",", "nesterov", "=", "True", ")", "\n", "", "else", ":", "\n", "# can't reinstantiate because that would break NVIDIA AMP", "\n", "            ", "self", ".", "optimizer", ".", "param_groups", "[", "0", "]", "[", "\"momentum\"", "]", "=", "current_momentum", "\n", "", "self", ".", "lr_scheduler", "=", "None", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.optimizer_and_lr.nnUNetTrainerV2_reduceMomentumDuringTraining.nnUNetTrainerV2_reduceMomentumDuringTraining.on_epoch_end": [[44, 47], ["nnUNetTrainerV2_reduceMomentumDuringTraining.nnUNetTrainerV2_reduceMomentumDuringTraining.initialize_optimizer_and_scheduler", "super().on_epoch_end"], "methods", ["home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.optimizer_and_lr.nnUNetTrainerV2_momentum09in2D.nnUNetTrainerV2_momentum09in2D.initialize_optimizer_and_scheduler", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.optimizer_and_lr.nnUNetTrainerV2_reduceMomentumDuringTraining.nnUNetTrainerV2_reduceMomentumDuringTraining.on_epoch_end"], ["", "def", "on_epoch_end", "(", "self", ")", ":", "\n", "        ", "self", ".", "initialize_optimizer_and_scheduler", "(", ")", "\n", "return", "super", "(", ")", ".", "on_epoch_end", "(", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.optimizer_and_lr.nnUNetTrainerV2_Adam_lr_3en4.nnUNetTrainerV2_Adam_nnUNetTrainerlr.__init__": [[20, 25], ["nnunet.training.network_training.nnUNet_variants.optimizer_and_lr.nnUNetTrainerV2_Adam.nnUNetTrainerV2_Adam.__init__"], "methods", ["home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.common.layers.ResizeImage.__init__"], ["    ", "def", "__init__", "(", "self", ",", "plans_file", ",", "fold", ",", "output_folder", "=", "None", ",", "dataset_directory", "=", "None", ",", "batch_dice", "=", "True", ",", "stage", "=", "None", ",", "\n", "unpack_data", "=", "True", ",", "deterministic", "=", "True", ",", "fp16", "=", "False", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "plans_file", ",", "fold", ",", "output_folder", ",", "dataset_directory", ",", "batch_dice", ",", "stage", ",", "unpack_data", ",", "\n", "deterministic", ",", "fp16", ")", "\n", "self", ".", "initial_lr", "=", "3e-4", "\n", "", "", ""]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.optimizer_and_lr.nnUNetTrainerV2_cycleAtEnd.nnUNetTrainerV2_cycleAtEnd.__init__": [[45, 50], ["nnunet.training.network_training.nnUNetTrainerV2.nnUNetTrainerV2.__init__"], "methods", ["home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.common.layers.ResizeImage.__init__"], ["def", "__init__", "(", "self", ",", "plans_file", ",", "fold", ",", "output_folder", "=", "None", ",", "dataset_directory", "=", "None", ",", "batch_dice", "=", "True", ",", "stage", "=", "None", ",", "\n", "unpack_data", "=", "True", ",", "deterministic", "=", "True", ",", "fp16", "=", "False", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "plans_file", ",", "fold", ",", "output_folder", ",", "dataset_directory", ",", "batch_dice", ",", "stage", ",", "unpack_data", ",", "\n", "deterministic", ",", "fp16", ")", "\n", "self", ".", "max_num_epochs", "=", "1100", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.optimizer_and_lr.nnUNetTrainerV2_cycleAtEnd.nnUNetTrainerV2_cycleAtEnd.maybe_update_lr": [[51, 64], ["nnunet.training.learning_rate.poly_lr.poly_lr", "nnUNetTrainerV2_cycleAtEnd.nnUNetTrainerV2_cycleAtEnd.print_to_log_file", "nnUNetTrainerV2_cycleAtEnd.cycle_lr", "nnUNetTrainerV2_cycleAtEnd.nnUNetTrainerV2_cycleAtEnd.print_to_log_file", "nnunet.training.learning_rate.poly_lr.poly_lr"], "methods", ["home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.learning_rate.poly_lr.poly_lr", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.network_training.nnUNetTrainerV2_DDP.nnUNetTrainerV2_DDP.print_to_log_file", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.optimizer_and_lr.nnUNetTrainerV2_cycleAtEnd.cycle_lr", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.network_training.nnUNetTrainerV2_DDP.nnUNetTrainerV2_DDP.print_to_log_file", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.learning_rate.poly_lr.poly_lr"], ["", "def", "maybe_update_lr", "(", "self", ",", "epoch", "=", "None", ")", ":", "\n", "        ", "if", "epoch", "is", "None", ":", "\n", "            ", "ep", "=", "self", ".", "epoch", "+", "1", "\n", "", "else", ":", "\n", "            ", "ep", "=", "epoch", "\n", "\n", "", "if", "ep", "<", "1000", ":", "\n", "            ", "self", ".", "optimizer", ".", "param_groups", "[", "0", "]", "[", "'lr'", "]", "=", "poly_lr", "(", "ep", ",", "1000", ",", "self", ".", "initial_lr", ",", "0.9", ")", "\n", "self", ".", "print_to_log_file", "(", "\"lr:\"", ",", "poly_lr", "(", "ep", ",", "1000", ",", "self", ".", "initial_lr", ",", "0.9", ")", ")", "\n", "", "else", ":", "\n", "            ", "new_lr", "=", "cycle_lr", "(", "ep", ",", "100", ",", "min_lr", "=", "1e-6", ",", "max_lr", "=", "1e-3", ")", "# we don't go all the way back up to initial lr", "\n", "self", ".", "optimizer", ".", "param_groups", "[", "0", "]", "[", "'lr'", "]", "=", "new_lr", "\n", "self", ".", "print_to_log_file", "(", "\"lr:\"", ",", "new_lr", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.optimizer_and_lr.nnUNetTrainerV2_cycleAtEnd.nnUNetTrainerV2_cycleAtEnd2.__init__": [[71, 76], ["nnunet.training.network_training.nnUNetTrainerV2.nnUNetTrainerV2.__init__"], "methods", ["home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.common.layers.ResizeImage.__init__"], ["def", "__init__", "(", "self", ",", "plans_file", ",", "fold", ",", "output_folder", "=", "None", ",", "dataset_directory", "=", "None", ",", "batch_dice", "=", "True", ",", "stage", "=", "None", ",", "\n", "unpack_data", "=", "True", ",", "deterministic", "=", "True", ",", "fp16", "=", "False", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "plans_file", ",", "fold", ",", "output_folder", ",", "dataset_directory", ",", "batch_dice", ",", "stage", ",", "unpack_data", ",", "\n", "deterministic", ",", "fp16", ")", "\n", "self", ".", "max_num_epochs", "=", "1200", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.optimizer_and_lr.nnUNetTrainerV2_cycleAtEnd.nnUNetTrainerV2_cycleAtEnd2.maybe_update_lr": [[77, 90], ["nnunet.training.learning_rate.poly_lr.poly_lr", "nnUNetTrainerV2_cycleAtEnd.nnUNetTrainerV2_cycleAtEnd2.print_to_log_file", "nnUNetTrainerV2_cycleAtEnd.cycle_lr", "nnUNetTrainerV2_cycleAtEnd.nnUNetTrainerV2_cycleAtEnd2.print_to_log_file", "nnunet.training.learning_rate.poly_lr.poly_lr"], "methods", ["home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.learning_rate.poly_lr.poly_lr", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.network_training.nnUNetTrainerV2_DDP.nnUNetTrainerV2_DDP.print_to_log_file", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.optimizer_and_lr.nnUNetTrainerV2_cycleAtEnd.cycle_lr", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.network_training.nnUNetTrainerV2_DDP.nnUNetTrainerV2_DDP.print_to_log_file", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.learning_rate.poly_lr.poly_lr"], ["", "def", "maybe_update_lr", "(", "self", ",", "epoch", "=", "None", ")", ":", "\n", "        ", "if", "epoch", "is", "None", ":", "\n", "            ", "ep", "=", "self", ".", "epoch", "+", "1", "\n", "", "else", ":", "\n", "            ", "ep", "=", "epoch", "\n", "\n", "", "if", "ep", "<", "1000", ":", "\n", "            ", "self", ".", "optimizer", ".", "param_groups", "[", "0", "]", "[", "'lr'", "]", "=", "poly_lr", "(", "ep", ",", "1000", ",", "self", ".", "initial_lr", ",", "0.9", ")", "\n", "self", ".", "print_to_log_file", "(", "\"lr:\"", ",", "poly_lr", "(", "ep", ",", "1000", ",", "self", ".", "initial_lr", ",", "0.9", ")", ")", "\n", "", "else", ":", "\n", "            ", "new_lr", "=", "cycle_lr", "(", "ep", ",", "200", ",", "min_lr", "=", "1e-6", ",", "max_lr", "=", "1e-2", ")", "# we don't go all the way back up to initial lr", "\n", "self", ".", "optimizer", ".", "param_groups", "[", "0", "]", "[", "'lr'", "]", "=", "new_lr", "\n", "self", ".", "print_to_log_file", "(", "\"lr:\"", ",", "new_lr", ")", "\n", "", "", "", ""]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.optimizer_and_lr.nnUNetTrainerV2_cycleAtEnd.cycle_lr": [[21, 29], ["None"], "function", ["None"], ["def", "cycle_lr", "(", "current_epoch", ",", "cycle_length", "=", "100", ",", "min_lr", "=", "1e-6", ",", "max_lr", "=", "1e-3", ")", ":", "\n", "    ", "num_rising", "=", "cycle_length", "//", "2", "\n", "epoch", "=", "current_epoch", "%", "cycle_length", "\n", "if", "epoch", "<", "num_rising", ":", "\n", "        ", "lr", "=", "min_lr", "+", "(", "max_lr", "-", "min_lr", ")", "/", "num_rising", "*", "epoch", "\n", "", "else", ":", "\n", "        ", "lr", "=", "max_lr", "-", "(", "max_lr", "-", "min_lr", ")", "/", "num_rising", "*", "(", "epoch", "-", "num_rising", ")", "\n", "", "return", "lr", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.optimizer_and_lr.nnUNetTrainerV2_cycleAtEnd.plot_cycle_lr": [[31, 38], ["list", "matplotlib.plot", "matplotlib.show", "matplotlib.savefig", "matplotlib.close", "range", "nnUNetTrainerV2_cycleAtEnd.cycle_lr"], "function", ["home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.optimizer_and_lr.nnUNetTrainerV2_cycleAtEnd.cycle_lr"], ["", "def", "plot_cycle_lr", "(", ")", ":", "\n", "    ", "xvals", "=", "list", "(", "range", "(", "1000", ")", ")", "\n", "yvals", "=", "[", "cycle_lr", "(", "i", ",", "100", ",", "1e-6", ",", "1e-3", ")", "for", "i", "in", "xvals", "]", "\n", "plt", ".", "plot", "(", "xvals", ",", "yvals", ")", "\n", "plt", ".", "show", "(", ")", "\n", "plt", ".", "savefig", "(", "\"/home/fabian/temp.png\"", ")", "\n", "plt", ".", "close", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.optimizer_and_lr.nnUNetTrainerV2_Adam.nnUNetTrainerV2_Adam.initialize_optimizer_and_scheduler": [[22, 25], ["torch.optim.Adam", "nnUNetTrainerV2_Adam.nnUNetTrainerV2_Adam.network.parameters"], "methods", ["None"], ["    ", "def", "initialize_optimizer_and_scheduler", "(", "self", ")", ":", "\n", "        ", "self", ".", "optimizer", "=", "torch", ".", "optim", ".", "Adam", "(", "self", ".", "network", ".", "parameters", "(", ")", ",", "self", ".", "initial_lr", ",", "weight_decay", "=", "self", ".", "weight_decay", ",", "amsgrad", "=", "True", ")", "\n", "self", ".", "lr_scheduler", "=", "None", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.optimizer_and_lr.nnUNetTrainerV2_Ranger_lr1en2.nnUNetTrainerV2_Ranger_lr1en2.__init__": [[21, 26], ["nnunet.training.network_training.nnUNetTrainerV2.nnUNetTrainerV2.__init__"], "methods", ["home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.common.layers.ResizeImage.__init__"], ["    ", "def", "__init__", "(", "self", ",", "plans_file", ",", "fold", ",", "output_folder", "=", "None", ",", "dataset_directory", "=", "None", ",", "batch_dice", "=", "True", ",", "stage", "=", "None", ",", "\n", "unpack_data", "=", "True", ",", "deterministic", "=", "True", ",", "fp16", "=", "False", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "plans_file", ",", "fold", ",", "output_folder", ",", "dataset_directory", ",", "batch_dice", ",", "stage", ",", "unpack_data", ",", "\n", "deterministic", ",", "fp16", ")", "\n", "self", ".", "initial_lr", "=", "1e-2", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.optimizer_and_lr.nnUNetTrainerV2_Ranger_lr1en2.nnUNetTrainerV2_Ranger_lr1en2.initialize_optimizer_and_scheduler": [[27, 31], ["nnunet.training.optimizer.ranger.Ranger", "nnUNetTrainerV2_Ranger_lr1en2.nnUNetTrainerV2_Ranger_lr1en2.network.parameters"], "methods", ["None"], ["", "def", "initialize_optimizer_and_scheduler", "(", "self", ")", ":", "\n", "        ", "self", ".", "optimizer", "=", "Ranger", "(", "self", ".", "network", ".", "parameters", "(", ")", ",", "self", ".", "initial_lr", ",", "k", "=", "6", ",", "N_sma_threshhold", "=", "5", ",", "\n", "weight_decay", "=", "self", ".", "weight_decay", ")", "\n", "self", ".", "lr_scheduler", "=", "None", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.optimizer_and_lr.nnUNetTrainerV2_warmup.nnUNetTrainerV2_warmup.__init__": [[20, 25], ["nnunet.training.network_training.nnUNetTrainerV2.nnUNetTrainerV2.__init__"], "methods", ["home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.common.layers.ResizeImage.__init__"], ["    ", "def", "__init__", "(", "self", ",", "plans_file", ",", "fold", ",", "output_folder", "=", "None", ",", "dataset_directory", "=", "None", ",", "batch_dice", "=", "True", ",", "stage", "=", "None", ",", "\n", "unpack_data", "=", "True", ",", "deterministic", "=", "True", ",", "fp16", "=", "False", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "plans_file", ",", "fold", ",", "output_folder", ",", "dataset_directory", ",", "batch_dice", ",", "stage", ",", "unpack_data", ",", "\n", "deterministic", ",", "fp16", ")", "\n", "self", ".", "max_num_epochs", "=", "1050", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.optimizer_and_lr.nnUNetTrainerV2_warmup.nnUNetTrainerV2_warmup.maybe_update_lr": [[26, 40], ["nnUNetTrainerV2_warmup.nnUNetTrainerV2_warmup.print_to_log_file", "super().maybe_update_lr"], "methods", ["home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.network_training.nnUNetTrainerV2_DDP.nnUNetTrainerV2_DDP.print_to_log_file", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.optimizer_and_lr.nnUNetTrainerV2_warmup.nnUNetTrainerV2_warmup.maybe_update_lr"], ["", "def", "maybe_update_lr", "(", "self", ",", "epoch", "=", "None", ")", ":", "\n", "        ", "if", "self", ".", "epoch", "<", "50", ":", "\n", "# epoch 49 is max", "\n", "# we increase lr linearly from 0 to initial_lr", "\n", "            ", "lr", "=", "(", "self", ".", "epoch", "+", "1", ")", "/", "50", "*", "self", ".", "initial_lr", "\n", "self", ".", "optimizer", ".", "param_groups", "[", "0", "]", "[", "'lr'", "]", "=", "lr", "\n", "self", ".", "print_to_log_file", "(", "\"epoch:\"", ",", "self", ".", "epoch", ",", "\"lr:\"", ",", "lr", ")", "\n", "", "else", ":", "\n", "            ", "if", "epoch", "is", "not", "None", ":", "\n", "                ", "ep", "=", "epoch", "-", "49", "\n", "", "else", ":", "\n", "                ", "ep", "=", "self", ".", "epoch", "-", "49", "\n", "", "assert", "ep", ">", "0", ",", "\"epoch must be >0\"", "\n", "return", "super", "(", ")", ".", "maybe_update_lr", "(", "ep", ")", "\n", "", "", "", ""]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.optimizer_and_lr.nnUNetTrainerV2_momentum09.nnUNetTrainerV2_momentum09.initialize_optimizer_and_scheduler": [[22, 27], ["torch.optim.SGD", "nnUNetTrainerV2_momentum09.nnUNetTrainerV2_momentum09.network.parameters"], "methods", ["None"], ["    ", "def", "initialize_optimizer_and_scheduler", "(", "self", ")", ":", "\n", "        ", "assert", "self", ".", "network", "is", "not", "None", ",", "\"self.initialize_network must be called first\"", "\n", "self", ".", "optimizer", "=", "torch", ".", "optim", ".", "SGD", "(", "self", ".", "network", ".", "parameters", "(", ")", ",", "self", ".", "initial_lr", ",", "weight_decay", "=", "self", ".", "weight_decay", ",", "\n", "momentum", "=", "0.9", ",", "nesterov", "=", "True", ")", "\n", "self", ".", "lr_scheduler", "=", "None", "\n", "", "", ""]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.optimizer_and_lr.nnUNetTrainerV2_fp16.nnUNetTrainerV2_fp16.__init__": [[20, 25], ["nnunet.training.network_training.nnUNetTrainerV2.nnUNetTrainerV2.__init__"], "methods", ["home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.common.layers.ResizeImage.__init__"], ["    ", "def", "__init__", "(", "self", ",", "plans_file", ",", "fold", ",", "output_folder", "=", "None", ",", "dataset_directory", "=", "None", ",", "batch_dice", "=", "True", ",", "stage", "=", "None", ",", "\n", "unpack_data", "=", "True", ",", "deterministic", "=", "True", ",", "fp16", "=", "False", ")", ":", "\n", "        ", "assert", "fp16", ",", "\"This one only accepts fp16=True\"", "\n", "super", "(", ")", ".", "__init__", "(", "plans_file", ",", "fold", ",", "output_folder", ",", "dataset_directory", ",", "batch_dice", ",", "stage", ",", "unpack_data", ",", "\n", "deterministic", ",", "fp16", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.optimizer_and_lr.nnUNetTrainerV2_momentum09in2D.nnUNetTrainerV2_momentum09in2D.initialize_optimizer_and_scheduler": [[21, 30], ["torch.optim.SGD", "nnUNetTrainerV2_momentum09in2D.nnUNetTrainerV2_momentum09in2D.network.parameters"], "methods", ["None"], ["    ", "def", "initialize_optimizer_and_scheduler", "(", "self", ")", ":", "\n", "        ", "if", "self", ".", "threeD", ":", "\n", "            ", "momentum", "=", "0.99", "\n", "", "else", ":", "\n", "            ", "momentum", "=", "0.9", "\n", "", "assert", "self", ".", "network", "is", "not", "None", ",", "\"self.initialize_network must be called first\"", "\n", "self", ".", "optimizer", "=", "torch", ".", "optim", ".", "SGD", "(", "self", ".", "network", ".", "parameters", "(", ")", ",", "self", ".", "initial_lr", ",", "weight_decay", "=", "self", ".", "weight_decay", ",", "\n", "momentum", "=", "momentum", ",", "nesterov", "=", "True", ")", "\n", "self", ".", "lr_scheduler", "=", "None", "\n", "", "", ""]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.BraTS_trainer.nnUNetTrainerV2BraTSRegions.nnUNetTrainerV2BraTSRegions_BN.initialize_network": [[43, 67], ["nnunet.network_architecture.generic_UNet.Generic_UNet", "torch.cuda.is_available", "torch.nn.Softmax", "len", "nnunet.network_architecture.initialization.InitWeights_He", "nnUNetTrainerV2BraTSRegions.nnUNetTrainerV2BraTSRegions_BN.network.cuda"], "methods", ["None"], ["    ", "def", "initialize_network", "(", "self", ")", ":", "\n", "        ", "if", "self", ".", "threeD", ":", "\n", "            ", "conv_op", "=", "nn", ".", "Conv3d", "\n", "dropout_op", "=", "nn", ".", "Dropout3d", "\n", "norm_op", "=", "nn", ".", "BatchNorm3d", "\n", "\n", "", "else", ":", "\n", "            ", "conv_op", "=", "nn", ".", "Conv2d", "\n", "dropout_op", "=", "nn", ".", "Dropout2d", "\n", "norm_op", "=", "nn", ".", "BatchNorm2d", "\n", "\n", "", "norm_op_kwargs", "=", "{", "'eps'", ":", "1e-5", ",", "'affine'", ":", "True", "}", "\n", "dropout_op_kwargs", "=", "{", "'p'", ":", "0", ",", "'inplace'", ":", "True", "}", "\n", "net_nonlin", "=", "nn", ".", "LeakyReLU", "\n", "net_nonlin_kwargs", "=", "{", "'negative_slope'", ":", "1e-2", ",", "'inplace'", ":", "True", "}", "\n", "self", ".", "network", "=", "Generic_UNet", "(", "self", ".", "num_input_channels", ",", "self", ".", "base_num_features", ",", "self", ".", "num_classes", ",", "\n", "len", "(", "self", ".", "net_num_pool_op_kernel_sizes", ")", ",", "\n", "self", ".", "conv_per_stage", ",", "2", ",", "conv_op", ",", "norm_op", ",", "norm_op_kwargs", ",", "dropout_op", ",", "\n", "dropout_op_kwargs", ",", "\n", "net_nonlin", ",", "net_nonlin_kwargs", ",", "True", ",", "False", ",", "lambda", "x", ":", "x", ",", "InitWeights_He", "(", "1e-2", ")", ",", "\n", "self", ".", "net_num_pool_op_kernel_sizes", ",", "self", ".", "net_conv_kernel_sizes", ",", "False", ",", "True", ",", "True", ")", "\n", "if", "torch", ".", "cuda", ".", "is_available", "(", ")", ":", "\n", "            ", "self", ".", "network", ".", "cuda", "(", ")", "\n", "", "self", ".", "network", ".", "inference_apply_nonlin", "=", "torch", ".", "nn", ".", "Softmax", "(", "1", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.BraTS_trainer.nnUNetTrainerV2BraTSRegions.nnUNetTrainerV2BraTSRegions.__init__": [[70, 77], ["nnunet.training.network_training.nnUNetTrainerV2.nnUNetTrainerV2.__init__", "nnunet.evaluation.region_based_evaluation.get_brats_regions", "nnunet.training.loss_functions.dice_loss.DC_and_BCE_loss"], "methods", ["home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.common.layers.ResizeImage.__init__", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.evaluation.region_based_evaluation.get_brats_regions"], ["    ", "def", "__init__", "(", "self", ",", "plans_file", ",", "fold", ",", "output_folder", "=", "None", ",", "dataset_directory", "=", "None", ",", "batch_dice", "=", "True", ",", "stage", "=", "None", ",", "\n", "unpack_data", "=", "True", ",", "deterministic", "=", "True", ",", "fp16", "=", "False", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "plans_file", ",", "fold", ",", "output_folder", ",", "dataset_directory", ",", "batch_dice", ",", "stage", ",", "unpack_data", ",", "\n", "deterministic", ",", "fp16", ")", "\n", "self", ".", "regions", "=", "get_brats_regions", "(", ")", "\n", "self", ".", "regions_class_order", "=", "(", "1", ",", "2", ",", "3", ")", "\n", "self", ".", "loss", "=", "DC_and_BCE_loss", "(", "{", "}", ",", "{", "'batch_dice'", ":", "False", ",", "'do_bg'", ":", "True", ",", "'smooth'", ":", "0", "}", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.BraTS_trainer.nnUNetTrainerV2BraTSRegions.nnUNetTrainerV2BraTSRegions.process_plans": [[78, 84], ["super().process_plans", "len"], "methods", ["home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.BraTS_trainer.nnUNetTrainerV2BraTSRegions.nnUNetTrainerV2BraTSRegions_DA3_BN.process_plans"], ["", "def", "process_plans", "(", "self", ",", "plans", ")", ":", "\n", "        ", "super", "(", ")", ".", "process_plans", "(", "plans", ")", "\n", "\"\"\"\n        The network has as many outputs as we have regions\n        \"\"\"", "\n", "self", ".", "num_classes", "=", "len", "(", "self", ".", "regions", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.BraTS_trainer.nnUNetTrainerV2BraTSRegions.nnUNetTrainerV2BraTSRegions.initialize_network": [[85, 89], ["super().initialize_network", "torch.nn.Sigmoid"], "methods", ["home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.BraTS_trainer.nnUNetTrainerV2BraTSRegions.nnUNetTrainerV2BraTSRegions_DA3.initialize_network"], ["", "def", "initialize_network", "(", "self", ")", ":", "\n", "        ", "\"\"\"inference_apply_nonlin to sigmoid\"\"\"", "\n", "super", "(", ")", ".", "initialize_network", "(", ")", "\n", "self", ".", "network", ".", "inference_apply_nonlin", "=", "nn", ".", "Sigmoid", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.BraTS_trainer.nnUNetTrainerV2BraTSRegions.nnUNetTrainerV2BraTSRegions.initialize": [[90, 157], ["maybe_mkdir_p", "nnUNetTrainerV2BraTSRegions.nnUNetTrainerV2BraTSRegions.process_plans", "nnUNetTrainerV2BraTSRegions.nnUNetTrainerV2BraTSRegions.setup_DA_params", "len", "numpy.array", "numpy.array", "nnunet.training.loss_functions.deep_supervision.MultipleOutputLoss2", "join", "nnUNetTrainerV2BraTSRegions.nnUNetTrainerV2BraTSRegions.initialize_network", "nnUNetTrainerV2BraTSRegions.nnUNetTrainerV2BraTSRegions.initialize_optimizer_and_scheduler", "isinstance", "nnUNetTrainerV2BraTSRegions.nnUNetTrainerV2BraTSRegions.print_to_log_file", "nnUNetTrainerV2BraTSRegions.nnUNetTrainerV2BraTSRegions.load_plans_file", "numpy.array.sum", "nnUNetTrainerV2BraTSRegions.nnUNetTrainerV2BraTSRegions.get_basic_generators", "nnunet.training.data_augmentation.default_data_augmentation.get_moreDA_augmentation", "nnUNetTrainerV2BraTSRegions.nnUNetTrainerV2BraTSRegions.print_to_log_file", "nnUNetTrainerV2BraTSRegions.nnUNetTrainerV2BraTSRegions.print_to_log_file", "print", "nnunet.training.dataloading.dataset_loading.unpack_dataset", "print", "print", "range", "range", "str", "str", "nnUNetTrainerV2BraTSRegions.nnUNetTrainerV2BraTSRegions.dataset_tr.keys", "nnUNetTrainerV2BraTSRegions.nnUNetTrainerV2BraTSRegions.dataset_val.keys"], "methods", ["home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.BraTS_trainer.nnUNetTrainerV2BraTSRegions.nnUNetTrainerV2BraTSRegions_DA3_BN.process_plans", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.BraTS_trainer.nnUNetTrainerV2BraTSRegions.nnUNetTrainerV2BraTSRegions_DA4_BN.setup_DA_params", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.BraTS_trainer.nnUNetTrainerV2BraTSRegions.nnUNetTrainerV2BraTSRegions_DA3.initialize_network", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.optimizer_and_lr.nnUNetTrainerV2_momentum09in2D.nnUNetTrainerV2_momentum09in2D.initialize_optimizer_and_scheduler", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.network_training.nnUNetTrainerV2_DDP.nnUNetTrainerV2_DDP.print_to_log_file", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.network_training.nnUNetTrainer.nnUNetTrainer.load_plans_file", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.nnUNet_variants.nnUNetTrainerNoDA.nnUNetTrainerNoDA.get_basic_generators", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.data_augmentation.default_data_augmentation.get_moreDA_augmentation", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.network_training.nnUNetTrainerV2_DDP.nnUNetTrainerV2_DDP.print_to_log_file", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.network_training.nnUNetTrainerV2_DDP.nnUNetTrainerV2_DDP.print_to_log_file", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.dataloading.dataset_loading.unpack_dataset"], ["", "def", "initialize", "(", "self", ",", "training", "=", "True", ",", "force_load_plans", "=", "False", ")", ":", "\n", "        ", "\"\"\"\n        this is a copy of nnUNetTrainerV2's initialize. We only add the regions to the data augmentation\n        :param training:\n        :param force_load_plans:\n        :return:\n        \"\"\"", "\n", "if", "not", "self", ".", "was_initialized", ":", "\n", "            ", "maybe_mkdir_p", "(", "self", ".", "output_folder", ")", "\n", "\n", "if", "force_load_plans", "or", "(", "self", ".", "plans", "is", "None", ")", ":", "\n", "                ", "self", ".", "load_plans_file", "(", ")", "\n", "\n", "", "self", ".", "process_plans", "(", "self", ".", "plans", ")", "\n", "\n", "self", ".", "setup_DA_params", "(", ")", "\n", "\n", "################# Here we wrap the loss for deep supervision ############", "\n", "# we need to know the number of outputs of the network", "\n", "net_numpool", "=", "len", "(", "self", ".", "net_num_pool_op_kernel_sizes", ")", "\n", "\n", "# we give each output a weight which decreases exponentially (division by 2) as the resolution decreases", "\n", "# this gives higher resolution outputs more weight in the loss", "\n", "weights", "=", "np", ".", "array", "(", "[", "1", "/", "(", "2", "**", "i", ")", "for", "i", "in", "range", "(", "net_numpool", ")", "]", ")", "\n", "\n", "# we don't use the lowest 2 outputs. Normalize weights so that they sum to 1", "\n", "mask", "=", "np", ".", "array", "(", "[", "True", "if", "i", "<", "net_numpool", "-", "1", "else", "False", "for", "i", "in", "range", "(", "net_numpool", ")", "]", ")", "\n", "weights", "[", "~", "mask", "]", "=", "0", "\n", "weights", "=", "weights", "/", "weights", ".", "sum", "(", ")", "\n", "self", ".", "ds_loss_weights", "=", "weights", "\n", "# now wrap the loss", "\n", "self", ".", "loss", "=", "MultipleOutputLoss2", "(", "self", ".", "loss", ",", "self", ".", "ds_loss_weights", ")", "\n", "################# END ###################", "\n", "\n", "self", ".", "folder_with_preprocessed_data", "=", "join", "(", "self", ".", "dataset_directory", ",", "self", ".", "plans", "[", "'data_identifier'", "]", "+", "\n", "\"_stage%d\"", "%", "self", ".", "stage", ")", "\n", "if", "training", ":", "\n", "                ", "self", ".", "dl_tr", ",", "self", ".", "dl_val", "=", "self", ".", "get_basic_generators", "(", ")", "\n", "if", "self", ".", "unpack_data", ":", "\n", "                    ", "print", "(", "\"unpacking dataset\"", ")", "\n", "unpack_dataset", "(", "self", ".", "folder_with_preprocessed_data", ")", "\n", "print", "(", "\"done\"", ")", "\n", "", "else", ":", "\n", "                    ", "print", "(", "\n", "\"INFO: Not unpacking data! Training may be slow due to that. Pray you are not using 2d or you \"", "\n", "\"will wait all winter for your model to finish!\"", ")", "\n", "\n", "", "self", ".", "tr_gen", ",", "self", ".", "val_gen", "=", "get_moreDA_augmentation", "(", "self", ".", "dl_tr", ",", "self", ".", "dl_val", ",", "\n", "self", ".", "data_aug_params", "[", "\n", "'patch_size_for_spatialtransform'", "]", ",", "\n", "self", ".", "data_aug_params", ",", "\n", "deep_supervision_scales", "=", "self", ".", "deep_supervision_scales", ",", "\n", "regions", "=", "self", ".", "regions", ")", "\n", "self", ".", "print_to_log_file", "(", "\"TRAINING KEYS:\\n %s\"", "%", "(", "str", "(", "self", ".", "dataset_tr", ".", "keys", "(", ")", ")", ")", ",", "\n", "also_print_to_console", "=", "False", ")", "\n", "self", ".", "print_to_log_file", "(", "\"VALIDATION KEYS:\\n %s\"", "%", "(", "str", "(", "self", ".", "dataset_val", ".", "keys", "(", ")", ")", ")", ",", "\n", "also_print_to_console", "=", "False", ")", "\n", "", "else", ":", "\n", "                ", "pass", "\n", "\n", "", "self", ".", "initialize_network", "(", ")", "\n", "self", ".", "initialize_optimizer_and_scheduler", "(", ")", "\n", "\n", "assert", "isinstance", "(", "self", ".", "network", ",", "(", "SegmentationNetwork", ",", "nn", ".", "DataParallel", ")", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "print_to_log_file", "(", "'self.was_initialized is True, not running self.initialize again'", ")", "\n", "", "self", ".", "was_initialized", "=", "True", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.BraTS_trainer.nnUNetTrainerV2BraTSRegions.nnUNetTrainerV2BraTSRegions.validate": [[158, 167], ["super().validate", "join", "nnunet.evaluation.region_based_evaluation.evaluate_regions"], "methods", ["home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.BraTS_trainer.nnUNetTrainerV2BraTSRegions.nnUNetTrainerV2BraTSRegions_DA3_BN.validate", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.evaluation.region_based_evaluation.evaluate_regions"], ["", "def", "validate", "(", "self", ",", "do_mirroring", ":", "bool", "=", "True", ",", "use_sliding_window", ":", "bool", "=", "True", ",", "\n", "step_size", ":", "int", "=", "0.5", ",", "save_softmax", ":", "bool", "=", "True", ",", "use_gaussian", ":", "bool", "=", "True", ",", "overwrite", ":", "bool", "=", "True", ",", "\n", "validation_folder_name", ":", "str", "=", "'validation_raw'", ",", "debug", ":", "bool", "=", "False", ",", "all_in_gpu", ":", "bool", "=", "False", ",", "\n", "segmentation_export_kwargs", ":", "dict", "=", "None", ")", ":", "\n", "        ", "super", "(", ")", ".", "validate", "(", "do_mirroring", ",", "use_sliding_window", ",", "step_size", ",", "save_softmax", ",", "use_gaussian", ",", "\n", "overwrite", ",", "validation_folder_name", ",", "debug", ",", "all_in_gpu", ",", "segmentation_export_kwargs", ")", "\n", "# run brats specific validation", "\n", "output_folder", "=", "join", "(", "self", ".", "output_folder", ",", "validation_folder_name", ")", "\n", "evaluate_regions", "(", "output_folder", ",", "self", ".", "gt_niftis_folder", ",", "self", ".", "regions", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.BraTS_trainer.nnUNetTrainerV2BraTSRegions.nnUNetTrainerV2BraTSRegions.run_online_evaluation": [[168, 190], ["torch.no_grad", "torch.sigmoid", "nnunet.training.loss_functions.dice_loss.get_tp_fp_fn_tn", "tp.detach().cpu().numpy", "fp.detach().cpu().numpy", "fn.detach().cpu().numpy", "nnUNetTrainerV2BraTSRegions.nnUNetTrainerV2BraTSRegions.online_eval_foreground_dc.append", "nnUNetTrainerV2BraTSRegions.nnUNetTrainerV2BraTSRegions.online_eval_tp.append", "nnUNetTrainerV2BraTSRegions.nnUNetTrainerV2BraTSRegions.online_eval_fp.append", "nnUNetTrainerV2BraTSRegions.nnUNetTrainerV2BraTSRegions.online_eval_fn.append", "list", "list", "list", "list", "tp.detach().cpu", "fp.detach().cpu", "fn.detach().cpu", "tp.detach", "fp.detach", "fn.detach"], "methods", ["home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.loss_functions.dice_loss.get_tp_fp_fn_tn"], ["", "def", "run_online_evaluation", "(", "self", ",", "output", ",", "target", ")", ":", "\n", "        ", "output", "=", "output", "[", "0", "]", "\n", "target", "=", "target", "[", "0", "]", "\n", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "            ", "out_sigmoid", "=", "torch", ".", "sigmoid", "(", "output", ")", "\n", "out_sigmoid", "=", "(", "out_sigmoid", ">", "0.5", ")", ".", "float", "(", ")", "\n", "\n", "if", "self", ".", "threeD", ":", "\n", "                ", "axes", "=", "(", "0", ",", "2", ",", "3", ",", "4", ")", "\n", "", "else", ":", "\n", "                ", "axes", "=", "(", "0", ",", "2", ",", "3", ")", "\n", "\n", "", "tp", ",", "fp", ",", "fn", ",", "_", "=", "get_tp_fp_fn_tn", "(", "out_sigmoid", ",", "target", ",", "axes", "=", "axes", ")", "\n", "\n", "tp_hard", "=", "tp", ".", "detach", "(", ")", ".", "cpu", "(", ")", ".", "numpy", "(", ")", "\n", "fp_hard", "=", "fp", ".", "detach", "(", ")", ".", "cpu", "(", ")", ".", "numpy", "(", ")", "\n", "fn_hard", "=", "fn", ".", "detach", "(", ")", ".", "cpu", "(", ")", ".", "numpy", "(", ")", "\n", "\n", "self", ".", "online_eval_foreground_dc", ".", "append", "(", "list", "(", "(", "2", "*", "tp_hard", ")", "/", "(", "2", "*", "tp_hard", "+", "fp_hard", "+", "fn_hard", "+", "1e-8", ")", ")", ")", "\n", "self", ".", "online_eval_tp", ".", "append", "(", "list", "(", "tp_hard", ")", ")", "\n", "self", ".", "online_eval_fp", ".", "append", "(", "list", "(", "fp_hard", ")", ")", "\n", "self", ".", "online_eval_fn", ".", "append", "(", "list", "(", "fn_hard", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.BraTS_trainer.nnUNetTrainerV2BraTSRegions.nnUNetTrainerV2BraTSRegions_DDP.__init__": [[193, 202], ["nnunet.training.network_training.nnUNetTrainerV2_DDP.nnUNetTrainerV2_DDP.__init__", "nnunet.evaluation.region_based_evaluation.get_brats_regions", "torch.nn.BCEWithLogitsLoss"], "methods", ["home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.common.layers.ResizeImage.__init__", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.evaluation.region_based_evaluation.get_brats_regions"], ["    ", "def", "__init__", "(", "self", ",", "plans_file", ",", "fold", ",", "local_rank", ",", "output_folder", "=", "None", ",", "dataset_directory", "=", "None", ",", "batch_dice", "=", "True", ",", "\n", "stage", "=", "None", ",", "\n", "unpack_data", "=", "True", ",", "deterministic", "=", "True", ",", "distribute_batch_size", "=", "False", ",", "fp16", "=", "False", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "plans_file", ",", "fold", ",", "local_rank", ",", "output_folder", ",", "dataset_directory", ",", "batch_dice", ",", "stage", ",", "unpack_data", ",", "\n", "deterministic", ",", "distribute_batch_size", ",", "fp16", ")", "\n", "self", ".", "regions", "=", "get_brats_regions", "(", ")", "\n", "self", ".", "regions_class_order", "=", "(", "1", ",", "2", ",", "3", ")", "\n", "self", ".", "loss", "=", "None", "\n", "self", ".", "ce_loss", "=", "nn", ".", "BCEWithLogitsLoss", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.BraTS_trainer.nnUNetTrainerV2BraTSRegions.nnUNetTrainerV2BraTSRegions_DDP.process_plans": [[203, 209], ["super().process_plans", "len"], "methods", ["home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.BraTS_trainer.nnUNetTrainerV2BraTSRegions.nnUNetTrainerV2BraTSRegions_DA3_BN.process_plans"], ["", "def", "process_plans", "(", "self", ",", "plans", ")", ":", "\n", "        ", "super", "(", ")", ".", "process_plans", "(", "plans", ")", "\n", "\"\"\"\n        The network has as many outputs as we have regions\n        \"\"\"", "\n", "self", ".", "num_classes", "=", "len", "(", "self", ".", "regions", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.BraTS_trainer.nnUNetTrainerV2BraTSRegions.nnUNetTrainerV2BraTSRegions_DDP.initialize_network": [[210, 214], ["super().initialize_network", "torch.nn.Sigmoid"], "methods", ["home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.BraTS_trainer.nnUNetTrainerV2BraTSRegions.nnUNetTrainerV2BraTSRegions_DA3.initialize_network"], ["", "def", "initialize_network", "(", "self", ")", ":", "\n", "        ", "\"\"\"inference_apply_nonlin to sigmoid\"\"\"", "\n", "super", "(", ")", ".", "initialize_network", "(", ")", "\n", "self", ".", "network", ".", "inference_apply_nonlin", "=", "nn", ".", "Sigmoid", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.BraTS_trainer.nnUNetTrainerV2BraTSRegions.nnUNetTrainerV2BraTSRegions_DDP.initialize": [[215, 303], ["maybe_mkdir_p", "nnUNetTrainerV2BraTSRegions.nnUNetTrainerV2BraTSRegions_DDP.process_plans", "nnUNetTrainerV2BraTSRegions.nnUNetTrainerV2BraTSRegions_DDP.setup_DA_params", "join", "nnUNetTrainerV2BraTSRegions.nnUNetTrainerV2BraTSRegions_DDP.initialize_network", "nnUNetTrainerV2BraTSRegions.nnUNetTrainerV2BraTSRegions_DDP.initialize_optimizer_and_scheduler", "nnUNetTrainerV2BraTSRegions.nnUNetTrainerV2BraTSRegions_DDP._maybe_init_amp", "torch.nn.parallel.DistributedDataParallel", "nnUNetTrainerV2BraTSRegions.nnUNetTrainerV2BraTSRegions_DDP.print_to_log_file", "nnUNetTrainerV2BraTSRegions.nnUNetTrainerV2BraTSRegions_DDP.load_plans_file", "nnUNetTrainerV2BraTSRegions.nnUNetTrainerV2BraTSRegions_DDP.get_basic_generators", "len", "numpy.array", "numpy.array", "numpy.random.random_integers", "numpy.random.random_integers", "print", "print", "nnunet.training.data_augmentation.default_data_augmentation.get_moreDA_augmentation", "nnUNetTrainerV2BraTSRegions.nnUNetTrainerV2BraTSRegions_DDP.print_to_log_file", "nnUNetTrainerV2BraTSRegions.nnUNetTrainerV2BraTSRegions_DDP.print_to_log_file", "print", "numpy.array.sum", "nnUNetTrainerV2BraTSRegions.nnUNetTrainerV2BraTSRegions_DDP.data_aug_params.get", "max", "print", "nnunet.training.dataloading.dataset_loading.unpack_dataset", "print", "subfiles", "all", "str", "str", "print", "time.sleep", "all", "range", "range", "nnUNetTrainerV2BraTSRegions.nnUNetTrainerV2BraTSRegions_DDP.data_aug_params.get", "nnUNetTrainerV2BraTSRegions.nnUNetTrainerV2BraTSRegions_DDP.dataset_tr.keys", "nnUNetTrainerV2BraTSRegions.nnUNetTrainerV2BraTSRegions_DDP.dataset_val.keys", "isfile", "join", "isfile", "join"], "methods", ["home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.BraTS_trainer.nnUNetTrainerV2BraTSRegions.nnUNetTrainerV2BraTSRegions_DA3_BN.process_plans", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.BraTS_trainer.nnUNetTrainerV2BraTSRegions.nnUNetTrainerV2BraTSRegions_DA4_BN.setup_DA_params", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.BraTS_trainer.nnUNetTrainerV2BraTSRegions.nnUNetTrainerV2BraTSRegions_DA3.initialize_network", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.optimizer_and_lr.nnUNetTrainerV2_momentum09in2D.nnUNetTrainerV2_momentum09in2D.initialize_optimizer_and_scheduler", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.network_training.network_trainer.NetworkTrainer._maybe_init_amp", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.network_training.nnUNetTrainerV2_DDP.nnUNetTrainerV2_DDP.print_to_log_file", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.network_training.nnUNetTrainer.nnUNetTrainer.load_plans_file", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.nnUNet_variants.nnUNetTrainerNoDA.nnUNetTrainerNoDA.get_basic_generators", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.data_augmentation.default_data_augmentation.get_moreDA_augmentation", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.network_training.nnUNetTrainerV2_DDP.nnUNetTrainerV2_DDP.print_to_log_file", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.network_training.nnUNetTrainerV2_DDP.nnUNetTrainerV2_DDP.print_to_log_file", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.dataloading.dataset_loading.unpack_dataset"], ["", "def", "initialize", "(", "self", ",", "training", "=", "True", ",", "force_load_plans", "=", "False", ")", ":", "\n", "        ", "\"\"\"\n        this is a copy of nnUNetTrainerV2's initialize. We only add the regions to the data augmentation\n        :param training:\n        :param force_load_plans:\n        :return:\n        \"\"\"", "\n", "if", "not", "self", ".", "was_initialized", ":", "\n", "            ", "maybe_mkdir_p", "(", "self", ".", "output_folder", ")", "\n", "\n", "if", "force_load_plans", "or", "(", "self", ".", "plans", "is", "None", ")", ":", "\n", "                ", "self", ".", "load_plans_file", "(", ")", "\n", "\n", "", "self", ".", "process_plans", "(", "self", ".", "plans", ")", "\n", "\n", "self", ".", "setup_DA_params", "(", ")", "\n", "\n", "self", ".", "folder_with_preprocessed_data", "=", "join", "(", "self", ".", "dataset_directory", ",", "self", ".", "plans", "[", "'data_identifier'", "]", "+", "\n", "\"_stage%d\"", "%", "self", ".", "stage", ")", "\n", "if", "training", ":", "\n", "                ", "self", ".", "dl_tr", ",", "self", ".", "dl_val", "=", "self", ".", "get_basic_generators", "(", ")", "\n", "if", "self", ".", "unpack_data", ":", "\n", "                    ", "if", "self", ".", "local_rank", "==", "0", ":", "\n", "                        ", "print", "(", "\"unpacking dataset\"", ")", "\n", "unpack_dataset", "(", "self", ".", "folder_with_preprocessed_data", ")", "\n", "print", "(", "\"done\"", ")", "\n", "", "else", ":", "\n", "# we need to wait until worker 0 has finished unpacking", "\n", "                        ", "npz_files", "=", "subfiles", "(", "self", ".", "folder_with_preprocessed_data", ",", "suffix", "=", "\".npz\"", ",", "join", "=", "False", ")", "\n", "case_ids", "=", "[", "i", "[", ":", "-", "4", "]", "for", "i", "in", "npz_files", "]", "\n", "all_present", "=", "all", "(", "\n", "[", "isfile", "(", "join", "(", "self", ".", "folder_with_preprocessed_data", ",", "i", "+", "\".npy\"", ")", ")", "for", "i", "in", "case_ids", "]", ")", "\n", "while", "not", "all_present", ":", "\n", "                            ", "print", "(", "\"worker\"", ",", "self", ".", "local_rank", ",", "\"is waiting for unpacking\"", ")", "\n", "sleep", "(", "3", ")", "\n", "all_present", "=", "all", "(", "\n", "[", "isfile", "(", "join", "(", "self", ".", "folder_with_preprocessed_data", ",", "i", "+", "\".npy\"", ")", ")", "for", "i", "in", "case_ids", "]", ")", "\n", "# there is some slight chance that there may arise some error because dataloader are loading a file", "\n", "# that is still being written by worker 0. We ignore this for now an address it only if it becomes", "\n", "# relevant", "\n", "# (this can occur because while worker 0 writes the file is technically present so the other workers", "\n", "# will proceed and eventually try to read it)", "\n", "", "", "", "else", ":", "\n", "                    ", "print", "(", "\n", "\"INFO: Not unpacking data! Training may be slow due to that. Pray you are not using 2d or you \"", "\n", "\"will wait all winter for your model to finish!\"", ")", "\n", "\n", "# setting weights for deep supervision losses", "\n", "", "net_numpool", "=", "len", "(", "self", ".", "net_num_pool_op_kernel_sizes", ")", "\n", "\n", "# we give each output a weight which decreases exponentially (division by 2) as the resolution decreases", "\n", "# this gives higher resolution outputs more weight in the loss", "\n", "weights", "=", "np", ".", "array", "(", "[", "1", "/", "(", "2", "**", "i", ")", "for", "i", "in", "range", "(", "net_numpool", ")", "]", ")", "\n", "\n", "# we don't use the lowest 2 outputs. Normalize weights so that they sum to 1", "\n", "mask", "=", "np", ".", "array", "(", "[", "True", "if", "i", "<", "net_numpool", "-", "1", "else", "False", "for", "i", "in", "range", "(", "net_numpool", ")", "]", ")", "\n", "weights", "[", "~", "mask", "]", "=", "0", "\n", "weights", "=", "weights", "/", "weights", ".", "sum", "(", ")", "\n", "self", ".", "ds_loss_weights", "=", "weights", "\n", "\n", "seeds_train", "=", "np", ".", "random", ".", "random_integers", "(", "0", ",", "99999", ",", "self", ".", "data_aug_params", ".", "get", "(", "'num_threads'", ")", ")", "\n", "seeds_val", "=", "np", ".", "random", ".", "random_integers", "(", "0", ",", "99999", ",", "max", "(", "self", ".", "data_aug_params", ".", "get", "(", "'num_threads'", ")", "//", "2", ",", "1", ")", ")", "\n", "print", "(", "\"seeds train\"", ",", "seeds_train", ")", "\n", "print", "(", "\"seeds_val\"", ",", "seeds_val", ")", "\n", "self", ".", "tr_gen", ",", "self", ".", "val_gen", "=", "get_moreDA_augmentation", "(", "self", ".", "dl_tr", ",", "self", ".", "dl_val", ",", "\n", "self", ".", "data_aug_params", "[", "\n", "'patch_size_for_spatialtransform'", "]", ",", "\n", "self", ".", "data_aug_params", ",", "\n", "deep_supervision_scales", "=", "self", ".", "deep_supervision_scales", ",", "\n", "seeds_train", "=", "seeds_train", ",", "\n", "seeds_val", "=", "seeds_val", ",", "\n", "pin_memory", "=", "self", ".", "pin_memory", ",", "\n", "regions", "=", "self", ".", "regions", ")", "\n", "self", ".", "print_to_log_file", "(", "\"TRAINING KEYS:\\n %s\"", "%", "(", "str", "(", "self", ".", "dataset_tr", ".", "keys", "(", ")", ")", ")", ",", "\n", "also_print_to_console", "=", "False", ")", "\n", "self", ".", "print_to_log_file", "(", "\"VALIDATION KEYS:\\n %s\"", "%", "(", "str", "(", "self", ".", "dataset_val", ".", "keys", "(", ")", ")", ")", ",", "\n", "also_print_to_console", "=", "False", ")", "\n", "", "else", ":", "\n", "                ", "pass", "\n", "\n", "", "self", ".", "initialize_network", "(", ")", "\n", "self", ".", "initialize_optimizer_and_scheduler", "(", ")", "\n", "self", ".", "_maybe_init_amp", "(", ")", "\n", "self", ".", "network", "=", "DDP", "(", "self", ".", "network", ",", "self", ".", "local_rank", ")", "\n", "\n", "", "else", ":", "\n", "            ", "self", ".", "print_to_log_file", "(", "'self.was_initialized is True, not running self.initialize again'", ")", "\n", "", "self", ".", "was_initialized", "=", "True", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.BraTS_trainer.nnUNetTrainerV2BraTSRegions.nnUNetTrainerV2BraTSRegions_DDP.validate": [[304, 313], ["super().validate", "join", "nnunet.evaluation.region_based_evaluation.evaluate_regions"], "methods", ["home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.BraTS_trainer.nnUNetTrainerV2BraTSRegions.nnUNetTrainerV2BraTSRegions_DA3_BN.validate", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.evaluation.region_based_evaluation.evaluate_regions"], ["", "def", "validate", "(", "self", ",", "do_mirroring", ":", "bool", "=", "True", ",", "use_sliding_window", ":", "bool", "=", "True", ",", "\n", "step_size", ":", "int", "=", "0.5", ",", "save_softmax", ":", "bool", "=", "True", ",", "use_gaussian", ":", "bool", "=", "True", ",", "overwrite", ":", "bool", "=", "True", ",", "\n", "validation_folder_name", ":", "str", "=", "'validation_raw'", ",", "debug", ":", "bool", "=", "False", ",", "all_in_gpu", ":", "bool", "=", "False", ",", "\n", "segmentation_export_kwargs", ":", "dict", "=", "None", ")", ":", "\n", "        ", "super", "(", ")", ".", "validate", "(", "do_mirroring", ",", "use_sliding_window", ",", "step_size", ",", "save_softmax", ",", "use_gaussian", ",", "\n", "overwrite", ",", "validation_folder_name", ",", "debug", ",", "all_in_gpu", ",", "segmentation_export_kwargs", ")", "\n", "# run brats specific validation", "\n", "output_folder", "=", "join", "(", "self", ".", "output_folder", ",", "validation_folder_name", ")", "\n", "evaluate_regions", "(", "output_folder", ",", "self", ".", "gt_niftis_folder", ",", "self", ".", "regions", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.BraTS_trainer.nnUNetTrainerV2BraTSRegions.nnUNetTrainerV2BraTSRegions_DDP.run_iteration": [[314, 402], ["NotImplementedError", "next", "nnunet.utilities.to_torch.maybe_to_torch", "nnunet.utilities.to_torch.maybe_to_torch", "torch.cuda.is_available", "nnUNetTrainerV2BraTSRegions.nnUNetTrainerV2BraTSRegions_DDP.optimizer.zero_grad", "nnUNetTrainerV2BraTSRegions.nnUNetTrainerV2BraTSRegions_DDP.network", "range", "total_loss.detach().cpu().numpy", "nnunet.utilities.to_torch.to_cuda", "nnunet.utilities.to_torch.to_cuda", "len", "tuple", "torch.sigmoid", "nnunet.training.loss_functions.dice_loss.get_tp_fp_fn_tn", "nnUNetTrainerV2BraTSRegions.nnUNetTrainerV2BraTSRegions_DDP.ce_loss", "torch.nn.utils.clip_grad_norm_", "nnUNetTrainerV2BraTSRegions.nnUNetTrainerV2BraTSRegions_DDP.optimizer.step", "range", "nnunet.utilities.distributed.awesome_allgather_function.apply", "nnunet.utilities.distributed.awesome_allgather_function.apply", "nominator.sum.sum.sum", "denominator.sum.sum.sum", "torch.no_grad", "torch.sigmoid", "nnunet.training.loss_functions.dice_loss.get_tp_fp_fn_tn", "nnunet.utilities.distributed.awesome_allgather_function.apply", "nnunet.utilities.distributed.awesome_allgather_function.apply", "nnunet.utilities.distributed.awesome_allgather_function.apply", "nnUNetTrainerV2BraTSRegions.nnUNetTrainerV2BraTSRegions_DDP.run_online_evaluation", "total_loss.backward", "nnUNetTrainerV2BraTSRegions.nnUNetTrainerV2BraTSRegions_DDP.network.parameters", "total_loss.detach().cpu", "len", "nnunet.utilities.distributed.awesome_allgather_function.apply.detach().cpu().numpy().sum", "nnunet.utilities.distributed.awesome_allgather_function.apply.detach().cpu().numpy().sum", "nnunet.utilities.distributed.awesome_allgather_function.apply.detach().cpu().numpy().sum", "torch.cuda.is_available", "amp.scale_loss", "scaled_loss.backward", "output[].size", "total_loss.detach", "nnunet.utilities.distributed.awesome_allgather_function.apply.detach().cpu().numpy", "nnunet.utilities.distributed.awesome_allgather_function.apply.detach().cpu().numpy", "nnunet.utilities.distributed.awesome_allgather_function.apply.detach().cpu().numpy", "nnunet.utilities.distributed.awesome_allgather_function.apply.detach().cpu", "nnunet.utilities.distributed.awesome_allgather_function.apply.detach().cpu", "nnunet.utilities.distributed.awesome_allgather_function.apply.detach().cpu", "nnunet.utilities.distributed.awesome_allgather_function.apply.detach", "nnunet.utilities.distributed.awesome_allgather_function.apply.detach", "nnunet.utilities.distributed.awesome_allgather_function.apply.detach"], "methods", ["home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.utilities.to_torch.maybe_to_torch", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.utilities.to_torch.maybe_to_torch", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.utilities.to_torch.to_cuda", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.utilities.to_torch.to_cuda", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.loss_functions.dice_loss.get_tp_fp_fn_tn", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.optimizer.ranger.Ranger.step", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.loss_functions.dice_loss.get_tp_fp_fn_tn", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.BraTS_trainer.nnUNetTrainerV2BraTSRegions.nnUNetTrainerV2BraTSRegions_DA3_BN.run_online_evaluation", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.utilities.distributed.awesome_allgather_function.backward", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.utilities.distributed.awesome_allgather_function.backward"], ["", "def", "run_iteration", "(", "self", ",", "data_generator", ",", "do_backprop", "=", "True", ",", "run_online_evaluation", "=", "False", ")", ":", "\n", "        ", "raise", "NotImplementedError", "(", "\"this class has not been changed to work with pytorch amp yet!\"", ")", "\n", "data_dict", "=", "next", "(", "data_generator", ")", "\n", "data", "=", "data_dict", "[", "'data'", "]", "\n", "target", "=", "data_dict", "[", "'target'", "]", "\n", "\n", "data", "=", "maybe_to_torch", "(", "data", ")", "\n", "target", "=", "maybe_to_torch", "(", "target", ")", "\n", "\n", "if", "torch", ".", "cuda", ".", "is_available", "(", ")", ":", "\n", "            ", "data", "=", "to_cuda", "(", "data", ",", "gpu_id", "=", "None", ")", "\n", "target", "=", "to_cuda", "(", "target", ",", "gpu_id", "=", "None", ")", "\n", "\n", "", "self", ".", "optimizer", ".", "zero_grad", "(", ")", "\n", "\n", "output", "=", "self", ".", "network", "(", "data", ")", "\n", "del", "data", "\n", "\n", "total_loss", "=", "None", "\n", "\n", "for", "i", "in", "range", "(", "len", "(", "output", ")", ")", ":", "\n", "# Starting here it gets spicy!", "\n", "            ", "axes", "=", "tuple", "(", "range", "(", "2", ",", "len", "(", "output", "[", "i", "]", ".", "size", "(", ")", ")", ")", ")", "\n", "\n", "# network does not do softmax. We need to do softmax for dice", "\n", "output_softmax", "=", "torch", ".", "sigmoid", "(", "output", "[", "i", "]", ")", "\n", "\n", "# get the tp, fp and fn terms we need", "\n", "tp", ",", "fp", ",", "fn", ",", "_", "=", "get_tp_fp_fn_tn", "(", "output_softmax", ",", "target", "[", "i", "]", ",", "axes", ",", "mask", "=", "None", ")", "\n", "# for dice, compute nominator and denominator so that we have to accumulate only 2 instead of 3 variables", "\n", "# do_bg=False in nnUNetTrainer -> [:, 1:]", "\n", "nominator", "=", "2", "*", "tp", "[", ":", ",", "1", ":", "]", "\n", "denominator", "=", "2", "*", "tp", "[", ":", ",", "1", ":", "]", "+", "fp", "[", ":", ",", "1", ":", "]", "+", "fn", "[", ":", ",", "1", ":", "]", "\n", "\n", "if", "self", ".", "batch_dice", ":", "\n", "# for DDP we need to gather all nominator and denominator terms from all GPUS to do proper batch dice", "\n", "                ", "nominator", "=", "awesome_allgather_function", ".", "apply", "(", "nominator", ")", "\n", "denominator", "=", "awesome_allgather_function", ".", "apply", "(", "denominator", ")", "\n", "nominator", "=", "nominator", ".", "sum", "(", "0", ")", "\n", "denominator", "=", "denominator", ".", "sum", "(", "0", ")", "\n", "", "else", ":", "\n", "                ", "pass", "\n", "\n", "", "ce_loss", "=", "self", ".", "ce_loss", "(", "output", "[", "i", "]", ",", "target", "[", "i", "]", ")", "\n", "\n", "# we smooth by 1e-5 to penalize false positives if tp is 0", "\n", "dice_loss", "=", "(", "-", "(", "nominator", "+", "1e-5", ")", "/", "(", "denominator", "+", "1e-5", ")", ")", ".", "mean", "(", ")", "\n", "if", "total_loss", "is", "None", ":", "\n", "                ", "total_loss", "=", "self", ".", "ds_loss_weights", "[", "i", "]", "*", "(", "ce_loss", "+", "dice_loss", ")", "\n", "", "else", ":", "\n", "                ", "total_loss", "+=", "self", ".", "ds_loss_weights", "[", "i", "]", "*", "(", "ce_loss", "+", "dice_loss", ")", "\n", "\n", "", "", "if", "run_online_evaluation", ":", "\n", "            ", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "                ", "output", "=", "output", "[", "0", "]", "\n", "target", "=", "target", "[", "0", "]", "\n", "out_sigmoid", "=", "torch", ".", "sigmoid", "(", "output", ")", "\n", "out_sigmoid", "=", "(", "out_sigmoid", ">", "0.5", ")", ".", "float", "(", ")", "\n", "\n", "if", "self", ".", "threeD", ":", "\n", "                    ", "axes", "=", "(", "2", ",", "3", ",", "4", ")", "\n", "", "else", ":", "\n", "                    ", "axes", "=", "(", "2", ",", "3", ")", "\n", "\n", "", "tp", ",", "fp", ",", "fn", ",", "_", "=", "get_tp_fp_fn_tn", "(", "out_sigmoid", ",", "target", ",", "axes", "=", "axes", ")", "\n", "\n", "tp_hard", "=", "awesome_allgather_function", ".", "apply", "(", "tp", ")", "\n", "fp_hard", "=", "awesome_allgather_function", ".", "apply", "(", "fp", ")", "\n", "fn_hard", "=", "awesome_allgather_function", ".", "apply", "(", "fn", ")", "\n", "# print_if_rank0(\"after allgather\", tp_hard.shape)", "\n", "\n", "# print_if_rank0(\"after sum\", tp_hard.shape)", "\n", "\n", "self", ".", "run_online_evaluation", "(", "tp_hard", ".", "detach", "(", ")", ".", "cpu", "(", ")", ".", "numpy", "(", ")", ".", "sum", "(", "0", ")", ",", "\n", "fp_hard", ".", "detach", "(", ")", ".", "cpu", "(", ")", ".", "numpy", "(", ")", ".", "sum", "(", "0", ")", ",", "\n", "fn_hard", ".", "detach", "(", ")", ".", "cpu", "(", ")", ".", "numpy", "(", ")", ".", "sum", "(", "0", ")", ")", "\n", "", "", "del", "target", "\n", "\n", "if", "do_backprop", ":", "\n", "            ", "if", "not", "self", ".", "fp16", "or", "amp", "is", "None", "or", "not", "torch", ".", "cuda", ".", "is_available", "(", ")", ":", "\n", "                ", "total_loss", ".", "backward", "(", ")", "\n", "", "else", ":", "\n", "                ", "with", "amp", ".", "scale_loss", "(", "total_loss", ",", "self", ".", "optimizer", ")", "as", "scaled_loss", ":", "\n", "                    ", "scaled_loss", ".", "backward", "(", ")", "\n", "", "", "_", "=", "clip_grad_norm_", "(", "self", ".", "network", ".", "parameters", "(", ")", ",", "12", ")", "\n", "self", ".", "optimizer", ".", "step", "(", ")", "\n", "\n", "", "return", "total_loss", ".", "detach", "(", ")", ".", "cpu", "(", ")", ".", "numpy", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.BraTS_trainer.nnUNetTrainerV2BraTSRegions.nnUNetTrainerV2BraTSRegions_DDP.run_online_evaluation": [[403, 408], ["nnUNetTrainerV2BraTSRegions.nnUNetTrainerV2BraTSRegions_DDP.online_eval_foreground_dc.append", "nnUNetTrainerV2BraTSRegions.nnUNetTrainerV2BraTSRegions_DDP.online_eval_tp.append", "nnUNetTrainerV2BraTSRegions.nnUNetTrainerV2BraTSRegions_DDP.online_eval_fp.append", "nnUNetTrainerV2BraTSRegions.nnUNetTrainerV2BraTSRegions_DDP.online_eval_fn.append", "list", "list", "list", "list"], "methods", ["None"], ["", "def", "run_online_evaluation", "(", "self", ",", "tp", ",", "fp", ",", "fn", ")", ":", "\n", "        ", "self", ".", "online_eval_foreground_dc", ".", "append", "(", "list", "(", "(", "2", "*", "tp", ")", "/", "(", "2", "*", "tp", "+", "fp", "+", "fn", "+", "1e-8", ")", ")", ")", "\n", "self", ".", "online_eval_tp", ".", "append", "(", "list", "(", "tp", ")", ")", "\n", "self", ".", "online_eval_fp", ".", "append", "(", "list", "(", "fp", ")", ")", "\n", "self", ".", "online_eval_fn", ".", "append", "(", "list", "(", "fn", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.BraTS_trainer.nnUNetTrainerV2BraTSRegions.nnUNetTrainerV2BraTSRegions_DA3_BN.__init__": [[411, 418], ["nnunet.training.network_training.nnUNet_variants.data_augmentation.nnUNetTrainerV2_DA3.nnUNetTrainerV2_DA3_BN.__init__", "nnunet.evaluation.region_based_evaluation.get_brats_regions", "nnunet.training.loss_functions.dice_loss.DC_and_BCE_loss"], "methods", ["home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.common.layers.ResizeImage.__init__", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.evaluation.region_based_evaluation.get_brats_regions"], ["    ", "def", "__init__", "(", "self", ",", "plans_file", ",", "fold", ",", "output_folder", "=", "None", ",", "dataset_directory", "=", "None", ",", "batch_dice", "=", "True", ",", "stage", "=", "None", ",", "\n", "unpack_data", "=", "True", ",", "deterministic", "=", "True", ",", "fp16", "=", "False", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "plans_file", ",", "fold", ",", "output_folder", ",", "dataset_directory", ",", "batch_dice", ",", "stage", ",", "unpack_data", ",", "\n", "deterministic", ",", "fp16", ")", "\n", "self", ".", "regions", "=", "get_brats_regions", "(", ")", "\n", "self", ".", "regions_class_order", "=", "(", "1", ",", "2", ",", "3", ")", "\n", "self", ".", "loss", "=", "DC_and_BCE_loss", "(", "{", "}", ",", "{", "'batch_dice'", ":", "False", ",", "'do_bg'", ":", "True", ",", "'smooth'", ":", "0", "}", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.BraTS_trainer.nnUNetTrainerV2BraTSRegions.nnUNetTrainerV2BraTSRegions_DA3_BN.process_plans": [[419, 425], ["super().process_plans", "len"], "methods", ["home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.BraTS_trainer.nnUNetTrainerV2BraTSRegions.nnUNetTrainerV2BraTSRegions_DA3_BN.process_plans"], ["", "def", "process_plans", "(", "self", ",", "plans", ")", ":", "\n", "        ", "super", "(", ")", ".", "process_plans", "(", "plans", ")", "\n", "\"\"\"\n        The network has as many outputs as we have regions\n        \"\"\"", "\n", "self", ".", "num_classes", "=", "len", "(", "self", ".", "regions", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.BraTS_trainer.nnUNetTrainerV2BraTSRegions.nnUNetTrainerV2BraTSRegions_DA3_BN.initialize_network": [[426, 430], ["super().initialize_network", "torch.nn.Sigmoid"], "methods", ["home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.BraTS_trainer.nnUNetTrainerV2BraTSRegions.nnUNetTrainerV2BraTSRegions_DA3.initialize_network"], ["", "def", "initialize_network", "(", "self", ")", ":", "\n", "        ", "\"\"\"inference_apply_nonlin to sigmoid\"\"\"", "\n", "super", "(", ")", ".", "initialize_network", "(", ")", "\n", "self", ".", "network", ".", "inference_apply_nonlin", "=", "nn", ".", "Sigmoid", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.BraTS_trainer.nnUNetTrainerV2BraTSRegions.nnUNetTrainerV2BraTSRegions_DA3_BN.initialize": [[431, 495], ["maybe_mkdir_p", "nnUNetTrainerV2BraTSRegions.nnUNetTrainerV2BraTSRegions_DA3_BN.process_plans", "nnUNetTrainerV2BraTSRegions.nnUNetTrainerV2BraTSRegions_DA3_BN.setup_DA_params", "len", "numpy.array", "numpy.array", "nnunet.training.loss_functions.deep_supervision.MultipleOutputLoss2", "join", "nnUNetTrainerV2BraTSRegions.nnUNetTrainerV2BraTSRegions_DA3_BN.initialize_network", "nnUNetTrainerV2BraTSRegions.nnUNetTrainerV2BraTSRegions_DA3_BN.initialize_optimizer_and_scheduler", "isinstance", "nnUNetTrainerV2BraTSRegions.nnUNetTrainerV2BraTSRegions_DA3_BN.print_to_log_file", "nnUNetTrainerV2BraTSRegions.nnUNetTrainerV2BraTSRegions_DA3_BN.load_plans_file", "numpy.array.sum", "nnUNetTrainerV2BraTSRegions.nnUNetTrainerV2BraTSRegions_DA3_BN.get_basic_generators", "nnunet.training.network_training.nnUNet_variants.data_augmentation.nnUNetTrainerV2_DA3.get_insaneDA_augmentation2", "nnUNetTrainerV2BraTSRegions.nnUNetTrainerV2BraTSRegions_DA3_BN.print_to_log_file", "nnUNetTrainerV2BraTSRegions.nnUNetTrainerV2BraTSRegions_DA3_BN.print_to_log_file", "print", "nnunet.training.dataloading.dataset_loading.unpack_dataset", "print", "print", "range", "str", "str", "range", "nnUNetTrainerV2BraTSRegions.nnUNetTrainerV2BraTSRegions_DA3_BN.dataset_tr.keys", "nnUNetTrainerV2BraTSRegions.nnUNetTrainerV2BraTSRegions_DA3_BN.dataset_val.keys"], "methods", ["home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.BraTS_trainer.nnUNetTrainerV2BraTSRegions.nnUNetTrainerV2BraTSRegions_DA3_BN.process_plans", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.BraTS_trainer.nnUNetTrainerV2BraTSRegions.nnUNetTrainerV2BraTSRegions_DA4_BN.setup_DA_params", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.BraTS_trainer.nnUNetTrainerV2BraTSRegions.nnUNetTrainerV2BraTSRegions_DA3.initialize_network", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.optimizer_and_lr.nnUNetTrainerV2_momentum09in2D.nnUNetTrainerV2_momentum09in2D.initialize_optimizer_and_scheduler", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.network_training.nnUNetTrainerV2_DDP.nnUNetTrainerV2_DDP.print_to_log_file", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.network_training.nnUNetTrainer.nnUNetTrainer.load_plans_file", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.nnUNet_variants.nnUNetTrainerNoDA.nnUNetTrainerNoDA.get_basic_generators", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.data_augmentation.nnUNetTrainerV2_DA3.get_insaneDA_augmentation2", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.network_training.nnUNetTrainerV2_DDP.nnUNetTrainerV2_DDP.print_to_log_file", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.network_training.nnUNetTrainerV2_DDP.nnUNetTrainerV2_DDP.print_to_log_file", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.dataloading.dataset_loading.unpack_dataset"], ["", "def", "initialize", "(", "self", ",", "training", "=", "True", ",", "force_load_plans", "=", "False", ")", ":", "\n", "        ", "if", "not", "self", ".", "was_initialized", ":", "\n", "            ", "maybe_mkdir_p", "(", "self", ".", "output_folder", ")", "\n", "\n", "if", "force_load_plans", "or", "(", "self", ".", "plans", "is", "None", ")", ":", "\n", "                ", "self", ".", "load_plans_file", "(", ")", "\n", "\n", "", "self", ".", "process_plans", "(", "self", ".", "plans", ")", "\n", "\n", "self", ".", "setup_DA_params", "(", ")", "\n", "\n", "################# Here we wrap the loss for deep supervision ############", "\n", "# we need to know the number of outputs of the network", "\n", "net_numpool", "=", "len", "(", "self", ".", "net_num_pool_op_kernel_sizes", ")", "\n", "\n", "# we give each output a weight which decreases exponentially (division by 2) as the resolution decreases", "\n", "# this gives higher resolution outputs more weight in the loss", "\n", "weights", "=", "np", ".", "array", "(", "[", "1", "/", "(", "2", "**", "i", ")", "for", "i", "in", "range", "(", "net_numpool", ")", "]", ")", "\n", "\n", "# we don't use the lowest 2 outputs. Normalize weights so that they sum to 1", "\n", "mask", "=", "np", ".", "array", "(", "[", "True", "]", "+", "[", "True", "if", "i", "<", "net_numpool", "-", "1", "else", "False", "for", "i", "in", "range", "(", "1", ",", "net_numpool", ")", "]", ")", "\n", "weights", "[", "~", "mask", "]", "=", "0", "\n", "weights", "=", "weights", "/", "weights", ".", "sum", "(", ")", "\n", "self", ".", "ds_loss_weights", "=", "weights", "\n", "# now wrap the loss", "\n", "self", ".", "loss", "=", "MultipleOutputLoss2", "(", "self", ".", "loss", ",", "self", ".", "ds_loss_weights", ")", "\n", "################# END ###################", "\n", "\n", "self", ".", "folder_with_preprocessed_data", "=", "join", "(", "self", ".", "dataset_directory", ",", "self", ".", "plans", "[", "'data_identifier'", "]", "+", "\n", "\"_stage%d\"", "%", "self", ".", "stage", ")", "\n", "if", "training", ":", "\n", "                ", "self", ".", "dl_tr", ",", "self", ".", "dl_val", "=", "self", ".", "get_basic_generators", "(", ")", "\n", "if", "self", ".", "unpack_data", ":", "\n", "                    ", "print", "(", "\"unpacking dataset\"", ")", "\n", "unpack_dataset", "(", "self", ".", "folder_with_preprocessed_data", ")", "\n", "print", "(", "\"done\"", ")", "\n", "", "else", ":", "\n", "                    ", "print", "(", "\n", "\"INFO: Not unpacking data! Training may be slow due to that. Pray you are not using 2d or you \"", "\n", "\"will wait all winter for your model to finish!\"", ")", "\n", "\n", "", "self", ".", "tr_gen", ",", "self", ".", "val_gen", "=", "get_insaneDA_augmentation2", "(", "\n", "self", ".", "dl_tr", ",", "self", ".", "dl_val", ",", "\n", "self", ".", "data_aug_params", "[", "\n", "'patch_size_for_spatialtransform'", "]", ",", "\n", "self", ".", "data_aug_params", ",", "\n", "deep_supervision_scales", "=", "self", ".", "deep_supervision_scales", ",", "\n", "pin_memory", "=", "self", ".", "pin_memory", ",", "\n", "regions", "=", "self", ".", "regions", "\n", ")", "\n", "self", ".", "print_to_log_file", "(", "\"TRAINING KEYS:\\n %s\"", "%", "(", "str", "(", "self", ".", "dataset_tr", ".", "keys", "(", ")", ")", ")", ",", "\n", "also_print_to_console", "=", "False", ")", "\n", "self", ".", "print_to_log_file", "(", "\"VALIDATION KEYS:\\n %s\"", "%", "(", "str", "(", "self", ".", "dataset_val", ".", "keys", "(", ")", ")", ")", ",", "\n", "also_print_to_console", "=", "False", ")", "\n", "", "else", ":", "\n", "                ", "pass", "\n", "\n", "", "self", ".", "initialize_network", "(", ")", "\n", "self", ".", "initialize_optimizer_and_scheduler", "(", ")", "\n", "\n", "assert", "isinstance", "(", "self", ".", "network", ",", "(", "SegmentationNetwork", ",", "nn", ".", "DataParallel", ")", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "print_to_log_file", "(", "'self.was_initialized is True, not running self.initialize again'", ")", "\n", "", "self", ".", "was_initialized", "=", "True", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.BraTS_trainer.nnUNetTrainerV2BraTSRegions.nnUNetTrainerV2BraTSRegions_DA3_BN.validate": [[496, 505], ["super().validate", "join", "nnunet.evaluation.region_based_evaluation.evaluate_regions"], "methods", ["home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.BraTS_trainer.nnUNetTrainerV2BraTSRegions.nnUNetTrainerV2BraTSRegions_DA3_BN.validate", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.evaluation.region_based_evaluation.evaluate_regions"], ["", "def", "validate", "(", "self", ",", "do_mirroring", ":", "bool", "=", "True", ",", "use_sliding_window", ":", "bool", "=", "True", ",", "\n", "step_size", ":", "int", "=", "0.5", ",", "save_softmax", ":", "bool", "=", "True", ",", "use_gaussian", ":", "bool", "=", "True", ",", "overwrite", ":", "bool", "=", "True", ",", "\n", "validation_folder_name", ":", "str", "=", "'validation_raw'", ",", "debug", ":", "bool", "=", "False", ",", "all_in_gpu", ":", "bool", "=", "False", ",", "\n", "segmentation_export_kwargs", ":", "dict", "=", "None", ")", ":", "\n", "        ", "super", "(", ")", ".", "validate", "(", "do_mirroring", ",", "use_sliding_window", ",", "step_size", ",", "save_softmax", ",", "use_gaussian", ",", "\n", "overwrite", ",", "validation_folder_name", ",", "debug", ",", "all_in_gpu", ",", "segmentation_export_kwargs", ")", "\n", "# run brats specific validation", "\n", "output_folder", "=", "join", "(", "self", ".", "output_folder", ",", "validation_folder_name", ")", "\n", "evaluate_regions", "(", "output_folder", ",", "self", ".", "gt_niftis_folder", ",", "self", ".", "regions", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.BraTS_trainer.nnUNetTrainerV2BraTSRegions.nnUNetTrainerV2BraTSRegions_DA3_BN.run_online_evaluation": [[506, 528], ["torch.no_grad", "torch.sigmoid", "nnunet.training.loss_functions.dice_loss.get_tp_fp_fn_tn", "tp.detach().cpu().numpy", "fp.detach().cpu().numpy", "fn.detach().cpu().numpy", "nnUNetTrainerV2BraTSRegions.nnUNetTrainerV2BraTSRegions_DA3_BN.online_eval_foreground_dc.append", "nnUNetTrainerV2BraTSRegions.nnUNetTrainerV2BraTSRegions_DA3_BN.online_eval_tp.append", "nnUNetTrainerV2BraTSRegions.nnUNetTrainerV2BraTSRegions_DA3_BN.online_eval_fp.append", "nnUNetTrainerV2BraTSRegions.nnUNetTrainerV2BraTSRegions_DA3_BN.online_eval_fn.append", "list", "list", "list", "list", "tp.detach().cpu", "fp.detach().cpu", "fn.detach().cpu", "tp.detach", "fp.detach", "fn.detach"], "methods", ["home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.loss_functions.dice_loss.get_tp_fp_fn_tn"], ["", "def", "run_online_evaluation", "(", "self", ",", "output", ",", "target", ")", ":", "\n", "        ", "output", "=", "output", "[", "0", "]", "\n", "target", "=", "target", "[", "0", "]", "\n", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "            ", "out_sigmoid", "=", "torch", ".", "sigmoid", "(", "output", ")", "\n", "out_sigmoid", "=", "(", "out_sigmoid", ">", "0.5", ")", ".", "float", "(", ")", "\n", "\n", "if", "self", ".", "threeD", ":", "\n", "                ", "axes", "=", "(", "0", ",", "2", ",", "3", ",", "4", ")", "\n", "", "else", ":", "\n", "                ", "axes", "=", "(", "0", ",", "2", ",", "3", ")", "\n", "\n", "", "tp", ",", "fp", ",", "fn", ",", "_", "=", "get_tp_fp_fn_tn", "(", "out_sigmoid", ",", "target", ",", "axes", "=", "axes", ")", "\n", "\n", "tp_hard", "=", "tp", ".", "detach", "(", ")", ".", "cpu", "(", ")", ".", "numpy", "(", ")", "\n", "fp_hard", "=", "fp", ".", "detach", "(", ")", ".", "cpu", "(", ")", ".", "numpy", "(", ")", "\n", "fn_hard", "=", "fn", ".", "detach", "(", ")", ".", "cpu", "(", ")", ".", "numpy", "(", ")", "\n", "\n", "self", ".", "online_eval_foreground_dc", ".", "append", "(", "list", "(", "(", "2", "*", "tp_hard", ")", "/", "(", "2", "*", "tp_hard", "+", "fp_hard", "+", "fn_hard", "+", "1e-8", ")", ")", ")", "\n", "self", ".", "online_eval_tp", ".", "append", "(", "list", "(", "tp_hard", ")", ")", "\n", "self", ".", "online_eval_fp", ".", "append", "(", "list", "(", "fp_hard", ")", ")", "\n", "self", ".", "online_eval_fn", ".", "append", "(", "list", "(", "fn_hard", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.BraTS_trainer.nnUNetTrainerV2BraTSRegions.nnUNetTrainerV2BraTSRegions_DA4_BN.setup_DA_params": [[530, 593], ["nnunet.training.network_training.nnUNetTrainerV2.nnUNetTrainerV2.setup_DA_params", "nnunet.training.data_augmentation.default_data_augmentation.get_patch_size", "numpy.array", "nnunet.training.data_augmentation.default_data_augmentation.get_patch_size", "list", "nnUNetTrainerV2BraTSRegions.nnUNetTrainerV2BraTSRegions_DA4_BN.print_to_log_file", "max", "min", "list", "list", "numpy.cumprod", "numpy.vstack"], "methods", ["home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.BraTS_trainer.nnUNetTrainerV2BraTSRegions.nnUNetTrainerV2BraTSRegions_DA4_BN.setup_DA_params", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.data_augmentation.default_data_augmentation.get_patch_size", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.data_augmentation.default_data_augmentation.get_patch_size", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.network_training.nnUNetTrainerV2_DDP.nnUNetTrainerV2_DDP.print_to_log_file"], ["    ", "def", "setup_DA_params", "(", "self", ")", ":", "\n", "        ", "nnUNetTrainerV2", ".", "setup_DA_params", "(", "self", ")", "\n", "self", ".", "deep_supervision_scales", "=", "[", "[", "1", ",", "1", ",", "1", "]", "]", "+", "list", "(", "list", "(", "i", ")", "for", "i", "in", "1", "/", "np", ".", "cumprod", "(", "\n", "np", ".", "vstack", "(", "self", ".", "net_num_pool_op_kernel_sizes", ")", ",", "axis", "=", "0", ")", ")", "[", ":", "-", "1", "]", "\n", "\n", "if", "self", ".", "threeD", ":", "\n", "            ", "self", ".", "data_aug_params", "=", "default_3D_augmentation_params", "\n", "self", ".", "data_aug_params", "[", "'rotation_x'", "]", "=", "(", "-", "90.", "/", "360", "*", "2.", "*", "np", ".", "pi", ",", "90.", "/", "360", "*", "2.", "*", "np", ".", "pi", ")", "\n", "self", ".", "data_aug_params", "[", "'rotation_y'", "]", "=", "(", "-", "90.", "/", "360", "*", "2.", "*", "np", ".", "pi", ",", "90.", "/", "360", "*", "2.", "*", "np", ".", "pi", ")", "\n", "self", ".", "data_aug_params", "[", "'rotation_z'", "]", "=", "(", "-", "90.", "/", "360", "*", "2.", "*", "np", ".", "pi", ",", "90.", "/", "360", "*", "2.", "*", "np", ".", "pi", ")", "\n", "if", "self", ".", "do_dummy_2D_aug", ":", "\n", "                ", "self", ".", "data_aug_params", "[", "\"dummy_2D\"", "]", "=", "True", "\n", "self", ".", "print_to_log_file", "(", "\"Using dummy2d data augmentation\"", ")", "\n", "self", ".", "data_aug_params", "[", "\"elastic_deform_alpha\"", "]", "=", "default_2D_augmentation_params", "[", "\"elastic_deform_alpha\"", "]", "\n", "self", ".", "data_aug_params", "[", "\"elastic_deform_sigma\"", "]", "=", "default_2D_augmentation_params", "[", "\"elastic_deform_sigma\"", "]", "\n", "self", ".", "data_aug_params", "[", "\"rotation_x\"", "]", "=", "default_2D_augmentation_params", "[", "\"rotation_x\"", "]", "\n", "", "", "else", ":", "\n", "            ", "self", ".", "do_dummy_2D_aug", "=", "False", "\n", "if", "max", "(", "self", ".", "patch_size", ")", "/", "min", "(", "self", ".", "patch_size", ")", ">", "1.5", ":", "\n", "                ", "default_2D_augmentation_params", "[", "'rotation_x'", "]", "=", "(", "-", "180.", "/", "360", "*", "2.", "*", "np", ".", "pi", ",", "180.", "/", "360", "*", "2.", "*", "np", ".", "pi", ")", "\n", "", "self", ".", "data_aug_params", "=", "default_2D_augmentation_params", "\n", "", "self", ".", "data_aug_params", "[", "\"mask_was_used_for_normalization\"", "]", "=", "self", ".", "use_mask_for_norm", "\n", "\n", "if", "self", ".", "do_dummy_2D_aug", ":", "\n", "            ", "self", ".", "basic_generator_patch_size", "=", "get_patch_size", "(", "self", ".", "patch_size", "[", "1", ":", "]", ",", "\n", "self", ".", "data_aug_params", "[", "'rotation_x'", "]", ",", "\n", "self", ".", "data_aug_params", "[", "'rotation_y'", "]", ",", "\n", "self", ".", "data_aug_params", "[", "'rotation_z'", "]", ",", "\n", "self", ".", "data_aug_params", "[", "'scale_range'", "]", ")", "\n", "self", ".", "basic_generator_patch_size", "=", "np", ".", "array", "(", "[", "self", ".", "patch_size", "[", "0", "]", "]", "+", "list", "(", "self", ".", "basic_generator_patch_size", ")", ")", "\n", "patch_size_for_spatialtransform", "=", "self", ".", "patch_size", "[", "1", ":", "]", "\n", "", "else", ":", "\n", "            ", "self", ".", "basic_generator_patch_size", "=", "get_patch_size", "(", "self", ".", "patch_size", ",", "self", ".", "data_aug_params", "[", "'rotation_x'", "]", ",", "\n", "self", ".", "data_aug_params", "[", "'rotation_y'", "]", ",", "\n", "self", ".", "data_aug_params", "[", "'rotation_z'", "]", ",", "\n", "self", ".", "data_aug_params", "[", "'scale_range'", "]", ")", "\n", "patch_size_for_spatialtransform", "=", "self", ".", "patch_size", "\n", "\n", "", "self", ".", "data_aug_params", "[", "'selected_seg_channels'", "]", "=", "[", "0", "]", "\n", "self", ".", "data_aug_params", "[", "'patch_size_for_spatialtransform'", "]", "=", "patch_size_for_spatialtransform", "\n", "\n", "self", ".", "data_aug_params", "[", "\"p_rot\"", "]", "=", "0.3", "\n", "\n", "self", ".", "data_aug_params", "[", "\"scale_range\"", "]", "=", "(", "0.65", ",", "1.6", ")", "\n", "self", ".", "data_aug_params", "[", "\"p_scale\"", "]", "=", "0.3", "\n", "self", ".", "data_aug_params", "[", "\"independent_scale_factor_for_each_axis\"", "]", "=", "True", "\n", "self", ".", "data_aug_params", "[", "\"p_independent_scale_per_axis\"", "]", "=", "0.3", "\n", "\n", "self", ".", "data_aug_params", "[", "\"do_elastic\"", "]", "=", "True", "\n", "self", ".", "data_aug_params", "[", "\"p_eldef\"", "]", "=", "0.2", "\n", "self", ".", "data_aug_params", "[", "\"eldef_deformation_scale\"", "]", "=", "(", "0", ",", "0.25", ")", "\n", "\n", "self", ".", "data_aug_params", "[", "\"do_additive_brightness\"", "]", "=", "True", "\n", "self", ".", "data_aug_params", "[", "\"additive_brightness_mu\"", "]", "=", "0", "\n", "self", ".", "data_aug_params", "[", "\"additive_brightness_sigma\"", "]", "=", "0.2", "\n", "self", ".", "data_aug_params", "[", "\"additive_brightness_p_per_sample\"", "]", "=", "0.3", "\n", "self", ".", "data_aug_params", "[", "\"additive_brightness_p_per_channel\"", "]", "=", "0.5", "\n", "\n", "self", ".", "data_aug_params", "[", "'gamma_range'", "]", "=", "(", "0.5", ",", "1.6", ")", "\n", "\n", "self", ".", "data_aug_params", "[", "'num_cached_per_thread'", "]", "=", "4", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.BraTS_trainer.nnUNetTrainerV2BraTSRegions.nnUNetTrainerV2BraTSRegions_DA3.initialize_network": [[596, 620], ["nnunet.network_architecture.generic_UNet.Generic_UNet", "torch.cuda.is_available", "torch.nn.Sigmoid", "len", "nnunet.network_architecture.initialization.InitWeights_He", "nnUNetTrainerV2BraTSRegions.nnUNetTrainerV2BraTSRegions_DA3.network.cuda"], "methods", ["None"], ["    ", "def", "initialize_network", "(", "self", ")", ":", "\n", "        ", "if", "self", ".", "threeD", ":", "\n", "            ", "conv_op", "=", "nn", ".", "Conv3d", "\n", "dropout_op", "=", "nn", ".", "Dropout3d", "\n", "norm_op", "=", "nn", ".", "InstanceNorm3d", "\n", "\n", "", "else", ":", "\n", "            ", "conv_op", "=", "nn", ".", "Conv2d", "\n", "dropout_op", "=", "nn", ".", "Dropout2d", "\n", "norm_op", "=", "nn", ".", "InstanceNorm2d", "\n", "\n", "", "norm_op_kwargs", "=", "{", "'eps'", ":", "1e-5", ",", "'affine'", ":", "True", "}", "\n", "dropout_op_kwargs", "=", "{", "'p'", ":", "0", ",", "'inplace'", ":", "True", "}", "\n", "net_nonlin", "=", "nn", ".", "LeakyReLU", "\n", "net_nonlin_kwargs", "=", "{", "'negative_slope'", ":", "1e-2", ",", "'inplace'", ":", "True", "}", "\n", "self", ".", "network", "=", "Generic_UNet", "(", "self", ".", "num_input_channels", ",", "self", ".", "base_num_features", ",", "self", ".", "num_classes", ",", "\n", "len", "(", "self", ".", "net_num_pool_op_kernel_sizes", ")", ",", "\n", "self", ".", "conv_per_stage", ",", "2", ",", "conv_op", ",", "norm_op", ",", "norm_op_kwargs", ",", "dropout_op", ",", "\n", "dropout_op_kwargs", ",", "\n", "net_nonlin", ",", "net_nonlin_kwargs", ",", "True", ",", "False", ",", "lambda", "x", ":", "x", ",", "InitWeights_He", "(", "1e-2", ")", ",", "\n", "self", ".", "net_num_pool_op_kernel_sizes", ",", "self", ".", "net_conv_kernel_sizes", ",", "False", ",", "True", ",", "True", ")", "\n", "if", "torch", ".", "cuda", ".", "is_available", "(", ")", ":", "\n", "            ", "self", ".", "network", ".", "cuda", "(", ")", "\n", "", "self", ".", "network", ".", "inference_apply_nonlin", "=", "nn", ".", "Sigmoid", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.BraTS_trainer.nnUNetTrainerV2BraTSRegions.nnUNetTrainerV2BraTSRegions_DA3_BD.__init__": [[623, 628], ["nnUNetTrainerV2BraTSRegions.nnUNetTrainerV2BraTSRegions_DA3_BN.__init__", "nnunet.training.loss_functions.dice_loss.DC_and_BCE_loss"], "methods", ["home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.common.layers.ResizeImage.__init__"], ["    ", "def", "__init__", "(", "self", ",", "plans_file", ",", "fold", ",", "output_folder", "=", "None", ",", "dataset_directory", "=", "None", ",", "batch_dice", "=", "True", ",", "stage", "=", "None", ",", "\n", "unpack_data", "=", "True", ",", "deterministic", "=", "True", ",", "fp16", "=", "False", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "plans_file", ",", "fold", ",", "output_folder", ",", "dataset_directory", ",", "batch_dice", ",", "stage", ",", "unpack_data", ",", "\n", "deterministic", ",", "fp16", ")", "\n", "self", ".", "loss", "=", "DC_and_BCE_loss", "(", "{", "}", ",", "{", "'batch_dice'", ":", "True", ",", "'do_bg'", ":", "True", ",", "'smooth'", ":", "0", "}", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.BraTS_trainer.nnUNetTrainerV2BraTSRegions.nnUNetTrainerV2BraTSRegions_DA4_BN_BD.__init__": [[631, 636], ["nnUNetTrainerV2BraTSRegions.nnUNetTrainerV2BraTSRegions_DA3_BN.__init__", "nnunet.training.loss_functions.dice_loss.DC_and_BCE_loss"], "methods", ["home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.common.layers.ResizeImage.__init__"], ["    ", "def", "__init__", "(", "self", ",", "plans_file", ",", "fold", ",", "output_folder", "=", "None", ",", "dataset_directory", "=", "None", ",", "batch_dice", "=", "True", ",", "stage", "=", "None", ",", "\n", "unpack_data", "=", "True", ",", "deterministic", "=", "True", ",", "fp16", "=", "False", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "plans_file", ",", "fold", ",", "output_folder", ",", "dataset_directory", ",", "batch_dice", ",", "stage", ",", "unpack_data", ",", "\n", "deterministic", ",", "fp16", ")", "\n", "self", ".", "loss", "=", "DC_and_BCE_loss", "(", "{", "}", ",", "{", "'batch_dice'", ":", "True", ",", "'do_bg'", ":", "True", ",", "'smooth'", ":", "0", "}", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.BraTS_trainer.nnUNetTrainerV2BraTSRegions.nnUNetTrainerV2BraTSRegions_DA3_BN_BD.__init__": [[639, 644], ["nnUNetTrainerV2BraTSRegions.nnUNetTrainerV2BraTSRegions_DA3_BN.__init__", "nnunet.training.loss_functions.dice_loss.DC_and_BCE_loss"], "methods", ["home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.common.layers.ResizeImage.__init__"], ["    ", "def", "__init__", "(", "self", ",", "plans_file", ",", "fold", ",", "output_folder", "=", "None", ",", "dataset_directory", "=", "None", ",", "batch_dice", "=", "True", ",", "stage", "=", "None", ",", "\n", "unpack_data", "=", "True", ",", "deterministic", "=", "True", ",", "fp16", "=", "False", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "plans_file", ",", "fold", ",", "output_folder", ",", "dataset_directory", ",", "batch_dice", ",", "stage", ",", "unpack_data", ",", "\n", "deterministic", ",", "fp16", ")", "\n", "self", ".", "loss", "=", "DC_and_BCE_loss", "(", "{", "}", ",", "{", "'batch_dice'", ":", "True", ",", "'do_bg'", ":", "True", ",", "'smooth'", ":", "0", "}", ")", "", "", "", ""]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.loss_functions.deep_supervision.MultipleOutputLoss2.__init__": [[20, 30], ["torch.nn.Module.__init__"], "methods", ["home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.common.layers.ResizeImage.__init__"], ["    ", "def", "__init__", "(", "self", ",", "loss", ",", "weight_factors", "=", "None", ")", ":", "\n", "        ", "\"\"\"\n        use this if you have several outputs and ground truth (both list of same len) and the loss should be computed\n        between them (x[0] and y[0], x[1] and y[1] etc)\n        :param loss:\n        :param weight_factors:\n        \"\"\"", "\n", "super", "(", "MultipleOutputLoss2", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "weight_factors", "=", "weight_factors", "\n", "self", ".", "loss", "=", "loss", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.loss_functions.deep_supervision.MultipleOutputLoss2.forward": [[31, 44], ["isinstance", "isinstance", "deep_supervision.MultipleOutputLoss2.loss", "len"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "x", ",", "y", ")", ":", "\n", "        ", "assert", "isinstance", "(", "x", ",", "(", "tuple", ",", "list", ")", ")", ",", "\"x must be either tuple or list\"", "\n", "assert", "isinstance", "(", "y", ",", "(", "tuple", ",", "list", ")", ")", ",", "\"y must be either tuple or list\"", "\n", "if", "self", ".", "weight_factors", "is", "None", ":", "\n", "            ", "weights", "=", "[", "1", "]", "*", "len", "(", "x", ")", "\n", "", "else", ":", "\n", "            ", "weights", "=", "self", ".", "weight_factors", "\n", "\n", "", "l", "=", "weights", "[", "0", "]", "*", "self", ".", "loss", "(", "x", "[", "-", "1", "]", ",", "y", "[", "0", "]", ")", "\n", "#for i in range(1, len(x)):", "\n", "#    if weights[i] != 0:", "\n", "#        l += weights[i] * self.loss(x[i], y[0])", "\n", "return", "l", "\n", "", "", ""]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.loss_functions.TopK_loss.TopKLoss.__init__": [[24, 27], ["nnunet.training.loss_functions.crossentropy.RobustCrossEntropyLoss.__init__"], "methods", ["home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.common.layers.ResizeImage.__init__"], ["def", "__init__", "(", "self", ",", "weight", "=", "None", ",", "ignore_index", "=", "-", "100", ",", "k", "=", "10", ")", ":", "\n", "        ", "self", ".", "k", "=", "k", "\n", "super", "(", "TopKLoss", ",", "self", ")", ".", "__init__", "(", "weight", ",", "False", ",", "ignore_index", ",", "reduce", "=", "False", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.loss_functions.TopK_loss.TopKLoss.forward": [[28, 34], ["target[].long", "super().forward", "numpy.prod", "torch.topk", "super().forward.mean", "super().forward.view", "int"], "methods", ["home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.utilities.distributed.awesome_allgather_function.forward"], ["", "def", "forward", "(", "self", ",", "inp", ",", "target", ")", ":", "\n", "        ", "target", "=", "target", "[", ":", ",", "0", "]", ".", "long", "(", ")", "\n", "res", "=", "super", "(", "TopKLoss", ",", "self", ")", ".", "forward", "(", "inp", ",", "target", ")", "\n", "num_voxels", "=", "np", ".", "prod", "(", "res", ".", "shape", ",", "dtype", "=", "np", ".", "int64", ")", "\n", "res", ",", "_", "=", "torch", ".", "topk", "(", "res", ".", "view", "(", "(", "-", "1", ",", ")", ")", ",", "int", "(", "num_voxels", "*", "self", ".", "k", "/", "100", ")", ",", "sorted", "=", "False", ")", "\n", "return", "res", ".", "mean", "(", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.loss_functions.dice_loss.GDL.__init__": [[26, 39], ["torch.nn.Module.__init__"], "methods", ["home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.common.layers.ResizeImage.__init__"], ["    ", "def", "__init__", "(", "self", ",", "apply_nonlin", "=", "None", ",", "batch_dice", "=", "False", ",", "do_bg", "=", "True", ",", "smooth", "=", "1.", ",", "\n", "square", "=", "False", ",", "square_volumes", "=", "False", ")", ":", "\n", "        ", "\"\"\"\n        square_volumes will square the weight term. The paper recommends square_volumes=True; I don't (just an intuition)\n        \"\"\"", "\n", "super", "(", "GDL", ",", "self", ")", ".", "__init__", "(", ")", "\n", "\n", "self", ".", "square_volumes", "=", "square_volumes", "\n", "self", ".", "square", "=", "square", "\n", "self", ".", "do_bg", "=", "do_bg", "\n", "self", ".", "batch_dice", "=", "batch_dice", "\n", "self", ".", "apply_nonlin", "=", "apply_nonlin", "\n", "self", ".", "smooth", "=", "smooth", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.loss_functions.dice_loss.GDL.forward": [[40, 98], ["all", "dice_loss.get_tp_fp_fn_tn", "tp.sum.sum.sum", "fp.sum.sum.sum", "fn.sum.sum.sum", "dc.mean.mean.mean", "list", "len", "len", "y.view.view.view", "y.view.view.long", "torch.zeros", "y_onehot.cuda.cuda.scatter_", "dice_loss.GDL.apply_nonlin", "nnunet.utilities.tensor_utilities.sum_tensor", "list", "range", "y_onehot.cuda.cuda.cuda", "range", "len", "zip", "len"], "methods", ["home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.loss_functions.dice_loss.get_tp_fp_fn_tn", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.utilities.tensor_utilities.sum_tensor"], ["", "def", "forward", "(", "self", ",", "x", ",", "y", ",", "loss_mask", "=", "None", ")", ":", "\n", "        ", "shp_x", "=", "x", ".", "shape", "\n", "shp_y", "=", "y", ".", "shape", "\n", "\n", "if", "self", ".", "batch_dice", ":", "\n", "            ", "axes", "=", "[", "0", "]", "+", "list", "(", "range", "(", "2", ",", "len", "(", "shp_x", ")", ")", ")", "\n", "", "else", ":", "\n", "            ", "axes", "=", "list", "(", "range", "(", "2", ",", "len", "(", "shp_x", ")", ")", ")", "\n", "\n", "", "if", "len", "(", "shp_x", ")", "!=", "len", "(", "shp_y", ")", ":", "\n", "            ", "y", "=", "y", ".", "view", "(", "(", "shp_y", "[", "0", "]", ",", "1", ",", "*", "shp_y", "[", "1", ":", "]", ")", ")", "\n", "\n", "", "if", "all", "(", "[", "i", "==", "j", "for", "i", ",", "j", "in", "zip", "(", "x", ".", "shape", ",", "y", ".", "shape", ")", "]", ")", ":", "\n", "# if this is the case then gt is probably already a one hot encoding", "\n", "            ", "y_onehot", "=", "y", "\n", "", "else", ":", "\n", "            ", "gt", "=", "y", ".", "long", "(", ")", "\n", "y_onehot", "=", "torch", ".", "zeros", "(", "shp_x", ")", "\n", "if", "x", ".", "device", ".", "type", "==", "\"cuda\"", ":", "\n", "                ", "y_onehot", "=", "y_onehot", ".", "cuda", "(", "x", ".", "device", ".", "index", ")", "\n", "", "y_onehot", ".", "scatter_", "(", "1", ",", "gt", ",", "1", ")", "\n", "\n", "", "if", "self", ".", "apply_nonlin", "is", "not", "None", ":", "\n", "            ", "x", "=", "self", ".", "apply_nonlin", "(", "x", ")", "\n", "\n", "", "if", "not", "self", ".", "do_bg", ":", "\n", "            ", "x", "=", "x", "[", ":", ",", "1", ":", "]", "\n", "y_onehot", "=", "y_onehot", "[", ":", ",", "1", ":", "]", "\n", "\n", "", "tp", ",", "fp", ",", "fn", ",", "_", "=", "get_tp_fp_fn_tn", "(", "x", ",", "y_onehot", ",", "axes", ",", "loss_mask", ",", "self", ".", "square", ")", "\n", "\n", "# GDL weight computation, we use 1/V", "\n", "volumes", "=", "sum_tensor", "(", "y_onehot", ",", "axes", ")", "+", "1e-6", "# add some eps to prevent div by zero", "\n", "\n", "if", "self", ".", "square_volumes", ":", "\n", "            ", "volumes", "=", "volumes", "**", "2", "\n", "\n", "# apply weights", "\n", "", "tp", "=", "tp", "/", "volumes", "\n", "fp", "=", "fp", "/", "volumes", "\n", "fn", "=", "fn", "/", "volumes", "\n", "\n", "# sum over classes", "\n", "if", "self", ".", "batch_dice", ":", "\n", "            ", "axis", "=", "0", "\n", "", "else", ":", "\n", "            ", "axis", "=", "1", "\n", "\n", "", "tp", "=", "tp", ".", "sum", "(", "axis", ",", "keepdim", "=", "False", ")", "\n", "fp", "=", "fp", ".", "sum", "(", "axis", ",", "keepdim", "=", "False", ")", "\n", "fn", "=", "fn", ".", "sum", "(", "axis", ",", "keepdim", "=", "False", ")", "\n", "\n", "# compute dice", "\n", "dc", "=", "(", "2", "*", "tp", "+", "self", ".", "smooth", ")", "/", "(", "2", "*", "tp", "+", "fp", "+", "fn", "+", "self", ".", "smooth", ")", "\n", "\n", "dc", "=", "dc", ".", "mean", "(", ")", "\n", "\n", "return", "-", "dc", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.loss_functions.dice_loss.SoftDiceLoss.__init__": [[159, 168], ["torch.nn.Module.__init__"], "methods", ["home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.common.layers.ResizeImage.__init__"], ["    ", "def", "__init__", "(", "self", ",", "apply_nonlin", "=", "None", ",", "batch_dice", "=", "False", ",", "do_bg", "=", "True", ",", "smooth", "=", "1.", ")", ":", "\n", "        ", "\"\"\"\n        \"\"\"", "\n", "super", "(", "SoftDiceLoss", ",", "self", ")", ".", "__init__", "(", ")", "\n", "\n", "self", ".", "do_bg", "=", "do_bg", "\n", "self", ".", "batch_dice", "=", "batch_dice", "\n", "self", ".", "apply_nonlin", "=", "apply_nonlin", "\n", "self", ".", "smooth", "=", "smooth", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.loss_functions.dice_loss.SoftDiceLoss.forward": [[169, 195], ["dice_loss.get_tp_fp_fn_tn", "dc.mean.mean.mean", "list", "dice_loss.SoftDiceLoss.apply_nonlin", "list", "range", "range", "len", "len"], "methods", ["home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.loss_functions.dice_loss.get_tp_fp_fn_tn"], ["", "def", "forward", "(", "self", ",", "x", ",", "y", ",", "loss_mask", "=", "None", ")", ":", "\n", "        ", "shp_x", "=", "x", ".", "shape", "\n", "\n", "if", "self", ".", "batch_dice", ":", "\n", "            ", "axes", "=", "[", "0", "]", "+", "list", "(", "range", "(", "2", ",", "len", "(", "shp_x", ")", ")", ")", "\n", "", "else", ":", "\n", "            ", "axes", "=", "list", "(", "range", "(", "2", ",", "len", "(", "shp_x", ")", ")", ")", "\n", "\n", "", "if", "self", ".", "apply_nonlin", "is", "not", "None", ":", "\n", "            ", "x", "=", "self", ".", "apply_nonlin", "(", "x", ")", "\n", "\n", "", "tp", ",", "fp", ",", "fn", ",", "_", "=", "get_tp_fp_fn_tn", "(", "x", ",", "y", ",", "axes", ",", "loss_mask", ",", "False", ")", "\n", "\n", "nominator", "=", "2", "*", "tp", "+", "self", ".", "smooth", "\n", "denominator", "=", "2", "*", "tp", "+", "fp", "+", "fn", "+", "self", ".", "smooth", "\n", "\n", "dc", "=", "nominator", "/", "(", "denominator", "+", "1e-8", ")", "\n", "\n", "if", "not", "self", ".", "do_bg", ":", "\n", "            ", "if", "self", ".", "batch_dice", ":", "\n", "                ", "dc", "=", "dc", "[", "1", ":", "]", "\n", "", "else", ":", "\n", "                ", "dc", "=", "dc", "[", ":", ",", "1", ":", "]", "\n", "", "", "dc", "=", "dc", ".", "mean", "(", ")", "\n", "\n", "return", "-", "dc", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.loss_functions.dice_loss.MCCLoss.__init__": [[198, 211], ["torch.nn.Module.__init__"], "methods", ["home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.common.layers.ResizeImage.__init__"], ["    ", "def", "__init__", "(", "self", ",", "apply_nonlin", "=", "None", ",", "batch_mcc", "=", "False", ",", "do_bg", "=", "True", ",", "smooth", "=", "0.0", ")", ":", "\n", "        ", "\"\"\"\n        based on matthews correlation coefficient\n        https://en.wikipedia.org/wiki/Matthews_correlation_coefficient\n\n        Does not work. Really unstable. F this.\n        \"\"\"", "\n", "super", "(", "MCCLoss", ",", "self", ")", ".", "__init__", "(", ")", "\n", "\n", "self", ".", "smooth", "=", "smooth", "\n", "self", ".", "do_bg", "=", "do_bg", "\n", "self", ".", "batch_mcc", "=", "batch_mcc", "\n", "self", ".", "apply_nonlin", "=", "apply_nonlin", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.loss_functions.dice_loss.MCCLoss.forward": [[212, 243], ["numpy.prod", "dice_loss.get_tp_fp_fn_tn", "mcc.mean.mean.mean", "list", "dice_loss.MCCLoss.apply_nonlin", "list", "range", "range", "len", "len"], "methods", ["home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.loss_functions.dice_loss.get_tp_fp_fn_tn"], ["", "def", "forward", "(", "self", ",", "x", ",", "y", ",", "loss_mask", "=", "None", ")", ":", "\n", "        ", "shp_x", "=", "x", ".", "shape", "\n", "voxels", "=", "np", ".", "prod", "(", "shp_x", "[", "2", ":", "]", ")", "\n", "\n", "if", "self", ".", "batch_mcc", ":", "\n", "            ", "axes", "=", "[", "0", "]", "+", "list", "(", "range", "(", "2", ",", "len", "(", "shp_x", ")", ")", ")", "\n", "", "else", ":", "\n", "            ", "axes", "=", "list", "(", "range", "(", "2", ",", "len", "(", "shp_x", ")", ")", ")", "\n", "\n", "", "if", "self", ".", "apply_nonlin", "is", "not", "None", ":", "\n", "            ", "x", "=", "self", ".", "apply_nonlin", "(", "x", ")", "\n", "\n", "", "tp", ",", "fp", ",", "fn", ",", "tn", "=", "get_tp_fp_fn_tn", "(", "x", ",", "y", ",", "axes", ",", "loss_mask", ",", "False", ")", "\n", "tp", "/=", "voxels", "\n", "fp", "/=", "voxels", "\n", "fn", "/=", "voxels", "\n", "tn", "/=", "voxels", "\n", "\n", "nominator", "=", "tp", "*", "tn", "-", "fp", "*", "fn", "+", "self", ".", "smooth", "\n", "denominator", "=", "(", "(", "tp", "+", "fp", ")", "*", "(", "tp", "+", "fn", ")", "*", "(", "tn", "+", "fp", ")", "*", "(", "tn", "+", "fn", ")", ")", "**", "0.5", "+", "self", ".", "smooth", "\n", "\n", "mcc", "=", "nominator", "/", "denominator", "\n", "\n", "if", "not", "self", ".", "do_bg", ":", "\n", "            ", "if", "self", ".", "batch_mcc", ":", "\n", "                ", "mcc", "=", "mcc", "[", "1", ":", "]", "\n", "", "else", ":", "\n", "                ", "mcc", "=", "mcc", "[", ":", ",", "1", ":", "]", "\n", "", "", "mcc", "=", "mcc", ".", "mean", "(", ")", "\n", "\n", "return", "-", "mcc", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.loss_functions.dice_loss.SoftDiceLossSquared.__init__": [[246, 256], ["torch.nn.Module.__init__"], "methods", ["home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.common.layers.ResizeImage.__init__"], ["    ", "def", "__init__", "(", "self", ",", "apply_nonlin", "=", "None", ",", "batch_dice", "=", "False", ",", "do_bg", "=", "True", ",", "smooth", "=", "1.", ")", ":", "\n", "        ", "\"\"\"\n        squares the terms in the denominator as proposed by Milletari et al.\n        \"\"\"", "\n", "super", "(", "SoftDiceLossSquared", ",", "self", ")", ".", "__init__", "(", ")", "\n", "\n", "self", ".", "do_bg", "=", "do_bg", "\n", "self", ".", "batch_dice", "=", "batch_dice", "\n", "self", ".", "apply_nonlin", "=", "apply_nonlin", "\n", "self", ".", "smooth", "=", "smooth", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.loss_functions.dice_loss.SoftDiceLossSquared.forward": [[257, 302], ["dc.mean.mean.mean", "list", "dice_loss.SoftDiceLossSquared.apply_nonlin", "torch.no_grad", "all", "nnunet.utilities.tensor_utilities.sum_tensor", "nnunet.utilities.tensor_utilities.sum_tensor", "list", "range", "len", "len", "y.long.long.view", "y.long.long.long", "torch.zeros", "y_onehot.cuda.cuda.scatter_().float", "range", "len", "y_onehot.cuda.cuda.cuda", "len", "zip", "y_onehot.cuda.cuda.scatter_"], "methods", ["home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.utilities.tensor_utilities.sum_tensor", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.utilities.tensor_utilities.sum_tensor"], ["", "def", "forward", "(", "self", ",", "x", ",", "y", ",", "loss_mask", "=", "None", ")", ":", "\n", "        ", "shp_x", "=", "x", ".", "shape", "\n", "shp_y", "=", "y", ".", "shape", "\n", "\n", "if", "self", ".", "batch_dice", ":", "\n", "            ", "axes", "=", "[", "0", "]", "+", "list", "(", "range", "(", "2", ",", "len", "(", "shp_x", ")", ")", ")", "\n", "", "else", ":", "\n", "            ", "axes", "=", "list", "(", "range", "(", "2", ",", "len", "(", "shp_x", ")", ")", ")", "\n", "\n", "", "if", "self", ".", "apply_nonlin", "is", "not", "None", ":", "\n", "            ", "x", "=", "self", ".", "apply_nonlin", "(", "x", ")", "\n", "\n", "", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "            ", "if", "len", "(", "shp_x", ")", "!=", "len", "(", "shp_y", ")", ":", "\n", "                ", "y", "=", "y", ".", "view", "(", "(", "shp_y", "[", "0", "]", ",", "1", ",", "*", "shp_y", "[", "1", ":", "]", ")", ")", "\n", "\n", "", "if", "all", "(", "[", "i", "==", "j", "for", "i", ",", "j", "in", "zip", "(", "x", ".", "shape", ",", "y", ".", "shape", ")", "]", ")", ":", "\n", "# if this is the case then gt is probably already a one hot encoding", "\n", "                ", "y_onehot", "=", "y", "\n", "", "else", ":", "\n", "                ", "y", "=", "y", ".", "long", "(", ")", "\n", "y_onehot", "=", "torch", ".", "zeros", "(", "shp_x", ")", "\n", "if", "x", ".", "device", ".", "type", "==", "\"cuda\"", ":", "\n", "                    ", "y_onehot", "=", "y_onehot", ".", "cuda", "(", "x", ".", "device", ".", "index", ")", "\n", "", "y_onehot", ".", "scatter_", "(", "1", ",", "y", ",", "1", ")", ".", "float", "(", ")", "\n", "\n", "", "", "intersect", "=", "x", "*", "y_onehot", "\n", "# values in the denominator get smoothed", "\n", "denominator", "=", "x", "**", "2", "+", "y_onehot", "**", "2", "\n", "\n", "# aggregation was previously done in get_tp_fp_fn, but needs to be done here now (needs to be done after", "\n", "# squaring)", "\n", "intersect", "=", "sum_tensor", "(", "intersect", ",", "axes", ",", "False", ")", "+", "self", ".", "smooth", "\n", "denominator", "=", "sum_tensor", "(", "denominator", ",", "axes", ",", "False", ")", "+", "self", ".", "smooth", "\n", "\n", "dc", "=", "2", "*", "intersect", "/", "denominator", "\n", "\n", "if", "not", "self", ".", "do_bg", ":", "\n", "            ", "if", "self", ".", "batch_dice", ":", "\n", "                ", "dc", "=", "dc", "[", "1", ":", "]", "\n", "", "else", ":", "\n", "                ", "dc", "=", "dc", "[", ":", ",", "1", ":", "]", "\n", "", "", "dc", "=", "dc", ".", "mean", "(", ")", "\n", "\n", "return", "-", "dc", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.loss_functions.dice_loss.DC_and_CE_loss.__init__": [[305, 332], ["torch.nn.Module.__init__", "nnunet.training.loss_functions.crossentropy.RobustCrossEntropyLoss", "dice_loss.SoftDiceLoss", "dice_loss.SoftDiceLossSquared"], "methods", ["home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.common.layers.ResizeImage.__init__"], ["    ", "def", "__init__", "(", "self", ",", "soft_dice_kwargs", ",", "ce_kwargs", ",", "aggregate", "=", "\"sum\"", ",", "square_dice", "=", "False", ",", "weight_ce", "=", "1", ",", "weight_dice", "=", "1", ",", "\n", "log_dice", "=", "False", ",", "ignore_label", "=", "None", ")", ":", "\n", "        ", "\"\"\"\n        CAREFUL. Weights for CE and Dice do not need to sum to one. You can set whatever you want.\n        :param soft_dice_kwargs:\n        :param ce_kwargs:\n        :param aggregate:\n        :param square_dice:\n        :param weight_ce:\n        :param weight_dice:\n        \"\"\"", "\n", "super", "(", "DC_and_CE_loss", ",", "self", ")", ".", "__init__", "(", ")", "\n", "if", "ignore_label", "is", "not", "None", ":", "\n", "            ", "assert", "not", "square_dice", ",", "'not implemented'", "\n", "ce_kwargs", "[", "'reduction'", "]", "=", "'none'", "\n", "", "self", ".", "log_dice", "=", "log_dice", "\n", "self", ".", "weight_dice", "=", "weight_dice", "\n", "self", ".", "weight_ce", "=", "weight_ce", "\n", "self", ".", "aggregate", "=", "aggregate", "\n", "self", ".", "ce", "=", "RobustCrossEntropyLoss", "(", "**", "ce_kwargs", ")", "\n", "\n", "self", ".", "ignore_label", "=", "ignore_label", "\n", "\n", "if", "not", "square_dice", ":", "\n", "            ", "self", ".", "dc", "=", "SoftDiceLoss", "(", "apply_nonlin", "=", "softmax_helper", ",", "**", "soft_dice_kwargs", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "dc", "=", "SoftDiceLossSquared", "(", "apply_nonlin", "=", "softmax_helper", ",", "**", "soft_dice_kwargs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.loss_functions.dice_loss.DC_and_CE_loss.forward": [[333, 362], ["mask.float.float.float", "dice_loss.DC_and_CE_loss.dc", "dice_loss.DC_and_CE_loss.ce", "NotImplementedError", "torch.log", "target[].long", "ce_loss.sum", "mask.float.float.sum"], "methods", ["None"], ["", "", "def", "forward", "(", "self", ",", "net_output", ",", "target", ")", ":", "\n", "        ", "\"\"\"\n        target must be b, c, x, y(, z) with c=1\n        :param net_output:\n        :param target:\n        :return:\n        \"\"\"", "\n", "if", "self", ".", "ignore_label", "is", "not", "None", ":", "\n", "            ", "assert", "target", ".", "shape", "[", "1", "]", "==", "1", ",", "'not implemented for one hot encoding'", "\n", "mask", "=", "target", "!=", "self", ".", "ignore_label", "\n", "target", "[", "~", "mask", "]", "=", "0", "\n", "mask", "=", "mask", ".", "float", "(", ")", "\n", "", "else", ":", "\n", "            ", "mask", "=", "None", "\n", "\n", "", "dc_loss", "=", "self", ".", "dc", "(", "net_output", ",", "target", ",", "loss_mask", "=", "mask", ")", "if", "self", ".", "weight_dice", "!=", "0", "else", "0", "\n", "if", "self", ".", "log_dice", ":", "\n", "            ", "dc_loss", "=", "-", "torch", ".", "log", "(", "-", "dc_loss", ")", "\n", "\n", "", "ce_loss", "=", "self", ".", "ce", "(", "net_output", ",", "target", "[", ":", ",", "0", "]", ".", "long", "(", ")", ")", "if", "self", ".", "weight_ce", "!=", "0", "else", "0", "\n", "if", "self", ".", "ignore_label", "is", "not", "None", ":", "\n", "            ", "ce_loss", "*=", "mask", "[", ":", ",", "0", "]", "\n", "ce_loss", "=", "ce_loss", ".", "sum", "(", ")", "/", "mask", ".", "sum", "(", ")", "\n", "\n", "", "if", "self", ".", "aggregate", "==", "\"sum\"", ":", "\n", "            ", "result", "=", "self", ".", "weight_ce", "*", "ce_loss", "+", "self", ".", "weight_dice", "*", "dc_loss", "\n", "", "else", ":", "\n", "            ", "raise", "NotImplementedError", "(", "\"nah son\"", ")", "# reserved for other stuff (later)", "\n", "", "return", "result", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.loss_functions.dice_loss.DC_and_BCE_loss.__init__": [[365, 379], ["torch.nn.Module.__init__", "torch.nn.BCEWithLogitsLoss", "dice_loss.SoftDiceLoss"], "methods", ["home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.common.layers.ResizeImage.__init__"], ["    ", "def", "__init__", "(", "self", ",", "bce_kwargs", ",", "soft_dice_kwargs", ",", "aggregate", "=", "\"sum\"", ")", ":", "\n", "        ", "\"\"\"\n        DO NOT APPLY NONLINEARITY IN YOUR NETWORK!\n\n        THIS LOSS IS INTENDED TO BE USED FOR BRATS REGIONS ONLY\n        :param soft_dice_kwargs:\n        :param bce_kwargs:\n        :param aggregate:\n        \"\"\"", "\n", "super", "(", "DC_and_BCE_loss", ",", "self", ")", ".", "__init__", "(", ")", "\n", "\n", "self", ".", "aggregate", "=", "aggregate", "\n", "self", ".", "ce", "=", "nn", ".", "BCEWithLogitsLoss", "(", "**", "bce_kwargs", ")", "\n", "self", ".", "dc", "=", "SoftDiceLoss", "(", "apply_nonlin", "=", "torch", ".", "sigmoid", ",", "**", "soft_dice_kwargs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.loss_functions.dice_loss.DC_and_BCE_loss.forward": [[380, 390], ["dice_loss.DC_and_BCE_loss.ce", "dice_loss.DC_and_BCE_loss.dc", "NotImplementedError"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "net_output", ",", "target", ")", ":", "\n", "        ", "ce_loss", "=", "self", ".", "ce", "(", "net_output", ",", "target", ")", "\n", "dc_loss", "=", "self", ".", "dc", "(", "net_output", ",", "target", ")", "\n", "\n", "if", "self", ".", "aggregate", "==", "\"sum\"", ":", "\n", "            ", "result", "=", "ce_loss", "+", "dc_loss", "\n", "", "else", ":", "\n", "            ", "raise", "NotImplementedError", "(", "\"nah son\"", ")", "# reserved for other stuff (later)", "\n", "\n", "", "return", "result", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.loss_functions.dice_loss.GDL_and_CE_loss.__init__": [[393, 398], ["torch.nn.Module.__init__", "nnunet.training.loss_functions.crossentropy.RobustCrossEntropyLoss", "dice_loss.GDL"], "methods", ["home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.common.layers.ResizeImage.__init__"], ["    ", "def", "__init__", "(", "self", ",", "gdl_dice_kwargs", ",", "ce_kwargs", ",", "aggregate", "=", "\"sum\"", ")", ":", "\n", "        ", "super", "(", "GDL_and_CE_loss", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "aggregate", "=", "aggregate", "\n", "self", ".", "ce", "=", "RobustCrossEntropyLoss", "(", "**", "ce_kwargs", ")", "\n", "self", ".", "dc", "=", "GDL", "(", "softmax_helper", ",", "**", "gdl_dice_kwargs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.loss_functions.dice_loss.GDL_and_CE_loss.forward": [[399, 407], ["dice_loss.GDL_and_CE_loss.dc", "dice_loss.GDL_and_CE_loss.ce", "NotImplementedError"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "net_output", ",", "target", ")", ":", "\n", "        ", "dc_loss", "=", "self", ".", "dc", "(", "net_output", ",", "target", ")", "\n", "ce_loss", "=", "self", ".", "ce", "(", "net_output", ",", "target", ")", "\n", "if", "self", ".", "aggregate", "==", "\"sum\"", ":", "\n", "            ", "result", "=", "ce_loss", "+", "dc_loss", "\n", "", "else", ":", "\n", "            ", "raise", "NotImplementedError", "(", "\"nah son\"", ")", "# reserved for other stuff (later)", "\n", "", "return", "result", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.loss_functions.dice_loss.DC_and_topk_loss.__init__": [[410, 418], ["torch.nn.Module.__init__", "nnunet.training.loss_functions.TopK_loss.TopKLoss", "dice_loss.SoftDiceLoss", "dice_loss.SoftDiceLossSquared"], "methods", ["home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.common.layers.ResizeImage.__init__"], ["    ", "def", "__init__", "(", "self", ",", "soft_dice_kwargs", ",", "ce_kwargs", ",", "aggregate", "=", "\"sum\"", ",", "square_dice", "=", "False", ")", ":", "\n", "        ", "super", "(", "DC_and_topk_loss", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "aggregate", "=", "aggregate", "\n", "self", ".", "ce", "=", "TopKLoss", "(", "**", "ce_kwargs", ")", "\n", "if", "not", "square_dice", ":", "\n", "            ", "self", ".", "dc", "=", "SoftDiceLoss", "(", "apply_nonlin", "=", "softmax_helper", ",", "**", "soft_dice_kwargs", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "dc", "=", "SoftDiceLossSquared", "(", "apply_nonlin", "=", "softmax_helper", ",", "**", "soft_dice_kwargs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.loss_functions.dice_loss.DC_and_topk_loss.forward": [[419, 427], ["dice_loss.DC_and_topk_loss.dc", "dice_loss.DC_and_topk_loss.ce", "NotImplementedError"], "methods", ["None"], ["", "", "def", "forward", "(", "self", ",", "net_output", ",", "target", ")", ":", "\n", "        ", "dc_loss", "=", "self", ".", "dc", "(", "net_output", ",", "target", ")", "\n", "ce_loss", "=", "self", ".", "ce", "(", "net_output", ",", "target", ")", "\n", "if", "self", ".", "aggregate", "==", "\"sum\"", ":", "\n", "            ", "result", "=", "ce_loss", "+", "dc_loss", "\n", "", "else", ":", "\n", "            ", "raise", "NotImplementedError", "(", "\"nah son\"", ")", "# reserved for other stuff (later?)", "\n", "", "return", "result", "\n", "", "", ""]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.loss_functions.dice_loss.get_tp_fp_fn_tn": [[100, 156], ["tuple", "torch.no_grad", "all", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "len", "nnunet.utilities.tensor_utilities.sum_tensor", "nnunet.utilities.tensor_utilities.sum_tensor", "nnunet.utilities.tensor_utilities.sum_tensor", "nnunet.utilities.tensor_utilities.sum_tensor", "range", "len", "len", "gt.long.view", "gt.long.long", "torch.zeros", "y_onehot.cuda.scatter_", "tuple", "tuple", "tuple", "tuple", "len", "y_onehot.cuda.cuda", "net_output.size", "zip", "torch.unbind", "torch.unbind", "torch.unbind", "torch.unbind"], "function", ["home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.utilities.tensor_utilities.sum_tensor", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.utilities.tensor_utilities.sum_tensor", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.utilities.tensor_utilities.sum_tensor", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.utilities.tensor_utilities.sum_tensor"], ["", "", "def", "get_tp_fp_fn_tn", "(", "net_output", ",", "gt", ",", "axes", "=", "None", ",", "mask", "=", "None", ",", "square", "=", "False", ")", ":", "\n", "    ", "\"\"\"\n    net_output must be (b, c, x, y(, z)))\n    gt must be a label map (shape (b, 1, x, y(, z)) OR shape (b, x, y(, z))) or one hot encoding (b, c, x, y(, z))\n    if mask is provided it must have shape (b, 1, x, y(, z)))\n    :param net_output:\n    :param gt:\n    :param axes: can be (, ) = no summation\n    :param mask: mask must be 1 for valid pixels and 0 for invalid pixels\n    :param square: if True then fp, tp and fn will be squared before summation\n    :return:\n    \"\"\"", "\n", "if", "axes", "is", "None", ":", "\n", "        ", "axes", "=", "tuple", "(", "range", "(", "2", ",", "len", "(", "net_output", ".", "size", "(", ")", ")", ")", ")", "\n", "\n", "", "shp_x", "=", "net_output", ".", "shape", "\n", "shp_y", "=", "gt", ".", "shape", "\n", "\n", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "        ", "if", "len", "(", "shp_x", ")", "!=", "len", "(", "shp_y", ")", ":", "\n", "            ", "gt", "=", "gt", ".", "view", "(", "(", "shp_y", "[", "0", "]", ",", "1", ",", "*", "shp_y", "[", "1", ":", "]", ")", ")", "\n", "\n", "", "if", "all", "(", "[", "i", "==", "j", "for", "i", ",", "j", "in", "zip", "(", "net_output", ".", "shape", ",", "gt", ".", "shape", ")", "]", ")", ":", "\n", "# if this is the case then gt is probably already a one hot encoding", "\n", "            ", "y_onehot", "=", "gt", "\n", "", "else", ":", "\n", "            ", "gt", "=", "gt", ".", "long", "(", ")", "\n", "y_onehot", "=", "torch", ".", "zeros", "(", "shp_x", ")", "\n", "if", "net_output", ".", "device", ".", "type", "==", "\"cuda\"", ":", "\n", "                ", "y_onehot", "=", "y_onehot", ".", "cuda", "(", "net_output", ".", "device", ".", "index", ")", "\n", "", "y_onehot", ".", "scatter_", "(", "1", ",", "gt", ",", "1", ")", "\n", "\n", "", "", "tp", "=", "net_output", "*", "y_onehot", "\n", "fp", "=", "net_output", "*", "(", "1", "-", "y_onehot", ")", "\n", "fn", "=", "(", "1", "-", "net_output", ")", "*", "y_onehot", "\n", "tn", "=", "(", "1", "-", "net_output", ")", "*", "(", "1", "-", "y_onehot", ")", "\n", "\n", "if", "mask", "is", "not", "None", ":", "\n", "        ", "tp", "=", "torch", ".", "stack", "(", "tuple", "(", "x_i", "*", "mask", "[", ":", ",", "0", "]", "for", "x_i", "in", "torch", ".", "unbind", "(", "tp", ",", "dim", "=", "1", ")", ")", ",", "dim", "=", "1", ")", "\n", "fp", "=", "torch", ".", "stack", "(", "tuple", "(", "x_i", "*", "mask", "[", ":", ",", "0", "]", "for", "x_i", "in", "torch", ".", "unbind", "(", "fp", ",", "dim", "=", "1", ")", ")", ",", "dim", "=", "1", ")", "\n", "fn", "=", "torch", ".", "stack", "(", "tuple", "(", "x_i", "*", "mask", "[", ":", ",", "0", "]", "for", "x_i", "in", "torch", ".", "unbind", "(", "fn", ",", "dim", "=", "1", ")", ")", ",", "dim", "=", "1", ")", "\n", "tn", "=", "torch", ".", "stack", "(", "tuple", "(", "x_i", "*", "mask", "[", ":", ",", "0", "]", "for", "x_i", "in", "torch", ".", "unbind", "(", "tn", ",", "dim", "=", "1", ")", ")", ",", "dim", "=", "1", ")", "\n", "\n", "", "if", "square", ":", "\n", "        ", "tp", "=", "tp", "**", "2", "\n", "fp", "=", "fp", "**", "2", "\n", "fn", "=", "fn", "**", "2", "\n", "tn", "=", "tn", "**", "2", "\n", "\n", "", "if", "len", "(", "axes", ")", ">", "0", ":", "\n", "        ", "tp", "=", "sum_tensor", "(", "tp", ",", "axes", ",", "keepdim", "=", "False", ")", "\n", "fp", "=", "sum_tensor", "(", "fp", ",", "axes", ",", "keepdim", "=", "False", ")", "\n", "fn", "=", "sum_tensor", "(", "fn", ",", "axes", ",", "keepdim", "=", "False", ")", "\n", "tn", "=", "sum_tensor", "(", "tn", ",", "axes", ",", "keepdim", "=", "False", ")", "\n", "\n", "", "return", "tp", ",", "fp", ",", "fn", ",", "tn", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.loss_functions.crossentropy.RobustCrossEntropyLoss.forward": [[8, 13], ["super().forward", "len", "len", "target.long"], "methods", ["home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.utilities.distributed.awesome_allgather_function.forward"], ["def", "forward", "(", "self", ",", "input", ":", "Tensor", ",", "target", ":", "Tensor", ")", "->", "Tensor", ":", "\n", "        ", "if", "len", "(", "target", ".", "shape", ")", "==", "len", "(", "input", ".", "shape", ")", ":", "\n", "            ", "assert", "target", ".", "shape", "[", "1", "]", "==", "1", "\n", "target", "=", "target", "[", ":", ",", "0", "]", "\n", "", "return", "super", "(", ")", ".", "forward", "(", "input", ",", "target", ".", "long", "(", ")", ")", "", "", "", ""]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.learning_rate.poly_lr.poly_lr": [[16, 18], ["None"], "function", ["None"], ["def", "poly_lr", "(", "epoch", ",", "max_epochs", ",", "initial_lr", ",", "exponent", "=", "0.9", ")", ":", "\n", "    ", "return", "initial_lr", "*", "(", "1", "-", "epoch", "/", "max_epochs", ")", "**", "exponent", "\n", "", ""]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.optimizer.ranger.Ranger.__init__": [[13, 50], ["dict", "torch.optim.optimizer.Optimizer.__init__", "ValueError", "ValueError", "ValueError", "ValueError", "range"], "methods", ["home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.common.layers.ResizeImage.__init__"], ["    ", "def", "__init__", "(", "self", ",", "params", ",", "lr", "=", "1e-3", ",", "alpha", "=", "0.5", ",", "k", "=", "6", ",", "N_sma_threshhold", "=", "5", ",", "betas", "=", "(", ".95", ",", "0.999", ")", ",", "eps", "=", "1e-5", ",", "\n", "weight_decay", "=", "0", ")", ":", "\n", "# parameter checks", "\n", "        ", "if", "not", "0.0", "<=", "alpha", "<=", "1.0", ":", "\n", "            ", "raise", "ValueError", "(", "f'Invalid slow update rate: {alpha}'", ")", "\n", "", "if", "not", "1", "<=", "k", ":", "\n", "            ", "raise", "ValueError", "(", "f'Invalid lookahead steps: {k}'", ")", "\n", "", "if", "not", "lr", ">", "0", ":", "\n", "            ", "raise", "ValueError", "(", "f'Invalid Learning Rate: {lr}'", ")", "\n", "", "if", "not", "eps", ">", "0", ":", "\n", "            ", "raise", "ValueError", "(", "f'Invalid eps: {eps}'", ")", "\n", "\n", "# parameter comments:", "\n", "# beta1 (momentum) of .95 seems to work better than .90...", "\n", "# N_sma_threshold of 5 seems better in testing than 4.", "\n", "# In both cases, worth testing on your dataset (.90 vs .95, 4 vs 5) to make sure which works best for you.", "\n", "\n", "# prep defaults and init torch.optim base", "\n", "", "defaults", "=", "dict", "(", "lr", "=", "lr", ",", "alpha", "=", "alpha", ",", "k", "=", "k", ",", "step_counter", "=", "0", ",", "betas", "=", "betas", ",", "N_sma_threshhold", "=", "N_sma_threshhold", ",", "\n", "eps", "=", "eps", ",", "weight_decay", "=", "weight_decay", ")", "\n", "super", "(", ")", ".", "__init__", "(", "params", ",", "defaults", ")", "\n", "\n", "# adjustable threshold", "\n", "self", ".", "N_sma_threshhold", "=", "N_sma_threshhold", "\n", "\n", "# now we can get to work...", "\n", "# removed as we now use step from RAdam...no need for duplicate step counting", "\n", "# for group in self.param_groups:", "\n", "#    group[\"step_counter\"] = 0", "\n", "# print(\"group step counter init\")", "\n", "\n", "# look ahead params", "\n", "self", ".", "alpha", "=", "alpha", "\n", "self", ".", "k", "=", "k", "\n", "\n", "# radam buffer for state", "\n", "self", ".", "radam_buffer", "=", "[", "[", "None", ",", "None", ",", "None", "]", "for", "ind", "in", "range", "(", "10", ")", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.optimizer.ranger.Ranger.__setstate__": [[64, 67], ["print", "super().__setstate__"], "methods", ["home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.optimizer.ranger.Ranger.__setstate__"], ["", "def", "__setstate__", "(", "self", ",", "state", ")", ":", "\n", "        ", "print", "(", "\"set state called\"", ")", "\n", "super", "(", "Ranger", ",", "self", ")", ".", "__setstate__", "(", "state", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.optimizer.ranger.Ranger.step": [[68, 153], ["p.grad.data.float", "p.data.float", "exp_avg_sq.mul_().addcmul_", "exp_avg.mul_().add_", "p.data.copy_", "RuntimeError", "len", "torch.zeros_like", "torch.zeros_like", "torch.empty_like", "state[].copy_", "state[].type_as", "state[].type_as", "p.data.float.add_", "exp_avg_sq.sqrt().add_", "p.data.float.addcdiv_", "p.data.float.add_", "slow_p.add_", "p.data.copy_", "exp_avg_sq.mul_", "exp_avg.mul_", "int", "math.sqrt", "exp_avg_sq.sqrt"], "methods", ["None"], ["", "def", "step", "(", "self", ",", "closure", "=", "None", ")", ":", "\n", "        ", "loss", "=", "None", "\n", "# note - below is commented out b/c I have other work that passes back the loss as a float, and thus not a callable closure.", "\n", "# Uncomment if you need to use the actual closure...", "\n", "\n", "# if closure is not None:", "\n", "# loss = closure()", "\n", "\n", "# Evaluate averages and grad, update param tensors", "\n", "for", "group", "in", "self", ".", "param_groups", ":", "\n", "\n", "            ", "for", "p", "in", "group", "[", "'params'", "]", ":", "\n", "                ", "if", "p", ".", "grad", "is", "None", ":", "\n", "                    ", "continue", "\n", "", "grad", "=", "p", ".", "grad", ".", "data", ".", "float", "(", ")", "\n", "if", "grad", ".", "is_sparse", ":", "\n", "                    ", "raise", "RuntimeError", "(", "'Ranger optimizer does not support sparse gradients'", ")", "\n", "\n", "", "p_data_fp32", "=", "p", ".", "data", ".", "float", "(", ")", "\n", "\n", "state", "=", "self", ".", "state", "[", "p", "]", "# get state dict for this param", "\n", "\n", "if", "len", "(", "state", ")", "==", "0", ":", "# if first time to run...init dictionary with our desired entries", "\n", "# if self.first_run_check==0:", "\n", "# self.first_run_check=1", "\n", "# print(\"Initializing slow buffer...should not see this at load from saved model!\")", "\n", "                    ", "state", "[", "'step'", "]", "=", "0", "\n", "state", "[", "'exp_avg'", "]", "=", "torch", ".", "zeros_like", "(", "p_data_fp32", ")", "\n", "state", "[", "'exp_avg_sq'", "]", "=", "torch", ".", "zeros_like", "(", "p_data_fp32", ")", "\n", "\n", "# look ahead weight storage now in state dict", "\n", "state", "[", "'slow_buffer'", "]", "=", "torch", ".", "empty_like", "(", "p", ".", "data", ")", "\n", "state", "[", "'slow_buffer'", "]", ".", "copy_", "(", "p", ".", "data", ")", "\n", "\n", "", "else", ":", "\n", "                    ", "state", "[", "'exp_avg'", "]", "=", "state", "[", "'exp_avg'", "]", ".", "type_as", "(", "p_data_fp32", ")", "\n", "state", "[", "'exp_avg_sq'", "]", "=", "state", "[", "'exp_avg_sq'", "]", ".", "type_as", "(", "p_data_fp32", ")", "\n", "\n", "# begin computations", "\n", "", "exp_avg", ",", "exp_avg_sq", "=", "state", "[", "'exp_avg'", "]", ",", "state", "[", "'exp_avg_sq'", "]", "\n", "beta1", ",", "beta2", "=", "group", "[", "'betas'", "]", "\n", "\n", "# compute variance mov avg", "\n", "exp_avg_sq", ".", "mul_", "(", "beta2", ")", ".", "addcmul_", "(", "1", "-", "beta2", ",", "grad", ",", "grad", ")", "\n", "# compute mean moving avg", "\n", "exp_avg", ".", "mul_", "(", "beta1", ")", ".", "add_", "(", "1", "-", "beta1", ",", "grad", ")", "\n", "\n", "state", "[", "'step'", "]", "+=", "1", "\n", "\n", "buffered", "=", "self", ".", "radam_buffer", "[", "int", "(", "state", "[", "'step'", "]", "%", "10", ")", "]", "\n", "if", "state", "[", "'step'", "]", "==", "buffered", "[", "0", "]", ":", "\n", "                    ", "N_sma", ",", "step_size", "=", "buffered", "[", "1", "]", ",", "buffered", "[", "2", "]", "\n", "", "else", ":", "\n", "                    ", "buffered", "[", "0", "]", "=", "state", "[", "'step'", "]", "\n", "beta2_t", "=", "beta2", "**", "state", "[", "'step'", "]", "\n", "N_sma_max", "=", "2", "/", "(", "1", "-", "beta2", ")", "-", "1", "\n", "N_sma", "=", "N_sma_max", "-", "2", "*", "state", "[", "'step'", "]", "*", "beta2_t", "/", "(", "1", "-", "beta2_t", ")", "\n", "buffered", "[", "1", "]", "=", "N_sma", "\n", "if", "N_sma", ">", "self", ".", "N_sma_threshhold", ":", "\n", "                        ", "step_size", "=", "math", ".", "sqrt", "(", "\n", "(", "1", "-", "beta2_t", ")", "*", "(", "N_sma", "-", "4", ")", "/", "(", "N_sma_max", "-", "4", ")", "*", "(", "N_sma", "-", "2", ")", "/", "N_sma", "*", "N_sma_max", "/", "(", "\n", "N_sma_max", "-", "2", ")", ")", "/", "(", "1", "-", "beta1", "**", "state", "[", "'step'", "]", ")", "\n", "", "else", ":", "\n", "                        ", "step_size", "=", "1.0", "/", "(", "1", "-", "beta1", "**", "state", "[", "'step'", "]", ")", "\n", "", "buffered", "[", "2", "]", "=", "step_size", "\n", "\n", "", "if", "group", "[", "'weight_decay'", "]", "!=", "0", ":", "\n", "                    ", "p_data_fp32", ".", "add_", "(", "-", "group", "[", "'weight_decay'", "]", "*", "group", "[", "'lr'", "]", ",", "p_data_fp32", ")", "\n", "\n", "", "if", "N_sma", ">", "self", ".", "N_sma_threshhold", ":", "\n", "                    ", "denom", "=", "exp_avg_sq", ".", "sqrt", "(", ")", ".", "add_", "(", "group", "[", "'eps'", "]", ")", "\n", "p_data_fp32", ".", "addcdiv_", "(", "-", "step_size", "*", "group", "[", "'lr'", "]", ",", "exp_avg", ",", "denom", ")", "\n", "", "else", ":", "\n", "                    ", "p_data_fp32", ".", "add_", "(", "-", "step_size", "*", "group", "[", "'lr'", "]", ",", "exp_avg", ")", "\n", "\n", "", "p", ".", "data", ".", "copy_", "(", "p_data_fp32", ")", "\n", "\n", "# integrated look ahead...", "\n", "# we do it at the param level instead of group level", "\n", "if", "state", "[", "'step'", "]", "%", "group", "[", "'k'", "]", "==", "0", ":", "\n", "                    ", "slow_p", "=", "state", "[", "'slow_buffer'", "]", "# get access to slow param tensor", "\n", "slow_p", ".", "add_", "(", "self", ".", "alpha", ",", "p", ".", "data", "-", "slow_p", ")", "# (fast weights - slow weights) * alpha", "\n", "p", ".", "data", ".", "copy_", "(", "slow_p", ")", "# copy interpolated weights to RAdam param tensor", "\n", "\n", "", "", "", "return", "loss", "\n", "", "", ""]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.dataloading.dataset_loading.DataLoader3D.__init__": [[156, 203], ["batchgenerators.dataloading.SlimDataLoaderBase.__init__", "list", "dataset_loading.DataLoader3D.determine_shapes", "collections.OrderedDict", "dataset_loading.DataLoader3D._data.keys", "isinstance", "numpy.array", "numpy.array", "numpy.array"], "methods", ["home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.common.layers.ResizeImage.__init__", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.dataloading.dataset_loading.DataLoader2D.determine_shapes"], ["    ", "def", "__init__", "(", "self", ",", "data", ",", "patch_size", ",", "final_patch_size", ",", "batch_size", ",", "has_prev_stage", "=", "False", ",", "\n", "oversample_foreground_percent", "=", "0.0", ",", "memmap_mode", "=", "\"r\"", ",", "pad_mode", "=", "\"edge\"", ",", "pad_kwargs_data", "=", "None", ",", "\n", "pad_sides", "=", "None", ")", ":", "\n", "        ", "\"\"\"\n        This is the basic data loader for 3D networks. It uses preprocessed data as produced by my (Fabian) preprocessing.\n        You can load the data with load_dataset(folder) where folder is the folder where the npz files are located. If there\n        are only npz files present in that folder, the data loader will unpack them on the fly. This may take a while\n        and increase CPU usage. Therefore, I advise you to call unpack_dataset(folder) first, which will unpack all npz\n        to npy. Don't forget to call delete_npy(folder) after you are done with training?\n        Why all the hassle? Well the decathlon dataset is huge. Using npy for everything will consume >1 TB and that is uncool\n        given that I (Fabian) will have to store that permanently on /datasets and my local computer. With this strategy all\n        data is stored in a compressed format (factor 10 smaller) and only unpacked when needed.\n        :param data: get this with load_dataset(folder, stage=0). Plug the return value in here and you are g2g (good to go)\n        :param patch_size: what patch size will this data loader return? it is common practice to first load larger\n        patches so that a central crop after data augmentation can be done to reduce border artifacts. If unsure, use\n        get_patch_size() from data_augmentation.default_data_augmentation\n        :param final_patch_size: what will the patch finally be cropped to (after data augmentation)? this is the patch\n        size that goes into your network. We need this here because we will pad patients in here so that patches at the\n        border of patients are sampled properly\n        :param batch_size:\n        :param num_batches: how many batches will the data loader produce before stopping? None=endless\n        :param seed:\n        :param stage: ignore this (Fabian only)\n        :param random: Sample keys randomly; CAREFUL! non-random sampling requires batch_size=1, otherwise you will iterate batch_size times over the dataset\n        :param oversample_foreground: half the batch will be forced to contain at least some foreground (equal prob for each of the foreground classes)\n        \"\"\"", "\n", "super", "(", "DataLoader3D", ",", "self", ")", ".", "__init__", "(", "data", ",", "batch_size", ",", "None", ")", "\n", "if", "pad_kwargs_data", "is", "None", ":", "\n", "            ", "pad_kwargs_data", "=", "OrderedDict", "(", ")", "\n", "", "self", ".", "pad_kwargs_data", "=", "pad_kwargs_data", "\n", "self", ".", "pad_mode", "=", "pad_mode", "\n", "self", ".", "oversample_foreground_percent", "=", "oversample_foreground_percent", "\n", "self", ".", "final_patch_size", "=", "final_patch_size", "\n", "self", ".", "has_prev_stage", "=", "has_prev_stage", "\n", "self", ".", "patch_size", "=", "patch_size", "\n", "self", ".", "list_of_keys", "=", "list", "(", "self", ".", "_data", ".", "keys", "(", ")", ")", "\n", "# need_to_pad denotes by how much we need to pad the data so that if we sample a patch of size final_patch_size", "\n", "# (which is what the network will get) these patches will also cover the border of the patients", "\n", "self", ".", "need_to_pad", "=", "(", "np", ".", "array", "(", "patch_size", ")", "-", "np", ".", "array", "(", "final_patch_size", ")", ")", ".", "astype", "(", "int", ")", "\n", "if", "pad_sides", "is", "not", "None", ":", "\n", "            ", "if", "not", "isinstance", "(", "pad_sides", ",", "np", ".", "ndarray", ")", ":", "\n", "                ", "pad_sides", "=", "np", ".", "array", "(", "pad_sides", ")", "\n", "", "self", ".", "need_to_pad", "+=", "pad_sides", "\n", "", "self", ".", "memmap_mode", "=", "memmap_mode", "\n", "self", ".", "num_channels", "=", "None", "\n", "self", ".", "pad_sides", "=", "pad_sides", "\n", "self", ".", "data_shape", ",", "self", ".", "seg_shape", "=", "self", ".", "determine_shapes", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.dataloading.dataset_loading.DataLoader3D.get_do_oversample": [[204, 206], ["round"], "methods", ["None"], ["", "def", "get_do_oversample", "(", "self", ",", "batch_idx", ")", ":", "\n", "        ", "return", "not", "batch_idx", "<", "round", "(", "self", ".", "batch_size", "*", "(", "1", "-", "self", ".", "oversample_foreground_percent", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.dataloading.dataset_loading.DataLoader3D.determine_shapes": [[207, 222], ["isfile", "list", "numpy.load", "dataset_loading.DataLoader3D._data.keys", "numpy.load"], "methods", ["None"], ["", "def", "determine_shapes", "(", "self", ")", ":", "\n", "        ", "if", "self", ".", "has_prev_stage", ":", "\n", "            ", "num_seg", "=", "2", "\n", "", "else", ":", "\n", "            ", "num_seg", "=", "1", "\n", "\n", "", "k", "=", "list", "(", "self", ".", "_data", ".", "keys", "(", ")", ")", "[", "0", "]", "\n", "if", "isfile", "(", "self", ".", "_data", "[", "k", "]", "[", "'data_file'", "]", "[", ":", "-", "4", "]", "+", "\".npy\"", ")", ":", "\n", "            ", "case_all_data", "=", "np", ".", "load", "(", "self", ".", "_data", "[", "k", "]", "[", "'data_file'", "]", "[", ":", "-", "4", "]", "+", "\".npy\"", ",", "self", ".", "memmap_mode", ")", "\n", "", "else", ":", "\n", "            ", "case_all_data", "=", "np", ".", "load", "(", "self", ".", "_data", "[", "k", "]", "[", "'data_file'", "]", ")", "[", "'data'", "]", "\n", "", "num_color_channels", "=", "case_all_data", ".", "shape", "[", "0", "]", "-", "1", "\n", "data_shape", "=", "(", "self", ".", "batch_size", ",", "num_color_channels", ",", "*", "self", ".", "patch_size", ")", "\n", "seg_shape", "=", "(", "self", ".", "batch_size", ",", "num_seg", ",", "*", "self", ".", "patch_size", ")", "\n", "return", "data_shape", ",", "seg_shape", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.dataloading.dataset_loading.DataLoader3D.generate_train_batch": [[223, 380], ["numpy.random.choice", "numpy.zeros", "numpy.zeros", "enumerate", "dataset_loading.DataLoader3D.get_do_oversample", "case_properties.append", "isfile", "range", "max", "min", "max", "min", "max", "min", "numpy.copy", "numpy.pad", "numpy.pad", "dataset_loading.DataLoader3D._data[].keys", "load_pickle", "numpy.load", "isfile", "numpy.random.choice", "all", "numpy.random.randint", "numpy.random.randint", "numpy.random.randint", "numpy.array", "numpy.pad", "numpy.load", "load_pickle.keys", "RuntimeError", "len", "print", "numpy.random.choice", "max", "max", "max", "numpy.random.randint", "numpy.random.randint", "numpy.random.randint", "numpy.load", "str", "str", "max", "max", "max", "max", "max", "max", "numpy.load", "zip", "properties[].keys", "numpy.random.choice", "min", "min", "min", "min", "min", "min", "max", "max", "max", "len", "len", "min", "min", "min"], "methods", ["home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.dataloading.dataset_loading.DataLoader2D.get_do_oversample"], ["", "def", "generate_train_batch", "(", "self", ")", ":", "\n", "        ", "selected_keys", "=", "np", ".", "random", ".", "choice", "(", "self", ".", "list_of_keys", ",", "self", ".", "batch_size", ",", "True", ",", "None", ")", "\n", "data", "=", "np", ".", "zeros", "(", "self", ".", "data_shape", ",", "dtype", "=", "np", ".", "float32", ")", "\n", "seg", "=", "np", ".", "zeros", "(", "self", ".", "seg_shape", ",", "dtype", "=", "np", ".", "float32", ")", "\n", "case_properties", "=", "[", "]", "\n", "for", "j", ",", "i", "in", "enumerate", "(", "selected_keys", ")", ":", "\n", "# oversampling foreground will improve stability of model training, especially if many patches are empty", "\n", "# (Lung for example)", "\n", "            ", "if", "self", ".", "get_do_oversample", "(", "j", ")", ":", "\n", "                ", "force_fg", "=", "True", "\n", "", "else", ":", "\n", "                ", "force_fg", "=", "False", "\n", "\n", "", "if", "'properties'", "in", "self", ".", "_data", "[", "i", "]", ".", "keys", "(", ")", ":", "\n", "                ", "properties", "=", "self", ".", "_data", "[", "i", "]", "[", "'properties'", "]", "\n", "", "else", ":", "\n", "                ", "properties", "=", "load_pickle", "(", "self", ".", "_data", "[", "i", "]", "[", "'properties_file'", "]", ")", "\n", "", "case_properties", ".", "append", "(", "properties", ")", "\n", "\n", "# cases are stored as npz, but we require unpack_dataset to be run. This will decompress them into npy", "\n", "# which is much faster to access", "\n", "if", "isfile", "(", "self", ".", "_data", "[", "i", "]", "[", "'data_file'", "]", "[", ":", "-", "4", "]", "+", "\".npy\"", ")", ":", "\n", "                ", "case_all_data", "=", "np", ".", "load", "(", "self", ".", "_data", "[", "i", "]", "[", "'data_file'", "]", "[", ":", "-", "4", "]", "+", "\".npy\"", ",", "self", ".", "memmap_mode", ")", "\n", "", "else", ":", "\n", "                ", "case_all_data", "=", "np", ".", "load", "(", "self", ".", "_data", "[", "i", "]", "[", "'data_file'", "]", ")", "[", "'data'", "]", "\n", "\n", "# If we are doing the cascade then we will also need to load the segmentation of the previous stage and", "\n", "# concatenate it. Here it will be concatenates to the segmentation because the augmentations need to be", "\n", "# applied to it in segmentation mode. Later in the data augmentation we move it from the segmentations to", "\n", "# the last channel of the data", "\n", "", "if", "self", ".", "has_prev_stage", ":", "\n", "                ", "if", "isfile", "(", "self", ".", "_data", "[", "i", "]", "[", "'seg_from_prev_stage_file'", "]", "[", ":", "-", "4", "]", "+", "\".npy\"", ")", ":", "\n", "                    ", "segs_from_previous_stage", "=", "np", ".", "load", "(", "self", ".", "_data", "[", "i", "]", "[", "'seg_from_prev_stage_file'", "]", "[", ":", "-", "4", "]", "+", "\".npy\"", ",", "\n", "mmap_mode", "=", "self", ".", "memmap_mode", ")", "[", "None", "]", "\n", "", "else", ":", "\n", "                    ", "segs_from_previous_stage", "=", "np", ".", "load", "(", "self", ".", "_data", "[", "i", "]", "[", "'seg_from_prev_stage_file'", "]", ")", "[", "'data'", "]", "[", "None", "]", "\n", "# we theoretically support several possible previsous segmentations from which only one is sampled. But", "\n", "# in practice this feature was never used so it's always only one segmentation", "\n", "", "seg_key", "=", "np", ".", "random", ".", "choice", "(", "segs_from_previous_stage", ".", "shape", "[", "0", "]", ")", "\n", "seg_from_previous_stage", "=", "segs_from_previous_stage", "[", "seg_key", ":", "seg_key", "+", "1", "]", "\n", "assert", "all", "(", "[", "i", "==", "j", "for", "i", ",", "j", "in", "zip", "(", "seg_from_previous_stage", ".", "shape", "[", "1", ":", "]", ",", "case_all_data", ".", "shape", "[", "1", ":", "]", ")", "]", ")", ",", "\"seg_from_previous_stage does not match the shape of case_all_data: %s vs %s\"", "%", "(", "str", "(", "seg_from_previous_stage", ".", "shape", "[", "1", ":", "]", ")", ",", "str", "(", "case_all_data", ".", "shape", "[", "1", ":", "]", ")", ")", "\n", "", "else", ":", "\n", "                ", "seg_from_previous_stage", "=", "None", "\n", "\n", "# do you trust me? You better do. Otherwise you'll have to go through this mess and honestly there are", "\n", "# better things you could do right now", "\n", "\n", "# (above) documentation of the day. Nice. Even myself coming back 1 months later I have not friggin idea", "\n", "# what's going on. I keep the above documentation just for fun but attempt to make things clearer now", "\n", "\n", "", "need_to_pad", "=", "self", ".", "need_to_pad", "\n", "for", "d", "in", "range", "(", "3", ")", ":", "\n", "# if case_all_data.shape + need_to_pad is still < patch size we need to pad more! We pad on both sides", "\n", "# always", "\n", "                ", "if", "need_to_pad", "[", "d", "]", "+", "case_all_data", ".", "shape", "[", "d", "+", "1", "]", "<", "self", ".", "patch_size", "[", "d", "]", ":", "\n", "                    ", "need_to_pad", "[", "d", "]", "=", "self", ".", "patch_size", "[", "d", "]", "-", "case_all_data", ".", "shape", "[", "d", "+", "1", "]", "\n", "\n", "# we can now choose the bbox from -need_to_pad // 2 to shape - patch_size + need_to_pad // 2. Here we", "\n", "# define what the upper and lower bound can be to then sample form them with np.random.randint", "\n", "", "", "shape", "=", "case_all_data", ".", "shape", "[", "1", ":", "]", "\n", "lb_x", "=", "-", "need_to_pad", "[", "0", "]", "//", "2", "\n", "ub_x", "=", "shape", "[", "0", "]", "+", "need_to_pad", "[", "0", "]", "//", "2", "+", "need_to_pad", "[", "0", "]", "%", "2", "-", "self", ".", "patch_size", "[", "0", "]", "\n", "lb_y", "=", "-", "need_to_pad", "[", "1", "]", "//", "2", "\n", "ub_y", "=", "shape", "[", "1", "]", "+", "need_to_pad", "[", "1", "]", "//", "2", "+", "need_to_pad", "[", "1", "]", "%", "2", "-", "self", ".", "patch_size", "[", "1", "]", "\n", "lb_z", "=", "-", "need_to_pad", "[", "2", "]", "//", "2", "\n", "ub_z", "=", "shape", "[", "2", "]", "+", "need_to_pad", "[", "2", "]", "//", "2", "+", "need_to_pad", "[", "2", "]", "%", "2", "-", "self", ".", "patch_size", "[", "2", "]", "\n", "\n", "# if not force_fg then we can just sample the bbox randomly from lb and ub. Else we need to make sure we get", "\n", "# at least one of the foreground classes in the patch", "\n", "if", "not", "force_fg", ":", "\n", "                ", "bbox_x_lb", "=", "np", ".", "random", ".", "randint", "(", "lb_x", ",", "ub_x", "+", "1", ")", "\n", "bbox_y_lb", "=", "np", ".", "random", ".", "randint", "(", "lb_y", ",", "ub_y", "+", "1", ")", "\n", "bbox_z_lb", "=", "np", ".", "random", ".", "randint", "(", "lb_z", ",", "ub_z", "+", "1", ")", "\n", "", "else", ":", "\n", "# these values should have been precomputed", "\n", "                ", "if", "'class_locations'", "not", "in", "properties", ".", "keys", "(", ")", ":", "\n", "                    ", "raise", "RuntimeError", "(", "\"Please rerun the preprocessing with the newest version of nnU-Net!\"", ")", "\n", "\n", "# this saves us a np.unique. Preprocessing already did that for all cases. Neat.", "\n", "", "foreground_classes", "=", "np", ".", "array", "(", "\n", "[", "i", "for", "i", "in", "properties", "[", "'class_locations'", "]", ".", "keys", "(", ")", "if", "len", "(", "properties", "[", "'class_locations'", "]", "[", "i", "]", ")", "!=", "0", "]", ")", "\n", "foreground_classes", "=", "foreground_classes", "[", "foreground_classes", ">", "0", "]", "\n", "\n", "if", "len", "(", "foreground_classes", ")", "==", "0", ":", "\n", "# this only happens if some image does not contain foreground voxels at all", "\n", "                    ", "selected_class", "=", "None", "\n", "voxels_of_that_class", "=", "None", "\n", "print", "(", "'case does not contain any foreground classes'", ",", "i", ")", "\n", "", "else", ":", "\n", "                    ", "selected_class", "=", "np", ".", "random", ".", "choice", "(", "foreground_classes", ")", "\n", "\n", "voxels_of_that_class", "=", "properties", "[", "'class_locations'", "]", "[", "selected_class", "]", "\n", "\n", "", "if", "voxels_of_that_class", "is", "not", "None", ":", "\n", "                    ", "selected_voxel", "=", "voxels_of_that_class", "[", "np", ".", "random", ".", "choice", "(", "len", "(", "voxels_of_that_class", ")", ")", "]", "\n", "# selected voxel is center voxel. Subtract half the patch size to get lower bbox voxel.", "\n", "# Make sure it is within the bounds of lb and ub", "\n", "bbox_x_lb", "=", "max", "(", "lb_x", ",", "selected_voxel", "[", "0", "]", "-", "self", ".", "patch_size", "[", "0", "]", "//", "2", ")", "\n", "bbox_y_lb", "=", "max", "(", "lb_y", ",", "selected_voxel", "[", "1", "]", "-", "self", ".", "patch_size", "[", "1", "]", "//", "2", ")", "\n", "bbox_z_lb", "=", "max", "(", "lb_z", ",", "selected_voxel", "[", "2", "]", "-", "self", ".", "patch_size", "[", "2", "]", "//", "2", ")", "\n", "", "else", ":", "\n", "# If the image does not contain any foreground classes, we fall back to random cropping", "\n", "                    ", "bbox_x_lb", "=", "np", ".", "random", ".", "randint", "(", "lb_x", ",", "ub_x", "+", "1", ")", "\n", "bbox_y_lb", "=", "np", ".", "random", ".", "randint", "(", "lb_y", ",", "ub_y", "+", "1", ")", "\n", "bbox_z_lb", "=", "np", ".", "random", ".", "randint", "(", "lb_z", ",", "ub_z", "+", "1", ")", "\n", "\n", "", "", "bbox_x_ub", "=", "bbox_x_lb", "+", "self", ".", "patch_size", "[", "0", "]", "\n", "bbox_y_ub", "=", "bbox_y_lb", "+", "self", ".", "patch_size", "[", "1", "]", "\n", "bbox_z_ub", "=", "bbox_z_lb", "+", "self", ".", "patch_size", "[", "2", "]", "\n", "\n", "# whoever wrote this knew what he was doing (hint: it was me). We first crop the data to the region of the", "\n", "# bbox that actually lies within the data. This will result in a smaller array which is then faster to pad.", "\n", "# valid_bbox is just the coord that lied within the data cube. It will be padded to match the patch size", "\n", "# later", "\n", "valid_bbox_x_lb", "=", "max", "(", "0", ",", "bbox_x_lb", ")", "\n", "valid_bbox_x_ub", "=", "min", "(", "shape", "[", "0", "]", ",", "bbox_x_ub", ")", "\n", "valid_bbox_y_lb", "=", "max", "(", "0", ",", "bbox_y_lb", ")", "\n", "valid_bbox_y_ub", "=", "min", "(", "shape", "[", "1", "]", ",", "bbox_y_ub", ")", "\n", "valid_bbox_z_lb", "=", "max", "(", "0", ",", "bbox_z_lb", ")", "\n", "valid_bbox_z_ub", "=", "min", "(", "shape", "[", "2", "]", ",", "bbox_z_ub", ")", "\n", "\n", "# At this point you might ask yourself why we would treat seg differently from seg_from_previous_stage.", "\n", "# Why not just concatenate them here and forget about the if statements? Well that's because segneeds to", "\n", "# be padded with -1 constant whereas seg_from_previous_stage needs to be padded with 0s (we could also", "\n", "# remove label -1 in the data augmentation but this way it is less error prone)", "\n", "case_all_data", "=", "np", ".", "copy", "(", "case_all_data", "[", ":", ",", "valid_bbox_x_lb", ":", "valid_bbox_x_ub", ",", "\n", "valid_bbox_y_lb", ":", "valid_bbox_y_ub", ",", "\n", "valid_bbox_z_lb", ":", "valid_bbox_z_ub", "]", ")", "\n", "if", "seg_from_previous_stage", "is", "not", "None", ":", "\n", "                ", "seg_from_previous_stage", "=", "seg_from_previous_stage", "[", ":", ",", "valid_bbox_x_lb", ":", "valid_bbox_x_ub", ",", "\n", "valid_bbox_y_lb", ":", "valid_bbox_y_ub", ",", "\n", "valid_bbox_z_lb", ":", "valid_bbox_z_ub", "]", "\n", "\n", "", "data", "[", "j", "]", "=", "np", ".", "pad", "(", "case_all_data", "[", ":", "-", "1", "]", ",", "(", "(", "0", ",", "0", ")", ",", "\n", "(", "-", "min", "(", "0", ",", "bbox_x_lb", ")", ",", "max", "(", "bbox_x_ub", "-", "shape", "[", "0", "]", ",", "0", ")", ")", ",", "\n", "(", "-", "min", "(", "0", ",", "bbox_y_lb", ")", ",", "max", "(", "bbox_y_ub", "-", "shape", "[", "1", "]", ",", "0", ")", ")", ",", "\n", "(", "-", "min", "(", "0", ",", "bbox_z_lb", ")", ",", "max", "(", "bbox_z_ub", "-", "shape", "[", "2", "]", ",", "0", ")", ")", ")", ",", "\n", "self", ".", "pad_mode", ",", "**", "self", ".", "pad_kwargs_data", ")", "\n", "\n", "seg", "[", "j", ",", "0", "]", "=", "np", ".", "pad", "(", "case_all_data", "[", "-", "1", ":", "]", ",", "(", "(", "0", ",", "0", ")", ",", "\n", "(", "-", "min", "(", "0", ",", "bbox_x_lb", ")", ",", "max", "(", "bbox_x_ub", "-", "shape", "[", "0", "]", ",", "0", ")", ")", ",", "\n", "(", "-", "min", "(", "0", ",", "bbox_y_lb", ")", ",", "max", "(", "bbox_y_ub", "-", "shape", "[", "1", "]", ",", "0", ")", ")", ",", "\n", "(", "-", "min", "(", "0", ",", "bbox_z_lb", ")", ",", "max", "(", "bbox_z_ub", "-", "shape", "[", "2", "]", ",", "0", ")", ")", ")", ",", "\n", "'constant'", ",", "**", "{", "'constant_values'", ":", "-", "1", "}", ")", "\n", "if", "seg_from_previous_stage", "is", "not", "None", ":", "\n", "                ", "seg", "[", "j", ",", "1", "]", "=", "np", ".", "pad", "(", "seg_from_previous_stage", ",", "(", "(", "0", ",", "0", ")", ",", "\n", "(", "-", "min", "(", "0", ",", "bbox_x_lb", ")", ",", "\n", "max", "(", "bbox_x_ub", "-", "shape", "[", "0", "]", ",", "0", ")", ")", ",", "\n", "(", "-", "min", "(", "0", ",", "bbox_y_lb", ")", ",", "\n", "max", "(", "bbox_y_ub", "-", "shape", "[", "1", "]", ",", "0", ")", ")", ",", "\n", "(", "-", "min", "(", "0", ",", "bbox_z_lb", ")", ",", "\n", "max", "(", "bbox_z_ub", "-", "shape", "[", "2", "]", ",", "0", ")", ")", ")", ",", "\n", "'constant'", ",", "**", "{", "'constant_values'", ":", "0", "}", ")", "\n", "\n", "", "", "return", "{", "'data'", ":", "data", ",", "'seg'", ":", "seg", ",", "'properties'", ":", "case_properties", ",", "'keys'", ":", "selected_keys", "}", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.dataloading.dataset_loading.DataLoader2D.__init__": [[383, 428], ["batchgenerators.dataloading.SlimDataLoaderBase.__init__", "list", "dataset_loading.DataLoader2D.determine_shapes", "collections.OrderedDict", "dataset_loading.DataLoader2D._data.keys", "numpy.array", "numpy.array", "isinstance", "numpy.array"], "methods", ["home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.common.layers.ResizeImage.__init__", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.dataloading.dataset_loading.DataLoader2D.determine_shapes"], ["    ", "def", "__init__", "(", "self", ",", "data", ",", "patch_size", ",", "final_patch_size", ",", "batch_size", ",", "oversample_foreground_percent", "=", "0.0", ",", "\n", "memmap_mode", "=", "\"r\"", ",", "pseudo_3d_slices", "=", "1", ",", "pad_mode", "=", "\"edge\"", ",", "\n", "pad_kwargs_data", "=", "None", ",", "pad_sides", "=", "None", ")", ":", "\n", "        ", "\"\"\"\n        This is the basic data loader for 2D networks. It uses preprocessed data as produced by my (Fabian) preprocessing.\n        You can load the data with load_dataset(folder) where folder is the folder where the npz files are located. If there\n        are only npz files present in that folder, the data loader will unpack them on the fly. This may take a while\n        and increase CPU usage. Therefore, I advise you to call unpack_dataset(folder) first, which will unpack all npz\n        to npy. Don't forget to call delete_npy(folder) after you are done with training?\n        Why all the hassle? Well the decathlon dataset is huge. Using npy for everything will consume >1 TB and that is uncool\n        given that I (Fabian) will have to store that permanently on /datasets and my local computer. With htis strategy all\n        data is stored in a compressed format (factor 10 smaller) and only unpacked when needed.\n        :param data: get this with load_dataset(folder, stage=0). Plug the return value in here and you are g2g (good to go)\n        :param patch_size: what patch size will this data loader return? it is common practice to first load larger\n        patches so that a central crop after data augmentation can be done to reduce border artifacts. If unsure, use\n        get_patch_size() from data_augmentation.default_data_augmentation\n        :param final_patch_size: what will the patch finally be cropped to (after data augmentation)? this is the patch\n        size that goes into your network. We need this here because we will pad patients in here so that patches at the\n        border of patients are sampled properly\n        :param batch_size:\n        :param num_batches: how many batches will the data loader produce before stopping? None=endless\n        :param seed:\n        :param stage: ignore this (Fabian only)\n        :param transpose: ignore this\n        :param random: sample randomly; CAREFUL! non-random sampling requires batch_size=1, otherwise you will iterate batch_size times over the dataset\n        :param pseudo_3d_slices: 7 = 3 below and 3 above the center slice\n        \"\"\"", "\n", "super", "(", "DataLoader2D", ",", "self", ")", ".", "__init__", "(", "data", ",", "batch_size", ",", "None", ")", "\n", "if", "pad_kwargs_data", "is", "None", ":", "\n", "            ", "pad_kwargs_data", "=", "OrderedDict", "(", ")", "\n", "", "self", ".", "pad_kwargs_data", "=", "pad_kwargs_data", "\n", "self", ".", "pad_mode", "=", "pad_mode", "\n", "self", ".", "pseudo_3d_slices", "=", "pseudo_3d_slices", "\n", "self", ".", "oversample_foreground_percent", "=", "oversample_foreground_percent", "\n", "self", ".", "final_patch_size", "=", "final_patch_size", "\n", "self", ".", "patch_size", "=", "patch_size", "\n", "self", ".", "list_of_keys", "=", "list", "(", "self", ".", "_data", ".", "keys", "(", ")", ")", "\n", "self", ".", "need_to_pad", "=", "np", ".", "array", "(", "patch_size", ")", "-", "np", ".", "array", "(", "final_patch_size", ")", "\n", "self", ".", "memmap_mode", "=", "memmap_mode", "\n", "if", "pad_sides", "is", "not", "None", ":", "\n", "            ", "if", "not", "isinstance", "(", "pad_sides", ",", "np", ".", "ndarray", ")", ":", "\n", "                ", "pad_sides", "=", "np", ".", "array", "(", "pad_sides", ")", "\n", "", "self", ".", "need_to_pad", "+=", "pad_sides", "\n", "", "self", ".", "pad_sides", "=", "pad_sides", "\n", "self", ".", "data_shape", ",", "self", ".", "seg_shape", "=", "self", ".", "determine_shapes", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.dataloading.dataset_loading.DataLoader2D.determine_shapes": [[429, 441], ["isfile", "list", "numpy.load", "dataset_loading.DataLoader2D._data.keys", "numpy.load"], "methods", ["None"], ["", "def", "determine_shapes", "(", "self", ")", ":", "\n", "        ", "num_seg", "=", "1", "\n", "\n", "k", "=", "list", "(", "self", ".", "_data", ".", "keys", "(", ")", ")", "[", "0", "]", "\n", "if", "isfile", "(", "self", ".", "_data", "[", "k", "]", "[", "'data_file'", "]", "[", ":", "-", "4", "]", "+", "\".npy\"", ")", ":", "\n", "            ", "case_all_data", "=", "np", ".", "load", "(", "self", ".", "_data", "[", "k", "]", "[", "'data_file'", "]", "[", ":", "-", "4", "]", "+", "\".npy\"", ",", "self", ".", "memmap_mode", ")", "\n", "", "else", ":", "\n", "            ", "case_all_data", "=", "np", ".", "load", "(", "self", ".", "_data", "[", "k", "]", "[", "'data_file'", "]", ")", "[", "'data'", "]", "\n", "", "num_color_channels", "=", "case_all_data", ".", "shape", "[", "0", "]", "-", "num_seg", "\n", "data_shape", "=", "(", "self", ".", "batch_size", ",", "num_color_channels", ",", "*", "self", ".", "patch_size", ")", "\n", "seg_shape", "=", "(", "self", ".", "batch_size", ",", "num_seg", ",", "*", "self", ".", "patch_size", ")", "\n", "return", "data_shape", ",", "seg_shape", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.dataloading.dataset_loading.DataLoader2D.get_do_oversample": [[442, 444], ["round"], "methods", ["None"], ["", "def", "get_do_oversample", "(", "self", ",", "batch_idx", ")", ":", "\n", "        ", "return", "not", "batch_idx", "<", "round", "(", "self", ".", "batch_size", "*", "(", "1", "-", "self", ".", "oversample_foreground_percent", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.dataloading.dataset_loading.DataLoader2D.generate_train_batch": [[445, 593], ["numpy.random.choice", "numpy.zeros", "numpy.zeros", "enumerate", "case_properties.append", "dataset_loading.DataLoader2D.get_do_oversample", "range", "max", "min", "max", "min", "numpy.pad", "numpy.pad", "dataset_loading.DataLoader2D._data[].keys", "load_pickle", "isfile", "numpy.load", "len", "numpy.random.choice", "numpy.array", "max", "min", "numpy.concatenate.reshape", "numpy.concatenate", "len", "numpy.random.randint", "numpy.random.randint", "max", "max", "numpy.load", "load_pickle.keys", "RuntimeError", "len", "numpy.random.choice", "print", "numpy.random.choice", "numpy.unique", "numpy.random.choice", "numpy.array", "numpy.concatenate", "numpy.array", "numpy.concatenate", "numpy.random.choice", "max", "max", "max", "max", "properties[].keys", "numpy.zeros", "numpy.zeros", "len", "min", "min", "min", "min", "len"], "methods", ["home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.dataloading.dataset_loading.DataLoader2D.get_do_oversample"], ["", "def", "generate_train_batch", "(", "self", ")", ":", "\n", "        ", "selected_keys", "=", "np", ".", "random", ".", "choice", "(", "self", ".", "list_of_keys", ",", "self", ".", "batch_size", ",", "True", ",", "None", ")", "\n", "\n", "data", "=", "np", ".", "zeros", "(", "self", ".", "data_shape", ",", "dtype", "=", "np", ".", "float32", ")", "\n", "seg", "=", "np", ".", "zeros", "(", "self", ".", "seg_shape", ",", "dtype", "=", "np", ".", "float32", ")", "\n", "\n", "case_properties", "=", "[", "]", "\n", "for", "j", ",", "i", "in", "enumerate", "(", "selected_keys", ")", ":", "\n", "            ", "if", "'properties'", "in", "self", ".", "_data", "[", "i", "]", ".", "keys", "(", ")", ":", "\n", "                ", "properties", "=", "self", ".", "_data", "[", "i", "]", "[", "'properties'", "]", "\n", "", "else", ":", "\n", "                ", "properties", "=", "load_pickle", "(", "self", ".", "_data", "[", "i", "]", "[", "'properties_file'", "]", ")", "\n", "", "case_properties", ".", "append", "(", "properties", ")", "\n", "\n", "if", "self", ".", "get_do_oversample", "(", "j", ")", ":", "\n", "                ", "force_fg", "=", "True", "\n", "", "else", ":", "\n", "                ", "force_fg", "=", "False", "\n", "\n", "", "if", "not", "isfile", "(", "self", ".", "_data", "[", "i", "]", "[", "'data_file'", "]", "[", ":", "-", "4", "]", "+", "\".npy\"", ")", ":", "\n", "# lets hope you know what you're doing", "\n", "                ", "case_all_data", "=", "np", ".", "load", "(", "self", ".", "_data", "[", "i", "]", "[", "'data_file'", "]", "[", ":", "-", "4", "]", "+", "\".npz\"", ")", "[", "'data'", "]", "\n", "", "else", ":", "\n", "                ", "case_all_data", "=", "np", ".", "load", "(", "self", ".", "_data", "[", "i", "]", "[", "'data_file'", "]", "[", ":", "-", "4", "]", "+", "\".npy\"", ",", "self", ".", "memmap_mode", ")", "\n", "\n", "# this is for when there is just a 2d slice in case_all_data (2d support)", "\n", "", "if", "len", "(", "case_all_data", ".", "shape", ")", "==", "3", ":", "\n", "                ", "case_all_data", "=", "case_all_data", "[", ":", ",", "None", "]", "\n", "\n", "# first select a slice. This can be either random (no force fg) or guaranteed to contain some class", "\n", "", "if", "not", "force_fg", ":", "\n", "                ", "random_slice", "=", "np", ".", "random", ".", "choice", "(", "case_all_data", ".", "shape", "[", "1", "]", ")", "\n", "selected_class", "=", "None", "\n", "", "else", ":", "\n", "# these values should have been precomputed", "\n", "                ", "if", "'class_locations'", "not", "in", "properties", ".", "keys", "(", ")", ":", "\n", "                    ", "raise", "RuntimeError", "(", "\"Please rerun the preprocessing with the newest version of nnU-Net!\"", ")", "\n", "\n", "", "foreground_classes", "=", "np", ".", "array", "(", "\n", "[", "i", "for", "i", "in", "properties", "[", "'class_locations'", "]", ".", "keys", "(", ")", "if", "len", "(", "properties", "[", "'class_locations'", "]", "[", "i", "]", ")", "!=", "0", "]", ")", "\n", "foreground_classes", "=", "foreground_classes", "[", "foreground_classes", ">", "0", "]", "\n", "if", "len", "(", "foreground_classes", ")", "==", "0", ":", "\n", "                    ", "selected_class", "=", "None", "\n", "random_slice", "=", "np", ".", "random", ".", "choice", "(", "case_all_data", ".", "shape", "[", "1", "]", ")", "\n", "print", "(", "'case does not contain any foreground classes'", ",", "i", ")", "\n", "", "else", ":", "\n", "                    ", "selected_class", "=", "np", ".", "random", ".", "choice", "(", "foreground_classes", ")", "\n", "\n", "voxels_of_that_class", "=", "properties", "[", "'class_locations'", "]", "[", "selected_class", "]", "\n", "valid_slices", "=", "np", ".", "unique", "(", "voxels_of_that_class", "[", ":", ",", "0", "]", ")", "\n", "random_slice", "=", "np", ".", "random", ".", "choice", "(", "valid_slices", ")", "\n", "voxels_of_that_class", "=", "voxels_of_that_class", "[", "voxels_of_that_class", "[", ":", ",", "0", "]", "==", "random_slice", "]", "\n", "\n", "# now crop case_all_data to contain just the slice of interest. If we want additional slice above and", "\n", "# below the current slice, here is where we get them. We stack those as additional color channels", "\n", "", "", "if", "self", ".", "pseudo_3d_slices", "==", "1", ":", "\n", "                ", "case_all_data", "=", "case_all_data", "[", ":", ",", "random_slice", "]", "\n", "", "else", ":", "\n", "# this is very deprecated and will probably not work anymore. If you intend to use this you need to", "\n", "# check this!", "\n", "                ", "mn", "=", "random_slice", "-", "(", "self", ".", "pseudo_3d_slices", "-", "1", ")", "//", "2", "\n", "mx", "=", "random_slice", "+", "(", "self", ".", "pseudo_3d_slices", "-", "1", ")", "//", "2", "+", "1", "\n", "valid_mn", "=", "max", "(", "mn", ",", "0", ")", "\n", "valid_mx", "=", "min", "(", "mx", ",", "case_all_data", ".", "shape", "[", "1", "]", ")", "\n", "case_all_seg", "=", "case_all_data", "[", "-", "1", ":", "]", "\n", "case_all_data", "=", "case_all_data", "[", ":", "-", "1", "]", "\n", "case_all_data", "=", "case_all_data", "[", ":", ",", "valid_mn", ":", "valid_mx", "]", "\n", "case_all_seg", "=", "case_all_seg", "[", ":", ",", "random_slice", "]", "\n", "need_to_pad_below", "=", "valid_mn", "-", "mn", "\n", "need_to_pad_above", "=", "mx", "-", "valid_mx", "\n", "if", "need_to_pad_below", ">", "0", ":", "\n", "                    ", "shp_for_pad", "=", "np", ".", "array", "(", "case_all_data", ".", "shape", ")", "\n", "shp_for_pad", "[", "1", "]", "=", "need_to_pad_below", "\n", "case_all_data", "=", "np", ".", "concatenate", "(", "(", "np", ".", "zeros", "(", "shp_for_pad", ")", ",", "case_all_data", ")", ",", "1", ")", "\n", "", "if", "need_to_pad_above", ">", "0", ":", "\n", "                    ", "shp_for_pad", "=", "np", ".", "array", "(", "case_all_data", ".", "shape", ")", "\n", "shp_for_pad", "[", "1", "]", "=", "need_to_pad_above", "\n", "case_all_data", "=", "np", ".", "concatenate", "(", "(", "case_all_data", ",", "np", ".", "zeros", "(", "shp_for_pad", ")", ")", ",", "1", ")", "\n", "", "case_all_data", "=", "case_all_data", ".", "reshape", "(", "(", "-", "1", ",", "case_all_data", ".", "shape", "[", "-", "2", "]", ",", "case_all_data", ".", "shape", "[", "-", "1", "]", ")", ")", "\n", "case_all_data", "=", "np", ".", "concatenate", "(", "(", "case_all_data", ",", "case_all_seg", ")", ",", "0", ")", "\n", "\n", "# case all data should now be (c, x, y)", "\n", "", "assert", "len", "(", "case_all_data", ".", "shape", ")", "==", "3", "\n", "\n", "# we can now choose the bbox from -need_to_pad // 2 to shape - patch_size + need_to_pad // 2. Here we", "\n", "# define what the upper and lower bound can be to then sample form them with np.random.randint", "\n", "\n", "need_to_pad", "=", "self", ".", "need_to_pad", "\n", "for", "d", "in", "range", "(", "2", ")", ":", "\n", "# if case_all_data.shape + need_to_pad is still < patch size we need to pad more! We pad on both sides", "\n", "# always", "\n", "                ", "if", "need_to_pad", "[", "d", "]", "+", "case_all_data", ".", "shape", "[", "d", "+", "1", "]", "<", "self", ".", "patch_size", "[", "d", "]", ":", "\n", "                    ", "need_to_pad", "[", "d", "]", "=", "self", ".", "patch_size", "[", "d", "]", "-", "case_all_data", ".", "shape", "[", "d", "+", "1", "]", "\n", "\n", "", "", "shape", "=", "case_all_data", ".", "shape", "[", "1", ":", "]", "\n", "lb_x", "=", "-", "need_to_pad", "[", "0", "]", "//", "2", "\n", "ub_x", "=", "shape", "[", "0", "]", "+", "need_to_pad", "[", "0", "]", "//", "2", "+", "need_to_pad", "[", "0", "]", "%", "2", "-", "self", ".", "patch_size", "[", "0", "]", "\n", "lb_y", "=", "-", "need_to_pad", "[", "1", "]", "//", "2", "\n", "ub_y", "=", "shape", "[", "1", "]", "+", "need_to_pad", "[", "1", "]", "//", "2", "+", "need_to_pad", "[", "1", "]", "%", "2", "-", "self", ".", "patch_size", "[", "1", "]", "\n", "\n", "# if not force_fg then we can just sample the bbox randomly from lb and ub. Else we need to make sure we get", "\n", "# at least one of the foreground classes in the patch", "\n", "if", "not", "force_fg", "or", "selected_class", "is", "None", ":", "\n", "                ", "bbox_x_lb", "=", "np", ".", "random", ".", "randint", "(", "lb_x", ",", "ub_x", "+", "1", ")", "\n", "bbox_y_lb", "=", "np", ".", "random", ".", "randint", "(", "lb_y", ",", "ub_y", "+", "1", ")", "\n", "", "else", ":", "\n", "# this saves us a np.unique. Preprocessing already did that for all cases. Neat.", "\n", "                ", "selected_voxel", "=", "voxels_of_that_class", "[", "np", ".", "random", ".", "choice", "(", "len", "(", "voxels_of_that_class", ")", ")", "]", "\n", "# selected voxel is center voxel. Subtract half the patch size to get lower bbox voxel.", "\n", "# Make sure it is within the bounds of lb and ub", "\n", "bbox_x_lb", "=", "max", "(", "lb_x", ",", "selected_voxel", "[", "0", "]", "-", "self", ".", "patch_size", "[", "0", "]", "//", "2", ")", "\n", "bbox_y_lb", "=", "max", "(", "lb_y", ",", "selected_voxel", "[", "1", "]", "-", "self", ".", "patch_size", "[", "1", "]", "//", "2", ")", "\n", "\n", "", "bbox_x_ub", "=", "bbox_x_lb", "+", "self", ".", "patch_size", "[", "0", "]", "\n", "bbox_y_ub", "=", "bbox_y_lb", "+", "self", ".", "patch_size", "[", "1", "]", "\n", "\n", "# whoever wrote this knew what he was doing (hint: it was me). We first crop the data to the region of the", "\n", "# bbox that actually lies within the data. This will result in a smaller array which is then faster to pad.", "\n", "# valid_bbox is just the coord that lied within the data cube. It will be padded to match the patch size", "\n", "# later", "\n", "valid_bbox_x_lb", "=", "max", "(", "0", ",", "bbox_x_lb", ")", "\n", "valid_bbox_x_ub", "=", "min", "(", "shape", "[", "0", "]", ",", "bbox_x_ub", ")", "\n", "valid_bbox_y_lb", "=", "max", "(", "0", ",", "bbox_y_lb", ")", "\n", "valid_bbox_y_ub", "=", "min", "(", "shape", "[", "1", "]", ",", "bbox_y_ub", ")", "\n", "\n", "# At this point you might ask yourself why we would treat seg differently from seg_from_previous_stage.", "\n", "# Why not just concatenate them here and forget about the if statements? Well that's because segneeds to", "\n", "# be padded with -1 constant whereas seg_from_previous_stage needs to be padded with 0s (we could also", "\n", "# remove label -1 in the data augmentation but this way it is less error prone)", "\n", "\n", "case_all_data", "=", "case_all_data", "[", ":", ",", "valid_bbox_x_lb", ":", "valid_bbox_x_ub", ",", "\n", "valid_bbox_y_lb", ":", "valid_bbox_y_ub", "]", "\n", "\n", "case_all_data_donly", "=", "np", ".", "pad", "(", "case_all_data", "[", ":", "-", "1", "]", ",", "(", "(", "0", ",", "0", ")", ",", "\n", "(", "-", "min", "(", "0", ",", "bbox_x_lb", ")", ",", "max", "(", "bbox_x_ub", "-", "shape", "[", "0", "]", ",", "0", ")", ")", ",", "\n", "(", "-", "min", "(", "0", ",", "bbox_y_lb", ")", ",", "max", "(", "bbox_y_ub", "-", "shape", "[", "1", "]", ",", "0", ")", ")", ")", ",", "\n", "self", ".", "pad_mode", ",", "**", "self", ".", "pad_kwargs_data", ")", "\n", "\n", "case_all_data_segonly", "=", "np", ".", "pad", "(", "case_all_data", "[", "-", "1", ":", "]", ",", "(", "(", "0", ",", "0", ")", ",", "\n", "(", "-", "min", "(", "0", ",", "bbox_x_lb", ")", ",", "max", "(", "bbox_x_ub", "-", "shape", "[", "0", "]", ",", "0", ")", ")", ",", "\n", "(", "-", "min", "(", "0", ",", "bbox_y_lb", ")", ",", "max", "(", "bbox_y_ub", "-", "shape", "[", "1", "]", ",", "0", ")", ")", ")", ",", "\n", "'constant'", ",", "**", "{", "'constant_values'", ":", "-", "1", "}", ")", "\n", "\n", "data", "[", "j", "]", "=", "case_all_data_donly", "\n", "seg", "[", "j", "]", "=", "case_all_data_segonly", "\n", "\n", "", "keys", "=", "selected_keys", "\n", "return", "{", "'data'", ":", "data", ",", "'seg'", ":", "seg", ",", "'properties'", ":", "case_properties", ",", "\"keys\"", ":", "keys", "}", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.dataloading.dataset_loading.get_case_identifiers": [[26, 29], ["os.listdir", "i.endswith", "i.find"], "function", ["None"], ["def", "get_case_identifiers", "(", "folder", ")", ":", "\n", "    ", "case_identifiers", "=", "[", "i", "[", ":", "-", "4", "]", "for", "i", "in", "os", ".", "listdir", "(", "folder", ")", "if", "i", ".", "endswith", "(", "\"npz\"", ")", "and", "(", "i", ".", "find", "(", "\"segFromPrevStage\"", ")", "==", "-", "1", ")", "]", "\n", "return", "case_identifiers", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.dataloading.dataset_loading.get_case_identifiers_from_raw_folder": [[31, 35], ["numpy.unique", "os.listdir", "i.endswith", "i.find"], "function", ["None"], ["", "def", "get_case_identifiers_from_raw_folder", "(", "folder", ")", ":", "\n", "    ", "case_identifiers", "=", "np", ".", "unique", "(", "\n", "[", "i", "[", ":", "-", "12", "]", "for", "i", "in", "os", ".", "listdir", "(", "folder", ")", "if", "i", ".", "endswith", "(", "\".nii.gz\"", ")", "and", "(", "i", ".", "find", "(", "\"segFromPrevStage\"", ")", "==", "-", "1", ")", "]", ")", "\n", "return", "case_identifiers", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.dataloading.dataset_loading.convert_to_npy": [[37, 46], ["isinstance", "isfile", "numpy.save", "numpy.load"], "function", ["None"], ["", "def", "convert_to_npy", "(", "args", ")", ":", "\n", "    ", "if", "not", "isinstance", "(", "args", ",", "tuple", ")", ":", "\n", "        ", "key", "=", "\"data\"", "\n", "npz_file", "=", "args", "\n", "", "else", ":", "\n", "        ", "npz_file", ",", "key", "=", "args", "\n", "", "if", "not", "isfile", "(", "npz_file", "[", ":", "-", "3", "]", "+", "\"npy\"", ")", ":", "\n", "        ", "a", "=", "np", ".", "load", "(", "npz_file", ")", "[", "key", "]", "\n", "np", ".", "save", "(", "npz_file", "[", ":", "-", "3", "]", "+", "\"npy\"", ",", "a", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.dataloading.dataset_loading.save_as_npz": [[48, 56], ["numpy.load", "numpy.savez_compressed", "isinstance"], "function", ["None"], ["", "", "def", "save_as_npz", "(", "args", ")", ":", "\n", "    ", "if", "not", "isinstance", "(", "args", ",", "tuple", ")", ":", "\n", "        ", "key", "=", "\"data\"", "\n", "npy_file", "=", "args", "\n", "", "else", ":", "\n", "        ", "npy_file", ",", "key", "=", "args", "\n", "", "d", "=", "np", ".", "load", "(", "npy_file", ")", "\n", "np", ".", "savez_compressed", "(", "npy_file", "[", ":", "-", "3", "]", "+", "\"npz\"", ",", "**", "{", "key", ":", "d", "}", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.dataloading.dataset_loading.unpack_dataset": [[58, 71], ["multiprocessing.Pool", "subfiles", "multiprocessing.Pool.map", "multiprocessing.Pool.close", "multiprocessing.Pool.join", "zip", "len"], "function", ["None"], ["", "def", "unpack_dataset", "(", "folder", ",", "threads", "=", "default_num_threads", ",", "key", "=", "\"data\"", ")", ":", "\n", "    ", "\"\"\"\n    unpacks all npz files in a folder to npy (whatever you want to have unpacked must be saved unter key)\n    :param folder:\n    :param threads:\n    :param key:\n    :return:\n    \"\"\"", "\n", "p", "=", "Pool", "(", "threads", ")", "\n", "npz_files", "=", "subfiles", "(", "folder", ",", "True", ",", "None", ",", "\".npz\"", ",", "True", ")", "\n", "p", ".", "map", "(", "convert_to_npy", ",", "zip", "(", "npz_files", ",", "[", "key", "]", "*", "len", "(", "npz_files", ")", ")", ")", "\n", "p", ".", "close", "(", ")", "\n", "p", ".", "join", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.dataloading.dataset_loading.pack_dataset": [[73, 79], ["multiprocessing.Pool", "subfiles", "multiprocessing.Pool.map", "multiprocessing.Pool.close", "multiprocessing.Pool.join", "zip", "len"], "function", ["None"], ["", "def", "pack_dataset", "(", "folder", ",", "threads", "=", "default_num_threads", ",", "key", "=", "\"data\"", ")", ":", "\n", "    ", "p", "=", "Pool", "(", "threads", ")", "\n", "npy_files", "=", "subfiles", "(", "folder", ",", "True", ",", "None", ",", "\".npy\"", ",", "True", ")", "\n", "p", ".", "map", "(", "save_as_npz", ",", "zip", "(", "npy_files", ",", "[", "key", "]", "*", "len", "(", "npy_files", ")", ")", ")", "\n", "p", ".", "close", "(", ")", "\n", "p", ".", "join", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.dataloading.dataset_loading.delete_npy": [[81, 87], ["dataset_loading.get_case_identifiers", "join", "os.remove", "isfile"], "function", ["home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.dataloading.dataset_loading.get_case_identifiers"], ["", "def", "delete_npy", "(", "folder", ")", ":", "\n", "    ", "case_identifiers", "=", "get_case_identifiers", "(", "folder", ")", "\n", "npy_files", "=", "[", "join", "(", "folder", ",", "i", "+", "\".npy\"", ")", "for", "i", "in", "case_identifiers", "]", "\n", "npy_files", "=", "[", "i", "for", "i", "in", "npy_files", "if", "isfile", "(", "i", ")", "]", "\n", "for", "n", "in", "npy_files", ":", "\n", "        ", "os", ".", "remove", "(", "n", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.dataloading.dataset_loading.load_dataset": [[89, 111], ["print", "dataset_loading.get_case_identifiers", "get_case_identifiers.sort", "collections.OrderedDict", "collections.OrderedDict", "join", "join", "len", "print", "collections.OrderedDict.keys", "dataset[].get", "join", "load_pickle"], "function", ["home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.dataloading.dataset_loading.get_case_identifiers"], ["", "", "def", "load_dataset", "(", "folder", ",", "num_cases_properties_loading_threshold", "=", "1000", ")", ":", "\n", "# we don't load the actual data but instead return the filename to the np file.", "\n", "    ", "print", "(", "'loading dataset'", ")", "\n", "case_identifiers", "=", "get_case_identifiers", "(", "folder", ")", "\n", "case_identifiers", ".", "sort", "(", ")", "\n", "dataset", "=", "OrderedDict", "(", ")", "\n", "for", "c", "in", "case_identifiers", ":", "\n", "        ", "dataset", "[", "c", "]", "=", "OrderedDict", "(", ")", "\n", "dataset", "[", "c", "]", "[", "'data_file'", "]", "=", "join", "(", "folder", ",", "\"%s.npz\"", "%", "c", ")", "\n", "\n", "# dataset[c]['properties'] = load_pickle(join(folder, \"%s.pkl\" % c))", "\n", "dataset", "[", "c", "]", "[", "'properties_file'", "]", "=", "join", "(", "folder", ",", "\"%s.pkl\"", "%", "c", ")", "\n", "\n", "if", "dataset", "[", "c", "]", ".", "get", "(", "'seg_from_prev_stage_file'", ")", "is", "not", "None", ":", "\n", "            ", "dataset", "[", "c", "]", "[", "'seg_from_prev_stage_file'", "]", "=", "join", "(", "folder", ",", "\"%s_segs.npz\"", "%", "c", ")", "\n", "\n", "", "", "if", "len", "(", "case_identifiers", ")", "<=", "num_cases_properties_loading_threshold", ":", "\n", "        ", "print", "(", "'loading all case properties'", ")", "\n", "for", "i", "in", "dataset", ".", "keys", "(", ")", ":", "\n", "            ", "dataset", "[", "i", "]", "[", "'properties'", "]", "=", "load_pickle", "(", "dataset", "[", "i", "]", "[", "'properties_file'", "]", ")", "\n", "\n", "", "", "return", "dataset", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.dataloading.dataset_loading.crop_2D_image_force_fg": [[113, 153], ["numpy.array", "range", "len", "type", "len", "max", "min", "len", "numpy.random.random_integers", "numpy.random.random_integers", "len", "len", "numpy.random.choice"], "function", ["None"], ["", "def", "crop_2D_image_force_fg", "(", "img", ",", "crop_size", ",", "valid_voxels", ")", ":", "\n", "    ", "\"\"\"\n    img must be [c, x, y]\n    img[-1] must be the segmentation with segmentation>0 being foreground\n    :param img:\n    :param crop_size:\n    :param valid_voxels: voxels belonging to the selected class\n    :return:\n    \"\"\"", "\n", "assert", "len", "(", "valid_voxels", ".", "shape", ")", "==", "2", "\n", "\n", "if", "type", "(", "crop_size", ")", "not", "in", "(", "tuple", ",", "list", ")", ":", "\n", "        ", "crop_size", "=", "[", "crop_size", "]", "*", "(", "len", "(", "img", ".", "shape", ")", "-", "1", ")", "\n", "", "else", ":", "\n", "        ", "assert", "len", "(", "crop_size", ")", "==", "(", "len", "(", "\n", "img", ".", "shape", ")", "-", "1", ")", ",", "\"If you provide a list/tuple as center crop make sure it has the same len as your data has dims (3d)\"", "\n", "\n", "# we need to find the center coords that we can crop to without exceeding the image border", "\n", "", "lb_x", "=", "crop_size", "[", "0", "]", "//", "2", "\n", "ub_x", "=", "img", ".", "shape", "[", "1", "]", "-", "crop_size", "[", "0", "]", "//", "2", "-", "crop_size", "[", "0", "]", "%", "2", "\n", "lb_y", "=", "crop_size", "[", "1", "]", "//", "2", "\n", "ub_y", "=", "img", ".", "shape", "[", "2", "]", "-", "crop_size", "[", "1", "]", "//", "2", "-", "crop_size", "[", "1", "]", "%", "2", "\n", "\n", "if", "len", "(", "valid_voxels", ")", "==", "0", ":", "\n", "        ", "selected_center_voxel", "=", "(", "np", ".", "random", ".", "random_integers", "(", "lb_x", ",", "ub_x", ")", ",", "\n", "np", ".", "random", ".", "random_integers", "(", "lb_y", ",", "ub_y", ")", ")", "\n", "", "else", ":", "\n", "        ", "selected_center_voxel", "=", "valid_voxels", "[", "np", ".", "random", ".", "choice", "(", "valid_voxels", ".", "shape", "[", "1", "]", ")", ",", ":", "]", "\n", "\n", "", "selected_center_voxel", "=", "np", ".", "array", "(", "selected_center_voxel", ")", "\n", "for", "i", "in", "range", "(", "2", ")", ":", "\n", "        ", "selected_center_voxel", "[", "i", "]", "=", "max", "(", "crop_size", "[", "i", "]", "//", "2", ",", "selected_center_voxel", "[", "i", "]", ")", "\n", "selected_center_voxel", "[", "i", "]", "=", "min", "(", "img", ".", "shape", "[", "i", "+", "1", "]", "-", "crop_size", "[", "i", "]", "//", "2", "-", "crop_size", "[", "i", "]", "%", "2", ",", "\n", "selected_center_voxel", "[", "i", "]", ")", "\n", "\n", "", "result", "=", "img", "[", ":", ",", "(", "selected_center_voxel", "[", "0", "]", "-", "crop_size", "[", "0", "]", "//", "2", ")", ":", "(", "\n", "selected_center_voxel", "[", "0", "]", "+", "crop_size", "[", "0", "]", "//", "2", "+", "crop_size", "[", "0", "]", "%", "2", ")", ",", "\n", "(", "selected_center_voxel", "[", "1", "]", "-", "crop_size", "[", "1", "]", "//", "2", ")", ":", "(", "\n", "selected_center_voxel", "[", "1", "]", "+", "crop_size", "[", "1", "]", "//", "2", "+", "crop_size", "[", "1", "]", "%", "2", ")", "]", "\n", "return", "result", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.inference.predict.preprocess_save_to_queue": [[35, 89], ["enumerate", "q.put", "len", "print", "print", "print", "print", "preprocess_fn", "print", "q.put", "SimpleITK.GetArrayFromImage", "SimpleITK.GetArrayFromImage", "all", "seg_prev.transpose.transpose", "batchgenerators.augmentations.utils.resize_segmentation", "nnunet.utilities.one_hot_encoding.to_one_hot", "numpy.vstack().astype", "numpy.prod", "print", "numpy.save", "print", "print", "isfile", "segs_from_prev_stage[].endswith", "SimpleITK.ReadImage", "SimpleITK.ReadImage", "numpy.vstack", "zip"], "function", ["home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.utilities.one_hot_encoding.to_one_hot"], ["def", "preprocess_save_to_queue", "(", "preprocess_fn", ",", "q", ",", "list_of_lists", ",", "output_files", ",", "segs_from_prev_stage", ",", "classes", ",", "\n", "transpose_forward", ")", ":", "\n", "# suppress output", "\n", "# sys.stdout = open(os.devnull, 'w')", "\n", "\n", "    ", "errors_in", "=", "[", "]", "\n", "for", "i", ",", "l", "in", "enumerate", "(", "list_of_lists", ")", ":", "\n", "        ", "try", ":", "\n", "            ", "output_file", "=", "output_files", "[", "i", "]", "\n", "print", "(", "\"preprocessing\"", ",", "output_file", ")", "\n", "d", ",", "_", ",", "dct", "=", "preprocess_fn", "(", "l", ")", "\n", "# print(output_file, dct)", "\n", "if", "segs_from_prev_stage", "[", "i", "]", "is", "not", "None", ":", "\n", "                ", "assert", "isfile", "(", "segs_from_prev_stage", "[", "i", "]", ")", "and", "segs_from_prev_stage", "[", "i", "]", ".", "endswith", "(", "\n", "\".nii.gz\"", ")", ",", "\"segs_from_prev_stage\"", "\" must point to a \"", "\"segmentation file\"", "\n", "seg_prev", "=", "sitk", ".", "GetArrayFromImage", "(", "sitk", ".", "ReadImage", "(", "segs_from_prev_stage", "[", "i", "]", ")", ")", "\n", "# check to see if shapes match", "\n", "img", "=", "sitk", ".", "GetArrayFromImage", "(", "sitk", ".", "ReadImage", "(", "l", "[", "0", "]", ")", ")", "\n", "assert", "all", "(", "[", "i", "==", "j", "for", "i", ",", "j", "in", "zip", "(", "seg_prev", ".", "shape", ",", "img", ".", "shape", ")", "]", ")", ",", "\"image and segmentation from previous \"", "\"stage don't have the same pixel array \"", "\"shape! image: %s, seg_prev: %s\"", "%", "(", "l", "[", "0", "]", ",", "segs_from_prev_stage", "[", "i", "]", ")", "\n", "seg_prev", "=", "seg_prev", ".", "transpose", "(", "transpose_forward", ")", "\n", "seg_reshaped", "=", "resize_segmentation", "(", "seg_prev", ",", "d", ".", "shape", "[", "1", ":", "]", ",", "order", "=", "1", ",", "cval", "=", "0", ")", "\n", "seg_reshaped", "=", "to_one_hot", "(", "seg_reshaped", ",", "classes", ")", "\n", "d", "=", "np", ".", "vstack", "(", "(", "d", ",", "seg_reshaped", ")", ")", ".", "astype", "(", "np", ".", "float32", ")", "\n", "", "\"\"\"There is a problem with python process communication that prevents us from communicating obejcts \n            larger than 2 GB between processes (basically when the length of the pickle string that will be sent is \n            communicated by the multiprocessing.Pipe object then the placeholder (\\%i I think) does not allow for long \n            enough strings (lol). This could be fixed by changing i to l (for long) but that would require manually \n            patching system python code. We circumvent that problem here by saving softmax_pred to a npy file that will \n            then be read (and finally deleted) by the Process. save_segmentation_nifti_from_softmax can take either \n            filename or np.ndarray and will handle this automatically\"\"\"", "\n", "print", "(", "d", ".", "shape", ")", "\n", "if", "np", ".", "prod", "(", "d", ".", "shape", ")", ">", "(", "2e9", "/", "4", "*", "0.85", ")", ":", "# *0.85 just to be save, 4 because float32 is 4 bytes", "\n", "                ", "print", "(", "\n", "\"This output is too large for python process-process communication. \"", "\n", "\"Saving output temporarily to disk\"", ")", "\n", "np", ".", "save", "(", "output_file", "[", ":", "-", "7", "]", "+", "\".npy\"", ",", "d", ")", "\n", "d", "=", "output_file", "[", ":", "-", "7", "]", "+", "\".npy\"", "\n", "", "q", ".", "put", "(", "(", "output_file", ",", "(", "d", ",", "dct", ")", ")", ")", "\n", "", "except", "KeyboardInterrupt", ":", "\n", "            ", "raise", "KeyboardInterrupt", "\n", "", "except", "Exception", "as", "e", ":", "\n", "            ", "print", "(", "\"error in\"", ",", "l", ")", "\n", "print", "(", "e", ")", "\n", "", "", "q", ".", "put", "(", "\"end\"", ")", "\n", "if", "len", "(", "errors_in", ")", ">", "0", ":", "\n", "        ", "print", "(", "\"There were some errors in the following cases:\"", ",", "errors_in", ")", "\n", "print", "(", "\"These cases were ignored.\"", ")", "\n", "", "else", ":", "\n", "        ", "print", "(", "\"This worker has ended successfully, no errors to report\"", ")", "\n", "# restore output", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.inference.predict.preprocess_multithreaded": [[93, 129], ["min", "list", "isinstance", "multiprocessing.Queue", "range", "len", "range", "multiprocessing.Process", "multiprocessing.Process.start", "processes.append", "multiprocessing.Queue.close", "len", "multiprocessing.Queue.get", "p.is_alive", "p.join", "p.terminate"], "function", ["None"], ["", "", "def", "preprocess_multithreaded", "(", "trainer", ",", "list_of_lists", ",", "output_files", ",", "num_processes", "=", "2", ",", "segs_from_prev_stage", "=", "None", ")", ":", "\n", "    ", "if", "segs_from_prev_stage", "is", "None", ":", "\n", "        ", "segs_from_prev_stage", "=", "[", "None", "]", "*", "len", "(", "list_of_lists", ")", "\n", "\n", "", "num_processes", "=", "min", "(", "len", "(", "list_of_lists", ")", ",", "num_processes", ")", "\n", "\n", "classes", "=", "list", "(", "range", "(", "1", ",", "trainer", ".", "num_classes", ")", ")", "\n", "assert", "isinstance", "(", "trainer", ",", "nnUNetTrainer", ")", "\n", "q", "=", "Queue", "(", "1", ")", "\n", "processes", "=", "[", "]", "\n", "for", "i", "in", "range", "(", "num_processes", ")", ":", "\n", "        ", "pr", "=", "Process", "(", "target", "=", "preprocess_save_to_queue", ",", "args", "=", "(", "trainer", ".", "preprocess_patient", ",", "q", ",", "\n", "list_of_lists", "[", "i", ":", ":", "num_processes", "]", ",", "\n", "output_files", "[", "i", ":", ":", "num_processes", "]", ",", "\n", "segs_from_prev_stage", "[", "i", ":", ":", "num_processes", "]", ",", "\n", "classes", ",", "trainer", ".", "plans", "[", "'transpose_forward'", "]", ")", ")", "\n", "pr", ".", "start", "(", ")", "\n", "processes", ".", "append", "(", "pr", ")", "\n", "\n", "", "try", ":", "\n", "        ", "end_ctr", "=", "0", "\n", "while", "end_ctr", "!=", "num_processes", ":", "\n", "            ", "item", "=", "q", ".", "get", "(", ")", "\n", "if", "item", "==", "\"end\"", ":", "\n", "                ", "end_ctr", "+=", "1", "\n", "continue", "\n", "", "else", ":", "\n", "                ", "yield", "item", "\n", "\n", "", "", "", "finally", ":", "\n", "        ", "for", "p", "in", "processes", ":", "\n", "            ", "if", "p", ".", "is_alive", "(", ")", ":", "\n", "                ", "p", ".", "terminate", "(", ")", "# this should not happen but better safe than sorry right", "\n", "", "p", ".", "join", "(", ")", "\n", "\n", "", "q", ".", "close", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.inference.predict.predict_cases": [[131, 283], ["multiprocessing.Pool", "print", "torch.cuda.empty_cache", "print", "nnunet.training.model_restore.load_model_and_checkpoint_files", "print", "predict.preprocess_multithreaded", "print", "print", "join", "isfile", "multiprocessing.Pool.close", "multiprocessing.Pool.join", "len", "len", "os.path.split", "cleaned_output_files.append", "print", "print", "all_output_files.append", "isinstance", "print", "numpy.vstack", "numpy.mean", "trainer.plans.get", "hasattr", "results.append", "i.get", "print", "shutil.copy", "nnunet.postprocessing.connected_components.load_postprocessing", "results.append", "print", "len", "len", "len", "maybe_mkdir_p", "f.endswith", "os.path.splitext", "join", "len", "len", "trainer.plans.keys", "numpy.load", "os.remove", "trainer.load_checkpoint_ram", "np.vstack.append", "trainer.plans.get", "softmax_mean.transpose.transpose", "numpy.prod", "print", "numpy.save", "multiprocessing.Pool.starmap_async", "os.path.abspath", "multiprocessing.Pool.starmap_async", "i.get", "enumerate", "os.path.dirname", "zip", "isfile", "trainer.predict_preprocessed_data_return_seg_and_softmax", "len", "len"], "function", ["home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.training.model_restore.load_model_and_checkpoint_files", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.inference.predict.preprocess_multithreaded", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.postprocessing.connected_components.load_postprocessing", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.loss_function.nnUNetTrainerV2_graduallyTransitionFromCEToDice.nnUNetTrainerV2_graduallyTransitionFromCEToDice.load_checkpoint_ram", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.architectural_variants.nnUNetTrainerV2_ResencUNet.nnUNetTrainerV2_ResencUNet.predict_preprocessed_data_return_seg_and_softmax"], ["", "", "def", "predict_cases", "(", "model", ",", "list_of_lists", ",", "output_filenames", ",", "folds", ",", "save_npz", ",", "num_threads_preprocessing", ",", "\n", "num_threads_nifti_save", ",", "segs_from_prev_stage", "=", "None", ",", "do_tta", "=", "True", ",", "mixed_precision", "=", "True", ",", "overwrite_existing", "=", "False", ",", "\n", "all_in_gpu", "=", "False", ",", "step_size", "=", "0.5", ",", "checkpoint_name", "=", "\"model_final_checkpoint\"", ",", "\n", "segmentation_export_kwargs", ":", "dict", "=", "None", ")", ":", "\n", "    ", "\"\"\"\n    :param segmentation_export_kwargs:\n    :param model: folder where the model is saved, must contain fold_x subfolders\n    :param list_of_lists: [[case0_0000.nii.gz, case0_0001.nii.gz], [case1_0000.nii.gz, case1_0001.nii.gz], ...]\n    :param output_filenames: [output_file_case0.nii.gz, output_file_case1.nii.gz, ...]\n    :param folds: default: (0, 1, 2, 3, 4) (but can also be 'all' or a subset of the five folds, for example use (0, )\n    for using only fold_0\n    :param save_npz: default: False\n    :param num_threads_preprocessing:\n    :param num_threads_nifti_save:\n    :param segs_from_prev_stage:\n    :param do_tta: default: True, can be set to False for a 8x speedup at the cost of a reduced segmentation quality\n    :param overwrite_existing: default: True\n    :param mixed_precision: if None then we take no action. If True/False we overwrite what the model has in its init\n    :return:\n    \"\"\"", "\n", "assert", "len", "(", "list_of_lists", ")", "==", "len", "(", "output_filenames", ")", "\n", "if", "segs_from_prev_stage", "is", "not", "None", ":", "assert", "len", "(", "segs_from_prev_stage", ")", "==", "len", "(", "output_filenames", ")", "\n", "\n", "pool", "=", "Pool", "(", "num_threads_nifti_save", ")", "\n", "results", "=", "[", "]", "\n", "\n", "cleaned_output_files", "=", "[", "]", "\n", "for", "o", "in", "output_filenames", ":", "\n", "        ", "dr", ",", "f", "=", "os", ".", "path", ".", "split", "(", "o", ")", "\n", "if", "len", "(", "dr", ")", ">", "0", ":", "\n", "            ", "maybe_mkdir_p", "(", "dr", ")", "\n", "", "if", "not", "f", ".", "endswith", "(", "\".nii.gz\"", ")", ":", "\n", "            ", "f", ",", "_", "=", "os", ".", "path", ".", "splitext", "(", "f", ")", "\n", "f", "=", "f", "+", "\".nii.gz\"", "\n", "", "cleaned_output_files", ".", "append", "(", "join", "(", "dr", ",", "f", ")", ")", "\n", "\n", "", "if", "not", "overwrite_existing", ":", "\n", "        ", "print", "(", "\"number of cases:\"", ",", "len", "(", "list_of_lists", ")", ")", "\n", "not_done_idx", "=", "[", "i", "for", "i", ",", "j", "in", "enumerate", "(", "cleaned_output_files", ")", "if", "not", "isfile", "(", "j", ")", "]", "\n", "\n", "cleaned_output_files", "=", "[", "cleaned_output_files", "[", "i", "]", "for", "i", "in", "not_done_idx", "]", "\n", "list_of_lists", "=", "[", "list_of_lists", "[", "i", "]", "for", "i", "in", "not_done_idx", "]", "\n", "if", "segs_from_prev_stage", "is", "not", "None", ":", "\n", "            ", "segs_from_prev_stage", "=", "[", "segs_from_prev_stage", "[", "i", "]", "for", "i", "in", "not_done_idx", "]", "\n", "\n", "", "print", "(", "\"number of cases that still need to be predicted:\"", ",", "len", "(", "cleaned_output_files", ")", ")", "\n", "\n", "", "print", "(", "\"emptying cuda cache\"", ")", "\n", "torch", ".", "cuda", ".", "empty_cache", "(", ")", "\n", "\n", "print", "(", "\"loading parameters for folds,\"", ",", "folds", ")", "\n", "trainer", ",", "params", "=", "load_model_and_checkpoint_files", "(", "model", ",", "folds", ",", "mixed_precision", "=", "mixed_precision", ",", "checkpoint_name", "=", "checkpoint_name", ")", "\n", "\n", "if", "segmentation_export_kwargs", "is", "None", ":", "\n", "        ", "if", "'segmentation_export_params'", "in", "trainer", ".", "plans", ".", "keys", "(", ")", ":", "\n", "            ", "force_separate_z", "=", "trainer", ".", "plans", "[", "'segmentation_export_params'", "]", "[", "'force_separate_z'", "]", "\n", "interpolation_order", "=", "trainer", ".", "plans", "[", "'segmentation_export_params'", "]", "[", "'interpolation_order'", "]", "\n", "interpolation_order_z", "=", "trainer", ".", "plans", "[", "'segmentation_export_params'", "]", "[", "'interpolation_order_z'", "]", "\n", "", "else", ":", "\n", "            ", "force_separate_z", "=", "None", "\n", "interpolation_order", "=", "1", "\n", "interpolation_order_z", "=", "0", "\n", "", "", "else", ":", "\n", "        ", "force_separate_z", "=", "segmentation_export_kwargs", "[", "'force_separate_z'", "]", "\n", "interpolation_order", "=", "segmentation_export_kwargs", "[", "'interpolation_order'", "]", "\n", "interpolation_order_z", "=", "segmentation_export_kwargs", "[", "'interpolation_order_z'", "]", "\n", "\n", "", "print", "(", "\"starting preprocessing generator\"", ")", "\n", "preprocessing", "=", "preprocess_multithreaded", "(", "trainer", ",", "list_of_lists", ",", "cleaned_output_files", ",", "num_threads_preprocessing", ",", "\n", "segs_from_prev_stage", ")", "\n", "print", "(", "\"starting prediction...\"", ")", "\n", "all_output_files", "=", "[", "]", "\n", "for", "preprocessed", "in", "preprocessing", ":", "\n", "        ", "output_filename", ",", "(", "d", ",", "dct", ")", "=", "preprocessed", "\n", "all_output_files", ".", "append", "(", "all_output_files", ")", "\n", "if", "isinstance", "(", "d", ",", "str", ")", ":", "\n", "            ", "data", "=", "np", ".", "load", "(", "d", ")", "\n", "os", ".", "remove", "(", "d", ")", "\n", "d", "=", "data", "\n", "\n", "", "print", "(", "\"predicting\"", ",", "output_filename", ")", "\n", "softmax", "=", "[", "]", "\n", "for", "p", "in", "params", ":", "\n", "            ", "trainer", ".", "load_checkpoint_ram", "(", "p", ",", "False", ")", "\n", "softmax", ".", "append", "(", "trainer", ".", "predict_preprocessed_data_return_seg_and_softmax", "(", "d", ",", "do_tta", ",", "trainer", ".", "data_aug_params", "[", "\n", "'mirror_axes'", "]", ",", "True", ",", "step_size", "=", "step_size", ",", "use_gaussian", "=", "True", ",", "all_in_gpu", "=", "all_in_gpu", ",", "\n", "mixed_precision", "=", "mixed_precision", ")", "[", "1", "]", "[", "None", "]", ")", "\n", "\n", "", "softmax", "=", "np", ".", "vstack", "(", "softmax", ")", "\n", "softmax_mean", "=", "np", ".", "mean", "(", "softmax", ",", "0", ")", "\n", "\n", "transpose_forward", "=", "trainer", ".", "plans", ".", "get", "(", "'transpose_forward'", ")", "\n", "if", "transpose_forward", "is", "not", "None", ":", "\n", "            ", "transpose_backward", "=", "trainer", ".", "plans", ".", "get", "(", "'transpose_backward'", ")", "\n", "softmax_mean", "=", "softmax_mean", ".", "transpose", "(", "[", "0", "]", "+", "[", "i", "+", "1", "for", "i", "in", "transpose_backward", "]", ")", "\n", "\n", "", "if", "save_npz", ":", "\n", "            ", "npz_file", "=", "output_filename", "[", ":", "-", "7", "]", "+", "\".npz\"", "\n", "", "else", ":", "\n", "            ", "npz_file", "=", "None", "\n", "\n", "", "if", "hasattr", "(", "trainer", ",", "'regions_class_order'", ")", ":", "\n", "            ", "region_class_order", "=", "trainer", ".", "regions_class_order", "\n", "", "else", ":", "\n", "            ", "region_class_order", "=", "None", "\n", "\n", "", "\"\"\"There is a problem with python process communication that prevents us from communicating obejcts \n        larger than 2 GB between processes (basically when the length of the pickle string that will be sent is \n        communicated by the multiprocessing.Pipe object then the placeholder (\\%i I think) does not allow for long \n        enough strings (lol). This could be fixed by changing i to l (for long) but that would require manually \n        patching system python code. We circumvent that problem here by saving softmax_pred to a npy file that will \n        then be read (and finally deleted) by the Process. save_segmentation_nifti_from_softmax can take either \n        filename or np.ndarray and will handle this automatically\"\"\"", "\n", "bytes_per_voxel", "=", "4", "\n", "if", "all_in_gpu", ":", "\n", "            ", "bytes_per_voxel", "=", "2", "# if all_in_gpu then the return value is half (float16)", "\n", "", "if", "np", ".", "prod", "(", "softmax_mean", ".", "shape", ")", ">", "(", "2e9", "/", "bytes_per_voxel", "*", "0.85", ")", ":", "# * 0.85 just to be save", "\n", "            ", "print", "(", "\n", "\"This output is too large for python process-process communication. Saving output temporarily to disk\"", ")", "\n", "np", ".", "save", "(", "output_filename", "[", ":", "-", "7", "]", "+", "\".npy\"", ",", "softmax_mean", ")", "\n", "softmax_mean", "=", "output_filename", "[", ":", "-", "7", "]", "+", "\".npy\"", "\n", "\n", "", "results", ".", "append", "(", "pool", ".", "starmap_async", "(", "save_segmentation_nifti_from_softmax", ",", "\n", "(", "(", "softmax_mean", ",", "output_filename", ",", "dct", ",", "interpolation_order", ",", "region_class_order", ",", "\n", "None", ",", "None", ",", "\n", "npz_file", ",", "None", ",", "force_separate_z", ",", "interpolation_order_z", ")", ",", ")", "\n", ")", ")", "\n", "\n", "", "print", "(", "\"inference done. Now waiting for the segmentation export to finish...\"", ")", "\n", "_", "=", "[", "i", ".", "get", "(", ")", "for", "i", "in", "results", "]", "\n", "# now apply postprocessing", "\n", "# first load the postprocessing properties if they are present. Else raise a well visible warning", "\n", "results", "=", "[", "]", "\n", "pp_file", "=", "join", "(", "model", ",", "\"postprocessing.json\"", ")", "\n", "if", "isfile", "(", "pp_file", ")", ":", "\n", "        ", "print", "(", "\"postprocessing...\"", ")", "\n", "shutil", ".", "copy", "(", "pp_file", ",", "os", ".", "path", ".", "abspath", "(", "os", ".", "path", ".", "dirname", "(", "output_filenames", "[", "0", "]", ")", ")", ")", "\n", "# for_which_classes stores for which of the classes everything but the largest connected component needs to be", "\n", "# removed", "\n", "for_which_classes", ",", "min_valid_obj_size", "=", "load_postprocessing", "(", "pp_file", ")", "\n", "results", ".", "append", "(", "pool", ".", "starmap_async", "(", "load_remove_save", ",", "\n", "zip", "(", "output_filenames", ",", "output_filenames", ",", "\n", "[", "for_which_classes", "]", "*", "len", "(", "output_filenames", ")", ",", "\n", "[", "min_valid_obj_size", "]", "*", "len", "(", "output_filenames", ")", ")", ")", ")", "\n", "_", "=", "[", "i", ".", "get", "(", ")", "for", "i", "in", "results", "]", "\n", "", "else", ":", "\n", "        ", "print", "(", "\"WARNING! Cannot run postprocessing because the postprocessing file is missing. Make sure to run \"", "\n", "\"consolidate_folds in the output folder of the model first!\\nThe folder you need to run this in is \"", "\n", "\"%s\"", "%", "model", ")", "\n", "\n", "", "pool", ".", "close", "(", ")", "\n", "pool", ".", "join", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.inference.predict.predict_cases_fast": [[285, 420], ["multiprocessing.Pool", "print", "torch.cuda.empty_cache", "print", "nnunet.training.model_restore.load_model_and_checkpoint_files", "print", "predict.preprocess_multithreaded", "print", "print", "join", "isfile", "multiprocessing.Pool.close", "multiprocessing.Pool.join", "len", "len", "os.path.split", "cleaned_output_files.append", "print", "print", "print", "print", "isinstance", "numpy.zeros", "print", "enumerate", "print", "print", "trainer.plans.get", "print", "results.append", "print", "i.get", "print", "shutil.copy", "nnunet.postprocessing.connected_components.load_postprocessing", "results.append", "print", "len", "len", "len", "maybe_mkdir_p", "f.endswith", "os.path.splitext", "join", "len", "len", "trainer.plans.keys", "print", "numpy.load", "os.remove", "trainer.load_checkpoint_ram", "trainer.predict_preprocessed_data_return_seg_and_softmax", "len", "softmax_aggr.argmax", "trainer.plans.get", "seg.transpose.transpose", "multiprocessing.Pool.starmap_async", "os.path.dirname", "multiprocessing.Pool.starmap_async", "i.get", "enumerate", "len", "len", "print", "zip", "isfile", "len", "len"], "function", ["home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.training.model_restore.load_model_and_checkpoint_files", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.inference.predict.preprocess_multithreaded", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.postprocessing.connected_components.load_postprocessing", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.loss_function.nnUNetTrainerV2_graduallyTransitionFromCEToDice.nnUNetTrainerV2_graduallyTransitionFromCEToDice.load_checkpoint_ram", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.architectural_variants.nnUNetTrainerV2_ResencUNet.nnUNetTrainerV2_ResencUNet.predict_preprocessed_data_return_seg_and_softmax"], ["", "def", "predict_cases_fast", "(", "model", ",", "list_of_lists", ",", "output_filenames", ",", "folds", ",", "num_threads_preprocessing", ",", "\n", "num_threads_nifti_save", ",", "segs_from_prev_stage", "=", "None", ",", "do_tta", "=", "True", ",", "mixed_precision", "=", "True", ",", "\n", "overwrite_existing", "=", "False", ",", "\n", "all_in_gpu", "=", "False", ",", "step_size", "=", "0.5", ",", "checkpoint_name", "=", "\"model_final_checkpoint\"", ",", "\n", "segmentation_export_kwargs", ":", "dict", "=", "None", ")", ":", "\n", "    ", "assert", "len", "(", "list_of_lists", ")", "==", "len", "(", "output_filenames", ")", "\n", "if", "segs_from_prev_stage", "is", "not", "None", ":", "assert", "len", "(", "segs_from_prev_stage", ")", "==", "len", "(", "output_filenames", ")", "\n", "\n", "pool", "=", "Pool", "(", "num_threads_nifti_save", ")", "\n", "results", "=", "[", "]", "\n", "\n", "cleaned_output_files", "=", "[", "]", "\n", "for", "o", "in", "output_filenames", ":", "\n", "        ", "dr", ",", "f", "=", "os", ".", "path", ".", "split", "(", "o", ")", "\n", "if", "len", "(", "dr", ")", ">", "0", ":", "\n", "            ", "maybe_mkdir_p", "(", "dr", ")", "\n", "", "if", "not", "f", ".", "endswith", "(", "\".nii.gz\"", ")", ":", "\n", "            ", "f", ",", "_", "=", "os", ".", "path", ".", "splitext", "(", "f", ")", "\n", "f", "=", "f", "+", "\".nii.gz\"", "\n", "", "cleaned_output_files", ".", "append", "(", "join", "(", "dr", ",", "f", ")", ")", "\n", "\n", "", "if", "not", "overwrite_existing", ":", "\n", "        ", "print", "(", "\"number of cases:\"", ",", "len", "(", "list_of_lists", ")", ")", "\n", "not_done_idx", "=", "[", "i", "for", "i", ",", "j", "in", "enumerate", "(", "cleaned_output_files", ")", "if", "not", "isfile", "(", "j", ")", "]", "\n", "\n", "cleaned_output_files", "=", "[", "cleaned_output_files", "[", "i", "]", "for", "i", "in", "not_done_idx", "]", "\n", "list_of_lists", "=", "[", "list_of_lists", "[", "i", "]", "for", "i", "in", "not_done_idx", "]", "\n", "if", "segs_from_prev_stage", "is", "not", "None", ":", "\n", "            ", "segs_from_prev_stage", "=", "[", "segs_from_prev_stage", "[", "i", "]", "for", "i", "in", "not_done_idx", "]", "\n", "\n", "", "print", "(", "\"number of cases that still need to be predicted:\"", ",", "len", "(", "cleaned_output_files", ")", ")", "\n", "\n", "", "print", "(", "\"emptying cuda cache\"", ")", "\n", "torch", ".", "cuda", ".", "empty_cache", "(", ")", "\n", "\n", "print", "(", "\"loading parameters for folds,\"", ",", "folds", ")", "\n", "trainer", ",", "params", "=", "load_model_and_checkpoint_files", "(", "model", ",", "folds", ",", "mixed_precision", "=", "mixed_precision", ",", "checkpoint_name", "=", "checkpoint_name", ")", "\n", "\n", "if", "segmentation_export_kwargs", "is", "None", ":", "\n", "        ", "if", "'segmentation_export_params'", "in", "trainer", ".", "plans", ".", "keys", "(", ")", ":", "\n", "            ", "force_separate_z", "=", "trainer", ".", "plans", "[", "'segmentation_export_params'", "]", "[", "'force_separate_z'", "]", "\n", "interpolation_order", "=", "trainer", ".", "plans", "[", "'segmentation_export_params'", "]", "[", "'interpolation_order'", "]", "\n", "interpolation_order_z", "=", "trainer", ".", "plans", "[", "'segmentation_export_params'", "]", "[", "'interpolation_order_z'", "]", "\n", "", "else", ":", "\n", "            ", "force_separate_z", "=", "None", "\n", "interpolation_order", "=", "1", "\n", "interpolation_order_z", "=", "0", "\n", "", "", "else", ":", "\n", "        ", "force_separate_z", "=", "segmentation_export_kwargs", "[", "'force_separate_z'", "]", "\n", "interpolation_order", "=", "segmentation_export_kwargs", "[", "'interpolation_order'", "]", "\n", "interpolation_order_z", "=", "segmentation_export_kwargs", "[", "'interpolation_order_z'", "]", "\n", "\n", "", "print", "(", "\"starting preprocessing generator\"", ")", "\n", "preprocessing", "=", "preprocess_multithreaded", "(", "trainer", ",", "list_of_lists", ",", "cleaned_output_files", ",", "num_threads_preprocessing", ",", "\n", "segs_from_prev_stage", ")", "\n", "\n", "print", "(", "\"starting prediction...\"", ")", "\n", "for", "preprocessed", "in", "preprocessing", ":", "\n", "        ", "print", "(", "\"getting data from preprocessor\"", ")", "\n", "output_filename", ",", "(", "d", ",", "dct", ")", "=", "preprocessed", "\n", "print", "(", "\"got something\"", ")", "\n", "if", "isinstance", "(", "d", ",", "str", ")", ":", "\n", "            ", "print", "(", "\"what I got is a string, so I need to load a file\"", ")", "\n", "data", "=", "np", ".", "load", "(", "d", ")", "\n", "os", ".", "remove", "(", "d", ")", "\n", "d", "=", "data", "\n", "\n", "# preallocate the output arrays", "\n", "# same dtype as the return value in predict_preprocessed_data_return_seg_and_softmax (saves time)", "\n", "", "softmax_aggr", "=", "None", "# np.zeros((trainer.num_classes, *d.shape[1:]), dtype=np.float16)", "\n", "all_seg_outputs", "=", "np", ".", "zeros", "(", "(", "len", "(", "params", ")", ",", "*", "d", ".", "shape", "[", "1", ":", "]", ")", ",", "dtype", "=", "int", ")", "\n", "print", "(", "\"predicting\"", ",", "output_filename", ")", "\n", "\n", "for", "i", ",", "p", "in", "enumerate", "(", "params", ")", ":", "\n", "            ", "trainer", ".", "load_checkpoint_ram", "(", "p", ",", "False", ")", "\n", "\n", "res", "=", "trainer", ".", "predict_preprocessed_data_return_seg_and_softmax", "(", "d", ",", "do_tta", ",", "\n", "trainer", ".", "data_aug_params", "[", "'mirror_axes'", "]", ",", "True", ",", "\n", "step_size", "=", "step_size", ",", "use_gaussian", "=", "True", ",", "\n", "all_in_gpu", "=", "all_in_gpu", ",", "\n", "mixed_precision", "=", "mixed_precision", ")", "\n", "\n", "if", "len", "(", "params", ")", ">", "1", ":", "\n", "# otherwise we dont need this and we can save ourselves the time it takes to copy that", "\n", "                ", "print", "(", "\"aggregating softmax\"", ")", "\n", "if", "softmax_aggr", "is", "None", ":", "\n", "                    ", "softmax_aggr", "=", "res", "[", "1", "]", "\n", "", "else", ":", "\n", "                    ", "softmax_aggr", "+=", "res", "[", "1", "]", "\n", "", "", "all_seg_outputs", "[", "i", "]", "=", "res", "[", "0", "]", "\n", "\n", "", "print", "(", "\"obtaining segmentation map\"", ")", "\n", "if", "len", "(", "params", ")", ">", "1", ":", "\n", "# we dont need to normalize the softmax by 1 / len(params) because this would not change the outcome of the argmax", "\n", "            ", "seg", "=", "softmax_aggr", ".", "argmax", "(", "0", ")", "\n", "", "else", ":", "\n", "            ", "seg", "=", "all_seg_outputs", "[", "0", "]", "\n", "\n", "", "print", "(", "\"applying transpose_backward\"", ")", "\n", "transpose_forward", "=", "trainer", ".", "plans", ".", "get", "(", "'transpose_forward'", ")", "\n", "if", "transpose_forward", "is", "not", "None", ":", "\n", "            ", "transpose_backward", "=", "trainer", ".", "plans", ".", "get", "(", "'transpose_backward'", ")", "\n", "seg", "=", "seg", ".", "transpose", "(", "[", "i", "for", "i", "in", "transpose_backward", "]", ")", "\n", "\n", "", "print", "(", "\"initializing segmentation export\"", ")", "\n", "results", ".", "append", "(", "pool", ".", "starmap_async", "(", "save_segmentation_nifti", ",", "\n", "(", "(", "seg", ",", "output_filename", ",", "dct", ",", "interpolation_order", ",", "force_separate_z", ",", "\n", "interpolation_order_z", ")", ",", ")", "\n", ")", ")", "\n", "print", "(", "\"done\"", ")", "\n", "\n", "", "print", "(", "\"inference done. Now waiting for the segmentation export to finish...\"", ")", "\n", "_", "=", "[", "i", ".", "get", "(", ")", "for", "i", "in", "results", "]", "\n", "# now apply postprocessing", "\n", "# first load the postprocessing properties if they are present. Else raise a well visible warning", "\n", "results", "=", "[", "]", "\n", "pp_file", "=", "join", "(", "model", ",", "\"postprocessing.json\"", ")", "\n", "if", "isfile", "(", "pp_file", ")", ":", "\n", "        ", "print", "(", "\"postprocessing...\"", ")", "\n", "shutil", ".", "copy", "(", "pp_file", ",", "os", ".", "path", ".", "dirname", "(", "output_filenames", "[", "0", "]", ")", ")", "\n", "# for_which_classes stores for which of the classes everything but the largest connected component needs to be", "\n", "# removed", "\n", "for_which_classes", ",", "min_valid_obj_size", "=", "load_postprocessing", "(", "pp_file", ")", "\n", "results", ".", "append", "(", "pool", ".", "starmap_async", "(", "load_remove_save", ",", "\n", "zip", "(", "output_filenames", ",", "output_filenames", ",", "\n", "[", "for_which_classes", "]", "*", "len", "(", "output_filenames", ")", ",", "\n", "[", "min_valid_obj_size", "]", "*", "len", "(", "output_filenames", ")", ")", ")", ")", "\n", "_", "=", "[", "i", ".", "get", "(", ")", "for", "i", "in", "results", "]", "\n", "", "else", ":", "\n", "        ", "print", "(", "\"WARNING! Cannot run postprocessing because the postprocessing file is missing. Make sure to run \"", "\n", "\"consolidate_folds in the output folder of the model first!\\nThe folder you need to run this in is \"", "\n", "\"%s\"", "%", "model", ")", "\n", "\n", "", "pool", ".", "close", "(", ")", "\n", "pool", ".", "join", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.inference.predict.predict_cases_fastest": [[422, 535], ["multiprocessing.Pool", "print", "torch.cuda.empty_cache", "print", "nnunet.training.model_restore.load_model_and_checkpoint_files", "print", "predict.preprocess_multithreaded", "print", "print", "join", "isfile", "multiprocessing.Pool.close", "multiprocessing.Pool.join", "len", "len", "os.path.split", "cleaned_output_files.append", "print", "print", "print", "print", "isinstance", "numpy.zeros", "numpy.zeros", "print", "enumerate", "print", "print", "trainer.plans.get", "print", "results.append", "print", "i.get", "print", "shutil.copy", "nnunet.postprocessing.connected_components.load_postprocessing", "results.append", "print", "len", "len", "len", "maybe_mkdir_p", "f.endswith", "os.path.splitext", "join", "len", "len", "print", "numpy.load", "os.remove", "trainer.load_checkpoint_ram", "trainer.predict_preprocessed_data_return_seg_and_softmax", "len", "numpy.mean", "np.mean.argmax", "trainer.plans.get", "seg.transpose.transpose", "multiprocessing.Pool.starmap_async", "os.path.dirname", "multiprocessing.Pool.starmap_async", "i.get", "enumerate", "len", "len", "len", "zip", "isfile", "len", "len"], "function", ["home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.training.model_restore.load_model_and_checkpoint_files", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.inference.predict.preprocess_multithreaded", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.postprocessing.connected_components.load_postprocessing", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.loss_function.nnUNetTrainerV2_graduallyTransitionFromCEToDice.nnUNetTrainerV2_graduallyTransitionFromCEToDice.load_checkpoint_ram", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.architectural_variants.nnUNetTrainerV2_ResencUNet.nnUNetTrainerV2_ResencUNet.predict_preprocessed_data_return_seg_and_softmax"], ["", "def", "predict_cases_fastest", "(", "model", ",", "list_of_lists", ",", "output_filenames", ",", "folds", ",", "num_threads_preprocessing", ",", "\n", "num_threads_nifti_save", ",", "segs_from_prev_stage", "=", "None", ",", "do_tta", "=", "True", ",", "mixed_precision", "=", "True", ",", "\n", "overwrite_existing", "=", "False", ",", "all_in_gpu", "=", "True", ",", "step_size", "=", "0.5", ",", "\n", "checkpoint_name", "=", "\"model_final_checkpoint\"", ")", ":", "\n", "    ", "assert", "len", "(", "list_of_lists", ")", "==", "len", "(", "output_filenames", ")", "\n", "if", "segs_from_prev_stage", "is", "not", "None", ":", "assert", "len", "(", "segs_from_prev_stage", ")", "==", "len", "(", "output_filenames", ")", "\n", "\n", "pool", "=", "Pool", "(", "num_threads_nifti_save", ")", "\n", "results", "=", "[", "]", "\n", "\n", "cleaned_output_files", "=", "[", "]", "\n", "for", "o", "in", "output_filenames", ":", "\n", "        ", "dr", ",", "f", "=", "os", ".", "path", ".", "split", "(", "o", ")", "\n", "if", "len", "(", "dr", ")", ">", "0", ":", "\n", "            ", "maybe_mkdir_p", "(", "dr", ")", "\n", "", "if", "not", "f", ".", "endswith", "(", "\".nii.gz\"", ")", ":", "\n", "            ", "f", ",", "_", "=", "os", ".", "path", ".", "splitext", "(", "f", ")", "\n", "f", "=", "f", "+", "\".nii.gz\"", "\n", "", "cleaned_output_files", ".", "append", "(", "join", "(", "dr", ",", "f", ")", ")", "\n", "\n", "", "if", "not", "overwrite_existing", ":", "\n", "        ", "print", "(", "\"number of cases:\"", ",", "len", "(", "list_of_lists", ")", ")", "\n", "not_done_idx", "=", "[", "i", "for", "i", ",", "j", "in", "enumerate", "(", "cleaned_output_files", ")", "if", "not", "isfile", "(", "j", ")", "]", "\n", "\n", "cleaned_output_files", "=", "[", "cleaned_output_files", "[", "i", "]", "for", "i", "in", "not_done_idx", "]", "\n", "list_of_lists", "=", "[", "list_of_lists", "[", "i", "]", "for", "i", "in", "not_done_idx", "]", "\n", "if", "segs_from_prev_stage", "is", "not", "None", ":", "\n", "            ", "segs_from_prev_stage", "=", "[", "segs_from_prev_stage", "[", "i", "]", "for", "i", "in", "not_done_idx", "]", "\n", "\n", "", "print", "(", "\"number of cases that still need to be predicted:\"", ",", "len", "(", "cleaned_output_files", ")", ")", "\n", "\n", "", "print", "(", "\"emptying cuda cache\"", ")", "\n", "torch", ".", "cuda", ".", "empty_cache", "(", ")", "\n", "\n", "print", "(", "\"loading parameters for folds,\"", ",", "folds", ")", "\n", "trainer", ",", "params", "=", "load_model_and_checkpoint_files", "(", "model", ",", "folds", ",", "mixed_precision", "=", "mixed_precision", ",", "checkpoint_name", "=", "checkpoint_name", ")", "\n", "\n", "print", "(", "\"starting preprocessing generator\"", ")", "\n", "preprocessing", "=", "preprocess_multithreaded", "(", "trainer", ",", "list_of_lists", ",", "cleaned_output_files", ",", "num_threads_preprocessing", ",", "\n", "segs_from_prev_stage", ")", "\n", "\n", "print", "(", "\"starting prediction...\"", ")", "\n", "for", "preprocessed", "in", "preprocessing", ":", "\n", "        ", "print", "(", "\"getting data from preprocessor\"", ")", "\n", "output_filename", ",", "(", "d", ",", "dct", ")", "=", "preprocessed", "\n", "print", "(", "\"got something\"", ")", "\n", "if", "isinstance", "(", "d", ",", "str", ")", ":", "\n", "            ", "print", "(", "\"what I got is a string, so I need to load a file\"", ")", "\n", "data", "=", "np", ".", "load", "(", "d", ")", "\n", "os", ".", "remove", "(", "d", ")", "\n", "d", "=", "data", "\n", "\n", "# preallocate the output arrays", "\n", "# same dtype as the return value in predict_preprocessed_data_return_seg_and_softmax (saves time)", "\n", "", "all_softmax_outputs", "=", "np", ".", "zeros", "(", "(", "len", "(", "params", ")", ",", "trainer", ".", "num_classes", ",", "*", "d", ".", "shape", "[", "1", ":", "]", ")", ",", "dtype", "=", "np", ".", "float16", ")", "\n", "all_seg_outputs", "=", "np", ".", "zeros", "(", "(", "len", "(", "params", ")", ",", "*", "d", ".", "shape", "[", "1", ":", "]", ")", ",", "dtype", "=", "int", ")", "\n", "print", "(", "\"predicting\"", ",", "output_filename", ")", "\n", "\n", "for", "i", ",", "p", "in", "enumerate", "(", "params", ")", ":", "\n", "            ", "trainer", ".", "load_checkpoint_ram", "(", "p", ",", "False", ")", "\n", "res", "=", "trainer", ".", "predict_preprocessed_data_return_seg_and_softmax", "(", "d", ",", "do_tta", ",", "\n", "trainer", ".", "data_aug_params", "[", "'mirror_axes'", "]", ",", "True", ",", "\n", "step_size", "=", "step_size", ",", "use_gaussian", "=", "True", ",", "\n", "all_in_gpu", "=", "all_in_gpu", ",", "\n", "mixed_precision", "=", "mixed_precision", ")", "\n", "if", "len", "(", "params", ")", ">", "1", ":", "\n", "# otherwise we dont need this and we can save ourselves the time it takes to copy that", "\n", "                ", "all_softmax_outputs", "[", "i", "]", "=", "res", "[", "1", "]", "\n", "", "all_seg_outputs", "[", "i", "]", "=", "res", "[", "0", "]", "\n", "\n", "", "print", "(", "\"aggregating predictions\"", ")", "\n", "if", "len", "(", "params", ")", ">", "1", ":", "\n", "            ", "softmax_mean", "=", "np", ".", "mean", "(", "all_softmax_outputs", ",", "0", ")", "\n", "seg", "=", "softmax_mean", ".", "argmax", "(", "0", ")", "\n", "", "else", ":", "\n", "            ", "seg", "=", "all_seg_outputs", "[", "0", "]", "\n", "\n", "", "print", "(", "\"applying transpose_backward\"", ")", "\n", "transpose_forward", "=", "trainer", ".", "plans", ".", "get", "(", "'transpose_forward'", ")", "\n", "if", "transpose_forward", "is", "not", "None", ":", "\n", "            ", "transpose_backward", "=", "trainer", ".", "plans", ".", "get", "(", "'transpose_backward'", ")", "\n", "seg", "=", "seg", ".", "transpose", "(", "[", "i", "for", "i", "in", "transpose_backward", "]", ")", "\n", "\n", "", "print", "(", "\"initializing segmentation export\"", ")", "\n", "results", ".", "append", "(", "pool", ".", "starmap_async", "(", "save_segmentation_nifti", ",", "\n", "(", "(", "seg", ",", "output_filename", ",", "dct", ",", "0", ",", "None", ")", ",", ")", "\n", ")", ")", "\n", "print", "(", "\"done\"", ")", "\n", "\n", "", "print", "(", "\"inference done. Now waiting for the segmentation export to finish...\"", ")", "\n", "_", "=", "[", "i", ".", "get", "(", ")", "for", "i", "in", "results", "]", "\n", "# now apply postprocessing", "\n", "# first load the postprocessing properties if they are present. Else raise a well visible warning", "\n", "results", "=", "[", "]", "\n", "pp_file", "=", "join", "(", "model", ",", "\"postprocessing.json\"", ")", "\n", "if", "isfile", "(", "pp_file", ")", ":", "\n", "        ", "print", "(", "\"postprocessing...\"", ")", "\n", "shutil", ".", "copy", "(", "pp_file", ",", "os", ".", "path", ".", "dirname", "(", "output_filenames", "[", "0", "]", ")", ")", "\n", "# for_which_classes stores for which of the classes everything but the largest connected component needs to be", "\n", "# removed", "\n", "for_which_classes", ",", "min_valid_obj_size", "=", "load_postprocessing", "(", "pp_file", ")", "\n", "results", ".", "append", "(", "pool", ".", "starmap_async", "(", "load_remove_save", ",", "\n", "zip", "(", "output_filenames", ",", "output_filenames", ",", "\n", "[", "for_which_classes", "]", "*", "len", "(", "output_filenames", ")", ",", "\n", "[", "min_valid_obj_size", "]", "*", "len", "(", "output_filenames", ")", ")", ")", ")", "\n", "_", "=", "[", "i", ".", "get", "(", ")", "for", "i", "in", "results", "]", "\n", "", "else", ":", "\n", "        ", "print", "(", "\"WARNING! Cannot run postprocessing because the postprocessing file is missing. Make sure to run \"", "\n", "\"consolidate_folds in the output folder of the model first!\\nThe folder you need to run this in is \"", "\n", "\"%s\"", "%", "model", ")", "\n", "\n", "", "pool", ".", "close", "(", ")", "\n", "pool", ".", "join", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.inference.predict.check_input_folder_and_return_caseIDs": [[537, 571], ["print", "subfiles", "numpy.unique", "copy.deepcopy", "print", "print", "len", "range", "numpy.random.choice", "len", "print", "len", "print", "print", "RuntimeError", "len", "min", "numpy.random.choice", "isfile", "missing.append", "copy.deepcopy.remove", "len", "len", "min", "join", "len"], "function", ["None"], ["", "def", "check_input_folder_and_return_caseIDs", "(", "input_folder", ",", "expected_num_modalities", ")", ":", "\n", "    ", "print", "(", "\"This model expects %d input modalities for each image\"", "%", "expected_num_modalities", ")", "\n", "files", "=", "subfiles", "(", "input_folder", ",", "suffix", "=", "\".nii.gz\"", ",", "join", "=", "False", ",", "sort", "=", "True", ")", "\n", "\n", "maybe_case_ids", "=", "np", ".", "unique", "(", "[", "i", "[", ":", "-", "12", "]", "for", "i", "in", "files", "]", ")", "\n", "\n", "remaining", "=", "deepcopy", "(", "files", ")", "\n", "missing", "=", "[", "]", "\n", "\n", "assert", "len", "(", "files", ")", ">", "0", ",", "\"input folder did not contain any images (expected to find .nii.gz file endings)\"", "\n", "\n", "# now check if all required files are present and that no unexpected files are remaining", "\n", "for", "c", "in", "maybe_case_ids", ":", "\n", "        ", "for", "n", "in", "range", "(", "expected_num_modalities", ")", ":", "\n", "            ", "expected_output_file", "=", "c", "+", "\"_%04.0d.nii.gz\"", "%", "n", "\n", "if", "not", "isfile", "(", "join", "(", "input_folder", ",", "expected_output_file", ")", ")", ":", "\n", "                ", "missing", ".", "append", "(", "expected_output_file", ")", "\n", "", "else", ":", "\n", "                ", "remaining", ".", "remove", "(", "expected_output_file", ")", "\n", "\n", "", "", "", "print", "(", "\"Found %d unique case ids, here are some examples:\"", "%", "len", "(", "maybe_case_ids", ")", ",", "\n", "np", ".", "random", ".", "choice", "(", "maybe_case_ids", ",", "min", "(", "len", "(", "maybe_case_ids", ")", ",", "10", ")", ")", ")", "\n", "print", "(", "\"If they don't look right, make sure to double check your filenames. They must end with _0000.nii.gz etc\"", ")", "\n", "\n", "if", "len", "(", "remaining", ")", ">", "0", ":", "\n", "        ", "print", "(", "\"found %d unexpected remaining files in the folder. Here are some examples:\"", "%", "len", "(", "remaining", ")", ",", "\n", "np", ".", "random", ".", "choice", "(", "remaining", ",", "min", "(", "len", "(", "remaining", ")", ",", "10", ")", ")", ")", "\n", "\n", "", "if", "len", "(", "missing", ")", ">", "0", ":", "\n", "        ", "print", "(", "\"Some files are missing:\"", ")", "\n", "print", "(", "missing", ")", "\n", "raise", "RuntimeError", "(", "\"missing files in input_folder\"", ")", "\n", "\n", "", "return", "maybe_case_ids", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.inference.predict.predict_from_folder": [[573, 657], ["maybe_mkdir_p", "shutil.copy", "isfile", "predict.check_input_folder_and_return_caseIDs", "subfiles", "join", "join", "load_pickle", "join", "isdir", "all", "predict.predict_cases", "join", "join", "join", "predict.predict_cases_fast", "isfile", "predict.predict_cases_fastest", "ValueError", "i[].startswith", "len", "len", "len"], "function", ["home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.inference.predict.check_input_folder_and_return_caseIDs", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.inference.predict.predict_cases", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.inference.predict.predict_cases_fast", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.inference.predict.predict_cases_fastest"], ["", "def", "predict_from_folder", "(", "model", ":", "str", ",", "input_folder", ":", "str", ",", "output_folder", ":", "str", ",", "folds", ":", "Union", "[", "Tuple", "[", "int", "]", ",", "List", "[", "int", "]", "]", ",", "\n", "save_npz", ":", "bool", ",", "num_threads_preprocessing", ":", "int", ",", "num_threads_nifti_save", ":", "int", ",", "\n", "lowres_segmentations", ":", "Union", "[", "str", ",", "None", "]", ",", "\n", "part_id", ":", "int", ",", "num_parts", ":", "int", ",", "tta", ":", "bool", ",", "mixed_precision", ":", "bool", "=", "True", ",", "\n", "overwrite_existing", ":", "bool", "=", "True", ",", "mode", ":", "str", "=", "'normal'", ",", "overwrite_all_in_gpu", ":", "bool", "=", "None", ",", "\n", "step_size", ":", "float", "=", "0.5", ",", "checkpoint_name", ":", "str", "=", "\"model_final_checkpoint\"", ",", "\n", "segmentation_export_kwargs", ":", "dict", "=", "None", ")", ":", "\n", "    ", "\"\"\"\n        here we use the standard naming scheme to generate list_of_lists and output_files needed by predict_cases\n\n    :param model:\n    :param input_folder:\n    :param output_folder:\n    :param folds:\n    :param save_npz:\n    :param num_threads_preprocessing:\n    :param num_threads_nifti_save:\n    :param lowres_segmentations:\n    :param part_id:\n    :param num_parts:\n    :param tta:\n    :param mixed_precision:\n    :param overwrite_existing: if not None then it will be overwritten with whatever is in there. None is default (no overwrite)\n    :return:\n    \"\"\"", "\n", "maybe_mkdir_p", "(", "output_folder", ")", "\n", "shutil", ".", "copy", "(", "join", "(", "model", ",", "'plans.pkl'", ")", ",", "output_folder", ")", "\n", "\n", "assert", "isfile", "(", "join", "(", "model", ",", "\"plans.pkl\"", ")", ")", ",", "\"Folder with saved model weights must contain a plans.pkl file\"", "\n", "expected_num_modalities", "=", "load_pickle", "(", "join", "(", "model", ",", "\"plans.pkl\"", ")", ")", "[", "'num_modalities'", "]", "\n", "\n", "# check input folder integrity", "\n", "case_ids", "=", "check_input_folder_and_return_caseIDs", "(", "input_folder", ",", "expected_num_modalities", ")", "\n", "\n", "output_files", "=", "[", "join", "(", "output_folder", ",", "i", "+", "\".nii.gz\"", ")", "for", "i", "in", "case_ids", "]", "\n", "all_files", "=", "subfiles", "(", "input_folder", ",", "suffix", "=", "\".nii.gz\"", ",", "join", "=", "False", ",", "sort", "=", "True", ")", "\n", "list_of_lists", "=", "[", "[", "join", "(", "input_folder", ",", "i", ")", "for", "i", "in", "all_files", "if", "i", "[", ":", "len", "(", "j", ")", "]", ".", "startswith", "(", "j", ")", "and", "\n", "len", "(", "i", ")", "==", "(", "len", "(", "j", ")", "+", "12", ")", "]", "for", "j", "in", "case_ids", "]", "\n", "\n", "if", "lowres_segmentations", "is", "not", "None", ":", "\n", "        ", "assert", "isdir", "(", "lowres_segmentations", ")", ",", "\"if lowres_segmentations is not None then it must point to a directory\"", "\n", "lowres_segmentations", "=", "[", "join", "(", "lowres_segmentations", ",", "i", "+", "\".nii.gz\"", ")", "for", "i", "in", "case_ids", "]", "\n", "assert", "all", "(", "[", "isfile", "(", "i", ")", "for", "i", "in", "lowres_segmentations", "]", ")", ",", "\"not all lowres_segmentations files are present. \"", "\"(I was searching for case_id.nii.gz in that folder)\"", "\n", "lowres_segmentations", "=", "lowres_segmentations", "[", "part_id", ":", ":", "num_parts", "]", "\n", "", "else", ":", "\n", "        ", "lowres_segmentations", "=", "None", "\n", "\n", "", "if", "mode", "==", "\"normal\"", ":", "\n", "        ", "if", "overwrite_all_in_gpu", "is", "None", ":", "\n", "            ", "all_in_gpu", "=", "False", "\n", "", "else", ":", "\n", "            ", "all_in_gpu", "=", "overwrite_all_in_gpu", "\n", "\n", "", "return", "predict_cases", "(", "model", ",", "list_of_lists", "[", "part_id", ":", ":", "num_parts", "]", ",", "output_files", "[", "part_id", ":", ":", "num_parts", "]", ",", "folds", ",", "\n", "save_npz", ",", "num_threads_preprocessing", ",", "num_threads_nifti_save", ",", "lowres_segmentations", ",", "tta", ",", "\n", "mixed_precision", "=", "mixed_precision", ",", "overwrite_existing", "=", "overwrite_existing", ",", "all_in_gpu", "=", "all_in_gpu", ",", "\n", "step_size", "=", "step_size", ",", "checkpoint_name", "=", "checkpoint_name", ",", "\n", "segmentation_export_kwargs", "=", "segmentation_export_kwargs", ")", "\n", "", "elif", "mode", "==", "\"fast\"", ":", "\n", "        ", "if", "overwrite_all_in_gpu", "is", "None", ":", "\n", "            ", "all_in_gpu", "=", "True", "\n", "", "else", ":", "\n", "            ", "all_in_gpu", "=", "overwrite_all_in_gpu", "\n", "\n", "", "assert", "save_npz", "is", "False", "\n", "return", "predict_cases_fast", "(", "model", ",", "list_of_lists", "[", "part_id", ":", ":", "num_parts", "]", ",", "output_files", "[", "part_id", ":", ":", "num_parts", "]", ",", "folds", ",", "\n", "num_threads_preprocessing", ",", "num_threads_nifti_save", ",", "lowres_segmentations", ",", "\n", "tta", ",", "mixed_precision", "=", "mixed_precision", ",", "overwrite_existing", "=", "overwrite_existing", ",", "all_in_gpu", "=", "all_in_gpu", ",", "\n", "step_size", "=", "step_size", ",", "checkpoint_name", "=", "checkpoint_name", ",", "\n", "segmentation_export_kwargs", "=", "segmentation_export_kwargs", ")", "\n", "", "elif", "mode", "==", "\"fastest\"", ":", "\n", "        ", "if", "overwrite_all_in_gpu", "is", "None", ":", "\n", "            ", "all_in_gpu", "=", "True", "\n", "", "else", ":", "\n", "            ", "all_in_gpu", "=", "overwrite_all_in_gpu", "\n", "\n", "", "assert", "save_npz", "is", "False", "\n", "return", "predict_cases_fastest", "(", "model", ",", "list_of_lists", "[", "part_id", ":", ":", "num_parts", "]", ",", "output_files", "[", "part_id", ":", ":", "num_parts", "]", ",", "folds", ",", "\n", "num_threads_preprocessing", ",", "num_threads_nifti_save", ",", "lowres_segmentations", ",", "\n", "tta", ",", "mixed_precision", "=", "mixed_precision", ",", "overwrite_existing", "=", "overwrite_existing", ",", "all_in_gpu", "=", "all_in_gpu", ",", "\n", "step_size", "=", "step_size", ",", "checkpoint_name", "=", "checkpoint_name", ")", "\n", "", "else", ":", "\n", "        ", "raise", "ValueError", "(", "\"unrecognized mode. Must be normal, fast or fastest\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.inference.segmentation_export.save_segmentation_nifti_from_softmax": [[27, 151], ["isinstance", "properties_dict.get", "properties_dict.get", "numpy.any", "properties_dict.get", "SimpleITK.GetImageFromArray", "sitk.GetImageFromArray.SetSpacing", "sitk.GetImageFromArray.SetOrigin", "sitk.GetImageFromArray.SetDirection", "SimpleITK.WriteImage", "print", "isfile", "copy.deepcopy", "numpy.load", "os.remove", "nnunet.preprocessing.preprocessing.resample_data_or_seg", "numpy.savez_compressed", "save_pickle", "seg_old_spacing.argmax.argmax", "numpy.zeros", "enumerate", "numpy.zeros", "range", "seg_postprogess_fn", "seg_postprogess_fn.astype", "SimpleITK.GetImageFromArray", "sitk.GetImageFromArray.SetSpacing", "sitk.GetImageFromArray.SetOrigin", "sitk.GetImageFromArray.SetDirection", "SimpleITK.WriteImage", "nnunet.preprocessing.preprocessing.get_do_separate_z", "print", "print", "numpy.min", "numpy.copy", "np.zeros.astype", "zip", "properties_dict.get", "nnunet.preprocessing.preprocessing.get_lowres_axis", "nnunet.preprocessing.preprocessing.get_do_separate_z", "nnunet.preprocessing.preprocessing.get_lowres_axis", "seg_old_spacing.argmax.astype", "numpy.array", "numpy.array", "properties_dict.get", "properties_dict.get", "nnunet.preprocessing.preprocessing.get_lowres_axis", "properties_dict.get", "properties_dict.get"], "function", ["home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.preprocessing.preprocessing.resample_data_or_seg", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.preprocessing.preprocessing.get_do_separate_z", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.preprocessing.preprocessing.get_lowres_axis", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.preprocessing.preprocessing.get_do_separate_z", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.preprocessing.preprocessing.get_lowres_axis", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.preprocessing.preprocessing.get_lowres_axis"], ["def", "save_segmentation_nifti_from_softmax", "(", "segmentation_softmax", ":", "Union", "[", "str", ",", "np", ".", "ndarray", "]", ",", "out_fname", ":", "str", ",", "\n", "properties_dict", ":", "dict", ",", "order", ":", "int", "=", "1", ",", "\n", "region_class_order", ":", "Tuple", "[", "Tuple", "[", "int", "]", "]", "=", "None", ",", "\n", "seg_postprogess_fn", ":", "callable", "=", "None", ",", "seg_postprocess_args", ":", "tuple", "=", "None", ",", "\n", "resampled_npz_fname", ":", "str", "=", "None", ",", "\n", "non_postprocessed_fname", ":", "str", "=", "None", ",", "force_separate_z", ":", "bool", "=", "None", ",", "\n", "interpolation_order_z", ":", "int", "=", "0", ",", "verbose", ":", "bool", "=", "True", ")", ":", "\n", "    ", "\"\"\"\n    This is a utility for writing segmentations to nifto and npz. It requires the data to have been preprocessed by\n    GenericPreprocessor because it depends on the property dictionary output (dct) to know the geometry of the original\n    data. segmentation_softmax does not have to have the same size in pixels as the original data, it will be\n    resampled to match that. This is generally useful because the spacings our networks operate on are most of the time\n    not the native spacings of the image data.\n    If seg_postprogess_fn is not None then seg_postprogess_fnseg_postprogess_fn(segmentation, *seg_postprocess_args)\n    will be called before nifto export\n    There is a problem with python process communication that prevents us from communicating obejcts\n    larger than 2 GB between processes (basically when the length of the pickle string that will be sent is\n    communicated by the multiprocessing.Pipe object then the placeholder (\\%i I think) does not allow for long\n    enough strings (lol). This could be fixed by changing i to l (for long) but that would require manually\n    patching system python code.) We circumvent that problem here by saving softmax_pred to a npy file that will\n    then be read (and finally deleted) by the Process. save_segmentation_nifti_from_softmax can take either\n    filename or np.ndarray for segmentation_softmax and will handle this automatically\n    :param segmentation_softmax:\n    :param out_fname:\n    :param properties_dict:\n    :param order:\n    :param region_class_order:\n    :param seg_postprogess_fn:\n    :param seg_postprocess_args:\n    :param resampled_npz_fname:\n    :param non_postprocessed_fname:\n    :param force_separate_z: if None then we dynamically decide how to resample along z, if True/False then always\n    /never resample along z separately. Do not touch unless you know what you are doing\n    :param interpolation_order_z: if separate z resampling is done then this is the order for resampling in z\n    :param verbose:\n    :return:\n    \"\"\"", "\n", "if", "verbose", ":", "print", "(", "\"force_separate_z:\"", ",", "force_separate_z", ",", "\"interpolation order:\"", ",", "order", ")", "\n", "\n", "if", "isinstance", "(", "segmentation_softmax", ",", "str", ")", ":", "\n", "        ", "assert", "isfile", "(", "segmentation_softmax", ")", ",", "\"If isinstance(segmentation_softmax, str) then \"", "\"isfile(segmentation_softmax) must be True\"", "\n", "del_file", "=", "deepcopy", "(", "segmentation_softmax", ")", "\n", "segmentation_softmax", "=", "np", ".", "load", "(", "segmentation_softmax", ")", "\n", "os", ".", "remove", "(", "del_file", ")", "\n", "\n", "# first resample, then put result into bbox of cropping, then save", "\n", "", "current_shape", "=", "segmentation_softmax", ".", "shape", "\n", "shape_original_after_cropping", "=", "properties_dict", ".", "get", "(", "'size_after_cropping'", ")", "\n", "shape_original_before_cropping", "=", "properties_dict", ".", "get", "(", "'original_size_of_raw_data'", ")", "\n", "# current_spacing = dct.get('spacing_after_resampling')", "\n", "# original_spacing = dct.get('original_spacing')", "\n", "\n", "if", "np", ".", "any", "(", "[", "i", "!=", "j", "for", "i", ",", "j", "in", "zip", "(", "np", ".", "array", "(", "current_shape", "[", "1", ":", "]", ")", ",", "np", ".", "array", "(", "shape_original_after_cropping", ")", ")", "]", ")", ":", "\n", "        ", "if", "force_separate_z", "is", "None", ":", "\n", "            ", "if", "get_do_separate_z", "(", "properties_dict", ".", "get", "(", "'original_spacing'", ")", ")", ":", "\n", "                ", "do_separate_z", "=", "True", "\n", "lowres_axis", "=", "get_lowres_axis", "(", "properties_dict", ".", "get", "(", "'original_spacing'", ")", ")", "\n", "", "elif", "get_do_separate_z", "(", "properties_dict", ".", "get", "(", "'spacing_after_resampling'", ")", ")", ":", "\n", "                ", "do_separate_z", "=", "True", "\n", "lowres_axis", "=", "get_lowres_axis", "(", "properties_dict", ".", "get", "(", "'spacing_after_resampling'", ")", ")", "\n", "", "else", ":", "\n", "                ", "do_separate_z", "=", "False", "\n", "lowres_axis", "=", "None", "\n", "", "", "else", ":", "\n", "            ", "do_separate_z", "=", "force_separate_z", "\n", "if", "do_separate_z", ":", "\n", "                ", "lowres_axis", "=", "get_lowres_axis", "(", "properties_dict", ".", "get", "(", "'original_spacing'", ")", ")", "\n", "", "else", ":", "\n", "                ", "lowres_axis", "=", "None", "\n", "\n", "", "", "if", "verbose", ":", "print", "(", "\"separate z:\"", ",", "do_separate_z", ",", "\"lowres axis\"", ",", "lowres_axis", ")", "\n", "seg_old_spacing", "=", "resample_data_or_seg", "(", "segmentation_softmax", ",", "shape_original_after_cropping", ",", "is_seg", "=", "False", ",", "\n", "axis", "=", "lowres_axis", ",", "order", "=", "order", ",", "do_separate_z", "=", "do_separate_z", ",", "cval", "=", "0", ",", "\n", "order_z", "=", "interpolation_order_z", ")", "\n", "# seg_old_spacing = resize_softmax_output(segmentation_softmax, shape_original_after_cropping, order=order)", "\n", "", "else", ":", "\n", "        ", "if", "verbose", ":", "print", "(", "\"no resampling necessary\"", ")", "\n", "seg_old_spacing", "=", "segmentation_softmax", "\n", "\n", "", "if", "resampled_npz_fname", "is", "not", "None", ":", "\n", "        ", "np", ".", "savez_compressed", "(", "resampled_npz_fname", ",", "softmax", "=", "seg_old_spacing", ".", "astype", "(", "np", ".", "float16", ")", ")", "\n", "# this is needed for ensembling if the nonlinearity is sigmoid", "\n", "if", "region_class_order", "is", "not", "None", ":", "\n", "            ", "properties_dict", "[", "'regions_class_order'", "]", "=", "region_class_order", "\n", "", "save_pickle", "(", "properties_dict", ",", "resampled_npz_fname", "[", ":", "-", "4", "]", "+", "\".pkl\"", ")", "\n", "\n", "", "if", "region_class_order", "is", "None", ":", "\n", "        ", "seg_old_spacing", "=", "seg_old_spacing", ".", "argmax", "(", "0", ")", "\n", "", "else", ":", "\n", "        ", "seg_old_spacing_final", "=", "np", ".", "zeros", "(", "seg_old_spacing", ".", "shape", "[", "1", ":", "]", ")", "\n", "for", "i", ",", "c", "in", "enumerate", "(", "region_class_order", ")", ":", "\n", "            ", "seg_old_spacing_final", "[", "seg_old_spacing", "[", "i", "]", ">", "0.5", "]", "=", "c", "\n", "", "seg_old_spacing", "=", "seg_old_spacing_final", "\n", "\n", "", "bbox", "=", "properties_dict", ".", "get", "(", "'crop_bbox'", ")", "\n", "\n", "if", "bbox", "is", "not", "None", ":", "\n", "        ", "seg_old_size", "=", "np", ".", "zeros", "(", "shape_original_before_cropping", ")", "\n", "for", "c", "in", "range", "(", "3", ")", ":", "\n", "            ", "bbox", "[", "c", "]", "[", "1", "]", "=", "np", ".", "min", "(", "(", "bbox", "[", "c", "]", "[", "0", "]", "+", "seg_old_spacing", ".", "shape", "[", "c", "]", ",", "shape_original_before_cropping", "[", "c", "]", ")", ")", "\n", "", "seg_old_size", "[", "bbox", "[", "0", "]", "[", "0", "]", ":", "bbox", "[", "0", "]", "[", "1", "]", ",", "\n", "bbox", "[", "1", "]", "[", "0", "]", ":", "bbox", "[", "1", "]", "[", "1", "]", ",", "\n", "bbox", "[", "2", "]", "[", "0", "]", ":", "bbox", "[", "2", "]", "[", "1", "]", "]", "=", "seg_old_spacing", "\n", "", "else", ":", "\n", "        ", "seg_old_size", "=", "seg_old_spacing", "\n", "\n", "", "if", "seg_postprogess_fn", "is", "not", "None", ":", "\n", "        ", "seg_old_size_postprocessed", "=", "seg_postprogess_fn", "(", "np", ".", "copy", "(", "seg_old_size", ")", ",", "*", "seg_postprocess_args", ")", "\n", "", "else", ":", "\n", "        ", "seg_old_size_postprocessed", "=", "seg_old_size", "\n", "\n", "", "seg_resized_itk", "=", "sitk", ".", "GetImageFromArray", "(", "seg_old_size_postprocessed", ".", "astype", "(", "np", ".", "uint8", ")", ")", "\n", "seg_resized_itk", ".", "SetSpacing", "(", "properties_dict", "[", "'itk_spacing'", "]", ")", "\n", "seg_resized_itk", ".", "SetOrigin", "(", "properties_dict", "[", "'itk_origin'", "]", ")", "\n", "seg_resized_itk", ".", "SetDirection", "(", "properties_dict", "[", "'itk_direction'", "]", ")", "\n", "sitk", ".", "WriteImage", "(", "seg_resized_itk", ",", "out_fname", ")", "\n", "\n", "if", "(", "non_postprocessed_fname", "is", "not", "None", ")", "and", "(", "seg_postprogess_fn", "is", "not", "None", ")", ":", "\n", "        ", "seg_resized_itk", "=", "sitk", ".", "GetImageFromArray", "(", "seg_old_size", ".", "astype", "(", "np", ".", "uint8", ")", ")", "\n", "seg_resized_itk", ".", "SetSpacing", "(", "properties_dict", "[", "'itk_spacing'", "]", ")", "\n", "seg_resized_itk", ".", "SetOrigin", "(", "properties_dict", "[", "'itk_origin'", "]", ")", "\n", "seg_resized_itk", ".", "SetDirection", "(", "properties_dict", "[", "'itk_direction'", "]", ")", "\n", "sitk", ".", "WriteImage", "(", "seg_resized_itk", ",", "non_postprocessed_fname", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.inference.segmentation_export.save_segmentation_nifti": [[153, 229], ["print", "open", "isinstance", "dct.get", "dct.get", "numpy.any", "dct.get", "SimpleITK.GetImageFromArray", "sitk.GetImageFromArray.SetSpacing", "sitk.GetImageFromArray.SetOrigin", "sitk.GetImageFromArray.SetDirection", "SimpleITK.WriteImage", "isfile", "copy.deepcopy", "numpy.load", "os.remove", "numpy.zeros", "range", "np.zeros.astype", "numpy.array", "numpy.array", "batchgenerators.augmentations.utils.resize_segmentation", "print", "numpy.min", "nnunet.preprocessing.preprocessing.get_do_separate_z", "nnunet.preprocessing.preprocessing.resample_data_or_seg", "dct.get", "nnunet.preprocessing.preprocessing.get_lowres_axis", "nnunet.preprocessing.preprocessing.get_do_separate_z", "nnunet.preprocessing.preprocessing.get_lowres_axis", "dct.get", "dct.get", "nnunet.preprocessing.preprocessing.get_lowres_axis", "dct.get", "dct.get"], "function", ["home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.preprocessing.preprocessing.get_do_separate_z", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.preprocessing.preprocessing.resample_data_or_seg", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.preprocessing.preprocessing.get_lowres_axis", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.preprocessing.preprocessing.get_do_separate_z", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.preprocessing.preprocessing.get_lowres_axis", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.preprocessing.preprocessing.get_lowres_axis"], ["", "", "def", "save_segmentation_nifti", "(", "segmentation", ",", "out_fname", ",", "dct", ",", "order", "=", "1", ",", "force_separate_z", "=", "None", ",", "order_z", "=", "0", ")", ":", "\n", "    ", "\"\"\"\n    faster and uses less ram than save_segmentation_nifti_from_softmax, but maybe less precise and also does not support\n    softmax export (which is needed for ensembling). So it's a niche function that may be useful in some cases.\n    :param segmentation:\n    :param out_fname:\n    :param dct:\n    :param order:\n    :param force_separate_z:\n    :return:\n    \"\"\"", "\n", "# suppress output", "\n", "print", "(", "\"force_separate_z:\"", ",", "force_separate_z", ",", "\"interpolation order:\"", ",", "order", ")", "\n", "sys", ".", "stdout", "=", "open", "(", "os", ".", "devnull", ",", "'w'", ")", "\n", "\n", "if", "isinstance", "(", "segmentation", ",", "str", ")", ":", "\n", "        ", "assert", "isfile", "(", "segmentation", ")", ",", "\"If isinstance(segmentation_softmax, str) then \"", "\"isfile(segmentation_softmax) must be True\"", "\n", "del_file", "=", "deepcopy", "(", "segmentation", ")", "\n", "segmentation", "=", "np", ".", "load", "(", "segmentation", ")", "\n", "os", ".", "remove", "(", "del_file", ")", "\n", "\n", "# first resample, then put result into bbox of cropping, then save", "\n", "", "current_shape", "=", "segmentation", ".", "shape", "\n", "shape_original_after_cropping", "=", "dct", ".", "get", "(", "'size_after_cropping'", ")", "\n", "shape_original_before_cropping", "=", "dct", ".", "get", "(", "'original_size_of_raw_data'", ")", "\n", "# current_spacing = dct.get('spacing_after_resampling')", "\n", "# original_spacing = dct.get('original_spacing')", "\n", "\n", "if", "np", ".", "any", "(", "np", ".", "array", "(", "current_shape", ")", "!=", "np", ".", "array", "(", "shape_original_after_cropping", ")", ")", ":", "\n", "        ", "if", "order", "==", "0", ":", "\n", "            ", "seg_old_spacing", "=", "resize_segmentation", "(", "segmentation", ",", "shape_original_after_cropping", ",", "0", ",", "0", ")", "\n", "", "else", ":", "\n", "            ", "if", "force_separate_z", "is", "None", ":", "\n", "                ", "if", "get_do_separate_z", "(", "dct", ".", "get", "(", "'original_spacing'", ")", ")", ":", "\n", "                    ", "do_separate_z", "=", "True", "\n", "lowres_axis", "=", "get_lowres_axis", "(", "dct", ".", "get", "(", "'original_spacing'", ")", ")", "\n", "", "elif", "get_do_separate_z", "(", "dct", ".", "get", "(", "'spacing_after_resampling'", ")", ")", ":", "\n", "                    ", "do_separate_z", "=", "True", "\n", "lowres_axis", "=", "get_lowres_axis", "(", "dct", ".", "get", "(", "'spacing_after_resampling'", ")", ")", "\n", "", "else", ":", "\n", "                    ", "do_separate_z", "=", "False", "\n", "lowres_axis", "=", "None", "\n", "", "", "else", ":", "\n", "                ", "do_separate_z", "=", "force_separate_z", "\n", "if", "do_separate_z", ":", "\n", "                    ", "lowres_axis", "=", "get_lowres_axis", "(", "dct", ".", "get", "(", "'original_spacing'", ")", ")", "\n", "", "else", ":", "\n", "                    ", "lowres_axis", "=", "None", "\n", "\n", "", "", "print", "(", "\"separate z:\"", ",", "do_separate_z", ",", "\"lowres axis\"", ",", "lowres_axis", ")", "\n", "seg_old_spacing", "=", "resample_data_or_seg", "(", "segmentation", "[", "None", "]", ",", "shape_original_after_cropping", ",", "is_seg", "=", "True", ",", "\n", "axis", "=", "lowres_axis", ",", "order", "=", "order", ",", "do_separate_z", "=", "do_separate_z", ",", "cval", "=", "0", ",", "\n", "order_z", "=", "order_z", ")", "[", "0", "]", "\n", "", "", "else", ":", "\n", "        ", "seg_old_spacing", "=", "segmentation", "\n", "\n", "", "bbox", "=", "dct", ".", "get", "(", "'crop_bbox'", ")", "\n", "\n", "if", "bbox", "is", "not", "None", ":", "\n", "        ", "seg_old_size", "=", "np", ".", "zeros", "(", "shape_original_before_cropping", ")", "\n", "for", "c", "in", "range", "(", "3", ")", ":", "\n", "            ", "bbox", "[", "c", "]", "[", "1", "]", "=", "np", ".", "min", "(", "(", "bbox", "[", "c", "]", "[", "0", "]", "+", "seg_old_spacing", ".", "shape", "[", "c", "]", ",", "shape_original_before_cropping", "[", "c", "]", ")", ")", "\n", "", "seg_old_size", "[", "bbox", "[", "0", "]", "[", "0", "]", ":", "bbox", "[", "0", "]", "[", "1", "]", ",", "\n", "bbox", "[", "1", "]", "[", "0", "]", ":", "bbox", "[", "1", "]", "[", "1", "]", ",", "\n", "bbox", "[", "2", "]", "[", "0", "]", ":", "bbox", "[", "2", "]", "[", "1", "]", "]", "=", "seg_old_spacing", "\n", "", "else", ":", "\n", "        ", "seg_old_size", "=", "seg_old_spacing", "\n", "\n", "", "seg_resized_itk", "=", "sitk", ".", "GetImageFromArray", "(", "seg_old_size", ".", "astype", "(", "np", ".", "uint8", ")", ")", "\n", "seg_resized_itk", ".", "SetSpacing", "(", "dct", "[", "'itk_spacing'", "]", ")", "\n", "seg_resized_itk", ".", "SetOrigin", "(", "dct", "[", "'itk_origin'", "]", ")", "\n", "seg_resized_itk", ".", "SetDirection", "(", "dct", "[", "'itk_direction'", "]", ")", "\n", "sitk", ".", "WriteImage", "(", "seg_resized_itk", ",", "out_fname", ")", "\n", "\n", "sys", ".", "stdout", "=", "sys", ".", "__stdout__", "\n", "", ""]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.inference.predict_simple.main": [[25, 222], ["argparse.ArgumentParser", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.parse_args", "isinstance", "batchgenerators.utilities.file_and_folder_operations.join", "print", "batchgenerators.utilities.file_and_folder_operations.isdir", "nnunet.inference.predict.predict_from_folder", "nnunet.utilities.task_name_id_conversion.convert_id_to_task_name.startswith", "int", "nnunet.utilities.task_name_id_conversion.convert_id_to_task_name", "print", "batchgenerators.utilities.file_and_folder_operations.join", "batchgenerators.utilities.file_and_folder_operations.isdir", "batchgenerators.utilities.file_and_folder_operations.join", "nnunet.inference.predict.predict_from_folder", "torch.cuda.empty_cache", "print", "ValueError", "len", "int"], "function", ["home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.inference.predict.predict_from_folder", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.utilities.task_name_id_conversion.convert_id_to_task_name", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.inference.predict.predict_from_folder"], ["def", "main", "(", ")", ":", "\n", "    ", "parser", "=", "argparse", ".", "ArgumentParser", "(", ")", "\n", "parser", ".", "add_argument", "(", "\"-i\"", ",", "'--input_folder'", ",", "help", "=", "\"Must contain all modalities for each patient in the correct\"", "\n", "\" order (same as training). Files must be named \"", "\n", "\"CASENAME_XXXX.nii.gz where XXXX is the modality \"", "\n", "\"identifier (0000, 0001, etc)\"", ",", "required", "=", "True", ")", "\n", "parser", ".", "add_argument", "(", "'-o'", ",", "\"--output_folder\"", ",", "required", "=", "True", ",", "help", "=", "\"folder for saving predictions\"", ")", "\n", "parser", ".", "add_argument", "(", "'-t'", ",", "'--task_name'", ",", "help", "=", "'task name or task ID, required.'", ",", "\n", "default", "=", "default_plans_identifier", ",", "required", "=", "True", ")", "\n", "\n", "parser", ".", "add_argument", "(", "'-tr'", ",", "'--trainer_class_name'", ",", "\n", "help", "=", "'Name of the nnUNetTrainer used for 2D U-Net, full resolution 3D U-Net and low resolution '", "\n", "'U-Net. The default is %s. If you are running inference with the cascade and the folder '", "\n", "'pointed to by --lowres_segmentations does not contain the segmentation maps generated by '", "\n", "'the low resolution U-Net then the low resolution segmentation maps will be automatically '", "\n", "'generated. For this case, make sure to set the trainer class here that matches your '", "\n", "'--cascade_trainer_class_name (this part can be ignored if defaults are used).'", "\n", "%", "default_trainer", ",", "\n", "required", "=", "False", ",", "\n", "default", "=", "default_trainer", ")", "\n", "parser", ".", "add_argument", "(", "'-ctr'", ",", "'--cascade_trainer_class_name'", ",", "\n", "help", "=", "\"Trainer class name used for predicting the 3D full resolution U-Net part of the cascade.\"", "\n", "\"Default is %s\"", "%", "default_cascade_trainer", ",", "required", "=", "False", ",", "\n", "default", "=", "default_cascade_trainer", ")", "\n", "\n", "parser", ".", "add_argument", "(", "'-m'", ",", "'--model'", ",", "help", "=", "\"2d, 3d_lowres, 3d_fullres or 3d_cascade_fullres. Default: 3d_fullres\"", ",", "\n", "default", "=", "\"3d_fullres\"", ",", "required", "=", "False", ")", "\n", "\n", "parser", ".", "add_argument", "(", "'-p'", ",", "'--plans_identifier'", ",", "help", "=", "'do not touch this unless you know what you are doing'", ",", "\n", "default", "=", "default_plans_identifier", ",", "required", "=", "False", ")", "\n", "\n", "parser", ".", "add_argument", "(", "'-f'", ",", "'--folds'", ",", "nargs", "=", "'+'", ",", "default", "=", "'None'", ",", "\n", "help", "=", "\"folds to use for prediction. Default is None which means that folds will be detected \"", "\n", "\"automatically in the model output folder\"", ")", "\n", "\n", "parser", ".", "add_argument", "(", "'-z'", ",", "'--save_npz'", ",", "required", "=", "False", ",", "action", "=", "'store_true'", ",", "\n", "help", "=", "\"use this if you want to ensemble these predictions with those of other models. Softmax \"", "\n", "\"probabilities will be saved as compressed numpy arrays in output_folder and can be \"", "\n", "\"merged between output_folders with nnUNet_ensemble_predictions\"", ")", "\n", "\n", "parser", ".", "add_argument", "(", "'-l'", ",", "'--lowres_segmentations'", ",", "required", "=", "False", ",", "default", "=", "'None'", ",", "\n", "help", "=", "\"if model is the highres stage of the cascade then you can use this folder to provide \"", "\n", "\"predictions from the low resolution 3D U-Net. If this is left at default, the \"", "\n", "\"predictions will be generated automatically (provided that the 3D low resolution U-Net \"", "\n", "\"network weights are present\"", ")", "\n", "\n", "parser", ".", "add_argument", "(", "\"--part_id\"", ",", "type", "=", "int", ",", "required", "=", "False", ",", "default", "=", "0", ",", "help", "=", "\"Used to parallelize the prediction of \"", "\n", "\"the folder over several GPUs. If you \"", "\n", "\"want to use n GPUs to predict this \"", "\n", "\"folder you need to run this command \"", "\n", "\"n times with --part_id=0, ... n-1 and \"", "\n", "\"--num_parts=n (each with a different \"", "\n", "\"GPU (for example via \"", "\n", "\"CUDA_VISIBLE_DEVICES=X)\"", ")", "\n", "\n", "parser", ".", "add_argument", "(", "\"--num_parts\"", ",", "type", "=", "int", ",", "required", "=", "False", ",", "default", "=", "1", ",", "\n", "help", "=", "\"Used to parallelize the prediction of \"", "\n", "\"the folder over several GPUs. If you \"", "\n", "\"want to use n GPUs to predict this \"", "\n", "\"folder you need to run this command \"", "\n", "\"n times with --part_id=0, ... n-1 and \"", "\n", "\"--num_parts=n (each with a different \"", "\n", "\"GPU (via \"", "\n", "\"CUDA_VISIBLE_DEVICES=X)\"", ")", "\n", "\n", "parser", ".", "add_argument", "(", "\"--num_threads_preprocessing\"", ",", "required", "=", "False", ",", "default", "=", "6", ",", "type", "=", "int", ",", "help", "=", "\n", "\"Determines many background processes will be used for data preprocessing. Reduce this if you \"", "\n", "\"run into out of memory (RAM) problems. Default: 6\"", ")", "\n", "\n", "parser", ".", "add_argument", "(", "\"--num_threads_nifti_save\"", ",", "required", "=", "False", ",", "default", "=", "2", ",", "type", "=", "int", ",", "help", "=", "\n", "\"Determines many background processes will be used for segmentation export. Reduce this if you \"", "\n", "\"run into out of memory (RAM) problems. Default: 2\"", ")", "\n", "\n", "parser", ".", "add_argument", "(", "\"--disable_tta\"", ",", "required", "=", "False", ",", "default", "=", "False", ",", "action", "=", "\"store_true\"", ",", "\n", "help", "=", "\"set this flag to disable test time data augmentation via mirroring. Speeds up inference \"", "\n", "\"by roughly factor 4 (2D) or 8 (3D)\"", ")", "\n", "\n", "parser", ".", "add_argument", "(", "\"--overwrite_existing\"", ",", "required", "=", "False", ",", "default", "=", "False", ",", "action", "=", "\"store_true\"", ",", "\n", "help", "=", "\"Set this flag if the target folder contains predictions that you would like to overwrite\"", ")", "\n", "\n", "parser", ".", "add_argument", "(", "\"--mode\"", ",", "type", "=", "str", ",", "default", "=", "\"normal\"", ",", "required", "=", "False", ",", "help", "=", "\"Hands off!\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--all_in_gpu\"", ",", "type", "=", "str", ",", "default", "=", "\"None\"", ",", "required", "=", "False", ",", "help", "=", "\"can be None, False or True. \"", "\n", "\"Do not touch.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--step_size\"", ",", "type", "=", "float", ",", "default", "=", "0.5", ",", "required", "=", "False", ",", "help", "=", "\"don't touch\"", ")", "\n", "# parser.add_argument(\"--interp_order\", required=False, default=3, type=int,", "\n", "#                     help=\"order of interpolation for segmentations, has no effect if mode=fastest. Do not touch this.\")", "\n", "# parser.add_argument(\"--interp_order_z\", required=False, default=0, type=int,", "\n", "#                     help=\"order of interpolation along z is z is done differently. Do not touch this.\")", "\n", "# parser.add_argument(\"--force_separate_z\", required=False, default=\"None\", type=str,", "\n", "#                     help=\"force_separate_z resampling. Can be None, True or False, has no effect if mode=fastest. \"", "\n", "#                          \"Do not touch this.\")", "\n", "parser", ".", "add_argument", "(", "'-chk'", ",", "\n", "help", "=", "'checkpoint name, default: model_final_checkpoint'", ",", "\n", "required", "=", "False", ",", "\n", "default", "=", "'model_final_checkpoint'", ")", "\n", "parser", ".", "add_argument", "(", "'--disable_mixed_precision'", ",", "default", "=", "False", ",", "action", "=", "'store_true'", ",", "required", "=", "False", ",", "\n", "help", "=", "'Predictions are done with mixed precision by default. This improves speed and reduces '", "\n", "'the required vram. If you want to disable mixed precision you can set this flag. Note '", "\n", "'that yhis is not recommended (mixed precision is ~2x faster!)'", ")", "\n", "\n", "args", "=", "parser", ".", "parse_args", "(", ")", "\n", "input_folder", "=", "args", ".", "input_folder", "\n", "output_folder", "=", "args", ".", "output_folder", "\n", "part_id", "=", "args", ".", "part_id", "\n", "num_parts", "=", "args", ".", "num_parts", "\n", "folds", "=", "args", ".", "folds", "\n", "save_npz", "=", "args", ".", "save_npz", "\n", "lowres_segmentations", "=", "args", ".", "lowres_segmentations", "\n", "num_threads_preprocessing", "=", "args", ".", "num_threads_preprocessing", "\n", "num_threads_nifti_save", "=", "args", ".", "num_threads_nifti_save", "\n", "disable_tta", "=", "args", ".", "disable_tta", "\n", "step_size", "=", "args", ".", "step_size", "\n", "# interp_order = args.interp_order", "\n", "# interp_order_z = args.interp_order_z", "\n", "# force_separate_z = args.force_separate_z", "\n", "overwrite_existing", "=", "args", ".", "overwrite_existing", "\n", "mode", "=", "args", ".", "mode", "\n", "all_in_gpu", "=", "args", ".", "all_in_gpu", "\n", "model", "=", "args", ".", "model", "\n", "trainer_class_name", "=", "args", ".", "trainer_class_name", "\n", "cascade_trainer_class_name", "=", "args", ".", "cascade_trainer_class_name", "\n", "\n", "task_name", "=", "args", ".", "task_name", "\n", "\n", "if", "not", "task_name", ".", "startswith", "(", "\"Task\"", ")", ":", "\n", "        ", "task_id", "=", "int", "(", "task_name", ")", "\n", "task_name", "=", "convert_id_to_task_name", "(", "task_id", ")", "\n", "\n", "", "assert", "model", "in", "[", "\"2d\"", ",", "\"3d_lowres\"", ",", "\"3d_fullres\"", ",", "\"3d_cascade_fullres\"", "]", ",", "\"-m must be 2d, 3d_lowres, 3d_fullres or \"", "\"3d_cascade_fullres\"", "\n", "\n", "# if force_separate_z == \"None\":", "\n", "#     force_separate_z = None", "\n", "# elif force_separate_z == \"False\":", "\n", "#     force_separate_z = False", "\n", "# elif force_separate_z == \"True\":", "\n", "#     force_separate_z = True", "\n", "# else:", "\n", "#     raise ValueError(\"force_separate_z must be None, True or False. Given: %s\" % force_separate_z)", "\n", "\n", "if", "lowres_segmentations", "==", "\"None\"", ":", "\n", "        ", "lowres_segmentations", "=", "None", "\n", "\n", "", "if", "isinstance", "(", "folds", ",", "list", ")", ":", "\n", "        ", "if", "folds", "[", "0", "]", "==", "'all'", "and", "len", "(", "folds", ")", "==", "1", ":", "\n", "            ", "pass", "\n", "", "else", ":", "\n", "            ", "folds", "=", "[", "int", "(", "i", ")", "for", "i", "in", "folds", "]", "\n", "", "", "elif", "folds", "==", "\"None\"", ":", "\n", "        ", "folds", "=", "None", "\n", "", "else", ":", "\n", "        ", "raise", "ValueError", "(", "\"Unexpected value for argument folds\"", ")", "\n", "\n", "", "assert", "all_in_gpu", "in", "[", "'None'", ",", "'False'", ",", "'True'", "]", "\n", "if", "all_in_gpu", "==", "\"None\"", ":", "\n", "        ", "all_in_gpu", "=", "None", "\n", "", "elif", "all_in_gpu", "==", "\"True\"", ":", "\n", "        ", "all_in_gpu", "=", "True", "\n", "", "elif", "all_in_gpu", "==", "\"False\"", ":", "\n", "        ", "all_in_gpu", "=", "False", "\n", "\n", "# we need to catch the case where model is 3d cascade fullres and the low resolution folder has not been set.", "\n", "# In that case we need to try and predict with 3d low res first", "\n", "", "if", "model", "==", "\"3d_cascade_fullres\"", "and", "lowres_segmentations", "is", "None", ":", "\n", "        ", "print", "(", "\"lowres_segmentations is None. Attempting to predict 3d_lowres first...\"", ")", "\n", "assert", "part_id", "==", "0", "and", "num_parts", "==", "1", ",", "\"if you don't specify a --lowres_segmentations folder for the \"", "\"inference of the cascade, custom values for part_id and num_parts \"", "\"are not supported. If you wish to have multiple parts, please \"", "\"run the 3d_lowres inference first (separately)\"", "\n", "model_folder_name", "=", "join", "(", "network_training_output_dir", ",", "\"3d_lowres\"", ",", "task_name", ",", "trainer_class_name", "+", "\"__\"", "+", "\n", "args", ".", "plans_identifier", ")", "\n", "assert", "isdir", "(", "model_folder_name", ")", ",", "\"model output folder not found. Expected: %s\"", "%", "model_folder_name", "\n", "lowres_output_folder", "=", "join", "(", "output_folder", ",", "\"3d_lowres_predictions\"", ")", "\n", "predict_from_folder", "(", "model_folder_name", ",", "input_folder", ",", "lowres_output_folder", ",", "folds", ",", "False", ",", "\n", "num_threads_preprocessing", ",", "num_threads_nifti_save", ",", "None", ",", "part_id", ",", "num_parts", ",", "not", "disable_tta", ",", "\n", "overwrite_existing", "=", "overwrite_existing", ",", "mode", "=", "mode", ",", "overwrite_all_in_gpu", "=", "all_in_gpu", ",", "\n", "mixed_precision", "=", "not", "args", ".", "disable_mixed_precision", ",", "\n", "step_size", "=", "step_size", ")", "\n", "lowres_segmentations", "=", "lowres_output_folder", "\n", "torch", ".", "cuda", ".", "empty_cache", "(", ")", "\n", "print", "(", "\"3d_lowres done\"", ")", "\n", "\n", "", "if", "model", "==", "\"3d_cascade_fullres\"", ":", "\n", "        ", "trainer", "=", "cascade_trainer_class_name", "\n", "", "else", ":", "\n", "        ", "trainer", "=", "trainer_class_name", "\n", "\n", "", "model_folder_name", "=", "join", "(", "network_training_output_dir", ",", "model", ",", "task_name", ",", "trainer", "+", "\"__\"", "+", "\n", "args", ".", "plans_identifier", ")", "\n", "print", "(", "\"using model stored in \"", ",", "model_folder_name", ")", "\n", "assert", "isdir", "(", "model_folder_name", ")", ",", "\"model output folder not found. Expected: %s\"", "%", "model_folder_name", "\n", "\n", "predict_from_folder", "(", "model_folder_name", ",", "input_folder", ",", "output_folder", ",", "folds", ",", "save_npz", ",", "num_threads_preprocessing", ",", "\n", "num_threads_nifti_save", ",", "lowres_segmentations", ",", "part_id", ",", "num_parts", ",", "not", "disable_tta", ",", "\n", "overwrite_existing", "=", "overwrite_existing", ",", "mode", "=", "mode", ",", "overwrite_all_in_gpu", "=", "all_in_gpu", ",", "\n", "mixed_precision", "=", "not", "args", ".", "disable_mixed_precision", ",", "\n", "step_size", "=", "step_size", ",", "checkpoint_name", "=", "args", ".", "chk", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.inference.change_trainer.pretend_to_be_nnUNetTrainer": [[19, 21], ["change_trainer.pretend_to_be_other_trainer"], "function", ["home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.inference.change_trainer.pretend_to_be_other_trainer"], ["def", "pretend_to_be_nnUNetTrainer", "(", "folder", ",", "checkpoints", "=", "(", "\"model_best.model.pkl\"", ",", "\"model_final_checkpoint.model.pkl\"", ")", ")", ":", "\n", "    ", "pretend_to_be_other_trainer", "(", "folder", ",", "\"nnUNetTrainer\"", ",", "checkpoints", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.inference.change_trainer.pretend_to_be_other_trainer": [[23, 36], ["subdirs", "isdir", "join", "subdirs.append", "join", "isfile", "load_pickle", "save_pickle"], "function", ["None"], ["", "def", "pretend_to_be_other_trainer", "(", "folder", ",", "new_trainer_name", ",", "checkpoints", "=", "(", "\"model_best.model.pkl\"", ",", "\"model_final_checkpoint.model.pkl\"", ")", ")", ":", "\n", "    ", "folds", "=", "subdirs", "(", "folder", ",", "prefix", "=", "\"fold_\"", ",", "join", "=", "False", ")", "\n", "\n", "if", "isdir", "(", "join", "(", "folder", ",", "'all'", ")", ")", ":", "\n", "        ", "folds", ".", "append", "(", "'all'", ")", "\n", "\n", "", "for", "c", "in", "checkpoints", ":", "\n", "        ", "for", "f", "in", "folds", ":", "\n", "            ", "checkpoint_file", "=", "join", "(", "folder", ",", "f", ",", "c", ")", "\n", "if", "isfile", "(", "checkpoint_file", ")", ":", "\n", "                ", "a", "=", "load_pickle", "(", "checkpoint_file", ")", "\n", "a", "[", "'name'", "]", "=", "new_trainer_name", "\n", "save_pickle", "(", "a", ",", "checkpoint_file", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.inference.change_trainer.main": [[38, 52], ["argparse.ArgumentParser", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.parse_args", "change_trainer.pretend_to_be_other_trainer"], "function", ["home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.inference.change_trainer.pretend_to_be_other_trainer"], ["", "", "", "", "def", "main", "(", ")", ":", "\n", "    ", "import", "argparse", "\n", "parser", "=", "argparse", ".", "ArgumentParser", "(", "description", "=", "'Use this script to change the nnunet trainer class of a saved '", "\n", "'model. Useful for models that were trained with trainers that do '", "\n", "'not support inference (multi GPU trainers) or for trainer classes '", "\n", "'whose source code is not available. For this to work the network '", "\n", "'architecture must be identical between the original trainer '", "\n", "'class and the trainer class we are changing to. This script is '", "\n", "'experimental and only to be used by advanced users.'", ")", "\n", "parser", ".", "add_argument", "(", "'-i'", ",", "help", "=", "'Folder containing the trained model. This folder is the one containing the '", "\n", "'fold_X subfolders.'", ")", "\n", "parser", ".", "add_argument", "(", "'-tr'", ",", "help", "=", "'Name of the new trainer class'", ")", "\n", "args", "=", "parser", ".", "parse_args", "(", ")", "\n", "pretend_to_be_other_trainer", "(", "args", ".", "i", ",", "args", ".", "tr", ")", "\n", "", ""]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.inference.ensemble_predictions.merge_files": [[26, 54], ["numpy.vstack", "numpy.mean", "nnunet.inference.segmentation_export.save_segmentation_nifti_from_softmax", "isfile", "load_pickle", "all", "numpy.savez_compressed", "save_pickle", "numpy.load", "p.keys", "str", "str"], "function", ["home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.inference.segmentation_export.save_segmentation_nifti_from_softmax"], ["def", "merge_files", "(", "files", ",", "properties_files", ",", "out_file", ",", "override", ",", "store_npz", ")", ":", "\n", "    ", "if", "override", "or", "not", "isfile", "(", "out_file", ")", ":", "\n", "        ", "softmax", "=", "[", "np", ".", "load", "(", "f", ")", "[", "'softmax'", "]", "[", "None", "]", "for", "f", "in", "files", "]", "\n", "softmax", "=", "np", ".", "vstack", "(", "softmax", ")", "\n", "softmax", "=", "np", ".", "mean", "(", "softmax", ",", "0", ")", "\n", "props", "=", "[", "load_pickle", "(", "f", ")", "for", "f", "in", "properties_files", "]", "\n", "\n", "reg_class_orders", "=", "[", "p", "[", "'regions_class_order'", "]", "if", "'regions_class_order'", "in", "p", ".", "keys", "(", ")", "else", "None", "\n", "for", "p", "in", "props", "]", "\n", "\n", "if", "not", "all", "(", "[", "i", "is", "None", "for", "i", "in", "reg_class_orders", "]", ")", ":", "\n", "# if reg_class_orders are not None then they must be the same in all pkls", "\n", "            ", "tmp", "=", "reg_class_orders", "[", "0", "]", "\n", "for", "r", "in", "reg_class_orders", "[", "1", ":", "]", ":", "\n", "                ", "assert", "tmp", "==", "r", ",", "'If merging files with regions_class_order, the regions_class_orders of all '", "'files must be the same. regions_class_order: %s, \\n files: %s'", "%", "(", "str", "(", "reg_class_orders", ")", ",", "str", "(", "files", ")", ")", "\n", "", "regions_class_order", "=", "tmp", "\n", "", "else", ":", "\n", "            ", "regions_class_order", "=", "None", "\n", "\n", "# Softmax probabilities are already at target spacing so this will not do any resampling (resampling parameters", "\n", "# don't matter here)", "\n", "", "save_segmentation_nifti_from_softmax", "(", "softmax", ",", "out_file", ",", "props", "[", "0", "]", ",", "3", ",", "regions_class_order", ",", "None", ",", "None", ",", "\n", "force_separate_z", "=", "None", ")", "\n", "if", "store_npz", ":", "\n", "            ", "np", ".", "savez_compressed", "(", "out_file", "[", ":", "-", "7", "]", "+", "\".npz\"", ",", "softmax", "=", "softmax", ")", "\n", "save_pickle", "(", "props", ",", "out_file", "[", ":", "-", "7", "]", "+", "\".pkl\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.inference.ensemble_predictions.merge": [[56, 96], ["maybe_mkdir_p", "numpy.unique", "multiprocessing.Pool", "multiprocessing.Pool.starmap", "multiprocessing.Pool.close", "multiprocessing.Pool.join", "copy.deepcopy", "join", "maybe_mkdir_p", "subfiles", "all", "all", "files.append", "property_files.append", "out_files.append", "zip", "nnunet.postprocessing.connected_components.load_postprocessing", "print", "nnunet.postprocessing.connected_components.apply_postprocessing_to_folder", "shutil.copy", "join", "isfile", "isfile", "join", "join", "len", "len", "join", "join"], "function", ["home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.postprocessing.connected_components.load_postprocessing", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.postprocessing.connected_components.apply_postprocessing_to_folder"], ["", "", "", "def", "merge", "(", "folders", ",", "output_folder", ",", "threads", ",", "override", "=", "True", ",", "postprocessing_file", "=", "None", ",", "store_npz", "=", "False", ")", ":", "\n", "    ", "maybe_mkdir_p", "(", "output_folder", ")", "\n", "\n", "if", "postprocessing_file", "is", "not", "None", ":", "\n", "        ", "output_folder_orig", "=", "deepcopy", "(", "output_folder", ")", "\n", "output_folder", "=", "join", "(", "output_folder", ",", "'not_postprocessed'", ")", "\n", "maybe_mkdir_p", "(", "output_folder", ")", "\n", "", "else", ":", "\n", "        ", "output_folder_orig", "=", "None", "\n", "\n", "", "patient_ids", "=", "[", "subfiles", "(", "i", ",", "suffix", "=", "\".npz\"", ",", "join", "=", "False", ")", "for", "i", "in", "folders", "]", "\n", "patient_ids", "=", "[", "i", "for", "j", "in", "patient_ids", "for", "i", "in", "j", "]", "\n", "patient_ids", "=", "[", "i", "[", ":", "-", "4", "]", "for", "i", "in", "patient_ids", "]", "\n", "patient_ids", "=", "np", ".", "unique", "(", "patient_ids", ")", "\n", "\n", "for", "f", "in", "folders", ":", "\n", "        ", "assert", "all", "(", "[", "isfile", "(", "join", "(", "f", ",", "i", "+", "\".npz\"", ")", ")", "for", "i", "in", "patient_ids", "]", ")", ",", "\"Not all patient npz are available in \"", "\"all folders\"", "\n", "assert", "all", "(", "[", "isfile", "(", "join", "(", "f", ",", "i", "+", "\".pkl\"", ")", ")", "for", "i", "in", "patient_ids", "]", ")", ",", "\"Not all patient pkl are available in \"", "\"all folders\"", "\n", "\n", "", "files", "=", "[", "]", "\n", "property_files", "=", "[", "]", "\n", "out_files", "=", "[", "]", "\n", "for", "p", "in", "patient_ids", ":", "\n", "        ", "files", ".", "append", "(", "[", "join", "(", "f", ",", "p", "+", "\".npz\"", ")", "for", "f", "in", "folders", "]", ")", "\n", "property_files", ".", "append", "(", "[", "join", "(", "f", ",", "p", "+", "\".pkl\"", ")", "for", "f", "in", "folders", "]", ")", "\n", "out_files", ".", "append", "(", "join", "(", "output_folder", ",", "p", "+", "\".nii.gz\"", ")", ")", "\n", "\n", "", "p", "=", "Pool", "(", "threads", ")", "\n", "p", ".", "starmap", "(", "merge_files", ",", "zip", "(", "files", ",", "property_files", ",", "out_files", ",", "[", "override", "]", "*", "len", "(", "out_files", ")", ",", "[", "store_npz", "]", "*", "len", "(", "out_files", ")", ")", ")", "\n", "p", ".", "close", "(", ")", "\n", "p", ".", "join", "(", ")", "\n", "\n", "if", "postprocessing_file", "is", "not", "None", ":", "\n", "        ", "for_which_classes", ",", "min_valid_obj_size", "=", "load_postprocessing", "(", "postprocessing_file", ")", "\n", "print", "(", "'Postprocessing...'", ")", "\n", "apply_postprocessing_to_folder", "(", "output_folder", ",", "output_folder_orig", ",", "\n", "for_which_classes", ",", "min_valid_obj_size", ",", "threads", ")", "\n", "shutil", ".", "copy", "(", "postprocessing_file", ",", "output_folder_orig", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.inference.ensemble_predictions.main": [[98, 125], ["argparse.ArgumentParser", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.parse_args", "ensemble_predictions.merge"], "function", ["home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.model_selection.ensemble.merge"], ["", "", "def", "main", "(", ")", ":", "\n", "    ", "import", "argparse", "\n", "parser", "=", "argparse", ".", "ArgumentParser", "(", "description", "=", "\"This script will merge predictions (that were prdicted with the \"", "\n", "\"-npz option!). You need to specify a postprocessing file so that \"", "\n", "\"we know here what postprocessing must be applied. Failing to do so \"", "\n", "\"will disable postprocessing\"", ")", "\n", "parser", ".", "add_argument", "(", "'-f'", ",", "'--folders'", ",", "nargs", "=", "'+'", ",", "help", "=", "\"list of folders to merge. All folders must contain npz \"", "\n", "\"files\"", ",", "required", "=", "True", ")", "\n", "parser", ".", "add_argument", "(", "'-o'", ",", "'--output_folder'", ",", "help", "=", "\"where to save the results\"", ",", "required", "=", "True", ",", "type", "=", "str", ")", "\n", "parser", ".", "add_argument", "(", "'-t'", ",", "'--threads'", ",", "help", "=", "\"number of threads used to saving niftis\"", ",", "required", "=", "False", ",", "default", "=", "2", ",", "\n", "type", "=", "int", ")", "\n", "parser", ".", "add_argument", "(", "'-pp'", ",", "'--postprocessing_file'", ",", "help", "=", "\"path to the file where the postprocessing configuration \"", "\n", "\"is stored. If this is not provided then no postprocessing \"", "\n", "\"will be made. It is strongly recommended to provide the \"", "\n", "\"postprocessing file!\"", ",", "\n", "required", "=", "False", ",", "type", "=", "str", ",", "default", "=", "None", ")", "\n", "parser", ".", "add_argument", "(", "'--npz'", ",", "action", "=", "\"store_true\"", ",", "required", "=", "False", ",", "help", "=", "\"stores npz and pkl\"", ")", "\n", "\n", "args", "=", "parser", ".", "parse_args", "(", ")", "\n", "\n", "folders", "=", "args", ".", "folders", "\n", "threads", "=", "args", ".", "threads", "\n", "output_folder", "=", "args", ".", "output_folder", "\n", "pp_file", "=", "args", ".", "postprocessing_file", "\n", "npz", "=", "args", ".", "npz", "\n", "\n", "merge", "(", "folders", ",", "output_folder", ",", "threads", ",", "override", "=", "True", ",", "postprocessing_file", "=", "pp_file", ",", "store_npz", "=", "npz", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.pretrained_models.collect_pretrained_models.copy_fold": [[26, 34], ["shutil.copy", "shutil.copy", "shutil.copy", "shutil.copy", "isfile", "join", "join", "join", "join", "join", "join", "join", "join", "join", "shutil.copy", "join", "join"], "function", ["None"], ["def", "copy_fold", "(", "in_folder", ":", "str", ",", "out_folder", ":", "str", ")", ":", "\n", "    ", "shutil", ".", "copy", "(", "join", "(", "in_folder", ",", "\"debug.json\"", ")", ",", "join", "(", "out_folder", ",", "\"debug.json\"", ")", ")", "\n", "shutil", ".", "copy", "(", "join", "(", "in_folder", ",", "\"model_final_checkpoint.model\"", ")", ",", "join", "(", "out_folder", ",", "\"model_final_checkpoint.model\"", ")", ")", "\n", "shutil", ".", "copy", "(", "join", "(", "in_folder", ",", "\"model_final_checkpoint.model.pkl\"", ")", ",", "\n", "join", "(", "out_folder", ",", "\"model_final_checkpoint.model.pkl\"", ")", ")", "\n", "shutil", ".", "copy", "(", "join", "(", "in_folder", ",", "\"progress.png\"", ")", ",", "join", "(", "out_folder", ",", "\"progress.png\"", ")", ")", "\n", "if", "isfile", "(", "join", "(", "in_folder", ",", "\"network_architecture.pdf\"", ")", ")", ":", "\n", "        ", "shutil", ".", "copy", "(", "join", "(", "in_folder", ",", "\"network_architecture.pdf\"", ")", ",", "join", "(", "out_folder", ",", "\"network_architecture.pdf\"", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.pretrained_models.collect_pretrained_models.copy_model": [[36, 55], ["all", "isfile", "isfile", "shutil.copy", "shutil.copy", "join", "join", "maybe_mkdir_p", "collect_pretrained_models.copy_fold", "join", "join", "join", "join", "range", "isdir", "join", "join", "join", "join"], "function", ["home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.pretrained_models.collect_pretrained_models.copy_fold"], ["", "", "def", "copy_model", "(", "directory", ":", "str", ",", "output_directory", ":", "str", ")", ":", "\n", "    ", "\"\"\"\n\n    :param directory: must have the 5 fold_X subfolders as well as a postprocessing.json and plans.pkl\n    :param output_directory:\n    :return:\n    \"\"\"", "\n", "expected_folders", "=", "[", "\"fold_%d\"", "%", "i", "for", "i", "in", "range", "(", "5", ")", "]", "\n", "assert", "all", "(", "[", "isdir", "(", "join", "(", "directory", ",", "i", ")", ")", "for", "i", "in", "expected_folders", "]", ")", ",", "\"not all folds present\"", "\n", "\n", "assert", "isfile", "(", "join", "(", "directory", ",", "\"plans.pkl\"", ")", ")", ",", "\"plans.pkl missing\"", "\n", "assert", "isfile", "(", "join", "(", "directory", ",", "\"postprocessing.json\"", ")", ")", ",", "\"postprocessing.json missing\"", "\n", "\n", "for", "e", "in", "expected_folders", ":", "\n", "        ", "maybe_mkdir_p", "(", "join", "(", "output_directory", ",", "e", ")", ")", "\n", "copy_fold", "(", "join", "(", "directory", ",", "e", ")", ",", "join", "(", "output_directory", ",", "e", ")", ")", "\n", "\n", "", "shutil", ".", "copy", "(", "join", "(", "directory", ",", "\"plans.pkl\"", ")", ",", "join", "(", "output_directory", ",", "\"plans.pkl\"", ")", ")", "\n", "shutil", ".", "copy", "(", "join", "(", "directory", ",", "\"postprocessing.json\"", ")", ",", "join", "(", "output_directory", ",", "\"postprocessing.json\"", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.pretrained_models.collect_pretrained_models.copy_pretrained_models_for_task": [[57, 77], ["join", "join", "maybe_mkdir_p", "collect_pretrained_models.copy_model", "isdir", "print", "RuntimeError"], "function", ["home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.pretrained_models.collect_pretrained_models.copy_model"], ["", "def", "copy_pretrained_models_for_task", "(", "task_name", ":", "str", ",", "output_directory", ":", "str", ",", "\n", "models", ":", "tuple", "=", "(", "\"2d\"", ",", "\"3d_lowres\"", ",", "\"3d_fullres\"", ",", "\"3d_cascade_fullres\"", ")", ",", "\n", "nnunet_trainer", "=", "default_trainer", ",", "\n", "nnunet_trainer_cascade", "=", "default_cascade_trainer", ",", "\n", "plans_identifier", "=", "default_plans_identifier", ")", ":", "\n", "    ", "trainer_output_dir", "=", "nnunet_trainer", "+", "\"__\"", "+", "plans_identifier", "\n", "trainer_output_dir_cascade", "=", "nnunet_trainer_cascade", "+", "\"__\"", "+", "plans_identifier", "\n", "\n", "for", "m", "in", "models", ":", "\n", "        ", "to", "=", "trainer_output_dir_cascade", "if", "m", "==", "\"3d_cascade_fullres\"", "else", "trainer_output_dir", "\n", "expected_output_folder", "=", "join", "(", "network_training_output_dir", ",", "m", ",", "task_name", ",", "to", ")", "\n", "if", "not", "isdir", "(", "expected_output_folder", ")", ":", "\n", "            ", "if", "m", "==", "\"3d_lowres\"", "or", "m", "==", "\"3d_cascade_fullres\"", ":", "\n", "                ", "print", "(", "\"Task\"", ",", "task_name", ",", "\"does not seem to have the cascade\"", ")", "\n", "continue", "\n", "", "else", ":", "\n", "                ", "raise", "RuntimeError", "(", "\"missing folder! %s\"", "%", "expected_output_folder", ")", "\n", "", "", "output_here", "=", "join", "(", "output_directory", ",", "m", ",", "task_name", ",", "to", ")", "\n", "maybe_mkdir_p", "(", "output_here", ")", "\n", "copy_model", "(", "expected_output_folder", ",", "output_here", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.pretrained_models.collect_pretrained_models.check_if_valid": [[79, 91], ["ensemble.split", "mb1.split", "mb2.split", "len"], "function", ["None"], ["", "", "def", "check_if_valid", "(", "ensemble", ":", "str", ",", "valid_models", ",", "valid_trainers", ",", "valid_plans", ")", ":", "\n", "    ", "ensemble", "=", "ensemble", "[", "len", "(", "\"ensemble_\"", ")", ":", "]", "\n", "mb1", ",", "mb2", "=", "ensemble", ".", "split", "(", "\"--\"", ")", "\n", "c1", ",", "tr1", ",", "p1", "=", "mb1", ".", "split", "(", "\"__\"", ")", "\n", "c2", ",", "tr2", ",", "p2", "=", "mb2", ".", "split", "(", "\"__\"", ")", "\n", "if", "c1", "not", "in", "valid_models", ":", "return", "False", "\n", "if", "c2", "not", "in", "valid_models", ":", "return", "False", "\n", "if", "tr1", "not", "in", "valid_trainers", ":", "return", "False", "\n", "if", "tr2", "not", "in", "valid_trainers", ":", "return", "False", "\n", "if", "p1", "not", "in", "valid_plans", ":", "return", "False", "\n", "if", "p2", "not", "in", "valid_plans", ":", "return", "False", "\n", "return", "True", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.pretrained_models.collect_pretrained_models.copy_ensembles": [[93, 112], ["join", "subdirs", "join", "maybe_mkdir_p", "isdir", "print", "collect_pretrained_models.check_if_valid", "join", "maybe_mkdir_p", "shutil.copy", "valid.append", "join"], "function", ["home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.pretrained_models.collect_pretrained_models.check_if_valid"], ["", "def", "copy_ensembles", "(", "taskname", ",", "output_folder", ",", "valid_models", "=", "(", "'2d'", ",", "'3d_fullres'", ",", "'3d_lowres'", ",", "'3d_cascade_fullres'", ")", ",", "\n", "valid_trainers", "=", "(", "default_trainer", ",", "default_cascade_trainer", ")", ",", "\n", "valid_plans", "=", "(", "default_plans_identifier", ",", ")", ")", ":", "\n", "    ", "ensemble_dir", "=", "join", "(", "network_training_output_dir", ",", "'ensembles'", ",", "taskname", ")", "\n", "if", "not", "isdir", "(", "ensemble_dir", ")", ":", "\n", "        ", "print", "(", "\"No ensemble directory found for task\"", ",", "taskname", ")", "\n", "return", "\n", "", "subd", "=", "subdirs", "(", "ensemble_dir", ",", "join", "=", "False", ")", "\n", "valid", "=", "[", "]", "\n", "for", "s", "in", "subd", ":", "\n", "        ", "v", "=", "check_if_valid", "(", "s", ",", "valid_models", ",", "valid_trainers", ",", "valid_plans", ")", "\n", "if", "v", ":", "\n", "            ", "valid", ".", "append", "(", "s", ")", "\n", "", "", "output_ensemble", "=", "join", "(", "output_folder", ",", "'ensembles'", ",", "taskname", ")", "\n", "maybe_mkdir_p", "(", "output_ensemble", ")", "\n", "for", "v", "in", "valid", ":", "\n", "        ", "this_output", "=", "join", "(", "output_ensemble", ",", "v", ")", "\n", "maybe_mkdir_p", "(", "this_output", ")", "\n", "shutil", ".", "copy", "(", "join", "(", "ensemble_dir", ",", "v", ",", "'postprocessing.json'", ")", ",", "this_output", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.pretrained_models.collect_pretrained_models.compress_everything": [[114, 124], ["multiprocessing.pool.Pool", "subfolders", "zip", "multiprocessing.pool.Pool.starmap", "multiprocessing.pool.Pool.close", "multiprocessing.pool.Pool.join", "args.append", "i.split", "join", "join"], "function", ["None"], ["", "", "def", "compress_everything", "(", "output_base", ",", "num_processes", "=", "8", ")", ":", "\n", "    ", "p", "=", "Pool", "(", "num_processes", ")", "\n", "tasks", "=", "subfolders", "(", "output_base", ",", "join", "=", "False", ")", "\n", "tasknames", "=", "[", "i", ".", "split", "(", "'/'", ")", "[", "-", "1", "]", "for", "i", "in", "tasks", "]", "\n", "args", "=", "[", "]", "\n", "for", "t", ",", "tn", "in", "zip", "(", "tasks", ",", "tasknames", ")", ":", "\n", "        ", "args", ".", "append", "(", "(", "join", "(", "output_base", ",", "tn", "+", "\".zip\"", ")", ",", "join", "(", "output_base", ",", "t", ")", ")", ")", "\n", "", "p", ".", "starmap", "(", "compress_folder", ",", "args", ")", "\n", "p", ".", "close", "(", ")", "\n", "p", ".", "join", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.pretrained_models.collect_pretrained_models.compress_folder": [[126, 132], ["zipfile.ZipFile", "os.walk", "zipfile.ZipFile.write", "join", "os.path.relpath", "join"], "function", ["None"], ["", "def", "compress_folder", "(", "zip_file", ",", "folder", ")", ":", "\n", "    ", "\"\"\"inspired by https://stackoverflow.com/questions/1855095/how-to-create-a-zip-archive-of-a-directory-in-python\"\"\"", "\n", "zipf", "=", "zipfile", ".", "ZipFile", "(", "zip_file", ",", "'w'", ",", "zipfile", ".", "ZIP_DEFLATED", ")", "\n", "for", "root", ",", "dirs", ",", "files", "in", "os", ".", "walk", "(", "folder", ")", ":", "\n", "        ", "for", "file", "in", "files", ":", "\n", "            ", "zipf", ".", "write", "(", "join", "(", "root", ",", "file", ")", ",", "os", ".", "path", ".", "relpath", "(", "join", "(", "root", ",", "file", ")", ",", "folder", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.pretrained_models.collect_pretrained_models.export_one_task": [[134, 141], ["collect_pretrained_models.copy_pretrained_models_for_task", "collect_pretrained_models.copy_ensembles", "collect_pretrained_models.compress_folder", "join", "join"], "function", ["home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.pretrained_models.collect_pretrained_models.copy_pretrained_models_for_task", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.pretrained_models.collect_pretrained_models.copy_ensembles", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.pretrained_models.collect_pretrained_models.compress_folder"], ["", "", "", "def", "export_one_task", "(", "taskname", ",", "models", ",", "output_folder", ",", "nnunet_trainer", "=", "default_trainer", ",", "\n", "nnunet_trainer_cascade", "=", "default_cascade_trainer", ",", "\n", "plans_identifier", "=", "default_plans_identifier", ")", ":", "\n", "    ", "copy_pretrained_models_for_task", "(", "taskname", ",", "output_folder", ",", "models", ",", "nnunet_trainer", ",", "nnunet_trainer_cascade", ",", "\n", "plans_identifier", ")", "\n", "copy_ensembles", "(", "taskname", ",", "output_folder", ",", "models", ",", "(", "nnunet_trainer", ",", "nnunet_trainer_cascade", ")", ",", "(", "plans_identifier", ",", ")", ")", "\n", "compress_folder", "(", "join", "(", "output_folder", ",", "taskname", "+", "'.zip'", ")", ",", "join", "(", "output_folder", ",", "taskname", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.pretrained_models.collect_pretrained_models.export_pretrained_model": [[143, 213], ["zipfile.ZipFile", "join", "subdirs", "zipfile.ZipFile.close", "join", "all", "isfile", "zipfile.ZipFile.write", "isdir", "print", "collect_pretrained_models.check_if_valid", "zipfile.ZipFile.write", "isdir", "join", "zipfile.ZipFile.write", "zipfile.ZipFile.write", "zipfile.ZipFile.write", "zipfile.ZipFile.write", "isfile", "join", "os.path.relpath", "isfile", "zipfile.ZipFile.write", "valid.append", "join", "os.path.relpath", "RuntimeError", "isdir", "join", "os.path.relpath", "join", "os.path.relpath", "join", "os.path.relpath", "join", "os.path.relpath", "join", "zipfile.ZipFile.write", "join", "join", "RuntimeError", "print", "join", "os.path.relpath", "join", "join", "join", "join", "join", "join", "join", "os.path.relpath", "join", "join"], "function", ["home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.pretrained_models.collect_pretrained_models.check_if_valid"], ["", "def", "export_pretrained_model", "(", "task_name", ":", "str", ",", "output_file", ":", "str", ",", "\n", "models", ":", "tuple", "=", "(", "\"2d\"", ",", "\"3d_lowres\"", ",", "\"3d_fullres\"", ",", "\"3d_cascade_fullres\"", ")", ",", "\n", "nnunet_trainer", "=", "default_trainer", ",", "\n", "nnunet_trainer_cascade", "=", "default_cascade_trainer", ",", "\n", "plans_identifier", "=", "default_plans_identifier", ",", "\n", "folds", "=", "(", "0", ",", "1", ",", "2", ",", "3", ",", "4", ")", ",", "strict", "=", "True", ")", ":", "\n", "    ", "zipf", "=", "zipfile", ".", "ZipFile", "(", "output_file", ",", "'w'", ",", "zipfile", ".", "ZIP_DEFLATED", ")", "\n", "trainer_output_dir", "=", "nnunet_trainer", "+", "\"__\"", "+", "plans_identifier", "\n", "trainer_output_dir_cascade", "=", "nnunet_trainer_cascade", "+", "\"__\"", "+", "plans_identifier", "\n", "\n", "for", "m", "in", "models", ":", "\n", "        ", "to", "=", "trainer_output_dir_cascade", "if", "m", "==", "\"3d_cascade_fullres\"", "else", "trainer_output_dir", "\n", "expected_output_folder", "=", "join", "(", "network_training_output_dir", ",", "m", ",", "task_name", ",", "to", ")", "\n", "if", "not", "isdir", "(", "expected_output_folder", ")", ":", "\n", "            ", "if", "strict", ":", "\n", "                ", "raise", "RuntimeError", "(", "\"Task %s is missing the model %s\"", "%", "(", "task_name", ",", "m", ")", ")", "\n", "", "else", ":", "\n", "                ", "continue", "\n", "\n", "", "", "expected_folders", "=", "[", "\"fold_%d\"", "%", "i", "if", "i", "!=", "'all'", "else", "i", "for", "i", "in", "folds", "]", "\n", "assert", "all", "(", "[", "isdir", "(", "join", "(", "expected_output_folder", ",", "i", ")", ")", "for", "i", "in", "expected_folders", "]", ")", ",", "\"not all requested folds \"", "\"present, \"", "\"Task %s model %s\"", "%", "(", "task_name", ",", "m", ")", "\n", "\n", "assert", "isfile", "(", "join", "(", "expected_output_folder", ",", "\"plans.pkl\"", ")", ")", ",", "\"plans.pkl missing, Task %s model %s\"", "%", "(", "task_name", ",", "m", ")", "\n", "\n", "for", "e", "in", "expected_folders", ":", "\n", "            ", "zipf", ".", "write", "(", "join", "(", "expected_output_folder", ",", "e", ",", "\"debug.json\"", ")", ",", "\n", "os", ".", "path", ".", "relpath", "(", "join", "(", "expected_output_folder", ",", "e", ",", "\"debug.json\"", ")", ",", "\n", "network_training_output_dir", ")", ")", "\n", "zipf", ".", "write", "(", "join", "(", "expected_output_folder", ",", "e", ",", "\"model_final_checkpoint.model\"", ")", ",", "\n", "os", ".", "path", ".", "relpath", "(", "join", "(", "expected_output_folder", ",", "e", ",", "\"model_final_checkpoint.model\"", ")", ",", "\n", "network_training_output_dir", ")", ")", "\n", "zipf", ".", "write", "(", "join", "(", "expected_output_folder", ",", "e", ",", "\"model_final_checkpoint.model.pkl\"", ")", ",", "\n", "os", ".", "path", ".", "relpath", "(", "join", "(", "expected_output_folder", ",", "e", ",", "\"model_final_checkpoint.model.pkl\"", ")", ",", "\n", "network_training_output_dir", ")", ")", "\n", "zipf", ".", "write", "(", "join", "(", "expected_output_folder", ",", "e", ",", "\"progress.png\"", ")", ",", "\n", "os", ".", "path", ".", "relpath", "(", "join", "(", "expected_output_folder", ",", "e", ",", "\"progress.png\"", ")", ",", "network_training_output_dir", ")", ")", "\n", "if", "isfile", "(", "join", "(", "expected_output_folder", ",", "e", ",", "\"network_architecture.pdf\"", ")", ")", ":", "\n", "                ", "zipf", ".", "write", "(", "join", "(", "expected_output_folder", ",", "e", ",", "\"network_architecture.pdf\"", ")", ",", "\n", "os", ".", "path", ".", "relpath", "(", "join", "(", "expected_output_folder", ",", "e", ",", "\"network_architecture.pdf\"", ")", ",", "\n", "network_training_output_dir", ")", ")", "\n", "\n", "", "", "zipf", ".", "write", "(", "join", "(", "expected_output_folder", ",", "\"plans.pkl\"", ")", ",", "\n", "os", ".", "path", ".", "relpath", "(", "join", "(", "expected_output_folder", ",", "\"plans.pkl\"", ")", ",", "network_training_output_dir", ")", ")", "\n", "if", "not", "isfile", "(", "join", "(", "expected_output_folder", ",", "\"postprocessing.json\"", ")", ")", ":", "\n", "            ", "if", "strict", ":", "\n", "                ", "raise", "RuntimeError", "(", "'postprocessing.json missing. Run nnUNet_determine_postprocessing or disable strict'", ")", "\n", "", "else", ":", "\n", "                ", "print", "(", "'WARNING: postprocessing.json missing'", ")", "\n", "", "", "else", ":", "\n", "            ", "zipf", ".", "write", "(", "join", "(", "expected_output_folder", ",", "\"postprocessing.json\"", ")", ",", "\n", "os", ".", "path", ".", "relpath", "(", "join", "(", "expected_output_folder", ",", "\"postprocessing.json\"", ")", ",", "network_training_output_dir", ")", ")", "\n", "\n", "", "", "ensemble_dir", "=", "join", "(", "network_training_output_dir", ",", "'ensembles'", ",", "task_name", ")", "\n", "if", "not", "isdir", "(", "ensemble_dir", ")", ":", "\n", "        ", "print", "(", "\"No ensemble directory found for task\"", ",", "task_name", ")", "\n", "return", "\n", "", "subd", "=", "subdirs", "(", "ensemble_dir", ",", "join", "=", "False", ")", "\n", "valid", "=", "[", "]", "\n", "for", "s", "in", "subd", ":", "\n", "        ", "v", "=", "check_if_valid", "(", "s", ",", "models", ",", "(", "nnunet_trainer", ",", "nnunet_trainer_cascade", ")", ",", "(", "plans_identifier", ")", ")", "\n", "if", "v", ":", "\n", "            ", "valid", ".", "append", "(", "s", ")", "\n", "", "", "for", "v", "in", "valid", ":", "\n", "        ", "zipf", ".", "write", "(", "join", "(", "ensemble_dir", ",", "v", ",", "'postprocessing.json'", ")", ",", "\n", "os", ".", "path", ".", "relpath", "(", "join", "(", "ensemble_dir", ",", "v", ",", "'postprocessing.json'", ")", ",", "\n", "network_training_output_dir", ")", ")", "\n", "", "zipf", ".", "close", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.pretrained_models.collect_pretrained_models.export_entry_point": [[215, 255], ["argparse.ArgumentParser", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.parse_args", "nnunet.utilities.task_name_id_conversion.convert_id_to_task_name.startswith", "collect_pretrained_models.export_pretrained_model", "nnunet.utilities.task_name_id_conversion.convert_id_to_task_name", "int", "int", "print"], "function", ["home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.pretrained_models.collect_pretrained_models.export_pretrained_model", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.utilities.task_name_id_conversion.convert_id_to_task_name"], ["", "def", "export_entry_point", "(", ")", ":", "\n", "    ", "import", "argparse", "\n", "parser", "=", "argparse", ".", "ArgumentParser", "(", "description", "=", "\"Use this script to export models to a zip file for sharing with \"", "\n", "\"others. You can upload the zip file and then either share the url \"", "\n", "\"for usage with nnUNet_download_pretrained_model_by_url, or share the \"", "\n", "\"zip for usage with nnUNet_install_pretrained_model_from_zip\"", ")", "\n", "parser", ".", "add_argument", "(", "'-t'", ",", "type", "=", "str", ",", "help", "=", "'task name or task id'", ")", "\n", "parser", ".", "add_argument", "(", "'-o'", ",", "type", "=", "str", ",", "help", "=", "'output file name. Should end with .zip'", ")", "\n", "parser", ".", "add_argument", "(", "'-m'", ",", "nargs", "=", "'+'", ",", "\n", "help", "=", "'list of model configurations. Default: 2d 3d_lowres 3d_fullres 3d_cascade_fullres. Must '", "\n", "'be adapted to fit the available models of a task'", ",", "\n", "default", "=", "(", "\"2d\"", ",", "\"3d_lowres\"", ",", "\"3d_fullres\"", ",", "\"3d_cascade_fullres\"", ")", ",", "required", "=", "False", ")", "\n", "parser", ".", "add_argument", "(", "'-tr'", ",", "type", "=", "str", ",", "help", "=", "'trainer class used for 2d 3d_lowres and 3d_fullres. '", "\n", "'Default: %s'", "%", "default_trainer", ",", "required", "=", "False", ",", "default", "=", "default_trainer", ")", "\n", "parser", ".", "add_argument", "(", "'-trc'", ",", "type", "=", "str", ",", "help", "=", "'trainer class used for 3d_cascade_fullres. '", "\n", "'Default: %s'", "%", "default_cascade_trainer", ",", "required", "=", "False", ",", "\n", "default", "=", "default_cascade_trainer", ")", "\n", "parser", ".", "add_argument", "(", "'-pl'", ",", "type", "=", "str", ",", "help", "=", "'nnunet plans identifier. Default: %s'", "%", "default_plans_identifier", ",", "\n", "required", "=", "False", ",", "default", "=", "default_plans_identifier", ")", "\n", "parser", ".", "add_argument", "(", "'--disable_strict'", ",", "action", "=", "'store_true'", ",", "help", "=", "'set this if you want to allow skipping '", "\n", "'missing things'", ",", "required", "=", "False", ")", "\n", "parser", ".", "add_argument", "(", "'-f'", ",", "nargs", "=", "'+'", ",", "type", "=", "str", ",", "help", "=", "'Folds. Default: 0 1 2 3 4'", ",", "required", "=", "False", ",", "default", "=", "default_trainer", ")", "\n", "args", "=", "parser", ".", "parse_args", "(", ")", "\n", "\n", "folds", "=", "args", ".", "f", "\n", "folds", "=", "[", "int", "(", "i", ")", "if", "i", "!=", "'all'", "else", "i", "for", "i", "in", "folds", "]", "\n", "\n", "taskname", "=", "args", ".", "t", "\n", "if", "taskname", ".", "startswith", "(", "\"Task\"", ")", ":", "\n", "        ", "pass", "\n", "", "else", ":", "\n", "        ", "try", ":", "\n", "            ", "taskid", "=", "int", "(", "taskname", ")", "\n", "", "except", "Exception", "as", "e", ":", "\n", "            ", "print", "(", "'-t must be either a Task name (TaskXXX_YYY) or a task id (integer)'", ")", "\n", "raise", "e", "\n", "", "taskname", "=", "convert_id_to_task_name", "(", "taskid", ")", "\n", "\n", "", "export_pretrained_model", "(", "taskname", ",", "args", ".", "o", ",", "args", ".", "m", ",", "args", ".", "tr", ",", "args", ".", "trc", ",", "args", ".", "pl", ",", "strict", "=", "not", "args", ".", "disable_strict", ",", "\n", "folds", "=", "folds", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.pretrained_models.collect_pretrained_models.export_for_paper": [[257, 272], ["collect_pretrained_models.compress_everything", "nnunet.utilities.task_name_id_conversion.convert_id_to_task_name", "print", "join", "maybe_mkdir_p", "collect_pretrained_models.copy_pretrained_models_for_task", "collect_pretrained_models.copy_ensembles"], "function", ["home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.pretrained_models.collect_pretrained_models.compress_everything", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.utilities.task_name_id_conversion.convert_id_to_task_name", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.pretrained_models.collect_pretrained_models.copy_pretrained_models_for_task", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.pretrained_models.collect_pretrained_models.copy_ensembles"], ["", "def", "export_for_paper", "(", ")", ":", "\n", "    ", "output_base", "=", "\"/media/fabian/DeepLearningData/nnunet_trained_models\"", "\n", "task_ids", "=", "[", "1", ",", "2", ",", "3", ",", "4", ",", "5", ",", "6", ",", "7", ",", "8", ",", "9", ",", "10", ",", "17", ",", "24", ",", "27", ",", "29", ",", "35", ",", "48", ",", "55", ",", "61", ",", "38", "]", "\n", "for", "t", "in", "task_ids", ":", "\n", "        ", "if", "t", "==", "61", ":", "\n", "            ", "models", "=", "(", "\"3d_fullres\"", ",", ")", "\n", "", "else", ":", "\n", "            ", "models", "=", "(", "\"2d\"", ",", "\"3d_lowres\"", ",", "\"3d_fullres\"", ",", "\"3d_cascade_fullres\"", ")", "\n", "", "taskname", "=", "convert_id_to_task_name", "(", "t", ")", "\n", "print", "(", "taskname", ")", "\n", "output_folder", "=", "join", "(", "output_base", ",", "taskname", ")", "\n", "maybe_mkdir_p", "(", "output_folder", ")", "\n", "copy_pretrained_models_for_task", "(", "taskname", ",", "output_folder", ",", "models", ")", "\n", "copy_ensembles", "(", "taskname", ",", "output_folder", ")", "\n", "", "compress_everything", "(", "output_base", ",", "8", ")", "\n", "", ""]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.pretrained_models.download_pretrained_model.get_available_models": [[27, 188], ["None"], "function", ["None"], ["def", "get_available_models", "(", ")", ":", "\n", "    ", "available_models", "=", "{", "\n", "\"Task001_BrainTumour\"", ":", "{", "\n", "'description'", ":", "\"Brain Tumor Segmentation. \\n\"", "\n", "\"Segmentation targets are edema, enhancing tumor and necrosis, \\n\"", "\n", "\"input modalities are 0: FLAIR, 1: T1, 2: T1 with contrast agent, 3: T2. \\n\"", "\n", "\"Also see Medical Segmentation Decathlon, http://medicaldecathlon.com/\"", ",", "\n", "'url'", ":", "\"https://zenodo.org/record/4003545/files/Task001_BrainTumour.zip?download=1\"", "\n", "}", ",", "\n", "\"Task002_Heart\"", ":", "{", "\n", "'description'", ":", "\"Left Atrium Segmentation. \\n\"", "\n", "\"Segmentation target is the left atrium, \\n\"", "\n", "\"input modalities are 0: MRI. \\n\"", "\n", "\"Also see Medical Segmentation Decathlon, http://medicaldecathlon.com/\"", ",", "\n", "'url'", ":", "\"https://zenodo.org/record/4003545/files/Task002_Heart.zip?download=1\"", "\n", "}", ",", "\n", "\"Task003_Liver\"", ":", "{", "\n", "'description'", ":", "\"Liver and Liver Tumor Segmentation. \\n\"", "\n", "\"Segmentation targets are liver and tumors, \\n\"", "\n", "\"input modalities are 0: abdominal CT scan. \\n\"", "\n", "\"Also see Medical Segmentation Decathlon, http://medicaldecathlon.com/\"", ",", "\n", "'url'", ":", "\"https://zenodo.org/record/4003545/files/Task003_Liver.zip?download=1\"", "\n", "}", ",", "\n", "\"Task004_Hippocampus\"", ":", "{", "\n", "'description'", ":", "\"Hippocampus Segmentation. \\n\"", "\n", "\"Segmentation targets posterior and anterior parts of the hippocampus, \\n\"", "\n", "\"input modalities are 0: MRI. \\n\"", "\n", "\"Also see Medical Segmentation Decathlon, http://medicaldecathlon.com/\"", ",", "\n", "'url'", ":", "\"https://zenodo.org/record/4003545/files/Task004_Hippocampus.zip?download=1\"", "\n", "}", ",", "\n", "\"Task005_Prostate\"", ":", "{", "\n", "'description'", ":", "\"Prostate Segmentation. \\n\"", "\n", "\"Segmentation targets are peripheral and central zone, \\n\"", "\n", "\"input modalities are 0: T2, 1: ADC. \\n\"", "\n", "\"Also see Medical Segmentation Decathlon, http://medicaldecathlon.com/\"", ",", "\n", "'url'", ":", "\"https://zenodo.org/record/4003545/files/Task005_Prostate.zip?download=1\"", "\n", "}", ",", "\n", "\"Task006_Lung\"", ":", "{", "\n", "'description'", ":", "\"Lung Nodule Segmentation. \\n\"", "\n", "\"Segmentation target are lung nodules, \\n\"", "\n", "\"input modalities are 0: abdominal CT scan. \\n\"", "\n", "\"Also see Medical Segmentation Decathlon, http://medicaldecathlon.com/\"", ",", "\n", "'url'", ":", "\"https://zenodo.org/record/4003545/files/Task006_Lung.zip?download=1\"", "\n", "}", ",", "\n", "\"Task007_Pancreas\"", ":", "{", "\n", "'description'", ":", "\"Pancreas Segmentation. \\n\"", "\n", "\"Segmentation targets are pancras and pancreas tumor, \\n\"", "\n", "\"input modalities are 0: abdominal CT scan. \\n\"", "\n", "\"Also see Medical Segmentation Decathlon, http://medicaldecathlon.com/\"", ",", "\n", "'url'", ":", "\"https://zenodo.org/record/4003545/files/Task007_Pancreas.zip?download=1\"", "\n", "}", ",", "\n", "\"Task008_HepaticVessel\"", ":", "{", "\n", "'description'", ":", "\"Hepatic Vessel Segmentation. \\n\"", "\n", "\"Segmentation targets are hepatic vesels and liver tumors, \\n\"", "\n", "\"input modalities are 0: abdominal CT scan. \\n\"", "\n", "\"Also see Medical Segmentation Decathlon, http://medicaldecathlon.com/\"", ",", "\n", "'url'", ":", "\"https://zenodo.org/record/4003545/files/Task008_HepaticVessel.zip?download=1\"", "\n", "}", ",", "\n", "\"Task009_Spleen\"", ":", "{", "\n", "'description'", ":", "\"Spleen Segmentation. \\n\"", "\n", "\"Segmentation target is the spleen, \\n\"", "\n", "\"input modalities are 0: abdominal CT scan. \\n\"", "\n", "\"Also see Medical Segmentation Decathlon, http://medicaldecathlon.com/\"", ",", "\n", "'url'", ":", "\"https://zenodo.org/record/4003545/files/Task009_Spleen.zip?download=1\"", "\n", "}", ",", "\n", "\"Task010_Colon\"", ":", "{", "\n", "'description'", ":", "\"Colon Cancer Segmentation. \\n\"", "\n", "\"Segmentation target are colon caner primaries, \\n\"", "\n", "\"input modalities are 0: CT scan. \\n\"", "\n", "\"Also see Medical Segmentation Decathlon, http://medicaldecathlon.com/\"", ",", "\n", "'url'", ":", "\"https://zenodo.org/record/4003545/files/Task010_Colon.zip?download=1\"", "\n", "}", ",", "\n", "\"Task017_AbdominalOrganSegmentation\"", ":", "{", "\n", "'description'", ":", "\"Multi-Atlas Labeling Beyond the Cranial Vault - Abdomen. \\n\"", "\n", "\"Segmentation targets are thirteen different abdominal organs, \\n\"", "\n", "\"input modalities are 0: abdominal CT scan. \\n\"", "\n", "\"Also see https://www.synapse.org/#!Synapse:syn3193805/wiki/217754\"", ",", "\n", "'url'", ":", "\"https://zenodo.org/record/4003545/files/Task017_AbdominalOrganSegmentation.zip?download=1\"", "\n", "}", ",", "\n", "\"Task024_Promise\"", ":", "{", "\n", "'description'", ":", "\"Prostate MR Image Segmentation 2012. \\n\"", "\n", "\"Segmentation target is the prostate, \\n\"", "\n", "\"input modalities are 0: T2. \\n\"", "\n", "\"Also see https://promise12.grand-challenge.org/\"", ",", "\n", "'url'", ":", "\"https://zenodo.org/record/4003545/files/Task024_Promise.zip?download=1\"", "\n", "}", ",", "\n", "\"Task027_ACDC\"", ":", "{", "\n", "'description'", ":", "\"Automatic Cardiac Diagnosis Challenge. \\n\"", "\n", "\"Segmentation targets are right ventricle, left ventricular cavity and left myocardium, \\n\"", "\n", "\"input modalities are 0: cine MRI. \\n\"", "\n", "\"Also see https://acdc.creatis.insa-lyon.fr/\"", ",", "\n", "'url'", ":", "\"https://zenodo.org/record/4003545/files/Task027_ACDC.zip?download=1\"", "\n", "}", ",", "\n", "\"Task029_LiTS\"", ":", "{", "\n", "'description'", ":", "\"Liver and Liver Tumor Segmentation Challenge. \\n\"", "\n", "\"Segmentation targets are liver and liver tumors, \\n\"", "\n", "\"input modalities are 0: abdominal CT scan. \\n\"", "\n", "\"Also see https://competitions.codalab.org/competitions/17094\"", ",", "\n", "'url'", ":", "\"https://zenodo.org/record/4003545/files/Task029_LITS.zip?download=1\"", "\n", "}", ",", "\n", "\"Task035_ISBILesionSegmentation\"", ":", "{", "\n", "'description'", ":", "\"Longitudinal multiple sclerosis lesion segmentation Challenge. \\n\"", "\n", "\"Segmentation target is MS lesions, \\n\"", "\n", "\"input modalities are 0: FLAIR, 1: MPRAGE, 2: proton density, 3: T2. \\n\"", "\n", "\"Also see https://smart-stats-tools.org/lesion-challenge\"", ",", "\n", "'url'", ":", "\"https://zenodo.org/record/4003545/files/Task035_ISBILesionSegmentation.zip?download=1\"", "\n", "}", ",", "\n", "\"Task038_CHAOS_Task_3_5_Variant2\"", ":", "{", "\n", "'description'", ":", "\"CHAOS - Combined (CT-MR) Healthy Abdominal Organ Segmentation Challenge (Task 3 & 5). \\n\"", "\n", "\"Segmentation targets are left and right kidney, liver, spleen, \\n\"", "\n", "\"input modalities are 0: T1 in-phase, T1 out-phase, T2 (can be any of those)\\n\"", "\n", "\"Also see https://chaos.grand-challenge.org/\"", ",", "\n", "'url'", ":", "\"https://zenodo.org/record/4003545/files/Task038_CHAOS_Task_3_5_Variant2.zip?download=1\"", "\n", "}", ",", "\n", "\"Task048_KiTS_clean\"", ":", "{", "\n", "'description'", ":", "\"Kidney and Kidney Tumor Segmentation Challenge. \"", "\n", "\"Segmentation targets kidney and kidney tumors, \"", "\n", "\"input modalities are 0: abdominal CT scan. \"", "\n", "\"Also see https://kits19.grand-challenge.org/\"", ",", "\n", "'url'", ":", "\"https://zenodo.org/record/4003545/files/Task048_KiTS_clean.zip?download=1\"", "\n", "}", ",", "\n", "\"Task055_SegTHOR\"", ":", "{", "\n", "'description'", ":", "\"SegTHOR: Segmentation of THoracic Organs at Risk in CT images. \\n\"", "\n", "\"Segmentation targets are aorta, esophagus, heart and trachea, \\n\"", "\n", "\"input modalities are 0: CT scan. \\n\"", "\n", "\"Also see https://competitions.codalab.org/competitions/21145\"", ",", "\n", "'url'", ":", "\"https://zenodo.org/record/4003545/files/Task055_SegTHOR.zip?download=1\"", "\n", "}", ",", "\n", "\"Task061_CREMI\"", ":", "{", "\n", "'description'", ":", "\"MICCAI Challenge on Circuit Reconstruction from Electron Microscopy Images (Synaptic Cleft segmentation task). \\n\"", "\n", "\"Segmentation target is synaptic clefts, \\n\"", "\n", "\"input modalities are 0: serial section transmission electron microscopy of neural tissue. \\n\"", "\n", "\"Also see https://cremi.org/\"", ",", "\n", "'url'", ":", "\"https://zenodo.org/record/4003545/files/Task061_CREMI.zip?download=1\"", "\n", "}", ",", "\n", "\"Task075_Fluo_C3DH_A549_ManAndSim\"", ":", "{", "\n", "'description'", ":", "\"Fluo-C3DH-A549-SIM and Fluo-C3DH-A549 datasets of the cell tracking challenge. Segmentation target are C3DH cells in fluorescence microscopy images.\\n\"", "\n", "\"input modalities are 0: fluorescence_microscopy\\n\"", "\n", "\"Also see http://celltrackingchallenge.net/\"", ",", "\n", "'url'", ":", "\"https://zenodo.org/record/4003545/files/Task075_Fluo_C3DH_A549_ManAndSim.zip?download=1\"", "\n", "}", ",", "\n", "\"Task076_Fluo_N3DH_SIM\"", ":", "{", "\n", "'description'", ":", "\"Fluo-N3DH-SIM dataset of the cell tracking challenge. Segmentation target are N3DH cells and cell borders in fluorescence microscopy images.\\n\"", "\n", "\"input modalities are 0: fluorescence_microscopy\\n\"", "\n", "\"Also see http://celltrackingchallenge.net/\\n\"", ",", "\n", "\"Note that the segmentation output of the models are cell center and cell border. These outputs mus tbe converted to an instance segmentation for the challenge. \\n\"", "\n", "\"See https://github.com/MIC-DKFZ/nnUNet/blob/master/nnunet/dataset_conversion/Task076_Fluo_N3DH_SIM.py\"", "\n", "'url'", ":", "\"https://zenodo.org/record/4003545/files/Task076_Fluo_N3DH_SIM.zip?download=1\"", "\n", "}", ",", "\n", "\"Task089_Fluo-N2DH-SIM_thickborder_time\"", ":", "{", "\n", "'description'", ":", "\"Fluo-N2DH-SIM dataset of the cell tracking challenge. Segmentation target are nuclei of N2DH cells and cell borders in fluorescence microscopy images.\\n\"", "\n", "\"input modalities are 0: t minus 4, 0: t minus 3, 0: t minus 2, 0: t minus 1, 0: frame of interest\\n\"", "\n", "\"Note that the input channels are different time steps from a time series acquisition\\n\"", "\n", "\"Note that the segmentation output of the models are cell center and cell border. These outputs mus tbe converted to an instance segmentation for the challenge. \\n\"", "\n", "\"See https://github.com/MIC-DKFZ/nnUNet/blob/master/nnunet/dataset_conversion/Task089_Fluo-N2DH-SIM.py\"", "\n", "\"Also see http://celltrackingchallenge.net/\"", ",", "\n", "'url'", ":", "\"https://zenodo.org/record/4003545/files/Task089_Fluo-N2DH-SIM_thickborder_time.zip?download=1\"", "\n", "}", ",", "\n", "\n", "}", "\n", "return", "available_models", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.pretrained_models.download_pretrained_model.print_available_pretrained_models": [[190, 197], ["print", "download_pretrained_model.get_available_models", "get_available_models.keys", "print", "print", "print"], "function", ["home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.pretrained_models.download_pretrained_model.get_available_models"], ["", "def", "print_available_pretrained_models", "(", ")", ":", "\n", "    ", "print", "(", "'The following pretrained models are available:\\n'", ")", "\n", "av_models", "=", "get_available_models", "(", ")", "\n", "for", "m", "in", "av_models", ".", "keys", "(", ")", ":", "\n", "        ", "print", "(", "''", ")", "\n", "print", "(", "m", ")", "\n", "print", "(", "av_models", "[", "m", "]", "[", "'description'", "]", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.pretrained_models.download_pretrained_model.download_and_install_pretrained_model_by_name": [[199, 206], ["download_pretrained_model.get_available_models", "download_pretrained_model.download_and_install_from_url", "get_available_models.keys", "RuntimeError", "len", "RuntimeError"], "function", ["home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.pretrained_models.download_pretrained_model.get_available_models", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.pretrained_models.download_pretrained_model.download_and_install_from_url"], ["", "", "def", "download_and_install_pretrained_model_by_name", "(", "taskname", ")", ":", "\n", "    ", "av_models", "=", "get_available_models", "(", ")", "\n", "if", "taskname", "not", "in", "av_models", ".", "keys", "(", ")", ":", "\n", "        ", "raise", "RuntimeError", "(", "\"\\nThe requested pretrained model ('%s') is not available.\"", "%", "taskname", ")", "\n", "", "if", "len", "(", "av_models", "[", "taskname", "]", "[", "'url'", "]", ")", "==", "0", ":", "\n", "        ", "raise", "RuntimeError", "(", "\"The requested model has not been uploaded yet. Please check back in a few days\"", ")", "\n", "", "download_and_install_from_url", "(", "av_models", "[", "taskname", "]", "[", "'url'", "]", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.pretrained_models.download_pretrained_model.download_and_install_from_url": [[208, 239], ["os.path.expanduser", "int", "batchgenerators.utilities.file_and_folder_operations.join", "print", "download_pretrained_model.install_model_from_zip_file", "print", "batchgenerators.utilities.file_and_folder_operations.isfile", "time.time", "str", "open", "os.remove", "requests.get", "r.raise_for_status", "r.iter_content", "f.write"], "function", ["home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.pretrained_models.download_pretrained_model.install_model_from_zip_file"], ["", "def", "download_and_install_from_url", "(", "url", ")", ":", "\n", "    ", "assert", "network_training_output_dir", "is", "not", "None", ",", "\"Cannot install model because network_training_output_dir is not \"", "\"set (RESULTS_FOLDER missing as environment variable, see \"", "\"Installation instructions)\"", "\n", "import", "http", ".", "client", "\n", "http", ".", "client", ".", "HTTPConnection", ".", "_http_vsn", "=", "10", "\n", "http", ".", "client", ".", "HTTPConnection", ".", "_http_vsn_str", "=", "'HTTP/1.0'", "\n", "\n", "import", "os", "\n", "home", "=", "os", ".", "path", ".", "expanduser", "(", "'~'", ")", "\n", "random_number", "=", "int", "(", "time", "(", ")", "*", "1e7", ")", "\n", "tempfile", "=", "join", "(", "home", ",", "'.nnunetdownload_%s'", "%", "str", "(", "random_number", ")", ")", "\n", "\n", "try", ":", "\n", "        ", "with", "open", "(", "tempfile", ",", "'wb'", ")", "as", "f", ":", "\n", "            ", "with", "requests", ".", "get", "(", "url", ",", "stream", "=", "True", ")", "as", "r", ":", "\n", "                ", "r", ".", "raise_for_status", "(", ")", "\n", "for", "chunk", "in", "r", ".", "iter_content", "(", "chunk_size", "=", "8192", "*", "16", ")", ":", "\n", "# If you have chunk encoded response uncomment if", "\n", "# and set chunk_size parameter to None.", "\n", "# if chunk:", "\n", "                    ", "f", ".", "write", "(", "chunk", ")", "\n", "\n", "", "", "", "print", "(", "\"Download finished. Extracting...\"", ")", "\n", "install_model_from_zip_file", "(", "tempfile", ")", "\n", "print", "(", "\"Done\"", ")", "\n", "", "except", "Exception", "as", "e", ":", "\n", "        ", "raise", "e", "\n", "", "finally", ":", "\n", "        ", "if", "isfile", "(", "tempfile", ")", ":", "\n", "            ", "os", ".", "remove", "(", "tempfile", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.pretrained_models.download_pretrained_model.download_file": [[241, 253], ["requests.get", "r.raise_for_status", "open", "r.iter_content", "f.write"], "function", ["None"], ["", "", "", "def", "download_file", "(", "url", ",", "local_filename", ")", ":", "\n", "# borrowed from https://stackoverflow.com/questions/16694907/download-large-file-in-python-with-requests", "\n", "# NOTE the stream=True parameter below", "\n", "    ", "with", "requests", ".", "get", "(", "url", ",", "stream", "=", "True", ")", "as", "r", ":", "\n", "        ", "r", ".", "raise_for_status", "(", ")", "\n", "with", "open", "(", "local_filename", ",", "'wb'", ")", "as", "f", ":", "\n", "            ", "for", "chunk", "in", "r", ".", "iter_content", "(", "chunk_size", "=", "None", ")", ":", "\n", "# If you have chunk encoded response uncomment if", "\n", "# and set chunk_size parameter to None.", "\n", "#if chunk:", "\n", "                ", "f", ".", "write", "(", "chunk", ")", "\n", "", "", "", "return", "local_filename", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.pretrained_models.download_pretrained_model.install_model_from_zip_file": [[255, 257], ["subprocess.call", "tempfile"], "function", ["home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.common.layers.ResizeImage.call"], ["", "def", "install_model_from_zip_file", "(", "zip_file", ":", "str", ")", ":", "\n", "    ", "call", "(", "[", "'unzip'", ",", "'-o'", ",", "'-d'", ",", "network_training_output_dir", ",", "zip_file", "]", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.pretrained_models.download_pretrained_model.print_license_warning": [[259, 269], ["print", "print", "print", "print", "print", "print", "print"], "function", ["None"], ["", "def", "print_license_warning", "(", ")", ":", "\n", "    ", "print", "(", "''", ")", "\n", "print", "(", "'######################################################'", ")", "\n", "print", "(", "'!!!!!!!!!!!!!!!!!!!!!!!!WARNING!!!!!!!!!!!!!!!!!!!!!!!'", ")", "\n", "print", "(", "'######################################################'", ")", "\n", "print", "(", "\"Using the pretrained model weights is subject to the license of the dataset they were trained on. Some \"", "\n", "\"allow commercial use, others don't. It is your responsibility to make sure you use them appropriately! Use \"", "\n", "\"nnUNet_print_pretrained_model_info(task_name) to see a summary of the dataset and where to find its license!\"", ")", "\n", "print", "(", "'######################################################'", ")", "\n", "print", "(", "''", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.pretrained_models.download_pretrained_model.download_by_name": [[271, 284], ["argparse.ArgumentParser", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.parse_args", "download_pretrained_model.print_license_warning", "download_pretrained_model.download_and_install_pretrained_model_by_name"], "function", ["home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.pretrained_models.download_pretrained_model.print_license_warning", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.pretrained_models.download_pretrained_model.download_and_install_pretrained_model_by_name"], ["", "def", "download_by_name", "(", ")", ":", "\n", "    ", "import", "argparse", "\n", "parser", "=", "argparse", ".", "ArgumentParser", "(", "description", "=", "\"Use this to download pretrained models. CAREFUL: This script will \"", "\n", "\"overwrite \"", "\n", "\"existing models (if they share the same trainer class and plans as \"", "\n", "\"the pretrained model\"", ")", "\n", "parser", ".", "add_argument", "(", "\"task_name\"", ",", "type", "=", "str", ",", "help", "=", "'Task name of the pretrained model. To see '", "\n", "'available task names, run nnUNet_print_available_'", "\n", "'pretrained_models'", ")", "\n", "args", "=", "parser", ".", "parse_args", "(", ")", "\n", "taskname", "=", "args", ".", "task_name", "\n", "print_license_warning", "(", ")", "\n", "download_and_install_pretrained_model_by_name", "(", "taskname", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.pretrained_models.download_pretrained_model.download_by_url": [[286, 298], ["argparse.ArgumentParser", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.parse_args", "download_pretrained_model.download_and_install_from_url"], "function", ["home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.pretrained_models.download_pretrained_model.download_and_install_from_url"], ["", "def", "download_by_url", "(", ")", ":", "\n", "    ", "import", "argparse", "\n", "parser", "=", "argparse", ".", "ArgumentParser", "(", "\n", "description", "=", "\"Use this to download pretrained models. This script is intended to download models via url only. \"", "\n", "\"If you want to download one of our pretrained models, please use nnUNet_download_pretrained_model. \"", "\n", "\"CAREFUL: This script will overwrite \"", "\n", "\"existing models (if they share the same trainer class and plans as \"", "\n", "\"the pretrained model.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"url\"", ",", "type", "=", "str", ",", "help", "=", "'URL of the pretrained model'", ")", "\n", "args", "=", "parser", ".", "parse_args", "(", ")", "\n", "url", "=", "args", ".", "url", "\n", "download_and_install_from_url", "(", "url", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.pretrained_models.download_pretrained_model.install_from_zip_entry_point": [[300, 308], ["argparse.ArgumentParser", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.parse_args", "download_pretrained_model.install_model_from_zip_file"], "function", ["home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.pretrained_models.download_pretrained_model.install_model_from_zip_file"], ["", "def", "install_from_zip_entry_point", "(", ")", ":", "\n", "    ", "import", "argparse", "\n", "parser", "=", "argparse", ".", "ArgumentParser", "(", "\n", "description", "=", "\"Use this to install a zip file containing a pretrained model.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"zip\"", ",", "type", "=", "str", ",", "help", "=", "'zip file'", ")", "\n", "args", "=", "parser", ".", "parse_args", "(", ")", "\n", "zip", "=", "args", ".", "zip", "\n", "install_model_from_zip_file", "(", "zip", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.pretrained_models.download_pretrained_model.print_pretrained_model_requirements": [[310, 324], ["argparse.ArgumentParser", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.parse_args", "download_pretrained_model.get_available_models", "print", "get_available_models.keys", "RuntimeError"], "function", ["home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.pretrained_models.download_pretrained_model.get_available_models"], ["", "def", "print_pretrained_model_requirements", "(", ")", ":", "\n", "    ", "import", "argparse", "\n", "parser", "=", "argparse", ".", "ArgumentParser", "(", "description", "=", "\"Use this to see the properties of a pretrained model, especially \"", "\n", "\"what input modalities it requires\"", ")", "\n", "parser", ".", "add_argument", "(", "\"task_name\"", ",", "type", "=", "str", ",", "help", "=", "'Task name of the pretrained model. To see '", "\n", "'available task names, run nnUNet_print_available_'", "\n", "'pretrained_models'", ")", "\n", "args", "=", "parser", ".", "parse_args", "(", ")", "\n", "taskname", "=", "args", ".", "task_name", "\n", "av", "=", "get_available_models", "(", ")", "\n", "if", "taskname", "not", "in", "av", ".", "keys", "(", ")", ":", "\n", "        ", "raise", "RuntimeError", "(", "\"Invalid task name. This pretrained model does not exist. To see available task names, \"", "\n", "\"run nnUNet_print_available_pretrained_models\"", ")", "\n", "", "print", "(", "av", "[", "taskname", "]", "[", "'description'", "]", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.evaluation.metrics.ConfusionMatrix.__init__": [[27, 40], ["metrics.ConfusionMatrix.set_reference", "metrics.ConfusionMatrix.set_test"], "methods", ["home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.evaluation.evaluator.NiftiEvaluator.set_reference", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.evaluation.evaluator.NiftiEvaluator.set_test"], ["    ", "def", "__init__", "(", "self", ",", "test", "=", "None", ",", "reference", "=", "None", ")", ":", "\n", "\n", "        ", "self", ".", "tp", "=", "None", "\n", "self", ".", "fp", "=", "None", "\n", "self", ".", "tn", "=", "None", "\n", "self", ".", "fn", "=", "None", "\n", "self", ".", "size", "=", "None", "\n", "self", ".", "reference_empty", "=", "None", "\n", "self", ".", "reference_full", "=", "None", "\n", "self", ".", "test_empty", "=", "None", "\n", "self", ".", "test_full", "=", "None", "\n", "self", ".", "set_reference", "(", "reference", ")", "\n", "self", ".", "set_test", "(", "test", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.evaluation.metrics.ConfusionMatrix.set_test": [[41, 45], ["metrics.ConfusionMatrix.reset"], "methods", ["home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.evaluation.metrics.ConfusionMatrix.reset"], ["", "def", "set_test", "(", "self", ",", "test", ")", ":", "\n", "\n", "        ", "self", ".", "test", "=", "test", "\n", "self", ".", "reset", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.evaluation.metrics.ConfusionMatrix.set_reference": [[46, 50], ["metrics.ConfusionMatrix.reset"], "methods", ["home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.evaluation.metrics.ConfusionMatrix.reset"], ["", "def", "set_reference", "(", "self", ",", "reference", ")", ":", "\n", "\n", "        ", "self", ".", "reference", "=", "reference", "\n", "self", ".", "reset", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.evaluation.metrics.ConfusionMatrix.reset": [[51, 62], ["None"], "methods", ["None"], ["", "def", "reset", "(", "self", ")", ":", "\n", "\n", "        ", "self", ".", "tp", "=", "None", "\n", "self", ".", "fp", "=", "None", "\n", "self", ".", "tn", "=", "None", "\n", "self", ".", "fn", "=", "None", "\n", "self", ".", "size", "=", "None", "\n", "self", ".", "test_empty", "=", "None", "\n", "self", ".", "test_full", "=", "None", "\n", "self", ".", "reference_empty", "=", "None", "\n", "self", ".", "reference_full", "=", "None", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.evaluation.metrics.ConfusionMatrix.compute": [[63, 79], ["metrics.assert_shape", "int", "int", "int", "int", "int", "numpy.all", "numpy.all", "ValueError", "numpy.prod", "numpy.any", "numpy.any"], "methods", ["home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.evaluation.metrics.assert_shape"], ["", "def", "compute", "(", "self", ")", ":", "\n", "\n", "        ", "if", "self", ".", "test", "is", "None", "or", "self", ".", "reference", "is", "None", ":", "\n", "            ", "raise", "ValueError", "(", "\"'test' and 'reference' must both be set to compute confusion matrix.\"", ")", "\n", "\n", "", "assert_shape", "(", "self", ".", "test", ",", "self", ".", "reference", ")", "\n", "\n", "self", ".", "tp", "=", "int", "(", "(", "(", "self", ".", "test", "!=", "0", ")", "*", "(", "self", ".", "reference", "!=", "0", ")", ")", ".", "sum", "(", ")", ")", "\n", "self", ".", "fp", "=", "int", "(", "(", "(", "self", ".", "test", "!=", "0", ")", "*", "(", "self", ".", "reference", "==", "0", ")", ")", ".", "sum", "(", ")", ")", "\n", "self", ".", "tn", "=", "int", "(", "(", "(", "self", ".", "test", "==", "0", ")", "*", "(", "self", ".", "reference", "==", "0", ")", ")", ".", "sum", "(", ")", ")", "\n", "self", ".", "fn", "=", "int", "(", "(", "(", "self", ".", "test", "==", "0", ")", "*", "(", "self", ".", "reference", "!=", "0", ")", ")", ".", "sum", "(", ")", ")", "\n", "self", ".", "size", "=", "int", "(", "np", ".", "prod", "(", "self", ".", "reference", ".", "shape", ",", "dtype", "=", "np", ".", "int64", ")", ")", "\n", "self", ".", "test_empty", "=", "not", "np", ".", "any", "(", "self", ".", "test", ")", "\n", "self", ".", "test_full", "=", "np", ".", "all", "(", "self", ".", "test", ")", "\n", "self", ".", "reference_empty", "=", "not", "np", ".", "any", "(", "self", ".", "reference", ")", "\n", "self", ".", "reference_full", "=", "np", ".", "all", "(", "self", ".", "reference", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.evaluation.metrics.ConfusionMatrix.get_matrix": [[80, 88], ["metrics.ConfusionMatrix.compute"], "methods", ["home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.evaluation.metrics.ConfusionMatrix.compute"], ["", "def", "get_matrix", "(", "self", ")", ":", "\n", "\n", "        ", "for", "entry", "in", "(", "self", ".", "tp", ",", "self", ".", "fp", ",", "self", ".", "tn", ",", "self", ".", "fn", ")", ":", "\n", "            ", "if", "entry", "is", "None", ":", "\n", "                ", "self", ".", "compute", "(", ")", "\n", "break", "\n", "\n", "", "", "return", "self", ".", "tp", ",", "self", ".", "fp", ",", "self", ".", "tn", ",", "self", ".", "fn", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.evaluation.metrics.ConfusionMatrix.get_size": [[89, 94], ["metrics.ConfusionMatrix.compute"], "methods", ["home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.evaluation.metrics.ConfusionMatrix.compute"], ["", "def", "get_size", "(", "self", ")", ":", "\n", "\n", "        ", "if", "self", ".", "size", "is", "None", ":", "\n", "            ", "self", ".", "compute", "(", ")", "\n", "", "return", "self", ".", "size", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.evaluation.metrics.ConfusionMatrix.get_existence": [[95, 103], ["metrics.ConfusionMatrix.compute"], "methods", ["home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.evaluation.metrics.ConfusionMatrix.compute"], ["", "def", "get_existence", "(", "self", ")", ":", "\n", "\n", "        ", "for", "case", "in", "(", "self", ".", "test_empty", ",", "self", ".", "test_full", ",", "self", ".", "reference_empty", ",", "self", ".", "reference_full", ")", ":", "\n", "            ", "if", "case", "is", "None", ":", "\n", "                ", "self", ".", "compute", "(", ")", "\n", "break", "\n", "\n", "", "", "return", "self", ".", "test_empty", ",", "self", ".", "test_full", ",", "self", ".", "reference_empty", ",", "self", ".", "reference_full", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.evaluation.metrics.assert_shape": [[19, 23], ["None"], "function", ["None"], ["def", "assert_shape", "(", "test", ",", "reference", ")", ":", "\n", "\n", "    ", "assert", "test", ".", "shape", "==", "reference", ".", "shape", ",", "\"Shape mismatch: {} and {}\"", ".", "format", "(", "\n", "test", ".", "shape", ",", "reference", ".", "shape", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.evaluation.metrics.dice": [[105, 121], ["metrics.ConfusionMatrix.get_matrix", "metrics.ConfusionMatrix.get_existence", "float", "metrics.ConfusionMatrix", "float"], "function", ["home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.evaluation.metrics.ConfusionMatrix.get_matrix", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.evaluation.metrics.ConfusionMatrix.get_existence"], ["", "", "def", "dice", "(", "test", "=", "None", ",", "reference", "=", "None", ",", "confusion_matrix", "=", "None", ",", "nan_for_nonexisting", "=", "True", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\"2TP / (2TP + FP + FN)\"\"\"", "\n", "\n", "if", "confusion_matrix", "is", "None", ":", "\n", "        ", "confusion_matrix", "=", "ConfusionMatrix", "(", "test", ",", "reference", ")", "\n", "\n", "", "tp", ",", "fp", ",", "tn", ",", "fn", "=", "confusion_matrix", ".", "get_matrix", "(", ")", "\n", "test_empty", ",", "test_full", ",", "reference_empty", ",", "reference_full", "=", "confusion_matrix", ".", "get_existence", "(", ")", "\n", "\n", "if", "test_empty", "and", "reference_empty", ":", "\n", "        ", "if", "nan_for_nonexisting", ":", "\n", "            ", "return", "float", "(", "\"NaN\"", ")", "\n", "", "else", ":", "\n", "            ", "return", "0.", "\n", "\n", "", "", "return", "float", "(", "2.", "*", "tp", "/", "(", "2", "*", "tp", "+", "fp", "+", "fn", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.evaluation.metrics.jaccard": [[123, 139], ["metrics.ConfusionMatrix.get_matrix", "metrics.ConfusionMatrix.get_existence", "float", "metrics.ConfusionMatrix", "float"], "function", ["home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.evaluation.metrics.ConfusionMatrix.get_matrix", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.evaluation.metrics.ConfusionMatrix.get_existence"], ["", "def", "jaccard", "(", "test", "=", "None", ",", "reference", "=", "None", ",", "confusion_matrix", "=", "None", ",", "nan_for_nonexisting", "=", "True", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\"TP / (TP + FP + FN)\"\"\"", "\n", "\n", "if", "confusion_matrix", "is", "None", ":", "\n", "        ", "confusion_matrix", "=", "ConfusionMatrix", "(", "test", ",", "reference", ")", "\n", "\n", "", "tp", ",", "fp", ",", "tn", ",", "fn", "=", "confusion_matrix", ".", "get_matrix", "(", ")", "\n", "test_empty", ",", "test_full", ",", "reference_empty", ",", "reference_full", "=", "confusion_matrix", ".", "get_existence", "(", ")", "\n", "\n", "if", "test_empty", "and", "reference_empty", ":", "\n", "        ", "if", "nan_for_nonexisting", ":", "\n", "            ", "return", "float", "(", "\"NaN\"", ")", "\n", "", "else", ":", "\n", "            ", "return", "0.", "\n", "\n", "", "", "return", "float", "(", "tp", "/", "(", "tp", "+", "fp", "+", "fn", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.evaluation.metrics.precision": [[141, 157], ["metrics.ConfusionMatrix.get_matrix", "metrics.ConfusionMatrix.get_existence", "float", "metrics.ConfusionMatrix", "float"], "function", ["home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.evaluation.metrics.ConfusionMatrix.get_matrix", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.evaluation.metrics.ConfusionMatrix.get_existence"], ["", "def", "precision", "(", "test", "=", "None", ",", "reference", "=", "None", ",", "confusion_matrix", "=", "None", ",", "nan_for_nonexisting", "=", "True", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\"TP / (TP + FP)\"\"\"", "\n", "\n", "if", "confusion_matrix", "is", "None", ":", "\n", "        ", "confusion_matrix", "=", "ConfusionMatrix", "(", "test", ",", "reference", ")", "\n", "\n", "", "tp", ",", "fp", ",", "tn", ",", "fn", "=", "confusion_matrix", ".", "get_matrix", "(", ")", "\n", "test_empty", ",", "test_full", ",", "reference_empty", ",", "reference_full", "=", "confusion_matrix", ".", "get_existence", "(", ")", "\n", "\n", "if", "test_empty", ":", "\n", "        ", "if", "nan_for_nonexisting", ":", "\n", "            ", "return", "float", "(", "\"NaN\"", ")", "\n", "", "else", ":", "\n", "            ", "return", "0.", "\n", "\n", "", "", "return", "float", "(", "tp", "/", "(", "tp", "+", "fp", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.evaluation.metrics.sensitivity": [[159, 175], ["metrics.ConfusionMatrix.get_matrix", "metrics.ConfusionMatrix.get_existence", "float", "metrics.ConfusionMatrix", "float"], "function", ["home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.evaluation.metrics.ConfusionMatrix.get_matrix", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.evaluation.metrics.ConfusionMatrix.get_existence"], ["", "def", "sensitivity", "(", "test", "=", "None", ",", "reference", "=", "None", ",", "confusion_matrix", "=", "None", ",", "nan_for_nonexisting", "=", "True", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\"TP / (TP + FN)\"\"\"", "\n", "\n", "if", "confusion_matrix", "is", "None", ":", "\n", "        ", "confusion_matrix", "=", "ConfusionMatrix", "(", "test", ",", "reference", ")", "\n", "\n", "", "tp", ",", "fp", ",", "tn", ",", "fn", "=", "confusion_matrix", ".", "get_matrix", "(", ")", "\n", "test_empty", ",", "test_full", ",", "reference_empty", ",", "reference_full", "=", "confusion_matrix", ".", "get_existence", "(", ")", "\n", "\n", "if", "reference_empty", ":", "\n", "        ", "if", "nan_for_nonexisting", ":", "\n", "            ", "return", "float", "(", "\"NaN\"", ")", "\n", "", "else", ":", "\n", "            ", "return", "0.", "\n", "\n", "", "", "return", "float", "(", "tp", "/", "(", "tp", "+", "fn", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.evaluation.metrics.recall": [[177, 181], ["metrics.sensitivity"], "function", ["home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.evaluation.metrics.sensitivity"], ["", "def", "recall", "(", "test", "=", "None", ",", "reference", "=", "None", ",", "confusion_matrix", "=", "None", ",", "nan_for_nonexisting", "=", "True", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\"TP / (TP + FN)\"\"\"", "\n", "\n", "return", "sensitivity", "(", "test", ",", "reference", ",", "confusion_matrix", ",", "nan_for_nonexisting", ",", "**", "kwargs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.evaluation.metrics.specificity": [[183, 199], ["metrics.ConfusionMatrix.get_matrix", "metrics.ConfusionMatrix.get_existence", "float", "metrics.ConfusionMatrix", "float"], "function", ["home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.evaluation.metrics.ConfusionMatrix.get_matrix", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.evaluation.metrics.ConfusionMatrix.get_existence"], ["", "def", "specificity", "(", "test", "=", "None", ",", "reference", "=", "None", ",", "confusion_matrix", "=", "None", ",", "nan_for_nonexisting", "=", "True", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\"TN / (TN + FP)\"\"\"", "\n", "\n", "if", "confusion_matrix", "is", "None", ":", "\n", "        ", "confusion_matrix", "=", "ConfusionMatrix", "(", "test", ",", "reference", ")", "\n", "\n", "", "tp", ",", "fp", ",", "tn", ",", "fn", "=", "confusion_matrix", ".", "get_matrix", "(", ")", "\n", "test_empty", ",", "test_full", ",", "reference_empty", ",", "reference_full", "=", "confusion_matrix", ".", "get_existence", "(", ")", "\n", "\n", "if", "reference_full", ":", "\n", "        ", "if", "nan_for_nonexisting", ":", "\n", "            ", "return", "float", "(", "\"NaN\"", ")", "\n", "", "else", ":", "\n", "            ", "return", "0.", "\n", "\n", "", "", "return", "float", "(", "tn", "/", "(", "tn", "+", "fp", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.evaluation.metrics.accuracy": [[201, 210], ["metrics.ConfusionMatrix.get_matrix", "float", "metrics.ConfusionMatrix"], "function", ["home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.evaluation.metrics.ConfusionMatrix.get_matrix"], ["", "def", "accuracy", "(", "test", "=", "None", ",", "reference", "=", "None", ",", "confusion_matrix", "=", "None", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\"(TP + TN) / (TP + FP + FN + TN)\"\"\"", "\n", "\n", "if", "confusion_matrix", "is", "None", ":", "\n", "        ", "confusion_matrix", "=", "ConfusionMatrix", "(", "test", ",", "reference", ")", "\n", "\n", "", "tp", ",", "fp", ",", "tn", ",", "fn", "=", "confusion_matrix", ".", "get_matrix", "(", ")", "\n", "\n", "return", "float", "(", "(", "tp", "+", "tn", ")", "/", "(", "tp", "+", "fp", "+", "tn", "+", "fn", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.evaluation.metrics.fscore": [[212, 220], ["metrics.precision", "metrics.recall"], "function", ["home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.evaluation.metrics.precision", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.evaluation.metrics.recall"], ["", "def", "fscore", "(", "test", "=", "None", ",", "reference", "=", "None", ",", "confusion_matrix", "=", "None", ",", "nan_for_nonexisting", "=", "True", ",", "beta", "=", "1.", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\"(1 + b^2) * TP / ((1 + b^2) * TP + b^2 * FN + FP)\"\"\"", "\n", "\n", "precision_", "=", "precision", "(", "test", ",", "reference", ",", "confusion_matrix", ",", "nan_for_nonexisting", ")", "\n", "recall_", "=", "recall", "(", "test", ",", "reference", ",", "confusion_matrix", ",", "nan_for_nonexisting", ")", "\n", "\n", "return", "(", "1", "+", "beta", "*", "beta", ")", "*", "precision_", "*", "recall_", "/", "(", "(", "beta", "*", "beta", "*", "precision_", ")", "+", "recall_", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.evaluation.metrics.false_positive_rate": [[222, 226], ["metrics.specificity"], "function", ["home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.evaluation.metrics.specificity"], ["", "def", "false_positive_rate", "(", "test", "=", "None", ",", "reference", "=", "None", ",", "confusion_matrix", "=", "None", ",", "nan_for_nonexisting", "=", "True", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\"FP / (FP + TN)\"\"\"", "\n", "\n", "return", "1", "-", "specificity", "(", "test", ",", "reference", ",", "confusion_matrix", ",", "nan_for_nonexisting", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.evaluation.metrics.false_omission_rate": [[228, 244], ["metrics.ConfusionMatrix.get_matrix", "metrics.ConfusionMatrix.get_existence", "float", "metrics.ConfusionMatrix", "float"], "function", ["home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.evaluation.metrics.ConfusionMatrix.get_matrix", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.evaluation.metrics.ConfusionMatrix.get_existence"], ["", "def", "false_omission_rate", "(", "test", "=", "None", ",", "reference", "=", "None", ",", "confusion_matrix", "=", "None", ",", "nan_for_nonexisting", "=", "True", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\"FN / (TN + FN)\"\"\"", "\n", "\n", "if", "confusion_matrix", "is", "None", ":", "\n", "        ", "confusion_matrix", "=", "ConfusionMatrix", "(", "test", ",", "reference", ")", "\n", "\n", "", "tp", ",", "fp", ",", "tn", ",", "fn", "=", "confusion_matrix", ".", "get_matrix", "(", ")", "\n", "test_empty", ",", "test_full", ",", "reference_empty", ",", "reference_full", "=", "confusion_matrix", ".", "get_existence", "(", ")", "\n", "\n", "if", "test_full", ":", "\n", "        ", "if", "nan_for_nonexisting", ":", "\n", "            ", "return", "float", "(", "\"NaN\"", ")", "\n", "", "else", ":", "\n", "            ", "return", "0.", "\n", "\n", "", "", "return", "float", "(", "fn", "/", "(", "fn", "+", "tn", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.evaluation.metrics.false_negative_rate": [[246, 250], ["metrics.sensitivity"], "function", ["home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.evaluation.metrics.sensitivity"], ["", "def", "false_negative_rate", "(", "test", "=", "None", ",", "reference", "=", "None", ",", "confusion_matrix", "=", "None", ",", "nan_for_nonexisting", "=", "True", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\"FN / (TP + FN)\"\"\"", "\n", "\n", "return", "1", "-", "sensitivity", "(", "test", ",", "reference", ",", "confusion_matrix", ",", "nan_for_nonexisting", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.evaluation.metrics.true_negative_rate": [[252, 256], ["metrics.specificity"], "function", ["home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.evaluation.metrics.specificity"], ["", "def", "true_negative_rate", "(", "test", "=", "None", ",", "reference", "=", "None", ",", "confusion_matrix", "=", "None", ",", "nan_for_nonexisting", "=", "True", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\"TN / (TN + FP)\"\"\"", "\n", "\n", "return", "specificity", "(", "test", ",", "reference", ",", "confusion_matrix", ",", "nan_for_nonexisting", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.evaluation.metrics.false_discovery_rate": [[258, 262], ["metrics.precision"], "function", ["home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.evaluation.metrics.precision"], ["", "def", "false_discovery_rate", "(", "test", "=", "None", ",", "reference", "=", "None", ",", "confusion_matrix", "=", "None", ",", "nan_for_nonexisting", "=", "True", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\"FP / (TP + FP)\"\"\"", "\n", "\n", "return", "1", "-", "precision", "(", "test", ",", "reference", ",", "confusion_matrix", ",", "nan_for_nonexisting", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.evaluation.metrics.negative_predictive_value": [[264, 268], ["metrics.false_omission_rate"], "function", ["home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.evaluation.metrics.false_omission_rate"], ["", "def", "negative_predictive_value", "(", "test", "=", "None", ",", "reference", "=", "None", ",", "confusion_matrix", "=", "None", ",", "nan_for_nonexisting", "=", "True", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\"TN / (TN + FN)\"\"\"", "\n", "\n", "return", "1", "-", "false_omission_rate", "(", "test", ",", "reference", ",", "confusion_matrix", ",", "nan_for_nonexisting", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.evaluation.metrics.total_positives_test": [[270, 279], ["metrics.ConfusionMatrix.get_matrix", "metrics.ConfusionMatrix"], "function", ["home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.evaluation.metrics.ConfusionMatrix.get_matrix"], ["", "def", "total_positives_test", "(", "test", "=", "None", ",", "reference", "=", "None", ",", "confusion_matrix", "=", "None", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\"TP + FP\"\"\"", "\n", "\n", "if", "confusion_matrix", "is", "None", ":", "\n", "        ", "confusion_matrix", "=", "ConfusionMatrix", "(", "test", ",", "reference", ")", "\n", "\n", "", "tp", ",", "fp", ",", "tn", ",", "fn", "=", "confusion_matrix", ".", "get_matrix", "(", ")", "\n", "\n", "return", "tp", "+", "fp", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.evaluation.metrics.total_negatives_test": [[281, 290], ["metrics.ConfusionMatrix.get_matrix", "metrics.ConfusionMatrix"], "function", ["home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.evaluation.metrics.ConfusionMatrix.get_matrix"], ["", "def", "total_negatives_test", "(", "test", "=", "None", ",", "reference", "=", "None", ",", "confusion_matrix", "=", "None", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\"TN + FN\"\"\"", "\n", "\n", "if", "confusion_matrix", "is", "None", ":", "\n", "        ", "confusion_matrix", "=", "ConfusionMatrix", "(", "test", ",", "reference", ")", "\n", "\n", "", "tp", ",", "fp", ",", "tn", ",", "fn", "=", "confusion_matrix", ".", "get_matrix", "(", ")", "\n", "\n", "return", "tn", "+", "fn", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.evaluation.metrics.total_positives_reference": [[292, 301], ["metrics.ConfusionMatrix.get_matrix", "metrics.ConfusionMatrix"], "function", ["home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.evaluation.metrics.ConfusionMatrix.get_matrix"], ["", "def", "total_positives_reference", "(", "test", "=", "None", ",", "reference", "=", "None", ",", "confusion_matrix", "=", "None", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\"TP + FN\"\"\"", "\n", "\n", "if", "confusion_matrix", "is", "None", ":", "\n", "        ", "confusion_matrix", "=", "ConfusionMatrix", "(", "test", ",", "reference", ")", "\n", "\n", "", "tp", ",", "fp", ",", "tn", ",", "fn", "=", "confusion_matrix", ".", "get_matrix", "(", ")", "\n", "\n", "return", "tp", "+", "fn", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.evaluation.metrics.total_negatives_reference": [[303, 312], ["metrics.ConfusionMatrix.get_matrix", "metrics.ConfusionMatrix"], "function", ["home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.evaluation.metrics.ConfusionMatrix.get_matrix"], ["", "def", "total_negatives_reference", "(", "test", "=", "None", ",", "reference", "=", "None", ",", "confusion_matrix", "=", "None", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\"TN + FP\"\"\"", "\n", "\n", "if", "confusion_matrix", "is", "None", ":", "\n", "        ", "confusion_matrix", "=", "ConfusionMatrix", "(", "test", ",", "reference", ")", "\n", "\n", "", "tp", ",", "fp", ",", "tn", ",", "fn", "=", "confusion_matrix", ".", "get_matrix", "(", ")", "\n", "\n", "return", "tn", "+", "fp", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.evaluation.metrics.hausdorff_distance": [[314, 330], ["metrics.ConfusionMatrix.get_existence", "medpy.metric.hd", "metrics.ConfusionMatrix", "float"], "function", ["home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.evaluation.metrics.ConfusionMatrix.get_existence"], ["", "def", "hausdorff_distance", "(", "test", "=", "None", ",", "reference", "=", "None", ",", "confusion_matrix", "=", "None", ",", "nan_for_nonexisting", "=", "True", ",", "voxel_spacing", "=", "None", ",", "connectivity", "=", "1", ",", "**", "kwargs", ")", ":", "\n", "\n", "    ", "if", "confusion_matrix", "is", "None", ":", "\n", "        ", "confusion_matrix", "=", "ConfusionMatrix", "(", "test", ",", "reference", ")", "\n", "\n", "", "test_empty", ",", "test_full", ",", "reference_empty", ",", "reference_full", "=", "confusion_matrix", ".", "get_existence", "(", ")", "\n", "\n", "if", "test_empty", "or", "test_full", "or", "reference_empty", "or", "reference_full", ":", "\n", "        ", "if", "nan_for_nonexisting", ":", "\n", "            ", "return", "float", "(", "\"NaN\"", ")", "\n", "", "else", ":", "\n", "            ", "return", "0", "\n", "\n", "", "", "test", ",", "reference", "=", "confusion_matrix", ".", "test", ",", "confusion_matrix", ".", "reference", "\n", "\n", "return", "metric", ".", "hd", "(", "test", ",", "reference", ",", "voxel_spacing", ",", "connectivity", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.evaluation.metrics.hausdorff_distance_95": [[332, 348], ["metrics.ConfusionMatrix.get_existence", "medpy.metric.hd95", "metrics.ConfusionMatrix", "float"], "function", ["home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.evaluation.metrics.ConfusionMatrix.get_existence"], ["", "def", "hausdorff_distance_95", "(", "test", "=", "None", ",", "reference", "=", "None", ",", "confusion_matrix", "=", "None", ",", "nan_for_nonexisting", "=", "True", ",", "voxel_spacing", "=", "None", ",", "connectivity", "=", "1", ",", "**", "kwargs", ")", ":", "\n", "\n", "    ", "if", "confusion_matrix", "is", "None", ":", "\n", "        ", "confusion_matrix", "=", "ConfusionMatrix", "(", "test", ",", "reference", ")", "\n", "\n", "", "test_empty", ",", "test_full", ",", "reference_empty", ",", "reference_full", "=", "confusion_matrix", ".", "get_existence", "(", ")", "\n", "\n", "if", "test_empty", "or", "test_full", "or", "reference_empty", "or", "reference_full", ":", "\n", "        ", "if", "nan_for_nonexisting", ":", "\n", "            ", "return", "float", "(", "\"NaN\"", ")", "\n", "", "else", ":", "\n", "            ", "return", "0", "\n", "\n", "", "", "test", ",", "reference", "=", "confusion_matrix", ".", "test", ",", "confusion_matrix", ".", "reference", "\n", "\n", "return", "metric", ".", "hd95", "(", "test", ",", "reference", ",", "voxel_spacing", ",", "connectivity", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.evaluation.metrics.avg_surface_distance": [[350, 366], ["metrics.ConfusionMatrix.get_existence", "medpy.metric.asd", "metrics.ConfusionMatrix", "float"], "function", ["home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.evaluation.metrics.ConfusionMatrix.get_existence"], ["", "def", "avg_surface_distance", "(", "test", "=", "None", ",", "reference", "=", "None", ",", "confusion_matrix", "=", "None", ",", "nan_for_nonexisting", "=", "True", ",", "voxel_spacing", "=", "None", ",", "connectivity", "=", "1", ",", "**", "kwargs", ")", ":", "\n", "\n", "    ", "if", "confusion_matrix", "is", "None", ":", "\n", "        ", "confusion_matrix", "=", "ConfusionMatrix", "(", "test", ",", "reference", ")", "\n", "\n", "", "test_empty", ",", "test_full", ",", "reference_empty", ",", "reference_full", "=", "confusion_matrix", ".", "get_existence", "(", ")", "\n", "\n", "if", "test_empty", "or", "test_full", "or", "reference_empty", "or", "reference_full", ":", "\n", "        ", "if", "nan_for_nonexisting", ":", "\n", "            ", "return", "float", "(", "\"NaN\"", ")", "\n", "", "else", ":", "\n", "            ", "return", "0", "\n", "\n", "", "", "test", ",", "reference", "=", "confusion_matrix", ".", "test", ",", "confusion_matrix", ".", "reference", "\n", "\n", "return", "metric", ".", "asd", "(", "test", ",", "reference", ",", "voxel_spacing", ",", "connectivity", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.evaluation.metrics.avg_surface_distance_symmetric": [[368, 384], ["metrics.ConfusionMatrix.get_existence", "medpy.metric.assd", "metrics.ConfusionMatrix", "float"], "function", ["home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.evaluation.metrics.ConfusionMatrix.get_existence"], ["", "def", "avg_surface_distance_symmetric", "(", "test", "=", "None", ",", "reference", "=", "None", ",", "confusion_matrix", "=", "None", ",", "nan_for_nonexisting", "=", "True", ",", "voxel_spacing", "=", "None", ",", "connectivity", "=", "1", ",", "**", "kwargs", ")", ":", "\n", "\n", "    ", "if", "confusion_matrix", "is", "None", ":", "\n", "        ", "confusion_matrix", "=", "ConfusionMatrix", "(", "test", ",", "reference", ")", "\n", "\n", "", "test_empty", ",", "test_full", ",", "reference_empty", ",", "reference_full", "=", "confusion_matrix", ".", "get_existence", "(", ")", "\n", "\n", "if", "test_empty", "or", "test_full", "or", "reference_empty", "or", "reference_full", ":", "\n", "        ", "if", "nan_for_nonexisting", ":", "\n", "            ", "return", "float", "(", "\"NaN\"", ")", "\n", "", "else", ":", "\n", "            ", "return", "0", "\n", "\n", "", "", "test", ",", "reference", "=", "confusion_matrix", ".", "test", ",", "confusion_matrix", ".", "reference", "\n", "\n", "return", "metric", ".", "assd", "(", "test", ",", "reference", ",", "voxel_spacing", ",", "connectivity", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.evaluation.surface_dice.normalized_surface_dice": [[20, 57], ["all", "medpy.metric.binary.__surface_distances", "medpy.metric.binary.__surface_distances", "len", "len", "tuple", "numpy.sum", "numpy.sum", "numpy.sum", "numpy.sum", "str", "str", "zip", "range", "len"], "function", ["None"], ["def", "normalized_surface_dice", "(", "a", ":", "np", ".", "ndarray", ",", "b", ":", "np", ".", "ndarray", ",", "threshold", ":", "float", ",", "spacing", ":", "tuple", "=", "None", ",", "connectivity", "=", "1", ")", ":", "\n", "    ", "\"\"\"\n    This implementation differs from the official surface dice implementation! These two are not comparable!!!!!\n\n    The normalized surface dice is symmetric, so it should not matter whether a or b is the reference image\n\n    This implementation natively supports 2D and 3D images. Whether other dimensions are supported depends on the\n    __surface_distances implementation in medpy\n\n    :param a: image 1, must have the same shape as b\n    :param b: image 2, must have the same shape as a\n    :param threshold: distances below this threshold will be counted as true positives. Threshold is in mm, not voxels!\n    (if spacing = (1, 1(, 1)) then one voxel=1mm so the threshold is effectively in voxels)\n    must be a tuple of len dimension(a)\n    :param spacing: how many mm is one voxel in reality? Can be left at None, we then assume an isotropic spacing of 1mm\n    :param connectivity: see scipy.ndimage.generate_binary_structure for more information. I suggest you leave that\n    one alone\n    :return:\n    \"\"\"", "\n", "assert", "all", "(", "[", "i", "==", "j", "for", "i", ",", "j", "in", "zip", "(", "a", ".", "shape", ",", "b", ".", "shape", ")", "]", ")", ",", "\"a and b must have the same shape. a.shape= %s, \"", "\"b.shape= %s\"", "%", "(", "str", "(", "a", ".", "shape", ")", ",", "str", "(", "b", ".", "shape", ")", ")", "\n", "if", "spacing", "is", "None", ":", "\n", "        ", "spacing", "=", "tuple", "(", "[", "1", "for", "_", "in", "range", "(", "len", "(", "a", ".", "shape", ")", ")", "]", ")", "\n", "", "a_to_b", "=", "__surface_distances", "(", "a", ",", "b", ",", "spacing", ",", "connectivity", ")", "\n", "b_to_a", "=", "__surface_distances", "(", "b", ",", "a", ",", "spacing", ",", "connectivity", ")", "\n", "\n", "numel_a", "=", "len", "(", "a_to_b", ")", "\n", "numel_b", "=", "len", "(", "b_to_a", ")", "\n", "\n", "tp_a", "=", "np", ".", "sum", "(", "a_to_b", "<=", "threshold", ")", "/", "numel_a", "\n", "tp_b", "=", "np", ".", "sum", "(", "b_to_a", "<=", "threshold", ")", "/", "numel_b", "\n", "\n", "fp", "=", "np", ".", "sum", "(", "a_to_b", ">", "threshold", ")", "/", "numel_a", "\n", "fn", "=", "np", ".", "sum", "(", "b_to_a", ">", "threshold", ")", "/", "numel_b", "\n", "\n", "dc", "=", "(", "tp_a", "+", "tp_b", ")", "/", "(", "tp_a", "+", "tp_b", "+", "fp", "+", "fn", "+", "1e-8", ")", "# 1e-8 just so that we don't get div by 0", "\n", "return", "dc", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.evaluation.collect_results_files.crawl_and_copy": [[20, 41], ["batchgenerators.utilities.file_and_folder_operations.subdirs", "batchgenerators.utilities.file_and_folder_operations.subfiles", "current_folder.find", "collect_results_files.crawl_and_copy", "i.endswith", "shutil.copy", "os.path.join", "os.path.join", "os.path.join"], "function", ["home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.evaluation.collect_results_files.crawl_and_copy"], ["def", "crawl_and_copy", "(", "current_folder", ",", "out_folder", ",", "prefix", "=", "\"fabian_\"", ",", "suffix", "=", "\"ummary.json\"", ")", ":", "\n", "    ", "\"\"\"\n    This script will run recursively through all subfolders of current_folder and copy all files that end with\n    suffix with some automatically generated prefix into out_folder\n    :param current_folder:\n    :param out_folder:\n    :param prefix:\n    :return:\n    \"\"\"", "\n", "s", "=", "subdirs", "(", "current_folder", ",", "join", "=", "False", ")", "\n", "f", "=", "subfiles", "(", "current_folder", ",", "join", "=", "False", ")", "\n", "f", "=", "[", "i", "for", "i", "in", "f", "if", "i", ".", "endswith", "(", "suffix", ")", "]", "\n", "if", "current_folder", ".", "find", "(", "\"fold0\"", ")", "!=", "-", "1", ":", "\n", "        ", "for", "fl", "in", "f", ":", "\n", "            ", "shutil", ".", "copy", "(", "os", ".", "path", ".", "join", "(", "current_folder", ",", "fl", ")", ",", "os", ".", "path", ".", "join", "(", "out_folder", ",", "prefix", "+", "fl", ")", ")", "\n", "", "", "for", "su", "in", "s", ":", "\n", "        ", "if", "prefix", "==", "\"\"", ":", "\n", "            ", "add", "=", "su", "\n", "", "else", ":", "\n", "            ", "add", "=", "\"__\"", "+", "su", "\n", "", "crawl_and_copy", "(", "os", ".", "path", ".", "join", "(", "current_folder", ",", "su", ")", ",", "out_folder", ",", "prefix", "=", "prefix", "+", "add", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.evaluation.region_based_evaluation.get_brats_regions": [[12, 24], ["None"], "function", ["None"], ["def", "get_brats_regions", "(", ")", ":", "\n", "    ", "\"\"\"\n    this is only valid for the brats data in here where the labels are 1, 2, and 3. The original brats data have a\n    different labeling convention!\n    :return:\n    \"\"\"", "\n", "regions", "=", "{", "\n", "\"whole tumor\"", ":", "(", "1", ",", "2", ",", "3", ")", ",", "\n", "\"tumor core\"", ":", "(", "2", ",", "3", ")", ",", "\n", "\"enhancing tumor\"", ":", "(", "3", ",", ")", "\n", "}", "\n", "return", "regions", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.evaluation.region_based_evaluation.get_KiTS_regions": [[26, 32], ["None"], "function", ["None"], ["", "def", "get_KiTS_regions", "(", ")", ":", "\n", "    ", "regions", "=", "{", "\n", "\"kidney incl tumor\"", ":", "(", "1", ",", "2", ")", ",", "\n", "\"tumor\"", ":", "(", "2", ",", ")", "\n", "}", "\n", "return", "regions", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.evaluation.region_based_evaluation.create_region_from_mask": [[34, 39], ["numpy.zeros_like"], "function", ["None"], ["", "def", "create_region_from_mask", "(", "mask", ",", "join_labels", ":", "tuple", ")", ":", "\n", "    ", "mask_new", "=", "np", ".", "zeros_like", "(", "mask", ",", "dtype", "=", "np", ".", "uint8", ")", "\n", "for", "l", "in", "join_labels", ":", "\n", "        ", "mask_new", "[", "mask", "==", "l", "]", "=", "1", "\n", "", "return", "mask_new", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.evaluation.region_based_evaluation.evaluate_case": [[41, 51], ["SimpleITK.GetArrayFromImage", "SimpleITK.GetArrayFromImage", "SimpleITK.ReadImage", "SimpleITK.ReadImage", "region_based_evaluation.create_region_from_mask", "region_based_evaluation.create_region_from_mask", "results.append", "medpy.metric.dc", "numpy.sum", "numpy.sum"], "function", ["home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.evaluation.region_based_evaluation.create_region_from_mask", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.evaluation.region_based_evaluation.create_region_from_mask"], ["", "def", "evaluate_case", "(", "file_pred", ":", "str", ",", "file_gt", ":", "str", ",", "regions", ")", ":", "\n", "    ", "image_gt", "=", "sitk", ".", "GetArrayFromImage", "(", "sitk", ".", "ReadImage", "(", "file_gt", ")", ")", "\n", "image_pred", "=", "sitk", ".", "GetArrayFromImage", "(", "sitk", ".", "ReadImage", "(", "file_pred", ")", ")", "\n", "results", "=", "[", "]", "\n", "for", "r", "in", "regions", ":", "\n", "        ", "mask_pred", "=", "create_region_from_mask", "(", "image_pred", ",", "r", ")", "\n", "mask_gt", "=", "create_region_from_mask", "(", "image_gt", ",", "r", ")", "\n", "dc", "=", "np", ".", "nan", "if", "np", ".", "sum", "(", "mask_gt", ")", "==", "0", "and", "np", ".", "sum", "(", "mask_pred", ")", "==", "0", "else", "metric", ".", "dc", "(", "mask_pred", ",", "mask_gt", ")", "\n", "results", ".", "append", "(", "dc", ")", "\n", "", "return", "results", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.evaluation.region_based_evaluation.evaluate_regions": [[53, 111], ["list", "subfiles", "subfiles", "subfiles.sort", "subfiles.sort", "multiprocessing.pool.Pool", "multiprocessing.pool.Pool.starmap", "multiprocessing.pool.Pool.close", "multiprocessing.pool.Pool.join", "regions.keys", "len", "len", "print", "join", "join", "zip", "open", "f.write", "f.write", "range", "f.write", "f.write", "f.write", "f.write", "f.write", "f.write", "f.write", "f.write", "join", "f.write", "len", "f.write", "enumerate", "f.write", "f.write", "f.write", "numpy.array", "f.write", "numpy.array", "f.write", "len", "f.write", "all_results[].append", "list", "numpy.nanmean", "numpy.nanmedian", "numpy.isnan", "numpy.mean", "numpy.isnan", "numpy.median", "regions.values"], "function", ["None"], ["", "def", "evaluate_regions", "(", "folder_predicted", ":", "str", ",", "folder_gt", ":", "str", ",", "regions", ":", "dict", ",", "processes", "=", "default_num_threads", ")", ":", "\n", "    ", "region_names", "=", "list", "(", "regions", ".", "keys", "(", ")", ")", "\n", "files_in_pred", "=", "subfiles", "(", "folder_predicted", ",", "suffix", "=", "'.nii.gz'", ",", "join", "=", "False", ")", "\n", "files_in_gt", "=", "subfiles", "(", "folder_gt", ",", "suffix", "=", "'.nii.gz'", ",", "join", "=", "False", ")", "\n", "have_no_gt", "=", "[", "i", "for", "i", "in", "files_in_pred", "if", "i", "not", "in", "files_in_gt", "]", "\n", "assert", "len", "(", "have_no_gt", ")", "==", "0", ",", "\"Some files in folder_predicted have not ground truth in folder_gt\"", "\n", "have_no_pred", "=", "[", "i", "for", "i", "in", "files_in_gt", "if", "i", "not", "in", "files_in_pred", "]", "\n", "if", "len", "(", "have_no_pred", ")", ">", "0", ":", "\n", "        ", "print", "(", "\"WARNING! Some files in folder_gt were not predicted (not present in folder_predicted)!\"", ")", "\n", "\n", "", "files_in_gt", ".", "sort", "(", ")", "\n", "files_in_pred", ".", "sort", "(", ")", "\n", "\n", "# run for all cases", "\n", "full_filenames_gt", "=", "[", "join", "(", "folder_gt", ",", "i", ")", "for", "i", "in", "files_in_pred", "]", "\n", "full_filenames_pred", "=", "[", "join", "(", "folder_predicted", ",", "i", ")", "for", "i", "in", "files_in_pred", "]", "\n", "\n", "p", "=", "Pool", "(", "processes", ")", "\n", "res", "=", "p", ".", "starmap", "(", "evaluate_case", ",", "zip", "(", "full_filenames_pred", ",", "full_filenames_gt", ",", "[", "list", "(", "regions", ".", "values", "(", ")", ")", "]", "*", "len", "(", "files_in_gt", ")", ")", ")", "\n", "p", ".", "close", "(", ")", "\n", "p", ".", "join", "(", ")", "\n", "\n", "all_results", "=", "{", "r", ":", "[", "]", "for", "r", "in", "region_names", "}", "\n", "with", "open", "(", "join", "(", "folder_predicted", ",", "'summary.csv'", ")", ",", "'w'", ")", "as", "f", ":", "\n", "        ", "f", ".", "write", "(", "\"casename\"", ")", "\n", "for", "r", "in", "region_names", ":", "\n", "            ", "f", ".", "write", "(", "\",%s\"", "%", "r", ")", "\n", "", "f", ".", "write", "(", "\"\\n\"", ")", "\n", "for", "i", "in", "range", "(", "len", "(", "files_in_pred", ")", ")", ":", "\n", "            ", "f", ".", "write", "(", "files_in_pred", "[", "i", "]", "[", ":", "-", "7", "]", ")", "\n", "result_here", "=", "res", "[", "i", "]", "\n", "for", "k", ",", "r", "in", "enumerate", "(", "region_names", ")", ":", "\n", "                ", "dc", "=", "result_here", "[", "k", "]", "\n", "f", ".", "write", "(", "\",%02.4f\"", "%", "dc", ")", "\n", "all_results", "[", "r", "]", ".", "append", "(", "dc", ")", "\n", "", "f", ".", "write", "(", "\"\\n\"", ")", "\n", "\n", "", "f", ".", "write", "(", "'mean'", ")", "\n", "for", "r", "in", "region_names", ":", "\n", "            ", "f", ".", "write", "(", "\",%02.4f\"", "%", "np", ".", "nanmean", "(", "all_results", "[", "r", "]", ")", ")", "\n", "", "f", ".", "write", "(", "\"\\n\"", ")", "\n", "f", ".", "write", "(", "'median'", ")", "\n", "for", "r", "in", "region_names", ":", "\n", "            ", "f", ".", "write", "(", "\",%02.4f\"", "%", "np", ".", "nanmedian", "(", "all_results", "[", "r", "]", ")", ")", "\n", "", "f", ".", "write", "(", "\"\\n\"", ")", "\n", "\n", "f", ".", "write", "(", "'mean (nan is 1)'", ")", "\n", "for", "r", "in", "region_names", ":", "\n", "            ", "tmp", "=", "np", ".", "array", "(", "all_results", "[", "r", "]", ")", "\n", "tmp", "[", "np", ".", "isnan", "(", "tmp", ")", "]", "=", "1", "\n", "f", ".", "write", "(", "\",%02.4f\"", "%", "np", ".", "mean", "(", "tmp", ")", ")", "\n", "", "f", ".", "write", "(", "\"\\n\"", ")", "\n", "f", ".", "write", "(", "'median (nan is 1)'", ")", "\n", "for", "r", "in", "region_names", ":", "\n", "            ", "tmp", "=", "np", ".", "array", "(", "all_results", "[", "r", "]", ")", "\n", "tmp", "[", "np", ".", "isnan", "(", "tmp", ")", "]", "=", "1", "\n", "f", ".", "write", "(", "\",%02.4f\"", "%", "np", ".", "median", "(", "tmp", ")", ")", "\n", "", "f", ".", "write", "(", "\"\\n\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.evaluation.add_mean_dice_to_json.foreground_mean": [[21, 40], ["numpy.array", "[].get", "[].keys", "collections.OrderedDict", "open", "json.load", "[].pop", "numpy.nanmean", "open", "json.dump", "int", "[].keys", "str"], "function", ["None"], ["def", "foreground_mean", "(", "filename", ")", ":", "\n", "    ", "with", "open", "(", "filename", ",", "'r'", ")", "as", "f", ":", "\n", "        ", "res", "=", "json", ".", "load", "(", "f", ")", "\n", "", "class_ids", "=", "np", ".", "array", "(", "[", "int", "(", "i", ")", "for", "i", "in", "res", "[", "'results'", "]", "[", "'mean'", "]", ".", "keys", "(", ")", "if", "(", "i", "!=", "'mean'", ")", "]", ")", "\n", "class_ids", "=", "class_ids", "[", "class_ids", "!=", "0", "]", "\n", "class_ids", "=", "class_ids", "[", "class_ids", "!=", "-", "1", "]", "\n", "class_ids", "=", "class_ids", "[", "class_ids", "!=", "99", "]", "\n", "\n", "tmp", "=", "res", "[", "'results'", "]", "[", "'mean'", "]", ".", "get", "(", "'99'", ")", "\n", "if", "tmp", "is", "not", "None", ":", "\n", "        ", "_", "=", "res", "[", "'results'", "]", "[", "'mean'", "]", ".", "pop", "(", "'99'", ")", "\n", "\n", "", "metrics", "=", "res", "[", "'results'", "]", "[", "'mean'", "]", "[", "'1'", "]", ".", "keys", "(", ")", "\n", "res", "[", "'results'", "]", "[", "'mean'", "]", "[", "\"mean\"", "]", "=", "OrderedDict", "(", ")", "\n", "for", "m", "in", "metrics", ":", "\n", "        ", "foreground_values", "=", "[", "res", "[", "'results'", "]", "[", "'mean'", "]", "[", "str", "(", "i", ")", "]", "[", "m", "]", "for", "i", "in", "class_ids", "]", "\n", "res", "[", "'results'", "]", "[", "'mean'", "]", "[", "\"mean\"", "]", "[", "m", "]", "=", "np", ".", "nanmean", "(", "foreground_values", ")", "\n", "", "with", "open", "(", "filename", ",", "'w'", ")", "as", "f", ":", "\n", "        ", "json", ".", "dump", "(", "res", ",", "f", ",", "indent", "=", "4", ",", "sort_keys", "=", "True", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.evaluation.add_mean_dice_to_json.run_in_folder": [[42, 47], ["batchgenerators.utilities.file_and_folder_operations.subfiles", "add_mean_dice_to_json.foreground_mean", "[].startswith", "i.endswith", "i.split"], "function", ["home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.evaluation.add_mean_dice_to_json.foreground_mean"], ["", "", "def", "run_in_folder", "(", "folder", ")", ":", "\n", "    ", "json_files", "=", "subfiles", "(", "folder", ",", "True", ",", "None", ",", "\".json\"", ",", "True", ")", "\n", "json_files", "=", "[", "i", "for", "i", "in", "json_files", "if", "not", "i", ".", "split", "(", "\"/\"", ")", "[", "-", "1", "]", ".", "startswith", "(", "\".\"", ")", "and", "not", "i", ".", "endswith", "(", "\"_globalMean.json\"", ")", "]", "# stupid mac", "\n", "for", "j", "in", "json_files", ":", "\n", "        ", "foreground_mean", "(", "j", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.evaluation.evaluator.Evaluator.__init__": [[60, 98], ["nnunet.evaluation.metrics.ConfusionMatrix", "evaluator.Evaluator.set_reference", "evaluator.Evaluator.set_test", "evaluator.Evaluator.set_labels", "evaluator.Evaluator.metrics.append", "evaluator.Evaluator.metrics.append", "evaluator.Evaluator.advanced_metrics.append", "evaluator.Evaluator.advanced_metrics.append", "evaluator.Evaluator.construct_labels"], "methods", ["home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.evaluation.evaluator.NiftiEvaluator.set_reference", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.evaluation.evaluator.NiftiEvaluator.set_test", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.evaluation.evaluator.Evaluator.set_labels", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.evaluation.evaluator.Evaluator.construct_labels"], ["def", "__init__", "(", "self", ",", "\n", "test", "=", "None", ",", "\n", "reference", "=", "None", ",", "\n", "labels", "=", "None", ",", "\n", "metrics", "=", "None", ",", "\n", "advanced_metrics", "=", "None", ",", "\n", "nan_for_nonexisting", "=", "True", ")", ":", "\n", "\n", "        ", "self", ".", "test", "=", "None", "\n", "self", ".", "reference", "=", "None", "\n", "self", ".", "confusion_matrix", "=", "ConfusionMatrix", "(", ")", "\n", "self", ".", "labels", "=", "None", "\n", "self", ".", "nan_for_nonexisting", "=", "nan_for_nonexisting", "\n", "self", ".", "result", "=", "None", "\n", "\n", "self", ".", "metrics", "=", "[", "]", "\n", "if", "metrics", "is", "None", ":", "\n", "            ", "for", "m", "in", "self", ".", "default_metrics", ":", "\n", "                ", "self", ".", "metrics", ".", "append", "(", "m", ")", "\n", "", "", "else", ":", "\n", "            ", "for", "m", "in", "metrics", ":", "\n", "                ", "self", ".", "metrics", ".", "append", "(", "m", ")", "\n", "\n", "", "", "self", ".", "advanced_metrics", "=", "[", "]", "\n", "if", "advanced_metrics", "is", "None", ":", "\n", "            ", "for", "m", "in", "self", ".", "default_advanced_metrics", ":", "\n", "                ", "self", ".", "advanced_metrics", ".", "append", "(", "m", ")", "\n", "", "", "else", ":", "\n", "            ", "for", "m", "in", "advanced_metrics", ":", "\n", "                ", "self", ".", "advanced_metrics", ".", "append", "(", "m", ")", "\n", "\n", "", "", "self", ".", "set_reference", "(", "reference", ")", "\n", "self", ".", "set_test", "(", "test", ")", "\n", "if", "labels", "is", "not", "None", ":", "\n", "            ", "self", ".", "set_labels", "(", "labels", ")", "\n", "", "else", ":", "\n", "            ", "if", "test", "is", "not", "None", "and", "reference", "is", "not", "None", ":", "\n", "                ", "self", ".", "construct_labels", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.evaluation.evaluator.Evaluator.set_test": [[99, 103], ["None"], "methods", ["None"], ["", "", "", "def", "set_test", "(", "self", ",", "test", ")", ":", "\n", "        ", "\"\"\"Set the test segmentation.\"\"\"", "\n", "\n", "self", ".", "test", "=", "test", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.evaluation.evaluator.Evaluator.set_reference": [[104, 108], ["None"], "methods", ["None"], ["", "def", "set_reference", "(", "self", ",", "reference", ")", ":", "\n", "        ", "\"\"\"Set the reference segmentation.\"\"\"", "\n", "\n", "self", ".", "reference", "=", "reference", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.evaluation.evaluator.Evaluator.set_labels": [[109, 124], ["isinstance", "collections.OrderedDict", "isinstance", "list", "isinstance", "isinstance", "TypeError", "type"], "methods", ["None"], ["", "def", "set_labels", "(", "self", ",", "labels", ")", ":", "\n", "        ", "\"\"\"Set the labels.\n        :param labels= may be a dictionary (int->str), a set (of ints), a tuple (of ints) or a list (of ints). Labels\n        will only have names if you pass a dictionary\"\"\"", "\n", "\n", "if", "isinstance", "(", "labels", ",", "dict", ")", ":", "\n", "            ", "self", ".", "labels", "=", "collections", ".", "OrderedDict", "(", "labels", ")", "\n", "", "elif", "isinstance", "(", "labels", ",", "set", ")", ":", "\n", "            ", "self", ".", "labels", "=", "list", "(", "labels", ")", "\n", "", "elif", "isinstance", "(", "labels", ",", "np", ".", "ndarray", ")", ":", "\n", "            ", "self", ".", "labels", "=", "[", "i", "for", "i", "in", "labels", "]", "\n", "", "elif", "isinstance", "(", "labels", ",", "(", "list", ",", "tuple", ")", ")", ":", "\n", "            ", "self", ".", "labels", "=", "labels", "\n", "", "else", ":", "\n", "            ", "raise", "TypeError", "(", "\"Can only handle dict, list, tuple, set & numpy array, but input is of type {}\"", ".", "format", "(", "type", "(", "labels", ")", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.evaluation.evaluator.Evaluator.construct_labels": [[125, 136], ["list", "ValueError", "map", "numpy.unique", "numpy.union1d", "numpy.unique", "numpy.unique", "int"], "methods", ["None"], ["", "", "def", "construct_labels", "(", "self", ")", ":", "\n", "        ", "\"\"\"Construct label set from unique entries in segmentations.\"\"\"", "\n", "\n", "if", "self", ".", "test", "is", "None", "and", "self", ".", "reference", "is", "None", ":", "\n", "            ", "raise", "ValueError", "(", "\"No test or reference segmentations.\"", ")", "\n", "", "elif", "self", ".", "test", "is", "None", ":", "\n", "            ", "labels", "=", "np", ".", "unique", "(", "self", ".", "reference", ")", "\n", "", "else", ":", "\n", "            ", "labels", "=", "np", ".", "union1d", "(", "np", ".", "unique", "(", "self", ".", "test", ")", ",", "\n", "np", ".", "unique", "(", "self", ".", "reference", ")", ")", "\n", "", "self", ".", "labels", "=", "list", "(", "map", "(", "lambda", "x", ":", "int", "(", "x", ")", ",", "labels", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.evaluation.evaluator.Evaluator.set_metrics": [[137, 146], ["isinstance", "list", "isinstance", "TypeError", "type"], "methods", ["None"], ["", "def", "set_metrics", "(", "self", ",", "metrics", ")", ":", "\n", "        ", "\"\"\"Set evaluation metrics\"\"\"", "\n", "\n", "if", "isinstance", "(", "metrics", ",", "set", ")", ":", "\n", "            ", "self", ".", "metrics", "=", "list", "(", "metrics", ")", "\n", "", "elif", "isinstance", "(", "metrics", ",", "(", "list", ",", "tuple", ",", "np", ".", "ndarray", ")", ")", ":", "\n", "            ", "self", ".", "metrics", "=", "metrics", "\n", "", "else", ":", "\n", "            ", "raise", "TypeError", "(", "\"Can only handle list, tuple, set & numpy array, but input is of type {}\"", ".", "format", "(", "type", "(", "metrics", ")", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.evaluation.evaluator.Evaluator.add_metric": [[147, 151], ["evaluator.Evaluator.metrics.append"], "methods", ["None"], ["", "", "def", "add_metric", "(", "self", ",", "metric", ")", ":", "\n", "\n", "        ", "if", "metric", "not", "in", "self", ".", "metrics", ":", "\n", "            ", "self", ".", "metrics", ".", "append", "(", "metric", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.evaluation.evaluator.Evaluator.evaluate": [[152, 226], ["evaluator.Evaluator.metrics.sort", "inspect.getouterframes", "collections.OrderedDict", "isinstance", "evaluator.Evaluator.set_test", "evaluator.Evaluator.set_reference", "ValueError", "evaluator.Evaluator.construct_labels", "inspect.currentframe", "evaluator.Evaluator.labels.items", "enumerate", "str", "collections.OrderedDict", "str", "collections.OrderedDict", "evaluator.Evaluator.confusion_matrix.set_test", "evaluator.Evaluator.confusion_matrix.set_reference", "NotImplementedError", "hasattr", "evaluator.Evaluator.confusion_matrix.set_test", "evaluator.Evaluator.confusion_matrix.set_reference", "evaluator.Evaluator.confusion_matrix.set_test", "evaluator.Evaluator.confusion_matrix.set_reference"], "methods", ["home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.evaluation.evaluator.NiftiEvaluator.set_test", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.evaluation.evaluator.NiftiEvaluator.set_reference", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.evaluation.evaluator.Evaluator.construct_labels", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.evaluation.evaluator.NiftiEvaluator.set_test", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.evaluation.evaluator.NiftiEvaluator.set_reference", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.evaluation.evaluator.NiftiEvaluator.set_test", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.evaluation.evaluator.NiftiEvaluator.set_reference", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.evaluation.evaluator.NiftiEvaluator.set_test", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.evaluation.evaluator.NiftiEvaluator.set_reference"], ["", "", "def", "evaluate", "(", "self", ",", "test", "=", "None", ",", "reference", "=", "None", ",", "advanced", "=", "False", ",", "**", "metric_kwargs", ")", ":", "\n", "        ", "\"\"\"Compute metrics for segmentations.\"\"\"", "\n", "if", "test", "is", "not", "None", ":", "\n", "            ", "self", ".", "set_test", "(", "test", ")", "\n", "\n", "", "if", "reference", "is", "not", "None", ":", "\n", "            ", "self", ".", "set_reference", "(", "reference", ")", "\n", "\n", "", "if", "self", ".", "test", "is", "None", "or", "self", ".", "reference", "is", "None", ":", "\n", "            ", "raise", "ValueError", "(", "\"Need both test and reference segmentations.\"", ")", "\n", "\n", "", "if", "self", ".", "labels", "is", "None", ":", "\n", "            ", "self", ".", "construct_labels", "(", ")", "\n", "\n", "", "self", ".", "metrics", ".", "sort", "(", ")", "\n", "\n", "# get functions for evaluation", "\n", "# somewhat convoluted, but allows users to define additonal metrics", "\n", "# on the fly, e.g. inside an IPython console", "\n", "_funcs", "=", "{", "m", ":", "ALL_METRICS", "[", "m", "]", "for", "m", "in", "self", ".", "metrics", "+", "self", ".", "advanced_metrics", "}", "\n", "frames", "=", "inspect", ".", "getouterframes", "(", "inspect", ".", "currentframe", "(", ")", ")", "\n", "for", "metric", "in", "self", ".", "metrics", ":", "\n", "            ", "for", "f", "in", "frames", ":", "\n", "                ", "if", "metric", "in", "f", "[", "0", "]", ".", "f_locals", ":", "\n", "                    ", "_funcs", "[", "metric", "]", "=", "f", "[", "0", "]", ".", "f_locals", "[", "metric", "]", "\n", "break", "\n", "", "", "else", ":", "\n", "                ", "if", "metric", "in", "_funcs", ":", "\n", "                    ", "continue", "\n", "", "else", ":", "\n", "                    ", "raise", "NotImplementedError", "(", "\n", "\"Metric {} not implemented.\"", ".", "format", "(", "metric", ")", ")", "\n", "\n", "# get results", "\n", "", "", "", "self", ".", "result", "=", "OrderedDict", "(", ")", "\n", "\n", "eval_metrics", "=", "self", ".", "metrics", "\n", "if", "advanced", ":", "\n", "            ", "eval_metrics", "+=", "self", ".", "advanced_metrics", "\n", "\n", "", "if", "isinstance", "(", "self", ".", "labels", ",", "dict", ")", ":", "\n", "\n", "            ", "for", "label", ",", "name", "in", "self", ".", "labels", ".", "items", "(", ")", ":", "\n", "                ", "k", "=", "str", "(", "name", ")", "\n", "self", ".", "result", "[", "k", "]", "=", "OrderedDict", "(", ")", "\n", "if", "not", "hasattr", "(", "label", ",", "\"__iter__\"", ")", ":", "\n", "                    ", "self", ".", "confusion_matrix", ".", "set_test", "(", "self", ".", "test", "==", "label", ")", "\n", "self", ".", "confusion_matrix", ".", "set_reference", "(", "self", ".", "reference", "==", "label", ")", "\n", "", "else", ":", "\n", "                    ", "current_test", "=", "0", "\n", "current_reference", "=", "0", "\n", "for", "l", "in", "label", ":", "\n", "                        ", "current_test", "+=", "(", "self", ".", "test", "==", "l", ")", "\n", "current_reference", "+=", "(", "self", ".", "reference", "==", "l", ")", "\n", "", "self", ".", "confusion_matrix", ".", "set_test", "(", "current_test", ")", "\n", "self", ".", "confusion_matrix", ".", "set_reference", "(", "current_reference", ")", "\n", "", "for", "metric", "in", "eval_metrics", ":", "\n", "                    ", "self", ".", "result", "[", "k", "]", "[", "metric", "]", "=", "_funcs", "[", "metric", "]", "(", "confusion_matrix", "=", "self", ".", "confusion_matrix", ",", "\n", "nan_for_nonexisting", "=", "self", ".", "nan_for_nonexisting", ",", "\n", "**", "metric_kwargs", ")", "\n", "\n", "", "", "", "else", ":", "\n", "\n", "            ", "for", "i", ",", "l", "in", "enumerate", "(", "self", ".", "labels", ")", ":", "\n", "                ", "k", "=", "str", "(", "l", ")", "\n", "self", ".", "result", "[", "k", "]", "=", "OrderedDict", "(", ")", "\n", "self", ".", "confusion_matrix", ".", "set_test", "(", "self", ".", "test", "==", "l", ")", "\n", "self", ".", "confusion_matrix", ".", "set_reference", "(", "self", ".", "reference", "==", "l", ")", "\n", "for", "metric", "in", "eval_metrics", ":", "\n", "                    ", "self", ".", "result", "[", "k", "]", "[", "metric", "]", "=", "_funcs", "[", "metric", "]", "(", "confusion_matrix", "=", "self", ".", "confusion_matrix", ",", "\n", "nan_for_nonexisting", "=", "self", ".", "nan_for_nonexisting", ",", "\n", "**", "metric_kwargs", ")", "\n", "\n", "", "", "", "return", "self", ".", "result", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.evaluation.evaluator.Evaluator.to_dict": [[227, 232], ["evaluator.Evaluator.evaluate"], "methods", ["home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.evaluation.evaluator.NiftiEvaluator.evaluate"], ["", "def", "to_dict", "(", "self", ")", ":", "\n", "\n", "        ", "if", "self", ".", "result", "is", "None", ":", "\n", "            ", "self", ".", "evaluate", "(", ")", "\n", "", "return", "self", ".", "result", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.evaluation.evaluator.Evaluator.to_array": [[233, 253], ["sorted", "numpy.zeros", "isinstance", "evaluator.Evaluator.result[].keys", "enumerate", "enumerate", "len", "len", "evaluator.Evaluator.labels.keys", "enumerate", "enumerate", "list", "evaluator.Evaluator.result.keys"], "methods", ["None"], ["", "def", "to_array", "(", "self", ")", ":", "\n", "        ", "\"\"\"Return result as numpy array (labels x metrics).\"\"\"", "\n", "\n", "if", "self", ".", "result", "is", "None", ":", "\n", "            ", "self", ".", "evaluate", "\n", "\n", "", "result_metrics", "=", "sorted", "(", "self", ".", "result", "[", "list", "(", "self", ".", "result", ".", "keys", "(", ")", ")", "[", "0", "]", "]", ".", "keys", "(", ")", ")", "\n", "\n", "a", "=", "np", ".", "zeros", "(", "(", "len", "(", "self", ".", "labels", ")", ",", "len", "(", "result_metrics", ")", ")", ",", "dtype", "=", "np", ".", "float32", ")", "\n", "\n", "if", "isinstance", "(", "self", ".", "labels", ",", "dict", ")", ":", "\n", "            ", "for", "i", ",", "label", "in", "enumerate", "(", "self", ".", "labels", ".", "keys", "(", ")", ")", ":", "\n", "                ", "for", "j", ",", "metric", "in", "enumerate", "(", "result_metrics", ")", ":", "\n", "                    ", "a", "[", "i", "]", "[", "j", "]", "=", "self", ".", "result", "[", "self", ".", "labels", "[", "label", "]", "]", "[", "metric", "]", "\n", "", "", "", "else", ":", "\n", "            ", "for", "i", ",", "label", "in", "enumerate", "(", "self", ".", "labels", ")", ":", "\n", "                ", "for", "j", ",", "metric", "in", "enumerate", "(", "result_metrics", ")", ":", "\n", "                    ", "a", "[", "i", "]", "[", "j", "]", "=", "self", ".", "result", "[", "label", "]", "[", "metric", "]", "\n", "\n", "", "", "", "return", "a", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.evaluation.evaluator.Evaluator.to_pandas": [[254, 267], ["evaluator.Evaluator.to_array", "isinstance", "sorted", "pandas.DataFrame", "list", "evaluator.Evaluator.result[].keys", "evaluator.Evaluator.labels.values", "list", "evaluator.Evaluator.result.keys"], "methods", ["home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.evaluation.evaluator.Evaluator.to_array"], ["", "def", "to_pandas", "(", "self", ")", ":", "\n", "        ", "\"\"\"Return result as pandas DataFrame.\"\"\"", "\n", "\n", "a", "=", "self", ".", "to_array", "(", ")", "\n", "\n", "if", "isinstance", "(", "self", ".", "labels", ",", "dict", ")", ":", "\n", "            ", "labels", "=", "list", "(", "self", ".", "labels", ".", "values", "(", ")", ")", "\n", "", "else", ":", "\n", "            ", "labels", "=", "self", ".", "labels", "\n", "\n", "", "result_metrics", "=", "sorted", "(", "self", ".", "result", "[", "list", "(", "self", ".", "result", ".", "keys", "(", ")", ")", "[", "0", "]", "]", ".", "keys", "(", ")", ")", "\n", "\n", "return", "pd", ".", "DataFrame", "(", "a", ",", "index", "=", "labels", ",", "columns", "=", "result_metrics", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.evaluation.evaluator.NiftiEvaluator.__init__": [[271, 276], ["evaluator.Evaluator.__init__"], "methods", ["home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.common.layers.ResizeImage.__init__"], ["    ", "def", "__init__", "(", "self", ",", "*", "args", ",", "**", "kwargs", ")", ":", "\n", "\n", "        ", "self", ".", "test_nifti", "=", "None", "\n", "self", ".", "reference_nifti", "=", "None", "\n", "super", "(", "NiftiEvaluator", ",", "self", ")", ".", "__init__", "(", "*", "args", ",", "**", "kwargs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.evaluation.evaluator.NiftiEvaluator.set_test": [[277, 286], ["SimpleITK.ReadImage", "evaluator.Evaluator.set_test", "evaluator.Evaluator.set_test", "SimpleITK.GetArrayFromImage"], "methods", ["home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.evaluation.evaluator.NiftiEvaluator.set_test", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.evaluation.evaluator.NiftiEvaluator.set_test"], ["", "def", "set_test", "(", "self", ",", "test", ")", ":", "\n", "        ", "\"\"\"Set the test segmentation.\"\"\"", "\n", "\n", "if", "test", "is", "not", "None", ":", "\n", "            ", "self", ".", "test_nifti", "=", "sitk", ".", "ReadImage", "(", "test", ")", "\n", "super", "(", "NiftiEvaluator", ",", "self", ")", ".", "set_test", "(", "sitk", ".", "GetArrayFromImage", "(", "self", ".", "test_nifti", ")", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "test_nifti", "=", "None", "\n", "super", "(", "NiftiEvaluator", ",", "self", ")", ".", "set_test", "(", "test", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.evaluation.evaluator.NiftiEvaluator.set_reference": [[287, 296], ["SimpleITK.ReadImage", "evaluator.Evaluator.set_reference", "evaluator.Evaluator.set_reference", "SimpleITK.GetArrayFromImage"], "methods", ["home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.evaluation.evaluator.NiftiEvaluator.set_reference", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.evaluation.evaluator.NiftiEvaluator.set_reference"], ["", "", "def", "set_reference", "(", "self", ",", "reference", ")", ":", "\n", "        ", "\"\"\"Set the reference segmentation.\"\"\"", "\n", "\n", "if", "reference", "is", "not", "None", ":", "\n", "            ", "self", ".", "reference_nifti", "=", "sitk", ".", "ReadImage", "(", "reference", ")", "\n", "super", "(", "NiftiEvaluator", ",", "self", ")", ".", "set_reference", "(", "sitk", ".", "GetArrayFromImage", "(", "self", ".", "reference_nifti", ")", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "reference_nifti", "=", "None", "\n", "super", "(", "NiftiEvaluator", ",", "self", ")", ".", "set_reference", "(", "reference", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.evaluation.evaluator.NiftiEvaluator.evaluate": [[297, 304], ["evaluator.Evaluator.evaluate", "numpy.array", "evaluator.NiftiEvaluator.test_nifti.GetSpacing"], "methods", ["home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.evaluation.evaluator.NiftiEvaluator.evaluate"], ["", "", "def", "evaluate", "(", "self", ",", "test", "=", "None", ",", "reference", "=", "None", ",", "voxel_spacing", "=", "None", ",", "**", "metric_kwargs", ")", ":", "\n", "\n", "        ", "if", "voxel_spacing", "is", "None", ":", "\n", "            ", "voxel_spacing", "=", "np", ".", "array", "(", "self", ".", "test_nifti", ".", "GetSpacing", "(", ")", ")", "[", ":", ":", "-", "1", "]", "\n", "metric_kwargs", "[", "\"voxel_spacing\"", "]", "=", "voxel_spacing", "\n", "\n", "", "return", "super", "(", "NiftiEvaluator", ",", "self", ")", ".", "evaluate", "(", "test", ",", "reference", ",", "**", "metric_kwargs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.evaluation.evaluator.run_evaluation": [[306, 319], ["evaluator.set_test", "evaluator.set_reference", "evaluator.evaluate", "evaluator.construct_labels", "type", "type"], "function", ["home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.evaluation.evaluator.NiftiEvaluator.set_test", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.evaluation.evaluator.NiftiEvaluator.set_reference", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.evaluation.evaluator.NiftiEvaluator.evaluate", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.evaluation.evaluator.Evaluator.construct_labels"], ["", "", "def", "run_evaluation", "(", "args", ")", ":", "\n", "    ", "test", ",", "ref", ",", "evaluator", ",", "metric_kwargs", "=", "args", "\n", "# evaluate", "\n", "evaluator", ".", "set_test", "(", "test", ")", "\n", "evaluator", ".", "set_reference", "(", "ref", ")", "\n", "if", "evaluator", ".", "labels", "is", "None", ":", "\n", "        ", "evaluator", ".", "construct_labels", "(", ")", "\n", "", "current_scores", "=", "evaluator", ".", "evaluate", "(", "**", "metric_kwargs", ")", "\n", "if", "type", "(", "test", ")", "==", "str", ":", "\n", "        ", "current_scores", "[", "\"test\"", "]", "=", "test", "\n", "", "if", "type", "(", "ref", ")", "==", "str", ":", "\n", "        ", "current_scores", "[", "\"reference\"", "]", "=", "ref", "\n", "", "return", "current_scores", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.evaluation.evaluator.aggregate_scores": [[321, 401], ["collections.OrderedDict", "collections.OrderedDict", "multiprocessing.pool.Pool", "multiprocessing.pool.Pool.map", "multiprocessing.pool.Pool.close", "multiprocessing.pool.Pool.join", "range", "type", "evaluator.", "evaluator.set_labels", "zip", "len", "all_scores[].append", "all_res[].items", "collections.OrderedDict", "datetime.datetime.today", "str", "batchgenerators.utilities.file_and_folder_operations.save_json", "score_dict.items", "hashlib.md5().hexdigest", "len", "len", "collections.OrderedDict", "[].append", "float", "float", "numpy.nanmean", "numpy.mean", "hashlib.md5", "json.dumps().encode", "json.dumps"], "function", ["home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.evaluation.evaluator.Evaluator.set_labels"], ["", "def", "aggregate_scores", "(", "test_ref_pairs", ",", "\n", "evaluator", "=", "NiftiEvaluator", ",", "\n", "labels", "=", "None", ",", "\n", "nanmean", "=", "True", ",", "\n", "json_output_file", "=", "None", ",", "\n", "json_name", "=", "\"\"", ",", "\n", "json_description", "=", "\"\"", ",", "\n", "json_author", "=", "\"Fabian\"", ",", "\n", "json_task", "=", "\"\"", ",", "\n", "num_threads", "=", "2", ",", "\n", "**", "metric_kwargs", ")", ":", "\n", "    ", "\"\"\"\n    test = predicted image\n    :param test_ref_pairs:\n    :param evaluator:\n    :param labels: must be a dict of int-> str or a list of int\n    :param nanmean:\n    :param json_output_file:\n    :param json_name:\n    :param json_description:\n    :param json_author:\n    :param json_task:\n    :param metric_kwargs:\n    :return:\n    \"\"\"", "\n", "\n", "if", "type", "(", "evaluator", ")", "==", "type", ":", "\n", "        ", "evaluator", "=", "evaluator", "(", ")", "\n", "\n", "", "if", "labels", "is", "not", "None", ":", "\n", "        ", "evaluator", ".", "set_labels", "(", "labels", ")", "\n", "\n", "", "all_scores", "=", "OrderedDict", "(", ")", "\n", "all_scores", "[", "\"all\"", "]", "=", "[", "]", "\n", "all_scores", "[", "\"mean\"", "]", "=", "OrderedDict", "(", ")", "\n", "\n", "test", "=", "[", "i", "[", "0", "]", "for", "i", "in", "test_ref_pairs", "]", "\n", "ref", "=", "[", "i", "[", "1", "]", "for", "i", "in", "test_ref_pairs", "]", "\n", "p", "=", "Pool", "(", "num_threads", ")", "\n", "all_res", "=", "p", ".", "map", "(", "run_evaluation", ",", "zip", "(", "test", ",", "ref", ",", "[", "evaluator", "]", "*", "len", "(", "ref", ")", ",", "[", "metric_kwargs", "]", "*", "len", "(", "ref", ")", ")", ")", "\n", "p", ".", "close", "(", ")", "\n", "p", ".", "join", "(", ")", "\n", "\n", "for", "i", "in", "range", "(", "len", "(", "all_res", ")", ")", ":", "\n", "        ", "all_scores", "[", "\"all\"", "]", ".", "append", "(", "all_res", "[", "i", "]", ")", "\n", "\n", "# append score list for mean", "\n", "for", "label", ",", "score_dict", "in", "all_res", "[", "i", "]", ".", "items", "(", ")", ":", "\n", "            ", "if", "label", "in", "(", "\"test\"", ",", "\"reference\"", ")", ":", "\n", "                ", "continue", "\n", "", "if", "label", "not", "in", "all_scores", "[", "\"mean\"", "]", ":", "\n", "                ", "all_scores", "[", "\"mean\"", "]", "[", "label", "]", "=", "OrderedDict", "(", ")", "\n", "", "for", "score", ",", "value", "in", "score_dict", ".", "items", "(", ")", ":", "\n", "                ", "if", "score", "not", "in", "all_scores", "[", "\"mean\"", "]", "[", "label", "]", ":", "\n", "                    ", "all_scores", "[", "\"mean\"", "]", "[", "label", "]", "[", "score", "]", "=", "[", "]", "\n", "", "all_scores", "[", "\"mean\"", "]", "[", "label", "]", "[", "score", "]", ".", "append", "(", "value", ")", "\n", "\n", "", "", "", "for", "label", "in", "all_scores", "[", "\"mean\"", "]", ":", "\n", "        ", "for", "score", "in", "all_scores", "[", "\"mean\"", "]", "[", "label", "]", ":", "\n", "            ", "if", "nanmean", ":", "\n", "                ", "all_scores", "[", "\"mean\"", "]", "[", "label", "]", "[", "score", "]", "=", "float", "(", "np", ".", "nanmean", "(", "all_scores", "[", "\"mean\"", "]", "[", "label", "]", "[", "score", "]", ")", ")", "\n", "", "else", ":", "\n", "                ", "all_scores", "[", "\"mean\"", "]", "[", "label", "]", "[", "score", "]", "=", "float", "(", "np", ".", "mean", "(", "all_scores", "[", "\"mean\"", "]", "[", "label", "]", "[", "score", "]", ")", ")", "\n", "\n", "# save to file if desired", "\n", "# we create a hopefully unique id by hashing the entire output dictionary", "\n", "", "", "", "if", "json_output_file", "is", "not", "None", ":", "\n", "        ", "json_dict", "=", "OrderedDict", "(", ")", "\n", "json_dict", "[", "\"name\"", "]", "=", "json_name", "\n", "json_dict", "[", "\"description\"", "]", "=", "json_description", "\n", "timestamp", "=", "datetime", ".", "today", "(", ")", "\n", "json_dict", "[", "\"timestamp\"", "]", "=", "str", "(", "timestamp", ")", "\n", "json_dict", "[", "\"task\"", "]", "=", "json_task", "\n", "json_dict", "[", "\"author\"", "]", "=", "json_author", "\n", "json_dict", "[", "\"results\"", "]", "=", "all_scores", "\n", "json_dict", "[", "\"id\"", "]", "=", "hashlib", ".", "md5", "(", "json", ".", "dumps", "(", "json_dict", ")", ".", "encode", "(", "\"utf-8\"", ")", ")", ".", "hexdigest", "(", ")", "[", ":", "12", "]", "\n", "save_json", "(", "json_dict", ",", "json_output_file", ")", "\n", "\n", "\n", "", "return", "all_scores", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.evaluation.evaluator.aggregate_scores_for_experiment": [[403, 444], ["numpy.load", "np.load.mean", "collections.OrderedDict", "range", "collections.OrderedDict", "datetime.datetime.today", "str", "list", "results.append", "enumerate", "hashlib.md5().hexdigest", "open", "json.dump", "open.close", "map", "collections.OrderedDict", "collections.OrderedDict", "collections.OrderedDict", "enumerate", "range", "float", "float", "hashlib.md5", "json.dumps().encode", "json.dumps"], "function", ["None"], ["", "def", "aggregate_scores_for_experiment", "(", "score_file", ",", "\n", "labels", "=", "None", ",", "\n", "metrics", "=", "Evaluator", ".", "default_metrics", ",", "\n", "nanmean", "=", "True", ",", "\n", "json_output_file", "=", "None", ",", "\n", "json_name", "=", "\"\"", ",", "\n", "json_description", "=", "\"\"", ",", "\n", "json_author", "=", "\"Fabian\"", ",", "\n", "json_task", "=", "\"\"", ")", ":", "\n", "\n", "    ", "scores", "=", "np", ".", "load", "(", "score_file", ")", "\n", "scores_mean", "=", "scores", ".", "mean", "(", "0", ")", "\n", "if", "labels", "is", "None", ":", "\n", "        ", "labels", "=", "list", "(", "map", "(", "str", ",", "range", "(", "scores", ".", "shape", "[", "1", "]", ")", ")", ")", "\n", "\n", "", "results", "=", "[", "]", "\n", "results_mean", "=", "OrderedDict", "(", ")", "\n", "for", "i", "in", "range", "(", "scores", ".", "shape", "[", "0", "]", ")", ":", "\n", "        ", "results", ".", "append", "(", "OrderedDict", "(", ")", ")", "\n", "for", "l", ",", "label", "in", "enumerate", "(", "labels", ")", ":", "\n", "            ", "results", "[", "-", "1", "]", "[", "label", "]", "=", "OrderedDict", "(", ")", "\n", "results_mean", "[", "label", "]", "=", "OrderedDict", "(", ")", "\n", "for", "m", ",", "metric", "in", "enumerate", "(", "metrics", ")", ":", "\n", "                ", "results", "[", "-", "1", "]", "[", "label", "]", "[", "metric", "]", "=", "float", "(", "scores", "[", "i", "]", "[", "l", "]", "[", "m", "]", ")", "\n", "results_mean", "[", "label", "]", "[", "metric", "]", "=", "float", "(", "scores_mean", "[", "l", "]", "[", "m", "]", ")", "\n", "\n", "", "", "", "json_dict", "=", "OrderedDict", "(", ")", "\n", "json_dict", "[", "\"name\"", "]", "=", "json_name", "\n", "json_dict", "[", "\"description\"", "]", "=", "json_description", "\n", "timestamp", "=", "datetime", ".", "today", "(", ")", "\n", "json_dict", "[", "\"timestamp\"", "]", "=", "str", "(", "timestamp", ")", "\n", "json_dict", "[", "\"task\"", "]", "=", "json_task", "\n", "json_dict", "[", "\"author\"", "]", "=", "json_author", "\n", "json_dict", "[", "\"results\"", "]", "=", "{", "\"all\"", ":", "results", ",", "\"mean\"", ":", "results_mean", "}", "\n", "json_dict", "[", "\"id\"", "]", "=", "hashlib", ".", "md5", "(", "json", ".", "dumps", "(", "json_dict", ")", ".", "encode", "(", "\"utf-8\"", ")", ")", ".", "hexdigest", "(", ")", "[", ":", "12", "]", "\n", "if", "json_output_file", "is", "not", "None", ":", "\n", "        ", "json_output_file", "=", "open", "(", "json_output_file", ",", "\"w\"", ")", "\n", "json", ".", "dump", "(", "json_dict", ",", "json_output_file", ",", "indent", "=", "4", ",", "separators", "=", "(", "\",\"", ",", "\": \"", ")", ")", "\n", "json_output_file", ".", "close", "(", ")", "\n", "\n", "", "return", "json_dict", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.evaluation.evaluator.evaluate_folder": [[446, 462], ["batchgenerators.utilities.file_and_folder_operations.subfiles", "batchgenerators.utilities.file_and_folder_operations.subfiles", "all", "all", "evaluator.aggregate_scores", "batchgenerators.utilities.file_and_folder_operations.join", "batchgenerators.utilities.file_and_folder_operations.join", "batchgenerators.utilities.file_and_folder_operations.join"], "function", ["home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.evaluation.evaluator.aggregate_scores"], ["", "def", "evaluate_folder", "(", "folder_with_gts", ":", "str", ",", "folder_with_predictions", ":", "str", ",", "labels", ":", "tuple", ",", "**", "metric_kwargs", ")", ":", "\n", "    ", "\"\"\"\n    writes a summary.json to folder_with_predictions\n    :param folder_with_gts: folder where the ground truth segmentations are saved. Must be nifti files.\n    :param folder_with_predictions: folder where the predicted segmentations are saved. Must be nifti files.\n    :param labels: tuple of int with the labels in the dataset. For example (0, 1, 2, 3) for Task001_BrainTumour.\n    :return:\n    \"\"\"", "\n", "files_gt", "=", "subfiles", "(", "folder_with_gts", ",", "suffix", "=", "\".nii.gz\"", ",", "join", "=", "False", ")", "\n", "files_pred", "=", "subfiles", "(", "folder_with_predictions", ",", "suffix", "=", "\".nii.gz\"", ",", "join", "=", "False", ")", "\n", "assert", "all", "(", "[", "i", "in", "files_pred", "for", "i", "in", "files_gt", "]", ")", ",", "\"files missing in folder_with_predictions\"", "\n", "assert", "all", "(", "[", "i", "in", "files_gt", "for", "i", "in", "files_pred", "]", ")", ",", "\"files missing in folder_with_gts\"", "\n", "test_ref_pairs", "=", "[", "(", "join", "(", "folder_with_predictions", ",", "i", ")", ",", "join", "(", "folder_with_gts", ",", "i", ")", ")", "for", "i", "in", "files_pred", "]", "\n", "res", "=", "aggregate_scores", "(", "test_ref_pairs", ",", "json_output_file", "=", "join", "(", "folder_with_predictions", ",", "\"summary.json\"", ")", ",", "\n", "num_threads", "=", "8", ",", "labels", "=", "labels", ",", "**", "metric_kwargs", ")", "\n", "return", "res", "\n", "", ""]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.model_selection.figure_out_what_to_submit.find_task_name": [[29, 34], ["subdirs", "len", "len"], "function", ["None"], ["def", "find_task_name", "(", "folder", ",", "task_id", ")", ":", "\n", "    ", "candidates", "=", "subdirs", "(", "folder", ",", "prefix", "=", "\"Task%03.0d_\"", "%", "task_id", ",", "join", "=", "False", ")", "\n", "assert", "len", "(", "candidates", ")", ">", "0", ",", "\"no candidate for Task id %d found in folder %s\"", "%", "(", "task_id", ",", "folder", ")", "\n", "assert", "len", "(", "candidates", ")", "==", "1", ",", "\"more than one candidate for Task id %d found in folder %s\"", "%", "(", "task_id", ",", "folder", ")", "\n", "return", "candidates", "[", "0", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.model_selection.figure_out_what_to_submit.get_mean_foreground_dice": [[36, 39], ["load_json", "figure_out_what_to_submit.get_foreground_mean"], "function", ["home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.model_selection.figure_out_what_to_submit.get_foreground_mean"], ["", "def", "get_mean_foreground_dice", "(", "json_file", ")", ":", "\n", "    ", "results", "=", "load_json", "(", "json_file", ")", "\n", "return", "get_foreground_mean", "(", "results", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.model_selection.figure_out_what_to_submit.get_foreground_mean": [[41, 45], ["numpy.mean", "results_mean.keys"], "function", ["None"], ["", "def", "get_foreground_mean", "(", "results", ")", ":", "\n", "    ", "results_mean", "=", "results", "[", "'results'", "]", "[", "'mean'", "]", "\n", "dice_scores", "=", "[", "results_mean", "[", "i", "]", "[", "'Dice'", "]", "for", "i", "in", "results_mean", ".", "keys", "(", ")", "if", "i", "!=", "\"0\"", "and", "i", "!=", "'mean'", "]", "\n", "return", "np", ".", "mean", "(", "dice_scores", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.model_selection.figure_out_what_to_submit.main": [[47, 198], ["argparse.ArgumentParser", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.parse_args", "tuple", "int", "print", "list", "numpy.max", "results.items", "results.items", "join", "maybe_mkdir_p", "len", "int", "len", "itertools.combinations", "results.values", "print", "open", "f.write", "open", "f.write", "range", "f.write", "f.write", "all_results.keys", "nnunet.utilities.folder_names.get_output_folder_name", "isdir", "join", "join", "isfile", "isdir", "join", "figure_out_what_to_submit.get_mean_foreground_dice", "nnunet.evaluation.add_mean_dice_to_json.foreground_mean", "valid_models.append", "join", "maybe_mkdir_p", "nnunet.utilities.folder_names.get_output_folder_name", "nnunet.utilities.folder_names.get_output_folder_name", "print", "nnunet.evaluation.model_selection.ensemble.ensemble", "figure_out_what_to_submit.get_mean_foreground_dice", "join", "nnunet.evaluation.add_mean_dice_to_json.foreground_mean", "print", "print", "k.startswith", "print", "join", "join", "f.write", "f.write", "range", "f.write", "f.write", "id_task_mapping.keys", "figure_out_what_to_submit.find_task_name", "print", "nnunet.postprocessing.consolidate_postprocessing.consolidate_folds", "join", "tmp.split", "model1.split", "model2.split", "all_results[].keys", "f.write", "nnunet.utilities.folder_names.get_output_folder_name", "isfile", "isdir", "load_json", "print", "print", "load_json", "len", "join", "str"], "function", ["home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.utilities.folder_names.get_output_folder_name", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.model_selection.figure_out_what_to_submit.get_mean_foreground_dice", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.evaluation.add_mean_dice_to_json.foreground_mean", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.utilities.folder_names.get_output_folder_name", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.utilities.folder_names.get_output_folder_name", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.model_selection.ensemble.ensemble", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.model_selection.figure_out_what_to_submit.get_mean_foreground_dice", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.evaluation.add_mean_dice_to_json.foreground_mean", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.model_selection.figure_out_what_to_submit.find_task_name", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.postprocessing.consolidate_postprocessing.consolidate_folds", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.utilities.folder_names.get_output_folder_name"], ["", "def", "main", "(", ")", ":", "\n", "    ", "import", "argparse", "\n", "parser", "=", "argparse", ".", "ArgumentParser", "(", "usage", "=", "\"This is intended to identify the best model based on the five fold \"", "\n", "\"cross-validation. Running this script requires all models to have been run \"", "\n", "\"already. This script will summarize the results of the five folds of all \"", "\n", "\"models in one json each for easy interpretability\"", ")", "\n", "\n", "parser", ".", "add_argument", "(", "\"-m\"", ",", "'--models'", ",", "nargs", "=", "\"+\"", ",", "required", "=", "False", ",", "default", "=", "[", "'2d'", ",", "'3d_lowres'", ",", "'3d_fullres'", ",", "\n", "'3d_cascade_fullres'", "]", ")", "\n", "parser", ".", "add_argument", "(", "\"-t\"", ",", "'--task_ids'", ",", "nargs", "=", "\"+\"", ",", "required", "=", "True", ")", "\n", "\n", "parser", ".", "add_argument", "(", "\"-tr\"", ",", "type", "=", "str", ",", "required", "=", "False", ",", "default", "=", "default_trainer", ",", "\n", "help", "=", "\"nnUNetTrainer class. Default: %s\"", "%", "default_trainer", ")", "\n", "parser", ".", "add_argument", "(", "\"-ctr\"", ",", "type", "=", "str", ",", "required", "=", "False", ",", "default", "=", "default_cascade_trainer", ",", "\n", "help", "=", "\"nnUNetTrainer class for cascade model. Default: %s\"", "%", "default_cascade_trainer", ")", "\n", "parser", ".", "add_argument", "(", "\"-pl\"", ",", "type", "=", "str", ",", "required", "=", "False", ",", "default", "=", "default_plans_identifier", ",", "\n", "help", "=", "\"plans name, Default: %s\"", "%", "default_plans_identifier", ")", "\n", "parser", ".", "add_argument", "(", "'-f'", ",", "'--folds'", ",", "nargs", "=", "'+'", ",", "default", "=", "(", "0", ",", "1", ",", "2", ",", "3", ",", "4", ")", ",", "help", "=", "\"use this if you have non-standard folds\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--strict\"", ",", "required", "=", "False", ",", "default", "=", "False", ",", "action", "=", "\"store_true\"", ",", "\n", "help", "=", "\"set this flag if you want this script to crash of one of the models is missing\"", ")", "\n", "\n", "args", "=", "parser", ".", "parse_args", "(", ")", "\n", "tasks", "=", "[", "int", "(", "i", ")", "for", "i", "in", "args", ".", "task_ids", "]", "\n", "\n", "models", "=", "args", ".", "models", "\n", "tr", "=", "args", ".", "tr", "\n", "trc", "=", "args", ".", "ctr", "\n", "strict", "=", "args", ".", "strict", "\n", "pl", "=", "args", ".", "pl", "\n", "folds", "=", "tuple", "(", "int", "(", "i", ")", "for", "i", "in", "args", ".", "folds", ")", "\n", "\n", "validation_folder", "=", "\"validation_raw\"", "\n", "\n", "# this script now acts independently from the summary jsons. That was unnecessary", "\n", "id_task_mapping", "=", "{", "}", "\n", "# for each task, run ensembling using all combinations of two models", "\n", "for", "t", "in", "tasks", ":", "\n", "# first collect pure model performance (postprocessed)", "\n", "        ", "results", "=", "{", "}", "\n", "all_results", "=", "{", "}", "\n", "valid_models", "=", "[", "]", "\n", "for", "m", "in", "models", ":", "\n", "            ", "try", ":", "\n", "                ", "if", "m", "==", "\"3d_cascade_fullres\"", ":", "\n", "                    ", "trainer", "=", "trc", "\n", "", "else", ":", "\n", "                    ", "trainer", "=", "tr", "\n", "\n", "", "if", "t", "not", "in", "id_task_mapping", ".", "keys", "(", ")", ":", "\n", "                    ", "task_name", "=", "find_task_name", "(", "get_output_folder_name", "(", "m", ")", ",", "t", ")", "\n", "id_task_mapping", "[", "t", "]", "=", "task_name", "\n", "\n", "", "output_folder", "=", "get_output_folder_name", "(", "m", ",", "id_task_mapping", "[", "t", "]", ",", "trainer", ",", "pl", ")", "\n", "assert", "isdir", "(", "output_folder", ")", ",", "\"Output folder for model %s is missing, expected: %s\"", "%", "(", "m", ",", "output_folder", ")", "\n", "\n", "# we need a postprocessing_json for inference, so that must be present", "\n", "postprocessing_json", "=", "join", "(", "output_folder", ",", "\"postprocessing.json\"", ")", "\n", "# we need cv_niftis_postprocessed to know the single model performance", "\n", "cv_niftis_folder", "=", "join", "(", "output_folder", ",", "\"cv_niftis_raw\"", ")", "\n", "if", "not", "isfile", "(", "postprocessing_json", ")", "or", "not", "isdir", "(", "cv_niftis_folder", ")", ":", "\n", "                    ", "print", "(", "\"running missing postprocessing for %s and model %s\"", "%", "(", "id_task_mapping", "[", "t", "]", ",", "m", ")", ")", "\n", "consolidate_folds", "(", "output_folder", ",", "folds", "=", "folds", ")", "\n", "", "assert", "isfile", "(", "postprocessing_json", ")", ",", "\"Postprocessing json missing, expected: %s\"", "%", "postprocessing_json", "\n", "assert", "isdir", "(", "cv_niftis_folder", ")", ",", "\"Folder with niftis from CV missing, expected: %s\"", "%", "cv_niftis_folder", "\n", "\n", "# obtain mean foreground dice", "\n", "summary_file", "=", "join", "(", "cv_niftis_folder", ",", "\"summary.json\"", ")", "\n", "results", "[", "m", "]", "=", "get_mean_foreground_dice", "(", "summary_file", ")", "\n", "foreground_mean", "(", "summary_file", ")", "\n", "all_results", "[", "m", "]", "=", "load_json", "(", "summary_file", ")", "[", "'results'", "]", "[", "'mean'", "]", "\n", "valid_models", ".", "append", "(", "m", ")", "\n", "\n", "", "except", "Exception", "as", "e", ":", "\n", "                ", "if", "strict", ":", "\n", "                    ", "raise", "e", "\n", "", "else", ":", "\n", "                    ", "print", "(", "\"WARNING!\"", ")", "\n", "print", "(", "e", ")", "\n", "\n", "# now run ensembling and add ensembling to results", "\n", "", "", "", "print", "(", "\"\\nFound the following valid models:\\n\"", ",", "valid_models", ")", "\n", "if", "len", "(", "valid_models", ")", ">", "1", ":", "\n", "            ", "for", "m1", ",", "m2", "in", "combinations", "(", "valid_models", ",", "2", ")", ":", "\n", "\n", "                ", "trainer_m1", "=", "trc", "if", "m1", "==", "\"3d_cascade_fullres\"", "else", "tr", "\n", "trainer_m2", "=", "trc", "if", "m2", "==", "\"3d_cascade_fullres\"", "else", "tr", "\n", "\n", "ensemble_name", "=", "\"ensemble_\"", "+", "m1", "+", "\"__\"", "+", "trainer_m1", "+", "\"__\"", "+", "pl", "+", "\"--\"", "+", "m2", "+", "\"__\"", "+", "trainer_m2", "+", "\"__\"", "+", "pl", "\n", "output_folder_base", "=", "join", "(", "network_training_output_dir", ",", "\"ensembles\"", ",", "id_task_mapping", "[", "t", "]", ",", "ensemble_name", ")", "\n", "maybe_mkdir_p", "(", "output_folder_base", ")", "\n", "\n", "network1_folder", "=", "get_output_folder_name", "(", "m1", ",", "id_task_mapping", "[", "t", "]", ",", "trainer_m1", ",", "pl", ")", "\n", "network2_folder", "=", "get_output_folder_name", "(", "m2", ",", "id_task_mapping", "[", "t", "]", ",", "trainer_m2", ",", "pl", ")", "\n", "\n", "print", "(", "\"ensembling\"", ",", "network1_folder", ",", "network2_folder", ")", "\n", "ensemble", "(", "network1_folder", ",", "network2_folder", ",", "output_folder_base", ",", "id_task_mapping", "[", "t", "]", ",", "validation_folder", ",", "folds", ")", "\n", "# ensembling will automatically do postprocessingget_foreground_mean", "\n", "\n", "# now get result of ensemble", "\n", "results", "[", "ensemble_name", "]", "=", "get_mean_foreground_dice", "(", "join", "(", "output_folder_base", ",", "\"ensembled_raw\"", ",", "\"summary.json\"", ")", ")", "\n", "summary_file", "=", "join", "(", "output_folder_base", ",", "\"ensembled_raw\"", ",", "\"summary.json\"", ")", "\n", "foreground_mean", "(", "summary_file", ")", "\n", "all_results", "[", "ensemble_name", "]", "=", "load_json", "(", "summary_file", ")", "[", "'results'", "]", "[", "'mean'", "]", "\n", "\n", "# now print all mean foreground dice and highlight the best", "\n", "", "", "foreground_dices", "=", "list", "(", "results", ".", "values", "(", ")", ")", "\n", "best", "=", "np", ".", "max", "(", "foreground_dices", ")", "\n", "for", "k", ",", "v", "in", "results", ".", "items", "(", ")", ":", "\n", "            ", "print", "(", "k", ",", "v", ")", "\n", "\n", "", "predict_str", "=", "\"\"", "\n", "best_model", "=", "None", "\n", "for", "k", ",", "v", "in", "results", ".", "items", "(", ")", ":", "\n", "            ", "if", "v", "==", "best", ":", "\n", "                ", "print", "(", "\"%s submit model %s\"", "%", "(", "id_task_mapping", "[", "t", "]", ",", "k", ")", ",", "v", ")", "\n", "best_model", "=", "k", "\n", "print", "(", "\"\\nHere is how you should predict test cases. Run in sequential order and replace all input and output folder names with your personalized ones\\n\"", ")", "\n", "if", "k", ".", "startswith", "(", "\"ensemble\"", ")", ":", "\n", "                    ", "tmp", "=", "k", "[", "len", "(", "\"ensemble_\"", ")", ":", "]", "\n", "model1", ",", "model2", "=", "tmp", ".", "split", "(", "\"--\"", ")", "\n", "m1", ",", "t1", ",", "pl1", "=", "model1", ".", "split", "(", "\"__\"", ")", "\n", "m2", ",", "t2", ",", "pl2", "=", "model2", ".", "split", "(", "\"__\"", ")", "\n", "predict_str", "+=", "\"nnUNet_predict -i FOLDER_WITH_TEST_CASES -o OUTPUT_FOLDER_MODEL1 -tr \"", "+", "tr", "+", "\" -ctr \"", "+", "trc", "+", "\" -m \"", "+", "m1", "+", "\" -p \"", "+", "pl", "+", "\" -t \"", "+", "id_task_mapping", "[", "t", "]", "+", "\"\\n\"", "\n", "predict_str", "+=", "\"nnUNet_predict -i FOLDER_WITH_TEST_CASES -o OUTPUT_FOLDER_MODEL2 -tr \"", "+", "tr", "+", "\" -ctr \"", "+", "trc", "+", "\" -m \"", "+", "m2", "+", "\" -p \"", "+", "pl", "+", "\" -t \"", "+", "id_task_mapping", "[", "t", "]", "+", "\"\\n\"", "\n", "\n", "predict_str", "+=", "\"nnUNet_ensemble -f OUTPUT_FOLDER_MODEL1 OUTPUT_FOLDER_MODEL2 -o OUTPUT_FOLDER -pp \"", "+", "join", "(", "network_training_output_dir", ",", "\"ensembles\"", ",", "id_task_mapping", "[", "t", "]", ",", "k", ",", "\"postprocessing.json\"", ")", "+", "\"\\n\"", "\n", "", "else", ":", "\n", "                    ", "predict_str", "+=", "\"nnUNet_predict -i FOLDER_WITH_TEST_CASES -o OUTPUT_FOLDER_MODEL1 -tr \"", "+", "tr", "+", "\" -ctr \"", "+", "trc", "+", "\" -m \"", "+", "k", "+", "\" -p \"", "+", "pl", "+", "\" -t \"", "+", "id_task_mapping", "[", "t", "]", "+", "\"\\n\"", "\n", "", "print", "(", "predict_str", ")", "\n", "\n", "", "", "summary_folder", "=", "join", "(", "network_training_output_dir", ",", "\"ensembles\"", ",", "id_task_mapping", "[", "t", "]", ")", "\n", "maybe_mkdir_p", "(", "summary_folder", ")", "\n", "with", "open", "(", "join", "(", "summary_folder", ",", "\"prediction_commands.txt\"", ")", ",", "'w'", ")", "as", "f", ":", "\n", "            ", "f", ".", "write", "(", "predict_str", ")", "\n", "\n", "", "num_classes", "=", "len", "(", "[", "i", "for", "i", "in", "all_results", "[", "best_model", "]", ".", "keys", "(", ")", "if", "i", "!=", "'mean'", "]", ")", "\n", "with", "open", "(", "join", "(", "summary_folder", ",", "\"summary.csv\"", ")", ",", "'w'", ")", "as", "f", ":", "\n", "            ", "f", ".", "write", "(", "\"model\"", ")", "\n", "for", "c", "in", "range", "(", "1", ",", "num_classes", ")", ":", "\n", "                ", "f", ".", "write", "(", "\",class%d\"", "%", "c", ")", "\n", "", "f", ".", "write", "(", "\",average\"", ")", "\n", "f", ".", "write", "(", "\"\\n\"", ")", "\n", "for", "m", "in", "all_results", ".", "keys", "(", ")", ":", "\n", "                ", "f", ".", "write", "(", "m", ")", "\n", "for", "c", "in", "range", "(", "1", ",", "num_classes", ")", ":", "\n", "                    ", "f", ".", "write", "(", "\",%01.4f\"", "%", "all_results", "[", "m", "]", "[", "str", "(", "c", ")", "]", "[", "\"Dice\"", "]", ")", "\n", "", "f", ".", "write", "(", "\",%01.4f\"", "%", "all_results", "[", "m", "]", "[", "'mean'", "]", "[", "\"Dice\"", "]", ")", "\n", "f", ".", "write", "(", "\"\\n\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.model_selection.summarize_results_in_one_json.summarize": [[22, 99], ["join", "maybe_mkdir_p", "list", "len", "range", "int", "int", "subfolders", "join", "subdirs", "isdir", "join", "len", "print", "trainer.startswith", "join", "join", "join", "subfolders", "collections.OrderedDict", "isdir", "join", "join", "join", "metrics_tmp.keys", "collections.OrderedDict.keys", "collections.OrderedDict", "collections.OrderedDict", "save_json", "save_json", "nnunet.evaluation.add_mean_dice_to_json.foreground_mean", "nnunet.evaluation.add_mean_dice_to_json.foreground_mean", "isdir", "isdir", "join", "isfile", "print", "metrics_tmp[].keys", "metrics[].keys", "join", "join", "join", "join", "isdir", "join", "load_json", "collections.OrderedDict.get", "collections.OrderedDict", "[].append", "numpy.mean", "join", "metrics[].get", "len", "len"], "function", ["home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.evaluation.add_mean_dice_to_json.foreground_mean", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.evaluation.add_mean_dice_to_json.foreground_mean"], ["def", "summarize", "(", "tasks", ",", "models", "=", "(", "'2d'", ",", "'3d_lowres'", ",", "'3d_fullres'", ",", "'3d_cascade_fullres'", ")", ",", "\n", "output_dir", "=", "join", "(", "network_training_output_dir", ",", "\"summary_jsons\"", ")", ",", "folds", "=", "(", "0", ",", "1", ",", "2", ",", "3", ",", "4", ")", ")", ":", "\n", "    ", "maybe_mkdir_p", "(", "output_dir", ")", "\n", "\n", "if", "len", "(", "tasks", ")", "==", "1", "and", "tasks", "[", "0", "]", "==", "\"all\"", ":", "\n", "        ", "tasks", "=", "list", "(", "range", "(", "100", ")", ")", "\n", "", "else", ":", "\n", "        ", "tasks", "=", "[", "int", "(", "i", ")", "for", "i", "in", "tasks", "]", "\n", "\n", "", "for", "model", "in", "models", ":", "\n", "        ", "for", "t", "in", "tasks", ":", "\n", "            ", "t", "=", "int", "(", "t", ")", "\n", "if", "not", "isdir", "(", "join", "(", "network_training_output_dir", ",", "model", ")", ")", ":", "\n", "                ", "continue", "\n", "", "task_name", "=", "subfolders", "(", "join", "(", "network_training_output_dir", ",", "model", ")", ",", "prefix", "=", "\"Task%03.0d\"", "%", "t", ",", "join", "=", "False", ")", "\n", "if", "len", "(", "task_name", ")", "!=", "1", ":", "\n", "                ", "print", "(", "\"did not find unique output folder for network %s and task %s\"", "%", "(", "model", ",", "t", ")", ")", "\n", "continue", "\n", "", "task_name", "=", "task_name", "[", "0", "]", "\n", "out_dir_task", "=", "join", "(", "network_training_output_dir", ",", "model", ",", "task_name", ")", "\n", "\n", "model_trainers", "=", "subdirs", "(", "out_dir_task", ",", "join", "=", "False", ")", "\n", "for", "trainer", "in", "model_trainers", ":", "\n", "                ", "if", "trainer", ".", "startswith", "(", "\"fold\"", ")", ":", "\n", "                    ", "continue", "\n", "", "out_dir", "=", "join", "(", "out_dir_task", ",", "trainer", ")", "\n", "\n", "validation_folders", "=", "[", "]", "\n", "for", "fld", "in", "folds", ":", "\n", "                    ", "d", "=", "join", "(", "out_dir", ",", "\"fold%d\"", "%", "fld", ")", "\n", "if", "not", "isdir", "(", "d", ")", ":", "\n", "                        ", "d", "=", "join", "(", "out_dir", ",", "\"fold_%d\"", "%", "fld", ")", "\n", "if", "not", "isdir", "(", "d", ")", ":", "\n", "                            ", "break", "\n", "", "", "validation_folders", "+=", "subfolders", "(", "d", ",", "prefix", "=", "\"validation\"", ",", "join", "=", "False", ")", "\n", "\n", "", "for", "v", "in", "validation_folders", ":", "\n", "                    ", "ok", "=", "True", "\n", "metrics", "=", "OrderedDict", "(", ")", "\n", "for", "fld", "in", "folds", ":", "\n", "                        ", "d", "=", "join", "(", "out_dir", ",", "\"fold%d\"", "%", "fld", ")", "\n", "if", "not", "isdir", "(", "d", ")", ":", "\n", "                            ", "d", "=", "join", "(", "out_dir", ",", "\"fold_%d\"", "%", "fld", ")", "\n", "if", "not", "isdir", "(", "d", ")", ":", "\n", "                                ", "ok", "=", "False", "\n", "break", "\n", "", "", "validation_folder", "=", "join", "(", "d", ",", "v", ")", "\n", "\n", "if", "not", "isfile", "(", "join", "(", "validation_folder", ",", "\"summary.json\"", ")", ")", ":", "\n", "                            ", "print", "(", "\"summary.json missing for net %s task %s fold %d\"", "%", "(", "model", ",", "task_name", ",", "fld", ")", ")", "\n", "ok", "=", "False", "\n", "break", "\n", "\n", "", "metrics_tmp", "=", "load_json", "(", "join", "(", "validation_folder", ",", "\"summary.json\"", ")", ")", "[", "\"results\"", "]", "[", "\"mean\"", "]", "\n", "for", "l", "in", "metrics_tmp", ".", "keys", "(", ")", ":", "\n", "                            ", "if", "metrics", ".", "get", "(", "l", ")", "is", "None", ":", "\n", "                                ", "metrics", "[", "l", "]", "=", "OrderedDict", "(", ")", "\n", "", "for", "m", "in", "metrics_tmp", "[", "l", "]", ".", "keys", "(", ")", ":", "\n", "                                ", "if", "metrics", "[", "l", "]", ".", "get", "(", "m", ")", "is", "None", ":", "\n", "                                    ", "metrics", "[", "l", "]", "[", "m", "]", "=", "[", "]", "\n", "", "metrics", "[", "l", "]", "[", "m", "]", ".", "append", "(", "metrics_tmp", "[", "l", "]", "[", "m", "]", ")", "\n", "", "", "", "if", "ok", ":", "\n", "                        ", "for", "l", "in", "metrics", ".", "keys", "(", ")", ":", "\n", "                            ", "for", "m", "in", "metrics", "[", "l", "]", ".", "keys", "(", ")", ":", "\n", "                                ", "assert", "len", "(", "metrics", "[", "l", "]", "[", "m", "]", ")", "==", "len", "(", "folds", ")", "\n", "metrics", "[", "l", "]", "[", "m", "]", "=", "np", ".", "mean", "(", "metrics", "[", "l", "]", "[", "m", "]", ")", "\n", "", "", "json_out", "=", "OrderedDict", "(", ")", "\n", "json_out", "[", "\"results\"", "]", "=", "OrderedDict", "(", ")", "\n", "json_out", "[", "\"results\"", "]", "[", "\"mean\"", "]", "=", "metrics", "\n", "json_out", "[", "\"task\"", "]", "=", "task_name", "\n", "json_out", "[", "\"description\"", "]", "=", "model", "+", "\" \"", "+", "task_name", "+", "\" all folds summary\"", "\n", "json_out", "[", "\"name\"", "]", "=", "model", "+", "\" \"", "+", "task_name", "+", "\" all folds summary\"", "\n", "json_out", "[", "\"experiment_name\"", "]", "=", "model", "\n", "save_json", "(", "json_out", ",", "join", "(", "out_dir", ",", "\"summary_allFolds__%s.json\"", "%", "v", ")", ")", "\n", "save_json", "(", "json_out", ",", "join", "(", "output_dir", ",", "\"%s__%s__%s__%s.json\"", "%", "(", "task_name", ",", "model", ",", "trainer", ",", "v", ")", ")", ")", "\n", "foreground_mean", "(", "join", "(", "out_dir", ",", "\"summary_allFolds__%s.json\"", "%", "v", ")", ")", "\n", "foreground_mean", "(", "join", "(", "output_dir", ",", "\"%s__%s__%s__%s.json\"", "%", "(", "task_name", ",", "model", ",", "trainer", ",", "v", ")", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.model_selection.summarize_results_in_one_json.summarize2": [[101, 201], ["join", "maybe_mkdir_p", "list", "len", "range", "int", "subfolders", "join", "subdirs", "isdir", "join", "len", "print", "trainer.startswith", "join", "numpy.unique", "join", "join", "subfolders", "collections.OrderedDict", "collections.OrderedDict", "collections.OrderedDict", "collections.OrderedDict", "metrics[].keys", "isdir", "join", "join", "list", "[].keys", "collections.OrderedDict", "collections.OrderedDict", "save_json", "summarize_results_in_one_json.foreground_mean2", "isdir", "print", "isdir", "print", "isfile", "print", "load_json", "len", "print", "mean_metrics[].keys", "numpy.nanmean", "numpy.nanmedian", "str", "str", "str", "join", "join", "join", "join", "list", "mean_metrics.keys", "metrics[].get", "collections.OrderedDict", "[].get", "[].append"], "function", ["home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.model_selection.summarize_results_in_one_json.foreground_mean2"], ["", "", "", "", "", "", "def", "summarize2", "(", "task_ids", ",", "models", "=", "(", "'2d'", ",", "'3d_lowres'", ",", "'3d_fullres'", ",", "'3d_cascade_fullres'", ")", ",", "\n", "output_dir", "=", "join", "(", "network_training_output_dir", ",", "\"summary_jsons\"", ")", ",", "folds", "=", "(", "0", ",", "1", ",", "2", ",", "3", ",", "4", ")", ")", ":", "\n", "    ", "maybe_mkdir_p", "(", "output_dir", ")", "\n", "\n", "if", "len", "(", "task_ids", ")", "==", "1", "and", "task_ids", "[", "0", "]", "==", "\"all\"", ":", "\n", "        ", "task_ids", "=", "list", "(", "range", "(", "100", ")", ")", "\n", "", "else", ":", "\n", "        ", "task_ids", "=", "[", "int", "(", "i", ")", "for", "i", "in", "task_ids", "]", "\n", "\n", "", "for", "model", "in", "models", ":", "\n", "        ", "for", "t", "in", "task_ids", ":", "\n", "            ", "if", "not", "isdir", "(", "join", "(", "network_training_output_dir", ",", "model", ")", ")", ":", "\n", "                ", "continue", "\n", "", "task_name", "=", "subfolders", "(", "join", "(", "network_training_output_dir", ",", "model", ")", ",", "prefix", "=", "\"Task%03.0d\"", "%", "t", ",", "join", "=", "False", ")", "\n", "if", "len", "(", "task_name", ")", "!=", "1", ":", "\n", "                ", "print", "(", "\"did not find unique output folder for network %s and task %s\"", "%", "(", "model", ",", "t", ")", ")", "\n", "continue", "\n", "", "task_name", "=", "task_name", "[", "0", "]", "\n", "out_dir_task", "=", "join", "(", "network_training_output_dir", ",", "model", ",", "task_name", ")", "\n", "\n", "model_trainers", "=", "subdirs", "(", "out_dir_task", ",", "join", "=", "False", ")", "\n", "for", "trainer", "in", "model_trainers", ":", "\n", "                ", "if", "trainer", ".", "startswith", "(", "\"fold\"", ")", ":", "\n", "                    ", "continue", "\n", "", "out_dir", "=", "join", "(", "out_dir_task", ",", "trainer", ")", "\n", "\n", "validation_folders", "=", "[", "]", "\n", "for", "fld", "in", "folds", ":", "\n", "                    ", "fold_output_dir", "=", "join", "(", "out_dir", ",", "\"fold_%d\"", "%", "fld", ")", "\n", "if", "not", "isdir", "(", "fold_output_dir", ")", ":", "\n", "                        ", "continue", "\n", "", "validation_folders", "+=", "subfolders", "(", "fold_output_dir", ",", "prefix", "=", "\"validation\"", ",", "join", "=", "False", ")", "\n", "\n", "", "validation_folders", "=", "np", ".", "unique", "(", "validation_folders", ")", "\n", "\n", "for", "v", "in", "validation_folders", ":", "\n", "                    ", "ok", "=", "True", "\n", "metrics", "=", "OrderedDict", "(", ")", "\n", "metrics", "[", "'mean'", "]", "=", "OrderedDict", "(", ")", "\n", "metrics", "[", "'median'", "]", "=", "OrderedDict", "(", ")", "\n", "metrics", "[", "'all'", "]", "=", "OrderedDict", "(", ")", "\n", "for", "fld", "in", "folds", ":", "\n", "                        ", "fold_output_dir", "=", "join", "(", "out_dir", ",", "\"fold_%d\"", "%", "fld", ")", "\n", "\n", "if", "not", "isdir", "(", "fold_output_dir", ")", ":", "\n", "                            ", "print", "(", "\"fold missing\"", ",", "model", ",", "task_name", ",", "trainer", ",", "fld", ")", "\n", "ok", "=", "False", "\n", "break", "\n", "", "validation_folder", "=", "join", "(", "fold_output_dir", ",", "v", ")", "\n", "\n", "if", "not", "isdir", "(", "validation_folder", ")", ":", "\n", "                            ", "print", "(", "\"validation folder missing\"", ",", "model", ",", "task_name", ",", "trainer", ",", "fld", ",", "v", ")", "\n", "ok", "=", "False", "\n", "break", "\n", "\n", "", "if", "not", "isfile", "(", "join", "(", "validation_folder", ",", "\"summary.json\"", ")", ")", ":", "\n", "                            ", "print", "(", "\"summary.json missing\"", ",", "model", ",", "task_name", ",", "trainer", ",", "fld", ",", "v", ")", "\n", "ok", "=", "False", "\n", "break", "\n", "\n", "", "all_metrics", "=", "load_json", "(", "join", "(", "validation_folder", ",", "\"summary.json\"", ")", ")", "[", "\"results\"", "]", "\n", "# we now need to get the mean and median metrics. We use the mean metrics just to get the", "\n", "# names of computed metics, we ignore the precomputed mean and do it ourselfes again", "\n", "mean_metrics", "=", "all_metrics", "[", "\"mean\"", "]", "\n", "all_labels", "=", "[", "i", "for", "i", "in", "list", "(", "mean_metrics", ".", "keys", "(", ")", ")", "if", "i", "!=", "\"mean\"", "]", "\n", "\n", "if", "len", "(", "all_labels", ")", "==", "0", ":", "print", "(", "v", ",", "fld", ")", ";", "break", "\n", "\n", "all_metrics_names", "=", "list", "(", "mean_metrics", "[", "all_labels", "[", "0", "]", "]", ".", "keys", "(", ")", ")", "\n", "for", "l", "in", "all_labels", ":", "\n", "# initialize the data structure, no values are copied yet", "\n", "                            ", "for", "k", "in", "[", "'mean'", ",", "'median'", ",", "'all'", "]", ":", "\n", "                                ", "if", "metrics", "[", "k", "]", ".", "get", "(", "l", ")", "is", "None", ":", "\n", "                                    ", "metrics", "[", "k", "]", "[", "l", "]", "=", "OrderedDict", "(", ")", "\n", "", "", "for", "m", "in", "all_metrics_names", ":", "\n", "                                ", "if", "metrics", "[", "'all'", "]", "[", "l", "]", ".", "get", "(", "m", ")", "is", "None", ":", "\n", "                                    ", "metrics", "[", "'all'", "]", "[", "l", "]", "[", "m", "]", "=", "[", "]", "\n", "", "", "", "for", "entry", "in", "all_metrics", "[", "'all'", "]", ":", "\n", "                            ", "for", "l", "in", "all_labels", ":", "\n", "                                ", "for", "m", "in", "all_metrics_names", ":", "\n", "                                    ", "metrics", "[", "'all'", "]", "[", "l", "]", "[", "m", "]", ".", "append", "(", "entry", "[", "l", "]", "[", "m", "]", ")", "\n", "# now compute mean and median", "\n", "", "", "", "", "for", "l", "in", "metrics", "[", "'all'", "]", ".", "keys", "(", ")", ":", "\n", "                        ", "for", "m", "in", "metrics", "[", "'all'", "]", "[", "l", "]", ".", "keys", "(", ")", ":", "\n", "                            ", "metrics", "[", "'mean'", "]", "[", "l", "]", "[", "m", "]", "=", "np", ".", "nanmean", "(", "metrics", "[", "'all'", "]", "[", "l", "]", "[", "m", "]", ")", "\n", "metrics", "[", "'median'", "]", "[", "l", "]", "[", "m", "]", "=", "np", ".", "nanmedian", "(", "metrics", "[", "'all'", "]", "[", "l", "]", "[", "m", "]", ")", "\n", "", "", "if", "ok", ":", "\n", "                        ", "fold_string", "=", "\"\"", "\n", "for", "f", "in", "folds", ":", "\n", "                            ", "fold_string", "+=", "str", "(", "f", ")", "\n", "", "json_out", "=", "OrderedDict", "(", ")", "\n", "json_out", "[", "\"results\"", "]", "=", "OrderedDict", "(", ")", "\n", "json_out", "[", "\"results\"", "]", "[", "\"mean\"", "]", "=", "metrics", "[", "'mean'", "]", "\n", "json_out", "[", "\"results\"", "]", "[", "\"median\"", "]", "=", "metrics", "[", "'median'", "]", "\n", "json_out", "[", "\"task\"", "]", "=", "task_name", "\n", "json_out", "[", "\"description\"", "]", "=", "model", "+", "\" \"", "+", "task_name", "+", "\"summary folds\"", "+", "str", "(", "folds", ")", "\n", "json_out", "[", "\"name\"", "]", "=", "model", "+", "\" \"", "+", "task_name", "+", "\"summary folds\"", "+", "str", "(", "folds", ")", "\n", "json_out", "[", "\"experiment_name\"", "]", "=", "model", "\n", "save_json", "(", "json_out", ",", "join", "(", "output_dir", ",", "\"%s__%s__%s__%s__%s.json\"", "%", "(", "task_name", ",", "model", ",", "trainer", ",", "v", ",", "fold_string", ")", ")", ")", "\n", "foreground_mean2", "(", "join", "(", "output_dir", ",", "\"%s__%s__%s__%s__%s.json\"", "%", "(", "task_name", ",", "model", ",", "trainer", ",", "v", ",", "fold_string", ")", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.model_selection.summarize_results_in_one_json.foreground_mean2": [[203, 218], ["numpy.array", "[].keys", "collections.OrderedDict", "collections.OrderedDict", "open", "json.load", "numpy.nanmean", "numpy.nanmean", "open", "json.dump", "int", "[].keys", "str", "str"], "function", ["None"], ["", "", "", "", "", "", "def", "foreground_mean2", "(", "filename", ")", ":", "\n", "    ", "with", "open", "(", "filename", ",", "'r'", ")", "as", "f", ":", "\n", "        ", "res", "=", "json", ".", "load", "(", "f", ")", "\n", "", "class_ids", "=", "np", ".", "array", "(", "[", "int", "(", "i", ")", "for", "i", "in", "res", "[", "'results'", "]", "[", "'mean'", "]", ".", "keys", "(", ")", "if", "(", "i", "!=", "'mean'", ")", "and", "i", "!=", "'0'", "]", ")", "\n", "\n", "metric_names", "=", "res", "[", "'results'", "]", "[", "'mean'", "]", "[", "'1'", "]", ".", "keys", "(", ")", "\n", "res", "[", "'results'", "]", "[", "'mean'", "]", "[", "\"mean\"", "]", "=", "OrderedDict", "(", ")", "\n", "res", "[", "'results'", "]", "[", "'median'", "]", "[", "\"mean\"", "]", "=", "OrderedDict", "(", ")", "\n", "for", "m", "in", "metric_names", ":", "\n", "        ", "foreground_values", "=", "[", "res", "[", "'results'", "]", "[", "'mean'", "]", "[", "str", "(", "i", ")", "]", "[", "m", "]", "for", "i", "in", "class_ids", "]", "\n", "res", "[", "'results'", "]", "[", "'mean'", "]", "[", "\"mean\"", "]", "[", "m", "]", "=", "np", ".", "nanmean", "(", "foreground_values", ")", "\n", "foreground_values", "=", "[", "res", "[", "'results'", "]", "[", "'median'", "]", "[", "str", "(", "i", ")", "]", "[", "m", "]", "for", "i", "in", "class_ids", "]", "\n", "res", "[", "'results'", "]", "[", "'median'", "]", "[", "\"mean\"", "]", "[", "m", "]", "=", "np", ".", "nanmean", "(", "foreground_values", ")", "\n", "", "with", "open", "(", "filename", ",", "'w'", ")", "as", "f", ":", "\n", "        ", "json", ".", "dump", "(", "res", ",", "f", ",", "indent", "=", "4", ",", "sort_keys", "=", "True", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.model_selection.summarize_results_with_plans.list_to_string": [[23, 28], ["None"], "function", ["None"], ["def", "list_to_string", "(", "l", ",", "delim", "=", "\",\"", ")", ":", "\n", "    ", "st", "=", "\"%03.3f\"", "%", "l", "[", "0", "]", "\n", "for", "i", "in", "l", "[", "1", ":", "]", ":", "\n", "        ", "st", "+=", "delim", "+", "\"%03.3f\"", "%", "i", "\n", "", "return", "st", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.model_selection.summarize_results_with_plans.write_plans_to_file": [[30, 55], ["load_pickle", "list", "list.sort", "f.write", "f.write", "f.write", "f.write", "f.write", "f.write", "f.write", "f.write", "f.write", "f.write", "f.write", "a[].keys", "f.write", "f.write", "f.write", "zip", "zip", "str", "str", "str", "summarize_results_with_plans.list_to_string", "str", "summarize_results_with_plans.list_to_string", "summarize_results_with_plans.list_to_string", "summarize_results_with_plans.list_to_string", "str", "str", "plans_file.split", "plans_file.split"], "function", ["home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.model_selection.summarize_results_with_plans.list_to_string", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.model_selection.summarize_results_with_plans.list_to_string", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.model_selection.summarize_results_with_plans.list_to_string", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.model_selection.summarize_results_with_plans.list_to_string"], ["", "def", "write_plans_to_file", "(", "f", ",", "plans_file", ",", "stage", "=", "0", ",", "do_linebreak_at_end", "=", "True", ",", "override_name", "=", "None", ")", ":", "\n", "    ", "a", "=", "load_pickle", "(", "plans_file", ")", "\n", "stages", "=", "list", "(", "a", "[", "'plans_per_stage'", "]", ".", "keys", "(", ")", ")", "\n", "stages", ".", "sort", "(", ")", "\n", "patch_size_in_mm", "=", "[", "i", "*", "j", "for", "i", ",", "j", "in", "zip", "(", "a", "[", "'plans_per_stage'", "]", "[", "stages", "[", "stage", "]", "]", "[", "'patch_size'", "]", ",", "\n", "a", "[", "'plans_per_stage'", "]", "[", "stages", "[", "stage", "]", "]", "[", "'current_spacing'", "]", ")", "]", "\n", "median_patient_size_in_mm", "=", "[", "i", "*", "j", "for", "i", ",", "j", "in", "zip", "(", "a", "[", "'plans_per_stage'", "]", "[", "stages", "[", "stage", "]", "]", "[", "'median_patient_size_in_voxels'", "]", ",", "\n", "a", "[", "'plans_per_stage'", "]", "[", "stages", "[", "stage", "]", "]", "[", "'current_spacing'", "]", ")", "]", "\n", "if", "override_name", "is", "None", ":", "\n", "        ", "f", ".", "write", "(", "plans_file", ".", "split", "(", "\"/\"", ")", "[", "-", "2", "]", "+", "\"__\"", "+", "plans_file", ".", "split", "(", "\"/\"", ")", "[", "-", "1", "]", ")", "\n", "", "else", ":", "\n", "        ", "f", ".", "write", "(", "override_name", ")", "\n", "", "f", ".", "write", "(", "\";%d\"", "%", "stage", ")", "\n", "f", ".", "write", "(", "\";%s\"", "%", "str", "(", "a", "[", "'plans_per_stage'", "]", "[", "stages", "[", "stage", "]", "]", "[", "'batch_size'", "]", ")", ")", "\n", "f", ".", "write", "(", "\";%s\"", "%", "str", "(", "a", "[", "'plans_per_stage'", "]", "[", "stages", "[", "stage", "]", "]", "[", "'num_pool_per_axis'", "]", ")", ")", "\n", "f", ".", "write", "(", "\";%s\"", "%", "str", "(", "a", "[", "'plans_per_stage'", "]", "[", "stages", "[", "stage", "]", "]", "[", "'patch_size'", "]", ")", ")", "\n", "f", ".", "write", "(", "\";%s\"", "%", "list_to_string", "(", "patch_size_in_mm", ")", ")", "\n", "f", ".", "write", "(", "\";%s\"", "%", "str", "(", "a", "[", "'plans_per_stage'", "]", "[", "stages", "[", "stage", "]", "]", "[", "'median_patient_size_in_voxels'", "]", ")", ")", "\n", "f", ".", "write", "(", "\";%s\"", "%", "list_to_string", "(", "median_patient_size_in_mm", ")", ")", "\n", "f", ".", "write", "(", "\";%s\"", "%", "list_to_string", "(", "a", "[", "'plans_per_stage'", "]", "[", "stages", "[", "stage", "]", "]", "[", "'current_spacing'", "]", ")", ")", "\n", "f", ".", "write", "(", "\";%s\"", "%", "list_to_string", "(", "a", "[", "'plans_per_stage'", "]", "[", "stages", "[", "stage", "]", "]", "[", "'original_spacing'", "]", ")", ")", "\n", "f", ".", "write", "(", "\";%s\"", "%", "str", "(", "a", "[", "'plans_per_stage'", "]", "[", "stages", "[", "stage", "]", "]", "[", "'pool_op_kernel_sizes'", "]", ")", ")", "\n", "f", ".", "write", "(", "\";%s\"", "%", "str", "(", "a", "[", "'plans_per_stage'", "]", "[", "stages", "[", "stage", "]", "]", "[", "'conv_kernel_sizes'", "]", ")", ")", "\n", "if", "do_linebreak_at_end", ":", "\n", "        ", "f", ".", "write", "(", "\"\\n\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.model_selection.ensemble.merge": [[26, 37], ["isfile", "load_pickle", "numpy.mean", "nnunet.inference.segmentation_export.save_segmentation_nifti_from_softmax", "numpy.load", "numpy.load"], "function", ["home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.inference.segmentation_export.save_segmentation_nifti_from_softmax"], ["def", "merge", "(", "args", ")", ":", "\n", "    ", "file1", ",", "file2", ",", "properties_file", ",", "out_file", "=", "args", "\n", "if", "not", "isfile", "(", "out_file", ")", ":", "\n", "        ", "res1", "=", "np", ".", "load", "(", "file1", ")", "[", "'softmax'", "]", "\n", "res2", "=", "np", ".", "load", "(", "file2", ")", "[", "'softmax'", "]", "\n", "props", "=", "load_pickle", "(", "properties_file", ")", "\n", "mn", "=", "np", ".", "mean", "(", "(", "res1", ",", "res2", ")", ",", "0", ")", "\n", "# Softmax probabilities are already at target spacing so this will not do any resampling (resampling parameters", "\n", "# don't matter here)", "\n", "save_segmentation_nifti_from_softmax", "(", "mn", ",", "out_file", ",", "props", ",", "3", ",", "None", ",", "None", ",", "None", ",", "force_separate_z", "=", "None", ",", "\n", "interpolation_order_z", "=", "0", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.model_selection.ensemble.ensemble": [[39, 114], ["print", "join", "join", "load_pickle", "join", "multiprocessing.pool.Pool", "multiprocessing.pool.Pool.map", "multiprocessing.pool.Pool.close", "multiprocessing.pool.Pool.join", "join", "join", "join", "subfiles", "subfiles", "all", "all", "all", "all", "maybe_mkdir_p", "zip", "nnunet.evaluation.evaluator.aggregate_scores", "isfile", "nnunet.postprocessing.connected_components.determine_postprocessing", "join", "load_json", "save_json", "maybe_mkdir_p", "shutil.copy", "len", "len", "len", "len", "files1.append", "files2.append", "property_files.append", "out_files.append", "gt_segmentations.append", "isfile", "len", "tuple", "join", "join", "output_folder_base.split", "join", "join", "join", "subfiles", "subfiles", "all_patient_identifiers.append", "isfile", "isfile", "join", "join", "join", "join", "join", "zip", "join", "zip", "zip", "join", "join", "i.endswith", "i.endswith", "i.endswith", "i.endswith", "join", "output_folder_base.split", "output_folder_base.split"], "function", ["home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.evaluation.evaluator.aggregate_scores", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.postprocessing.connected_components.determine_postprocessing"], ["", "", "def", "ensemble", "(", "training_output_folder1", ",", "training_output_folder2", ",", "output_folder", ",", "task", ",", "validation_folder", ",", "folds", ")", ":", "\n", "    ", "print", "(", "\"\\nEnsembling folders\\n\"", ",", "training_output_folder1", ",", "\"\\n\"", ",", "training_output_folder2", ")", "\n", "\n", "output_folder_base", "=", "output_folder", "\n", "output_folder", "=", "join", "(", "output_folder_base", ",", "\"ensembled_raw\"", ")", "\n", "\n", "# only_keep_largest_connected_component is the same for all stages", "\n", "dataset_directory", "=", "join", "(", "preprocessing_output_dir", ",", "task", ")", "\n", "plans", "=", "load_pickle", "(", "join", "(", "training_output_folder1", ",", "\"plans.pkl\"", ")", ")", "# we need this only for the labels", "\n", "\n", "files1", "=", "[", "]", "\n", "files2", "=", "[", "]", "\n", "property_files", "=", "[", "]", "\n", "out_files", "=", "[", "]", "\n", "gt_segmentations", "=", "[", "]", "\n", "\n", "folder_with_gt_segs", "=", "join", "(", "dataset_directory", ",", "\"gt_segmentations\"", ")", "\n", "# in the correct shape and we need the original geometry to restore the niftis", "\n", "\n", "for", "f", "in", "folds", ":", "\n", "        ", "validation_folder_net1", "=", "join", "(", "training_output_folder1", ",", "\"fold_%d\"", "%", "f", ",", "validation_folder", ")", "\n", "validation_folder_net2", "=", "join", "(", "training_output_folder2", ",", "\"fold_%d\"", "%", "f", ",", "validation_folder", ")", "\n", "patient_identifiers1", "=", "subfiles", "(", "validation_folder_net1", ",", "False", ",", "None", ",", "'npz'", ",", "True", ")", "\n", "patient_identifiers2", "=", "subfiles", "(", "validation_folder_net2", ",", "False", ",", "None", ",", "'npz'", ",", "True", ")", "\n", "# we don't do postprocessing anymore so there should not be any of that noPostProcess", "\n", "patient_identifiers1_nii", "=", "[", "i", "for", "i", "in", "subfiles", "(", "validation_folder_net1", ",", "False", ",", "None", ",", "suffix", "=", "'nii.gz'", ",", "sort", "=", "True", ")", "if", "not", "i", ".", "endswith", "(", "\"noPostProcess.nii.gz\"", ")", "and", "not", "i", ".", "endswith", "(", "'_postprocessed.nii.gz'", ")", "]", "\n", "patient_identifiers2_nii", "=", "[", "i", "for", "i", "in", "subfiles", "(", "validation_folder_net2", ",", "False", ",", "None", ",", "suffix", "=", "'nii.gz'", ",", "sort", "=", "True", ")", "if", "not", "i", ".", "endswith", "(", "\"noPostProcess.nii.gz\"", ")", "and", "not", "i", ".", "endswith", "(", "'_postprocessed.nii.gz'", ")", "]", "\n", "assert", "len", "(", "patient_identifiers1", ")", "==", "len", "(", "patient_identifiers1_nii", ")", ",", "\"npz seem to be missing. run validation with --npz\"", "\n", "assert", "len", "(", "patient_identifiers1", ")", "==", "len", "(", "patient_identifiers1_nii", ")", ",", "\"npz seem to be missing. run validation with --npz\"", "\n", "assert", "all", "(", "[", "i", "[", ":", "-", "4", "]", "==", "j", "[", ":", "-", "7", "]", "for", "i", ",", "j", "in", "zip", "(", "patient_identifiers1", ",", "patient_identifiers1_nii", ")", "]", ")", ",", "\"npz seem to be missing. run validation with --npz\"", "\n", "assert", "all", "(", "[", "i", "[", ":", "-", "4", "]", "==", "j", "[", ":", "-", "7", "]", "for", "i", ",", "j", "in", "zip", "(", "patient_identifiers2", ",", "patient_identifiers2_nii", ")", "]", ")", ",", "\"npz seem to be missing. run validation with --npz\"", "\n", "\n", "all_patient_identifiers", "=", "patient_identifiers1", "\n", "for", "p", "in", "patient_identifiers2", ":", "\n", "            ", "if", "p", "not", "in", "all_patient_identifiers", ":", "\n", "                ", "all_patient_identifiers", ".", "append", "(", "p", ")", "\n", "\n", "# assert these patients exist for both methods", "\n", "", "", "assert", "all", "(", "[", "isfile", "(", "join", "(", "validation_folder_net1", ",", "i", ")", ")", "for", "i", "in", "all_patient_identifiers", "]", ")", "\n", "assert", "all", "(", "[", "isfile", "(", "join", "(", "validation_folder_net2", ",", "i", ")", ")", "for", "i", "in", "all_patient_identifiers", "]", ")", "\n", "\n", "maybe_mkdir_p", "(", "output_folder", ")", "\n", "\n", "for", "p", "in", "all_patient_identifiers", ":", "\n", "            ", "files1", ".", "append", "(", "join", "(", "validation_folder_net1", ",", "p", ")", ")", "\n", "files2", ".", "append", "(", "join", "(", "validation_folder_net2", ",", "p", ")", ")", "\n", "property_files", ".", "append", "(", "join", "(", "validation_folder_net1", ",", "p", ")", "[", ":", "-", "3", "]", "+", "\"pkl\"", ")", "\n", "out_files", ".", "append", "(", "join", "(", "output_folder", ",", "p", "[", ":", "-", "4", "]", "+", "\".nii.gz\"", ")", ")", "\n", "gt_segmentations", ".", "append", "(", "join", "(", "folder_with_gt_segs", ",", "p", "[", ":", "-", "4", "]", "+", "\".nii.gz\"", ")", ")", "\n", "\n", "", "", "p", "=", "Pool", "(", "default_num_threads", ")", "\n", "p", ".", "map", "(", "merge", ",", "zip", "(", "files1", ",", "files2", ",", "property_files", ",", "out_files", ")", ")", "\n", "p", ".", "close", "(", ")", "\n", "p", ".", "join", "(", ")", "\n", "\n", "if", "not", "isfile", "(", "join", "(", "output_folder", ",", "\"summary.json\"", ")", ")", "and", "len", "(", "out_files", ")", ">", "0", ":", "\n", "        ", "aggregate_scores", "(", "tuple", "(", "zip", "(", "out_files", ",", "gt_segmentations", ")", ")", ",", "labels", "=", "plans", "[", "'all_classes'", "]", ",", "\n", "json_output_file", "=", "join", "(", "output_folder", ",", "\"summary.json\"", ")", ",", "json_task", "=", "task", ",", "\n", "json_name", "=", "task", "+", "\"__\"", "+", "output_folder_base", ".", "split", "(", "\"/\"", ")", "[", "-", "1", "]", ",", "num_threads", "=", "default_num_threads", ")", "\n", "\n", "", "if", "not", "isfile", "(", "join", "(", "output_folder_base", ",", "\"postprocessing.json\"", ")", ")", ":", "\n", "# now lets also look at postprocessing. We cannot just take what we determined in cross-validation and apply it", "\n", "# here because things may have changed and may also be too inconsistent between the two networks", "\n", "        ", "determine_postprocessing", "(", "output_folder_base", ",", "folder_with_gt_segs", ",", "\"ensembled_raw\"", ",", "\"temp\"", ",", "\n", "\"ensembled_postprocessed\"", ",", "default_num_threads", ",", "dice_threshold", "=", "0", ")", "\n", "\n", "out_dir_all_json", "=", "join", "(", "network_training_output_dir", ",", "\"summary_jsons\"", ")", "\n", "json_out", "=", "load_json", "(", "join", "(", "output_folder_base", ",", "\"ensembled_postprocessed\"", ",", "\"summary.json\"", ")", ")", "\n", "\n", "json_out", "[", "\"experiment_name\"", "]", "=", "output_folder_base", ".", "split", "(", "\"/\"", ")", "[", "-", "1", "]", "\n", "save_json", "(", "json_out", ",", "join", "(", "output_folder_base", ",", "\"ensembled_postprocessed\"", ",", "\"summary.json\"", ")", ")", "\n", "\n", "maybe_mkdir_p", "(", "out_dir_all_json", ")", "\n", "shutil", ".", "copy", "(", "join", "(", "output_folder_base", ",", "\"ensembled_postprocessed\"", ",", "\"summary.json\"", ")", ",", "\n", "join", "(", "out_dir_all_json", ",", "\"%s__%s.json\"", "%", "(", "task", ",", "output_folder_base", ".", "split", "(", "\"/\"", ")", "[", "-", "1", "]", ")", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.utilities.file_conversions.convert_2d_image_to_nifti": [[8, 54], ["skimage.io.imread", "enumerate", "transform", "len", "img.transpose.transpose", "SimpleITK.GetImageFromArray", "sitk.GetImageFromArray.SetSpacing", "len", "str", "SimpleITK.WriteImage", "SimpleITK.WriteImage", "list"], "function", ["None"], ["def", "convert_2d_image_to_nifti", "(", "filename", ":", "str", ",", "output_name", ":", "str", ",", "spacing", "=", "(", "999", ",", "1", ",", "1", ")", ",", "transform", "=", "None", ",", "is_seg", ":", "bool", "=", "False", ")", "->", "None", ":", "\n", "    ", "\"\"\"\n    Reads an image (must be a format that it recognized by skimage.io.imread) and converts it into a series of niftis.\n    The image can have an arbitrary number of input channels which will be exported separately (_0000.nii.gz,\n    _0001.nii.gz, etc for images and only .nii.gz for seg).\n    Spacing can be ignored most of the time.\n    !!!2D images are often natural images which do not have a voxel spacing that could be used for resampling. These images\n    must be resampled by you prior to converting them to nifti!!!\n\n    Datasets converted with this utility can only be used with the 2d U-Net configuration of nnU-Net\n\n    If Transform is not None it will be applied to the image after loading.\n\n    :param is_seg:\n    :param transform:\n    :param filename:\n    :param output_name: do not use a file ending for this one! Example: output_name='./converted/image1'. This\n    function will add the suffix (_0000) and file ending (.nii.gz) for you.\n    :param spacing:\n    :return:\n    \"\"\"", "\n", "img", "=", "io", ".", "imread", "(", "filename", ")", "\n", "\n", "if", "transform", "is", "not", "None", ":", "\n", "        ", "img", "=", "transform", "(", "img", ")", "\n", "\n", "", "if", "len", "(", "img", ".", "shape", ")", "==", "2", ":", "# 2d image with no color channels", "\n", "        ", "img", "=", "img", "[", "None", ",", "None", "]", "# add dimensions", "\n", "", "else", ":", "\n", "        ", "assert", "len", "(", "img", ".", "shape", ")", "==", "3", ",", "\"image should be 3d with color channel last but has shape %s\"", "%", "str", "(", "img", ".", "shape", ")", "\n", "# we assume that the color channel is the last dimension. Transpose it to be in first", "\n", "img", "=", "img", ".", "transpose", "(", "(", "2", ",", "0", ",", "1", ")", ")", "\n", "# add third dimension", "\n", "img", "=", "img", "[", ":", ",", "None", "]", "\n", "\n", "# image is now (c, x, x, z) where x=1 since it's 2d", "\n", "", "if", "is_seg", ":", "\n", "        ", "assert", "img", ".", "shape", "[", "0", "]", "==", "1", ",", "'segmentations can only have one color channel, not sure what happened here'", "\n", "\n", "", "for", "j", ",", "i", "in", "enumerate", "(", "img", ")", ":", "\n", "        ", "itk_img", "=", "sitk", ".", "GetImageFromArray", "(", "i", ")", "\n", "itk_img", ".", "SetSpacing", "(", "list", "(", "spacing", ")", "[", ":", ":", "-", "1", "]", ")", "\n", "if", "not", "is_seg", ":", "\n", "            ", "sitk", ".", "WriteImage", "(", "itk_img", ",", "output_name", "+", "\"_%04.0d.nii.gz\"", "%", "j", ")", "\n", "", "else", ":", "\n", "            ", "sitk", ".", "WriteImage", "(", "itk_img", ",", "output_name", "+", "\".nii.gz\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.utilities.file_conversions.convert_3d_tiff_to_nifti": [[56, 90], ["enumerate", "tifffile.imread", "SimpleITK.GetImageFromArray", "sitk.GetImageFromArray.SetSpacing", "len", "transform", "SimpleITK.WriteImage", "SimpleITK.WriteImage", "list"], "function", ["None"], ["", "", "", "def", "convert_3d_tiff_to_nifti", "(", "filenames", ":", "List", "[", "str", "]", ",", "output_name", ":", "str", ",", "spacing", ":", "Tuple", "[", "tuple", ",", "list", "]", ",", "transform", "=", "None", ",", "is_seg", "=", "False", ")", "->", "None", ":", "\n", "    ", "\"\"\"\n    filenames must be a list of strings, each pointing to a separate 3d tiff file. One file per modality. If your data\n    only has one imaging modality, simply pass a list with only a single entry\n\n    Files in filenames must be readable with\n\n    Note: we always only pass one file into tifffile.imread, not multiple (even though it supports it). This is because\n    I am not familiar enough with this functionality and would like to have control over what happens.\n\n    If Transform is not None it will be applied to the image after loading.\n\n    :param transform:\n    :param filenames:\n    :param output_name:\n    :param spacing:\n    :return:\n    \"\"\"", "\n", "if", "is_seg", ":", "\n", "        ", "assert", "len", "(", "filenames", ")", "==", "1", "\n", "\n", "", "for", "j", ",", "i", "in", "enumerate", "(", "filenames", ")", ":", "\n", "        ", "img", "=", "tifffile", ".", "imread", "(", "i", ")", "\n", "\n", "if", "transform", "is", "not", "None", ":", "\n", "            ", "img", "=", "transform", "(", "img", ")", "\n", "\n", "", "itk_img", "=", "sitk", ".", "GetImageFromArray", "(", "img", ")", "\n", "itk_img", ".", "SetSpacing", "(", "list", "(", "spacing", ")", "[", ":", ":", "-", "1", "]", ")", "\n", "\n", "if", "not", "is_seg", ":", "\n", "            ", "sitk", ".", "WriteImage", "(", "itk_img", ",", "output_name", "+", "\"_%04.0d.nii.gz\"", "%", "j", ")", "\n", "", "else", ":", "\n", "            ", "sitk", ".", "WriteImage", "(", "itk_img", ",", "output_name", "+", "\".nii.gz\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.utilities.file_conversions.convert_2d_segmentation_nifti_to_img": [[92, 100], ["SimpleITK.GetArrayFromImage", "skimage.io.imsave", "SimpleITK.ReadImage", "transform", "transform.astype"], "function", ["None"], ["", "", "", "def", "convert_2d_segmentation_nifti_to_img", "(", "nifti_file", ":", "str", ",", "output_filename", ":", "str", ",", "transform", "=", "None", ",", "export_dtype", "=", "np", ".", "uint8", ")", ":", "\n", "    ", "img", "=", "sitk", ".", "GetArrayFromImage", "(", "sitk", ".", "ReadImage", "(", "nifti_file", ")", ")", "\n", "assert", "img", ".", "shape", "[", "0", "]", "==", "1", ",", "\"This function can only export 2D segmentations!\"", "\n", "img", "=", "img", "[", "0", "]", "\n", "if", "transform", "is", "not", "None", ":", "\n", "        ", "img", "=", "transform", "(", "img", ")", "\n", "\n", "", "io", ".", "imsave", "(", "output_filename", ",", "img", ".", "astype", "(", "export_dtype", ")", ",", "check_contrast", "=", "False", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.utilities.file_conversions.convert_3d_segmentation_nifti_to_tiff": [[102, 109], ["SimpleITK.GetArrayFromImage", "tifffile.imsave", "SimpleITK.ReadImage", "len", "transform", "transform.astype"], "function", ["None"], ["", "def", "convert_3d_segmentation_nifti_to_tiff", "(", "nifti_file", ":", "str", ",", "output_filename", ":", "str", ",", "transform", "=", "None", ",", "export_dtype", "=", "np", ".", "uint8", ")", ":", "\n", "    ", "img", "=", "sitk", ".", "GetArrayFromImage", "(", "sitk", ".", "ReadImage", "(", "nifti_file", ")", ")", "\n", "assert", "len", "(", "img", ".", "shape", ")", "==", "3", ",", "\"This function can only export 3D segmentations!\"", "\n", "if", "transform", "is", "not", "None", ":", "\n", "        ", "img", "=", "transform", "(", "img", ")", "\n", "\n", "", "tifffile", ".", "imsave", "(", "output_filename", ",", "img", ".", "astype", "(", "export_dtype", ")", ")", "\n", "", ""]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.utilities.distributed.awesome_allgather_function.forward": [[28, 37], ["torch.distributed.get_world_size", "torch.distributed.all_gather", "torch.cat", "torch.empty_like", "range"], "methods", ["None"], ["    ", "@", "staticmethod", "\n", "def", "forward", "(", "ctx", ",", "input", ")", ":", "\n", "        ", "world_size", "=", "distributed", ".", "get_world_size", "(", ")", "\n", "# create a destination list for the allgather.  I'm assuming you're gathering from 3 workers.", "\n", "allgather_list", "=", "[", "torch", ".", "empty_like", "(", "input", ")", "for", "_", "in", "range", "(", "world_size", ")", "]", "\n", "#if distributed.get_rank() == 0:", "\n", "#    import IPython;IPython.embed()", "\n", "distributed", ".", "all_gather", "(", "allgather_list", ",", "input", ")", "\n", "return", "torch", ".", "cat", "(", "allgather_list", ",", "dim", "=", "0", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.utilities.distributed.awesome_allgather_function.backward": [[38, 49], ["torch.distributed.get_rank", "slice", "torch.distributed.get_world_size"], "methods", ["None"], ["", "@", "staticmethod", "\n", "def", "backward", "(", "ctx", ",", "grad_output", ")", ":", "\n", "#print_if_rank0(\"backward grad_output len\", len(grad_output))", "\n", "#print_if_rank0(\"backward grad_output shape\", grad_output.shape)", "\n", "        ", "grads_per_rank", "=", "grad_output", ".", "shape", "[", "0", "]", "//", "distributed", ".", "get_world_size", "(", ")", "\n", "rank", "=", "distributed", ".", "get_rank", "(", ")", "\n", "# We'll receive gradients for the entire catted forward output, so to mimic DataParallel,", "\n", "# return only the slice that corresponds to this process's input:", "\n", "sl", "=", "slice", "(", "rank", "*", "grads_per_rank", ",", "(", "rank", "+", "1", ")", "*", "grads_per_rank", ")", "\n", "#print(\"worker\", rank, \"backward slice\", sl)", "\n", "return", "grad_output", "[", "sl", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.utilities.distributed.print_if_rank0": [[22, 25], ["torch.distributed.get_rank", "print"], "function", ["None"], ["def", "print_if_rank0", "(", "*", "args", ")", ":", "\n", "    ", "if", "distributed", ".", "get_rank", "(", ")", "==", "0", ":", "\n", "        ", "print", "(", "*", "args", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.utilities.random_stuff.no_op.__enter__": [[17, 19], ["None"], "methods", ["None"], ["    ", "def", "__enter__", "(", "self", ")", ":", "\n", "        ", "pass", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.utilities.random_stuff.no_op.__exit__": [[20, 22], ["None"], "methods", ["None"], ["", "def", "__exit__", "(", "self", ",", "*", "args", ")", ":", "\n", "        ", "pass", "\n", "", "", ""]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.utilities.recursive_delete_npz.recursive_delete_npz": [[21, 28], ["subfiles", "subdirs", "os.remove", "recursive_delete_npz.recursive_delete_npz", "i.endswith", "join"], "function", ["home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.utilities.recursive_delete_npz.recursive_delete_npz"], ["def", "recursive_delete_npz", "(", "current_directory", ":", "str", ")", ":", "\n", "    ", "npz_files", "=", "subfiles", "(", "current_directory", ",", "join", "=", "True", ",", "suffix", "=", "\".npz\"", ")", "\n", "npz_files", "=", "[", "i", "for", "i", "in", "npz_files", "if", "not", "i", ".", "endswith", "(", "\"segFromPrevStage.npz\"", ")", "]", "# to be extra safe", "\n", "_", "=", "[", "os", ".", "remove", "(", "i", ")", "for", "i", "in", "npz_files", "]", "\n", "for", "d", "in", "subdirs", "(", "current_directory", ",", "join", "=", "False", ")", ":", "\n", "        ", "if", "d", "!=", "\"pred_next_stage\"", ":", "\n", "            ", "recursive_delete_npz", "(", "join", "(", "current_directory", ",", "d", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.utilities.file_endings.remove_trailing_slash": [[19, 23], ["filename.endswith"], "function", ["None"], ["def", "remove_trailing_slash", "(", "filename", ":", "str", ")", ":", "\n", "    ", "while", "filename", ".", "endswith", "(", "'/'", ")", ":", "\n", "        ", "filename", "=", "filename", "[", ":", "-", "1", "]", "\n", "", "return", "filename", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.utilities.file_endings.maybe_add_0000_to_all_niigz": [[25, 31], ["subfiles", "file_endings.remove_trailing_slash", "remove_trailing_slash.endswith", "os.rename"], "function", ["home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.utilities.file_endings.remove_trailing_slash"], ["", "def", "maybe_add_0000_to_all_niigz", "(", "folder", ")", ":", "\n", "    ", "nii_gz", "=", "subfiles", "(", "folder", ",", "suffix", "=", "'.nii.gz'", ")", "\n", "for", "n", "in", "nii_gz", ":", "\n", "        ", "n", "=", "remove_trailing_slash", "(", "n", ")", "\n", "if", "not", "n", ".", "endswith", "(", "'_0000.nii.gz'", ")", ":", "\n", "            ", "os", ".", "rename", "(", "n", ",", "n", "[", ":", "-", "7", "]", "+", "'_0000.nii.gz'", ")", "\n", "", "", "", ""]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.utilities.to_torch.maybe_to_torch": [[18, 24], ["isinstance", "isinstance", "torch.from_numpy().float", "to_torch.maybe_to_torch", "isinstance", "torch.from_numpy"], "function", ["home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.utilities.to_torch.maybe_to_torch"], ["def", "maybe_to_torch", "(", "d", ")", ":", "\n", "    ", "if", "isinstance", "(", "d", ",", "list", ")", ":", "\n", "        ", "d", "=", "[", "maybe_to_torch", "(", "i", ")", "if", "not", "isinstance", "(", "i", ",", "torch", ".", "Tensor", ")", "else", "i", "for", "i", "in", "d", "]", "\n", "", "elif", "not", "isinstance", "(", "d", ",", "torch", ".", "Tensor", ")", ":", "\n", "        ", "d", "=", "torch", ".", "from_numpy", "(", "d", ")", ".", "float", "(", ")", "\n", "", "return", "d", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.utilities.to_torch.to_cuda": [[26, 32], ["isinstance", "data.cuda.cuda", "i.cuda"], "function", ["None"], ["", "def", "to_cuda", "(", "data", ",", "non_blocking", "=", "True", ",", "gpu_id", "=", "0", ")", ":", "\n", "    ", "if", "isinstance", "(", "data", ",", "list", ")", ":", "\n", "        ", "data", "=", "[", "i", ".", "cuda", "(", "gpu_id", ",", "non_blocking", "=", "non_blocking", ")", "for", "i", "in", "data", "]", "\n", "", "else", ":", "\n", "        ", "data", "=", "data", ".", "cuda", "(", "gpu_id", ",", "non_blocking", "=", "True", ")", "\n", "", "return", "data", "\n", "", ""]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.utilities.tensor_utilities.sum_tensor": [[20, 29], ["numpy.unique().astype", "sorted", "numpy.unique", "inp.sum.sum", "inp.sum.sum", "int", "int"], "function", ["None"], ["def", "sum_tensor", "(", "inp", ",", "axes", ",", "keepdim", "=", "False", ")", ":", "\n", "    ", "axes", "=", "np", ".", "unique", "(", "axes", ")", ".", "astype", "(", "int", ")", "\n", "if", "keepdim", ":", "\n", "        ", "for", "ax", "in", "axes", ":", "\n", "            ", "inp", "=", "inp", ".", "sum", "(", "int", "(", "ax", ")", ",", "keepdim", "=", "True", ")", "\n", "", "", "else", ":", "\n", "        ", "for", "ax", "in", "sorted", "(", "axes", ",", "reverse", "=", "True", ")", ":", "\n", "            ", "inp", "=", "inp", ".", "sum", "(", "int", "(", "ax", ")", ")", "\n", "", "", "return", "inp", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.utilities.tensor_utilities.mean_tensor": [[31, 40], ["numpy.unique().astype", "sorted", "numpy.unique", "inp.mean.mean", "inp.mean.mean", "int", "int"], "function", ["None"], ["", "def", "mean_tensor", "(", "inp", ",", "axes", ",", "keepdim", "=", "False", ")", ":", "\n", "    ", "axes", "=", "np", ".", "unique", "(", "axes", ")", ".", "astype", "(", "int", ")", "\n", "if", "keepdim", ":", "\n", "        ", "for", "ax", "in", "axes", ":", "\n", "            ", "inp", "=", "inp", ".", "mean", "(", "int", "(", "ax", ")", ",", "keepdim", "=", "True", ")", "\n", "", "", "else", ":", "\n", "        ", "for", "ax", "in", "sorted", "(", "axes", ",", "reverse", "=", "True", ")", ":", "\n", "            ", "inp", "=", "inp", ".", "mean", "(", "int", "(", "ax", ")", ")", "\n", "", "", "return", "inp", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.utilities.tensor_utilities.flip": [[42, 53], ["torch.arange", "x.dim", "slice", "x.size", "tuple"], "function", ["None"], ["", "def", "flip", "(", "x", ",", "dim", ")", ":", "\n", "    ", "\"\"\"\n    flips the tensor at dimension dim (mirroring!)\n    :param x:\n    :param dim:\n    :return:\n    \"\"\"", "\n", "indices", "=", "[", "slice", "(", "None", ")", "]", "*", "x", ".", "dim", "(", ")", "\n", "indices", "[", "dim", "]", "=", "torch", ".", "arange", "(", "x", ".", "size", "(", "dim", ")", "-", "1", ",", "-", "1", ",", "-", "1", ",", "\n", "dtype", "=", "torch", ".", "long", ",", "device", "=", "x", ".", "device", ")", "\n", "return", "x", "[", "tuple", "(", "indices", ")", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.utilities.one_hot_encoding.to_one_hot": [[18, 25], ["numpy.zeros", "enumerate", "numpy.unique", "len"], "function", ["None"], ["def", "to_one_hot", "(", "seg", ",", "all_seg_labels", "=", "None", ")", ":", "\n", "    ", "if", "all_seg_labels", "is", "None", ":", "\n", "        ", "all_seg_labels", "=", "np", ".", "unique", "(", "seg", ")", "\n", "", "result", "=", "np", ".", "zeros", "(", "(", "len", "(", "all_seg_labels", ")", ",", "*", "seg", ".", "shape", ")", ",", "dtype", "=", "seg", ".", "dtype", ")", "\n", "for", "i", ",", "l", "in", "enumerate", "(", "all_seg_labels", ")", ":", "\n", "        ", "result", "[", "i", "]", "[", "seg", "==", "l", "]", "=", "1", "\n", "", "return", "result", "\n", "", ""]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.utilities.folder_names.get_output_folder_name": [[20, 48], ["join", "join", "join", "join"], "function", ["None"], ["def", "get_output_folder_name", "(", "model", ":", "str", ",", "task", ":", "str", "=", "None", ",", "trainer", ":", "str", "=", "None", ",", "plans", ":", "str", "=", "None", ",", "fold", ":", "int", "=", "None", ",", "\n", "overwrite_training_output_dir", ":", "str", "=", "None", ")", ":", "\n", "    ", "\"\"\"\n    Retrieves the correct output directory for the nnU-Net model described by the input parameters\n\n    :param model:\n    :param task:\n    :param trainer:\n    :param plans:\n    :param fold:\n    :param overwrite_training_output_dir:\n    :return:\n    \"\"\"", "\n", "assert", "model", "in", "[", "\"2d\"", ",", "\"3d_cascade_fullres\"", ",", "'3d_fullres'", ",", "'3d_lowres'", "]", "\n", "\n", "if", "overwrite_training_output_dir", "is", "not", "None", ":", "\n", "        ", "tr_dir", "=", "overwrite_training_output_dir", "\n", "", "else", ":", "\n", "        ", "tr_dir", "=", "network_training_output_dir", "\n", "\n", "", "current", "=", "join", "(", "tr_dir", ",", "model", ")", "\n", "if", "task", "is", "not", "None", ":", "\n", "        ", "current", "=", "join", "(", "current", ",", "task", ")", "\n", "if", "trainer", "is", "not", "None", "and", "plans", "is", "not", "None", ":", "\n", "            ", "current", "=", "join", "(", "current", ",", "trainer", "+", "\"__\"", "+", "plans", ")", "\n", "if", "fold", "is", "not", "None", ":", "\n", "                ", "current", "=", "join", "(", "current", ",", "\"fold_%d\"", "%", "fold", ")", "\n", "", "", "", "return", "current", "\n", "", ""]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.utilities.sitk_stuff.copy_geometry": [[19, 24], ["image.SetOrigin", "image.SetDirection", "image.SetSpacing", "ref.GetOrigin", "ref.GetDirection", "ref.GetSpacing"], "function", ["None"], ["def", "copy_geometry", "(", "image", ":", "sitk", ".", "Image", ",", "ref", ":", "sitk", ".", "Image", ")", ":", "\n", "    ", "image", ".", "SetOrigin", "(", "ref", ".", "GetOrigin", "(", ")", ")", "\n", "image", ".", "SetDirection", "(", "ref", ".", "GetDirection", "(", ")", ")", "\n", "image", ".", "SetSpacing", "(", "ref", ".", "GetSpacing", "(", ")", ")", "\n", "return", "image", "\n", "", ""]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.utilities.task_name_id_conversion.convert_id_to_task_name": [[21, 55], ["numpy.unique", "subdirs", "subdirs", "subdirs", "len", "RuntimeError", "len", "RuntimeError", "isdir", "join", "subdirs", "join"], "function", ["None"], ["def", "convert_id_to_task_name", "(", "task_id", ":", "int", ")", ":", "\n", "    ", "startswith", "=", "\"Task%03.0d\"", "%", "task_id", "\n", "if", "preprocessing_output_dir", "is", "not", "None", ":", "\n", "        ", "candidates_preprocessed", "=", "subdirs", "(", "preprocessing_output_dir", ",", "prefix", "=", "startswith", ",", "join", "=", "False", ")", "\n", "", "else", ":", "\n", "        ", "candidates_preprocessed", "=", "[", "]", "\n", "\n", "", "if", "nnUNet_raw_data", "is", "not", "None", ":", "\n", "        ", "candidates_raw", "=", "subdirs", "(", "nnUNet_raw_data", ",", "prefix", "=", "startswith", ",", "join", "=", "False", ")", "\n", "", "else", ":", "\n", "        ", "candidates_raw", "=", "[", "]", "\n", "\n", "", "if", "nnUNet_cropped_data", "is", "not", "None", ":", "\n", "        ", "candidates_cropped", "=", "subdirs", "(", "nnUNet_cropped_data", ",", "prefix", "=", "startswith", ",", "join", "=", "False", ")", "\n", "", "else", ":", "\n", "        ", "candidates_cropped", "=", "[", "]", "\n", "\n", "", "candidates_trained_models", "=", "[", "]", "\n", "if", "network_training_output_dir", "is", "not", "None", ":", "\n", "        ", "for", "m", "in", "[", "'2d'", ",", "'3d_lowres'", ",", "'3d_fullres'", ",", "'3d_cascade_fullres'", "]", ":", "\n", "            ", "if", "isdir", "(", "join", "(", "network_training_output_dir", ",", "m", ")", ")", ":", "\n", "                ", "candidates_trained_models", "+=", "subdirs", "(", "join", "(", "network_training_output_dir", ",", "m", ")", ",", "prefix", "=", "startswith", ",", "join", "=", "False", ")", "\n", "\n", "", "", "", "all_candidates", "=", "candidates_cropped", "+", "candidates_preprocessed", "+", "candidates_raw", "+", "candidates_trained_models", "\n", "unique_candidates", "=", "np", ".", "unique", "(", "all_candidates", ")", "\n", "if", "len", "(", "unique_candidates", ")", ">", "1", ":", "\n", "        ", "raise", "RuntimeError", "(", "\"More than one task name found for task id %d. Please correct that. (I looked in the \"", "\n", "\"following folders:\\n%s\\n%s\\n%s\"", "%", "(", "task_id", ",", "nnUNet_raw_data", ",", "preprocessing_output_dir", ",", "\n", "nnUNet_cropped_data", ")", ")", "\n", "", "if", "len", "(", "unique_candidates", ")", "==", "0", ":", "\n", "        ", "raise", "RuntimeError", "(", "\"Could not find a task with the ID %d. Make sure the requested task ID exists and that \"", "\n", "\"nnU-Net knows where raw and preprocessed data are located (see Documentation - \"", "\n", "\"Installation).\"", "%", "task_id", ")", "\n", "", "return", "unique_candidates", "[", "0", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.utilities.task_name_id_conversion.convert_task_name_to_id": [[57, 61], ["task_name.startswith", "int"], "function", ["None"], ["", "def", "convert_task_name_to_id", "(", "task_name", ":", "str", ")", ":", "\n", "    ", "assert", "task_name", ".", "startswith", "(", "\"Task\"", ")", "\n", "task_id", "=", "int", "(", "task_name", "[", "4", ":", "7", "]", ")", "\n", "return", "task_id", "\n", "", ""]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.utilities.recursive_rename_taskXX_to_taskXXX.recursive_rename": [[20, 30], ["subdirs", "subdirs", "recursive_rename_taskXX_to_taskXXX.recursive_rename", "ss.startswith", "int", "os.rename", "ss.find", "join", "join"], "function", ["home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.utilities.recursive_rename_taskXX_to_taskXXX.recursive_rename"], ["def", "recursive_rename", "(", "folder", ")", ":", "\n", "    ", "s", "=", "subdirs", "(", "folder", ",", "join", "=", "False", ")", "\n", "for", "ss", "in", "s", ":", "\n", "        ", "if", "ss", ".", "startswith", "(", "\"Task\"", ")", "and", "ss", ".", "find", "(", "\"_\"", ")", "==", "6", ":", "\n", "            ", "task_id", "=", "int", "(", "ss", "[", "4", ":", "6", "]", ")", "\n", "name", "=", "ss", "[", "7", ":", "]", "\n", "os", ".", "rename", "(", "join", "(", "folder", ",", "ss", ")", ",", "join", "(", "folder", ",", "\"Task%03.0d_\"", "%", "task_id", "+", "name", ")", ")", "\n", "", "", "s", "=", "subdirs", "(", "folder", ",", "join", "=", "True", ")", "\n", "for", "ss", "in", "s", ":", "\n", "        ", "recursive_rename", "(", "ss", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.keras.BRATS2013_application.setup_config.__init__": [[110, 143], ["None"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "model", "=", "\"\"", ",", "\n", "backbone", "=", "\"\"", ",", "\n", "init", "=", "\"\"", ",", "\n", "data_augmentation", "=", "True", ",", "\n", "input_rows", "=", "256", ",", "\n", "input_cols", "=", "256", ",", "\n", "input_deps", "=", "3", ",", "\n", "batch_size", "=", "64", ",", "\n", "verbose", "=", "1", ",", "\n", "decoder_block_type", "=", "None", ",", "\n", "nb_class", "=", "None", ",", "\n", "DATA_DIR", "=", "'Data/BRATS'", ",", "\n", ")", ":", "\n", "        ", "self", ".", "model", "=", "model", "\n", "self", ".", "backbone", "=", "backbone", "\n", "self", ".", "init", "=", "init", "\n", "self", ".", "exp_name", "=", "model", "+", "\"-\"", "+", "backbone", "+", "\"-\"", "+", "init", "\n", "self", ".", "data_augmentation", "=", "data_augmentation", "\n", "self", ".", "input_rows", ",", "self", ".", "input_cols", "=", "input_rows", ",", "input_cols", "\n", "self", ".", "input_deps", "=", "input_deps", "\n", "self", ".", "batch_size", "=", "batch_size", "\n", "self", ".", "verbose", "=", "verbose", "\n", "self", ".", "decoder_block_type", "=", "decoder_block_type", "\n", "self", ".", "nb_class", "=", "nb_class", "\n", "self", ".", "DATA_DIR", "=", "DATA_DIR", "\n", "if", "nb_class", ">", "1", ":", "\n", "            ", "self", ".", "activation", "=", "\"softmax\"", "\n", "", "else", ":", "\n", "            ", "self", ".", "activation", "=", "\"sigmoid\"", "\n", "", "if", "self", ".", "init", "!=", "\"finetune\"", ":", "\n", "            ", "self", ".", "weights", "=", "None", "\n", "", "else", ":", "\n", "            ", "self", ".", "weights", "=", "\"imagenet\"", "\n", "", "", "def", "display", "(", "self", ")", ":", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.keras.BRATS2013_application.setup_config.display": [[143, 150], ["print", "dir", "print", "print", "a.startswith", "callable", "getattr", "getattr"], "methods", ["None"], ["", "", "def", "display", "(", "self", ")", ":", "\n", "        ", "\"\"\"Display Configuration values.\"\"\"", "\n", "print", "(", "\"\\nConfigurations:\"", ")", "\n", "for", "a", "in", "dir", "(", "self", ")", ":", "\n", "            ", "if", "not", "a", ".", "startswith", "(", "\"__\"", ")", "and", "not", "callable", "(", "getattr", "(", "self", ",", "a", ")", ")", "and", "\"ids\"", "not", "in", "a", ":", "\n", "                ", "print", "(", "\"{:30} {}\"", ".", "format", "(", "a", ",", "getattr", "(", "self", ",", "a", ")", ")", ")", "\n", "", "", "print", "(", "\"\\n\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.keras.helper_functions.mean_iou": [[25, 35], ["numpy.arange", "keras.backend.mean", "tensorflow.to_int32", "tensorflow.metrics.mean_iou", "keras.backend.get_session().run", "prec.append", "keras.backend.stack", "tensorflow.local_variables_initializer", "tensorflow.control_dependencies", "tensorflow.identity", "keras.backend.get_session"], "function", ["home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.keras.helper_functions.mean_iou", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.preprocessing.preprocessing.PreprocessorFor2D.run"], ["def", "mean_iou", "(", "y_true", ",", "y_pred", ")", ":", "\n", "    ", "prec", "=", "[", "]", "\n", "for", "t", "in", "np", ".", "arange", "(", "0.5", ",", "1.0", ",", "0.05", ")", ":", "\n", "        ", "y_pred_", "=", "tf", ".", "to_int32", "(", "y_pred", ">", "t", ")", "\n", "score", ",", "up_opt", "=", "tf", ".", "metrics", ".", "mean_iou", "(", "y_true", ",", "y_pred_", ",", "2", ")", "\n", "K", ".", "get_session", "(", ")", ".", "run", "(", "tf", ".", "local_variables_initializer", "(", ")", ")", "\n", "with", "tf", ".", "control_dependencies", "(", "[", "up_opt", "]", ")", ":", "\n", "            ", "score", "=", "tf", ".", "identity", "(", "score", ")", "\n", "", "prec", ".", "append", "(", "score", ")", "\n", "", "return", "K", ".", "mean", "(", "K", ".", "stack", "(", "prec", ")", ",", "axis", "=", "0", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.keras.helper_functions.dice_coef": [[37, 43], ["keras.backend.flatten", "keras.backend.flatten", "keras.backend.sum", "keras.backend.sum", "keras.backend.sum"], "function", ["None"], ["", "def", "dice_coef", "(", "y_true", ",", "y_pred", ")", ":", "\n", "    ", "smooth", "=", "1.", "\n", "y_true_f", "=", "K", ".", "flatten", "(", "y_true", ")", "\n", "y_pred_f", "=", "K", ".", "flatten", "(", "y_pred", ")", "\n", "intersection", "=", "K", ".", "sum", "(", "y_true_f", "*", "y_pred_f", ")", "\n", "return", "(", "2.", "*", "intersection", "+", "smooth", ")", "/", "(", "K", ".", "sum", "(", "y_true_f", ")", "+", "K", ".", "sum", "(", "y_pred_f", ")", "+", "smooth", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.keras.helper_functions.dice_coef_loss": [[44, 46], ["helper_functions.dice_coef"], "function", ["home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.keras.helper_functions.dice_coef"], ["", "def", "dice_coef_loss", "(", "y_true", ",", "y_pred", ")", ":", "\n", "    ", "return", "1.", "-", "dice_coef", "(", "y_true", ",", "y_pred", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.keras.helper_functions.bce_dice_loss": [[47, 49], ["helper_functions.dice_coef", "keras.losses.binary_crossentropy"], "function", ["home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.keras.helper_functions.dice_coef"], ["", "def", "bce_dice_loss", "(", "y_true", ",", "y_pred", ")", ":", "\n", "    ", "return", "0.5", "*", "keras", ".", "losses", ".", "binary_crossentropy", "(", "y_true", ",", "y_pred", ")", "-", "dice_coef", "(", "y_true", ",", "y_pred", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.keras.helper_functions.compute_iou": [[51, 55], ["overlap.sum", "float", "union.sum"], "function", ["None"], ["", "def", "compute_iou", "(", "im1", ",", "im2", ")", ":", "\n", "    ", "overlap", "=", "(", "im1", ">", "0.5", ")", "*", "(", "im2", ">", "0.5", ")", "\n", "union", "=", "(", "im1", ">", "0.5", ")", "+", "(", "im2", ">", "0.5", ")", "\n", "return", "overlap", ".", "sum", "(", ")", "/", "float", "(", "union", ".", "sum", "(", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.keras.helper_functions.compute_dice": [[57, 71], ["numpy.asarray().astype", "numpy.asarray().astype", "numpy.logical_and", "ValueError", "np.asarray().astype.sum", "np.asarray().astype.sum", "numpy.asarray", "numpy.asarray", "np.logical_and.sum"], "function", ["None"], ["", "def", "compute_dice", "(", "im1", ",", "im2", ",", "empty_score", "=", "1.0", ")", ":", "\n", "    ", "im1", "=", "np", ".", "asarray", "(", "im1", ">", "0.5", ")", ".", "astype", "(", "np", ".", "bool", ")", "\n", "im2", "=", "np", ".", "asarray", "(", "im2", ">", "0.5", ")", ".", "astype", "(", "np", ".", "bool", ")", "\n", "\n", "if", "im1", ".", "shape", "!=", "im2", ".", "shape", ":", "\n", "        ", "raise", "ValueError", "(", "\"Shape mismatch: im1 and im2 must have the same shape.\"", ")", "\n", "\n", "", "im_sum", "=", "im1", ".", "sum", "(", ")", "+", "im2", ".", "sum", "(", ")", "\n", "if", "im_sum", "==", "0", ":", "\n", "        ", "return", "empty_score", "\n", "\n", "", "intersection", "=", "np", ".", "logical_and", "(", "im1", ",", "im2", ")", "\n", "\n", "return", "2.", "*", "intersection", ".", "sum", "(", ")", "/", "im_sum", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.keras.helper_functions.standard_unit": [[77, 85], ["keras.layers.Conv2D", "keras.layers.core.Dropout", "keras.layers.Conv2D", "keras.layers.core.Dropout", "keras.regularizers.l2", "keras.regularizers.l2"], "function", ["None"], ["", "def", "standard_unit", "(", "input_tensor", ",", "stage", ",", "nb_filter", ",", "kernel_size", "=", "3", ")", ":", "\n", "\n", "    ", "x", "=", "Conv2D", "(", "nb_filter", ",", "(", "kernel_size", ",", "kernel_size", ")", ",", "activation", "=", "act", ",", "name", "=", "'conv'", "+", "stage", "+", "'_1'", ",", "kernel_initializer", "=", "'he_normal'", ",", "padding", "=", "'same'", ",", "kernel_regularizer", "=", "l2", "(", "1e-4", ")", ")", "(", "input_tensor", ")", "\n", "x", "=", "Dropout", "(", "dropout_rate", ",", "name", "=", "'dp'", "+", "stage", "+", "'_1'", ")", "(", "x", ")", "\n", "x", "=", "Conv2D", "(", "nb_filter", ",", "(", "kernel_size", ",", "kernel_size", ")", ",", "activation", "=", "act", ",", "name", "=", "'conv'", "+", "stage", "+", "'_2'", ",", "kernel_initializer", "=", "'he_normal'", ",", "padding", "=", "'same'", ",", "kernel_regularizer", "=", "l2", "(", "1e-4", ")", ")", "(", "x", ")", "\n", "x", "=", "Dropout", "(", "dropout_rate", ",", "name", "=", "'dp'", "+", "stage", "+", "'_2'", ")", "(", "x", ")", "\n", "\n", "return", "x", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.keras.helper_functions.U_Net": [[92, 140], ["helper_functions.standard_unit", "helper_functions.standard_unit", "helper_functions.standard_unit", "helper_functions.standard_unit", "helper_functions.standard_unit", "keras.layers.concatenate", "helper_functions.standard_unit", "keras.layers.concatenate", "helper_functions.standard_unit", "keras.layers.concatenate", "helper_functions.standard_unit", "keras.layers.concatenate", "helper_functions.standard_unit", "keras.models.Model", "keras.backend.image_dim_ordering", "keras.layers.Input", "keras.layers.Input", "keras.layers.pooling.MaxPooling2D", "keras.layers.pooling.MaxPooling2D", "keras.layers.pooling.MaxPooling2D", "keras.layers.pooling.MaxPooling2D", "keras.layers.Conv2DTranspose", "keras.layers.Conv2DTranspose", "keras.layers.Conv2DTranspose", "keras.layers.Conv2DTranspose", "keras.layers.Conv2D", "keras.regularizers.l2"], "function", ["home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.keras.helper_functions.standard_unit", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.keras.helper_functions.standard_unit", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.keras.helper_functions.standard_unit", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.keras.helper_functions.standard_unit", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.keras.helper_functions.standard_unit", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.keras.helper_functions.standard_unit", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.keras.helper_functions.standard_unit", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.keras.helper_functions.standard_unit", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.keras.helper_functions.standard_unit", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.linknet.blocks.Conv2DTranspose", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.linknet.blocks.Conv2DTranspose", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.linknet.blocks.Conv2DTranspose", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.linknet.blocks.Conv2DTranspose"], ["def", "U_Net", "(", "img_rows", ",", "img_cols", ",", "color_type", "=", "1", ",", "num_class", "=", "1", ")", ":", "\n", "\n", "    ", "nb_filter", "=", "[", "32", ",", "64", ",", "128", ",", "256", ",", "512", "]", "\n", "\n", "# Handle Dimension Ordering for different backends", "\n", "global", "bn_axis", "\n", "if", "K", ".", "image_dim_ordering", "(", ")", "==", "'tf'", ":", "\n", "      ", "bn_axis", "=", "3", "\n", "img_input", "=", "Input", "(", "shape", "=", "(", "img_rows", ",", "img_cols", ",", "color_type", ")", ",", "name", "=", "'main_input'", ")", "\n", "", "else", ":", "\n", "      ", "bn_axis", "=", "1", "\n", "img_input", "=", "Input", "(", "shape", "=", "(", "color_type", ",", "img_rows", ",", "img_cols", ")", ",", "name", "=", "'main_input'", ")", "\n", "\n", "", "conv1_1", "=", "standard_unit", "(", "img_input", ",", "stage", "=", "'11'", ",", "nb_filter", "=", "nb_filter", "[", "0", "]", ")", "\n", "pool1", "=", "MaxPooling2D", "(", "(", "2", ",", "2", ")", ",", "strides", "=", "(", "2", ",", "2", ")", ",", "name", "=", "'pool1'", ")", "(", "conv1_1", ")", "\n", "\n", "conv2_1", "=", "standard_unit", "(", "pool1", ",", "stage", "=", "'21'", ",", "nb_filter", "=", "nb_filter", "[", "1", "]", ")", "\n", "pool2", "=", "MaxPooling2D", "(", "(", "2", ",", "2", ")", ",", "strides", "=", "(", "2", ",", "2", ")", ",", "name", "=", "'pool2'", ")", "(", "conv2_1", ")", "\n", "\n", "conv3_1", "=", "standard_unit", "(", "pool2", ",", "stage", "=", "'31'", ",", "nb_filter", "=", "nb_filter", "[", "2", "]", ")", "\n", "pool3", "=", "MaxPooling2D", "(", "(", "2", ",", "2", ")", ",", "strides", "=", "(", "2", ",", "2", ")", ",", "name", "=", "'pool3'", ")", "(", "conv3_1", ")", "\n", "\n", "conv4_1", "=", "standard_unit", "(", "pool3", ",", "stage", "=", "'41'", ",", "nb_filter", "=", "nb_filter", "[", "3", "]", ")", "\n", "pool4", "=", "MaxPooling2D", "(", "(", "2", ",", "2", ")", ",", "strides", "=", "(", "2", ",", "2", ")", ",", "name", "=", "'pool4'", ")", "(", "conv4_1", ")", "\n", "\n", "conv5_1", "=", "standard_unit", "(", "pool4", ",", "stage", "=", "'51'", ",", "nb_filter", "=", "nb_filter", "[", "4", "]", ")", "\n", "\n", "up4_2", "=", "Conv2DTranspose", "(", "nb_filter", "[", "3", "]", ",", "(", "2", ",", "2", ")", ",", "strides", "=", "(", "2", ",", "2", ")", ",", "name", "=", "'up42'", ",", "padding", "=", "'same'", ")", "(", "conv5_1", ")", "\n", "conv4_2", "=", "concatenate", "(", "[", "up4_2", ",", "conv4_1", "]", ",", "name", "=", "'merge42'", ",", "axis", "=", "bn_axis", ")", "\n", "conv4_2", "=", "standard_unit", "(", "conv4_2", ",", "stage", "=", "'42'", ",", "nb_filter", "=", "nb_filter", "[", "3", "]", ")", "\n", "\n", "up3_3", "=", "Conv2DTranspose", "(", "nb_filter", "[", "2", "]", ",", "(", "2", ",", "2", ")", ",", "strides", "=", "(", "2", ",", "2", ")", ",", "name", "=", "'up33'", ",", "padding", "=", "'same'", ")", "(", "conv4_2", ")", "\n", "conv3_3", "=", "concatenate", "(", "[", "up3_3", ",", "conv3_1", "]", ",", "name", "=", "'merge33'", ",", "axis", "=", "bn_axis", ")", "\n", "conv3_3", "=", "standard_unit", "(", "conv3_3", ",", "stage", "=", "'33'", ",", "nb_filter", "=", "nb_filter", "[", "2", "]", ")", "\n", "\n", "up2_4", "=", "Conv2DTranspose", "(", "nb_filter", "[", "1", "]", ",", "(", "2", ",", "2", ")", ",", "strides", "=", "(", "2", ",", "2", ")", ",", "name", "=", "'up24'", ",", "padding", "=", "'same'", ")", "(", "conv3_3", ")", "\n", "conv2_4", "=", "concatenate", "(", "[", "up2_4", ",", "conv2_1", "]", ",", "name", "=", "'merge24'", ",", "axis", "=", "bn_axis", ")", "\n", "conv2_4", "=", "standard_unit", "(", "conv2_4", ",", "stage", "=", "'24'", ",", "nb_filter", "=", "nb_filter", "[", "1", "]", ")", "\n", "\n", "up1_5", "=", "Conv2DTranspose", "(", "nb_filter", "[", "0", "]", ",", "(", "2", ",", "2", ")", ",", "strides", "=", "(", "2", ",", "2", ")", ",", "name", "=", "'up15'", ",", "padding", "=", "'same'", ")", "(", "conv2_4", ")", "\n", "conv1_5", "=", "concatenate", "(", "[", "up1_5", ",", "conv1_1", "]", ",", "name", "=", "'merge15'", ",", "axis", "=", "bn_axis", ")", "\n", "conv1_5", "=", "standard_unit", "(", "conv1_5", ",", "stage", "=", "'15'", ",", "nb_filter", "=", "nb_filter", "[", "0", "]", ")", "\n", "\n", "unet_output", "=", "Conv2D", "(", "num_class", ",", "(", "1", ",", "1", ")", ",", "activation", "=", "'sigmoid'", ",", "name", "=", "'output'", ",", "kernel_initializer", "=", "'he_normal'", ",", "padding", "=", "'same'", ",", "kernel_regularizer", "=", "l2", "(", "1e-4", ")", ")", "(", "conv1_5", ")", "\n", "\n", "model", "=", "Model", "(", "input", "=", "img_input", ",", "output", "=", "unet_output", ")", "\n", "\n", "return", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.keras.helper_functions.wU_Net": [[145, 194], ["helper_functions.standard_unit", "helper_functions.standard_unit", "helper_functions.standard_unit", "helper_functions.standard_unit", "helper_functions.standard_unit", "keras.layers.concatenate", "helper_functions.standard_unit", "keras.layers.concatenate", "helper_functions.standard_unit", "keras.layers.concatenate", "helper_functions.standard_unit", "keras.layers.concatenate", "helper_functions.standard_unit", "keras.models.Model", "keras.backend.image_dim_ordering", "keras.layers.Input", "keras.layers.Input", "keras.layers.pooling.MaxPooling2D", "keras.layers.pooling.MaxPooling2D", "keras.layers.pooling.MaxPooling2D", "keras.layers.pooling.MaxPooling2D", "keras.layers.Conv2DTranspose", "keras.layers.Conv2DTranspose", "keras.layers.Conv2DTranspose", "keras.layers.Conv2DTranspose", "keras.layers.Conv2D", "keras.regularizers.l2"], "function", ["home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.keras.helper_functions.standard_unit", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.keras.helper_functions.standard_unit", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.keras.helper_functions.standard_unit", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.keras.helper_functions.standard_unit", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.keras.helper_functions.standard_unit", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.keras.helper_functions.standard_unit", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.keras.helper_functions.standard_unit", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.keras.helper_functions.standard_unit", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.keras.helper_functions.standard_unit", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.linknet.blocks.Conv2DTranspose", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.linknet.blocks.Conv2DTranspose", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.linknet.blocks.Conv2DTranspose", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.linknet.blocks.Conv2DTranspose"], ["def", "wU_Net", "(", "img_rows", ",", "img_cols", ",", "color_type", "=", "1", ",", "num_class", "=", "1", ")", ":", "\n", "\n", "# nb_filter = [32,64,128,256,512]", "\n", "    ", "nb_filter", "=", "[", "35", ",", "70", ",", "140", ",", "280", ",", "560", "]", "\n", "\n", "# Handle Dimension Ordering for different backends", "\n", "global", "bn_axis", "\n", "if", "K", ".", "image_dim_ordering", "(", ")", "==", "'tf'", ":", "\n", "      ", "bn_axis", "=", "3", "\n", "img_input", "=", "Input", "(", "shape", "=", "(", "img_rows", ",", "img_cols", ",", "color_type", ")", ",", "name", "=", "'main_input'", ")", "\n", "", "else", ":", "\n", "      ", "bn_axis", "=", "1", "\n", "img_input", "=", "Input", "(", "shape", "=", "(", "color_type", ",", "img_rows", ",", "img_cols", ")", ",", "name", "=", "'main_input'", ")", "\n", "\n", "", "conv1_1", "=", "standard_unit", "(", "img_input", ",", "stage", "=", "'11'", ",", "nb_filter", "=", "nb_filter", "[", "0", "]", ")", "\n", "pool1", "=", "MaxPooling2D", "(", "(", "2", ",", "2", ")", ",", "strides", "=", "(", "2", ",", "2", ")", ",", "name", "=", "'pool1'", ")", "(", "conv1_1", ")", "\n", "\n", "conv2_1", "=", "standard_unit", "(", "pool1", ",", "stage", "=", "'21'", ",", "nb_filter", "=", "nb_filter", "[", "1", "]", ")", "\n", "pool2", "=", "MaxPooling2D", "(", "(", "2", ",", "2", ")", ",", "strides", "=", "(", "2", ",", "2", ")", ",", "name", "=", "'pool2'", ")", "(", "conv2_1", ")", "\n", "\n", "conv3_1", "=", "standard_unit", "(", "pool2", ",", "stage", "=", "'31'", ",", "nb_filter", "=", "nb_filter", "[", "2", "]", ")", "\n", "pool3", "=", "MaxPooling2D", "(", "(", "2", ",", "2", ")", ",", "strides", "=", "(", "2", ",", "2", ")", ",", "name", "=", "'pool3'", ")", "(", "conv3_1", ")", "\n", "\n", "conv4_1", "=", "standard_unit", "(", "pool3", ",", "stage", "=", "'41'", ",", "nb_filter", "=", "nb_filter", "[", "3", "]", ")", "\n", "pool4", "=", "MaxPooling2D", "(", "(", "2", ",", "2", ")", ",", "strides", "=", "(", "2", ",", "2", ")", ",", "name", "=", "'pool4'", ")", "(", "conv4_1", ")", "\n", "\n", "conv5_1", "=", "standard_unit", "(", "pool4", ",", "stage", "=", "'51'", ",", "nb_filter", "=", "nb_filter", "[", "4", "]", ")", "\n", "\n", "up4_2", "=", "Conv2DTranspose", "(", "nb_filter", "[", "3", "]", ",", "(", "2", ",", "2", ")", ",", "strides", "=", "(", "2", ",", "2", ")", ",", "name", "=", "'up42'", ",", "padding", "=", "'same'", ")", "(", "conv5_1", ")", "\n", "conv4_2", "=", "concatenate", "(", "[", "up4_2", ",", "conv4_1", "]", ",", "name", "=", "'merge42'", ",", "axis", "=", "bn_axis", ")", "\n", "conv4_2", "=", "standard_unit", "(", "conv4_2", ",", "stage", "=", "'42'", ",", "nb_filter", "=", "nb_filter", "[", "3", "]", ")", "\n", "\n", "up3_3", "=", "Conv2DTranspose", "(", "nb_filter", "[", "2", "]", ",", "(", "2", ",", "2", ")", ",", "strides", "=", "(", "2", ",", "2", ")", ",", "name", "=", "'up33'", ",", "padding", "=", "'same'", ")", "(", "conv4_2", ")", "\n", "conv3_3", "=", "concatenate", "(", "[", "up3_3", ",", "conv3_1", "]", ",", "name", "=", "'merge33'", ",", "axis", "=", "bn_axis", ")", "\n", "conv3_3", "=", "standard_unit", "(", "conv3_3", ",", "stage", "=", "'33'", ",", "nb_filter", "=", "nb_filter", "[", "2", "]", ")", "\n", "\n", "up2_4", "=", "Conv2DTranspose", "(", "nb_filter", "[", "1", "]", ",", "(", "2", ",", "2", ")", ",", "strides", "=", "(", "2", ",", "2", ")", ",", "name", "=", "'up24'", ",", "padding", "=", "'same'", ")", "(", "conv3_3", ")", "\n", "conv2_4", "=", "concatenate", "(", "[", "up2_4", ",", "conv2_1", "]", ",", "name", "=", "'merge24'", ",", "axis", "=", "bn_axis", ")", "\n", "conv2_4", "=", "standard_unit", "(", "conv2_4", ",", "stage", "=", "'24'", ",", "nb_filter", "=", "nb_filter", "[", "1", "]", ")", "\n", "\n", "up1_5", "=", "Conv2DTranspose", "(", "nb_filter", "[", "0", "]", ",", "(", "2", ",", "2", ")", ",", "strides", "=", "(", "2", ",", "2", ")", ",", "name", "=", "'up15'", ",", "padding", "=", "'same'", ")", "(", "conv2_4", ")", "\n", "conv1_5", "=", "concatenate", "(", "[", "up1_5", ",", "conv1_1", "]", ",", "name", "=", "'merge15'", ",", "axis", "=", "bn_axis", ")", "\n", "conv1_5", "=", "standard_unit", "(", "conv1_5", ",", "stage", "=", "'15'", ",", "nb_filter", "=", "nb_filter", "[", "0", "]", ")", "\n", "\n", "unet_output", "=", "Conv2D", "(", "num_class", ",", "(", "1", ",", "1", ")", ",", "activation", "=", "'sigmoid'", ",", "name", "=", "'output'", ",", "kernel_initializer", "=", "'he_normal'", ",", "padding", "=", "'same'", ",", "kernel_regularizer", "=", "l2", "(", "1e-4", ")", ")", "(", "conv1_5", ")", "\n", "\n", "model", "=", "Model", "(", "input", "=", "img_input", ",", "output", "=", "unet_output", ")", "\n", "\n", "return", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.keras.helper_functions.UNetPlusPlus": [[199, 280], ["helper_functions.standard_unit", "helper_functions.standard_unit", "keras.layers.concatenate", "helper_functions.standard_unit", "helper_functions.standard_unit", "keras.layers.concatenate", "helper_functions.standard_unit", "keras.layers.concatenate", "helper_functions.standard_unit", "helper_functions.standard_unit", "keras.layers.concatenate", "helper_functions.standard_unit", "keras.layers.concatenate", "helper_functions.standard_unit", "keras.layers.concatenate", "helper_functions.standard_unit", "helper_functions.standard_unit", "keras.layers.concatenate", "helper_functions.standard_unit", "keras.layers.concatenate", "helper_functions.standard_unit", "keras.layers.concatenate", "helper_functions.standard_unit", "keras.layers.concatenate", "helper_functions.standard_unit", "keras.backend.image_dim_ordering", "keras.layers.Input", "keras.layers.Input", "keras.layers.pooling.MaxPooling2D", "keras.layers.pooling.MaxPooling2D", "keras.layers.Conv2DTranspose", "keras.layers.pooling.MaxPooling2D", "keras.layers.Conv2DTranspose", "keras.layers.Conv2DTranspose", "keras.layers.pooling.MaxPooling2D", "keras.layers.Conv2DTranspose", "keras.layers.Conv2DTranspose", "keras.layers.Conv2DTranspose", "keras.layers.Conv2DTranspose", "keras.layers.Conv2DTranspose", "keras.layers.Conv2DTranspose", "keras.layers.Conv2DTranspose", "keras.layers.Conv2D", "keras.layers.Conv2D", "keras.layers.Conv2D", "keras.layers.Conv2D", "keras.models.Model", "keras.models.Model", "keras.regularizers.l2", "keras.regularizers.l2", "keras.regularizers.l2", "keras.regularizers.l2"], "function", ["home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.keras.helper_functions.standard_unit", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.keras.helper_functions.standard_unit", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.keras.helper_functions.standard_unit", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.keras.helper_functions.standard_unit", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.keras.helper_functions.standard_unit", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.keras.helper_functions.standard_unit", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.keras.helper_functions.standard_unit", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.keras.helper_functions.standard_unit", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.keras.helper_functions.standard_unit", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.keras.helper_functions.standard_unit", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.keras.helper_functions.standard_unit", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.keras.helper_functions.standard_unit", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.keras.helper_functions.standard_unit", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.keras.helper_functions.standard_unit", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.keras.helper_functions.standard_unit", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.linknet.blocks.Conv2DTranspose", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.linknet.blocks.Conv2DTranspose", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.linknet.blocks.Conv2DTranspose", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.linknet.blocks.Conv2DTranspose", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.linknet.blocks.Conv2DTranspose", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.linknet.blocks.Conv2DTranspose", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.linknet.blocks.Conv2DTranspose", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.linknet.blocks.Conv2DTranspose", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.linknet.blocks.Conv2DTranspose", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.linknet.blocks.Conv2DTranspose"], ["def", "UNetPlusPlus", "(", "img_rows", ",", "img_cols", ",", "color_type", "=", "1", ",", "num_class", "=", "1", ",", "deep_supervision", "=", "False", ")", ":", "\n", "\n", "    ", "nb_filter", "=", "[", "32", ",", "64", ",", "128", ",", "256", ",", "512", "]", "\n", "\n", "# Handle Dimension Ordering for different backends", "\n", "global", "bn_axis", "\n", "if", "K", ".", "image_dim_ordering", "(", ")", "==", "'tf'", ":", "\n", "      ", "bn_axis", "=", "3", "\n", "img_input", "=", "Input", "(", "shape", "=", "(", "img_rows", ",", "img_cols", ",", "color_type", ")", ",", "name", "=", "'main_input'", ")", "\n", "", "else", ":", "\n", "      ", "bn_axis", "=", "1", "\n", "img_input", "=", "Input", "(", "shape", "=", "(", "color_type", ",", "img_rows", ",", "img_cols", ")", ",", "name", "=", "'main_input'", ")", "\n", "\n", "", "conv1_1", "=", "standard_unit", "(", "img_input", ",", "stage", "=", "'11'", ",", "nb_filter", "=", "nb_filter", "[", "0", "]", ")", "\n", "pool1", "=", "MaxPooling2D", "(", "(", "2", ",", "2", ")", ",", "strides", "=", "(", "2", ",", "2", ")", ",", "name", "=", "'pool1'", ")", "(", "conv1_1", ")", "\n", "\n", "conv2_1", "=", "standard_unit", "(", "pool1", ",", "stage", "=", "'21'", ",", "nb_filter", "=", "nb_filter", "[", "1", "]", ")", "\n", "pool2", "=", "MaxPooling2D", "(", "(", "2", ",", "2", ")", ",", "strides", "=", "(", "2", ",", "2", ")", ",", "name", "=", "'pool2'", ")", "(", "conv2_1", ")", "\n", "\n", "up1_2", "=", "Conv2DTranspose", "(", "nb_filter", "[", "0", "]", ",", "(", "2", ",", "2", ")", ",", "strides", "=", "(", "2", ",", "2", ")", ",", "name", "=", "'up12'", ",", "padding", "=", "'same'", ")", "(", "conv2_1", ")", "\n", "conv1_2", "=", "concatenate", "(", "[", "up1_2", ",", "conv1_1", "]", ",", "name", "=", "'merge12'", ",", "axis", "=", "bn_axis", ")", "\n", "conv1_2", "=", "standard_unit", "(", "conv1_2", ",", "stage", "=", "'12'", ",", "nb_filter", "=", "nb_filter", "[", "0", "]", ")", "\n", "\n", "conv3_1", "=", "standard_unit", "(", "pool2", ",", "stage", "=", "'31'", ",", "nb_filter", "=", "nb_filter", "[", "2", "]", ")", "\n", "pool3", "=", "MaxPooling2D", "(", "(", "2", ",", "2", ")", ",", "strides", "=", "(", "2", ",", "2", ")", ",", "name", "=", "'pool3'", ")", "(", "conv3_1", ")", "\n", "\n", "up2_2", "=", "Conv2DTranspose", "(", "nb_filter", "[", "1", "]", ",", "(", "2", ",", "2", ")", ",", "strides", "=", "(", "2", ",", "2", ")", ",", "name", "=", "'up22'", ",", "padding", "=", "'same'", ")", "(", "conv3_1", ")", "\n", "conv2_2", "=", "concatenate", "(", "[", "up2_2", ",", "conv2_1", "]", ",", "name", "=", "'merge22'", ",", "axis", "=", "bn_axis", ")", "\n", "conv2_2", "=", "standard_unit", "(", "conv2_2", ",", "stage", "=", "'22'", ",", "nb_filter", "=", "nb_filter", "[", "1", "]", ")", "\n", "\n", "up1_3", "=", "Conv2DTranspose", "(", "nb_filter", "[", "0", "]", ",", "(", "2", ",", "2", ")", ",", "strides", "=", "(", "2", ",", "2", ")", ",", "name", "=", "'up13'", ",", "padding", "=", "'same'", ")", "(", "conv2_2", ")", "\n", "conv1_3", "=", "concatenate", "(", "[", "up1_3", ",", "conv1_1", ",", "conv1_2", "]", ",", "name", "=", "'merge13'", ",", "axis", "=", "bn_axis", ")", "\n", "conv1_3", "=", "standard_unit", "(", "conv1_3", ",", "stage", "=", "'13'", ",", "nb_filter", "=", "nb_filter", "[", "0", "]", ")", "\n", "\n", "conv4_1", "=", "standard_unit", "(", "pool3", ",", "stage", "=", "'41'", ",", "nb_filter", "=", "nb_filter", "[", "3", "]", ")", "\n", "pool4", "=", "MaxPooling2D", "(", "(", "2", ",", "2", ")", ",", "strides", "=", "(", "2", ",", "2", ")", ",", "name", "=", "'pool4'", ")", "(", "conv4_1", ")", "\n", "\n", "up3_2", "=", "Conv2DTranspose", "(", "nb_filter", "[", "2", "]", ",", "(", "2", ",", "2", ")", ",", "strides", "=", "(", "2", ",", "2", ")", ",", "name", "=", "'up32'", ",", "padding", "=", "'same'", ")", "(", "conv4_1", ")", "\n", "conv3_2", "=", "concatenate", "(", "[", "up3_2", ",", "conv3_1", "]", ",", "name", "=", "'merge32'", ",", "axis", "=", "bn_axis", ")", "\n", "conv3_2", "=", "standard_unit", "(", "conv3_2", ",", "stage", "=", "'32'", ",", "nb_filter", "=", "nb_filter", "[", "2", "]", ")", "\n", "\n", "up2_3", "=", "Conv2DTranspose", "(", "nb_filter", "[", "1", "]", ",", "(", "2", ",", "2", ")", ",", "strides", "=", "(", "2", ",", "2", ")", ",", "name", "=", "'up23'", ",", "padding", "=", "'same'", ")", "(", "conv3_2", ")", "\n", "conv2_3", "=", "concatenate", "(", "[", "up2_3", ",", "conv2_1", ",", "conv2_2", "]", ",", "name", "=", "'merge23'", ",", "axis", "=", "bn_axis", ")", "\n", "conv2_3", "=", "standard_unit", "(", "conv2_3", ",", "stage", "=", "'23'", ",", "nb_filter", "=", "nb_filter", "[", "1", "]", ")", "\n", "\n", "up1_4", "=", "Conv2DTranspose", "(", "nb_filter", "[", "0", "]", ",", "(", "2", ",", "2", ")", ",", "strides", "=", "(", "2", ",", "2", ")", ",", "name", "=", "'up14'", ",", "padding", "=", "'same'", ")", "(", "conv2_3", ")", "\n", "conv1_4", "=", "concatenate", "(", "[", "up1_4", ",", "conv1_1", ",", "conv1_2", ",", "conv1_3", "]", ",", "name", "=", "'merge14'", ",", "axis", "=", "bn_axis", ")", "\n", "conv1_4", "=", "standard_unit", "(", "conv1_4", ",", "stage", "=", "'14'", ",", "nb_filter", "=", "nb_filter", "[", "0", "]", ")", "\n", "\n", "conv5_1", "=", "standard_unit", "(", "pool4", ",", "stage", "=", "'51'", ",", "nb_filter", "=", "nb_filter", "[", "4", "]", ")", "\n", "\n", "up4_2", "=", "Conv2DTranspose", "(", "nb_filter", "[", "3", "]", ",", "(", "2", ",", "2", ")", ",", "strides", "=", "(", "2", ",", "2", ")", ",", "name", "=", "'up42'", ",", "padding", "=", "'same'", ")", "(", "conv5_1", ")", "\n", "conv4_2", "=", "concatenate", "(", "[", "up4_2", ",", "conv4_1", "]", ",", "name", "=", "'merge42'", ",", "axis", "=", "bn_axis", ")", "\n", "conv4_2", "=", "standard_unit", "(", "conv4_2", ",", "stage", "=", "'42'", ",", "nb_filter", "=", "nb_filter", "[", "3", "]", ")", "\n", "\n", "up3_3", "=", "Conv2DTranspose", "(", "nb_filter", "[", "2", "]", ",", "(", "2", ",", "2", ")", ",", "strides", "=", "(", "2", ",", "2", ")", ",", "name", "=", "'up33'", ",", "padding", "=", "'same'", ")", "(", "conv4_2", ")", "\n", "conv3_3", "=", "concatenate", "(", "[", "up3_3", ",", "conv3_1", ",", "conv3_2", "]", ",", "name", "=", "'merge33'", ",", "axis", "=", "bn_axis", ")", "\n", "conv3_3", "=", "standard_unit", "(", "conv3_3", ",", "stage", "=", "'33'", ",", "nb_filter", "=", "nb_filter", "[", "2", "]", ")", "\n", "\n", "up2_4", "=", "Conv2DTranspose", "(", "nb_filter", "[", "1", "]", ",", "(", "2", ",", "2", ")", ",", "strides", "=", "(", "2", ",", "2", ")", ",", "name", "=", "'up24'", ",", "padding", "=", "'same'", ")", "(", "conv3_3", ")", "\n", "conv2_4", "=", "concatenate", "(", "[", "up2_4", ",", "conv2_1", ",", "conv2_2", ",", "conv2_3", "]", ",", "name", "=", "'merge24'", ",", "axis", "=", "bn_axis", ")", "\n", "conv2_4", "=", "standard_unit", "(", "conv2_4", ",", "stage", "=", "'24'", ",", "nb_filter", "=", "nb_filter", "[", "1", "]", ")", "\n", "\n", "up1_5", "=", "Conv2DTranspose", "(", "nb_filter", "[", "0", "]", ",", "(", "2", ",", "2", ")", ",", "strides", "=", "(", "2", ",", "2", ")", ",", "name", "=", "'up15'", ",", "padding", "=", "'same'", ")", "(", "conv2_4", ")", "\n", "conv1_5", "=", "concatenate", "(", "[", "up1_5", ",", "conv1_1", ",", "conv1_2", ",", "conv1_3", ",", "conv1_4", "]", ",", "name", "=", "'merge15'", ",", "axis", "=", "bn_axis", ")", "\n", "conv1_5", "=", "standard_unit", "(", "conv1_5", ",", "stage", "=", "'15'", ",", "nb_filter", "=", "nb_filter", "[", "0", "]", ")", "\n", "\n", "nestnet_output_1", "=", "Conv2D", "(", "num_class", ",", "(", "1", ",", "1", ")", ",", "activation", "=", "'sigmoid'", ",", "name", "=", "'output_1'", ",", "kernel_initializer", "=", "'he_normal'", ",", "padding", "=", "'same'", ",", "kernel_regularizer", "=", "l2", "(", "1e-4", ")", ")", "(", "conv1_2", ")", "\n", "nestnet_output_2", "=", "Conv2D", "(", "num_class", ",", "(", "1", ",", "1", ")", ",", "activation", "=", "'sigmoid'", ",", "name", "=", "'output_2'", ",", "kernel_initializer", "=", "'he_normal'", ",", "padding", "=", "'same'", ",", "kernel_regularizer", "=", "l2", "(", "1e-4", ")", ")", "(", "conv1_3", ")", "\n", "nestnet_output_3", "=", "Conv2D", "(", "num_class", ",", "(", "1", ",", "1", ")", ",", "activation", "=", "'sigmoid'", ",", "name", "=", "'output_3'", ",", "kernel_initializer", "=", "'he_normal'", ",", "padding", "=", "'same'", ",", "kernel_regularizer", "=", "l2", "(", "1e-4", ")", ")", "(", "conv1_4", ")", "\n", "nestnet_output_4", "=", "Conv2D", "(", "num_class", ",", "(", "1", ",", "1", ")", ",", "activation", "=", "'sigmoid'", ",", "name", "=", "'output_4'", ",", "kernel_initializer", "=", "'he_normal'", ",", "padding", "=", "'same'", ",", "kernel_regularizer", "=", "l2", "(", "1e-4", ")", ")", "(", "conv1_5", ")", "\n", "\n", "if", "deep_supervision", ":", "\n", "        ", "model", "=", "Model", "(", "input", "=", "img_input", ",", "output", "=", "[", "nestnet_output_1", ",", "\n", "nestnet_output_2", ",", "\n", "nestnet_output_3", ",", "\n", "nestnet_output_4", "]", ")", "\n", "", "else", ":", "\n", "        ", "model", "=", "Model", "(", "input", "=", "img_input", ",", "output", "=", "[", "nestnet_output_4", "]", ")", "\n", "\n", "", "return", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.segmentation_models.utils.get_layer_number": [[5, 22], ["enumerate", "ValueError"], "function", ["None"], ["#    You may obtain a copy of the License at", "\n", "#", "\n", "#        http://www.apache.org/licenses/LICENSE-2.0", "\n", "#", "\n", "#    Unless required by applicable law or agreed to in writing, software", "\n", "#    distributed under the License is distributed on an \"AS IS\" BASIS,", "\n", "#    WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.", "\n", "#    See the License for the specific language governing permissions and", "\n", "#    limitations under the License.", "\n", "\n", "import", "json", "\n", "import", "os", "\n", "import", "pickle", "\n", "import", "shutil", "\n", "from", "collections", "import", "OrderedDict", "\n", "from", "multiprocessing", "import", "Pool", "\n", "\n", "import", "numpy", "as", "np", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.segmentation_models.utils.extract_outputs": [[24, 43], ["outputs.insert", "isinstance", "utils.get_layer_number"], "function", ["home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.segmentation_models.utils.get_layer_number"], ["from", "nnunet", ".", "configuration", "import", "default_num_threads", "\n", "from", "nnunet", ".", "experiment_planning", ".", "DatasetAnalyzer", "import", "DatasetAnalyzer", "\n", "from", "nnunet", ".", "experiment_planning", ".", "common_utils", "import", "split_4d_nifti", "\n", "from", "nnunet", ".", "paths", "import", "nnUNet_raw_data", ",", "nnUNet_cropped_data", ",", "preprocessing_output_dir", "\n", "from", "nnunet", ".", "preprocessing", ".", "cropping", "import", "ImageCropper", "\n", "\n", "\n", "def", "split_4d", "(", "input_folder", ",", "num_processes", "=", "default_num_threads", ",", "overwrite_task_output_id", "=", "None", ")", ":", "\n", "    ", "assert", "isdir", "(", "join", "(", "input_folder", ",", "\"imagesTr\"", ")", ")", "and", "isdir", "(", "join", "(", "input_folder", ",", "\"labelsTr\"", ")", ")", "and", "isfile", "(", "join", "(", "input_folder", ",", "\"dataset.json\"", ")", ")", ",", "\"The input folder must be a valid Task folder from the Medical Segmentation Decathlon with at least the \"", "\"imagesTr and labelsTr subfolders and the dataset.json file\"", "\n", "\n", "while", "input_folder", ".", "endswith", "(", "\"/\"", ")", ":", "\n", "        ", "input_folder", "=", "input_folder", "[", ":", "-", "1", "]", "\n", "\n", "", "full_task_name", "=", "input_folder", ".", "split", "(", "\"/\"", ")", "[", "-", "1", "]", "\n", "\n", "assert", "full_task_name", ".", "startswith", "(", "\"Task\"", ")", ",", "\"The input folder must point to a folder that starts with TaskXX_\"", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.segmentation_models.utils.reverse": [[45, 48], ["list", "reversed"], "function", ["None"], ["assert", "first_underscore", "==", "6", ",", "\"Input folder start with TaskXX with XX being a 3-digit id: 00, 01, 02 etc\"", "\n", "\n", "input_task_id", "=", "int", "(", "full_task_name", "[", "4", ":", "6", "]", ")", "\n", "if", "overwrite_task_output_id", "is", "None", ":", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.segmentation_models.utils.add_docstring": [[51, 63], ["functools.wraps", "fn"], "function", ["None"], ["", "task_name", "=", "full_task_name", "[", "7", ":", "]", "\n", "\n", "output_folder", "=", "join", "(", "nnUNet_raw_data", ",", "\"Task%03.0d_\"", "%", "overwrite_task_output_id", "+", "task_name", ")", "\n", "\n", "if", "isdir", "(", "output_folder", ")", ":", "\n", "        ", "shutil", ".", "rmtree", "(", "output_folder", ")", "\n", "\n", "", "files", "=", "[", "]", "\n", "output_dirs", "=", "[", "]", "\n", "\n", "maybe_mkdir_p", "(", "output_folder", ")", "\n", "for", "subdir", "in", "[", "\"imagesTr\"", ",", "\"imagesTs\"", "]", ":", "\n", "        ", "curr_out_dir", "=", "join", "(", "output_folder", ",", "subdir", ")", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.segmentation_models.utils.recompile": [[65, 67], ["model.compile"], "function", ["None"], ["            ", "os", ".", "mkdir", "(", "curr_out_dir", ")", "\n", "", "curr_dir", "=", "join", "(", "input_folder", ",", "subdir", ")", "\n", "nii_files", "=", "[", "join", "(", "curr_dir", ",", "i", ")", "for", "i", "in", "os", ".", "listdir", "(", "curr_dir", ")", "if", "i", ".", "endswith", "(", "\".nii.gz\"", ")", "]", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.segmentation_models.utils.freeze_model": [[69, 73], ["None"], "function", ["None"], ["for", "n", "in", "nii_files", ":", "\n", "            ", "files", ".", "append", "(", "n", ")", "\n", "output_dirs", ".", "append", "(", "curr_out_dir", ")", "\n", "\n", "", "", "shutil", ".", "copytree", "(", "join", "(", "input_folder", ",", "\"labelsTr\"", ")", ",", "join", "(", "output_folder", ",", "\"labelsTr\"", ")", ")", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.segmentation_models.utils.set_trainable": [[75, 79], ["utils.recompile"], "function", ["home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.segmentation_models.utils.recompile"], ["p", "=", "Pool", "(", "num_processes", ")", "\n", "p", ".", "starmap", "(", "split_4d_nifti", ",", "zip", "(", "files", ",", "output_dirs", ")", ")", "\n", "p", ".", "close", "(", ")", "\n", "p", ".", "join", "(", ")", "\n", "shutil", ".", "copy", "(", "join", "(", "input_folder", ",", "\"dataset.json\"", ")", ",", "output_folder", ")", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.segmentation_models.utils.to_tuple": [[81, 89], ["isinstance", "ValueError", "numpy.isscalar", "len"], "function", ["None"], ["\n", "", "def", "create_lists_from_splitted_dataset", "(", "base_folder_splitted", ")", ":", "\n", "    ", "lists", "=", "[", "]", "\n", "\n", "json_file", "=", "join", "(", "base_folder_splitted", ",", "\"dataset.json\"", ")", "\n", "with", "open", "(", "json_file", ")", "as", "jsn", ":", "\n", "        ", "d", "=", "json", ".", "load", "(", "jsn", ")", "\n", "training_files", "=", "d", "[", "'training'", "]", "\n", "", "num_modalities", "=", "len", "(", "d", "[", "'modality'", "]", ".", "keys", "(", ")", ")", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.nestnet.blocks.handle_block_names": [[9, 16], ["None"], "function", ["None"], ["def", "handle_block_names", "(", "stage", ",", "cols", ")", ":", "\n", "    ", "conv_name", "=", "'decoder_stage{}-{}_conv'", ".", "format", "(", "stage", ",", "cols", ")", "\n", "bn_name", "=", "'decoder_stage{}-{}_bn'", ".", "format", "(", "stage", ",", "cols", ")", "\n", "relu_name", "=", "'decoder_stage{}-{}_relu'", ".", "format", "(", "stage", ",", "cols", ")", "\n", "up_name", "=", "'decoder_stage{}-{}_upsample'", ".", "format", "(", "stage", ",", "cols", ")", "\n", "merge_name", "=", "'merge_{}-{}'", ".", "format", "(", "stage", ",", "cols", ")", "\n", "return", "conv_name", ",", "bn_name", ",", "relu_name", ",", "up_name", ",", "merge_name", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.nestnet.blocks.ConvRelu": [[18, 26], ["keras.layers.Conv2D", "keras.layers.Activation", "keras.layers.BatchNormalization"], "function", ["None"], ["", "def", "ConvRelu", "(", "filters", ",", "kernel_size", ",", "use_batchnorm", "=", "False", ",", "conv_name", "=", "'conv'", ",", "bn_name", "=", "'bn'", ",", "relu_name", "=", "'relu'", ")", ":", "\n", "    ", "def", "layer", "(", "x", ")", ":", "\n", "        ", "x", "=", "Conv2D", "(", "filters", ",", "kernel_size", ",", "padding", "=", "\"same\"", ",", "name", "=", "conv_name", ",", "use_bias", "=", "not", "(", "use_batchnorm", ")", ")", "(", "x", ")", "\n", "if", "use_batchnorm", ":", "\n", "            ", "x", "=", "BatchNormalization", "(", "name", "=", "bn_name", ")", "(", "x", ")", "\n", "", "x", "=", "Activation", "(", "'relu'", ",", "name", "=", "relu_name", ")", "(", "x", ")", "\n", "return", "x", "\n", "", "return", "layer", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.nestnet.blocks.Upsample2D_block": [[28, 48], ["blocks.handle_block_names", "keras.layers.UpSampling2D", "blocks.ConvRelu", "blocks.ConvRelu", "keras.layers.Concatenate"], "function", ["home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.unet.blocks.handle_block_names", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.unet.blocks.ConvRelu", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.unet.blocks.ConvRelu"], ["", "def", "Upsample2D_block", "(", "filters", ",", "stage", ",", "cols", ",", "kernel_size", "=", "(", "3", ",", "3", ")", ",", "upsample_rate", "=", "(", "2", ",", "2", ")", ",", "\n", "use_batchnorm", "=", "False", ",", "skip", "=", "None", ")", ":", "\n", "\n", "    ", "def", "layer", "(", "input_tensor", ")", ":", "\n", "\n", "        ", "conv_name", ",", "bn_name", ",", "relu_name", ",", "up_name", ",", "merge_name", "=", "handle_block_names", "(", "stage", ",", "cols", ")", "\n", "\n", "x", "=", "UpSampling2D", "(", "size", "=", "upsample_rate", ",", "name", "=", "up_name", ")", "(", "input_tensor", ")", "\n", "\n", "if", "skip", "is", "not", "None", ":", "\n", "            ", "x", "=", "Concatenate", "(", "name", "=", "merge_name", ")", "(", "[", "x", ",", "skip", "]", ")", "\n", "\n", "", "x", "=", "ConvRelu", "(", "filters", ",", "kernel_size", ",", "use_batchnorm", "=", "use_batchnorm", ",", "\n", "conv_name", "=", "conv_name", "+", "'1'", ",", "bn_name", "=", "bn_name", "+", "'1'", ",", "relu_name", "=", "relu_name", "+", "'1'", ")", "(", "x", ")", "\n", "\n", "x", "=", "ConvRelu", "(", "filters", ",", "kernel_size", ",", "use_batchnorm", "=", "use_batchnorm", ",", "\n", "conv_name", "=", "conv_name", "+", "'2'", ",", "bn_name", "=", "bn_name", "+", "'2'", ",", "relu_name", "=", "relu_name", "+", "'2'", ")", "(", "x", ")", "\n", "\n", "return", "x", "\n", "", "return", "layer", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.nestnet.blocks.Transpose2D_block": [[50, 71], ["blocks.handle_block_names", "keras.layers.Conv2DTranspose", "keras.layers.Activation", "blocks.ConvRelu", "keras.layers.BatchNormalization", "keras.layers.Concatenate"], "function", ["home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.unet.blocks.handle_block_names", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.linknet.blocks.Conv2DTranspose", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.unet.blocks.ConvRelu"], ["", "def", "Transpose2D_block", "(", "filters", ",", "stage", ",", "cols", ",", "kernel_size", "=", "(", "3", ",", "3", ")", ",", "upsample_rate", "=", "(", "2", ",", "2", ")", ",", "\n", "transpose_kernel_size", "=", "(", "4", ",", "4", ")", ",", "use_batchnorm", "=", "False", ",", "skip", "=", "None", ")", ":", "\n", "\n", "    ", "def", "layer", "(", "input_tensor", ")", ":", "\n", "\n", "        ", "conv_name", ",", "bn_name", ",", "relu_name", ",", "up_name", ",", "merge_name", "=", "handle_block_names", "(", "stage", ",", "cols", ")", "\n", "\n", "x", "=", "Conv2DTranspose", "(", "filters", ",", "transpose_kernel_size", ",", "strides", "=", "upsample_rate", ",", "\n", "padding", "=", "'same'", ",", "name", "=", "up_name", ",", "use_bias", "=", "not", "(", "use_batchnorm", ")", ")", "(", "input_tensor", ")", "\n", "if", "use_batchnorm", ":", "\n", "            ", "x", "=", "BatchNormalization", "(", "name", "=", "bn_name", "+", "'1'", ")", "(", "x", ")", "\n", "", "x", "=", "Activation", "(", "'relu'", ",", "name", "=", "relu_name", "+", "'1'", ")", "(", "x", ")", "\n", "\n", "if", "skip", "is", "not", "None", ":", "\n", "            ", "x", "=", "Concatenate", "(", "name", "=", "merge_name", ")", "(", "[", "x", ",", "skip", "]", ")", "\n", "\n", "", "x", "=", "ConvRelu", "(", "filters", ",", "kernel_size", ",", "use_batchnorm", "=", "use_batchnorm", ",", "\n", "conv_name", "=", "conv_name", "+", "'2'", ",", "bn_name", "=", "bn_name", "+", "'2'", ",", "relu_name", "=", "relu_name", "+", "'2'", ")", "(", "x", ")", "\n", "\n", "return", "x", "\n", "", "return", "layer", "", "", ""]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.nestnet.builder.build_nestnet": [[12, 175], ["range", "range", "range", "keras.models.Model", "len", "len", "len", "range", "keras.layers.Conv2D", "keras.layers.Activation", "isinstance", "utils.get_layer_number", "range", "isinstance", "utils.get_layer_number", "range", "utils.to_tuple", "int", "int", "len", "len", "len", "len", "len", "up_block", "up_block"], "function", ["home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.segmentation_models.utils.get_layer_number", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.segmentation_models.utils.get_layer_number", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.segmentation_models.utils.to_tuple"], ["def", "build_nestnet", "(", "backbone", ",", "classes", ",", "skip_connection_layers", ",", "\n", "decoder_filters", "=", "(", "256", ",", "128", ",", "64", ",", "32", ",", "16", ")", ",", "\n", "upsample_rates", "=", "(", "2", ",", "2", ",", "2", ",", "2", ",", "2", ")", ",", "\n", "n_upsample_blocks", "=", "5", ",", "\n", "block_type", "=", "'upsampling'", ",", "\n", "activation", "=", "'sigmoid'", ",", "\n", "use_batchnorm", "=", "True", ")", ":", "\n", "\n", "    ", "input", "=", "backbone", ".", "input", "\n", "# print(n_upsample_blocks)", "\n", "\n", "if", "block_type", "==", "'transpose'", ":", "\n", "        ", "up_block", "=", "Transpose2D_block", "\n", "", "else", ":", "\n", "        ", "up_block", "=", "Upsample2D_block", "\n", "\n", "", "if", "len", "(", "skip_connection_layers", ")", ">", "n_upsample_blocks", ":", "\n", "        ", "downsampling_layers", "=", "skip_connection_layers", "[", "int", "(", "len", "(", "skip_connection_layers", ")", "/", "2", ")", ":", "]", "\n", "skip_connection_layers", "=", "skip_connection_layers", "[", ":", "int", "(", "len", "(", "skip_connection_layers", ")", "/", "2", ")", "]", "\n", "", "else", ":", "\n", "        ", "downsampling_layers", "=", "skip_connection_layers", "\n", "\n", "\n", "# convert layer names to indices", "\n", "", "skip_connection_idx", "=", "(", "[", "get_layer_number", "(", "backbone", ",", "l", ")", "if", "isinstance", "(", "l", ",", "str", ")", "else", "l", "\n", "for", "l", "in", "skip_connection_layers", "]", ")", "\n", "skip_layers_list", "=", "[", "backbone", ".", "layers", "[", "skip_connection_idx", "[", "i", "]", "]", ".", "output", "for", "i", "in", "range", "(", "len", "(", "skip_connection_idx", ")", ")", "]", "\n", "downsampling_idx", "=", "(", "[", "get_layer_number", "(", "backbone", ",", "l", ")", "if", "isinstance", "(", "l", ",", "str", ")", "else", "l", "\n", "for", "l", "in", "downsampling_layers", "]", ")", "\n", "downsampling_list", "=", "[", "backbone", ".", "layers", "[", "downsampling_idx", "[", "i", "]", "]", ".", "output", "for", "i", "in", "range", "(", "len", "(", "downsampling_idx", ")", ")", "]", "\n", "downterm", "=", "[", "None", "]", "*", "(", "n_upsample_blocks", "+", "1", ")", "\n", "for", "i", "in", "range", "(", "len", "(", "downsampling_idx", ")", ")", ":", "\n", "# print(downsampling_list[0])", "\n", "# print(backbone.output)", "\n", "# print(\"\")", "\n", "        ", "if", "downsampling_list", "[", "0", "]", "==", "backbone", ".", "output", ":", "\n", "# print(\"VGG16 should be!\")", "\n", "            ", "downterm", "[", "n_upsample_blocks", "-", "i", "]", "=", "downsampling_list", "[", "i", "]", "\n", "", "else", ":", "\n", "            ", "downterm", "[", "n_upsample_blocks", "-", "i", "-", "1", "]", "=", "downsampling_list", "[", "i", "]", "\n", "", "", "downterm", "[", "-", "1", "]", "=", "backbone", ".", "output", "\n", "# print(\"downterm = {}\".format(downterm))", "\n", "\n", "interm", "=", "[", "None", "]", "*", "(", "n_upsample_blocks", "+", "1", ")", "*", "(", "n_upsample_blocks", "+", "1", ")", "\n", "for", "i", "in", "range", "(", "len", "(", "skip_connection_idx", ")", ")", ":", "\n", "        ", "interm", "[", "-", "i", "*", "(", "n_upsample_blocks", "+", "1", ")", "+", "(", "n_upsample_blocks", "+", "1", ")", "*", "(", "n_upsample_blocks", "-", "1", ")", "]", "=", "skip_layers_list", "[", "i", "]", "\n", "", "interm", "[", "(", "n_upsample_blocks", "+", "1", ")", "*", "n_upsample_blocks", "]", "=", "backbone", ".", "output", "\n", "\n", "for", "j", "in", "range", "(", "n_upsample_blocks", ")", ":", "\n", "        ", "for", "i", "in", "range", "(", "n_upsample_blocks", "-", "j", ")", ":", "\n", "            ", "upsample_rate", "=", "to_tuple", "(", "upsample_rates", "[", "i", "]", ")", "\n", "# print(j, i)", "\n", "\n", "if", "i", "==", "0", "and", "j", "<", "n_upsample_blocks", "-", "1", "and", "len", "(", "skip_connection_layers", ")", "<", "n_upsample_blocks", ":", "\n", "                ", "interm", "[", "(", "n_upsample_blocks", "+", "1", ")", "*", "i", "+", "j", "+", "1", "]", "=", "None", "\n", "", "elif", "j", "==", "0", ":", "\n", "                ", "if", "downterm", "[", "i", "+", "1", "]", "is", "not", "None", ":", "\n", "                    ", "interm", "[", "(", "n_upsample_blocks", "+", "1", ")", "*", "i", "+", "j", "+", "1", "]", "=", "up_block", "(", "decoder_filters", "[", "n_upsample_blocks", "-", "i", "-", "2", "]", ",", "\n", "i", "+", "1", ",", "j", "+", "1", ",", "upsample_rate", "=", "upsample_rate", ",", "\n", "skip", "=", "interm", "[", "(", "n_upsample_blocks", "+", "1", ")", "*", "i", "+", "j", "]", ",", "\n", "use_batchnorm", "=", "use_batchnorm", ")", "(", "downterm", "[", "i", "+", "1", "]", ")", "\n", "", "else", ":", "\n", "                    ", "interm", "[", "(", "n_upsample_blocks", "+", "1", ")", "*", "i", "+", "j", "+", "1", "]", "=", "None", "\n", "# print(\"\\n{} = {} + {}\\n\".format(interm[(n_upsample_blocks+1)*i+j+1],", "\n", "#                             interm[(n_upsample_blocks+1)*i+j], ", "\n", "#                             downterm[i+1]))", "\n", "", "", "else", ":", "\n", "                ", "interm", "[", "(", "n_upsample_blocks", "+", "1", ")", "*", "i", "+", "j", "+", "1", "]", "=", "up_block", "(", "decoder_filters", "[", "n_upsample_blocks", "-", "i", "-", "2", "]", ",", "\n", "i", "+", "1", ",", "j", "+", "1", ",", "upsample_rate", "=", "upsample_rate", ",", "\n", "skip", "=", "interm", "[", "(", "n_upsample_blocks", "+", "1", ")", "*", "i", "+", "j", "]", ",", "\n", "use_batchnorm", "=", "use_batchnorm", ")", "(", "interm", "[", "(", "n_upsample_blocks", "+", "1", ")", "*", "(", "i", "+", "1", ")", "+", "j", "]", ")", "\n", "# print(\"\\n{} = {} + {}\\n\".format(interm[(n_upsample_blocks+1)*i+j+1],", "\n", "#                             interm[(n_upsample_blocks+1)*i+j], ", "\n", "#                             interm[(n_upsample_blocks+1)*(i+1)+j]))", "\n", "# print('\\n\\n\\n')", "\n", "# for x in range(n_upsample_blocks+1):", "\n", "#     for y in range(n_upsample_blocks+1):", "\n", "#         print(interm[x*(n_upsample_blocks+1)+y], end=' ', flush=True)", "\n", "#     print('\\n')", "\n", "# print('\\n\\n\\n')", "\n", "#print(interm)", "\n", "\n", "", "", "", "\"\"\"\n    for i in range(n_upsample_blocks-2):\n        interm = []\n        x = skip_layers_list[n_upsample_blocks-i-2]\n\n    \n    x = {}\n    for stage in range(n_upsample_blocks-1):\n        i = n_upsample_blocks - stage - 1\n        x = backbone.layers[skip_connection_idx[i-1]].output\n        for col in range(stage+1):\n            print(\"i = {}, col = {}, index = {}\".format(i, col, i+col))\n            skip_connection = None\n            if i-col < len(skip_connection_idx):\n                skip_connection = skip_layers_list[i-col]\n            upsample_rate = to_tuple(upsample_rates[i-col])\n            x = up_block(decoder_filters[i-col], stage-col+1, col+1, upsample_rate=upsample_rate,\n                         skip=skip_connection, use_batchnorm=use_batchnorm)(x)\n            skip_layers_list[i+col] = x\n\n    x = backbone.output\n    for i in range(n_upsample_blocks):\n\n        # check if there is a skip connection\n        skip_connection = None\n        if i < len(skip_connection_idx):\n            # skip_connection = backbone.layers[skip_connection_idx[i]].output\n            skip_connection = skip_layers_list[i]\n\n        upsample_rate = to_tuple(upsample_rates[i])\n\n        x = up_block(decoder_filters[i], n_upsample_blocks-i, 0, upsample_rate=upsample_rate,\n                     skip=skip_connection, use_batchnorm=use_batchnorm)(x)\n    \"\"\"", "\n", "\n", "\"\"\"\n    i = n_upsample_blocks - 1\n    xx = backbone.layers[skip_connection_idx[i-0-1]].output\n    skip_connection = skip_layers_list[i-0]\n    upsample_rate = to_tuple(upsample_rates[i-0])\n    xx = up_block(decoder_filters[i-0], n_upsample_blocks-i-0, 1+0, upsample_rate=upsample_rate,\n                 skip=skip_connection, use_batchnorm=use_batchnorm)(xx)\n    skip_layers_list[i-0] = xx\n\n    i = n_upsample_blocks - 2\n    xx = backbone.layers[skip_connection_idx[i-0-1]].output\n\n    skip_connection = skip_layers_list[i-0]\n    upsample_rate = to_tuple(upsample_rates[i-0])\n    xx = up_block(decoder_filters[i-0], n_upsample_blocks-i-0, 1+0, upsample_rate=upsample_rate,\n                 skip=skip_connection, use_batchnorm=use_batchnorm)(xx)\n    skip_layers_list[i-0] = xx\n\n    skip_connection = skip_layers_list[i-1]\n    upsample_rate = to_tuple(upsample_rates[i-1])\n    xx = up_block(decoder_filters[i-1], n_upsample_blocks-i-1, 1+1, upsample_rate=upsample_rate,\n                 skip=skip_connection, use_batchnorm=use_batchnorm)(xx)\n    skip_layers_list[i-1] = xx\n    \"\"\"", "\n", "\n", "\"\"\"\n    for i in range(n_upsample_blocks):\n\n        # check if there is a skip connection\n        skip_connection = None\n        if i < len(skip_connection_idx):\n            # skip_connection = backbone.layers[skip_connection_idx[i]].output\n            skip_connection = skip_layers_list[i]\n\n        upsample_rate = to_tuple(upsample_rates[i])\n\n        x = up_block(decoder_filters[i], n_upsample_blocks-i, 0, upsample_rate=upsample_rate,\n                     skip=skip_connection, use_batchnorm=use_batchnorm)(x)\n    \"\"\"", "\n", "\n", "x", "=", "Conv2D", "(", "classes", ",", "(", "3", ",", "3", ")", ",", "padding", "=", "'same'", ",", "name", "=", "'final_conv'", ")", "(", "interm", "[", "n_upsample_blocks", "]", ")", "\n", "x", "=", "Activation", "(", "activation", ",", "name", "=", "activation", ")", "(", "x", ")", "\n", "\n", "model", "=", "Model", "(", "input", ",", "x", ")", "\n", "\n", "return", "model", "\n", "", ""]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.nestnet.model.Nestnet": [[42, 109], ["backbones.get_backbone", "builder.build_nestnet", "utils.freeze_model"], "function", ["home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.backbones.backbones.get_backbone", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.nestnet.builder.build_nestnet", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.segmentation_models.utils.freeze_model"], ["def", "Nestnet", "(", "backbone_name", "=", "'vgg16'", ",", "\n", "input_shape", "=", "(", "None", ",", "None", ",", "3", ")", ",", "\n", "input_tensor", "=", "None", ",", "\n", "encoder_weights", "=", "'imagenet'", ",", "\n", "freeze_encoder", "=", "False", ",", "\n", "skip_connections", "=", "'default'", ",", "\n", "decoder_block_type", "=", "'upsampling'", ",", "\n", "decoder_filters", "=", "(", "256", ",", "128", ",", "64", ",", "32", ",", "16", ")", ",", "\n", "decoder_use_batchnorm", "=", "True", ",", "\n", "n_upsample_blocks", "=", "5", ",", "\n", "upsample_rates", "=", "(", "2", ",", "2", ",", "2", ",", "2", ",", "2", ")", ",", "\n", "classes", "=", "1", ",", "\n", "activation", "=", "'sigmoid'", ")", ":", "\n", "    ", "\"\"\"\n\n    Args:\n        backbone_name: (str) look at list of available backbones.\n        input_shape:  (tuple) dimensions of input data (H, W, C)\n        input_tensor: keras tensor\n        encoder_weights: one of `None` (random initialization), \n            'imagenet' (pre-training on ImageNet), \n            'dof' (pre-training on DoF)\n        freeze_encoder: (bool) Set encoder layers weights as non-trainable. Useful for fine-tuning\n        skip_connections: if 'default' is used take default skip connections,\n            else provide a list of layer numbers or names starting from top of model\n        decoder_block_type: (str) one of 'upsampling' and 'transpose' (look at blocks.py)\n        decoder_filters: (int) number of convolution layer filters in decoder blocks\n        decoder_use_batchnorm: (bool) if True add batch normalisation layer between `Conv2D` ad `Activation` layers\n        n_upsample_blocks: (int) a number of upsampling blocks\n        upsample_rates: (tuple of int) upsampling rates decoder blocks\n        classes: (int) a number of classes for output\n        activation: (str) one of keras activations for last model layer\n\n    Returns:\n        keras.models.Model instance\n\n    \"\"\"", "\n", "\n", "\n", "\n", "backbone", "=", "get_backbone", "(", "backbone_name", ",", "\n", "input_shape", "=", "input_shape", ",", "\n", "input_tensor", "=", "input_tensor", ",", "\n", "weights", "=", "encoder_weights", ",", "\n", "include_top", "=", "False", ")", "\n", "\n", "if", "skip_connections", "==", "'default'", ":", "\n", "        ", "skip_connections", "=", "DEFAULT_SKIP_CONNECTIONS", "[", "backbone_name", "]", "\n", "# n_upsample_blocks = len(skip_connections)", "\n", "\n", "", "model", "=", "build_nestnet", "(", "backbone", ",", "\n", "classes", ",", "\n", "skip_connections", ",", "\n", "decoder_filters", "=", "decoder_filters", ",", "\n", "block_type", "=", "decoder_block_type", ",", "\n", "activation", "=", "activation", ",", "\n", "n_upsample_blocks", "=", "n_upsample_blocks", ",", "\n", "upsample_rates", "=", "upsample_rates", ",", "\n", "use_batchnorm", "=", "decoder_use_batchnorm", ")", "\n", "\n", "# lock encoder weights for fine-tuning", "\n", "if", "freeze_encoder", ":", "\n", "        ", "freeze_model", "(", "backbone", ")", "\n", "\n", "", "model", ".", "name", "=", "'nest-{}'", ".", "format", "(", "backbone_name", ")", "\n", "\n", "return", "model", "", "", ""]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.xnet.blocks.handle_block_names": [[9, 16], ["None"], "function", ["None"], ["def", "handle_block_names", "(", "stage", ",", "cols", ")", ":", "\n", "    ", "conv_name", "=", "'decoder_stage{}-{}_conv'", ".", "format", "(", "stage", ",", "cols", ")", "\n", "bn_name", "=", "'decoder_stage{}-{}_bn'", ".", "format", "(", "stage", ",", "cols", ")", "\n", "relu_name", "=", "'decoder_stage{}-{}_relu'", ".", "format", "(", "stage", ",", "cols", ")", "\n", "up_name", "=", "'decoder_stage{}-{}_upsample'", ".", "format", "(", "stage", ",", "cols", ")", "\n", "merge_name", "=", "'merge_{}-{}'", ".", "format", "(", "stage", ",", "cols", ")", "\n", "return", "conv_name", ",", "bn_name", ",", "relu_name", ",", "up_name", ",", "merge_name", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.xnet.blocks.ConvRelu": [[18, 26], ["keras.layers.Conv2D", "keras.layers.Activation", "keras.layers.BatchNormalization"], "function", ["None"], ["", "def", "ConvRelu", "(", "filters", ",", "kernel_size", ",", "use_batchnorm", "=", "False", ",", "conv_name", "=", "'conv'", ",", "bn_name", "=", "'bn'", ",", "relu_name", "=", "'relu'", ")", ":", "\n", "    ", "def", "layer", "(", "x", ")", ":", "\n", "        ", "x", "=", "Conv2D", "(", "filters", ",", "kernel_size", ",", "padding", "=", "\"same\"", ",", "name", "=", "conv_name", ",", "use_bias", "=", "not", "(", "use_batchnorm", ")", ")", "(", "x", ")", "\n", "if", "use_batchnorm", ":", "\n", "            ", "x", "=", "BatchNormalization", "(", "name", "=", "bn_name", ")", "(", "x", ")", "\n", "", "x", "=", "Activation", "(", "'relu'", ",", "name", "=", "relu_name", ")", "(", "x", ")", "\n", "return", "x", "\n", "", "return", "layer", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.xnet.blocks.Upsample2D_block": [[28, 51], ["blocks.handle_block_names", "keras.layers.UpSampling2D", "blocks.ConvRelu", "blocks.ConvRelu", "type", "type", "type", "keras.layers.Concatenate", "keras.layers.Concatenate"], "function", ["home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.unet.blocks.handle_block_names", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.unet.blocks.ConvRelu", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.unet.blocks.ConvRelu"], ["", "def", "Upsample2D_block", "(", "filters", ",", "stage", ",", "cols", ",", "kernel_size", "=", "(", "3", ",", "3", ")", ",", "upsample_rate", "=", "(", "2", ",", "2", ")", ",", "\n", "use_batchnorm", "=", "False", ",", "skip", "=", "None", ")", ":", "\n", "\n", "    ", "def", "layer", "(", "input_tensor", ")", ":", "\n", "\n", "        ", "conv_name", ",", "bn_name", ",", "relu_name", ",", "up_name", ",", "merge_name", "=", "handle_block_names", "(", "stage", ",", "cols", ")", "\n", "\n", "x", "=", "UpSampling2D", "(", "size", "=", "upsample_rate", ",", "name", "=", "up_name", ")", "(", "input_tensor", ")", "\n", "\n", "if", "skip", "is", "not", "None", ":", "\n", "            ", "x", "=", "Concatenate", "(", "name", "=", "merge_name", ")", "(", "[", "x", ",", "skip", "]", ")", "\n", "\n", "", "x", "=", "ConvRelu", "(", "filters", ",", "kernel_size", ",", "use_batchnorm", "=", "use_batchnorm", ",", "\n", "conv_name", "=", "conv_name", "+", "'1'", ",", "bn_name", "=", "bn_name", "+", "'1'", ",", "relu_name", "=", "relu_name", "+", "'1'", ")", "(", "x", ")", "\n", "\n", "x", "=", "ConvRelu", "(", "filters", ",", "kernel_size", ",", "use_batchnorm", "=", "use_batchnorm", ",", "\n", "conv_name", "=", "conv_name", "+", "'2'", ",", "bn_name", "=", "bn_name", "+", "'2'", ",", "relu_name", "=", "relu_name", "+", "'2'", ")", "(", "x", ")", "\n", "\n", "return", "x", "\n", "", "return", "layer", "\n", "\n", "\n", "", "def", "Transpose2D_block", "(", "filters", ",", "stage", ",", "cols", ",", "kernel_size", "=", "(", "3", ",", "3", ")", ",", "upsample_rate", "=", "(", "2", ",", "2", ")", ",", "\n", "transpose_kernel_size", "=", "(", "4", ",", "4", ")", ",", "use_batchnorm", "=", "False", ",", "skip", "=", "None", ")", ":", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.xnet.blocks.Transpose2D_block": [[53, 82], ["blocks.handle_block_names", "keras.layers.Conv2DTranspose", "keras.layers.Activation", "blocks.ConvRelu", "keras.layers.BatchNormalization", "type", "merge_list.append", "type", "type", "merge_list.append", "keras.layers.Concatenate", "keras.layers.Concatenate"], "function", ["home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.unet.blocks.handle_block_names", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.linknet.blocks.Conv2DTranspose", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.unet.blocks.ConvRelu"], ["    ", "def", "layer", "(", "input_tensor", ")", ":", "\n", "\n", "        ", "conv_name", ",", "bn_name", ",", "relu_name", ",", "up_name", ",", "merge_name", "=", "handle_block_names", "(", "stage", ",", "cols", ")", "\n", "\n", "x", "=", "Conv2DTranspose", "(", "filters", ",", "transpose_kernel_size", ",", "strides", "=", "upsample_rate", ",", "\n", "padding", "=", "'same'", ",", "name", "=", "up_name", ",", "use_bias", "=", "not", "(", "use_batchnorm", ")", ")", "(", "input_tensor", ")", "\n", "if", "use_batchnorm", ":", "\n", "            ", "x", "=", "BatchNormalization", "(", "name", "=", "bn_name", "+", "'1'", ")", "(", "x", ")", "\n", "", "x", "=", "Activation", "(", "'relu'", ",", "name", "=", "relu_name", "+", "'1'", ")", "(", "x", ")", "\n", "\n", "if", "skip", "is", "not", "None", ":", "\n", "            ", "x", "=", "Concatenate", "(", "name", "=", "merge_name", ")", "(", "[", "x", ",", "skip", "]", ")", "\n", "\n", "", "x", "=", "ConvRelu", "(", "filters", ",", "kernel_size", ",", "use_batchnorm", "=", "use_batchnorm", ",", "\n", "conv_name", "=", "conv_name", "+", "'2'", ",", "bn_name", "=", "bn_name", "+", "'2'", ",", "relu_name", "=", "relu_name", "+", "'2'", ")", "(", "x", ")", "\n", "\n", "return", "x", "\n", "", "return", "layer", "", "", ""]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.xnet.builder.build_xnet": [[12, 176], ["range", "range", "range", "keras.models.Model", "len", "len", "len", "range", "keras.layers.Conv2D", "keras.layers.Activation", "isinstance", "utils.get_layer_number", "range", "isinstance", "utils.get_layer_number", "range", "utils.to_tuple", "int", "int", "len", "len", "len", "len", "len", "up_block", "up_block"], "function", ["home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.segmentation_models.utils.get_layer_number", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.segmentation_models.utils.get_layer_number", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.segmentation_models.utils.to_tuple"], ["def", "build_nestnet", "(", "backbone", ",", "classes", ",", "skip_connection_layers", ",", "\n", "decoder_filters", "=", "(", "256", ",", "128", ",", "64", ",", "32", ",", "16", ")", ",", "\n", "upsample_rates", "=", "(", "2", ",", "2", ",", "2", ",", "2", ",", "2", ")", ",", "\n", "n_upsample_blocks", "=", "5", ",", "\n", "block_type", "=", "'upsampling'", ",", "\n", "activation", "=", "'sigmoid'", ",", "\n", "use_batchnorm", "=", "True", ")", ":", "\n", "\n", "    ", "input", "=", "backbone", ".", "input", "\n", "# print(n_upsample_blocks)", "\n", "\n", "if", "block_type", "==", "'transpose'", ":", "\n", "        ", "up_block", "=", "Transpose2D_block", "\n", "", "else", ":", "\n", "        ", "up_block", "=", "Upsample2D_block", "\n", "\n", "", "if", "len", "(", "skip_connection_layers", ")", ">", "n_upsample_blocks", ":", "\n", "        ", "downsampling_layers", "=", "skip_connection_layers", "[", "int", "(", "len", "(", "skip_connection_layers", ")", "/", "2", ")", ":", "]", "\n", "skip_connection_layers", "=", "skip_connection_layers", "[", ":", "int", "(", "len", "(", "skip_connection_layers", ")", "/", "2", ")", "]", "\n", "", "else", ":", "\n", "        ", "downsampling_layers", "=", "skip_connection_layers", "\n", "\n", "\n", "# convert layer names to indices", "\n", "", "skip_connection_idx", "=", "(", "[", "get_layer_number", "(", "backbone", ",", "l", ")", "if", "isinstance", "(", "l", ",", "str", ")", "else", "l", "\n", "for", "l", "in", "skip_connection_layers", "]", ")", "\n", "skip_layers_list", "=", "[", "backbone", ".", "layers", "[", "skip_connection_idx", "[", "i", "]", "]", ".", "output", "for", "i", "in", "range", "(", "len", "(", "skip_connection_idx", ")", ")", "]", "\n", "downsampling_idx", "=", "(", "[", "get_layer_number", "(", "backbone", ",", "l", ")", "if", "isinstance", "(", "l", ",", "str", ")", "else", "l", "\n", "for", "l", "in", "downsampling_layers", "]", ")", "\n", "downsampling_list", "=", "[", "backbone", ".", "layers", "[", "downsampling_idx", "[", "i", "]", "]", ".", "output", "for", "i", "in", "range", "(", "len", "(", "downsampling_idx", ")", ")", "]", "\n", "downterm", "=", "[", "None", "]", "*", "(", "n_upsample_blocks", "+", "1", ")", "\n", "for", "i", "in", "range", "(", "len", "(", "downsampling_idx", ")", ")", ":", "\n", "# print(downsampling_list[0])", "\n", "# print(backbone.output)", "\n", "# print(\"\")", "\n", "        ", "if", "downsampling_list", "[", "0", "]", "==", "backbone", ".", "output", ":", "\n", "# print(\"VGG16 should be!\")", "\n", "            ", "downterm", "[", "n_upsample_blocks", "-", "i", "]", "=", "downsampling_list", "[", "i", "]", "\n", "", "else", ":", "\n", "            ", "downterm", "[", "n_upsample_blocks", "-", "i", "-", "1", "]", "=", "downsampling_list", "[", "i", "]", "\n", "", "", "downterm", "[", "-", "1", "]", "=", "backbone", ".", "output", "\n", "# print(\"downterm = {}\".format(downterm))", "\n", "\n", "interm", "=", "[", "None", "]", "*", "(", "n_upsample_blocks", "+", "1", ")", "*", "(", "n_upsample_blocks", "+", "1", ")", "\n", "for", "i", "in", "range", "(", "len", "(", "skip_connection_idx", ")", ")", ":", "\n", "        ", "interm", "[", "-", "i", "*", "(", "n_upsample_blocks", "+", "1", ")", "+", "(", "n_upsample_blocks", "+", "1", ")", "*", "(", "n_upsample_blocks", "-", "1", ")", "]", "=", "skip_layers_list", "[", "i", "]", "\n", "", "interm", "[", "(", "n_upsample_blocks", "+", "1", ")", "*", "n_upsample_blocks", "]", "=", "backbone", ".", "output", "\n", "\n", "for", "j", "in", "range", "(", "n_upsample_blocks", ")", ":", "\n", "        ", "for", "i", "in", "range", "(", "n_upsample_blocks", "-", "j", ")", ":", "\n", "            ", "upsample_rate", "=", "to_tuple", "(", "upsample_rates", "[", "i", "]", ")", "\n", "# print(j, i)", "\n", "\n", "if", "i", "==", "0", "and", "j", "<", "n_upsample_blocks", "-", "1", "and", "len", "(", "skip_connection_layers", ")", "<", "n_upsample_blocks", ":", "\n", "                ", "interm", "[", "(", "n_upsample_blocks", "+", "1", ")", "*", "i", "+", "j", "+", "1", "]", "=", "None", "\n", "", "elif", "j", "==", "0", ":", "\n", "                ", "if", "downterm", "[", "i", "+", "1", "]", "is", "not", "None", ":", "\n", "                    ", "interm", "[", "(", "n_upsample_blocks", "+", "1", ")", "*", "i", "+", "j", "+", "1", "]", "=", "up_block", "(", "decoder_filters", "[", "n_upsample_blocks", "-", "i", "-", "2", "]", ",", "\n", "i", "+", "1", ",", "j", "+", "1", ",", "upsample_rate", "=", "upsample_rate", ",", "\n", "skip", "=", "interm", "[", "(", "n_upsample_blocks", "+", "1", ")", "*", "i", "+", "j", "]", ",", "\n", "use_batchnorm", "=", "use_batchnorm", ")", "(", "downterm", "[", "i", "+", "1", "]", ")", "\n", "", "else", ":", "\n", "                    ", "interm", "[", "(", "n_upsample_blocks", "+", "1", ")", "*", "i", "+", "j", "+", "1", "]", "=", "None", "\n", "# print(\"\\n{} = {} + {}\\n\".format(interm[(n_upsample_blocks+1)*i+j+1],", "\n", "#                             interm[(n_upsample_blocks+1)*i+j], ", "\n", "#                             downterm[i+1]))", "\n", "", "", "else", ":", "\n", "                ", "interm", "[", "(", "n_upsample_blocks", "+", "1", ")", "*", "i", "+", "j", "+", "1", "]", "=", "up_block", "(", "decoder_filters", "[", "n_upsample_blocks", "-", "i", "-", "2", "]", ",", "\n", "i", "+", "1", ",", "j", "+", "1", ",", "upsample_rate", "=", "upsample_rate", ",", "\n", "skip", "=", "interm", "[", "(", "n_upsample_blocks", "+", "1", ")", "*", "i", "+", "j", "]", ",", "\n", "use_batchnorm", "=", "use_batchnorm", ")", "(", "interm", "[", "(", "n_upsample_blocks", "+", "1", ")", "*", "(", "i", "+", "1", ")", "+", "j", "]", ")", "\n", "# print(\"\\n{} = {} + {}\\n\".format(interm[(n_upsample_blocks+1)*i+j+1],", "\n", "#                             interm[(n_upsample_blocks+1)*i+j], ", "\n", "#                             interm[(n_upsample_blocks+1)*(i+1)+j]))", "\n", "# print('\\n\\n\\n')", "\n", "# for x in range(n_upsample_blocks+1):", "\n", "#     for y in range(n_upsample_blocks+1):", "\n", "#         print(interm[x*(n_upsample_blocks+1)+y], end=' ', flush=True)", "\n", "#     print('\\n')", "\n", "# print('\\n\\n\\n')", "\n", "#print(interm)", "\n", "\n", "", "", "", "\"\"\"\n    for i in range(n_upsample_blocks-2):\n        interm = []\n        x = skip_layers_list[n_upsample_blocks-i-2]\n\n    \n    x = {}\n    for stage in range(n_upsample_blocks-1):\n        i = n_upsample_blocks - stage - 1\n        x = backbone.layers[skip_connection_idx[i-1]].output\n        for col in range(stage+1):\n            print(\"i = {}, col = {}, index = {}\".format(i, col, i+col))\n            skip_connection = None\n            if i-col < len(skip_connection_idx):\n                skip_connection = skip_layers_list[i-col]\n            upsample_rate = to_tuple(upsample_rates[i-col])\n            x = up_block(decoder_filters[i-col], stage-col+1, col+1, upsample_rate=upsample_rate,\n                         skip=skip_connection, use_batchnorm=use_batchnorm)(x)\n            skip_layers_list[i+col] = x\n\n    x = backbone.output\n    for i in range(n_upsample_blocks):\n\n        # check if there is a skip connection\n        skip_connection = None\n        if i < len(skip_connection_idx):\n            # skip_connection = backbone.layers[skip_connection_idx[i]].output\n            skip_connection = skip_layers_list[i]\n\n        upsample_rate = to_tuple(upsample_rates[i])\n\n        x = up_block(decoder_filters[i], n_upsample_blocks-i, 0, upsample_rate=upsample_rate,\n                     skip=skip_connection, use_batchnorm=use_batchnorm)(x)\n    \"\"\"", "\n", "\n", "\"\"\"\n    i = n_upsample_blocks - 1\n    xx = backbone.layers[skip_connection_idx[i-0-1]].output\n    skip_connection = skip_layers_list[i-0]\n    upsample_rate = to_tuple(upsample_rates[i-0])\n    xx = up_block(decoder_filters[i-0], n_upsample_blocks-i-0, 1+0, upsample_rate=upsample_rate,\n                 skip=skip_connection, use_batchnorm=use_batchnorm)(xx)\n    skip_layers_list[i-0] = xx\n\n    i = n_upsample_blocks - 2\n    xx = backbone.layers[skip_connection_idx[i-0-1]].output\n\n    skip_connection = skip_layers_list[i-0]\n    upsample_rate = to_tuple(upsample_rates[i-0])\n    xx = up_block(decoder_filters[i-0], n_upsample_blocks-i-0, 1+0, upsample_rate=upsample_rate,\n                 skip=skip_connection, use_batchnorm=use_batchnorm)(xx)\n    skip_layers_list[i-0] = xx\n\n    skip_connection = skip_layers_list[i-1]\n    upsample_rate = to_tuple(upsample_rates[i-1])\n    xx = up_block(decoder_filters[i-1], n_upsample_blocks-i-1, 1+1, upsample_rate=upsample_rate,\n                 skip=skip_connection, use_batchnorm=use_batchnorm)(xx)\n    skip_layers_list[i-1] = xx\n    \"\"\"", "\n", "\n", "\"\"\"\n    for i in range(n_upsample_blocks):\n\n        # check if there is a skip connection\n        skip_connection = None\n        if i < len(skip_connection_idx):\n            # skip_connection = backbone.layers[skip_connection_idx[i]].output\n            skip_connection = skip_layers_list[i]\n\n        upsample_rate = to_tuple(upsample_rates[i])\n\n        x = up_block(decoder_filters[i], n_upsample_blocks-i, 0, upsample_rate=upsample_rate,\n                     skip=skip_connection, use_batchnorm=use_batchnorm)(x)\n    \"\"\"", "\n", "\n", "x", "=", "Conv2D", "(", "classes", ",", "(", "3", ",", "3", ")", ",", "padding", "=", "'same'", ",", "name", "=", "'final_conv'", ")", "(", "interm", "[", "n_upsample_blocks", "]", ")", "\n", "x", "=", "Activation", "(", "activation", ",", "name", "=", "activation", ")", "(", "x", ")", "\n", "\n", "model", "=", "Model", "(", "input", ",", "x", ")", "\n", "\n", "return", "model", "\n", "", ""]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.xnet.model.Xnet": [[42, 109], ["backbones.get_backbone", "builder.build_xnet", "utils.freeze_model"], "function", ["home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.backbones.backbones.get_backbone", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.xnet.builder.build_xnet", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.segmentation_models.utils.freeze_model"], ["def", "Nestnet", "(", "backbone_name", "=", "'vgg16'", ",", "\n", "input_shape", "=", "(", "None", ",", "None", ",", "3", ")", ",", "\n", "input_tensor", "=", "None", ",", "\n", "encoder_weights", "=", "'imagenet'", ",", "\n", "freeze_encoder", "=", "False", ",", "\n", "skip_connections", "=", "'default'", ",", "\n", "decoder_block_type", "=", "'upsampling'", ",", "\n", "decoder_filters", "=", "(", "256", ",", "128", ",", "64", ",", "32", ",", "16", ")", ",", "\n", "decoder_use_batchnorm", "=", "True", ",", "\n", "n_upsample_blocks", "=", "5", ",", "\n", "upsample_rates", "=", "(", "2", ",", "2", ",", "2", ",", "2", ",", "2", ")", ",", "\n", "classes", "=", "1", ",", "\n", "activation", "=", "'sigmoid'", ")", ":", "\n", "    ", "\"\"\"\n\n    Args:\n        backbone_name: (str) look at list of available backbones.\n        input_shape:  (tuple) dimensions of input data (H, W, C)\n        input_tensor: keras tensor\n        encoder_weights: one of `None` (random initialization), \n            'imagenet' (pre-training on ImageNet), \n            'dof' (pre-training on DoF)\n        freeze_encoder: (bool) Set encoder layers weights as non-trainable. Useful for fine-tuning\n        skip_connections: if 'default' is used take default skip connections,\n            else provide a list of layer numbers or names starting from top of model\n        decoder_block_type: (str) one of 'upsampling' and 'transpose' (look at blocks.py)\n        decoder_filters: (int) number of convolution layer filters in decoder blocks\n        decoder_use_batchnorm: (bool) if True add batch normalisation layer between `Conv2D` ad `Activation` layers\n        n_upsample_blocks: (int) a number of upsampling blocks\n        upsample_rates: (tuple of int) upsampling rates decoder blocks\n        classes: (int) a number of classes for output\n        activation: (str) one of keras activations for last model layer\n\n    Returns:\n        keras.models.Model instance\n\n    \"\"\"", "\n", "\n", "\n", "\n", "backbone", "=", "get_backbone", "(", "backbone_name", ",", "\n", "input_shape", "=", "input_shape", ",", "\n", "input_tensor", "=", "input_tensor", ",", "\n", "weights", "=", "encoder_weights", ",", "\n", "include_top", "=", "False", ")", "\n", "\n", "if", "skip_connections", "==", "'default'", ":", "\n", "        ", "skip_connections", "=", "DEFAULT_SKIP_CONNECTIONS", "[", "backbone_name", "]", "\n", "# n_upsample_blocks = len(skip_connections)", "\n", "\n", "", "model", "=", "build_nestnet", "(", "backbone", ",", "\n", "classes", ",", "\n", "skip_connections", ",", "\n", "decoder_filters", "=", "decoder_filters", ",", "\n", "block_type", "=", "decoder_block_type", ",", "\n", "activation", "=", "activation", ",", "\n", "n_upsample_blocks", "=", "n_upsample_blocks", ",", "\n", "upsample_rates", "=", "upsample_rates", ",", "\n", "use_batchnorm", "=", "decoder_use_batchnorm", ")", "\n", "\n", "# lock encoder weights for fine-tuning", "\n", "if", "freeze_encoder", ":", "\n", "        ", "freeze_model", "(", "backbone", ")", "\n", "\n", "", "model", ".", "name", "=", "'nest-{}'", ".", "format", "(", "backbone_name", ")", "\n", "\n", "return", "model", "", "", ""]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.pspnet.blocks.InterpBlock": [[13, 50], ["ValueError", "int", "int", "Pool2D", "common.Conv2DBlock", "common.ResizeImage", "numpy.round", "numpy.round"], "function", ["home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.common.blocks.Conv2DBlock"], ["up_name", "=", "'decoder_stage{}-{}_upsample'", ".", "format", "(", "stage", ",", "cols", ")", "\n", "merge_name", "=", "'merge_{}-{}'", ".", "format", "(", "stage", ",", "cols", ")", "\n", "return", "conv_name", ",", "bn_name", ",", "relu_name", ",", "up_name", ",", "merge_name", "\n", "\n", "\n", "", "def", "ConvRelu", "(", "filters", ",", "kernel_size", ",", "use_batchnorm", "=", "False", ",", "conv_name", "=", "'conv'", ",", "bn_name", "=", "'bn'", ",", "relu_name", "=", "'relu'", ")", ":", "\n", "    ", "def", "layer", "(", "x", ")", ":", "\n", "        ", "x", "=", "Conv2D", "(", "filters", ",", "kernel_size", ",", "padding", "=", "\"same\"", ",", "name", "=", "conv_name", ",", "use_bias", "=", "not", "(", "use_batchnorm", ")", ")", "(", "x", ")", "\n", "if", "use_batchnorm", ":", "\n", "            ", "x", "=", "BatchNormalization", "(", "name", "=", "bn_name", ")", "(", "x", ")", "\n", "", "x", "=", "Activation", "(", "'relu'", ",", "name", "=", "relu_name", ")", "(", "x", ")", "\n", "return", "x", "\n", "", "return", "layer", "\n", "\n", "\n", "", "def", "Upsample2D_block", "(", "filters", ",", "stage", ",", "cols", ",", "kernel_size", "=", "(", "3", ",", "3", ")", ",", "upsample_rate", "=", "(", "2", ",", "2", ")", ",", "\n", "use_batchnorm", "=", "False", ",", "skip", "=", "None", ")", ":", "\n", "\n", "    ", "def", "layer", "(", "input_tensor", ")", ":", "\n", "\n", "        ", "conv_name", ",", "bn_name", ",", "relu_name", ",", "up_name", ",", "merge_name", "=", "handle_block_names", "(", "stage", ",", "cols", ")", "\n", "\n", "x", "=", "UpSampling2D", "(", "size", "=", "upsample_rate", ",", "name", "=", "up_name", ")", "(", "input_tensor", ")", "\n", "\n", "if", "skip", "is", "not", "None", ":", "\n", "            ", "x", "=", "Concatenate", "(", "name", "=", "merge_name", ")", "(", "[", "x", ",", "skip", "]", ")", "\n", "\n", "", "x", "=", "ConvRelu", "(", "filters", ",", "kernel_size", ",", "use_batchnorm", "=", "use_batchnorm", ",", "\n", "conv_name", "=", "conv_name", "+", "'1'", ",", "bn_name", "=", "bn_name", "+", "'1'", ",", "relu_name", "=", "relu_name", "+", "'1'", ")", "(", "x", ")", "\n", "\n", "x", "=", "ConvRelu", "(", "filters", ",", "kernel_size", ",", "use_batchnorm", "=", "use_batchnorm", ",", "\n", "conv_name", "=", "conv_name", "+", "'2'", ",", "bn_name", "=", "bn_name", "+", "'2'", ",", "relu_name", "=", "relu_name", "+", "'2'", ")", "(", "x", ")", "\n", "\n", "return", "x", "\n", "", "return", "layer", "\n", "\n", "\n", "", "def", "Transpose2D_block", "(", "filters", ",", "stage", ",", "cols", ",", "kernel_size", "=", "(", "3", ",", "3", ")", ",", "upsample_rate", "=", "(", "2", ",", "2", ")", ",", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.pspnet.blocks.DUC": [[52, 75], ["ValueError", "keras.backend.int_shape", "common.Conv2DBlock", "keras.layers.Permute", "keras.layers.Reshape", "keras.layers.Permute", "keras.layers.Reshape", "keras.layers.Permute"], "function", ["home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.common.functions.int_shape", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.common.blocks.Conv2DBlock"], ["\n", "    ", "def", "layer", "(", "input_tensor", ")", ":", "\n", "\n", "        ", "conv_name", ",", "bn_name", ",", "relu_name", ",", "up_name", ",", "merge_name", "=", "handle_block_names", "(", "stage", ",", "cols", ")", "\n", "\n", "x", "=", "Conv2DTranspose", "(", "filters", ",", "transpose_kernel_size", ",", "strides", "=", "upsample_rate", ",", "\n", "padding", "=", "'same'", ",", "name", "=", "up_name", ",", "use_bias", "=", "not", "(", "use_batchnorm", ")", ")", "(", "input_tensor", ")", "\n", "if", "use_batchnorm", ":", "\n", "            ", "x", "=", "BatchNormalization", "(", "name", "=", "bn_name", "+", "'1'", ")", "(", "x", ")", "\n", "", "x", "=", "Activation", "(", "'relu'", ",", "name", "=", "relu_name", "+", "'1'", ")", "(", "x", ")", "\n", "\n", "if", "skip", "is", "not", "None", ":", "\n", "            ", "x", "=", "Concatenate", "(", "name", "=", "merge_name", ")", "(", "[", "x", ",", "skip", "]", ")", "\n", "\n", "", "x", "=", "ConvRelu", "(", "filters", ",", "kernel_size", ",", "use_batchnorm", "=", "use_batchnorm", ",", "\n", "conv_name", "=", "conv_name", "+", "'2'", ",", "bn_name", "=", "bn_name", "+", "'2'", ",", "relu_name", "=", "relu_name", "+", "'2'", ")", "(", "x", ")", "\n", "\n", "return", "x", "\n", "", "return", "layer", "", "", ""]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.pspnet.blocks.PyramidPoolingModule": [[77, 107], ["_params.update", "keras.backend.int_shape", "blocks.InterpBlock", "blocks.InterpBlock", "blocks.InterpBlock", "blocks.InterpBlock", "keras.layers.Concatenate"], "function", ["home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.common.functions.int_shape", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.pspnet.blocks.InterpBlock", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.pspnet.blocks.InterpBlock", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.pspnet.blocks.InterpBlock", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.pspnet.blocks.InterpBlock"], []], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.pspnet.builder.build_psp": [[23, 64], ["keras.models.Model", "utils.extract_outputs", "blocks.PyramidPoolingModule", "common.Conv2DBlock", "keras.layers.Conv2D", "keras.layers.Activation", "keras.layers.SpatialDropout2D", "common.ResizeImage", "ValueError", "utils.to_tuple", "blocks.DUC", "utils.to_tuple"], "function", ["home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.segmentation_models.utils.extract_outputs", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.pspnet.blocks.PyramidPoolingModule", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.common.blocks.Conv2DBlock", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.segmentation_models.utils.to_tuple", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.pspnet.blocks.DUC", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.segmentation_models.utils.to_tuple"], ["if", "block_type", "==", "'transpose'", ":", "\n", "        ", "up_block", "=", "Transpose2D_block", "\n", "", "else", ":", "\n", "        ", "up_block", "=", "Upsample2D_block", "\n", "\n", "", "if", "len", "(", "skip_connection_layers", ")", ">", "n_upsample_blocks", ":", "\n", "        ", "downsampling_layers", "=", "skip_connection_layers", "[", "int", "(", "len", "(", "skip_connection_layers", ")", "/", "2", ")", ":", "]", "\n", "skip_connection_layers", "=", "skip_connection_layers", "[", ":", "int", "(", "len", "(", "skip_connection_layers", ")", "/", "2", ")", "]", "\n", "", "else", ":", "\n", "        ", "downsampling_layers", "=", "skip_connection_layers", "\n", "\n", "\n", "# convert layer names to indices", "\n", "", "skip_connection_idx", "=", "(", "[", "get_layer_number", "(", "backbone", ",", "l", ")", "if", "isinstance", "(", "l", ",", "str", ")", "else", "l", "\n", "for", "l", "in", "skip_connection_layers", "]", ")", "\n", "skip_layers_list", "=", "[", "backbone", ".", "layers", "[", "skip_connection_idx", "[", "i", "]", "]", ".", "output", "for", "i", "in", "range", "(", "len", "(", "skip_connection_idx", ")", ")", "]", "\n", "downsampling_idx", "=", "(", "[", "get_layer_number", "(", "backbone", ",", "l", ")", "if", "isinstance", "(", "l", ",", "str", ")", "else", "l", "\n", "for", "l", "in", "downsampling_layers", "]", ")", "\n", "downsampling_list", "=", "[", "backbone", ".", "layers", "[", "downsampling_idx", "[", "i", "]", "]", ".", "output", "for", "i", "in", "range", "(", "len", "(", "downsampling_idx", ")", ")", "]", "\n", "downterm", "=", "[", "None", "]", "*", "(", "n_upsample_blocks", "+", "1", ")", "\n", "for", "i", "in", "range", "(", "len", "(", "downsampling_idx", ")", ")", ":", "\n", "# print(downsampling_list[0])", "\n", "# print(backbone.output)", "\n", "# print(\"\")", "\n", "        ", "if", "downsampling_list", "[", "0", "]", "==", "backbone", ".", "output", ":", "\n", "# print(\"VGG16 should be!\")", "\n", "            ", "downterm", "[", "n_upsample_blocks", "-", "i", "]", "=", "downsampling_list", "[", "i", "]", "\n", "", "else", ":", "\n", "            ", "downterm", "[", "n_upsample_blocks", "-", "i", "-", "1", "]", "=", "downsampling_list", "[", "i", "]", "\n", "", "", "downterm", "[", "-", "1", "]", "=", "backbone", ".", "output", "\n", "# print(\"downterm = {}\".format(downterm))", "\n", "\n", "interm", "=", "[", "None", "]", "*", "(", "n_upsample_blocks", "+", "1", ")", "*", "(", "n_upsample_blocks", "+", "1", ")", "\n", "for", "i", "in", "range", "(", "len", "(", "skip_connection_idx", ")", ")", ":", "\n", "        ", "interm", "[", "-", "i", "*", "(", "n_upsample_blocks", "+", "1", ")", "+", "(", "n_upsample_blocks", "+", "1", ")", "*", "(", "n_upsample_blocks", "-", "1", ")", "]", "=", "skip_layers_list", "[", "i", "]", "\n", "", "interm", "[", "(", "n_upsample_blocks", "+", "1", ")", "*", "n_upsample_blocks", "]", "=", "backbone", ".", "output", "\n", "\n", "for", "j", "in", "range", "(", "n_upsample_blocks", ")", ":", "\n", "        ", "for", "i", "in", "range", "(", "n_upsample_blocks", "-", "j", ")", ":", "\n", "            ", "upsample_rate", "=", "to_tuple", "(", "upsample_rates", "[", "i", "]", ")", "\n", "# print(j, i)", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.pspnet.model._get_layer_by_factor": [[23, 33], ["ValueError"], "function", ["None"], ["'relu1'", ",", "'stage3_unit2_relu1'", ",", "'stage2_unit2_relu1'", ",", "'stage1_unit2_relu1'", ",", "\n", ")", ",", "\n", "'resnet152'", ":", "(", "'stage4_unit1_relu1'", ",", "'stage3_unit1_relu1'", ",", "'stage2_unit1_relu1'", ",", "'relu0'", ",", "\n", "'relu1'", ",", "'stage3_unit2_relu1'", ",", "'stage2_unit2_relu1'", ",", "'stage1_unit2_relu1'", ",", "\n", ")", ",", "\n", "'resnext50'", ":", "(", "'stage4_unit1_relu1'", ",", "'stage3_unit1_relu1'", ",", "'stage2_unit1_relu1'", ",", "'relu0'", ",", "\n", "'stage4_unit1_relu1'", ",", "'stage3_unit2_relu1'", ",", "'stage2_unit2_relu1'", ",", "'stage1_unit2_relu1'", ",", "\n", ")", ",", "\n", "'resnext101'", ":", "(", "'stage4_unit1_relu1'", ",", "'stage3_unit1_relu1'", ",", "'stage2_unit1_relu1'", ",", "'relu0'", ",", "\n", "'stage4_unit1_relu1'", ",", "'stage3_unit2_relu1'", ",", "'stage2_unit2_relu1'", ",", "'stage1_unit2_relu1'", ",", "\n", ")", ",", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.pspnet.model._shape_guard": [[35, 45], ["ValueError"], "function", ["None"], ["'inceptionresnetv2'", ":", "(", "594", ",", "260", ",", "16", ",", "9", ")", ",", "\n", "'densenet121'", ":", "(", "311", ",", "139", ",", "51", ",", "4", ")", ",", "\n", "'densenet169'", ":", "(", "367", ",", "139", ",", "51", ",", "4", ")", ",", "\n", "'densenet201'", ":", "(", "479", ",", "139", ",", "51", ",", "4", ")", ",", "\n", "}", "\n", "\n", "\n", "def", "Nestnet", "(", "backbone_name", "=", "'vgg16'", ",", "\n", "input_shape", "=", "(", "None", ",", "None", ",", "3", ")", ",", "\n", "input_tensor", "=", "None", ",", "\n", "encoder_weights", "=", "'imagenet'", ",", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.pspnet.model.PSPNet": [[47, 122], ["model._shape_guard", "backbones.get_backbone", "model._get_layer_by_factor", "builder.build_psp", "utils.freeze_model"], "function", ["home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.pspnet.model._shape_guard", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.backbones.backbones.get_backbone", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.pspnet.model._get_layer_by_factor", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.pspnet.builder.build_psp", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.segmentation_models.utils.freeze_model"], ["skip_connections", "=", "'default'", ",", "\n", "decoder_block_type", "=", "'upsampling'", ",", "\n", "decoder_filters", "=", "(", "256", ",", "128", ",", "64", ",", "32", ",", "16", ")", ",", "\n", "decoder_use_batchnorm", "=", "True", ",", "\n", "n_upsample_blocks", "=", "5", ",", "\n", "upsample_rates", "=", "(", "2", ",", "2", ",", "2", ",", "2", ",", "2", ")", ",", "\n", "classes", "=", "1", ",", "\n", "activation", "=", "'sigmoid'", ")", ":", "\n", "    ", "\"\"\"\n\n    Args:\n        backbone_name: (str) look at list of available backbones.\n        input_shape:  (tuple) dimensions of input data (H, W, C)\n        input_tensor: keras tensor\n        encoder_weights: one of `None` (random initialization), \n            'imagenet' (pre-training on ImageNet), \n            'dof' (pre-training on DoF)\n        freeze_encoder: (bool) Set encoder layers weights as non-trainable. Useful for fine-tuning\n        skip_connections: if 'default' is used take default skip connections,\n            else provide a list of layer numbers or names starting from top of model\n        decoder_block_type: (str) one of 'upsampling' and 'transpose' (look at blocks.py)\n        decoder_filters: (int) number of convolution layer filters in decoder blocks\n        decoder_use_batchnorm: (bool) if True add batch normalisation layer between `Conv2D` ad `Activation` layers\n        n_upsample_blocks: (int) a number of upsampling blocks\n        upsample_rates: (tuple of int) upsampling rates decoder blocks\n        classes: (int) a number of classes for output\n        activation: (str) one of keras activations for last model layer\n\n    Returns:\n        keras.models.Model instance\n\n    \"\"\"", "\n", "\n", "\n", "\n", "backbone", "=", "get_backbone", "(", "backbone_name", ",", "\n", "input_shape", "=", "input_shape", ",", "\n", "input_tensor", "=", "input_tensor", ",", "\n", "weights", "=", "encoder_weights", ",", "\n", "include_top", "=", "False", ")", "\n", "\n", "if", "skip_connections", "==", "'default'", ":", "\n", "        ", "skip_connections", "=", "DEFAULT_SKIP_CONNECTIONS", "[", "backbone_name", "]", "\n", "# n_upsample_blocks = len(skip_connections)", "\n", "\n", "", "model", "=", "build_nestnet", "(", "backbone", ",", "\n", "classes", ",", "\n", "skip_connections", ",", "\n", "decoder_filters", "=", "decoder_filters", ",", "\n", "block_type", "=", "decoder_block_type", ",", "\n", "activation", "=", "activation", ",", "\n", "n_upsample_blocks", "=", "n_upsample_blocks", ",", "\n", "upsample_rates", "=", "upsample_rates", ",", "\n", "use_batchnorm", "=", "decoder_use_batchnorm", ")", "\n", "\n", "# lock encoder weights for fine-tuning", "\n", "if", "freeze_encoder", ":", "\n", "        ", "freeze_model", "(", "backbone", ")", "\n", "\n", "", "model", ".", "name", "=", "'nest-{}'", ".", "format", "(", "backbone_name", ")", "\n", "\n", "return", "model", "", "", ""]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.linknet.blocks.handle_block_names": [[10, 16], ["None"], "function", ["None"], ["    ", "conv_name", "=", "'decoder_stage{}-{}_conv'", ".", "format", "(", "stage", ",", "cols", ")", "\n", "bn_name", "=", "'decoder_stage{}-{}_bn'", ".", "format", "(", "stage", ",", "cols", ")", "\n", "relu_name", "=", "'decoder_stage{}-{}_relu'", ".", "format", "(", "stage", ",", "cols", ")", "\n", "up_name", "=", "'decoder_stage{}-{}_upsample'", ".", "format", "(", "stage", ",", "cols", ")", "\n", "merge_name", "=", "'merge_{}-{}'", ".", "format", "(", "stage", ",", "cols", ")", "\n", "return", "conv_name", ",", "bn_name", ",", "relu_name", ",", "up_name", ",", "merge_name", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.linknet.blocks.ConvRelu": [[18, 40], ["keras.layers.Conv2D", "keras.layers.Activation", "keras.layers.BatchNormalization"], "function", ["None"], ["", "def", "ConvRelu", "(", "filters", ",", "kernel_size", ",", "use_batchnorm", "=", "False", ",", "conv_name", "=", "'conv'", ",", "bn_name", "=", "'bn'", ",", "relu_name", "=", "'relu'", ")", ":", "\n", "    ", "def", "layer", "(", "x", ")", ":", "\n", "        ", "x", "=", "Conv2D", "(", "filters", ",", "kernel_size", ",", "padding", "=", "\"same\"", ",", "name", "=", "conv_name", ",", "use_bias", "=", "not", "(", "use_batchnorm", ")", ")", "(", "x", ")", "\n", "if", "use_batchnorm", ":", "\n", "            ", "x", "=", "BatchNormalization", "(", "name", "=", "bn_name", ")", "(", "x", ")", "\n", "", "x", "=", "Activation", "(", "'relu'", ",", "name", "=", "relu_name", ")", "(", "x", ")", "\n", "return", "x", "\n", "", "return", "layer", "\n", "\n", "\n", "", "def", "Upsample2D_block", "(", "filters", ",", "stage", ",", "cols", ",", "kernel_size", "=", "(", "3", ",", "3", ")", ",", "upsample_rate", "=", "(", "2", ",", "2", ")", ",", "\n", "use_batchnorm", "=", "False", ",", "skip", "=", "None", ")", ":", "\n", "\n", "    ", "def", "layer", "(", "input_tensor", ")", ":", "\n", "\n", "        ", "conv_name", ",", "bn_name", ",", "relu_name", ",", "up_name", ",", "merge_name", "=", "handle_block_names", "(", "stage", ",", "cols", ")", "\n", "\n", "x", "=", "UpSampling2D", "(", "size", "=", "upsample_rate", ",", "name", "=", "up_name", ")", "(", "input_tensor", ")", "\n", "\n", "if", "skip", "is", "not", "None", ":", "\n", "            ", "x", "=", "Concatenate", "(", "name", "=", "merge_name", ")", "(", "[", "x", ",", "skip", "]", ")", "\n", "\n", "", "x", "=", "ConvRelu", "(", "filters", ",", "kernel_size", ",", "use_batchnorm", "=", "use_batchnorm", ",", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.linknet.blocks.Conv2DUpsample": [[42, 58], ["keras.layers.UpSampling2D", "keras.layers.Conv2D"], "function", ["None"], ["\n", "x", "=", "ConvRelu", "(", "filters", ",", "kernel_size", ",", "use_batchnorm", "=", "use_batchnorm", ",", "\n", "conv_name", "=", "conv_name", "+", "'2'", ",", "bn_name", "=", "bn_name", "+", "'2'", ",", "relu_name", "=", "relu_name", "+", "'2'", ")", "(", "x", ")", "\n", "\n", "return", "x", "\n", "", "return", "layer", "\n", "\n", "\n", "", "def", "Transpose2D_block", "(", "filters", ",", "stage", ",", "cols", ",", "kernel_size", "=", "(", "3", ",", "3", ")", ",", "upsample_rate", "=", "(", "2", ",", "2", ")", ",", "\n", "transpose_kernel_size", "=", "(", "4", ",", "4", ")", ",", "use_batchnorm", "=", "False", ",", "skip", "=", "None", ")", ":", "\n", "\n", "    ", "def", "layer", "(", "input_tensor", ")", ":", "\n", "\n", "        ", "conv_name", ",", "bn_name", ",", "relu_name", ",", "up_name", ",", "merge_name", "=", "handle_block_names", "(", "stage", ",", "cols", ")", "\n", "\n", "x", "=", "Conv2DTranspose", "(", "filters", ",", "transpose_kernel_size", ",", "strides", "=", "upsample_rate", ",", "\n", "padding", "=", "'same'", ",", "name", "=", "up_name", ",", "use_bias", "=", "not", "(", "use_batchnorm", ")", ")", "(", "input_tensor", ")", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.linknet.blocks.Conv2DTranspose": [[60, 78], ["NotImplementedError", "tuple", "keras.layers.Conv2DTranspose"], "function", ["home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.linknet.blocks.Conv2DTranspose"], ["            ", "x", "=", "BatchNormalization", "(", "name", "=", "bn_name", "+", "'1'", ")", "(", "x", ")", "\n", "", "x", "=", "Activation", "(", "'relu'", ",", "name", "=", "relu_name", "+", "'1'", ")", "(", "x", ")", "\n", "\n", "if", "skip", "is", "not", "None", ":", "\n", "            ", "x", "=", "Concatenate", "(", "name", "=", "merge_name", ")", "(", "[", "x", ",", "skip", "]", ")", "\n", "\n", "", "x", "=", "ConvRelu", "(", "filters", ",", "kernel_size", ",", "use_batchnorm", "=", "use_batchnorm", ",", "\n", "conv_name", "=", "conv_name", "+", "'2'", ",", "bn_name", "=", "bn_name", "+", "'2'", ",", "relu_name", "=", "relu_name", "+", "'2'", ")", "(", "x", ")", "\n", "\n", "return", "x", "\n", "", "return", "layer", "", "", ""]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.linknet.blocks.UpsampleBlock": [[80, 117], ["ValueError", "UpBlock", "keras.layers.Activation", "keras.layers.BatchNormalization"], "function", ["None"], []], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.linknet.blocks.DecoderBlock": [[119, 166], ["blocks.handle_block_names", "keras.int_shape", "blocks.ConvRelu", "blocks.UpsampleBlock", "blocks.ConvRelu", "keras.int_shape", "keras.layers.Add"], "function", ["home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.unet.blocks.handle_block_names", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.common.functions.int_shape", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.unet.blocks.ConvRelu", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.linknet.blocks.UpsampleBlock", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.unet.blocks.ConvRelu", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.common.functions.int_shape"], []], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.linknet.builder.build_linknet": [[9, 50], ["range", "keras.models.Model", "utils.to_tuple", "keras.layers.Conv2D", "keras.layers.Activation", "isinstance", "utils.get_layer_number", "len", "blocks.DecoderBlock"], "function", ["home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.segmentation_models.utils.to_tuple", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.segmentation_models.utils.get_layer_number", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.linknet.blocks.DecoderBlock"], ["import", "copy", "\n", "\n", "\n", "def", "build_nestnet", "(", "backbone", ",", "classes", ",", "skip_connection_layers", ",", "\n", "decoder_filters", "=", "(", "256", ",", "128", ",", "64", ",", "32", ",", "16", ")", ",", "\n", "upsample_rates", "=", "(", "2", ",", "2", ",", "2", ",", "2", ",", "2", ")", ",", "\n", "n_upsample_blocks", "=", "5", ",", "\n", "block_type", "=", "'upsampling'", ",", "\n", "activation", "=", "'sigmoid'", ",", "\n", "use_batchnorm", "=", "True", ")", ":", "\n", "\n", "    ", "input", "=", "backbone", ".", "input", "\n", "# print(n_upsample_blocks)", "\n", "\n", "if", "block_type", "==", "'transpose'", ":", "\n", "        ", "up_block", "=", "Transpose2D_block", "\n", "", "else", ":", "\n", "        ", "up_block", "=", "Upsample2D_block", "\n", "\n", "", "if", "len", "(", "skip_connection_layers", ")", ">", "n_upsample_blocks", ":", "\n", "        ", "downsampling_layers", "=", "skip_connection_layers", "[", "int", "(", "len", "(", "skip_connection_layers", ")", "/", "2", ")", ":", "]", "\n", "skip_connection_layers", "=", "skip_connection_layers", "[", ":", "int", "(", "len", "(", "skip_connection_layers", ")", "/", "2", ")", "]", "\n", "", "else", ":", "\n", "        ", "downsampling_layers", "=", "skip_connection_layers", "\n", "\n", "\n", "# convert layer names to indices", "\n", "", "skip_connection_idx", "=", "(", "[", "get_layer_number", "(", "backbone", ",", "l", ")", "if", "isinstance", "(", "l", ",", "str", ")", "else", "l", "\n", "for", "l", "in", "skip_connection_layers", "]", ")", "\n", "skip_layers_list", "=", "[", "backbone", ".", "layers", "[", "skip_connection_idx", "[", "i", "]", "]", ".", "output", "for", "i", "in", "range", "(", "len", "(", "skip_connection_idx", ")", ")", "]", "\n", "downsampling_idx", "=", "(", "[", "get_layer_number", "(", "backbone", ",", "l", ")", "if", "isinstance", "(", "l", ",", "str", ")", "else", "l", "\n", "for", "l", "in", "downsampling_layers", "]", ")", "\n", "downsampling_list", "=", "[", "backbone", ".", "layers", "[", "downsampling_idx", "[", "i", "]", "]", ".", "output", "for", "i", "in", "range", "(", "len", "(", "downsampling_idx", ")", ")", "]", "\n", "downterm", "=", "[", "None", "]", "*", "(", "n_upsample_blocks", "+", "1", ")", "\n", "for", "i", "in", "range", "(", "len", "(", "downsampling_idx", ")", ")", ":", "\n", "# print(downsampling_list[0])", "\n", "# print(backbone.output)", "\n", "# print(\"\")", "\n", "        ", "if", "downsampling_list", "[", "0", "]", "==", "backbone", ".", "output", ":", "\n", "# print(\"VGG16 should be!\")", "\n", "            ", "downterm", "[", "n_upsample_blocks", "-", "i", "]", "=", "downsampling_list", "[", "i", "]", "\n", "", "else", ":", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.linknet.model.Linknet": [[24, 90], ["backbones.get_backbone", "builder.build_linknet", "utils.freeze_model"], "function", ["home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.backbones.backbones.get_backbone", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.linknet.builder.build_linknet", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.segmentation_models.utils.freeze_model"], [")", ",", "\n", "'resnet152'", ":", "(", "'stage4_unit1_relu1'", ",", "'stage3_unit1_relu1'", ",", "'stage2_unit1_relu1'", ",", "'relu0'", ",", "\n", "'relu1'", ",", "'stage3_unit2_relu1'", ",", "'stage2_unit2_relu1'", ",", "'stage1_unit2_relu1'", ",", "\n", ")", ",", "\n", "'resnext50'", ":", "(", "'stage4_unit1_relu1'", ",", "'stage3_unit1_relu1'", ",", "'stage2_unit1_relu1'", ",", "'relu0'", ",", "\n", "'stage4_unit1_relu1'", ",", "'stage3_unit2_relu1'", ",", "'stage2_unit2_relu1'", ",", "'stage1_unit2_relu1'", ",", "\n", ")", ",", "\n", "'resnext101'", ":", "(", "'stage4_unit1_relu1'", ",", "'stage3_unit1_relu1'", ",", "'stage2_unit1_relu1'", ",", "'relu0'", ",", "\n", "'stage4_unit1_relu1'", ",", "'stage3_unit2_relu1'", ",", "'stage2_unit2_relu1'", ",", "'stage1_unit2_relu1'", ",", "\n", ")", ",", "\n", "'inceptionv3'", ":", "(", "228", ",", "86", ",", "16", ",", "9", ")", ",", "\n", "'inceptionresnetv2'", ":", "(", "594", ",", "260", ",", "16", ",", "9", ")", ",", "\n", "'densenet121'", ":", "(", "311", ",", "139", ",", "51", ",", "4", ")", ",", "\n", "'densenet169'", ":", "(", "367", ",", "139", ",", "51", ",", "4", ")", ",", "\n", "'densenet201'", ":", "(", "479", ",", "139", ",", "51", ",", "4", ")", ",", "\n", "}", "\n", "\n", "\n", "def", "Nestnet", "(", "backbone_name", "=", "'vgg16'", ",", "\n", "input_shape", "=", "(", "None", ",", "None", ",", "3", ")", ",", "\n", "input_tensor", "=", "None", ",", "\n", "encoder_weights", "=", "'imagenet'", ",", "\n", "freeze_encoder", "=", "False", ",", "\n", "skip_connections", "=", "'default'", ",", "\n", "decoder_block_type", "=", "'upsampling'", ",", "\n", "decoder_filters", "=", "(", "256", ",", "128", ",", "64", ",", "32", ",", "16", ")", ",", "\n", "decoder_use_batchnorm", "=", "True", ",", "\n", "n_upsample_blocks", "=", "5", ",", "\n", "upsample_rates", "=", "(", "2", ",", "2", ",", "2", ",", "2", ",", "2", ")", ",", "\n", "classes", "=", "1", ",", "\n", "activation", "=", "'sigmoid'", ")", ":", "\n", "    ", "\"\"\"\n\n    Args:\n        backbone_name: (str) look at list of available backbones.\n        input_shape:  (tuple) dimensions of input data (H, W, C)\n        input_tensor: keras tensor\n        encoder_weights: one of `None` (random initialization), \n            'imagenet' (pre-training on ImageNet), \n            'dof' (pre-training on DoF)\n        freeze_encoder: (bool) Set encoder layers weights as non-trainable. Useful for fine-tuning\n        skip_connections: if 'default' is used take default skip connections,\n            else provide a list of layer numbers or names starting from top of model\n        decoder_block_type: (str) one of 'upsampling' and 'transpose' (look at blocks.py)\n        decoder_filters: (int) number of convolution layer filters in decoder blocks\n        decoder_use_batchnorm: (bool) if True add batch normalisation layer between `Conv2D` ad `Activation` layers\n        n_upsample_blocks: (int) a number of upsampling blocks\n        upsample_rates: (tuple of int) upsampling rates decoder blocks\n        classes: (int) a number of classes for output\n        activation: (str) one of keras activations for last model layer\n\n    Returns:\n        keras.models.Model instance\n\n    \"\"\"", "\n", "\n", "\n", "\n", "backbone", "=", "get_backbone", "(", "backbone_name", ",", "\n", "input_shape", "=", "input_shape", ",", "\n", "input_tensor", "=", "input_tensor", ",", "\n", "weights", "=", "encoder_weights", ",", "\n", "include_top", "=", "False", ")", "\n", "\n", "if", "skip_connections", "==", "'default'", ":", "\n", "        ", "skip_connections", "=", "DEFAULT_SKIP_CONNECTIONS", "[", "backbone_name", "]", "\n", "# n_upsample_blocks = len(skip_connections)", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.backbones.inception_v3.conv2d_bn": [[45, 85], ["keras.backend.image_data_format", "keras.layers.Conv2D", "keras.layers.BatchNormalization", "keras.layers.Activation"], "function", ["None"], ["def", "conv2d_bn", "(", "x", ",", "\n", "filters", ",", "\n", "num_row", ",", "\n", "num_col", ",", "\n", "padding", "=", "'same'", ",", "\n", "strides", "=", "(", "1", ",", "1", ")", ",", "\n", "name", "=", "None", ")", ":", "\n", "    ", "\"\"\"Utility function to apply conv + BN.\n    # Arguments\n        x: input tensor.\n        filters: filters in `Conv2D`.\n        num_row: height of the convolution kernel.\n        num_col: width of the convolution kernel.\n        padding: padding mode in `Conv2D`.\n        strides: strides in `Conv2D`.\n        name: name of the ops; will become `name + '_conv'`\n            for the convolution and `name + '_bn'` for the\n            batch norm layer.\n    # Returns\n        Output tensor after applying `Conv2D` and `BatchNormalization`.\n    \"\"\"", "\n", "if", "name", "is", "not", "None", ":", "\n", "        ", "bn_name", "=", "name", "+", "'_bn'", "\n", "conv_name", "=", "name", "+", "'_conv'", "\n", "", "else", ":", "\n", "        ", "bn_name", "=", "None", "\n", "conv_name", "=", "None", "\n", "", "if", "K", ".", "image_data_format", "(", ")", "==", "'channels_first'", ":", "\n", "        ", "bn_axis", "=", "1", "\n", "", "else", ":", "\n", "        ", "bn_axis", "=", "3", "\n", "", "x", "=", "Conv2D", "(", "\n", "filters", ",", "(", "num_row", ",", "num_col", ")", ",", "\n", "strides", "=", "strides", ",", "\n", "padding", "=", "padding", ",", "\n", "use_bias", "=", "False", ",", "\n", "name", "=", "conv_name", ")", "(", "x", ")", "\n", "x", "=", "BatchNormalization", "(", "axis", "=", "bn_axis", ",", "scale", "=", "False", ",", "name", "=", "bn_name", ")", "(", "x", ")", "\n", "x", "=", "Activation", "(", "'relu'", ",", "name", "=", "name", ")", "(", "x", ")", "\n", "return", "x", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.backbones.inception_v3.InceptionV3": [[87, 393], ["_obtain_input_shape", "inception_v3.conv2d_bn", "inception_v3.conv2d_bn", "inception_v3.conv2d_bn", "inception_v3.conv2d_bn", "inception_v3.conv2d_bn", "inception_v3.conv2d_bn", "inception_v3.conv2d_bn", "inception_v3.conv2d_bn", "inception_v3.conv2d_bn", "inception_v3.conv2d_bn", "inception_v3.conv2d_bn", "inception_v3.conv2d_bn", "keras.layers.concatenate", "inception_v3.conv2d_bn", "inception_v3.conv2d_bn", "inception_v3.conv2d_bn", "inception_v3.conv2d_bn", "inception_v3.conv2d_bn", "inception_v3.conv2d_bn", "inception_v3.conv2d_bn", "keras.layers.concatenate", "inception_v3.conv2d_bn", "inception_v3.conv2d_bn", "inception_v3.conv2d_bn", "inception_v3.conv2d_bn", "inception_v3.conv2d_bn", "inception_v3.conv2d_bn", "inception_v3.conv2d_bn", "keras.layers.concatenate", "inception_v3.conv2d_bn", "inception_v3.conv2d_bn", "inception_v3.conv2d_bn", "inception_v3.conv2d_bn", "keras.layers.concatenate", "inception_v3.conv2d_bn", "inception_v3.conv2d_bn", "inception_v3.conv2d_bn", "inception_v3.conv2d_bn", "inception_v3.conv2d_bn", "inception_v3.conv2d_bn", "inception_v3.conv2d_bn", "inception_v3.conv2d_bn", "inception_v3.conv2d_bn", "inception_v3.conv2d_bn", "keras.layers.concatenate", "range", "inception_v3.conv2d_bn", "inception_v3.conv2d_bn", "inception_v3.conv2d_bn", "inception_v3.conv2d_bn", "inception_v3.conv2d_bn", "inception_v3.conv2d_bn", "inception_v3.conv2d_bn", "inception_v3.conv2d_bn", "inception_v3.conv2d_bn", "inception_v3.conv2d_bn", "keras.layers.concatenate", "inception_v3.conv2d_bn", "inception_v3.conv2d_bn", "inception_v3.conv2d_bn", "inception_v3.conv2d_bn", "inception_v3.conv2d_bn", "inception_v3.conv2d_bn", "keras.layers.concatenate", "range", "keras.models.Model", "ValueError", "ValueError", "keras.layers.Input", "keras.backend.image_data_format", "keras.layers.MaxPooling2D", "keras.layers.MaxPooling2D", "keras.layers.AveragePooling2D", "keras.layers.AveragePooling2D", "keras.layers.AveragePooling2D", "keras.layers.MaxPooling2D", "keras.layers.AveragePooling2D", "inception_v3.conv2d_bn", "inception_v3.conv2d_bn", "inception_v3.conv2d_bn", "inception_v3.conv2d_bn", "inception_v3.conv2d_bn", "inception_v3.conv2d_bn", "inception_v3.conv2d_bn", "inception_v3.conv2d_bn", "inception_v3.conv2d_bn", "inception_v3.conv2d_bn", "keras.layers.concatenate", "keras.layers.AveragePooling2D", "keras.layers.MaxPooling2D", "inception_v3.conv2d_bn", "inception_v3.conv2d_bn", "inception_v3.conv2d_bn", "inception_v3.conv2d_bn", "keras.layers.concatenate", "inception_v3.conv2d_bn", "inception_v3.conv2d_bn", "inception_v3.conv2d_bn", "inception_v3.conv2d_bn", "keras.layers.concatenate", "inception_v3.conv2d_bn", "keras.layers.concatenate", "keras.engine.topology.get_source_inputs", "keras.models.Model.load_weights", "os.path.exists", "keras.backend.image_data_format", "keras.backend.is_keras_tensor", "keras.layers.Input", "keras.layers.AveragePooling2D", "keras.layers.AveragePooling2D", "keras.layers.GlobalAveragePooling2D", "keras.layers.Dense", "keras.backend.image_data_format", "keras.utils.data_utils.get_file", "keras.utils.data_utils.get_file", "keras.models.Model.load_weights", "keras.layers.GlobalAveragePooling2D", "keras.backend.backend", "warnings.warn", "str", "str", "str", "keras.layers.GlobalMaxPooling2D"], "function", ["home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.backbones.inception_resnet_v2.conv2d_bn", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.backbones.inception_resnet_v2.conv2d_bn", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.backbones.inception_resnet_v2.conv2d_bn", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.backbones.inception_resnet_v2.conv2d_bn", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.backbones.inception_resnet_v2.conv2d_bn", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.backbones.inception_resnet_v2.conv2d_bn", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.backbones.inception_resnet_v2.conv2d_bn", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.backbones.inception_resnet_v2.conv2d_bn", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.backbones.inception_resnet_v2.conv2d_bn", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.backbones.inception_resnet_v2.conv2d_bn", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.backbones.inception_resnet_v2.conv2d_bn", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.backbones.inception_resnet_v2.conv2d_bn", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.backbones.inception_resnet_v2.conv2d_bn", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.backbones.inception_resnet_v2.conv2d_bn", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.backbones.inception_resnet_v2.conv2d_bn", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.backbones.inception_resnet_v2.conv2d_bn", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.backbones.inception_resnet_v2.conv2d_bn", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.backbones.inception_resnet_v2.conv2d_bn", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.backbones.inception_resnet_v2.conv2d_bn", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.backbones.inception_resnet_v2.conv2d_bn", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.backbones.inception_resnet_v2.conv2d_bn", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.backbones.inception_resnet_v2.conv2d_bn", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.backbones.inception_resnet_v2.conv2d_bn", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.backbones.inception_resnet_v2.conv2d_bn", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.backbones.inception_resnet_v2.conv2d_bn", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.backbones.inception_resnet_v2.conv2d_bn", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.backbones.inception_resnet_v2.conv2d_bn", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.backbones.inception_resnet_v2.conv2d_bn", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.backbones.inception_resnet_v2.conv2d_bn", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.backbones.inception_resnet_v2.conv2d_bn", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.backbones.inception_resnet_v2.conv2d_bn", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.backbones.inception_resnet_v2.conv2d_bn", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.backbones.inception_resnet_v2.conv2d_bn", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.backbones.inception_resnet_v2.conv2d_bn", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.backbones.inception_resnet_v2.conv2d_bn", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.backbones.inception_resnet_v2.conv2d_bn", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.backbones.inception_resnet_v2.conv2d_bn", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.backbones.inception_resnet_v2.conv2d_bn", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.backbones.inception_resnet_v2.conv2d_bn", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.backbones.inception_resnet_v2.conv2d_bn", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.backbones.inception_resnet_v2.conv2d_bn", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.backbones.inception_resnet_v2.conv2d_bn", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.backbones.inception_resnet_v2.conv2d_bn", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.backbones.inception_resnet_v2.conv2d_bn", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.backbones.inception_resnet_v2.conv2d_bn", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.backbones.inception_resnet_v2.conv2d_bn", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.backbones.inception_resnet_v2.conv2d_bn", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.backbones.inception_resnet_v2.conv2d_bn", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.backbones.inception_resnet_v2.conv2d_bn", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.backbones.inception_resnet_v2.conv2d_bn", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.backbones.inception_resnet_v2.conv2d_bn", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.backbones.inception_resnet_v2.conv2d_bn", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.backbones.inception_resnet_v2.conv2d_bn", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.backbones.inception_resnet_v2.conv2d_bn", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.backbones.inception_resnet_v2.conv2d_bn", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.backbones.inception_resnet_v2.conv2d_bn", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.backbones.inception_resnet_v2.conv2d_bn", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.backbones.inception_resnet_v2.conv2d_bn", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.backbones.inception_resnet_v2.conv2d_bn", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.backbones.inception_resnet_v2.conv2d_bn", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.backbones.inception_resnet_v2.conv2d_bn", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.backbones.inception_resnet_v2.conv2d_bn", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.backbones.inception_resnet_v2.conv2d_bn", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.backbones.inception_resnet_v2.conv2d_bn", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.backbones.inception_resnet_v2.conv2d_bn", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.backbones.inception_resnet_v2.conv2d_bn", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.backbones.inception_resnet_v2.conv2d_bn", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.backbones.inception_resnet_v2.conv2d_bn", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.backbones.inception_resnet_v2.conv2d_bn", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.backbones.inception_resnet_v2.conv2d_bn", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.backbones.inception_resnet_v2.conv2d_bn", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.backbones.inception_resnet_v2.conv2d_bn", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.backbones.inception_resnet_v2.conv2d_bn", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.backbones.inception_resnet_v2.conv2d_bn", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.backbones.inception_resnet_v2.conv2d_bn"], ["", "def", "InceptionV3", "(", "include_top", "=", "True", ",", "\n", "weights", "=", "'imagenet'", ",", "\n", "input_tensor", "=", "None", ",", "\n", "input_shape", "=", "None", ",", "\n", "pooling", "=", "None", ",", "\n", "classes", "=", "1000", ")", ":", "\n", "    ", "\"\"\"Instantiates the Inception v3 architecture.\n    Optionally loads weights pre-trained\n    on ImageNet. Note that when using TensorFlow,\n    for best performance you should set\n    `image_data_format='channels_last'` in your Keras config\n    at ~/.keras/keras.json.\n    The model and the weights are compatible with both\n    TensorFlow and Theano. The data format\n    convention used by the model is the one\n    specified in your Keras config file.\n    Note that the default input image size for this model is 299x299.\n    # Arguments\n        include_top: whether to include the fully-connected\n            layer at the top of the network.\n        weights: one of `None` (random initialization),\n              'imagenet' (pre-training on ImageNet),\n              or the path to the weights file to be loaded.\n        input_tensor: optional Keras tensor (i.e. output of `layers.Input()`)\n            to use as image input for the model.\n        input_shape: optional shape tuple, only to be specified\n            if `include_top` is False (otherwise the input shape\n            has to be `(299, 299, 3)` (with `channels_last` data format)\n            or `(3, 299, 299)` (with `channels_first` data format).\n            It should have exactly 3 inputs channels,\n            and width and height should be no smaller than 139.\n            E.g. `(150, 150, 3)` would be one valid value.\n        pooling: Optional pooling mode for feature extraction\n            when `include_top` is `False`.\n            - `None` means that the output of the model will be\n                the 4D tensor output of the\n                last convolutional layer.\n            - `avg` means that global average pooling\n                will be applied to the output of the\n                last convolutional layer, and thus\n                the output of the model will be a 2D tensor.\n            - `max` means that global max pooling will\n                be applied.\n        classes: optional number of classes to classify images\n            into, only to be specified if `include_top` is True, and\n            if no `weights` argument is specified.\n    # Returns\n        A Keras model instance.\n    # Raises\n        ValueError: in case of invalid argument for `weights`,\n            or invalid input shape.\n    \"\"\"", "\n", "if", "not", "(", "weights", "in", "{", "'imagenet'", ",", "None", "}", "or", "os", ".", "path", ".", "exists", "(", "weights", ")", ")", ":", "\n", "        ", "raise", "ValueError", "(", "'The `weights` argument should be either '", "\n", "'`None` (random initialization), `imagenet` '", "\n", "'(pre-training on ImageNet), '", "\n", "'or the path to the weights file to be loaded.'", ")", "\n", "\n", "", "if", "weights", "==", "'imagenet'", "and", "include_top", "and", "classes", "!=", "1000", ":", "\n", "        ", "raise", "ValueError", "(", "'If using `weights` as imagenet with `include_top`'", "\n", "' as true, `classes` should be 1000'", ")", "\n", "\n", "# Determine proper input shape", "\n", "", "input_shape", "=", "_obtain_input_shape", "(", "\n", "input_shape", ",", "\n", "default_size", "=", "299", ",", "\n", "min_size", "=", "139", ",", "\n", "data_format", "=", "K", ".", "image_data_format", "(", ")", ",", "\n", "require_flatten", "=", "False", ",", "\n", "weights", "=", "weights", ")", "\n", "\n", "if", "input_tensor", "is", "None", ":", "\n", "        ", "img_input", "=", "Input", "(", "shape", "=", "input_shape", ")", "\n", "", "else", ":", "\n", "        ", "if", "not", "K", ".", "is_keras_tensor", "(", "input_tensor", ")", ":", "\n", "            ", "img_input", "=", "Input", "(", "tensor", "=", "input_tensor", ",", "shape", "=", "input_shape", ")", "\n", "", "else", ":", "\n", "            ", "img_input", "=", "input_tensor", "\n", "\n", "", "", "if", "K", ".", "image_data_format", "(", ")", "==", "'channels_first'", ":", "\n", "        ", "channel_axis", "=", "1", "\n", "", "else", ":", "\n", "        ", "channel_axis", "=", "3", "\n", "\n", "", "x", "=", "conv2d_bn", "(", "img_input", ",", "32", ",", "3", ",", "3", ",", "strides", "=", "(", "2", ",", "2", ")", ",", "padding", "=", "'same'", ")", "\n", "x", "=", "conv2d_bn", "(", "x", ",", "32", ",", "3", ",", "3", ",", "padding", "=", "'same'", ")", "\n", "x", "=", "conv2d_bn", "(", "x", ",", "64", ",", "3", ",", "3", ")", "\n", "x", "=", "MaxPooling2D", "(", "(", "3", ",", "3", ")", ",", "strides", "=", "(", "2", ",", "2", ")", ",", "padding", "=", "'same'", ")", "(", "x", ")", "\n", "\n", "x", "=", "conv2d_bn", "(", "x", ",", "80", ",", "1", ",", "1", ",", "padding", "=", "'same'", ")", "\n", "x", "=", "conv2d_bn", "(", "x", ",", "192", ",", "3", ",", "3", ",", "padding", "=", "'same'", ")", "\n", "x", "=", "MaxPooling2D", "(", "(", "3", ",", "3", ")", ",", "strides", "=", "(", "2", ",", "2", ")", ",", "padding", "=", "'same'", ")", "(", "x", ")", "\n", "\n", "# mixed 0, 1, 2: 35 x 35 x 256", "\n", "branch1x1", "=", "conv2d_bn", "(", "x", ",", "64", ",", "1", ",", "1", ")", "\n", "\n", "branch5x5", "=", "conv2d_bn", "(", "x", ",", "48", ",", "1", ",", "1", ")", "\n", "branch5x5", "=", "conv2d_bn", "(", "branch5x5", ",", "64", ",", "5", ",", "5", ")", "\n", "\n", "branch3x3dbl", "=", "conv2d_bn", "(", "x", ",", "64", ",", "1", ",", "1", ")", "\n", "branch3x3dbl", "=", "conv2d_bn", "(", "branch3x3dbl", ",", "96", ",", "3", ",", "3", ")", "\n", "branch3x3dbl", "=", "conv2d_bn", "(", "branch3x3dbl", ",", "96", ",", "3", ",", "3", ")", "\n", "\n", "branch_pool", "=", "AveragePooling2D", "(", "(", "3", ",", "3", ")", ",", "strides", "=", "(", "1", ",", "1", ")", ",", "padding", "=", "'same'", ")", "(", "x", ")", "\n", "branch_pool", "=", "conv2d_bn", "(", "branch_pool", ",", "32", ",", "1", ",", "1", ")", "\n", "x", "=", "layers", ".", "concatenate", "(", "\n", "[", "branch1x1", ",", "branch5x5", ",", "branch3x3dbl", ",", "branch_pool", "]", ",", "\n", "axis", "=", "channel_axis", ",", "\n", "name", "=", "'mixed0'", ")", "\n", "\n", "# mixed 1: 35 x 35 x 256", "\n", "branch1x1", "=", "conv2d_bn", "(", "x", ",", "64", ",", "1", ",", "1", ")", "\n", "\n", "branch5x5", "=", "conv2d_bn", "(", "x", ",", "48", ",", "1", ",", "1", ")", "\n", "branch5x5", "=", "conv2d_bn", "(", "branch5x5", ",", "64", ",", "5", ",", "5", ")", "\n", "\n", "branch3x3dbl", "=", "conv2d_bn", "(", "x", ",", "64", ",", "1", ",", "1", ")", "\n", "branch3x3dbl", "=", "conv2d_bn", "(", "branch3x3dbl", ",", "96", ",", "3", ",", "3", ")", "\n", "branch3x3dbl", "=", "conv2d_bn", "(", "branch3x3dbl", ",", "96", ",", "3", ",", "3", ")", "\n", "\n", "branch_pool", "=", "AveragePooling2D", "(", "(", "3", ",", "3", ")", ",", "strides", "=", "(", "1", ",", "1", ")", ",", "padding", "=", "'same'", ")", "(", "x", ")", "\n", "branch_pool", "=", "conv2d_bn", "(", "branch_pool", ",", "64", ",", "1", ",", "1", ")", "\n", "x", "=", "layers", ".", "concatenate", "(", "\n", "[", "branch1x1", ",", "branch5x5", ",", "branch3x3dbl", ",", "branch_pool", "]", ",", "\n", "axis", "=", "channel_axis", ",", "\n", "name", "=", "'mixed1'", ")", "\n", "\n", "# mixed 2: 35 x 35 x 256", "\n", "branch1x1", "=", "conv2d_bn", "(", "x", ",", "64", ",", "1", ",", "1", ")", "\n", "\n", "branch5x5", "=", "conv2d_bn", "(", "x", ",", "48", ",", "1", ",", "1", ")", "\n", "branch5x5", "=", "conv2d_bn", "(", "branch5x5", ",", "64", ",", "5", ",", "5", ")", "\n", "\n", "branch3x3dbl", "=", "conv2d_bn", "(", "x", ",", "64", ",", "1", ",", "1", ")", "\n", "branch3x3dbl", "=", "conv2d_bn", "(", "branch3x3dbl", ",", "96", ",", "3", ",", "3", ")", "\n", "branch3x3dbl", "=", "conv2d_bn", "(", "branch3x3dbl", ",", "96", ",", "3", ",", "3", ")", "\n", "\n", "branch_pool", "=", "AveragePooling2D", "(", "(", "3", ",", "3", ")", ",", "strides", "=", "(", "1", ",", "1", ")", ",", "padding", "=", "'same'", ")", "(", "x", ")", "\n", "branch_pool", "=", "conv2d_bn", "(", "branch_pool", ",", "64", ",", "1", ",", "1", ")", "\n", "x", "=", "layers", ".", "concatenate", "(", "\n", "[", "branch1x1", ",", "branch5x5", ",", "branch3x3dbl", ",", "branch_pool", "]", ",", "\n", "axis", "=", "channel_axis", ",", "\n", "name", "=", "'mixed2'", ")", "\n", "\n", "# mixed 3: 17 x 17 x 768", "\n", "branch3x3", "=", "conv2d_bn", "(", "x", ",", "384", ",", "3", ",", "3", ",", "strides", "=", "(", "2", ",", "2", ")", ",", "padding", "=", "'same'", ")", "\n", "\n", "branch3x3dbl", "=", "conv2d_bn", "(", "x", ",", "64", ",", "1", ",", "1", ")", "\n", "branch3x3dbl", "=", "conv2d_bn", "(", "branch3x3dbl", ",", "96", ",", "3", ",", "3", ")", "\n", "branch3x3dbl", "=", "conv2d_bn", "(", "\n", "branch3x3dbl", ",", "96", ",", "3", ",", "3", ",", "strides", "=", "(", "2", ",", "2", ")", ",", "padding", "=", "'same'", ")", "\n", "\n", "branch_pool", "=", "MaxPooling2D", "(", "(", "3", ",", "3", ")", ",", "strides", "=", "(", "2", ",", "2", ")", ",", "padding", "=", "'same'", ")", "(", "x", ")", "\n", "x", "=", "layers", ".", "concatenate", "(", "\n", "[", "branch3x3", ",", "branch3x3dbl", ",", "branch_pool", "]", ",", "axis", "=", "channel_axis", ",", "name", "=", "'mixed3'", ")", "\n", "\n", "# mixed 4: 17 x 17 x 768", "\n", "branch1x1", "=", "conv2d_bn", "(", "x", ",", "192", ",", "1", ",", "1", ")", "\n", "\n", "branch7x7", "=", "conv2d_bn", "(", "x", ",", "128", ",", "1", ",", "1", ")", "\n", "branch7x7", "=", "conv2d_bn", "(", "branch7x7", ",", "128", ",", "1", ",", "7", ")", "\n", "branch7x7", "=", "conv2d_bn", "(", "branch7x7", ",", "192", ",", "7", ",", "1", ")", "\n", "\n", "branch7x7dbl", "=", "conv2d_bn", "(", "x", ",", "128", ",", "1", ",", "1", ")", "\n", "branch7x7dbl", "=", "conv2d_bn", "(", "branch7x7dbl", ",", "128", ",", "7", ",", "1", ")", "\n", "branch7x7dbl", "=", "conv2d_bn", "(", "branch7x7dbl", ",", "128", ",", "1", ",", "7", ")", "\n", "branch7x7dbl", "=", "conv2d_bn", "(", "branch7x7dbl", ",", "128", ",", "7", ",", "1", ")", "\n", "branch7x7dbl", "=", "conv2d_bn", "(", "branch7x7dbl", ",", "192", ",", "1", ",", "7", ")", "\n", "\n", "branch_pool", "=", "AveragePooling2D", "(", "(", "3", ",", "3", ")", ",", "strides", "=", "(", "1", ",", "1", ")", ",", "padding", "=", "'same'", ")", "(", "x", ")", "\n", "branch_pool", "=", "conv2d_bn", "(", "branch_pool", ",", "192", ",", "1", ",", "1", ")", "\n", "x", "=", "layers", ".", "concatenate", "(", "\n", "[", "branch1x1", ",", "branch7x7", ",", "branch7x7dbl", ",", "branch_pool", "]", ",", "\n", "axis", "=", "channel_axis", ",", "\n", "name", "=", "'mixed4'", ")", "\n", "\n", "# mixed 5, 6: 17 x 17 x 768", "\n", "for", "i", "in", "range", "(", "2", ")", ":", "\n", "        ", "branch1x1", "=", "conv2d_bn", "(", "x", ",", "192", ",", "1", ",", "1", ")", "\n", "\n", "branch7x7", "=", "conv2d_bn", "(", "x", ",", "160", ",", "1", ",", "1", ")", "\n", "branch7x7", "=", "conv2d_bn", "(", "branch7x7", ",", "160", ",", "1", ",", "7", ")", "\n", "branch7x7", "=", "conv2d_bn", "(", "branch7x7", ",", "192", ",", "7", ",", "1", ")", "\n", "\n", "branch7x7dbl", "=", "conv2d_bn", "(", "x", ",", "160", ",", "1", ",", "1", ")", "\n", "branch7x7dbl", "=", "conv2d_bn", "(", "branch7x7dbl", ",", "160", ",", "7", ",", "1", ")", "\n", "branch7x7dbl", "=", "conv2d_bn", "(", "branch7x7dbl", ",", "160", ",", "1", ",", "7", ")", "\n", "branch7x7dbl", "=", "conv2d_bn", "(", "branch7x7dbl", ",", "160", ",", "7", ",", "1", ")", "\n", "branch7x7dbl", "=", "conv2d_bn", "(", "branch7x7dbl", ",", "192", ",", "1", ",", "7", ")", "\n", "\n", "branch_pool", "=", "AveragePooling2D", "(", "\n", "(", "3", ",", "3", ")", ",", "strides", "=", "(", "1", ",", "1", ")", ",", "padding", "=", "'same'", ")", "(", "x", ")", "\n", "branch_pool", "=", "conv2d_bn", "(", "branch_pool", ",", "192", ",", "1", ",", "1", ")", "\n", "x", "=", "layers", ".", "concatenate", "(", "\n", "[", "branch1x1", ",", "branch7x7", ",", "branch7x7dbl", ",", "branch_pool", "]", ",", "\n", "axis", "=", "channel_axis", ",", "\n", "name", "=", "'mixed'", "+", "str", "(", "5", "+", "i", ")", ")", "\n", "\n", "# mixed 7: 17 x 17 x 768", "\n", "", "branch1x1", "=", "conv2d_bn", "(", "x", ",", "192", ",", "1", ",", "1", ")", "\n", "\n", "branch7x7", "=", "conv2d_bn", "(", "x", ",", "192", ",", "1", ",", "1", ")", "\n", "branch7x7", "=", "conv2d_bn", "(", "branch7x7", ",", "192", ",", "1", ",", "7", ")", "\n", "branch7x7", "=", "conv2d_bn", "(", "branch7x7", ",", "192", ",", "7", ",", "1", ")", "\n", "\n", "branch7x7dbl", "=", "conv2d_bn", "(", "x", ",", "192", ",", "1", ",", "1", ")", "\n", "branch7x7dbl", "=", "conv2d_bn", "(", "branch7x7dbl", ",", "192", ",", "7", ",", "1", ")", "\n", "branch7x7dbl", "=", "conv2d_bn", "(", "branch7x7dbl", ",", "192", ",", "1", ",", "7", ")", "\n", "branch7x7dbl", "=", "conv2d_bn", "(", "branch7x7dbl", ",", "192", ",", "7", ",", "1", ")", "\n", "branch7x7dbl", "=", "conv2d_bn", "(", "branch7x7dbl", ",", "192", ",", "1", ",", "7", ")", "\n", "\n", "branch_pool", "=", "AveragePooling2D", "(", "(", "3", ",", "3", ")", ",", "strides", "=", "(", "1", ",", "1", ")", ",", "padding", "=", "'same'", ")", "(", "x", ")", "\n", "branch_pool", "=", "conv2d_bn", "(", "branch_pool", ",", "192", ",", "1", ",", "1", ")", "\n", "x", "=", "layers", ".", "concatenate", "(", "\n", "[", "branch1x1", ",", "branch7x7", ",", "branch7x7dbl", ",", "branch_pool", "]", ",", "\n", "axis", "=", "channel_axis", ",", "\n", "name", "=", "'mixed7'", ")", "\n", "\n", "# mixed 8: 8 x 8 x 1280", "\n", "branch3x3", "=", "conv2d_bn", "(", "x", ",", "192", ",", "1", ",", "1", ")", "\n", "branch3x3", "=", "conv2d_bn", "(", "branch3x3", ",", "320", ",", "3", ",", "3", ",", "\n", "strides", "=", "(", "2", ",", "2", ")", ",", "padding", "=", "'same'", ")", "\n", "\n", "branch7x7x3", "=", "conv2d_bn", "(", "x", ",", "192", ",", "1", ",", "1", ")", "\n", "branch7x7x3", "=", "conv2d_bn", "(", "branch7x7x3", ",", "192", ",", "1", ",", "7", ")", "\n", "branch7x7x3", "=", "conv2d_bn", "(", "branch7x7x3", ",", "192", ",", "7", ",", "1", ")", "\n", "branch7x7x3", "=", "conv2d_bn", "(", "\n", "branch7x7x3", ",", "192", ",", "3", ",", "3", ",", "strides", "=", "(", "2", ",", "2", ")", ",", "padding", "=", "'same'", ")", "\n", "\n", "branch_pool", "=", "MaxPooling2D", "(", "(", "3", ",", "3", ")", ",", "strides", "=", "(", "2", ",", "2", ")", ",", "padding", "=", "'same'", ")", "(", "x", ")", "\n", "x", "=", "layers", ".", "concatenate", "(", "\n", "[", "branch3x3", ",", "branch7x7x3", ",", "branch_pool", "]", ",", "axis", "=", "channel_axis", ",", "name", "=", "'mixed8'", ")", "\n", "\n", "# mixed 9: 8 x 8 x 2048", "\n", "for", "i", "in", "range", "(", "2", ")", ":", "\n", "        ", "branch1x1", "=", "conv2d_bn", "(", "x", ",", "320", ",", "1", ",", "1", ")", "\n", "\n", "branch3x3", "=", "conv2d_bn", "(", "x", ",", "384", ",", "1", ",", "1", ")", "\n", "branch3x3_1", "=", "conv2d_bn", "(", "branch3x3", ",", "384", ",", "1", ",", "3", ")", "\n", "branch3x3_2", "=", "conv2d_bn", "(", "branch3x3", ",", "384", ",", "3", ",", "1", ")", "\n", "branch3x3", "=", "layers", ".", "concatenate", "(", "\n", "[", "branch3x3_1", ",", "branch3x3_2", "]", ",", "axis", "=", "channel_axis", ",", "name", "=", "'mixed9_'", "+", "str", "(", "i", ")", ")", "\n", "\n", "branch3x3dbl", "=", "conv2d_bn", "(", "x", ",", "448", ",", "1", ",", "1", ")", "\n", "branch3x3dbl", "=", "conv2d_bn", "(", "branch3x3dbl", ",", "384", ",", "3", ",", "3", ")", "\n", "branch3x3dbl_1", "=", "conv2d_bn", "(", "branch3x3dbl", ",", "384", ",", "1", ",", "3", ")", "\n", "branch3x3dbl_2", "=", "conv2d_bn", "(", "branch3x3dbl", ",", "384", ",", "3", ",", "1", ")", "\n", "branch3x3dbl", "=", "layers", ".", "concatenate", "(", "\n", "[", "branch3x3dbl_1", ",", "branch3x3dbl_2", "]", ",", "axis", "=", "channel_axis", ")", "\n", "\n", "branch_pool", "=", "AveragePooling2D", "(", "\n", "(", "3", ",", "3", ")", ",", "strides", "=", "(", "1", ",", "1", ")", ",", "padding", "=", "'same'", ")", "(", "x", ")", "\n", "branch_pool", "=", "conv2d_bn", "(", "branch_pool", ",", "192", ",", "1", ",", "1", ")", "\n", "x", "=", "layers", ".", "concatenate", "(", "\n", "[", "branch1x1", ",", "branch3x3", ",", "branch3x3dbl", ",", "branch_pool", "]", ",", "\n", "axis", "=", "channel_axis", ",", "\n", "name", "=", "'mixed'", "+", "str", "(", "9", "+", "i", ")", ")", "\n", "\n", "", "if", "include_top", ":", "\n", "# Classification block", "\n", "        ", "x", "=", "GlobalAveragePooling2D", "(", "name", "=", "'avg_pool'", ")", "(", "x", ")", "\n", "x", "=", "Dense", "(", "classes", ",", "activation", "=", "'softmax'", ",", "name", "=", "'predictions'", ")", "(", "x", ")", "\n", "", "else", ":", "\n", "        ", "if", "pooling", "==", "'avg'", ":", "\n", "            ", "x", "=", "GlobalAveragePooling2D", "(", ")", "(", "x", ")", "\n", "", "elif", "pooling", "==", "'max'", ":", "\n", "            ", "x", "=", "GlobalMaxPooling2D", "(", ")", "(", "x", ")", "\n", "\n", "# Ensure that the model takes into account", "\n", "# any potential predecessors of `input_tensor`.", "\n", "", "", "if", "input_tensor", "is", "not", "None", ":", "\n", "        ", "inputs", "=", "get_source_inputs", "(", "input_tensor", ")", "\n", "", "else", ":", "\n", "        ", "inputs", "=", "img_input", "\n", "# Create model.", "\n", "", "model", "=", "Model", "(", "inputs", ",", "x", ",", "name", "=", "'inception_v3'", ")", "\n", "\n", "# load weights", "\n", "if", "weights", "==", "'imagenet'", ":", "\n", "        ", "if", "K", ".", "image_data_format", "(", ")", "==", "'channels_first'", ":", "\n", "            ", "if", "K", ".", "backend", "(", ")", "==", "'tensorflow'", ":", "\n", "                ", "warnings", ".", "warn", "(", "'You are using the TensorFlow backend, yet you '", "\n", "'are using the Theano '", "\n", "'image data format convention '", "\n", "'(`image_data_format=\"channels_first\"`). '", "\n", "'For best performance, set '", "\n", "'`image_data_format=\"channels_last\"` in '", "\n", "'your Keras config '", "\n", "'at ~/.keras/keras.json.'", ")", "\n", "", "", "if", "include_top", ":", "\n", "            ", "weights_path", "=", "get_file", "(", "\n", "'inception_v3_weights_tf_dim_ordering_tf_kernels.h5'", ",", "\n", "WEIGHTS_PATH", ",", "\n", "cache_subdir", "=", "'models'", ",", "\n", "file_hash", "=", "'9a0d58056eeedaa3f26cb7ebd46da564'", ")", "\n", "", "else", ":", "\n", "            ", "weights_path", "=", "get_file", "(", "\n", "'inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5'", ",", "\n", "WEIGHTS_PATH_NO_TOP", ",", "\n", "cache_subdir", "=", "'models'", ",", "\n", "file_hash", "=", "'bcbd6486424b2319ff4ef7d526e38f63'", ")", "\n", "", "model", ".", "load_weights", "(", "weights_path", ")", "\n", "", "elif", "weights", "is", "not", "None", ":", "\n", "        ", "model", ".", "load_weights", "(", "weights", ")", "\n", "\n", "", "return", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.backbones.inception_v3.preprocess_input": [[395, 403], ["keras.applications.imagenet_utils.preprocess_input"], "function", ["home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.resnext.preprocessing.preprocess_input"], ["", "def", "preprocess_input", "(", "x", ")", ":", "\n", "    ", "\"\"\"Preprocesses a numpy array encoding a batch of images.\n    # Arguments\n        x: a 4D numpy array consists of RGB values within [0, 255].\n    # Returns\n        Preprocessed array.\n    \"\"\"", "\n", "return", "imagenet_utils", ".", "preprocess_input", "(", "x", ",", "mode", "=", "'tf'", ")", "\n", "", ""]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.backbones.inception_resnet_v2.preprocess_input": [[49, 57], ["keras.applications.imagenet_utils.preprocess_input"], "function", ["home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.resnext.preprocessing.preprocess_input"], ["def", "preprocess_input", "(", "x", ")", ":", "\n", "    ", "\"\"\"Preprocesses a numpy array encoding a batch of images.\n    # Arguments\n        x: a 4D numpy array consists of RGB values within [0, 255].\n    # Returns\n        Preprocessed array.\n    \"\"\"", "\n", "return", "imagenet_utils", ".", "preprocess_input", "(", "x", ",", "mode", "=", "'tf'", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.backbones.inception_resnet_v2.conv2d_bn": [[59, 95], ["keras.layers.Conv2D", "keras.layers.BatchNormalization", "keras.layers.Activation", "keras.backend.image_data_format"], "function", ["None"], ["", "def", "conv2d_bn", "(", "x", ",", "\n", "filters", ",", "\n", "kernel_size", ",", "\n", "strides", "=", "1", ",", "\n", "padding", "=", "'same'", ",", "\n", "activation", "=", "'relu'", ",", "\n", "use_bias", "=", "False", ",", "\n", "name", "=", "None", ")", ":", "\n", "    ", "\"\"\"Utility function to apply conv + BN.\n    # Arguments\n        x: input tensor.\n        filters: filters in `Conv2D`.\n        kernel_size: kernel size as in `Conv2D`.\n        strides: strides in `Conv2D`.\n        padding: padding mode in `Conv2D`.\n        activation: activation in `Conv2D`.\n        use_bias: whether to use a bias in `Conv2D`.\n        name: name of the ops; will become `name + '_ac'` for the activation\n            and `name + '_bn'` for the batch norm layer.\n    # Returns\n        Output tensor after applying `Conv2D` and `BatchNormalization`.\n    \"\"\"", "\n", "x", "=", "Conv2D", "(", "filters", ",", "\n", "kernel_size", ",", "\n", "strides", "=", "strides", ",", "\n", "padding", "=", "padding", ",", "\n", "use_bias", "=", "use_bias", ",", "\n", "name", "=", "name", ")", "(", "x", ")", "\n", "if", "not", "use_bias", ":", "\n", "        ", "bn_axis", "=", "1", "if", "K", ".", "image_data_format", "(", ")", "==", "'channels_first'", "else", "3", "\n", "bn_name", "=", "None", "if", "name", "is", "None", "else", "name", "+", "'_bn'", "\n", "x", "=", "BatchNormalization", "(", "axis", "=", "bn_axis", ",", "scale", "=", "False", ",", "name", "=", "bn_name", ")", "(", "x", ")", "\n", "", "if", "activation", "is", "not", "None", ":", "\n", "        ", "ac_name", "=", "None", "if", "name", "is", "None", "else", "name", "+", "'_ac'", "\n", "x", "=", "Activation", "(", "activation", ",", "name", "=", "ac_name", ")", "(", "x", ")", "\n", "", "return", "x", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.backbones.inception_resnet_v2.inception_resnet_block": [[97, 171], ["inception_resnet_v2.conv2d_bn", "inception_resnet_v2.conv2d_bn", "inception_resnet_v2.conv2d_bn", "inception_resnet_v2.conv2d_bn", "inception_resnet_v2.conv2d_bn", "inception_resnet_v2.conv2d_bn", "inception_resnet_v2.conv2d_bn", "str", "keras.layers.Concatenate", "keras.layers.Lambda", "inception_resnet_v2.conv2d_bn", "inception_resnet_v2.conv2d_bn", "inception_resnet_v2.conv2d_bn", "inception_resnet_v2.conv2d_bn", "keras.backend.image_data_format", "keras.backend.int_shape", "keras.layers.Activation", "inception_resnet_v2.conv2d_bn", "inception_resnet_v2.conv2d_bn", "inception_resnet_v2.conv2d_bn", "inception_resnet_v2.conv2d_bn", "ValueError", "keras.backend.int_shape", "str"], "function", ["home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.backbones.inception_resnet_v2.conv2d_bn", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.backbones.inception_resnet_v2.conv2d_bn", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.backbones.inception_resnet_v2.conv2d_bn", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.backbones.inception_resnet_v2.conv2d_bn", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.backbones.inception_resnet_v2.conv2d_bn", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.backbones.inception_resnet_v2.conv2d_bn", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.backbones.inception_resnet_v2.conv2d_bn", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.backbones.inception_resnet_v2.conv2d_bn", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.backbones.inception_resnet_v2.conv2d_bn", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.backbones.inception_resnet_v2.conv2d_bn", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.backbones.inception_resnet_v2.conv2d_bn", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.common.functions.int_shape", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.backbones.inception_resnet_v2.conv2d_bn", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.backbones.inception_resnet_v2.conv2d_bn", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.backbones.inception_resnet_v2.conv2d_bn", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.backbones.inception_resnet_v2.conv2d_bn", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.common.functions.int_shape"], ["", "def", "inception_resnet_block", "(", "x", ",", "scale", ",", "block_type", ",", "block_idx", ",", "activation", "=", "'relu'", ")", ":", "\n", "    ", "\"\"\"Adds a Inception-ResNet block.\n    This function builds 3 types of Inception-ResNet blocks mentioned\n    in the paper, controlled by the `block_type` argument (which is the\n    block name used in the official TF-slim implementation):\n        - Inception-ResNet-A: `block_type='block35'`\n        - Inception-ResNet-B: `block_type='block17'`\n        - Inception-ResNet-C: `block_type='block8'`\n    # Arguments\n        x: input tensor.\n        scale: scaling factor to scale the residuals (i.e., the output of\n            passing `x` through an inception module) before adding them\n            to the shortcut branch. Let `r` be the output from the residual branch,\n            the output of this block will be `x + scale * r`.\n        block_type: `'block35'`, `'block17'` or `'block8'`, determines\n            the network structure in the residual branch.\n        block_idx: an `int` used for generating layer names. The Inception-ResNet blocks\n            are repeated many times in this network. We use `block_idx` to identify\n            each of the repetitions. For example, the first Inception-ResNet-A block\n            will have `block_type='block35', block_idx=0`, ane the layer names will have\n            a common prefix `'block35_0'`.\n        activation: activation function to use at the end of the block\n            (see [activations](../activations.md)).\n            When `activation=None`, no activation is applied\n            (i.e., \"linear\" activation: `a(x) = x`).\n    # Returns\n        Output tensor for the block.\n    # Raises\n        ValueError: if `block_type` is not one of `'block35'`,\n            `'block17'` or `'block8'`.\n    \"\"\"", "\n", "if", "block_type", "==", "'block35'", ":", "\n", "        ", "branch_0", "=", "conv2d_bn", "(", "x", ",", "32", ",", "1", ")", "\n", "branch_1", "=", "conv2d_bn", "(", "x", ",", "32", ",", "1", ")", "\n", "branch_1", "=", "conv2d_bn", "(", "branch_1", ",", "32", ",", "3", ")", "\n", "branch_2", "=", "conv2d_bn", "(", "x", ",", "32", ",", "1", ")", "\n", "branch_2", "=", "conv2d_bn", "(", "branch_2", ",", "48", ",", "3", ")", "\n", "branch_2", "=", "conv2d_bn", "(", "branch_2", ",", "64", ",", "3", ")", "\n", "branches", "=", "[", "branch_0", ",", "branch_1", ",", "branch_2", "]", "\n", "", "elif", "block_type", "==", "'block17'", ":", "\n", "        ", "branch_0", "=", "conv2d_bn", "(", "x", ",", "192", ",", "1", ")", "\n", "branch_1", "=", "conv2d_bn", "(", "x", ",", "128", ",", "1", ")", "\n", "branch_1", "=", "conv2d_bn", "(", "branch_1", ",", "160", ",", "[", "1", ",", "7", "]", ")", "\n", "branch_1", "=", "conv2d_bn", "(", "branch_1", ",", "192", ",", "[", "7", ",", "1", "]", ")", "\n", "branches", "=", "[", "branch_0", ",", "branch_1", "]", "\n", "", "elif", "block_type", "==", "'block8'", ":", "\n", "        ", "branch_0", "=", "conv2d_bn", "(", "x", ",", "192", ",", "1", ")", "\n", "branch_1", "=", "conv2d_bn", "(", "x", ",", "192", ",", "1", ")", "\n", "branch_1", "=", "conv2d_bn", "(", "branch_1", ",", "224", ",", "[", "1", ",", "3", "]", ")", "\n", "branch_1", "=", "conv2d_bn", "(", "branch_1", ",", "256", ",", "[", "3", ",", "1", "]", ")", "\n", "branches", "=", "[", "branch_0", ",", "branch_1", "]", "\n", "", "else", ":", "\n", "        ", "raise", "ValueError", "(", "'Unknown Inception-ResNet block type. '", "\n", "'Expects \"block35\", \"block17\" or \"block8\", '", "\n", "'but got: '", "+", "str", "(", "block_type", ")", ")", "\n", "\n", "", "block_name", "=", "block_type", "+", "'_'", "+", "str", "(", "block_idx", ")", "\n", "channel_axis", "=", "1", "if", "K", ".", "image_data_format", "(", ")", "==", "'channels_first'", "else", "3", "\n", "mixed", "=", "Concatenate", "(", "axis", "=", "channel_axis", ",", "name", "=", "block_name", "+", "'_mixed'", ")", "(", "branches", ")", "\n", "up", "=", "conv2d_bn", "(", "mixed", ",", "\n", "K", ".", "int_shape", "(", "x", ")", "[", "channel_axis", "]", ",", "\n", "1", ",", "\n", "activation", "=", "None", ",", "\n", "use_bias", "=", "True", ",", "\n", "name", "=", "block_name", "+", "'_conv'", ")", "\n", "\n", "x", "=", "Lambda", "(", "lambda", "inputs", ",", "scale", ":", "inputs", "[", "0", "]", "+", "inputs", "[", "1", "]", "*", "scale", ",", "\n", "output_shape", "=", "K", ".", "int_shape", "(", "x", ")", "[", "1", ":", "]", ",", "\n", "arguments", "=", "{", "'scale'", ":", "scale", "}", ",", "\n", "name", "=", "block_name", ")", "(", "[", "x", ",", "up", "]", ")", "\n", "\n", "if", "activation", "is", "not", "None", ":", "\n", "        ", "x", "=", "Activation", "(", "activation", ",", "name", "=", "block_name", "+", "'_ac'", ")", "(", "x", ")", "\n", "", "return", "x", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.backbones.inception_resnet_v2.InceptionResNetV2": [[173, 372], ["_obtain_input_shape", "inception_resnet_v2.conv2d_bn", "inception_resnet_v2.conv2d_bn", "inception_resnet_v2.conv2d_bn", "inception_resnet_v2.conv2d_bn", "inception_resnet_v2.conv2d_bn", "inception_resnet_v2.conv2d_bn", "inception_resnet_v2.conv2d_bn", "inception_resnet_v2.conv2d_bn", "inception_resnet_v2.conv2d_bn", "inception_resnet_v2.conv2d_bn", "inception_resnet_v2.conv2d_bn", "inception_resnet_v2.conv2d_bn", "range", "inception_resnet_v2.conv2d_bn", "inception_resnet_v2.conv2d_bn", "inception_resnet_v2.conv2d_bn", "inception_resnet_v2.conv2d_bn", "range", "inception_resnet_v2.conv2d_bn", "inception_resnet_v2.conv2d_bn", "inception_resnet_v2.conv2d_bn", "inception_resnet_v2.conv2d_bn", "inception_resnet_v2.conv2d_bn", "inception_resnet_v2.conv2d_bn", "inception_resnet_v2.conv2d_bn", "range", "inception_resnet_v2.inception_resnet_block", "inception_resnet_v2.conv2d_bn", "keras.models.Model", "ValueError", "ValueError", "keras.layers.Input", "keras.layers.MaxPooling2D", "keras.layers.MaxPooling2D", "keras.layers.AveragePooling2D", "keras.layers.Concatenate", "inception_resnet_v2.inception_resnet_block", "keras.layers.MaxPooling2D", "keras.layers.Concatenate", "inception_resnet_v2.inception_resnet_block", "keras.layers.MaxPooling2D", "keras.layers.Concatenate", "inception_resnet_v2.inception_resnet_block", "keras.engine.topology.get_source_inputs", "keras.models.Model.load_weights", "os.path.exists", "keras.backend.image_data_format", "keras.backend.is_keras_tensor", "keras.layers.Input", "keras.backend.image_data_format", "keras.layers.GlobalAveragePooling2D", "keras.layers.Dense", "keras.backend.image_data_format", "keras.utils.data_utils.get_file", "keras.utils.data_utils.get_file", "keras.models.Model.load_weights", "keras.layers.GlobalAveragePooling2D", "keras.backend.backend", "warnings.warn", "keras.layers.GlobalMaxPooling2D"], "function", ["home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.backbones.inception_resnet_v2.conv2d_bn", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.backbones.inception_resnet_v2.conv2d_bn", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.backbones.inception_resnet_v2.conv2d_bn", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.backbones.inception_resnet_v2.conv2d_bn", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.backbones.inception_resnet_v2.conv2d_bn", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.backbones.inception_resnet_v2.conv2d_bn", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.backbones.inception_resnet_v2.conv2d_bn", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.backbones.inception_resnet_v2.conv2d_bn", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.backbones.inception_resnet_v2.conv2d_bn", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.backbones.inception_resnet_v2.conv2d_bn", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.backbones.inception_resnet_v2.conv2d_bn", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.backbones.inception_resnet_v2.conv2d_bn", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.backbones.inception_resnet_v2.conv2d_bn", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.backbones.inception_resnet_v2.conv2d_bn", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.backbones.inception_resnet_v2.conv2d_bn", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.backbones.inception_resnet_v2.conv2d_bn", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.backbones.inception_resnet_v2.conv2d_bn", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.backbones.inception_resnet_v2.conv2d_bn", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.backbones.inception_resnet_v2.conv2d_bn", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.backbones.inception_resnet_v2.conv2d_bn", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.backbones.inception_resnet_v2.conv2d_bn", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.backbones.inception_resnet_v2.conv2d_bn", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.backbones.inception_resnet_v2.conv2d_bn", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.backbones.inception_resnet_v2.inception_resnet_block", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.backbones.inception_resnet_v2.conv2d_bn", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.backbones.inception_resnet_v2.inception_resnet_block", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.backbones.inception_resnet_v2.inception_resnet_block", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.backbones.inception_resnet_v2.inception_resnet_block"], ["", "def", "InceptionResNetV2", "(", "include_top", "=", "True", ",", "\n", "weights", "=", "'imagenet'", ",", "\n", "input_tensor", "=", "None", ",", "\n", "input_shape", "=", "None", ",", "\n", "pooling", "=", "None", ",", "\n", "classes", "=", "1000", ")", ":", "\n", "    ", "\"\"\"Instantiates the Inception-ResNet v2 architecture.\n    Optionally loads weights pre-trained on ImageNet.\n    Note that when using TensorFlow, for best performance you should\n    set `\"image_data_format\": \"channels_last\"` in your Keras config\n    at `~/.keras/keras.json`.\n    The model and the weights are compatible with TensorFlow, Theano and\n    CNTK backends. The data format convention used by the model is\n    the one specified in your Keras config file.\n    Note that the default input image size for this model is 299x299, instead\n    of 224x224 as in the VGG16 and ResNet models. Also, the input preprocessing\n    function is different (i.e., do not use `imagenet_utils.preprocess_input()`\n    with this model. Use `preprocess_input()` defined in this module instead).\n    # Arguments\n        include_top: whether to include the fully-connected\n            layer at the top of the network.\n        weights: one of `None` (random initialization),\n              'imagenet' (pre-training on ImageNet),\n              or the path to the weights file to be loaded.\n        input_tensor: optional Keras tensor (i.e. output of `layers.Input()`)\n            to use as image input for the model.\n        input_shape: optional shape tuple, only to be specified\n            if `include_top` is `False` (otherwise the input shape\n            has to be `(299, 299, 3)` (with `'channels_last'` data format)\n            or `(3, 299, 299)` (with `'channels_first'` data format).\n            It should have exactly 3 inputs channels,\n            and width and height should be no smaller than 139.\n            E.g. `(150, 150, 3)` would be one valid value.\n        pooling: Optional pooling mode for feature extraction\n            when `include_top` is `False`.\n            - `None` means that the output of the model will be\n                the 4D tensor output of the last convolutional layer.\n            - `'avg'` means that global average pooling\n                will be applied to the output of the\n                last convolutional layer, and thus\n                the output of the model will be a 2D tensor.\n            - `'max'` means that global max pooling will be applied.\n        classes: optional number of classes to classify images\n            into, only to be specified if `include_top` is `True`, and\n            if no `weights` argument is specified.\n    # Returns\n        A Keras `Model` instance.\n    # Raises\n        ValueError: in case of invalid argument for `weights`,\n            or invalid input shape.\n    \"\"\"", "\n", "if", "not", "(", "weights", "in", "{", "'imagenet'", ",", "None", "}", "or", "os", ".", "path", ".", "exists", "(", "weights", ")", ")", ":", "\n", "        ", "raise", "ValueError", "(", "'The `weights` argument should be either '", "\n", "'`None` (random initialization), `imagenet` '", "\n", "'(pre-training on ImageNet), '", "\n", "'or the path to the weights file to be loaded.'", ")", "\n", "\n", "", "if", "weights", "==", "'imagenet'", "and", "include_top", "and", "classes", "!=", "1000", ":", "\n", "        ", "raise", "ValueError", "(", "'If using `weights` as imagenet with `include_top`'", "\n", "' as true, `classes` should be 1000'", ")", "\n", "\n", "# Determine proper input shape", "\n", "", "input_shape", "=", "_obtain_input_shape", "(", "\n", "input_shape", ",", "\n", "default_size", "=", "299", ",", "\n", "min_size", "=", "139", ",", "\n", "data_format", "=", "K", ".", "image_data_format", "(", ")", ",", "\n", "require_flatten", "=", "False", ",", "\n", "weights", "=", "weights", ")", "\n", "\n", "if", "input_tensor", "is", "None", ":", "\n", "        ", "img_input", "=", "Input", "(", "shape", "=", "input_shape", ")", "\n", "", "else", ":", "\n", "        ", "if", "not", "K", ".", "is_keras_tensor", "(", "input_tensor", ")", ":", "\n", "            ", "img_input", "=", "Input", "(", "tensor", "=", "input_tensor", ",", "shape", "=", "input_shape", ")", "\n", "", "else", ":", "\n", "            ", "img_input", "=", "input_tensor", "\n", "\n", "# Stem block: 35 x 35 x 192", "\n", "", "", "x", "=", "conv2d_bn", "(", "img_input", ",", "32", ",", "3", ",", "strides", "=", "2", ",", "padding", "=", "'same'", ")", "\n", "x", "=", "conv2d_bn", "(", "x", ",", "32", ",", "3", ",", "padding", "=", "'same'", ")", "\n", "x", "=", "conv2d_bn", "(", "x", ",", "64", ",", "3", ")", "\n", "x", "=", "MaxPooling2D", "(", "3", ",", "strides", "=", "2", ",", "padding", "=", "'same'", ")", "(", "x", ")", "\n", "x", "=", "conv2d_bn", "(", "x", ",", "80", ",", "1", ",", "padding", "=", "'same'", ")", "\n", "x", "=", "conv2d_bn", "(", "x", ",", "192", ",", "3", ",", "padding", "=", "'same'", ")", "\n", "x", "=", "MaxPooling2D", "(", "3", ",", "strides", "=", "2", ",", "padding", "=", "'same'", ")", "(", "x", ")", "\n", "\n", "# Mixed 5b (Inception-A block): 35 x 35 x 320", "\n", "branch_0", "=", "conv2d_bn", "(", "x", ",", "96", ",", "1", ")", "\n", "branch_1", "=", "conv2d_bn", "(", "x", ",", "48", ",", "1", ")", "\n", "branch_1", "=", "conv2d_bn", "(", "branch_1", ",", "64", ",", "5", ")", "\n", "branch_2", "=", "conv2d_bn", "(", "x", ",", "64", ",", "1", ")", "\n", "branch_2", "=", "conv2d_bn", "(", "branch_2", ",", "96", ",", "3", ")", "\n", "branch_2", "=", "conv2d_bn", "(", "branch_2", ",", "96", ",", "3", ")", "\n", "branch_pool", "=", "AveragePooling2D", "(", "3", ",", "strides", "=", "1", ",", "padding", "=", "'same'", ")", "(", "x", ")", "\n", "branch_pool", "=", "conv2d_bn", "(", "branch_pool", ",", "64", ",", "1", ")", "\n", "branches", "=", "[", "branch_0", ",", "branch_1", ",", "branch_2", ",", "branch_pool", "]", "\n", "channel_axis", "=", "1", "if", "K", ".", "image_data_format", "(", ")", "==", "'channels_first'", "else", "3", "\n", "x", "=", "Concatenate", "(", "axis", "=", "channel_axis", ",", "name", "=", "'mixed_5b'", ")", "(", "branches", ")", "\n", "\n", "# 10x block35 (Inception-ResNet-A block): 35 x 35 x 320", "\n", "for", "block_idx", "in", "range", "(", "1", ",", "11", ")", ":", "\n", "        ", "x", "=", "inception_resnet_block", "(", "x", ",", "\n", "scale", "=", "0.17", ",", "\n", "block_type", "=", "'block35'", ",", "\n", "block_idx", "=", "block_idx", ")", "\n", "\n", "# Mixed 6a (Reduction-A block): 17 x 17 x 1088", "\n", "", "branch_0", "=", "conv2d_bn", "(", "x", ",", "384", ",", "3", ",", "strides", "=", "2", ",", "padding", "=", "'same'", ")", "\n", "branch_1", "=", "conv2d_bn", "(", "x", ",", "256", ",", "1", ")", "\n", "branch_1", "=", "conv2d_bn", "(", "branch_1", ",", "256", ",", "3", ")", "\n", "branch_1", "=", "conv2d_bn", "(", "branch_1", ",", "384", ",", "3", ",", "strides", "=", "2", ",", "padding", "=", "'same'", ")", "\n", "branch_pool", "=", "MaxPooling2D", "(", "3", ",", "strides", "=", "2", ",", "padding", "=", "'same'", ")", "(", "x", ")", "\n", "branches", "=", "[", "branch_0", ",", "branch_1", ",", "branch_pool", "]", "\n", "x", "=", "Concatenate", "(", "axis", "=", "channel_axis", ",", "name", "=", "'mixed_6a'", ")", "(", "branches", ")", "\n", "\n", "# 20x block17 (Inception-ResNet-B block): 17 x 17 x 1088", "\n", "for", "block_idx", "in", "range", "(", "1", ",", "21", ")", ":", "\n", "        ", "x", "=", "inception_resnet_block", "(", "x", ",", "\n", "scale", "=", "0.1", ",", "\n", "block_type", "=", "'block17'", ",", "\n", "block_idx", "=", "block_idx", ")", "\n", "\n", "# Mixed 7a (Reduction-B block): 8 x 8 x 2080", "\n", "", "branch_0", "=", "conv2d_bn", "(", "x", ",", "256", ",", "1", ")", "\n", "branch_0", "=", "conv2d_bn", "(", "branch_0", ",", "384", ",", "3", ",", "strides", "=", "2", ",", "padding", "=", "'same'", ")", "\n", "branch_1", "=", "conv2d_bn", "(", "x", ",", "256", ",", "1", ")", "\n", "branch_1", "=", "conv2d_bn", "(", "branch_1", ",", "288", ",", "3", ",", "strides", "=", "2", ",", "padding", "=", "'same'", ")", "\n", "branch_2", "=", "conv2d_bn", "(", "x", ",", "256", ",", "1", ")", "\n", "branch_2", "=", "conv2d_bn", "(", "branch_2", ",", "288", ",", "3", ")", "\n", "branch_2", "=", "conv2d_bn", "(", "branch_2", ",", "320", ",", "3", ",", "strides", "=", "2", ",", "padding", "=", "'same'", ")", "\n", "branch_pool", "=", "MaxPooling2D", "(", "3", ",", "strides", "=", "2", ",", "padding", "=", "'same'", ")", "(", "x", ")", "\n", "branches", "=", "[", "branch_0", ",", "branch_1", ",", "branch_2", ",", "branch_pool", "]", "\n", "x", "=", "Concatenate", "(", "axis", "=", "channel_axis", ",", "name", "=", "'mixed_7a'", ")", "(", "branches", ")", "\n", "\n", "# 10x block8 (Inception-ResNet-C block): 8 x 8 x 2080", "\n", "for", "block_idx", "in", "range", "(", "1", ",", "10", ")", ":", "\n", "        ", "x", "=", "inception_resnet_block", "(", "x", ",", "\n", "scale", "=", "0.2", ",", "\n", "block_type", "=", "'block8'", ",", "\n", "block_idx", "=", "block_idx", ")", "\n", "", "x", "=", "inception_resnet_block", "(", "x", ",", "\n", "scale", "=", "1.", ",", "\n", "activation", "=", "None", ",", "\n", "block_type", "=", "'block8'", ",", "\n", "block_idx", "=", "10", ")", "\n", "\n", "# Final convolution block: 8 x 8 x 1536", "\n", "x", "=", "conv2d_bn", "(", "x", ",", "1536", ",", "1", ",", "name", "=", "'conv_7b'", ")", "\n", "\n", "if", "include_top", ":", "\n", "# Classification block", "\n", "        ", "x", "=", "GlobalAveragePooling2D", "(", "name", "=", "'avg_pool'", ")", "(", "x", ")", "\n", "x", "=", "Dense", "(", "classes", ",", "activation", "=", "'softmax'", ",", "name", "=", "'predictions'", ")", "(", "x", ")", "\n", "", "else", ":", "\n", "        ", "if", "pooling", "==", "'avg'", ":", "\n", "            ", "x", "=", "GlobalAveragePooling2D", "(", ")", "(", "x", ")", "\n", "", "elif", "pooling", "==", "'max'", ":", "\n", "            ", "x", "=", "GlobalMaxPooling2D", "(", ")", "(", "x", ")", "\n", "\n", "# Ensure that the model takes into account", "\n", "# any potential predecessors of `input_tensor`", "\n", "", "", "if", "input_tensor", "is", "not", "None", ":", "\n", "        ", "inputs", "=", "get_source_inputs", "(", "input_tensor", ")", "\n", "", "else", ":", "\n", "        ", "inputs", "=", "img_input", "\n", "\n", "# Create model", "\n", "", "model", "=", "Model", "(", "inputs", ",", "x", ",", "name", "=", "'inception_resnet_v2'", ")", "\n", "\n", "# Load weights", "\n", "if", "weights", "==", "'imagenet'", ":", "\n", "        ", "if", "K", ".", "image_data_format", "(", ")", "==", "'channels_first'", ":", "\n", "            ", "if", "K", ".", "backend", "(", ")", "==", "'tensorflow'", ":", "\n", "                ", "warnings", ".", "warn", "(", "'You are using the TensorFlow backend, yet you '", "\n", "'are using the Theano '", "\n", "'image data format convention '", "\n", "'(`image_data_format=\"channels_first\"`). '", "\n", "'For best performance, set '", "\n", "'`image_data_format=\"channels_last\"` in '", "\n", "'your Keras config '", "\n", "'at ~/.keras/keras.json.'", ")", "\n", "", "", "if", "include_top", ":", "\n", "            ", "fname", "=", "'inception_resnet_v2_weights_tf_dim_ordering_tf_kernels.h5'", "\n", "weights_path", "=", "get_file", "(", "fname", ",", "\n", "BASE_WEIGHT_URL", "+", "fname", ",", "\n", "cache_subdir", "=", "'models'", ",", "\n", "file_hash", "=", "'e693bd0210a403b3192acc6073ad2e96'", ")", "\n", "", "else", ":", "\n", "            ", "fname", "=", "'inception_resnet_v2_weights_tf_dim_ordering_tf_kernels_notop.h5'", "\n", "weights_path", "=", "get_file", "(", "fname", ",", "\n", "BASE_WEIGHT_URL", "+", "fname", ",", "\n", "cache_subdir", "=", "'models'", ",", "\n", "file_hash", "=", "'d19885ff4a710c122648d3b5c3b684e4'", ")", "\n", "", "model", ".", "load_weights", "(", "weights_path", ")", "\n", "", "elif", "weights", "is", "not", "None", ":", "\n", "        ", "model", ".", "load_weights", "(", "weights", ")", "\n", "\n", "", "return", "model", "", "", ""]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.backbones.preprocessing.get_preprocessing": [[33, 35], ["None"], "function", ["None"], ["return", "axis", "\n", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.backbones.backbones.get_backbone": [[31, 33], ["None"], "function", ["None"], ["def", "get_backbone", "(", "name", ",", "*", "args", ",", "**", "kwargs", ")", ":", "\n", "    ", "return", "backbones", "[", "name", "]", "(", "*", "args", ",", "**", "kwargs", ")", "", "", ""]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.classification_models.utils.find_weights": [[4, 9], ["list", "list", "list", "filter", "filter", "filter"], "function", ["None"], ["#    you may not use this file except in compliance with the License.", "\n", "#    You may obtain a copy of the License at", "\n", "#", "\n", "#        http://www.apache.org/licenses/LICENSE-2.0", "\n", "#", "\n", "#    Unless required by applicable law or agreed to in writing, software", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.classification_models.utils.load_model_weights": [[11, 32], ["utils.find_weights", "keras.utils.get_file", "model.load_weights", "ValueError", "ValueError"], "function", ["home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.classification_models.utils.find_weights"], ["#    WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.", "\n", "#    See the License for the specific language governing permissions and", "\n", "#    limitations under the License.", "\n", "\n", "import", "json", "\n", "import", "os", "\n", "import", "pickle", "\n", "import", "shutil", "\n", "from", "collections", "import", "OrderedDict", "\n", "from", "multiprocessing", "import", "Pool", "\n", "\n", "import", "numpy", "as", "np", "\n", "from", "batchgenerators", ".", "utilities", ".", "file_and_folder_operations", "import", "join", ",", "isdir", ",", "maybe_mkdir_p", ",", "subfiles", ",", "subdirs", ",", "isfile", "\n", "from", "nnunet", ".", "configuration", "import", "default_num_threads", "\n", "from", "nnunet", ".", "experiment_planning", ".", "DatasetAnalyzer", "import", "DatasetAnalyzer", "\n", "from", "nnunet", ".", "experiment_planning", ".", "common_utils", "import", "split_4d_nifti", "\n", "from", "nnunet", ".", "paths", "import", "nnUNet_raw_data", ",", "nnUNet_cropped_data", ",", "preprocessing_output_dir", "\n", "from", "nnunet", ".", "preprocessing", ".", "cropping", "import", "ImageCropper", "\n", "\n", "\n", "def", "split_4d", "(", "input_folder", ",", "num_processes", "=", "default_num_threads", ",", "overwrite_task_output_id", "=", "None", ")", ":", "\n", "    ", "assert", "isdir", "(", "join", "(", "input_folder", ",", "\"imagesTr\"", ")", ")", "and", "isdir", "(", "join", "(", "input_folder", ",", "\"labelsTr\"", ")", ")", "and"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.resnet.models.ResNet18": [[6, 18], ["builder.build_resnet", "utils.load_model_weights"], "function", ["home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.resnet.builder.build_resnet", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.classification_models.utils.load_model_weights"], ["def", "ResNet18", "(", "input_shape", ",", "input_tensor", "=", "None", ",", "weights", "=", "None", ",", "classes", "=", "1000", ",", "include_top", "=", "True", ")", ":", "\n", "    ", "model", "=", "build_resnet", "(", "input_tensor", "=", "input_tensor", ",", "\n", "input_shape", "=", "input_shape", ",", "\n", "repetitions", "=", "(", "2", ",", "2", ",", "2", ",", "2", ")", ",", "\n", "classes", "=", "classes", ",", "\n", "include_top", "=", "include_top", ",", "\n", "block_type", "=", "'basic'", ")", "\n", "model", ".", "name", "=", "'resnet18'", "\n", "\n", "if", "weights", ":", "\n", "        ", "load_model_weights", "(", "weights_collection", ",", "model", ",", "weights", ",", "classes", ",", "include_top", ")", "\n", "", "return", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.resnet.models.ResNet34": [[20, 32], ["builder.build_resnet", "utils.load_model_weights"], "function", ["home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.resnet.builder.build_resnet", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.classification_models.utils.load_model_weights"], ["", "def", "ResNet34", "(", "input_shape", ",", "input_tensor", "=", "None", ",", "weights", "=", "None", ",", "classes", "=", "1000", ",", "include_top", "=", "True", ")", ":", "\n", "    ", "model", "=", "build_resnet", "(", "input_tensor", "=", "input_tensor", ",", "\n", "input_shape", "=", "input_shape", ",", "\n", "repetitions", "=", "(", "3", ",", "4", ",", "6", ",", "3", ")", ",", "\n", "classes", "=", "classes", ",", "\n", "include_top", "=", "include_top", ",", "\n", "block_type", "=", "'basic'", ")", "\n", "model", ".", "name", "=", "'resnet34'", "\n", "\n", "if", "weights", ":", "\n", "        ", "load_model_weights", "(", "weights_collection", ",", "model", ",", "weights", ",", "classes", ",", "include_top", ")", "\n", "", "return", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.resnet.models.ResNet50": [[34, 45], ["builder.build_resnet", "utils.load_model_weights"], "function", ["home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.resnet.builder.build_resnet", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.classification_models.utils.load_model_weights"], ["", "def", "ResNet50", "(", "input_shape", ",", "input_tensor", "=", "None", ",", "weights", "=", "None", ",", "classes", "=", "1000", ",", "include_top", "=", "True", ")", ":", "\n", "    ", "model", "=", "build_resnet", "(", "input_tensor", "=", "input_tensor", ",", "\n", "input_shape", "=", "input_shape", ",", "\n", "repetitions", "=", "(", "3", ",", "4", ",", "6", ",", "3", ")", ",", "\n", "classes", "=", "classes", ",", "\n", "include_top", "=", "include_top", ")", "\n", "model", ".", "name", "=", "'resnet50'", "\n", "\n", "if", "weights", ":", "\n", "        ", "load_model_weights", "(", "weights_collection", ",", "model", ",", "weights", ",", "classes", ",", "include_top", ")", "\n", "", "return", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.resnet.models.ResNet101": [[47, 58], ["builder.build_resnet", "utils.load_model_weights"], "function", ["home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.resnet.builder.build_resnet", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.classification_models.utils.load_model_weights"], ["", "def", "ResNet101", "(", "input_shape", ",", "input_tensor", "=", "None", ",", "weights", "=", "None", ",", "classes", "=", "1000", ",", "include_top", "=", "True", ")", ":", "\n", "    ", "model", "=", "build_resnet", "(", "input_tensor", "=", "input_tensor", ",", "\n", "input_shape", "=", "input_shape", ",", "\n", "repetitions", "=", "(", "3", ",", "4", ",", "23", ",", "3", ")", ",", "\n", "classes", "=", "classes", ",", "\n", "include_top", "=", "include_top", ")", "\n", "model", ".", "name", "=", "'resnet101'", "\n", "\n", "if", "weights", ":", "\n", "        ", "load_model_weights", "(", "weights_collection", ",", "model", ",", "weights", ",", "classes", ",", "include_top", ")", "\n", "", "return", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.resnet.models.ResNet152": [[60, 71], ["builder.build_resnet", "utils.load_model_weights"], "function", ["home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.resnet.builder.build_resnet", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.classification_models.utils.load_model_weights"], ["", "def", "ResNet152", "(", "input_shape", ",", "input_tensor", "=", "None", ",", "weights", "=", "None", ",", "classes", "=", "1000", ",", "include_top", "=", "True", ")", ":", "\n", "    ", "model", "=", "build_resnet", "(", "input_tensor", "=", "input_tensor", ",", "\n", "input_shape", "=", "input_shape", ",", "\n", "repetitions", "=", "(", "3", ",", "8", ",", "36", ",", "3", ")", ",", "\n", "classes", "=", "classes", ",", "\n", "include_top", "=", "include_top", ")", "\n", "model", ".", "name", "=", "'resnet152'", "\n", "\n", "if", "weights", ":", "\n", "        ", "load_model_weights", "(", "weights_collection", ",", "model", ",", "weights", ",", "classes", ",", "include_top", ")", "\n", "", "return", "model", "\n", "", ""]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.resnet.blocks.handle_block_names": [[11, 18], ["None"], "function", ["None"], ["bn_name", "=", "'decoder_stage{}-{}_bn'", ".", "format", "(", "stage", ",", "cols", ")", "\n", "relu_name", "=", "'decoder_stage{}-{}_relu'", ".", "format", "(", "stage", ",", "cols", ")", "\n", "up_name", "=", "'decoder_stage{}-{}_upsample'", ".", "format", "(", "stage", ",", "cols", ")", "\n", "merge_name", "=", "'merge_{}-{}'", ".", "format", "(", "stage", ",", "cols", ")", "\n", "return", "conv_name", ",", "bn_name", ",", "relu_name", ",", "up_name", ",", "merge_name", "\n", "\n", "\n", "", "def", "ConvRelu", "(", "filters", ",", "kernel_size", ",", "use_batchnorm", "=", "False", ",", "conv_name", "=", "'conv'", ",", "bn_name", "=", "'bn'", ",", "relu_name", "=", "'relu'", ")", ":", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.resnet.blocks.basic_identity_block": [[20, 51], ["params.get_conv_params", "params.get_bn_params", "blocks.handle_block_names", "keras.layers.BatchNormalization", "keras.layers.Activation", "keras.layers.ZeroPadding2D", "keras.layers.Conv2D", "keras.layers.BatchNormalization", "keras.layers.Activation", "keras.layers.ZeroPadding2D", "keras.layers.Conv2D", "keras.layers.Add"], "function", ["home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.resnext.params.get_conv_params", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.resnext.params.get_bn_params", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.unet.blocks.handle_block_names"], ["        ", "x", "=", "Conv2D", "(", "filters", ",", "kernel_size", ",", "padding", "=", "\"same\"", ",", "name", "=", "conv_name", ",", "use_bias", "=", "not", "(", "use_batchnorm", ")", ")", "(", "x", ")", "\n", "if", "use_batchnorm", ":", "\n", "            ", "x", "=", "BatchNormalization", "(", "name", "=", "bn_name", ")", "(", "x", ")", "\n", "", "x", "=", "Activation", "(", "'relu'", ",", "name", "=", "relu_name", ")", "(", "x", ")", "\n", "return", "x", "\n", "", "return", "layer", "\n", "\n", "\n", "", "def", "Upsample2D_block", "(", "filters", ",", "stage", ",", "cols", ",", "kernel_size", "=", "(", "3", ",", "3", ")", ",", "upsample_rate", "=", "(", "2", ",", "2", ")", ",", "\n", "use_batchnorm", "=", "False", ",", "skip", "=", "None", ")", ":", "\n", "\n", "    ", "def", "layer", "(", "input_tensor", ")", ":", "\n", "\n", "        ", "conv_name", ",", "bn_name", ",", "relu_name", ",", "up_name", ",", "merge_name", "=", "handle_block_names", "(", "stage", ",", "cols", ")", "\n", "\n", "x", "=", "UpSampling2D", "(", "size", "=", "upsample_rate", ",", "name", "=", "up_name", ")", "(", "input_tensor", ")", "\n", "\n", "if", "skip", "is", "not", "None", ":", "\n", "            ", "x", "=", "Concatenate", "(", "name", "=", "merge_name", ")", "(", "[", "x", ",", "skip", "]", ")", "\n", "\n", "", "x", "=", "ConvRelu", "(", "filters", ",", "kernel_size", ",", "use_batchnorm", "=", "use_batchnorm", ",", "\n", "conv_name", "=", "conv_name", "+", "'1'", ",", "bn_name", "=", "bn_name", "+", "'1'", ",", "relu_name", "=", "relu_name", "+", "'1'", ")", "(", "x", ")", "\n", "\n", "x", "=", "ConvRelu", "(", "filters", ",", "kernel_size", ",", "use_batchnorm", "=", "use_batchnorm", ",", "\n", "conv_name", "=", "conv_name", "+", "'2'", ",", "bn_name", "=", "bn_name", "+", "'2'", ",", "relu_name", "=", "relu_name", "+", "'2'", ")", "(", "x", ")", "\n", "\n", "return", "x", "\n", "", "return", "layer", "\n", "\n", "\n", "", "def", "Transpose2D_block", "(", "filters", ",", "stage", ",", "cols", ",", "kernel_size", "=", "(", "3", ",", "3", ")", ",", "upsample_rate", "=", "(", "2", ",", "2", ")", ",", "\n", "transpose_kernel_size", "=", "(", "4", ",", "4", ")", ",", "use_batchnorm", "=", "False", ",", "skip", "=", "None", ")", ":", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.resnet.blocks.basic_conv_block": [[53, 87], ["params.get_conv_params", "params.get_bn_params", "blocks.handle_block_names", "keras.layers.BatchNormalization", "keras.layers.Activation", "keras.layers.ZeroPadding2D", "keras.layers.Conv2D", "keras.layers.BatchNormalization", "keras.layers.Activation", "keras.layers.ZeroPadding2D", "keras.layers.Conv2D", "keras.layers.Conv2D", "keras.layers.Add"], "function", ["home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.resnext.params.get_conv_params", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.resnext.params.get_bn_params", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.unet.blocks.handle_block_names"], ["    ", "def", "layer", "(", "input_tensor", ")", ":", "\n", "\n", "        ", "conv_name", ",", "bn_name", ",", "relu_name", ",", "up_name", ",", "merge_name", "=", "handle_block_names", "(", "stage", ",", "cols", ")", "\n", "\n", "x", "=", "Conv2DTranspose", "(", "filters", ",", "transpose_kernel_size", ",", "strides", "=", "upsample_rate", ",", "\n", "padding", "=", "'same'", ",", "name", "=", "up_name", ",", "use_bias", "=", "not", "(", "use_batchnorm", ")", ")", "(", "input_tensor", ")", "\n", "if", "use_batchnorm", ":", "\n", "            ", "x", "=", "BatchNormalization", "(", "name", "=", "bn_name", "+", "'1'", ")", "(", "x", ")", "\n", "", "x", "=", "Activation", "(", "'relu'", ",", "name", "=", "relu_name", "+", "'1'", ")", "(", "x", ")", "\n", "\n", "if", "skip", "is", "not", "None", ":", "\n", "            ", "x", "=", "Concatenate", "(", "name", "=", "merge_name", ")", "(", "[", "x", ",", "skip", "]", ")", "\n", "\n", "", "x", "=", "ConvRelu", "(", "filters", ",", "kernel_size", ",", "use_batchnorm", "=", "use_batchnorm", ",", "\n", "conv_name", "=", "conv_name", "+", "'2'", ",", "bn_name", "=", "bn_name", "+", "'2'", ",", "relu_name", "=", "relu_name", "+", "'2'", ")", "(", "x", ")", "\n", "\n", "return", "x", "\n", "", "return", "layer", "", "", ""]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.resnet.blocks.conv_block": [[89, 126], ["params.get_conv_params", "params.get_bn_params", "blocks.handle_block_names", "keras.layers.BatchNormalization", "keras.layers.Activation", "keras.layers.Conv2D", "keras.layers.BatchNormalization", "keras.layers.Activation", "keras.layers.ZeroPadding2D", "keras.layers.Conv2D", "keras.layers.BatchNormalization", "keras.layers.Activation", "keras.layers.Conv2D", "keras.layers.Conv2D", "keras.layers.Add"], "function", ["home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.resnext.params.get_conv_params", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.resnext.params.get_bn_params", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.unet.blocks.handle_block_names"], []], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.resnet.blocks.identity_block": [[128, 162], ["params.get_conv_params", "params.get_bn_params", "blocks.handle_block_names", "keras.layers.BatchNormalization", "keras.layers.Activation", "keras.layers.Conv2D", "keras.layers.BatchNormalization", "keras.layers.Activation", "keras.layers.ZeroPadding2D", "keras.layers.Conv2D", "keras.layers.BatchNormalization", "keras.layers.Activation", "keras.layers.Conv2D", "keras.layers.Add"], "function", ["home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.resnext.params.get_conv_params", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.resnext.params.get_bn_params", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.unet.blocks.handle_block_names"], []], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.resnet.params.get_conv_params": [[5, 13], ["default_conv_params.update"], "function", ["None"], ["def", "get_conv_params", "(", "**", "params", ")", ":", "\n", "    ", "default_conv_params", "=", "{", "\n", "'kernel_initializer'", ":", "'glorot_uniform'", ",", "\n", "'use_bias'", ":", "False", ",", "\n", "'padding'", ":", "'valid'", ",", "\n", "}", "\n", "default_conv_params", ".", "update", "(", "params", ")", "\n", "return", "default_conv_params", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.resnet.params.get_bn_params": [[15, 25], ["default_bn_params.update"], "function", ["None"], ["", "def", "get_bn_params", "(", "**", "params", ")", ":", "\n", "    ", "default_bn_params", "=", "{", "\n", "'axis'", ":", "3", ",", "\n", "'momentum'", ":", "0.99", ",", "\n", "'epsilon'", ":", "2e-5", ",", "\n", "'center'", ":", "True", ",", "\n", "'scale'", ":", "True", ",", "\n", "}", "\n", "default_bn_params", ".", "update", "(", "params", ")", "\n", "return", "default_bn_params", "\n", "", ""]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.resnet.builder.build_resnet": [[28, 112], ["_obtain_input_shape", "params.get_bn_params", "params.get_bn_params", "params.get_conv_params", "enumerate", "keras.models.Model", "keras.layers.Input", "keras.layers.BatchNormalization", "keras.layers.ZeroPadding2D", "keras.layers.Conv2D", "keras.layers.BatchNormalization", "keras.layers.Activation", "keras.layers.ZeroPadding2D", "keras.layers.MaxPooling2D", "range", "keras.layers.BatchNormalization", "keras.layers.Activation", "keras.engine.get_source_inputs", "keras.is_keras_tensor", "keras.layers.Input", "keras.layers.GlobalAveragePooling2D", "keras.layers.Dense", "keras.layers.Activation", "blocks.conv_block", "blocks.conv_block", "blocks.identity_block"], "function", ["home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.resnext.params.get_bn_params", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.resnext.params.get_bn_params", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.resnext.params.get_conv_params", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.resnext.blocks.conv_block", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.resnext.blocks.conv_block", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.resnext.blocks.identity_block"], ["", "if", "len", "(", "skip_connection_layers", ")", ">", "n_upsample_blocks", ":", "\n", "        ", "downsampling_layers", "=", "skip_connection_layers", "[", "int", "(", "len", "(", "skip_connection_layers", ")", "/", "2", ")", ":", "]", "\n", "skip_connection_layers", "=", "skip_connection_layers", "[", ":", "int", "(", "len", "(", "skip_connection_layers", ")", "/", "2", ")", "]", "\n", "", "else", ":", "\n", "        ", "downsampling_layers", "=", "skip_connection_layers", "\n", "\n", "\n", "# convert layer names to indices", "\n", "", "skip_connection_idx", "=", "(", "[", "get_layer_number", "(", "backbone", ",", "l", ")", "if", "isinstance", "(", "l", ",", "str", ")", "else", "l", "\n", "for", "l", "in", "skip_connection_layers", "]", ")", "\n", "skip_layers_list", "=", "[", "backbone", ".", "layers", "[", "skip_connection_idx", "[", "i", "]", "]", ".", "output", "for", "i", "in", "range", "(", "len", "(", "skip_connection_idx", ")", ")", "]", "\n", "downsampling_idx", "=", "(", "[", "get_layer_number", "(", "backbone", ",", "l", ")", "if", "isinstance", "(", "l", ",", "str", ")", "else", "l", "\n", "for", "l", "in", "downsampling_layers", "]", ")", "\n", "downsampling_list", "=", "[", "backbone", ".", "layers", "[", "downsampling_idx", "[", "i", "]", "]", ".", "output", "for", "i", "in", "range", "(", "len", "(", "downsampling_idx", ")", ")", "]", "\n", "downterm", "=", "[", "None", "]", "*", "(", "n_upsample_blocks", "+", "1", ")", "\n", "for", "i", "in", "range", "(", "len", "(", "downsampling_idx", ")", ")", ":", "\n", "# print(downsampling_list[0])", "\n", "# print(backbone.output)", "\n", "# print(\"\")", "\n", "        ", "if", "downsampling_list", "[", "0", "]", "==", "backbone", ".", "output", ":", "\n", "# print(\"VGG16 should be!\")", "\n", "            ", "downterm", "[", "n_upsample_blocks", "-", "i", "]", "=", "downsampling_list", "[", "i", "]", "\n", "", "else", ":", "\n", "            ", "downterm", "[", "n_upsample_blocks", "-", "i", "-", "1", "]", "=", "downsampling_list", "[", "i", "]", "\n", "", "", "downterm", "[", "-", "1", "]", "=", "backbone", ".", "output", "\n", "# print(\"downterm = {}\".format(downterm))", "\n", "\n", "interm", "=", "[", "None", "]", "*", "(", "n_upsample_blocks", "+", "1", ")", "*", "(", "n_upsample_blocks", "+", "1", ")", "\n", "for", "i", "in", "range", "(", "len", "(", "skip_connection_idx", ")", ")", ":", "\n", "        ", "interm", "[", "-", "i", "*", "(", "n_upsample_blocks", "+", "1", ")", "+", "(", "n_upsample_blocks", "+", "1", ")", "*", "(", "n_upsample_blocks", "-", "1", ")", "]", "=", "skip_layers_list", "[", "i", "]", "\n", "", "interm", "[", "(", "n_upsample_blocks", "+", "1", ")", "*", "n_upsample_blocks", "]", "=", "backbone", ".", "output", "\n", "\n", "for", "j", "in", "range", "(", "n_upsample_blocks", ")", ":", "\n", "        ", "for", "i", "in", "range", "(", "n_upsample_blocks", "-", "j", ")", ":", "\n", "            ", "upsample_rate", "=", "to_tuple", "(", "upsample_rates", "[", "i", "]", ")", "\n", "# print(j, i)", "\n", "\n", "if", "i", "==", "0", "and", "j", "<", "n_upsample_blocks", "-", "1", "and", "len", "(", "skip_connection_layers", ")", "<", "n_upsample_blocks", ":", "\n", "                ", "interm", "[", "(", "n_upsample_blocks", "+", "1", ")", "*", "i", "+", "j", "+", "1", "]", "=", "None", "\n", "", "elif", "j", "==", "0", ":", "\n", "                ", "if", "downterm", "[", "i", "+", "1", "]", "is", "not", "None", ":", "\n", "                    ", "interm", "[", "(", "n_upsample_blocks", "+", "1", ")", "*", "i", "+", "j", "+", "1", "]", "=", "up_block", "(", "decoder_filters", "[", "n_upsample_blocks", "-", "i", "-", "2", "]", ",", "\n", "i", "+", "1", ",", "j", "+", "1", ",", "upsample_rate", "=", "upsample_rate", ",", "\n", "skip", "=", "interm", "[", "(", "n_upsample_blocks", "+", "1", ")", "*", "i", "+", "j", "]", ",", "\n", "use_batchnorm", "=", "use_batchnorm", ")", "(", "downterm", "[", "i", "+", "1", "]", ")", "\n", "", "else", ":", "\n", "                    ", "interm", "[", "(", "n_upsample_blocks", "+", "1", ")", "*", "i", "+", "j", "+", "1", "]", "=", "None", "\n", "# print(\"\\n{} = {} + {}\\n\".format(interm[(n_upsample_blocks+1)*i+j+1],", "\n", "#                             interm[(n_upsample_blocks+1)*i+j], ", "\n", "#                             downterm[i+1]))", "\n", "", "", "else", ":", "\n", "                ", "interm", "[", "(", "n_upsample_blocks", "+", "1", ")", "*", "i", "+", "j", "+", "1", "]", "=", "up_block", "(", "decoder_filters", "[", "n_upsample_blocks", "-", "i", "-", "2", "]", ",", "\n", "i", "+", "1", ",", "j", "+", "1", ",", "upsample_rate", "=", "upsample_rate", ",", "\n", "skip", "=", "interm", "[", "(", "n_upsample_blocks", "+", "1", ")", "*", "i", "+", "j", "]", ",", "\n", "use_batchnorm", "=", "use_batchnorm", ")", "(", "interm", "[", "(", "n_upsample_blocks", "+", "1", ")", "*", "(", "i", "+", "1", ")", "+", "j", "]", ")", "\n", "# print(\"\\n{} = {} + {}\\n\".format(interm[(n_upsample_blocks+1)*i+j+1],", "\n", "#                             interm[(n_upsample_blocks+1)*i+j], ", "\n", "#                             interm[(n_upsample_blocks+1)*(i+1)+j]))", "\n", "# print('\\n\\n\\n')", "\n", "# for x in range(n_upsample_blocks+1):", "\n", "#     for y in range(n_upsample_blocks+1):", "\n", "#         print(interm[x*(n_upsample_blocks+1)+y], end=' ', flush=True)", "\n", "#     print('\\n')", "\n", "# print('\\n\\n\\n')", "\n", "#print(interm)", "\n", "\n", "", "", ""]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.resnet.preprocessing.preprocess_input": [[4, 19], ["skimage.transform.resize"], "function", ["None"], ["#    you may not use this file except in compliance with the License.", "\n", "#    You may obtain a copy of the License at", "\n", "#", "\n", "#        http://www.apache.org/licenses/LICENSE-2.0", "\n", "#", "\n", "#    Unless required by applicable law or agreed to in writing, software", "\n", "#    distributed under the License is distributed on an \"AS IS\" BASIS,", "\n", "#    WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.", "\n", "#    See the License for the specific language governing permissions and", "\n", "#    limitations under the License.", "\n", "\n", "from", "collections", "import", "OrderedDict", "\n", "from", "batchgenerators", ".", "augmentations", ".", "utils", "import", "resize_segmentation", "\n", "from", "nnunet", ".", "configuration", "import", "default_num_threads", ",", "RESAMPLING_SEPARATE_Z_ANISO_THRESHOLD", "\n", "from", "nnunet", ".", "preprocessing", ".", "cropping", "import", "get_case_identifier_from_npz", ",", "ImageCropper", "\n", "from", "skimage", ".", "transform", "import", "resize", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.resnext.models.ResNeXt50": [[6, 18], ["builder.build_resnext", "utils.load_model_weights"], "function", ["home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.resnext.builder.build_resnext", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.classification_models.utils.load_model_weights"], ["def", "ResNet18", "(", "input_shape", ",", "input_tensor", "=", "None", ",", "weights", "=", "None", ",", "classes", "=", "1000", ",", "include_top", "=", "True", ")", ":", "\n", "    ", "model", "=", "build_resnet", "(", "input_tensor", "=", "input_tensor", ",", "\n", "input_shape", "=", "input_shape", ",", "\n", "repetitions", "=", "(", "2", ",", "2", ",", "2", ",", "2", ")", ",", "\n", "classes", "=", "classes", ",", "\n", "include_top", "=", "include_top", ",", "\n", "block_type", "=", "'basic'", ")", "\n", "model", ".", "name", "=", "'resnet18'", "\n", "\n", "if", "weights", ":", "\n", "        ", "load_model_weights", "(", "weights_collection", ",", "model", ",", "weights", ",", "classes", ",", "include_top", ")", "\n", "", "return", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.resnext.models.ResNeXt101": [[20, 32], ["builder.build_resnext", "utils.load_model_weights"], "function", ["home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.resnext.builder.build_resnext", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.classification_models.utils.load_model_weights"], ["", "def", "ResNet34", "(", "input_shape", ",", "input_tensor", "=", "None", ",", "weights", "=", "None", ",", "classes", "=", "1000", ",", "include_top", "=", "True", ")", ":", "\n", "    ", "model", "=", "build_resnet", "(", "input_tensor", "=", "input_tensor", ",", "\n", "input_shape", "=", "input_shape", ",", "\n", "repetitions", "=", "(", "3", ",", "4", ",", "6", ",", "3", ")", ",", "\n", "classes", "=", "classes", ",", "\n", "include_top", "=", "include_top", ",", "\n", "block_type", "=", "'basic'", ")", "\n", "model", ".", "name", "=", "'resnet34'", "\n", "\n", "if", "weights", ":", "\n", "        ", "load_model_weights", "(", "weights_collection", ",", "model", ",", "weights", ",", "classes", ",", "include_top", ")", "\n", "", "return", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.resnext.blocks.handle_block_names": [[13, 20], ["None"], "function", ["None"], ["up_name", "=", "'decoder_stage{}-{}_upsample'", ".", "format", "(", "stage", ",", "cols", ")", "\n", "merge_name", "=", "'merge_{}-{}'", ".", "format", "(", "stage", ",", "cols", ")", "\n", "return", "conv_name", ",", "bn_name", ",", "relu_name", ",", "up_name", ",", "merge_name", "\n", "\n", "\n", "", "def", "ConvRelu", "(", "filters", ",", "kernel_size", ",", "use_batchnorm", "=", "False", ",", "conv_name", "=", "'conv'", ",", "bn_name", "=", "'bn'", ",", "relu_name", "=", "'relu'", ")", ":", "\n", "    ", "def", "layer", "(", "x", ")", ":", "\n", "        ", "x", "=", "Conv2D", "(", "filters", ",", "kernel_size", ",", "padding", "=", "\"same\"", ",", "name", "=", "conv_name", ",", "use_bias", "=", "not", "(", "use_batchnorm", ")", ")", "(", "x", ")", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.resnext.blocks.GroupConv2D": [[22, 39], ["range", "int", "blocks.append", "keras.layers.Concatenate", "keras.layers.Lambda", "str", "keras.layers.Conv2D"], "function", ["None"], ["            ", "x", "=", "BatchNormalization", "(", "name", "=", "bn_name", ")", "(", "x", ")", "\n", "", "x", "=", "Activation", "(", "'relu'", ",", "name", "=", "relu_name", ")", "(", "x", ")", "\n", "return", "x", "\n", "", "return", "layer", "\n", "\n", "\n", "", "def", "Upsample2D_block", "(", "filters", ",", "stage", ",", "cols", ",", "kernel_size", "=", "(", "3", ",", "3", ")", ",", "upsample_rate", "=", "(", "2", ",", "2", ")", ",", "\n", "use_batchnorm", "=", "False", ",", "skip", "=", "None", ")", ":", "\n", "\n", "    ", "def", "layer", "(", "input_tensor", ")", ":", "\n", "\n", "        ", "conv_name", ",", "bn_name", ",", "relu_name", ",", "up_name", ",", "merge_name", "=", "handle_block_names", "(", "stage", ",", "cols", ")", "\n", "\n", "x", "=", "UpSampling2D", "(", "size", "=", "upsample_rate", ",", "name", "=", "up_name", ")", "(", "input_tensor", ")", "\n", "\n", "if", "skip", "is", "not", "None", ":", "\n", "            ", "x", "=", "Concatenate", "(", "name", "=", "merge_name", ")", "(", "[", "x", ",", "skip", "]", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.resnext.blocks.conv_block": [[41, 79], ["params.get_conv_params", "params.get_bn_params", "blocks.handle_block_names", "keras.layers.Conv2D", "keras.layers.BatchNormalization", "keras.layers.Activation", "keras.layers.ZeroPadding2D", "blocks.GroupConv2D", "keras.layers.BatchNormalization", "keras.layers.Activation", "keras.layers.Conv2D", "keras.layers.BatchNormalization", "keras.layers.Conv2D", "keras.layers.BatchNormalization", "keras.layers.Add", "keras.layers.Activation"], "function", ["home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.resnext.params.get_conv_params", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.resnext.params.get_bn_params", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.unet.blocks.handle_block_names", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.resnext.blocks.GroupConv2D"], ["conv_name", "=", "conv_name", "+", "'1'", ",", "bn_name", "=", "bn_name", "+", "'1'", ",", "relu_name", "=", "relu_name", "+", "'1'", ")", "(", "x", ")", "\n", "\n", "x", "=", "ConvRelu", "(", "filters", ",", "kernel_size", ",", "use_batchnorm", "=", "use_batchnorm", ",", "\n", "conv_name", "=", "conv_name", "+", "'2'", ",", "bn_name", "=", "bn_name", "+", "'2'", ",", "relu_name", "=", "relu_name", "+", "'2'", ")", "(", "x", ")", "\n", "\n", "return", "x", "\n", "", "return", "layer", "\n", "\n", "\n", "", "def", "Transpose2D_block", "(", "filters", ",", "stage", ",", "cols", ",", "kernel_size", "=", "(", "3", ",", "3", ")", ",", "upsample_rate", "=", "(", "2", ",", "2", ")", ",", "\n", "transpose_kernel_size", "=", "(", "4", ",", "4", ")", ",", "use_batchnorm", "=", "False", ",", "skip", "=", "None", ")", ":", "\n", "\n", "    ", "def", "layer", "(", "input_tensor", ")", ":", "\n", "\n", "        ", "conv_name", ",", "bn_name", ",", "relu_name", ",", "up_name", ",", "merge_name", "=", "handle_block_names", "(", "stage", ",", "cols", ")", "\n", "\n", "x", "=", "Conv2DTranspose", "(", "filters", ",", "transpose_kernel_size", ",", "strides", "=", "upsample_rate", ",", "\n", "padding", "=", "'same'", ",", "name", "=", "up_name", ",", "use_bias", "=", "not", "(", "use_batchnorm", ")", ")", "(", "input_tensor", ")", "\n", "if", "use_batchnorm", ":", "\n", "            ", "x", "=", "BatchNormalization", "(", "name", "=", "bn_name", "+", "'1'", ")", "(", "x", ")", "\n", "", "x", "=", "Activation", "(", "'relu'", ",", "name", "=", "relu_name", "+", "'1'", ")", "(", "x", ")", "\n", "\n", "if", "skip", "is", "not", "None", ":", "\n", "            ", "x", "=", "Concatenate", "(", "name", "=", "merge_name", ")", "(", "[", "x", ",", "skip", "]", ")", "\n", "\n", "", "x", "=", "ConvRelu", "(", "filters", ",", "kernel_size", ",", "use_batchnorm", "=", "use_batchnorm", ",", "\n", "conv_name", "=", "conv_name", "+", "'2'", ",", "bn_name", "=", "bn_name", "+", "'2'", ",", "relu_name", "=", "relu_name", "+", "'2'", ")", "(", "x", ")", "\n", "\n", "return", "x", "\n", "", "return", "layer", "", "", ""]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.resnext.blocks.identity_block": [[81, 114], ["params.get_conv_params", "params.get_bn_params", "blocks.handle_block_names", "keras.layers.Conv2D", "keras.layers.BatchNormalization", "keras.layers.Activation", "keras.layers.ZeroPadding2D", "blocks.GroupConv2D", "keras.layers.BatchNormalization", "keras.layers.Activation", "keras.layers.Conv2D", "keras.layers.BatchNormalization", "keras.layers.Add", "keras.layers.Activation"], "function", ["home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.resnext.params.get_conv_params", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.resnext.params.get_bn_params", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.unet.blocks.handle_block_names", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.resnext.blocks.GroupConv2D"], []], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.resnext.params.get_conv_params": [[5, 13], ["default_conv_params.update"], "function", ["None"], ["def", "get_conv_params", "(", "**", "params", ")", ":", "\n", "    ", "default_conv_params", "=", "{", "\n", "'kernel_initializer'", ":", "'glorot_uniform'", ",", "\n", "'use_bias'", ":", "False", ",", "\n", "'padding'", ":", "'valid'", ",", "\n", "}", "\n", "default_conv_params", ".", "update", "(", "params", ")", "\n", "return", "default_conv_params", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.resnext.params.get_bn_params": [[15, 25], ["default_bn_params.update"], "function", ["None"], ["", "def", "get_bn_params", "(", "**", "params", ")", ":", "\n", "    ", "default_bn_params", "=", "{", "\n", "'axis'", ":", "3", ",", "\n", "'momentum'", ":", "0.99", ",", "\n", "'epsilon'", ":", "2e-5", ",", "\n", "'center'", ":", "True", ",", "\n", "'scale'", ":", "True", ",", "\n", "}", "\n", "default_bn_params", ".", "update", "(", "params", ")", "\n", "return", "default_bn_params", "\n", "", ""]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.resnext.builder.build_resnext": [[29, 104], ["_obtain_input_shape", "params.get_bn_params", "params.get_bn_params", "params.get_conv_params", "enumerate", "keras.models.Model", "keras.layers.Input", "keras.layers.BatchNormalization", "keras.layers.ZeroPadding2D", "keras.layers.Conv2D", "keras.layers.BatchNormalization", "keras.layers.Activation", "keras.layers.ZeroPadding2D", "keras.layers.MaxPooling2D", "range", "keras.engine.get_source_inputs", "keras.is_keras_tensor", "keras.layers.Input", "keras.layers.GlobalAveragePooling2D", "keras.layers.Dense", "keras.layers.Activation", "blocks.conv_block", "blocks.conv_block", "blocks.identity_block"], "function", ["home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.resnext.params.get_bn_params", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.resnext.params.get_bn_params", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.resnext.params.get_conv_params", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.resnext.blocks.conv_block", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.resnext.blocks.conv_block", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.resnext.blocks.identity_block"], ["        ", "downsampling_layers", "=", "skip_connection_layers", "[", "int", "(", "len", "(", "skip_connection_layers", ")", "/", "2", ")", ":", "]", "\n", "skip_connection_layers", "=", "skip_connection_layers", "[", ":", "int", "(", "len", "(", "skip_connection_layers", ")", "/", "2", ")", "]", "\n", "", "else", ":", "\n", "        ", "downsampling_layers", "=", "skip_connection_layers", "\n", "\n", "\n", "# convert layer names to indices", "\n", "", "skip_connection_idx", "=", "(", "[", "get_layer_number", "(", "backbone", ",", "l", ")", "if", "isinstance", "(", "l", ",", "str", ")", "else", "l", "\n", "for", "l", "in", "skip_connection_layers", "]", ")", "\n", "skip_layers_list", "=", "[", "backbone", ".", "layers", "[", "skip_connection_idx", "[", "i", "]", "]", ".", "output", "for", "i", "in", "range", "(", "len", "(", "skip_connection_idx", ")", ")", "]", "\n", "downsampling_idx", "=", "(", "[", "get_layer_number", "(", "backbone", ",", "l", ")", "if", "isinstance", "(", "l", ",", "str", ")", "else", "l", "\n", "for", "l", "in", "downsampling_layers", "]", ")", "\n", "downsampling_list", "=", "[", "backbone", ".", "layers", "[", "downsampling_idx", "[", "i", "]", "]", ".", "output", "for", "i", "in", "range", "(", "len", "(", "downsampling_idx", ")", ")", "]", "\n", "downterm", "=", "[", "None", "]", "*", "(", "n_upsample_blocks", "+", "1", ")", "\n", "for", "i", "in", "range", "(", "len", "(", "downsampling_idx", ")", ")", ":", "\n", "# print(downsampling_list[0])", "\n", "# print(backbone.output)", "\n", "# print(\"\")", "\n", "        ", "if", "downsampling_list", "[", "0", "]", "==", "backbone", ".", "output", ":", "\n", "# print(\"VGG16 should be!\")", "\n", "            ", "downterm", "[", "n_upsample_blocks", "-", "i", "]", "=", "downsampling_list", "[", "i", "]", "\n", "", "else", ":", "\n", "            ", "downterm", "[", "n_upsample_blocks", "-", "i", "-", "1", "]", "=", "downsampling_list", "[", "i", "]", "\n", "", "", "downterm", "[", "-", "1", "]", "=", "backbone", ".", "output", "\n", "# print(\"downterm = {}\".format(downterm))", "\n", "\n", "interm", "=", "[", "None", "]", "*", "(", "n_upsample_blocks", "+", "1", ")", "*", "(", "n_upsample_blocks", "+", "1", ")", "\n", "for", "i", "in", "range", "(", "len", "(", "skip_connection_idx", ")", ")", ":", "\n", "        ", "interm", "[", "-", "i", "*", "(", "n_upsample_blocks", "+", "1", ")", "+", "(", "n_upsample_blocks", "+", "1", ")", "*", "(", "n_upsample_blocks", "-", "1", ")", "]", "=", "skip_layers_list", "[", "i", "]", "\n", "", "interm", "[", "(", "n_upsample_blocks", "+", "1", ")", "*", "n_upsample_blocks", "]", "=", "backbone", ".", "output", "\n", "\n", "for", "j", "in", "range", "(", "n_upsample_blocks", ")", ":", "\n", "        ", "for", "i", "in", "range", "(", "n_upsample_blocks", "-", "j", ")", ":", "\n", "            ", "upsample_rate", "=", "to_tuple", "(", "upsample_rates", "[", "i", "]", ")", "\n", "# print(j, i)", "\n", "\n", "if", "i", "==", "0", "and", "j", "<", "n_upsample_blocks", "-", "1", "and", "len", "(", "skip_connection_layers", ")", "<", "n_upsample_blocks", ":", "\n", "                ", "interm", "[", "(", "n_upsample_blocks", "+", "1", ")", "*", "i", "+", "j", "+", "1", "]", "=", "None", "\n", "", "elif", "j", "==", "0", ":", "\n", "                ", "if", "downterm", "[", "i", "+", "1", "]", "is", "not", "None", ":", "\n", "                    ", "interm", "[", "(", "n_upsample_blocks", "+", "1", ")", "*", "i", "+", "j", "+", "1", "]", "=", "up_block", "(", "decoder_filters", "[", "n_upsample_blocks", "-", "i", "-", "2", "]", ",", "\n", "i", "+", "1", ",", "j", "+", "1", ",", "upsample_rate", "=", "upsample_rate", ",", "\n", "skip", "=", "interm", "[", "(", "n_upsample_blocks", "+", "1", ")", "*", "i", "+", "j", "]", ",", "\n", "use_batchnorm", "=", "use_batchnorm", ")", "(", "downterm", "[", "i", "+", "1", "]", ")", "\n", "", "else", ":", "\n", "                    ", "interm", "[", "(", "n_upsample_blocks", "+", "1", ")", "*", "i", "+", "j", "+", "1", "]", "=", "None", "\n", "# print(\"\\n{} = {} + {}\\n\".format(interm[(n_upsample_blocks+1)*i+j+1],", "\n", "#                             interm[(n_upsample_blocks+1)*i+j], ", "\n", "#                             downterm[i+1]))", "\n", "", "", "else", ":", "\n", "                ", "interm", "[", "(", "n_upsample_blocks", "+", "1", ")", "*", "i", "+", "j", "+", "1", "]", "=", "up_block", "(", "decoder_filters", "[", "n_upsample_blocks", "-", "i", "-", "2", "]", ",", "\n", "i", "+", "1", ",", "j", "+", "1", ",", "upsample_rate", "=", "upsample_rate", ",", "\n", "skip", "=", "interm", "[", "(", "n_upsample_blocks", "+", "1", ")", "*", "i", "+", "j", "]", ",", "\n", "use_batchnorm", "=", "use_batchnorm", ")", "(", "interm", "[", "(", "n_upsample_blocks", "+", "1", ")", "*", "(", "i", "+", "1", ")", "+", "j", "]", ")", "\n", "# print(\"\\n{} = {} + {}\\n\".format(interm[(n_upsample_blocks+1)*i+j+1],", "\n", "#                             interm[(n_upsample_blocks+1)*i+j], ", "\n", "#                             interm[(n_upsample_blocks+1)*(i+1)+j]))", "\n", "# print('\\n\\n\\n')", "\n", "# for x in range(n_upsample_blocks+1):", "\n", "#     for y in range(n_upsample_blocks+1):", "\n", "#         print(interm[x*(n_upsample_blocks+1)+y], end=' ', flush=True)", "\n", "#     print('\\n')", "\n", "# print('\\n\\n\\n')", "\n", "#print(interm)", "\n", "\n", "", "", ""]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.resnext.preprocessing.preprocess_input": [[4, 16], ["skimage.transform.resize"], "function", ["None"], ["#    you may not use this file except in compliance with the License.", "\n", "#    You may obtain a copy of the License at", "\n", "#", "\n", "#        http://www.apache.org/licenses/LICENSE-2.0", "\n", "#", "\n", "#    Unless required by applicable law or agreed to in writing, software", "\n", "#    distributed under the License is distributed on an \"AS IS\" BASIS,", "\n", "#    WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.", "\n", "#    See the License for the specific language governing permissions and", "\n", "#    limitations under the License.", "\n", "\n", "from", "collections", "import", "OrderedDict", "\n", "from", "batchgenerators", ".", "augmentations", ".", "utils", "import", "resize_segmentation", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.unet.blocks.handle_block_names": [[9, 15], ["None"], "function", ["None"], ["def", "handle_block_names", "(", "stage", ",", "cols", ")", ":", "\n", "    ", "conv_name", "=", "'decoder_stage{}-{}_conv'", ".", "format", "(", "stage", ",", "cols", ")", "\n", "bn_name", "=", "'decoder_stage{}-{}_bn'", ".", "format", "(", "stage", ",", "cols", ")", "\n", "relu_name", "=", "'decoder_stage{}-{}_relu'", ".", "format", "(", "stage", ",", "cols", ")", "\n", "up_name", "=", "'decoder_stage{}-{}_upsample'", ".", "format", "(", "stage", ",", "cols", ")", "\n", "merge_name", "=", "'merge_{}-{}'", ".", "format", "(", "stage", ",", "cols", ")", "\n", "return", "conv_name", ",", "bn_name", ",", "relu_name", ",", "up_name", ",", "merge_name", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.unet.blocks.ConvRelu": [[17, 25], ["keras.layers.Conv2D", "keras.layers.Activation", "keras.layers.BatchNormalization"], "function", ["None"], ["\n", "", "def", "ConvRelu", "(", "filters", ",", "kernel_size", ",", "use_batchnorm", "=", "False", ",", "conv_name", "=", "'conv'", ",", "bn_name", "=", "'bn'", ",", "relu_name", "=", "'relu'", ")", ":", "\n", "    ", "def", "layer", "(", "x", ")", ":", "\n", "        ", "x", "=", "Conv2D", "(", "filters", ",", "kernel_size", ",", "padding", "=", "\"same\"", ",", "name", "=", "conv_name", ",", "use_bias", "=", "not", "(", "use_batchnorm", ")", ")", "(", "x", ")", "\n", "if", "use_batchnorm", ":", "\n", "            ", "x", "=", "BatchNormalization", "(", "name", "=", "bn_name", ")", "(", "x", ")", "\n", "", "x", "=", "Activation", "(", "'relu'", ",", "name", "=", "relu_name", ")", "(", "x", ")", "\n", "return", "x", "\n", "", "return", "layer", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.unet.blocks.Upsample2D_block": [[27, 47], ["blocks.handle_block_names", "keras.layers.UpSampling2D", "blocks.ConvRelu", "blocks.ConvRelu", "keras.layers.Concatenate"], "function", ["home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.unet.blocks.handle_block_names", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.unet.blocks.ConvRelu", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.unet.blocks.ConvRelu"], ["\n", "", "def", "Upsample2D_block", "(", "filters", ",", "stage", ",", "cols", ",", "kernel_size", "=", "(", "3", ",", "3", ")", ",", "upsample_rate", "=", "(", "2", ",", "2", ")", ",", "\n", "use_batchnorm", "=", "False", ",", "skip", "=", "None", ")", ":", "\n", "\n", "    ", "def", "layer", "(", "input_tensor", ")", ":", "\n", "\n", "        ", "conv_name", ",", "bn_name", ",", "relu_name", ",", "up_name", ",", "merge_name", "=", "handle_block_names", "(", "stage", ",", "cols", ")", "\n", "\n", "x", "=", "UpSampling2D", "(", "size", "=", "upsample_rate", ",", "name", "=", "up_name", ")", "(", "input_tensor", ")", "\n", "\n", "if", "skip", "is", "not", "None", ":", "\n", "            ", "x", "=", "Concatenate", "(", "name", "=", "merge_name", ")", "(", "[", "x", ",", "skip", "]", ")", "\n", "\n", "", "x", "=", "ConvRelu", "(", "filters", ",", "kernel_size", ",", "use_batchnorm", "=", "use_batchnorm", ",", "\n", "conv_name", "=", "conv_name", "+", "'1'", ",", "bn_name", "=", "bn_name", "+", "'1'", ",", "relu_name", "=", "relu_name", "+", "'1'", ")", "(", "x", ")", "\n", "\n", "x", "=", "ConvRelu", "(", "filters", ",", "kernel_size", ",", "use_batchnorm", "=", "use_batchnorm", ",", "\n", "conv_name", "=", "conv_name", "+", "'2'", ",", "bn_name", "=", "bn_name", "+", "'2'", ",", "relu_name", "=", "relu_name", "+", "'2'", ")", "(", "x", ")", "\n", "\n", "return", "x", "\n", "", "return", "layer", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.unet.blocks.Transpose2D_block": [[49, 70], ["blocks.handle_block_names", "keras.layers.Conv2DTranspose", "keras.layers.Activation", "blocks.ConvRelu", "keras.layers.BatchNormalization", "keras.layers.Concatenate"], "function", ["home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.unet.blocks.handle_block_names", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.linknet.blocks.Conv2DTranspose", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.unet.blocks.ConvRelu"], ["\n", "", "def", "Transpose2D_block", "(", "filters", ",", "stage", ",", "cols", ",", "kernel_size", "=", "(", "3", ",", "3", ")", ",", "upsample_rate", "=", "(", "2", ",", "2", ")", ",", "\n", "transpose_kernel_size", "=", "(", "4", ",", "4", ")", ",", "use_batchnorm", "=", "False", ",", "skip", "=", "None", ")", ":", "\n", "\n", "    ", "def", "layer", "(", "input_tensor", ")", ":", "\n", "\n", "        ", "conv_name", ",", "bn_name", ",", "relu_name", ",", "up_name", ",", "merge_name", "=", "handle_block_names", "(", "stage", ",", "cols", ")", "\n", "\n", "x", "=", "Conv2DTranspose", "(", "filters", ",", "transpose_kernel_size", ",", "strides", "=", "upsample_rate", ",", "\n", "padding", "=", "'same'", ",", "name", "=", "up_name", ",", "use_bias", "=", "not", "(", "use_batchnorm", ")", ")", "(", "input_tensor", ")", "\n", "if", "use_batchnorm", ":", "\n", "            ", "x", "=", "BatchNormalization", "(", "name", "=", "bn_name", "+", "'1'", ")", "(", "x", ")", "\n", "", "x", "=", "Activation", "(", "'relu'", ",", "name", "=", "relu_name", "+", "'1'", ")", "(", "x", ")", "\n", "\n", "if", "skip", "is", "not", "None", ":", "\n", "            ", "x", "=", "Concatenate", "(", "name", "=", "merge_name", ")", "(", "[", "x", ",", "skip", "]", ")", "\n", "\n", "", "x", "=", "ConvRelu", "(", "filters", ",", "kernel_size", ",", "use_batchnorm", "=", "use_batchnorm", ",", "\n", "conv_name", "=", "conv_name", "+", "'2'", ",", "bn_name", "=", "bn_name", "+", "'2'", ",", "relu_name", "=", "relu_name", "+", "'2'", ")", "(", "x", ")", "\n", "\n", "return", "x", "\n", "", "return", "layer", ""]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.unet.builder.build_unet": [[10, 48], ["range", "keras.models.Model", "utils.to_tuple", "keras.layers.Conv2D", "keras.layers.Activation", "isinstance", "utils.get_layer_number", "len", "up_block"], "function", ["home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.segmentation_models.utils.to_tuple", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.segmentation_models.utils.get_layer_number"], ["\n", "\n", "def", "build_nestnet", "(", "backbone", ",", "classes", ",", "skip_connection_layers", ",", "\n", "decoder_filters", "=", "(", "256", ",", "128", ",", "64", ",", "32", ",", "16", ")", ",", "\n", "upsample_rates", "=", "(", "2", ",", "2", ",", "2", ",", "2", ",", "2", ")", ",", "\n", "n_upsample_blocks", "=", "5", ",", "\n", "block_type", "=", "'upsampling'", ",", "\n", "activation", "=", "'sigmoid'", ",", "\n", "use_batchnorm", "=", "True", ")", ":", "\n", "\n", "    ", "input", "=", "backbone", ".", "input", "\n", "# print(n_upsample_blocks)", "\n", "\n", "if", "block_type", "==", "'transpose'", ":", "\n", "        ", "up_block", "=", "Transpose2D_block", "\n", "", "else", ":", "\n", "        ", "up_block", "=", "Upsample2D_block", "\n", "\n", "", "if", "len", "(", "skip_connection_layers", ")", ">", "n_upsample_blocks", ":", "\n", "        ", "downsampling_layers", "=", "skip_connection_layers", "[", "int", "(", "len", "(", "skip_connection_layers", ")", "/", "2", ")", ":", "]", "\n", "skip_connection_layers", "=", "skip_connection_layers", "[", ":", "int", "(", "len", "(", "skip_connection_layers", ")", "/", "2", ")", "]", "\n", "", "else", ":", "\n", "        ", "downsampling_layers", "=", "skip_connection_layers", "\n", "\n", "\n", "# convert layer names to indices", "\n", "", "skip_connection_idx", "=", "(", "[", "get_layer_number", "(", "backbone", ",", "l", ")", "if", "isinstance", "(", "l", ",", "str", ")", "else", "l", "\n", "for", "l", "in", "skip_connection_layers", "]", ")", "\n", "skip_layers_list", "=", "[", "backbone", ".", "layers", "[", "skip_connection_idx", "[", "i", "]", "]", ".", "output", "for", "i", "in", "range", "(", "len", "(", "skip_connection_idx", ")", ")", "]", "\n", "downsampling_idx", "=", "(", "[", "get_layer_number", "(", "backbone", ",", "l", ")", "if", "isinstance", "(", "l", ",", "str", ")", "else", "l", "\n", "for", "l", "in", "downsampling_layers", "]", ")", "\n", "downsampling_list", "=", "[", "backbone", ".", "layers", "[", "downsampling_idx", "[", "i", "]", "]", ".", "output", "for", "i", "in", "range", "(", "len", "(", "downsampling_idx", ")", ")", "]", "\n", "downterm", "=", "[", "None", "]", "*", "(", "n_upsample_blocks", "+", "1", ")", "\n", "for", "i", "in", "range", "(", "len", "(", "downsampling_idx", ")", ")", ":", "\n", "# print(downsampling_list[0])", "\n", "# print(backbone.output)", "\n", "# print(\"\")", "\n", "        ", "if", "downsampling_list", "[", "0", "]", "==", "backbone", ".", "output", ":", "\n", "# print(\"VGG16 should be!\")", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.unet.model.Unet": [[24, 90], ["backbones.get_backbone", "builder.build_unet", "utils.freeze_model"], "function", ["home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.backbones.backbones.get_backbone", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.unet.builder.build_unet", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.segmentation_models.utils.freeze_model"], [")", ",", "\n", "'resnet152'", ":", "(", "'stage4_unit1_relu1'", ",", "'stage3_unit1_relu1'", ",", "'stage2_unit1_relu1'", ",", "'relu0'", ",", "\n", "'relu1'", ",", "'stage3_unit2_relu1'", ",", "'stage2_unit2_relu1'", ",", "'stage1_unit2_relu1'", ",", "\n", ")", ",", "\n", "'resnext50'", ":", "(", "'stage4_unit1_relu1'", ",", "'stage3_unit1_relu1'", ",", "'stage2_unit1_relu1'", ",", "'relu0'", ",", "\n", "'stage4_unit1_relu1'", ",", "'stage3_unit2_relu1'", ",", "'stage2_unit2_relu1'", ",", "'stage1_unit2_relu1'", ",", "\n", ")", ",", "\n", "'resnext101'", ":", "(", "'stage4_unit1_relu1'", ",", "'stage3_unit1_relu1'", ",", "'stage2_unit1_relu1'", ",", "'relu0'", ",", "\n", "'stage4_unit1_relu1'", ",", "'stage3_unit2_relu1'", ",", "'stage2_unit2_relu1'", ",", "'stage1_unit2_relu1'", ",", "\n", ")", ",", "\n", "'inceptionv3'", ":", "(", "228", ",", "86", ",", "16", ",", "9", ")", ",", "\n", "'inceptionresnetv2'", ":", "(", "594", ",", "260", ",", "16", ",", "9", ")", ",", "\n", "'densenet121'", ":", "(", "311", ",", "139", ",", "51", ",", "4", ")", ",", "\n", "'densenet169'", ":", "(", "367", ",", "139", ",", "51", ",", "4", ")", ",", "\n", "'densenet201'", ":", "(", "479", ",", "139", ",", "51", ",", "4", ")", ",", "\n", "}", "\n", "\n", "\n", "def", "Nestnet", "(", "backbone_name", "=", "'vgg16'", ",", "\n", "input_shape", "=", "(", "None", ",", "None", ",", "3", ")", ",", "\n", "input_tensor", "=", "None", ",", "\n", "encoder_weights", "=", "'imagenet'", ",", "\n", "freeze_encoder", "=", "False", ",", "\n", "skip_connections", "=", "'default'", ",", "\n", "decoder_block_type", "=", "'upsampling'", ",", "\n", "decoder_filters", "=", "(", "256", ",", "128", ",", "64", ",", "32", ",", "16", ")", ",", "\n", "decoder_use_batchnorm", "=", "True", ",", "\n", "n_upsample_blocks", "=", "5", ",", "\n", "upsample_rates", "=", "(", "2", ",", "2", ",", "2", ",", "2", ",", "2", ")", ",", "\n", "classes", "=", "1", ",", "\n", "activation", "=", "'sigmoid'", ")", ":", "\n", "    ", "\"\"\"\n\n    Args:\n        backbone_name: (str) look at list of available backbones.\n        input_shape:  (tuple) dimensions of input data (H, W, C)\n        input_tensor: keras tensor\n        encoder_weights: one of `None` (random initialization), \n            'imagenet' (pre-training on ImageNet), \n            'dof' (pre-training on DoF)\n        freeze_encoder: (bool) Set encoder layers weights as non-trainable. Useful for fine-tuning\n        skip_connections: if 'default' is used take default skip connections,\n            else provide a list of layer numbers or names starting from top of model\n        decoder_block_type: (str) one of 'upsampling' and 'transpose' (look at blocks.py)\n        decoder_filters: (int) number of convolution layer filters in decoder blocks\n        decoder_use_batchnorm: (bool) if True add batch normalisation layer between `Conv2D` ad `Activation` layers\n        n_upsample_blocks: (int) a number of upsampling blocks\n        upsample_rates: (tuple of int) upsampling rates decoder blocks\n        classes: (int) a number of classes for output\n        activation: (str) one of keras activations for last model layer\n\n    Returns:\n        keras.models.Model instance\n\n    \"\"\"", "\n", "\n", "\n", "\n", "backbone", "=", "get_backbone", "(", "backbone_name", ",", "\n", "input_shape", "=", "input_shape", ",", "\n", "input_tensor", "=", "input_tensor", ",", "\n", "weights", "=", "encoder_weights", ",", "\n", "include_top", "=", "False", ")", "\n", "\n", "if", "skip_connections", "==", "'default'", ":", "\n", "        ", "skip_connections", "=", "DEFAULT_SKIP_CONNECTIONS", "[", "backbone_name", "]", "\n", "# n_upsample_blocks = len(skip_connections)", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.fpn.blocks.pyramid_block": [[8, 51], ["common.Conv2DBlock", "common.Conv2DBlock", "common.Conv2DBlock", "common.ResizeImage", "keras.layers.Add", "utils.to_tuple"], "function", ["home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.common.blocks.Conv2DBlock", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.common.blocks.Conv2DBlock", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.common.blocks.Conv2DBlock", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.segmentation_models.utils.to_tuple"], ["\n", "def", "handle_block_names", "(", "stage", ",", "cols", ")", ":", "\n", "    ", "conv_name", "=", "'decoder_stage{}-{}_conv'", ".", "format", "(", "stage", ",", "cols", ")", "\n", "bn_name", "=", "'decoder_stage{}-{}_bn'", ".", "format", "(", "stage", ",", "cols", ")", "\n", "relu_name", "=", "'decoder_stage{}-{}_relu'", ".", "format", "(", "stage", ",", "cols", ")", "\n", "up_name", "=", "'decoder_stage{}-{}_upsample'", ".", "format", "(", "stage", ",", "cols", ")", "\n", "merge_name", "=", "'merge_{}-{}'", ".", "format", "(", "stage", ",", "cols", ")", "\n", "return", "conv_name", ",", "bn_name", ",", "relu_name", ",", "up_name", ",", "merge_name", "\n", "\n", "\n", "", "def", "ConvRelu", "(", "filters", ",", "kernel_size", ",", "use_batchnorm", "=", "False", ",", "conv_name", "=", "'conv'", ",", "bn_name", "=", "'bn'", ",", "relu_name", "=", "'relu'", ")", ":", "\n", "    ", "def", "layer", "(", "x", ")", ":", "\n", "        ", "x", "=", "Conv2D", "(", "filters", ",", "kernel_size", ",", "padding", "=", "\"same\"", ",", "name", "=", "conv_name", ",", "use_bias", "=", "not", "(", "use_batchnorm", ")", ")", "(", "x", ")", "\n", "if", "use_batchnorm", ":", "\n", "            ", "x", "=", "BatchNormalization", "(", "name", "=", "bn_name", ")", "(", "x", ")", "\n", "", "x", "=", "Activation", "(", "'relu'", ",", "name", "=", "relu_name", ")", "(", "x", ")", "\n", "return", "x", "\n", "", "return", "layer", "\n", "\n", "\n", "", "def", "Upsample2D_block", "(", "filters", ",", "stage", ",", "cols", ",", "kernel_size", "=", "(", "3", ",", "3", ")", ",", "upsample_rate", "=", "(", "2", ",", "2", ")", ",", "\n", "use_batchnorm", "=", "False", ",", "skip", "=", "None", ")", ":", "\n", "\n", "    ", "def", "layer", "(", "input_tensor", ")", ":", "\n", "\n", "        ", "conv_name", ",", "bn_name", ",", "relu_name", ",", "up_name", ",", "merge_name", "=", "handle_block_names", "(", "stage", ",", "cols", ")", "\n", "\n", "x", "=", "UpSampling2D", "(", "size", "=", "upsample_rate", ",", "name", "=", "up_name", ")", "(", "input_tensor", ")", "\n", "\n", "if", "skip", "is", "not", "None", ":", "\n", "            ", "x", "=", "Concatenate", "(", "name", "=", "merge_name", ")", "(", "[", "x", ",", "skip", "]", ")", "\n", "\n", "", "x", "=", "ConvRelu", "(", "filters", ",", "kernel_size", ",", "use_batchnorm", "=", "use_batchnorm", ",", "\n", "conv_name", "=", "conv_name", "+", "'1'", ",", "bn_name", "=", "bn_name", "+", "'1'", ",", "relu_name", "=", "relu_name", "+", "'1'", ")", "(", "x", ")", "\n", "\n", "x", "=", "ConvRelu", "(", "filters", ",", "kernel_size", ",", "use_batchnorm", "=", "use_batchnorm", ",", "\n", "conv_name", "=", "conv_name", "+", "'2'", ",", "bn_name", "=", "bn_name", "+", "'2'", ",", "relu_name", "=", "relu_name", "+", "'2'", ")", "(", "x", ")", "\n", "\n", "return", "x", "\n", "", "return", "layer", "\n", "\n", "\n", "", "def", "Transpose2D_block", "(", "filters", ",", "stage", ",", "cols", ",", "kernel_size", "=", "(", "3", ",", "3", ")", ",", "upsample_rate", "=", "(", "2", ",", "2", ")", ",", "\n", "transpose_kernel_size", "=", "(", "4", ",", "4", ")", ",", "use_batchnorm", "=", "False", ",", "skip", "=", "None", ")", ":", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.fpn.builder.build_fpn": [[14, 96], ["utils.extract_outputs", "enumerate", "enumerate", "keras.models.Model", "len", "len", "ValueError", "list", "pyramid.append", "upsampled_pyramid.append", "keras.layers.Concatenate", "len", "common.Conv2DBlock", "keras.layers.Conv2D", "common.ResizeImage", "keras.layers.Activation", "blocks.pyramid_block", "utils.to_tuple", "keras.layers.SpatialDropout2D", "utils.to_tuple", "numpy.prod", "common.ResizeImage"], "function", ["home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.segmentation_models.utils.extract_outputs", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.common.blocks.Conv2DBlock", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.fpn.blocks.pyramid_block", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.segmentation_models.utils.to_tuple", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.segmentation_models.utils.to_tuple"], ["upsample_rates", "=", "(", "2", ",", "2", ",", "2", ",", "2", ",", "2", ")", ",", "\n", "n_upsample_blocks", "=", "5", ",", "\n", "block_type", "=", "'upsampling'", ",", "\n", "activation", "=", "'sigmoid'", ",", "\n", "use_batchnorm", "=", "True", ")", ":", "\n", "\n", "    ", "input", "=", "backbone", ".", "input", "\n", "# print(n_upsample_blocks)", "\n", "\n", "if", "block_type", "==", "'transpose'", ":", "\n", "        ", "up_block", "=", "Transpose2D_block", "\n", "", "else", ":", "\n", "        ", "up_block", "=", "Upsample2D_block", "\n", "\n", "", "if", "len", "(", "skip_connection_layers", ")", ">", "n_upsample_blocks", ":", "\n", "        ", "downsampling_layers", "=", "skip_connection_layers", "[", "int", "(", "len", "(", "skip_connection_layers", ")", "/", "2", ")", ":", "]", "\n", "skip_connection_layers", "=", "skip_connection_layers", "[", ":", "int", "(", "len", "(", "skip_connection_layers", ")", "/", "2", ")", "]", "\n", "", "else", ":", "\n", "        ", "downsampling_layers", "=", "skip_connection_layers", "\n", "\n", "\n", "# convert layer names to indices", "\n", "", "skip_connection_idx", "=", "(", "[", "get_layer_number", "(", "backbone", ",", "l", ")", "if", "isinstance", "(", "l", ",", "str", ")", "else", "l", "\n", "for", "l", "in", "skip_connection_layers", "]", ")", "\n", "skip_layers_list", "=", "[", "backbone", ".", "layers", "[", "skip_connection_idx", "[", "i", "]", "]", ".", "output", "for", "i", "in", "range", "(", "len", "(", "skip_connection_idx", ")", ")", "]", "\n", "downsampling_idx", "=", "(", "[", "get_layer_number", "(", "backbone", ",", "l", ")", "if", "isinstance", "(", "l", ",", "str", ")", "else", "l", "\n", "for", "l", "in", "downsampling_layers", "]", ")", "\n", "downsampling_list", "=", "[", "backbone", ".", "layers", "[", "downsampling_idx", "[", "i", "]", "]", ".", "output", "for", "i", "in", "range", "(", "len", "(", "downsampling_idx", ")", ")", "]", "\n", "downterm", "=", "[", "None", "]", "*", "(", "n_upsample_blocks", "+", "1", ")", "\n", "for", "i", "in", "range", "(", "len", "(", "downsampling_idx", ")", ")", ":", "\n", "# print(downsampling_list[0])", "\n", "# print(backbone.output)", "\n", "# print(\"\")", "\n", "        ", "if", "downsampling_list", "[", "0", "]", "==", "backbone", ".", "output", ":", "\n", "# print(\"VGG16 should be!\")", "\n", "            ", "downterm", "[", "n_upsample_blocks", "-", "i", "]", "=", "downsampling_list", "[", "i", "]", "\n", "", "else", ":", "\n", "            ", "downterm", "[", "n_upsample_blocks", "-", "i", "-", "1", "]", "=", "downsampling_list", "[", "i", "]", "\n", "", "", "downterm", "[", "-", "1", "]", "=", "backbone", ".", "output", "\n", "# print(\"downterm = {}\".format(downterm))", "\n", "\n", "interm", "=", "[", "None", "]", "*", "(", "n_upsample_blocks", "+", "1", ")", "*", "(", "n_upsample_blocks", "+", "1", ")", "\n", "for", "i", "in", "range", "(", "len", "(", "skip_connection_idx", ")", ")", ":", "\n", "        ", "interm", "[", "-", "i", "*", "(", "n_upsample_blocks", "+", "1", ")", "+", "(", "n_upsample_blocks", "+", "1", ")", "*", "(", "n_upsample_blocks", "-", "1", ")", "]", "=", "skip_layers_list", "[", "i", "]", "\n", "", "interm", "[", "(", "n_upsample_blocks", "+", "1", ")", "*", "n_upsample_blocks", "]", "=", "backbone", ".", "output", "\n", "\n", "for", "j", "in", "range", "(", "n_upsample_blocks", ")", ":", "\n", "        ", "for", "i", "in", "range", "(", "n_upsample_blocks", "-", "j", ")", ":", "\n", "            ", "upsample_rate", "=", "to_tuple", "(", "upsample_rates", "[", "i", "]", ")", "\n", "# print(j, i)", "\n", "\n", "if", "i", "==", "0", "and", "j", "<", "n_upsample_blocks", "-", "1", "and", "len", "(", "skip_connection_layers", ")", "<", "n_upsample_blocks", ":", "\n", "                ", "interm", "[", "(", "n_upsample_blocks", "+", "1", ")", "*", "i", "+", "j", "+", "1", "]", "=", "None", "\n", "", "elif", "j", "==", "0", ":", "\n", "                ", "if", "downterm", "[", "i", "+", "1", "]", "is", "not", "None", ":", "\n", "                    ", "interm", "[", "(", "n_upsample_blocks", "+", "1", ")", "*", "i", "+", "j", "+", "1", "]", "=", "up_block", "(", "decoder_filters", "[", "n_upsample_blocks", "-", "i", "-", "2", "]", ",", "\n", "i", "+", "1", ",", "j", "+", "1", ",", "upsample_rate", "=", "upsample_rate", ",", "\n", "skip", "=", "interm", "[", "(", "n_upsample_blocks", "+", "1", ")", "*", "i", "+", "j", "]", ",", "\n", "use_batchnorm", "=", "use_batchnorm", ")", "(", "downterm", "[", "i", "+", "1", "]", ")", "\n", "", "else", ":", "\n", "                    ", "interm", "[", "(", "n_upsample_blocks", "+", "1", ")", "*", "i", "+", "j", "+", "1", "]", "=", "None", "\n", "# print(\"\\n{} = {} + {}\\n\".format(interm[(n_upsample_blocks+1)*i+j+1],", "\n", "#                             interm[(n_upsample_blocks+1)*i+j], ", "\n", "#                             downterm[i+1]))", "\n", "", "", "else", ":", "\n", "                ", "interm", "[", "(", "n_upsample_blocks", "+", "1", ")", "*", "i", "+", "j", "+", "1", "]", "=", "up_block", "(", "decoder_filters", "[", "n_upsample_blocks", "-", "i", "-", "2", "]", ",", "\n", "i", "+", "1", ",", "j", "+", "1", ",", "upsample_rate", "=", "upsample_rate", ",", "\n", "skip", "=", "interm", "[", "(", "n_upsample_blocks", "+", "1", ")", "*", "i", "+", "j", "]", ",", "\n", "use_batchnorm", "=", "use_batchnorm", ")", "(", "interm", "[", "(", "n_upsample_blocks", "+", "1", ")", "*", "(", "i", "+", "1", ")", "+", "j", "]", ")", "\n", "# print(\"\\n{} = {} + {}\\n\".format(interm[(n_upsample_blocks+1)*i+j+1],", "\n", "#                             interm[(n_upsample_blocks+1)*i+j], ", "\n", "#                             interm[(n_upsample_blocks+1)*(i+1)+j]))", "\n", "# print('\\n\\n\\n')", "\n", "# for x in range(n_upsample_blocks+1):", "\n", "#     for y in range(n_upsample_blocks+1):", "\n", "#         print(interm[x*(n_upsample_blocks+1)+y], end=' ', flush=True)", "\n", "#     print('\\n')", "\n", "# print('\\n\\n\\n')", "\n", "#print(interm)", "\n", "\n", "", "", ""]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.fpn.model.FPN": [[24, 92], ["backbones.get_backbone", "builder.build_fpn", "utils.freeze_model"], "function", ["home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.backbones.backbones.get_backbone", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.fpn.builder.build_fpn", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.segmentation_models.utils.freeze_model"], [")", ",", "\n", "'resnet152'", ":", "(", "'stage4_unit1_relu1'", ",", "'stage3_unit1_relu1'", ",", "'stage2_unit1_relu1'", ",", "'relu0'", ",", "\n", "'relu1'", ",", "'stage3_unit2_relu1'", ",", "'stage2_unit2_relu1'", ",", "'stage1_unit2_relu1'", ",", "\n", ")", ",", "\n", "'resnext50'", ":", "(", "'stage4_unit1_relu1'", ",", "'stage3_unit1_relu1'", ",", "'stage2_unit1_relu1'", ",", "'relu0'", ",", "\n", "'stage4_unit1_relu1'", ",", "'stage3_unit2_relu1'", ",", "'stage2_unit2_relu1'", ",", "'stage1_unit2_relu1'", ",", "\n", ")", ",", "\n", "'resnext101'", ":", "(", "'stage4_unit1_relu1'", ",", "'stage3_unit1_relu1'", ",", "'stage2_unit1_relu1'", ",", "'relu0'", ",", "\n", "'stage4_unit1_relu1'", ",", "'stage3_unit2_relu1'", ",", "'stage2_unit2_relu1'", ",", "'stage1_unit2_relu1'", ",", "\n", ")", ",", "\n", "'inceptionv3'", ":", "(", "228", ",", "86", ",", "16", ",", "9", ")", ",", "\n", "'inceptionresnetv2'", ":", "(", "594", ",", "260", ",", "16", ",", "9", ")", ",", "\n", "'densenet121'", ":", "(", "311", ",", "139", ",", "51", ",", "4", ")", ",", "\n", "'densenet169'", ":", "(", "367", ",", "139", ",", "51", ",", "4", ")", ",", "\n", "'densenet201'", ":", "(", "479", ",", "139", ",", "51", ",", "4", ")", ",", "\n", "}", "\n", "\n", "\n", "def", "Nestnet", "(", "backbone_name", "=", "'vgg16'", ",", "\n", "input_shape", "=", "(", "None", ",", "None", ",", "3", ")", ",", "\n", "input_tensor", "=", "None", ",", "\n", "encoder_weights", "=", "'imagenet'", ",", "\n", "freeze_encoder", "=", "False", ",", "\n", "skip_connections", "=", "'default'", ",", "\n", "decoder_block_type", "=", "'upsampling'", ",", "\n", "decoder_filters", "=", "(", "256", ",", "128", ",", "64", ",", "32", ",", "16", ")", ",", "\n", "decoder_use_batchnorm", "=", "True", ",", "\n", "n_upsample_blocks", "=", "5", ",", "\n", "upsample_rates", "=", "(", "2", ",", "2", ",", "2", ",", "2", ",", "2", ")", ",", "\n", "classes", "=", "1", ",", "\n", "activation", "=", "'sigmoid'", ")", ":", "\n", "    ", "\"\"\"\n\n    Args:\n        backbone_name: (str) look at list of available backbones.\n        input_shape:  (tuple) dimensions of input data (H, W, C)\n        input_tensor: keras tensor\n        encoder_weights: one of `None` (random initialization), \n            'imagenet' (pre-training on ImageNet), \n            'dof' (pre-training on DoF)\n        freeze_encoder: (bool) Set encoder layers weights as non-trainable. Useful for fine-tuning\n        skip_connections: if 'default' is used take default skip connections,\n            else provide a list of layer numbers or names starting from top of model\n        decoder_block_type: (str) one of 'upsampling' and 'transpose' (look at blocks.py)\n        decoder_filters: (int) number of convolution layer filters in decoder blocks\n        decoder_use_batchnorm: (bool) if True add batch normalisation layer between `Conv2D` ad `Activation` layers\n        n_upsample_blocks: (int) a number of upsampling blocks\n        upsample_rates: (tuple of int) upsampling rates decoder blocks\n        classes: (int) a number of classes for output\n        activation: (str) one of keras activations for last model layer\n\n    Returns:\n        keras.models.Model instance\n\n    \"\"\"", "\n", "\n", "\n", "\n", "backbone", "=", "get_backbone", "(", "backbone_name", ",", "\n", "input_shape", "=", "input_shape", ",", "\n", "input_tensor", "=", "input_tensor", ",", "\n", "weights", "=", "encoder_weights", ",", "\n", "include_top", "=", "False", ")", "\n", "\n", "if", "skip_connections", "==", "'default'", ":", "\n", "        ", "skip_connections", "=", "DEFAULT_SKIP_CONNECTIONS", "[", "backbone_name", "]", "\n", "# n_upsample_blocks = len(skip_connections)", "\n", "\n", "", "model", "=", "build_nestnet", "(", "backbone", ",", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.common.layers.ResizeImage.__init__": [[44, 54], ["keras.engine.Layer.__init__", "keras.utils.conv_utils.normalize_tuple", "keras.engine.InputSpec", "ValueError"], "methods", ["home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.common.layers.ResizeImage.__init__"], ["@", "interfaces", ".", "legacy_upsampling2d_support", "\n", "def", "__init__", "(", "self", ",", "factor", "=", "(", "2", ",", "2", ")", ",", "data_format", "=", "'channels_last'", ",", "interpolation", "=", "'nearest'", ",", "**", "kwargs", ")", ":", "\n", "        ", "super", "(", "ResizeImage", ",", "self", ")", ".", "__init__", "(", "**", "kwargs", ")", "\n", "self", ".", "data_format", "=", "data_format", "\n", "self", ".", "factor", "=", "conv_utils", ".", "normalize_tuple", "(", "factor", ",", "2", ",", "'factor'", ")", "\n", "self", ".", "input_spec", "=", "InputSpec", "(", "ndim", "=", "4", ")", "\n", "if", "interpolation", "not", "in", "[", "'nearest'", ",", "'bilinear'", "]", ":", "\n", "            ", "raise", "ValueError", "(", "'interpolation should be one '", "\n", "'of \"nearest\" or \"bilinear\".'", ")", "\n", "", "self", ".", "interpolation", "=", "interpolation", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.common.layers.ResizeImage.compute_output_shape": [[55, 70], ["None"], "methods", ["None"], ["", "def", "compute_output_shape", "(", "self", ",", "input_shape", ")", ":", "\n", "        ", "if", "self", ".", "data_format", "==", "'channels_first'", ":", "\n", "            ", "height", "=", "self", ".", "factor", "[", "0", "]", "*", "input_shape", "[", "2", "]", "if", "input_shape", "[", "2", "]", "is", "not", "None", "else", "None", "\n", "width", "=", "self", ".", "factor", "[", "1", "]", "*", "input_shape", "[", "3", "]", "if", "input_shape", "[", "3", "]", "is", "not", "None", "else", "None", "\n", "return", "(", "input_shape", "[", "0", "]", ",", "\n", "input_shape", "[", "1", "]", ",", "\n", "height", ",", "\n", "width", ")", "\n", "", "elif", "self", ".", "data_format", "==", "'channels_last'", ":", "\n", "            ", "height", "=", "self", ".", "factor", "[", "0", "]", "*", "input_shape", "[", "1", "]", "if", "input_shape", "[", "1", "]", "is", "not", "None", "else", "None", "\n", "width", "=", "self", ".", "factor", "[", "1", "]", "*", "input_shape", "[", "2", "]", "if", "input_shape", "[", "2", "]", "is", "not", "None", "else", "None", "\n", "return", "(", "input_shape", "[", "0", "]", ",", "\n", "height", ",", "\n", "width", ",", "\n", "input_shape", "[", "3", "]", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.common.layers.ResizeImage.call": [[71, 74], ["functions.resize_images"], "methods", ["home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.common.functions.resize_images"], ["", "", "def", "call", "(", "self", ",", "inputs", ")", ":", "\n", "        ", "return", "resize_images", "(", "inputs", ",", "self", ".", "factor", "[", "0", "]", ",", "self", ".", "factor", "[", "1", "]", ",", "\n", "self", ".", "data_format", ",", "self", ".", "interpolation", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.common.layers.ResizeImage.get_config": [[75, 80], ["super().get_config", "dict", "list", "list", "super().get_config.items", "config.items"], "methods", ["home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.common.layers.ResizeImage.get_config"], ["", "def", "get_config", "(", "self", ")", ":", "\n", "        ", "config", "=", "{", "'factor'", ":", "self", ".", "factor", ",", "\n", "'data_format'", ":", "self", ".", "data_format", "}", "\n", "base_config", "=", "super", "(", "ResizeImage", ",", "self", ")", ".", "get_config", "(", ")", "\n", "return", "dict", "(", "list", "(", "base_config", ".", "items", "(", ")", ")", "+", "list", "(", "config", ".", "items", "(", ")", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.common.blocks.Conv2DBlock": [[6, 22], ["keras.layers.Conv2D", "keras.layers.Activation", "keras.layers.BatchNormalization"], "function", ["None"], ["from", "keras", ".", "layers", "import", "Concatenate", "\n", "\n", "\n", "def", "handle_block_names", "(", "stage", ",", "cols", ")", ":", "\n", "    ", "conv_name", "=", "'decoder_stage{}-{}_conv'", ".", "format", "(", "stage", ",", "cols", ")", "\n", "bn_name", "=", "'decoder_stage{}-{}_bn'", ".", "format", "(", "stage", ",", "cols", ")", "\n", "relu_name", "=", "'decoder_stage{}-{}_relu'", ".", "format", "(", "stage", ",", "cols", ")", "\n", "up_name", "=", "'decoder_stage{}-{}_upsample'", ".", "format", "(", "stage", ",", "cols", ")", "\n", "merge_name", "=", "'merge_{}-{}'", ".", "format", "(", "stage", ",", "cols", ")", "\n", "return", "conv_name", ",", "bn_name", ",", "relu_name", ",", "up_name", ",", "merge_name", "\n", "\n", "\n", "", "def", "ConvRelu", "(", "filters", ",", "kernel_size", ",", "use_batchnorm", "=", "False", ",", "conv_name", "=", "'conv'", ",", "bn_name", "=", "'bn'", ",", "relu_name", "=", "'relu'", ")", ":", "\n", "    ", "def", "layer", "(", "x", ")", ":", "\n", "        ", "x", "=", "Conv2D", "(", "filters", ",", "kernel_size", ",", "padding", "=", "\"same\"", ",", "name", "=", "conv_name", ",", "use_bias", "=", "not", "(", "use_batchnorm", ")", ")", "(", "x", ")", "\n", "if", "use_batchnorm", ":", "\n", "            ", "x", "=", "BatchNormalization", "(", "name", "=", "bn_name", ")", "(", "x", ")", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.common.functions.transpose_shape": [[5, 38], ["tuple", "isinstance", "list", "ValueError", "str"], "function", ["None"], ["def", "transpose_shape", "(", "shape", ",", "target_format", ",", "spatial_axes", ")", ":", "\n", "    ", "\"\"\"Converts a tuple or a list to the correct `data_format`.\n    It does so by switching the positions of its elements.\n    # Arguments\n        shape: Tuple or list, often representing shape,\n            corresponding to `'channels_last'`.\n        target_format: A string, either `'channels_first'` or `'channels_last'`.\n        spatial_axes: A tuple of integers.\n            Correspond to the indexes of the spatial axes.\n            For example, if you pass a shape\n            representing (batch_size, timesteps, rows, cols, channels),\n            then `spatial_axes=(2, 3)`.\n    # Returns\n        A tuple or list, with the elements permuted according\n        to `target_format`.\n    # Example\n    # Raises\n        ValueError: if `value` or the global `data_format` invalid.\n    \"\"\"", "\n", "if", "target_format", "==", "'channels_first'", ":", "\n", "        ", "new_values", "=", "shape", "[", ":", "spatial_axes", "[", "0", "]", "]", "\n", "new_values", "+=", "(", "shape", "[", "-", "1", "]", ",", ")", "\n", "new_values", "+=", "tuple", "(", "shape", "[", "x", "]", "for", "x", "in", "spatial_axes", ")", "\n", "\n", "if", "isinstance", "(", "shape", ",", "list", ")", ":", "\n", "            ", "return", "list", "(", "new_values", ")", "\n", "", "return", "new_values", "\n", "", "elif", "target_format", "==", "'channels_last'", ":", "\n", "        ", "return", "shape", "\n", "", "else", ":", "\n", "        ", "raise", "ValueError", "(", "'The `data_format` argument must be one of '", "\n", "'\"channels_first\", \"channels_last\". Received: '", "+", "\n", "str", "(", "target_format", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.common.functions.permute_dimensions": [[40, 50], ["tensorflow.transpose"], "function", ["None"], ["", "", "def", "permute_dimensions", "(", "x", ",", "pattern", ")", ":", "\n", "    ", "\"\"\"Permutes axes in a tensor.\n    # Arguments\n        x: Tensor or variable.\n        pattern: A tuple of\n            dimension indices, e.g. `(0, 2, 1)`.\n    # Returns\n        A tensor.\n    \"\"\"", "\n", "return", "tf", ".", "transpose", "(", "x", ",", "perm", "=", "pattern", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.common.functions.int_shape": [[52, 65], ["hasattr", "tuple", "x.get_shape().as_list", "x.get_shape"], "function", ["None"], ["", "def", "int_shape", "(", "x", ")", ":", "\n", "    ", "\"\"\"Returns the shape of tensor or variable as a tuple of int or None entries.\n    # Arguments\n        x: Tensor or variable.\n    # Returns\n        A tuple of integers (or None entries).\n    \"\"\"", "\n", "if", "hasattr", "(", "x", ",", "'_keras_shape'", ")", ":", "\n", "        ", "return", "x", ".", "_keras_shape", "\n", "", "try", ":", "\n", "        ", "return", "tuple", "(", "x", ".", "get_shape", "(", ")", ".", "as_list", "(", ")", ")", "\n", "", "except", "ValueError", ":", "\n", "        ", "return", "None", "\n", "\n"]], "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.common.functions.resize_images": [[67, 118], ["functions.int_shape", "tensorflow.constant", "tf.image.resize_bilinear.set_shape", "tensorflow.shape", "numpy.array", "functions.permute_dimensions", "tensorflow.image.resize_nearest_neighbor", "functions.permute_dimensions", "functions.transpose_shape", "tensorflow.image.resize_bilinear", "ValueError"], "function", ["home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.common.functions.int_shape", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.common.functions.permute_dimensions", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.common.functions.permute_dimensions", "home.repos.pwc.inspect_result.MrGiovanni_UNetPlusPlus.common.functions.transpose_shape"], ["", "", "def", "resize_images", "(", "x", ",", "\n", "height_factor", ",", "\n", "width_factor", ",", "\n", "data_format", ",", "\n", "interpolation", "=", "'nearest'", ")", ":", "\n", "    ", "\"\"\"Resizes the images contained in a 4D tensor.\n    # Arguments\n        x: Tensor or variable to resize.\n        height_factor: Positive integer.\n        width_factor: Positive integer.\n        data_format: string, `\"channels_last\"` or `\"channels_first\"`.\n        interpolation: A string, one of `nearest` or `bilinear`.\n    # Returns\n        A tensor.\n    # Raises\n        ValueError: if `data_format` is neither `\"channels_last\"` or `\"channels_first\"`.\n    \"\"\"", "\n", "if", "data_format", "==", "'channels_first'", ":", "\n", "        ", "rows", ",", "cols", "=", "2", ",", "3", "\n", "", "else", ":", "\n", "        ", "rows", ",", "cols", "=", "1", ",", "2", "\n", "\n", "", "original_shape", "=", "int_shape", "(", "x", ")", "\n", "new_shape", "=", "tf", ".", "shape", "(", "x", ")", "[", "rows", ":", "cols", "+", "1", "]", "\n", "new_shape", "*=", "tf", ".", "constant", "(", "np", ".", "array", "(", "[", "height_factor", ",", "width_factor", "]", ",", "dtype", "=", "'int32'", ")", ")", "\n", "\n", "if", "data_format", "==", "'channels_first'", ":", "\n", "        ", "x", "=", "permute_dimensions", "(", "x", ",", "[", "0", ",", "2", ",", "3", ",", "1", "]", ")", "\n", "", "if", "interpolation", "==", "'nearest'", ":", "\n", "        ", "x", "=", "tf", ".", "image", ".", "resize_nearest_neighbor", "(", "x", ",", "new_shape", ")", "\n", "", "elif", "interpolation", "==", "'bilinear'", ":", "\n", "        ", "x", "=", "tf", ".", "image", ".", "resize_bilinear", "(", "x", ",", "new_shape", ")", "\n", "", "else", ":", "\n", "        ", "raise", "ValueError", "(", "'interpolation should be one '", "\n", "'of \"nearest\" or \"bilinear\".'", ")", "\n", "", "if", "data_format", "==", "'channels_first'", ":", "\n", "        ", "x", "=", "permute_dimensions", "(", "x", ",", "[", "0", ",", "3", ",", "1", ",", "2", "]", ")", "\n", "\n", "", "if", "original_shape", "[", "rows", "]", "is", "None", ":", "\n", "        ", "new_height", "=", "None", "\n", "", "else", ":", "\n", "        ", "new_height", "=", "original_shape", "[", "rows", "]", "*", "height_factor", "\n", "\n", "", "if", "original_shape", "[", "cols", "]", "is", "None", ":", "\n", "        ", "new_width", "=", "None", "\n", "", "else", ":", "\n", "        ", "new_width", "=", "original_shape", "[", "cols", "]", "*", "width_factor", "\n", "\n", "", "output_shape", "=", "(", "None", ",", "new_height", ",", "new_width", ",", "None", ")", "\n", "x", ".", "set_shape", "(", "transpose_shape", "(", "output_shape", ",", "data_format", ",", "spatial_axes", "=", "(", "1", ",", "2", ")", ")", ")", "\n", "return", "x", "", "", ""]]}