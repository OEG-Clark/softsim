{"home.repos.pwc.inspect_result.neulab_awesome-align.tools.aer.parse_args": [[11, 31], ["argparse.ArgumentParser", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.parse_args"], "function", ["home.repos.pwc.inspect_result.neulab_awesome-align.tools.aer.parse_args"], ["def", "parse_args", "(", ")", ":", "\n", "    ", "parser", "=", "argparse", ".", "ArgumentParser", "(", "\"Calculates Alignment Error Rate, output format: AER (Precision, Recall, Alginment-Links-Hypothesis)\"", ")", "\n", "parser", ".", "add_argument", "(", "\"reference\"", ",", "help", "=", "\"path of reference alignment, e.g. '10-9 11p42'\"", ")", "\n", "parser", ".", "add_argument", "(", "\"hypothesis\"", ",", "help", "=", "\"path to hypothesis alignment\"", ")", "\n", "\n", "parser", ".", "add_argument", "(", "\"--reverseRef\"", ",", "help", "=", "\"reverse reference alignment\"", ",", "action", "=", "'store_true'", ")", "\n", "parser", ".", "add_argument", "(", "\"--reverseHyp\"", ",", "help", "=", "\"reverse hypothesis alignment\"", ",", "action", "=", "'store_true'", ")", "\n", "\n", "parser", ".", "add_argument", "(", "\"--oneRef\"", ",", "help", "=", "\"reference indices start at index 1\"", ",", "action", "=", "'store_true'", ")", "\n", "parser", ".", "add_argument", "(", "\"--oneHyp\"", ",", "help", "=", "\"hypothesis indices start at index 1\"", ",", "action", "=", "'store_true'", ")", "\n", "parser", ".", "add_argument", "(", "\"--allSure\"", ",", "help", "=", "\"treat all alignments in the reference as sure alignments\"", ",", "action", "=", "'store_true'", ")", "\n", "parser", ".", "add_argument", "(", "\"--ignorePossible\"", ",", "help", "=", "\"Ignore all possible links\"", ",", "action", "=", "'store_true'", ")", "\n", "parser", ".", "add_argument", "(", "\"--fAlpha\"", ",", "help", "=", "\"alpha parameter used to calculate f measure (has to be set to a value >= 0.0 to report the f-measure)\"", ",", "default", "=", "0.5", ",", "type", "=", "float", ")", "\n", "\n", "parser", ".", "add_argument", "(", "\"--source\"", ",", "default", "=", "\"\"", ",", "help", "=", "\"the source sentence, used for an error analysis\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--target\"", ",", "default", "=", "\"\"", ",", "help", "=", "\"the target sentence, used for an error analysis\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--cleanPunctuation\"", ",", "action", "=", "\"store_true\"", ",", "help", "=", "\"Removes alignments including punctuation marks, that are not aligned to the same punctuation mark (e.g. ','-'that')\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--most_common_errors\"", ",", "default", "=", "10", ",", "type", "=", "int", ")", "\n", "\n", "return", "parser", ".", "parse_args", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.neulab_awesome-align.tools.aer.calculate_internal_jumps": [[33, 51], ["len", "len", "aer.calculate_internal_jumps.contiguous"], "function", ["None"], ["", "def", "calculate_internal_jumps", "(", "alignments", ")", ":", "\n", "    ", "\"\"\" Count number of times the set of source word indices aligned to a target word index are not adjacent\n        Each non adjacent set of source word indices counts only once\n    >>> calculate_internal_jumps([{1,2,4}, {42}])\n    1\n    >>> calculate_internal_jumps([{1,2,3,4}])\n    0\n    >>> calculate_internal_jumps([set()])\n    0\n    \"\"\"", "\n", "def", "contiguous", "(", "s", ")", ":", "\n", "        ", "if", "len", "(", "s", ")", "<=", "1", ":", "\n", "            ", "return", "True", "\n", "", "else", ":", "\n", "            ", "elements_in_contiguous_set", "=", "max", "(", "s", ")", "-", "min", "(", "s", ")", "+", "1", "\n", "return", "elements_in_contiguous_set", "==", "len", "(", "s", ")", "\n", "\n", "", "", "return", "[", "contiguous", "(", "s", ")", "for", "s", "in", "alignments", "]", ".", "count", "(", "False", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.neulab_awesome-align.tools.aer.calculate_external_jumps": [[53, 70], ["zip", "len", "len", "sorted"], "function", ["None"], ["", "def", "calculate_external_jumps", "(", "alignments", ")", ":", "\n", "    ", "\"\"\" Count number of times the (smallest) source index aligned to target word x is not adjacent or identical to any source word index aligned to the next target word index x+1\n        Target words which do not have any source word aligned to it are ignored\n    >>> calculate_external_jumps([set(), {1,2,4}, {2}, {4}, set()])\n    1\n    \"\"\"", "\n", "\n", "jumps", "=", "0", "\n", "\n", "for", "prev", ",", "current", "in", "zip", "(", "alignments", ",", "alignments", "[", "1", ":", "]", ")", ":", "\n", "        ", "if", "len", "(", "prev", ")", ">", "0", "and", "len", "(", "current", ")", ">", "0", ":", "\n", "            ", "src", "=", "sorted", "(", "prev", ")", "[", "0", "]", "\n", "if", "src", "in", "current", "or", "src", "+", "1", "in", "current", "or", "src", "-", "1", "in", "current", ":", "\n", "                ", "pass", "\n", "", "else", ":", "\n", "                ", "jumps", "+=", "1", "\n", "", "", "", "return", "jumps", "\n", "\n"]], "home.repos.pwc.inspect_result.neulab_awesome-align.tools.aer.to_list": [[72, 82], ["max", "set", "lst[].add", "len", "range"], "function", ["home.repos.pwc.inspect_result.neulab_awesome-align.awesome_align.modeling_utils.BeamHypotheses.add"], ["", "def", "to_list", "(", "A", ")", ":", "\n", "    ", "\"\"\" converts set of src-tgt alignments to a list containing a set of aligned source word for each target position\n    >>> to_list({(2,1)})\n    [set(), {2}]\n    \"\"\"", "\n", "max_tgt_idx", "=", "max", "(", "{", "y", "for", "x", ",", "y", "in", "A", "}", ")", "if", "len", "(", "A", ")", ">", "0", "else", "0", "\n", "lst", "=", "[", "set", "(", ")", "for", "_", "in", "range", "(", "max_tgt_idx", "+", "1", ")", "]", "\n", "for", "x", ",", "y", "in", "A", ":", "\n", "        ", "lst", "[", "y", "]", ".", "add", "(", "x", ")", "\n", "", "return", "lst", "\n", "\n"]], "home.repos.pwc.inspect_result.neulab_awesome-align.tools.aer.calculate_metrics": [[84, 142], ["len", "collections.Counter", "map", "itertools.zip_longest", "len", "len", "len", "len", "len", "len", "len", "len", "aer.to_list", "aer.calculate_internal_jumps", "aer.calculate_external_jumps", "max", "A.intersection", "A.intersection", "sum", "print", "print", "len", "len", "len", "len", "len"], "function", ["home.repos.pwc.inspect_result.neulab_awesome-align.tools.aer.to_list", "home.repos.pwc.inspect_result.neulab_awesome-align.tools.aer.calculate_internal_jumps", "home.repos.pwc.inspect_result.neulab_awesome-align.tools.aer.calculate_external_jumps"], ["", "def", "calculate_metrics", "(", "array_sure", ",", "array_possible", ",", "array_hypothesis", ",", "f_alpha", ",", "source_sentences", "=", "(", ")", ",", "target_sentences", "=", "(", ")", ",", "clean_punctuation", "=", "False", ")", ":", "\n", "    ", "\"\"\" Calculates precision, recall and alignment error rate as described in \"A Systematic Comparison of Various\n        Statistical Alignment Models\" (https://www.aclweb.org/anthology/J/J03/J03-1002.pdf) in chapter 5\n\n\n    Args:\n        array_sure: array of sure alignment links\n        array_possible: array of possible alignment links\n        array_hypothesis: array of hypothesis alignment links\n    \"\"\"", "\n", "\n", "number_of_sentences", "=", "len", "(", "array_sure", ")", "\n", "assert", "number_of_sentences", "==", "len", "(", "array_possible", ")", "\n", "assert", "number_of_sentences", "==", "len", "(", "array_hypothesis", ")", "\n", "\n", "errors", "=", "Counter", "(", ")", "\n", "\n", "sum_a_intersect_p", ",", "sum_a_intersect_s", ",", "sum_s", ",", "sum_a", ",", "aligned_source_words", ",", "aligned_target_words", "=", "6", "*", "[", "0.0", "]", "\n", "sum_source_words", ",", "sum_target_words", "=", "map", "(", "lambda", "s", ":", "max", "(", "1.0", ",", "sum", "(", "len", "(", "x", ")", "for", "x", "in", "s", ")", ")", ",", "[", "source_sentences", ",", "target_sentences", "]", ")", "\n", "internal_jumps", ",", "external_jumps", "=", "0", ",", "0", "\n", "\n", "for", "S", ",", "P", ",", "A", ",", "source", ",", "target", "in", "itertools", ".", "zip_longest", "(", "array_sure", ",", "array_possible", ",", "array_hypothesis", ",", "source_sentences", ",", "target_sentences", ")", ":", "\n", "        ", "if", "clean_punctuation", ":", "\n", "            ", "A", "=", "{", "(", "s", ",", "t", ")", "for", "(", "s", ",", "t", ")", "in", "A", "if", "not", "(", "(", "source", "[", "s", "]", "in", "PUNCTUATION_MARKS", "or", "target", "[", "t", "]", "in", "PUNCTUATION_MARKS", ")", "and", "source", "[", "s", "]", "!=", "target", "[", "t", "]", ")", "}", "\n", "", "sum_a", "+=", "len", "(", "A", ")", "\n", "sum_s", "+=", "len", "(", "S", ")", "\n", "sum_a_intersect_p", "+=", "len", "(", "A", ".", "intersection", "(", "P", ")", ")", "\n", "sum_a_intersect_s", "+=", "len", "(", "A", ".", "intersection", "(", "S", ")", ")", "\n", "aligned_source_words", "+=", "len", "(", "{", "x", "for", "x", ",", "y", "in", "A", "}", ")", "\n", "aligned_target_words", "+=", "len", "(", "{", "y", "for", "x", ",", "y", "in", "A", "}", ")", "\n", "al", "=", "to_list", "(", "A", ")", "\n", "internal_jumps", "+=", "calculate_internal_jumps", "(", "al", ")", "\n", "external_jumps", "+=", "calculate_external_jumps", "(", "al", ")", "\n", "\n", "if", "source", "and", "target", ":", "\n", "            ", "for", "src_pos", ",", "tgt_pos", "in", "A", ":", "\n", "                ", "if", "not", "src_pos", "<", "len", "(", "source", ")", ":", "\n", "                    ", "print", "(", "source", ",", "len", "(", "source", ")", ",", "src_pos", ")", "\n", "", "if", "not", "tgt_pos", "<", "len", "(", "target", ")", ":", "\n", "                    ", "print", "(", "target", ",", "len", "(", "target", ")", ",", "tgt_pos", ")", "\n", "", "if", "(", "src_pos", ",", "tgt_pos", ")", "not", "in", "P", ":", "\n", "                    ", "errors", "[", "source", "[", "src_pos", "]", ",", "target", "[", "tgt_pos", "]", "]", "+=", "1", "\n", "\n", "", "", "", "", "precision", "=", "sum_a_intersect_p", "/", "sum_a", "\n", "recall", "=", "sum_a_intersect_s", "/", "sum_s", "\n", "aer", "=", "1.0", "-", "(", "(", "sum_a_intersect_p", "+", "sum_a_intersect_s", ")", "/", "(", "sum_a", "+", "sum_s", ")", ")", "\n", "\n", "if", "f_alpha", "<", "0.0", ":", "\n", "        ", "f_measure", "=", "0.0", "\n", "", "else", ":", "\n", "        ", "f_divident", "=", "f_alpha", "/", "precision", "\n", "f_divident", "+=", "(", "1.0", "-", "f_alpha", ")", "/", "recall", "\n", "f_measure", "=", "1.0", "/", "f_divident", "\n", "\n", "", "source_coverage", "=", "aligned_source_words", "/", "sum_source_words", "\n", "target_coverage", "=", "aligned_target_words", "/", "sum_target_words", "\n", "\n", "return", "precision", ",", "recall", ",", "aer", ",", "f_measure", ",", "errors", ",", "source_coverage", ",", "target_coverage", ",", "internal_jumps", ",", "external_jumps", "\n", "\n"]], "home.repos.pwc.inspect_result.neulab_awesome-align.tools.aer.parse_single_alignment": [[144, 158], ["string.replace().split", "int", "int", "string.replace"], "function", ["None"], ["", "def", "parse_single_alignment", "(", "string", ",", "reverse", "=", "False", ",", "one_indexed", "=", "False", ")", ":", "\n", "    ", "assert", "(", "'-'", "in", "string", "or", "'p'", "in", "string", ")", "and", "'Bad Alignment separator'", "\n", "\n", "a", ",", "b", "=", "string", ".", "replace", "(", "'p'", ",", "'-'", ")", ".", "split", "(", "'-'", ")", "\n", "a", ",", "b", "=", "int", "(", "a", ")", ",", "int", "(", "b", ")", "\n", "\n", "if", "one_indexed", ":", "\n", "        ", "a", "=", "a", "-", "1", "\n", "b", "=", "b", "-", "1", "\n", "\n", "", "if", "reverse", ":", "\n", "        ", "a", ",", "b", "=", "b", ",", "a", "\n", "\n", "", "return", "a", ",", "b", "\n", "\n"]], "home.repos.pwc.inspect_result.neulab_awesome-align.tools.aer.read_text": [[160, 165], ["open", "l.split"], "function", ["None"], ["", "def", "read_text", "(", "path", ")", ":", "\n", "    ", "if", "path", "==", "\"\"", ":", "\n", "        ", "return", "[", "]", "\n", "", "with", "open", "(", "path", ",", "\"r\"", ",", "encoding", "=", "\"utf-8\"", ")", "as", "f", ":", "\n", "        ", "return", "[", "l", ".", "split", "(", ")", "for", "l", "in", "f", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.neulab_awesome-align.awesome_align.sparsemax.SparsemaxFunction.forward": [[137, 146], ["X.max", "sparsemax._sparsemax_threshold_and_support", "torch.clamp", "torch.clamp", "torch.clamp", "torch.clamp", "ctx.save_for_backward"], "methods", ["home.repos.pwc.inspect_result.neulab_awesome-align.awesome_align.sparsemax._sparsemax_threshold_and_support"], ["    ", "@", "classmethod", "\n", "def", "forward", "(", "cls", ",", "ctx", ",", "X", ",", "dim", "=", "-", "1", ",", "k", "=", "None", ")", ":", "\n", "        ", "ctx", ".", "dim", "=", "dim", "\n", "max_val", ",", "_", "=", "X", ".", "max", "(", "dim", "=", "dim", ",", "keepdim", "=", "True", ")", "\n", "X", "=", "X", "-", "max_val", "# same numerical stability trick as softmax", "\n", "tau", ",", "supp_size", "=", "_sparsemax_threshold_and_support", "(", "X", ",", "dim", "=", "dim", ",", "k", "=", "k", ")", "\n", "output", "=", "torch", ".", "clamp", "(", "X", "-", "tau", ",", "min", "=", "0", ")", "\n", "ctx", ".", "save_for_backward", "(", "supp_size", ",", "output", ")", "\n", "return", "output", "\n", "\n"]], "home.repos.pwc.inspect_result.neulab_awesome-align.awesome_align.sparsemax.SparsemaxFunction.backward": [[147, 158], ["grad_output.clone", "v_hat.unsqueeze.unsqueeze.unsqueeze", "torch.where", "torch.where", "torch.where", "torch.where", "torch.where.sum", "torch.where.sum", "supp_size.to().squeeze", "supp_size.to"], "methods", ["None"], ["", "@", "classmethod", "\n", "def", "backward", "(", "cls", ",", "ctx", ",", "grad_output", ")", ":", "\n", "        ", "supp_size", ",", "output", "=", "ctx", ".", "saved_tensors", "\n", "dim", "=", "ctx", ".", "dim", "\n", "grad_input", "=", "grad_output", ".", "clone", "(", ")", "\n", "grad_input", "[", "output", "==", "0", "]", "=", "0", "\n", "\n", "v_hat", "=", "grad_input", ".", "sum", "(", "dim", "=", "dim", ")", "/", "supp_size", ".", "to", "(", "output", ".", "dtype", ")", ".", "squeeze", "(", ")", "\n", "v_hat", "=", "v_hat", ".", "unsqueeze", "(", "dim", ")", "\n", "grad_input", "=", "torch", ".", "where", "(", "output", "!=", "0", ",", "grad_input", "-", "v_hat", ",", "grad_input", ")", "\n", "return", "grad_input", ",", "None", ",", "None", "\n", "\n"]], "home.repos.pwc.inspect_result.neulab_awesome-align.awesome_align.sparsemax.Entmax15Function.forward": [[161, 174], ["X.max", "sparsemax._entmax_threshold_and_support", "ctx.save_for_backward", "torch.clamp", "torch.clamp", "torch.clamp", "torch.clamp"], "methods", ["home.repos.pwc.inspect_result.neulab_awesome-align.awesome_align.sparsemax._entmax_threshold_and_support"], ["    ", "@", "classmethod", "\n", "def", "forward", "(", "cls", ",", "ctx", ",", "X", ",", "dim", "=", "0", ",", "k", "=", "None", ")", ":", "\n", "        ", "ctx", ".", "dim", "=", "dim", "\n", "\n", "max_val", ",", "_", "=", "X", ".", "max", "(", "dim", "=", "dim", ",", "keepdim", "=", "True", ")", "\n", "X", "=", "X", "-", "max_val", "# same numerical stability trick as for softmax", "\n", "X", "=", "X", "/", "2", "# divide by 2 to solve actual Entmax", "\n", "\n", "tau_star", ",", "_", "=", "_entmax_threshold_and_support", "(", "X", ",", "dim", "=", "dim", ",", "k", "=", "k", ")", "\n", "\n", "Y", "=", "torch", ".", "clamp", "(", "X", "-", "tau_star", ",", "min", "=", "0", ")", "**", "2", "\n", "ctx", ".", "save_for_backward", "(", "Y", ")", "\n", "return", "Y", "\n", "\n"]], "home.repos.pwc.inspect_result.neulab_awesome-align.awesome_align.sparsemax.Entmax15Function.backward": [[175, 184], ["Y.sqrt", "q.unsqueeze.unsqueeze.unsqueeze", "dX.sum", "Y.sqrt.sum"], "methods", ["None"], ["", "@", "classmethod", "\n", "def", "backward", "(", "cls", ",", "ctx", ",", "dY", ")", ":", "\n", "        ", "Y", ",", "=", "ctx", ".", "saved_tensors", "\n", "gppr", "=", "Y", ".", "sqrt", "(", ")", "# = 1 / g'' (Y)", "\n", "dX", "=", "dY", "*", "gppr", "\n", "q", "=", "dX", ".", "sum", "(", "ctx", ".", "dim", ")", "/", "gppr", ".", "sum", "(", "ctx", ".", "dim", ")", "\n", "q", "=", "q", ".", "unsqueeze", "(", "ctx", ".", "dim", ")", "\n", "dX", "-=", "q", "*", "gppr", "\n", "return", "dX", ",", "None", ",", "None", "\n", "\n"]], "home.repos.pwc.inspect_result.neulab_awesome-align.awesome_align.sparsemax.Sparsemax.__init__": [[238, 256], ["torch.Module.__init__"], "methods", ["home.repos.pwc.inspect_result.neulab_awesome-align.awesome_align.train_utils.AdamW.__init__"], ["    ", "def", "__init__", "(", "self", ",", "dim", "=", "-", "1", ",", "k", "=", "None", ")", ":", "\n", "        ", "\"\"\"sparsemax: normalizing sparse transform (a la softmax).\n        Solves the projection:\n            min_p ||x - p||_2   s.t.    p >= 0, sum(p) == 1.\n        Parameters\n        ----------\n        dim : int\n            The dimension along which to apply sparsemax.\n        k : int or None\n            number of largest elements to partial-sort over. For optimal\n            performance, should be slightly bigger than the expected number of\n            nonzeros in the solution. If the solution is more than k-sparse,\n            this function is recursively called with a 2*k schedule.\n            If `None`, full sorting is performed from the beginning.\n        \"\"\"", "\n", "self", ".", "dim", "=", "dim", "\n", "self", ".", "k", "=", "k", "\n", "super", "(", "Sparsemax", ",", "self", ")", ".", "__init__", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.neulab_awesome-align.awesome_align.sparsemax.Sparsemax.forward": [[257, 259], ["sparsemax.sparsemax"], "methods", ["home.repos.pwc.inspect_result.neulab_awesome-align.awesome_align.sparsemax.sparsemax"], ["", "def", "forward", "(", "self", ",", "X", ")", ":", "\n", "        ", "return", "sparsemax", "(", "X", ",", "dim", "=", "self", ".", "dim", ",", "k", "=", "self", ".", "k", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.neulab_awesome-align.awesome_align.sparsemax.Entmax15.__init__": [[262, 281], ["torch.Module.__init__"], "methods", ["home.repos.pwc.inspect_result.neulab_awesome-align.awesome_align.train_utils.AdamW.__init__"], ["    ", "def", "__init__", "(", "self", ",", "dim", "=", "-", "1", ",", "k", "=", "None", ")", ":", "\n", "        ", "\"\"\"1.5-entmax: normalizing sparse transform (a la softmax).\n        Solves the optimization problem:\n            max_p <x, p> - H_1.5(p)    s.t.    p >= 0, sum(p) == 1.\n        where H_1.5(p) is the Tsallis alpha-entropy with alpha=1.5.\n        Parameters\n        ----------\n        dim : int\n            The dimension along which to apply 1.5-entmax.\n        k : int or None\n            number of largest elements to partial-sort over. For optimal\n            performance, should be slightly bigger than the expected number of\n            nonzeros in the solution. If the solution is more than k-sparse,\n            this function is recursively called with a 2*k schedule.\n            If `None`, full sorting is performed from the beginning.\n        \"\"\"", "\n", "self", ".", "dim", "=", "dim", "\n", "self", ".", "k", "=", "k", "\n", "super", "(", "Entmax15", ",", "self", ")", ".", "__init__", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.neulab_awesome-align.awesome_align.sparsemax.Entmax15.forward": [[282, 284], ["sparsemax.entmax15"], "methods", ["home.repos.pwc.inspect_result.neulab_awesome-align.awesome_align.sparsemax.entmax15"], ["", "def", "forward", "(", "self", ",", "X", ")", ":", "\n", "        ", "return", "entmax15", "(", "X", ",", "dim", "=", "self", ".", "dim", ",", "k", "=", "self", ".", "k", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.neulab_awesome-align.awesome_align.sparsemax._make_ix_like": [[17, 23], ["X.size", "torch.arange", "torch.arange", "torch.arange.view().transpose", "X.dim", "torch.arange.view"], "function", ["None"], ["def", "_make_ix_like", "(", "X", ",", "dim", ")", ":", "\n", "    ", "d", "=", "X", ".", "size", "(", "dim", ")", "\n", "rho", "=", "torch", ".", "arange", "(", "1", ",", "d", "+", "1", ",", "device", "=", "X", ".", "device", ",", "dtype", "=", "X", ".", "dtype", ")", "\n", "view", "=", "[", "1", "]", "*", "X", ".", "dim", "(", ")", "\n", "view", "[", "0", "]", "=", "-", "1", "\n", "return", "rho", ".", "view", "(", "view", ")", ".", "transpose", "(", "0", ",", "dim", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.neulab_awesome-align.awesome_align.sparsemax._roll_last": [[25, 33], ["X.permute", "X.dim", "range", "X.dim"], "function", ["None"], ["", "def", "_roll_last", "(", "X", ",", "dim", ")", ":", "\n", "    ", "if", "dim", "==", "-", "1", ":", "\n", "        ", "return", "X", "\n", "", "elif", "dim", "<", "0", ":", "\n", "        ", "dim", "=", "X", ".", "dim", "(", ")", "-", "dim", "\n", "\n", "", "perm", "=", "[", "i", "for", "i", "in", "range", "(", "X", ".", "dim", "(", ")", ")", "if", "i", "!=", "dim", "]", "+", "[", "dim", "]", "\n", "return", "X", ".", "permute", "(", "perm", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.neulab_awesome-align.awesome_align.sparsemax._sparsemax_threshold_and_support": [[35, 80], ["sparsemax._make_ix_like", "support.sum().unsqueeze", "topk_cumsum.gather", "support.sum().unsqueeze.to", "torch.sort", "torch.sort", "torch.topk", "torch.topk", "topk.cumsum", "torch.any", "torch.any", "support.sum", "sparsemax._sparsemax_threshold_and_support", "sparsemax._roll_last", "sparsemax._roll_last", "sparsemax._roll_last"], "function", ["home.repos.pwc.inspect_result.neulab_awesome-align.awesome_align.sparsemax._make_ix_like", "home.repos.pwc.inspect_result.neulab_awesome-align.awesome_align.sparsemax._sparsemax_threshold_and_support", "home.repos.pwc.inspect_result.neulab_awesome-align.awesome_align.sparsemax._roll_last", "home.repos.pwc.inspect_result.neulab_awesome-align.awesome_align.sparsemax._roll_last", "home.repos.pwc.inspect_result.neulab_awesome-align.awesome_align.sparsemax._roll_last"], ["", "def", "_sparsemax_threshold_and_support", "(", "X", ",", "dim", "=", "-", "1", ",", "k", "=", "None", ")", ":", "\n", "    ", "\"\"\"Core computation for sparsemax: optimal threshold and support size.\n    Parameters\n    ----------\n    X : torch.Tensor\n        The input tensor to compute thresholds over.\n    dim : int\n        The dimension along which to apply sparsemax.\n    k : int or None\n        number of largest elements to partial-sort over. For optimal\n        performance, should be slightly bigger than the expected number of\n        nonzeros in the solution. If the solution is more than k-sparse,\n        this function is recursively called with a 2*k schedule.\n        If `None`, full sorting is performed from the beginning.\n    Returns\n    -------\n    tau : torch.Tensor like `X`, with all but the `dim` dimension intact\n        the threshold value for each vector\n    support_size : torch LongTensor, shape like `tau`\n        the number of nonzeros in each vector.\n    \"\"\"", "\n", "\n", "if", "k", "is", "None", "or", "k", ">=", "X", ".", "shape", "[", "dim", "]", ":", "# do full sort", "\n", "        ", "topk", ",", "_", "=", "torch", ".", "sort", "(", "X", ",", "dim", "=", "dim", ",", "descending", "=", "True", ")", "\n", "", "else", ":", "\n", "        ", "topk", ",", "_", "=", "torch", ".", "topk", "(", "X", ",", "k", "=", "k", ",", "dim", "=", "dim", ")", "\n", "\n", "", "topk_cumsum", "=", "topk", ".", "cumsum", "(", "dim", ")", "-", "1", "\n", "rhos", "=", "_make_ix_like", "(", "topk", ",", "dim", ")", "\n", "support", "=", "rhos", "*", "topk", ">", "topk_cumsum", "\n", "\n", "support_size", "=", "support", ".", "sum", "(", "dim", "=", "dim", ")", ".", "unsqueeze", "(", "dim", ")", "\n", "tau", "=", "topk_cumsum", ".", "gather", "(", "dim", ",", "support_size", "-", "1", ")", "\n", "tau", "/=", "support_size", ".", "to", "(", "X", ".", "dtype", ")", "\n", "\n", "if", "k", "is", "not", "None", "and", "k", "<", "X", ".", "shape", "[", "dim", "]", ":", "\n", "        ", "unsolved", "=", "(", "support_size", "==", "k", ")", ".", "squeeze", "(", "dim", ")", "\n", "\n", "if", "torch", ".", "any", "(", "unsolved", ")", ":", "\n", "            ", "in_", "=", "_roll_last", "(", "X", ",", "dim", ")", "[", "unsolved", "]", "\n", "tau_", ",", "ss_", "=", "_sparsemax_threshold_and_support", "(", "in_", ",", "dim", "=", "-", "1", ",", "k", "=", "2", "*", "k", ")", "\n", "_roll_last", "(", "tau", ",", "dim", ")", "[", "unsolved", "]", "=", "tau_", "\n", "_roll_last", "(", "support_size", ",", "dim", ")", "[", "unsolved", "]", "=", "ss_", "\n", "\n", "", "", "return", "tau", ",", "support_size", "\n", "\n"]], "home.repos.pwc.inspect_result.neulab_awesome-align.awesome_align.sparsemax._entmax_threshold_and_support": [[82, 134], ["sparsemax._make_ix_like", "torch.clamp", "torch.clamp", "tau.gather", "torch.sort", "torch.sort", "torch.topk", "torch.topk", "Xsrt.cumsum", "torch.sqrt", "torch.sqrt", "torch.any", "torch.any", "sparsemax._entmax_threshold_and_support", "sparsemax._roll_last", "sparsemax._roll_last", "sparsemax._roll_last"], "function", ["home.repos.pwc.inspect_result.neulab_awesome-align.awesome_align.sparsemax._make_ix_like", "home.repos.pwc.inspect_result.neulab_awesome-align.awesome_align.sparsemax._entmax_threshold_and_support", "home.repos.pwc.inspect_result.neulab_awesome-align.awesome_align.sparsemax._roll_last", "home.repos.pwc.inspect_result.neulab_awesome-align.awesome_align.sparsemax._roll_last", "home.repos.pwc.inspect_result.neulab_awesome-align.awesome_align.sparsemax._roll_last"], ["", "def", "_entmax_threshold_and_support", "(", "X", ",", "dim", "=", "-", "1", ",", "k", "=", "None", ")", ":", "\n", "    ", "\"\"\"Core computation for 1.5-entmax: optimal threshold and support size.\n    Parameters\n    ----------\n    X : torch.Tensor\n        The input tensor to compute thresholds over.\n    dim : int\n        The dimension along which to apply 1.5-entmax.\n    k : int or None\n        number of largest elements to partial-sort over. For optimal\n        performance, should be slightly bigger than the expected number of\n        nonzeros in the solution. If the solution is more than k-sparse,\n        this function is recursively called with a 2*k schedule.\n        If `None`, full sorting is performed from the beginning.\n    Returns\n    -------\n    tau : torch.Tensor like `X`, with all but the `dim` dimension intact\n        the threshold value for each vector\n    support_size : torch LongTensor, shape like `tau`\n        the number of nonzeros in each vector.\n    \"\"\"", "\n", "\n", "if", "k", "is", "None", "or", "k", ">=", "X", ".", "shape", "[", "dim", "]", ":", "# do full sort", "\n", "        ", "Xsrt", ",", "_", "=", "torch", ".", "sort", "(", "X", ",", "dim", "=", "dim", ",", "descending", "=", "True", ")", "\n", "", "else", ":", "\n", "        ", "Xsrt", ",", "_", "=", "torch", ".", "topk", "(", "X", ",", "k", "=", "k", ",", "dim", "=", "dim", ")", "\n", "\n", "", "rho", "=", "_make_ix_like", "(", "Xsrt", ",", "dim", ")", "\n", "mean", "=", "Xsrt", ".", "cumsum", "(", "dim", ")", "/", "rho", "\n", "mean_sq", "=", "(", "Xsrt", "**", "2", ")", ".", "cumsum", "(", "dim", ")", "/", "rho", "\n", "ss", "=", "rho", "*", "(", "mean_sq", "-", "mean", "**", "2", ")", "\n", "delta", "=", "(", "1", "-", "ss", ")", "/", "rho", "\n", "\n", "# NOTE this is not exactly the same as in reference algo", "\n", "# Fortunately it seems the clamped values never wrongly", "\n", "# get selected by tau <= sorted_z. Prove this!", "\n", "delta_nz", "=", "torch", ".", "clamp", "(", "delta", ",", "0", ")", "\n", "tau", "=", "mean", "-", "torch", ".", "sqrt", "(", "delta_nz", ")", "\n", "\n", "support_size", "=", "(", "tau", "<=", "Xsrt", ")", ".", "sum", "(", "dim", ")", ".", "unsqueeze", "(", "dim", ")", "\n", "tau_star", "=", "tau", ".", "gather", "(", "dim", ",", "support_size", "-", "1", ")", "\n", "\n", "if", "k", "is", "not", "None", "and", "k", "<", "X", ".", "shape", "[", "dim", "]", ":", "\n", "        ", "unsolved", "=", "(", "support_size", "==", "k", ")", ".", "squeeze", "(", "dim", ")", "\n", "\n", "if", "torch", ".", "any", "(", "unsolved", ")", ":", "\n", "            ", "X_", "=", "_roll_last", "(", "X", ",", "dim", ")", "[", "unsolved", "]", "\n", "tau_", ",", "ss_", "=", "_entmax_threshold_and_support", "(", "X_", ",", "dim", "=", "-", "1", ",", "k", "=", "2", "*", "k", ")", "\n", "_roll_last", "(", "tau_star", ",", "dim", ")", "[", "unsolved", "]", "=", "tau_", "\n", "_roll_last", "(", "support_size", ",", "dim", ")", "[", "unsolved", "]", "=", "ss_", "\n", "\n", "", "", "return", "tau_star", ",", "support_size", "\n", "\n"]], "home.repos.pwc.inspect_result.neulab_awesome-align.awesome_align.sparsemax.sparsemax": [[186, 209], ["SparsemaxFunction.apply"], "function", ["None"], ["", "", "def", "sparsemax", "(", "X", ",", "dim", "=", "-", "1", ",", "k", "=", "None", ")", ":", "\n", "    ", "\"\"\"sparsemax: normalizing sparse transform (a la softmax).\n    Solves the projection:\n        min_p ||x - p||_2   s.t.    p >= 0, sum(p) == 1.\n    Parameters\n    ----------\n    X : torch.Tensor\n        The input tensor.\n    dim : int\n        The dimension along which to apply sparsemax.\n    k : int or None\n        number of largest elements to partial-sort over. For optimal\n        performance, should be slightly bigger than the expected number of\n        nonzeros in the solution. If the solution is more than k-sparse,\n        this function is recursively called with a 2*k schedule.\n        If `None`, full sorting is performed from the beginning.\n    Returns\n    -------\n    P : torch tensor, same shape as X\n        The projection result, such that P.sum(dim=dim) == 1 elementwise.\n    \"\"\"", "\n", "\n", "return", "SparsemaxFunction", ".", "apply", "(", "X", ",", "dim", ",", "k", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.neulab_awesome-align.awesome_align.sparsemax.entmax15": [[211, 235], ["Entmax15Function.apply"], "function", ["None"], ["", "def", "entmax15", "(", "X", ",", "dim", "=", "-", "1", ",", "k", "=", "None", ")", ":", "\n", "    ", "\"\"\"1.5-entmax: normalizing sparse transform (a la softmax).\n    Solves the optimization problem:\n        max_p <x, p> - H_1.5(p)    s.t.    p >= 0, sum(p) == 1.\n    where H_1.5(p) is the Tsallis alpha-entropy with alpha=1.5.\n    Parameters\n    ----------\n    X : torch.Tensor\n        The input tensor.\n    dim : int\n        The dimension along which to apply 1.5-entmax.\n    k : int or None\n        number of largest elements to partial-sort over. For optimal\n        performance, should be slightly bigger than the expected number of\n        nonzeros in the solution. If the solution is more than k-sparse,\n        this function is recursively called with a 2*k schedule.\n        If `None`, full sorting is performed from the beginning.\n    Returns\n    -------\n    P : torch tensor, same shape as X\n        The projection result, such that P.sum(dim=dim) == 1 elementwise.\n    \"\"\"", "\n", "\n", "return", "Entmax15Function", ".", "apply", "(", "X", ",", "dim", ",", "k", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.neulab_awesome-align.awesome_align.run_train.LineByLineTextDataset.__init__": [[49, 120], ["os.path.isfile", "logger.info", "os.path.isfile", "logger.info", "torch.load", "tqdm.tqdm.tqdm", "open", "f.readlines", "os.path.isfile", "logger.info", "enumerate", "logger.info", "torch.save", "open", "f.readlines", "len", "len", "len", "enumerate", "enumerate", "len", "line.isspace", "len", "line.split", "src.strip().split", "tgt.strip().split", "logger.info", "run_train.LineByLineTextDataset.examples.append", "line.split", "logger.info", "logger.info", "tokenizer.tokenize", "tokenizer.tokenize", "tokenizer.convert_tokens_to_ids", "tokenizer.convert_tokens_to_ids", "tokenizer.prepare_for_model", "tokenizer.prepare_for_model", "len", "len", "gold_lines[].strip().split", "run_train.LineByLineTextDataset.examples.append", "src.rstrip", "tgt.rstrip", "src.strip", "tgt.strip", "list", "list", "gold_word_pairs.append", "logger.info", "itertools.chain", "itertools.chain", "gold_lines[].strip", "src_tgt.split", "src_tgt.split", "gold_lines[].strip", "int", "int", "int", "int"], "methods", ["home.repos.pwc.inspect_result.neulab_awesome-align.awesome_align.tokenization_utils.PreTrainedTokenizerFast.tokenize", "home.repos.pwc.inspect_result.neulab_awesome-align.awesome_align.tokenization_utils.PreTrainedTokenizerFast.tokenize", "home.repos.pwc.inspect_result.neulab_awesome-align.awesome_align.tokenization_utils.PreTrainedTokenizer.convert_tokens_to_ids", "home.repos.pwc.inspect_result.neulab_awesome-align.awesome_align.tokenization_utils.PreTrainedTokenizer.convert_tokens_to_ids", "home.repos.pwc.inspect_result.neulab_awesome-align.awesome_align.tokenization_utils.PreTrainedTokenizer.prepare_for_model", "home.repos.pwc.inspect_result.neulab_awesome-align.awesome_align.tokenization_utils.PreTrainedTokenizer.prepare_for_model"], []], "home.repos.pwc.inspect_result.neulab_awesome-align.awesome_align.run_train.LineByLineTextDataset.__len__": [[121, 123], ["len"], "methods", ["None"], []], "home.repos.pwc.inspect_result.neulab_awesome-align.awesome_align.run_train.LineByLineTextDataset.__getitem__": [[124, 129], ["random.randint", "tuple", "random.randint", "len", "list", "list", "len"], "methods", ["None"], []], "home.repos.pwc.inspect_result.neulab_awesome-align.awesome_align.run_train.load_and_cache_examples": [[131, 135], ["run_train.LineByLineTextDataset"], "function", ["None"], []], "home.repos.pwc.inspect_result.neulab_awesome-align.awesome_align.run_train.set_seed": [[137, 143], ["random.seed", "numpy.random.seed", "torch.manual_seed", "torch.cuda.manual_seed_all"], "function", ["None"], []], "home.repos.pwc.inspect_result.neulab_awesome-align.awesome_align.run_train.mask_tokens": [[145, 185], ["inputs.clone", "torch.full", "torch.full.masked_fill_", "torch.bernoulli().to", "tokenizer.convert_tokens_to_ids", "torch.randint", "ValueError", "tokenizer.get_special_tokens_mask", "torch.tensor", "inputs.clone.eq", "torch.full.masked_fill_", "langid_mask.eq", "torch.full.masked_fill_", "torch.bernoulli().to", "len", "inputs.clone.tolist", "torch.bernoulli", "torch.bernoulli().to", "torch.bernoulli", "torch.full", "torch.bernoulli", "torch.full"], "function", ["home.repos.pwc.inspect_result.neulab_awesome-align.awesome_align.tokenization_utils.PreTrainedTokenizer.convert_tokens_to_ids", "home.repos.pwc.inspect_result.neulab_awesome-align.awesome_align.tokenization_utils.PreTrainedTokenizer.get_special_tokens_mask"], []], "home.repos.pwc.inspect_result.neulab_awesome-align.awesome_align.run_train.train": [[187, 434], ["torch.utils.data.DataLoader", "awesome_align.train_utils.AdamW", "awesome_align.train_utils.get_linear_schedule_with_warmup", "logger.info", "logger.info", "logger.info", "logger.info", "logger.info", "logger.info", "logger.info", "model_to_resize.resize_token_embeddings", "torch.nn.parallel.DistributedDataParallel.zero_grad", "run_train.set_seed", "tqdm.trange", "range", "max", "torch.nn.parallel.DistributedDataParallel.eval", "torch.nn.utils.rnn.pad_sequence", "torch.nn.utils.rnn.pad_sequence", "torch.nn.utils.rnn.pad_sequence", "torch.nn.utils.rnn.pad_sequence", "torch.nn.utils.rnn.pad_sequence", "torch.nn.utils.rnn.pad_sequence", "torch.nn.utils.rnn.pad_sequence", "torch.tensor", "torch.utils.data.RandomSampler", "torch.utils.data.distributed.DistributedSampler", "amp.initialize", "torch.nn.DataParallel", "torch.nn.parallel.DistributedDataParallel", "len", "hasattr", "len", "model.item", "int", "int", "enumerate", "[].view", "torch.cat", "torch.cat", "int", "torch.cat", "torch.cat", "torch.nn.utils.rnn.pad_sequence.append", "torch.nn.utils.rnn.pad_sequence.append", "max", "max", "torch.cat", "torch.cat", "torch.nn.utils.rnn.pad_sequence.append", "torch.nn.utils.rnn.pad_sequence.append", "torch.cat", "torch.cat", "torch.nn.utils.rnn.pad_sequence.append", "torch.nn.utils.rnn.pad_sequence.append", "torch.cat", "torch.cat", "torch.nn.utils.rnn.pad_sequence.append", "torch.tensor.append", "random.random", "torch.nn.utils.rnn.pad_sequence.append", "torch.tensor.append", "bpe2word_map_src.append", "bpe2word_map_tgt.append", "word_aligns.append", "torch.nn.parallel.DistributedDataParallel.module.get_aligned_word", "torch.nn.parallel.DistributedDataParallel.get_aligned_word", "len", "model.mean", "model.backward", "torch.nn.parallel.DistributedDataParallel.train", "len", "len", "random.random", "torch.cat", "torch.cat", "torch.cat", "ImportError", "torch.distributed.get_world_size", "amp.scale_loss", "scaled_loss.backward", "batch[].to", "torch.nn.parallel.DistributedDataParallel.", "run_train.train.backward_loss"], "function", ["home.repos.pwc.inspect_result.neulab_awesome-align.awesome_align.train_utils.get_linear_schedule_with_warmup", "home.repos.pwc.inspect_result.neulab_awesome-align.awesome_align.modeling_utils.PreTrainedModel.resize_token_embeddings", "home.repos.pwc.inspect_result.neulab_awesome-align.awesome_align.run_align.set_seed", "home.repos.pwc.inspect_result.neulab_awesome-align.awesome_align.modeling.BertForMaskedLM.get_aligned_word", "home.repos.pwc.inspect_result.neulab_awesome-align.awesome_align.modeling.BertForMaskedLM.get_aligned_word", "home.repos.pwc.inspect_result.neulab_awesome-align.awesome_align.sparsemax.Entmax15Function.backward", "home.repos.pwc.inspect_result.neulab_awesome-align.awesome_align.run_train.train", "home.repos.pwc.inspect_result.neulab_awesome-align.awesome_align.sparsemax.Entmax15Function.backward"], []], "home.repos.pwc.inspect_result.neulab_awesome-align.awesome_align.run_train.evaluate": [[435, 592], ["run_train.load_and_cache_examples", "torch.utils.data.SequentialSampler", "torch.utils.data.DataLoader", "logger.info", "logger.info", "logger.info", "torch.nn.DataParallel.eval", "run_train.set_seed", "tqdm.tqdm", "torch.exp", "os.path.join", "os.makedirs", "max", "torch.nn.DataParallel.eval", "torch.nn.utils.rnn.pad_sequence", "torch.nn.utils.rnn.pad_sequence", "torch.nn.utils.rnn.pad_sequence", "torch.nn.utils.rnn.pad_sequence", "torch.nn.utils.rnn.pad_sequence", "torch.nn.utils.rnn.pad_sequence", "torch.nn.utils.rnn.pad_sequence", "torch.tensor", "torch.nn.DataParallel", "len", "model.item", "torch.tensor", "open", "logger.info", "sorted", "[].view", "torch.cat", "torch.cat", "int", "torch.cat", "torch.cat", "torch.nn.utils.rnn.pad_sequence.append", "torch.nn.utils.rnn.pad_sequence.append", "max", "max", "torch.cat", "torch.cat", "torch.nn.utils.rnn.pad_sequence.append", "torch.nn.utils.rnn.pad_sequence.append", "torch.cat", "torch.cat", "torch.nn.utils.rnn.pad_sequence.append", "torch.nn.utils.rnn.pad_sequence.append", "torch.cat", "torch.cat", "torch.cat", "torch.nn.utils.rnn.pad_sequence.append", "torch.tensor.append", "torch.cat", "torch.nn.utils.rnn.pad_sequence.append", "torch.tensor.append", "bpe2word_map_src.append", "bpe2word_map_tgt.append", "word_aligns.append", "torch.nn.DataParallel.module.get_aligned_word", "torch.nn.DataParallel.get_aligned_word", "model.mean", "torch.no_grad", "result.keys", "logger.info", "writer.write", "len", "len", "batch[].to", "torch.nn.DataParallel.", "run_train.evaluate.post_loss"], "function", ["home.repos.pwc.inspect_result.neulab_awesome-align.awesome_align.run_train.load_and_cache_examples", "home.repos.pwc.inspect_result.neulab_awesome-align.awesome_align.run_align.set_seed", "home.repos.pwc.inspect_result.neulab_awesome-align.awesome_align.modeling.BertForMaskedLM.get_aligned_word", "home.repos.pwc.inspect_result.neulab_awesome-align.awesome_align.modeling.BertForMaskedLM.get_aligned_word"], []], "home.repos.pwc.inspect_result.neulab_awesome-align.awesome_align.run_train.main": [[594, 884], ["argparse.ArgumentParser", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.parse_args", "logging.basicConfig", "logger.warning", "run_train.set_seed", "model_class.from_pretrained.to", "logger.info", "ValueError", "awesome_align.train_utils._sorted_checkpoints", "os.path.exists", "os.listdir", "ValueError", "torch.device", "torch.cuda.set_device", "torch.device", "torch.distributed.init_process_group", "bool", "torch.distributed.barrier", "config_class.from_pretrained", "tokenizer_class.from_pretrained", "min", "model_class.from_pretrained", "logger.info", "model_class", "torch.distributed.barrier", "run_train.load_and_cache_examples", "run_train.train", "logger.info", "logger.info", "len", "ValueError", "torch.cuda.device_count", "config_class.from_pretrained", "config_class", "tokenizer_class.from_pretrained", "ValueError", "torch.distributed.barrier", "torch.distributed.barrier", "logger.info", "os.makedirs", "model_to_save.save_pretrained", "tokenizer_class.from_pretrained.save_pretrained", "torch.save", "model_class.from_pretrained", "tokenizer_class.from_pretrained", "model_class.from_pretrained.to", "model_class.from_pretrained", "model_class.from_pretrained.to", "run_train.evaluate", "dict", "results.update", "bool", "torch.distributed.get_rank", "hasattr", "os.path.join", "torch.cuda.is_available", "len", "checkpoint.split", "checkpoint.find", "checkpoint.split", "dict.items"], "function", ["home.repos.pwc.inspect_result.neulab_awesome-align.tools.aer.parse_args", "home.repos.pwc.inspect_result.neulab_awesome-align.awesome_align.run_align.set_seed", "home.repos.pwc.inspect_result.neulab_awesome-align.awesome_align.train_utils._sorted_checkpoints", "home.repos.pwc.inspect_result.neulab_awesome-align.awesome_align.configuration_utils.PretrainedConfig.from_pretrained", "home.repos.pwc.inspect_result.neulab_awesome-align.awesome_align.configuration_utils.PretrainedConfig.from_pretrained", "home.repos.pwc.inspect_result.neulab_awesome-align.awesome_align.configuration_utils.PretrainedConfig.from_pretrained", "home.repos.pwc.inspect_result.neulab_awesome-align.awesome_align.run_train.load_and_cache_examples", "home.repos.pwc.inspect_result.neulab_awesome-align.awesome_align.run_train.train", "home.repos.pwc.inspect_result.neulab_awesome-align.awesome_align.configuration_utils.PretrainedConfig.from_pretrained", "home.repos.pwc.inspect_result.neulab_awesome-align.awesome_align.configuration_utils.PretrainedConfig.from_pretrained", "home.repos.pwc.inspect_result.neulab_awesome-align.awesome_align.configuration_utils.PretrainedConfig.save_pretrained", "home.repos.pwc.inspect_result.neulab_awesome-align.awesome_align.configuration_utils.PretrainedConfig.save_pretrained", "home.repos.pwc.inspect_result.neulab_awesome-align.awesome_align.configuration_utils.PretrainedConfig.from_pretrained", "home.repos.pwc.inspect_result.neulab_awesome-align.awesome_align.configuration_utils.PretrainedConfig.from_pretrained", "home.repos.pwc.inspect_result.neulab_awesome-align.awesome_align.configuration_utils.PretrainedConfig.from_pretrained", "home.repos.pwc.inspect_result.neulab_awesome-align.awesome_align.run_train.evaluate"], []], "home.repos.pwc.inspect_result.neulab_awesome-align.awesome_align.modeling.BertEmbeddings.__init__": [[152, 162], ["torch.nn.Module.__init__", "torch.nn.Embedding", "torch.nn.Embedding", "torch.nn.Embedding", "BertLayerNorm", "torch.nn.Dropout"], "methods", ["home.repos.pwc.inspect_result.neulab_awesome-align.awesome_align.train_utils.AdamW.__init__"], ["def", "__init__", "(", "self", ",", "config", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "word_embeddings", "=", "nn", ".", "Embedding", "(", "config", ".", "vocab_size", ",", "config", ".", "hidden_size", ",", "padding_idx", "=", "0", ")", "\n", "self", ".", "position_embeddings", "=", "nn", ".", "Embedding", "(", "config", ".", "max_position_embeddings", ",", "config", ".", "hidden_size", ")", "\n", "self", ".", "token_type_embeddings", "=", "nn", ".", "Embedding", "(", "config", ".", "type_vocab_size", ",", "config", ".", "hidden_size", ")", "\n", "\n", "# self.LayerNorm is not snake-cased to stick with TensorFlow model variable name and be able to load", "\n", "# any TensorFlow checkpoint file", "\n", "self", ".", "LayerNorm", "=", "BertLayerNorm", "(", "config", ".", "hidden_size", ",", "eps", "=", "config", ".", "layer_norm_eps", ")", "\n", "self", ".", "dropout", "=", "nn", ".", "Dropout", "(", "config", ".", "hidden_dropout_prob", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.neulab_awesome-align.awesome_align.modeling.BertEmbeddings.forward": [[163, 185], ["modeling.BertEmbeddings.position_embeddings", "modeling.BertEmbeddings.token_type_embeddings", "modeling.BertEmbeddings.LayerNorm", "modeling.BertEmbeddings.dropout", "input_ids.size", "torch.arange", "position_ids.unsqueeze().expand.unsqueeze().expand.unsqueeze().expand", "torch.zeros", "modeling.BertEmbeddings.word_embeddings", "modeling.BertEmbeddings.size", "position_ids.unsqueeze().expand.unsqueeze().expand.unsqueeze"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "input_ids", "=", "None", ",", "token_type_ids", "=", "None", ",", "position_ids", "=", "None", ",", "inputs_embeds", "=", "None", ")", ":", "\n", "        ", "if", "input_ids", "is", "not", "None", ":", "\n", "            ", "input_shape", "=", "input_ids", ".", "size", "(", ")", "\n", "", "else", ":", "\n", "            ", "input_shape", "=", "inputs_embeds", ".", "size", "(", ")", "[", ":", "-", "1", "]", "\n", "\n", "", "seq_length", "=", "input_shape", "[", "1", "]", "\n", "device", "=", "input_ids", ".", "device", "if", "input_ids", "is", "not", "None", "else", "inputs_embeds", ".", "device", "\n", "if", "position_ids", "is", "None", ":", "\n", "            ", "position_ids", "=", "torch", ".", "arange", "(", "seq_length", ",", "dtype", "=", "torch", ".", "long", ",", "device", "=", "device", ")", "\n", "position_ids", "=", "position_ids", ".", "unsqueeze", "(", "0", ")", ".", "expand", "(", "input_shape", ")", "\n", "", "if", "token_type_ids", "is", "None", ":", "\n", "            ", "token_type_ids", "=", "torch", ".", "zeros", "(", "input_shape", ",", "dtype", "=", "torch", ".", "long", ",", "device", "=", "device", ")", "\n", "", "if", "inputs_embeds", "is", "None", ":", "\n", "            ", "inputs_embeds", "=", "self", ".", "word_embeddings", "(", "input_ids", ")", "\n", "", "position_embeddings", "=", "self", ".", "position_embeddings", "(", "position_ids", ")", "\n", "token_type_embeddings", "=", "self", ".", "token_type_embeddings", "(", "token_type_ids", ")", "\n", "\n", "embeddings", "=", "inputs_embeds", "+", "position_embeddings", "+", "token_type_embeddings", "\n", "embeddings", "=", "self", ".", "LayerNorm", "(", "embeddings", ")", "\n", "embeddings", "=", "self", ".", "dropout", "(", "embeddings", ")", "\n", "return", "embeddings", "\n", "\n"]], "home.repos.pwc.inspect_result.neulab_awesome-align.awesome_align.modeling.BertSelfAttention.__init__": [[188, 204], ["torch.nn.Module.__init__", "int", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Dropout", "ValueError"], "methods", ["home.repos.pwc.inspect_result.neulab_awesome-align.awesome_align.train_utils.AdamW.__init__"], ["    ", "def", "__init__", "(", "self", ",", "config", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "if", "config", ".", "hidden_size", "%", "config", ".", "num_attention_heads", "!=", "0", ":", "\n", "            ", "raise", "ValueError", "(", "\n", "\"The hidden size (%d) is not a multiple of the number of attention \"", "\n", "\"heads (%d)\"", "%", "(", "config", ".", "hidden_size", ",", "config", ".", "num_attention_heads", ")", "\n", ")", "\n", "", "self", ".", "num_attention_heads", "=", "config", ".", "num_attention_heads", "\n", "self", ".", "attention_head_size", "=", "int", "(", "config", ".", "hidden_size", "/", "config", ".", "num_attention_heads", ")", "\n", "self", ".", "all_head_size", "=", "self", ".", "num_attention_heads", "*", "self", ".", "attention_head_size", "\n", "\n", "self", ".", "query", "=", "nn", ".", "Linear", "(", "config", ".", "hidden_size", ",", "self", ".", "all_head_size", ")", "\n", "self", ".", "key", "=", "nn", ".", "Linear", "(", "config", ".", "hidden_size", ",", "self", ".", "all_head_size", ")", "\n", "self", ".", "value", "=", "nn", ".", "Linear", "(", "config", ".", "hidden_size", ",", "self", ".", "all_head_size", ")", "\n", "\n", "self", ".", "dropout", "=", "nn", ".", "Dropout", "(", "config", ".", "attention_probs_dropout_prob", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.neulab_awesome-align.awesome_align.modeling.BertSelfAttention.transpose_for_scores": [[205, 209], ["x.view.view.view", "x.view.view.permute", "x.view.view.size"], "methods", ["None"], ["", "def", "transpose_for_scores", "(", "self", ",", "x", ")", ":", "\n", "        ", "new_x_shape", "=", "x", ".", "size", "(", ")", "[", ":", "-", "1", "]", "+", "(", "self", ".", "num_attention_heads", ",", "self", ".", "attention_head_size", ")", "\n", "x", "=", "x", ".", "view", "(", "*", "new_x_shape", ")", "\n", "return", "x", ".", "permute", "(", "0", ",", "2", ",", "1", ",", "3", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.neulab_awesome-align.awesome_align.modeling.BertSelfAttention.forward": [[210, 246], ["modeling.BertSelfAttention.query", "modeling.BertSelfAttention.key", "modeling.BertSelfAttention.value", "modeling.BertSelfAttention.transpose_for_scores", "modeling.BertSelfAttention.transpose_for_scores", "modeling.BertSelfAttention.transpose_for_scores", "torch.matmul", "modeling.BertSelfAttention.dropout", "torch.matmul", "context_layer.view.view.permute().contiguous", "context_layer.view.view.view", "modeling.BertSelfAttention.transpose", "math.sqrt", "torch.nn.Softmax", "context_layer.view.view.permute", "context_layer.view.view.size"], "methods", ["home.repos.pwc.inspect_result.neulab_awesome-align.awesome_align.modeling.BertGuideHead.transpose_for_scores", "home.repos.pwc.inspect_result.neulab_awesome-align.awesome_align.modeling.BertGuideHead.transpose_for_scores", "home.repos.pwc.inspect_result.neulab_awesome-align.awesome_align.modeling.BertGuideHead.transpose_for_scores"], ["", "def", "forward", "(", "\n", "self", ",", "\n", "hidden_states", ",", "\n", "attention_mask", "=", "None", ",", "\n", ")", ":", "\n", "        ", "mixed_query_layer", "=", "self", ".", "query", "(", "hidden_states", ")", "\n", "\n", "mixed_key_layer", "=", "self", ".", "key", "(", "hidden_states", ")", "\n", "mixed_value_layer", "=", "self", ".", "value", "(", "hidden_states", ")", "\n", "\n", "query_layer", "=", "self", ".", "transpose_for_scores", "(", "mixed_query_layer", ")", "\n", "key_layer", "=", "self", ".", "transpose_for_scores", "(", "mixed_key_layer", ")", "\n", "value_layer", "=", "self", ".", "transpose_for_scores", "(", "mixed_value_layer", ")", "\n", "\n", "# Take the dot product between \"query\" and \"key\" to get the raw attention scores.", "\n", "attention_scores", "=", "torch", ".", "matmul", "(", "query_layer", ",", "key_layer", ".", "transpose", "(", "-", "1", ",", "-", "2", ")", ")", "\n", "attention_scores", "=", "attention_scores", "/", "math", ".", "sqrt", "(", "self", ".", "attention_head_size", ")", "\n", "if", "attention_mask", "is", "not", "None", ":", "\n", "# Apply the attention mask is (precomputed for all layers in BertModel forward() function)", "\n", "            ", "attention_scores", "=", "attention_scores", "+", "attention_mask", "\n", "\n", "# Normalize the attention scores to probabilities.", "\n", "", "attention_probs", "=", "nn", ".", "Softmax", "(", "dim", "=", "-", "1", ")", "(", "attention_scores", ")", "\n", "\n", "# This is actually dropping out entire tokens to attend to, which might", "\n", "# seem a bit unusual, but is taken from the original Transformer paper.", "\n", "attention_probs", "=", "self", ".", "dropout", "(", "attention_probs", ")", "\n", "\n", "context_layer", "=", "torch", ".", "matmul", "(", "attention_probs", ",", "value_layer", ")", "\n", "\n", "context_layer", "=", "context_layer", ".", "permute", "(", "0", ",", "2", ",", "1", ",", "3", ")", ".", "contiguous", "(", ")", "\n", "new_context_layer_shape", "=", "context_layer", ".", "size", "(", ")", "[", ":", "-", "2", "]", "+", "(", "self", ".", "all_head_size", ",", ")", "\n", "context_layer", "=", "context_layer", ".", "view", "(", "*", "new_context_layer_shape", ")", "\n", "\n", "outputs", "=", "context_layer", "\n", "return", "outputs", "\n", "\n"]], "home.repos.pwc.inspect_result.neulab_awesome-align.awesome_align.modeling.BertSelfOutput.__init__": [[249, 254], ["torch.nn.Module.__init__", "torch.nn.Linear", "BertLayerNorm", "torch.nn.Dropout"], "methods", ["home.repos.pwc.inspect_result.neulab_awesome-align.awesome_align.train_utils.AdamW.__init__"], ["    ", "def", "__init__", "(", "self", ",", "config", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "dense", "=", "nn", ".", "Linear", "(", "config", ".", "hidden_size", ",", "config", ".", "hidden_size", ")", "\n", "self", ".", "LayerNorm", "=", "BertLayerNorm", "(", "config", ".", "hidden_size", ",", "eps", "=", "config", ".", "layer_norm_eps", ")", "\n", "self", ".", "dropout", "=", "nn", ".", "Dropout", "(", "config", ".", "hidden_dropout_prob", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.neulab_awesome-align.awesome_align.modeling.BertSelfOutput.forward": [[255, 260], ["modeling.BertSelfOutput.dense", "modeling.BertSelfOutput.dropout", "modeling.BertSelfOutput.LayerNorm"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "hidden_states", ",", "input_tensor", ")", ":", "\n", "        ", "hidden_states", "=", "self", ".", "dense", "(", "hidden_states", ")", "\n", "hidden_states", "=", "self", ".", "dropout", "(", "hidden_states", ")", "\n", "hidden_states", "=", "self", ".", "LayerNorm", "(", "hidden_states", "+", "input_tensor", ")", "\n", "return", "hidden_states", "\n", "\n"]], "home.repos.pwc.inspect_result.neulab_awesome-align.awesome_align.modeling.BertAttention.__init__": [[263, 267], ["torch.nn.Module.__init__", "modeling.BertSelfAttention", "modeling.BertSelfOutput"], "methods", ["home.repos.pwc.inspect_result.neulab_awesome-align.awesome_align.train_utils.AdamW.__init__"], ["    ", "def", "__init__", "(", "self", ",", "config", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "self", "=", "BertSelfAttention", "(", "config", ")", "\n", "self", ".", "output", "=", "BertSelfOutput", "(", "config", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.neulab_awesome-align.awesome_align.modeling.BertAttention.forward": [[268, 278], ["modeling.BertAttention.self", "modeling.BertAttention.output"], "methods", ["None"], ["", "def", "forward", "(", "\n", "self", ",", "\n", "hidden_states", ",", "\n", "attention_mask", "=", "None", ",", "\n", ")", ":", "\n", "        ", "self_outputs", "=", "self", ".", "self", "(", "\n", "hidden_states", ",", "attention_mask", "\n", ")", "\n", "outputs", "=", "self", ".", "output", "(", "self_outputs", ",", "hidden_states", ")", "\n", "return", "outputs", "\n", "\n"]], "home.repos.pwc.inspect_result.neulab_awesome-align.awesome_align.modeling.BertIntermediate.__init__": [[281, 288], ["torch.nn.Module.__init__", "torch.nn.Linear", "isinstance"], "methods", ["home.repos.pwc.inspect_result.neulab_awesome-align.awesome_align.train_utils.AdamW.__init__"], ["    ", "def", "__init__", "(", "self", ",", "config", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "dense", "=", "nn", ".", "Linear", "(", "config", ".", "hidden_size", ",", "config", ".", "intermediate_size", ")", "\n", "if", "isinstance", "(", "config", ".", "hidden_act", ",", "str", ")", ":", "\n", "            ", "self", ".", "intermediate_act_fn", "=", "ACT2FN", "[", "config", ".", "hidden_act", "]", "\n", "", "else", ":", "\n", "            ", "self", ".", "intermediate_act_fn", "=", "config", ".", "hidden_act", "\n", "\n"]], "home.repos.pwc.inspect_result.neulab_awesome-align.awesome_align.modeling.BertIntermediate.forward": [[289, 293], ["modeling.BertIntermediate.dense", "modeling.BertIntermediate.intermediate_act_fn"], "methods", ["None"], ["", "", "def", "forward", "(", "self", ",", "hidden_states", ")", ":", "\n", "        ", "hidden_states", "=", "self", ".", "dense", "(", "hidden_states", ")", "\n", "hidden_states", "=", "self", ".", "intermediate_act_fn", "(", "hidden_states", ")", "\n", "return", "hidden_states", "\n", "\n"]], "home.repos.pwc.inspect_result.neulab_awesome-align.awesome_align.modeling.BertOutput.__init__": [[296, 301], ["torch.nn.Module.__init__", "torch.nn.Linear", "BertLayerNorm", "torch.nn.Dropout"], "methods", ["home.repos.pwc.inspect_result.neulab_awesome-align.awesome_align.train_utils.AdamW.__init__"], ["    ", "def", "__init__", "(", "self", ",", "config", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "dense", "=", "nn", ".", "Linear", "(", "config", ".", "intermediate_size", ",", "config", ".", "hidden_size", ")", "\n", "self", ".", "LayerNorm", "=", "BertLayerNorm", "(", "config", ".", "hidden_size", ",", "eps", "=", "config", ".", "layer_norm_eps", ")", "\n", "self", ".", "dropout", "=", "nn", ".", "Dropout", "(", "config", ".", "hidden_dropout_prob", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.neulab_awesome-align.awesome_align.modeling.BertOutput.forward": [[302, 307], ["modeling.BertOutput.dense", "modeling.BertOutput.dropout", "modeling.BertOutput.LayerNorm"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "hidden_states", ",", "input_tensor", ")", ":", "\n", "        ", "hidden_states", "=", "self", ".", "dense", "(", "hidden_states", ")", "\n", "hidden_states", "=", "self", ".", "dropout", "(", "hidden_states", ")", "\n", "hidden_states", "=", "self", ".", "LayerNorm", "(", "hidden_states", "+", "input_tensor", ")", "\n", "return", "hidden_states", "\n", "\n"]], "home.repos.pwc.inspect_result.neulab_awesome-align.awesome_align.modeling.BertLayer.__init__": [[309, 314], ["torch.nn.Module.__init__", "modeling.BertAttention", "modeling.BertIntermediate", "modeling.BertOutput"], "methods", ["home.repos.pwc.inspect_result.neulab_awesome-align.awesome_align.train_utils.AdamW.__init__"], ["    ", "def", "__init__", "(", "self", ",", "config", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "attention", "=", "BertAttention", "(", "config", ")", "\n", "self", ".", "intermediate", "=", "BertIntermediate", "(", "config", ")", "\n", "self", ".", "output", "=", "BertOutput", "(", "config", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.neulab_awesome-align.awesome_align.modeling.BertLayer.forward": [[316, 326], ["modeling.BertLayer.attention", "modeling.BertLayer.intermediate", "modeling.BertLayer.output"], "methods", ["None"], ["", "def", "forward", "(", "\n", "self", ",", "\n", "hidden_states", ",", "\n", "attention_mask", "=", "None", ",", "\n", ")", ":", "\n", "        ", "attention_outputs", "=", "self", ".", "attention", "(", "hidden_states", ",", "attention_mask", ")", "\n", "intermediate_output", "=", "self", ".", "intermediate", "(", "attention_outputs", ")", "\n", "layer_output", "=", "self", ".", "output", "(", "intermediate_output", ",", "attention_outputs", ")", "\n", "\n", "return", "layer_output", "\n", "\n"]], "home.repos.pwc.inspect_result.neulab_awesome-align.awesome_align.modeling.BertEncoder.__init__": [[329, 334], ["torch.nn.Module.__init__", "torch.nn.ModuleList", "modeling.BertLayer", "range"], "methods", ["home.repos.pwc.inspect_result.neulab_awesome-align.awesome_align.train_utils.AdamW.__init__"], ["    ", "def", "__init__", "(", "self", ",", "config", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "output_hidden_states", "=", "config", ".", "output_hidden_states", "\n", "\n", "self", ".", "layer", "=", "nn", ".", "ModuleList", "(", "[", "BertLayer", "(", "config", ")", "for", "_", "in", "range", "(", "config", ".", "num_hidden_layers", ")", "]", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.neulab_awesome-align.awesome_align.modeling.BertEncoder.forward": [[335, 350], ["enumerate", "layer_module"], "methods", ["None"], ["", "def", "forward", "(", "\n", "self", ",", "\n", "hidden_states", ",", "\n", "align_layer", ",", "\n", "attention_mask", "=", "None", ",", "\n", ")", ":", "\n", "        ", "for", "i", ",", "layer_module", "in", "enumerate", "(", "self", ".", "layer", ")", ":", "\n", "            ", "layer_outputs", "=", "layer_module", "(", "\n", "hidden_states", ",", "attention_mask", "\n", ")", "\n", "hidden_states", "=", "layer_outputs", "\n", "if", "i", "==", "align_layer", "-", "1", ":", "\n", "                ", "break", "\n", "\n", "", "", "return", "hidden_states", "\n", "\n"]], "home.repos.pwc.inspect_result.neulab_awesome-align.awesome_align.modeling.BertPredictionHeadTransform.__init__": [[352, 360], ["torch.nn.Module.__init__", "torch.nn.Linear", "isinstance", "BertLayerNorm"], "methods", ["home.repos.pwc.inspect_result.neulab_awesome-align.awesome_align.train_utils.AdamW.__init__"], ["    ", "def", "__init__", "(", "self", ",", "config", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "dense", "=", "nn", ".", "Linear", "(", "config", ".", "hidden_size", ",", "config", ".", "hidden_size", ")", "\n", "if", "isinstance", "(", "config", ".", "hidden_act", ",", "str", ")", ":", "\n", "            ", "self", ".", "transform_act_fn", "=", "ACT2FN", "[", "config", ".", "hidden_act", "]", "\n", "", "else", ":", "\n", "            ", "self", ".", "transform_act_fn", "=", "config", ".", "hidden_act", "\n", "", "self", ".", "LayerNorm", "=", "BertLayerNorm", "(", "config", ".", "hidden_size", ",", "eps", "=", "config", ".", "layer_norm_eps", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.neulab_awesome-align.awesome_align.modeling.BertPredictionHeadTransform.forward": [[361, 366], ["modeling.BertPredictionHeadTransform.dense", "modeling.BertPredictionHeadTransform.transform_act_fn", "modeling.BertPredictionHeadTransform.LayerNorm"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "hidden_states", ")", ":", "\n", "        ", "hidden_states", "=", "self", ".", "dense", "(", "hidden_states", ")", "\n", "hidden_states", "=", "self", ".", "transform_act_fn", "(", "hidden_states", ")", "\n", "hidden_states", "=", "self", ".", "LayerNorm", "(", "hidden_states", ")", "\n", "return", "hidden_states", "\n", "\n"]], "home.repos.pwc.inspect_result.neulab_awesome-align.awesome_align.modeling.BertLMPredictionHead.__init__": [[368, 380], ["torch.nn.Module.__init__", "modeling.BertPredictionHeadTransform", "torch.nn.Linear", "torch.nn.Parameter", "torch.zeros"], "methods", ["home.repos.pwc.inspect_result.neulab_awesome-align.awesome_align.train_utils.AdamW.__init__"], ["    ", "def", "__init__", "(", "self", ",", "config", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "transform", "=", "BertPredictionHeadTransform", "(", "config", ")", "\n", "\n", "# The output weights are the same as the input embeddings, but there is", "\n", "# an output-only bias for each token.", "\n", "self", ".", "decoder", "=", "nn", ".", "Linear", "(", "config", ".", "hidden_size", ",", "config", ".", "vocab_size", ",", "bias", "=", "False", ")", "\n", "\n", "self", ".", "bias", "=", "nn", ".", "Parameter", "(", "torch", ".", "zeros", "(", "config", ".", "vocab_size", ")", ")", "\n", "\n", "# Need a link between the two variables so that the bias is correctly resized with `resize_token_embeddings`", "\n", "self", ".", "decoder", ".", "bias", "=", "self", ".", "bias", "\n", "\n"]], "home.repos.pwc.inspect_result.neulab_awesome-align.awesome_align.modeling.BertLMPredictionHead.forward": [[381, 385], ["modeling.BertLMPredictionHead.transform", "modeling.BertLMPredictionHead.decoder"], "methods", ["home.repos.pwc.inspect_result.neulab_awesome-align.awesome_align.tokenization_utils.PreTrainedTokenizerFast.decoder"], ["", "def", "forward", "(", "self", ",", "hidden_states", ")", ":", "\n", "        ", "hidden_states", "=", "self", ".", "transform", "(", "hidden_states", ")", "\n", "hidden_states", "=", "self", ".", "decoder", "(", "hidden_states", ")", "\n", "return", "hidden_states", "\n", "\n"]], "home.repos.pwc.inspect_result.neulab_awesome-align.awesome_align.modeling.BertMLMHead.__init__": [[387, 390], ["torch.nn.Module.__init__", "modeling.BertLMPredictionHead"], "methods", ["home.repos.pwc.inspect_result.neulab_awesome-align.awesome_align.train_utils.AdamW.__init__"], ["    ", "def", "__init__", "(", "self", ",", "config", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "predictions", "=", "BertLMPredictionHead", "(", "config", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.neulab_awesome-align.awesome_align.modeling.BertMLMHead.forward": [[391, 394], ["modeling.BertMLMHead.predictions"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "sequence_output", ")", ":", "\n", "        ", "prediction_scores", "=", "self", ".", "predictions", "(", "sequence_output", ")", "\n", "return", "prediction_scores", "\n", "\n"]], "home.repos.pwc.inspect_result.neulab_awesome-align.awesome_align.modeling.BertPSIHead.__init__": [[396, 409], ["torch.nn.Module.__init__", "torch.nn.Linear", "torch.nn.Tanh", "torch.nn.Linear", "torch.nn.Parameter", "torch.zeros"], "methods", ["home.repos.pwc.inspect_result.neulab_awesome-align.awesome_align.train_utils.AdamW.__init__"], ["    ", "def", "__init__", "(", "self", ",", "config", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "\n", "self", ".", "transform", "=", "nn", ".", "Linear", "(", "config", ".", "hidden_size", ",", "config", ".", "hidden_size", ")", "\n", "self", ".", "activation", "=", "nn", ".", "Tanh", "(", ")", "\n", "# The output weights are the same as the input embeddings, but there is", "\n", "# an output-only bias for each token.", "\n", "self", ".", "decoder", "=", "nn", ".", "Linear", "(", "config", ".", "hidden_size", ",", "2", ",", "bias", "=", "False", ")", "\n", "\n", "self", ".", "bias", "=", "nn", ".", "Parameter", "(", "torch", ".", "zeros", "(", "2", ")", ")", "\n", "\n", "# Need a link between the two variables so that the bias is correctly resized with `resize_token_embeddings`", "\n", "self", ".", "decoder", ".", "bias", "=", "self", ".", "bias", "\n", "\n"]], "home.repos.pwc.inspect_result.neulab_awesome-align.awesome_align.modeling.BertPSIHead.forward": [[410, 416], ["modeling.BertPSIHead.transform", "modeling.BertPSIHead.activation", "modeling.BertPSIHead.decoder"], "methods", ["home.repos.pwc.inspect_result.neulab_awesome-align.awesome_align.tokenization_utils.PreTrainedTokenizerFast.decoder"], ["", "def", "forward", "(", "self", ",", "hidden_states", ")", ":", "\n", "        ", "hidden_states", "=", "hidden_states", "[", ":", ",", "0", "]", "\n", "hidden_states", "=", "self", ".", "transform", "(", "hidden_states", ")", "\n", "hidden_states", "=", "self", ".", "activation", "(", "hidden_states", ")", "\n", "hidden_states", "=", "self", ".", "decoder", "(", "hidden_states", ")", "\n", "return", "hidden_states", "\n", "\n"]], "home.repos.pwc.inspect_result.neulab_awesome-align.awesome_align.modeling.BertPreTrainedModel._init_weights": [[428, 439], ["isinstance", "module.weight.data.normal_", "isinstance", "isinstance", "module.bias.data.zero_", "module.bias.data.zero_", "module.weight.data.fill_"], "methods", ["None"], ["def", "_init_weights", "(", "self", ",", "module", ")", ":", "\n", "        ", "\"\"\" Initialize the weights \"\"\"", "\n", "if", "isinstance", "(", "module", ",", "(", "nn", ".", "Linear", ",", "nn", ".", "Embedding", ")", ")", ":", "\n", "# Slightly different from the TF version which uses truncated_normal for initialization", "\n", "# cf https://github.com/pytorch/pytorch/pull/5617", "\n", "            ", "module", ".", "weight", ".", "data", ".", "normal_", "(", "mean", "=", "0.0", ",", "std", "=", "self", ".", "config", ".", "initializer_range", ")", "\n", "", "elif", "isinstance", "(", "module", ",", "BertLayerNorm", ")", ":", "\n", "            ", "module", ".", "bias", ".", "data", ".", "zero_", "(", ")", "\n", "module", ".", "weight", ".", "data", ".", "fill_", "(", "1.0", ")", "\n", "", "if", "isinstance", "(", "module", ",", "nn", ".", "Linear", ")", "and", "module", ".", "bias", "is", "not", "None", ":", "\n", "            ", "module", ".", "bias", ".", "data", ".", "zero_", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.neulab_awesome-align.awesome_align.modeling.BertModel.__init__": [[481, 489], ["awesome_align.modeling_utils.PreTrainedModel.__init__", "modeling.BertEmbeddings", "modeling.BertEncoder", "modeling.BertModel.init_weights"], "methods", ["home.repos.pwc.inspect_result.neulab_awesome-align.awesome_align.train_utils.AdamW.__init__", "home.repos.pwc.inspect_result.neulab_awesome-align.awesome_align.modeling_utils.PreTrainedModel.init_weights"], ["def", "__init__", "(", "self", ",", "config", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "config", ")", "\n", "self", ".", "config", "=", "config", "\n", "\n", "self", ".", "embeddings", "=", "BertEmbeddings", "(", "config", ")", "\n", "self", ".", "encoder", "=", "BertEncoder", "(", "config", ")", "\n", "\n", "self", ".", "init_weights", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.neulab_awesome-align.awesome_align.modeling.BertModel.get_input_embeddings": [[490, 492], ["None"], "methods", ["None"], ["", "def", "get_input_embeddings", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "embeddings", ".", "word_embeddings", "\n", "\n"]], "home.repos.pwc.inspect_result.neulab_awesome-align.awesome_align.modeling.BertModel.set_input_embeddings": [[493, 495], ["None"], "methods", ["None"], ["", "def", "set_input_embeddings", "(", "self", ",", "value", ")", ":", "\n", "        ", "self", ".", "embeddings", ".", "word_embeddings", "=", "value", "\n", "\n"]], "home.repos.pwc.inspect_result.neulab_awesome-align.awesome_align.modeling.BertModel.get_parameter_dtype": [[496, 509], ["next", "modeling.BertModel._named_members", "next", "modeling.BertModel.parameters", "module.__dict__.items", "torch.is_tensor"], "methods", ["None"], ["", "def", "get_parameter_dtype", "(", "self", ")", ":", "\n", "        ", "try", ":", "\n", "            ", "return", "next", "(", "self", ".", "parameters", "(", ")", ")", ".", "dtype", "\n", "", "except", "StopIteration", ":", "\n", "# For nn.DataParallel compatibility in PyTorch 1.5", "\n", "\n", "            ", "def", "find_tensor_attributes", "(", "module", ")", ":", "\n", "                ", "tuples", "=", "[", "(", "k", ",", "v", ")", "for", "k", ",", "v", "in", "module", ".", "__dict__", ".", "items", "(", ")", "if", "torch", ".", "is_tensor", "(", "v", ")", "]", "\n", "return", "tuples", "\n", "\n", "", "gen", "=", "self", ".", "_named_members", "(", "get_members_fn", "=", "find_tensor_attributes", ")", "\n", "first_tuple", "=", "next", "(", "gen", ")", "\n", "return", "first_tuple", "[", "1", "]", ".", "dtype", "\n", "\n"]], "home.repos.pwc.inspect_result.neulab_awesome-align.awesome_align.modeling.BertModel.forward": [[510, 541], ["awesome_align.file_utils.add_start_docstrings_to_callable", "input_ids.size", "torch.zeros", "modeling.return_extended_attention_mask", "modeling.BertModel.embeddings", "modeling.BertModel.encoder", "torch.ones", "modeling.BertModel.get_parameter_dtype"], "methods", ["home.repos.pwc.inspect_result.neulab_awesome-align.awesome_align.file_utils.add_start_docstrings_to_callable", "home.repos.pwc.inspect_result.neulab_awesome-align.awesome_align.modeling.return_extended_attention_mask", "home.repos.pwc.inspect_result.neulab_awesome-align.awesome_align.modeling.BertModel.get_parameter_dtype"], ["", "", "@", "add_start_docstrings_to_callable", "(", "BERT_INPUTS_DOCSTRING", ")", "\n", "def", "forward", "(", "\n", "self", ",", "\n", "input_ids", "=", "None", ",", "\n", "align_layer", "=", "-", "1", ",", "\n", "attention_mask", "=", "None", ",", "\n", "position_ids", "=", "None", ",", "\n", ")", ":", "\n", "        ", "input_shape", "=", "input_ids", ".", "size", "(", ")", "\n", "\n", "device", "=", "input_ids", ".", "device", "\n", "\n", "if", "attention_mask", "is", "None", ":", "\n", "            ", "attention_mask", "=", "torch", ".", "ones", "(", "input_shape", ",", "device", "=", "device", ")", "\n", "attention_mask", "[", "input_ids", "==", "PAD_ID", "]", "=", "0", "\n", "\n", "", "token_type_ids", "=", "torch", ".", "zeros", "(", "input_shape", ",", "dtype", "=", "torch", ".", "long", ",", "device", "=", "device", ")", "\n", "\n", "# We can provide a self-attention mask of dimensions [batch_size, from_seq_length, to_seq_length]", "\n", "# ourselves in which case we just need to make it broadcastable to all heads.", "\n", "extended_attention_mask", "=", "return_extended_attention_mask", "(", "attention_mask", ",", "self", ".", "get_parameter_dtype", "(", ")", ")", "\n", "\n", "embedding_output", "=", "self", ".", "embeddings", "(", "\n", "input_ids", "=", "input_ids", ",", "token_type_ids", "=", "token_type_ids", ",", "position_ids", "=", "position_ids", ",", "\n", ")", "\n", "encoder_outputs", "=", "self", ".", "encoder", "(", "\n", "embedding_output", ",", "\n", "attention_mask", "=", "extended_attention_mask", ",", "\n", "align_layer", "=", "align_layer", ",", "\n", ")", "\n", "return", "encoder_outputs", "\n", "\n"]], "home.repos.pwc.inspect_result.neulab_awesome-align.awesome_align.modeling.BertGuideHead.__init__": [[543, 545], ["torch.nn.Module.__init__"], "methods", ["home.repos.pwc.inspect_result.neulab_awesome-align.awesome_align.train_utils.AdamW.__init__"], ["    ", "def", "__init__", "(", "self", ",", "config", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.neulab_awesome-align.awesome_align.modeling.BertGuideHead.transpose_for_scores": [[546, 550], ["x.view.view.view", "x.view.view.permute", "x.view.view.size", "x.view.view.size"], "methods", ["None"], ["", "def", "transpose_for_scores", "(", "self", ",", "x", ")", ":", "\n", "        ", "new_x_shape", "=", "x", ".", "size", "(", ")", "[", ":", "-", "1", "]", "+", "(", "1", ",", "x", ".", "size", "(", "-", "1", ")", ")", "\n", "x", "=", "x", ".", "view", "(", "*", "new_x_shape", ")", "\n", "return", "x", ".", "permute", "(", "0", ",", "2", ",", "1", ",", "3", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.neulab_awesome-align.awesome_align.modeling.BertGuideHead.forward": [[551, 611], ["torch.sum", "torch.sum", "modeling.return_extended_attention_mask", "modeling.return_extended_attention_mask", "modeling.BertGuideHead.transpose_for_scores", "modeling.BertGuideHead.transpose_for_scores", "torch.matmul", "key_tgt.transpose", "return_extended_attention_mask.transpose", "awesome_align.sparsemax.entmax15", "awesome_align.sparsemax.entmax15", "torch.sum().view", "torch.sum().view", "torch.min", "torch.matmul().squeeze", "torch.einsum", "torch.nn.Softmax", "torch.nn.Softmax", "torch.nn.Softmax", "torch.nn.Softmax", "torch.mean", "torch.mean", "torch.sqrt", "torch.sqrt", "torch.sum", "torch.sum", "torch.matmul", "torch.sum.view", "torch.sum.view", "torch.sum", "torch.sum", "attention_probs_tgt.transpose"], "methods", ["home.repos.pwc.inspect_result.neulab_awesome-align.awesome_align.modeling.return_extended_attention_mask", "home.repos.pwc.inspect_result.neulab_awesome-align.awesome_align.modeling.return_extended_attention_mask", "home.repos.pwc.inspect_result.neulab_awesome-align.awesome_align.modeling.BertGuideHead.transpose_for_scores", "home.repos.pwc.inspect_result.neulab_awesome-align.awesome_align.modeling.BertGuideHead.transpose_for_scores", "home.repos.pwc.inspect_result.neulab_awesome-align.awesome_align.sparsemax.entmax15", "home.repos.pwc.inspect_result.neulab_awesome-align.awesome_align.sparsemax.entmax15"], ["", "def", "forward", "(", "\n", "self", ",", "\n", "hidden_states_src", ",", "hidden_states_tgt", ",", "\n", "inputs_src", ",", "inputs_tgt", ",", "\n", "guide", "=", "None", ",", "\n", "extraction", "=", "'softmax'", ",", "softmax_threshold", "=", "0.001", ",", "\n", "train_so", "=", "True", ",", "train_co", "=", "False", ",", "\n", "output_prob", "=", "False", ",", "\n", ")", ":", "\n", "#mask", "\n", "        ", "attention_mask_src", "=", "(", "(", "inputs_src", "==", "PAD_ID", ")", "+", "(", "inputs_src", "==", "CLS_ID", ")", "+", "(", "inputs_src", "==", "SEP_ID", ")", ")", ".", "float", "(", ")", "\n", "attention_mask_tgt", "=", "(", "(", "inputs_tgt", "==", "PAD_ID", ")", "+", "(", "inputs_tgt", "==", "CLS_ID", ")", "+", "(", "inputs_tgt", "==", "SEP_ID", ")", ")", ".", "float", "(", ")", "\n", "len_src", "=", "torch", ".", "sum", "(", "1", "-", "attention_mask_src", ",", "-", "1", ")", "\n", "len_tgt", "=", "torch", ".", "sum", "(", "1", "-", "attention_mask_tgt", ",", "-", "1", ")", "\n", "attention_mask_src", "=", "return_extended_attention_mask", "(", "1", "-", "attention_mask_src", ",", "hidden_states_src", ".", "dtype", ")", "\n", "attention_mask_tgt", "=", "return_extended_attention_mask", "(", "1", "-", "attention_mask_tgt", ",", "hidden_states_src", ".", "dtype", ")", "\n", "\n", "#qkv", "\n", "query_src", "=", "self", ".", "transpose_for_scores", "(", "hidden_states_src", ")", "\n", "query_tgt", "=", "self", ".", "transpose_for_scores", "(", "hidden_states_tgt", ")", "\n", "key_src", "=", "query_src", "\n", "key_tgt", "=", "query_tgt", "\n", "value_src", "=", "query_src", "\n", "value_tgt", "=", "query_tgt", "\n", "\n", "#att", "\n", "attention_scores", "=", "torch", ".", "matmul", "(", "query_src", ",", "key_tgt", ".", "transpose", "(", "-", "1", ",", "-", "2", ")", ")", "\n", "attention_scores_src", "=", "attention_scores", "+", "attention_mask_tgt", "\n", "attention_scores_tgt", "=", "attention_scores", "+", "attention_mask_src", ".", "transpose", "(", "-", "1", ",", "-", "2", ")", "\n", "\n", "attention_probs_src", "=", "nn", ".", "Softmax", "(", "dim", "=", "-", "1", ")", "(", "attention_scores_src", ")", "if", "extraction", "==", "'softmax'", "else", "entmax15", "(", "attention_scores_src", ",", "dim", "=", "-", "1", ")", "\n", "attention_probs_tgt", "=", "nn", ".", "Softmax", "(", "dim", "=", "-", "2", ")", "(", "attention_scores_tgt", ")", "if", "extraction", "==", "'softmax'", "else", "entmax15", "(", "attention_scores_tgt", ",", "dim", "=", "-", "2", ")", "\n", "\n", "if", "guide", "is", "None", ":", "\n", "            ", "threshold", "=", "softmax_threshold", "if", "extraction", "==", "'softmax'", "else", "0", "\n", "align_matrix", "=", "(", "attention_probs_src", ">", "threshold", ")", "*", "(", "attention_probs_tgt", ">", "threshold", ")", "\n", "if", "not", "output_prob", ":", "\n", "                ", "return", "align_matrix", "\n", "# A heuristic of generating the alignment probability", "\n", "", "attention_probs_src", "=", "nn", ".", "Softmax", "(", "dim", "=", "-", "1", ")", "(", "attention_scores_src", "/", "torch", ".", "sqrt", "(", "len_tgt", ".", "view", "(", "-", "1", ",", "1", ",", "1", ",", "1", ")", ")", ")", "\n", "attention_probs_tgt", "=", "nn", ".", "Softmax", "(", "dim", "=", "-", "2", ")", "(", "attention_scores_tgt", "/", "torch", ".", "sqrt", "(", "len_src", ".", "view", "(", "-", "1", ",", "1", ",", "1", ",", "1", ")", ")", ")", "\n", "align_prob", "=", "(", "2", "*", "attention_probs_src", "*", "attention_probs_tgt", ")", "/", "(", "attention_probs_src", "+", "attention_probs_tgt", "+", "1e-9", ")", "\n", "return", "align_matrix", ",", "align_prob", "\n", "\n", "", "so_loss", "=", "0", "\n", "if", "train_so", ":", "\n", "            ", "so_loss_src", "=", "torch", ".", "sum", "(", "torch", ".", "sum", "(", "attention_probs_src", "*", "guide", ",", "-", "1", ")", ",", "-", "1", ")", ".", "view", "(", "-", "1", ")", "\n", "so_loss_tgt", "=", "torch", ".", "sum", "(", "torch", ".", "sum", "(", "attention_probs_tgt", "*", "guide", ",", "-", "1", ")", ",", "-", "1", ")", ".", "view", "(", "-", "1", ")", "\n", "\n", "so_loss", "=", "so_loss_src", "/", "len_src", "+", "so_loss_tgt", "/", "len_tgt", "\n", "so_loss", "=", "-", "torch", ".", "mean", "(", "so_loss", ")", "\n", "\n", "", "co_loss", "=", "0", "\n", "if", "train_co", ":", "\n", "            ", "min_len", "=", "torch", ".", "min", "(", "len_src", ",", "len_tgt", ")", "\n", "trace", "=", "torch", ".", "matmul", "(", "attention_probs_src", ",", "(", "attention_probs_tgt", ")", ".", "transpose", "(", "-", "1", ",", "-", "2", ")", ")", ".", "squeeze", "(", "1", ")", "\n", "trace", "=", "torch", ".", "einsum", "(", "'bii->b'", ",", "trace", ")", "\n", "co_loss", "=", "-", "torch", ".", "mean", "(", "trace", "/", "min_len", ")", "\n", "\n", "", "return", "so_loss", "+", "co_loss", "\n", "\n"]], "home.repos.pwc.inspect_result.neulab_awesome-align.awesome_align.modeling.BertForMaskedLM.__init__": [[615, 622], ["awesome_align.modeling_utils.PreTrainedModel.__init__", "modeling.BertModel", "modeling.BertMLMHead", "modeling.BertPSIHead", "modeling.BertGuideHead", "modeling.BertForMaskedLM.init_weights"], "methods", ["home.repos.pwc.inspect_result.neulab_awesome-align.awesome_align.train_utils.AdamW.__init__", "home.repos.pwc.inspect_result.neulab_awesome-align.awesome_align.modeling_utils.PreTrainedModel.init_weights"], ["    ", "def", "__init__", "(", "self", ",", "config", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "config", ")", "\n", "self", ".", "bert", "=", "BertModel", "(", "config", ")", "\n", "self", ".", "cls", "=", "BertMLMHead", "(", "config", ")", "\n", "self", ".", "psi_cls", "=", "BertPSIHead", "(", "config", ")", "\n", "self", ".", "guide_layer", "=", "BertGuideHead", "(", "config", ")", "\n", "self", ".", "init_weights", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.neulab_awesome-align.awesome_align.modeling.BertForMaskedLM.forward": [[623, 677], ["awesome_align.file_utils.add_start_docstrings_to_callable", "torch.nn.CrossEntropyLoss", "inputs_src.size", "modeling.BertForMaskedLM.bert", "modeling.BertForMaskedLM.bert", "modeling.BertForMaskedLM.guide_layer", "modeling.BertForMaskedLM.psi_cls", "torch.nn.CrossEntropyLoss.", "torch.mean", "modeling.BertForMaskedLM.cls", "torch.nn.CrossEntropyLoss.", "torch.mean", "ValueError", "modeling.BertForMaskedLM.view", "labels_psi.view", "modeling.BertForMaskedLM.view", "labels_src.view", "torch.sum", "torch.mean.view", "torch.sum().float", "torch.sum", "labels_src.view"], "methods", ["home.repos.pwc.inspect_result.neulab_awesome-align.awesome_align.file_utils.add_start_docstrings_to_callable"], ["", "@", "add_start_docstrings_to_callable", "(", "BERT_INPUTS_DOCSTRING", ")", "\n", "def", "forward", "(", "\n", "self", ",", "\n", "inputs_src", ",", "\n", "inputs_tgt", "=", "None", ",", "\n", "labels_src", "=", "None", ",", "\n", "labels_tgt", "=", "None", ",", "\n", "attention_mask_src", "=", "None", ",", "\n", "attention_mask_tgt", "=", "None", ",", "\n", "align_layer", "=", "-", "1", ",", "\n", "guide", "=", "None", ",", "\n", "extraction", "=", "'softmax'", ",", "softmax_threshold", "=", "0.001", ",", "\n", "position_ids1", "=", "None", ",", "\n", "position_ids2", "=", "None", ",", "\n", "labels_psi", "=", "None", ",", "\n", "train_so", "=", "True", ",", "\n", "train_co", "=", "False", ",", "\n", ")", ":", "\n", "\n", "        ", "loss_fct", "=", "CrossEntropyLoss", "(", "reduction", "=", "'none'", ")", "\n", "batch_size", "=", "inputs_src", ".", "size", "(", "0", ")", "\n", "\n", "outputs_src", "=", "self", ".", "bert", "(", "\n", "inputs_src", ",", "\n", "attention_mask", "=", "attention_mask_src", ",", "\n", "align_layer", "=", "align_layer", ",", "\n", "position_ids", "=", "position_ids1", ",", "\n", ")", "\n", "\n", "if", "labels_psi", "is", "not", "None", ":", "\n", "            ", "prediction_scores_psi", "=", "self", ".", "psi_cls", "(", "outputs_src", ")", "\n", "psi_loss", "=", "loss_fct", "(", "prediction_scores_psi", ".", "view", "(", "-", "1", ",", "2", ")", ",", "labels_psi", ".", "view", "(", "-", "1", ")", ")", "\n", "psi_loss", "=", "torch", ".", "mean", "(", "psi_loss", ")", "\n", "return", "psi_loss", "\n", "\n", "", "if", "inputs_tgt", "is", "None", ":", "\n", "            ", "prediction_scores_src", "=", "self", ".", "cls", "(", "outputs_src", ")", "\n", "masked_lm_loss", "=", "loss_fct", "(", "prediction_scores_src", ".", "view", "(", "-", "1", ",", "self", ".", "config", ".", "vocab_size", ")", ",", "labels_src", ".", "view", "(", "-", "1", ")", ")", "\n", "masked_lm_loss", "=", "torch", ".", "sum", "(", "masked_lm_loss", ".", "view", "(", "batch_size", ",", "-", "1", ")", ",", "-", "1", ")", "/", "(", "torch", ".", "sum", "(", "labels_src", ".", "view", "(", "batch_size", ",", "-", "1", ")", "!=", "-", "100", ",", "-", "1", ")", ".", "float", "(", ")", "+", "1e-9", ")", "#ignore_index=-100", "\n", "masked_lm_loss", "=", "torch", ".", "mean", "(", "masked_lm_loss", ")", "\n", "return", "masked_lm_loss", "\n", "\n", "", "if", "guide", "is", "None", ":", "\n", "            ", "raise", "ValueError", "(", "'must specify labels for the self-trianing objective'", ")", "\n", "\n", "", "outputs_tgt", "=", "self", ".", "bert", "(", "\n", "inputs_tgt", ",", "\n", "attention_mask", "=", "attention_mask_tgt", ",", "\n", "align_layer", "=", "align_layer", ",", "\n", "position_ids", "=", "position_ids2", ",", "\n", ")", "\n", "\n", "sco_loss", "=", "self", ".", "guide_layer", "(", "outputs_src", ",", "outputs_tgt", ",", "inputs_src", ",", "inputs_tgt", ",", "guide", "=", "guide", ",", "extraction", "=", "extraction", ",", "softmax_threshold", "=", "softmax_threshold", ",", "train_so", "=", "train_so", ",", "train_co", "=", "train_co", ")", "\n", "return", "sco_loss", "\n", "\n"]], "home.repos.pwc.inspect_result.neulab_awesome-align.awesome_align.modeling.BertForMaskedLM.get_aligned_word": [[678, 733], ["inputs_src.to().clone.to().clone.size", "torch.zeros", "enumerate", "inputs_src.to().clone.to().clone.to().clone", "inputs_tgt.to().clone.to().clone.to().clone", "enumerate", "zip", "min", "min", "range", "inputs_src.to().clone.to().clone.size", "inputs_tgt.to().clone.to().clone.size", "torch.no_grad", "modeling.BertForMaskedLM.bert", "modeling.BertForMaskedLM.bert", "modeling.BertForMaskedLM.guide_layer", "attention_probs_inter.float.float.float", "zip", "torch.nonzero", "word_aligns.append", "len", "len", "range", "inputs_src.to().clone.to().clone.to", "inputs_tgt.to().clone.to().clone.to", "set", "dict", "aligns.add", "max"], "methods", ["home.repos.pwc.inspect_result.neulab_awesome-align.awesome_align.modeling_utils.BeamHypotheses.add"], ["", "def", "get_aligned_word", "(", "self", ",", "inputs_src", ",", "inputs_tgt", ",", "bpe2word_map_src", ",", "bpe2word_map_tgt", ",", "device", ",", "src_len", ",", "tgt_len", ",", "align_layer", "=", "8", ",", "extraction", "=", "'softmax'", ",", "softmax_threshold", "=", "0.001", ",", "test", "=", "False", ",", "output_prob", "=", "False", ",", "word_aligns", "=", "None", ")", ":", "\n", "        ", "batch_size", "=", "inputs_src", ".", "size", "(", "0", ")", "\n", "bpelen_src", ",", "bpelen_tgt", "=", "inputs_src", ".", "size", "(", "1", ")", "-", "2", ",", "inputs_tgt", ".", "size", "(", "1", ")", "-", "2", "\n", "if", "word_aligns", "is", "None", ":", "\n", "            ", "inputs_src", "=", "inputs_src", ".", "to", "(", "dtype", "=", "torch", ".", "long", ",", "device", "=", "device", ")", ".", "clone", "(", ")", "\n", "inputs_tgt", "=", "inputs_tgt", ".", "to", "(", "dtype", "=", "torch", ".", "long", ",", "device", "=", "device", ")", ".", "clone", "(", ")", "\n", "\n", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "                ", "outputs_src", "=", "self", ".", "bert", "(", "\n", "inputs_src", ",", "\n", "align_layer", "=", "align_layer", ",", "\n", "attention_mask", "=", "(", "inputs_src", "!=", "PAD_ID", ")", ",", "\n", ")", "\n", "outputs_tgt", "=", "self", ".", "bert", "(", "\n", "inputs_tgt", ",", "\n", "align_layer", "=", "align_layer", ",", "\n", "attention_mask", "=", "(", "inputs_tgt", "!=", "PAD_ID", ")", ",", "\n", ")", "\n", "\n", "attention_probs_inter", "=", "self", ".", "guide_layer", "(", "outputs_src", ",", "outputs_tgt", ",", "inputs_src", ",", "inputs_tgt", ",", "extraction", "=", "extraction", ",", "softmax_threshold", "=", "softmax_threshold", ",", "output_prob", "=", "output_prob", ")", "\n", "if", "output_prob", ":", "\n", "                    ", "attention_probs_inter", ",", "alignment_probs", "=", "attention_probs_inter", "\n", "alignment_probs", "=", "alignment_probs", "[", ":", ",", "0", ",", "1", ":", "-", "1", ",", "1", ":", "-", "1", "]", "\n", "", "attention_probs_inter", "=", "attention_probs_inter", ".", "float", "(", ")", "\n", "\n", "", "word_aligns", "=", "[", "]", "\n", "attention_probs_inter", "=", "attention_probs_inter", "[", ":", ",", "0", ",", "1", ":", "-", "1", ",", "1", ":", "-", "1", "]", "\n", "\n", "for", "idx", ",", "(", "attention", ",", "b2w_src", ",", "b2w_tgt", ")", "in", "enumerate", "(", "zip", "(", "attention_probs_inter", ",", "bpe2word_map_src", ",", "bpe2word_map_tgt", ")", ")", ":", "\n", "                ", "aligns", "=", "set", "(", ")", "if", "not", "output_prob", "else", "dict", "(", ")", "\n", "non_zeros", "=", "torch", ".", "nonzero", "(", "attention", ")", "\n", "for", "i", ",", "j", "in", "non_zeros", ":", "\n", "                    ", "word_pair", "=", "(", "b2w_src", "[", "i", "]", ",", "b2w_tgt", "[", "j", "]", ")", "\n", "if", "output_prob", ":", "\n", "                        ", "prob", "=", "alignment_probs", "[", "idx", ",", "i", ",", "j", "]", "\n", "if", "not", "word_pair", "in", "aligns", ":", "\n", "                            ", "aligns", "[", "word_pair", "]", "=", "prob", "\n", "", "else", ":", "\n", "                            ", "aligns", "[", "word_pair", "]", "=", "max", "(", "aligns", "[", "word_pair", "]", ",", "prob", ")", "\n", "", "", "else", ":", "\n", "                        ", "aligns", ".", "add", "(", "word_pair", ")", "\n", "", "", "word_aligns", ".", "append", "(", "aligns", ")", "\n", "\n", "", "", "if", "test", ":", "\n", "            ", "return", "word_aligns", "\n", "\n", "", "guide", "=", "torch", ".", "zeros", "(", "batch_size", ",", "1", ",", "src_len", ",", "tgt_len", ")", "\n", "for", "idx", ",", "(", "word_align", ",", "b2w_src", ",", "b2w_tgt", ")", "in", "enumerate", "(", "zip", "(", "word_aligns", ",", "bpe2word_map_src", ",", "bpe2word_map_tgt", ")", ")", ":", "\n", "            ", "len_src", "=", "min", "(", "bpelen_src", ",", "len", "(", "b2w_src", ")", ")", "\n", "len_tgt", "=", "min", "(", "bpelen_tgt", ",", "len", "(", "b2w_tgt", ")", ")", "\n", "for", "i", "in", "range", "(", "len_src", ")", ":", "\n", "                ", "for", "j", "in", "range", "(", "len_tgt", ")", ":", "\n", "                    ", "if", "(", "b2w_src", "[", "i", "]", ",", "b2w_tgt", "[", "j", "]", ")", "in", "word_align", ":", "\n", "                        ", "guide", "[", "idx", ",", "0", ",", "i", "+", "1", ",", "j", "+", "1", "]", "=", "1.0", "\n", "", "", "", "", "return", "guide", "\n", "", "", ""]], "home.repos.pwc.inspect_result.neulab_awesome-align.awesome_align.modeling.load_tf_weights_in_bert": [[66, 136], ["os.path.abspath", "logger.info", "tf.train.list_variables", "zip", "logger.info", "tf.train.load_variable", "names.append", "arrays.append", "name.split.split", "any", "logger.info", "torch.from_numpy", "logger.error", "logger.info", "re.fullmatch", "getattr", "re.split", "getattr", "len", "int", "np.transpose", "getattr", "getattr", "getattr", "getattr", "logger.info"], "function", ["None"], ["def", "load_tf_weights_in_bert", "(", "model", ",", "config", ",", "tf_checkpoint_path", ")", ":", "\n", "    ", "\"\"\" Load tf checkpoints in a pytorch model.\n    \"\"\"", "\n", "try", ":", "\n", "        ", "import", "re", "\n", "import", "numpy", "as", "np", "\n", "import", "tensorflow", "as", "tf", "\n", "", "except", "ImportError", ":", "\n", "        ", "logger", ".", "error", "(", "\n", "\"Loading a TensorFlow model in PyTorch, requires TensorFlow to be installed. Please see \"", "\n", "\"https://www.tensorflow.org/install/ for installation instructions.\"", "\n", ")", "\n", "raise", "\n", "", "tf_path", "=", "os", ".", "path", ".", "abspath", "(", "tf_checkpoint_path", ")", "\n", "logger", ".", "info", "(", "\"Converting TensorFlow checkpoint from {}\"", ".", "format", "(", "tf_path", ")", ")", "\n", "# Load weights from TF model", "\n", "init_vars", "=", "tf", ".", "train", ".", "list_variables", "(", "tf_path", ")", "\n", "names", "=", "[", "]", "\n", "arrays", "=", "[", "]", "\n", "for", "name", ",", "shape", "in", "init_vars", ":", "\n", "        ", "logger", ".", "info", "(", "\"Loading TF weight {} with shape {}\"", ".", "format", "(", "name", ",", "shape", ")", ")", "\n", "array", "=", "tf", ".", "train", ".", "load_variable", "(", "tf_path", ",", "name", ")", "\n", "names", ".", "append", "(", "name", ")", "\n", "arrays", ".", "append", "(", "array", ")", "\n", "\n", "", "for", "name", ",", "array", "in", "zip", "(", "names", ",", "arrays", ")", ":", "\n", "        ", "name", "=", "name", ".", "split", "(", "\"/\"", ")", "\n", "# adam_v and adam_m are variables used in AdamWeightDecayOptimizer to calculated m and v", "\n", "# which are not required for using pretrained model", "\n", "if", "any", "(", "\n", "n", "in", "[", "\"adam_v\"", ",", "\"adam_m\"", ",", "\"AdamWeightDecayOptimizer\"", ",", "\"AdamWeightDecayOptimizer_1\"", ",", "\"global_step\"", "]", "\n", "for", "n", "in", "name", "\n", ")", ":", "\n", "            ", "logger", ".", "info", "(", "\"Skipping {}\"", ".", "format", "(", "\"/\"", ".", "join", "(", "name", ")", ")", ")", "\n", "continue", "\n", "", "pointer", "=", "model", "\n", "for", "m_name", "in", "name", ":", "\n", "            ", "if", "re", ".", "fullmatch", "(", "r\"[A-Za-z]+_\\d+\"", ",", "m_name", ")", ":", "\n", "                ", "scope_names", "=", "re", ".", "split", "(", "r\"_(\\d+)\"", ",", "m_name", ")", "\n", "", "else", ":", "\n", "                ", "scope_names", "=", "[", "m_name", "]", "\n", "", "if", "scope_names", "[", "0", "]", "==", "\"kernel\"", "or", "scope_names", "[", "0", "]", "==", "\"gamma\"", ":", "\n", "                ", "pointer", "=", "getattr", "(", "pointer", ",", "\"weight\"", ")", "\n", "", "elif", "scope_names", "[", "0", "]", "==", "\"output_bias\"", "or", "scope_names", "[", "0", "]", "==", "\"beta\"", ":", "\n", "                ", "pointer", "=", "getattr", "(", "pointer", ",", "\"bias\"", ")", "\n", "", "elif", "scope_names", "[", "0", "]", "==", "\"output_weights\"", ":", "\n", "                ", "pointer", "=", "getattr", "(", "pointer", ",", "\"weight\"", ")", "\n", "", "elif", "scope_names", "[", "0", "]", "==", "\"squad\"", ":", "\n", "                ", "pointer", "=", "getattr", "(", "pointer", ",", "\"classifier\"", ")", "\n", "", "else", ":", "\n", "                ", "try", ":", "\n", "                    ", "pointer", "=", "getattr", "(", "pointer", ",", "scope_names", "[", "0", "]", ")", "\n", "", "except", "AttributeError", ":", "\n", "                    ", "logger", ".", "info", "(", "\"Skipping {}\"", ".", "format", "(", "\"/\"", ".", "join", "(", "name", ")", ")", ")", "\n", "continue", "\n", "", "", "if", "len", "(", "scope_names", ")", ">=", "2", ":", "\n", "                ", "num", "=", "int", "(", "scope_names", "[", "1", "]", ")", "\n", "pointer", "=", "pointer", "[", "num", "]", "\n", "", "", "if", "m_name", "[", "-", "11", ":", "]", "==", "\"_embeddings\"", ":", "\n", "            ", "pointer", "=", "getattr", "(", "pointer", ",", "\"weight\"", ")", "\n", "", "elif", "m_name", "==", "\"kernel\"", ":", "\n", "            ", "array", "=", "np", ".", "transpose", "(", "array", ")", "\n", "", "try", ":", "\n", "            ", "assert", "pointer", ".", "shape", "==", "array", ".", "shape", "\n", "", "except", "AssertionError", "as", "e", ":", "\n", "            ", "e", ".", "args", "+=", "(", "pointer", ".", "shape", ",", "array", ".", "shape", ")", "\n", "raise", "\n", "", "logger", ".", "info", "(", "\"Initialize PyTorch weight {}\"", ".", "format", "(", "name", ")", ")", "\n", "pointer", ".", "data", "=", "torch", ".", "from_numpy", "(", "array", ")", "\n", "", "return", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.neulab_awesome-align.awesome_align.modeling.mish": [[138, 140], ["torch.tanh", "torch.nn.functional.softplus"], "function", ["None"], ["", "def", "mish", "(", "x", ")", ":", "\n", "    ", "return", "x", "*", "torch", ".", "tanh", "(", "nn", ".", "functional", ".", "softplus", "(", "x", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.neulab_awesome-align.awesome_align.modeling.return_extended_attention_mask": [[447, 459], ["extended_attention_mask.to.to", "attention_mask.dim", "attention_mask.dim", "ValueError"], "function", ["None"], ["def", "return_extended_attention_mask", "(", "attention_mask", ",", "dtype", ")", ":", "\n", "    ", "if", "attention_mask", ".", "dim", "(", ")", "==", "3", ":", "\n", "        ", "extended_attention_mask", "=", "attention_mask", "[", ":", ",", "None", ",", ":", ",", ":", "]", "\n", "", "elif", "attention_mask", ".", "dim", "(", ")", "==", "2", ":", "\n", "        ", "extended_attention_mask", "=", "attention_mask", "[", ":", ",", "None", ",", "None", ",", ":", "]", "\n", "", "else", ":", "\n", "        ", "raise", "ValueError", "(", "\n", "\"Wrong shape for input_ids or attention_mask\"", "\n", ")", "\n", "", "extended_attention_mask", "=", "extended_attention_mask", ".", "to", "(", "dtype", "=", "dtype", ")", "\n", "extended_attention_mask", "=", "(", "1.0", "-", "extended_attention_mask", ")", "*", "-", "10000.0", "\n", "return", "extended_attention_mask", "\n", "\n"]], "home.repos.pwc.inspect_result.neulab_awesome-align.awesome_align.configuration_bert.BertConfig.__init__": [[112, 142], ["awesome_align.configuration_utils.PretrainedConfig.__init__"], "methods", ["home.repos.pwc.inspect_result.neulab_awesome-align.awesome_align.train_utils.AdamW.__init__"], ["def", "__init__", "(", "\n", "self", ",", "\n", "vocab_size", "=", "30522", ",", "\n", "hidden_size", "=", "768", ",", "\n", "num_hidden_layers", "=", "12", ",", "\n", "num_attention_heads", "=", "12", ",", "\n", "intermediate_size", "=", "3072", ",", "\n", "hidden_act", "=", "\"gelu\"", ",", "\n", "hidden_dropout_prob", "=", "0.1", ",", "\n", "attention_probs_dropout_prob", "=", "0.1", ",", "\n", "max_position_embeddings", "=", "512", ",", "\n", "type_vocab_size", "=", "2", ",", "\n", "initializer_range", "=", "0.02", ",", "\n", "layer_norm_eps", "=", "1e-12", ",", "\n", "**", "kwargs", "\n", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "**", "kwargs", ")", "\n", "\n", "self", ".", "vocab_size", "=", "vocab_size", "\n", "self", ".", "hidden_size", "=", "hidden_size", "\n", "self", ".", "num_hidden_layers", "=", "num_hidden_layers", "\n", "self", ".", "num_attention_heads", "=", "num_attention_heads", "\n", "self", ".", "hidden_act", "=", "hidden_act", "\n", "self", ".", "intermediate_size", "=", "intermediate_size", "\n", "self", ".", "hidden_dropout_prob", "=", "hidden_dropout_prob", "\n", "self", ".", "attention_probs_dropout_prob", "=", "attention_probs_dropout_prob", "\n", "self", ".", "max_position_embeddings", "=", "max_position_embeddings", "\n", "self", ".", "type_vocab_size", "=", "type_vocab_size", "\n", "self", ".", "initializer_range", "=", "initializer_range", "\n", "self", ".", "layer_norm_eps", "=", "layer_norm_eps", "\n", "", "", ""]], "home.repos.pwc.inspect_result.neulab_awesome-align.awesome_align.run_align.LineByLineTextDataset.__init__": [[48, 55], ["os.path.isfile", "print"], "methods", ["None"], []], "home.repos.pwc.inspect_result.neulab_awesome-align.awesome_align.run_align.LineByLineTextDataset.process_line": [[56, 79], ["line.split", "enumerate", "enumerate", "line.isspace", "src.strip().split", "tgt.strip().split", "len", "src.rstrip", "tgt.rstrip", "run_align.LineByLineTextDataset.tokenizer.tokenize", "run_align.LineByLineTextDataset.tokenizer.tokenize", "run_align.LineByLineTextDataset.tokenizer.convert_tokens_to_ids", "run_align.LineByLineTextDataset.tokenizer.convert_tokens_to_ids", "run_align.LineByLineTextDataset.tokenizer.prepare_for_model", "run_align.LineByLineTextDataset.tokenizer.prepare_for_model", "len", "len", "len", "src.strip", "tgt.strip", "list", "list", "line.split", "itertools.chain", "itertools.chain"], "methods", ["home.repos.pwc.inspect_result.neulab_awesome-align.awesome_align.tokenization_utils.PreTrainedTokenizerFast.tokenize", "home.repos.pwc.inspect_result.neulab_awesome-align.awesome_align.tokenization_utils.PreTrainedTokenizerFast.tokenize", "home.repos.pwc.inspect_result.neulab_awesome-align.awesome_align.tokenization_utils.PreTrainedTokenizer.convert_tokens_to_ids", "home.repos.pwc.inspect_result.neulab_awesome-align.awesome_align.tokenization_utils.PreTrainedTokenizer.convert_tokens_to_ids", "home.repos.pwc.inspect_result.neulab_awesome-align.awesome_align.tokenization_utils.PreTrainedTokenizer.prepare_for_model", "home.repos.pwc.inspect_result.neulab_awesome-align.awesome_align.tokenization_utils.PreTrainedTokenizer.prepare_for_model"], []], "home.repos.pwc.inspect_result.neulab_awesome-align.awesome_align.run_align.LineByLineTextDataset.__iter__": [[80, 106], ["torch.utils.data.get_worker_info", "open", "f.seek", "f.readline", "run_align.LineByLineTextDataset.process_line", "f.readline", "len", "print", "torch.tensor", "f.tell", "f.readline.strip", "f.tell"], "methods", ["home.repos.pwc.inspect_result.neulab_awesome-align.awesome_align.run_align.LineByLineTextDataset.process_line"], []], "home.repos.pwc.inspect_result.neulab_awesome-align.awesome_align.run_align.set_seed": [[40, 46], ["random.seed", "numpy.random.seed", "torch.manual_seed", "torch.cuda.manual_seed_all"], "function", ["None"], []], "home.repos.pwc.inspect_result.neulab_awesome-align.awesome_align.run_align.find_offsets": [[107, 126], ["open", "range", "os.fstat", "f.seek", "f.tell", "offsets.append", "f.fileno", "f.tell", "f.readline", "f.seek"], "function", ["None"], []], "home.repos.pwc.inspect_result.neulab_awesome-align.awesome_align.run_align.open_writer_list": [[127, 133], ["open", "writers.extend", "tempfile.TemporaryFile", "range"], "function", ["None"], []], "home.repos.pwc.inspect_result.neulab_awesome-align.awesome_align.run_align.merge_files": [[134, 145], ["enumerate", "writers[].close", "len", "writers[].close", "writer.seek", "shutil.copyfileobj", "writer.close"], "function", ["None"], []], "home.repos.pwc.inspect_result.neulab_awesome-align.awesome_align.run_align.word_align": [[147, 200], ["run_align.find_offsets", "run_align.LineByLineTextDataset", "torch.utils.data.DataLoader", "model.to", "model.eval", "tqdm.trange", "run_align.open_writer_list", "run_align.merge_files", "zip", "torch.nn.utils.rnn.pad_sequence", "torch.nn.utils.rnn.pad_sequence", "run_align.open_writer_list", "run_align.open_writer_list", "run_align.merge_files", "run_align.merge_files", "torch.no_grad", "model.get_aligned_word", "zip", "tqdm.trange.update", "writers[].write", "len", "prob_writers[].write", "word_writers[].write", "output_str.append", "output_prob_str.append", "output_word_str.append"], "function", ["home.repos.pwc.inspect_result.neulab_awesome-align.awesome_align.run_align.find_offsets", "home.repos.pwc.inspect_result.neulab_awesome-align.awesome_align.run_align.open_writer_list", "home.repos.pwc.inspect_result.neulab_awesome-align.awesome_align.run_align.merge_files", "home.repos.pwc.inspect_result.neulab_awesome-align.awesome_align.run_align.open_writer_list", "home.repos.pwc.inspect_result.neulab_awesome-align.awesome_align.run_align.open_writer_list", "home.repos.pwc.inspect_result.neulab_awesome-align.awesome_align.run_align.merge_files", "home.repos.pwc.inspect_result.neulab_awesome-align.awesome_align.run_align.merge_files", "home.repos.pwc.inspect_result.neulab_awesome-align.awesome_align.modeling.BertForMaskedLM.get_aligned_word"], []], "home.repos.pwc.inspect_result.neulab_awesome-align.awesome_align.run_align.main": [[202, 295], ["argparse.ArgumentParser", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.parse_args", "torch.device", "run_align.set_seed", "run_align.word_align", "config_class.from_pretrained", "tokenizer_class.from_pretrained", "model_class.from_pretrained", "model_class", "config_class.from_pretrained", "config_class", "tokenizer_class.from_pretrained", "ValueError", "torch.cuda.is_available", "bool"], "function", ["home.repos.pwc.inspect_result.neulab_awesome-align.tools.aer.parse_args", "home.repos.pwc.inspect_result.neulab_awesome-align.awesome_align.run_align.set_seed", "home.repos.pwc.inspect_result.neulab_awesome-align.awesome_align.run_align.word_align", "home.repos.pwc.inspect_result.neulab_awesome-align.awesome_align.configuration_utils.PretrainedConfig.from_pretrained", "home.repos.pwc.inspect_result.neulab_awesome-align.awesome_align.configuration_utils.PretrainedConfig.from_pretrained", "home.repos.pwc.inspect_result.neulab_awesome-align.awesome_align.configuration_utils.PretrainedConfig.from_pretrained", "home.repos.pwc.inspect_result.neulab_awesome-align.awesome_align.configuration_utils.PretrainedConfig.from_pretrained", "home.repos.pwc.inspect_result.neulab_awesome-align.awesome_align.configuration_utils.PretrainedConfig.from_pretrained"], []], "home.repos.pwc.inspect_result.neulab_awesome-align.awesome_align.tokenization_bert.BertTokenizer.__init__": [[163, 201], ["awesome_align.tokenization_utils.PreTrainedTokenizer.__init__", "tokenization_bert.load_vocab", "collections.OrderedDict", "tokenization_bert.WordpieceTokenizer", "os.path.isfile", "ValueError", "tokenization_bert.BasicTokenizer", "tokenization_bert.BertTokenizer.vocab.items"], "methods", ["home.repos.pwc.inspect_result.neulab_awesome-align.awesome_align.train_utils.AdamW.__init__", "home.repos.pwc.inspect_result.neulab_awesome-align.awesome_align.tokenization_bert.load_vocab"], ["def", "__init__", "(", "\n", "self", ",", "\n", "vocab_file", ",", "\n", "do_lower_case", "=", "True", ",", "\n", "do_basic_tokenize", "=", "True", ",", "\n", "never_split", "=", "None", ",", "\n", "unk_token", "=", "\"[UNK]\"", ",", "\n", "sep_token", "=", "\"[SEP]\"", ",", "\n", "pad_token", "=", "\"[PAD]\"", ",", "\n", "cls_token", "=", "\"[CLS]\"", ",", "\n", "mask_token", "=", "\"[MASK]\"", ",", "\n", "tokenize_chinese_chars", "=", "True", ",", "\n", "**", "kwargs", "\n", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "\n", "unk_token", "=", "unk_token", ",", "\n", "sep_token", "=", "sep_token", ",", "\n", "pad_token", "=", "pad_token", ",", "\n", "cls_token", "=", "cls_token", ",", "\n", "mask_token", "=", "mask_token", ",", "\n", "**", "kwargs", ",", "\n", ")", "\n", "self", ".", "max_len_single_sentence", "=", "self", ".", "max_len", "-", "2", "# take into account special tokens", "\n", "self", ".", "max_len_sentences_pair", "=", "self", ".", "max_len", "-", "3", "# take into account special tokens", "\n", "\n", "if", "not", "os", ".", "path", ".", "isfile", "(", "vocab_file", ")", ":", "\n", "            ", "raise", "ValueError", "(", "\n", "\"Can't find a vocabulary file at path '{}'. To load the vocabulary from a Google pretrained \"", "\n", "\"model use `tokenizer = BertTokenizer.from_pretrained(PRETRAINED_MODEL_NAME)`\"", ".", "format", "(", "vocab_file", ")", "\n", ")", "\n", "", "self", ".", "vocab", "=", "load_vocab", "(", "vocab_file", ")", "\n", "self", ".", "ids_to_tokens", "=", "collections", ".", "OrderedDict", "(", "[", "(", "ids", ",", "tok", ")", "for", "tok", ",", "ids", "in", "self", ".", "vocab", ".", "items", "(", ")", "]", ")", "\n", "self", ".", "do_basic_tokenize", "=", "do_basic_tokenize", "\n", "if", "do_basic_tokenize", ":", "\n", "            ", "self", ".", "basic_tokenizer", "=", "BasicTokenizer", "(", "\n", "do_lower_case", "=", "do_lower_case", ",", "never_split", "=", "never_split", ",", "tokenize_chinese_chars", "=", "tokenize_chinese_chars", "\n", ")", "\n", "", "self", ".", "wordpiece_tokenizer", "=", "WordpieceTokenizer", "(", "vocab", "=", "self", ".", "vocab", ",", "unk_token", "=", "self", ".", "unk_token", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.neulab_awesome-align.awesome_align.tokenization_bert.BertTokenizer.vocab_size": [[202, 205], ["len"], "methods", ["None"], ["", "@", "property", "\n", "def", "vocab_size", "(", "self", ")", ":", "\n", "        ", "return", "len", "(", "self", ".", "vocab", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.neulab_awesome-align.awesome_align.tokenization_bert.BertTokenizer.get_vocab": [[206, 208], ["dict"], "methods", ["None"], ["", "def", "get_vocab", "(", "self", ")", ":", "\n", "        ", "return", "dict", "(", "self", ".", "vocab", ",", "**", "self", ".", "added_tokens_encoder", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.neulab_awesome-align.awesome_align.tokenization_bert.BertTokenizer._tokenize": [[209, 218], ["tokenization_bert.BertTokenizer.basic_tokenizer.tokenize", "tokenization_bert.BertTokenizer.wordpiece_tokenizer.tokenize", "tokenization_bert.BertTokenizer.wordpiece_tokenizer.tokenize", "tokenization_bert.BertTokenizer.append"], "methods", ["home.repos.pwc.inspect_result.neulab_awesome-align.awesome_align.tokenization_utils.PreTrainedTokenizerFast.tokenize", "home.repos.pwc.inspect_result.neulab_awesome-align.awesome_align.tokenization_utils.PreTrainedTokenizerFast.tokenize", "home.repos.pwc.inspect_result.neulab_awesome-align.awesome_align.tokenization_utils.PreTrainedTokenizerFast.tokenize"], ["", "def", "_tokenize", "(", "self", ",", "text", ")", ":", "\n", "        ", "split_tokens", "=", "[", "]", "\n", "if", "self", ".", "do_basic_tokenize", ":", "\n", "            ", "for", "token", "in", "self", ".", "basic_tokenizer", ".", "tokenize", "(", "text", ",", "never_split", "=", "self", ".", "all_special_tokens", ")", ":", "\n", "                ", "for", "sub_token", "in", "self", ".", "wordpiece_tokenizer", ".", "tokenize", "(", "token", ")", ":", "\n", "                    ", "split_tokens", ".", "append", "(", "sub_token", ")", "\n", "", "", "", "else", ":", "\n", "            ", "split_tokens", "=", "self", ".", "wordpiece_tokenizer", ".", "tokenize", "(", "text", ")", "\n", "", "return", "split_tokens", "\n", "\n"]], "home.repos.pwc.inspect_result.neulab_awesome-align.awesome_align.tokenization_bert.BertTokenizer._convert_token_to_id": [[219, 222], ["tokenization_bert.BertTokenizer.vocab.get", "tokenization_bert.BertTokenizer.vocab.get"], "methods", ["None"], ["", "def", "_convert_token_to_id", "(", "self", ",", "token", ")", ":", "\n", "        ", "\"\"\" Converts a token (str) in an id using the vocab. \"\"\"", "\n", "return", "self", ".", "vocab", ".", "get", "(", "token", ",", "self", ".", "vocab", ".", "get", "(", "self", ".", "unk_token", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.neulab_awesome-align.awesome_align.tokenization_bert.BertTokenizer._convert_id_to_token": [[223, 226], ["tokenization_bert.BertTokenizer.ids_to_tokens.get"], "methods", ["None"], ["", "def", "_convert_id_to_token", "(", "self", ",", "index", ")", ":", "\n", "        ", "\"\"\"Converts an index (integer) in a token (str) using the vocab.\"\"\"", "\n", "return", "self", ".", "ids_to_tokens", ".", "get", "(", "index", ",", "self", ".", "unk_token", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.neulab_awesome-align.awesome_align.tokenization_bert.BertTokenizer.convert_tokens_to_string": [[227, 231], ["None"], "methods", ["None"], ["", "def", "convert_tokens_to_string", "(", "self", ",", "tokens", ")", ":", "\n", "        ", "\"\"\" Converts a sequence of tokens (string) in a single string. \"\"\"", "\n", "out_string", "=", "\" \"", ".", "join", "(", "tokens", ")", ".", "replace", "(", "\" ##\"", ",", "\"\"", ")", ".", "strip", "(", ")", "\n", "return", "out_string", "\n", "\n"]], "home.repos.pwc.inspect_result.neulab_awesome-align.awesome_align.tokenization_bert.BertTokenizer.build_inputs_with_special_tokens": [[232, 257], ["None"], "methods", ["None"], ["", "def", "build_inputs_with_special_tokens", "(", "\n", "self", ",", "token_ids_0", ":", "List", "[", "int", "]", ",", "token_ids_1", ":", "Optional", "[", "List", "[", "int", "]", "]", "=", "None", "\n", ")", "->", "List", "[", "int", "]", ":", "\n", "        ", "\"\"\"\n        Build model inputs from a sequence or a pair of sequence for sequence classification tasks\n        by concatenating and adding special tokens.\n        A BERT sequence has the following format:\n\n        - single sequence: ``[CLS] X [SEP]``\n        - pair of sequences: ``[CLS] A [SEP] B [SEP]``\n\n        Args:\n            token_ids_0 (:obj:`List[int]`):\n                List of IDs to which the special tokens will be added\n            token_ids_1 (:obj:`List[int]`, `optional`, defaults to :obj:`None`):\n                Optional second list of IDs for sequence pairs.\n\n        Returns:\n            :obj:`List[int]`: list of `input IDs <../glossary.html#input-ids>`__ with the appropriate special tokens.\n        \"\"\"", "\n", "if", "token_ids_1", "is", "None", ":", "\n", "            ", "return", "[", "self", ".", "cls_token_id", "]", "+", "token_ids_0", "+", "[", "self", ".", "sep_token_id", "]", "\n", "", "cls", "=", "[", "self", ".", "cls_token_id", "]", "\n", "sep", "=", "[", "self", ".", "sep_token_id", "]", "\n", "return", "cls", "+", "token_ids_0", "+", "sep", "+", "token_ids_1", "+", "sep", "\n", "\n"]], "home.repos.pwc.inspect_result.neulab_awesome-align.awesome_align.tokenization_bert.BertTokenizer.get_special_tokens_mask": [[258, 288], ["list", "ValueError", "map", "len", "len", "len"], "methods", ["None"], ["", "def", "get_special_tokens_mask", "(", "\n", "self", ",", "token_ids_0", ":", "List", "[", "int", "]", ",", "token_ids_1", ":", "Optional", "[", "List", "[", "int", "]", "]", "=", "None", ",", "already_has_special_tokens", ":", "bool", "=", "False", "\n", ")", "->", "List", "[", "int", "]", ":", "\n", "        ", "\"\"\"\n        Retrieves sequence ids from a token list that has no special tokens added. This method is called when adding\n        special tokens using the tokenizer ``prepare_for_model`` or ``encode_plus`` methods.\n\n        Args:\n            token_ids_0 (:obj:`List[int]`):\n                List of ids.\n            token_ids_1 (:obj:`List[int]`, `optional`, defaults to :obj:`None`):\n                Optional second list of IDs for sequence pairs.\n            already_has_special_tokens (:obj:`bool`, `optional`, defaults to :obj:`False`):\n                Set to True if the token list is already formatted with special tokens for the model\n\n        Returns:\n            :obj:`List[int]`: A list of integers in the range [0, 1]: 0 for a special token, 1 for a sequence token.\n        \"\"\"", "\n", "\n", "if", "already_has_special_tokens", ":", "\n", "            ", "if", "token_ids_1", "is", "not", "None", ":", "\n", "                ", "raise", "ValueError", "(", "\n", "\"You should not supply a second sequence if the provided sequence of \"", "\n", "\"ids is already formated with special tokens for the model.\"", "\n", ")", "\n", "", "return", "list", "(", "map", "(", "lambda", "x", ":", "1", "if", "x", "in", "[", "self", ".", "sep_token_id", ",", "self", ".", "cls_token_id", "]", "else", "0", ",", "token_ids_0", ")", ")", "\n", "\n", "", "if", "token_ids_1", "is", "not", "None", ":", "\n", "            ", "return", "[", "1", "]", "+", "(", "[", "0", "]", "*", "len", "(", "token_ids_0", ")", ")", "+", "[", "1", "]", "+", "(", "[", "0", "]", "*", "len", "(", "token_ids_1", ")", ")", "+", "[", "1", "]", "\n", "", "return", "[", "1", "]", "+", "(", "[", "0", "]", "*", "len", "(", "token_ids_0", ")", ")", "+", "[", "1", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.neulab_awesome-align.awesome_align.tokenization_bert.BertTokenizer.create_token_type_ids_from_sequences": [[289, 318], ["len", "len", "len"], "methods", ["None"], ["", "def", "create_token_type_ids_from_sequences", "(", "\n", "self", ",", "token_ids_0", ":", "List", "[", "int", "]", ",", "token_ids_1", ":", "Optional", "[", "List", "[", "int", "]", "]", "=", "None", "\n", ")", "->", "List", "[", "int", "]", ":", "\n", "        ", "\"\"\"\n        Creates a mask from the two sequences passed to be used in a sequence-pair classification task.\n        A BERT sequence pair mask has the following format:\n\n        ::\n\n            0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1\n            | first sequence    | second sequence |\n\n        if token_ids_1 is None, only returns the first portion of the mask (0's).\n\n        Args:\n            token_ids_0 (:obj:`List[int]`):\n                List of ids.\n            token_ids_1 (:obj:`List[int]`, `optional`, defaults to :obj:`None`):\n                Optional second list of IDs for sequence pairs.\n\n        Returns:\n            :obj:`List[int]`: List of `token type IDs <../glossary.html#token-type-ids>`_ according to the given\n            sequence(s).\n        \"\"\"", "\n", "sep", "=", "[", "self", ".", "sep_token_id", "]", "\n", "cls", "=", "[", "self", ".", "cls_token_id", "]", "\n", "if", "token_ids_1", "is", "None", ":", "\n", "            ", "return", "len", "(", "cls", "+", "token_ids_0", "+", "sep", ")", "*", "[", "0", "]", "\n", "", "return", "len", "(", "cls", "+", "token_ids_0", "+", "sep", ")", "*", "[", "0", "]", "+", "len", "(", "token_ids_1", "+", "sep", ")", "*", "[", "1", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.neulab_awesome-align.awesome_align.tokenization_bert.BertTokenizer.save_vocabulary": [[319, 346], ["os.path.isdir", "os.path.join", "open", "sorted", "tokenization_bert.BertTokenizer.vocab.items", "writer.write", "logger.warning"], "methods", ["None"], ["", "def", "save_vocabulary", "(", "self", ",", "vocab_path", ")", ":", "\n", "        ", "\"\"\"\n        Save the sentencepiece vocabulary (copy original file) and special tokens file to a directory.\n\n        Args:\n            vocab_path (:obj:`str`):\n                The directory in which to save the vocabulary.\n\n        Returns:\n            :obj:`Tuple(str)`: Paths to the files saved.\n        \"\"\"", "\n", "index", "=", "0", "\n", "if", "os", ".", "path", ".", "isdir", "(", "vocab_path", ")", ":", "\n", "            ", "vocab_file", "=", "os", ".", "path", ".", "join", "(", "vocab_path", ",", "VOCAB_FILES_NAMES", "[", "\"vocab_file\"", "]", ")", "\n", "", "else", ":", "\n", "            ", "vocab_file", "=", "vocab_path", "\n", "", "with", "open", "(", "vocab_file", ",", "\"w\"", ",", "encoding", "=", "\"utf-8\"", ")", "as", "writer", ":", "\n", "            ", "for", "token", ",", "token_index", "in", "sorted", "(", "self", ".", "vocab", ".", "items", "(", ")", ",", "key", "=", "lambda", "kv", ":", "kv", "[", "1", "]", ")", ":", "\n", "                ", "if", "index", "!=", "token_index", ":", "\n", "                    ", "logger", ".", "warning", "(", "\n", "\"Saving vocabulary to {}: vocabulary indices are not consecutive.\"", "\n", "\" Please check that the vocabulary is not corrupted!\"", ".", "format", "(", "vocab_file", ")", "\n", ")", "\n", "index", "=", "token_index", "\n", "", "writer", ".", "write", "(", "token", "+", "\"\\n\"", ")", "\n", "index", "+=", "1", "\n", "", "", "return", "(", "vocab_file", ",", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.neulab_awesome-align.awesome_align.tokenization_bert.BasicTokenizer.__init__": [[351, 370], ["None"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "do_lower_case", "=", "True", ",", "never_split", "=", "None", ",", "tokenize_chinese_chars", "=", "True", ")", ":", "\n", "        ", "\"\"\" Constructs a BasicTokenizer.\n\n        Args:\n            **do_lower_case**: Whether to lower case the input.\n            **never_split**: (`optional`) list of str\n                Kept for backward compatibility purposes.\n                Now implemented directly at the base class level (see :func:`PreTrainedTokenizer.tokenize`)\n                List of token not to split.\n            **tokenize_chinese_chars**: (`optional`) boolean (default True)\n                Whether to tokenize Chinese characters.\n                This should likely be deactivated for Japanese:\n                see: https://github.com/huggingface/pytorch-pretrained-BERT/issues/328\n        \"\"\"", "\n", "if", "never_split", "is", "None", ":", "\n", "            ", "never_split", "=", "[", "]", "\n", "", "self", ".", "do_lower_case", "=", "do_lower_case", "\n", "self", ".", "never_split", "=", "never_split", "\n", "self", ".", "tokenize_chinese_chars", "=", "tokenize_chinese_chars", "\n", "\n"]], "home.repos.pwc.inspect_result.neulab_awesome-align.awesome_align.tokenization_bert.BasicTokenizer.tokenize": [[371, 401], ["tokenization_bert.BasicTokenizer._clean_text", "tokenization_bert.whitespace_tokenize", "tokenization_bert.whitespace_tokenize", "tokenization_bert.BasicTokenizer._tokenize_chinese_chars", "split_tokens.extend", "tokenization_bert.BasicTokenizer.lower", "tokenization_bert.BasicTokenizer._run_strip_accents", "tokenization_bert.BasicTokenizer._run_split_on_punc"], "methods", ["home.repos.pwc.inspect_result.neulab_awesome-align.awesome_align.tokenization_bert.BasicTokenizer._clean_text", "home.repos.pwc.inspect_result.neulab_awesome-align.awesome_align.tokenization_bert.whitespace_tokenize", "home.repos.pwc.inspect_result.neulab_awesome-align.awesome_align.tokenization_bert.whitespace_tokenize", "home.repos.pwc.inspect_result.neulab_awesome-align.awesome_align.tokenization_bert.BasicTokenizer._tokenize_chinese_chars", "home.repos.pwc.inspect_result.neulab_awesome-align.awesome_align.tokenization_bert.BasicTokenizer._run_strip_accents", "home.repos.pwc.inspect_result.neulab_awesome-align.awesome_align.tokenization_bert.BasicTokenizer._run_split_on_punc"], ["", "def", "tokenize", "(", "self", ",", "text", ",", "never_split", "=", "None", ")", ":", "\n", "        ", "\"\"\" Basic Tokenization of a piece of text.\n            Split on \"white spaces\" only, for sub-word tokenization, see WordPieceTokenizer.\n\n        Args:\n            **never_split**: (`optional`) list of str\n                Kept for backward compatibility purposes.\n                Now implemented directly at the base class level (see :func:`PreTrainedTokenizer.tokenize`)\n                List of token not to split.\n        \"\"\"", "\n", "never_split", "=", "self", ".", "never_split", "+", "(", "never_split", "if", "never_split", "is", "not", "None", "else", "[", "]", ")", "\n", "text", "=", "self", ".", "_clean_text", "(", "text", ")", "\n", "# This was added on November 1st, 2018 for the multilingual and Chinese", "\n", "# models. This is also applied to the English models now, but it doesn't", "\n", "# matter since the English models were not trained on any Chinese data", "\n", "# and generally don't have any Chinese data in them (there are Chinese", "\n", "# characters in the vocabulary because Wikipedia does have some Chinese", "\n", "# words in the English Wikipedia.).", "\n", "if", "self", ".", "tokenize_chinese_chars", ":", "\n", "            ", "text", "=", "self", ".", "_tokenize_chinese_chars", "(", "text", ")", "\n", "", "orig_tokens", "=", "whitespace_tokenize", "(", "text", ")", "\n", "split_tokens", "=", "[", "]", "\n", "for", "token", "in", "orig_tokens", ":", "\n", "            ", "if", "self", ".", "do_lower_case", "and", "token", "not", "in", "never_split", ":", "\n", "                ", "token", "=", "token", ".", "lower", "(", ")", "\n", "token", "=", "self", ".", "_run_strip_accents", "(", "token", ")", "\n", "", "split_tokens", ".", "extend", "(", "self", ".", "_run_split_on_punc", "(", "token", ",", "never_split", ")", ")", "\n", "\n", "", "output_tokens", "=", "whitespace_tokenize", "(", "\" \"", ".", "join", "(", "split_tokens", ")", ")", "\n", "return", "output_tokens", "\n", "\n"]], "home.repos.pwc.inspect_result.neulab_awesome-align.awesome_align.tokenization_bert.BasicTokenizer._run_strip_accents": [[402, 412], ["unicodedata.normalize", "unicodedata.category", "output.append"], "methods", ["None"], ["", "def", "_run_strip_accents", "(", "self", ",", "text", ")", ":", "\n", "        ", "\"\"\"Strips accents from a piece of text.\"\"\"", "\n", "text", "=", "unicodedata", ".", "normalize", "(", "\"NFD\"", ",", "text", ")", "\n", "output", "=", "[", "]", "\n", "for", "char", "in", "text", ":", "\n", "            ", "cat", "=", "unicodedata", ".", "category", "(", "char", ")", "\n", "if", "cat", "==", "\"Mn\"", ":", "\n", "                ", "continue", "\n", "", "output", ".", "append", "(", "char", ")", "\n", "", "return", "\"\"", ".", "join", "(", "output", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.neulab_awesome-align.awesome_align.tokenization_bert.BasicTokenizer._run_split_on_punc": [[413, 434], ["list", "len", "tokenization_bert._is_punctuation", "output.append", "output[].append", "output.append"], "methods", ["home.repos.pwc.inspect_result.neulab_awesome-align.awesome_align.tokenization_bert._is_punctuation"], ["", "def", "_run_split_on_punc", "(", "self", ",", "text", ",", "never_split", "=", "None", ")", ":", "\n", "        ", "\"\"\"Splits punctuation on a piece of text.\"\"\"", "\n", "if", "never_split", "is", "not", "None", "and", "text", "in", "never_split", ":", "\n", "            ", "return", "[", "text", "]", "\n", "", "chars", "=", "list", "(", "text", ")", "\n", "i", "=", "0", "\n", "start_new_word", "=", "True", "\n", "output", "=", "[", "]", "\n", "while", "i", "<", "len", "(", "chars", ")", ":", "\n", "            ", "char", "=", "chars", "[", "i", "]", "\n", "if", "_is_punctuation", "(", "char", ")", ":", "\n", "                ", "output", ".", "append", "(", "[", "char", "]", ")", "\n", "start_new_word", "=", "True", "\n", "", "else", ":", "\n", "                ", "if", "start_new_word", ":", "\n", "                    ", "output", ".", "append", "(", "[", "]", ")", "\n", "", "start_new_word", "=", "False", "\n", "output", "[", "-", "1", "]", ".", "append", "(", "char", ")", "\n", "", "i", "+=", "1", "\n", "\n", "", "return", "[", "\"\"", ".", "join", "(", "x", ")", "for", "x", "in", "output", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.neulab_awesome-align.awesome_align.tokenization_bert.BasicTokenizer._tokenize_chinese_chars": [[435, 447], ["ord", "tokenization_bert.BasicTokenizer._is_chinese_char", "output.append", "output.append", "output.append", "output.append"], "methods", ["home.repos.pwc.inspect_result.neulab_awesome-align.awesome_align.tokenization_bert.BasicTokenizer._is_chinese_char"], ["", "def", "_tokenize_chinese_chars", "(", "self", ",", "text", ")", ":", "\n", "        ", "\"\"\"Adds whitespace around any CJK character.\"\"\"", "\n", "output", "=", "[", "]", "\n", "for", "char", "in", "text", ":", "\n", "            ", "cp", "=", "ord", "(", "char", ")", "\n", "if", "self", ".", "_is_chinese_char", "(", "cp", ")", ":", "\n", "                ", "output", ".", "append", "(", "\" \"", ")", "\n", "output", ".", "append", "(", "char", ")", "\n", "output", ".", "append", "(", "\" \"", ")", "\n", "", "else", ":", "\n", "                ", "output", ".", "append", "(", "char", ")", "\n", "", "", "return", "\"\"", ".", "join", "(", "output", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.neulab_awesome-align.awesome_align.tokenization_bert.BasicTokenizer._is_chinese_char": [[448, 471], ["None"], "methods", ["None"], ["", "def", "_is_chinese_char", "(", "self", ",", "cp", ")", ":", "\n", "        ", "\"\"\"Checks whether CP is the codepoint of a CJK character.\"\"\"", "\n", "# This defines a \"chinese character\" as anything in the CJK Unicode block:", "\n", "#   https://en.wikipedia.org/wiki/CJK_Unified_Ideographs_(Unicode_block)", "\n", "#", "\n", "# Note that the CJK Unicode block is NOT all Japanese and Korean characters,", "\n", "# despite its name. The modern Korean Hangul alphabet is a different block,", "\n", "# as is Japanese Hiragana and Katakana. Those alphabets are used to write", "\n", "# space-separated words, so they are not treated specially and handled", "\n", "# like the all of the other languages.", "\n", "if", "(", "\n", "(", "cp", ">=", "0x4E00", "and", "cp", "<=", "0x9FFF", ")", "\n", "or", "(", "cp", ">=", "0x3400", "and", "cp", "<=", "0x4DBF", ")", "#", "\n", "or", "(", "cp", ">=", "0x20000", "and", "cp", "<=", "0x2A6DF", ")", "#", "\n", "or", "(", "cp", ">=", "0x2A700", "and", "cp", "<=", "0x2B73F", ")", "#", "\n", "or", "(", "cp", ">=", "0x2B740", "and", "cp", "<=", "0x2B81F", ")", "#", "\n", "or", "(", "cp", ">=", "0x2B820", "and", "cp", "<=", "0x2CEAF", ")", "#", "\n", "or", "(", "cp", ">=", "0xF900", "and", "cp", "<=", "0xFAFF", ")", "\n", "or", "(", "cp", ">=", "0x2F800", "and", "cp", "<=", "0x2FA1F", ")", "#", "\n", ")", ":", "#", "\n", "            ", "return", "True", "\n", "\n", "", "return", "False", "\n", "\n"]], "home.repos.pwc.inspect_result.neulab_awesome-align.awesome_align.tokenization_bert.BasicTokenizer._clean_text": [[472, 484], ["ord", "tokenization_bert._is_whitespace", "tokenization_bert._is_control", "output.append", "output.append"], "methods", ["home.repos.pwc.inspect_result.neulab_awesome-align.awesome_align.tokenization_bert._is_whitespace", "home.repos.pwc.inspect_result.neulab_awesome-align.awesome_align.tokenization_bert._is_control"], ["", "def", "_clean_text", "(", "self", ",", "text", ")", ":", "\n", "        ", "\"\"\"Performs invalid character removal and whitespace cleanup on text.\"\"\"", "\n", "output", "=", "[", "]", "\n", "for", "char", "in", "text", ":", "\n", "            ", "cp", "=", "ord", "(", "char", ")", "\n", "if", "cp", "==", "0", "or", "cp", "==", "0xFFFD", "or", "_is_control", "(", "char", ")", ":", "\n", "                ", "continue", "\n", "", "if", "_is_whitespace", "(", "char", ")", ":", "\n", "                ", "output", ".", "append", "(", "\" \"", ")", "\n", "", "else", ":", "\n", "                ", "output", ".", "append", "(", "char", ")", "\n", "", "", "return", "\"\"", ".", "join", "(", "output", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.neulab_awesome-align.awesome_align.tokenization_bert.WordpieceTokenizer.__init__": [[489, 493], ["None"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "vocab", ",", "unk_token", ",", "max_input_chars_per_word", "=", "100", ")", ":", "\n", "        ", "self", ".", "vocab", "=", "vocab", "\n", "self", ".", "unk_token", "=", "unk_token", "\n", "self", ".", "max_input_chars_per_word", "=", "max_input_chars_per_word", "\n", "\n"]], "home.repos.pwc.inspect_result.neulab_awesome-align.awesome_align.tokenization_bert.WordpieceTokenizer.tokenize": [[494, 544], ["tokenization_bert.whitespace_tokenize", "list", "len", "output_tokens.append", "len", "len", "sub_tokens.append", "output_tokens.append", "output_tokens.extend"], "methods", ["home.repos.pwc.inspect_result.neulab_awesome-align.awesome_align.tokenization_bert.whitespace_tokenize"], ["", "def", "tokenize", "(", "self", ",", "text", ")", ":", "\n", "        ", "\"\"\"Tokenizes a piece of text into its word pieces.\n\n        This uses a greedy longest-match-first algorithm to perform tokenization\n        using the given vocabulary.\n\n        For example:\n          input = \"unaffable\"\n          output = [\"un\", \"##aff\", \"##able\"]\n\n        Args:\n          text: A single token or whitespace separated tokens. This should have\n            already been passed through `BasicTokenizer`.\n\n        Returns:\n          A list of wordpiece tokens.\n        \"\"\"", "\n", "\n", "output_tokens", "=", "[", "]", "\n", "for", "token", "in", "whitespace_tokenize", "(", "text", ")", ":", "\n", "            ", "chars", "=", "list", "(", "token", ")", "\n", "if", "len", "(", "chars", ")", ">", "self", ".", "max_input_chars_per_word", ":", "\n", "                ", "output_tokens", ".", "append", "(", "self", ".", "unk_token", ")", "\n", "continue", "\n", "\n", "", "is_bad", "=", "False", "\n", "start", "=", "0", "\n", "sub_tokens", "=", "[", "]", "\n", "while", "start", "<", "len", "(", "chars", ")", ":", "\n", "                ", "end", "=", "len", "(", "chars", ")", "\n", "cur_substr", "=", "None", "\n", "while", "start", "<", "end", ":", "\n", "                    ", "substr", "=", "\"\"", ".", "join", "(", "chars", "[", "start", ":", "end", "]", ")", "\n", "if", "start", ">", "0", ":", "\n", "                        ", "substr", "=", "\"##\"", "+", "substr", "\n", "", "if", "substr", "in", "self", ".", "vocab", ":", "\n", "                        ", "cur_substr", "=", "substr", "\n", "break", "\n", "", "end", "-=", "1", "\n", "", "if", "cur_substr", "is", "None", ":", "\n", "                    ", "is_bad", "=", "True", "\n", "break", "\n", "", "sub_tokens", ".", "append", "(", "cur_substr", ")", "\n", "start", "=", "end", "\n", "\n", "", "if", "is_bad", ":", "\n", "                ", "output_tokens", ".", "append", "(", "self", ".", "unk_token", ")", "\n", "", "else", ":", "\n", "                ", "output_tokens", ".", "extend", "(", "sub_tokens", ")", "\n", "", "", "return", "output_tokens", "\n", "\n"]], "home.repos.pwc.inspect_result.neulab_awesome-align.awesome_align.tokenization_bert.BertTokenizerFast.__init__": [[591, 631], ["awesome_align.tokenization_utils.PreTrainedTokenizerFast.__init__", "tokenizers.BertWordPieceTokenizer"], "methods", ["home.repos.pwc.inspect_result.neulab_awesome-align.awesome_align.train_utils.AdamW.__init__"], ["def", "__init__", "(", "\n", "self", ",", "\n", "vocab_file", ",", "\n", "do_lower_case", "=", "True", ",", "\n", "do_basic_tokenize", "=", "True", ",", "\n", "never_split", "=", "None", ",", "\n", "unk_token", "=", "\"[UNK]\"", ",", "\n", "sep_token", "=", "\"[SEP]\"", ",", "\n", "pad_token", "=", "\"[PAD]\"", ",", "\n", "cls_token", "=", "\"[CLS]\"", ",", "\n", "mask_token", "=", "\"[MASK]\"", ",", "\n", "clean_text", "=", "True", ",", "\n", "tokenize_chinese_chars", "=", "True", ",", "\n", "add_special_tokens", "=", "True", ",", "\n", "strip_accents", "=", "True", ",", "\n", "wordpieces_prefix", "=", "\"##\"", ",", "\n", "**", "kwargs", "\n", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "\n", "BertWordPieceTokenizer", "(", "\n", "vocab_file", "=", "vocab_file", ",", "\n", "add_special_tokens", "=", "add_special_tokens", ",", "\n", "unk_token", "=", "unk_token", ",", "\n", "sep_token", "=", "sep_token", ",", "\n", "cls_token", "=", "cls_token", ",", "\n", "clean_text", "=", "clean_text", ",", "\n", "handle_chinese_chars", "=", "tokenize_chinese_chars", ",", "\n", "strip_accents", "=", "strip_accents", ",", "\n", "lowercase", "=", "do_lower_case", ",", "\n", "wordpieces_prefix", "=", "wordpieces_prefix", ",", "\n", ")", ",", "\n", "unk_token", "=", "unk_token", ",", "\n", "sep_token", "=", "sep_token", ",", "\n", "pad_token", "=", "pad_token", ",", "\n", "cls_token", "=", "cls_token", ",", "\n", "mask_token", "=", "mask_token", ",", "\n", "**", "kwargs", ",", "\n", ")", "\n", "\n", "self", ".", "do_lower_case", "=", "do_lower_case", "\n", "\n"]], "home.repos.pwc.inspect_result.neulab_awesome-align.awesome_align.tokenization_bert.BertTokenizerFast.build_inputs_with_special_tokens": [[632, 639], ["None"], "methods", ["None"], ["", "def", "build_inputs_with_special_tokens", "(", "self", ",", "token_ids_0", ",", "token_ids_1", "=", "None", ")", ":", "\n", "        ", "output", "=", "[", "self", ".", "cls_token_id", "]", "+", "token_ids_0", "+", "[", "self", ".", "sep_token_id", "]", "\n", "\n", "if", "token_ids_1", ":", "\n", "            ", "output", "+=", "token_ids_1", "+", "[", "self", ".", "sep_token_id", "]", "\n", "\n", "", "return", "output", "\n", "", "", ""]], "home.repos.pwc.inspect_result.neulab_awesome-align.awesome_align.tokenization_bert.load_vocab": [[99, 108], ["collections.OrderedDict", "enumerate", "open", "reader.readlines", "token.rstrip.rstrip"], "function", ["None"], ["def", "load_vocab", "(", "vocab_file", ")", ":", "\n", "    ", "\"\"\"Loads a vocabulary file into a dictionary.\"\"\"", "\n", "vocab", "=", "collections", ".", "OrderedDict", "(", ")", "\n", "with", "open", "(", "vocab_file", ",", "\"r\"", ",", "encoding", "=", "\"utf-8\"", ")", "as", "reader", ":", "\n", "        ", "tokens", "=", "reader", ".", "readlines", "(", ")", "\n", "", "for", "index", ",", "token", "in", "enumerate", "(", "tokens", ")", ":", "\n", "        ", "token", "=", "token", ".", "rstrip", "(", "\"\\n\"", ")", "\n", "vocab", "[", "token", "]", "=", "index", "\n", "", "return", "vocab", "\n", "\n"]], "home.repos.pwc.inspect_result.neulab_awesome-align.awesome_align.tokenization_bert.whitespace_tokenize": [[110, 117], ["text.strip.strip", "text.strip.split"], "function", ["None"], ["", "def", "whitespace_tokenize", "(", "text", ")", ":", "\n", "    ", "\"\"\"Runs basic whitespace cleaning and splitting on a piece of text.\"\"\"", "\n", "text", "=", "text", ".", "strip", "(", ")", "\n", "if", "not", "text", ":", "\n", "        ", "return", "[", "]", "\n", "", "tokens", "=", "text", ".", "split", "(", ")", "\n", "return", "tokens", "\n", "\n"]], "home.repos.pwc.inspect_result.neulab_awesome-align.awesome_align.tokenization_bert._is_whitespace": [[546, 556], ["unicodedata.category"], "function", ["None"], ["", "", "def", "_is_whitespace", "(", "char", ")", ":", "\n", "    ", "\"\"\"Checks whether `chars` is a whitespace character.\"\"\"", "\n", "# \\t, \\n, and \\r are technically contorl characters but we treat them", "\n", "# as whitespace since they are generally considered as such.", "\n", "if", "char", "==", "\" \"", "or", "char", "==", "\"\\t\"", "or", "char", "==", "\"\\n\"", "or", "char", "==", "\"\\r\"", ":", "\n", "        ", "return", "True", "\n", "", "cat", "=", "unicodedata", ".", "category", "(", "char", ")", "\n", "if", "cat", "==", "\"Zs\"", ":", "\n", "        ", "return", "True", "\n", "", "return", "False", "\n", "\n"]], "home.repos.pwc.inspect_result.neulab_awesome-align.awesome_align.tokenization_bert._is_control": [[558, 568], ["unicodedata.category", "unicodedata.category.startswith"], "function", ["None"], ["", "def", "_is_control", "(", "char", ")", ":", "\n", "    ", "\"\"\"Checks whether `chars` is a control character.\"\"\"", "\n", "# These are technically control characters but we count them as whitespace", "\n", "# characters.", "\n", "if", "char", "==", "\"\\t\"", "or", "char", "==", "\"\\n\"", "or", "char", "==", "\"\\r\"", ":", "\n", "        ", "return", "False", "\n", "", "cat", "=", "unicodedata", ".", "category", "(", "char", ")", "\n", "if", "cat", ".", "startswith", "(", "\"C\"", ")", ":", "\n", "        ", "return", "True", "\n", "", "return", "False", "\n", "\n"]], "home.repos.pwc.inspect_result.neulab_awesome-align.awesome_align.tokenization_bert._is_punctuation": [[570, 583], ["ord", "unicodedata.category", "unicodedata.category.startswith"], "function", ["None"], ["", "def", "_is_punctuation", "(", "char", ")", ":", "\n", "    ", "\"\"\"Checks whether `chars` is a punctuation character.\"\"\"", "\n", "cp", "=", "ord", "(", "char", ")", "\n", "# We treat all non-letter/number ASCII as punctuation.", "\n", "# Characters such as \"^\", \"$\", and \"`\" are not in the Unicode", "\n", "# Punctuation class but we treat them as punctuation anyways, for", "\n", "# consistency.", "\n", "if", "(", "cp", ">=", "33", "and", "cp", "<=", "47", ")", "or", "(", "cp", ">=", "58", "and", "cp", "<=", "64", ")", "or", "(", "cp", ">=", "91", "and", "cp", "<=", "96", ")", "or", "(", "cp", ">=", "123", "and", "cp", "<=", "126", ")", ":", "\n", "        ", "return", "True", "\n", "", "cat", "=", "unicodedata", ".", "category", "(", "char", ")", "\n", "if", "cat", ".", "startswith", "(", "\"P\"", ")", ":", "\n", "        ", "return", "True", "\n", "", "return", "False", "\n", "\n"]], "home.repos.pwc.inspect_result.neulab_awesome-align.awesome_align.file_utils.is_torch_available": [[103, 105], ["None"], "function", ["None"], ["def", "is_torch_available", "(", ")", ":", "\n", "    ", "return", "_torch_available", "\n", "\n"]], "home.repos.pwc.inspect_result.neulab_awesome-align.awesome_align.file_utils.is_tf_available": [[107, 109], ["None"], "function", ["None"], ["", "def", "is_tf_available", "(", ")", ":", "\n", "    ", "return", "_tf_available", "\n", "\n"]], "home.repos.pwc.inspect_result.neulab_awesome-align.awesome_align.file_utils.add_start_docstrings": [[111, 117], ["None"], "function", ["None"], ["", "def", "add_start_docstrings", "(", "*", "docstr", ")", ":", "\n", "    ", "def", "docstring_decorator", "(", "fn", ")", ":", "\n", "        ", "fn", ".", "__doc__", "=", "\"\"", ".", "join", "(", "docstr", ")", "+", "(", "fn", ".", "__doc__", "if", "fn", ".", "__doc__", "is", "not", "None", "else", "\"\"", ")", "\n", "return", "fn", "\n", "\n", "", "return", "docstring_decorator", "\n", "\n"]], "home.repos.pwc.inspect_result.neulab_awesome-align.awesome_align.file_utils.add_start_docstrings_to_callable": [[119, 135], ["fn.__qualname__.split"], "function", ["None"], ["", "def", "add_start_docstrings_to_callable", "(", "*", "docstr", ")", ":", "\n", "    ", "def", "docstring_decorator", "(", "fn", ")", ":", "\n", "        ", "class_name", "=", "\":class:`~transformers.{}`\"", ".", "format", "(", "fn", ".", "__qualname__", ".", "split", "(", "\".\"", ")", "[", "0", "]", ")", "\n", "intro", "=", "\"   The {} forward method, overrides the :func:`__call__` special method.\"", ".", "format", "(", "class_name", ")", "\n", "note", "=", "r\"\"\"\n\n    .. note::\n        Although the recipe for forward pass needs to be defined within\n        this function, one should call the :class:`Module` instance afterwards\n        instead of this since the former takes care of running the\n        pre and post processing steps while the latter silently ignores them.\n        \"\"\"", "\n", "fn", ".", "__doc__", "=", "intro", "+", "note", "+", "\"\"", ".", "join", "(", "docstr", ")", "+", "(", "fn", ".", "__doc__", "if", "fn", ".", "__doc__", "is", "not", "None", "else", "\"\"", ")", "\n", "return", "fn", "\n", "\n", "", "return", "docstring_decorator", "\n", "\n"]], "home.repos.pwc.inspect_result.neulab_awesome-align.awesome_align.file_utils.add_end_docstrings": [[137, 143], ["None"], "function", ["None"], ["", "def", "add_end_docstrings", "(", "*", "docstr", ")", ":", "\n", "    ", "def", "docstring_decorator", "(", "fn", ")", ":", "\n", "        ", "fn", ".", "__doc__", "=", "fn", ".", "__doc__", "+", "\"\"", ".", "join", "(", "docstr", ")", "\n", "return", "fn", "\n", "\n", "", "return", "docstring_decorator", "\n", "\n"]], "home.repos.pwc.inspect_result.neulab_awesome-align.awesome_align.file_utils.is_remote_url": [[145, 148], ["urllib.parse.urlparse"], "function", ["None"], ["", "def", "is_remote_url", "(", "url_or_filename", ")", ":", "\n", "    ", "parsed", "=", "urlparse", "(", "url_or_filename", ")", "\n", "return", "parsed", ".", "scheme", "in", "(", "\"http\"", ",", "\"https\"", ",", "\"s3\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.neulab_awesome-align.awesome_align.file_utils.hf_bucket_url": [[150, 156], ["None"], "function", ["None"], ["", "def", "hf_bucket_url", "(", "identifier", ",", "postfix", "=", "None", ",", "cdn", "=", "False", ")", "->", "str", ":", "\n", "    ", "endpoint", "=", "CLOUDFRONT_DISTRIB_PREFIX", "if", "cdn", "else", "S3_BUCKET_PREFIX", "\n", "if", "postfix", "is", "None", ":", "\n", "        ", "return", "\"/\"", ".", "join", "(", "(", "endpoint", ",", "identifier", ")", ")", "\n", "", "else", ":", "\n", "        ", "return", "\"/\"", ".", "join", "(", "(", "endpoint", ",", "identifier", ",", "postfix", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.neulab_awesome-align.awesome_align.file_utils.url_to_filename": [[158, 180], ["url.encode", "hashlib.sha256", "hashlib.sha256.hexdigest", "url.endswith", "etag.encode", "hashlib.sha256", "hashlib.sha256.hexdigest"], "function", ["home.repos.pwc.inspect_result.neulab_awesome-align.awesome_align.tokenization_utils.PreTrainedTokenizer.encode", "home.repos.pwc.inspect_result.neulab_awesome-align.awesome_align.tokenization_utils.PreTrainedTokenizer.encode"], ["", "", "def", "url_to_filename", "(", "url", ",", "etag", "=", "None", ")", ":", "\n", "    ", "\"\"\"\n    Convert `url` into a hashed filename in a repeatable way.\n    If `etag` is specified, append its hash to the url's, delimited\n    by a period.\n    If the url ends with .h5 (Keras HDF5 weights) adds '.h5' to the name\n    so that TF 2.0 can identify it as a HDF5 file\n    (see https://github.com/tensorflow/tensorflow/blob/00fad90125b18b80fe054de1055770cfb8fe4ba3/tensorflow/python/keras/engine/network.py#L1380)\n    \"\"\"", "\n", "url_bytes", "=", "url", ".", "encode", "(", "\"utf-8\"", ")", "\n", "url_hash", "=", "sha256", "(", "url_bytes", ")", "\n", "filename", "=", "url_hash", ".", "hexdigest", "(", ")", "\n", "\n", "if", "etag", ":", "\n", "        ", "etag_bytes", "=", "etag", ".", "encode", "(", "\"utf-8\"", ")", "\n", "etag_hash", "=", "sha256", "(", "etag_bytes", ")", "\n", "filename", "+=", "\".\"", "+", "etag_hash", ".", "hexdigest", "(", ")", "\n", "\n", "", "if", "url", ".", "endswith", "(", "\".h5\"", ")", ":", "\n", "        ", "filename", "+=", "\".h5\"", "\n", "\n", "", "return", "filename", "\n", "\n"]], "home.repos.pwc.inspect_result.neulab_awesome-align.awesome_align.file_utils.filename_to_url": [[182, 206], ["isinstance", "os.path.join", "str", "os.path.exists", "EnvironmentError", "os.path.exists", "EnvironmentError", "open", "json.load"], "function", ["None"], ["", "def", "filename_to_url", "(", "filename", ",", "cache_dir", "=", "None", ")", ":", "\n", "    ", "\"\"\"\n    Return the url and etag (which may be ``None``) stored for `filename`.\n    Raise ``EnvironmentError`` if `filename` or its stored metadata do not exist.\n    \"\"\"", "\n", "if", "cache_dir", "is", "None", ":", "\n", "        ", "cache_dir", "=", "TRANSFORMERS_CACHE", "\n", "", "if", "isinstance", "(", "cache_dir", ",", "Path", ")", ":", "\n", "        ", "cache_dir", "=", "str", "(", "cache_dir", ")", "\n", "\n", "", "cache_path", "=", "os", ".", "path", ".", "join", "(", "cache_dir", ",", "filename", ")", "\n", "if", "not", "os", ".", "path", ".", "exists", "(", "cache_path", ")", ":", "\n", "        ", "raise", "EnvironmentError", "(", "\"file {} not found\"", ".", "format", "(", "cache_path", ")", ")", "\n", "\n", "", "meta_path", "=", "cache_path", "+", "\".json\"", "\n", "if", "not", "os", ".", "path", ".", "exists", "(", "meta_path", ")", ":", "\n", "        ", "raise", "EnvironmentError", "(", "\"file {} not found\"", ".", "format", "(", "meta_path", ")", ")", "\n", "\n", "", "with", "open", "(", "meta_path", ",", "encoding", "=", "\"utf-8\"", ")", "as", "meta_file", ":", "\n", "        ", "metadata", "=", "json", ".", "load", "(", "meta_file", ")", "\n", "", "url", "=", "metadata", "[", "\"url\"", "]", "\n", "etag", "=", "metadata", "[", "\"etag\"", "]", "\n", "\n", "return", "url", ",", "etag", "\n", "\n"]], "home.repos.pwc.inspect_result.neulab_awesome-align.awesome_align.file_utils.cached_path": [[208, 298], ["isinstance", "isinstance", "file_utils.is_remote_url", "str", "str", "file_utils.get_from_cache", "os.path.exists", "os.path.split", "os.path.join", "output_file.replace", "os.path.isdir", "os.listdir", "filelock.FileLock", "shutil.rmtree", "os.makedirs", "zipfile.is_zipfile", "EnvironmentError", "ValueError", "zipfile.is_zipfile", "tarfile.is_tarfile", "tarfile.is_tarfile", "urllib.parse.urlparse", "zipfile.ZipFile", "zip_file.extractall", "zip_file.close", "tarfile.open", "tarfile.open.extractall", "tarfile.open.close", "EnvironmentError"], "function", ["home.repos.pwc.inspect_result.neulab_awesome-align.awesome_align.file_utils.is_remote_url", "home.repos.pwc.inspect_result.neulab_awesome-align.awesome_align.file_utils.get_from_cache"], ["", "def", "cached_path", "(", "\n", "url_or_filename", ",", "\n", "cache_dir", "=", "None", ",", "\n", "force_download", "=", "False", ",", "\n", "proxies", "=", "None", ",", "\n", "resume_download", "=", "False", ",", "\n", "user_agent", "=", "None", ",", "\n", "extract_compressed_file", "=", "False", ",", "\n", "force_extract", "=", "False", ",", "\n", "local_files_only", "=", "False", ",", "\n", ")", "->", "Optional", "[", "str", "]", ":", "\n", "    ", "\"\"\"\n    Given something that might be a URL (or might be a local path),\n    determine which. If it's a URL, download the file and cache it, and\n    return the path to the cached file. If it's already a local path,\n    make sure the file exists and then return the path.\n    Args:\n        cache_dir: specify a cache directory to save the file to (overwrite the default cache dir).\n        force_download: if True, re-dowload the file even if it's already cached in the cache dir.\n        resume_download: if True, resume the download if incompletly recieved file is found.\n        user_agent: Optional string or dict that will be appended to the user-agent on remote requests.\n        extract_compressed_file: if True and the path point to a zip or tar file, extract the compressed\n            file in a folder along the archive.\n        force_extract: if True when extract_compressed_file is True and the archive was already extracted,\n            re-extract the archive and overide the folder where it was extracted.\n\n    Return:\n        None in case of non-recoverable file (non-existent or inaccessible url + no cache on disk).\n        Local path (string) otherwise\n    \"\"\"", "\n", "if", "cache_dir", "is", "None", ":", "\n", "        ", "cache_dir", "=", "TRANSFORMERS_CACHE", "\n", "", "if", "isinstance", "(", "url_or_filename", ",", "Path", ")", ":", "\n", "        ", "url_or_filename", "=", "str", "(", "url_or_filename", ")", "\n", "", "if", "isinstance", "(", "cache_dir", ",", "Path", ")", ":", "\n", "        ", "cache_dir", "=", "str", "(", "cache_dir", ")", "\n", "\n", "", "if", "is_remote_url", "(", "url_or_filename", ")", ":", "\n", "# URL, so get it from the cache (downloading if necessary)", "\n", "        ", "output_path", "=", "get_from_cache", "(", "\n", "url_or_filename", ",", "\n", "cache_dir", "=", "cache_dir", ",", "\n", "force_download", "=", "force_download", ",", "\n", "proxies", "=", "proxies", ",", "\n", "resume_download", "=", "resume_download", ",", "\n", "user_agent", "=", "user_agent", ",", "\n", "local_files_only", "=", "local_files_only", ",", "\n", ")", "\n", "", "elif", "os", ".", "path", ".", "exists", "(", "url_or_filename", ")", ":", "\n", "# File, and it exists.", "\n", "        ", "output_path", "=", "url_or_filename", "\n", "", "elif", "urlparse", "(", "url_or_filename", ")", ".", "scheme", "==", "\"\"", ":", "\n", "# File, but it doesn't exist.", "\n", "        ", "raise", "EnvironmentError", "(", "\"file {} not found\"", ".", "format", "(", "url_or_filename", ")", ")", "\n", "", "else", ":", "\n", "# Something unknown", "\n", "        ", "raise", "ValueError", "(", "\"unable to parse {} as a URL or as a local path\"", ".", "format", "(", "url_or_filename", ")", ")", "\n", "\n", "", "if", "extract_compressed_file", ":", "\n", "        ", "if", "not", "is_zipfile", "(", "output_path", ")", "and", "not", "tarfile", ".", "is_tarfile", "(", "output_path", ")", ":", "\n", "            ", "return", "output_path", "\n", "\n", "# Path where we extract compressed archives", "\n", "# We avoid '.' in dir name and add \"-extracted\" at the end: \"./model.zip\" => \"./model-zip-extracted/\"", "\n", "", "output_dir", ",", "output_file", "=", "os", ".", "path", ".", "split", "(", "output_path", ")", "\n", "output_extract_dir_name", "=", "output_file", ".", "replace", "(", "\".\"", ",", "\"-\"", ")", "+", "\"-extracted\"", "\n", "output_path_extracted", "=", "os", ".", "path", ".", "join", "(", "output_dir", ",", "output_extract_dir_name", ")", "\n", "\n", "if", "os", ".", "path", ".", "isdir", "(", "output_path_extracted", ")", "and", "os", ".", "listdir", "(", "output_path_extracted", ")", "and", "not", "force_extract", ":", "\n", "            ", "return", "output_path_extracted", "\n", "\n", "# Prevent parallel extractions", "\n", "", "lock_path", "=", "output_path", "+", "\".lock\"", "\n", "with", "FileLock", "(", "lock_path", ")", ":", "\n", "            ", "shutil", ".", "rmtree", "(", "output_path_extracted", ",", "ignore_errors", "=", "True", ")", "\n", "os", ".", "makedirs", "(", "output_path_extracted", ")", "\n", "if", "is_zipfile", "(", "output_path", ")", ":", "\n", "                ", "with", "ZipFile", "(", "output_path", ",", "\"r\"", ")", "as", "zip_file", ":", "\n", "                    ", "zip_file", ".", "extractall", "(", "output_path_extracted", ")", "\n", "zip_file", ".", "close", "(", ")", "\n", "", "", "elif", "tarfile", ".", "is_tarfile", "(", "output_path", ")", ":", "\n", "                ", "tar_file", "=", "tarfile", ".", "open", "(", "output_path", ")", "\n", "tar_file", ".", "extractall", "(", "output_path_extracted", ")", "\n", "tar_file", ".", "close", "(", ")", "\n", "", "else", ":", "\n", "                ", "raise", "EnvironmentError", "(", "\"Archive format of {} could not be identified\"", ".", "format", "(", "output_path", ")", ")", "\n", "\n", "", "", "return", "output_path_extracted", "\n", "\n", "", "return", "output_path", "\n", "\n"]], "home.repos.pwc.inspect_result.neulab_awesome-align.awesome_align.file_utils.split_s3_path": [[300, 311], ["urllib.parse.urlparse", "s3_path.startswith", "ValueError"], "function", ["None"], ["", "def", "split_s3_path", "(", "url", ")", ":", "\n", "    ", "\"\"\"Split a full s3 path into the bucket name and path.\"\"\"", "\n", "parsed", "=", "urlparse", "(", "url", ")", "\n", "if", "not", "parsed", ".", "netloc", "or", "not", "parsed", ".", "path", ":", "\n", "        ", "raise", "ValueError", "(", "\"bad s3 path {}\"", ".", "format", "(", "url", ")", ")", "\n", "", "bucket_name", "=", "parsed", ".", "netloc", "\n", "s3_path", "=", "parsed", ".", "path", "\n", "# Remove '/' at beginning of path.", "\n", "if", "s3_path", ".", "startswith", "(", "\"/\"", ")", ":", "\n", "        ", "s3_path", "=", "s3_path", "[", "1", ":", "]", "\n", "", "return", "bucket_name", ",", "s3_path", "\n", "\n"]], "home.repos.pwc.inspect_result.neulab_awesome-align.awesome_align.file_utils.s3_request": [[313, 330], ["functools.wraps", "func", "int", "EnvironmentError"], "function", ["None"], ["", "def", "s3_request", "(", "func", ")", ":", "\n", "    ", "\"\"\"\n    Wrapper function for s3 requests in order to create more helpful error\n    messages.\n    \"\"\"", "\n", "\n", "@", "wraps", "(", "func", ")", "\n", "def", "wrapper", "(", "url", ",", "*", "args", ",", "**", "kwargs", ")", ":", "\n", "        ", "try", ":", "\n", "            ", "return", "func", "(", "url", ",", "*", "args", ",", "**", "kwargs", ")", "\n", "", "except", "ClientError", "as", "exc", ":", "\n", "            ", "if", "int", "(", "exc", ".", "response", "[", "\"Error\"", "]", "[", "\"Code\"", "]", ")", "==", "404", ":", "\n", "                ", "raise", "EnvironmentError", "(", "\"file {} not found\"", ".", "format", "(", "url", ")", ")", "\n", "", "else", ":", "\n", "                ", "raise", "\n", "\n", "", "", "", "return", "wrapper", "\n", "\n"]], "home.repos.pwc.inspect_result.neulab_awesome-align.awesome_align.file_utils.s3_etag": [[332, 339], ["boto3.resource", "file_utils.split_s3_path", "boto3.resource.Object", "botocore.config.Config"], "function", ["home.repos.pwc.inspect_result.neulab_awesome-align.awesome_align.file_utils.split_s3_path"], ["", "@", "s3_request", "\n", "def", "s3_etag", "(", "url", ",", "proxies", "=", "None", ")", ":", "\n", "    ", "\"\"\"Check ETag on S3 object.\"\"\"", "\n", "s3_resource", "=", "boto3", ".", "resource", "(", "\"s3\"", ",", "config", "=", "Config", "(", "proxies", "=", "proxies", ")", ")", "\n", "bucket_name", ",", "s3_path", "=", "split_s3_path", "(", "url", ")", "\n", "s3_object", "=", "s3_resource", ".", "Object", "(", "bucket_name", ",", "s3_path", ")", "\n", "return", "s3_object", ".", "e_tag", "\n", "\n"]], "home.repos.pwc.inspect_result.neulab_awesome-align.awesome_align.file_utils.s3_get": [[341, 347], ["boto3.resource", "file_utils.split_s3_path", "boto3.resource.Bucket().download_fileobj", "botocore.config.Config", "boto3.resource.Bucket"], "function", ["home.repos.pwc.inspect_result.neulab_awesome-align.awesome_align.file_utils.split_s3_path"], ["", "@", "s3_request", "\n", "def", "s3_get", "(", "url", ",", "temp_file", ",", "proxies", "=", "None", ")", ":", "\n", "    ", "\"\"\"Pull a file directly from S3.\"\"\"", "\n", "s3_resource", "=", "boto3", ".", "resource", "(", "\"s3\"", ",", "config", "=", "Config", "(", "proxies", "=", "proxies", ")", ")", "\n", "bucket_name", ",", "s3_path", "=", "split_s3_path", "(", "url", ")", "\n", "s3_resource", ".", "Bucket", "(", "bucket_name", ")", ".", "download_fileobj", "(", "s3_path", ",", "temp_file", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.neulab_awesome-align.awesome_align.file_utils.http_get": [[349, 380], ["file_utils.is_torch_available", "file_utils.is_tf_available", "isinstance", "requests.get", "requests.get.headers.get", "tqdm.auto.tqdm", "requests.get.iter_content", "tqdm.auto.tqdm.close", "isinstance", "sys.version.split", "int", "bool", "tqdm.auto.tqdm.update", "temp_file.write", "len", "logger.getEffectiveLevel", "user_agent.items"], "function", ["home.repos.pwc.inspect_result.neulab_awesome-align.awesome_align.file_utils.is_torch_available", "home.repos.pwc.inspect_result.neulab_awesome-align.awesome_align.file_utils.is_tf_available"], ["", "def", "http_get", "(", "url", ",", "temp_file", ",", "proxies", "=", "None", ",", "resume_size", "=", "0", ",", "user_agent", "=", "None", ")", ":", "\n", "    ", "ua", "=", "\"transformers/{}; python/{}\"", ".", "format", "(", "__version__", ",", "sys", ".", "version", ".", "split", "(", ")", "[", "0", "]", ")", "\n", "if", "is_torch_available", "(", ")", ":", "\n", "        ", "ua", "+=", "\"; torch/{}\"", ".", "format", "(", "torch", ".", "__version__", ")", "\n", "", "if", "is_tf_available", "(", ")", ":", "\n", "        ", "ua", "+=", "\"; tensorflow/{}\"", ".", "format", "(", "tf", ".", "__version__", ")", "\n", "", "if", "isinstance", "(", "user_agent", ",", "dict", ")", ":", "\n", "        ", "ua", "+=", "\"; \"", "+", "\"; \"", ".", "join", "(", "\"{}/{}\"", ".", "format", "(", "k", ",", "v", ")", "for", "k", ",", "v", "in", "user_agent", ".", "items", "(", ")", ")", "\n", "", "elif", "isinstance", "(", "user_agent", ",", "str", ")", ":", "\n", "        ", "ua", "+=", "\"; \"", "+", "user_agent", "\n", "", "headers", "=", "{", "\"user-agent\"", ":", "ua", "}", "\n", "if", "resume_size", ">", "0", ":", "\n", "        ", "headers", "[", "\"Range\"", "]", "=", "\"bytes=%d-\"", "%", "(", "resume_size", ",", ")", "\n", "", "response", "=", "requests", ".", "get", "(", "url", ",", "stream", "=", "True", ",", "proxies", "=", "proxies", ",", "headers", "=", "headers", ")", "\n", "if", "response", ".", "status_code", "==", "416", ":", "# Range not satisfiable", "\n", "        ", "return", "\n", "", "content_length", "=", "response", ".", "headers", ".", "get", "(", "\"Content-Length\"", ")", "\n", "total", "=", "resume_size", "+", "int", "(", "content_length", ")", "if", "content_length", "is", "not", "None", "else", "None", "\n", "progress", "=", "tqdm", "(", "\n", "unit", "=", "\"B\"", ",", "\n", "unit_scale", "=", "True", ",", "\n", "total", "=", "total", ",", "\n", "initial", "=", "resume_size", ",", "\n", "desc", "=", "\"Downloading\"", ",", "\n", "disable", "=", "bool", "(", "logger", ".", "getEffectiveLevel", "(", ")", "==", "logging", ".", "NOTSET", ")", ",", "\n", ")", "\n", "for", "chunk", "in", "response", ".", "iter_content", "(", "chunk_size", "=", "1024", ")", ":", "\n", "        ", "if", "chunk", ":", "# filter out keep-alive new chunks", "\n", "            ", "progress", ".", "update", "(", "len", "(", "chunk", ")", ")", "\n", "temp_file", ".", "write", "(", "chunk", ")", "\n", "", "", "progress", ".", "close", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.neulab_awesome-align.awesome_align.file_utils.get_from_cache": [[382, 499], ["isinstance", "os.makedirs", "file_utils.url_to_filename", "os.path.join", "str", "url.startswith", "os.path.exists", "os.path.exists", "filelock.FileLock", "logger.info", "os.rename", "logger.info", "file_utils.s3_etag", "os.path.exists", "functools.partial", "functools.partial.", "logger.info", "url.startswith", "open", "json.dump", "requests.head", "len", "os.path.join", "file_utils.s3_get", "file_utils.http_get", "requests.head.headers.get", "fnmatch.filter", "ValueError", "open", "os.stat", "logger.warn", "os.listdir", "file.endswith", "file.endswith"], "function", ["home.repos.pwc.inspect_result.neulab_awesome-align.awesome_align.file_utils.url_to_filename", "home.repos.pwc.inspect_result.neulab_awesome-align.awesome_align.file_utils.s3_etag", "home.repos.pwc.inspect_result.neulab_awesome-align.awesome_align.file_utils.s3_get", "home.repos.pwc.inspect_result.neulab_awesome-align.awesome_align.file_utils.http_get"], ["", "def", "get_from_cache", "(", "\n", "url", ",", "\n", "cache_dir", "=", "None", ",", "\n", "force_download", "=", "False", ",", "\n", "proxies", "=", "None", ",", "\n", "etag_timeout", "=", "10", ",", "\n", "resume_download", "=", "False", ",", "\n", "user_agent", "=", "None", ",", "\n", "local_files_only", "=", "False", ",", "\n", ")", "->", "Optional", "[", "str", "]", ":", "\n", "    ", "\"\"\"\n    Given a URL, look for the corresponding file in the local cache.\n    If it's not there, download it. Then return the path to the cached file.\n\n    Return:\n        None in case of non-recoverable file (non-existent or inaccessible url + no cache on disk).\n        Local path (string) otherwise\n    \"\"\"", "\n", "if", "cache_dir", "is", "None", ":", "\n", "        ", "cache_dir", "=", "TRANSFORMERS_CACHE", "\n", "", "if", "isinstance", "(", "cache_dir", ",", "Path", ")", ":", "\n", "        ", "cache_dir", "=", "str", "(", "cache_dir", ")", "\n", "\n", "", "os", ".", "makedirs", "(", "cache_dir", ",", "exist_ok", "=", "True", ")", "\n", "\n", "etag", "=", "None", "\n", "if", "not", "local_files_only", ":", "\n", "# Get eTag to add to filename, if it exists.", "\n", "        ", "if", "url", ".", "startswith", "(", "\"s3://\"", ")", ":", "\n", "            ", "etag", "=", "s3_etag", "(", "url", ",", "proxies", "=", "proxies", ")", "\n", "", "else", ":", "\n", "            ", "try", ":", "\n", "                ", "response", "=", "requests", ".", "head", "(", "url", ",", "allow_redirects", "=", "True", ",", "proxies", "=", "proxies", ",", "timeout", "=", "etag_timeout", ")", "\n", "if", "response", ".", "status_code", "==", "200", ":", "\n", "                    ", "etag", "=", "response", ".", "headers", ".", "get", "(", "\"ETag\"", ")", "\n", "", "", "except", "(", "EnvironmentError", ",", "requests", ".", "exceptions", ".", "Timeout", ")", ":", "\n", "# etag is already None", "\n", "                ", "pass", "\n", "\n", "", "", "", "filename", "=", "url_to_filename", "(", "url", ",", "etag", ")", "\n", "\n", "# get cache path to put the file", "\n", "cache_path", "=", "os", ".", "path", ".", "join", "(", "cache_dir", ",", "filename", ")", "\n", "\n", "# etag is None = we don't have a connection, or url doesn't exist, or is otherwise inaccessible.", "\n", "# try to get the last downloaded one", "\n", "if", "etag", "is", "None", ":", "\n", "        ", "if", "os", ".", "path", ".", "exists", "(", "cache_path", ")", ":", "\n", "            ", "return", "cache_path", "\n", "", "else", ":", "\n", "            ", "matching_files", "=", "[", "\n", "file", "\n", "for", "file", "in", "fnmatch", ".", "filter", "(", "os", ".", "listdir", "(", "cache_dir", ")", ",", "filename", "+", "\".*\"", ")", "\n", "if", "not", "file", ".", "endswith", "(", "\".json\"", ")", "and", "not", "file", ".", "endswith", "(", "\".lock\"", ")", "\n", "]", "\n", "if", "len", "(", "matching_files", ")", ">", "0", ":", "\n", "                ", "return", "os", ".", "path", ".", "join", "(", "cache_dir", ",", "matching_files", "[", "-", "1", "]", ")", "\n", "", "else", ":", "\n", "# If files cannot be found and local_files_only=True,", "\n", "# the models might've been found if local_files_only=False", "\n", "# Notify the user about that", "\n", "                ", "if", "local_files_only", ":", "\n", "                    ", "raise", "ValueError", "(", "\n", "\"Cannot find the requested files in the cached path and outgoing traffic has been\"", "\n", "\" disabled. To enable model look-ups and downloads online, set 'local_files_only'\"", "\n", "\" to False.\"", "\n", ")", "\n", "", "return", "None", "\n", "\n", "# From now on, etag is not None.", "\n", "", "", "", "if", "os", ".", "path", ".", "exists", "(", "cache_path", ")", "and", "not", "force_download", ":", "\n", "        ", "return", "cache_path", "\n", "\n", "# Prevent parallel downloads of the same file with a lock.", "\n", "", "lock_path", "=", "cache_path", "+", "\".lock\"", "\n", "with", "FileLock", "(", "lock_path", ")", ":", "\n", "\n", "        ", "if", "resume_download", ":", "\n", "            ", "incomplete_path", "=", "cache_path", "+", "\".incomplete\"", "\n", "\n", "@", "contextmanager", "\n", "def", "_resumable_file_manager", "(", ")", ":", "\n", "                ", "with", "open", "(", "incomplete_path", ",", "\"a+b\"", ")", "as", "f", ":", "\n", "                    ", "yield", "f", "\n", "\n", "", "", "temp_file_manager", "=", "_resumable_file_manager", "\n", "if", "os", ".", "path", ".", "exists", "(", "incomplete_path", ")", ":", "\n", "                ", "resume_size", "=", "os", ".", "stat", "(", "incomplete_path", ")", ".", "st_size", "\n", "", "else", ":", "\n", "                ", "resume_size", "=", "0", "\n", "", "", "else", ":", "\n", "            ", "temp_file_manager", "=", "partial", "(", "tempfile", ".", "NamedTemporaryFile", ",", "dir", "=", "cache_dir", ",", "delete", "=", "False", ")", "\n", "resume_size", "=", "0", "\n", "\n", "# Download to temporary file, then copy to cache dir once finished.", "\n", "# Otherwise you get corrupt cache entries if the download gets interrupted.", "\n", "", "with", "temp_file_manager", "(", ")", "as", "temp_file", ":", "\n", "            ", "logger", ".", "info", "(", "\"%s not found in cache or force_download set to True, downloading to %s\"", ",", "url", ",", "temp_file", ".", "name", ")", "\n", "\n", "# GET file object", "\n", "if", "url", ".", "startswith", "(", "\"s3://\"", ")", ":", "\n", "                ", "if", "resume_download", ":", "\n", "                    ", "logger", ".", "warn", "(", "'Warning: resumable downloads are not implemented for \"s3://\" urls'", ")", "\n", "", "s3_get", "(", "url", ",", "temp_file", ",", "proxies", "=", "proxies", ")", "\n", "", "else", ":", "\n", "                ", "http_get", "(", "url", ",", "temp_file", ",", "proxies", "=", "proxies", ",", "resume_size", "=", "resume_size", ",", "user_agent", "=", "user_agent", ")", "\n", "\n", "", "", "logger", ".", "info", "(", "\"storing %s in cache at %s\"", ",", "url", ",", "cache_path", ")", "\n", "os", ".", "rename", "(", "temp_file", ".", "name", ",", "cache_path", ")", "\n", "\n", "logger", ".", "info", "(", "\"creating metadata file for %s\"", ",", "cache_path", ")", "\n", "meta", "=", "{", "\"url\"", ":", "url", ",", "\"etag\"", ":", "etag", "}", "\n", "meta_path", "=", "cache_path", "+", "\".json\"", "\n", "with", "open", "(", "meta_path", ",", "\"w\"", ")", "as", "meta_file", ":", "\n", "            ", "json", ".", "dump", "(", "meta", ",", "meta_file", ")", "\n", "\n", "", "", "return", "cache_path", "\n", "", ""]], "home.repos.pwc.inspect_result.neulab_awesome-align.awesome_align.tokenization_utils.PreTrainedTokenizer.bos_token": [[223, 226], ["None"], "methods", ["None"], ["", "@", "bos_token", ".", "setter", "\n", "def", "bos_token", "(", "self", ",", "value", ")", ":", "\n", "        ", "self", ".", "_bos_token", "=", "value", "\n", "\n"]], "home.repos.pwc.inspect_result.neulab_awesome-align.awesome_align.tokenization_utils.PreTrainedTokenizer.eos_token": [[227, 230], ["None"], "methods", ["None"], ["", "@", "eos_token", ".", "setter", "\n", "def", "eos_token", "(", "self", ",", "value", ")", ":", "\n", "        ", "self", ".", "_eos_token", "=", "value", "\n", "\n"]], "home.repos.pwc.inspect_result.neulab_awesome-align.awesome_align.tokenization_utils.PreTrainedTokenizer.unk_token": [[231, 234], ["None"], "methods", ["None"], ["", "@", "unk_token", ".", "setter", "\n", "def", "unk_token", "(", "self", ",", "value", ")", ":", "\n", "        ", "self", ".", "_unk_token", "=", "value", "\n", "\n"]], "home.repos.pwc.inspect_result.neulab_awesome-align.awesome_align.tokenization_utils.PreTrainedTokenizer.sep_token": [[235, 238], ["None"], "methods", ["None"], ["", "@", "sep_token", ".", "setter", "\n", "def", "sep_token", "(", "self", ",", "value", ")", ":", "\n", "        ", "self", ".", "_sep_token", "=", "value", "\n", "\n"]], "home.repos.pwc.inspect_result.neulab_awesome-align.awesome_align.tokenization_utils.PreTrainedTokenizer.pad_token": [[239, 242], ["None"], "methods", ["None"], ["", "@", "pad_token", ".", "setter", "\n", "def", "pad_token", "(", "self", ",", "value", ")", ":", "\n", "        ", "self", ".", "_pad_token", "=", "value", "\n", "\n"]], "home.repos.pwc.inspect_result.neulab_awesome-align.awesome_align.tokenization_utils.PreTrainedTokenizer.cls_token": [[243, 246], ["None"], "methods", ["None"], ["", "@", "cls_token", ".", "setter", "\n", "def", "cls_token", "(", "self", ",", "value", ")", ":", "\n", "        ", "self", ".", "_cls_token", "=", "value", "\n", "\n"]], "home.repos.pwc.inspect_result.neulab_awesome-align.awesome_align.tokenization_utils.PreTrainedTokenizer.mask_token": [[247, 250], ["None"], "methods", ["None"], ["", "@", "mask_token", ".", "setter", "\n", "def", "mask_token", "(", "self", ",", "value", ")", ":", "\n", "        ", "self", ".", "_mask_token", "=", "value", "\n", "\n"]], "home.repos.pwc.inspect_result.neulab_awesome-align.awesome_align.tokenization_utils.PreTrainedTokenizer.additional_special_tokens": [[251, 254], ["None"], "methods", ["None"], ["", "@", "additional_special_tokens", ".", "setter", "\n", "def", "additional_special_tokens", "(", "self", ",", "value", ")", ":", "\n", "        ", "self", ".", "_additional_special_tokens", "=", "value", "\n", "\n"]], "home.repos.pwc.inspect_result.neulab_awesome-align.awesome_align.tokenization_utils.PreTrainedTokenizer.bos_token_id": [[255, 259], ["tokenization_utils.PreTrainedTokenizer.convert_tokens_to_ids"], "methods", ["home.repos.pwc.inspect_result.neulab_awesome-align.awesome_align.tokenization_utils.PreTrainedTokenizer.convert_tokens_to_ids"], ["", "@", "property", "\n", "def", "bos_token_id", "(", "self", ")", ":", "\n", "        ", "\"\"\" Id of the beginning of sentence token in the vocabulary. Log an error if used while not having been set. \"\"\"", "\n", "return", "self", ".", "convert_tokens_to_ids", "(", "self", ".", "bos_token", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.neulab_awesome-align.awesome_align.tokenization_utils.PreTrainedTokenizer.eos_token_id": [[260, 264], ["tokenization_utils.PreTrainedTokenizer.convert_tokens_to_ids"], "methods", ["home.repos.pwc.inspect_result.neulab_awesome-align.awesome_align.tokenization_utils.PreTrainedTokenizer.convert_tokens_to_ids"], ["", "@", "property", "\n", "def", "eos_token_id", "(", "self", ")", ":", "\n", "        ", "\"\"\" Id of the end of sentence token in the vocabulary. Log an error if used while not having been set. \"\"\"", "\n", "return", "self", ".", "convert_tokens_to_ids", "(", "self", ".", "eos_token", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.neulab_awesome-align.awesome_align.tokenization_utils.PreTrainedTokenizer.unk_token_id": [[265, 269], ["tokenization_utils.PreTrainedTokenizer.convert_tokens_to_ids"], "methods", ["home.repos.pwc.inspect_result.neulab_awesome-align.awesome_align.tokenization_utils.PreTrainedTokenizer.convert_tokens_to_ids"], ["", "@", "property", "\n", "def", "unk_token_id", "(", "self", ")", ":", "\n", "        ", "\"\"\" Id of the unknown token in the vocabulary. Log an error if used while not having been set. \"\"\"", "\n", "return", "self", ".", "convert_tokens_to_ids", "(", "self", ".", "unk_token", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.neulab_awesome-align.awesome_align.tokenization_utils.PreTrainedTokenizer.sep_token_id": [[270, 274], ["tokenization_utils.PreTrainedTokenizer.convert_tokens_to_ids"], "methods", ["home.repos.pwc.inspect_result.neulab_awesome-align.awesome_align.tokenization_utils.PreTrainedTokenizer.convert_tokens_to_ids"], ["", "@", "property", "\n", "def", "sep_token_id", "(", "self", ")", ":", "\n", "        ", "\"\"\" Id of the separation token in the vocabulary. E.g. separate context and query in an input sequence. Log an error if used while not having been set. \"\"\"", "\n", "return", "self", ".", "convert_tokens_to_ids", "(", "self", ".", "sep_token", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.neulab_awesome-align.awesome_align.tokenization_utils.PreTrainedTokenizer.pad_token_id": [[275, 279], ["tokenization_utils.PreTrainedTokenizer.convert_tokens_to_ids"], "methods", ["home.repos.pwc.inspect_result.neulab_awesome-align.awesome_align.tokenization_utils.PreTrainedTokenizer.convert_tokens_to_ids"], ["", "@", "property", "\n", "def", "pad_token_id", "(", "self", ")", ":", "\n", "        ", "\"\"\" Id of the padding token in the vocabulary. Log an error if used while not having been set. \"\"\"", "\n", "return", "self", ".", "convert_tokens_to_ids", "(", "self", ".", "pad_token", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.neulab_awesome-align.awesome_align.tokenization_utils.PreTrainedTokenizer.pad_token_type_id": [[280, 284], ["None"], "methods", ["None"], ["", "@", "property", "\n", "def", "pad_token_type_id", "(", "self", ")", ":", "\n", "        ", "\"\"\" Id of the padding token type in the vocabulary.\"\"\"", "\n", "return", "self", ".", "_pad_token_type_id", "\n", "\n"]], "home.repos.pwc.inspect_result.neulab_awesome-align.awesome_align.tokenization_utils.PreTrainedTokenizer.cls_token_id": [[285, 289], ["tokenization_utils.PreTrainedTokenizer.convert_tokens_to_ids"], "methods", ["home.repos.pwc.inspect_result.neulab_awesome-align.awesome_align.tokenization_utils.PreTrainedTokenizer.convert_tokens_to_ids"], ["", "@", "property", "\n", "def", "cls_token_id", "(", "self", ")", ":", "\n", "        ", "\"\"\" Id of the classification token in the vocabulary. E.g. to extract a summary of an input sequence leveraging self-attention along the full depth of the model. Log an error if used while not having been set. \"\"\"", "\n", "return", "self", ".", "convert_tokens_to_ids", "(", "self", ".", "cls_token", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.neulab_awesome-align.awesome_align.tokenization_utils.PreTrainedTokenizer.mask_token_id": [[290, 294], ["tokenization_utils.PreTrainedTokenizer.convert_tokens_to_ids"], "methods", ["home.repos.pwc.inspect_result.neulab_awesome-align.awesome_align.tokenization_utils.PreTrainedTokenizer.convert_tokens_to_ids"], ["", "@", "property", "\n", "def", "mask_token_id", "(", "self", ")", ":", "\n", "        ", "\"\"\" Id of the mask token in the vocabulary. E.g. when training a model with masked-language modeling. Log an error if used while not having been set. \"\"\"", "\n", "return", "self", ".", "convert_tokens_to_ids", "(", "self", ".", "mask_token", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.neulab_awesome-align.awesome_align.tokenization_utils.PreTrainedTokenizer.additional_special_tokens_ids": [[295, 299], ["tokenization_utils.PreTrainedTokenizer.convert_tokens_to_ids"], "methods", ["home.repos.pwc.inspect_result.neulab_awesome-align.awesome_align.tokenization_utils.PreTrainedTokenizer.convert_tokens_to_ids"], ["", "@", "property", "\n", "def", "additional_special_tokens_ids", "(", "self", ")", ":", "\n", "        ", "\"\"\" Ids of all the additional special tokens in the vocabulary (list of integers). Log an error if used while not having been set. \"\"\"", "\n", "return", "self", ".", "convert_tokens_to_ids", "(", "self", ".", "additional_special_tokens", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.neulab_awesome-align.awesome_align.tokenization_utils.PreTrainedTokenizer.get_vocab": [[300, 303], ["NotImplementedError"], "methods", ["None"], ["", "def", "get_vocab", "(", "self", ")", ":", "\n", "        ", "\"\"\" Returns the vocabulary as a dict of {token: index} pairs. `tokenizer.get_vocab()[token]` is equivalent to `tokenizer.convert_tokens_to_ids(token)` when `token` is in the vocab. \"\"\"", "\n", "raise", "NotImplementedError", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.neulab_awesome-align.awesome_align.tokenization_utils.PreTrainedTokenizer.__init__": [[304, 338], ["kwargs.pop", "set", "kwargs.items", "kwargs.pop", "int", "setattr", "isinstance", "isinstance", "all", "isinstance"], "methods", ["None"], ["", "def", "__init__", "(", "self", ",", "max_len", "=", "None", ",", "**", "kwargs", ")", ":", "\n", "        ", "self", ".", "_bos_token", "=", "None", "\n", "self", ".", "_eos_token", "=", "None", "\n", "self", ".", "_unk_token", "=", "None", "\n", "self", ".", "_sep_token", "=", "None", "\n", "self", ".", "_pad_token", "=", "None", "\n", "self", ".", "_cls_token", "=", "None", "\n", "self", ".", "_mask_token", "=", "None", "\n", "self", ".", "_pad_token_type_id", "=", "0", "\n", "self", ".", "_additional_special_tokens", "=", "[", "]", "\n", "\n", "if", "\"model_max_length\"", "in", "kwargs", ":", "\n", "            ", "max_len", "=", "kwargs", ".", "pop", "(", "\"model_max_length\"", ")", "\n", "", "self", ".", "max_len", "=", "max_len", "if", "max_len", "is", "not", "None", "else", "int", "(", "512", ")", "\n", "\n", "# Padding side is right by default and over-riden in subclasses. If specified in the kwargs, it is changed.", "\n", "self", ".", "padding_side", "=", "kwargs", ".", "pop", "(", "\"padding_side\"", ",", "self", ".", "padding_side", ")", "\n", "\n", "# Added tokens", "\n", "self", ".", "added_tokens_encoder", "=", "{", "}", "\n", "self", ".", "unique_added_tokens_encoder", "=", "set", "(", ")", "\n", "self", ".", "added_tokens_decoder", "=", "{", "}", "\n", "\n", "# inputs and kwargs for saving and re-loading (see ``from_pretrained`` and ``save_pretrained``)", "\n", "self", ".", "init_inputs", "=", "(", ")", "\n", "self", ".", "init_kwargs", "=", "{", "}", "\n", "\n", "for", "key", ",", "value", "in", "kwargs", ".", "items", "(", ")", ":", "\n", "            ", "if", "key", "in", "self", ".", "SPECIAL_TOKENS_ATTRIBUTES", ":", "\n", "                ", "if", "key", "==", "\"additional_special_tokens\"", ":", "\n", "                    ", "assert", "isinstance", "(", "value", ",", "(", "list", ",", "tuple", ")", ")", "and", "all", "(", "isinstance", "(", "t", ",", "str", ")", "for", "t", "in", "value", ")", "\n", "", "else", ":", "\n", "                    ", "assert", "isinstance", "(", "value", ",", "str", ")", "\n", "", "setattr", "(", "self", ",", "key", ",", "value", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.neulab_awesome-align.awesome_align.tokenization_utils.PreTrainedTokenizer.from_pretrained": [[339, 393], ["cls._from_pretrained"], "methods", ["home.repos.pwc.inspect_result.neulab_awesome-align.awesome_align.tokenization_utils.PreTrainedTokenizer._from_pretrained"], ["", "", "", "@", "classmethod", "\n", "def", "from_pretrained", "(", "cls", ",", "*", "inputs", ",", "**", "kwargs", ")", ":", "\n", "        ", "r\"\"\"\n        Instantiate a :class:`~transformers.PreTrainedTokenizer` (or a derived class) from a predefined tokenizer.\n\n        Args:\n            pretrained_model_name_or_path: either:\n\n                - a string with the `shortcut name` of a predefined tokenizer to load from cache or download, e.g.: ``bert-base-uncased``.\n                - a string with the `identifier name` of a predefined tokenizer that was user-uploaded to our S3, e.g.: ``dbmdz/bert-base-german-cased``.\n                - a path to a `directory` containing vocabulary files required by the tokenizer, for instance saved using the :func:`~transformers.PreTrainedTokenizer.save_pretrained` method, e.g.: ``./my_model_directory/``.\n                - (not applicable to all derived classes, deprecated) a path or url to a single saved vocabulary file if and only if the tokenizer only requires a single vocabulary file (e.g. Bert, XLNet), e.g.: ``./my_model_directory/vocab.txt``.\n\n            cache_dir: (`optional`) string:\n                Path to a directory in which a downloaded predefined tokenizer vocabulary files should be cached if the standard cache should not be used.\n\n            force_download: (`optional`) boolean, default False:\n                Force to (re-)download the vocabulary files and override the cached versions if they exists.\n\n            resume_download: (`optional`) boolean, default False:\n                Do not delete incompletely recieved file. Attempt to resume the download if such a file exists.\n\n            proxies: (`optional`) dict, default None:\n                A dictionary of proxy servers to use by protocol or endpoint, e.g.: {'http': 'foo.bar:3128', 'http://hostname': 'foo.bar:4012'}.\n                The proxies are used on each request.\n\n            inputs: (`optional`) positional arguments: will be passed to the Tokenizer ``__init__`` method.\n\n            kwargs: (`optional`) keyword arguments: will be passed to the Tokenizer ``__init__`` method. Can be used to set special tokens like ``bos_token``, ``eos_token``, ``unk_token``, ``sep_token``, ``pad_token``, ``cls_token``, ``mask_token``, ``additional_special_tokens``. See parameters in the doc string of :class:`~transformers.PreTrainedTokenizer` for details.\n\n        Examples::\n\n            # We can't instantiate directly the base class `PreTrainedTokenizer` so let's show our examples on a derived class: BertTokenizer\n\n            # Download vocabulary from S3 and cache.\n            tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n\n            # Download vocabulary from S3 (user-uploaded) and cache.\n            tokenizer = BertTokenizer.from_pretrained('dbmdz/bert-base-german-cased')\n\n            # If vocabulary files are in a directory (e.g. tokenizer was saved using `save_pretrained('./test/saved_model/')`)\n            tokenizer = BertTokenizer.from_pretrained('./test/saved_model/')\n\n            # If the tokenizer uses a single vocabulary file, you can point directly to this file\n            tokenizer = BertTokenizer.from_pretrained('./test/saved_model/my_vocab.txt')\n\n            # You can link tokens to special vocabulary when instantiating\n            tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', unk_token='<unk>')\n            # You should be sure '<unk>' is in the vocabulary when doing that.\n            # Otherwise use tokenizer.add_special_tokens({'unk_token': '<unk>'}) instead)\n            assert tokenizer.unk_token == '<unk>'\n\n        \"\"\"", "\n", "return", "cls", ".", "_from_pretrained", "(", "*", "inputs", ",", "**", "kwargs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.neulab_awesome-align.awesome_align.tokenization_utils.PreTrainedTokenizer._from_pretrained": [[394, 567], ["kwargs.pop", "kwargs.pop", "kwargs.pop", "kwargs.pop", "kwargs.pop", "list", "all", "vocab_files.items", "resolved_vocab_files.pop", "json.load.update", "resolved_vocab_files.pop", "resolved_vocab_files.pop", "resolved_vocab_files.items", "cls.unique_added_tokens_encoder.update", "cls.max_model_input_sizes.keys", "cls.pretrained_vocab_files_map.items", "logger.info", "vocab_files.items", "EnvironmentError", "json.load.pop", "json.load.items", "cls", "set", "cls.added_tokens_encoder.update", "cls.added_tokens_decoder.update", "cls.unique_added_tokens_encoder.update", "cls.pretrained_init_configuration[].copy", "os.path.isfile", "awesome_align.file_utils.is_remote_url", "logger.warning", "EnvironmentError", "logger.info", "logger.info", "open", "json.load", "isinstance", "min", "open", "json.load", "OSError", "open", "json.load", "set", "len", "ValueError", "list", "os.path.isdir", "awesome_align.file_utils.cached_path", "resolved_vocab_files.values", "list", "json.load.get", "json.load.items", "cls.added_tokens_encoder.keys", "cls.vocab_files_names.keys", "os.path.join", "awesome_align.file_utils.hf_bucket_url", "list", "cls.vocab_files_names.values", "int", "os.path.exists", "logger.info", "cls.vocab_files_names.values"], "methods", ["home.repos.pwc.inspect_result.neulab_awesome-align.awesome_align.file_utils.is_remote_url", "home.repos.pwc.inspect_result.neulab_awesome-align.awesome_align.file_utils.cached_path", "home.repos.pwc.inspect_result.neulab_awesome-align.awesome_align.file_utils.hf_bucket_url"], ["", "@", "classmethod", "\n", "def", "_from_pretrained", "(", "cls", ",", "pretrained_model_name_or_path", ",", "*", "init_inputs", ",", "**", "kwargs", ")", ":", "\n", "        ", "cache_dir", "=", "kwargs", ".", "pop", "(", "\"cache_dir\"", ",", "None", ")", "\n", "force_download", "=", "kwargs", ".", "pop", "(", "\"force_download\"", ",", "False", ")", "\n", "resume_download", "=", "kwargs", ".", "pop", "(", "\"resume_download\"", ",", "False", ")", "\n", "proxies", "=", "kwargs", ".", "pop", "(", "\"proxies\"", ",", "None", ")", "\n", "local_files_only", "=", "kwargs", ".", "pop", "(", "\"local_files_only\"", ",", "False", ")", "\n", "\n", "s3_models", "=", "list", "(", "cls", ".", "max_model_input_sizes", ".", "keys", "(", ")", ")", "\n", "vocab_files", "=", "{", "}", "\n", "init_configuration", "=", "{", "}", "\n", "if", "pretrained_model_name_or_path", "in", "s3_models", ":", "\n", "# Get the vocabulary from AWS S3 bucket", "\n", "            ", "for", "file_id", ",", "map_list", "in", "cls", ".", "pretrained_vocab_files_map", ".", "items", "(", ")", ":", "\n", "                ", "vocab_files", "[", "file_id", "]", "=", "map_list", "[", "pretrained_model_name_or_path", "]", "\n", "", "if", "(", "\n", "cls", ".", "pretrained_init_configuration", "\n", "and", "pretrained_model_name_or_path", "in", "cls", ".", "pretrained_init_configuration", "\n", ")", ":", "\n", "                ", "init_configuration", "=", "cls", ".", "pretrained_init_configuration", "[", "pretrained_model_name_or_path", "]", ".", "copy", "(", ")", "\n", "", "", "else", ":", "\n", "# Get the vocabulary from local files", "\n", "            ", "logger", ".", "info", "(", "\n", "\"Model name '{}' not found in model shortcut name list ({}). \"", "\n", "\"Assuming '{}' is a path, a model identifier, or url to a directory containing tokenizer files.\"", ".", "format", "(", "\n", "pretrained_model_name_or_path", ",", "\", \"", ".", "join", "(", "s3_models", ")", ",", "pretrained_model_name_or_path", "\n", ")", "\n", ")", "\n", "\n", "if", "os", ".", "path", ".", "isfile", "(", "pretrained_model_name_or_path", ")", "or", "is_remote_url", "(", "pretrained_model_name_or_path", ")", ":", "\n", "                ", "if", "len", "(", "cls", ".", "vocab_files_names", ")", ">", "1", ":", "\n", "                    ", "raise", "ValueError", "(", "\n", "\"Calling {}.from_pretrained() with the path to a single file or url is not supported.\"", "\n", "\"Use a model identifier or the path to a directory instead.\"", ".", "format", "(", "cls", ".", "__name__", ")", "\n", ")", "\n", "", "logger", ".", "warning", "(", "\n", "\"Calling {}.from_pretrained() with the path to a single file or url is deprecated\"", ".", "format", "(", "\n", "cls", ".", "__name__", "\n", ")", "\n", ")", "\n", "file_id", "=", "list", "(", "cls", ".", "vocab_files_names", ".", "keys", "(", ")", ")", "[", "0", "]", "\n", "vocab_files", "[", "file_id", "]", "=", "pretrained_model_name_or_path", "\n", "", "else", ":", "\n", "# At this point pretrained_model_name_or_path is either a directory or a model identifier name", "\n", "                ", "additional_files_names", "=", "{", "\n", "\"added_tokens_file\"", ":", "ADDED_TOKENS_FILE", ",", "\n", "\"special_tokens_map_file\"", ":", "SPECIAL_TOKENS_MAP_FILE", ",", "\n", "\"tokenizer_config_file\"", ":", "TOKENIZER_CONFIG_FILE", ",", "\n", "}", "\n", "# Look for the tokenizer main vocabulary files + the additional tokens files", "\n", "for", "file_id", ",", "file_name", "in", "{", "**", "cls", ".", "vocab_files_names", ",", "**", "additional_files_names", "}", ".", "items", "(", ")", ":", "\n", "                    ", "if", "os", ".", "path", ".", "isdir", "(", "pretrained_model_name_or_path", ")", ":", "\n", "                        ", "full_file_name", "=", "os", ".", "path", ".", "join", "(", "pretrained_model_name_or_path", ",", "file_name", ")", "\n", "if", "not", "os", ".", "path", ".", "exists", "(", "full_file_name", ")", ":", "\n", "                            ", "logger", ".", "info", "(", "\"Didn't find file {}. We won't load it.\"", ".", "format", "(", "full_file_name", ")", ")", "\n", "full_file_name", "=", "None", "\n", "", "", "else", ":", "\n", "                        ", "full_file_name", "=", "hf_bucket_url", "(", "pretrained_model_name_or_path", ",", "postfix", "=", "file_name", ")", "\n", "\n", "", "vocab_files", "[", "file_id", "]", "=", "full_file_name", "\n", "\n", "# Get files from url, cache, or disk depending on the case", "\n", "", "", "", "try", ":", "\n", "            ", "resolved_vocab_files", "=", "{", "}", "\n", "for", "file_id", ",", "file_path", "in", "vocab_files", ".", "items", "(", ")", ":", "\n", "                ", "if", "file_path", "is", "None", ":", "\n", "                    ", "resolved_vocab_files", "[", "file_id", "]", "=", "None", "\n", "", "else", ":", "\n", "                    ", "resolved_vocab_files", "[", "file_id", "]", "=", "cached_path", "(", "\n", "file_path", ",", "\n", "cache_dir", "=", "cache_dir", ",", "\n", "force_download", "=", "force_download", ",", "\n", "proxies", "=", "proxies", ",", "\n", "resume_download", "=", "resume_download", ",", "\n", "local_files_only", "=", "local_files_only", ",", "\n", ")", "\n", "", "", "", "except", "EnvironmentError", ":", "\n", "            ", "if", "pretrained_model_name_or_path", "in", "s3_models", ":", "\n", "                ", "msg", "=", "\"Couldn't reach server at '{}' to download vocabulary files.\"", "\n", "", "else", ":", "\n", "                ", "msg", "=", "(", "\n", "\"Model name '{}' was not found in tokenizers model name list ({}). \"", "\n", "\"We assumed '{}' was a path or url to a directory containing vocabulary files \"", "\n", "\"named {}, but couldn't find such vocabulary files at this path or url.\"", ".", "format", "(", "\n", "pretrained_model_name_or_path", ",", "\n", "\", \"", ".", "join", "(", "s3_models", ")", ",", "\n", "pretrained_model_name_or_path", ",", "\n", "list", "(", "cls", ".", "vocab_files_names", ".", "values", "(", ")", ")", ",", "\n", ")", "\n", ")", "\n", "\n", "", "raise", "EnvironmentError", "(", "msg", ")", "\n", "\n", "", "if", "all", "(", "full_file_name", "is", "None", "for", "full_file_name", "in", "resolved_vocab_files", ".", "values", "(", ")", ")", ":", "\n", "            ", "raise", "EnvironmentError", "(", "\n", "\"Model name '{}' was not found in tokenizers model name list ({}). \"", "\n", "\"We assumed '{}' was a path, a model identifier, or url to a directory containing vocabulary files \"", "\n", "\"named {} but couldn't find such vocabulary files at this path or url.\"", ".", "format", "(", "\n", "pretrained_model_name_or_path", ",", "\n", "\", \"", ".", "join", "(", "s3_models", ")", ",", "\n", "pretrained_model_name_or_path", ",", "\n", "list", "(", "cls", ".", "vocab_files_names", ".", "values", "(", ")", ")", ",", "\n", ")", "\n", ")", "\n", "\n", "", "for", "file_id", ",", "file_path", "in", "vocab_files", ".", "items", "(", ")", ":", "\n", "            ", "if", "file_path", "==", "resolved_vocab_files", "[", "file_id", "]", ":", "\n", "                ", "logger", ".", "info", "(", "\"loading file {}\"", ".", "format", "(", "file_path", ")", ")", "\n", "", "else", ":", "\n", "                ", "logger", ".", "info", "(", "\"loading file {} from cache at {}\"", ".", "format", "(", "file_path", ",", "resolved_vocab_files", "[", "file_id", "]", ")", ")", "\n", "\n", "# Prepare tokenizer initialization kwargs", "\n", "# Did we saved some inputs and kwargs to reload ?", "\n", "", "", "tokenizer_config_file", "=", "resolved_vocab_files", ".", "pop", "(", "\"tokenizer_config_file\"", ",", "None", ")", "\n", "if", "tokenizer_config_file", "is", "not", "None", ":", "\n", "            ", "with", "open", "(", "tokenizer_config_file", ",", "encoding", "=", "\"utf-8\"", ")", "as", "tokenizer_config_handle", ":", "\n", "                ", "init_kwargs", "=", "json", ".", "load", "(", "tokenizer_config_handle", ")", "\n", "", "saved_init_inputs", "=", "init_kwargs", ".", "pop", "(", "\"init_inputs\"", ",", "(", ")", ")", "\n", "if", "not", "init_inputs", ":", "\n", "                ", "init_inputs", "=", "saved_init_inputs", "\n", "", "", "else", ":", "\n", "            ", "init_kwargs", "=", "init_configuration", "\n", "\n", "# Update with newly provided kwargs", "\n", "", "init_kwargs", ".", "update", "(", "kwargs", ")", "\n", "\n", "# Set max length if needed", "\n", "if", "pretrained_model_name_or_path", "in", "cls", ".", "max_model_input_sizes", ":", "\n", "# if we're using a pretrained model, ensure the tokenizer", "\n", "# wont index sequences longer than the number of positional embeddings", "\n", "            ", "max_len", "=", "cls", ".", "max_model_input_sizes", "[", "pretrained_model_name_or_path", "]", "\n", "if", "max_len", "is", "not", "None", "and", "isinstance", "(", "max_len", ",", "(", "int", ",", "float", ")", ")", ":", "\n", "                ", "init_kwargs", "[", "\"max_len\"", "]", "=", "min", "(", "init_kwargs", ".", "get", "(", "\"max_len\"", ",", "int", "(", "1e12", ")", ")", ",", "max_len", ")", "\n", "\n", "# Merge resolved_vocab_files arguments in init_kwargs.", "\n", "", "", "added_tokens_file", "=", "resolved_vocab_files", ".", "pop", "(", "\"added_tokens_file\"", ",", "None", ")", "\n", "special_tokens_map_file", "=", "resolved_vocab_files", ".", "pop", "(", "\"special_tokens_map_file\"", ",", "None", ")", "\n", "for", "args_name", ",", "file_path", "in", "resolved_vocab_files", ".", "items", "(", ")", ":", "\n", "            ", "if", "args_name", "not", "in", "init_kwargs", ":", "\n", "                ", "init_kwargs", "[", "args_name", "]", "=", "file_path", "\n", "", "", "if", "special_tokens_map_file", "is", "not", "None", ":", "\n", "            ", "with", "open", "(", "special_tokens_map_file", ",", "encoding", "=", "\"utf-8\"", ")", "as", "special_tokens_map_handle", ":", "\n", "                ", "special_tokens_map", "=", "json", ".", "load", "(", "special_tokens_map_handle", ")", "\n", "", "for", "key", ",", "value", "in", "special_tokens_map", ".", "items", "(", ")", ":", "\n", "                ", "if", "key", "not", "in", "init_kwargs", ":", "\n", "                    ", "init_kwargs", "[", "key", "]", "=", "value", "\n", "\n", "# Instantiate tokenizer.", "\n", "", "", "", "try", ":", "\n", "            ", "tokenizer", "=", "cls", "(", "*", "init_inputs", ",", "**", "init_kwargs", ")", "\n", "", "except", "OSError", ":", "\n", "            ", "raise", "OSError", "(", "\n", "\"Unable to load vocabulary from file. \"", "\n", "\"Please check that the provided vocabulary is accessible and not corrupted.\"", "\n", ")", "\n", "\n", "# Save inputs and kwargs for saving and re-loading with ``save_pretrained``", "\n", "", "tokenizer", ".", "init_inputs", "=", "init_inputs", "\n", "tokenizer", ".", "init_kwargs", "=", "init_kwargs", "\n", "\n", "# update unique_added_tokens_encoder with special tokens for correct tokenization", "\n", "tokenizer", ".", "unique_added_tokens_encoder", ".", "update", "(", "set", "(", "tokenizer", ".", "all_special_tokens", ")", ")", "\n", "\n", "# Add supplementary tokens.", "\n", "if", "added_tokens_file", "is", "not", "None", ":", "\n", "            ", "with", "open", "(", "added_tokens_file", ",", "encoding", "=", "\"utf-8\"", ")", "as", "added_tokens_handle", ":", "\n", "                ", "added_tok_encoder", "=", "json", ".", "load", "(", "added_tokens_handle", ")", "\n", "", "added_tok_decoder", "=", "{", "v", ":", "k", "for", "k", ",", "v", "in", "added_tok_encoder", ".", "items", "(", ")", "}", "\n", "tokenizer", ".", "added_tokens_encoder", ".", "update", "(", "added_tok_encoder", ")", "\n", "tokenizer", ".", "added_tokens_decoder", ".", "update", "(", "added_tok_decoder", ")", "\n", "tokenizer", ".", "unique_added_tokens_encoder", ".", "update", "(", "set", "(", "tokenizer", ".", "added_tokens_encoder", ".", "keys", "(", ")", ")", ")", "\n", "\n", "", "return", "tokenizer", "\n", "\n"]], "home.repos.pwc.inspect_result.neulab_awesome-align.awesome_align.tokenization_utils.PreTrainedTokenizer.save_pretrained": [[568, 607], ["os.path.join", "os.path.join", "os.path.join", "copy.deepcopy", "tokenization_utils.PreTrainedTokenizer.vocab_files_names.keys", "tokenization_utils.PreTrainedTokenizer.save_vocabulary", "os.path.isdir", "logger.error", "len", "copy.deepcopy", "copy.deepcopy.pop", "open", "f.write", "open", "f.write", "len", "json.dumps", "json.dumps", "open", "json.dumps", "f.write"], "methods", ["home.repos.pwc.inspect_result.neulab_awesome-align.awesome_align.tokenization_utils.PreTrainedTokenizerFast.save_vocabulary"], ["", "def", "save_pretrained", "(", "self", ",", "save_directory", ")", ":", "\n", "        ", "\"\"\" Save the tokenizer vocabulary files together with:\n                - added tokens,\n                - special-tokens-to-class-attributes-mapping,\n                - tokenizer instantiation positional and keywords inputs (e.g. do_lower_case for Bert).\n\n            This won't save modifications other than (added tokens and special token mapping) you may have\n            applied to the tokenizer after the instantiation (e.g. modifying tokenizer.do_lower_case after creation).\n\n            This method make sure the full tokenizer can then be re-loaded using the :func:`~transformers.PreTrainedTokenizer.from_pretrained` class method.\n        \"\"\"", "\n", "if", "not", "os", ".", "path", ".", "isdir", "(", "save_directory", ")", ":", "\n", "            ", "logger", ".", "error", "(", "\"Saving directory ({}) should be a directory\"", ".", "format", "(", "save_directory", ")", ")", "\n", "return", "\n", "\n", "", "special_tokens_map_file", "=", "os", ".", "path", ".", "join", "(", "save_directory", ",", "SPECIAL_TOKENS_MAP_FILE", ")", "\n", "added_tokens_file", "=", "os", ".", "path", ".", "join", "(", "save_directory", ",", "ADDED_TOKENS_FILE", ")", "\n", "tokenizer_config_file", "=", "os", ".", "path", ".", "join", "(", "save_directory", ",", "TOKENIZER_CONFIG_FILE", ")", "\n", "\n", "tokenizer_config", "=", "copy", ".", "deepcopy", "(", "self", ".", "init_kwargs", ")", "\n", "if", "len", "(", "self", ".", "init_inputs", ")", ">", "0", ":", "\n", "            ", "tokenizer_config", "[", "\"init_inputs\"", "]", "=", "copy", ".", "deepcopy", "(", "self", ".", "init_inputs", ")", "\n", "", "for", "file_id", "in", "self", ".", "vocab_files_names", ".", "keys", "(", ")", ":", "\n", "            ", "tokenizer_config", ".", "pop", "(", "file_id", ",", "None", ")", "\n", "\n", "", "with", "open", "(", "tokenizer_config_file", ",", "\"w\"", ",", "encoding", "=", "\"utf-8\"", ")", "as", "f", ":", "\n", "            ", "f", ".", "write", "(", "json", ".", "dumps", "(", "tokenizer_config", ",", "ensure_ascii", "=", "False", ")", ")", "\n", "\n", "", "with", "open", "(", "special_tokens_map_file", ",", "\"w\"", ",", "encoding", "=", "\"utf-8\"", ")", "as", "f", ":", "\n", "            ", "f", ".", "write", "(", "json", ".", "dumps", "(", "self", ".", "special_tokens_map", ",", "ensure_ascii", "=", "False", ")", ")", "\n", "\n", "", "if", "len", "(", "self", ".", "added_tokens_encoder", ")", ">", "0", ":", "\n", "            ", "with", "open", "(", "added_tokens_file", ",", "\"w\"", ",", "encoding", "=", "\"utf-8\"", ")", "as", "f", ":", "\n", "                ", "out_str", "=", "json", ".", "dumps", "(", "self", ".", "added_tokens_encoder", ",", "ensure_ascii", "=", "False", ")", "\n", "f", ".", "write", "(", "out_str", ")", "\n", "\n", "", "", "vocab_files", "=", "self", ".", "save_vocabulary", "(", "save_directory", ")", "\n", "\n", "return", "vocab_files", "+", "(", "special_tokens_map_file", ",", "added_tokens_file", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.neulab_awesome-align.awesome_align.tokenization_utils.PreTrainedTokenizer.save_vocabulary": [[608, 615], ["None"], "methods", ["None"], ["", "def", "save_vocabulary", "(", "self", ",", "save_directory", ")", ":", "\n", "        ", "\"\"\" Save the tokenizer vocabulary to a directory. This method does *NOT* save added tokens\n            and special token mappings.\n\n            Please use :func:`~transformers.PreTrainedTokenizer.save_pretrained` `()` to save the full Tokenizer state if you want to reload it using the :func:`~transformers.PreTrainedTokenizer.from_pretrained` class method.\n        \"\"\"", "\n", "raise", "NotImplementedError", "\n", "\n"]], "home.repos.pwc.inspect_result.neulab_awesome-align.awesome_align.tokenization_utils.PreTrainedTokenizer.vocab_size": [[616, 619], ["None"], "methods", ["None"], ["", "def", "vocab_size", "(", "self", ")", ":", "\n", "        ", "\"\"\" Size of the base vocabulary (without the added tokens) \"\"\"", "\n", "raise", "NotImplementedError", "\n", "\n"]], "home.repos.pwc.inspect_result.neulab_awesome-align.awesome_align.tokenization_utils.PreTrainedTokenizer.__len__": [[620, 623], ["len"], "methods", ["None"], ["", "def", "__len__", "(", "self", ")", ":", "\n", "        ", "\"\"\" Size of the full vocabulary with the added tokens \"\"\"", "\n", "return", "self", ".", "vocab_size", "+", "len", "(", "self", ".", "added_tokens_encoder", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.neulab_awesome-align.awesome_align.tokenization_utils.PreTrainedTokenizer.add_tokens": [[624, 671], ["dict", "tokenization_utils.PreTrainedTokenizer.added_tokens_encoder.update", "set().union", "tokenization_utils.PreTrainedTokenizer.added_tokens_decoder.update", "len", "isinstance", "isinstance", "set", "tokenization_utils.PreTrainedTokenizer.init_kwargs.get", "token.lower.lower.lower", "to_add_tokens.append", "logger.info", "dict.items", "set", "tokenization_utils.PreTrainedTokenizer.convert_tokens_to_ids", "tokenization_utils.PreTrainedTokenizer.convert_tokens_to_ids", "enumerate", "tokenization_utils.PreTrainedTokenizer.added_tokens_encoder.keys", "len"], "methods", ["home.repos.pwc.inspect_result.neulab_awesome-align.awesome_align.tokenization_utils.PreTrainedTokenizer.convert_tokens_to_ids", "home.repos.pwc.inspect_result.neulab_awesome-align.awesome_align.tokenization_utils.PreTrainedTokenizer.convert_tokens_to_ids"], ["", "def", "add_tokens", "(", "self", ",", "new_tokens", ")", ":", "\n", "        ", "\"\"\"\n        Add a list of new tokens to the tokenizer class. If the new tokens are not in the\n        vocabulary, they are added to it with indices starting from length of the current vocabulary.\n\n        Args:\n            new_tokens: string or list of string. Each string is a token to add. Tokens are only added if they are not already in the vocabulary (tested by checking if the tokenizer assign the index of the ``unk_token`` to them).\n\n        Returns:\n            Number of tokens added to the vocabulary.\n\n        Examples::\n\n            # Let's see how to increase the vocabulary of Bert model and tokenizer\n            tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n            model = BertModel.from_pretrained('bert-base-uncased')\n\n            num_added_toks = tokenizer.add_tokens(['new_tok1', 'my_new-tok2'])\n            print('We have added', num_added_toks, 'tokens')\n            model.resize_token_embeddings(len(tokenizer))  # Notice: resize_token_embeddings expect to receive the full size of the new vocabulary, i.e. the length of the tokenizer.\n        \"\"\"", "\n", "if", "not", "new_tokens", ":", "\n", "            ", "return", "0", "\n", "\n", "", "if", "not", "isinstance", "(", "new_tokens", ",", "list", ")", ":", "\n", "            ", "new_tokens", "=", "[", "new_tokens", "]", "\n", "\n", "", "to_add_tokens", "=", "[", "]", "\n", "for", "token", "in", "new_tokens", ":", "\n", "            ", "assert", "isinstance", "(", "token", ",", "str", ")", "\n", "if", "self", ".", "init_kwargs", ".", "get", "(", "\"do_lower_case\"", ",", "False", ")", "and", "token", "not", "in", "self", ".", "all_special_tokens", ":", "\n", "                ", "token", "=", "token", ".", "lower", "(", ")", "\n", "", "if", "(", "\n", "token", "!=", "self", ".", "unk_token", "\n", "and", "self", ".", "convert_tokens_to_ids", "(", "token", ")", "==", "self", ".", "convert_tokens_to_ids", "(", "self", ".", "unk_token", ")", "\n", "and", "token", "not", "in", "to_add_tokens", "\n", ")", ":", "\n", "                ", "to_add_tokens", ".", "append", "(", "token", ")", "\n", "logger", ".", "info", "(", "\"Adding %s to the vocabulary\"", ",", "token", ")", "\n", "\n", "", "", "added_tok_encoder", "=", "dict", "(", "(", "tok", ",", "len", "(", "self", ")", "+", "i", ")", "for", "i", ",", "tok", "in", "enumerate", "(", "to_add_tokens", ")", ")", "\n", "added_tok_decoder", "=", "{", "v", ":", "k", "for", "k", ",", "v", "in", "added_tok_encoder", ".", "items", "(", ")", "}", "\n", "self", ".", "added_tokens_encoder", ".", "update", "(", "added_tok_encoder", ")", "\n", "self", ".", "unique_added_tokens_encoder", "=", "set", "(", "self", ".", "added_tokens_encoder", ".", "keys", "(", ")", ")", ".", "union", "(", "set", "(", "self", ".", "all_special_tokens", ")", ")", "\n", "self", ".", "added_tokens_decoder", ".", "update", "(", "added_tok_decoder", ")", "\n", "\n", "return", "len", "(", "to_add_tokens", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.neulab_awesome-align.awesome_align.tokenization_utils.PreTrainedTokenizer.num_added_tokens": [[672, 690], ["len", "tokenization_utils.PreTrainedTokenizer.build_inputs_with_special_tokens"], "methods", ["home.repos.pwc.inspect_result.neulab_awesome-align.awesome_align.tokenization_utils.PreTrainedTokenizerFast.build_inputs_with_special_tokens"], ["", "def", "num_added_tokens", "(", "self", ",", "pair", "=", "False", ")", ":", "\n", "        ", "\"\"\"\n        Returns the number of added tokens when encoding a sequence with special tokens.\n\n        Note:\n            This encodes inputs and checks the number of added tokens, and is therefore not efficient. Do not put this\n            inside your training loop.\n\n        Args:\n            pair: Returns the number of added tokens in the case of a sequence pair if set to True, returns the\n                number of added tokens in the case of a single sequence if set to False.\n\n        Returns:\n            Number of tokens added to sequences\n        \"\"\"", "\n", "token_ids_0", "=", "[", "]", "\n", "token_ids_1", "=", "[", "]", "\n", "return", "len", "(", "self", ".", "build_inputs_with_special_tokens", "(", "token_ids_0", ",", "token_ids_1", "if", "pair", "else", "None", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.neulab_awesome-align.awesome_align.tokenization_utils.PreTrainedTokenizer.add_special_tokens": [[691, 744], ["special_tokens_dict.items", "logger.info", "setattr", "tokenization_utils.PreTrainedTokenizer.add_tokens", "isinstance", "tokenization_utils.PreTrainedTokenizer.add_tokens", "isinstance", "all", "isinstance"], "methods", ["home.repos.pwc.inspect_result.neulab_awesome-align.awesome_align.tokenization_utils.PreTrainedTokenizerFast.add_tokens", "home.repos.pwc.inspect_result.neulab_awesome-align.awesome_align.tokenization_utils.PreTrainedTokenizerFast.add_tokens"], ["", "def", "add_special_tokens", "(", "self", ",", "special_tokens_dict", ")", ":", "\n", "        ", "\"\"\"\n        Add a dictionary of special tokens (eos, pad, cls...) to the encoder and link them\n        to class attributes. If special tokens are NOT in the vocabulary, they are added\n        to it (indexed starting from the last index of the current vocabulary).\n\n        Using `add_special_tokens` will ensure your special tokens can be used in several ways:\n\n        - special tokens are carefully handled by the tokenizer (they are never split)\n        - you can easily refer to special tokens using tokenizer class attributes like `tokenizer.cls_token`. This makes it easy to develop model-agnostic training and fine-tuning scripts.\n\n        When possible, special tokens are already registered for provided pretrained models (ex: BertTokenizer cls_token is already registered to be '[CLS]' and XLM's one is also registered to be '</s>')\n\n        Args:\n            special_tokens_dict: dict of string. Keys should be in the list of predefined special attributes:\n                [``bos_token``, ``eos_token``, ``unk_token``, ``sep_token``, ``pad_token``, ``cls_token``, ``mask_token``,\n                ``additional_special_tokens``].\n\n                Tokens are only added if they are not already in the vocabulary (tested by checking if the tokenizer assign the index of the ``unk_token`` to them).\n\n        Returns:\n            Number of tokens added to the vocabulary.\n\n        Examples::\n\n            # Let's see how to add a new classification token to GPT-2\n            tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n            model = GPT2Model.from_pretrained('gpt2')\n\n            special_tokens_dict = {'cls_token': '<CLS>'}\n\n            num_added_toks = tokenizer.add_special_tokens(special_tokens_dict)\n            print('We have added', num_added_toks, 'tokens')\n            model.resize_token_embeddings(len(tokenizer))  # Notice: resize_token_embeddings expect to receive the full size of the new vocabulary, i.e. the length of the tokenizer.\n\n            assert tokenizer.cls_token == '<CLS>'\n        \"\"\"", "\n", "if", "not", "special_tokens_dict", ":", "\n", "            ", "return", "0", "\n", "\n", "", "added_tokens", "=", "0", "\n", "for", "key", ",", "value", "in", "special_tokens_dict", ".", "items", "(", ")", ":", "\n", "            ", "assert", "key", "in", "self", ".", "SPECIAL_TOKENS_ATTRIBUTES", "\n", "if", "key", "==", "\"additional_special_tokens\"", ":", "\n", "                ", "assert", "isinstance", "(", "value", ",", "(", "list", ",", "tuple", ")", ")", "and", "all", "(", "isinstance", "(", "t", ",", "str", ")", "for", "t", "in", "value", ")", "\n", "added_tokens", "+=", "self", ".", "add_tokens", "(", "value", ")", "\n", "", "else", ":", "\n", "                ", "assert", "isinstance", "(", "value", ",", "str", ")", "\n", "added_tokens", "+=", "self", ".", "add_tokens", "(", "[", "value", "]", ")", "\n", "", "logger", ".", "info", "(", "\"Assigning %s to the %s key of the tokenizer\"", ",", "value", ",", "key", ")", "\n", "setattr", "(", "self", ",", "key", ",", "value", ")", "\n", "\n", "", "return", "added_tokens", "\n", "\n"]], "home.repos.pwc.inspect_result.neulab_awesome-align.awesome_align.tokenization_utils.PreTrainedTokenizer.tokenize": [[745, 816], ["tokenization_utils.PreTrainedTokenizer.prepare_for_tokenization", "tokenization_utils.PreTrainedTokenizer.init_kwargs.get", "tokenization_utils.PreTrainedTokenizer.tokenize.split_on_tokens"], "methods", ["home.repos.pwc.inspect_result.neulab_awesome-align.awesome_align.tokenization_utils.PreTrainedTokenizer.prepare_for_tokenization"], ["", "def", "tokenize", "(", "self", ",", "text", ",", "**", "kwargs", ")", ":", "\n", "        ", "\"\"\" Converts a string in a sequence of tokens (string), using the tokenizer.\n            Split in words for word-based vocabulary or sub-words for sub-word-based\n            vocabularies (BPE/SentencePieces/WordPieces).\n\n            Take care of added tokens.\n\n            text: The sequence to be encoded.\n            add_prefix_space: Only applies to GPT-2 and RoBERTa tokenizers. When `True`, this ensures that the sequence\n                begins with an empty space. False by default except for when using RoBERTa with `add_special_tokens=True`.\n            **kwargs: passed to the `prepare_for_tokenization` preprocessing method.\n        \"\"\"", "\n", "all_special_tokens", "=", "self", ".", "all_special_tokens", "\n", "text", "=", "self", ".", "prepare_for_tokenization", "(", "text", ",", "**", "kwargs", ")", "\n", "\n", "def", "lowercase_text", "(", "t", ")", ":", "\n", "# convert non-special tokens to lowercase", "\n", "            ", "escaped_special_toks", "=", "[", "re", ".", "escape", "(", "s_tok", ")", "for", "s_tok", "in", "all_special_tokens", "]", "\n", "pattern", "=", "r\"(\"", "+", "r\"|\"", ".", "join", "(", "escaped_special_toks", ")", "+", "r\")|\"", "+", "r\"(.+?)\"", "\n", "return", "re", ".", "sub", "(", "pattern", ",", "lambda", "m", ":", "m", ".", "groups", "(", ")", "[", "0", "]", "or", "m", ".", "groups", "(", ")", "[", "1", "]", ".", "lower", "(", ")", ",", "t", ")", "\n", "\n", "", "if", "self", ".", "init_kwargs", ".", "get", "(", "\"do_lower_case\"", ",", "False", ")", ":", "\n", "            ", "text", "=", "lowercase_text", "(", "text", ")", "\n", "\n", "", "def", "split_on_token", "(", "tok", ",", "text", ")", ":", "\n", "            ", "result", "=", "[", "]", "\n", "split_text", "=", "text", ".", "split", "(", "tok", ")", "\n", "for", "i", ",", "sub_text", "in", "enumerate", "(", "split_text", ")", ":", "\n", "                ", "sub_text", "=", "sub_text", ".", "rstrip", "(", ")", "\n", "if", "i", "==", "0", "and", "not", "sub_text", ":", "\n", "                    ", "result", "+=", "[", "tok", "]", "\n", "", "elif", "i", "==", "len", "(", "split_text", ")", "-", "1", ":", "\n", "                    ", "if", "sub_text", ":", "\n", "                        ", "result", "+=", "[", "sub_text", "]", "\n", "", "else", ":", "\n", "                        ", "pass", "\n", "", "", "else", ":", "\n", "                    ", "if", "sub_text", ":", "\n", "                        ", "result", "+=", "[", "sub_text", "]", "\n", "", "result", "+=", "[", "tok", "]", "\n", "", "", "return", "result", "\n", "\n", "", "def", "split_on_tokens", "(", "tok_list", ",", "text", ")", ":", "\n", "            ", "if", "not", "text", ".", "strip", "(", ")", ":", "\n", "                ", "return", "[", "]", "\n", "", "if", "not", "tok_list", ":", "\n", "                ", "return", "self", ".", "_tokenize", "(", "text", ")", "\n", "\n", "", "tokenized_text", "=", "[", "]", "\n", "text_list", "=", "[", "text", "]", "\n", "for", "tok", "in", "tok_list", ":", "\n", "                ", "tokenized_text", "=", "[", "]", "\n", "for", "sub_text", "in", "text_list", ":", "\n", "                    ", "if", "sub_text", "not", "in", "self", ".", "unique_added_tokens_encoder", ":", "\n", "                        ", "tokenized_text", "+=", "split_on_token", "(", "tok", ",", "sub_text", ")", "\n", "", "else", ":", "\n", "                        ", "tokenized_text", "+=", "[", "sub_text", "]", "\n", "", "", "text_list", "=", "tokenized_text", "\n", "\n", "", "return", "list", "(", "\n", "itertools", ".", "chain", ".", "from_iterable", "(", "\n", "(", "\n", "self", ".", "_tokenize", "(", "token", ")", "if", "token", "not", "in", "self", ".", "unique_added_tokens_encoder", "else", "[", "token", "]", "\n", "for", "token", "in", "tokenized_text", "\n", ")", "\n", ")", "\n", ")", "\n", "\n", "", "added_tokens", "=", "self", ".", "unique_added_tokens_encoder", "\n", "tokenized_text", "=", "split_on_tokens", "(", "added_tokens", ",", "text", ")", "\n", "return", "tokenized_text", "\n", "\n"]], "home.repos.pwc.inspect_result.neulab_awesome-align.awesome_align.tokenization_utils.PreTrainedTokenizer._tokenize": [[817, 825], ["None"], "methods", ["None"], ["", "def", "_tokenize", "(", "self", ",", "text", ",", "**", "kwargs", ")", ":", "\n", "        ", "\"\"\" Converts a string in a sequence of tokens (string), using the tokenizer.\n            Split in words for word-based vocabulary or sub-words for sub-word-based\n            vocabularies (BPE/SentencePieces/WordPieces).\n\n            Do NOT take care of added tokens.\n        \"\"\"", "\n", "raise", "NotImplementedError", "\n", "\n"]], "home.repos.pwc.inspect_result.neulab_awesome-align.awesome_align.tokenization_utils.PreTrainedTokenizer.convert_tokens_to_ids": [[826, 840], ["isinstance", "tokenization_utils.PreTrainedTokenizer._convert_token_to_id_with_added_voc", "ids.append", "tokenization_utils.PreTrainedTokenizer._convert_token_to_id_with_added_voc"], "methods", ["home.repos.pwc.inspect_result.neulab_awesome-align.awesome_align.tokenization_utils.PreTrainedTokenizerFast._convert_token_to_id_with_added_voc", "home.repos.pwc.inspect_result.neulab_awesome-align.awesome_align.tokenization_utils.PreTrainedTokenizerFast._convert_token_to_id_with_added_voc"], ["", "def", "convert_tokens_to_ids", "(", "self", ",", "tokens", ")", ":", "\n", "        ", "\"\"\" Converts a single token, or a sequence of tokens, (str) in a single integer id\n            (resp. a sequence of ids), using the vocabulary.\n        \"\"\"", "\n", "if", "tokens", "is", "None", ":", "\n", "            ", "return", "None", "\n", "\n", "", "if", "isinstance", "(", "tokens", ",", "str", ")", ":", "\n", "            ", "return", "self", ".", "_convert_token_to_id_with_added_voc", "(", "tokens", ")", "\n", "\n", "", "ids", "=", "[", "]", "\n", "for", "token", "in", "tokens", ":", "\n", "            ", "ids", ".", "append", "(", "self", ".", "_convert_token_to_id_with_added_voc", "(", "token", ")", ")", "\n", "", "return", "ids", "\n", "\n"]], "home.repos.pwc.inspect_result.neulab_awesome-align.awesome_align.tokenization_utils.PreTrainedTokenizer._convert_token_to_id_with_added_voc": [[841, 848], ["tokenization_utils.PreTrainedTokenizer._convert_token_to_id"], "methods", ["home.repos.pwc.inspect_result.neulab_awesome-align.awesome_align.tokenization_utils.PreTrainedTokenizer._convert_token_to_id"], ["", "def", "_convert_token_to_id_with_added_voc", "(", "self", ",", "token", ")", ":", "\n", "        ", "if", "token", "is", "None", ":", "\n", "            ", "return", "None", "\n", "\n", "", "if", "token", "in", "self", ".", "added_tokens_encoder", ":", "\n", "            ", "return", "self", ".", "added_tokens_encoder", "[", "token", "]", "\n", "", "return", "self", ".", "_convert_token_to_id", "(", "token", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.neulab_awesome-align.awesome_align.tokenization_utils.PreTrainedTokenizer._convert_token_to_id": [[849, 851], ["None"], "methods", ["None"], ["", "def", "_convert_token_to_id", "(", "self", ",", "token", ")", ":", "\n", "        ", "raise", "NotImplementedError", "\n", "\n"]], "home.repos.pwc.inspect_result.neulab_awesome-align.awesome_align.tokenization_utils.PreTrainedTokenizer.encode": [[852, 913], ["tokenization_utils.PreTrainedTokenizer.encode_plus"], "methods", ["home.repos.pwc.inspect_result.neulab_awesome-align.awesome_align.tokenization_utils.PreTrainedTokenizerFast.encode_plus"], ["", "def", "encode", "(", "\n", "self", ",", "\n", "text", ",", "\n", "text_pair", "=", "None", ",", "\n", "add_special_tokens", "=", "True", ",", "\n", "max_length", "=", "None", ",", "\n", "stride", "=", "0", ",", "\n", "truncation_strategy", "=", "\"longest_first\"", ",", "\n", "pad_to_max_length", "=", "False", ",", "\n", "return_tensors", "=", "None", ",", "\n", "**", "kwargs", "\n", ")", ":", "\n", "        ", "\"\"\"\n        Converts a string in a sequence of ids (integer), using the tokenizer and vocabulary.\n\n        Same as doing ``self.convert_tokens_to_ids(self.tokenize(text))``.\n\n        Args:\n            text: The first sequence to be encoded. This can be a string, a list of strings (tokenized string using\n                the `tokenize` method) or a list of integers (tokenized string ids using the `convert_tokens_to_ids`\n                method)\n            text_pair: Optional second sequence to be encoded. This can be a string, a list of strings (tokenized\n                string using the `tokenize` method) or a list of integers (tokenized string ids using the\n                `convert_tokens_to_ids` method)\n            add_special_tokens: if set to ``True``, the sequences will be encoded with the special tokens relative\n                to their model.\n            max_length: if set to a number, will limit the total sequence returned so that it has a maximum length.\n                If there are overflowing tokens, those will be added to the returned dictionary\n            stride: if set to a number along with max_length, the overflowing tokens returned will contain some tokens\n                from the main sequence returned. The value of this argument defines the number of additional tokens.\n            truncation_strategy: string selected in the following options:\n                - 'longest_first' (default) Iteratively reduce the inputs sequence until the input is under max_length\n                    starting from the longest one at each token (when there is a pair of input sequences)\n                - 'only_first': Only truncate the first sequence\n                - 'only_second': Only truncate the second sequence\n                - 'do_not_truncate': Does not truncate (raise an error if the input sequence is longer than max_length)\n            pad_to_max_length: if set to True, the returned sequences will be padded according to the model's padding side and\n                padding index, up to their max length. If no max length is specified, the padding is done up to the model's max length.\n                The tokenizer padding sides are handled by the class attribute `padding_side` which can be set to the following strings:\n                - 'left': pads on the left of the sequences\n                - 'right': pads on the right of the sequences\n                Defaults to False: no padding.\n            return_tensors: (optional) can be set to 'tf' or 'pt' to return respectively TensorFlow tf.constant\n                or PyTorch torch.Tensor instead of a list of python integers.\n            add_prefix_space: Only applies to GPT-2 and RoBERTa tokenizers. When `True`, this ensures that the sequence\n                begins with an empty space. False by default except for when using RoBERTa with `add_special_tokens=True`.\n            **kwargs: passed to the `self.tokenize()` method\n        \"\"\"", "\n", "encoded_inputs", "=", "self", ".", "encode_plus", "(", "\n", "text", ",", "\n", "text_pair", "=", "text_pair", ",", "\n", "max_length", "=", "max_length", ",", "\n", "add_special_tokens", "=", "add_special_tokens", ",", "\n", "stride", "=", "stride", ",", "\n", "truncation_strategy", "=", "truncation_strategy", ",", "\n", "pad_to_max_length", "=", "pad_to_max_length", ",", "\n", "return_tensors", "=", "return_tensors", ",", "\n", "**", "kwargs", ",", "\n", ")", "\n", "\n", "return", "encoded_inputs", "[", "\"input_ids\"", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.neulab_awesome-align.awesome_align.tokenization_utils.PreTrainedTokenizer.encode_plus": [[914, 1039], ["tokenization_utils.PreTrainedTokenizer.encode_plus.get_input_ids"], "methods", ["None"], ["", "def", "encode_plus", "(", "\n", "self", ",", "\n", "text", ",", "\n", "text_pair", "=", "None", ",", "\n", "add_special_tokens", "=", "True", ",", "\n", "max_length", "=", "None", ",", "\n", "stride", "=", "0", ",", "\n", "truncation_strategy", "=", "\"longest_first\"", ",", "\n", "pad_to_max_length", "=", "False", ",", "\n", "return_tensors", "=", "None", ",", "\n", "return_token_type_ids", "=", "True", ",", "\n", "return_attention_mask", "=", "True", ",", "\n", "return_overflowing_tokens", "=", "False", ",", "\n", "return_special_tokens_mask", "=", "False", ",", "\n", "return_offsets_mapping", "=", "False", ",", "\n", "**", "kwargs", "\n", ")", ":", "\n", "        ", "\"\"\"\n        Returns a dictionary containing the encoded sequence or sequence pair and additional informations:\n        the mask for sequence classification and the overflowing elements if a ``max_length`` is specified.\n\n        Args:\n            text: The first sequence to be encoded. This can be a string, a list of strings (tokenized string using\n                the `tokenize` method) or a list of integers (tokenized string ids using the `convert_tokens_to_ids`\n                method)\n            text_pair: Optional second sequence to be encoded. This can be a string, a list of strings (tokenized\n                string using the `tokenize` method) or a list of integers (tokenized string ids using the\n                `convert_tokens_to_ids` method)\n            add_special_tokens: if set to ``True``, the sequences will be encoded with the special tokens relative\n                to their model.\n            max_length: if set to a number, will limit the total sequence returned so that it has a maximum length.\n                If there are overflowing tokens, those will be added to the returned dictionary\n            stride: if set to a number along with max_length, the overflowing tokens returned will contain some tokens\n                from the main sequence returned. The value of this argument defines the number of additional tokens.\n            truncation_strategy: string selected in the following options:\n                - 'longest_first' (default) Iteratively reduce the inputs sequence until the input is under max_length\n                    starting from the longest one at each token (when there is a pair of input sequences)\n                - 'only_first': Only truncate the first sequence\n                - 'only_second': Only truncate the second sequence\n                - 'do_not_truncate': Does not truncate (raise an error if the input sequence is longer than max_length)\n            pad_to_max_length: if set to True, the returned sequences will be padded according to the model's padding side and\n                padding index, up to their max length. If no max length is specified, the padding is done up to the model's max length.\n                The tokenizer padding sides are handled by the class attribute `padding_side` which can be set to the following strings:\n                - 'left': pads on the left of the sequences\n                - 'right': pads on the right of the sequences\n                Defaults to False: no padding.\n            return_tensors: (optional) can be set to 'tf' or 'pt' to return respectively TensorFlow tf.constant\n                or PyTorch torch.Tensor instead of a list of python integers.\n            add_prefix_space: Only applies to GPT-2 and RoBERTa tokenizers. When `True`, this ensures that the sequence\n                begins with an empty space. False by default except for when using RoBERTa with `add_special_tokens=True`.\n            return_token_type_ids: (optional) Set to False to avoid returning token_type_ids (default True).\n            return_attention_mask: (optional) Set to False to avoid returning attention mask (default True)\n            return_overflowing_tokens: (optional) Set to True to return overflowing token information (default False).\n            return_special_tokens_mask: (optional) Set to True to return special tokens mask information (default False).\n            return_offsets_mapping: (optional) Set to True to return (char_start, char_end) for each token (default False).\n                If using Python's tokenizer, this method will raise NotImplementedError. This one is only available on\n                Rust-based tokenizers inheriting from PreTrainedTokenizerFast.\n            **kwargs: passed to the `self.tokenize()` method\n\n        Return:\n            A Dictionary of shape::\n\n                {\n                    input_ids: list[int],\n                    token_type_ids: list[int] if return_token_type_ids is True (default)\n                    attention_mask: list[int] if return_attention_mask is True (default)\n                    overflowing_tokens: list[int] if a ``max_length`` is specified and return_overflowing_tokens is True\n                    num_truncated_tokens: int if a ``max_length`` is specified and return_overflowing_tokens is True\n                    special_tokens_mask: list[int] if ``add_special_tokens`` if set to ``True`` and return_special_tokens_mask is True\n                }\n\n            With the fields:\n                ``input_ids``: list of token ids to be fed to a model\n                ``token_type_ids``: list of token type ids to be fed to a model\n                ``attention_mask``: list of indices specifying which tokens should be attended to by the model\n                ``overflowing_tokens``: list of overflowing tokens if a max length is specified.\n                ``num_truncated_tokens``: number of overflowing tokens a ``max_length`` is specified\n                ``special_tokens_mask``: if adding special tokens, this is a list of [0, 1], with 0 specifying special added\n                tokens and 1 specifying sequence tokens.\n        \"\"\"", "\n", "\n", "def", "get_input_ids", "(", "text", ")", ":", "\n", "            ", "if", "isinstance", "(", "text", ",", "str", ")", ":", "\n", "                ", "tokens", "=", "self", ".", "tokenize", "(", "text", ",", "add_special_tokens", "=", "add_special_tokens", ",", "**", "kwargs", ")", "\n", "return", "self", ".", "convert_tokens_to_ids", "(", "tokens", ")", "\n", "", "elif", "isinstance", "(", "text", ",", "(", "list", ",", "tuple", ")", ")", "and", "len", "(", "text", ")", ">", "0", "and", "isinstance", "(", "text", "[", "0", "]", ",", "str", ")", ":", "\n", "                ", "return", "self", ".", "convert_tokens_to_ids", "(", "text", ")", "\n", "", "elif", "isinstance", "(", "text", ",", "(", "list", ",", "tuple", ")", ")", "and", "len", "(", "text", ")", ">", "0", "and", "isinstance", "(", "text", "[", "0", "]", ",", "int", ")", ":", "\n", "                ", "return", "text", "\n", "", "else", ":", "\n", "                ", "raise", "ValueError", "(", "\n", "\"Input is not valid. Should be a string, a list/tuple of strings or a list/tuple of integers.\"", "\n", ")", "\n", "\n", "", "", "if", "return_offsets_mapping", ":", "\n", "            ", "raise", "NotImplementedError", "(", "\n", "\"return_offset_mapping is not available when using Python tokenizers.\"", "\n", "\"To use this feature, change your tokenizer to one deriving from \"", "\n", "\"transformers.PreTrainedTokenizerFast.\"", "\n", "\"More information on available tokenizers at \"", "\n", "\"https://github.com/huggingface/transformers/pull/2674\"", "\n", ")", "\n", "\n", "# Throw an error if we can pad because there is no padding token", "\n", "", "if", "pad_to_max_length", "and", "self", ".", "pad_token_id", "is", "None", ":", "\n", "            ", "raise", "ValueError", "(", "\n", "\"Unable to set proper padding strategy as the tokenizer does not have a padding token. In this case please set the `pad_token` `(tokenizer.pad_token = tokenizer.eos_token e.g.)` or add a new pad token via the function add_special_tokens if you want to use a padding strategy\"", "\n", ")", "\n", "\n", "", "first_ids", "=", "get_input_ids", "(", "text", ")", "\n", "second_ids", "=", "get_input_ids", "(", "text_pair", ")", "if", "text_pair", "is", "not", "None", "else", "None", "\n", "\n", "return", "self", ".", "prepare_for_model", "(", "\n", "first_ids", ",", "\n", "pair_ids", "=", "second_ids", ",", "\n", "max_length", "=", "max_length", ",", "\n", "pad_to_max_length", "=", "pad_to_max_length", ",", "\n", "add_special_tokens", "=", "add_special_tokens", ",", "\n", "stride", "=", "stride", ",", "\n", "truncation_strategy", "=", "truncation_strategy", ",", "\n", "return_tensors", "=", "return_tensors", ",", "\n", "return_attention_mask", "=", "return_attention_mask", ",", "\n", "return_token_type_ids", "=", "return_token_type_ids", ",", "\n", "return_overflowing_tokens", "=", "return_overflowing_tokens", ",", "\n", "return_special_tokens_mask", "=", "return_special_tokens_mask", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.neulab_awesome-align.awesome_align.tokenization_utils.PreTrainedTokenizer.batch_encode_plus": [[1041, 1222], ["isinstance", "ValueError", "NotImplementedError", "tokenization_utils.PreTrainedTokenizer.encode_plus.get_input_ids"], "methods", ["None"], ["", "def", "batch_encode_plus", "(", "\n", "self", ",", "\n", "batch_text_or_text_pairs", "=", "None", ",", "\n", "add_special_tokens", "=", "True", ",", "\n", "max_length", "=", "None", ",", "\n", "stride", "=", "0", ",", "\n", "truncation_strategy", "=", "\"longest_first\"", ",", "\n", "pad_to_max_length", "=", "False", ",", "\n", "return_tensors", "=", "None", ",", "\n", "return_token_type_ids", "=", "True", ",", "\n", "return_attention_masks", "=", "True", ",", "\n", "return_overflowing_tokens", "=", "False", ",", "\n", "return_special_tokens_masks", "=", "False", ",", "\n", "return_offsets_mapping", "=", "False", ",", "\n", "return_input_lengths", "=", "False", ",", "\n", "**", "kwargs", "\n", ")", ":", "\n", "        ", "\"\"\"\n        Returns a dictionary containing the encoded sequence or sequence pair and additional information:\n        the mask for sequence classification and the overflowing elements if a ``max_length`` is specified.\n\n        Args:\n            batch_text_or_text_pairs: Batch of sequences or pair of sequences to be encoded.\n                This can be a list of string/string-sequences/int-sequences or a list of pair of\n                string/string-sequences/int-sequence (see details in encode_plus)\n            add_special_tokens: if set to ``True``, the sequences will be encoded with the special tokens relative\n                to their model.\n            max_length: if set to a number, will limit the total sequence returned so that it has a maximum length.\n                If there are overflowing tokens, those will be added to the returned dictionary`\n            stride: if set to a number along with max_length, the overflowing tokens returned will contain some tokens\n                from the main sequence returned. The value of this argument defines the number of additional tokens.\n            truncation_strategy: string selected in the following options:\n                - 'longest_first' (default) Iteratively reduce the inputs sequence until the input is under max_length\n                    starting from the longest one at each token (when there is a pair of input sequences)\n                - 'only_first': Only truncate the first sequence\n                - 'only_second': Only truncate the second sequence\n                - 'do_not_truncate': Does not truncate (raise an error if the input sequence is longer than max_length)\n            pad_to_max_length: if set to True, the returned sequences will be padded according to the model's padding side and\n                padding index, up to their max length. If no max length is specified, the padding is done up to the model's max length.\n                The tokenizer padding sides are handled by the class attribute `padding_side` which can be set to the following strings:\n                - 'left': pads on the left of the sequences\n                - 'right': pads on the right of the sequences\n                Defaults to False: no padding.\n            return_tensors: (optional) can be set to 'tf' or 'pt' to return respectively TensorFlow tf.constant\n                or PyTorch torch.Tensor instead of a list of python integers.\n            return_input_lengths: (optional) If set the resulting dictionary will include the length of each sample\n            return_attention_masks: (optional) Set to True to return the attention mask (default False)\n            return_offsets_mapping: (optional) Not available, should be set to False or it will throw NotImplementError\n            **kwargs: passed to the `self.tokenize()` method\n\n        Return:\n            A Dictionary of shape::\n\n                {\n                    input_ids: list[List[int]],\n                    token_type_ids: list[List[int]] if return_token_type_ids is True (default)\n                    attention_mask: list[List[int]] if return_attention_mask is True (default)\n                    overflowing_tokens: list[List[int]] if a ``max_length`` is specified and return_overflowing_tokens is True\n                    num_truncated_tokens: List[int] if a ``max_length`` is specified and return_overflowing_tokens is True\n                    special_tokens_mask: list[List[int]] if ``add_special_tokens`` if set to ``True`` and return_special_tokens_mask is True\n                }\n\n            With the fields:\n                ``input_ids``: list of token ids to be fed to a model\n                ``token_type_ids``: list of token type ids to be fed to a model\n                ``attention_mask``: list of indices specifying which tokens should be attended to by the model\n                ``overflowing_tokens``: list of overflowing tokens if a max length is specified.\n                ``num_truncated_tokens``: number of overflowing tokens a ``max_length`` is specified\n                ``special_tokens_mask``: if adding special tokens, this is a list of [0, 1], with 0 specifying special added\n                tokens and 1 specifying sequence tokens.\n        \"\"\"", "\n", "\n", "def", "get_input_ids", "(", "text", ")", ":", "\n", "            ", "if", "isinstance", "(", "text", ",", "str", ")", ":", "\n", "                ", "tokens", "=", "self", ".", "tokenize", "(", "text", ",", "add_special_tokens", "=", "add_special_tokens", ",", "**", "kwargs", ")", "\n", "return", "self", ".", "convert_tokens_to_ids", "(", "tokens", ")", "\n", "", "elif", "isinstance", "(", "text", ",", "(", "list", ",", "tuple", ")", ")", "and", "len", "(", "text", ")", ">", "0", "and", "isinstance", "(", "text", "[", "0", "]", ",", "str", ")", ":", "\n", "                ", "return", "self", ".", "convert_tokens_to_ids", "(", "text", ")", "\n", "", "elif", "isinstance", "(", "text", ",", "(", "list", ",", "tuple", ")", ")", "and", "len", "(", "text", ")", ">", "0", "and", "isinstance", "(", "text", "[", "0", "]", ",", "int", ")", ":", "\n", "                ", "return", "text", "\n", "", "else", ":", "\n", "                ", "raise", "ValueError", "(", "\n", "\"Input is not valid. Should be a string, a list/tuple of strings or a list/tuple of integers.\"", "\n", ")", "\n", "\n", "# Throw an error if we can pad because there is no padding token", "\n", "", "", "if", "pad_to_max_length", "and", "self", ".", "pad_token_id", "is", "None", ":", "\n", "            ", "raise", "ValueError", "(", "\n", "\"Unable to set proper padding strategy as the tokenizer does not have a padding token. In this case please set the `pad_token` `(tokenizer.pad_token = tokenizer.eos_token e.g.)` or add a new pad token via the function add_special_tokens if you want to use a padding strategy\"", "\n", ")", "\n", "\n", "", "if", "return_offsets_mapping", ":", "\n", "            ", "raise", "NotImplementedError", "(", "\n", "\"return_offset_mapping is not available when using Python tokenizers.\"", "\n", "\"To use this feature, change your tokenizer to one deriving from \"", "\n", "\"transformers.PreTrainedTokenizerFast.\"", "\n", "\"More information on available tokenizers at \"", "\n", "\"https://github.com/huggingface/transformers/pull/2674\"", "\n", ")", "\n", "\n", "", "input_ids", "=", "[", "]", "\n", "for", "ids_or_pair_ids", "in", "batch_text_or_text_pairs", ":", "\n", "            ", "if", "isinstance", "(", "ids_or_pair_ids", ",", "(", "list", ",", "tuple", ")", ")", "and", "len", "(", "ids_or_pair_ids", ")", "==", "2", ":", "\n", "                ", "ids", ",", "pair_ids", "=", "ids_or_pair_ids", "\n", "", "else", ":", "\n", "                ", "ids", ",", "pair_ids", "=", "ids_or_pair_ids", ",", "None", "\n", "\n", "", "first_ids", "=", "get_input_ids", "(", "ids", ")", "\n", "second_ids", "=", "get_input_ids", "(", "pair_ids", ")", "if", "pair_ids", "is", "not", "None", "else", "None", "\n", "input_ids", ".", "append", "(", "(", "first_ids", ",", "second_ids", ")", ")", "\n", "\n", "", "if", "max_length", "is", "None", "and", "pad_to_max_length", ":", "\n", "\n", "            ", "def", "total_sequence_length", "(", "input_pairs", ")", ":", "\n", "                ", "first_ids", ",", "second_ids", "=", "input_pairs", "\n", "return", "len", "(", "first_ids", ")", "+", "(", "\n", "self", ".", "num_added_tokens", "(", ")", "\n", "if", "second_ids", "is", "None", "\n", "else", "(", "len", "(", "second_ids", ")", "+", "self", ".", "num_added_tokens", "(", "pair", "=", "True", ")", ")", "\n", ")", "\n", "\n", "", "max_length", "=", "max", "(", "[", "total_sequence_length", "(", "ids", ")", "for", "ids", "in", "input_ids", "]", ")", "\n", "\n", "", "batch_outputs", "=", "{", "}", "\n", "for", "first_ids", ",", "second_ids", "in", "input_ids", ":", "\n", "# Prepares a sequence of input id, or a pair of sequences of inputs ids so that it can be used by", "\n", "# the model. It adds special tokens, truncates sequences if overflowing while taking into account", "\n", "# the special tokens and manages a window stride for overflowing tokens", "\n", "            ", "outputs", "=", "self", ".", "prepare_for_model", "(", "\n", "first_ids", ",", "\n", "pair_ids", "=", "second_ids", ",", "\n", "max_length", "=", "max_length", ",", "\n", "pad_to_max_length", "=", "pad_to_max_length", ",", "\n", "add_special_tokens", "=", "add_special_tokens", ",", "\n", "stride", "=", "stride", ",", "\n", "truncation_strategy", "=", "truncation_strategy", ",", "\n", "return_attention_mask", "=", "return_attention_masks", ",", "\n", "return_token_type_ids", "=", "return_token_type_ids", ",", "\n", "return_overflowing_tokens", "=", "return_overflowing_tokens", ",", "\n", "return_special_tokens_mask", "=", "return_special_tokens_masks", ",", "\n", ")", "\n", "\n", "# Append the non-padded length to the output", "\n", "if", "return_input_lengths", ":", "\n", "                ", "outputs", "[", "\"input_len\"", "]", "=", "len", "(", "outputs", "[", "\"input_ids\"", "]", ")", "\n", "\n", "", "for", "key", ",", "value", "in", "outputs", ".", "items", "(", ")", ":", "\n", "                ", "if", "key", "not", "in", "batch_outputs", ":", "\n", "                    ", "batch_outputs", "[", "key", "]", "=", "[", "]", "\n", "", "batch_outputs", "[", "key", "]", ".", "append", "(", "value", ")", "\n", "\n", "", "", "if", "return_tensors", "is", "not", "None", ":", "\n", "\n", "# Do the tensor conversion in batch", "\n", "            ", "for", "key", ",", "value", "in", "batch_outputs", ".", "items", "(", ")", ":", "\n", "                ", "if", "return_tensors", "==", "\"tf\"", "and", "is_tf_available", "(", ")", ":", "\n", "                    ", "try", ":", "\n", "                        ", "batch_outputs", "[", "key", "]", "=", "tf", ".", "constant", "(", "value", ")", "\n", "", "except", "ValueError", ":", "\n", "                        ", "if", "None", "in", "[", "item", "for", "sequence", "in", "value", "for", "item", "in", "sequence", "]", ":", "\n", "                            ", "raise", "ValueError", "(", "self", ".", "NO_PAD_TOKEN_FOR_BATCH_MSG", ")", "\n", "", "else", ":", "\n", "                            ", "raise", "ValueError", "(", "self", ".", "UNEVEN_SEQUENCES_FOR_BATCH_MSG", ")", "\n", "", "", "", "elif", "return_tensors", "==", "\"pt\"", "and", "is_torch_available", "(", ")", ":", "\n", "                    ", "try", ":", "\n", "                        ", "batch_outputs", "[", "key", "]", "=", "torch", ".", "tensor", "(", "value", ")", "\n", "", "except", "ValueError", ":", "\n", "                        ", "raise", "ValueError", "(", "self", ".", "UNEVEN_SEQUENCES_FOR_BATCH_MSG", ")", "\n", "", "except", "RuntimeError", ":", "\n", "                        ", "if", "None", "in", "[", "item", "for", "sequence", "in", "value", "for", "item", "in", "sequence", "]", ":", "\n", "                            ", "raise", "ValueError", "(", "self", ".", "NO_PAD_TOKEN_FOR_BATCH_MSG", ")", "\n", "", "else", ":", "\n", "                            ", "raise", "\n", "", "", "", "elif", "return_tensors", "is", "not", "None", ":", "\n", "                    ", "logger", ".", "warning", "(", "\n", "\"Unable to convert output to tensors format {}, PyTorch or TensorFlow is not available.\"", ".", "format", "(", "\n", "return_tensors", "\n", ")", "\n", ")", "\n", "\n", "", "", "", "return", "batch_outputs", "\n", "\n"]], "home.repos.pwc.inspect_result.neulab_awesome-align.awesome_align.tokenization_utils.PreTrainedTokenizer.prepare_for_model": [[1223, 1414], ["bool", "len", "len", "tokenization_utils.PreTrainedTokenizer.truncate_sequences", "tokenization_utils.PreTrainedTokenizer.build_inputs_with_special_tokens", "tokenization_utils.PreTrainedTokenizer.create_token_type_ids_from_sequences", "logger.warning", "logger.warning", "awesome_align.file_utils.is_tf_available", "tf.constant", "tokenization_utils.PreTrainedTokenizer.num_added_tokens", "tokenization_utils.PreTrainedTokenizer.get_special_tokens_mask", "len", "len", "len", "tf.constant", "tf.constant", "awesome_align.file_utils.is_torch_available", "torch.tensor", "len", "len", "len", "ValueError", "len", "torch.tensor", "torch.tensor", "logger.warning", "len", "len", "len", "len", "str", "len"], "methods", ["home.repos.pwc.inspect_result.neulab_awesome-align.awesome_align.tokenization_utils.PreTrainedTokenizer.truncate_sequences", "home.repos.pwc.inspect_result.neulab_awesome-align.awesome_align.tokenization_utils.PreTrainedTokenizerFast.build_inputs_with_special_tokens", "home.repos.pwc.inspect_result.neulab_awesome-align.awesome_align.tokenization_utils.PreTrainedTokenizer.create_token_type_ids_from_sequences", "home.repos.pwc.inspect_result.neulab_awesome-align.awesome_align.file_utils.is_tf_available", "home.repos.pwc.inspect_result.neulab_awesome-align.awesome_align.tokenization_utils.PreTrainedTokenizerFast.num_added_tokens", "home.repos.pwc.inspect_result.neulab_awesome-align.awesome_align.tokenization_utils.PreTrainedTokenizer.get_special_tokens_mask", "home.repos.pwc.inspect_result.neulab_awesome-align.awesome_align.file_utils.is_torch_available"], ["", "def", "prepare_for_model", "(", "\n", "self", ",", "\n", "ids", ",", "\n", "pair_ids", "=", "None", ",", "\n", "max_length", "=", "None", ",", "\n", "add_special_tokens", "=", "True", ",", "\n", "stride", "=", "0", ",", "\n", "truncation_strategy", "=", "\"longest_first\"", ",", "\n", "pad_to_max_length", "=", "False", ",", "\n", "return_tensors", "=", "None", ",", "\n", "return_token_type_ids", "=", "True", ",", "\n", "return_attention_mask", "=", "True", ",", "\n", "return_overflowing_tokens", "=", "False", ",", "\n", "return_special_tokens_mask", "=", "False", ",", "\n", ")", ":", "\n", "        ", "\"\"\"\n        Prepares a sequence of input id, or a pair of sequences of inputs ids so that it can be used by the model.\n        It adds special tokens, truncates\n        sequences if overflowing while taking into account the special tokens and manages a window stride for\n        overflowing tokens\n\n        Args:\n            ids: list of tokenized input ids. Can be obtained from a string by chaining the\n                `tokenize` and `convert_tokens_to_ids` methods.\n            pair_ids: Optional second list of input ids. Can be obtained from a string by chaining the\n                `tokenize` and `convert_tokens_to_ids` methods.\n            max_length: maximum length of the returned list. Will truncate by taking into account the special tokens.\n            add_special_tokens: if set to ``True``, the sequences will be encoded with the special tokens relative\n                to their model.\n            stride: window stride for overflowing tokens. Can be useful for edge effect removal when using sequential\n                list of inputs.\n            truncation_strategy: string selected in the following options:\n                - 'longest_first' (default) Iteratively reduce the inputs sequence until the input is under max_length\n                    starting from the longest one at each token (when there is a pair of input sequences)\n                - 'only_first': Only truncate the first sequence\n                - 'only_second': Only truncate the second sequence\n                - 'do_not_truncate': Does not truncate (raise an error if the input sequence is longer than max_length)\n            pad_to_max_length: if set to True, the returned sequences will be padded according to the model's padding side and\n                padding index, up to their max length. If no max length is specified, the padding is done up to the model's max length.\n                The tokenizer padding sides are handled by the following strings:\n                - 'left': pads on the left of the sequences\n                - 'right': pads on the right of the sequences\n                Defaults to False: no padding.\n            return_tensors: (optional) can be set to 'tf' or 'pt' to return respectively TensorFlow tf.constant\n                or PyTorch torch.Tensor instead of a list of python integers.\n            return_token_type_ids: (optional) Set to False to avoid returning token_type_ids (default True).\n            return_attention_mask: (optional) Set to False to avoid returning attention mask (default True)\n            return_overflowing_tokens: (optional) Set to True to return overflowing token information (default False).\n            return_special_tokens_mask: (optional) Set to True to return special tokens mask information (default False).\n\n        Return:\n            A Dictionary of shape::\n\n                {\n                    input_ids: list[int],\n                    token_type_ids: list[int] if return_token_type_ids is True (default)\n                    overflowing_tokens: list[int] if a ``max_length`` is specified and return_overflowing_tokens is True\n                    num_truncated_tokens: int if a ``max_length`` is specified and return_overflowing_tokens is True\n                    special_tokens_mask: list[int] if ``add_special_tokens`` if set to ``True`` and return_special_tokens_mask is True\n                }\n\n            With the fields:\n                ``input_ids``: list of token ids to be fed to a model\n                ``token_type_ids``: list of token type ids to be fed to a model\n\n                ``overflowing_tokens``: list of overflowing tokens if a max length is specified.\n                ``num_truncated_tokens``: number of overflowing tokens a ``max_length`` is specified\n                ``special_tokens_mask``: if adding special tokens, this is a list of [0, 1], with 0 specifying special added\n                tokens and 1 specifying sequence tokens.\n        \"\"\"", "\n", "pair", "=", "bool", "(", "pair_ids", "is", "not", "None", ")", "\n", "len_ids", "=", "len", "(", "ids", ")", "\n", "len_pair_ids", "=", "len", "(", "pair_ids", ")", "if", "pair", "else", "0", "\n", "\n", "encoded_inputs", "=", "{", "}", "\n", "\n", "# Handle max sequence length", "\n", "total_len", "=", "len_ids", "+", "len_pair_ids", "+", "(", "self", ".", "num_added_tokens", "(", "pair", "=", "pair", ")", "if", "add_special_tokens", "else", "0", ")", "\n", "if", "max_length", "and", "total_len", ">", "max_length", ":", "\n", "            ", "ids", ",", "pair_ids", ",", "overflowing_tokens", "=", "self", ".", "truncate_sequences", "(", "\n", "ids", ",", "\n", "pair_ids", "=", "pair_ids", ",", "\n", "num_tokens_to_remove", "=", "total_len", "-", "max_length", ",", "\n", "truncation_strategy", "=", "truncation_strategy", ",", "\n", "stride", "=", "stride", ",", "\n", ")", "\n", "if", "return_overflowing_tokens", ":", "\n", "                ", "encoded_inputs", "[", "\"overflowing_tokens\"", "]", "=", "overflowing_tokens", "\n", "encoded_inputs", "[", "\"num_truncated_tokens\"", "]", "=", "total_len", "-", "max_length", "\n", "\n", "# Handle special_tokens", "\n", "", "", "if", "add_special_tokens", ":", "\n", "            ", "sequence", "=", "self", ".", "build_inputs_with_special_tokens", "(", "ids", ",", "pair_ids", ")", "\n", "token_type_ids", "=", "self", ".", "create_token_type_ids_from_sequences", "(", "ids", ",", "pair_ids", ")", "\n", "", "else", ":", "\n", "            ", "sequence", "=", "ids", "+", "pair_ids", "if", "pair", "else", "ids", "\n", "token_type_ids", "=", "[", "0", "]", "*", "len", "(", "ids", ")", "+", "(", "[", "1", "]", "*", "len", "(", "pair_ids", ")", "if", "pair", "else", "[", "]", ")", "\n", "\n", "", "if", "return_special_tokens_mask", ":", "\n", "            ", "if", "add_special_tokens", ":", "\n", "                ", "encoded_inputs", "[", "\"special_tokens_mask\"", "]", "=", "self", ".", "get_special_tokens_mask", "(", "ids", ",", "pair_ids", ")", "\n", "", "else", ":", "\n", "                ", "encoded_inputs", "[", "\"special_tokens_mask\"", "]", "=", "[", "0", "]", "*", "len", "(", "sequence", ")", "\n", "\n", "", "", "encoded_inputs", "[", "\"input_ids\"", "]", "=", "sequence", "\n", "if", "return_token_type_ids", ":", "\n", "            ", "encoded_inputs", "[", "\"token_type_ids\"", "]", "=", "token_type_ids", "\n", "\n", "", "if", "max_length", "and", "len", "(", "encoded_inputs", "[", "\"input_ids\"", "]", ")", ">", "max_length", ":", "\n", "            ", "encoded_inputs", "[", "\"input_ids\"", "]", "=", "encoded_inputs", "[", "\"input_ids\"", "]", "[", ":", "max_length", "]", "\n", "if", "return_token_type_ids", ":", "\n", "                ", "encoded_inputs", "[", "\"token_type_ids\"", "]", "=", "encoded_inputs", "[", "\"token_type_ids\"", "]", "[", ":", "max_length", "]", "\n", "", "if", "return_special_tokens_mask", ":", "\n", "                ", "encoded_inputs", "[", "\"special_tokens_mask\"", "]", "=", "encoded_inputs", "[", "\"special_tokens_mask\"", "]", "[", ":", "max_length", "]", "\n", "\n", "", "", "if", "max_length", "is", "None", "and", "len", "(", "encoded_inputs", "[", "\"input_ids\"", "]", ")", ">", "self", ".", "max_len", ":", "\n", "            ", "logger", ".", "warning", "(", "\n", "\"Token indices sequence length is longer than the specified maximum sequence length \"", "\n", "\"for this model ({} > {}). Running this sequence through the model will result in \"", "\n", "\"indexing errors\"", ".", "format", "(", "len", "(", "ids", ")", ",", "self", ".", "max_len", ")", "\n", ")", "\n", "\n", "", "needs_to_be_padded", "=", "pad_to_max_length", "and", "(", "\n", "max_length", "\n", "and", "len", "(", "encoded_inputs", "[", "\"input_ids\"", "]", ")", "<", "max_length", "\n", "or", "max_length", "is", "None", "\n", "and", "len", "(", "encoded_inputs", "[", "\"input_ids\"", "]", ")", "<", "self", ".", "max_len", "\n", "and", "self", ".", "max_len", "<=", "10000", "\n", ")", "\n", "\n", "if", "pad_to_max_length", "and", "max_length", "is", "None", "and", "self", ".", "max_len", ">", "10000", ":", "\n", "            ", "logger", ".", "warning", "(", "\n", "\"Sequence can't be padded as no maximum length is specified and the model maximum length is too high.\"", "\n", ")", "\n", "\n", "", "if", "needs_to_be_padded", ":", "\n", "            ", "difference", "=", "(", "max_length", "if", "max_length", "is", "not", "None", "else", "self", ".", "max_len", ")", "-", "len", "(", "encoded_inputs", "[", "\"input_ids\"", "]", ")", "\n", "\n", "if", "self", ".", "padding_side", "==", "\"right\"", ":", "\n", "                ", "if", "return_attention_mask", ":", "\n", "                    ", "encoded_inputs", "[", "\"attention_mask\"", "]", "=", "[", "1", "]", "*", "len", "(", "encoded_inputs", "[", "\"input_ids\"", "]", ")", "+", "[", "0", "]", "*", "difference", "\n", "", "if", "return_token_type_ids", ":", "\n", "                    ", "encoded_inputs", "[", "\"token_type_ids\"", "]", "=", "(", "\n", "encoded_inputs", "[", "\"token_type_ids\"", "]", "+", "[", "self", ".", "pad_token_type_id", "]", "*", "difference", "\n", ")", "\n", "", "if", "return_special_tokens_mask", ":", "\n", "                    ", "encoded_inputs", "[", "\"special_tokens_mask\"", "]", "=", "encoded_inputs", "[", "\"special_tokens_mask\"", "]", "+", "[", "1", "]", "*", "difference", "\n", "", "encoded_inputs", "[", "\"input_ids\"", "]", "=", "encoded_inputs", "[", "\"input_ids\"", "]", "+", "[", "self", ".", "pad_token_id", "]", "*", "difference", "\n", "", "elif", "self", ".", "padding_side", "==", "\"left\"", ":", "\n", "                ", "if", "return_attention_mask", ":", "\n", "                    ", "encoded_inputs", "[", "\"attention_mask\"", "]", "=", "[", "0", "]", "*", "difference", "+", "[", "1", "]", "*", "len", "(", "encoded_inputs", "[", "\"input_ids\"", "]", ")", "\n", "", "if", "return_token_type_ids", ":", "\n", "                    ", "encoded_inputs", "[", "\"token_type_ids\"", "]", "=", "[", "self", ".", "pad_token_type_id", "]", "*", "difference", "+", "encoded_inputs", "[", "\n", "\"token_type_ids\"", "\n", "]", "\n", "", "if", "return_special_tokens_mask", ":", "\n", "                    ", "encoded_inputs", "[", "\"special_tokens_mask\"", "]", "=", "[", "1", "]", "*", "difference", "+", "encoded_inputs", "[", "\"special_tokens_mask\"", "]", "\n", "", "encoded_inputs", "[", "\"input_ids\"", "]", "=", "[", "self", ".", "pad_token_id", "]", "*", "difference", "+", "encoded_inputs", "[", "\"input_ids\"", "]", "\n", "\n", "", "else", ":", "\n", "                ", "raise", "ValueError", "(", "\"Invalid padding strategy:\"", "+", "str", "(", "self", ".", "padding_side", ")", ")", "\n", "\n", "", "", "elif", "return_attention_mask", ":", "\n", "            ", "encoded_inputs", "[", "\"attention_mask\"", "]", "=", "[", "1", "]", "*", "len", "(", "encoded_inputs", "[", "\"input_ids\"", "]", ")", "\n", "\n", "# Prepare inputs as tensors if asked", "\n", "", "if", "return_tensors", "==", "\"tf\"", "and", "is_tf_available", "(", ")", ":", "\n", "            ", "encoded_inputs", "[", "\"input_ids\"", "]", "=", "tf", ".", "constant", "(", "[", "encoded_inputs", "[", "\"input_ids\"", "]", "]", ")", "\n", "\n", "if", "\"token_type_ids\"", "in", "encoded_inputs", ":", "\n", "                ", "encoded_inputs", "[", "\"token_type_ids\"", "]", "=", "tf", ".", "constant", "(", "[", "encoded_inputs", "[", "\"token_type_ids\"", "]", "]", ")", "\n", "\n", "", "if", "\"attention_mask\"", "in", "encoded_inputs", ":", "\n", "                ", "encoded_inputs", "[", "\"attention_mask\"", "]", "=", "tf", ".", "constant", "(", "[", "encoded_inputs", "[", "\"attention_mask\"", "]", "]", ")", "\n", "\n", "", "", "elif", "return_tensors", "==", "\"pt\"", "and", "is_torch_available", "(", ")", ":", "\n", "            ", "encoded_inputs", "[", "\"input_ids\"", "]", "=", "torch", ".", "tensor", "(", "[", "encoded_inputs", "[", "\"input_ids\"", "]", "]", ")", "\n", "\n", "if", "\"token_type_ids\"", "in", "encoded_inputs", ":", "\n", "                ", "encoded_inputs", "[", "\"token_type_ids\"", "]", "=", "torch", ".", "tensor", "(", "[", "encoded_inputs", "[", "\"token_type_ids\"", "]", "]", ")", "\n", "\n", "", "if", "\"attention_mask\"", "in", "encoded_inputs", ":", "\n", "                ", "encoded_inputs", "[", "\"attention_mask\"", "]", "=", "torch", ".", "tensor", "(", "[", "encoded_inputs", "[", "\"attention_mask\"", "]", "]", ")", "\n", "", "", "elif", "return_tensors", "is", "not", "None", ":", "\n", "            ", "logger", ".", "warning", "(", "\n", "\"Unable to convert output to tensors format {}, PyTorch or TensorFlow is not available.\"", ".", "format", "(", "\n", "return_tensors", "\n", ")", "\n", ")", "\n", "\n", "", "return", "encoded_inputs", "\n", "\n"]], "home.repos.pwc.inspect_result.neulab_awesome-align.awesome_align.tokenization_utils.PreTrainedTokenizer.prepare_for_tokenization": [[1415, 1418], ["None"], "methods", ["None"], ["", "def", "prepare_for_tokenization", "(", "self", ",", "text", ",", "**", "kwargs", ")", ":", "\n", "        ", "\"\"\" Performs any necessary transformations before tokenization \"\"\"", "\n", "return", "text", "\n", "\n"]], "home.repos.pwc.inspect_result.neulab_awesome-align.awesome_align.tokenization_utils.PreTrainedTokenizer.truncate_sequences": [[1419, 1462], ["range", "min", "len", "min", "len", "len", "min", "len", "len", "len", "ValueError", "ValueError", "len"], "methods", ["None"], ["", "def", "truncate_sequences", "(", "\n", "self", ",", "ids", ",", "pair_ids", "=", "None", ",", "num_tokens_to_remove", "=", "0", ",", "truncation_strategy", "=", "\"longest_first\"", ",", "stride", "=", "0", "\n", ")", ":", "\n", "        ", "\"\"\"Truncates a sequence pair in place to the maximum length.\n            truncation_strategy: string selected in the following options:\n                - 'longest_first' (default) Iteratively reduce the inputs sequence until the input is under max_length\n                    starting from the longest one at each token (when there is a pair of input sequences).\n                    Overflowing tokens only contains overflow from the first sequence.\n                - 'only_first': Only truncate the first sequence. raise an error if the first sequence is shorter or equal to than num_tokens_to_remove.\n                - 'only_second': Only truncate the second sequence\n                - 'do_not_truncate': Does not truncate (raise an error if the input sequence is longer than max_length)\n        \"\"\"", "\n", "if", "num_tokens_to_remove", "<=", "0", ":", "\n", "            ", "return", "ids", ",", "pair_ids", ",", "[", "]", "\n", "\n", "", "if", "truncation_strategy", "==", "\"longest_first\"", ":", "\n", "            ", "overflowing_tokens", "=", "[", "]", "\n", "for", "_", "in", "range", "(", "num_tokens_to_remove", ")", ":", "\n", "                ", "if", "pair_ids", "is", "None", "or", "len", "(", "ids", ")", ">", "len", "(", "pair_ids", ")", ":", "\n", "                    ", "overflowing_tokens", "=", "[", "ids", "[", "-", "1", "]", "]", "+", "overflowing_tokens", "\n", "ids", "=", "ids", "[", ":", "-", "1", "]", "\n", "", "else", ":", "\n", "                    ", "pair_ids", "=", "pair_ids", "[", ":", "-", "1", "]", "\n", "", "", "window_len", "=", "min", "(", "len", "(", "ids", ")", ",", "stride", ")", "\n", "if", "window_len", ">", "0", ":", "\n", "                ", "overflowing_tokens", "=", "ids", "[", "-", "window_len", ":", "]", "+", "overflowing_tokens", "\n", "", "", "elif", "truncation_strategy", "==", "\"only_first\"", ":", "\n", "            ", "assert", "len", "(", "ids", ")", ">", "num_tokens_to_remove", "\n", "window_len", "=", "min", "(", "len", "(", "ids", ")", ",", "stride", "+", "num_tokens_to_remove", ")", "\n", "overflowing_tokens", "=", "ids", "[", "-", "window_len", ":", "]", "\n", "ids", "=", "ids", "[", ":", "-", "num_tokens_to_remove", "]", "\n", "", "elif", "truncation_strategy", "==", "\"only_second\"", ":", "\n", "            ", "assert", "pair_ids", "is", "not", "None", "and", "len", "(", "pair_ids", ")", ">", "num_tokens_to_remove", "\n", "window_len", "=", "min", "(", "len", "(", "pair_ids", ")", ",", "stride", "+", "num_tokens_to_remove", ")", "\n", "overflowing_tokens", "=", "pair_ids", "[", "-", "window_len", ":", "]", "\n", "pair_ids", "=", "pair_ids", "[", ":", "-", "num_tokens_to_remove", "]", "\n", "", "elif", "truncation_strategy", "==", "\"do_not_truncate\"", ":", "\n", "            ", "raise", "ValueError", "(", "\"Input sequence are too long for max_length. Please select a truncation strategy.\"", ")", "\n", "", "else", ":", "\n", "            ", "raise", "ValueError", "(", "\n", "\"Truncation_strategy should be selected in ['longest_first', 'only_first', 'only_second', 'do_not_truncate']\"", "\n", ")", "\n", "", "return", "(", "ids", ",", "pair_ids", ",", "overflowing_tokens", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.neulab_awesome-align.awesome_align.tokenization_utils.PreTrainedTokenizer.create_token_type_ids_from_sequences": [[1463, 1467], ["len", "len", "len"], "methods", ["None"], ["", "def", "create_token_type_ids_from_sequences", "(", "self", ",", "token_ids_0", ",", "token_ids_1", "=", "None", ")", ":", "\n", "        ", "if", "token_ids_1", "is", "None", ":", "\n", "            ", "return", "len", "(", "token_ids_0", ")", "*", "[", "0", "]", "\n", "", "return", "[", "0", "]", "*", "len", "(", "token_ids_0", ")", "+", "[", "1", "]", "*", "len", "(", "token_ids_1", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.neulab_awesome-align.awesome_align.tokenization_utils.PreTrainedTokenizer.build_inputs_with_special_tokens": [[1468, 1479], ["None"], "methods", ["None"], ["", "def", "build_inputs_with_special_tokens", "(", "self", ",", "token_ids_0", ",", "token_ids_1", "=", "None", ")", ":", "\n", "        ", "\"\"\"\n        Build model inputs from a sequence or a pair of sequence for sequence classification tasks\n        by concatenating and adding special tokens.\n        A RoBERTa sequence has the following format:\n            single sequence: <s> X </s>\n            pair of sequences: <s> A </s></s> B </s>\n        \"\"\"", "\n", "if", "token_ids_1", "is", "None", ":", "\n", "            ", "return", "token_ids_0", "\n", "", "return", "token_ids_0", "+", "token_ids_1", "\n", "\n"]], "home.repos.pwc.inspect_result.neulab_awesome-align.awesome_align.tokenization_utils.PreTrainedTokenizer.get_special_tokens_mask": [[1480, 1496], ["len", "len"], "methods", ["None"], ["", "def", "get_special_tokens_mask", "(", "self", ",", "token_ids_0", ",", "token_ids_1", "=", "None", ",", "already_has_special_tokens", "=", "False", ")", ":", "\n", "        ", "\"\"\"\n        Retrieves sequence ids from a token list that has no special tokens added. This method is called when adding\n        special tokens using the tokenizer ``prepare_for_model`` or ``encode_plus`` methods.\n\n        Args:\n            token_ids_0: list of ids (must not contain special tokens)\n            token_ids_1: Optional list of ids (must not contain special tokens), necessary when fetching sequence ids\n                for sequence pairs\n            already_has_special_tokens: (default False) Set to True if the token list is already formated with\n                special tokens for the model\n\n        Returns:\n            A list of integers in the range [0, 1]: 1 for a special token, 0 for a sequence token.\n        \"\"\"", "\n", "return", "[", "0", "]", "*", "(", "(", "len", "(", "token_ids_1", ")", "if", "token_ids_1", "else", "0", ")", "+", "len", "(", "token_ids_0", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.neulab_awesome-align.awesome_align.tokenization_utils.PreTrainedTokenizer.convert_ids_to_tokens": [[1497, 1519], ["isinstance", "int", "tokenization_utils.PreTrainedTokenizer._convert_id_to_token", "tokens.append", "tokens.append", "tokenization_utils.PreTrainedTokenizer._convert_id_to_token"], "methods", ["home.repos.pwc.inspect_result.neulab_awesome-align.awesome_align.tokenization_utils.PreTrainedTokenizerFast._convert_id_to_token", "home.repos.pwc.inspect_result.neulab_awesome-align.awesome_align.tokenization_utils.PreTrainedTokenizerFast._convert_id_to_token"], ["", "def", "convert_ids_to_tokens", "(", "self", ",", "ids", ",", "skip_special_tokens", "=", "False", ")", ":", "\n", "        ", "\"\"\" Converts a single index or a sequence of indices (integers) in a token \"\n            (resp.) a sequence of tokens (str), using the vocabulary and added tokens.\n\n            Args:\n                skip_special_tokens: Don't decode special tokens (self.all_special_tokens). Default: False\n        \"\"\"", "\n", "if", "isinstance", "(", "ids", ",", "int", ")", ":", "\n", "            ", "if", "ids", "in", "self", ".", "added_tokens_decoder", ":", "\n", "                ", "return", "self", ".", "added_tokens_decoder", "[", "ids", "]", "\n", "", "else", ":", "\n", "                ", "return", "self", ".", "_convert_id_to_token", "(", "ids", ")", "\n", "", "", "tokens", "=", "[", "]", "\n", "for", "index", "in", "ids", ":", "\n", "            ", "index", "=", "int", "(", "index", ")", "\n", "if", "skip_special_tokens", "and", "index", "in", "self", ".", "all_special_ids", ":", "\n", "                ", "continue", "\n", "", "if", "index", "in", "self", ".", "added_tokens_decoder", ":", "\n", "                ", "tokens", ".", "append", "(", "self", ".", "added_tokens_decoder", "[", "index", "]", ")", "\n", "", "else", ":", "\n", "                ", "tokens", ".", "append", "(", "self", ".", "_convert_id_to_token", "(", "index", ")", ")", "\n", "", "", "return", "tokens", "\n", "\n"]], "home.repos.pwc.inspect_result.neulab_awesome-align.awesome_align.tokenization_utils.PreTrainedTokenizer._convert_id_to_token": [[1520, 1522], ["None"], "methods", ["None"], ["", "def", "_convert_id_to_token", "(", "self", ",", "index", ")", ":", "\n", "        ", "raise", "NotImplementedError", "\n", "\n"]], "home.repos.pwc.inspect_result.neulab_awesome-align.awesome_align.tokenization_utils.PreTrainedTokenizer.convert_tokens_to_string": [[1523, 1529], ["tokenization_utils.PreTrainedTokenizer.convert_ids_to_tokens"], "methods", ["home.repos.pwc.inspect_result.neulab_awesome-align.awesome_align.tokenization_utils.PreTrainedTokenizer.convert_ids_to_tokens"], ["", "def", "convert_tokens_to_string", "(", "self", ",", "tokens", ")", ":", "\n", "        ", "\"\"\" Converts a sequence of tokens (string) in a single string.\n            The most simple way to do it is ' '.join(self.convert_ids_to_tokens(token_ids))\n            but we often want to remove sub-word tokenization artifacts at the same time.\n        \"\"\"", "\n", "return", "\" \"", ".", "join", "(", "self", ".", "convert_ids_to_tokens", "(", "tokens", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.neulab_awesome-align.awesome_align.tokenization_utils.PreTrainedTokenizer.decode": [[1530, 1567], ["tokenization_utils.PreTrainedTokenizer.convert_ids_to_tokens", "sub_texts.append", "tokenization_utils.PreTrainedTokenizer.clean_up_tokenization", "sub_texts.append", "current_sub_text.append", "tokenization_utils.PreTrainedTokenizer.convert_tokens_to_string", "sub_texts.append", "tokenization_utils.PreTrainedTokenizer.convert_tokens_to_string"], "methods", ["home.repos.pwc.inspect_result.neulab_awesome-align.awesome_align.tokenization_utils.PreTrainedTokenizer.convert_ids_to_tokens", "home.repos.pwc.inspect_result.neulab_awesome-align.awesome_align.tokenization_utils.PreTrainedTokenizer.clean_up_tokenization", "home.repos.pwc.inspect_result.neulab_awesome-align.awesome_align.tokenization_utils.PreTrainedTokenizerFast.convert_tokens_to_string", "home.repos.pwc.inspect_result.neulab_awesome-align.awesome_align.tokenization_utils.PreTrainedTokenizerFast.convert_tokens_to_string"], ["", "def", "decode", "(", "self", ",", "token_ids", ",", "skip_special_tokens", "=", "False", ",", "clean_up_tokenization_spaces", "=", "True", ")", ":", "\n", "        ", "\"\"\"\n        Converts a sequence of ids (integer) in a string, using the tokenizer and vocabulary\n        with options to remove special tokens and clean up tokenization spaces.\n        Similar to doing ``self.convert_tokens_to_string(self.convert_ids_to_tokens(token_ids))``.\n\n        Args:\n            token_ids: list of tokenized input ids. Can be obtained using the `encode` or `encode_plus` methods.\n            skip_special_tokens: if set to True, will replace special tokens.\n            clean_up_tokenization_spaces: if set to True, will clean up the tokenization spaces.\n        \"\"\"", "\n", "filtered_tokens", "=", "self", ".", "convert_ids_to_tokens", "(", "token_ids", ",", "skip_special_tokens", "=", "skip_special_tokens", ")", "\n", "\n", "# To avoid mixing byte-level and unicode for byte-level BPT", "\n", "# we need to build string separatly for added tokens and byte-level tokens", "\n", "# cf. https://github.com/huggingface/transformers/issues/1133", "\n", "sub_texts", "=", "[", "]", "\n", "current_sub_text", "=", "[", "]", "\n", "for", "token", "in", "filtered_tokens", ":", "\n", "            ", "if", "skip_special_tokens", "and", "token", "in", "self", ".", "all_special_ids", ":", "\n", "                ", "continue", "\n", "", "if", "token", "in", "self", ".", "added_tokens_encoder", ":", "\n", "                ", "if", "current_sub_text", ":", "\n", "                    ", "sub_texts", ".", "append", "(", "self", ".", "convert_tokens_to_string", "(", "current_sub_text", ")", ")", "\n", "current_sub_text", "=", "[", "]", "\n", "", "sub_texts", ".", "append", "(", "token", ")", "\n", "", "else", ":", "\n", "                ", "current_sub_text", ".", "append", "(", "token", ")", "\n", "", "", "if", "current_sub_text", ":", "\n", "            ", "sub_texts", ".", "append", "(", "self", ".", "convert_tokens_to_string", "(", "current_sub_text", ")", ")", "\n", "", "text", "=", "\" \"", ".", "join", "(", "sub_texts", ")", "\n", "\n", "if", "clean_up_tokenization_spaces", ":", "\n", "            ", "clean_text", "=", "self", ".", "clean_up_tokenization", "(", "text", ")", "\n", "return", "clean_text", "\n", "", "else", ":", "\n", "            ", "return", "text", "\n", "\n"]], "home.repos.pwc.inspect_result.neulab_awesome-align.awesome_align.tokenization_utils.PreTrainedTokenizer.special_tokens_map": [[1568, 1579], ["getattr"], "methods", ["None"], ["", "", "@", "property", "\n", "def", "special_tokens_map", "(", "self", ")", ":", "\n", "        ", "\"\"\" A dictionary mapping special token class attribute (cls_token, unk_token...) to their\n            values ('<unk>', '<cls>'...)\n        \"\"\"", "\n", "set_attr", "=", "{", "}", "\n", "for", "attr", "in", "self", ".", "SPECIAL_TOKENS_ATTRIBUTES", ":", "\n", "            ", "attr_value", "=", "getattr", "(", "self", ",", "\"_\"", "+", "attr", ")", "\n", "if", "attr_value", ":", "\n", "                ", "set_attr", "[", "attr", "]", "=", "attr_value", "\n", "", "", "return", "set_attr", "\n", "\n"]], "home.repos.pwc.inspect_result.neulab_awesome-align.awesome_align.tokenization_utils.PreTrainedTokenizer.all_special_tokens": [[1580, 1591], ["set_attr.values", "list", "set", "isinstance", "list"], "methods", ["None"], ["", "@", "property", "\n", "def", "all_special_tokens", "(", "self", ")", ":", "\n", "        ", "\"\"\" List all the special tokens ('<unk>', '<cls>'...) mapped to class attributes\n            (cls_token, unk_token...).\n        \"\"\"", "\n", "all_toks", "=", "[", "]", "\n", "set_attr", "=", "self", ".", "special_tokens_map", "\n", "for", "attr_value", "in", "set_attr", ".", "values", "(", ")", ":", "\n", "            ", "all_toks", "=", "all_toks", "+", "(", "list", "(", "attr_value", ")", "if", "isinstance", "(", "attr_value", ",", "(", "list", ",", "tuple", ")", ")", "else", "[", "attr_value", "]", ")", "\n", "", "all_toks", "=", "list", "(", "set", "(", "all_toks", ")", ")", "\n", "return", "all_toks", "\n", "\n"]], "home.repos.pwc.inspect_result.neulab_awesome-align.awesome_align.tokenization_utils.PreTrainedTokenizer.all_special_ids": [[1592, 1600], ["tokenization_utils.PreTrainedTokenizer.convert_tokens_to_ids"], "methods", ["home.repos.pwc.inspect_result.neulab_awesome-align.awesome_align.tokenization_utils.PreTrainedTokenizer.convert_tokens_to_ids"], ["", "@", "property", "\n", "def", "all_special_ids", "(", "self", ")", ":", "\n", "        ", "\"\"\" List the vocabulary indices of the special tokens ('<unk>', '<cls>'...) mapped to\n            class attributes (cls_token, unk_token...).\n        \"\"\"", "\n", "all_toks", "=", "self", ".", "all_special_tokens", "\n", "all_ids", "=", "self", ".", "convert_tokens_to_ids", "(", "all_toks", ")", "\n", "return", "all_ids", "\n", "\n"]], "home.repos.pwc.inspect_result.neulab_awesome-align.awesome_align.tokenization_utils.PreTrainedTokenizer.clean_up_tokenization": [[1601, 1619], ["out_string.replace().replace().replace().replace().replace().replace().replace().replace().replace().replace().replace.replace().replace().replace().replace().replace().replace().replace().replace().replace().replace().replace.replace().replace().replace().replace().replace().replace().replace().replace().replace().replace().replace", "out_string.replace().replace().replace().replace().replace().replace().replace().replace().replace().replace().replace.replace().replace().replace().replace().replace().replace().replace().replace().replace().replace().replace.replace().replace().replace().replace().replace().replace().replace().replace().replace().replace", "out_string.replace().replace().replace().replace().replace().replace().replace().replace().replace().replace().replace.replace().replace().replace().replace().replace().replace().replace().replace().replace().replace().replace.replace().replace().replace().replace().replace().replace().replace().replace().replace", "out_string.replace().replace().replace().replace().replace().replace().replace().replace().replace().replace().replace.replace().replace().replace().replace().replace().replace().replace().replace().replace().replace().replace.replace().replace().replace().replace().replace().replace().replace().replace", "out_string.replace().replace().replace().replace().replace().replace().replace().replace().replace().replace().replace.replace().replace().replace().replace().replace().replace().replace().replace().replace().replace().replace.replace().replace().replace().replace().replace().replace().replace", "out_string.replace().replace().replace().replace().replace().replace().replace().replace().replace().replace().replace.replace().replace().replace().replace().replace().replace().replace().replace().replace().replace().replace.replace().replace().replace().replace().replace().replace", "out_string.replace().replace().replace().replace().replace().replace().replace().replace().replace().replace().replace.replace().replace().replace().replace().replace().replace().replace().replace().replace().replace().replace.replace().replace().replace().replace().replace", "out_string.replace().replace().replace().replace().replace().replace().replace().replace().replace().replace().replace.replace().replace().replace().replace().replace().replace().replace().replace().replace().replace().replace.replace().replace().replace().replace", "out_string.replace().replace().replace().replace().replace().replace().replace().replace().replace().replace().replace.replace().replace().replace().replace().replace().replace().replace().replace().replace().replace().replace.replace().replace().replace", "out_string.replace().replace().replace().replace().replace().replace().replace().replace().replace().replace().replace.replace().replace().replace().replace().replace().replace().replace().replace().replace().replace().replace.replace().replace", "out_string.replace().replace().replace().replace().replace().replace().replace().replace().replace().replace().replace.replace().replace().replace().replace().replace().replace().replace().replace().replace().replace().replace.replace"], "methods", ["None"], ["", "@", "staticmethod", "\n", "def", "clean_up_tokenization", "(", "out_string", ")", ":", "\n", "        ", "\"\"\" Clean up a list of simple English tokenization artifacts like spaces before punctuations and abreviated forms.\n        \"\"\"", "\n", "out_string", "=", "(", "\n", "out_string", ".", "replace", "(", "\" .\"", ",", "\".\"", ")", "\n", ".", "replace", "(", "\" ?\"", ",", "\"?\"", ")", "\n", ".", "replace", "(", "\" !\"", ",", "\"!\"", ")", "\n", ".", "replace", "(", "\" ,\"", ",", "\",\"", ")", "\n", ".", "replace", "(", "\" ' \"", ",", "\"'\"", ")", "\n", ".", "replace", "(", "\" n't\"", ",", "\"n't\"", ")", "\n", ".", "replace", "(", "\" 'm\"", ",", "\"'m\"", ")", "\n", ".", "replace", "(", "\" do not\"", ",", "\" don't\"", ")", "\n", ".", "replace", "(", "\" 's\"", ",", "\"'s\"", ")", "\n", ".", "replace", "(", "\" 've\"", ",", "\"'ve\"", ")", "\n", ".", "replace", "(", "\" 're\"", ",", "\"'re\"", ")", "\n", ")", "\n", "return", "out_string", "\n", "\n"]], "home.repos.pwc.inspect_result.neulab_awesome-align.awesome_align.tokenization_utils.PreTrainedTokenizerFast.__init__": [[1622, 1630], ["tokenization_utils.PreTrainedTokenizer.__init__", "ValueError", "tokenization_utils.PreTrainedTokenizerFast.num_added_tokens", "tokenization_utils.PreTrainedTokenizerFast.num_added_tokens"], "methods", ["home.repos.pwc.inspect_result.neulab_awesome-align.awesome_align.train_utils.AdamW.__init__", "home.repos.pwc.inspect_result.neulab_awesome-align.awesome_align.tokenization_utils.PreTrainedTokenizerFast.num_added_tokens", "home.repos.pwc.inspect_result.neulab_awesome-align.awesome_align.tokenization_utils.PreTrainedTokenizerFast.num_added_tokens"], ["    ", "def", "__init__", "(", "self", ",", "tokenizer", ":", "BaseTokenizer", ",", "**", "kwargs", ")", ":", "\n", "        ", "if", "tokenizer", "is", "None", ":", "\n", "            ", "raise", "ValueError", "(", "\"Provided tokenizer cannot be None\"", ")", "\n", "", "self", ".", "_tokenizer", "=", "tokenizer", "\n", "\n", "super", "(", ")", ".", "__init__", "(", "**", "kwargs", ")", "\n", "self", ".", "max_len_single_sentence", "=", "self", ".", "max_len", "-", "self", ".", "num_added_tokens", "(", "False", ")", "# take into account special tokens", "\n", "self", ".", "max_len_sentences_pair", "=", "self", ".", "max_len", "-", "self", ".", "num_added_tokens", "(", "True", ")", "# take into account special tokens", "\n", "\n"]], "home.repos.pwc.inspect_result.neulab_awesome-align.awesome_align.tokenization_utils.PreTrainedTokenizerFast.tokenizer": [[1631, 1634], ["None"], "methods", ["None"], ["", "@", "property", "\n", "def", "tokenizer", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "_tokenizer", "\n", "\n"]], "home.repos.pwc.inspect_result.neulab_awesome-align.awesome_align.tokenization_utils.PreTrainedTokenizerFast.decoder": [[1635, 1638], ["None"], "methods", ["None"], ["", "@", "property", "\n", "def", "decoder", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "_tokenizer", ".", "_tokenizer", ".", "decoder", "\n", "\n"]], "home.repos.pwc.inspect_result.neulab_awesome-align.awesome_align.tokenization_utils.PreTrainedTokenizerFast.vocab_size": [[1639, 1642], ["tokenization_utils.PreTrainedTokenizerFast._tokenizer.get_vocab_size"], "methods", ["None"], ["", "@", "property", "\n", "def", "vocab_size", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "_tokenizer", ".", "get_vocab_size", "(", "with_added_tokens", "=", "False", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.neulab_awesome-align.awesome_align.tokenization_utils.PreTrainedTokenizerFast.__len__": [[1643, 1645], ["tokenization_utils.PreTrainedTokenizerFast._tokenizer.get_vocab_size"], "methods", ["None"], ["", "def", "__len__", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "_tokenizer", ".", "get_vocab_size", "(", "with_added_tokens", "=", "True", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.neulab_awesome-align.awesome_align.tokenization_utils.PreTrainedTokenizerFast.bos_token": [[1646, 1650], ["tokenization_utils.PreTrainedTokenizerFast._update_special_tokens"], "methods", ["home.repos.pwc.inspect_result.neulab_awesome-align.awesome_align.tokenization_utils.PreTrainedTokenizerFast._update_special_tokens"], ["", "@", "PreTrainedTokenizer", ".", "bos_token", ".", "setter", "\n", "def", "bos_token", "(", "self", ",", "value", ")", ":", "\n", "        ", "self", ".", "_bos_token", "=", "value", "\n", "self", ".", "_update_special_tokens", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.neulab_awesome-align.awesome_align.tokenization_utils.PreTrainedTokenizerFast.eos_token": [[1651, 1655], ["tokenization_utils.PreTrainedTokenizerFast._update_special_tokens"], "methods", ["home.repos.pwc.inspect_result.neulab_awesome-align.awesome_align.tokenization_utils.PreTrainedTokenizerFast._update_special_tokens"], ["", "@", "PreTrainedTokenizer", ".", "eos_token", ".", "setter", "\n", "def", "eos_token", "(", "self", ",", "value", ")", ":", "\n", "        ", "self", ".", "_eos_token", "=", "value", "\n", "self", ".", "_update_special_tokens", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.neulab_awesome-align.awesome_align.tokenization_utils.PreTrainedTokenizerFast.unk_token": [[1656, 1660], ["tokenization_utils.PreTrainedTokenizerFast._update_special_tokens"], "methods", ["home.repos.pwc.inspect_result.neulab_awesome-align.awesome_align.tokenization_utils.PreTrainedTokenizerFast._update_special_tokens"], ["", "@", "PreTrainedTokenizer", ".", "unk_token", ".", "setter", "\n", "def", "unk_token", "(", "self", ",", "value", ")", ":", "\n", "        ", "self", ".", "_unk_token", "=", "value", "\n", "self", ".", "_update_special_tokens", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.neulab_awesome-align.awesome_align.tokenization_utils.PreTrainedTokenizerFast.sep_token": [[1661, 1665], ["tokenization_utils.PreTrainedTokenizerFast._update_special_tokens"], "methods", ["home.repos.pwc.inspect_result.neulab_awesome-align.awesome_align.tokenization_utils.PreTrainedTokenizerFast._update_special_tokens"], ["", "@", "PreTrainedTokenizer", ".", "sep_token", ".", "setter", "\n", "def", "sep_token", "(", "self", ",", "value", ")", ":", "\n", "        ", "self", ".", "_sep_token", "=", "value", "\n", "self", ".", "_update_special_tokens", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.neulab_awesome-align.awesome_align.tokenization_utils.PreTrainedTokenizerFast.pad_token": [[1666, 1670], ["tokenization_utils.PreTrainedTokenizerFast._update_special_tokens"], "methods", ["home.repos.pwc.inspect_result.neulab_awesome-align.awesome_align.tokenization_utils.PreTrainedTokenizerFast._update_special_tokens"], ["", "@", "PreTrainedTokenizer", ".", "pad_token", ".", "setter", "\n", "def", "pad_token", "(", "self", ",", "value", ")", ":", "\n", "        ", "self", ".", "_pad_token", "=", "value", "\n", "self", ".", "_update_special_tokens", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.neulab_awesome-align.awesome_align.tokenization_utils.PreTrainedTokenizerFast.cls_token": [[1671, 1675], ["tokenization_utils.PreTrainedTokenizerFast._update_special_tokens"], "methods", ["home.repos.pwc.inspect_result.neulab_awesome-align.awesome_align.tokenization_utils.PreTrainedTokenizerFast._update_special_tokens"], ["", "@", "PreTrainedTokenizer", ".", "cls_token", ".", "setter", "\n", "def", "cls_token", "(", "self", ",", "value", ")", ":", "\n", "        ", "self", ".", "_cls_token", "=", "value", "\n", "self", ".", "_update_special_tokens", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.neulab_awesome-align.awesome_align.tokenization_utils.PreTrainedTokenizerFast.mask_token": [[1676, 1680], ["tokenization_utils.PreTrainedTokenizerFast._update_special_tokens"], "methods", ["home.repos.pwc.inspect_result.neulab_awesome-align.awesome_align.tokenization_utils.PreTrainedTokenizerFast._update_special_tokens"], ["", "@", "PreTrainedTokenizer", ".", "mask_token", ".", "setter", "\n", "def", "mask_token", "(", "self", ",", "value", ")", ":", "\n", "        ", "self", ".", "_mask_token", "=", "value", "\n", "self", ".", "_update_special_tokens", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.neulab_awesome-align.awesome_align.tokenization_utils.PreTrainedTokenizerFast.additional_special_tokens": [[1681, 1685], ["tokenization_utils.PreTrainedTokenizerFast._update_special_tokens"], "methods", ["home.repos.pwc.inspect_result.neulab_awesome-align.awesome_align.tokenization_utils.PreTrainedTokenizerFast._update_special_tokens"], ["", "@", "PreTrainedTokenizer", ".", "additional_special_tokens", ".", "setter", "\n", "def", "additional_special_tokens", "(", "self", ",", "value", ")", ":", "\n", "        ", "self", ".", "_additional_special_tokens", "=", "value", "\n", "self", ".", "_update_special_tokens", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.neulab_awesome-align.awesome_align.tokenization_utils.PreTrainedTokenizerFast._update_special_tokens": [[1686, 1689], ["tokenization_utils.PreTrainedTokenizerFast._tokenizer.add_special_tokens"], "methods", ["home.repos.pwc.inspect_result.neulab_awesome-align.awesome_align.tokenization_utils.PreTrainedTokenizerFast.add_special_tokens"], ["", "def", "_update_special_tokens", "(", "self", ")", ":", "\n", "        ", "if", "self", ".", "_tokenizer", "is", "not", "None", ":", "\n", "            ", "self", ".", "_tokenizer", ".", "add_special_tokens", "(", "self", ".", "all_special_tokens", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.neulab_awesome-align.awesome_align.tokenization_utils.PreTrainedTokenizerFast._convert_encoding": [[1690, 1742], ["collections.defaultdict", "encoding_dict[].append", "awesome_align.file_utils.is_tf_available", "tf.constant", "encoding_dict[].append", "encoding_dict[].append", "encoding_dict[].append", "encoding_dict[].append", "tf.constant", "tf.constant", "awesome_align.file_utils.is_torch_available", "torch.tensor", "torch.tensor", "torch.tensor", "logger.warning", "e.original_str.offsets"], "methods", ["home.repos.pwc.inspect_result.neulab_awesome-align.awesome_align.file_utils.is_tf_available", "home.repos.pwc.inspect_result.neulab_awesome-align.awesome_align.file_utils.is_torch_available"], ["", "", "@", "staticmethod", "\n", "def", "_convert_encoding", "(", "\n", "encoding", ",", "\n", "return_tensors", "=", "None", ",", "\n", "return_token_type_ids", "=", "True", ",", "\n", "return_attention_mask", "=", "True", ",", "\n", "return_overflowing_tokens", "=", "False", ",", "\n", "return_special_tokens_mask", "=", "False", ",", "\n", "return_offsets_mapping", "=", "False", ",", "\n", ")", ":", "\n", "        ", "if", "return_overflowing_tokens", "and", "encoding", ".", "overflowing", "is", "not", "None", ":", "\n", "            ", "encodings", "=", "[", "encoding", "]", "+", "encoding", ".", "overflowing", "\n", "", "else", ":", "\n", "            ", "encodings", "=", "[", "encoding", "]", "\n", "\n", "", "encoding_dict", "=", "defaultdict", "(", "list", ")", "\n", "for", "e", "in", "encodings", ":", "\n", "            ", "encoding_dict", "[", "\"input_ids\"", "]", ".", "append", "(", "e", ".", "ids", ")", "\n", "\n", "if", "return_token_type_ids", ":", "\n", "                ", "encoding_dict", "[", "\"token_type_ids\"", "]", ".", "append", "(", "e", ".", "type_ids", ")", "\n", "", "if", "return_attention_mask", ":", "\n", "                ", "encoding_dict", "[", "\"attention_mask\"", "]", ".", "append", "(", "e", ".", "attention_mask", ")", "\n", "", "if", "return_special_tokens_mask", ":", "\n", "                ", "encoding_dict", "[", "\"special_tokens_mask\"", "]", ".", "append", "(", "e", ".", "special_tokens_mask", ")", "\n", "", "if", "return_offsets_mapping", ":", "\n", "                ", "encoding_dict", "[", "\"offset_mapping\"", "]", ".", "append", "(", "[", "e", ".", "original_str", ".", "offsets", "(", "o", ")", "for", "o", "in", "e", ".", "offsets", "]", ")", "\n", "\n", "# Prepare inputs as tensors if asked", "\n", "", "", "if", "return_tensors", "==", "\"tf\"", "and", "is_tf_available", "(", ")", ":", "\n", "            ", "encoding_dict", "[", "\"input_ids\"", "]", "=", "tf", ".", "constant", "(", "encoding_dict", "[", "\"input_ids\"", "]", ")", "\n", "if", "\"token_type_ids\"", "in", "encoding_dict", ":", "\n", "                ", "encoding_dict", "[", "\"token_type_ids\"", "]", "=", "tf", ".", "constant", "(", "encoding_dict", "[", "\"token_type_ids\"", "]", ")", "\n", "\n", "", "if", "\"attention_mask\"", "in", "encoding_dict", ":", "\n", "                ", "encoding_dict", "[", "\"attention_mask\"", "]", "=", "tf", ".", "constant", "(", "encoding_dict", "[", "\"attention_mask\"", "]", ")", "\n", "\n", "", "", "elif", "return_tensors", "==", "\"pt\"", "and", "is_torch_available", "(", ")", ":", "\n", "            ", "encoding_dict", "[", "\"input_ids\"", "]", "=", "torch", ".", "tensor", "(", "encoding_dict", "[", "\"input_ids\"", "]", ")", "\n", "if", "\"token_type_ids\"", "in", "encoding_dict", ":", "\n", "                ", "encoding_dict", "[", "\"token_type_ids\"", "]", "=", "torch", ".", "tensor", "(", "encoding_dict", "[", "\"token_type_ids\"", "]", ")", "\n", "\n", "", "if", "\"attention_mask\"", "in", "encoding_dict", ":", "\n", "                ", "encoding_dict", "[", "\"attention_mask\"", "]", "=", "torch", ".", "tensor", "(", "encoding_dict", "[", "\"attention_mask\"", "]", ")", "\n", "", "", "elif", "return_tensors", "is", "not", "None", ":", "\n", "            ", "logger", ".", "warning", "(", "\n", "\"Unable to convert output to tensors format {}, PyTorch or TensorFlow is not available.\"", ".", "format", "(", "\n", "return_tensors", "\n", ")", "\n", ")", "\n", "\n", "", "return", "encoding_dict", "\n", "\n"]], "home.repos.pwc.inspect_result.neulab_awesome-align.awesome_align.tokenization_utils.PreTrainedTokenizerFast._convert_token_to_id_with_added_voc": [[1743, 1748], ["tokenization_utils.PreTrainedTokenizerFast._tokenizer.token_to_id"], "methods", ["None"], ["", "def", "_convert_token_to_id_with_added_voc", "(", "self", ",", "token", ")", ":", "\n", "        ", "id", "=", "self", ".", "_tokenizer", ".", "token_to_id", "(", "token", ")", "\n", "if", "id", "is", "None", ":", "\n", "            ", "return", "self", ".", "unk_token_id", "\n", "", "return", "id", "\n", "\n"]], "home.repos.pwc.inspect_result.neulab_awesome-align.awesome_align.tokenization_utils.PreTrainedTokenizerFast._convert_id_to_token": [[1749, 1751], ["tokenization_utils.PreTrainedTokenizerFast._tokenizer.id_to_token", "int"], "methods", ["None"], ["", "def", "_convert_id_to_token", "(", "self", ",", "index", ")", ":", "\n", "        ", "return", "self", ".", "_tokenizer", ".", "id_to_token", "(", "int", "(", "index", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.neulab_awesome-align.awesome_align.tokenization_utils.PreTrainedTokenizerFast.convert_tokens_to_string": [[1752, 1754], ["tokenization_utils.PreTrainedTokenizerFast._tokenizer.decode"], "methods", ["home.repos.pwc.inspect_result.neulab_awesome-align.awesome_align.tokenization_utils.PreTrainedTokenizerFast.decode"], ["", "def", "convert_tokens_to_string", "(", "self", ",", "tokens", ")", ":", "\n", "        ", "return", "self", ".", "_tokenizer", ".", "decode", "(", "tokens", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.neulab_awesome-align.awesome_align.tokenization_utils.PreTrainedTokenizerFast.add_tokens": [[1755, 1759], ["isinstance", "tokenization_utils.PreTrainedTokenizerFast._tokenizer.add_tokens"], "methods", ["home.repos.pwc.inspect_result.neulab_awesome-align.awesome_align.tokenization_utils.PreTrainedTokenizerFast.add_tokens"], ["", "def", "add_tokens", "(", "self", ",", "new_tokens", ")", ":", "\n", "        ", "if", "isinstance", "(", "new_tokens", ",", "str", ")", ":", "\n", "            ", "new_tokens", "=", "[", "new_tokens", "]", "\n", "", "return", "self", ".", "_tokenizer", ".", "add_tokens", "(", "new_tokens", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.neulab_awesome-align.awesome_align.tokenization_utils.PreTrainedTokenizerFast.add_special_tokens": [[1760, 1764], ["tokenization_utils.PreTrainedTokenizer.add_special_tokens", "tokenization_utils.PreTrainedTokenizerFast._update_special_tokens"], "methods", ["home.repos.pwc.inspect_result.neulab_awesome-align.awesome_align.tokenization_utils.PreTrainedTokenizerFast.add_special_tokens", "home.repos.pwc.inspect_result.neulab_awesome-align.awesome_align.tokenization_utils.PreTrainedTokenizerFast._update_special_tokens"], ["", "def", "add_special_tokens", "(", "self", ",", "special_tokens_dict", ")", ":", "\n", "        ", "added", "=", "super", "(", ")", ".", "add_special_tokens", "(", "special_tokens_dict", ")", "\n", "self", ".", "_update_special_tokens", "(", ")", "\n", "return", "added", "\n", "\n"]], "home.repos.pwc.inspect_result.neulab_awesome-align.awesome_align.tokenization_utils.PreTrainedTokenizerFast.build_inputs_with_special_tokens": [[1765, 1770], ["None"], "methods", ["None"], ["", "def", "build_inputs_with_special_tokens", "(", "self", ",", "token_ids_0", ",", "token_ids_1", "=", "None", ")", ":", "\n", "        ", "if", "token_ids_1", "is", "None", ":", "\n", "            ", "return", "token_ids_0", "\n", "", "else", ":", "\n", "            ", "return", "token_ids_0", "+", "token_ids_1", "\n", "\n"]], "home.repos.pwc.inspect_result.neulab_awesome-align.awesome_align.tokenization_utils.PreTrainedTokenizerFast.num_added_tokens": [[1771, 1773], ["tokenization_utils.PreTrainedTokenizerFast.tokenizer.num_special_tokens_to_add"], "methods", ["None"], ["", "", "def", "num_added_tokens", "(", "self", ",", "pair", "=", "False", ")", ":", "\n", "        ", "return", "self", ".", "tokenizer", ".", "num_special_tokens_to_add", "(", "pair", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.neulab_awesome-align.awesome_align.tokenization_utils.PreTrainedTokenizerFast.tokenize": [[1774, 1776], ["tokenization_utils.PreTrainedTokenizerFast.tokenizer.encode"], "methods", ["home.repos.pwc.inspect_result.neulab_awesome-align.awesome_align.tokenization_utils.PreTrainedTokenizer.encode"], ["", "def", "tokenize", "(", "self", ",", "text", ",", "**", "kwargs", ")", ":", "\n", "        ", "return", "self", ".", "tokenizer", ".", "encode", "(", "text", ")", ".", "tokens", "\n", "\n"]], "home.repos.pwc.inspect_result.neulab_awesome-align.awesome_align.tokenization_utils.PreTrainedTokenizerFast.batch_encode_plus": [[1777, 1870], ["tokens[].keys", "logger.warning", "ValueError", "tokenization_utils.truncate_and_pad", "tokenization_utils.PreTrainedTokenizerFast._convert_encoding", "isinstance", "TypeError", "len", "isinstance", "tokenization_utils.PreTrainedTokenizerFast._tokenizer.encode_batch", "tf.stack", "tokenization_utils.PreTrainedTokenizerFast._tokenizer.encode", "tokenization_utils.PreTrainedTokenizerFast._tokenizer.encode", "torch.stack", "enumerate", "type", "len", "len", "len"], "methods", ["home.repos.pwc.inspect_result.neulab_awesome-align.awesome_align.tokenization_utils.truncate_and_pad", "home.repos.pwc.inspect_result.neulab_awesome-align.awesome_align.tokenization_utils.PreTrainedTokenizerFast._convert_encoding", "home.repos.pwc.inspect_result.neulab_awesome-align.awesome_align.tokenization_utils.PreTrainedTokenizer.encode", "home.repos.pwc.inspect_result.neulab_awesome-align.awesome_align.tokenization_utils.PreTrainedTokenizer.encode"], ["", "def", "batch_encode_plus", "(", "\n", "self", ",", "\n", "batch_text_or_text_pairs", "=", "None", ",", "\n", "add_special_tokens", "=", "True", ",", "\n", "max_length", "=", "None", ",", "\n", "stride", "=", "0", ",", "\n", "truncation_strategy", "=", "\"longest_first\"", ",", "\n", "pad_to_max_length", "=", "False", ",", "\n", "return_tensors", "=", "None", ",", "\n", "return_token_type_ids", "=", "True", ",", "\n", "return_attention_mask", "=", "True", ",", "\n", "return_overflowing_tokens", "=", "False", ",", "\n", "return_special_tokens_mask", "=", "False", ",", "\n", "return_offsets_mapping", "=", "False", ",", "\n", "**", "kwargs", "\n", ")", ":", "\n", "        ", "if", "not", "add_special_tokens", ":", "\n", "            ", "logger", ".", "warning", "(", "\n", "\"Fast tokenizers add special tokens by default. To remove special tokens, please specify\"", "\n", "\"`add_special_tokens=False` during the initialisation rather than when calling `encode`,\"", "\n", "\"`encode_plus` or `batch_encode_plus`.\"", "\n", ")", "\n", "\n", "# Needed if we have to return a tensor", "\n", "", "pad_to_max_length", "=", "pad_to_max_length", "or", "(", "return_tensors", "is", "not", "None", ")", "\n", "\n", "# Throw an error if we can pad because there is no padding token", "\n", "if", "pad_to_max_length", "and", "self", ".", "pad_token_id", "is", "None", ":", "\n", "            ", "raise", "ValueError", "(", "\"Unable to set proper padding strategy as the tokenizer does not have a padding token\"", ")", "\n", "\n", "# Set the truncation and padding strategy and restore the initial configuration", "\n", "", "with", "truncate_and_pad", "(", "\n", "tokenizer", "=", "self", ".", "_tokenizer", ",", "\n", "max_length", "=", "max_length", ",", "\n", "stride", "=", "stride", ",", "\n", "strategy", "=", "truncation_strategy", ",", "\n", "pad_to_max_length", "=", "pad_to_max_length", ",", "\n", "padding_side", "=", "self", ".", "padding_side", ",", "\n", "pad_token_id", "=", "self", ".", "pad_token_id", ",", "\n", "pad_token_type_id", "=", "self", ".", "pad_token_type_id", ",", "\n", "pad_token", "=", "self", ".", "_pad_token", ",", "\n", ")", ":", "\n", "\n", "            ", "if", "not", "isinstance", "(", "batch_text_or_text_pairs", ",", "list", ")", ":", "\n", "                ", "raise", "TypeError", "(", "\n", "\"batch_text_or_text_pairs has to be a list (got {})\"", ".", "format", "(", "type", "(", "batch_text_or_text_pairs", ")", ")", "\n", ")", "\n", "\n", "# Avoid thread overhead if only one example.", "\n", "", "if", "len", "(", "batch_text_or_text_pairs", ")", "==", "1", ":", "\n", "                ", "if", "isinstance", "(", "batch_text_or_text_pairs", "[", "0", "]", ",", "(", "tuple", ",", "list", ")", ")", ":", "\n", "                    ", "tokens", "=", "self", ".", "_tokenizer", ".", "encode", "(", "*", "batch_text_or_text_pairs", "[", "0", "]", ")", "\n", "", "else", ":", "\n", "                    ", "tokens", "=", "self", ".", "_tokenizer", ".", "encode", "(", "batch_text_or_text_pairs", "[", "0", "]", ")", "\n", "", "tokens", "=", "[", "tokens", "]", "\n", "", "else", ":", "\n", "                ", "tokens", "=", "self", ".", "_tokenizer", ".", "encode_batch", "(", "batch_text_or_text_pairs", ")", "\n", "\n", "# Convert encoding to dict", "\n", "", "", "tokens", "=", "[", "\n", "self", ".", "_convert_encoding", "(", "\n", "encoding", "=", "encoding", ",", "\n", "return_tensors", "=", "return_tensors", ",", "\n", "return_token_type_ids", "=", "return_token_type_ids", ",", "\n", "return_attention_mask", "=", "return_attention_mask", ",", "\n", "return_overflowing_tokens", "=", "return_overflowing_tokens", ",", "\n", "return_special_tokens_mask", "=", "return_special_tokens_mask", ",", "\n", "return_offsets_mapping", "=", "return_offsets_mapping", ",", "\n", ")", "\n", "for", "encoding", "in", "tokens", "\n", "]", "\n", "\n", "# Sanitize the output to have dict[list] from list[dict]", "\n", "sanitized", "=", "{", "}", "\n", "for", "key", "in", "tokens", "[", "0", "]", ".", "keys", "(", ")", ":", "\n", "            ", "stack", "=", "[", "e", "for", "item", "in", "tokens", "for", "e", "in", "item", "[", "key", "]", "]", "\n", "if", "return_tensors", "==", "\"tf\"", ":", "\n", "                ", "stack", "=", "tf", ".", "stack", "(", "stack", ",", "axis", "=", "0", ")", "\n", "", "elif", "return_tensors", "==", "\"pt\"", ":", "\n", "                ", "stack", "=", "torch", ".", "stack", "(", "stack", ",", "dim", "=", "0", ")", "\n", "", "elif", "not", "return_tensors", "and", "len", "(", "stack", ")", "==", "1", ":", "\n", "                ", "stack", "=", "stack", "[", "0", "]", "\n", "\n", "", "sanitized", "[", "key", "]", "=", "stack", "\n", "\n", "# If returning overflowing tokens, we need to return a mapping", "\n", "# from the batch idx to the original sample", "\n", "", "if", "return_overflowing_tokens", ":", "\n", "            ", "overflow_to_sample_mapping", "=", "[", "\n", "i", "if", "len", "(", "item", "[", "\"input_ids\"", "]", ")", "==", "1", "else", "[", "i", "]", "*", "len", "(", "item", "[", "\"input_ids\"", "]", ")", "for", "i", ",", "item", "in", "enumerate", "(", "tokens", ")", "\n", "]", "\n", "sanitized", "[", "\"overflow_to_sample_mapping\"", "]", "=", "overflow_to_sample_mapping", "\n", "", "return", "sanitized", "\n", "\n"]], "home.repos.pwc.inspect_result.neulab_awesome-align.awesome_align.tokenization_utils.PreTrainedTokenizerFast.encode_plus": [[1871, 1910], ["tokenization_utils.PreTrainedTokenizerFast.batch_encode_plus", "isinstance", "tokenization_utils.PreTrainedTokenizerFast.items"], "methods", ["home.repos.pwc.inspect_result.neulab_awesome-align.awesome_align.tokenization_utils.PreTrainedTokenizerFast.batch_encode_plus"], ["", "def", "encode_plus", "(", "\n", "self", ",", "\n", "text", ",", "\n", "text_pair", "=", "None", ",", "\n", "add_special_tokens", "=", "False", ",", "\n", "max_length", "=", "None", ",", "\n", "pad_to_max_length", "=", "False", ",", "\n", "stride", "=", "0", ",", "\n", "truncation_strategy", "=", "\"longest_first\"", ",", "\n", "return_tensors", "=", "None", ",", "\n", "return_token_type_ids", "=", "True", ",", "\n", "return_attention_mask", "=", "True", ",", "\n", "return_overflowing_tokens", "=", "False", ",", "\n", "return_special_tokens_mask", "=", "False", ",", "\n", "return_offsets_mapping", "=", "False", ",", "\n", "**", "kwargs", "\n", ")", ":", "\n", "        ", "batched_input", "=", "[", "(", "text", ",", "text_pair", ")", "]", "if", "text_pair", "else", "[", "text", "]", "\n", "batched_output", "=", "self", ".", "batch_encode_plus", "(", "\n", "batched_input", ",", "\n", "add_special_tokens", "=", "add_special_tokens", ",", "\n", "max_length", "=", "max_length", ",", "\n", "stride", "=", "stride", ",", "\n", "truncation_strategy", "=", "truncation_strategy", ",", "\n", "return_tensors", "=", "return_tensors", ",", "\n", "return_token_type_ids", "=", "return_token_type_ids", ",", "\n", "return_attention_mask", "=", "return_attention_mask", ",", "\n", "return_overflowing_tokens", "=", "return_overflowing_tokens", ",", "\n", "return_special_tokens_mask", "=", "return_special_tokens_mask", ",", "\n", "return_offsets_mapping", "=", "return_offsets_mapping", ",", "\n", "pad_to_max_length", "=", "pad_to_max_length", ",", "\n", "**", "kwargs", ",", "\n", ")", "\n", "\n", "# Return tensor is None, then we can remove the leading batch axis", "\n", "if", "not", "return_tensors", ":", "\n", "            ", "return", "{", "key", ":", "value", "[", "0", "]", "if", "isinstance", "(", "value", "[", "0", "]", ",", "list", ")", "else", "value", "for", "key", ",", "value", "in", "batched_output", ".", "items", "(", ")", "}", "\n", "", "else", ":", "\n", "            ", "return", "batched_output", "\n", "\n"]], "home.repos.pwc.inspect_result.neulab_awesome-align.awesome_align.tokenization_utils.PreTrainedTokenizerFast.decode": [[1911, 1919], ["tokenization_utils.PreTrainedTokenizerFast.tokenizer.decode", "tokenization_utils.PreTrainedTokenizerFast.clean_up_tokenization"], "methods", ["home.repos.pwc.inspect_result.neulab_awesome-align.awesome_align.tokenization_utils.PreTrainedTokenizerFast.decode", "home.repos.pwc.inspect_result.neulab_awesome-align.awesome_align.tokenization_utils.PreTrainedTokenizer.clean_up_tokenization"], ["", "", "def", "decode", "(", "self", ",", "token_ids", ",", "skip_special_tokens", "=", "False", ",", "clean_up_tokenization_spaces", "=", "True", ")", ":", "\n", "        ", "text", "=", "self", ".", "tokenizer", ".", "decode", "(", "token_ids", ",", "skip_special_tokens", ")", "\n", "\n", "if", "clean_up_tokenization_spaces", ":", "\n", "            ", "clean_text", "=", "self", ".", "clean_up_tokenization", "(", "text", ")", "\n", "return", "clean_text", "\n", "", "else", ":", "\n", "            ", "return", "text", "\n", "\n"]], "home.repos.pwc.inspect_result.neulab_awesome-align.awesome_align.tokenization_utils.PreTrainedTokenizerFast.save_vocabulary": [[1920, 1928], ["os.path.isdir", "tuple", "tokenization_utils.PreTrainedTokenizerFast._tokenizer.save", "os.path.split", "tokenization_utils.PreTrainedTokenizerFast._tokenizer.save", "os.path.abspath"], "methods", ["None"], ["", "", "def", "save_vocabulary", "(", "self", ",", "save_directory", ")", ":", "\n", "        ", "if", "os", ".", "path", ".", "isdir", "(", "save_directory", ")", ":", "\n", "            ", "files", "=", "self", ".", "_tokenizer", ".", "save", "(", "save_directory", ")", "\n", "", "else", ":", "\n", "            ", "folder", ",", "file", "=", "os", ".", "path", ".", "split", "(", "os", ".", "path", ".", "abspath", "(", "save_directory", ")", ")", "\n", "files", "=", "self", ".", "_tokenizer", ".", "save", "(", "folder", ",", "name", "=", "file", ")", "\n", "\n", "", "return", "tuple", "(", "files", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.neulab_awesome-align.awesome_align.tokenization_utils.truncate_and_pad": [[43, 103], ["tokenizer.enable_truncation", "tokenizer.enable_padding", "tokenizer.no_truncation", "tokenizer.no_padding", "logger.warning"], "function", ["None"], ["@", "contextmanager", "\n", "def", "truncate_and_pad", "(", "\n", "tokenizer", ":", "BaseTokenizer", ",", "\n", "max_length", ":", "int", ",", "\n", "stride", ":", "int", ",", "\n", "strategy", ":", "str", ",", "\n", "pad_to_max_length", ":", "bool", ",", "\n", "padding_side", ":", "str", ",", "\n", "pad_token_id", ":", "int", ",", "\n", "pad_token_type_id", ":", "int", ",", "\n", "pad_token", ":", "str", ",", "\n", ")", ":", "\n", "    ", "\"\"\"\n    This contextmanager is in charge of defining the truncation and the padding strategies and then\n    restore the tokenizer settings afterwards.\n\n    This contextmanager assumes the provider tokenizer has no padding / truncation strategy\n    before the managed section. If your tokenizer set a padding / truncation strategy before,\n    then it will be reset to no padding/truncation when exiting the managed section.\n\n    :param tokenizer:\n    :param max_length:\n    :param stride:\n    :param strategy:\n    :param pad_to_max_length:\n    :param padding_side:\n    :param pad_token_id:\n    :param pad_token_type_id:\n    :param pad_token:\n    :return:\n    \"\"\"", "\n", "\n", "# Handle all the truncation and padding stuff", "\n", "if", "max_length", "is", "not", "None", ":", "\n", "        ", "tokenizer", ".", "enable_truncation", "(", "max_length", ",", "stride", "=", "stride", ",", "strategy", "=", "strategy", ")", "\n", "\n", "", "if", "pad_to_max_length", "and", "(", "pad_token", "and", "pad_token_id", ">=", "0", ")", ":", "\n", "        ", "tokenizer", ".", "enable_padding", "(", "\n", "max_length", "=", "max_length", ",", "\n", "direction", "=", "padding_side", ",", "\n", "pad_id", "=", "pad_token_id", ",", "\n", "pad_type_id", "=", "pad_token_type_id", ",", "\n", "pad_token", "=", "pad_token", ",", "\n", ")", "\n", "", "elif", "pad_to_max_length", ":", "\n", "        ", "logger", ".", "warning", "(", "\n", "\"Disabled padding because no padding token set (pad_token: {}, pad_token_id: {}).\\n\"", "\n", "\"To remove this error, you can add a new pad token and then resize model embedding:\\n\"", "\n", "\"\\ttokenizer.pad_token = '<PAD>'\\n\\tmodel.resize_token_embeddings(len(tokenizer))\"", ".", "format", "(", "\n", "pad_token", ",", "pad_token_id", "\n", ")", "\n", ")", "\n", "\n", "", "yield", "\n", "\n", "if", "max_length", "is", "not", "None", ":", "\n", "        ", "tokenizer", ".", "no_truncation", "(", ")", "\n", "\n", "", "if", "pad_to_max_length", "and", "(", "pad_token", "and", "pad_token_id", ">=", "0", ")", ":", "\n", "        ", "tokenizer", ".", "no_padding", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.neulab_awesome-align.awesome_align.modeling_utils.ModuleUtilsMixin.num_parameters": [[63, 69], ["sum", "filter", "modeling_utils.ModuleUtilsMixin.parameters", "modeling_utils.ModuleUtilsMixin.parameters", "p.numel"], "methods", ["None"], ["def", "num_parameters", "(", "self", ",", "only_trainable", ":", "bool", "=", "False", ")", "->", "int", ":", "\n", "        ", "\"\"\"\n        Get number of (optionally, trainable) parameters in the module.\n        \"\"\"", "\n", "params", "=", "filter", "(", "lambda", "x", ":", "x", ".", "requires_grad", ",", "self", ".", "parameters", "(", ")", ")", "if", "only_trainable", "else", "self", ".", "parameters", "(", ")", "\n", "return", "sum", "(", "p", ".", "numel", "(", ")", "for", "p", "in", "params", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.neulab_awesome-align.awesome_align.modeling_utils.PreTrainedModel.dummy_inputs": [[92, 100], ["torch.tensor"], "methods", ["None"], ["@", "property", "\n", "def", "dummy_inputs", "(", "self", ")", ":", "\n", "        ", "\"\"\" Dummy inputs to do a forward pass in the network.\n\n        Returns:\n            torch.Tensor with dummy inputs\n        \"\"\"", "\n", "return", "{", "\"input_ids\"", ":", "torch", ".", "tensor", "(", "DUMMY_INPUTS", ")", "}", "\n", "\n"]], "home.repos.pwc.inspect_result.neulab_awesome-align.awesome_align.modeling_utils.PreTrainedModel.__init__": [[101, 113], ["super().__init__", "isinstance", "ValueError"], "methods", ["home.repos.pwc.inspect_result.neulab_awesome-align.awesome_align.train_utils.AdamW.__init__"], ["", "def", "__init__", "(", "self", ",", "config", ",", "*", "inputs", ",", "**", "kwargs", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "if", "not", "isinstance", "(", "config", ",", "PretrainedConfig", ")", ":", "\n", "            ", "raise", "ValueError", "(", "\n", "\"Parameter config in `{}(config)` should be an instance of class `PretrainedConfig`. \"", "\n", "\"To create a model from a pretrained model use \"", "\n", "\"`model = {}.from_pretrained(PRETRAINED_MODEL_NAME)`\"", ".", "format", "(", "\n", "self", ".", "__class__", ".", "__name__", ",", "self", ".", "__class__", ".", "__name__", "\n", ")", "\n", ")", "\n", "# Save config in model", "\n", "", "self", ".", "config", "=", "config", "\n", "\n"]], "home.repos.pwc.inspect_result.neulab_awesome-align.awesome_align.modeling_utils.PreTrainedModel.base_model": [[114, 117], ["getattr"], "methods", ["None"], ["", "@", "property", "\n", "def", "base_model", "(", "self", ")", ":", "\n", "        ", "return", "getattr", "(", "self", ",", "self", ".", "base_model_prefix", ",", "self", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.neulab_awesome-align.awesome_align.modeling_utils.PreTrainedModel.get_input_embeddings": [[118, 131], ["getattr", "getattr.get_input_embeddings"], "methods", ["home.repos.pwc.inspect_result.neulab_awesome-align.awesome_align.modeling_utils.PreTrainedModel.get_input_embeddings"], ["", "def", "get_input_embeddings", "(", "self", ")", ":", "\n", "        ", "\"\"\"\n        Returns the model's input embeddings.\n\n        Returns:\n            :obj:`nn.Module`:\n                A torch module mapping vocabulary to hidden states.\n        \"\"\"", "\n", "base_model", "=", "getattr", "(", "self", ",", "self", ".", "base_model_prefix", ",", "self", ")", "\n", "if", "base_model", "is", "not", "self", ":", "\n", "            ", "return", "base_model", ".", "get_input_embeddings", "(", ")", "\n", "", "else", ":", "\n", "            ", "raise", "NotImplementedError", "\n", "\n"]], "home.repos.pwc.inspect_result.neulab_awesome-align.awesome_align.modeling_utils.PreTrainedModel.set_input_embeddings": [[132, 145], ["getattr", "getattr.set_input_embeddings"], "methods", ["home.repos.pwc.inspect_result.neulab_awesome-align.awesome_align.modeling_utils.PreTrainedModel.set_input_embeddings"], ["", "", "def", "set_input_embeddings", "(", "self", ",", "value", ")", ":", "\n", "        ", "\"\"\"\n        Set model's input embeddings\n\n        Args:\n            value (:obj:`nn.Module`):\n                A module mapping vocabulary to hidden states.\n        \"\"\"", "\n", "base_model", "=", "getattr", "(", "self", ",", "self", ".", "base_model_prefix", ",", "self", ")", "\n", "if", "base_model", "is", "not", "self", ":", "\n", "            ", "base_model", ".", "set_input_embeddings", "(", "value", ")", "\n", "", "else", ":", "\n", "            ", "raise", "NotImplementedError", "\n", "\n"]], "home.repos.pwc.inspect_result.neulab_awesome-align.awesome_align.modeling_utils.PreTrainedModel.get_output_embeddings": [[146, 155], ["None"], "methods", ["None"], ["", "", "def", "get_output_embeddings", "(", "self", ")", ":", "\n", "        ", "\"\"\"\n        Returns the model's output embeddings.\n\n        Returns:\n            :obj:`nn.Module`:\n                A torch module mapping hidden states to vocabulary.\n        \"\"\"", "\n", "return", "None", "# Overwrite for models with output embeddings", "\n", "\n"]], "home.repos.pwc.inspect_result.neulab_awesome-align.awesome_align.modeling_utils.PreTrainedModel.tie_weights": [[156, 165], ["modeling_utils.PreTrainedModel.get_output_embeddings", "modeling_utils.PreTrainedModel._tie_or_clone_weights", "modeling_utils.PreTrainedModel.get_input_embeddings"], "methods", ["home.repos.pwc.inspect_result.neulab_awesome-align.awesome_align.modeling_utils.PreTrainedModel.get_output_embeddings", "home.repos.pwc.inspect_result.neulab_awesome-align.awesome_align.modeling_utils.PreTrainedModel._tie_or_clone_weights", "home.repos.pwc.inspect_result.neulab_awesome-align.awesome_align.modeling_utils.PreTrainedModel.get_input_embeddings"], ["", "def", "tie_weights", "(", "self", ")", ":", "\n", "        ", "\"\"\"\n        Tie the weights between the input embeddings and the output embeddings.\n        If the `torchscript` flag is set in the configuration, can't handle parameter sharing so we are cloning\n        the weights instead.\n        \"\"\"", "\n", "output_embeddings", "=", "self", ".", "get_output_embeddings", "(", ")", "\n", "if", "output_embeddings", "is", "not", "None", ":", "\n", "            ", "self", ".", "_tie_or_clone_weights", "(", "output_embeddings", ",", "self", ".", "get_input_embeddings", "(", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.neulab_awesome-align.awesome_align.modeling_utils.PreTrainedModel._tie_or_clone_weights": [[166, 183], ["torch.nn.Parameter", "getattr", "torch.nn.functional.pad", "hasattr", "hasattr", "input_embeddings.weight.clone"], "methods", ["None"], ["", "", "def", "_tie_or_clone_weights", "(", "self", ",", "output_embeddings", ",", "input_embeddings", ")", ":", "\n", "        ", "\"\"\" Tie or clone module weights depending of weither we are using TorchScript or not\n        \"\"\"", "\n", "if", "self", ".", "config", ".", "torchscript", ":", "\n", "            ", "output_embeddings", ".", "weight", "=", "nn", ".", "Parameter", "(", "input_embeddings", ".", "weight", ".", "clone", "(", ")", ")", "\n", "", "else", ":", "\n", "            ", "output_embeddings", ".", "weight", "=", "input_embeddings", ".", "weight", "\n", "\n", "", "if", "getattr", "(", "output_embeddings", ",", "\"bias\"", ",", "None", ")", "is", "not", "None", ":", "\n", "            ", "output_embeddings", ".", "bias", ".", "data", "=", "torch", ".", "nn", ".", "functional", ".", "pad", "(", "\n", "output_embeddings", ".", "bias", ".", "data", ",", "\n", "(", "0", ",", "output_embeddings", ".", "weight", ".", "shape", "[", "0", "]", "-", "output_embeddings", ".", "bias", ".", "shape", "[", "0", "]", ")", ",", "\n", "\"constant\"", ",", "\n", "0", ",", "\n", ")", "\n", "", "if", "hasattr", "(", "output_embeddings", ",", "\"out_features\"", ")", "and", "hasattr", "(", "input_embeddings", ",", "\"num_embeddings\"", ")", ":", "\n", "            ", "output_embeddings", ".", "out_features", "=", "input_embeddings", ".", "num_embeddings", "\n", "\n"]], "home.repos.pwc.inspect_result.neulab_awesome-align.awesome_align.modeling_utils.PreTrainedModel.resize_token_embeddings": [[184, 210], ["getattr", "getattr._resize_token_embeddings", "modeling_utils.PreTrainedModel.tie_weights"], "methods", ["home.repos.pwc.inspect_result.neulab_awesome-align.awesome_align.modeling_utils.PreTrainedModel._resize_token_embeddings", "home.repos.pwc.inspect_result.neulab_awesome-align.awesome_align.modeling_utils.PreTrainedModel.tie_weights"], ["", "", "def", "resize_token_embeddings", "(", "self", ",", "new_num_tokens", "=", "None", ")", ":", "\n", "        ", "\"\"\" Resize input token embeddings matrix of the model if new_num_tokens != config.vocab_size.\n        Take care of tying weights embeddings afterwards if the model class has a `tie_weights()` method.\n\n        Arguments:\n\n            new_num_tokens: (`optional`) int:\n                New number of tokens in the embedding matrix. Increasing the size will add newly initialized vectors at the end. Reducing the size will remove vectors from the end.\n                If not provided or None: does nothing and just returns a pointer to the input tokens ``torch.nn.Embeddings`` Module of the model.\n\n        Return: ``torch.nn.Embeddings``\n            Pointer to the input tokens Embeddings Module of the model\n        \"\"\"", "\n", "base_model", "=", "getattr", "(", "self", ",", "self", ".", "base_model_prefix", ",", "self", ")", "# get the base model if needed", "\n", "model_embeds", "=", "base_model", ".", "_resize_token_embeddings", "(", "new_num_tokens", ")", "\n", "if", "new_num_tokens", "is", "None", ":", "\n", "            ", "return", "model_embeds", "\n", "\n", "# Update base model and current model config", "\n", "", "self", ".", "config", ".", "vocab_size", "=", "new_num_tokens", "\n", "base_model", ".", "vocab_size", "=", "new_num_tokens", "\n", "\n", "# Tie weights again if needed", "\n", "self", ".", "tie_weights", "(", ")", "\n", "\n", "return", "model_embeds", "\n", "\n"]], "home.repos.pwc.inspect_result.neulab_awesome-align.awesome_align.modeling_utils.PreTrainedModel._resize_token_embeddings": [[211, 216], ["modeling_utils.PreTrainedModel.get_input_embeddings", "modeling_utils.PreTrainedModel._get_resized_embeddings", "modeling_utils.PreTrainedModel.set_input_embeddings", "modeling_utils.PreTrainedModel.get_input_embeddings"], "methods", ["home.repos.pwc.inspect_result.neulab_awesome-align.awesome_align.modeling_utils.PreTrainedModel.get_input_embeddings", "home.repos.pwc.inspect_result.neulab_awesome-align.awesome_align.modeling_utils.PreTrainedModel._get_resized_embeddings", "home.repos.pwc.inspect_result.neulab_awesome-align.awesome_align.modeling_utils.PreTrainedModel.set_input_embeddings", "home.repos.pwc.inspect_result.neulab_awesome-align.awesome_align.modeling_utils.PreTrainedModel.get_input_embeddings"], ["", "def", "_resize_token_embeddings", "(", "self", ",", "new_num_tokens", ")", ":", "\n", "        ", "old_embeddings", "=", "self", ".", "get_input_embeddings", "(", ")", "\n", "new_embeddings", "=", "self", ".", "_get_resized_embeddings", "(", "old_embeddings", ",", "new_num_tokens", ")", "\n", "self", ".", "set_input_embeddings", "(", "new_embeddings", ")", "\n", "return", "self", ".", "get_input_embeddings", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.neulab_awesome-align.awesome_align.modeling_utils.PreTrainedModel._get_resized_embeddings": [[217, 250], ["old_embeddings.weight.size", "torch.nn.Embedding", "torch.nn.Embedding.to", "modeling_utils.PreTrainedModel._init_weights", "min"], "methods", ["home.repos.pwc.inspect_result.neulab_awesome-align.awesome_align.modeling.BertPreTrainedModel._init_weights"], ["", "def", "_get_resized_embeddings", "(", "self", ",", "old_embeddings", ",", "new_num_tokens", "=", "None", ")", ":", "\n", "        ", "\"\"\" Build a resized Embedding Module from a provided token Embedding Module.\n            Increasing the size will add newly initialized vectors at the end\n            Reducing the size will remove vectors from the end\n\n        Args:\n            new_num_tokens: (`optional`) int\n                New number of tokens in the embedding matrix.\n                Increasing the size will add newly initialized vectors at the end\n                Reducing the size will remove vectors from the end\n                If not provided or None: return the provided token Embedding Module.\n        Return: ``torch.nn.Embeddings``\n            Pointer to the resized Embedding Module or the old Embedding Module if new_num_tokens is None\n        \"\"\"", "\n", "if", "new_num_tokens", "is", "None", ":", "\n", "            ", "return", "old_embeddings", "\n", "\n", "", "old_num_tokens", ",", "old_embedding_dim", "=", "old_embeddings", ".", "weight", ".", "size", "(", ")", "\n", "if", "old_num_tokens", "==", "new_num_tokens", ":", "\n", "            ", "return", "old_embeddings", "\n", "\n", "# Build new embeddings", "\n", "", "new_embeddings", "=", "nn", ".", "Embedding", "(", "new_num_tokens", ",", "old_embedding_dim", ")", "\n", "new_embeddings", ".", "to", "(", "old_embeddings", ".", "weight", ".", "device", ")", "\n", "\n", "# initialize all new embeddings (in particular added tokens)", "\n", "self", ".", "_init_weights", "(", "new_embeddings", ")", "\n", "\n", "# Copy word embeddings from the previous weights", "\n", "num_tokens_to_copy", "=", "min", "(", "old_num_tokens", ",", "new_num_tokens", ")", "\n", "new_embeddings", ".", "weight", ".", "data", "[", ":", "num_tokens_to_copy", ",", ":", "]", "=", "old_embeddings", ".", "weight", ".", "data", "[", ":", "num_tokens_to_copy", ",", ":", "]", "\n", "\n", "return", "new_embeddings", "\n", "\n"]], "home.repos.pwc.inspect_result.neulab_awesome-align.awesome_align.modeling_utils.PreTrainedModel.init_weights": [[251, 259], ["modeling_utils.PreTrainedModel.apply", "modeling_utils.PreTrainedModel.tie_weights"], "methods", ["home.repos.pwc.inspect_result.neulab_awesome-align.awesome_align.modeling_utils.PreTrainedModel.tie_weights"], ["", "def", "init_weights", "(", "self", ")", ":", "\n", "        ", "\"\"\" Initialize and prunes weights if needed. \"\"\"", "\n", "# Initialize weights", "\n", "self", ".", "apply", "(", "self", ".", "_init_weights", ")", "\n", "\n", "\n", "# Tie weights if needed", "\n", "self", ".", "tie_weights", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.neulab_awesome-align.awesome_align.modeling_utils.PreTrainedModel.prune_heads": [[260, 274], ["heads_to_prune.items", "modeling_utils.PreTrainedModel.base_model._prune_heads", "list", "set", "set", "modeling_utils.PreTrainedModel.config.pruned_heads.get"], "methods", ["None"], ["", "def", "prune_heads", "(", "self", ",", "heads_to_prune", ")", ":", "\n", "        ", "\"\"\" Prunes heads of the base model.\n\n            Arguments:\n\n                heads_to_prune: dict with keys being selected layer indices (`int`) and associated values being the list of heads to prune in said layer (list of `int`).\n                E.g. {1: [0, 2], 2: [2, 3]} will prune heads 0 and 2 on layer 1 and heads 2 and 3 on layer 2.\n        \"\"\"", "\n", "# save new sets of pruned heads as union of previously stored pruned heads and newly pruned heads", "\n", "for", "layer", ",", "heads", "in", "heads_to_prune", ".", "items", "(", ")", ":", "\n", "            ", "union_heads", "=", "set", "(", "self", ".", "config", ".", "pruned_heads", ".", "get", "(", "layer", ",", "[", "]", ")", ")", "|", "set", "(", "heads", ")", "\n", "self", ".", "config", ".", "pruned_heads", "[", "layer", "]", "=", "list", "(", "union_heads", ")", "# Unfortunately we have to store it as list for JSON", "\n", "\n", "", "self", ".", "base_model", ".", "_prune_heads", "(", "heads_to_prune", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.neulab_awesome-align.awesome_align.modeling_utils.PreTrainedModel.save_pretrained": [[275, 296], ["os.path.isdir", "model_to_save.config.save_pretrained", "os.path.join", "torch.save", "logger.info", "hasattr", "model_to_save.state_dict"], "methods", ["home.repos.pwc.inspect_result.neulab_awesome-align.awesome_align.configuration_utils.PretrainedConfig.save_pretrained"], ["", "def", "save_pretrained", "(", "self", ",", "save_directory", ")", ":", "\n", "        ", "\"\"\" Save a model and its configuration file to a directory, so that it\n            can be re-loaded using the `:func:`~transformers.PreTrainedModel.from_pretrained`` class method.\n        \"\"\"", "\n", "assert", "os", ".", "path", ".", "isdir", "(", "\n", "save_directory", "\n", ")", ",", "\"Saving path should be a directory where the model and configuration can be saved\"", "\n", "\n", "# Only save the model itself if we are using distributed training", "\n", "model_to_save", "=", "self", ".", "module", "if", "hasattr", "(", "self", ",", "\"module\"", ")", "else", "self", "\n", "\n", "# Attach architecture to the config", "\n", "model_to_save", ".", "config", ".", "architectures", "=", "[", "model_to_save", ".", "__class__", ".", "__name__", "]", "\n", "\n", "# Save configuration file", "\n", "model_to_save", ".", "config", ".", "save_pretrained", "(", "save_directory", ")", "\n", "\n", "# If we save using the predefined names, we can load using `from_pretrained`", "\n", "output_model_file", "=", "os", ".", "path", ".", "join", "(", "save_directory", ",", "WEIGHTS_NAME", ")", "\n", "torch", ".", "save", "(", "model_to_save", ".", "state_dict", "(", ")", ",", "output_model_file", ")", "\n", "logger", ".", "info", "(", "\"Model weights saved in {}\"", ".", "format", "(", "output_model_file", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.neulab_awesome-align.awesome_align.modeling_utils.PreTrainedModel.from_pretrained": [[297, 571], ["kwargs.pop", "kwargs.pop", "kwargs.pop", "kwargs.pop", "kwargs.pop", "kwargs.pop", "kwargs.pop", "kwargs.pop", "kwargs.pop", "cls", "load_tf2_checkpoint_in_pytorch_model.tie_weights", "load_tf2_checkpoint_in_pytorch_model.eval", "isinstance", "cls.config_class.from_pretrained", "awesome_align.file_utils.cached_path.endswith", "torch.load.keys", "zip", "getattr", "torch.load.copy", "modeling_utils.PreTrainedModel.from_pretrained.load"], "methods", ["home.repos.pwc.inspect_result.neulab_awesome-align.awesome_align.modeling_utils.PreTrainedModel.tie_weights", "home.repos.pwc.inspect_result.neulab_awesome-align.awesome_align.configuration_utils.PretrainedConfig.from_pretrained"], ["", "@", "classmethod", "\n", "def", "from_pretrained", "(", "cls", ",", "pretrained_model_name_or_path", ",", "*", "model_args", ",", "**", "kwargs", ")", ":", "\n", "        ", "r\"\"\"Instantiate a pretrained pytorch model from a pre-trained model configuration.\n\n        The model is set in evaluation mode by default using ``model.eval()`` (Dropout modules are deactivated)\n        To train the model, you should first set it back in training mode with ``model.train()``\n\n        The warning ``Weights from XXX not initialized from pretrained model`` means that the weights of XXX do not come pre-trained with the rest of the model.\n        It is up to you to train those weights with a downstream fine-tuning task.\n\n        The warning ``Weights from XXX not used in YYY`` means that the layer XXX is not used by YYY, therefore those weights are discarded.\n\n        Parameters:\n            pretrained_model_name_or_path: either:\n              - a string with the `shortcut name` of a pre-trained model to load from cache or download, e.g.: ``bert-base-uncased``.\n              - a string with the `identifier name` of a pre-trained model that was user-uploaded to our S3, e.g.: ``dbmdz/bert-base-german-cased``.\n              - a path to a `directory` containing model weights saved using :func:`~transformers.PreTrainedModel.save_pretrained`, e.g.: ``./my_model_directory/``.\n              - a path or url to a `tensorflow index checkpoint file` (e.g. `./tf_model/model.ckpt.index`). In this case, ``from_tf`` should be set to True and a configuration object should be provided as ``config`` argument. This loading path is slower than converting the TensorFlow checkpoint in a PyTorch model using the provided conversion scripts and loading the PyTorch model afterwards.\n              - None if you are both providing the configuration and state dictionary (resp. with keyword arguments ``config`` and ``state_dict``)\n\n            model_args: (`optional`) Sequence of positional arguments:\n                All remaning positional arguments will be passed to the underlying model's ``__init__`` method\n\n            config: (`optional`) one of:\n                - an instance of a class derived from :class:`~transformers.PretrainedConfig`, or\n                - a string valid as input to :func:`~transformers.PretrainedConfig.from_pretrained()`\n                Configuration for the model to use instead of an automatically loaded configuation. Configuration can be automatically loaded when:\n                    - the model is a model provided by the library (loaded with the ``shortcut-name`` string of a pretrained model), or\n                    - the model was saved using :func:`~transformers.PreTrainedModel.save_pretrained` and is reloaded by suppling the save directory.\n                    - the model is loaded by suppling a local directory as ``pretrained_model_name_or_path`` and a configuration JSON file named `config.json` is found in the directory.\n\n            state_dict: (`optional`) dict:\n                an optional state dictionnary for the model to use instead of a state dictionary loaded from saved weights file.\n                This option can be used if you want to create a model from a pretrained configuration but load your own weights.\n                In this case though, you should check if using :func:`~transformers.PreTrainedModel.save_pretrained` and :func:`~transformers.PreTrainedModel.from_pretrained` is not a simpler option.\n\n            cache_dir: (`optional`) string:\n                Path to a directory in which a downloaded pre-trained model\n                configuration should be cached if the standard cache should not be used.\n\n            force_download: (`optional`) boolean, default False:\n                Force to (re-)download the model weights and configuration files and override the cached versions if they exists.\n\n            resume_download: (`optional`) boolean, default False:\n                Do not delete incompletely recieved file. Attempt to resume the download if such a file exists.\n\n            proxies: (`optional`) dict, default None:\n                A dictionary of proxy servers to use by protocol or endpoint, e.g.: {'http': 'foo.bar:3128', 'http://hostname': 'foo.bar:4012'}.\n                The proxies are used on each request.\n\n            output_loading_info: (`optional`) boolean:\n                Set to ``True`` to also return a dictionnary containing missing keys, unexpected keys and error messages.\n\n            kwargs: (`optional`) Remaining dictionary of keyword arguments:\n                Can be used to update the configuration object (after it being loaded) and initiate the model. (e.g. ``output_attention=True``). Behave differently depending on whether a `config` is provided or automatically loaded:\n\n                - If a configuration is provided with ``config``, ``**kwargs`` will be directly passed to the underlying model's ``__init__`` method (we assume all relevant updates to the configuration have already been done)\n                - If a configuration is not provided, ``kwargs`` will be first passed to the configuration class initialization function (:func:`~transformers.PretrainedConfig.from_pretrained`). Each key of ``kwargs`` that corresponds to a configuration attribute will be used to override said attribute with the supplied ``kwargs`` value. Remaining keys that do not correspond to any configuration attribute will be passed to the underlying model's ``__init__`` function.\n\n        Examples::\n\n            # For example purposes. Not runnable.\n            model = BertModel.from_pretrained('bert-base-uncased')    # Download model and configuration from S3 and cache.\n            model = BertModel.from_pretrained('./test/saved_model/')  # E.g. model was saved using `save_pretrained('./test/saved_model/')`\n            model = BertModel.from_pretrained('bert-base-uncased', output_attention=True)  # Update configuration during loading\n            assert model.config.output_attention == True\n            # Loading from a TF checkpoint file instead of a PyTorch model (slower)\n            config = BertConfig.from_json_file('./tf_model/my_tf_model_config.json')\n            model = BertModel.from_pretrained('./tf_model/my_tf_checkpoint.ckpt.index', from_tf=True, config=config)\n\n        \"\"\"", "\n", "config", "=", "kwargs", ".", "pop", "(", "\"config\"", ",", "None", ")", "\n", "state_dict", "=", "kwargs", ".", "pop", "(", "\"state_dict\"", ",", "None", ")", "\n", "cache_dir", "=", "kwargs", ".", "pop", "(", "\"cache_dir\"", ",", "None", ")", "\n", "from_tf", "=", "kwargs", ".", "pop", "(", "\"from_tf\"", ",", "False", ")", "\n", "force_download", "=", "kwargs", ".", "pop", "(", "\"force_download\"", ",", "False", ")", "\n", "resume_download", "=", "kwargs", ".", "pop", "(", "\"resume_download\"", ",", "False", ")", "\n", "proxies", "=", "kwargs", ".", "pop", "(", "\"proxies\"", ",", "None", ")", "\n", "output_loading_info", "=", "kwargs", ".", "pop", "(", "\"output_loading_info\"", ",", "False", ")", "\n", "local_files_only", "=", "kwargs", ".", "pop", "(", "\"local_files_only\"", ",", "False", ")", "\n", "\n", "# Load config if we don't provide a configuration", "\n", "if", "not", "isinstance", "(", "config", ",", "PretrainedConfig", ")", ":", "\n", "            ", "config_path", "=", "config", "if", "config", "is", "not", "None", "else", "pretrained_model_name_or_path", "\n", "config", ",", "model_kwargs", "=", "cls", ".", "config_class", ".", "from_pretrained", "(", "\n", "config_path", ",", "\n", "*", "model_args", ",", "\n", "cache_dir", "=", "cache_dir", ",", "\n", "return_unused_kwargs", "=", "True", ",", "\n", "force_download", "=", "force_download", ",", "\n", "resume_download", "=", "resume_download", ",", "\n", "proxies", "=", "proxies", ",", "\n", "local_files_only", "=", "local_files_only", ",", "\n", "**", "kwargs", ",", "\n", ")", "\n", "", "else", ":", "\n", "            ", "model_kwargs", "=", "kwargs", "\n", "\n", "# Load model", "\n", "", "if", "pretrained_model_name_or_path", "is", "not", "None", ":", "\n", "            ", "if", "pretrained_model_name_or_path", "in", "cls", ".", "pretrained_model_archive_map", ":", "\n", "                ", "archive_file", "=", "cls", ".", "pretrained_model_archive_map", "[", "pretrained_model_name_or_path", "]", "\n", "", "elif", "os", ".", "path", ".", "isdir", "(", "pretrained_model_name_or_path", ")", ":", "\n", "                ", "if", "from_tf", "and", "os", ".", "path", ".", "isfile", "(", "os", ".", "path", ".", "join", "(", "pretrained_model_name_or_path", ",", "TF_WEIGHTS_NAME", "+", "\".index\"", ")", ")", ":", "\n", "# Load from a TF 1.0 checkpoint", "\n", "                    ", "archive_file", "=", "os", ".", "path", ".", "join", "(", "pretrained_model_name_or_path", ",", "TF_WEIGHTS_NAME", "+", "\".index\"", ")", "\n", "", "elif", "from_tf", "and", "os", ".", "path", ".", "isfile", "(", "os", ".", "path", ".", "join", "(", "pretrained_model_name_or_path", ",", "TF2_WEIGHTS_NAME", ")", ")", ":", "\n", "# Load from a TF 2.0 checkpoint", "\n", "                    ", "archive_file", "=", "os", ".", "path", ".", "join", "(", "pretrained_model_name_or_path", ",", "TF2_WEIGHTS_NAME", ")", "\n", "", "elif", "os", ".", "path", ".", "isfile", "(", "os", ".", "path", ".", "join", "(", "pretrained_model_name_or_path", ",", "WEIGHTS_NAME", ")", ")", ":", "\n", "# Load from a PyTorch checkpoint", "\n", "                    ", "archive_file", "=", "os", ".", "path", ".", "join", "(", "pretrained_model_name_or_path", ",", "WEIGHTS_NAME", ")", "\n", "", "else", ":", "\n", "                    ", "raise", "EnvironmentError", "(", "\n", "\"Error no file named {} found in directory {} or `from_tf` set to False\"", ".", "format", "(", "\n", "[", "WEIGHTS_NAME", ",", "TF2_WEIGHTS_NAME", ",", "TF_WEIGHTS_NAME", "+", "\".index\"", "]", ",", "pretrained_model_name_or_path", "\n", ")", "\n", ")", "\n", "", "", "elif", "os", ".", "path", ".", "isfile", "(", "pretrained_model_name_or_path", ")", "or", "is_remote_url", "(", "pretrained_model_name_or_path", ")", ":", "\n", "                ", "archive_file", "=", "pretrained_model_name_or_path", "\n", "", "elif", "os", ".", "path", ".", "isfile", "(", "pretrained_model_name_or_path", "+", "\".index\"", ")", ":", "\n", "                ", "assert", "(", "\n", "from_tf", "\n", ")", ",", "\"We found a TensorFlow checkpoint at {}, please set from_tf to True to load from this checkpoint\"", ".", "format", "(", "\n", "pretrained_model_name_or_path", "+", "\".index\"", "\n", ")", "\n", "archive_file", "=", "pretrained_model_name_or_path", "+", "\".index\"", "\n", "", "else", ":", "\n", "                ", "archive_file", "=", "hf_bucket_url", "(", "\n", "pretrained_model_name_or_path", ",", "postfix", "=", "(", "TF2_WEIGHTS_NAME", "if", "from_tf", "else", "WEIGHTS_NAME", ")", "\n", ")", "\n", "\n", "# redirect to the cache, if necessary", "\n", "", "try", ":", "\n", "                ", "resolved_archive_file", "=", "cached_path", "(", "\n", "archive_file", ",", "\n", "cache_dir", "=", "cache_dir", ",", "\n", "force_download", "=", "force_download", ",", "\n", "proxies", "=", "proxies", ",", "\n", "resume_download", "=", "resume_download", ",", "\n", "local_files_only", "=", "local_files_only", ",", "\n", ")", "\n", "", "except", "EnvironmentError", ":", "\n", "                ", "if", "pretrained_model_name_or_path", "in", "cls", ".", "pretrained_model_archive_map", ":", "\n", "                    ", "msg", "=", "\"Couldn't reach server at '{}' to download pretrained weights.\"", ".", "format", "(", "archive_file", ")", "\n", "", "else", ":", "\n", "                    ", "msg", "=", "(", "\n", "\"Model name '{}' was not found in model name list ({}). \"", "\n", "\"We assumed '{}' was a path or url to model weight files named one of {} but \"", "\n", "\"couldn't find any such file at this path or url.\"", ".", "format", "(", "\n", "pretrained_model_name_or_path", ",", "\n", "\", \"", ".", "join", "(", "cls", ".", "pretrained_model_archive_map", ".", "keys", "(", ")", ")", ",", "\n", "archive_file", ",", "\n", "[", "WEIGHTS_NAME", ",", "TF2_WEIGHTS_NAME", ",", "TF_WEIGHTS_NAME", "]", ",", "\n", ")", "\n", ")", "\n", "", "raise", "EnvironmentError", "(", "msg", ")", "\n", "\n", "", "if", "resolved_archive_file", "==", "archive_file", ":", "\n", "                ", "logger", ".", "info", "(", "\"loading weights file {}\"", ".", "format", "(", "archive_file", ")", ")", "\n", "", "else", ":", "\n", "                ", "logger", ".", "info", "(", "\"loading weights file {} from cache at {}\"", ".", "format", "(", "archive_file", ",", "resolved_archive_file", ")", ")", "\n", "", "", "else", ":", "\n", "            ", "resolved_archive_file", "=", "None", "\n", "\n", "# Instantiate model.", "\n", "", "model", "=", "cls", "(", "config", ",", "*", "model_args", ",", "**", "model_kwargs", ")", "\n", "\n", "if", "state_dict", "is", "None", "and", "not", "from_tf", ":", "\n", "            ", "try", ":", "\n", "                ", "state_dict", "=", "torch", ".", "load", "(", "resolved_archive_file", ",", "map_location", "=", "\"cpu\"", ")", "\n", "", "except", "Exception", ":", "\n", "#print(resolved_archive_file)", "\n", "#resolved_archive_file='cache_dir/5b5b80054cd2c95a946a8e0ce0b93f56326dff9fbda6a6c3e02de3c91c918342.7131dcb754361639a7d5526985f880879c9bfd144b65a0bf50590bddb7de9059'", "\n", "#state_dict = torch.load(resolved_archive_file, map_location=\"cpu\")", "\n", "                ", "raise", "OSError", "(", "\n", "\"Unable to load weights from pytorch checkpoint file. \"", "\n", "\"If you tried to load a PyTorch model from a TF 2.0 checkpoint, please set from_tf=True. \"", "\n", ")", "\n", "\n", "", "", "missing_keys", "=", "[", "]", "\n", "unexpected_keys", "=", "[", "]", "\n", "error_msgs", "=", "[", "]", "\n", "\n", "if", "from_tf", ":", "\n", "            ", "if", "resolved_archive_file", ".", "endswith", "(", "\".index\"", ")", ":", "\n", "# Load from a TensorFlow 1.X checkpoint - provided by original authors", "\n", "                ", "model", "=", "cls", ".", "load_tf_weights", "(", "model", ",", "config", ",", "resolved_archive_file", "[", ":", "-", "6", "]", ")", "# Remove the '.index'", "\n", "", "else", ":", "\n", "# Load from our TensorFlow 2.0 checkpoints", "\n", "                ", "try", ":", "\n", "                    ", "from", "transformers", "import", "load_tf2_checkpoint_in_pytorch_model", "\n", "\n", "model", "=", "load_tf2_checkpoint_in_pytorch_model", "(", "model", ",", "resolved_archive_file", ",", "allow_missing_keys", "=", "True", ")", "\n", "", "except", "ImportError", ":", "\n", "                    ", "logger", ".", "error", "(", "\n", "\"Loading a TensorFlow model in PyTorch, requires both PyTorch and TensorFlow to be installed. Please see \"", "\n", "\"https://pytorch.org/ and https://www.tensorflow.org/install/ for installation instructions.\"", "\n", ")", "\n", "raise", "\n", "", "", "", "else", ":", "\n", "# Convert old format to new format if needed from a PyTorch state_dict", "\n", "            ", "old_keys", "=", "[", "]", "\n", "new_keys", "=", "[", "]", "\n", "for", "key", "in", "state_dict", ".", "keys", "(", ")", ":", "\n", "                ", "new_key", "=", "None", "\n", "if", "\"gamma\"", "in", "key", ":", "\n", "                    ", "new_key", "=", "key", ".", "replace", "(", "\"gamma\"", ",", "\"weight\"", ")", "\n", "", "if", "\"beta\"", "in", "key", ":", "\n", "                    ", "new_key", "=", "key", ".", "replace", "(", "\"beta\"", ",", "\"bias\"", ")", "\n", "", "if", "new_key", ":", "\n", "                    ", "old_keys", ".", "append", "(", "key", ")", "\n", "new_keys", ".", "append", "(", "new_key", ")", "\n", "", "", "for", "old_key", ",", "new_key", "in", "zip", "(", "old_keys", ",", "new_keys", ")", ":", "\n", "                ", "state_dict", "[", "new_key", "]", "=", "state_dict", ".", "pop", "(", "old_key", ")", "\n", "\n", "# copy state_dict so _load_from_state_dict can modify it", "\n", "", "metadata", "=", "getattr", "(", "state_dict", ",", "\"_metadata\"", ",", "None", ")", "\n", "state_dict", "=", "state_dict", ".", "copy", "(", ")", "\n", "if", "metadata", "is", "not", "None", ":", "\n", "                ", "state_dict", ".", "_metadata", "=", "metadata", "\n", "\n", "# PyTorch's `_load_from_state_dict` does not copy parameters in a module's descendants", "\n", "# so we need to apply the function recursively.", "\n", "", "def", "load", "(", "module", ":", "nn", ".", "Module", ",", "prefix", "=", "\"\"", ")", ":", "\n", "                ", "local_metadata", "=", "{", "}", "if", "metadata", "is", "None", "else", "metadata", ".", "get", "(", "prefix", "[", ":", "-", "1", "]", ",", "{", "}", ")", "\n", "module", ".", "_load_from_state_dict", "(", "\n", "state_dict", ",", "prefix", ",", "local_metadata", ",", "True", ",", "missing_keys", ",", "unexpected_keys", ",", "error_msgs", "\n", ")", "\n", "for", "name", ",", "child", "in", "module", ".", "_modules", ".", "items", "(", ")", ":", "\n", "                    ", "if", "child", "is", "not", "None", ":", "\n", "                        ", "load", "(", "child", ",", "prefix", "+", "name", "+", "\".\"", ")", "\n", "\n", "# Make sure we are able to load base models as well as derived models (with heads)", "\n", "", "", "", "start_prefix", "=", "\"\"", "\n", "model_to_load", "=", "model", "\n", "if", "not", "hasattr", "(", "model", ",", "cls", ".", "base_model_prefix", ")", "and", "any", "(", "\n", "s", ".", "startswith", "(", "cls", ".", "base_model_prefix", ")", "for", "s", "in", "state_dict", ".", "keys", "(", ")", "\n", ")", ":", "\n", "                ", "start_prefix", "=", "cls", ".", "base_model_prefix", "+", "\".\"", "\n", "", "if", "hasattr", "(", "model", ",", "cls", ".", "base_model_prefix", ")", "and", "not", "any", "(", "\n", "s", ".", "startswith", "(", "cls", ".", "base_model_prefix", ")", "for", "s", "in", "state_dict", ".", "keys", "(", ")", "\n", ")", ":", "\n", "                ", "model_to_load", "=", "getattr", "(", "model", ",", "cls", ".", "base_model_prefix", ")", "\n", "\n", "", "load", "(", "model_to_load", ",", "prefix", "=", "start_prefix", ")", "\n", "if", "len", "(", "missing_keys", ")", ">", "0", ":", "\n", "                ", "logger", ".", "info", "(", "\n", "\"Weights of {} not initialized from pretrained model: {}\"", ".", "format", "(", "\n", "model", ".", "__class__", ".", "__name__", ",", "missing_keys", "\n", ")", "\n", ")", "\n", "", "if", "len", "(", "unexpected_keys", ")", ">", "0", ":", "\n", "                ", "logger", ".", "info", "(", "\n", "\"Weights from pretrained model not used in {}: {}\"", ".", "format", "(", "\n", "model", ".", "__class__", ".", "__name__", ",", "unexpected_keys", "\n", ")", "\n", ")", "\n", "", "if", "len", "(", "error_msgs", ")", ">", "0", ":", "\n", "                ", "raise", "RuntimeError", "(", "\n", "\"Error(s) in loading state_dict for {}:\\n\\t{}\"", ".", "format", "(", "\n", "model", ".", "__class__", ".", "__name__", ",", "\"\\n\\t\"", ".", "join", "(", "error_msgs", ")", "\n", ")", "\n", ")", "\n", "", "", "model", ".", "tie_weights", "(", ")", "# make sure word embedding weights are still tied if needed", "\n", "\n", "# Set model in evaluation mode to desactivate DropOut modules by default", "\n", "model", ".", "eval", "(", ")", "\n", "\n", "if", "output_loading_info", ":", "\n", "            ", "loading_info", "=", "{", "\"missing_keys\"", ":", "missing_keys", ",", "\"unexpected_keys\"", ":", "unexpected_keys", ",", "\"error_msgs\"", ":", "error_msgs", "}", "\n", "return", "model", ",", "loading_info", "\n", "\n", "", "return", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.neulab_awesome-align.awesome_align.modeling_utils.PreTrainedModel.prepare_inputs_for_generation": [[572, 574], ["None"], "methods", ["None"], ["", "def", "prepare_inputs_for_generation", "(", "self", ",", "input_ids", ",", "**", "kwargs", ")", ":", "\n", "        ", "return", "{", "\"input_ids\"", ":", "input_ids", "}", "\n", "\n"]], "home.repos.pwc.inspect_result.neulab_awesome-align.awesome_align.modeling_utils.PreTrainedModel._do_output_past": [[575, 584], ["getattr", "getattr", "len"], "methods", ["None"], ["", "def", "_do_output_past", "(", "self", ",", "outputs", ")", ":", "\n", "        ", "\"\"\"During generation, decide whether to pass the `past` variable to the next forward pass.\"\"\"", "\n", "has_output_past", "=", "getattr", "(", "self", ".", "config", ",", "\"output_past\"", ",", "False", ")", "\n", "mem_len", "=", "getattr", "(", "self", ".", "config", ",", "\"mem_len\"", ",", "0", ")", "\n", "if", "len", "(", "outputs", ")", "<=", "1", ":", "\n", "            ", "return", "False", "\n", "", "if", "mem_len", ">", "0", "or", "has_output_past", ":", "\n", "            ", "return", "True", "\n", "", "return", "False", "\n", "\n"]], "home.repos.pwc.inspect_result.neulab_awesome-align.awesome_align.modeling_utils.PreTrainedModel.enforce_repetition_penalty_": [[585, 594], ["range", "set", "prev_output_tokens[].tolist"], "methods", ["None"], ["", "def", "enforce_repetition_penalty_", "(", "self", ",", "lprobs", ",", "batch_size", ",", "num_beams", ",", "prev_output_tokens", ",", "repetition_penalty", ")", ":", "\n", "        ", "\"\"\"repetition penalty (from CTRL paper https://arxiv.org/abs/1909.05858). \"\"\"", "\n", "for", "i", "in", "range", "(", "batch_size", "*", "num_beams", ")", ":", "\n", "            ", "for", "previous_token", "in", "set", "(", "prev_output_tokens", "[", "i", "]", ".", "tolist", "(", ")", ")", ":", "\n", "# if score < 0 then repetition penalty has to multiplied to reduce the previous token probability", "\n", "                ", "if", "lprobs", "[", "i", ",", "previous_token", "]", "<", "0", ":", "\n", "                    ", "lprobs", "[", "i", ",", "previous_token", "]", "*=", "repetition_penalty", "\n", "", "else", ":", "\n", "                    ", "lprobs", "[", "i", ",", "previous_token", "]", "/=", "repetition_penalty", "\n", "\n"]], "home.repos.pwc.inspect_result.neulab_awesome-align.awesome_align.modeling_utils.PreTrainedModel.generate": [[595, 824], ["torch.no_grad", "isinstance", "isinstance", "modeling_utils.PreTrainedModel.get_output_embeddings", "AttributeError", "isinstance", "isinstance", "isinstance", "isinstance", "torch.full", "logger.warning", "input_ids.contiguous().view.contiguous().view.unsqueeze().expand", "input_ids.contiguous().view.contiguous().view.contiguous().view", "modeling_utils.PreTrainedModel._generate_beam_search", "modeling_utils.PreTrainedModel._generate_no_beam_search", "isinstance", "isinstance", "isinstance", "isinstance", "input_ids.contiguous().view.contiguous().view.dim", "input_ids.contiguous().view.contiguous().view.unsqueeze", "input_ids.contiguous().view.contiguous().view.contiguous", "isinstance", "next", "modeling_utils.PreTrainedModel.parameters"], "methods", ["home.repos.pwc.inspect_result.neulab_awesome-align.awesome_align.modeling_utils.PreTrainedModel.get_output_embeddings", "home.repos.pwc.inspect_result.neulab_awesome-align.awesome_align.modeling_utils.PreTrainedModel._generate_beam_search", "home.repos.pwc.inspect_result.neulab_awesome-align.awesome_align.modeling_utils.PreTrainedModel._generate_no_beam_search"], ["", "", "", "", "@", "torch", ".", "no_grad", "(", ")", "\n", "def", "generate", "(", "\n", "self", ",", "\n", "input_ids", "=", "None", ",", "\n", "max_length", "=", "None", ",", "\n", "do_sample", "=", "True", ",", "\n", "num_beams", "=", "None", ",", "\n", "temperature", "=", "None", ",", "\n", "top_k", "=", "None", ",", "\n", "top_p", "=", "None", ",", "\n", "repetition_penalty", "=", "None", ",", "\n", "bos_token_id", "=", "None", ",", "\n", "pad_token_id", "=", "None", ",", "\n", "eos_token_ids", "=", "None", ",", "\n", "length_penalty", "=", "None", ",", "\n", "num_return_sequences", "=", "None", ",", "\n", ")", ":", "\n", "        ", "r\"\"\" Generates sequences for models with a LM head. The method currently supports greedy or penalized greedy decoding, sampling with top-k or nucleus sampling\n        and beam-search.\n\n        Adapted in part from `Facebook's XLM beam search code`_.\n\n        .. _`Facebook's XLM beam search code`:\n           https://github.com/facebookresearch/XLM/blob/9e6f6814d17be4fe5b15f2e6c43eb2b2d76daeb4/src/model/transformer.py#L529\n\n\n        Parameters:\n\n            input_ids: (`optional`) `torch.LongTensor` of shape `(batch_size, sequence_length)`\n                The sequence used as a prompt for the generation. If `None` the method initializes\n                it as an empty `torch.LongTensor` of shape `(1,)`.\n\n            max_length: (`optional`) int\n                The max length of the sequence to be generated.  Between 1 and infinity. Default to 20.\n\n            do_sample: (`optional`) bool\n                If set to `False` greedy decoding is used. Otherwise sampling is used. Defaults to `True`.\n\n            num_beams: (`optional`) int\n                Number of beams for beam search. Must be between 1 and infinity. 1 means no beam search. Default to 1.\n\n            temperature: (`optional`) float\n                The value used to module the next token probabilities. Must be strictly positive. Default to 1.0.\n\n            top_k: (`optional`) int\n                The number of highest probability vocabulary tokens to keep for top-k-filtering. Between 1 and infinity. Default to 50.\n\n            top_p: (`optional`) float\n                The cumulative probability of parameter highest probability vocabulary tokens to keep for nucleus sampling. Must be between 0 and 1. Default to 1.\n\n            repetition_penalty: (`optional`) float\n                The parameter for repetition penalty. Between 1.0 and infinity. 1.0 means no penalty. Default to 1.0.\n\n            bos_token_id: (`optional`) int\n                Beginning of sentence token if no prompt is provided. Default to 0.\n\n            eos_token_ids: (`optional`) int or list of int\n                End of sequence token or list of tokens to stop the generation. Default to 0.\n            length_penalty: (`optional`) float\n                Exponential penalty to the length. Default to 1.\n\n            num_return_sequences: (`optional`) int\n                The number of independently computed returned sequences for each element in the batch. Default to 1.\n\n        Return:\n\n            output: `torch.LongTensor` of shape `(batch_size * num_return_sequences, sequence_length)`\n                sequence_length is either equal to max_length or shorter if all batches finished early due to the `eos_token_id`\n\n        Examples::\n\n            tokenizer = AutoTokenizer.from_pretrained('distilgpt2')   # Initialize tokenizer\n            model = AutoModelWithLMHead.from_pretrained('distilgpt2')    # Download model and configuration from S3 and cache.\n            outputs = model.generate(max_length=40, bos_token_id=tokenizer.bos_token_id, eos_token_ids=tokenizer.eos_token_id, do_sample=False)  # do greedy decoding\n            print('Generated: {}'.format(tokenizer.decode(outputs[0], skip_special_tokens=True)))\n\n            tokenizer = AutoTokenizer.from_pretrained('openai-gpt')   # Initialize tokenizer\n            model = AutoModelWithLMHead.from_pretrained('openai-gpt')    # Download model and configuration from S3 and cache.\n            input_context = 'The dog'\n            input_ids = torch.tensor(tokenizer.encode(input_context)).unsqueeze(0)  # encode input context\n            outputs = model.generate(input_ids=input_ids, num_beams=5, num_return_sequences=3, temperature=1.5)  # generate 3 independent sequences using beam search decoding (5 beams) with sampling from initial context 'The dog'\n            for i in range(3): #  3 output sequences were generated\n                print('Generated {}: {}'.format(i, tokenizer.decode(outputs[i], skip_special_tokens=True)))\n\n            tokenizer = AutoTokenizer.from_pretrained('distilgpt2')   # Initialize tokenizer\n            model = AutoModelWithLMHead.from_pretrained('distilgpt2')    # Download model and configuration from S3 and cache.\n            input_context = 'The dog'\n            input_ids = torch.tensor(tokenizer.encode(input_context)).unsqueeze(0)  # encode input context\n            outputs = model.generate(input_ids=input_ids, max_length=40, temperature=0.7, bos_token_id=tokenizer.bos_token_id, pad_token_id=tokenizer.pad_token_id, eos_token_ids=tokenizer.eos_token_id, num_return_sequences=3)  # 3 generate sequences using by sampling\n            for i in range(3): #  3 output sequences were generated\n                print('Generated {}: {}'.format(i, tokenizer.decode(outputs[i], skip_special_tokens=True)))\n\n            tokenizer = AutoTokenizer.from_pretrained('ctrl')   # Initialize tokenizer\n            model = AutoModelWithLMHead.from_pretrained('ctrl')    # Download model and configuration from S3 and cache.\n            input_context = 'Legal My neighbor is'  # \"Legal\" is one of the control codes for ctrl\n            input_ids = torch.tensor(tokenizer.encode(input_context)).unsqueeze(0)  # encode input context\n            outputs = model.generate(input_ids=input_ids, max_length=50, temperature=0.7, repetition_penalty=1.2)  # generate sequences\n            print('Generated: {}'.format(tokenizer.decode(outputs[0], skip_special_tokens=True)))\n\n        \"\"\"", "\n", "\n", "# We cannot generate if the model does not have a LM head", "\n", "if", "self", ".", "get_output_embeddings", "(", ")", "is", "None", ":", "\n", "            ", "raise", "AttributeError", "(", "\n", "\"You tried to generate sequences with a model that does not have a LM Head.\"", "\n", "\"Please use another model class (e.g. `OpenAIGPTLMHeadModel`, `XLNetLMHeadModel`, `GPT2LMHeadModel`, `CTRLLMHeadModel`, `T5WithLMHeadModel`, `TransfoXLLMHeadModel`)\"", "\n", ")", "\n", "\n", "", "max_length", "=", "max_length", "if", "max_length", "is", "not", "None", "else", "self", ".", "config", ".", "max_length", "\n", "do_sample", "=", "do_sample", "if", "do_sample", "is", "not", "None", "else", "self", ".", "config", ".", "do_sample", "\n", "num_beams", "=", "num_beams", "if", "num_beams", "is", "not", "None", "else", "self", ".", "config", ".", "num_beams", "\n", "temperature", "=", "temperature", "if", "temperature", "is", "not", "None", "else", "self", ".", "config", ".", "temperature", "\n", "top_k", "=", "top_k", "if", "top_k", "is", "not", "None", "else", "self", ".", "config", ".", "top_k", "\n", "top_p", "=", "top_p", "if", "top_p", "is", "not", "None", "else", "self", ".", "config", ".", "top_p", "\n", "repetition_penalty", "=", "repetition_penalty", "if", "repetition_penalty", "is", "not", "None", "else", "self", ".", "config", ".", "repetition_penalty", "\n", "bos_token_id", "=", "bos_token_id", "if", "bos_token_id", "is", "not", "None", "else", "self", ".", "config", ".", "bos_token_id", "\n", "pad_token_id", "=", "pad_token_id", "if", "pad_token_id", "is", "not", "None", "else", "self", ".", "config", ".", "pad_token_id", "\n", "eos_token_ids", "=", "eos_token_ids", "if", "eos_token_ids", "is", "not", "None", "else", "self", ".", "config", ".", "eos_token_ids", "\n", "length_penalty", "=", "length_penalty", "if", "length_penalty", "is", "not", "None", "else", "self", ".", "config", ".", "length_penalty", "\n", "num_return_sequences", "=", "(", "\n", "num_return_sequences", "if", "num_return_sequences", "is", "not", "None", "else", "self", ".", "config", ".", "num_return_sequences", "\n", ")", "\n", "\n", "if", "input_ids", "is", "not", "None", ":", "\n", "            ", "batch_size", "=", "input_ids", ".", "shape", "[", "0", "]", "# overriden by the input batch_size", "\n", "", "else", ":", "\n", "            ", "batch_size", "=", "1", "\n", "", "if", "isinstance", "(", "eos_token_ids", ",", "int", ")", ":", "\n", "            ", "eos_token_ids", "=", "[", "eos_token_ids", "]", "\n", "\n", "", "assert", "isinstance", "(", "max_length", ",", "int", ")", "and", "max_length", ">", "0", ",", "\"`max_length` should be a strictly positive integer.\"", "\n", "assert", "isinstance", "(", "do_sample", ",", "bool", ")", ",", "\"`do_sample` should be a boolean.\"", "\n", "assert", "isinstance", "(", "num_beams", ",", "int", ")", "and", "num_beams", ">", "0", ",", "\"`num_beams` should be a strictly positive integer.\"", "\n", "assert", "temperature", ">", "0", ",", "\"`temperature` should be strictly positive.\"", "\n", "assert", "isinstance", "(", "top_k", ",", "int", ")", "and", "top_k", ">=", "0", ",", "\"`top_k` should be a positive integer.\"", "\n", "assert", "0", "<=", "top_p", "<=", "1", ",", "\"`top_p` should be between 0 and 1.\"", "\n", "assert", "repetition_penalty", ">=", "1.0", ",", "\"`repetition_penalty` should be >= 1.\"", "\n", "assert", "input_ids", "is", "not", "None", "or", "(", "\n", "isinstance", "(", "bos_token_id", ",", "int", ")", "and", "bos_token_id", ">=", "0", "\n", ")", ",", "\"If input_ids is not defined, `bos_token_id` should be a positive integer.\"", "\n", "assert", "pad_token_id", "is", "None", "or", "(", "\n", "isinstance", "(", "pad_token_id", ",", "int", ")", "and", "(", "pad_token_id", ">=", "0", ")", "\n", ")", ",", "\"`pad_token_id` should be a positive integer.\"", "\n", "assert", "(", "eos_token_ids", "is", "None", ")", "or", "(", "\n", "isinstance", "(", "eos_token_ids", ",", "(", "list", ",", "tuple", ")", ")", "and", "(", "(", "isinstance", "(", "e", ",", "int", ")", "and", "e", ">=", "0", ")", "for", "e", "in", "eos_token_ids", ")", "\n", ")", ",", "\"`eos_token_ids` should be a positive integer or a list/tuple of positive integers.\"", "\n", "assert", "length_penalty", ">", "0", ",", "\"`length_penalty` should be strictly positive.\"", "\n", "assert", "(", "\n", "isinstance", "(", "num_return_sequences", ",", "int", ")", "and", "num_return_sequences", ">", "0", "\n", ")", ",", "\"`num_return_sequences` should be a strictly positive integer.\"", "\n", "\n", "if", "input_ids", "is", "None", ":", "\n", "            ", "assert", "isinstance", "(", "bos_token_id", ",", "int", ")", "and", "bos_token_id", ">=", "0", ",", "(", "\n", "\"you should either supply a context to complete as `input_ids` input \"", "\n", "\"or a `bos_token_id` (integer >= 0) as a first token to start the generation.\"", "\n", ")", "\n", "input_ids", "=", "torch", ".", "full", "(", "\n", "(", "batch_size", ",", "1", ")", ",", "bos_token_id", ",", "dtype", "=", "torch", ".", "long", ",", "device", "=", "next", "(", "self", ".", "parameters", "(", ")", ")", ".", "device", "\n", ")", "\n", "", "else", ":", "\n", "            ", "assert", "input_ids", ".", "dim", "(", ")", "==", "2", ",", "\"Input prompt should be of shape (batch_size, sequence length).\"", "\n", "\n", "", "if", "do_sample", "is", "False", ":", "\n", "            ", "if", "num_beams", "==", "1", ":", "\n", "# no_beam_search greedy generation conditions", "\n", "                ", "assert", "(", "\n", "num_return_sequences", "==", "1", "\n", ")", ",", "\"Greedy decoding will always produce the same output for num_beams == 1 and num_return_sequences > 1. Please set num_return_sequences = 1\"", "\n", "\n", "", "else", ":", "\n", "# beam_search greedy generation conditions", "\n", "                ", "assert", "(", "\n", "num_beams", ">=", "num_return_sequences", "\n", ")", ",", "\"Greedy beam search decoding cannot return more sequences than it has beams. Please set num_beams >= num_return_sequences\"", "\n", "\n", "", "", "if", "pad_token_id", "is", "None", "and", "eos_token_ids", "is", "not", "None", ":", "\n", "            ", "logger", ".", "warning", "(", "\n", "\"Setting `pad_token_id` to {} (first `eos_token_id`) to generate sequence\"", ".", "format", "(", "eos_token_ids", "[", "0", "]", ")", "\n", ")", "\n", "pad_token_id", "=", "eos_token_ids", "[", "0", "]", "\n", "\n", "# current position and vocab size", "\n", "", "cur_len", "=", "input_ids", ".", "shape", "[", "1", "]", "\n", "vocab_size", "=", "self", ".", "config", ".", "vocab_size", "\n", "\n", "if", "num_return_sequences", "!=", "1", "and", "do_sample", ":", "\n", "# Expand input to num return sequences", "\n", "            ", "input_ids", "=", "input_ids", ".", "unsqueeze", "(", "1", ")", ".", "expand", "(", "batch_size", ",", "num_return_sequences", ",", "cur_len", ")", "\n", "input_ids", "=", "input_ids", ".", "contiguous", "(", ")", ".", "view", "(", "\n", "batch_size", "*", "num_return_sequences", ",", "cur_len", "\n", ")", "# shape: (batch_size * num_return_sequences, cur_len)", "\n", "effective_batch_size", "=", "batch_size", "*", "num_return_sequences", "\n", "", "else", ":", "\n", "            ", "effective_batch_size", "=", "batch_size", "\n", "\n", "", "if", "num_beams", ">", "1", ":", "\n", "            ", "output", "=", "self", ".", "_generate_beam_search", "(", "\n", "input_ids", ",", "\n", "cur_len", ",", "\n", "max_length", ",", "\n", "do_sample", ",", "\n", "temperature", ",", "\n", "top_k", ",", "\n", "top_p", ",", "\n", "repetition_penalty", ",", "\n", "pad_token_id", ",", "\n", "eos_token_ids", ",", "\n", "effective_batch_size", ",", "\n", "num_return_sequences", ",", "\n", "length_penalty", ",", "\n", "num_beams", ",", "\n", "vocab_size", ",", "\n", ")", "\n", "", "else", ":", "\n", "            ", "output", "=", "self", ".", "_generate_no_beam_search", "(", "\n", "input_ids", ",", "\n", "cur_len", ",", "\n", "max_length", ",", "\n", "do_sample", ",", "\n", "temperature", ",", "\n", "top_k", ",", "\n", "top_p", ",", "\n", "repetition_penalty", ",", "\n", "pad_token_id", ",", "\n", "eos_token_ids", ",", "\n", "effective_batch_size", ",", "\n", ")", "\n", "\n", "", "return", "output", "\n", "\n"]], "home.repos.pwc.inspect_result.neulab_awesome-align.awesome_align.modeling_utils.PreTrainedModel._generate_no_beam_search": [[825, 910], ["torch.cat.new().fill_", "torch.cat.new().fill_", "enumerate", "modeling_utils.PreTrainedModel.prepare_inputs_for_generation", "modeling_utils.PreTrainedModel.", "modeling_utils.PreTrainedModel._do_output_past", "torch.cat", "torch.cat.new().fill_.min().item", "torch.cat.new().fill_.max().item", "torch.cat.new().fill_", "torch.cat.new", "torch.cat.new", "modeling_utils.PreTrainedModel.enforce_repetition_penalty_", "modeling_utils.top_k_top_p_filtering", "torch.multinomial().squeeze", "torch.argmax", "torch.cat.new().fill_.max", "tokens_to_add.unsqueeze", "torch.cat.new().fill_.mul().bool", "torch.cat.new().fill_.masked_fill_", "torch.cat.new().fill_.mul_", "torch.cat.new().fill_.min", "torch.cat.new().fill_.max", "torch.cat.new", "torch.multinomial", "torch.cat.new().fill_.max().item", "torch.nn.functional.softmax", "torch.cat.new().fill_.mul", "eos_in_sents.long", "torch.cat.new().fill_.max"], "methods", ["home.repos.pwc.inspect_result.neulab_awesome-align.awesome_align.modeling_utils.PreTrainedModel.prepare_inputs_for_generation", "home.repos.pwc.inspect_result.neulab_awesome-align.awesome_align.modeling_utils.PreTrainedModel._do_output_past", "home.repos.pwc.inspect_result.neulab_awesome-align.awesome_align.modeling_utils.PreTrainedModel.enforce_repetition_penalty_", "home.repos.pwc.inspect_result.neulab_awesome-align.awesome_align.modeling_utils.top_k_top_p_filtering"], ["", "def", "_generate_no_beam_search", "(", "\n", "self", ",", "\n", "input_ids", ",", "\n", "cur_len", ",", "\n", "max_length", ",", "\n", "do_sample", ",", "\n", "temperature", ",", "\n", "top_k", ",", "\n", "top_p", ",", "\n", "repetition_penalty", ",", "\n", "pad_token_id", ",", "\n", "eos_token_ids", ",", "\n", "batch_size", ",", "\n", ")", ":", "\n", "        ", "\"\"\" Generate sequences for each example without beam search (num_beams == 1).\n            All returned sequence are generated independantly.\n        \"\"\"", "\n", "# current position / max lengths / length of generated sentences / unfinished sentences", "\n", "\n", "unfinished_sents", "=", "input_ids", ".", "new", "(", "batch_size", ")", ".", "fill_", "(", "1", ")", "\n", "sent_lengths", "=", "input_ids", ".", "new", "(", "batch_size", ")", ".", "fill_", "(", "max_length", ")", "\n", "\n", "past", "=", "None", "\n", "while", "cur_len", "<", "max_length", ":", "\n", "            ", "model_inputs", "=", "self", ".", "prepare_inputs_for_generation", "(", "input_ids", ",", "past", "=", "past", ")", "\n", "\n", "outputs", "=", "self", "(", "**", "model_inputs", ")", "\n", "next_token_logits", "=", "outputs", "[", "0", "]", "[", ":", ",", "-", "1", ",", ":", "]", "\n", "\n", "# if model has past, then set the past variable to speed up decoding", "\n", "if", "self", ".", "_do_output_past", "(", "outputs", ")", ":", "\n", "                ", "past", "=", "outputs", "[", "1", "]", "\n", "\n", "# repetition penalty from CTRL paper (https://arxiv.org/abs/1909.05858)", "\n", "", "if", "repetition_penalty", "!=", "1.0", ":", "\n", "                ", "self", ".", "enforce_repetition_penalty_", "(", "next_token_logits", ",", "batch_size", ",", "1", ",", "input_ids", ",", "repetition_penalty", ")", "\n", "\n", "", "if", "do_sample", ":", "\n", "# Temperature (higher temperature => more likely to sample low probability tokens)", "\n", "                ", "if", "temperature", "!=", "1.0", ":", "\n", "                    ", "next_token_logits", "=", "next_token_logits", "/", "temperature", "\n", "# Top-p/top-k filtering", "\n", "", "next_token_logits", "=", "top_k_top_p_filtering", "(", "next_token_logits", ",", "top_k", "=", "top_k", ",", "top_p", "=", "top_p", ")", "\n", "# Sample", "\n", "next_token", "=", "torch", ".", "multinomial", "(", "F", ".", "softmax", "(", "next_token_logits", ",", "dim", "=", "-", "1", ")", ",", "num_samples", "=", "1", ")", ".", "squeeze", "(", "1", ")", "\n", "", "else", ":", "\n", "# Greedy decoding", "\n", "                ", "next_token", "=", "torch", ".", "argmax", "(", "next_token_logits", ",", "dim", "=", "-", "1", ")", "\n", "\n", "# update generations and finished sentences", "\n", "", "if", "eos_token_ids", "is", "not", "None", ":", "\n", "# pad finished sentences if eos_token_ids exist", "\n", "                ", "tokens_to_add", "=", "next_token", "*", "unfinished_sents", "+", "(", "pad_token_id", ")", "*", "(", "1", "-", "unfinished_sents", ")", "\n", "", "else", ":", "\n", "                ", "tokens_to_add", "=", "next_token", "\n", "\n", "", "input_ids", "=", "torch", ".", "cat", "(", "[", "input_ids", ",", "tokens_to_add", ".", "unsqueeze", "(", "-", "1", ")", "]", ",", "dim", "=", "-", "1", ")", "\n", "\n", "if", "eos_token_ids", "is", "not", "None", ":", "\n", "                ", "for", "eos_token_id", "in", "eos_token_ids", ":", "\n", "                    ", "eos_in_sents", "=", "tokens_to_add", "==", "eos_token_id", "\n", "# if sentence is unfinished and the token to add is eos, sent_lengths is filled with current length", "\n", "is_sents_unfinished_and_token_to_add_is_eos", "=", "unfinished_sents", ".", "mul", "(", "eos_in_sents", ".", "long", "(", ")", ")", ".", "bool", "(", ")", "\n", "sent_lengths", ".", "masked_fill_", "(", "is_sents_unfinished_and_token_to_add_is_eos", ",", "cur_len", "+", "1", ")", "\n", "# unfinished_sents is set to zero if eos in sentence", "\n", "unfinished_sents", ".", "mul_", "(", "(", "~", "eos_in_sents", ")", ".", "long", "(", ")", ")", "\n", "\n", "", "", "cur_len", "=", "cur_len", "+", "1", "\n", "\n", "# stop when there is a </s> in each sentence, or if we exceed the maximul length", "\n", "if", "unfinished_sents", ".", "max", "(", ")", "==", "0", ":", "\n", "                ", "break", "\n", "\n", "# if there are different sentences lengths in the batch, some batches have to be padded", "\n", "", "", "if", "sent_lengths", ".", "min", "(", ")", ".", "item", "(", ")", "!=", "sent_lengths", ".", "max", "(", ")", ".", "item", "(", ")", ":", "\n", "            ", "assert", "pad_token_id", "is", "not", "None", ",", "\"`Pad_token_id` has to be defined if batches have different lengths\"", "\n", "# finished sents are filled with pad_token", "\n", "decoded", "=", "input_ids", ".", "new", "(", "batch_size", ",", "sent_lengths", ".", "max", "(", ")", ".", "item", "(", ")", ")", ".", "fill_", "(", "pad_token_id", ")", "\n", "", "else", ":", "\n", "            ", "decoded", "=", "input_ids", "\n", "\n", "", "for", "hypo_idx", ",", "hypo", "in", "enumerate", "(", "input_ids", ")", ":", "\n", "            ", "decoded", "[", "hypo_idx", ",", ":", "sent_lengths", "[", "hypo_idx", "]", "]", "=", "hypo", "[", ":", "sent_lengths", "[", "hypo_idx", "]", "]", "\n", "\n", "", "return", "decoded", "\n", "\n"]], "home.repos.pwc.inspect_result.neulab_awesome-align.awesome_align.modeling_utils.PreTrainedModel._generate_beam_search": [[911, 1111], ["torch.cat.unsqueeze().expand", "torch.cat.contiguous().view", "torch.zeros", "beam_scores.new.new.view", "range", "torch.cat.new", "enumerate", "modeling_utils.BeamHypotheses", "modeling_utils.PreTrainedModel.prepare_inputs_for_generation", "modeling_utils.PreTrainedModel.", "modeling_utils.PreTrainedModel._do_output_past", "range", "beam_scores.new.new.new", "torch.cat.new", "torch.cat.new", "torch.cat", "all", "sorted", "range", "torch.cat.new.min().item", "torch.cat.new.max().item", "min", "torch.cat.new().fill_", "enumerate", "torch.stack().type().to", "torch.cat.unsqueeze", "torch.cat.contiguous", "range", "range", "modeling_utils.PreTrainedModel.enforce_repetition_penalty_", "modeling_utils.top_k_top_p_filtering", "torch.multinomial", "torch.nn.functional.log_softmax", "torch.gather", "next_words.view.view.view", "next_scores.view.view.view", "torch.nn.functional.log_softmax", "_scores.view.view.view", "torch.topk", "next_scores.view.view.size", "next_words.view.view.size", "zip", "next_batch_beam.extend", "len", "modeling_utils.PreTrainedModel._reorder_cache", "zip", "len", "best.append", "torch.nn.functional.softmax", "beam_scores[].expand_as", "torch.nn.functional.log_softmax.size", "beam_scores[].expand_as", "generated_hyps[].is_done", "next_batch_beam.extend", "len", "len", "torch.cat.new.unsqueeze", "generated_hyps[].add", "sorted.pop", "torch.cat.new.min", "torch.cat.new.max", "torch.cat.new.max().item", "torch.cat.new", "len", "torch.stack().type", "next", "next_scores[].max().item", "len", "generated_hyps[].add", "next_sent_beam.append", "len", "input_ids[].clone", "score.item", "modeling_utils.PreTrainedModel.parameters", "word_id.item", "input_ids[].clone", "score.item", "torch.cat.new.max", "torch.stack", "next_scores[].max"], "methods", ["home.repos.pwc.inspect_result.neulab_awesome-align.awesome_align.modeling_utils.PreTrainedModel.prepare_inputs_for_generation", "home.repos.pwc.inspect_result.neulab_awesome-align.awesome_align.modeling_utils.PreTrainedModel._do_output_past", "home.repos.pwc.inspect_result.neulab_awesome-align.awesome_align.modeling_utils.PreTrainedModel.enforce_repetition_penalty_", "home.repos.pwc.inspect_result.neulab_awesome-align.awesome_align.modeling_utils.top_k_top_p_filtering", "home.repos.pwc.inspect_result.neulab_awesome-align.awesome_align.modeling_utils.PreTrainedModel._reorder_cache", "home.repos.pwc.inspect_result.neulab_awesome-align.awesome_align.modeling_utils.BeamHypotheses.is_done", "home.repos.pwc.inspect_result.neulab_awesome-align.awesome_align.modeling_utils.BeamHypotheses.add", "home.repos.pwc.inspect_result.neulab_awesome-align.awesome_align.modeling_utils.BeamHypotheses.add"], ["", "def", "_generate_beam_search", "(", "\n", "self", ",", "\n", "input_ids", ",", "\n", "cur_len", ",", "\n", "max_length", ",", "\n", "do_sample", ",", "\n", "temperature", ",", "\n", "top_k", ",", "\n", "top_p", ",", "\n", "repetition_penalty", ",", "\n", "pad_token_id", ",", "\n", "eos_token_ids", ",", "\n", "batch_size", ",", "\n", "num_return_sequences", ",", "\n", "length_penalty", ",", "\n", "num_beams", ",", "\n", "vocab_size", ",", "\n", ")", ":", "\n", "        ", "\"\"\" Generate sequences for each example with beam search.\n        \"\"\"", "\n", "\n", "# Expand input to num beams", "\n", "# assert input_ids.shape == (batch_size * num_beams, cur_len)", "\n", "input_ids", "=", "input_ids", ".", "unsqueeze", "(", "1", ")", ".", "expand", "(", "batch_size", ",", "num_beams", ",", "cur_len", ")", "\n", "input_ids", "=", "input_ids", ".", "contiguous", "(", ")", ".", "view", "(", "batch_size", "*", "num_beams", ",", "cur_len", ")", "# (batch_size * num_beams, cur_len)", "\n", "\n", "# generated hypotheses", "\n", "generated_hyps", "=", "[", "\n", "BeamHypotheses", "(", "num_beams", ",", "max_length", ",", "length_penalty", ",", "early_stopping", "=", "False", ")", "for", "_", "in", "range", "(", "batch_size", ")", "\n", "]", "\n", "\n", "# scores for each sentence in the beam", "\n", "beam_scores", "=", "torch", ".", "zeros", "(", "(", "batch_size", ",", "num_beams", ")", ",", "dtype", "=", "torch", ".", "float", ",", "device", "=", "input_ids", ".", "device", ")", "\n", "beam_scores", "[", ":", ",", "1", ":", "]", "=", "-", "1e9", "\n", "beam_scores", "=", "beam_scores", ".", "view", "(", "-", "1", ")", "# shape (batch_size * num_beams,)", "\n", "\n", "# cache compute states", "\n", "past", "=", "None", "\n", "\n", "# done sentences", "\n", "done", "=", "[", "False", "for", "_", "in", "range", "(", "batch_size", ")", "]", "\n", "\n", "while", "cur_len", "<", "max_length", ":", "\n", "            ", "model_inputs", "=", "self", ".", "prepare_inputs_for_generation", "(", "input_ids", ",", "past", "=", "past", ")", "\n", "outputs", "=", "self", "(", "**", "model_inputs", ")", "# (batch_size * num_beams, cur_len, vocab_size)", "\n", "scores", "=", "outputs", "[", "0", "]", "[", ":", ",", "-", "1", ",", ":", "]", "# (batch_size * num_beams, vocab_size)", "\n", "\n", "# if model has past, then set the past variable to speed up decoding", "\n", "if", "self", ".", "_do_output_past", "(", "outputs", ")", ":", "\n", "                ", "past", "=", "outputs", "[", "1", "]", "\n", "\n", "# repetition penalty (from CTRL paper https://arxiv.org/abs/1909.05858)", "\n", "", "if", "repetition_penalty", "!=", "1.0", ":", "\n", "                ", "self", ".", "enforce_repetition_penalty_", "(", "scores", ",", "batch_size", ",", "num_beams", ",", "input_ids", ",", "repetition_penalty", ")", "\n", "\n", "", "if", "do_sample", ":", "\n", "# Temperature (higher temperature => more likely to sample low probability tokens)", "\n", "                ", "if", "temperature", "!=", "1.0", ":", "\n", "                    ", "scores", "=", "scores", "/", "temperature", "\n", "# Top-p/top-k filtering", "\n", "", "scores", "=", "top_k_top_p_filtering", "(", "\n", "scores", ",", "top_k", "=", "top_k", ",", "top_p", "=", "top_p", ",", "min_tokens_to_keep", "=", "2", "\n", ")", "# (batch_size * num_beams, vocab_size)", "\n", "# Sample 2 next words for each beam (so we have some spare tokens and match output of greedy beam search)", "\n", "next_words", "=", "torch", ".", "multinomial", "(", "F", ".", "softmax", "(", "scores", ",", "dim", "=", "-", "1", ")", ",", "num_samples", "=", "2", ")", "# (batch_size * num_beams, 2)", "\n", "# Compute next scores", "\n", "_scores", "=", "F", ".", "log_softmax", "(", "scores", ",", "dim", "=", "-", "1", ")", "# (batch_size * num_beams, vocab_size)", "\n", "_scores", "=", "torch", ".", "gather", "(", "_scores", ",", "-", "1", ",", "next_words", ")", "# (batch_size * num_beams, 2)", "\n", "next_scores", "=", "_scores", "+", "beam_scores", "[", ":", ",", "None", "]", ".", "expand_as", "(", "_scores", ")", "# (batch_size * num_beams, 2)", "\n", "# Match shape of greedy beam search", "\n", "next_words", "=", "next_words", ".", "view", "(", "batch_size", ",", "2", "*", "num_beams", ")", "# (batch_size, 2 * num_beams)", "\n", "next_scores", "=", "next_scores", ".", "view", "(", "batch_size", ",", "2", "*", "num_beams", ")", "# (batch_size, 2 * num_beams)", "\n", "", "else", ":", "\n", "# do greedy beam search", "\n", "                ", "scores", "=", "F", ".", "log_softmax", "(", "scores", ",", "dim", "=", "-", "1", ")", "# (batch_size * num_beams, vocab_size)", "\n", "assert", "scores", ".", "size", "(", ")", "==", "(", "batch_size", "*", "num_beams", ",", "vocab_size", ")", "\n", "# Add the log prob of the new beams to the log prob of the beginning of the sequence (sum of logs == log of the product)", "\n", "_scores", "=", "scores", "+", "beam_scores", "[", ":", ",", "None", "]", ".", "expand_as", "(", "scores", ")", "# (batch_size * num_beams, vocab_size)", "\n", "# re-organize to group the beam together (we are keeping top hypothesis accross beams)", "\n", "_scores", "=", "_scores", ".", "view", "(", "batch_size", ",", "num_beams", "*", "vocab_size", ")", "# (batch_size, num_beams * vocab_size)", "\n", "next_scores", ",", "next_words", "=", "torch", ".", "topk", "(", "_scores", ",", "2", "*", "num_beams", ",", "dim", "=", "1", ",", "largest", "=", "True", ",", "sorted", "=", "True", ")", "\n", "\n", "", "assert", "next_scores", ".", "size", "(", ")", "==", "next_words", ".", "size", "(", ")", "==", "(", "batch_size", ",", "2", "*", "num_beams", ")", "\n", "\n", "# next batch beam content", "\n", "# list of (batch_size * num_beams) tuple(next hypothesis score, next word, current position in the batch)", "\n", "next_batch_beam", "=", "[", "]", "\n", "\n", "# for each sentence", "\n", "for", "batch_idx", "in", "range", "(", "batch_size", ")", ":", "\n", "\n", "# if we are done with this sentence", "\n", "                ", "done", "[", "batch_idx", "]", "=", "done", "[", "batch_idx", "]", "or", "generated_hyps", "[", "batch_idx", "]", ".", "is_done", "(", "\n", "next_scores", "[", "batch_idx", "]", ".", "max", "(", ")", ".", "item", "(", ")", "\n", ")", "\n", "if", "done", "[", "batch_idx", "]", ":", "\n", "                    ", "assert", "(", "\n", "len", "(", "generated_hyps", "[", "batch_idx", "]", ")", ">=", "num_beams", "\n", ")", ",", "\"Batch can only be done if at least {} beams have been generated\"", ".", "format", "(", "num_beams", ")", "\n", "assert", "(", "\n", "eos_token_ids", "is", "not", "None", "and", "pad_token_id", "is", "not", "None", "\n", ")", ",", "\"generated beams >= num_beams -> eos_token_id and pad_token have to be defined\"", "\n", "next_batch_beam", ".", "extend", "(", "[", "(", "0", ",", "pad_token_id", ",", "0", ")", "]", "*", "num_beams", ")", "# pad the batch", "\n", "continue", "\n", "\n", "# next sentence beam content", "\n", "", "next_sent_beam", "=", "[", "]", "\n", "\n", "# next words for this sentence", "\n", "for", "idx", ",", "score", "in", "zip", "(", "next_words", "[", "batch_idx", "]", ",", "next_scores", "[", "batch_idx", "]", ")", ":", "\n", "\n", "# get beam and word IDs", "\n", "                    ", "beam_id", "=", "idx", "//", "vocab_size", "\n", "word_id", "=", "idx", "%", "vocab_size", "\n", "\n", "# add to generated hypotheses if end of sentence or last iteration", "\n", "if", "eos_token_ids", "is", "not", "None", "and", "word_id", ".", "item", "(", ")", "in", "eos_token_ids", ":", "\n", "                        ", "generated_hyps", "[", "batch_idx", "]", ".", "add", "(", "\n", "input_ids", "[", "batch_idx", "*", "num_beams", "+", "beam_id", ",", ":", "cur_len", "]", ".", "clone", "(", ")", ",", "score", ".", "item", "(", ")", "\n", ")", "\n", "", "else", ":", "\n", "# add next predicted word if it is not eos_token", "\n", "                        ", "next_sent_beam", ".", "append", "(", "(", "score", ",", "word_id", ",", "batch_idx", "*", "num_beams", "+", "beam_id", ")", ")", "\n", "\n", "# the beam for next step is full", "\n", "", "if", "len", "(", "next_sent_beam", ")", "==", "num_beams", ":", "\n", "                        ", "break", "\n", "\n", "# update next beam content", "\n", "", "", "assert", "len", "(", "next_sent_beam", ")", "==", "num_beams", ",", "\"Beam should always be full\"", "\n", "next_batch_beam", ".", "extend", "(", "next_sent_beam", ")", "\n", "assert", "len", "(", "next_batch_beam", ")", "==", "num_beams", "*", "(", "batch_idx", "+", "1", ")", "\n", "\n", "# sanity check / prepare next batch", "\n", "", "assert", "len", "(", "next_batch_beam", ")", "==", "batch_size", "*", "num_beams", "\n", "beam_scores", "=", "beam_scores", ".", "new", "(", "[", "x", "[", "0", "]", "for", "x", "in", "next_batch_beam", "]", ")", "\n", "beam_words", "=", "input_ids", ".", "new", "(", "[", "x", "[", "1", "]", "for", "x", "in", "next_batch_beam", "]", ")", "\n", "beam_idx", "=", "input_ids", ".", "new", "(", "[", "x", "[", "2", "]", "for", "x", "in", "next_batch_beam", "]", ")", "\n", "\n", "# re-order batch", "\n", "input_ids", "=", "input_ids", "[", "beam_idx", ",", ":", "]", "\n", "input_ids", "=", "torch", ".", "cat", "(", "[", "input_ids", ",", "beam_words", ".", "unsqueeze", "(", "1", ")", "]", ",", "dim", "=", "-", "1", ")", "\n", "\n", "# re-order internal states", "\n", "if", "past", ":", "\n", "                ", "past", "=", "self", ".", "_reorder_cache", "(", "past", ",", "beam_idx", ")", "\n", "\n", "# update current length", "\n", "", "cur_len", "=", "cur_len", "+", "1", "\n", "\n", "# stop when we are done with each sentence", "\n", "if", "all", "(", "done", ")", ":", "\n", "                ", "break", "\n", "\n", "", "", "for", "batch_idx", "in", "range", "(", "batch_size", ")", ":", "\n", "# Add all open beam hypothesis to generated_hyps", "\n", "            ", "if", "not", "done", "[", "batch_idx", "]", ":", "\n", "                ", "for", "idx", ",", "score", "in", "zip", "(", "next_words", "[", "batch_idx", "]", ",", "next_scores", "[", "batch_idx", "]", ")", ":", "\n", "\n", "# get beam and word IDs", "\n", "                    ", "beam_id", "=", "idx", "//", "vocab_size", "\n", "word_id", "=", "idx", "%", "vocab_size", "\n", "generated_hyps", "[", "batch_idx", "]", ".", "add", "(", "\n", "input_ids", "[", "batch_idx", "*", "num_beams", "+", "beam_id", ",", ":", "cur_len", "]", ".", "clone", "(", ")", ",", "score", ".", "item", "(", ")", "\n", ")", "\n", "\n", "# depending on whether greedy generation is wanted or not define different output_batch_size and output_num_return_sequences_per_batch", "\n", "", "", "", "output_batch_size", "=", "batch_size", "if", "do_sample", "else", "batch_size", "*", "num_return_sequences", "\n", "output_num_return_sequences_per_batch", "=", "1", "if", "do_sample", "else", "num_return_sequences", "\n", "\n", "# select the best hypotheses", "\n", "sent_lengths", "=", "input_ids", ".", "new", "(", "output_batch_size", ")", "\n", "best", "=", "[", "]", "\n", "\n", "# retrieve best hypotheses", "\n", "for", "i", ",", "hypotheses", "in", "enumerate", "(", "generated_hyps", ")", ":", "\n", "            ", "sorted_hyps", "=", "sorted", "(", "hypotheses", ".", "beams", ",", "key", "=", "lambda", "x", ":", "x", "[", "0", "]", ")", "\n", "for", "j", "in", "range", "(", "output_num_return_sequences_per_batch", ")", ":", "\n", "                ", "effective_batch_idx", "=", "output_num_return_sequences_per_batch", "*", "i", "+", "j", "\n", "best_hyp", "=", "sorted_hyps", ".", "pop", "(", ")", "[", "1", "]", "\n", "sent_lengths", "[", "effective_batch_idx", "]", "=", "len", "(", "best_hyp", ")", "\n", "best", ".", "append", "(", "best_hyp", ")", "\n", "\n", "# shorter batches are filled with pad_token", "\n", "", "", "if", "sent_lengths", ".", "min", "(", ")", ".", "item", "(", ")", "!=", "sent_lengths", ".", "max", "(", ")", ".", "item", "(", ")", ":", "\n", "            ", "assert", "pad_token_id", "is", "not", "None", ",", "\"`Pad_token_id` has to be defined\"", "\n", "sent_max_len", "=", "min", "(", "sent_lengths", ".", "max", "(", ")", ".", "item", "(", ")", "+", "1", ",", "max_length", ")", "\n", "decoded", "=", "input_ids", ".", "new", "(", "output_batch_size", ",", "sent_max_len", ")", ".", "fill_", "(", "pad_token_id", ")", "\n", "\n", "# fill with hypothesis and eos_token_id if necessary", "\n", "for", "i", ",", "hypo", "in", "enumerate", "(", "best", ")", ":", "\n", "                ", "decoded", "[", "i", ",", ":", "sent_lengths", "[", "i", "]", "]", "=", "hypo", "\n", "if", "sent_lengths", "[", "i", "]", "<", "max_length", ":", "\n", "                    ", "decoded", "[", "i", ",", "sent_lengths", "[", "i", "]", "]", "=", "eos_token_ids", "[", "0", "]", "\n", "", "", "", "else", ":", "\n", "# none of the hypotheses have an eos_token", "\n", "            ", "assert", "(", "len", "(", "hypo", ")", "==", "max_length", "for", "hypo", "in", "best", ")", "\n", "decoded", "=", "torch", ".", "stack", "(", "best", ")", ".", "type", "(", "torch", ".", "long", ")", ".", "to", "(", "next", "(", "self", ".", "parameters", "(", ")", ")", ".", "device", ")", "\n", "\n", "", "return", "decoded", "\n", "\n"]], "home.repos.pwc.inspect_result.neulab_awesome-align.awesome_align.modeling_utils.PreTrainedModel._reorder_cache": [[1112, 1125], ["tuple", "torch.cat", "reordered_past.append", "layer_past[].unsqueeze().clone().detach", "layer_past[].unsqueeze().clone", "layer_past[].unsqueeze"], "methods", ["None"], ["", "@", "staticmethod", "\n", "def", "_reorder_cache", "(", "past", ",", "beam_idx", ")", ":", "\n", "        ", "reordered_past", "=", "[", "]", "\n", "for", "layer_past", "in", "past", ":", "\n", "# get the correct batch idx from layer past batch dim", "\n", "# batch dim of `past` and `mems` is at 2nd position", "\n", "            ", "reordered_layer_past", "=", "[", "layer_past", "[", ":", ",", "i", "]", ".", "unsqueeze", "(", "1", ")", ".", "clone", "(", ")", ".", "detach", "(", ")", "for", "i", "in", "beam_idx", "]", "\n", "reordered_layer_past", "=", "torch", ".", "cat", "(", "reordered_layer_past", ",", "dim", "=", "1", ")", "\n", "# check that shape matches", "\n", "assert", "reordered_layer_past", ".", "shape", "==", "layer_past", ".", "shape", "\n", "reordered_past", ".", "append", "(", "reordered_layer_past", ")", "\n", "", "past", "=", "tuple", "(", "reordered_past", ")", "\n", "return", "past", "\n", "\n"]], "home.repos.pwc.inspect_result.neulab_awesome-align.awesome_align.modeling_utils.BeamHypotheses.__init__": [[1163, 1173], ["None"], "methods", ["None"], ["    ", "def", "__init__", "(", "self", ",", "num_beams", ",", "max_length", ",", "length_penalty", ",", "early_stopping", ")", ":", "\n", "        ", "\"\"\"\n        Initialize n-best list of hypotheses.\n        \"\"\"", "\n", "self", ".", "max_length", "=", "max_length", "-", "1", "# ignoring bos_token", "\n", "self", ".", "length_penalty", "=", "length_penalty", "\n", "self", ".", "early_stopping", "=", "early_stopping", "\n", "self", ".", "num_beams", "=", "num_beams", "\n", "self", ".", "beams", "=", "[", "]", "\n", "self", ".", "worst_score", "=", "1e9", "\n", "\n"]], "home.repos.pwc.inspect_result.neulab_awesome-align.awesome_align.modeling_utils.BeamHypotheses.__len__": [[1174, 1179], ["len"], "methods", ["None"], ["", "def", "__len__", "(", "self", ")", ":", "\n", "        ", "\"\"\"\n        Number of hypotheses in the list.\n        \"\"\"", "\n", "return", "len", "(", "self", ".", "beams", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.neulab_awesome-align.awesome_align.modeling_utils.BeamHypotheses.add": [[1180, 1193], ["modeling_utils.BeamHypotheses.beams.append", "len", "len", "len", "sorted", "min", "enumerate"], "methods", ["None"], ["", "def", "add", "(", "self", ",", "hyp", ",", "sum_logprobs", ")", ":", "\n", "        ", "\"\"\"\n        Add a new hypothesis to the list.\n        \"\"\"", "\n", "score", "=", "sum_logprobs", "/", "len", "(", "hyp", ")", "**", "self", ".", "length_penalty", "\n", "if", "len", "(", "self", ")", "<", "self", ".", "num_beams", "or", "score", ">", "self", ".", "worst_score", ":", "\n", "            ", "self", ".", "beams", ".", "append", "(", "(", "score", ",", "hyp", ")", ")", "\n", "if", "len", "(", "self", ")", ">", "self", ".", "num_beams", ":", "\n", "                ", "sorted_scores", "=", "sorted", "(", "[", "(", "s", ",", "idx", ")", "for", "idx", ",", "(", "s", ",", "_", ")", "in", "enumerate", "(", "self", ".", "beams", ")", "]", ")", "\n", "del", "self", ".", "beams", "[", "sorted_scores", "[", "0", "]", "[", "1", "]", "]", "\n", "self", ".", "worst_score", "=", "sorted_scores", "[", "1", "]", "[", "0", "]", "\n", "", "else", ":", "\n", "                ", "self", ".", "worst_score", "=", "min", "(", "score", ",", "self", ".", "worst_score", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.neulab_awesome-align.awesome_align.modeling_utils.BeamHypotheses.is_done": [[1194, 1210], ["len"], "methods", ["None"], ["", "", "", "def", "is_done", "(", "self", ",", "best_sum_logprobs", ",", "cur_len", "=", "None", ")", ":", "\n", "        ", "\"\"\"\n        If there are enough hypotheses and that none of the hypotheses being generated\n        can become better than the worst one in the heap, then we are done with this sentence.\n        \"\"\"", "\n", "\n", "if", "len", "(", "self", ")", "<", "self", ".", "num_beams", ":", "\n", "            ", "return", "False", "\n", "", "elif", "self", ".", "early_stopping", ":", "\n", "            ", "return", "True", "\n", "", "else", ":", "\n", "            ", "if", "cur_len", "is", "None", ":", "\n", "                ", "cur_len", "=", "self", ".", "max_length", "\n", "", "cur_score", "=", "best_sum_logprobs", "/", "cur_len", "**", "self", ".", "length_penalty", "\n", "ret", "=", "self", ".", "worst_score", ">=", "cur_score", "\n", "return", "ret", "\n", "\n"]], "home.repos.pwc.inspect_result.neulab_awesome-align.awesome_align.modeling_utils.Conv1D.__init__": [[1213, 1223], ["torch.nn.Module.__init__", "torch.empty", "torch.nn.init.normal_", "torch.nn.Parameter", "torch.nn.Parameter", "torch.zeros"], "methods", ["home.repos.pwc.inspect_result.neulab_awesome-align.awesome_align.train_utils.AdamW.__init__"], ["    ", "def", "__init__", "(", "self", ",", "nf", ",", "nx", ")", ":", "\n", "        ", "\"\"\" Conv1D layer as defined by Radford et al. for OpenAI GPT (and also used in GPT-2)\n            Basically works like a Linear layer but the weights are transposed\n        \"\"\"", "\n", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "nf", "=", "nf", "\n", "w", "=", "torch", ".", "empty", "(", "nx", ",", "nf", ")", "\n", "nn", ".", "init", ".", "normal_", "(", "w", ",", "std", "=", "0.02", ")", "\n", "self", ".", "weight", "=", "nn", ".", "Parameter", "(", "w", ")", "\n", "self", ".", "bias", "=", "nn", ".", "Parameter", "(", "torch", ".", "zeros", "(", "nf", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.neulab_awesome-align.awesome_align.modeling_utils.Conv1D.forward": [[1224, 1229], ["torch.addmm", "x.view.view.view", "x.view.view.view", "x.view.view.size", "x.view.view.size"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "size_out", "=", "x", ".", "size", "(", ")", "[", ":", "-", "1", "]", "+", "(", "self", ".", "nf", ",", ")", "\n", "x", "=", "torch", ".", "addmm", "(", "self", ".", "bias", ",", "x", ".", "view", "(", "-", "1", ",", "x", ".", "size", "(", "-", "1", ")", ")", ",", "self", ".", "weight", ")", "\n", "x", "=", "x", ".", "view", "(", "*", "size_out", ")", "\n", "return", "x", "\n", "\n"]], "home.repos.pwc.inspect_result.neulab_awesome-align.awesome_align.modeling_utils.PoolerStartLogits.__init__": [[1234, 1237], ["torch.nn.Module.__init__", "torch.nn.Linear"], "methods", ["home.repos.pwc.inspect_result.neulab_awesome-align.awesome_align.train_utils.AdamW.__init__"], ["def", "__init__", "(", "self", ",", "config", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "dense", "=", "nn", ".", "Linear", "(", "config", ".", "hidden_size", ",", "1", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.neulab_awesome-align.awesome_align.modeling_utils.PoolerStartLogits.forward": [[1238, 1253], ["modeling_utils.PoolerStartLogits.dense().squeeze", "modeling_utils.PoolerStartLogits.dense", "next", "modeling_utils.PoolerStartLogits.parameters"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "hidden_states", ",", "p_mask", "=", "None", ")", ":", "\n", "        ", "\"\"\" Args:\n            **p_mask**: (`optional`) ``torch.FloatTensor`` of shape `(batch_size, seq_len)`\n                invalid position mask such as query and special symbols (PAD, SEP, CLS)\n                1.0 means token should be masked.\n        \"\"\"", "\n", "x", "=", "self", ".", "dense", "(", "hidden_states", ")", ".", "squeeze", "(", "-", "1", ")", "\n", "\n", "if", "p_mask", "is", "not", "None", ":", "\n", "            ", "if", "next", "(", "self", ".", "parameters", "(", ")", ")", ".", "dtype", "==", "torch", ".", "float16", ":", "\n", "                ", "x", "=", "x", "*", "(", "1", "-", "p_mask", ")", "-", "65500", "*", "p_mask", "\n", "", "else", ":", "\n", "                ", "x", "=", "x", "*", "(", "1", "-", "p_mask", ")", "-", "1e30", "*", "p_mask", "\n", "\n", "", "", "return", "x", "\n", "\n"]], "home.repos.pwc.inspect_result.neulab_awesome-align.awesome_align.modeling_utils.PoolerEndLogits.__init__": [[1259, 1265], ["torch.nn.Module.__init__", "torch.nn.Linear", "torch.nn.Tanh", "torch.nn.LayerNorm", "torch.nn.Linear"], "methods", ["home.repos.pwc.inspect_result.neulab_awesome-align.awesome_align.train_utils.AdamW.__init__"], ["def", "__init__", "(", "self", ",", "config", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "dense_0", "=", "nn", ".", "Linear", "(", "config", ".", "hidden_size", "*", "2", ",", "config", ".", "hidden_size", ")", "\n", "self", ".", "activation", "=", "nn", ".", "Tanh", "(", ")", "\n", "self", ".", "LayerNorm", "=", "nn", ".", "LayerNorm", "(", "config", ".", "hidden_size", ",", "eps", "=", "config", ".", "layer_norm_eps", ")", "\n", "self", ".", "dense_1", "=", "nn", ".", "Linear", "(", "config", ".", "hidden_size", ",", "1", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.neulab_awesome-align.awesome_align.modeling_utils.PoolerEndLogits.forward": [[1266, 1300], ["modeling_utils.PoolerEndLogits.dense_0", "modeling_utils.PoolerEndLogits.activation", "modeling_utils.PoolerEndLogits.LayerNorm", "modeling_utils.PoolerEndLogits.dense_1().squeeze", "start_positions[].expand", "hidden_states.gather", "start_states.expand.expand.expand", "torch.cat", "modeling_utils.PoolerEndLogits.dense_1", "next", "modeling_utils.PoolerEndLogits.parameters"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "hidden_states", ",", "start_states", "=", "None", ",", "start_positions", "=", "None", ",", "p_mask", "=", "None", ")", ":", "\n", "        ", "\"\"\" Args:\n            One of ``start_states``, ``start_positions`` should be not None.\n            If both are set, ``start_positions`` overrides ``start_states``.\n\n            **start_states**: ``torch.LongTensor`` of shape identical to hidden_states\n                hidden states of the first tokens for the labeled span.\n            **start_positions**: ``torch.LongTensor`` of shape ``(batch_size,)``\n                position of the first token for the labeled span:\n            **p_mask**: (`optional`) ``torch.FloatTensor`` of shape ``(batch_size, seq_len)``\n                Mask of invalid position such as query and special symbols (PAD, SEP, CLS)\n                1.0 means token should be masked.\n        \"\"\"", "\n", "assert", "(", "\n", "start_states", "is", "not", "None", "or", "start_positions", "is", "not", "None", "\n", ")", ",", "\"One of start_states, start_positions should be not None\"", "\n", "if", "start_positions", "is", "not", "None", ":", "\n", "            ", "slen", ",", "hsz", "=", "hidden_states", ".", "shape", "[", "-", "2", ":", "]", "\n", "start_positions", "=", "start_positions", "[", ":", ",", "None", ",", "None", "]", ".", "expand", "(", "-", "1", ",", "-", "1", ",", "hsz", ")", "# shape (bsz, 1, hsz)", "\n", "start_states", "=", "hidden_states", ".", "gather", "(", "-", "2", ",", "start_positions", ")", "# shape (bsz, 1, hsz)", "\n", "start_states", "=", "start_states", ".", "expand", "(", "-", "1", ",", "slen", ",", "-", "1", ")", "# shape (bsz, slen, hsz)", "\n", "\n", "", "x", "=", "self", ".", "dense_0", "(", "torch", ".", "cat", "(", "[", "hidden_states", ",", "start_states", "]", ",", "dim", "=", "-", "1", ")", ")", "\n", "x", "=", "self", ".", "activation", "(", "x", ")", "\n", "x", "=", "self", ".", "LayerNorm", "(", "x", ")", "\n", "x", "=", "self", ".", "dense_1", "(", "x", ")", ".", "squeeze", "(", "-", "1", ")", "\n", "\n", "if", "p_mask", "is", "not", "None", ":", "\n", "            ", "if", "next", "(", "self", ".", "parameters", "(", ")", ")", ".", "dtype", "==", "torch", ".", "float16", ":", "\n", "                ", "x", "=", "x", "*", "(", "1", "-", "p_mask", ")", "-", "65500", "*", "p_mask", "\n", "", "else", ":", "\n", "                ", "x", "=", "x", "*", "(", "1", "-", "p_mask", ")", "-", "1e30", "*", "p_mask", "\n", "\n", "", "", "return", "x", "\n", "\n"]], "home.repos.pwc.inspect_result.neulab_awesome-align.awesome_align.modeling_utils.PoolerAnswerClass.__init__": [[1305, 1310], ["torch.nn.Module.__init__", "torch.nn.Linear", "torch.nn.Tanh", "torch.nn.Linear"], "methods", ["home.repos.pwc.inspect_result.neulab_awesome-align.awesome_align.train_utils.AdamW.__init__"], ["def", "__init__", "(", "self", ",", "config", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "dense_0", "=", "nn", ".", "Linear", "(", "config", ".", "hidden_size", "*", "2", ",", "config", ".", "hidden_size", ")", "\n", "self", ".", "activation", "=", "nn", ".", "Tanh", "(", ")", "\n", "self", ".", "dense_1", "=", "nn", ".", "Linear", "(", "config", ".", "hidden_size", ",", "1", ",", "bias", "=", "False", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.neulab_awesome-align.awesome_align.modeling_utils.PoolerAnswerClass.forward": [[1311, 1347], ["modeling_utils.PoolerAnswerClass.dense_0", "modeling_utils.PoolerAnswerClass.activation", "modeling_utils.PoolerAnswerClass.dense_1().squeeze", "start_positions[].expand", "hidden_states.gather().squeeze", "cls_index[].expand", "hidden_states.gather().squeeze", "torch.cat", "modeling_utils.PoolerAnswerClass.dense_1", "hidden_states.gather", "hidden_states.gather"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "hidden_states", ",", "start_states", "=", "None", ",", "start_positions", "=", "None", ",", "cls_index", "=", "None", ")", ":", "\n", "        ", "\"\"\"\n        Args:\n            One of ``start_states``, ``start_positions`` should be not None.\n            If both are set, ``start_positions`` overrides ``start_states``.\n\n            **start_states**: ``torch.LongTensor`` of shape identical to ``hidden_states``.\n                hidden states of the first tokens for the labeled span.\n            **start_positions**: ``torch.LongTensor`` of shape ``(batch_size,)``\n                position of the first token for the labeled span.\n            **cls_index**: torch.LongTensor of shape ``(batch_size,)``\n                position of the CLS token. If None, take the last token.\n\n            note(Original repo):\n                no dependency on end_feature so that we can obtain one single `cls_logits`\n                for each sample\n        \"\"\"", "\n", "hsz", "=", "hidden_states", ".", "shape", "[", "-", "1", "]", "\n", "assert", "(", "\n", "start_states", "is", "not", "None", "or", "start_positions", "is", "not", "None", "\n", ")", ",", "\"One of start_states, start_positions should be not None\"", "\n", "if", "start_positions", "is", "not", "None", ":", "\n", "            ", "start_positions", "=", "start_positions", "[", ":", ",", "None", ",", "None", "]", ".", "expand", "(", "-", "1", ",", "-", "1", ",", "hsz", ")", "# shape (bsz, 1, hsz)", "\n", "start_states", "=", "hidden_states", ".", "gather", "(", "-", "2", ",", "start_positions", ")", ".", "squeeze", "(", "-", "2", ")", "# shape (bsz, hsz)", "\n", "\n", "", "if", "cls_index", "is", "not", "None", ":", "\n", "            ", "cls_index", "=", "cls_index", "[", ":", ",", "None", ",", "None", "]", ".", "expand", "(", "-", "1", ",", "-", "1", ",", "hsz", ")", "# shape (bsz, 1, hsz)", "\n", "cls_token_state", "=", "hidden_states", ".", "gather", "(", "-", "2", ",", "cls_index", ")", ".", "squeeze", "(", "-", "2", ")", "# shape (bsz, hsz)", "\n", "", "else", ":", "\n", "            ", "cls_token_state", "=", "hidden_states", "[", ":", ",", "-", "1", ",", ":", "]", "# shape (bsz, hsz)", "\n", "\n", "", "x", "=", "self", ".", "dense_0", "(", "torch", ".", "cat", "(", "[", "start_states", ",", "cls_token_state", "]", ",", "dim", "=", "-", "1", ")", ")", "\n", "x", "=", "self", ".", "activation", "(", "x", ")", "\n", "x", "=", "self", ".", "dense_1", "(", "x", ")", ".", "squeeze", "(", "-", "1", ")", "\n", "\n", "return", "x", "\n", "\n"]], "home.repos.pwc.inspect_result.neulab_awesome-align.awesome_align.modeling_utils.SQuADHead.__init__": [[1390, 1398], ["torch.nn.Module.__init__", "modeling_utils.PoolerStartLogits", "modeling_utils.PoolerEndLogits", "modeling_utils.PoolerAnswerClass"], "methods", ["home.repos.pwc.inspect_result.neulab_awesome-align.awesome_align.train_utils.AdamW.__init__"], ["def", "__init__", "(", "self", ",", "config", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "start_n_top", "=", "config", ".", "start_n_top", "\n", "self", ".", "end_n_top", "=", "config", ".", "end_n_top", "\n", "\n", "self", ".", "start_logits", "=", "PoolerStartLogits", "(", "config", ")", "\n", "self", ".", "end_logits", "=", "PoolerEndLogits", "(", "config", ")", "\n", "self", ".", "answer_class", "=", "PoolerAnswerClass", "(", "config", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.neulab_awesome-align.awesome_align.modeling_utils.SQuADHead.forward": [[1399, 1464], ["modeling_utils.SQuADHead.start_logits", "modeling_utils.SQuADHead.end_logits", "torch.nn.CrossEntropyLoss", "torch.nn.CrossEntropyLoss.", "torch.nn.CrossEntropyLoss.", "hidden_states.size", "torch.nn.functional.softmax", "torch.topk", "start_top_index.unsqueeze().expand", "torch.gather", "torch.einsum.unsqueeze().expand", "hidden_states.unsqueeze().expand_as", "modeling_utils.SQuADHead.end_logits", "torch.nn.functional.softmax", "torch.topk", "end_top_log_probs.view.view.view", "end_top_index.view.view.view", "torch.einsum", "modeling_utils.SQuADHead.answer_class", "modeling_utils.SQuADHead.answer_class", "torch.nn.BCEWithLogitsLoss", "torch.nn.BCEWithLogitsLoss.", "p_mask.unsqueeze", "x.squeeze_", "start_top_index.unsqueeze", "torch.einsum.unsqueeze", "hidden_states.unsqueeze", "x.dim"], "methods", ["None"], ["", "def", "forward", "(", "\n", "self", ",", "hidden_states", ",", "start_positions", "=", "None", ",", "end_positions", "=", "None", ",", "cls_index", "=", "None", ",", "is_impossible", "=", "None", ",", "p_mask", "=", "None", "\n", ")", ":", "\n", "        ", "outputs", "=", "(", ")", "\n", "\n", "start_logits", "=", "self", ".", "start_logits", "(", "hidden_states", ",", "p_mask", "=", "p_mask", ")", "\n", "\n", "if", "start_positions", "is", "not", "None", "and", "end_positions", "is", "not", "None", ":", "\n", "# If we are on multi-GPU, let's remove the dimension added by batch splitting", "\n", "            ", "for", "x", "in", "(", "start_positions", ",", "end_positions", ",", "cls_index", ",", "is_impossible", ")", ":", "\n", "                ", "if", "x", "is", "not", "None", "and", "x", ".", "dim", "(", ")", ">", "1", ":", "\n", "                    ", "x", ".", "squeeze_", "(", "-", "1", ")", "\n", "\n", "# during training, compute the end logits based on the ground truth of the start position", "\n", "", "", "end_logits", "=", "self", ".", "end_logits", "(", "hidden_states", ",", "start_positions", "=", "start_positions", ",", "p_mask", "=", "p_mask", ")", "\n", "\n", "loss_fct", "=", "CrossEntropyLoss", "(", ")", "\n", "start_loss", "=", "loss_fct", "(", "start_logits", ",", "start_positions", ")", "\n", "end_loss", "=", "loss_fct", "(", "end_logits", ",", "end_positions", ")", "\n", "total_loss", "=", "(", "start_loss", "+", "end_loss", ")", "/", "2", "\n", "\n", "if", "cls_index", "is", "not", "None", "and", "is_impossible", "is", "not", "None", ":", "\n", "# Predict answerability from the representation of CLS and START", "\n", "                ", "cls_logits", "=", "self", ".", "answer_class", "(", "hidden_states", ",", "start_positions", "=", "start_positions", ",", "cls_index", "=", "cls_index", ")", "\n", "loss_fct_cls", "=", "nn", ".", "BCEWithLogitsLoss", "(", ")", "\n", "cls_loss", "=", "loss_fct_cls", "(", "cls_logits", ",", "is_impossible", ")", "\n", "\n", "# note(zhiliny): by default multiply the loss by 0.5 so that the scale is comparable to start_loss and end_loss", "\n", "total_loss", "+=", "cls_loss", "*", "0.5", "\n", "\n", "", "outputs", "=", "(", "total_loss", ",", ")", "+", "outputs", "\n", "\n", "", "else", ":", "\n", "# during inference, compute the end logits based on beam search", "\n", "            ", "bsz", ",", "slen", ",", "hsz", "=", "hidden_states", ".", "size", "(", ")", "\n", "start_log_probs", "=", "F", ".", "softmax", "(", "start_logits", ",", "dim", "=", "-", "1", ")", "# shape (bsz, slen)", "\n", "\n", "start_top_log_probs", ",", "start_top_index", "=", "torch", ".", "topk", "(", "\n", "start_log_probs", ",", "self", ".", "start_n_top", ",", "dim", "=", "-", "1", "\n", ")", "# shape (bsz, start_n_top)", "\n", "start_top_index_exp", "=", "start_top_index", ".", "unsqueeze", "(", "-", "1", ")", ".", "expand", "(", "-", "1", ",", "-", "1", ",", "hsz", ")", "# shape (bsz, start_n_top, hsz)", "\n", "start_states", "=", "torch", ".", "gather", "(", "hidden_states", ",", "-", "2", ",", "start_top_index_exp", ")", "# shape (bsz, start_n_top, hsz)", "\n", "start_states", "=", "start_states", ".", "unsqueeze", "(", "1", ")", ".", "expand", "(", "-", "1", ",", "slen", ",", "-", "1", ",", "-", "1", ")", "# shape (bsz, slen, start_n_top, hsz)", "\n", "\n", "hidden_states_expanded", "=", "hidden_states", ".", "unsqueeze", "(", "2", ")", ".", "expand_as", "(", "\n", "start_states", "\n", ")", "# shape (bsz, slen, start_n_top, hsz)", "\n", "p_mask", "=", "p_mask", ".", "unsqueeze", "(", "-", "1", ")", "if", "p_mask", "is", "not", "None", "else", "None", "\n", "end_logits", "=", "self", ".", "end_logits", "(", "hidden_states_expanded", ",", "start_states", "=", "start_states", ",", "p_mask", "=", "p_mask", ")", "\n", "end_log_probs", "=", "F", ".", "softmax", "(", "end_logits", ",", "dim", "=", "1", ")", "# shape (bsz, slen, start_n_top)", "\n", "\n", "end_top_log_probs", ",", "end_top_index", "=", "torch", ".", "topk", "(", "\n", "end_log_probs", ",", "self", ".", "end_n_top", ",", "dim", "=", "1", "\n", ")", "# shape (bsz, end_n_top, start_n_top)", "\n", "end_top_log_probs", "=", "end_top_log_probs", ".", "view", "(", "-", "1", ",", "self", ".", "start_n_top", "*", "self", ".", "end_n_top", ")", "\n", "end_top_index", "=", "end_top_index", ".", "view", "(", "-", "1", ",", "self", ".", "start_n_top", "*", "self", ".", "end_n_top", ")", "\n", "\n", "start_states", "=", "torch", ".", "einsum", "(", "\"blh,bl->bh\"", ",", "hidden_states", ",", "start_log_probs", ")", "\n", "cls_logits", "=", "self", ".", "answer_class", "(", "hidden_states", ",", "start_states", "=", "start_states", ",", "cls_index", "=", "cls_index", ")", "\n", "\n", "outputs", "=", "(", "start_top_log_probs", ",", "start_top_index", ",", "end_top_log_probs", ",", "end_top_index", ",", "cls_logits", ")", "+", "outputs", "\n", "\n", "# return start_top_log_probs, start_top_index, end_top_log_probs, end_top_index, cls_logits", "\n", "# or (if labels are provided) (total_loss,)", "\n", "", "return", "outputs", "\n", "\n"]], "home.repos.pwc.inspect_result.neulab_awesome-align.awesome_align.modeling_utils.SequenceSummary.__init__": [[1482, 1512], ["torch.nn.Module.__init__", "getattr", "Identity", "getattr", "Identity", "Identity", "hasattr", "torch.nn.Linear", "awesome_align.activations.get_activation", "Identity", "hasattr", "torch.nn.Dropout", "hasattr", "torch.nn.Dropout", "hasattr"], "methods", ["home.repos.pwc.inspect_result.neulab_awesome-align.awesome_align.train_utils.AdamW.__init__", "home.repos.pwc.inspect_result.neulab_awesome-align.awesome_align.activations.get_activation"], ["def", "__init__", "(", "self", ",", "config", ":", "PretrainedConfig", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "\n", "self", ".", "summary_type", "=", "getattr", "(", "config", ",", "\"summary_type\"", ",", "\"last\"", ")", "\n", "if", "self", ".", "summary_type", "==", "\"attn\"", ":", "\n", "# We should use a standard multi-head attention module with absolute positional embedding for that.", "\n", "# Cf. https://github.com/zihangdai/xlnet/blob/master/modeling.py#L253-L276", "\n", "# We can probably just use the multi-head attention module of PyTorch >=1.1.0", "\n", "            ", "raise", "NotImplementedError", "\n", "\n", "", "self", ".", "summary", "=", "Identity", "(", ")", "\n", "if", "hasattr", "(", "config", ",", "\"summary_use_proj\"", ")", "and", "config", ".", "summary_use_proj", ":", "\n", "            ", "if", "hasattr", "(", "config", ",", "\"summary_proj_to_labels\"", ")", "and", "config", ".", "summary_proj_to_labels", "and", "config", ".", "num_labels", ">", "0", ":", "\n", "                ", "num_classes", "=", "config", ".", "num_labels", "\n", "", "else", ":", "\n", "                ", "num_classes", "=", "config", ".", "hidden_size", "\n", "", "self", ".", "summary", "=", "nn", ".", "Linear", "(", "config", ".", "hidden_size", ",", "num_classes", ")", "\n", "\n", "", "activation_string", "=", "getattr", "(", "config", ",", "\"summary_activation\"", ",", "None", ")", "\n", "self", ".", "activation", "=", "(", "\n", "get_activation", "(", "activation_string", ")", "if", "activation_string", "else", "Identity", "(", ")", "\n", ")", "# type: typing.Callable", "\n", "\n", "self", ".", "first_dropout", "=", "Identity", "(", ")", "\n", "if", "hasattr", "(", "config", ",", "\"summary_first_dropout\"", ")", "and", "config", ".", "summary_first_dropout", ">", "0", ":", "\n", "            ", "self", ".", "first_dropout", "=", "nn", ".", "Dropout", "(", "config", ".", "summary_first_dropout", ")", "\n", "\n", "", "self", ".", "last_dropout", "=", "Identity", "(", ")", "\n", "if", "hasattr", "(", "config", ",", "\"summary_last_dropout\"", ")", "and", "config", ".", "summary_last_dropout", ">", "0", ":", "\n", "            ", "self", ".", "last_dropout", "=", "nn", ".", "Dropout", "(", "config", ".", "summary_last_dropout", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.neulab_awesome-align.awesome_align.modeling_utils.SequenceSummary.forward": [[1513, 1543], ["modeling_utils.SequenceSummary.first_dropout", "modeling_utils.SequenceSummary.summary", "modeling_utils.SequenceSummary.activation", "modeling_utils.SequenceSummary.last_dropout", "hidden_states.mean", "hidden_states.gather().squeeze", "torch.full_like", "cls_index.expand.expand.unsqueeze().unsqueeze", "cls_index.expand.expand.expand", "hidden_states.gather", "cls_index.expand.expand.unsqueeze", "hidden_states.size", "cls_index.expand.expand.dim"], "methods", ["None"], ["", "", "def", "forward", "(", "self", ",", "hidden_states", ",", "cls_index", "=", "None", ")", ":", "\n", "        ", "\"\"\" hidden_states: float Tensor in shape [bsz, ..., seq_len, hidden_size], the hidden-states of the last layer.\n            cls_index: [optional] position of the classification token if summary_type == 'cls_index',\n                shape (bsz,) or more generally (bsz, ...) where ... are optional leading dimensions of hidden_states.\n                if summary_type == 'cls_index' and cls_index is None:\n                    we take the last token of the sequence as classification token\n        \"\"\"", "\n", "if", "self", ".", "summary_type", "==", "\"last\"", ":", "\n", "            ", "output", "=", "hidden_states", "[", ":", ",", "-", "1", "]", "\n", "", "elif", "self", ".", "summary_type", "==", "\"first\"", ":", "\n", "            ", "output", "=", "hidden_states", "[", ":", ",", "0", "]", "\n", "", "elif", "self", ".", "summary_type", "==", "\"mean\"", ":", "\n", "            ", "output", "=", "hidden_states", ".", "mean", "(", "dim", "=", "1", ")", "\n", "", "elif", "self", ".", "summary_type", "==", "\"cls_index\"", ":", "\n", "            ", "if", "cls_index", "is", "None", ":", "\n", "                ", "cls_index", "=", "torch", ".", "full_like", "(", "hidden_states", "[", "...", ",", ":", "1", ",", ":", "]", ",", "hidden_states", ".", "shape", "[", "-", "2", "]", "-", "1", ",", "dtype", "=", "torch", ".", "long", ")", "\n", "", "else", ":", "\n", "                ", "cls_index", "=", "cls_index", ".", "unsqueeze", "(", "-", "1", ")", ".", "unsqueeze", "(", "-", "1", ")", "\n", "cls_index", "=", "cls_index", ".", "expand", "(", "(", "-", "1", ",", ")", "*", "(", "cls_index", ".", "dim", "(", ")", "-", "1", ")", "+", "(", "hidden_states", ".", "size", "(", "-", "1", ")", ",", ")", ")", "\n", "# shape of cls_index: (bsz, XX, 1, hidden_size) where XX are optional leading dim of hidden_states", "\n", "", "output", "=", "hidden_states", ".", "gather", "(", "-", "2", ",", "cls_index", ")", ".", "squeeze", "(", "-", "2", ")", "# shape (bsz, XX, hidden_size)", "\n", "", "elif", "self", ".", "summary_type", "==", "\"attn\"", ":", "\n", "            ", "raise", "NotImplementedError", "\n", "\n", "", "output", "=", "self", ".", "first_dropout", "(", "output", ")", "\n", "output", "=", "self", ".", "summary", "(", "output", ")", "\n", "output", "=", "self", ".", "activation", "(", "output", ")", "\n", "output", "=", "self", ".", "last_dropout", "(", "output", ")", "\n", "\n", "return", "output", "\n", "\n"]], "home.repos.pwc.inspect_result.neulab_awesome-align.awesome_align.modeling_utils.top_k_top_p_filtering": [[1127, 1160], ["float", "min", "torch.sort", "torch.cumsum", "sorted_indices_to_remove[].clone", "sorted_indices_to_remove.scatter", "max", "logits.size", "torch.nn.functional.softmax", "torch.topk"], "function", ["None"], ["", "", "def", "top_k_top_p_filtering", "(", "logits", ",", "top_k", "=", "0", ",", "top_p", "=", "1.0", ",", "filter_value", "=", "-", "float", "(", "\"Inf\"", ")", ",", "min_tokens_to_keep", "=", "1", ")", ":", "\n", "    ", "\"\"\" Filter a distribution of logits using top-k and/or nucleus (top-p) filtering\n        Args:\n            logits: logits distribution shape (batch size, vocabulary size)\n            if top_k > 0: keep only top k tokens with highest probability (top-k filtering).\n            if top_p < 1.0: keep the top tokens with cumulative probability >= top_p (nucleus filtering).\n                Nucleus filtering is described in Holtzman et al. (http://arxiv.org/abs/1904.09751)\n            Make sure we keep at least min_tokens_to_keep per batch example in the output\n        From: https://gist.github.com/thomwolf/1a5a29f6962089e871b94cbd09daf317\n    \"\"\"", "\n", "if", "top_k", ">", "0", ":", "\n", "        ", "top_k", "=", "min", "(", "max", "(", "top_k", ",", "min_tokens_to_keep", ")", ",", "logits", ".", "size", "(", "-", "1", ")", ")", "# Safety check", "\n", "# Remove all tokens with a probability less than the last token of the top-k", "\n", "indices_to_remove", "=", "logits", "<", "torch", ".", "topk", "(", "logits", ",", "top_k", ")", "[", "0", "]", "[", "...", ",", "-", "1", ",", "None", "]", "\n", "logits", "[", "indices_to_remove", "]", "=", "filter_value", "\n", "\n", "", "if", "top_p", "<", "1.0", ":", "\n", "        ", "sorted_logits", ",", "sorted_indices", "=", "torch", ".", "sort", "(", "logits", ",", "descending", "=", "True", ")", "\n", "cumulative_probs", "=", "torch", ".", "cumsum", "(", "F", ".", "softmax", "(", "sorted_logits", ",", "dim", "=", "-", "1", ")", ",", "dim", "=", "-", "1", ")", "\n", "\n", "# Remove tokens with cumulative probability above the threshold (token with 0 are kept)", "\n", "sorted_indices_to_remove", "=", "cumulative_probs", ">", "top_p", "\n", "if", "min_tokens_to_keep", ">", "1", ":", "\n", "# Keep at least min_tokens_to_keep (set to min_tokens_to_keep-1 because we add the first one below)", "\n", "            ", "sorted_indices_to_remove", "[", "...", ",", ":", "min_tokens_to_keep", "]", "=", "0", "\n", "# Shift the indices to the right to keep also the first token above the threshold", "\n", "", "sorted_indices_to_remove", "[", "...", ",", "1", ":", "]", "=", "sorted_indices_to_remove", "[", "...", ",", ":", "-", "1", "]", ".", "clone", "(", ")", "\n", "sorted_indices_to_remove", "[", "...", ",", "0", "]", "=", "0", "\n", "\n", "# scatter sorted tensors to original indexing", "\n", "indices_to_remove", "=", "sorted_indices_to_remove", ".", "scatter", "(", "1", ",", "sorted_indices", ",", "sorted_indices_to_remove", ")", "\n", "logits", "[", "indices_to_remove", "]", "=", "filter_value", "\n", "", "return", "logits", "\n", "\n"]], "home.repos.pwc.inspect_result.neulab_awesome-align.awesome_align.modeling_utils.create_position_ids_from_input_ids": [[1545, 1557], ["input_ids.ne().int", "torch.cumsum().type_as", "incremental_indicies.long", "input_ids.ne", "torch.cumsum"], "function", ["None"], ["", "", "def", "create_position_ids_from_input_ids", "(", "input_ids", ",", "padding_idx", ")", ":", "\n", "    ", "\"\"\" Replace non-padding symbols with their position numbers. Position numbers begin at\n    padding_idx+1. Padding symbols are ignored. This is modified from fairseq's\n    `utils.make_positions`.\n\n    :param torch.Tensor x:\n    :return torch.Tensor:\n    \"\"\"", "\n", "# The series of casts and type-conversions here are carefully balanced to both work with ONNX export and XLA.", "\n", "mask", "=", "input_ids", ".", "ne", "(", "padding_idx", ")", ".", "int", "(", ")", "\n", "incremental_indicies", "=", "torch", ".", "cumsum", "(", "mask", ",", "dim", "=", "1", ")", ".", "type_as", "(", "mask", ")", "*", "mask", "\n", "return", "incremental_indicies", ".", "long", "(", ")", "+", "padding_idx", "\n", "\n"]], "home.repos.pwc.inspect_result.neulab_awesome-align.awesome_align.modeling_utils.prune_linear_layer": [[1559, 1582], ["index.to.to", "layer.weight.index_select().clone().detach", "list", "len", "torch.nn.Linear().to", "nn.Linear().to.weight.copy_", "layer.weight.size", "layer.weight.index_select().clone().detach.contiguous", "nn.Linear().to.bias.copy_", "layer.weight.index_select().clone", "layer.bias.clone().detach", "layer.bias[].clone().detach", "torch.nn.Linear", "layer.bias[].clone().detach.contiguous", "layer.weight.index_select", "layer.bias.clone", "layer.bias[].clone"], "function", ["None"], ["", "def", "prune_linear_layer", "(", "layer", ",", "index", ",", "dim", "=", "0", ")", ":", "\n", "    ", "\"\"\" Prune a linear layer (a model parameters) to keep only entries in index.\n        Return the pruned layer as a new layer with requires_grad=True.\n        Used to remove heads.\n    \"\"\"", "\n", "index", "=", "index", ".", "to", "(", "layer", ".", "weight", ".", "device", ")", "\n", "W", "=", "layer", ".", "weight", ".", "index_select", "(", "dim", ",", "index", ")", ".", "clone", "(", ")", ".", "detach", "(", ")", "\n", "if", "layer", ".", "bias", "is", "not", "None", ":", "\n", "        ", "if", "dim", "==", "1", ":", "\n", "            ", "b", "=", "layer", ".", "bias", ".", "clone", "(", ")", ".", "detach", "(", ")", "\n", "", "else", ":", "\n", "            ", "b", "=", "layer", ".", "bias", "[", "index", "]", ".", "clone", "(", ")", ".", "detach", "(", ")", "\n", "", "", "new_size", "=", "list", "(", "layer", ".", "weight", ".", "size", "(", ")", ")", "\n", "new_size", "[", "dim", "]", "=", "len", "(", "index", ")", "\n", "new_layer", "=", "nn", ".", "Linear", "(", "new_size", "[", "1", "]", ",", "new_size", "[", "0", "]", ",", "bias", "=", "layer", ".", "bias", "is", "not", "None", ")", ".", "to", "(", "layer", ".", "weight", ".", "device", ")", "\n", "new_layer", ".", "weight", ".", "requires_grad", "=", "False", "\n", "new_layer", ".", "weight", ".", "copy_", "(", "W", ".", "contiguous", "(", ")", ")", "\n", "new_layer", ".", "weight", ".", "requires_grad", "=", "True", "\n", "if", "layer", ".", "bias", "is", "not", "None", ":", "\n", "        ", "new_layer", ".", "bias", ".", "requires_grad", "=", "False", "\n", "new_layer", ".", "bias", ".", "copy_", "(", "b", ".", "contiguous", "(", ")", ")", "\n", "new_layer", ".", "bias", ".", "requires_grad", "=", "True", "\n", "", "return", "new_layer", "\n", "\n"]], "home.repos.pwc.inspect_result.neulab_awesome-align.awesome_align.modeling_utils.prune_conv1d_layer": [[1584, 1606], ["index.to.to", "layer.weight.index_select().clone().detach", "list", "len", "Conv1D().to", "Conv1D().to.weight.copy_", "Conv1D().to.bias.copy_", "layer.bias.clone().detach", "layer.bias[].clone().detach", "layer.weight.size", "layer.weight.index_select().clone().detach.contiguous", "layer.bias[].clone().detach.contiguous", "layer.weight.index_select().clone", "modeling_utils.Conv1D", "layer.bias.clone", "layer.bias[].clone", "layer.weight.index_select"], "function", ["None"], ["", "def", "prune_conv1d_layer", "(", "layer", ",", "index", ",", "dim", "=", "1", ")", ":", "\n", "    ", "\"\"\" Prune a Conv1D layer (a model parameters) to keep only entries in index.\n        A Conv1D work as a Linear layer (see e.g. BERT) but the weights are transposed.\n        Return the pruned layer as a new layer with requires_grad=True.\n        Used to remove heads.\n    \"\"\"", "\n", "index", "=", "index", ".", "to", "(", "layer", ".", "weight", ".", "device", ")", "\n", "W", "=", "layer", ".", "weight", ".", "index_select", "(", "dim", ",", "index", ")", ".", "clone", "(", ")", ".", "detach", "(", ")", "\n", "if", "dim", "==", "0", ":", "\n", "        ", "b", "=", "layer", ".", "bias", ".", "clone", "(", ")", ".", "detach", "(", ")", "\n", "", "else", ":", "\n", "        ", "b", "=", "layer", ".", "bias", "[", "index", "]", ".", "clone", "(", ")", ".", "detach", "(", ")", "\n", "", "new_size", "=", "list", "(", "layer", ".", "weight", ".", "size", "(", ")", ")", "\n", "new_size", "[", "dim", "]", "=", "len", "(", "index", ")", "\n", "new_layer", "=", "Conv1D", "(", "new_size", "[", "1", "]", ",", "new_size", "[", "0", "]", ")", ".", "to", "(", "layer", ".", "weight", ".", "device", ")", "\n", "new_layer", ".", "weight", ".", "requires_grad", "=", "False", "\n", "new_layer", ".", "weight", ".", "copy_", "(", "W", ".", "contiguous", "(", ")", ")", "\n", "new_layer", ".", "weight", ".", "requires_grad", "=", "True", "\n", "new_layer", ".", "bias", ".", "requires_grad", "=", "False", "\n", "new_layer", ".", "bias", ".", "copy_", "(", "b", ".", "contiguous", "(", ")", ")", "\n", "new_layer", ".", "bias", ".", "requires_grad", "=", "True", "\n", "return", "new_layer", "\n", "\n"]], "home.repos.pwc.inspect_result.neulab_awesome-align.awesome_align.modeling_utils.prune_layer": [[1608, 1619], ["isinstance", "modeling_utils.prune_linear_layer", "isinstance", "modeling_utils.prune_conv1d_layer", "ValueError"], "function", ["home.repos.pwc.inspect_result.neulab_awesome-align.awesome_align.modeling_utils.prune_linear_layer", "home.repos.pwc.inspect_result.neulab_awesome-align.awesome_align.modeling_utils.prune_conv1d_layer"], ["", "def", "prune_layer", "(", "layer", ",", "index", ",", "dim", "=", "None", ")", ":", "\n", "    ", "\"\"\" Prune a Conv1D or nn.Linear layer (a model parameters) to keep only entries in index.\n        Return the pruned layer as a new layer with requires_grad=True.\n        Used to remove heads.\n    \"\"\"", "\n", "if", "isinstance", "(", "layer", ",", "nn", ".", "Linear", ")", ":", "\n", "        ", "return", "prune_linear_layer", "(", "layer", ",", "index", ",", "dim", "=", "0", "if", "dim", "is", "None", "else", "dim", ")", "\n", "", "elif", "isinstance", "(", "layer", ",", "Conv1D", ")", ":", "\n", "        ", "return", "prune_conv1d_layer", "(", "layer", ",", "index", ",", "dim", "=", "1", "if", "dim", "is", "None", "else", "dim", ")", "\n", "", "else", ":", "\n", "        ", "raise", "ValueError", "(", "\"Can't prune layer of class {}\"", ".", "format", "(", "layer", ".", "__class__", ")", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.neulab_awesome-align.awesome_align.configuration_utils.PretrainedConfig.__init__": [[58, 99], ["kwargs.pop", "kwargs.pop", "kwargs.pop", "kwargs.pop", "kwargs.pop", "kwargs.pop", "kwargs.pop", "kwargs.pop", "kwargs.pop", "kwargs.pop", "kwargs.pop", "kwargs.pop", "kwargs.pop", "kwargs.pop", "kwargs.pop", "kwargs.pop", "kwargs.pop", "kwargs.pop", "kwargs.pop", "kwargs.pop", "kwargs.pop", "kwargs.pop", "dict", "kwargs.pop", "dict", "kwargs.items", "dict", "zip", "setattr", "range", "int", "configuration_utils.PretrainedConfig.id2label.items", "configuration_utils.PretrainedConfig.id2label.values", "configuration_utils.PretrainedConfig.id2label.keys", "int", "configuration_utils.PretrainedConfig.label2id.items", "logger.error"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "**", "kwargs", ")", ":", "\n", "# Attributes with defaults", "\n", "        ", "self", ".", "output_attentions", "=", "kwargs", ".", "pop", "(", "\"output_attentions\"", ",", "False", ")", "\n", "self", ".", "output_hidden_states", "=", "kwargs", ".", "pop", "(", "\"output_hidden_states\"", ",", "False", ")", "\n", "self", ".", "output_past", "=", "kwargs", ".", "pop", "(", "\"output_past\"", ",", "True", ")", "# Not used by all models", "\n", "self", ".", "torchscript", "=", "kwargs", ".", "pop", "(", "\"torchscript\"", ",", "False", ")", "# Only used by PyTorch models", "\n", "self", ".", "use_bfloat16", "=", "kwargs", ".", "pop", "(", "\"use_bfloat16\"", ",", "False", ")", "\n", "\n", "# Is decoder is used in encoder-decoder models to differentiate encoder from decoder", "\n", "self", ".", "is_decoder", "=", "kwargs", ".", "pop", "(", "\"is_decoder\"", ",", "False", ")", "\n", "\n", "# Parameters for sequence generation", "\n", "self", ".", "max_length", "=", "kwargs", ".", "pop", "(", "\"max_length\"", ",", "20", ")", "\n", "self", ".", "do_sample", "=", "kwargs", ".", "pop", "(", "\"do_sample\"", ",", "False", ")", "\n", "self", ".", "num_beams", "=", "kwargs", ".", "pop", "(", "\"num_beams\"", ",", "1", ")", "\n", "self", ".", "temperature", "=", "kwargs", ".", "pop", "(", "\"temperature\"", ",", "1.0", ")", "\n", "self", ".", "top_k", "=", "kwargs", ".", "pop", "(", "\"top_k\"", ",", "50", ")", "\n", "self", ".", "top_p", "=", "kwargs", ".", "pop", "(", "\"top_p\"", ",", "1.0", ")", "\n", "self", ".", "repetition_penalty", "=", "kwargs", ".", "pop", "(", "\"repetition_penalty\"", ",", "1.0", ")", "\n", "self", ".", "bos_token_id", "=", "kwargs", ".", "pop", "(", "\"bos_token_id\"", ",", "None", ")", "\n", "self", ".", "pad_token_id", "=", "kwargs", ".", "pop", "(", "\"pad_token_id\"", ",", "None", ")", "\n", "self", ".", "eos_token_ids", "=", "kwargs", ".", "pop", "(", "\"eos_token_ids\"", ",", "None", ")", "\n", "self", ".", "length_penalty", "=", "kwargs", ".", "pop", "(", "\"length_penalty\"", ",", "1.0", ")", "\n", "self", ".", "num_return_sequences", "=", "kwargs", ".", "pop", "(", "\"num_return_sequences\"", ",", "1", ")", "\n", "\n", "# Fine-tuning task arguments", "\n", "self", ".", "architectures", "=", "kwargs", ".", "pop", "(", "\"architectures\"", ",", "None", ")", "\n", "self", ".", "finetuning_task", "=", "kwargs", ".", "pop", "(", "\"finetuning_task\"", ",", "None", ")", "\n", "self", ".", "num_labels", "=", "kwargs", ".", "pop", "(", "\"num_labels\"", ",", "2", ")", "\n", "self", ".", "id2label", "=", "kwargs", ".", "pop", "(", "\"id2label\"", ",", "{", "i", ":", "\"LABEL_{}\"", ".", "format", "(", "i", ")", "for", "i", "in", "range", "(", "self", ".", "num_labels", ")", "}", ")", "\n", "self", ".", "id2label", "=", "dict", "(", "(", "int", "(", "key", ")", ",", "value", ")", "for", "key", ",", "value", "in", "self", ".", "id2label", ".", "items", "(", ")", ")", "\n", "self", ".", "label2id", "=", "kwargs", ".", "pop", "(", "\"label2id\"", ",", "dict", "(", "zip", "(", "self", ".", "id2label", ".", "values", "(", ")", ",", "self", ".", "id2label", ".", "keys", "(", ")", ")", ")", ")", "\n", "self", ".", "label2id", "=", "dict", "(", "(", "key", ",", "int", "(", "value", ")", ")", "for", "key", ",", "value", "in", "self", ".", "label2id", ".", "items", "(", ")", ")", "\n", "\n", "# Additional attributes without default values", "\n", "for", "key", ",", "value", "in", "kwargs", ".", "items", "(", ")", ":", "\n", "            ", "try", ":", "\n", "                ", "setattr", "(", "self", ",", "key", ",", "value", ")", "\n", "", "except", "AttributeError", "as", "err", ":", "\n", "                ", "logger", ".", "error", "(", "\"Can't set {} with value {} for {}\"", ".", "format", "(", "key", ",", "value", ",", "self", ")", ")", "\n", "raise", "err", "\n", "\n"]], "home.repos.pwc.inspect_result.neulab_awesome-align.awesome_align.configuration_utils.PretrainedConfig.save_pretrained": [[100, 118], ["os.path.isdir", "os.path.join", "configuration_utils.PretrainedConfig.to_json_file", "logger.info"], "methods", ["home.repos.pwc.inspect_result.neulab_awesome-align.awesome_align.configuration_utils.PretrainedConfig.to_json_file"], ["", "", "", "def", "save_pretrained", "(", "self", ",", "save_directory", ")", ":", "\n", "        ", "\"\"\"\n        Save a configuration object to the directory `save_directory`, so that it\n        can be re-loaded using the :func:`~transformers.PretrainedConfig.from_pretrained` class method.\n\n        Args:\n            save_directory (:obj:`string`):\n                Directory where the configuration JSON file will be saved.\n        \"\"\"", "\n", "assert", "os", ".", "path", ".", "isdir", "(", "\n", "save_directory", "\n", ")", ",", "\"Saving path should be a directory where the model and configuration can be saved\"", "\n", "\n", "# If we save using the predefined names, we can load using `from_pretrained`", "\n", "output_config_file", "=", "os", ".", "path", ".", "join", "(", "save_directory", ",", "CONFIG_NAME", ")", "\n", "\n", "self", ".", "to_json_file", "(", "output_config_file", ")", "\n", "logger", ".", "info", "(", "\"Configuration saved in {}\"", ".", "format", "(", "output_config_file", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.neulab_awesome-align.awesome_align.configuration_utils.PretrainedConfig.from_pretrained": [[119, 177], ["cls.get_config_dict", "cls.from_dict"], "methods", ["home.repos.pwc.inspect_result.neulab_awesome-align.awesome_align.configuration_utils.PretrainedConfig.get_config_dict", "home.repos.pwc.inspect_result.neulab_awesome-align.awesome_align.configuration_utils.PretrainedConfig.from_dict"], ["", "@", "classmethod", "\n", "def", "from_pretrained", "(", "cls", ",", "pretrained_model_name_or_path", ",", "**", "kwargs", ")", "->", "\"PretrainedConfig\"", ":", "\n", "        ", "r\"\"\"\n\n        Instantiate a :class:`~transformers.PretrainedConfig` (or a derived class) from a pre-trained model configuration.\n\n        Args:\n            pretrained_model_name_or_path (:obj:`string`):\n                either:\n                  - a string with the `shortcut name` of a pre-trained model configuration to load from cache or\n                    download, e.g.: ``bert-base-uncased``.\n                  - a string with the `identifier name` of a pre-trained model configuration that was user-uploaded to\n                    our S3, e.g.: ``dbmdz/bert-base-german-cased``.\n                  - a path to a `directory` containing a configuration file saved using the\n                    :func:`~transformers.PretrainedConfig.save_pretrained` method, e.g.: ``./my_model_directory/``.\n                  - a path or url to a saved configuration JSON `file`, e.g.:\n                    ``./my_model_directory/configuration.json``.\n            cache_dir (:obj:`string`, `optional`):\n                Path to a directory in which a downloaded pre-trained model\n                configuration should be cached if the standard cache should not be used.\n            kwargs (:obj:`Dict[str, any]`, `optional`):\n                The values in kwargs of any keys which are configuration attributes will be used to override the loaded\n                values. Behavior concerning key/value pairs whose keys are *not* configuration attributes is\n                controlled by the `return_unused_kwargs` keyword parameter.\n            force_download (:obj:`bool`, `optional`, defaults to :obj:`False`):\n                Force to (re-)download the model weights and configuration files and override the cached versions if they exist.\n            resume_download (:obj:`bool`, `optional`, defaults to :obj:`False`):\n                Do not delete incompletely recieved file. Attempt to resume the download if such a file exists.\n            proxies (:obj:`Dict`, `optional`):\n                A dictionary of proxy servers to use by protocol or endpoint, e.g.:\n                :obj:`{'http': 'foo.bar:3128', 'http://hostname': 'foo.bar:4012'}.`\n                The proxies are used on each request.\n            return_unused_kwargs: (`optional`) bool:\n                If False, then this function returns just the final configuration object.\n                If True, then this functions returns a :obj:`Tuple(config, unused_kwargs)` where `unused_kwargs` is a\n                dictionary consisting of the key/value pairs whose keys are not configuration attributes: ie the part\n                of kwargs which has not been used to update `config` and is otherwise ignored.\n\n        Returns:\n            :class:`PretrainedConfig`: An instance of a configuration object\n\n        Examples::\n\n            # We can't instantiate directly the base class `PretrainedConfig` so let's show the examples on a\n            # derived class: BertConfig\n            config = BertConfig.from_pretrained('bert-base-uncased')    # Download configuration from S3 and cache.\n            config = BertConfig.from_pretrained('./test/saved_model/')  # E.g. config (or model) was saved using `save_pretrained('./test/saved_model/')`\n            config = BertConfig.from_pretrained('./test/saved_model/my_configuration.json')\n            config = BertConfig.from_pretrained('bert-base-uncased', output_attention=True, foo=False)\n            assert config.output_attention == True\n            config, unused_kwargs = BertConfig.from_pretrained('bert-base-uncased', output_attention=True,\n                                                               foo=False, return_unused_kwargs=True)\n            assert config.output_attention == True\n            assert unused_kwargs == {'foo': False}\n\n        \"\"\"", "\n", "config_dict", ",", "kwargs", "=", "cls", ".", "get_config_dict", "(", "pretrained_model_name_or_path", ",", "**", "kwargs", ")", "\n", "return", "cls", ".", "from_dict", "(", "config_dict", ",", "**", "kwargs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.neulab_awesome-align.awesome_align.configuration_utils.PretrainedConfig.get_config_dict": [[178, 258], ["kwargs.pop", "kwargs.pop", "kwargs.pop", "kwargs.pop", "kwargs.pop", "os.path.isdir", "awesome_align.file_utils.cached_path", "cls._dict_from_json_file", "logger.info", "logger.info", "os.path.join", "EnvironmentError", "EnvironmentError", "os.path.isfile", "awesome_align.file_utils.is_remote_url", "awesome_align.file_utils.hf_bucket_url"], "methods", ["home.repos.pwc.inspect_result.neulab_awesome-align.awesome_align.file_utils.cached_path", "home.repos.pwc.inspect_result.neulab_awesome-align.awesome_align.configuration_utils.PretrainedConfig._dict_from_json_file", "home.repos.pwc.inspect_result.neulab_awesome-align.awesome_align.file_utils.is_remote_url", "home.repos.pwc.inspect_result.neulab_awesome-align.awesome_align.file_utils.hf_bucket_url"], ["", "@", "classmethod", "\n", "def", "get_config_dict", "(", "\n", "cls", ",", "pretrained_model_name_or_path", ":", "str", ",", "pretrained_config_archive_map", ":", "Optional", "[", "Dict", "]", "=", "None", ",", "**", "kwargs", "\n", ")", "->", "Tuple", "[", "Dict", ",", "Dict", "]", ":", "\n", "        ", "\"\"\"\n        From a `pretrained_model_name_or_path`, resolve to a dictionary of parameters, to be used\n        for instantiating a Config using `from_dict`.\n\n        Parameters:\n            pretrained_model_name_or_path (:obj:`string`):\n                The identifier of the pre-trained checkpoint from which we want the dictionary of parameters.\n            pretrained_config_archive_map: (:obj:`Dict[str, str]`, `optional`) Dict:\n                A map of `shortcut names` to `url`. By default, will use the current class attribute.\n\n        Returns:\n            :obj:`Tuple[Dict, Dict]`: The dictionary that will be used to instantiate the configuration object.\n\n        \"\"\"", "\n", "cache_dir", "=", "kwargs", ".", "pop", "(", "\"cache_dir\"", ",", "None", ")", "\n", "force_download", "=", "kwargs", ".", "pop", "(", "\"force_download\"", ",", "False", ")", "\n", "resume_download", "=", "kwargs", ".", "pop", "(", "\"resume_download\"", ",", "False", ")", "\n", "proxies", "=", "kwargs", ".", "pop", "(", "\"proxies\"", ",", "None", ")", "\n", "local_files_only", "=", "kwargs", ".", "pop", "(", "\"local_files_only\"", ",", "False", ")", "\n", "\n", "if", "pretrained_config_archive_map", "is", "None", ":", "\n", "            ", "pretrained_config_archive_map", "=", "cls", ".", "pretrained_config_archive_map", "\n", "\n", "", "if", "pretrained_model_name_or_path", "in", "pretrained_config_archive_map", ":", "\n", "            ", "config_file", "=", "pretrained_config_archive_map", "[", "pretrained_model_name_or_path", "]", "\n", "", "elif", "os", ".", "path", ".", "isdir", "(", "pretrained_model_name_or_path", ")", ":", "\n", "            ", "config_file", "=", "os", ".", "path", ".", "join", "(", "pretrained_model_name_or_path", ",", "CONFIG_NAME", ")", "\n", "", "elif", "os", ".", "path", ".", "isfile", "(", "pretrained_model_name_or_path", ")", "or", "is_remote_url", "(", "pretrained_model_name_or_path", ")", ":", "\n", "            ", "config_file", "=", "pretrained_model_name_or_path", "\n", "", "else", ":", "\n", "            ", "config_file", "=", "hf_bucket_url", "(", "pretrained_model_name_or_path", ",", "postfix", "=", "CONFIG_NAME", ")", "\n", "\n", "", "try", ":", "\n", "# Load from URL or cache if already cached", "\n", "            ", "resolved_config_file", "=", "cached_path", "(", "\n", "config_file", ",", "\n", "cache_dir", "=", "cache_dir", ",", "\n", "force_download", "=", "force_download", ",", "\n", "proxies", "=", "proxies", ",", "\n", "resume_download", "=", "resume_download", ",", "\n", "local_files_only", "=", "local_files_only", ",", "\n", ")", "\n", "# Load config dict", "\n", "if", "resolved_config_file", "is", "None", ":", "\n", "                ", "raise", "EnvironmentError", "\n", "", "config_dict", "=", "cls", ".", "_dict_from_json_file", "(", "resolved_config_file", ")", "\n", "\n", "", "except", "EnvironmentError", ":", "\n", "            ", "if", "pretrained_model_name_or_path", "in", "pretrained_config_archive_map", ":", "\n", "                ", "msg", "=", "\"Couldn't reach server at '{}' to download pretrained model configuration file.\"", ".", "format", "(", "\n", "config_file", "\n", ")", "\n", "", "else", ":", "\n", "                ", "msg", "=", "(", "\n", "\"Model name '{}' was not found in model name list. \"", "\n", "\"We assumed '{}' was a path, a model identifier, or url to a configuration file named {} or \"", "\n", "\"a directory containing such a file but couldn't find any such file at this path or url.\"", ".", "format", "(", "\n", "pretrained_model_name_or_path", ",", "config_file", ",", "CONFIG_NAME", ",", "\n", ")", "\n", ")", "\n", "", "raise", "EnvironmentError", "(", "msg", ")", "\n", "\n", "", "except", "json", ".", "JSONDecodeError", ":", "\n", "            ", "msg", "=", "(", "\n", "\"Couldn't reach server at '{}' to download configuration file or \"", "\n", "\"configuration file is not a valid JSON file. \"", "\n", "\"Please check network or file content here: {}.\"", ".", "format", "(", "config_file", ",", "resolved_config_file", ")", "\n", ")", "\n", "raise", "EnvironmentError", "(", "msg", ")", "\n", "\n", "", "if", "resolved_config_file", "==", "config_file", ":", "\n", "            ", "logger", ".", "info", "(", "\"loading configuration file {}\"", ".", "format", "(", "config_file", ")", ")", "\n", "", "else", ":", "\n", "            ", "logger", ".", "info", "(", "\"loading configuration file {} from cache at {}\"", ".", "format", "(", "config_file", ",", "resolved_config_file", ")", ")", "\n", "\n", "", "return", "config_dict", ",", "kwargs", "\n", "\n"]], "home.repos.pwc.inspect_result.neulab_awesome-align.awesome_align.configuration_utils.PretrainedConfig.from_dict": [[259, 293], ["kwargs.pop", "cls", "kwargs.items", "logger.info", "hasattr", "kwargs.pop", "str", "setattr", "to_remove.append"], "methods", ["None"], ["", "@", "classmethod", "\n", "def", "from_dict", "(", "cls", ",", "config_dict", ":", "Dict", ",", "**", "kwargs", ")", "->", "\"PretrainedConfig\"", ":", "\n", "        ", "\"\"\"\n        Constructs a `Config` from a Python dictionary of parameters.\n\n        Args:\n            config_dict (:obj:`Dict[str, any]`):\n                Dictionary that will be used to instantiate the configuration object. Such a dictionary can be retrieved\n                from a pre-trained checkpoint by leveraging the :func:`~transformers.PretrainedConfig.get_config_dict`\n                method.\n            kwargs (:obj:`Dict[str, any]`):\n                Additional parameters from which to initialize the configuration object.\n\n        Returns:\n            :class:`PretrainedConfig`: An instance of a configuration object\n        \"\"\"", "\n", "return_unused_kwargs", "=", "kwargs", ".", "pop", "(", "\"return_unused_kwargs\"", ",", "False", ")", "\n", "\n", "config", "=", "cls", "(", "**", "config_dict", ")", "\n", "\n", "# Update config with kwargs if needed", "\n", "to_remove", "=", "[", "]", "\n", "for", "key", ",", "value", "in", "kwargs", ".", "items", "(", ")", ":", "\n", "            ", "if", "hasattr", "(", "config", ",", "key", ")", ":", "\n", "                ", "setattr", "(", "config", ",", "key", ",", "value", ")", "\n", "to_remove", ".", "append", "(", "key", ")", "\n", "", "", "for", "key", "in", "to_remove", ":", "\n", "            ", "kwargs", ".", "pop", "(", "key", ",", "None", ")", "\n", "\n", "", "logger", ".", "info", "(", "\"Model config %s\"", ",", "str", "(", "config", ")", ")", "\n", "if", "return_unused_kwargs", ":", "\n", "            ", "return", "config", ",", "kwargs", "\n", "", "else", ":", "\n", "            ", "return", "config", "\n", "\n"]], "home.repos.pwc.inspect_result.neulab_awesome-align.awesome_align.configuration_utils.PretrainedConfig.from_json_file": [[294, 309], ["cls._dict_from_json_file", "cls"], "methods", ["home.repos.pwc.inspect_result.neulab_awesome-align.awesome_align.configuration_utils.PretrainedConfig._dict_from_json_file"], ["", "", "@", "classmethod", "\n", "def", "from_json_file", "(", "cls", ",", "json_file", ":", "str", ")", "->", "\"PretrainedConfig\"", ":", "\n", "        ", "\"\"\"\n        Constructs a `Config` from the path to a json file of parameters.\n\n        Args:\n            json_file (:obj:`string`):\n                Path to the JSON file containing the parameters.\n\n        Returns:\n            :class:`PretrainedConfig`: An instance of a configuration object\n\n        \"\"\"", "\n", "config_dict", "=", "cls", ".", "_dict_from_json_file", "(", "json_file", ")", "\n", "return", "cls", "(", "**", "config_dict", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.neulab_awesome-align.awesome_align.configuration_utils.PretrainedConfig._dict_from_json_file": [[310, 315], ["json.loads", "open", "reader.read"], "methods", ["None"], ["", "@", "classmethod", "\n", "def", "_dict_from_json_file", "(", "cls", ",", "json_file", ":", "str", ")", ":", "\n", "        ", "with", "open", "(", "json_file", ",", "\"r\"", ",", "encoding", "=", "\"utf-8\"", ")", "as", "reader", ":", "\n", "            ", "text", "=", "reader", ".", "read", "(", ")", "\n", "", "return", "json", ".", "loads", "(", "text", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.neulab_awesome-align.awesome_align.configuration_utils.PretrainedConfig.__eq__": [[316, 318], ["None"], "methods", ["None"], ["", "def", "__eq__", "(", "self", ",", "other", ")", ":", "\n", "        ", "return", "self", ".", "__dict__", "==", "other", ".", "__dict__", "\n", "\n"]], "home.repos.pwc.inspect_result.neulab_awesome-align.awesome_align.configuration_utils.PretrainedConfig.__repr__": [[319, 321], ["configuration_utils.PretrainedConfig.to_json_string"], "methods", ["home.repos.pwc.inspect_result.neulab_awesome-align.awesome_align.configuration_utils.PretrainedConfig.to_json_string"], ["", "def", "__repr__", "(", "self", ")", ":", "\n", "        ", "return", "\"{} {}\"", ".", "format", "(", "self", ".", "__class__", ".", "__name__", ",", "self", ".", "to_json_string", "(", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.neulab_awesome-align.awesome_align.configuration_utils.PretrainedConfig.to_dict": [[322, 333], ["copy.deepcopy", "hasattr"], "methods", ["None"], ["", "def", "to_dict", "(", "self", ")", ":", "\n", "        ", "\"\"\"\n        Serializes this instance to a Python dictionary.\n\n        Returns:\n            :obj:`Dict[str, any]`: Dictionary of all the attributes that make up this configuration instance,\n        \"\"\"", "\n", "output", "=", "copy", ".", "deepcopy", "(", "self", ".", "__dict__", ")", "\n", "if", "hasattr", "(", "self", ".", "__class__", ",", "\"model_type\"", ")", ":", "\n", "            ", "output", "[", "\"model_type\"", "]", "=", "self", ".", "__class__", ".", "model_type", "\n", "", "return", "output", "\n", "\n"]], "home.repos.pwc.inspect_result.neulab_awesome-align.awesome_align.configuration_utils.PretrainedConfig.to_json_string": [[334, 342], ["json.dumps", "configuration_utils.PretrainedConfig.to_dict"], "methods", ["home.repos.pwc.inspect_result.neulab_awesome-align.awesome_align.configuration_utils.PretrainedConfig.to_dict"], ["", "def", "to_json_string", "(", "self", ")", ":", "\n", "        ", "\"\"\"\n        Serializes this instance to a JSON string.\n\n        Returns:\n            :obj:`string`: String containing all the attributes that make up this configuration instance in JSON format.\n        \"\"\"", "\n", "return", "json", ".", "dumps", "(", "self", ".", "to_dict", "(", ")", ",", "indent", "=", "2", ",", "sort_keys", "=", "True", ")", "+", "\"\\n\"", "\n", "\n"]], "home.repos.pwc.inspect_result.neulab_awesome-align.awesome_align.configuration_utils.PretrainedConfig.to_json_file": [[343, 353], ["open", "writer.write", "configuration_utils.PretrainedConfig.to_json_string"], "methods", ["home.repos.pwc.inspect_result.neulab_awesome-align.awesome_align.configuration_utils.PretrainedConfig.to_json_string"], ["", "def", "to_json_file", "(", "self", ",", "json_file_path", ")", ":", "\n", "        ", "\"\"\"\n        Save this instance to a json file.\n\n        Args:\n            json_file_path (:obj:`string`):\n                Path to the JSON file in which this configuration instance's parameters will be saved.\n        \"\"\"", "\n", "with", "open", "(", "json_file_path", ",", "\"w\"", ",", "encoding", "=", "\"utf-8\"", ")", "as", "writer", ":", "\n", "            ", "writer", ".", "write", "(", "self", ".", "to_json_string", "(", ")", ")", "\n", "", "", "", ""]], "home.repos.pwc.inspect_result.neulab_awesome-align.awesome_align.train_utils.AdamW.__init__": [[148, 159], ["dict", "torch.optim.Optimizer.__init__", "ValueError", "ValueError", "ValueError", "ValueError"], "methods", ["home.repos.pwc.inspect_result.neulab_awesome-align.awesome_align.train_utils.AdamW.__init__"], ["def", "__init__", "(", "self", ",", "params", ",", "lr", "=", "1e-3", ",", "betas", "=", "(", "0.9", ",", "0.999", ")", ",", "eps", "=", "1e-6", ",", "weight_decay", "=", "0.0", ",", "correct_bias", "=", "True", ")", ":", "\n", "        ", "if", "lr", "<", "0.0", ":", "\n", "            ", "raise", "ValueError", "(", "\"Invalid learning rate: {} - should be >= 0.0\"", ".", "format", "(", "lr", ")", ")", "\n", "", "if", "not", "0.0", "<=", "betas", "[", "0", "]", "<", "1.0", ":", "\n", "            ", "raise", "ValueError", "(", "\"Invalid beta parameter: {} - should be in [0.0, 1.0[\"", ".", "format", "(", "betas", "[", "0", "]", ")", ")", "\n", "", "if", "not", "0.0", "<=", "betas", "[", "1", "]", "<", "1.0", ":", "\n", "            ", "raise", "ValueError", "(", "\"Invalid beta parameter: {} - should be in [0.0, 1.0[\"", ".", "format", "(", "betas", "[", "1", "]", ")", ")", "\n", "", "if", "not", "0.0", "<=", "eps", ":", "\n", "            ", "raise", "ValueError", "(", "\"Invalid epsilon value: {} - should be >= 0.0\"", ".", "format", "(", "eps", ")", ")", "\n", "", "defaults", "=", "dict", "(", "lr", "=", "lr", ",", "betas", "=", "betas", ",", "eps", "=", "eps", ",", "weight_decay", "=", "weight_decay", ",", "correct_bias", "=", "correct_bias", ")", "\n", "super", "(", ")", ".", "__init__", "(", "params", ",", "defaults", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.neulab_awesome-align.awesome_align.train_utils.AdamW.step": [[160, 220], ["closure", "exp_avg.mul_().add_", "exp_avg_sq.mul_().addcmul_", "exp_avg_sq.sqrt().add_", "p.data.addcdiv_", "RuntimeError", "len", "torch.zeros_like", "torch.zeros_like", "p.data.add_", "exp_avg.mul_", "exp_avg_sq.mul_", "exp_avg_sq.sqrt", "math.sqrt"], "methods", ["None"], ["", "def", "step", "(", "self", ",", "closure", "=", "None", ")", ":", "\n", "        ", "\"\"\"Performs a single optimization step.\n\n        Arguments:\n            closure (callable, optional): A closure that reevaluates the model\n                and returns the loss.\n        \"\"\"", "\n", "loss", "=", "None", "\n", "if", "closure", "is", "not", "None", ":", "\n", "            ", "loss", "=", "closure", "(", ")", "\n", "\n", "", "for", "group", "in", "self", ".", "param_groups", ":", "\n", "            ", "for", "p", "in", "group", "[", "\"params\"", "]", ":", "\n", "                ", "if", "p", ".", "grad", "is", "None", ":", "\n", "                    ", "continue", "\n", "", "grad", "=", "p", ".", "grad", ".", "data", "\n", "if", "grad", ".", "is_sparse", ":", "\n", "                    ", "raise", "RuntimeError", "(", "\"Adam does not support sparse gradients, please consider SparseAdam instead\"", ")", "\n", "\n", "", "state", "=", "self", ".", "state", "[", "p", "]", "\n", "\n", "# State initialization", "\n", "if", "len", "(", "state", ")", "==", "0", ":", "\n", "                    ", "state", "[", "\"step\"", "]", "=", "0", "\n", "# Exponential moving average of gradient values", "\n", "state", "[", "\"exp_avg\"", "]", "=", "torch", ".", "zeros_like", "(", "p", ".", "data", ")", "\n", "# Exponential moving average of squared gradient values", "\n", "state", "[", "\"exp_avg_sq\"", "]", "=", "torch", ".", "zeros_like", "(", "p", ".", "data", ")", "\n", "\n", "", "exp_avg", ",", "exp_avg_sq", "=", "state", "[", "\"exp_avg\"", "]", ",", "state", "[", "\"exp_avg_sq\"", "]", "\n", "beta1", ",", "beta2", "=", "group", "[", "\"betas\"", "]", "\n", "\n", "state", "[", "\"step\"", "]", "+=", "1", "\n", "\n", "# Decay the first and second moment running average coefficient", "\n", "# In-place operations to update the averages at the same time", "\n", "exp_avg", ".", "mul_", "(", "beta1", ")", ".", "add_", "(", "alpha", "=", "1.0", "-", "beta1", ",", "other", "=", "grad", ")", "\n", "exp_avg_sq", ".", "mul_", "(", "beta2", ")", ".", "addcmul_", "(", "value", "=", "1.0", "-", "beta2", ",", "tensor1", "=", "grad", ",", "tensor2", "=", "grad", ")", "\n", "denom", "=", "exp_avg_sq", ".", "sqrt", "(", ")", ".", "add_", "(", "group", "[", "\"eps\"", "]", ")", "\n", "\n", "step_size", "=", "group", "[", "\"lr\"", "]", "\n", "if", "group", "[", "\"correct_bias\"", "]", ":", "# No bias correction for Bert", "\n", "                    ", "bias_correction1", "=", "1.0", "-", "beta1", "**", "state", "[", "\"step\"", "]", "\n", "bias_correction2", "=", "1.0", "-", "beta2", "**", "state", "[", "\"step\"", "]", "\n", "step_size", "=", "step_size", "*", "math", ".", "sqrt", "(", "bias_correction2", ")", "/", "bias_correction1", "\n", "\n", "", "p", ".", "data", ".", "addcdiv_", "(", "value", "=", "-", "step_size", ",", "tensor1", "=", "exp_avg", ",", "tensor2", "=", "denom", ")", "\n", "\n", "# Just adding the square of the weights to the loss function is *not*", "\n", "# the correct way of using L2 regularization/weight decay with Adam,", "\n", "# since that will interact with the m and v parameters in strange ways.", "\n", "#", "\n", "# Instead we want to decay the weights in a manner that doesn't interact", "\n", "# with the m/v parameters. This is equivalent to adding the square", "\n", "# of the weights to the loss with plain (non-momentum) SGD.", "\n", "# Add weight decay at the end (fixed version)", "\n", "if", "group", "[", "\"weight_decay\"", "]", ">", "0.0", ":", "\n", "                    ", "p", ".", "data", ".", "add_", "(", "-", "group", "[", "\"lr\"", "]", "*", "group", "[", "\"weight_decay\"", "]", ",", "p", ".", "data", ")", "\n", "\n", "", "", "", "return", "loss", "\n", "", "", ""]], "home.repos.pwc.inspect_result.neulab_awesome-align.awesome_align.train_utils._sorted_checkpoints": [[33, 49], ["glob.glob", "sorted", "os.path.join", "ordering_and_checkpoint_path.append", "re.match", "re.match.groups", "ordering_and_checkpoint_path.append", "os.path.getmtime", "int", "re.match.groups"], "function", ["None"], ["def", "_sorted_checkpoints", "(", "args", ",", "checkpoint_prefix", "=", "\"checkpoint\"", ",", "use_mtime", "=", "False", ")", "->", "List", "[", "str", "]", ":", "\n", "    ", "ordering_and_checkpoint_path", "=", "[", "]", "\n", "\n", "glob_checkpoints", "=", "glob", ".", "glob", "(", "os", ".", "path", ".", "join", "(", "args", ".", "output_dir", ",", "\"{}-*\"", ".", "format", "(", "checkpoint_prefix", ")", ")", ")", "\n", "\n", "for", "path", "in", "glob_checkpoints", ":", "\n", "        ", "if", "use_mtime", ":", "\n", "            ", "ordering_and_checkpoint_path", ".", "append", "(", "(", "os", ".", "path", ".", "getmtime", "(", "path", ")", ",", "path", ")", ")", "\n", "", "else", ":", "\n", "            ", "regex_match", "=", "re", ".", "match", "(", "\".*{}-([0-9]+)\"", ".", "format", "(", "checkpoint_prefix", ")", ",", "path", ")", "\n", "if", "regex_match", "and", "regex_match", ".", "groups", "(", ")", ":", "\n", "                ", "ordering_and_checkpoint_path", ".", "append", "(", "(", "int", "(", "regex_match", ".", "groups", "(", ")", "[", "0", "]", ")", ",", "path", ")", ")", "\n", "\n", "", "", "", "checkpoints_sorted", "=", "sorted", "(", "ordering_and_checkpoint_path", ")", "\n", "checkpoints_sorted", "=", "[", "checkpoint", "[", "1", "]", "for", "checkpoint", "in", "checkpoints_sorted", "]", "\n", "return", "checkpoints_sorted", "\n", "\n"]], "home.repos.pwc.inspect_result.neulab_awesome-align.awesome_align.train_utils._rotate_checkpoints": [[50, 66], ["train_utils._sorted_checkpoints", "max", "len", "logger.info", "shutil.rmtree", "len"], "function", ["home.repos.pwc.inspect_result.neulab_awesome-align.awesome_align.train_utils._sorted_checkpoints"], ["", "def", "_rotate_checkpoints", "(", "args", ",", "checkpoint_prefix", "=", "\"checkpoint\"", ",", "use_mtime", "=", "False", ")", "->", "None", ":", "\n", "    ", "if", "not", "args", ".", "save_total_limit", ":", "\n", "        ", "return", "\n", "", "if", "args", ".", "save_total_limit", "<=", "0", ":", "\n", "        ", "return", "\n", "\n", "# Check if we should delete older checkpoint(s)", "\n", "", "checkpoints_sorted", "=", "_sorted_checkpoints", "(", "args", ",", "checkpoint_prefix", ",", "use_mtime", ")", "\n", "if", "len", "(", "checkpoints_sorted", ")", "<=", "args", ".", "save_total_limit", ":", "\n", "        ", "return", "\n", "\n", "", "number_of_checkpoints_to_delete", "=", "max", "(", "0", ",", "len", "(", "checkpoints_sorted", ")", "-", "args", ".", "save_total_limit", ")", "\n", "checkpoints_to_be_deleted", "=", "checkpoints_sorted", "[", ":", "number_of_checkpoints_to_delete", "]", "\n", "for", "checkpoint", "in", "checkpoints_to_be_deleted", ":", "\n", "        ", "logger", ".", "info", "(", "\"Deleting older checkpoint [{}] due to args.save_total_limit\"", ".", "format", "(", "checkpoint", ")", ")", "\n", "shutil", ".", "rmtree", "(", "checkpoint", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.neulab_awesome-align.awesome_align.train_utils.get_constant_schedule": [[69, 73], ["torch.optim.lr_scheduler.LambdaLR"], "function", ["None"], ["", "", "def", "get_constant_schedule", "(", "optimizer", ",", "last_epoch", "=", "-", "1", ")", ":", "\n", "    ", "\"\"\" Create a schedule with a constant learning rate.\n    \"\"\"", "\n", "return", "LambdaLR", "(", "optimizer", ",", "lambda", "_", ":", "1", ",", "last_epoch", "=", "last_epoch", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.neulab_awesome-align.awesome_align.train_utils.get_constant_schedule_with_warmup": [[75, 86], ["torch.optim.lr_scheduler.LambdaLR", "float", "float", "max"], "function", ["None"], ["", "def", "get_constant_schedule_with_warmup", "(", "optimizer", ",", "num_warmup_steps", ",", "last_epoch", "=", "-", "1", ")", ":", "\n", "    ", "\"\"\" Create a schedule with a constant learning rate preceded by a warmup\n    period during which the learning rate increases linearly between 0 and 1.\n    \"\"\"", "\n", "\n", "def", "lr_lambda", "(", "current_step", ")", ":", "\n", "        ", "if", "current_step", "<", "num_warmup_steps", ":", "\n", "            ", "return", "float", "(", "current_step", ")", "/", "float", "(", "max", "(", "1.0", ",", "num_warmup_steps", ")", ")", "\n", "", "return", "1.0", "\n", "\n", "", "return", "LambdaLR", "(", "optimizer", ",", "lr_lambda", ",", "last_epoch", "=", "last_epoch", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.neulab_awesome-align.awesome_align.train_utils.get_linear_schedule_with_warmup": [[88, 101], ["torch.optim.lr_scheduler.LambdaLR", "max", "float", "float", "float", "float", "max", "max"], "function", ["None"], ["", "def", "get_linear_schedule_with_warmup", "(", "optimizer", ",", "num_warmup_steps", ",", "num_training_steps", ",", "last_epoch", "=", "-", "1", ")", ":", "\n", "    ", "\"\"\" Create a schedule with a learning rate that decreases linearly after\n    linearly increasing during a warmup period.\n    \"\"\"", "\n", "\n", "def", "lr_lambda", "(", "current_step", ")", ":", "\n", "        ", "if", "current_step", "<", "num_warmup_steps", ":", "\n", "            ", "return", "float", "(", "current_step", ")", "/", "float", "(", "max", "(", "1", ",", "num_warmup_steps", ")", ")", "\n", "", "return", "max", "(", "\n", "0.0", ",", "float", "(", "num_training_steps", "-", "current_step", ")", "/", "float", "(", "max", "(", "1", ",", "num_training_steps", "-", "num_warmup_steps", ")", ")", "\n", ")", "\n", "\n", "", "return", "LambdaLR", "(", "optimizer", ",", "lr_lambda", ",", "last_epoch", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.neulab_awesome-align.awesome_align.train_utils.get_cosine_schedule_with_warmup": [[103, 116], ["torch.optim.lr_scheduler.LambdaLR", "max", "float", "float", "float", "float", "max", "max", "math.cos", "float"], "function", ["None"], ["", "def", "get_cosine_schedule_with_warmup", "(", "optimizer", ",", "num_warmup_steps", ",", "num_training_steps", ",", "num_cycles", "=", "0.5", ",", "last_epoch", "=", "-", "1", ")", ":", "\n", "    ", "\"\"\" Create a schedule with a learning rate that decreases following the\n    values of the cosine function between 0 and `pi * cycles` after a warmup\n    period during which it increases linearly between 0 and 1.\n    \"\"\"", "\n", "\n", "def", "lr_lambda", "(", "current_step", ")", ":", "\n", "        ", "if", "current_step", "<", "num_warmup_steps", ":", "\n", "            ", "return", "float", "(", "current_step", ")", "/", "float", "(", "max", "(", "1", ",", "num_warmup_steps", ")", ")", "\n", "", "progress", "=", "float", "(", "current_step", "-", "num_warmup_steps", ")", "/", "float", "(", "max", "(", "1", ",", "num_training_steps", "-", "num_warmup_steps", ")", ")", "\n", "return", "max", "(", "0.0", ",", "0.5", "*", "(", "1.0", "+", "math", ".", "cos", "(", "math", ".", "pi", "*", "float", "(", "num_cycles", ")", "*", "2.0", "*", "progress", ")", ")", ")", "\n", "\n", "", "return", "LambdaLR", "(", "optimizer", ",", "lr_lambda", ",", "last_epoch", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.neulab_awesome-align.awesome_align.train_utils.get_cosine_with_hard_restarts_schedule_with_warmup": [[118, 135], ["torch.optim.lr_scheduler.LambdaLR", "max", "float", "float", "float", "float", "max", "max", "math.cos", "float"], "function", ["None"], ["", "def", "get_cosine_with_hard_restarts_schedule_with_warmup", "(", "\n", "optimizer", ",", "num_warmup_steps", ",", "num_training_steps", ",", "num_cycles", "=", "1.0", ",", "last_epoch", "=", "-", "1", "\n", ")", ":", "\n", "    ", "\"\"\" Create a schedule with a learning rate that decreases following the\n    values of the cosine function with several hard restarts, after a warmup\n    period during which it increases linearly between 0 and 1.\n    \"\"\"", "\n", "\n", "def", "lr_lambda", "(", "current_step", ")", ":", "\n", "        ", "if", "current_step", "<", "num_warmup_steps", ":", "\n", "            ", "return", "float", "(", "current_step", ")", "/", "float", "(", "max", "(", "1", ",", "num_warmup_steps", ")", ")", "\n", "", "progress", "=", "float", "(", "current_step", "-", "num_warmup_steps", ")", "/", "float", "(", "max", "(", "1", ",", "num_training_steps", "-", "num_warmup_steps", ")", ")", "\n", "if", "progress", ">=", "1.0", ":", "\n", "            ", "return", "0.0", "\n", "", "return", "max", "(", "0.0", ",", "0.5", "*", "(", "1.0", "+", "math", ".", "cos", "(", "math", ".", "pi", "*", "(", "(", "float", "(", "num_cycles", ")", "*", "progress", ")", "%", "1.0", ")", ")", ")", ")", "\n", "\n", "", "return", "LambdaLR", "(", "optimizer", ",", "lr_lambda", ",", "last_epoch", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.neulab_awesome-align.awesome_align.activations.swish": [[7, 9], ["torch.sigmoid", "torch.sigmoid"], "function", ["None"], ["def", "swish", "(", "x", ")", ":", "\n", "    ", "return", "x", "*", "torch", ".", "sigmoid", "(", "x", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.neulab_awesome-align.awesome_align.activations._gelu_python": [[11, 19], ["torch.erf", "torch.erf", "math.sqrt"], "function", ["None"], ["", "def", "_gelu_python", "(", "x", ")", ":", "\n", "    ", "\"\"\" Original Implementation of the gelu activation function in Google Bert repo when initially created.\n        For information: OpenAI GPT's gelu is slightly different (and gives slightly different results):\n        0.5 * x * (1 + torch.tanh(math.sqrt(2 / math.pi) * (x + 0.044715 * torch.pow(x, 3))))\n        This is now written in C in torch.nn.functional\n        Also see https://arxiv.org/abs/1606.08415\n    \"\"\"", "\n", "return", "x", "*", "0.5", "*", "(", "1.0", "+", "torch", ".", "erf", "(", "x", "/", "math", ".", "sqrt", "(", "2.0", ")", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.neulab_awesome-align.awesome_align.activations.gelu_new": [[27, 32], ["torch.tanh", "torch.tanh", "math.sqrt", "torch.pow", "torch.pow"], "function", ["None"], ["", "def", "gelu_new", "(", "x", ")", ":", "\n", "    ", "\"\"\" Implementation of the gelu activation function currently in Google Bert repo (identical to OpenAI GPT).\n        Also see https://arxiv.org/abs/1606.08415\n    \"\"\"", "\n", "return", "0.5", "*", "x", "*", "(", "1", "+", "torch", ".", "tanh", "(", "math", ".", "sqrt", "(", "2", "/", "math", ".", "pi", ")", "*", "(", "x", "+", "0.044715", "*", "torch", ".", "pow", "(", "x", ",", "3", ")", ")", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.neulab_awesome-align.awesome_align.activations.get_activation": [[43, 50], ["KeyError", "list", "ACT2FN.keys"], "function", ["None"], ["def", "get_activation", "(", "activation_string", ")", ":", "\n", "    ", "if", "activation_string", "in", "ACT2FN", ":", "\n", "        ", "return", "ACT2FN", "[", "activation_string", "]", "\n", "", "else", ":", "\n", "        ", "raise", "KeyError", "(", "\n", "\"function {} not found in ACT2FN mapping {} or torch.nn.functional\"", ".", "format", "(", "\n", "activation_string", ",", "list", "(", "ACT2FN", ".", "keys", "(", ")", ")", "\n", ")", "\n"]]}