{"home.repos.pwc.inspect_result.speech-lab-iitm_pada.None.train.pruning_bert": [[46, 82], ["range", "tuple", "torch.global_unstructured", "tuple.append", "tuple.append", "tuple.append", "tuple.append", "tuple.append", "tuple.append", "tuple.append", "tuple.append", "tuple.append", "tuple.append", "tuple.append", "tuple.append", "print"], "function", ["None"], ["def", "pruning_bert", "(", "model", ",", "px", ",", "model_type", "=", "'wav2vec_small'", ")", ":", "\n", "    ", "\"\"\"\n    prune out wav2vec 2.0 BERT: 12 transformer layers for BASE, and 24 \n                                transformer layers for LARGE\n\n    note: position encoding, projection heads, layernorm statistics are not pruned. \n    \"\"\"", "\n", "if", "model_type", "==", "'wav2vec_small'", ":", "\n", "        ", "num_transformer_blocks", "=", "12", "\n", "", "elif", "model_type", "==", "'libri960_big'", "or", "model_type", "==", "'xlsr_53_56k'", ":", "\n", "        ", "num_transformer_blocks", "=", "24", "\n", "", "else", ":", "\n", "        ", "print", "(", "'model type {} not supported'", ".", "format", "(", "model_type", ")", ")", "\n", "# print('num_transformer_blocks is', num_transformer_blocks)", "\n", "\n", "", "parameters_to_prune", "=", "[", "]", "\n", "for", "ii", "in", "range", "(", "num_transformer_blocks", ")", ":", "\n", "        ", "parameters_to_prune", ".", "append", "(", "(", "model", ".", "w2v_encoder", ".", "w2v_model", ".", "encoder", ".", "layers", "[", "ii", "]", ".", "self_attn", ".", "k_proj", ",", "'weight'", ")", ")", "\n", "parameters_to_prune", ".", "append", "(", "(", "model", ".", "w2v_encoder", ".", "w2v_model", ".", "encoder", ".", "layers", "[", "ii", "]", ".", "self_attn", ".", "k_proj", ",", "'bias'", ")", ")", "\n", "parameters_to_prune", ".", "append", "(", "(", "model", ".", "w2v_encoder", ".", "w2v_model", ".", "encoder", ".", "layers", "[", "ii", "]", ".", "self_attn", ".", "v_proj", ",", "'weight'", ")", ")", "\n", "parameters_to_prune", ".", "append", "(", "(", "model", ".", "w2v_encoder", ".", "w2v_model", ".", "encoder", ".", "layers", "[", "ii", "]", ".", "self_attn", ".", "v_proj", ",", "'bias'", ")", ")", "\n", "parameters_to_prune", ".", "append", "(", "(", "model", ".", "w2v_encoder", ".", "w2v_model", ".", "encoder", ".", "layers", "[", "ii", "]", ".", "self_attn", ".", "q_proj", ",", "'weight'", ")", ")", "\n", "parameters_to_prune", ".", "append", "(", "(", "model", ".", "w2v_encoder", ".", "w2v_model", ".", "encoder", ".", "layers", "[", "ii", "]", ".", "self_attn", ".", "q_proj", ",", "'bias'", ")", ")", "\n", "parameters_to_prune", ".", "append", "(", "(", "model", ".", "w2v_encoder", ".", "w2v_model", ".", "encoder", ".", "layers", "[", "ii", "]", ".", "self_attn", ".", "out_proj", ",", "'weight'", ")", ")", "\n", "parameters_to_prune", ".", "append", "(", "(", "model", ".", "w2v_encoder", ".", "w2v_model", ".", "encoder", ".", "layers", "[", "ii", "]", ".", "self_attn", ".", "out_proj", ",", "'bias'", ")", ")", "\n", "parameters_to_prune", ".", "append", "(", "(", "model", ".", "w2v_encoder", ".", "w2v_model", ".", "encoder", ".", "layers", "[", "ii", "]", ".", "fc1", ",", "'weight'", ")", ")", "\n", "parameters_to_prune", ".", "append", "(", "(", "model", ".", "w2v_encoder", ".", "w2v_model", ".", "encoder", ".", "layers", "[", "ii", "]", ".", "fc1", ",", "'bias'", ")", ")", "\n", "parameters_to_prune", ".", "append", "(", "(", "model", ".", "w2v_encoder", ".", "w2v_model", ".", "encoder", ".", "layers", "[", "ii", "]", ".", "fc2", ",", "'weight'", ")", ")", "\n", "parameters_to_prune", ".", "append", "(", "(", "model", ".", "w2v_encoder", ".", "w2v_model", ".", "encoder", ".", "layers", "[", "ii", "]", ".", "fc2", ",", "'bias'", ")", ")", "\n", "\n", "", "parameters_to_prune", "=", "tuple", "(", "parameters_to_prune", ")", "\n", "\n", "prune", ".", "global_unstructured", "(", "\n", "parameters_to_prune", ",", "\n", "pruning_method", "=", "prune", ".", "L1Unstructured", ",", "\n", "amount", "=", "px", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.speech-lab-iitm_pada.None.train.unprune_bert": [[84, 108], ["range", "range", "parameters_to_prune.append", "parameters_to_prune.append", "parameters_to_prune.append", "parameters_to_prune.append", "parameters_to_prune.append", "parameters_to_prune.append", "len", "torch.remove", "torch.remove", "print"], "function", ["None"], ["", "def", "unprune_bert", "(", "model", ",", "model_type", "=", "'libri960_big'", ")", ":", "\n", "    ", "\"\"\"\n    remove pruning forward pre-hook. This is useful when we want to tweek the learned pruned mask, which is used in PARP.\n    \"\"\"", "\n", "if", "model_type", "==", "'wav2vec_small'", ":", "\n", "        ", "num_transformer_blocks", "=", "12", "\n", "", "elif", "model_type", "==", "'libri960_big'", "or", "model_type", "==", "'xlsr_53_56k'", ":", "\n", "        ", "num_transformer_blocks", "=", "24", "\n", "", "else", ":", "\n", "        ", "print", "(", "'model type {} not supported'", ".", "format", "(", "model_type", ")", ")", "\n", "# print('num_transformer_blocks is', num_transformer_blocks)", "\n", "\n", "", "parameters_to_prune", "=", "[", "]", "\n", "for", "ii", "in", "range", "(", "num_transformer_blocks", ")", ":", "\n", "        ", "parameters_to_prune", ".", "append", "(", "model", ".", "w2v_encoder", ".", "w2v_model", ".", "encoder", ".", "layers", "[", "ii", "]", ".", "self_attn", ".", "k_proj", ")", "\n", "parameters_to_prune", ".", "append", "(", "model", ".", "w2v_encoder", ".", "w2v_model", ".", "encoder", ".", "layers", "[", "ii", "]", ".", "self_attn", ".", "v_proj", ")", "\n", "parameters_to_prune", ".", "append", "(", "model", ".", "w2v_encoder", ".", "w2v_model", ".", "encoder", ".", "layers", "[", "ii", "]", ".", "self_attn", ".", "q_proj", ")", "\n", "parameters_to_prune", ".", "append", "(", "model", ".", "w2v_encoder", ".", "w2v_model", ".", "encoder", ".", "layers", "[", "ii", "]", ".", "self_attn", ".", "out_proj", ")", "\n", "parameters_to_prune", ".", "append", "(", "model", ".", "w2v_encoder", ".", "w2v_model", ".", "encoder", ".", "layers", "[", "ii", "]", ".", "fc1", ")", "\n", "parameters_to_prune", ".", "append", "(", "model", ".", "w2v_encoder", ".", "w2v_model", ".", "encoder", ".", "layers", "[", "ii", "]", ".", "fc2", ")", "\n", "\n", "", "for", "ii", "in", "range", "(", "0", ",", "len", "(", "parameters_to_prune", ")", ")", ":", "# applying both weight+bias masks", "\n", "        ", "prune", ".", "remove", "(", "parameters_to_prune", "[", "ii", "]", ",", "'weight'", ")", "\n", "prune", ".", "remove", "(", "parameters_to_prune", "[", "ii", "]", ",", "'bias'", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.speech-lab-iitm_pada.None.train.see_weight_rate": [[109, 155], ["range", "print", "float", "float", "float", "float", "float", "float", "float", "float", "float", "float", "float", "float", "float", "float", "float", "float", "float", "float", "float", "float", "float", "float", "float", "float", "model.w2v_encoder.w2v_model.encoder.layers[].self_attn.k_proj.weight.nelement", "torch.sum", "torch.sum", "model.w2v_encoder.w2v_model.encoder.layers[].self_attn.k_proj.bias.nelement", "torch.sum", "torch.sum", "model.w2v_encoder.w2v_model.encoder.layers[].self_attn.v_proj.weight.nelement", "torch.sum", "torch.sum", "model.w2v_encoder.w2v_model.encoder.layers[].self_attn.v_proj.bias.nelement", "torch.sum", "torch.sum", "model.w2v_encoder.w2v_model.encoder.layers[].self_attn.q_proj.weight.nelement", "torch.sum", "torch.sum", "model.w2v_encoder.w2v_model.encoder.layers[].self_attn.q_proj.bias.nelement", "torch.sum", "torch.sum", "model.w2v_encoder.w2v_model.encoder.layers[].self_attn.out_proj.weight.nelement", "torch.sum", "torch.sum", "model.w2v_encoder.w2v_model.encoder.layers[].self_attn.out_proj.bias.nelement", "torch.sum", "torch.sum", "model.w2v_encoder.w2v_model.encoder.layers[].fc1.weight.nelement", "torch.sum", "torch.sum", "model.w2v_encoder.w2v_model.encoder.layers[].fc1.bias.nelement", "torch.sum", "torch.sum", "model.w2v_encoder.w2v_model.encoder.layers[].fc2.weight.nelement", "torch.sum", "torch.sum", "model.w2v_encoder.w2v_model.encoder.layers[].fc2.bias.nelement", "torch.sum", "torch.sum"], "function", ["None"], ["", "", "def", "see_weight_rate", "(", "model", ",", "model_type", "=", "'wav2vec_small'", ")", ":", "\n", "    ", "\"\"\" check a model's zero rate \n    \"\"\"", "\n", "if", "model_type", "==", "'wav2vec_small'", ":", "\n", "        ", "num_transformer_blocks", "=", "12", "\n", "", "elif", "model_type", "==", "'libri960_big'", "or", "model_type", "==", "'xlsr_53_56k'", ":", "\n", "        ", "num_transformer_blocks", "=", "24", "\n", "", "else", ":", "\n", "        ", "print", "(", "'model type {} not supported'", ".", "format", "(", "model_type", ")", ")", "\n", "# print('num_transformer_blocks is', num_transformer_blocks)", "\n", "\n", "", "sum_list_2", ",", "zero_sum_2", "=", "0", ",", "0", "\n", "for", "ii", "in", "range", "(", "num_transformer_blocks", ")", ":", "\n", "        ", "sum_list_2", "=", "sum_list_2", "+", "float", "(", "model", ".", "w2v_encoder", ".", "w2v_model", ".", "encoder", ".", "layers", "[", "ii", "]", ".", "self_attn", ".", "k_proj", ".", "weight", ".", "nelement", "(", ")", ")", "\n", "zero_sum_2", "=", "zero_sum_2", "+", "float", "(", "torch", ".", "sum", "(", "model", ".", "w2v_encoder", ".", "w2v_model", ".", "encoder", ".", "layers", "[", "ii", "]", ".", "self_attn", ".", "k_proj", ".", "weight", "==", "0", ")", ")", "\n", "sum_list_2", "=", "sum_list_2", "+", "float", "(", "model", ".", "w2v_encoder", ".", "w2v_model", ".", "encoder", ".", "layers", "[", "ii", "]", ".", "self_attn", ".", "k_proj", ".", "bias", ".", "nelement", "(", ")", ")", "\n", "zero_sum_2", "=", "zero_sum_2", "+", "float", "(", "torch", ".", "sum", "(", "model", ".", "w2v_encoder", ".", "w2v_model", ".", "encoder", ".", "layers", "[", "ii", "]", ".", "self_attn", ".", "k_proj", ".", "bias", "==", "0", ")", ")", "\n", "\n", "sum_list_2", "=", "sum_list_2", "+", "float", "(", "model", ".", "w2v_encoder", ".", "w2v_model", ".", "encoder", ".", "layers", "[", "ii", "]", ".", "self_attn", ".", "v_proj", ".", "weight", ".", "nelement", "(", ")", ")", "\n", "zero_sum_2", "=", "zero_sum_2", "+", "float", "(", "torch", ".", "sum", "(", "model", ".", "w2v_encoder", ".", "w2v_model", ".", "encoder", ".", "layers", "[", "ii", "]", ".", "self_attn", ".", "v_proj", ".", "weight", "==", "0", ")", ")", "\n", "sum_list_2", "=", "sum_list_2", "+", "float", "(", "model", ".", "w2v_encoder", ".", "w2v_model", ".", "encoder", ".", "layers", "[", "ii", "]", ".", "self_attn", ".", "v_proj", ".", "bias", ".", "nelement", "(", ")", ")", "\n", "zero_sum_2", "=", "zero_sum_2", "+", "float", "(", "torch", ".", "sum", "(", "model", ".", "w2v_encoder", ".", "w2v_model", ".", "encoder", ".", "layers", "[", "ii", "]", ".", "self_attn", ".", "v_proj", ".", "bias", "==", "0", ")", ")", "\n", "\n", "sum_list_2", "=", "sum_list_2", "+", "float", "(", "model", ".", "w2v_encoder", ".", "w2v_model", ".", "encoder", ".", "layers", "[", "ii", "]", ".", "self_attn", ".", "q_proj", ".", "weight", ".", "nelement", "(", ")", ")", "\n", "zero_sum_2", "=", "zero_sum_2", "+", "float", "(", "torch", ".", "sum", "(", "model", ".", "w2v_encoder", ".", "w2v_model", ".", "encoder", ".", "layers", "[", "ii", "]", ".", "self_attn", ".", "q_proj", ".", "weight", "==", "0", ")", ")", "\n", "sum_list_2", "=", "sum_list_2", "+", "float", "(", "model", ".", "w2v_encoder", ".", "w2v_model", ".", "encoder", ".", "layers", "[", "ii", "]", ".", "self_attn", ".", "q_proj", ".", "bias", ".", "nelement", "(", ")", ")", "\n", "zero_sum_2", "=", "zero_sum_2", "+", "float", "(", "torch", ".", "sum", "(", "model", ".", "w2v_encoder", ".", "w2v_model", ".", "encoder", ".", "layers", "[", "ii", "]", ".", "self_attn", ".", "q_proj", ".", "bias", "==", "0", ")", ")", "\n", "\n", "sum_list_2", "=", "sum_list_2", "+", "float", "(", "model", ".", "w2v_encoder", ".", "w2v_model", ".", "encoder", ".", "layers", "[", "ii", "]", ".", "self_attn", ".", "out_proj", ".", "weight", ".", "nelement", "(", ")", ")", "\n", "zero_sum_2", "=", "zero_sum_2", "+", "float", "(", "torch", ".", "sum", "(", "model", ".", "w2v_encoder", ".", "w2v_model", ".", "encoder", ".", "layers", "[", "ii", "]", ".", "self_attn", ".", "out_proj", ".", "weight", "==", "0", ")", ")", "\n", "sum_list_2", "=", "sum_list_2", "+", "float", "(", "model", ".", "w2v_encoder", ".", "w2v_model", ".", "encoder", ".", "layers", "[", "ii", "]", ".", "self_attn", ".", "out_proj", ".", "bias", ".", "nelement", "(", ")", ")", "\n", "zero_sum_2", "=", "zero_sum_2", "+", "float", "(", "torch", ".", "sum", "(", "model", ".", "w2v_encoder", ".", "w2v_model", ".", "encoder", ".", "layers", "[", "ii", "]", ".", "self_attn", ".", "out_proj", ".", "bias", "==", "0", ")", ")", "\n", "\n", "sum_list_2", "=", "sum_list_2", "+", "float", "(", "model", ".", "w2v_encoder", ".", "w2v_model", ".", "encoder", ".", "layers", "[", "ii", "]", ".", "fc1", ".", "weight", ".", "nelement", "(", ")", ")", "\n", "zero_sum_2", "=", "zero_sum_2", "+", "float", "(", "torch", ".", "sum", "(", "model", ".", "w2v_encoder", ".", "w2v_model", ".", "encoder", ".", "layers", "[", "ii", "]", ".", "fc1", ".", "weight", "==", "0", ")", ")", "\n", "sum_list_2", "=", "sum_list_2", "+", "float", "(", "model", ".", "w2v_encoder", ".", "w2v_model", ".", "encoder", ".", "layers", "[", "ii", "]", ".", "fc1", ".", "bias", ".", "nelement", "(", ")", ")", "\n", "zero_sum_2", "=", "zero_sum_2", "+", "float", "(", "torch", ".", "sum", "(", "model", ".", "w2v_encoder", ".", "w2v_model", ".", "encoder", ".", "layers", "[", "ii", "]", ".", "fc1", ".", "bias", "==", "0", ")", ")", "\n", "\n", "sum_list_2", "=", "sum_list_2", "+", "float", "(", "model", ".", "w2v_encoder", ".", "w2v_model", ".", "encoder", ".", "layers", "[", "ii", "]", ".", "fc2", ".", "weight", ".", "nelement", "(", ")", ")", "\n", "zero_sum_2", "=", "zero_sum_2", "+", "float", "(", "torch", ".", "sum", "(", "model", ".", "w2v_encoder", ".", "w2v_model", ".", "encoder", ".", "layers", "[", "ii", "]", ".", "fc2", ".", "weight", "==", "0", ")", ")", "\n", "sum_list_2", "=", "sum_list_2", "+", "float", "(", "model", ".", "w2v_encoder", ".", "w2v_model", ".", "encoder", ".", "layers", "[", "ii", "]", ".", "fc2", ".", "bias", ".", "nelement", "(", ")", ")", "\n", "zero_sum_2", "=", "zero_sum_2", "+", "float", "(", "torch", ".", "sum", "(", "model", ".", "w2v_encoder", ".", "w2v_model", ".", "encoder", ".", "layers", "[", "ii", "]", ".", "fc2", ".", "bias", "==", "0", ")", ")", "\n", "\n", "", "bert_zero_rate", "=", "100", "*", "zero_sum_2", "/", "sum_list_2", "\n", "# print('BERT zero rate is {0:.2f}'.format(bert_zero_rate))", "\n", "return", "bert_zero_rate", "\n", "\n"]], "home.repos.pwc.inspect_result.speech-lab-iitm_pada.None.train.apply_pruning_mask": [[156, 195], ["print", "range", "range", "parameters_to_prune.append", "mask_list_w.append", "mask_list_b.append", "parameters_to_prune.append", "mask_list_w.append", "mask_list_b.append", "parameters_to_prune.append", "mask_list_w.append", "mask_list_b.append", "parameters_to_prune.append", "mask_list_w.append", "mask_list_b.append", "parameters_to_prune.append", "mask_list_w.append", "mask_list_b.append", "parameters_to_prune.append", "mask_list_w.append", "mask_list_b.append", "len", "torch.CustomFromMask.apply", "torch.CustomFromMask.apply", "torch.remove", "torch.remove", "print", "str", "str", "str", "str", "str", "str", "str", "str", "str", "str", "str", "str"], "function", ["None"], ["", "def", "apply_pruning_mask", "(", "model", ",", "mask_dict", ",", "model_type", "=", "'libri960_big'", ")", ":", "\n", "    ", "\"\"\"\n    apply existing pruning mask to a pre-trained wav2vec 2.0. \n    \"\"\"", "\n", "if", "model_type", "==", "'wav2vec_small'", ":", "\n", "        ", "num_transformer_blocks", "=", "12", "\n", "", "elif", "model_type", "==", "'libri960_big'", "or", "model_type", "==", "'xlsr_53_56k'", ":", "\n", "        ", "num_transformer_blocks", "=", "24", "\n", "", "else", ":", "\n", "        ", "print", "(", "'model type {} not supported'", ".", "format", "(", "model_type", ")", ")", "\n", "", "print", "(", "'num_transformer_blocks is'", ",", "num_transformer_blocks", ")", "\n", "\n", "parameters_to_prune", "=", "[", "]", "\n", "mask_list_w", ",", "mask_list_b", "=", "[", "]", ",", "[", "]", "# maks list for weight and bias", "\n", "for", "ii", "in", "range", "(", "num_transformer_blocks", ")", ":", "\n", "        ", "parameters_to_prune", ".", "append", "(", "model", ".", "w2v_encoder", ".", "w2v_model", ".", "encoder", ".", "layers", "[", "ii", "]", ".", "self_attn", ".", "k_proj", ")", "\n", "mask_list_w", ".", "append", "(", "mask_dict", "[", "'w2v_encoder.w2v_model.encoder.layers.'", "+", "str", "(", "ii", ")", "+", "'.self_attn.k_proj.weight_mask'", "]", ")", "\n", "mask_list_b", ".", "append", "(", "mask_dict", "[", "'w2v_encoder.w2v_model.encoder.layers.'", "+", "str", "(", "ii", ")", "+", "'.self_attn.k_proj.bias_mask'", "]", ")", "\n", "parameters_to_prune", ".", "append", "(", "model", ".", "w2v_encoder", ".", "w2v_model", ".", "encoder", ".", "layers", "[", "ii", "]", ".", "self_attn", ".", "v_proj", ")", "\n", "mask_list_w", ".", "append", "(", "mask_dict", "[", "'w2v_encoder.w2v_model.encoder.layers.'", "+", "str", "(", "ii", ")", "+", "'.self_attn.v_proj.weight_mask'", "]", ")", "\n", "mask_list_b", ".", "append", "(", "mask_dict", "[", "'w2v_encoder.w2v_model.encoder.layers.'", "+", "str", "(", "ii", ")", "+", "'.self_attn.v_proj.bias_mask'", "]", ")", "\n", "parameters_to_prune", ".", "append", "(", "model", ".", "w2v_encoder", ".", "w2v_model", ".", "encoder", ".", "layers", "[", "ii", "]", ".", "self_attn", ".", "q_proj", ")", "\n", "mask_list_w", ".", "append", "(", "mask_dict", "[", "'w2v_encoder.w2v_model.encoder.layers.'", "+", "str", "(", "ii", ")", "+", "'.self_attn.q_proj.weight_mask'", "]", ")", "\n", "mask_list_b", ".", "append", "(", "mask_dict", "[", "'w2v_encoder.w2v_model.encoder.layers.'", "+", "str", "(", "ii", ")", "+", "'.self_attn.q_proj.bias_mask'", "]", ")", "\n", "parameters_to_prune", ".", "append", "(", "model", ".", "w2v_encoder", ".", "w2v_model", ".", "encoder", ".", "layers", "[", "ii", "]", ".", "self_attn", ".", "out_proj", ")", "\n", "mask_list_w", ".", "append", "(", "mask_dict", "[", "'w2v_encoder.w2v_model.encoder.layers.'", "+", "str", "(", "ii", ")", "+", "'.self_attn.out_proj.weight_mask'", "]", ")", "\n", "mask_list_b", ".", "append", "(", "mask_dict", "[", "'w2v_encoder.w2v_model.encoder.layers.'", "+", "str", "(", "ii", ")", "+", "'.self_attn.out_proj.bias_mask'", "]", ")", "\n", "parameters_to_prune", ".", "append", "(", "model", ".", "w2v_encoder", ".", "w2v_model", ".", "encoder", ".", "layers", "[", "ii", "]", ".", "fc1", ")", "\n", "mask_list_w", ".", "append", "(", "mask_dict", "[", "'w2v_encoder.w2v_model.encoder.layers.'", "+", "str", "(", "ii", ")", "+", "'.fc1.weight_mask'", "]", ")", "\n", "mask_list_b", ".", "append", "(", "mask_dict", "[", "'w2v_encoder.w2v_model.encoder.layers.'", "+", "str", "(", "ii", ")", "+", "'.fc1.bias_mask'", "]", ")", "\n", "parameters_to_prune", ".", "append", "(", "model", ".", "w2v_encoder", ".", "w2v_model", ".", "encoder", ".", "layers", "[", "ii", "]", ".", "fc2", ")", "\n", "mask_list_w", ".", "append", "(", "mask_dict", "[", "'w2v_encoder.w2v_model.encoder.layers.'", "+", "str", "(", "ii", ")", "+", "'.fc2.weight_mask'", "]", ")", "\n", "mask_list_b", ".", "append", "(", "mask_dict", "[", "'w2v_encoder.w2v_model.encoder.layers.'", "+", "str", "(", "ii", ")", "+", "'.fc2.bias_mask'", "]", ")", "\n", "\n", "", "for", "ii", "in", "range", "(", "0", ",", "len", "(", "parameters_to_prune", ")", ")", ":", "# applying both weight+bias masks", "\n", "        ", "prune", ".", "CustomFromMask", ".", "apply", "(", "parameters_to_prune", "[", "ii", "]", ",", "'weight'", ",", "mask", "=", "mask_list_w", "[", "ii", "]", ")", "\n", "prune", ".", "CustomFromMask", ".", "apply", "(", "parameters_to_prune", "[", "ii", "]", ",", "'bias'", ",", "mask", "=", "mask_list_b", "[", "ii", "]", ")", "\n", "prune", ".", "remove", "(", "parameters_to_prune", "[", "ii", "]", ",", "'weight'", ")", "\n", "prune", ".", "remove", "(", "parameters_to_prune", "[", "ii", "]", ",", "'bias'", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.speech-lab-iitm_pada.None.train.main": [[197, 386], ["isinstance", "fairseq.utils.import_user_module", "fairseq.dataclass.initialize.add_defaults", "fairseq.logging.metrics.reset", "numpy.random.seed", "fairseq.utils.set_torch_seed", "fairseq.distributed.utils.is_master", "logger.info", "fairseq.tasks.setup_task", "tasks.setup_task.build_criterion", "logger.info", "torch.load", "torch.load", "torch.load.keys", "train.apply_pruning_mask", "train.pruning_bert", "train.unprune_bert", "logger.info", "logger.info", "logger.info", "logger.info", "logger.info", "fairseq.data.data_utils.raise_if_valid_subsets_unintentionally_ignored", "logger.info", "logger.info", "fairseq.checkpoint_utils.load_checkpoint", "fairseq.model_parallel.megatron_trainer.MegatronTrainer.get_lr", "fairseq.logging.meters.StopwatchMeter", "meters.StopwatchMeter.start", "meters.StopwatchMeter.stop", "logger.info", "fairseq.dataclass.utils.convert_namespace_to_omegaconf", "fairseq.distributed.utils.is_master", "logging.config.dictConfig", "logging.FileHandler", "logger.addHandler", "fairseq.checkpoint_utils.verify_checkpoint_directory", "tasks.setup_task.build_model", "tasks.setup_task.load_dataset", "fairseq.dataclass.utils.convert_namespace_to_omegaconf.dataset.valid_subset.split", "fairseq.quantization_utils.Quantizer", "fairseq.trainer.Trainer", "fairseq.model_parallel.megatron_trainer.MegatronTrainer", "xm.rendezvous", "train.train", "fairseq.model_parallel.megatron_trainer.MegatronTrainer.lr_step", "fairseq.model_parallel.megatron_trainer.MegatronTrainer.get_train_iterator", "logger.info", "fairseq.file_io.PathManager.async_close", "logger.info", "omegaconf.OmegaConf.to_container", "fairseq.distributed.fsdp_enable_wrap", "fairseq.distributed.fsdp_wrap", "sum", "sum", "sum", "sum", "tasks.setup_task.load_dataset", "tasks.setup_task.has_sharded_data", "logger.info", "logging.exception", "tasks.setup_task.build_model", "tasks.setup_task.has_sharded_data", "tasks.setup_task.has_sharded_data", "p.numel", "p.numel", "p.numel", "p.numel", "fairseq.distributed.fsdp_wrap.parameters", "fairseq.distributed.fsdp_wrap.parameters", "fairseq.distributed.fsdp_wrap.parameters", "getattr", "fairseq.distributed.fsdp_wrap.parameters", "getattr", "getattr", "getattr"], "function", ["home.repos.pwc.inspect_result.speech-lab-iitm_pada.None.train.apply_pruning_mask", "home.repos.pwc.inspect_result.speech-lab-iitm_pada.None.train.pruning_bert", "home.repos.pwc.inspect_result.speech-lab-iitm_pada.None.train.unprune_bert", "home.repos.pwc.inspect_result.speech-lab-iitm_pada.None.train.train"], ["", "", "def", "main", "(", "cfg", ":", "FairseqConfig", ")", "->", "None", ":", "\n", "    ", "if", "isinstance", "(", "cfg", ",", "argparse", ".", "Namespace", ")", ":", "\n", "        ", "cfg", "=", "convert_namespace_to_omegaconf", "(", "cfg", ")", "\n", "\n", "", "utils", ".", "import_user_module", "(", "cfg", ".", "common", ")", "\n", "add_defaults", "(", "cfg", ")", "\n", "\n", "if", "(", "\n", "distributed_utils", ".", "is_master", "(", "cfg", ".", "distributed_training", ")", "\n", "and", "\"job_logging_cfg\"", "in", "cfg", "\n", ")", ":", "\n", "# make hydra logging work with ddp (see # see https://github.com/facebookresearch/hydra/issues/1126)", "\n", "        ", "logging", ".", "config", ".", "dictConfig", "(", "OmegaConf", ".", "to_container", "(", "cfg", ".", "job_logging_cfg", ")", ")", "\n", "\n", "", "assert", "(", "\n", "cfg", ".", "dataset", ".", "max_tokens", "is", "not", "None", "or", "cfg", ".", "dataset", ".", "batch_size", "is", "not", "None", "\n", ")", ",", "\"Must specify batch size either with --max-tokens or --batch-size\"", "\n", "metrics", ".", "reset", "(", ")", "\n", "\n", "if", "cfg", ".", "common", ".", "log_file", "is", "not", "None", ":", "\n", "        ", "handler", "=", "logging", ".", "FileHandler", "(", "filename", "=", "cfg", ".", "common", ".", "log_file", ")", "\n", "logger", ".", "addHandler", "(", "handler", ")", "\n", "\n", "", "np", ".", "random", ".", "seed", "(", "cfg", ".", "common", ".", "seed", ")", "\n", "utils", ".", "set_torch_seed", "(", "cfg", ".", "common", ".", "seed", ")", "\n", "\n", "if", "distributed_utils", ".", "is_master", "(", "cfg", ".", "distributed_training", ")", ":", "\n", "        ", "checkpoint_utils", ".", "verify_checkpoint_directory", "(", "cfg", ".", "checkpoint", ".", "save_dir", ")", "\n", "\n", "# Print args", "\n", "", "logger", ".", "info", "(", "cfg", ")", "\n", "\n", "if", "cfg", ".", "checkpoint", ".", "write_checkpoints_asynchronously", ":", "\n", "        ", "try", ":", "\n", "            ", "import", "iopath", "# noqa: F401", "\n", "", "except", "ImportError", ":", "\n", "            ", "logging", ".", "exception", "(", "\n", "\"Asynchronous checkpoint writing is specified but iopath is \"", "\n", "\"not installed: `pip install iopath`\"", "\n", ")", "\n", "return", "\n", "\n", "# Setup task, e.g., translation, language modeling, etc.", "\n", "", "", "task", "=", "tasks", ".", "setup_task", "(", "cfg", ".", "task", ")", "\n", "\n", "assert", "cfg", ".", "criterion", ",", "\"Please specify criterion to train a model\"", "\n", "\n", "# Build model and criterion", "\n", "if", "cfg", ".", "distributed_training", ".", "ddp_backend", "==", "\"fully_sharded\"", ":", "\n", "        ", "with", "fsdp_enable_wrap", "(", "cfg", ".", "distributed_training", ")", ":", "\n", "            ", "model", "=", "fsdp_wrap", "(", "task", ".", "build_model", "(", "cfg", ".", "model", ")", ")", "\n", "", "", "else", ":", "\n", "        ", "model", "=", "task", ".", "build_model", "(", "cfg", ".", "model", ")", "\n", "", "criterion", "=", "task", ".", "build_criterion", "(", "cfg", ".", "criterion", ")", "\n", "logger", ".", "info", "(", "model", ")", "\n", "\n", "# Initial pruning for TAW and CD-TAW", "\n", "mask_dict", "=", "{", "}", "\n", "ft_model_state_dict", "=", "torch", ".", "load", "(", "'path_to_the_state_dict_carrying_the_mask'", ",", "map_location", "=", "'cpu'", ")", "\n", "\n", "for", "key", "in", "ft_model_state_dict", ".", "keys", "(", ")", ":", "\n", "        ", "if", "'mask'", "in", "key", ":", "\n", "            ", "mask_dict", "[", "key", "]", "=", "ft_model_state_dict", "[", "key", "]", "\n", "\n", "", "", "apply_pruning_mask", "(", "model", ",", "mask_dict", ")", "\n", "\n", "# Initial pruning for TAG", "\n", "# a 0.3 prune_rate refers to the pruning percentage of 30%. The least 30% of the weights in terms of magnitude are marked for zeroing out.", "\n", "prune_rate", "=", "0.3", "\n", "# Change the model_type to 'wav2vec_small' if you're using a base model of wav2vec2", "\n", "pruning_bert", "(", "trainer", ".", "model", ",", "prune_rate", ",", "model_type", "=", "'libri960_big'", ")", "\n", "unprune_bert", "(", "trainer", ".", "model", ",", "model_type", "=", "'libri960_big'", ")", "\n", "\n", "logger", ".", "info", "(", "\"task: {}\"", ".", "format", "(", "task", ".", "__class__", ".", "__name__", ")", ")", "\n", "logger", ".", "info", "(", "\"model: {}\"", ".", "format", "(", "model", ".", "__class__", ".", "__name__", ")", ")", "\n", "logger", ".", "info", "(", "\"criterion: {}\"", ".", "format", "(", "criterion", ".", "__class__", ".", "__name__", ")", ")", "\n", "logger", ".", "info", "(", "\n", "\"num. shared model params: {:,} (num. trained: {:,})\"", ".", "format", "(", "\n", "sum", "(", "\n", "p", ".", "numel", "(", ")", "for", "p", "in", "model", ".", "parameters", "(", ")", "if", "not", "getattr", "(", "p", ",", "\"expert\"", ",", "False", ")", "\n", ")", ",", "\n", "sum", "(", "\n", "p", ".", "numel", "(", ")", "\n", "for", "p", "in", "model", ".", "parameters", "(", ")", "\n", "if", "not", "getattr", "(", "p", ",", "\"expert\"", ",", "False", ")", "and", "p", ".", "requires_grad", "\n", ")", ",", "\n", ")", "\n", ")", "\n", "\n", "logger", ".", "info", "(", "\n", "\"num. expert model params: {} (num. trained: {})\"", ".", "format", "(", "\n", "sum", "(", "p", ".", "numel", "(", ")", "for", "p", "in", "model", ".", "parameters", "(", ")", "if", "getattr", "(", "p", ",", "\"expert\"", ",", "False", ")", ")", ",", "\n", "sum", "(", "\n", "p", ".", "numel", "(", ")", "\n", "for", "p", "in", "model", ".", "parameters", "(", ")", "\n", "if", "getattr", "(", "p", ",", "\"expert\"", ",", "False", ")", "and", "p", ".", "requires_grad", "\n", ")", ",", "\n", ")", "\n", ")", "\n", "\n", "# Load valid dataset (we load training data below, based on the latest checkpoint)", "\n", "# We load the valid dataset AFTER building the model", "\n", "data_utils", ".", "raise_if_valid_subsets_unintentionally_ignored", "(", "cfg", ")", "\n", "if", "cfg", ".", "dataset", ".", "combine_valid_subsets", ":", "\n", "        ", "task", ".", "load_dataset", "(", "\"valid\"", ",", "combine", "=", "True", ",", "epoch", "=", "1", ")", "\n", "", "else", ":", "\n", "        ", "for", "valid_sub_split", "in", "cfg", ".", "dataset", ".", "valid_subset", ".", "split", "(", "\",\"", ")", ":", "\n", "            ", "task", ".", "load_dataset", "(", "valid_sub_split", ",", "combine", "=", "False", ",", "epoch", "=", "1", ")", "\n", "\n", "# (optionally) Configure quantization", "\n", "", "", "if", "cfg", ".", "common", ".", "quantization_config_path", "is", "not", "None", ":", "\n", "        ", "quantizer", "=", "quantization_utils", ".", "Quantizer", "(", "\n", "config_path", "=", "cfg", ".", "common", ".", "quantization_config_path", ",", "\n", "max_epoch", "=", "cfg", ".", "optimization", ".", "max_epoch", ",", "\n", "max_update", "=", "cfg", ".", "optimization", ".", "max_update", ",", "\n", ")", "\n", "", "else", ":", "\n", "        ", "quantizer", "=", "None", "\n", "\n", "# Build trainer", "\n", "", "if", "cfg", ".", "common", ".", "model_parallel_size", "==", "1", ":", "\n", "        ", "trainer", "=", "Trainer", "(", "cfg", ",", "task", ",", "model", ",", "criterion", ",", "quantizer", ")", "\n", "", "else", ":", "\n", "        ", "trainer", "=", "MegatronTrainer", "(", "cfg", ",", "task", ",", "model", ",", "criterion", ")", "\n", "", "logger", ".", "info", "(", "\n", "\"training on {} devices (GPUs/TPUs)\"", ".", "format", "(", "\n", "cfg", ".", "distributed_training", ".", "distributed_world_size", "\n", ")", "\n", ")", "\n", "logger", ".", "info", "(", "\n", "\"max tokens per device = {} and max sentences per device = {}\"", ".", "format", "(", "\n", "cfg", ".", "dataset", ".", "max_tokens", ",", "\n", "cfg", ".", "dataset", ".", "batch_size", ",", "\n", ")", "\n", ")", "\n", "\n", "# Load the latest checkpoint if one is available and restore the", "\n", "# corresponding train iterator", "\n", "extra_state", ",", "epoch_itr", "=", "checkpoint_utils", ".", "load_checkpoint", "(", "\n", "cfg", ".", "checkpoint", ",", "\n", "trainer", ",", "\n", "# don't cache epoch iterators for sharded datasets", "\n", "disable_iterator_cache", "=", "task", ".", "has_sharded_data", "(", "\"train\"", ")", ",", "\n", ")", "\n", "if", "cfg", ".", "common", ".", "tpu", ":", "\n", "        ", "import", "torch_xla", ".", "core", ".", "xla_model", "as", "xm", "\n", "\n", "xm", ".", "rendezvous", "(", "\"load_checkpoint\"", ")", "# wait for all workers", "\n", "\n", "", "max_epoch", "=", "cfg", ".", "optimization", ".", "max_epoch", "or", "math", ".", "inf", "\n", "lr", "=", "trainer", ".", "get_lr", "(", ")", "\n", "\n", "train_meter", "=", "meters", ".", "StopwatchMeter", "(", ")", "\n", "train_meter", ".", "start", "(", ")", "\n", "while", "epoch_itr", ".", "next_epoch_idx", "<=", "max_epoch", ":", "\n", "        ", "if", "lr", "<=", "cfg", ".", "optimization", ".", "stop_min_lr", ":", "\n", "            ", "logger", ".", "info", "(", "\n", "f\"stopping training because current learning rate ({lr}) is smaller \"", "\n", "\"than or equal to minimum learning rate \"", "\n", "f\"(--stop-min-lr={cfg.optimization.stop_min_lr})\"", "\n", ")", "\n", "break", "\n", "\n", "# train for one epoch", "\n", "", "valid_losses", ",", "should_stop", "=", "train", "(", "cfg", ",", "trainer", ",", "task", ",", "epoch_itr", ")", "\n", "if", "should_stop", ":", "\n", "            ", "break", "\n", "\n", "# only use first validation loss to update the learning rate", "\n", "", "lr", "=", "trainer", ".", "lr_step", "(", "epoch_itr", ".", "epoch", ",", "valid_losses", "[", "0", "]", ")", "\n", "\n", "epoch_itr", "=", "trainer", ".", "get_train_iterator", "(", "\n", "epoch_itr", ".", "next_epoch_idx", ",", "\n", "# sharded data: get train iterator for next epoch", "\n", "load_dataset", "=", "task", ".", "has_sharded_data", "(", "\"train\"", ")", ",", "\n", "# don't cache epoch iterators for sharded datasets", "\n", "disable_iterator_cache", "=", "task", ".", "has_sharded_data", "(", "\"train\"", ")", ",", "\n", ")", "\n", "", "train_meter", ".", "stop", "(", ")", "\n", "logger", ".", "info", "(", "\"done training in {:.1f} seconds\"", ".", "format", "(", "train_meter", ".", "sum", ")", ")", "\n", "\n", "# ioPath implementation to wait for all asynchronous file writes to complete.", "\n", "if", "cfg", ".", "checkpoint", ".", "write_checkpoints_asynchronously", ":", "\n", "        ", "logger", ".", "info", "(", "\n", "\"ioPath PathManager waiting for all asynchronous checkpoint \"", "\n", "\"writes to finish.\"", "\n", ")", "\n", "PathManager", ".", "async_close", "(", ")", "\n", "logger", ".", "info", "(", "\"ioPath PathManager finished waiting.\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.speech-lab-iitm_pada.None.train.should_stop_early": [[388, 414], ["getattr", "train.should_stop_early.is_better"], "function", ["None"], ["", "", "def", "should_stop_early", "(", "cfg", ":", "DictConfig", ",", "valid_loss", ":", "float", ")", "->", "bool", ":", "\n", "# skip check if no validation was done in the current epoch", "\n", "    ", "if", "valid_loss", "is", "None", ":", "\n", "        ", "return", "False", "\n", "", "if", "cfg", ".", "checkpoint", ".", "patience", "<=", "0", ":", "\n", "        ", "return", "False", "\n", "\n", "", "def", "is_better", "(", "a", ",", "b", ")", ":", "\n", "        ", "return", "a", ">", "b", "if", "cfg", ".", "checkpoint", ".", "maximize_best_checkpoint_metric", "else", "a", "<", "b", "\n", "\n", "", "prev_best", "=", "getattr", "(", "should_stop_early", ",", "\"best\"", ",", "None", ")", "\n", "if", "prev_best", "is", "None", "or", "is_better", "(", "valid_loss", ",", "prev_best", ")", ":", "\n", "        ", "should_stop_early", ".", "best", "=", "valid_loss", "\n", "should_stop_early", ".", "num_runs", "=", "0", "\n", "return", "False", "\n", "", "else", ":", "\n", "        ", "should_stop_early", ".", "num_runs", "+=", "1", "\n", "if", "should_stop_early", ".", "num_runs", ">=", "cfg", ".", "checkpoint", ".", "patience", ":", "\n", "            ", "logger", ".", "info", "(", "\n", "\"early stop since valid performance hasn't improved for last {} runs\"", ".", "format", "(", "\n", "cfg", ".", "checkpoint", ".", "patience", "\n", ")", "\n", ")", "\n", "return", "True", "\n", "", "else", ":", "\n", "            ", "return", "False", "\n", "\n"]], "home.repos.pwc.inspect_result.speech-lab-iitm_pada.None.train.train": [[418, 535], ["fairseq.logging.metrics.aggregate", "epoch_itr.next_epoch_itr", "fairseq.data.iterators.GroupedIterator", "fairseq.logging.progress_bar.progress_bar", "progress_bar.progress_bar.update_config", "trainer.begin_epoch", "cfg.dataset.valid_subset.split", "torch.tensor().cuda", "torch.tensor().cuda", "trainer.get_num_updates", "logger.info", "enumerate", "logger.info", "train.get_training_stats", "progress_bar.progress_bar.print", "fairseq.logging.metrics.reset_meters", "fairseq.utils.tpu_data_loader", "train._flatten_config", "train.validate_and_save", "fairseq.logging.metrics.get_smoothed_values", "torch.distributed.all_reduce", "torch.distributed.all_reduce", "len", "os.environ.get", "torch.tensor", "torch.tensor", "fairseq.logging.metrics.aggregate", "torch.autograd.profiler.record_function", "torch.autograd.profiler.record_function", "trainer.train_step", "trainer.get_num_updates", "utils.tpu_data_loader.has_next", "train.pruning_bert", "train.unprune_bert", "train.pruning_bert", "train.unprune_bert", "fairseq.distributed.utils.is_master", "fairseq.distributed.utils.is_master", "os.path.basename", "fairseq.distributed.utils.is_master", "train.get_training_stats", "progress_bar.progress_bar.log", "fairseq.logging.metrics.reset_meters", "abs", "abs", "fairseq.logging.metrics.get_smoothed_values"], "function", ["home.repos.pwc.inspect_result.speech-lab-iitm_pada.None.train.get_training_stats", "home.repos.pwc.inspect_result.speech-lab-iitm_pada.None.train._flatten_config", "home.repos.pwc.inspect_result.speech-lab-iitm_pada.None.train.validate_and_save", "home.repos.pwc.inspect_result.speech-lab-iitm_pada.None.train.pruning_bert", "home.repos.pwc.inspect_result.speech-lab-iitm_pada.None.train.unprune_bert", "home.repos.pwc.inspect_result.speech-lab-iitm_pada.None.train.pruning_bert", "home.repos.pwc.inspect_result.speech-lab-iitm_pada.None.train.unprune_bert", "home.repos.pwc.inspect_result.speech-lab-iitm_pada.None.train.get_training_stats"], ["@", "metrics", ".", "aggregate", "(", "\"train\"", ")", "\n", "def", "train", "(", "\n", "cfg", ":", "DictConfig", ",", "trainer", ":", "Trainer", ",", "task", ":", "tasks", ".", "FairseqTask", ",", "epoch_itr", "\n", ")", "->", "Tuple", "[", "List", "[", "Optional", "[", "float", "]", "]", ",", "bool", "]", ":", "\n", "    ", "\"\"\"Train the model for one epoch and return validation losses.\"\"\"", "\n", "# Initialize data iterator", "\n", "itr", "=", "epoch_itr", ".", "next_epoch_itr", "(", "\n", "fix_batches_to_gpus", "=", "cfg", ".", "distributed_training", ".", "fix_batches_to_gpus", ",", "\n", "shuffle", "=", "(", "epoch_itr", ".", "next_epoch_idx", ">", "cfg", ".", "dataset", ".", "curriculum", ")", ",", "\n", ")", "\n", "update_freq", "=", "(", "\n", "cfg", ".", "optimization", ".", "update_freq", "[", "epoch_itr", ".", "epoch", "-", "1", "]", "\n", "if", "epoch_itr", ".", "epoch", "<=", "len", "(", "cfg", ".", "optimization", ".", "update_freq", ")", "\n", "else", "cfg", ".", "optimization", ".", "update_freq", "[", "-", "1", "]", "\n", ")", "\n", "itr", "=", "iterators", ".", "GroupedIterator", "(", "\n", "itr", ",", "\n", "update_freq", ",", "\n", "skip_remainder_batch", "=", "cfg", ".", "optimization", ".", "skip_remainder_batch", ",", "\n", ")", "\n", "if", "cfg", ".", "common", ".", "tpu", ":", "\n", "        ", "itr", "=", "utils", ".", "tpu_data_loader", "(", "itr", ")", "\n", "", "progress", "=", "progress_bar", ".", "progress_bar", "(", "\n", "itr", ",", "\n", "log_format", "=", "cfg", ".", "common", ".", "log_format", ",", "\n", "log_file", "=", "cfg", ".", "common", ".", "log_file", ",", "\n", "log_interval", "=", "cfg", ".", "common", ".", "log_interval", ",", "\n", "epoch", "=", "epoch_itr", ".", "epoch", ",", "\n", "tensorboard_logdir", "=", "(", "\n", "cfg", ".", "common", ".", "tensorboard_logdir", "\n", "if", "distributed_utils", ".", "is_master", "(", "cfg", ".", "distributed_training", ")", "\n", "else", "None", "\n", ")", ",", "\n", "default_log_format", "=", "(", "\"tqdm\"", "if", "not", "cfg", ".", "common", ".", "no_progress_bar", "else", "\"simple\"", ")", ",", "\n", "wandb_project", "=", "(", "\n", "cfg", ".", "common", ".", "wandb_project", "\n", "if", "distributed_utils", ".", "is_master", "(", "cfg", ".", "distributed_training", ")", "\n", "else", "None", "\n", ")", ",", "\n", "wandb_run_name", "=", "os", ".", "environ", ".", "get", "(", "\n", "\"WANDB_NAME\"", ",", "os", ".", "path", ".", "basename", "(", "cfg", ".", "checkpoint", ".", "save_dir", ")", "\n", ")", ",", "\n", "azureml_logging", "=", "(", "\n", "cfg", ".", "common", ".", "azureml_logging", "\n", "if", "distributed_utils", ".", "is_master", "(", "cfg", ".", "distributed_training", ")", "\n", "else", "False", "\n", ")", ",", "\n", ")", "\n", "progress", ".", "update_config", "(", "_flatten_config", "(", "cfg", ")", ")", "\n", "\n", "trainer", ".", "begin_epoch", "(", "epoch_itr", ".", "epoch", ")", "\n", "\n", "valid_subsets", "=", "cfg", ".", "dataset", ".", "valid_subset", ".", "split", "(", "\",\"", ")", "\n", "should_stop", "=", "False", "\n", "done_with_epoch", "=", "torch", ".", "tensor", "(", "[", "0", "]", ")", ".", "cuda", "(", ")", "\n", "num_updates", "=", "trainer", ".", "get_num_updates", "(", ")", "\n", "logger", ".", "info", "(", "\"Start iterating over samples\"", ")", "\n", "for", "i", ",", "samples", "in", "enumerate", "(", "progress", ")", ":", "\n", "        ", "with", "metrics", ".", "aggregate", "(", "\"train_inner\"", ")", ",", "torch", ".", "autograd", ".", "profiler", ".", "record_function", "(", "\n", "\"train_step-%d\"", "%", "i", "\n", ")", ":", "\n", "            ", "log_output", "=", "trainer", ".", "train_step", "(", "samples", ")", "\n", "\n", "", "if", "log_output", "is", "not", "None", ":", "# not OOM, overflow, ...", "\n", "# log mid-epoch stats", "\n", "            ", "num_updates", "=", "trainer", ".", "get_num_updates", "(", ")", "\n", "if", "num_updates", "%", "cfg", ".", "common", ".", "log_interval", "==", "0", ":", "\n", "                ", "stats", "=", "get_training_stats", "(", "metrics", ".", "get_smoothed_values", "(", "\"train_inner\"", ")", ")", "\n", "progress", ".", "log", "(", "stats", ",", "tag", "=", "\"train_inner\"", ",", "step", "=", "num_updates", ")", "\n", "\n", "# reset mid-epoch stats after each log interval", "\n", "# the end-of-epoch stats will still be preserved", "\n", "metrics", ".", "reset_meters", "(", "\"train_inner\"", ")", "\n", "\n", "", "", "end_of_epoch", "=", "not", "itr", ".", "has_next", "(", ")", "\n", "valid_losses", ",", "should_stop", "=", "validate_and_save", "(", "\n", "cfg", ",", "trainer", ",", "task", ",", "epoch_itr", ",", "valid_subsets", ",", "end_of_epoch", "\n", ")", "\n", "\n", "if", "should_stop", ":", "\n", "            ", "break", "\n", "\n", "# log end-of-epoch stats", "\n", "", "", "logger", ".", "info", "(", "\"end of epoch {} (average epoch stats below)\"", ".", "format", "(", "epoch_itr", ".", "epoch", ")", ")", "\n", "stats", "=", "get_training_stats", "(", "metrics", ".", "get_smoothed_values", "(", "\"train\"", ")", ")", "\n", "progress", ".", "print", "(", "stats", ",", "tag", "=", "\"train\"", ",", "step", "=", "num_updates", ")", "\n", "\n", "done_with_epoch", "+=", "1", "\n", "if", "cfg", ".", "distributed_training", ".", "distributed_world_size", ">", "1", ":", "\n", "        ", "torch", ".", "distributed", ".", "all_reduce", "(", "done_with_epoch", ")", "\n", "\n", "# Here's where we mention the numbers r_2, r_3, ..., r_k as mentioned in the algorithm of the paper.", "\n", "", "r", "=", "[", "0.2", ",", "0.1", "]", "\n", "\n", "# n as mentioned in the algorithm of the paper", "\n", "n", "=", "1000", "\n", "\n", "# Change the model_type to 'wav2vec_small' if you're using a base model of wav2vec2", "\n", "model_type", "=", "'libri960_big'", "\n", "\n", "if", "done_with_epoch", "==", "cfg", ".", "distributed_training", ".", "distributed_world_size", ":", "\n", "\n", "        ", "global", "num_prunes", "\n", "\n", "if", "abs", "(", "num_updates", "-", "n", "*", "1", ")", "<", "19", "and", "num_prunes", "==", "1", ":", "\n", "            ", "num_prunes", "+=", "1", "\n", "pruning_bert", "(", "trainer", ".", "model", ",", "r", "[", "0", "]", ",", "model_type", ")", "\n", "unprune_bert", "(", "trainer", ".", "model", ",", "model_type", ")", "\n", "\n", "", "if", "abs", "(", "num_updates", "-", "n", "*", "2", ")", "<", "19", "and", "num_prunes", "==", "2", ":", "\n", "            ", "num_prunes", "+=", "1", "\n", "pruning_bert", "(", "trainer", ".", "model", ",", "r", "[", "1", "]", ",", "model_type", ")", "\n", "unprune_bert", "(", "trainer", ".", "model", ",", "model_type", ")", "\n", "\n", "# reset epoch-level meters", "\n", "", "", "metrics", ".", "reset_meters", "(", "\"train\"", ")", "\n", "return", "valid_losses", ",", "should_stop", "\n", "\n"]], "home.repos.pwc.inspect_result.speech-lab-iitm_pada.None.train._flatten_config": [[537, 548], ["omegaconf.OmegaConf.to_container", "list", "OmegaConf.to_container.items", "isinstance", "vars"], "function", ["None"], ["", "def", "_flatten_config", "(", "cfg", ":", "DictConfig", ")", ":", "\n", "    ", "config", "=", "OmegaConf", ".", "to_container", "(", "cfg", ")", "\n", "# remove any legacy Namespaces and replace with a single \"args\"", "\n", "namespace", "=", "None", "\n", "for", "k", ",", "v", "in", "list", "(", "config", ".", "items", "(", ")", ")", ":", "\n", "        ", "if", "isinstance", "(", "v", ",", "argparse", ".", "Namespace", ")", ":", "\n", "            ", "namespace", "=", "v", "\n", "del", "config", "[", "k", "]", "\n", "", "", "if", "namespace", "is", "not", "None", ":", "\n", "        ", "config", "[", "\"args\"", "]", "=", "vars", "(", "namespace", ")", "\n", "", "return", "config", "\n", "\n"]], "home.repos.pwc.inspect_result.speech-lab-iitm_pada.None.train.validate_and_save": [[550, 622], ["trainer.get_num_updates", "train.should_stop_early", "logger.info", "trainer.cumulative_training_time", "logger.info", "train.validate", "fairseq.checkpoint_utils.save_checkpoint"], "function", ["home.repos.pwc.inspect_result.speech-lab-iitm_pada.None.train.should_stop_early", "home.repos.pwc.inspect_result.speech-lab-iitm_pada.None.train.validate"], ["", "def", "validate_and_save", "(", "\n", "cfg", ":", "DictConfig", ",", "\n", "trainer", ":", "Trainer", ",", "\n", "task", ":", "tasks", ".", "FairseqTask", ",", "\n", "epoch_itr", ",", "\n", "valid_subsets", ":", "List", "[", "str", "]", ",", "\n", "end_of_epoch", ":", "bool", ",", "\n", ")", "->", "Tuple", "[", "List", "[", "Optional", "[", "float", "]", "]", ",", "bool", "]", ":", "\n", "    ", "num_updates", "=", "trainer", ".", "get_num_updates", "(", ")", "\n", "max_update", "=", "cfg", ".", "optimization", ".", "max_update", "or", "math", ".", "inf", "\n", "\n", "# Stopping conditions (and an additional one based on validation loss later", "\n", "# on)", "\n", "should_stop", "=", "False", "\n", "if", "num_updates", ">=", "max_update", ":", "\n", "        ", "should_stop", "=", "True", "\n", "logger", ".", "info", "(", "\n", "f\"Stopping training due to \"", "\n", "f\"num_updates: {num_updates} >= max_update: {max_update}\"", "\n", ")", "\n", "\n", "", "training_time_hours", "=", "trainer", ".", "cumulative_training_time", "(", ")", "/", "(", "60", "*", "60", ")", "\n", "if", "(", "\n", "cfg", ".", "optimization", ".", "stop_time_hours", ">", "0", "\n", "and", "training_time_hours", ">", "cfg", ".", "optimization", ".", "stop_time_hours", "\n", ")", ":", "\n", "        ", "should_stop", "=", "True", "\n", "logger", ".", "info", "(", "\n", "f\"Stopping training due to \"", "\n", "f\"cumulative_training_time: {training_time_hours} > \"", "\n", "f\"stop_time_hours: {cfg.optimization.stop_time_hours} hour(s)\"", "\n", ")", "\n", "\n", "", "do_save", "=", "(", "\n", "(", "end_of_epoch", "and", "epoch_itr", ".", "epoch", "%", "cfg", ".", "checkpoint", ".", "save_interval", "==", "0", ")", "\n", "or", "should_stop", "\n", "or", "(", "\n", "cfg", ".", "checkpoint", ".", "save_interval_updates", ">", "0", "\n", "and", "num_updates", ">", "0", "\n", "and", "num_updates", "%", "cfg", ".", "checkpoint", ".", "save_interval_updates", "==", "0", "\n", "and", "num_updates", ">=", "cfg", ".", "dataset", ".", "validate_after_updates", "\n", ")", "\n", ")", "\n", "do_validate", "=", "(", "\n", "(", "\n", "(", "not", "end_of_epoch", "and", "do_save", ")", "# validate during mid-epoch saves", "\n", "or", "(", "end_of_epoch", "and", "epoch_itr", ".", "epoch", "%", "cfg", ".", "dataset", ".", "validate_interval", "==", "0", ")", "\n", "or", "should_stop", "\n", "or", "(", "\n", "cfg", ".", "dataset", ".", "validate_interval_updates", ">", "0", "\n", "and", "num_updates", ">", "0", "\n", "and", "num_updates", "%", "cfg", ".", "dataset", ".", "validate_interval_updates", "==", "0", "\n", ")", "\n", ")", "\n", "and", "not", "cfg", ".", "dataset", ".", "disable_validation", "\n", "and", "num_updates", ">=", "cfg", ".", "dataset", ".", "validate_after_updates", "\n", ")", "\n", "\n", "# Validate", "\n", "valid_losses", "=", "[", "None", "]", "\n", "if", "do_validate", ":", "\n", "        ", "valid_losses", "=", "validate", "(", "cfg", ",", "trainer", ",", "task", ",", "epoch_itr", ",", "valid_subsets", ")", "\n", "\n", "", "should_stop", "|=", "should_stop_early", "(", "cfg", ",", "valid_losses", "[", "0", "]", ")", "\n", "\n", "# Save checkpoint", "\n", "if", "do_save", "or", "should_stop", ":", "\n", "        ", "checkpoint_utils", ".", "save_checkpoint", "(", "\n", "cfg", ".", "checkpoint", ",", "trainer", ",", "epoch_itr", ",", "valid_losses", "[", "0", "]", "\n", ")", "\n", "\n", "", "return", "valid_losses", ",", "should_stop", "\n", "\n"]], "home.repos.pwc.inspect_result.speech-lab-iitm_pada.None.train.get_training_stats": [[624, 627], ["round", "fairseq.logging.metrics.get_meter"], "function", ["None"], ["", "def", "get_training_stats", "(", "stats", ":", "Dict", "[", "str", ",", "Any", "]", ")", "->", "Dict", "[", "str", ",", "Any", "]", ":", "\n", "    ", "stats", "[", "\"wall\"", "]", "=", "round", "(", "metrics", ".", "get_meter", "(", "\"default\"", ",", "\"wall\"", ")", ".", "elapsed_time", ",", "0", ")", "\n", "return", "stats", "\n", "\n"]], "home.repos.pwc.inspect_result.speech-lab-iitm_pada.None.train.validate": [[629, 696], ["trainer.begin_valid_epoch", "fairseq.utils.set_torch_seed", "logger.info", "trainer.get_valid_iterator().next_epoch_itr", "fairseq.logging.progress_bar.progress_bar", "train.get_valid_stats", "hasattr", "progress_bar.progress_bar.print", "valid_losses.append", "fairseq.utils.tpu_data_loader", "fairseq.logging.metrics.aggregate", "enumerate", "agg.get_smoothed_values", "task.post_validate", "trainer.get_valid_iterator", "os.environ.get", "trainer.valid_step", "trainer.get_model", "trainer.get_num_updates", "fairseq.distributed.utils.is_master", "fairseq.distributed.utils.is_master", "os.path.basename"], "function", ["home.repos.pwc.inspect_result.speech-lab-iitm_pada.None.train.get_valid_stats"], ["", "def", "validate", "(", "\n", "cfg", ":", "DictConfig", ",", "\n", "trainer", ":", "Trainer", ",", "\n", "task", ":", "tasks", ".", "FairseqTask", ",", "\n", "epoch_itr", ",", "\n", "subsets", ":", "List", "[", "str", "]", ",", "\n", ")", "->", "List", "[", "Optional", "[", "float", "]", "]", ":", "\n", "    ", "\"\"\"Evaluate the model on the validation set(s) and return the losses.\"\"\"", "\n", "\n", "if", "cfg", ".", "dataset", ".", "fixed_validation_seed", "is", "not", "None", ":", "\n", "# set fixed seed for every validation", "\n", "        ", "utils", ".", "set_torch_seed", "(", "cfg", ".", "dataset", ".", "fixed_validation_seed", ")", "\n", "\n", "", "trainer", ".", "begin_valid_epoch", "(", "epoch_itr", ".", "epoch", ")", "\n", "valid_losses", "=", "[", "]", "\n", "for", "subset", "in", "subsets", ":", "\n", "        ", "logger", ".", "info", "(", "'begin validation on \"{}\" subset'", ".", "format", "(", "subset", ")", ")", "\n", "\n", "# Initialize data iterator", "\n", "itr", "=", "trainer", ".", "get_valid_iterator", "(", "subset", ")", ".", "next_epoch_itr", "(", "\n", "shuffle", "=", "False", ",", "set_dataset_epoch", "=", "False", "# use a fixed valid set", "\n", ")", "\n", "if", "cfg", ".", "common", ".", "tpu", ":", "\n", "            ", "itr", "=", "utils", ".", "tpu_data_loader", "(", "itr", ")", "\n", "", "progress", "=", "progress_bar", ".", "progress_bar", "(", "\n", "itr", ",", "\n", "log_format", "=", "cfg", ".", "common", ".", "log_format", ",", "\n", "log_interval", "=", "cfg", ".", "common", ".", "log_interval", ",", "\n", "epoch", "=", "epoch_itr", ".", "epoch", ",", "\n", "prefix", "=", "f\"valid on '{subset}' subset\"", ",", "\n", "tensorboard_logdir", "=", "(", "\n", "cfg", ".", "common", ".", "tensorboard_logdir", "\n", "if", "distributed_utils", ".", "is_master", "(", "cfg", ".", "distributed_training", ")", "\n", "else", "None", "\n", ")", ",", "\n", "default_log_format", "=", "(", "\"tqdm\"", "if", "not", "cfg", ".", "common", ".", "no_progress_bar", "else", "\"simple\"", ")", ",", "\n", "wandb_project", "=", "(", "\n", "cfg", ".", "common", ".", "wandb_project", "\n", "if", "distributed_utils", ".", "is_master", "(", "cfg", ".", "distributed_training", ")", "\n", "else", "None", "\n", ")", ",", "\n", "wandb_run_name", "=", "os", ".", "environ", ".", "get", "(", "\n", "\"WANDB_NAME\"", ",", "os", ".", "path", ".", "basename", "(", "cfg", ".", "checkpoint", ".", "save_dir", ")", "\n", ")", ",", "\n", ")", "\n", "\n", "# create a new root metrics aggregator so validation metrics", "\n", "# don't pollute other aggregators (e.g., train meters)", "\n", "with", "metrics", ".", "aggregate", "(", "new_root", "=", "True", ")", "as", "agg", ":", "\n", "            ", "for", "i", ",", "sample", "in", "enumerate", "(", "progress", ")", ":", "\n", "                ", "if", "(", "\n", "cfg", ".", "dataset", ".", "max_valid_steps", "is", "not", "None", "\n", "and", "i", ">", "cfg", ".", "dataset", ".", "max_valid_steps", "\n", ")", ":", "\n", "                    ", "break", "\n", "", "trainer", ".", "valid_step", "(", "sample", ")", "\n", "\n", "# log validation stats", "\n", "", "", "stats", "=", "get_valid_stats", "(", "cfg", ",", "trainer", ",", "agg", ".", "get_smoothed_values", "(", ")", ")", "\n", "\n", "if", "hasattr", "(", "task", ",", "\"post_validate\"", ")", ":", "\n", "            ", "task", ".", "post_validate", "(", "trainer", ".", "get_model", "(", ")", ",", "stats", ",", "agg", ")", "\n", "\n", "", "progress", ".", "print", "(", "stats", ",", "tag", "=", "subset", ",", "step", "=", "trainer", ".", "get_num_updates", "(", ")", ")", "\n", "\n", "valid_losses", ".", "append", "(", "stats", "[", "cfg", ".", "checkpoint", ".", "best_checkpoint_metric", "]", ")", "\n", "", "return", "valid_losses", "\n", "\n"]], "home.repos.pwc.inspect_result.speech-lab-iitm_pada.None.train.get_valid_stats": [[698, 710], ["trainer.get_num_updates", "hasattr", "best_function"], "function", ["None"], ["", "def", "get_valid_stats", "(", "\n", "cfg", ":", "DictConfig", ",", "trainer", ":", "Trainer", ",", "stats", ":", "Dict", "[", "str", ",", "Any", "]", "\n", ")", "->", "Dict", "[", "str", ",", "Any", "]", ":", "\n", "    ", "stats", "[", "\"num_updates\"", "]", "=", "trainer", ".", "get_num_updates", "(", ")", "\n", "if", "hasattr", "(", "checkpoint_utils", ".", "save_checkpoint", ",", "\"best\"", ")", ":", "\n", "        ", "key", "=", "\"best_{0}\"", ".", "format", "(", "cfg", ".", "checkpoint", ".", "best_checkpoint_metric", ")", "\n", "best_function", "=", "max", "if", "cfg", ".", "checkpoint", ".", "maximize_best_checkpoint_metric", "else", "min", "\n", "stats", "[", "key", "]", "=", "best_function", "(", "\n", "checkpoint_utils", ".", "save_checkpoint", ".", "best", ",", "\n", "stats", "[", "cfg", ".", "checkpoint", ".", "best_checkpoint_metric", "]", ",", "\n", ")", "\n", "", "return", "stats", "\n", "\n"]], "home.repos.pwc.inspect_result.speech-lab-iitm_pada.None.train.cli_main": [[712, 732], ["fairseq.options.get_training_parser", "fairseq.options.parse_args_and_arch", "fairseq.dataclass.utils.convert_namespace_to_omegaconf", "fairseq.data.plasma_utils.PlasmaStore", "logger.info", "fairseq.distributed.utils.call_main", "torch.cuda.profiler.profile", "torch.cuda.profiler.profile", "torch.autograd.profiler.emit_nvtx", "torch.autograd.profiler.emit_nvtx", "fairseq.distributed.utils.call_main"], "function", ["None"], ["", "def", "cli_main", "(", "\n", "modify_parser", ":", "Optional", "[", "Callable", "[", "[", "argparse", ".", "ArgumentParser", "]", ",", "None", "]", "]", "=", "None", "\n", ")", "->", "None", ":", "\n", "    ", "parser", "=", "options", ".", "get_training_parser", "(", ")", "\n", "args", "=", "options", ".", "parse_args_and_arch", "(", "parser", ",", "modify_parser", "=", "modify_parser", ")", "\n", "\n", "cfg", "=", "convert_namespace_to_omegaconf", "(", "args", ")", "\n", "\n", "if", "cfg", ".", "common", ".", "use_plasma_view", ":", "\n", "        ", "server", "=", "PlasmaStore", "(", "path", "=", "cfg", ".", "common", ".", "plasma_path", ")", "\n", "logger", ".", "info", "(", "\n", "f\"Started plasma server pid {server.server.pid} {cfg.common.plasma_path}\"", "\n", ")", "\n", "\n", "", "if", "args", ".", "profile", ":", "\n", "        ", "with", "torch", ".", "cuda", ".", "profiler", ".", "profile", "(", ")", ":", "\n", "            ", "with", "torch", ".", "autograd", ".", "profiler", ".", "emit_nvtx", "(", ")", ":", "\n", "                ", "distributed_utils", ".", "call_main", "(", "cfg", ",", "main", ")", "\n", "", "", "", "else", ":", "\n", "        ", "distributed_utils", ".", "call_main", "(", "cfg", ",", "main", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.speech-lab-iitm_pada.None.infer.ExistingEmissionsDecoder.__init__": [[202, 205], ["None"], "methods", ["None"], ["    ", "def", "__init__", "(", "self", ",", "decoder", ",", "emissions", ")", ":", "\n", "        ", "self", ".", "decoder", "=", "decoder", "\n", "self", ".", "emissions", "=", "emissions", "\n", "\n"]], "home.repos.pwc.inspect_result.speech-lab-iitm_pada.None.infer.ExistingEmissionsDecoder.generate": [[206, 215], ["sample[].cpu().numpy", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "infer.ExistingEmissionsDecoder.decoder.decode", "numpy.stack", "sample[].cpu", "print", "Exception"], "methods", ["None"], ["", "def", "generate", "(", "self", ",", "models", ",", "sample", ",", "**", "unused", ")", ":", "\n", "        ", "ids", "=", "sample", "[", "\"id\"", "]", ".", "cpu", "(", ")", ".", "numpy", "(", ")", "\n", "try", ":", "\n", "            ", "emissions", "=", "np", ".", "stack", "(", "self", ".", "emissions", "[", "ids", "]", ")", "\n", "", "except", ":", "\n", "            ", "print", "(", "[", "x", ".", "shape", "for", "x", "in", "self", ".", "emissions", "[", "ids", "]", "]", ")", "\n", "raise", "Exception", "(", "\"invalid sizes\"", ")", "\n", "", "emissions", "=", "torch", ".", "from_numpy", "(", "emissions", ")", "\n", "return", "self", ".", "decoder", ".", "decode", "(", "emissions", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.speech-lab-iitm_pada.None.infer.add_asr_eval_argument": [[32, 88], ["parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument"], "function", ["None"], ["def", "add_asr_eval_argument", "(", "parser", ")", ":", "\n", "    ", "parser", ".", "add_argument", "(", "\"--kspmodel\"", ",", "default", "=", "None", ",", "help", "=", "\"sentence piece model\"", ")", "\n", "parser", ".", "add_argument", "(", "\n", "\"--wfstlm\"", ",", "default", "=", "None", ",", "help", "=", "\"wfstlm on dictonary output units\"", "\n", ")", "\n", "parser", ".", "add_argument", "(", "\n", "\"--rnnt_decoding_type\"", ",", "\n", "default", "=", "\"greedy\"", ",", "\n", "help", "=", "\"wfstlm on dictonary\\\noutput units\"", ",", "\n", ")", "\n", "try", ":", "\n", "        ", "parser", ".", "add_argument", "(", "\n", "\"--lm-weight\"", ",", "\n", "\"--lm_weight\"", ",", "\n", "type", "=", "float", ",", "\n", "default", "=", "0.2", ",", "\n", "help", "=", "\"weight for lm while interpolating with neural score\"", ",", "\n", ")", "\n", "", "except", ":", "\n", "        ", "pass", "\n", "", "parser", ".", "add_argument", "(", "\n", "\"--rnnt_len_penalty\"", ",", "default", "=", "-", "0.5", ",", "help", "=", "\"rnnt length penalty on word level\"", "\n", ")", "\n", "parser", ".", "add_argument", "(", "\n", "\"--w2l-decoder\"", ",", "\n", "choices", "=", "[", "\"viterbi\"", ",", "\"kenlm\"", ",", "\"fairseqlm\"", "]", ",", "\n", "help", "=", "\"use a w2l decoder\"", ",", "\n", ")", "\n", "parser", ".", "add_argument", "(", "\"--lexicon\"", ",", "help", "=", "\"lexicon for w2l decoder\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--unit-lm\"", ",", "action", "=", "\"store_true\"", ",", "help", "=", "\"if using a unit lm\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--kenlm-model\"", ",", "\"--lm-model\"", ",", "help", "=", "\"lm model for w2l decoder\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--beam-threshold\"", ",", "type", "=", "float", ",", "default", "=", "25.0", ")", "\n", "parser", ".", "add_argument", "(", "\"--beam-size-token\"", ",", "type", "=", "float", ",", "default", "=", "100", ")", "\n", "parser", ".", "add_argument", "(", "\"--word-score\"", ",", "type", "=", "float", ",", "default", "=", "1.0", ")", "\n", "parser", ".", "add_argument", "(", "\"--unk-weight\"", ",", "type", "=", "float", ",", "default", "=", "-", "math", ".", "inf", ")", "\n", "parser", ".", "add_argument", "(", "\"--sil-weight\"", ",", "type", "=", "float", ",", "default", "=", "0.0", ")", "\n", "parser", ".", "add_argument", "(", "\n", "\"--dump-emissions\"", ",", "\n", "type", "=", "str", ",", "\n", "default", "=", "None", ",", "\n", "help", "=", "\"if present, dumps emissions into this file and exits\"", ",", "\n", ")", "\n", "parser", ".", "add_argument", "(", "\n", "\"--dump-features\"", ",", "\n", "type", "=", "str", ",", "\n", "default", "=", "None", ",", "\n", "help", "=", "\"if present, dumps features into this file and exits\"", ",", "\n", ")", "\n", "parser", ".", "add_argument", "(", "\n", "\"--load-emissions\"", ",", "\n", "type", "=", "str", ",", "\n", "default", "=", "None", ",", "\n", "help", "=", "\"if present, loads emissions from this file\"", ",", "\n", ")", "\n", "return", "parser", "\n", "\n"]], "home.repos.pwc.inspect_result.speech-lab-iitm_pada.None.infer.check_args": [[90, 99], ["None"], "function", ["None"], ["", "def", "check_args", "(", "args", ")", ":", "\n", "# assert args.path is not None, \"--path required for generation!\"", "\n", "# assert args.results_path is not None, \"--results_path required for generation!\"", "\n", "    ", "assert", "(", "\n", "not", "args", ".", "sampling", "or", "args", ".", "nbest", "==", "args", ".", "beam", "\n", ")", ",", "\"--sampling requires --nbest to be equal to --beam\"", "\n", "assert", "(", "\n", "args", ".", "replace_unk", "is", "None", "or", "args", ".", "raw_text", "\n", ")", ",", "\"--replace-unk requires a raw text dataset (--raw-text)\"", "\n", "\n"]], "home.repos.pwc.inspect_result.speech-lab-iitm_pada.None.infer.get_dataset_itr": [[101, 114], ["task.get_batch_iterator().next_epoch_itr", "task.get_batch_iterator", "task.dataset"], "function", ["None"], ["", "def", "get_dataset_itr", "(", "args", ",", "task", ",", "models", ")", ":", "\n", "    ", "return", "task", ".", "get_batch_iterator", "(", "\n", "dataset", "=", "task", ".", "dataset", "(", "args", ".", "gen_subset", ")", ",", "\n", "max_tokens", "=", "args", ".", "max_tokens", ",", "\n", "max_sentences", "=", "args", ".", "batch_size", ",", "\n", "max_positions", "=", "(", "sys", ".", "maxsize", ",", "sys", ".", "maxsize", ")", ",", "\n", "ignore_invalid_inputs", "=", "args", ".", "skip_invalid_size_inputs_valid_test", ",", "\n", "required_batch_size_multiple", "=", "args", ".", "required_batch_size_multiple", ",", "\n", "num_shards", "=", "args", ".", "num_shards", ",", "\n", "shard_id", "=", "args", ".", "shard_id", ",", "\n", "num_workers", "=", "args", ".", "num_workers", ",", "\n", "data_buffer_size", "=", "args", ".", "data_buffer_size", ",", "\n", ")", ".", "next_epoch_itr", "(", "shuffle", "=", "False", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.speech-lab-iitm_pada.None.infer.process_predictions": [[116, 157], ["tgt_dict.string", "tgt_dict.string", "fairseq.data.data_utils.post_process", "fairseq.data.data_utils.post_process.split", "tgt_words.split.split", "min", "hypo[].int().cpu", "fairseq.data.data_utils.post_process", "print", "print", "print", "print", "logger.info", "logger.info", "logger.info", "editdistance.eval", "len", "len", "hypo[].int"], "function", ["None"], ["", "def", "process_predictions", "(", "\n", "args", ",", "hypos", ",", "sp", ",", "tgt_dict", ",", "target_tokens", ",", "res_files", ",", "speaker", ",", "id", "\n", ")", ":", "\n", "    ", "for", "hypo", "in", "hypos", "[", ":", "min", "(", "len", "(", "hypos", ")", ",", "args", ".", "nbest", ")", "]", ":", "\n", "        ", "hyp_pieces", "=", "tgt_dict", ".", "string", "(", "hypo", "[", "\"tokens\"", "]", ".", "int", "(", ")", ".", "cpu", "(", ")", ")", "\n", "\n", "if", "\"words\"", "in", "hypo", ":", "\n", "            ", "hyp_words", "=", "\" \"", ".", "join", "(", "hypo", "[", "\"words\"", "]", ")", "\n", "", "else", ":", "\n", "            ", "hyp_words", "=", "post_process", "(", "hyp_pieces", ",", "args", ".", "post_process", ")", "\n", "\n", "", "if", "res_files", "is", "not", "None", ":", "\n", "            ", "print", "(", "\n", "\"{} ({}-{})\"", ".", "format", "(", "hyp_pieces", ",", "speaker", ",", "id", ")", ",", "\n", "file", "=", "res_files", "[", "\"hypo.units\"", "]", ",", "\n", ")", "\n", "print", "(", "\n", "\"{} ({}-{})\"", ".", "format", "(", "hyp_words", ",", "speaker", ",", "id", ")", ",", "\n", "file", "=", "res_files", "[", "\"hypo.words\"", "]", ",", "\n", ")", "\n", "\n", "", "tgt_pieces", "=", "tgt_dict", ".", "string", "(", "target_tokens", ")", "\n", "tgt_words", "=", "post_process", "(", "tgt_pieces", ",", "args", ".", "post_process", ")", "\n", "\n", "if", "res_files", "is", "not", "None", ":", "\n", "            ", "print", "(", "\n", "\"{} ({}-{})\"", ".", "format", "(", "tgt_pieces", ",", "speaker", ",", "id", ")", ",", "\n", "file", "=", "res_files", "[", "\"ref.units\"", "]", ",", "\n", ")", "\n", "print", "(", "\n", "\"{} ({}-{})\"", ".", "format", "(", "tgt_words", ",", "speaker", ",", "id", ")", ",", "file", "=", "res_files", "[", "\"ref.words\"", "]", "\n", ")", "\n", "\n", "", "if", "not", "args", ".", "quiet", ":", "\n", "            ", "logger", ".", "info", "(", "\"HYPO:\"", "+", "hyp_words", ")", "\n", "logger", ".", "info", "(", "\"TARGET:\"", "+", "tgt_words", ")", "\n", "logger", ".", "info", "(", "\"___________________\"", ")", "\n", "\n", "", "hyp_words", "=", "hyp_words", ".", "split", "(", ")", "\n", "tgt_words", "=", "tgt_words", ".", "split", "(", ")", "\n", "return", "editdistance", ".", "eval", "(", "hyp_words", ",", "tgt_words", ")", ",", "len", "(", "tgt_words", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.speech-lab-iitm_pada.None.infer.prepare_result_files": [[159, 179], ["os.path.join", "open", "infer.prepare_result_files.get_res_file"], "function", ["None"], ["", "", "def", "prepare_result_files", "(", "args", ")", ":", "\n", "    ", "def", "get_res_file", "(", "file_prefix", ")", ":", "\n", "        ", "if", "args", ".", "num_shards", ">", "1", ":", "\n", "            ", "file_prefix", "=", "f\"{args.shard_id}_{file_prefix}\"", "\n", "", "path", "=", "os", ".", "path", ".", "join", "(", "\n", "args", ".", "results_path", ",", "\n", "\"{}-{}-{}.txt\"", ".", "format", "(", "\n", "file_prefix", ",", "os", ".", "path", ".", "basename", "(", "args", ".", "path", ")", ",", "args", ".", "gen_subset", "\n", ")", ",", "\n", ")", "\n", "return", "open", "(", "path", ",", "\"w\"", ",", "buffering", "=", "1", ")", "\n", "\n", "", "if", "not", "args", ".", "results_path", ":", "\n", "        ", "return", "None", "\n", "\n", "", "return", "{", "\n", "\"hypo.words\"", ":", "get_res_file", "(", "\"hypo.word\"", ")", ",", "\n", "\"hypo.units\"", ":", "get_res_file", "(", "\"hypo.units\"", ")", ",", "\n", "\"ref.words\"", ":", "get_res_file", "(", "\"ref.word\"", ")", ",", "\n", "\"ref.units\"", ":", "get_res_file", "(", "\"ref.units\"", ")", ",", "\n", "}", "\n"]], "home.repos.pwc.inspect_result.speech-lab-iitm_pada.None.infer.optimize_models": [[182, 193], ["model.make_generation_fast_", "model.half", "model.cuda"], "function", ["None"], ["", "def", "optimize_models", "(", "args", ",", "use_cuda", ",", "models", ")", ":", "\n", "    ", "\"\"\"Optimize ensemble for generation\"\"\"", "\n", "for", "model", "in", "models", ":", "\n", "        ", "model", ".", "make_generation_fast_", "(", "\n", "beamable_mm_beam_size", "=", "None", "if", "args", ".", "no_beamable_mm", "else", "args", ".", "beam", ",", "\n", "need_attn", "=", "args", ".", "print_alignment", ",", "\n", ")", "\n", "if", "args", ".", "fp16", ":", "\n", "            ", "model", ".", "half", "(", ")", "\n", "", "if", "use_cuda", ":", "\n", "            ", "model", ".", "cuda", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.speech-lab-iitm_pada.None.infer.apply_half": [[195, 199], ["t.to"], "function", ["None"], ["", "", "", "def", "apply_half", "(", "t", ")", ":", "\n", "    ", "if", "t", ".", "dtype", "is", "torch", ".", "float32", ":", "\n", "        ", "return", "t", ".", "to", "(", "dtype", "=", "torch", ".", "half", ")", "\n", "", "return", "t", "\n", "\n"]], "home.repos.pwc.inspect_result.speech-lab-iitm_pada.None.infer.pruning_bert_fairseq": [[216, 252], ["range", "tuple", "torch.global_unstructured", "tuple.append", "tuple.append", "tuple.append", "tuple.append", "tuple.append", "tuple.append", "tuple.append", "tuple.append", "tuple.append", "tuple.append", "tuple.append", "tuple.append", "print"], "function", ["None"], ["", "", "def", "pruning_bert_fairseq", "(", "model", ",", "px", ",", "model_type", "=", "'wav2vec_small'", ")", ":", "\n", "    ", "\"\"\"\n    prune out wav2vec 2.0 BERT: 12 transformer layers for BASE, and 24 \n                                transformer layers for LARGE\n\n    note: position encoding, projection heads, layernorm statistics are not pruned. \n    \"\"\"", "\n", "if", "model_type", "==", "'wav2vec_small'", ":", "\n", "        ", "num_transformer_blocks", "=", "12", "\n", "", "elif", "model_type", "==", "'libri960_big'", "or", "model_type", "==", "'xlsr_53_56k'", ":", "\n", "        ", "num_transformer_blocks", "=", "24", "\n", "", "else", ":", "\n", "        ", "print", "(", "'model type {} not supported'", ".", "format", "(", "model_type", ")", ")", "\n", "# print('num_transformer_blocks is', num_transformer_blocks)", "\n", "\n", "", "parameters_to_prune", "=", "[", "]", "\n", "for", "ii", "in", "range", "(", "num_transformer_blocks", ")", ":", "\n", "        ", "parameters_to_prune", ".", "append", "(", "(", "model", ".", "w2v_model", ".", "encoder", ".", "layers", "[", "ii", "]", ".", "self_attn", ".", "k_proj", ",", "'weight'", ")", ")", "\n", "parameters_to_prune", ".", "append", "(", "(", "model", ".", "w2v_model", ".", "encoder", ".", "layers", "[", "ii", "]", ".", "self_attn", ".", "k_proj", ",", "'bias'", ")", ")", "\n", "parameters_to_prune", ".", "append", "(", "(", "model", ".", "w2v_model", ".", "encoder", ".", "layers", "[", "ii", "]", ".", "self_attn", ".", "v_proj", ",", "'weight'", ")", ")", "\n", "parameters_to_prune", ".", "append", "(", "(", "model", ".", "w2v_model", ".", "encoder", ".", "layers", "[", "ii", "]", ".", "self_attn", ".", "v_proj", ",", "'bias'", ")", ")", "\n", "parameters_to_prune", ".", "append", "(", "(", "model", ".", "w2v_model", ".", "encoder", ".", "layers", "[", "ii", "]", ".", "self_attn", ".", "q_proj", ",", "'weight'", ")", ")", "\n", "parameters_to_prune", ".", "append", "(", "(", "model", ".", "w2v_model", ".", "encoder", ".", "layers", "[", "ii", "]", ".", "self_attn", ".", "q_proj", ",", "'bias'", ")", ")", "\n", "parameters_to_prune", ".", "append", "(", "(", "model", ".", "w2v_model", ".", "encoder", ".", "layers", "[", "ii", "]", ".", "self_attn", ".", "out_proj", ",", "'weight'", ")", ")", "\n", "parameters_to_prune", ".", "append", "(", "(", "model", ".", "w2v_model", ".", "encoder", ".", "layers", "[", "ii", "]", ".", "self_attn", ".", "out_proj", ",", "'bias'", ")", ")", "\n", "parameters_to_prune", ".", "append", "(", "(", "model", ".", "w2v_model", ".", "encoder", ".", "layers", "[", "ii", "]", ".", "fc1", ",", "'weight'", ")", ")", "\n", "parameters_to_prune", ".", "append", "(", "(", "model", ".", "w2v_model", ".", "encoder", ".", "layers", "[", "ii", "]", ".", "fc1", ",", "'bias'", ")", ")", "\n", "parameters_to_prune", ".", "append", "(", "(", "model", ".", "w2v_model", ".", "encoder", ".", "layers", "[", "ii", "]", ".", "fc2", ",", "'weight'", ")", ")", "\n", "parameters_to_prune", ".", "append", "(", "(", "model", ".", "w2v_model", ".", "encoder", ".", "layers", "[", "ii", "]", ".", "fc2", ",", "'bias'", ")", ")", "\n", "\n", "", "parameters_to_prune", "=", "tuple", "(", "parameters_to_prune", ")", "\n", "\n", "prune", ".", "global_unstructured", "(", "\n", "parameters_to_prune", ",", "\n", "pruning_method", "=", "prune", ".", "L1Unstructured", ",", "\n", "amount", "=", "px", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.speech-lab-iitm_pada.None.infer.see_weight_rate": [[254, 300], ["range", "print", "float", "float", "float", "float", "float", "float", "float", "float", "float", "float", "float", "float", "float", "float", "float", "float", "float", "float", "float", "float", "float", "float", "float", "float", "model.w2v_model.encoder.layers[].self_attn.k_proj.weight.nelement", "torch.sum", "torch.sum", "model.w2v_model.encoder.layers[].self_attn.k_proj.bias.nelement", "torch.sum", "torch.sum", "model.w2v_model.encoder.layers[].self_attn.v_proj.weight.nelement", "torch.sum", "torch.sum", "model.w2v_model.encoder.layers[].self_attn.v_proj.bias.nelement", "torch.sum", "torch.sum", "model.w2v_model.encoder.layers[].self_attn.q_proj.weight.nelement", "torch.sum", "torch.sum", "model.w2v_model.encoder.layers[].self_attn.q_proj.bias.nelement", "torch.sum", "torch.sum", "model.w2v_model.encoder.layers[].self_attn.out_proj.weight.nelement", "torch.sum", "torch.sum", "model.w2v_model.encoder.layers[].self_attn.out_proj.bias.nelement", "torch.sum", "torch.sum", "model.w2v_model.encoder.layers[].fc1.weight.nelement", "torch.sum", "torch.sum", "model.w2v_model.encoder.layers[].fc1.bias.nelement", "torch.sum", "torch.sum", "model.w2v_model.encoder.layers[].fc2.weight.nelement", "torch.sum", "torch.sum", "model.w2v_model.encoder.layers[].fc2.bias.nelement", "torch.sum", "torch.sum"], "function", ["None"], ["", "def", "see_weight_rate", "(", "model", ",", "model_type", "=", "'wav2vec_small'", ")", ":", "\n", "    ", "\"\"\" check a model's zero rate \n    \"\"\"", "\n", "if", "model_type", "==", "'wav2vec_small'", ":", "\n", "        ", "num_transformer_blocks", "=", "12", "\n", "", "elif", "model_type", "==", "'libri960_big'", "or", "model_type", "==", "'xlsr_53_56k'", ":", "\n", "        ", "num_transformer_blocks", "=", "24", "\n", "", "else", ":", "\n", "        ", "print", "(", "'model type {} not supported'", ".", "format", "(", "model_type", ")", ")", "\n", "# print('num_transformer_blocks is', num_transformer_blocks)", "\n", "\n", "", "sum_list_2", ",", "zero_sum_2", "=", "0", ",", "0", "\n", "for", "ii", "in", "range", "(", "num_transformer_blocks", ")", ":", "\n", "        ", "sum_list_2", "=", "sum_list_2", "+", "float", "(", "model", ".", "w2v_model", ".", "encoder", ".", "layers", "[", "ii", "]", ".", "self_attn", ".", "k_proj", ".", "weight", ".", "nelement", "(", ")", ")", "\n", "zero_sum_2", "=", "zero_sum_2", "+", "float", "(", "torch", ".", "sum", "(", "model", ".", "w2v_model", ".", "encoder", ".", "layers", "[", "ii", "]", ".", "self_attn", ".", "k_proj", ".", "weight", "==", "0", ")", ")", "\n", "sum_list_2", "=", "sum_list_2", "+", "float", "(", "model", ".", "w2v_model", ".", "encoder", ".", "layers", "[", "ii", "]", ".", "self_attn", ".", "k_proj", ".", "bias", ".", "nelement", "(", ")", ")", "\n", "zero_sum_2", "=", "zero_sum_2", "+", "float", "(", "torch", ".", "sum", "(", "model", ".", "w2v_model", ".", "encoder", ".", "layers", "[", "ii", "]", ".", "self_attn", ".", "k_proj", ".", "bias", "==", "0", ")", ")", "\n", "\n", "sum_list_2", "=", "sum_list_2", "+", "float", "(", "model", ".", "w2v_model", ".", "encoder", ".", "layers", "[", "ii", "]", ".", "self_attn", ".", "v_proj", ".", "weight", ".", "nelement", "(", ")", ")", "\n", "zero_sum_2", "=", "zero_sum_2", "+", "float", "(", "torch", ".", "sum", "(", "model", ".", "w2v_model", ".", "encoder", ".", "layers", "[", "ii", "]", ".", "self_attn", ".", "v_proj", ".", "weight", "==", "0", ")", ")", "\n", "sum_list_2", "=", "sum_list_2", "+", "float", "(", "model", ".", "w2v_model", ".", "encoder", ".", "layers", "[", "ii", "]", ".", "self_attn", ".", "v_proj", ".", "bias", ".", "nelement", "(", ")", ")", "\n", "zero_sum_2", "=", "zero_sum_2", "+", "float", "(", "torch", ".", "sum", "(", "model", ".", "w2v_model", ".", "encoder", ".", "layers", "[", "ii", "]", ".", "self_attn", ".", "v_proj", ".", "bias", "==", "0", ")", ")", "\n", "\n", "sum_list_2", "=", "sum_list_2", "+", "float", "(", "model", ".", "w2v_model", ".", "encoder", ".", "layers", "[", "ii", "]", ".", "self_attn", ".", "q_proj", ".", "weight", ".", "nelement", "(", ")", ")", "\n", "zero_sum_2", "=", "zero_sum_2", "+", "float", "(", "torch", ".", "sum", "(", "model", ".", "w2v_model", ".", "encoder", ".", "layers", "[", "ii", "]", ".", "self_attn", ".", "q_proj", ".", "weight", "==", "0", ")", ")", "\n", "sum_list_2", "=", "sum_list_2", "+", "float", "(", "model", ".", "w2v_model", ".", "encoder", ".", "layers", "[", "ii", "]", ".", "self_attn", ".", "q_proj", ".", "bias", ".", "nelement", "(", ")", ")", "\n", "zero_sum_2", "=", "zero_sum_2", "+", "float", "(", "torch", ".", "sum", "(", "model", ".", "w2v_model", ".", "encoder", ".", "layers", "[", "ii", "]", ".", "self_attn", ".", "q_proj", ".", "bias", "==", "0", ")", ")", "\n", "\n", "sum_list_2", "=", "sum_list_2", "+", "float", "(", "model", ".", "w2v_model", ".", "encoder", ".", "layers", "[", "ii", "]", ".", "self_attn", ".", "out_proj", ".", "weight", ".", "nelement", "(", ")", ")", "\n", "zero_sum_2", "=", "zero_sum_2", "+", "float", "(", "torch", ".", "sum", "(", "model", ".", "w2v_model", ".", "encoder", ".", "layers", "[", "ii", "]", ".", "self_attn", ".", "out_proj", ".", "weight", "==", "0", ")", ")", "\n", "sum_list_2", "=", "sum_list_2", "+", "float", "(", "model", ".", "w2v_model", ".", "encoder", ".", "layers", "[", "ii", "]", ".", "self_attn", ".", "out_proj", ".", "bias", ".", "nelement", "(", ")", ")", "\n", "zero_sum_2", "=", "zero_sum_2", "+", "float", "(", "torch", ".", "sum", "(", "model", ".", "w2v_model", ".", "encoder", ".", "layers", "[", "ii", "]", ".", "self_attn", ".", "out_proj", ".", "bias", "==", "0", ")", ")", "\n", "\n", "sum_list_2", "=", "sum_list_2", "+", "float", "(", "model", ".", "w2v_model", ".", "encoder", ".", "layers", "[", "ii", "]", ".", "fc1", ".", "weight", ".", "nelement", "(", ")", ")", "\n", "zero_sum_2", "=", "zero_sum_2", "+", "float", "(", "torch", ".", "sum", "(", "model", ".", "w2v_model", ".", "encoder", ".", "layers", "[", "ii", "]", ".", "fc1", ".", "weight", "==", "0", ")", ")", "\n", "sum_list_2", "=", "sum_list_2", "+", "float", "(", "model", ".", "w2v_model", ".", "encoder", ".", "layers", "[", "ii", "]", ".", "fc1", ".", "bias", ".", "nelement", "(", ")", ")", "\n", "zero_sum_2", "=", "zero_sum_2", "+", "float", "(", "torch", ".", "sum", "(", "model", ".", "w2v_model", ".", "encoder", ".", "layers", "[", "ii", "]", ".", "fc1", ".", "bias", "==", "0", ")", ")", "\n", "\n", "sum_list_2", "=", "sum_list_2", "+", "float", "(", "model", ".", "w2v_model", ".", "encoder", ".", "layers", "[", "ii", "]", ".", "fc2", ".", "weight", ".", "nelement", "(", ")", ")", "\n", "zero_sum_2", "=", "zero_sum_2", "+", "float", "(", "torch", ".", "sum", "(", "model", ".", "w2v_model", ".", "encoder", ".", "layers", "[", "ii", "]", ".", "fc2", ".", "weight", "==", "0", ")", ")", "\n", "sum_list_2", "=", "sum_list_2", "+", "float", "(", "model", ".", "w2v_model", ".", "encoder", ".", "layers", "[", "ii", "]", ".", "fc2", ".", "bias", ".", "nelement", "(", ")", ")", "\n", "zero_sum_2", "=", "zero_sum_2", "+", "float", "(", "torch", ".", "sum", "(", "model", ".", "w2v_model", ".", "encoder", ".", "layers", "[", "ii", "]", ".", "fc2", ".", "bias", "==", "0", ")", ")", "\n", "\n", "", "bert_zero_rate", "=", "100", "*", "zero_sum_2", "/", "sum_list_2", "\n", "# print('BERT zero rate is {0:.2f}'.format(bert_zero_rate))", "\n", "return", "bert_zero_rate", "\n", "\n"]], "home.repos.pwc.inspect_result.speech-lab-iitm_pada.None.infer.main": [[301, 527], ["infer.check_args", "logger.info", "logger.info", "fairseq.tasks.setup_task", "logger.info", "infer.get_dataset_itr", "fairseq.logging.meters.StopwatchMeter", "infer.main.build_generator"], "function", ["home.repos.pwc.inspect_result.speech-lab-iitm_pada.None.infer.check_args", "home.repos.pwc.inspect_result.speech-lab-iitm_pada.None.infer.get_dataset_itr"], ["", "def", "main", "(", "args", ",", "task", "=", "None", ",", "model_state", "=", "None", ")", ":", "\n", "    ", "check_args", "(", "args", ")", "\n", "\n", "use_fp16", "=", "args", ".", "fp16", "\n", "if", "args", ".", "max_tokens", "is", "None", "and", "args", ".", "batch_size", "is", "None", ":", "\n", "        ", "args", ".", "max_tokens", "=", "4000000", "\n", "", "logger", ".", "info", "(", "args", ")", "\n", "\n", "use_cuda", "=", "torch", ".", "cuda", ".", "is_available", "(", ")", "and", "not", "args", ".", "cpu", "\n", "\n", "logger", ".", "info", "(", "\"| decoding with criterion {}\"", ".", "format", "(", "args", ".", "criterion", ")", ")", "\n", "\n", "task", "=", "tasks", ".", "setup_task", "(", "args", ")", "\n", "\n", "# Load ensemble", "\n", "if", "args", ".", "load_emissions", ":", "\n", "        ", "models", ",", "criterions", "=", "[", "]", ",", "[", "]", "\n", "task", ".", "load_dataset", "(", "args", ".", "gen_subset", ")", "\n", "", "else", ":", "\n", "        ", "logger", ".", "info", "(", "\"| loading model(s) from {}\"", ".", "format", "(", "args", ".", "path", ")", ")", "\n", "models", ",", "saved_cfg", ",", "task", "=", "checkpoint_utils", ".", "load_model_ensemble_and_task", "(", "\n", "utils", ".", "split_paths", "(", "args", ".", "path", ",", "separator", "=", "\"\\\\\"", ")", ",", "\n", "arg_overrides", "=", "ast", ".", "literal_eval", "(", "args", ".", "model_overrides", ")", ",", "\n", "task", "=", "task", ",", "\n", "suffix", "=", "args", ".", "checkpoint_suffix", ",", "\n", "strict", "=", "(", "args", ".", "checkpoint_shard_count", "==", "1", ")", ",", "\n", "num_shards", "=", "args", ".", "checkpoint_shard_count", ",", "\n", "state", "=", "model_state", ",", "\n", ")", "\n", "\n", "# a 0.3 prune_rate refers to the pruning percentage of 30%. The least 30% of the weights in terms of magnitude are marked for zeroing out.", "\n", "prune_rate", "=", "0.3", "\n", "\n", "# Change the model_type to 'wav2vec_small' if you're using a base model of wav2vec2", "\n", "pruning_bert_fairseq", "(", "models", "[", "0", "]", ".", "w2v_encoder", ",", "prune_rate", ",", "model_type", "=", "'libri960_big'", ")", "\n", "\n", "# You may save the state_dict carrying the mask, with a name of your choice.", "\n", "torch", ".", "save", "(", "models", "[", "0", "]", ".", "state_dict", "(", ")", ",", "'960_30_large_libri_ft_model_state_dict.pth'", ")", "\n", "\n", "optimize_models", "(", "args", ",", "use_cuda", ",", "models", ")", "\n", "task", ".", "load_dataset", "(", "args", ".", "gen_subset", ",", "task_cfg", "=", "saved_cfg", ".", "task", ")", "\n", "\n", "\n", "# Set dictionary", "\n", "", "tgt_dict", "=", "task", ".", "target_dictionary", "\n", "\n", "logger", ".", "info", "(", "\n", "\"| {} {} {} examples\"", ".", "format", "(", "\n", "args", ".", "data", ",", "args", ".", "gen_subset", ",", "len", "(", "task", ".", "dataset", "(", "args", ".", "gen_subset", ")", ")", "\n", ")", "\n", ")", "\n", "\n", "# hack to pass transitions to W2lDecoder", "\n", "if", "args", ".", "criterion", "==", "\"asg_loss\"", ":", "\n", "        ", "raise", "NotImplementedError", "(", "\"asg_loss is currently not supported\"", ")", "\n", "# trans = criterions[0].asg.trans.data", "\n", "# args.asg_transitions = torch.flatten(trans).tolist()", "\n", "\n", "# Load dataset (possibly sharded)", "\n", "", "itr", "=", "get_dataset_itr", "(", "args", ",", "task", ",", "models", ")", "\n", "\n", "# Initialize generator", "\n", "gen_timer", "=", "StopwatchMeter", "(", ")", "\n", "\n", "def", "build_generator", "(", "args", ")", ":", "\n", "        ", "w2l_decoder", "=", "getattr", "(", "args", ",", "\"w2l_decoder\"", ",", "None", ")", "\n", "if", "w2l_decoder", "==", "\"viterbi\"", ":", "\n", "            ", "from", "examples", ".", "speech_recognition", ".", "w2l_decoder", "import", "W2lViterbiDecoder", "\n", "\n", "return", "W2lViterbiDecoder", "(", "args", ",", "task", ".", "target_dictionary", ")", "\n", "", "elif", "w2l_decoder", "==", "\"kenlm\"", ":", "\n", "            ", "from", "examples", ".", "speech_recognition", ".", "w2l_decoder", "import", "W2lKenLMDecoder", "\n", "\n", "return", "W2lKenLMDecoder", "(", "args", ",", "task", ".", "target_dictionary", ")", "\n", "", "elif", "w2l_decoder", "==", "\"fairseqlm\"", ":", "\n", "            ", "from", "examples", ".", "speech_recognition", ".", "w2l_decoder", "import", "W2lFairseqLMDecoder", "\n", "\n", "return", "W2lFairseqLMDecoder", "(", "args", ",", "task", ".", "target_dictionary", ")", "\n", "", "else", ":", "\n", "            ", "print", "(", "\n", "\"only flashlight decoders with (viterbi, kenlm, fairseqlm) options are supported at the moment\"", "\n", ")", "\n", "\n", "# please do not touch this unless you test both generate.py and infer.py with audio_pretraining task", "\n", "", "", "generator", "=", "build_generator", "(", "args", ")", "\n", "\n", "if", "args", ".", "load_emissions", ":", "\n", "        ", "generator", "=", "ExistingEmissionsDecoder", "(", "\n", "generator", ",", "np", ".", "load", "(", "args", ".", "load_emissions", ",", "allow_pickle", "=", "True", ")", "\n", ")", "\n", "logger", ".", "info", "(", "\"loaded emissions from \"", "+", "args", ".", "load_emissions", ")", "\n", "\n", "", "num_sentences", "=", "0", "\n", "\n", "if", "args", ".", "results_path", "is", "not", "None", "and", "not", "os", ".", "path", ".", "exists", "(", "args", ".", "results_path", ")", ":", "\n", "        ", "os", ".", "makedirs", "(", "args", ".", "results_path", ")", "\n", "\n", "", "max_source_pos", "=", "(", "\n", "utils", ".", "resolve_max_positions", "(", "\n", "task", ".", "max_positions", "(", ")", ",", "*", "[", "model", ".", "max_positions", "(", ")", "for", "model", "in", "models", "]", "\n", ")", ",", "\n", ")", "\n", "\n", "if", "max_source_pos", "is", "not", "None", ":", "\n", "        ", "max_source_pos", "=", "max_source_pos", "[", "0", "]", "\n", "if", "max_source_pos", "is", "not", "None", ":", "\n", "            ", "max_source_pos", "=", "max_source_pos", "[", "0", "]", "-", "1", "\n", "\n", "", "", "if", "args", ".", "dump_emissions", ":", "\n", "        ", "emissions", "=", "{", "}", "\n", "", "if", "args", ".", "dump_features", ":", "\n", "        ", "features", "=", "{", "}", "\n", "models", "[", "0", "]", ".", "bert", ".", "proj", "=", "None", "\n", "", "else", ":", "\n", "        ", "res_files", "=", "prepare_result_files", "(", "args", ")", "\n", "", "errs_t", "=", "0", "\n", "lengths_t", "=", "0", "\n", "with", "progress_bar", ".", "build_progress_bar", "(", "args", ",", "itr", ")", "as", "t", ":", "\n", "        ", "wps_meter", "=", "TimeMeter", "(", ")", "\n", "for", "sample", "in", "t", ":", "\n", "            ", "sample", "=", "utils", ".", "move_to_cuda", "(", "sample", ")", "if", "use_cuda", "else", "sample", "\n", "if", "use_fp16", ":", "\n", "                ", "sample", "=", "utils", ".", "apply_to_sample", "(", "apply_half", ",", "sample", ")", "\n", "", "if", "\"net_input\"", "not", "in", "sample", ":", "\n", "                ", "continue", "\n", "\n", "", "prefix_tokens", "=", "None", "\n", "if", "args", ".", "prefix_size", ">", "0", ":", "\n", "                ", "prefix_tokens", "=", "sample", "[", "\"target\"", "]", "[", ":", ",", ":", "args", ".", "prefix_size", "]", "\n", "\n", "", "gen_timer", ".", "start", "(", ")", "\n", "if", "args", ".", "dump_emissions", ":", "\n", "                ", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "                    ", "encoder_out", "=", "models", "[", "0", "]", "(", "**", "sample", "[", "\"net_input\"", "]", ")", "\n", "emm", "=", "models", "[", "0", "]", ".", "get_normalized_probs", "(", "encoder_out", ",", "log_probs", "=", "True", ")", "\n", "emm", "=", "emm", ".", "transpose", "(", "0", ",", "1", ")", ".", "cpu", "(", ")", ".", "numpy", "(", ")", "\n", "for", "i", ",", "id", "in", "enumerate", "(", "sample", "[", "\"id\"", "]", ")", ":", "\n", "                        ", "emissions", "[", "id", ".", "item", "(", ")", "]", "=", "emm", "[", "i", "]", "\n", "", "continue", "\n", "", "", "elif", "args", ".", "dump_features", ":", "\n", "                ", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "                    ", "encoder_out", "=", "models", "[", "0", "]", "(", "**", "sample", "[", "\"net_input\"", "]", ")", "\n", "feat", "=", "encoder_out", "[", "\"encoder_out\"", "]", ".", "transpose", "(", "0", ",", "1", ")", ".", "cpu", "(", ")", ".", "numpy", "(", ")", "\n", "for", "i", ",", "id", "in", "enumerate", "(", "sample", "[", "\"id\"", "]", ")", ":", "\n", "                        ", "padding", "=", "(", "\n", "encoder_out", "[", "\"encoder_padding_mask\"", "]", "[", "i", "]", ".", "cpu", "(", ")", ".", "numpy", "(", ")", "\n", "if", "encoder_out", "[", "\"encoder_padding_mask\"", "]", "is", "not", "None", "\n", "else", "None", "\n", ")", "\n", "features", "[", "id", ".", "item", "(", ")", "]", "=", "(", "feat", "[", "i", "]", ",", "padding", ")", "\n", "", "continue", "\n", "", "", "hypos", "=", "task", ".", "inference_step", "(", "generator", ",", "models", ",", "sample", ",", "prefix_tokens", ")", "\n", "num_generated_tokens", "=", "sum", "(", "len", "(", "h", "[", "0", "]", "[", "\"tokens\"", "]", ")", "for", "h", "in", "hypos", ")", "\n", "gen_timer", ".", "stop", "(", "num_generated_tokens", ")", "\n", "\n", "for", "i", ",", "sample_id", "in", "enumerate", "(", "sample", "[", "\"id\"", "]", ".", "tolist", "(", ")", ")", ":", "\n", "                ", "speaker", "=", "None", "\n", "# id = task.dataset(args.gen_subset).ids[int(sample_id)]", "\n", "id", "=", "sample_id", "\n", "toks", "=", "(", "\n", "sample", "[", "\"target\"", "]", "[", "i", ",", ":", "]", "\n", "if", "\"target_label\"", "not", "in", "sample", "\n", "else", "sample", "[", "\"target_label\"", "]", "[", "i", ",", ":", "]", "\n", ")", "\n", "target_tokens", "=", "utils", ".", "strip_pad", "(", "toks", ",", "tgt_dict", ".", "pad", "(", ")", ")", ".", "int", "(", ")", ".", "cpu", "(", ")", "\n", "# Process top predictions", "\n", "errs", ",", "length", "=", "process_predictions", "(", "\n", "args", ",", "\n", "hypos", "[", "i", "]", ",", "\n", "None", ",", "\n", "tgt_dict", ",", "\n", "target_tokens", ",", "\n", "res_files", ",", "\n", "speaker", ",", "\n", "id", ",", "\n", ")", "\n", "errs_t", "+=", "errs", "\n", "lengths_t", "+=", "length", "\n", "\n", "", "wps_meter", ".", "update", "(", "num_generated_tokens", ")", "\n", "t", ".", "log", "(", "{", "\"wps\"", ":", "round", "(", "wps_meter", ".", "avg", ")", "}", ")", "\n", "num_sentences", "+=", "(", "\n", "sample", "[", "\"nsentences\"", "]", "if", "\"nsentences\"", "in", "sample", "else", "sample", "[", "\"id\"", "]", ".", "numel", "(", ")", "\n", ")", "\n", "\n", "", "", "wer", "=", "None", "\n", "if", "args", ".", "dump_emissions", ":", "\n", "        ", "emm_arr", "=", "[", "]", "\n", "for", "i", "in", "range", "(", "len", "(", "emissions", ")", ")", ":", "\n", "            ", "emm_arr", ".", "append", "(", "emissions", "[", "i", "]", ")", "\n", "", "np", ".", "save", "(", "args", ".", "dump_emissions", ",", "emm_arr", ")", "\n", "logger", ".", "info", "(", "f\"saved {len(emissions)} emissions to {args.dump_emissions}\"", ")", "\n", "", "elif", "args", ".", "dump_features", ":", "\n", "        ", "feat_arr", "=", "[", "]", "\n", "for", "i", "in", "range", "(", "len", "(", "features", ")", ")", ":", "\n", "            ", "feat_arr", ".", "append", "(", "features", "[", "i", "]", ")", "\n", "", "np", ".", "save", "(", "args", ".", "dump_features", ",", "feat_arr", ")", "\n", "logger", ".", "info", "(", "f\"saved {len(features)} emissions to {args.dump_features}\"", ")", "\n", "", "else", ":", "\n", "        ", "if", "lengths_t", ">", "0", ":", "\n", "            ", "wer", "=", "errs_t", "*", "100.0", "/", "lengths_t", "\n", "logger", ".", "info", "(", "f\"WER: {wer}\"", ")", "\n", "\n", "", "logger", ".", "info", "(", "\n", "\"| Processed {} sentences ({} tokens) in {:.1f}s ({:.2f}\"", "\n", "\"sentences/s, {:.2f} tokens/s)\"", ".", "format", "(", "\n", "num_sentences", ",", "\n", "gen_timer", ".", "n", ",", "\n", "gen_timer", ".", "sum", ",", "\n", "num_sentences", "/", "gen_timer", ".", "sum", ",", "\n", "1.0", "/", "gen_timer", ".", "avg", ",", "\n", ")", "\n", ")", "\n", "logger", ".", "info", "(", "\"| Generate {} with beam={}\"", ".", "format", "(", "args", ".", "gen_subset", ",", "args", ".", "beam", ")", ")", "\n", "\n", "", "print", "(", "'WER: '", ",", "wer", ")", "\n", "print", "(", "\"\\n| Processed {} sentences ({} tokens) in {:.1f}s ({:.2f}\"", "\n", "\"sentences/s, {:.2f} tokens/s)\"", ".", "format", "(", "\n", "num_sentences", ",", "\n", "gen_timer", ".", "n", ",", "\n", "gen_timer", ".", "sum", ",", "\n", "num_sentences", "/", "gen_timer", ".", "sum", ",", "\n", "1.0", "/", "gen_timer", ".", "avg", ",", "\n", ")", ")", "\n", "print", "(", "\"\\n| Generate {} with beam={}\"", ".", "format", "(", "args", ".", "gen_subset", ",", "args", ".", "beam", ")", ")", "\n", "return", "task", ",", "wer", "\n", "\n"]], "home.repos.pwc.inspect_result.speech-lab-iitm_pada.None.infer.make_parser": [[529, 533], ["fairseq.options.get_generation_parser", "infer.add_asr_eval_argument"], "function", ["home.repos.pwc.inspect_result.speech-lab-iitm_pada.None.infer.add_asr_eval_argument"], ["", "def", "make_parser", "(", ")", ":", "\n", "    ", "parser", "=", "options", ".", "get_generation_parser", "(", ")", "\n", "parser", "=", "add_asr_eval_argument", "(", "parser", ")", "\n", "return", "parser", "\n", "\n"]], "home.repos.pwc.inspect_result.speech-lab-iitm_pada.None.infer.cli_main": [[535, 539], ["infer.make_parser", "fairseq.options.parse_args_and_arch", "infer.main"], "function", ["home.repos.pwc.inspect_result.speech-lab-iitm_pada.None.infer.make_parser", "home.repos.pwc.inspect_result.speech-lab-iitm_pada.None.infer.main"], ["", "def", "cli_main", "(", ")", ":", "\n", "    ", "parser", "=", "make_parser", "(", ")", "\n", "args", "=", "options", ".", "parse_args_and_arch", "(", "parser", ")", "\n", "main", "(", "args", ")", "\n", "\n"]]}