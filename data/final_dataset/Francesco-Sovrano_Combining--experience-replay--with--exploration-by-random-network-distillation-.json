{"home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.framework.options.build": [[5, 115], ["tensorflow.app.flags.DEFINE_integer", "tensorflow.app.flags.DEFINE_integer", "tensorflow.app.flags.DEFINE_string", "tensorflow.app.flags.DEFINE_string", "tensorflow.app.flags.DEFINE_string", "tensorflow.app.flags.DEFINE_string", "tensorflow.app.flags.DEFINE_boolean", "tensorflow.app.flags.DEFINE_string", "tensorflow.app.flags.DEFINE_boolean", "tensorflow.app.flags.DEFINE_string", "tensorflow.app.flags.DEFINE_string", "tensorflow.app.flags.DEFINE_string", "tensorflow.app.flags.DEFINE_float", "tensorflow.app.flags.DEFINE_boolean", "tensorflow.app.flags.DEFINE_string", "tensorflow.app.flags.DEFINE_integer", "tensorflow.app.flags.DEFINE_float", "tensorflow.app.flags.DEFINE_float", "tensorflow.app.flags.DEFINE_boolean", "tensorflow.app.flags.DEFINE_string", "tensorflow.app.flags.DEFINE_integer", "tensorflow.app.flags.DEFINE_float", "tensorflow.app.flags.DEFINE_boolean", "tensorflow.app.flags.DEFINE_boolean", "tensorflow.app.flags.DEFINE_integer", "tensorflow.app.flags.DEFINE_boolean", "tensorflow.app.flags.DEFINE_float", "tensorflow.app.flags.DEFINE_float", "tensorflow.app.flags.DEFINE_float", "tensorflow.app.flags.DEFINE_float", "tensorflow.app.flags.DEFINE_boolean", "tensorflow.app.flags.DEFINE_boolean", "tensorflow.app.flags.DEFINE_float", "tensorflow.app.flags.DEFINE_integer", "tensorflow.app.flags.DEFINE_integer", "tensorflow.app.flags.DEFINE_integer", "tensorflow.app.flags.DEFINE_boolean", "tensorflow.app.flags.DEFINE_boolean", "tensorflow.app.flags.DEFINE_boolean", "tensorflow.app.flags.DEFINE_boolean", "tensorflow.app.flags.DEFINE_boolean", "tensorflow.app.flags.DEFINE_boolean", "tensorflow.app.flags.DEFINE_float", "tensorflow.app.flags.DEFINE_float", "tensorflow.app.flags.DEFINE_string", "tensorflow.app.flags.DEFINE_string", "tensorflow.app.flags.DEFINE_boolean", "tensorflow.app.flags.DEFINE_float", "tensorflow.app.flags.DEFINE_integer", "tensorflow.app.flags.DEFINE_integer", "tensorflow.app.flags.DEFINE_integer", "tensorflow.app.flags.DEFINE_integer", "tensorflow.app.flags.DEFINE_float", "tensorflow.app.flags.DEFINE_boolean", "tensorflow.app.flags.DEFINE_float", "tensorflow.app.flags.DEFINE_boolean", "tensorflow.app.flags.DEFINE_float", "tensorflow.app.flags.DEFINE_integer", "tensorflow.app.flags.DEFINE_boolean", "tensorflow.app.flags.DEFINE_integer", "tensorflow.app.flags.DEFINE_boolean", "tensorflow.app.flags.DEFINE_boolean", "tensorflow.app.flags.DEFINE_integer", "tensorflow.app.flags.DEFINE_integer", "tensorflow.app.flags.DEFINE_string", "tensorflow.app.flags.DEFINE_string", "tensorflow.app.flags.DEFINE_string", "tensorflow.app.flags.DEFINE_boolean", "tensorflow.app.flags.DEFINE_boolean", "tensorflow.app.flags.DEFINE_string", "tensorflow.app.flags.DEFINE_float", "tensorflow.app.flags.DEFINE_boolean", "tensorflow.app.flags.DEFINE_boolean", "tensorflow.app.flags.DEFINE_float", "tensorflow.app.flags.DEFINE_boolean", "tensorflow.app.flags.DEFINE_boolean", "tensorflow.app.flags.DEFINE_boolean", "tensorflow.app.flags.DEFINE_boolean", "tensorflow.app.flags.DEFINE_integer"], "function", ["None"], ["def", "build", "(", ")", ":", "\n", "\t", "tf", ".", "app", ".", "flags", ".", "DEFINE_integer", "(", "\"max_timestep\"", ",", "2", "**", "30", ",", "\"Max training time steps\"", ")", "\n", "tf", ".", "app", ".", "flags", ".", "DEFINE_integer", "(", "\"timesteps_before_starting_training\"", ",", "2", "**", "10", ",", "\"Max training time steps\"", ")", "\n", "# Environment", "\n", "# tf.app.flags.DEFINE_string(\"env_type\", \"car_controller\", \"environment types: rogue, car_controller, sentipolc, or environments from https://gym.openai.com/envs\")", "\n", "tf", ".", "app", ".", "flags", ".", "DEFINE_string", "(", "\"env_type\"", ",", "\"MontezumaRevengeDeterministic-v4\"", ",", "\"Environment types: rogue, car_controller, sentipolc, or environments from https://gym.openai.com/envs\"", ")", "\n", "# tf.app.flags.DEFINE_string(\"env_type\", \"MultipleSequenceAlignment-BaliBase\", \"Environment types: rogue, car_controller, sentipolc, MultipleSequenceAlignment-BaliBase, or environments from https://gym.openai.com/envs\")", "\n", "# tf.app.flags.DEFINE_string(\"env_type\", \"sentipolc\", \"environment types: rogue, car_controller, sentipolc, or environments from https://gym.openai.com/envs\")", "\n", "# tf.app.flags.DEFINE_string(\"env_type\", \"rogue\", \"environment types: rogue, car_controller, sentipolc, or environments from https://gym.openai.com/envs\")", "\n", "# Gradient optimization parameters", "\n", "tf", ".", "app", ".", "flags", ".", "DEFINE_string", "(", "\"parameters_type\"", ",", "\"float32\"", ",", "\"The type used to represent parameters: float32, float64\"", ")", "\n", "tf", ".", "app", ".", "flags", ".", "DEFINE_string", "(", "\"algorithm\"", ",", "\"AC\"", ",", "\"algorithms: AC, ACER\"", ")", "\n", "tf", ".", "app", ".", "flags", ".", "DEFINE_string", "(", "\"network_configuration\"", ",", "\"OpenAILarge\"", ",", "\"neural network configurations: Base, Towers, HybridTowers, SA, OpenAISmall, OpenAILarge, Impala\"", ")", "\n", "tf", ".", "app", ".", "flags", ".", "DEFINE_boolean", "(", "\"network_has_internal_state\"", ",", "False", ",", "\"Whether the network has an internal state to keep updated (eg. RNNs state).\"", ")", "\n", "tf", ".", "app", ".", "flags", ".", "DEFINE_string", "(", "\"optimizer\"", ",", "\"Adam\"", ",", "\"gradient optimizer: PowerSign, AddSign, ElasticAverage, LazyAdam, Nadam, Adadelta, AdagradDA, Adagrad, Adam, Ftrl, GradientDescent, Momentum, ProximalAdagrad, ProximalGradientDescent, RMSProp\"", ")", "# default is Adam, for vanilla A3C is RMSProp", "\n", "# In information theory, the cross entropy between two probability distributions p and q over the same underlying set of events measures the average number of bits needed to identify an event drawn from the set.", "\n", "tf", ".", "app", ".", "flags", ".", "DEFINE_boolean", "(", "\"only_non_negative_entropy\"", ",", "True", ",", "\"Cross-entropy and entropy are used for policy loss and if this flag is true, then entropy=max(0,entropy). If cross-entropy measures the average number of bits needed to identify an event, then it cannot be negative.\"", ")", "\n", "# Use mean losses if max_batch_size is too big, in order to avoid NaN", "\n", "tf", ".", "app", ".", "flags", ".", "DEFINE_string", "(", "\"loss_type\"", ",", "\"mean\"", ",", "\"type of loss reduction: sum, mean\"", ")", "\n", "tf", ".", "app", ".", "flags", ".", "DEFINE_string", "(", "\"policy_loss\"", ",", "\"PPO\"", ",", "\"policy loss function: Vanilla, PPO\"", ")", "\n", "tf", ".", "app", ".", "flags", ".", "DEFINE_string", "(", "\"value_loss\"", ",", "\"PVO\"", ",", "\"value loss function: Vanilla, PVO\"", ")", "\n", "# Loss clip range", "\n", "tf", ".", "app", ".", "flags", ".", "DEFINE_float", "(", "\"clip\"", ",", "0.1", ",", "\"PPO/PVO initial clip range\"", ")", "# default is 0.2, for openAI is 0.1", "\n", "tf", ".", "app", ".", "flags", ".", "DEFINE_boolean", "(", "\"clip_decay\"", ",", "False", ",", "\"Whether to decay the clip range\"", ")", "\n", "tf", ".", "app", ".", "flags", ".", "DEFINE_string", "(", "\"clip_annealing_function\"", ",", "\"exponential_decay\"", ",", "\"annealing function: exponential_decay, inverse_time_decay, natural_exp_decay\"", ")", "# default is inverse_time_decay", "\n", "tf", ".", "app", ".", "flags", ".", "DEFINE_integer", "(", "\"clip_decay_steps\"", ",", "10", "**", "5", ",", "\"decay clip every x steps\"", ")", "# default is 10**6", "\n", "tf", ".", "app", ".", "flags", ".", "DEFINE_float", "(", "\"clip_decay_rate\"", ",", "0.96", ",", "\"decay rate\"", ")", "# default is 0.25", "\n", "# Learning rate", "\n", "tf", ".", "app", ".", "flags", ".", "DEFINE_float", "(", "\"alpha\"", ",", "1e-4", ",", "\"initial learning rate\"", ")", "# default is 7.0e-4, for openAI is 2.5e-4", "\n", "tf", ".", "app", ".", "flags", ".", "DEFINE_boolean", "(", "\"alpha_decay\"", ",", "False", ",", "\"whether to decay the learning rate\"", ")", "\n", "tf", ".", "app", ".", "flags", ".", "DEFINE_string", "(", "\"alpha_annealing_function\"", ",", "\"exponential_decay\"", ",", "\"annealing function: exponential_decay, inverse_time_decay, natural_exp_decay\"", ")", "# default is inverse_time_decay", "\n", "tf", ".", "app", ".", "flags", ".", "DEFINE_integer", "(", "\"alpha_decay_steps\"", ",", "10", "**", "8", ",", "\"decay alpha every x steps\"", ")", "# default is 10**6", "\n", "tf", ".", "app", ".", "flags", ".", "DEFINE_float", "(", "\"alpha_decay_rate\"", ",", "0.96", ",", "\"decay rate\"", ")", "# default is 0.25", "\n", "# Intrinsic Rewards: Burda, Yuri, et al. \"Exploration by Random Network Distillation.\" arXiv preprint arXiv:1810.12894 (2018).", "\n", "tf", ".", "app", ".", "flags", ".", "DEFINE_boolean", "(", "\"intrinsic_reward\"", ",", "True", ",", "\"An intrinisc reward is given for exploring new states.\"", ")", "\n", "tf", ".", "app", ".", "flags", ".", "DEFINE_boolean", "(", "\"split_values\"", ",", "True", ",", "\"Estimate separate values for extrinsic and intrinsic rewards.\"", ")", "\n", "tf", ".", "app", ".", "flags", ".", "DEFINE_integer", "(", "\"intrinsic_reward_step\"", ",", "2", "**", "20", ",", "\"Start using the intrinsic reward only when global step is greater than n.\"", ")", "\n", "tf", ".", "app", ".", "flags", ".", "DEFINE_boolean", "(", "\"scale_intrinsic_reward\"", ",", "False", ",", "\"Whether to scale the intrinsic reward with its standard deviation.\"", ")", "\n", "tf", ".", "app", ".", "flags", ".", "DEFINE_float", "(", "\"intrinsic_rewards_mini_batch_fraction\"", ",", "0", ",", "\"Keep only the best intrinsic reward in a mini-batch of size 'batch_size*fraction', and set other intrinsic rewards to 0.\"", ")", "\n", "tf", ".", "app", ".", "flags", ".", "DEFINE_float", "(", "\"intrinsic_reward_gamma\"", ",", "0.99", ",", "\"Discount factor for intrinsic rewards\"", ")", "# default is 0.95, for openAI is 0.99", "\n", "tf", ".", "app", ".", "flags", ".", "DEFINE_float", "(", "\"extrinsic_coefficient\"", ",", "2.", ",", "\"Scale factor for the extrinsic part of the advantage.\"", ")", "\n", "tf", ".", "app", ".", "flags", ".", "DEFINE_float", "(", "\"intrinsic_coefficient\"", ",", "1.", ",", "\"Scale factor for the intrinsic part of the advantage.\"", ")", "\n", "tf", ".", "app", ".", "flags", ".", "DEFINE_boolean", "(", "\"episodic_extrinsic_reward\"", ",", "True", ",", "\"Bootstrap 0 for extrinsic value if state is terminal.\"", ")", "\n", "tf", ".", "app", ".", "flags", ".", "DEFINE_boolean", "(", "\"episodic_intrinsic_reward\"", ",", "False", ",", "\"Bootstrap 0 for intrinsic value if state is terminal.\"", ")", "\n", "# Experience Replay", "\n", "# Replay mean > 0 increases off-policyness", "\n", "tf", ".", "app", ".", "flags", ".", "DEFINE_float", "(", "\"replay_mean\"", ",", "0.5", ",", "\"Mean number of experience replays per batch. Lambda parameter of a Poisson distribution. When replay_mean is 0, then experience replay is not active.\"", ")", "# for A3C is 0, for ACER default is 4", "\n", "tf", ".", "app", ".", "flags", ".", "DEFINE_integer", "(", "\"replay_step\"", ",", "2", "**", "20", ",", "\"Start replaying experience when global step is greater than replay_step.\"", ")", "\n", "tf", ".", "app", ".", "flags", ".", "DEFINE_integer", "(", "\"replay_buffer_size\"", ",", "2", "**", "7", ",", "\"Maximum number of batches stored in the experience buffer.\"", ")", "\n", "tf", ".", "app", ".", "flags", ".", "DEFINE_integer", "(", "\"replay_start\"", ",", "1", ",", "\"Buffer minimum size before starting replay. Should be greater than 0 and lower than replay_buffer_size.\"", ")", "\n", "tf", ".", "app", ".", "flags", ".", "DEFINE_boolean", "(", "\"replay_only_best_batches\"", ",", "False", ",", "\"Whether to replay only those batches leading to an extrinsic reward (the best ones).\"", ")", "\n", "tf", ".", "app", ".", "flags", ".", "DEFINE_boolean", "(", "\"constraining_replay\"", ",", "False", ",", "\"Use constraining replay loss for the Actor, in order to minimize the quadratic distance between the sampled batch actions and the Actor mean actions (softmax output). \"", ")", "\n", "tf", ".", "app", ".", "flags", ".", "DEFINE_boolean", "(", "\"recompute_value_when_replaying\"", ",", "False", ",", "\"Whether to recompute values, advantages and discounted cumulative rewards when replaying, even if not required by the model.\"", ")", "\n", "tf", ".", "app", ".", "flags", ".", "DEFINE_boolean", "(", "\"train_critic_when_replaying\"", ",", "True", ",", "\"Whether to train also the critic when replaying. Works only when separate_actor_from_critic=True.\"", ")", "\n", "tf", ".", "app", ".", "flags", ".", "DEFINE_boolean", "(", "\"runtime_advantage\"", ",", "True", ",", "\"Whether to compute advantage at runtime, using always up to date state values instead of old ones.\"", ")", "# default False", "\n", "# tf.app.flags.DEFINE_float(\"loss_stationarity_range\", 5e-3, \"Used to decide when to interrupt experience replay. If the mean actor loss is whithin this range, then no replay is performed.\")", "\n", "# Prioritized Experience Replay: Schaul, Tom, et al. \"Prioritized experience replay.\" arXiv preprint arXiv:1511.05952 (2015).", "\n", "tf", ".", "app", ".", "flags", ".", "DEFINE_boolean", "(", "\"prioritized_replay\"", ",", "True", ",", "\"Whether to use prioritized sampling (if replay_mean > 0)\"", ")", "# default is True", "\n", "tf", ".", "app", ".", "flags", ".", "DEFINE_float", "(", "\"prioritized_replay_alpha\"", ",", "0.5", ",", "\"How much prioritization is used (0 - no prioritization, 1 - full prioritization).\"", ")", "\n", "tf", ".", "app", ".", "flags", ".", "DEFINE_float", "(", "\"prioritized_drop_probability\"", ",", "1", ",", "\"Probability of removing the batch with the lowest priority instead of the oldest batch.\"", ")", "\n", "# Reward manipulators", "\n", "tf", ".", "app", ".", "flags", ".", "DEFINE_string", "(", "\"extrinsic_reward_manipulator\"", ",", "'lambda x: np.clip(x,-1,1)'", ",", "\"Set to 'lambda x: x' for no manipulation. A lambda expression used to manipulate the extrinsic rewards.\"", ")", "\n", "tf", ".", "app", ".", "flags", ".", "DEFINE_string", "(", "\"intrinsic_reward_manipulator\"", ",", "'lambda x: x'", ",", "\"Set to 'lambda x: x' for no manipulation. A lambda expression used to manipulate the intrinsic rewards.\"", ")", "\n", "# Actor-Critic parameters", "\n", "tf", ".", "app", ".", "flags", ".", "DEFINE_boolean", "(", "\"separate_actor_from_critic\"", ",", "False", ",", "\"Set to True if you want actor and critic not sharing any part of their computational graphs.\"", ")", "# default False", "\n", "tf", ".", "app", ".", "flags", ".", "DEFINE_float", "(", "\"value_coefficient\"", ",", "1", ",", "\"Value coefficient for tuning Critic learning rate.\"", ")", "# default is 0.5", "\n", "tf", ".", "app", ".", "flags", ".", "DEFINE_integer", "(", "\"environment_count\"", ",", "128", ",", "\"Number of different parallel environments, used for training.\"", ")", "\n", "tf", ".", "app", ".", "flags", ".", "DEFINE_integer", "(", "\"threads_per_cpu\"", ",", "1", ",", "\"Number of threads per CPU. Set to 0 to use only one CPU.\"", ")", "\n", "tf", ".", "app", ".", "flags", ".", "DEFINE_integer", "(", "\"batch_size\"", ",", "128", ",", "\"Maximum batch size.\"", ")", "# default is 8", "\n", "# A big enough big_batch_size can significantly speed up the algorithm when training on GPU", "\n", "tf", ".", "app", ".", "flags", ".", "DEFINE_integer", "(", "\"big_batch_size\"", ",", "16", ",", "\"Number n > 0 of batches that compose a big-batch used for training. The bigger is n the more is the memory consumption.\"", ")", "\n", "# Taking gamma < 1 introduces bias into the policy gradient estimate, regardless of the value function accuracy.", "\n", "tf", ".", "app", ".", "flags", ".", "DEFINE_float", "(", "\"gamma\"", ",", "0.999", ",", "\"Discount factor for extrinsic rewards\"", ")", "# default is 0.95, for openAI is 0.99", "\n", "# Entropy regularization", "\n", "tf", ".", "app", ".", "flags", ".", "DEFINE_boolean", "(", "\"entropy_regularization\"", ",", "True", ",", "\"Whether to add entropy regularization to policy loss.\"", ")", "# default True", "\n", "tf", ".", "app", ".", "flags", ".", "DEFINE_float", "(", "\"beta\"", ",", "1e-3", ",", "\"entropy regularization constant\"", ")", "# default is 0.001, for openAI is 0.01", "\n", "# Generalized Advantage Estimation: Schulman, John, et al. \"High-dimensional continuous control using generalized advantage estimation.\" arXiv preprint arXiv:1506.02438 (2015).", "\n", "tf", ".", "app", ".", "flags", ".", "DEFINE_boolean", "(", "\"use_GAE\"", ",", "True", ",", "\"Whether to use Generalized Advantage Estimation.\"", ")", "# default in openAI's PPO implementation", "\n", "# Taking lambda < 1 introduces bias only when the value function is inaccurate", "\n", "tf", ".", "app", ".", "flags", ".", "DEFINE_float", "(", "\"lambd\"", ",", "0.95", ",", "\"generalized advantage estimator decay parameter\"", ")", "# default is 0.95", "\n", "# Log", "\n", "tf", ".", "app", ".", "flags", ".", "DEFINE_integer", "(", "\"save_interval_step\"", ",", "2", "**", "22", ",", "\"Save a checkpoint every n steps.\"", ")", "\n", "# rebuild_network_after_checkpoint_is_saved may help saving RAM, but may be slow proportionally to save_interval_step.", "\n", "tf", ".", "app", ".", "flags", ".", "DEFINE_boolean", "(", "\"rebuild_network_after_checkpoint_is_saved\"", ",", "False", ",", "\"Rebuild the whole network after checkpoint is saved. This may help saving RAM, but it's slow.\"", ")", "\n", "tf", ".", "app", ".", "flags", ".", "DEFINE_integer", "(", "\"max_checkpoint_to_keep\"", ",", "3", ",", "\"Keep the last n checkpoints, delete the others\"", ")", "\n", "tf", ".", "app", ".", "flags", ".", "DEFINE_boolean", "(", "\"test_after_saving\"", ",", "False", ",", "\"Whether to test after saving\"", ")", "\n", "tf", ".", "app", ".", "flags", ".", "DEFINE_boolean", "(", "\"print_test_results\"", ",", "False", ",", "\"Whether to print test results when testing\"", ")", "\n", "tf", ".", "app", ".", "flags", ".", "DEFINE_integer", "(", "\"episode_count_for_evaluation\"", ",", "2", "**", "5", ",", "\"Number of matches used for evaluation scores\"", ")", "\n", "tf", ".", "app", ".", "flags", ".", "DEFINE_integer", "(", "\"seconds_to_wait_for_printing_performance\"", ",", "60", ",", "\"Number of seconds to wait for printing algorithm performance in terms of memory and time usage\"", ")", "\n", "tf", ".", "app", ".", "flags", ".", "DEFINE_string", "(", "\"checkpoint_dir\"", ",", "\"./checkpoint\"", ",", "\"checkpoint directory\"", ")", "\n", "tf", ".", "app", ".", "flags", ".", "DEFINE_string", "(", "\"event_dir\"", ",", "\"./events\"", ",", "\"events directory\"", ")", "\n", "tf", ".", "app", ".", "flags", ".", "DEFINE_string", "(", "\"log_dir\"", ",", "\"./log\"", ",", "\"events directory\"", ")", "\n", "tf", ".", "app", ".", "flags", ".", "DEFINE_boolean", "(", "\"print_loss\"", ",", "True", ",", "\"Whether to print losses inside statistics\"", ")", "# print_loss = True might slow down the algorithm", "\n", "tf", ".", "app", ".", "flags", ".", "DEFINE_boolean", "(", "\"print_policy_info\"", ",", "True", ",", "\"Whether to print debug information about the actor inside statistics\"", ")", "# print_policy_info = True might slow down the algorithm", "\n", "tf", ".", "app", ".", "flags", ".", "DEFINE_string", "(", "\"show_episodes\"", ",", "'random'", ",", "\"What type of episodes to save: random, best, all, none\"", ")", "\n", "tf", ".", "app", ".", "flags", ".", "DEFINE_float", "(", "\"show_episode_probability\"", ",", "2e-3", ",", "\"Probability of showing an episode when show_episodes == random\"", ")", "\n", "# save_episode_screen = True might slow down the algorithm -> use in combination with show_episodes = 'random' for best perfomance", "\n", "tf", ".", "app", ".", "flags", ".", "DEFINE_boolean", "(", "\"save_episode_screen\"", ",", "True", ",", "\"Whether to save episode screens\"", ")", "\n", "# save_episode_gif = True slows down the algorithm, requires save_episode_screen = True to work", "\n", "tf", ".", "app", ".", "flags", ".", "DEFINE_boolean", "(", "\"save_episode_gif\"", ",", "True", ",", "\"Whether to save episode GIF, requires save_episode_screen == True.\"", ")", "\n", "tf", ".", "app", ".", "flags", ".", "DEFINE_float", "(", "\"gif_speed\"", ",", "0.1", ",", "\"GIF frame speed in seconds.\"", ")", "\n", "tf", ".", "app", ".", "flags", ".", "DEFINE_boolean", "(", "\"compress_gif\"", ",", "False", ",", "\"Whether to zip the episode GIF.\"", ")", "\n", "tf", ".", "app", ".", "flags", ".", "DEFINE_boolean", "(", "\"delete_screens_after_making_gif\"", ",", "True", ",", "\"Whether to delete the screens after the GIF has been made.\"", ")", "\n", "tf", ".", "app", ".", "flags", ".", "DEFINE_boolean", "(", "\"monitor_memory_usage\"", ",", "False", ",", "\"Whether to monitor memory usage\"", ")", "\n", "# Plot", "\n", "tf", ".", "app", ".", "flags", ".", "DEFINE_boolean", "(", "\"compute_plot_when_saving\"", ",", "True", ",", "\"Whether to compute the plot when saving checkpoints\"", ")", "\n", "tf", ".", "app", ".", "flags", ".", "DEFINE_integer", "(", "\"max_plot_size\"", ",", "25", ",", "\"Maximum number of points in the plot. The smaller it is, the less RAM is required. If the log file has more than max_plot_size points, then max_plot_size means of slices are used instead.\"", ")", "\n", "\n", "global", "options_built", "\n", "options_built", "=", "True", "\n", "\n"]], "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.framework.options.get": [[116, 121], ["options.build"], "function", ["home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.intrinsic_reward.intrinsic_reward_network.IntrinsicReward_Network.build"], ["", "def", "get", "(", ")", ":", "\n", "\t", "global", "options_built", "\n", "if", "not", "options_built", ":", "\n", "\t\t", "build", "(", ")", "\n", "", "return", "tf", ".", "app", ".", "flags", ".", "FLAGS", "", "", ""]], "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.framework.train.main": [[5, 7], ["agent.server.train"], "function", ["home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.agent.server.train"], ["def", "main", "(", "argv", ")", ":", "\n", "\t", "train", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.framework.test.main": [[5, 8], ["agent.server.Application", "agent.server.Application.test"], "function", ["home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.agent.server.Application.test"], ["def", "main", "(", "argv", ")", ":", "\n", "\t", "app", "=", "Application", "(", ")", "\n", "app", ".", "test", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.utils.statistics.Statistics.__init__": [[7, 11], ["None"], "methods", ["None"], ["\t", "def", "__init__", "(", "self", ",", "max_count", ",", "buffer_must_be_full", "=", "False", ")", ":", "\n", "\t\t", "self", ".", "_max_count", "=", "max_count", "\n", "self", ".", "_stats", "=", "{", "}", "\n", "self", ".", "_buffer_must_be_full", "=", "buffer_must_be_full", "\n", "\n"]], "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.utils.statistics.Statistics.add": [[12, 16], ["statistics.Statistics._stats[].append", "collections.deque"], "methods", ["home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.worker.batch.ExperienceBatch.append"], ["", "def", "add", "(", "self", ",", "stat_dict", ",", "type", "=", "''", ")", ":", "\n", "\t\t", "if", "type", "not", "in", "self", ".", "_stats", ":", "\n", "\t\t\t", "self", ".", "_stats", "[", "type", "]", "=", "deque", "(", "maxlen", "=", "self", ".", "_max_count", ")", "\n", "", "self", ".", "_stats", "[", "type", "]", ".", "append", "(", "stat_dict", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.utils.statistics.Statistics.buffer_is_full": [[17, 19], ["len"], "methods", ["None"], ["", "def", "buffer_is_full", "(", "self", ",", "type", ")", ":", "\n", "\t\t", "return", "len", "(", "self", ".", "_stats", "[", "type", "]", ")", "==", "self", ".", "_max_count", "\n", "\n"]], "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.utils.statistics.Statistics.get": [[20, 31], ["statistics.Statistics._stats.items", "max", "max.keys", "final_dict.update", "statistics.Statistics.buffer_is_full", "numpy.mean", "len", "x.keys"], "methods", ["home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.utils.segment_tree.MaxSegmentTree.max", "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.utils.running_std.RunningMeanStd.update", "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.utils.statistics.IndexedStatistics.buffer_is_full", "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.utils.distributions.Normal.mean"], ["", "def", "get", "(", "self", ")", ":", "\n", "\t\t", "final_dict", "=", "{", "}", "\n", "for", "type", ",", "stat", "in", "self", ".", "_stats", ".", "items", "(", ")", ":", "\n", "\t\t\t", "if", "not", "stat", ":", "\n", "\t\t\t\t", "continue", "\n", "", "if", "self", ".", "_buffer_must_be_full", "and", "not", "self", ".", "buffer_is_full", "(", "type", ")", ":", "\n", "\t\t\t\t", "continue", "\n", "", "biggest_dictionary", "=", "max", "(", "stat", ",", "key", "=", "lambda", "x", ":", "len", "(", "x", ".", "keys", "(", ")", ")", ")", "\n", "keys", "=", "biggest_dictionary", ".", "keys", "(", ")", "\n", "final_dict", ".", "update", "(", "{", "'{1}{0}'", ".", "format", "(", "k", ",", "type", ")", ":", "np", ".", "mean", "(", "[", "e", "[", "k", "]", "for", "e", "in", "stat", "if", "k", "in", "e", "]", ")", "for", "k", "in", "keys", "}", ")", "\n", "", "return", "final_dict", "\n", "\n"]], "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.utils.statistics.IndexedStatistics.__init__": [[34, 37], ["statistics.Statistics.__init__"], "methods", ["home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.worker.batch.CompositeBatch.__init__"], ["\t", "def", "__init__", "(", "self", ",", "max_count", ",", "buffer_must_be_full", "=", "False", ")", ":", "\n", "\t\t", "super", "(", ")", ".", "__init__", "(", "max_count", ",", "buffer_must_be_full", ")", "\n", "self", ".", "_non_empty_stats", "=", "{", "}", "\n", "\n"]], "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.utils.statistics.IndexedStatistics.add": [[38, 40], ["None"], "methods", ["None"], ["", "def", "add", "(", "self", ",", "stat_dict", ",", "type", "=", "''", ")", ":", "\n", "\t\t", "raise", "AttributeError", "\n", "\n"]], "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.utils.statistics.IndexedStatistics.set": [[41, 53], ["None"], "methods", ["None"], ["", "def", "set", "(", "self", ",", "stat_dict", ",", "index", ",", "type", "=", "''", ")", ":", "\n", "\t\t", "if", "type", "not", "in", "self", ".", "_stats", ":", "\n", "\t\t\t", "self", ".", "_stats", "[", "type", "]", "=", "[", "None", "]", "*", "self", ".", "_max_count", "\n", "self", ".", "_non_empty_stats", "[", "type", "]", "=", "0", "\n", "", "stat_was_empty", "=", "self", ".", "_stats", "[", "type", "]", "[", "index", "]", "is", "None", "\n", "self", ".", "_stats", "[", "type", "]", "[", "index", "]", "=", "stat_dict", "\n", "stat_is_empty", "=", "stat_dict", "is", "None", "\n", "if", "stat_was_empty", ":", "\n", "\t\t\t", "if", "not", "stat_is_empty", ":", "\n", "\t\t\t\t", "self", ".", "_non_empty_stats", "[", "type", "]", "+=", "1", "\n", "", "", "elif", "stat_is_empty", ":", "\n", "\t\t\t", "self", ".", "_non_empty_stats", "[", "type", "]", "-=", "1", "\n", "\n"]], "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.utils.statistics.IndexedStatistics.buffer_is_full": [[54, 56], ["None"], "methods", ["None"], ["", "", "def", "buffer_is_full", "(", "self", ",", "type", ")", ":", "\n", "\t\t", "return", "self", ".", "_non_empty_stats", "[", "type", "]", "==", "self", ".", "_max_count", "", "", "", ""]], "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.utils.plots.plot": [[24, 145], ["len", "range", "len", "math.ceil", "matplotlib.figure.Figure", "matplotlib.backends.backend_agg.FigureCanvasAgg", "matplotlib.gridspec.GridSpec", "range", "matplotlib.figure.Figure.savefig", "print", "plots.parse_line", "sorted", "print", "matplotlib.figure.Figure.add_subplot", "range", "print", "range", "obj.keys", "range", "print", "range", "len", "float", "float", "print", "ax.set_ylabel", "ax.set_xlabel", "numpy.array", "numpy.array", "ax.plot", "ax.fill_between", "ax.legend", "ax.grid", "values[].append", "len", "[].append", "[].append", "x[].append", "len", "numpy.mean", "numpy.std"], "function", ["home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.utils.plots.parse_line", "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.utils.plots.plot", "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.worker.batch.ExperienceBatch.append", "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.worker.batch.ExperienceBatch.append", "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.worker.batch.ExperienceBatch.append", "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.worker.batch.ExperienceBatch.append", "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.utils.distributions.Normal.mean"], ["def", "plot", "(", "logs", ",", "figure_file", ")", ":", "\n", "\t", "log_count", "=", "len", "(", "logs", ")", "\n", "# Get plot types", "\n", "stats", "=", "[", "None", "]", "*", "log_count", "\n", "key_ids", "=", "{", "}", "\n", "for", "i", "in", "range", "(", "log_count", ")", ":", "\n", "\t\t", "log", "=", "logs", "[", "i", "]", "\n", "# Get statistics keys", "\n", "if", "log", "[", "\"length\"", "]", "<", "2", ":", "\n", "\t\t\t", "continue", "\n", "", "(", "step", ",", "obj", ")", "=", "parse_line", "(", "log", "[", "\"line_example\"", "]", ")", "\n", "log_keys", "=", "sorted", "(", "obj", ".", "keys", "(", ")", ")", "# statistics keys sorted by name", "\n", "for", "key", "in", "log_keys", ":", "\n", "\t\t\t", "if", "key", "not", "in", "key_ids", ":", "\n", "\t\t\t\t", "key_ids", "[", "key", "]", "=", "len", "(", "key_ids", ")", "\n", "", "", "stats", "[", "i", "]", "=", "log_keys", "\n", "", "max_stats_count", "=", "len", "(", "key_ids", ")", "\n", "if", "max_stats_count", "<=", "0", ":", "\n", "\t\t", "print", "(", "\"Not enough data for a reasonable plot\"", ")", "\n", "return", "\n", "# Create new figure and two subplots, sharing both axes", "\n", "", "ncols", "=", "3", "if", "max_stats_count", ">=", "3", "else", "max_stats_count", "\n", "nrows", "=", "math", ".", "ceil", "(", "max_stats_count", "/", "ncols", ")", "\n", "# First set up the figure and the axis", "\n", "# fig, ax = matplotlib.pyplot.subplots(nrows=1, ncols=1, sharey=False, sharex=False, figsize=(10,10)) # this method causes memory leaks", "\n", "figure", "=", "Figure", "(", "figsize", "=", "(", "10", "*", "ncols", ",", "7", "*", "nrows", ")", ")", "\n", "canvas", "=", "FigureCanvas", "(", "figure", ")", "\n", "grid", "=", "GridSpec", "(", "ncols", "=", "ncols", ",", "nrows", "=", "nrows", ")", "\n", "axes", "=", "[", "figure", ".", "add_subplot", "(", "grid", "[", "id", "//", "ncols", ",", "id", "%", "ncols", "]", ")", "for", "id", "in", "range", "(", "max_stats_count", ")", "]", "\n", "# Populate axes", "\n", "for", "log_id", "in", "range", "(", "log_count", ")", ":", "\n", "\t\t", "log", "=", "logs", "[", "log_id", "]", "\n", "name", "=", "log", "[", "\"name\"", "]", "\n", "data", "=", "log", "[", "\"data\"", "]", "\n", "length", "=", "log", "[", "\"length\"", "]", "\n", "if", "length", "<", "2", ":", "\n", "\t\t\t", "print", "(", "name", ",", "\" has not enough data for a reasonable plot\"", ")", "\n", "continue", "\n", "", "if", "length", ">", "flags", ".", "max_plot_size", ":", "\n", "\t\t\t", "plot_size", "=", "flags", ".", "max_plot_size", "\n", "data_per_plotpoint", "=", "length", "//", "plot_size", "\n", "", "else", ":", "\n", "\t\t\t", "plot_size", "=", "length", "\n", "data_per_plotpoint", "=", "1", "\n", "# Build x, y", "\n", "", "x", "=", "{", "}", "\n", "y", "=", "{", "}", "\n", "stat", "=", "stats", "[", "log_id", "]", "\n", "for", "key", "in", "stat", ":", "# foreach statistic", "\n", "\t\t\t", "y", "[", "key", "]", "=", "{", "\"min\"", ":", "float", "(", "\"+inf\"", ")", ",", "\"max\"", ":", "float", "(", "\"-inf\"", ")", ",", "\"data\"", ":", "[", "]", ",", "\"std\"", ":", "[", "]", "}", "\n", "x", "[", "key", "]", "=", "[", "]", "\n", "", "last_step", "=", "0", "\n", "for", "_", "in", "range", "(", "plot_size", ")", ":", "\n", "\t\t\t", "values", "=", "{", "}", "\n", "# initialize", "\n", "for", "key", "in", "stat", ":", "# foreach statistic", "\n", "\t\t\t\t", "values", "[", "key", "]", "=", "[", "]", "\n", "# compute values foreach key", "\n", "", "plotpoint_i", "=", "0", "\n", "for", "(", "step", ",", "obj", ")", "in", "data", ":", "\n", "\t\t\t\t", "plotpoint_i", "+=", "1", "\n", "if", "step", "<=", "last_step", ":", "\n", "\t\t\t\t\t", "continue", "\n", "", "last_step", "=", "step", "\n", "for", "key", "in", "stat", ":", "# foreach statistic", "\n", "\t\t\t\t\t", "if", "key", "not", "in", "obj", ":", "\n", "\t\t\t\t\t\t", "continue", "\n", "", "v", "=", "obj", "[", "key", "]", "\n", "values", "[", "key", "]", ".", "append", "(", "v", ")", "\n", "if", "v", ">", "y", "[", "key", "]", "[", "\"max\"", "]", ":", "\n", "\t\t\t\t\t\t", "y", "[", "key", "]", "[", "\"max\"", "]", "=", "v", "\n", "", "if", "v", "<", "y", "[", "key", "]", "[", "\"min\"", "]", ":", "\n", "\t\t\t\t\t\t", "y", "[", "key", "]", "[", "\"min\"", "]", "=", "v", "\n", "", "", "if", "plotpoint_i", ">", "data_per_plotpoint", ":", "# save plotpoint", "\n", "\t\t\t\t\t", "break", "\n", "# add average to data for plotting", "\n", "", "", "for", "key", "in", "stat", ":", "# foreach statistic", "\n", "\t\t\t\t", "if", "len", "(", "values", "[", "key", "]", ")", ">", "0", ":", "\n", "\t\t\t\t\t", "y", "[", "key", "]", "[", "\"data\"", "]", ".", "append", "(", "np", ".", "mean", "(", "values", "[", "key", "]", ")", ")", "\n", "y", "[", "key", "]", "[", "\"std\"", "]", ".", "append", "(", "np", ".", "std", "(", "values", "[", "key", "]", ")", ")", "\n", "x", "[", "key", "]", ".", "append", "(", "last_step", ")", "\n", "# Populate axes", "\n", "", "", "", "print", "(", "name", ")", "\n", "for", "j", "in", "range", "(", "ncols", ")", ":", "\n", "\t\t\t", "for", "i", "in", "range", "(", "nrows", ")", ":", "\n", "\t\t\t\t", "idx", "=", "j", "if", "nrows", "==", "1", "else", "i", "*", "ncols", "+", "j", "\n", "if", "idx", ">=", "len", "(", "stat", ")", ":", "\n", "\t\t\t\t\t", "continue", "\n", "", "key", "=", "stat", "[", "idx", "]", "\n", "ax_id", "=", "key_ids", "[", "key", "]", "\n", "ax", "=", "axes", "[", "ax_id", "]", "\n", "y_key", "=", "y", "[", "key", "]", "\n", "x_key", "=", "x", "[", "key", "]", "\n", "# print stats", "\n", "print", "(", "\"    \"", ",", "y_key", "[", "\"min\"", "]", ",", "\" < \"", ",", "key", ",", "\" < \"", ",", "y_key", "[", "\"max\"", "]", ")", "\n", "# ax", "\n", "ax", ".", "set_ylabel", "(", "key", ",", "fontdict", "=", "font_dict", ")", "\n", "ax", ".", "set_xlabel", "(", "'step'", ",", "fontdict", "=", "font_dict", ")", "\n", "# ax.plot(x, y, linewidth=linewidth, markersize=markersize)", "\n", "y_key_mean", "=", "np", ".", "array", "(", "y_key", "[", "\"data\"", "]", ")", "\n", "y_key_std", "=", "np", ".", "array", "(", "y_key", "[", "\"std\"", "]", ")", "\n", "#===============================================================", "\n", "# # build interpolators", "\n", "# mean_interpolator = interp1d(x_key, y_key_mean, kind='linear')", "\n", "# min_interpolator = interp1d(x_key, y_key_mean-y_key_std, kind='linear')", "\n", "# max_interpolator = interp1d(x_key, y_key_mean+y_key_std, kind='linear')", "\n", "# xnew = np.linspace(x_key[0], x_key[-1], num=plot_size, endpoint=True)", "\n", "# # plot mean line", "\n", "# ax.plot(xnew, mean_interpolator(xnew), label=name)", "\n", "#===============================================================", "\n", "# plot mean line", "\n", "ax", ".", "plot", "(", "x_key", ",", "y_key_mean", ",", "label", "=", "name", ")", "\n", "# plot std range", "\n", "ax", ".", "fill_between", "(", "x_key", ",", "y_key_mean", "-", "y_key_std", ",", "y_key_mean", "+", "y_key_std", ",", "alpha", "=", "0.25", ")", "\n", "# show legend", "\n", "ax", ".", "legend", "(", ")", "\n", "# display grid", "\n", "ax", ".", "grid", "(", "True", ")", "\n", "", "", "", "figure", ".", "savefig", "(", "figure_file", ",", "bbox_inches", "=", "'tight'", ")", "\n", "print", "(", "\"Plot figure saved in \"", ",", "figure_file", ")", "\n", "figure", "=", "None", "\n", "\n"]], "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.utils.plots.plot_files": [[146, 152], ["plots.plot", "plots.get_length_and_line_example", "logs.append", "plots.parse"], "function", ["home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.utils.plots.plot", "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.utils.plots.get_length_and_line_example", "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.worker.batch.ExperienceBatch.append", "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.utils.plots.parse"], ["", "def", "plot_files", "(", "log_files", ",", "figure_file", ")", ":", "\n", "\t", "logs", "=", "[", "]", "\n", "for", "fname", "in", "log_files", ":", "\n", "\t\t", "length", ",", "line_example", "=", "get_length_and_line_example", "(", "fname", ")", "\n", "logs", ".", "append", "(", "{", "'name'", ":", "fname", ",", "'data'", ":", "parse", "(", "fname", ")", ",", "'length'", ":", "length", ",", "'line_example'", ":", "line_example", "}", ")", "\n", "", "plot", "(", "logs", ",", "figure_file", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.utils.plots.get_length_and_line_example": [[153, 165], ["open", "next", "len", "len"], "function", ["None"], ["", "def", "get_length_and_line_example", "(", "file", ")", ":", "\n", "\t", "try", ":", "\n", "\t\t", "lines_generator", "=", "open", "(", "file", ")", "\n", "tot", "=", "1", "\n", "line_example", "=", "next", "(", "lines_generator", ")", "\n", "", "except", ":", "\n", "\t\t", "return", "0", ",", "None", "\n", "", "for", "line", "in", "lines_generator", ":", "\n", "\t\t", "tot", "+=", "1", "\n", "if", "len", "(", "line", ")", ">", "len", "(", "line_example", ")", ":", "\n", "\t\t\t", "line_example", "=", "line", "\n", "", "", "return", "tot", ",", "line_example", "\n", "\n"]], "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.utils.plots.parse_line": [[166, 188], ["line.split", "re.sub", "int", "re.sub", "re.sub.split", "float"], "function", ["None"], ["", "def", "parse_line", "(", "line", ",", "i", "=", "0", ")", ":", "\n", "\t", "splitted", "=", "line", ".", "split", "(", "' '", ")", "\n", "# date_str = splitted[0] + ' ' + splitted[1]", "\n", "# date = datetime.datetime.strptime(date_str, '%Y-%m-%d %H:%M:%S,%f')", "\n", "# obj = {'date': date}", "\n", "# Get step", "\n", "if", "'<'", "in", "splitted", "[", "2", "]", ":", "\n", "\t\t", "step", "=", "re", ".", "sub", "(", "'[<>]'", ",", "''", ",", "splitted", "[", "2", "]", ")", "# remove following chars: <>", "\n", "step", "=", "int", "(", "step", ")", "\n", "xs", "=", "splitted", "[", "3", ":", "]", "\n", "", "else", ":", "\n", "\t\t", "step", "=", "i", "\n", "xs", "=", "splitted", "[", "2", ":", "]", "\n", "# Get objects", "\n", "", "obj", "=", "{", "}", "\n", "for", "x", "in", "xs", ":", "\n", "\t\t", "x", "=", "re", ".", "sub", "(", "'[\\',\\[\\]]'", ",", "''", ",", "x", ")", "# remove following chars: ',[]", "\n", "# print(x)", "\n", "key", ",", "val", "=", "x", ".", "split", "(", "'='", ")", "\n", "obj", "[", "key", "]", "=", "float", "(", "val", ")", "\n", "# print (obj)", "\n", "", "return", "(", "step", ",", "obj", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.utils.plots.parse": [[189, 198], ["open", "enumerate", "plots.parse_line", "print", "print", "repr"], "function", ["home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.utils.plots.parse_line"], ["", "def", "parse", "(", "log_fname", ")", ":", "\n", "\t", "with", "open", "(", "log_fname", ",", "'r'", ")", "as", "logfile", ":", "\n", "\t\t", "for", "i", ",", "line", "in", "enumerate", "(", "logfile", ")", ":", "\n", "\t\t\t", "try", ":", "\n", "\t\t\t\t", "yield", "parse_line", "(", "line", ",", "i", ")", "\n", "", "except", "Exception", "as", "e", ":", "\n", "\t\t\t\t", "print", "(", "\"exc %s on line %s\"", "%", "(", "repr", "(", "e", ")", ",", "i", "+", "1", ")", ")", "\n", "print", "(", "\"skipping line\"", ")", "\n", "continue", "\n", "\n"]], "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.utils.plots.heatmap": [[199, 206], ["matplotlib.figure.Figure", "matplotlib.backends.backend_agg.FigureCanvasAgg", "matplotlib.figure.Figure.add_subplot", "seaborn.heatmap", "matplotlib.figure.Figure.savefig"], "function", ["home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.utils.plots.heatmap"], ["", "", "", "", "def", "heatmap", "(", "heatmap", ",", "figure_file", ")", ":", "\n", "# fig, ax = matplotlib.pyplot.subplots(nrows=1, ncols=1, sharey=False, sharex=False, figsize=(10,10)) # this method causes memory leaks", "\n", "\t", "figure", "=", "Figure", "(", ")", "\n", "canvas", "=", "FigureCanvas", "(", "figure", ")", "\n", "ax", "=", "figure", ".", "add_subplot", "(", "111", ")", "# nrows=1, ncols=1, index=1", "\n", "seaborn_heatmap", "(", "data", "=", "heatmap", ",", "ax", "=", "ax", ")", "\n", "figure", ".", "savefig", "(", "figure_file", ",", "bbox_inches", "=", "'tight'", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.utils.plots.ascii_image": [[207, 223], ["PIL.ImageFont.load_default", "string.splitlines", "PIL.Image.new", "PIL.ImageDraw.Draw", "ImageDraw.Draw.text", "Image.new.save", "ImageFont.load_default.getsize", "max"], "function", ["home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.utils.segment_tree.MaxSegmentTree.max"], ["", "def", "ascii_image", "(", "string", ",", "file_name", ")", ":", "\n", "# find image size", "\n", "\t", "font", "=", "ImageFont", ".", "load_default", "(", ")", "\n", "splitlines", "=", "string", ".", "splitlines", "(", ")", "\n", "text_width", "=", "0", "\n", "text_height", "=", "0", "\n", "for", "line", "in", "splitlines", ":", "\n", "\t\t", "text_size", "=", "font", ".", "getsize", "(", "line", ")", "# for efficiency's sake, split only on the first newline, discard the rest", "\n", "text_width", "=", "max", "(", "text_width", ",", "text_size", "[", "0", "]", ")", "\n", "text_height", "+=", "text_size", "[", "1", "]", "+", "5", "\n", "", "text_width", "+=", "10", "\n", "# create image", "\n", "source_img", "=", "Image", ".", "new", "(", "'RGB'", ",", "(", "text_width", ",", "text_height", ")", ",", "\"black\"", ")", "\n", "draw", "=", "ImageDraw", ".", "Draw", "(", "source_img", ")", "\n", "draw", ".", "text", "(", "(", "5", ",", "5", ")", ",", "string", ",", "font", "=", "font", ")", "\n", "source_img", ".", "save", "(", "file_name", ",", "\"JPEG\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.utils.plots.combine_images": [[224, 232], ["numpy.hstack", "PIL.Image.fromarray", "Image.fromarray.save", "PIL.Image.open", "sorted", "numpy.asarray", "i.resize", "numpy.sum"], "function", ["home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.utils.segment_tree.SumSegmentTree.sum"], ["", "def", "combine_images", "(", "images_list", ",", "file_name", ")", ":", "\n", "\t", "imgs", "=", "[", "Image", ".", "open", "(", "i", ")", "for", "i", "in", "images_list", "]", "\n", "# pick the smallest image, and resize the others to match it (can be arbitrary image shape here)", "\n", "min_shape", "=", "sorted", "(", "[", "(", "np", ".", "sum", "(", "i", ".", "size", ")", ",", "i", ".", "size", ")", "for", "i", "in", "imgs", "]", ")", "[", "0", "]", "[", "1", "]", "\n", "imgs_comb", "=", "np", ".", "hstack", "(", "[", "np", ".", "asarray", "(", "i", ".", "resize", "(", "min_shape", ")", ")", "for", "i", "in", "imgs", "]", ")", "\n", "# save the picture", "\n", "imgs_comb", "=", "Image", ".", "fromarray", "(", "imgs_comb", ")", "\n", "imgs_comb", ".", "save", "(", "file_name", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.utils.plots.rgb_array_image": [[233, 236], ["PIL.Image.fromarray", "Image.fromarray.save"], "function", ["None"], ["", "def", "rgb_array_image", "(", "array", ",", "file_name", ")", ":", "\n", "\t", "img", "=", "Image", ".", "fromarray", "(", "array", ",", "'RGB'", ")", "\n", "img", ".", "save", "(", "file_name", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.utils.plots.make_gif": [[237, 242], ["imageio.get_writer", "imageio.imread", "writer.append_data"], "function", ["None"], ["", "def", "make_gif", "(", "gif_path", ",", "file_list", ")", ":", "\n", "\t", "with", "imageio_get_writer", "(", "gif_path", ",", "mode", "=", "'I'", ",", "duration", "=", "flags", ".", "gif_speed", ")", "as", "writer", ":", "\n", "\t\t", "for", "filename", "in", "file_list", ":", "\n", "\t\t\t", "image", "=", "imageio_imread", "(", "filename", ")", "\n", "writer", ".", "append_data", "(", "image", ")", "", "", "", "", ""]], "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.utils.schedules.Schedule.value": [[13, 16], ["NotImplementedError"], "methods", ["None"], ["    ", "def", "value", "(", "self", ",", "t", ")", ":", "\n", "        ", "\"\"\"Value of the schedule at time t\"\"\"", "\n", "raise", "NotImplementedError", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.utils.schedules.ConstantSchedule.__init__": [[19, 28], ["None"], "methods", ["None"], ["    ", "def", "__init__", "(", "self", ",", "value", ")", ":", "\n", "        ", "\"\"\"Value remains constant over time.\n\n        Parameters\n        ----------\n        value: float\n            Constant value of the schedule\n        \"\"\"", "\n", "self", ".", "_v", "=", "value", "\n", "\n"]], "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.utils.schedules.ConstantSchedule.value": [[29, 32], ["None"], "methods", ["None"], ["", "def", "value", "(", "self", ",", "t", ")", ":", "\n", "        ", "\"\"\"See Schedule.value\"\"\"", "\n", "return", "self", ".", "_v", "\n", "\n"]], "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.utils.schedules.PiecewiseSchedule.__init__": [[39, 63], ["sorted"], "methods", ["None"], ["    ", "def", "__init__", "(", "self", ",", "endpoints", ",", "interpolation", "=", "linear_interpolation", ",", "outside_value", "=", "None", ")", ":", "\n", "        ", "\"\"\"Piecewise schedule.\n\n        endpoints: [(int, int)]\n            list of pairs `(time, value)` meanining that schedule should output\n            `value` when `t==time`. All the values for time must be sorted in\n            an increasing order. When t is between two times, e.g. `(time_a, value_a)`\n            and `(time_b, value_b)`, such that `time_a <= t < time_b` then value outputs\n            `interpolation(value_a, value_b, alpha)` where alpha is a fraction of\n            time passed between `time_a` and `time_b` for time `t`.\n        interpolation: lambda float, float, float: float\n            a function that takes value to the left and to the right of t according\n            to the `endpoints`. Alpha is the fraction of distance from left endpoint to\n            right endpoint that t has covered. See linear_interpolation for example.\n        outside_value: float\n            if the value is requested outside of all the intervals sepecified in\n            `endpoints` this value is returned. If None then AssertionError is\n            raised when outside value is requested.\n        \"\"\"", "\n", "idxes", "=", "[", "e", "[", "0", "]", "for", "e", "in", "endpoints", "]", "\n", "assert", "idxes", "==", "sorted", "(", "idxes", ")", "\n", "self", ".", "_interpolation", "=", "interpolation", "\n", "self", ".", "_outside_value", "=", "outside_value", "\n", "self", ".", "_endpoints", "=", "endpoints", "\n", "\n"]], "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.utils.schedules.PiecewiseSchedule.value": [[64, 74], ["zip", "schedules.PiecewiseSchedule._interpolation", "float"], "methods", ["None"], ["", "def", "value", "(", "self", ",", "t", ")", ":", "\n", "        ", "\"\"\"See Schedule.value\"\"\"", "\n", "for", "(", "l_t", ",", "l", ")", ",", "(", "r_t", ",", "r", ")", "in", "zip", "(", "self", ".", "_endpoints", "[", ":", "-", "1", "]", ",", "self", ".", "_endpoints", "[", "1", ":", "]", ")", ":", "\n", "            ", "if", "l_t", "<=", "t", "and", "t", "<", "r_t", ":", "\n", "                ", "alpha", "=", "float", "(", "t", "-", "l_t", ")", "/", "(", "r_t", "-", "l_t", ")", "\n", "return", "self", ".", "_interpolation", "(", "l", ",", "r", ",", "alpha", ")", "\n", "\n", "# t does not belong to any of the pieces, so doom.", "\n", "", "", "assert", "self", ".", "_outside_value", "is", "not", "None", "\n", "return", "self", ".", "_outside_value", "\n", "\n"]], "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.utils.schedules.LinearSchedule.__init__": [[77, 95], ["None"], "methods", ["None"], ["    ", "def", "__init__", "(", "self", ",", "schedule_timesteps", ",", "final_p", ",", "initial_p", "=", "1.0", ")", ":", "\n", "        ", "\"\"\"Linear interpolation between initial_p and final_p over\n        schedule_timesteps. After this many timesteps pass final_p is\n        returned.\n\n        Parameters\n        ----------\n        schedule_timesteps: int\n            Number of timesteps for which to linearly anneal initial_p\n            to final_p\n        initial_p: float\n            initial output value\n        final_p: float\n            final output value\n        \"\"\"", "\n", "self", ".", "schedule_timesteps", "=", "schedule_timesteps", "\n", "self", ".", "final_p", "=", "final_p", "\n", "self", ".", "initial_p", "=", "initial_p", "\n", "\n"]], "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.utils.schedules.LinearSchedule.value": [[96, 100], ["min", "float"], "methods", ["home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.utils.segment_tree.MinSegmentTree.min"], ["", "def", "value", "(", "self", ",", "t", ")", ":", "\n", "        ", "\"\"\"See Schedule.value\"\"\"", "\n", "fraction", "=", "min", "(", "float", "(", "t", ")", "/", "self", ".", "schedule_timesteps", ",", "1.0", ")", "\n", "return", "self", ".", "initial_p", "+", "fraction", "*", "(", "self", ".", "final_p", "-", "self", ".", "initial_p", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.utils.schedules.TimeStepSchedule.__init__": [[102, 105], ["None"], "methods", ["None"], ["    ", "def", "__init__", "(", "self", ",", "step", "=", "1", ",", "initial_p", "=", "1.0", ")", ":", "\n", "        ", "self", ".", "step", "=", "step", "\n", "self", ".", "initial_p", "=", "initial_p", "\n", "\n"]], "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.utils.schedules.TimeStepSchedule.value": [[106, 108], ["None"], "methods", ["None"], ["", "def", "value", "(", "self", ",", "t", ")", ":", "\n", "        ", "return", "self", ".", "initial_p", "+", "t", "//", "self", ".", "step", "\n", "", "", ""]], "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.utils.schedules.linear_interpolation": [[34, 36], ["None"], "function", ["None"], ["", "", "def", "linear_interpolation", "(", "l", ",", "r", ",", "alpha", ")", ":", "\n", "    ", "return", "l", "+", "alpha", "*", "(", "r", "-", "l", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.utils.running_std.RunningMeanStd.__init__": [[8, 19], ["sklearn.preprocessing.StandardScaler", "numpy.zeros", "numpy.ones", "numpy.ones", "len", "collections.deque"], "methods", ["None"], ["\t", "def", "__init__", "(", "self", ",", "batch_size", "=", "0", ",", "shape", "=", "(", ")", ")", ":", "\n", "\t\t", "self", ".", "shape", "=", "shape", "\n", "self", ".", "do_reshape", "=", "len", "(", "self", ".", "shape", ")", ">", "1", "\n", "self", ".", "scaler", "=", "StandardScaler", "(", "copy", "=", "False", ")", "\n", "self", ".", "batch_size", "=", "batch_size", "\n", "if", "self", ".", "batch_size", ">", "0", ":", "\n", "\t\t\t", "self", ".", "batch", "=", "deque", "(", "maxlen", "=", "self", ".", "batch_size", ")", "\n", "", "self", ".", "initialized", "=", "False", "\n", "self", ".", "mean", "=", "np", ".", "zeros", "(", "self", ".", "shape", ",", "np", ".", "float16", ")", "\n", "self", ".", "var", "=", "np", ".", "ones", "(", "self", ".", "shape", ",", "np", ".", "float16", ")", "\n", "self", ".", "std", "=", "np", ".", "ones", "(", "self", ".", "shape", ",", "np", ".", "float16", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.utils.running_std.RunningMeanStd.copy": [[20, 26], ["running_std.RunningMeanStd"], "methods", ["None"], ["", "def", "copy", "(", "self", ")", ":", "\n", "\t\t", "new_obj", "=", "RunningMeanStd", "(", "batch_size", "=", "self", ".", "batch_size", ")", "\n", "new_obj", ".", "scaler", "=", "self", ".", "scaler", "\n", "new_obj", ".", "initialized", "=", "self", ".", "initialized", "\n", "# return without copying the batch, for saving memory", "\n", "return", "new_obj", "\n", "\n"]], "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.utils.running_std.RunningMeanStd.update": [[27, 50], ["numpy.reshape().astype", "running_std.RunningMeanStd.scaler.partial_fit", "running_std.RunningMeanStd.scaler.mean_.astype", "running_std.RunningMeanStd.scaler.var_.astype", "running_std.RunningMeanStd.scaler.scale_.astype", "running_std.RunningMeanStd.batch.extend", "numpy.reshape", "numpy.reshape", "numpy.reshape", "len", "collections.deque", "numpy.reshape", "len"], "methods", ["None"], ["", "def", "update", "(", "self", ",", "x", ")", ":", "\n", "\t\t", "if", "self", ".", "batch_size", ">", "0", ":", "\n", "\t\t\t", "self", ".", "batch", ".", "extend", "(", "x", ")", "\n", "if", "len", "(", "self", ".", "batch", ")", ">=", "self", ".", "batch_size", ":", "\n", "\t\t\t\t", "batch", "=", "self", ".", "batch", "\n", "self", ".", "batch", "=", "deque", "(", "maxlen", "=", "self", ".", "batch_size", ")", "\n", "", "else", ":", "\n", "\t\t\t\t", "batch", "=", "None", "\n", "", "", "else", ":", "\n", "\t\t\t", "batch", "=", "x", "\n", "", "if", "batch", "is", "None", ":", "\n", "\t\t\t", "return", "False", "\n", "", "batch", "=", "np", ".", "reshape", "(", "batch", ",", "(", "len", "(", "batch", ")", ",", "-", "1", ")", ")", ".", "astype", "(", "np", ".", "float64", ")", "\n", "self", ".", "scaler", ".", "partial_fit", "(", "batch", ")", "\n", "self", ".", "mean", "=", "self", ".", "scaler", ".", "mean_", ".", "astype", "(", "np", ".", "float16", ")", "\n", "self", ".", "var", "=", "self", ".", "scaler", ".", "var_", ".", "astype", "(", "np", ".", "float16", ")", "\n", "self", ".", "std", "=", "self", ".", "scaler", ".", "scale_", ".", "astype", "(", "np", ".", "float16", ")", "\n", "if", "self", ".", "do_reshape", ":", "\n", "\t\t\t", "self", ".", "mean", "=", "np", ".", "reshape", "(", "self", ".", "mean", ",", "self", ".", "shape", ")", "\n", "self", ".", "var", "=", "np", ".", "reshape", "(", "self", ".", "var", ",", "self", ".", "shape", ")", "\n", "self", ".", "std", "=", "np", ".", "reshape", "(", "self", ".", "std", ",", "self", ".", "shape", ")", "\n", "", "self", ".", "initialized", "=", "True", "\n", "return", "True", "\n", "", "", ""]], "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.utils.segment_tree.SegmentTree.__init__": [[6, 31], ["None"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "capacity", ",", "neutral_element", ")", ":", "\n", "\t\t", "\"\"\"Build a Segment Tree data structure.\n\t\thttps://en.wikipedia.org/wiki/Segment_tree\n\t\tCan be used as regular array, but with two\n\t\timportant differences:\n\t\t\ta) setting item's value is slightly slower.\n\t\t\t   It is O(log capacity) instead of O(1).\n\t\t\tb) user has access to an efficient ( O(log segment size) )\n\t\t\t   `reduce` operation which reduces `operation` over\n\t\t\t   a contiguous subsequence of items in the array.\n\t\tParamters\n\t\t---------\n\t\tcapacity: int\n\t\t\tTotal size of the array - must be a power of two.\n\t\toperation: lambda obj, obj -> obj\n\t\t\tand operation for combining elements (eg. sum, max)\n\t\t\tmust form a mathematical group together with the set of\n\t\t\tpossible values for array elements (i.e. be associative)\n\t\tneutral_element: obj\n\t\t\tneutral element for the operation above. eg. float('-inf')\n\t\t\tfor max and 0 for sum.\n\t\t\"\"\"", "\n", "assert", "capacity", ">", "0", "and", "capacity", "&", "(", "capacity", "-", "1", ")", "==", "0", ",", "\"capacity must be positive and a power of 2.\"", "\n", "self", ".", "_capacity", "=", "capacity", "\n", "self", ".", "_value", "=", "[", "neutral_element", "]", "*", "(", "2", "*", "capacity", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.utils.segment_tree.SegmentTree._reduce_helper": [[32, 45], ["segment_tree.SegmentTree._reduce_helper", "segment_tree.SegmentTree._reduce_helper", "segment_tree.SegmentTree._operation", "segment_tree.SegmentTree._reduce_helper", "segment_tree.SegmentTree._reduce_helper"], "methods", ["home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.utils.segment_tree.SegmentTree._reduce_helper", "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.utils.segment_tree.SegmentTree._reduce_helper", "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.utils.segment_tree.MaxSegmentTree._operation", "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.utils.segment_tree.SegmentTree._reduce_helper", "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.utils.segment_tree.SegmentTree._reduce_helper"], ["", "def", "_reduce_helper", "(", "self", ",", "start", ",", "end", ",", "node", ",", "node_start", ",", "node_end", ")", ":", "# O(log)", "\n", "\t\t", "if", "(", "start", "==", "node_start", "and", "end", "==", "node_end", ")", "or", "node_start", ">=", "node_end", ":", "\n", "\t\t\t", "return", "self", ".", "_value", "[", "node", "]", "\n", "", "mid", "=", "(", "node_start", "+", "node_end", ")", "//", "2", "\n", "if", "end", "<=", "mid", ":", "\n", "\t\t\t", "return", "self", ".", "_reduce_helper", "(", "start", ",", "end", ",", "2", "*", "node", ",", "node_start", ",", "mid", ")", "\n", "", "else", ":", "\n", "\t\t\t", "if", "mid", "+", "1", "<=", "start", ":", "\n", "\t\t\t\t", "return", "self", ".", "_reduce_helper", "(", "start", ",", "end", ",", "2", "*", "node", "+", "1", ",", "mid", "+", "1", ",", "node_end", ")", "\n", "", "else", ":", "\n", "\t\t\t\t", "return", "self", ".", "_operation", "(", "\n", "self", ".", "_reduce_helper", "(", "start", ",", "mid", ",", "2", "*", "node", ",", "node_start", ",", "mid", ")", ",", "\n", "self", ".", "_reduce_helper", "(", "mid", "+", "1", ",", "end", ",", "2", "*", "node", "+", "1", ",", "mid", "+", "1", ",", "node_end", ")", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.utils.segment_tree.SegmentTree.reduce": [[47, 68], ["segment_tree.SegmentTree._reduce_helper"], "methods", ["home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.utils.segment_tree.SegmentTree._reduce_helper"], ["", "", "", "def", "reduce", "(", "self", ",", "start", "=", "0", ",", "end", "=", "None", ")", ":", "# O(log)", "\n", "\t\t", "\"\"\"Returns result of applying `self.operation`\n\t\tto a contiguous subsequence of the array.\n\t\t\tself.operation(arr[start], operation(arr[start+1], operation(... arr[end])))\n\t\tParameters\n\t\t----------\n\t\tstart: int\n\t\t\tbeginning of the subsequence\n\t\tend: int\n\t\t\tend of the subsequences\n\t\tReturns\n\t\t-------\n\t\treduced: obj\n\t\t\tresult of reducing self.operation over the specified range of array elements.\n\t\t\"\"\"", "\n", "if", "end", "is", "None", ":", "\n", "\t\t\t", "end", "=", "self", ".", "_capacity", "\n", "", "if", "end", "<", "0", ":", "\n", "\t\t\t", "end", "+=", "self", ".", "_capacity", "\n", "", "end", "-=", "1", "\n", "return", "self", ".", "_reduce_helper", "(", "start", ",", "end", ",", "1", ",", "0", ",", "self", ".", "_capacity", "-", "1", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.utils.segment_tree.SegmentTree.__setitem__": [[69, 80], ["segment_tree.SegmentTree._operation"], "methods", ["home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.utils.segment_tree.MaxSegmentTree._operation"], ["", "def", "__setitem__", "(", "self", ",", "idx", ",", "val", ")", ":", "# O(log)", "\n", "# index of the leaf", "\n", "\t\t", "idx", "+=", "self", ".", "_capacity", "\n", "self", ".", "_value", "[", "idx", "]", "=", "val", "\n", "idx", "//=", "2", "\n", "while", "idx", ">=", "1", ":", "\n", "\t\t\t", "self", ".", "_value", "[", "idx", "]", "=", "self", ".", "_operation", "(", "\n", "self", ".", "_value", "[", "2", "*", "idx", "]", ",", "\n", "self", ".", "_value", "[", "2", "*", "idx", "+", "1", "]", "\n", ")", "\n", "idx", "//=", "2", "\n", "\n"]], "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.utils.segment_tree.SegmentTree.__getitem__": [[81, 84], ["None"], "methods", ["None"], ["", "", "def", "__getitem__", "(", "self", ",", "idx", ")", ":", "# O(1)", "\n", "\t\t", "assert", "0", "<=", "idx", "<", "self", ".", "_capacity", "\n", "return", "self", ".", "_value", "[", "self", ".", "_capacity", "+", "idx", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.utils.segment_tree.SumSegmentTree.__init__": [[87, 91], ["segment_tree.SegmentTree.__init__"], "methods", ["home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.worker.batch.CompositeBatch.__init__"], ["\t", "def", "__init__", "(", "self", ",", "capacity", ",", "neutral_element", "=", "0.", ")", ":", "\n", "\t\t", "super", "(", "SumSegmentTree", ",", "self", ")", ".", "__init__", "(", "\n", "capacity", "=", "capacity", ",", "\n", "neutral_element", "=", "neutral_element", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.utils.segment_tree.SumSegmentTree._operation": [[93, 96], ["None"], "methods", ["None"], ["", "@", "staticmethod", "\n", "def", "_operation", "(", "a", ",", "b", ")", ":", "\n", "\t\t", "return", "a", "+", "b", "\n", "\n"]], "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.utils.segment_tree.SumSegmentTree.sum": [[97, 100], ["segment_tree.SegmentTree.reduce"], "methods", ["home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.utils.segment_tree.SegmentTree.reduce"], ["", "def", "sum", "(", "self", ",", "start", "=", "0", ",", "end", "=", "None", ")", ":", "# O(log)", "\n", "\t\t", "\"\"\"Returns arr[start] + ... + arr[end]\"\"\"", "\n", "return", "super", "(", "SumSegmentTree", ",", "self", ")", ".", "reduce", "(", "start", ",", "end", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.utils.segment_tree.SumSegmentTree.find_prefixsum_idx": [[101, 125], ["None"], "methods", ["None"], ["", "def", "find_prefixsum_idx", "(", "self", ",", "prefixsum", ")", ":", "# O(log)", "\n", "\t\t", "\"\"\"Find the highest index `i` in the array such that\n\t\t\tsum(arr[0] + arr[1] + ... + arr[i - i]) <= prefixsum\n\t\tif array values are probabilities, this function\n\t\tallows to sample indexes according to the discrete\n\t\tprobability efficiently.\n\t\tParameters\n\t\t----------\n\t\tperfixsum: float\n\t\t\tupperbound on the sum of array prefix\n\t\tReturns\n\t\t-------\n\t\tidx: int\n\t\t\thighest index satisfying the prefixsum constraint\n\t\t\"\"\"", "\n", "# assert 0 <= prefixsum <= self.sum() + 1e-5 # O(1)", "\n", "idx", "=", "1", "\n", "while", "idx", "<", "self", ".", "_capacity", ":", "# while non-leaf", "\n", "\t\t\t", "if", "self", ".", "_value", "[", "2", "*", "idx", "]", ">", "prefixsum", ":", "\n", "\t\t\t\t", "idx", "=", "2", "*", "idx", "\n", "", "else", ":", "\n", "\t\t\t\t", "prefixsum", "-=", "self", ".", "_value", "[", "2", "*", "idx", "]", "\n", "idx", "=", "2", "*", "idx", "+", "1", "\n", "", "", "return", "idx", "-", "self", ".", "_capacity", "\n", "\n"]], "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.utils.segment_tree.MinSegmentTree.__init__": [[127, 131], ["float", "segment_tree.SegmentTree.__init__"], "methods", ["home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.worker.batch.CompositeBatch.__init__"], ["\t", "def", "__init__", "(", "self", ",", "capacity", ",", "neutral_element", "=", "float", "(", "'inf'", ")", ")", ":", "\n", "\t\t", "super", "(", "MinSegmentTree", ",", "self", ")", ".", "__init__", "(", "\n", "capacity", "=", "capacity", ",", "\n", "neutral_element", "=", "neutral_element", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.utils.segment_tree.MinSegmentTree._operation": [[133, 136], ["None"], "methods", ["None"], ["", "@", "staticmethod", "\n", "def", "_operation", "(", "a", ",", "b", ")", ":", "\n", "\t\t", "return", "a", "if", "a", "<", "b", "else", "b", "\n", "\n"]], "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.utils.segment_tree.MinSegmentTree.min": [[137, 140], ["segment_tree.SegmentTree.reduce"], "methods", ["home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.utils.segment_tree.SegmentTree.reduce"], ["", "def", "min", "(", "self", ",", "start", "=", "0", ",", "end", "=", "None", ")", ":", "# O(log)", "\n", "\t\t", "\"\"\"Returns min(arr[start], ...,  arr[end])\"\"\"", "\n", "return", "super", "(", "MinSegmentTree", ",", "self", ")", ".", "reduce", "(", "start", ",", "end", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.utils.segment_tree.MaxSegmentTree.__init__": [[142, 146], ["float", "segment_tree.SegmentTree.__init__"], "methods", ["home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.worker.batch.CompositeBatch.__init__"], ["\t", "def", "__init__", "(", "self", ",", "capacity", ",", "neutral_element", "=", "float", "(", "'-inf'", ")", ")", ":", "\n", "\t\t", "super", "(", "MinSegmentTree", ",", "self", ")", ".", "__init__", "(", "\n", "capacity", "=", "capacity", ",", "\n", "neutral_element", "=", "neutral_element", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.utils.segment_tree.MaxSegmentTree._operation": [[148, 151], ["None"], "methods", ["None"], ["", "@", "staticmethod", "\n", "def", "_operation", "(", "a", ",", "b", ")", ":", "\n", "\t\t", "return", "a", "if", "a", ">", "b", "else", "b", "\n", "\n"]], "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.utils.segment_tree.MaxSegmentTree.max": [[152, 155], ["segment_tree.SegmentTree.reduce"], "methods", ["home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.utils.segment_tree.SegmentTree.reduce"], ["", "def", "max", "(", "self", ",", "start", "=", "0", ",", "end", "=", "None", ")", ":", "# O(log)", "\n", "\t\t", "\"\"\"Returns min(arr[start], ...,  arr[end])\"\"\"", "\n", "return", "super", "(", "MaxSegmentTree", ",", "self", ")", ".", "reduce", "(", "start", ",", "end", ")", "", "", "", ""]], "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.utils.tensorflow_utils.separate": [[6, 14], ["tensorflow.shape", "tensorflow.ones", "tensorflow.zeros", "tensorflow.where", "tensorflow.greater_equal"], "function", ["None"], ["def", "separate", "(", "input", ",", "value", ")", ":", "\n", "\t", "input_shape", "=", "tf", ".", "shape", "(", "input", ")", "\n", "true_labels", "=", "tf", ".", "ones", "(", "input_shape", ")", "\n", "false_labels", "=", "tf", ".", "zeros", "(", "input_shape", ")", "\n", "mask", "=", "tf", ".", "where", "(", "tf", ".", "greater_equal", "(", "input", ",", "value", ")", ",", "true_labels", ",", "false_labels", ")", "\n", "greater_equal", "=", "mask", "*", "input", "\n", "lower", "=", "input", "-", "greater_equal", "\n", "return", "greater_equal", ",", "lower", "\n", "\n"]], "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.utils.tensorflow_utils.get_optimization_function": [[15, 18], ["hasattr", "eval", "eval"], "function", ["None"], ["", "def", "get_optimization_function", "(", "name", ")", ":", "\n", "\t", "opt_name", "=", "name", "+", "'Optimizer'", "\n", "return", "eval", "(", "'tf.train.'", "+", "opt_name", ")", "if", "hasattr", "(", "tf", ".", "train", ",", "opt_name", ")", "else", "eval", "(", "'tf.contrib.opt.'", "+", "opt_name", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.utils.tensorflow_utils.get_annealable_variable": [[19, 21], ["eval"], "function", ["None"], ["", "def", "get_annealable_variable", "(", "function_name", ",", "initial_value", ",", "global_step", ",", "decay_steps", ",", "decay_rate", ")", ":", "\n", "\t", "return", "eval", "(", "'tf.train.'", "+", "function_name", ")", "(", "learning_rate", "=", "initial_value", ",", "global_step", "=", "global_step", ",", "decay_steps", "=", "decay_steps", ",", "decay_rate", "=", "decay_rate", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.utils.tensorflow_utils.get_available_gpus": [[22, 26], ["tensorflow.python.client.device_lib.list_local_devices"], "function", ["None"], ["", "def", "get_available_gpus", "(", ")", ":", "\n", "# recipe from here: https://stackoverflow.com/questions/38559755/how-to-get-current-available-gpus-in-tensorflow?utm_medium=organic&utm_source=google_rich_qa&utm_campaign=google_rich_qa", "\n", "\t", "local_device_protos", "=", "device_lib", ".", "list_local_devices", "(", ")", "\n", "return", "[", "x", ".", "name", "for", "x", "in", "local_device_protos", "if", "x", ".", "device_type", "==", "'GPU'", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.utils.tensorflow_utils.gpu_count": [[27, 29], ["len", "tensorflow_utils.get_available_gpus"], "function", ["home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.utils.tensorflow_utils.get_available_gpus"], ["", "def", "gpu_count", "(", ")", ":", "\n", "\t", "return", "len", "(", "get_available_gpus", "(", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.utils.tensorflow_utils.orthogonal_initializer": [[30, 46], ["tuple", "numpy.random.normal", "numpy.linalg.svd", "q.reshape.reshape", "len", "len", "numpy.prod"], "function", ["None"], ["", "def", "orthogonal_initializer", "(", "scale", "=", "1.0", ")", ":", "\n", "    ", "def", "_ortho_init", "(", "shape", ",", "dtype", ",", "partition_info", "=", "None", ")", ":", "\n", "#lasagne ortho init for tf", "\n", "        ", "shape", "=", "tuple", "(", "shape", ")", "\n", "if", "len", "(", "shape", ")", "==", "2", ":", "\n", "            ", "flat_shape", "=", "shape", "\n", "", "elif", "len", "(", "shape", ")", "==", "4", ":", "# assumes NHWC", "\n", "            ", "flat_shape", "=", "(", "np", ".", "prod", "(", "shape", "[", ":", "-", "1", "]", ")", ",", "shape", "[", "-", "1", "]", ")", "\n", "", "else", ":", "\n", "            ", "raise", "NotImplementedError", "\n", "", "a", "=", "np", ".", "random", ".", "normal", "(", "0.0", ",", "1.0", ",", "flat_shape", ")", "\n", "u", ",", "_", ",", "v", "=", "np", ".", "linalg", ".", "svd", "(", "a", ",", "full_matrices", "=", "False", ")", "\n", "q", "=", "u", "if", "u", ".", "shape", "==", "flat_shape", "else", "v", "# pick the one with the correct shape", "\n", "q", "=", "q", ".", "reshape", "(", "shape", ")", "\n", "return", "(", "scale", "*", "q", "[", ":", "shape", "[", "0", "]", ",", ":", "shape", "[", "1", "]", "]", ")", ".", "astype", "(", "np", ".", "float32", ")", "\n", "", "return", "_ortho_init", "", "", ""]], "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.utils.important_information.ImportantInformation.__init__": [[8, 11], ["isinstance"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "obj", ",", "name", ")", ":", "\n", "\t\t", "assert", "isinstance", "(", "obj", ",", "object", ")", ",", "'Important information must be an object'", "\n", "self", ".", "instances", "[", "name", "]", "=", "obj", "\n", "\n"]], "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.utils.important_information.ImportantInformation.get": [[12, 15], ["None"], "methods", ["None"], ["", "@", "staticmethod", "\n", "def", "get", "(", ")", ":", "\n", "\t\t", "return", "ImportantInformation", ".", "instances", "\n", "\n"]], "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.utils.important_information.ImportantInformation.set": [[16, 25], ["instances.items", "print", "important_information.ImportantInformation.load", "traceback.print_exc"], "methods", ["home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.utils.important_information.ImportantInformation.load"], ["", "@", "staticmethod", "\n", "def", "set", "(", "instances", ")", ":", "\n", "\t\t", "for", "name", ",", "source_obj", "in", "instances", ".", "items", "(", ")", ":", "\n", "\t\t\t", "print", "(", "'Loading important object:'", ",", "name", ")", "\n", "try", ":", "\n", "\t\t\t\t", "target_obj", "=", "ImportantInformation", ".", "instances", "[", "name", "]", "\n", "ImportantInformation", ".", "load", "(", "source_obj", ",", "target_obj", ")", "\n", "", "except", ":", "\n", "\t\t\t\t", "traceback", ".", "print_exc", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.utils.important_information.ImportantInformation.load": [[26, 48], ["type", "zip", "important_information.ImportantInformation.load", "source_obj.items", "list", "print", "print", "hasattr", "list", "itertools.chain.from_iterable", "getattr", "setattr", "source_obj.__dict__.keys", "getattr"], "methods", ["home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.utils.important_information.ImportantInformation.load"], ["", "", "", "@", "staticmethod", "\n", "def", "load", "(", "source_obj", ",", "target_obj", ")", ":", "\n", "\t\t", "vtype", "=", "type", "(", "source_obj", ")", "\n", "# loading lists and tuples", "\n", "if", "vtype", "in", "[", "list", ",", "tuple", "]", ":", "\n", "\t\t\t", "for", "s", ",", "t", "in", "zip", "(", "source_obj", ",", "target_obj", ")", ":", "\n", "\t\t\t\t", "ImportantInformation", ".", "load", "(", "s", ",", "t", ")", "\n", "# loading dictionaries", "\n", "", "", "elif", "vtype", "is", "dict", ":", "\n", "\t\t\t", "for", "key", ",", "value", "in", "source_obj", ".", "items", "(", ")", ":", "\n", "\t\t\t\t", "target_obj", "[", "key", "]", "=", "value", "\n", "# loading other objects", "\n", "", "", "else", ":", "\n", "\t\t\t", "dicts", "=", "list", "(", "source_obj", ".", "__dict__", ".", "keys", "(", ")", ")", "if", "hasattr", "(", "source_obj", ",", "'__dict__'", ")", "else", "[", "]", "\n", "slots", "=", "list", "(", "chain", ".", "from_iterable", "(", "getattr", "(", "cls", ",", "'__slots__'", ",", "[", "]", ")", "for", "cls", "in", "source_obj", ".", "__class__", ".", "__mro__", ")", ")", "\n", "print", "(", "'\t..slots'", ",", "slots", ")", "\n", "print", "(", "'\t..dicts'", ",", "dicts", ")", "\n", "# get slot from current class and parent classes (if any)", "\n", "for", "key", "in", "slots", "+", "dicts", ":", "\n", "# print('\t..loading attribute', key)", "\n", "\t\t\t\t", "value", "=", "getattr", "(", "source_obj", ",", "key", ")", "\n", "setattr", "(", "target_obj", ",", "key", ",", "value", ")", "", "", "", "", "", ""]], "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.utils.distributions.Categorical.__init__": [[6, 9], ["distributions.Categorical.logits.get_shape().as_list", "distributions.Categorical.logits.get_shape"], "methods", ["None"], ["\t", "def", "__init__", "(", "self", ",", "logits", ")", ":", "\n", "\t\t", "self", ".", "logits", "=", "logits", "\n", "self", ".", "logits_shape", "=", "self", ".", "logits", ".", "get_shape", "(", ")", ".", "as_list", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.utils.distributions.Categorical.is_multi_categorical": [[10, 12], ["len"], "methods", ["None"], ["", "def", "is_multi_categorical", "(", "self", ")", ":", "\n", "\t\t", "return", "len", "(", "self", ".", "logits_shape", ")", ">", "2", "\n", "\n"]], "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.utils.distributions.Categorical.mean": [[13, 15], ["tensorflow.contrib.layers.softmax"], "methods", ["home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.utils.misc.softmax"], ["", "def", "mean", "(", "self", ")", ":", "\n", "\t\t", "return", "tf", ".", "contrib", ".", "layers", ".", "softmax", "(", "self", ".", "logits", ")", "# automatically handles the multi-categorical case", "\n", "\n"]], "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.utils.distributions.Categorical.mode": [[16, 18], ["tensorflow.argmax"], "methods", ["None"], ["", "def", "mode", "(", "self", ")", ":", "\n", "\t\t", "return", "tf", ".", "argmax", "(", "self", ".", "logits", ",", "axis", "=", "-", "1", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.utils.distributions.Categorical.kl_divergence": [[19, 29], ["isinstance", "tensorflow.exp", "tensorflow.exp", "tensorflow.reduce_sum", "tensorflow.reduce_sum", "tensorflow.reduce_sum", "tensorflow.reduce_max", "tensorflow.reduce_max", "tensorflow.log", "tensorflow.log"], "methods", ["None"], ["", "def", "kl_divergence", "(", "self", ",", "other", ")", ":", "# https://en.wikipedia.org/wiki/Kullback-Leibler_divergence", "\n", "\t\t", "assert", "isinstance", "(", "other", ",", "Categorical", ")", "\n", "a0", "=", "self", ".", "logits", "-", "tf", ".", "reduce_max", "(", "self", ".", "logits", ",", "axis", "=", "-", "1", ",", "keepdims", "=", "True", ")", "\n", "a1", "=", "other", ".", "logits", "-", "tf", ".", "reduce_max", "(", "other", ".", "logits", ",", "axis", "=", "-", "1", ",", "keepdims", "=", "True", ")", "\n", "ea0", "=", "tf", ".", "exp", "(", "a0", ")", "\n", "ea1", "=", "tf", ".", "exp", "(", "a1", ")", "\n", "z0", "=", "tf", ".", "reduce_sum", "(", "ea0", ",", "axis", "=", "-", "1", ",", "keepdims", "=", "True", ")", "\n", "z1", "=", "tf", ".", "reduce_sum", "(", "ea1", ",", "axis", "=", "-", "1", ",", "keepdims", "=", "True", ")", "\n", "p0", "=", "ea0", "/", "z0", "\n", "return", "tf", ".", "reduce_sum", "(", "p0", "*", "(", "a0", "-", "tf", ".", "log", "(", "z0", ")", "-", "a1", "+", "tf", ".", "log", "(", "z1", ")", ")", ",", "axis", "=", "-", "1", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.utils.distributions.Categorical.cross_entropy": [[30, 32], ["tensorflow.nn.softmax_cross_entropy_with_logits_v2"], "methods", ["None"], ["", "def", "cross_entropy", "(", "self", ",", "samples", ")", ":", "\n", "\t\t", "return", "tf", ".", "nn", ".", "softmax_cross_entropy_with_logits_v2", "(", "labels", "=", "samples", ",", "logits", "=", "self", ".", "logits", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.utils.distributions.Categorical.entropy": [[33, 39], ["tensorflow.exp", "tensorflow.reduce_sum", "tensorflow.reduce_sum", "tensorflow.reduce_max", "tensorflow.log"], "methods", ["None"], ["", "def", "entropy", "(", "self", ")", ":", "\n", "\t\t", "scaled_logits", "=", "self", ".", "logits", "-", "tf", ".", "reduce_max", "(", "self", ".", "logits", ",", "axis", "=", "-", "1", ",", "keepdims", "=", "True", ")", "\n", "exp_scaled_logits", "=", "tf", ".", "exp", "(", "scaled_logits", ")", "\n", "sum_exp_scaled_logits", "=", "tf", ".", "reduce_sum", "(", "exp_scaled_logits", ",", "axis", "=", "-", "1", ",", "keepdims", "=", "True", ")", "\n", "avg_exp_scaled_logits", "=", "exp_scaled_logits", "/", "sum_exp_scaled_logits", "\n", "return", "tf", ".", "reduce_sum", "(", "avg_exp_scaled_logits", "*", "(", "tf", ".", "log", "(", "sum_exp_scaled_logits", ")", "-", "scaled_logits", ")", ",", "axis", "=", "-", "1", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.utils.distributions.Categorical.sample": [[40, 51], ["distributions.Categorical.is_multi_categorical", "tensorflow.random_uniform", "tensorflow.argmax", "distributions.Categorical.is_multi_categorical", "tensorflow.reshape", "tensorflow.shape", "tensorflow.reshape", "distributions.Categorical.get_sample_one_hot", "tensorflow.log", "tensorflow.log"], "methods", ["home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.utils.distributions.Categorical.is_multi_categorical", "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.utils.distributions.Categorical.is_multi_categorical", "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.utils.distributions.Categorical.get_sample_one_hot"], ["", "def", "sample", "(", "self", ",", "one_hot", "=", "True", ")", ":", "\n", "\t\t", "depth", "=", "self", ".", "logits_shape", "[", "-", "1", "]", "# depth of the one hot vector", "\n", "if", "self", ".", "is_multi_categorical", "(", ")", ":", "# multi-categorical sampling", "\n", "\t\t\t", "logits", "=", "tf", ".", "reshape", "(", "self", ".", "logits", ",", "[", "-", "1", ",", "depth", "]", ")", "\n", "", "else", ":", "\n", "\t\t\t", "logits", "=", "self", ".", "logits", "\n", "", "u", "=", "tf", ".", "random_uniform", "(", "tf", ".", "shape", "(", "logits", ")", ",", "dtype", "=", "logits", ".", "dtype", ")", "\n", "samples", "=", "tf", ".", "argmax", "(", "logits", "-", "tf", ".", "log", "(", "-", "tf", ".", "log", "(", "u", ")", ")", ",", "axis", "=", "-", "1", ")", "\n", "if", "self", ".", "is_multi_categorical", "(", ")", ":", "# multi-categorical sampling", "\n", "\t\t\t", "samples", "=", "tf", ".", "reshape", "(", "samples", ",", "[", "-", "1", ",", "self", ".", "logits_shape", "[", "-", "2", "]", "]", ")", "\n", "", "return", "self", ".", "get_sample_one_hot", "(", "samples", ")", "if", "one_hot", "else", "samples", "\n", "\n"]], "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.utils.distributions.Categorical.get_sample_one_hot": [[52, 55], ["tensorflow.one_hot"], "methods", ["None"], ["", "def", "get_sample_one_hot", "(", "self", ",", "samples", ")", ":", "\n", "\t\t", "depth", "=", "self", ".", "logits_shape", "[", "-", "1", "]", "# depth of the one hot vector", "\n", "return", "tf", ".", "one_hot", "(", "indices", "=", "samples", ",", "depth", "=", "depth", ",", "dtype", "=", "tf", ".", "uint8", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.utils.distributions.Normal.__init__": [[58, 62], ["tensorflow.distributions.Normal"], "methods", ["None"], ["\t", "def", "__init__", "(", "self", ",", "mean", ",", "std", ")", ":", "\n", "\t\t", "self", ".", "mu", "=", "mean", "\n", "self", ".", "std", "=", "std", "\n", "self", ".", "distribution", "=", "tf", ".", "distributions", ".", "Normal", "(", "mean", ",", "std", ",", "validate_args", "=", "False", ")", "# validate_args is computationally more expensive", "\n", "\n"]], "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.utils.distributions.Normal.mean": [[63, 65], ["None"], "methods", ["None"], ["", "def", "mean", "(", "self", ")", ":", "\n", "\t\t", "return", "self", ".", "mu", "\n", "\n"]], "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.utils.distributions.Normal.mode": [[66, 68], ["None"], "methods", ["None"], ["", "def", "mode", "(", "self", ")", ":", "\n", "\t\t", "return", "self", ".", "mu", "\n", "\n"]], "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.utils.distributions.Normal.kl_divergence": [[69, 72], ["isinstance", "distributions.Normal.distribution.kl_divergence"], "methods", ["home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.utils.distributions.Normal.kl_divergence"], ["", "def", "kl_divergence", "(", "self", ",", "other", ")", ":", "# https://en.wikipedia.org/wiki/Kullback-Leibler_divergence", "\n", "\t\t", "assert", "isinstance", "(", "other", ",", "Normal", ")", "\n", "return", "self", ".", "distribution", ".", "kl_divergence", "(", "other", ".", "distribution", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.utils.distributions.Normal.cross_entropy": [[73, 75], ["distributions.Normal.distribution.log_prob"], "methods", ["None"], ["", "def", "cross_entropy", "(", "self", ",", "samples", ")", ":", "\n", "\t\t", "return", "-", "self", ".", "distribution", ".", "log_prob", "(", "samples", ")", "# probability density function", "\n", "\n"]], "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.utils.distributions.Normal.entropy": [[76, 78], ["distributions.Normal.distribution.entropy"], "methods", ["home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.utils.distributions.Normal.entropy"], ["", "def", "entropy", "(", "self", ")", ":", "\n", "\t\t", "return", "self", ".", "distribution", ".", "entropy", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.utils.distributions.Normal.sample": [[79, 81], ["distributions.Normal.distribution.sample"], "methods", ["home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.buffer.pseudo_prioritized_buffer.PseudoPrioritizedBuffer.sample"], ["", "def", "sample", "(", "self", ")", ":", "\n", "\t\t", "return", "self", ".", "distribution", ".", "sample", "(", ")", "", "", "", ""]], "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.utils.misc.BoolLock.__init__": [[55, 58], ["threading.RLock"], "methods", ["None"], ["\t", "def", "__init__", "(", "self", ",", "use_lock", "=", "False", ")", ":", "\n", "\t\t", "self", ".", "use_lock", "=", "use_lock", "\n", "self", ".", "lock", "=", "RLock", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.utils.misc.BoolLock.release": [[59, 62], ["misc.BoolLock.lock.release"], "methods", ["home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.utils.misc.BoolLock.release"], ["", "def", "release", "(", "self", ")", ":", "\n", "\t\t", "if", "self", ".", "use_lock", ":", "\n", "\t\t\t", "self", ".", "lock", ".", "release", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.utils.misc.BoolLock.acquire": [[63, 66], ["misc.BoolLock.lock.acquire"], "methods", ["home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.utils.misc.BoolLock.acquire"], ["", "", "def", "acquire", "(", "self", ",", "blocking", "=", "True", ",", "timeout", "=", "-", "1", ")", ":", "\n", "\t\t", "if", "self", ".", "use_lock", ":", "\n", "\t\t\t", "self", ".", "lock", ".", "acquire", "(", "blocking", ",", "timeout", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.utils.misc.BoolLock.__enter__": [[67, 69], ["misc.BoolLock.lock.__enter__"], "methods", ["home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.utils.misc.BoolLock.__enter__"], ["", "", "def", "__enter__", "(", "self", ",", "blocking", "=", "True", ",", "timeout", "=", "-", "1", ")", ":", "\n", "\t\t", "return", "self", "if", "not", "self", ".", "use_lock", "else", "self", ".", "lock", ".", "__enter__", "(", "blocking", ",", "timeout", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.utils.misc.BoolLock.__exit__": [[70, 73], ["misc.BoolLock.lock.__exit__"], "methods", ["home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.utils.misc.BoolLock.__exit__"], ["", "def", "__exit__", "(", "self", ",", "exc_type", ",", "exc_val", ",", "exc_tb", ")", ":", "\n", "\t\t", "if", "self", ".", "use_lock", ":", "\n", "\t\t\t", "self", ".", "lock", ".", "__exit__", "(", "exc_type", ",", "exc_val", ",", "exc_tb", ")", "\n", "", "", "", ""]], "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.utils.misc.accumulate": [[9, 18], ["iter", "func"], "function", ["None"], ["def", "accumulate", "(", "iterable", ",", "func", "=", "operator", ".", "add", ",", "initial_value", "=", "0.", ")", ":", "\n", "\t", "total", "=", "initial_value", "\n", "it", "=", "iter", "(", "iterable", ")", "\n", "try", ":", "\n", "\t\t", "for", "element", "in", "it", ":", "\n", "\t\t\t", "total", "=", "func", "(", "total", ",", "element", ")", "\n", "yield", "total", "\n", "", "", "except", "StopIteration", ":", "\n", "\t\t", "return", "\n", "\n"]], "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.utils.misc.get_padded_size": [[19, 21], ["None"], "function", ["None"], ["", "", "def", "get_padded_size", "(", "size", ",", "kernel", ",", "stride", ")", ":", "\n", "\t", "return", "kernel", "if", "size", "<=", "kernel", "else", "size", "+", "(", "stride", "-", "(", "size", "-", "kernel", ")", "%", "stride", ")", "%", "stride", "\n", "\n"]], "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.utils.misc.flatten": [[22, 24], ["None"], "function", ["None"], ["", "def", "flatten", "(", "l", ")", ":", "\n", "\t", "return", "[", "item", "for", "sublist", "in", "l", "for", "item", "in", "sublist", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.utils.misc.softmax": [[25, 28], ["numpy.exp", "numpy.sum"], "function", ["home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.utils.segment_tree.SumSegmentTree.sum"], ["", "def", "softmax", "(", "logits", ",", "axis", "=", "-", "1", ")", ":", "\n", "\t", "val", "=", "np", ".", "exp", "(", "logits", ")", "\n", "return", "val", "/", "np", ".", "sum", "(", "val", ",", "axis", "=", "axis", ",", "keepdims", "=", "True", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.utils.misc.is_tuple": [[29, 31], ["type"], "function", ["None"], ["", "def", "is_tuple", "(", "val", ")", ":", "\n", "\t", "return", "type", "(", "val", ")", "in", "[", "list", ",", "tuple", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.utils.misc.compress": [[32, 41], ["type", "type.", "io.BytesIO", "numpy.savez_compressed", "misc.compress"], "function", ["home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.worker.batch.ExperienceBatch.compress"], ["", "def", "compress", "(", "val", ")", ":", "\n", "\t", "vtype", "=", "type", "(", "val", ")", "\n", "if", "vtype", "in", "[", "list", ",", "tuple", "]", ":", "\n", "\t\t", "return", "vtype", "(", "compress", "(", "v", ")", "for", "v", "in", "val", ")", "\n", "", "elif", "vtype", "is", "np", ".", "ndarray", ":", "\n", "\t\t", "compressed_array", "=", "io", ".", "BytesIO", "(", ")", "\n", "np", ".", "savez_compressed", "(", "compressed_array", ",", "val", ")", "\n", "return", "compressed_array", "\n", "", "return", "val", "\n", "\n"]], "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.utils.misc.decompress": [[42, 50], ["type", "type.", "val.seek", "misc.decompress", "numpy.load"], "function", ["home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.worker.batch.ExperienceBatch.decompress", "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.utils.important_information.ImportantInformation.load"], ["", "def", "decompress", "(", "val", ")", ":", "\n", "\t", "vtype", "=", "type", "(", "val", ")", "\n", "if", "vtype", "in", "[", "list", ",", "tuple", "]", ":", "\n", "\t\t", "return", "vtype", "(", "decompress", "(", "v", ")", "for", "v", "in", "val", ")", "\n", "", "elif", "vtype", "is", "io", ".", "BytesIO", ":", "\n", "\t\t", "val", ".", "seek", "(", "0", ")", "# seek back to the beginning of the file-like object", "\n", "return", "np", ".", "load", "(", "val", ")", "[", "'arr_0'", "]", "\n", "", "return", "val", "\n", "\n"]], "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.utils.misc.get_cpu_count": [[51, 53], ["int", "os.getenv", "multiprocessing.cpu_count"], "function", ["None"], ["", "def", "get_cpu_count", "(", ")", ":", "\n", "\t", "return", "int", "(", "os", ".", "getenv", "(", "'RCALL_NUM_CPU'", ",", "cpu_count", "(", ")", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.buffer.buffer.Buffer.__init__": [[8, 11], ["buffer.Buffer.clean"], "methods", ["home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.buffer.pseudo_prioritized_buffer.PseudoPrioritizedBuffer.clean"], ["def", "__init__", "(", "self", ",", "size", ")", ":", "\n", "\t\t", "self", ".", "size", "=", "size", "\n", "self", ".", "clean", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.buffer.buffer.Buffer.clean": [[12, 17], ["None"], "methods", ["None"], ["", "def", "clean", "(", "self", ")", ":", "\n", "\t\t", "self", ".", "types", "=", "{", "}", "\n", "self", ".", "type_values", "=", "[", "]", "\n", "self", ".", "type_keys", "=", "[", "]", "\n", "self", ".", "batches", "=", "[", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.buffer.buffer.Buffer._add_type_if_not_exist": [[18, 26], ["len", "buffer.Buffer.type_values.append", "buffer.Buffer.type_keys.append", "buffer.Buffer.batches.append", "collections.deque"], "methods", ["home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.worker.batch.ExperienceBatch.append", "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.worker.batch.ExperienceBatch.append", "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.worker.batch.ExperienceBatch.append"], ["", "def", "_add_type_if_not_exist", "(", "self", ",", "type_id", ")", ":", "# private method", "\n", "\t\t", "if", "type_id", "in", "self", ".", "types", ":", "# check it to avoid double insertion", "\n", "\t\t\t", "return", "False", "\n", "", "self", ".", "types", "[", "type_id", "]", "=", "len", "(", "self", ".", "types", ")", "\n", "self", ".", "type_values", ".", "append", "(", "self", ".", "types", "[", "type_id", "]", ")", "\n", "self", ".", "type_keys", ".", "append", "(", "type_id", ")", "\n", "self", ".", "batches", ".", "append", "(", "deque", "(", "maxlen", "=", "self", ".", "size", ")", ")", "\n", "return", "True", "\n", "\n"]], "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.buffer.buffer.Buffer.set": [[27, 31], ["isinstance", "setattr", "getattr"], "methods", ["None"], ["", "def", "set", "(", "self", ",", "buffer", ")", ":", "\n", "\t\t", "assert", "isinstance", "(", "buffer", ",", "Buffer", ")", "\n", "for", "key", "in", "self", ".", "__slots__", ":", "\n", "\t\t\t", "setattr", "(", "self", ",", "key", ",", "getattr", "(", "buffer", ",", "key", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.buffer.buffer.Buffer.get_batches": [[32, 39], ["buffer.Buffer.get_type"], "methods", ["home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.buffer.buffer.Buffer.get_type"], ["", "", "def", "get_batches", "(", "self", ",", "type_id", "=", "None", ")", ":", "\n", "\t\t", "if", "type_id", "is", "None", ":", "\n", "\t\t\t", "result", "=", "[", "]", "\n", "for", "batch", "in", "self", ".", "batches", ":", "\n", "\t\t\t\t", "result", "+=", "batch", "\n", "", "return", "result", "\n", "", "return", "self", ".", "batches", "[", "self", ".", "get_type", "(", "type_id", ")", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.buffer.buffer.Buffer.has_atleast": [[40, 42], ["buffer.Buffer.count"], "methods", ["home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.buffer.buffer.Buffer.count"], ["", "def", "has_atleast", "(", "self", ",", "frames", ",", "type", "=", "None", ")", ":", "\n", "\t\t", "return", "self", ".", "count", "(", "type", ")", ">=", "frames", "\n", "\n"]], "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.buffer.buffer.Buffer.has": [[43, 45], ["buffer.Buffer.count"], "methods", ["home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.buffer.buffer.Buffer.count"], ["", "def", "has", "(", "self", ",", "frames", ",", "type", "=", "None", ")", ":", "\n", "\t\t", "return", "self", ".", "count", "(", "type", ")", "==", "frames", "\n", "\n"]], "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.buffer.buffer.Buffer.count": [[46, 52], ["len", "sum", "len", "len"], "methods", ["home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.utils.segment_tree.SumSegmentTree.sum"], ["", "def", "count", "(", "self", ",", "type", "=", "None", ")", ":", "\n", "\t\t", "if", "type", "is", "None", ":", "\n", "\t\t\t", "if", "len", "(", "self", ".", "batches", ")", "==", "0", ":", "\n", "\t\t\t\t", "return", "0", "\n", "", "return", "sum", "(", "len", "(", "batch", ")", "for", "batch", "in", "self", ".", "batches", ")", "\n", "", "return", "len", "(", "self", ".", "batches", "[", "type", "]", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.buffer.buffer.Buffer.id_is_full": [[53, 55], ["buffer.Buffer.has", "buffer.Buffer.get_type"], "methods", ["home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.buffer.buffer.Buffer.has", "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.buffer.buffer.Buffer.get_type"], ["", "def", "id_is_full", "(", "self", ",", "type_id", ")", ":", "\n", "\t\t", "return", "self", ".", "has", "(", "self", ".", "size", ",", "self", ".", "get_type", "(", "type_id", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.buffer.buffer.Buffer.is_full": [[56, 60], ["buffer.Buffer.has", "buffer.Buffer.has", "len"], "methods", ["home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.buffer.buffer.Buffer.has", "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.buffer.buffer.Buffer.has"], ["", "def", "is_full", "(", "self", ",", "type", "=", "None", ")", ":", "\n", "\t\t", "if", "type", "is", "None", ":", "\n", "\t\t\t", "return", "self", ".", "has", "(", "self", ".", "size", "*", "len", "(", "self", ".", "types", ")", ")", "\n", "", "return", "self", ".", "has", "(", "self", ".", "size", ",", "type", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.buffer.buffer.Buffer.is_empty": [[61, 63], ["buffer.Buffer.has_atleast"], "methods", ["home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.buffer.buffer.Buffer.has_atleast"], ["", "def", "is_empty", "(", "self", ",", "type", "=", "None", ")", ":", "\n", "\t\t", "return", "not", "self", ".", "has_atleast", "(", "1", ",", "type", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.buffer.buffer.Buffer.get_type": [[64, 66], ["None"], "methods", ["None"], ["", "def", "get_type", "(", "self", ",", "type_id", ")", ":", "\n", "\t\t", "return", "self", ".", "types", "[", "type_id", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.buffer.buffer.Buffer.put": [[67, 71], ["buffer.Buffer._add_type_if_not_exist", "buffer.Buffer.get_type", "buffer.Buffer.batches[].append"], "methods", ["home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.buffer.pseudo_prioritized_buffer.PseudoPrioritizedBuffer._add_type_if_not_exist", "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.buffer.buffer.Buffer.get_type", "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.worker.batch.ExperienceBatch.append"], ["", "def", "put", "(", "self", ",", "batch", ",", "type_id", "=", "0", ")", ":", "# put batch into buffer", "\n", "\t\t", "self", ".", "_add_type_if_not_exist", "(", "type_id", ")", "\n", "type", "=", "self", ".", "get_type", "(", "type_id", ")", "\n", "self", ".", "batches", "[", "type", "]", ".", "append", "(", "batch", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.buffer.buffer.Buffer.sample": [[72, 75], ["random.choice", "random.choice"], "methods", ["None"], ["", "def", "sample", "(", "self", ")", ":", "\n", "\t\t", "type", "=", "choice", "(", "self", ".", "type_values", ")", "\n", "return", "choice", "(", "self", ".", "batches", "[", "type", "]", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.buffer.prioritized_buffer.PrioritizedBuffer.__init__": [[10, 13], ["utils.buffer.buffer.Buffer.__init__"], "methods", ["home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.worker.batch.CompositeBatch.__init__"], ["def", "__init__", "(", "self", ",", "size", ",", "alpha", "=", "1", ")", ":", "\n", "\t\t", "self", ".", "alpha", "=", "alpha", "# how much prioritization is used (0 - no prioritization, 1 - full prioritization)", "\n", "super", "(", ")", ".", "__init__", "(", "size", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.buffer.prioritized_buffer.PrioritizedBuffer.set": [[14, 17], ["isinstance", "super().set"], "methods", ["home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.buffer.pseudo_prioritized_buffer.PseudoPrioritizedBuffer.set"], ["", "def", "set", "(", "self", ",", "buffer", ")", ":", "\n", "\t\t", "assert", "isinstance", "(", "buffer", ",", "PrioritizedBuffer", ")", "\n", "super", "(", ")", ".", "set", "(", "buffer", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.buffer.prioritized_buffer.PrioritizedBuffer.clean": [[18, 22], ["super().clean"], "methods", ["home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.buffer.pseudo_prioritized_buffer.PseudoPrioritizedBuffer.clean"], ["", "def", "clean", "(", "self", ")", ":", "\n", "\t\t", "super", "(", ")", ".", "clean", "(", ")", "\n", "self", ".", "prefixsum", "=", "[", "]", "\n", "self", ".", "priorities", "=", "[", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.buffer.prioritized_buffer.PrioritizedBuffer.get_batches": [[23, 30], ["prioritized_buffer.PrioritizedBuffer.batches[].values", "list", "batch.values", "prioritized_buffer.PrioritizedBuffer.get_type"], "methods", ["home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.buffer.buffer.Buffer.get_type"], ["", "def", "get_batches", "(", "self", ",", "type_id", "=", "None", ")", ":", "\n", "\t\t", "if", "type_id", "is", "None", ":", "\n", "\t\t\t", "result", "=", "[", "]", "\n", "for", "batch", "in", "self", ".", "batches", ":", "\n", "\t\t\t\t", "result", "+=", "list", "(", "batch", ".", "values", "(", ")", ")", "\n", "", "return", "result", "\n", "", "return", "self", ".", "batches", "[", "self", ".", "get_type", "(", "type_id", ")", "]", ".", "values", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.buffer.prioritized_buffer.PrioritizedBuffer._add_type_if_not_exist": [[31, 39], ["len", "prioritized_buffer.PrioritizedBuffer.batches.append", "prioritized_buffer.PrioritizedBuffer.prefixsum.append", "prioritized_buffer.PrioritizedBuffer.priorities.append", "sortedcontainers.SortedDict"], "methods", ["home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.worker.batch.ExperienceBatch.append", "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.worker.batch.ExperienceBatch.append", "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.worker.batch.ExperienceBatch.append"], ["", "def", "_add_type_if_not_exist", "(", "self", ",", "type_id", ")", ":", "\n", "\t\t", "if", "type_id", "in", "self", ".", "types", ":", "\n", "\t\t\t", "return", "False", "\n", "", "self", ".", "types", "[", "type_id", "]", "=", "len", "(", "self", ".", "types", ")", "\n", "self", ".", "batches", ".", "append", "(", "SortedDict", "(", ")", ")", "\n", "self", ".", "prefixsum", ".", "append", "(", "[", "]", ")", "\n", "self", ".", "priorities", ".", "append", "(", "{", "}", ")", "\n", "return", "True", "\n", "\n"]], "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.buffer.prioritized_buffer.PrioritizedBuffer.get_priority_from_unique": [[40, 42], ["float", "unique.split"], "methods", ["None"], ["", "def", "get_priority_from_unique", "(", "self", ",", "unique", ")", ":", "\n", "\t\t", "return", "float", "(", "unique", ".", "split", "(", "'#'", ",", "1", ")", "[", "0", "]", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.buffer.prioritized_buffer.PrioritizedBuffer.build_unique": [[43, 45], ["None"], "methods", ["None"], ["", "def", "build_unique", "(", "self", ",", "priority", ",", "count", ")", ":", "\n", "\t\t", "return", "'{:.5f}#{}'", ".", "format", "(", "priority", ",", "count", ")", "# new batch has higher unique priority than old ones with same shared priority", "\n", "\n"]], "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.buffer.prioritized_buffer.PrioritizedBuffer.put": [[46, 63], ["prioritized_buffer.PrioritizedBuffer._add_type_if_not_exist", "prioritized_buffer.PrioritizedBuffer.get_type", "prioritized_buffer.PrioritizedBuffer.is_full", "prioritized_buffer.PrioritizedBuffer.build_unique", "prioritized_buffer.PrioritizedBuffer.batches[].update", "numpy.power", "prioritized_buffer.PrioritizedBuffer.batches[].popitem", "prioritized_buffer.PrioritizedBuffer.get_priority_from_unique", "numpy.absolute", "random.random.randint", "random.random.randint", "len"], "methods", ["home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.buffer.pseudo_prioritized_buffer.PseudoPrioritizedBuffer._add_type_if_not_exist", "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.buffer.buffer.Buffer.get_type", "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.buffer.buffer.Buffer.is_full", "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.buffer.prioritized_buffer.PrioritizedBuffer.build_unique", "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.utils.running_std.RunningMeanStd.update", "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.buffer.prioritized_buffer.PrioritizedBuffer.get_priority_from_unique"], ["", "def", "put", "(", "self", ",", "batch", ",", "priority", ",", "type_id", "=", "0", ")", ":", "# O(log)", "\n", "\t\t", "priority_sign", "=", "-", "1", "if", "priority", "<", "0", "else", "1", "\n", "priority", "=", "priority_sign", "*", "np", ".", "power", "(", "np", ".", "absolute", "(", "priority", ")", ",", "self", ".", "alpha", ")", "\n", "self", ".", "_add_type_if_not_exist", "(", "type_id", ")", "\n", "type", "=", "self", ".", "get_type", "(", "type_id", ")", "\n", "if", "self", ".", "is_full", "(", "type", ")", ":", "\n", "\t\t\t", "index", "=", "randint", "(", "len", "(", "self", ".", "batches", "[", "type", "]", ")", ")", "if", "randint", "(", "2", ")", "==", "1", "else", "0", "# argument with lowest priority is always 0 because buffer is sorted by priority", "\n", "old_unique_batch_priority", ",", "_", "=", "self", ".", "batches", "[", "type", "]", ".", "popitem", "(", "index", ")", "\n", "old_priority", "=", "self", ".", "get_priority_from_unique", "(", "old_unique_batch_priority", ")", "\n", "if", "old_priority", "in", "self", ".", "priorities", "[", "type", "]", "and", "self", ".", "priorities", "[", "type", "]", "[", "old_priority", "]", "==", "1", ":", "# remove from priority dictionary in order to prevent buffer overflow", "\n", "\t\t\t\t", "del", "self", ".", "priorities", "[", "type", "]", "[", "old_priority", "]", "\n", "", "", "priority_count", "=", "self", ".", "priorities", "[", "type", "]", "[", "priority", "]", "if", "priority", "in", "self", ".", "priorities", "[", "type", "]", "else", "0", "\n", "priority_count", "=", "(", "priority_count", "%", "self", ".", "size", ")", "+", "1", "# modular counter to avoid overflow", "\n", "self", ".", "priorities", "[", "type", "]", "[", "priority", "]", "=", "priority_count", "\n", "unique_batch_priority", "=", "self", ".", "build_unique", "(", "priority", ",", "priority_count", ")", "\n", "self", ".", "batches", "[", "type", "]", ".", "update", "(", "{", "unique_batch_priority", ":", "batch", "}", ")", "# O(log)", "\n", "self", ".", "prefixsum", "[", "type", "]", "=", "None", "# compute prefixsum only if needed, when sampling", "\n", "\n"]], "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.buffer.prioritized_buffer.PrioritizedBuffer.keyed_sample": [[64, 75], ["random.random.choice", "prioritized_buffer.PrioritizedBuffer.get_type", "numpy.searchsorted", "prioritized_buffer.PrioritizedBuffer.batches[].keys", "numpy.cumsum", "random.random.random", "len", "prioritized_buffer.PrioritizedBuffer.get_priority_from_unique", "prioritized_buffer.PrioritizedBuffer.batches[].keys"], "methods", ["home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.buffer.buffer.Buffer.get_type", "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.buffer.prioritized_buffer.PrioritizedBuffer.get_priority_from_unique"], ["", "def", "keyed_sample", "(", "self", ")", ":", "# O(n) after a new put, O(log) otherwise", "\n", "\t\t", "type_id", "=", "choice", "(", "self", ".", "type_keys", ")", "\n", "type", "=", "self", ".", "get_type", "(", "type_id", ")", "\n", "if", "self", ".", "prefixsum", "[", "type", "]", "is", "None", ":", "# compute prefixsum", "\n", "\t\t\t", "self", ".", "prefixsum", "[", "type", "]", "=", "np", ".", "cumsum", "(", "[", "self", ".", "get_priority_from_unique", "(", "k", ")", "for", "k", "in", "self", ".", "batches", "[", "type", "]", ".", "keys", "(", ")", "]", ")", "# O(n)", "\n", "", "mass", "=", "random", "(", ")", "*", "self", ".", "prefixsum", "[", "type", "]", "[", "-", "1", "]", "\n", "idx", "=", "np", ".", "searchsorted", "(", "self", ".", "prefixsum", "[", "type", "]", ",", "mass", ")", "# O(log) # Find arg of leftmost item greater than or equal to x", "\n", "keys", "=", "self", ".", "batches", "[", "type", "]", ".", "keys", "(", ")", "\n", "if", "idx", "==", "len", "(", "keys", ")", ":", "# this may happen when self.prefixsum[type] is negative", "\n", "\t\t\t", "idx", "=", "-", "1", "\n", "", "return", "self", ".", "batches", "[", "type", "]", "[", "keys", "[", "idx", "]", "]", ",", "idx", ",", "type_id", "\n", "\n"]], "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.buffer.prioritized_buffer.PrioritizedBuffer.sample": [[76, 78], ["prioritized_buffer.PrioritizedBuffer.keyed_sample"], "methods", ["home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.buffer.pseudo_prioritized_buffer.PseudoPrioritizedBuffer.keyed_sample"], ["", "def", "sample", "(", "self", ")", ":", "# O(n) after a new put, O(log) otherwise", "\n", "\t\t", "return", "self", ".", "keyed_sample", "(", ")", "[", "0", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.buffer.prioritized_buffer.PrioritizedBuffer.update_priority": [[79, 83], ["prioritized_buffer.PrioritizedBuffer.get_type", "prioritized_buffer.PrioritizedBuffer.batches[].popitem", "prioritized_buffer.PrioritizedBuffer.put"], "methods", ["home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.buffer.buffer.Buffer.get_type", "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.buffer.pseudo_prioritized_buffer.PseudoPrioritizedBuffer.put"], ["", "def", "update_priority", "(", "self", ",", "idx", ",", "priority", ",", "type_id", "=", "0", ")", ":", "# O(log)", "\n", "\t\t", "type", "=", "self", ".", "get_type", "(", "type_id", ")", "\n", "_", ",", "batch", "=", "self", ".", "batches", "[", "type", "]", ".", "popitem", "(", "index", "=", "idx", ")", "# argument with lowest priority is always 0 because buffer is sorted by priority", "\n", "self", ".", "put", "(", "batch", ",", "priority", ",", "type_id", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.buffer.pseudo_prioritized_buffer.PseudoPrioritizedBuffer.__init__": [[10, 18], ["utils.buffer.buffer.Buffer.__init__"], "methods", ["home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.worker.batch.CompositeBatch.__init__"], ["def", "__init__", "(", "self", ",", "size", ",", "alpha", "=", "1", ",", "prioritized_drop_probability", "=", "0.5", ")", ":", "# O(1)", "\n", "\t\t", "self", ".", "_epsilon", "=", "1e-6", "\n", "self", ".", "_alpha", "=", "alpha", "# how much prioritization is used (0 - no prioritization, 1 - full prioritization)", "\n", "self", ".", "_it_capacity", "=", "1", "\n", "self", ".", "_prioritized_drop_probability", "=", "prioritized_drop_probability", "# remove the worst batch with this probability otherwise remove the oldest one", "\n", "while", "self", ".", "_it_capacity", "<", "size", ":", "\n", "\t\t\t", "self", ".", "_it_capacity", "*=", "2", "\n", "", "super", "(", ")", ".", "__init__", "(", "size", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.buffer.pseudo_prioritized_buffer.PseudoPrioritizedBuffer.set": [[19, 22], ["isinstance", "super().set"], "methods", ["home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.buffer.pseudo_prioritized_buffer.PseudoPrioritizedBuffer.set"], ["", "def", "set", "(", "self", ",", "buffer", ")", ":", "# O(1)", "\n", "\t\t", "assert", "isinstance", "(", "buffer", ",", "PseudoPrioritizedBuffer", ")", "\n", "super", "(", ")", ".", "set", "(", "buffer", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.buffer.pseudo_prioritized_buffer.PseudoPrioritizedBuffer.clean": [[23, 28], ["super().clean"], "methods", ["home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.buffer.pseudo_prioritized_buffer.PseudoPrioritizedBuffer.clean"], ["", "def", "clean", "(", "self", ")", ":", "# O(1)", "\n", "\t\t", "super", "(", ")", ".", "clean", "(", ")", "\n", "self", ".", "_batches_next_idx", "=", "[", "]", "\n", "self", ".", "_it_sum", "=", "[", "]", "\n", "self", ".", "_it_min", "=", "[", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.buffer.pseudo_prioritized_buffer.PseudoPrioritizedBuffer._add_type_if_not_exist": [[29, 40], ["len", "pseudo_prioritized_buffer.PseudoPrioritizedBuffer.type_values.append", "pseudo_prioritized_buffer.PseudoPrioritizedBuffer.type_keys.append", "pseudo_prioritized_buffer.PseudoPrioritizedBuffer.batches.append", "pseudo_prioritized_buffer.PseudoPrioritizedBuffer._batches_next_idx.append", "pseudo_prioritized_buffer.PseudoPrioritizedBuffer._it_sum.append", "pseudo_prioritized_buffer.PseudoPrioritizedBuffer._it_min.append", "utils.segment_tree.SumSegmentTree", "utils.segment_tree.MinSegmentTree", "float"], "methods", ["home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.worker.batch.ExperienceBatch.append", "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.worker.batch.ExperienceBatch.append", "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.worker.batch.ExperienceBatch.append", "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.worker.batch.ExperienceBatch.append", "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.worker.batch.ExperienceBatch.append", "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.worker.batch.ExperienceBatch.append"], ["", "def", "_add_type_if_not_exist", "(", "self", ",", "type_id", ")", ":", "# O(1)", "\n", "\t\t", "if", "type_id", "in", "self", ".", "types", ":", "# check it to avoid double insertion", "\n", "\t\t\t", "return", "False", "\n", "", "self", ".", "types", "[", "type_id", "]", "=", "len", "(", "self", ".", "types", ")", "\n", "self", ".", "type_values", ".", "append", "(", "self", ".", "types", "[", "type_id", "]", ")", "\n", "self", ".", "type_keys", ".", "append", "(", "type_id", ")", "\n", "self", ".", "batches", ".", "append", "(", "[", "]", ")", "\n", "self", ".", "_batches_next_idx", ".", "append", "(", "0", ")", "\n", "self", ".", "_it_sum", ".", "append", "(", "SumSegmentTree", "(", "self", ".", "_it_capacity", ")", ")", "\n", "self", ".", "_it_min", ".", "append", "(", "MinSegmentTree", "(", "self", ".", "_it_capacity", ",", "neutral_element", "=", "(", "float", "(", "'inf'", ")", ",", "-", "1", ")", ")", ")", "\n", "return", "True", "\n", "\n"]], "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.buffer.pseudo_prioritized_buffer.PseudoPrioritizedBuffer.normalize_priority": [[41, 45], ["numpy.absolute", "numpy.power"], "methods", ["None"], ["", "def", "normalize_priority", "(", "self", ",", "priority", ")", ":", "# O(1)", "\n", "\t\t", "priority_sign", "=", "-", "1", "if", "priority", "<", "0", "else", "1", "\n", "priority", "=", "np", ".", "absolute", "(", "priority", ")", "+", "self", ".", "_epsilon", "\n", "return", "priority_sign", "*", "np", ".", "power", "(", "priority", ",", "self", ".", "_alpha", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.buffer.pseudo_prioritized_buffer.PseudoPrioritizedBuffer.put": [[46, 65], ["pseudo_prioritized_buffer.PseudoPrioritizedBuffer._add_type_if_not_exist", "pseudo_prioritized_buffer.PseudoPrioritizedBuffer.get_type", "pseudo_prioritized_buffer.PseudoPrioritizedBuffer.is_full", "pseudo_prioritized_buffer.PseudoPrioritizedBuffer.update_priority", "pseudo_prioritized_buffer.PseudoPrioritizedBuffer.batches[].append", "random.random.random", "pseudo_prioritized_buffer.PseudoPrioritizedBuffer._it_min[].min"], "methods", ["home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.buffer.pseudo_prioritized_buffer.PseudoPrioritizedBuffer._add_type_if_not_exist", "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.buffer.buffer.Buffer.get_type", "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.buffer.buffer.Buffer.is_full", "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.buffer.pseudo_prioritized_buffer.PseudoPrioritizedBuffer.update_priority", "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.worker.batch.ExperienceBatch.append", "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.utils.segment_tree.MinSegmentTree.min"], ["", "def", "put", "(", "self", ",", "batch", ",", "priority", ",", "type_id", "=", "0", ")", ":", "# O(log)", "\n", "\t\t", "self", ".", "_add_type_if_not_exist", "(", "type_id", ")", "\n", "type", "=", "self", ".", "get_type", "(", "type_id", ")", "\n", "removing_worst_batch", "=", "False", "\n", "if", "self", ".", "is_full", "(", "type", ")", ":", "\n", "\t\t\t", "if", "random", "(", ")", "<", "self", ".", "_prioritized_drop_probability", ":", "# Remove the batch with lowest priority", "\n", "\t\t\t\t", "_", ",", "idx", "=", "self", ".", "_it_min", "[", "type", "]", ".", "min", "(", ")", "# O(1)", "\n", "removing_worst_batch", "=", "True", "\n", "", "else", ":", "# Remove oldest batch", "\n", "\t\t\t\t", "idx", "=", "self", ".", "_batches_next_idx", "[", "type", "]", "\n", "", "self", ".", "batches", "[", "type", "]", "[", "idx", "]", "=", "batch", "\n", "", "else", ":", "\n", "\t\t\t", "idx", "=", "self", ".", "_batches_next_idx", "[", "type", "]", "\n", "self", ".", "batches", "[", "type", "]", ".", "append", "(", "batch", ")", "\n", "# Update next id", "\n", "", "if", "not", "removing_worst_batch", ":", "\n", "\t\t\t", "self", ".", "_batches_next_idx", "[", "type", "]", "=", "(", "self", ".", "_batches_next_idx", "[", "type", "]", "+", "1", ")", "%", "self", ".", "size", "\n", "# Update priority", "\n", "", "self", ".", "update_priority", "(", "idx", ",", "priority", ",", "type_id", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.buffer.pseudo_prioritized_buffer.PseudoPrioritizedBuffer.keyed_sample": [[66, 74], ["random.random.choice", "pseudo_prioritized_buffer.PseudoPrioritizedBuffer.get_type", "pseudo_prioritized_buffer.PseudoPrioritizedBuffer._it_sum[].find_prefixsum_idx", "random.random.random", "pseudo_prioritized_buffer.PseudoPrioritizedBuffer._it_sum[].sum"], "methods", ["home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.buffer.buffer.Buffer.get_type", "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.utils.segment_tree.SumSegmentTree.find_prefixsum_idx", "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.utils.segment_tree.SumSegmentTree.sum"], ["", "def", "keyed_sample", "(", "self", ")", ":", "# O(log)", "\n", "\t\t", "type_id", "=", "choice", "(", "self", ".", "type_keys", ")", "\n", "type", "=", "self", ".", "get_type", "(", "type_id", ")", "\n", "mass", "=", "random", "(", ")", "*", "self", ".", "_it_sum", "[", "type", "]", ".", "sum", "(", ")", "# O(1)", "\n", "idx", "=", "self", ".", "_it_sum", "[", "type", "]", ".", "find_prefixsum_idx", "(", "mass", ")", "# O(log)", "\n", "# weight = (self._it_sum[idx]/self._it_min.min()) ** (-beta) # importance weight", "\n", "# return self.batches[0][idx], idx, weight # multiply weight for advantage", "\n", "return", "self", ".", "batches", "[", "type", "]", "[", "idx", "]", ",", "idx", ",", "type_id", "\n", "\n"]], "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.buffer.pseudo_prioritized_buffer.PseudoPrioritizedBuffer.sample": [[75, 77], ["pseudo_prioritized_buffer.PseudoPrioritizedBuffer.keyed_sample"], "methods", ["home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.buffer.pseudo_prioritized_buffer.PseudoPrioritizedBuffer.keyed_sample"], ["", "def", "sample", "(", "self", ")", ":", "# O(log)", "\n", "\t\t", "return", "self", ".", "keyed_sample", "(", ")", "[", "0", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.buffer.pseudo_prioritized_buffer.PseudoPrioritizedBuffer.update_priority": [[78, 85], ["pseudo_prioritized_buffer.PseudoPrioritizedBuffer.get_type", "pseudo_prioritized_buffer.PseudoPrioritizedBuffer.normalize_priority"], "methods", ["home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.buffer.buffer.Buffer.get_type", "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.buffer.pseudo_prioritized_buffer.PseudoPrioritizedBuffer.normalize_priority"], ["", "def", "update_priority", "(", "self", ",", "idx", ",", "priority", ",", "type_id", "=", "0", ")", ":", "# O(log)", "\n", "\t\t", "type", "=", "self", ".", "get_type", "(", "type_id", ")", "\n", "normalized_priority", "=", "self", ".", "normalize_priority", "(", "priority", ")", "\n", "# Update min", "\n", "self", ".", "_it_min", "[", "type", "]", "[", "idx", "]", "=", "(", "normalized_priority", ",", "idx", ")", "# O(log)", "\n", "# Update priority", "\n", "self", ".", "_it_sum", "[", "type", "]", "[", "idx", "]", "=", "normalized_priority", "# O(log)", "\n", "", "", ""]], "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.rnn.rnn.RNN.__init__": [[10, 39], ["eval", "eval", "utils.tensorflow_utils.gpu_count"], "methods", ["home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.utils.tensorflow_utils.gpu_count"], ["\t", "def", "__init__", "(", "self", ",", "type", ",", "units", ",", "batch_size", ",", "direction", "=", "1", ",", "dtype", "=", "'float32'", ",", "stack_size", "=", "1", ",", "dropout", "=", "0.", ",", "training", "=", "True", ")", ":", "\n", "\t\t", "self", ".", "training", "=", "training", "\n", "self", ".", "stack_size", "=", "stack_size", "\n", "self", ".", "tf_dtype", "=", "eval", "(", "'tf.{}'", ".", "format", "(", "dtype", ")", ")", "\n", "self", ".", "np_dtype", "=", "eval", "(", "'np.{}'", ".", "format", "(", "dtype", ")", ")", "\n", "self", ".", "batch_size", "=", "batch_size", "\n", "self", ".", "units", "=", "units", "\n", "self", ".", "type", "=", "type", "\n", "self", ".", "use_gpu", "=", "gpu_count", "(", ")", ">", "0", "\n", "# When you calculate the accuracy and validation, you need to manually set the keep_probability to 1 (or the dropout to 0) so that you don't actually drop any of your weight values when you are evaluating your network. If you don't do this, you'll essentially miscalculate the value you've trained your network to predict thus far. This could certainly negatively affect your acc/val scores. Especially with a 50% dropout_probability rate.", "\n", "self", ".", "dropout_probability", "=", "dropout", "if", "not", "self", ".", "training", "else", "0.", "\n", "self", ".", "direction", "=", "direction", "\n", "self", ".", "rnn_layer", "=", "None", "\n", "if", "type", "==", "'LSTM'", ":", "\n", "\t\t\t", "self", ".", "cell", "=", "CudnnLSTM", "if", "self", ".", "use_gpu", "else", "LSTMBlockCell", "\n", "", "elif", "type", "==", "'GRU'", ":", "\n", "\t\t\t", "self", ".", "cell", "=", "CudnnGRU", "if", "self", ".", "use_gpu", "else", "GRUBlockCellV2", "\n", "# State shape", "\n", "", "if", "self", ".", "use_gpu", ":", "\n", "# initial_state: a tuple of tensor(s) of shape [num_layers * num_dirs, batch_size, num_units]", "\n", "\t\t\t", "if", "self", ".", "type", "==", "'LSTM'", ":", "\n", "\t\t\t\t", "self", ".", "state_shape", "=", "[", "2", ",", "self", ".", "stack_size", "*", "self", ".", "direction", ",", "self", ".", "batch_size", ",", "self", ".", "units", "]", "\n", "", "elif", "type", "==", "'GRU'", ":", "\n", "\t\t\t\t", "self", ".", "state_shape", "=", "[", "1", ",", "self", ".", "stack_size", "*", "self", ".", "direction", ",", "self", ".", "batch_size", ",", "self", ".", "units", "]", "\n", "", "", "else", ":", "\n", "\t\t\t", "if", "self", ".", "type", "==", "'LSTM'", ":", "\n", "\t\t\t\t", "self", ".", "state_shape", "=", "[", "self", ".", "direction", ",", "self", ".", "stack_size", ",", "2", ",", "self", ".", "batch_size", ",", "self", ".", "units", "]", "\n", "", "elif", "type", "==", "'GRU'", ":", "\n", "\t\t\t\t", "self", ".", "state_shape", "=", "[", "self", ".", "direction", ",", "self", ".", "stack_size", ",", "self", ".", "batch_size", ",", "self", ".", "units", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.rnn.rnn.RNN.default_state": [[40, 42], ["numpy.zeros"], "methods", ["None"], ["", "", "", "def", "default_state", "(", "self", ")", ":", "\n", "\t\t", "return", "np", ".", "zeros", "(", "self", ".", "state_shape", ",", "self", ".", "np_dtype", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.rnn.rnn.RNN.state_placeholder": [[43, 45], ["tensorflow.placeholder"], "methods", ["None"], ["", "def", "state_placeholder", "(", "self", ",", "name", "=", "''", ")", ":", "\n", "\t\t", "return", "tf", ".", "placeholder", "(", "shape", "=", "[", "None", "]", "+", "self", ".", "state_shape", ",", "name", "=", "'{}_{}'", ".", "format", "(", "name", ",", "self", ".", "type", ")", ",", "dtype", "=", "self", ".", "tf_dtype", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.rnn.rnn.RNN._process_single_batch": [[46, 66], ["rnn.RNN.rnn_layer", "tuple", "tensorflow.unstack", "range", "range", "state_list.append", "state_list.append", "tensorflow.unstack", "tensorflow.nn.rnn_cell.LSTMStateTuple", "range"], "methods", ["home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.worker.batch.ExperienceBatch.append", "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.worker.batch.ExperienceBatch.append"], ["", "def", "_process_single_batch", "(", "self", ",", "input", ",", "initial_state", ")", ":", "\n", "# Build initial state", "\n", "\t\t", "if", "self", ".", "use_gpu", ":", "\n", "# initial_state: a tuple of tensor(s) of shape [num_layers * num_dirs, batch_size, num_units]", "\n", "\t\t\t", "initial_state", "=", "tuple", "(", "tf", ".", "unstack", "(", "initial_state", ")", ")", "\n", "", "else", ":", "\n", "\t\t\t", "state_list", "=", "[", "]", "\n", "if", "self", ".", "type", "==", "'LSTM'", ":", "\n", "\t\t\t\t", "for", "d", "in", "range", "(", "self", ".", "direction", ")", ":", "\n", "\t\t\t\t\t", "state", "=", "initial_state", "[", "d", "]", "\n", "state_list", ".", "append", "(", "[", "\n", "tf", ".", "nn", ".", "rnn_cell", ".", "LSTMStateTuple", "(", "state", "[", "i", "]", "[", "0", "]", ",", "state", "[", "i", "]", "[", "1", "]", ")", "\n", "for", "i", "in", "range", "(", "self", ".", "stack_size", ")", "\n", "]", ")", "\n", "", "", "else", ":", "\n", "\t\t\t\t", "for", "d", "in", "range", "(", "self", ".", "direction", ")", ":", "\n", "\t\t\t\t\t", "state_list", ".", "append", "(", "tf", ".", "unstack", "(", "initial_state", "[", "d", "]", ")", ")", "\n", "", "", "initial_state", "=", "state_list", "\n", "", "output", ",", "final_state", "=", "self", ".", "rnn_layer", "(", "inputs", "=", "input", ",", "initial_state", "=", "initial_state", ")", "\n", "return", "output", ",", "final_state", "\n", "\n"]], "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.rnn.rnn.RNN.process_batches": [[67, 114], ["tensorflow.reshape", "tensorflow.concat", "tensorflow.zeros", "tensorflow.zeros", "tensorflow.while_loop", "tensorflow.layers.flatten", "len", "tensorflow.layers.flatten", "tensorflow.layers.flatten.get_shape().as_list", "rnn.RNN._build", "tensorflow.shape", "rnn.RNN._process_single_batch", "tensorflow.concat", "tensorflow.concat", "tensorflow.layers.flatten.get_shape", "int", "tensorflow.cumsum", "tensorflow.layers.flatten.get_shape", "tensorflow.TensorShape", "tensorflow.TensorShape", "tensorflow.TensorShape"], "methods", ["home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.utils.misc.flatten", "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.utils.misc.flatten", "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.rnn.rnn.RNN._build", "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.rnn.rnn.RNN._process_single_batch"], ["", "def", "process_batches", "(", "self", ",", "input", ",", "initial_states", ",", "sizes", ")", ":", "\n", "# Add batch dimension, sequence length here is input batch size, while sequence batch size is batch_size", "\n", "\t\t", "if", "len", "(", "input", ".", "get_shape", "(", ")", ")", ">", "2", ":", "\n", "\t\t\t", "input", "=", "tf", ".", "layers", ".", "flatten", "(", "input", ")", "\n", "", "input_depth", "=", "input", ".", "get_shape", "(", ")", ".", "as_list", "(", ")", "[", "-", "1", "]", "\n", "input", "=", "tf", ".", "reshape", "(", "input", ",", "[", "-", "1", ",", "self", ".", "batch_size", ",", "int", "(", "input_depth", "/", "self", ".", "batch_size", ")", "]", ")", "\n", "# Build RNN layer", "\n", "if", "self", ".", "rnn_layer", "is", "None", ":", "\n", "\t\t\t", "self", ".", "rnn_layer", "=", "self", ".", "_build", "(", "input", ")", "\n", "# Get loop constants", "\n", "", "batch_limits", "=", "tf", ".", "concat", "(", "[", "[", "0", "]", ",", "tf", ".", "cumsum", "(", "sizes", ")", "]", ",", "0", ")", "\n", "batch_count", "=", "tf", ".", "shape", "(", "sizes", ")", "[", "0", "]", "\n", "output_shape", "=", "[", "self", ".", "batch_size", ",", "self", ".", "direction", "*", "self", ".", "units", "]", "\n", "# Build condition", "\n", "condition", "=", "lambda", "i", ",", "output", ",", "final_states", ":", "i", "<", "batch_count", "\n", "# Build body", "\n", "def", "body", "(", "i", ",", "output", ",", "final_states", ")", ":", "\n", "\t\t\t", "start", "=", "batch_limits", "[", "i", "]", "\n", "end", "=", "batch_limits", "[", "i", "+", "1", "]", "\n", "ith_output", ",", "ith_final_state", "=", "self", ".", "_process_single_batch", "(", "input", "=", "input", "[", "start", ":", "end", "]", ",", "initial_state", "=", "initial_states", "[", "i", "]", ")", "\n", "output", "=", "tf", ".", "concat", "(", "(", "output", ",", "ith_output", ")", ",", "0", ")", "\n", "final_states", "=", "tf", ".", "concat", "(", "(", "final_states", ",", "[", "ith_final_state", "]", ")", ",", "0", ")", "\n", "return", "i", "+", "1", ",", "output", ",", "final_states", "\n", "# Build input", "\n", "", "i", "=", "0", "\n", "output", "=", "tf", ".", "zeros", "(", "[", "1", "]", "+", "output_shape", ",", "self", ".", "tf_dtype", ")", "\n", "final_states", "=", "tf", ".", "zeros", "(", "[", "1", "]", "+", "self", ".", "state_shape", ",", "self", ".", "tf_dtype", ")", "\n", "# Loop", "\n", "_", ",", "output", ",", "final_states", "=", "tf", ".", "while_loop", "(", "\n", "cond", "=", "condition", ",", "# A callable that represents the termination condition of the loop.", "\n", "body", "=", "body", ",", "# A callable that represents the loop body.", "\n", "loop_vars", "=", "[", "\n", "i", ",", "# 1st variable # batch index", "\n", "output", ",", "# 2nd variable # RNN outputs", "\n", "final_states", "# 3rd variable # RNN final states", "\n", "]", ",", "\n", "shape_invariants", "=", "[", "# The shape invariants for the loop variables.", "\n", "tf", ".", "TensorShape", "(", "(", ")", ")", ",", "# 1st variable", "\n", "tf", ".", "TensorShape", "(", "[", "None", "]", "+", "output_shape", ")", ",", "# 2nd variable", "\n", "tf", ".", "TensorShape", "(", "[", "None", "]", "+", "self", ".", "state_shape", ")", "# 3rd variable", "\n", "]", ",", "\n", "swap_memory", "=", "True", ",", "# Whether GPU-CPU memory swap is enabled for this loop", "\n", "return_same_structure", "=", "True", "# If True, output has same structure as loop_vars.", "\n", ")", "\n", "output", "=", "tf", ".", "layers", ".", "flatten", "(", "output", ")", "\n", "# Return result", "\n", "return", "output", "[", "1", ":", "]", ",", "final_states", "[", "1", ":", "]", "# Remove first element we used to allow concatenations inside loop body\t", "\n", "\n"]], "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.rnn.rnn.RNN._build": [[115, 175], ["rnn.RNN.RNN.cell", "rnn.RNN.RNN.build", "range", "inputs.get_shape", "rnn.RNN.RNN.call", "cell_list.append", "enumerate", "tensorflow.reshape", "tensorflow.contrib.rnn.stack_bidirectional_dynamic_rnn", "tensorflow.reshape", "rnn.RNN.RNN.cell", "zip", "tensorflow.nn.dynamic_rnn", "tensorflow.reshape.append", "range", "dropout"], "methods", ["home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.intrinsic_reward.intrinsic_reward_network.IntrinsicReward_Network.build", "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.worker.batch.ExperienceBatch.append", "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.worker.batch.ExperienceBatch.append"], ["", "def", "_build", "(", "self", ",", "inputs", ")", ":", "\n", "\t\t", "if", "self", ".", "use_gpu", ":", "\n", "\t\t\t", "rnn", "=", "self", ".", "cell", "(", "\n", "num_layers", "=", "self", ".", "stack_size", ",", "\n", "num_units", "=", "self", ".", "units", ",", "\n", "direction", "=", "'unidirectional'", "if", "self", ".", "direction", "==", "1", "else", "'bidirectional'", ",", "\n", "dropout", "=", "self", ".", "dropout_probability", ",", "# set to 0. for no dropout ", "\n", "dtype", "=", "self", ".", "tf_dtype", "\n", ")", "\n", "rnn", ".", "build", "(", "inputs", ".", "get_shape", "(", ")", ")", "# Build now", "\n", "def", "rnn_layer", "(", "inputs", ",", "initial_state", ")", ":", "\n", "# Do not build here, just call", "\n", "\t\t\t\t", "return", "rnn", ".", "call", "(", "inputs", "=", "inputs", ",", "initial_state", "=", "initial_state", ",", "training", "=", "self", ".", "training", ")", "\n", "", "return", "rnn_layer", "\n", "", "else", ":", "\n", "# Build RNN cells", "\n", "\t\t\t", "cell_list", "=", "[", "]", "\n", "for", "d", "in", "range", "(", "self", ".", "direction", ")", ":", "\n", "\t\t\t\t", "cell_list", ".", "append", "(", "[", "self", ".", "cell", "(", "num_units", "=", "self", ".", "units", ",", "name", "=", "'{}_cell{}_{}'", ".", "format", "(", "self", ".", "type", ",", "d", ",", "i", ")", ")", "for", "i", "in", "range", "(", "self", ".", "stack_size", ")", "]", ")", "\n", "# Apply dropout_probability", "\n", "", "if", "self", ".", "dropout_probability", ">", "0.", ":", "\n", "\t\t\t\t", "dropout", "=", "tf", ".", "nn", ".", "rnn_cell", ".", "DropoutWrapper", "\n", "for", "rnn_cells", "in", "cell_list", ":", "\n", "\t\t\t\t\t", "rnn_cells", "=", "[", "\n", "dropout", "(", "cell", "=", "cell", ",", "output_keep_prob", "=", "1", "-", "self", ".", "dropout_probability", ")", "\n", "for", "cell", "in", "rnn_cells", "[", ":", "-", "1", "]", "\n", "]", "+", "rnn_cells", "[", "-", "1", ":", "]", "# Do not apply dropout_probability to last layer, as in GPU counterpart implementation", "\n", "# Build stacked dynamic RNN <https://stackoverflow.com/questions/49242266/difference-between-multirnncell-and-stack-bidirectional-dynamic-rnn-in-tensorflo>", "\n", "# sequence_length = [tf.shape(inputs)[0]]", "\n", "", "", "if", "self", ".", "direction", "==", "1", ":", "# Unidirectional", "\n", "\t\t\t\t", "def", "unidirectional_rnn_layer", "(", "inputs", ",", "initial_state", ")", ":", "\n", "\t\t\t\t\t", "output", "=", "inputs", "\n", "final_state", "=", "[", "]", "\n", "for", "i", ",", "(", "cell", ",", "state", ")", "in", "enumerate", "(", "zip", "(", "cell_list", "[", "0", "]", ",", "initial_state", "[", "0", "]", ")", ")", ":", "\n", "\t\t\t\t\t\t", "output", ",", "output_state", "=", "tf", ".", "nn", ".", "dynamic_rnn", "(", "\n", "cell", "=", "cell", ",", "\n", "inputs", "=", "output", ",", "\n", "initial_state", "=", "state", ",", "\n", "# sequence_length = sequence_length,", "\n", "time_major", "=", "True", "# The shape format of the inputs and outputs Tensors. If true, these Tensors must be shaped [max_time, batch_size, depth]. If false, these Tensors must be shaped [batch_size, max_time, depth]. Using time_major = True is a bit more efficient because it avoids transposes at the beginning and end of the RNN calculation. However, most TensorFlow data is batch-major, so by default this function accepts input and emits output in batch-major form.", "\n", ")", "\n", "final_state", ".", "append", "(", "output_state", ")", "\n", "", "final_state", "=", "tf", ".", "reshape", "(", "[", "final_state", "]", ",", "self", ".", "state_shape", ")", "\n", "return", "output", ",", "final_state", "\n", "", "return", "unidirectional_rnn_layer", "\n", "", "else", ":", "# Bidirectional", "\n", "\t\t\t\t", "def", "bidirectional_rnn_layer", "(", "inputs", ",", "initial_state", ")", ":", "\n", "\t\t\t\t\t", "output", ",", "output_state_fw", ",", "output_state_bw", "=", "tf", ".", "contrib", ".", "rnn", ".", "stack_bidirectional_dynamic_rnn", "(", "\n", "cells_fw", "=", "cell_list", "[", "0", "]", ",", "# List of instances of RNNCell, one per layer, to be used for forward direction.", "\n", "cells_bw", "=", "cell_list", "[", "1", "]", ",", "# List of instances of RNNCell, one per layer, to be used for backward direction.", "\n", "inputs", "=", "inputs", ",", "# The RNN inputs. this must be a tensor of shape: [batch_size, max_time, ...], or a nested tuple of such elements.", "\n", "initial_states_fw", "=", "initial_state", "[", "0", "]", ",", "# (optional) A list of the initial states (one per layer) for the forward RNN. Each tensor must has an appropriate type and shape [batch_size, cell_fw.state_size].", "\n", "initial_states_bw", "=", "initial_state", "[", "1", "]", ",", "# (optional) Same as for initial_states_fw, but using the corresponding properties of cells_bw.", "\n", "# sequence_length = sequence_length, # (optional) An int32/int64 vector, size [batch_size], containing the actual lengths for each of the sequences.", "\n", "time_major", "=", "True", "# The shape format of the inputs and outputs Tensors. If true, these Tensors must be shaped [max_time, batch_size, depth]. If false, these Tensors must be shaped [batch_size, max_time, depth]. Using time_major = True is a bit more efficient because it avoids transposes at the beginning and end of the RNN calculation. However, most TensorFlow data is batch-major, so by default this function accepts input and emits output in batch-major form.", "\n", ")", "\n", "final_state", "=", "[", "output_state_fw", ",", "output_state_bw", "]", "\n", "final_state", "=", "tf", ".", "reshape", "(", "final_state", ",", "self", ".", "state_shape", ")", "\n", "return", "output", ",", "final_state", "\n", "", "return", "bidirectional_rnn_layer", "\n", "", "", "", "", ""]], "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.trnn.trnn.EinsumTensorRNNCell.__init__": [[15, 20], ["tensorflow.contrib.rnn.RNNCell.__init__"], "methods", ["home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.worker.batch.CompositeBatch.__init__"], ["def", "__init__", "(", "self", ",", "num_units", ",", "rank_vals", ",", "activation", "=", "tanh", ",", "**", "kwargs", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "**", "kwargs", ")", "\n", "self", ".", "_num_units", "=", "num_units", "\n", "self", ".", "_rank_vals", "=", "rank_vals", "\n", "self", ".", "_activation", "=", "activation", "\n", "\n"]], "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.trnn.trnn.EinsumTensorRNNCell.state_size": [[21, 24], ["None"], "methods", ["None"], ["", "@", "property", "\n", "def", "state_size", "(", "self", ")", ":", "\n", "            ", "return", "self", ".", "_num_units", "\n", "\n"]], "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.trnn.trnn.EinsumTensorRNNCell.output_size": [[25, 28], ["None"], "methods", ["None"], ["", "@", "property", "\n", "def", "output_size", "(", "self", ")", ":", "\n", "            ", "return", "self", ".", "_num_units", "\n", "\n"]], "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.trnn.trnn.EinsumTensorRNNCell.__call__": [[29, 33], ["trnn.tensor_network_tt_einsum", "trnn.EinsumTensorRNNCell._activation"], "methods", ["home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.trnn.trnn.tensor_network_tt_einsum"], ["", "def", "__call__", "(", "self", ",", "inputs", ",", "states", ")", ":", "\n", "            ", "output", "=", "tensor_network_tt_einsum", "(", "inputs", ",", "states", ",", "self", ".", "_num_units", ",", "self", ".", "_rank_vals", ",", "True", ")", "\n", "new_state", "=", "self", ".", "_activation", "(", "output", ")", "\n", "return", "new_state", ",", "new_state", "\n", "\n"]], "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.trnn.trnn.TensorLSTMCell.__init__": [[36, 43], ["tensorflow.contrib.rnn.RNNCell.__init__"], "methods", ["home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.worker.batch.CompositeBatch.__init__"], ["def", "__init__", "(", "self", ",", "num_units", ",", "rank_vals", ",", "forget_bias", "=", "1.0", ",", "state_is_tuple", "=", "True", ",", "activation", "=", "tanh", ",", "**", "kwargs", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "**", "kwargs", ")", "\n", "self", ".", "_num_units", "=", "num_units", "\n", "self", ".", "_rank_vals", "=", "rank_vals", "\n", "self", ".", "_forget_bias", "=", "forget_bias", "\n", "self", ".", "_state_is_tuple", "=", "state_is_tuple", "\n", "self", ".", "_activation", "=", "activation", "\n", "\n"]], "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.trnn.trnn.TensorLSTMCell.state_size": [[44, 48], ["tensorflow.python.ops.rnn_cell_impl.LSTMStateTuple"], "methods", ["None"], ["", "@", "property", "\n", "def", "state_size", "(", "self", ")", ":", "\n", "        ", "return", "(", "LSTMStateTuple", "(", "self", ".", "_num_units", ",", "self", ".", "_num_units", ")", "\n", "if", "self", ".", "_state_is_tuple", "else", "2", "*", "self", ".", "_num_units", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.trnn.trnn.TensorLSTMCell.output_size": [[49, 52], ["None"], "methods", ["None"], ["", "@", "property", "\n", "def", "output_size", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "_num_units", "\n", "\n"]], "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.trnn.trnn.TensorLSTMCell.__call__": [[53, 83], ["trnn.tensor_network_tt_einsum", "tensorflow.python.ops.array_ops.split", "trnn.TensorLSTMCell._activation", "sigmoid", "tensorflow.python.ops.rnn_cell_impl.LSTMStateTuple", "tensorflow.python.ops.array_ops.concat", "tensorflow.python.ops.array_ops.split", "sigmoid", "sigmoid", "trnn.TensorLSTMCell._activation"], "methods", ["home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.trnn.trnn.tensor_network_tt_einsum"], ["", "def", "__call__", "(", "self", ",", "inputs", ",", "states", ")", ":", "\n", "        ", "\"\"\"Now we have multiple states, state->states\"\"\"", "\n", "sigmoid", "=", "tf", ".", "sigmoid", "\n", "# Parameters of gates are concatenated into one multiply for efficiency.", "\n", "if", "self", ".", "_state_is_tuple", ":", "\n", "            ", "hs", "=", "(", ")", "\n", "for", "state", "in", "states", ":", "\n", "# every state is a tuple of (c,h)", "\n", "                ", "c", ",", "h", "=", "state", "\n", "hs", "+=", "(", "h", ",", ")", "\n", "", "", "else", ":", "\n", "            ", "hs", "=", "(", ")", "\n", "for", "state", "in", "states", ":", "\n", "                ", "c", ",", "h", "=", "array_ops", ".", "split", "(", "value", "=", "state", ",", "num_or_size_splits", "=", "2", ",", "axis", "=", "1", ")", "\n", "hs", "+=", "(", "h", ",", ")", "\n", "\n", "", "", "output_size", "=", "4", "*", "self", ".", "_num_units", "\n", "concat", "=", "tensor_network_tt_einsum", "(", "inputs", ",", "hs", ",", "output_size", ",", "self", ".", "_rank_vals", ",", "True", ")", "\n", "# i = input_gate, j = new_input, f = forget_gate, o = output_gate", "\n", "i", ",", "j", ",", "f", ",", "o", "=", "array_ops", ".", "split", "(", "value", "=", "concat", ",", "num_or_size_splits", "=", "4", ",", "axis", "=", "1", ")", "\n", "\n", "new_c", "=", "(", "\n", "c", "*", "sigmoid", "(", "f", "+", "self", ".", "_forget_bias", ")", "+", "sigmoid", "(", "i", ")", "*", "self", ".", "_activation", "(", "j", ")", ")", "\n", "new_h", "=", "self", ".", "_activation", "(", "new_c", ")", "*", "sigmoid", "(", "o", ")", "\n", "\n", "if", "self", ".", "_state_is_tuple", ":", "\n", "            ", "new_state", "=", "LSTMStateTuple", "(", "new_c", ",", "new_h", ")", "\n", "", "else", ":", "\n", "            ", "new_state", "=", "array_ops", ".", "concat", "(", "[", "new_c", ",", "new_h", "]", ",", "1", ")", "\n", "", "return", "new_h", ",", "new_state", "\n", "\n"]], "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.trnn.trnn.MTRNNCell.__init__": [[86, 92], ["tensorflow.contrib.rnn.RNNCell.__init__"], "methods", ["home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.worker.batch.CompositeBatch.__init__"], ["def", "__init__", "(", "self", ",", "num_units", ",", "num_freq", ",", "rank_vals", ",", "activation", "=", "tanh", ",", "**", "kwargs", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "**", "kwargs", ")", "\n", "self", ".", "_num_units", "=", "num_units", "\n", "self", ".", "_num_freq", "=", "num_freq", "# frequency for the 2nd tt state", "\n", "self", ".", "_rank_vals", "=", "rank_vals", "\n", "self", ".", "_activation", "=", "activation", "\n", "\n"]], "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.trnn.trnn.MTRNNCell.state_size": [[93, 96], ["None"], "methods", ["None"], ["", "@", "property", "\n", "def", "state_size", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "_num_units", "\n", "\n"]], "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.trnn.trnn.MTRNNCell.output_size": [[97, 100], ["None"], "methods", ["None"], ["", "@", "property", "\n", "def", "output_size", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "_num_units", "\n", "\n"]], "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.trnn.trnn.MTRNNCell.__call__": [[101, 105], ["trnn.tensor_network_mtrnn", "trnn.MTRNNCell._activation"], "methods", ["home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.trnn.trnn.tensor_network_mtrnn"], ["", "def", "__call__", "(", "self", ",", "inputs", ",", "states", ",", "scope", "=", "None", ")", ":", "\n", "        ", "output", "=", "tensor_network_mtrnn", "(", "inputs", ",", "states", ",", "self", ".", "_num_units", ",", "self", ".", "_rank_vals", ",", "self", ".", "_num_freq", ",", "True", ")", "\n", "new_state", "=", "self", ".", "_activation", "(", "output", ")", "\n", "return", "new_state", ",", "new_state", "\n", "\n"]], "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.trnn.trnn._linear": [[106, 123], ["tensorflow.python.ops.variable_scope.get_variable_scope", "tensorflow.python.ops.nn_ops.bias_add", "a.get_shape", "tensorflow.python.ops.variable_scope.variable_scope", "tensorflow.python.ops.variable_scope.get_variable", "tensorflow.matmul", "tensorflow.concat", "tensorflow.python.ops.variable_scope.variable_scope", "tensorflow.python.ops.variable_scope.get_variable"], "function", ["None"], ["", "", "def", "_linear", "(", "args", ",", "output_size", ",", "bias", ",", "bias_start", "=", "0.0", ")", ":", "\n", "    ", "total_arg_size", "=", "0", "\n", "shapes", "=", "[", "a", ".", "get_shape", "(", ")", "for", "a", "in", "args", "]", "\n", "for", "shape", "in", "shapes", ":", "\n", "        ", "total_arg_size", "+=", "shape", "[", "1", "]", ".", "value", "\n", "", "dtype", "=", "[", "a", ".", "dtype", "for", "a", "in", "args", "]", "[", "0", "]", "\n", "\n", "scope", "=", "vs", ".", "get_variable_scope", "(", ")", "\n", "with", "vs", ".", "variable_scope", "(", "scope", ")", "as", "outer_scope", ":", "\n", "        ", "weights", "=", "vs", ".", "get_variable", "(", "\"weights\"", ",", "[", "total_arg_size", ",", "output_size", "]", ",", "dtype", "=", "dtype", ")", "\n", "\"\"\"y = [batch_size x total_arg_size] * [total_arg_size x output_size]\"\"\"", "\n", "res", "=", "tf", ".", "matmul", "(", "tf", ".", "concat", "(", "args", ",", "1", ")", ",", "weights", ")", "\n", "if", "not", "bias", ":", "\n", "            ", "return", "res", "\n", "", "with", "vs", ".", "variable_scope", "(", "outer_scope", ")", "as", "inner_scope", ":", "\n", "            ", "biases", "=", "vs", ".", "get_variable", "(", "\"biases\"", ",", "[", "output_size", "]", ",", "dtype", "=", "dtype", ")", "\n", "", "", "return", "nn_ops", ".", "bias_add", "(", "res", ",", "biases", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.trnn.trnn._shape_value": [[124, 127], ["tensor.get_shape"], "function", ["None"], ["", "def", "_shape_value", "(", "tensor", ")", ":", "\n", "    ", "shape", "=", "tensor", ".", "get_shape", "(", ")", "\n", "return", "[", "s", ".", "value", "for", "s", "in", "shape", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.trnn.trnn._outer_product": [[128, 136], ["tensorflow.expand_dims", "tensorflow.expand_dims", "tensorflow.matmul", "tensorflow.reshape", "tensorflow.reshape", "trnn._shape_value", "trnn._shape_value"], "function", ["home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.trnn.trnn._shape_value", "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.trnn.trnn._shape_value"], ["", "def", "_outer_product", "(", "batch_size", ",", "tensor", ",", "vector", ")", ":", "\n", "    ", "\"\"\"tensor-vector outer-product\"\"\"", "\n", "tensor_flat", "=", "tf", ".", "expand_dims", "(", "tf", ".", "reshape", "(", "tensor", ",", "[", "batch_size", ",", "-", "1", "]", ")", ",", "2", ")", "\n", "vector_flat", "=", "tf", ".", "expand_dims", "(", "vector", ",", "1", ")", "\n", "res", "=", "tf", ".", "matmul", "(", "tensor_flat", ",", "vector_flat", ")", "\n", "new_shape", "=", "[", "batch_size", "]", "+", "_shape_value", "(", "tensor", ")", "[", "1", ":", "]", "+", "_shape_value", "(", "vector", ")", "[", "1", ":", "]", "\n", "res", "=", "tf", ".", "reshape", "(", "res", ",", "new_shape", ")", "\n", "return", "res", "\n", "\n"]], "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.trnn.trnn.tensor_network_linear": [[138, 146], ["tensorflow.python.util.nest.flatten", "total_inputs.extend", "trnn._linear"], "function", ["home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.utils.misc.flatten", "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.trnn.trnn._linear"], ["", "def", "tensor_network_linear", "(", "inputs", ",", "states", ",", "output_size", ",", "bias", ",", "bias_start", "=", "0.0", ")", ":", "\n", "    ", "\"\"\"tensor network [inputs, states]-> output with tensor models\"\"\"", "\n", "# each coordinate of hidden state is independent- parallel", "\n", "states_tensor", "=", "nest", ".", "flatten", "(", "states", ")", "\n", "total_inputs", "=", "[", "inputs", "]", "\n", "total_inputs", ".", "extend", "(", "states", ")", "\n", "output", "=", "_linear", "(", "total_inputs", ",", "output_size", ",", "True", ")", "\n", "return", "output", "\n", "\n"]], "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.trnn.trnn.tensor_network_highorder": [[148, 169], ["len", "tensorflow.python.util.nest.flatten", "tensorflow.concat", "tensorflow.concat", "range", "tensorflow.reshape", "total_inputs.append", "trnn._linear", "tensorflow.shape", "trnn._outer_product", "states[].get_shape", "tensorflow.ones"], "function", ["home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.utils.misc.flatten", "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.worker.batch.ExperienceBatch.append", "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.trnn.trnn._linear", "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.trnn.trnn._outer_product"], ["", "def", "tensor_network_highorder", "(", "inputs", ",", "states", ",", "output_size", ",", "num_orders", ",", "bias", ",", "bias_start", "=", "0.0", ")", ":", "\n", "    ", "\"\"\"tensor network [inputs, states]-> output with tensor models\"\"\"", "\n", "# each coordinate of hidden state is independent- parallel", "\n", "num_lags", "=", "len", "(", "states", ")", "\n", "batch_size", "=", "tf", ".", "shape", "(", "inputs", ")", "[", "0", "]", "\n", "state_size", "=", "states", "[", "0", "]", ".", "get_shape", "(", ")", "[", "1", "]", ".", "value", "#hidden layer size", "\n", "total_state_size", "=", "(", "state_size", "*", "num_lags", "+", "1", ")", "\n", "\n", "states_tensor", "=", "nest", ".", "flatten", "(", "states", ")", "\n", "total_inputs", "=", "[", "inputs", "]", "\n", "\n", "states_vector", "=", "tf", ".", "concat", "(", "states", ",", "1", ")", "\n", "states_vector", "=", "tf", ".", "concat", "(", "[", "states_vector", ",", "tf", ".", "ones", "(", "[", "batch_size", ",", "1", "]", ")", "]", ",", "1", ")", "\n", "\"\"\"form high order state tensor\"\"\"", "\n", "states_tensor", "=", "states_vector", "\n", "for", "order", "in", "range", "(", "num_orders", "-", "1", ")", ":", "\n", "        ", "states_tensor", "=", "_outer_product", "(", "batch_size", ",", "states_tensor", ",", "states_vector", ")", "\n", "", "states_tensor", "=", "tf", ".", "reshape", "(", "states_tensor", ",", "[", "-", "1", ",", "total_state_size", "**", "num_orders", "]", ")", "\n", "total_inputs", ".", "append", "(", "states_tensor", ")", "\n", "output", "=", "_linear", "(", "total_inputs", ",", "output_size", ",", "True", ")", "\n", "return", "output", "\n", "\n"]], "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.trnn.trnn.tensor_train_contraction": [[170, 230], ["len", "tensorflow.einsum", "range", "tensorflow.squeeze", "trnn.tensor_train_contraction._get_indices"], "function", ["None"], ["", "def", "tensor_train_contraction", "(", "states_tensor", ",", "cores", ")", ":", "\n", "# print(\"input:\", states_tensor.name, states_tensor.get_shape().as_list())", "\n", "# print(\"mat_dims\", mat_dims)", "\n", "# print(\"mat_ranks\", mat_ranks)", "\n", "# print(\"mat_ps\", mat_ps)", "\n", "# print(\"mat_size\", mat_size)", "\n", "\n", "    ", "abc", "=", "\"abcdefgh\"", "\n", "ijk", "=", "\"ijklmnopqrstuvwxy\"", "\n", "\n", "def", "_get_indices", "(", "r", ")", ":", "\n", "        ", "indices", "=", "\"%s%s%s\"", "%", "(", "abc", "[", "r", "]", ",", "ijk", "[", "r", "]", ",", "abc", "[", "r", "+", "1", "]", ")", "\n", "return", "indices", "\n", "\n", "", "def", "_get_einsum", "(", "i", ",", "s2", ")", ":", "\n", "#", "\n", "        ", "s1", "=", "_get_indices", "(", "i", ")", "\n", "_s1", "=", "s1", ".", "replace", "(", "s1", "[", "1", "]", ",", "\"\"", ")", "\n", "_s2", "=", "s2", ".", "replace", "(", "s2", "[", "1", "]", ",", "\"\"", ")", "\n", "_s3", "=", "_s2", "+", "_s1", "\n", "_s3", "=", "_s3", "[", ":", "-", "3", "]", "+", "_s3", "[", "-", "1", ":", "]", "\n", "s3", "=", "s1", "+", "\",\"", "+", "s2", "+", "\"->\"", "+", "_s3", "\n", "return", "s3", ",", "_s3", "\n", "\n", "", "num_orders", "=", "len", "(", "cores", ")", "\n", "# first factor", "\n", "x", "=", "\"z\"", "+", "ijk", "[", ":", "num_orders", "]", "# \"z\" is the batch dimension", "\n", "\n", "# print(mat_core.get_shape().as_list())", "\n", "\n", "_s3", "=", "x", "[", ":", "1", "]", "+", "x", "[", "2", ":", "]", "+", "\"ab\"", "\n", "einsum", "=", "\"aib,\"", "+", "x", "+", "\"->\"", "+", "_s3", "\n", "x", "=", "_s3", "\n", "\n", "# print(\"einsum\", einsum, cores[0].get_shape().as_list, states_tensor.get_shape().as_list)", "\n", "\n", "out_h", "=", "tf", ".", "einsum", "(", "einsum", ",", "cores", "[", "0", "]", ",", "states_tensor", ")", "\n", "# print(out_h.name, out_h.get_shape().as_list())", "\n", "\n", "# 2nd - penultimate latent factor", "\n", "for", "i", "in", "range", "(", "1", ",", "num_orders", ")", ":", "\n", "\n", "# We now compute the tensor inner product W * H, where W is decomposed", "\n", "# into a tensor-train with D factors A^i. Each factor A^i is a 3-tensor,", "\n", "# with dimensions [mat_rank[i], hidden_size, mat_rank[i+1] ]", "\n", "# The lag index, indexing the components of the state vector H,", "\n", "# runs from 1 <= i < K.", "\n", "\n", "# print mat_core.get_shape().as_list()", "\n", "\n", "        ", "einsum", ",", "x", "=", "ss", ",", "_s3", "=", "_get_einsum", "(", "i", ",", "x", ")", "\n", "\n", "# print \"order\", i, ss", "\n", "\n", "out_h", "=", "tf", ".", "einsum", "(", "einsum", ",", "cores", "[", "i", "]", ",", "out_h", ")", "\n", "# print(out_h.name, out_h.get_shape().as_list())", "\n", "\n", "# print \"Squeeze out the dimension-1 dummy dim (first dim of 1st latent factor)\"", "\n", "", "out_h", "=", "tf", ".", "squeeze", "(", "out_h", ",", "[", "1", "]", ")", "\n", "return", "out_h", "\n", "\n"]], "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.trnn.trnn.tensor_network_tt_einsum": [[232, 302], ["len", "numpy.concatenate", "numpy.cumsum", "tensorflow.python.ops.variable_scope.get_variable", "tensorflow.matmul", "tensorflow.python.ops.variable_scope.get_variable", "tensorflow.concat", "tensorflow.concat", "range", "range", "trnn.tensor_train_contraction", "tensorflow.add", "tensorflow.python.ops.variable_scope.get_variable", "tensorflow.python.ops.nn_ops.bias_add", "len", "tensorflow.shape", "numpy.ones", "numpy.concatenate", "trnn._outer_product", "tensorflow.slice", "tensorflow.reshape", "cores.append", "states[].get_shape", "inputs.get_shape", "tensorflow.ones"], "function", ["home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.trnn.trnn.tensor_train_contraction", "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.worker.batch.CompositeBatch.add", "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.trnn.trnn._outer_product", "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.worker.batch.ExperienceBatch.append"], ["", "def", "tensor_network_tt_einsum", "(", "inputs", ",", "states", ",", "output_size", ",", "rank_vals", ",", "bias", ",", "bias_start", "=", "0.0", ")", ":", "\n", "\n", "# print(\"Using Einsum Tensor-Train decomposition.\")", "\n", "\n", "    ", "\"\"\"tensor train decomposition for the full tenosr \"\"\"", "\n", "num_orders", "=", "len", "(", "rank_vals", ")", "+", "1", "#alpha_1 to alpha_{K-1}", "\n", "num_lags", "=", "len", "(", "states", ")", "\n", "batch_size", "=", "tf", ".", "shape", "(", "inputs", ")", "[", "0", "]", "\n", "state_size", "=", "states", "[", "0", "]", ".", "get_shape", "(", ")", "[", "1", "]", ".", "value", "#hidden layer size", "\n", "input_size", "=", "inputs", ".", "get_shape", "(", ")", "[", "1", "]", ".", "value", "\n", "\n", "\n", "total_state_size", "=", "(", "state_size", "*", "num_lags", "+", "1", ")", "\n", "\n", "# These bookkeeping variables hold the dimension information that we'll", "\n", "# use to store and access the transition tensor W efficiently.", "\n", "mat_dims", "=", "np", ".", "ones", "(", "(", "num_orders", ",", ")", ")", "*", "total_state_size", "\n", "\n", "# The latent dimensions used in our tensor-train decomposition.", "\n", "# Each factor A^i is a 3-tensor, with dimensions [a_i, hidden_size, a_{i+1}]", "\n", "# with dimensions [mat_rank[i], hidden_size, mat_rank[i+1] ]", "\n", "# The last", "\n", "# entry is the output dimension, output_size: that dimension will be the", "\n", "# output.", "\n", "mat_ranks", "=", "np", ".", "concatenate", "(", "(", "[", "1", "]", ",", "rank_vals", ",", "[", "output_size", "]", ")", ")", "\n", "\n", "# This stores the boundary indices for the factors A. Starting from 0,", "\n", "# each index i is computed by adding the number of weights in the i'th", "\n", "# factor A^i.", "\n", "mat_ps", "=", "np", ".", "cumsum", "(", "np", ".", "concatenate", "(", "(", "[", "0", "]", ",", "mat_ranks", "[", ":", "-", "1", "]", "*", "mat_dims", "*", "mat_ranks", "[", "1", ":", "]", ")", ")", ",", "dtype", "=", "np", ".", "int32", ")", "\n", "mat_size", "=", "mat_ps", "[", "-", "1", "]", "\n", "\n", "# Compute U * x", "\n", "weights_x", "=", "vs", ".", "get_variable", "(", "\"weights_x\"", ",", "[", "input_size", ",", "output_size", "]", ")", "\n", "out_x", "=", "tf", ".", "matmul", "(", "inputs", ",", "weights_x", ")", "\n", "\n", "# Get a variable that holds all the weights of the factors A^i of the", "\n", "# transition tensor W. All weights are stored serially, so we need to do", "\n", "# some bookkeeping to keep track of where each factor is stored.", "\n", "mat", "=", "vs", ".", "get_variable", "(", "\"weights_h\"", ",", "mat_size", ")", "# h_z x h_z... x output_size", "\n", "\n", "#mat = tf.Variable(mat, name=\"weights\")", "\n", "states_vector", "=", "tf", ".", "concat", "(", "states", ",", "1", ")", "\n", "states_vector", "=", "tf", ".", "concat", "(", "[", "states_vector", ",", "tf", ".", "ones", "(", "[", "batch_size", ",", "1", "]", ")", "]", ",", "1", ")", "\n", "\"\"\"form high order state tensor\"\"\"", "\n", "states_tensor", "=", "states_vector", "\n", "for", "order", "in", "range", "(", "num_orders", "-", "1", ")", ":", "\n", "        ", "states_tensor", "=", "_outer_product", "(", "batch_size", ",", "states_tensor", ",", "states_vector", ")", "\n", "\n", "# print(\"tensor product\", states_tensor.name, states_tensor.get_shape().as_list())", "\n", "", "cores", "=", "[", "]", "\n", "for", "i", "in", "range", "(", "num_orders", ")", ":", "\n", "# Fetch the weights of factor A^i from our big serialized variable weights_h.", "\n", "        ", "mat_core", "=", "tf", ".", "slice", "(", "mat", ",", "[", "mat_ps", "[", "i", "]", "]", ",", "[", "mat_ps", "[", "i", "+", "1", "]", "-", "mat_ps", "[", "i", "]", "]", ")", "\n", "mat_core", "=", "tf", ".", "reshape", "(", "mat_core", ",", "[", "mat_ranks", "[", "i", "]", ",", "total_state_size", ",", "mat_ranks", "[", "i", "+", "1", "]", "]", ")", "\n", "cores", ".", "append", "(", "mat_core", ")", "\n", "\n", "", "out_h", "=", "tensor_train_contraction", "(", "states_tensor", ",", "cores", ")", "\n", "# Compute h_t = U*x_t + W*H_{t-1}", "\n", "res", "=", "tf", ".", "add", "(", "out_x", ",", "out_h", ")", "\n", "\n", "# print \"END OF CELL CONSTRUCTION\"", "\n", "# print \"========================\"", "\n", "# print \"\"", "\n", "\n", "if", "not", "bias", ":", "\n", "        ", "return", "res", "\n", "", "biases", "=", "vs", ".", "get_variable", "(", "\"biases\"", ",", "[", "output_size", "]", ")", "\n", "\n", "return", "nn_ops", ".", "bias_add", "(", "res", ",", "biases", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.trnn.trnn.tensor_network_aug": [[303, 344], ["len", "numpy.concatenate", "numpy.cumsum", "tensorflow.python.ops.variable_scope.get_variable", "tensorflow.python.util.nest.flatten", "tensorflow.concat", "tensorflow.concat", "range", "range", "trnn.tensor_train_contraction", "tensorflow.python.ops.variable_scope.get_variable", "tensorflow.python.ops.nn_ops.bias_add", "len", "tensorflow.shape", "numpy.ones", "numpy.concatenate", "trnn._outer_product", "tensorflow.slice", "tensorflow.reshape", "cores.append", "states[].get_shape", "inputs.get_shape", "tensorflow.ones"], "function", ["home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.utils.misc.flatten", "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.trnn.trnn.tensor_train_contraction", "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.trnn.trnn._outer_product", "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.worker.batch.ExperienceBatch.append"], ["", "def", "tensor_network_aug", "(", "inputs", ",", "states", ",", "output_size", ",", "rank_vals", ",", "bias", ",", "bias_start", "=", "0.0", ")", ":", "\n", "    ", "\"\"\"tensor network [inputs, states]-> output with tensor models\"\"\"", "\n", "# each coordinate of hidden state is independent- parallel", "\n", "num_orders", "=", "len", "(", "rank_vals", ")", "+", "1", "\n", "num_lags", "=", "len", "(", "states", ")", "\n", "batch_size", "=", "tf", ".", "shape", "(", "inputs", ")", "[", "0", "]", "\n", "state_size", "=", "states", "[", "0", "]", ".", "get_shape", "(", ")", "[", "1", "]", ".", "value", "#hidden layer size", "\n", "inp_size", "=", "inputs", ".", "get_shape", "(", ")", "[", "1", "]", ".", "value", "\n", "total_state_size", "=", "(", "inp_size", "+", "state_size", "*", "num_lags", "+", "1", ")", "\n", "\n", "mat_dims", "=", "np", ".", "ones", "(", "(", "num_orders", ",", ")", ")", "*", "total_state_size", "\n", "mat_ranks", "=", "np", ".", "concatenate", "(", "(", "[", "1", "]", ",", "rank_vals", ",", "[", "output_size", "]", ")", ")", "\n", "mat_ps", "=", "np", ".", "cumsum", "(", "np", ".", "concatenate", "(", "(", "[", "0", "]", ",", "mat_ranks", "[", ":", "-", "1", "]", "*", "mat_dims", "*", "mat_ranks", "[", "1", ":", "]", ")", ")", ",", "dtype", "=", "np", ".", "int32", ")", "\n", "mat_size", "=", "mat_ps", "[", "-", "1", "]", "\n", "mat", "=", "vs", ".", "get_variable", "(", "\"weights\"", ",", "mat_size", ")", "# h_z x h_z... x output_size", "\n", "\n", "states", "=", "(", "inputs", ",", ")", "+", "states", "# concatenate the [x, h] ", "\n", "\n", "states_tensor", "=", "nest", ".", "flatten", "(", "states", ")", "\n", "#total_inputs = [inputs]", "\n", "states_vector", "=", "tf", ".", "concat", "(", "states", ",", "1", ")", "\n", "states_vector", "=", "tf", ".", "concat", "(", "[", "states_vector", ",", "tf", ".", "ones", "(", "[", "batch_size", ",", "1", "]", ")", "]", ",", "1", ")", "\n", "\"\"\"form high order state tensor\"\"\"", "\n", "states_tensor", "=", "states_vector", "\n", "for", "order", "in", "range", "(", "num_orders", "-", "1", ")", ":", "\n", "        ", "states_tensor", "=", "_outer_product", "(", "batch_size", ",", "states_tensor", ",", "states_vector", ")", "\n", "\n", "# states_tensor= tf.reshape(states_tensor, [-1,total_state_size**num_orders] )", "\n", "\n", "", "cores", "=", "[", "]", "\n", "for", "i", "in", "range", "(", "num_orders", ")", ":", "\n", "# Fetch the weights of factor A^i from our big serialized variable weights_h.", "\n", "        ", "mat_core", "=", "tf", ".", "slice", "(", "mat", ",", "[", "mat_ps", "[", "i", "]", "]", ",", "[", "mat_ps", "[", "i", "+", "1", "]", "-", "mat_ps", "[", "i", "]", "]", ")", "\n", "mat_core", "=", "tf", ".", "reshape", "(", "mat_core", ",", "[", "mat_ranks", "[", "i", "]", ",", "total_state_size", ",", "mat_ranks", "[", "i", "+", "1", "]", "]", ")", "\n", "cores", ".", "append", "(", "mat_core", ")", "\n", "\n", "", "res", "=", "tensor_train_contraction", "(", "states_tensor", ",", "cores", ")", "\n", "if", "not", "bias", ":", "\n", "        ", "return", "res", "\n", "", "biases", "=", "vs", ".", "get_variable", "(", "\"biases\"", ",", "[", "output_size", "]", ")", "\n", "return", "nn_ops", ".", "bias_add", "(", "res", ",", "biases", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.trnn.trnn.tensor_network_mtrnn": [[345, 424], ["len", "tensorflow.python.ops.variable_scope.get_variable", "tensorflow.matmul", "numpy.concatenate", "numpy.cumsum", "tensorflow.python.ops.variable_scope.get_variable", "tensorflow.concat", "tensorflow.concat", "range", "range", "numpy.concatenate", "numpy.cumsum", "tensorflow.python.ops.variable_scope.get_variable", "tensorflow.concat", "tensorflow.concat", "range", "range", "print", "print", "print", "trnn.tensor_train_contraction", "tensorflow.add", "tensorflow.python.ops.variable_scope.get_variable", "tensorflow.python.ops.nn_ops.bias_add", "len", "tensorflow.shape", "numpy.ones", "numpy.concatenate", "trnn._outer_product", "tensorflow.slice", "tensorflow.reshape", "cores.append", "print", "print", "print", "trnn.tensor_train_contraction", "numpy.ones", "numpy.concatenate", "trnn._outer_product", "tensorflow.slice", "tensorflow.reshape", "cores.append", "len", "_outer_product.get_shape", "inputs.get_shape", "tensorflow.ones", "len", "_outer_product.get_shape", "tensorflow.ones"], "function", ["home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.trnn.trnn.tensor_train_contraction", "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.worker.batch.CompositeBatch.add", "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.trnn.trnn._outer_product", "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.worker.batch.ExperienceBatch.append", "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.trnn.trnn.tensor_train_contraction", "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.trnn.trnn._outer_product", "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.worker.batch.ExperienceBatch.append"], ["", "def", "tensor_network_mtrnn", "(", "inputs", ",", "states", ",", "output_size", ",", "rank_vals", ",", "num_freq", ",", "bias", ",", "bias_start", "=", "0.0", ")", ":", "\n", "    ", "\"states to output mapping for multi-resolution tensor rnn\"", "\n", "\"\"\"tensor train decomposition for the full tenosr \"\"\"", "\n", "num_orders", "=", "len", "(", "rank_vals", ")", "+", "1", "#alpha_1 to alpha_{K-1}", "\n", "num_lags", "=", "len", "(", "states", ")", "\n", "batch_size", "=", "tf", ".", "shape", "(", "inputs", ")", "[", "0", "]", "\n", "state_size", "=", "output_size", "#hidden layer size", "\n", "input_size", "=", "inputs", ".", "get_shape", "(", ")", "[", "1", "]", ".", "value", "\n", "\n", "\n", "# input weights W_x ", "\n", "weights_x", "=", "vs", ".", "get_variable", "(", "\"weights_x\"", ",", "[", "input_size", ",", "output_size", "]", ")", "\n", "out_x", "=", "tf", ".", "matmul", "(", "inputs", ",", "weights_x", ")", "\n", "\n", "\n", "# 1st tensor train layer W_h", "\n", "total_state_size", "=", "(", "state_size", "*", "num_lags", "+", "1", ")", "\n", "mat_dims", "=", "np", ".", "ones", "(", "(", "num_orders", ",", ")", ")", "*", "total_state_size", "\n", "mat_ranks", "=", "np", ".", "concatenate", "(", "(", "[", "1", "]", ",", "rank_vals", ",", "[", "output_size", "]", ")", ")", "\n", "mat_ps", "=", "np", ".", "cumsum", "(", "np", ".", "concatenate", "(", "(", "[", "0", "]", ",", "mat_ranks", "[", ":", "-", "1", "]", "*", "mat_dims", "*", "mat_ranks", "[", "1", ":", "]", ")", ")", ",", "dtype", "=", "np", ".", "int32", ")", "\n", "mat_size", "=", "mat_ps", "[", "-", "1", "]", "\n", "mat", "=", "vs", ".", "get_variable", "(", "\"weights_h\"", ",", "mat_size", ")", "# h_z x h_z... x output_size", "\n", "\n", "states_vector", "=", "tf", ".", "concat", "(", "states", ",", "1", ")", "\n", "states_vector", "=", "tf", ".", "concat", "(", "[", "states_vector", ",", "tf", ".", "ones", "(", "[", "batch_size", ",", "1", "]", ")", "]", ",", "1", ")", "\n", "\"\"\"form high order state tensor\"\"\"", "\n", "states_tensor", "=", "states_vector", "\n", "for", "order", "in", "range", "(", "num_orders", "-", "1", ")", ":", "\n", "        ", "states_tensor", "=", "_outer_product", "(", "batch_size", ",", "states_tensor", ",", "states_vector", ")", "\n", "\n", "", "cores", "=", "[", "]", "\n", "for", "i", "in", "range", "(", "num_orders", ")", ":", "\n", "# Fetch the weights of factor A^i from our big serialized variable weights_h.", "\n", "        ", "mat_core", "=", "tf", ".", "slice", "(", "mat", ",", "[", "mat_ps", "[", "i", "]", "]", ",", "[", "mat_ps", "[", "i", "+", "1", "]", "-", "mat_ps", "[", "i", "]", "]", ")", "\n", "mat_core", "=", "tf", ".", "reshape", "(", "mat_core", ",", "[", "mat_ranks", "[", "i", "]", ",", "total_state_size", ",", "mat_ranks", "[", "i", "+", "1", "]", "]", ")", "\n", "cores", ".", "append", "(", "mat_core", ")", "\n", "\n", "print", "(", "'-'", "*", "80", ")", "\n", "print", "(", "'1st layer tensor train\\n'", ")", "\n", "print", "(", "'|states res|'", ",", "1", ",", "'|states len|'", ",", "len", "(", "states", ")", ",", "'|states size|'", ",", "states_tensor", ".", "get_shape", "(", ")", ")", "\n", "h_1", "=", "tensor_train_contraction", "(", "states_tensor", ",", "cores", ")", "\n", "\n", "# 2nd tensor train layer W_h2", "\n", "", "total_state_size", "=", "(", "state_size", "*", "num_lags", "//", "num_freq", "+", "1", ")", "\n", "mat_dims", "=", "np", ".", "ones", "(", "(", "num_orders", ",", ")", ")", "*", "total_state_size", "\n", "mat_ranks", "=", "np", ".", "concatenate", "(", "(", "[", "1", "]", ",", "rank_vals", ",", "[", "output_size", "]", ")", ")", "\n", "mat_ps", "=", "np", ".", "cumsum", "(", "np", ".", "concatenate", "(", "(", "[", "0", "]", ",", "mat_ranks", "[", ":", "-", "1", "]", "*", "mat_dims", "*", "mat_ranks", "[", "1", ":", "]", ")", ")", ",", "dtype", "=", "np", ".", "int32", ")", "\n", "mat_size", "=", "mat_ps", "[", "-", "1", "]", "\n", "mat", "=", "vs", ".", "get_variable", "(", "\"weights_h2\"", ",", "mat_size", ")", "# h_z x h_z... x output_size", "\n", "\n", "\n", "new_states", "=", "states", "[", ":", ":", "num_freq", "]", "\n", "states_vector", "=", "tf", ".", "concat", "(", "new_states", ",", "1", ")", "\n", "states_vector", "=", "tf", ".", "concat", "(", "[", "states_vector", ",", "tf", ".", "ones", "(", "[", "batch_size", ",", "1", "]", ")", "]", ",", "1", ")", "\n", "\"\"\"form high order state tensor\"\"\"", "\n", "states_tensor", "=", "states_vector", "\n", "for", "order", "in", "range", "(", "num_orders", "-", "1", ")", ":", "\n", "        ", "states_tensor", "=", "_outer_product", "(", "batch_size", ",", "states_tensor", ",", "states_vector", ")", "\n", "\n", "", "cores", "=", "[", "]", "\n", "for", "i", "in", "range", "(", "num_orders", ")", ":", "\n", "# Fetch the weights of factor A^i from our big serialized variable weights_h.", "\n", "        ", "mat_core", "=", "tf", ".", "slice", "(", "mat", ",", "[", "mat_ps", "[", "i", "]", "]", ",", "[", "mat_ps", "[", "i", "+", "1", "]", "-", "mat_ps", "[", "i", "]", "]", ")", "\n", "mat_core", "=", "tf", ".", "reshape", "(", "mat_core", ",", "[", "mat_ranks", "[", "i", "]", ",", "total_state_size", ",", "mat_ranks", "[", "i", "+", "1", "]", "]", ")", "\n", "cores", ".", "append", "(", "mat_core", ")", "\n", "", "print", "(", "'-'", "*", "80", ")", "\n", "print", "(", "'2nd layer tensor train\\n'", ")", "\n", "print", "(", "'|states res|'", ",", "num_freq", ",", "'|states len|'", ",", "len", "(", "new_states", ")", ",", "'|states size|'", ",", "states_tensor", ".", "get_shape", "(", ")", ")", "\n", "h_2", "=", "tensor_train_contraction", "(", "states_tensor", ",", "cores", ")", "\n", "\n", "# Combine two tensor train ", "\n", "out_h", "=", "h_1", "+", "h_2", "\n", "# Compute h_t = W_x*x_t + W_h*H_{t-1}", "\n", "res", "=", "tf", ".", "add", "(", "out_x", ",", "out_h", ")", "\n", "\n", "if", "not", "bias", ":", "\n", "        ", "return", "\n", "", "biases", "=", "vs", ".", "get_variable", "(", "\"biases\"", ",", "[", "output_size", "]", ")", "\n", "return", "nn_ops", ".", "bias_add", "(", "res", ",", "biases", ")", "", "", ""]], "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.trnn.trnn_imply.rnn_with_feed_prev": [[28, 97], ["tensorflow.stack", "print", "print", "print", "print", "tensorflow.variable_scope", "inputs.get_shape().with_rank_at_least", "int", "tensorflow.expand_dims", "tensorflow.contrib.distributions.Bernoulli", "tensorflow.contrib.distributions.Bernoulli.sample", "range", "varscope.set_caching_device", "tensorflow.shape", "tensorflow.range", "cell.zero_state", "isinstance", "inputs.get_shape", "tensorflow.get_variable_scope().reuse_variables", "cell", "cell", "tensorflow.variable_scope", "tensorflow.contrib.layers.fully_connected", "tf.stack.append", "tensorflow.variable_scope", "tensorflow.cond", "tensorflow.variable_scope", "tensorflow.contrib.layers.fully_connected", "tensorflow.get_variable_scope", "tensorflow.get_variable_scope", "tensorflow.get_variable_scope", "tensorflow.cast", "tensorflow.get_variable_scope", "tensorflow.identity", "tensorflow.contrib.layers.fully_connected"], "function", ["home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.buffer.pseudo_prioritized_buffer.PseudoPrioritizedBuffer.sample", "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.worker.batch.ExperienceBatch.append"], ["", "def", "rnn_with_feed_prev", "(", "cell", ",", "inputs", ",", "is_training", ",", "config", ",", "initial_state", "=", "None", ")", ":", "\n", "    ", "prev", "=", "None", "\n", "outputs", "=", "[", "]", "\n", "sample_prob", "=", "config", ".", "sample_prob", "# scheduled sampling probability", "\n", "\n", "feed_prev", "=", "not", "is_training", "if", "config", ".", "use_error_prop", "else", "False", "\n", "is_sample", "=", "is_training", "and", "initial_state", "is", "not", "None", "# decoder  ", "\n", "\n", "if", "is_sample", ":", "\n", "        ", "print", "(", "\"Creating model @ training  --> Using scheduled sampling.\"", ")", "\n", "", "else", ":", "\n", "        ", "print", "(", "\"Creating model @ training  --> Not using scheduled sampling.\"", ")", "\n", "\n", "", "if", "feed_prev", ":", "\n", "        ", "print", "(", "' '", "*", "30", "+", "\" --> Feeding output back into input.\"", ")", "\n", "", "else", ":", "\n", "        ", "print", "(", "' '", "*", "30", "+", "\" --> Feeding ground truth into input.\"", ")", "\n", "\n", "", "with", "tf", ".", "variable_scope", "(", "\"rnn\"", ")", "as", "varscope", ":", "\n", "        ", "if", "varscope", ".", "caching_device", "is", "None", ":", "\n", "            ", "varscope", ".", "set_caching_device", "(", "lambda", "op", ":", "op", ".", "device", ")", "\n", "\n", "", "inputs_shape", "=", "inputs", ".", "get_shape", "(", ")", ".", "with_rank_at_least", "(", "3", ")", "\n", "batch_size", "=", "tf", ".", "shape", "(", "inputs", ")", "[", "0", "]", "\n", "num_steps", "=", "inputs_shape", "[", "1", "]", "\n", "input_size", "=", "int", "(", "inputs_shape", "[", "2", "]", ")", "\n", "burn_in_steps", "=", "config", ".", "burn_in_steps", "\n", "output_size", "=", "cell", ".", "output_size", "\n", "\n", "# phased lstm input", "\n", "inp_t", "=", "tf", ".", "expand_dims", "(", "tf", ".", "range", "(", "1", ",", "batch_size", "+", "1", ")", ",", "1", ")", "\n", "\n", "dist", "=", "Bernoulli", "(", "probs", "=", "config", ".", "sample_prob", ")", "\n", "samples", "=", "dist", ".", "sample", "(", "sample_shape", "=", "num_steps", ")", "\n", "# with tf.Session() as sess:", "\n", "#     print('bernoulli',samples.eval())", "\n", "if", "initial_state", "is", "None", ":", "\n", "            ", "initial_state", "=", "cell", ".", "zero_state", "(", "batch_size", ",", "dtype", "=", "tf", ".", "float32", ")", "\n", "", "state", "=", "initial_state", "\n", "\n", "for", "time_step", "in", "range", "(", "num_steps", ")", ":", "\n", "            ", "if", "time_step", ">", "0", ":", "\n", "                ", "tf", ".", "get_variable_scope", "(", ")", ".", "reuse_variables", "(", ")", "\n", "\n", "", "inp", "=", "inputs", "[", ":", ",", "time_step", ",", ":", "]", "\n", "\n", "if", "is_sample", "and", "time_step", ">", "0", ":", "\n", "                ", "with", "tf", ".", "variable_scope", "(", "tf", ".", "get_variable_scope", "(", ")", ",", "reuse", "=", "True", ")", ":", "\n", "                    ", "inp", "=", "tf", ".", "cond", "(", "tf", ".", "cast", "(", "samples", "[", "time_step", "]", ",", "tf", ".", "bool", ")", ",", "lambda", ":", "tf", ".", "identity", "(", "inp", ")", ",", "lambda", ":", "fully_connected", "(", "cell_output", ",", "input_size", ",", "activation_fn", "=", "tf", ".", "sigmoid", ")", ")", "\n", "\n", "\n", "", "", "if", "feed_prev", "and", "prev", "is", "not", "None", "and", "time_step", ">=", "burn_in_steps", ":", "\n", "                ", "with", "tf", ".", "variable_scope", "(", "tf", ".", "get_variable_scope", "(", ")", ",", "reuse", "=", "True", ")", ":", "\n", "                    ", "inp", "=", "fully_connected", "(", "prev", ",", "input_size", ",", "activation_fn", "=", "tf", ".", "sigmoid", ")", "\n", "#print(\"t\", time_step, \">=\", burn_in_steps, \"--> feeding back output into input.\")", "\n", "\n", "", "", "if", "isinstance", "(", "cell", ".", "_cells", "[", "0", "]", ",", "tf", ".", "contrib", ".", "rnn", ".", "PhasedLSTMCell", ")", ":", "\n", "                ", "(", "cell_output", ",", "state", ")", "=", "cell", "(", "(", "inp_t", ",", "inp", ")", ",", "state", ")", "\n", "", "else", ":", "\n", "                ", "(", "cell_output", ",", "state", ")", "=", "cell", "(", "inp", ",", "state", ")", "\n", "\n", "", "prev", "=", "cell_output", "\n", "with", "tf", ".", "variable_scope", "(", "tf", ".", "get_variable_scope", "(", ")", ",", "reuse", "=", "False", ")", ":", "\n", "                ", "output", "=", "fully_connected", "(", "cell_output", ",", "input_size", ",", "activation_fn", "=", "tf", ".", "sigmoid", ")", "\n", "outputs", ".", "append", "(", "output", ")", "\n", "\n", "", "", "", "outputs", "=", "tf", ".", "stack", "(", "outputs", ",", "1", ")", "\n", "return", "outputs", ",", "state", "\n", "\n"]], "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.trnn.trnn_imply._shift": [[98, 105], ["copy.copy", "collections.deque", "collections.deque.append", "collections.deque.popleft"], "function", ["home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.utils.running_std.RunningMeanStd.copy", "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.worker.batch.ExperienceBatch.append"], ["", "def", "_shift", "(", "input_list", ",", "new_item", ")", ":", "\n", "    ", "\"\"\"Update lag number of states\"\"\"", "\n", "output_list", "=", "copy", ".", "copy", "(", "input_list", ")", "\n", "output_list", "=", "deque", "(", "output_list", ")", "\n", "output_list", ".", "append", "(", "new_item", ")", "# deque = [1, 2, 3]", "\n", "output_list", ".", "popleft", "(", ")", "# deque =[2, 3]", "\n", "return", "output_list", "\n", "\n"]], "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.trnn.trnn_imply._list_to_states": [[106, 119], ["len", "range"], "function", ["None"], ["", "def", "_list_to_states", "(", "states_list", ")", ":", "\n", "    ", "\"\"\"Transform a list of state tuples into an augmented tuple state\n    customizable function, depends on how long history is used\"\"\"", "\n", "num_layers", "=", "len", "(", "states_list", "[", "0", "]", ")", "# state = (layer1, layer2...), layer1 = (c,h), c = tensor(batch_size, num_steps)", "\n", "output_states", "=", "(", ")", "\n", "for", "layer", "in", "range", "(", "num_layers", ")", ":", "\n", "        ", "output_state", "=", "(", ")", "\n", "for", "states", "in", "states_list", ":", "\n", "#c,h = states[layer] for LSTM", "\n", "                ", "output_state", "+=", "(", "states", "[", "layer", "]", ",", ")", "\n", "", "output_states", "+=", "(", "output_state", ",", ")", "\n", "# new cell has s*num_lags states", "\n", "", "return", "output_states", "\n", "\n"]], "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.trnn.trnn_imply.tensor_rnn_with_feed_prev": [[120, 196], ["tensorflow.stack", "print", "print", "print", "print", "tensorflow.variable_scope", "inputs.get_shape().with_rank_at_least", "int", "tensorflow.contrib.distributions.Bernoulli", "tensorflow.contrib.distributions.Bernoulli.sample", "range", "varscope.set_caching_device", "tensorflow.shape", "range", "trnn_imply._list_to_states", "cell", "tensorflow.nn.dropout", "trnn_imply._shift", "inputs.get_shape", "cell.zero_state", "initial_states.append", "tensorflow.get_variable_scope().reuse_variables", "tensorflow.variable_scope", "tensorflow.contrib.layers.fully_connected", "tf.stack.append", "tensorflow.variable_scope", "tensorflow.cond", "tensorflow.variable_scope", "tensorflow.contrib.layers.fully_connected", "tensorflow.get_variable_scope", "tensorflow.get_variable_scope", "tensorflow.get_variable_scope", "tensorflow.cast", "tensorflow.get_variable_scope", "tensorflow.identity", "tensorflow.contrib.layers.fully_connected"], "function", ["home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.buffer.pseudo_prioritized_buffer.PseudoPrioritizedBuffer.sample", "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.trnn.trnn_imply._list_to_states", "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.trnn.trnn_imply._shift", "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.worker.batch.ExperienceBatch.append", "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.worker.batch.ExperienceBatch.append"], ["", "def", "tensor_rnn_with_feed_prev", "(", "cell", ",", "inputs", ",", "is_training", ",", "config", ",", "initial_states", "=", "None", ")", ":", "\n", "    ", "\"\"\"High Order Recurrent Neural Network Layer\n    \"\"\"", "\n", "#tuple of 2-d tensor (batch_size, s)", "\n", "outputs", "=", "[", "]", "\n", "prev", "=", "None", "\n", "feed_prev", "=", "not", "is_training", "if", "config", ".", "use_error_prop", "else", "False", "\n", "is_sample", "=", "is_training", "and", "initial_states", "is", "not", "None", "\n", "\n", "if", "is_sample", ":", "\n", "        ", "print", "(", "\"Creating model @ training  --> Using scheduled sampling.\"", ")", "\n", "", "else", ":", "\n", "        ", "print", "(", "\"Creating model @ training  --> Not using scheduled sampling.\"", ")", "\n", "\n", "", "if", "feed_prev", ":", "\n", "        ", "print", "(", "' '", "*", "30", "+", "\" --> Feeding output back into input.\"", ")", "\n", "", "else", ":", "\n", "        ", "print", "(", "' '", "*", "30", "+", "\" --> Feeding ground truth into input.\"", ")", "\n", "\n", "", "with", "tf", ".", "variable_scope", "(", "\"trnn\"", ")", "as", "varscope", ":", "\n", "        ", "if", "varscope", ".", "caching_device", "is", "None", ":", "\n", "                    ", "varscope", ".", "set_caching_device", "(", "lambda", "op", ":", "op", ".", "device", ")", "\n", "\n", "", "inputs_shape", "=", "inputs", ".", "get_shape", "(", ")", ".", "with_rank_at_least", "(", "3", ")", "\n", "batch_size", "=", "tf", ".", "shape", "(", "inputs", ")", "[", "0", "]", "\n", "num_steps", "=", "inputs_shape", "[", "1", "]", "\n", "input_size", "=", "int", "(", "inputs_shape", "[", "2", "]", ")", "\n", "output_size", "=", "cell", ".", "output_size", "\n", "burn_in_steps", "=", "config", ".", "burn_in_steps", "\n", "\n", "# Scheduled sampling", "\n", "dist", "=", "Bernoulli", "(", "probs", "=", "config", ".", "sample_prob", ")", "\n", "samples", "=", "dist", ".", "sample", "(", "sample_shape", "=", "num_steps", ")", "\n", "\n", "if", "initial_states", "is", "None", ":", "\n", "            ", "initial_states", "=", "[", "]", "\n", "for", "lag", "in", "range", "(", "config", ".", "num_lags", ")", ":", "\n", "                ", "initial_state", "=", "cell", ".", "zero_state", "(", "batch_size", ",", "dtype", "=", "tf", ".", "float32", ")", "\n", "initial_states", ".", "append", "(", "initial_state", ")", "\n", "\n", "", "", "states_list", "=", "initial_states", "#list of high order states", "\n", "\n", "for", "time_step", "in", "range", "(", "num_steps", ")", ":", "\n", "            ", "if", "time_step", ">", "0", ":", "\n", "                ", "tf", ".", "get_variable_scope", "(", ")", ".", "reuse_variables", "(", ")", "\n", "\n", "", "inp", "=", "inputs", "[", ":", ",", "time_step", ",", ":", "]", "\n", "\n", "if", "is_sample", "and", "time_step", ">", "0", ":", "\n", "                ", "with", "tf", ".", "variable_scope", "(", "tf", ".", "get_variable_scope", "(", ")", ",", "reuse", "=", "True", ")", ":", "\n", "                    ", "inp", "=", "tf", ".", "cond", "(", "tf", ".", "cast", "(", "samples", "[", "time_step", "]", ",", "tf", ".", "bool", ")", ",", "lambda", ":", "tf", ".", "identity", "(", "inp", ")", ",", "lambda", ":", "fully_connected", "(", "cell_output", ",", "input_size", ",", "activation_fn", "=", "tf", ".", "sigmoid", ")", ")", "\n", "\n", "", "", "if", "feed_prev", "and", "prev", "is", "not", "None", "and", "time_step", ">=", "burn_in_steps", ":", "\n", "                ", "with", "tf", ".", "variable_scope", "(", "tf", ".", "get_variable_scope", "(", ")", ",", "reuse", "=", "True", ")", ":", "\n", "                    ", "inp", "=", "fully_connected", "(", "cell_output", ",", "input_size", ",", "activation_fn", "=", "tf", ".", "sigmoid", ")", "\n", "#print(\"t\", time_step, \">=\", burn_in_steps, \"--> feeding back output into input.\")", "\n", "\n", "", "", "states", "=", "_list_to_states", "(", "states_list", ")", "\n", "\"\"\"input tensor is [batch_size, num_steps, input_size]\"\"\"", "\n", "(", "cell_output", ",", "state", ")", "=", "cell", "(", "inp", ",", "states", ")", "\n", "\n", "# dropout ", "\n", "# keep_prob = tf.placeholder(tf.float32)", "\n", "keep_prob", "=", "0.5", "\n", "cell_output", "=", "tf", ".", "nn", ".", "dropout", "(", "cell_output", ",", "keep_prob", ")", "\n", "\n", "states_list", "=", "_shift", "(", "states_list", ",", "state", ")", "\n", "\n", "prev", "=", "cell_output", "\n", "with", "tf", ".", "variable_scope", "(", "tf", ".", "get_variable_scope", "(", ")", ",", "reuse", "=", "False", ")", ":", "\n", "                ", "output", "=", "fully_connected", "(", "cell_output", ",", "input_size", ",", "activation_fn", "=", "tf", ".", "sigmoid", ")", "\n", "outputs", ".", "append", "(", "output", ")", "\n", "\n", "", "", "", "outputs", "=", "tf", ".", "stack", "(", "outputs", ",", "1", ")", "\n", "return", "outputs", ",", "states_list", "\n", "\n"]], "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.environment.gym_environment.GymEnvironment.get_action_shape": [[16, 18], ["len"], "methods", ["None"], ["def", "get_action_shape", "(", "self", ")", ":", "\n", "\t\t", "return", "[", "(", "1", ",", "len", "(", "self", ".", "actions_set", ")", ")", "]", "# take 1 action of n possible types", "\n", "\n"]], "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.environment.gym_environment.GymEnvironment.get_state_shape": [[19, 23], ["None"], "methods", ["None"], ["", "def", "get_state_shape", "(", "self", ")", ":", "\n", "\t\t", "if", "self", ".", "use_ram", ":", "\n", "\t\t\t", "return", "[", "(", "128", ",", "1", ",", "self", ".", "frames_per_state", ")", "]", "\n", "", "return", "[", "(", "42", ",", "42", ",", "self", ".", "frames_per_state", ")", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.environment.gym_environment.GymEnvironment.__init__": [[24, 62], ["gym.make", "collections.deque", "list", "gym_environment.GymEnvironment.get_state_shape", "range"], "methods", ["home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.environment.environment.Environment.get_state_shape"], ["", "def", "__init__", "(", "self", ",", "id", ",", "environment_name", ")", ":", "\n", "\t\t", "self", ".", "id", "=", "id", "\n", "# setup environment", "\n", "self", ".", "__game", "=", "gym", ".", "make", "(", "environment_name", ")", "\n", "# collect minimal action set", "\n", "if", "\"Montezuma\"", "in", "environment_name", ":", "\n", "\t\t\t", "self", ".", "actions_set", "=", "[", "1", ",", "2", ",", "3", ",", "4", ",", "5", ",", "11", ",", "12", "]", "\n", "", "else", ":", "\n", "\t\t\t", "self", ".", "actions_set", "=", "list", "(", "range", "(", "self", ".", "__game", ".", "action_space", ".", "n", ")", ")", "\n", "#=======================================================================", "\n", "# For atari games:", "\n", "# ACTION_MEANING = {", "\n", "#     0 : \"NOOP\",", "\n", "#     1 : \"FIRE\",", "\n", "#     2 : \"UP\",", "\n", "#     3 : \"RIGHT\",", "\n", "#     4 : \"LEFT\",", "\n", "#     5 : \"DOWN\",", "\n", "#     6 : \"UPRIGHT\",", "\n", "#     7 : \"UPLEFT\",", "\n", "#     8 : \"DOWNRIGHT\",", "\n", "#     9 : \"DOWNLEFT\",", "\n", "#     10 : \"UPFIRE\",", "\n", "#     11 : \"RIGHTFIRE\",", "\n", "#     12 : \"LEFTFIRE\",", "\n", "#     13 : \"DOWNFIRE\",", "\n", "#     14 : \"UPRIGHTFIRE\",", "\n", "#     15 : \"UPLEFTFIRE\",", "\n", "#     16 : \"DOWNRIGHTFIRE\",", "\n", "#     17 : \"DOWNLEFTFIRE\",", "\n", "# }", "\n", "#=======================================================================", "\n", "# evaluator stuff", "\n", "", "self", ".", "use_ram", "=", "\"-ram\"", "in", "environment_name", "\n", "# observation stack", "\n", "self", ".", "__observation_stack", "=", "deque", "(", "maxlen", "=", "self", ".", "frames_per_state", ")", "\n", "self", ".", "__state_shape", "=", "self", ".", "get_state_shape", "(", ")", "[", "0", "]", "\n", "self", ".", "__frame_shape", "=", "self", ".", "__state_shape", "[", ":", "-", "1", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.environment.gym_environment.GymEnvironment.reset": [[63, 78], ["gym_environment.GymEnvironment.stop", "gym_environment.GymEnvironment.__game.reset", "gym_environment.GymEnvironment.normalize", "gym_environment.GymEnvironment.__observation_stack.clear", "range", "gym_environment.GymEnvironment.get_state_from_observation_stack", "gym_environment.GymEnvironment.__observation_stack.append"], "methods", ["home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.worker.working_group.Group.stop", "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.environment.environment.Environment.reset", "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.environment.gym_environment.GymEnvironment.normalize", "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.worker.batch.CompositeBatch.clear", "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.environment.gym_environment.GymEnvironment.get_state_from_observation_stack", "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.worker.batch.ExperienceBatch.append"], ["", "def", "reset", "(", "self", ",", "data_id", "=", "None", ")", ":", "\n", "\t\t", "self", ".", "stop", "(", ")", "\n", "observation", "=", "self", ".", "__game", ".", "reset", "(", ")", "\n", "observation", "=", "self", ".", "normalize", "(", "observation", ")", "\n", "self", ".", "__observation_stack", ".", "clear", "(", ")", "\n", "for", "_", "in", "range", "(", "self", ".", "frames_per_state", ")", ":", "\n", "\t\t\t", "self", ".", "__observation_stack", ".", "append", "(", "observation", ")", "\n", "", "self", ".", "last_state", "=", "self", ".", "get_state_from_observation_stack", "(", ")", "\n", "self", ".", "last_action", "=", "None", "\n", "self", ".", "last_reward", "=", "0", "\n", "#=======================================================================", "\n", "# self.last_lives = -1", "\n", "#=======================================================================", "\n", "self", ".", "step", "=", "0", "\n", "return", "self", ".", "last_state", "\n", "\n"]], "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.environment.gym_environment.GymEnvironment.normalize": [[79, 88], ["cv2.cvtColor", "cv2.resize", "numpy.reshape"], "methods", ["None"], ["", "def", "normalize", "(", "self", ",", "observation", ")", ":", "\n", "\t\t", "if", "not", "self", ".", "use_ram", ":", "\n", "# RGB to Gray", "\n", "\t\t\t", "observation", "=", "cv2", ".", "cvtColor", "(", "observation", ",", "cv2", ".", "COLOR_RGB2GRAY", ")", "\n", "# Resize image", "\n", "observation", "=", "cv2", ".", "resize", "(", "src", "=", "observation", ",", "dsize", "=", "self", ".", "__frame_shape", ",", "interpolation", "=", "cv2", ".", "INTER_AREA", ")", "\n", "", "else", ":", "\n", "\t\t\t", "observation", "=", "np", ".", "reshape", "(", "observation", ",", "self", ".", "__frame_shape", ")", "\n", "", "return", "observation", "\n", "\n"]], "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.environment.gym_environment.GymEnvironment.get_state_from_observation_stack": [[89, 91], ["numpy.reshape"], "methods", ["None"], ["", "def", "get_state_from_observation_stack", "(", "self", ")", ":", "\n", "\t\t", "return", "[", "np", ".", "reshape", "(", "self", ".", "__observation_stack", ",", "self", ".", "__state_shape", ")", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.environment.gym_environment.GymEnvironment.stop": [[92, 94], ["gym_environment.GymEnvironment.__game.close"], "methods", ["None"], ["", "def", "stop", "(", "self", ")", ":", "\n", "\t\t", "self", ".", "__game", ".", "close", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.environment.gym_environment.GymEnvironment.get_screen": [[95, 97], ["gym_environment.GymEnvironment.__game.render"], "methods", ["None"], ["", "def", "get_screen", "(", "self", ")", ":", "\n", "\t\t", "return", "{", "'RGB'", ":", "self", ".", "__game", ".", "render", "(", "'rgb_array'", ")", "}", "\n", "\n"]], "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.environment.gym_environment.GymEnvironment.process": [[98, 120], ["gym_environment.GymEnvironment.__game.step", "gym_environment.GymEnvironment.normalize", "gym_environment.GymEnvironment.__observation_stack.append", "gym_environment.GymEnvironment.get_state_from_observation_stack"], "methods", ["home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.environment.gym_environment.GymEnvironment.normalize", "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.worker.batch.ExperienceBatch.append", "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.environment.gym_environment.GymEnvironment.get_state_from_observation_stack"], ["", "def", "process", "(", "self", ",", "action_vector", ")", ":", "\n", "\t\t", "observation", ",", "reward", ",", "is_terminal", ",", "info", "=", "self", ".", "__game", ".", "step", "(", "self", ".", "actions_set", "[", "action_vector", "[", "0", "]", "]", ")", "\n", "# build state from observation", "\n", "observation", "=", "self", ".", "normalize", "(", "observation", ")", "\n", "self", ".", "__observation_stack", ".", "append", "(", "observation", ")", "\n", "state", "=", "self", ".", "get_state_from_observation_stack", "(", ")", "\n", "# store last results", "\n", "self", ".", "last_state", "=", "state", "\n", "self", ".", "last_action", "=", "action_vector", "\n", "self", ".", "last_reward", "=", "reward", "\n", "# complete step", "\n", "self", ".", "step", "+=", "1", "\n", "#=======================================================================", "\n", "# lives = info[\"ale.lives\"]", "\n", "# if lives < self.last_lives:", "\n", "# \tis_terminal = True", "\n", "# self.last_lives = lives", "\n", "#=======================================================================", "\n", "# Check steps constraints, cannot exceed given limit", "\n", "if", "self", ".", "step", ">", "self", ".", "max_step", ":", "\n", "\t\t\t", "is_terminal", "=", "True", "\n", "", "return", "state", ",", "reward", ",", "is_terminal", ",", "None", "\n", "", "", ""]], "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.environment.environment.Environment.create_environment": [[10, 27], ["rogue_environment.RogueEnvironment", "multiple_protein_alignment_environment.MSAEnvironment", "car_controller_environment.CarControllerEnvironment", "sentipolc_environment.SentiPolcEnvironment", "gym_environment.GymEnvironment"], "methods", ["None"], ["@", "staticmethod", "\n", "def", "create_environment", "(", "env_type", ",", "id", "=", "0", ",", "training", "=", "True", ")", ":", "\n", "\t\t", "if", "env_type", "==", "'rogue'", ":", "\n", "\t\t\t", "from", ".", "import", "rogue_environment", "\n", "return", "rogue_environment", ".", "RogueEnvironment", "(", "id", ")", "\n", "", "elif", "env_type", "==", "'MultipleProteinAlignment'", ":", "\n", "\t\t\t", "from", ".", "import", "multiple_protein_alignment_environment", "\n", "return", "multiple_protein_alignment_environment", ".", "MSAEnvironment", "(", "id", ",", "training", ")", "\n", "", "elif", "env_type", "==", "'car_controller'", ":", "\n", "\t\t\t", "from", ".", "import", "car_controller_environment", "\n", "return", "car_controller_environment", ".", "CarControllerEnvironment", "(", "id", ")", "\n", "", "elif", "env_type", "==", "'sentipolc'", ":", "\n", "\t\t\t", "from", ".", "import", "sentipolc_environment", "\n", "return", "sentipolc_environment", ".", "SentiPolcEnvironment", "(", "id", ",", "training", ")", "\n", "", "else", ":", "\n", "\t\t\t", "from", ".", "import", "gym_environment", "\n", "return", "gym_environment", ".", "GymEnvironment", "(", "id", ",", "env_type", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.environment.environment.Environment.get_concatenation_size": [[28, 30], ["sum", "map", "environment.Environment.get_action_shape"], "methods", ["home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.utils.segment_tree.SumSegmentTree.sum", "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.environment.gym_environment.GymEnvironment.get_action_shape"], ["", "", "def", "get_concatenation_size", "(", "self", ")", ":", "\n", "\t\t", "return", "sum", "(", "map", "(", "lambda", "x", ":", "x", "[", "0", "]", ",", "self", ".", "get_action_shape", "(", ")", ")", ")", "+", "1", "\n", "\n"]], "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.environment.environment.Environment.get_concatenation": [[32, 37], ["numpy.concatenate", "numpy.concatenate", "numpy.zeros", "environment.Environment.get_concatenation_size", "numpy.reshape"], "methods", ["home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.environment.environment.Environment.get_concatenation_size"], ["", "def", "get_concatenation", "(", "self", ")", ":", "\n", "\t\t", "if", "self", ".", "last_action", "is", "None", ":", "\n", "\t\t\t", "return", "np", ".", "zeros", "(", "self", ".", "get_concatenation_size", "(", ")", ")", "\n", "", "flatten_action", "=", "np", ".", "concatenate", "(", "[", "np", ".", "reshape", "(", "a", ",", "-", "1", ")", "for", "a", "in", "self", ".", "last_action", "]", ",", "-", "1", ")", "\n", "return", "np", ".", "concatenate", "(", "(", "flatten_action", ",", "[", "self", ".", "last_reward", "]", ")", ",", "-", "1", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.environment.environment.Environment.process": [[38, 40], ["None"], "methods", ["None"], ["", "def", "process", "(", "self", ",", "action", ")", ":", "\n", "\t\t", "pass", "\n", "\n"]], "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.environment.environment.Environment.reset": [[41, 43], ["None"], "methods", ["None"], ["", "def", "reset", "(", "self", ",", "data_id", "=", "None", ")", ":", "\n", "\t\t", "pass", "\n", "\n"]], "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.environment.environment.Environment.stop": [[44, 46], ["None"], "methods", ["None"], ["", "def", "stop", "(", "self", ")", ":", "\n", "\t\t", "pass", "\n", "\n"]], "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.environment.environment.Environment.sample_random_action": [[47, 61], ["environment.Environment.get_action_shape", "len", "result.append", "result.append", "numpy.random.rand", "range", "random.random"], "methods", ["home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.environment.gym_environment.GymEnvironment.get_action_shape", "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.worker.batch.ExperienceBatch.append", "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.worker.batch.ExperienceBatch.append"], ["", "def", "sample_random_action", "(", "self", ")", ":", "\n", "\t\t", "result", "=", "[", "]", "\n", "for", "action_shape", "in", "self", ".", "get_action_shape", "(", ")", ":", "\n", "\t\t\t", "if", "len", "(", "action_shape", ")", ">", "1", ":", "\n", "\t\t\t\t", "count", ",", "size", "=", "action_shape", "\n", "", "else", ":", "\n", "\t\t\t\t", "count", "=", "action_shape", "[", "0", "]", "\n", "size", "=", "0", "\n", "", "if", "size", ">", "0", ":", "# categorical sampling", "\n", "\t\t\t\t", "samples", "=", "(", "np", ".", "random", ".", "rand", "(", "count", ")", "*", "size", ")", ".", "astype", "(", "np", ".", "uint8", ")", "\n", "result", ".", "append", "(", "samples", "if", "count", ">", "1", "else", "samples", "[", "0", "]", ")", "\n", "", "else", ":", "# uniform sampling", "\n", "\t\t\t\t", "result", ".", "append", "(", "[", "2", "*", "random", ".", "random", "(", ")", "-", "1", "for", "_", "in", "range", "(", "count", ")", "]", ")", "\n", "", "", "return", "result", "\n", "\n"]], "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.environment.environment.Environment.get_state_shape": [[62, 64], ["None"], "methods", ["None"], ["", "def", "get_state_shape", "(", "self", ")", ":", "\n", "\t\t", "pass", "\n", "\n"]], "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.environment.environment.Environment.get_test_result": [[65, 67], ["None"], "methods", ["None"], ["", "def", "get_test_result", "(", "self", ")", ":", "\n", "\t\t", "return", "None", "\n", "\n"]], "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.environment.environment.Environment.get_dataset_size": [[68, 70], ["None"], "methods", ["None"], ["", "def", "get_dataset_size", "(", "self", ")", ":", "\n", "\t\t", "return", "flags", ".", "episode_count_for_evaluation", "\n", "\n"]], "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.environment.environment.Environment.evaluate_test_results": [[71, 73], ["None"], "methods", ["None"], ["", "def", "evaluate_test_results", "(", "self", ",", "test_result_file", ")", ":", "\n", "\t\t", "pass", "\n", "\n"]], "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.environment.environment.Environment.get_screen_shape": [[74, 76], ["environment.Environment.get_state_shape"], "methods", ["home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.environment.environment.Environment.get_state_shape"], ["", "def", "get_screen_shape", "(", "self", ")", ":", "\n", "\t\t", "return", "self", ".", "get_state_shape", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.environment.environment.Environment.get_info": [[77, 79], ["None"], "methods", ["None"], ["", "def", "get_info", "(", "self", ")", ":", "\n", "\t\t", "return", "None", "\n", "\n"]], "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.environment.environment.Environment.get_screen": [[80, 82], ["None"], "methods", ["None"], ["", "def", "get_screen", "(", "self", ")", ":", "\n", "\t\t", "return", "None", "\n", "\n"]], "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.environment.environment.Environment.get_statistics": [[83, 85], ["None"], "methods", ["None"], ["", "def", "get_statistics", "(", "self", ")", ":", "\n", "\t\t", "return", "{", "}", "\n", "\n"]], "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.environment.environment.Environment.has_masked_actions": [[86, 88], ["None"], "methods", ["None"], ["", "def", "has_masked_actions", "(", "self", ")", ":", "\n", "\t\t", "return", "False", "\n", "\n"]], "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.agent.server.Application.__init__": [[88, 117], ["logging.getLogger", "logging.FileHandler", "logging.FileHandler.setFormatter", "server.Application.training_logger.addHandler", "server.Application.training_logger.setLevel", "utils.misc.get_cpu_count", "utils.misc.get_cpu_count", "utils.gpu_count", "utils.gpu_count", "print", "print", "max", "psutil.Process", "collections.deque", "time.time", "server.Application.build_network", "os.path.isdir", "os.mkdir", "logging.Formatter", "min", "os.getpid", "threading.Thread", "threading.Thread.start"], "methods", ["home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.utils.misc.get_cpu_count", "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.utils.misc.get_cpu_count", "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.utils.tensorflow_utils.gpu_count", "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.utils.tensorflow_utils.gpu_count", "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.utils.segment_tree.MaxSegmentTree.max", "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.algorithm.ac_algorithm.AC_Algorithm.build_network", "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.utils.segment_tree.MinSegmentTree.min"], ["\t", "def", "__init__", "(", "self", ")", ":", "\n", "\t\t", "if", "not", "os", ".", "path", ".", "isdir", "(", "flags", ".", "log_dir", ")", ":", "\n", "\t\t\t", "os", ".", "mkdir", "(", "flags", ".", "log_dir", ")", "\n", "", "self", ".", "train_logfile", "=", "flags", ".", "log_dir", "+", "'/train_results.log'", "\n", "# Training logger", "\n", "self", ".", "training_logger", "=", "logging", ".", "getLogger", "(", "'results'", ")", "\n", "hdlr", "=", "logging", ".", "FileHandler", "(", "self", ".", "train_logfile", ")", "\n", "hdlr", ".", "setFormatter", "(", "logging", ".", "Formatter", "(", "'%(asctime)s %(message)s'", ")", ")", "\n", "self", ".", "training_logger", ".", "addHandler", "(", "hdlr", ")", "\n", "self", ".", "training_logger", ".", "setLevel", "(", "logging", ".", "DEBUG", ")", "\n", "# Initialize network", "\n", "self", ".", "num_cpu", "=", "get_cpu_count", "(", ")", "\n", "self", ".", "num_gpu", "=", "tf_utils", ".", "gpu_count", "(", ")", "\n", "print", "(", "\"Available CPUs: {}\"", ".", "format", "(", "self", ".", "num_cpu", ")", ")", "\n", "print", "(", "\"Available GPUs: {}\"", ".", "format", "(", "self", ".", "num_gpu", ")", ")", "\n", "self", ".", "thread_count", "=", "min", "(", "flags", ".", "environment_count", ",", "flags", ".", "threads_per_cpu", "*", "self", ".", "num_cpu", ")", "if", "flags", ".", "threads_per_cpu", ">", "0", "else", "1", "\n", "self", ".", "env_per_worker", "=", "max", "(", "1", ",", "flags", ".", "environment_count", "//", "self", ".", "thread_count", ")", "\n", "self", ".", "is_alive", "=", "True", "\n", "self", ".", "terminate_requested", "=", "False", "\n", "self", ".", "process", "=", "psutil", ".", "Process", "(", "os", ".", "getpid", "(", ")", ")", "\n", "self", ".", "checkpoint_list", "=", "deque", "(", ")", "\n", "self", ".", "performance_timer", "=", "None", "\n", "# Set start time", "\n", "self", ".", "start_time", "=", "time", ".", "time", "(", ")", "\n", "# Build network", "\n", "self", ".", "build_network", "(", ")", "\n", "if", "flags", ".", "monitor_memory_usage", ":", "\n", "\t\t\t", "monitor_thread", "=", "Thread", "(", "target", "=", "memory_monitor", ")", "\n", "monitor_thread", ".", "start", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.agent.server.Application.create_session": [[118, 137], ["print", "tensorflow.GPUOptions", "tensorflow.ConfigProto", "tensorflow.InteractiveSession"], "methods", ["None"], ["", "", "def", "create_session", "(", "self", ")", ":", "\n", "\t\t", "print", "(", "'Creating new tensorflow session..'", ")", "\n", "# GPU options", "\n", "gpu_options", "=", "tf", ".", "GPUOptions", "(", "\n", "allow_growth", "=", "True", ",", "\n", "# per_process_gpu_memory_fraction=1/self.num_cpu,", "\n", ")", "\n", "# Config proto", "\n", "config_proto", "=", "tf", ".", "ConfigProto", "(", "\n", "# https://www.tensorflow.org/guide/performance/overview", "\n", "intra_op_parallelism_threads", "=", "self", ".", "num_cpu", ",", "\n", "inter_op_parallelism_threads", "=", "self", ".", "num_cpu", ",", "\n", "allow_soft_placement", "=", "True", ",", "\n", "gpu_options", "=", "gpu_options", ",", "\n", "#===================================================================", "\n", "# log_device_placement=True,", "\n", "#===================================================================", "\n", ")", "\n", "return", "tf", ".", "InteractiveSession", "(", "config", "=", "config_proto", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.agent.server.Application.build_global_network": [[138, 142], ["tensorflow.get_collection", "agent.worker.working_group.Group"], "methods", ["None"], ["", "def", "build_global_network", "(", "self", ")", ":", "\n", "\t\t", "global_network", "=", "Group", "(", "group_id", "=", "0", ",", "environment_count", "=", "0", ",", "global_network", "=", "None", ",", "training", "=", "True", ")", ".", "network_manager", "\n", "variables_to_save", "=", "tf", ".", "get_collection", "(", "tf", ".", "GraphKeys", ".", "GLOBAL_VARIABLES", ")", "\n", "return", "global_network", ",", "variables_to_save", "\n", "\n"]], "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.agent.server.Application.build_network": [[143, 165], ["server.Application.create_session", "server.Application.build_global_network", "utils.statistics.IndexedStatistics", "utils.statistics.IndexedStatistics", "tensorflow.train.Saver", "server.Application.load_checkpoint", "tensorflow.summary.FileWriter().close", "agent.worker.working_group.Group", "range", "tensorflow.summary.FileWriter"], "methods", ["home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.agent.server.Application.create_session", "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.agent.server.Application.build_global_network", "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.agent.server.Application.load_checkpoint"], ["", "def", "build_network", "(", "self", ")", ":", "\n", "# Create session", "\n", "\t\t", "tf_session", "=", "self", ".", "create_session", "(", ")", "\n", "# Build global network", "\n", "self", ".", "global_network", ",", "variables_to_save", "=", "self", ".", "build_global_network", "(", ")", "\n", "# Build local networks", "\n", "self", ".", "working_groups", "=", "[", "\n", "Group", "(", "group_id", "=", "c", "+", "1", ",", "environment_count", "=", "self", ".", "env_per_worker", ",", "global_network", "=", "self", ".", "global_network", ",", "training", "=", "True", ")", "\n", "for", "c", "in", "range", "(", "self", ".", "thread_count", ")", "\n", "]", "\n", "# Statistics", "\n", "self", ".", "training_statistics", "=", "IndexedStatistics", "(", "max_count", "=", "self", ".", "thread_count", ",", "buffer_must_be_full", "=", "True", ")", "\n", "# Load checkpoint", "\n", "self", ".", "saver", "=", "tf", ".", "train", ".", "Saver", "(", "\n", "var_list", "=", "variables_to_save", ",", "max_to_keep", "=", "flags", ".", "max_checkpoint_to_keep", ",", "\n", "restore_sequentially", "=", "True", "# Causes restore of different variables to happen sequentially within each device. This can lower memory usage when restoring very large models.", "\n", ")", "\n", "self", ".", "global_step", ",", "self", ".", "start_elapsed_time", "=", "self", ".", "load_checkpoint", "(", "self", ".", "saver", ",", "tf_session", ")", "\n", "self", ".", "last_global_step", "=", "self", ".", "global_step", "\n", "self", ".", "next_save_steps", "=", "self", ".", "global_step", "+", "flags", ".", "save_interval_step", "\n", "# Print graph summary", "\n", "tf", ".", "summary", ".", "FileWriter", "(", "'summary'", ",", "tf_session", ".", "graph", ")", ".", "close", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.agent.server.Application.train": [[166, 201], ["tensorflow.get_default_session", "threading.RLock", "threading.Thread", "t.start", "server.Application.print_performance", "t.join", "server.Application.save_checkpoint", "server.Application.train", "server.Application.performance_timer.cancel", "range", "server.Application.test", "traceback.print_exc"], "methods", ["home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.agent.server.Application.print_performance", "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.agent.server.Application.save_checkpoint", "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.agent.server.train", "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.agent.server.Application.test"], ["", "def", "train", "(", "self", ",", "initialize", "=", "True", ")", ":", "\n", "# Initialize threads", "\n", "\t\t", "if", "initialize", ":", "\n", "\t\t\t", "self", ".", "step_lock", "=", "RLock", "(", ")", "# The standard Lock doesn\u2019t know which thread is currently holding the lock. If the lock is held, any thread that attempts to acquire it will block, even if the same thread itself is already holding the lock.", "\n", "", "tf_session", "=", "tf", ".", "get_default_session", "(", ")", "\n", "self", ".", "train_threads", "=", "[", "Thread", "(", "target", "=", "self", ".", "train_function", ",", "args", "=", "(", "i", ",", "tf_session", ")", ")", "for", "i", "in", "range", "(", "self", ".", "thread_count", ")", "]", "\n", "# Run training threads", "\n", "for", "t", "in", "self", ".", "train_threads", ":", "\n", "\t\t\t", "t", ".", "start", "(", ")", "\n", "# Init", "\n", "", "if", "initialize", ":", "\n", "#===================================================================", "\n", "# # Set signal handler", "\n", "# signal.signal(signal.SIGINT, self.signal_handler)", "\n", "# print('Press Ctrl+C to stop')", "\n", "#===================================================================", "\n", "\t\t\t", "self", ".", "print_performance", "(", ")", "\n", "# Wait for all threads to stop", "\n", "", "for", "t", "in", "self", ".", "train_threads", ":", "\n", "\t\t\t", "t", ".", "join", "(", ")", "\n", "# Save checkpoint", "\n", "", "try", ":", "\n", "\t\t\t", "self", ".", "save_checkpoint", "(", "self", ".", "global_step", ",", "self", ".", "saver", ",", "tf_session", ")", "\n", "# Test", "\n", "if", "flags", ".", "test_after_saving", ":", "\n", "\t\t\t\t", "self", ".", "test", "(", ")", "\n", "", "", "except", ":", "\n", "\t\t\t", "traceback", ".", "print_exc", "(", ")", "\n", "# Restart workers", "\n", "", "if", "self", ".", "is_alive", "and", "not", "flags", ".", "rebuild_network_after_checkpoint_is_saved", ":", "\n", "\t\t\t", "self", ".", "next_save_steps", "+=", "flags", ".", "save_interval_step", "\n", "return", "self", ".", "train", "(", "initialize", "=", "False", ")", "\n", "", "if", "self", ".", "performance_timer", "is", "not", "None", ":", "\n", "\t\t\t", "self", ".", "performance_timer", ".", "cancel", "(", ")", "\n", "", "return", "self", ".", "is_alive", "\n", "\n"]], "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.agent.server.Application.train_function": [[202, 237], ["group.initialize_environments", "session.as_default", "group.process", "traceback.print_exc", "group.stop", "server.Application.training_statistics.set", "server.Application.training_statistics.get", "group.get_statistics", "server.Application.training_logger.info", "sorted", "server.Application.items"], "methods", ["home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.worker.working_group.Group.initialize_environments", "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.worker.working_group.Group.process", "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.worker.working_group.Group.stop", "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.buffer.pseudo_prioritized_buffer.PseudoPrioritizedBuffer.set", "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.worker.batch.CompositeBatch.get", "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.worker.working_group.Group.get_statistics"], ["", "def", "train_function", "(", "self", ",", "parallel_index", ",", "session", ")", ":", "\n", "\t\t", "\"\"\" Train each environment. \"\"\"", "\n", "group", "=", "self", ".", "working_groups", "[", "parallel_index", "]", "\n", "\n", "if", "self", ".", "global_step", "==", "0", ":", "\n", "\t\t\t", "group", ".", "initialize_environments", "(", "step_count", "=", "flags", ".", "timesteps_before_starting_training", ")", "\n", "\n", "", "while", "True", ":", "\n", "# Work", "\n", "\t\t\t", "try", ":", "\n", "# Setup default session", "\n", "\t\t\t\t", "with", "session", ".", "as_default", "(", ")", ":", "\n", "# Process group", "\n", "\t\t\t\t\t", "thread_steps", "=", "group", ".", "process", "(", "global_step", "=", "self", ".", "global_step", ",", "batch", "=", "True", ")", "\n", "# Update shared memory, lock", "\n", "", "with", "self", ".", "step_lock", ":", "\n", "\t\t\t\t\t", "self", ".", "global_step", "+=", "thread_steps", "\n", "# Print global statistics, after training if needed # Ignore the initial logs, because they are too noisy", "\n", "if", "group", ".", "has_terminal_worker", "and", "group", ".", "terminated_episodes", ">", "flags", ".", "episode_count_for_evaluation", ":", "\n", "\t\t\t\t\t\t", "self", ".", "training_statistics", ".", "set", "(", "group", ".", "get_statistics", "(", ")", ",", "parallel_index", ")", "\n", "info", "=", "self", ".", "training_statistics", ".", "get", "(", ")", "\n", "if", "info", ":", "\n", "# Print statistics", "\n", "\t\t\t\t\t\t\t", "self", ".", "training_logger", ".", "info", "(", "\"<{}> {}\"", ".", "format", "(", "self", ".", "global_step", ",", "[", "\"{}={}\"", ".", "format", "(", "key", ",", "value", ")", "for", "key", ",", "value", "in", "sorted", "(", "info", ".", "items", "(", ")", ",", "key", "=", "lambda", "t", ":", "t", "[", "0", "]", ")", "]", ")", ")", "\n", "", "", "", "", "except", ":", "\n", "\t\t\t\t", "traceback", ".", "print_exc", "(", ")", "\n", "# Check whether training is still alive", "\n", "", "if", "self", ".", "global_step", ">=", "flags", ".", "max_timestep", "or", "self", ".", "terminate_requested", ":", "\n", "\t\t\t\t", "self", ".", "is_alive", "=", "False", "\n", "for", "group", "in", "trainer_list", ":", "\n", "\t\t\t\t\t", "group", ".", "stop", "(", ")", "\n", "", "return", "\n", "# Save checkpoint", "\n", "", "if", "self", ".", "global_step", ">=", "self", ".", "next_save_steps", ":", "\n", "\t\t\t\t", "return", "\n", "\n"]], "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.agent.server.Application.test": [[238, 287], ["os.path.exists", "threading.RLock", "print", "tensorflow.get_default_session", "environment.environment.Environment.create_environment", "environment.environment.Environment.create_environment.get_dataset_size", "max", "range", "print", "print", "time.sleep", "print", "utils.statistics.Statistics", "utils.statistics.Statistics", "utils.statistics.Statistics.get", "utils.statistics.Statistics.get", "print", "print", "environment.environment.Environment.create_environment.evaluate_test_results", "print", "agent.worker.working_group.Group", "threading.Thread", "threading.Thread.start", "threads.append", "testers.append", "threading.Thread.join", "utils.statistics.Statistics.add", "utils.statistics.Statistics.add", "open", "file.write", "group.get_statistics", "sorted", "utils.statistics.Statistics.get.items"], "methods", ["home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.environment.environment.Environment.create_environment", "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.environment.environment.Environment.get_dataset_size", "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.utils.segment_tree.MaxSegmentTree.max", "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.worker.batch.CompositeBatch.get", "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.worker.batch.CompositeBatch.get", "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.environment.environment.Environment.evaluate_test_results", "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.worker.batch.ExperienceBatch.append", "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.worker.batch.ExperienceBatch.append", "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.worker.batch.CompositeBatch.add", "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.worker.batch.CompositeBatch.add", "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.worker.working_group.Group.get_statistics"], ["", "", "", "def", "test", "(", "self", ")", ":", "\n", "\t\t", "result_file", "=", "'{}/test_results_{}.log'", ".", "format", "(", "flags", ".", "log_dir", ",", "self", ".", "global_step", ")", "\n", "if", "os", ".", "path", ".", "exists", "(", "result_file", ")", ":", "\n", "\t\t\t", "print", "(", "'Test results already produced and evaluated for {}'", ".", "format", "(", "result_file", ")", ")", "\n", "return", "\n", "", "result_lock", "=", "RLock", "(", ")", "\n", "\n", "print", "(", "'Start testing'", ")", "\n", "testers", "=", "[", "]", "\n", "threads", "=", "[", "]", "\n", "tf_session", "=", "tf", ".", "get_default_session", "(", ")", "\n", "tmp_environment", "=", "Environment", ".", "create_environment", "(", "env_type", "=", "flags", ".", "env_type", ",", "training", "=", "False", ")", "\n", "dataset_size", "=", "tmp_environment", ".", "get_dataset_size", "(", ")", "\n", "data_per_thread", "=", "max", "(", "1", ",", "dataset_size", "//", "self", ".", "thread_count", ")", "\n", "for", "i", "in", "range", "(", "self", ".", "thread_count", ")", ":", "# parallel testing", "\n", "\t\t\t", "tester", "=", "Group", "(", "group_id", "=", "-", "(", "i", "+", "1", ")", ",", "environment_count", "=", "data_per_thread", ",", "global_network", "=", "self", ".", "global_network", ",", "training", "=", "False", ")", "\n", "data_range_start", "=", "i", "*", "data_per_thread", "\n", "data_range_end", "=", "data_range_start", "+", "data_per_thread", "\n", "# print(data_range_start, data_per_thread, dataset_size)", "\n", "thread", "=", "Thread", "(", "\n", "target", "=", "self", ".", "test_function", ",", "\n", "args", "=", "(", "\n", "result_file", ",", "result_lock", ",", "\n", "tester", ",", "\n", "(", "data_range_start", ",", "data_range_end", ")", ",", "\n", "tf_session", "\n", ")", "\n", ")", "\n", "thread", ".", "start", "(", ")", "\n", "threads", ".", "append", "(", "thread", ")", "\n", "testers", ".", "append", "(", "tester", ")", "\n", "", "print", "(", "'Test Set size:'", ",", "dataset_size", ")", "\n", "print", "(", "'Tests per thread:'", ",", "data_per_thread", ")", "\n", "time", ".", "sleep", "(", "5", ")", "\n", "for", "thread", "in", "threads", ":", "# wait for all threads to end", "\n", "\t\t\t", "thread", ".", "join", "(", ")", "\n", "", "print", "(", "'End testing'", ")", "\n", "# get overall statistics", "\n", "test_statistics", "=", "Statistics", "(", "self", ".", "thread_count", ")", "\n", "for", "group", "in", "testers", ":", "\n", "\t\t\t", "test_statistics", ".", "add", "(", "group", ".", "get_statistics", "(", ")", ")", "\n", "", "info", "=", "test_statistics", ".", "get", "(", ")", "\n", "# write results to file", "\n", "stats_file", "=", "'{}/test_statistics.log'", ".", "format", "(", "flags", ".", "log_dir", ")", "\n", "with", "open", "(", "stats_file", ",", "\"a\"", ",", "encoding", "=", "\"utf-8\"", ")", "as", "file", ":", "# write stats to file", "\n", "\t\t\t", "file", ".", "write", "(", "'{}\\n'", ".", "format", "(", "[", "\"{}={}\"", ".", "format", "(", "key", ",", "value", ")", "for", "key", ",", "value", "in", "sorted", "(", "info", ".", "items", "(", ")", ",", "key", "=", "lambda", "t", ":", "t", "[", "0", "]", ")", "]", ")", ")", "\n", "", "print", "(", "'Test statistics saved in {}'", ".", "format", "(", "stats_file", ")", ")", "\n", "print", "(", "'Test results saved in {}'", ".", "format", "(", "result_file", ")", ")", "\n", "return", "tmp_environment", ".", "evaluate_test_results", "(", "result_file", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.agent.server.Application.test_function": [[288, 298], ["session.as_default", "range", "tester.process", "str", "worker.environment.get_test_result", "open", "file.write"], "methods", ["home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.worker.working_group.Group.process", "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.environment.environment.Environment.get_test_result"], ["", "def", "test_function", "(", "self", ",", "result_file", ",", "result_lock", ",", "tester", ",", "data_range", ",", "session", ")", ":", "\n", "\t\t", "with", "session", ".", "as_default", "(", ")", ":", "\n", "\t\t\t", "for", "data_id", "in", "range", "(", "*", "data_range", ",", "tester", ".", "environment_count", ")", ":", "\n", "\t\t\t\t", "tester", ".", "process", "(", "batch", "=", "False", ",", "data_id", "=", "data_id", ")", "\n", "if", "flags", ".", "print_test_results", ":", "\n", "\t\t\t\t\t", "result_list", "=", "(", "str", "(", "worker", ".", "environment", ".", "get_test_result", "(", ")", ")", "for", "worker", "in", "tester", ".", "worker_list", ")", "\n", "result_string", "=", "'\\n'", ".", "join", "(", "result_list", ")", "\n", "with", "result_lock", ":", "\n", "\t\t\t\t\t\t", "with", "open", "(", "result_file", ",", "\"a\"", ",", "encoding", "=", "\"utf-8\"", ")", "as", "file", ":", "# write results to file", "\n", "\t\t\t\t\t\t\t", "file", ".", "write", "(", "'{}\\n'", ".", "format", "(", "result_string", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.agent.server.Application.print_performance": [[299, 313], ["server.Application.get_elapsed_time", "print", "sys.stdout.flush", "len", "print", "threading.Timer", "server.Application.performance_timer.start", "server.Application.process.memory_info", "len"], "methods", ["home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.agent.server.Application.get_elapsed_time"], ["", "", "", "", "", "", "def", "print_performance", "(", "self", ")", ":", "\n", "\t\t", "step_delta", "=", "self", ".", "global_step", "-", "self", ".", "last_global_step", "\n", "self", ".", "last_global_step", "=", "self", ".", "global_step", "\n", "ram_usage_in_gb", "=", "self", ".", "process", ".", "memory_info", "(", ")", ".", "rss", "/", "2", "**", "30", "\n", "elapsed_time", "=", "self", ".", "get_elapsed_time", "(", ")", "\n", "steps_per_sec_tot", "=", "self", ".", "global_step", "/", "elapsed_time", "\n", "steps_per_sec_now", "=", "step_delta", "/", "flags", ".", "seconds_to_wait_for_printing_performance", "\n", "print", "(", "\"### [Used {:.2f} GB] {} STEPS in {:.0f} sec. with {:.0f} STEPS/sec. and {:.0f} dSTEPS/sec.\"", ".", "format", "(", "ram_usage_in_gb", ",", "self", ".", "global_step", ",", "elapsed_time", ",", "steps_per_sec_tot", ",", "steps_per_sec_now", ")", ")", "\n", "if", "len", "(", "gc", ".", "garbage", ")", ">", "0", ":", "\n", "\t\t\t", "print", "(", "'Cannot collect {} garbage objects'", ".", "format", "(", "len", "(", "gc", ".", "garbage", ")", ")", ")", "\n", "", "sys", ".", "stdout", ".", "flush", "(", ")", "# force print immediately what is in output buffer", "\n", "if", "self", ".", "is_alive", ":", "\n", "\t\t\t", "self", ".", "performance_timer", "=", "Timer", "(", "flags", ".", "seconds_to_wait_for_printing_performance", ",", "self", ".", "print_performance", ")", "\n", "self", ".", "performance_timer", ".", "start", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.agent.server.Application.load_checkpoint": [[314, 340], ["tf_session.run", "tensorflow.train.get_checkpoint_state", "tensorflow.global_variables_initializer", "print", "saver.restore", "tensorflow.train.get_checkpoint_state.model_checkpoint_path.split", "int", "print", "server.Application.load_important_information", "print", "print", "open", "float", "f.read"], "methods", ["home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.agent.server.Application.load_important_information"], ["", "", "def", "load_checkpoint", "(", "self", ",", "saver", ",", "tf_session", ")", ":", "\n", "# Initialize network variables", "\n", "\t\t", "tf_session", ".", "run", "(", "tf", ".", "global_variables_initializer", "(", ")", ")", "# do it before loading checkpoint", "\n", "# Initialize or load checkpoint with saver", "\n", "checkpoint", "=", "tf", ".", "train", ".", "get_checkpoint_state", "(", "flags", ".", "checkpoint_dir", ")", "\n", "if", "checkpoint", "and", "checkpoint", ".", "model_checkpoint_path", ":", "\n", "\t\t\t", "print", "(", "\"Loading checkpoint\"", ",", "checkpoint", ".", "model_checkpoint_path", ")", "\n", "saver", ".", "restore", "(", "tf_session", ",", "checkpoint", ".", "model_checkpoint_path", ")", "\n", "tokens", "=", "checkpoint", ".", "model_checkpoint_path", ".", "split", "(", "\"-\"", ")", "\n", "# set global step", "\n", "global_step", "=", "int", "(", "tokens", "[", "1", "]", ")", "\n", "print", "(", "\">>> global step set: \"", ",", "global_step", ")", "\n", "# set wall time", "\n", "elapsed_time_fname", "=", "'{}/elapsed_time.{}'", ".", "format", "(", "flags", ".", "checkpoint_dir", ",", "global_step", ")", "\n", "with", "open", "(", "elapsed_time_fname", ",", "'r'", ")", "as", "f", ":", "\n", "\t\t\t\t", "start_elapsed_time", "=", "float", "(", "f", ".", "read", "(", ")", ")", "\n", "", "self", ".", "load_important_information", "(", "flags", ".", "checkpoint_dir", "+", "'/{}.pkl'", ".", "format", "(", "global_step", ")", ")", "\n", "print", "(", "\"Checkpoint loaded\"", ")", "\n", "", "else", ":", "\n", "\t\t\t", "print", "(", "\"Could not find old checkpoint\"", ")", "\n", "global_step", ",", "start_elapsed_time", "=", "0", ",", "0", "\n", "# Finalize graph", "\n", "#=======================================================================", "\n", "# tf_session.graph.finalize()", "\n", "#=======================================================================", "\n", "", "return", "global_step", ",", "start_elapsed_time", "\n", "\n"]], "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.agent.server.Application.save_checkpoint": [[341, 370], ["server.Application.checkpoint_list.append", "print", "saver.save", "server.Application.save_important_information", "print", "os.path.exists", "os.mkdir", "len", "server.Application.checkpoint_list.popleft", "os.remove", "os.remove", "open", "f.write", "utils.plot_files", "utils.plot_files", "str", "server.Application.get_elapsed_time"], "methods", ["home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.worker.batch.ExperienceBatch.append", "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.agent.server.Application.save_important_information", "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.utils.plots.plot_files", "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.utils.plots.plot_files", "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.agent.server.Application.get_elapsed_time"], ["", "def", "save_checkpoint", "(", "self", ",", "global_step", ",", "saver", ",", "tf_session", ")", ":", "\n", "# Create checkpoint directory", "\n", "\t\t", "if", "not", "os", ".", "path", ".", "exists", "(", "flags", ".", "checkpoint_dir", ")", ":", "\n", "\t\t\t", "os", ".", "mkdir", "(", "flags", ".", "checkpoint_dir", ")", "\n", "# Delete old checkpoints", "\n", "", "self", ".", "checkpoint_list", ".", "append", "(", "global_step", ")", "\n", "if", "len", "(", "self", ".", "checkpoint_list", ")", ">", "flags", ".", "max_checkpoint_to_keep", ":", "\n", "\t\t\t", "checkpoint_to_delete", "=", "self", ".", "checkpoint_list", ".", "popleft", "(", ")", "\n", "# Delete the old pickle files, the other checkpoint files are automatically deleted by tensorflow", "\n", "os", ".", "remove", "(", "'{}/{}.pkl'", ".", "format", "(", "flags", ".", "checkpoint_dir", ",", "checkpoint_to_delete", ")", ")", "\n", "# Delete the old wall time files", "\n", "os", ".", "remove", "(", "'{}/elapsed_time.{}'", ".", "format", "(", "flags", ".", "checkpoint_dir", ",", "checkpoint_to_delete", ")", ")", "\n", "# The other checkpoint files are automatically deleted by tensorflow", "\n", "# Write wall time", "\n", "", "elapsed_time_fname", "=", "'{}/elapsed_time.{}'", ".", "format", "(", "flags", ".", "checkpoint_dir", ",", "global_step", ")", "\n", "with", "open", "(", "elapsed_time_fname", ",", "'w'", ")", "as", "f", ":", "\n", "\t\t\t", "f", ".", "write", "(", "str", "(", "self", ".", "get_elapsed_time", "(", ")", ")", ")", "\n", "# Print plot", "\n", "", "if", "flags", ".", "compute_plot_when_saving", ":", "\n", "\t\t\t", "plt", ".", "plot_files", "(", "log_files", "=", "[", "self", ".", "train_logfile", "]", ",", "figure_file", "=", "flags", ".", "log_dir", "+", "'/train_plot.jpg'", ")", "\n", "# Save Checkpoint", "\n", "", "print", "(", "'Start saving..'", ")", "\n", "saver", ".", "save", "(", "\n", "sess", "=", "tf_session", ",", "\n", "save_path", "=", "'{}/checkpoint'", ".", "format", "(", "flags", ".", "checkpoint_dir", ")", ",", "\n", "global_step", "=", "global_step", "\n", ")", "\n", "self", ".", "save_important_information", "(", "'{}/{}.pkl'", ".", "format", "(", "flags", ".", "checkpoint_dir", ",", "global_step", ")", ")", "\n", "print", "(", "'Checkpoint saved in {}'", ".", "format", "(", "flags", ".", "checkpoint_dir", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.agent.server.Application.get_elapsed_time": [[371, 373], ["time.time"], "methods", ["None"], ["", "def", "get_elapsed_time", "(", "self", ")", ":", "\n", "\t\t", "return", "time", ".", "time", "(", ")", "-", "self", ".", "start_time", "+", "self", ".", "start_elapsed_time", "\n", "\n"]], "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.agent.server.Application.save_important_information": [[374, 380], ["gc.collect", "open", "pickle.Pickler().dump", "copy.deepcopy", "pickle.Pickler", "utils.important_information.ImportantInformation.get", "utils.important_information.ImportantInformation.get"], "methods", ["home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.worker.batch.CompositeBatch.get", "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.worker.batch.CompositeBatch.get"], ["", "def", "save_important_information", "(", "self", ",", "path", ")", ":", "\n", "# Write", "\n", "\t\t", "with", "open", "(", "path", ",", "'wb'", ")", "as", "file", ":", "\n", "\t\t\t", "Pickler", "(", "file", ",", "protocol", "=", "-", "1", ")", ".", "dump", "(", "deepcopy", "(", "ImportantInformation", ".", "get", "(", ")", ")", ")", "\n", "# Collect garbage", "\n", "", "gc", ".", "collect", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.agent.server.Application.load_important_information": [[381, 387], ["gc.collect", "open", "utils.important_information.ImportantInformation.set", "utils.important_information.ImportantInformation.set", "copy.deepcopy", "pickle.Unpickler().load", "pickle.Unpickler"], "methods", ["home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.buffer.pseudo_prioritized_buffer.PseudoPrioritizedBuffer.set", "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.buffer.pseudo_prioritized_buffer.PseudoPrioritizedBuffer.set", "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.utils.important_information.ImportantInformation.load"], ["", "def", "load_important_information", "(", "self", ",", "path", ")", ":", "\n", "# Read pickle file ", "\n", "\t\t", "with", "open", "(", "path", ",", "'rb'", ")", "as", "file", ":", "\n", "\t\t\t", "ImportantInformation", ".", "set", "(", "deepcopy", "(", "Unpickler", "(", "file", ")", ".", "load", "(", ")", ")", ")", "\n", "# Collect garbage", "\n", "", "gc", ".", "collect", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.agent.server.train": [[36, 45], ["multiprocessing.Queue", "multiprocessing.Process", "multiprocessing.Process.start", "multiprocessing.Process.join", "multiprocessing.Queue.empty", "multiprocessing.Queue.get", "server.train", "q.put", "server.Application.train", "server.Application"], "function", ["home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.worker.batch.CompositeBatch.get", "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.agent.server.train", "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.buffer.pseudo_prioritized_buffer.PseudoPrioritizedBuffer.put", "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.agent.server.train"], ["def", "train", "(", ")", ":", "\n", "\t", "result_queue", "=", "Queue", "(", ")", "\n", "p", "=", "Process", "(", "target", "=", "lambda", "q", ":", "q", ".", "put", "(", "Application", "(", ")", ".", "train", "(", ")", ")", ",", "args", "=", "(", "result_queue", ",", ")", ")", "\n", "p", ".", "start", "(", ")", "\n", "p", ".", "join", "(", ")", "\n", "if", "not", "result_queue", ".", "empty", "(", ")", ":", "\n", "\t\t", "is_alive", "=", "result_queue", ".", "get", "(", ")", "\n", "if", "is_alive", ":", "\n", "\t\t\t", "train", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.agent.server.display_top": [[46, 70], ["snapshot.filter_traces.filter_traces", "snapshot.filter_traces.statistics", "print", "enumerate", "sum", "print", "os.sep.join", "print", "linecache.getline().strip", "sum", "print", "tracemalloc.Filter", "tracemalloc.Filter", "print", "frame.filename.split", "linecache.getline", "len"], "function", ["home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.utils.segment_tree.SumSegmentTree.sum", "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.utils.segment_tree.SumSegmentTree.sum"], ["", "", "", "def", "display_top", "(", "snapshot", ",", "key_type", "=", "'lineno'", ",", "limit", "=", "3", ")", ":", "\n", "\t", "snapshot", "=", "snapshot", ".", "filter_traces", "(", "(", "\n", "tracemalloc", ".", "Filter", "(", "False", ",", "\"<frozen importlib._bootstrap>\"", ")", ",", "\n", "tracemalloc", ".", "Filter", "(", "False", ",", "\"<unknown>\"", ")", ",", "\n", ")", ")", "\n", "top_stats", "=", "snapshot", ".", "statistics", "(", "key_type", ")", "\n", "\n", "print", "(", "\"Top %s lines\"", "%", "limit", ")", "\n", "for", "index", ",", "stat", "in", "enumerate", "(", "top_stats", "[", ":", "limit", "]", ",", "1", ")", ":", "\n", "\t\t", "frame", "=", "stat", ".", "traceback", "[", "0", "]", "\n", "# replace \"/path/to/module/file.py\" with \"module/file.py\"", "\n", "filename", "=", "os", ".", "sep", ".", "join", "(", "frame", ".", "filename", ".", "split", "(", "os", ".", "sep", ")", "[", "-", "2", ":", "]", ")", "\n", "print", "(", "\"#%s: %s:%s: %.1f KiB\"", "\n", "%", "(", "index", ",", "filename", ",", "frame", ".", "lineno", ",", "stat", ".", "size", "/", "1024", ")", ")", "\n", "line", "=", "linecache", ".", "getline", "(", "frame", ".", "filename", ",", "frame", ".", "lineno", ")", ".", "strip", "(", ")", "\n", "if", "line", ":", "\n", "\t\t\t", "print", "(", "'\t%s'", "%", "line", ")", "\n", "\n", "", "", "other", "=", "top_stats", "[", "limit", ":", "]", "\n", "if", "other", ":", "\n", "\t\t", "size", "=", "sum", "(", "stat", ".", "size", "for", "stat", "in", "other", ")", "\n", "print", "(", "\"%s other: %.1f KiB\"", "%", "(", "len", "(", "other", ")", ",", "size", "/", "1024", ")", ")", "\n", "", "total", "=", "sum", "(", "stat", ".", "size", "for", "stat", "in", "top_stats", ")", "\n", "print", "(", "\"Total allocated size: %.1f KiB\"", "%", "(", "total", "/", "1024", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.agent.server.memory_monitor": [[71, 85], ["tracemalloc.start", "time.sleep", "resource.getrusage", "tracemalloc.take_snapshot", "print", "server.display_top", "traceback.print_exc", "datetime.datetime.now"], "function", ["home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.agent.server.display_top"], ["", "def", "memory_monitor", "(", ")", ":", "\n", "\t", "tracemalloc", ".", "start", "(", ")", "\n", "old_max", "=", "0", "\n", "while", "True", ":", "\n", "\t\t", "sleep", "(", "60", ")", "\n", "try", ":", "\n", "\t\t\t", "max_rss", "=", "getrusage", "(", "RUSAGE_SELF", ")", ".", "ru_maxrss", "\n", "if", "max_rss", ">", "old_max", ":", "\n", "\t\t\t\t", "old_max", "=", "max_rss", "\n", "snapshot", "=", "tracemalloc", ".", "take_snapshot", "(", ")", "\n", "print", "(", "datetime", ".", "now", "(", ")", ",", "'max RSS'", ",", "max_rss", ")", "\n", "display_top", "(", "snapshot", ")", "\n", "", "", "except", ":", "\n", "\t\t\t", "traceback", ".", "print_exc", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.network.network.Network.__init__": [[5, 12], ["None"], "methods", ["None"], ["\t", "def", "__init__", "(", "self", ",", "id", ",", "training", ")", ":", "\n", "\t\t", "self", ".", "training", "=", "training", "\n", "self", ".", "id", "=", "id", "\n", "self", ".", "use_internal_state", "=", "False", "\n", "# Initialize keys collections", "\n", "self", ".", "shared_keys", "=", "[", "]", "\n", "self", ".", "update_keys", "=", "[", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.network.network.Network._update_keys": [[13, 17], ["tensorflow.get_collection", "tensorflow.get_collection"], "methods", ["None"], ["", "def", "_update_keys", "(", "self", ",", "scope_name", ",", "share_trainables", ")", ":", "\n", "\t\t", "if", "share_trainables", ":", "\n", "\t\t\t", "self", ".", "shared_keys", "+=", "tf", ".", "get_collection", "(", "tf", ".", "GraphKeys", ".", "TRAINABLE_VARIABLES", ",", "scope", "=", "scope_name", ")", "\n", "", "self", ".", "update_keys", "+=", "tf", ".", "get_collection", "(", "tf", ".", "GraphKeys", ".", "UPDATE_OPS", ",", "scope", "=", "scope_name", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.network.network.Network._batch_normalization_layer": [[18, 28], ["tensorflow.variable_scope", "print", "tensorflow.layers.BatchNormalization", "tensorflow.layers.BatchNormalization.apply", "network.Network._update_keys"], "methods", ["home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.network.network.Network._update_keys"], ["", "def", "_batch_normalization_layer", "(", "self", ",", "input", ",", "scope", ",", "name", "=", "\"\"", ",", "share_trainables", "=", "True", ",", "renorm", "=", "False", ",", "center", "=", "True", ",", "scale", "=", "True", ")", ":", "\n", "\t\t", "layer_type", "=", "'BatchNorm'", "\n", "with", "tf", ".", "variable_scope", "(", "\"{}/{}{}\"", ".", "format", "(", "scope", ",", "layer_type", ",", "name", ")", ",", "reuse", "=", "tf", ".", "AUTO_REUSE", ")", "as", "variable_scope", ":", "\n", "\t\t\t", "print", "(", "\"\t[{}]Building or reusing scope: {}\"", ".", "format", "(", "self", ".", "id", ",", "variable_scope", ".", "name", ")", ")", "\n", "batch_norm", "=", "tf", ".", "layers", ".", "BatchNormalization", "(", "renorm", "=", "renorm", ",", "center", "=", "center", ",", "scale", "=", "scale", ")", "# renorm when minibaches are too small", "\n", "norm_input", "=", "batch_norm", ".", "apply", "(", "input", ",", "training", "=", "self", ".", "training", ")", "\n", "# update keys", "\n", "self", ".", "_update_keys", "(", "variable_scope", ".", "name", ",", "share_trainables", ")", "\n", "# return result", "\n", "return", "batch_norm", ",", "norm_input", "\n", "\n"]], "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.network.network.Network._feature_entropy_layer": [[29, 40], ["network.Network._batch_normalization_layer", "tensorflow.variable_scope", "Normal().cross_entropy", "tensorflow.layers.flatten", "network.Network._update_keys", "len", "tensorflow.reduce_mean", "Normal", "tensorflow.reduce_mean.get_shape", "tensorflow.sqrt"], "methods", ["home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.network.network.Network._batch_normalization_layer", "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.utils.distributions.Normal.cross_entropy", "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.utils.misc.flatten", "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.network.network.Network._update_keys"], ["", "", "def", "_feature_entropy_layer", "(", "self", ",", "input", ",", "scope", ",", "name", "=", "\"\"", ",", "share_trainables", "=", "True", ")", ":", "# feature entropy measures how much the input is uncommon", "\n", "\t\t", "layer_type", "=", "'Fentropy'", "\n", "batch_norm", ",", "_", "=", "self", ".", "_batch_normalization_layer", "(", "input", "=", "input", ",", "scope", "=", "scope", ",", "name", "=", "layer_type", ")", "\n", "with", "tf", ".", "variable_scope", "(", "\"{}/{}{}\"", ".", "format", "(", "scope", ",", "layer_type", ",", "name", ")", ",", "reuse", "=", "tf", ".", "AUTO_REUSE", ")", "as", "variable_scope", ":", "\n", "\t\t\t", "fentropy", "=", "Normal", "(", "batch_norm", ".", "moving_mean", ",", "tf", ".", "sqrt", "(", "batch_norm", ".", "moving_variance", ")", ")", ".", "cross_entropy", "(", "input", ")", "\n", "fentropy", "=", "tf", ".", "layers", ".", "flatten", "(", "fentropy", ")", "\n", "if", "len", "(", "fentropy", ".", "get_shape", "(", ")", ")", ">", "1", ":", "\n", "\t\t\t\t", "fentropy", "=", "tf", ".", "reduce_mean", "(", "fentropy", ",", "axis", "=", "-", "1", ")", "\n", "# update keys", "\n", "", "self", ".", "_update_keys", "(", "variable_scope", ".", "name", ",", "share_trainables", ")", "\n", "return", "fentropy", "\n", "", "", "", ""]], "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.actor_critic.towers_network.Towers_Network._cnn_layer": [[8, 36], ["input.get_shape().as_list", "tensorflow.variable_scope", "print", "tensorflow.concat", "towers_network.Towers_Network._update_keys", "input.get_shape", "tensorflow.variable_scope", "tensorflow.layers.conv2d", "tensorflow.layers.conv2d", "tensorflow.layers.max_pooling2d", "tensorflow.layers.flatten", "tensorflow.variable_scope", "tensorflow.layers.max_pooling2d", "range", "tensorflow.layers.max_pooling2d", "tensorflow.layers.flatten", "tensorflow.variable_scope", "tensorflow.layers.max_pooling2d", "range", "tensorflow.layers.max_pooling2d", "tensorflow.layers.flatten", "tensorflow.layers.conv2d", "tensorflow.layers.conv2d", "max", "max", "max", "max", "max", "max", "max", "max"], "methods", ["home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.network.network.Network._update_keys", "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.utils.misc.flatten", "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.utils.misc.flatten", "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.utils.misc.flatten", "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.utils.segment_tree.MaxSegmentTree.max", "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.utils.segment_tree.MaxSegmentTree.max", "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.utils.segment_tree.MaxSegmentTree.max", "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.utils.segment_tree.MaxSegmentTree.max", "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.utils.segment_tree.MaxSegmentTree.max", "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.utils.segment_tree.MaxSegmentTree.max", "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.utils.segment_tree.MaxSegmentTree.max", "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.utils.segment_tree.MaxSegmentTree.max"], ["\t", "def", "_cnn_layer", "(", "self", ",", "input", ",", "scope", ",", "name", "=", "\"\"", ",", "share_trainables", "=", "True", ")", ":", "\n", "\t\t", "depth", "=", "2", "\n", "input_shape", "=", "input", ".", "get_shape", "(", ")", ".", "as_list", "(", ")", "\n", "layer_type", "=", "'CNN'", "\n", "with", "tf", ".", "variable_scope", "(", "\"{}/{}{}\"", ".", "format", "(", "scope", ",", "layer_type", ",", "name", ")", ",", "reuse", "=", "tf", ".", "AUTO_REUSE", ")", "as", "variable_scope", ":", "\n", "\t\t\t", "print", "(", "\"    [{}]Building or reusing scope: {}\"", ".", "format", "(", "self", ".", "id", ",", "variable_scope", ".", "name", ")", ")", "\n", "with", "tf", ".", "variable_scope", "(", "\"tower_1\"", ")", ":", "\n", "\t\t\t\t", "tower1", "=", "tf", ".", "layers", ".", "conv2d", "(", "name", "=", "'CNN_Tower1_Conv1'", ",", "inputs", "=", "input", ",", "activation", "=", "tf", ".", "nn", ".", "relu", ",", "filters", "=", "64", ",", "kernel_size", "=", "(", "3", ",", "3", ")", ",", "strides", "=", "(", "1", ",", "1", ")", ",", "padding", "=", "'SAME'", ",", "kernel_initializer", "=", "tf", ".", "initializers", ".", "variance_scaling", ")", "\n", "tower1", "=", "tf", ".", "layers", ".", "conv2d", "(", "name", "=", "'CNN_Tower1_Conv2'", ",", "inputs", "=", "tower1", ",", "activation", "=", "tf", ".", "nn", ".", "relu", ",", "filters", "=", "32", ",", "kernel_size", "=", "(", "3", ",", "3", ")", ",", "strides", "=", "(", "1", ",", "1", ")", ",", "padding", "=", "'SAME'", ",", "kernel_initializer", "=", "tf", ".", "initializers", ".", "variance_scaling", ")", "\n", "tower1", "=", "tf", ".", "layers", ".", "max_pooling2d", "(", "name", "=", "'CNN_Tower1_MaxPool1'", ",", "inputs", "=", "tower1", ",", "pool_size", "=", "(", "input_shape", "[", "1", "]", ",", "input_shape", "[", "2", "]", ")", ",", "strides", "=", "(", "input_shape", "[", "1", "]", ",", "input_shape", "[", "2", "]", ")", ")", "\n", "tower1", "=", "tf", ".", "layers", ".", "flatten", "(", "tower1", ")", "\n", "", "with", "tf", ".", "variable_scope", "(", "\"tower_2\"", ")", ":", "\n", "\t\t\t\t", "tower2", "=", "tf", ".", "layers", ".", "max_pooling2d", "(", "name", "=", "'CNN_Tower2_MaxPool1'", ",", "inputs", "=", "input", ",", "pool_size", "=", "(", "2", ",", "2", ")", ",", "strides", "=", "(", "2", ",", "2", ")", ",", "padding", "=", "'SAME'", ")", "\n", "for", "i", "in", "range", "(", "depth", ")", ":", "\n", "\t\t\t\t\t", "tower2", "=", "tf", ".", "layers", ".", "conv2d", "(", "name", "=", "'CNN_Tower2_Conv{}'", ".", "format", "(", "i", ")", ",", "inputs", "=", "tower2", ",", "activation", "=", "tf", ".", "nn", ".", "relu", ",", "filters", "=", "32", ",", "kernel_size", "=", "(", "3", ",", "3", ")", ",", "strides", "=", "(", "1", ",", "1", ")", ",", "padding", "=", "'SAME'", ",", "kernel_initializer", "=", "tf", ".", "initializers", ".", "variance_scaling", ")", "\n", "", "tower2", "=", "tf", ".", "layers", ".", "max_pooling2d", "(", "name", "=", "'CNN_Tower2_MaxPool2'", ",", "inputs", "=", "tower2", ",", "pool_size", "=", "(", "max", "(", "1", ",", "input_shape", "[", "1", "]", "//", "2", ")", ",", "max", "(", "1", ",", "input_shape", "[", "2", "]", "//", "2", ")", ")", ",", "strides", "=", "(", "max", "(", "1", ",", "input_shape", "[", "1", "]", "//", "2", ")", ",", "max", "(", "1", ",", "input_shape", "[", "2", "]", "//", "2", ")", ")", ")", "\n", "tower2", "=", "tf", ".", "layers", ".", "flatten", "(", "tower2", ")", "\n", "", "with", "tf", ".", "variable_scope", "(", "\"tower_3\"", ")", ":", "\n", "\t\t\t\t", "tower3", "=", "tf", ".", "layers", ".", "max_pooling2d", "(", "name", "=", "'CNN_Tower3_MaxPool1'", ",", "inputs", "=", "input", ",", "pool_size", "=", "(", "4", ",", "4", ")", ",", "strides", "=", "(", "4", ",", "4", ")", ",", "padding", "=", "'SAME'", ")", "\n", "for", "i", "in", "range", "(", "depth", ")", ":", "\n", "\t\t\t\t\t", "tower3", "=", "tf", ".", "layers", ".", "conv2d", "(", "name", "=", "'CNN_Tower3_Conv{}'", ".", "format", "(", "i", ")", ",", "inputs", "=", "tower3", ",", "activation", "=", "tf", ".", "nn", ".", "relu", ",", "filters", "=", "32", ",", "kernel_size", "=", "(", "3", ",", "3", ")", ",", "strides", "=", "(", "1", ",", "1", ")", ",", "padding", "=", "'SAME'", ",", "kernel_initializer", "=", "tf", ".", "initializers", ".", "variance_scaling", ")", "\n", "", "tower3", "=", "tf", ".", "layers", ".", "max_pooling2d", "(", "name", "=", "'CNN_Tower3_MaxPool2'", ",", "inputs", "=", "tower3", ",", "pool_size", "=", "(", "max", "(", "1", ",", "input_shape", "[", "1", "]", "//", "4", ")", ",", "max", "(", "1", ",", "input_shape", "[", "2", "]", "//", "4", ")", ")", ",", "strides", "=", "(", "max", "(", "1", ",", "input_shape", "[", "1", "]", "//", "4", ")", ",", "max", "(", "1", ",", "input_shape", "[", "2", "]", "//", "4", ")", ")", ")", "\n", "tower3", "=", "tf", ".", "layers", ".", "flatten", "(", "tower3", ")", "\n", "", "concat", "=", "tf", ".", "concat", "(", "[", "tower1", ",", "tower2", ",", "tower3", "]", ",", "axis", "=", "-", "1", ")", "\n", "# update keys", "\n", "self", ".", "_update_keys", "(", "variable_scope", ".", "name", ",", "share_trainables", ")", "\n", "# return result", "\n", "return", "concat", "", "", "", "", ""]], "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.actor_critic.impala_network.Impala_Network.conv_layer": [[20, 27], ["tensorflow.layers.conv2d", "tensorflow.layers.dropout", "tensorflow.contrib.layers.batch_norm"], "methods", ["None"], ["def", "conv_layer", "(", "self", ",", "out", ",", "depth", ")", ":", "\n", "\t\t", "out", "=", "tf", ".", "layers", ".", "conv2d", "(", "out", ",", "depth", ",", "3", ",", "padding", "=", "'same'", ")", "\n", "if", "self", ".", "dropout_probability", ">", "0", ":", "\n", "\t\t\t", "out", "=", "tf", ".", "layers", ".", "dropout", "(", "inputs", "=", "out", ",", "rate", "=", "self", ".", "dropout_probability", ")", "\n", "", "if", "self", ".", "use_batch_norm", ":", "\n", "\t\t\t", "out", "=", "tf", ".", "contrib", ".", "layers", ".", "batch_norm", "(", "out", ",", "center", "=", "True", ",", "scale", "=", "True", ",", "is_training", "=", "True", ")", "\n", "", "return", "out", "\n", "\n"]], "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.actor_critic.impala_network.Impala_Network.residual_block": [[28, 35], ["tensorflow.nn.relu", "impala_network.Impala_Network.conv_layer", "tensorflow.nn.relu", "impala_network.Impala_Network.conv_layer", "inputs.get_shape"], "methods", ["home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.actor_critic.impala_network.Impala_Network.conv_layer", "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.actor_critic.impala_network.Impala_Network.conv_layer"], ["", "def", "residual_block", "(", "self", ",", "inputs", ")", ":", "\n", "\t\t", "depth", "=", "inputs", ".", "get_shape", "(", ")", "[", "-", "1", "]", ".", "value", "\n", "out", "=", "tf", ".", "nn", ".", "relu", "(", "inputs", ")", "\n", "out", "=", "self", ".", "conv_layer", "(", "out", ",", "depth", ")", "\n", "out", "=", "tf", ".", "nn", ".", "relu", "(", "out", ")", "\n", "out", "=", "self", ".", "conv_layer", "(", "out", ",", "depth", ")", "\n", "return", "out", "+", "inputs", "\n", "\n"]], "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.actor_critic.impala_network.Impala_Network.conv_sequence": [[36, 42], ["impala_network.Impala_Network.conv_layer", "tensorflow.layers.max_pooling2d", "impala_network.Impala_Network.residual_block", "impala_network.Impala_Network.residual_block"], "methods", ["home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.actor_critic.impala_network.Impala_Network.conv_layer", "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.actor_critic.impala_network.Impala_Network.residual_block", "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.actor_critic.impala_network.Impala_Network.residual_block"], ["", "def", "conv_sequence", "(", "self", ",", "inputs", ",", "depth", ")", ":", "\n", "\t\t", "out", "=", "self", ".", "conv_layer", "(", "inputs", ",", "depth", ")", "\n", "out", "=", "tf", ".", "layers", ".", "max_pooling2d", "(", "out", ",", "pool_size", "=", "3", ",", "strides", "=", "2", ",", "padding", "=", "'same'", ")", "\n", "out", "=", "self", ".", "residual_block", "(", "out", ")", "\n", "out", "=", "self", ".", "residual_block", "(", "out", ")", "\n", "return", "out", "\n", "\n"]], "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.actor_critic.impala_network.Impala_Network._cnn_layer": [[43, 55], ["tensorflow.variable_scope", "print", "tensorflow.layers.flatten", "tensorflow.nn.relu", "impala_network.Impala_Network._update_keys", "impala_network.Impala_Network.conv_sequence"], "methods", ["home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.utils.misc.flatten", "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.network.network.Network._update_keys", "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.actor_critic.impala_network.Impala_Network.conv_sequence"], ["", "def", "_cnn_layer", "(", "self", ",", "input", ",", "scope", ",", "name", "=", "\"\"", ",", "share_trainables", "=", "True", ")", ":", "\n", "\t\t", "layer_type", "=", "'CNN'", "\n", "with", "tf", ".", "variable_scope", "(", "\"{}/{}{}\"", ".", "format", "(", "scope", ",", "layer_type", ",", "name", ")", ",", "reuse", "=", "tf", ".", "AUTO_REUSE", ")", "as", "variable_scope", ":", "\n", "\t\t\t", "print", "(", "\"\t[{}]Building or reusing scope: {}\"", ".", "format", "(", "self", ".", "id", ",", "variable_scope", ".", "name", ")", ")", "\n", "for", "depth", "in", "self", ".", "depths", ":", "\n", "\t\t\t\t", "input", "=", "self", ".", "conv_sequence", "(", "input", ",", "depth", ")", "\n", "", "input", "=", "tf", ".", "layers", ".", "flatten", "(", "input", ")", "\n", "input", "=", "tf", ".", "nn", ".", "relu", "(", "input", ")", "\n", "# update keys", "\n", "self", ".", "_update_keys", "(", "variable_scope", ".", "name", ",", "share_trainables", ")", "\n", "# return result", "\n", "return", "input", "\n", "\n"]], "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.actor_critic.impala_network.Impala_Network._concat_layer": [[56, 69], ["tensorflow.variable_scope", "print", "tensorflow.layers.flatten", "tensorflow.layers.dense", "impala_network.Impala_Network._update_keys", "tensorflow.layers.flatten", "tensorflow.concat", "tensorflow.layers.flatten.get_shape"], "methods", ["home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.utils.misc.flatten", "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.network.network.Network._update_keys", "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.utils.misc.flatten"], ["", "", "def", "_concat_layer", "(", "self", ",", "input", ",", "concat", ",", "scope", ",", "name", "=", "\"\"", ",", "share_trainables", "=", "True", ")", ":", "\n", "\t\t", "layer_type", "=", "'Concat'", "\n", "with", "tf", ".", "variable_scope", "(", "\"{}/{}{}\"", ".", "format", "(", "scope", ",", "layer_type", ",", "name", ")", ",", "reuse", "=", "tf", ".", "AUTO_REUSE", ")", "as", "variable_scope", ":", "\n", "\t\t\t", "print", "(", "\"\t[{}]Building or reusing scope: {}\"", ".", "format", "(", "self", ".", "id", ",", "variable_scope", ".", "name", ")", ")", "\n", "input", "=", "tf", ".", "layers", ".", "flatten", "(", "input", ")", "\n", "if", "concat", ".", "get_shape", "(", ")", "[", "-", "1", "]", ">", "0", ":", "\n", "\t\t\t\t", "concat", "=", "tf", ".", "layers", ".", "flatten", "(", "concat", ")", "\n", "input", "=", "tf", ".", "concat", "(", "[", "input", ",", "concat", "]", ",", "-", "1", ")", "# shape: (batch, concat_size+units)", "\n", "", "input", "=", "tf", ".", "layers", ".", "dense", "(", "name", "=", "'Concat_Dense1'", ",", "inputs", "=", "input", ",", "units", "=", "256", ",", "activation", "=", "tf", ".", "nn", ".", "relu", ")", "\n", "# Update keys", "\n", "self", ".", "_update_keys", "(", "variable_scope", ".", "name", ",", "share_trainables", ")", "\n", "# Return result", "\n", "return", "input", "\n", "", "", "", ""]], "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.actor_critic.sentiment_analysis_network.SA_Network._cnn_layer": [[8, 17], ["tensorflow.variable_scope", "print", "tensorflow.layers.conv2d", "sentiment_analysis_network.SA_Network._update_keys"], "methods", ["home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.network.network.Network._update_keys"], ["def", "_cnn_layer", "(", "self", ",", "input", ",", "scope", ",", "name", "=", "\"\"", ",", "share_trainables", "=", "True", ")", ":", "\n", "\t\t", "layer_type", "=", "'CNN'", "\n", "with", "tf", ".", "variable_scope", "(", "\"{}/{}{}\"", ".", "format", "(", "scope", ",", "layer_type", ",", "name", ")", ",", "reuse", "=", "tf", ".", "AUTO_REUSE", ")", "as", "variable_scope", ":", "\n", "\t\t\t", "print", "(", "\"    [{}]Building or reusing scope: {}\"", ".", "format", "(", "self", ".", "id", ",", "variable_scope", ".", "name", ")", ")", "\n", "input", "=", "tf", ".", "layers", ".", "conv2d", "(", "name", "=", "'CNN_Conv1'", ",", "inputs", "=", "input", ",", "filters", "=", "16", ",", "kernel_size", "=", "(", "1", ",", "3", ")", ",", "padding", "=", "'SAME'", ",", "activation", "=", "tf", ".", "nn", ".", "relu", ",", "kernel_initializer", "=", "tf", ".", "initializers", ".", "variance_scaling", ")", "\n", "# update keys", "\n", "self", ".", "_update_keys", "(", "variable_scope", ".", "name", ",", "share_trainables", ")", "\n", "# return result", "\n", "return", "input", "\n", "\n"]], "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.actor_critic.sentiment_analysis_network.SA_Network._concat_layer": [[18, 33], ["tensorflow.variable_scope", "print", "tensorflow.layers.flatten", "tensorflow.layers.dense", "tensorflow.contrib.layers.maxout", "tensorflow.reshape", "sentiment_analysis_network.SA_Network._update_keys", "tensorflow.layers.flatten", "tensorflow.concat", "tensorflow.layers.flatten.get_shape"], "methods", ["home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.utils.misc.flatten", "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.network.network.Network._update_keys", "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.utils.misc.flatten"], ["", "", "def", "_concat_layer", "(", "self", ",", "input", ",", "concat", ",", "scope", ",", "name", "=", "\"\"", ",", "share_trainables", "=", "True", ")", ":", "\n", "\t\t", "layer_type", "=", "'Concat'", "\n", "with", "tf", ".", "variable_scope", "(", "\"{}/{}{}\"", ".", "format", "(", "scope", ",", "layer_type", ",", "name", ")", ",", "reuse", "=", "tf", ".", "AUTO_REUSE", ")", "as", "variable_scope", ":", "\n", "\t\t\t", "print", "(", "\"    [{}]Building or reusing scope: {}\"", ".", "format", "(", "self", ".", "id", ",", "variable_scope", ".", "name", ")", ")", "\n", "input", "=", "tf", ".", "layers", ".", "flatten", "(", "input", ")", "\n", "input", "=", "tf", ".", "layers", ".", "dense", "(", "name", "=", "'Concat_Dense1'", ",", "inputs", "=", "input", ",", "units", "=", "128", ",", "activation", "=", "None", ",", "kernel_initializer", "=", "tf", ".", "initializers", ".", "variance_scaling", ")", "\n", "input", "=", "tf", ".", "contrib", ".", "layers", ".", "maxout", "(", "inputs", "=", "input", ",", "num_units", "=", "64", ",", "axis", "=", "-", "1", ")", "\n", "input", "=", "tf", ".", "reshape", "(", "input", ",", "[", "-", "1", ",", "64", "]", ")", "\n", "if", "concat", ".", "get_shape", "(", ")", "[", "-", "1", "]", ">", "0", ":", "\n", "\t\t\t\t", "concat", "=", "tf", ".", "layers", ".", "flatten", "(", "concat", ")", "\n", "input", "=", "tf", ".", "concat", "(", "[", "input", ",", "concat", "]", ",", "-", "1", ")", "# shape: (batch, concat_size+units)", "\n", "# Update keys", "\n", "", "self", ".", "_update_keys", "(", "variable_scope", ".", "name", ",", "share_trainables", ")", "\n", "# Return result", "\n", "return", "input", "\n", "", "", "", ""]], "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.actor_critic.msa_network.MSA_Network._cnn_layer": [[14, 26], ["tensorflow.layers.conv2d.get_shape().as_list", "tensorflow.variable_scope", "print", "tensorflow.layers.conv2d", "tensorflow.layers.conv2d", "tensorflow.layers.conv2d", "msa_network.MSA_Network._update_keys", "tensorflow.layers.conv2d.get_shape", "utils.orthogonal_initializer", "utils.orthogonal_initializer", "utils.orthogonal_initializer", "numpy.sqrt", "numpy.sqrt", "numpy.sqrt"], "methods", ["home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.network.network.Network._update_keys", "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.utils.tensorflow_utils.orthogonal_initializer", "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.utils.tensorflow_utils.orthogonal_initializer", "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.utils.tensorflow_utils.orthogonal_initializer"], ["\t", "def", "_cnn_layer", "(", "self", ",", "input", ",", "scope", ",", "name", "=", "\"\"", ",", "share_trainables", "=", "True", ")", ":", "\n", "\t\t", "layer_type", "=", "'CNN'", "\n", "_", ",", "input_height", ",", "input_width", ",", "input_channel", "=", "input", ".", "get_shape", "(", ")", ".", "as_list", "(", ")", "\n", "with", "tf", ".", "variable_scope", "(", "\"{}/{}{}\"", ".", "format", "(", "scope", ",", "layer_type", ",", "name", ")", ",", "reuse", "=", "tf", ".", "AUTO_REUSE", ")", "as", "variable_scope", ":", "\n", "\t\t\t", "print", "(", "\"\t[{}]Building or reusing scope: {}\"", ".", "format", "(", "self", ".", "id", ",", "variable_scope", ".", "name", ")", ")", "\n", "input", "=", "tf", ".", "layers", ".", "conv2d", "(", "name", "=", "'CNN_Conv1'", ",", "inputs", "=", "input", ",", "filters", "=", "16", ",", "kernel_size", "=", "(", "input_height", ",", "1", ")", ",", "dilation_rate", "=", "(", "1", ",", "3", ")", ",", "padding", "=", "'SAME'", ",", "activation", "=", "tf", ".", "nn", ".", "relu", ",", "kernel_initializer", "=", "tf_utils", ".", "orthogonal_initializer", "(", "np", ".", "sqrt", "(", "2", ")", ")", ")", "\n", "input", "=", "tf", ".", "layers", ".", "conv2d", "(", "name", "=", "'CNN_Conv2'", ",", "inputs", "=", "input", ",", "filters", "=", "16", ",", "kernel_size", "=", "(", "1", ",", "input_width", ")", ",", "dilation_rate", "=", "(", "3", ",", "1", ")", ",", "padding", "=", "'SAME'", ",", "activation", "=", "tf", ".", "nn", ".", "relu", ",", "kernel_initializer", "=", "tf_utils", ".", "orthogonal_initializer", "(", "np", ".", "sqrt", "(", "2", ")", ")", ")", "\n", "input", "=", "tf", ".", "layers", ".", "conv2d", "(", "name", "=", "'CNN_Conv3'", ",", "inputs", "=", "input", ",", "filters", "=", "32", ",", "kernel_size", "=", "3", ",", "padding", "=", "'SAME'", ",", "activation", "=", "tf", ".", "nn", ".", "relu", ",", "kernel_initializer", "=", "tf_utils", ".", "orthogonal_initializer", "(", "np", ".", "sqrt", "(", "2", ")", ")", ")", "\n", "# update keys", "\n", "self", ".", "_update_keys", "(", "variable_scope", ".", "name", ",", "share_trainables", ")", "\n", "# return result", "\n", "return", "input", "\n", "\n"]], "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.actor_critic.msa_network.MSA_Network._concat_layer": [[27, 39], ["tensorflow.variable_scope", "print", "tensorflow.layers.flatten", "tensorflow.layers.flatten", "tensorflow.concat", "tensorflow.layers.dense", "msa_network.MSA_Network._update_keys", "utils.orthogonal_initializer", "numpy.sqrt"], "methods", ["home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.utils.misc.flatten", "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.utils.misc.flatten", "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.network.network.Network._update_keys", "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.utils.tensorflow_utils.orthogonal_initializer"], ["", "", "def", "_concat_layer", "(", "self", ",", "input", ",", "concat", ",", "scope", ",", "name", "=", "\"\"", ",", "share_trainables", "=", "True", ")", ":", "\n", "\t\t", "layer_type", "=", "'Concat'", "\n", "with", "tf", ".", "variable_scope", "(", "\"{}/{}{}\"", ".", "format", "(", "scope", ",", "layer_type", ",", "name", ")", ",", "reuse", "=", "tf", ".", "AUTO_REUSE", ")", "as", "variable_scope", ":", "\n", "\t\t\t", "print", "(", "\"\t[{}]Building or reusing scope: {}\"", ".", "format", "(", "self", ".", "id", ",", "variable_scope", ".", "name", ")", ")", "\n", "input", "=", "tf", ".", "layers", ".", "flatten", "(", "input", ")", "\n", "concat", "=", "tf", ".", "layers", ".", "flatten", "(", "concat", ")", "\n", "input", "=", "tf", ".", "concat", "(", "[", "input", ",", "concat", "]", ",", "-", "1", ")", "# shape: (batch, concat_size+input_size)", "\n", "input", "=", "tf", ".", "layers", ".", "dense", "(", "name", "=", "'Concat_Dense1'", ",", "inputs", "=", "input", ",", "units", "=", "256", ",", "activation", "=", "tf", ".", "nn", ".", "relu", ",", "kernel_initializer", "=", "tf_utils", ".", "orthogonal_initializer", "(", "np", ".", "sqrt", "(", "2", ")", ")", ")", "\n", "# Update keys", "\n", "self", ".", "_update_keys", "(", "variable_scope", ".", "name", ",", "share_trainables", ")", "\n", "# Return result", "\n", "return", "input", "\n", "\n"]], "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.actor_critic.msa_network.MSA_Network._rnn_layer": [[40, 56], ["utils.rnn.RNN", "utils.rnn.RNN.state_placeholder", "utils.rnn.RNN.default_state", "tensorflow.variable_scope", "print", "utils.rnn.RNN.process_batches", "tensorflow.layers.dropout", "msa_network.MSA_Network._update_keys"], "methods", ["home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.rnn.rnn.RNN.state_placeholder", "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.rnn.rnn.RNN.default_state", "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.rnn.rnn.RNN.process_batches", "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.network.network.Network._update_keys"], ["", "", "def", "_rnn_layer", "(", "self", ",", "input", ",", "scope", ",", "name", "=", "\"\"", ",", "share_trainables", "=", "True", ")", ":", "\n", "\t\t", "rnn", "=", "RNN", "(", "type", "=", "'LSTM'", ",", "direction", "=", "2", ",", "units", "=", "128", ",", "batch_size", "=", "1", ",", "stack_size", "=", "1", ",", "training", "=", "self", ".", "training", ",", "dtype", "=", "flags", ".", "parameters_type", ")", "\n", "internal_initial_state", "=", "rnn", ".", "state_placeholder", "(", "name", "=", "\"initial_lstm_state\"", ")", "# for stateful lstm", "\n", "internal_default_state", "=", "rnn", ".", "default_state", "(", ")", "\n", "layer_type", "=", "rnn", ".", "type", "\n", "with", "tf", ".", "variable_scope", "(", "\"{}/{}{}\"", ".", "format", "(", "scope", ",", "layer_type", ",", "name", ")", ",", "reuse", "=", "tf", ".", "AUTO_REUSE", ")", "as", "variable_scope", ":", "\n", "\t\t\t", "print", "(", "\"\t[{}]Building or reusing scope: {}\"", ".", "format", "(", "self", ".", "id", ",", "variable_scope", ".", "name", ")", ")", "\n", "output", ",", "internal_final_state", "=", "rnn", ".", "process_batches", "(", "\n", "input", "=", "input", ",", "\n", "initial_states", "=", "internal_initial_state", ",", "\n", "sizes", "=", "self", ".", "size_batch", "\n", ")", "\n", "output", "=", "tf", ".", "layers", ".", "dropout", "(", "output", ",", "rate", "=", "0.75", ",", "training", "=", "self", ".", "training", ")", "\n", "# Update keys", "\n", "self", ".", "_update_keys", "(", "variable_scope", ".", "name", ",", "share_trainables", ")", "\n", "return", "output", ",", "(", "[", "internal_initial_state", "]", ",", "[", "internal_default_state", "]", ",", "[", "internal_final_state", "]", ")", "\n", "", "", "", ""]], "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.actor_critic.openai_large_network.OpenAILarge_Network._concat_layer": [[15, 29], ["tensorflow.variable_scope", "print", "tensorflow.layers.flatten", "tensorflow.layers.dense", "tensorflow.layers.dense", "openai_large_network.OpenAILarge_Network._update_keys", "tensorflow.layers.flatten", "tensorflow.concat", "tensorflow.layers.flatten.get_shape", "utils.orthogonal_initializer", "utils.orthogonal_initializer", "numpy.sqrt", "numpy.sqrt"], "methods", ["home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.utils.misc.flatten", "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.network.network.Network._update_keys", "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.utils.misc.flatten", "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.utils.tensorflow_utils.orthogonal_initializer", "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.utils.tensorflow_utils.orthogonal_initializer"], ["\t", "def", "_concat_layer", "(", "self", ",", "input", ",", "concat", ",", "scope", ",", "name", "=", "\"\"", ",", "share_trainables", "=", "True", ")", ":", "\n", "\t\t", "layer_type", "=", "'Concat'", "\n", "with", "tf", ".", "variable_scope", "(", "\"{}/{}{}\"", ".", "format", "(", "scope", ",", "layer_type", ",", "name", ")", ",", "reuse", "=", "tf", ".", "AUTO_REUSE", ")", "as", "variable_scope", ":", "\n", "\t\t\t", "print", "(", "\"\t[{}]Building or reusing scope: {}\"", ".", "format", "(", "self", ".", "id", ",", "variable_scope", ".", "name", ")", ")", "\n", "input", "=", "tf", ".", "layers", ".", "flatten", "(", "input", ")", "\n", "if", "concat", ".", "get_shape", "(", ")", "[", "-", "1", "]", ">", "0", ":", "\n", "\t\t\t\t", "concat", "=", "tf", ".", "layers", ".", "flatten", "(", "concat", ")", "\n", "input", "=", "tf", ".", "concat", "(", "[", "input", ",", "concat", "]", ",", "-", "1", ")", "# shape: (batch, concat_size+units)", "\n", "", "input", "=", "tf", ".", "layers", ".", "dense", "(", "name", "=", "'Concat_Dense1'", ",", "inputs", "=", "input", ",", "units", "=", "256", ",", "activation", "=", "tf", ".", "nn", ".", "relu", ",", "kernel_initializer", "=", "tf_utils", ".", "orthogonal_initializer", "(", "np", ".", "sqrt", "(", "2", ")", ")", ")", "\n", "input", "=", "tf", ".", "layers", ".", "dense", "(", "name", "=", "'Concat_Dense2'", ",", "inputs", "=", "input", ",", "units", "=", "448", ",", "activation", "=", "tf", ".", "nn", ".", "relu", ",", "kernel_initializer", "=", "tf_utils", ".", "orthogonal_initializer", "(", "np", ".", "sqrt", "(", "2", ")", ")", ")", "\n", "# Update keys", "\n", "self", ".", "_update_keys", "(", "variable_scope", ".", "name", ",", "share_trainables", ")", "\n", "# Return result", "\n", "return", "input", "\n", "\n"]], "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.actor_critic.openai_large_network.OpenAILarge_Network._rnn_layer": [[30, 46], ["utils.rnn.RNN", "utils.rnn.RNN.state_placeholder", "utils.rnn.RNN.default_state", "tensorflow.variable_scope", "print", "utils.rnn.RNN.process_batches", "openai_large_network.OpenAILarge_Network._update_keys"], "methods", ["home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.rnn.rnn.RNN.state_placeholder", "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.rnn.rnn.RNN.default_state", "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.rnn.rnn.RNN.process_batches", "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.network.network.Network._update_keys"], ["", "", "def", "_rnn_layer", "(", "self", ",", "input", ",", "scope", ",", "name", "=", "\"\"", ",", "share_trainables", "=", "True", ")", ":", "\n", "\t\t", "rnn", "=", "RNN", "(", "type", "=", "'LSTM'", ",", "direction", "=", "1", ",", "units", "=", "448", ",", "batch_size", "=", "1", ",", "stack_size", "=", "1", ",", "training", "=", "self", ".", "training", ",", "dtype", "=", "flags", ".", "parameters_type", ")", "\n", "internal_initial_state", "=", "rnn", ".", "state_placeholder", "(", "name", "=", "\"initial_lstm_state\"", ")", "# for stateful lstm", "\n", "internal_default_state", "=", "rnn", ".", "default_state", "(", ")", "\n", "layer_type", "=", "rnn", ".", "type", "\n", "with", "tf", ".", "variable_scope", "(", "\"{}/{}{}\"", ".", "format", "(", "scope", ",", "layer_type", ",", "name", ")", ",", "reuse", "=", "tf", ".", "AUTO_REUSE", ")", "as", "variable_scope", ":", "\n", "\t\t\t", "print", "(", "\"\t[{}]Building or reusing scope: {}\"", ".", "format", "(", "self", ".", "id", ",", "variable_scope", ".", "name", ")", ")", "\n", "output", ",", "internal_final_state", "=", "rnn", ".", "process_batches", "(", "\n", "input", "=", "input", ",", "\n", "initial_states", "=", "internal_initial_state", ",", "\n", "sizes", "=", "self", ".", "size_batch", "\n", ")", "\n", "# output = tf.layers.dropout(inputs=output, rate=0.5, training=self.training)", "\n", "# Update keys", "\n", "self", ".", "_update_keys", "(", "variable_scope", ".", "name", ",", "share_trainables", ")", "\n", "return", "output", ",", "(", "internal_initial_state", ",", "internal_default_state", ",", "internal_final_state", ")", "\n", "", "", "", ""]], "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.actor_critic.openai_small_network.OpenAISmall_Network._cnn_layer": [[14, 27], ["tensorflow.variable_scope", "print", "tensorflow.layers.conv2d", "tensorflow.layers.conv2d", "tensorflow.layers.conv2d", "openai_small_network.OpenAISmall_Network._update_keys", "utils.orthogonal_initializer", "utils.orthogonal_initializer", "utils.orthogonal_initializer", "numpy.sqrt", "numpy.sqrt", "numpy.sqrt"], "methods", ["home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.network.network.Network._update_keys", "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.utils.tensorflow_utils.orthogonal_initializer", "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.utils.tensorflow_utils.orthogonal_initializer", "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.utils.tensorflow_utils.orthogonal_initializer"], ["\t", "def", "_cnn_layer", "(", "self", ",", "input", ",", "scope", ",", "name", "=", "\"\"", ",", "share_trainables", "=", "True", ")", ":", "\n", "\t\t", "layer_type", "=", "'CNN'", "\n", "with", "tf", ".", "variable_scope", "(", "\"{}/{}{}\"", ".", "format", "(", "scope", ",", "layer_type", ",", "name", ")", ",", "reuse", "=", "tf", ".", "AUTO_REUSE", ")", "as", "variable_scope", ":", "\n", "\t\t\t", "print", "(", "\"\t[{}]Building or reusing scope: {}\"", ".", "format", "(", "self", ".", "id", ",", "variable_scope", ".", "name", ")", ")", "\n", "# input = tf.contrib.model_pruning.masked_conv2d(inputs=input, num_outputs=16, kernel_size=(3,3), padding='SAME', activation_fn=tf.nn.leaky_relu) # xavier initializer", "\n", "# input = tf.contrib.model_pruning.masked_conv2d(inputs=input, num_outputs=32, kernel_size=(3,3), padding='SAME', activation_fn=tf.nn.leaky_relu) # xavier initializer", "\n", "input", "=", "tf", ".", "layers", ".", "conv2d", "(", "name", "=", "'CNN_Conv1'", ",", "inputs", "=", "input", ",", "filters", "=", "32", ",", "kernel_size", "=", "8", ",", "strides", "=", "4", ",", "padding", "=", "'SAME'", ",", "activation", "=", "tf", ".", "nn", ".", "relu", ",", "kernel_initializer", "=", "tf_utils", ".", "orthogonal_initializer", "(", "np", ".", "sqrt", "(", "2", ")", ")", ")", "\n", "input", "=", "tf", ".", "layers", ".", "conv2d", "(", "name", "=", "'CNN_Conv2'", ",", "inputs", "=", "input", ",", "filters", "=", "64", ",", "kernel_size", "=", "4", ",", "strides", "=", "2", ",", "padding", "=", "'SAME'", ",", "activation", "=", "tf", ".", "nn", ".", "relu", ",", "kernel_initializer", "=", "tf_utils", ".", "orthogonal_initializer", "(", "np", ".", "sqrt", "(", "2", ")", ")", ")", "\n", "input", "=", "tf", ".", "layers", ".", "conv2d", "(", "name", "=", "'CNN_Conv3'", ",", "inputs", "=", "input", ",", "filters", "=", "64", ",", "kernel_size", "=", "4", ",", "strides", "=", "1", ",", "padding", "=", "'SAME'", ",", "activation", "=", "tf", ".", "nn", ".", "relu", ",", "kernel_initializer", "=", "tf_utils", ".", "orthogonal_initializer", "(", "np", ".", "sqrt", "(", "2", ")", ")", ")", "\n", "# update keys", "\n", "self", ".", "_update_keys", "(", "variable_scope", ".", "name", ",", "share_trainables", ")", "\n", "# return result", "\n", "return", "input", "\n", "\n"]], "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.actor_critic.openai_small_network.OpenAISmall_Network._weights_layer": [[28, 47], ["tensorflow.variable_scope", "print", "tensorflow.stop_gradient", "tensorflow.transpose", "tensorflow.layers.dense", "tensorflow.reshape", "tensorflow.stop_gradient", "tensorflow.reshape", "tensorflow.concat", "tensorflow.layers.dense", "tensorflow.reshape", "tensorflow.layers.flatten", "tensorflow.map_fn", "openai_small_network.OpenAISmall_Network._update_keys", "utils.orthogonal_initializer", "utils.orthogonal_initializer", "numpy.sqrt", "numpy.sqrt", "tensorflow.concat"], "methods", ["home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.utils.misc.flatten", "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.network.network.Network._update_keys", "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.utils.tensorflow_utils.orthogonal_initializer", "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.utils.tensorflow_utils.orthogonal_initializer"], ["", "", "def", "_weights_layer", "(", "self", ",", "input", ",", "weights", ",", "scope", ",", "name", "=", "\"\"", ",", "share_trainables", "=", "True", ")", ":", "\n", "\t\t", "layer_type", "=", "'Weights'", "\n", "with", "tf", ".", "variable_scope", "(", "\"{}/{}{}\"", ".", "format", "(", "scope", ",", "layer_type", ",", "name", ")", ",", "reuse", "=", "tf", ".", "AUTO_REUSE", ")", "as", "variable_scope", ":", "\n", "\t\t\t", "print", "(", "\"\t[{}]Building or reusing scope: {}\"", ".", "format", "(", "self", ".", "id", ",", "variable_scope", ".", "name", ")", ")", "\n", "kernel", "=", "tf", ".", "stop_gradient", "(", "weights", "[", "'kernel'", "]", ")", "\n", "kernel", "=", "tf", ".", "transpose", "(", "kernel", ",", "[", "1", ",", "0", "]", ")", "\n", "kernel", "=", "tf", ".", "layers", ".", "dense", "(", "name", "=", "'TS_Dense0'", ",", "inputs", "=", "kernel", ",", "units", "=", "1", ",", "activation", "=", "tf", ".", "nn", ".", "relu", ",", "kernel_initializer", "=", "tf_utils", ".", "orthogonal_initializer", "(", "np", ".", "sqrt", "(", "2", ")", ")", ")", "\n", "kernel", "=", "tf", ".", "reshape", "(", "kernel", ",", "[", "1", ",", "-", "1", "]", ")", "\n", "bias", "=", "tf", ".", "stop_gradient", "(", "weights", "[", "'bias'", "]", ")", "\n", "bias", "=", "tf", ".", "reshape", "(", "bias", ",", "[", "1", ",", "-", "1", "]", ")", "\n", "weight_state", "=", "tf", ".", "concat", "(", "(", "kernel", ",", "bias", ")", ",", "-", "1", ")", "\n", "weight_state", "=", "tf", ".", "layers", ".", "dense", "(", "name", "=", "'TS_Dense1'", ",", "inputs", "=", "weight_state", ",", "units", "=", "64", ",", "activation", "=", "tf", ".", "nn", ".", "relu", ",", "kernel_initializer", "=", "tf_utils", ".", "orthogonal_initializer", "(", "np", ".", "sqrt", "(", "2", ")", ")", ")", "\n", "weight_state", "=", "tf", ".", "reshape", "(", "weight_state", ",", "[", "-", "1", "]", ")", "\n", "input", "=", "tf", ".", "layers", ".", "flatten", "(", "input", ")", "\n", "input", "=", "tf", ".", "map_fn", "(", "fn", "=", "lambda", "b", ":", "tf", ".", "concat", "(", "(", "b", ",", "weight_state", ")", ",", "-", "1", ")", ",", "elems", "=", "input", ")", "\n", "# Update keys", "\n", "self", ".", "_update_keys", "(", "variable_scope", ".", "name", ",", "share_trainables", ")", "\n", "# Return result", "\n", "return", "input", "\n", "\n"]], "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.actor_critic.openai_small_network.OpenAISmall_Network._concat_layer": [[48, 61], ["tensorflow.variable_scope", "print", "tensorflow.layers.flatten", "tensorflow.layers.dense", "openai_small_network.OpenAISmall_Network._update_keys", "tensorflow.layers.flatten", "tensorflow.concat", "tensorflow.layers.flatten.get_shape", "utils.orthogonal_initializer", "numpy.sqrt"], "methods", ["home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.utils.misc.flatten", "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.network.network.Network._update_keys", "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.utils.misc.flatten", "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.utils.tensorflow_utils.orthogonal_initializer"], ["", "", "def", "_concat_layer", "(", "self", ",", "input", ",", "concat", ",", "scope", ",", "name", "=", "\"\"", ",", "share_trainables", "=", "True", ")", ":", "\n", "\t\t", "layer_type", "=", "'Concat'", "\n", "with", "tf", ".", "variable_scope", "(", "\"{}/{}{}\"", ".", "format", "(", "scope", ",", "layer_type", ",", "name", ")", ",", "reuse", "=", "tf", ".", "AUTO_REUSE", ")", "as", "variable_scope", ":", "\n", "\t\t\t", "print", "(", "\"\t[{}]Building or reusing scope: {}\"", ".", "format", "(", "self", ".", "id", ",", "variable_scope", ".", "name", ")", ")", "\n", "input", "=", "tf", ".", "layers", ".", "flatten", "(", "input", ")", "\n", "if", "concat", ".", "get_shape", "(", ")", "[", "-", "1", "]", ">", "0", ":", "\n", "\t\t\t\t", "concat", "=", "tf", ".", "layers", ".", "flatten", "(", "concat", ")", "\n", "input", "=", "tf", ".", "concat", "(", "[", "input", ",", "concat", "]", ",", "-", "1", ")", "# shape: (batch, concat_size+units)", "\n", "", "input", "=", "tf", ".", "layers", ".", "dense", "(", "name", "=", "'Concat_Dense1'", ",", "inputs", "=", "input", ",", "units", "=", "256", ",", "activation", "=", "tf", ".", "nn", ".", "relu", ",", "kernel_initializer", "=", "tf_utils", ".", "orthogonal_initializer", "(", "np", ".", "sqrt", "(", "2", ")", ")", ")", "\n", "# Update keys", "\n", "self", ".", "_update_keys", "(", "variable_scope", ".", "name", ",", "share_trainables", ")", "\n", "# Return result", "\n", "return", "input", "\n", "\n"]], "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.actor_critic.openai_small_network.OpenAISmall_Network._value_layer": [[62, 91], ["tensorflow.variable_scope", "print", "openai_small_network.OpenAISmall_Network._update_keys", "tensorflow.layers.dense", "sum", "sum", "tensorflow.stack", "tensorflow.transpose", "tensorflow.stack", "tensorflow.transpose", "tensorflow.layers.flatten", "max", "tensorflow.layers.dense", "tensorflow.reshape", "tensorflow.layers.dense", "utils.orthogonal_initializer", "range", "range", "input.get_shape().as_list", "utils.orthogonal_initializer", "utils.orthogonal_initializer", "input.get_shape"], "methods", ["home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.network.network.Network._update_keys", "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.utils.segment_tree.SumSegmentTree.sum", "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.utils.segment_tree.SumSegmentTree.sum", "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.utils.misc.flatten", "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.utils.segment_tree.MaxSegmentTree.max", "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.utils.tensorflow_utils.orthogonal_initializer", "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.utils.tensorflow_utils.orthogonal_initializer", "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.utils.tensorflow_utils.orthogonal_initializer"], ["", "", "def", "_value_layer", "(", "self", ",", "input", ",", "scope", ",", "name", "=", "\"\"", ",", "share_trainables", "=", "True", ")", ":", "\n", "\t\t", "layer_type", "=", "'Value'", "\n", "with", "tf", ".", "variable_scope", "(", "\"{}/{}{}\"", ".", "format", "(", "scope", ",", "layer_type", ",", "name", ")", ",", "reuse", "=", "tf", ".", "AUTO_REUSE", ")", "as", "variable_scope", ":", "\n", "\t\t\t", "print", "(", "\"\t[{}]Building or reusing scope: {}\"", ".", "format", "(", "self", ".", "id", ",", "variable_scope", ".", "name", ")", ")", "\n", "input", "=", "input", "+", "tf", ".", "layers", ".", "dense", "(", "name", "=", "'Value_Dense1'", ",", "inputs", "=", "input", ",", "units", "=", "input", ".", "get_shape", "(", ")", ".", "as_list", "(", ")", "[", "-", "1", "]", ",", "activation", "=", "tf", ".", "nn", ".", "relu", ",", "kernel_initializer", "=", "tf_utils", ".", "orthogonal_initializer", "(", "0.1", ")", ")", "\n", "if", "self", ".", "qvalue_estimation", ":", "\n", "\t\t\t\t", "policy_depth", "=", "sum", "(", "h", "[", "'depth'", "]", "for", "h", "in", "self", ".", "policy_heads", ")", "\n", "policy_size", "=", "sum", "(", "h", "[", "'size'", "]", "for", "h", "in", "self", ".", "policy_heads", ")", "\n", "units", "=", "policy_size", "*", "max", "(", "1", ",", "policy_depth", ")", "\n", "output", "=", "[", "\n", "tf", ".", "layers", ".", "dense", "(", "name", "=", "'Value_Q{}_Dense1'", ".", "format", "(", "i", ")", ",", "inputs", "=", "input", ",", "units", "=", "units", ",", "activation", "=", "None", ",", "kernel_initializer", "=", "tf_utils", ".", "orthogonal_initializer", "(", "0.01", ")", ")", "\n", "for", "i", "in", "range", "(", "self", ".", "value_count", ")", "\n", "]", "\n", "output", "=", "tf", ".", "stack", "(", "output", ")", "\n", "output", "=", "tf", ".", "transpose", "(", "output", ",", "[", "1", ",", "0", ",", "2", "]", ")", "\n", "if", "policy_size", ">", "1", "and", "policy_depth", ">", "1", ":", "\n", "\t\t\t\t\t", "output", "=", "tf", ".", "reshape", "(", "output", ",", "[", "-", "1", ",", "self", ".", "value_count", ",", "policy_size", ",", "policy_depth", "]", ")", "\n", "", "", "else", ":", "\n", "\t\t\t\t", "output", "=", "[", "# Keep value heads separated", "\n", "tf", ".", "layers", ".", "dense", "(", "name", "=", "'Value_V{}_Dense1'", ".", "format", "(", "i", ")", ",", "inputs", "=", "input", ",", "units", "=", "1", ",", "activation", "=", "None", ",", "kernel_initializer", "=", "tf_utils", ".", "orthogonal_initializer", "(", "0.01", ")", ")", "\n", "for", "i", "in", "range", "(", "self", ".", "value_count", ")", "\n", "]", "\n", "output", "=", "tf", ".", "stack", "(", "output", ")", "\n", "output", "=", "tf", ".", "transpose", "(", "output", ",", "[", "1", ",", "0", ",", "2", "]", ")", "\n", "output", "=", "tf", ".", "layers", ".", "flatten", "(", "output", ")", "\n", "# update keys", "\n", "", "self", ".", "_update_keys", "(", "variable_scope", ".", "name", ",", "share_trainables", ")", "\n", "# return result", "\n", "return", "output", "\n", "\n"]], "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.actor_critic.openai_small_network.OpenAISmall_Network._policy_layer": [[92, 121], ["tensorflow.variable_scope", "print", "enumerate", "openai_small_network.OpenAISmall_Network._update_keys", "tensorflow.layers.dense", "agent.network.actor_critic.base_network.is_continuous_control", "output_list.append", "tensorflow.layers.dense", "tensorflow.layers.dense", "tensorflow.clip_by_value", "tensorflow.clip_by_value", "tensorflow.stack", "tensorflow.transpose", "tensorflow.layers.dense", "utils.orthogonal_initializer", "tensorflow.abs", "tensorflow.reshape", "input.get_shape().as_list", "utils.orthogonal_initializer", "utils.orthogonal_initializer", "utils.orthogonal_initializer", "input.get_shape"], "methods", ["home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.network.network.Network._update_keys", "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.actor_critic.base_network.is_continuous_control", "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.worker.batch.ExperienceBatch.append", "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.utils.tensorflow_utils.orthogonal_initializer", "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.utils.tensorflow_utils.orthogonal_initializer", "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.utils.tensorflow_utils.orthogonal_initializer", "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.utils.tensorflow_utils.orthogonal_initializer"], ["", "", "def", "_policy_layer", "(", "self", ",", "input", ",", "scope", ",", "name", "=", "\"\"", ",", "share_trainables", "=", "True", ")", ":", "\n", "\t\t", "layer_type", "=", "'Policy'", "\n", "with", "tf", ".", "variable_scope", "(", "\"{}/{}{}\"", ".", "format", "(", "scope", ",", "layer_type", ",", "name", ")", ",", "reuse", "=", "tf", ".", "AUTO_REUSE", ")", "as", "variable_scope", ":", "\n", "\t\t\t", "print", "(", "\"\t[{}]Building or reusing scope: {}\"", ".", "format", "(", "self", ".", "id", ",", "variable_scope", ".", "name", ")", ")", "\n", "input", "=", "input", "+", "tf", ".", "layers", ".", "dense", "(", "name", "=", "'Policy_Dense1'", ",", "inputs", "=", "input", ",", "units", "=", "input", ".", "get_shape", "(", ")", ".", "as_list", "(", ")", "[", "-", "1", "]", ",", "activation", "=", "tf", ".", "nn", ".", "relu", ",", "kernel_initializer", "=", "tf_utils", ".", "orthogonal_initializer", "(", "0.1", ")", ")", "\n", "output_list", "=", "[", "]", "\n", "for", "h", ",", "policy_head", "in", "enumerate", "(", "self", ".", "policy_heads", ")", ":", "\n", "\t\t\t\t", "policy_depth", "=", "policy_head", "[", "'depth'", "]", "\n", "policy_size", "=", "policy_head", "[", "'size'", "]", "\n", "if", "is_continuous_control", "(", "policy_depth", ")", ":", "\n", "# build mean", "\n", "\t\t\t\t\t", "mu", "=", "tf", ".", "layers", ".", "dense", "(", "name", "=", "'Policy_Mu_Dense{}'", ".", "format", "(", "h", ")", ",", "inputs", "=", "input", ",", "units", "=", "policy_size", ",", "activation", "=", "None", ",", "kernel_initializer", "=", "tf_utils", ".", "orthogonal_initializer", "(", "0.01", ")", ")", "# in (-inf,inf)", "\n", "# build standard deviation", "\n", "sigma", "=", "tf", ".", "layers", ".", "dense", "(", "name", "=", "'Policy_Sigma_Dense{}'", ".", "format", "(", "h", ")", ",", "inputs", "=", "input", ",", "units", "=", "policy_size", ",", "activation", "=", "None", ",", "kernel_initializer", "=", "tf_utils", ".", "orthogonal_initializer", "(", "0.01", ")", ")", "# in (-inf,inf)", "\n", "# clip mu and sigma to avoid numerical instabilities", "\n", "clipped_mu", "=", "tf", ".", "clip_by_value", "(", "mu", ",", "-", "1", ",", "1", ")", "# in [-1,1]", "\n", "clipped_sigma", "=", "tf", ".", "clip_by_value", "(", "tf", ".", "abs", "(", "sigma", ")", ",", "1e-4", ",", "1", ")", "# in [1e-4,1] # sigma must be greater than 0", "\n", "# build policy batch", "\n", "policy_batch", "=", "tf", ".", "stack", "(", "[", "clipped_mu", ",", "clipped_sigma", "]", ")", "\n", "policy_batch", "=", "tf", ".", "transpose", "(", "policy_batch", ",", "[", "1", ",", "0", ",", "2", "]", ")", "\n", "", "else", ":", "# discrete control", "\n", "\t\t\t\t\t", "policy_batch", "=", "tf", ".", "layers", ".", "dense", "(", "name", "=", "'Policy_Logits_Dense{}'", ".", "format", "(", "h", ")", ",", "inputs", "=", "input", ",", "units", "=", "policy_size", "*", "policy_depth", ",", "activation", "=", "None", ",", "kernel_initializer", "=", "tf_utils", ".", "orthogonal_initializer", "(", "0.01", ")", ")", "\n", "if", "policy_size", ">", "1", ":", "\n", "\t\t\t\t\t\t", "policy_batch", "=", "tf", ".", "reshape", "(", "policy_batch", ",", "[", "-", "1", ",", "policy_size", ",", "policy_depth", "]", ")", "\n", "", "", "output_list", ".", "append", "(", "policy_batch", ")", "\n", "# update keys", "\n", "", "self", ".", "_update_keys", "(", "variable_scope", ".", "name", ",", "share_trainables", ")", "\n", "# return result", "\n", "return", "output_list", "\n", "\n"]], "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.actor_critic.openai_small_network.OpenAISmall_Network._rnn_layer": [[122, 137], ["utils.rnn.RNN", "utils.rnn.RNN.state_placeholder", "utils.rnn.RNN.default_state", "tensorflow.variable_scope", "print", "utils.rnn.RNN.process_batches", "openai_small_network.OpenAISmall_Network._update_keys"], "methods", ["home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.rnn.rnn.RNN.state_placeholder", "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.rnn.rnn.RNN.default_state", "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.rnn.rnn.RNN.process_batches", "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.network.network.Network._update_keys"], ["", "", "def", "_rnn_layer", "(", "self", ",", "input", ",", "scope", ",", "name", "=", "\"\"", ",", "share_trainables", "=", "True", ")", ":", "\n", "\t\t", "rnn", "=", "RNN", "(", "type", "=", "'GRU'", ",", "direction", "=", "1", ",", "units", "=", "256", ",", "batch_size", "=", "1", ",", "stack_size", "=", "1", ",", "training", "=", "self", ".", "training", ",", "dtype", "=", "flags", ".", "parameters_type", ")", "\n", "internal_initial_state", "=", "rnn", ".", "state_placeholder", "(", "name", "=", "\"initial_lstm_state\"", ")", "# for stateful lstm", "\n", "internal_default_state", "=", "rnn", ".", "default_state", "(", ")", "\n", "layer_type", "=", "rnn", ".", "type", "\n", "with", "tf", ".", "variable_scope", "(", "\"{}/{}{}\"", ".", "format", "(", "scope", ",", "layer_type", ",", "name", ")", ",", "reuse", "=", "tf", ".", "AUTO_REUSE", ")", "as", "variable_scope", ":", "\n", "\t\t\t", "print", "(", "\"\t[{}]Building or reusing scope: {}\"", ".", "format", "(", "self", ".", "id", ",", "variable_scope", ".", "name", ")", ")", "\n", "output", ",", "internal_final_state", "=", "rnn", ".", "process_batches", "(", "\n", "input", "=", "input", ",", "\n", "initial_states", "=", "internal_initial_state", ",", "\n", "sizes", "=", "self", ".", "size_batch", "\n", ")", "\n", "# Update keys", "\n", "self", ".", "_update_keys", "(", "variable_scope", ".", "name", ",", "share_trainables", ")", "\n", "return", "output", ",", "(", "internal_initial_state", ",", "internal_default_state", ",", "internal_final_state", ")", "\n", "", "", "", ""]], "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.actor_critic.base_network.Base_Network.__init__": [[14, 32], ["agent.network.network.Network.__init__", "eval", "len", "len", "s.get_shape", "s.get_shape"], "methods", ["home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.worker.batch.CompositeBatch.__init__"], ["\t", "def", "__init__", "(", "self", ",", "id", ",", "qvalue_estimation", ",", "policy_heads", ",", "batch_dict", ",", "scope_dict", ",", "training", "=", "True", ",", "value_count", "=", "1", ",", "state_scaler", "=", "1", ")", ":", "\n", "\t\t", "super", "(", ")", ".", "__init__", "(", "id", ",", "training", ")", "\n", "self", ".", "value_count", "=", "value_count", "\n", "# scope names", "\n", "self", ".", "scope_name", "=", "scope_dict", "[", "'self'", "]", "\n", "self", ".", "parent_scope_name", "=", "scope_dict", "[", "'parent'", "]", "\n", "self", ".", "sibling_scope_name", "=", "scope_dict", "[", "'sibling'", "]", "\n", "# Shape network", "\n", "states", "=", "batch_dict", "[", "'state'", "]", "\n", "self", ".", "state_batch", "=", "[", "s", "for", "s", "in", "states", "if", "len", "(", "s", ".", "get_shape", "(", ")", ")", "==", "4", "]", "\n", "self", ".", "concat_batch", "=", "[", "s", "for", "s", "in", "states", "if", "len", "(", "s", ".", "get_shape", "(", ")", ")", "!=", "4", "]", "\n", "self", ".", "size_batch", "=", "batch_dict", "[", "'size'", "]", "\n", "if", "flags", ".", "intrinsic_reward", ":", "\n", "\t\t\t", "self", ".", "training_state", "=", "batch_dict", "[", "'training_state'", "]", "\n", "", "self", ".", "parameters_type", "=", "eval", "(", "'tf.{}'", ".", "format", "(", "flags", ".", "parameters_type", ")", ")", "\n", "self", ".", "policy_heads", "=", "policy_heads", "\n", "self", ".", "qvalue_estimation", "=", "qvalue_estimation", "\n", "self", ".", "state_scaler", "=", "state_scaler", "\n", "\n"]], "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.actor_critic.base_network.Base_Network.build": [[33, 71], ["print", "print", "print", "print", "tensorflow.concat", "print", "base_network.Base_Network._cnn_layer", "tensorflow.layers.flatten", "base_network.Base_Network._weights_layer", "print", "len", "tensorflow.concat", "base_network.Base_Network._concat_layer", "print", "base_network.Base_Network._rnn_layer", "print", "base_network.Base_Network._policy_layer", "base_network.Base_Network._value_layer", "enumerate", "base_network.Base_Network.get_shape", "base_network.Base_Network.get_shape", "base_network.Base_Network.get_shape", "base_network.Base_Network.get_shape"], "methods", ["home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.actor_critic.hybrid_towers_network.HybridTowers_Network._cnn_layer", "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.utils.misc.flatten", "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.actor_critic.base_network.Base_Network._weights_layer", "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.actor_critic.base_network.Base_Network._concat_layer", "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.actor_critic.base_network.Base_Network._rnn_layer", "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.actor_critic.base_network.Base_Network._policy_layer", "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.actor_critic.base_network.Base_Network._value_layer"], ["", "def", "build", "(", "self", ",", "has_actor", "=", "True", ",", "has_critic", "=", "True", ",", "use_internal_state", "=", "True", ",", "name", "=", "'default'", ")", ":", "\n", "\t\t", "print", "(", "\"\t[{}]Building partition {} with has_actor={}, has_critic={}, use_internal_state={}\"", ".", "format", "(", "self", ".", "id", ",", "name", ",", "has_actor", ",", "has_critic", ",", "use_internal_state", ")", ")", "\n", "print", "(", "\"\t[{}]Parameters type: {}\"", ".", "format", "(", "self", ".", "id", ",", "flags", ".", "parameters_type", ")", ")", "\n", "print", "(", "\"\t[{}]Algorithm: {}\"", ".", "format", "(", "self", ".", "id", ",", "flags", ".", "algorithm", ")", ")", "\n", "print", "(", "\"\t[{}]Network configuration: {}\"", ".", "format", "(", "self", ".", "id", ",", "flags", ".", "network_configuration", ")", ")", "\n", "# [CNN]", "\n", "input", "=", "[", "\n", "self", ".", "_cnn_layer", "(", "name", "=", "i", ",", "input", "=", "substate_batch", "/", "self", ".", "state_scaler", ",", "scope", "=", "self", ".", "parent_scope_name", ")", "\n", "for", "i", ",", "substate_batch", "in", "enumerate", "(", "self", ".", "state_batch", ")", "\n", "]", "\n", "input", "=", "[", "\n", "tf", ".", "layers", ".", "flatten", "(", "i", ")", "\n", "for", "i", "in", "input", "\n", "]", "\n", "input", "=", "tf", ".", "concat", "(", "input", ",", "-", "1", ")", "\n", "print", "(", "\"\t[{}]CNN layer output shape: {}\"", ".", "format", "(", "self", ".", "id", ",", "input", ".", "get_shape", "(", ")", ")", ")", "\n", "# [Training state]", "\n", "if", "flags", ".", "intrinsic_reward", ":", "\n", "\t\t\t", "input", "=", "self", ".", "_weights_layer", "(", "input", "=", "input", ",", "weights", "=", "self", ".", "training_state", ",", "scope", "=", "self", ".", "scope_name", ")", "\n", "print", "(", "\"\t[{}]Weights layer output shape: {}\"", ".", "format", "(", "self", ".", "id", ",", "input", ".", "get_shape", "(", ")", ")", ")", "\n", "# [Concat]", "\n", "", "if", "len", "(", "self", ".", "concat_batch", ")", ">", "0", ":", "\n", "\t\t\t", "self", ".", "concat_batch", "=", "tf", ".", "concat", "(", "self", ".", "concat_batch", ",", "-", "1", ")", "\n", "input", "=", "self", ".", "_concat_layer", "(", "input", "=", "input", ",", "concat", "=", "self", ".", "concat_batch", ",", "scope", "=", "self", ".", "scope_name", ")", "\n", "print", "(", "\"\t[{}]Concat layer output shape: {}\"", ".", "format", "(", "self", ".", "id", ",", "input", ".", "get_shape", "(", ")", ")", ")", "\n", "# [RNN]", "\n", "", "if", "use_internal_state", ":", "\n", "\t\t\t", "self", ".", "use_internal_state", "=", "True", "\n", "input", ",", "internal_state_tuple", "=", "self", ".", "_rnn_layer", "(", "input", "=", "input", ",", "scope", "=", "self", ".", "scope_name", ")", "\n", "self", ".", "internal_initial_state", ",", "self", ".", "internal_default_state", ",", "self", ".", "internal_final_state", "=", "internal_state_tuple", "\n", "print", "(", "\"\t[{}]RNN layer output shape: {}\"", ".", "format", "(", "self", ".", "id", ",", "input", ".", "get_shape", "(", ")", ")", ")", "\n", "# [Policy]", "\n", "", "self", ".", "policy_batch", "=", "self", ".", "_policy_layer", "(", "input", "=", "input", ",", "scope", "=", "self", ".", "scope_name", ")", "if", "has_actor", "else", "None", "\n", "# print( \"\t[{}]Policy shape: {}\".format(self.id, self.policy_batch.get_shape()) )", "\n", "# [Value]", "\n", "self", ".", "value_batch", "=", "self", ".", "_value_layer", "(", "input", "=", "input", ",", "scope", "=", "self", ".", "scope_name", ")", "if", "has_critic", "else", "None", "\n", "# print( \"\t[{}]Value shape: {}\".format(self.id, self.value_batch.get_shape()) )", "\n", "return", "self", ".", "policy_batch", ",", "self", ".", "value_batch", "\n", "\n"]], "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.actor_critic.base_network.Base_Network._cnn_layer": [[72, 83], ["tensorflow.variable_scope", "print", "tensorflow.layers.conv2d", "tensorflow.layers.conv2d", "tensorflow.layers.conv2d", "base_network.Base_Network._update_keys"], "methods", ["home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.network.network.Network._update_keys"], ["", "def", "_cnn_layer", "(", "self", ",", "input", ",", "scope", ",", "name", "=", "\"\"", ",", "share_trainables", "=", "True", ")", ":", "\n", "\t\t", "layer_type", "=", "'CNN'", "\n", "with", "tf", ".", "variable_scope", "(", "\"{}/{}{}\"", ".", "format", "(", "scope", ",", "layer_type", ",", "name", ")", ",", "reuse", "=", "tf", ".", "AUTO_REUSE", ")", "as", "variable_scope", ":", "\n", "\t\t\t", "print", "(", "\"\t[{}]Building or reusing scope: {}\"", ".", "format", "(", "self", ".", "id", ",", "variable_scope", ".", "name", ")", ")", "\n", "input", "=", "tf", ".", "layers", ".", "conv2d", "(", "name", "=", "'CNN_Conv1'", ",", "inputs", "=", "input", ",", "filters", "=", "32", ",", "kernel_size", "=", "8", ",", "strides", "=", "4", ",", "padding", "=", "'SAME'", ",", "activation", "=", "tf", ".", "nn", ".", "relu", ",", "kernel_initializer", "=", "tf", ".", "initializers", ".", "variance_scaling", ")", "\n", "input", "=", "tf", ".", "layers", ".", "conv2d", "(", "name", "=", "'CNN_Conv2'", ",", "inputs", "=", "input", ",", "filters", "=", "64", ",", "kernel_size", "=", "4", ",", "strides", "=", "2", ",", "padding", "=", "'SAME'", ",", "activation", "=", "tf", ".", "nn", ".", "relu", ",", "kernel_initializer", "=", "tf", ".", "initializers", ".", "variance_scaling", ")", "\n", "input", "=", "tf", ".", "layers", ".", "conv2d", "(", "name", "=", "'CNN_Conv3'", ",", "inputs", "=", "input", ",", "filters", "=", "64", ",", "kernel_size", "=", "4", ",", "strides", "=", "1", ",", "padding", "=", "'SAME'", ",", "activation", "=", "tf", ".", "nn", ".", "relu", ",", "kernel_initializer", "=", "tf", ".", "initializers", ".", "variance_scaling", ")", "\n", "# update keys", "\n", "self", ".", "_update_keys", "(", "variable_scope", ".", "name", ",", "share_trainables", ")", "\n", "# return result", "\n", "return", "input", "\n", "\n"]], "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.actor_critic.base_network.Base_Network._weights_layer": [[84, 101], ["tensorflow.variable_scope", "print", "tensorflow.stop_gradient", "tensorflow.layers.dense", "tensorflow.reshape", "tensorflow.stop_gradient", "tensorflow.reshape", "tensorflow.concat", "tensorflow.layers.flatten", "tensorflow.map_fn", "base_network.Base_Network._update_keys", "tensorflow.concat"], "methods", ["home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.utils.misc.flatten", "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.network.network.Network._update_keys"], ["", "", "def", "_weights_layer", "(", "self", ",", "input", ",", "weights", ",", "scope", ",", "name", "=", "\"\"", ",", "share_trainables", "=", "True", ")", ":", "\n", "\t\t", "layer_type", "=", "'Weights'", "\n", "with", "tf", ".", "variable_scope", "(", "\"{}/{}{}\"", ".", "format", "(", "scope", ",", "layer_type", ",", "name", ")", ",", "reuse", "=", "tf", ".", "AUTO_REUSE", ")", "as", "variable_scope", ":", "\n", "\t\t\t", "print", "(", "\"\t[{}]Building or reusing scope: {}\"", ".", "format", "(", "self", ".", "id", ",", "variable_scope", ".", "name", ")", ")", "\n", "kernel", "=", "tf", ".", "stop_gradient", "(", "weights", "[", "'kernel'", "]", ")", "\n", "# kernel = tf.transpose(kernel, [1, 0])", "\n", "kernel", "=", "tf", ".", "layers", ".", "dense", "(", "name", "=", "'Concat_Dense0'", ",", "inputs", "=", "kernel", ",", "units", "=", "1", ",", "activation", "=", "tf", ".", "nn", ".", "relu", ",", "kernel_initializer", "=", "tf", ".", "initializers", ".", "variance_scaling", ")", "\n", "kernel", "=", "tf", ".", "reshape", "(", "kernel", ",", "[", "-", "1", "]", ")", "\n", "bias", "=", "tf", ".", "stop_gradient", "(", "weights", "[", "'bias'", "]", ")", "\n", "bias", "=", "tf", ".", "reshape", "(", "bias", ",", "[", "-", "1", "]", ")", "\n", "weight_state", "=", "tf", ".", "concat", "(", "(", "kernel", ",", "bias", ")", ",", "-", "1", ")", "\n", "input", "=", "tf", ".", "layers", ".", "flatten", "(", "input", ")", "\n", "input", "=", "tf", ".", "map_fn", "(", "fn", "=", "lambda", "b", ":", "tf", ".", "concat", "(", "(", "b", ",", "weight_state", ")", ",", "-", "1", ")", ",", "elems", "=", "input", ")", "\n", "# Update keys", "\n", "self", ".", "_update_keys", "(", "variable_scope", ".", "name", ",", "share_trainables", ")", "\n", "# Return result", "\n", "return", "input", "\n", "\n"]], "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.actor_critic.base_network.Base_Network._concat_layer": [[102, 115], ["tensorflow.variable_scope", "print", "tensorflow.layers.flatten", "tensorflow.layers.dense", "base_network.Base_Network._update_keys", "tensorflow.layers.flatten", "tensorflow.concat", "tensorflow.layers.flatten.get_shape"], "methods", ["home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.utils.misc.flatten", "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.network.network.Network._update_keys", "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.utils.misc.flatten"], ["", "", "def", "_concat_layer", "(", "self", ",", "input", ",", "concat", ",", "scope", ",", "name", "=", "\"\"", ",", "share_trainables", "=", "True", ")", ":", "\n", "\t\t", "layer_type", "=", "'Concat'", "\n", "with", "tf", ".", "variable_scope", "(", "\"{}/{}{}\"", ".", "format", "(", "scope", ",", "layer_type", ",", "name", ")", ",", "reuse", "=", "tf", ".", "AUTO_REUSE", ")", "as", "variable_scope", ":", "\n", "\t\t\t", "print", "(", "\"\t[{}]Building or reusing scope: {}\"", ".", "format", "(", "self", ".", "id", ",", "variable_scope", ".", "name", ")", ")", "\n", "input", "=", "tf", ".", "layers", ".", "flatten", "(", "input", ")", "\n", "input", "=", "tf", ".", "layers", ".", "dense", "(", "name", "=", "'Concat_Dense1'", ",", "inputs", "=", "input", ",", "units", "=", "64", ",", "activation", "=", "tf", ".", "nn", ".", "elu", ",", "kernel_initializer", "=", "tf", ".", "initializers", ".", "variance_scaling", ")", "\n", "if", "concat", ".", "get_shape", "(", ")", "[", "-", "1", "]", ">", "0", ":", "\n", "\t\t\t\t", "concat", "=", "tf", ".", "layers", ".", "flatten", "(", "concat", ")", "\n", "input", "=", "tf", ".", "concat", "(", "[", "input", ",", "concat", "]", ",", "-", "1", ")", "# shape: (batch, concat_size+units)", "\n", "# Update keys", "\n", "", "self", ".", "_update_keys", "(", "variable_scope", ".", "name", ",", "share_trainables", ")", "\n", "# Return result", "\n", "return", "input", "\n", "\n"]], "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.actor_critic.base_network.Base_Network._value_layer": [[116, 144], ["tensorflow.variable_scope", "print", "base_network.Base_Network._update_keys", "sum", "sum", "tensorflow.stack", "tensorflow.transpose", "tensorflow.stack", "tensorflow.transpose", "tensorflow.layers.flatten", "max", "tensorflow.layers.dense", "tensorflow.reshape", "tensorflow.layers.dense", "range", "range"], "methods", ["home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.network.network.Network._update_keys", "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.utils.segment_tree.SumSegmentTree.sum", "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.utils.segment_tree.SumSegmentTree.sum", "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.utils.misc.flatten", "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.utils.segment_tree.MaxSegmentTree.max"], ["", "", "def", "_value_layer", "(", "self", ",", "input", ",", "scope", ",", "name", "=", "\"\"", ",", "share_trainables", "=", "True", ")", ":", "\n", "\t\t", "layer_type", "=", "'Value'", "\n", "with", "tf", ".", "variable_scope", "(", "\"{}/{}{}\"", ".", "format", "(", "scope", ",", "layer_type", ",", "name", ")", ",", "reuse", "=", "tf", ".", "AUTO_REUSE", ")", "as", "variable_scope", ":", "\n", "\t\t\t", "print", "(", "\"\t[{}]Building or reusing scope: {}\"", ".", "format", "(", "self", ".", "id", ",", "variable_scope", ".", "name", ")", ")", "\n", "if", "self", ".", "qvalue_estimation", ":", "\n", "\t\t\t\t", "policy_depth", "=", "sum", "(", "h", "[", "'depth'", "]", "for", "h", "in", "self", ".", "policy_heads", ")", "\n", "policy_size", "=", "sum", "(", "h", "[", "'size'", "]", "for", "h", "in", "self", ".", "policy_heads", ")", "\n", "units", "=", "policy_size", "*", "max", "(", "1", ",", "policy_depth", ")", "\n", "output", "=", "[", "\n", "tf", ".", "layers", ".", "dense", "(", "name", "=", "'Value_Q{}_Dense1'", ".", "format", "(", "i", ")", ",", "inputs", "=", "input", ",", "units", "=", "units", ",", "activation", "=", "None", ",", "kernel_initializer", "=", "tf", ".", "initializers", ".", "variance_scaling", ")", "\n", "for", "i", "in", "range", "(", "self", ".", "value_count", ")", "\n", "]", "\n", "output", "=", "tf", ".", "stack", "(", "output", ")", "\n", "output", "=", "tf", ".", "transpose", "(", "output", ",", "[", "1", ",", "0", ",", "2", "]", ")", "\n", "if", "policy_size", ">", "1", "and", "policy_depth", ">", "1", ":", "\n", "\t\t\t\t\t", "output", "=", "tf", ".", "reshape", "(", "output", ",", "[", "-", "1", ",", "self", ".", "value_count", ",", "policy_size", ",", "policy_depth", "]", ")", "\n", "", "", "else", ":", "\n", "\t\t\t\t", "output", "=", "[", "# Keep value heads separated", "\n", "tf", ".", "layers", ".", "dense", "(", "name", "=", "'Value_V{}_Dense1'", ".", "format", "(", "i", ")", ",", "inputs", "=", "input", ",", "units", "=", "1", ",", "activation", "=", "None", ",", "kernel_initializer", "=", "tf", ".", "initializers", ".", "variance_scaling", ")", "\n", "for", "i", "in", "range", "(", "self", ".", "value_count", ")", "\n", "]", "\n", "output", "=", "tf", ".", "stack", "(", "output", ")", "\n", "output", "=", "tf", ".", "transpose", "(", "output", ",", "[", "1", ",", "0", ",", "2", "]", ")", "\n", "output", "=", "tf", ".", "layers", ".", "flatten", "(", "output", ")", "\n", "# update keys", "\n", "", "self", ".", "_update_keys", "(", "variable_scope", ".", "name", ",", "share_trainables", ")", "\n", "# return result", "\n", "return", "output", "\n", "\n"]], "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.actor_critic.base_network.Base_Network._policy_layer": [[145, 173], ["tensorflow.variable_scope", "print", "enumerate", "base_network.Base_Network._update_keys", "base_network.is_continuous_control", "output_list.append", "tensorflow.layers.dense", "tensorflow.layers.dense", "tensorflow.clip_by_value", "tensorflow.clip_by_value", "tensorflow.stack", "tensorflow.transpose", "tensorflow.layers.dense", "tensorflow.abs", "tensorflow.reshape"], "methods", ["home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.network.network.Network._update_keys", "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.actor_critic.base_network.is_continuous_control", "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.worker.batch.ExperienceBatch.append"], ["", "", "def", "_policy_layer", "(", "self", ",", "input", ",", "scope", ",", "name", "=", "\"\"", ",", "share_trainables", "=", "True", ")", ":", "\n", "\t\t", "layer_type", "=", "'Policy'", "\n", "with", "tf", ".", "variable_scope", "(", "\"{}/{}{}\"", ".", "format", "(", "scope", ",", "layer_type", ",", "name", ")", ",", "reuse", "=", "tf", ".", "AUTO_REUSE", ")", "as", "variable_scope", ":", "\n", "\t\t\t", "print", "(", "\"\t[{}]Building or reusing scope: {}\"", ".", "format", "(", "self", ".", "id", ",", "variable_scope", ".", "name", ")", ")", "\n", "output_list", "=", "[", "]", "\n", "for", "h", ",", "policy_head", "in", "enumerate", "(", "self", ".", "policy_heads", ")", ":", "\n", "\t\t\t\t", "policy_depth", "=", "policy_head", "[", "'depth'", "]", "\n", "policy_size", "=", "policy_head", "[", "'size'", "]", "\n", "if", "is_continuous_control", "(", "policy_depth", ")", ":", "\n", "# build mean", "\n", "\t\t\t\t\t", "mu", "=", "tf", ".", "layers", ".", "dense", "(", "name", "=", "'Policy_Mu_Dense{}'", ".", "format", "(", "h", ")", ",", "inputs", "=", "input", ",", "units", "=", "policy_size", ",", "activation", "=", "None", ",", "kernel_initializer", "=", "tf", ".", "initializers", ".", "variance_scaling", ")", "# in (-inf,inf)", "\n", "# build standard deviation", "\n", "sigma", "=", "tf", ".", "layers", ".", "dense", "(", "name", "=", "'Policy_Sigma_Dense{}'", ".", "format", "(", "h", ")", ",", "inputs", "=", "input", ",", "units", "=", "policy_size", ",", "activation", "=", "None", ",", "kernel_initializer", "=", "tf", ".", "initializers", ".", "variance_scaling", ")", "# in (-inf,inf)", "\n", "# clip mu and sigma to avoid numerical instabilities", "\n", "clipped_mu", "=", "tf", ".", "clip_by_value", "(", "mu", ",", "-", "1", ",", "1", ")", "# in [-1,1]", "\n", "clipped_sigma", "=", "tf", ".", "clip_by_value", "(", "tf", ".", "abs", "(", "sigma", ")", ",", "1e-4", ",", "1", ")", "# in [1e-4,1] # sigma must be greater than 0", "\n", "# build policy batch", "\n", "policy_batch", "=", "tf", ".", "stack", "(", "[", "clipped_mu", ",", "clipped_sigma", "]", ")", "\n", "policy_batch", "=", "tf", ".", "transpose", "(", "policy_batch", ",", "[", "1", ",", "0", ",", "2", "]", ")", "\n", "", "else", ":", "# discrete control", "\n", "\t\t\t\t\t", "policy_batch", "=", "tf", ".", "layers", ".", "dense", "(", "name", "=", "'Policy_Logits_Dense{}'", ".", "format", "(", "h", ")", ",", "inputs", "=", "input", ",", "units", "=", "policy_size", "*", "policy_depth", ",", "activation", "=", "None", ",", "kernel_initializer", "=", "tf", ".", "initializers", ".", "variance_scaling", ")", "\n", "if", "policy_size", ">", "1", ":", "\n", "\t\t\t\t\t\t", "policy_batch", "=", "tf", ".", "reshape", "(", "policy_batch", ",", "[", "-", "1", ",", "policy_size", ",", "policy_depth", "]", ")", "\n", "", "", "output_list", ".", "append", "(", "policy_batch", ")", "\n", "# update keys", "\n", "", "self", ".", "_update_keys", "(", "variable_scope", ".", "name", ",", "share_trainables", ")", "\n", "# return result", "\n", "return", "output_list", "\n", "\n"]], "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.actor_critic.base_network.Base_Network._rnn_layer": [[174, 189], ["utils.rnn.RNN", "utils.rnn.RNN.state_placeholder", "utils.rnn.RNN.default_state", "tensorflow.variable_scope", "print", "utils.rnn.RNN.process_batches", "base_network.Base_Network._update_keys"], "methods", ["home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.rnn.rnn.RNN.state_placeholder", "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.rnn.rnn.RNN.default_state", "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.rnn.rnn.RNN.process_batches", "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.network.network.Network._update_keys"], ["", "", "def", "_rnn_layer", "(", "self", ",", "input", ",", "scope", ",", "name", "=", "\"\"", ",", "share_trainables", "=", "True", ")", ":", "\n", "\t\t", "rnn", "=", "RNN", "(", "type", "=", "'LSTM'", ",", "direction", "=", "1", ",", "units", "=", "64", ",", "batch_size", "=", "1", ",", "stack_size", "=", "1", ",", "training", "=", "self", ".", "training", ",", "dtype", "=", "flags", ".", "parameters_type", ")", "\n", "internal_initial_state", "=", "rnn", ".", "state_placeholder", "(", "name", "=", "\"initial_lstm_state\"", ")", "# for stateful lstm", "\n", "internal_default_state", "=", "rnn", ".", "default_state", "(", ")", "\n", "layer_type", "=", "rnn", ".", "type", "\n", "with", "tf", ".", "variable_scope", "(", "\"{}/{}{}\"", ".", "format", "(", "scope", ",", "layer_type", ",", "name", ")", ",", "reuse", "=", "tf", ".", "AUTO_REUSE", ")", "as", "variable_scope", ":", "\n", "\t\t\t", "print", "(", "\"\t[{}]Building or reusing scope: {}\"", ".", "format", "(", "self", ".", "id", ",", "variable_scope", ".", "name", ")", ")", "\n", "output", ",", "internal_final_state", "=", "rnn", ".", "process_batches", "(", "\n", "input", "=", "input", ",", "\n", "initial_states", "=", "internal_initial_state", ",", "\n", "sizes", "=", "self", ".", "size_batch", "\n", ")", "\n", "# Update keys", "\n", "self", ".", "_update_keys", "(", "variable_scope", ".", "name", ",", "share_trainables", ")", "\n", "return", "output", ",", "(", "[", "internal_initial_state", "]", ",", "[", "internal_default_state", "]", ",", "[", "internal_final_state", "]", ")", "\n", "", "", "", ""]], "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.actor_critic.base_network.is_continuous_control": [[10, 12], ["None"], "function", ["None"], ["def", "is_continuous_control", "(", "policy_depth", ")", ":", "\n", "\t", "return", "policy_depth", "<=", "1", "\n", "\n"]], "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.actor_critic.hybrid_towers_network.HybridTowers_Network._cnn_layer": [[8, 25], ["tensorflow.layers.flatten.get_shape().as_list", "tensorflow.variable_scope", "print", "tensorflow.layers.conv2d", "tensorflow.layers.conv2d", "tensorflow.layers.max_pooling2d", "tensorflow.layers.flatten", "tensorflow.layers.conv2d", "tensorflow.layers.conv2d", "tensorflow.layers.flatten", "tensorflow.concat", "hybrid_towers_network.HybridTowers_Network._update_keys", "tensorflow.layers.flatten.get_shape"], "methods", ["home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.utils.misc.flatten", "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.utils.misc.flatten", "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.network.network.Network._update_keys"], ["\t", "def", "_cnn_layer", "(", "self", ",", "input", ",", "scope", ",", "name", "=", "\"\"", ",", "share_trainables", "=", "True", ")", ":", "\n", "\t\t", "input_shape", "=", "input", ".", "get_shape", "(", ")", ".", "as_list", "(", ")", "\n", "layer_type", "=", "'CNN'", "\n", "with", "tf", ".", "variable_scope", "(", "\"{}/{}{}\"", ".", "format", "(", "scope", ",", "layer_type", ",", "name", ")", ",", "reuse", "=", "tf", ".", "AUTO_REUSE", ")", "as", "variable_scope", ":", "\n", "\t\t\t", "print", "(", "\"    [{}]Building or reusing scope: {}\"", ".", "format", "(", "self", ".", "id", ",", "variable_scope", ".", "name", ")", ")", "\n", "tower1", "=", "tf", ".", "layers", ".", "conv2d", "(", "name", "=", "'CNN_Tower1_Conv1'", ",", "inputs", "=", "input", ",", "filters", "=", "64", ",", "kernel_size", "=", "(", "3", ",", "3", ")", ",", "strides", "=", "(", "1", ",", "1", ")", ",", "padding", "=", "'SAME'", ",", "activation", "=", "tf", ".", "nn", ".", "relu", ",", "kernel_initializer", "=", "tf", ".", "initializers", ".", "variance_scaling", ")", "\n", "tower1", "=", "tf", ".", "layers", ".", "conv2d", "(", "name", "=", "'CNN_Tower1_Conv2'", ",", "inputs", "=", "tower1", ",", "filters", "=", "32", ",", "kernel_size", "=", "(", "3", ",", "3", ")", ",", "strides", "=", "(", "1", ",", "1", ")", ",", "padding", "=", "'SAME'", ",", "activation", "=", "tf", ".", "nn", ".", "relu", ",", "kernel_initializer", "=", "tf", ".", "initializers", ".", "variance_scaling", ")", "\n", "tower1", "=", "tf", ".", "layers", ".", "max_pooling2d", "(", "tower1", ",", "pool_size", "=", "(", "input_shape", "[", "1", "]", ",", "input_shape", "[", "2", "]", ")", ",", "strides", "=", "(", "input_shape", "[", "1", "]", ",", "input_shape", "[", "2", "]", ")", ")", "\n", "tower1", "=", "tf", ".", "layers", ".", "flatten", "(", "tower1", ")", "\n", "input", "=", "tf", ".", "layers", ".", "conv2d", "(", "name", "=", "'CNN_Tower2_Conv1'", ",", "inputs", "=", "input", ",", "filters", "=", "16", ",", "kernel_size", "=", "(", "3", ",", "3", ")", ",", "padding", "=", "'SAME'", ",", "activation", "=", "tf", ".", "nn", ".", "relu", ",", "kernel_initializer", "=", "tf", ".", "initializers", ".", "variance_scaling", ")", "\n", "input", "=", "tf", ".", "layers", ".", "conv2d", "(", "name", "=", "'CNN_Tower2_Conv2'", ",", "inputs", "=", "input", ",", "filters", "=", "8", ",", "kernel_size", "=", "(", "3", ",", "3", ")", ",", "padding", "=", "'SAME'", ",", "activation", "=", "tf", ".", "nn", ".", "relu", ",", "kernel_initializer", "=", "tf", ".", "initializers", ".", "variance_scaling", ")", "\n", "input", "=", "tf", ".", "layers", ".", "flatten", "(", "input", ")", "\n", "concat", "=", "tf", ".", "concat", "(", "[", "tower1", ",", "input", "]", ",", "axis", "=", "-", "1", ")", "\n", "# update keys", "\n", "self", ".", "_update_keys", "(", "variable_scope", ".", "name", ",", "share_trainables", ")", "\n", "# return result", "\n", "return", "concat", "", "", "", "", ""]], "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.intrinsic_reward.intrinsic_reward_network.IntrinsicReward_Network.__init__": [[8, 15], ["agent.network.network.Network.__init__"], "methods", ["home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.worker.batch.CompositeBatch.__init__"], ["\t", "def", "__init__", "(", "self", ",", "id", ",", "batch_dict", ",", "scope_dict", ",", "training", "=", "True", ")", ":", "\n", "\t\t", "super", "(", ")", ".", "__init__", "(", "id", ",", "training", ")", "\n", "self", ".", "scope_name", "=", "scope_dict", "[", "'self'", "]", "\n", "# Shape network", "\n", "self", ".", "state_batch", "=", "batch_dict", "[", "'state'", "]", "\n", "self", ".", "state_mean_batch", "=", "batch_dict", "[", "'state_mean'", "]", "\n", "self", ".", "state_std_batch", "=", "batch_dict", "[", "'state_std'", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.intrinsic_reward.intrinsic_reward_network.IntrinsicReward_Network.build": [[16, 38], ["tensorflow.clip_by_value", "intrinsic_reward_network.IntrinsicReward_Network._intrinsic_reward_layer", "tensorflow.stop_gradient", "tensorflow.reduce_mean", "tensorflow.reduce_mean", "tensorflow.reshape", "tensorflow.squared_difference", "tensorflow.nn.dropout"], "methods", ["home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.intrinsic_reward.intrinsic_reward_network.IntrinsicReward_Network._intrinsic_reward_layer"], ["", "def", "build", "(", "self", ")", ":", "\n", "# Use state_batch instead of new_state_batch, to save memory", "\n", "\t\t", "normalized_state_batch", "=", "(", "self", ".", "state_batch", "[", "0", "]", "-", "self", ".", "state_mean_batch", "[", "0", "]", "[", "-", "1", "]", ")", "/", "self", ".", "state_std_batch", "[", "0", "]", "[", "-", "1", "]", "\n", "# normalized_state_batch = normalized_state_batch[:, :, :, -1:]", "\n", "normalized_state_batch", "=", "tf", ".", "clip_by_value", "(", "normalized_state_batch", ",", "-", "5.0", ",", "5.0", ")", "\n", "# Build layer", "\n", "target", ",", "prediction", ",", "training_state", "=", "self", ".", "_intrinsic_reward_layer", "(", "normalized_state_batch", ",", "scope", "=", "self", ".", "scope_name", ")", "\n", "noisy_target", "=", "tf", ".", "stop_gradient", "(", "target", ")", "\n", "#=======================================================================", "\n", "# # Get feature variance", "\n", "# feature_variance = tf.reduce_mean(tf.nn.moments(target, axes=[0])[1])", "\n", "# # Get maximum feature", "\n", "# max_feature = tf.reduce_max(tf.abs(target))", "\n", "#=======================================================================", "\n", "# Get intrinsic reward", "\n", "intrinsic_reward", "=", "tf", ".", "reduce_mean", "(", "tf", ".", "squared_difference", "(", "noisy_target", ",", "prediction", ")", ",", "axis", "=", "-", "1", ")", "\n", "# Get loss", "\n", "loss", "=", "tf", ".", "reduce_mean", "(", "tf", ".", "nn", ".", "dropout", "(", "intrinsic_reward", ",", "0.5", ")", ")", "\n", "# loss = tf.reduce_mean(intrinsic_reward)", "\n", "# Return results", "\n", "intrinsic_reward", "=", "tf", ".", "reshape", "(", "intrinsic_reward", ",", "[", "-", "1", "]", ")", "\n", "return", "intrinsic_reward", ",", "loss", ",", "training_state", "\n", "\n"]], "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.intrinsic_reward.intrinsic_reward_network.IntrinsicReward_Network._intrinsic_reward_layer": [[39, 67], ["tensorflow.variable_scope", "print", "tensorflow.layers.conv2d", "tensorflow.layers.conv2d", "tensorflow.layers.conv2d", "tensorflow.layers.flatten", "tensorflow.layers.dense", "tensorflow.layers.conv2d", "tensorflow.layers.conv2d", "tensorflow.layers.conv2d", "tensorflow.layers.flatten", "tensorflow.layers.dense", "tensorflow.layers.dense", "tensorflow.layers.dense", "intrinsic_reward_network.IntrinsicReward_Network._update_keys", "tensorflow.variable_scope", "utils.orthogonal_initializer", "utils.orthogonal_initializer", "utils.orthogonal_initializer", "utils.orthogonal_initializer", "utils.orthogonal_initializer", "utils.orthogonal_initializer", "utils.orthogonal_initializer", "utils.orthogonal_initializer", "utils.orthogonal_initializer", "utils.orthogonal_initializer", "tensorflow.get_variable", "tensorflow.get_variable", "numpy.sqrt", "numpy.sqrt", "numpy.sqrt", "numpy.sqrt", "numpy.sqrt", "numpy.sqrt", "numpy.sqrt", "numpy.sqrt", "numpy.sqrt", "numpy.sqrt"], "methods", ["home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.utils.misc.flatten", "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.utils.misc.flatten", "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.network.network.Network._update_keys", "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.utils.tensorflow_utils.orthogonal_initializer", "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.utils.tensorflow_utils.orthogonal_initializer", "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.utils.tensorflow_utils.orthogonal_initializer", "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.utils.tensorflow_utils.orthogonal_initializer", "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.utils.tensorflow_utils.orthogonal_initializer", "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.utils.tensorflow_utils.orthogonal_initializer", "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.utils.tensorflow_utils.orthogonal_initializer", "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.utils.tensorflow_utils.orthogonal_initializer", "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.utils.tensorflow_utils.orthogonal_initializer", "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.utils.tensorflow_utils.orthogonal_initializer"], ["", "def", "_intrinsic_reward_layer", "(", "self", ",", "input", ",", "scope", ",", "name", "=", "\"\"", ",", "share_trainables", "=", "True", ")", ":", "\n", "\t\t", "layer_type", "=", "'RandomNetworkDistillation'", "\n", "with", "tf", ".", "variable_scope", "(", "\"{}/{}{}\"", ".", "format", "(", "scope", ",", "layer_type", ",", "name", ")", ",", "reuse", "=", "tf", ".", "AUTO_REUSE", ")", "as", "variable_scope", ":", "\n", "\t\t\t", "print", "(", "\"\t[{}]Building or reusing scope: {}\"", ".", "format", "(", "self", ".", "id", ",", "variable_scope", ".", "name", ")", ")", "\n", "# Here we use leaky_relu instead of relu as activation function", "\n", "# Target network", "\n", "target", "=", "tf", ".", "layers", ".", "conv2d", "(", "name", "=", "'RND_Target_Conv1'", ",", "inputs", "=", "input", ",", "filters", "=", "32", ",", "kernel_size", "=", "8", ",", "strides", "=", "4", ",", "padding", "=", "'SAME'", ",", "activation", "=", "tf", ".", "nn", ".", "leaky_relu", ",", "kernel_initializer", "=", "tf_utils", ".", "orthogonal_initializer", "(", "np", ".", "sqrt", "(", "2", ")", ")", ")", "\n", "target", "=", "tf", ".", "layers", ".", "conv2d", "(", "name", "=", "'RND_Target_Conv2'", ",", "inputs", "=", "target", ",", "filters", "=", "64", ",", "kernel_size", "=", "4", ",", "strides", "=", "2", ",", "padding", "=", "'SAME'", ",", "activation", "=", "tf", ".", "nn", ".", "leaky_relu", ",", "kernel_initializer", "=", "tf_utils", ".", "orthogonal_initializer", "(", "np", ".", "sqrt", "(", "2", ")", ")", ")", "\n", "target", "=", "tf", ".", "layers", ".", "conv2d", "(", "name", "=", "'RND_Target_Conv3'", ",", "inputs", "=", "target", ",", "filters", "=", "64", ",", "kernel_size", "=", "3", ",", "strides", "=", "1", ",", "padding", "=", "'SAME'", ",", "activation", "=", "tf", ".", "nn", ".", "leaky_relu", ",", "kernel_initializer", "=", "tf_utils", ".", "orthogonal_initializer", "(", "np", ".", "sqrt", "(", "2", ")", ")", ")", "\n", "target", "=", "tf", ".", "layers", ".", "flatten", "(", "target", ")", "\n", "target", "=", "tf", ".", "layers", ".", "dense", "(", "name", "=", "'RND_Target_Dense1'", ",", "inputs", "=", "target", ",", "units", "=", "512", ",", "activation", "=", "None", ",", "kernel_initializer", "=", "tf_utils", ".", "orthogonal_initializer", "(", "np", ".", "sqrt", "(", "2", ")", ")", ")", "\n", "# Predictor network", "\n", "prediction", "=", "tf", ".", "layers", ".", "conv2d", "(", "name", "=", "'RND_Prediction_Conv1'", ",", "inputs", "=", "input", ",", "filters", "=", "32", ",", "kernel_size", "=", "8", ",", "strides", "=", "4", ",", "padding", "=", "'SAME'", ",", "activation", "=", "tf", ".", "nn", ".", "leaky_relu", ",", "kernel_initializer", "=", "tf_utils", ".", "orthogonal_initializer", "(", "np", ".", "sqrt", "(", "2", ")", ")", ")", "\n", "prediction", "=", "tf", ".", "layers", ".", "conv2d", "(", "name", "=", "'RND_Prediction_Conv2'", ",", "inputs", "=", "prediction", ",", "filters", "=", "64", ",", "kernel_size", "=", "4", ",", "strides", "=", "2", ",", "padding", "=", "'SAME'", ",", "activation", "=", "tf", ".", "nn", ".", "leaky_relu", ",", "kernel_initializer", "=", "tf_utils", ".", "orthogonal_initializer", "(", "np", ".", "sqrt", "(", "2", ")", ")", ")", "\n", "prediction", "=", "tf", ".", "layers", ".", "conv2d", "(", "name", "=", "'RND_Prediction_Conv3'", ",", "inputs", "=", "prediction", ",", "filters", "=", "64", ",", "kernel_size", "=", "3", ",", "strides", "=", "1", ",", "padding", "=", "'SAME'", ",", "activation", "=", "tf", ".", "nn", ".", "leaky_relu", ",", "kernel_initializer", "=", "tf_utils", ".", "orthogonal_initializer", "(", "np", ".", "sqrt", "(", "2", ")", ")", ")", "\n", "prediction", "=", "tf", ".", "layers", ".", "flatten", "(", "prediction", ")", "\n", "prediction", "=", "tf", ".", "layers", ".", "dense", "(", "name", "=", "'RND_Prediction_Dense1'", ",", "inputs", "=", "prediction", ",", "units", "=", "512", ",", "activation", "=", "tf", ".", "nn", ".", "relu", ",", "kernel_initializer", "=", "tf_utils", ".", "orthogonal_initializer", "(", "np", ".", "sqrt", "(", "2", ")", ")", ")", "\n", "prediction", "=", "tf", ".", "layers", ".", "dense", "(", "name", "=", "'RND_Prediction_Dense2'", ",", "inputs", "=", "prediction", ",", "units", "=", "512", ",", "activation", "=", "tf", ".", "nn", ".", "relu", ",", "kernel_initializer", "=", "tf_utils", ".", "orthogonal_initializer", "(", "np", ".", "sqrt", "(", "2", ")", ")", ")", "\n", "prediction", "=", "tf", ".", "layers", ".", "dense", "(", "name", "=", "'RND_Prediction_Dense3'", ",", "inputs", "=", "prediction", ",", "units", "=", "512", ",", "activation", "=", "None", ",", "kernel_initializer", "=", "tf_utils", ".", "orthogonal_initializer", "(", "np", ".", "sqrt", "(", "2", ")", ")", ")", "\n", "with", "tf", ".", "variable_scope", "(", "'RND_Prediction_Dense3'", ",", "reuse", "=", "True", ")", ":", "\n", " \t\t\t\t", "prediction_weights", "=", "{", "\n", "'kernel'", ":", "tf", ".", "get_variable", "(", "\"kernel\"", ")", ",", "\n", "'bias'", ":", "tf", ".", "get_variable", "(", "\"bias\"", ")", "\n", "}", "\n", "# update keys", "\n", "", "self", ".", "_update_keys", "(", "variable_scope", ".", "name", ",", "share_trainables", ")", "\n", "# return result", "\n", "return", "target", ",", "prediction", ",", "prediction_weights", "\n", "", "", "", ""]], "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.algorithm.acer_algorithm.ACER_Algorithm.get_reversed_cumulative_return": [[19, 46], ["zip", "acer_algorithm.ACER_Algorithm.get_reversed_cumulative_return.retrace"], "methods", ["None"], ["@", "staticmethod", "\n", "def", "get_reversed_cumulative_return", "(", "gamma", ",", "last_value", ",", "reversed_reward", ",", "reversed_value", ",", "reversed_extra", ")", ":", "\n", "\t\t", "def", "retrace", "(", "gamma", ",", "last_value", ",", "reversed_reward", ",", "reversed_value", ",", "reversed_qvalue", ",", "reversed_policy_ratio", ")", ":", "\n", "\t\t\t", "def", "get_return", "(", "state_value_retraced_after", ",", "reward", ",", "state_value", ",", "policy_ratio", ",", "action_state_value", ")", ":", "\n", "\t\t\t\t", "state_value_retraced_now", "=", "reward", "+", "gamma", "*", "state_value_retraced_after", "\n", "state_value_retraced_after", "=", "state_value", "+", "min", "(", "1.", ",", "policy_ratio", ")", "*", "(", "state_value_retraced_now", "-", "action_state_value", ")", "\n", "return", "state_value_retraced_now", ",", "state_value_retraced_after", "\n", "", "reversed_cumulative_return", ",", "_", "=", "zip", "(", "*", "accumulate", "(", "\n", "iterable", "=", "zip", "(", "reversed_reward", ",", "reversed_value", ",", "reversed_policy_ratio", ",", "reversed_qvalue", ")", ",", "\n", "func", "=", "lambda", "cumulative_value", ",", "reward_v_ratio_q", ":", "get_return", "(", "\n", "state_value_retraced_after", "=", "cumulative_value", "[", "1", "]", ",", "\n", "reward", "=", "reward_v_ratio_q", "[", "0", "]", ",", "\n", "state_value", "=", "reward_v_ratio_q", "[", "1", "]", ",", "\n", "policy_ratio", "=", "reward_v_ratio_q", "[", "2", "]", ",", "\n", "action_state_value", "=", "reward_v_ratio_q", "[", "3", "]", "\n", ")", ",", "\n", "initial_value", "=", "(", "0.", ",", "last_value", ")", "# initial cumulative_value", "\n", ")", ")", "\n", "return", "reversed_cumulative_return", "\n", "", "reversed_policy_ratio", ",", "reversed_qvalue", "=", "zip", "(", "*", "reversed_extra", ")", "\n", "return", "retrace", "(", "\n", "gamma", "=", "gamma", ",", "\n", "last_value", "=", "last_value", ",", "\n", "reversed_reward", "=", "reversed_reward", ",", "\n", "reversed_value", "=", "reversed_value", ",", "\n", "reversed_qvalue", "=", "reversed_qvalue", ",", "\n", "reversed_policy_ratio", "=", "reversed_policy_ratio", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.algorithm.acer_algorithm.ACER_Algorithm.initialize_network": [[48, 50], ["super().initialize_network"], "methods", ["home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.algorithm.ac_algorithm.AC_Algorithm.initialize_network"], ["", "def", "initialize_network", "(", "self", ",", "qvalue_estimation", "=", "False", ")", ":", "\n", "\t\t", "super", "(", ")", ".", "initialize_network", "(", "qvalue_estimation", "=", "True", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.algorithm.acer_algorithm.ACER_Algorithm._get_policy_loss": [[51, 100], ["acer_algorithm.ACER_Algorithm.old_policy_distributions.mean", "acer_algorithm.ACER_Algorithm.old_policy_distributions.cross_entropy", "acer_algorithm.ACER_Algorithm.new_policy_distributions.mean", "acer_algorithm.ACER_Algorithm.new_policy_distributions.cross_entropy", "builder.get_ratio", "acer_algorithm.ACER_Algorithm.is_continuous_control", "tensorflow.expand_dims", "tensorflow.expand_dims", "tensorflow.reduce_sum", "tensorflow.reduce_sum", "builder.get", "tensorflow.expand_dims", "tensorflow.expand_dims", "agent.algorithm.loss.policy_loss.PolicyLoss", "tensorflow.nn.relu", "agent.algorithm.loss.policy_loss.PolicyLoss.get", "tensorflow.exp", "tensorflow.exp", "tensorflow.minimum", "tensorflow.expand_dims", "tensorflow.expand_dims", "tensorflow.reduce_sum", "tensorflow.expand_dims", "tensorflow.map_fn", "agent.algorithm.loss.policy_loss.PolicyLoss.get_ratio"], "methods", ["home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.utils.distributions.Normal.mean", "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.utils.distributions.Normal.cross_entropy", "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.utils.distributions.Normal.mean", "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.utils.distributions.Normal.cross_entropy", "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.loss.policy_loss.PolicyLoss.get_ratio", "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.actor_critic.base_network.is_continuous_control", "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.worker.batch.CompositeBatch.get", "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.worker.batch.CompositeBatch.get", "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.loss.policy_loss.PolicyLoss.get_ratio"], ["", "def", "_get_policy_loss", "(", "self", ",", "builder", ")", ":", "\n", "# bugged!", "\n", "\t\t", "old_mean_action", "=", "self", ".", "old_policy_distributions", ".", "mean", "(", ")", "\n", "old_mean_cross_entropy", "=", "self", ".", "old_policy_distributions", ".", "cross_entropy", "(", "old_mean_action", ")", "\n", "new_mean_action", "=", "self", ".", "new_policy_distributions", ".", "mean", "(", ")", "\n", "new_mean_cross_entropy", "=", "self", ".", "new_policy_distributions", ".", "cross_entropy", "(", "new_mean_action", ")", "\n", "# Truncated importance sampling", "\n", "# Build variables used for retracing -> retrace outside graph and then feed cumulative_reward_batch to graph", "\n", "self", ".", "ratio_batch", "=", "builder", ".", "get_ratio", "(", ")", "\n", "if", "self", ".", "is_continuous_control", "(", ")", ":", "\n", "\t\t\t", "mean_action", "=", "tf", ".", "exp", "(", "-", "new_mean_cross_entropy", ")", "\n", "chosen_action", "=", "tf", ".", "exp", "(", "-", "self", ".", "new_cross_entropy_sample", ")", "\n", "value_reduction_axis", "=", "-", "1", "\n", "", "else", ":", "\n", "\t\t\t", "mean_action", "=", "new_mean_action", "\n", "chosen_action", "=", "self", ".", "old_action_batch", "\n", "value_reduction_axis", "=", "-", "1", "if", "self", ".", "policy_size", "==", "1", "else", "[", "-", "2", ",", "-", "1", "]", "\n", "# Add extra dimension to actions for multiplicating them with critic batches that may have multiple heads", "\n", "", "mean_action", "=", "tf", ".", "expand_dims", "(", "mean_action", ",", "axis", "=", "1", ")", "\n", "chosen_action", "=", "tf", ".", "expand_dims", "(", "chosen_action", ",", "axis", "=", "1", ")", "\n", "# State value", "\n", "self", ".", "state_value_batch", "=", "tf", ".", "reduce_sum", "(", "mean_action", "*", "self", ".", "critic_batch", ",", "axis", "=", "value_reduction_axis", ")", "\n", "# Action-State value", "\n", "self", ".", "action_state_value_batch", "=", "tf", ".", "reduce_sum", "(", "chosen_action", "*", "self", ".", "critic_batch", ",", "axis", "=", "value_reduction_axis", ")", "\n", "# Build advantage using retraced cumulative_reward_batch", "\n", "advantage_batch", "=", "tf", ".", "minimum", "(", "self", ".", "importance_weight_clipping_factor", ",", "self", ".", "ratio_batch", ")", "*", "self", ".", "advantage_batch", "\n", "loss_truncated_importance", "=", "builder", ".", "get", "(", "advantage_batch", ")", "\n", "# Bias correction for the truncation", "\n", "new_mean_cross_entropy", "=", "tf", ".", "expand_dims", "(", "new_mean_cross_entropy", ",", "-", "1", ")", "\n", "old_mean_cross_entropy", "=", "tf", ".", "expand_dims", "(", "old_mean_cross_entropy", ",", "-", "1", ")", "\n", "if", "self", ".", "policy_size", "==", "1", ":", "\n", "\t\t\t", "new_mean_cross_entropy", "=", "tf", ".", "expand_dims", "(", "new_mean_cross_entropy", ",", "1", ")", "\n", "old_mean_cross_entropy", "=", "tf", ".", "expand_dims", "(", "old_mean_cross_entropy", ",", "1", ")", "\n", "", "builder_bc", "=", "PolicyLoss", "(", "\n", "global_step", "=", "self", ".", "global_step", ",", "\n", "type", "=", "flags", ".", "policy_loss", ",", "\n", "cross_entropy", "=", "new_mean_cross_entropy", ",", "\n", "old_cross_entropy", "=", "old_mean_cross_entropy", "\n", ")", "\n", "value_batch", "=", "tf", ".", "reduce_sum", "(", "self", ".", "critic_batch", ",", "axis", "=", "-", "2", ")", "if", "self", ".", "policy_size", ">", "1", "else", "self", ".", "critic_batch", "\n", "advantage_batch_bc", "=", "value_batch", "-", "tf", ".", "expand_dims", "(", "self", ".", "state_value_batch", ",", "axis", "=", "-", "1", ")", "\n", "# Merge intrisic and extrinsic rewards", "\n", "if", "self", ".", "value_count", ">", "1", ":", "\n", "\t\t\t", "advantage_batch_bc", "=", "tf", ".", "map_fn", "(", "fn", "=", "merge_splitted_advantages", ",", "elems", "=", "advantage_batch_bc", ")", "\n", "# Clip", "\n", "", "advantage_batch_bc", "*=", "tf", ".", "nn", ".", "relu", "(", "1.0", "-", "self", ".", "importance_weight_clipping_factor", "/", "builder_bc", ".", "get_ratio", "(", ")", ")", "\n", "loss_bias_correction", "=", "builder_bc", ".", "get", "(", "advantage_batch_bc", ")", "\n", "# Total policy loss", "\n", "return", "loss_truncated_importance", "+", "loss_bias_correction", "\n", "\n"]], "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.algorithm.acer_algorithm.ACER_Algorithm.predict_value": [[101, 115], ["super().predict_value", "feed_dict.update", "tensorflow.get_default_session().run", "tuple", "acer_algorithm.ACER_Algorithm._get_internal_state_feed", "feed_dict.update", "zip", "tensorflow.get_default_session"], "methods", ["home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.algorithm.ac_algorithm.AC_Algorithm.predict_value", "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.utils.running_std.RunningMeanStd.update", "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.algorithm.ac_algorithm.AC_Algorithm._get_internal_state_feed", "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.utils.running_std.RunningMeanStd.update"], ["", "def", "predict_value", "(", "self", ",", "value_dict", ")", ":", "\n", "\t\t", "value_batch", ",", "bootstrap_value", ",", "_", "=", "super", "(", ")", ".", "predict_value", "(", "value_dict", ")", "\n", "# Get value batch", "\n", "feed_dict", "=", "{", "\n", "self", ".", "state_batch", ":", "value_dict", "[", "'states'", "]", ",", "\n", "self", ".", "old_policy_batch", ":", "value_dict", "[", "'policies'", "]", ",", "\n", "self", ".", "old_action_batch", ":", "value_dict", "[", "'actions'", "]", "\n", "}", "\n", "feed_dict", ".", "update", "(", "self", ".", "_get_internal_state_feed", "(", "value_dict", "[", "'internal_state'", "]", ")", ")", "\n", "if", "self", ".", "concat_size", ">", "0", ":", "\n", "\t\t\t", "feed_dict", ".", "update", "(", "{", "self", ".", "state_concat_batch", ":", "value_dict", "[", "'concats'", "]", "}", ")", "\n", "", "extra_batch", "=", "tf", ".", "get_default_session", "(", ")", ".", "run", "(", "fetches", "=", "[", "self", ".", "ratio_batch", ",", "self", ".", "action_state_value_batch", "]", ",", "feed_dict", "=", "feed_dict", ")", "\n", "extra_batch", "=", "tuple", "(", "zip", "(", "*", "extra_batch", ")", ")", "\n", "return", "value_batch", ",", "bootstrap_value", ",", "extra_batch", "\n", "", "", ""]], "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.algorithm.ac_algorithm.AC_Algorithm.get_reversed_cumulative_return": [[25, 69], ["ac_algorithm.AC_Algorithm.get_reversed_cumulative_return.generalized_advantage_estimator"], "methods", ["None"], ["@", "staticmethod", "\n", "def", "get_reversed_cumulative_return", "(", "gamma", ",", "last_value", ",", "reversed_reward", ",", "reversed_value", ",", "reversed_extra", ")", ":", "\n", "# GAE", "\n", "\t\t", "if", "flags", ".", "use_GAE", ":", "\n", "# Schulman, John, et al. \"High-dimensional continuous control using generalized advantage estimation.\" arXiv preprint arXiv:1506.02438 (2015).", "\n", "\t\t\t", "def", "generalized_advantage_estimator", "(", "gamma", ",", "lambd", ",", "last_value", ",", "reversed_reward", ",", "reversed_value", ")", ":", "\n", "# AC_Algorithm.replay_critic = True", "\n", "\t\t\t\t", "def", "get_return", "(", "last_gae", ",", "last_value", ",", "reward", ",", "value", ")", ":", "\n", "\t\t\t\t\t", "new_gae", "=", "reward", "+", "gamma", "*", "last_value", "-", "value", "+", "gamma", "*", "lambd", "*", "last_gae", "\n", "return", "new_gae", ",", "value", "\n", "", "reversed_cumulative_advantage", ",", "_", "=", "zip", "(", "*", "accumulate", "(", "\n", "iterable", "=", "zip", "(", "reversed_reward", ",", "reversed_value", ")", ",", "\n", "func", "=", "lambda", "cumulative_value", ",", "reward_value", ":", "get_return", "(", "\n", "last_gae", "=", "cumulative_value", "[", "0", "]", ",", "\n", "last_value", "=", "cumulative_value", "[", "1", "]", ",", "\n", "reward", "=", "reward_value", "[", "0", "]", ",", "\n", "value", "=", "reward_value", "[", "1", "]", "\n", ")", ",", "\n", "initial_value", "=", "(", "0.", ",", "last_value", ")", "# initial cumulative_value", "\n", ")", ")", "\n", "reversed_cumulative_return", "=", "tuple", "(", "map", "(", "lambda", "adv", ",", "val", ":", "adv", "+", "val", ",", "reversed_cumulative_advantage", ",", "reversed_value", ")", ")", "\n", "return", "reversed_cumulative_return", "\n", "", "return", "generalized_advantage_estimator", "(", "\n", "gamma", "=", "gamma", ",", "\n", "lambd", "=", "flags", ".", "lambd", ",", "\n", "last_value", "=", "last_value", ",", "\n", "reversed_reward", "=", "reversed_reward", ",", "\n", "reversed_value", "=", "reversed_value", "\n", ")", "\n", "# Vanilla discounted cumulative reward", "\n", "", "else", ":", "\n", "\t\t\t", "def", "vanilla", "(", "gamma", ",", "last_value", ",", "reversed_reward", ")", ":", "\n", "\t\t\t\t", "def", "get_return", "(", "last_return", ",", "reward", ")", ":", "\n", "\t\t\t\t\t", "return", "reward", "+", "gamma", "*", "last_return", "\n", "", "reversed_cumulative_return", "=", "tuple", "(", "accumulate", "(", "\n", "iterable", "=", "reversed_reward", ",", "\n", "func", "=", "lambda", "cumulative_value", ",", "reward", ":", "get_return", "(", "last_return", "=", "cumulative_value", ",", "reward", "=", "reward", ")", ",", "\n", "initial_value", "=", "last_value", "# initial cumulative_value", "\n", ")", ")", "\n", "return", "reversed_cumulative_return", "\n", "", "return", "vanilla", "(", "\n", "gamma", "=", "gamma", ",", "\n", "last_value", "=", "last_value", ",", "\n", "reversed_reward", "=", "reversed_reward", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.algorithm.ac_algorithm.AC_Algorithm.__init__": [[71, 112], ["eval", "ac_algorithm.AC_Algorithm.build_input_placeholders", "ac_algorithm.AC_Algorithm.initialize_network", "ac_algorithm.AC_Algorithm.build_network", "utils.statistics.Statistics", "len"], "methods", ["home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.algorithm.ac_algorithm.AC_Algorithm.build_input_placeholders", "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.algorithm.ac_algorithm.AC_Algorithm.initialize_network", "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.algorithm.ac_algorithm.AC_Algorithm.build_network"], ["", "", "def", "__init__", "(", "self", ",", "group_id", ",", "model_id", ",", "environment_info", ",", "beta", "=", "None", ",", "training", "=", "True", ",", "parent", "=", "None", ",", "sibling", "=", "None", ")", ":", "\n", "\t\t", "self", ".", "parameters_type", "=", "eval", "(", "'tf.{}'", ".", "format", "(", "flags", ".", "parameters_type", ")", ")", "\n", "self", ".", "beta", "=", "beta", "if", "beta", "is", "not", "None", "else", "flags", ".", "beta", "\n", "self", ".", "value_count", "=", "2", "if", "flags", ".", "split_values", "else", "1", "\n", "# initialize", "\n", "self", ".", "training", "=", "training", "\n", "self", ".", "group_id", "=", "group_id", "\n", "self", ".", "model_id", "=", "model_id", "\n", "self", ".", "id", "=", "'{0}_{1}'", ".", "format", "(", "self", ".", "group_id", ",", "self", ".", "model_id", ")", "# model id", "\n", "self", ".", "parent", "=", "parent", "if", "parent", "is", "not", "None", "else", "self", "# used for sharing with other models in hierarchy, if any", "\n", "self", ".", "sibling", "=", "sibling", "if", "sibling", "is", "not", "None", "else", "self", "# used for sharing with other models in hierarchy, if any", "\n", "# Environment info", "\n", "action_shape", "=", "environment_info", "[", "'action_shape'", "]", "\n", "self", ".", "policy_heads", "=", "[", "\n", "{", "\n", "'size'", ":", "head", "[", "0", "]", ",", "# number of actions to take", "\n", "'depth'", ":", "head", "[", "1", "]", "if", "len", "(", "head", ")", ">", "1", "else", "0", "# number of discrete action types: set 0 for continuous control", "\n", "}", "\n", "for", "head", "in", "action_shape", "\n", "]", "\n", "state_shape", "=", "environment_info", "[", "'state_shape'", "]", "\n", "self", ".", "state_heads", "=", "[", "\n", "{", "'shape'", ":", "head", "}", "\n", "for", "head", "in", "state_shape", "\n", "]", "\n", "self", ".", "state_scaler", "=", "environment_info", "[", "'state_scaler'", "]", "# state scaler, for saving memory (eg. in case of RGB input: uint8 takes less memory than float64)", "\n", "self", ".", "has_masked_actions", "=", "environment_info", "[", "'has_masked_actions'", "]", "\n", "# Create the network", "\n", "self", ".", "build_input_placeholders", "(", ")", "\n", "self", ".", "initialize_network", "(", ")", "\n", "self", ".", "build_network", "(", ")", "\n", "# Stuff for building the big-batch and optimize training computations", "\n", "self", ".", "_big_batch_feed", "=", "[", "{", "}", ",", "{", "}", "]", "\n", "self", ".", "_batch_count", "=", "[", "0", ",", "0", "]", "\n", "self", ".", "_train_batch_size", "=", "flags", ".", "batch_size", "*", "flags", ".", "big_batch_size", "\n", "# Statistics", "\n", "self", ".", "_train_statistics", "=", "Statistics", "(", "flags", ".", "episode_count_for_evaluation", ")", "\n", "#=======================================================================", "\n", "# self.loss_distribution_estimator = RunningMeanStd(batch_size=flags.batch_size)", "\n", "#=======================================================================", "\n", "self", ".", "actor_loss_is_too_small", "=", "False", "\n", "\n"]], "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.algorithm.ac_algorithm.AC_Algorithm.get_statistics": [[113, 115], ["ac_algorithm.AC_Algorithm._train_statistics.get"], "methods", ["home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.worker.batch.CompositeBatch.get"], ["", "def", "get_statistics", "(", "self", ")", ":", "\n", "\t\t", "return", "self", ".", "_train_statistics", ".", "get", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.algorithm.ac_algorithm.AC_Algorithm.build_input_placeholders": [[116, 138], ["print", "ac_algorithm.AC_Algorithm._scalar_placeholder", "ac_algorithm.AC_Algorithm._scalar_placeholder", "enumerate", "ac_algorithm.AC_Algorithm._value_placeholder", "print", "ac_algorithm.AC_Algorithm._value_placeholder", "print", "ac_algorithm.AC_Algorithm._value_placeholder", "ac_algorithm.AC_Algorithm._state_placeholder", "ac_algorithm.AC_Algorithm._state_placeholder", "ac_algorithm.AC_Algorithm._state_placeholder", "print", "ac_algorithm.AC_Algorithm._scalar_placeholder", "print", "ac_algorithm.AC_Algorithm._policy_placeholder", "ac_algorithm.AC_Algorithm._action_placeholder", "enumerate", "enumerate", "enumerate", "ac_algorithm.AC_Algorithm.reward_batch.get_shape", "ac_algorithm.AC_Algorithm.cumulative_return_batch.get_shape", "enumerate", "enumerate", "ac_algorithm.AC_Algorithm._action_placeholder", "state.get_shape", "ac_algorithm.AC_Algorithm.advantage_batch.get_shape", "enumerate"], "methods", ["home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.algorithm.ac_algorithm.AC_Algorithm._scalar_placeholder", "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.algorithm.ac_algorithm.AC_Algorithm._scalar_placeholder", "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.algorithm.ac_algorithm.AC_Algorithm._value_placeholder", "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.algorithm.ac_algorithm.AC_Algorithm._value_placeholder", "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.algorithm.ac_algorithm.AC_Algorithm._value_placeholder", "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.algorithm.ac_algorithm.AC_Algorithm._state_placeholder", "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.algorithm.ac_algorithm.AC_Algorithm._state_placeholder", "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.algorithm.ac_algorithm.AC_Algorithm._state_placeholder", "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.algorithm.ac_algorithm.AC_Algorithm._scalar_placeholder", "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.algorithm.ac_algorithm.AC_Algorithm._policy_placeholder", "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.algorithm.ac_algorithm.AC_Algorithm._action_placeholder", "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.algorithm.ac_algorithm.AC_Algorithm._action_placeholder"], ["", "def", "build_input_placeholders", "(", "self", ")", ":", "\n", "\t\t", "print", "(", "\"Building network {} input placeholders\"", ".", "format", "(", "self", ".", "id", ")", ")", "\n", "self", ".", "constrain_replay", "=", "flags", ".", "constraining_replay", "and", "flags", ".", "replay_mean", ">", "0", "\n", "self", ".", "is_replayed_batch", "=", "self", ".", "_scalar_placeholder", "(", "dtype", "=", "tf", ".", "bool", ",", "batch_size", "=", "1", ",", "name", "=", "\"replay\"", ")", "\n", "self", ".", "state_mean_batch", "=", "[", "self", ".", "_state_placeholder", "(", "shape", "=", "head", "[", "'shape'", "]", ",", "batch_size", "=", "1", ",", "name", "=", "\"state_mean{}\"", ".", "format", "(", "i", ")", ")", "for", "i", ",", "head", "in", "enumerate", "(", "self", ".", "state_heads", ")", "]", "\n", "self", ".", "state_std_batch", "=", "[", "self", ".", "_state_placeholder", "(", "shape", "=", "head", "[", "'shape'", "]", ",", "batch_size", "=", "1", ",", "name", "=", "\"state_std{}\"", ".", "format", "(", "i", ")", ")", "for", "i", ",", "head", "in", "enumerate", "(", "self", ".", "state_heads", ")", "]", "\n", "self", ".", "state_batch", "=", "[", "self", ".", "_state_placeholder", "(", "shape", "=", "head", "[", "'shape'", "]", ",", "name", "=", "\"state{}\"", ".", "format", "(", "i", ")", ")", "for", "i", ",", "head", "in", "enumerate", "(", "self", ".", "state_heads", ")", "]", "\n", "self", ".", "size_batch", "=", "self", ".", "_scalar_placeholder", "(", "dtype", "=", "tf", ".", "int32", ",", "name", "=", "\"size\"", ")", "\n", "for", "i", ",", "state", "in", "enumerate", "(", "self", ".", "state_batch", ")", ":", "\n", "\t\t\t", "print", "(", "\"\t[{}]State{} shape: {}\"", ".", "format", "(", "self", ".", "id", ",", "i", ",", "state", ".", "get_shape", "(", ")", ")", ")", "\n", "", "self", ".", "reward_batch", "=", "self", ".", "_value_placeholder", "(", "\"reward\"", ")", "\n", "print", "(", "\"\t[{}]Reward shape: {}\"", ".", "format", "(", "self", ".", "id", ",", "self", ".", "reward_batch", ".", "get_shape", "(", ")", ")", ")", "\n", "self", ".", "cumulative_return_batch", "=", "self", ".", "_value_placeholder", "(", "\"cumulative_return\"", ")", "\n", "print", "(", "\"\t[{}]Cumulative Return shape: {}\"", ".", "format", "(", "self", ".", "id", ",", "self", ".", "cumulative_return_batch", ".", "get_shape", "(", ")", ")", ")", "\n", "if", "not", "flags", ".", "runtime_advantage", ":", "\n", "\t\t\t", "self", ".", "advantage_batch", "=", "self", ".", "_scalar_placeholder", "(", "\"advantage\"", ")", "\n", "print", "(", "\"\t[{}]Advantage shape: {}\"", ".", "format", "(", "self", ".", "id", ",", "self", ".", "advantage_batch", ".", "get_shape", "(", ")", ")", ")", "\n", "", "self", ".", "old_state_value_batch", "=", "self", ".", "_value_placeholder", "(", "\"old_state_value\"", ")", "\n", "self", ".", "old_policy_batch", "=", "[", "self", ".", "_policy_placeholder", "(", "policy_size", "=", "head", "[", "'size'", "]", ",", "policy_depth", "=", "head", "[", "'depth'", "]", ",", "name", "=", "\"old_policy{}\"", ".", "format", "(", "i", ")", ")", "for", "i", ",", "head", "in", "enumerate", "(", "self", ".", "policy_heads", ")", "]", "\n", "self", ".", "old_action_batch", "=", "[", "self", ".", "_action_placeholder", "(", "policy_size", "=", "head", "[", "'size'", "]", ",", "policy_depth", "=", "head", "[", "'depth'", "]", ",", "name", "=", "\"old_action_batch{}\"", ".", "format", "(", "i", ")", ")", "for", "i", ",", "head", "in", "enumerate", "(", "self", ".", "policy_heads", ")", "]", "\n", "if", "self", ".", "has_masked_actions", ":", "\n", "\t\t\t", "self", ".", "old_action_mask_batch", "=", "[", "self", ".", "_action_placeholder", "(", "policy_size", "=", "head", "[", "'size'", "]", ",", "policy_depth", "=", "1", ",", "name", "=", "\"old_action_mask_batch{}\"", ".", "format", "(", "i", ")", ")", "for", "i", ",", "head", "in", "enumerate", "(", "self", ".", "policy_heads", ")", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.algorithm.ac_algorithm.AC_Algorithm._policy_placeholder": [[139, 145], ["is_continuous_control", "tensorflow.placeholder"], "methods", ["home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.actor_critic.base_network.is_continuous_control"], ["", "", "def", "_policy_placeholder", "(", "self", ",", "policy_size", ",", "policy_depth", ",", "name", "=", "None", ",", "batch_size", "=", "None", ")", ":", "\n", "\t\t", "if", "is_continuous_control", "(", "policy_depth", ")", ":", "\n", "\t\t\t", "shape", "=", "[", "batch_size", ",", "2", ",", "policy_size", "]", "\n", "", "else", ":", "# Discrete control", "\n", "\t\t\t", "shape", "=", "[", "batch_size", ",", "policy_size", ",", "policy_depth", "]", "if", "policy_size", ">", "1", "else", "[", "batch_size", ",", "policy_depth", "]", "\n", "", "return", "tf", ".", "placeholder", "(", "dtype", "=", "self", ".", "parameters_type", ",", "shape", "=", "shape", ",", "name", "=", "name", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.algorithm.ac_algorithm.AC_Algorithm._action_placeholder": [[146, 153], ["tensorflow.placeholder", "is_continuous_control", "shape.append", "shape.append"], "methods", ["home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.actor_critic.base_network.is_continuous_control", "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.worker.batch.ExperienceBatch.append", "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.worker.batch.ExperienceBatch.append"], ["", "def", "_action_placeholder", "(", "self", ",", "policy_size", ",", "policy_depth", ",", "name", "=", "None", ",", "batch_size", "=", "None", ")", ":", "\n", "\t\t", "shape", "=", "[", "batch_size", "]", "\n", "if", "policy_size", ">", "1", "or", "is_continuous_control", "(", "policy_depth", ")", ":", "\n", "\t\t\t", "shape", ".", "append", "(", "policy_size", ")", "\n", "", "if", "policy_depth", ">", "1", ":", "\n", "\t\t\t", "shape", ".", "append", "(", "policy_depth", ")", "\n", "", "return", "tf", ".", "placeholder", "(", "dtype", "=", "self", ".", "parameters_type", ",", "shape", "=", "shape", ",", "name", "=", "name", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.algorithm.ac_algorithm.AC_Algorithm._value_placeholder": [[154, 156], ["tensorflow.placeholder"], "methods", ["None"], ["", "def", "_value_placeholder", "(", "self", ",", "name", "=", "None", ",", "batch_size", "=", "None", ")", ":", "\n", "\t\t", "return", "tf", ".", "placeholder", "(", "dtype", "=", "self", ".", "parameters_type", ",", "shape", "=", "[", "batch_size", ",", "self", ".", "value_count", "]", ",", "name", "=", "name", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.algorithm.ac_algorithm.AC_Algorithm._scalar_placeholder": [[157, 161], ["tensorflow.placeholder"], "methods", ["None"], ["", "def", "_scalar_placeholder", "(", "self", ",", "name", "=", "None", ",", "batch_size", "=", "None", ",", "dtype", "=", "None", ")", ":", "\n", "\t\t", "if", "dtype", "is", "None", ":", "\n", "\t\t\t", "dtype", "=", "self", ".", "parameters_type", "\n", "", "return", "tf", ".", "placeholder", "(", "dtype", "=", "dtype", ",", "shape", "=", "[", "batch_size", "]", ",", "name", "=", "name", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.algorithm.ac_algorithm.AC_Algorithm._state_placeholder": [[162, 166], ["tensorflow.zeros", "tensorflow.placeholder_with_default", "list"], "methods", ["None"], ["", "def", "_state_placeholder", "(", "self", ",", "shape", ",", "name", "=", "None", ",", "batch_size", "=", "None", ")", ":", "\n", "\t\t", "shape", "=", "[", "batch_size", "]", "+", "list", "(", "shape", ")", "\n", "input", "=", "tf", ".", "zeros", "(", "shape", "if", "batch_size", "is", "not", "None", "else", "[", "1", "]", "+", "shape", "[", "1", ":", "]", ",", "dtype", "=", "self", ".", "parameters_type", ")", "# default value", "\n", "return", "tf", ".", "placeholder_with_default", "(", "input", "=", "input", ",", "shape", "=", "shape", ",", "name", "=", "name", ")", "# with default we can use batch normalization directly on it", "\n", "\n"]], "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.algorithm.ac_algorithm.AC_Algorithm.build_optimizer": [[167, 184], ["tensorflow.Variable", "ac_algorithm.AC_Algorithm.get_network_partitions", "print", "utils.get_annealable_variable", "utils.get_optimization_function"], "methods", ["home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.algorithm.ac_algorithm.AC_Algorithm.get_network_partitions", "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.utils.tensorflow_utils.get_annealable_variable", "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.utils.tensorflow_utils.get_optimization_function"], ["", "def", "build_optimizer", "(", "self", ",", "optimization_algoritmh", ")", ":", "\n", "# global step", "\n", "\t\t", "global_step", "=", "tf", ".", "Variable", "(", "0", ",", "trainable", "=", "False", ")", "\n", "# learning rate", "\n", "learning_rate", "=", "tf_utils", ".", "get_annealable_variable", "(", "\n", "function_name", "=", "flags", ".", "alpha_annealing_function", ",", "\n", "initial_value", "=", "flags", ".", "alpha", ",", "\n", "global_step", "=", "global_step", ",", "\n", "decay_steps", "=", "flags", ".", "alpha_decay_steps", ",", "\n", "decay_rate", "=", "flags", ".", "alpha_decay_rate", "\n", ")", "if", "flags", ".", "alpha_decay", "else", "flags", ".", "alpha", "\n", "# gradient optimizer", "\n", "optimizer", "=", "{", "}", "\n", "for", "p", "in", "self", ".", "get_network_partitions", "(", ")", ":", "\n", "\t\t\t", "optimizer", "[", "p", "]", "=", "tf_utils", ".", "get_optimization_function", "(", "optimization_algoritmh", ")", "(", "learning_rate", "=", "learning_rate", ",", "use_locking", "=", "True", ")", "\n", "", "print", "(", "\"Gradient {} optimized by {}\"", ".", "format", "(", "self", ".", "id", ",", "optimization_algoritmh", ")", ")", "\n", "return", "optimizer", ",", "global_step", "\n", "\n"]], "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.algorithm.ac_algorithm.AC_Algorithm.get_network_partitions": [[185, 187], ["None"], "methods", ["None"], ["", "def", "get_network_partitions", "(", "self", ")", ":", "\n", "\t\t", "return", "[", "'Actor'", ",", "'Critic'", ",", "'Reward'", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.algorithm.ac_algorithm.AC_Algorithm.initialize_network": [[188, 231], ["IntrinsicReward_Network", "ac_algorithm.AC_Algorithm.network[].build", "print", "print", "print", "eval", "ac_algorithm.AC_Algorithm.intrinsic_reward_batch.get_shape", "ac_algorithm.AC_Algorithm.training_state[].get_shape", "ac_algorithm.AC_Algorithm.training_state[].get_shape"], "methods", ["home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.intrinsic_reward.intrinsic_reward_network.IntrinsicReward_Network.build"], ["", "def", "initialize_network", "(", "self", ",", "qvalue_estimation", "=", "False", ")", ":", "\n", "\t\t", "self", ".", "network", "=", "{", "}", "\n", "batch_dict", "=", "{", "\n", "'state'", ":", "self", ".", "state_batch", ",", "\n", "'state_mean'", ":", "self", ".", "state_mean_batch", ",", "\n", "'state_std'", ":", "self", ".", "state_std_batch", ",", "\n", "'size'", ":", "self", ".", "size_batch", "\n", "}", "\n", "# Build intrinsic reward network here because we need its internal state for building actor and critic", "\n", "self", ".", "network", "[", "'Reward'", "]", "=", "IntrinsicReward_Network", "(", "id", "=", "self", ".", "id", ",", "batch_dict", "=", "batch_dict", ",", "scope_dict", "=", "{", "'self'", ":", "\"IRNet{0}\"", ".", "format", "(", "self", ".", "id", ")", "}", ",", "training", "=", "self", ".", "training", ")", "\n", "if", "flags", ".", "intrinsic_reward", ":", "\n", "\t\t\t", "reward_network_output", "=", "self", ".", "network", "[", "'Reward'", "]", ".", "build", "(", ")", "\n", "self", ".", "intrinsic_reward_batch", "=", "reward_network_output", "[", "0", "]", "\n", "self", ".", "intrinsic_reward_loss", "=", "reward_network_output", "[", "1", "]", "\n", "self", ".", "training_state", "=", "reward_network_output", "[", "2", "]", "\n", "print", "(", "\"\t[{}]Intrinsic Reward shape: {}\"", ".", "format", "(", "self", ".", "id", ",", "self", ".", "intrinsic_reward_batch", ".", "get_shape", "(", ")", ")", ")", "\n", "print", "(", "\"\t[{}]Training State Kernel shape: {}\"", ".", "format", "(", "self", ".", "id", ",", "self", ".", "training_state", "[", "'kernel'", "]", ".", "get_shape", "(", ")", ")", ")", "\n", "print", "(", "\"\t[{}]Training State Bias shape: {}\"", ".", "format", "(", "self", ".", "id", ",", "self", ".", "training_state", "[", "'bias'", "]", ".", "get_shape", "(", ")", ")", ")", "\n", "batch_dict", "[", "'training_state'", "]", "=", "self", ".", "training_state", "\n", "# Build actor and critic", "\n", "", "for", "p", "in", "(", "'Actor'", ",", "'Critic'", ")", ":", "\n", "\t\t\t", "if", "flags", ".", "separate_actor_from_critic", ":", "# non-shared graph", "\n", "\t\t\t\t", "node_id", "=", "self", ".", "id", "+", "p", "\n", "parent_id", "=", "self", ".", "parent", ".", "id", "+", "p", "\n", "sibling_id", "=", "self", ".", "sibling", ".", "id", "+", "p", "\n", "", "else", ":", "# shared graph", "\n", "\t\t\t\t", "node_id", "=", "self", ".", "id", "\n", "parent_id", "=", "self", ".", "parent", ".", "id", "\n", "sibling_id", "=", "self", ".", "sibling", ".", "id", "\n", "", "scope_dict", "=", "{", "\n", "'self'", ":", "\"Net{0}\"", ".", "format", "(", "node_id", ")", ",", "\n", "'parent'", ":", "\"Net{0}\"", ".", "format", "(", "parent_id", ")", ",", "\n", "'sibling'", ":", "\"Net{0}\"", ".", "format", "(", "sibling_id", ")", "\n", "}", "\n", "self", ".", "network", "[", "p", "]", "=", "eval", "(", "'{}_Network'", ".", "format", "(", "flags", ".", "network_configuration", ")", ")", "(", "\n", "id", "=", "node_id", ",", "\n", "qvalue_estimation", "=", "qvalue_estimation", ",", "\n", "policy_heads", "=", "self", ".", "policy_heads", ",", "\n", "batch_dict", "=", "batch_dict", ",", "\n", "scope_dict", "=", "scope_dict", ",", "\n", "training", "=", "self", ".", "training", ",", "\n", "value_count", "=", "self", ".", "value_count", ",", "\n", "state_scaler", "=", "self", ".", "state_scaler", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.algorithm.ac_algorithm.AC_Algorithm.build_network": [[233, 246], ["ac_algorithm.AC_Algorithm.network[].build", "enumerate", "ac_algorithm.AC_Algorithm.network[].build", "print", "ac_algorithm.AC_Algorithm.sample_actions", "enumerate", "enumerate", "print", "print", "print", "ac_algorithm.AC_Algorithm.critic_batch.get_shape", "b.get_shape", "b.get_shape", "b.get_shape"], "methods", ["home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.intrinsic_reward.intrinsic_reward_network.IntrinsicReward_Network.build", "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.intrinsic_reward.intrinsic_reward_network.IntrinsicReward_Network.build", "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.algorithm.ac_algorithm.AC_Algorithm.sample_actions"], ["", "", "def", "build_network", "(", "self", ")", ":", "\n", "# Actor & Critic", "\n", "\t\t", "self", ".", "actor_batch", ",", "_", "=", "self", ".", "network", "[", "'Actor'", "]", ".", "build", "(", "name", "=", "'Actor'", ",", "has_actor", "=", "True", ",", "has_critic", "=", "False", ",", "use_internal_state", "=", "flags", ".", "network_has_internal_state", ")", "\n", "for", "i", ",", "b", "in", "enumerate", "(", "self", ".", "actor_batch", ")", ":", "\n", "\t\t\t", "print", "(", "\"\t[{}]Actor{} output shape: {}\"", ".", "format", "(", "self", ".", "id", ",", "i", ",", "b", ".", "get_shape", "(", ")", ")", ")", "\n", "", "_", ",", "self", ".", "critic_batch", "=", "self", ".", "network", "[", "'Critic'", "]", ".", "build", "(", "name", "=", "'Critic'", ",", "has_actor", "=", "False", ",", "has_critic", "=", "True", ",", "use_internal_state", "=", "flags", ".", "network_has_internal_state", ")", "\n", "print", "(", "\"\t[{}]Critic output shape: {}\"", ".", "format", "(", "self", ".", "id", ",", "self", ".", "critic_batch", ".", "get_shape", "(", ")", ")", ")", "\n", "# Sample action, after getting keys", "\n", "self", ".", "action_batch", ",", "self", ".", "hot_action_batch", "=", "self", ".", "sample_actions", "(", ")", "\n", "for", "i", ",", "b", "in", "enumerate", "(", "self", ".", "action_batch", ")", ":", "\n", "\t\t\t", "print", "(", "\"\t[{}]Action{} output shape: {}\"", ".", "format", "(", "self", ".", "id", ",", "i", ",", "b", ".", "get_shape", "(", ")", ")", ")", "\n", "", "for", "i", ",", "b", "in", "enumerate", "(", "self", ".", "hot_action_batch", ")", ":", "\n", "\t\t\t", "print", "(", "\"\t[{}]HotAction{} output shape: {}\"", ".", "format", "(", "self", ".", "id", ",", "i", ",", "b", ".", "get_shape", "(", ")", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.algorithm.ac_algorithm.AC_Algorithm.sample_actions": [[247, 265], ["enumerate", "is_continuous_control", "tensorflow.transpose", "utils.distributions.Normal().sample", "tensorflow.clip_by_value", "action_batch.append", "hot_action_batch.append", "utils.distributions.Categorical", "utils.distributions.Categorical.sample", "action_batch.append", "hot_action_batch.append", "utils.distributions.Categorical.get_sample_one_hot", "utils.distributions.Normal"], "methods", ["home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.actor_critic.base_network.is_continuous_control", "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.buffer.pseudo_prioritized_buffer.PseudoPrioritizedBuffer.sample", "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.worker.batch.ExperienceBatch.append", "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.worker.batch.ExperienceBatch.append", "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.buffer.pseudo_prioritized_buffer.PseudoPrioritizedBuffer.sample", "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.worker.batch.ExperienceBatch.append", "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.worker.batch.ExperienceBatch.append", "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.utils.distributions.Categorical.get_sample_one_hot"], ["", "", "def", "sample_actions", "(", "self", ")", ":", "\n", "\t\t", "action_batch", "=", "[", "]", "\n", "hot_action_batch", "=", "[", "]", "\n", "for", "h", ",", "actor_head", "in", "enumerate", "(", "self", ".", "actor_batch", ")", ":", "\n", "\t\t\t", "if", "is_continuous_control", "(", "self", ".", "policy_heads", "[", "h", "]", "[", "'depth'", "]", ")", ":", "\n", "\t\t\t\t", "new_policy_batch", "=", "tf", ".", "transpose", "(", "actor_head", ",", "[", "1", ",", "0", ",", "2", "]", ")", "\n", "sample_batch", "=", "Normal", "(", "new_policy_batch", "[", "0", "]", ",", "new_policy_batch", "[", "1", "]", ")", ".", "sample", "(", ")", "\n", "action", "=", "tf", ".", "clip_by_value", "(", "sample_batch", ",", "-", "1", ",", "1", ")", "\n", "action_batch", ".", "append", "(", "action", ")", "# Sample action batch in forward direction, use old action in backward direction", "\n", "hot_action_batch", ".", "append", "(", "action", ")", "\n", "", "else", ":", "# discrete control", "\n", "\t\t\t\t", "distribution", "=", "Categorical", "(", "actor_head", ")", "\n", "action", "=", "distribution", ".", "sample", "(", "one_hot", "=", "False", ")", "# Sample action batch in forward direction, use old action in backward direction", "\n", "action_batch", ".", "append", "(", "action", ")", "\n", "hot_action_batch", ".", "append", "(", "distribution", ".", "get_sample_one_hot", "(", "action", ")", ")", "\n", "# Give self esplicative name to output for easily retrieving it in frozen graph", "\n", "# tf.identity(action_batch, name=\"action\")", "\n", "", "", "return", "action_batch", ",", "hot_action_batch", "\n", "\n"]], "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.algorithm.ac_algorithm.AC_Algorithm._get_policy_loss_builder": [[266, 288], ["new_policy_distributions.cross_entropy", "old_policy_distributions.cross_entropy", "agent.algorithm.loss.policy_loss.PolicyLoss", "tensorflow.where", "tensorflow.where", "tensorflow.equal", "tensorflow.equal", "new_policy_distributions.entropy", "tensorflow.stop_gradient", "tensorflow.stop_gradient"], "methods", ["home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.utils.distributions.Normal.cross_entropy", "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.utils.distributions.Normal.cross_entropy", "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.utils.distributions.Normal.entropy"], ["", "def", "_get_policy_loss_builder", "(", "self", ",", "new_policy_distributions", ",", "old_policy_distributions", ",", "old_action_batch", ",", "old_action_mask_batch", "=", "None", ")", ":", "\n", "\t\t", "cross_entropy", "=", "new_policy_distributions", ".", "cross_entropy", "(", "old_action_batch", ")", "\n", "old_cross_entropy", "=", "old_policy_distributions", ".", "cross_entropy", "(", "old_action_batch", ")", "\n", "if", "old_action_mask_batch", "is", "not", "None", ":", "\n", "# stop gradient computation on masked elements and remove them from loss (zeroing)", "\n", "\t\t\t", "cross_entropy", "=", "tf", ".", "where", "(", "\n", "tf", ".", "equal", "(", "old_action_mask_batch", ",", "1", ")", ",", "\n", "x", "=", "cross_entropy", ",", "# true branch", "\n", "y", "=", "tf", ".", "stop_gradient", "(", "old_action_mask_batch", ")", "# false branch", "\n", ")", "\n", "old_cross_entropy", "=", "tf", ".", "where", "(", "\n", "tf", ".", "equal", "(", "old_action_mask_batch", ",", "1", ")", ",", "\n", "x", "=", "old_cross_entropy", ",", "# true branch", "\n", "y", "=", "tf", ".", "stop_gradient", "(", "old_action_mask_batch", ")", "# false branch", "\n", ")", "\n", "", "return", "PolicyLoss", "(", "\n", "global_step", "=", "self", ".", "global_step", ",", "\n", "type", "=", "flags", ".", "policy_loss", ",", "\n", "cross_entropy", "=", "cross_entropy", ",", "\n", "old_cross_entropy", "=", "old_cross_entropy", ",", "\n", "entropy", "=", "new_policy_distributions", ".", "entropy", "(", ")", ",", "\n", "beta", "=", "self", ".", "beta", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.algorithm.ac_algorithm.AC_Algorithm._get_policy_loss": [[290, 296], ["builder.get", "tensorflow.map_fn"], "methods", ["home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.worker.batch.CompositeBatch.get"], ["", "def", "_get_policy_loss", "(", "self", ",", "builder", ")", ":", "\n", "\t\t", "if", "flags", ".", "runtime_advantage", ":", "\n", "\t\t\t", "self", ".", "advantage_batch", "=", "self", ".", "cumulative_return_batch", "-", "self", ".", "state_value_batch", "# baseline is always up to date", "\n", "if", "self", ".", "value_count", ">", "1", ":", "\n", "\t\t\t\t", "self", ".", "advantage_batch", "=", "tf", ".", "map_fn", "(", "fn", "=", "merge_splitted_advantages", ",", "elems", "=", "self", ".", "advantage_batch", ")", "\n", "", "", "return", "builder", ".", "get", "(", "self", ".", "advantage_batch", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.algorithm.ac_algorithm.AC_Algorithm._get_value_loss_builder": [[297, 304], ["agent.algorithm.loss.value_loss.ValueLoss"], "methods", ["None"], ["", "def", "_get_value_loss_builder", "(", "self", ")", ":", "\n", "\t\t", "return", "ValueLoss", "(", "\n", "global_step", "=", "self", ".", "global_step", ",", "\n", "type", "=", "flags", ".", "value_loss", ",", "\n", "estimation", "=", "self", ".", "state_value_batch", ",", "\n", "old_estimation", "=", "self", ".", "old_state_value_batch", ",", "\n", "cumulative_reward", "=", "self", ".", "cumulative_return_batch", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.algorithm.ac_algorithm.AC_Algorithm._get_value_loss": [[306, 308], ["builder.get"], "methods", ["home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.worker.batch.CompositeBatch.get"], ["", "def", "_get_value_loss", "(", "self", ",", "builder", ")", ":", "\n", "\t\t", "return", "flags", ".", "value_coefficient", "*", "builder", ".", "get", "(", ")", "# usually critic has lower learning rate", "\n", "\n"]], "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.algorithm.ac_algorithm.AC_Algorithm.prepare_loss": [[309, 357], ["print", "enumerate", "sum", "sum", "sum", "ac_algorithm.AC_Algorithm._get_value_loss_builder", "ac_algorithm.AC_Algorithm._get_value_loss", "is_continuous_control", "ac_algorithm.AC_Algorithm._get_policy_loss_builder", "policy_loss_builder.append", "sum", "len", "sum", "tensorflow.cond", "tensorflow.transpose", "old_policy_distributions.append", "tensorflow.transpose", "new_policy_distributions.append", "old_policy_distributions.append", "new_policy_distributions.append", "ac_algorithm.AC_Algorithm._get_policy_loss", "b.approximate_kullback_leibler_divergence", "b.get_entropy_regularization", "utils.distributions.Normal", "utils.distributions.Normal", "utils.distributions.Categorical", "utils.distributions.Categorical", "b.get_clipping_frequency", "ac_algorithm.AC_Algorithm.reduce_function", "zip", "tensorflow.constant", "tensorflow.squared_difference", "new_distribution.mean", "tensorflow.stop_gradient"], "methods", ["home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.utils.segment_tree.SumSegmentTree.sum", "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.utils.segment_tree.SumSegmentTree.sum", "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.utils.segment_tree.SumSegmentTree.sum", "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.algorithm.ac_algorithm.AC_Algorithm._get_value_loss_builder", "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.algorithm.ac_algorithm.AC_Algorithm._get_value_loss", "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.actor_critic.base_network.is_continuous_control", "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.algorithm.ac_algorithm.AC_Algorithm._get_policy_loss_builder", "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.worker.batch.ExperienceBatch.append", "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.utils.segment_tree.SumSegmentTree.sum", "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.utils.segment_tree.SumSegmentTree.sum", "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.worker.batch.ExperienceBatch.append", "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.worker.batch.ExperienceBatch.append", "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.worker.batch.ExperienceBatch.append", "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.worker.batch.ExperienceBatch.append", "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.algorithm.ac_algorithm.AC_Algorithm._get_policy_loss", "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.loss.policy_loss.PolicyLoss.approximate_kullback_leibler_divergence", "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.loss.policy_loss.PolicyLoss.get_entropy_regularization", "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.loss.policy_loss.PolicyLoss.get_clipping_frequency", "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.utils.distributions.Normal.mean"], ["", "def", "prepare_loss", "(", "self", ",", "global_step", ")", ":", "\n", "\t\t", "self", ".", "global_step", "=", "global_step", "\n", "print", "(", "\"Preparing loss {}\"", ".", "format", "(", "self", ".", "id", ")", ")", "\n", "self", ".", "state_value_batch", "=", "self", ".", "critic_batch", "\n", "# [Policy distribution]", "\n", "old_policy_distributions", "=", "[", "]", "\n", "new_policy_distributions", "=", "[", "]", "\n", "policy_loss_builder", "=", "[", "]", "\n", "for", "h", ",", "policy_head", "in", "enumerate", "(", "self", ".", "policy_heads", ")", ":", "\n", "\t\t\t", "if", "is_continuous_control", "(", "policy_head", "[", "'depth'", "]", ")", ":", "\n", "# Old policy", "\n", "\t\t\t\t", "old_policy_batch", "=", "tf", ".", "transpose", "(", "self", ".", "old_policy_batch", "[", "h", "]", ",", "[", "1", ",", "0", ",", "2", "]", ")", "\n", "old_policy_distributions", ".", "append", "(", "Normal", "(", "old_policy_batch", "[", "0", "]", ",", "old_policy_batch", "[", "1", "]", ")", ")", "\n", "# New policy", "\n", "new_policy_batch", "=", "tf", ".", "transpose", "(", "self", ".", "actor_batch", "[", "h", "]", ",", "[", "1", ",", "0", ",", "2", "]", ")", "\n", "new_policy_distributions", ".", "append", "(", "Normal", "(", "new_policy_batch", "[", "0", "]", ",", "new_policy_batch", "[", "1", "]", ")", ")", "\n", "", "else", ":", "# discrete control", "\n", "\t\t\t\t", "old_policy_distributions", ".", "append", "(", "Categorical", "(", "self", ".", "old_policy_batch", "[", "h", "]", ")", ")", "# Old policy", "\n", "new_policy_distributions", ".", "append", "(", "Categorical", "(", "self", ".", "actor_batch", "[", "h", "]", ")", ")", "# New policy", "\n", "", "builder", "=", "self", ".", "_get_policy_loss_builder", "(", "new_policy_distributions", "[", "h", "]", ",", "old_policy_distributions", "[", "h", "]", ",", "self", ".", "old_action_batch", "[", "h", "]", ",", "self", ".", "old_action_mask_batch", "[", "h", "]", "if", "self", ".", "has_masked_actions", "else", "None", ")", "\n", "policy_loss_builder", ".", "append", "(", "builder", ")", "\n", "# [Actor loss]", "\n", "", "self", ".", "policy_loss", "=", "sum", "(", "self", ".", "_get_policy_loss", "(", "b", ")", "for", "b", "in", "policy_loss_builder", ")", "\n", "# [Debug variables]", "\n", "self", ".", "policy_kl_divergence", "=", "sum", "(", "b", ".", "approximate_kullback_leibler_divergence", "(", ")", "for", "b", "in", "policy_loss_builder", ")", "\n", "self", ".", "policy_clipping_frequency", "=", "sum", "(", "b", ".", "get_clipping_frequency", "(", ")", "for", "b", "in", "policy_loss_builder", ")", "/", "len", "(", "policy_loss_builder", ")", "# take average because clipping frequency must be in [0,1]", "\n", "self", ".", "policy_entropy_regularization", "=", "sum", "(", "b", ".", "get_entropy_regularization", "(", ")", "for", "b", "in", "policy_loss_builder", ")", "\n", "# [Critic loss]", "\n", "value_loss_builder", "=", "self", ".", "_get_value_loss_builder", "(", ")", "\n", "self", ".", "value_loss", "=", "self", ".", "_get_value_loss", "(", "value_loss_builder", ")", "\n", "# [Entropy regularization]", "\n", "if", "flags", ".", "entropy_regularization", ":", "\n", "\t\t\t", "self", ".", "policy_loss", "+=", "-", "self", ".", "policy_entropy_regularization", "\n", "# [Constraining Replay]", "\n", "", "if", "self", ".", "constrain_replay", ":", "\n", "\t\t\t", "constrain_loss", "=", "sum", "(", "\n", "0.5", "*", "builder", ".", "reduce_function", "(", "tf", ".", "squared_difference", "(", "new_distribution", ".", "mean", "(", ")", ",", "tf", ".", "stop_gradient", "(", "old_action", ")", ")", ")", "\n", "for", "builder", ",", "new_distribution", ",", "old_action", "in", "zip", "(", "policy_loss_builder", ",", "new_policy_distributions", ",", "self", ".", "old_action_batch", ")", "\n", ")", "\n", "self", ".", "policy_loss", "+=", "tf", ".", "cond", "(", "\n", "pred", "=", "self", ".", "is_replayed_batch", "[", "0", "]", ",", "\n", "true_fn", "=", "lambda", ":", "constrain_loss", ",", "\n", "false_fn", "=", "lambda", ":", "tf", ".", "constant", "(", "0.", ",", "dtype", "=", "self", ".", "parameters_type", ")", "\n", ")", "\n", "# [Total loss]", "\n", "", "self", ".", "total_loss", "=", "self", ".", "policy_loss", "+", "self", ".", "value_loss", "\n", "if", "flags", ".", "intrinsic_reward", ":", "\n", "\t\t\t", "self", ".", "total_loss", "+=", "self", ".", "intrinsic_reward_loss", "\n", "\n"]], "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.algorithm.ac_algorithm.AC_Algorithm.get_shared_keys": [[358, 364], ["set", "sorted", "ac_algorithm.AC_Algorithm.get_network_partitions", "itertools.chain.from_iterable"], "methods", ["home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.buffer.pseudo_prioritized_buffer.PseudoPrioritizedBuffer.set", "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.algorithm.ac_algorithm.AC_Algorithm.get_network_partitions"], ["", "", "def", "get_shared_keys", "(", "self", ",", "partitions", "=", "None", ")", ":", "\n", "\t\t", "if", "partitions", "is", "None", ":", "\n", "\t\t\t", "partitions", "=", "self", ".", "get_network_partitions", "(", ")", "\n", "# set removes duplicates", "\n", "", "key_list", "=", "set", "(", "it", ".", "chain", ".", "from_iterable", "(", "self", ".", "network", "[", "p", "]", ".", "shared_keys", "for", "p", "in", "partitions", ")", ")", "\n", "return", "sorted", "(", "key_list", ",", "key", "=", "lambda", "x", ":", "x", ".", "name", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.algorithm.ac_algorithm.AC_Algorithm.get_update_keys": [[365, 371], ["set", "sorted", "ac_algorithm.AC_Algorithm.get_network_partitions", "itertools.chain.from_iterable"], "methods", ["home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.buffer.pseudo_prioritized_buffer.PseudoPrioritizedBuffer.set", "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.algorithm.ac_algorithm.AC_Algorithm.get_network_partitions"], ["", "def", "get_update_keys", "(", "self", ",", "partitions", "=", "None", ")", ":", "\n", "\t\t", "if", "partitions", "is", "None", ":", "\n", "\t\t\t", "partitions", "=", "self", ".", "get_network_partitions", "(", ")", "\n", "# set removes duplicates", "\n", "", "key_list", "=", "set", "(", "it", ".", "chain", ".", "from_iterable", "(", "self", ".", "network", "[", "p", "]", ".", "update_keys", "for", "p", "in", "partitions", ")", ")", "\n", "return", "sorted", "(", "key_list", ",", "key", "=", "lambda", "x", ":", "x", ".", "name", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.algorithm.ac_algorithm.AC_Algorithm._get_train_op": [[372, 379], ["tensorflow.control_dependencies", "optimizer.compute_gradients", "zip", "tuple", "optimizer.apply_gradients", "zip"], "methods", ["None"], ["", "def", "_get_train_op", "(", "self", ",", "global_step", ",", "optimizer", ",", "loss", ",", "shared_keys", ",", "update_keys", ",", "global_keys", ")", ":", "\n", "\t\t", "with", "tf", ".", "control_dependencies", "(", "update_keys", ")", ":", "# control_dependencies is for batch normalization", "\n", "\t\t\t", "grads_and_vars", "=", "optimizer", ".", "compute_gradients", "(", "loss", "=", "loss", ",", "var_list", "=", "shared_keys", ")", "\n", "# grads_and_vars = [(g, v) for g, v in grads_and_vars if g is not None]", "\n", "grad", ",", "vars", "=", "zip", "(", "*", "grads_and_vars", ")", "\n", "global_grads_and_vars", "=", "tuple", "(", "zip", "(", "grad", ",", "global_keys", ")", ")", "\n", "return", "optimizer", ".", "apply_gradients", "(", "global_grads_and_vars", ",", "global_step", "=", "global_step", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.algorithm.ac_algorithm.AC_Algorithm.minimize_local_loss": [[380, 406], ["optimizer.values", "ac_algorithm.AC_Algorithm._get_train_op", "ac_algorithm.AC_Algorithm._get_train_op", "ac_algorithm.AC_Algorithm._get_train_op", "ac_algorithm.AC_Algorithm.get_shared_keys", "global_agent.get_shared_keys", "ac_algorithm.AC_Algorithm.get_update_keys", "ac_algorithm.AC_Algorithm.get_shared_keys", "global_agent.get_shared_keys", "ac_algorithm.AC_Algorithm.get_update_keys", "ac_algorithm.AC_Algorithm.get_shared_keys", "global_agent.get_shared_keys", "ac_algorithm.AC_Algorithm.get_update_keys"], "methods", ["home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.algorithm.ac_algorithm.AC_Algorithm._get_train_op", "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.algorithm.ac_algorithm.AC_Algorithm._get_train_op", "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.algorithm.ac_algorithm.AC_Algorithm._get_train_op", "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.algorithm.ac_algorithm.AC_Algorithm.get_shared_keys", "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.algorithm.ac_algorithm.AC_Algorithm.get_shared_keys", "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.algorithm.ac_algorithm.AC_Algorithm.get_update_keys", "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.algorithm.ac_algorithm.AC_Algorithm.get_shared_keys", "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.algorithm.ac_algorithm.AC_Algorithm.get_shared_keys", "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.algorithm.ac_algorithm.AC_Algorithm.get_update_keys", "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.algorithm.ac_algorithm.AC_Algorithm.get_shared_keys", "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.algorithm.ac_algorithm.AC_Algorithm.get_shared_keys", "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.algorithm.ac_algorithm.AC_Algorithm.get_update_keys"], ["", "", "def", "minimize_local_loss", "(", "self", ",", "optimizer", ",", "global_step", ",", "global_agent", ")", ":", "# minimize loss and apply gradients to global vars.", "\n", "\t\t", "actor_optimizer", ",", "critic_optimizer", ",", "reward_optimizer", "=", "optimizer", ".", "values", "(", ")", "\n", "self", ".", "actor_op", "=", "self", ".", "_get_train_op", "(", "\n", "global_step", "=", "global_step", ",", "\n", "optimizer", "=", "actor_optimizer", ",", "\n", "loss", "=", "self", ".", "policy_loss", ",", "\n", "shared_keys", "=", "self", ".", "get_shared_keys", "(", "[", "'Actor'", "]", ")", ",", "\n", "global_keys", "=", "global_agent", ".", "get_shared_keys", "(", "[", "'Actor'", "]", ")", ",", "\n", "update_keys", "=", "self", ".", "get_update_keys", "(", "[", "'Actor'", "]", ")", "\n", ")", "\n", "self", ".", "critic_op", "=", "self", ".", "_get_train_op", "(", "\n", "global_step", "=", "global_step", ",", "\n", "optimizer", "=", "critic_optimizer", ",", "\n", "loss", "=", "self", ".", "value_loss", ",", "\n", "shared_keys", "=", "self", ".", "get_shared_keys", "(", "[", "'Critic'", "]", ")", ",", "\n", "global_keys", "=", "global_agent", ".", "get_shared_keys", "(", "[", "'Critic'", "]", ")", ",", "\n", "update_keys", "=", "self", ".", "get_update_keys", "(", "[", "'Critic'", "]", ")", "\n", ")", "\n", "if", "flags", ".", "intrinsic_reward", ":", "\n", "\t\t\t", "self", ".", "reward_op", "=", "self", ".", "_get_train_op", "(", "\n", "global_step", "=", "global_step", ",", "\n", "optimizer", "=", "reward_optimizer", ",", "\n", "loss", "=", "self", ".", "intrinsic_reward_loss", ",", "\n", "shared_keys", "=", "self", ".", "get_shared_keys", "(", "[", "'Reward'", "]", ")", ",", "\n", "global_keys", "=", "global_agent", ".", "get_shared_keys", "(", "[", "'Reward'", "]", ")", ",", "\n", "update_keys", "=", "self", ".", "get_update_keys", "(", "[", "'Reward'", "]", ")", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.algorithm.ac_algorithm.AC_Algorithm.bind_sync": [[408, 417], ["tensorflow.name_scope", "src_network.get_shared_keys", "ac_algorithm.AC_Algorithm.get_shared_keys", "zip", "tensorflow.group", "tensorflow.assign", "sync_ops.append"], "methods", ["home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.algorithm.ac_algorithm.AC_Algorithm.get_shared_keys", "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.algorithm.ac_algorithm.AC_Algorithm.get_shared_keys", "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.worker.batch.ExperienceBatch.append"], ["", "", "def", "bind_sync", "(", "self", ",", "src_network", ",", "name", "=", "None", ")", ":", "\n", "\t\t", "with", "tf", ".", "name_scope", "(", "name", ",", "\"Sync{0}\"", ".", "format", "(", "self", ".", "id", ")", ",", "[", "]", ")", "as", "name", ":", "\n", "\t\t\t", "src_vars", "=", "src_network", ".", "get_shared_keys", "(", ")", "\n", "dst_vars", "=", "self", ".", "get_shared_keys", "(", ")", "\n", "sync_ops", "=", "[", "]", "\n", "for", "(", "src_var", ",", "dst_var", ")", "in", "zip", "(", "src_vars", ",", "dst_vars", ")", ":", "\n", "\t\t\t\t", "sync_op", "=", "tf", ".", "assign", "(", "dst_var", ",", "src_var", ")", "# no need for locking dst_var", "\n", "sync_ops", ".", "append", "(", "sync_op", ")", "\n", "", "self", ".", "sync_op", "=", "tf", ".", "group", "(", "*", "sync_ops", ",", "name", "=", "name", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.algorithm.ac_algorithm.AC_Algorithm.sync": [[418, 420], ["tensorflow.get_default_session().run", "tensorflow.get_default_session"], "methods", ["None"], ["", "", "def", "sync", "(", "self", ")", ":", "\n", "\t\t", "tf", ".", "get_default_session", "(", ")", ".", "run", "(", "fetches", "=", "self", ".", "sync_op", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.algorithm.ac_algorithm.AC_Algorithm.predict_reward": [[421, 429], ["ac_algorithm.AC_Algorithm._get_multihead_feed", "ac_algorithm.AC_Algorithm.update", "ac_algorithm.AC_Algorithm.update", "tensorflow.get_default_session().run", "ac_algorithm.AC_Algorithm._get_multihead_feed", "ac_algorithm.AC_Algorithm._get_multihead_feed", "tensorflow.get_default_session"], "methods", ["home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.algorithm.ac_algorithm.AC_Algorithm._get_multihead_feed", "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.utils.running_std.RunningMeanStd.update", "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.utils.running_std.RunningMeanStd.update", "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.algorithm.ac_algorithm.AC_Algorithm._get_multihead_feed", "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.algorithm.ac_algorithm.AC_Algorithm._get_multihead_feed"], ["", "def", "predict_reward", "(", "self", ",", "reward_dict", ")", ":", "\n", "\t\t", "assert", "flags", ".", "intrinsic_reward", ",", "\"Cannot get intrinsic reward if the RND layer is not built\"", "\n", "# State", "\n", "feed_dict", "=", "self", ".", "_get_multihead_feed", "(", "target", "=", "self", ".", "state_batch", ",", "source", "=", "reward_dict", "[", "'states'", "]", ")", "\n", "feed_dict", ".", "update", "(", "self", ".", "_get_multihead_feed", "(", "target", "=", "self", ".", "state_mean_batch", ",", "source", "=", "[", "reward_dict", "[", "'state_mean'", "]", "]", ")", ")", "\n", "feed_dict", ".", "update", "(", "self", ".", "_get_multihead_feed", "(", "target", "=", "self", ".", "state_std_batch", ",", "source", "=", "[", "reward_dict", "[", "'state_std'", "]", "]", ")", ")", "\n", "# Return intrinsic_reward", "\n", "return", "tf", ".", "get_default_session", "(", ")", ".", "run", "(", "fetches", "=", "self", ".", "intrinsic_reward_batch", ",", "feed_dict", "=", "feed_dict", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.algorithm.ac_algorithm.AC_Algorithm.predict_value": [[430, 446], ["enumerate", "ac_algorithm.AC_Algorithm._get_multihead_feed", "tensorflow.get_default_session().run", "ac_algorithm.AC_Algorithm.update", "ac_algorithm.AC_Algorithm.update", "ac_algorithm.AC_Algorithm._get_internal_state_feed", "tensorflow.get_default_session"], "methods", ["home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.algorithm.ac_algorithm.AC_Algorithm._get_multihead_feed", "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.utils.running_std.RunningMeanStd.update", "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.utils.running_std.RunningMeanStd.update", "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.algorithm.ac_algorithm.AC_Algorithm._get_internal_state_feed"], ["", "def", "predict_value", "(", "self", ",", "value_dict", ")", ":", "\n", "\t\t", "state_batch", "=", "value_dict", "[", "'states'", "]", "\n", "size_batch", "=", "value_dict", "[", "'sizes'", "]", "\n", "bootstrap", "=", "value_dict", "[", "'bootstrap'", "]", "\n", "for", "i", ",", "b", "in", "enumerate", "(", "bootstrap", ")", ":", "\n", "\t\t\t", "state_batch", "=", "state_batch", "+", "[", "b", "[", "'state'", "]", "]", "\n", "size_batch", "[", "i", "]", "+=", "1", "\n", "# State", "\n", "", "feed_dict", "=", "self", ".", "_get_multihead_feed", "(", "target", "=", "self", ".", "state_batch", ",", "source", "=", "state_batch", ")", "\n", "# Internal State", "\n", "if", "flags", ".", "network_has_internal_state", ":", "\n", "\t\t\t", "feed_dict", ".", "update", "(", "self", ".", "_get_internal_state_feed", "(", "value_dict", "[", "'internal_states'", "]", ")", ")", "\n", "feed_dict", ".", "update", "(", "{", "self", ".", "size_batch", ":", "size_batch", "}", ")", "\n", "# Return value_batch", "\n", "", "value_batch", "=", "tf", ".", "get_default_session", "(", ")", ".", "run", "(", "fetches", "=", "self", ".", "state_value_batch", ",", "feed_dict", "=", "feed_dict", ")", "\n", "return", "value_batch", "[", ":", "-", "1", "]", ",", "value_batch", "[", "-", "1", "]", ",", "None", "\n", "\n"]], "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.algorithm.ac_algorithm.AC_Algorithm.predict_action": [[447, 478], ["len", "ac_algorithm.AC_Algorithm._get_multihead_feed", "tensorflow.get_default_session().run", "tuple", "tuple", "tuple", "ac_algorithm.AC_Algorithm.update", "ac_algorithm.AC_Algorithm.update", "len", "zip", "zip", "zip", "ac_algorithm.AC_Algorithm._get_internal_state_feed", "tensorflow.get_default_session", "ac_algorithm.AC_Algorithm._get_internal_state", "range"], "methods", ["home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.algorithm.ac_algorithm.AC_Algorithm._get_multihead_feed", "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.utils.running_std.RunningMeanStd.update", "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.utils.running_std.RunningMeanStd.update", "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.algorithm.ac_algorithm.AC_Algorithm._get_internal_state_feed", "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.algorithm.ac_algorithm.AC_Algorithm._get_internal_state"], ["", "def", "predict_action", "(", "self", ",", "action_dict", ")", ":", "\n", "\t\t", "batch_size", "=", "action_dict", "[", "'sizes'", "]", "\n", "batch_count", "=", "len", "(", "batch_size", ")", "\n", "# State", "\n", "feed_dict", "=", "self", ".", "_get_multihead_feed", "(", "target", "=", "self", ".", "state_batch", ",", "source", "=", "action_dict", "[", "'states'", "]", ")", "\n", "# Internal state", "\n", "if", "flags", ".", "network_has_internal_state", ":", "\n", "\t\t\t", "feed_dict", ".", "update", "(", "self", ".", "_get_internal_state_feed", "(", "action_dict", "[", "'internal_states'", "]", ")", ")", "\n", "feed_dict", ".", "update", "(", "{", "self", ".", "size_batch", ":", "batch_size", "}", ")", "\n", "# Return action_batch, policy_batch, new_internal_state", "\n", "", "action_batch", ",", "hot_action_batch", ",", "policy_batch", ",", "value_batch", ",", "new_internal_states", "=", "tf", ".", "get_default_session", "(", ")", ".", "run", "(", "fetches", "=", "[", "self", ".", "action_batch", ",", "self", ".", "hot_action_batch", ",", "self", ".", "actor_batch", ",", "self", ".", "state_value_batch", ",", "self", ".", "_get_internal_state", "(", ")", "]", ",", "feed_dict", "=", "feed_dict", ")", "\n", "# Properly format for output the internal state", "\n", "if", "len", "(", "new_internal_states", ")", "==", "0", ":", "\n", "\t\t\t", "new_internal_states", "=", "[", "new_internal_states", "]", "*", "batch_count", "\n", "", "else", ":", "\n", "\t\t\t", "new_internal_states", "=", "[", "\n", "[", "\n", "[", "\n", "sub_partition_new_internal_state", "[", "i", "]", "\n", "for", "sub_partition_new_internal_state", "in", "partition_new_internal_states", "\n", "]", "\n", "for", "partition_new_internal_states", "in", "new_internal_states", "\n", "]", "\n", "for", "i", "in", "range", "(", "batch_count", ")", "\n", "]", "\n", "# Properly format for output: action and policy may have multiple heads, swap 1st and 2nd axis", "\n", "", "action_batch", "=", "tuple", "(", "zip", "(", "*", "action_batch", ")", ")", "\n", "hot_action_batch", "=", "tuple", "(", "zip", "(", "*", "hot_action_batch", ")", ")", "\n", "policy_batch", "=", "tuple", "(", "zip", "(", "*", "policy_batch", ")", ")", "\n", "# Return output", "\n", "return", "action_batch", ",", "hot_action_batch", ",", "policy_batch", ",", "value_batch", ",", "new_internal_states", "\n", "\n"]], "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.algorithm.ac_algorithm.AC_Algorithm._get_internal_state": [[479, 481], ["tuple", "ac_algorithm.AC_Algorithm.get_network_partitions"], "methods", ["home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.algorithm.ac_algorithm.AC_Algorithm.get_network_partitions"], ["", "def", "_get_internal_state", "(", "self", ")", ":", "\n", "\t\t", "return", "tuple", "(", "self", ".", "network", "[", "p", "]", ".", "internal_initial_state", "for", "p", "in", "self", ".", "get_network_partitions", "(", ")", "if", "self", ".", "network", "[", "p", "]", ".", "use_internal_state", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.algorithm.ac_algorithm.AC_Algorithm._get_internal_state_feed": [[482, 498], ["ac_algorithm.AC_Algorithm.get_network_partitions", "enumerate", "zip", "feed_dict.update"], "methods", ["home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.algorithm.ac_algorithm.AC_Algorithm.get_network_partitions", "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.utils.running_std.RunningMeanStd.update"], ["", "def", "_get_internal_state_feed", "(", "self", ",", "internal_states", ")", ":", "\n", "\t\t", "if", "not", "flags", ".", "network_has_internal_state", ":", "\n", "\t\t\t", "return", "{", "}", "\n", "", "feed_dict", "=", "{", "}", "\n", "i", "=", "0", "\n", "for", "partition", "in", "self", ".", "get_network_partitions", "(", ")", ":", "\n", "\t\t\t", "network_partition", "=", "self", ".", "network", "[", "partition", "]", "\n", "if", "network_partition", ".", "use_internal_state", ":", "\n", "\t\t\t\t", "partition_batch_states", "=", "[", "\n", "network_partition", ".", "internal_default_state", "if", "internal_state", "is", "None", "else", "internal_state", "[", "i", "]", "\n", "for", "internal_state", "in", "internal_states", "\n", "]", "\n", "for", "j", ",", "initial_state", "in", "enumerate", "(", "zip", "(", "*", "partition_batch_states", ")", ")", ":", "\n", "\t\t\t\t\t", "feed_dict", ".", "update", "(", "{", "network_partition", ".", "internal_initial_state", "[", "j", "]", ":", "initial_state", "}", ")", "\n", "", "i", "+=", "1", "\n", "", "", "return", "feed_dict", "\n", "\n"]], "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.algorithm.ac_algorithm.AC_Algorithm._get_multihead_feed": [[499, 502], ["zip", "zip"], "methods", ["None"], ["", "def", "_get_multihead_feed", "(", "self", ",", "source", ",", "target", ")", ":", "\n", "# Action and policy may have multiple heads, swap 1st and 2nd axis of source with zip*", "\n", "\t\t", "return", "{", "t", ":", "s", "for", "t", ",", "s", "in", "zip", "(", "target", ",", "zip", "(", "*", "source", ")", ")", "}", "\n", "\n"]], "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.algorithm.ac_algorithm.AC_Algorithm.prepare_train": [[503, 525], ["ac_algorithm.AC_Algorithm._build_train_feed", "ac_algorithm.AC_Algorithm.items", "current_global_feed[].extend", "ac_algorithm.AC_Algorithm._train", "collections.deque"], "methods", ["home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.algorithm.ac_algorithm.AC_Algorithm._build_train_feed", "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.worker.network_manager.NetworkManager._train"], ["", "def", "prepare_train", "(", "self", ",", "train_dict", ",", "replay", ")", ":", "\n", "\t\t", "''' Prepare training batch, then _train once using the biggest possible batch '''", "\n", "train_type", "=", "1", "if", "replay", "else", "0", "\n", "# Get global feed", "\n", "current_global_feed", "=", "self", ".", "_big_batch_feed", "[", "train_type", "]", "\n", "# Build local feed", "\n", "local_feed", "=", "self", ".", "_build_train_feed", "(", "train_dict", ")", "\n", "# Merge feed dictionary", "\n", "for", "key", ",", "value", "in", "local_feed", ".", "items", "(", ")", ":", "\n", "\t\t\t", "if", "key", "not", "in", "current_global_feed", ":", "\n", "\t\t\t\t", "current_global_feed", "[", "key", "]", "=", "deque", "(", "maxlen", "=", "self", ".", "_train_batch_size", ")", "# Initializing the main_feed_dict ", "\n", "", "current_global_feed", "[", "key", "]", ".", "extend", "(", "value", ")", "\n", "# Increase the number of batches composing the big batch", "\n", "", "self", ".", "_batch_count", "[", "train_type", "]", "+=", "1", "\n", "if", "self", ".", "_batch_count", "[", "train_type", "]", "%", "flags", ".", "big_batch_size", "==", "0", ":", "# can _train", "\n", "# Reset batch counter", "\n", "\t\t\t", "self", ".", "_batch_count", "[", "train_type", "]", "=", "0", "\n", "# Reset big-batch (especially if network_has_internal_state) otherwise when in GPU mode it's more time and memory efficient to not reset the big-batch, in order to keep its size fixed", "\n", "self", ".", "_big_batch_feed", "[", "train_type", "]", "=", "{", "}", "\n", "# Train", "\n", "return", "self", ".", "_train", "(", "feed_dict", "=", "current_global_feed", ",", "replay", "=", "replay", ",", "state_mean_std", "=", "(", "train_dict", "[", "'state_mean'", "]", ",", "train_dict", "[", "'state_std'", "]", ")", ")", "\n", "", "return", "None", "\n", "\n"]], "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.algorithm.ac_algorithm.AC_Algorithm._train": [[526, 572], ["feed_dict.update", "tensorflow.get_default_session().run", "ac_algorithm.AC_Algorithm.sync", "feed_dict.update", "feed_dict.update", "ac_algorithm.AC_Algorithm._train_statistics.add", "ac_algorithm.AC_Algorithm._get_multihead_feed", "ac_algorithm.AC_Algorithm._get_multihead_feed", "tensorflow.get_default_session"], "methods", ["home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.utils.running_std.RunningMeanStd.update", "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.worker.network_manager.NetworkManager.sync", "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.utils.running_std.RunningMeanStd.update", "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.utils.running_std.RunningMeanStd.update", "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.worker.batch.CompositeBatch.add", "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.algorithm.ac_algorithm.AC_Algorithm._get_multihead_feed", "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.algorithm.ac_algorithm.AC_Algorithm._get_multihead_feed"], ["", "def", "_train", "(", "self", ",", "feed_dict", ",", "replay", "=", "False", ",", "state_mean_std", "=", "None", ")", ":", "\n", "# Add replay boolean to feed dictionary", "\n", "\t\t", "feed_dict", ".", "update", "(", "{", "self", ".", "is_replayed_batch", ":", "[", "replay", "]", "}", ")", "\n", "# Intrinsic Reward", "\n", "if", "flags", ".", "intrinsic_reward", ":", "\n", "\t\t\t", "state_mean", ",", "state_std", "=", "state_mean_std", "\n", "feed_dict", ".", "update", "(", "self", ".", "_get_multihead_feed", "(", "target", "=", "self", ".", "state_mean_batch", ",", "source", "=", "[", "state_mean", "]", ")", ")", "\n", "feed_dict", ".", "update", "(", "self", ".", "_get_multihead_feed", "(", "target", "=", "self", ".", "state_std_batch", ",", "source", "=", "[", "state_std", "]", ")", ")", "\n", "# Build _train fetches", "\n", "", "train_tuple", "=", "(", "self", ".", "actor_op", ",", "self", ".", "critic_op", ")", "if", "not", "replay", "or", "flags", ".", "train_critic_when_replaying", "else", "(", "self", ".", "actor_op", ",", ")", "\n", "# Do not replay intrinsic reward training otherwise it would start to reward higher the states distant from extrinsic rewards", "\n", "if", "flags", ".", "intrinsic_reward", "and", "not", "replay", ":", "\n", "\t\t\t", "train_tuple", "+=", "(", "self", ".", "reward_op", ",", ")", "\n", "# Build fetch", "\n", "", "fetches", "=", "[", "train_tuple", "]", "# Minimize loss", "\n", "if", "flags", ".", "print_loss", ":", "# Get loss values for logging", "\n", "\t\t\t", "fetches", "+=", "[", "(", "self", ".", "total_loss", ",", "self", ".", "policy_loss", ",", "self", ".", "value_loss", ")", "]", "\n", "", "else", ":", "\n", "\t\t\t", "fetches", "+=", "[", "(", ")", "]", "\n", "", "if", "flags", ".", "print_policy_info", ":", "# Debug info", "\n", "\t\t\t", "fetches", "+=", "[", "(", "self", ".", "policy_kl_divergence", ",", "self", ".", "policy_clipping_frequency", ",", "self", ".", "policy_entropy_regularization", ")", "]", "\n", "", "else", ":", "\n", "\t\t\t", "fetches", "+=", "[", "(", ")", "]", "\n", "", "if", "flags", ".", "intrinsic_reward", ":", "\n", "\t\t\t", "fetches", "+=", "[", "(", "self", ".", "intrinsic_reward_loss", ",", ")", "]", "\n", "", "else", ":", "\n", "\t\t\t", "fetches", "+=", "[", "(", ")", "]", "\n", "# Run", "\n", "", "_", ",", "loss", ",", "policy_info", ",", "reward_info", "=", "tf", ".", "get_default_session", "(", ")", ".", "run", "(", "fetches", "=", "fetches", ",", "feed_dict", "=", "feed_dict", ")", "\n", "self", ".", "sync", "(", ")", "\n", "# Build and return loss dict", "\n", "train_info", "=", "{", "}", "\n", "if", "flags", ".", "print_loss", ":", "\n", "\t\t\t", "train_info", "[", "\"loss_total\"", "]", ",", "train_info", "[", "\"loss_actor\"", "]", ",", "train_info", "[", "\"loss_critic\"", "]", "=", "loss", "\n", "", "if", "flags", ".", "print_policy_info", ":", "\n", "\t\t\t", "train_info", "[", "\"actor_kl_divergence\"", "]", ",", "train_info", "[", "\"actor_clipping_frequency\"", "]", ",", "train_info", "[", "\"actor_entropy\"", "]", "=", "policy_info", "\n", "", "if", "flags", ".", "intrinsic_reward", ":", "\n", "\t\t\t", "train_info", "[", "\"intrinsic_reward_loss\"", "]", "=", "reward_info", "\n", "# Build loss statistics", "\n", "", "if", "train_info", ":", "\n", "\t\t\t", "self", ".", "_train_statistics", ".", "add", "(", "stat_dict", "=", "train_info", ",", "type", "=", "'train{}_'", ".", "format", "(", "self", ".", "model_id", ")", ")", "\n", "#=======================================================================", "\n", "# if self.loss_distribution_estimator.update([abs(train_info['loss_actor'])]):", "\n", "# \tself.actor_loss_is_too_small = self.loss_distribution_estimator.mean <= flags.loss_stationarity_range", "\n", "#=======================================================================", "\n", "", "return", "train_info", "\n", "\n"]], "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.algorithm.ac_algorithm.AC_Algorithm._build_train_feed": [[573, 593], ["feed_dict.update", "feed_dict.update", "feed_dict.update", "ac_algorithm.AC_Algorithm._get_multihead_feed", "feed_dict.update", "ac_algorithm.AC_Algorithm._get_multihead_feed", "ac_algorithm.AC_Algorithm._get_multihead_feed", "feed_dict.update", "feed_dict.update", "feed_dict.update", "ac_algorithm.AC_Algorithm._get_multihead_feed", "ac_algorithm.AC_Algorithm._get_internal_state_feed", "len"], "methods", ["home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.utils.running_std.RunningMeanStd.update", "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.utils.running_std.RunningMeanStd.update", "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.utils.running_std.RunningMeanStd.update", "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.algorithm.ac_algorithm.AC_Algorithm._get_multihead_feed", "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.utils.running_std.RunningMeanStd.update", "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.algorithm.ac_algorithm.AC_Algorithm._get_multihead_feed", "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.algorithm.ac_algorithm.AC_Algorithm._get_multihead_feed", "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.utils.running_std.RunningMeanStd.update", "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.utils.running_std.RunningMeanStd.update", "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.utils.running_std.RunningMeanStd.update", "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.algorithm.ac_algorithm.AC_Algorithm._get_multihead_feed", "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.algorithm.ac_algorithm.AC_Algorithm._get_internal_state_feed"], ["", "def", "_build_train_feed", "(", "self", ",", "train_dict", ")", ":", "\n", "# State & Cumulative Return & Old Value", "\n", "\t\t", "feed_dict", "=", "{", "\n", "self", ".", "cumulative_return_batch", ":", "train_dict", "[", "'cumulative_returns'", "]", ",", "\n", "self", ".", "old_state_value_batch", ":", "train_dict", "[", "'values'", "]", ",", "\n", "}", "\n", "feed_dict", ".", "update", "(", "self", ".", "_get_multihead_feed", "(", "target", "=", "self", ".", "state_batch", ",", "source", "=", "train_dict", "[", "'states'", "]", ")", ")", "\n", "# Advantage", "\n", "if", "not", "flags", ".", "runtime_advantage", ":", "\n", "\t\t\t", "feed_dict", ".", "update", "(", "{", "self", ".", "advantage_batch", ":", "train_dict", "[", "'advantages'", "]", "}", ")", "\n", "# Old Policy & Action", "\n", "", "feed_dict", ".", "update", "(", "self", ".", "_get_multihead_feed", "(", "target", "=", "self", ".", "old_policy_batch", ",", "source", "=", "train_dict", "[", "'policies'", "]", ")", ")", "\n", "feed_dict", ".", "update", "(", "self", ".", "_get_multihead_feed", "(", "target", "=", "self", ".", "old_action_batch", ",", "source", "=", "train_dict", "[", "'actions'", "]", ")", ")", "\n", "if", "self", ".", "has_masked_actions", ":", "\n", "\t\t\t", "feed_dict", ".", "update", "(", "self", ".", "_get_multihead_feed", "(", "target", "=", "self", ".", "old_action_mask_batch", ",", "source", "=", "train_dict", "[", "'action_masks'", "]", ")", ")", "\n", "# Internal State", "\n", "", "if", "flags", ".", "network_has_internal_state", ":", "\n", "\t\t\t", "feed_dict", ".", "update", "(", "self", ".", "_get_internal_state_feed", "(", "[", "train_dict", "[", "'internal_state'", "]", "]", ")", ")", "\n", "feed_dict", ".", "update", "(", "{", "self", ".", "size_batch", ":", "[", "len", "(", "train_dict", "[", "'cumulative_returns'", "]", ")", "]", "}", ")", "\n", "", "return", "feed_dict", "\n", "", "", ""]], "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.algorithm.ac_algorithm.merge_splitted_advantages": [[19, 21], ["None"], "function", ["None"], ["def", "merge_splitted_advantages", "(", "advantage", ")", ":", "\n", "\t", "return", "flags", ".", "extrinsic_coefficient", "*", "advantage", "[", "0", "]", "+", "flags", ".", "intrinsic_coefficient", "*", "advantage", "[", "1", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.loss.value_loss.ValueLoss.__init__": [[9, 18], ["type.lower", "tensorflow.stop_gradient", "tensorflow.stop_gradient", "eval"], "methods", ["None"], ["\t", "def", "__init__", "(", "self", ",", "global_step", ",", "type", ",", "estimation", ",", "old_estimation", ",", "cumulative_reward", ")", ":", "\n", "\t\t", "self", ".", "global_step", "=", "global_step", "\n", "self", ".", "type", "=", "type", ".", "lower", "(", ")", "\n", "self", ".", "estimation", "=", "estimation", "\n", "# Stop gradients", "\n", "self", ".", "old_estimation", "=", "tf", ".", "stop_gradient", "(", "old_estimation", ")", "\n", "self", ".", "cumulative_return", "=", "tf", ".", "stop_gradient", "(", "cumulative_reward", ")", "\n", "# Get reduce function", "\n", "self", ".", "reduce_function", "=", "eval", "(", "'tf.reduce_{}'", ".", "format", "(", "flags", ".", "loss_type", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.loss.value_loss.ValueLoss.get": [[19, 21], ["eval"], "methods", ["None"], ["", "def", "get", "(", "self", ")", ":", "\n", "\t\t", "return", "eval", "(", "'self.{}'", ".", "format", "(", "self", ".", "type", ")", ")", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.loss.value_loss.ValueLoss.vanilla": [[22, 27], ["value_loss.ValueLoss.reduce_function", "tensorflow.squared_difference", "tensorflow.reduce_sum"], "methods", ["None"], ["", "def", "vanilla", "(", "self", ")", ":", "\n", "# reduce over batches (1st ax)", "\n", "\t\t", "losses", "=", "self", ".", "reduce_function", "(", "tf", ".", "squared_difference", "(", "self", ".", "cumulative_return", ",", "self", ".", "estimation", ")", ",", "0", ")", "\n", "# sum values (last ax)", "\n", "return", "0.5", "*", "tf", ".", "reduce_sum", "(", "losses", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.loss.value_loss.ValueLoss.pvo": [[29, 46], ["tensorflow.cast", "tensorflow.maximum", "value_loss.ValueLoss.reduce_function", "utils.get_annealable_variable", "tensorflow.clip_by_value", "tensorflow.abs", "tensorflow.abs", "tensorflow.square", "tensorflow.reduce_sum"], "methods", ["home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.utils.tensorflow_utils.get_annealable_variable"], ["", "def", "pvo", "(", "self", ")", ":", "\n", "# clip", "\n", "\t\t", "clip_range", "=", "tf_utils", ".", "get_annealable_variable", "(", "\n", "function_name", "=", "flags", ".", "clip_annealing_function", ",", "\n", "initial_value", "=", "flags", ".", "clip", ",", "\n", "global_step", "=", "self", ".", "global_step", ",", "\n", "decay_steps", "=", "flags", ".", "clip_decay_steps", ",", "\n", "decay_rate", "=", "flags", ".", "clip_decay_rate", "\n", ")", "if", "flags", ".", "clip_decay", "else", "flags", ".", "clip", "\n", "clip_range", "=", "tf", ".", "cast", "(", "clip_range", ",", "self", ".", "estimation", ".", "dtype", ")", "\n", "# clipped estimation", "\n", "estimation_clipped", "=", "self", ".", "old_estimation", "+", "tf", ".", "clip_by_value", "(", "self", ".", "estimation", "-", "self", ".", "old_estimation", ",", "-", "clip_range", ",", "clip_range", ")", "\n", "max", "=", "tf", ".", "maximum", "(", "tf", ".", "abs", "(", "self", ".", "cumulative_return", "-", "self", ".", "estimation", ")", ",", "tf", ".", "abs", "(", "self", ".", "cumulative_return", "-", "estimation_clipped", ")", ")", "\n", "# reduce over batches (1st ax)", "\n", "losses", "=", "self", ".", "reduce_function", "(", "tf", ".", "square", "(", "max", ")", ",", "0", ")", "\n", "# sum values (last ax)", "\n", "return", "0.5", "*", "tf", ".", "reduce_sum", "(", "losses", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.loss.policy_loss.PolicyLoss.__init__": [[10, 41], ["type.lower", "tensorflow.constant", "tensorflow.constant", "tensorflow.cast", "tensorflow.stop_gradient", "eval", "policy_loss.PolicyLoss.get_ratio", "utils.get_annealable_variable", "tensorflow.maximum", "tensorflow.maximum", "tensorflow.maximum", "len", "tensorflow.reduce_sum", "len", "tensorflow.reduce_sum", "len", "tensorflow.reduce_sum", "policy_loss.PolicyLoss.cross_entropy.get_shape", "policy_loss.PolicyLoss.old_cross_entropy.get_shape", "policy_loss.PolicyLoss.entropy.get_shape"], "methods", ["home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.loss.policy_loss.PolicyLoss.get_ratio", "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.utils.tensorflow_utils.get_annealable_variable"], ["\t", "def", "__init__", "(", "self", ",", "global_step", ",", "type", ",", "cross_entropy", ",", "old_cross_entropy", ",", "entropy", "=", "0", ",", "beta", "=", "0", ")", ":", "\n", "\t\t", "self", ".", "global_step", "=", "global_step", "\n", "self", ".", "type", "=", "type", ".", "lower", "(", ")", "\n", "self", ".", "zero", "=", "tf", ".", "constant", "(", "0.", ",", "dtype", "=", "cross_entropy", ".", "dtype", ")", "\n", "self", ".", "one", "=", "tf", ".", "constant", "(", "1.", ",", "dtype", "=", "cross_entropy", ".", "dtype", ")", "\n", "# Clip", "\n", "self", ".", "clip_range", "=", "tf_utils", ".", "get_annealable_variable", "(", "\n", "function_name", "=", "flags", ".", "clip_annealing_function", ",", "\n", "initial_value", "=", "flags", ".", "clip", ",", "\n", "global_step", "=", "self", ".", "global_step", ",", "\n", "decay_steps", "=", "flags", ".", "clip_decay_steps", ",", "\n", "decay_rate", "=", "flags", ".", "clip_decay_rate", "\n", ")", "if", "flags", ".", "clip_decay", "else", "flags", ".", "clip", "\n", "self", ".", "clip_range", "=", "tf", ".", "cast", "(", "self", ".", "clip_range", ",", "cross_entropy", ".", "dtype", ")", "\n", "# Entropy", "\n", "self", ".", "beta", "=", "beta", "\n", "self", ".", "entropy", "=", "tf", ".", "maximum", "(", "self", ".", "zero", ",", "entropy", ")", "if", "flags", ".", "only_non_negative_entropy", "else", "entropy", "\n", "self", ".", "cross_entropy", "=", "tf", ".", "maximum", "(", "self", ".", "zero", ",", "cross_entropy", ")", "if", "flags", ".", "only_non_negative_entropy", "else", "cross_entropy", "\n", "self", ".", "old_cross_entropy", "=", "tf", ".", "maximum", "(", "self", ".", "zero", ",", "old_cross_entropy", ")", "if", "flags", ".", "only_non_negative_entropy", "else", "old_cross_entropy", "\n", "# Sum entropies in case the agent has to predict more than one action", "\n", "if", "len", "(", "self", ".", "cross_entropy", ".", "get_shape", "(", ")", ")", ">", "1", ":", "\n", "\t\t\t", "self", ".", "cross_entropy", "=", "tf", ".", "reduce_sum", "(", "self", ".", "cross_entropy", ",", "1", ")", "\n", "", "if", "len", "(", "self", ".", "old_cross_entropy", ".", "get_shape", "(", ")", ")", ">", "1", ":", "\n", "\t\t\t", "self", ".", "old_cross_entropy", "=", "tf", ".", "reduce_sum", "(", "self", ".", "old_cross_entropy", ",", "1", ")", "\n", "", "if", "len", "(", "self", ".", "entropy", ".", "get_shape", "(", ")", ")", ">", "1", ":", "\n", "\t\t\t", "self", ".", "entropy", "=", "tf", ".", "reduce_sum", "(", "self", ".", "entropy", ",", "1", ")", "\n", "# Stop gradient", "\n", "", "self", ".", "old_cross_entropy", "=", "tf", ".", "stop_gradient", "(", "self", ".", "old_cross_entropy", ")", "\n", "# Reduction function", "\n", "self", ".", "reduce_function", "=", "eval", "(", "'tf.reduce_{}'", ".", "format", "(", "flags", ".", "loss_type", ")", ")", "\n", "self", ".", "ratio_batch", "=", "self", ".", "get_ratio", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.loss.policy_loss.PolicyLoss.get": [[42, 45], ["tensorflow.stop_gradient", "eval"], "methods", ["None"], ["", "def", "get", "(", "self", ",", "advantage", ")", ":", "\n", "\t\t", "self", ".", "advantage", "=", "tf", ".", "stop_gradient", "(", "advantage", ")", "\n", "return", "eval", "(", "'self.{}'", ".", "format", "(", "self", ".", "type", ")", ")", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.loss.policy_loss.PolicyLoss.approximate_kullback_leibler_divergence": [[46, 48], ["policy_loss.PolicyLoss.reduce_function", "tensorflow.squared_difference"], "methods", ["None"], ["", "def", "approximate_kullback_leibler_divergence", "(", "self", ")", ":", "# https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence", "\n", "\t\t", "return", "0.5", "*", "self", ".", "reduce_function", "(", "tf", ".", "squared_difference", "(", "self", ".", "old_cross_entropy", ",", "self", ".", "cross_entropy", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.loss.policy_loss.PolicyLoss.get_clipping_frequency": [[49, 51], ["tensorflow.reduce_mean", "tensorflow.to_float", "tensorflow.greater", "tensorflow.abs"], "methods", ["None"], ["", "def", "get_clipping_frequency", "(", "self", ")", ":", "\n", "\t\t", "return", "tf", ".", "reduce_mean", "(", "tf", ".", "to_float", "(", "tf", ".", "greater", "(", "tf", ".", "abs", "(", "self", ".", "ratio_batch", "-", "self", ".", "one", ")", ",", "self", ".", "clip_range", ")", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.loss.policy_loss.PolicyLoss.get_entropy_regularization": [[52, 56], ["policy_loss.PolicyLoss.reduce_function"], "methods", ["None"], ["", "def", "get_entropy_regularization", "(", "self", ")", ":", "\n", "\t\t", "if", "self", ".", "beta", "==", "0", ":", "\n", "\t\t\t", "return", "self", ".", "zero", "\n", "", "return", "self", ".", "beta", "*", "self", ".", "reduce_function", "(", "self", ".", "entropy", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.loss.policy_loss.PolicyLoss.get_ratio": [[57, 59], ["tensorflow.exp"], "methods", ["None"], ["", "def", "get_ratio", "(", "self", ")", ":", "\n", "\t\t", "return", "tf", ".", "exp", "(", "self", ".", "old_cross_entropy", "-", "self", ".", "cross_entropy", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.loss.policy_loss.PolicyLoss.vanilla": [[60, 64], ["tensorflow.reduce_sum", "policy_loss.PolicyLoss.reduce_function"], "methods", ["None"], ["", "def", "vanilla", "(", "self", ")", ":", "\n", "\t\t", "gain", "=", "self", ".", "advantage", "*", "self", ".", "cross_entropy", "\n", "# Reduce over batch and then sum all components", "\n", "return", "tf", ".", "reduce_sum", "(", "self", ".", "reduce_function", "(", "gain", ",", "0", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.loss.policy_loss.PolicyLoss.ppo": [[66, 71], ["tensorflow.clip_by_value", "tensorflow.maximum", "tensorflow.reduce_sum", "policy_loss.PolicyLoss.reduce_function"], "methods", ["None"], ["", "def", "ppo", "(", "self", ")", ":", "\n", "\t\t", "clipped_ratio", "=", "tf", ".", "clip_by_value", "(", "self", ".", "ratio_batch", ",", "self", ".", "one", "-", "self", ".", "clip_range", ",", "self", ".", "one", "+", "self", ".", "clip_range", ")", "\n", "gain", "=", "tf", ".", "maximum", "(", "-", "self", ".", "advantage", "*", "self", ".", "ratio_batch", ",", "-", "self", ".", "advantage", "*", "clipped_ratio", ")", "\n", "# Reduce over batch and then sum all components", "\n", "return", "tf", ".", "reduce_sum", "(", "self", ".", "reduce_function", "(", "gain", ",", "0", ")", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.worker.environment_manager.EnvironmentManager.__init__": [[20, 46], ["environment.environment.Environment.create_environment", "eval", "agent.worker.batch.CompositeBatch", "utils.statistics.Statistics", "logging.Formatter", "logging.getLogger", "logging.FileHandler", "logging.FileHandler.setFormatter", "environment_manager.EnvironmentManager.__reward_logger.addHandler", "environment_manager.EnvironmentManager.__reward_logger.setLevel", "float", "os.path.isdir", "os.mkdir", "os.path.isdir", "os.mkdir"], "methods", ["home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.environment.environment.Environment.create_environment"], ["\t", "def", "__init__", "(", "self", ",", "model_size", ",", "group_id", ",", "environment_id", "=", "0", ",", "training", "=", "True", ")", ":", "\n", "\t\t", "self", ".", "model_size", "=", "model_size", "\n", "self", ".", "_training", "=", "training", "\n", "self", ".", "environment_id", "=", "environment_id", "\n", "self", ".", "group_id", "=", "group_id", "\n", "# Build environment", "\n", "self", ".", "environment", "=", "Environment", ".", "create_environment", "(", "flags", ".", "env_type", ",", "self", ".", "environment_id", ",", "self", ".", "_training", ")", "\n", "self", ".", "extrinsic_reward_manipulator", "=", "eval", "(", "flags", ".", "extrinsic_reward_manipulator", ")", "\n", "self", ".", "terminal", "=", "True", "\n", "self", ".", "_composite_batch", "=", "CompositeBatch", "(", "maxlen", "=", "flags", ".", "replay_buffer_size", "if", "flags", ".", "replay_mean", ">", "0", "else", "1", ")", "\n", "# Statistics", "\n", "self", ".", "__client_statistics", "=", "Statistics", "(", "flags", ".", "episode_count_for_evaluation", ")", "\n", "if", "self", ".", "_training", ":", "\n", "#logs", "\n", "\t\t\t", "if", "not", "os", ".", "path", ".", "isdir", "(", "flags", ".", "log_dir", "+", "\"/performance\"", ")", ":", "\n", "\t\t\t\t", "os", ".", "mkdir", "(", "flags", ".", "log_dir", "+", "\"/performance\"", ")", "\n", "", "if", "not", "os", ".", "path", ".", "isdir", "(", "flags", ".", "log_dir", "+", "\"/episodes\"", ")", ":", "\n", "\t\t\t\t", "os", ".", "mkdir", "(", "flags", ".", "log_dir", "+", "\"/episodes\"", ")", "\n", "", "formatter", "=", "logging", ".", "Formatter", "(", "'%(asctime)s %(message)s'", ")", "\n", "# reward logger", "\n", "self", ".", "__reward_logger", "=", "logging", ".", "getLogger", "(", "'reward_{}_{}'", ".", "format", "(", "self", ".", "group_id", ",", "self", ".", "environment_id", ")", ")", "\n", "hdlr", "=", "logging", ".", "FileHandler", "(", "flags", ".", "log_dir", "+", "'/performance/reward_{}_{}.log'", ".", "format", "(", "self", ".", "group_id", ",", "self", ".", "environment_id", ")", ")", "\n", "hdlr", ".", "setFormatter", "(", "formatter", ")", "\n", "self", ".", "__reward_logger", ".", "addHandler", "(", "hdlr", ")", "\n", "self", ".", "__reward_logger", ".", "setLevel", "(", "logging", ".", "DEBUG", ")", "\n", "self", ".", "__max_reward", "=", "float", "(", "\"-inf\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.worker.environment_manager.EnvironmentManager.run_random_steps": [[47, 57], ["environment_manager.EnvironmentManager.environment.reset", "range", "print", "environment_manager.EnvironmentManager.environment.process", "state_batch.append", "environment_manager.EnvironmentManager.environment.sample_random_action", "environment_manager.EnvironmentManager.environment.reset"], "methods", ["home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.environment.environment.Environment.reset", "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.worker.working_group.Group.process", "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.worker.batch.ExperienceBatch.append", "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.environment.environment.Environment.sample_random_action", "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.environment.environment.Environment.reset"], ["", "", "def", "run_random_steps", "(", "self", ",", "step_count", "=", "0", ")", ":", "\n", "\t\t", "state_batch", "=", "[", "]", "\n", "self", ".", "environment", ".", "reset", "(", ")", "\n", "for", "_", "in", "range", "(", "step_count", ")", ":", "\n", "\t\t\t", "new_state", ",", "_", ",", "terminal", ",", "_", "=", "self", ".", "environment", ".", "process", "(", "self", ".", "environment", ".", "sample_random_action", "(", ")", ")", "\n", "state_batch", ".", "append", "(", "new_state", ")", "\n", "if", "terminal", ":", "\n", "\t\t\t\t", "self", ".", "environment", ".", "reset", "(", ")", "\n", "", "", "print", "(", "\"Environment {}.{} initialized\"", ".", "format", "(", "self", ".", "group_id", ",", "self", ".", "environment_id", ")", ")", "\n", "return", "state_batch", "\n", "\n"]], "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.worker.environment_manager.EnvironmentManager.prepare_episode": [[58, 80], ["environment_manager.EnvironmentManager.environment.reset", "environment_manager.EnvironmentManager._composite_batch.clear", "numpy.random.random"], "methods", ["home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.environment.environment.Environment.reset", "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.worker.batch.CompositeBatch.clear"], ["", "def", "prepare_episode", "(", "self", ",", "data_id", "=", "None", ")", ":", "# initialize a new episode", "\n", "\t\t", "self", ".", "terminal", "=", "False", "\n", "# Reset environment", "\n", "self", ".", "environment", ".", "reset", "(", "data_id", ")", "\n", "# Internal state", "\n", "self", ".", "_last_internal_state", "=", "None", "\n", "self", ".", "_batch", "=", "None", "\n", "# Episode batches", "\n", "self", ".", "_composite_batch", ".", "clear", "(", ")", "\n", "# Episode info", "\n", "self", ".", "__episode_step", "=", "[", "]", "\n", "self", ".", "__episode_info", "=", "{", "\n", "'tot_reward'", ":", "0", ",", "\n", "'tot_manipulated_reward'", ":", "0", ",", "\n", "'tot_value'", ":", "0", ",", "\n", "'tot_step'", ":", "0", "\n", "}", "\n", "# Frame info", "\n", "if", "flags", ".", "show_episodes", "==", "'none'", ":", "\n", "\t\t\t", "self", ".", "save_frame_info", "=", "False", "\n", "", "else", ":", "\n", "\t\t\t", "self", ".", "save_frame_info", "=", "flags", ".", "show_episodes", "!=", "'random'", "or", "np", ".", "random", ".", "random", "(", ")", "<=", "flags", ".", "show_episode_probability", "\n", "\n"]], "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.worker.environment_manager.EnvironmentManager.stop": [[81, 83], ["environment_manager.EnvironmentManager.environment.stop"], "methods", ["home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.worker.working_group.Group.stop"], ["", "", "def", "stop", "(", "self", ")", ":", "# stop current episode", "\n", "\t\t", "self", ".", "environment", ".", "stop", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.worker.environment_manager.EnvironmentManager.print_frames": [[84, 152], ["len", "os.mkdir", "os.mkdir", "range", "open", "range", "len", "utils.make_gif", "screen_file.write", "utils.ascii_image", "file_list.append", "utils.rgb_array_image", "file_list.append", "utils.heatmap", "file_list.append", "utils.combine_images", "screen_filenames.append", "len", "shutil.rmtree", "os.remove", "screen_filenames.append", "zipfile.ZipFile", "zip.write"], "methods", ["home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.utils.plots.make_gif", "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.utils.plots.ascii_image", "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.worker.batch.ExperienceBatch.append", "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.utils.plots.rgb_array_image", "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.worker.batch.ExperienceBatch.append", "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.utils.plots.heatmap", "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.worker.batch.ExperienceBatch.append", "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.utils.plots.combine_images", "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.worker.batch.ExperienceBatch.append", "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.worker.batch.ExperienceBatch.append"], ["", "def", "print_frames", "(", "self", ",", "frames", ",", "episode_directory", ")", ":", "\n", "\t\t", "print_frame", "=", "False", "\n", "if", "flags", ".", "show_episodes", "==", "'best'", ":", "\n", "\t\t\t", "if", "self", ".", "__episode_info", "[", "'tot_reward'", "]", ">", "self", ".", "__max_reward", ":", "\n", "\t\t\t\t", "self", ".", "__max_reward", "=", "self", ".", "__episode_info", "[", "'tot_reward'", "]", "\n", "print_frame", "=", "True", "\n", "", "", "elif", "self", ".", "save_frame_info", ":", "\n", "\t\t\t", "print_frame", "=", "True", "\n", "", "if", "not", "print_frame", ":", "\n", "\t\t\t", "return", "\n", "", "frames_count", "=", "len", "(", "frames", ")", "\n", "if", "frames_count", "<", "1", ":", "\n", "\t\t\t", "return", "\n", "# Make directory", "\n", "", "first_frame", "=", "frames", "[", "0", "]", "\n", "has_log", "=", "\"log\"", "in", "first_frame", "\n", "has_screen", "=", "\"screen\"", "in", "first_frame", "\n", "if", "not", "has_log", "and", "not", "has_screen", ":", "\n", "\t\t\t", "return", "\n", "", "os", ".", "mkdir", "(", "episode_directory", ")", "\n", "# Log", "\n", "if", "has_log", ":", "\n", "\t\t\t", "with", "open", "(", "episode_directory", "+", "'/episode.log'", ",", "\"w\"", ")", "as", "screen_file", ":", "\n", "\t\t\t\t", "for", "i", "in", "range", "(", "frames_count", ")", ":", "\n", "\t\t\t\t\t", "frame_info", "=", "frames", "[", "i", "]", "\n", "screen_file", ".", "write", "(", "frame_info", "[", "\"log\"", "]", ")", "\n", "# Screen", "\n", "", "", "", "if", "has_screen", ":", "\n", "\t\t\t", "screen_filenames", "=", "[", "]", "\n", "screens_directory", "=", "episode_directory", "+", "'/screens'", "\n", "os", ".", "mkdir", "(", "screens_directory", ")", "\n", "for", "i", "in", "range", "(", "frames_count", ")", ":", "\n", "\t\t\t\t", "filename", "=", "screens_directory", "+", "'/frame{}'", ".", "format", "(", "i", ")", "\n", "frame_info_screen", "=", "frames", "[", "i", "]", "[", "\"screen\"", "]", "\n", "file_list", "=", "[", "]", "\n", "if", "'ASCII'", "in", "frame_info_screen", ":", "\n", "\t\t\t\t\t", "ascii_filename", "=", "filename", "+", "'_ASCII.jpg'", "\n", "plt", ".", "ascii_image", "(", "frame_info_screen", "[", "'ASCII'", "]", ",", "ascii_filename", ")", "\n", "file_list", ".", "append", "(", "ascii_filename", ")", "\n", "", "if", "'RGB'", "in", "frame_info_screen", ":", "\n", "\t\t\t\t\t", "rgb_filename", "=", "filename", "+", "'_RGB.jpg'", "\n", "plt", ".", "rgb_array_image", "(", "frame_info_screen", "[", "'RGB'", "]", ",", "rgb_filename", ")", "\n", "file_list", ".", "append", "(", "rgb_filename", ")", "\n", "", "if", "'HeatMap'", "in", "frame_info_screen", ":", "\n", "\t\t\t\t\t", "hm_filename", "=", "filename", "+", "'_HM.jpg'", "\n", "plt", ".", "heatmap", "(", "heatmap", "=", "frame_info_screen", "[", "'HeatMap'", "]", ",", "figure_file", "=", "hm_filename", ")", "\n", "file_list", ".", "append", "(", "hm_filename", ")", "\n", "# save file", "\n", "", "file_list_len", "=", "len", "(", "file_list", ")", "\n", "if", "file_list_len", ">", "1", ":", "\n", "\t\t\t\t\t", "combined_filename", "=", "filename", "+", "'.jpg'", "\n", "plt", ".", "combine_images", "(", "images_list", "=", "file_list", ",", "file_name", "=", "combined_filename", ")", "\n", "screen_filenames", ".", "append", "(", "combined_filename", ")", "\n", "", "elif", "file_list_len", ">", "0", ":", "\n", "\t\t\t\t\t", "screen_filenames", ".", "append", "(", "file_list", "[", "0", "]", ")", "\n", "# Gif", "\n", "", "", "if", "flags", ".", "save_episode_gif", "and", "len", "(", "screen_filenames", ")", ">", "0", ":", "\n", "\t\t\t\t", "gif_filename", "=", "episode_directory", "+", "'/episode.gif'", "\n", "plt", ".", "make_gif", "(", "file_list", "=", "screen_filenames", ",", "gif_path", "=", "gif_filename", ")", "\n", "# Delete screens, to save memory", "\n", "if", "flags", ".", "delete_screens_after_making_gif", ":", "\n", "\t\t\t\t\t", "shutil", ".", "rmtree", "(", "screens_directory", ")", "\n", "# Zip GIF, to save memory", "\n", "", "if", "flags", ".", "compress_gif", ":", "\n", "\t\t\t\t\t", "with", "zipfile", ".", "ZipFile", "(", "gif_filename", "+", "'.zip'", ",", "mode", "=", "'w'", ",", "compression", "=", "zipfile", ".", "ZIP_DEFLATED", ")", "as", "zip", ":", "\n", "\t\t\t\t\t\t", "zip", ".", "write", "(", "gif_filename", ")", "\n", "# Remove unzipped GIF", "\n", "", "os", ".", "remove", "(", "gif_filename", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.worker.environment_manager.EnvironmentManager.get_statistics": [[153, 157], ["environment_manager.EnvironmentManager.__client_statistics.get", "environment_manager.EnvironmentManager.update", "environment_manager.EnvironmentManager.environment.get_statistics"], "methods", ["home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.worker.batch.CompositeBatch.get", "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.utils.running_std.RunningMeanStd.update", "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.worker.working_group.Group.get_statistics"], ["", "", "", "", "def", "get_statistics", "(", "self", ")", ":", "\n", "\t\t", "stats", "=", "self", ".", "__client_statistics", ".", "get", "(", ")", "\n", "stats", ".", "update", "(", "self", ".", "environment", ".", "get_statistics", "(", ")", ")", "\n", "return", "stats", "\n", "\n"]], "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.worker.environment_manager.EnvironmentManager.log_episode_statistics": [[158, 192], ["environment_manager.EnvironmentManager.__client_statistics.add", "environment_manager.EnvironmentManager.get_statistics", "environment_manager.EnvironmentManager.__reward_logger.info", "len", "episode_stats.update", "episode_stats.update", "str", "numpy.around", "numpy.around", "environment_manager.EnvironmentManager.print_frames", "environment_manager.EnvironmentManager.get_frame_info", "episode_stats.items"], "methods", ["home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.worker.batch.CompositeBatch.add", "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.worker.working_group.Group.get_statistics", "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.utils.running_std.RunningMeanStd.update", "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.utils.running_std.RunningMeanStd.update", "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.worker.environment_manager.EnvironmentManager.print_frames", "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.worker.environment_manager.EnvironmentManager.get_frame_info"], ["", "def", "log_episode_statistics", "(", "self", ",", "global_step", ")", ":", "\n", "# Get episode info", "\n", "\t\t", "tot_step", "=", "self", ".", "__episode_info", "[", "'tot_step'", "]", "\n", "tot_reward", "=", "self", ".", "__episode_info", "[", "'tot_reward'", "]", "\n", "tot_manipulated_reward", "=", "self", ".", "__episode_info", "[", "'tot_manipulated_reward'", "]", "\n", "tot_extrinsic_reward", ",", "tot_intrinsic_reward", "=", "tot_reward", "\n", "# Update statistics", "\n", "episode_stats", "=", "{", "\n", "'intrinsic_reward_per_step'", ":", "tot_intrinsic_reward", "/", "tot_step", ",", "\n", "'intrinsic_reward'", ":", "tot_intrinsic_reward", ",", "\n", "'extrinsic_reward_per_step'", ":", "tot_extrinsic_reward", "/", "tot_step", ",", "\n", "'extrinsic_reward'", ":", "tot_extrinsic_reward", ",", "\n", "'step'", ":", "tot_step", "\n", "}", "\n", "tot_value", "=", "self", ".", "__episode_info", "[", "'tot_value'", "]", "\n", "avg_value", "=", "tot_value", "/", "tot_step", "\n", "if", "len", "(", "avg_value", ")", ">", "1", ":", "\n", "\t\t\t", "episode_stats", ".", "update", "(", "{", "\n", "'extrinsic_value_per_step'", ":", "avg_value", "[", "0", "]", ",", "\n", "'intrinsic_value_per_step'", ":", "avg_value", "[", "1", "]", ",", "\n", "}", ")", "\n", "", "else", ":", "\n", "\t\t\t", "episode_stats", ".", "update", "(", "{", "'value_per_step'", ":", "avg_value", "[", "0", "]", "}", ")", "\n", "", "self", ".", "__client_statistics", ".", "add", "(", "episode_stats", ")", "\n", "self", ".", "stats", "=", "self", ".", "get_statistics", "(", ")", "\n", "# Print statistics", "\n", "self", ".", "__reward_logger", ".", "info", "(", "str", "(", "[", "\"{0}={1}\"", ".", "format", "(", "key", ",", "value", ")", "for", "key", ",", "value", "in", "episode_stats", ".", "items", "(", ")", "]", ")", ")", "\n", "# Print frames", "\n", "if", "self", ".", "save_frame_info", ":", "\n", "\t\t\t", "tot_reward", "=", "np", ".", "around", "(", "tot_reward", ",", "decimals", "=", "1", ")", "\n", "tot_manipulated_reward", "=", "np", ".", "around", "(", "tot_manipulated_reward", ",", "decimals", "=", "1", ")", "\n", "frames", "=", "[", "self", ".", "get_frame_info", "(", "step_info", ")", "for", "step_info", "in", "self", ".", "__episode_step", "]", "\n", "episode_directory", "=", "\"{}/episodes/reward({}-{})_value_({})_step({})_thread({})\"", ".", "format", "(", "flags", ".", "log_dir", ",", "tot_reward", ",", "tot_manipulated_reward", ",", "avg_value", ",", "global_step", ",", "self", ".", "environment_id", ")", "\n", "self", ".", "print_frames", "(", "frames", ",", "episode_directory", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.worker.environment_manager.EnvironmentManager.get_frame_info": [[193, 206], ["numpy.around", "numpy.around", "numpy.around", "utils.misc.softmax"], "methods", ["home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.utils.misc.softmax"], ["", "", "def", "get_frame_info", "(", "self", ",", "frame", ")", ":", "\n", "\t\t", "actor", "=", "frame", "[", "'policy'", "]", "\n", "distribution", "=", "[", "np", ".", "around", "(", "softmax", "(", "head", ")", ",", "decimals", "=", "3", ")", "for", "head", "in", "actor", "]", "\n", "logits", "=", "[", "np", ".", "around", "(", "head", ",", "decimals", "=", "3", ")", "for", "head", "in", "actor", "]", "\n", "value", "=", "np", ".", "around", "(", "frame", "[", "'value'", "]", ",", "decimals", "=", "3", ")", "\n", "value_info", "=", "\"reward={}, manipulated_reward={}, value={}\\n\"", ".", "format", "(", "frame", "[", "'reward'", "]", ",", "frame", "[", "'manipulated_reward'", "]", ",", "value", ")", "\n", "actor_info", "=", "\"logits={}, distribution={}\\n\"", ".", "format", "(", "logits", ",", "distribution", ")", "\n", "action_info", "=", "\"action={}\\n\"", ".", "format", "(", "frame", "[", "'action'", "]", ")", "\n", "extra_info", "=", "\"extra={}\\n\"", ".", "format", "(", "frame", "[", "'extra'", "]", ")", "\n", "frame_info", "=", "{", "\"log\"", ":", "value_info", "+", "actor_info", "+", "action_info", "+", "extra_info", "}", "\n", "if", "flags", ".", "save_episode_screen", "and", "frame", "[", "'screen'", "]", "is", "not", "None", ":", "\n", "\t\t\t", "frame_info", "[", "\"screen\"", "]", "=", "frame", "[", "'screen'", "]", "\n", "", "return", "frame_info", "\n", "\n"]], "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.worker.environment_manager.EnvironmentManager.initialize_new_batch": [[207, 209], ["agent.worker.batch.ExperienceBatch"], "methods", ["None"], ["", "def", "initialize_new_batch", "(", "self", ")", ":", "\n", "\t\t", "self", ".", "_batch", "=", "ExperienceBatch", "(", "self", ".", "model_size", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.worker.environment_manager.EnvironmentManager.get_internal_states": [[210, 212], ["None"], "methods", ["None"], ["", "def", "get_internal_states", "(", "self", ")", ":", "\n", "\t\t", "return", "self", ".", "_last_internal_state", "\n", "\n"]], "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.worker.environment_manager.EnvironmentManager.update_batch_state": [[213, 216], ["None"], "methods", ["None"], ["", "def", "update_batch_state", "(", "self", ",", "terminal", ",", "internal_state", ")", ":", "\n", "\t\t", "self", ".", "terminal", "=", "terminal", "\n", "self", ".", "_last_internal_state", "=", "internal_state", "\n", "\n"]], "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.worker.environment_manager.EnvironmentManager.apply_action_to_batch": [[217, 237], ["numpy.array", "numpy.array", "action_dict.update", "environment_manager.EnvironmentManager._batch.add_action", "environment_manager.EnvironmentManager.__episode_step.append", "environment_manager.EnvironmentManager.extrinsic_reward_manipulator", "environment_manager.EnvironmentManager.environment.get_screen", "environment_manager.EnvironmentManager.environment.get_info"], "methods", ["home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.utils.running_std.RunningMeanStd.update", "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.worker.batch.ExperienceBatch.add_action", "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.worker.batch.ExperienceBatch.append", "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.environment.environment.Environment.get_screen", "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.environment.environment.Environment.get_info"], ["", "def", "apply_action_to_batch", "(", "self", ",", "agent", ",", "action_dict", ",", "extrinsic_reward", ")", ":", "\n", "# Build total reward (intrinsic reward is computed later, more efficiently)", "\n", "\t\t", "reward", "=", "np", ".", "array", "(", "[", "extrinsic_reward", ",", "0.", "]", ")", "\n", "manipulated_reward", "=", "np", ".", "array", "(", "[", "self", ".", "extrinsic_reward_manipulator", "(", "extrinsic_reward", ")", ",", "0.", "]", ")", "\n", "# Add action to _batch", "\n", "action_dict", ".", "update", "(", "{", "\n", "'rewards'", ":", "reward", ",", "\n", "'manipulated_rewards'", ":", "manipulated_reward", ",", "\n", "}", ")", "\n", "self", ".", "_batch", ".", "add_action", "(", "agent_id", "=", "agent", ",", "feed_dict", "=", "action_dict", ")", "\n", "# Save frame info", "\n", "if", "self", ".", "save_frame_info", ":", "\n", "\t\t\t", "self", ".", "__episode_step", ".", "append", "(", "{", "\n", "'screen'", ":", "self", ".", "environment", ".", "get_screen", "(", ")", ",", "\n", "'extra'", ":", "self", ".", "environment", ".", "get_info", "(", ")", ",", "\n", "'action'", ":", "action_dict", "[", "'actions'", "]", ",", "\n", "'policy'", ":", "action_dict", "[", "'policies'", "]", ",", "\n", "'value'", ":", "action_dict", "[", "'values'", "]", ",", "\n", "'reward'", ":", "reward", ",", "\n", "'manipulated_reward'", ":", "manipulated_reward", ",", "\n", "}", ")", "\n"]], "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.worker.environment_manager.EnvironmentManager.finalize_batch": [[239, 245], ["environment_manager.EnvironmentManager._composite_batch.add"], "methods", ["home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.worker.batch.CompositeBatch.add"], ["", "", "def", "finalize_batch", "(", "self", ",", "global_step", ")", ":", "\n", "# Terminate _batch", "\n", "\t\t", "self", ".", "_batch", ".", "terminal", "=", "self", ".", "terminal", "\n", "# Add batch to episode list", "\n", "self", ".", "_composite_batch", ".", "add", "(", "self", ".", "_batch", ")", "\n", "return", "self", ".", "_composite_batch", "\n", "\n"]], "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.worker.environment_manager.EnvironmentManager.log_batch": [[246, 256], ["environment_manager.EnvironmentManager._batch.get_all_actions", "sum", "sum", "sum", "len", "environment_manager.EnvironmentManager.log_episode_statistics"], "methods", ["home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.worker.batch.ExperienceBatch.get_all_actions", "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.utils.segment_tree.SumSegmentTree.sum", "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.utils.segment_tree.SumSegmentTree.sum", "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.utils.segment_tree.SumSegmentTree.sum", "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.worker.environment_manager.EnvironmentManager.log_episode_statistics"], ["", "def", "log_batch", "(", "self", ",", "global_step", ",", "agents", ")", ":", "\n", "# Save _batch info for building statistics", "\n", "\t\t", "rewards", ",", "values", ",", "manipulated_rewards", "=", "self", ".", "_batch", ".", "get_all_actions", "(", "actions", "=", "[", "'rewards'", ",", "'values'", ",", "'manipulated_rewards'", "]", ",", "agents", "=", "agents", ")", "\n", "self", ".", "__episode_info", "[", "'tot_reward'", "]", "+=", "sum", "(", "rewards", ")", "\n", "self", ".", "__episode_info", "[", "'tot_manipulated_reward'", "]", "+=", "sum", "(", "manipulated_rewards", ")", "\n", "self", ".", "__episode_info", "[", "'tot_value'", "]", "+=", "sum", "(", "values", ")", "\n", "self", ".", "__episode_info", "[", "'tot_step'", "]", "+=", "len", "(", "rewards", ")", "\n", "# Terminate episode, if _batch is terminal", "\n", "if", "self", ".", "terminal", ":", "# an episode has terminated", "\n", "\t\t\t", "self", ".", "log_episode_statistics", "(", "global_step", ")", "\n", "", "", "", ""]], "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.worker.network_manager.NetworkManager.__init__": [[28, 58], ["network_manager.NetworkManager.set_model_size", "eval", "network_manager.NetworkManager.build_agents", "network_manager.NetworkManager.is_global_network", "network_manager.NetworkManager.prepare_loss", "eval", "int", "network_manager.NetworkManager.build_gradient_optimizer", "network_manager.NetworkManager.is_global_network", "network_manager.NetworkManager.bind_to_global", "utils.important_information.ImportantInformation", "print", "network_manager.NetworkManager.is_global_network", "network_manager.NetworkManager.minimize_local_loss", "utils.running_std.RunningMeanStd", "range"], "methods", ["home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.worker.network_manager.NetworkManager.set_model_size", "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.worker.network_manager.NetworkManager.build_agents", "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.worker.network_manager.NetworkManager.is_global_network", "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.worker.network_manager.NetworkManager.prepare_loss", "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.worker.network_manager.NetworkManager.build_gradient_optimizer", "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.worker.network_manager.NetworkManager.is_global_network", "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.worker.network_manager.NetworkManager.bind_to_global", "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.worker.network_manager.NetworkManager.is_global_network", "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.worker.network_manager.NetworkManager.minimize_local_loss"], ["", "def", "__init__", "(", "self", ",", "group_id", ",", "environment_info", ",", "global_network", "=", "None", ",", "training", "=", "True", ")", ":", "\n", "\t\t", "self", ".", "training", "=", "training", "\n", "self", ".", "group_id", "=", "group_id", "\n", "self", ".", "set_model_size", "(", ")", "\n", "self", ".", "global_network", "=", "global_network", "\n", "# Build agents", "\n", "self", ".", "algorithm", "=", "eval", "(", "'{}_Algorithm'", ".", "format", "(", "flags", ".", "algorithm", ")", ")", "\n", "self", ".", "model_list", "=", "self", ".", "build_agents", "(", "algorithm", "=", "self", ".", "algorithm", ",", "environment_info", "=", "environment_info", ")", "\n", "# Build global_step and gradient_optimizer", "\n", "if", "self", ".", "is_global_network", "(", ")", ":", "\n", "\t\t\t", "self", ".", "global_step", ",", "self", ".", "gradient_optimizer", "=", "self", ".", "build_gradient_optimizer", "(", ")", "\n", "", "else", ":", "\n", "\t\t\t", "self", ".", "global_step", "=", "self", ".", "global_network", ".", "global_step", "\n", "# Prepare loss", "\n", "", "self", ".", "prepare_loss", "(", "self", ".", "global_step", ")", "\n", "if", "self", ".", "training", ":", "\n", "\t\t\t", "if", "not", "self", ".", "is_global_network", "(", ")", ":", "\n", "\t\t\t\t", "self", ".", "minimize_local_loss", "(", "self", ".", "global_network", ")", "\n", "# Bind optimizer to global", "\n", "", "", "if", "not", "self", ".", "is_global_network", "(", ")", ":", "\n", "\t\t\t", "self", ".", "bind_to_global", "(", "self", ".", "global_network", ")", "\n", "# Intrinsic reward", "\n", "", "if", "flags", ".", "intrinsic_reward", "and", "flags", ".", "scale_intrinsic_reward", ":", "\n", "\t\t\t", "self", ".", "intrinsic_reward_scaler", "=", "[", "RunningMeanStd", "(", ")", "for", "_", "in", "range", "(", "self", ".", "model_size", ")", "]", "\n", "ImportantInformation", "(", "self", ".", "intrinsic_reward_scaler", ",", "'intrinsic_reward_scaler{}'", ".", "format", "(", "self", ".", "group_id", ")", ")", "\n", "# Reward manipulators", "\n", "", "self", ".", "intrinsic_reward_manipulator", "=", "eval", "(", "flags", ".", "intrinsic_reward_manipulator", ")", "\n", "self", ".", "intrinsic_reward_mini_batch_size", "=", "int", "(", "flags", ".", "batch_size", "*", "flags", ".", "intrinsic_rewards_mini_batch_fraction", ")", "\n", "if", "flags", ".", "intrinsic_reward", ":", "\n", "\t\t\t", "print", "(", "'[Group{}] Intrinsic rewards mini-batch size: {}'", ".", "format", "(", "self", ".", "group_id", ",", "self", ".", "intrinsic_reward_mini_batch_size", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.worker.network_manager.NetworkManager.get_statistics": [[59, 64], ["stats.update", "model.get_statistics"], "methods", ["home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.utils.running_std.RunningMeanStd.update", "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.worker.working_group.Group.get_statistics"], ["", "", "def", "get_statistics", "(", "self", ")", ":", "\n", "\t\t", "stats", "=", "{", "}", "\n", "for", "model", "in", "self", ".", "model_list", ":", "\n", "\t\t\t", "stats", ".", "update", "(", "model", ".", "get_statistics", "(", ")", ")", "\n", "", "return", "stats", "\n", "\n"]], "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.worker.network_manager.NetworkManager.is_global_network": [[65, 67], ["None"], "methods", ["None"], ["", "def", "is_global_network", "(", "self", ")", ":", "\n", "\t\t", "return", "self", ".", "global_network", "is", "None", "\n", "\n"]], "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.worker.network_manager.NetworkManager.set_model_size": [[68, 71], ["None"], "methods", ["None"], ["", "def", "set_model_size", "(", "self", ")", ":", "\n", "\t\t", "self", ".", "model_size", "=", "1", "\n", "self", ".", "agents_set", "=", "(", "0", ",", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.worker.network_manager.NetworkManager.build_agents": [[72, 82], ["algorithm", "model_list.append"], "methods", ["home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.worker.batch.ExperienceBatch.append"], ["", "def", "build_agents", "(", "self", ",", "algorithm", ",", "environment_info", ")", ":", "\n", "\t\t", "model_list", "=", "[", "]", "\n", "agent", "=", "algorithm", "(", "\n", "group_id", "=", "self", ".", "group_id", ",", "\n", "model_id", "=", "0", ",", "\n", "environment_info", "=", "environment_info", ",", "\n", "training", "=", "self", ".", "training", "\n", ")", "\n", "model_list", ".", "append", "(", "agent", ")", "\n", "return", "model_list", "\n", "\n"]], "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.worker.network_manager.NetworkManager.sync": [[83, 88], ["network_manager.NetworkManager.is_global_network", "model.sync"], "methods", ["home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.worker.network_manager.NetworkManager.is_global_network", "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.worker.network_manager.NetworkManager.sync"], ["", "def", "sync", "(", "self", ")", ":", "\n", "\t\t", "assert", "not", "self", ".", "is_global_network", "(", ")", ",", "'Trying to sync the global network with itself'", "\n", "# Synchronize models", "\n", "for", "model", "in", "self", ".", "model_list", ":", "\n", "\t\t\t", "model", ".", "sync", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.worker.network_manager.NetworkManager.build_gradient_optimizer": [[89, 95], ["range", "network_manager.NetworkManager.model_list[].build_optimizer"], "methods", ["home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.algorithm.ac_algorithm.AC_Algorithm.build_optimizer"], ["", "", "def", "build_gradient_optimizer", "(", "self", ")", ":", "\n", "\t\t", "global_step", "=", "[", "None", "]", "*", "self", ".", "model_size", "\n", "gradient_optimizer", "=", "[", "None", "]", "*", "self", ".", "model_size", "\n", "for", "i", "in", "range", "(", "self", ".", "model_size", ")", ":", "\n", "\t\t\t", "gradient_optimizer", "[", "i", "]", ",", "global_step", "[", "i", "]", "=", "self", ".", "model_list", "[", "i", "]", ".", "build_optimizer", "(", "flags", ".", "optimizer", ")", "\n", "", "return", "global_step", ",", "gradient_optimizer", "\n", "\n"]], "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.worker.network_manager.NetworkManager.minimize_local_loss": [[96, 99], ["enumerate", "zip", "local_agent.minimize_local_loss"], "methods", ["home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.worker.network_manager.NetworkManager.minimize_local_loss"], ["", "def", "minimize_local_loss", "(", "self", ",", "global_network", ")", ":", "\n", "\t\t", "for", "i", ",", "(", "local_agent", ",", "global_agent", ")", "in", "enumerate", "(", "zip", "(", "self", ".", "model_list", ",", "global_network", ".", "model_list", ")", ")", ":", "\n", "\t\t\t", "local_agent", ".", "minimize_local_loss", "(", "optimizer", "=", "global_network", ".", "gradient_optimizer", "[", "i", "]", ",", "global_step", "=", "global_network", ".", "global_step", "[", "i", "]", ",", "global_agent", "=", "global_agent", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.worker.network_manager.NetworkManager.prepare_loss": [[100, 103], ["enumerate", "model.prepare_loss"], "methods", ["home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.worker.network_manager.NetworkManager.prepare_loss"], ["", "", "def", "prepare_loss", "(", "self", ",", "global_step", ")", ":", "\n", "\t\t", "for", "i", ",", "model", "in", "enumerate", "(", "self", ".", "model_list", ")", ":", "\n", "\t\t\t", "model", ".", "prepare_loss", "(", "global_step", "[", "i", "]", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.worker.network_manager.NetworkManager.bind_to_global": [[104, 108], ["zip", "local_agent.bind_sync"], "methods", ["home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.algorithm.ac_algorithm.AC_Algorithm.bind_sync"], ["", "", "def", "bind_to_global", "(", "self", ",", "global_network", ")", ":", "\n", "# for synching local network with global one", "\n", "\t\t", "for", "local_agent", ",", "global_agent", "in", "zip", "(", "self", ".", "model_list", ",", "global_network", ".", "model_list", ")", ":", "\n", "\t\t\t", "local_agent", ".", "bind_sync", "(", "global_agent", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.worker.network_manager.NetworkManager.get_model": [[109, 111], ["None"], "methods", ["None"], ["", "", "def", "get_model", "(", "self", ",", "id", "=", "0", ")", ":", "\n", "\t\t", "return", "self", ".", "model_list", "[", "id", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.worker.network_manager.NetworkManager.predict_action": [[112, 121], ["network_manager.NetworkManager.get_model().predict_action", "len", "network_manager.NetworkManager.get_model", "range", "len"], "methods", ["home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.worker.network_manager.NetworkManager.predict_action", "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.worker.network_manager.NetworkManager.get_model"], ["", "def", "predict_action", "(", "self", ",", "states", ",", "internal_states", ")", ":", "\n", "\t\t", "action_dict", "=", "{", "\n", "'states'", ":", "states", ",", "\n", "'internal_states'", ":", "internal_states", ",", "\n", "'sizes'", ":", "[", "1", "for", "_", "in", "range", "(", "len", "(", "states", ")", ")", "]", "# states are from different environments with different internal states", "\n", "}", "\n", "actions", ",", "hot_actions", ",", "policies", ",", "values", ",", "new_internal_states", "=", "self", ".", "get_model", "(", ")", ".", "predict_action", "(", "action_dict", ")", "\n", "agents", "=", "[", "0", "]", "*", "len", "(", "actions", ")", "\n", "return", "actions", ",", "hot_actions", ",", "policies", ",", "values", ",", "new_internal_states", ",", "agents", "\n", "\n"]], "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.worker.network_manager.NetworkManager._play_critic": [[122, 146], ["range", "network_manager.NetworkManager._compute_intrinsic_rewards", "network_manager.NetworkManager._compute_discounted_cumulative_reward", "network_manager.NetworkManager.get_model().predict_value", "list", "network_manager.NetworkManager._bootstrap", "list", "len", "len", "len", "network_manager.NetworkManager.get_model"], "methods", ["home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.worker.network_manager.NetworkManager._compute_intrinsic_rewards", "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.worker.network_manager.NetworkManager._compute_discounted_cumulative_reward", "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.algorithm.ac_algorithm.AC_Algorithm.predict_value", "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.worker.network_manager.NetworkManager._bootstrap", "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.worker.network_manager.NetworkManager.get_model"], ["", "def", "_play_critic", "(", "self", ",", "batch", ",", "with_value", "=", "True", ",", "with_bootstrap", "=", "True", ",", "with_intrinsic_reward", "=", "True", ")", ":", "\n", "# Compute values and bootstrap", "\n", "\t\t", "if", "with_value", ":", "\n", "\t\t\t", "for", "agent_id", "in", "range", "(", "self", ".", "model_size", ")", ":", "\n", "\t\t\t\t", "value_dict", "=", "{", "\n", "'states'", ":", "batch", ".", "states", "[", "agent_id", "]", ",", "\n", "'actions'", ":", "batch", ".", "actions", "[", "agent_id", "]", ",", "\n", "'policies'", ":", "batch", ".", "policies", "[", "agent_id", "]", ",", "\n", "'internal_states'", ":", "[", "batch", ".", "internal_states", "[", "agent_id", "]", "[", "0", "]", "]", ",", "# a single internal state", "\n", "'bootstrap'", ":", "[", "{", "'state'", ":", "batch", ".", "new_states", "[", "agent_id", "]", "[", "-", "1", "]", "}", "]", ",", "\n", "'sizes'", ":", "[", "len", "(", "batch", ".", "states", "[", "agent_id", "]", ")", "]", "# playing critic on one single batch", "\n", "}", "\n", "value_batch", ",", "bootstrap_value", ",", "extra_batch", "=", "self", ".", "get_model", "(", "agent_id", ")", ".", "predict_value", "(", "value_dict", ")", "\n", "if", "extra_batch", "is", "not", "None", ":", "\n", "\t\t\t\t\t", "batch", ".", "extras", "[", "agent_id", "]", "=", "list", "(", "extra_batch", ")", "\n", "", "batch", ".", "values", "[", "agent_id", "]", "=", "list", "(", "value_batch", ")", "\n", "batch", ".", "bootstrap", "[", "agent_id", "]", "=", "bootstrap_value", "\n", "assert", "len", "(", "batch", ".", "states", "[", "agent_id", "]", ")", "==", "len", "(", "batch", ".", "values", "[", "agent_id", "]", ")", ",", "\"Number of values does not match the number of states\"", "\n", "", "", "elif", "with_bootstrap", ":", "\n", "\t\t\t", "self", ".", "_bootstrap", "(", "batch", ")", "\n", "", "if", "with_intrinsic_reward", ":", "\n", "\t\t\t", "self", ".", "_compute_intrinsic_rewards", "(", "batch", ")", "\n", "", "if", "with_value", "or", "with_intrinsic_reward", "or", "with_bootstrap", ":", "\n", "\t\t\t", "self", ".", "_compute_discounted_cumulative_reward", "(", "batch", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.worker.network_manager.NetworkManager._bootstrap": [[147, 154], ["range", "network_manager.NetworkManager.predict_action"], "methods", ["home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.worker.network_manager.NetworkManager.predict_action"], ["", "", "def", "_bootstrap", "(", "self", ",", "batch", ")", ":", "\n", "\t\t", "for", "agent_id", "in", "range", "(", "self", ".", "model_size", ")", ":", "\n", "\t\t\t", "_", ",", "_", ",", "_", ",", "(", "bootstrap_value", ",", ")", ",", "_", ",", "_", "=", "self", ".", "predict_action", "(", "\n", "states", "=", "batch", ".", "new_states", "[", "agent_id", "]", "[", "-", "1", ":", "]", ",", "\n", "internal_states", "=", "batch", ".", "new_internal_states", "[", "agent_id", "]", "[", "-", "1", ":", "]", "\n", ")", "\n", "batch", ".", "bootstrap", "[", "agent_id", "]", "=", "bootstrap_value", "\n", "\n"]], "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.worker.network_manager.NetworkManager._compute_intrinsic_rewards": [[155, 195], ["range", "network_manager.NetworkManager.get_model().predict_reward", "scaler.update", "range", "network_manager.NetworkManager.intrinsic_reward_manipulator", "range", "network_manager.NetworkManager.get_model", "len", "network_manager.NetworkManager.intrinsic_reward_manipulator", "len", "numpy.argmax"], "methods", ["home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.algorithm.ac_algorithm.AC_Algorithm.predict_reward", "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.utils.running_std.RunningMeanStd.update", "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.worker.network_manager.NetworkManager.get_model"], ["", "", "def", "_compute_intrinsic_rewards", "(", "self", ",", "batch", ")", ":", "\n", "\t\t", "for", "agent_id", "in", "range", "(", "self", ".", "model_size", ")", ":", "\n", "# Get actual rewards", "\n", "\t\t\t", "rewards", "=", "batch", ".", "rewards", "[", "agent_id", "]", "\n", "manipulated_rewards", "=", "batch", ".", "manipulated_rewards", "[", "agent_id", "]", "\n", "# Predict intrinsic rewards", "\n", "reward_dict", "=", "{", "\n", "'states'", ":", "batch", ".", "new_states", "[", "agent_id", "]", ",", "\n", "'state_mean'", ":", "self", ".", "state_mean", ",", "\n", "'state_std'", ":", "self", ".", "state_std", "\n", "}", "\n", "intrinsic_rewards", "=", "self", ".", "get_model", "(", "agent_id", ")", ".", "predict_reward", "(", "reward_dict", ")", "\n", "# Scale intrinsic rewards", "\n", "if", "flags", ".", "scale_intrinsic_reward", ":", "\n", "\t\t\t\t", "scaler", "=", "self", ".", "intrinsic_reward_scaler", "[", "agent_id", "]", "\n", "# Build intrinsic_reward scaler", "\n", "scaler", ".", "update", "(", "intrinsic_rewards", ")", "\n", "# If the reward scaler is initialized, we can compute the intrinsic reward", "\n", "if", "not", "scaler", ".", "initialized", ":", "\n", "\t\t\t\t\t", "continue", "\n", "# Add intrinsic rewards to batch", "\n", "", "", "if", "self", ".", "intrinsic_reward_mini_batch_size", ">", "1", ":", "\n", "# Keep only best intrinsic rewards", "\n", "\t\t\t\t", "for", "i", "in", "range", "(", "0", ",", "len", "(", "intrinsic_rewards", ")", ",", "self", ".", "intrinsic_reward_mini_batch_size", ")", ":", "\n", "\t\t\t\t\t", "best_intrinsic_reward_index", "=", "i", "+", "np", ".", "argmax", "(", "intrinsic_rewards", "[", "i", ":", "i", "+", "self", ".", "intrinsic_reward_mini_batch_size", "]", ")", "\n", "best_intrinsic_reward", "=", "intrinsic_rewards", "[", "best_intrinsic_reward_index", "]", "\n", "# print(i, best_intrinsic_reward_index, best_intrinsic_reward)", "\n", "if", "flags", ".", "scale_intrinsic_reward", ":", "\n", "\t\t\t\t\t\t", "best_intrinsic_reward", "=", "best_intrinsic_reward", "/", "scaler", ".", "std", "\n", "", "rewards", "[", "best_intrinsic_reward_index", "]", "[", "1", "]", "=", "best_intrinsic_reward", "\n", "manipulated_rewards", "[", "best_intrinsic_reward_index", "]", "[", "1", "]", "=", "self", ".", "intrinsic_reward_manipulator", "(", "best_intrinsic_reward", ")", "\n", "# print(best_intrinsic_reward_index,best_intrinsic_reward)", "\n", "", "", "else", ":", "\n", "# Keep all intrinsic rewards", "\n", "\t\t\t\t", "if", "flags", ".", "scale_intrinsic_reward", ":", "\n", "\t\t\t\t\t", "intrinsic_rewards", "=", "intrinsic_rewards", "/", "scaler", ".", "std", "\n", "", "manipulated_intrinsic_rewards", "=", "self", ".", "intrinsic_reward_manipulator", "(", "intrinsic_rewards", ")", "\n", "for", "i", "in", "range", "(", "len", "(", "intrinsic_rewards", ")", ")", ":", "\n", "\t\t\t\t\t", "rewards", "[", "i", "]", "[", "1", "]", "=", "intrinsic_rewards", "[", "i", "]", "\n", "manipulated_rewards", "[", "i", "]", "[", "1", "]", "=", "manipulated_intrinsic_rewards", "[", "i", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.worker.network_manager.NetworkManager._compute_discounted_cumulative_reward": [[196, 201], ["batch.compute_discounted_cumulative_reward"], "methods", ["home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.worker.batch.CompositeBatch.compute_discounted_cumulative_reward"], ["", "", "", "", "def", "_compute_discounted_cumulative_reward", "(", "self", ",", "batch", ")", ":", "\n", "\t\t", "batch", ".", "compute_discounted_cumulative_reward", "(", "\n", "agents", "=", "self", ".", "agents_set", ",", "\n", "gamma", "=", "flags", ".", "gamma", ",", "\n", "cumulative_return_builder", "=", "self", ".", "algorithm", ".", "get_reversed_cumulative_return", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.worker.network_manager.NetworkManager._train": [[203, 236], ["enumerate", "len", "model.prepare_train"], "methods", ["home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.algorithm.ac_algorithm.AC_Algorithm.prepare_train"], ["", "def", "_train", "(", "self", ",", "batch", ",", "replay", "=", "False", ",", "start", "=", "None", ",", "end", "=", "None", ")", ":", "\n", "\t\t", "assert", "self", ".", "global_network", "is", "not", "None", ",", "'Cannot directly _train the global network.'", "\n", "# Train every model", "\n", "for", "i", ",", "model", "in", "enumerate", "(", "self", ".", "model_list", ")", ":", "\n", "\t\t\t", "batch_size", "=", "len", "(", "batch", ".", "states", "[", "i", "]", ")", "\n", "# Ignore empty batches", "\n", "if", "batch_size", "==", "0", ":", "\n", "\t\t\t\t", "continue", "\n", "# Check whether to slice the batch", "\n", "", "is_valid_start", "=", "start", "is", "not", "None", "and", "start", "!=", "0", "and", "start", ">", "-", "batch_size", "\n", "is_valid_end", "=", "end", "is", "not", "None", "and", "end", "!=", "0", "and", "end", "<", "batch_size", "\n", "do_slice", "=", "is_valid_start", "or", "is_valid_end", "\n", "if", "do_slice", ":", "\n", "\t\t\t\t", "if", "not", "is_valid_start", ":", "\n", "\t\t\t\t\t", "start", "=", "None", "\n", "", "if", "not", "is_valid_end", ":", "\n", "\t\t\t\t\t", "end", "=", "None", "\n", "# Build _train dictionary", "\n", "", "", "train_dict", "=", "{", "\n", "'states'", ":", "batch", ".", "states", "[", "i", "]", "[", "start", ":", "end", "]", "if", "do_slice", "else", "batch", ".", "states", "[", "i", "]", ",", "\n", "'actions'", ":", "batch", ".", "actions", "[", "i", "]", "[", "start", ":", "end", "]", "if", "do_slice", "else", "batch", ".", "actions", "[", "i", "]", ",", "\n", "'action_masks'", ":", "batch", ".", "action_masks", "[", "i", "]", "[", "start", ":", "end", "]", "if", "do_slice", "else", "batch", ".", "action_masks", "[", "i", "]", ",", "\n", "'values'", ":", "batch", ".", "values", "[", "i", "]", "[", "start", ":", "end", "]", "if", "do_slice", "else", "batch", ".", "values", "[", "i", "]", ",", "\n", "'policies'", ":", "batch", ".", "policies", "[", "i", "]", "[", "start", ":", "end", "]", "if", "do_slice", "else", "batch", ".", "policies", "[", "i", "]", ",", "\n", "'cumulative_returns'", ":", "batch", ".", "cumulative_returns", "[", "i", "]", "[", "start", ":", "end", "]", "if", "do_slice", "else", "batch", ".", "cumulative_returns", "[", "i", "]", ",", "\n", "'internal_state'", ":", "batch", ".", "internal_states", "[", "i", "]", "[", "start", "]", "if", "is_valid_start", "else", "batch", ".", "internal_states", "[", "i", "]", "[", "0", "]", ",", "\n", "'state_mean'", ":", "self", ".", "state_mean", ",", "\n", "'state_std'", ":", "self", ".", "state_std", ",", "\n", "}", "\n", "if", "not", "flags", ".", "runtime_advantage", ":", "\n", "\t\t\t\t", "train_dict", "[", "'advantages'", "]", "=", "batch", ".", "advantages", "[", "i", "]", "[", "start", ":", "end", "]", "if", "do_slice", "else", "batch", ".", "advantages", "[", "i", "]", "\n", "# Prepare _train", "\n", "", "train_result", "=", "model", ".", "prepare_train", "(", "train_dict", "=", "train_dict", ",", "replay", "=", "replay", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.worker.network_manager.NetworkManager._add_to_replay_buffer": [[237, 258], ["batch.is_empty", "batch.get_cumulative_reward", "network_manager.NetworkManager.experience_buffer.put", "network_manager.NetworkManager.experience_buffer.put"], "methods", ["home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.worker.batch.ExperienceBatch.is_empty", "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.worker.batch.ExperienceBatch.get_cumulative_reward", "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.buffer.pseudo_prioritized_buffer.PseudoPrioritizedBuffer.put", "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.buffer.pseudo_prioritized_buffer.PseudoPrioritizedBuffer.put"], ["", "", "def", "_add_to_replay_buffer", "(", "self", ",", "batch", ",", "is_best", ")", ":", "\n", "# Check whether batch is empty", "\n", "\t\t", "if", "batch", ".", "is_empty", "(", "self", ".", "agents_set", ")", ":", "\n", "\t\t\t", "return", "False", "\n", "# Build batch type", "\n", "", "batch_extrinsic_reward", ",", "batch_intrinsic_reward", "=", "batch", ".", "get_cumulative_reward", "(", "self", ".", "agents_set", ")", "\n", "#=======================================================================", "\n", "# if batch_extrinsic_reward > 0:", "\n", "# \tprint(\"Adding new batch with reward: extrinsic {}, intrinsic {}\".format(batch_extrinsic_reward, batch_intrinsic_reward))", "\n", "#=======================================================================", "\n", "type_id", "=", "'1'", "if", "batch_extrinsic_reward", ">", "0", "else", "'0'", "\n", "type_id", "+=", "'1'", "if", "is_best", "else", "'0'", "\n", "# Populate buffer", "\n", "if", "flags", ".", "prioritized_replay", ":", "\n", "\t\t\t", "priority", "=", "batch_intrinsic_reward", "if", "flags", ".", "intrinsic_reward", "else", "batch_extrinsic_reward", "\n", "with", "self", ".", "experience_buffer_lock", ":", "\n", "\t\t\t\t", "self", ".", "experience_buffer", ".", "put", "(", "batch", "=", "batch", ",", "priority", "=", "priority", ",", "type_id", "=", "type_id", ")", "\n", "", "", "else", ":", "\n", "\t\t\t", "with", "self", ".", "experience_buffer_lock", ":", "\n", "\t\t\t\t", "self", ".", "experience_buffer", ".", "put", "(", "batch", "=", "batch", ",", "type_id", "=", "type_id", ")", "\n", "", "", "return", "True", "\n", "\n"]], "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.worker.network_manager.NetworkManager.try_to_replay_experience": [[259, 290], ["numpy.random.poisson", "range", "network_manager.NetworkManager.experience_buffer.has_atleast", "network_manager.NetworkManager._play_critic", "network_manager.NetworkManager._train", "batch_to_update.append", "batch.get_cumulative_reward", "network_manager.NetworkManager.experience_buffer.keyed_sample", "network_manager.NetworkManager.experience_buffer.sample", "network_manager.NetworkManager.experience_buffer.update_priority"], "methods", ["home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.buffer.buffer.Buffer.has_atleast", "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.worker.network_manager.NetworkManager._play_critic", "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.worker.network_manager.NetworkManager._train", "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.worker.batch.ExperienceBatch.append", "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.worker.batch.ExperienceBatch.get_cumulative_reward", "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.buffer.pseudo_prioritized_buffer.PseudoPrioritizedBuffer.keyed_sample", "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.buffer.pseudo_prioritized_buffer.PseudoPrioritizedBuffer.sample", "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.buffer.pseudo_prioritized_buffer.PseudoPrioritizedBuffer.update_priority"], ["", "def", "try_to_replay_experience", "(", "self", ")", ":", "\n", "\t\t", "if", "flags", ".", "replay_mean", "<=", "0", ":", "\n", "\t\t\t", "return", "\n", "# Check whether experience buffer has enough elements for replaying", "\n", "", "if", "not", "self", ".", "experience_buffer", ".", "has_atleast", "(", "flags", ".", "replay_start", ")", ":", "\n", "\t\t\t", "return", "\n", "", "prioritized_replay_with_update", "=", "flags", ".", "prioritized_replay", "and", "flags", ".", "intrinsic_reward", "\n", "if", "prioritized_replay_with_update", ":", "\n", "\t\t\t", "batch_to_update", "=", "[", "]", "\n", "# Sample n batches from experience buffer", "\n", "", "n", "=", "np", ".", "random", ".", "poisson", "(", "flags", ".", "replay_mean", ")", "\n", "for", "_", "in", "range", "(", "n", ")", ":", "\n", "# Sample batch", "\n", "\t\t\t", "if", "prioritized_replay_with_update", ":", "\n", "\t\t\t\t", "with", "self", ".", "experience_buffer_lock", ":", "\n", "\t\t\t\t\t", "keyed_sample", "=", "self", ".", "experience_buffer", ".", "keyed_sample", "(", ")", "\n", "", "batch_to_update", ".", "append", "(", "keyed_sample", ")", "\n", "old_batch", ",", "_", ",", "_", "=", "keyed_sample", "\n", "", "else", ":", "\n", "\t\t\t\t", "with", "self", ".", "experience_buffer_lock", ":", "\n", "\t\t\t\t\t", "old_batch", "=", "self", ".", "experience_buffer", ".", "sample", "(", ")", "\n", "# Replay value, without keeping experience_buffer_lock the buffer update might be not consistent anymore", "\n", "", "", "self", ".", "_play_critic", "(", "batch", "=", "old_batch", ",", "with_value", "=", "flags", ".", "recompute_value_when_replaying", ",", "with_bootstrap", "=", "False", ",", "with_intrinsic_reward", "=", "flags", ".", "intrinsic_reward", ")", "\n", "# Train", "\n", "self", ".", "_train", "(", "replay", "=", "True", ",", "batch", "=", "old_batch", ")", "\n", "# Update buffer", "\n", "", "if", "prioritized_replay_with_update", ":", "\n", "\t\t\t", "for", "batch", ",", "id", ",", "type", "in", "batch_to_update", ":", "\n", "\t\t\t\t", "_", ",", "batch_intrinsic_reward", "=", "batch", ".", "get_cumulative_reward", "(", "self", ".", "agents_set", ")", "\n", "with", "self", ".", "experience_buffer_lock", ":", "\n", "\t\t\t\t\t", "self", ".", "experience_buffer", ".", "update_priority", "(", "id", ",", "batch_intrinsic_reward", ",", "type", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.worker.network_manager.NetworkManager.finalize_batch": [[291, 316], ["network_manager.NetworkManager._play_critic", "network_manager.NetworkManager._train", "composite_batch.get", "batch.get_cumulative_reward", "composite_batch.get", "composite_batch.clear", "network_manager.NetworkManager._add_to_replay_buffer"], "methods", ["home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.worker.network_manager.NetworkManager._play_critic", "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.worker.network_manager.NetworkManager._train", "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.worker.batch.CompositeBatch.get", "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.worker.batch.ExperienceBatch.get_cumulative_reward", "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.worker.batch.CompositeBatch.get", "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.worker.batch.CompositeBatch.clear", "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.worker.network_manager.NetworkManager._add_to_replay_buffer"], ["", "", "", "", "def", "finalize_batch", "(", "self", ",", "composite_batch", ",", "global_step", ")", ":", "\n", "\t\t", "batch", "=", "composite_batch", ".", "get", "(", ")", "[", "-", "1", "]", "\n", "# Decide whether to compute intrinsic reward", "\n", "with_intrinsic_reward", "=", "flags", ".", "intrinsic_reward", "and", "global_step", ">", "flags", ".", "intrinsic_reward_step", "\n", "self", ".", "_play_critic", "(", "batch", ",", "with_value", "=", "False", ",", "with_bootstrap", "=", "True", ",", "with_intrinsic_reward", "=", "with_intrinsic_reward", ")", "\n", "# Train", "\n", "self", ".", "_train", "(", "replay", "=", "False", ",", "batch", "=", "batch", ")", "\n", "# Populate replay buffer", "\n", "if", "flags", ".", "replay_mean", ">", "0", ":", "\n", "# Check whether to save the whole episode list into the replay buffer", "\n", "\t\t\t", "extrinsic_reward", ",", "_", "=", "batch", ".", "get_cumulative_reward", "(", "self", ".", "agents_set", ")", "\n", "is_best", "=", "extrinsic_reward", ">", "0", "# Best batches = batches that lead to positive extrinsic reward", "\n", "# Build the best known cumulative return", "\n", "#===================================================================", "\n", "# if is_best and not flags.recompute_value_when_replaying:", "\n", "# \tif composite_batch.size() > 1: # No need to recompute the cumulative return if composite batch has only 1 batch", "\n", "# \t\tself._compute_discounted_cumulative_reward(composite_batch)", "\n", "#===================================================================", "\n", "# Add to experience buffer if is good batch or batch has terminated", "\n", "add_composite_batch_to_buffer", "=", "is_best", "or", "(", "not", "flags", ".", "replay_only_best_batches", "and", "batch", ".", "terminal", ")", "\n", "if", "add_composite_batch_to_buffer", ":", "\n", "\t\t\t\t", "for", "old_batch", "in", "composite_batch", ".", "get", "(", ")", ":", "\n", "\t\t\t\t\t", "self", ".", "_add_to_replay_buffer", "(", "batch", "=", "old_batch", ",", "is_best", "=", "is_best", ")", "\n", "# Clear composite batch", "\n", "", "composite_batch", ".", "clear", "(", ")", "\n", "", "", "", "", ""]], "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.worker.working_group.Group.__init__": [[13, 51], ["environment.environment.Environment.create_environment", "agent.worker.network_manager.NetworkManager", "utils.important_information.ImportantInformation", "utils.statistics.IndexedStatistics", "environment.environment.Environment.create_environment.get_state_shape", "environment.environment.Environment.create_environment.get_action_shape", "environment.environment.Environment.create_environment.has_masked_actions", "agent.worker.environment_manager.EnvironmentManager", "utils.running_std.RunningMeanStd", "range"], "methods", ["home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.environment.environment.Environment.create_environment", "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.environment.environment.Environment.get_state_shape", "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.environment.gym_environment.GymEnvironment.get_action_shape", "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.environment.environment.Environment.has_masked_actions"], ["\t", "def", "__init__", "(", "self", ",", "group_id", ",", "environment_count", ",", "global_network", ",", "training", "=", "True", ")", ":", "\n", "\t\t", "self", ".", "group_id", "=", "group_id", "\n", "self", ".", "training", "=", "training", "\n", "# Get environment info", "\n", "tmp_environment", "=", "Environment", ".", "create_environment", "(", "env_type", "=", "flags", ".", "env_type", ",", "training", "=", "training", ")", "\n", "self", ".", "environment_info", "=", "{", "\n", "'state_shape'", ":", "tmp_environment", ".", "get_state_shape", "(", ")", ",", "\n", "'action_shape'", ":", "tmp_environment", ".", "get_action_shape", "(", ")", ",", "\n", "'state_scaler'", ":", "tmp_environment", ".", "state_scaler", ",", "\n", "'has_masked_actions'", ":", "tmp_environment", ".", "has_masked_actions", "(", ")", ",", "\n", "}", "\n", "# Build network_manager", "\n", "self", ".", "network_manager", "=", "NetworkManager", "(", "\n", "group_id", "=", "self", ".", "group_id", ",", "\n", "environment_info", "=", "self", ".", "environment_info", ",", "\n", "global_network", "=", "global_network", ",", "\n", "training", "=", "self", ".", "training", "\n", ")", "\n", "# Build environments", "\n", "self", ".", "environment_count", "=", "environment_count", "\n", "self", ".", "worker_list", "=", "[", "\n", "EnvironmentManager", "(", "\n", "model_size", "=", "self", ".", "network_manager", ".", "model_size", ",", "\n", "environment_id", "=", "env_id", ",", "\n", "group_id", "=", "group_id", ",", "\n", "training", "=", "training", "\n", ")", "\n", "for", "env_id", "in", "range", "(", "self", ".", "environment_count", ")", "\n", "]", "\n", "# State distribution estimator", "\n", "self", ".", "state_distribution_estimator", "=", "[", "RunningMeanStd", "(", "batch_size", "=", "flags", ".", "batch_size", ",", "shape", "=", "shape", ")", "for", "shape", "in", "self", ".", "environment_info", "[", "'state_shape'", "]", "]", "\n", "self", ".", "network_manager", ".", "state_mean", "=", "[", "estimator", ".", "mean", "for", "estimator", "in", "self", ".", "state_distribution_estimator", "]", "\n", "self", ".", "network_manager", ".", "state_std", "=", "[", "estimator", ".", "std", "for", "estimator", "in", "self", ".", "state_distribution_estimator", "]", "\n", "ImportantInformation", "(", "self", ".", "state_distribution_estimator", ",", "'state_distribution_estimator{}'", ".", "format", "(", "self", ".", "group_id", ")", ")", "\n", "# Statistics", "\n", "self", ".", "group_statistics", "=", "IndexedStatistics", "(", "max_count", "=", "self", ".", "environment_count", ",", "buffer_must_be_full", "=", "True", ")", "\n", "self", ".", "has_terminal_worker", "=", "False", "\n", "self", ".", "terminated_episodes", "=", "0", "\n", "\n"]], "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.worker.working_group.Group.initialize_environments": [[52, 61], ["print", "len", "worker.run_random_steps", "enumerate", "zip", "working_group.Group.state_distribution_estimator[].update"], "methods", ["home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.worker.environment_manager.EnvironmentManager.run_random_steps", "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.utils.running_std.RunningMeanStd.update"], ["", "def", "initialize_environments", "(", "self", ",", "step_count", "=", "0", ")", ":", "\n", "\t\t", "step_per_worker", "=", "step_count", "//", "len", "(", "self", ".", "worker_list", ")", "\n", "for", "worker", "in", "self", ".", "worker_list", ":", "\n", "\t\t\t", "states", "=", "worker", ".", "run_random_steps", "(", "step_per_worker", ")", "\n", "for", "i", ",", "sub_state", "in", "enumerate", "(", "zip", "(", "*", "states", ")", ")", ":", "\n", "\t\t\t\t", "self", ".", "state_distribution_estimator", "[", "i", "]", ".", "update", "(", "sub_state", ")", "\n", "", "", "self", ".", "network_manager", ".", "state_mean", "=", "[", "estimator", ".", "mean", "for", "estimator", "in", "self", ".", "state_distribution_estimator", "]", "\n", "self", ".", "network_manager", ".", "state_std", "=", "[", "estimator", ".", "std", "for", "estimator", "in", "self", ".", "state_distribution_estimator", "]", "\n", "print", "(", "\"Environment group {} initialized\"", ".", "format", "(", "self", ".", "group_id", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.worker.working_group.Group.stop": [[62, 65], ["worker.stop"], "methods", ["home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.worker.working_group.Group.stop"], ["", "def", "stop", "(", "self", ")", ":", "# stop current episode", "\n", "\t\t", "for", "worker", "in", "self", ".", "worker_list", ":", "\n", "\t\t\t", "worker", ".", "stop", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.worker.working_group.Group.get_statistics": [[66, 70], ["working_group.Group.network_manager.get_statistics", "working_group.Group.update", "working_group.Group.group_statistics.get"], "methods", ["home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.worker.working_group.Group.get_statistics", "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.utils.running_std.RunningMeanStd.update", "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.worker.batch.CompositeBatch.get"], ["", "", "def", "get_statistics", "(", "self", ")", ":", "\n", "\t\t", "stats", "=", "self", ".", "network_manager", ".", "get_statistics", "(", ")", "\n", "stats", ".", "update", "(", "self", ".", "group_statistics", ".", "get", "(", ")", ")", "\n", "return", "stats", "\n", "\n"]], "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.worker.working_group.Group._process_step": [[71, 110], ["tuple", "working_group.Group.network_manager.predict_action", "zip", "range", "working_group.Group.network_manager.sync", "worker.get_internal_states", "len", "workers[].update_batch_state", "enumerate", "range", "zip", "estimator.update", "len", "workers[].apply_action_to_batch", "worker.environment.process", "zip"], "methods", ["home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.worker.network_manager.NetworkManager.predict_action", "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.worker.network_manager.NetworkManager.sync", "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.worker.environment_manager.EnvironmentManager.get_internal_states", "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.worker.environment_manager.EnvironmentManager.update_batch_state", "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.utils.running_std.RunningMeanStd.update", "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.worker.environment_manager.EnvironmentManager.apply_action_to_batch", "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.worker.working_group.Group.process"], ["", "def", "_process_step", "(", "self", ",", "workers", ")", ":", "\n", "\t\t", "if", "self", ".", "training", ":", "\n", "\t\t\t", "self", ".", "network_manager", ".", "sync", "(", ")", "\n", "", "internal_states", "=", "[", "worker", ".", "get_internal_states", "(", ")", "for", "worker", "in", "workers", "]", "\n", "states", "=", "tuple", "(", "worker", ".", "environment", ".", "last_state", "for", "worker", "in", "workers", ")", "\n", "actions", ",", "hot_actions", ",", "policies", ",", "values", ",", "new_internal_states", ",", "agents", "=", "self", ".", "network_manager", ".", "predict_action", "(", "states", ",", "internal_states", ")", "\n", "new_states", ",", "extrinsic_rewards", ",", "terminals", ",", "action_masks", "=", "zip", "(", "*", "[", "worker", ".", "environment", ".", "process", "(", "action", ")", "for", "worker", ",", "action", "in", "zip", "(", "workers", ",", "actions", ")", "]", ")", "\n", "# Update batch state", "\n", "for", "i", "in", "range", "(", "len", "(", "workers", ")", ")", ":", "\n", "\t\t\t", "workers", "[", "i", "]", ".", "update_batch_state", "(", "\n", "terminal", "=", "terminals", "[", "i", "]", ",", "\n", "internal_state", "=", "new_internal_states", "[", "i", "]", "\n", ")", "\n", "# Build batch", "\n", "", "if", "self", ".", "training", ":", "\n", "# Update state distribution estimator", "\n", "\t\t\t", "for", "i", ",", "sub_state", "in", "enumerate", "(", "zip", "(", "*", "new_states", ")", ")", ":", "\n", "\t\t\t\t", "estimator", "=", "self", ".", "state_distribution_estimator", "[", "i", "]", "\n", "if", "estimator", ".", "update", "(", "sub_state", ")", ":", "\n", "\t\t\t\t\t", "self", ".", "network_manager", ".", "state_mean", "[", "i", "]", "=", "estimator", ".", "mean", "\n", "self", ".", "network_manager", ".", "state_std", "[", "i", "]", "=", "estimator", ".", "std", "\n", "# Apply actions to batches ", "\n", "", "", "for", "i", "in", "range", "(", "len", "(", "workers", ")", ")", ":", "\n", "# Build action dictionary", "\n", "\t\t\t\t", "action_dict", "=", "{", "\n", "'states'", ":", "states", "[", "i", "]", ",", "\n", "'new_states'", ":", "new_states", "[", "i", "]", ",", "\n", "'actions'", ":", "hot_actions", "[", "i", "]", ",", "\n", "'action_masks'", ":", "action_masks", "[", "i", "]", ",", "\n", "'values'", ":", "values", "[", "i", "]", ",", "\n", "'policies'", ":", "policies", "[", "i", "]", ",", "\n", "'internal_states'", ":", "internal_states", "[", "i", "]", ",", "\n", "'new_internal_states'", ":", "new_internal_states", "[", "i", "]", ",", "\n", "}", "\n", "# Apply action", "\n", "workers", "[", "i", "]", ".", "apply_action_to_batch", "(", "\n", "agent", "=", "agents", "[", "i", "]", ",", "\n", "action_dict", "=", "action_dict", ",", "\n", "extrinsic_reward", "=", "extrinsic_rewards", "[", "i", "]", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.worker.working_group.Group.process": [[112, 151], ["enumerate", "enumerate", "worker.initialize_new_batch", "working_group.Group.network_manager.sync", "len", "working_group.Group._process_step", "len", "len", "worker.prepare_episode", "len", "working_group.Group.network_manager.finalize_batch", "worker.log_batch", "working_group.Group.group_statistics.set", "working_group.Group.network_manager.try_to_replay_experience", "worker.get_statistics", "worker.finalize_batch"], "methods", ["home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.worker.environment_manager.EnvironmentManager.initialize_new_batch", "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.worker.network_manager.NetworkManager.sync", "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.worker.working_group.Group._process_step", "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.worker.environment_manager.EnvironmentManager.prepare_episode", "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.worker.network_manager.NetworkManager.finalize_batch", "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.worker.environment_manager.EnvironmentManager.log_batch", "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.buffer.pseudo_prioritized_buffer.PseudoPrioritizedBuffer.set", "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.worker.network_manager.NetworkManager.try_to_replay_experience", "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.worker.working_group.Group.get_statistics", "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.worker.network_manager.NetworkManager.finalize_batch"], ["", "", "", "def", "process", "(", "self", ",", "global_step", "=", "0", ",", "batch", "=", "True", ",", "data_id", "=", "0", ")", ":", "\n", "# Prepare episode", "\n", "\t\t", "for", "i", ",", "worker", "in", "enumerate", "(", "self", ".", "worker_list", ")", ":", "\n", "\t\t\t", "if", "worker", ".", "terminal", ":", "\n", "\t\t\t\t", "worker", ".", "prepare_episode", "(", "data_id", "+", "i", ")", "\n", "# Initialize new batch", "\n", "", "", "for", "worker", "in", "self", ".", "worker_list", ":", "\n", "\t\t\t", "worker", ".", "initialize_new_batch", "(", ")", "\n", "# Sync to global, if not training", "\n", "", "if", "not", "self", ".", "training", ":", "\n", "\t\t\t", "self", ".", "network_manager", ".", "sync", "(", ")", "\n", "# Build batch", "\n", "", "group_step", "=", "0", "\n", "step", "=", "0", "\n", "valid_workers", "=", "self", ".", "worker_list", "\n", "while", "step", "<", "flags", ".", "batch_size", "and", "len", "(", "valid_workers", ")", ">", "0", ":", "\n", "\t\t\t", "group_step", "+=", "len", "(", "valid_workers", ")", "\n", "self", ".", "_process_step", "(", "valid_workers", ")", "\n", "valid_workers", "=", "[", "worker", "for", "worker", "in", "self", ".", "worker_list", "if", "not", "worker", ".", "terminal", "]", "\n", "if", "batch", ":", "\n", "\t\t\t\t", "step", "+=", "1", "\n", "", "", "terminated_works", "=", "len", "(", "self", ".", "worker_list", ")", "-", "len", "(", "valid_workers", ")", "\n", "# Terminate batch & replay experience", "\n", "if", "self", ".", "training", ":", "\n", "\t\t\t", "for", "worker", "in", "self", ".", "worker_list", ":", "\n", "# Replay experience, before finalizing the new batch", "\n", "\t\t\t\t", "if", "global_step", ">", "flags", ".", "replay_step", ":", "\n", "\t\t\t\t\t", "self", ".", "network_manager", ".", "try_to_replay_experience", "(", ")", "\n", "# Finalize the new batch and train on it", "\n", "", "self", ".", "network_manager", ".", "finalize_batch", "(", "composite_batch", "=", "worker", ".", "finalize_batch", "(", "global_step", ")", ",", "global_step", "=", "global_step", ")", "\n", "# Log the new batch", "\n", "worker", ".", "log_batch", "(", "global_step", ",", "self", ".", "network_manager", ".", "agents_set", ")", "\n", "# Statistics", "\n", "", "", "self", ".", "terminated_episodes", "+=", "terminated_works", "\n", "self", ".", "has_terminal_worker", "=", "terminated_works", "!=", "0", "\n", "for", "i", ",", "worker", "in", "enumerate", "(", "self", ".", "worker_list", ")", ":", "\n", "\t\t\t", "if", "worker", ".", "terminal", ":", "\n", "\t\t\t\t", "self", ".", "group_statistics", ".", "set", "(", "worker", ".", "get_statistics", "(", ")", ",", "i", ")", "\n", "", "", "return", "group_step", "\n", "", "", ""]], "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.worker.batch.ExperienceBatch.__init__": [[21, 36], ["tuple", "batch.ExperienceBatch._add_batch_key_if_not_exist", "range"], "methods", ["home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.worker.batch.ExperienceBatch._add_batch_key_if_not_exist"], ["def", "__init__", "(", "self", ",", "model_size", ")", ":", "\n", "\t\t", "self", ".", "_model_size", "=", "model_size", "\n", "self", ".", "_agent_list", "=", "tuple", "(", "range", "(", "self", ".", "_model_size", ")", ")", "\n", "self", ".", "_batch_keys", "=", "[", "]", "\n", "# Add batch keys, after setting model size", "\n", "self", ".", "_add_batch_key_if_not_exist", "(", "[", "\n", "'states'", ",", "'new_states'", ",", "\n", "'internal_states'", ",", "'new_internal_states'", ",", "\n", "'actions'", ",", "'action_masks'", ",", "'policies'", ",", "'values'", ",", "\n", "'rewards'", ",", "'manipulated_rewards'", ",", "\n", "'cumulative_returns'", ",", "'advantages'", ",", "'extras'", "\n", "]", ")", "\n", "self", ".", "_agent_position_list", "=", "[", "]", "\n", "self", ".", "bootstrap", "=", "{", "}", "\n", "self", ".", "terminal", "=", "False", "\n", "\n"]], "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.worker.batch.ExperienceBatch._add_batch_key_if_not_exist": [[37, 45], ["utils.misc.is_tuple", "setattr", "batch.ExperienceBatch._batch_keys.append", "range"], "methods", ["home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.utils.misc.is_tuple", "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.worker.batch.ExperienceBatch.append"], ["", "def", "_add_batch_key_if_not_exist", "(", "self", ",", "key_list", ")", ":", "\n", "\t\t", "if", "not", "is_tuple", "(", "key_list", ")", ":", "\n", "\t\t\t", "key_list", "=", "[", "key_list", "]", "\n", "", "for", "key", "in", "key_list", ":", "\n", "\t\t\t", "if", "key", "in", "self", ".", "_batch_keys", ":", "\n", "\t\t\t\t", "continue", "\n", "", "setattr", "(", "self", ",", "key", ",", "[", "[", "]", "for", "_", "in", "range", "(", "self", ".", "_model_size", ")", "]", ")", "# do NOT use [[]]*_model_size  ", "\n", "self", ".", "_batch_keys", ".", "append", "(", "key", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.worker.batch.ExperienceBatch.is_empty": [[46, 53], ["len"], "methods", ["None"], ["", "", "def", "is_empty", "(", "self", ",", "agents", "=", "None", ")", ":", "\n", "\t\t", "if", "agents", "is", "None", ":", "\n", "\t\t\t", "return", "not", "self", ".", "_agent_position_list", "\n", "", "for", "agent", "in", "agents", ":", "\n", "\t\t\t", "if", "len", "(", "self", ".", "states", "[", "agent", "]", ")", ">", "0", ":", "\n", "\t\t\t\t", "return", "False", "\n", "", "", "return", "True", "\n", "\n"]], "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.worker.batch.ExperienceBatch.get_action": [[54, 60], ["getattr", "getattr"], "methods", ["None"], ["", "def", "get_action", "(", "self", ",", "agent", ",", "position", ",", "actions", "=", "None", ",", "as_dict", "=", "False", ")", ":", "\n", "\t\t", "if", "actions", "is", "None", ":", "\n", "\t\t\t", "actions", "=", "self", ".", "_batch_keys", "\n", "", "if", "not", "as_dict", ":", "\n", "\t\t\t", "return", "[", "getattr", "(", "self", ",", "key", ")", "[", "agent", "]", "[", "position", "]", "for", "key", "in", "actions", "]", "\n", "", "return", "{", "key", ":", "getattr", "(", "self", ",", "key", ")", "[", "agent", "]", "[", "position", "]", "for", "key", "in", "actions", "}", "\n", "\n"]], "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.worker.batch.ExperienceBatch.has_actions": [[61, 80], ["utils.misc.is_tuple", "getattr", "len", "len"], "methods", ["home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.utils.misc.is_tuple"], ["", "def", "has_actions", "(", "self", ",", "actions", "=", "None", ",", "agents", "=", "None", ")", ":", "\n", "\t\t", "if", "actions", "is", "None", ":", "# All actions", "\n", "\t\t\t", "actions", "=", "self", ".", "_batch_keys", "\n", "", "else", ":", "# Check if (action) keys are valid", "\n", "\t\t\t", "if", "not", "is_tuple", "(", "actions", ")", ":", "\n", "\t\t\t\t", "actions", "=", "(", "actions", ",", ")", "\n", "", "for", "action", "in", "actions", ":", "\n", "\t\t\t\t", "if", "action", "not", "in", "self", ".", "_batch_keys", ":", "\n", "\t\t\t\t\t", "return", "False", "\n", "", "", "", "if", "agents", "is", "None", ":", "# All agents", "\n", "\t\t\t", "agents", "=", "self", ".", "_agent_list", "\n", "# Check whether there is at least one action (for every key) inside the batch", "\n", "", "for", "action", "in", "actions", ":", "\n", "\t\t\t", "for", "agent", "in", "agents", ":", "\n", "\t\t\t\t", "action_list", "=", "getattr", "(", "self", ",", "action", ")", "\n", "exist", "=", "len", "(", "action_list", ")", ">", "agent", "and", "len", "(", "action_list", "[", "agent", "]", ")", ">", "0", "\n", "if", "not", "exist", ":", "\n", "\t\t\t\t\t", "return", "False", "\n", "", "", "", "return", "True", "\n", "\n"]], "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.worker.batch.ExperienceBatch.get_all_actions": [[81, 98], ["tuple", "len", "tuple", "batch.ExperienceBatch.get_action", "len", "zip", "batch.ExperienceBatch.step_generator", "tuple", "iteration_function", "reversed", "getattr"], "methods", ["home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.worker.batch.ExperienceBatch.get_action", "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.worker.batch.ExperienceBatch.step_generator"], ["", "def", "get_all_actions", "(", "self", ",", "actions", "=", "None", ",", "agents", "=", "None", ",", "reverse", "=", "False", ")", ":", "\n", "\t\t", "if", "actions", "is", "None", ":", "# All actions", "\n", "\t\t\t", "actions", "=", "self", ".", "_batch_keys", "\n", "", "if", "agents", "is", "None", ":", "# All agents", "\n", "\t\t\t", "agents", "=", "self", ".", "_agent_list", "\n", "", "if", "len", "(", "agents", ")", "==", "1", ":", "# Single agent", "\n", "\t\t\t", "iteration_function", "=", "(", "lambda", "x", ":", "x", ")", "if", "not", "reverse", "else", "(", "lambda", "x", ":", "tuple", "(", "reversed", "(", "x", ")", ")", ")", "\n", "agent", "=", "agents", "[", "0", "]", "\n", "return", "tuple", "(", "iteration_function", "(", "getattr", "(", "self", ",", "key", ")", "[", "agent", "]", ")", "for", "key", "in", "actions", ")", "\n", "# Not single agent", "\n", "", "result", "=", "(", "\n", "self", ".", "get_action", "(", "actions", "=", "actions", ",", "agent", "=", "agent_id", ",", "position", "=", "pos", ")", "\n", "for", "(", "agent_id", ",", "pos", ")", "in", "self", ".", "step_generator", "(", "agents", ",", "reverse", "=", "reverse", ")", "\n", ")", "\n", "if", "len", "(", "actions", ")", ">", "1", ":", "# More than 1 action", "\n", "\t\t\t", "result", "=", "zip", "(", "*", "result", ")", "\n", "", "return", "tuple", "(", "result", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.worker.batch.ExperienceBatch.set_action": [[99, 105], ["feed_dict.items", "getattr", "len", "len"], "methods", ["None"], ["", "def", "set_action", "(", "self", ",", "feed_dict", ",", "agent", ",", "position", ")", ":", "\n", "\t\t", "for", "(", "key", ",", "value", ")", "in", "feed_dict", ".", "items", "(", ")", ":", "\n", "\t\t\t", "q", "=", "getattr", "(", "self", ",", "key", ")", "[", "agent", "]", "\n", "if", "len", "(", "q", ")", "<=", "position", ":", "# Add missing steps", "\n", "\t\t\t\t", "q", "+=", "[", "None", "]", "*", "(", "position", "-", "len", "(", "q", ")", "+", "1", ")", "\n", "", "q", "[", "position", "]", "=", "value", "\n", "\n"]], "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.worker.batch.ExperienceBatch.add_action": [[106, 111], ["feed_dict.items", "batch.ExperienceBatch._agent_position_list.append", "[].append", "len", "getattr"], "methods", ["home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.worker.batch.ExperienceBatch.append", "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.worker.batch.ExperienceBatch.append"], ["", "", "def", "add_action", "(", "self", ",", "feed_dict", ",", "agent_id", ")", ":", "\n", "\t\t", "for", "(", "key", ",", "value", ")", "in", "feed_dict", ".", "items", "(", ")", ":", "\n", "\t\t\t", "getattr", "(", "self", ",", "key", ")", "[", "agent_id", "]", ".", "append", "(", "value", ")", "\n", "# (agent_id, batch_position), for step_generator ", "\n", "", "self", ".", "_agent_position_list", ".", "append", "(", "(", "agent_id", ",", "len", "(", "self", ".", "states", "[", "agent_id", "]", ")", "-", "1", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.worker.batch.ExperienceBatch.get_cumulative_reward": [[112, 118], ["sum", "sum", "len", "sum", "sum", "enumerate"], "methods", ["home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.utils.segment_tree.SumSegmentTree.sum", "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.utils.segment_tree.SumSegmentTree.sum", "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.utils.segment_tree.SumSegmentTree.sum", "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.utils.segment_tree.SumSegmentTree.sum"], ["", "def", "get_cumulative_reward", "(", "self", ",", "agents", "=", "None", ")", ":", "\n", "# All agents", "\n", "\t\t", "if", "agents", "is", "None", "or", "len", "(", "agents", ")", "==", "self", ".", "_model_size", ":", "\n", "\t\t\t", "return", "sum", "(", "sum", "(", "rewards", ")", "for", "rewards", "in", "self", ".", "rewards", ")", "\n", "# Not all agents", "\n", "", "return", "sum", "(", "sum", "(", "rewards", ")", "for", "agent", ",", "rewards", "in", "enumerate", "(", "self", ".", "rewards", ")", "if", "agent", "in", "agents", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.worker.batch.ExperienceBatch.get_size": [[119, 125], ["sum", "sum", "len", "len", "len", "enumerate"], "methods", ["home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.utils.segment_tree.SumSegmentTree.sum", "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.utils.segment_tree.SumSegmentTree.sum"], ["", "def", "get_size", "(", "self", ",", "agents", "=", "None", ")", ":", "\n", "# All agents", "\n", "\t\t", "if", "agents", "is", "None", "or", "len", "(", "agents", ")", "==", "self", ".", "_model_size", ":", "\n", "\t\t\t", "return", "sum", "(", "len", "(", "s", ")", "for", "s", "in", "self", ".", "states", ")", "\n", "# Not all agents", "\n", "", "return", "sum", "(", "len", "(", "s", ")", "for", "agent", ",", "s", "in", "enumerate", "(", "self", ".", "states", ")", "if", "agent", "in", "agents", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.worker.batch.ExperienceBatch.step_generator": [[126, 137], ["iteration_function", "len", "len", "iteration_function", "iteration_function", "range", "batch.ExperienceBatch.get_size"], "methods", ["home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.worker.batch.ExperienceBatch.get_size"], ["", "def", "step_generator", "(", "self", ",", "agents", "=", "None", ",", "reverse", "=", "False", ")", ":", "\n", "\t\t", "iteration_function", "=", "(", "lambda", "x", ":", "x", ")", "if", "not", "reverse", "else", "reversed", "\n", "# All agents", "\n", "if", "agents", "is", "None", "or", "len", "(", "agents", ")", "==", "self", ".", "_model_size", ":", "\n", "\t\t\t", "return", "iteration_function", "(", "self", ".", "_agent_position_list", ")", "\n", "# Single agent, not all agents", "\n", "", "if", "len", "(", "agents", ")", "==", "1", ":", "\n", "\t\t\t", "agent", "=", "agents", "[", "0", "]", "\n", "return", "(", "(", "agent", ",", "pos", ")", "for", "pos", "in", "iteration_function", "(", "range", "(", "self", ".", "get_size", "(", "agents", ")", ")", ")", ")", "\n", "# Nor single agent, nor all agents", "\n", "", "return", "(", "(", "agent", ",", "pos", ")", "for", "(", "agent", ",", "pos", ")", "in", "iteration_function", "(", "self", ".", "_agent_position_list", ")", "if", "agent", "in", "agents", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.worker.batch.ExperienceBatch.compute_discounted_cumulative_reward": [[138, 184], ["next", "batch.ExperienceBatch.get_all_actions", "cumulative_return_builder", "batch.ExperienceBatch.step_generator", "numpy.array", "list", "len", "list", "enumerate", "map", "list", "list", "reversed", "list", "batch.ExperienceBatch.step_generator", "batch.ExperienceBatch.set_action", "batch.ExperienceBatch.has_actions", "map", "map", "reversed", "numpy.sum", "batch.ExperienceBatch.get_all_actions", "agent.algorithm.ac_algorithm.merge_splitted_advantages"], "methods", ["home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.worker.batch.ExperienceBatch.get_all_actions", "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.worker.batch.ExperienceBatch.step_generator", "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.worker.batch.ExperienceBatch.step_generator", "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.worker.batch.ExperienceBatch.set_action", "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.worker.batch.ExperienceBatch.has_actions", "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.utils.segment_tree.SumSegmentTree.sum", "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.worker.batch.ExperienceBatch.get_all_actions", "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.algorithm.ac_algorithm.merge_splitted_advantages"], ["", "def", "compute_discounted_cumulative_reward", "(", "self", ",", "agents", ",", "gamma", ",", "cumulative_return_builder", ")", ":", "\n", "# Bootstrap", "\n", "\t\t", "terminal", "=", "self", ".", "terminal", "\n", "(", "last_agent", ",", "_", ")", "=", "next", "(", "self", ".", "step_generator", "(", "agents", ",", "reverse", "=", "True", ")", ")", "\n", "# assert last_agent in self.bootstrap, \"Must bootstrap next value before computing the discounted cumulative reward\"", "\n", "last_value", "=", "0", "+", "self", ".", "bootstrap", "[", "last_agent", "]", "# copy by value, summing zero", "\n", "# Get accumulation sequences and initial values", "\n", "reversed_reward", ",", "reversed_value", "=", "self", ".", "get_all_actions", "(", "actions", "=", "[", "'manipulated_rewards'", ",", "'values'", "]", ",", "agents", "=", "agents", ",", "reverse", "=", "True", ")", "\n", "if", "flags", ".", "split_values", ":", "# There are 2 value heads: one for intrinsinc and one for extrinsic rewards", "\n", "\t\t\t", "gamma", "=", "np", ".", "array", "(", "[", "gamma", ",", "flags", ".", "intrinsic_reward_gamma", "]", ")", "\n", "if", "terminal", ":", "# episodic reward", "\n", "\t\t\t\t", "if", "flags", ".", "episodic_extrinsic_reward", ":", "\n", "\t\t\t\t\t", "last_value", "[", "0", "]", "=", "0.", "\n", "", "if", "flags", ".", "episodic_intrinsic_reward", ":", "\n", "\t\t\t\t\t", "last_value", "[", "1", "]", "=", "0.", "\n", "", "", "", "else", ":", "\n", "# Sum intrinsic and extrinsic rewards", "\n", "\t\t\t", "reversed_reward", "=", "list", "(", "map", "(", "lambda", "reward", ":", "np", ".", "sum", "(", "reward", ")", ",", "reversed_reward", ")", ")", "\n", "if", "terminal", "and", "flags", ".", "episodic_extrinsic_reward", ":", "# episodic reward", "\n", "\t\t\t\t", "last_value", "=", "0.", "\n", "# Get reversed cumulative return", "\n", "", "", "reversed_cumulative_return", "=", "cumulative_return_builder", "(", "\n", "gamma", "=", "gamma", ",", "\n", "last_value", "=", "last_value", ",", "\n", "reversed_reward", "=", "reversed_reward", ",", "\n", "reversed_value", "=", "reversed_value", ",", "\n", "reversed_extra", "=", "self", ".", "get_all_actions", "(", "actions", "=", "[", "'extras'", "]", ",", "agents", "=", "agents", ",", "reverse", "=", "True", ")", "[", "0", "]", "if", "self", ".", "has_actions", "(", "actions", "=", "[", "'extras'", "]", ",", "agents", "=", "agents", ")", "else", "None", "\n", ")", "\n", "# Get advantage", "\n", "if", "not", "flags", ".", "runtime_advantage", ":", "\n", "\t\t\t", "if", "flags", ".", "split_values", ":", "# merge intrisic and extrinsic rewards", "\n", "\t\t\t\t", "reversed_advantage", "=", "list", "(", "map", "(", "lambda", "ret", ",", "val", ":", "merge_splitted_advantages", "(", "ret", "-", "val", ")", ",", "reversed_cumulative_return", ",", "reversed_value", ")", ")", "\n", "", "else", ":", "\n", "\t\t\t\t", "reversed_advantage", "=", "list", "(", "map", "(", "lambda", "ret", ",", "val", ":", "ret", "-", "val", ",", "reversed_cumulative_return", ",", "reversed_value", ")", ")", "\n", "# Add accumulations to batch", "\n", "", "", "if", "len", "(", "agents", ")", "==", "1", ":", "# Optimized code for single agent", "\n", "\t\t\t", "agent_id", "=", "agents", "[", "0", "]", "\n", "self", ".", "cumulative_returns", "[", "agent_id", "]", "=", "list", "(", "reversed", "(", "reversed_cumulative_return", ")", ")", "\n", "if", "not", "flags", ".", "runtime_advantage", ":", "\n", "\t\t\t\t", "self", ".", "advantages", "[", "agent_id", "]", "=", "list", "(", "reversed", "(", "reversed_advantage", ")", ")", "\n", "", "", "else", ":", "# Multi-agents code", "\n", "\t\t\t", "for", "i", ",", "agent_pos", "in", "enumerate", "(", "self", ".", "step_generator", "(", "agents", ",", "reverse", "=", "True", ")", ")", ":", "\n", "\t\t\t\t", "feed_dict", "=", "{", "'cumulative_returns'", ":", "reversed_cumulative_return", "[", "i", "]", "}", "\n", "if", "not", "flags", ".", "runtime_advantage", ":", "\n", "\t\t\t\t\t", "feed_dict", "[", "'advantages'", "]", "=", "reversed_advantage", "[", "i", "]", "\n", "", "self", ".", "set_action", "(", "feed_dict", ",", "*", "agent_pos", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.worker.batch.ExperienceBatch.append": [[185, 201], ["range", "len", "range", "[].extend", "getattr", "getattr"], "methods", ["None"], ["", "", "", "def", "append", "(", "self", ",", "batch", ")", ":", "\n", "# Append input batch to current batch (self)", "\n", "\t\t", "assert", "self", ".", "_model_size", "<=", "batch", ".", "_model_size", ",", "\"Trying to append a batch with model_size {} to a batch with model_size {}\"", ".", "format", "(", "batch", ".", "_model_size", ",", "self", ".", "_model_size", ")", "\n", "#=======================================================================", "\n", "# print('before', self.get_cumulative_reward([0]), batch.get_cumulative_reward([0]))", "\n", "#=======================================================================", "\n", "pos_base", "=", "[", "len", "(", "self", ".", "states", "[", "agent", "]", ")", "for", "agent", "in", "range", "(", "self", ".", "_model_size", ")", "]", "\n", "self", ".", "_agent_position_list", "+=", "[", "(", "agent", ",", "pos", "+", "pos_base", "[", "agent", "]", ")", "for", "(", "agent", ",", "pos", ")", "in", "batch", ".", "_agent_position_list", "]", "\n", "for", "i", "in", "range", "(", "self", ".", "_model_size", ")", ":", "\n", "\t\t\t", "for", "key", "in", "self", ".", "_batch_keys", ":", "\n", "\t\t\t\t", "getattr", "(", "self", ",", "key", ")", "[", "i", "]", ".", "extend", "(", "getattr", "(", "batch", ",", "key", ")", "[", "i", "]", ")", "\n", "#=======================================================================", "\n", "# print('after', self.get_cumulative_reward([0]), batch.get_cumulative_reward([0]))", "\n", "#=======================================================================", "\n", "", "", "self", ".", "bootstrap", "=", "batch", ".", "bootstrap", "\n", "self", ".", "terminal", "=", "batch", ".", "terminal", "\n", "\n"]], "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.worker.batch.ExperienceBatch.compress": [[202, 205], ["setattr", "utils.misc.compress", "getattr"], "methods", ["home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.worker.batch.ExperienceBatch.compress"], ["", "def", "compress", "(", "self", ")", ":", "\n", "\t\t", "for", "key", "in", "self", ".", "_batch_keys", ":", "\n", "\t\t\t", "setattr", "(", "self", ",", "key", ",", "compress", "(", "getattr", "(", "self", ",", "key", ")", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.worker.batch.ExperienceBatch.decompress": [[206, 209], ["setattr", "utils.misc.decompress", "getattr"], "methods", ["home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.worker.batch.ExperienceBatch.decompress"], ["", "", "def", "decompress", "(", "self", ")", ":", "\n", "\t\t", "for", "key", "in", "self", ".", "_batch_keys", ":", "\n", "\t\t\t", "setattr", "(", "self", ",", "key", ",", "decompress", "(", "getattr", "(", "self", ",", "key", ")", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.worker.batch.CompositeBatch.__init__": [[211, 214], ["collections.deque"], "methods", ["None"], ["\t", "def", "__init__", "(", "self", ",", "maxlen", "=", "None", ")", ":", "\n", "\t\t", "self", ".", "maxlen", "=", "maxlen", "\n", "self", ".", "_batch_list", "=", "deque", "(", "maxlen", "=", "self", ".", "maxlen", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.worker.batch.CompositeBatch.add": [[215, 219], ["isinstance", "batch.CompositeBatch._batch_list.append"], "methods", ["home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.worker.batch.ExperienceBatch.append"], ["", "def", "add", "(", "self", ",", "batch", ")", ":", "\n", "# Can add only ExperienceBatch items", "\n", "\t\t", "assert", "isinstance", "(", "batch", ",", "ExperienceBatch", ")", "\n", "self", ".", "_batch_list", ".", "append", "(", "batch", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.worker.batch.CompositeBatch.get": [[220, 222], ["None"], "methods", ["None"], ["", "def", "get", "(", "self", ")", ":", "\n", "\t\t", "return", "self", ".", "_batch_list", "\n", "\n"]], "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.worker.batch.CompositeBatch.size": [[223, 225], ["len"], "methods", ["None"], ["", "def", "size", "(", "self", ")", ":", "\n", "\t\t", "return", "len", "(", "self", ".", "_batch_list", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.worker.batch.CompositeBatch.clear": [[226, 229], ["collections.deque"], "methods", ["None"], ["", "def", "clear", "(", "self", ")", ":", "\n", "\t\t", "if", "self", ".", "maxlen", ">", "1", ":", "\n", "\t\t\t", "self", ".", "_batch_list", "=", "deque", "(", "maxlen", "=", "self", ".", "maxlen", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.worker.batch.CompositeBatch.compute_discounted_cumulative_reward": [[230, 243], ["next", "reversed", "last_batch.step_generator", "next", "batch.compute_discounted_cumulative_reward", "next", "batch.step_generator", "batch.step_generator"], "methods", ["home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.worker.batch.ExperienceBatch.step_generator", "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.worker.batch.CompositeBatch.compute_discounted_cumulative_reward", "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.worker.batch.ExperienceBatch.step_generator", "home.repos.pwc.inspect_result.Francesco-Sovrano_Combining--experience-replay--with--exploration-by-random-network-distillation-.worker.batch.ExperienceBatch.step_generator"], ["", "", "def", "compute_discounted_cumulative_reward", "(", "self", ",", "agents", ",", "gamma", ",", "cumulative_return_builder", ")", ":", "\n", "\t\t", "last_batch", "=", "self", ".", "_batch_list", "[", "-", "1", "]", "\n", "(", "last_agent", ",", "_", ")", "=", "next", "(", "last_batch", ".", "step_generator", "(", "agents", ",", "reverse", "=", "True", ")", ")", "\n", "last_value", "=", "last_batch", ".", "bootstrap", "[", "last_agent", "]", "\n", "for", "batch", "in", "reversed", "(", "self", ".", "_batch_list", ")", ":", "\n", "# Update bootstrap value", "\n", "\t\t\t", "(", "last_agent", ",", "_", ")", "=", "next", "(", "batch", ".", "step_generator", "(", "agents", ",", "reverse", "=", "True", ")", ")", "\n", "batch", ".", "bootstrap", "[", "last_agent", "]", "=", "last_value", "\n", "# Compute cumulative return", "\n", "batch", ".", "compute_discounted_cumulative_reward", "(", "agents", ",", "gamma", ",", "cumulative_return_builder", ")", "\n", "# Get next bootstrap value", "\n", "(", "first_agent", ",", "_", ")", "=", "next", "(", "batch", ".", "step_generator", "(", "agents", ",", "reverse", "=", "False", ")", ")", "\n", "last_value", "=", "batch", ".", "cumulative_returns", "[", "first_agent", "]", "[", "0", "]", "\n", "", "", "", ""]]}