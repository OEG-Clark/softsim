{"home.repos.pwc.inspect_result.bychao100_towards-image-compression-and-analysis-with-transformers.full_model.engine.train_one_epoch": [[18, 61], ["model.train", "utils.MetricLogger", "utils.MetricLogger.add_meter", "utils.MetricLogger.log_every", "utils.MetricLogger.synchronize_between_processes", "print", "utils.SmoothedValue", "samples.to.to", "targets.to.to", "optimizer.zero_grad", "aux_optimizer.zero_grad", "model", "criterion", "criterion.item", "criterion.backward", "optimizer.step", "aux_loss.item", "aux_loss.backward", "aux_optimizer.step", "torch.cuda.synchronize", "utils.MetricLogger.update", "utils.MetricLogger.update", "utils.MetricLogger.update", "math.isfinite", "print", "sys.exit", "torch.nn.utils.clip_grad_norm_", "utils.MetricLogger.meters.items", "model.parameters"], "function", ["home.repos.pwc.inspect_result.bychao100_towards-image-compression-and-analysis-with-transformers.pretrained_model.utils.MetricLogger.add_meter", "home.repos.pwc.inspect_result.bychao100_towards-image-compression-and-analysis-with-transformers.pretrained_model.utils.MetricLogger.log_every", "home.repos.pwc.inspect_result.bychao100_towards-image-compression-and-analysis-with-transformers.pretrained_model.utils.MetricLogger.synchronize_between_processes", "home.repos.pwc.inspect_result.bychao100_towards-image-compression-and-analysis-with-transformers.pretrained_model.utils.MetricLogger.update", "home.repos.pwc.inspect_result.bychao100_towards-image-compression-and-analysis-with-transformers.pretrained_model.utils.MetricLogger.update", "home.repos.pwc.inspect_result.bychao100_towards-image-compression-and-analysis-with-transformers.pretrained_model.utils.MetricLogger.update"], ["def", "train_one_epoch", "(", "model", ":", "torch", ".", "nn", ".", "Module", ",", "criterion", ":", "JointLoss", ",", "\n", "data_loader", ":", "Iterable", ",", "optimizer", ":", "torch", ".", "optim", ".", "Optimizer", ",", "aux_optimizer", ":", "torch", ".", "optim", ".", "Optimizer", ",", "\n", "device", ":", "torch", ".", "device", ",", "epoch", ":", "int", ",", "max_norm", ":", "float", "=", "0", ",", "\n", "set_training_mode", "=", "True", ")", ":", "\n", "    ", "model", ".", "train", "(", "set_training_mode", ")", "\n", "metric_logger", "=", "utils", ".", "MetricLogger", "(", "delimiter", "=", "\"  \"", ")", "\n", "metric_logger", ".", "add_meter", "(", "'lr'", ",", "utils", ".", "SmoothedValue", "(", "window_size", "=", "1", ",", "fmt", "=", "'{value:.6f}'", ")", ")", "\n", "header", "=", "'Epoch: [{}]'", ".", "format", "(", "epoch", ")", "\n", "print_freq", "=", "10", "\n", "\n", "for", "samples", ",", "targets", "in", "metric_logger", ".", "log_every", "(", "data_loader", ",", "print_freq", ",", "header", ")", ":", "\n", "        ", "samples", "=", "samples", ".", "to", "(", "device", ",", "non_blocking", "=", "True", ")", "\n", "targets", "=", "targets", ".", "to", "(", "device", ",", "non_blocking", "=", "True", ")", "\n", "\n", "optimizer", ".", "zero_grad", "(", ")", "\n", "aux_optimizer", ".", "zero_grad", "(", ")", "\n", "\n", "outputs", ",", "aux_loss", "=", "model", "(", "samples", ")", "\n", "loss", "=", "criterion", "(", "samples", ",", "outputs", ",", "targets", ")", "\n", "loss_value", "=", "loss", ".", "item", "(", ")", "\n", "loss", ".", "backward", "(", ")", "\n", "\n", "if", "not", "math", ".", "isfinite", "(", "loss_value", ")", ":", "\n", "            ", "print", "(", "\"Loss is {}, stopping training\"", ".", "format", "(", "loss_value", ")", ")", "\n", "sys", ".", "exit", "(", "1", ")", "\n", "\n", "", "if", "max_norm", "is", "not", "None", "and", "max_norm", ">", "0", ":", "\n", "            ", "torch", ".", "nn", ".", "utils", ".", "clip_grad_norm_", "(", "model", ".", "parameters", "(", ")", ",", "max_norm", ")", "\n", "", "optimizer", ".", "step", "(", ")", "\n", "\n", "aux_loss_value", "=", "aux_loss", ".", "item", "(", ")", "\n", "aux_loss", ".", "backward", "(", ")", "\n", "aux_optimizer", ".", "step", "(", ")", "\n", "\n", "torch", ".", "cuda", ".", "synchronize", "(", ")", "\n", "\n", "metric_logger", ".", "update", "(", "loss", "=", "loss_value", ")", "\n", "metric_logger", ".", "update", "(", "aux_loss", "=", "aux_loss_value", ")", "\n", "metric_logger", ".", "update", "(", "lr", "=", "optimizer", ".", "param_groups", "[", "0", "]", "[", "\"lr\"", "]", ")", "\n", "# gather the stats from all processes", "\n", "", "metric_logger", ".", "synchronize_between_processes", "(", ")", "\n", "print", "(", "\"Averaged stats:\"", ",", "metric_logger", ")", "\n", "return", "{", "k", ":", "meter", ".", "global_avg", "for", "k", ",", "meter", "in", "metric_logger", ".", "meters", ".", "items", "(", ")", "}", "\n", "\n"]], "home.repos.pwc.inspect_result.bychao100_towards-image-compression-and-analysis-with-transformers.full_model.engine.evaluate": [[63, 107], ["torch.no_grad", "torch.nn.CrossEntropyLoss", "losses.DenormalizedMSELoss", "utils.MetricLogger", "model.eval", "utils.MetricLogger.log_every", "utils.MetricLogger.synchronize_between_processes", "print", "images.to.to", "target.to.to", "model", "torch.nn.CrossEntropyLoss.", "losses.DenormalizedMSELoss.", "utils.img_distortion", "timm.utils.accuracy", "utils.MetricLogger.update", "utils.MetricLogger.meters[].update", "utils.MetricLogger.meters[].update", "utils.MetricLogger.meters[].update", "utils.MetricLogger.meters[].update", "utils.MetricLogger.meters[].update", "utils.imwrite", "utils.imwrite", "acc1.item", "acc5.item", "criterion_rec.item", "loss_bpp.item", "utils.MetricLogger.meters.items", "torch.log().sum", "torch.log().sum", "os.path.join", "os.path.join", "criterion_cls.item", "math.log", "torch.log", "torch.log"], "function", ["home.repos.pwc.inspect_result.bychao100_towards-image-compression-and-analysis-with-transformers.pretrained_model.utils.MetricLogger.log_every", "home.repos.pwc.inspect_result.bychao100_towards-image-compression-and-analysis-with-transformers.pretrained_model.utils.MetricLogger.synchronize_between_processes", "home.repos.pwc.inspect_result.bychao100_towards-image-compression-and-analysis-with-transformers.pretrained_model.utils.img_distortion", "home.repos.pwc.inspect_result.bychao100_towards-image-compression-and-analysis-with-transformers.pretrained_model.utils.MetricLogger.update", "home.repos.pwc.inspect_result.bychao100_towards-image-compression-and-analysis-with-transformers.pretrained_model.utils.MetricLogger.update", "home.repos.pwc.inspect_result.bychao100_towards-image-compression-and-analysis-with-transformers.pretrained_model.utils.MetricLogger.update", "home.repos.pwc.inspect_result.bychao100_towards-image-compression-and-analysis-with-transformers.pretrained_model.utils.MetricLogger.update", "home.repos.pwc.inspect_result.bychao100_towards-image-compression-and-analysis-with-transformers.pretrained_model.utils.MetricLogger.update", "home.repos.pwc.inspect_result.bychao100_towards-image-compression-and-analysis-with-transformers.pretrained_model.utils.MetricLogger.update", "home.repos.pwc.inspect_result.bychao100_towards-image-compression-and-analysis-with-transformers.pretrained_model.utils.imwrite", "home.repos.pwc.inspect_result.bychao100_towards-image-compression-and-analysis-with-transformers.pretrained_model.utils.imwrite"], ["", "@", "torch", ".", "no_grad", "(", ")", "\n", "def", "evaluate", "(", "data_loader", ",", "model", ",", "device", ",", "output_dir", ",", "write_img", "=", "True", ")", ":", "\n", "    ", "criterion_cls", "=", "torch", ".", "nn", ".", "CrossEntropyLoss", "(", ")", "\n", "criterion_rec", "=", "DenormalizedMSELoss", "(", ")", "\n", "\n", "metric_logger", "=", "utils", ".", "MetricLogger", "(", "delimiter", "=", "\"  \"", ")", "\n", "header", "=", "'Test:'", "\n", "\n", "# switch to evaluation mode", "\n", "model", ".", "eval", "(", ")", "\n", "for", "images", ",", "target", "in", "metric_logger", ".", "log_every", "(", "data_loader", ",", "10", ",", "header", ")", ":", "\n", "        ", "images", "=", "images", ".", "to", "(", "device", ",", "non_blocking", "=", "True", ")", "\n", "target", "=", "target", ".", "to", "(", "device", ",", "non_blocking", "=", "True", ")", "\n", "\n", "B", ",", "_", ",", "H", ",", "W", "=", "images", ".", "shape", "\n", "num_pixels", "=", "B", "*", "H", "*", "W", "\n", "\n", "# compute output", "\n", "output", ",", "_", "=", "model", "(", "images", ")", "\n", "loss_cls", "=", "criterion_cls", "(", "output", "[", "0", "]", ",", "target", ")", "\n", "loss_rec", "=", "criterion_rec", "(", "output", "[", "1", "]", ",", "images", ")", "\n", "loss_bpp", "=", "(", "torch", ".", "log", "(", "output", "[", "2", "]", ")", ".", "sum", "(", ")", "+", "torch", ".", "log", "(", "output", "[", "3", "]", ")", ".", "sum", "(", ")", ")", "/", "(", "-", "math", ".", "log", "(", "2", ")", "*", "num_pixels", ")", "\n", "psnr", "=", "utils", ".", "img_distortion", "(", "output", "[", "1", "]", ",", "images", ")", "\n", "\n", "if", "write_img", ":", "\n", "            ", "utils", ".", "imwrite", "(", "images", "[", ":", "4", "]", ",", "os", ".", "path", ".", "join", "(", "output_dir", ",", "'example_org.png'", ")", ")", "\n", "utils", ".", "imwrite", "(", "output", "[", "1", "]", "[", ":", "4", "]", ",", "os", ".", "path", ".", "join", "(", "output_dir", ",", "'example_rec.png'", ")", ")", "\n", "write_img", "=", "False", "\n", "\n", "", "acc1", ",", "acc5", "=", "accuracy", "(", "output", "[", "0", "]", ",", "target", ",", "topk", "=", "(", "1", ",", "5", ")", ")", "\n", "\n", "batch_size", "=", "images", ".", "shape", "[", "0", "]", "\n", "metric_logger", ".", "update", "(", "loss_cls", "=", "loss_cls", ".", "item", "(", ")", ")", "\n", "metric_logger", ".", "meters", "[", "'acc1'", "]", ".", "update", "(", "acc1", ".", "item", "(", ")", ",", "n", "=", "batch_size", ")", "\n", "metric_logger", ".", "meters", "[", "'acc5'", "]", ".", "update", "(", "acc5", ".", "item", "(", ")", ",", "n", "=", "batch_size", ")", "\n", "metric_logger", ".", "meters", "[", "'loss_rec'", "]", ".", "update", "(", "loss_rec", ".", "item", "(", ")", ",", "n", "=", "batch_size", ")", "\n", "metric_logger", ".", "meters", "[", "'loss_bpp'", "]", ".", "update", "(", "loss_bpp", ".", "item", "(", ")", ",", "n", "=", "batch_size", ")", "\n", "metric_logger", ".", "meters", "[", "'psnr'", "]", ".", "update", "(", "psnr", ",", "n", "=", "batch_size", ")", "\n", "# gather the stats from all processes", "\n", "", "metric_logger", ".", "synchronize_between_processes", "(", ")", "\n", "print", "(", "'* Acc@1 {top1.global_avg:.3f} Acc@5 {top5.global_avg:.3f} loss_cls {losses_cls.global_avg:.3f} loss_rec {losses_rec.global_avg:.3f} loss bpp {losses_bpp.global_avg:.3f} psnr {psnr.global_avg:.3f}'", "\n", ".", "format", "(", "top1", "=", "metric_logger", ".", "acc1", ",", "top5", "=", "metric_logger", ".", "acc5", ",", "losses_cls", "=", "metric_logger", ".", "loss_cls", ",", "losses_rec", "=", "metric_logger", ".", "loss_rec", ",", "losses_bpp", "=", "metric_logger", ".", "loss_bpp", ",", "psnr", "=", "metric_logger", ".", "psnr", ")", ")", "\n", "\n", "return", "{", "k", ":", "meter", ".", "global_avg", "for", "k", ",", "meter", "in", "metric_logger", ".", "meters", ".", "items", "(", ")", "}", "\n", "\n"]], "home.repos.pwc.inspect_result.bychao100_towards-image-compression-and-analysis-with-transformers.full_model.engine.evaluate_real": [[109, 157], ["torch.no_grad", "torch.nn.CrossEntropyLoss", "losses.DenormalizedMSELoss", "utils.MetricLogger", "model.eval", "model.update", "utils.MetricLogger.log_every", "utils.MetricLogger.synchronize_between_processes", "print", "images.to.to", "target.to.to", "model.compress", "model.decompress", "torch.nn.CrossEntropyLoss.", "losses.DenormalizedMSELoss.", "utils.img_distortion", "sum", "timm.utils.accuracy", "utils.MetricLogger.update", "utils.MetricLogger.meters[].update", "utils.MetricLogger.meters[].update", "utils.MetricLogger.meters[].update", "utils.MetricLogger.meters[].update", "utils.MetricLogger.meters[].update", "utils.imwrite", "utils.imwrite", "acc1.item", "acc5.item", "criterion_rec.item", "utils.MetricLogger.meters.items", "os.path.join", "os.path.join", "criterion_cls.item", "range", "len", "len"], "function", ["home.repos.pwc.inspect_result.bychao100_towards-image-compression-and-analysis-with-transformers.pretrained_model.utils.MetricLogger.update", "home.repos.pwc.inspect_result.bychao100_towards-image-compression-and-analysis-with-transformers.pretrained_model.utils.MetricLogger.log_every", "home.repos.pwc.inspect_result.bychao100_towards-image-compression-and-analysis-with-transformers.pretrained_model.utils.MetricLogger.synchronize_between_processes", "home.repos.pwc.inspect_result.bychao100_towards-image-compression-and-analysis-with-transformers.full_model.model.JCCTransformer.compress", "home.repos.pwc.inspect_result.bychao100_towards-image-compression-and-analysis-with-transformers.full_model.model.JCCTransformer.decompress", "home.repos.pwc.inspect_result.bychao100_towards-image-compression-and-analysis-with-transformers.pretrained_model.utils.img_distortion", "home.repos.pwc.inspect_result.bychao100_towards-image-compression-and-analysis-with-transformers.pretrained_model.utils.MetricLogger.update", "home.repos.pwc.inspect_result.bychao100_towards-image-compression-and-analysis-with-transformers.pretrained_model.utils.MetricLogger.update", "home.repos.pwc.inspect_result.bychao100_towards-image-compression-and-analysis-with-transformers.pretrained_model.utils.MetricLogger.update", "home.repos.pwc.inspect_result.bychao100_towards-image-compression-and-analysis-with-transformers.pretrained_model.utils.MetricLogger.update", "home.repos.pwc.inspect_result.bychao100_towards-image-compression-and-analysis-with-transformers.pretrained_model.utils.MetricLogger.update", "home.repos.pwc.inspect_result.bychao100_towards-image-compression-and-analysis-with-transformers.pretrained_model.utils.MetricLogger.update", "home.repos.pwc.inspect_result.bychao100_towards-image-compression-and-analysis-with-transformers.pretrained_model.utils.imwrite", "home.repos.pwc.inspect_result.bychao100_towards-image-compression-and-analysis-with-transformers.pretrained_model.utils.imwrite"], ["", "@", "torch", ".", "no_grad", "(", ")", "\n", "def", "evaluate_real", "(", "data_loader", ",", "model", ",", "device", ",", "output_dir", ",", "write_img", "=", "True", ")", ":", "\n", "    ", "criterion_cls", "=", "torch", ".", "nn", ".", "CrossEntropyLoss", "(", ")", "\n", "criterion_rec", "=", "DenormalizedMSELoss", "(", ")", "\n", "\n", "metric_logger", "=", "utils", ".", "MetricLogger", "(", "delimiter", "=", "\"  \"", ")", "\n", "header", "=", "'Test:'", "\n", "\n", "# switch to evaluation mode", "\n", "model", ".", "eval", "(", ")", "\n", "model", ".", "update", "(", "force", "=", "True", ")", "\n", "for", "images", ",", "target", "in", "metric_logger", ".", "log_every", "(", "data_loader", ",", "10", ",", "header", ")", ":", "\n", "        ", "images", "=", "images", ".", "to", "(", "device", ",", "non_blocking", "=", "True", ")", "\n", "target", "=", "target", ".", "to", "(", "device", ",", "non_blocking", "=", "True", ")", "\n", "\n", "B", ",", "_", ",", "H", ",", "W", "=", "images", ".", "shape", "\n", "num_pixels", "=", "B", "*", "H", "*", "W", "\n", "\n", "# compute output", "\n", "out_enc", "=", "model", ".", "compress", "(", "images", ")", "\n", "output", "=", "model", ".", "decompress", "(", "out_enc", "[", "'strings'", "]", ",", "out_enc", "[", "'shape'", "]", ")", "\n", "loss_cls", "=", "criterion_cls", "(", "output", "[", "0", "]", ",", "target", ")", "\n", "loss_rec", "=", "criterion_rec", "(", "output", "[", "1", "]", ",", "images", ")", "\n", "psnr", "=", "utils", ".", "img_distortion", "(", "output", "[", "1", "]", ",", "images", ")", "\n", "\n", "bitstream", "=", "sum", "(", "[", "len", "(", "out_enc", "[", "'strings'", "]", "[", "0", "]", "[", "i", "]", ")", "+", "len", "(", "out_enc", "[", "'strings'", "]", "[", "1", "]", "[", "i", "]", ")", "+", "16", "for", "i", "in", "range", "(", "B", ")", "]", ")", "\n", "loss_bpp", "=", "bitstream", "*", "8", "/", "num_pixels", "\n", "\n", "if", "write_img", ":", "\n", "            ", "utils", ".", "imwrite", "(", "images", "[", ":", "4", "]", ",", "os", ".", "path", ".", "join", "(", "output_dir", ",", "'example_org.png'", ")", ")", "\n", "utils", ".", "imwrite", "(", "output", "[", "1", "]", "[", ":", "4", "]", ",", "os", ".", "path", ".", "join", "(", "output_dir", ",", "'example_rec.png'", ")", ")", "\n", "write_img", "=", "False", "\n", "\n", "", "acc1", ",", "acc5", "=", "accuracy", "(", "output", "[", "0", "]", ",", "target", ",", "topk", "=", "(", "1", ",", "5", ")", ")", "\n", "\n", "batch_size", "=", "images", ".", "shape", "[", "0", "]", "\n", "metric_logger", ".", "update", "(", "loss_cls", "=", "loss_cls", ".", "item", "(", ")", ")", "\n", "metric_logger", ".", "meters", "[", "'acc1'", "]", ".", "update", "(", "acc1", ".", "item", "(", ")", ",", "n", "=", "batch_size", ")", "\n", "metric_logger", ".", "meters", "[", "'acc5'", "]", ".", "update", "(", "acc5", ".", "item", "(", ")", ",", "n", "=", "batch_size", ")", "\n", "metric_logger", ".", "meters", "[", "'loss_rec'", "]", ".", "update", "(", "loss_rec", ".", "item", "(", ")", ",", "n", "=", "batch_size", ")", "\n", "metric_logger", ".", "meters", "[", "'loss_bpp'", "]", ".", "update", "(", "loss_bpp", ",", "n", "=", "batch_size", ")", "\n", "metric_logger", ".", "meters", "[", "'psnr'", "]", ".", "update", "(", "psnr", ",", "n", "=", "batch_size", ")", "\n", "# gather the stats from all processes", "\n", "", "metric_logger", ".", "synchronize_between_processes", "(", ")", "\n", "print", "(", "'* Acc@1 {top1.global_avg:.3f} Acc@5 {top5.global_avg:.3f} loss_cls {losses_cls.global_avg:.3f} loss_rec {losses_rec.global_avg:.3f} loss bpp {losses_bpp.global_avg:.3f} psnr {psnr.global_avg:.3f}'", "\n", ".", "format", "(", "top1", "=", "metric_logger", ".", "acc1", ",", "top5", "=", "metric_logger", ".", "acc5", ",", "losses_cls", "=", "metric_logger", ".", "loss_cls", ",", "losses_rec", "=", "metric_logger", ".", "loss_rec", ",", "losses_bpp", "=", "metric_logger", ".", "loss_bpp", ",", "psnr", "=", "metric_logger", ".", "psnr", ")", ")", "\n", "\n", "return", "{", "k", ":", "meter", ".", "global_avg", "for", "k", ",", "meter", "in", "metric_logger", ".", "meters", ".", "items", "(", ")", "}", "\n", "", ""]], "home.repos.pwc.inspect_result.bychao100_towards-image-compression-and-analysis-with-transformers.full_model.main.get_args_parser": [[34, 136], ["argparse.ArgumentParser", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.set_defaults", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.set_defaults", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument"], "function", ["None"], ["def", "get_args_parser", "(", ")", ":", "\n", "    ", "parser", "=", "argparse", ".", "ArgumentParser", "(", "'JCCTransformer training and evaluation script'", ",", "add_help", "=", "False", ")", "\n", "parser", ".", "add_argument", "(", "'--batch-size'", ",", "default", "=", "64", ",", "type", "=", "int", ")", "\n", "parser", ".", "add_argument", "(", "'--epochs'", ",", "default", "=", "300", ",", "type", "=", "int", ")", "\n", "\n", "# Model parameters", "\n", "parser", ".", "add_argument", "(", "'--model'", ",", "default", "=", "'finetune_tiny_patch16_224'", ",", "type", "=", "str", ",", "metavar", "=", "'MODEL'", ",", "\n", "help", "=", "'Name of model to train'", ")", "\n", "parser", ".", "add_argument", "(", "'--input-size'", ",", "default", "=", "224", ",", "type", "=", "int", ",", "help", "=", "'images input size'", ")", "\n", "\n", "parser", ".", "add_argument", "(", "'--quality'", ",", "default", "=", "1", ",", "type", "=", "int", ",", "help", "=", "'quality'", ")", "\n", "\n", "parser", ".", "add_argument", "(", "'--drop'", ",", "type", "=", "float", ",", "default", "=", "0.0", ",", "metavar", "=", "'PCT'", ",", "\n", "help", "=", "'Dropout rate (default: 0.)'", ")", "\n", "parser", ".", "add_argument", "(", "'--drop-path'", ",", "type", "=", "float", ",", "default", "=", "0.1", ",", "metavar", "=", "'PCT'", ",", "\n", "help", "=", "'Drop path rate (default: 0.1)'", ")", "\n", "\n", "parser", ".", "add_argument", "(", "'--pretrained'", ",", "action", "=", "'store_true'", ")", "\n", "parser", ".", "add_argument", "(", "'--no-pretrained'", ",", "action", "=", "'store_false'", ",", "dest", "=", "'pretrained'", ")", "\n", "parser", ".", "set_defaults", "(", "pretrained", "=", "True", ")", "\n", "\n", "# Optimizer parameters", "\n", "parser", ".", "add_argument", "(", "'--opt'", ",", "default", "=", "'adam'", ",", "type", "=", "str", ",", "metavar", "=", "'OPTIMIZER'", ",", "\n", "help", "=", "'Optimizer (default: \"adam\"'", ")", "\n", "parser", ".", "add_argument", "(", "'--opt-eps'", ",", "default", "=", "1e-8", ",", "type", "=", "float", ",", "metavar", "=", "'EPSILON'", ",", "\n", "help", "=", "'Optimizer Epsilon (default: 1e-8)'", ")", "\n", "parser", ".", "add_argument", "(", "'--opt-betas'", ",", "default", "=", "None", ",", "type", "=", "float", ",", "nargs", "=", "'+'", ",", "metavar", "=", "'BETA'", ",", "\n", "help", "=", "'Optimizer Betas (default: None, use opt default)'", ")", "\n", "parser", ".", "add_argument", "(", "'--clip-grad'", ",", "type", "=", "float", ",", "default", "=", "None", ",", "metavar", "=", "'NORM'", ",", "\n", "help", "=", "'Clip gradient norm (default: None, no clipping)'", ")", "\n", "parser", ".", "add_argument", "(", "'--momentum'", ",", "type", "=", "float", ",", "default", "=", "0.9", ",", "metavar", "=", "'M'", ",", "\n", "help", "=", "'SGD momentum (default: 0.9)'", ")", "\n", "parser", ".", "add_argument", "(", "'--weight-decay'", ",", "type", "=", "float", ",", "default", "=", "0.", ",", "\n", "help", "=", "'weight decay (default: 0.)'", ")", "\n", "# Learning rate schedule parameters", "\n", "parser", ".", "add_argument", "(", "'--sched'", ",", "default", "=", "'cosine'", ",", "type", "=", "str", ",", "metavar", "=", "'SCHEDULER'", ",", "\n", "help", "=", "'LR scheduler (default: \"cosine\"'", ")", "\n", "parser", ".", "add_argument", "(", "'--lr'", ",", "type", "=", "float", ",", "default", "=", "1e-4", ",", "metavar", "=", "'LR'", ",", "\n", "help", "=", "'learning rate (default: 1e-4)'", ")", "\n", "parser", ".", "add_argument", "(", "'--aux-lr'", ",", "type", "=", "float", ",", "default", "=", "1e-3", ",", "metavar", "=", "'LR'", ",", "\n", "help", "=", "'aux learning rate (default: 1e-3)'", ")", "\n", "parser", ".", "add_argument", "(", "'--lr-noise'", ",", "type", "=", "float", ",", "nargs", "=", "'+'", ",", "default", "=", "None", ",", "metavar", "=", "'pct, pct'", ",", "\n", "help", "=", "'learning rate noise on/off epoch percentages'", ")", "\n", "parser", ".", "add_argument", "(", "'--lr-noise-pct'", ",", "type", "=", "float", ",", "default", "=", "0.67", ",", "metavar", "=", "'PERCENT'", ",", "\n", "help", "=", "'learning rate noise limit percent (default: 0.67)'", ")", "\n", "parser", ".", "add_argument", "(", "'--lr-noise-std'", ",", "type", "=", "float", ",", "default", "=", "1.0", ",", "metavar", "=", "'STDDEV'", ",", "\n", "help", "=", "'learning rate noise std-dev (default: 1.0)'", ")", "\n", "parser", ".", "add_argument", "(", "'--warmup-lr'", ",", "type", "=", "float", ",", "default", "=", "1e-6", ",", "metavar", "=", "'LR'", ",", "\n", "help", "=", "'warmup learning rate (default: 1e-6)'", ")", "\n", "parser", ".", "add_argument", "(", "'--min-lr'", ",", "type", "=", "float", ",", "default", "=", "1e-5", ",", "metavar", "=", "'LR'", ",", "\n", "help", "=", "'lower lr bound for cyclic schedulers that hit 0 (1e-5)'", ")", "\n", "\n", "parser", ".", "add_argument", "(", "'--decay-epochs'", ",", "type", "=", "float", ",", "default", "=", "30", ",", "metavar", "=", "'N'", ",", "\n", "help", "=", "'epoch interval to decay LR'", ")", "\n", "parser", ".", "add_argument", "(", "'--warmup-epochs'", ",", "type", "=", "int", ",", "default", "=", "5", ",", "metavar", "=", "'N'", ",", "\n", "help", "=", "'epochs to warmup LR, if scheduler supports'", ")", "\n", "parser", ".", "add_argument", "(", "'--cooldown-epochs'", ",", "type", "=", "int", ",", "default", "=", "10", ",", "metavar", "=", "'N'", ",", "\n", "help", "=", "'epochs to cooldown LR at min_lr, after cyclic schedule ends'", ")", "\n", "parser", ".", "add_argument", "(", "'--patience-epochs'", ",", "type", "=", "int", ",", "default", "=", "10", ",", "metavar", "=", "'N'", ",", "\n", "help", "=", "'patience epochs for Plateau LR scheduler (default: 10'", ")", "\n", "parser", ".", "add_argument", "(", "'--decay-rate'", ",", "'--dr'", ",", "type", "=", "float", ",", "default", "=", "0.1", ",", "metavar", "=", "'RATE'", ",", "\n", "help", "=", "'LR decay rate (default: 0.1)'", ")", "\n", "\n", "# Augmentation parameters", "\n", "parser", ".", "add_argument", "(", "'--smoothing'", ",", "type", "=", "float", ",", "default", "=", "0.1", ",", "help", "=", "'Label smoothing (default: 0.1)'", ")", "\n", "parser", ".", "add_argument", "(", "'--train-interpolation'", ",", "type", "=", "str", ",", "default", "=", "'bicubic'", ",", "\n", "help", "=", "'Training interpolation (random, bilinear, bicubic default: \"bicubic\")'", ")", "\n", "\n", "# * Finetuning params", "\n", "parser", ".", "add_argument", "(", "'--finetune'", ",", "default", "=", "''", ",", "help", "=", "'finetune from checkpoint'", ")", "\n", "\n", "# Dataset parameters", "\n", "parser", ".", "add_argument", "(", "'--data-path'", ",", "default", "=", "'/datasets01/imagenet_full_size/061417/'", ",", "type", "=", "str", ",", "\n", "help", "=", "'dataset path'", ")", "\n", "parser", ".", "add_argument", "(", "'--data-set'", ",", "default", "=", "'IMNET'", ",", "choices", "=", "[", "'IMNET'", ",", "'INAT19'", "]", ",", "\n", "type", "=", "str", ",", "help", "=", "'Image Net dataset path'", ")", "\n", "parser", ".", "add_argument", "(", "'--inat-category'", ",", "default", "=", "'name'", ",", "\n", "choices", "=", "[", "'kingdom'", ",", "'phylum'", ",", "'class'", ",", "'order'", ",", "'supercategory'", ",", "'family'", ",", "'genus'", ",", "'name'", "]", ",", "\n", "type", "=", "str", ",", "help", "=", "'semantic granularity'", ")", "\n", "\n", "parser", ".", "add_argument", "(", "'--output_dir'", ",", "default", "=", "''", ",", "\n", "help", "=", "'path where to save, empty for no saving'", ")", "\n", "parser", ".", "add_argument", "(", "'--device'", ",", "default", "=", "'cuda'", ",", "\n", "help", "=", "'device to use for training / testing'", ")", "\n", "parser", ".", "add_argument", "(", "'--seed'", ",", "default", "=", "0", ",", "type", "=", "int", ")", "\n", "parser", ".", "add_argument", "(", "'--resume'", ",", "default", "=", "''", ",", "help", "=", "'resume from checkpoint'", ")", "\n", "parser", ".", "add_argument", "(", "'--start_epoch'", ",", "default", "=", "0", ",", "type", "=", "int", ",", "metavar", "=", "'N'", ",", "\n", "help", "=", "'start epoch'", ")", "\n", "parser", ".", "add_argument", "(", "'--eval'", ",", "action", "=", "'store_true'", ",", "help", "=", "'Perform evaluation only'", ")", "\n", "parser", ".", "add_argument", "(", "'--dist-eval'", ",", "action", "=", "'store_true'", ",", "default", "=", "False", ",", "help", "=", "'Enabling distributed evaluation'", ")", "\n", "parser", ".", "add_argument", "(", "'--num_workers'", ",", "default", "=", "10", ",", "type", "=", "int", ")", "\n", "parser", ".", "add_argument", "(", "'--pin-mem'", ",", "action", "=", "'store_true'", ",", "\n", "help", "=", "'Pin CPU memory in DataLoader for more efficient (sometimes) transfer to GPU.'", ")", "\n", "parser", ".", "add_argument", "(", "'--no-pin-mem'", ",", "action", "=", "'store_false'", ",", "dest", "=", "'pin_mem'", ",", "\n", "help", "=", "''", ")", "\n", "parser", ".", "set_defaults", "(", "pin_mem", "=", "True", ")", "\n", "\n", "# distributed training parameters", "\n", "parser", ".", "add_argument", "(", "'--world_size'", ",", "default", "=", "1", ",", "type", "=", "int", ",", "\n", "help", "=", "'number of distributed processes'", ")", "\n", "parser", ".", "add_argument", "(", "'--dist_url'", ",", "default", "=", "'env://'", ",", "help", "=", "'url used to set up distributed training'", ")", "\n", "return", "parser", "\n", "\n"]], "home.repos.pwc.inspect_result.bychao100_towards-image-compression-and-analysis-with-transformers.full_model.main.main": [[138, 317], ["utils.init_distributed_mode", "print", "torch.device", "torch.device", "torch.manual_seed", "torch.manual_seed", "numpy.random.seed", "datasets.build_dataset", "datasets.build_dataset", "utils.get_world_size", "utils.get_rank", "torch.utils.data.DistributedSampler", "torch.utils.data.DistributedSampler", "torch.utils.data.DataLoader", "torch.utils.data.DataLoader", "torch.utils.data.DataLoader", "torch.utils.data.DataLoader", "print", "timm.models.create_model", "torch.nn.parallel.DistributedDataParallel.to", "sum", "print", "optim.configure_optimizers", "timm.scheduler.create_scheduler", "losses.JointLoss", "pathlib.Path", "print", "time.time", "range", "str", "print", "utils.get_rank", "torch.utils.data.DistributedSampler", "torch.utils.data.DistributedSampler", "torch.utils.data.SequentialSampler", "torch.utils.data.SequentialSampler", "args.finetune.startswith", "torch.nn.parallel.DistributedDataParallel.state_dict", "int", "int", "pos_tokens.permute().flatten.reshape().permute", "torch.nn.functional.interpolate", "torch.nn.functional.interpolate", "pos_tokens.permute().flatten.permute().flatten", "torch.cat", "torch.cat", "torch.nn.parallel.DistributedDataParallel.load_state_dict", "torch.nn.parallel.DistributedDataParallel", "torch.nn.parallel.DistributedDataParallel", "timm.loss.LabelSmoothingCrossEntropy", "torch.nn.CrossEntropyLoss", "torch.nn.CrossEntropyLoss", "args.resume.startswith", "model_without_ddp.load_state_dict", "engine.evaluate_real", "print", "engine.train_one_epoch", "lr_scheduler.step", "engine.evaluate", "print", "max", "print", "time.time", "datetime.timedelta", "print", "int", "torch.hub.load_state_dict_from_url", "torch.hub.load_state_dict_from_url", "torch.load", "torch.load", "p.numel", "torch.hub.load_state_dict_from_url", "torch.hub.load_state_dict_from_url", "torch.load", "torch.load", "optimizer.load_state_dict", "aux_optimizer.load_state_dict", "lr_scheduler.load_state_dict", "torch.utils.data.DataLoader.sampler.set_epoch", "utils.is_main_process", "len", "print", "pos_tokens.permute().flatten.reshape", "pos_tokens.permute().flatten.permute", "torch.nn.parallel.DistributedDataParallel.parameters", "utils.save_on_master", "f.write", "int", "len", "len", "engine.train_one_epoch.items", "engine.evaluate.items", "model_without_ddp.state_dict", "optimizer.state_dict", "aux_optimizer.state_dict", "lr_scheduler.state_dict", "json.dumps"], "function", ["home.repos.pwc.inspect_result.bychao100_towards-image-compression-and-analysis-with-transformers.pretrained_model.utils.init_distributed_mode", "home.repos.pwc.inspect_result.bychao100_towards-image-compression-and-analysis-with-transformers.pretrained_model.datasets.build_dataset", "home.repos.pwc.inspect_result.bychao100_towards-image-compression-and-analysis-with-transformers.pretrained_model.datasets.build_dataset", "home.repos.pwc.inspect_result.bychao100_towards-image-compression-and-analysis-with-transformers.pretrained_model.utils.get_world_size", "home.repos.pwc.inspect_result.bychao100_towards-image-compression-and-analysis-with-transformers.pretrained_model.utils.get_rank", "home.repos.pwc.inspect_result.bychao100_towards-image-compression-and-analysis-with-transformers.full_model.optim.configure_optimizers", "home.repos.pwc.inspect_result.bychao100_towards-image-compression-and-analysis-with-transformers.pretrained_model.utils.get_rank", "home.repos.pwc.inspect_result.bychao100_towards-image-compression-and-analysis-with-transformers.full_model.model.JCCTransformer.load_state_dict", "home.repos.pwc.inspect_result.bychao100_towards-image-compression-and-analysis-with-transformers.full_model.model.JCCTransformer.load_state_dict", "home.repos.pwc.inspect_result.bychao100_towards-image-compression-and-analysis-with-transformers.full_model.engine.evaluate_real", "home.repos.pwc.inspect_result.bychao100_towards-image-compression-and-analysis-with-transformers.pretrained_model.engine.train_one_epoch", "home.repos.pwc.inspect_result.bychao100_towards-image-compression-and-analysis-with-transformers.pretrained_model.engine.evaluate", "home.repos.pwc.inspect_result.bychao100_towards-image-compression-and-analysis-with-transformers.pretrained_model.utils.SmoothedValue.max", "home.repos.pwc.inspect_result.bychao100_towards-image-compression-and-analysis-with-transformers.full_model.model.JCCTransformer.load_state_dict", "home.repos.pwc.inspect_result.bychao100_towards-image-compression-and-analysis-with-transformers.full_model.model.JCCTransformer.load_state_dict", "home.repos.pwc.inspect_result.bychao100_towards-image-compression-and-analysis-with-transformers.full_model.model.JCCTransformer.load_state_dict", "home.repos.pwc.inspect_result.bychao100_towards-image-compression-and-analysis-with-transformers.pretrained_model.samplers.RASampler.set_epoch", "home.repos.pwc.inspect_result.bychao100_towards-image-compression-and-analysis-with-transformers.pretrained_model.utils.is_main_process", "home.repos.pwc.inspect_result.bychao100_towards-image-compression-and-analysis-with-transformers.pretrained_model.utils.save_on_master"], ["", "def", "main", "(", "args", ")", ":", "\n", "    ", "utils", ".", "init_distributed_mode", "(", "args", ")", "\n", "\n", "print", "(", "args", ")", "\n", "\n", "device", "=", "torch", ".", "device", "(", "args", ".", "device", ")", "\n", "\n", "# fix the seed for reproducibility", "\n", "seed", "=", "args", ".", "seed", "+", "utils", ".", "get_rank", "(", ")", "\n", "torch", ".", "manual_seed", "(", "seed", ")", "\n", "np", ".", "random", ".", "seed", "(", "seed", ")", "\n", "\n", "cudnn", ".", "benchmark", "=", "True", "\n", "\n", "dataset_train", ",", "args", ".", "nb_classes", "=", "build_dataset", "(", "is_train", "=", "True", ",", "args", "=", "args", ")", "\n", "dataset_val", ",", "_", "=", "build_dataset", "(", "is_train", "=", "False", ",", "args", "=", "args", ")", "\n", "\n", "num_tasks", "=", "utils", ".", "get_world_size", "(", ")", "\n", "global_rank", "=", "utils", ".", "get_rank", "(", ")", "\n", "sampler_train", "=", "torch", ".", "utils", ".", "data", ".", "DistributedSampler", "(", "\n", "dataset_train", ",", "num_replicas", "=", "num_tasks", ",", "rank", "=", "global_rank", ",", "shuffle", "=", "True", "\n", ")", "\n", "if", "args", ".", "dist_eval", ":", "\n", "        ", "if", "len", "(", "dataset_val", ")", "%", "num_tasks", "!=", "0", ":", "\n", "            ", "print", "(", "'Warning: Enabling distributed evaluation with an eval dataset not divisible by process number. '", "\n", "'This will slightly alter validation results as extra duplicate entries are added to achieve '", "\n", "'equal num of samples per-process.'", ")", "\n", "", "sampler_val", "=", "torch", ".", "utils", ".", "data", ".", "DistributedSampler", "(", "\n", "dataset_val", ",", "num_replicas", "=", "num_tasks", ",", "rank", "=", "global_rank", ",", "shuffle", "=", "False", ")", "\n", "", "else", ":", "\n", "        ", "sampler_val", "=", "torch", ".", "utils", ".", "data", ".", "SequentialSampler", "(", "dataset_val", ")", "\n", "\n", "", "data_loader_train", "=", "torch", ".", "utils", ".", "data", ".", "DataLoader", "(", "\n", "dataset_train", ",", "sampler", "=", "sampler_train", ",", "\n", "batch_size", "=", "args", ".", "batch_size", ",", "\n", "num_workers", "=", "args", ".", "num_workers", ",", "\n", "pin_memory", "=", "args", ".", "pin_mem", ",", "\n", "drop_last", "=", "True", ",", "\n", ")", "\n", "\n", "data_loader_val", "=", "torch", ".", "utils", ".", "data", ".", "DataLoader", "(", "\n", "dataset_val", ",", "sampler", "=", "sampler_val", ",", "\n", "batch_size", "=", "int", "(", "2.", "*", "args", ".", "batch_size", ")", ",", "\n", "num_workers", "=", "args", ".", "num_workers", ",", "\n", "pin_memory", "=", "args", ".", "pin_mem", ",", "\n", "drop_last", "=", "False", "\n", ")", "\n", "\n", "print", "(", "f\"Creating model: {args.model}\"", ")", "\n", "model", "=", "create_model", "(", "\n", "args", ".", "model", ",", "\n", "pretrained", "=", "args", ".", "pretrained", ",", "\n", "num_classes", "=", "args", ".", "nb_classes", ",", "\n", "drop_rate", "=", "args", ".", "drop", ",", "\n", "drop_path_rate", "=", "args", ".", "drop_path", ",", "\n", "drop_block_rate", "=", "None", ",", "\n", ")", "\n", "\n", "if", "args", ".", "finetune", ":", "\n", "        ", "if", "args", ".", "finetune", ".", "startswith", "(", "'https'", ")", ":", "\n", "            ", "checkpoint", "=", "torch", ".", "hub", ".", "load_state_dict_from_url", "(", "\n", "args", ".", "finetune", ",", "map_location", "=", "'cpu'", ",", "check_hash", "=", "True", ")", "\n", "", "else", ":", "\n", "            ", "checkpoint", "=", "torch", ".", "load", "(", "args", ".", "finetune", ",", "map_location", "=", "'cpu'", ")", "\n", "\n", "", "checkpoint_model", "=", "checkpoint", "[", "'model'", "]", "\n", "state_dict", "=", "model", ".", "state_dict", "(", ")", "\n", "for", "k", "in", "[", "'head.weight'", ",", "'head.bias'", ",", "'head_dist.weight'", ",", "'head_dist.bias'", "]", ":", "\n", "            ", "if", "k", "in", "checkpoint_model", "and", "checkpoint_model", "[", "k", "]", ".", "shape", "!=", "state_dict", "[", "k", "]", ".", "shape", ":", "\n", "                ", "print", "(", "f\"Removing key {k} from pretrained checkpoint\"", ")", "\n", "del", "checkpoint_model", "[", "k", "]", "\n", "\n", "# interpolate position embedding", "\n", "", "", "pos_embed_checkpoint", "=", "checkpoint_model", "[", "'pos_embed'", "]", "\n", "embedding_size", "=", "pos_embed_checkpoint", ".", "shape", "[", "-", "1", "]", "\n", "num_patches", "=", "model", ".", "patch_embed", ".", "num_patches", "\n", "num_extra_tokens", "=", "model", ".", "pos_embed", ".", "shape", "[", "-", "2", "]", "-", "num_patches", "\n", "# height (== width) for the checkpoint position embedding", "\n", "orig_size", "=", "int", "(", "(", "pos_embed_checkpoint", ".", "shape", "[", "-", "2", "]", "-", "num_extra_tokens", ")", "**", "0.5", ")", "\n", "# height (== width) for the new position embedding", "\n", "new_size", "=", "int", "(", "num_patches", "**", "0.5", ")", "\n", "# class_token and dist_token are kept unchanged", "\n", "extra_tokens", "=", "pos_embed_checkpoint", "[", ":", ",", ":", "num_extra_tokens", "]", "\n", "# only the position tokens are interpolated", "\n", "pos_tokens", "=", "pos_embed_checkpoint", "[", ":", ",", "num_extra_tokens", ":", "]", "\n", "pos_tokens", "=", "pos_tokens", ".", "reshape", "(", "-", "1", ",", "orig_size", ",", "orig_size", ",", "embedding_size", ")", ".", "permute", "(", "0", ",", "3", ",", "1", ",", "2", ")", "\n", "pos_tokens", "=", "torch", ".", "nn", ".", "functional", ".", "interpolate", "(", "\n", "pos_tokens", ",", "size", "=", "(", "new_size", ",", "new_size", ")", ",", "mode", "=", "'bicubic'", ",", "align_corners", "=", "False", ")", "\n", "pos_tokens", "=", "pos_tokens", ".", "permute", "(", "0", ",", "2", ",", "3", ",", "1", ")", ".", "flatten", "(", "1", ",", "2", ")", "\n", "new_pos_embed", "=", "torch", ".", "cat", "(", "(", "extra_tokens", ",", "pos_tokens", ")", ",", "dim", "=", "1", ")", "\n", "checkpoint_model", "[", "'pos_embed'", "]", "=", "new_pos_embed", "\n", "\n", "model", ".", "load_state_dict", "(", "checkpoint_model", ",", "strict", "=", "False", ")", "\n", "\n", "", "model", ".", "to", "(", "device", ")", "\n", "\n", "model_without_ddp", "=", "model", "\n", "if", "args", ".", "distributed", ":", "\n", "        ", "model", "=", "torch", ".", "nn", ".", "parallel", ".", "DistributedDataParallel", "(", "model", ",", "device_ids", "=", "[", "args", ".", "gpu", "]", ")", "\n", "model_without_ddp", "=", "model", ".", "module", "\n", "", "n_parameters", "=", "sum", "(", "p", ".", "numel", "(", ")", "for", "p", "in", "model", ".", "parameters", "(", ")", "if", "p", ".", "requires_grad", ")", "\n", "print", "(", "'number of params:'", ",", "n_parameters", ")", "\n", "\n", "optimizer", ",", "aux_optimizer", "=", "configure_optimizers", "(", "args", ",", "model_without_ddp", ")", "\n", "\n", "lr_scheduler", ",", "_", "=", "create_scheduler", "(", "args", ",", "optimizer", ")", "\n", "\n", "if", "args", ".", "smoothing", ":", "\n", "        ", "criterion", "=", "LabelSmoothingCrossEntropy", "(", "smoothing", "=", "args", ".", "smoothing", ")", "\n", "", "else", ":", "\n", "        ", "criterion", "=", "torch", ".", "nn", ".", "CrossEntropyLoss", "(", ")", "\n", "\n", "", "alpha", "=", "alpha_beta", "[", "args", ".", "quality", "]", "[", "0", "]", "\n", "beta", "=", "alpha_beta", "[", "args", ".", "quality", "]", "[", "1", "]", "\n", "criterion", "=", "JointLoss", "(", "criterion", ",", "alpha", "=", "alpha", ",", "beta", "=", "beta", ")", "\n", "\n", "output_dir", "=", "Path", "(", "args", ".", "output_dir", ")", "\n", "if", "args", ".", "resume", ":", "\n", "        ", "if", "args", ".", "resume", ".", "startswith", "(", "'https'", ")", ":", "\n", "            ", "checkpoint", "=", "torch", ".", "hub", ".", "load_state_dict_from_url", "(", "\n", "args", ".", "resume", ",", "map_location", "=", "'cpu'", ",", "check_hash", "=", "True", ")", "\n", "", "else", ":", "\n", "            ", "checkpoint", "=", "torch", ".", "load", "(", "args", ".", "resume", ",", "map_location", "=", "'cpu'", ")", "\n", "", "model_without_ddp", ".", "load_state_dict", "(", "checkpoint", "[", "'model'", "]", ")", "\n", "if", "not", "args", ".", "eval", "and", "'optimizer'", "in", "checkpoint", "and", "'lr_scheduler'", "in", "checkpoint", "and", "'epoch'", "in", "checkpoint", ":", "\n", "            ", "optimizer", ".", "load_state_dict", "(", "checkpoint", "[", "'optimizer'", "]", ")", "\n", "aux_optimizer", ".", "load_state_dict", "(", "checkpoint", "[", "'aux_optimizer'", "]", ")", "\n", "lr_scheduler", ".", "load_state_dict", "(", "checkpoint", "[", "'lr_scheduler'", "]", ")", "\n", "args", ".", "start_epoch", "=", "checkpoint", "[", "'epoch'", "]", "+", "1", "\n", "\n", "", "", "if", "args", ".", "eval", ":", "\n", "        ", "test_stats", "=", "evaluate_real", "(", "data_loader_val", ",", "model", ",", "device", ",", "output_dir", ")", "\n", "print", "(", "f\"Accuracy of the network on the {len(dataset_val)} test images: {test_stats['acc1']:.1f}%\"", ")", "\n", "return", "\n", "\n", "\n", "", "print", "(", "f\"Start training for {args.epochs} epochs\"", ")", "\n", "start_time", "=", "time", ".", "time", "(", ")", "\n", "max_accuracy", "=", "0.0", "\n", "for", "epoch", "in", "range", "(", "args", ".", "start_epoch", ",", "args", ".", "epochs", ")", ":", "\n", "        ", "if", "args", ".", "distributed", ":", "\n", "            ", "data_loader_train", ".", "sampler", ".", "set_epoch", "(", "epoch", ")", "\n", "\n", "", "train_stats", "=", "train_one_epoch", "(", "\n", "model", ",", "criterion", ",", "data_loader_train", ",", "\n", "optimizer", ",", "aux_optimizer", ",", "device", ",", "epoch", ",", "\n", "args", ".", "clip_grad", ",", "set_training_mode", "=", "args", ".", "finetune", "==", "''", "# keep in eval mode during finetuning", "\n", ")", "\n", "\n", "lr_scheduler", ".", "step", "(", "epoch", ")", "\n", "if", "args", ".", "output_dir", ":", "\n", "            ", "checkpoint_paths", "=", "[", "output_dir", "/", "'checkpoint.pth'", "]", "\n", "for", "checkpoint_path", "in", "checkpoint_paths", ":", "\n", "                ", "utils", ".", "save_on_master", "(", "{", "\n", "'model'", ":", "model_without_ddp", ".", "state_dict", "(", ")", ",", "\n", "'optimizer'", ":", "optimizer", ".", "state_dict", "(", ")", ",", "\n", "'aux_optimizer'", ":", "aux_optimizer", ".", "state_dict", "(", ")", ",", "\n", "'lr_scheduler'", ":", "lr_scheduler", ".", "state_dict", "(", ")", ",", "\n", "'epoch'", ":", "epoch", ",", "\n", "'args'", ":", "args", ",", "\n", "}", ",", "checkpoint_path", ")", "\n", "\n", "", "", "test_stats", "=", "evaluate", "(", "data_loader_val", ",", "model", ",", "device", ",", "output_dir", ")", "\n", "print", "(", "f\"Accuracy of the network on the {len(dataset_val)} test images: {test_stats['acc1']:.1f}%\"", ")", "\n", "max_accuracy", "=", "max", "(", "max_accuracy", ",", "test_stats", "[", "\"acc1\"", "]", ")", "\n", "print", "(", "f'Max accuracy: {max_accuracy:.2f}%'", ")", "\n", "\n", "log_stats", "=", "{", "**", "{", "f'train_{k}'", ":", "v", "for", "k", ",", "v", "in", "train_stats", ".", "items", "(", ")", "}", ",", "\n", "**", "{", "f'test_{k}'", ":", "v", "for", "k", ",", "v", "in", "test_stats", ".", "items", "(", ")", "}", ",", "\n", "'epoch'", ":", "epoch", ",", "\n", "'n_parameters'", ":", "n_parameters", "}", "\n", "\n", "if", "args", ".", "output_dir", "and", "utils", ".", "is_main_process", "(", ")", ":", "\n", "            ", "with", "(", "output_dir", "/", "\"log.txt\"", ")", ".", "open", "(", "\"a\"", ")", "as", "f", ":", "\n", "                ", "f", ".", "write", "(", "json", ".", "dumps", "(", "log_stats", ")", "+", "\"\\n\"", ")", "\n", "\n", "", "", "", "total_time", "=", "time", ".", "time", "(", ")", "-", "start_time", "\n", "total_time_str", "=", "str", "(", "datetime", ".", "timedelta", "(", "seconds", "=", "int", "(", "total_time", ")", ")", ")", "\n", "print", "(", "'Training time {}'", ".", "format", "(", "total_time_str", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.bychao100_towards-image-compression-and-analysis-with-transformers.full_model.model.Img2Embed.__init__": [[66, 84], ["torch.Module.__init__", "timm.models.layers.to_2tuple", "timm.models.layers.to_2tuple", "torch.Sequential", "torch.Sequential", "model.conv", "torch.LeakyReLU", "torch.LeakyReLU", "model.conv", "torch.LeakyReLU", "torch.LeakyReLU", "model.conv", "torch.LeakyReLU", "torch.LeakyReLU", "model.conv"], "methods", ["home.repos.pwc.inspect_result.bychao100_towards-image-compression-and-analysis-with-transformers.pretrained_model.datasets.INatDataset.__init__", "home.repos.pwc.inspect_result.bychao100_towards-image-compression-and-analysis-with-transformers.pretrained_model.model.conv", "home.repos.pwc.inspect_result.bychao100_towards-image-compression-and-analysis-with-transformers.pretrained_model.model.conv", "home.repos.pwc.inspect_result.bychao100_towards-image-compression-and-analysis-with-transformers.pretrained_model.model.conv", "home.repos.pwc.inspect_result.bychao100_towards-image-compression-and-analysis-with-transformers.pretrained_model.model.conv"], ["def", "__init__", "(", "self", ",", "img_size", "=", "224", ",", "patch_size", "=", "16", ",", "in_chans", "=", "3", ",", "embed_dim", "=", "768", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "img_size", "=", "to_2tuple", "(", "img_size", ")", "\n", "patch_size", "=", "to_2tuple", "(", "patch_size", ")", "\n", "num_patches", "=", "(", "img_size", "[", "1", "]", "//", "patch_size", "[", "1", "]", ")", "*", "(", "img_size", "[", "0", "]", "//", "patch_size", "[", "0", "]", ")", "\n", "self", ".", "img_size", "=", "img_size", "\n", "self", ".", "patch_size", "=", "patch_size", "\n", "self", ".", "num_patches", "=", "num_patches", "\n", "\n", "middle_chans", "=", "[", "128", ",", "128", ",", "128", "]", "\n", "self", ".", "proj", "=", "nn", ".", "Sequential", "(", "\n", "conv", "(", "in_chans", ",", "middle_chans", "[", "0", "]", ")", ",", "\n", "nn", ".", "LeakyReLU", "(", "inplace", "=", "True", ")", ",", "\n", "conv", "(", "middle_chans", "[", "0", "]", ",", "middle_chans", "[", "1", "]", ")", ",", "\n", "nn", ".", "LeakyReLU", "(", "inplace", "=", "True", ")", ",", "\n", "conv", "(", "middle_chans", "[", "1", "]", ",", "middle_chans", "[", "2", "]", ")", ",", "\n", "nn", ".", "LeakyReLU", "(", "inplace", "=", "True", ")", ",", "\n", "conv", "(", "middle_chans", "[", "2", "]", ",", "embed_dim", ")", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.bychao100_towards-image-compression-and-analysis-with-transformers.full_model.model.Img2Embed.forward": [[86, 93], ["model.Img2Embed.proj().flatten().transpose", "model.Img2Embed.proj().flatten", "model.Img2Embed.proj"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "B", ",", "C", ",", "H", ",", "W", "=", "x", ".", "shape", "\n", "# FIXME look at relaxing size constraints", "\n", "assert", "H", "==", "self", ".", "img_size", "[", "0", "]", "and", "W", "==", "self", ".", "img_size", "[", "1", "]", ",", "f\"Input image size ({H}*{W}) doesn't match model ({self.img_size[0]}*{self.img_size[1]}).\"", "\n", "x", "=", "self", ".", "proj", "(", "x", ")", ".", "flatten", "(", "2", ")", ".", "transpose", "(", "1", ",", "2", ")", "\n", "return", "x", "\n", "\n"]], "home.repos.pwc.inspect_result.bychao100_towards-image-compression-and-analysis-with-transformers.full_model.model.Embed2Img.__init__": [[98, 118], ["torch.Module.__init__", "timm.models.layers.to_2tuple", "timm.models.layers.to_2tuple", "torch.Sequential", "torch.Sequential", "model.deconv", "torch.LeakyReLU", "torch.LeakyReLU", "model.deconv", "torch.LeakyReLU", "torch.LeakyReLU", "model.deconv", "torch.LeakyReLU", "torch.LeakyReLU", "model.deconv"], "methods", ["home.repos.pwc.inspect_result.bychao100_towards-image-compression-and-analysis-with-transformers.pretrained_model.datasets.INatDataset.__init__", "home.repos.pwc.inspect_result.bychao100_towards-image-compression-and-analysis-with-transformers.pretrained_model.model.deconv", "home.repos.pwc.inspect_result.bychao100_towards-image-compression-and-analysis-with-transformers.pretrained_model.model.deconv", "home.repos.pwc.inspect_result.bychao100_towards-image-compression-and-analysis-with-transformers.pretrained_model.model.deconv", "home.repos.pwc.inspect_result.bychao100_towards-image-compression-and-analysis-with-transformers.pretrained_model.model.deconv"], ["def", "__init__", "(", "self", ",", "img_size", "=", "224", ",", "patch_size", "=", "16", ",", "out_chans", "=", "3", ",", "embed_dim", "=", "768", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "img_size", "=", "to_2tuple", "(", "img_size", ")", "\n", "patch_size", "=", "to_2tuple", "(", "patch_size", ")", "\n", "embed_size", "=", "(", "img_size", "[", "1", "]", "//", "patch_size", "[", "1", "]", ",", "img_size", "[", "0", "]", "//", "patch_size", "[", "0", "]", ")", "\n", "num_patches", "=", "embed_size", "[", "0", "]", "*", "embed_size", "[", "1", "]", "\n", "self", ".", "img_size", "=", "img_size", "\n", "self", ".", "patch_size", "=", "patch_size", "\n", "self", ".", "embed_size", "=", "embed_size", "\n", "self", ".", "num_patches", "=", "num_patches", "\n", "\n", "middle_chans", "=", "[", "128", ",", "128", ",", "128", "]", "\n", "self", ".", "proj", "=", "nn", ".", "Sequential", "(", "\n", "deconv", "(", "embed_dim", ",", "middle_chans", "[", "0", "]", ")", ",", "\n", "nn", ".", "LeakyReLU", "(", "inplace", "=", "True", ")", ",", "\n", "deconv", "(", "middle_chans", "[", "0", "]", ",", "middle_chans", "[", "1", "]", ")", ",", "\n", "nn", ".", "LeakyReLU", "(", "inplace", "=", "True", ")", ",", "\n", "deconv", "(", "middle_chans", "[", "1", "]", ",", "middle_chans", "[", "2", "]", ")", ",", "\n", "nn", ".", "LeakyReLU", "(", "inplace", "=", "True", ")", ",", "\n", "deconv", "(", "middle_chans", "[", "2", "]", ",", "out_chans", ")", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.bychao100_towards-image-compression-and-analysis-with-transformers.full_model.model.Embed2Img.forward": [[120, 126], ["model.Embed2Img.proj", "model.Embed2Img.transpose().reshape", "model.Embed2Img.transpose"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "B", ",", "HW", ",", "C", "=", "x", ".", "shape", "\n", "assert", "HW", "==", "self", ".", "num_patches", ",", "f\"Input embeding size ({HW}) doesn't match patches size ({self.num_patches}).\"", "\n", "x", "=", "self", ".", "proj", "(", "x", ".", "transpose", "(", "1", ",", "2", ")", ".", "reshape", "(", "B", ",", "C", ",", "self", ".", "embed_size", "[", "0", "]", ",", "self", ".", "embed_size", "[", "1", "]", ")", ")", "\n", "return", "x", "\n", "\n"]], "home.repos.pwc.inspect_result.bychao100_towards-image-compression-and-analysis-with-transformers.full_model.model.Mlp.__init__": [[129, 137], ["torch.Module.__init__", "torch.Linear", "torch.Linear", "act_layer", "torch.Linear", "torch.Linear", "torch.Dropout", "torch.Dropout"], "methods", ["home.repos.pwc.inspect_result.bychao100_towards-image-compression-and-analysis-with-transformers.pretrained_model.datasets.INatDataset.__init__"], ["    ", "def", "__init__", "(", "self", ",", "in_features", ",", "hidden_features", "=", "None", ",", "out_features", "=", "None", ",", "act_layer", "=", "nn", ".", "GELU", ",", "drop", "=", "0.", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "out_features", "=", "out_features", "or", "in_features", "\n", "hidden_features", "=", "hidden_features", "or", "in_features", "\n", "self", ".", "fc1", "=", "nn", ".", "Linear", "(", "in_features", ",", "hidden_features", ")", "\n", "self", ".", "act", "=", "act_layer", "(", ")", "\n", "self", ".", "fc2", "=", "nn", ".", "Linear", "(", "hidden_features", ",", "out_features", ")", "\n", "self", ".", "drop", "=", "nn", ".", "Dropout", "(", "drop", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.bychao100_towards-image-compression-and-analysis-with-transformers.full_model.model.Mlp.forward": [[138, 145], ["model.Mlp.fc1", "model.Mlp.act", "model.Mlp.drop", "model.Mlp.fc2", "model.Mlp.drop"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "x", "=", "self", ".", "fc1", "(", "x", ")", "\n", "x", "=", "self", ".", "act", "(", "x", ")", "\n", "x", "=", "self", ".", "drop", "(", "x", ")", "\n", "x", "=", "self", ".", "fc2", "(", "x", ")", "\n", "x", "=", "self", ".", "drop", "(", "x", ")", "\n", "return", "x", "\n", "\n"]], "home.repos.pwc.inspect_result.bychao100_towards-image-compression-and-analysis-with-transformers.full_model.model.Attention.__init__": [[148, 159], ["torch.Module.__init__", "torch.Linear", "torch.Linear", "torch.Dropout", "torch.Dropout", "torch.Linear", "torch.Linear", "torch.Dropout", "torch.Dropout"], "methods", ["home.repos.pwc.inspect_result.bychao100_towards-image-compression-and-analysis-with-transformers.pretrained_model.datasets.INatDataset.__init__"], ["    ", "def", "__init__", "(", "self", ",", "dim", ",", "num_heads", "=", "8", ",", "qkv_bias", "=", "False", ",", "qk_scale", "=", "None", ",", "attn_drop", "=", "0.", ",", "proj_drop", "=", "0.", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "num_heads", "=", "num_heads", "\n", "head_dim", "=", "dim", "//", "num_heads", "\n", "# NOTE scale factor was wrong in my original version, can set manually to be compat with prev weights", "\n", "self", ".", "scale", "=", "qk_scale", "or", "head_dim", "**", "-", "0.5", "\n", "\n", "self", ".", "qkv", "=", "nn", ".", "Linear", "(", "dim", ",", "dim", "*", "3", ",", "bias", "=", "qkv_bias", ")", "\n", "self", ".", "attn_drop", "=", "nn", ".", "Dropout", "(", "attn_drop", ")", "\n", "self", ".", "proj", "=", "nn", ".", "Linear", "(", "dim", ",", "dim", ")", "\n", "self", ".", "proj_drop", "=", "nn", ".", "Dropout", "(", "proj_drop", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.bychao100_towards-image-compression-and-analysis-with-transformers.full_model.model.Attention.forward": [[160, 173], ["model.Attention.qkv().reshape().permute", "model.Attention.softmax", "model.Attention.attn_drop", "model.Attention.proj", "model.Attention.proj_drop", "model.Attention.qkv().reshape", "k.transpose", "model.Attention.qkv"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "B", ",", "N", ",", "C", "=", "x", ".", "shape", "\n", "qkv", "=", "self", ".", "qkv", "(", "x", ")", ".", "reshape", "(", "B", ",", "N", ",", "3", ",", "self", ".", "num_heads", ",", "C", "//", "self", ".", "num_heads", ")", ".", "permute", "(", "2", ",", "0", ",", "3", ",", "1", ",", "4", ")", "\n", "q", ",", "k", ",", "v", "=", "qkv", "[", "0", "]", ",", "qkv", "[", "1", "]", ",", "qkv", "[", "2", "]", "# make torchscript happy (cannot use tensor as tuple)", "\n", "\n", "attn", "=", "(", "q", "@", "k", ".", "transpose", "(", "-", "2", ",", "-", "1", ")", ")", "*", "self", ".", "scale", "\n", "attn", "=", "attn", ".", "softmax", "(", "dim", "=", "-", "1", ")", "\n", "attn", "=", "self", ".", "attn_drop", "(", "attn", ")", "\n", "\n", "x", "=", "(", "attn", "@", "v", ")", ".", "transpose", "(", "1", ",", "2", ")", ".", "reshape", "(", "B", ",", "N", ",", "C", ")", "\n", "x", "=", "self", ".", "proj", "(", "x", ")", "\n", "x", "=", "self", ".", "proj_drop", "(", "x", ")", "\n", "return", "x", "\n", "\n"]], "home.repos.pwc.inspect_result.bychao100_towards-image-compression-and-analysis-with-transformers.full_model.model.Block.__init__": [[177, 188], ["torch.Module.__init__", "norm_layer", "model.Attention", "norm_layer", "int", "model.Mlp", "timm.models.layers.DropPath", "torch.Identity", "torch.Identity"], "methods", ["home.repos.pwc.inspect_result.bychao100_towards-image-compression-and-analysis-with-transformers.pretrained_model.datasets.INatDataset.__init__"], ["    ", "def", "__init__", "(", "self", ",", "dim", ",", "num_heads", ",", "mlp_ratio", "=", "4.", ",", "qkv_bias", "=", "False", ",", "qk_scale", "=", "None", ",", "drop", "=", "0.", ",", "attn_drop", "=", "0.", ",", "\n", "drop_path", "=", "0.", ",", "act_layer", "=", "nn", ".", "GELU", ",", "norm_layer", "=", "nn", ".", "LayerNorm", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "norm1", "=", "norm_layer", "(", "dim", ")", "\n", "self", ".", "attn", "=", "Attention", "(", "\n", "dim", ",", "num_heads", "=", "num_heads", ",", "qkv_bias", "=", "qkv_bias", ",", "qk_scale", "=", "qk_scale", ",", "attn_drop", "=", "attn_drop", ",", "proj_drop", "=", "drop", ")", "\n", "# NOTE: drop path for stochastic depth, we shall see if this is better than dropout here", "\n", "self", ".", "drop_path", "=", "DropPath", "(", "drop_path", ")", "if", "drop_path", ">", "0.", "else", "nn", ".", "Identity", "(", ")", "\n", "self", ".", "norm2", "=", "norm_layer", "(", "dim", ")", "\n", "mlp_hidden_dim", "=", "int", "(", "dim", "*", "mlp_ratio", ")", "\n", "self", ".", "mlp", "=", "Mlp", "(", "in_features", "=", "dim", ",", "hidden_features", "=", "mlp_hidden_dim", ",", "act_layer", "=", "act_layer", ",", "drop", "=", "drop", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.bychao100_towards-image-compression-and-analysis-with-transformers.full_model.model.Block.forward": [[189, 193], ["model.Block.drop_path", "model.Block.drop_path", "model.Block.attn", "model.Block.mlp", "model.Block.norm1", "model.Block.norm2"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "x", "=", "x", "+", "self", ".", "drop_path", "(", "self", ".", "attn", "(", "self", ".", "norm1", "(", "x", ")", ")", ")", "\n", "x", "=", "x", "+", "self", ".", "drop_path", "(", "self", ".", "mlp", "(", "self", ".", "norm2", "(", "x", ")", ")", ")", "\n", "return", "x", "\n", "\n"]], "home.repos.pwc.inspect_result.bychao100_towards-image-compression-and-analysis-with-transformers.full_model.model.JCCTransformer.__init__": [[198, 257], ["torch.Module.__init__", "model.Img2Embed", "torch.Linear", "torch.Linear", "torch.Parameter", "torch.Parameter", "torch.Parameter", "torch.Parameter", "torch.Dropout", "torch.Dropout", "torch.ModuleList", "torch.ModuleList", "norm_layer", "model.Embed2Img", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "compressai.entropy_models.EntropyBottleneck", "torch.Sequential", "torch.Sequential", "torch.Sequential", "torch.Sequential", "compressai.entropy_models.GaussianConditional", "timm.models.layers.trunc_normal_", "timm.models.layers.trunc_normal_", "model.JCCTransformer.apply", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "x.item", "torch.Linear", "torch.Linear", "torch.Identity", "torch.Identity", "model.conv", "torch.LeakyReLU", "torch.LeakyReLU", "model.conv", "torch.LeakyReLU", "torch.LeakyReLU", "model.conv", "model.conv", "torch.LeakyReLU", "torch.LeakyReLU", "model.deconv", "torch.LeakyReLU", "torch.LeakyReLU", "model.conv", "torch.linspace", "torch.linspace", "torch.linspace", "torch.linspace", "model.Block", "range"], "methods", ["home.repos.pwc.inspect_result.bychao100_towards-image-compression-and-analysis-with-transformers.pretrained_model.datasets.INatDataset.__init__", "home.repos.pwc.inspect_result.bychao100_towards-image-compression-and-analysis-with-transformers.pretrained_model.model.conv", "home.repos.pwc.inspect_result.bychao100_towards-image-compression-and-analysis-with-transformers.pretrained_model.model.conv", "home.repos.pwc.inspect_result.bychao100_towards-image-compression-and-analysis-with-transformers.pretrained_model.model.conv", "home.repos.pwc.inspect_result.bychao100_towards-image-compression-and-analysis-with-transformers.pretrained_model.model.conv", "home.repos.pwc.inspect_result.bychao100_towards-image-compression-and-analysis-with-transformers.pretrained_model.model.deconv", "home.repos.pwc.inspect_result.bychao100_towards-image-compression-and-analysis-with-transformers.pretrained_model.model.conv"], ["def", "__init__", "(", "self", ",", "img_size", "=", "224", ",", "patch_size", "=", "16", ",", "in_chans", "=", "3", ",", "num_classes", "=", "1000", ",", "embed_dim", "=", "768", ",", "depth", "=", "12", ",", "\n", "num_heads", "=", "12", ",", "mlp_ratio", "=", "4.", ",", "qkv_bias", "=", "False", ",", "qk_scale", "=", "None", ",", "drop_rate", "=", "0.", ",", "attn_drop_rate", "=", "0.", ",", "\n", "drop_path_rate", "=", "0.", ",", "norm_layer", "=", "nn", ".", "LayerNorm", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "num_classes", "=", "num_classes", "\n", "self", ".", "num_features", "=", "self", ".", "embed_dim", "=", "embed_dim", "# num_features for consistency with other models", "\n", "self", ".", "depth", "=", "depth", "\n", "\n", "self", ".", "patch_embed", "=", "Img2Embed", "(", "img_size", "=", "img_size", ",", "patch_size", "=", "patch_size", ",", "in_chans", "=", "in_chans", ",", "embed_dim", "=", "192", ")", "\n", "self", ".", "chans_embed", "=", "nn", ".", "Linear", "(", "192", ",", "embed_dim", ")", "\n", "num_patches", "=", "self", ".", "patch_embed", ".", "num_patches", "\n", "\n", "self", ".", "cls_token", "=", "nn", ".", "Parameter", "(", "torch", ".", "zeros", "(", "1", ",", "1", ",", "embed_dim", ")", ")", "\n", "self", ".", "pos_embed", "=", "nn", ".", "Parameter", "(", "torch", ".", "zeros", "(", "1", ",", "num_patches", "+", "1", ",", "embed_dim", ")", ")", "\n", "self", ".", "pos_drop", "=", "nn", ".", "Dropout", "(", "p", "=", "drop_rate", ")", "\n", "\n", "dpr", "=", "[", "x", ".", "item", "(", ")", "for", "x", "in", "torch", ".", "linspace", "(", "0", ",", "drop_path_rate", ",", "depth", ")", "]", "# stochastic depth decay rule", "\n", "self", ".", "blocks", "=", "nn", ".", "ModuleList", "(", "[", "\n", "Block", "(", "\n", "dim", "=", "embed_dim", ",", "num_heads", "=", "num_heads", ",", "mlp_ratio", "=", "mlp_ratio", ",", "qkv_bias", "=", "qkv_bias", ",", "qk_scale", "=", "qk_scale", ",", "\n", "drop", "=", "drop_rate", ",", "attn_drop", "=", "attn_drop_rate", ",", "drop_path", "=", "dpr", "[", "i", "]", ",", "norm_layer", "=", "norm_layer", ")", "\n", "for", "i", "in", "range", "(", "depth", ")", "]", ")", "\n", "self", ".", "norm", "=", "norm_layer", "(", "embed_dim", ")", "\n", "\n", "# Classifier head", "\n", "self", ".", "head", "=", "nn", ".", "Linear", "(", "embed_dim", ",", "num_classes", ")", "if", "num_classes", ">", "0", "else", "nn", ".", "Identity", "(", ")", "\n", "\n", "# Reconstruction head", "\n", "self", ".", "head_rec", "=", "Embed2Img", "(", "img_size", "=", "img_size", ",", "patch_size", "=", "patch_size", ",", "out_chans", "=", "in_chans", ",", "embed_dim", "=", "embed_dim", ")", "\n", "\n", "# fusion", "\n", "self", ".", "fusion0", "=", "nn", ".", "Linear", "(", "embed_dim", ",", "embed_dim", "//", "4", ")", "\n", "self", ".", "fusion1", "=", "nn", ".", "Linear", "(", "embed_dim", ",", "embed_dim", "//", "4", ")", "\n", "self", ".", "fusion2", "=", "nn", ".", "Linear", "(", "embed_dim", ",", "embed_dim", "//", "4", ")", "\n", "self", ".", "fusion3", "=", "nn", ".", "Linear", "(", "embed_dim", ",", "embed_dim", "//", "4", ")", "\n", "self", ".", "fusion", "=", "nn", ".", "Linear", "(", "embed_dim", ",", "embed_dim", ")", "\n", "\n", "# Compression model", "\n", "hyper_dim", "=", "128", "\n", "self", ".", "entropy_bottleneck", "=", "EntropyBottleneck", "(", "hyper_dim", ")", "\n", "self", ".", "h_a", "=", "nn", ".", "Sequential", "(", "\n", "conv", "(", "192", ",", "hyper_dim", ",", "stride", "=", "1", ",", "kernel_size", "=", "3", ")", ",", "\n", "nn", ".", "LeakyReLU", "(", "inplace", "=", "True", ")", ",", "\n", "conv", "(", "hyper_dim", ",", "hyper_dim", ")", ",", "\n", "nn", ".", "LeakyReLU", "(", "inplace", "=", "True", ")", ",", "\n", "conv", "(", "hyper_dim", ",", "hyper_dim", ",", "stride", "=", "1", ")", ",", "\n", ")", "\n", "self", ".", "h_s", "=", "nn", ".", "Sequential", "(", "\n", "conv", "(", "hyper_dim", ",", "192", ",", "stride", "=", "1", ")", ",", "\n", "nn", ".", "LeakyReLU", "(", "inplace", "=", "True", ")", ",", "\n", "deconv", "(", "192", ",", "192", "*", "3", "//", "2", ")", ",", "\n", "nn", ".", "LeakyReLU", "(", "inplace", "=", "True", ")", ",", "\n", "conv", "(", "192", "*", "3", "//", "2", ",", "192", "*", "2", ",", "stride", "=", "1", ",", "kernel_size", "=", "3", ")", ",", "\n", ")", "\n", "self", ".", "gaussian_conditional", "=", "GaussianConditional", "(", "None", ")", "\n", "\n", "trunc_normal_", "(", "self", ".", "pos_embed", ",", "std", "=", ".02", ")", "\n", "trunc_normal_", "(", "self", ".", "cls_token", ",", "std", "=", ".02", ")", "\n", "self", ".", "apply", "(", "self", ".", "_init_weights", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.bychao100_towards-image-compression-and-analysis-with-transformers.full_model.model.JCCTransformer.aux_loss": [[258, 266], ["sum", "m.loss", "model.JCCTransformer.modules", "isinstance"], "methods", ["None"], ["", "def", "aux_loss", "(", "self", ")", ":", "\n", "        ", "\"\"\"Return the aggregated loss over the auxiliary entropy bottleneck\n        module(s).\n        \"\"\"", "\n", "aux_loss", "=", "sum", "(", "\n", "m", ".", "loss", "(", ")", "for", "m", "in", "self", ".", "modules", "(", ")", "if", "isinstance", "(", "m", ",", "EntropyBottleneck", ")", "\n", ")", "\n", "return", "aux_loss", "\n", "\n"]], "home.repos.pwc.inspect_result.bychao100_towards-image-compression-and-analysis-with-transformers.full_model.model.JCCTransformer._update_entropybottleneck": [[267, 287], ["model.JCCTransformer.children", "m.update", "isinstance"], "methods", ["home.repos.pwc.inspect_result.bychao100_towards-image-compression-and-analysis-with-transformers.pretrained_model.utils.MetricLogger.update"], ["", "def", "_update_entropybottleneck", "(", "self", ",", "force", "=", "False", ")", ":", "\n", "        ", "\"\"\"Updates the entropy bottleneck(s) CDF values.\n\n        Needs to be called once after training to be able to later perform the\n        evaluation with an actual entropy coder.\n\n        Args:\n            force (bool): overwrite previous values (default: False)\n\n        Returns:\n            updated (bool): True if one of the EntropyBottlenecks was updated.\n\n        \"\"\"", "\n", "updated", "=", "False", "\n", "for", "m", "in", "self", ".", "children", "(", ")", ":", "\n", "            ", "if", "not", "isinstance", "(", "m", ",", "EntropyBottleneck", ")", ":", "\n", "                ", "continue", "\n", "", "rv", "=", "m", ".", "update", "(", "force", "=", "force", ")", "\n", "updated", "|=", "rv", "\n", "", "return", "updated", "\n", "\n"]], "home.repos.pwc.inspect_result.bychao100_towards-image-compression-and-analysis-with-transformers.full_model.model.JCCTransformer.update": [[288, 294], ["model.JCCTransformer.gaussian_conditional.update_scale_table", "model.JCCTransformer._update_entropybottleneck", "model.get_scale_table"], "methods", ["home.repos.pwc.inspect_result.bychao100_towards-image-compression-and-analysis-with-transformers.full_model.model.JCCTransformer._update_entropybottleneck", "home.repos.pwc.inspect_result.bychao100_towards-image-compression-and-analysis-with-transformers.full_model.model.get_scale_table"], ["", "def", "update", "(", "self", ",", "scale_table", "=", "None", ",", "force", "=", "False", ")", ":", "\n", "        ", "if", "scale_table", "is", "None", ":", "\n", "            ", "scale_table", "=", "get_scale_table", "(", ")", "\n", "", "updated", "=", "self", ".", "gaussian_conditional", ".", "update_scale_table", "(", "scale_table", ",", "force", "=", "force", ")", "\n", "updated", "|=", "self", ".", "_update_entropybottleneck", "(", "force", "=", "force", ")", "\n", "return", "updated", "\n", "\n"]], "home.repos.pwc.inspect_result.bychao100_towards-image-compression-and-analysis-with-transformers.full_model.model.JCCTransformer.load_state_dict": [[295, 311], ["super().load_state_dict", "compressai.models.utils.update_registered_buffers", "compressai.models.utils.update_registered_buffers"], "methods", ["home.repos.pwc.inspect_result.bychao100_towards-image-compression-and-analysis-with-transformers.full_model.model.JCCTransformer.load_state_dict"], ["", "def", "load_state_dict", "(", "self", ",", "state_dict", ",", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "# Dynamically update the entropy bottleneck buffers related to the CDFs", "\n", "        ", "if", "not", "pretrained", ":", "\n", "            ", "update_registered_buffers", "(", "\n", "self", ".", "entropy_bottleneck", ",", "\n", "\"entropy_bottleneck\"", ",", "\n", "[", "\"_quantized_cdf\"", ",", "\"_offset\"", ",", "\"_cdf_length\"", "]", ",", "\n", "state_dict", ",", "\n", ")", "\n", "update_registered_buffers", "(", "\n", "self", ".", "gaussian_conditional", ",", "\n", "\"gaussian_conditional\"", ",", "\n", "[", "\"_quantized_cdf\"", ",", "\"_offset\"", ",", "\"_cdf_length\"", ",", "\"scale_table\"", "]", ",", "\n", "state_dict", ",", "\n", ")", "\n", "", "super", "(", ")", ".", "load_state_dict", "(", "state_dict", ",", "**", "kwargs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.bychao100_towards-image-compression-and-analysis-with-transformers.full_model.model.JCCTransformer._init_weights": [[312, 320], ["isinstance", "timm.models.layers.trunc_normal_", "isinstance", "isinstance", "torch.init.constant_", "torch.init.constant_", "torch.init.constant_", "torch.init.constant_", "torch.init.constant_", "torch.init.constant_"], "methods", ["None"], ["", "def", "_init_weights", "(", "self", ",", "m", ")", ":", "\n", "        ", "if", "isinstance", "(", "m", ",", "nn", ".", "Linear", ")", ":", "\n", "            ", "trunc_normal_", "(", "m", ".", "weight", ",", "std", "=", ".02", ")", "\n", "if", "isinstance", "(", "m", ",", "nn", ".", "Linear", ")", "and", "m", ".", "bias", "is", "not", "None", ":", "\n", "                ", "nn", ".", "init", ".", "constant_", "(", "m", ".", "bias", ",", "0", ")", "\n", "", "", "elif", "isinstance", "(", "m", ",", "nn", ".", "LayerNorm", ")", ":", "\n", "            ", "nn", ".", "init", ".", "constant_", "(", "m", ".", "bias", ",", "0", ")", "\n", "nn", ".", "init", ".", "constant_", "(", "m", ".", "weight", ",", "1.0", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.bychao100_towards-image-compression-and-analysis-with-transformers.full_model.model.JCCTransformer.no_weight_decay": [[321, 324], ["None"], "methods", ["None"], ["", "", "@", "torch", ".", "jit", ".", "ignore", "\n", "def", "no_weight_decay", "(", "self", ")", ":", "\n", "        ", "return", "{", "'pos_embed'", ",", "'cls_token'", "}", "\n", "\n"]], "home.repos.pwc.inspect_result.bychao100_towards-image-compression-and-analysis-with-transformers.full_model.model.JCCTransformer.get_classifier": [[325, 327], ["None"], "methods", ["None"], ["", "def", "get_classifier", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "head", "\n", "\n"]], "home.repos.pwc.inspect_result.bychao100_towards-image-compression-and-analysis-with-transformers.full_model.model.JCCTransformer.reset_classifier": [[328, 331], ["torch.Linear", "torch.Linear", "torch.Identity", "torch.Identity"], "methods", ["None"], ["", "def", "reset_classifier", "(", "self", ",", "num_classes", ",", "global_pool", "=", "''", ")", ":", "\n", "        ", "self", ".", "num_classes", "=", "num_classes", "\n", "self", ".", "head", "=", "nn", ".", "Linear", "(", "self", ".", "embed_dim", ",", "num_classes", ")", "if", "num_classes", ">", "0", "else", "nn", ".", "Identity", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.bychao100_towards-image-compression-and-analysis-with-transformers.full_model.model.JCCTransformer.forward_features": [[332, 378], ["model.JCCTransformer.patch_embed", "int", "model.JCCTransformer.transpose().reshape", "model.JCCTransformer.h_a", "model.JCCTransformer.entropy_bottleneck", "model.JCCTransformer.h_s", "model.JCCTransformer.chunk", "model.JCCTransformer.gaussian_conditional", "model.JCCTransformer.flatten().transpose", "model.JCCTransformer.chans_embed", "model.JCCTransformer.cls_token.expand", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "model.JCCTransformer.pos_drop", "model.JCCTransformer.norm", "model.JCCTransformer.fusion0", "model.JCCTransformer.fusion1", "model.JCCTransformer.fusion2", "model.JCCTransformer.fusion3", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "model.JCCTransformer.fusion", "model.JCCTransformer.transpose", "model.JCCTransformer.flatten"], "methods", ["None"], ["", "def", "forward_features", "(", "self", ",", "x", ")", ":", "\n", "        ", "x", "=", "self", ".", "patch_embed", "(", "x", ")", "\n", "B", ",", "N", ",", "C", "=", "x", ".", "shape", "\n", "H", "=", "W", "=", "int", "(", "N", "**", "0.5", ")", "\n", "\n", "# Bottleneck Compression", "\n", "y", "=", "x", ".", "transpose", "(", "1", ",", "2", ")", ".", "reshape", "(", "B", ",", "C", ",", "H", ",", "W", ")", "\n", "z", "=", "self", ".", "h_a", "(", "y", ")", "\n", "z_hat", ",", "z_likelihoods", "=", "self", ".", "entropy_bottleneck", "(", "z", ")", "\n", "gaussian_params", "=", "self", ".", "h_s", "(", "z_hat", ")", "\n", "scales_hat", ",", "means_hat", "=", "gaussian_params", ".", "chunk", "(", "2", ",", "1", ")", "\n", "y_hat", ",", "y_likelihoods", "=", "self", ".", "gaussian_conditional", "(", "y", ",", "scales_hat", ",", "means", "=", "means_hat", ")", "\n", "\n", "# Transformer", "\n", "y_hat", "=", "y_hat", ".", "flatten", "(", "2", ")", ".", "transpose", "(", "1", ",", "2", ")", "\n", "y_hat", "=", "self", ".", "chans_embed", "(", "y_hat", ")", "\n", "\n", "cls_tokens", "=", "self", ".", "cls_token", ".", "expand", "(", "B", ",", "-", "1", ",", "-", "1", ")", "# stole cls_tokens impl from Phil Wang, thanks", "\n", "y0", "=", "torch", ".", "cat", "(", "(", "cls_tokens", ",", "y_hat", ")", ",", "dim", "=", "1", ")", "\n", "y0", "=", "y0", "+", "self", ".", "pos_embed", "\n", "y0", "=", "self", ".", "pos_drop", "(", "y0", ")", "\n", "\n", "y1", "=", "self", ".", "blocks", "[", "0", "]", "(", "y0", ")", "\n", "y2", "=", "self", ".", "blocks", "[", "1", "]", "(", "y1", ")", "\n", "y3", "=", "self", ".", "blocks", "[", "2", "]", "(", "y2", ")", "\n", "y4", "=", "self", ".", "blocks", "[", "3", "]", "(", "y3", ")", "\n", "y5", "=", "self", ".", "blocks", "[", "4", "]", "(", "y4", ")", "\n", "y6", "=", "self", ".", "blocks", "[", "5", "]", "(", "y5", ")", "\n", "y7", "=", "self", ".", "blocks", "[", "6", "]", "(", "y6", ")", "\n", "y8", "=", "self", ".", "blocks", "[", "7", "]", "(", "y7", ")", "\n", "y9", "=", "self", ".", "blocks", "[", "8", "]", "(", "y8", ")", "\n", "y10", "=", "self", ".", "blocks", "[", "9", "]", "(", "y9", ")", "\n", "y11", "=", "self", ".", "blocks", "[", "10", "]", "(", "y10", ")", "\n", "y12", "=", "self", ".", "blocks", "[", "11", "]", "(", "y11", ")", "\n", "\n", "y_out", "=", "self", ".", "norm", "(", "y12", ")", "\n", "\n", "y0", "=", "self", ".", "fusion0", "(", "y_hat", ")", "\n", "y1", "=", "self", ".", "fusion1", "(", "y1", "[", ":", ",", "1", ":", "]", ")", "\n", "y2", "=", "self", ".", "fusion2", "(", "y2", "[", ":", ",", "1", ":", "]", ")", "\n", "y3", "=", "self", ".", "fusion3", "(", "y3", "[", ":", ",", "1", ":", "]", ")", "\n", "\n", "y_rec", "=", "torch", ".", "cat", "(", "(", "y0", ",", "y1", ",", "y2", ",", "y3", ")", ",", "dim", "=", "2", ")", "\n", "y_rec", "=", "self", ".", "fusion", "(", "y_rec", ")", "\n", "\n", "return", "(", "y_out", "[", ":", ",", "0", "]", ",", "y_rec", ",", "y_likelihoods", ",", "z_likelihoods", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.bychao100_towards-image-compression-and-analysis-with-transformers.full_model.model.JCCTransformer.forward": [[379, 384], ["model.JCCTransformer.forward_features", "model.JCCTransformer.head", "model.JCCTransformer.head_rec", "model.JCCTransformer.aux_loss"], "methods", ["home.repos.pwc.inspect_result.bychao100_towards-image-compression-and-analysis-with-transformers.pretrained_model.model.PretrainTransformer.forward_features", "home.repos.pwc.inspect_result.bychao100_towards-image-compression-and-analysis-with-transformers.full_model.model.JCCTransformer.aux_loss"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "y_cls", ",", "y_rec", ",", "y_likelihoods", ",", "z_likelihoods", "=", "self", ".", "forward_features", "(", "x", ")", "\n", "cls", "=", "self", ".", "head", "(", "y_cls", ")", "\n", "rec", "=", "self", ".", "head_rec", "(", "y_rec", ")", "\n", "return", "(", "cls", ",", "rec", ",", "y_likelihoods", ",", "z_likelihoods", ")", ",", "self", ".", "aux_loss", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.bychao100_towards-image-compression-and-analysis-with-transformers.full_model.model.JCCTransformer.compress": [[385, 402], ["model.JCCTransformer.patch_embed", "int", "model.JCCTransformer.transpose().reshape", "model.JCCTransformer.h_a", "model.JCCTransformer.entropy_bottleneck.compress", "model.JCCTransformer.entropy_bottleneck.decompress", "model.JCCTransformer.h_s", "model.JCCTransformer.chunk", "model.JCCTransformer.gaussian_conditional.build_indexes", "model.JCCTransformer.gaussian_conditional.compress", "model.JCCTransformer.transpose", "model.JCCTransformer.size", "model.JCCTransformer.size"], "methods", ["home.repos.pwc.inspect_result.bychao100_towards-image-compression-and-analysis-with-transformers.full_model.model.JCCTransformer.compress", "home.repos.pwc.inspect_result.bychao100_towards-image-compression-and-analysis-with-transformers.full_model.model.JCCTransformer.decompress", "home.repos.pwc.inspect_result.bychao100_towards-image-compression-and-analysis-with-transformers.full_model.model.JCCTransformer.compress"], ["", "def", "compress", "(", "self", ",", "x", ")", ":", "\n", "        ", "x", "=", "self", ".", "patch_embed", "(", "x", ")", "\n", "B", ",", "N", ",", "C", "=", "x", ".", "shape", "\n", "H", "=", "W", "=", "int", "(", "N", "**", "0.5", ")", "\n", "\n", "# Bottleneck Compression", "\n", "y", "=", "x", ".", "transpose", "(", "1", ",", "2", ")", ".", "reshape", "(", "B", ",", "C", ",", "H", ",", "W", ")", "\n", "z", "=", "self", ".", "h_a", "(", "y", ")", "\n", "z_strings", "=", "self", ".", "entropy_bottleneck", ".", "compress", "(", "z", ")", "\n", "z_hat", "=", "self", ".", "entropy_bottleneck", ".", "decompress", "(", "z_strings", ",", "z", ".", "size", "(", ")", "[", "-", "2", ":", "]", ")", "\n", "\n", "gaussian_params", "=", "self", ".", "h_s", "(", "z_hat", ")", "\n", "scales_hat", ",", "means_hat", "=", "gaussian_params", ".", "chunk", "(", "2", ",", "1", ")", "\n", "indexes", "=", "self", ".", "gaussian_conditional", ".", "build_indexes", "(", "scales_hat", ")", "\n", "y_strings", "=", "self", ".", "gaussian_conditional", ".", "compress", "(", "y", ",", "indexes", ",", "means", "=", "means_hat", ")", "\n", "\n", "return", "{", "\"strings\"", ":", "[", "y_strings", ",", "z_strings", "]", ",", "\"shape\"", ":", "z", ".", "size", "(", ")", "[", "-", "2", ":", "]", "}", "\n", "\n"]], "home.repos.pwc.inspect_result.bychao100_towards-image-compression-and-analysis-with-transformers.full_model.model.JCCTransformer.decompress": [[403, 450], ["model.JCCTransformer.entropy_bottleneck.decompress", "model.JCCTransformer.h_s", "model.JCCTransformer.chunk", "model.JCCTransformer.gaussian_conditional.build_indexes", "model.JCCTransformer.gaussian_conditional.decompress", "model.JCCTransformer.flatten().transpose", "model.JCCTransformer.chans_embed", "model.JCCTransformer.cls_token.expand", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "model.JCCTransformer.pos_drop", "model.JCCTransformer.norm", "model.JCCTransformer.fusion0", "model.JCCTransformer.fusion1", "model.JCCTransformer.fusion2", "model.JCCTransformer.fusion3", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "model.JCCTransformer.fusion", "model.JCCTransformer.head", "model.JCCTransformer.head_rec", "isinstance", "len", "model.JCCTransformer.flatten"], "methods", ["home.repos.pwc.inspect_result.bychao100_towards-image-compression-and-analysis-with-transformers.full_model.model.JCCTransformer.decompress", "home.repos.pwc.inspect_result.bychao100_towards-image-compression-and-analysis-with-transformers.full_model.model.JCCTransformer.decompress"], ["", "def", "decompress", "(", "self", ",", "strings", ",", "shape", ")", ":", "\n", "        ", "assert", "isinstance", "(", "strings", ",", "list", ")", "and", "len", "(", "strings", ")", "==", "2", "\n", "z_hat", "=", "self", ".", "entropy_bottleneck", ".", "decompress", "(", "strings", "[", "1", "]", ",", "shape", ")", "\n", "gaussian_params", "=", "self", ".", "h_s", "(", "z_hat", ")", "\n", "scales_hat", ",", "means_hat", "=", "gaussian_params", ".", "chunk", "(", "2", ",", "1", ")", "\n", "indexes", "=", "self", ".", "gaussian_conditional", ".", "build_indexes", "(", "scales_hat", ")", "\n", "y_hat", "=", "self", ".", "gaussian_conditional", ".", "decompress", "(", "\n", "strings", "[", "0", "]", ",", "indexes", ",", "means", "=", "means_hat", "\n", ")", "\n", "\n", "# Transformer", "\n", "y_hat", "=", "y_hat", ".", "flatten", "(", "2", ")", ".", "transpose", "(", "1", ",", "2", ")", "\n", "y_hat", "=", "self", ".", "chans_embed", "(", "y_hat", ")", "\n", "\n", "B", "=", "y_hat", ".", "shape", "[", "0", "]", "\n", "cls_tokens", "=", "self", ".", "cls_token", ".", "expand", "(", "B", ",", "-", "1", ",", "-", "1", ")", "# stole cls_tokens impl from Phil Wang, thanks", "\n", "y0", "=", "torch", ".", "cat", "(", "(", "cls_tokens", ",", "y_hat", ")", ",", "dim", "=", "1", ")", "\n", "y0", "=", "y0", "+", "self", ".", "pos_embed", "\n", "y0", "=", "self", ".", "pos_drop", "(", "y0", ")", "\n", "\n", "y1", "=", "self", ".", "blocks", "[", "0", "]", "(", "y0", ")", "\n", "y2", "=", "self", ".", "blocks", "[", "1", "]", "(", "y1", ")", "\n", "y3", "=", "self", ".", "blocks", "[", "2", "]", "(", "y2", ")", "\n", "y4", "=", "self", ".", "blocks", "[", "3", "]", "(", "y3", ")", "\n", "y5", "=", "self", ".", "blocks", "[", "4", "]", "(", "y4", ")", "\n", "y6", "=", "self", ".", "blocks", "[", "5", "]", "(", "y5", ")", "\n", "y7", "=", "self", ".", "blocks", "[", "6", "]", "(", "y6", ")", "\n", "y8", "=", "self", ".", "blocks", "[", "7", "]", "(", "y7", ")", "\n", "y9", "=", "self", ".", "blocks", "[", "8", "]", "(", "y8", ")", "\n", "y10", "=", "self", ".", "blocks", "[", "9", "]", "(", "y9", ")", "\n", "y11", "=", "self", ".", "blocks", "[", "10", "]", "(", "y10", ")", "\n", "y12", "=", "self", ".", "blocks", "[", "11", "]", "(", "y11", ")", "\n", "\n", "y_out", "=", "self", ".", "norm", "(", "y12", ")", "\n", "\n", "y0", "=", "self", ".", "fusion0", "(", "y_hat", ")", "\n", "y1", "=", "self", ".", "fusion1", "(", "y1", "[", ":", ",", "1", ":", "]", ")", "\n", "y2", "=", "self", ".", "fusion2", "(", "y2", "[", ":", ",", "1", ":", "]", ")", "\n", "y3", "=", "self", ".", "fusion3", "(", "y3", "[", ":", ",", "1", ":", "]", ")", "\n", "\n", "y_rec", "=", "torch", ".", "cat", "(", "(", "y0", ",", "y1", ",", "y2", ",", "y3", ")", ",", "dim", "=", "2", ")", "\n", "y_rec", "=", "self", ".", "fusion", "(", "y_rec", ")", "\n", "\n", "cls", "=", "self", ".", "head", "(", "y_out", "[", ":", ",", "0", "]", ")", "\n", "rec", "=", "self", ".", "head_rec", "(", "y_rec", ")", "\n", "\n", "return", "cls", ",", "rec", "\n", "\n"]], "home.repos.pwc.inspect_result.bychao100_towards-image-compression-and-analysis-with-transformers.full_model.model._cfg": [[20, 28], ["None"], "function", ["None"], ["def", "_cfg", "(", "url", "=", "''", ",", "**", "kwargs", ")", ":", "\n", "    ", "return", "{", "\n", "'url'", ":", "url", ",", "\n", "'num_classes'", ":", "1000", ",", "'input_size'", ":", "(", "3", ",", "224", ",", "224", ")", ",", "'pool_size'", ":", "None", ",", "\n", "'crop_pct'", ":", ".9", ",", "'interpolation'", ":", "'bicubic'", ",", "\n", "'mean'", ":", "IMAGENET_DEFAULT_MEAN", ",", "'std'", ":", "IMAGENET_DEFAULT_STD", ",", "\n", "'first_conv'", ":", "'patch_embed.proj'", ",", "'classifier'", ":", "'head'", ",", "\n", "**", "kwargs", "\n", "}", "\n"]], "home.repos.pwc.inspect_result.bychao100_towards-image-compression-and-analysis-with-transformers.full_model.model.get_scale_table": [[36, 40], ["torch.exp", "torch.exp", "torch.linspace", "torch.linspace", "math.log", "math.log"], "function", ["None"], ["def", "get_scale_table", "(", "\n", "min", "=", "SCALES_MIN", ",", "max", "=", "SCALES_MAX", ",", "levels", "=", "SCALES_LEVELS", "\n", ")", ":", "# pylint: disable=W0622", "\n", "    ", "return", "torch", ".", "exp", "(", "torch", ".", "linspace", "(", "math", ".", "log", "(", "min", ")", ",", "math", ".", "log", "(", "max", ")", ",", "levels", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.bychao100_towards-image-compression-and-analysis-with-transformers.full_model.model.conv": [[42, 49], ["torch.Conv2d"], "function", ["None"], ["", "def", "conv", "(", "in_channels", ",", "out_channels", ",", "kernel_size", "=", "5", ",", "stride", "=", "2", ")", ":", "\n", "    ", "return", "nn", ".", "Conv2d", "(", "\n", "in_channels", ",", "\n", "out_channels", ",", "\n", "kernel_size", "=", "kernel_size", ",", "\n", "stride", "=", "stride", ",", "\n", "padding", "=", "kernel_size", "//", "2", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.bychao100_towards-image-compression-and-analysis-with-transformers.full_model.model.deconv": [[52, 60], ["torch.ConvTranspose2d"], "function", ["None"], ["", "def", "deconv", "(", "in_channels", ",", "out_channels", ",", "kernel_size", "=", "5", ",", "stride", "=", "2", ")", ":", "\n", "    ", "return", "nn", ".", "ConvTranspose2d", "(", "\n", "in_channels", ",", "\n", "out_channels", ",", "\n", "kernel_size", "=", "kernel_size", ",", "\n", "stride", "=", "stride", ",", "\n", "output_padding", "=", "stride", "-", "1", ",", "\n", "padding", "=", "kernel_size", "//", "2", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.bychao100_towards-image-compression-and-analysis-with-transformers.full_model.model.full_model": [[452, 462], ["model.JCCTransformer", "model._cfg", "torch.load", "torch.load", "model.JCCTransformer.load_state_dict", "functools.partial"], "function", ["home.repos.pwc.inspect_result.bychao100_towards-image-compression-and-analysis-with-transformers.pretrained_model.model._cfg", "home.repos.pwc.inspect_result.bychao100_towards-image-compression-and-analysis-with-transformers.full_model.model.JCCTransformer.load_state_dict"], ["", "", "@", "register_model", "\n", "def", "full_model", "(", "pretrained", "=", "False", ",", "**", "kwargs", ")", ":", "\n", "    ", "model", "=", "JCCTransformer", "(", "\n", "patch_size", "=", "16", ",", "embed_dim", "=", "384", ",", "depth", "=", "12", ",", "num_heads", "=", "6", ",", "mlp_ratio", "=", "4", ",", "qkv_bias", "=", "True", ",", "\n", "norm_layer", "=", "partial", "(", "nn", ".", "LayerNorm", ",", "eps", "=", "1e-6", ")", ",", "**", "kwargs", ")", "\n", "model", ".", "default_cfg", "=", "_cfg", "(", ")", "\n", "if", "pretrained", ":", "\n", "        ", "checkpoint", "=", "torch", ".", "load", "(", "\"./pretrain_s/checkpoint.pth\"", ",", "map_location", "=", "'cpu'", ")", "\n", "model", ".", "load_state_dict", "(", "checkpoint", "[", "\"model\"", "]", ",", "pretrained", "=", "True", ",", "strict", "=", "False", ")", "\n", "", "return", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.bychao100_towards-image-compression-and-analysis-with-transformers.full_model.optim.configure_optimizers": [[8, 44], ["dict", "model.named_parameters", "len", "torch.Adam", "torch.Adam", "model.named_parameters", "model.named_parameters", "len", "len", "n.endswith", "dict.keys", "n.endswith", "sorted", "sorted"], "function", ["None"], ["def", "configure_optimizers", "(", "args", ",", "model", ")", ":", "\n", "\n", "    ", "parameters", "=", "{", "\n", "n", "\n", "for", "n", ",", "p", "in", "model", ".", "named_parameters", "(", ")", "\n", "if", "not", "n", ".", "endswith", "(", "\".quantiles\"", ")", "and", "p", ".", "requires_grad", "\n", "}", "\n", "aux_parameters", "=", "{", "\n", "n", "\n", "for", "n", ",", "p", "in", "model", ".", "named_parameters", "(", ")", "\n", "if", "n", ".", "endswith", "(", "\".quantiles\"", ")", "and", "p", ".", "requires_grad", "\n", "}", "\n", "\n", "# Make sure we don't have an intersection of parameters", "\n", "params_dict", "=", "dict", "(", "model", ".", "named_parameters", "(", ")", ")", "\n", "inter_params", "=", "parameters", "&", "aux_parameters", "\n", "union_params", "=", "parameters", "|", "aux_parameters", "\n", "\n", "assert", "len", "(", "inter_params", ")", "==", "0", "\n", "assert", "len", "(", "union_params", ")", "-", "len", "(", "params_dict", ".", "keys", "(", ")", ")", "==", "0", "\n", "\n", "if", "args", ".", "opt", "==", "\"adam\"", ":", "\n", "        ", "optimizer", "=", "optim", ".", "Adam", "(", "\n", "(", "params_dict", "[", "n", "]", "for", "n", "in", "sorted", "(", "parameters", ")", ")", ",", "\n", "lr", "=", "args", ".", "lr", ",", "\n", "weight_decay", "=", "args", ".", "weight_decay", ",", "\n", "eps", "=", "args", ".", "opt_eps", ",", "\n", ")", "\n", "aux_optimizer", "=", "optim", ".", "Adam", "(", "\n", "(", "params_dict", "[", "n", "]", "for", "n", "in", "sorted", "(", "aux_parameters", ")", ")", ",", "\n", "lr", "=", "args", ".", "aux_lr", ",", "\n", "weight_decay", "=", "args", ".", "weight_decay", ",", "\n", "eps", "=", "args", ".", "opt_eps", ",", "\n", ")", "\n", "\n", "", "return", "optimizer", ",", "aux_optimizer", "\n", "", ""]], "home.repos.pwc.inspect_result.bychao100_towards-image-compression-and-analysis-with-transformers.full_model.utils.SmoothedValue.__init__": [[25, 32], ["collections.deque"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "window_size", "=", "20", ",", "fmt", "=", "None", ")", ":", "\n", "        ", "if", "fmt", "is", "None", ":", "\n", "            ", "fmt", "=", "\"{median:.4f} ({global_avg:.4f})\"", "\n", "", "self", ".", "deque", "=", "deque", "(", "maxlen", "=", "window_size", ")", "\n", "self", ".", "total", "=", "0.0", "\n", "self", ".", "count", "=", "0", "\n", "self", ".", "fmt", "=", "fmt", "\n", "\n"]], "home.repos.pwc.inspect_result.bychao100_towards-image-compression-and-analysis-with-transformers.full_model.utils.SmoothedValue.update": [[33, 37], ["utils.SmoothedValue.deque.append"], "methods", ["None"], ["", "def", "update", "(", "self", ",", "value", ",", "n", "=", "1", ")", ":", "\n", "        ", "self", ".", "deque", ".", "append", "(", "value", ")", "\n", "self", ".", "count", "+=", "n", "\n", "self", ".", "total", "+=", "value", "*", "n", "\n", "\n"]], "home.repos.pwc.inspect_result.bychao100_towards-image-compression-and-analysis-with-transformers.full_model.utils.SmoothedValue.synchronize_between_processes": [[38, 50], ["torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.barrier", "torch.barrier", "torch.all_reduce", "torch.all_reduce", "t.tolist.tolist.tolist", "int", "utils.is_dist_avail_and_initialized"], "methods", ["home.repos.pwc.inspect_result.bychao100_towards-image-compression-and-analysis-with-transformers.pretrained_model.utils.is_dist_avail_and_initialized"], ["", "def", "synchronize_between_processes", "(", "self", ")", ":", "\n", "        ", "\"\"\"\n        Warning: does not synchronize the deque!\n        \"\"\"", "\n", "if", "not", "is_dist_avail_and_initialized", "(", ")", ":", "\n", "            ", "return", "\n", "", "t", "=", "torch", ".", "tensor", "(", "[", "self", ".", "count", ",", "self", ".", "total", "]", ",", "dtype", "=", "torch", ".", "float64", ",", "device", "=", "'cuda'", ")", "\n", "dist", ".", "barrier", "(", ")", "\n", "dist", ".", "all_reduce", "(", "t", ")", "\n", "t", "=", "t", ".", "tolist", "(", ")", "\n", "self", ".", "count", "=", "int", "(", "t", "[", "0", "]", ")", "\n", "self", ".", "total", "=", "t", "[", "1", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.bychao100_towards-image-compression-and-analysis-with-transformers.full_model.utils.SmoothedValue.median": [[51, 55], ["torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor.median().item", "torch.tensor.median().item", "list", "torch.tensor.median", "torch.tensor.median"], "methods", ["home.repos.pwc.inspect_result.bychao100_towards-image-compression-and-analysis-with-transformers.pretrained_model.utils.SmoothedValue.median", "home.repos.pwc.inspect_result.bychao100_towards-image-compression-and-analysis-with-transformers.pretrained_model.utils.SmoothedValue.median"], ["", "@", "property", "\n", "def", "median", "(", "self", ")", ":", "\n", "        ", "d", "=", "torch", ".", "tensor", "(", "list", "(", "self", ".", "deque", ")", ")", "\n", "return", "d", ".", "median", "(", ")", ".", "item", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.bychao100_towards-image-compression-and-analysis-with-transformers.full_model.utils.SmoothedValue.avg": [[56, 60], ["torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor.mean().item", "torch.tensor.mean().item", "list", "torch.tensor.mean", "torch.tensor.mean"], "methods", ["None"], ["", "@", "property", "\n", "def", "avg", "(", "self", ")", ":", "\n", "        ", "d", "=", "torch", ".", "tensor", "(", "list", "(", "self", ".", "deque", ")", ",", "dtype", "=", "torch", ".", "float32", ")", "\n", "return", "d", ".", "mean", "(", ")", ".", "item", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.bychao100_towards-image-compression-and-analysis-with-transformers.full_model.utils.SmoothedValue.global_avg": [[61, 64], ["None"], "methods", ["None"], ["", "@", "property", "\n", "def", "global_avg", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "total", "/", "self", ".", "count", "\n", "\n"]], "home.repos.pwc.inspect_result.bychao100_towards-image-compression-and-analysis-with-transformers.full_model.utils.SmoothedValue.max": [[65, 68], ["utils.SmoothedValue.max"], "methods", ["home.repos.pwc.inspect_result.bychao100_towards-image-compression-and-analysis-with-transformers.pretrained_model.utils.SmoothedValue.max"], ["", "@", "property", "\n", "def", "max", "(", "self", ")", ":", "\n", "        ", "return", "max", "(", "self", ".", "deque", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.bychao100_towards-image-compression-and-analysis-with-transformers.full_model.utils.SmoothedValue.value": [[69, 72], ["None"], "methods", ["None"], ["", "@", "property", "\n", "def", "value", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "deque", "[", "-", "1", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.bychao100_towards-image-compression-and-analysis-with-transformers.full_model.utils.SmoothedValue.__str__": [[73, 80], ["utils.SmoothedValue.fmt.format"], "methods", ["None"], ["", "def", "__str__", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "fmt", ".", "format", "(", "\n", "median", "=", "self", ".", "median", ",", "\n", "avg", "=", "self", ".", "avg", ",", "\n", "global_avg", "=", "self", ".", "global_avg", ",", "\n", "max", "=", "self", ".", "max", ",", "\n", "value", "=", "self", ".", "value", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.bychao100_towards-image-compression-and-analysis-with-transformers.full_model.utils.MetricLogger.__init__": [[83, 86], ["collections.defaultdict"], "methods", ["None"], ["    ", "def", "__init__", "(", "self", ",", "delimiter", "=", "\"\\t\"", ")", ":", "\n", "        ", "self", ".", "meters", "=", "defaultdict", "(", "SmoothedValue", ")", "\n", "self", ".", "delimiter", "=", "delimiter", "\n", "\n"]], "home.repos.pwc.inspect_result.bychao100_towards-image-compression-and-analysis-with-transformers.full_model.utils.MetricLogger.update": [[87, 93], ["kwargs.items", "isinstance", "isinstance", "utils.MetricLogger.meters[].update", "v.item.item.item"], "methods", ["home.repos.pwc.inspect_result.bychao100_towards-image-compression-and-analysis-with-transformers.pretrained_model.utils.MetricLogger.update"], ["", "def", "update", "(", "self", ",", "**", "kwargs", ")", ":", "\n", "        ", "for", "k", ",", "v", "in", "kwargs", ".", "items", "(", ")", ":", "\n", "            ", "if", "isinstance", "(", "v", ",", "torch", ".", "Tensor", ")", ":", "\n", "                ", "v", "=", "v", ".", "item", "(", ")", "\n", "", "assert", "isinstance", "(", "v", ",", "(", "float", ",", "int", ")", ")", "\n", "self", ".", "meters", "[", "k", "]", ".", "update", "(", "v", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.bychao100_towards-image-compression-and-analysis-with-transformers.full_model.utils.MetricLogger.__getattr__": [[94, 101], ["AttributeError", "type"], "methods", ["None"], ["", "", "def", "__getattr__", "(", "self", ",", "attr", ")", ":", "\n", "        ", "if", "attr", "in", "self", ".", "meters", ":", "\n", "            ", "return", "self", ".", "meters", "[", "attr", "]", "\n", "", "if", "attr", "in", "self", ".", "__dict__", ":", "\n", "            ", "return", "self", ".", "__dict__", "[", "attr", "]", "\n", "", "raise", "AttributeError", "(", "\"'{}' object has no attribute '{}'\"", ".", "format", "(", "\n", "type", "(", "self", ")", ".", "__name__", ",", "attr", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.bychao100_towards-image-compression-and-analysis-with-transformers.full_model.utils.MetricLogger.__str__": [[102, 109], ["utils.MetricLogger.meters.items", "utils.MetricLogger.delimiter.join", "loss_str.append", "str"], "methods", ["None"], ["", "def", "__str__", "(", "self", ")", ":", "\n", "        ", "loss_str", "=", "[", "]", "\n", "for", "name", ",", "meter", "in", "self", ".", "meters", ".", "items", "(", ")", ":", "\n", "            ", "loss_str", ".", "append", "(", "\n", "\"{}: {}\"", ".", "format", "(", "name", ",", "str", "(", "meter", ")", ")", "\n", ")", "\n", "", "return", "self", ".", "delimiter", ".", "join", "(", "loss_str", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.bychao100_towards-image-compression-and-analysis-with-transformers.full_model.utils.MetricLogger.synchronize_between_processes": [[110, 113], ["utils.MetricLogger.meters.values", "meter.synchronize_between_processes"], "methods", ["home.repos.pwc.inspect_result.bychao100_towards-image-compression-and-analysis-with-transformers.pretrained_model.utils.MetricLogger.synchronize_between_processes"], ["", "def", "synchronize_between_processes", "(", "self", ")", ":", "\n", "        ", "for", "meter", "in", "self", ".", "meters", ".", "values", "(", ")", ":", "\n", "            ", "meter", ".", "synchronize_between_processes", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.bychao100_towards-image-compression-and-analysis-with-transformers.full_model.utils.MetricLogger.add_meter": [[114, 116], ["None"], "methods", ["None"], ["", "", "def", "add_meter", "(", "self", ",", "name", ",", "meter", ")", ":", "\n", "        ", "self", ".", "meters", "[", "name", "]", "=", "meter", "\n", "\n"]], "home.repos.pwc.inspect_result.bychao100_towards-image-compression-and-analysis-with-transformers.full_model.utils.MetricLogger.log_every": [[117, 163], ["time.time", "time.time", "utils.SmoothedValue", "utils.SmoothedValue", "torch.cuda.is_available", "torch.cuda.is_available", "torch.cuda.is_available", "torch.cuda.is_available", "utils.MetricLogger.delimiter.join", "str", "print", "utils.MetricLogger.append", "utils.SmoothedValue.update", "utils.SmoothedValue.update", "time.time", "time.time", "datetime.timedelta", "str", "str", "torch.cuda.is_available", "torch.cuda.is_available", "torch.cuda.is_available", "torch.cuda.is_available", "len", "time.time", "time.time", "datetime.timedelta", "print", "print", "int", "len", "str", "len", "len", "utils.MetricLogger.format", "utils.MetricLogger.format", "len", "int", "len", "len", "str", "str", "str", "str", "str", "str", "torch.cuda.max_memory_allocated", "torch.cuda.max_memory_allocated", "torch.cuda.max_memory_allocated", "torch.cuda.max_memory_allocated"], "methods", ["home.repos.pwc.inspect_result.bychao100_towards-image-compression-and-analysis-with-transformers.pretrained_model.utils.MetricLogger.update", "home.repos.pwc.inspect_result.bychao100_towards-image-compression-and-analysis-with-transformers.pretrained_model.utils.MetricLogger.update"], ["", "def", "log_every", "(", "self", ",", "iterable", ",", "print_freq", ",", "header", "=", "None", ")", ":", "\n", "        ", "i", "=", "0", "\n", "if", "not", "header", ":", "\n", "            ", "header", "=", "''", "\n", "", "start_time", "=", "time", ".", "time", "(", ")", "\n", "end", "=", "time", ".", "time", "(", ")", "\n", "iter_time", "=", "SmoothedValue", "(", "fmt", "=", "'{avg:.4f}'", ")", "\n", "data_time", "=", "SmoothedValue", "(", "fmt", "=", "'{avg:.4f}'", ")", "\n", "space_fmt", "=", "':'", "+", "str", "(", "len", "(", "str", "(", "len", "(", "iterable", ")", ")", ")", ")", "+", "'d'", "\n", "log_msg", "=", "[", "\n", "header", ",", "\n", "'[{0'", "+", "space_fmt", "+", "'}/{1}]'", ",", "\n", "'eta: {eta}'", ",", "\n", "'{meters}'", ",", "\n", "'time: {time}'", ",", "\n", "'data: {data}'", "\n", "]", "\n", "if", "torch", ".", "cuda", ".", "is_available", "(", ")", ":", "\n", "            ", "log_msg", ".", "append", "(", "'max mem: {memory:.0f}'", ")", "\n", "", "log_msg", "=", "self", ".", "delimiter", ".", "join", "(", "log_msg", ")", "\n", "MB", "=", "1024.0", "*", "1024.0", "\n", "for", "obj", "in", "iterable", ":", "\n", "            ", "data_time", ".", "update", "(", "time", ".", "time", "(", ")", "-", "end", ")", "\n", "yield", "obj", "\n", "iter_time", ".", "update", "(", "time", ".", "time", "(", ")", "-", "end", ")", "\n", "if", "i", "%", "print_freq", "==", "0", "or", "i", "==", "len", "(", "iterable", ")", "-", "1", ":", "\n", "                ", "eta_seconds", "=", "iter_time", ".", "global_avg", "*", "(", "len", "(", "iterable", ")", "-", "i", ")", "\n", "eta_string", "=", "str", "(", "datetime", ".", "timedelta", "(", "seconds", "=", "int", "(", "eta_seconds", ")", ")", ")", "\n", "if", "torch", ".", "cuda", ".", "is_available", "(", ")", ":", "\n", "                    ", "print", "(", "log_msg", ".", "format", "(", "\n", "i", ",", "len", "(", "iterable", ")", ",", "eta", "=", "eta_string", ",", "\n", "meters", "=", "str", "(", "self", ")", ",", "\n", "time", "=", "str", "(", "iter_time", ")", ",", "data", "=", "str", "(", "data_time", ")", ",", "\n", "memory", "=", "torch", ".", "cuda", ".", "max_memory_allocated", "(", ")", "/", "MB", ")", ")", "\n", "# ipdb.set_trace()", "\n", "", "else", ":", "\n", "                    ", "print", "(", "log_msg", ".", "format", "(", "\n", "i", ",", "len", "(", "iterable", ")", ",", "eta", "=", "eta_string", ",", "\n", "meters", "=", "str", "(", "self", ")", ",", "\n", "time", "=", "str", "(", "iter_time", ")", ",", "data", "=", "str", "(", "data_time", ")", ")", ")", "\n", "", "", "i", "+=", "1", "\n", "end", "=", "time", ".", "time", "(", ")", "\n", "", "total_time", "=", "time", ".", "time", "(", ")", "-", "start_time", "\n", "total_time_str", "=", "str", "(", "datetime", ".", "timedelta", "(", "seconds", "=", "int", "(", "total_time", ")", ")", ")", "\n", "print", "(", "'{} Total time: {} ({:.4f} s / it)'", ".", "format", "(", "\n", "header", ",", "total_time_str", ",", "total_time", "/", "len", "(", "iterable", ")", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.bychao100_towards-image-compression-and-analysis-with-transformers.full_model.utils._load_checkpoint_for_ema": [[165, 173], ["io.BytesIO", "torch.save", "torch.save", "io.BytesIO.seek", "model_ema._load_checkpoint"], "function", ["None"], ["", "", "def", "_load_checkpoint_for_ema", "(", "model_ema", ",", "checkpoint", ")", ":", "\n", "    ", "\"\"\"\n    Workaround for ModelEma._load_checkpoint to accept an already-loaded object\n    \"\"\"", "\n", "mem_file", "=", "io", ".", "BytesIO", "(", ")", "\n", "torch", ".", "save", "(", "checkpoint", ",", "mem_file", ")", "\n", "mem_file", ".", "seek", "(", "0", ")", "\n", "model_ema", ".", "_load_checkpoint", "(", "mem_file", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.bychao100_towards-image-compression-and-analysis-with-transformers.full_model.utils.setup_for_distributed": [[175, 188], ["kwargs.pop", "builtin_print"], "function", ["None"], ["", "def", "setup_for_distributed", "(", "is_master", ")", ":", "\n", "    ", "\"\"\"\n    This function disables printing when not in master process\n    \"\"\"", "\n", "import", "builtins", "as", "__builtin__", "\n", "builtin_print", "=", "__builtin__", ".", "print", "\n", "\n", "def", "print", "(", "*", "args", ",", "**", "kwargs", ")", ":", "\n", "        ", "force", "=", "kwargs", ".", "pop", "(", "'force'", ",", "False", ")", "\n", "if", "is_master", "or", "force", ":", "\n", "            ", "builtin_print", "(", "*", "args", ",", "**", "kwargs", ")", "\n", "\n", "", "", "__builtin__", ".", "print", "=", "print", "\n", "\n"]], "home.repos.pwc.inspect_result.bychao100_towards-image-compression-and-analysis-with-transformers.full_model.utils.is_dist_avail_and_initialized": [[190, 196], ["torch.is_available", "torch.is_initialized"], "function", ["None"], ["", "def", "is_dist_avail_and_initialized", "(", ")", ":", "\n", "    ", "if", "not", "dist", ".", "is_available", "(", ")", ":", "\n", "        ", "return", "False", "\n", "", "if", "not", "dist", ".", "is_initialized", "(", ")", ":", "\n", "        ", "return", "False", "\n", "", "return", "True", "\n", "\n"]], "home.repos.pwc.inspect_result.bychao100_towards-image-compression-and-analysis-with-transformers.full_model.utils.get_world_size": [[198, 202], ["torch.get_world_size", "utils.is_dist_avail_and_initialized"], "function", ["home.repos.pwc.inspect_result.bychao100_towards-image-compression-and-analysis-with-transformers.pretrained_model.utils.get_world_size", "home.repos.pwc.inspect_result.bychao100_towards-image-compression-and-analysis-with-transformers.pretrained_model.utils.is_dist_avail_and_initialized"], ["", "def", "get_world_size", "(", ")", ":", "\n", "    ", "if", "not", "is_dist_avail_and_initialized", "(", ")", ":", "\n", "        ", "return", "1", "\n", "", "return", "dist", ".", "get_world_size", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.bychao100_towards-image-compression-and-analysis-with-transformers.full_model.utils.get_rank": [[204, 208], ["torch.get_rank", "utils.is_dist_avail_and_initialized"], "function", ["home.repos.pwc.inspect_result.bychao100_towards-image-compression-and-analysis-with-transformers.pretrained_model.utils.get_rank", "home.repos.pwc.inspect_result.bychao100_towards-image-compression-and-analysis-with-transformers.pretrained_model.utils.is_dist_avail_and_initialized"], ["", "def", "get_rank", "(", ")", ":", "\n", "    ", "if", "not", "is_dist_avail_and_initialized", "(", ")", ":", "\n", "        ", "return", "0", "\n", "", "return", "dist", ".", "get_rank", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.bychao100_towards-image-compression-and-analysis-with-transformers.full_model.utils.is_main_process": [[210, 212], ["utils.get_rank"], "function", ["home.repos.pwc.inspect_result.bychao100_towards-image-compression-and-analysis-with-transformers.pretrained_model.utils.get_rank"], ["", "def", "is_main_process", "(", ")", ":", "\n", "    ", "return", "get_rank", "(", ")", "==", "0", "\n", "\n"]], "home.repos.pwc.inspect_result.bychao100_towards-image-compression-and-analysis-with-transformers.full_model.utils.save_on_master": [[214, 217], ["utils.is_main_process", "torch.save", "torch.save"], "function", ["home.repos.pwc.inspect_result.bychao100_towards-image-compression-and-analysis-with-transformers.pretrained_model.utils.is_main_process"], ["", "def", "save_on_master", "(", "*", "args", ",", "**", "kwargs", ")", ":", "\n", "    ", "if", "is_main_process", "(", ")", ":", "\n", "        ", "torch", ".", "save", "(", "*", "args", ",", "**", "kwargs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.bychao100_towards-image-compression-and-analysis-with-transformers.full_model.utils.init_distributed_mode": [[219, 242], ["torch.cuda.set_device", "torch.cuda.set_device", "utils.setup_for_distributed.print", "torch.distributed.init_process_group", "torch.distributed.init_process_group", "torch.distributed.barrier", "torch.distributed.barrier", "utils.setup_for_distributed", "int", "int", "int", "int", "utils.setup_for_distributed.print", "torch.cuda.device_count", "torch.cuda.device_count"], "function", ["home.repos.pwc.inspect_result.bychao100_towards-image-compression-and-analysis-with-transformers.pretrained_model.utils.setup_for_distributed"], ["", "", "def", "init_distributed_mode", "(", "args", ")", ":", "\n", "    ", "if", "'RANK'", "in", "os", ".", "environ", "and", "'WORLD_SIZE'", "in", "os", ".", "environ", ":", "\n", "        ", "args", ".", "rank", "=", "int", "(", "os", ".", "environ", "[", "\"RANK\"", "]", ")", "\n", "args", ".", "world_size", "=", "int", "(", "os", ".", "environ", "[", "'WORLD_SIZE'", "]", ")", "\n", "args", ".", "gpu", "=", "int", "(", "os", ".", "environ", "[", "'LOCAL_RANK'", "]", ")", "\n", "", "elif", "'SLURM_PROCID'", "in", "os", ".", "environ", ":", "\n", "        ", "args", ".", "rank", "=", "int", "(", "os", ".", "environ", "[", "'SLURM_PROCID'", "]", ")", "\n", "args", ".", "gpu", "=", "args", ".", "rank", "%", "torch", ".", "cuda", ".", "device_count", "(", ")", "\n", "", "else", ":", "\n", "        ", "print", "(", "'Not using distributed mode'", ")", "\n", "args", ".", "distributed", "=", "False", "\n", "return", "\n", "\n", "", "args", ".", "distributed", "=", "True", "\n", "\n", "torch", ".", "cuda", ".", "set_device", "(", "args", ".", "gpu", ")", "\n", "args", ".", "dist_backend", "=", "'nccl'", "\n", "print", "(", "'| distributed init (rank {}): {}'", ".", "format", "(", "\n", "args", ".", "rank", ",", "args", ".", "dist_url", ")", ",", "flush", "=", "True", ")", "\n", "torch", ".", "distributed", ".", "init_process_group", "(", "backend", "=", "args", ".", "dist_backend", ",", "init_method", "=", "args", ".", "dist_url", ",", "\n", "world_size", "=", "args", ".", "world_size", ",", "rank", "=", "args", ".", "rank", ")", "\n", "torch", ".", "distributed", ".", "barrier", "(", ")", "\n", "setup_for_distributed", "(", "args", ".", "rank", "==", "0", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.bychao100_towards-image-compression-and-analysis-with-transformers.full_model.utils.imwrite": [[244, 253], ["torchvision.utils.make_grid.cpu", "torch.tensor().reshape", "torch.tensor().reshape", "torch.tensor().reshape", "torch.tensor().reshape", "torchvision.utils.make_grid", "imgs_pil.save", "torch.clamp", "torch.clamp", "torchvision.transforms.ToPILImage", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor"], "function", ["None"], ["", "def", "imwrite", "(", "imgs", ",", "path", ")", ":", "\n", "    ", "imgs", "=", "imgs", ".", "cpu", "(", ")", "\n", "imagenet_std", "=", "torch", ".", "tensor", "(", "IMAGENET_DEFAULT_STD", ")", ".", "reshape", "(", "1", ",", "3", ",", "1", ",", "1", ")", "\n", "imagenet_mean", "=", "torch", ".", "tensor", "(", "IMAGENET_DEFAULT_MEAN", ")", ".", "reshape", "(", "1", ",", "3", ",", "1", ",", "1", ")", "\n", "\n", "imgs", "=", "torchvision", ".", "utils", ".", "make_grid", "(", "torch", ".", "clamp", "(", "imgs", "*", "imagenet_std", "+", "imagenet_mean", ",", "min", "=", "0.", ",", "max", "=", "1.", ")", ")", "\n", "\n", "imgs_pil", "=", "torchvision", ".", "transforms", ".", "ToPILImage", "(", ")", "(", "imgs", ")", "\n", "imgs_pil", ".", "save", "(", "path", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.bychao100_towards-image-compression-and-analysis-with-transformers.full_model.utils.img_distortion": [[255, 269], ["torch.device", "torch.device", "torch.tensor().reshape().to", "torch.tensor().reshape().to", "torch.tensor().reshape().to", "torch.tensor().reshape().to", "torch.mean", "torch.mean", "torch.mean", "torch.mean", "torch.mean.item", "torch.clamp", "torch.clamp", "torch.clamp", "torch.clamp", "torch.nn.functional.mse_loss", "torch.tensor().reshape", "torch.tensor().reshape", "torch.tensor().reshape", "torch.tensor().reshape", "torch.log10", "torch.log10", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor"], "function", ["None"], ["", "def", "img_distortion", "(", "recs", ",", "orgs", ")", ":", "\n", "\n", "    ", "device", "=", "torch", ".", "device", "(", "'cuda'", ")", "\n", "\n", "imagenet_std", "=", "torch", ".", "tensor", "(", "IMAGENET_DEFAULT_STD", ")", ".", "reshape", "(", "1", ",", "3", ",", "1", ",", "1", ")", ".", "to", "(", "device", ")", "\n", "imagenet_mean", "=", "torch", ".", "tensor", "(", "IMAGENET_DEFAULT_MEAN", ")", ".", "reshape", "(", "1", ",", "3", ",", "1", ",", "1", ")", ".", "to", "(", "device", ")", "\n", "\n", "org_imgs", "=", "torch", ".", "clamp", "(", "orgs", "*", "imagenet_std", "+", "imagenet_mean", ",", "min", "=", "0.", ",", "max", "=", "1.", ")", "*", "255.", "\n", "rec_imgs", "=", "torch", ".", "clamp", "(", "recs", "*", "imagenet_std", "+", "imagenet_mean", ",", "min", "=", "0.", ",", "max", "=", "1.", ")", "*", "255.", "\n", "\n", "mse_no_reduction", "=", "torch", ".", "mean", "(", "mse_loss", "(", "org_imgs", ",", "rec_imgs", ",", "reduction", "=", "'none'", ")", ",", "dim", "=", "(", "1", ",", "2", ",", "3", ")", ")", "\n", "psnr", "=", "torch", ".", "mean", "(", "10.", "*", "torch", ".", "log10", "(", "255.", "**", "2", "/", "mse_no_reduction", ")", ")", "\n", "\n", "return", "psnr", ".", "item", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.bychao100_towards-image-compression-and-analysis-with-transformers.full_model.losses.DenormalizedMSELoss.__init__": [[15, 19], ["torch.Module.__init__", "torch.tensor().reshape", "torch.tensor().reshape", "torch.tensor().reshape", "torch.tensor().reshape", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor"], "methods", ["home.repos.pwc.inspect_result.bychao100_towards-image-compression-and-analysis-with-transformers.pretrained_model.datasets.INatDataset.__init__"], ["    ", "def", "__init__", "(", "self", ",", "scale", "=", "255.", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "imagenet_std", "=", "torch", ".", "tensor", "(", "IMAGENET_DEFAULT_STD", ")", ".", "reshape", "(", "1", ",", "3", ",", "1", ",", "1", ")", "\n", "self", ".", "scale", "=", "scale", "\n", "\n"]], "home.repos.pwc.inspect_result.bychao100_towards-image-compression-and-analysis-with-transformers.full_model.losses.DenormalizedMSELoss.forward": [[20, 25], ["torch.mean", "torch.mean", "torch.mean", "torch.mean", "losses.DenormalizedMSELoss.imagenet_std.to"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "x", ",", "y", ")", ":", "\n", "        ", "diff", "=", "(", "x", "-", "y", ")", "*", "self", ".", "imagenet_std", ".", "to", "(", "x", ".", "device", ")", "*", "self", ".", "scale", "\n", "mse_loss", "=", "torch", ".", "mean", "(", "diff", "**", "2", ")", "\n", "\n", "return", "mse_loss", "\n", "\n"]], "home.repos.pwc.inspect_result.bychao100_towards-image-compression-and-analysis-with-transformers.full_model.losses.JointLoss.__init__": [[29, 35], ["torch.Module.__init__", "losses.DenormalizedMSELoss"], "methods", ["home.repos.pwc.inspect_result.bychao100_towards-image-compression-and-analysis-with-transformers.pretrained_model.datasets.INatDataset.__init__"], ["    ", "def", "__init__", "(", "self", ",", "base_criterion", ":", "torch", ".", "nn", ".", "Module", ",", "alpha", ":", "float", ",", "beta", ":", "float", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "base_criterion", "=", "base_criterion", "\n", "self", ".", "alpha", "=", "alpha", "\n", "self", ".", "d_mse", "=", "DenormalizedMSELoss", "(", ")", "\n", "self", ".", "beta", "=", "beta", "\n", "\n"]], "home.repos.pwc.inspect_result.bychao100_towards-image-compression-and-analysis-with-transformers.full_model.losses.JointLoss.forward": [[36, 47], ["inputs.size", "losses.JointLoss.base_criterion", "losses.JointLoss.d_mse", "torch.log().sum", "torch.log().sum", "torch.log().sum", "torch.log().sum", "torch.log().sum", "torch.log().sum", "torch.log().sum", "torch.log().sum", "math.log", "torch.log", "torch.log", "torch.log", "torch.log", "torch.log", "torch.log", "torch.log", "torch.log"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "inputs", ",", "outputs", ",", "labels", ")", ":", "\n", "        ", "B", ",", "_", ",", "H", ",", "W", "=", "inputs", ".", "size", "(", ")", "\n", "num_pixels", "=", "B", "*", "H", "*", "W", "\n", "\n", "outputs_cls", ",", "outputs_rec", ",", "outputs_y_likelihoods", ",", "outputs_z_likelihoods", "=", "outputs", "\n", "\n", "cls_loss", "=", "self", ".", "base_criterion", "(", "outputs_cls", ",", "labels", ")", "\n", "mse_loss", "=", "self", ".", "d_mse", "(", "outputs_rec", ",", "inputs", ")", "\n", "bpp_loss", "=", "(", "torch", ".", "log", "(", "outputs_y_likelihoods", ")", ".", "sum", "(", ")", "+", "torch", ".", "log", "(", "outputs_z_likelihoods", ")", ".", "sum", "(", ")", ")", "/", "(", "-", "math", ".", "log", "(", "2", ")", "*", "num_pixels", ")", "\n", "\n", "return", "self", ".", "alpha", "*", "cls_loss", "+", "self", ".", "beta", "*", "mse_loss", "+", "bpp_loss", "\n", "\n"]], "home.repos.pwc.inspect_result.bychao100_towards-image-compression-and-analysis-with-transformers.full_model.datasets.INatDataset.__init__": [[12, 50], ["os.path.join", "os.path.join", "len", "open", "json.load", "open", "json.load", "open", "json.load", "king.append", "elem[].split", "int", "os.path.join", "torchvision.datasets.INatDataset.samples.append", "os.path.join", "targeter.keys", "int"], "methods", ["None"], ["    ", "def", "__init__", "(", "self", ",", "root", ",", "train", "=", "True", ",", "year", "=", "2018", ",", "transform", "=", "None", ",", "target_transform", "=", "None", ",", "\n", "category", "=", "'name'", ",", "loader", "=", "default_loader", ")", ":", "\n", "        ", "self", ".", "transform", "=", "transform", "\n", "self", ".", "loader", "=", "loader", "\n", "self", ".", "target_transform", "=", "target_transform", "\n", "self", ".", "year", "=", "year", "\n", "# assert category in ['kingdom','phylum','class','order','supercategory','family','genus','name']", "\n", "path_json", "=", "os", ".", "path", ".", "join", "(", "root", ",", "f'{\"train\" if train else \"val\"}{year}.json'", ")", "\n", "with", "open", "(", "path_json", ")", "as", "json_file", ":", "\n", "            ", "data", "=", "json", ".", "load", "(", "json_file", ")", "\n", "\n", "", "with", "open", "(", "os", ".", "path", ".", "join", "(", "root", ",", "'categories.json'", ")", ")", "as", "json_file", ":", "\n", "            ", "data_catg", "=", "json", ".", "load", "(", "json_file", ")", "\n", "\n", "", "path_json_for_targeter", "=", "os", ".", "path", ".", "join", "(", "root", ",", "f\"train{year}.json\"", ")", "\n", "\n", "with", "open", "(", "path_json_for_targeter", ")", "as", "json_file", ":", "\n", "            ", "data_for_targeter", "=", "json", ".", "load", "(", "json_file", ")", "\n", "\n", "", "targeter", "=", "{", "}", "\n", "indexer", "=", "0", "\n", "for", "elem", "in", "data_for_targeter", "[", "'annotations'", "]", ":", "\n", "            ", "king", "=", "[", "]", "\n", "king", ".", "append", "(", "data_catg", "[", "int", "(", "elem", "[", "'category_id'", "]", ")", "]", "[", "category", "]", ")", "\n", "if", "king", "[", "0", "]", "not", "in", "targeter", ".", "keys", "(", ")", ":", "\n", "                ", "targeter", "[", "king", "[", "0", "]", "]", "=", "indexer", "\n", "indexer", "+=", "1", "\n", "", "", "self", ".", "nb_classes", "=", "len", "(", "targeter", ")", "\n", "\n", "self", ".", "samples", "=", "[", "]", "\n", "for", "elem", "in", "data", "[", "'images'", "]", ":", "\n", "            ", "cut", "=", "elem", "[", "'file_name'", "]", ".", "split", "(", "'/'", ")", "\n", "target_current", "=", "int", "(", "cut", "[", "2", "]", ")", "\n", "path_current", "=", "os", ".", "path", ".", "join", "(", "root", ",", "cut", "[", "0", "]", ",", "cut", "[", "2", "]", ",", "cut", "[", "3", "]", ")", "\n", "\n", "categors", "=", "data_catg", "[", "target_current", "]", "\n", "target_current_true", "=", "targeter", "[", "categors", "[", "category", "]", "]", "\n", "self", ".", "samples", ".", "append", "(", "(", "path_current", ",", "target_current_true", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.bychao100_towards-image-compression-and-analysis-with-transformers.full_model.datasets.build_dataset": [[52, 65], ["datasets.build_transform", "os.path.join", "torchvision.datasets.ImageFolder", "datasets.INatDataset"], "function", ["home.repos.pwc.inspect_result.bychao100_towards-image-compression-and-analysis-with-transformers.pretrained_model.datasets.build_transform"], ["", "", "", "def", "build_dataset", "(", "is_train", ",", "args", ")", ":", "\n", "    ", "transform", "=", "build_transform", "(", "is_train", ",", "args", ")", "\n", "\n", "if", "args", ".", "data_set", "==", "'IMNET'", ":", "\n", "        ", "root", "=", "os", ".", "path", ".", "join", "(", "args", ".", "data_path", ",", "'train'", "if", "is_train", "else", "'val'", ")", "\n", "dataset", "=", "datasets", ".", "ImageFolder", "(", "root", ",", "transform", "=", "transform", ")", "\n", "nb_classes", "=", "1000", "\n", "", "elif", "args", ".", "data_set", "==", "'INAT19'", ":", "\n", "        ", "dataset", "=", "INatDataset", "(", "args", ".", "data_path", ",", "train", "=", "is_train", ",", "year", "=", "2019", ",", "\n", "category", "=", "args", ".", "inat_category", ",", "transform", "=", "transform", ")", "\n", "nb_classes", "=", "dataset", ".", "nb_classes", "\n", "\n", "", "return", "dataset", ",", "nb_classes", "\n", "\n"]], "home.repos.pwc.inspect_result.bychao100_towards-image-compression-and-analysis-with-transformers.full_model.datasets.build_transform": [[67, 95], ["t.append", "t.append", "torchvision.transforms.Compose", "timm.data.create_transform", "int", "t.append", "t.append", "torchvision.transforms.ToTensor", "torchvision.transforms.Normalize", "torchvision.transforms.RandomCrop", "torchvision.transforms.Resize", "torchvision.transforms.CenterCrop"], "function", ["None"], ["", "def", "build_transform", "(", "is_train", ",", "args", ")", ":", "\n", "    ", "resize_im", "=", "args", ".", "input_size", ">", "32", "\n", "if", "is_train", ":", "\n", "# this should always dispatch to transforms_imagenet_train", "\n", "        ", "transform", "=", "create_transform", "(", "\n", "input_size", "=", "args", ".", "input_size", ",", "\n", "is_training", "=", "True", ",", "\n", "color_jitter", "=", "None", ",", "\n", "interpolation", "=", "args", ".", "train_interpolation", ",", "\n", ")", "\n", "if", "not", "resize_im", ":", "\n", "# replace RandomResizedCropAndInterpolation with", "\n", "# RandomCrop", "\n", "            ", "transform", ".", "transforms", "[", "0", "]", "=", "transforms", ".", "RandomCrop", "(", "\n", "args", ".", "input_size", ",", "padding", "=", "4", ")", "\n", "", "return", "transform", "\n", "\n", "", "t", "=", "[", "]", "\n", "if", "resize_im", ":", "\n", "        ", "size", "=", "int", "(", "(", "256", "/", "224", ")", "*", "args", ".", "input_size", ")", "\n", "t", ".", "append", "(", "\n", "transforms", ".", "Resize", "(", "size", ",", "interpolation", "=", "3", ")", ",", "# to maintain same ratio w.r.t. 224 images", "\n", ")", "\n", "t", ".", "append", "(", "transforms", ".", "CenterCrop", "(", "args", ".", "input_size", ")", ")", "\n", "\n", "", "t", ".", "append", "(", "transforms", ".", "ToTensor", "(", ")", ")", "\n", "t", ".", "append", "(", "transforms", ".", "Normalize", "(", "IMAGENET_DEFAULT_MEAN", ",", "IMAGENET_DEFAULT_STD", ")", ")", "\n", "return", "transforms", ".", "Compose", "(", "t", ")", "", "", ""]], "home.repos.pwc.inspect_result.bychao100_towards-image-compression-and-analysis-with-transformers.pretrained_model.engine.train_one_epoch": [[18, 63], ["model.train", "utils.MetricLogger", "utils.MetricLogger.add_meter", "utils.MetricLogger.log_every", "utils.MetricLogger.synchronize_between_processes", "print", "utils.SmoothedValue", "samples.to.to", "targets.to.to", "criterion.item", "optimizer.zero_grad", "loss_scaler", "torch.cuda.synchronize", "utils.MetricLogger.update", "utils.MetricLogger.update", "mixup_fn", "torch.cuda.amp.autocast", "model", "criterion", "math.isfinite", "print", "sys.exit", "hasattr", "model_ema.update", "utils.MetricLogger.meters.items", "model.parameters"], "function", ["home.repos.pwc.inspect_result.bychao100_towards-image-compression-and-analysis-with-transformers.pretrained_model.utils.MetricLogger.add_meter", "home.repos.pwc.inspect_result.bychao100_towards-image-compression-and-analysis-with-transformers.pretrained_model.utils.MetricLogger.log_every", "home.repos.pwc.inspect_result.bychao100_towards-image-compression-and-analysis-with-transformers.pretrained_model.utils.MetricLogger.synchronize_between_processes", "home.repos.pwc.inspect_result.bychao100_towards-image-compression-and-analysis-with-transformers.pretrained_model.utils.MetricLogger.update", "home.repos.pwc.inspect_result.bychao100_towards-image-compression-and-analysis-with-transformers.pretrained_model.utils.MetricLogger.update", "home.repos.pwc.inspect_result.bychao100_towards-image-compression-and-analysis-with-transformers.pretrained_model.utils.MetricLogger.update"], ["def", "train_one_epoch", "(", "model", ":", "torch", ".", "nn", ".", "Module", ",", "criterion", ":", "JointLoss", ",", "\n", "data_loader", ":", "Iterable", ",", "optimizer", ":", "torch", ".", "optim", ".", "Optimizer", ",", "aux_optimizer", ":", "torch", ".", "optim", ".", "Optimizer", ",", "\n", "device", ":", "torch", ".", "device", ",", "epoch", ":", "int", ",", "max_norm", ":", "float", "=", "0", ",", "\n", "set_training_mode", "=", "True", ")", ":", "\n", "    ", "model", ".", "train", "(", "set_training_mode", ")", "\n", "metric_logger", "=", "utils", ".", "MetricLogger", "(", "delimiter", "=", "\"  \"", ")", "\n", "metric_logger", ".", "add_meter", "(", "'lr'", ",", "utils", ".", "SmoothedValue", "(", "window_size", "=", "1", ",", "fmt", "=", "'{value:.6f}'", ")", ")", "\n", "header", "=", "'Epoch: [{}]'", ".", "format", "(", "epoch", ")", "\n", "print_freq", "=", "10", "\n", "\n", "for", "samples", ",", "targets", "in", "metric_logger", ".", "log_every", "(", "data_loader", ",", "print_freq", ",", "header", ")", ":", "\n", "        ", "samples", "=", "samples", ".", "to", "(", "device", ",", "non_blocking", "=", "True", ")", "\n", "targets", "=", "targets", ".", "to", "(", "device", ",", "non_blocking", "=", "True", ")", "\n", "\n", "optimizer", ".", "zero_grad", "(", ")", "\n", "aux_optimizer", ".", "zero_grad", "(", ")", "\n", "\n", "outputs", ",", "aux_loss", "=", "model", "(", "samples", ")", "\n", "loss", "=", "criterion", "(", "samples", ",", "outputs", ",", "targets", ")", "\n", "loss_value", "=", "loss", ".", "item", "(", ")", "\n", "loss", ".", "backward", "(", ")", "\n", "\n", "if", "not", "math", ".", "isfinite", "(", "loss_value", ")", ":", "\n", "            ", "print", "(", "\"Loss is {}, stopping training\"", ".", "format", "(", "loss_value", ")", ")", "\n", "sys", ".", "exit", "(", "1", ")", "\n", "\n", "", "if", "max_norm", "is", "not", "None", "and", "max_norm", ">", "0", ":", "\n", "            ", "torch", ".", "nn", ".", "utils", ".", "clip_grad_norm_", "(", "model", ".", "parameters", "(", ")", ",", "max_norm", ")", "\n", "", "optimizer", ".", "step", "(", ")", "\n", "\n", "aux_loss_value", "=", "aux_loss", ".", "item", "(", ")", "\n", "aux_loss", ".", "backward", "(", ")", "\n", "aux_optimizer", ".", "step", "(", ")", "\n", "\n", "torch", ".", "cuda", ".", "synchronize", "(", ")", "\n", "\n", "metric_logger", ".", "update", "(", "loss", "=", "loss_value", ")", "\n", "metric_logger", ".", "update", "(", "aux_loss", "=", "aux_loss_value", ")", "\n", "metric_logger", ".", "update", "(", "lr", "=", "optimizer", ".", "param_groups", "[", "0", "]", "[", "\"lr\"", "]", ")", "\n", "# gather the stats from all processes", "\n", "", "metric_logger", ".", "synchronize_between_processes", "(", ")", "\n", "print", "(", "\"Averaged stats:\"", ",", "metric_logger", ")", "\n", "return", "{", "k", ":", "meter", ".", "global_avg", "for", "k", ",", "meter", "in", "metric_logger", ".", "meters", ".", "items", "(", ")", "}", "\n", "\n", "\n", "", "@", "torch", ".", "no_grad", "(", ")", "\n"]], "home.repos.pwc.inspect_result.bychao100_towards-image-compression-and-analysis-with-transformers.pretrained_model.engine.evaluate": [[65, 107], ["torch.no_grad", "torch.nn.CrossEntropyLoss", "losses.DenormalizedMSELoss", "utils.MetricLogger", "model.eval", "utils.MetricLogger.log_every", "utils.MetricLogger.synchronize_between_processes", "print", "images.to.to", "target.to.to", "timm.utils.accuracy", "utils.MetricLogger.update", "utils.MetricLogger.meters[].update", "utils.MetricLogger.meters[].update", "utils.MetricLogger.meters[].update", "utils.MetricLogger.meters[].update", "torch.cuda.amp.autocast", "model", "torch.nn.CrossEntropyLoss.", "losses.DenormalizedMSELoss.", "utils.img_distortion", "utils.imwrite", "utils.imwrite", "acc1.item", "acc5.item", "criterion_rec.item", "utils.MetricLogger.meters.items", "os.path.join", "os.path.join", "criterion_cls.item"], "function", ["home.repos.pwc.inspect_result.bychao100_towards-image-compression-and-analysis-with-transformers.pretrained_model.utils.MetricLogger.log_every", "home.repos.pwc.inspect_result.bychao100_towards-image-compression-and-analysis-with-transformers.pretrained_model.utils.MetricLogger.synchronize_between_processes", "home.repos.pwc.inspect_result.bychao100_towards-image-compression-and-analysis-with-transformers.pretrained_model.utils.MetricLogger.update", "home.repos.pwc.inspect_result.bychao100_towards-image-compression-and-analysis-with-transformers.pretrained_model.utils.MetricLogger.update", "home.repos.pwc.inspect_result.bychao100_towards-image-compression-and-analysis-with-transformers.pretrained_model.utils.MetricLogger.update", "home.repos.pwc.inspect_result.bychao100_towards-image-compression-and-analysis-with-transformers.pretrained_model.utils.MetricLogger.update", "home.repos.pwc.inspect_result.bychao100_towards-image-compression-and-analysis-with-transformers.pretrained_model.utils.MetricLogger.update", "home.repos.pwc.inspect_result.bychao100_towards-image-compression-and-analysis-with-transformers.pretrained_model.utils.img_distortion", "home.repos.pwc.inspect_result.bychao100_towards-image-compression-and-analysis-with-transformers.pretrained_model.utils.imwrite", "home.repos.pwc.inspect_result.bychao100_towards-image-compression-and-analysis-with-transformers.pretrained_model.utils.imwrite"], ["    ", "criterion_cls", "=", "torch", ".", "nn", ".", "CrossEntropyLoss", "(", ")", "\n", "criterion_rec", "=", "DenormalizedMSELoss", "(", ")", "\n", "\n", "metric_logger", "=", "utils", ".", "MetricLogger", "(", "delimiter", "=", "\"  \"", ")", "\n", "header", "=", "'Test:'", "\n", "\n", "# switch to evaluation mode", "\n", "model", ".", "eval", "(", ")", "\n", "for", "images", ",", "target", "in", "metric_logger", ".", "log_every", "(", "data_loader", ",", "10", ",", "header", ")", ":", "\n", "        ", "images", "=", "images", ".", "to", "(", "device", ",", "non_blocking", "=", "True", ")", "\n", "target", "=", "target", ".", "to", "(", "device", ",", "non_blocking", "=", "True", ")", "\n", "\n", "B", ",", "_", ",", "H", ",", "W", "=", "images", ".", "shape", "\n", "num_pixels", "=", "B", "*", "H", "*", "W", "\n", "\n", "# compute output", "\n", "output", ",", "_", "=", "model", "(", "images", ")", "\n", "loss_cls", "=", "criterion_cls", "(", "output", "[", "0", "]", ",", "target", ")", "\n", "loss_rec", "=", "criterion_rec", "(", "output", "[", "1", "]", ",", "images", ")", "\n", "loss_bpp", "=", "(", "torch", ".", "log", "(", "output", "[", "2", "]", ")", ".", "sum", "(", ")", "+", "torch", ".", "log", "(", "output", "[", "3", "]", ")", ".", "sum", "(", ")", ")", "/", "(", "-", "math", ".", "log", "(", "2", ")", "*", "num_pixels", ")", "\n", "psnr", "=", "utils", ".", "img_distortion", "(", "output", "[", "1", "]", ",", "images", ")", "\n", "\n", "if", "write_img", ":", "\n", "            ", "utils", ".", "imwrite", "(", "images", "[", ":", "4", "]", ",", "os", ".", "path", ".", "join", "(", "output_dir", ",", "'example_org.png'", ")", ")", "\n", "utils", ".", "imwrite", "(", "output", "[", "1", "]", "[", ":", "4", "]", ",", "os", ".", "path", ".", "join", "(", "output_dir", ",", "'example_rec.png'", ")", ")", "\n", "write_img", "=", "False", "\n", "\n", "", "acc1", ",", "acc5", "=", "accuracy", "(", "output", "[", "0", "]", ",", "target", ",", "topk", "=", "(", "1", ",", "5", ")", ")", "\n", "\n", "batch_size", "=", "images", ".", "shape", "[", "0", "]", "\n", "metric_logger", ".", "update", "(", "loss_cls", "=", "loss_cls", ".", "item", "(", ")", ")", "\n", "metric_logger", ".", "meters", "[", "'acc1'", "]", ".", "update", "(", "acc1", ".", "item", "(", ")", ",", "n", "=", "batch_size", ")", "\n", "metric_logger", ".", "meters", "[", "'acc5'", "]", ".", "update", "(", "acc5", ".", "item", "(", ")", ",", "n", "=", "batch_size", ")", "\n", "metric_logger", ".", "meters", "[", "'loss_rec'", "]", ".", "update", "(", "loss_rec", ".", "item", "(", ")", ",", "n", "=", "batch_size", ")", "\n", "metric_logger", ".", "meters", "[", "'loss_bpp'", "]", ".", "update", "(", "loss_bpp", ".", "item", "(", ")", ",", "n", "=", "batch_size", ")", "\n", "metric_logger", ".", "meters", "[", "'psnr'", "]", ".", "update", "(", "psnr", ",", "n", "=", "batch_size", ")", "\n", "# gather the stats from all processes", "\n", "", "metric_logger", ".", "synchronize_between_processes", "(", ")", "\n", "print", "(", "'* Acc@1 {top1.global_avg:.3f} Acc@5 {top5.global_avg:.3f} loss_cls {losses_cls.global_avg:.3f} loss_rec {losses_rec.global_avg:.3f} loss bpp {losses_bpp.global_avg:.3f} psnr {psnr.global_avg:.3f}'", "\n", ".", "format", "(", "top1", "=", "metric_logger", ".", "acc1", ",", "top5", "=", "metric_logger", ".", "acc5", ",", "losses_cls", "=", "metric_logger", ".", "loss_cls", ",", "losses_rec", "=", "metric_logger", ".", "loss_rec", ",", "losses_bpp", "=", "metric_logger", ".", "loss_bpp", ",", "psnr", "=", "metric_logger", ".", "psnr", ")", ")", "\n", "\n", "return", "{", "k", ":", "meter", ".", "global_avg", "for", "k", ",", "meter", "in", "metric_logger", ".", "meters", ".", "items", "(", ")", "}", "\n", "\n"]], "home.repos.pwc.inspect_result.bychao100_towards-image-compression-and-analysis-with-transformers.pretrained_model.samplers.RASampler.__init__": [[14, 32], ["int", "int", "torch.get_world_size", "torch.get_world_size", "torch.get_rank", "torch.get_rank", "math.ceil", "math.floor", "torch.is_available", "torch.is_available", "RuntimeError", "torch.is_available", "torch.is_available", "RuntimeError", "len", "len"], "methods", ["home.repos.pwc.inspect_result.bychao100_towards-image-compression-and-analysis-with-transformers.pretrained_model.utils.get_world_size", "home.repos.pwc.inspect_result.bychao100_towards-image-compression-and-analysis-with-transformers.pretrained_model.utils.get_world_size", "home.repos.pwc.inspect_result.bychao100_towards-image-compression-and-analysis-with-transformers.pretrained_model.utils.get_rank", "home.repos.pwc.inspect_result.bychao100_towards-image-compression-and-analysis-with-transformers.pretrained_model.utils.get_rank"], ["def", "__init__", "(", "self", ",", "dataset", ",", "num_replicas", "=", "None", ",", "rank", "=", "None", ",", "shuffle", "=", "True", ")", ":", "\n", "        ", "if", "num_replicas", "is", "None", ":", "\n", "            ", "if", "not", "dist", ".", "is_available", "(", ")", ":", "\n", "                ", "raise", "RuntimeError", "(", "\"Requires distributed package to be available\"", ")", "\n", "", "num_replicas", "=", "dist", ".", "get_world_size", "(", ")", "\n", "", "if", "rank", "is", "None", ":", "\n", "            ", "if", "not", "dist", ".", "is_available", "(", ")", ":", "\n", "                ", "raise", "RuntimeError", "(", "\"Requires distributed package to be available\"", ")", "\n", "", "rank", "=", "dist", ".", "get_rank", "(", ")", "\n", "", "self", ".", "dataset", "=", "dataset", "\n", "self", ".", "num_replicas", "=", "num_replicas", "\n", "self", ".", "rank", "=", "rank", "\n", "self", ".", "epoch", "=", "0", "\n", "self", ".", "num_samples", "=", "int", "(", "math", ".", "ceil", "(", "len", "(", "self", ".", "dataset", ")", "*", "3.0", "/", "self", ".", "num_replicas", ")", ")", "\n", "self", ".", "total_size", "=", "self", ".", "num_samples", "*", "self", ".", "num_replicas", "\n", "# self.num_selected_samples = int(math.ceil(len(self.dataset) / self.num_replicas))", "\n", "self", ".", "num_selected_samples", "=", "int", "(", "math", ".", "floor", "(", "len", "(", "self", ".", "dataset", ")", "//", "256", "*", "256", "/", "self", ".", "num_replicas", ")", ")", "\n", "self", ".", "shuffle", "=", "shuffle", "\n", "\n"]], "home.repos.pwc.inspect_result.bychao100_towards-image-compression-and-analysis-with-transformers.pretrained_model.samplers.RASampler.__iter__": [[33, 52], ["torch.Generator", "torch.Generator", "torch.Generator", "torch.Generator", "torch.Generator.manual_seed", "torch.Generator.manual_seed", "iter", "torch.randperm().tolist", "torch.randperm().tolist", "torch.randperm().tolist", "torch.randperm().tolist", "list", "len", "len", "range", "range", "torch.randperm", "torch.randperm", "torch.randperm", "torch.randperm", "len", "len", "len"], "methods", ["None"], ["", "def", "__iter__", "(", "self", ")", ":", "\n", "# deterministically shuffle based on epoch", "\n", "        ", "g", "=", "torch", ".", "Generator", "(", ")", "\n", "g", ".", "manual_seed", "(", "self", ".", "epoch", ")", "\n", "if", "self", ".", "shuffle", ":", "\n", "            ", "indices", "=", "torch", ".", "randperm", "(", "len", "(", "self", ".", "dataset", ")", ",", "generator", "=", "g", ")", ".", "tolist", "(", ")", "\n", "", "else", ":", "\n", "            ", "indices", "=", "list", "(", "range", "(", "len", "(", "self", ".", "dataset", ")", ")", ")", "\n", "\n", "# add extra samples to make it evenly divisible", "\n", "", "indices", "=", "[", "ele", "for", "ele", "in", "indices", "for", "i", "in", "range", "(", "3", ")", "]", "\n", "indices", "+=", "indices", "[", ":", "(", "self", ".", "total_size", "-", "len", "(", "indices", ")", ")", "]", "\n", "assert", "len", "(", "indices", ")", "==", "self", ".", "total_size", "\n", "\n", "# subsample", "\n", "indices", "=", "indices", "[", "self", ".", "rank", ":", "self", ".", "total_size", ":", "self", ".", "num_replicas", "]", "\n", "assert", "len", "(", "indices", ")", "==", "self", ".", "num_samples", "\n", "\n", "return", "iter", "(", "indices", "[", ":", "self", ".", "num_selected_samples", "]", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.bychao100_towards-image-compression-and-analysis-with-transformers.pretrained_model.samplers.RASampler.__len__": [[53, 55], ["None"], "methods", ["None"], ["", "def", "__len__", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "num_selected_samples", "\n", "\n"]], "home.repos.pwc.inspect_result.bychao100_towards-image-compression-and-analysis-with-transformers.pretrained_model.samplers.RASampler.set_epoch": [[56, 58], ["None"], "methods", ["None"], ["", "def", "set_epoch", "(", "self", ",", "epoch", ")", ":", "\n", "        ", "self", ".", "epoch", "=", "epoch", "\n", "", "", ""]], "home.repos.pwc.inspect_result.bychao100_towards-image-compression-and-analysis-with-transformers.pretrained_model.main.get_args_parser": [[30, 171], ["argparse.ArgumentParser", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.set_defaults", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.set_defaults", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.set_defaults", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument"], "function", ["None"], ["2", ":", "(", "0.3", ",", "0.003", ")", ",", "\n", "3", ":", "(", "0.6", ",", "0.006", ")", "\n", "}", "\n", "\n", "def", "get_args_parser", "(", ")", ":", "\n", "    ", "parser", "=", "argparse", ".", "ArgumentParser", "(", "'JCCTransformer training and evaluation script'", ",", "add_help", "=", "False", ")", "\n", "parser", ".", "add_argument", "(", "'--batch-size'", ",", "default", "=", "64", ",", "type", "=", "int", ")", "\n", "parser", ".", "add_argument", "(", "'--epochs'", ",", "default", "=", "300", ",", "type", "=", "int", ")", "\n", "\n", "# Model parameters", "\n", "parser", ".", "add_argument", "(", "'--model'", ",", "default", "=", "'finetune_tiny_patch16_224'", ",", "type", "=", "str", ",", "metavar", "=", "'MODEL'", ",", "\n", "help", "=", "'Name of model to train'", ")", "\n", "parser", ".", "add_argument", "(", "'--input-size'", ",", "default", "=", "224", ",", "type", "=", "int", ",", "help", "=", "'images input size'", ")", "\n", "\n", "parser", ".", "add_argument", "(", "'--quality'", ",", "default", "=", "1", ",", "type", "=", "int", ",", "help", "=", "'quality'", ")", "\n", "\n", "parser", ".", "add_argument", "(", "'--drop'", ",", "type", "=", "float", ",", "default", "=", "0.0", ",", "metavar", "=", "'PCT'", ",", "\n", "help", "=", "'Dropout rate (default: 0.)'", ")", "\n", "parser", ".", "add_argument", "(", "'--drop-path'", ",", "type", "=", "float", ",", "default", "=", "0.1", ",", "metavar", "=", "'PCT'", ",", "\n", "help", "=", "'Drop path rate (default: 0.1)'", ")", "\n", "\n", "parser", ".", "add_argument", "(", "'--pretrained'", ",", "action", "=", "'store_true'", ")", "\n", "parser", ".", "add_argument", "(", "'--no-pretrained'", ",", "action", "=", "'store_false'", ",", "dest", "=", "'pretrained'", ")", "\n", "parser", ".", "set_defaults", "(", "pretrained", "=", "True", ")", "\n", "\n", "# Optimizer parameters", "\n", "parser", ".", "add_argument", "(", "'--opt'", ",", "default", "=", "'adam'", ",", "type", "=", "str", ",", "metavar", "=", "'OPTIMIZER'", ",", "\n", "help", "=", "'Optimizer (default: \"adam\"'", ")", "\n", "parser", ".", "add_argument", "(", "'--opt-eps'", ",", "default", "=", "1e-8", ",", "type", "=", "float", ",", "metavar", "=", "'EPSILON'", ",", "\n", "help", "=", "'Optimizer Epsilon (default: 1e-8)'", ")", "\n", "parser", ".", "add_argument", "(", "'--opt-betas'", ",", "default", "=", "None", ",", "type", "=", "float", ",", "nargs", "=", "'+'", ",", "metavar", "=", "'BETA'", ",", "\n", "help", "=", "'Optimizer Betas (default: None, use opt default)'", ")", "\n", "parser", ".", "add_argument", "(", "'--clip-grad'", ",", "type", "=", "float", ",", "default", "=", "None", ",", "metavar", "=", "'NORM'", ",", "\n", "help", "=", "'Clip gradient norm (default: None, no clipping)'", ")", "\n", "parser", ".", "add_argument", "(", "'--momentum'", ",", "type", "=", "float", ",", "default", "=", "0.9", ",", "metavar", "=", "'M'", ",", "\n", "help", "=", "'SGD momentum (default: 0.9)'", ")", "\n", "parser", ".", "add_argument", "(", "'--weight-decay'", ",", "type", "=", "float", ",", "default", "=", "0.", ",", "\n", "help", "=", "'weight decay (default: 0.)'", ")", "\n", "# Learning rate schedule parameters", "\n", "parser", ".", "add_argument", "(", "'--sched'", ",", "default", "=", "'cosine'", ",", "type", "=", "str", ",", "metavar", "=", "'SCHEDULER'", ",", "\n", "help", "=", "'LR scheduler (default: \"cosine\"'", ")", "\n", "parser", ".", "add_argument", "(", "'--lr'", ",", "type", "=", "float", ",", "default", "=", "1e-4", ",", "metavar", "=", "'LR'", ",", "\n", "help", "=", "'learning rate (default: 1e-4)'", ")", "\n", "parser", ".", "add_argument", "(", "'--aux-lr'", ",", "type", "=", "float", ",", "default", "=", "1e-3", ",", "metavar", "=", "'LR'", ",", "\n", "help", "=", "'aux learning rate (default: 1e-3)'", ")", "\n", "parser", ".", "add_argument", "(", "'--lr-noise'", ",", "type", "=", "float", ",", "nargs", "=", "'+'", ",", "default", "=", "None", ",", "metavar", "=", "'pct, pct'", ",", "\n", "help", "=", "'learning rate noise on/off epoch percentages'", ")", "\n", "parser", ".", "add_argument", "(", "'--lr-noise-pct'", ",", "type", "=", "float", ",", "default", "=", "0.67", ",", "metavar", "=", "'PERCENT'", ",", "\n", "help", "=", "'learning rate noise limit percent (default: 0.67)'", ")", "\n", "parser", ".", "add_argument", "(", "'--lr-noise-std'", ",", "type", "=", "float", ",", "default", "=", "1.0", ",", "metavar", "=", "'STDDEV'", ",", "\n", "help", "=", "'learning rate noise std-dev (default: 1.0)'", ")", "\n", "parser", ".", "add_argument", "(", "'--warmup-lr'", ",", "type", "=", "float", ",", "default", "=", "1e-6", ",", "metavar", "=", "'LR'", ",", "\n", "help", "=", "'warmup learning rate (default: 1e-6)'", ")", "\n", "parser", ".", "add_argument", "(", "'--min-lr'", ",", "type", "=", "float", ",", "default", "=", "1e-5", ",", "metavar", "=", "'LR'", ",", "\n", "help", "=", "'lower lr bound for cyclic schedulers that hit 0 (1e-5)'", ")", "\n", "\n", "parser", ".", "add_argument", "(", "'--decay-epochs'", ",", "type", "=", "float", ",", "default", "=", "30", ",", "metavar", "=", "'N'", ",", "\n", "help", "=", "'epoch interval to decay LR'", ")", "\n", "parser", ".", "add_argument", "(", "'--warmup-epochs'", ",", "type", "=", "int", ",", "default", "=", "5", ",", "metavar", "=", "'N'", ",", "\n", "help", "=", "'epochs to warmup LR, if scheduler supports'", ")", "\n", "parser", ".", "add_argument", "(", "'--cooldown-epochs'", ",", "type", "=", "int", ",", "default", "=", "10", ",", "metavar", "=", "'N'", ",", "\n", "help", "=", "'epochs to cooldown LR at min_lr, after cyclic schedule ends'", ")", "\n", "parser", ".", "add_argument", "(", "'--patience-epochs'", ",", "type", "=", "int", ",", "default", "=", "10", ",", "metavar", "=", "'N'", ",", "\n", "help", "=", "'patience epochs for Plateau LR scheduler (default: 10'", ")", "\n", "parser", ".", "add_argument", "(", "'--decay-rate'", ",", "'--dr'", ",", "type", "=", "float", ",", "default", "=", "0.1", ",", "metavar", "=", "'RATE'", ",", "\n", "help", "=", "'LR decay rate (default: 0.1)'", ")", "\n", "\n", "# Augmentation parameters", "\n", "parser", ".", "add_argument", "(", "'--smoothing'", ",", "type", "=", "float", ",", "default", "=", "0.1", ",", "help", "=", "'Label smoothing (default: 0.1)'", ")", "\n", "parser", ".", "add_argument", "(", "'--train-interpolation'", ",", "type", "=", "str", ",", "default", "=", "'bicubic'", ",", "\n", "help", "=", "'Training interpolation (random, bilinear, bicubic default: \"bicubic\")'", ")", "\n", "\n", "# * Finetuning params", "\n", "parser", ".", "add_argument", "(", "'--finetune'", ",", "default", "=", "''", ",", "help", "=", "'finetune from checkpoint'", ")", "\n", "\n", "# Dataset parameters", "\n", "parser", ".", "add_argument", "(", "'--data-path'", ",", "default", "=", "'/datasets01/imagenet_full_size/061417/'", ",", "type", "=", "str", ",", "\n", "help", "=", "'dataset path'", ")", "\n", "parser", ".", "add_argument", "(", "'--data-set'", ",", "default", "=", "'IMNET'", ",", "choices", "=", "[", "'IMNET'", ",", "'INAT19'", "]", ",", "\n", "type", "=", "str", ",", "help", "=", "'Image Net dataset path'", ")", "\n", "parser", ".", "add_argument", "(", "'--inat-category'", ",", "default", "=", "'name'", ",", "\n", "choices", "=", "[", "'kingdom'", ",", "'phylum'", ",", "'class'", ",", "'order'", ",", "'supercategory'", ",", "'family'", ",", "'genus'", ",", "'name'", "]", ",", "\n", "type", "=", "str", ",", "help", "=", "'semantic granularity'", ")", "\n", "\n", "parser", ".", "add_argument", "(", "'--output_dir'", ",", "default", "=", "''", ",", "\n", "help", "=", "'path where to save, empty for no saving'", ")", "\n", "parser", ".", "add_argument", "(", "'--device'", ",", "default", "=", "'cuda'", ",", "\n", "help", "=", "'device to use for training / testing'", ")", "\n", "parser", ".", "add_argument", "(", "'--seed'", ",", "default", "=", "0", ",", "type", "=", "int", ")", "\n", "parser", ".", "add_argument", "(", "'--resume'", ",", "default", "=", "''", ",", "help", "=", "'resume from checkpoint'", ")", "\n", "parser", ".", "add_argument", "(", "'--start_epoch'", ",", "default", "=", "0", ",", "type", "=", "int", ",", "metavar", "=", "'N'", ",", "\n", "help", "=", "'start epoch'", ")", "\n", "parser", ".", "add_argument", "(", "'--eval'", ",", "action", "=", "'store_true'", ",", "help", "=", "'Perform evaluation only'", ")", "\n", "parser", ".", "add_argument", "(", "'--dist-eval'", ",", "action", "=", "'store_true'", ",", "default", "=", "False", ",", "help", "=", "'Enabling distributed evaluation'", ")", "\n", "parser", ".", "add_argument", "(", "'--num_workers'", ",", "default", "=", "10", ",", "type", "=", "int", ")", "\n", "parser", ".", "add_argument", "(", "'--pin-mem'", ",", "action", "=", "'store_true'", ",", "\n", "help", "=", "'Pin CPU memory in DataLoader for more efficient (sometimes) transfer to GPU.'", ")", "\n", "parser", ".", "add_argument", "(", "'--no-pin-mem'", ",", "action", "=", "'store_false'", ",", "dest", "=", "'pin_mem'", ",", "\n", "help", "=", "''", ")", "\n", "parser", ".", "set_defaults", "(", "pin_mem", "=", "True", ")", "\n", "\n", "# distributed training parameters", "\n", "parser", ".", "add_argument", "(", "'--world_size'", ",", "default", "=", "1", ",", "type", "=", "int", ",", "\n", "help", "=", "'number of distributed processes'", ")", "\n", "parser", ".", "add_argument", "(", "'--dist_url'", ",", "default", "=", "'env://'", ",", "help", "=", "'url used to set up distributed training'", ")", "\n", "return", "parser", "\n", "\n", "\n", "", "def", "main", "(", "args", ")", ":", "\n", "    ", "utils", ".", "init_distributed_mode", "(", "args", ")", "\n", "\n", "print", "(", "args", ")", "\n", "\n", "device", "=", "torch", ".", "device", "(", "args", ".", "device", ")", "\n", "\n", "# fix the seed for reproducibility", "\n", "seed", "=", "args", ".", "seed", "+", "utils", ".", "get_rank", "(", ")", "\n", "torch", ".", "manual_seed", "(", "seed", ")", "\n", "np", ".", "random", ".", "seed", "(", "seed", ")", "\n", "\n", "cudnn", ".", "benchmark", "=", "True", "\n", "\n", "dataset_train", ",", "args", ".", "nb_classes", "=", "build_dataset", "(", "is_train", "=", "True", ",", "args", "=", "args", ")", "\n", "dataset_val", ",", "_", "=", "build_dataset", "(", "is_train", "=", "False", ",", "args", "=", "args", ")", "\n", "\n", "num_tasks", "=", "utils", ".", "get_world_size", "(", ")", "\n", "global_rank", "=", "utils", ".", "get_rank", "(", ")", "\n", "sampler_train", "=", "torch", ".", "utils", ".", "data", ".", "DistributedSampler", "(", "\n", "dataset_train", ",", "num_replicas", "=", "num_tasks", ",", "rank", "=", "global_rank", ",", "shuffle", "=", "True", "\n", ")", "\n", "if", "args", ".", "dist_eval", ":", "\n", "        ", "if", "len", "(", "dataset_val", ")", "%", "num_tasks", "!=", "0", ":", "\n", "            ", "print", "(", "'Warning: Enabling distributed evaluation with an eval dataset not divisible by process number. '", "\n", "'This will slightly alter validation results as extra duplicate entries are added to achieve '", "\n", "'equal num of samples per-process.'", ")", "\n", "", "sampler_val", "=", "torch", ".", "utils", ".", "data", ".", "DistributedSampler", "(", "\n", "dataset_val", ",", "num_replicas", "=", "num_tasks", ",", "rank", "=", "global_rank", ",", "shuffle", "=", "False", ")", "\n", "", "else", ":", "\n", "        ", "sampler_val", "=", "torch", ".", "utils", ".", "data", ".", "SequentialSampler", "(", "dataset_val", ")", "\n", "\n", "", "data_loader_train", "=", "torch", ".", "utils", ".", "data", ".", "DataLoader", "(", "\n", "dataset_train", ",", "sampler", "=", "sampler_train", ",", "\n"]], "home.repos.pwc.inspect_result.bychao100_towards-image-compression-and-analysis-with-transformers.pretrained_model.main.main": [[173, 424], ["utils.init_distributed_mode", "print", "torch.device", "torch.device", "torch.manual_seed", "torch.manual_seed", "numpy.random.seed", "datasets.build_dataset", "datasets.build_dataset", "torch.utils.data.DataLoader", "torch.utils.data.DataLoader", "torch.utils.data.DataLoader", "torch.utils.data.DataLoader", "print", "timm.models.create_model", "torch.nn.parallel.DistributedDataParallel.to", "sum", "print", "timm.optim.create_optimizer", "timm.utils.NativeScaler", "timm.scheduler.create_scheduler", "timm.loss.LabelSmoothingCrossEntropy", "losses.JointLoss", "pathlib.Path", "print", "time.time", "range", "str", "print", "NotImplementedError", "utils.get_rank", "utils.get_world_size", "utils.get_rank", "torch.utils.data.RandomSampler", "torch.utils.data.RandomSampler", "torch.utils.data.SequentialSampler", "torch.utils.data.SequentialSampler", "timm.data.Mixup", "args.finetune.startswith", "torch.nn.parallel.DistributedDataParallel.state_dict", "int", "int", "pos_tokens.permute().flatten.reshape().permute", "torch.nn.functional.interpolate", "torch.nn.functional.interpolate", "pos_tokens.permute().flatten.permute().flatten", "torch.cat", "torch.cat", "torch.nn.parallel.DistributedDataParallel.load_state_dict", "timm.utils.ModelEma", "torch.nn.parallel.DistributedDataParallel", "torch.nn.parallel.DistributedDataParallel", "timm.loss.SoftTargetCrossEntropy", "print", "timm.models.create_model", "args.teacher_path.startswith", "timm.models.create_model.load_state_dict", "timm.models.create_model.to", "timm.models.create_model.eval", "args.resume.startswith", "model_without_ddp.load_state_dict", "engine.evaluate", "print", "engine.train_one_epoch", "lr_scheduler.step", "engine.evaluate", "print", "max", "print", "time.time", "datetime.timedelta", "samplers.RASampler", "torch.utils.data.DistributedSampler", "torch.utils.data.DistributedSampler", "torch.utils.data.DistributedSampler", "torch.utils.data.DistributedSampler", "torch.utils.data.SequentialSampler", "torch.utils.data.SequentialSampler", "int", "torch.hub.load_state_dict_from_url", "torch.hub.load_state_dict_from_url", "torch.load", "torch.load", "p.numel", "utils.get_world_size", "timm.loss.LabelSmoothingCrossEntropy", "torch.nn.CrossEntropyLoss", "torch.nn.CrossEntropyLoss", "torch.hub.load_state_dict_from_url", "torch.hub.load_state_dict_from_url", "torch.load", "torch.load", "torch.hub.load_state_dict_from_url", "torch.hub.load_state_dict_from_url", "torch.load", "torch.load", "timm.optim.create_optimizer.load_state_dict", "lr_scheduler.load_state_dict", "os.path.exists", "os.makedirs", "torch.utils.data.DataLoader.sampler.set_epoch", "utils.is_main_process", "print", "print", "pos_tokens.permute().flatten.reshape", "pos_tokens.permute().flatten.permute", "torch.nn.parallel.DistributedDataParallel.parameters", "utils._load_checkpoint_for_ema", "timm.utils.NativeScaler.load_state_dict", "utils.save_on_master", "f.write", "int", "len", "len", "utils.save_on_master", "len", "engine.train_one_epoch.items", "engine.evaluate.items", "model_without_ddp.state_dict", "timm.optim.create_optimizer.state_dict", "lr_scheduler.state_dict", "timm.utils.get_state_dict", "timm.utils.NativeScaler.state_dict", "json.dumps", "model_without_ddp.state_dict", "timm.optim.create_optimizer.state_dict", "lr_scheduler.state_dict", "timm.utils.NativeScaler.state_dict"], "function", ["home.repos.pwc.inspect_result.bychao100_towards-image-compression-and-analysis-with-transformers.pretrained_model.utils.init_distributed_mode", "home.repos.pwc.inspect_result.bychao100_towards-image-compression-and-analysis-with-transformers.pretrained_model.datasets.build_dataset", "home.repos.pwc.inspect_result.bychao100_towards-image-compression-and-analysis-with-transformers.pretrained_model.datasets.build_dataset", "home.repos.pwc.inspect_result.bychao100_towards-image-compression-and-analysis-with-transformers.pretrained_model.utils.get_rank", "home.repos.pwc.inspect_result.bychao100_towards-image-compression-and-analysis-with-transformers.pretrained_model.utils.get_world_size", "home.repos.pwc.inspect_result.bychao100_towards-image-compression-and-analysis-with-transformers.pretrained_model.utils.get_rank", "home.repos.pwc.inspect_result.bychao100_towards-image-compression-and-analysis-with-transformers.full_model.model.JCCTransformer.load_state_dict", "home.repos.pwc.inspect_result.bychao100_towards-image-compression-and-analysis-with-transformers.full_model.model.JCCTransformer.load_state_dict", "home.repos.pwc.inspect_result.bychao100_towards-image-compression-and-analysis-with-transformers.full_model.model.JCCTransformer.load_state_dict", "home.repos.pwc.inspect_result.bychao100_towards-image-compression-and-analysis-with-transformers.pretrained_model.engine.evaluate", "home.repos.pwc.inspect_result.bychao100_towards-image-compression-and-analysis-with-transformers.pretrained_model.engine.train_one_epoch", "home.repos.pwc.inspect_result.bychao100_towards-image-compression-and-analysis-with-transformers.pretrained_model.engine.evaluate", "home.repos.pwc.inspect_result.bychao100_towards-image-compression-and-analysis-with-transformers.pretrained_model.utils.SmoothedValue.max", "home.repos.pwc.inspect_result.bychao100_towards-image-compression-and-analysis-with-transformers.pretrained_model.utils.get_world_size", "home.repos.pwc.inspect_result.bychao100_towards-image-compression-and-analysis-with-transformers.full_model.model.JCCTransformer.load_state_dict", "home.repos.pwc.inspect_result.bychao100_towards-image-compression-and-analysis-with-transformers.full_model.model.JCCTransformer.load_state_dict", "home.repos.pwc.inspect_result.bychao100_towards-image-compression-and-analysis-with-transformers.pretrained_model.samplers.RASampler.set_epoch", "home.repos.pwc.inspect_result.bychao100_towards-image-compression-and-analysis-with-transformers.pretrained_model.utils.is_main_process", "home.repos.pwc.inspect_result.bychao100_towards-image-compression-and-analysis-with-transformers.pretrained_model.utils._load_checkpoint_for_ema", "home.repos.pwc.inspect_result.bychao100_towards-image-compression-and-analysis-with-transformers.full_model.model.JCCTransformer.load_state_dict", "home.repos.pwc.inspect_result.bychao100_towards-image-compression-and-analysis-with-transformers.pretrained_model.utils.save_on_master", "home.repos.pwc.inspect_result.bychao100_towards-image-compression-and-analysis-with-transformers.pretrained_model.utils.save_on_master"], ["num_workers", "=", "args", ".", "num_workers", ",", "\n", "pin_memory", "=", "args", ".", "pin_mem", ",", "\n", "drop_last", "=", "True", ",", "\n", ")", "\n", "\n", "data_loader_val", "=", "torch", ".", "utils", ".", "data", ".", "DataLoader", "(", "\n", "dataset_val", ",", "sampler", "=", "sampler_val", ",", "\n", "batch_size", "=", "int", "(", "2.", "*", "args", ".", "batch_size", ")", ",", "\n", "num_workers", "=", "args", ".", "num_workers", ",", "\n", "pin_memory", "=", "args", ".", "pin_mem", ",", "\n", "drop_last", "=", "False", "\n", ")", "\n", "\n", "print", "(", "f\"Creating model: {args.model}\"", ")", "\n", "model", "=", "create_model", "(", "\n", "args", ".", "model", ",", "\n", "pretrained", "=", "args", ".", "pretrained", ",", "\n", "num_classes", "=", "args", ".", "nb_classes", ",", "\n", "drop_rate", "=", "args", ".", "drop", ",", "\n", "drop_path_rate", "=", "args", ".", "drop_path", ",", "\n", "drop_block_rate", "=", "None", ",", "\n", ")", "\n", "\n", "if", "args", ".", "finetune", ":", "\n", "        ", "if", "args", ".", "finetune", ".", "startswith", "(", "'https'", ")", ":", "\n", "            ", "checkpoint", "=", "torch", ".", "hub", ".", "load_state_dict_from_url", "(", "\n", "args", ".", "finetune", ",", "map_location", "=", "'cpu'", ",", "check_hash", "=", "True", ")", "\n", "", "else", ":", "\n", "            ", "checkpoint", "=", "torch", ".", "load", "(", "args", ".", "finetune", ",", "map_location", "=", "'cpu'", ")", "\n", "\n", "", "checkpoint_model", "=", "checkpoint", "[", "'model'", "]", "\n", "state_dict", "=", "model", ".", "state_dict", "(", ")", "\n", "for", "k", "in", "[", "'head.weight'", ",", "'head.bias'", ",", "'head_dist.weight'", ",", "'head_dist.bias'", "]", ":", "\n", "            ", "if", "k", "in", "checkpoint_model", "and", "checkpoint_model", "[", "k", "]", ".", "shape", "!=", "state_dict", "[", "k", "]", ".", "shape", ":", "\n", "                ", "print", "(", "f\"Removing key {k} from pretrained checkpoint\"", ")", "\n", "del", "checkpoint_model", "[", "k", "]", "\n", "\n", "# interpolate position embedding", "\n", "", "", "pos_embed_checkpoint", "=", "checkpoint_model", "[", "'pos_embed'", "]", "\n", "embedding_size", "=", "pos_embed_checkpoint", ".", "shape", "[", "-", "1", "]", "\n", "num_patches", "=", "model", ".", "patch_embed", ".", "num_patches", "\n", "num_extra_tokens", "=", "model", ".", "pos_embed", ".", "shape", "[", "-", "2", "]", "-", "num_patches", "\n", "# height (== width) for the checkpoint position embedding", "\n", "orig_size", "=", "int", "(", "(", "pos_embed_checkpoint", ".", "shape", "[", "-", "2", "]", "-", "num_extra_tokens", ")", "**", "0.5", ")", "\n", "# height (== width) for the new position embedding", "\n", "new_size", "=", "int", "(", "num_patches", "**", "0.5", ")", "\n", "# class_token and dist_token are kept unchanged", "\n", "extra_tokens", "=", "pos_embed_checkpoint", "[", ":", ",", ":", "num_extra_tokens", "]", "\n", "# only the position tokens are interpolated", "\n", "pos_tokens", "=", "pos_embed_checkpoint", "[", ":", ",", "num_extra_tokens", ":", "]", "\n", "pos_tokens", "=", "pos_tokens", ".", "reshape", "(", "-", "1", ",", "orig_size", ",", "orig_size", ",", "embedding_size", ")", ".", "permute", "(", "0", ",", "3", ",", "1", ",", "2", ")", "\n", "pos_tokens", "=", "torch", ".", "nn", ".", "functional", ".", "interpolate", "(", "\n", "pos_tokens", ",", "size", "=", "(", "new_size", ",", "new_size", ")", ",", "mode", "=", "'bicubic'", ",", "align_corners", "=", "False", ")", "\n", "pos_tokens", "=", "pos_tokens", ".", "permute", "(", "0", ",", "2", ",", "3", ",", "1", ")", ".", "flatten", "(", "1", ",", "2", ")", "\n", "new_pos_embed", "=", "torch", ".", "cat", "(", "(", "extra_tokens", ",", "pos_tokens", ")", ",", "dim", "=", "1", ")", "\n", "checkpoint_model", "[", "'pos_embed'", "]", "=", "new_pos_embed", "\n", "\n", "model", ".", "load_state_dict", "(", "checkpoint_model", ",", "strict", "=", "False", ")", "\n", "\n", "", "model", ".", "to", "(", "device", ")", "\n", "\n", "model_without_ddp", "=", "model", "\n", "if", "args", ".", "distributed", ":", "\n", "        ", "model", "=", "torch", ".", "nn", ".", "parallel", ".", "DistributedDataParallel", "(", "model", ",", "device_ids", "=", "[", "args", ".", "gpu", "]", ")", "\n", "model_without_ddp", "=", "model", ".", "module", "\n", "", "n_parameters", "=", "sum", "(", "p", ".", "numel", "(", ")", "for", "p", "in", "model", ".", "parameters", "(", ")", "if", "p", ".", "requires_grad", ")", "\n", "print", "(", "'number of params:'", ",", "n_parameters", ")", "\n", "\n", "optimizer", ",", "aux_optimizer", "=", "configure_optimizers", "(", "args", ",", "model_without_ddp", ")", "\n", "\n", "lr_scheduler", ",", "_", "=", "create_scheduler", "(", "args", ",", "optimizer", ")", "\n", "\n", "if", "args", ".", "smoothing", ":", "\n", "        ", "criterion", "=", "LabelSmoothingCrossEntropy", "(", "smoothing", "=", "args", ".", "smoothing", ")", "\n", "", "else", ":", "\n", "        ", "criterion", "=", "torch", ".", "nn", ".", "CrossEntropyLoss", "(", ")", "\n", "\n", "", "alpha", "=", "alpha_beta", "[", "args", ".", "quality", "]", "[", "0", "]", "\n", "beta", "=", "alpha_beta", "[", "args", ".", "quality", "]", "[", "1", "]", "\n", "criterion", "=", "JointLoss", "(", "criterion", ",", "alpha", "=", "alpha", ",", "beta", "=", "beta", ")", "\n", "\n", "output_dir", "=", "Path", "(", "args", ".", "output_dir", ")", "\n", "if", "args", ".", "resume", ":", "\n", "        ", "if", "args", ".", "resume", ".", "startswith", "(", "'https'", ")", ":", "\n", "            ", "checkpoint", "=", "torch", ".", "hub", ".", "load_state_dict_from_url", "(", "\n", "args", ".", "resume", ",", "map_location", "=", "'cpu'", ",", "check_hash", "=", "True", ")", "\n", "", "else", ":", "\n", "            ", "checkpoint", "=", "torch", ".", "load", "(", "args", ".", "resume", ",", "map_location", "=", "'cpu'", ")", "\n", "", "model_without_ddp", ".", "load_state_dict", "(", "checkpoint", "[", "'model'", "]", ")", "\n", "if", "not", "args", ".", "eval", "and", "'optimizer'", "in", "checkpoint", "and", "'lr_scheduler'", "in", "checkpoint", "and", "'epoch'", "in", "checkpoint", ":", "\n", "            ", "optimizer", ".", "load_state_dict", "(", "checkpoint", "[", "'optimizer'", "]", ")", "\n", "aux_optimizer", ".", "load_state_dict", "(", "checkpoint", "[", "'aux_optimizer'", "]", ")", "\n", "lr_scheduler", ".", "load_state_dict", "(", "checkpoint", "[", "'lr_scheduler'", "]", ")", "\n", "args", ".", "start_epoch", "=", "checkpoint", "[", "'epoch'", "]", "+", "1", "\n", "\n", "", "", "if", "args", ".", "eval", ":", "\n", "        ", "test_stats", "=", "evaluate_real", "(", "data_loader_val", ",", "model", ",", "device", ",", "output_dir", ")", "\n", "print", "(", "f\"Accuracy of the network on the {len(dataset_val)} test images: {test_stats['acc1']:.1f}%\"", ")", "\n", "return", "\n", "\n", "\n", "", "print", "(", "f\"Start training for {args.epochs} epochs\"", ")", "\n", "start_time", "=", "time", ".", "time", "(", ")", "\n", "max_accuracy", "=", "0.0", "\n", "for", "epoch", "in", "range", "(", "args", ".", "start_epoch", ",", "args", ".", "epochs", ")", ":", "\n", "        ", "if", "args", ".", "distributed", ":", "\n", "            ", "data_loader_train", ".", "sampler", ".", "set_epoch", "(", "epoch", ")", "\n", "\n", "", "train_stats", "=", "train_one_epoch", "(", "\n", "model", ",", "criterion", ",", "data_loader_train", ",", "\n", "optimizer", ",", "aux_optimizer", ",", "device", ",", "epoch", ",", "\n", "args", ".", "clip_grad", ",", "set_training_mode", "=", "args", ".", "finetune", "==", "''", "# keep in eval mode during finetuning", "\n", ")", "\n", "\n", "lr_scheduler", ".", "step", "(", "epoch", ")", "\n", "if", "args", ".", "output_dir", ":", "\n", "            ", "checkpoint_paths", "=", "[", "output_dir", "/", "'checkpoint.pth'", "]", "\n", "for", "checkpoint_path", "in", "checkpoint_paths", ":", "\n", "                ", "utils", ".", "save_on_master", "(", "{", "\n", "'model'", ":", "model_without_ddp", ".", "state_dict", "(", ")", ",", "\n", "'optimizer'", ":", "optimizer", ".", "state_dict", "(", ")", ",", "\n", "'aux_optimizer'", ":", "aux_optimizer", ".", "state_dict", "(", ")", ",", "\n", "'lr_scheduler'", ":", "lr_scheduler", ".", "state_dict", "(", ")", ",", "\n", "'epoch'", ":", "epoch", ",", "\n", "'args'", ":", "args", ",", "\n", "}", ",", "checkpoint_path", ")", "\n", "\n", "", "", "test_stats", "=", "evaluate", "(", "data_loader_val", ",", "model", ",", "device", ",", "output_dir", ")", "\n", "print", "(", "f\"Accuracy of the network on the {len(dataset_val)} test images: {test_stats['acc1']:.1f}%\"", ")", "\n", "max_accuracy", "=", "max", "(", "max_accuracy", ",", "test_stats", "[", "\"acc1\"", "]", ")", "\n", "print", "(", "f'Max accuracy: {max_accuracy:.2f}%'", ")", "\n", "\n", "log_stats", "=", "{", "**", "{", "f'train_{k}'", ":", "v", "for", "k", ",", "v", "in", "train_stats", ".", "items", "(", ")", "}", ",", "\n", "**", "{", "f'test_{k}'", ":", "v", "for", "k", ",", "v", "in", "test_stats", ".", "items", "(", ")", "}", ",", "\n", "'epoch'", ":", "epoch", ",", "\n", "'n_parameters'", ":", "n_parameters", "}", "\n", "\n", "if", "args", ".", "output_dir", "and", "utils", ".", "is_main_process", "(", ")", ":", "\n", "            ", "with", "(", "output_dir", "/", "\"log.txt\"", ")", ".", "open", "(", "\"a\"", ")", "as", "f", ":", "\n", "                ", "f", ".", "write", "(", "json", ".", "dumps", "(", "log_stats", ")", "+", "\"\\n\"", ")", "\n", "\n", "", "", "", "total_time", "=", "time", ".", "time", "(", ")", "-", "start_time", "\n", "total_time_str", "=", "str", "(", "datetime", ".", "timedelta", "(", "seconds", "=", "int", "(", "total_time", ")", ")", ")", "\n", "print", "(", "'Training time {}'", ".", "format", "(", "total_time_str", ")", ")", "\n", "\n", "\n", "", "if", "__name__", "==", "'__main__'", ":", "\n", "    ", "parser", "=", "argparse", ".", "ArgumentParser", "(", "'JCCTransformer training and evaluation script'", ",", "parents", "=", "[", "get_args_parser", "(", ")", "]", ")", "\n", "args", "=", "parser", ".", "parse_args", "(", ")", "\n", "if", "args", ".", "output_dir", ":", "\n", "        ", "Path", "(", "args", ".", "output_dir", ")", ".", "mkdir", "(", "parents", "=", "True", ",", "exist_ok", "=", "True", ")", "\n", "", "main", "(", "args", ")", "\n", "", ""]], "home.repos.pwc.inspect_result.bychao100_towards-image-compression-and-analysis-with-transformers.pretrained_model.model.Img2Embed.__init__": [[51, 69], ["torch.Module.__init__", "timm.models.layers.to_2tuple", "timm.models.layers.to_2tuple", "torch.Sequential", "torch.Sequential", "model.conv", "torch.LeakyReLU", "torch.LeakyReLU", "model.conv", "torch.LeakyReLU", "torch.LeakyReLU", "model.conv", "torch.LeakyReLU", "torch.LeakyReLU", "model.conv"], "methods", ["home.repos.pwc.inspect_result.bychao100_towards-image-compression-and-analysis-with-transformers.pretrained_model.datasets.INatDataset.__init__", "home.repos.pwc.inspect_result.bychao100_towards-image-compression-and-analysis-with-transformers.pretrained_model.model.conv", "home.repos.pwc.inspect_result.bychao100_towards-image-compression-and-analysis-with-transformers.pretrained_model.model.conv", "home.repos.pwc.inspect_result.bychao100_towards-image-compression-and-analysis-with-transformers.pretrained_model.model.conv", "home.repos.pwc.inspect_result.bychao100_towards-image-compression-and-analysis-with-transformers.pretrained_model.model.conv"], ["\n", "", "def", "deconv", "(", "in_channels", ",", "out_channels", ",", "kernel_size", "=", "5", ",", "stride", "=", "2", ")", ":", "\n", "    ", "return", "nn", ".", "ConvTranspose2d", "(", "\n", "in_channels", ",", "\n", "out_channels", ",", "\n", "kernel_size", "=", "kernel_size", ",", "\n", "stride", "=", "stride", ",", "\n", "output_padding", "=", "stride", "-", "1", ",", "\n", "padding", "=", "kernel_size", "//", "2", ",", "\n", ")", "\n", "\n", "\n", "", "class", "Img2Embed", "(", "nn", ".", "Module", ")", ":", "\n", "    ", "\"\"\" Encoding Image to Embedding\n    \"\"\"", "\n", "def", "__init__", "(", "self", ",", "img_size", "=", "224", ",", "patch_size", "=", "16", ",", "in_chans", "=", "3", ",", "embed_dim", "=", "768", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "img_size", "=", "to_2tuple", "(", "img_size", ")", "\n", "patch_size", "=", "to_2tuple", "(", "patch_size", ")", "\n"]], "home.repos.pwc.inspect_result.bychao100_towards-image-compression-and-analysis-with-transformers.pretrained_model.model.Img2Embed.forward": [[71, 78], ["model.Img2Embed.proj().flatten().transpose", "model.Img2Embed.proj().flatten", "model.Img2Embed.proj"], "methods", ["None"], ["self", ".", "img_size", "=", "img_size", "\n", "self", ".", "patch_size", "=", "patch_size", "\n", "self", ".", "num_patches", "=", "num_patches", "\n", "\n", "middle_chans", "=", "[", "128", ",", "128", ",", "128", "]", "\n", "self", ".", "proj", "=", "nn", ".", "Sequential", "(", "\n", "conv", "(", "in_chans", ",", "middle_chans", "[", "0", "]", ")", ",", "\n", "nn", ".", "LeakyReLU", "(", "inplace", "=", "True", ")", ",", "\n"]], "home.repos.pwc.inspect_result.bychao100_towards-image-compression-and-analysis-with-transformers.pretrained_model.model.Embed2Img.__init__": [[83, 103], ["torch.Module.__init__", "timm.models.layers.to_2tuple", "timm.models.layers.to_2tuple", "torch.Sequential", "torch.Sequential", "model.deconv", "torch.LeakyReLU", "torch.LeakyReLU", "model.deconv", "torch.LeakyReLU", "torch.LeakyReLU", "model.deconv", "torch.LeakyReLU", "torch.LeakyReLU", "model.deconv"], "methods", ["home.repos.pwc.inspect_result.bychao100_towards-image-compression-and-analysis-with-transformers.pretrained_model.datasets.INatDataset.__init__", "home.repos.pwc.inspect_result.bychao100_towards-image-compression-and-analysis-with-transformers.pretrained_model.model.deconv", "home.repos.pwc.inspect_result.bychao100_towards-image-compression-and-analysis-with-transformers.pretrained_model.model.deconv", "home.repos.pwc.inspect_result.bychao100_towards-image-compression-and-analysis-with-transformers.pretrained_model.model.deconv", "home.repos.pwc.inspect_result.bychao100_towards-image-compression-and-analysis-with-transformers.pretrained_model.model.deconv"], ["conv", "(", "middle_chans", "[", "2", "]", ",", "embed_dim", ")", "\n", ")", "\n", "\n", "", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "B", ",", "C", ",", "H", ",", "W", "=", "x", ".", "shape", "\n", "# FIXME look at relaxing size constraints", "\n", "assert", "H", "==", "self", ".", "img_size", "[", "0", "]", "and", "W", "==", "self", ".", "img_size", "[", "1", "]", ",", "f\"Input image size ({H}*{W}) doesn't match model ({self.img_size[0]}*{self.img_size[1]}).\"", "\n", "x", "=", "self", ".", "proj", "(", "x", ")", ".", "flatten", "(", "2", ")", ".", "transpose", "(", "1", ",", "2", ")", "\n", "return", "x", "\n", "\n", "\n", "", "", "class", "Embed2Img", "(", "nn", ".", "Module", ")", ":", "\n", "    ", "\"\"\" Decode Embedding to Image\n    \"\"\"", "\n", "def", "__init__", "(", "self", ",", "img_size", "=", "224", ",", "patch_size", "=", "16", ",", "out_chans", "=", "3", ",", "embed_dim", "=", "768", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "img_size", "=", "to_2tuple", "(", "img_size", ")", "\n", "patch_size", "=", "to_2tuple", "(", "patch_size", ")", "\n", "embed_size", "=", "(", "img_size", "[", "1", "]", "//", "patch_size", "[", "1", "]", ",", "img_size", "[", "0", "]", "//", "patch_size", "[", "0", "]", ")", "\n", "num_patches", "=", "embed_size", "[", "0", "]", "*", "embed_size", "[", "1", "]", "\n"]], "home.repos.pwc.inspect_result.bychao100_towards-image-compression-and-analysis-with-transformers.pretrained_model.model.Embed2Img.forward": [[105, 111], ["model.Embed2Img.proj", "model.Embed2Img.transpose().reshape", "model.Embed2Img.transpose"], "methods", ["None"], ["self", ".", "patch_size", "=", "patch_size", "\n", "self", ".", "embed_size", "=", "embed_size", "\n", "self", ".", "num_patches", "=", "num_patches", "\n", "\n", "middle_chans", "=", "[", "128", ",", "128", ",", "128", "]", "\n", "self", ".", "proj", "=", "nn", ".", "Sequential", "(", "\n", "deconv", "(", "embed_dim", ",", "middle_chans", "[", "0", "]", ")", ",", "\n"]], "home.repos.pwc.inspect_result.bychao100_towards-image-compression-and-analysis-with-transformers.pretrained_model.model.Mlp.__init__": [[114, 122], ["torch.Module.__init__", "torch.Linear", "torch.Linear", "act_layer", "torch.Linear", "torch.Linear", "torch.Dropout", "torch.Dropout"], "methods", ["home.repos.pwc.inspect_result.bychao100_towards-image-compression-and-analysis-with-transformers.pretrained_model.datasets.INatDataset.__init__"], ["nn", ".", "LeakyReLU", "(", "inplace", "=", "True", ")", ",", "\n", "deconv", "(", "middle_chans", "[", "1", "]", ",", "middle_chans", "[", "2", "]", ")", ",", "\n", "nn", ".", "LeakyReLU", "(", "inplace", "=", "True", ")", ",", "\n", "deconv", "(", "middle_chans", "[", "2", "]", ",", "out_chans", ")", "\n", ")", "\n", "\n", "", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "B", ",", "HW", ",", "C", "=", "x", ".", "shape", "\n", "assert", "HW", "==", "self", ".", "num_patches", ","]], "home.repos.pwc.inspect_result.bychao100_towards-image-compression-and-analysis-with-transformers.pretrained_model.model.Mlp.forward": [[123, 130], ["model.Mlp.fc1", "model.Mlp.act", "model.Mlp.drop", "model.Mlp.fc2", "model.Mlp.drop"], "methods", ["None"], ["f\"Input embeding size ({HW}) doesn't match patches size ({self.num_patches}).\"", "\n", "x", "=", "self", ".", "proj", "(", "x", ".", "transpose", "(", "1", ",", "2", ")", ".", "reshape", "(", "B", ",", "C", ",", "self", ".", "embed_size", "[", "0", "]", ",", "self", ".", "embed_size", "[", "1", "]", ")", ")", "\n", "return", "x", "\n", "\n", "\n", "", "", "class", "Mlp", "(", "nn", ".", "Module", ")", ":", "\n", "    ", "def", "__init__", "(", "self", ",", "in_features", ",", "hidden_features", "=", "None", ",", "out_features", "=", "None", ",", "act_layer", "=", "nn", ".", "GELU", ",", "drop", "=", "0.", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n"]], "home.repos.pwc.inspect_result.bychao100_towards-image-compression-and-analysis-with-transformers.pretrained_model.model.Attention.__init__": [[133, 144], ["torch.Module.__init__", "torch.Linear", "torch.Linear", "torch.Dropout", "torch.Dropout", "torch.Linear", "torch.Linear", "torch.Dropout", "torch.Dropout"], "methods", ["home.repos.pwc.inspect_result.bychao100_towards-image-compression-and-analysis-with-transformers.pretrained_model.datasets.INatDataset.__init__"], ["self", ".", "fc1", "=", "nn", ".", "Linear", "(", "in_features", ",", "hidden_features", ")", "\n", "self", ".", "act", "=", "act_layer", "(", ")", "\n", "self", ".", "fc2", "=", "nn", ".", "Linear", "(", "hidden_features", ",", "out_features", ")", "\n", "self", ".", "drop", "=", "nn", ".", "Dropout", "(", "drop", ")", "\n", "\n", "", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "x", "=", "self", ".", "fc1", "(", "x", ")", "\n", "x", "=", "self", ".", "act", "(", "x", ")", "\n", "x", "=", "self", ".", "drop", "(", "x", ")", "\n", "x", "=", "self", ".", "fc2", "(", "x", ")", "\n", "x", "=", "self", ".", "drop", "(", "x", ")", "\n", "return", "x", "\n"]], "home.repos.pwc.inspect_result.bychao100_towards-image-compression-and-analysis-with-transformers.pretrained_model.model.Attention.forward": [[145, 158], ["model.Attention.qkv().reshape().permute", "model.Attention.softmax", "model.Attention.attn_drop", "model.Attention.proj", "model.Attention.proj_drop", "model.Attention.qkv().reshape", "k.transpose", "model.Attention.qkv"], "methods", ["None"], ["\n", "\n", "", "", "class", "Attention", "(", "nn", ".", "Module", ")", ":", "\n", "    ", "def", "__init__", "(", "self", ",", "dim", ",", "num_heads", "=", "8", ",", "qkv_bias", "=", "False", ",", "qk_scale", "=", "None", ",", "attn_drop", "=", "0.", ",", "proj_drop", "=", "0.", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "num_heads", "=", "num_heads", "\n", "head_dim", "=", "dim", "//", "num_heads", "\n", "# NOTE scale factor was wrong in my original version, can set manually to be compat with prev weights", "\n", "self", ".", "scale", "=", "qk_scale", "or", "head_dim", "**", "-", "0.5", "\n", "\n", "self", ".", "qkv", "=", "nn", ".", "Linear", "(", "dim", ",", "dim", "*", "3", ",", "bias", "=", "qkv_bias", ")", "\n", "self", ".", "attn_drop", "=", "nn", ".", "Dropout", "(", "attn_drop", ")", "\n", "self", ".", "proj", "=", "nn", ".", "Linear", "(", "dim", ",", "dim", ")", "\n", "self", ".", "proj_drop", "=", "nn", ".", "Dropout", "(", "proj_drop", ")", "\n"]], "home.repos.pwc.inspect_result.bychao100_towards-image-compression-and-analysis-with-transformers.pretrained_model.model.Block.__init__": [[162, 173], ["torch.Module.__init__", "norm_layer", "model.Attention", "norm_layer", "int", "model.Mlp", "timm.models.layers.DropPath", "torch.Identity", "torch.Identity"], "methods", ["home.repos.pwc.inspect_result.bychao100_towards-image-compression-and-analysis-with-transformers.pretrained_model.datasets.INatDataset.__init__"], ["qkv", "=", "self", ".", "qkv", "(", "x", ")", ".", "reshape", "(", "B", ",", "N", ",", "3", ",", "self", ".", "num_heads", ",", "C", "//", "self", ".", "num_heads", ")", ".", "permute", "(", "2", ",", "0", ",", "3", ",", "1", ",", "4", ")", "\n", "q", ",", "k", ",", "v", "=", "qkv", "[", "0", "]", ",", "qkv", "[", "1", "]", ",", "qkv", "[", "2", "]", "# make torchscript happy (cannot use tensor as tuple)", "\n", "\n", "attn", "=", "(", "q", "@", "k", ".", "transpose", "(", "-", "2", ",", "-", "1", ")", ")", "*", "self", ".", "scale", "\n", "attn", "=", "attn", ".", "softmax", "(", "dim", "=", "-", "1", ")", "\n", "attn", "=", "self", ".", "attn_drop", "(", "attn", ")", "\n", "\n", "x", "=", "(", "attn", "@", "v", ")", ".", "transpose", "(", "1", ",", "2", ")", ".", "reshape", "(", "B", ",", "N", ",", "C", ")", "\n", "x", "=", "self", ".", "proj", "(", "x", ")", "\n", "x", "=", "self", ".", "proj_drop", "(", "x", ")", "\n", "return", "x", "\n", "\n"]], "home.repos.pwc.inspect_result.bychao100_towards-image-compression-and-analysis-with-transformers.pretrained_model.model.Block.forward": [[174, 178], ["model.Block.drop_path", "model.Block.drop_path", "model.Block.attn", "model.Block.mlp", "model.Block.norm1", "model.Block.norm2"], "methods", ["None"], ["\n", "", "", "class", "Block", "(", "nn", ".", "Module", ")", ":", "\n", "\n", "    ", "def", "__init__", "(", "self", ",", "dim", ",", "num_heads", ",", "mlp_ratio", "=", "4.", ",", "qkv_bias", "=", "False", ",", "qk_scale", "=", "None", ",", "drop", "=", "0.", ",", "attn_drop", "=", "0.", ",", "\n", "drop_path", "=", "0.", ",", "act_layer", "=", "nn", ".", "GELU", ",", "norm_layer", "=", "nn", ".", "LayerNorm", ")", ":", "\n"]], "home.repos.pwc.inspect_result.bychao100_towards-image-compression-and-analysis-with-transformers.pretrained_model.model.PretrainTransformer.__init__": [[183, 223], ["torch.Module.__init__", "model.Img2Embed", "torch.Linear", "torch.Linear", "torch.Parameter", "torch.Parameter", "torch.Parameter", "torch.Parameter", "torch.Dropout", "torch.Dropout", "torch.ModuleList", "torch.ModuleList", "norm_layer", "model.Embed2Img", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "timm.models.layers.trunc_normal_", "timm.models.layers.trunc_normal_", "model.PretrainTransformer.apply", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "x.item", "torch.Linear", "torch.Linear", "torch.Identity", "torch.Identity", "torch.linspace", "torch.linspace", "torch.linspace", "torch.linspace", "model.Block", "range"], "methods", ["home.repos.pwc.inspect_result.bychao100_towards-image-compression-and-analysis-with-transformers.pretrained_model.datasets.INatDataset.__init__"], ["# NOTE: drop path for stochastic depth, we shall see if this is better than dropout here", "\n", "self", ".", "drop_path", "=", "DropPath", "(", "drop_path", ")", "if", "drop_path", ">", "0.", "else", "nn", ".", "Identity", "(", ")", "\n", "self", ".", "norm2", "=", "norm_layer", "(", "dim", ")", "\n", "mlp_hidden_dim", "=", "int", "(", "dim", "*", "mlp_ratio", ")", "\n", "self", ".", "mlp", "=", "Mlp", "(", "in_features", "=", "dim", ",", "hidden_features", "=", "mlp_hidden_dim", ",", "act_layer", "=", "act_layer", ",", "drop", "=", "drop", ")", "\n", "\n", "", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "x", "=", "x", "+", "self", ".", "drop_path", "(", "self", ".", "attn", "(", "self", ".", "norm1", "(", "x", ")", ")", ")", "\n", "x", "=", "x", "+", "self", ".", "drop_path", "(", "self", ".", "mlp", "(", "self", ".", "norm2", "(", "x", ")", ")", ")", "\n", "return", "x", "\n", "\n", "\n", "", "", "class", "JCCTransformer", "(", "nn", ".", "Module", ")", ":", "\n", "    ", "\"\"\" Joint Compression and Classification Transformer\n    \"\"\"", "\n", "def", "__init__", "(", "self", ",", "img_size", "=", "224", ",", "patch_size", "=", "16", ",", "in_chans", "=", "3", ",", "num_classes", "=", "1000", ",", "embed_dim", "=", "768", ",", "depth", "=", "12", ",", "\n", "num_heads", "=", "12", ",", "mlp_ratio", "=", "4.", ",", "qkv_bias", "=", "False", ",", "qk_scale", "=", "None", ",", "drop_rate", "=", "0.", ",", "attn_drop_rate", "=", "0.", ",", "\n", "drop_path_rate", "=", "0.", ",", "norm_layer", "=", "nn", ".", "LayerNorm", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "num_classes", "=", "num_classes", "\n", "self", ".", "num_features", "=", "self", ".", "embed_dim", "=", "embed_dim", "# num_features for consistency with other models", "\n", "self", ".", "depth", "=", "depth", "\n", "\n", "self", ".", "patch_embed", "=", "Img2Embed", "(", "img_size", "=", "img_size", ",", "patch_size", "=", "patch_size", ",", "in_chans", "=", "in_chans", ",", "embed_dim", "=", "192", ")", "\n", "self", ".", "chans_embed", "=", "nn", ".", "Linear", "(", "192", ",", "embed_dim", ")", "\n", "num_patches", "=", "self", ".", "patch_embed", ".", "num_patches", "\n", "\n", "self", ".", "cls_token", "=", "nn", ".", "Parameter", "(", "torch", ".", "zeros", "(", "1", ",", "1", ",", "embed_dim", ")", ")", "\n", "self", ".", "pos_embed", "=", "nn", ".", "Parameter", "(", "torch", ".", "zeros", "(", "1", ",", "num_patches", "+", "1", ",", "embed_dim", ")", ")", "\n", "self", ".", "pos_drop", "=", "nn", ".", "Dropout", "(", "p", "=", "drop_rate", ")", "\n", "\n", "dpr", "=", "[", "x", ".", "item", "(", ")", "for", "x", "in", "torch", ".", "linspace", "(", "0", ",", "drop_path_rate", ",", "depth", ")", "]", "# stochastic depth decay rule", "\n", "self", ".", "blocks", "=", "nn", ".", "ModuleList", "(", "[", "\n", "Block", "(", "\n", "dim", "=", "embed_dim", ",", "num_heads", "=", "num_heads", ",", "mlp_ratio", "=", "mlp_ratio", ",", "qkv_bias", "=", "qkv_bias", ",", "qk_scale", "=", "qk_scale", ",", "\n", "drop", "=", "drop_rate", ",", "attn_drop", "=", "attn_drop_rate", ",", "drop_path", "=", "dpr", "[", "i", "]", ",", "norm_layer", "=", "norm_layer", ")", "\n", "for", "i", "in", "range", "(", "depth", ")", "]", ")", "\n", "self", ".", "norm", "=", "norm_layer", "(", "embed_dim", ")", "\n", "\n", "# Classifier head", "\n", "self", ".", "head", "=", "nn", ".", "Linear", "(", "embed_dim", ",", "num_classes", ")", "if", "num_classes", ">", "0", "else", "nn", ".", "Identity", "(", ")", "\n"]], "home.repos.pwc.inspect_result.bychao100_towards-image-compression-and-analysis-with-transformers.pretrained_model.model.PretrainTransformer._init_weights": [[224, 232], ["isinstance", "timm.models.layers.trunc_normal_", "isinstance", "isinstance", "torch.init.constant_", "torch.init.constant_", "torch.init.constant_", "torch.init.constant_", "torch.init.constant_", "torch.init.constant_"], "methods", ["None"], ["\n", "# Reconstruction head", "\n", "self", ".", "head_rec", "=", "Embed2Img", "(", "img_size", "=", "img_size", ",", "patch_size", "=", "patch_size", ",", "out_chans", "=", "in_chans", ",", "embed_dim", "=", "embed_dim", ")", "\n", "\n", "# fusion", "\n", "self", ".", "fusion0", "=", "nn", ".", "Linear", "(", "embed_dim", ",", "embed_dim", "//", "4", ")", "\n", "self", ".", "fusion1", "=", "nn", ".", "Linear", "(", "embed_dim", ",", "embed_dim", "//", "4", ")", "\n", "self", ".", "fusion2", "=", "nn", ".", "Linear", "(", "embed_dim", ",", "embed_dim", "//", "4", ")", "\n", "self", ".", "fusion3", "=", "nn", ".", "Linear", "(", "embed_dim", ",", "embed_dim", "//", "4", ")", "\n"]], "home.repos.pwc.inspect_result.bychao100_towards-image-compression-and-analysis-with-transformers.pretrained_model.model.PretrainTransformer.no_weight_decay": [[233, 236], ["None"], "methods", ["None"], ["self", ".", "fusion", "=", "nn", ".", "Linear", "(", "embed_dim", ",", "embed_dim", ")", "\n", "\n", "# Compression model", "\n", "hyper_dim", "=", "128", "\n"]], "home.repos.pwc.inspect_result.bychao100_towards-image-compression-and-analysis-with-transformers.pretrained_model.model.PretrainTransformer.get_classifier": [[237, 239], ["None"], "methods", ["None"], ["self", ".", "entropy_bottleneck", "=", "EntropyBottleneck", "(", "hyper_dim", ")", "\n", "self", ".", "h_a", "=", "nn", ".", "Sequential", "(", "\n", "conv", "(", "192", ",", "hyper_dim", ",", "stride", "=", "1", ",", "kernel_size", "=", "3", ")", ",", "\n"]], "home.repos.pwc.inspect_result.bychao100_towards-image-compression-and-analysis-with-transformers.pretrained_model.model.PretrainTransformer.reset_classifier": [[240, 243], ["torch.Linear", "torch.Linear", "torch.Identity", "torch.Identity"], "methods", ["None"], ["nn", ".", "LeakyReLU", "(", "inplace", "=", "True", ")", ",", "\n", "conv", "(", "hyper_dim", ",", "hyper_dim", ")", ",", "\n", "nn", ".", "LeakyReLU", "(", "inplace", "=", "True", ")", ",", "\n", "conv", "(", "hyper_dim", ",", "hyper_dim", ",", "stride", "=", "1", ")", ",", "\n"]], "home.repos.pwc.inspect_result.bychao100_towards-image-compression-and-analysis-with-transformers.pretrained_model.model.PretrainTransformer.forward_features": [[244, 278], ["model.PretrainTransformer.patch_embed", "model.PretrainTransformer.chans_embed", "model.PretrainTransformer.cls_token.expand", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "model.PretrainTransformer.pos_drop", "model.PretrainTransformer.norm", "model.PretrainTransformer.fusion0", "model.PretrainTransformer.fusion1", "model.PretrainTransformer.fusion2", "model.PretrainTransformer.fusion3", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "model.PretrainTransformer.fusion"], "methods", ["None"], [")", "\n", "self", ".", "h_s", "=", "nn", ".", "Sequential", "(", "\n", "conv", "(", "hyper_dim", ",", "192", ",", "stride", "=", "1", ")", ",", "\n", "nn", ".", "LeakyReLU", "(", "inplace", "=", "True", ")", ",", "\n", "deconv", "(", "192", ",", "192", "*", "3", "//", "2", ")", ",", "\n", "nn", ".", "LeakyReLU", "(", "inplace", "=", "True", ")", ",", "\n", "conv", "(", "192", "*", "3", "//", "2", ",", "192", "*", "2", ",", "stride", "=", "1", ",", "kernel_size", "=", "3", ")", ",", "\n", ")", "\n", "self", ".", "gaussian_conditional", "=", "GaussianConditional", "(", "None", ")", "\n", "\n", "trunc_normal_", "(", "self", ".", "pos_embed", ",", "std", "=", ".02", ")", "\n", "trunc_normal_", "(", "self", ".", "cls_token", ",", "std", "=", ".02", ")", "\n", "self", ".", "apply", "(", "self", ".", "_init_weights", ")", "\n", "\n", "", "def", "aux_loss", "(", "self", ")", ":", "\n", "        ", "\"\"\"Return the aggregated loss over the auxiliary entropy bottleneck\n        module(s).\n        \"\"\"", "\n", "aux_loss", "=", "sum", "(", "\n", "m", ".", "loss", "(", ")", "for", "m", "in", "self", ".", "modules", "(", ")", "if", "isinstance", "(", "m", ",", "EntropyBottleneck", ")", "\n", ")", "\n", "return", "aux_loss", "\n", "\n", "", "def", "_update_entropybottleneck", "(", "self", ",", "force", "=", "False", ")", ":", "\n", "        "]], "home.repos.pwc.inspect_result.bychao100_towards-image-compression-and-analysis-with-transformers.pretrained_model.model.PretrainTransformer.forward": [[279, 284], ["model.PretrainTransformer.forward_features", "model.PretrainTransformer.head", "model.PretrainTransformer.head_rec"], "methods", ["home.repos.pwc.inspect_result.bychao100_towards-image-compression-and-analysis-with-transformers.pretrained_model.model.PretrainTransformer.forward_features"], ["\n", "updated", "=", "False", "\n", "for", "m", "in", "self", ".", "children", "(", ")", ":", "\n", "            ", "if", "not", "isinstance", "(", "m", ",", "EntropyBottleneck", ")", ":", "\n", "                ", "continue", "\n", "", "rv", "=", "m", ".", "update", "(", "force", "=", "force", ")", "\n"]], "home.repos.pwc.inspect_result.bychao100_towards-image-compression-and-analysis-with-transformers.pretrained_model.model._cfg": [[16, 24], ["None"], "function", ["None"], ["__all__", "=", "[", "\n", "\"full_model\"", "\n", "]", "\n", "\n", "def", "_cfg", "(", "url", "=", "''", ",", "**", "kwargs", ")", ":", "\n", "    ", "return", "{", "\n", "'url'", ":", "url", ",", "\n", "'num_classes'", ":", "1000", ",", "'input_size'", ":", "(", "3", ",", "224", ",", "224", ")", ",", "'pool_size'", ":", "None", ",", "\n", "'crop_pct'", ":", ".9", ",", "'interpolation'", ":", "'bicubic'", ",", "\n"]], "home.repos.pwc.inspect_result.bychao100_towards-image-compression-and-analysis-with-transformers.pretrained_model.model.conv": [[27, 34], ["torch.Conv2d"], "function", ["None"], ["**", "kwargs", "\n", "}", "\n", "\n", "# From Balle's tensorflow compression examples", "\n", "", "SCALES_MIN", "=", "0.11", "\n", "SCALES_MAX", "=", "256", "\n", "SCALES_LEVELS", "=", "64", "\n", "\n"]], "home.repos.pwc.inspect_result.bychao100_towards-image-compression-and-analysis-with-transformers.pretrained_model.model.deconv": [[37, 45], ["torch.ConvTranspose2d"], "function", ["None"], ["min", "=", "SCALES_MIN", ",", "max", "=", "SCALES_MAX", ",", "levels", "=", "SCALES_LEVELS", "\n", ")", ":", "# pylint: disable=W0622", "\n", "    ", "return", "torch", ".", "exp", "(", "torch", ".", "linspace", "(", "math", ".", "log", "(", "min", ")", ",", "math", ".", "log", "(", "max", ")", ",", "levels", ")", ")", "\n", "\n", "\n", "", "def", "conv", "(", "in_channels", ",", "out_channels", ",", "kernel_size", "=", "5", ",", "stride", "=", "2", ")", ":", "\n", "    ", "return", "nn", ".", "Conv2d", "(", "\n", "in_channels", ",", "\n", "out_channels", ",", "\n"]], "home.repos.pwc.inspect_result.bychao100_towards-image-compression-and-analysis-with-transformers.pretrained_model.model.pretrained_model": [[286, 294], ["model.PretrainTransformer", "model._cfg", "functools.partial"], "function", ["home.repos.pwc.inspect_result.bychao100_towards-image-compression-and-analysis-with-transformers.pretrained_model.model._cfg"], ["", "return", "updated", "\n", "\n", "", "def", "update", "(", "self", ",", "scale_table", "=", "None", ",", "force", "=", "False", ")", ":", "\n", "        ", "if", "scale_table", "is", "None", ":", "\n", "            ", "scale_table", "=", "get_scale_table", "(", ")", "\n", "", "updated", "=", "self", ".", "gaussian_conditional", ".", "update_scale_table", "(", "scale_table", ",", "force", "=", "force", ")", "\n", "updated", "|=", "self", ".", "_update_entropybottleneck", "(", "force", "=", "force", ")", "\n", "return", "updated", "\n", "\n"]], "home.repos.pwc.inspect_result.bychao100_towards-image-compression-and-analysis-with-transformers.pretrained_model.utils.SmoothedValue.__init__": [[27, 34], ["collections.deque"], "methods", ["None"], ["            ", "fmt", "=", "\"{median:.4f} ({global_avg:.4f})\"", "\n", "", "self", ".", "deque", "=", "deque", "(", "maxlen", "=", "window_size", ")", "\n", "self", ".", "total", "=", "0.0", "\n", "self", ".", "count", "=", "0", "\n", "self", ".", "fmt", "=", "fmt", "\n", "\n", "", "def", "update", "(", "self", ",", "value", ",", "n", "=", "1", ")", ":", "\n", "        ", "self", ".", "deque", ".", "append", "(", "value", ")", "\n"]], "home.repos.pwc.inspect_result.bychao100_towards-image-compression-and-analysis-with-transformers.pretrained_model.utils.SmoothedValue.update": [[35, 39], ["utils.SmoothedValue.deque.append"], "methods", ["None"], ["self", ".", "count", "+=", "n", "\n", "self", ".", "total", "+=", "value", "*", "n", "\n", "\n", "", "def", "synchronize_between_processes", "(", "self", ")", ":", "\n", "        "]], "home.repos.pwc.inspect_result.bychao100_towards-image-compression-and-analysis-with-transformers.pretrained_model.utils.SmoothedValue.synchronize_between_processes": [[40, 52], ["torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.barrier", "torch.barrier", "torch.all_reduce", "torch.all_reduce", "t.tolist.tolist.tolist", "int", "utils.is_dist_avail_and_initialized"], "methods", ["home.repos.pwc.inspect_result.bychao100_towards-image-compression-and-analysis-with-transformers.pretrained_model.utils.is_dist_avail_and_initialized"], ["\n", "if", "not", "is_dist_avail_and_initialized", "(", ")", ":", "\n", "            ", "return", "\n", "", "t", "=", "torch", ".", "tensor", "(", "[", "self", ".", "count", ",", "self", ".", "total", "]", ",", "dtype", "=", "torch", ".", "float64", ",", "device", "=", "'cuda'", ")", "\n", "dist", ".", "barrier", "(", ")", "\n", "dist", ".", "all_reduce", "(", "t", ")", "\n", "t", "=", "t", ".", "tolist", "(", ")", "\n", "self", ".", "count", "=", "int", "(", "t", "[", "0", "]", ")", "\n", "self", ".", "total", "=", "t", "[", "1", "]", "\n", "\n", "", "@", "property", "\n", "def", "median", "(", "self", ")", ":", "\n"]], "home.repos.pwc.inspect_result.bychao100_towards-image-compression-and-analysis-with-transformers.pretrained_model.utils.SmoothedValue.median": [[53, 57], ["torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor.median().item", "torch.tensor.median().item", "list", "torch.tensor.median", "torch.tensor.median"], "methods", ["home.repos.pwc.inspect_result.bychao100_towards-image-compression-and-analysis-with-transformers.pretrained_model.utils.SmoothedValue.median", "home.repos.pwc.inspect_result.bychao100_towards-image-compression-and-analysis-with-transformers.pretrained_model.utils.SmoothedValue.median"], ["        ", "d", "=", "torch", ".", "tensor", "(", "list", "(", "self", ".", "deque", ")", ")", "\n", "return", "d", ".", "median", "(", ")", ".", "item", "(", ")", "\n", "\n", "", "@", "property", "\n", "def", "avg", "(", "self", ")", ":", "\n"]], "home.repos.pwc.inspect_result.bychao100_towards-image-compression-and-analysis-with-transformers.pretrained_model.utils.SmoothedValue.avg": [[58, 62], ["torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor.mean().item", "torch.tensor.mean().item", "list", "torch.tensor.mean", "torch.tensor.mean"], "methods", ["None"], ["        ", "d", "=", "torch", ".", "tensor", "(", "list", "(", "self", ".", "deque", ")", ",", "dtype", "=", "torch", ".", "float32", ")", "\n", "return", "d", ".", "mean", "(", ")", ".", "item", "(", ")", "\n", "\n", "", "@", "property", "\n", "def", "global_avg", "(", "self", ")", ":", "\n"]], "home.repos.pwc.inspect_result.bychao100_towards-image-compression-and-analysis-with-transformers.pretrained_model.utils.SmoothedValue.global_avg": [[63, 66], ["None"], "methods", ["None"], ["        ", "return", "self", ".", "total", "/", "self", ".", "count", "\n", "\n", "", "@", "property", "\n", "def", "max", "(", "self", ")", ":", "\n"]], "home.repos.pwc.inspect_result.bychao100_towards-image-compression-and-analysis-with-transformers.pretrained_model.utils.SmoothedValue.max": [[67, 70], ["utils.SmoothedValue.max"], "methods", ["home.repos.pwc.inspect_result.bychao100_towards-image-compression-and-analysis-with-transformers.pretrained_model.utils.SmoothedValue.max"], ["        ", "return", "max", "(", "self", ".", "deque", ")", "\n", "\n", "", "@", "property", "\n", "def", "value", "(", "self", ")", ":", "\n"]], "home.repos.pwc.inspect_result.bychao100_towards-image-compression-and-analysis-with-transformers.pretrained_model.utils.SmoothedValue.value": [[71, 74], ["None"], "methods", ["None"], ["        ", "return", "self", ".", "deque", "[", "-", "1", "]", "\n", "\n", "", "def", "__str__", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "fmt", ".", "format", "(", "\n"]], "home.repos.pwc.inspect_result.bychao100_towards-image-compression-and-analysis-with-transformers.pretrained_model.utils.SmoothedValue.__str__": [[75, 82], ["utils.SmoothedValue.fmt.format"], "methods", ["None"], ["median", "=", "self", ".", "median", ",", "\n", "avg", "=", "self", ".", "avg", ",", "\n", "global_avg", "=", "self", ".", "global_avg", ",", "\n", "max", "=", "self", ".", "max", ",", "\n", "value", "=", "self", ".", "value", ")", "\n", "\n", "\n", "", "", "class", "MetricLogger", "(", "object", ")", ":", "\n"]], "home.repos.pwc.inspect_result.bychao100_towards-image-compression-and-analysis-with-transformers.pretrained_model.utils.MetricLogger.__init__": [[85, 88], ["collections.defaultdict"], "methods", ["None"], ["self", ".", "delimiter", "=", "delimiter", "\n", "\n", "", "def", "update", "(", "self", ",", "**", "kwargs", ")", ":", "\n", "        ", "for", "k", ",", "v", "in", "kwargs", ".", "items", "(", ")", ":", "\n"]], "home.repos.pwc.inspect_result.bychao100_towards-image-compression-and-analysis-with-transformers.pretrained_model.utils.MetricLogger.update": [[89, 95], ["kwargs.items", "isinstance", "isinstance", "utils.MetricLogger.meters[].update", "v.item.item.item"], "methods", ["home.repos.pwc.inspect_result.bychao100_towards-image-compression-and-analysis-with-transformers.pretrained_model.utils.MetricLogger.update"], ["            ", "if", "isinstance", "(", "v", ",", "torch", ".", "Tensor", ")", ":", "\n", "                ", "v", "=", "v", ".", "item", "(", ")", "\n", "", "assert", "isinstance", "(", "v", ",", "(", "float", ",", "int", ")", ")", "\n", "self", ".", "meters", "[", "k", "]", ".", "update", "(", "v", ")", "\n", "\n", "", "", "def", "__getattr__", "(", "self", ",", "attr", ")", ":", "\n", "        ", "if", "attr", "in", "self", ".", "meters", ":", "\n"]], "home.repos.pwc.inspect_result.bychao100_towards-image-compression-and-analysis-with-transformers.pretrained_model.utils.MetricLogger.__getattr__": [[96, 103], ["AttributeError", "type"], "methods", ["None"], ["            ", "return", "self", ".", "meters", "[", "attr", "]", "\n", "", "if", "attr", "in", "self", ".", "__dict__", ":", "\n", "            ", "return", "self", ".", "__dict__", "[", "attr", "]", "\n", "", "raise", "AttributeError", "(", "\"'{}' object has no attribute '{}'\"", ".", "format", "(", "\n", "type", "(", "self", ")", ".", "__name__", ",", "attr", ")", ")", "\n", "\n", "", "def", "__str__", "(", "self", ")", ":", "\n", "        ", "loss_str", "=", "[", "]", "\n"]], "home.repos.pwc.inspect_result.bychao100_towards-image-compression-and-analysis-with-transformers.pretrained_model.utils.MetricLogger.__str__": [[104, 111], ["utils.MetricLogger.meters.items", "utils.MetricLogger.delimiter.join", "loss_str.append", "str"], "methods", ["None"], ["for", "name", ",", "meter", "in", "self", ".", "meters", ".", "items", "(", ")", ":", "\n", "            ", "loss_str", ".", "append", "(", "\n", "\"{}: {}\"", ".", "format", "(", "name", ",", "str", "(", "meter", ")", ")", "\n", ")", "\n", "", "return", "self", ".", "delimiter", ".", "join", "(", "loss_str", ")", "\n", "\n", "", "def", "synchronize_between_processes", "(", "self", ")", ":", "\n", "        ", "for", "meter", "in", "self", ".", "meters", ".", "values", "(", ")", ":", "\n"]], "home.repos.pwc.inspect_result.bychao100_towards-image-compression-and-analysis-with-transformers.pretrained_model.utils.MetricLogger.synchronize_between_processes": [[112, 115], ["utils.MetricLogger.meters.values", "meter.synchronize_between_processes"], "methods", ["home.repos.pwc.inspect_result.bychao100_towards-image-compression-and-analysis-with-transformers.pretrained_model.utils.MetricLogger.synchronize_between_processes"], ["            ", "meter", ".", "synchronize_between_processes", "(", ")", "\n", "\n", "", "", "def", "add_meter", "(", "self", ",", "name", ",", "meter", ")", ":", "\n", "        ", "self", ".", "meters", "[", "name", "]", "=", "meter", "\n"]], "home.repos.pwc.inspect_result.bychao100_towards-image-compression-and-analysis-with-transformers.pretrained_model.utils.MetricLogger.add_meter": [[116, 118], ["None"], "methods", ["None"], ["\n", "", "def", "log_every", "(", "self", ",", "iterable", ",", "print_freq", ",", "header", "=", "None", ")", ":", "\n", "        ", "i", "=", "0", "\n"]], "home.repos.pwc.inspect_result.bychao100_towards-image-compression-and-analysis-with-transformers.pretrained_model.utils.MetricLogger.log_every": [[119, 164], ["time.time", "time.time", "utils.SmoothedValue", "utils.SmoothedValue", "torch.cuda.is_available", "torch.cuda.is_available", "torch.cuda.is_available", "torch.cuda.is_available", "utils.MetricLogger.delimiter.join", "str", "print", "utils.MetricLogger.append", "utils.SmoothedValue.update", "utils.SmoothedValue.update", "time.time", "time.time", "datetime.timedelta", "str", "str", "torch.cuda.is_available", "torch.cuda.is_available", "torch.cuda.is_available", "torch.cuda.is_available", "len", "time.time", "time.time", "datetime.timedelta", "print", "print", "int", "len", "str", "len", "len", "utils.MetricLogger.format", "utils.MetricLogger.format", "len", "int", "len", "len", "str", "str", "str", "str", "str", "str", "torch.cuda.max_memory_allocated", "torch.cuda.max_memory_allocated", "torch.cuda.max_memory_allocated", "torch.cuda.max_memory_allocated"], "methods", ["home.repos.pwc.inspect_result.bychao100_towards-image-compression-and-analysis-with-transformers.pretrained_model.utils.MetricLogger.update", "home.repos.pwc.inspect_result.bychao100_towards-image-compression-and-analysis-with-transformers.pretrained_model.utils.MetricLogger.update"], ["if", "not", "header", ":", "\n", "            ", "header", "=", "''", "\n", "", "start_time", "=", "time", ".", "time", "(", ")", "\n", "end", "=", "time", ".", "time", "(", ")", "\n", "iter_time", "=", "SmoothedValue", "(", "fmt", "=", "'{avg:.4f}'", ")", "\n", "data_time", "=", "SmoothedValue", "(", "fmt", "=", "'{avg:.4f}'", ")", "\n", "space_fmt", "=", "':'", "+", "str", "(", "len", "(", "str", "(", "len", "(", "iterable", ")", ")", ")", ")", "+", "'d'", "\n", "log_msg", "=", "[", "\n", "header", ",", "\n", "'[{0'", "+", "space_fmt", "+", "'}/{1}]'", ",", "\n", "'eta: {eta}'", ",", "\n", "'{meters}'", ",", "\n", "'time: {time}'", ",", "\n", "'data: {data}'", "\n", "]", "\n", "if", "torch", ".", "cuda", ".", "is_available", "(", ")", ":", "\n", "            ", "log_msg", ".", "append", "(", "'max mem: {memory:.0f}'", ")", "\n", "", "log_msg", "=", "self", ".", "delimiter", ".", "join", "(", "log_msg", ")", "\n", "MB", "=", "1024.0", "*", "1024.0", "\n", "for", "obj", "in", "iterable", ":", "\n", "            ", "data_time", ".", "update", "(", "time", ".", "time", "(", ")", "-", "end", ")", "\n", "yield", "obj", "\n", "iter_time", ".", "update", "(", "time", ".", "time", "(", ")", "-", "end", ")", "\n", "if", "i", "%", "print_freq", "==", "0", "or", "i", "==", "len", "(", "iterable", ")", "-", "1", ":", "\n", "                ", "eta_seconds", "=", "iter_time", ".", "global_avg", "*", "(", "len", "(", "iterable", ")", "-", "i", ")", "\n", "eta_string", "=", "str", "(", "datetime", ".", "timedelta", "(", "seconds", "=", "int", "(", "eta_seconds", ")", ")", ")", "\n", "if", "torch", ".", "cuda", ".", "is_available", "(", ")", ":", "\n", "                    ", "print", "(", "log_msg", ".", "format", "(", "\n", "i", ",", "len", "(", "iterable", ")", ",", "eta", "=", "eta_string", ",", "\n", "meters", "=", "str", "(", "self", ")", ",", "\n", "time", "=", "str", "(", "iter_time", ")", ",", "data", "=", "str", "(", "data_time", ")", ",", "\n", "memory", "=", "torch", ".", "cuda", ".", "max_memory_allocated", "(", ")", "/", "MB", ")", ")", "\n", "# ipdb.set_trace()", "\n", "", "else", ":", "\n", "                    ", "print", "(", "log_msg", ".", "format", "(", "\n", "i", ",", "len", "(", "iterable", ")", ",", "eta", "=", "eta_string", ",", "\n", "meters", "=", "str", "(", "self", ")", ",", "\n", "time", "=", "str", "(", "iter_time", ")", ",", "data", "=", "str", "(", "data_time", ")", ")", ")", "\n", "", "", "i", "+=", "1", "\n", "end", "=", "time", ".", "time", "(", ")", "\n", "", "total_time", "=", "time", ".", "time", "(", ")", "-", "start_time", "\n", "total_time_str", "=", "str", "(", "datetime", ".", "timedelta", "(", "seconds", "=", "int", "(", "total_time", ")", ")", ")", "\n", "print", "(", "'{} Total time: {} ({:.4f} s / it)'", ".", "format", "(", "\n", "header", ",", "total_time_str", ",", "total_time", "/", "len", "(", "iterable", ")", ")", ")", "\n", "\n", "\n"]], "home.repos.pwc.inspect_result.bychao100_towards-image-compression-and-analysis-with-transformers.pretrained_model.utils._load_checkpoint_for_ema": [[166, 174], ["io.BytesIO", "torch.save", "torch.save", "io.BytesIO.seek", "model_ema._load_checkpoint"], "function", ["None"], ["    ", "\"\"\"\n    Workaround for ModelEma._load_checkpoint to accept an already-loaded object\n    \"\"\"", "\n", "mem_file", "=", "io", ".", "BytesIO", "(", ")", "\n", "torch", ".", "save", "(", "checkpoint", ",", "mem_file", ")", "\n", "mem_file", ".", "seek", "(", "0", ")", "\n", "model_ema", ".", "_load_checkpoint", "(", "mem_file", ")", "\n", "\n", "\n"]], "home.repos.pwc.inspect_result.bychao100_towards-image-compression-and-analysis-with-transformers.pretrained_model.utils.setup_for_distributed": [[176, 189], ["kwargs.pop", "builtin_print"], "function", ["None"], ["    ", "\"\"\"\n    This function disables printing when not in master process\n    \"\"\"", "\n", "import", "builtins", "as", "__builtin__", "\n", "builtin_print", "=", "__builtin__", ".", "print", "\n", "\n", "def", "print", "(", "*", "args", ",", "**", "kwargs", ")", ":", "\n", "        ", "force", "=", "kwargs", ".", "pop", "(", "'force'", ",", "False", ")", "\n", "if", "is_master", "or", "force", ":", "\n", "            ", "builtin_print", "(", "*", "args", ",", "**", "kwargs", ")", "\n", "\n", "", "", "__builtin__", ".", "print", "=", "print", "\n", "\n", "\n"]], "home.repos.pwc.inspect_result.bychao100_towards-image-compression-and-analysis-with-transformers.pretrained_model.utils.is_dist_avail_and_initialized": [[191, 197], ["torch.is_available", "torch.is_initialized"], "function", ["None"], ["    ", "if", "not", "dist", ".", "is_available", "(", ")", ":", "\n", "        ", "return", "False", "\n", "", "if", "not", "dist", ".", "is_initialized", "(", ")", ":", "\n", "        ", "return", "False", "\n", "", "return", "True", "\n", "\n", "\n"]], "home.repos.pwc.inspect_result.bychao100_towards-image-compression-and-analysis-with-transformers.pretrained_model.utils.get_world_size": [[199, 203], ["torch.get_world_size", "utils.is_dist_avail_and_initialized"], "function", ["home.repos.pwc.inspect_result.bychao100_towards-image-compression-and-analysis-with-transformers.pretrained_model.utils.get_world_size", "home.repos.pwc.inspect_result.bychao100_towards-image-compression-and-analysis-with-transformers.pretrained_model.utils.is_dist_avail_and_initialized"], ["    ", "if", "not", "is_dist_avail_and_initialized", "(", ")", ":", "\n", "        ", "return", "1", "\n", "", "return", "dist", ".", "get_world_size", "(", ")", "\n", "\n", "\n"]], "home.repos.pwc.inspect_result.bychao100_towards-image-compression-and-analysis-with-transformers.pretrained_model.utils.get_rank": [[205, 209], ["torch.get_rank", "utils.is_dist_avail_and_initialized"], "function", ["home.repos.pwc.inspect_result.bychao100_towards-image-compression-and-analysis-with-transformers.pretrained_model.utils.get_rank", "home.repos.pwc.inspect_result.bychao100_towards-image-compression-and-analysis-with-transformers.pretrained_model.utils.is_dist_avail_and_initialized"], ["    ", "if", "not", "is_dist_avail_and_initialized", "(", ")", ":", "\n", "        ", "return", "0", "\n", "", "return", "dist", ".", "get_rank", "(", ")", "\n", "\n", "\n"]], "home.repos.pwc.inspect_result.bychao100_towards-image-compression-and-analysis-with-transformers.pretrained_model.utils.is_main_process": [[211, 213], ["utils.get_rank"], "function", ["home.repos.pwc.inspect_result.bychao100_towards-image-compression-and-analysis-with-transformers.pretrained_model.utils.get_rank"], ["    ", "return", "get_rank", "(", ")", "==", "0", "\n", "\n", "\n"]], "home.repos.pwc.inspect_result.bychao100_towards-image-compression-and-analysis-with-transformers.pretrained_model.utils.save_on_master": [[215, 218], ["utils.is_main_process", "torch.save", "torch.save"], "function", ["home.repos.pwc.inspect_result.bychao100_towards-image-compression-and-analysis-with-transformers.pretrained_model.utils.is_main_process"], ["    ", "if", "is_main_process", "(", ")", ":", "\n", "        ", "torch", ".", "save", "(", "*", "args", ",", "**", "kwargs", ")", "\n", "\n", "\n"]], "home.repos.pwc.inspect_result.bychao100_towards-image-compression-and-analysis-with-transformers.pretrained_model.utils.init_distributed_mode": [[220, 243], ["torch.cuda.set_device", "torch.cuda.set_device", "utils.setup_for_distributed.print", "torch.distributed.init_process_group", "torch.distributed.init_process_group", "torch.distributed.barrier", "torch.distributed.barrier", "utils.setup_for_distributed", "int", "int", "int", "int", "utils.setup_for_distributed.print", "torch.cuda.device_count", "torch.cuda.device_count"], "function", ["home.repos.pwc.inspect_result.bychao100_towards-image-compression-and-analysis-with-transformers.pretrained_model.utils.setup_for_distributed"], ["    ", "if", "'RANK'", "in", "os", ".", "environ", "and", "'WORLD_SIZE'", "in", "os", ".", "environ", ":", "\n", "        ", "args", ".", "rank", "=", "int", "(", "os", ".", "environ", "[", "\"RANK\"", "]", ")", "\n", "args", ".", "world_size", "=", "int", "(", "os", ".", "environ", "[", "'WORLD_SIZE'", "]", ")", "\n", "args", ".", "gpu", "=", "int", "(", "os", ".", "environ", "[", "'LOCAL_RANK'", "]", ")", "\n", "", "elif", "'SLURM_PROCID'", "in", "os", ".", "environ", ":", "\n", "        ", "args", ".", "rank", "=", "int", "(", "os", ".", "environ", "[", "'SLURM_PROCID'", "]", ")", "\n", "args", ".", "gpu", "=", "args", ".", "rank", "%", "torch", ".", "cuda", ".", "device_count", "(", ")", "\n", "", "else", ":", "\n", "        ", "print", "(", "'Not using distributed mode'", ")", "\n", "args", ".", "distributed", "=", "False", "\n", "return", "\n", "\n", "", "args", ".", "distributed", "=", "True", "\n", "\n", "torch", ".", "cuda", ".", "set_device", "(", "args", ".", "gpu", ")", "\n", "args", ".", "dist_backend", "=", "'nccl'", "\n", "print", "(", "'| distributed init (rank {}): {}'", ".", "format", "(", "\n", "args", ".", "rank", ",", "args", ".", "dist_url", ")", ",", "flush", "=", "True", ")", "\n", "torch", ".", "distributed", ".", "init_process_group", "(", "backend", "=", "args", ".", "dist_backend", ",", "init_method", "=", "args", ".", "dist_url", ",", "\n", "world_size", "=", "args", ".", "world_size", ",", "rank", "=", "args", ".", "rank", ")", "\n", "torch", ".", "distributed", ".", "barrier", "(", ")", "\n", "setup_for_distributed", "(", "args", ".", "rank", "==", "0", ")", "\n", "\n", "\n"]], "home.repos.pwc.inspect_result.bychao100_towards-image-compression-and-analysis-with-transformers.pretrained_model.utils.imwrite": [[245, 254], ["torchvision.utils.make_grid.cpu", "torch.tensor().reshape", "torch.tensor().reshape", "torch.tensor().reshape", "torch.tensor().reshape", "torchvision.utils.make_grid", "imgs_pil.save", "torch.clamp", "torch.clamp", "torchvision.transforms.ToPILImage", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor"], "function", ["None"], ["    ", "imgs", "=", "imgs", ".", "cpu", "(", ")", "\n", "imagenet_std", "=", "torch", ".", "tensor", "(", "IMAGENET_DEFAULT_STD", ")", ".", "reshape", "(", "1", ",", "3", ",", "1", ",", "1", ")", "\n", "imagenet_mean", "=", "torch", ".", "tensor", "(", "IMAGENET_DEFAULT_MEAN", ")", ".", "reshape", "(", "1", ",", "3", ",", "1", ",", "1", ")", "\n", "\n", "imgs", "=", "torchvision", ".", "utils", ".", "make_grid", "(", "torch", ".", "clamp", "(", "imgs", "*", "imagenet_std", "+", "imagenet_mean", ",", "min", "=", "0.", ",", "max", "=", "1.", ")", ")", "\n", "\n", "imgs_pil", "=", "torchvision", ".", "transforms", ".", "ToPILImage", "(", ")", "(", "imgs", ")", "\n", "imgs_pil", ".", "save", "(", "path", ")", "\n", "\n", "\n"]], "home.repos.pwc.inspect_result.bychao100_towards-image-compression-and-analysis-with-transformers.pretrained_model.utils.img_distortion": [[256, 270], ["torch.device", "torch.device", "torch.tensor().reshape().to", "torch.tensor().reshape().to", "torch.tensor().reshape().to", "torch.tensor().reshape().to", "torch.mean", "torch.mean", "torch.mean", "torch.mean", "torch.mean.item", "torch.clamp", "torch.clamp", "torch.clamp", "torch.clamp", "torch.nn.functional.mse_loss", "torch.tensor().reshape", "torch.tensor().reshape", "torch.tensor().reshape", "torch.tensor().reshape", "torch.log10", "torch.log10", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor"], "function", ["None"], ["\n", "    ", "device", "=", "torch", ".", "device", "(", "'cuda'", ")", "\n", "\n", "imagenet_std", "=", "torch", ".", "tensor", "(", "IMAGENET_DEFAULT_STD", ")", ".", "reshape", "(", "1", ",", "3", ",", "1", ",", "1", ")", ".", "to", "(", "device", ")", "\n", "imagenet_mean", "=", "torch", ".", "tensor", "(", "IMAGENET_DEFAULT_MEAN", ")", ".", "reshape", "(", "1", ",", "3", ",", "1", ",", "1", ")", ".", "to", "(", "device", ")", "\n", "\n", "org_imgs", "=", "torch", ".", "clamp", "(", "orgs", "*", "imagenet_std", "+", "imagenet_mean", ",", "min", "=", "0.", ",", "max", "=", "1.", ")", "*", "255.", "\n", "rec_imgs", "=", "torch", ".", "clamp", "(", "recs", "*", "imagenet_std", "+", "imagenet_mean", ",", "min", "=", "0.", ",", "max", "=", "1.", ")", "*", "255.", "\n", "\n", "mse_no_reduction", "=", "torch", ".", "mean", "(", "mse_loss", "(", "org_imgs", ",", "rec_imgs", ",", "reduction", "=", "'none'", ")", ",", "dim", "=", "(", "1", ",", "2", ",", "3", ")", ")", "\n", "psnr", "=", "torch", ".", "mean", "(", "10.", "*", "torch", ".", "log10", "(", "255.", "**", "2", "/", "mse_no_reduction", ")", ")", "\n", "\n", "return", "psnr", ".", "item", "(", ")", "\n", "\n", "\n"]], "home.repos.pwc.inspect_result.bychao100_towards-image-compression-and-analysis-with-transformers.pretrained_model.losses.DenormalizedMSELoss.__init__": [[13, 17], ["torch.Module.__init__", "torch.tensor().reshape", "torch.tensor().reshape", "torch.tensor().reshape", "torch.tensor().reshape", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor"], "methods", ["home.repos.pwc.inspect_result.bychao100_towards-image-compression-and-analysis-with-transformers.pretrained_model.datasets.INatDataset.__init__"], ["class", "DenormalizedMSELoss", "(", "nn", ".", "Module", ")", ":", "\n", "\n", "    ", "def", "__init__", "(", "self", ",", "scale", "=", "255.", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "imagenet_std", "=", "torch", ".", "tensor", "(", "IMAGENET_DEFAULT_STD", ")", ".", "reshape", "(", "1", ",", "3", ",", "1", ",", "1", ")", "\n"]], "home.repos.pwc.inspect_result.bychao100_towards-image-compression-and-analysis-with-transformers.pretrained_model.losses.DenormalizedMSELoss.forward": [[18, 23], ["torch.mean", "torch.mean", "torch.mean", "torch.mean", "losses.DenormalizedMSELoss.imagenet_std.to"], "methods", ["None"], ["self", ".", "scale", "=", "scale", "\n", "\n", "", "def", "forward", "(", "self", ",", "x", ",", "y", ")", ":", "\n", "        ", "diff", "=", "(", "x", "-", "y", ")", "*", "self", ".", "imagenet_std", ".", "to", "(", "x", ".", "device", ")", "*", "self", ".", "scale", "\n", "mse_loss", "=", "torch", ".", "mean", "(", "diff", "**", "2", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.bychao100_towards-image-compression-and-analysis-with-transformers.pretrained_model.losses.JointLoss.__init__": [[27, 32], ["torch.Module.__init__", "losses.DenormalizedMSELoss"], "methods", ["home.repos.pwc.inspect_result.bychao100_towards-image-compression-and-analysis-with-transformers.pretrained_model.datasets.INatDataset.__init__"], ["", "", "class", "JointLoss", "(", "nn", ".", "Module", ")", ":", "\n", "\n", "    ", "def", "__init__", "(", "self", ",", "base_criterion", ":", "torch", ".", "nn", ".", "Module", ",", "alpha", ":", "float", ",", "beta", ":", "float", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "base_criterion", "=", "base_criterion", "\n", "self", ".", "alpha", "=", "alpha", "\n"]], "home.repos.pwc.inspect_result.bychao100_towards-image-compression-and-analysis-with-transformers.pretrained_model.losses.JointLoss.forward": [[33, 40], ["losses.JointLoss.base_criterion", "losses.JointLoss.d_mse"], "methods", ["None"], ["self", ".", "d_mse", "=", "DenormalizedMSELoss", "(", ")", "\n", "self", ".", "beta", "=", "beta", "\n", "\n", "", "def", "forward", "(", "self", ",", "inputs", ",", "outputs", ",", "labels", ")", ":", "\n", "        ", "B", ",", "_", ",", "H", ",", "W", "=", "inputs", ".", "size", "(", ")", "\n", "num_pixels", "=", "B", "*", "H", "*", "W", "\n", "\n", "outputs_cls", ",", "outputs_rec", ",", "outputs_y_likelihoods", ",", "outputs_z_likelihoods", "=", "outputs", "\n"]], "home.repos.pwc.inspect_result.bychao100_towards-image-compression-and-analysis-with-transformers.pretrained_model.datasets.INatDataset.__init__": [[12, 50], ["os.path.join", "os.path.join", "len", "open", "json.load", "open", "json.load", "open", "json.load", "king.append", "elem[].split", "int", "os.path.join", "torchvision.datasets.INatDataset.samples.append", "os.path.join", "targeter.keys", "int"], "methods", ["None"], ["    ", "def", "__init__", "(", "self", ",", "root", ",", "train", "=", "True", ",", "year", "=", "2018", ",", "transform", "=", "None", ",", "target_transform", "=", "None", ",", "\n", "category", "=", "'name'", ",", "loader", "=", "default_loader", ")", ":", "\n", "        ", "self", ".", "transform", "=", "transform", "\n", "self", ".", "loader", "=", "loader", "\n", "self", ".", "target_transform", "=", "target_transform", "\n", "self", ".", "year", "=", "year", "\n", "# assert category in ['kingdom','phylum','class','order','supercategory','family','genus','name']", "\n", "path_json", "=", "os", ".", "path", ".", "join", "(", "root", ",", "f'{\"train\" if train else \"val\"}{year}.json'", ")", "\n", "with", "open", "(", "path_json", ")", "as", "json_file", ":", "\n", "            ", "data", "=", "json", ".", "load", "(", "json_file", ")", "\n", "\n", "", "with", "open", "(", "os", ".", "path", ".", "join", "(", "root", ",", "'categories.json'", ")", ")", "as", "json_file", ":", "\n", "            ", "data_catg", "=", "json", ".", "load", "(", "json_file", ")", "\n", "\n", "", "path_json_for_targeter", "=", "os", ".", "path", ".", "join", "(", "root", ",", "f\"train{year}.json\"", ")", "\n", "\n", "with", "open", "(", "path_json_for_targeter", ")", "as", "json_file", ":", "\n", "            ", "data_for_targeter", "=", "json", ".", "load", "(", "json_file", ")", "\n", "\n", "", "targeter", "=", "{", "}", "\n", "indexer", "=", "0", "\n", "for", "elem", "in", "data_for_targeter", "[", "'annotations'", "]", ":", "\n", "            ", "king", "=", "[", "]", "\n", "king", ".", "append", "(", "data_catg", "[", "int", "(", "elem", "[", "'category_id'", "]", ")", "]", "[", "category", "]", ")", "\n", "if", "king", "[", "0", "]", "not", "in", "targeter", ".", "keys", "(", ")", ":", "\n", "                ", "targeter", "[", "king", "[", "0", "]", "]", "=", "indexer", "\n", "indexer", "+=", "1", "\n", "", "", "self", ".", "nb_classes", "=", "len", "(", "targeter", ")", "\n", "\n", "self", ".", "samples", "=", "[", "]", "\n", "for", "elem", "in", "data", "[", "'images'", "]", ":", "\n", "            ", "cut", "=", "elem", "[", "'file_name'", "]", ".", "split", "(", "'/'", ")", "\n", "target_current", "=", "int", "(", "cut", "[", "2", "]", ")", "\n", "path_current", "=", "os", ".", "path", ".", "join", "(", "root", ",", "cut", "[", "0", "]", ",", "cut", "[", "2", "]", ",", "cut", "[", "3", "]", ")", "\n", "\n", "categors", "=", "data_catg", "[", "target_current", "]", "\n", "target_current_true", "=", "targeter", "[", "categors", "[", "category", "]", "]", "\n", "self", ".", "samples", ".", "append", "(", "(", "path_current", ",", "target_current_true", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.bychao100_towards-image-compression-and-analysis-with-transformers.pretrained_model.datasets.build_dataset": [[54, 67], ["datasets.build_transform", "os.path.join", "torchvision.datasets.ImageFolder", "datasets.INatDataset"], "function", ["home.repos.pwc.inspect_result.bychao100_towards-image-compression-and-analysis-with-transformers.pretrained_model.datasets.build_transform"], ["\n", "if", "args", ".", "data_set", "==", "'IMNET'", ":", "\n", "        ", "root", "=", "os", ".", "path", ".", "join", "(", "args", ".", "data_path", ",", "'train'", "if", "is_train", "else", "'val'", ")", "\n", "dataset", "=", "datasets", ".", "ImageFolder", "(", "root", ",", "transform", "=", "transform", ")", "\n", "nb_classes", "=", "1000", "\n", "", "elif", "args", ".", "data_set", "==", "'INAT19'", ":", "\n", "        ", "dataset", "=", "INatDataset", "(", "args", ".", "data_path", ",", "train", "=", "is_train", ",", "year", "=", "2019", ",", "\n", "category", "=", "args", ".", "inat_category", ",", "transform", "=", "transform", ")", "\n", "nb_classes", "=", "dataset", ".", "nb_classes", "\n", "\n", "", "return", "dataset", ",", "nb_classes", "\n", "\n", "\n", "", "def", "build_transform", "(", "is_train", ",", "args", ")", ":", "\n"]], "home.repos.pwc.inspect_result.bychao100_towards-image-compression-and-analysis-with-transformers.pretrained_model.datasets.build_transform": [[69, 101], ["t.append", "t.append", "torchvision.transforms.Compose", "timm.data.create_transform", "int", "t.append", "t.append", "torchvision.transforms.ToTensor", "torchvision.transforms.Normalize", "torchvision.transforms.RandomCrop", "torchvision.transforms.Resize", "torchvision.transforms.CenterCrop"], "function", ["None"], ["if", "is_train", ":", "\n", "# this should always dispatch to transforms_imagenet_train", "\n", "        ", "transform", "=", "create_transform", "(", "\n", "input_size", "=", "args", ".", "input_size", ",", "\n", "is_training", "=", "True", ",", "\n", "color_jitter", "=", "None", ",", "\n", "interpolation", "=", "args", ".", "train_interpolation", ",", "\n", ")", "\n", "if", "not", "resize_im", ":", "\n", "# replace RandomResizedCropAndInterpolation with", "\n", "# RandomCrop", "\n", "            ", "transform", ".", "transforms", "[", "0", "]", "=", "transforms", ".", "RandomCrop", "(", "\n", "args", ".", "input_size", ",", "padding", "=", "4", ")", "\n", "", "return", "transform", "\n", "\n", "", "t", "=", "[", "]", "\n", "if", "resize_im", ":", "\n", "        ", "size", "=", "int", "(", "(", "256", "/", "224", ")", "*", "args", ".", "input_size", ")", "\n", "t", ".", "append", "(", "\n", "transforms", ".", "Resize", "(", "size", ",", "interpolation", "=", "3", ")", ",", "# to maintain same ratio w.r.t. 224 images", "\n", ")", "\n", "t", ".", "append", "(", "transforms", ".", "CenterCrop", "(", "args", ".", "input_size", ")", ")", "\n", "\n", "", "t", ".", "append", "(", "transforms", ".", "ToTensor", "(", ")", ")", "\n", "t", ".", "append", "(", "transforms", ".", "Normalize", "(", "IMAGENET_DEFAULT_MEAN", ",", "IMAGENET_DEFAULT_STD", ")", ")", "\n", "return", "transforms", ".", "Compose", "(", "t", ")", "", "", ""]]}