{"home.repos.pwc.inspect_result.apmoore1_language-model.None.helper.parse_path": [[10, 13], ["pathlib.Path().resolve", "pathlib.Path"], "function", ["None"], ["def", "parse_path", "(", "path_string", ":", "str", ")", "->", "Path", ":", "\n", "    ", "path_string", "=", "Path", "(", "path_string", ")", ".", "resolve", "(", ")", "\n", "return", "path_string", "\n", "\n"]], "home.repos.pwc.inspect_result.apmoore1_language-model.None.helper.get_tokeniser": [[14, 26], ["print", "print", "ValueError"], "function", ["None"], ["", "def", "get_tokeniser", "(", "tokeniser_name", ":", "str", ")", "->", "Callable", "[", "[", "str", "]", ",", "List", "[", "str", "]", "]", ":", "\n", "    ", "'''\n    Given a tokeniser name it will return that tokeniser function.\n    '''", "\n", "if", "tokeniser_name", "==", "'spacy'", ":", "\n", "        ", "print", "(", "'Using the spaCy tokeniser'", ")", "\n", "return", "spacy_tokeniser", "\n", "", "elif", "tokeniser_name", "==", "'whitespace'", ":", "\n", "        ", "print", "(", "'Using the WhiteSpace tokeniser'", ")", "\n", "return", "str", ".", "split", "\n", "", "else", ":", "\n", "        ", "raise", "ValueError", "(", "f'Not a recognised tokeniser name: {tokeniser_name}'", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.apmoore1_language-model.None.helper.valid_text_file_name_prefix": [[27, 33], ["re.search"], "function", ["None"], ["", "", "def", "valid_text_file_name_prefix", "(", "prefix_pattern", ":", "str", ",", "text_file_name", ":", "str", "\n", ")", "->", "bool", ":", "\n", "    ", "if", "prefix_pattern", ":", "\n", "        ", "if", "re", ".", "search", "(", "rf'^{prefix_pattern}'", ",", "text_file_name", ")", "is", "None", ":", "\n", "            ", "return", "False", "\n", "", "", "return", "True", "\n", "\n"]], "home.repos.pwc.inspect_result.apmoore1_language-model.None.helper.load_dataset_splits": [[34, 44], ["pathlib.Path().resolve", "Path().resolve.exists", "FileNotFoundError", "bella.data_types.TargetCollection.load_from_json", "pathlib.Path"], "function", ["None"], ["", "def", "load_dataset_splits", "(", "directory", ":", "Path", ",", "dataset_name", ":", "str", ",", "\n", "split_names", ":", "Optional", "[", "List", "[", "str", "]", "]", "=", "None", "\n", ")", "->", "Iterable", "[", "TargetCollection", "]", ":", "\n", "    ", "split_names", "=", "split_names", "or", "[", "'Train'", ",", "'Val'", ",", "'Test'", "]", "\n", "for", "split_name", "in", "split_names", ":", "\n", "        ", "data_split_fp", "=", "Path", "(", "directory", ",", "f'{dataset_name} {split_name}'", ")", ".", "resolve", "(", ")", "\n", "if", "not", "data_split_fp", ".", "exists", "(", ")", ":", "\n", "            ", "raise", "FileNotFoundError", "(", "'Cannot find the following TDSA data '", "\n", "f'split file {data_split_fp}'", ")", "\n", "", "yield", "TargetCollection", ".", "load_from_json", "(", "data_split_fp", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.apmoore1_language-model.dataset_analysis.TDSA_create_splits.parse_path": [[13, 16], ["pathlib.Path().resolve", "pathlib.Path"], "function", ["None"], ["def", "parse_path", "(", "path_string", ":", "str", ")", "->", "Path", ":", "\n", "    ", "path_string", "=", "Path", "(", "path_string", ")", ".", "resolve", "(", ")", "\n", "return", "path_string", "\n", "\n"]], "home.repos.pwc.inspect_result.apmoore1_language-model.dataset_analysis.data_analysis_helper.yelp_text_generator": [[5, 13], ["yelp_fp.open", "line.strip.strip", "json.loads"], "function", ["None"], ["def", "yelp_text_generator", "(", "yelp_fp", ":", "Path", ")", "->", "Iterable", "[", "str", "]", ":", "\n", "    ", "with", "yelp_fp", ".", "open", "(", "'r'", ")", "as", "yelp_file", ":", "\n", "        ", "for", "line", "in", "yelp_file", ":", "\n", "            ", "line", "=", "line", ".", "strip", "(", ")", "\n", "if", "line", ":", "\n", "                ", "review", "=", "json", ".", "loads", "(", "line", ")", "\n", "review_text", "=", "review", "[", "'text'", "]", "\n", "yield", "review_text", "\n", "\n"]], "home.repos.pwc.inspect_result.apmoore1_language-model.dataset_analysis.data_analysis_helper.amazon_text_generator": [[14, 22], ["amazon_fp.open", "line.strip.strip", "json.loads"], "function", ["None"], ["", "", "", "", "def", "amazon_text_generator", "(", "amazon_fp", ":", "Path", ")", "->", "Iterable", "[", "str", "]", ":", "\n", "    ", "with", "amazon_fp", ".", "open", "(", "'r'", ")", "as", "amazon_file", ":", "\n", "        ", "for", "line", "in", "amazon_file", ":", "\n", "            ", "line", "=", "line", ".", "strip", "(", ")", "\n", "if", "line", ":", "\n", "                ", "review", "=", "json", ".", "loads", "(", "line", ")", "\n", "review_text", "=", "review", "[", "'reviewText'", "]", "\n", "yield", "review_text", "\n", "\n"]], "home.repos.pwc.inspect_result.apmoore1_language-model.dataset_analysis.data_analysis_helper.mp_text_generator": [[23, 31], ["mp_fp.open", "line.strip.strip", "json.loads"], "function", ["None"], ["", "", "", "", "def", "mp_text_generator", "(", "mp_fp", ":", "Path", ")", "->", "Iterable", "[", "str", "]", ":", "\n", "    ", "with", "mp_fp", ".", "open", "(", "'r'", ")", "as", "mp_file", ":", "\n", "        ", "for", "line", "in", "mp_file", ":", "\n", "            ", "line", "=", "line", ".", "strip", "(", ")", "\n", "if", "line", ":", "\n", "                ", "tweet", "=", "json", ".", "loads", "(", "line", ")", "\n", "text", "=", "tweet", "[", "'text'", "]", "\n", "yield", "text", "\n", "\n"]], "home.repos.pwc.inspect_result.apmoore1_language-model.dataset_analysis.data_analysis_helper.sentence_text_generator": [[32, 38], ["data_fp.open", "line.strip.strip"], "function", ["None"], ["", "", "", "", "def", "sentence_text_generator", "(", "data_fp", ":", "Path", ")", "->", "Iterable", "[", "str", "]", ":", "\n", "    ", "with", "data_fp", ".", "open", "(", "'r'", ")", "as", "data_file", ":", "\n", "        ", "for", "line", "in", "data_file", ":", "\n", "            ", "line", "=", "line", ".", "strip", "(", ")", "\n", "if", "line", ":", "\n", "                ", "yield", "line", "\n", "\n"]], "home.repos.pwc.inspect_result.apmoore1_language-model.dataset_analysis.data_analysis_helper.text_generator_func_mapper": [[39, 51], ["ValueError"], "function", ["None"], ["", "", "", "", "def", "text_generator_func_mapper", "(", "review_data_name", ":", "str", ")", "->", "Callable", "[", "[", "Path", "]", ",", "Iterable", "[", "str", "]", "]", ":", "\n", "    ", "if", "review_data_name", "==", "'yelp'", ":", "\n", "        ", "return", "yelp_text_generator", "\n", "", "elif", "review_data_name", "==", "'yelp_sentences'", "or", "review_data_name", "==", "'billion_word_corpus'", ":", "\n", "        ", "return", "sentence_text_generator", "\n", "", "elif", "review_data_name", "==", "'amazon'", ":", "\n", "        ", "return", "amazon_text_generator", "\n", "", "elif", "review_data_name", "==", "'mp'", ":", "\n", "        ", "return", "mp_text_generator", "\n", "", "else", ":", "\n", "        ", "raise", "ValueError", "(", "'Do not have a text generator function for the '", "\n", "f'{review_data_name} review data'", ")", ""]], "home.repos.pwc.inspect_result.apmoore1_language-model.dataset_analysis.to_sentences_tokens.get_sentence_spitter": [[21, 28], ["spacy.lang.en.English", "spacy.lang.en.English.create_pipe", "spacy.lang.en.English.add_pipe"], "function", ["None"], ["def", "get_sentence_spitter", "(", ")", "->", "SpacyModelType", ":", "\n", "    ", "if", "'english'", "not", "in", "SPACY_MODEL", ":", "\n", "        ", "nlp", "=", "English", "(", ")", "# just the language with no model", "\n", "sentencizer", "=", "nlp", ".", "create_pipe", "(", "\"sentencizer\"", ")", "\n", "nlp", ".", "add_pipe", "(", "sentencizer", ")", "\n", "SPACY_MODEL", "[", "'english'", "]", "=", "nlp", "\n", "", "return", "SPACY_MODEL", "[", "'english'", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.apmoore1_language-model.dataset_analysis.to_sentences_tokens.sentence_token_generator": [[29, 48], ["to_sentences_tokens.get_sentence_spitter", "get_sentence_spitter.", "print", "tokens.append"], "function", ["home.repos.pwc.inspect_result.apmoore1_language-model.dataset_analysis.to_sentences_tokens.get_sentence_spitter"], ["", "def", "sentence_token_generator", "(", "text", ":", "str", ")", "->", "Iterable", "[", "str", "]", ":", "\n", "    ", "'''\n    Given some text that can contain multiple sentences it breaks the document\n    into sentences, tokenises the sentence and yields each sentence where the \n    original tokenisation can be found by splitting on whitespace.\n\n    Tokenisation and sentence splitting done by spacy.\n    '''", "\n", "spacy_pipeline", "=", "get_sentence_spitter", "(", ")", "\n", "spacy_annotations", "=", "spacy_pipeline", "(", "text", ")", "\n", "try", ":", "\n", "        ", "for", "sentence", "in", "spacy_annotations", ".", "sents", ":", "\n", "            ", "tokens", "=", "[", "]", "\n", "for", "token", "in", "sentence", ":", "\n", "                ", "if", "not", "token", ".", "is_space", ":", "\n", "                    ", "tokens", ".", "append", "(", "token", ".", "text", ")", "\n", "", "", "yield", "' '", ".", "join", "(", "tokens", ")", "\n", "", "", "except", ":", "\n", "        ", "print", "(", "f'Text that Spacy cannot parse {text}\\nThis text will not be included.'", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.apmoore1_language-model.dataset_analysis.to_sentences_tokens.split_save_tweets": [[49, 72], ["target_extraction.tokenizers.spacy_tokenizer", "zip", "pathlib.Path().resolve", "pathlib.Path().resolve", "print", "print", "Path().resolve.open", "text_generator_func", "pathlib.Path", "pathlib.Path", "save_split_file.write", "target_extraction.tokenizers.spacy_tokenizer."], "function", ["None"], ["", "", "def", "split_save_tweets", "(", "text_generator_func", ":", "Callable", "[", "[", "Path", "]", ",", "Iterable", "[", "str", "]", "]", ",", "\n", "review_data_dir", ":", "Path", ",", "\n", "split_names", ":", "Optional", "[", "List", "[", "str", "]", "]", "=", "None", ")", "->", "None", ":", "\n", "    ", "split_names", "=", "split_names", "or", "[", "'train.json'", ",", "'val.json'", ",", "'test.json'", "]", "\n", "save_split_names", "=", "[", "'split_train.txt'", ",", "'split_val.txt'", ",", "'split_test.txt'", "]", "\n", "\n", "tokenizer", "=", "spacy_tokenizer", "(", ")", "\n", "\n", "for", "split_name", ",", "save_split_name", "in", "zip", "(", "split_names", ",", "save_split_names", ")", ":", "\n", "        ", "split_fp", "=", "Path", "(", "review_data_dir", ",", "split_name", ")", ".", "resolve", "(", ")", "\n", "save_split_fp", "=", "Path", "(", "review_data_dir", ",", "save_split_name", ")", ".", "resolve", "(", ")", "\n", "\n", "count", "=", "0", "\n", "with", "save_split_fp", ".", "open", "(", "'w+'", ")", "as", "save_split_file", ":", "\n", "            ", "for", "tweet", "in", "text_generator_func", "(", "split_fp", ")", ":", "\n", "                ", "tweet", "=", "' '", ".", "join", "(", "tokenizer", "(", "tweet", ")", ")", "\n", "if", "count", "!=", "0", ":", "\n", "                    ", "tweet", "=", "f'\\n{tweet}'", "\n", "", "save_split_file", ".", "write", "(", "tweet", ")", "\n", "count", "+=", "1", "\n", "", "", "print", "(", "f'{split_fp} has been tokenised and the corresponding'", "\n", "f' sentences have been written to the following file {save_split_fp}'", ")", "\n", "print", "(", "f'Number of tweets created {count}'", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.apmoore1_language-model.dataset_analysis.to_sentences_tokens.split_save_reviews": [[73, 94], ["zip", "pathlib.Path().resolve", "pathlib.Path().resolve", "print", "print", "Path().resolve.open", "text_generator_func", "pathlib.Path", "pathlib.Path", "to_sentences_tokens.sentence_token_generator", "save_split_file.write"], "function", ["home.repos.pwc.inspect_result.apmoore1_language-model.dataset_analysis.to_sentences_tokens.sentence_token_generator"], ["", "", "def", "split_save_reviews", "(", "text_generator_func", ":", "Callable", "[", "[", "Path", "]", ",", "Iterable", "[", "str", "]", "]", ",", "\n", "review_data_dir", ":", "Path", ",", "\n", "split_names", ":", "Optional", "[", "List", "[", "str", "]", "]", "=", "None", ")", "->", "None", ":", "\n", "    ", "split_names", "=", "split_names", "or", "[", "'train.json'", ",", "'val.json'", ",", "'test.json'", "]", "\n", "save_split_names", "=", "[", "'split_train.txt'", ",", "'split_val.txt'", ",", "'split_test.txt'", "]", "\n", "\n", "for", "split_name", ",", "save_split_name", "in", "zip", "(", "split_names", ",", "save_split_names", ")", ":", "\n", "        ", "split_fp", "=", "Path", "(", "review_data_dir", ",", "split_name", ")", ".", "resolve", "(", ")", "\n", "save_split_fp", "=", "Path", "(", "review_data_dir", ",", "save_split_name", ")", ".", "resolve", "(", ")", "\n", "\n", "count", "=", "0", "\n", "with", "save_split_fp", ".", "open", "(", "'w+'", ")", "as", "save_split_file", ":", "\n", "            ", "for", "review_text", "in", "text_generator_func", "(", "split_fp", ")", ":", "\n", "                ", "for", "sentence", "in", "sentence_token_generator", "(", "review_text", ")", ":", "\n", "                    ", "if", "count", "!=", "0", ":", "\n", "                        ", "sentence", "=", "f'\\n{sentence}'", "\n", "", "save_split_file", ".", "write", "(", "sentence", ")", "\n", "count", "+=", "1", "\n", "", "", "", "print", "(", "f'{split_fp} has been sentence split and tokenised and the corresponding'", "\n", "f' sentences have been written to the following file {save_split_fp}'", ")", "\n", "print", "(", "f'Number of sentences created {count}'", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.apmoore1_language-model.dataset_analysis.filter_businesses_by_category.parse_path": [[6, 9], ["pathlib.Path().resolve", "pathlib.Path"], "function", ["None"], ["def", "parse_path", "(", "path_string", ":", "str", ")", "->", "Path", ":", "\n", "    ", "path_string", "=", "Path", "(", "path_string", ")", ".", "resolve", "(", ")", "\n", "return", "path_string", "\n", "\n"]], "home.repos.pwc.inspect_result.apmoore1_language-model.dataset_analysis.predict_targets.parse_path": [[9, 12], ["pathlib.Path().resolve", "pathlib.Path"], "function", ["None"], ["def", "parse_path", "(", "path_string", ":", "str", ")", "->", "Path", ":", "\n", "    ", "path_string", "=", "Path", "(", "path_string", ")", ".", "resolve", "(", ")", "\n", "return", "path_string", "\n", "\n"]], "home.repos.pwc.inspect_result.apmoore1_language-model.dataset_analysis.predict_targets.token_iter": [[13, 20], ["fp.open", "line.strip.strip", "line.strip.split"], "function", ["None"], ["", "def", "token_iter", "(", "fp", ":", "Path", ")", "->", "Iterable", "[", "Dict", "[", "str", ",", "List", "[", "str", "]", "]", "]", ":", "\n", "    ", "with", "fp", ".", "open", "(", "'r'", ")", "as", "_file", ":", "\n", "        ", "for", "line", "in", "_file", ":", "\n", "            ", "line", "=", "line", ".", "strip", "(", ")", "\n", "if", "line", ":", "\n", "                ", "tokens", "=", "line", ".", "split", "(", ")", "\n", "yield", "{", "'tokens'", ":", "tokens", ",", "'text'", ":", "line", "}", "\n", "\n"]], "home.repos.pwc.inspect_result.apmoore1_language-model.dataset_analysis.data_stats.yelp_review_generator": [[17, 24], ["pathlib.Path().resolve", "pathlib.Path"], "function", ["None"], ["def", "yelp_review_generator", "(", "data_dir", ":", "Path", ",", "\n", "split_names", ":", "Optional", "[", "List", "[", "str", "]", "]", "=", "None", "\n", ")", "->", "Iterable", "[", "Path", "]", ":", "\n", "    ", "split_names", "=", "split_names", "or", "[", "'train.json'", ",", "'val.json'", ",", "'test.json'", "]", "\n", "for", "split_name", "in", "split_names", ":", "\n", "        ", "data_fp", "=", "Path", "(", "data_dir", ",", "split_name", ")", ".", "resolve", "(", ")", "\n", "yield", "data_fp", "\n", "\n"]], "home.repos.pwc.inspect_result.apmoore1_language-model.dataset_analysis.data_stats.yelp_sentence_generator": [[25, 32], ["pathlib.Path().resolve", "pathlib.Path"], "function", ["None"], ["", "", "def", "yelp_sentence_generator", "(", "data_dir", ":", "Path", ",", "\n", "split_names", ":", "Optional", "[", "List", "[", "str", "]", "]", "=", "None", "\n", ")", "->", "Iterable", "[", "Path", "]", ":", "\n", "    ", "split_names", "=", "split_names", "or", "[", "'split_train.txt'", ",", "'split_val.txt'", ",", "'split_test.txt'", "]", "\n", "for", "split_name", "in", "split_names", ":", "\n", "        ", "data_fp", "=", "Path", "(", "data_dir", ",", "split_name", ")", ".", "resolve", "(", ")", "\n", "yield", "data_fp", "\n", "\n"]], "home.repos.pwc.inspect_result.apmoore1_language-model.dataset_analysis.data_stats.one_billion_generator": [[33, 46], ["pathlib.Path", "pathlib.Path", "pathlib.Path.iterdir", "helper.valid_text_file_name_prefix", "all_fps.append", "print"], "function", ["home.repos.pwc.inspect_result.apmoore1_language-model.None.helper.valid_text_file_name_prefix"], ["", "", "def", "one_billion_generator", "(", "data_dir", ":", "Path", ")", "->", "Iterable", "[", "Path", "]", ":", "\n", "    ", "test_data", "=", "Path", "(", "data_dir", ",", "'heldout-monolingual.tokenized.shuffled'", ",", "\n", "'news.en-00000-of-00100'", ")", "\n", "train_data_dir", "=", "Path", "(", "data_dir", ",", "'training-monolingual.tokenized.shuffled'", ")", "\n", "all_fps", "=", "[", "test_data", "]", "\n", "for", "train_fp", "in", "train_data_dir", ".", "iterdir", "(", ")", ":", "\n", "        ", "if", "helper", ".", "valid_text_file_name_prefix", "(", "'news'", ",", "train_fp", ".", "name", ")", ":", "\n", "            ", "all_fps", ".", "append", "(", "train_fp", ")", "\n", "", "else", ":", "\n", "            ", "print", "(", "f'Not including this file {train_fp} within the training '", "\n", "'directory of the one billion word generator'", ")", "\n", "", "", "for", "data_fp", "in", "all_fps", ":", "\n", "        ", "yield", "data_fp", "\n", "\n"]], "home.repos.pwc.inspect_result.apmoore1_language-model.dataset_analysis.data_stats.sent_len_greater_than_filter": [[47, 54], ["count_dict.items"], "function", ["None"], ["", "", "def", "sent_len_greater_than_filter", "(", "count_dict", ":", "Dict", "[", "Any", ",", "Union", "[", "int", ",", "float", "]", "]", ",", "\n", "filter_number", ":", "Union", "[", "int", ",", "float", "]", ")", "->", "int", ":", "\n", "    ", "count", "=", "0", "\n", "for", "length", ",", "number_in_length", "in", "count_dict", ".", "items", "(", ")", ":", "\n", "        ", "if", "length", ">", "filter_number", ":", "\n", "            ", "count", "+=", "number_in_length", "\n", "", "", "return", "count", "\n", "\n"]], "home.repos.pwc.inspect_result.apmoore1_language-model.dataset_analysis.data_stats.token_greater_than_filter": [[55, 62], ["count_dict.values"], "function", ["None"], ["", "def", "token_greater_than_filter", "(", "count_dict", ":", "Dict", "[", "Any", ",", "Union", "[", "int", ",", "float", "]", "]", ",", "\n", "filter_number", ":", "Union", "[", "int", ",", "float", "]", ")", "->", "int", ":", "\n", "    ", "count", "=", "0", "\n", "for", "value", "in", "count_dict", ".", "values", "(", ")", ":", "\n", "        ", "if", "value", ">", "filter_number", ":", "\n", "            ", "count", "+=", "1", "\n", "", "", "return", "count", "\n", "\n"]], "home.repos.pwc.inspect_result.apmoore1_language-model.dataset_analysis.data_stats.data_generator_mapper": [[64, 73], ["data_stats.yelp_review_generator", "data_stats.yelp_sentence_generator", "data_stats.one_billion_generator", "ValueError"], "function", ["home.repos.pwc.inspect_result.apmoore1_language-model.dataset_analysis.data_stats.yelp_review_generator", "home.repos.pwc.inspect_result.apmoore1_language-model.dataset_analysis.data_stats.yelp_sentence_generator", "home.repos.pwc.inspect_result.apmoore1_language-model.dataset_analysis.data_stats.one_billion_generator"], ["", "def", "data_generator_mapper", "(", "data_name", ":", "str", ",", "data_dir", ":", "Path", ")", "->", "Iterable", "[", "Path", "]", ":", "\n", "    ", "if", "data_name", "==", "'yelp'", ":", "\n", "        ", "return", "yelp_review_generator", "(", "data_dir", ")", "\n", "", "elif", "data_name", "==", "'yelp_sentences'", ":", "\n", "        ", "return", "yelp_sentence_generator", "(", "data_dir", ")", "\n", "", "elif", "data_name", "==", "'billion_word_corpus'", ":", "\n", "        ", "return", "one_billion_generator", "(", "data_dir", ")", "\n", "", "else", ":", "\n", "        ", "raise", "ValueError", "(", "'Do not have a text generator function for the '", "\n", "f'{data_name} review data'", ")", "\n"]], "home.repos.pwc.inspect_result.apmoore1_language-model.dataset_analysis.create_train_val_test.parse_path": [[9, 12], ["pathlib.Path().resolve", "pathlib.Path"], "function", ["None"], ["def", "parse_path", "(", "path_string", ":", "str", ")", "->", "Path", ":", "\n", "    ", "path_string", "=", "Path", "(", "path_string", ")", ".", "resolve", "(", ")", "\n", "return", "path_string", "\n", "\n"]], "home.repos.pwc.inspect_result.apmoore1_language-model.dataset_analysis.create_train_val_test.review_id_data": [[13, 38], ["review_fp.open", "json.loads", "review_fp.open", "enumerate", "eval", "review_fp.open", "str", "json.loads"], "function", ["None"], ["", "def", "review_id_data", "(", "review_fp", ":", "Path", ",", "dataset_name", ":", "str", "\n", ")", "->", "Iterable", "[", "Tuple", "[", "str", ",", "Dict", "[", "str", ",", "Any", "]", "]", "]", ":", "\n", "    ", "'''\n    Given a file path to the Yelp review data, it will generate for each \n    review the ID and a dictionary of all the review data e.g. text, user_id, \n    stars etc.\n    '''", "\n", "if", "dataset_name", "==", "'yelp'", ":", "\n", "        ", "with", "review_fp", ".", "open", "(", "'r'", ")", "as", "review_data", ":", "\n", "            ", "for", "line", "in", "review_data", ":", "\n", "                ", "review", "=", "json", ".", "loads", "(", "line", ")", "\n", "review_id", "=", "review", "[", "'review_id'", "]", "\n", "yield", "review_id", ",", "review", "\n", "", "", "", "elif", "dataset_name", "==", "'amazon'", ":", "\n", "        ", "with", "review_fp", ".", "open", "(", "'r'", ")", "as", "review_data", ":", "\n", "            ", "for", "index", ",", "line", "in", "enumerate", "(", "review_data", ")", ":", "\n", "                ", "review", "=", "eval", "(", "line", ")", "\n", "review_id", "=", "review", "[", "'reviewerID'", "]", "+", "review", "[", "'asin'", "]", "+", "str", "(", "index", ")", "\n", "yield", "review_id", ",", "review", "\n", "", "", "", "elif", "dataset_name", "==", "'mp'", ":", "\n", "        ", "with", "review_fp", ".", "open", "(", "'r'", ")", "as", "review_data", ":", "\n", "            ", "for", "line", "in", "review_data", ":", "\n", "                ", "review", "=", "json", ".", "loads", "(", "line", ")", "\n", "review_id", "=", "review", "[", "'id'", "]", "\n", "yield", "review_id", ",", "review", "\n", "\n"]], "home.repos.pwc.inspect_result.apmoore1_language-model.dataset_analysis.create_train_val_test.write_review": [[71, 84], ["json.dumps", "data_fp.exists", "data_fp.open", "data_file.write"], "function", ["None"], ["", "", "", "", "def", "write_review", "(", "data_fp", ":", "Path", ",", "review", ":", "Dict", "[", "str", ",", "Any", "]", ")", "->", "None", ":", "\n", "    ", "'''\n    Given a File path will join the tokens back up based on whitespace and \n    append the string to the given File Path.\n\n    The File path should be the file path to either the training, validation, \n    or test file. \n    '''", "\n", "review", "=", "json", ".", "dumps", "(", "review", ")", "\n", "if", "data_fp", ".", "exists", "(", ")", ":", "\n", "        ", "review", "=", "f\"\\n{review}\"", "\n", "", "with", "data_fp", ".", "open", "(", "'a+'", ")", "as", "data_file", ":", "\n", "        ", "data_file", ".", "write", "(", "review", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.apmoore1_language-model.dataset_analysis.create_train_val_test.data_stats": [[85, 96], ["None"], "function", ["None"], ["", "", "def", "data_stats", "(", "num_train_reviews", ":", "int", ",", "num_val_reviews", ":", "int", ",", "\n", "num_test_reviews", ":", "int", ")", "->", "str", ":", "\n", "    ", "total_num_reviews", "=", "num_train_reviews", "+", "num_val_reviews", "+", "num_test_reviews", "\n", "\n", "train_statement", "=", "f'Number of training reviews {num_train_reviews}'", "f'({num_train_reviews/total_num_reviews}%)\\n'", "\n", "val_statement", "=", "f'Number of validation reviews {num_val_reviews}'", "f'({num_val_reviews/total_num_reviews}%)\\n'", "\n", "test_statement", "=", "f'Number of test reviews {num_test_reviews}'", "f'({num_test_reviews/total_num_reviews}%)'", "\n", "return", "train_statement", "+", "val_statement", "+", "test_statement", "\n", "\n"]], "home.repos.pwc.inspect_result.apmoore1_language-model.dataset_analysis.create_train_val_test.do_data_files_exist": [[97, 109], ["pathlib.Path().exists", "pathlib.Path().exists", "pathlib.Path().exists", "pathlib.Path", "pathlib.Path", "pathlib.Path"], "function", ["None"], ["", "def", "do_data_files_exist", "(", "data_dir_fp", ":", "Path", ")", "->", "bool", ":", "\n", "    ", "'''\n    Given the directory which will store the Yelp review data splits if the\n    splits already exist returns True else False.\n    '''", "\n", "if", "not", "Path", "(", "data_dir_fp", ",", "'train.json'", ")", ".", "exists", "(", ")", ":", "\n", "        ", "return", "False", "\n", "", "if", "not", "Path", "(", "data_dir_fp", ",", "'val.json'", ")", ".", "exists", "(", ")", ":", "\n", "        ", "return", "False", "\n", "", "if", "not", "Path", "(", "data_dir_fp", ",", "'test.json'", ")", ".", "exists", "(", ")", ":", "\n", "        ", "return", "False", "\n", "", "return", "True", "\n", "\n"]], "home.repos.pwc.inspect_result.apmoore1_language-model.fine_tune_lm.create_lm_vocab.parse_path": [[7, 10], ["pathlib.Path().resolve", "pathlib.Path"], "function", ["None"], ["def", "parse_path", "(", "path_string", ":", "str", ")", "->", "Path", ":", "\n", "    ", "path_string", "=", "Path", "(", "path_string", ")", ".", "resolve", "(", ")", "\n", "return", "path_string", "\n", "\n"]], "home.repos.pwc.inspect_result.apmoore1_language-model.vocab_comparison.tdsa_vocab.get_tokens": [[14, 25], ["set", "data.grouped_sentences.items", "tokeniser", "len", "ValueError", "set.add"], "function", ["None"], ["def", "get_tokens", "(", "data", ":", "TargetCollection", ",", "tokeniser", ":", "Callable", "[", "[", "str", "]", ",", "List", "[", "str", "]", "]", ",", "\n", "field", ":", "str", "=", "'text'", ")", "->", "Set", "[", "str", "]", ":", "\n", "    ", "unique_tokens", "=", "set", "(", ")", "\n", "for", "sentence_id", ",", "targets", "in", "data", ".", "grouped_sentences", ".", "items", "(", ")", ":", "\n", "        ", "if", "len", "(", "targets", ")", "==", "0", ":", "\n", "            ", "raise", "ValueError", "(", "'There should be at least one sentence per id'", ")", "\n", "", "target", "=", "targets", "[", "0", "]", "\n", "text", "=", "target", "[", "field", "]", "\n", "for", "token", "in", "tokeniser", "(", "text", ")", ":", "\n", "            ", "unique_tokens", ".", "add", "(", "token", ")", "\n", "", "", "return", "unique_tokens", "\n", "\n"]], "home.repos.pwc.inspect_result.apmoore1_language-model.vocab_comparison.tdsa_vocab.dataset_vocab": [[26, 34], ["set", "helper.load_dataset_splits", "tdsa_vocab.get_tokens", "all_tokens.union.union"], "function", ["home.repos.pwc.inspect_result.apmoore1_language-model.None.helper.load_dataset_splits", "home.repos.pwc.inspect_result.apmoore1_language-model.vocab_comparison.tdsa_vocab.get_tokens"], ["", "def", "dataset_vocab", "(", "directory", ":", "Path", ",", "dataset_name", ":", "str", ",", "\n", "tokeniser", ":", "Callable", "[", "[", "str", "]", ",", "List", "[", "str", "]", "]", ",", "\n", "field", ":", "str", "=", "'text'", ")", "->", "Set", "[", "str", "]", ":", "\n", "    ", "all_tokens", "=", "set", "(", ")", "\n", "for", "data", "in", "load_dataset_splits", "(", "directory", ",", "dataset_name", ")", ":", "\n", "        ", "data_tokens", "=", "get_tokens", "(", "data", ",", "tokeniser", ",", "field", ")", "\n", "all_tokens", "=", "all_tokens", ".", "union", "(", "data_tokens", ")", "\n", "", "return", "all_tokens", "\n", "\n"]], "home.repos.pwc.inspect_result.apmoore1_language-model.vocab_comparison.comapre_vocabs.load_vocab": [[12, 16], ["set", "vocab_fp.open", "json.load"], "function", ["None"], ["def", "load_vocab", "(", "vocab_fp", ":", "Path", ")", "->", "Set", "[", "str", "]", ":", "\n", "    ", "with", "vocab_fp", ".", "open", "(", "'r'", ")", "as", "vocab_file", ":", "\n", "        ", "vocab", "=", "json", ".", "load", "(", "vocab_file", ")", "\n", "", "return", "set", "(", "vocab", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.apmoore1_language-model.vocab_comparison.create_vocab.create_vocab": [[14, 29], ["set", "text_fp.open", "line.strip.strip", "tokeniser", "set.add"], "function", ["None"], ["def", "create_vocab", "(", "text_fp", ":", "Path", ",", "tokeniser", ":", "Callable", "[", "[", "str", "]", ",", "List", "[", "str", "]", "]", "\n", ")", "->", "Set", "[", "str", "]", ":", "\n", "    ", "'''\n    Given a file that contains just text on each new line it will tokenise that \n    text and return a Set of all the unique words (tokens).\n    '''", "\n", "vocab", "=", "set", "(", ")", "\n", "with", "text_fp", ".", "open", "(", "'r'", ")", "as", "text_file", ":", "\n", "        ", "for", "line", "in", "text_file", ":", "\n", "            ", "line", "=", "line", ".", "strip", "(", ")", "\n", "if", "line", ":", "\n", "                ", "tokens", "=", "tokeniser", "(", "line", ")", "\n", "for", "token", "in", "tokens", ":", "\n", "                    ", "vocab", ".", "add", "(", "token", ")", "\n", "", "", "", "", "return", "vocab", "\n", "\n"]], "home.repos.pwc.inspect_result.apmoore1_language-model.vocab_comparison.targets_affected.load_vocab": [[12, 20], ["vocab_fp.open", "line.strip.strip", "vocab.append"], "function", ["None"], ["def", "load_vocab", "(", "vocab_fp", ":", "Path", ")", "->", "List", "[", "str", "]", ":", "\n", "    ", "vocab", "=", "[", "]", "\n", "with", "vocab_fp", ".", "open", "(", "'r'", ")", "as", "vocab_file", ":", "\n", "        ", "for", "line", "in", "vocab_file", ":", "\n", "            ", "line", "=", "line", ".", "strip", "(", ")", "\n", "if", "line", ":", "\n", "                ", "vocab", ".", "append", "(", "line", ")", "\n", "", "", "", "return", "vocab", "\n", "\n"]], "home.repos.pwc.inspect_result.apmoore1_language-model.vocab_comparison.frequency_words.read_text_file_update_counter": [[12, 21], ["text_fp.open", "line.strip.strip", "str.split", "vocab_counter.update"], "function", ["None"], ["def", "read_text_file_update_counter", "(", "text_fp", ":", "Path", ",", "vocab_counter", ":", "Counter", "\n", ")", "->", "Counter", ":", "\n", "    ", "with", "text_fp", ".", "open", "(", "'r'", ")", "as", "text_file", ":", "\n", "        ", "for", "line", "in", "text_file", ":", "\n", "            ", "line", "=", "line", ".", "strip", "(", ")", "\n", "if", "line", ":", "\n", "                ", "tokens", "=", "str", ".", "split", "(", "line", ")", "\n", "vocab_counter", ".", "update", "(", "tokens", ")", "\n", "", "", "", "return", "vocab_counter", "\n", "\n"]], "home.repos.pwc.inspect_result.apmoore1_language-model.word_embeddings.create_embeddings.TokenIter.__init__": [[15, 23], ["None"], "methods", ["None"], ["    ", "def", "__init__", "(", "self", ",", "token_fp", ":", "Path", ",", "lower", ":", "bool", "=", "True", ")", "->", "None", ":", "\n", "        ", "'''\n        :param token_fp: Path to a file that contains a line of text that \n                            has already been tokenized.\n        :param lower: Whether to lower case the text\n        '''", "\n", "self", ".", "token_fp", "=", "token_fp", "\n", "self", ".", "lower", "=", "lower", "\n", "\n"]], "home.repos.pwc.inspect_result.apmoore1_language-model.word_embeddings.create_embeddings.TokenIter.__iter__": [[24, 37], ["create_embeddings.TokenIter.token_fp.open", "line.lower.lower.strip", "line.lower.lower.split", "line.lower.lower.lower"], "methods", ["None"], ["", "def", "__iter__", "(", "self", ")", "->", "Iterable", "[", "List", "[", "str", "]", "]", ":", "\n", "        ", "'''\n        Iterates over the file path given within the constructor and generates\n        the tokens of each json object per iteration.\n        :return: Tokens\n        '''", "\n", "with", "self", ".", "token_fp", ".", "open", "(", "'r'", ")", "as", "token_file", ":", "\n", "            ", "for", "line", "in", "token_file", ":", "\n", "                ", "if", "line", ".", "strip", "(", ")", ":", "\n", "                    ", "if", "self", ".", "lower", ":", "\n", "                        ", "line", "=", "line", ".", "lower", "(", ")", "\n", "", "tokens", "=", "line", ".", "split", "(", ")", "\n", "yield", "tokens", "\n", "\n"]], "home.repos.pwc.inspect_result.apmoore1_language-model.word_embeddings.create_embeddings.parse_path": [[10, 13], ["pathlib.Path().resolve", "pathlib.Path"], "function", ["None"], ["def", "parse_path", "(", "path_string", ":", "str", ")", "->", "Path", ":", "\n", "    ", "path_string", "=", "Path", "(", "path_string", ")", ".", "resolve", "(", ")", "\n", "return", "path_string", "\n", "\n"]], "home.repos.pwc.inspect_result.apmoore1_language-model.word_embeddings.create_embeddings.creating_embeddings": [[38, 89], ["embedding_fp.is_file", "text_file.parent.mkdir", "text_file.with_name", "create_embeddings.TokenIter", "embedding_class", "embedding_fp.parent.mkdir", "str", "embedding_class.save", "text_file.with_name", "text_file.with_name.is_file", "create_embeddings.TokenIter", "range", "embedding_fp.resolve", "text_file.with_name.open", "enumerate", "text_file.with_name.with_name", "phrase_fp.with_suffix.with_suffix", "phrase_fp.with_suffix.with_suffix", "gensim.models.phrases.Phrases", "gensim.models.phrases.Phrases.save", "create_embeddings.TokenIter", "create_embeddings.TokenIter", "id_token_file.write", "text_file.with_name.name.split", "phrase_fp.with_suffix.exists", "phrase_fp.with_suffix.exists", "create_embeddings.TokenIter", "phrase_fp.with_suffix.open", "enumerate", "str", "id_token_file.write", "new_tokens_file.write", "phrase_fp.with_suffix.resolve", "token.lower", "new_tokens_file.write"], "function", ["None"], ["", "", "", "", "", "def", "creating_embeddings", "(", "text_file", ":", "Path", ",", "\n", "embedding_fp", ":", "Path", ",", "\n", "embedding_class", ":", "BaseWordEmbeddingsModel", ",", "\n", "lower", ":", "bool", "=", "True", ",", "\n", "n_grams", ":", "Optional", "[", "int", "]", "=", "None", ",", "\n", "**", "embedding_kwargs", ")", "->", "Path", ":", "\n", "    ", "'''\n    '''", "\n", "if", "embedding_fp", ".", "is_file", "(", ")", ":", "\n", "        ", "return", "embedding_fp", "\n", "# Create the id, token file if it does not exist", "\n", "", "text_file", ".", "parent", ".", "mkdir", "(", "parents", "=", "True", ",", "exist_ok", "=", "True", ")", "\n", "id_token_fp", "=", "text_file", ".", "with_name", "(", "f'{text_file.name}'", ")", "\n", "if", "lower", ":", "\n", "        ", "id_token_fp", "=", "text_file", ".", "with_name", "(", "f'lower {text_file.name}'", ")", "\n", "", "if", "not", "id_token_fp", ".", "is_file", "(", ")", ":", "\n", "        ", "with", "id_token_fp", ".", "open", "(", "'w+'", ")", "as", "id_token_file", ":", "\n", "            ", "for", "index", ",", "tokens", "in", "enumerate", "(", "TokenIter", "(", "text_file", ",", "lower", "=", "lower", ")", ")", ":", "\n", "                ", "if", "index", "!=", "0", ":", "\n", "                    ", "id_token_file", ".", "write", "(", "'\\n'", ")", "\n", "", "if", "lower", ":", "\n", "                    ", "tokens", "=", "[", "token", ".", "lower", "(", ")", "for", "token", "in", "tokens", "]", "\n", "", "id_token_file", ".", "write", "(", "' '", ".", "join", "(", "tokens", ")", ")", "\n", "", "", "", "token_generator", "=", "TokenIter", "(", "id_token_fp", ",", "lower", "=", "lower", ")", "\n", "# Adds the phrase based logic", "\n", "if", "n_grams", "is", "not", "None", ":", "\n", "        ", "all_tokens", "=", "TokenIter", "(", "id_token_fp", ",", "lower", "=", "lower", ")", "\n", "for", "n_gram", "in", "range", "(", "2", ",", "n_grams", "+", "1", ")", ":", "\n", "            ", "phrase_fp", "=", "id_token_fp", ".", "name", ".", "split", "(", "'.'", ")", "[", "0", "]", "\n", "phrase_fp", "=", "id_token_fp", ".", "with_name", "(", "f'{phrase_fp} phrases_{n_gram}'", ")", "\n", "phrase_fp", "=", "phrase_fp", ".", "with_suffix", "(", "'.npy'", ")", "\n", "new_tokens_fp", "=", "phrase_fp", ".", "with_suffix", "(", "'.txt'", ")", "\n", "if", "phrase_fp", ".", "exists", "(", ")", "and", "new_tokens_fp", ".", "exists", "(", ")", ":", "\n", "                ", "all_tokens", "=", "TokenIter", "(", "id_token_fp", ",", "lower", "=", "lower", ")", "\n", "continue", "\n", "", "phrases", "=", "Phrases", "(", "token_generator", ")", "\n", "with", "new_tokens_fp", ".", "open", "(", "'w+'", ")", "as", "new_tokens_file", ":", "\n", "                ", "for", "index", ",", "tokens", "in", "enumerate", "(", "all_tokens", ")", ":", "\n", "                    ", "if", "index", "!=", "0", ":", "\n", "                        ", "new_tokens_file", ".", "write", "(", "'\\n'", ")", "\n", "", "phrase_tokens", "=", "phrases", "[", "tokens", "]", "\n", "new_tokens_file", ".", "write", "(", "' '", ".", "join", "(", "phrase_tokens", ")", ")", "\n", "", "", "phrases", ".", "save", "(", "str", "(", "phrase_fp", ".", "resolve", "(", ")", ")", ")", "\n", "all_tokens", "=", "TokenIter", "(", "new_tokens_fp", ",", "lower", "=", "lower", ")", "\n", "", "token_generator", "=", "all_tokens", "\n", "\n", "", "embedding_model", "=", "embedding_class", "(", "token_generator", ",", "**", "embedding_kwargs", ")", "\n", "embedding_fp", ".", "parent", ".", "mkdir", "(", "parents", "=", "True", ",", "exist_ok", "=", "True", ")", "\n", "embedding_path", "=", "str", "(", "embedding_fp", ".", "resolve", "(", ")", ")", "\n", "embedding_model", ".", "save", "(", "embedding_path", ")", "\n", "return", "embedding_fp", "\n", "\n"]]}