{"home.repos.pwc.inspect_result.alibaba_self-augmentation-strategy.None.train.parse_config": [[107, 185], ["argparse.ArgumentParser", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.parse_args"], "function", ["None"], ["", "def", "parse_config", "(", ")", ":", "\n", "\n", "    ", "parser", "=", "argparse", ".", "ArgumentParser", "(", ")", "\n", "\n", "# training hyper-parameters", "\n", "parser", ".", "add_argument", "(", "'-bert_setting'", ",", "type", "=", "str", ",", "default", "=", "\"SAS_small\"", ")", "\n", "parser", ".", "add_argument", "(", "'-num_epoch'", ",", "type", "=", "int", ",", "default", "=", "5", ")", "\n", "parser", ".", "add_argument", "(", "'-max_steps'", ",", "type", "=", "int", ",", "default", "=", "250_000", ")", "\n", "parser", ".", "add_argument", "(", "'-warmup_steps'", ",", "type", "=", "float", ",", "default", "=", "10000", ")", "\n", "parser", ".", "add_argument", "(", "'-batch_size'", ",", "type", "=", "int", ",", "default", "=", "8", ",", "help", "=", "''", ")", "# 128 in case of Electra", "\n", "parser", ".", "add_argument", "(", "'-gradient_accumulation_steps'", ",", "type", "=", "int", ",", "default", "=", "1", ")", "\n", "parser", ".", "add_argument", "(", "'-lr'", ",", "type", "=", "float", ",", "default", "=", "0.00175", ",", "help", "=", "''", ")", "# 5e-4 in case of Electra", "\n", "parser", ".", "add_argument", "(", "'-weight_decay'", ",", "type", "=", "float", ",", "default", "=", "0.01", ")", "\n", "parser", ".", "add_argument", "(", "'-correct_bias'", ",", "type", "=", "int", ",", "default", "=", "1", ")", "\n", "parser", ".", "add_argument", "(", "'-adam_epsilon'", ",", "type", "=", "float", ",", "default", "=", "1e-6", ")", "\n", "parser", ".", "add_argument", "(", "'-adam_beta1'", ",", "type", "=", "float", ",", "default", "=", "0.9", ")", "\n", "parser", ".", "add_argument", "(", "'-adam_beta2'", ",", "type", "=", "float", ",", "default", "=", "0.999", ")", "\n", "\n", "# SAS related", "\n", "parser", ".", "add_argument", "(", "'-model_type'", ",", "type", "=", "str", ",", "default", "=", "\"sas\"", ")", "\n", "parser", ".", "add_argument", "(", "'-model_name_or_path'", ",", "type", "=", "str", ",", "default", "=", "None", ")", "\n", "parser", ".", "add_argument", "(", "'-overwrite_pretrained_config'", ",", "type", "=", "int", ",", "default", "=", "0", ")", "\n", "parser", ".", "add_argument", "(", "'-cold_start_epochs'", ",", "type", "=", "float", ",", "default", "=", "1.0", ",", "help", "=", "'# of epochs for cold start'", ")", "\n", "parser", ".", "add_argument", "(", "'-cold_start_augumentation_method'", ",", "type", "=", "str", ",", "default", "=", "\"unigram\"", ",", "help", "=", "'mlm/unigram/random'", ")", "\n", "parser", ".", "add_argument", "(", "'-mlm_probability'", ",", "type", "=", "float", ",", "default", "=", "0.15", ",", "help", "=", "'0-1'", ")", "\n", "parser", ".", "add_argument", "(", "'-whole_word_masking'", ",", "type", "=", "int", ",", "default", "=", "0", ",", "help", "=", "''", ")", "\n", "\n", "parser", ".", "add_argument", "(", "'-position_embedding_type'", ",", "type", "=", "str", ",", "default", "=", "'absolute'", ",", "help", "=", "'absolute;absolute_self_only;relative_key;relative_key_query'", ")", "\n", "parser", ".", "add_argument", "(", "'-max_position_embeddings'", ",", "type", "=", "int", ",", "default", "=", "128", ",", "help", "=", "'Used in absolute and relative position embeddings'", ")", "\n", "\n", "parser", ".", "add_argument", "(", "'-relative_position_embedding'", ",", "type", "=", "int", ",", "default", "=", "0", ",", "help", "=", "'0: absolute only; < 0: relative only; >0: both'", ")", "\n", "parser", ".", "add_argument", "(", "'-absolute_position_embedding'", ",", "type", "=", "int", ",", "default", "=", "1", ",", "help", "=", "'1: BERT default position embedding at input layer'", ")", "\n", "\n", "parser", ".", "add_argument", "(", "'-augmentation_temperature'", ",", "type", "=", "float", ",", "default", "=", "1.0", ")", "\n", "parser", ".", "add_argument", "(", "'-augmentation_copies'", ",", "type", "=", "int", ",", "default", "=", "1", ")", "\n", "parser", ".", "add_argument", "(", "'-gen_weight'", ",", "type", "=", "float", ",", "default", "=", "1", ")", "\n", "parser", ".", "add_argument", "(", "'-dis_weight'", ",", "type", "=", "str", ",", "default", "=", "'50-50'", ",", "\n", "help", "=", "'It should be a string that includes either a single number such as \"50\" to indicate a flat value, '", "+", "\n", "'or two numbers such as \"50-100\" to indicate the startng end ending value'", ")", "# Stating and ending weight. Will be convert to an array ", "\n", "parser", ".", "add_argument", "(", "'-dis_weight_scheduler'", ",", "type", "=", "int", ",", "default", "=", "4", ",", "help", "=", "'0: Flat; 1: Linear increasing; 2: Epoch-wise Step Increasing; 4: Two-stages'", ")", "\n", "parser", ".", "add_argument", "(", "'-dynamic_masking'", ",", "type", "=", "int", ",", "default", "=", "0", ",", "help", "=", "'0: Static masking; 1: Dynamic masking'", ")", "\n", "\n", "# Logistic related", "\n", "parser", ".", "add_argument", "(", "'-seed'", ",", "type", "=", "int", ",", "default", "=", "50", ",", "help", "=", "''", ")", "\n", "parser", ".", "add_argument", "(", "'-tokenizer_name'", ",", "type", "=", "str", ",", "default", "=", "\"google/electra-small-generator\"", ")", "\n", "parser", ".", "add_argument", "(", "'-use_fast_tokenizer'", ",", "type", "=", "bool", ",", "default", "=", "True", ")", "# Our current data was generated with using fast tokenizer. This setting will be automatically applied in fine-tuning.", "\n", "parser", ".", "add_argument", "(", "'-dataset'", ",", "type", "=", "str", ",", "default", "=", "\"1M\"", ")", "\n", "parser", ".", "add_argument", "(", "'-dataset_eval'", ",", "type", "=", "str", ",", "default", "=", "\"eval\"", ")", "\n", "parser", ".", "add_argument", "(", "'-data_size'", ",", "type", "=", "int", ",", "default", "=", "100000000", ")", "\n", "parser", ".", "add_argument", "(", "'-eval_data_size'", ",", "type", "=", "int", ",", "default", "=", "51200", ")", "\n", "parser", ".", "add_argument", "(", "'-eval_steps'", ",", "type", "=", "int", ",", "default", "=", "4000", ",", "help", "=", "''", ")", "\n", "parser", ".", "add_argument", "(", "'-logging_steps'", ",", "type", "=", "int", ",", "default", "=", "200", ",", "help", "=", "''", ")", "\n", "parser", ".", "add_argument", "(", "'-dataloader_num_workers'", ",", "type", "=", "int", ",", "default", "=", "4", ",", "help", "=", "''", ")", "\n", "parser", ".", "add_argument", "(", "'-save_total_limit'", ",", "type", "=", "int", ",", "default", "=", "2", ",", "help", "=", "''", ")", "\n", "parser", ".", "add_argument", "(", "'-save_data_augmentation'", ",", "type", "=", "int", ",", "default", "=", "0", ",", "help", "=", "'0 or 1: Save data augmentation together with checkpoints or not'", ")", "\n", "parser", ".", "add_argument", "(", "'-save_steps'", ",", "type", "=", "int", ",", "default", "=", "50000", ",", "help", "=", "''", ")", "\n", "parser", ".", "add_argument", "(", "'--local_rank'", ",", "type", "=", "int", ",", "default", "=", "-", "1", ",", "help", "=", "''", ")", "\n", "\n", "parser", ".", "add_argument", "(", "'-debug_config'", ",", "type", "=", "dict", ",", "default", "=", "None", ",", "help", "=", "''", ")", "\n", "parser", ".", "add_argument", "(", "'-debugExtraMetrics'", ",", "type", "=", "int", ",", "default", "=", "1", ",", "help", "=", "'Show extra metrics or not (some of these metrics require extra notable calculation)'", ")", "\n", "parser", ".", "add_argument", "(", "'-debugMemStatsInterval'", ",", "type", "=", "int", ",", "default", "=", "1000", ",", "help", "=", "'Show memory stats or not'", ")", "\n", "parser", ".", "add_argument", "(", "'-debugGradOverflowInterval'", ",", "type", "=", "int", ",", "default", "=", "100", ",", "help", "=", "'Show gradient relatd data or not'", ")", "\n", "parser", ".", "add_argument", "(", "'-debugActivationInterval'", ",", "type", "=", "int", ",", "default", "=", "100_000_000", ",", "help", "=", "'Show activatiom relatd data or not'", ")", "\n", "parser", ".", "add_argument", "(", "'-debugMultiTasksConflictInterval'", ",", "type", "=", "int", ",", "default", "=", "1000", ",", "help", "=", "'Show gradient analysis between RTD vs. MLM '", ")", "\n", "\n", "parser", ".", "add_argument", "(", "'-has_apex'", ",", "type", "=", "str", ",", "default", "=", "_has_apex", ",", "help", "=", "'has apex or not'", ")", "\n", "parser", ".", "add_argument", "(", "'-fp16'", ",", "type", "=", "str", ",", "default", "=", "\"O2\"", ",", "help", "=", "'O1/O2'", ")", "\n", "parser", ".", "add_argument", "(", "'-cuda'", ",", "type", "=", "str", ",", "default", "=", "\"0\"", ",", "help", "=", "''", ")", "\n", "parser", ".", "add_argument", "(", "'-option'", ",", "type", "=", "str", ",", "default", "=", "\"0\"", ",", "help", "=", "''", ")", "\n", "\n", "parser", ".", "add_argument", "(", "'-pretrain_path'", ",", "type", "=", "str", ",", "default", "=", "None", ",", "help", "=", "''", ")", "\n", "parser", ".", "add_argument", "(", "'-do_train'", ",", "type", "=", "bool", ",", "default", "=", "True", ",", "help", "=", "''", ")", "\n", "parser", ".", "add_argument", "(", "'-do_eval'", ",", "type", "=", "bool", ",", "default", "=", "False", ",", "help", "=", "''", ")", "\n", "parser", ".", "add_argument", "(", "'-start_from_checkpoint'", ",", "type", "=", "bool", ",", "default", "=", "False", ",", "help", "=", "''", ")", "\n", "parser", ".", "add_argument", "(", "'-data_path'", ",", "type", "=", "str", ",", "default", "=", "\"../datasets/\"", ")", "\n", "parser", ".", "add_argument", "(", "'-output_dir'", ",", "type", "=", "str", ",", "default", "=", "\"default\"", ")", "\n", "\n", "return", "parser", ".", "parse_args", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.alibaba_self-augmentation-strategy.None.train.main": [[187, 404], ["train.parse_config", "print", "train.ModelArguments", "transformers.TrainingArguments", "logging.basicConfig", "logger.warning", "transformers.trainer_utils.is_main_process", "logger.info", "transformers.set_seed", "dict", "utils.data_collator_sas.SASTrainDataset", "utils.data_collator_sas.SASValidationDataset", "utils.data_collator_sas.DataCollatorForSasPretraining", "models.modeling_sas.SasForPreTraining.resize_token_embeddings", "print", "models.trainer_sas.SasTrainerForPretrain", "len", "transformers.utils.logging.set_verbosity_info", "transformers.utils.logging.enable_default_handler", "transformers.utils.logging.enable_explicit_format", "transformers.AutoTokenizer.from_pretrained", "utils.configuration_sas.SasConfig.from_pretrained", "utils.configuration_sas.SasConfig", "logger.warning", "utils.configuration_sas.SasConfig", "models.modeling_sas.SasForPreTraining.from_pretrained", "logger.info", "models.modeling_sas.SasForPreTraining", "len", "models.trainer_sas.SasTrainerForPretrain.remove_callback", "models.trainer_sas.SasTrainerForPretrain.train", "models.trainer_sas.SasTrainerForPretrain.save_model", "os.path.join", "models.trainer_sas.SasTrainerForPretrain.is_world_process_zero", "logger.info", "models.trainer_sas.SasTrainerForPretrain.evaluate", "math.exp", "os.path.join", "models.trainer_sas.SasTrainerForPretrain.is_world_process_zero", "str", "transformers.AutoTokenizer.from_pretrained", "ValueError", "models.trainer_sas.SasTrainerForPretrain.state.save_to_json", "transformers.trainer_utils.is_main_process", "len", "len", "bool", "type", "os.path.isdir", "open", "logger.info", "sorted", "os.path.join", "open", "logger.info", "sorted", "bool", "trainer.train.metrics.items", "logger.info", "writer.write", "results.items", "logger.info", "writer.write"], "function", ["home.repos.pwc.inspect_result.alibaba_self-augmentation-strategy.None.train.parse_config"], ["", "def", "main", "(", ")", ":", "\n", "# See all possible arguments in src/transformers/training_args.py", "\n", "# or by passing the --help flag to this script.", "\n", "# We now keep distinct sets of args, for a cleaner separation of concerns.", "\n", "\n", "    ", "opt", "=", "parse_config", "(", ")", "\n", "print", "(", "opt", ")", "\n", "opt", ".", "debug_config", "=", "{", "\n", "'logging_steps'", ":", "opt", ".", "logging_steps", ",", "\n", "'debugExtraMetrics'", ":", "opt", ".", "debugExtraMetrics", ",", "\n", "'debugMemStatsInterval'", ":", "opt", ".", "debugMemStatsInterval", ",", "\n", "'debugGradOverflowInterval'", ":", "opt", ".", "debugGradOverflowInterval", ",", "\n", "'debugActivationInterval'", ":", "opt", ".", "debugActivationInterval", ",", "\n", "'debugMultiTasksConflictInterval'", ":", "opt", ".", "debugMultiTasksConflictInterval", ",", "\n", "}", "\n", "if", "len", "(", "opt", ".", "cuda", ")", ">", "0", ":", "\n", "        ", "os", ".", "environ", "[", "\"CUDA_VISIBLE_DEVICES\"", "]", "=", "str", "(", "opt", ".", "cuda", ")", "if", "opt", ".", "cuda", "!=", "\"-1\"", "else", "\"\"", "\n", "\n", "", "opt", ".", "model_name_or_path", "=", "None", "if", "opt", ".", "model_name_or_path", "==", "'None'", "else", "opt", ".", "model_name_or_path", "\n", "\n", "model_args", "=", "ModelArguments", "(", "tokenizer_name", "=", "opt", ".", "tokenizer_name", ",", "\n", "use_fast_tokenizer", "=", "opt", ".", "use_fast_tokenizer", ",", "\n", "model_type", "=", "opt", ".", "model_type", ",", "\n", "model_name_or_path", "=", "opt", ".", "model_name_or_path", "# support restore checkpoint and continue training", "\n", ")", "\n", "\n", "training_args", "=", "TrainingArguments", "(", "\n", "output_dir", "=", "\"../output/%s\"", "%", "opt", ".", "output_dir", ",", "\n", "logging_dir", "=", "\"../output/tb/%s\"", "%", "opt", ".", "output_dir", ",", "\n", "overwrite_output_dir", "=", "True", ",", "\n", "do_train", "=", "opt", ".", "do_train", ",", "\n", "do_eval", "=", "opt", ".", "do_eval", ",", "\n", "num_train_epochs", "=", "-", "1", ",", "\n", "max_steps", "=", "opt", ".", "max_steps", ",", "\n", "gradient_accumulation_steps", "=", "opt", ".", "gradient_accumulation_steps", ",", "\n", "logging_first_step", "=", "True", ",", "\n", "logging_steps", "=", "opt", ".", "logging_steps", ",", "# 20,", "\n", "save_steps", "=", "opt", ".", "save_steps", ",", "# 50000,", "\n", "save_total_limit", "=", "opt", ".", "save_total_limit", ",", "# 2", "\n", "eval_steps", "=", "opt", ".", "eval_steps", ",", "# 4000,", "\n", "warmup_steps", "=", "opt", ".", "warmup_steps", ",", "\n", "weight_decay", "=", "opt", ".", "weight_decay", ",", "\n", "adam_epsilon", "=", "opt", ".", "adam_epsilon", ",", "\n", "per_device_train_batch_size", "=", "opt", ".", "batch_size", ",", "\n", "per_device_eval_batch_size", "=", "opt", ".", "batch_size", ",", "\n", "learning_rate", "=", "opt", ".", "lr", ",", "\n", "fp16", "=", "_has_apex", "and", "(", "opt", ".", "fp16", "!=", "\"None\"", ")", ",", "\n", "fp16_opt_level", "=", "opt", ".", "fp16", ",", "\n", "dataloader_num_workers", "=", "opt", ".", "dataloader_num_workers", ",", "\n", "seed", "=", "opt", ".", "seed", ",", "\n", "local_rank", "=", "opt", ".", "local_rank", ",", "\n", ")", "\n", "\n", "# Setup logging", "\n", "logging", ".", "basicConfig", "(", "\n", "format", "=", "\"%(asctime)s - %(levelname)s - %(name)s -   %(message)s\"", ",", "\n", "datefmt", "=", "\"%m/%d/%Y %H:%M:%S\"", ",", "\n", "level", "=", "logging", ".", "INFO", "if", "is_main_process", "(", "training_args", ".", "local_rank", ")", "else", "logging", ".", "WARN", ",", "\n", ")", "\n", "\n", "# Log on each process the small summary:", "\n", "logger", ".", "warning", "(", "\n", "f\"Process rank: {training_args.local_rank}, device: {training_args.device}, n_gpu: {training_args.n_gpu}\"", "\n", "+", "f\"distributed training: {bool(training_args.local_rank != -1)}, 16-bits training: {training_args.fp16}\"", "\n", ")", "\n", "\n", "# Set the verbosity to info of the Transformers logger (on main process only):", "\n", "if", "is_main_process", "(", "training_args", ".", "local_rank", ")", ":", "\n", "        ", "transformers", ".", "utils", ".", "logging", ".", "set_verbosity_info", "(", ")", "\n", "transformers", ".", "utils", ".", "logging", ".", "enable_default_handler", "(", ")", "\n", "transformers", ".", "utils", ".", "logging", ".", "enable_explicit_format", "(", ")", "\n", "", "logger", ".", "info", "(", "\"Training/evaluation parameters %s\"", ",", "training_args", ")", "\n", "\n", "# Set seed before initializing model.", "\n", "set_seed", "(", "training_args", ".", "seed", ")", "\n", "\n", "if", "model_args", ".", "tokenizer_name", ":", "\n", "        ", "tokenizer", "=", "AutoTokenizer", ".", "from_pretrained", "(", "model_args", ".", "tokenizer_name", ",", "use_fast", "=", "model_args", ".", "use_fast_tokenizer", ")", "\n", "", "elif", "model_args", ".", "model_name_or_path", ":", "\n", "        ", "tokenizer", "=", "AutoTokenizer", ".", "from_pretrained", "(", "model_args", ".", "model_name_or_path", ")", "\n", "", "else", ":", "\n", "        ", "raise", "ValueError", "(", "\n", "\"You are instantiating a new tokenizer from scratch. This is not supported by this script.\"", "\n", "\"You can do it from another script, save it, and load it from here, using --tokenizer_name.\"", "\n", ")", "\n", "\n", "", "tokenized_datasets", "=", "dict", "(", ")", "\n", "tokenized_datasets", "[", "\"train\"", "]", "=", "SASTrainDataset", "(", "\n", "file_path", "=", "opt", ".", "data_path", "+", "\"wiki_corpus_%s.npy\"", "%", "opt", ".", "dataset", ",", "\n", "dataset_size", "=", "opt", ".", "data_size", ",", "\n", "tokenizer", "=", "tokenizer", ",", "\n", "seed", "=", "opt", ".", "seed", ",", "\n", ")", "\n", "tokenized_datasets", "[", "\"validation\"", "]", "=", "SASValidationDataset", "(", "\n", "file_path", "=", "opt", ".", "data_path", "+", "\"wiki_corpus_%s.npy\"", "%", "opt", ".", "dataset_eval", ",", "\n", "dataset_size", "=", "opt", ".", "eval_data_size", ",", "\n", "tokenizer", "=", "tokenizer", ",", "\n", ")", "\n", "# the \"data_collator\" class implements how data is preprocessed for the language model and pre-training tasks.", "\n", "data_collator", "=", "DataCollatorForSasPretraining", "(", "\n", "tokenizer", "=", "tokenizer", ",", "\n", "cold_start_augumentation_method", "=", "opt", ".", "cold_start_augumentation_method", ",", "\n", "mlm_probability", "=", "opt", ".", "mlm_probability", ",", "\n", "whole_word_masking", "=", "opt", ".", "whole_word_masking", ",", "\n", "dynamic_masking", "=", "opt", ".", "dynamic_masking", ",", "\n", "empirical_distribution_file", "=", "opt", ".", "data_path", "+", "\"unigram.npy\"", ",", "\n", "dataset_sizes", "=", "{", "'train'", ":", "len", "(", "tokenized_datasets", "[", "\"train\"", "]", ")", ",", "'eval'", ":", "len", "(", "tokenized_datasets", "[", "\"validation\"", "]", ")", "}", "\n", ")", "\n", "\n", "# Load pretrained model and tokenizer", "\n", "if", "model_args", ".", "model_name_or_path", "and", "not", "opt", ".", "overwrite_pretrained_config", ":", "\n", "        ", "config", "=", "SasConfig", ".", "from_pretrained", "(", "model_args", ".", "model_name_or_path", ")", "\n", "", "else", ":", "\n", "        ", "config", "=", "SasConfig", "(", ")", "\n", "logger", ".", "warning", "(", "\"You are instantiating a new config instance from scratch.\"", ")", "\n", "\n", "bert_size", "=", "{", "\n", "# Electra-version small: [4, 12, 256, 1024],; BERT-24checkpoitns version small: [8, 4, 512, 2048]", "\n", "\"SAS_small\"", ":", "[", "4", ",", "12", ",", "256", ",", "1024", "]", ",", "\n", "\"SAS_base\"", ":", "[", "12", ",", "12", ",", "768", ",", "3072", "]", ",", "\n", "\"SAS_large\"", ":", "[", "16", ",", "24", ",", "1024", ",", "4096", "]", "\n", "}", "\n", "\n", "config", "=", "SasConfig", "(", "\n", "num_attention_heads", "=", "bert_size", "[", "opt", ".", "bert_setting", "]", "[", "0", "]", ",", "\n", "num_hidden_layers", "=", "bert_size", "[", "opt", ".", "bert_setting", "]", "[", "1", "]", ",", "\n", "hidden_size", "=", "bert_size", "[", "opt", ".", "bert_setting", "]", "[", "2", "]", ",", "\n", "embedding_size", "=", "bert_size", "[", "opt", ".", "bert_setting", "]", "[", "2", "]", ",", "\n", "intermediate_size", "=", "bert_size", "[", "opt", ".", "bert_setting", "]", "[", "3", "]", ",", "\n", "tie_word_embeddings", "=", "True", ",", "\n", "position_embedding_type", "=", "opt", ".", "position_embedding_type", ",", "\n", "max_position_embeddings", "=", "opt", ".", "max_position_embeddings", ",", "\n", "\n", "# Below is special parameter for SAS", "\n", "gen_weight", "=", "opt", ".", "gen_weight", ",", "\n", "dis_weight", "=", "opt", ".", "dis_weight", ",", "\n", "dis_weight_scheduler", "=", "opt", ".", "dis_weight_scheduler", ",", "\n", "dynamic_masking", "=", "opt", ".", "dynamic_masking", ",", "\n", "\n", "absolute_position_embedding", "=", "opt", ".", "absolute_position_embedding", ",", "\n", "relative_position_embedding", "=", "opt", ".", "relative_position_embedding", ",", "\n", "\n", "cold_start_epochs", "=", "opt", ".", "cold_start_epochs", ",", "\n", "debug_config", "=", "opt", ".", "debug_config", "\n", ")", "\n", "\n", "", "if", "model_args", ".", "model_name_or_path", ":", "\n", "        ", "model", "=", "SasForPreTraining", ".", "from_pretrained", "(", "\n", "model_args", ".", "model_name_or_path", ",", "\n", "from_tf", "=", "bool", "(", "\".ckpt\"", "in", "model_args", ".", "model_name_or_path", ")", ",", "\n", "config", "=", "config", ",", "\n", "cache_dir", "=", "model_args", ".", "cache_dir", ",", "\n", "revision", "=", "model_args", ".", "model_revision", ",", "\n", "use_auth_token", "=", "True", "if", "model_args", ".", "use_auth_token", "else", "None", ",", "\n", ")", "\n", "", "else", ":", "\n", "        ", "logger", ".", "info", "(", "\"Training new model from scratch\"", ")", "\n", "model", "=", "SasForPreTraining", "(", "config", ")", "\n", "", "model", ".", "resize_token_embeddings", "(", "len", "(", "tokenizer", ")", ")", "\n", "model", ".", "dataset", "=", "tokenized_datasets", "[", "\"train\"", "]", "\n", "model", ".", "model_name_or_path", "=", "model_args", ".", "model_name_or_path", "\n", "model", ".", "save_data_augmentation", "=", "opt", ".", "save_data_augmentation", "\n", "print", "(", "model", ")", "\n", "\n", "# Initialize our Trainer", "\n", "trainer", "=", "Trainer", "(", "\n", "model", "=", "model", ",", "\n", "callbacks", "=", "[", "model", ".", "trainer_callback", ",", "model", ".", "wandb_callback", ",", "model", ".", "tensorboard_callback", "]", ",", "\n", "args", "=", "training_args", ",", "\n", "train_dataset", "=", "tokenized_datasets", "[", "\"train\"", "]", "if", "training_args", ".", "do_train", "else", "None", ",", "\n", "eval_dataset", "=", "tokenized_datasets", "[", "\"validation\"", "]", "if", "training_args", ".", "do_eval", "else", "None", ",", "\n", "tokenizer", "=", "tokenizer", ",", "\n", "data_collator", "=", "data_collator", ",", "\n", ")", "\n", "for", "cb", "in", "[", "cb", "for", "cb", "in", "trainer", ".", "callback_handler", ".", "callbacks", "if", "type", "(", "cb", ")", "in", "[", "DefaultFlowCallback", ",", "WandbCallback", ",", "TensorBoardCallback", "]", "]", ":", "\n", "        ", "trainer", ".", "remove_callback", "(", "cb", ")", "\n", "\n", "# Training", "\n", "", "if", "training_args", ".", "do_train", ":", "\n", "        ", "model_path", "=", "(", "\n", "model_args", ".", "model_name_or_path", "\n", "if", "(", "model_args", ".", "model_name_or_path", "is", "not", "None", "and", "os", ".", "path", ".", "isdir", "(", "model_args", ".", "model_name_or_path", ")", ")", "\n", "else", "None", "\n", ")", "\n", "train_result", "=", "trainer", ".", "train", "(", "model_path", "=", "model_path", ")", "\n", "trainer", ".", "save_model", "(", ")", "# Saves the tokenizer too for easy upload", "\n", "\n", "output_train_file", "=", "os", ".", "path", ".", "join", "(", "training_args", ".", "output_dir", ",", "\"train_results.txt\"", ")", "\n", "if", "trainer", ".", "is_world_process_zero", "(", ")", ":", "\n", "            ", "with", "open", "(", "output_train_file", ",", "\"w\"", ")", "as", "writer", ":", "\n", "                ", "logger", ".", "info", "(", "\"***** Train results *****\"", ")", "\n", "for", "key", ",", "value", "in", "sorted", "(", "train_result", ".", "metrics", ".", "items", "(", ")", ")", ":", "\n", "                    ", "logger", ".", "info", "(", "f\"  {key} = {value}\"", ")", "\n", "writer", ".", "write", "(", "f\"{key} = {value}\\n\"", ")", "\n", "\n", "# Need to save the state, since Trainer.save_model saves only the tokenizer with the model", "\n", "", "", "trainer", ".", "state", ".", "save_to_json", "(", "os", ".", "path", ".", "join", "(", "training_args", ".", "output_dir", ",", "\"trainer_state.json\"", ")", ")", "\n", "\n", "# Evaluation", "\n", "", "", "results", "=", "{", "}", "\n", "if", "training_args", ".", "do_eval", ":", "\n", "        ", "logger", ".", "info", "(", "\"*** Evaluate ***\"", ")", "\n", "\n", "eval_output", "=", "trainer", ".", "evaluate", "(", ")", "\n", "\n", "perplexity", "=", "math", ".", "exp", "(", "eval_output", "[", "\"eval_loss\"", "]", ")", "\n", "results", "[", "\"perplexity\"", "]", "=", "perplexity", "\n", "\n", "output_eval_file", "=", "os", ".", "path", ".", "join", "(", "training_args", ".", "output_dir", ",", "\"eval_results_mlm.txt\"", ")", "\n", "if", "trainer", ".", "is_world_process_zero", "(", ")", ":", "\n", "            ", "with", "open", "(", "output_eval_file", ",", "\"w\"", ")", "as", "writer", ":", "\n", "                ", "logger", ".", "info", "(", "\"***** Eval results *****\"", ")", "\n", "for", "key", ",", "value", "in", "sorted", "(", "results", ".", "items", "(", ")", ")", ":", "\n", "                    ", "logger", ".", "info", "(", "f\"  {key} = {value}\"", ")", "\n", "writer", ".", "write", "(", "f\"{key} = {value}\\n\"", ")", "\n", "\n", "", "", "", "", "return", "results", "\n", "\n"]], "home.repos.pwc.inspect_result.alibaba_self-augmentation-strategy.None.train._mp_fn": [[406, 409], ["train.main"], "function", ["home.repos.pwc.inspect_result.alibaba_self-augmentation-strategy.None.train.main"], ["", "def", "_mp_fn", "(", "index", ")", ":", "\n", "# For xla_spawn (TPUs)", "\n", "    ", "main", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.alibaba_self-augmentation-strategy.utils.glue_load_dataset.GlueConfig.__init__": [[96, 136], ["datasets.BuilderConfig.__init__", "datasets.Version"], "methods", ["home.repos.pwc.inspect_result.alibaba_self-augmentation-strategy.models.trainer_sas.SequentialDistributedSampler.__init__"], ["def", "__init__", "(", "\n", "self", ",", "\n", "text_features", ",", "\n", "label_column", ",", "\n", "data_url", ",", "\n", "data_dir", ",", "\n", "citation", ",", "\n", "url", ",", "\n", "label_classes", "=", "None", ",", "\n", "process_label", "=", "lambda", "x", ":", "x", ",", "\n", "**", "kwargs", ",", "\n", ")", ":", "\n", "        ", "\"\"\"BuilderConfig for GLUE.\n\n        Args:\n          text_features: `dict[string, string]`, map from the name of the feature\n            dict for each text field to the name of the column in the tsv file\n          label_column: `string`, name of the column in the tsv file corresponding\n            to the label\n          data_url: `string`, url to download the zip file from\n          data_dir: `string`, the path to the folder containing the tsv files in the\n            downloaded zip\n          citation: `string`, citation for the data set\n          url: `string`, url for information about the data set\n          label_classes: `list[string]`, the list of classes if the label is\n            categorical. If not provided, then the label will be of type\n            `datasets.Value('float32')`.\n          process_label: `Function[string, any]`, function  taking in the raw value\n            of the label and processing it to the form required by the label feature\n          **kwargs: keyword arguments forwarded to super.\n        \"\"\"", "\n", "super", "(", "GlueConfig", ",", "self", ")", ".", "__init__", "(", "version", "=", "datasets", ".", "Version", "(", "\"1.0.0\"", ",", "\"\"", ")", ",", "**", "kwargs", ")", "\n", "self", ".", "text_features", "=", "text_features", "\n", "self", ".", "label_column", "=", "label_column", "\n", "self", ".", "label_classes", "=", "label_classes", "\n", "self", ".", "data_url", "=", "data_url", "\n", "self", ".", "data_dir", "=", "data_dir", "\n", "self", ".", "citation", "=", "citation", "\n", "self", ".", "url", "=", "url", "\n", "self", ".", "process_label", "=", "process_label", "\n", "\n"]], "home.repos.pwc.inspect_result.alibaba_self-augmentation-strategy.utils.glue_load_dataset.Glue._info": [[458, 470], ["datasets.Value", "datasets.DatasetInfo", "datasets.Value", "datasets.features.ClassLabel", "datasets.Value", "six.iterkeys", "datasets.Features"], "methods", ["None"], ["def", "_info", "(", "self", ")", ":", "\n", "        ", "features", "=", "{", "text_feature", ":", "datasets", ".", "Value", "(", "\"string\"", ")", "for", "text_feature", "in", "six", ".", "iterkeys", "(", "self", ".", "config", ".", "text_features", ")", "}", "\n", "if", "self", ".", "config", ".", "label_classes", ":", "\n", "            ", "features", "[", "\"label\"", "]", "=", "datasets", ".", "features", ".", "ClassLabel", "(", "names", "=", "self", ".", "config", ".", "label_classes", ")", "\n", "", "else", ":", "\n", "            ", "features", "[", "\"label\"", "]", "=", "datasets", ".", "Value", "(", "\"float32\"", ")", "\n", "", "features", "[", "\"idx\"", "]", "=", "datasets", ".", "Value", "(", "\"int32\"", ")", "\n", "return", "datasets", ".", "DatasetInfo", "(", "\n", "description", "=", "_GLUE_DESCRIPTION", ",", "\n", "features", "=", "datasets", ".", "Features", "(", "features", ")", ",", "\n", "homepage", "=", "self", ".", "config", ".", "url", ",", "\n", "citation", "=", "self", ".", "config", ".", "citation", "+", "\"\\n\"", "+", "_GLUE_CITATION", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.alibaba_self-augmentation-strategy.utils.glue_load_dataset.Glue._split_generators": [[472, 541], ["datasets.SplitGenerator", "dl_manager.download", "dl_manager.download", "dl_manager.download_and_extract", "os.path.join", "datasets.SplitGenerator", "glue_load_dataset._mnli_split_generator", "glue_load_dataset._mnli_split_generator", "glue_load_dataset._mnli_split_generator", "glue_load_dataset._mnli_split_generator", "os.path.join", "glue_load_dataset._mnli_split_generator", "glue_load_dataset._mnli_split_generator", "glue_load_dataset._mnli_split_generator", "glue_load_dataset._mnli_split_generator", "datasets.SplitGenerator", "datasets.SplitGenerator", "os.path.join", "os.path.join"], "methods", ["home.repos.pwc.inspect_result.alibaba_self-augmentation-strategy.utils.glue_load_dataset._mnli_split_generator", "home.repos.pwc.inspect_result.alibaba_self-augmentation-strategy.utils.glue_load_dataset._mnli_split_generator", "home.repos.pwc.inspect_result.alibaba_self-augmentation-strategy.utils.glue_load_dataset._mnli_split_generator", "home.repos.pwc.inspect_result.alibaba_self-augmentation-strategy.utils.glue_load_dataset._mnli_split_generator", "home.repos.pwc.inspect_result.alibaba_self-augmentation-strategy.utils.glue_load_dataset._mnli_split_generator", "home.repos.pwc.inspect_result.alibaba_self-augmentation-strategy.utils.glue_load_dataset._mnli_split_generator", "home.repos.pwc.inspect_result.alibaba_self-augmentation-strategy.utils.glue_load_dataset._mnli_split_generator", "home.repos.pwc.inspect_result.alibaba_self-augmentation-strategy.utils.glue_load_dataset._mnli_split_generator"], ["", "def", "_split_generators", "(", "self", ",", "dl_manager", ")", ":", "\n", "        ", "if", "self", ".", "config", ".", "name", "==", "\"ax\"", ":", "\n", "            ", "data_file", "=", "dl_manager", ".", "download", "(", "self", ".", "config", ".", "data_url", ")", "\n", "return", "[", "\n", "datasets", ".", "SplitGenerator", "(", "\n", "name", "=", "datasets", ".", "Split", ".", "TEST", ",", "\n", "gen_kwargs", "=", "{", "\n", "\"data_file\"", ":", "data_file", ",", "\n", "\"split\"", ":", "\"test\"", ",", "\n", "}", ",", "\n", ")", "\n", "]", "\n", "\n", "", "if", "self", ".", "config", ".", "name", "==", "\"mrpc\"", ":", "\n", "            ", "data_dir", "=", "None", "\n", "mrpc_files", "=", "dl_manager", ".", "download", "(", "\n", "{", "\n", "\"dev_ids\"", ":", "_MRPC_DEV_IDS", ",", "\n", "\"train\"", ":", "_MRPC_TRAIN", ",", "\n", "\"test\"", ":", "_MRPC_TEST", ",", "\n", "}", "\n", ")", "\n", "", "else", ":", "\n", "            ", "dl_dir", "=", "dl_manager", ".", "download_and_extract", "(", "self", ".", "config", ".", "data_url", ")", "\n", "data_dir", "=", "os", ".", "path", ".", "join", "(", "dl_dir", ",", "self", ".", "config", ".", "data_dir", ")", "\n", "mrpc_files", "=", "None", "\n", "", "train_split", "=", "datasets", ".", "SplitGenerator", "(", "\n", "name", "=", "datasets", ".", "Split", ".", "TRAIN", ",", "\n", "gen_kwargs", "=", "{", "\n", "\"data_file\"", ":", "os", ".", "path", ".", "join", "(", "data_dir", "or", "\"\"", ",", "\"train.tsv\"", ")", ",", "\n", "\"split\"", ":", "\"train\"", ",", "\n", "\"mrpc_files\"", ":", "mrpc_files", ",", "\n", "}", ",", "\n", ")", "\n", "if", "self", ".", "config", ".", "name", "==", "\"mnli\"", ":", "\n", "            ", "return", "[", "\n", "train_split", ",", "\n", "_mnli_split_generator", "(", "\"validation_matched\"", ",", "data_dir", ",", "\"dev\"", ",", "matched", "=", "True", ")", ",", "\n", "_mnli_split_generator", "(", "\"validation_mismatched\"", ",", "data_dir", ",", "\"dev\"", ",", "matched", "=", "False", ")", ",", "\n", "_mnli_split_generator", "(", "\"test_matched\"", ",", "data_dir", ",", "\"test\"", ",", "matched", "=", "True", ")", ",", "\n", "_mnli_split_generator", "(", "\"test_mismatched\"", ",", "data_dir", ",", "\"test\"", ",", "matched", "=", "False", ")", ",", "\n", "]", "\n", "", "elif", "self", ".", "config", ".", "name", "==", "\"mnli_matched\"", ":", "\n", "            ", "return", "[", "\n", "_mnli_split_generator", "(", "\"validation\"", ",", "data_dir", ",", "\"dev\"", ",", "matched", "=", "True", ")", ",", "\n", "_mnli_split_generator", "(", "\"test\"", ",", "data_dir", ",", "\"test\"", ",", "matched", "=", "True", ")", ",", "\n", "]", "\n", "", "elif", "self", ".", "config", ".", "name", "==", "\"mnli_mismatched\"", ":", "\n", "            ", "return", "[", "\n", "_mnli_split_generator", "(", "\"validation\"", ",", "data_dir", ",", "\"dev\"", ",", "matched", "=", "False", ")", ",", "\n", "_mnli_split_generator", "(", "\"test\"", ",", "data_dir", ",", "\"test\"", ",", "matched", "=", "False", ")", ",", "\n", "]", "\n", "", "else", ":", "\n", "            ", "return", "[", "\n", "train_split", ",", "\n", "datasets", ".", "SplitGenerator", "(", "\n", "name", "=", "datasets", ".", "Split", ".", "VALIDATION", ",", "\n", "gen_kwargs", "=", "{", "\n", "\"data_file\"", ":", "os", ".", "path", ".", "join", "(", "data_dir", "or", "\"\"", ",", "\"dev.tsv\"", ")", ",", "\n", "\"split\"", ":", "\"dev\"", ",", "\n", "\"mrpc_files\"", ":", "mrpc_files", ",", "\n", "}", ",", "\n", ")", ",", "\n", "datasets", ".", "SplitGenerator", "(", "\n", "name", "=", "datasets", ".", "Split", ".", "TEST", ",", "\n", "gen_kwargs", "=", "{", "\n", "\"data_file\"", ":", "os", ".", "path", ".", "join", "(", "data_dir", "or", "\"\"", ",", "\"test.tsv\"", ")", ",", "\n", "\"split\"", ":", "\"test\"", ",", "\n", "\"mrpc_files\"", ":", "mrpc_files", ",", "\n", "}", ",", "\n"]], "home.repos.pwc.inspect_result.alibaba_self-augmentation-strategy.utils.glue_load_dataset.Glue._generate_examples": [[545, 590], ["glue_load_dataset.Glue._generate_example_mrpc_files", "open", "csv.DictReader", "enumerate", "csv.reader", "six.itervalues", "process_label", "process_label", "six.iteritems", "int"], "methods", ["home.repos.pwc.inspect_result.alibaba_self-augmentation-strategy.utils.glue_load_dataset.Glue._generate_example_mrpc_files"], ["", "", "def", "_generate_examples", "(", "self", ",", "data_file", ",", "split", ",", "mrpc_files", "=", "None", ")", ":", "\n", "        ", "if", "self", ".", "config", ".", "name", "==", "\"mrpc\"", ":", "\n", "# We have to prepare the MRPC dataset from the original sources ourselves.", "\n", "            ", "examples", "=", "self", ".", "_generate_example_mrpc_files", "(", "mrpc_files", "=", "mrpc_files", ",", "split", "=", "split", ")", "\n", "for", "example", "in", "examples", ":", "\n", "                ", "yield", "example", "[", "\"idx\"", "]", ",", "example", "\n", "", "", "else", ":", "\n", "            ", "process_label", "=", "self", ".", "config", ".", "process_label", "\n", "label_classes", "=", "self", ".", "config", ".", "label_classes", "\n", "\n", "# The train and dev files for CoLA are the only tsv files without a", "\n", "# header.", "\n", "is_cola_non_test", "=", "self", ".", "config", ".", "name", "==", "\"cola\"", "and", "split", "!=", "\"test\"", "\n", "\n", "with", "open", "(", "data_file", ",", "encoding", "=", "\"utf8\"", ")", "as", "f", ":", "\n", "                ", "reader", "=", "csv", ".", "DictReader", "(", "f", ",", "delimiter", "=", "\"\\t\"", ",", "quoting", "=", "csv", ".", "QUOTE_NONE", ")", "\n", "if", "is_cola_non_test", ":", "\n", "                    ", "reader", "=", "csv", ".", "reader", "(", "f", ",", "delimiter", "=", "\"\\t\"", ",", "quoting", "=", "csv", ".", "QUOTE_NONE", ")", "\n", "\n", "", "for", "n", ",", "row", "in", "enumerate", "(", "reader", ")", ":", "\n", "                    ", "if", "is_cola_non_test", ":", "\n", "                        ", "row", "=", "{", "\n", "\"sentence\"", ":", "row", "[", "3", "]", ",", "\n", "\"is_acceptable\"", ":", "row", "[", "1", "]", ",", "\n", "}", "\n", "\n", "", "example", "=", "{", "feat", ":", "row", "[", "col", "]", "for", "feat", ",", "col", "in", "six", ".", "iteritems", "(", "self", ".", "config", ".", "text_features", ")", "}", "\n", "example", "[", "\"idx\"", "]", "=", "n", "\n", "\n", "if", "self", ".", "config", ".", "label_column", "in", "row", ":", "\n", "                        ", "label", "=", "row", "[", "self", ".", "config", ".", "label_column", "]", "\n", "# For some tasks, the label is represented as 0 and 1 in the tsv", "\n", "# files and needs to be cast to integer to work with the feature.", "\n", "if", "label_classes", "and", "label", "not", "in", "label_classes", ":", "\n", "                            ", "label", "=", "int", "(", "label", ")", "if", "label", "else", "None", "\n", "", "example", "[", "\"label\"", "]", "=", "process_label", "(", "label", ")", "\n", "", "else", ":", "\n", "                        ", "example", "[", "\"label\"", "]", "=", "process_label", "(", "-", "1", ")", "\n", "\n", "# Filter out corrupted rows.", "\n", "", "for", "value", "in", "six", ".", "itervalues", "(", "example", ")", ":", "\n", "                        ", "if", "value", "is", "None", ":", "\n", "                            ", "break", "\n", "", "", "else", ":", "\n", "                        ", "yield", "example", "[", "\"idx\"", "]", ",", "example", "\n", "\n"]], "home.repos.pwc.inspect_result.alibaba_self-augmentation-strategy.utils.glue_load_dataset.Glue._generate_example_mrpc_files": [[591, 619], ["open", "csv.DictReader", "enumerate", "open", "csv.reader", "open", "f.seek", "csv.DictReader", "enumerate", "int"], "methods", ["None"], ["", "", "", "", "", "def", "_generate_example_mrpc_files", "(", "self", ",", "mrpc_files", ",", "split", ")", ":", "\n", "        ", "if", "split", "==", "\"test\"", ":", "\n", "            ", "with", "open", "(", "mrpc_files", "[", "\"test\"", "]", ",", "encoding", "=", "\"utf8\"", ")", "as", "f", ":", "\n", "                ", "reader", "=", "csv", ".", "DictReader", "(", "f", ",", "delimiter", "=", "\"\\t\"", ",", "quoting", "=", "csv", ".", "QUOTE_NONE", ")", "\n", "for", "n", ",", "row", "in", "enumerate", "(", "reader", ")", ":", "\n", "                    ", "yield", "{", "\n", "\"sentence1\"", ":", "row", "[", "\"#1 String\"", "]", ",", "\n", "\"sentence2\"", ":", "row", "[", "\"#2 String\"", "]", ",", "\n", "\"label\"", ":", "-", "1", ",", "\n", "\"idx\"", ":", "n", ",", "\n", "}", "\n", "", "", "", "else", ":", "\n", "            ", "with", "open", "(", "mrpc_files", "[", "\"dev_ids\"", "]", ",", "encoding", "=", "\"utf8\"", ")", "as", "f", ":", "\n", "                ", "reader", "=", "csv", ".", "reader", "(", "f", ",", "delimiter", "=", "\"\\t\"", ",", "quoting", "=", "csv", ".", "QUOTE_NONE", ")", "\n", "dev_ids", "=", "[", "[", "row", "[", "0", "]", ",", "row", "[", "1", "]", "]", "for", "row", "in", "reader", "]", "\n", "", "with", "open", "(", "mrpc_files", "[", "\"train\"", "]", ",", "encoding", "=", "\"utf8\"", ")", "as", "f", ":", "\n", "# The first 3 bytes are the utf-8 BOM \\xef\\xbb\\xbf, which messes with", "\n", "# the Quality key.", "\n", "                ", "f", ".", "seek", "(", "3", ")", "\n", "reader", "=", "csv", ".", "DictReader", "(", "f", ",", "delimiter", "=", "\"\\t\"", ",", "quoting", "=", "csv", ".", "QUOTE_NONE", ")", "\n", "for", "n", ",", "row", "in", "enumerate", "(", "reader", ")", ":", "\n", "                    ", "is_row_in_dev", "=", "[", "row", "[", "\"#1 ID\"", "]", ",", "row", "[", "\"#2 ID\"", "]", "]", "in", "dev_ids", "\n", "if", "is_row_in_dev", "==", "(", "split", "==", "\"dev\"", ")", ":", "\n", "                        ", "yield", "{", "\n", "\"sentence1\"", ":", "row", "[", "\"#1 String\"", "]", ",", "\n", "\"sentence2\"", ":", "row", "[", "\"#2 String\"", "]", ",", "\n", "\"label\"", ":", "int", "(", "row", "[", "\"Quality\"", "]", ")", ",", "\n", "\"idx\"", ":", "n", ",", "\n", "}", "\n"]], "home.repos.pwc.inspect_result.alibaba_self-augmentation-strategy.utils.glue_load_dataset._mnli_split_generator": [[622, 629], ["datasets.SplitGenerator", "os.path.join"], "function", ["None"], ["", "", "", "", "", "", "def", "_mnli_split_generator", "(", "name", ",", "data_dir", ",", "split", ",", "matched", ")", ":", "\n", "    ", "return", "datasets", ".", "SplitGenerator", "(", "\n", "name", "=", "name", ",", "\n", "gen_kwargs", "=", "{", "\n", "\"data_file\"", ":", "os", ".", "path", ".", "join", "(", "data_dir", ",", "\"%s_%s.tsv\"", "%", "(", "split", ",", "\"matched\"", "if", "matched", "else", "\"mismatched\"", ")", ")", ",", "\n", "\"split\"", ":", "split", ",", "\n", "\"mrpc_files\"", ":", "None", ",", "\n", "}", ",", "\n"]], "home.repos.pwc.inspect_result.alibaba_self-augmentation-strategy.utils.glue_load_metric.Glue._info": [[77, 110], ["datasets.MetricInfo", "KeyError", "datasets.Features", "datasets.Value", "datasets.Value"], "methods", ["None"], ["    ", "def", "_info", "(", "self", ")", ":", "\n", "        ", "if", "self", ".", "config_name", "not", "in", "[", "\n", "\"sst2\"", ",", "\n", "\"mnli\"", ",", "\n", "\"mnli_mismatched\"", ",", "\n", "\"mnli_matched\"", ",", "\n", "\"cola\"", ",", "\n", "\"stsb\"", ",", "\n", "\"mrpc\"", ",", "\n", "\"qqp\"", ",", "\n", "\"qnli\"", ",", "\n", "\"rte\"", ",", "\n", "\"wnli\"", ",", "\n", "\"hans\"", ",", "\n", "]", ":", "\n", "            ", "raise", "KeyError", "(", "\n", "\"You should supply a configuration name selected in \"", "\n", "'[\"sst2\", \"mnli\", \"mnli_mismatched\", \"mnli_matched\", '", "\n", "'\"cola\", \"stsb\", \"mrpc\", \"qqp\", \"qnli\", \"rte\", \"wnli\", \"hans\"]'", "\n", ")", "\n", "", "return", "datasets", ".", "MetricInfo", "(", "\n", "description", "=", "_DESCRIPTION", ",", "\n", "citation", "=", "_CITATION", ",", "\n", "inputs_description", "=", "_KWARGS_DESCRIPTION", ",", "\n", "features", "=", "datasets", ".", "Features", "(", "\n", "{", "\n", "\"predictions\"", ":", "datasets", ".", "Value", "(", "\"int64\"", "if", "self", ".", "config_name", "!=", "\"stsb\"", "else", "\"float32\"", ")", ",", "\n", "\"references\"", ":", "datasets", ".", "Value", "(", "\"int64\"", "if", "self", ".", "config_name", "!=", "\"stsb\"", "else", "\"float32\"", ")", ",", "\n", "}", "\n", ")", ",", "\n", "codebase_urls", "=", "[", "]", ",", "\n", "reference_urls", "=", "[", "]", ",", "\n", "format", "=", "\"numpy\"", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.alibaba_self-augmentation-strategy.utils.glue_load_metric.Glue._compute": [[112, 124], ["sklearn.metrics.matthews_corrcoef", "glue_load_metric.pearson_and_spearman", "glue_load_metric.acc_and_f1", "KeyError", "glue_load_metric.simple_accuracy"], "methods", ["home.repos.pwc.inspect_result.alibaba_self-augmentation-strategy.utils.glue_load_metric.pearson_and_spearman", "home.repos.pwc.inspect_result.alibaba_self-augmentation-strategy.utils.glue_load_metric.acc_and_f1", "home.repos.pwc.inspect_result.alibaba_self-augmentation-strategy.utils.glue_load_metric.simple_accuracy"], ["", "def", "_compute", "(", "self", ",", "predictions", ",", "references", ")", ":", "\n", "        ", "if", "self", ".", "config_name", "==", "\"cola\"", ":", "\n", "            ", "return", "{", "\"matthews_correlation\"", ":", "matthews_corrcoef", "(", "references", ",", "predictions", ")", "}", "\n", "", "elif", "self", ".", "config_name", "==", "\"stsb\"", ":", "\n", "            ", "return", "pearson_and_spearman", "(", "predictions", ",", "references", ")", "\n", "", "elif", "self", ".", "config_name", "in", "[", "\"mrpc\"", ",", "\"qqp\"", "]", ":", "\n", "            ", "return", "acc_and_f1", "(", "predictions", ",", "references", ")", "\n", "", "elif", "self", ".", "config_name", "in", "[", "\"sst2\"", ",", "\"mnli\"", ",", "\"mnli_mismatched\"", ",", "\"mnli_matched\"", ",", "\"qnli\"", ",", "\"rte\"", ",", "\"wnli\"", ",", "\"hans\"", "]", ":", "\n", "            ", "return", "{", "\"accuracy\"", ":", "simple_accuracy", "(", "predictions", ",", "references", ")", "}", "\n", "", "else", ":", "\n", "            ", "raise", "KeyError", "(", "\n", "\"You should supply a configuration name selected in \"", "\n", "'[\"sst2\", \"mnli\", \"mnli_mismatched\", \"mnli_matched\", '", "\n"]], "home.repos.pwc.inspect_result.alibaba_self-augmentation-strategy.utils.glue_load_metric.simple_accuracy": [[54, 56], ["None"], "function", ["None"], ["def", "simple_accuracy", "(", "preds", ",", "labels", ")", ":", "\n", "    ", "return", "(", "preds", "==", "labels", ")", ".", "mean", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.alibaba_self-augmentation-strategy.utils.glue_load_metric.acc_and_f1": [[58, 64], ["glue_load_metric.simple_accuracy", "sklearn.metrics.f1_score"], "function", ["home.repos.pwc.inspect_result.alibaba_self-augmentation-strategy.utils.glue_load_metric.simple_accuracy"], ["", "def", "acc_and_f1", "(", "preds", ",", "labels", ")", ":", "\n", "    ", "acc", "=", "simple_accuracy", "(", "preds", ",", "labels", ")", "\n", "f1", "=", "f1_score", "(", "y_true", "=", "labels", ",", "y_pred", "=", "preds", ")", "\n", "return", "{", "\n", "\"accuracy\"", ":", "acc", ",", "\n", "\"f1\"", ":", "f1", ",", "\n", "}", "\n"]], "home.repos.pwc.inspect_result.alibaba_self-augmentation-strategy.utils.glue_load_metric.pearson_and_spearman": [[67, 73], ["scipy.stats.pearsonr", "scipy.stats.spearmanr"], "function", ["None"], ["", "def", "pearson_and_spearman", "(", "preds", ",", "labels", ")", ":", "\n", "    ", "pearson_corr", "=", "pearsonr", "(", "preds", ",", "labels", ")", "[", "0", "]", "\n", "spearman_corr", "=", "spearmanr", "(", "preds", ",", "labels", ")", "[", "0", "]", "\n", "return", "{", "\n", "\"pearson\"", ":", "pearson_corr", ",", "\n", "\"spearmanr\"", ":", "spearman_corr", ",", "\n", "}", "\n"]], "home.repos.pwc.inspect_result.alibaba_self-augmentation-strategy.utils.sas_utils.SequenceSideInfo.__init__": [[61, 75], ["nltk.tokenize.punkt.PunktSentenceTokenizer", "set", "torch.tensor", "ElectraTokenizer.from_pretrained", "sas_utils.SequenceSideInfo.tokenizer.decode", "range", "range", "len", "min", "min", "len", "len"], "methods", ["None"], ["    ", "def", "__init__", "(", "self", ",", "tokenizer", "=", "None", ")", ":", "\n", "        ", "if", "tokenizer", "is", "not", "None", ":", "\n", "            ", "self", ".", "tokenizer", "=", "tokenizer", "\n", "", "else", ":", "\n", "            ", "from", "transformers", "import", "ElectraTokenizer", "\n", "self", ".", "tokenizer", "=", "ElectraTokenizer", ".", "from_pretrained", "(", "'google/electra-small-generator'", ")", "\n", "\n", "", "self", ".", "sen_tokenizer", "=", "nltk", ".", "tokenize", ".", "punkt", ".", "PunktSentenceTokenizer", "(", ")", "\n", "\n", "tokens", "=", "[", "self", ".", "tokenizer", ".", "decode", "(", "[", "i", "]", ")", "for", "i", "in", "range", "(", "self", ".", "tokenizer", ".", "vocab_size", ")", "]", "\n", "self", ".", "ind_subtokens", "=", "set", "(", "[", "i", "for", "i", "in", "range", "(", "len", "(", "tokens", ")", ")", "if", "tokens", "[", "i", "]", "[", "0", ":", "2", "]", "==", "'##'", "]", ")", "\n", "self", ".", "len_tokens", "=", "torch", ".", "tensor", "(", "[", "0", "if", "t", "[", "0", "]", "==", "'['", "and", "t", "[", "-", "1", "]", "==", "']'", "\n", "else", "(", "10", "+", "min", "(", "5", ",", "len", "(", "t", ")", "-", "2", ")", "if", "t", "[", "0", ":", "2", "]", "==", "'##'", "else", "min", "(", "10", ",", "len", "(", "t", ")", ")", ")", "\n", "for", "t", "in", "tokens", "]", ",", "dtype", "=", "torch", ".", "int8", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.alibaba_self-augmentation-strategy.utils.sas_utils.SequenceSideInfo.getSenTokIdx": [[76, 102], ["sas_utils.SequenceSideInfo.sen_tokenizer.tokenize", "numpy.array", "numpy.concatenate", "numpy.concatenate", "numpy.concatenate", "sen_lengths[].sum", "numpy.arange", "len", "numpy.ones", "range", "range", "sas_utils.SequenceSideInfo.tokenizer.batch_encode_plus", "len", "len"], "methods", ["None"], ["", "def", "getSenTokIdx", "(", "self", ",", "sentence_position_embedding", ",", "inputs_str", ",", "seq_len_total", ")", ":", "\n", "# CLS_mask = (example == tokenizer.convert_tokens_to_ids(tokenizer.cls_token))", "\n", "# SEP_mask = (example == tokenizer.convert_tokens_to_ids(tokenizer.sep_token))", "\n", "\n", "# Example from CoLA", "\n", "# \"[CLS] john has been captured by the cops and i'm afraid he'll talk. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\"", "\n", "# [[101, 101, 2198, 2038, 2042, 4110, 2011, 1996, 10558, ...], [101, 102, 0, 0, 0, 0, 0, 0, 0, ...]]", "\n", "\n", "# Example from RTE", "\n", "# '[CLS] franz liszt ( hungarian : liszt ferenc ) was a hungarian / austrian virtuoso pianist and composer who lived from 1811 to 1886. liszt is widely considered to be one of the greatest piano virtuosi of all time, and certainly the most famous of the nineteenth century.'", "\n", "# '[SEP] franz liszt lived from 1811 to 1886.'", "\n", "# '[SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]'", "\n", "\n", "        ", "sentences", "=", "self", ".", "sen_tokenizer", ".", "tokenize", "(", "inputs_str", ")", "\n", "sen_lengths", "=", "np", ".", "array", "(", "[", "len", "(", "x", ")", "-", "2", "for", "x", "in", "self", ".", "tokenizer", ".", "batch_encode_plus", "(", "sentences", ")", "[", "'input_ids'", "]", "]", ")", "# -2: to drop the extra [CLS] and [SEP] added by sen_tokenizer", "\n", "\n", "# length_diff = seq_len_total - sen_lengths.sum()", "\n", "# if length_diff != 0: ", "\n", "#     print(length_diff)  # Roughly 1% non-zero cases", "\n", "\n", "sen_lengths", "[", "0", "]", "=", "seq_len_total", "-", "sen_lengths", "[", "1", ":", "]", ".", "sum", "(", ")", "# The first sentence might start with a sub-word like \"##construction\". In this case, we need to adjust its length. Example: tokenizer.decode(tokenizer.encode(\"##construction\")[1:-1]) will return '# # construction' ", "\n", "\n", "idx_sen", "=", "np", ".", "concatenate", "(", "[", "i", "*", "np", ".", "ones", "(", "sen_lengths", "[", "i", "]", ",", "dtype", "=", "np", ".", "int8", ")", "for", "i", "in", "range", "(", "len", "(", "sen_lengths", ")", ")", "]", ")", "\n", "idx_tok", "=", "np", ".", "concatenate", "(", "[", "np", ".", "arange", "(", "sen_lengths", "[", "i", "]", ",", "dtype", "=", "np", ".", "int8", ")", "for", "i", "in", "range", "(", "len", "(", "sen_lengths", ")", ")", "]", ")", "\n", "\n", "return", "np", ".", "concatenate", "(", "(", "idx_sen", ",", "idx_tok", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.alibaba_self-augmentation-strategy.utils.sas_utils.SequenceSideInfo.generate_seq_side_info": [[103, 149], ["isinstance", "hasattr", "dict", "torch.tensor", "sas_utils.SequenceSideInfo.tokenizer.batch_decode", "torch.tensor", "torch.tensor", "numpy.unique", "sas_utils.SequenceSideInfo.ind_subtokens.intersection", "range", "dict.keys", "numpy.array", "numpy.array", "set", "len", "torch.stack().any().char", "torch.zeros", "torch.logical_and", "side_info_dict[].numpy", "len", "torch.tensor.long", "sas_utils.SequenceSideInfo.getSenTokIdx", "sas_utils.SequenceSideInfo.getSenTokIdx", "torch.stack().any", "sas_utils.SequenceSideInfo.tokenizer.decode", "torch.tensor.numpy", "torch.stack"], "methods", ["home.repos.pwc.inspect_result.alibaba_self-augmentation-strategy.utils.sas_utils.SequenceSideInfo.getSenTokIdx", "home.repos.pwc.inspect_result.alibaba_self-augmentation-strategy.utils.sas_utils.SequenceSideInfo.getSenTokIdx"], ["", "def", "generate_seq_side_info", "(", "self", ",", "sentence_position_embedding", ",", "inputs_id", ")", ":", "\n", "        ", "is_np_array", "=", "False", "\n", "if", "isinstance", "(", "inputs_id", "[", "0", "]", ",", "(", "list", ",", "np", ".", "ndarray", ")", ")", ":", "\n", "            ", "is_np_array", "=", "True", "\n", "inputs_id", "=", "torch", ".", "tensor", "(", "inputs_id", ")", "\n", "\n", "", "if", "hasattr", "(", "self", ".", "tokenizer", ",", "'batch_decode'", ")", ":", "\n", "            ", "inputs_str", "=", "self", ".", "tokenizer", ".", "batch_decode", "(", "inputs_id", ")", "\n", "sen_tok_idx", "=", "torch", ".", "tensor", "(", "np", ".", "array", "(", "[", "self", ".", "getSenTokIdx", "(", "sentence_position_embedding", ",", "input_str", ",", "inputs_id", ".", "shape", "[", "1", "]", ")", "\n", "for", "input_str", "in", "inputs_str", "]", ")", ",", "device", "=", "inputs_id", ".", "device", ")", "\n", "", "else", ":", "\n", "            ", "sen_tok_idx", "=", "torch", ".", "tensor", "(", "np", ".", "array", "(", "[", "self", ".", "getSenTokIdx", "(", "sentence_position_embedding", ",", "self", ".", "tokenizer", ".", "decode", "(", "input_ori", ")", ",", "inputs_id", ".", "shape", "[", "1", "]", ")", "\n", "for", "input_ori", "in", "inputs_id", ".", "numpy", "(", ")", "]", ")", ",", "device", "=", "inputs_id", ".", "device", ")", "\n", "\n", "", "side_info_dict", "=", "dict", "(", ")", "\n", "seq_length", "=", "inputs_id", ".", "shape", "[", "1", "]", "\n", "side_info_dict", "[", "'ss_sentence_position_in_sequence'", "]", "=", "sen_tok_idx", "[", ":", ",", "0", ":", "seq_length", "]", "\n", "side_info_dict", "[", "'ss_token_position_in_sentence'", "]", "=", "sen_tok_idx", "[", ":", ",", "1", "*", "seq_length", ":", "2", "*", "seq_length", "]", "\n", "\n", "if", "sentence_position_embedding", ">=", "2", ":", "\n", "# consider sub-word tokens             ", "\n", "            ", "unique", ",", "_", "=", "np", ".", "unique", "(", "inputs_id", ",", "return_inverse", "=", "True", ")", "\n", "ind_subtokens", "=", "self", ".", "ind_subtokens", ".", "intersection", "(", "set", "(", "unique", ")", ")", "\n", "\n", "if", "len", "(", "ind_subtokens", ")", ">", "0", ":", "\n", "                ", "idx_tok_ww", "=", "torch", ".", "stack", "(", "[", "inputs_id", "==", "st", "for", "st", "in", "ind_subtokens", "]", ")", ".", "any", "(", "axis", "=", "0", ")", ".", "char", "(", ")", "\n", "", "else", ":", "\n", "                ", "idx_tok_ww", "=", "torch", ".", "zeros", "(", "inputs_id", ".", "shape", ",", "dtype", "=", "torch", ".", "int8", ")", "\n", "\n", "", "idx_tok_ww", "[", ":", ",", "0", "]", "=", "0", "\n", "idx_tok_ww_1", "=", "idx_tok_ww", "[", ":", ",", "1", ":", "]", "\n", "for", "i", "in", "range", "(", "1", ",", "11", ")", ":", "\n", "                ", "pos", "=", "torch", ".", "logical_and", "(", "idx_tok_ww_1", "==", "i", ",", "idx_tok_ww", "[", ":", ",", "0", ":", "-", "1", "]", "==", "i", ")", "\n", "if", "len", "(", "pos", ")", "==", "0", ":", "\n", "                    ", "break", "\n", "", "idx_tok_ww_1", "[", "pos", "]", "=", "i", "+", "1", "\n", "", "side_info_dict", "[", "'ss_token_position_in_whole_word'", "]", "=", "idx_tok_ww", "\n", "\n", "inputs_str_len", "=", "self", ".", "len_tokens", "[", "inputs_id", ".", "long", "(", ")", "]", "\n", "side_info_dict", "[", "'ss_token_string_length'", "]", "=", "inputs_str_len", "\n", "\n", "", "if", "is_np_array", ":", "\n", "            ", "for", "key", "in", "side_info_dict", ".", "keys", "(", ")", ":", "\n", "                ", "side_info_dict", "[", "key", "]", "=", "side_info_dict", "[", "key", "]", ".", "numpy", "(", ")", "\n", "\n", "", "", "return", "side_info_dict", "\n", "", "", ""]], "home.repos.pwc.inspect_result.alibaba_self-augmentation-strategy.utils.sas_utils.get_random_states": [[7, 17], ["torch.get_rng_state", "numpy.random.get_state", "random.getstate", "torch.cuda.get_rng_state"], "function", ["None"], ["def", "get_random_states", "(", "device", "=", "None", ")", ":", "\n", "    ", "random_states", "=", "{", "}", "\n", "\n", "random_states", "[", "'rng_state_torch'", "]", "=", "torch", ".", "get_rng_state", "(", ")", "\n", "random_states", "[", "'rng_state_np'", "]", "=", "np", ".", "random", ".", "get_state", "(", ")", "\n", "random_states", "[", "'rng_state_rnd'", "]", "=", "random", ".", "getstate", "(", ")", "\n", "if", "device", "is", "not", "None", "and", "device", ".", "type", "==", "'cuda'", ":", "\n", "        ", "random_states", "[", "'rng_state_torch_cuda'", "]", "=", "torch", ".", "cuda", ".", "get_rng_state", "(", "device", ")", "\n", "\n", "", "return", "random_states", "\n", "\n"]], "home.repos.pwc.inspect_result.alibaba_self-augmentation-strategy.utils.sas_utils.set_random_states": [[19, 26], ["torch.set_rng_state", "numpy.random.set_state", "random.setstate", "torch.cuda.set_rng_state"], "function", ["None"], ["", "def", "set_random_states", "(", "random_states", ",", "device", "=", "None", ")", ":", "\n", "\n", "    ", "torch", ".", "set_rng_state", "(", "random_states", "[", "'rng_state_torch'", "]", ")", "\n", "np", ".", "random", ".", "set_state", "(", "random_states", "[", "'rng_state_np'", "]", ")", "\n", "random", ".", "setstate", "(", "random_states", "[", "'rng_state_rnd'", "]", ")", "\n", "if", "device", "is", "not", "None", "and", "device", ".", "type", "==", "'cuda'", ":", "\n", "        ", "torch", ".", "cuda", ".", "set_rng_state", "(", "random_states", "[", "'rng_state_torch_cuda'", "]", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.alibaba_self-augmentation-strategy.utils.sas_utils.check_nan_inf": [[34, 59], ["torch.is_tensor", "torch.isnan().any", "torch.isinf().any", "type", "range", "sum", "torch.isnan", "torch.isinf", "len", "torch.is_tensor", "torch.isnan().any", "torch.isinf().any", "torch.isnan", "torch.isinf"], "function", ["None"], ["", "", "def", "check_nan_inf", "(", "data", ")", ":", "\n", "    ", "if", "data", "is", "None", ":", "\n", "        ", "return", "None", "\n", "\n", "", "result", "=", "[", "0", ",", "0", "]", "\n", "if", "torch", ".", "is_tensor", "(", "data", ")", ":", "\n", "        ", "if", "torch", ".", "isnan", "(", "data", ")", ".", "any", "(", ")", ":", "\n", "            ", "result", "[", "0", "]", "=", "1", "\n", "", "if", "torch", ".", "isinf", "(", "data", ")", ".", "any", "(", ")", ":", "\n", "            ", "result", "[", "1", "]", "=", "1", "\n", "\n", "", "", "elif", "type", "(", "data", ")", "is", "tuple", ":", "\n", "        ", "for", "i", "in", "range", "(", "len", "(", "data", ")", ")", ":", "\n", "            ", "if", "torch", ".", "is_tensor", "(", "data", "[", "i", "]", ")", ":", "\n", "                ", "if", "torch", ".", "isnan", "(", "data", "[", "i", "]", ")", ".", "any", "(", ")", ":", "\n", "                    ", "result", "[", "0", "]", "+=", "1", "\n", "", "if", "torch", ".", "isinf", "(", "data", "[", "i", "]", ")", ".", "any", "(", ")", ":", "\n", "                    ", "result", "[", "1", "]", "+=", "1", "\n", "\n", "", "", "", "if", "result", "[", "0", "]", ">", "0", ":", "\n", "            ", "result", "[", "0", "]", "+=", "10", "\n", "", "if", "result", "[", "1", "]", ">", "0", ":", "\n", "            ", "result", "[", "1", "]", "+=", "10", "\n", "\n", "", "", "return", "result", "if", "sum", "(", "result", ")", ">", "0", "else", "None", "\n", "\n"]], "home.repos.pwc.inspect_result.alibaba_self-augmentation-strategy.utils.data_collator_sas.SASValidationDataset.__init__": [[21, 42], ["numpy.load", "numpy.random.shuffle", "data_collator_sas.SASValidationDataset.tokenizer.convert_tokens_to_ids", "data_collator_sas.SASValidationDataset.tokenizer.convert_tokens_to_ids", "data_collator_sas.SASValidationDataset.examples[].copy", "numpy.roll"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "file_path", ",", "dataset_size", "=", "100000000", ",", "tokenizer", "=", "None", ")", ":", "\n", "\n", "        ", "self", ".", "examples", "=", "np", ".", "load", "(", "file_path", ")", "\n", "np", ".", "random", ".", "shuffle", "(", "self", ".", "examples", ")", "\n", "if", "dataset_size", "<=", "self", ".", "examples", ".", "shape", "[", "0", "]", ":", "\n", "            ", "self", ".", "examples", "=", "self", ".", "examples", "[", ":", "dataset_size", "]", ".", "copy", "(", ")", "# Scling creates a view. Use copy to release the memory of the original big nparray (See https://github.com/numpy/numpy/issues/15746)", "\n", "\n", "", "self", ".", "tokenizer", "=", "tokenizer", "\n", "\n", "self", ".", "cls_index", "=", "self", ".", "tokenizer", ".", "convert_tokens_to_ids", "(", "self", ".", "tokenizer", ".", "cls_token", ")", "\n", "self", ".", "sep_index", "=", "self", ".", "tokenizer", ".", "convert_tokens_to_ids", "(", "self", ".", "tokenizer", ".", "sep_token", ")", "\n", "\n", "# The current dataset doesn't include CLS at all. It is merely sequence of 128 tokens. Replace the frist one as [CLS]", "\n", "if", "self", ".", "examples", "[", "0", ",", "0", "]", "!=", "self", ".", "cls_index", ":", "\n", "            ", "self", ".", "examples", "=", "np", ".", "roll", "(", "self", ".", "examples", ",", "shift", "=", "1", ",", "axis", "=", "1", ")", "\n", "self", ".", "examples", "[", ":", ",", "0", "]", "=", "self", ".", "cls_index", "\n", "\n", "", "self", ".", "vocab_size", "=", "self", ".", "tokenizer", ".", "vocab_size", "\n", "self", ".", "vocab_dtype", "=", "(", "torch", ".", "short", "if", "self", ".", "vocab_size", "<=", "32768", "else", "torch", ".", "long", ")", "\n", "\n", "self", ".", "dataset_size", "=", "self", ".", "examples", ".", "shape", "[", "0", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.alibaba_self-augmentation-strategy.utils.data_collator_sas.SASValidationDataset.__len__": [[43, 45], ["len"], "methods", ["None"], ["", "def", "__len__", "(", "self", ")", ":", "\n", "        ", "return", "len", "(", "self", ".", "examples", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.alibaba_self-augmentation-strategy.utils.data_collator_sas.SASValidationDataset.__getitem__": [[46, 48], ["None"], "methods", ["None"], ["", "def", "__getitem__", "(", "self", ",", "i", ")", "->", "torch", ".", "Tensor", ":", "\n", "        ", "return", "{", "'idx'", ":", "i", ",", "'inputids'", ":", "self", ".", "examples", "[", "i", "]", "}", "\n", "\n"]], "home.repos.pwc.inspect_result.alibaba_self-augmentation-strategy.utils.data_collator_sas.SASTrainDataset.__init__": [[57, 84], ["numpy.random.seed", "numpy.load", "numpy.random.shuffle", "data_collator_sas.SASTrainDataset.tokenizer.convert_tokens_to_ids", "data_collator_sas.SASTrainDataset.tokenizer.convert_tokens_to_ids", "data_collator_sas.SASTrainDataset.examples[].copy", "numpy.roll"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "file_path", ",", "dataset_size", "=", "100000000", ",", "tokenizer", "=", "None", ",", "seed", "=", "None", ")", ":", "\n", "\n", "        ", "np", ".", "random", ".", "seed", "(", "seed", ")", "\n", "self", ".", "examples", "=", "np", ".", "load", "(", "file_path", ")", "\n", "np", ".", "random", ".", "shuffle", "(", "self", ".", "examples", ")", "\n", "\n", "if", "dataset_size", "<=", "self", ".", "examples", ".", "shape", "[", "0", "]", ":", "\n", "            ", "self", ".", "examples", "=", "self", ".", "examples", "[", ":", "dataset_size", "]", ".", "copy", "(", ")", "# Scling creates a view. Use copy to release the memory of the original big nparray (See https://github.com/numpy/numpy/issues/15746)", "\n", "\n", "", "self", ".", "dataset_size", "=", "self", ".", "examples", ".", "shape", "[", "0", "]", "\n", "\n", "self", ".", "tokenizer", "=", "tokenizer", "\n", "\n", "self", ".", "cls_index", "=", "self", ".", "tokenizer", ".", "convert_tokens_to_ids", "(", "self", ".", "tokenizer", ".", "cls_token", ")", "\n", "self", ".", "sep_index", "=", "self", ".", "tokenizer", ".", "convert_tokens_to_ids", "(", "self", ".", "tokenizer", ".", "sep_token", ")", "\n", "# The current dataset doesn't include CLS at all. It is merely sequence of 128 tokens. Replace the frist one as [CLS]", "\n", "if", "self", ".", "examples", "[", "0", ",", "0", "]", "!=", "self", ".", "cls_index", ":", "\n", "            ", "self", ".", "examples", "=", "np", ".", "roll", "(", "self", ".", "examples", ",", "shift", "=", "1", ",", "axis", "=", "1", ")", "\n", "self", ".", "examples", "[", ":", ",", "0", "]", "=", "self", ".", "cls_index", "\n", "\n", "# Used vocab_dtype to reduce memory by using short int in case of small vocab dataset_size", "\n", "", "self", ".", "vocab_size", "=", "self", ".", "tokenizer", ".", "vocab_size", "\n", "self", ".", "vocab_dtype", "=", "(", "torch", ".", "short", "if", "self", ".", "vocab_size", "<=", "32768", "else", "torch", ".", "long", ")", "\n", "\n", "# Used to store data augmentations", "\n", "self", ".", "buffer_da", "=", "None", "\n", "self", ".", "buffer_da_flag", "=", "None", "\n", "\n"]], "home.repos.pwc.inspect_result.alibaba_self-augmentation-strategy.utils.data_collator_sas.SASTrainDataset.__len__": [[85, 87], ["len"], "methods", ["None"], ["", "def", "__len__", "(", "self", ")", ":", "\n", "        ", "return", "len", "(", "self", ".", "examples", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.alibaba_self-augmentation-strategy.utils.data_collator_sas.SASTrainDataset.update_buffer_da": [[88, 99], ["torch.roll"], "methods", ["None"], ["", "def", "update_buffer_da", "(", "self", ",", "idx", ",", "flag_freeze_generation", "=", "False", ")", ":", "\n", "        ", "'''\n        Pop the first element in the buffer. \n        Typically used in the beginning of the forward pass which need fake_tokens. \n        It cannot be used in data_collator! Data_collator have multi-threads and cannot modify variable in dataset.\n        '''", "\n", "# shift dimension -2 (augmentations per instance, which is used when freezing generator) by 1, in order to use the different augmentations in different epochs.", "\n", "if", "self", ".", "buffer_da", "is", "not", "None", "and", "self", ".", "buffer_da", ".", "shape", "[", "-", "2", "]", ">", "1", ":", "\n", "            ", "tmp", "=", "torch", ".", "roll", "(", "self", ".", "buffer_da", "[", "idx", ",", "...", "]", ",", "1", ",", "-", "2", ")", "\n", "self", ".", "buffer_da", "[", "idx", ",", "...", "]", "=", "tmp", "\n", "del", "tmp", "# Jingqiao's note: Shanzhu tested that the roll function doesn't release memory right away in cpu, althoug it does in gpu", "\n", "\n"]], "home.repos.pwc.inspect_result.alibaba_self-augmentation-strategy.utils.data_collator_sas.SASTrainDataset.__getitem__": [[100, 113], ["da_flag.sum"], "methods", ["None"], ["", "", "def", "__getitem__", "(", "self", ",", "i", ")", ":", "\n", "# Read-only. Used by data_collator.", "\n", "\n", "        ", "if", "self", ".", "buffer_da", "is", "None", ":", "\n", "            ", "return", "{", "'idx'", ":", "i", ",", "'input_ids'", ":", "self", ".", "examples", "[", "i", "]", ",", "'data_augmentation'", ":", "None", "}", "\n", "", "else", ":", "\n", "# -2 dimension: return one augmentation Per Instance: Only reture one (always return the last one after rolling augmentations each time)", "\n", "# -1 dimension: return all augmentations Per Mixing (to be mixed in data collator): Always return all valid ones", "\n", "            ", "da_flag", "=", "self", ".", "buffer_da_flag", "[", "i", ",", ":", "]", "\n", "if", "da_flag", ".", "sum", "(", ")", "==", "0", ":", "# DA is not prepared yet", "\n", "                ", "return", "{", "'idx'", ":", "i", ",", "'input_ids'", ":", "self", ".", "examples", "[", "i", "]", ",", "'data_augmentation'", ":", "None", "}", "\n", "", "else", ":", "\n", "                ", "return", "{", "'idx'", ":", "i", ",", "'input_ids'", ":", "self", ".", "examples", "[", "i", "]", ",", "'data_augmentation'", ":", "self", ".", "buffer_da", "[", "i", ",", ":", ",", "-", "1", ",", "da_flag", "]", "}", "\n", "\n"]], "home.repos.pwc.inspect_result.alibaba_self-augmentation-strategy.utils.data_collator_sas.SASTrainDataset.save_buffer_da": [[114, 140], ["buf.type().cpu", "torch.zeros", "torch.ones", "torch.ones", "buf.type"], "methods", ["None"], ["", "", "", "def", "save_buffer_da", "(", "self", ",", "idx", ",", "buf", ",", "aug_positions", "=", "None", ",", "augPerMixing", "=", "1", ")", ":", "\n", "        ", "'''\n        Save fake tokens into the buffer. \n        Typically used in the forward pass after calling `update_buffer_da` to clean the buffer. \n        It cannot be used in data_collator! Data_collator have multi-threads and cannot modify variable in dataset.\n        '''", "\n", "\n", "dtype", "=", "self", ".", "vocab_dtype", "\n", "# Create empty buffer all in once, instead of incrementally increase its dataset_size after each mini-batch. ", "\n", "# This is to avoid fragmented memory allocation and test possible memory shortage at the earliest time.", "\n", "if", "self", ".", "buffer_da", "is", "None", ":", "\n", "# shape: dataset_size x seq_len x count_augPerInstance x count_augPerMixing", "\n", "            ", "count_augPerInstance", "=", "buf", ".", "shape", "[", "1", "]", "# Number of augmentations per instance (used when freezing the generator, and using one augmentation per subsequent epoch)", "\n", "count_augPerMixing", "=", "augPerMixing", "# Number of historical distributions to be mixed together. E.g., we might want to mix the distributions predicted in recent two epochs for each instance", "\n", "self", ".", "buffer_da", "=", "-", "100", "*", "torch", ".", "ones", "(", "[", "self", ".", "dataset_size", ",", "aug_positions", ".", "shape", "[", "1", "]", ",", "count_augPerInstance", ",", "count_augPerMixing", "]", ",", "dtype", "=", "dtype", ")", "# Use sys.getsizeof(self.buffer_da.storage()) to measure its memory usage. In case of nparray, use self.buffer_da.__i", "\n", "self", ".", "buffer_da_flag", "=", "torch", ".", "zeros", "(", "self", ".", "dataset_size", ",", "count_augPerMixing", ",", "dtype", "=", "torch", ".", "bool", ")", "\n", "\n", "# Shape = batch_size x seq_len x count_augPerInstance", "\n", "", "token_to_save", "=", "-", "100", "*", "torch", ".", "ones", "(", "(", "idx", ".", "shape", "[", "0", "]", ",", "aug_positions", ".", "shape", "[", "1", "]", ",", "buf", ".", "shape", "[", "1", "]", ")", ",", "dtype", "=", "dtype", ")", "\n", "token_to_save", "[", "aug_positions", "]", "=", "buf", ".", "type", "(", "dtype", ")", ".", "cpu", "(", ")", "# Convert to cpu after changing its type from long to short (save time to convert from gpu to cpu)", "\n", "if", "self", ".", "buffer_da", ".", "shape", "[", "-", "1", "]", ">=", "2", ":", "\n", "# In case of augPerMixing>=2 (i.e., abs(opt.augMixing)=1), it means to mix teacher's outputs in previous epochs. Shift to remove old ones before adding new one to the end", "\n", "            ", "self", ".", "buffer_da", "[", "idx", ",", "...", ",", "0", ":", "-", "1", "]", "=", "self", ".", "buffer_da", "[", "idx", ",", "...", ",", "1", ":", "]", "# Shift old distributions to the left", "\n", "self", ".", "buffer_da_flag", "[", "idx", ",", "0", ":", "-", "1", "]", "=", "self", ".", "buffer_da_flag", "[", "idx", ",", "1", ":", "]", "\n", "", "self", ".", "buffer_da", "[", "idx", ",", "...", ",", "-", "1", "]", "=", "token_to_save", "# Now, add the new one to the end of the dimension", "\n", "self", ".", "buffer_da_flag", "[", "idx", ",", "-", "1", "]", "=", "True", "\n", "\n"]], "home.repos.pwc.inspect_result.alibaba_self-augmentation-strategy.utils.data_collator_sas.DataCollatorForSasPretraining.__init__": [[174, 212], ["data_collator_sas.DataCollatorForSasPretraining.tokenizer.convert_tokens_to_ids", "data_collator_sas.DataCollatorForSasPretraining.tokenizer.convert_tokens_to_ids", "torch.Tensor", "numpy.power", "torch.Tensor", "sas_utils.SequenceSideInfo", "ValueError", "os.path.exists", "numpy.load", "empirical_distribution.sum", "numpy.power.sum"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "tokenizer", ",", "empirical_distribution_file", "=", "None", ",", "\n", "cold_start_augumentation_method", "=", "'unigram'", ",", "\n", "mlm_probability", "=", "0.15", ",", "\n", "whole_word_masking", "=", "0", ",", "\n", "dynamic_masking", "=", "0", ",", "\n", "dataset_sizes", "=", "None", ")", ":", "\n", "\n", "        ", "self", ".", "tokenizer", "=", "tokenizer", "\n", "self", ".", "cls_index", "=", "self", ".", "tokenizer", ".", "convert_tokens_to_ids", "(", "self", ".", "tokenizer", ".", "cls_token", ")", "\n", "self", ".", "sep_index", "=", "self", ".", "tokenizer", ".", "convert_tokens_to_ids", "(", "self", ".", "tokenizer", ".", "sep_token", ")", "\n", "\n", "self", ".", "cold_start_augumentation_method", "=", "cold_start_augumentation_method", "\n", "self", ".", "mlm_probability", "=", "mlm_probability", "\n", "self", ".", "whole_word_masking", "=", "whole_word_masking", "\n", "self", ".", "dynamic_masking", "=", "dynamic_masking", "\n", "self", ".", "dataset_sizes", "=", "dataset_sizes", "\n", "\n", "# We found that in first 1000 step, unigram distribution is too hard for discriminator. We modify its temperature to do warmup. ", "\n", "if", "self", ".", "cold_start_augumentation_method", "==", "\"unigram\"", ":", "\n", "            ", "empirical_distribution", "=", "np", ".", "load", "(", "empirical_distribution_file", ")", "if", "os", ".", "path", ".", "exists", "(", "empirical_distribution_file", ")", "else", "None", "\n", "self", ".", "empirical_distribution", "=", "torch", ".", "Tensor", "(", "empirical_distribution", "/", "empirical_distribution", ".", "sum", "(", ")", ")", "\n", "\n", "empirical_distribution_warmup", "=", "np", ".", "power", "(", "empirical_distribution", ",", "1", "/", "2", ")", "# avoid divide zero warning", "\n", "self", ".", "empirical_distribution_warmup", "=", "torch", ".", "Tensor", "(", "empirical_distribution_warmup", "/", "empirical_distribution_warmup", ".", "sum", "(", ")", ")", "\n", "self", ".", "unigram_warmup_step", "=", "1000", "# Note: it is initialized to 1000 at the beginning of every single epoch. Fix it by setting it to 0 after 1st epoch. See related fix in trainer_advanced.py", "\n", "\n", "", "else", ":", "\n", "            ", "self", ".", "unigram_warmup_step", "=", "0", "\n", "\n", "", "self", ".", "training", "=", "None", "\n", "self", ".", "epoch_train", "=", "-", "1", "\n", "self", ".", "epoch_eval", "=", "-", "1", "\n", "\n", "if", "self", ".", "whole_word_masking", ":", "\n", "            ", "self", ".", "sequence_side_info", "=", "SequenceSideInfo", "(", ")", "\n", "\n", "", "if", "self", ".", "tokenizer", ".", "mask_token", "is", "None", ":", "\n", "            ", "raise", "ValueError", "(", "\"This tokenizer does not have a mask token which is necessary for masked language modeling.\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.alibaba_self-augmentation-strategy.utils.data_collator_sas.DataCollatorForSasPretraining.create_masked_lm_predictions": [[214, 254], ["tokens.numpy.numpy.numpy", "enumerate", "numpy.random.shuffle", "max", "torch.zeros", "range", "int", "list", "range", "cand_indexes[].append", "cand_indexes.append", "round", "len", "len", "len", "len", "len"], "methods", ["None"], ["", "", "def", "create_masked_lm_predictions", "(", "self", ",", "tokens", ",", "mlm_probability", ",", "whole_word_masking", "=", "False", ",", "aug_per_step", "=", "1", ")", ":", "\n", "        ", "\"\"\"Creates the predictions for the masked LM objective.\"\"\"", "\n", "tokens_length", "=", "tokens", ".", "shape", "[", "0", "]", "# remember the length of the original sequence with end padding", "\n", "tokens", "=", "tokens", "[", "tokens", "!=", "self", ".", "tokenizer", ".", "pad_token_id", "]", "# added to omit the ending padding tokens", "\n", "\n", "tokens", "=", "tokens", ".", "numpy", "(", ")", "\n", "cand_indexes", "=", "[", "]", "\n", "for", "(", "i", ",", "token", ")", "in", "enumerate", "(", "tokens", ")", ":", "\n", "            ", "if", "token", "in", "[", "self", ".", "cls_index", ",", "self", ".", "sep_index", "]", ":", "\n", "                ", "continue", "\n", "# Whenever we see the ## token, we append it to the previous set of word indexes.", "\n", "", "if", "(", "whole_word_masking", "and", "len", "(", "cand_indexes", ")", ">=", "1", "and", "token", "in", "self", ".", "sequence_side_info", ".", "ind_subtokens", ")", ":", "\n", "                ", "cand_indexes", "[", "-", "1", "]", ".", "append", "(", "i", ")", "\n", "", "else", ":", "\n", "                ", "cand_indexes", ".", "append", "(", "[", "i", "]", ")", "\n", "\n", "", "", "np", ".", "random", ".", "shuffle", "(", "cand_indexes", ")", "\n", "\n", "num_to_mask", "=", "max", "(", "1", ",", "int", "(", "round", "(", "len", "(", "tokens", ")", "*", "mlm_probability", ")", ")", ")", "\n", "masked_indices", "=", "torch", ".", "zeros", "(", "(", "aug_per_step", ",", "tokens_length", ")", ",", "dtype", "=", "torch", ".", "bool", ")", "\n", "\n", "set_checked", "=", "0", "\n", "for", "aug_count", "in", "range", "(", "aug_per_step", ")", ":", "\n", "            ", "num_masked", "=", "0", "\n", "covered_indexes", "=", "list", "(", ")", "\n", "for", "i", "in", "range", "(", "set_checked", ",", "len", "(", "cand_indexes", ")", ")", ":", "\n", "                ", "index_set", "=", "cand_indexes", "[", "i", "]", "\n", "\n", "if", "num_masked", ">=", "num_to_mask", ":", "\n", "                    ", "break", "\n", "", "if", "num_masked", "+", "len", "(", "index_set", ")", ">", "num_to_mask", ":", "\n", "                    ", "continue", "\n", "\n", "", "covered_indexes", "+=", "index_set", "\n", "num_masked", "+=", "len", "(", "index_set", ")", "\n", "\n", "", "set_checked", "=", "i", "\n", "masked_indices", "[", "aug_count", ",", "covered_indexes", "]", "=", "True", "\n", "\n", "", "return", "masked_indices", "\n", "\n"]], "home.repos.pwc.inspect_result.alibaba_self-augmentation-strategy.utils.data_collator_sas.DataCollatorForSasPretraining.generate_masked_indices": [[255, 276], ["torch.stack().view", "torch.full", "torch.full.masked_fill_", "torch.bernoulli().bool", "data_collator_sas.DataCollatorForSasPretraining.tokenizer.get_special_tokens_mask", "inputs.eq", "torch.full.masked_fill_", "torch.tensor", "torch.stack", "inputs.tolist", "torch.bernoulli", "data_collator_sas.DataCollatorForSasPretraining.create_masked_lm_predictions", "range"], "methods", ["home.repos.pwc.inspect_result.alibaba_self-augmentation-strategy.utils.data_collator_sas.DataCollatorForSasPretraining.create_masked_lm_predictions"], ["", "def", "generate_masked_indices", "(", "self", ",", "inputs", ",", "inputs_ori", ")", ":", "\n", "# Sample RTD / Masked position. By default, mask and fake token are the same.", "\n", "\n", "        ", "if", "self", ".", "whole_word_masking", "==", "1", ":", "\n", "            ", "masked_indices", "=", "torch", ".", "stack", "(", "[", "self", ".", "create_masked_lm_predictions", "(", "\n", "inputs_ori", "[", "r", ",", ":", "]", ",", "\n", "mlm_probability", "=", "self", ".", "mlm_probability", ",", "\n", "whole_word_masking", "=", "self", ".", "whole_word_masking", ",", "\n", ")", "for", "r", "in", "range", "(", "inputs_ori", ".", "shape", "[", "0", "]", ")", "]", ")", ".", "view", "(", "-", "1", ",", "inputs_ori", ".", "shape", "[", "1", "]", ")", "\n", "\n", "", "else", ":", "\n", "            ", "probability_matrix", "=", "torch", ".", "full", "(", "inputs", ".", "shape", ",", "self", ".", "mlm_probability", ")", "\n", "special_tokens_mask", "=", "[", "self", ".", "tokenizer", ".", "get_special_tokens_mask", "(", "val", ",", "already_has_special_tokens", "=", "True", ")", "for", "val", "in", "inputs", ".", "tolist", "(", ")", "]", "\n", "if", "self", ".", "tokenizer", ".", "_pad_token", "is", "not", "None", ":", "\n", "                ", "padding_mask", "=", "inputs", ".", "eq", "(", "self", ".", "tokenizer", ".", "pad_token_id", ")", "\n", "probability_matrix", ".", "masked_fill_", "(", "padding_mask", ",", "value", "=", "0.0", ")", "\n", "", "probability_matrix", ".", "masked_fill_", "(", "torch", ".", "tensor", "(", "special_tokens_mask", ",", "dtype", "=", "torch", ".", "bool", ")", ",", "value", "=", "0.0", ")", "\n", "\n", "masked_indices", "=", "torch", ".", "bernoulli", "(", "probability_matrix", ")", ".", "bool", "(", ")", "\n", "\n", "", "return", "masked_indices", "\n", "\n"]], "home.repos.pwc.inspect_result.alibaba_self-augmentation-strategy.utils.data_collator_sas.DataCollatorForSasPretraining.__call__": [[277, 354], ["torch.nn.utils.rnn.pad_sequence().type", "torch.where.clone", "torch.LongTensor", "data_collator_sas.DataCollatorForSasPretraining.generate_masked_indices", "int", "torch.where.clone.clone", "type", "torch.tensor", "numpy.array().sum", "data_collator_sas.DataCollatorForSasPretraining.sum", "data_collator_sas.DataCollatorForSasPretraining.tokenizer.convert_tokens_to_ids", "torch.randint", "torch.logical_and", "torch.where.long", "inputs.clone.clone.long", "torch.logical_and.long", "torch.nn.utils.rnn.pad_sequence", "torch.stack", "torch.logical_and", "torch.where.clone.clone", "torch.where", "torch.bernoulli().bool", "len", "torch.where.long", "inputs.clone.clone.long", "data_collator_sas.DataCollatorForSasPretraining.tokenizer.convert_tokens_to_ids", "torch.LongTensor.long", "numpy.array", "torch.where.long", "inputs.clone.clone.long", "torch.logical_and.long", "len", "torch.bernoulli().bool", "torch.LongTensor.long", "torch.multinomial().type", "torch.tensor", "torch.LongTensor.long", "torch.bernoulli", "torch.randint", "torch.full", "torch.bernoulli", "torch.multinomial", "len", "torch.full"], "methods", ["home.repos.pwc.inspect_result.alibaba_self-augmentation-strategy.utils.data_collator_sas.DataCollatorForSasPretraining.generate_masked_indices"], ["", "def", "__call__", "(", "self", ",", "examples", ")", "->", "Dict", "[", "str", ",", "torch", ".", "Tensor", "]", ":", "\n", "        ", "assert", "type", "(", "examples", "[", "0", "]", ")", "is", "dict", "\n", "self", ".", "training", "=", "(", "True", "if", "'data_augmentation'", "in", "examples", "[", "0", "]", "else", "False", ")", "\n", "\n", "vocab_dtype", "=", "torch", ".", "tensor", "(", "examples", "[", "0", "]", "[", "'input_ids'", "]", ")", ".", "dtype", "\n", "inputs", "=", "torch", ".", "nn", ".", "utils", ".", "rnn", ".", "pad_sequence", "(", "[", "torch", ".", "tensor", "(", "a", "[", "'input_ids'", "]", ")", "for", "a", "in", "examples", "]", ",", "batch_first", "=", "True", ",", "padding_value", "=", "self", ".", "tokenizer", ".", "pad_token_id", ")", ".", "type", "(", "vocab_dtype", ")", "\n", "inputs_ori", "=", "inputs", ".", "clone", "(", ")", "\n", "idx", "=", "torch", ".", "LongTensor", "(", "[", "a", "[", "'idx'", "]", "for", "a", "in", "examples", "]", ")", "# We use it as index - pytorch required dtype=long when using an integer as index", "\n", "\n", "dynamic_masked_indices", "=", "None", ";", "dynamic_masked_labels", "=", "None", "\n", "if", "self", ".", "training", ":", "# Include self_trained data augmentation", "\n", "\n", "# For \"incremental\" setting, the examples will return tuple-3 and the 3rd one is the data augmentation. ", "\n", "# For the first epoch, tuple[1] is None. ", "\n", "\n", "            ", "none_count", "=", "np", ".", "array", "(", "[", "a", "[", "'data_augmentation'", "]", "is", "None", "for", "a", "in", "examples", "]", ")", ".", "sum", "(", ")", "\n", "if", "none_count", "==", "0", ":", "# This means buffer is filled with data augmentation from previous epochs. Use it!", "\n", "\n", "# If second element in tuple is None, it means the first epoch will not do incremental RTD but original.", "\n", "                ", "buffer_da", "=", "torch", ".", "stack", "(", "[", "a", "[", "'data_augmentation'", "]", "for", "a", "in", "examples", "]", ",", "dim", "=", "0", ")", "\n", "rtd_token", "=", "buffer_da", "[", "...", ",", "-", "1", "]", "\n", "\n", "# To enforce rtd_labels is correct, because fake token could happen to the original token (~1% in case of unigram distribution)", "\n", "rtd_labels", "=", "torch", ".", "logical_and", "(", "rtd_token", ">=", "0", ",", "rtd_token", "!=", "inputs_ori", ")", "\n", "\n", "# mlm label: incldue the original tokens at the positions to do mlm prediction", "\n", "mlm_labels", "=", "inputs_ori", ".", "clone", "(", ")", "\n", "mlm_labels", "[", "~", "rtd_labels", "]", "=", "-", "100", "\n", "\n", "# Training data inputs after applying data augmentation", "\n", "inputs", "=", "torch", ".", "where", "(", "rtd_labels", ",", "rtd_token", ",", "inputs_ori", ")", "\n", "\n", "return", "{", "\"input_ids\"", ":", "inputs", ".", "long", "(", ")", ",", "\"mlm_labels\"", ":", "mlm_labels", ".", "long", "(", ")", ",", "\"rtd_labels\"", ":", "rtd_labels", ".", "long", "(", ")", ",", "\n", "\"side_info_sets\"", ":", "{", "\"batch_idx\"", ":", "idx", ".", "long", "(", ")", ",", "\"dataset_sizes\"", ":", "self", ".", "dataset_sizes", ",", "\"dynamic_masked_indices\"", ":", "dynamic_masked_indices", ",", "\"dynamic_masked_labels\"", ":", "dynamic_masked_labels", "}", "}", "\n", "\n", "", "else", ":", "\n", "                ", "assert", "none_count", "==", "len", "(", "examples", ")", ",", "(", "\"! none_count error found. %d (%d~%d)\"", "%", "(", "none_count", ",", "examples", "[", "0", "]", "[", "'idx'", "]", ",", "examples", "[", "-", "1", "]", "[", "'idx'", "]", ")", ")", "\n", "\n", "", "", "masked_indices", "=", "self", ".", "generate_masked_indices", "(", "inputs", ",", "inputs_ori", ")", "\n", "\n", "n_sample", "=", "int", "(", "masked_indices", ".", "sum", "(", ")", ")", "\n", "mlm_labels", "=", "inputs_ori", ".", "clone", "(", ")", "\n", "mlm_labels", "[", "~", "masked_indices", "]", "=", "-", "100", "\n", "\n", "if", "self", ".", "cold_start_augumentation_method", "==", "\"mlm\"", ":", "\n", "# Original BERT's MLM input : inputs and mlm_labels", "\n", "\n", "# 80% of the time, we replace masked input tokens with tokenizer.mask_token ([MASK])", "\n", "            ", "indices_replaced", "=", "torch", ".", "bernoulli", "(", "torch", ".", "full", "(", "mlm_labels", ".", "shape", ",", "0.8", ")", ")", ".", "bool", "(", ")", "&", "masked_indices", "\n", "inputs", "[", "indices_replaced", "]", "=", "self", ".", "tokenizer", ".", "convert_tokens_to_ids", "(", "self", ".", "tokenizer", ".", "mask_token", ")", "\n", "\n", "# 10% of the time, we replace masked input tokens with random word", "\n", "indices_random", "=", "torch", ".", "bernoulli", "(", "torch", ".", "full", "(", "mlm_labels", ".", "shape", ",", "0.5", ")", ")", ".", "bool", "(", ")", "&", "masked_indices", "&", "~", "indices_replaced", "\n", "random_words", "=", "torch", ".", "randint", "(", "len", "(", "self", ".", "tokenizer", ")", ",", "mlm_labels", ".", "shape", ",", "dtype", "=", "vocab_dtype", ")", "\n", "inputs", "[", "indices_random", "]", "=", "random_words", "[", "indices_random", "]", "\n", "\n", "return", "{", "\"input_ids\"", ":", "inputs", ".", "long", "(", ")", ",", "\"mlm_labels\"", ":", "mlm_labels", ".", "long", "(", ")", ",", "\n", "\"side_info_sets\"", ":", "{", "\"batch_idx\"", ":", "idx", ".", "long", "(", ")", ",", "\"dataset_sizes\"", ":", "self", ".", "dataset_sizes", "}", "}", "\n", "\n", "", "elif", "self", ".", "cold_start_augumentation_method", "==", "\"mask\"", ":", "\n", "            ", "inputs", "[", "masked_indices", "]", "=", "self", ".", "tokenizer", ".", "convert_tokens_to_ids", "(", "self", ".", "tokenizer", ".", "mask_token", ")", "\n", "\n", "", "elif", "self", ".", "cold_start_augumentation_method", "==", "\"unigram\"", ":", "\n", "            ", "if", "self", ".", "unigram_warmup_step", ">", "0", ":", "\n", "                ", "self", ".", "unigram_warmup_step", "-=", "1", "\n", "dis_to_sample", "=", "self", ".", "empirical_distribution_warmup", "\n", "", "else", ":", "\n", "                ", "dis_to_sample", "=", "self", ".", "empirical_distribution", "\n", "", "inputs", "[", "masked_indices", "]", "=", "torch", ".", "multinomial", "(", "dis_to_sample", ",", "num_samples", "=", "n_sample", ",", "replacement", "=", "True", ")", ".", "type", "(", "vocab_dtype", ")", "\n", "\n", "", "elif", "self", ".", "cold_start_augumentation_method", "==", "\"random\"", ":", "\n", "            ", "inputs", "[", "masked_indices", "]", "=", "torch", ".", "randint", "(", "len", "(", "self", ".", "tokenizer", ")", ",", "(", "n_sample", ",", ")", ",", "dtype", "=", "vocab_dtype", ")", "\n", "\n", "", "rtd_labels", "=", "torch", ".", "logical_and", "(", "masked_indices", ",", "inputs", "!=", "mlm_labels", ")", "*", "1", "\n", "\n", "return", "{", "\"input_ids\"", ":", "inputs", ".", "long", "(", ")", ",", "\"mlm_labels\"", ":", "mlm_labels", ".", "long", "(", ")", ",", "\"rtd_labels\"", ":", "rtd_labels", ".", "long", "(", ")", ",", "\n", "\"side_info_sets\"", ":", "{", "\"batch_idx\"", ":", "idx", ".", "long", "(", ")", ",", "\"dataset_sizes\"", ":", "self", ".", "dataset_sizes", ",", "\"dynamic_masked_indices\"", ":", "dynamic_masked_indices", ",", "\"dynamic_masked_labels\"", ":", "dynamic_masked_labels", "}", "}", "\n", "\n"]], "home.repos.pwc.inspect_result.alibaba_self-augmentation-strategy.utils.data_collator_sas.DataCollatorForSwitchCaseAugmentation.__init__": [[356, 361], ["None"], "methods", ["None"], ["    ", "def", "__init__", "(", "self", ",", "switch_case_prob", ",", "tokenizer", ",", "padding", ",", "max_length", ")", "->", "None", ":", "\n", "        ", "self", ".", "switch_case_prob", "=", "switch_case_prob", "\n", "self", ".", "tokenizer", "=", "tokenizer", "\n", "self", ".", "padding", "=", "padding", "\n", "self", ".", "max_length", "=", "max_length", "\n", "\n"]], "home.repos.pwc.inspect_result.alibaba_self-augmentation-strategy.utils.data_collator_sas.DataCollatorForSwitchCaseAugmentation.__call__": [[362, 400], ["len", "range", "data_collator_sas.DataCollatorForSwitchCaseAugmentation.tokenizer", "data_collator_sas.DataCollatorForSwitchCaseAugmentation.tokenizer", "range", "transformers.default_data_collator", "cased_examples.append", "features[].keys", "range", "len", "data_collator_sas.DataCollatorForSwitchCaseAugmentation.keys", "range", "numpy.random.rand", "text_augmentation.randomly_switch_case_str", "cased_examples_2.append", "features[].keys", "features[].keys", "range", "features[].keys", "range", "numpy.random.rand", "text_augmentation.randomly_switch_case_str", "len"], "methods", ["None"], ["", "def", "__call__", "(", "self", ",", "features", ")", ":", "\n", "        ", "total", "=", "len", "(", "features", ")", "\n", "cased_examples_org", "=", "[", "features", "[", "i", "]", "[", "'input_sent1'", "]", "for", "i", "in", "range", "(", "total", ")", "]", "\n", "cased_examples", "=", "[", "]", "\n", "for", "idx", "in", "range", "(", "total", ")", ":", "\n", "            ", "tmp", "=", "features", "[", "idx", "]", "[", "'input_sent1'", "]", "\n", "if", "np", ".", "random", ".", "rand", "(", "1", ")", ">", "0.5", ":", "\n", "                ", "tmp", "=", "randomly_switch_case_str", "(", "features", "[", "idx", "]", "[", "'input_sent1'", "]", ",", "switch_case_probability", "=", "self", ".", "switch_case_prob", ")", "\n", "", "cased_examples", ".", "append", "(", "tmp", ")", "\n", "\n", "", "if", "'input_sent2'", "in", "features", "[", "0", "]", ".", "keys", "(", ")", ":", "\n", "            ", "cased_examples_2_org", "=", "[", "features", "[", "i", "]", "[", "'input_sent2'", "]", "for", "i", "in", "range", "(", "total", ")", "]", "\n", "cased_examples_2", "=", "[", "]", "\n", "for", "idx", "in", "range", "(", "total", ")", ":", "\n", "                ", "tmp", "=", "features", "[", "idx", "]", "[", "'input_sent2'", "]", "\n", "if", "np", ".", "random", ".", "rand", "(", "1", ")", ">", "0.5", ":", "\n", "                    ", "tmp", "=", "randomly_switch_case_str", "(", "features", "[", "idx", "]", "[", "'input_sent2'", "]", ",", "switch_case_probability", "=", "self", ".", "switch_case_prob", ")", "\n", "", "cased_examples_2", ".", "append", "(", "tmp", ")", "\n", "\n", "", "", "args_org", "=", "(", "\n", "(", "cased_examples_org", ",", ")", "if", "'input_sent2'", "not", "in", "features", "[", "0", "]", ".", "keys", "(", ")", "else", "(", "cased_examples_org", ",", "cased_examples_2_org", ")", "\n", ")", "\n", "args", "=", "(", "\n", "(", "cased_examples", ",", ")", "if", "'input_sent2'", "not", "in", "features", "[", "0", "]", ".", "keys", "(", ")", "else", "(", "cased_examples", ",", "cased_examples_2", ")", "\n", ")", "\n", "result_org", "=", "self", ".", "tokenizer", "(", "*", "args_org", ",", "padding", "=", "self", ".", "padding", ",", "max_length", "=", "self", ".", "max_length", ",", "truncation", "=", "True", ")", "\n", "result", "=", "self", ".", "tokenizer", "(", "*", "args", ",", "padding", "=", "self", ".", "padding", ",", "max_length", "=", "self", ".", "max_length", ",", "truncation", "=", "True", ")", "\n", "\n", "final_result", "=", "[", "{", "}", "for", "i", "in", "range", "(", "len", "(", "result_org", "[", "'input_ids'", "]", ")", ")", "]", "\n", "for", "i", "in", "range", "(", "len", "(", "result", "[", "'input_ids'", "]", ")", ")", ":", "\n", "            ", "for", "key", "in", "result_org", ".", "keys", "(", ")", ":", "\n", "                ", "final_result", "[", "i", "]", "[", "key", "]", "=", "result_org", "[", "key", "]", "[", "i", "]", "\n", "final_result", "[", "i", "]", "[", "'switch_'", "+", "key", "]", "=", "result", "[", "key", "]", "[", "i", "]", "\n", "", "if", "'label'", "in", "features", "[", "i", "]", ".", "keys", "(", ")", ":", "\n", "                ", "final_result", "[", "i", "]", "[", "'label'", "]", "=", "features", "[", "i", "]", "[", "'label'", "]", "\n", "\n", "", "", "batch", "=", "default_data_collator", "(", "final_result", ")", "\n", "return", "batch", "\n", "\n"]], "home.repos.pwc.inspect_result.alibaba_self-augmentation-strategy.utils.process_pretrain_data.generate_dataset": [[9, 45], ["os.path.split", "os.path.join", "os.path.isfile", "open", "print", "str", "print", "f.readline", "line.strip.strip", "tokenizer.convert_tokens_to_ids", "current_block.extend", "len", "open", "pickle.dump", "tokenizer.tokenize", "examples.append"], "function", ["None"], ["def", "generate_dataset", "(", "tokenizer", ":", "PreTrainedTokenizer", ",", "file_path", ":", "str", ",", "sequence_length", ":", "int", ")", ":", "\n", "    ", "\"\"\"\n    Input : BertTokenizer, and wiki_books_corpus_data \n    Output : compressed wiki corpus data stored in numpy.int16.\n    \"\"\"", "\n", "directory", ",", "filename", "=", "os", ".", "path", ".", "split", "(", "file_path", ")", "\n", "cached_features_file", "=", "os", ".", "path", ".", "join", "(", "\n", "directory", ",", "\"mlm_{}_{}_{}\"", ".", "format", "(", "tokenizer", ".", "__class__", ".", "__name__", ",", "str", "(", "sequence_length", ")", ",", "filename", ")", ")", "\n", "assert", "os", ".", "path", ".", "isfile", "(", "file_path", ")", "\n", "examples", "=", "[", "]", "\n", "current_block", "=", "[", "]", "\n", "current_length", "=", "0", "\n", "idx", "=", "0", "\n", "with", "open", "(", "file_path", ",", "encoding", "=", "\"utf-8\"", ")", "as", "f", ":", "\n", "        ", "while", "True", ":", "\n", "            ", "idx", "+=", "1", "\n", "print", "(", "idx", ")", "\n", "line", "=", "f", ".", "readline", "(", ")", "\n", "if", "not", "line", ":", "\n", "                ", "break", "\n", "", "line", "=", "line", ".", "strip", "(", ")", "\n", "if", "not", "line", ":", "\n", "                ", "current_block", "=", "[", "]", "\n", "current_length", "=", "0", "\n", "continue", "\n", "", "sentence", "=", "tokenizer", ".", "convert_tokens_to_ids", "(", "tokenizer", ".", "tokenize", "(", "line", ")", ")", "\n", "current_block", ".", "extend", "(", "sentence", ")", "\n", "current_length", "+=", "len", "(", "sentence", ")", "\n", "if", "current_length", ">=", "sequence_length", ":", "\n", "                ", "examples", ".", "append", "(", "current_block", ")", "\n", "current_block", "=", "[", "]", "\n", "current_length", "=", "0", "\n", "\n", "", "", "print", "(", "examples", "[", ":", "5", "]", ")", "\n", "with", "open", "(", "cached_features_file", ",", "\"wb\"", ")", "as", "handle", ":", "\n", "            ", "pickle", ".", "dump", "(", "examples", ",", "handle", ",", "protocol", "=", "pickle", ".", "HIGHEST_PROTOCOL", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.alibaba_self-augmentation-strategy.utils.process_pretrain_data.slice_to_length": [[47, 63], ["range", "numpy.concatenate", "print", "numpy.save", "open", "pickle.load", "numpy.array", "np.concatenate.append", "len"], "function", ["None"], ["", "", "", "def", "slice_to_length", "(", "path", "=", "\"mlm_BertTokenizer_1024_wiki_books_corpus_data_train\"", ",", "length", "=", "128", ")", ":", "\n", "    ", "\"\"\"\n    Input : Tokenized wiki_books_corpus data \n    Output : compressed wiki corpus data stored in numpy.int16.\n    \"\"\"", "\n", "\n", "with", "open", "(", "path", ",", "'rb'", ")", "as", "f", ":", "\n", "        ", "data", "=", "pickle", ".", "load", "(", "f", ")", "\n", "", "output", "=", "[", "]", "\n", "for", "i", "in", "range", "(", "128", "//", "length", ")", ":", "\n", "        ", "data128", "=", "np", ".", "array", "(", "[", "col", "[", "length", "*", "i", ":", "length", "*", "i", "+", "length", "]", "for", "col", "in", "data", "\n", "if", "len", "(", "col", ")", ">=", "length", "*", "i", "+", "length", "]", ",", "dtype", "=", "np", ".", "int16", ")", "\n", "output", ".", "append", "(", "data128", ")", "\n", "", "output", "=", "np", ".", "concatenate", "(", "output", ")", "\n", "print", "(", "output", ".", "shape", ")", "\n", "np", ".", "save", "(", "\"wiki_corpus_full\"", ",", "output", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.alibaba_self-augmentation-strategy.utils.process_pretrain_data.calculat_unigram": [[64, 82], ["numpy.load", "numpy.random.shuffle", "numpy.unique", "numpy.zeros", "numpy.argsort", "transformers.BertTokenizer.from_pretrained", "print", "numpy.save", "BertTokenizer.from_pretrained.decode"], "function", ["None"], ["", "def", "calculat_unigram", "(", ")", ":", "\n", "    ", "\"\"\"\n    Count total number of tokens. Only support 30K word-version dataset. \n    Input : A npy file with arbitrary size.\n    Output: An array include number of tokens.\n    \"\"\"", "\n", "\n", "import", "numpy", "as", "np", "\n", "examples", "=", "np", ".", "load", "(", "\"dataset/wiki_corpus_full.npy\"", ")", "\n", "np", ".", "random", ".", "shuffle", "(", "examples", ")", "\n", "unique_elements", ",", "empirical_distribution", "=", "np", ".", "unique", "(", "examples", ",", "return_counts", "=", "True", ")", "\n", "dis", "=", "np", ".", "zeros", "(", "(", "30522", ")", ")", "\n", "dis", "[", "unique_elements", "]", "=", "empirical_distribution", "\n", "sst", "=", "np", ".", "argsort", "(", "dis", ")", "\n", "from", "transformers", "import", "BertTokenizer", "\n", "tokenizer", "=", "BertTokenizer", ".", "from_pretrained", "(", "'bert-base-uncased'", ")", "\n", "print", "(", "[", "\"%s: %d\"", "%", "(", "tokenizer", ".", "decode", "(", "[", "s", "]", ")", ",", "dis", "[", "s", "]", ")", "for", "s", "in", "sst", "[", "-", "1000", ":", "]", "]", ")", "\n", "np", ".", "save", "(", "\"unigram.npy\"", ",", "dis", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.alibaba_self-augmentation-strategy.utils.configuration_sas.SasConfig.__init__": [[123, 190], ["dict", "transformers.configuration_utils.PretrainedConfig.__init__", "type", "position_embedding_type.split.split.split"], "methods", ["home.repos.pwc.inspect_result.alibaba_self-augmentation-strategy.models.trainer_sas.SequentialDistributedSampler.__init__"], ["def", "__init__", "(", "\n", "self", ",", "\n", "vocab_size", "=", "30522", ",", "\n", "embedding_size", "=", "128", ",", "\n", "hidden_size", "=", "256", ",", "\n", "num_hidden_layers", "=", "12", ",", "\n", "num_attention_heads", "=", "4", ",", "\n", "intermediate_size", "=", "1024", ",", "\n", "hidden_act", "=", "\"gelu\"", ",", "\n", "hidden_dropout_prob", "=", "0.1", ",", "\n", "attention_probs_dropout_prob", "=", "0.1", ",", "\n", "max_position_embeddings", "=", "512", ",", "\n", "type_vocab_size", "=", "2", ",", "\n", "initializer_range", "=", "0.02", ",", "\n", "layer_norm_eps", "=", "1e-12", ",", "\n", "summary_type", "=", "\"first\"", ",", "\n", "summary_use_proj", "=", "True", ",", "\n", "summary_activation", "=", "\"gelu\"", ",", "\n", "summary_last_dropout", "=", "0.1", ",", "\n", "pad_token_id", "=", "0", ",", "\n", "position_embedding_type", "=", "\"absolute\"", ",", "\n", "gen_weight", "=", "1", ",", "\n", "dis_weight", "=", "50", ",", "\n", "dis_weight_scheduler", "=", "1", ",", "\n", "augmentation_copies", "=", "1", ",", "\n", "augmentation_temperature", "=", "1", ",", "\n", "absolute_position_embedding", "=", "1", ",", "\n", "relative_position_embedding", "=", "32", ",", "\n", "cold_start_epochs", "=", "1.25", ",", "\n", "debug_config", "=", "dict", "(", ")", ",", "\n", "**", "kwargs", "\n", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "pad_token_id", "=", "pad_token_id", ",", "**", "kwargs", ")", "\n", "\n", "self", ".", "vocab_size", "=", "vocab_size", "\n", "self", ".", "embedding_size", "=", "embedding_size", "\n", "self", ".", "hidden_size", "=", "hidden_size", "\n", "self", ".", "num_hidden_layers", "=", "num_hidden_layers", "\n", "self", ".", "num_attention_heads", "=", "num_attention_heads", "\n", "self", ".", "intermediate_size", "=", "intermediate_size", "\n", "self", ".", "hidden_act", "=", "hidden_act", "\n", "self", ".", "hidden_dropout_prob", "=", "hidden_dropout_prob", "\n", "self", ".", "attention_probs_dropout_prob", "=", "attention_probs_dropout_prob", "\n", "self", ".", "max_position_embeddings", "=", "max_position_embeddings", "\n", "self", ".", "type_vocab_size", "=", "type_vocab_size", "\n", "self", ".", "initializer_range", "=", "initializer_range", "\n", "self", ".", "layer_norm_eps", "=", "layer_norm_eps", "\n", "\n", "self", ".", "summary_type", "=", "summary_type", "\n", "self", ".", "summary_use_proj", "=", "summary_use_proj", "\n", "self", ".", "summary_activation", "=", "summary_activation", "\n", "self", ".", "summary_last_dropout", "=", "summary_last_dropout", "\n", "if", "type", "(", "position_embedding_type", ")", "==", "str", ":", "\n", "            ", "position_embedding_type", "=", "position_embedding_type", ".", "split", "(", "'+'", ")", "\n", "", "self", ".", "position_embedding_type", "=", "position_embedding_type", "\n", "self", ".", "augmentation_temperature", "=", "augmentation_temperature", "\n", "\n", "self", ".", "gen_weight", "=", "gen_weight", "\n", "self", ".", "dis_weight", "=", "dis_weight", "\n", "self", ".", "dis_weight_scheduler", "=", "dis_weight_scheduler", "\n", "self", ".", "augmentation_copies", "=", "augmentation_copies", "\n", "\n", "self", ".", "absolute_position_embedding", "=", "absolute_position_embedding", "\n", "self", ".", "relative_position_embedding", "=", "relative_position_embedding", "\n", "\n", "self", ".", "cold_start_epochs", "=", "cold_start_epochs", "\n", "self", ".", "debug_config", "=", "debug_config", "\n", "", "", ""]], "home.repos.pwc.inspect_result.alibaba_self-augmentation-strategy.models.trainer_callback_sas.SasTrainerCallback.__init__": [[10, 12], ["None"], "methods", ["None"], ["    ", "def", "__init__", "(", "self", ")", ":", "\n", "        ", "self", ".", "state", "=", "None", "\n", "\n"]], "home.repos.pwc.inspect_result.alibaba_self-augmentation-strategy.models.trainer_callback_sas.SasTrainerCallback.on_step_begin": [[13, 15], ["None"], "methods", ["None"], ["", "def", "on_step_begin", "(", "self", ",", "args", ":", "TrainingArguments", ",", "state", ":", "TrainerState", ",", "control", ":", "TrainerControl", ",", "**", "kwargs", ")", ":", "\n", "        ", "self", ".", "state", "=", "state", "\n", "\n"]], "home.repos.pwc.inspect_result.alibaba_self-augmentation-strategy.models.trainer_callback_sas.SasTrainerCallback.on_step_end": [[16, 22], ["super().on_step_end"], "methods", ["home.repos.pwc.inspect_result.alibaba_self-augmentation-strategy.models.trainer_callback_sas.SasTrainerCallback.on_step_end"], ["", "def", "on_step_end", "(", "self", ",", "args", ":", "TrainingArguments", ",", "state", ":", "TrainerState", ",", "control", ":", "TrainerControl", ",", "**", "kwargs", ")", ":", "\n", "        ", "control", "=", "super", "(", ")", ".", "on_step_end", "(", "args", ",", "state", ",", "control", ",", "**", "kwargs", ")", "\n", "if", "state", ".", "global_step", "<=", "500", "and", "(", "state", ".", "global_step", "-", "1", ")", "%", "10", "==", "0", "and", "args", ".", "logging_first_step", ":", "\n", "            ", "control", ".", "should_log", "=", "True", "\n", "\n", "", "return", "control", "\n", "\n"]], "home.repos.pwc.inspect_result.alibaba_self-augmentation-strategy.models.trainer_callback_sas.SasTrainerCallback.on_train_begin": [[23, 35], ["super().on_train_begin", "getattr", "os.path.isfile", "os.path.join", "torch.load", "torch.load", "logger.info", "os.path.join", "os.path.join"], "methods", ["home.repos.pwc.inspect_result.alibaba_self-augmentation-strategy.models.trainer_callback_sas.SasWandbCallback.on_train_begin"], ["", "def", "on_train_begin", "(", "self", ",", "args", ":", "TrainingArguments", ",", "state", ":", "TrainerState", ",", "control", ":", "TrainerControl", ",", "**", "kwargs", ")", ":", "\n", "        ", "super", "(", ")", ".", "on_train_begin", "(", "TrainingArguments", ",", "state", ",", "control", ",", "**", "kwargs", ")", "\n", "\n", "model_path", "=", "getattr", "(", "kwargs", "[", "'model'", "]", ",", "'model_name_or_path'", ",", "''", ")", "\n", "if", "model_path", "and", "os", ".", "path", ".", "isfile", "(", "os", ".", "path", ".", "join", "(", "model_path", ",", "\"trainer_state.json\"", ")", ")", ":", "\n", "            ", "if", "not", "args", ".", "ignore_data_skip", ":", "\n", "                ", "kwargs", "[", "'model'", "]", ".", "dataset", ".", "buffer_da", "=", "torch", ".", "load", "(", "os", ".", "path", ".", "join", "(", "model_path", ",", "\"data_augmentation.pt\"", ")", ")", "\n", "kwargs", "[", "'model'", "]", ".", "dataset", ".", "buffer_da_flag", "=", "torch", ".", "load", "(", "os", ".", "path", ".", "join", "(", "model_path", ",", "\"data_augmentation_flag.pt\"", ")", ")", "\n", "\n", "logger", ".", "info", "(", "\"  Continuing training from checkpoint and load data augmentation\"", ")", "\n", "\n", "", "", "return", "control", "\n", "\n"]], "home.repos.pwc.inspect_result.alibaba_self-augmentation-strategy.models.trainer_callback_sas.SasTrainerCallback.on_epoch_begin": [[37, 44], ["None"], "methods", ["None"], ["", "def", "on_epoch_begin", "(", "self", ",", "args", ":", "TrainingArguments", ",", "state", ":", "TrainerState", ",", "control", ":", "TrainerControl", ",", "**", "kwargs", ")", ":", "\n", "        ", "kwargs", "[", "'train_dataloader'", "]", ".", "collate_fn", ".", "epoch_train", "=", "state", ".", "epoch", "\n", "if", "state", ".", "epoch", ">=", "1", ":", "# Disable the warmup period of data augmentation", "\n", "            ", "kwargs", "[", "'train_dataloader'", "]", ".", "collate_fn", ".", "unigram_warmup_step", "=", "0", "\n", "\n", "", "control", ".", "should_log", "=", "True", "# first step of each epoch", "\n", "return", "control", "\n", "\n"]], "home.repos.pwc.inspect_result.alibaba_self-augmentation-strategy.models.trainer_callback_sas.SasWandbCallback.__init__": [[47, 50], ["transformers.integrations.WandbCallback.__init__", "dict"], "methods", ["home.repos.pwc.inspect_result.alibaba_self-augmentation-strategy.models.trainer_sas.SequentialDistributedSampler.__init__"], ["    ", "def", "__init__", "(", "self", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "log", "=", "dict", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.alibaba_self-augmentation-strategy.models.trainer_callback_sas.SasWandbCallback.on_train_begin": [[51, 56], ["super().on_train_begin", "args.output_dir.split", "args.output_dir.split"], "methods", ["home.repos.pwc.inspect_result.alibaba_self-augmentation-strategy.models.trainer_callback_sas.SasWandbCallback.on_train_begin"], ["", "def", "on_train_begin", "(", "self", ",", "args", ",", "state", ",", "control", ",", "model", "=", "None", ",", "**", "kwargs", ")", ":", "\n", "        ", "super", "(", ")", ".", "on_train_begin", "(", "args", ",", "state", ",", "control", ",", "model", ",", "**", "kwargs", ")", "\n", "if", "self", ".", "_wandb", "is", "not", "None", ":", "\n", "            ", "self", ".", "_wandb", ".", "run", ".", "name", "=", "args", ".", "output_dir", ".", "split", "(", "\"/\"", ")", "[", "-", "1", "]", "[", "0", ":", "60", "]", "\n", "self", ".", "_wandb", ".", "run", ".", "notes", "=", "args", ".", "output_dir", ".", "split", "(", "\"/\"", ")", "[", "-", "1", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.alibaba_self-augmentation-strategy.models.trainer_callback_sas.SasWandbCallback.on_log": [[58, 65], ["trainer_callback_sas.SasWandbCallback.setup", "trainer_callback_sas.SasWandbCallback._wandb.log"], "methods", ["None"], ["", "", "def", "on_log", "(", "self", ",", "args", ",", "state", ",", "control", ",", "model", "=", "None", ",", "logs", "=", "None", ",", "**", "kwargs", ")", ":", "\n", "        ", "if", "self", ".", "_wandb", "is", "None", ":", "\n", "            ", "return", "\n", "", "if", "not", "self", ".", "_initialized", ":", "\n", "            ", "self", ".", "setup", "(", "args", ",", "state", ",", "model", ",", "reinit", "=", "False", ")", "\n", "", "if", "state", ".", "is_world_process_zero", ":", "\n", "            ", "self", ".", "_wandb", ".", "log", "(", "{", "**", "logs", ",", "**", "self", ".", "log", "}", ",", "step", "=", "state", ".", "global_step", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.alibaba_self-augmentation-strategy.models.trainer_callback_sas.SasTensorBoardCallback.__init__": [[68, 71], ["transformers.integrations.TensorBoardCallback.__init__", "dict"], "methods", ["home.repos.pwc.inspect_result.alibaba_self-augmentation-strategy.models.trainer_sas.SequentialDistributedSampler.__init__"], ["    ", "def", "__init__", "(", "self", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "log", "=", "dict", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.alibaba_self-augmentation-strategy.models.trainer_callback_sas.SasTensorBoardCallback.on_log": [[72, 75], ["super().on_log"], "methods", ["home.repos.pwc.inspect_result.alibaba_self-augmentation-strategy.models.trainer_callback_sas.SasTensorBoardCallback.on_log"], ["", "def", "on_log", "(", "self", ",", "args", ",", "state", ",", "control", ",", "model", "=", "None", ",", "logs", "=", "None", ",", "**", "kwargs", ")", ":", "\n", "        ", "logs", "=", "{", "**", "logs", ",", "**", "self", ".", "log", "}", "\n", "super", "(", ")", ".", "on_log", "(", "args", ",", "state", ",", "control", ",", "logs", ",", "**", "kwargs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.alibaba_self-augmentation-strategy.models.modeling_sas.SasEmbeddings.__init__": [[80, 94], ["torch.Module.__init__", "torch.Embedding", "torch.Embedding", "torch.Embedding", "torch.Embedding", "torch.Embedding", "torch.Embedding", "torch.Embedding", "torch.Embedding", "torch.Embedding", "torch.LayerNorm", "torch.LayerNorm", "torch.LayerNorm", "torch.Dropout", "torch.Dropout", "torch.Dropout", "modeling_sas.SasEmbeddings.register_buffer", "getattr", "torch.arange().expand", "torch.arange().expand", "torch.arange().expand", "torch.arange().expand", "torch.arange().expand", "torch.arange().expand", "torch.arange().expand", "torch.arange().expand", "torch.arange().expand", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange"], "methods", ["home.repos.pwc.inspect_result.alibaba_self-augmentation-strategy.models.trainer_sas.SequentialDistributedSampler.__init__"], ["def", "__init__", "(", "self", ",", "config", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "word_embeddings", "=", "nn", ".", "Embedding", "(", "config", ".", "vocab_size", ",", "config", ".", "embedding_size", ",", "padding_idx", "=", "config", ".", "pad_token_id", ")", "\n", "self", ".", "position_embeddings", "=", "nn", ".", "Embedding", "(", "config", ".", "max_position_embeddings", ",", "config", ".", "embedding_size", ")", "\n", "self", ".", "token_type_embeddings", "=", "nn", ".", "Embedding", "(", "config", ".", "type_vocab_size", ",", "config", ".", "embedding_size", ")", "\n", "\n", "# self.LayerNorm is not snake-cased to stick with TensorFlow model variable name and be able to load", "\n", "# any TensorFlow checkpoint file", "\n", "self", ".", "LayerNorm", "=", "nn", ".", "LayerNorm", "(", "config", ".", "embedding_size", ",", "eps", "=", "config", ".", "layer_norm_eps", ")", "\n", "self", ".", "dropout", "=", "nn", ".", "Dropout", "(", "config", ".", "hidden_dropout_prob", ")", "\n", "\n", "# position_ids (1, len position emb) is contiguous in memory and exported when serialized", "\n", "self", ".", "register_buffer", "(", "\"position_ids\"", ",", "torch", ".", "arange", "(", "config", ".", "max_position_embeddings", ")", ".", "expand", "(", "(", "1", ",", "-", "1", ")", ")", ")", "\n", "self", ".", "position_embedding_type", "=", "getattr", "(", "config", ",", "\"position_embedding_type\"", ",", "[", "\"absolute\"", "]", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.alibaba_self-augmentation-strategy.models.modeling_sas.SasEmbeddings.forward": [[96, 124], ["dict", "modeling_sas.SasEmbeddings.token_type_embeddings", "modeling_sas.SasEmbeddings.LayerNorm", "modeling_sas.SasEmbeddings.dropout", "input_ids.size", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "modeling_sas.SasEmbeddings.word_embeddings", "modeling_sas.SasEmbeddings.position_embeddings", "modeling_sas.SasEmbeddings.size"], "methods", ["None"], ["", "def", "forward", "(", "\n", "self", ",", "input_ids", "=", "None", ",", "token_type_ids", "=", "None", ",", "position_ids", "=", "None", ",", "inputs_embeds", "=", "None", ",", "past_key_values_length", "=", "0", ",", "side_info_sets", "=", "dict", "(", ")", ",", "\n", ")", ":", "\n", "        ", "if", "input_ids", "is", "not", "None", ":", "\n", "            ", "input_shape", "=", "input_ids", ".", "size", "(", ")", "\n", "", "else", ":", "\n", "            ", "input_shape", "=", "inputs_embeds", ".", "size", "(", ")", "[", ":", "-", "1", "]", "\n", "\n", "", "seq_length", "=", "input_shape", "[", "1", "]", "\n", "\n", "if", "position_ids", "is", "None", ":", "\n", "            ", "position_ids", "=", "self", ".", "position_ids", "[", ":", ",", "past_key_values_length", ":", "seq_length", "+", "past_key_values_length", "]", "\n", "\n", "", "if", "token_type_ids", "is", "None", ":", "\n", "            ", "token_type_ids", "=", "torch", ".", "zeros", "(", "input_shape", ",", "dtype", "=", "torch", ".", "long", ",", "device", "=", "self", ".", "position_ids", ".", "device", ")", "\n", "\n", "", "if", "inputs_embeds", "is", "None", ":", "\n", "            ", "inputs_embeds", "=", "self", ".", "word_embeddings", "(", "input_ids", ")", "\n", "", "token_type_embeddings", "=", "self", ".", "token_type_embeddings", "(", "token_type_ids", ")", "\n", "\n", "embeddings", "=", "inputs_embeds", "+", "token_type_embeddings", "\n", "if", "\"absolute\"", "in", "self", ".", "position_embedding_type", ":", "\n", "            ", "position_embeddings", "=", "self", ".", "position_embeddings", "(", "position_ids", ")", "\n", "embeddings", "+=", "position_embeddings", "\n", "\n", "", "embeddings", "=", "self", ".", "LayerNorm", "(", "embeddings", ")", "\n", "embeddings", "=", "self", ".", "dropout", "(", "embeddings", ")", "\n", "return", "embeddings", "\n", "\n"]], "home.repos.pwc.inspect_result.alibaba_self-augmentation-strategy.models.modeling_sas.SasSelfAttention.__init__": [[128, 148], ["torch.Module.__init__", "int", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Dropout", "torch.Dropout", "torch.Dropout", "getattr", "ValueError", "hasattr"], "methods", ["home.repos.pwc.inspect_result.alibaba_self-augmentation-strategy.models.trainer_sas.SequentialDistributedSampler.__init__"], ["    ", "def", "__init__", "(", "self", ",", "config", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "if", "config", ".", "hidden_size", "%", "config", ".", "num_attention_heads", "!=", "0", "and", "not", "hasattr", "(", "config", ",", "\"embedding_size\"", ")", ":", "\n", "            ", "raise", "ValueError", "(", "\n", "\"The hidden size (%d) is not a multiple of the number of attention \"", "\n", "\"heads (%d)\"", "%", "(", "config", ".", "hidden_size", ",", "config", ".", "num_attention_heads", ")", "\n", ")", "\n", "\n", "", "self", ".", "num_attention_heads", "=", "config", ".", "num_attention_heads", "\n", "self", ".", "attention_head_size", "=", "int", "(", "config", ".", "hidden_size", "/", "config", ".", "num_attention_heads", ")", "\n", "self", ".", "all_head_size", "=", "self", ".", "num_attention_heads", "*", "self", ".", "attention_head_size", "\n", "\n", "self", ".", "query", "=", "nn", ".", "Linear", "(", "config", ".", "hidden_size", ",", "self", ".", "all_head_size", ")", "\n", "self", ".", "key", "=", "nn", ".", "Linear", "(", "config", ".", "hidden_size", ",", "self", ".", "all_head_size", ")", "\n", "self", ".", "value", "=", "nn", ".", "Linear", "(", "config", ".", "hidden_size", ",", "self", ".", "all_head_size", ")", "\n", "\n", "self", ".", "dropout", "=", "nn", ".", "Dropout", "(", "config", ".", "attention_probs_dropout_prob", ")", "\n", "self", ".", "position_embedding_type", "=", "getattr", "(", "config", ",", "\"position_embedding_type\"", ",", "[", "\"absolute\"", "]", ")", "\n", "\n", "self", ".", "is_decoder", "=", "config", ".", "is_decoder", "\n", "\n"]], "home.repos.pwc.inspect_result.alibaba_self-augmentation-strategy.models.modeling_sas.SasSelfAttention.transpose_for_scores": [[149, 153], ["x.view.view.view", "x.view.view.permute", "x.view.view.size"], "methods", ["None"], ["", "def", "transpose_for_scores", "(", "self", ",", "x", ")", ":", "\n", "        ", "new_x_shape", "=", "x", ".", "size", "(", ")", "[", ":", "-", "1", "]", "+", "(", "self", ".", "num_attention_heads", ",", "self", ".", "attention_head_size", ")", "\n", "x", "=", "x", ".", "view", "(", "*", "new_x_shape", ")", "\n", "return", "x", ".", "permute", "(", "0", ",", "2", ",", "1", ",", "3", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.alibaba_self-augmentation-strategy.models.modeling_sas.SasSelfAttention.forward": [[154, 235], ["dict", "modeling_sas.SasSelfAttention.query", "modeling_sas.SasSelfAttention.transpose_for_scores", "modeling_sas.SasSelfAttention.dropout", "torch.matmul", "torch.matmul", "torch.matmul", "torch.matmul", "torch.matmul", "torch.matmul", "torch.matmul", "torch.matmul", "torch.matmul", "context_layer.view.view.permute().contiguous", "context_layer.view.view.view", "torch.matmul", "torch.matmul", "torch.matmul", "torch.matmul", "torch.matmul", "torch.matmul", "torch.matmul", "torch.matmul", "torch.matmul", "math.sqrt", "math.sqrt", "torch.Softmax", "torch.Softmax", "torch.Softmax", "modeling_sas.SasSelfAttention.transpose_for_scores", "modeling_sas.SasSelfAttention.transpose_for_scores", "modeling_sas.SasSelfAttention.transpose", "context_layer.view.view.permute", "context_layer.view.view.size", "modeling_sas.SasSelfAttention.key", "modeling_sas.SasSelfAttention.value", "modeling_sas.SasSelfAttention.transpose_for_scores", "modeling_sas.SasSelfAttention.transpose_for_scores", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "modeling_sas.SasSelfAttention.transpose_for_scores", "modeling_sas.SasSelfAttention.transpose_for_scores", "modeling_sas.SasSelfAttention.key", "modeling_sas.SasSelfAttention.value", "modeling_sas.SasSelfAttention.key", "modeling_sas.SasSelfAttention.value"], "methods", ["home.repos.pwc.inspect_result.alibaba_self-augmentation-strategy.models.modeling_sas.SasSelfAttention.transpose_for_scores", "home.repos.pwc.inspect_result.alibaba_self-augmentation-strategy.models.modeling_sas.SasSelfAttention.transpose_for_scores", "home.repos.pwc.inspect_result.alibaba_self-augmentation-strategy.models.modeling_sas.SasSelfAttention.transpose_for_scores", "home.repos.pwc.inspect_result.alibaba_self-augmentation-strategy.models.modeling_sas.SasSelfAttention.transpose_for_scores", "home.repos.pwc.inspect_result.alibaba_self-augmentation-strategy.models.modeling_sas.SasSelfAttention.transpose_for_scores", "home.repos.pwc.inspect_result.alibaba_self-augmentation-strategy.models.modeling_sas.SasSelfAttention.transpose_for_scores", "home.repos.pwc.inspect_result.alibaba_self-augmentation-strategy.models.modeling_sas.SasSelfAttention.transpose_for_scores"], ["", "def", "forward", "(", "\n", "self", ",", "\n", "hidden_states", ",", "\n", "attention_mask", "=", "None", ",", "\n", "head_mask", "=", "None", ",", "\n", "encoder_hidden_states", "=", "None", ",", "\n", "encoder_attention_mask", "=", "None", ",", "\n", "past_key_value", "=", "None", ",", "\n", "output_attentions", "=", "False", ",", "\n", "side_info_sets", "=", "dict", "(", ")", ",", "\n", ")", ":", "\n", "        ", "mixed_query_layer", "=", "self", ".", "query", "(", "hidden_states", ")", "\n", "\n", "# If this is instantiated as a cross-attention module, the keys", "\n", "# and values come from an encoder; the attention mask needs to be", "\n", "# such that the encoder's padding tokens are not attended to.", "\n", "is_cross_attention", "=", "encoder_hidden_states", "is", "not", "None", "\n", "\n", "if", "is_cross_attention", "and", "past_key_value", "is", "not", "None", ":", "\n", "# reuse k,v, cross_attentions", "\n", "            ", "key_layer", "=", "past_key_value", "[", "0", "]", "\n", "value_layer", "=", "past_key_value", "[", "1", "]", "\n", "attention_mask", "=", "encoder_attention_mask", "\n", "", "elif", "is_cross_attention", ":", "\n", "            ", "key_layer", "=", "self", ".", "transpose_for_scores", "(", "self", ".", "key", "(", "encoder_hidden_states", ")", ")", "\n", "value_layer", "=", "self", ".", "transpose_for_scores", "(", "self", ".", "value", "(", "encoder_hidden_states", ")", ")", "\n", "attention_mask", "=", "encoder_attention_mask", "\n", "", "elif", "past_key_value", "is", "not", "None", ":", "\n", "            ", "key_layer", "=", "self", ".", "transpose_for_scores", "(", "self", ".", "key", "(", "hidden_states", ")", ")", "\n", "value_layer", "=", "self", ".", "transpose_for_scores", "(", "self", ".", "value", "(", "hidden_states", ")", ")", "\n", "key_layer", "=", "torch", ".", "cat", "(", "[", "past_key_value", "[", "0", "]", ",", "key_layer", "]", ",", "dim", "=", "2", ")", "\n", "value_layer", "=", "torch", ".", "cat", "(", "[", "past_key_value", "[", "1", "]", ",", "value_layer", "]", ",", "dim", "=", "2", ")", "\n", "", "else", ":", "\n", "            ", "key_layer", "=", "self", ".", "transpose_for_scores", "(", "self", ".", "key", "(", "hidden_states", ")", ")", "\n", "value_layer", "=", "self", ".", "transpose_for_scores", "(", "self", ".", "value", "(", "hidden_states", ")", ")", "\n", "\n", "", "query_layer", "=", "self", ".", "transpose_for_scores", "(", "mixed_query_layer", ")", "\n", "\n", "if", "self", ".", "is_decoder", ":", "\n", "# if cross_attention save Tuple(torch.Tensor, torch.Tensor) of all cross attention key/value_states.", "\n", "# Further calls to cross_attention layer can then reuse all cross-attention", "\n", "# key/value_states (first \"if\" case)", "\n", "# if uni-directional self-attention (decoder) save Tuple(torch.Tensor, torch.Tensor) of", "\n", "# all previous decoder key/value_states. Further calls to uni-directional self-attention", "\n", "# can concat previous decoder key/value_states to current projected key/value_states (third \"elif\" case)", "\n", "# if encoder bi-directional self-attention `past_key_value` is always `None`", "\n", "            ", "past_key_value", "=", "(", "key_layer", ",", "value_layer", ")", "\n", "\n", "# Take the dot product between \"query\" and \"key\" to get the raw attention scores.", "\n", "", "attention_scores", "=", "torch", ".", "matmul", "(", "query_layer", ",", "key_layer", ".", "transpose", "(", "-", "1", ",", "-", "2", ")", ")", "/", "math", ".", "sqrt", "(", "self", ".", "attention_head_size", ")", "\n", "attention_scores_terms", "=", "1", "\n", "\n", "attention_scores", "=", "attention_scores", "/", "math", ".", "sqrt", "(", "attention_scores_terms", ")", "\n", "\n", "\n", "if", "attention_mask", "is", "not", "None", ":", "\n", "# Apply the attention mask is (precomputed for all layers in SasModel forward() function)", "\n", "            ", "attention_scores", "=", "attention_scores", "+", "attention_mask", "\n", "\n", "# Normalize the attention scores to probabilities.", "\n", "", "attention_probs", "=", "nn", ".", "Softmax", "(", "dim", "=", "-", "1", ")", "(", "attention_scores", ")", "\n", "\n", "# This is actually dropping out entire tokens to attend to, which might", "\n", "# seem a bit unusual, but is taken from the original Transformer paper.", "\n", "attention_probs", "=", "self", ".", "dropout", "(", "attention_probs", ")", "\n", "\n", "# Mask heads if we want to", "\n", "if", "head_mask", "is", "not", "None", ":", "\n", "            ", "attention_probs", "=", "attention_probs", "*", "head_mask", "\n", "\n", "", "context_layer", "=", "torch", ".", "matmul", "(", "attention_probs", ",", "value_layer", ")", "\n", "\n", "context_layer", "=", "context_layer", ".", "permute", "(", "0", ",", "2", ",", "1", ",", "3", ")", ".", "contiguous", "(", ")", "\n", "new_context_layer_shape", "=", "context_layer", ".", "size", "(", ")", "[", ":", "-", "2", "]", "+", "(", "self", ".", "all_head_size", ",", ")", "\n", "context_layer", "=", "context_layer", ".", "view", "(", "*", "new_context_layer_shape", ")", "\n", "\n", "outputs", "=", "(", "context_layer", ",", "attention_probs", ")", "if", "output_attentions", "else", "(", "context_layer", ",", ")", "\n", "\n", "if", "self", ".", "is_decoder", ":", "\n", "            ", "outputs", "=", "outputs", "+", "(", "past_key_value", ",", ")", "\n", "", "return", "outputs", "\n", "\n"]], "home.repos.pwc.inspect_result.alibaba_self-augmentation-strategy.models.modeling_sas.SasSelfOutput.__init__": [[239, 244], ["torch.Module.__init__", "torch.Linear", "torch.Linear", "torch.Linear", "torch.LayerNorm", "torch.LayerNorm", "torch.LayerNorm", "torch.Dropout", "torch.Dropout", "torch.Dropout"], "methods", ["home.repos.pwc.inspect_result.alibaba_self-augmentation-strategy.models.trainer_sas.SequentialDistributedSampler.__init__"], ["    ", "def", "__init__", "(", "self", ",", "config", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "dense", "=", "nn", ".", "Linear", "(", "config", ".", "hidden_size", ",", "config", ".", "hidden_size", ")", "\n", "self", ".", "LayerNorm", "=", "nn", ".", "LayerNorm", "(", "config", ".", "hidden_size", ",", "eps", "=", "config", ".", "layer_norm_eps", ")", "\n", "self", ".", "dropout", "=", "nn", ".", "Dropout", "(", "config", ".", "hidden_dropout_prob", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.alibaba_self-augmentation-strategy.models.modeling_sas.SasSelfOutput.forward": [[245, 250], ["modeling_sas.SasSelfOutput.dense", "modeling_sas.SasSelfOutput.dropout", "modeling_sas.SasSelfOutput.LayerNorm"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "hidden_states", ",", "input_tensor", ")", ":", "\n", "        ", "hidden_states", "=", "self", ".", "dense", "(", "hidden_states", ")", "\n", "hidden_states", "=", "self", ".", "dropout", "(", "hidden_states", ")", "\n", "hidden_states", "=", "self", ".", "LayerNorm", "(", "hidden_states", "+", "input_tensor", ")", "\n", "return", "hidden_states", "\n", "\n"]], "home.repos.pwc.inspect_result.alibaba_self-augmentation-strategy.models.modeling_sas.SasAttention.__init__": [[254, 259], ["torch.Module.__init__", "modeling_sas.SasSelfAttention", "modeling_sas.SasSelfOutput", "set"], "methods", ["home.repos.pwc.inspect_result.alibaba_self-augmentation-strategy.models.trainer_sas.SequentialDistributedSampler.__init__"], ["    ", "def", "__init__", "(", "self", ",", "config", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "self", "=", "SasSelfAttention", "(", "config", ")", "\n", "self", ".", "output", "=", "SasSelfOutput", "(", "config", ")", "\n", "self", ".", "pruned_heads", "=", "set", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.alibaba_self-augmentation-strategy.models.modeling_sas.SasAttention.prune_heads": [[260, 277], ["transformers.modeling_utils.find_pruneable_heads_and_indices", "transformers.modeling_utils.prune_linear_layer", "transformers.modeling_utils.prune_linear_layer", "transformers.modeling_utils.prune_linear_layer", "transformers.modeling_utils.prune_linear_layer", "modeling_sas.SasAttention.pruned_heads.union", "len", "len"], "methods", ["None"], ["", "def", "prune_heads", "(", "self", ",", "heads", ")", ":", "\n", "        ", "if", "len", "(", "heads", ")", "==", "0", ":", "\n", "            ", "return", "\n", "", "heads", ",", "index", "=", "find_pruneable_heads_and_indices", "(", "\n", "heads", ",", "self", ".", "self", ".", "num_attention_heads", ",", "self", ".", "self", ".", "attention_head_size", ",", "self", ".", "pruned_heads", "\n", ")", "\n", "\n", "# Prune linear layers", "\n", "self", ".", "self", ".", "query", "=", "prune_linear_layer", "(", "self", ".", "self", ".", "query", ",", "index", ")", "\n", "self", ".", "self", ".", "key", "=", "prune_linear_layer", "(", "self", ".", "self", ".", "key", ",", "index", ")", "\n", "self", ".", "self", ".", "value", "=", "prune_linear_layer", "(", "self", ".", "self", ".", "value", ",", "index", ")", "\n", "self", ".", "output", ".", "dense", "=", "prune_linear_layer", "(", "self", ".", "output", ".", "dense", ",", "index", ",", "dim", "=", "1", ")", "\n", "\n", "# Update hyper params and store pruned heads", "\n", "self", ".", "self", ".", "num_attention_heads", "=", "self", ".", "self", ".", "num_attention_heads", "-", "len", "(", "heads", ")", "\n", "self", ".", "self", ".", "all_head_size", "=", "self", ".", "self", ".", "attention_head_size", "*", "self", ".", "self", ".", "num_attention_heads", "\n", "self", ".", "pruned_heads", "=", "self", ".", "pruned_heads", ".", "union", "(", "heads", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.alibaba_self-augmentation-strategy.models.modeling_sas.SasAttention.forward": [[278, 302], ["dict", "modeling_sas.SasAttention.self", "modeling_sas.SasAttention.output"], "methods", ["None"], ["", "def", "forward", "(", "\n", "self", ",", "\n", "hidden_states", ",", "\n", "attention_mask", "=", "None", ",", "\n", "head_mask", "=", "None", ",", "\n", "encoder_hidden_states", "=", "None", ",", "\n", "encoder_attention_mask", "=", "None", ",", "\n", "past_key_value", "=", "None", ",", "\n", "output_attentions", "=", "False", ",", "\n", "side_info_sets", "=", "dict", "(", ")", ",", "\n", ")", ":", "\n", "        ", "self_outputs", "=", "self", ".", "self", "(", "\n", "hidden_states", ",", "\n", "attention_mask", ",", "\n", "head_mask", ",", "\n", "encoder_hidden_states", ",", "\n", "encoder_attention_mask", ",", "\n", "past_key_value", ",", "\n", "output_attentions", ",", "\n", "side_info_sets", ",", "\n", ")", "\n", "attention_output", "=", "self", ".", "output", "(", "self_outputs", "[", "0", "]", ",", "hidden_states", ")", "\n", "outputs", "=", "(", "attention_output", ",", ")", "+", "self_outputs", "[", "1", ":", "]", "# add attentions if we output them", "\n", "return", "outputs", "\n", "\n"]], "home.repos.pwc.inspect_result.alibaba_self-augmentation-strategy.models.modeling_sas.SasIntermediate.__init__": [[306, 313], ["torch.Module.__init__", "torch.Linear", "torch.Linear", "torch.Linear", "isinstance"], "methods", ["home.repos.pwc.inspect_result.alibaba_self-augmentation-strategy.models.trainer_sas.SequentialDistributedSampler.__init__"], ["    ", "def", "__init__", "(", "self", ",", "config", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "dense", "=", "nn", ".", "Linear", "(", "config", ".", "hidden_size", ",", "config", ".", "intermediate_size", ")", "\n", "if", "isinstance", "(", "config", ".", "hidden_act", ",", "str", ")", ":", "\n", "            ", "self", ".", "intermediate_act_fn", "=", "ACT2FN", "[", "config", ".", "hidden_act", "]", "\n", "", "else", ":", "\n", "            ", "self", ".", "intermediate_act_fn", "=", "config", ".", "hidden_act", "\n", "\n"]], "home.repos.pwc.inspect_result.alibaba_self-augmentation-strategy.models.modeling_sas.SasIntermediate.forward": [[314, 318], ["modeling_sas.SasIntermediate.dense", "modeling_sas.SasIntermediate.intermediate_act_fn"], "methods", ["None"], ["", "", "def", "forward", "(", "self", ",", "hidden_states", ")", ":", "\n", "        ", "hidden_states", "=", "self", ".", "dense", "(", "hidden_states", ")", "\n", "hidden_states", "=", "self", ".", "intermediate_act_fn", "(", "hidden_states", ")", "\n", "return", "hidden_states", "\n", "\n"]], "home.repos.pwc.inspect_result.alibaba_self-augmentation-strategy.models.modeling_sas.SasOutput.__init__": [[322, 327], ["torch.Module.__init__", "torch.Linear", "torch.Linear", "torch.Linear", "torch.LayerNorm", "torch.LayerNorm", "torch.LayerNorm", "torch.Dropout", "torch.Dropout", "torch.Dropout"], "methods", ["home.repos.pwc.inspect_result.alibaba_self-augmentation-strategy.models.trainer_sas.SequentialDistributedSampler.__init__"], ["    ", "def", "__init__", "(", "self", ",", "config", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "dense", "=", "nn", ".", "Linear", "(", "config", ".", "intermediate_size", ",", "config", ".", "hidden_size", ")", "\n", "self", ".", "LayerNorm", "=", "nn", ".", "LayerNorm", "(", "config", ".", "hidden_size", ",", "eps", "=", "config", ".", "layer_norm_eps", ")", "\n", "self", ".", "dropout", "=", "nn", ".", "Dropout", "(", "config", ".", "hidden_dropout_prob", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.alibaba_self-augmentation-strategy.models.modeling_sas.SasOutput.forward": [[328, 333], ["modeling_sas.SasOutput.dense", "modeling_sas.SasOutput.dropout", "modeling_sas.SasOutput.LayerNorm"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "hidden_states", ",", "input_tensor", ")", ":", "\n", "        ", "hidden_states", "=", "self", ".", "dense", "(", "hidden_states", ")", "\n", "hidden_states", "=", "self", ".", "dropout", "(", "hidden_states", ")", "\n", "hidden_states", "=", "self", ".", "LayerNorm", "(", "hidden_states", "+", "input_tensor", ")", "\n", "return", "hidden_states", "\n", "\n"]], "home.repos.pwc.inspect_result.alibaba_self-augmentation-strategy.models.modeling_sas.SasLayer.__init__": [[337, 349], ["torch.Module.__init__", "modeling_sas.SasAttention", "modeling_sas.SasIntermediate", "modeling_sas.SasOutput", "modeling_sas.SasAttention"], "methods", ["home.repos.pwc.inspect_result.alibaba_self-augmentation-strategy.models.trainer_sas.SequentialDistributedSampler.__init__"], ["    ", "def", "__init__", "(", "self", ",", "config", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "chunk_size_feed_forward", "=", "config", ".", "chunk_size_feed_forward", "\n", "self", ".", "seq_len_dim", "=", "1", "\n", "self", ".", "attention", "=", "SasAttention", "(", "config", ")", "\n", "self", ".", "is_decoder", "=", "config", ".", "is_decoder", "\n", "self", ".", "add_cross_attention", "=", "config", ".", "add_cross_attention", "\n", "if", "self", ".", "add_cross_attention", ":", "\n", "            ", "assert", "self", ".", "is_decoder", ",", "f\"{self} should be used as a decoder model if cross attention is added\"", "\n", "self", ".", "crossattention", "=", "SasAttention", "(", "config", ")", "\n", "", "self", ".", "intermediate", "=", "SasIntermediate", "(", "config", ")", "\n", "self", ".", "output", "=", "SasOutput", "(", "config", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.alibaba_self-augmentation-strategy.models.modeling_sas.SasLayer.forward": [[350, 414], ["dict", "modeling_sas.SasLayer.attention", "transformers.modeling_utils.apply_chunking_to_forward", "hasattr", "modeling_sas.SasLayer.crossattention"], "methods", ["None"], ["", "def", "forward", "(", "\n", "self", ",", "\n", "hidden_states", ",", "\n", "attention_mask", "=", "None", ",", "\n", "head_mask", "=", "None", ",", "\n", "encoder_hidden_states", "=", "None", ",", "\n", "encoder_attention_mask", "=", "None", ",", "\n", "past_key_value", "=", "None", ",", "\n", "output_attentions", "=", "False", ",", "\n", "side_info_sets", "=", "dict", "(", ")", ",", "\n", ")", ":", "\n", "# decoder uni-directional self-attention cached key/values tuple is at positions 1,2", "\n", "        ", "self_attn_past_key_value", "=", "past_key_value", "[", ":", "2", "]", "if", "past_key_value", "is", "not", "None", "else", "None", "\n", "self_attention_outputs", "=", "self", ".", "attention", "(", "\n", "hidden_states", ",", "\n", "attention_mask", ",", "\n", "head_mask", ",", "\n", "output_attentions", "=", "output_attentions", ",", "\n", "past_key_value", "=", "self_attn_past_key_value", ",", "\n", "side_info_sets", "=", "side_info_sets", ",", "\n", ")", "\n", "attention_output", "=", "self_attention_outputs", "[", "0", "]", "\n", "\n", "# if decoder, the last output is tuple of self-attn cache", "\n", "if", "self", ".", "is_decoder", ":", "\n", "            ", "outputs", "=", "self_attention_outputs", "[", "1", ":", "-", "1", "]", "\n", "present_key_value", "=", "self_attention_outputs", "[", "-", "1", "]", "\n", "", "else", ":", "\n", "            ", "outputs", "=", "self_attention_outputs", "[", "1", ":", "]", "# add self attentions if we output attention weights", "\n", "\n", "", "cross_attn_present_key_value", "=", "None", "\n", "if", "self", ".", "is_decoder", "and", "encoder_hidden_states", "is", "not", "None", ":", "\n", "            ", "assert", "hasattr", "(", "\n", "self", ",", "\"crossattention\"", "\n", ")", ",", "f\"If `encoder_hidden_states` are passed, {self} has to be instantiated with cross-attention layers by setting `config.add_cross_attention=True`\"", "\n", "\n", "# cross_attn cached key/values tuple is at positions 3,4 of past_key_value tuple", "\n", "cross_attn_past_key_value", "=", "past_key_value", "[", "-", "2", ":", "]", "if", "past_key_value", "is", "not", "None", "else", "None", "\n", "cross_attention_outputs", "=", "self", ".", "crossattention", "(", "\n", "attention_output", ",", "\n", "attention_mask", ",", "\n", "head_mask", ",", "\n", "encoder_hidden_states", ",", "\n", "encoder_attention_mask", ",", "\n", "cross_attn_past_key_value", ",", "\n", "output_attentions", ",", "\n", ")", "\n", "attention_output", "=", "cross_attention_outputs", "[", "0", "]", "\n", "outputs", "=", "outputs", "+", "cross_attention_outputs", "[", "1", ":", "-", "1", "]", "# add cross attentions if we output attention weights", "\n", "\n", "# add cross-attn cache to positions 3,4 of present_key_value tuple", "\n", "cross_attn_present_key_value", "=", "cross_attention_outputs", "[", "-", "1", "]", "\n", "present_key_value", "=", "present_key_value", "+", "cross_attn_present_key_value", "\n", "\n", "", "layer_output", "=", "apply_chunking_to_forward", "(", "\n", "self", ".", "feed_forward_chunk", ",", "self", ".", "chunk_size_feed_forward", ",", "self", ".", "seq_len_dim", ",", "attention_output", "\n", ")", "\n", "outputs", "=", "(", "layer_output", ",", ")", "+", "outputs", "\n", "\n", "# if decoder, return the attn key/values as the last output", "\n", "if", "self", ".", "is_decoder", ":", "\n", "            ", "outputs", "=", "outputs", "+", "(", "present_key_value", ",", ")", "\n", "\n", "", "return", "outputs", "\n", "\n"]], "home.repos.pwc.inspect_result.alibaba_self-augmentation-strategy.models.modeling_sas.SasLayer.feed_forward_chunk": [[415, 419], ["modeling_sas.SasLayer.intermediate", "modeling_sas.SasLayer.output"], "methods", ["None"], ["", "def", "feed_forward_chunk", "(", "self", ",", "attention_output", ")", ":", "\n", "        ", "intermediate_output", "=", "self", ".", "intermediate", "(", "attention_output", ")", "\n", "layer_output", "=", "self", ".", "output", "(", "intermediate_output", ",", "attention_output", ")", "\n", "return", "layer_output", "\n", "\n"]], "home.repos.pwc.inspect_result.alibaba_self-augmentation-strategy.models.modeling_sas.SasEncoder.__init__": [[423, 439], ["torch.Module.__init__", "torch.ModuleList", "torch.ModuleList", "torch.ModuleList", "getattr", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "modeling_sas.SasLayer", "range"], "methods", ["home.repos.pwc.inspect_result.alibaba_self-augmentation-strategy.models.trainer_sas.SequentialDistributedSampler.__init__"], ["    ", "def", "__init__", "(", "self", ",", "config", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "config", "=", "config", "\n", "self", ".", "layer", "=", "nn", ".", "ModuleList", "(", "[", "SasLayer", "(", "config", ")", "for", "_", "in", "range", "(", "config", ".", "num_hidden_layers", ")", "]", ")", "\n", "\n", "self", ".", "position_embedding_type", "=", "getattr", "(", "config", ",", "\"position_embedding_type\"", ",", "[", "\"absolute\"", "]", ")", "\n", "if", "\"absolute_self_only\"", "in", "self", ".", "position_embedding_type", ":", "\n", "# To be used/shared in all self-attention layers. Copy their dimensions here to be consistent.", "\n", "            ", "self", ".", "self_attention", "=", "self", ".", "layer", "[", "0", "]", ".", "attention", ".", "self", "\n", "\n", "self", ".", "num_attention_heads", "=", "self", ".", "self_attention", ".", "num_attention_heads", "\n", "self", ".", "attention_head_size", "=", "self", ".", "self_attention", ".", "attention_head_size", "\n", "self", ".", "all_head_size", "=", "self", ".", "self_attention", ".", "all_head_size", "\n", "\n", "self", ".", "pos_query", "=", "nn", ".", "Linear", "(", "self", ".", "self_attention", ".", "query", ".", "in_features", ",", "self", ".", "self_attention", ".", "query", ".", "out_features", ")", "\n", "self", ".", "pos_key", "=", "nn", ".", "Linear", "(", "self", ".", "self_attention", ".", "key", ".", "in_features", ",", "self", ".", "self_attention", ".", "key", ".", "out_features", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.alibaba_self-augmentation-strategy.models.modeling_sas.SasEncoder.get_position_attention_score": [[440, 448], ["modeling_sas.SasEncoder.self_attention.transpose_for_scores", "modeling_sas.SasEncoder.self_attention.transpose_for_scores", "torch.matmul", "torch.matmul", "torch.matmul", "torch.matmul", "torch.matmul", "torch.matmul", "torch.matmul", "torch.matmul", "torch.matmul", "modeling_sas.SasEncoder.pos_query", "modeling_sas.SasEncoder.pos_key", "modeling_sas.SasEncoder.transpose", "math.sqrt"], "methods", ["home.repos.pwc.inspect_result.alibaba_self-augmentation-strategy.models.modeling_sas.SasSelfAttention.transpose_for_scores", "home.repos.pwc.inspect_result.alibaba_self-augmentation-strategy.models.modeling_sas.SasSelfAttention.transpose_for_scores"], ["", "", "def", "get_position_attention_score", "(", "self", ",", "hidden_states", ")", ":", "\n", "        ", "query_layer", "=", "self", ".", "self_attention", ".", "transpose_for_scores", "(", "self", ".", "pos_query", "(", "hidden_states", ")", ")", "\n", "key_layer", "=", "self", ".", "self_attention", ".", "transpose_for_scores", "(", "self", ".", "pos_key", "(", "hidden_states", ")", ")", "\n", "\n", "# Take the dot product between \"query\" and \"key\" to get the raw attention scores.", "\n", "attention_scores", "=", "torch", ".", "matmul", "(", "query_layer", ",", "key_layer", ".", "transpose", "(", "-", "1", ",", "-", "2", ")", ")", "\n", "attention_scores", "=", "attention_scores", "/", "math", ".", "sqrt", "(", "self", ".", "attention_head_size", ")", "\n", "return", "attention_scores", "\n", "\n"]], "home.repos.pwc.inspect_result.alibaba_self-augmentation-strategy.models.modeling_sas.SasEncoder.forward": [[450, 533], ["dict", "enumerate", "transformers.modeling_outputs.BaseModelOutputWithPastAndCrossAttentions", "getattr", "tuple", "torch.utils.checkpoint.checkpoint", "torch.utils.checkpoint.checkpoint", "torch.utils.checkpoint.checkpoint", "torch.utils.checkpoint.checkpoint", "torch.utils.checkpoint.checkpoint", "torch.utils.checkpoint.checkpoint", "torch.utils.checkpoint.checkpoint", "torch.utils.checkpoint.checkpoint", "torch.utils.checkpoint.checkpoint", "layer_module", "modeling_sas.SasEncoder.forward.create_custom_forward"], "methods", ["None"], ["", "def", "forward", "(", "\n", "self", ",", "\n", "hidden_states", ",", "\n", "attention_mask", "=", "None", ",", "\n", "head_mask", "=", "None", ",", "\n", "encoder_hidden_states", "=", "None", ",", "\n", "encoder_attention_mask", "=", "None", ",", "\n", "past_key_values", "=", "None", ",", "\n", "use_cache", "=", "None", ",", "\n", "output_attentions", "=", "False", ",", "\n", "output_hidden_states", "=", "False", ",", "\n", "side_info_sets", "=", "dict", "(", ")", ",", "\n", "return_dict", "=", "True", ",", "\n", ")", ":", "\n", "        ", "all_hidden_states", "=", "(", ")", "if", "output_hidden_states", "else", "None", "\n", "all_self_attentions", "=", "(", ")", "if", "output_attentions", "else", "None", "\n", "all_cross_attentions", "=", "(", ")", "if", "output_attentions", "and", "self", ".", "config", ".", "add_cross_attention", "else", "None", "\n", "\n", "next_decoder_cache", "=", "(", ")", "if", "use_cache", "else", "None", "\n", "for", "i", ",", "layer_module", "in", "enumerate", "(", "self", ".", "layer", ")", ":", "\n", "            ", "if", "output_hidden_states", ":", "\n", "                ", "all_hidden_states", "=", "all_hidden_states", "+", "(", "hidden_states", ",", ")", "\n", "\n", "", "layer_head_mask", "=", "head_mask", "[", "i", "]", "if", "head_mask", "is", "not", "None", "else", "None", "\n", "past_key_value", "=", "past_key_values", "[", "i", "]", "if", "past_key_values", "is", "not", "None", "else", "None", "\n", "if", "getattr", "(", "self", ".", "config", ",", "\"gradient_checkpointing\"", ",", "False", ")", ":", "\n", "\n", "                ", "def", "create_custom_forward", "(", "module", ")", ":", "\n", "                    ", "def", "custom_forward", "(", "*", "inputs", ")", ":", "\n", "                        ", "return", "module", "(", "*", "inputs", ",", "past_key_value", ",", "output_attentions", ")", "\n", "\n", "", "return", "custom_forward", "\n", "\n", "", "layer_outputs", "=", "torch", ".", "utils", ".", "checkpoint", ".", "checkpoint", "(", "\n", "create_custom_forward", "(", "layer_module", ")", ",", "\n", "hidden_states", ",", "\n", "attention_mask", ",", "\n", "layer_head_mask", ",", "\n", "encoder_hidden_states", ",", "\n", "encoder_attention_mask", ",", "\n", "side_info_sets", ",", "\n", ")", "\n", "", "else", ":", "\n", "                ", "layer_outputs", "=", "layer_module", "(", "\n", "hidden_states", ",", "\n", "attention_mask", ",", "\n", "layer_head_mask", ",", "\n", "encoder_hidden_states", ",", "\n", "encoder_attention_mask", ",", "\n", "past_key_value", ",", "\n", "output_attentions", ",", "\n", "side_info_sets", ",", "\n", ")", "\n", "\n", "", "hidden_states", "=", "layer_outputs", "[", "0", "]", "\n", "if", "use_cache", ":", "\n", "                ", "next_decoder_cache", "+=", "(", "layer_outputs", "[", "-", "1", "]", ",", ")", "\n", "", "if", "output_attentions", ":", "\n", "                ", "all_self_attentions", "=", "all_self_attentions", "+", "(", "layer_outputs", "[", "1", "]", ",", ")", "\n", "if", "self", ".", "config", ".", "add_cross_attention", ":", "\n", "                    ", "all_cross_attentions", "=", "all_cross_attentions", "+", "(", "layer_outputs", "[", "2", "]", ",", ")", "\n", "\n", "", "", "", "if", "output_hidden_states", ":", "\n", "            ", "all_hidden_states", "=", "all_hidden_states", "+", "(", "hidden_states", ",", ")", "\n", "\n", "", "if", "not", "return_dict", ":", "\n", "            ", "return", "tuple", "(", "\n", "v", "\n", "for", "v", "in", "[", "\n", "hidden_states", ",", "\n", "next_decoder_cache", ",", "\n", "all_hidden_states", ",", "\n", "all_self_attentions", ",", "\n", "all_cross_attentions", ",", "\n", "]", "\n", "if", "v", "is", "not", "None", "\n", ")", "\n", "", "return", "BaseModelOutputWithPastAndCrossAttentions", "(", "\n", "last_hidden_state", "=", "hidden_states", ",", "\n", "past_key_values", "=", "next_decoder_cache", ",", "\n", "hidden_states", "=", "all_hidden_states", ",", "\n", "attentions", "=", "all_self_attentions", ",", "\n", "cross_attentions", "=", "all_cross_attentions", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.alibaba_self-augmentation-strategy.models.modeling_sas.SasDiscriminatorPredictions.__init__": [[539, 545], ["torch.Module.__init__", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear"], "methods", ["home.repos.pwc.inspect_result.alibaba_self-augmentation-strategy.models.trainer_sas.SequentialDistributedSampler.__init__"], ["def", "__init__", "(", "self", ",", "config", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "\n", "self", ".", "dense", "=", "nn", ".", "Linear", "(", "config", ".", "hidden_size", ",", "config", ".", "hidden_size", ")", "\n", "self", ".", "dense_prediction", "=", "nn", ".", "Linear", "(", "config", ".", "hidden_size", ",", "1", ")", "\n", "self", ".", "config", "=", "config", "\n", "\n"]], "home.repos.pwc.inspect_result.alibaba_self-augmentation-strategy.models.modeling_sas.SasDiscriminatorPredictions.forward": [[546, 552], ["modeling_sas.SasDiscriminatorPredictions.dense", "modeling_sas.SasDiscriminatorPredictions.dense_prediction().squeeze", "transformers.activations.get_activation", "modeling_sas.SasDiscriminatorPredictions.dense_prediction"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "discriminator_hidden_states", ")", ":", "\n", "        ", "hidden_states", "=", "self", ".", "dense", "(", "discriminator_hidden_states", ")", "\n", "hidden_states", "=", "get_activation", "(", "self", ".", "config", ".", "hidden_act", ")", "(", "hidden_states", ")", "\n", "logits", "=", "self", ".", "dense_prediction", "(", "hidden_states", ")", ".", "squeeze", "(", "-", "1", ")", "\n", "\n", "return", "logits", "\n", "\n"]], "home.repos.pwc.inspect_result.alibaba_self-augmentation-strategy.models.modeling_sas.SasGeneratorPredictions.__init__": [[557, 562], ["torch.Module.__init__", "torch.LayerNorm", "torch.LayerNorm", "torch.LayerNorm", "torch.Linear", "torch.Linear", "torch.Linear"], "methods", ["home.repos.pwc.inspect_result.alibaba_self-augmentation-strategy.models.trainer_sas.SequentialDistributedSampler.__init__"], ["def", "__init__", "(", "self", ",", "config", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "\n", "self", ".", "LayerNorm", "=", "nn", ".", "LayerNorm", "(", "config", ".", "embedding_size", ")", "\n", "self", ".", "dense", "=", "nn", ".", "Linear", "(", "config", ".", "hidden_size", ",", "config", ".", "embedding_size", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.alibaba_self-augmentation-strategy.models.modeling_sas.SasGeneratorPredictions.forward": [[563, 569], ["modeling_sas.SasGeneratorPredictions.dense", "modeling_sas.SasGeneratorPredictions.LayerNorm", "transformers.activations.get_activation"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "generator_hidden_states", ")", ":", "\n", "        ", "hidden_states", "=", "self", ".", "dense", "(", "generator_hidden_states", ")", "\n", "hidden_states", "=", "get_activation", "(", "\"gelu\"", ")", "(", "hidden_states", ")", "\n", "hidden_states", "=", "self", ".", "LayerNorm", "(", "hidden_states", ")", "\n", "\n", "return", "hidden_states", "\n", "\n"]], "home.repos.pwc.inspect_result.alibaba_self-augmentation-strategy.models.modeling_sas.SasPreTrainedModel._init_weights": [[583, 594], ["isinstance", "module.weight.data.normal_", "isinstance", "isinstance", "module.bias.data.zero_", "module.bias.data.zero_", "module.weight.data.fill_"], "methods", ["None"], ["def", "_init_weights", "(", "self", ",", "module", ")", ":", "\n", "        ", "\"\"\" Initialize the weights \"\"\"", "\n", "if", "isinstance", "(", "module", ",", "(", "nn", ".", "Linear", ",", "nn", ".", "Embedding", ")", ")", ":", "\n", "# Slightly different from the TF version which uses truncated_normal for initialization", "\n", "# cf https://github.com/pytorch/pytorch/pull/5617", "\n", "            ", "module", ".", "weight", ".", "data", ".", "normal_", "(", "mean", "=", "0.0", ",", "std", "=", "self", ".", "config", ".", "initializer_range", ")", "\n", "", "elif", "isinstance", "(", "module", ",", "nn", ".", "LayerNorm", ")", ":", "\n", "            ", "module", ".", "bias", ".", "data", ".", "zero_", "(", ")", "\n", "module", ".", "weight", ".", "data", ".", "fill_", "(", "1.0", ")", "\n", "", "if", "isinstance", "(", "module", ",", "nn", ".", "Linear", ")", "and", "module", ".", "bias", "is", "not", "None", ":", "\n", "            ", "module", ".", "bias", ".", "data", ".", "zero_", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.alibaba_self-augmentation-strategy.models.modeling_sas.SasModel.__init__": [[744, 754], ["transformers.modeling_utils.PreTrainedModel.__init__", "modeling_sas.SasEmbeddings", "modeling_sas.SasEncoder", "modeling_sas.SasModel.init_weights", "torch.Linear", "torch.Linear", "torch.Linear"], "methods", ["home.repos.pwc.inspect_result.alibaba_self-augmentation-strategy.models.trainer_sas.SequentialDistributedSampler.__init__"], ["    ", "def", "__init__", "(", "self", ",", "config", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "config", ")", "\n", "self", ".", "embeddings", "=", "SasEmbeddings", "(", "config", ")", "\n", "\n", "if", "config", ".", "embedding_size", "!=", "config", ".", "hidden_size", ":", "\n", "            ", "self", ".", "embeddings_project", "=", "nn", ".", "Linear", "(", "config", ".", "embedding_size", ",", "config", ".", "hidden_size", ")", "\n", "\n", "", "self", ".", "encoder", "=", "SasEncoder", "(", "config", ")", "\n", "self", ".", "config", "=", "config", "\n", "self", ".", "init_weights", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.alibaba_self-augmentation-strategy.models.modeling_sas.SasModel.get_input_embeddings": [[755, 757], ["None"], "methods", ["None"], ["", "def", "get_input_embeddings", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "embeddings", ".", "word_embeddings", "\n", "\n"]], "home.repos.pwc.inspect_result.alibaba_self-augmentation-strategy.models.modeling_sas.SasModel.set_input_embeddings": [[758, 760], ["None"], "methods", ["None"], ["", "def", "set_input_embeddings", "(", "self", ",", "value", ")", ":", "\n", "        ", "self", ".", "embeddings", ".", "word_embeddings", "=", "value", "\n", "\n"]], "home.repos.pwc.inspect_result.alibaba_self-augmentation-strategy.models.modeling_sas.SasModel._prune_heads": [[761, 768], ["heads_to_prune.items", "modeling_sas.SasModel.encoder.layer[].attention.prune_heads"], "methods", ["home.repos.pwc.inspect_result.alibaba_self-augmentation-strategy.models.modeling_sas.SasAttention.prune_heads"], ["", "def", "_prune_heads", "(", "self", ",", "heads_to_prune", ")", ":", "\n", "        ", "\"\"\"\n        Prunes heads of the model. heads_to_prune: dict of {layer_num: list of heads to prune in this layer} See base\n        class PreTrainedModel\n        \"\"\"", "\n", "for", "layer", ",", "heads", "in", "heads_to_prune", ".", "items", "(", ")", ":", "\n", "            ", "self", ".", "encoder", ".", "layer", "[", "layer", "]", ".", "attention", ".", "prune_heads", "(", "heads", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.alibaba_self-augmentation-strategy.models.modeling_sas.SasModel.forward": [[770, 833], ["transformers.file_utils.add_start_docstrings_to_model_forward", "transformers.file_utils.add_code_sample_docstrings", "dict", "modeling_sas.SasModel.get_extended_attention_mask", "modeling_sas.SasModel.get_head_mask", "modeling_sas.SasModel.embeddings", "hasattr", "modeling_sas.SasModel.encoder", "SAS_INPUTS_DOCSTRING.format", "ValueError", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "modeling_sas.SasModel.embeddings_project", "input_ids.size", "ValueError", "inputs_embeds.size"], "methods", ["None"], ["", "", "@", "add_start_docstrings_to_model_forward", "(", "SAS_INPUTS_DOCSTRING", ".", "format", "(", "\"batch_size, sequence_length\"", ")", ")", "\n", "@", "add_code_sample_docstrings", "(", "\n", "tokenizer_class", "=", "_TOKENIZER_FOR_DOC", ",", "\n", "checkpoint", "=", "\"google/sas-small-discriminator\"", ",", "\n", "output_type", "=", "BaseModelOutputWithCrossAttentions", ",", "\n", "config_class", "=", "_CONFIG_FOR_DOC", ",", "\n", ")", "\n", "def", "forward", "(", "\n", "self", ",", "\n", "input_ids", "=", "None", ",", "\n", "attention_mask", "=", "None", ",", "\n", "token_type_ids", "=", "None", ",", "\n", "position_ids", "=", "None", ",", "\n", "head_mask", "=", "None", ",", "\n", "inputs_embeds", "=", "None", ",", "\n", "output_attentions", "=", "None", ",", "\n", "output_hidden_states", "=", "None", ",", "\n", "side_info_sets", "=", "dict", "(", ")", ",", "\n", "return_dict", "=", "None", ",", "\n", ")", ":", "\n", "        ", "output_attentions", "=", "output_attentions", "if", "output_attentions", "is", "not", "None", "else", "self", ".", "config", ".", "output_attentions", "\n", "output_hidden_states", "=", "(", "\n", "output_hidden_states", "if", "output_hidden_states", "is", "not", "None", "else", "self", ".", "config", ".", "output_hidden_states", "\n", ")", "\n", "return_dict", "=", "return_dict", "if", "return_dict", "is", "not", "None", "else", "self", ".", "config", ".", "use_return_dict", "\n", "\n", "if", "input_ids", "is", "not", "None", "and", "inputs_embeds", "is", "not", "None", ":", "\n", "            ", "raise", "ValueError", "(", "\"You cannot specify both input_ids and inputs_embeds at the same time\"", ")", "\n", "", "elif", "input_ids", "is", "not", "None", ":", "\n", "            ", "input_shape", "=", "input_ids", ".", "size", "(", ")", "\n", "", "elif", "inputs_embeds", "is", "not", "None", ":", "\n", "            ", "input_shape", "=", "inputs_embeds", ".", "size", "(", ")", "[", ":", "-", "1", "]", "\n", "", "else", ":", "\n", "            ", "raise", "ValueError", "(", "\"You have to specify either input_ids or inputs_embeds\"", ")", "\n", "\n", "", "device", "=", "input_ids", ".", "device", "if", "input_ids", "is", "not", "None", "else", "inputs_embeds", ".", "device", "\n", "\n", "if", "attention_mask", "is", "None", ":", "\n", "            ", "attention_mask", "=", "torch", ".", "ones", "(", "input_shape", ",", "device", "=", "device", ")", "\n", "", "if", "token_type_ids", "is", "None", ":", "\n", "            ", "token_type_ids", "=", "torch", ".", "zeros", "(", "input_shape", ",", "dtype", "=", "torch", ".", "long", ",", "device", "=", "device", ")", "\n", "\n", "", "extended_attention_mask", "=", "self", ".", "get_extended_attention_mask", "(", "attention_mask", ",", "input_shape", ",", "device", ")", "\n", "head_mask", "=", "self", ".", "get_head_mask", "(", "head_mask", ",", "self", ".", "config", ".", "num_hidden_layers", ")", "\n", "\n", "hidden_states", "=", "self", ".", "embeddings", "(", "\n", "input_ids", "=", "input_ids", ",", "position_ids", "=", "position_ids", ",", "token_type_ids", "=", "token_type_ids", ",", "inputs_embeds", "=", "inputs_embeds", ",", "side_info_sets", "=", "side_info_sets", ",", "\n", ")", "\n", "\n", "if", "hasattr", "(", "self", ",", "\"embeddings_project\"", ")", ":", "\n", "            ", "hidden_states", "=", "self", ".", "embeddings_project", "(", "hidden_states", ")", "\n", "\n", "", "hidden_states", "=", "self", ".", "encoder", "(", "\n", "hidden_states", ",", "\n", "attention_mask", "=", "extended_attention_mask", ",", "\n", "head_mask", "=", "head_mask", ",", "\n", "output_attentions", "=", "output_attentions", ",", "\n", "output_hidden_states", "=", "output_hidden_states", ",", "\n", "side_info_sets", "=", "side_info_sets", ",", "\n", "return_dict", "=", "return_dict", ",", "\n", ")", "\n", "\n", "return", "hidden_states", "\n", "\n"]], "home.repos.pwc.inspect_result.alibaba_self-augmentation-strategy.models.modeling_sas.SasClassificationHead.__init__": [[838, 847], ["torch.Module.__init__", "torch.Linear", "torch.Linear", "torch.Linear", "getattr", "torch.Dropout", "torch.Dropout", "torch.Dropout", "torch.Linear", "torch.Linear", "torch.Linear"], "methods", ["home.repos.pwc.inspect_result.alibaba_self-augmentation-strategy.models.trainer_sas.SequentialDistributedSampler.__init__"], ["def", "__init__", "(", "self", ",", "config", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "dense", "=", "nn", ".", "Linear", "(", "config", ".", "hidden_size", ",", "config", ".", "hidden_size", ")", "\n", "\n", "# configure the cls dropout rate", "\n", "drop_out", "=", "getattr", "(", "config", ",", "\"cls_dropout\"", ",", "None", ")", "\n", "drop_out", "=", "config", ".", "hidden_dropout_prob", "if", "drop_out", "is", "None", "else", "drop_out", "\n", "self", ".", "dropout", "=", "nn", ".", "Dropout", "(", "drop_out", ")", "\n", "self", ".", "out_proj", "=", "nn", ".", "Linear", "(", "config", ".", "hidden_size", ",", "config", ".", "num_labels", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.alibaba_self-augmentation-strategy.models.modeling_sas.SasClassificationHead.forward": [[848, 856], ["modeling_sas.SasClassificationHead.dropout", "modeling_sas.SasClassificationHead.dense", "modeling_sas.SasClassificationHead.dropout", "modeling_sas.SasClassificationHead.out_proj", "transformers.activations.get_activation"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "features", ",", "**", "kwargs", ")", ":", "\n", "        ", "x", "=", "features", "[", ":", ",", "0", ",", ":", "]", "# take <s> token (equiv. to [CLS])", "\n", "x", "=", "self", ".", "dropout", "(", "x", ")", "\n", "x", "=", "self", ".", "dense", "(", "x", ")", "\n", "x", "=", "get_activation", "(", "\"gelu\"", ")", "(", "x", ")", "# although BERT uses tanh here, it seems Sas authors used gelu here", "\n", "x", "=", "self", ".", "dropout", "(", "x", ")", "\n", "x", "=", "self", ".", "out_proj", "(", "x", ")", "\n", "return", "x", "\n", "\n"]], "home.repos.pwc.inspect_result.alibaba_self-augmentation-strategy.models.modeling_sas.Similarity.__init__": [[861, 865], ["torch.Module.__init__", "torch.CosineSimilarity", "torch.CosineSimilarity", "torch.CosineSimilarity"], "methods", ["home.repos.pwc.inspect_result.alibaba_self-augmentation-strategy.models.trainer_sas.SequentialDistributedSampler.__init__"], ["def", "__init__", "(", "self", ",", "temp", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "temp", "=", "temp", "\n", "self", ".", "cos", "=", "nn", ".", "CosineSimilarity", "(", "dim", "=", "-", "1", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.alibaba_self-augmentation-strategy.models.modeling_sas.Similarity.forward": [[866, 868], ["modeling_sas.Similarity.cos"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "x", ",", "y", ")", ":", "\n", "        ", "return", "self", ".", "cos", "(", "x", ",", "y", ")", "/", "self", ".", "temp", "\n", "\n"]], "home.repos.pwc.inspect_result.alibaba_self-augmentation-strategy.models.modeling_sas.EMA.__init__": [[870, 873], ["super().__init__"], "methods", ["home.repos.pwc.inspect_result.alibaba_self-augmentation-strategy.models.trainer_sas.SequentialDistributedSampler.__init__"], ["    ", "def", "__init__", "(", "self", ",", "beta", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "beta", "=", "beta", "\n", "\n"]], "home.repos.pwc.inspect_result.alibaba_self-augmentation-strategy.models.modeling_sas.EMA.__call__": [[874, 878], ["None"], "methods", ["None"], ["", "def", "__call__", "(", "self", ",", "params_ema", ",", "params", ")", ":", "\n", "        ", "if", "params_ema", "is", "None", ":", "\n", "            ", "return", "params", "\n", "", "return", "params_ema", "*", "self", ".", "beta", "+", "(", "1", "-", "self", ".", "beta", ")", "*", "params", "\n", "\n"]], "home.repos.pwc.inspect_result.alibaba_self-augmentation-strategy.models.modeling_sas.SasForSequenceClassification.__init__": [[888, 914], ["transformers.modeling_utils.PreTrainedModel.__init__", "modeling_sas.SasModel", "modeling_sas.SasClassificationHead", "getattr", "trainer_callback_sas.SasTrainerCallback", "trainer_callback_sas.SasTensorBoardCallback", "getattr", "modeling_sas.SasForSequenceClassification.init_weights", "getattr", "modeling_sas.Similarity", "modeling_sas.EMA"], "methods", ["home.repos.pwc.inspect_result.alibaba_self-augmentation-strategy.models.trainer_sas.SequentialDistributedSampler.__init__"], ["    ", "def", "__init__", "(", "self", ",", "config", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "config", ")", "\n", "self", ".", "num_labels", "=", "config", ".", "num_labels", "\n", "self", ".", "sas", "=", "SasModel", "(", "config", ")", "\n", "self", ".", "classifier", "=", "SasClassificationHead", "(", "config", ")", "\n", "\n", "self", ".", "mixup_ratio", "=", "getattr", "(", "config", ",", "\"mixup_ratio\"", ",", "0", ")", "\n", "assert", "(", "0", "<=", "self", ".", "mixup_ratio", "<", "1", ")", "\n", "\n", "self", ".", "trainer_callback", "=", "SasTrainerCallback", "(", ")", "\n", "self", ".", "tensorboard_callback", "=", "SasTensorBoardCallback", "(", ")", "\n", "self", ".", "global_step", "=", "None", "\n", "self", ".", "max_steps", "=", "None", "\n", "\n", "if", "getattr", "(", "config", ",", "\"contrast_temp\"", ",", "None", ")", ":", "\n", "            ", "self", ".", "sim", "=", "Similarity", "(", "temp", "=", "config", ".", "contrast_temp", ")", "\n", "\n", "", "self", ".", "init_weights", "(", ")", "\n", "\n", "momentum_encoder_beta", "=", "getattr", "(", "config", ",", "\"momentum_encoder_beta\"", ",", "0", ")", "\n", "self", ".", "momentum_encoder_beta", "=", "momentum_encoder_beta", "\n", "\n", "if", "self", ".", "momentum_encoder_beta", ">", "0.0", ":", "\n", "            ", "self", ".", "params_ema_updater", "=", "EMA", "(", "self", ".", "momentum_encoder_beta", ")", "\n", "self", ".", "sas_ema", "=", "None", "\n", "self", ".", "classifier_ema", "=", "None", "\n", "\n"]], "home.repos.pwc.inspect_result.alibaba_self-augmentation-strategy.models.modeling_sas.SasForSequenceClassification.get_model_ema": [[915, 926], ["copy.deepcopy", "zip", "copy.deepcopy", "zip", "modeling_sas.SasForSequenceClassification.sas_ema.parameters", "modeling_sas.SasForSequenceClassification.sas.parameters", "modeling_sas.SasForSequenceClassification.params_ema_updater", "modeling_sas.SasForSequenceClassification.classifier_ema.parameters", "modeling_sas.SasForSequenceClassification.classifier.parameters", "modeling_sas.SasForSequenceClassification.params_ema_updater"], "methods", ["None"], ["", "", "def", "get_model_ema", "(", "self", ")", ":", "\n", "        ", "if", "self", ".", "sas_ema", "is", "None", ":", "\n", "            ", "self", ".", "sas_ema", "=", "copy", ".", "deepcopy", "(", "self", ".", "sas", ")", "\n", "", "else", ":", "\n", "            ", "for", "params_ema", ",", "params", "in", "zip", "(", "self", ".", "sas_ema", ".", "parameters", "(", ")", ",", "self", ".", "sas", ".", "parameters", "(", ")", ")", ":", "\n", "                ", "params_ema", ".", "data", "=", "self", ".", "params_ema_updater", "(", "params_ema", ",", "params", ")", "\n", "", "", "if", "self", ".", "classifier_ema", "is", "None", ":", "\n", "            ", "self", ".", "classifier_ema", "=", "copy", ".", "deepcopy", "(", "self", ".", "classifier", ")", "\n", "", "else", ":", "\n", "            ", "for", "params_ema", ",", "params", "in", "zip", "(", "self", ".", "classifier_ema", ".", "parameters", "(", ")", ",", "self", ".", "classifier", ".", "parameters", "(", ")", ")", ":", "\n", "                ", "params_ema", ".", "data", "=", "self", ".", "params_ema_updater", "(", "params_ema", ",", "params", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.alibaba_self-augmentation-strategy.models.modeling_sas.SasForSequenceClassification.batch_initial_status_update": [[927, 937], ["None"], "methods", ["None"], ["", "", "", "def", "batch_initial_status_update", "(", "self", ")", ":", "\n", "        ", "if", "self", ".", "training", ":", "\n", "            ", "self", ".", "global_step", "=", "self", ".", "trainer_callback", ".", "state", ".", "global_step", "\n", "if", "self", ".", "global_step", "<=", "1", ":", "\n", "                ", "self", ".", "max_steps", "=", "self", ".", "trainer_callback", ".", "state", ".", "max_steps", "\n", "\n", "", "", "mixup_ratio", "=", "self", ".", "mixup_ratio", "\n", "if", "self", ".", "mixup_ratio", ">=", "0.5", "and", "self", ".", "global_step", "<", "0.5", "*", "self", ".", "max_steps", ":", "\n", "            ", "mixup_ratio", "=", "0", "\n", "", "return", "mixup_ratio", "\n", "\n"]], "home.repos.pwc.inspect_result.alibaba_self-augmentation-strategy.models.modeling_sas.SasForSequenceClassification.forward": [[938, 1060], ["transformers.file_utils.add_start_docstrings_to_model_forward", "transformers.file_utils.add_code_sample_docstrings", "dict", "modeling_sas.SasForSequenceClassification.batch_initial_status_update", "getattr", "getattr", "getattr", "transformers.modeling_outputs.SequenceClassifierOutput", "SAS_INPUTS_DOCSTRING.format", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "modeling_sas.SasForSequenceClassification.sas_kl_within_batch", "modeling_sas.SasForSequenceClassification.sas_cross_entropy", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "modeling_sas.SasForSequenceClassification.sas_momentum_kl", "modeling_sas.SasForSequenceClassification.sas_kl", "modeling_sas.SasForSequenceClassification.sas_cross_entropy_with_reverse_input", "modeling_sas.SasForSequenceClassification.sas", "modeling_sas.SasForSequenceClassification.classifier", "numpy.roll", "modeling_sas.SasForSequenceClassification.classifier", "torch.zeros_like().scatter_", "torch.zeros_like().scatter_", "torch.zeros_like().scatter_", "torch.zeros_like().scatter_", "torch.zeros_like().scatter_", "torch.zeros_like().scatter_", "torch.zeros_like().scatter_", "torch.zeros_like().scatter_", "torch.zeros_like().scatter_", "torch.log_softmax", "torch.log_softmax", "torch.log_softmax", "torch.nn.MSELoss", "torch.nn.MSELoss", "torch.nn.MSELoss", "torch.nn.CrossEntropyLoss.", "torch.nn.CrossEntropyLoss.", "torch.nn.CrossEntropyLoss.", "torch.nn.CrossEntropyLoss", "torch.nn.CrossEntropyLoss", "torch.nn.CrossEntropyLoss", "torch.nn.CrossEntropyLoss.", "torch.nn.CrossEntropyLoss.", "torch.nn.CrossEntropyLoss.", "numpy.arange", "torch.cat.view", "torch.cat.view", "torch.cat.view", "modeling_sas.SasForSequenceClassification.view", "torch.cat.view", "torch.cat.view", "torch.cat.view", "modeling_sas.SasForSequenceClassification.view", "torch.cat.view", "torch.cat.view", "torch.cat.view", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like"], "methods", ["home.repos.pwc.inspect_result.alibaba_self-augmentation-strategy.models.modeling_sas.SasForPreTraining.batch_initial_status_update", "home.repos.pwc.inspect_result.alibaba_self-augmentation-strategy.models.modeling_sas.SasForSequenceClassification.sas_kl_within_batch", "home.repos.pwc.inspect_result.alibaba_self-augmentation-strategy.models.modeling_sas.SasForSequenceClassification.sas_cross_entropy", "home.repos.pwc.inspect_result.alibaba_self-augmentation-strategy.models.modeling_sas.SasForSequenceClassification.sas_momentum_kl", "home.repos.pwc.inspect_result.alibaba_self-augmentation-strategy.models.modeling_sas.SasForSequenceClassification.sas_kl", "home.repos.pwc.inspect_result.alibaba_self-augmentation-strategy.models.modeling_sas.SasForSequenceClassification.sas_cross_entropy_with_reverse_input"], ["", "@", "add_start_docstrings_to_model_forward", "(", "SAS_INPUTS_DOCSTRING", ".", "format", "(", "\"batch_size, sequence_length\"", ")", ")", "\n", "@", "add_code_sample_docstrings", "(", "\n", "tokenizer_class", "=", "_TOKENIZER_FOR_DOC", ",", "\n", "checkpoint", "=", "\"google/sas-small-discriminator\"", ",", "\n", "output_type", "=", "SequenceClassifierOutput", ",", "\n", "config_class", "=", "_CONFIG_FOR_DOC", ",", "\n", ")", "\n", "def", "forward", "(", "\n", "self", ",", "\n", "input_ids", "=", "None", ",", "\n", "attention_mask", "=", "None", ",", "\n", "token_type_ids", "=", "None", ",", "\n", "position_ids", "=", "None", ",", "\n", "head_mask", "=", "None", ",", "\n", "inputs_embeds", "=", "None", ",", "\n", "labels", "=", "None", ",", "\n", "output_attentions", "=", "None", ",", "\n", "output_hidden_states", "=", "None", ",", "\n", "side_info_sets", "=", "dict", "(", ")", ",", "\n", "return_dict", "=", "None", ",", "\n", "# Temporary solution: Need the following three fields as placeholder so that they are passed to dataloader (in trainer.py) and then is used for efficient calculation side information embedding in GLUE fine-tuning", "\n", "idx", "=", "None", ",", "\n", "ss_sentence_position_in_sequence", "=", "None", ",", "\n", "ss_token_position_in_sentence", "=", "None", ",", "\n", "# reversed sentence pair input ids, processed by process function in run_glue.py", "\n", "reverse_input_ids", "=", "None", ",", "\n", "reverse_token_type_ids", "=", "None", ",", "\n", "reverse_attention_mask", "=", "None", ",", "\n", "# For switch case ", "\n", "input_sent1", "=", "None", ",", "\n", "input_sent2", "=", "None", ",", "\n", "switch_input_ids", "=", "None", ",", "\n", "switch_token_type_ids", "=", "None", ",", "\n", "switch_attention_mask", "=", "None", "\n", ")", ":", "\n", "        ", "r\"\"\"\n        labels (:obj:`torch.LongTensor` of shape :obj:`(batch_size,)`, `optional`):\n            Labels for computing the sequence classification/regression loss. Indices should be in :obj:`[0, ...,\n            config.num_labels - 1]`. If :obj:`config.num_labels == 1` a regression loss is computed (Mean-Square loss),\n            If :obj:`config.num_labels > 1` a classification loss is computed (Cross-Entropy).\n        \"\"\"", "\n", "return_dict", "=", "return_dict", "if", "return_dict", "is", "not", "None", "else", "self", ".", "config", ".", "use_return_dict", "\n", "mixup_ratio", "=", "self", ".", "batch_initial_status_update", "(", ")", "\n", "\n", "self", ".", "config", ".", "rdrop_weight", "=", "getattr", "(", "self", ".", "config", ",", "\"rdrop_weight\"", ",", "0", ")", "\n", "self", ".", "config", ".", "contrast_weight", "=", "getattr", "(", "self", ".", "config", ",", "\"contrast_weight\"", ",", "0", ")", "\n", "self", ".", "config", ".", "switch_case_prob", "=", "getattr", "(", "self", ".", "config", ",", "\"switch_case_prob\"", ",", "0", ")", "\n", "\n", "if", "self", ".", "config", ".", "rdrop_weight", ">", "0", "or", "self", ".", "config", ".", "contrast_weight", ">", "0", ":", "\n", "            ", "if", "self", ".", "config", ".", "switch_case_prob", ">", "0.0", ":", "\n", "                ", "input_ids", "=", "torch", ".", "cat", "(", "[", "input_ids", ",", "switch_input_ids", "]", ",", "dim", "=", "0", ")", "\n", "attention_mask", "=", "torch", ".", "cat", "(", "[", "attention_mask", ",", "switch_attention_mask", "]", ",", "dim", "=", "0", ")", "\n", "token_type_ids", "=", "torch", ".", "cat", "(", "[", "token_type_ids", ",", "switch_token_type_ids", "]", ",", "dim", "=", "0", ")", "\n", "if", "labels", "is", "not", "None", ":", "\n", "                    ", "labels", "=", "torch", ".", "cat", "(", "[", "labels", ",", "labels", "]", ",", "dim", "=", "0", ")", "\n", "", "loss", ",", "logits", ",", "outputs", "=", "self", ".", "sas_kl_within_batch", "(", "input_ids", ",", "attention_mask", ",", "token_type_ids", ",", "position_ids", ",", "inputs_embeds", ",", "labels", ",", "output_attentions", ",", "output_hidden_states", ",", "return_dict", ")", "\n", "\n", "", "elif", "self", ".", "momentum_encoder_beta", ">", "0.0", ":", "\n", "                ", "loss", ",", "logits", ",", "outputs", "=", "self", ".", "sas_momentum_kl", "(", "input_ids", ",", "attention_mask", ",", "token_type_ids", ",", "position_ids", ",", "inputs_embeds", ",", "labels", ",", "output_attentions", ",", "output_hidden_states", ",", "return_dict", ")", "\n", "\n", "", "else", ":", "\n", "                ", "loss", ",", "logits", ",", "outputs", "=", "self", ".", "sas_kl", "(", "input_ids", ",", "attention_mask", ",", "token_type_ids", ",", "position_ids", ",", "inputs_embeds", ",", "labels", ",", "output_attentions", ",", "output_hidden_states", ",", "return_dict", ")", "\n", "\n", "\n", "", "", "elif", "self", ".", "config", ".", "switch_case_prob", ">", "0.0", ":", "\n", "            ", "if", "self", ".", "training", ":", "\n", "                ", "input_ids", "=", "switch_input_ids", "\n", "attention_mask", "=", "switch_attention_mask", "\n", "token_type_ids", "=", "switch_token_type_ids", "\n", "", "loss", ",", "logits", ",", "outputs", "=", "self", ".", "sas_cross_entropy", "(", "input_ids", ",", "attention_mask", ",", "token_type_ids", ",", "position_ids", ",", "inputs_embeds", ",", "labels", ",", "output_attentions", ",", "output_hidden_states", ",", "return_dict", ")", "\n", "\n", "", "elif", "reverse_input_ids", "is", "not", "None", ":", "\n", "            ", "loss", ",", "logits", ",", "outputs", "=", "self", ".", "sas_cross_entropy_with_reverse_input", "(", "input_ids", ",", "attention_mask", ",", "token_type_ids", ",", "position_ids", ",", "inputs_embeds", ",", "labels", ",", "output_attentions", ",", "output_hidden_states", ",", "return_dict", ")", "\n", "\n", "", "else", ":", "\n", "            ", "discriminator_hidden_states", "=", "self", ".", "sas", "(", "\n", "input_ids", ",", "\n", "attention_mask", ",", "\n", "token_type_ids", ",", "\n", "position_ids", ",", "\n", "head_mask", ",", "\n", "inputs_embeds", ",", "\n", "output_attentions", ",", "\n", "output_hidden_states", ",", "\n", "side_info_sets", ",", "\n", "return_dict", ",", "\n", ")", "\n", "\n", "sequence_output", "=", "discriminator_hidden_states", "[", "0", "]", "\n", "\n", "logits", "=", "self", ".", "classifier", "(", "sequence_output", ")", "\n", "loss", "=", "None", "\n", "if", "labels", "is", "not", "None", ":", "\n", "                ", "if", "not", "self", ".", "training", "or", "mixup_ratio", "==", "0", "or", "self", ".", "num_labels", "==", "1", ":", "\n", "                    ", "if", "self", ".", "num_labels", "==", "1", ":", "\n", "#  We are doing regression", "\n", "                        ", "loss_fct", "=", "MSELoss", "(", ")", "\n", "loss", "=", "loss_fct", "(", "logits", ".", "view", "(", "-", "1", ")", ",", "labels", ".", "view", "(", "-", "1", ")", ")", "\n", "", "else", ":", "\n", "                        ", "loss_fct", "=", "CrossEntropyLoss", "(", ")", "\n", "loss", "=", "loss_fct", "(", "logits", ".", "view", "(", "-", "1", ",", "self", ".", "num_labels", ")", ",", "labels", ".", "view", "(", "-", "1", ")", ")", "\n", "", "", "else", ":", "# Do mixup", "\n", "                    ", "batch_size", "=", "sequence_output", ".", "shape", "[", "0", "]", "\n", "mixup_idx", "=", "np", ".", "roll", "(", "np", ".", "arange", "(", "batch_size", ")", ",", "shift", "=", "-", "1", ")", "\n", "sequence_output_mixup", "=", "(", "1", "-", "mixup_ratio", ")", "*", "sequence_output", "+", "mixup_ratio", "*", "sequence_output", "[", "mixup_idx", ",", "...", "]", "\n", "logits_mixup", "=", "self", ".", "classifier", "(", "sequence_output_mixup", ")", "\n", "\n", "one_hot", "=", "torch", ".", "zeros_like", "(", "logits_mixup", ")", ".", "scatter_", "(", "-", "1", ",", "labels", ".", "view", "(", "-", "1", ",", "1", ")", ",", "1", ")", "\n", "labels_mixup", "=", "(", "1", "-", "mixup_ratio", ")", "*", "one_hot", "+", "mixup_ratio", "*", "one_hot", "[", "mixup_idx", ",", "...", "]", "\n", "\n", "log_prb", "=", "F", ".", "log_softmax", "(", "logits_mixup", ",", "dim", "=", "1", ")", "\n", "loss", "=", "-", "(", "labels_mixup", "*", "log_prb", ")", ".", "sum", "(", "dim", "=", "1", ")", ".", "mean", "(", ")", "\n", "\n", "", "", "", "if", "not", "return_dict", ":", "\n", "            ", "output", "=", "(", "logits", ",", ")", "+", "outputs", "[", "1", ":", "]", "\n", "return", "(", "(", "loss", ",", ")", "+", "output", ")", "if", "loss", "is", "not", "None", "else", "output", "\n", "\n", "", "return", "SequenceClassifierOutput", "(", "\n", "loss", "=", "loss", ",", "\n", "logits", "=", "logits", ",", "\n", "hidden_states", "=", "outputs", ".", "hidden_states", ",", "\n", "attentions", "=", "outputs", ".", "attentions", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.alibaba_self-augmentation-strategy.models.modeling_sas.SasForSequenceClassification.sas_cross_entropy_with_reverse_input": [[1062, 1129], ["range", "modeling_sas.SasForSequenceClassification.classifier", "prediction_scores_list.append", "outputs_list.append", "modeling_sas.SasForSequenceClassification.sas", "modeling_sas.SasForSequenceClassification.sas", "torch.nn.MSELoss", "torch.nn.MSELoss", "torch.nn.MSELoss", "torch.log_softmax", "torch.log_softmax", "torch.log_softmax", "torch.log_softmax", "torch.log_softmax", "torch.log_softmax", "torch.log_softmax", "torch.log_softmax", "torch.log_softmax", "torch.softmax", "torch.softmax", "torch.softmax", "torch.softmax", "torch.softmax", "torch.softmax", "torch.softmax", "torch.softmax", "torch.softmax", "torch.log_softmax", "torch.log_softmax", "torch.log_softmax", "torch.log_softmax", "torch.log_softmax", "torch.log_softmax", "torch.log_softmax", "torch.log_softmax", "torch.log_softmax", "torch.softmax", "torch.softmax", "torch.softmax", "torch.softmax", "torch.softmax", "torch.softmax", "torch.softmax", "torch.softmax", "torch.softmax", "torch.nn.functional.kl_div().sum", "torch.nn.functional.kl_div().sum", "torch.nn.functional.kl_div().sum", "torch.nn.functional.kl_div().sum", "torch.nn.functional.kl_div().sum", "torch.nn.functional.kl_div().sum", "torch.nn.functional.kl_div().sum", "torch.nn.functional.kl_div().sum", "torch.nn.functional.kl_div().sum", "torch.nn.functional.kl_div().sum", "torch.nn.functional.kl_div().sum", "torch.nn.functional.kl_div().sum", "torch.nn.functional.kl_div().sum", "torch.nn.functional.kl_div().sum", "torch.nn.functional.kl_div().sum", "torch.nn.functional.kl_div().sum", "torch.nn.functional.kl_div().sum", "torch.nn.functional.kl_div().sum", "torch.nn.MSELoss", "torch.nn.MSELoss", "torch.nn.MSELoss", "torch.nn.MSELoss", "torch.nn.MSELoss", "torch.nn.MSELoss", "torch.nn.MSELoss", "torch.nn.MSELoss", "torch.nn.MSELoss", "torch.nn.CrossEntropyLoss", "torch.nn.CrossEntropyLoss", "torch.nn.CrossEntropyLoss", "torch.nn.CrossEntropyLoss.", "torch.nn.CrossEntropyLoss.", "torch.nn.CrossEntropyLoss.", "prediction_scores_list[].view", "prediction_scores_list[].view", "prediction_scores_list[].view", "prediction_scores_list[].view", "torch.nn.CrossEntropyLoss.", "torch.nn.CrossEntropyLoss.", "torch.nn.CrossEntropyLoss.", "torch.nn.CrossEntropyLoss.", "torch.nn.CrossEntropyLoss.", "torch.nn.CrossEntropyLoss.", "torch.nn.CrossEntropyLoss.", "torch.nn.CrossEntropyLoss.", "torch.nn.CrossEntropyLoss.", "torch.nn.CrossEntropyLoss.", "torch.nn.CrossEntropyLoss.", "torch.nn.CrossEntropyLoss.", "prediction_scores_list[].view", "prediction_scores_list[].view", "torch.nn.functional.kl_div", "torch.nn.functional.kl_div", "torch.nn.functional.kl_div", "torch.nn.functional.kl_div", "torch.nn.functional.kl_div", "torch.nn.functional.kl_div", "torch.nn.functional.kl_div", "torch.nn.functional.kl_div", "torch.nn.functional.kl_div", "torch.nn.functional.kl_div", "torch.nn.functional.kl_div", "torch.nn.functional.kl_div", "torch.nn.functional.kl_div", "torch.nn.functional.kl_div", "torch.nn.functional.kl_div", "torch.nn.functional.kl_div", "torch.nn.functional.kl_div", "torch.nn.functional.kl_div", "modeling_sas.SasForSequenceClassification.view", "labels.view", "modeling_sas.SasForSequenceClassification.view", "labels.view", "modeling_sas.SasForSequenceClassification.view", "labels.view", "modeling_sas.SasForSequenceClassification.view", "labels.view"], "methods", ["None"], ["", "def", "sas_cross_entropy_with_reverse_input", "(", "self", ",", "input_ids", ",", "attention_mask", ",", "token_type_ids", ",", "position_ids", ",", "inputs_embeds", ",", "labels", ",", "output_attentions", ",", "output_hidden_states", ",", "return_dict", ",", "reverse_input_ids", ",", "reverse_token_type_ids", ",", "reverse_attention_mask", ")", ":", "\n", "        ", "prediction_scores_list", "=", "[", "]", "\n", "outputs_list", "=", "[", "]", "\n", "for", "i", "in", "range", "(", "2", ")", ":", "\n", "            ", "if", "i", "==", "1", ":", "\n", "                ", "outputs", "=", "self", ".", "sas", "(", "\n", "reverse_input_ids", ",", "\n", "token_type_ids", "=", "reverse_token_type_ids", ",", "\n", "attention_mask", "=", "reverse_attention_mask", ",", "\n", "position_ids", "=", "position_ids", ",", "\n", "inputs_embeds", "=", "inputs_embeds", ",", "\n", "output_attentions", "=", "output_attentions", ",", "\n", "output_hidden_states", "=", "output_hidden_states", ",", "\n", "return_dict", "=", "return_dict", "\n", ")", "\n", "", "else", ":", "\n", "                ", "outputs", "=", "self", ".", "sas", "(", "\n", "input_ids", ",", "\n", "token_type_ids", "=", "token_type_ids", ",", "\n", "attention_mask", "=", "attention_mask", ",", "\n", "position_ids", "=", "position_ids", ",", "\n", "inputs_embeds", "=", "inputs_embeds", ",", "\n", "output_attentions", "=", "output_attentions", ",", "\n", "output_hidden_states", "=", "output_hidden_states", ",", "\n", "return_dict", "=", "return_dict", "\n", ")", "\n", "\n", "", "sequence_output", "=", "outputs", "[", "0", "]", "\n", "logits", "=", "self", ".", "classifier", "(", "sequence_output", ")", "\n", "prediction_scores_list", ".", "append", "(", "logits", ")", "\n", "outputs_list", ".", "append", "(", "outputs", ")", "\n", "\n", "", "loss", "=", "None", "\n", "for", "logits", "in", "prediction_scores_list", ":", "\n", "            ", "if", "labels", "is", "not", "None", ":", "\n", "                ", "if", "self", ".", "num_labels", "==", "1", ":", "\n", "                    ", "loss_fn", "=", "torch", ".", "nn", ".", "MSELoss", "(", ")", "\n", "if", "loss", ":", "\n", "                        ", "loss", "+=", "loss_fn", "(", "logits", ".", "view", "(", "-", "1", ")", ",", "labels", ".", "view", "(", "-", "1", ")", ")", "\n", "", "else", ":", "\n", "                        ", "loss", "=", "loss_fn", "(", "logits", ".", "view", "(", "-", "1", ")", ",", "labels", ".", "view", "(", "-", "1", ")", ")", "\n", "", "", "else", ":", "\n", "                    ", "loss_fn", "=", "CrossEntropyLoss", "(", ")", "\n", "if", "loss", ":", "\n", "                        ", "loss", "+=", "loss_fn", "(", "logits", ".", "view", "(", "-", "1", ",", "self", ".", "num_labels", ")", ",", "labels", ".", "view", "(", "-", "1", ")", ")", "\n", "", "else", ":", "\n", "                        ", "loss", "=", "loss_fn", "(", "logits", ".", "view", "(", "-", "1", ",", "self", ".", "num_labels", ")", ",", "labels", ".", "view", "(", "-", "1", ")", ")", "\n", "\n", "", "", "", "", "if", "loss", "is", "not", "None", ":", "\n", "            ", "if", "self", ".", "num_labels", "==", "1", ":", "\n", "                ", "loss_fn", "=", "MSELoss", "(", ")", "\n", "rdrop_loss", "=", "self", ".", "config", ".", "reverse_weight", "*", "loss_fn", "(", "prediction_scores_list", "[", "0", "]", ".", "view", "(", "-", "1", ")", ",", "prediction_scores_list", "[", "-", "1", "]", ".", "view", "(", "-", "1", ")", ")", "\n", "loss", "+=", "rdrop_loss", "\n", "", "else", ":", "\n", "# use kl divergence loss", "\n", "                ", "p", "=", "torch", ".", "log_softmax", "(", "prediction_scores_list", "[", "0", "]", ".", "view", "(", "-", "1", ",", "self", ".", "num_labels", ")", ",", "dim", "=", "-", "1", ")", "\n", "p_tec", "=", "torch", ".", "softmax", "(", "prediction_scores_list", "[", "0", "]", ".", "view", "(", "-", "1", ",", "self", ".", "num_labels", ")", ",", "dim", "=", "-", "1", ")", "\n", "q", "=", "torch", ".", "log_softmax", "(", "prediction_scores_list", "[", "-", "1", "]", ".", "view", "(", "-", "1", ",", "self", ".", "num_labels", ")", ",", "dim", "=", "-", "1", ")", "\n", "q_tec", "=", "torch", ".", "softmax", "(", "prediction_scores_list", "[", "-", "1", "]", ".", "view", "(", "-", "1", ",", "self", ".", "num_labels", ")", ",", "dim", "=", "-", "1", ")", "\n", "\n", "kl_loss", "=", "torch", ".", "nn", ".", "functional", ".", "kl_div", "(", "p", ",", "q_tec", ",", "reduction", "=", "'none'", ")", ".", "sum", "(", ")", "\n", "reverse_kl_loss", "=", "torch", ".", "nn", ".", "functional", ".", "kl_div", "(", "q", ",", "p_tec", ",", "reduction", "=", "'none'", ")", ".", "sum", "(", ")", "\n", "\n", "rdrop_loss", "=", "self", ".", "config", ".", "reverse_weight", "*", "(", "kl_loss", "+", "reverse_kl_loss", ")", "/", "2.0", "\n", "loss", "+=", "rdrop_loss", "\n", "\n", "", "", "return", "loss", ",", "prediction_scores_list", "[", "0", "]", ",", "outputs_list", "[", "0", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.alibaba_self-augmentation-strategy.models.modeling_sas.SasForSequenceClassification.sas_cross_entropy": [[1131, 1154], ["modeling_sas.SasForSequenceClassification.sas", "modeling_sas.SasForSequenceClassification.classifier", "torch.nn.MSELoss", "torch.nn.MSELoss", "torch.nn.MSELoss", "torch.nn.MSELoss", "torch.nn.MSELoss", "torch.nn.MSELoss", "torch.nn.MSELoss", "torch.nn.MSELoss", "torch.nn.MSELoss", "torch.nn.CrossEntropyLoss.", "torch.nn.CrossEntropyLoss.", "torch.nn.CrossEntropyLoss.", "torch.nn.CrossEntropyLoss", "torch.nn.CrossEntropyLoss", "torch.nn.CrossEntropyLoss", "torch.nn.CrossEntropyLoss.", "torch.nn.CrossEntropyLoss.", "torch.nn.CrossEntropyLoss.", "modeling_sas.SasForSequenceClassification.view", "labels.view", "modeling_sas.SasForSequenceClassification.view", "labels.view"], "methods", ["None"], ["", "def", "sas_cross_entropy", "(", "self", ",", "input_ids", ",", "attention_mask", ",", "token_type_ids", ",", "position_ids", ",", "inputs_embeds", ",", "labels", ",", "output_attentions", ",", "output_hidden_states", ",", "return_dict", ")", ":", "\n", "        ", "outputs", "=", "self", ".", "sas", "(", "\n", "input_ids", ",", "\n", "token_type_ids", "=", "token_type_ids", ",", "\n", "attention_mask", "=", "attention_mask", ",", "\n", "position_ids", "=", "position_ids", ",", "\n", "inputs_embeds", "=", "inputs_embeds", ",", "\n", "output_attentions", "=", "output_attentions", ",", "\n", "output_hidden_states", "=", "output_hidden_states", ",", "\n", "return_dict", "=", "return_dict", "\n", ")", "\n", "sequence_output", "=", "outputs", "[", "0", "]", "\n", "logits", "=", "self", ".", "classifier", "(", "sequence_output", ")", "\n", "\n", "loss", "=", "None", "\n", "if", "labels", "is", "not", "None", ":", "\n", "            ", "if", "self", ".", "num_labels", "==", "1", ":", "\n", "                ", "loss_fn", "=", "torch", ".", "nn", ".", "MSELoss", "(", ")", "\n", "loss", "=", "loss_fn", "(", "logits", ".", "view", "(", "-", "1", ")", ",", "labels", ".", "view", "(", "-", "1", ")", ")", "\n", "", "else", ":", "\n", "                ", "loss_fn", "=", "CrossEntropyLoss", "(", ")", "\n", "loss", "=", "loss_fn", "(", "logits", ".", "view", "(", "-", "1", ",", "self", ".", "num_labels", ")", ",", "labels", ".", "view", "(", "-", "1", ")", ")", "\n", "", "", "return", "loss", ",", "logits", ",", "outputs", "\n", "\n"]], "home.repos.pwc.inspect_result.alibaba_self-augmentation-strategy.models.modeling_sas.SasForSequenceClassification.sas_kl_within_batch": [[1156, 1203], ["input_ids.size", "modeling_sas.SasForSequenceClassification.sas", "modeling_sas.SasForSequenceClassification.classifier", "torch.nn.MSELoss", "torch.nn.MSELoss", "torch.nn.MSELoss", "torch.nn.MSELoss", "torch.nn.MSELoss", "torch.nn.MSELoss", "torch.nn.MSELoss", "torch.nn.MSELoss", "torch.nn.MSELoss", "torch.nn.MSELoss.", "torch.nn.MSELoss.", "torch.nn.MSELoss.", "torch.nn.CrossEntropyLoss", "torch.nn.CrossEntropyLoss", "torch.nn.CrossEntropyLoss", "torch.nn.MSELoss.", "torch.nn.MSELoss.", "torch.nn.MSELoss.", "torch.nn.MSELoss", "torch.nn.MSELoss", "torch.nn.MSELoss", "torch.log_softmax", "torch.log_softmax", "torch.log_softmax", "torch.log_softmax", "torch.log_softmax", "torch.log_softmax", "torch.log_softmax", "torch.log_softmax", "torch.log_softmax", "torch.softmax", "torch.softmax", "torch.softmax", "torch.softmax", "torch.softmax", "torch.softmax", "torch.softmax", "torch.softmax", "torch.softmax", "torch.log_softmax", "torch.log_softmax", "torch.log_softmax", "torch.log_softmax", "torch.log_softmax", "torch.log_softmax", "torch.log_softmax", "torch.log_softmax", "torch.log_softmax", "torch.softmax", "torch.softmax", "torch.softmax", "torch.softmax", "torch.softmax", "torch.softmax", "torch.softmax", "torch.softmax", "torch.softmax", "torch.nn.functional.kl_div().sum", "torch.nn.functional.kl_div().sum", "torch.nn.functional.kl_div().sum", "torch.nn.functional.kl_div().sum", "torch.nn.functional.kl_div().sum", "torch.nn.functional.kl_div().sum", "torch.nn.functional.kl_div().sum", "torch.nn.functional.kl_div().sum", "torch.nn.functional.kl_div().sum", "torch.nn.functional.kl_div().sum", "torch.nn.functional.kl_div().sum", "torch.nn.functional.kl_div().sum", "torch.nn.functional.kl_div().sum", "torch.nn.functional.kl_div().sum", "torch.nn.functional.kl_div().sum", "torch.nn.functional.kl_div().sum", "torch.nn.functional.kl_div().sum", "torch.nn.functional.kl_div().sum", "modeling_sas.SasForSequenceClassification.view", "labels.view", "modeling_sas.SasForSequenceClassification.view", "labels.view", "torch.nn.MSELoss.", "torch.nn.MSELoss.", "torch.nn.MSELoss.", "logits_0.view", "logits_0.view", "logits_1.view", "logits_1.view", "logits_0.view", "logits_1.view", "torch.nn.functional.kl_div", "torch.nn.functional.kl_div", "torch.nn.functional.kl_div", "torch.nn.functional.kl_div", "torch.nn.functional.kl_div", "torch.nn.functional.kl_div", "torch.nn.functional.kl_div", "torch.nn.functional.kl_div", "torch.nn.functional.kl_div", "torch.nn.functional.kl_div", "torch.nn.functional.kl_div", "torch.nn.functional.kl_div", "torch.nn.functional.kl_div", "torch.nn.functional.kl_div", "torch.nn.functional.kl_div", "torch.nn.functional.kl_div", "torch.nn.functional.kl_div", "torch.nn.functional.kl_div"], "methods", ["None"], ["", "def", "sas_kl_within_batch", "(", "self", ",", "input_ids", ",", "attention_mask", ",", "token_type_ids", ",", "\n", "position_ids", ",", "inputs_embeds", ",", "labels", ",", "output_attentions", ",", "output_hidden_states", ",", "return_dict", ")", ":", "\n", "        ", "batch_size", "=", "input_ids", ".", "size", "(", "0", ")", "\n", "outputs", "=", "self", ".", "sas", "(", "\n", "input_ids", ",", "\n", "token_type_ids", "=", "token_type_ids", ",", "\n", "attention_mask", "=", "attention_mask", ",", "\n", "position_ids", "=", "position_ids", ",", "\n", "inputs_embeds", "=", "inputs_embeds", ",", "\n", "output_attentions", "=", "output_attentions", ",", "\n", "output_hidden_states", "=", "output_hidden_states", ",", "\n", "return_dict", "=", "return_dict", "\n", ")", "\n", "sequence_output", "=", "outputs", "[", "0", "]", "\n", "logits", "=", "self", ".", "classifier", "(", "sequence_output", ")", "\n", "\n", "loss", "=", "None", "\n", "if", "labels", "is", "not", "None", ":", "\n", "            ", "if", "self", ".", "num_labels", "==", "1", ":", "\n", "                ", "loss_fn", "=", "torch", ".", "nn", ".", "MSELoss", "(", ")", "\n", "loss", "=", "loss_fn", "(", "logits", ".", "view", "(", "-", "1", ")", ",", "labels", ".", "view", "(", "-", "1", ")", ")", "\n", "", "else", ":", "\n", "                ", "loss_fn", "=", "CrossEntropyLoss", "(", ")", "\n", "loss", "=", "loss_fn", "(", "logits", ".", "view", "(", "-", "1", ",", "self", ".", "num_labels", ")", ",", "labels", ".", "view", "(", "-", "1", ")", ")", "\n", "", "loss", "=", "2", "*", "loss", "# to match the scale in rdrop", "\n", "\n", "", "logits_0", ",", "logits_1", "=", "logits", "[", ":", "batch_size", "//", "2", "]", ",", "logits", "[", "batch_size", "//", "2", ":", "]", "\n", "if", "loss", "is", "not", "None", ":", "\n", "            ", "if", "self", ".", "num_labels", "==", "1", ":", "\n", "                ", "loss_fn", "=", "MSELoss", "(", ")", "\n", "rdrop_loss", "=", "self", ".", "config", ".", "rdrop_weight", "*", "loss_fn", "(", "logits_0", ".", "view", "(", "-", "1", ")", ",", "logits_1", ".", "view", "(", "-", "1", ")", ")", "\n", "loss", "+=", "rdrop_loss", "\n", "", "else", ":", "\n", "# use kl divergence loss", "\n", "                ", "p", "=", "torch", ".", "log_softmax", "(", "logits_0", ".", "view", "(", "-", "1", ",", "self", ".", "num_labels", ")", ",", "dim", "=", "-", "1", ")", "\n", "p_tec", "=", "torch", ".", "softmax", "(", "logits_0", ".", "view", "(", "-", "1", ",", "self", ".", "num_labels", ")", ",", "dim", "=", "-", "1", ")", "\n", "q", "=", "torch", ".", "log_softmax", "(", "logits_1", ".", "view", "(", "-", "1", ",", "self", ".", "num_labels", ")", ",", "dim", "=", "-", "1", ")", "\n", "q_tec", "=", "torch", ".", "softmax", "(", "logits_1", ".", "view", "(", "-", "1", ",", "self", ".", "num_labels", ")", ",", "dim", "=", "-", "1", ")", "\n", "\n", "kl_loss", "=", "torch", ".", "nn", ".", "functional", ".", "kl_div", "(", "p", ",", "q_tec", ",", "reduction", "=", "'none'", ")", ".", "sum", "(", ")", "\n", "reverse_kl_loss", "=", "torch", ".", "nn", ".", "functional", ".", "kl_div", "(", "q", ",", "p_tec", ",", "reduction", "=", "'none'", ")", ".", "sum", "(", ")", "\n", "\n", "rdrop_loss", "=", "self", ".", "config", ".", "rdrop_weight", "*", "(", "kl_loss", "+", "reverse_kl_loss", ")", "/", "2.0", "\n", "loss", "+=", "rdrop_loss", "\n", "\n", "# outputs may cause size mis-match", "\n", "", "", "return", "loss", ",", "logits_0", ",", "outputs", "\n", "\n"]], "home.repos.pwc.inspect_result.alibaba_self-augmentation-strategy.models.modeling_sas.SasForSequenceClassification.sas_kl": [[1204, 1337], ["range", "modeling_sas.SasForSequenceClassification.sas", "torch.nn.CrossEntropyLoss.item", "modeling_sas.SasForSequenceClassification.sim", "torch.arange().long().to", "torch.arange().long().to", "torch.arange().long().to", "torch.arange().long().to", "torch.arange().long().to", "torch.arange().long().to", "torch.arange().long().to", "torch.arange().long().to", "torch.arange().long().to", "cls_representation_embeddings_list.append", "modeling_sas.SasForSequenceClassification.classifier", "prediction_scores_list.append", "outputs_list.append", "torch.nn.MSELoss", "torch.nn.MSELoss", "torch.nn.MSELoss", "torch.log_softmax", "torch.log_softmax", "torch.log_softmax", "torch.log_softmax", "torch.log_softmax", "torch.log_softmax", "torch.log_softmax", "torch.log_softmax", "torch.log_softmax", "torch.softmax", "torch.softmax", "torch.softmax", "torch.softmax", "torch.softmax", "torch.softmax", "torch.softmax", "torch.softmax", "torch.softmax", "torch.log_softmax", "torch.log_softmax", "torch.log_softmax", "torch.log_softmax", "torch.log_softmax", "torch.log_softmax", "torch.log_softmax", "torch.log_softmax", "torch.log_softmax", "torch.softmax", "torch.softmax", "torch.softmax", "torch.softmax", "torch.softmax", "torch.softmax", "torch.softmax", "torch.softmax", "torch.softmax", "torch.distributed.is_initialized", "torch.distributed.is_initialized", "torch.distributed.is_initialized", "torch.distributed.is_initialized", "torch.distributed.is_initialized", "torch.distributed.is_initialized", "torch.distributed.is_initialized", "torch.distributed.is_initialized", "torch.distributed.is_initialized", "torch.distributed.all_gather", "torch.distributed.all_gather", "torch.distributed.all_gather", "torch.distributed.all_gather", "torch.distributed.all_gather", "torch.distributed.all_gather", "torch.distributed.all_gather", "torch.distributed.all_gather", "torch.distributed.all_gather", "torch.distributed.all_gather", "torch.distributed.all_gather", "torch.distributed.all_gather", "torch.distributed.all_gather", "torch.distributed.all_gather", "torch.distributed.all_gather", "torch.distributed.all_gather", "torch.distributed.all_gather", "torch.distributed.all_gather", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat.unsqueeze", "torch.cat.unsqueeze", "torch.cat.unsqueeze", "torch.cat.unsqueeze", "torch.cat.unsqueeze", "torch.cat.unsqueeze", "print", "torch.nn.CrossEntropyLoss.item", "torch.nn.MSELoss", "torch.nn.MSELoss", "torch.nn.MSELoss", "torch.nn.MSELoss", "torch.nn.MSELoss", "torch.nn.MSELoss", "torch.nn.MSELoss", "torch.nn.MSELoss", "torch.nn.MSELoss", "torch.nn.CrossEntropyLoss", "torch.nn.CrossEntropyLoss", "torch.nn.CrossEntropyLoss", "torch.nn.CrossEntropyLoss.", "torch.nn.CrossEntropyLoss.", "torch.nn.CrossEntropyLoss.", "prediction_scores_list[].view", "prediction_scores_list[].view", "prediction_scores_list[].view", "prediction_scores_list[].view", "torch.nn.functional.kl_div().sum", "torch.nn.functional.kl_div().sum", "torch.nn.functional.kl_div().sum", "torch.nn.functional.kl_div().sum", "torch.nn.functional.kl_div().sum", "torch.nn.functional.kl_div().sum", "torch.nn.functional.kl_div().sum", "torch.nn.functional.kl_div().sum", "torch.nn.functional.kl_div().sum", "torch.nn.functional.kl_div().sum", "torch.nn.functional.kl_div().sum", "torch.nn.functional.kl_div().sum", "torch.nn.functional.kl_div().sum", "torch.nn.functional.kl_div().sum", "torch.nn.functional.kl_div().sum", "torch.nn.functional.kl_div().sum", "torch.nn.functional.kl_div().sum", "torch.nn.functional.kl_div().sum", "numpy.quantile", "numpy.quantile", "torch.where", "torch.where", "torch.where", "torch.where", "torch.where", "torch.where", "torch.where", "torch.where", "torch.where", "torch.cat.sum", "torch.cat.sum", "torch.cat.sum", "torch.where", "torch.where", "torch.where", "torch.where", "torch.where", "torch.where", "torch.where", "torch.where", "torch.where", "torch.cat.sum", "torch.cat.sum", "torch.cat.sum", "torch.nn.functional.kl_div().sum", "torch.nn.functional.kl_div().sum", "torch.nn.functional.kl_div().sum", "torch.nn.functional.kl_div().sum", "torch.nn.functional.kl_div().sum", "torch.nn.functional.kl_div().sum", "torch.nn.functional.kl_div().sum", "torch.nn.functional.kl_div().sum", "torch.nn.functional.kl_div().sum", "torch.nn.functional.kl_div().sum", "torch.nn.functional.kl_div().sum", "torch.nn.functional.kl_div().sum", "torch.nn.functional.kl_div().sum", "torch.nn.functional.kl_div().sum", "torch.nn.functional.kl_div().sum", "torch.nn.functional.kl_div().sum", "torch.nn.functional.kl_div().sum", "torch.nn.functional.kl_div().sum", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "torch.arange().long", "torch.arange().long", "torch.arange().long", "torch.arange().long", "torch.arange().long", "torch.arange().long", "torch.arange().long", "torch.arange().long", "torch.arange().long", "torch.CrossEntropyLoss", "torch.CrossEntropyLoss", "torch.CrossEntropyLoss", "rdrop_loss.item", "contrast_loss.item", "torch.nn.CrossEntropyLoss.", "torch.nn.CrossEntropyLoss.", "torch.nn.CrossEntropyLoss.", "torch.nn.CrossEntropyLoss.", "torch.nn.CrossEntropyLoss.", "torch.nn.CrossEntropyLoss.", "torch.nn.CrossEntropyLoss.", "torch.nn.CrossEntropyLoss.", "torch.nn.CrossEntropyLoss.", "torch.nn.CrossEntropyLoss.", "torch.nn.CrossEntropyLoss.", "torch.nn.CrossEntropyLoss.", "prediction_scores_list[].view", "prediction_scores_list[].view", "torch.distributed.is_initialized", "torch.distributed.is_initialized", "torch.distributed.is_initialized", "torch.distributed.is_initialized", "torch.distributed.is_initialized", "torch.distributed.is_initialized", "torch.distributed.is_initialized", "torch.distributed.is_initialized", "torch.distributed.is_initialized", "torch.distributed.all_gather", "torch.distributed.all_gather", "torch.distributed.all_gather", "torch.distributed.all_gather", "torch.distributed.all_gather", "torch.distributed.all_gather", "torch.distributed.all_gather", "torch.distributed.all_gather", "torch.distributed.all_gather", "torch.distributed.all_gather", "torch.distributed.all_gather", "torch.distributed.all_gather", "torch.distributed.all_gather", "torch.distributed.all_gather", "torch.distributed.all_gather", "torch.distributed.all_gather", "torch.distributed.all_gather", "torch.distributed.all_gather", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat.detach().cpu().numpy", "torch.cat.detach().cpu().numpy", "torch.cat.detach().cpu().numpy", "torch.cat.detach().cpu().numpy", "torch.cat.detach().cpu().numpy", "torch.cat.detach().cpu().numpy", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "range", "range", "torch.cat.contiguous", "torch.cat.contiguous", "torch.cat.contiguous", "torch.cat.contiguous", "torch.cat.contiguous", "torch.cat.contiguous", "torch.distributed.get_rank", "torch.distributed.get_rank", "torch.distributed.get_rank", "torch.distributed.get_rank", "torch.distributed.get_rank", "torch.distributed.get_rank", "torch.distributed.get_rank", "torch.distributed.get_rank", "torch.distributed.get_rank", "torch.distributed.get_rank", "torch.distributed.get_rank", "torch.distributed.get_rank", "torch.distributed.get_rank", "torch.distributed.get_rank", "torch.distributed.get_rank", "torch.distributed.get_rank", "torch.distributed.get_rank", "torch.distributed.get_rank", "modeling_sas.SasForSequenceClassification.view", "torch.arange().long().to.view", "torch.arange().long().to.view", "torch.arange().long().to.view", "modeling_sas.SasForSequenceClassification.view", "torch.arange().long().to.view", "torch.arange().long().to.view", "torch.arange().long().to.view", "modeling_sas.SasForSequenceClassification.view", "torch.arange().long().to.view", "torch.arange().long().to.view", "torch.arange().long().to.view", "modeling_sas.SasForSequenceClassification.view", "torch.arange().long().to.view", "torch.arange().long().to.view", "torch.arange().long().to.view", "torch.nn.functional.kl_div", "torch.nn.functional.kl_div", "torch.nn.functional.kl_div", "torch.nn.functional.kl_div", "torch.nn.functional.kl_div", "torch.nn.functional.kl_div", "torch.nn.functional.kl_div", "torch.nn.functional.kl_div", "torch.nn.functional.kl_div", "torch.nn.functional.kl_div", "torch.nn.functional.kl_div", "torch.nn.functional.kl_div", "torch.nn.functional.kl_div", "torch.nn.functional.kl_div", "torch.nn.functional.kl_div", "torch.nn.functional.kl_div", "torch.nn.functional.kl_div", "torch.nn.functional.kl_div", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "torch.nn.functional.kl_div", "torch.nn.functional.kl_div", "torch.nn.functional.kl_div", "torch.nn.functional.kl_div", "torch.nn.functional.kl_div", "torch.nn.functional.kl_div", "torch.nn.functional.kl_div", "torch.nn.functional.kl_div", "torch.nn.functional.kl_div", "torch.nn.functional.kl_div", "torch.nn.functional.kl_div", "torch.nn.functional.kl_div", "torch.nn.functional.kl_div", "torch.nn.functional.kl_div", "torch.nn.functional.kl_div", "torch.nn.functional.kl_div", "torch.nn.functional.kl_div", "torch.nn.functional.kl_div", "torch.distributed.get_world_size", "torch.distributed.get_world_size", "torch.distributed.get_world_size", "torch.distributed.get_world_size", "torch.distributed.get_world_size", "torch.distributed.get_world_size", "torch.distributed.get_world_size", "torch.distributed.get_world_size", "torch.distributed.get_world_size", "torch.distributed.get_world_size", "torch.distributed.get_world_size", "torch.distributed.get_world_size", "torch.distributed.get_world_size", "torch.distributed.get_world_size", "torch.distributed.get_world_size", "torch.distributed.get_world_size", "torch.distributed.get_world_size", "torch.distributed.get_world_size", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.nn.CrossEntropyLoss.item", "range", "range", "torch.cat.contiguous", "torch.cat.contiguous", "torch.cat.contiguous", "torch.cat.contiguous", "torch.cat.contiguous", "torch.cat.contiguous", "torch.distributed.get_rank", "torch.distributed.get_rank", "torch.distributed.get_rank", "torch.distributed.get_rank", "torch.distributed.get_rank", "torch.distributed.get_rank", "torch.distributed.get_rank", "torch.distributed.get_rank", "torch.distributed.get_rank", "torch.distributed.get_rank", "torch.distributed.get_rank", "torch.distributed.get_rank", "torch.distributed.get_rank", "torch.distributed.get_rank", "torch.distributed.get_rank", "torch.distributed.get_rank", "torch.distributed.get_rank", "torch.distributed.get_rank", "torch.cat.detach().cpu", "torch.cat.detach().cpu", "torch.cat.detach().cpu", "torch.cat.detach().cpu", "torch.cat.detach().cpu", "torch.cat.detach().cpu", "modeling_sas.SasForSequenceClassification.size", "rdrop_loss.item", "contrast_loss.item", "torch.distributed.get_world_size", "torch.distributed.get_world_size", "torch.distributed.get_world_size", "torch.distributed.get_world_size", "torch.distributed.get_world_size", "torch.distributed.get_world_size", "torch.distributed.get_world_size", "torch.distributed.get_world_size", "torch.distributed.get_world_size", "torch.distributed.get_world_size", "torch.distributed.get_world_size", "torch.distributed.get_world_size", "torch.distributed.get_world_size", "torch.distributed.get_world_size", "torch.distributed.get_world_size", "torch.distributed.get_world_size", "torch.distributed.get_world_size", "torch.distributed.get_world_size", "torch.cat.detach", "torch.cat.detach", "torch.cat.detach", "torch.cat.detach", "torch.cat.detach", "torch.cat.detach"], "methods", ["None"], ["", "def", "sas_kl", "(", "self", ",", "input_ids", ",", "attention_mask", ",", "token_type_ids", ",", "position_ids", ",", "inputs_embeds", ",", "labels", ",", "output_attentions", ",", "output_hidden_states", ",", "\n", "return_dict", ")", ":", "\n", "        ", "prediction_scores_list", "=", "[", "]", "\n", "outputs_list", "=", "[", "]", "\n", "cls_representation_embeddings_list", "=", "[", "]", "\n", "for", "i", "in", "range", "(", "2", ")", ":", "\n", "            ", "outputs", "=", "self", ".", "sas", "(", "\n", "input_ids", ",", "\n", "token_type_ids", "=", "token_type_ids", ",", "\n", "attention_mask", "=", "attention_mask", ",", "\n", "position_ids", "=", "position_ids", ",", "\n", "inputs_embeds", "=", "inputs_embeds", ",", "\n", "output_attentions", "=", "output_attentions", ",", "\n", "output_hidden_states", "=", "output_hidden_states", ",", "\n", "return_dict", "=", "return_dict", "\n", ")", "\n", "sequence_output", "=", "outputs", "[", "0", "]", "\n", "if", "self", ".", "config", ".", "contrast_weight", ">", "0.0", ":", "\n", "                ", "cls_representation_embeddings_list", ".", "append", "(", "sequence_output", "[", ":", ",", "0", ",", ":", "]", ")", "\n", "\n", "", "if", "self", ".", "config", ".", "rdrop_weight", ">", "0.0", "or", "(", "self", ".", "config", ".", "rdrop_weight", "==", "0", "and", "i", "==", "0", ")", ":", "\n", "                ", "logits", "=", "self", ".", "classifier", "(", "sequence_output", ")", "\n", "prediction_scores_list", ".", "append", "(", "logits", ")", "\n", "outputs_list", ".", "append", "(", "outputs", ")", "\n", "\n", "", "", "loss", "=", "None", "\n", "for", "logits", "in", "prediction_scores_list", ":", "\n", "            ", "if", "labels", "is", "not", "None", ":", "\n", "                ", "if", "self", ".", "num_labels", "==", "1", ":", "\n", "                    ", "loss_fn", "=", "torch", ".", "nn", ".", "MSELoss", "(", ")", "\n", "if", "loss", ":", "\n", "                        ", "loss", "+=", "loss_fn", "(", "logits", ".", "view", "(", "-", "1", ")", ",", "labels", ".", "view", "(", "-", "1", ")", ")", "\n", "", "else", ":", "\n", "                        ", "loss", "=", "loss_fn", "(", "logits", ".", "view", "(", "-", "1", ")", ",", "labels", ".", "view", "(", "-", "1", ")", ")", "\n", "", "", "else", ":", "\n", "                    ", "loss_fn", "=", "CrossEntropyLoss", "(", ")", "\n", "if", "loss", ":", "\n", "                        ", "loss", "+=", "loss_fn", "(", "logits", ".", "view", "(", "-", "1", ",", "self", ".", "num_labels", ")", ",", "labels", ".", "view", "(", "-", "1", ")", ")", "\n", "", "else", ":", "\n", "                        ", "loss", "=", "loss_fn", "(", "logits", ".", "view", "(", "-", "1", ",", "self", ".", "num_labels", ")", ",", "labels", ".", "view", "(", "-", "1", ")", ")", "\n", "\n", "", "", "", "", "if", "loss", "is", "not", "None", ":", "\n", "            ", "cls_loss", "=", "loss", ".", "item", "(", ")", "\n", "", "if", "loss", "is", "not", "None", "and", "self", ".", "config", ".", "rdrop_weight", ">", "0", ":", "\n", "            ", "if", "self", ".", "num_labels", "==", "1", ":", "\n", "                ", "loss_fn", "=", "MSELoss", "(", ")", "\n", "rdrop_loss", "=", "self", ".", "config", ".", "rdrop_weight", "*", "loss_fn", "(", "prediction_scores_list", "[", "0", "]", ".", "view", "(", "-", "1", ")", ",", "prediction_scores_list", "[", "-", "1", "]", ".", "view", "(", "-", "1", ")", ")", "\n", "loss", "+=", "rdrop_loss", "\n", "\n", "", "else", ":", "\n", "# use kl divergence loss", "\n", "                ", "p", "=", "torch", ".", "log_softmax", "(", "prediction_scores_list", "[", "0", "]", ".", "view", "(", "-", "1", ",", "self", ".", "num_labels", ")", ",", "dim", "=", "-", "1", ")", "\n", "p_tec", "=", "torch", ".", "softmax", "(", "prediction_scores_list", "[", "0", "]", ".", "view", "(", "-", "1", ",", "self", ".", "num_labels", ")", ",", "dim", "=", "-", "1", ")", "\n", "q", "=", "torch", ".", "log_softmax", "(", "prediction_scores_list", "[", "-", "1", "]", ".", "view", "(", "-", "1", ",", "self", ".", "num_labels", ")", ",", "dim", "=", "-", "1", ")", "\n", "q_tec", "=", "torch", ".", "softmax", "(", "prediction_scores_list", "[", "-", "1", "]", ".", "view", "(", "-", "1", ",", "self", ".", "num_labels", ")", ",", "dim", "=", "-", "1", ")", "\n", "\n", "if", "self", ".", "config", ".", "filter_outliers", ":", "\n", "                    ", "kl_loss", "=", "torch", ".", "nn", ".", "functional", ".", "kl_div", "(", "p", ",", "q_tec", ",", "reduction", "=", "'none'", ")", ".", "sum", "(", "1", ")", "\n", "reverse_kl_loss", "=", "torch", ".", "nn", ".", "functional", ".", "kl_div", "(", "q", ",", "p_tec", ",", "reduction", "=", "'none'", ")", ".", "sum", "(", "1", ")", "\n", "\n", "if", "torch", ".", "distributed", ".", "is_initialized", "(", ")", "and", "self", ".", "training", ":", "\n", "                        ", "kl_loss_list", "=", "[", "torch", ".", "zeros_like", "(", "kl_loss", ")", "for", "_", "in", "range", "(", "torch", ".", "distributed", ".", "get_world_size", "(", ")", ")", "]", "\n", "reverse_kl_loss_list", "=", "[", "torch", ".", "zeros_like", "(", "reverse_kl_loss", ")", "for", "_", "in", "range", "(", "torch", ".", "distributed", ".", "get_world_size", "(", ")", ")", "]", "\n", "torch", ".", "distributed", ".", "all_gather", "(", "tensor_list", "=", "kl_loss_list", ",", "tensor", "=", "kl_loss", ".", "contiguous", "(", ")", ")", "\n", "torch", ".", "distributed", ".", "all_gather", "(", "tensor_list", "=", "reverse_kl_loss_list", ",", "tensor", "=", "reverse_kl_loss", ".", "contiguous", "(", ")", ")", "\n", "kl_loss_list", "[", "torch", ".", "distributed", ".", "get_rank", "(", ")", "]", "=", "kl_loss", "\n", "reverse_kl_loss_list", "[", "torch", ".", "distributed", ".", "get_rank", "(", ")", "]", "=", "reverse_kl_loss", "\n", "\n", "kl_loss", "=", "torch", ".", "cat", "(", "kl_loss_list", ",", "0", ")", "\n", "reverse_kl_loss", "=", "torch", ".", "cat", "(", "reverse_kl_loss_list", ",", "0", ")", "\n", "\n", "# filter outliers", "\n", "", "kl_range", "=", "np", ".", "quantile", "(", "kl_loss", ".", "detach", "(", ")", ".", "cpu", "(", ")", ".", "numpy", "(", ")", ",", "[", "0.25", ",", "0.75", "]", ")", "\n", "rv_kl_range", "=", "np", ".", "quantile", "(", "reverse_kl_loss", ".", "detach", "(", ")", ".", "cpu", "(", ")", ".", "numpy", "(", ")", ",", "[", "0.25", ",", "0.75", "]", ")", "\n", "\n", "kl_loss", "=", "torch", ".", "where", "(", "kl_loss", ">", "kl_range", "[", "0", "]", "-", "1.5", "*", "(", "kl_range", "[", "1", "]", "-", "kl_range", "[", "0", "]", ")", ",", "kl_loss", ",", "torch", ".", "tensor", "(", "[", "0.0", "]", ",", "device", "=", "loss", ".", "device", ")", ")", "\n", "false_positive_count", "=", "(", "kl_loss", "==", "0", ")", ".", "sum", "(", ")", ".", "item", "(", ")", "\n", "kl_loss", "=", "kl_loss", ".", "sum", "(", ")", "\n", "\n", "reverse_kl_loss", "=", "torch", ".", "where", "(", "reverse_kl_loss", ">", "rv_kl_range", "[", "0", "]", "-", "1.5", "*", "(", "rv_kl_range", "[", "1", "]", "-", "rv_kl_range", "[", "0", "]", ")", ",", "reverse_kl_loss", ",", "torch", ".", "tensor", "(", "[", "0.0", "]", ",", "device", "=", "loss", ".", "device", ")", ")", "\n", "false_positive_count_reverse", "=", "(", "reverse_kl_loss", "==", "0", ")", ".", "sum", "(", ")", ".", "item", "(", ")", "\n", "reverse_kl_loss", "=", "reverse_kl_loss", ".", "sum", "(", ")", "\n", "\n", "", "else", ":", "\n", "                    ", "kl_loss", "=", "torch", ".", "nn", ".", "functional", ".", "kl_div", "(", "p", ",", "q_tec", ",", "reduction", "=", "'none'", ")", ".", "sum", "(", ")", "\n", "reverse_kl_loss", "=", "torch", ".", "nn", ".", "functional", ".", "kl_div", "(", "q", ",", "p_tec", ",", "reduction", "=", "'none'", ")", ".", "sum", "(", ")", "\n", "\n", "", "rdrop_loss", "=", "self", ".", "config", ".", "rdrop_weight", "*", "(", "kl_loss", "+", "reverse_kl_loss", ")", "/", "2.0", "\n", "loss", "+=", "rdrop_loss", "\n", "\n", "", "", "if", "loss", "is", "not", "None", "and", "self", ".", "config", ".", "contrast_weight", ">", "0", ":", "\n", "            ", "z1", ",", "z2", "=", "cls_representation_embeddings_list", "[", "0", "]", ",", "cls_representation_embeddings_list", "[", "1", "]", "\n", "if", "self", ".", "config", ".", "sub_dim", "is", "not", "None", ":", "\n", "                ", "z1", ",", "z2", "=", "z1", "[", ":", ",", ":", "self", ".", "config", ".", "sub_dim", "]", ",", "z2", "[", ":", ",", ":", "self", ".", "config", ".", "sub_dim", "]", "\n", "", "if", "torch", ".", "distributed", ".", "is_initialized", "(", ")", "and", "self", ".", "training", ":", "\n", "                ", "z1_list", "=", "[", "torch", ".", "zeros_like", "(", "z1", ")", "for", "_", "in", "range", "(", "torch", ".", "distributed", ".", "get_world_size", "(", ")", ")", "]", "\n", "z2_list", "=", "[", "torch", ".", "zeros_like", "(", "z2", ")", "for", "_", "in", "range", "(", "torch", ".", "distributed", ".", "get_world_size", "(", ")", ")", "]", "\n", "torch", ".", "distributed", ".", "all_gather", "(", "tensor_list", "=", "z1_list", ",", "tensor", "=", "z1", ".", "contiguous", "(", ")", ")", "\n", "torch", ".", "distributed", ".", "all_gather", "(", "tensor_list", "=", "z2_list", ",", "tensor", "=", "z2", ".", "contiguous", "(", ")", ")", "\n", "\n", "z1_list", "[", "torch", ".", "distributed", ".", "get_rank", "(", ")", "]", "=", "z1", "\n", "z2_list", "[", "torch", ".", "distributed", ".", "get_rank", "(", ")", "]", "=", "z2", "\n", "\n", "z1", "=", "torch", ".", "cat", "(", "z1_list", ",", "0", ")", "\n", "z2", "=", "torch", ".", "cat", "(", "z2_list", ",", "0", ")", "\n", "\n", "", "cos_sim", "=", "self", ".", "sim", "(", "z1", ".", "unsqueeze", "(", "1", ")", ",", "z2", ".", "unsqueeze", "(", "0", ")", ")", "\n", "\n", "labels", "=", "torch", ".", "arange", "(", "cos_sim", ".", "size", "(", "0", ")", ")", ".", "long", "(", ")", ".", "to", "(", "self", ".", "device", ")", "\n", "\n", "contrast_loss", "=", "self", ".", "config", ".", "contrast_weight", "*", "nn", ".", "CrossEntropyLoss", "(", ")", "(", "cos_sim", ",", "labels", ")", "\n", "\n", "loss", "+=", "contrast_loss", "\n", "\n", "", "if", "self", ".", "training", ":", "\n", "            ", "global_step", "=", "self", ".", "trainer_callback", ".", "state", ".", "global_step", "\n", "if", "global_step", "%", "5", "==", "0", ":", "\n", "                ", "print", "(", "\"\\r step %d: total loss %.5f classification loss %.5f kl loss %.5f contrast loss %.5f\"", "%", "(", "\n", "global_step", ",", "\n", "loss", ".", "item", "(", ")", ",", "\n", "cls_loss", ",", "\n", "rdrop_loss", ".", "item", "(", ")", "if", "self", ".", "config", ".", "rdrop_weight", ">", "0", "else", "0", ",", "\n", "contrast_loss", ".", "item", "(", ")", "if", "self", ".", "config", ".", "contrast_weight", ">", "0", "else", "0", "\n", ")", ")", "\n", "self", ".", "inform", "[", "'global_step'", "]", "=", "global_step", "\n", "self", ".", "inform", "[", "'total_loss'", "]", "=", "loss", ".", "item", "(", ")", "\n", "self", ".", "inform", "[", "'classfication_loss'", "]", "=", "cls_loss", "\n", "self", ".", "inform", "[", "'rdrop_loss'", "]", "=", "rdrop_loss", ".", "item", "(", ")", "if", "self", ".", "config", ".", "rdrop_weight", ">", "0", "else", "0", "\n", "self", ".", "inform", "[", "'contrast_loss'", "]", "=", "contrast_loss", ".", "item", "(", ")", "if", "self", ".", "config", ".", "contrast_weight", ">", "0", "else", "0", "\n", "self", ".", "inform", "[", "'false_positive_count'", "]", "=", "false_positive_count", "if", "self", ".", "config", ".", "filter_outliers", ">", "0", "else", "0", "\n", "self", ".", "inform", "[", "'false_positive_count_reverse'", "]", "=", "false_positive_count_reverse", "if", "self", ".", "config", ".", "filter_outliers", ">", "0", "else", "0", "\n", "self", ".", "tensorboard_callback", ".", "log", "=", "self", ".", "inform", "\n", "", "", "return", "loss", ",", "prediction_scores_list", "[", "0", "]", ",", "outputs_list", "[", "0", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.alibaba_self-augmentation-strategy.models.modeling_sas.SasForSequenceClassification.sas_momentum_kl": [[1339, 1423], ["modeling_sas.SasForSequenceClassification.sas", "modeling_sas.SasForSequenceClassification.classifier", "prediction_scores_list.append", "outputs_list.append", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "modeling_sas.SasForSequenceClassification.get_model_ema", "modeling_sas.SasForSequenceClassification.sas_ema", "modeling_sas.SasForSequenceClassification.classifier_ema", "prediction_scores_list.append", "outputs_list.append", "torch.nn.CrossEntropyLoss.item", "torch.nn.MSELoss", "torch.nn.MSELoss", "torch.nn.MSELoss", "torch.log_softmax", "torch.log_softmax", "torch.log_softmax", "torch.log_softmax", "torch.log_softmax", "torch.log_softmax", "torch.log_softmax", "torch.log_softmax", "torch.log_softmax", "torch.softmax", "torch.softmax", "torch.softmax", "torch.softmax", "torch.softmax", "torch.softmax", "torch.softmax", "torch.softmax", "torch.softmax", "torch.log_softmax", "torch.log_softmax", "torch.log_softmax", "torch.log_softmax", "torch.log_softmax", "torch.log_softmax", "torch.log_softmax", "torch.log_softmax", "torch.log_softmax", "torch.softmax", "torch.softmax", "torch.softmax", "torch.softmax", "torch.softmax", "torch.softmax", "torch.softmax", "torch.softmax", "torch.softmax", "torch.nn.functional.kl_div().sum", "torch.nn.functional.kl_div().sum", "torch.nn.functional.kl_div().sum", "torch.nn.functional.kl_div().sum", "torch.nn.functional.kl_div().sum", "torch.nn.functional.kl_div().sum", "torch.nn.functional.kl_div().sum", "torch.nn.functional.kl_div().sum", "torch.nn.functional.kl_div().sum", "torch.nn.functional.kl_div().sum", "torch.nn.functional.kl_div().sum", "torch.nn.functional.kl_div().sum", "torch.nn.functional.kl_div().sum", "torch.nn.functional.kl_div().sum", "torch.nn.functional.kl_div().sum", "torch.nn.functional.kl_div().sum", "torch.nn.functional.kl_div().sum", "torch.nn.functional.kl_div().sum", "print", "torch.nn.MSELoss", "torch.nn.MSELoss", "torch.nn.MSELoss", "torch.nn.MSELoss", "torch.nn.MSELoss", "torch.nn.MSELoss", "torch.nn.MSELoss", "torch.nn.MSELoss", "torch.nn.MSELoss", "torch.nn.CrossEntropyLoss", "torch.nn.CrossEntropyLoss", "torch.nn.CrossEntropyLoss", "torch.nn.CrossEntropyLoss.", "torch.nn.CrossEntropyLoss.", "torch.nn.CrossEntropyLoss.", "prediction_scores_list[].view", "prediction_scores_list[].view", "prediction_scores_list[].view", "prediction_scores_list[].view", "torch.nn.CrossEntropyLoss.", "torch.nn.CrossEntropyLoss.", "torch.nn.CrossEntropyLoss.", "torch.nn.CrossEntropyLoss.", "torch.nn.CrossEntropyLoss.", "torch.nn.CrossEntropyLoss.", "torch.nn.CrossEntropyLoss.", "torch.nn.CrossEntropyLoss.", "torch.nn.CrossEntropyLoss.", "torch.nn.CrossEntropyLoss.", "torch.nn.CrossEntropyLoss.", "torch.nn.CrossEntropyLoss.", "prediction_scores_list[].view", "prediction_scores_list[].view", "torch.nn.functional.kl_div", "torch.nn.functional.kl_div", "torch.nn.functional.kl_div", "torch.nn.functional.kl_div", "torch.nn.functional.kl_div", "torch.nn.functional.kl_div", "torch.nn.functional.kl_div", "torch.nn.functional.kl_div", "torch.nn.functional.kl_div", "torch.nn.functional.kl_div", "torch.nn.functional.kl_div", "torch.nn.functional.kl_div", "torch.nn.functional.kl_div", "torch.nn.functional.kl_div", "torch.nn.functional.kl_div", "torch.nn.functional.kl_div", "torch.nn.functional.kl_div", "torch.nn.functional.kl_div", "modeling_sas.SasForSequenceClassification.view", "labels.view", "modeling_sas.SasForSequenceClassification.view", "labels.view", "modeling_sas.SasForSequenceClassification.view", "labels.view", "modeling_sas.SasForSequenceClassification.view", "labels.view", "torch.nn.CrossEntropyLoss.item", "rdrop_loss.item"], "methods", ["home.repos.pwc.inspect_result.alibaba_self-augmentation-strategy.models.modeling_sas.SasForSequenceClassification.get_model_ema"], ["", "def", "sas_momentum_kl", "(", "self", ",", "input_ids", ",", "attention_mask", ",", "token_type_ids", ",", "position_ids", ",", "inputs_embeds", ",", "labels", ",", "output_attentions", ",", "output_hidden_states", ",", "\n", "return_dict", ")", ":", "\n", "        ", "prediction_scores_list", "=", "[", "]", "\n", "outputs_list", "=", "[", "]", "\n", "outputs", "=", "self", ".", "sas", "(", "\n", "input_ids", ",", "\n", "token_type_ids", "=", "token_type_ids", ",", "\n", "attention_mask", "=", "attention_mask", ",", "\n", "position_ids", "=", "position_ids", ",", "\n", "inputs_embeds", "=", "inputs_embeds", ",", "\n", "output_attentions", "=", "output_attentions", ",", "\n", "output_hidden_states", "=", "output_hidden_states", ",", "\n", "return_dict", "=", "return_dict", "\n", ")", "\n", "sequence_output", "=", "outputs", "[", "0", "]", "\n", "logits", "=", "self", ".", "classifier", "(", "sequence_output", ")", "\n", "\n", "prediction_scores_list", ".", "append", "(", "logits", ")", "\n", "outputs_list", ".", "append", "(", "outputs", ")", "\n", "\n", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "            ", "self", ".", "get_model_ema", "(", ")", "\n", "outputs", "=", "self", ".", "sas_ema", "(", "\n", "input_ids", ",", "\n", "token_type_ids", "=", "token_type_ids", ",", "\n", "attention_mask", "=", "attention_mask", ",", "\n", "position_ids", "=", "position_ids", ",", "\n", "inputs_embeds", "=", "inputs_embeds", ",", "\n", "output_attentions", "=", "output_attentions", ",", "\n", "output_hidden_states", "=", "output_hidden_states", ",", "\n", "return_dict", "=", "return_dict", "\n", ")", "\n", "sequence_output", "=", "outputs", "[", "0", "]", "\n", "logits", "=", "self", ".", "classifier_ema", "(", "sequence_output", ")", "\n", "prediction_scores_list", ".", "append", "(", "logits", ")", "\n", "outputs_list", ".", "append", "(", "outputs", ")", "\n", "\n", "", "loss", "=", "None", "\n", "for", "logits", "in", "prediction_scores_list", ":", "\n", "            ", "if", "labels", "is", "not", "None", ":", "\n", "                ", "if", "self", ".", "num_labels", "==", "1", ":", "\n", "                    ", "loss_fn", "=", "torch", ".", "nn", ".", "MSELoss", "(", ")", "\n", "if", "loss", ":", "\n", "                        ", "loss", "+=", "loss_fn", "(", "logits", ".", "view", "(", "-", "1", ")", ",", "labels", ".", "view", "(", "-", "1", ")", ")", "\n", "", "else", ":", "\n", "                        ", "loss", "=", "loss_fn", "(", "logits", ".", "view", "(", "-", "1", ")", ",", "labels", ".", "view", "(", "-", "1", ")", ")", "\n", "", "", "else", ":", "\n", "                    ", "loss_fn", "=", "CrossEntropyLoss", "(", ")", "\n", "if", "loss", ":", "\n", "                        ", "loss", "+=", "loss_fn", "(", "logits", ".", "view", "(", "-", "1", ",", "self", ".", "num_labels", ")", ",", "labels", ".", "view", "(", "-", "1", ")", ")", "\n", "", "else", ":", "\n", "                        ", "loss", "=", "loss_fn", "(", "logits", ".", "view", "(", "-", "1", ",", "self", ".", "num_labels", ")", ",", "labels", ".", "view", "(", "-", "1", ")", ")", "\n", "\n", "", "", "", "", "if", "loss", "is", "not", "None", ":", "\n", "            ", "cls_loss", "=", "loss", ".", "item", "(", ")", "\n", "", "if", "loss", "is", "not", "None", "and", "self", ".", "config", ".", "rdrop_weight", ">", "0", ":", "\n", "            ", "if", "self", ".", "num_labels", "==", "1", ":", "\n", "                ", "loss_fn", "=", "MSELoss", "(", ")", "\n", "rdrop_loss", "=", "self", ".", "config", ".", "rdrop_weight", "*", "loss_fn", "(", "prediction_scores_list", "[", "0", "]", ".", "view", "(", "-", "1", ")", ",", "prediction_scores_list", "[", "-", "1", "]", ".", "view", "(", "-", "1", ")", ")", "\n", "loss", "+=", "rdrop_loss", "\n", "\n", "", "else", ":", "\n", "# use kl divergence loss", "\n", "                ", "p", "=", "torch", ".", "log_softmax", "(", "prediction_scores_list", "[", "0", "]", ".", "view", "(", "-", "1", ",", "self", ".", "num_labels", ")", ",", "dim", "=", "-", "1", ")", "\n", "p_tec", "=", "torch", ".", "softmax", "(", "prediction_scores_list", "[", "0", "]", ".", "view", "(", "-", "1", ",", "self", ".", "num_labels", ")", ",", "dim", "=", "-", "1", ")", "\n", "q", "=", "torch", ".", "log_softmax", "(", "prediction_scores_list", "[", "-", "1", "]", ".", "view", "(", "-", "1", ",", "self", ".", "num_labels", ")", ",", "dim", "=", "-", "1", ")", "\n", "q_tec", "=", "torch", ".", "softmax", "(", "prediction_scores_list", "[", "-", "1", "]", ".", "view", "(", "-", "1", ",", "self", ".", "num_labels", ")", ",", "dim", "=", "-", "1", ")", "\n", "\n", "kl_loss", "=", "torch", ".", "nn", ".", "functional", ".", "kl_div", "(", "p", ",", "q_tec", ",", "reduction", "=", "'none'", ")", ".", "sum", "(", ")", "\n", "reverse_kl_loss", "=", "torch", ".", "nn", ".", "functional", ".", "kl_div", "(", "q", ",", "p_tec", ",", "reduction", "=", "'none'", ")", ".", "sum", "(", ")", "\n", "\n", "rdrop_loss", "=", "self", ".", "config", ".", "rdrop_weight", "*", "(", "kl_loss", "+", "reverse_kl_loss", ")", "/", "2.0", "\n", "loss", "+=", "rdrop_loss", "\n", "\n", "", "", "if", "self", ".", "training", ":", "\n", "            ", "global_step", "=", "self", ".", "trainer_callback", ".", "state", ".", "global_step", "\n", "if", "global_step", "%", "10", "==", "0", ":", "\n", "                ", "print", "(", "\"\\r step %d: total loss %.5f classification loss %.5f kl loss %.5f\"", "%", "(", "\n", "global_step", ",", "\n", "loss", ".", "item", "(", ")", ",", "\n", "cls_loss", ",", "\n", "rdrop_loss", ".", "item", "(", ")", "if", "self", ".", "config", ".", "rdrop_weight", ">", "0", "else", "0", ",", "\n", ")", ")", "\n", "", "", "return", "loss", ",", "prediction_scores_list", "[", "0", "]", ",", "outputs_list", "[", "0", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.alibaba_self-augmentation-strategy.models.modeling_sas.SasForRTD.__init__": [[1437, 1443], ["transformers.modeling_utils.PreTrainedModel.__init__", "modeling_sas.SasModel", "modeling_sas.SasDiscriminatorPredictions", "modeling_sas.SasForRTD.init_weights"], "methods", ["home.repos.pwc.inspect_result.alibaba_self-augmentation-strategy.models.trainer_sas.SequentialDistributedSampler.__init__"], ["    ", "def", "__init__", "(", "self", ",", "config", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "config", ")", "\n", "\n", "self", ".", "sas", "=", "SasModel", "(", "config", ")", "\n", "self", ".", "discriminator_predictions", "=", "SasDiscriminatorPredictions", "(", "config", ")", "\n", "self", ".", "init_weights", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.alibaba_self-augmentation-strategy.models.modeling_sas.SasForRTD.forward": [[1444, 1519], ["transformers.file_utils.add_start_docstrings_to_model_forward", "transformers.file_utils.replace_return_docstrings", "dict", "modeling_sas.SasForRTD.sas", "modeling_sas.SasForRTD.discriminator_predictions", "modeling_sas.SasForRTDOutput", "SAS_INPUTS_DOCSTRING.format", "torch.BCEWithLogitsLoss", "torch.BCEWithLogitsLoss", "torch.BCEWithLogitsLoss", "torch.BCEWithLogitsLoss.", "torch.BCEWithLogitsLoss.", "attention_mask.view", "modeling_sas.SasForRTD.view", "active_labels.float", "modeling_sas.SasForRTD.view", "labels.float"], "methods", ["None"], ["", "@", "add_start_docstrings_to_model_forward", "(", "SAS_INPUTS_DOCSTRING", ".", "format", "(", "\"batch_size, sequence_length\"", ")", ")", "\n", "@", "replace_return_docstrings", "(", "output_type", "=", "SasForRTDOutput", ",", "config_class", "=", "_CONFIG_FOR_DOC", ")", "\n", "def", "forward", "(", "\n", "self", ",", "\n", "input_ids", "=", "None", ",", "\n", "attention_mask", "=", "None", ",", "\n", "token_type_ids", "=", "None", ",", "\n", "position_ids", "=", "None", ",", "\n", "head_mask", "=", "None", ",", "\n", "inputs_embeds", "=", "None", ",", "\n", "labels", "=", "None", ",", "\n", "output_attentions", "=", "None", ",", "\n", "output_hidden_states", "=", "None", ",", "\n", "side_info_sets", "=", "dict", "(", ")", ",", "\n", "return_dict", "=", "None", ",", "\n", ")", ":", "\n", "        ", "r\"\"\"\n        labels (``torch.LongTensor`` of shape ``(batch_size, sequence_length)``, `optional`):\n            Labels for computing the SAS loss. Input should be a sequence of tokens (see :obj:`input_ids`\n            docstring) Indices should be in ``[0, 1]``:\n\n            - 0 indicates the token is an original token,\n            - 1 indicates the token was replaced.\n\n        Returns:\n\n        Examples::\n\n            >>> from transformers import SasTokenizer, SasForRTD\n            >>> import torch\n\n            >>> tokenizer = SasTokenizer.from_pretrained('google/sas-small-discriminator')\n            >>> model = SasForRTD.from_pretrained('google/sas-small-discriminator')\n\n            >>> input_ids = torch.tensor(tokenizer.encode(\"Hello, my dog is cute\", add_special_tokens=True)).unsqueeze(0)  # Batch size 1\n            >>> logits = model(input_ids).logits\n        \"\"\"", "\n", "return_dict", "=", "return_dict", "if", "return_dict", "is", "not", "None", "else", "self", ".", "config", ".", "use_return_dict", "\n", "\n", "discriminator_hidden_states", "=", "self", ".", "sas", "(", "\n", "input_ids", ",", "\n", "attention_mask", ",", "\n", "token_type_ids", ",", "\n", "position_ids", ",", "\n", "head_mask", ",", "\n", "inputs_embeds", ",", "\n", "output_attentions", ",", "\n", "output_hidden_states", ",", "\n", "side_info_sets", ",", "\n", "return_dict", ",", "\n", ")", "\n", "discriminator_sequence_output", "=", "discriminator_hidden_states", "[", "0", "]", "\n", "\n", "logits", "=", "self", ".", "discriminator_predictions", "(", "discriminator_sequence_output", ")", "\n", "\n", "loss", "=", "None", "\n", "if", "labels", "is", "not", "None", ":", "\n", "            ", "loss_fct", "=", "nn", ".", "BCEWithLogitsLoss", "(", ")", "\n", "if", "attention_mask", "is", "not", "None", ":", "\n", "                ", "active_loss", "=", "attention_mask", ".", "view", "(", "-", "1", ",", "discriminator_sequence_output", ".", "shape", "[", "1", "]", ")", "==", "1", "\n", "active_logits", "=", "logits", ".", "view", "(", "-", "1", ",", "discriminator_sequence_output", ".", "shape", "[", "1", "]", ")", "[", "active_loss", "]", "\n", "active_labels", "=", "labels", "[", "active_loss", "]", "\n", "loss", "=", "loss_fct", "(", "active_logits", ",", "active_labels", ".", "float", "(", ")", ")", "\n", "", "else", ":", "\n", "                ", "loss", "=", "loss_fct", "(", "logits", ".", "view", "(", "-", "1", ",", "discriminator_sequence_output", ".", "shape", "[", "1", "]", ")", ",", "labels", ".", "float", "(", ")", ")", "\n", "\n", "", "", "if", "not", "return_dict", ":", "\n", "            ", "output", "=", "(", "logits", ",", ")", "+", "discriminator_hidden_states", "[", "1", ":", "]", "\n", "return", "(", "(", "loss", ",", ")", "+", "output", ")", "if", "loss", "is", "not", "None", "else", "output", "\n", "\n", "", "return", "SasForRTDOutput", "(", "\n", "loss", "=", "loss", ",", "\n", "logits", "=", "logits", ",", "\n", "hidden_states", "=", "discriminator_hidden_states", ".", "hidden_states", ",", "\n", "attentions", "=", "discriminator_hidden_states", ".", "attentions", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.alibaba_self-augmentation-strategy.models.modeling_sas.SasForPreTraining.__init__": [[1530, 1567], ["transformers.modeling_utils.PreTrainedModel.__init__", "modeling_sas.SasModel", "modeling_sas.SasGeneratorPredictions", "torch.Linear", "torch.Linear", "torch.Linear", "modeling_sas.SasDiscriminatorPredictions", "modeling_sas.SasForPreTraining.init_weights", "dict", "dict", "time.time", "dict", "trainer_callback_sas.SasTrainerCallback", "trainer_callback_sas.SasWandbCallback", "trainer_callback_sas.SasTensorBoardCallback"], "methods", ["home.repos.pwc.inspect_result.alibaba_self-augmentation-strategy.models.trainer_sas.SequentialDistributedSampler.__init__"], ["    ", "def", "__init__", "(", "self", ",", "config", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "config", ")", "\n", "\n", "self", ".", "sas", "=", "SasModel", "(", "config", ")", "\n", "self", ".", "generator_predictions", "=", "SasGeneratorPredictions", "(", "config", ")", "\n", "self", ".", "generator_lm_head", "=", "nn", ".", "Linear", "(", "config", ".", "embedding_size", ",", "config", ".", "vocab_size", ")", "\n", "\n", "self", ".", "discriminator_predictions", "=", "SasDiscriminatorPredictions", "(", "config", ")", "\n", "self", ".", "init_weights", "(", ")", "\n", "\n", "self", ".", "config", "=", "config", "\n", "self", ".", "global_step", "=", "0", "\n", "self", ".", "max_steps", "=", "None", "\n", "self", ".", "epoch", "=", "0", "\n", "self", ".", "max_epochs_int", "=", "None", "\n", "self", ".", "dataset", "=", "None", "\n", "self", ".", "metrics", "=", "dict", "(", ")", "\n", "self", ".", "inform", "=", "dict", "(", ")", "\n", "\n", "# Saved for self-augmentation", "\n", "self", ".", "mlm_positions", "=", "None", "\n", "self", ".", "mlm_labels", "=", "None", "\n", "self", ".", "mlm_logits", "=", "None", "\n", "\n", "self", ".", "gen_weight", "=", "1", "\n", "self", ".", "dis_weight", "=", "1", "\n", "\n", "self", ".", "flag_debugExtraMetrics", "=", "False", "\n", "self", ".", "time", "=", "time", ".", "time", "(", ")", "\n", "self", ".", "durations", "=", "dict", "(", ")", "\n", "self", ".", "max_memory_allocated", "=", "0", "\n", "self", ".", "memory_allocated", "=", "0", "\n", "\n", "\n", "self", ".", "trainer_callback", "=", "SasTrainerCallback", "(", ")", "\n", "self", ".", "wandb_callback", "=", "SasWandbCallback", "(", ")", "\n", "self", ".", "tensorboard_callback", "=", "SasTensorBoardCallback", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.alibaba_self-augmentation-strategy.models.modeling_sas.SasForPreTraining.get_output_embeddings": [[1568, 1570], ["None"], "methods", ["None"], ["", "def", "get_output_embeddings", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "generator_lm_head", "\n", "\n"]], "home.repos.pwc.inspect_result.alibaba_self-augmentation-strategy.models.modeling_sas.SasForPreTraining.set_output_embeddings": [[1571, 1573], ["None"], "methods", ["None"], ["", "def", "set_output_embeddings", "(", "self", ",", "word_embeddings", ")", ":", "\n", "        ", "self", ".", "generator_lm_head", "=", "word_embeddings", "\n", "\n"]], "home.repos.pwc.inspect_result.alibaba_self-augmentation-strategy.models.modeling_sas.SasForPreTraining.track_compute_statistics": [[1576, 1617], ["time.time", "torch.cuda.is_available", "torch.cuda.is_available", "torch.cuda.is_available", "torch.cuda.is_available", "torch.cuda.is_available", "torch.cuda.is_available", "torch.cuda.is_available", "torch.cuda.is_available", "torch.cuda.is_available", "torch.cuda.reset_peak_memory_stats", "torch.cuda.reset_peak_memory_stats", "torch.cuda.reset_peak_memory_stats", "torch.cuda.reset_peak_memory_stats", "torch.cuda.reset_peak_memory_stats", "torch.cuda.reset_peak_memory_stats", "torch.cuda.reset_peak_memory_stats", "torch.cuda.reset_peak_memory_stats", "torch.cuda.reset_peak_memory_stats", "time.time", "print", "torch.tensor().float().item", "torch.tensor().float().item", "torch.tensor().float().item", "torch.tensor().float().item", "torch.tensor().float().item", "torch.tensor().float().item", "torch.tensor().float().item", "torch.tensor().float().item", "torch.tensor().float().item", "torch.tensor().float().item", "torch.tensor().float().item", "torch.tensor().float().item", "torch.tensor().float().item", "torch.tensor().float().item", "torch.tensor().float().item", "torch.tensor().float().item", "torch.tensor().float().item", "torch.tensor().float().item", "torch.tensor().float().item", "torch.tensor().float().item", "torch.tensor().float().item", "torch.tensor().float().item", "torch.tensor().float().item", "torch.tensor().float().item", "torch.tensor().float().item", "torch.tensor().float().item", "torch.tensor().float().item", "torch.tensor().float().item", "torch.tensor().float().item", "torch.tensor().float().item", "torch.tensor().float().item", "torch.tensor().float().item", "torch.tensor().float().item", "torch.tensor().float().item", "torch.tensor().float().item", "torch.tensor().float().item", "torch.tensor().float", "torch.tensor().float", "torch.tensor().float", "torch.tensor().float", "torch.tensor().float", "torch.tensor().float", "torch.tensor().float", "torch.tensor().float", "torch.tensor().float", "torch.tensor().float", "torch.tensor().float", "torch.tensor().float", "torch.tensor().float", "torch.tensor().float", "torch.tensor().float", "torch.tensor().float", "torch.tensor().float", "torch.tensor().float", "torch.tensor().float", "torch.tensor().float", "torch.tensor().float", "torch.tensor().float", "torch.tensor().float", "torch.tensor().float", "torch.tensor().float", "torch.tensor().float", "torch.tensor().float", "torch.tensor().float", "torch.tensor().float", "torch.tensor().float", "torch.tensor().float", "torch.tensor().float", "torch.tensor().float", "torch.tensor().float", "torch.tensor().float", "torch.tensor().float", "name.ljust", "datetime.datetime.now", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.cuda.max_memory_reserved", "torch.cuda.max_memory_reserved", "torch.cuda.max_memory_reserved", "torch.cuda.max_memory_reserved", "torch.cuda.max_memory_reserved", "torch.cuda.max_memory_reserved", "torch.cuda.max_memory_reserved", "torch.cuda.max_memory_reserved", "torch.cuda.max_memory_reserved", "torch.cuda.max_memory_cached", "torch.cuda.max_memory_cached", "torch.cuda.max_memory_cached", "torch.cuda.max_memory_cached", "torch.cuda.max_memory_cached", "torch.cuda.max_memory_cached", "torch.cuda.max_memory_cached", "torch.cuda.max_memory_cached", "torch.cuda.max_memory_cached", "torch.cuda.max_memory_allocated", "torch.cuda.max_memory_allocated", "torch.cuda.max_memory_allocated", "torch.cuda.max_memory_allocated", "torch.cuda.max_memory_allocated", "torch.cuda.max_memory_allocated", "torch.cuda.max_memory_allocated", "torch.cuda.max_memory_allocated", "torch.cuda.max_memory_allocated", "torch.cuda.memory_allocated", "torch.cuda.memory_allocated", "torch.cuda.memory_allocated", "torch.cuda.memory_allocated", "torch.cuda.memory_allocated", "torch.cuda.memory_allocated", "torch.cuda.memory_allocated", "torch.cuda.memory_allocated", "torch.cuda.memory_allocated"], "methods", ["None"], ["", "def", "track_compute_statistics", "(", "self", ",", "mod", ",", "inp", ",", "out", ",", "name", ":", "Optional", "[", "str", "]", "=", "'Unknown'", ")", ":", "\n", "        ", "if", "not", "self", ".", "training", ":", "\n", "            ", "return", "\n", "\n", "", "current_time", "=", "time", ".", "time", "(", ")", "\n", "# Note: use comma \",\" to seperate metrics during printing, in order to copy-paste log to Excel for easier analysis", "\n", "# In Excel, try \"Data, then Text to Columns\" to seperate fields by comma", "\n", "\n", "if", "torch", ".", "cuda", ".", "is_available", "(", ")", "and", "(", "(", "self", ".", "global_step", "+", "1", ")", "%", "self", ".", "config", ".", "debug_config", "[", "'debugMemStatsInterval'", "]", "in", "[", "0", ",", "1", "]", "or", "self", ".", "global_step", "==", "10", ")", ":", "\n", "\n", "            ", "self", ".", "durations", "[", "name", "]", "=", "(", "current_time", "-", "self", ".", "time", ")", "\n", "self", ".", "inform", "[", "\"gpu/max_memory_reserved_%s\"", "%", "(", "name", ")", "]", "=", "torch", ".", "tensor", "(", "torch", ".", "cuda", ".", "max_memory_reserved", "(", ")", ")", ".", "float", "(", ")", ".", "item", "(", ")", "/", "1e6", "\n", "self", ".", "inform", "[", "\"gpu/max_memory_cached_%s\"", "%", "(", "name", ")", "]", "=", "torch", ".", "tensor", "(", "torch", ".", "cuda", ".", "max_memory_cached", "(", ")", ")", ".", "float", "(", ")", ".", "item", "(", ")", "/", "1e6", "\n", "self", ".", "inform", "[", "\"gpu/max_memory_allocated_%s\"", "%", "(", "name", ")", "]", "=", "torch", ".", "tensor", "(", "torch", ".", "cuda", ".", "max_memory_allocated", "(", ")", ")", ".", "float", "(", ")", ".", "item", "(", ")", "/", "1e6", "\n", "self", ".", "inform", "[", "\"gpu/memory_allocated_%s\"", "%", "(", "name", ")", "]", "=", "torch", ".", "tensor", "(", "torch", ".", "cuda", ".", "memory_allocated", "(", ")", ")", ".", "float", "(", ")", ".", "item", "(", ")", "/", "1e6", "\n", "torch", ".", "cuda", ".", "reset_peak_memory_stats", "(", ")", "\n", "\n", "printStr", "=", "'CUDA Allocated (M), %04d, %04d, %04d, Change, %03.2f, %03.2f'", "%", "(", "\n", "self", ".", "memory_allocated", ",", "\n", "self", ".", "inform", "[", "\"gpu/max_memory_allocated_%s\"", "%", "(", "name", ")", "]", ",", "\n", "self", ".", "inform", "[", "\"gpu/memory_allocated_%s\"", "%", "(", "name", ")", "]", ",", "\n", "self", ".", "inform", "[", "\"gpu/max_memory_allocated_%s\"", "%", "(", "name", ")", "]", "-", "self", ".", "memory_allocated", ",", "\n", "self", ".", "inform", "[", "\"gpu/memory_allocated_%s\"", "%", "(", "name", ")", "]", "-", "self", ".", "inform", "[", "\"gpu/max_memory_allocated_%s\"", "%", "(", "name", ")", "]", "\n", ")", "\n", "\n", "self", ".", "max_memory_allocated", "=", "self", ".", "inform", "[", "\"gpu/max_memory_allocated_%s\"", "%", "(", "name", ")", "]", "\n", "self", ".", "memory_allocated", "=", "self", ".", "inform", "[", "\"gpu/memory_allocated_%s\"", "%", "(", "name", ")", "]", "\n", "\n", "new_time", "=", "time", ".", "time", "(", ")", "\n", "print", "(", "'%sE%d.%03d, track_compute_statistics, %s, %s, \\t@ Took, %.4f, sec, %s'", "%", "(", "\n", "\"\\n\"", "if", "'Fwd,Pre,Other'", "in", "name", "else", "\"\"", ",", "\n", "self", ".", "epoch", ",", "self", ".", "global_step", ",", "\n", "name", ".", "ljust", "(", "40", ",", "' '", ")", ",", "printStr", ",", "\n", "new_time", "-", "current_time", ",", "datetime", ".", "datetime", ".", "now", "(", ")", ")", "\n", ")", "\n", "\n", "self", ".", "time", "=", "new_time", "\n", "\n", "", "else", ":", "\n", "            ", "self", ".", "time", "=", "current_time", "\n", "\n"]], "home.repos.pwc.inspect_result.alibaba_self-augmentation-strategy.models.modeling_sas.SasForPreTraining.track_extra_algorithm_metrics": [[1619, 1682], ["torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "modeling_sas.SasForPreTraining.metrics[].item", "print", "torch.nn.Softmax", "torch.nn.Softmax", "torch.nn.Softmax", "torch.nn.Softmax", "torch.nn.Softmax", "torch.nn.Softmax", "torch.nn.Softmax", "torch.nn.Softmax", "torch.nn.Softmax", "torch.nn.Softmax.", "torch.nn.Softmax.", "torch.nn.Softmax.", "torch.topk", "torch.topk", "torch.topk", "torch.topk", "torch.topk", "torch.topk", "torch.topk", "torch.topk", "torch.topk", "hit_top1.float().mean", "fake_top_label_prob[].mean", "hit_top2to10.sum().float().mean", "fake_top_label_prob[].sum().mean", "modeling_sas.SasForPreTraining.mlm_positions.numel", "modeling_sas.SasForPreTraining.mlm_positions.sum().item", "modeling_sas.SasForPreTraining.mlm_positions.numel", "modeling_sas.SasForPreTraining.mlm_labels.unsqueeze", "torch.round().int", "torch.round().int", "torch.round().int", "torch.round().int", "torch.round().int", "torch.round().int", "torch.round().int", "torch.round().int", "torch.round().int", "neg_positions.float().mean", "torch.logical_and().sum().item", "torch.logical_and().sum().item", "torch.logical_and().sum().item", "torch.logical_and().sum().item", "torch.logical_and().sum().item", "torch.logical_and().sum().item", "torch.logical_and().sum().item", "torch.logical_and().sum().item", "torch.logical_and().sum().item", "hit_top1.float", "hit_top2to10.sum().float", "fake_top_label_prob[].sum", "float", "float", "float", "modeling_sas.SasForPreTraining.mlm_positions.sum", "torch.round", "torch.round", "torch.round", "torch.round", "torch.round", "torch.round", "torch.round", "torch.round", "torch.round", "neg_positions.float", "float", "torch.logical_and().sum", "torch.logical_and().sum", "torch.logical_and().sum", "torch.logical_and().sum", "torch.logical_and().sum", "torch.logical_and().sum", "torch.logical_and().sum", "torch.logical_and().sum", "torch.logical_and().sum", "hit_top2to10.sum", "torch.logical_and", "torch.logical_and", "torch.logical_and", "torch.logical_and", "torch.logical_and", "torch.logical_and", "torch.logical_and", "torch.logical_and", "torch.logical_and", "torch.sign", "torch.sign", "torch.sign", "torch.sign", "torch.sign", "torch.sign", "torch.sign", "torch.sign", "torch.sign", "torch.roll", "torch.roll", "torch.roll", "torch.roll", "torch.roll", "torch.roll", "torch.roll", "torch.roll", "torch.roll"], "methods", ["None"], ["", "", "def", "track_extra_algorithm_metrics", "(", "self", ",", "topic", ":", "str", ",", "**", "kwargs", ")", ":", "\n", "\n", "# Jingqiao's Special Note: This calculation is expensive (both memory and time).", "\n", "        ", "if", "not", "(", "self", ".", "flag_debugExtraMetrics", "or", "topic", "==", "'end_of_forward'", ")", ":", "\n", "            ", "return", "\n", "\n", "", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "\n", "            ", "if", "topic", "==", "'end_of_forward'", ":", "\n", "                ", "self", ".", "metrics", "[", "'total_loss'", "]", "=", "kwargs", "[", "'total_loss'", "]", "\n", "\n", "for", "metric_name", "in", "self", ".", "metrics", ":", "\n", "                    ", "self", ".", "inform", "[", "'sanity/'", "+", "self", ".", "tb_prefix", "+", "metric_name", "]", "=", "self", ".", "metrics", "[", "metric_name", "]", ".", "item", "(", ")", "\n", "\n", "", "if", "self", ".", "flag_debugExtraMetrics", ":", "\n", "                    ", "self", ".", "wandb_callback", ".", "log", "=", "self", ".", "inform", "# pass it to the trainer to log extra metrics", "\n", "self", ".", "tensorboard_callback", ".", "log", "=", "self", ".", "inform", "# pass it to the trainer to log extra metrics", "\n", "\n", "", "if", "self", ".", "training", "and", "self", ".", "global_step", "%", "self", ".", "config", ".", "debug_config", "[", "'logging_steps'", "]", "==", "0", ":", "\n", "                    ", "print", "(", "\"\\rE%d.%03d: [Dis+Gen] Loss %.5f + %.5f; Acc: %.5f + %.5f\"", "%", "(", "\n", "self", ".", "epoch", ",", "self", ".", "global_step", ",", "\n", "float", "(", "self", ".", "metrics", "[", "'dis_loss'", "]", ")", "if", "'dis_loss'", "in", "self", ".", "metrics", "else", "0", ",", "\n", "float", "(", "self", ".", "metrics", "[", "'gen_loss'", "]", ")", ",", "\n", "float", "(", "self", ".", "metrics", "[", "'dis_accuracy'", "]", "if", "'dis_accuracy'", "in", "self", ".", "metrics", "else", "0", ")", ",", "\n", "float", "(", "self", ".", "metrics", "[", "'gen_accuracy'", "]", ")", ")", ",", "end", "=", "''", ")", "\n", "\n", "", "", "elif", "topic", "==", "'mlm'", ":", "\n", "                ", "self", ".", "metrics", "[", "'gen_loss'", "]", "=", "kwargs", "[", "'mlm_loss'", "]", "\n", "\n", "self", ".", "inform", "[", "'sanity/'", "+", "self", ".", "tb_prefix", "+", "'mask_ratio_2gram'", "]", "=", "torch", ".", "logical_and", "(", "\n", "self", ".", "mlm_positions", ",", "\n", "torch", ".", "roll", "(", "self", ".", "mlm_positions", ",", "shifts", "=", "1", ",", "dims", "=", "1", ")", "==", "self", ".", "mlm_positions", "\n", ")", ".", "sum", "(", ")", ".", "item", "(", ")", "*", "2.0", "/", "self", ".", "mlm_positions", ".", "numel", "(", ")", "\n", "\n", "self", ".", "inform", "[", "'sanity/'", "+", "self", ".", "tb_prefix", "+", "'mask_ratio_total'", "]", "=", "self", ".", "mlm_positions", ".", "sum", "(", ")", ".", "item", "(", ")", "/", "self", ".", "mlm_positions", ".", "numel", "(", ")", "\n", "\n", "self", ".", "inform", "[", "'sanity/'", "+", "self", ".", "tb_prefix", "+", "'mask_2gram_ratio'", "]", "=", "self", ".", "inform", "[", "'sanity/'", "+", "self", ".", "tb_prefix", "+", "'mask_ratio_2gram'", "]", "/", "self", ".", "inform", "[", "'sanity/'", "+", "self", ".", "tb_prefix", "+", "'mask_ratio_total'", "]", "\n", "\n", "# In case of \"alow hit\", this calcuation can be done after generating fake_token so that we don't need to self.sm(logits) twice.", "\n", "# In case of \"not allow hit\", however, we have to do it before generating fake_token (otherwise, the prob of the hard label is already set to 0)", "\n", "softmax_fct", "=", "torch", ".", "nn", ".", "Softmax", "(", "dim", "=", "-", "1", ")", "\n", "probs", "=", "softmax_fct", "(", "self", ".", "mlm_logits", ")", "\n", "fake_top_label_prob", ",", "fake_top_label", "=", "torch", ".", "topk", "(", "probs", ",", "10", ")", "\n", "hit_top1", "=", "fake_top_label", "[", "...", ",", "0", "]", "==", "self", ".", "mlm_labels", "\n", "hit_top2to10", "=", "fake_top_label", "[", "...", ",", "1", ":", "]", "==", "self", ".", "mlm_labels", ".", "unsqueeze", "(", "1", ")", "\n", "\n", "self", ".", "metrics", "[", "'gen_accuracy'", "]", "=", "hit_top1", ".", "float", "(", ")", ".", "mean", "(", ")", "\n", "self", ".", "metrics", "[", "'gen_probability'", "]", "=", "fake_top_label_prob", "[", "...", ",", "0", "]", ".", "mean", "(", ")", "\n", "self", ".", "metrics", "[", "'gen_accuracy_top2to10'", "]", "=", "hit_top2to10", ".", "sum", "(", "dim", "=", "-", "1", ")", ".", "float", "(", ")", ".", "mean", "(", ")", "\n", "self", ".", "metrics", "[", "'gen_prob_top2to10'", "]", "=", "fake_top_label_prob", "[", "...", ",", "1", ":", "10", "]", ".", "sum", "(", "dim", "=", "-", "1", ")", ".", "mean", "(", ")", "\n", "\n", "", "elif", "topic", "==", "'rtd'", ":", "\n", "                ", "self", ".", "metrics", "[", "'dis_loss'", "]", "=", "kwargs", "[", "'rtd_loss'", "]", "\n", "rtd_logits", ",", "rtd_labels", "=", "kwargs", "[", "'rtd_logits'", "]", ",", "kwargs", "[", "'rtd_labels'", "]", "\n", "dis_predictions", "=", "torch", ".", "round", "(", "(", "torch", ".", "sign", "(", "rtd_logits", ")", "+", "1", ")", "/", "2", ")", ".", "int", "(", ")", "\n", "neg_positions", "=", "(", "rtd_labels", "==", "0", ")", "\n", "\n", "self", ".", "metrics", "[", "'dis_accuracy_base'", "]", "=", "neg_positions", ".", "float", "(", ")", ".", "mean", "(", ")", "\n", "self", ".", "metrics", "[", "'dis_accuracy'", "]", "=", "(", "(", "dis_predictions", "==", "rtd_labels", ")", "*", "1.0", ")", ".", "mean", "(", ")", "\n", "self", ".", "metrics", "[", "'dis_accuracy_neg'", "]", "=", "(", "(", "dis_predictions", "[", "neg_positions", "]", "==", "rtd_labels", "[", "neg_positions", "]", ")", "*", "1.0", ")", ".", "mean", "(", ")", "\n", "self", ".", "metrics", "[", "'dis_accuracy_pos'", "]", "=", "(", "(", "dis_predictions", "[", "~", "neg_positions", "]", "==", "rtd_labels", "[", "~", "neg_positions", "]", ")", "*", "1.0", ")", ".", "mean", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.alibaba_self-augmentation-strategy.models.modeling_sas.SasForPreTraining.batch_initial_status_update": [[1683, 1705], ["dict", "dict", "modeling_sas.SasForPreTraining.schedule_loss_weights"], "methods", ["home.repos.pwc.inspect_result.alibaba_self-augmentation-strategy.models.modeling_sas.SasForPreTraining.schedule_loss_weights"], ["", "", "", "def", "batch_initial_status_update", "(", "self", ",", "side_info_sets", ")", ":", "\n", "        ", "self", ".", "metrics", "=", "dict", "(", ")", "\n", "self", ".", "inform", "=", "dict", "(", ")", "\n", "\n", "self", ".", "tb_prefix", "=", "''", "if", "self", ".", "training", "else", "'E-'", "\n", "self", ".", "batch_idx", "=", "side_info_sets", "[", "'batch_idx'", "]", "\n", "\n", "if", "self", ".", "training", ":", "\n", "            ", "self", ".", "global_step", "=", "self", ".", "trainer_callback", ".", "state", ".", "global_step", "\n", "self", ".", "epoch", "=", "self", ".", "trainer_callback", ".", "state", ".", "epoch", "\n", "self", ".", "inform", "[", "'all/global_step'", "]", "=", "self", ".", "global_step", "\n", "self", ".", "inform", "[", "'all/global_epoch'", "]", "=", "self", ".", "epoch", "\n", "if", "self", ".", "max_steps", "is", "None", ":", "\n", "                ", "self", ".", "max_steps", "=", "self", ".", "trainer_callback", ".", "state", ".", "max_steps", "\n", "self", ".", "max_epochs_int", "=", "self", ".", "trainer_callback", ".", "state", ".", "num_train_epochs", "\n", "\n", "", "self", ".", "schedule_loss_weights", "(", ")", "\n", "\n", "self", ".", "flag_debugExtraMetrics", "=", "self", ".", "config", ".", "debug_config", "[", "'debugExtraMetrics'", "]", "and", "(", "\n", "self", ".", "global_step", "<", "100", "or", "self", ".", "global_step", "%", "self", ".", "config", ".", "debug_config", "[", "'debugExtraMetrics'", "]", "==", "0", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "flag_debugExtraMetrics", "=", "True", "\n", "\n"]], "home.repos.pwc.inspect_result.alibaba_self-augmentation-strategy.models.modeling_sas.SasForPreTraining.schedule_loss_weights": [[1706, 1736], ["numpy.fromstring", "min", "min", "min", "int"], "methods", ["None"], ["", "", "def", "schedule_loss_weights", "(", "self", ")", ":", "\n", "        ", "if", "self", ".", "global_step", "==", "1", ":", "# Check it once at the beginning of the training", "\n", "            ", "assert", "self", ".", "config", ".", "dis_weight_scheduler", "in", "[", "0", ",", "1", ",", "2", ",", "3", ",", "4", "]", ",", "\"Don't support other dis_weight_scheduler yet\"", "\n", "\n", "# gen_weight_scheduler", "\n", "", "self", ".", "gen_weight", "=", "self", ".", "config", ".", "gen_weight", "\n", "\n", "# dis_weight_scheduler", "\n", "self", ".", "dis_weight_setting", "=", "np", ".", "fromstring", "(", "self", ".", "config", ".", "dis_weight", ",", "dtype", "=", "float", ",", "sep", "=", "'-'", ")", "\n", "if", "self", ".", "config", ".", "dis_weight_scheduler", "==", "0", ":", "\n", "            ", "self", ".", "dis_weight", "=", "self", ".", "dis_weight_setting", "[", "0", "]", "\n", "", "elif", "self", ".", "config", ".", "dis_weight_scheduler", "in", "[", "1", ",", "2", "]", ":", "\n", "            ", "training_perc", "=", "min", "(", "1", ",", "self", ".", "global_step", "/", "self", ".", "max_steps", ")", "\n", "if", "self", ".", "config", ".", "dis_weight_scheduler", "==", "1", ":", "# Update every step", "\n", "                ", "pass", "\n", "", "elif", "self", ".", "config", ".", "dis_weight_scheduler", "==", "2", ":", "# Update at the end of each epoch", "\n", "                ", "training_perc", "=", "min", "(", "1", ",", "int", "(", "self", ".", "epoch", ")", "/", "(", "self", ".", "max_epochs_int", "-", "1", ")", ")", "\n", "\n", "", "self", ".", "dis_weight", "=", "self", ".", "dis_weight_setting", "[", "0", "]", "*", "(", "1", "-", "training_perc", ")", "+", "self", ".", "dis_weight_setting", "[", "1", "]", "*", "training_perc", "\n", "\n", "", "elif", "self", ".", "config", ".", "dis_weight_scheduler", "==", "4", ":", "\n", "            ", "if", "self", ".", "epoch", "<", "self", ".", "config", ".", "cold_start_epochs", ":", "\n", "                ", "self", ".", "dis_weight", "=", "self", ".", "dis_weight_setting", "[", "0", "]", "\n", "", "else", ":", "\n", "                ", "self", ".", "dis_weight", "=", "self", ".", "dis_weight_setting", "[", "1", "]", "\n", "\n", "", "", "self", ".", "inform", "[", "'loss_weights/dis_weight'", "]", "=", "self", ".", "dis_weight", "\n", "\n", "\n", "training_perc", "=", "min", "(", "1", ",", "self", ".", "global_step", "/", "(", "self", ".", "max_steps", "-", "1", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.alibaba_self-augmentation-strategy.models.modeling_sas.SasForPreTraining.run_SA_and_LS_preparation": [[1738, 1806], ["dict", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "modeling_sas.SasForPreTraining.dataset.update_buffer_da", "torch.nn.Softmax", "torch.nn.Softmax", "torch.nn.Softmax", "torch.nn.Softmax", "torch.nn.Softmax", "torch.nn.Softmax", "torch.nn.Softmax", "torch.nn.Softmax", "torch.nn.Softmax", "torch.isnan().any", "torch.isnan().any", "torch.isnan().any", "torch.isnan().any", "torch.isnan().any", "torch.isnan().any", "torch.isnan().any", "torch.isnan().any", "torch.isnan().any", "torch.multinomial", "torch.multinomial", "torch.multinomial", "torch.multinomial", "torch.multinomial", "torch.multinomial", "torch.multinomial", "torch.multinomial", "torch.multinomial", "modeling_sas.SasForPreTraining.track_compute_statistics", "modeling_sas.SasForPreTraining.track_compute_statistics", "modeling_sas.SasForPreTraining.batch_idx.cpu().data.numpy", "mlm_logits.clone().detach", "mlm_logits.clone().detach.scatter_", "torch.nn.Softmax.", "torch.nn.Softmax.", "torch.nn.Softmax.", "torch.nn.Softmax.", "torch.nn.Softmax.", "torch.nn.Softmax.", "modeling_sas.SasForPreTraining.dataset.save_buffer_da", "torch.gather", "torch.gather", "torch.gather", "torch.gather", "torch.gather", "torch.gather", "torch.gather", "torch.gather", "torch.gather", "hit.float().mean().item", "torch.gather.mean().item", "torch.gather.mean().item", "torch.gather.mean().item", "torch.gather", "torch.gather", "torch.gather", "torch.gather", "torch.gather", "torch.gather", "torch.gather", "torch.gather", "torch.gather", "torch.gather.mean().item", "torch.gather.mean().item", "torch.gather.mean().item", "mlm_labels.view", "torch.isnan", "torch.isnan", "torch.isnan", "torch.isnan", "torch.isnan", "torch.isnan", "torch.isnan", "torch.isnan", "torch.isnan", "max", "modeling_sas.SasForPreTraining.batch_idx.cpu().data.numpy", "mlm_logits.clone", "abs", "abs", "mlm_positions.cpu", "hit.float().mean", "torch.gather.mean", "torch.gather.mean", "torch.gather.mean", "mlm_labels.view", "torch.gather.mean", "torch.gather.mean", "torch.gather.mean", "modeling_sas.SasForPreTraining.batch_idx.cpu", "modeling_sas.SasForPreTraining.batch_idx.cpu", "hit.float"], "methods", ["home.repos.pwc.inspect_result.alibaba_self-augmentation-strategy.utils.data_collator_sas.SASTrainDataset.update_buffer_da", "home.repos.pwc.inspect_result.alibaba_self-augmentation-strategy.models.modeling_sas.SasForPreTraining.track_compute_statistics", "home.repos.pwc.inspect_result.alibaba_self-augmentation-strategy.models.modeling_sas.SasForPreTraining.track_compute_statistics", "home.repos.pwc.inspect_result.alibaba_self-augmentation-strategy.utils.data_collator_sas.SASTrainDataset.save_buffer_da"], ["", "def", "run_SA_and_LS_preparation", "(", "self", ",", "inform", "=", "dict", "(", ")", ")", ":", "\n", "\n", "        ", "if", "not", "self", ".", "training", ":", "\n", "            ", "return", "inform", "\n", "", "if", "not", "(", "self", ".", "dataset", "and", "self", ".", "batch_idx", "is", "not", "None", ")", ":", "\n", "            ", "return", "inform", "\n", "\n", "# Don't need gradient information during SA and LS's data preparation", "\n", "", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "            ", "self", ".", "inform", "=", "{", "**", "self", ".", "inform", ",", "**", "inform", "}", "\n", "\n", "# No longer need to clone, if we do the SA and LA after backward step", "\n", "mlm_logits", "=", "self", ".", "mlm_logits", "\n", "mlm_positions", "=", "self", ".", "mlm_positions", "\n", "mlm_labels", "=", "self", ".", "mlm_labels", "\n", "\n", "\n", "# Freeze generation", "\n", "flag_freeze_generation", "=", "(", "self", ".", "config", ".", "augmentation_copies", ">", "1", "and", "self", ".", "epoch", ">=", "self", ".", "config", ".", "cold_start_epochs", ")", "\n", "# Shift the multiple augmentations generated per instance before generation freezing, so that different augmentations are used in different epochs for each instance after freezing.", "\n", "if", "flag_freeze_generation", "and", "self", ".", "training", ":", "\n", "                ", "self", ".", "dataset", ".", "update_buffer_da", "(", "self", ".", "batch_idx", ".", "cpu", "(", ")", ".", "data", ".", "numpy", "(", ")", ",", "flag_freeze_generation", "=", "flag_freeze_generation", ")", "\n", "\n", "# To save computation time, only start generating data augmentation in the last epoch of the cold start period.", "\n", "", "num_to_sample_da", "=", "(", "self", ".", "config", ".", "augmentation_copies", "if", "not", "flag_freeze_generation", "and", "self", ".", "epoch", ">=", "self", ".", "config", ".", "cold_start_epochs", "-", "1", "\n", "else", "0", ")", "\n", "\n", "if", "num_to_sample_da", ">", "0", "or", "self", ".", "flag_debugExtraMetrics", ":", "\n", "                ", "softmax_fct", "=", "torch", ".", "nn", ".", "Softmax", "(", "dim", "=", "-", "1", ")", "\n", "if", "self", ".", "config", ".", "augmentation_temperature", "<", "0", ":", "# In case of being negative, use no-hit schedule", "\n", "# Avoid modify logits directly, because it might be used in some other place", "\n", "                    ", "logits_non_label", "=", "mlm_logits", ".", "clone", "(", ")", ".", "detach", "(", ")", "\n", "logits_non_label", ".", "scatter_", "(", "-", "1", ",", "mlm_labels", ".", "view", "(", "-", "1", ",", "1", ")", ",", "-", "10000", ")", "\n", "probs", "=", "softmax_fct", "(", "logits_non_label", "/", "abs", "(", "self", ".", "config", ".", "augmentation_temperature", ")", ")", "\n", "", "else", ":", "\n", "                    ", "probs", "=", "softmax_fct", "(", "mlm_logits", "/", "abs", "(", "self", ".", "config", ".", "augmentation_temperature", ")", ")", "\n", "\n", "", "if", "torch", ".", "isnan", "(", "probs", ")", ".", "any", "(", ")", ":", "\n", "                    ", "probs", "[", "...", "]", "=", "1.0", "/", "probs", ".", "shape", "[", "1", "]", "\n", "\n", "# Set num_samples to a minimal 2, otherwise multinomial is not controlled by random seed", "\n", "# (a bug in multinomial - happens when some element of probs is less than 1e-5 when using float16)", "\n", "# For now, we use this as a temporary workaround to make experiments reproducible.", "\n", "", "fake_token", "=", "torch", ".", "multinomial", "(", "probs", ",", "num_samples", "=", "max", "(", "2", ",", "num_to_sample_da", ")", ",", "replacement", "=", "True", ")", "\n", "\n", "self", ".", "track_compute_statistics", "(", "mod", "=", "None", ",", "inp", "=", "None", ",", "out", "=", "None", ",", "name", "=", "'DataAugmentation,Multinomial etc.,'", ")", "\n", "\n", "if", "num_to_sample_da", ">", "0", ":", "\n", "                    ", "self", ".", "dataset", ".", "save_buffer_da", "(", "self", ".", "batch_idx", ".", "cpu", "(", ")", ".", "data", ".", "numpy", "(", ")", ",", "fake_token", "[", "...", ",", ":", "num_to_sample_da", "]", ",", "\n", "aug_positions", "=", "mlm_positions", ".", "cpu", "(", ")", ")", "\n", "\n", "", "if", "self", ".", "flag_debugExtraMetrics", ":", "\n", "                    ", "hit", "=", "(", "fake_token", "[", "...", ",", "0", "]", "==", "mlm_labels", ")", "\n", "fake_prob", "=", "torch", ".", "gather", "(", "probs", ",", "1", ",", "fake_token", "[", "...", ",", ":", "1", "]", ")", "\n", "self", ".", "inform", "[", "'sanity/'", "+", "self", ".", "tb_prefix", "+", "'hit_ratio_mask'", "]", "=", "hit", ".", "float", "(", ")", ".", "mean", "(", ")", ".", "item", "(", ")", "\n", "self", ".", "inform", "[", "'sanity/'", "+", "self", ".", "tb_prefix", "+", "'fake_prob_average'", "]", "=", "fake_prob", ".", "mean", "(", ")", ".", "item", "(", ")", "\n", "self", ".", "inform", "[", "'sanity/'", "+", "self", ".", "tb_prefix", "+", "'fake_prob_larger_50%'", "]", "=", "(", "(", "fake_prob", ">", "0.5", ")", "*", "1.0", ")", ".", "mean", "(", ")", ".", "item", "(", ")", "\n", "\n", "label_probs", "=", "torch", ".", "gather", "(", "probs", ",", "dim", "=", "1", ",", "index", "=", "mlm_labels", ".", "view", "(", "-", "1", ",", "1", ")", ")", "\n", "self", ".", "inform", "[", "'sanity/'", "+", "self", ".", "tb_prefix", "+", "'label_probs'", "]", "=", "label_probs", ".", "mean", "(", ")", ".", "item", "(", ")", "\n", "\n", "", "self", ".", "track_compute_statistics", "(", "mod", "=", "None", ",", "inp", "=", "None", ",", "out", "=", "None", ",", "name", "=", "'DataAugmentation,SaveBuffer,'", ")", "\n", "\n", "if", "self", ".", "flag_debugExtraMetrics", ":", "\n", "                    ", "self", ".", "wandb_callback", ".", "log", "=", "self", ".", "inform", "# pass it to the trainer to log extra metrics", "\n", "self", ".", "tensorboard_callback", ".", "log", "=", "self", ".", "inform", "# pass it to the trainer to log extra metrics", "\n", "\n", "", "", "", "return", "self", ".", "inform", "# Update inform in case of calculation after backward, so that SA and LS's inform can be logged in the training process", "\n", "\n"]], "home.repos.pwc.inspect_result.alibaba_self-augmentation-strategy.models.modeling_sas.SasForPreTraining.forward": [[1808, 1910], ["transformers.file_utils.add_start_docstrings_to_model_forward", "transformers.file_utils.replace_return_docstrings", "modeling_sas.SasForPreTraining.batch_initial_status_update", "modeling_sas.SasForPreTraining.sas", "modeling_sas.SasForPreTraining.generator_lm_head", "torch.CrossEntropyLoss", "torch.CrossEntropyLoss", "torch.CrossEntropyLoss", "torch.BCEWithLogitsLoss.", "modeling_sas.SasForPreTraining.track_extra_algorithm_metrics", "modeling_sas.SasForPreTraining.track_extra_algorithm_metrics", "modeling_sas.SasForPreTraining.run_SA_and_LS_preparation", "modeling_sas.SasForPreTrainingOutput", "SAS_INPUTS_DOCSTRING.format", "modeling_sas.SasForPreTraining.generator_predictions", "modeling_sas.SasForPreTraining.mlm_logits.view", "modeling_sas.SasForPreTraining.mlm_labels.view", "modeling_sas.SasForPreTraining.discriminator_predictions", "torch.BCEWithLogitsLoss", "torch.BCEWithLogitsLoss", "torch.BCEWithLogitsLoss", "modeling_sas.SasForPreTraining.track_extra_algorithm_metrics", "torch.BCEWithLogitsLoss.", "torch.BCEWithLogitsLoss.", "attention_mask.view", "modeling_sas.SasForPreTraining.view", "active_labels.float", "modeling_sas.SasForPreTraining.view", "rtd_labels.float"], "methods", ["home.repos.pwc.inspect_result.alibaba_self-augmentation-strategy.models.modeling_sas.SasForPreTraining.batch_initial_status_update", "home.repos.pwc.inspect_result.alibaba_self-augmentation-strategy.models.modeling_sas.SasForPreTraining.track_extra_algorithm_metrics", "home.repos.pwc.inspect_result.alibaba_self-augmentation-strategy.models.modeling_sas.SasForPreTraining.track_extra_algorithm_metrics", "home.repos.pwc.inspect_result.alibaba_self-augmentation-strategy.models.modeling_sas.SasForPreTraining.run_SA_and_LS_preparation", "home.repos.pwc.inspect_result.alibaba_self-augmentation-strategy.models.modeling_sas.SasForPreTraining.track_extra_algorithm_metrics"], ["", "@", "add_start_docstrings_to_model_forward", "(", "SAS_INPUTS_DOCSTRING", ".", "format", "(", "\"batch_size, sequence_length\"", ")", ")", "\n", "@", "replace_return_docstrings", "(", "output_type", "=", "SasForRTDOutput", ",", "config_class", "=", "_CONFIG_FOR_DOC", ")", "\n", "def", "forward", "(", "\n", "self", ",", "\n", "input_ids", ":", "torch", ".", "Tensor", "=", "None", ",", "\n", "attention_mask", ":", "torch", ".", "Tensor", "=", "None", ",", "\n", "token_type_ids", ":", "torch", ".", "Tensor", "=", "None", ",", "\n", "position_ids", ":", "torch", ".", "Tensor", "=", "None", ",", "\n", "head_mask", ":", "torch", ".", "Tensor", "=", "None", ",", "\n", "inputs_embeds", ":", "torch", ".", "Tensor", "=", "None", ",", "\n", "mlm_labels", ":", "torch", ".", "Tensor", "=", "None", ",", "\n", "rtd_labels", ":", "torch", ".", "Tensor", "=", "None", ",", "\n", "output_attentions", "=", "None", ",", "\n", "output_hidden_states", "=", "None", ",", "\n", "side_info_sets", ":", "Optional", "[", "dict", "]", "=", "{", "}", ",", "\n", "return_dict", "=", "None", ",", "\n", ")", ":", "\n", "        ", "r\"\"\"\n        labels (``torch.LongTensor`` of shape ``(batch_size, sequence_length)``, `optional`):\n            Labels for computing the SAS loss. Input should be a sequence of tokens (see :obj:`input_ids`\n            docstring) Indices should be in ``[0, 1]``:\n\n            - 0 indicates the token is an original token,\n            - 1 indicates the token was replaced.\n\n        Returns:\n\n        Examples::\n\n            >>> from transformers import SasTokenizer, SasForPreTraining\n            >>> import torch\n\n            >>> tokenizer = SasTokenizer.from_pretrained('google/sas-small-discriminator')\n            >>> model = SasForPreTraining.from_pretrained('google/sas-small-discriminator')\n\n            >>> input_ids = torch.tensor(tokenizer.encode(\"Hello, my dog is cute\", add_special_tokens=True)).unsqueeze(0)  # Batch size 1\n            >>> logits = model(input_ids).logits\n        \"\"\"", "\n", "self", ".", "batch_initial_status_update", "(", "side_info_sets", ")", "\n", "\n", "return_dict", "=", "return_dict", "if", "return_dict", "is", "not", "None", "else", "self", ".", "config", ".", "use_return_dict", "\n", "\n", "hidden_states", "=", "self", ".", "sas", "(", "\n", "input_ids", ",", "\n", "attention_mask", ",", "\n", "token_type_ids", ",", "\n", "position_ids", ",", "\n", "head_mask", ",", "\n", "inputs_embeds", ",", "\n", "output_attentions", ",", "\n", "output_hidden_states", ",", "\n", "side_info_sets", ",", "\n", "return_dict", ",", "\n", ")", "\n", "\n", "# Masked language modeling", "\n", "self", ".", "generator_sequence_output", "=", "hidden_states", "[", "0", "]", "\n", "self", ".", "mlm_positions", "=", "(", "mlm_labels", "!=", "-", "100", ")", "\n", "self", ".", "mlm_labels", "=", "mlm_labels", "[", "self", ".", "mlm_positions", "]", "\n", "self", ".", "mlm_logits", "=", "self", ".", "generator_lm_head", "(", "self", ".", "generator_predictions", "(", "self", ".", "generator_sequence_output", "[", "self", ".", "mlm_positions", "]", ")", ")", "\n", "\n", "loss_fct", "=", "nn", ".", "CrossEntropyLoss", "(", ")", "# -100 index = padding token", "\n", "\n", "\n", "mlm_loss", "=", "loss_fct", "(", "self", ".", "mlm_logits", ".", "view", "(", "-", "1", ",", "self", ".", "config", ".", "vocab_size", ")", ",", "self", ".", "mlm_labels", ".", "view", "(", "-", "1", ")", ")", "\n", "\n", "self", ".", "track_extra_algorithm_metrics", "(", "topic", "=", "'mlm'", ",", "mlm_loss", "=", "mlm_loss", ")", "\n", "\n", "# RTD task", "\n", "if", "rtd_labels", "is", "not", "None", "and", "self", ".", "dis_weight", ">", "0", ":", "\n", "            ", "discriminator_sequence_output", "=", "hidden_states", "[", "0", "]", "\n", "rtd_logits", "=", "self", ".", "discriminator_predictions", "(", "discriminator_sequence_output", ")", "\n", "loss_fct", "=", "nn", ".", "BCEWithLogitsLoss", "(", ")", "\n", "if", "attention_mask", "is", "not", "None", ":", "\n", "                ", "active_loss", "=", "attention_mask", ".", "view", "(", "-", "1", ",", "discriminator_sequence_output", ".", "shape", "[", "1", "]", ")", "==", "1", "\n", "active_logits", "=", "rtd_logits", ".", "view", "(", "-", "1", ",", "discriminator_sequence_output", ".", "shape", "[", "1", "]", ")", "[", "active_loss", "]", "\n", "active_labels", "=", "rtd_labels", "[", "active_loss", "]", "\n", "rtd_loss", "=", "loss_fct", "(", "active_logits", ",", "active_labels", ".", "float", "(", ")", ")", "\n", "", "else", ":", "\n", "                ", "rtd_loss", "=", "loss_fct", "(", "rtd_logits", ".", "view", "(", "-", "1", ",", "discriminator_sequence_output", ".", "shape", "[", "1", "]", ")", ",", "rtd_labels", ".", "float", "(", ")", ")", "\n", "\n", "", "self", ".", "track_extra_algorithm_metrics", "(", "topic", "=", "'rtd'", ",", "rtd_loss", "=", "rtd_loss", ",", "rtd_logits", "=", "rtd_logits", ",", "rtd_labels", "=", "rtd_labels", ")", "\n", "", "else", ":", "\n", "            ", "rtd_loss", "=", "0", ";", "rtd_logits", "=", "None", "\n", "\n", "", "loss", "=", "self", ".", "gen_weight", "*", "mlm_loss", "+", "self", ".", "dis_weight", "*", "rtd_loss", "\n", "self", ".", "track_extra_algorithm_metrics", "(", "topic", "=", "'end_of_forward'", ",", "total_loss", "=", "loss", ")", "\n", "\n", "self", ".", "run_SA_and_LS_preparation", "(", ")", "\n", "\n", "if", "not", "return_dict", ":", "\n", "            ", "output", "=", "(", "self", ".", "mlm_logits", ",", "rtd_logits", ",", ")", "+", "hidden_states", "[", "1", ":", "]", "\n", "return", "(", "(", "loss", ",", ")", "+", "output", ")", "if", "loss", "is", "not", "None", "else", "output", "\n", "\n", "", "return", "SasForPreTrainingOutput", "(", "\n", "loss", "=", "loss", ",", "\n", "mlm_loss", "=", "mlm_loss", ",", "\n", "mlm_logits", "=", "self", ".", "mlm_logits", ",", "\n", "rtd_loss", "=", "rtd_loss", ",", "\n", "rtd_logits", "=", "rtd_logits", ",", "\n", "hidden_states", "=", "hidden_states", ".", "hidden_states", ",", "\n", "attentions", "=", "hidden_states", ".", "attentions", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.alibaba_self-augmentation-strategy.models.modeling_sas.SasForMaskedLM.__init__": [[1924, 1932], ["transformers.modeling_utils.PreTrainedModel.__init__", "modeling_sas.SasModel", "modeling_sas.SasGeneratorPredictions", "torch.Linear", "torch.Linear", "torch.Linear", "modeling_sas.SasForMaskedLM.init_weights"], "methods", ["home.repos.pwc.inspect_result.alibaba_self-augmentation-strategy.models.trainer_sas.SequentialDistributedSampler.__init__"], ["    ", "def", "__init__", "(", "self", ",", "config", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "config", ")", "\n", "\n", "self", ".", "sas", "=", "SasModel", "(", "config", ")", "\n", "self", ".", "generator_predictions", "=", "SasGeneratorPredictions", "(", "config", ")", "\n", "\n", "self", ".", "generator_lm_head", "=", "nn", ".", "Linear", "(", "config", ".", "embedding_size", ",", "config", ".", "vocab_size", ")", "\n", "self", ".", "init_weights", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.alibaba_self-augmentation-strategy.models.modeling_sas.SasForMaskedLM.get_output_embeddings": [[1933, 1935], ["None"], "methods", ["None"], ["", "def", "get_output_embeddings", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "generator_lm_head", "\n", "\n"]], "home.repos.pwc.inspect_result.alibaba_self-augmentation-strategy.models.modeling_sas.SasForMaskedLM.set_output_embeddings": [[1936, 1938], ["None"], "methods", ["None"], ["", "def", "set_output_embeddings", "(", "self", ",", "word_embeddings", ")", ":", "\n", "        ", "self", ".", "generator_lm_head", "=", "word_embeddings", "\n", "\n"]], "home.repos.pwc.inspect_result.alibaba_self-augmentation-strategy.models.modeling_sas.SasForMaskedLM.forward": [[1939, 2000], ["transformers.file_utils.add_start_docstrings_to_model_forward", "transformers.file_utils.add_code_sample_docstrings", "dict", "modeling_sas.SasForMaskedLM.sas", "modeling_sas.SasForMaskedLM.generator_predictions", "modeling_sas.SasForMaskedLM.generator_lm_head", "transformers.modeling_outputs.MaskedLMOutput", "SAS_INPUTS_DOCSTRING.format", "torch.CrossEntropyLoss", "torch.CrossEntropyLoss", "torch.CrossEntropyLoss", "torch.CrossEntropyLoss.", "modeling_sas.SasForMaskedLM.view", "labels.view"], "methods", ["None"], ["", "@", "add_start_docstrings_to_model_forward", "(", "SAS_INPUTS_DOCSTRING", ".", "format", "(", "\"batch_size, sequence_length\"", ")", ")", "\n", "@", "add_code_sample_docstrings", "(", "\n", "tokenizer_class", "=", "_TOKENIZER_FOR_DOC", ",", "\n", "checkpoint", "=", "\"google/sas-small-discriminator\"", ",", "\n", "output_type", "=", "MaskedLMOutput", ",", "\n", "config_class", "=", "_CONFIG_FOR_DOC", ",", "\n", ")", "\n", "def", "forward", "(", "\n", "self", ",", "\n", "input_ids", "=", "None", ",", "\n", "attention_mask", "=", "None", ",", "\n", "token_type_ids", "=", "None", ",", "\n", "position_ids", "=", "None", ",", "\n", "head_mask", "=", "None", ",", "\n", "inputs_embeds", "=", "None", ",", "\n", "labels", "=", "None", ",", "\n", "output_attentions", "=", "None", ",", "\n", "output_hidden_states", "=", "None", ",", "\n", "side_info_sets", "=", "dict", "(", ")", ",", "\n", "return_dict", "=", "None", ",", "\n", ")", ":", "\n", "        ", "r\"\"\"\n        labels (:obj:`torch.LongTensor` of shape :obj:`(batch_size, sequence_length)`, `optional`):\n            Labels for computing the masked language modeling loss. Indices should be in ``[-100, 0, ...,\n            config.vocab_size]`` (see ``input_ids`` docstring) Tokens with indices set to ``-100`` are ignored\n            (masked), the loss is only computed for the tokens with labels in ``[0, ..., config.vocab_size]``\n        \"\"\"", "\n", "return_dict", "=", "return_dict", "if", "return_dict", "is", "not", "None", "else", "self", ".", "config", ".", "use_return_dict", "\n", "\n", "generator_hidden_states", "=", "self", ".", "sas", "(", "\n", "input_ids", ",", "\n", "attention_mask", ",", "\n", "token_type_ids", ",", "\n", "position_ids", ",", "\n", "head_mask", ",", "\n", "inputs_embeds", ",", "\n", "output_attentions", ",", "\n", "output_hidden_states", ",", "\n", "side_info_sets", ",", "\n", "return_dict", ",", "\n", ")", "\n", "generator_sequence_output", "=", "generator_hidden_states", "[", "0", "]", "\n", "\n", "prediction_scores", "=", "self", ".", "generator_predictions", "(", "generator_sequence_output", ")", "\n", "prediction_scores", "=", "self", ".", "generator_lm_head", "(", "prediction_scores", ")", "\n", "\n", "loss", "=", "None", "\n", "# Masked language modeling softmax layer", "\n", "if", "labels", "is", "not", "None", ":", "\n", "            ", "loss_fct", "=", "nn", ".", "CrossEntropyLoss", "(", ")", "# -100 index = padding token", "\n", "loss", "=", "loss_fct", "(", "prediction_scores", ".", "view", "(", "-", "1", ",", "self", ".", "config", ".", "vocab_size", ")", ",", "labels", ".", "view", "(", "-", "1", ")", ")", "\n", "\n", "", "if", "not", "return_dict", ":", "\n", "            ", "output", "=", "(", "prediction_scores", ",", ")", "+", "generator_hidden_states", "[", "1", ":", "]", "\n", "return", "(", "(", "loss", ",", ")", "+", "output", ")", "if", "loss", "is", "not", "None", "else", "output", "\n", "\n", "", "return", "MaskedLMOutput", "(", "\n", "loss", "=", "loss", ",", "\n", "logits", "=", "prediction_scores", ",", "\n", "hidden_states", "=", "generator_hidden_states", ".", "hidden_states", ",", "\n", "attentions", "=", "generator_hidden_states", ".", "attentions", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.alibaba_self-augmentation-strategy.models.modeling_sas.SasForTokenClassification.__init__": [[2012, 2019], ["transformers.modeling_utils.PreTrainedModel.__init__", "modeling_sas.SasModel", "torch.Dropout", "torch.Dropout", "torch.Dropout", "torch.Linear", "torch.Linear", "torch.Linear", "modeling_sas.SasForTokenClassification.init_weights"], "methods", ["home.repos.pwc.inspect_result.alibaba_self-augmentation-strategy.models.trainer_sas.SequentialDistributedSampler.__init__"], ["    ", "def", "__init__", "(", "self", ",", "config", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "config", ")", "\n", "\n", "self", ".", "sas", "=", "SasModel", "(", "config", ")", "\n", "self", ".", "dropout", "=", "nn", ".", "Dropout", "(", "config", ".", "hidden_dropout_prob", ")", "\n", "self", ".", "classifier", "=", "nn", ".", "Linear", "(", "config", ".", "hidden_size", ",", "config", ".", "num_labels", ")", "\n", "self", ".", "init_weights", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.alibaba_self-augmentation-strategy.models.modeling_sas.SasForTokenClassification.forward": [[2020, 2086], ["transformers.file_utils.add_start_docstrings_to_model_forward", "transformers.file_utils.add_code_sample_docstrings", "dict", "modeling_sas.SasForTokenClassification.sas", "modeling_sas.SasForTokenClassification.dropout", "modeling_sas.SasForTokenClassification.classifier", "transformers.modeling_outputs.TokenClassifierOutput", "SAS_INPUTS_DOCSTRING.format", "torch.CrossEntropyLoss", "torch.CrossEntropyLoss", "torch.CrossEntropyLoss", "torch.CrossEntropyLoss.", "torch.CrossEntropyLoss.", "attention_mask.view", "modeling_sas.SasForTokenClassification.view", "labels.view", "modeling_sas.SasForTokenClassification.view", "labels.view"], "methods", ["None"], ["", "@", "add_start_docstrings_to_model_forward", "(", "SAS_INPUTS_DOCSTRING", ".", "format", "(", "\"batch_size, sequence_length\"", ")", ")", "\n", "@", "add_code_sample_docstrings", "(", "\n", "tokenizer_class", "=", "_TOKENIZER_FOR_DOC", ",", "\n", "checkpoint", "=", "\"google/sas-small-discriminator\"", ",", "\n", "output_type", "=", "TokenClassifierOutput", ",", "\n", "config_class", "=", "_CONFIG_FOR_DOC", ",", "\n", ")", "\n", "def", "forward", "(", "\n", "self", ",", "\n", "input_ids", "=", "None", ",", "\n", "attention_mask", "=", "None", ",", "\n", "token_type_ids", "=", "None", ",", "\n", "position_ids", "=", "None", ",", "\n", "head_mask", "=", "None", ",", "\n", "inputs_embeds", "=", "None", ",", "\n", "labels", "=", "None", ",", "\n", "output_attentions", "=", "None", ",", "\n", "output_hidden_states", "=", "None", ",", "\n", "side_info_sets", "=", "dict", "(", ")", ",", "\n", "return_dict", "=", "None", ",", "\n", ")", ":", "\n", "        ", "r\"\"\"\n        labels (:obj:`torch.LongTensor` of shape :obj:`(batch_size, sequence_length)`, `optional`):\n            Labels for computing the token classification loss. Indices should be in ``[0, ..., config.num_labels -\n            1]``.\n        \"\"\"", "\n", "return_dict", "=", "return_dict", "if", "return_dict", "is", "not", "None", "else", "self", ".", "config", ".", "use_return_dict", "\n", "\n", "discriminator_hidden_states", "=", "self", ".", "sas", "(", "\n", "input_ids", ",", "\n", "attention_mask", ",", "\n", "token_type_ids", ",", "\n", "position_ids", ",", "\n", "head_mask", ",", "\n", "inputs_embeds", ",", "\n", "output_attentions", ",", "\n", "output_hidden_states", ",", "\n", "side_info_sets", ",", "\n", "return_dict", ",", "\n", ")", "\n", "discriminator_sequence_output", "=", "discriminator_hidden_states", "[", "0", "]", "\n", "\n", "discriminator_sequence_output", "=", "self", ".", "dropout", "(", "discriminator_sequence_output", ")", "\n", "logits", "=", "self", ".", "classifier", "(", "discriminator_sequence_output", ")", "\n", "\n", "loss", "=", "None", "\n", "if", "labels", "is", "not", "None", ":", "\n", "            ", "loss_fct", "=", "nn", ".", "CrossEntropyLoss", "(", ")", "\n", "# Only keep active parts of the loss", "\n", "if", "attention_mask", "is", "not", "None", ":", "\n", "                ", "active_loss", "=", "attention_mask", ".", "view", "(", "-", "1", ")", "==", "1", "\n", "active_logits", "=", "logits", ".", "view", "(", "-", "1", ",", "self", ".", "config", ".", "num_labels", ")", "[", "active_loss", "]", "\n", "active_labels", "=", "labels", ".", "view", "(", "-", "1", ")", "[", "active_loss", "]", "\n", "loss", "=", "loss_fct", "(", "active_logits", ",", "active_labels", ")", "\n", "", "else", ":", "\n", "                ", "loss", "=", "loss_fct", "(", "logits", ".", "view", "(", "-", "1", ",", "self", ".", "config", ".", "num_labels", ")", ",", "labels", ".", "view", "(", "-", "1", ")", ")", "\n", "\n", "", "", "if", "not", "return_dict", ":", "\n", "            ", "output", "=", "(", "logits", ",", ")", "+", "discriminator_hidden_states", "[", "1", ":", "]", "\n", "return", "(", "(", "loss", ",", ")", "+", "output", ")", "if", "loss", "is", "not", "None", "else", "output", "\n", "\n", "", "return", "TokenClassifierOutput", "(", "\n", "loss", "=", "loss", ",", "\n", "logits", "=", "logits", ",", "\n", "hidden_states", "=", "discriminator_hidden_states", ".", "hidden_states", ",", "\n", "attentions", "=", "discriminator_hidden_states", ".", "attentions", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.alibaba_self-augmentation-strategy.models.modeling_sas.SasForQuestionAnswering.__init__": [[2100, 2108], ["transformers.modeling_utils.PreTrainedModel.__init__", "modeling_sas.SasModel", "torch.Linear", "torch.Linear", "torch.Linear", "modeling_sas.SasForQuestionAnswering.init_weights"], "methods", ["home.repos.pwc.inspect_result.alibaba_self-augmentation-strategy.models.trainer_sas.SequentialDistributedSampler.__init__"], ["def", "__init__", "(", "self", ",", "config", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "config", ")", "\n", "self", ".", "num_labels", "=", "config", ".", "num_labels", "\n", "\n", "self", ".", "sas", "=", "SasModel", "(", "config", ")", "\n", "self", ".", "qa_outputs", "=", "nn", ".", "Linear", "(", "config", ".", "hidden_size", ",", "config", ".", "num_labels", ")", "\n", "\n", "self", ".", "init_weights", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.alibaba_self-augmentation-strategy.models.modeling_sas.SasForQuestionAnswering.forward": [[2109, 2192], ["transformers.file_utils.add_start_docstrings_to_model_forward", "transformers.file_utils.add_code_sample_docstrings", "dict", "modeling_sas.SasForQuestionAnswering.sas", "modeling_sas.SasForQuestionAnswering.qa_outputs", "modeling_sas.SasForQuestionAnswering.split", "start_logits.squeeze.squeeze.squeeze", "end_logits.squeeze.squeeze.squeeze", "transformers.modeling_outputs.QuestionAnsweringModelOutput", "SAS_INPUTS_DOCSTRING.format", "start_logits.squeeze.squeeze.size", "start_positions.squeeze.squeeze.clamp_", "end_positions.squeeze.squeeze.clamp_", "torch.nn.CrossEntropyLoss", "torch.nn.CrossEntropyLoss", "torch.nn.CrossEntropyLoss", "torch.nn.CrossEntropyLoss.", "torch.nn.CrossEntropyLoss.", "torch.nn.CrossEntropyLoss.", "torch.nn.CrossEntropyLoss.", "torch.nn.CrossEntropyLoss.", "torch.nn.CrossEntropyLoss.", "len", "start_positions.squeeze.squeeze.squeeze", "len", "end_positions.squeeze.squeeze.squeeze", "start_positions.squeeze.squeeze.size", "end_positions.squeeze.squeeze.size"], "methods", ["None"], ["", "@", "add_start_docstrings_to_model_forward", "(", "SAS_INPUTS_DOCSTRING", ".", "format", "(", "\"batch_size, sequence_length\"", ")", ")", "\n", "@", "add_code_sample_docstrings", "(", "\n", "tokenizer_class", "=", "_TOKENIZER_FOR_DOC", ",", "\n", "checkpoint", "=", "\"google/sas-small-discriminator\"", ",", "\n", "output_type", "=", "QuestionAnsweringModelOutput", ",", "\n", "config_class", "=", "_CONFIG_FOR_DOC", ",", "\n", ")", "\n", "def", "forward", "(", "\n", "self", ",", "\n", "input_ids", "=", "None", ",", "\n", "attention_mask", "=", "None", ",", "\n", "token_type_ids", "=", "None", ",", "\n", "position_ids", "=", "None", ",", "\n", "head_mask", "=", "None", ",", "\n", "inputs_embeds", "=", "None", ",", "\n", "start_positions", "=", "None", ",", "\n", "end_positions", "=", "None", ",", "\n", "output_attentions", "=", "None", ",", "\n", "output_hidden_states", "=", "None", ",", "\n", "side_info_sets", "=", "dict", "(", ")", ",", "\n", "return_dict", "=", "None", ",", "\n", ")", ":", "\n", "        ", "r\"\"\"\n        start_positions (:obj:`torch.LongTensor` of shape :obj:`(batch_size,)`, `optional`):\n            Labels for position (index) of the start of the labelled span for computing the token classification loss.\n            Positions are clamped to the length of the sequence (:obj:`sequence_length`). Position outside of the\n            sequence are not taken into account for computing the loss.\n        end_positions (:obj:`torch.LongTensor` of shape :obj:`(batch_size,)`, `optional`):\n            Labels for position (index) of the end of the labelled span for computing the token classification loss.\n            Positions are clamped to the length of the sequence (:obj:`sequence_length`). Position outside of the\n            sequence are not taken into account for computing the loss.\n        \"\"\"", "\n", "return_dict", "=", "return_dict", "if", "return_dict", "is", "not", "None", "else", "self", ".", "config", ".", "use_return_dict", "\n", "\n", "discriminator_hidden_states", "=", "self", ".", "sas", "(", "\n", "input_ids", ",", "\n", "attention_mask", "=", "attention_mask", ",", "\n", "token_type_ids", "=", "token_type_ids", ",", "\n", "position_ids", "=", "position_ids", ",", "\n", "head_mask", "=", "head_mask", ",", "\n", "inputs_embeds", "=", "inputs_embeds", ",", "\n", "output_attentions", "=", "output_attentions", ",", "\n", "output_hidden_states", "=", "output_hidden_states", ",", "\n", "side_info_sets", "=", "side_info_sets", ",", "\n", ")", "\n", "\n", "sequence_output", "=", "discriminator_hidden_states", "[", "0", "]", "\n", "\n", "logits", "=", "self", ".", "qa_outputs", "(", "sequence_output", ")", "\n", "start_logits", ",", "end_logits", "=", "logits", ".", "split", "(", "1", ",", "dim", "=", "-", "1", ")", "\n", "start_logits", "=", "start_logits", ".", "squeeze", "(", "-", "1", ")", "\n", "end_logits", "=", "end_logits", ".", "squeeze", "(", "-", "1", ")", "\n", "\n", "total_loss", "=", "None", "\n", "if", "start_positions", "is", "not", "None", "and", "end_positions", "is", "not", "None", ":", "\n", "# If we are on multi-GPU, split add a dimension", "\n", "            ", "if", "len", "(", "start_positions", ".", "size", "(", ")", ")", ">", "1", ":", "\n", "                ", "start_positions", "=", "start_positions", ".", "squeeze", "(", "-", "1", ")", "\n", "", "if", "len", "(", "end_positions", ".", "size", "(", ")", ")", ">", "1", ":", "\n", "                ", "end_positions", "=", "end_positions", ".", "squeeze", "(", "-", "1", ")", "\n", "# sometimes the start/end positions are outside our model inputs, we ignore these terms", "\n", "", "ignored_index", "=", "start_logits", ".", "size", "(", "1", ")", "\n", "start_positions", ".", "clamp_", "(", "0", ",", "ignored_index", ")", "\n", "end_positions", ".", "clamp_", "(", "0", ",", "ignored_index", ")", "\n", "\n", "loss_fct", "=", "CrossEntropyLoss", "(", "ignore_index", "=", "ignored_index", ")", "\n", "start_loss", "=", "loss_fct", "(", "start_logits", ",", "start_positions", ")", "\n", "end_loss", "=", "loss_fct", "(", "end_logits", ",", "end_positions", ")", "\n", "total_loss", "=", "(", "start_loss", "+", "end_loss", ")", "/", "2", "\n", "\n", "", "if", "not", "return_dict", ":", "\n", "            ", "output", "=", "(", "\n", "start_logits", ",", "\n", "end_logits", ",", "\n", ")", "+", "discriminator_hidden_states", "[", "1", ":", "]", "\n", "return", "(", "(", "total_loss", ",", ")", "+", "output", ")", "if", "total_loss", "is", "not", "None", "else", "output", "\n", "\n", "", "return", "QuestionAnsweringModelOutput", "(", "\n", "loss", "=", "total_loss", ",", "\n", "start_logits", "=", "start_logits", ",", "\n", "end_logits", "=", "end_logits", ",", "\n", "hidden_states", "=", "discriminator_hidden_states", ".", "hidden_states", ",", "\n", "attentions", "=", "discriminator_hidden_states", ".", "attentions", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.alibaba_self-augmentation-strategy.models.modeling_sas.SasForMultipleChoice.__init__": [[2203, 2211], ["transformers.modeling_utils.PreTrainedModel.__init__", "modeling_sas.SasModel", "transformers.modeling_utils.SequenceSummary", "torch.Linear", "torch.Linear", "torch.Linear", "modeling_sas.SasForMultipleChoice.init_weights"], "methods", ["home.repos.pwc.inspect_result.alibaba_self-augmentation-strategy.models.trainer_sas.SequentialDistributedSampler.__init__"], ["    ", "def", "__init__", "(", "self", ",", "config", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "config", ")", "\n", "\n", "self", ".", "sas", "=", "SasModel", "(", "config", ")", "\n", "self", ".", "sequence_summary", "=", "SequenceSummary", "(", "config", ")", "\n", "self", ".", "classifier", "=", "nn", ".", "Linear", "(", "config", ".", "hidden_size", ",", "1", ")", "\n", "\n", "self", ".", "init_weights", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.alibaba_self-augmentation-strategy.models.modeling_sas.SasForMultipleChoice.forward": [[2212, 2285], ["transformers.file_utils.add_start_docstrings_to_model_forward", "transformers.file_utils.add_code_sample_docstrings", "dict", "modeling_sas.SasForMultipleChoice.sas", "modeling_sas.SasForMultipleChoice.sequence_summary", "modeling_sas.SasForMultipleChoice.classifier", "modeling_sas.SasForMultipleChoice.view", "transformers.modeling_outputs.MultipleChoiceModelOutput", "SAS_INPUTS_DOCSTRING.format", "input_ids.view", "attention_mask.view", "token_type_ids.view", "position_ids.view", "inputs_embeds.view", "torch.nn.CrossEntropyLoss", "torch.nn.CrossEntropyLoss", "torch.nn.CrossEntropyLoss", "torch.nn.CrossEntropyLoss.", "torch.nn.CrossEntropyLoss.", "torch.nn.CrossEntropyLoss.", "input_ids.size", "attention_mask.size", "token_type_ids.size", "position_ids.size", "inputs_embeds.size", "inputs_embeds.size"], "methods", ["None"], ["", "@", "add_start_docstrings_to_model_forward", "(", "SAS_INPUTS_DOCSTRING", ".", "format", "(", "\"batch_size, num_choices, sequence_length\"", ")", ")", "\n", "@", "add_code_sample_docstrings", "(", "\n", "tokenizer_class", "=", "_TOKENIZER_FOR_DOC", ",", "\n", "checkpoint", "=", "\"google/sas-small-discriminator\"", ",", "\n", "output_type", "=", "MultipleChoiceModelOutput", ",", "\n", "config_class", "=", "_CONFIG_FOR_DOC", ",", "\n", ")", "\n", "def", "forward", "(", "\n", "self", ",", "\n", "input_ids", "=", "None", ",", "\n", "attention_mask", "=", "None", ",", "\n", "token_type_ids", "=", "None", ",", "\n", "position_ids", "=", "None", ",", "\n", "head_mask", "=", "None", ",", "\n", "inputs_embeds", "=", "None", ",", "\n", "labels", "=", "None", ",", "\n", "output_attentions", "=", "None", ",", "\n", "output_hidden_states", "=", "None", ",", "\n", "side_info_sets", "=", "dict", "(", ")", ",", "\n", "return_dict", "=", "None", ",", "\n", ")", ":", "\n", "        ", "r\"\"\"\n        labels (:obj:`torch.LongTensor` of shape :obj:`(batch_size,)`, `optional`):\n            Labels for computing the multiple choice classification loss. Indices should be in ``[0, ...,\n            num_choices-1]`` where :obj:`num_choices` is the size of the second dimension of the input tensors. (See\n            :obj:`input_ids` above)\n        \"\"\"", "\n", "return_dict", "=", "return_dict", "if", "return_dict", "is", "not", "None", "else", "self", ".", "config", ".", "use_return_dict", "\n", "num_choices", "=", "input_ids", ".", "shape", "[", "1", "]", "if", "input_ids", "is", "not", "None", "else", "inputs_embeds", ".", "shape", "[", "1", "]", "\n", "\n", "input_ids", "=", "input_ids", ".", "view", "(", "-", "1", ",", "input_ids", ".", "size", "(", "-", "1", ")", ")", "if", "input_ids", "is", "not", "None", "else", "None", "\n", "attention_mask", "=", "attention_mask", ".", "view", "(", "-", "1", ",", "attention_mask", ".", "size", "(", "-", "1", ")", ")", "if", "attention_mask", "is", "not", "None", "else", "None", "\n", "token_type_ids", "=", "token_type_ids", ".", "view", "(", "-", "1", ",", "token_type_ids", ".", "size", "(", "-", "1", ")", ")", "if", "token_type_ids", "is", "not", "None", "else", "None", "\n", "position_ids", "=", "position_ids", ".", "view", "(", "-", "1", ",", "position_ids", ".", "size", "(", "-", "1", ")", ")", "if", "position_ids", "is", "not", "None", "else", "None", "\n", "inputs_embeds", "=", "(", "\n", "inputs_embeds", ".", "view", "(", "-", "1", ",", "inputs_embeds", ".", "size", "(", "-", "2", ")", ",", "inputs_embeds", ".", "size", "(", "-", "1", ")", ")", "\n", "if", "inputs_embeds", "is", "not", "None", "\n", "else", "None", "\n", ")", "\n", "\n", "discriminator_hidden_states", "=", "self", ".", "sas", "(", "\n", "input_ids", ",", "\n", "attention_mask", "=", "attention_mask", ",", "\n", "token_type_ids", "=", "token_type_ids", ",", "\n", "position_ids", "=", "position_ids", ",", "\n", "head_mask", "=", "head_mask", ",", "\n", "inputs_embeds", "=", "inputs_embeds", ",", "\n", "output_attentions", "=", "output_attentions", ",", "\n", "output_hidden_states", "=", "output_hidden_states", ",", "\n", "side_info_sets", "=", "side_info_sets", ",", "\n", "return_dict", "=", "return_dict", ",", "\n", ")", "\n", "\n", "sequence_output", "=", "discriminator_hidden_states", "[", "0", "]", "\n", "\n", "pooled_output", "=", "self", ".", "sequence_summary", "(", "sequence_output", ")", "\n", "logits", "=", "self", ".", "classifier", "(", "pooled_output", ")", "\n", "reshaped_logits", "=", "logits", ".", "view", "(", "-", "1", ",", "num_choices", ")", "\n", "\n", "loss", "=", "None", "\n", "if", "labels", "is", "not", "None", ":", "\n", "            ", "loss_fct", "=", "CrossEntropyLoss", "(", ")", "\n", "loss", "=", "loss_fct", "(", "reshaped_logits", ",", "labels", ")", "\n", "\n", "", "if", "not", "return_dict", ":", "\n", "            ", "output", "=", "(", "reshaped_logits", ",", ")", "+", "discriminator_hidden_states", "[", "1", ":", "]", "\n", "return", "(", "(", "loss", ",", ")", "+", "output", ")", "if", "loss", "is", "not", "None", "else", "output", "\n", "\n", "", "return", "MultipleChoiceModelOutput", "(", "\n", "loss", "=", "loss", ",", "\n", "logits", "=", "reshaped_logits", ",", "\n", "hidden_states", "=", "discriminator_hidden_states", ".", "hidden_states", ",", "\n", "attentions", "=", "discriminator_hidden_states", ".", "attentions", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.alibaba_self-augmentation-strategy.models.trainer_sas.SasTrainerForFinetune.create_optimizer_and_scheduler": [[42, 47], ["super().create_optimizer_and_scheduler", "int"], "methods", ["home.repos.pwc.inspect_result.alibaba_self-augmentation-strategy.models.trainer_sas.SasTrainerForFinetune.create_optimizer_and_scheduler"], ["    ", "def", "create_optimizer_and_scheduler", "(", "self", ",", "num_training_steps", ":", "int", ")", ":", "\n", "        ", "if", "0", "<", "self", ".", "args", ".", "warmup_steps", "<", "1", ":", "\n", "            ", "self", ".", "args", ".", "warmup_steps", "=", "int", "(", "self", ".", "args", ".", "warmup_steps", "*", "num_training_steps", ")", "\n", "\n", "", "super", "(", ")", ".", "create_optimizer_and_scheduler", "(", "num_training_steps", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.alibaba_self-augmentation-strategy.models.trainer_sas.SasTrainerForFinetune._load_optimizer_and_scheduler": [[48, 62], ["os.path.isfile", "os.path.isfile", "trainer_sas.SasTrainerForFinetune.deepspeed.load_checkpoint", "os.path.join", "os.path.join"], "methods", ["None"], ["", "def", "_load_optimizer_and_scheduler", "(", "self", ",", "model_path", ")", ":", "\n", "        ", "\"\"\"If optimizer and scheduler states exist, load them.\"\"\"", "\n", "if", "model_path", "is", "None", ":", "\n", "            ", "return", "\n", "\n", "", "if", "os", ".", "path", ".", "isfile", "(", "os", ".", "path", ".", "join", "(", "model_path", ",", "\"optimizer.pt\"", ")", ")", "and", "os", ".", "path", ".", "isfile", "(", "\n", "os", ".", "path", ".", "join", "(", "model_path", ",", "\"scheduler.pt\"", ")", "\n", ")", ":", "\n", "# SAS-specific: Don't resume from the optiizer status of pretraining", "\n", "            ", "pass", "\n", "\n", "", "if", "self", ".", "deepspeed", ":", "\n", "# Not sure how to check if there is a saved deepspeed checkpoint, but since it just return None if it fails to find a deepspeed checkpoint this is sort of a check-n-load function", "\n", "            ", "self", ".", "deepspeed", ".", "load_checkpoint", "(", "model_path", ",", "load_optimizer_states", "=", "True", ",", "load_lr_scheduler_states", "=", "True", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.alibaba_self-augmentation-strategy.models.trainer_sas.SasTrainerForPretrain.get_train_dataloader": [[66, 82], ["torch.utils.data.dataloader.DataLoader", "torch.utils.data.dataloader.DataLoader", "torch.utils.data.sampler.SequentialSampler", "torch.utils.data.sampler.SequentialSampler", "trainer_sas.SequentialDistributedSampler"], "methods", ["None"], ["    ", "def", "get_train_dataloader", "(", "self", ")", "->", "DataLoader", ":", "\n", "        ", "train_sampler", "=", "(", "\n", "SequentialSampler", "(", "self", ".", "train_dataset", ")", "\n", "if", "self", ".", "args", ".", "local_rank", "==", "-", "1", "\n", "else", "SequentialDistributedSampler", "(", "self", ".", "train_dataset", ")", "\n", ")", "\n", "\n", "# print(train_sampler)", "\n", "\n", "return", "DataLoader", "(", "\n", "self", ".", "train_dataset", ",", "\n", "batch_size", "=", "self", ".", "args", ".", "train_batch_size", ",", "\n", "sampler", "=", "train_sampler", ",", "\n", "collate_fn", "=", "self", ".", "data_collator", ",", "\n", "drop_last", "=", "True", ",", "\n", "num_workers", "=", "self", ".", "args", ".", "dataloader_num_workers", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.alibaba_self-augmentation-strategy.models.trainer_sas.SasTrainerForPretrain._save_checkpoint": [[84, 94], ["super()._save_checkpoint", "trainer_sas.SasTrainerForPretrain.is_world_process_zero", "os.path.join", "torch.save", "torch.save", "torch.save", "torch.save", "torch.save", "torch.save", "torch.save", "torch.save", "os.path.join", "os.path.join"], "methods", ["home.repos.pwc.inspect_result.alibaba_self-augmentation-strategy.models.trainer_sas.SasTrainerForPretrain._save_checkpoint"], ["", "def", "_save_checkpoint", "(", "self", ",", "model", ",", "trial", ",", "metrics", "=", "None", ")", ":", "\n", "# Save model checkpoint", "\n", "        ", "super", "(", ")", ".", "_save_checkpoint", "(", "model", ",", "trial", ",", "metrics", ")", "\n", "\n", "if", "self", ".", "model", ".", "save_data_augmentation", ">", "0", "and", "self", ".", "is_world_process_zero", "(", ")", ":", "\n", "            ", "checkpoint_folder", "=", "f\"{PREFIX_CHECKPOINT_DIR}-{self.state.global_step}\"", "\n", "output_dir", "=", "os", ".", "path", ".", "join", "(", "self", ".", "args", ".", "output_dir", ",", "checkpoint_folder", ")", "\n", "\n", "torch", ".", "save", "(", "self", ".", "model", ".", "dataset", ".", "buffer_da", ",", "os", ".", "path", ".", "join", "(", "output_dir", ",", "\"data_augmentation.pt\"", ")", ")", "\n", "torch", ".", "save", "(", "self", ".", "model", ".", "dataset", ".", "buffer_da_flag", ",", "os", ".", "path", ".", "join", "(", "output_dir", ",", "\"data_augmentation_flag.pt\"", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.alibaba_self-augmentation-strategy.models.trainer_sas.SequentialDistributedSampler.__init__": [[107, 121], ["int", "torch.distributed.get_world_size", "torch.distributed.get_world_size", "torch.distributed.get_world_size", "torch.distributed.get_world_size", "torch.distributed.get_rank", "torch.distributed.get_rank", "torch.distributed.get_rank", "torch.distributed.get_rank", "math.ceil", "torch.distributed.is_available", "torch.distributed.is_available", "torch.distributed.is_available", "torch.distributed.is_available", "RuntimeError", "torch.distributed.is_available", "torch.distributed.is_available", "torch.distributed.is_available", "torch.distributed.is_available", "RuntimeError", "len"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "dataset", ",", "num_replicas", "=", "None", ",", "rank", "=", "None", ")", ":", "\n", "        ", "if", "num_replicas", "is", "None", ":", "\n", "            ", "if", "not", "torch", ".", "distributed", ".", "is_available", "(", ")", ":", "\n", "                ", "raise", "RuntimeError", "(", "\"Requires distributed package to be available\"", ")", "\n", "", "num_replicas", "=", "torch", ".", "distributed", ".", "get_world_size", "(", ")", "\n", "", "if", "rank", "is", "None", ":", "\n", "            ", "if", "not", "torch", ".", "distributed", ".", "is_available", "(", ")", ":", "\n", "                ", "raise", "RuntimeError", "(", "\"Requires distributed package to be available\"", ")", "\n", "", "rank", "=", "torch", ".", "distributed", ".", "get_rank", "(", ")", "\n", "", "self", ".", "dataset", "=", "dataset", "\n", "self", ".", "num_replicas", "=", "num_replicas", "\n", "self", ".", "rank", "=", "rank", "\n", "self", ".", "num_samples", "=", "int", "(", "math", ".", "ceil", "(", "len", "(", "self", ".", "dataset", ")", "*", "1.0", "/", "self", ".", "num_replicas", ")", ")", "\n", "self", ".", "total_size", "=", "self", ".", "num_samples", "*", "self", ".", "num_replicas", "\n", "\n"]], "home.repos.pwc.inspect_result.alibaba_self-augmentation-strategy.models.trainer_sas.SequentialDistributedSampler.__iter__": [[122, 134], ["list", "iter", "range", "len", "len", "len", "len"], "methods", ["None"], ["", "def", "__iter__", "(", "self", ")", ":", "\n", "        ", "indices", "=", "list", "(", "range", "(", "len", "(", "self", ".", "dataset", ")", ")", ")", "\n", "\n", "# add extra samples to make it evenly divisible", "\n", "indices", "+=", "indices", "[", ":", "(", "self", ".", "total_size", "-", "len", "(", "indices", ")", ")", "]", "\n", "assert", "len", "(", "indices", ")", "==", "self", ".", "total_size", "\n", "\n", "# subsample", "\n", "indices", "=", "indices", "[", "self", ".", "rank", "*", "self", ".", "num_samples", ":", "(", "self", ".", "rank", "+", "1", ")", "*", "self", ".", "num_samples", "]", "\n", "assert", "len", "(", "indices", ")", "==", "self", ".", "num_samples", "\n", "\n", "return", "iter", "(", "indices", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.alibaba_self-augmentation-strategy.models.trainer_sas.SequentialDistributedSampler.__len__": [[135, 137], ["None"], "methods", ["None"], ["", "def", "__len__", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "num_samples", "\n", "", "", ""]], "home.repos.pwc.inspect_result.alibaba_self-augmentation-strategy.models.trainer_sas.js_div": [[25, 31], ["torch.kl_div", "torch.kl_div", "p.log", "q.log"], "function", ["None"], ["def", "js_div", "(", "p", ",", "q", ")", ":", "\n", "    ", "m", "=", "(", "p", "+", "q", ")", "/", "2", "\n", "a", "=", "F", ".", "kl_div", "(", "p", ".", "log", "(", ")", ",", "m", ",", "reduction", "=", "'batchmean'", ")", "\n", "b", "=", "F", ".", "kl_div", "(", "q", ".", "log", "(", ")", ",", "m", ",", "reduction", "=", "'batchmean'", ")", "\n", "jsd", "=", "(", "(", "a", "+", "b", ")", "/", "2", ")", "\n", "return", "jsd", "\n", "", "@", "dataclass", "\n"]]}