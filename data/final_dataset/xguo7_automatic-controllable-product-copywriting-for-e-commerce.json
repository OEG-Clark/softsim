{"home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.pretraininglm_ecommerce.eval.load_file": [[9, 13], ["open", "f.readlines"], "function", ["None"], ["def", "load_file", "(", "path", ")", ":", "\n", "    ", "with", "open", "(", "path", ",", "\"r\"", ",", "encoding", "=", "\"utf-8\"", ")", "as", "f", ":", "\n", "        ", "content", "=", "f", ".", "readlines", "(", ")", "\n", "", "return", "content", "\n", "\n"]], "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.pretraininglm_ecommerce.eval.remove_blank": [[14, 16], ["l.replace"], "function", ["None"], ["", "def", "remove_blank", "(", "lines", ")", ":", "\n", "    ", "return", "[", "l", ".", "replace", "(", "\" \"", ",", "\"\"", ")", "for", "l", "in", "lines", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.pytorch_pretrained_bert.modeling.BertConfig.__init__": [[61, 132], ["isinstance", "json.loads.items", "isinstance", "open", "json.loads", "ValueError", "reader.read"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "\n", "vocab_size_or_config_json_file", ",", "\n", "hidden_size", "=", "768", ",", "\n", "num_hidden_layers", "=", "12", ",", "\n", "num_attention_heads", "=", "12", ",", "\n", "intermediate_size", "=", "3072", ",", "\n", "hidden_act", "=", "\"gelu\"", ",", "\n", "hidden_dropout_prob", "=", "0.1", ",", "\n", "attention_probs_dropout_prob", "=", "0.1", ",", "\n", "max_position_embeddings", "=", "512", ",", "\n", "type_vocab_size", "=", "2", ",", "\n", "relax_projection", "=", "0", ",", "\n", "new_pos_ids", "=", "False", ",", "\n", "initializer_range", "=", "0.02", ",", "\n", "task_idx", "=", "None", ",", "\n", "fp32_embedding", "=", "False", ",", "\n", "ffn_type", "=", "0", ",", "\n", "label_smoothing", "=", "None", ",", "\n", "num_qkv", "=", "0", ",", "\n", "seg_emb", "=", "False", ")", ":", "\n", "        ", "\"\"\"Constructs BertConfig.\n\n        Args:\n            vocab_size_or_config_json_file: Vocabulary size of `inputs_ids` in `BertModel`.\n            hidden_size: Size of the encoder layers and the pooler layer.\n            num_hidden_layers: Number of hidden layers in the Transformer encoder.\n            num_attention_heads: Number of attention heads for each attention layer in\n                the Transformer encoder.\n            intermediate_size: The size of the \"intermediate\" (i.e., feed-forward)\n                layer in the Transformer encoder.\n            hidden_act: The non-linear activation function (function or string) in the\n                encoder and pooler. If string, \"gelu\", \"relu\" and \"swish\" are supported.\n            hidden_dropout_prob: The dropout probabilitiy for all fully connected\n                layers in the embeddings, encoder, and pooler.\n            attention_probs_dropout_prob: The dropout ratio for the attention\n                probabilities.\n            max_position_embeddings: The maximum sequence length that this model might\n                ever be used with. Typically set this to something large just in case\n                (e.g., 512 or 1024 or 2048).\n            type_vocab_size: The vocabulary size of the `token_type_ids` passed into\n                `BertModel`.\n            initializer_range: The sttdev of the truncated_normal_initializer for\n                initializing all weight matrices.\n        \"\"\"", "\n", "if", "isinstance", "(", "vocab_size_or_config_json_file", ",", "str", ")", ":", "\n", "            ", "with", "open", "(", "vocab_size_or_config_json_file", ",", "\"r\"", ",", "encoding", "=", "'utf-8'", ")", "as", "reader", ":", "\n", "                ", "json_config", "=", "json", ".", "loads", "(", "reader", ".", "read", "(", ")", ")", "\n", "", "for", "key", ",", "value", "in", "json_config", ".", "items", "(", ")", ":", "\n", "                ", "self", ".", "__dict__", "[", "key", "]", "=", "value", "\n", "", "", "elif", "isinstance", "(", "vocab_size_or_config_json_file", ",", "int", ")", ":", "\n", "            ", "self", ".", "vocab_size", "=", "vocab_size_or_config_json_file", "\n", "self", ".", "hidden_size", "=", "hidden_size", "\n", "self", ".", "num_hidden_layers", "=", "num_hidden_layers", "\n", "self", ".", "num_attention_heads", "=", "num_attention_heads", "\n", "self", ".", "hidden_act", "=", "hidden_act", "\n", "self", ".", "intermediate_size", "=", "intermediate_size", "\n", "self", ".", "hidden_dropout_prob", "=", "hidden_dropout_prob", "\n", "self", ".", "attention_probs_dropout_prob", "=", "attention_probs_dropout_prob", "\n", "self", ".", "max_position_embeddings", "=", "max_position_embeddings", "\n", "self", ".", "type_vocab_size", "=", "type_vocab_size", "\n", "self", ".", "relax_projection", "=", "relax_projection", "\n", "self", ".", "new_pos_ids", "=", "new_pos_ids", "\n", "self", ".", "initializer_range", "=", "initializer_range", "\n", "self", ".", "task_idx", "=", "task_idx", "\n", "self", ".", "fp32_embedding", "=", "fp32_embedding", "\n", "self", ".", "ffn_type", "=", "ffn_type", "\n", "self", ".", "label_smoothing", "=", "label_smoothing", "\n", "self", ".", "num_qkv", "=", "num_qkv", "\n", "self", ".", "seg_emb", "=", "seg_emb", "\n", "", "else", ":", "\n", "            ", "raise", "ValueError", "(", "\"First argument must be either a vocabulary size (int)\"", "\n", "\"or the path to a pretrained model config file (str)\"", ")", "\n"]], "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.pytorch_pretrained_bert.modeling.BertConfig.from_dict": [[134, 141], ["modeling.BertConfig", "json_object.items"], "methods", ["None"], ["", "", "@", "classmethod", "\n", "def", "from_dict", "(", "cls", ",", "json_object", ")", ":", "\n", "        ", "\"\"\"Constructs a `BertConfig` from a Python dictionary of parameters.\"\"\"", "\n", "config", "=", "BertConfig", "(", "vocab_size_or_config_json_file", "=", "-", "1", ")", "\n", "for", "key", ",", "value", "in", "json_object", ".", "items", "(", ")", ":", "\n", "            ", "config", ".", "__dict__", "[", "key", "]", "=", "value", "\n", "", "return", "config", "\n", "\n"]], "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.pytorch_pretrained_bert.modeling.BertConfig.from_json_file": [[142, 148], ["cls.from_dict", "open", "reader.read", "json.loads"], "methods", ["home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.pytorch_pretrained_bert.modeling.BertConfig.from_dict"], ["", "@", "classmethod", "\n", "def", "from_json_file", "(", "cls", ",", "json_file", ")", ":", "\n", "        ", "\"\"\"Constructs a `BertConfig` from a json file of parameters.\"\"\"", "\n", "with", "open", "(", "json_file", ",", "\"r\"", ",", "encoding", "=", "'utf-8'", ")", "as", "reader", ":", "\n", "            ", "text", "=", "reader", ".", "read", "(", ")", "\n", "", "return", "cls", ".", "from_dict", "(", "json", ".", "loads", "(", "text", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.pytorch_pretrained_bert.modeling.BertConfig.__repr__": [[149, 151], ["str", "modeling.BertConfig.to_json_string"], "methods", ["home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.pytorch_pretrained_bert.modeling.BertConfig.to_json_string"], ["", "def", "__repr__", "(", "self", ")", ":", "\n", "        ", "return", "str", "(", "self", ".", "to_json_string", "(", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.pytorch_pretrained_bert.modeling.BertConfig.to_dict": [[152, 156], ["copy.deepcopy"], "methods", ["None"], ["", "def", "to_dict", "(", "self", ")", ":", "\n", "        ", "\"\"\"Serializes this instance to a Python dictionary.\"\"\"", "\n", "output", "=", "copy", ".", "deepcopy", "(", "self", ".", "__dict__", ")", "\n", "return", "output", "\n", "\n"]], "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.pytorch_pretrained_bert.modeling.BertConfig.to_json_string": [[157, 160], ["json.dumps", "modeling.BertConfig.to_dict"], "methods", ["home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.pytorch_pretrained_bert.modeling.BertConfig.to_dict"], ["", "def", "to_json_string", "(", "self", ")", ":", "\n", "        ", "\"\"\"Serializes this instance to a JSON string.\"\"\"", "\n", "return", "json", ".", "dumps", "(", "self", ".", "to_dict", "(", ")", ",", "indent", "=", "2", ",", "sort_keys", "=", "True", ")", "+", "\"\\n\"", "\n", "\n"]], "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.pytorch_pretrained_bert.modeling.BertLayerNorm.__init__": [[162, 169], ["torch.nn.Module.__init__", "torch.nn.Parameter", "torch.nn.Parameter", "torch.nn.Parameter", "torch.nn.Parameter", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros"], "methods", ["home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.unilm.eval.Eval.__init__"], ["    ", "def", "__init__", "(", "self", ",", "hidden_size", ",", "eps", "=", "1e-5", ")", ":", "\n", "        ", "\"\"\"Construct a layernorm module in the TF style (epsilon inside the square root).\n        \"\"\"", "\n", "super", "(", "BertLayerNorm", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "weight", "=", "nn", ".", "Parameter", "(", "torch", ".", "ones", "(", "hidden_size", ")", ")", "\n", "self", ".", "bias", "=", "nn", ".", "Parameter", "(", "torch", ".", "zeros", "(", "hidden_size", ")", ")", "\n", "self", ".", "variance_epsilon", "=", "eps", "\n", "\n"]], "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.pytorch_pretrained_bert.modeling.BertLayerNorm.forward": [[170, 175], ["x.mean", "torch.sqrt", "torch.sqrt", "torch.sqrt", "torch.sqrt"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "u", "=", "x", ".", "mean", "(", "-", "1", ",", "keepdim", "=", "True", ")", "\n", "s", "=", "(", "x", "-", "u", ")", ".", "pow", "(", "2", ")", ".", "mean", "(", "-", "1", ",", "keepdim", "=", "True", ")", "\n", "x", "=", "(", "x", "-", "u", ")", "/", "torch", ".", "sqrt", "(", "s", "+", "self", ".", "variance_epsilon", ")", "\n", "return", "self", ".", "weight", "*", "x", "+", "self", ".", "bias", "\n", "\n"]], "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.pytorch_pretrained_bert.modeling.PositionalEmbedding.__init__": [[198, 205], ["torch.nn.Module.__init__", "modeling.PositionalEmbedding.register_buffer", "torch.arange", "torch.arange", "torch.arange", "torch.arange"], "methods", ["home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.unilm.eval.Eval.__init__"], ["    ", "def", "__init__", "(", "self", ",", "demb", ")", ":", "\n", "        ", "super", "(", "PositionalEmbedding", ",", "self", ")", ".", "__init__", "(", ")", "\n", "\n", "self", ".", "demb", "=", "demb", "\n", "\n", "inv_freq", "=", "1", "/", "(", "10000", "**", "(", "torch", ".", "arange", "(", "0.0", ",", "demb", ",", "2.0", ")", "/", "demb", ")", ")", "\n", "self", ".", "register_buffer", "(", "'inv_freq'", ",", "inv_freq", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.pytorch_pretrained_bert.modeling.PositionalEmbedding.forward": [[206, 214], ["torch.ger", "torch.ger", "torch.ger", "torch.ger", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "pos_emb[].expand", "torch.ger.sin", "torch.ger.sin", "torch.ger.cos", "torch.ger.cos"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "pos_seq", ",", "bsz", "=", "None", ")", ":", "\n", "        ", "sinusoid_inp", "=", "torch", ".", "ger", "(", "pos_seq", ",", "self", ".", "inv_freq", ")", "\n", "pos_emb", "=", "torch", ".", "cat", "(", "[", "sinusoid_inp", ".", "sin", "(", ")", ",", "sinusoid_inp", ".", "cos", "(", ")", "]", ",", "dim", "=", "-", "1", ")", "\n", "\n", "if", "bsz", "is", "not", "None", ":", "\n", "            ", "return", "pos_emb", "[", ":", ",", "None", ",", ":", "]", ".", "expand", "(", "-", "1", ",", "bsz", ",", "-", "1", ")", "\n", "", "else", ":", "\n", "            ", "return", "pos_emb", "[", ":", ",", "None", ",", ":", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.pytorch_pretrained_bert.modeling.BertEmbeddings.__init__": [[220, 242], ["torch.nn.Module.__init__", "torch.nn.Embedding", "torch.nn.Embedding", "torch.nn.Embedding", "torch.nn.Embedding", "hasattr", "torch.nn.Embedding", "torch.nn.Embedding", "modeling.BertLayerNorm", "torch.nn.Dropout", "torch.nn.Dropout", "hasattr"], "methods", ["home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.unilm.eval.Eval.__init__"], ["def", "__init__", "(", "self", ",", "config", ")", ":", "\n", "        ", "super", "(", "BertEmbeddings", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "word_embeddings", "=", "nn", ".", "Embedding", "(", "\n", "config", ".", "vocab_size", ",", "config", ".", "hidden_size", ")", "\n", "self", ".", "token_type_embeddings", "=", "nn", ".", "Embedding", "(", "\n", "config", ".", "type_vocab_size", ",", "config", ".", "hidden_size", ")", "\n", "if", "hasattr", "(", "config", ",", "'fp32_embedding'", ")", ":", "\n", "            ", "self", ".", "fp32_embedding", "=", "config", ".", "fp32_embedding", "\n", "", "else", ":", "\n", "            ", "self", ".", "fp32_embedding", "=", "False", "\n", "\n", "", "if", "hasattr", "(", "config", ",", "'new_pos_ids'", ")", "and", "config", ".", "new_pos_ids", ":", "\n", "            ", "self", ".", "num_pos_emb", "=", "4", "\n", "", "else", ":", "\n", "            ", "self", ".", "num_pos_emb", "=", "1", "\n", "", "self", ".", "position_embeddings", "=", "nn", ".", "Embedding", "(", "\n", "config", ".", "max_position_embeddings", ",", "config", ".", "hidden_size", "*", "self", ".", "num_pos_emb", ")", "\n", "\n", "# self.LayerNorm is not snake-cased to stick with TensorFlow model variable name and be able to load", "\n", "# any TensorFlow checkpoint file", "\n", "self", ".", "LayerNorm", "=", "BertLayerNorm", "(", "config", ".", "hidden_size", ",", "eps", "=", "1e-5", ")", "\n", "self", ".", "dropout", "=", "nn", ".", "Dropout", "(", "config", ".", "hidden_dropout_prob", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.pytorch_pretrained_bert.modeling.BertEmbeddings.forward": [[243, 268], ["input_ids.size", "modeling.BertEmbeddings.word_embeddings", "modeling.BertEmbeddings.position_embeddings", "modeling.BertEmbeddings.token_type_embeddings", "modeling.BertEmbeddings.LayerNorm", "modeling.BertEmbeddings.dropout", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "position_ids.unsqueeze().expand_as.unsqueeze().expand_as.unsqueeze().expand_as", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "modeling.BertEmbeddings.size", "modeling.BertEmbeddings.size", "embeddings.half.half.half", "modeling.BertEmbeddings.view", "position_ids.unsqueeze().expand_as.unsqueeze().expand_as.unsqueeze", "torch.arange().long", "torch.arange().long", "torch.arange().long", "torch.arange().long", "torch.arange", "torch.arange", "torch.arange", "torch.arange"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "input_ids", ",", "token_type_ids", "=", "None", ",", "position_ids", "=", "None", ",", "task_idx", "=", "None", ")", ":", "\n", "        ", "seq_length", "=", "input_ids", ".", "size", "(", "1", ")", "\n", "if", "position_ids", "is", "None", ":", "\n", "            ", "position_ids", "=", "torch", ".", "arange", "(", "\n", "seq_length", ",", "dtype", "=", "torch", ".", "long", ",", "device", "=", "input_ids", ".", "device", ")", "\n", "position_ids", "=", "position_ids", ".", "unsqueeze", "(", "0", ")", ".", "expand_as", "(", "input_ids", ")", "\n", "", "if", "token_type_ids", "is", "None", ":", "\n", "            ", "token_type_ids", "=", "torch", ".", "zeros_like", "(", "input_ids", ")", "\n", "\n", "", "words_embeddings", "=", "self", ".", "word_embeddings", "(", "input_ids", ")", "\n", "position_embeddings", "=", "self", ".", "position_embeddings", "(", "position_ids", ")", "\n", "token_type_embeddings", "=", "self", ".", "token_type_embeddings", "(", "token_type_ids", ")", "\n", "\n", "if", "self", ".", "num_pos_emb", ">", "1", ":", "\n", "            ", "num_batch", "=", "position_embeddings", ".", "size", "(", "0", ")", "\n", "num_pos", "=", "position_embeddings", ".", "size", "(", "1", ")", "\n", "position_embeddings", "=", "position_embeddings", ".", "view", "(", "\n", "num_batch", ",", "num_pos", ",", "self", ".", "num_pos_emb", ",", "-", "1", ")", "[", "torch", ".", "arange", "(", "0", ",", "num_batch", ")", ".", "long", "(", ")", ",", ":", ",", "task_idx", ",", ":", "]", "\n", "\n", "", "embeddings", "=", "words_embeddings", "+", "position_embeddings", "+", "token_type_embeddings", "\n", "if", "self", ".", "fp32_embedding", ":", "\n", "            ", "embeddings", "=", "embeddings", ".", "half", "(", ")", "\n", "", "embeddings", "=", "self", ".", "LayerNorm", "(", "embeddings", ")", "\n", "embeddings", "=", "self", ".", "dropout", "(", "embeddings", ")", "\n", "return", "embeddings", "\n", "\n"]], "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.pytorch_pretrained_bert.modeling.BertSelfAttention.__init__": [[271, 309], ["torch.nn.Module.__init__", "int", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Dropout", "torch.nn.Dropout", "ValueError", "hasattr", "os.getenv", "modeling.BertSelfAttention.register_buffer", "hasattr", "torch.nn.Parameter", "torch.nn.Parameter", "torch.nn.Embedding", "torch.nn.Embedding", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros"], "methods", ["home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.unilm.eval.Eval.__init__"], ["    ", "def", "__init__", "(", "self", ",", "config", ")", ":", "\n", "        ", "super", "(", "BertSelfAttention", ",", "self", ")", ".", "__init__", "(", ")", "\n", "if", "config", ".", "hidden_size", "%", "config", ".", "num_attention_heads", "!=", "0", ":", "\n", "            ", "raise", "ValueError", "(", "\n", "\"The hidden size (%d) is not a multiple of the number of attention \"", "\n", "\"heads (%d)\"", "%", "(", "config", ".", "hidden_size", ",", "config", ".", "num_attention_heads", ")", ")", "\n", "", "self", ".", "num_attention_heads", "=", "config", ".", "num_attention_heads", "\n", "self", ".", "attention_head_size", "=", "int", "(", "\n", "config", ".", "hidden_size", "/", "config", ".", "num_attention_heads", ")", "\n", "self", ".", "all_head_size", "=", "self", ".", "num_attention_heads", "*", "self", ".", "attention_head_size", "\n", "\n", "if", "hasattr", "(", "config", ",", "'num_qkv'", ")", "and", "(", "config", ".", "num_qkv", ">", "1", ")", ":", "\n", "            ", "self", ".", "num_qkv", "=", "config", ".", "num_qkv", "\n", "", "else", ":", "\n", "            ", "self", ".", "num_qkv", "=", "1", "\n", "\n", "", "self", ".", "query", "=", "nn", ".", "Linear", "(", "\n", "config", ".", "hidden_size", ",", "self", ".", "all_head_size", "*", "self", ".", "num_qkv", ")", "\n", "self", ".", "key", "=", "nn", ".", "Linear", "(", "config", ".", "hidden_size", ",", "\n", "self", ".", "all_head_size", "*", "self", ".", "num_qkv", ")", "\n", "self", ".", "value", "=", "nn", ".", "Linear", "(", "\n", "config", ".", "hidden_size", ",", "self", ".", "all_head_size", "*", "self", ".", "num_qkv", ")", "\n", "\n", "self", ".", "dropout", "=", "nn", ".", "Dropout", "(", "config", ".", "attention_probs_dropout_prob", ")", "\n", "\n", "self", ".", "uni_debug_flag", "=", "True", "if", "os", ".", "getenv", "(", "\n", "'UNI_DEBUG_FLAG'", ",", "''", ")", "else", "False", "\n", "if", "self", ".", "uni_debug_flag", ":", "\n", "            ", "self", ".", "register_buffer", "(", "'debug_attention_probs'", ",", "\n", "torch", ".", "zeros", "(", "(", "512", ",", "512", ")", ")", ")", "\n", "", "if", "hasattr", "(", "config", ",", "'seg_emb'", ")", "and", "config", ".", "seg_emb", ":", "\n", "            ", "self", ".", "b_q_s", "=", "nn", ".", "Parameter", "(", "torch", ".", "zeros", "(", "\n", "1", ",", "self", ".", "num_attention_heads", ",", "1", ",", "self", ".", "attention_head_size", ")", ")", "\n", "self", ".", "seg_emb", "=", "nn", ".", "Embedding", "(", "\n", "config", ".", "type_vocab_size", ",", "self", ".", "all_head_size", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "b_q_s", "=", "None", "\n", "self", ".", "seg_emb", "=", "None", "\n", "\n"]], "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.pytorch_pretrained_bert.modeling.BertSelfAttention.transpose_for_scores": [[310, 334], ["x.gather().squeeze.gather().squeeze.permute", "x.gather().squeeze.gather().squeeze.view", "x.gather().squeeze.gather().squeeze.view", "isinstance", "x.gather().squeeze.gather().squeeze.size", "x.gather().squeeze.gather().squeeze.gather().squeeze", "x.gather().squeeze.gather().squeeze.size", "mask_qkv.size", "x.gather().squeeze.gather().squeeze.gather", "mask_qkv.view().expand", "mask_qkv.view"], "methods", ["None"], ["", "", "def", "transpose_for_scores", "(", "self", ",", "x", ",", "mask_qkv", "=", "None", ")", ":", "\n", "        ", "if", "self", ".", "num_qkv", ">", "1", ":", "\n", "            ", "sz", "=", "x", ".", "size", "(", ")", "[", ":", "-", "1", "]", "+", "(", "self", ".", "num_qkv", ",", "\n", "self", ".", "num_attention_heads", ",", "self", ".", "all_head_size", ")", "\n", "# (batch, pos, num_qkv, head, head_hid)", "\n", "x", "=", "x", ".", "view", "(", "*", "sz", ")", "\n", "if", "mask_qkv", "is", "None", ":", "\n", "                ", "x", "=", "x", "[", ":", ",", ":", ",", "0", ",", ":", ",", ":", "]", "\n", "", "elif", "isinstance", "(", "mask_qkv", ",", "int", ")", ":", "\n", "                ", "x", "=", "x", "[", ":", ",", ":", ",", "mask_qkv", ",", ":", ",", ":", "]", "\n", "", "else", ":", "\n", "# mask_qkv: (batch, pos)", "\n", "                ", "if", "mask_qkv", ".", "size", "(", "1", ")", ">", "sz", "[", "1", "]", ":", "\n", "                    ", "mask_qkv", "=", "mask_qkv", "[", ":", ",", ":", "sz", "[", "1", "]", "]", "\n", "# -> x: (batch, pos, head, head_hid)", "\n", "", "x", "=", "x", ".", "gather", "(", "2", ",", "mask_qkv", ".", "view", "(", "sz", "[", "0", "]", ",", "sz", "[", "1", "]", ",", "1", ",", "1", ",", "1", ")", ".", "expand", "(", "\n", "sz", "[", "0", "]", ",", "sz", "[", "1", "]", ",", "1", ",", "sz", "[", "3", "]", ",", "sz", "[", "4", "]", ")", ")", ".", "squeeze", "(", "2", ")", "\n", "", "", "else", ":", "\n", "            ", "sz", "=", "x", ".", "size", "(", ")", "[", ":", "-", "1", "]", "+", "(", "self", ".", "num_attention_heads", ",", "\n", "self", ".", "attention_head_size", ")", "\n", "# (batch, pos, head, head_hid)", "\n", "x", "=", "x", ".", "view", "(", "*", "sz", ")", "\n", "# (batch, head, pos, head_hid)", "\n", "", "return", "x", ".", "permute", "(", "0", ",", "2", ",", "1", ",", "3", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.pytorch_pretrained_bert.modeling.BertSelfAttention.forward": [[335, 387], ["modeling.BertSelfAttention.transpose_for_scores", "modeling.BertSelfAttention.transpose_for_scores", "modeling.BertSelfAttention.transpose_for_scores", "torch.matmul", "torch.matmul", "torch.matmul", "torch.matmul", "modeling.BertSelfAttention.dropout", "torch.matmul", "torch.matmul", "torch.matmul", "torch.matmul", "context_layer.view.view.permute().contiguous", "context_layer.view.view.view", "modeling.BertSelfAttention.query", "modeling.BertSelfAttention.key", "modeling.BertSelfAttention.value", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "modeling.BertSelfAttention.query", "modeling.BertSelfAttention.key", "modeling.BertSelfAttention.value", "modeling.BertSelfAttention.transpose", "modeling.BertSelfAttention.seg_emb", "seg_rep.view.view.view", "torch.einsum", "torch.einsum", "torch.einsum", "torch.einsum", "torch.nn.Softmax", "torch.nn.Softmax", "modeling.BertSelfAttention.size", "modeling.BertSelfAttention.debug_attention_probs[].copy_", "math.sqrt", "seg_rep.view.view.size", "seg_rep.view.view.size", "attention_probs[].mean().view", "context_layer.view.view.permute", "context_layer.view.view.size", "attention_probs[].mean"], "methods", ["home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.pytorch_pretrained_bert.modeling.BertSelfAttention.transpose_for_scores", "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.pytorch_pretrained_bert.modeling.BertSelfAttention.transpose_for_scores", "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.pytorch_pretrained_bert.modeling.BertSelfAttention.transpose_for_scores"], ["", "def", "forward", "(", "self", ",", "hidden_states", ",", "attention_mask", ",", "history_states", "=", "None", ",", "mask_qkv", "=", "None", ",", "seg_ids", "=", "None", ")", ":", "\n", "        ", "if", "history_states", "is", "None", ":", "\n", "            ", "mixed_query_layer", "=", "self", ".", "query", "(", "hidden_states", ")", "\n", "mixed_key_layer", "=", "self", ".", "key", "(", "hidden_states", ")", "\n", "mixed_value_layer", "=", "self", ".", "value", "(", "hidden_states", ")", "\n", "", "else", ":", "\n", "            ", "x_states", "=", "torch", ".", "cat", "(", "(", "history_states", ",", "hidden_states", ")", ",", "dim", "=", "1", ")", "\n", "mixed_query_layer", "=", "self", ".", "query", "(", "hidden_states", ")", "\n", "mixed_key_layer", "=", "self", ".", "key", "(", "x_states", ")", "\n", "mixed_value_layer", "=", "self", ".", "value", "(", "x_states", ")", "\n", "\n", "", "query_layer", "=", "self", ".", "transpose_for_scores", "(", "mixed_query_layer", ",", "mask_qkv", ")", "\n", "key_layer", "=", "self", ".", "transpose_for_scores", "(", "mixed_key_layer", ",", "mask_qkv", ")", "\n", "value_layer", "=", "self", ".", "transpose_for_scores", "(", "mixed_value_layer", ",", "mask_qkv", ")", "\n", "\n", "# Take the dot product between \"query\" and \"key\" to get the raw attention scores.", "\n", "# (batch, head, pos, pos)", "\n", "attention_scores", "=", "torch", ".", "matmul", "(", "\n", "query_layer", "/", "math", ".", "sqrt", "(", "self", ".", "attention_head_size", ")", ",", "key_layer", ".", "transpose", "(", "-", "1", ",", "-", "2", ")", ")", "\n", "\n", "if", "self", ".", "seg_emb", "is", "not", "None", ":", "\n", "            ", "seg_rep", "=", "self", ".", "seg_emb", "(", "seg_ids", ")", "\n", "# (batch, pos, head, head_hid)", "\n", "seg_rep", "=", "seg_rep", ".", "view", "(", "seg_rep", ".", "size", "(", "0", ")", ",", "seg_rep", ".", "size", "(", "\n", "1", ")", ",", "self", ".", "num_attention_heads", ",", "self", ".", "attention_head_size", ")", "\n", "qs", "=", "torch", ".", "einsum", "(", "'bnih,bjnh->bnij'", ",", "\n", "query_layer", "+", "self", ".", "b_q_s", ",", "seg_rep", ")", "\n", "attention_scores", "=", "attention_scores", "+", "qs", "\n", "\n", "# attention_scores = attention_scores / math.sqrt(self.attention_head_size)", "\n", "\n", "# Apply the attention mask is (precomputed for all layers in BertModel forward() function)", "\n", "", "attention_scores", "=", "attention_scores", "+", "attention_mask", "\n", "\n", "# Normalize the attention scores to probabilities.", "\n", "attention_probs", "=", "nn", ".", "Softmax", "(", "dim", "=", "-", "1", ")", "(", "attention_scores", ")", "\n", "\n", "if", "self", ".", "uni_debug_flag", ":", "\n", "            ", "_pos", "=", "attention_probs", ".", "size", "(", "-", "1", ")", "\n", "self", ".", "debug_attention_probs", "[", ":", "_pos", ",", ":", "_pos", "]", ".", "copy_", "(", "\n", "attention_probs", "[", "0", "]", ".", "mean", "(", "0", ")", ".", "view", "(", "_pos", ",", "_pos", ")", ")", "\n", "\n", "# This is actually dropping out entire tokens to attend to, which might", "\n", "# seem a bit unusual, but is taken from the original Transformer paper.", "\n", "", "attention_probs", "=", "self", ".", "dropout", "(", "attention_probs", ")", "\n", "\n", "context_layer", "=", "torch", ".", "matmul", "(", "attention_probs", ",", "value_layer", ")", "\n", "context_layer", "=", "context_layer", ".", "permute", "(", "0", ",", "2", ",", "1", ",", "3", ")", ".", "contiguous", "(", ")", "\n", "new_context_layer_shape", "=", "context_layer", ".", "size", "(", ")", "[", "\n", ":", "-", "2", "]", "+", "(", "self", ".", "all_head_size", ",", ")", "\n", "context_layer", "=", "context_layer", ".", "view", "(", "*", "new_context_layer_shape", ")", "\n", "return", "context_layer", "\n", "\n"]], "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.pytorch_pretrained_bert.modeling.BertSelfOutput.__init__": [[390, 395], ["torch.nn.Module.__init__", "torch.nn.Linear", "torch.nn.Linear", "modeling.BertLayerNorm", "torch.nn.Dropout", "torch.nn.Dropout"], "methods", ["home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.unilm.eval.Eval.__init__"], ["    ", "def", "__init__", "(", "self", ",", "config", ")", ":", "\n", "        ", "super", "(", "BertSelfOutput", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "dense", "=", "nn", ".", "Linear", "(", "config", ".", "hidden_size", ",", "config", ".", "hidden_size", ")", "\n", "self", ".", "LayerNorm", "=", "BertLayerNorm", "(", "config", ".", "hidden_size", ",", "eps", "=", "1e-5", ")", "\n", "self", ".", "dropout", "=", "nn", ".", "Dropout", "(", "config", ".", "hidden_dropout_prob", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.pytorch_pretrained_bert.modeling.BertSelfOutput.forward": [[396, 401], ["modeling.BertSelfOutput.dense", "modeling.BertSelfOutput.dropout", "modeling.BertSelfOutput.LayerNorm"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "hidden_states", ",", "input_tensor", ")", ":", "\n", "        ", "hidden_states", "=", "self", ".", "dense", "(", "hidden_states", ")", "\n", "hidden_states", "=", "self", ".", "dropout", "(", "hidden_states", ")", "\n", "hidden_states", "=", "self", ".", "LayerNorm", "(", "hidden_states", "+", "input_tensor", ")", "\n", "return", "hidden_states", "\n", "\n"]], "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.pytorch_pretrained_bert.modeling.BertAttention.__init__": [[404, 408], ["torch.nn.Module.__init__", "modeling.BertSelfAttention", "modeling.BertSelfOutput"], "methods", ["home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.unilm.eval.Eval.__init__"], ["    ", "def", "__init__", "(", "self", ",", "config", ")", ":", "\n", "        ", "super", "(", "BertAttention", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "self", "=", "BertSelfAttention", "(", "config", ")", "\n", "self", ".", "output", "=", "BertSelfOutput", "(", "config", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.pytorch_pretrained_bert.modeling.BertAttention.forward": [[409, 414], ["modeling.BertAttention.self", "modeling.BertAttention.output"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "input_tensor", ",", "attention_mask", ",", "history_states", "=", "None", ",", "mask_qkv", "=", "None", ",", "seg_ids", "=", "None", ")", ":", "\n", "        ", "self_output", "=", "self", ".", "self", "(", "\n", "input_tensor", ",", "attention_mask", ",", "history_states", "=", "history_states", ",", "mask_qkv", "=", "mask_qkv", ",", "seg_ids", "=", "seg_ids", ")", "\n", "attention_output", "=", "self", ".", "output", "(", "self_output", ",", "input_tensor", ")", "\n", "return", "attention_output", "\n", "\n"]], "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.pytorch_pretrained_bert.modeling.BertIntermediate.__init__": [[417, 422], ["torch.nn.Module.__init__", "torch.nn.Linear", "torch.nn.Linear", "isinstance"], "methods", ["home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.unilm.eval.Eval.__init__"], ["    ", "def", "__init__", "(", "self", ",", "config", ")", ":", "\n", "        ", "super", "(", "BertIntermediate", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "dense", "=", "nn", ".", "Linear", "(", "config", ".", "hidden_size", ",", "config", ".", "intermediate_size", ")", "\n", "self", ".", "intermediate_act_fn", "=", "ACT2FN", "[", "config", ".", "hidden_act", "]", "if", "isinstance", "(", "config", ".", "hidden_act", ",", "str", ")", "else", "config", ".", "hidden_act", "\n", "\n"]], "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.pytorch_pretrained_bert.modeling.BertIntermediate.forward": [[423, 427], ["modeling.BertIntermediate.dense", "modeling.BertIntermediate.intermediate_act_fn"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "hidden_states", ")", ":", "\n", "        ", "hidden_states", "=", "self", ".", "dense", "(", "hidden_states", ")", "\n", "hidden_states", "=", "self", ".", "intermediate_act_fn", "(", "hidden_states", ")", "\n", "return", "hidden_states", "\n", "\n"]], "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.pytorch_pretrained_bert.modeling.BertOutput.__init__": [[430, 435], ["torch.nn.Module.__init__", "torch.nn.Linear", "torch.nn.Linear", "modeling.BertLayerNorm", "torch.nn.Dropout", "torch.nn.Dropout"], "methods", ["home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.unilm.eval.Eval.__init__"], ["    ", "def", "__init__", "(", "self", ",", "config", ")", ":", "\n", "        ", "super", "(", "BertOutput", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "dense", "=", "nn", ".", "Linear", "(", "config", ".", "intermediate_size", ",", "config", ".", "hidden_size", ")", "\n", "self", ".", "LayerNorm", "=", "BertLayerNorm", "(", "config", ".", "hidden_size", ",", "eps", "=", "1e-5", ")", "\n", "self", ".", "dropout", "=", "nn", ".", "Dropout", "(", "config", ".", "hidden_dropout_prob", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.pytorch_pretrained_bert.modeling.BertOutput.forward": [[436, 441], ["modeling.BertOutput.dense", "modeling.BertOutput.dropout", "modeling.BertOutput.LayerNorm"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "hidden_states", ",", "input_tensor", ")", ":", "\n", "        ", "hidden_states", "=", "self", ".", "dense", "(", "hidden_states", ")", "\n", "hidden_states", "=", "self", ".", "dropout", "(", "hidden_states", ")", "\n", "hidden_states", "=", "self", ".", "LayerNorm", "(", "hidden_states", "+", "input_tensor", ")", "\n", "return", "hidden_states", "\n", "\n"]], "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.pytorch_pretrained_bert.modeling.TransformerFFN.__init__": [[444, 456], ["torch.nn.Module.__init__", "modeling.BertLayerNorm", "torch.nn.Dropout", "torch.nn.Dropout", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear"], "methods", ["home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.unilm.eval.Eval.__init__"], ["    ", "def", "__init__", "(", "self", ",", "config", ")", ":", "\n", "        ", "super", "(", "TransformerFFN", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "ffn_type", "=", "config", ".", "ffn_type", "\n", "assert", "self", ".", "ffn_type", "in", "(", "1", ",", "2", ")", "\n", "if", "self", ".", "ffn_type", "in", "(", "1", ",", "2", ")", ":", "\n", "            ", "self", ".", "wx0", "=", "nn", ".", "Linear", "(", "config", ".", "hidden_size", ",", "config", ".", "hidden_size", ")", "\n", "", "if", "self", ".", "ffn_type", "in", "(", "2", ",", ")", ":", "\n", "            ", "self", ".", "wx1", "=", "nn", ".", "Linear", "(", "config", ".", "hidden_size", ",", "config", ".", "hidden_size", ")", "\n", "", "if", "self", ".", "ffn_type", "in", "(", "1", ",", "2", ")", ":", "\n", "            ", "self", ".", "output", "=", "nn", ".", "Linear", "(", "config", ".", "hidden_size", ",", "config", ".", "hidden_size", ")", "\n", "", "self", ".", "LayerNorm", "=", "BertLayerNorm", "(", "config", ".", "hidden_size", ",", "eps", "=", "1e-5", ")", "\n", "self", ".", "dropout", "=", "nn", ".", "Dropout", "(", "config", ".", "hidden_dropout_prob", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.pytorch_pretrained_bert.modeling.TransformerFFN.forward": [[457, 468], ["modeling.TransformerFFN.dropout", "modeling.TransformerFFN.LayerNorm", "modeling.TransformerFFN.wx0", "modeling.TransformerFFN.output", "modeling.TransformerFFN.wx1"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "if", "self", ".", "ffn_type", "in", "(", "1", ",", "2", ")", ":", "\n", "            ", "x0", "=", "self", ".", "wx0", "(", "x", ")", "\n", "if", "self", ".", "ffn_type", "==", "1", ":", "\n", "                ", "x1", "=", "x", "\n", "", "elif", "self", ".", "ffn_type", "==", "2", ":", "\n", "                ", "x1", "=", "self", ".", "wx1", "(", "x", ")", "\n", "", "out", "=", "self", ".", "output", "(", "x0", "*", "x1", ")", "\n", "", "out", "=", "self", ".", "dropout", "(", "out", ")", "\n", "out", "=", "self", ".", "LayerNorm", "(", "out", "+", "x", ")", "\n", "return", "out", "\n", "\n"]], "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.pytorch_pretrained_bert.modeling.BertLayer.__init__": [[471, 480], ["torch.nn.Module.__init__", "modeling.BertAttention", "modeling.TransformerFFN", "modeling.BertIntermediate", "modeling.BertOutput"], "methods", ["home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.unilm.eval.Eval.__init__"], ["    ", "def", "__init__", "(", "self", ",", "config", ")", ":", "\n", "        ", "super", "(", "BertLayer", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "attention", "=", "BertAttention", "(", "config", ")", "\n", "self", ".", "ffn_type", "=", "config", ".", "ffn_type", "\n", "if", "self", ".", "ffn_type", ":", "\n", "            ", "self", ".", "ffn", "=", "TransformerFFN", "(", "config", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "intermediate", "=", "BertIntermediate", "(", "config", ")", "\n", "self", ".", "output", "=", "BertOutput", "(", "config", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.pytorch_pretrained_bert.modeling.BertLayer.forward": [[481, 490], ["modeling.BertLayer.attention", "modeling.BertLayer.ffn", "modeling.BertLayer.intermediate", "modeling.BertLayer.output"], "methods", ["None"], ["", "", "def", "forward", "(", "self", ",", "hidden_states", ",", "attention_mask", ",", "history_states", "=", "None", ",", "mask_qkv", "=", "None", ",", "seg_ids", "=", "None", ")", ":", "\n", "        ", "attention_output", "=", "self", ".", "attention", "(", "\n", "hidden_states", ",", "attention_mask", ",", "history_states", "=", "history_states", ",", "mask_qkv", "=", "mask_qkv", ",", "seg_ids", "=", "seg_ids", ")", "\n", "if", "self", ".", "ffn_type", ":", "\n", "            ", "layer_output", "=", "self", ".", "ffn", "(", "attention_output", ")", "\n", "", "else", ":", "\n", "            ", "intermediate_output", "=", "self", ".", "intermediate", "(", "attention_output", ")", "\n", "layer_output", "=", "self", ".", "output", "(", "intermediate_output", ",", "attention_output", ")", "\n", "", "return", "layer_output", "\n", "\n"]], "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.pytorch_pretrained_bert.modeling.BertEncoder.__init__": [[493, 498], ["torch.nn.Module.__init__", "modeling.BertLayer", "torch.nn.ModuleList", "torch.nn.ModuleList", "copy.deepcopy", "range"], "methods", ["home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.unilm.eval.Eval.__init__"], ["    ", "def", "__init__", "(", "self", ",", "config", ")", ":", "\n", "        ", "super", "(", "BertEncoder", ",", "self", ")", ".", "__init__", "(", ")", "\n", "layer", "=", "BertLayer", "(", "config", ")", "\n", "self", ".", "layer", "=", "nn", ".", "ModuleList", "(", "[", "copy", ".", "deepcopy", "(", "layer", ")", "\n", "for", "_", "in", "range", "(", "config", ".", "num_hidden_layers", ")", "]", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.pytorch_pretrained_bert.modeling.BertEncoder.forward": [[499, 522], ["enumerate", "all_encoder_layers.append", "layer_module", "layer_module", "all_encoder_layers.append", "all_encoder_layers.append"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "hidden_states", ",", "attention_mask", ",", "output_all_encoded_layers", "=", "True", ",", "prev_embedding", "=", "None", ",", "prev_encoded_layers", "=", "None", ",", "mask_qkv", "=", "None", ",", "seg_ids", "=", "None", ")", ":", "\n", "# history embedding and encoded layer must be simultanously given", "\n", "        ", "assert", "(", "prev_embedding", "is", "None", ")", "==", "(", "prev_encoded_layers", "is", "None", ")", "\n", "\n", "all_encoder_layers", "=", "[", "]", "\n", "if", "(", "prev_embedding", "is", "not", "None", ")", "and", "(", "prev_encoded_layers", "is", "not", "None", ")", ":", "\n", "            ", "history_states", "=", "prev_embedding", "\n", "for", "i", ",", "layer_module", "in", "enumerate", "(", "self", ".", "layer", ")", ":", "\n", "                ", "hidden_states", "=", "layer_module", "(", "\n", "hidden_states", ",", "attention_mask", ",", "history_states", "=", "history_states", ",", "mask_qkv", "=", "mask_qkv", ",", "seg_ids", "=", "seg_ids", ")", "\n", "if", "output_all_encoded_layers", ":", "\n", "                    ", "all_encoder_layers", ".", "append", "(", "hidden_states", ")", "\n", "", "if", "prev_encoded_layers", "is", "not", "None", ":", "\n", "                    ", "history_states", "=", "prev_encoded_layers", "[", "i", "]", "\n", "", "", "", "else", ":", "\n", "            ", "for", "layer_module", "in", "self", ".", "layer", ":", "\n", "                ", "hidden_states", "=", "layer_module", "(", "\n", "hidden_states", ",", "attention_mask", ",", "mask_qkv", "=", "mask_qkv", ",", "seg_ids", "=", "seg_ids", ")", "\n", "if", "output_all_encoded_layers", ":", "\n", "                    ", "all_encoder_layers", ".", "append", "(", "hidden_states", ")", "\n", "", "", "", "if", "not", "output_all_encoded_layers", ":", "\n", "            ", "all_encoder_layers", ".", "append", "(", "hidden_states", ")", "\n", "", "return", "all_encoder_layers", "\n", "\n"]], "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.pytorch_pretrained_bert.modeling.BertPooler.__init__": [[525, 529], ["torch.nn.Module.__init__", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Tanh", "torch.nn.Tanh"], "methods", ["home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.unilm.eval.Eval.__init__"], ["    ", "def", "__init__", "(", "self", ",", "config", ")", ":", "\n", "        ", "super", "(", "BertPooler", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "dense", "=", "nn", ".", "Linear", "(", "config", ".", "hidden_size", ",", "config", ".", "hidden_size", ")", "\n", "self", ".", "activation", "=", "nn", ".", "Tanh", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.pytorch_pretrained_bert.modeling.BertPooler.forward": [[530, 537], ["modeling.BertPooler.dense", "modeling.BertPooler.activation"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "hidden_states", ")", ":", "\n", "# We \"pool\" the model by simply taking the hidden state corresponding", "\n", "# to the first token.", "\n", "        ", "first_token_tensor", "=", "hidden_states", "[", ":", ",", "0", "]", "\n", "pooled_output", "=", "self", ".", "dense", "(", "first_token_tensor", ")", "\n", "pooled_output", "=", "self", ".", "activation", "(", "pooled_output", ")", "\n", "return", "pooled_output", "\n", "\n"]], "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.pytorch_pretrained_bert.modeling.BertPredictionHeadTransform.__init__": [[540, 549], ["torch.nn.Module.__init__", "torch.nn.Linear", "torch.nn.Linear", "modeling.BertLayerNorm", "isinstance", "hasattr"], "methods", ["home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.unilm.eval.Eval.__init__"], ["    ", "def", "__init__", "(", "self", ",", "config", ")", ":", "\n", "        ", "super", "(", "BertPredictionHeadTransform", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "transform_act_fn", "=", "ACT2FN", "[", "config", ".", "hidden_act", "]", "if", "isinstance", "(", "config", ".", "hidden_act", ",", "str", ")", "else", "config", ".", "hidden_act", "\n", "hid_size", "=", "config", ".", "hidden_size", "\n", "if", "hasattr", "(", "config", ",", "'relax_projection'", ")", "and", "(", "config", ".", "relax_projection", ">", "1", ")", ":", "\n", "            ", "hid_size", "*=", "config", ".", "relax_projection", "\n", "", "self", ".", "dense", "=", "nn", ".", "Linear", "(", "config", ".", "hidden_size", ",", "hid_size", ")", "\n", "self", ".", "LayerNorm", "=", "BertLayerNorm", "(", "hid_size", ",", "eps", "=", "1e-5", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.pytorch_pretrained_bert.modeling.BertPredictionHeadTransform.forward": [[550, 555], ["modeling.BertPredictionHeadTransform.dense", "modeling.BertPredictionHeadTransform.transform_act_fn", "modeling.BertPredictionHeadTransform.LayerNorm"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "hidden_states", ")", ":", "\n", "        ", "hidden_states", "=", "self", ".", "dense", "(", "hidden_states", ")", "\n", "hidden_states", "=", "self", ".", "transform_act_fn", "(", "hidden_states", ")", "\n", "hidden_states", "=", "self", ".", "LayerNorm", "(", "hidden_states", ")", "\n", "return", "hidden_states", "\n", "\n"]], "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.pytorch_pretrained_bert.modeling.BertLMPredictionHead.__init__": [[558, 583], ["torch.nn.Module.__init__", "modeling.BertPredictionHeadTransform", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Parameter", "torch.nn.Parameter", "bert_model_embedding_weights.size", "bert_model_embedding_weights.size", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "hasattr", "bert_model_embedding_weights.size", "tensor.half"], "methods", ["home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.unilm.eval.Eval.__init__"], ["    ", "def", "__init__", "(", "self", ",", "config", ",", "bert_model_embedding_weights", ")", ":", "\n", "        ", "super", "(", "BertLMPredictionHead", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "transform", "=", "BertPredictionHeadTransform", "(", "config", ")", "\n", "\n", "# The output weights are the same as the input embeddings, but there is", "\n", "# an output-only bias for each token.", "\n", "self", ".", "decoder", "=", "nn", ".", "Linear", "(", "bert_model_embedding_weights", ".", "size", "(", "1", ")", ",", "\n", "bert_model_embedding_weights", ".", "size", "(", "0", ")", ",", "\n", "bias", "=", "False", ")", "\n", "self", ".", "decoder", ".", "weight", "=", "bert_model_embedding_weights", "\n", "self", ".", "bias", "=", "nn", ".", "Parameter", "(", "torch", ".", "zeros", "(", "\n", "bert_model_embedding_weights", ".", "size", "(", "0", ")", ")", ")", "\n", "if", "hasattr", "(", "config", ",", "'relax_projection'", ")", "and", "(", "config", ".", "relax_projection", ">", "1", ")", ":", "\n", "            ", "self", ".", "relax_projection", "=", "config", ".", "relax_projection", "\n", "", "else", ":", "\n", "            ", "self", ".", "relax_projection", "=", "0", "\n", "", "self", ".", "fp32_embedding", "=", "config", ".", "fp32_embedding", "\n", "\n", "def", "convert_to_type", "(", "tensor", ")", ":", "\n", "            ", "if", "self", ".", "fp32_embedding", ":", "\n", "                ", "return", "tensor", ".", "half", "(", ")", "\n", "", "else", ":", "\n", "                ", "return", "tensor", "\n", "", "", "self", ".", "type_converter", "=", "convert_to_type", "\n", "self", ".", "converted", "=", "False", "\n", "\n"]], "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.pytorch_pretrained_bert.modeling.BertLMPredictionHead.forward": [[584, 602], ["modeling.BertLMPredictionHead.transform", "modeling.BertLMPredictionHead.type_converter", "torch.linear.size", "torch.linear.size", "torch.linear", "torch.linear", "modeling.BertLMPredictionHead.transform.half", "torch.linear.view", "modeling.BertLMPredictionHead.type_converter", "modeling.BertLMPredictionHead.type_converter", "modeling.BertLMPredictionHead.type_converter", "modeling.BertLMPredictionHead.decoder", "torch.arange().long", "torch.arange().long", "torch.arange().long", "torch.arange().long", "torch.arange", "torch.arange", "torch.arange", "torch.arange"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "hidden_states", ",", "task_idx", "=", "None", ")", ":", "\n", "        ", "if", "not", "self", ".", "converted", ":", "\n", "            ", "self", ".", "converted", "=", "True", "\n", "if", "self", ".", "fp32_embedding", ":", "\n", "                ", "self", ".", "transform", ".", "half", "(", ")", "\n", "", "", "hidden_states", "=", "self", ".", "transform", "(", "self", ".", "type_converter", "(", "hidden_states", ")", ")", "\n", "if", "self", ".", "relax_projection", ">", "1", ":", "\n", "            ", "num_batch", "=", "hidden_states", ".", "size", "(", "0", ")", "\n", "num_pos", "=", "hidden_states", ".", "size", "(", "1", ")", "\n", "# (batch, num_pos, relax_projection*hid) -> (batch, num_pos, relax_projection, hid) -> (batch, num_pos, hid)", "\n", "hidden_states", "=", "hidden_states", ".", "view", "(", "\n", "num_batch", ",", "num_pos", ",", "self", ".", "relax_projection", ",", "-", "1", ")", "[", "torch", ".", "arange", "(", "0", ",", "num_batch", ")", ".", "long", "(", ")", ",", ":", ",", "task_idx", ",", ":", "]", "\n", "", "if", "self", ".", "fp32_embedding", ":", "\n", "            ", "hidden_states", "=", "F", ".", "linear", "(", "self", ".", "type_converter", "(", "hidden_states", ")", ",", "self", ".", "type_converter", "(", "\n", "self", ".", "decoder", ".", "weight", ")", ",", "self", ".", "type_converter", "(", "self", ".", "bias", ")", ")", "\n", "", "else", ":", "\n", "            ", "hidden_states", "=", "self", ".", "decoder", "(", "hidden_states", ")", "+", "self", ".", "bias", "\n", "", "return", "hidden_states", "\n", "\n"]], "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.pytorch_pretrained_bert.modeling.BertOnlyMLMHead.__init__": [[605, 609], ["torch.nn.Module.__init__", "modeling.BertLMPredictionHead"], "methods", ["home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.unilm.eval.Eval.__init__"], ["    ", "def", "__init__", "(", "self", ",", "config", ",", "bert_model_embedding_weights", ")", ":", "\n", "        ", "super", "(", "BertOnlyMLMHead", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "predictions", "=", "BertLMPredictionHead", "(", "\n", "config", ",", "bert_model_embedding_weights", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.pytorch_pretrained_bert.modeling.BertOnlyMLMHead.forward": [[610, 613], ["modeling.BertOnlyMLMHead.predictions"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "sequence_output", ")", ":", "\n", "        ", "prediction_scores", "=", "self", ".", "predictions", "(", "sequence_output", ")", "\n", "return", "prediction_scores", "\n", "\n"]], "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.pytorch_pretrained_bert.modeling.BertOnlyNSPHead.__init__": [[616, 619], ["torch.nn.Module.__init__", "torch.nn.Linear", "torch.nn.Linear"], "methods", ["home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.unilm.eval.Eval.__init__"], ["    ", "def", "__init__", "(", "self", ",", "config", ")", ":", "\n", "        ", "super", "(", "BertOnlyNSPHead", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "seq_relationship", "=", "nn", ".", "Linear", "(", "config", ".", "hidden_size", ",", "2", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.pytorch_pretrained_bert.modeling.BertOnlyNSPHead.forward": [[620, 623], ["modeling.BertOnlyNSPHead.seq_relationship"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "pooled_output", ")", ":", "\n", "        ", "seq_relationship_score", "=", "self", ".", "seq_relationship", "(", "pooled_output", ")", "\n", "return", "seq_relationship_score", "\n", "\n"]], "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.pytorch_pretrained_bert.modeling.BertPreTrainingHeads.__init__": [[626, 631], ["torch.nn.Module.__init__", "modeling.BertLMPredictionHead", "torch.nn.Linear", "torch.nn.Linear"], "methods", ["home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.unilm.eval.Eval.__init__"], ["    ", "def", "__init__", "(", "self", ",", "config", ",", "bert_model_embedding_weights", ",", "num_labels", "=", "2", ")", ":", "\n", "        ", "super", "(", "BertPreTrainingHeads", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "predictions", "=", "BertLMPredictionHead", "(", "\n", "config", ",", "bert_model_embedding_weights", ")", "\n", "self", ".", "seq_relationship", "=", "nn", ".", "Linear", "(", "config", ".", "hidden_size", ",", "num_labels", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.pytorch_pretrained_bert.modeling.BertPreTrainingHeads.forward": [[632, 639], ["modeling.BertPreTrainingHeads.predictions", "modeling.BertPreTrainingHeads.seq_relationship"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "sequence_output", ",", "pooled_output", ",", "task_idx", "=", "None", ")", ":", "\n", "        ", "prediction_scores", "=", "self", ".", "predictions", "(", "sequence_output", ",", "task_idx", ")", "\n", "if", "pooled_output", "is", "None", ":", "\n", "            ", "seq_relationship_score", "=", "None", "\n", "", "else", ":", "\n", "            ", "seq_relationship_score", "=", "self", ".", "seq_relationship", "(", "pooled_output", ")", "\n", "", "return", "prediction_scores", ",", "seq_relationship_score", "\n", "\n"]], "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.pytorch_pretrained_bert.modeling.PreTrainedBertModel.__init__": [[646, 656], ["torch.nn.Module.__init__", "isinstance", "ValueError"], "methods", ["home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.unilm.eval.Eval.__init__"], ["def", "__init__", "(", "self", ",", "config", ",", "*", "inputs", ",", "**", "kwargs", ")", ":", "\n", "        ", "super", "(", "PreTrainedBertModel", ",", "self", ")", ".", "__init__", "(", ")", "\n", "if", "not", "isinstance", "(", "config", ",", "BertConfig", ")", ":", "\n", "            ", "raise", "ValueError", "(", "\n", "\"Parameter config in `{}(config)` should be an instance of class `BertConfig`. \"", "\n", "\"To create a model from a Google pretrained model use \"", "\n", "\"`model = {}.from_pretrained(PRETRAINED_MODEL_NAME)`\"", ".", "format", "(", "\n", "self", ".", "__class__", ".", "__name__", ",", "self", ".", "__class__", ".", "__name__", "\n", ")", ")", "\n", "", "self", ".", "config", "=", "config", "\n", "\n"]], "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.pytorch_pretrained_bert.modeling.PreTrainedBertModel.init_bert_weights": [[657, 667], ["isinstance", "module.weight.data.normal_", "isinstance", "isinstance", "module.bias.data.zero_", "module.bias.data.zero_", "module.weight.data.fill_"], "methods", ["None"], ["", "def", "init_bert_weights", "(", "self", ",", "module", ")", ":", "\n", "        ", "\"\"\" Initialize the weights.\n        \"\"\"", "\n", "if", "isinstance", "(", "module", ",", "(", "nn", ".", "Linear", ",", "nn", ".", "Embedding", ")", ")", ":", "\n", "            ", "module", ".", "weight", ".", "data", ".", "normal_", "(", "mean", "=", "0.0", ",", "std", "=", "self", ".", "config", ".", "initializer_range", ")", "\n", "", "elif", "isinstance", "(", "module", ",", "BertLayerNorm", ")", ":", "\n", "            ", "module", ".", "bias", ".", "data", ".", "zero_", "(", ")", "\n", "module", ".", "weight", ".", "data", ".", "fill_", "(", "1.0", ")", "\n", "", "if", "isinstance", "(", "module", ",", "nn", ".", "Linear", ")", "and", "module", ".", "bias", "is", "not", "None", ":", "\n", "            ", "module", ".", "bias", ".", "data", ".", "zero_", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.pytorch_pretrained_bert.modeling.PreTrainedBertModel.from_pretrained": [[668, 977], ["print", "print", "os.path.isdir", "modeling.BertConfig.from_json_file", "logger.info", "cls", "torch.load.keys", "torch.load.keys", "zip", "getattr", "torch.load.copy", "torch.load.copy", "modeling.PreTrainedBertModel.from_pretrained.load"], "methods", ["home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.pytorch_pretrained_bert.modeling.BertConfig.from_json_file"], ["", "", "@", "classmethod", "\n", "def", "from_pretrained", "(", "cls", ",", "pretrained_model_name", ",", "state_dict", "=", "None", ",", "cache_dir", "=", "None", ",", "*", "inputs", ",", "**", "kwargs", ")", ":", "\n", "        ", "\"\"\"\n        Instantiate a PreTrainedBertModel from a pre-trained model file or a pytorch state dict.\n        Download and cache the pre-trained model file if needed.\n\n        Params:\n            pretrained_model_name: either:\n                - a str with the name of a pre-trained model to load selected in the list of:\n                    . `bert-base-uncased`\n                    . `bert-large-uncased`\n                    . `bert-base-cased`\n                    . `bert-base-multilingual`\n                    . `bert-base-chinese`\n                - a path or url to a pretrained model archive containing:\n                    . `bert_config.json` a configuration file for the model\n                    . `pytorch_model.bin` a PyTorch dump of a BertForPreTraining instance\n            cache_dir: an optional path to a folder in which the pre-trained models will be cached.\n            state_dict: an optional state dictionnary (collections.OrderedDict object) to use instead of Google pre-trained models\n            *inputs, **kwargs: additional input for the specific Bert class\n                (ex: num_labels for BertForSequenceClassification)\n        \"\"\"", "\n", "if", "pretrained_model_name", "in", "PRETRAINED_MODEL_ARCHIVE_MAP", ":", "\n", "            ", "archive_file", "=", "PRETRAINED_MODEL_ARCHIVE_MAP", "[", "pretrained_model_name", "]", "\n", "", "else", ":", "\n", "            ", "archive_file", "=", "pretrained_model_name", "\n", "# redirect to the cache, if necessary", "\n", "", "try", ":", "\n", "            ", "resolved_archive_file", "=", "cached_path", "(", "\n", "archive_file", ",", "cache_dir", "=", "cache_dir", ")", "\n", "", "except", "FileNotFoundError", ":", "\n", "            ", "logger", ".", "error", "(", "\n", "\"Model name '{}' was not found in model name list ({}). \"", "\n", "\"We assumed '{}' was a path or url but couldn't find any file \"", "\n", "\"associated to this path or url.\"", ".", "format", "(", "\n", "pretrained_model_name", ",", "\n", "', '", ".", "join", "(", "PRETRAINED_MODEL_ARCHIVE_MAP", ".", "keys", "(", ")", ")", ",", "\n", "archive_file", ")", ")", "\n", "return", "None", "\n", "", "if", "resolved_archive_file", "==", "archive_file", ":", "\n", "            ", "logger", ".", "info", "(", "\"loading archive file {}\"", ".", "format", "(", "archive_file", ")", ")", "\n", "", "else", ":", "\n", "            ", "logger", ".", "info", "(", "\"loading archive file {} from cache at {}\"", ".", "format", "(", "\n", "archive_file", ",", "resolved_archive_file", ")", ")", "\n", "", "tempdir", "=", "None", "\n", "if", "os", ".", "path", ".", "isdir", "(", "resolved_archive_file", ")", ":", "\n", "            ", "serialization_dir", "=", "resolved_archive_file", "\n", "", "else", ":", "\n", "# Extract archive to temp dir", "\n", "            ", "tempdir", "=", "tempfile", ".", "mkdtemp", "(", ")", "\n", "logger", ".", "info", "(", "\"extracting archive file {} to temp dir {}\"", ".", "format", "(", "\n", "resolved_archive_file", ",", "tempdir", ")", ")", "\n", "with", "tarfile", ".", "open", "(", "resolved_archive_file", ",", "'r:gz'", ")", "as", "archive", ":", "\n", "                ", "archive", ".", "extractall", "(", "tempdir", ")", "\n", "", "serialization_dir", "=", "tempdir", "\n", "# Load config", "\n", "", "if", "(", "'config_path'", "in", "kwargs", ")", "and", "kwargs", "[", "'config_path'", "]", ":", "\n", "            ", "config_file", "=", "kwargs", "[", "'config_path'", "]", "\n", "", "else", ":", "\n", "            ", "config_file", "=", "os", ".", "path", ".", "join", "(", "serialization_dir", ",", "CONFIG_NAME", ")", "\n", "", "config", "=", "BertConfig", ".", "from_json_file", "(", "config_file", ")", "\n", "\n", "# define new type_vocab_size (there might be different numbers of segment ids)", "\n", "if", "'type_vocab_size'", "in", "kwargs", ":", "\n", "            ", "config", ".", "type_vocab_size", "=", "kwargs", "[", "'type_vocab_size'", "]", "\n", "# define new relax_projection", "\n", "", "if", "(", "'relax_projection'", "in", "kwargs", ")", "and", "kwargs", "[", "'relax_projection'", "]", ":", "\n", "            ", "config", ".", "relax_projection", "=", "kwargs", "[", "'relax_projection'", "]", "\n", "# new position embedding", "\n", "", "if", "(", "'new_pos_ids'", "in", "kwargs", ")", "and", "kwargs", "[", "'new_pos_ids'", "]", ":", "\n", "            ", "config", ".", "new_pos_ids", "=", "kwargs", "[", "'new_pos_ids'", "]", "\n", "# define new relax_projection", "\n", "", "if", "(", "'task_idx'", "in", "kwargs", ")", "and", "kwargs", "[", "'task_idx'", "]", ":", "\n", "            ", "config", ".", "task_idx", "=", "kwargs", "[", "'task_idx'", "]", "\n", "# define new max position embedding for length expansion", "\n", "", "if", "(", "'max_position_embeddings'", "in", "kwargs", ")", "and", "kwargs", "[", "'max_position_embeddings'", "]", ":", "\n", "            ", "config", ".", "max_position_embeddings", "=", "kwargs", "[", "'max_position_embeddings'", "]", "\n", "# use fp32 for embeddings", "\n", "", "if", "(", "'fp32_embedding'", "in", "kwargs", ")", "and", "kwargs", "[", "'fp32_embedding'", "]", ":", "\n", "            ", "config", ".", "fp32_embedding", "=", "kwargs", "[", "'fp32_embedding'", "]", "\n", "# type of FFN in transformer blocks", "\n", "", "if", "(", "'ffn_type'", "in", "kwargs", ")", "and", "kwargs", "[", "'ffn_type'", "]", ":", "\n", "            ", "config", ".", "ffn_type", "=", "kwargs", "[", "'ffn_type'", "]", "\n", "# label smoothing", "\n", "", "if", "(", "'label_smoothing'", "in", "kwargs", ")", "and", "kwargs", "[", "'label_smoothing'", "]", ":", "\n", "            ", "config", ".", "label_smoothing", "=", "kwargs", "[", "'label_smoothing'", "]", "\n", "# dropout", "\n", "", "if", "(", "'hidden_dropout_prob'", "in", "kwargs", ")", "and", "kwargs", "[", "'hidden_dropout_prob'", "]", ":", "\n", "            ", "config", ".", "hidden_dropout_prob", "=", "kwargs", "[", "'hidden_dropout_prob'", "]", "\n", "", "if", "(", "'attention_probs_dropout_prob'", "in", "kwargs", ")", "and", "kwargs", "[", "'attention_probs_dropout_prob'", "]", ":", "\n", "            ", "config", ".", "attention_probs_dropout_prob", "=", "kwargs", "[", "'attention_probs_dropout_prob'", "]", "\n", "# different QKV", "\n", "", "if", "(", "'num_qkv'", "in", "kwargs", ")", "and", "kwargs", "[", "'num_qkv'", "]", ":", "\n", "            ", "config", ".", "num_qkv", "=", "kwargs", "[", "'num_qkv'", "]", "\n", "# segment embedding for self-attention", "\n", "", "if", "(", "'seg_emb'", "in", "kwargs", ")", "and", "kwargs", "[", "'seg_emb'", "]", ":", "\n", "            ", "config", ".", "seg_emb", "=", "kwargs", "[", "'seg_emb'", "]", "\n", "# initialize word embeddings", "\n", "", "_word_emb_map", "=", "None", "\n", "if", "(", "'word_emb_map'", "in", "kwargs", ")", "and", "kwargs", "[", "'word_emb_map'", "]", ":", "\n", "            ", "_word_emb_map", "=", "kwargs", "[", "'word_emb_map'", "]", "\n", "\n", "", "logger", ".", "info", "(", "\"Model config {}\"", ".", "format", "(", "config", ")", ")", "\n", "\n", "# clean the arguments in kwargs", "\n", "for", "arg_clean", "in", "(", "'config_path'", ",", "'type_vocab_size'", ",", "'relax_projection'", ",", "'new_pos_ids'", ",", "'task_idx'", ",", "'max_position_embeddings'", ",", "'fp32_embedding'", ",", "'ffn_type'", ",", "'label_smoothing'", ",", "'hidden_dropout_prob'", ",", "'attention_probs_dropout_prob'", ",", "'num_qkv'", ",", "'seg_emb'", ",", "'word_emb_map'", ")", ":", "\n", "            ", "if", "arg_clean", "in", "kwargs", ":", "\n", "                ", "del", "kwargs", "[", "arg_clean", "]", "\n", "\n", "# Instantiate model.", "\n", "", "", "model", "=", "cls", "(", "config", ",", "*", "inputs", ",", "**", "kwargs", ")", "\n", "if", "state_dict", "is", "None", ":", "\n", "            ", "weights_path", "=", "os", ".", "path", ".", "join", "(", "serialization_dir", ",", "WEIGHTS_NAME", ")", "\n", "state_dict", "=", "torch", ".", "load", "(", "weights_path", ")", "\n", "\n", "", "old_keys", "=", "[", "]", "\n", "new_keys", "=", "[", "]", "\n", "for", "key", "in", "state_dict", ".", "keys", "(", ")", ":", "\n", "            ", "new_key", "=", "None", "\n", "if", "'gamma'", "in", "key", ":", "\n", "                ", "new_key", "=", "key", ".", "replace", "(", "'gamma'", ",", "'weight'", ")", "\n", "", "if", "'beta'", "in", "key", ":", "\n", "                ", "new_key", "=", "key", ".", "replace", "(", "'beta'", ",", "'bias'", ")", "\n", "", "if", "new_key", ":", "\n", "                ", "old_keys", ".", "append", "(", "key", ")", "\n", "new_keys", ".", "append", "(", "new_key", ")", "\n", "", "", "for", "old_key", ",", "new_key", "in", "zip", "(", "old_keys", ",", "new_keys", ")", ":", "\n", "            ", "state_dict", "[", "new_key", "]", "=", "state_dict", ".", "pop", "(", "old_key", ")", "\n", "\n", "# initialize new segment embeddings", "\n", "", "_k", "=", "'bert.embeddings.token_type_embeddings.weight'", "\n", "if", "(", "_k", "in", "state_dict", ")", "and", "(", "config", ".", "type_vocab_size", "!=", "state_dict", "[", "_k", "]", ".", "shape", "[", "0", "]", ")", ":", "\n", "            ", "logger", ".", "info", "(", "\"config.type_vocab_size != state_dict[bert.embeddings.token_type_embeddings.weight] ({0} != {1})\"", ".", "format", "(", "\n", "config", ".", "type_vocab_size", ",", "state_dict", "[", "_k", "]", ".", "shape", "[", "0", "]", ")", ")", "\n", "if", "config", ".", "type_vocab_size", ">", "state_dict", "[", "_k", "]", ".", "shape", "[", "0", "]", ":", "\n", "# state_dict[_k].data = state_dict[_k].data.resize_(config.type_vocab_size, state_dict[_k].shape[1])", "\n", "                ", "state_dict", "[", "_k", "]", ".", "resize_", "(", "\n", "config", ".", "type_vocab_size", ",", "state_dict", "[", "_k", "]", ".", "shape", "[", "1", "]", ")", "\n", "# L2R", "\n", "if", "config", ".", "type_vocab_size", ">=", "3", ":", "\n", "                    ", "state_dict", "[", "_k", "]", ".", "data", "[", "2", ",", ":", "]", ".", "copy_", "(", "state_dict", "[", "_k", "]", ".", "data", "[", "0", ",", ":", "]", ")", "\n", "# R2L", "\n", "", "if", "config", ".", "type_vocab_size", ">=", "4", ":", "\n", "                    ", "state_dict", "[", "_k", "]", ".", "data", "[", "3", ",", ":", "]", ".", "copy_", "(", "state_dict", "[", "_k", "]", ".", "data", "[", "0", ",", ":", "]", ")", "\n", "# S2S", "\n", "", "if", "config", ".", "type_vocab_size", ">=", "6", ":", "\n", "                    ", "state_dict", "[", "_k", "]", ".", "data", "[", "4", ",", ":", "]", ".", "copy_", "(", "state_dict", "[", "_k", "]", ".", "data", "[", "0", ",", ":", "]", ")", "\n", "state_dict", "[", "_k", "]", ".", "data", "[", "5", ",", ":", "]", ".", "copy_", "(", "state_dict", "[", "_k", "]", ".", "data", "[", "1", ",", ":", "]", ")", "\n", "", "if", "config", ".", "type_vocab_size", ">=", "7", ":", "\n", "                    ", "state_dict", "[", "_k", "]", ".", "data", "[", "6", ",", ":", "]", ".", "copy_", "(", "state_dict", "[", "_k", "]", ".", "data", "[", "1", ",", ":", "]", ")", "\n", "", "", "elif", "config", ".", "type_vocab_size", "<", "state_dict", "[", "_k", "]", ".", "shape", "[", "0", "]", ":", "\n", "                ", "state_dict", "[", "_k", "]", ".", "data", "=", "state_dict", "[", "_k", "]", ".", "data", "[", ":", "config", ".", "type_vocab_size", ",", ":", "]", "\n", "\n", "", "", "_k", "=", "'bert.embeddings.position_embeddings.weight'", "\n", "n_config_pos_emb", "=", "4", "if", "config", ".", "new_pos_ids", "else", "1", "\n", "if", "(", "_k", "in", "state_dict", ")", "and", "(", "n_config_pos_emb", "*", "config", ".", "hidden_size", "!=", "state_dict", "[", "_k", "]", ".", "shape", "[", "1", "]", ")", ":", "\n", "            ", "logger", ".", "info", "(", "\"n_config_pos_emb*config.hidden_size != state_dict[bert.embeddings.position_embeddings.weight] ({0}*{1} != {2})\"", ".", "format", "(", "\n", "n_config_pos_emb", ",", "config", ".", "hidden_size", ",", "state_dict", "[", "_k", "]", ".", "shape", "[", "1", "]", ")", ")", "\n", "assert", "state_dict", "[", "_k", "]", ".", "shape", "[", "1", "]", "%", "config", ".", "hidden_size", "==", "0", "\n", "n_state_pos_emb", "=", "int", "(", "state_dict", "[", "_k", "]", ".", "shape", "[", "1", "]", "/", "config", ".", "hidden_size", ")", "\n", "assert", "(", "n_state_pos_emb", "==", "1", ")", "!=", "(", "n_config_pos_emb", "==", "\n", "1", ")", ",", "\"!!!!n_state_pos_emb == 1 xor n_config_pos_emb == 1!!!!\"", "\n", "if", "n_state_pos_emb", "==", "1", ":", "\n", "                ", "state_dict", "[", "_k", "]", ".", "data", "=", "state_dict", "[", "_k", "]", ".", "data", ".", "unsqueeze", "(", "1", ")", ".", "repeat", "(", "\n", "1", ",", "n_config_pos_emb", ",", "1", ")", ".", "reshape", "(", "(", "config", ".", "max_position_embeddings", ",", "n_config_pos_emb", "*", "config", ".", "hidden_size", ")", ")", "\n", "", "elif", "n_config_pos_emb", "==", "1", ":", "\n", "                ", "if", "hasattr", "(", "config", ",", "'task_idx'", ")", "and", "(", "config", ".", "task_idx", "is", "not", "None", ")", "and", "(", "0", "<=", "config", ".", "task_idx", "<=", "3", ")", ":", "\n", "                    ", "_task_idx", "=", "config", ".", "task_idx", "\n", "", "else", ":", "\n", "                    ", "_task_idx", "=", "0", "\n", "", "state_dict", "[", "_k", "]", ".", "data", "=", "state_dict", "[", "_k", "]", ".", "data", ".", "view", "(", "\n", "config", ".", "max_position_embeddings", ",", "n_state_pos_emb", ",", "config", ".", "hidden_size", ")", ".", "select", "(", "1", ",", "_task_idx", ")", "\n", "\n", "# initialize new position embeddings", "\n", "", "", "_k", "=", "'bert.embeddings.position_embeddings.weight'", "\n", "if", "_k", "in", "state_dict", "and", "config", ".", "max_position_embeddings", "!=", "state_dict", "[", "_k", "]", ".", "shape", "[", "0", "]", ":", "\n", "            ", "logger", ".", "info", "(", "\"config.max_position_embeddings != state_dict[bert.embeddings.position_embeddings.weight] ({0} - {1})\"", ".", "format", "(", "\n", "config", ".", "max_position_embeddings", ",", "state_dict", "[", "_k", "]", ".", "shape", "[", "0", "]", ")", ")", "\n", "if", "config", ".", "max_position_embeddings", ">", "state_dict", "[", "_k", "]", ".", "shape", "[", "0", "]", ":", "\n", "                ", "old_size", "=", "state_dict", "[", "_k", "]", ".", "shape", "[", "0", "]", "\n", "# state_dict[_k].data = state_dict[_k].data.resize_(config.max_position_embeddings, state_dict[_k].shape[1])", "\n", "state_dict", "[", "_k", "]", ".", "resize_", "(", "\n", "config", ".", "max_position_embeddings", ",", "state_dict", "[", "_k", "]", ".", "shape", "[", "1", "]", ")", "\n", "start", "=", "old_size", "\n", "while", "start", "<", "config", ".", "max_position_embeddings", ":", "\n", "                    ", "chunk_size", "=", "min", "(", "\n", "old_size", ",", "config", ".", "max_position_embeddings", "-", "start", ")", "\n", "state_dict", "[", "_k", "]", ".", "data", "[", "start", ":", "start", "+", "chunk_size", ",", "\n", ":", "]", ".", "copy_", "(", "state_dict", "[", "_k", "]", ".", "data", "[", ":", "chunk_size", ",", ":", "]", ")", "\n", "start", "+=", "chunk_size", "\n", "", "", "elif", "config", ".", "max_position_embeddings", "<", "state_dict", "[", "_k", "]", ".", "shape", "[", "0", "]", ":", "\n", "                ", "state_dict", "[", "_k", "]", ".", "data", "=", "state_dict", "[", "_k", "]", ".", "data", "[", ":", "config", ".", "max_position_embeddings", ",", ":", "]", "\n", "\n", "# initialize relax projection", "\n", "", "", "_k", "=", "'cls.predictions.transform.dense.weight'", "\n", "n_config_relax", "=", "1", "if", "(", "config", ".", "relax_projection", "<", "\n", "1", ")", "else", "config", ".", "relax_projection", "\n", "if", "(", "_k", "in", "state_dict", ")", "and", "(", "n_config_relax", "*", "config", ".", "hidden_size", "!=", "state_dict", "[", "_k", "]", ".", "shape", "[", "0", "]", ")", ":", "\n", "            ", "logger", ".", "info", "(", "\"n_config_relax*config.hidden_size != state_dict[cls.predictions.transform.dense.weight] ({0}*{1} != {2})\"", ".", "format", "(", "\n", "n_config_relax", ",", "config", ".", "hidden_size", ",", "state_dict", "[", "_k", "]", ".", "shape", "[", "0", "]", ")", ")", "\n", "assert", "state_dict", "[", "_k", "]", ".", "shape", "[", "0", "]", "%", "config", ".", "hidden_size", "==", "0", "\n", "n_state_relax", "=", "int", "(", "state_dict", "[", "_k", "]", ".", "shape", "[", "0", "]", "/", "config", ".", "hidden_size", ")", "\n", "assert", "(", "n_state_relax", "==", "1", ")", "!=", "(", "n_config_relax", "==", "\n", "1", ")", ",", "\"!!!!n_state_relax == 1 xor n_config_relax == 1!!!!\"", "\n", "if", "n_state_relax", "==", "1", ":", "\n", "                ", "_k", "=", "'cls.predictions.transform.dense.weight'", "\n", "state_dict", "[", "_k", "]", ".", "data", "=", "state_dict", "[", "_k", "]", ".", "data", ".", "unsqueeze", "(", "0", ")", ".", "repeat", "(", "\n", "n_config_relax", ",", "1", ",", "1", ")", ".", "reshape", "(", "(", "n_config_relax", "*", "config", ".", "hidden_size", ",", "config", ".", "hidden_size", ")", ")", "\n", "for", "_k", "in", "(", "'cls.predictions.transform.dense.bias'", ",", "'cls.predictions.transform.LayerNorm.weight'", ",", "'cls.predictions.transform.LayerNorm.bias'", ")", ":", "\n", "                    ", "state_dict", "[", "_k", "]", ".", "data", "=", "state_dict", "[", "_k", "]", ".", "data", ".", "unsqueeze", "(", "\n", "0", ")", ".", "repeat", "(", "n_config_relax", ",", "1", ")", ".", "view", "(", "-", "1", ")", "\n", "", "", "elif", "n_config_relax", "==", "1", ":", "\n", "                ", "if", "hasattr", "(", "config", ",", "'task_idx'", ")", "and", "(", "config", ".", "task_idx", "is", "not", "None", ")", "and", "(", "0", "<=", "config", ".", "task_idx", "<=", "3", ")", ":", "\n", "                    ", "_task_idx", "=", "config", ".", "task_idx", "\n", "", "else", ":", "\n", "                    ", "_task_idx", "=", "0", "\n", "", "_k", "=", "'cls.predictions.transform.dense.weight'", "\n", "state_dict", "[", "_k", "]", ".", "data", "=", "state_dict", "[", "_k", "]", ".", "data", ".", "view", "(", "\n", "n_state_relax", ",", "config", ".", "hidden_size", ",", "config", ".", "hidden_size", ")", ".", "select", "(", "0", ",", "_task_idx", ")", "\n", "for", "_k", "in", "(", "'cls.predictions.transform.dense.bias'", ",", "'cls.predictions.transform.LayerNorm.weight'", ",", "'cls.predictions.transform.LayerNorm.bias'", ")", ":", "\n", "                    ", "state_dict", "[", "_k", "]", ".", "data", "=", "state_dict", "[", "_k", "]", ".", "data", ".", "view", "(", "\n", "n_state_relax", ",", "config", ".", "hidden_size", ")", ".", "select", "(", "0", ",", "_task_idx", ")", "\n", "\n", "# initialize QKV", "\n", "", "", "", "_all_head_size", "=", "config", ".", "num_attention_heads", "*", "int", "(", "config", ".", "hidden_size", "/", "config", ".", "num_attention_heads", ")", "\n", "n_config_num_qkv", "=", "1", "if", "(", "config", ".", "num_qkv", "<", "1", ")", "else", "config", ".", "num_qkv", "\n", "for", "qkv_name", "in", "(", "'query'", ",", "'key'", ",", "'value'", ")", ":", "\n", "            ", "_k", "=", "'bert.encoder.layer.0.attention.self.{0}.weight'", ".", "format", "(", "\n", "qkv_name", ")", "\n", "if", "(", "_k", "in", "state_dict", ")", "and", "(", "n_config_num_qkv", "*", "_all_head_size", "!=", "state_dict", "[", "_k", "]", ".", "shape", "[", "0", "]", ")", ":", "\n", "                ", "logger", ".", "info", "(", "\"n_config_num_qkv*_all_head_size != state_dict[_k] ({0}*{1} != {2})\"", ".", "format", "(", "\n", "n_config_num_qkv", ",", "_all_head_size", ",", "state_dict", "[", "_k", "]", ".", "shape", "[", "0", "]", ")", ")", "\n", "for", "layer_idx", "in", "range", "(", "config", ".", "num_hidden_layers", ")", ":", "\n", "                    ", "_k", "=", "'bert.encoder.layer.{0}.attention.self.{1}.weight'", ".", "format", "(", "\n", "layer_idx", ",", "qkv_name", ")", "\n", "assert", "state_dict", "[", "_k", "]", ".", "shape", "[", "0", "]", "%", "_all_head_size", "==", "0", "\n", "n_state_qkv", "=", "int", "(", "state_dict", "[", "_k", "]", ".", "shape", "[", "0", "]", "/", "_all_head_size", ")", "\n", "assert", "(", "n_state_qkv", "==", "1", ")", "!=", "(", "n_config_num_qkv", "==", "\n", "1", ")", ",", "\"!!!!n_state_qkv == 1 xor n_config_num_qkv == 1!!!!\"", "\n", "if", "n_state_qkv", "==", "1", ":", "\n", "                        ", "_k", "=", "'bert.encoder.layer.{0}.attention.self.{1}.weight'", ".", "format", "(", "\n", "layer_idx", ",", "qkv_name", ")", "\n", "state_dict", "[", "_k", "]", ".", "data", "=", "state_dict", "[", "_k", "]", ".", "data", ".", "unsqueeze", "(", "0", ")", ".", "repeat", "(", "\n", "n_config_num_qkv", ",", "1", ",", "1", ")", ".", "reshape", "(", "(", "n_config_num_qkv", "*", "_all_head_size", ",", "_all_head_size", ")", ")", "\n", "_k", "=", "'bert.encoder.layer.{0}.attention.self.{1}.bias'", ".", "format", "(", "\n", "layer_idx", ",", "qkv_name", ")", "\n", "state_dict", "[", "_k", "]", ".", "data", "=", "state_dict", "[", "_k", "]", ".", "data", ".", "unsqueeze", "(", "\n", "0", ")", ".", "repeat", "(", "n_config_num_qkv", ",", "1", ")", ".", "view", "(", "-", "1", ")", "\n", "", "elif", "n_config_num_qkv", "==", "1", ":", "\n", "                        ", "if", "hasattr", "(", "config", ",", "'task_idx'", ")", "and", "(", "config", ".", "task_idx", "is", "not", "None", ")", "and", "(", "0", "<=", "config", ".", "task_idx", "<=", "3", ")", ":", "\n", "                            ", "_task_idx", "=", "config", ".", "task_idx", "\n", "", "else", ":", "\n", "                            ", "_task_idx", "=", "0", "\n", "", "assert", "_task_idx", "!=", "3", ",", "\"[INVALID] _task_idx=3: n_config_num_qkv=1 (should be 2)\"", "\n", "if", "_task_idx", "==", "0", ":", "\n", "                            ", "_qkv_idx", "=", "0", "\n", "", "else", ":", "\n", "                            ", "_qkv_idx", "=", "1", "\n", "", "_k", "=", "'bert.encoder.layer.{0}.attention.self.{1}.weight'", ".", "format", "(", "\n", "layer_idx", ",", "qkv_name", ")", "\n", "state_dict", "[", "_k", "]", ".", "data", "=", "state_dict", "[", "_k", "]", ".", "data", ".", "view", "(", "\n", "n_state_qkv", ",", "_all_head_size", ",", "_all_head_size", ")", ".", "select", "(", "0", ",", "_qkv_idx", ")", "\n", "_k", "=", "'bert.encoder.layer.{0}.attention.self.{1}.bias'", ".", "format", "(", "\n", "layer_idx", ",", "qkv_name", ")", "\n", "state_dict", "[", "_k", "]", ".", "data", "=", "state_dict", "[", "_k", "]", ".", "data", ".", "view", "(", "\n", "n_state_qkv", ",", "_all_head_size", ")", ".", "select", "(", "0", ",", "_qkv_idx", ")", "\n", "\n", "", "", "", "", "if", "_word_emb_map", ":", "\n", "            ", "_k", "=", "'bert.embeddings.word_embeddings.weight'", "\n", "for", "_tgt", ",", "_src", "in", "_word_emb_map", ":", "\n", "                ", "state_dict", "[", "_k", "]", ".", "data", "[", "_tgt", ",", ":", "]", ".", "copy_", "(", "\n", "state_dict", "[", "_k", "]", ".", "data", "[", "_src", ",", ":", "]", ")", "\n", "\n", "", "", "missing_keys", "=", "[", "]", "\n", "unexpected_keys", "=", "[", "]", "\n", "error_msgs", "=", "[", "]", "\n", "# copy state_dict so _load_from_state_dict can modify it", "\n", "metadata", "=", "getattr", "(", "state_dict", ",", "'_metadata'", ",", "None", ")", "\n", "state_dict", "=", "state_dict", ".", "copy", "(", ")", "\n", "if", "metadata", "is", "not", "None", ":", "\n", "            ", "state_dict", ".", "_metadata", "=", "metadata", "\n", "\n", "", "def", "load", "(", "module", ",", "prefix", "=", "''", ")", ":", "\n", "            ", "local_metadata", "=", "{", "}", "if", "metadata", "is", "None", "else", "metadata", ".", "get", "(", "\n", "prefix", "[", ":", "-", "1", "]", ",", "{", "}", ")", "\n", "module", ".", "_load_from_state_dict", "(", "\n", "state_dict", ",", "prefix", ",", "local_metadata", ",", "True", ",", "missing_keys", ",", "unexpected_keys", ",", "error_msgs", ")", "\n", "for", "name", ",", "child", "in", "module", ".", "_modules", ".", "items", "(", ")", ":", "\n", "                ", "if", "child", "is", "not", "None", ":", "\n", "                    ", "load", "(", "child", ",", "prefix", "+", "name", "+", "'.'", ")", "\n", "", "", "", "load", "(", "model", ",", "prefix", "=", "''", "if", "hasattr", "(", "model", ",", "'bert'", ")", "else", "'bert.'", ")", "\n", "model", ".", "missing_keys", "=", "missing_keys", "\n", "if", "len", "(", "missing_keys", ")", ">", "0", ":", "\n", "            ", "logger", ".", "info", "(", "\"Weights of {} not initialized from pretrained model: {}\"", ".", "format", "(", "\n", "model", ".", "__class__", ".", "__name__", ",", "missing_keys", ")", ")", "\n", "", "if", "len", "(", "unexpected_keys", ")", ">", "0", ":", "\n", "            ", "logger", ".", "info", "(", "\"Weights from pretrained model not used in {}: {}\"", ".", "format", "(", "\n", "model", ".", "__class__", ".", "__name__", ",", "unexpected_keys", ")", ")", "\n", "", "if", "len", "(", "error_msgs", ")", ">", "0", ":", "\n", "            ", "logger", ".", "info", "(", "'\\n'", ".", "join", "(", "error_msgs", ")", ")", "\n", "", "if", "tempdir", ":", "\n", "# Clean up temp dir", "\n", "            ", "shutil", ".", "rmtree", "(", "tempdir", ")", "\n", "", "return", "model", "\n", "\n", "\n", "", "", "class", "BertModel", "(", "PreTrainedBertModel", ")", ":", "\n", "    "]], "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.pytorch_pretrained_bert.modeling.BertModel.__init__": [[1011, 1017], ["modeling.PreTrainedBertModel.__init__", "modeling.BertEmbeddings", "modeling.BertEncoder", "modeling.BertPooler", "modeling.BertModel.apply"], "methods", ["home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.unilm.eval.Eval.__init__"], ["self", ".", "pooler", "=", "BertPooler", "(", "config", ")", "\n", "self", ".", "apply", "(", "self", ".", "init_bert_weights", ")", "\n", "\n", "", "def", "rescale_some_parameters", "(", "self", ")", ":", "\n", "        ", "for", "layer_id", ",", "layer", "in", "enumerate", "(", "self", ".", "encoder", ".", "layer", ")", ":", "\n", "            ", "layer", ".", "attention", ".", "output", ".", "dense", ".", "weight", ".", "data", ".", "div_", "(", "\n", "math", ".", "sqrt", "(", "2.0", "*", "(", "layer_id", "+", "1", ")", ")", ")", "\n"]], "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.pytorch_pretrained_bert.modeling.BertModel.rescale_some_parameters": [[1018, 1023], ["enumerate", "layer.attention.output.dense.weight.data.div_", "layer.output.dense.weight.data.div_", "math.sqrt", "math.sqrt"], "methods", ["None"], ["layer", ".", "output", ".", "dense", ".", "weight", ".", "data", ".", "div_", "(", "math", ".", "sqrt", "(", "2.0", "*", "(", "layer_id", "+", "1", ")", ")", ")", "\n", "\n", "", "", "def", "get_extended_attention_mask", "(", "self", ",", "input_ids", ",", "token_type_ids", ",", "attention_mask", ")", ":", "\n", "        ", "if", "attention_mask", "is", "None", ":", "\n", "            ", "attention_mask", "=", "torch", ".", "ones_like", "(", "input_ids", ")", "\n", "", "if", "token_type_ids", "is", "None", ":", "\n"]], "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.pytorch_pretrained_bert.modeling.BertModel.get_extended_attention_mask": [[1024, 1051], ["torch.ones_like.unsqueeze.to", "torch.ones_like", "torch.ones_like", "torch.ones_like", "torch.ones_like", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "torch.ones_like.dim", "torch.ones_like.dim", "torch.ones_like.unsqueeze().unsqueeze", "torch.ones_like.unsqueeze().unsqueeze", "torch.ones_like.dim", "torch.ones_like.dim", "torch.ones_like.unsqueeze", "torch.ones_like.unsqueeze", "torch.ones_like.unsqueeze", "torch.ones_like.unsqueeze"], "methods", ["None"], ["            ", "token_type_ids", "=", "torch", ".", "zeros_like", "(", "input_ids", ")", "\n", "\n", "# We create a 3D attention mask from a 2D tensor mask.", "\n", "# Sizes are [batch_size, 1, 1, to_seq_length]", "\n", "# So we can broadcast to [batch_size, num_heads, from_seq_length, to_seq_length]", "\n", "# this attention mask is more simple than the triangular masking of causal attention", "\n", "# used in OpenAI GPT, we just need to prepare the broadcast dimension here.", "\n", "", "if", "attention_mask", ".", "dim", "(", ")", "==", "2", ":", "\n", "            ", "extended_attention_mask", "=", "attention_mask", ".", "unsqueeze", "(", "1", ")", ".", "unsqueeze", "(", "2", ")", "\n", "", "elif", "attention_mask", ".", "dim", "(", ")", "==", "3", ":", "\n", "            ", "extended_attention_mask", "=", "attention_mask", ".", "unsqueeze", "(", "1", ")", "\n", "", "else", ":", "\n", "            ", "raise", "NotImplementedError", "\n", "\n", "# Since attention_mask is 1.0 for positions we want to attend and 0.0 for", "\n", "# masked positions, this operation will create a tensor which is 0.0 for", "\n", "# positions we want to attend and -10000.0 for masked positions.", "\n", "# Since we are adding it to the raw scores before the softmax, this is", "\n", "# effectively the same as removing these entirely.", "\n", "", "extended_attention_mask", "=", "extended_attention_mask", ".", "to", "(", "\n", "dtype", "=", "input_ids", ".", "dtype", ")", "# fp16 compatibility", "\n", "extended_attention_mask", "=", "(", "1.0", "-", "extended_attention_mask", ")", "*", "-", "10000.0", "\n", "return", "extended_attention_mask", "\n", "\n", "", "def", "forward", "(", "self", ",", "input_ids", ",", "token_type_ids", "=", "None", ",", "attention_mask", "=", "None", ",", "output_all_encoded_layers", "=", "True", ",", "mask_qkv", "=", "None", ",", "task_idx", "=", "None", ")", ":", "\n", "        ", "extended_attention_mask", "=", "self", ".", "get_extended_attention_mask", "(", "\n", "input_ids", ",", "token_type_ids", ",", "attention_mask", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.pytorch_pretrained_bert.modeling.BertModel.forward": [[1052, 1065], ["modeling.BertModel.get_extended_attention_mask", "modeling.BertModel.embeddings", "modeling.BertModel.encoder", "modeling.BertModel.pooler"], "methods", ["home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.pytorch_pretrained_bert.modeling.BertModel.get_extended_attention_mask"], ["embedding_output", "=", "self", ".", "embeddings", "(", "\n", "input_ids", ",", "token_type_ids", ",", "task_idx", "=", "task_idx", ")", "\n", "encoded_layers", "=", "self", ".", "encoder", "(", "embedding_output", ",", "extended_attention_mask", ",", "\n", "output_all_encoded_layers", "=", "output_all_encoded_layers", ",", "mask_qkv", "=", "mask_qkv", ",", "seg_ids", "=", "token_type_ids", ")", "\n", "sequence_output", "=", "encoded_layers", "[", "-", "1", "]", "\n", "pooled_output", "=", "self", ".", "pooler", "(", "sequence_output", ")", "\n", "if", "not", "output_all_encoded_layers", ":", "\n", "            ", "encoded_layers", "=", "encoded_layers", "[", "-", "1", "]", "\n", "", "return", "encoded_layers", ",", "pooled_output", "\n", "\n", "\n", "", "", "class", "BertModelIncr", "(", "BertModel", ")", ":", "\n", "    ", "def", "__init__", "(", "self", ",", "config", ")", ":", "\n", "        ", "super", "(", "BertModelIncr", ",", "self", ")", ".", "__init__", "(", "config", ")", "\n"]], "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.pytorch_pretrained_bert.modeling.BertModelIncr.__init__": [[1068, 1070], ["modeling.BertModel.__init__"], "methods", ["home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.unilm.eval.Eval.__init__"], ["prev_encoded_layers", "=", "None", ",", "mask_qkv", "=", "None", ",", "task_idx", "=", "None", ")", ":", "\n", "        ", "extended_attention_mask", "=", "self", ".", "get_extended_attention_mask", "(", "\n", "input_ids", ",", "token_type_ids", ",", "attention_mask", ")", "\n"]], "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.pytorch_pretrained_bert.modeling.BertModelIncr.forward": [[1071, 1088], ["modeling.BertModelIncr.get_extended_attention_mask", "modeling.BertModelIncr.embeddings", "modeling.BertModelIncr.encoder", "modeling.BertModelIncr.pooler"], "methods", ["home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.pytorch_pretrained_bert.modeling.BertModel.get_extended_attention_mask"], ["\n", "embedding_output", "=", "self", ".", "embeddings", "(", "\n", "input_ids", ",", "token_type_ids", ",", "position_ids", ",", "task_idx", "=", "task_idx", ")", "\n", "encoded_layers", "=", "self", ".", "encoder", "(", "embedding_output", ",", "\n", "extended_attention_mask", ",", "\n", "output_all_encoded_layers", "=", "output_all_encoded_layers", ",", "\n", "prev_embedding", "=", "prev_embedding", ",", "\n", "prev_encoded_layers", "=", "prev_encoded_layers", ",", "mask_qkv", "=", "mask_qkv", ",", "seg_ids", "=", "token_type_ids", ")", "\n", "sequence_output", "=", "encoded_layers", "[", "-", "1", "]", "\n", "pooled_output", "=", "self", ".", "pooler", "(", "sequence_output", ")", "\n", "if", "not", "output_all_encoded_layers", ":", "\n", "            ", "encoded_layers", "=", "encoded_layers", "[", "-", "1", "]", "\n", "", "return", "embedding_output", ",", "encoded_layers", ",", "pooled_output", "\n", "\n", "\n", "", "", "class", "BertForPreTraining", "(", "PreTrainedBertModel", ")", ":", "\n", "    "]], "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.pytorch_pretrained_bert.modeling.BertForPreTraining.__init__": [[1135, 1141], ["modeling.PreTrainedBertModel.__init__", "modeling.BertModel", "modeling.BertPreTrainingHeads", "modeling.BertForPreTraining.apply"], "methods", ["home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.unilm.eval.Eval.__init__"], ["config", ",", "self", ".", "bert", ".", "embeddings", ".", "word_embeddings", ".", "weight", ")", "\n", "self", ".", "apply", "(", "self", ".", "init_bert_weights", ")", "\n", "\n", "", "def", "forward", "(", "self", ",", "input_ids", ",", "token_type_ids", "=", "None", ",", "attention_mask", "=", "None", ",", "masked_lm_labels", "=", "None", ",", "next_sentence_label", "=", "None", ",", "mask_qkv", "=", "None", ",", "task_idx", "=", "None", ")", ":", "\n", "        ", "sequence_output", ",", "pooled_output", "=", "self", ".", "bert", "(", "input_ids", ",", "token_type_ids", ",", "attention_mask", ",", "\n", "output_all_encoded_layers", "=", "False", ",", "mask_qkv", "=", "mask_qkv", ",", "task_idx", "=", "task_idx", ")", "\n", "prediction_scores", ",", "seq_relationship_score", "=", "self", ".", "cls", "(", "\n"]], "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.pytorch_pretrained_bert.modeling.BertForPreTraining.forward": [[1142, 1158], ["modeling.BertForPreTraining.bert", "modeling.BertForPreTraining.cls", "torch.nn.CrossEntropyLoss", "torch.nn.CrossEntropyLoss", "torch.nn.CrossEntropyLoss.", "torch.nn.CrossEntropyLoss.", "torch.nn.CrossEntropyLoss.", "torch.nn.CrossEntropyLoss.", "prediction_scores.view", "masked_lm_labels.view", "seq_relationship_score.view", "next_sentence_label.view"], "methods", ["None"], ["sequence_output", ",", "pooled_output", ")", "\n", "\n", "if", "masked_lm_labels", "is", "not", "None", "and", "next_sentence_label", "is", "not", "None", ":", "\n", "            ", "loss_fct", "=", "CrossEntropyLoss", "(", "ignore_index", "=", "-", "1", ")", "\n", "masked_lm_loss", "=", "loss_fct", "(", "\n", "prediction_scores", ".", "view", "(", "-", "1", ",", "self", ".", "config", ".", "vocab_size", ")", ",", "masked_lm_labels", ".", "view", "(", "-", "1", ")", ")", "\n", "next_sentence_loss", "=", "loss_fct", "(", "\n", "seq_relationship_score", ".", "view", "(", "-", "1", ",", "2", ")", ",", "next_sentence_label", ".", "view", "(", "-", "1", ")", ")", "\n", "total_loss", "=", "masked_lm_loss", "+", "next_sentence_loss", "\n", "return", "total_loss", "\n", "", "else", ":", "\n", "            ", "return", "prediction_scores", ",", "seq_relationship_score", "\n", "\n", "\n", "", "", "", "class", "BertPreTrainingPairTransform", "(", "nn", ".", "Module", ")", ":", "\n", "    ", "def", "__init__", "(", "self", ",", "config", ")", ":", "\n", "        ", "super", "(", "BertPreTrainingPairTransform", ",", "self", ")", ".", "__init__", "(", ")", "\n"]], "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.pytorch_pretrained_bert.modeling.BertPreTrainingPairTransform.__init__": [[1161, 1166], ["torch.nn.Module.__init__", "torch.nn.Linear", "torch.nn.Linear", "isinstance"], "methods", ["home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.unilm.eval.Eval.__init__"], ["if", "isinstance", "(", "config", ".", "hidden_act", ",", "str", ")", "else", "config", ".", "hidden_act", "\n", "# self.LayerNorm = BertLayerNorm(config.hidden_size, eps=1e-5)", "\n", "\n", "", "def", "forward", "(", "self", ",", "pair_x", ",", "pair_y", ")", ":", "\n", "        ", "hidden_states", "=", "torch", ".", "cat", "(", "[", "pair_x", ",", "pair_y", "]", ",", "dim", "=", "-", "1", ")", "\n", "hidden_states", "=", "self", ".", "dense", "(", "hidden_states", ")", "\n"]], "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.pytorch_pretrained_bert.modeling.BertPreTrainingPairTransform.forward": [[1168, 1174], ["torch.cat", "torch.cat", "torch.cat", "torch.cat", "modeling.BertPreTrainingPairTransform.dense", "modeling.BertPreTrainingPairTransform.transform_act_fn"], "methods", ["None"], ["# hidden_states = self.LayerNorm(hidden_states)", "\n", "return", "hidden_states", "\n", "\n", "\n", "", "", "class", "BertPreTrainingPairRel", "(", "nn", ".", "Module", ")", ":", "\n", "    ", "def", "__init__", "(", "self", ",", "config", ",", "num_rel", "=", "0", ")", ":", "\n", "        ", "super", "(", "BertPreTrainingPairRel", ",", "self", ")", ".", "__init__", "(", ")", "\n"]], "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.pytorch_pretrained_bert.modeling.BertPreTrainingPairRel.__init__": [[1177, 1181], ["torch.nn.Module.__init__", "modeling.BertPreTrainingPairTransform", "torch.nn.Embedding", "torch.nn.Embedding"], "methods", ["home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.unilm.eval.Eval.__init__"], ["\n", "", "def", "forward", "(", "self", ",", "pair_x", ",", "pair_y", ",", "pair_r", ",", "pair_pos_neg_mask", ")", ":", "\n", "# (batch, num_pair, hidden)", "\n", "        ", "xy", "=", "self", ".", "R_xy", "(", "pair_x", ",", "pair_y", ")", "\n", "r", "=", "self", ".", "rel_emb", "(", "pair_r", ")", "\n"]], "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.pytorch_pretrained_bert.modeling.BertPreTrainingPairRel.forward": [[1182, 1191], ["modeling.BertPreTrainingPairRel.R_xy", "modeling.BertPreTrainingPairRel.rel_emb", "modeling.BertPreTrainingPairRel.size", "torch.logsigmoid().mul_", "torch.logsigmoid().mul_", "torch.logsigmoid", "torch.logsigmoid", "pair_pos_neg_mask.type_as"], "methods", ["None"], ["_batch", ",", "_num_pair", ",", "_hidden", "=", "xy", ".", "size", "(", ")", "\n", "pair_score", "=", "(", "xy", "*", "r", ")", ".", "sum", "(", "-", "1", ")", "\n", "# torch.bmm(xy.view(-1, 1, _hidden),r.view(-1, _hidden, 1)).view(_batch, _num_pair)", "\n", "# .mul_(-1.0): objective to loss", "\n", "return", "F", ".", "logsigmoid", "(", "pair_score", "*", "pair_pos_neg_mask", ".", "type_as", "(", "pair_score", ")", ")", ".", "mul_", "(", "-", "1.0", ")", "\n", "\n", "\n", "", "", "class", "BertForPreTrainingLossMask", "(", "PreTrainedBertModel", ")", ":", "\n", "    ", "\"\"\"refer to BertForPreTraining\"\"\"", "\n", "def", "__init__", "(", "self", ",", "config", ",", "num_labels", "=", "2", ",", "num_rel", "=", "0", ",", "num_sentlvl_labels", "=", "0", ",", "no_nsp", "=", "False", ")", ":", "\n"]], "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.pytorch_pretrained_bert.modeling.BertForPreTrainingLossMask.__init__": [[1196, 1225], ["modeling.PreTrainedBertModel.__init__", "modeling.BertModel", "modeling.BertPreTrainingHeads", "torch.nn.CrossEntropyLoss", "torch.nn.CrossEntropyLoss", "modeling.BertForPreTrainingLossMask.apply", "modeling.BertForPreTrainingLossMask.bert.rescale_some_parameters", "torch.nn.Embedding", "torch.nn.Embedding", "modeling.BertPreTrainingHeads", "torch.nn.CrossEntropyLoss", "torch.nn.CrossEntropyLoss", "modeling.BertPreTrainingPairRel", "hasattr", "loss.LabelSmoothingLoss"], "methods", ["home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.unilm.eval.Eval.__init__", "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.pytorch_pretrained_bert.modeling.BertModel.rescale_some_parameters"], ["self", ".", "num_sentlvl_labels", "=", "num_sentlvl_labels", "\n", "self", ".", "cls2", "=", "None", "\n", "if", "self", ".", "num_sentlvl_labels", ">", "0", ":", "\n", "            ", "self", ".", "secondary_pred_proj", "=", "nn", ".", "Embedding", "(", "\n", "num_sentlvl_labels", ",", "config", ".", "hidden_size", ")", "\n", "self", ".", "cls2", "=", "BertPreTrainingHeads", "(", "\n", "config", ",", "self", ".", "secondary_pred_proj", ".", "weight", ",", "num_labels", "=", "num_sentlvl_labels", ")", "\n", "", "self", ".", "crit_mask_lm", "=", "nn", ".", "CrossEntropyLoss", "(", "reduction", "=", "'none'", ")", "\n", "if", "no_nsp", ":", "\n", "            ", "self", ".", "crit_next_sent", "=", "None", "\n", "", "else", ":", "\n", "            ", "self", ".", "crit_next_sent", "=", "nn", ".", "CrossEntropyLoss", "(", "ignore_index", "=", "-", "1", ")", "\n", "", "self", ".", "num_labels", "=", "num_labels", "\n", "self", ".", "num_rel", "=", "num_rel", "\n", "if", "self", ".", "num_rel", ">", "0", ":", "\n", "            ", "self", ".", "crit_pair_rel", "=", "BertPreTrainingPairRel", "(", "\n", "config", ",", "num_rel", "=", "num_rel", ")", "\n", "", "if", "hasattr", "(", "config", ",", "'label_smoothing'", ")", "and", "config", ".", "label_smoothing", ":", "\n", "            ", "self", ".", "crit_mask_lm_smoothed", "=", "LabelSmoothingLoss", "(", "\n", "config", ".", "label_smoothing", ",", "config", ".", "vocab_size", ",", "ignore_index", "=", "0", ",", "reduction", "=", "'none'", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "crit_mask_lm_smoothed", "=", "None", "\n", "", "self", ".", "apply", "(", "self", ".", "init_bert_weights", ")", "\n", "self", ".", "bert", ".", "rescale_some_parameters", "(", ")", "\n", "\n", "", "def", "forward", "(", "self", ",", "input_ids", ",", "token_type_ids", "=", "None", ",", "attention_mask", "=", "None", ",", "masked_lm_labels", "=", "None", ",", "\n", "next_sentence_label", "=", "None", ",", "masked_pos", "=", "None", ",", "masked_weights", "=", "None", ",", "task_idx", "=", "None", ",", "pair_x", "=", "None", ",", "\n", "pair_x_mask", "=", "None", ",", "pair_y", "=", "None", ",", "pair_y_mask", "=", "None", ",", "pair_r", "=", "None", ",", "pair_pos_neg_mask", "=", "None", ",", "\n", "pair_loss_mask", "=", "None", ",", "masked_pos_2", "=", "None", ",", "masked_weights_2", "=", "None", ",", "masked_labels_2", "=", "None", ",", "\n", "num_tokens_a", "=", "None", ",", "num_tokens_b", "=", "None", ",", "mask_qkv", "=", "None", ")", ":", "\n"]], "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.pytorch_pretrained_bert.modeling.BertForPreTrainingLossMask.forward": [[1226, 1357], ["modeling.BertForPreTrainingLossMask.bert", "modeling.BertForPreTrainingLossMask.forward.gather_seq_out_by_pos"], "methods", ["None"], ["# next_sentence_label (``torch.LongTensor`` of shape ``(batch_size,)``, `optional`):", "\n", "# Labels for computing the next sequence prediction (classification) loss. Input should be a sequence pair", "\n", "# (see :obj:`input_ids` docstring) Indices should be in ``[0, 1]``:", "\n", "\n", "# - 0 indicates sequence B is a continuation of sequence A,", "\n", "# - 1 indicates sequence B is a random sequence.", "\n", "\n", "        ", "if", "token_type_ids", "is", "None", "and", "attention_mask", "is", "None", ":", "\n", "            ", "task_0", "=", "(", "task_idx", "==", "0", ")", "\n", "task_1", "=", "(", "task_idx", "==", "1", ")", "\n", "task_2", "=", "(", "task_idx", "==", "2", ")", "\n", "task_3", "=", "(", "task_idx", "==", "3", ")", "\n", "\n", "sequence_length", "=", "input_ids", ".", "shape", "[", "-", "1", "]", "\n", "index_matrix", "=", "torch", ".", "arange", "(", "sequence_length", ")", ".", "view", "(", "\n", "1", ",", "sequence_length", ")", ".", "to", "(", "input_ids", ".", "device", ")", "\n", "\n", "num_tokens", "=", "num_tokens_a", "+", "num_tokens_b", "\n", "\n", "base_mask", "=", "(", "index_matrix", "<", "num_tokens", ".", "view", "(", "-", "1", ",", "1", ")", "\n", ")", ".", "type_as", "(", "input_ids", ")", "\n", "segment_a_mask", "=", "(", "\n", "index_matrix", "<", "num_tokens_a", ".", "view", "(", "-", "1", ",", "1", ")", ")", ".", "type_as", "(", "input_ids", ")", "\n", "\n", "token_type_ids", "=", "(", "\n", "task_idx", "+", "1", "+", "task_3", ".", "type_as", "(", "task_idx", ")", ")", ".", "view", "(", "-", "1", ",", "1", ")", "*", "base_mask", "\n", "token_type_ids", "=", "token_type_ids", "-", "segment_a_mask", "*", "(", "task_0", "|", "task_3", ")", ".", "type_as", "(", "segment_a_mask", ")", ".", "view", "(", "-", "1", ",", "1", ")", "\n", "\n", "index_matrix", "=", "index_matrix", ".", "view", "(", "1", ",", "1", ",", "sequence_length", ")", "\n", "index_matrix_t", "=", "index_matrix", ".", "view", "(", "1", ",", "sequence_length", ",", "1", ")", "\n", "\n", "tril", "=", "index_matrix", "<=", "index_matrix_t", "\n", "\n", "attention_mask_task_0", "=", "(", "\n", "index_matrix", "<", "num_tokens", ".", "view", "(", "-", "1", ",", "1", ",", "1", ")", ")", "&", "(", "index_matrix_t", "<", "num_tokens", ".", "view", "(", "-", "1", ",", "1", ",", "1", ")", ")", "\n", "attention_mask_task_1", "=", "tril", "&", "attention_mask_task_0", "\n", "attention_mask_task_2", "=", "torch", ".", "transpose", "(", "\n", "tril", ",", "dim0", "=", "-", "2", ",", "dim1", "=", "-", "1", ")", "&", "attention_mask_task_0", "\n", "attention_mask_task_3", "=", "(", "\n", "(", "index_matrix", "<", "num_tokens_a", ".", "view", "(", "-", "1", ",", "1", ",", "1", ")", ")", "|", "tril", ")", "&", "attention_mask_task_0", "\n", "\n", "attention_mask", "=", "(", "attention_mask_task_0", "&", "task_0", ".", "view", "(", "-", "1", ",", "1", ",", "1", ")", ")", "|", "(", "attention_mask_task_1", "&", "task_1", ".", "view", "(", "-", "1", ",", "1", ",", "1", ")", ")", "|", "(", "attention_mask_task_2", "&", "task_2", ".", "view", "(", "-", "1", ",", "1", ",", "1", ")", ")", "|", "(", "attention_mask_task_3", "&", "task_3", ".", "view", "(", "-", "1", ",", "1", ",", "1", ")", ")", "\n", "attention_mask", "=", "attention_mask", ".", "type_as", "(", "input_ids", ")", "\n", "", "sequence_output", ",", "pooled_output", "=", "self", ".", "bert", "(", "\n", "input_ids", ",", "token_type_ids", ",", "attention_mask", ",", "output_all_encoded_layers", "=", "False", ",", "mask_qkv", "=", "mask_qkv", ",", "task_idx", "=", "task_idx", ")", "\n", "\n", "def", "gather_seq_out_by_pos", "(", "seq", ",", "pos", ")", ":", "\n", "            ", "return", "torch", ".", "gather", "(", "seq", ",", "1", ",", "pos", ".", "unsqueeze", "(", "2", ")", ".", "expand", "(", "-", "1", ",", "-", "1", ",", "seq", ".", "size", "(", "-", "1", ")", ")", ")", "\n", "\n", "", "def", "gather_seq_out_by_pos_average", "(", "seq", ",", "pos", ",", "mask", ")", ":", "\n", "# pos/mask: (batch, num_pair, max_token_num)", "\n", "            ", "batch_size", ",", "max_token_num", "=", "pos", ".", "size", "(", "0", ")", ",", "pos", ".", "size", "(", "-", "1", ")", "\n", "# (batch, num_pair, max_token_num, seq.size(-1))", "\n", "pos_vec", "=", "torch", ".", "gather", "(", "seq", ",", "1", ",", "pos", ".", "view", "(", "batch_size", ",", "-", "1", ")", ".", "unsqueeze", "(", "\n", "2", ")", ".", "expand", "(", "-", "1", ",", "-", "1", ",", "seq", ".", "size", "(", "-", "1", ")", ")", ")", ".", "view", "(", "batch_size", ",", "-", "1", ",", "max_token_num", ",", "seq", ".", "size", "(", "-", "1", ")", ")", "\n", "# (batch, num_pair, seq.size(-1))", "\n", "mask", "=", "mask", ".", "type_as", "(", "pos_vec", ")", "\n", "pos_vec_masked_sum", "=", "(", "\n", "pos_vec", "*", "mask", ".", "unsqueeze", "(", "3", ")", ".", "expand_as", "(", "pos_vec", ")", ")", ".", "sum", "(", "2", ")", "\n", "return", "pos_vec_masked_sum", "/", "mask", ".", "sum", "(", "2", ",", "keepdim", "=", "True", ")", ".", "expand_as", "(", "pos_vec_masked_sum", ")", "\n", "\n", "", "def", "loss_mask_and_normalize", "(", "loss", ",", "mask", ")", ":", "\n", "            ", "mask", "=", "mask", ".", "type_as", "(", "loss", ")", "\n", "loss", "=", "loss", "*", "mask", "\n", "denominator", "=", "torch", ".", "sum", "(", "mask", ")", "+", "1e-5", "\n", "return", "(", "loss", "/", "denominator", ")", ".", "sum", "(", ")", "\n", "\n", "", "if", "masked_lm_labels", "is", "None", ":", "\n", "            ", "if", "masked_pos", "is", "None", ":", "\n", "                ", "prediction_scores", ",", "seq_relationship_score", "=", "self", ".", "cls", "(", "\n", "sequence_output", ",", "pooled_output", ",", "task_idx", "=", "task_idx", ")", "\n", "", "else", ":", "\n", "                ", "sequence_output_masked", "=", "gather_seq_out_by_pos", "(", "\n", "sequence_output", ",", "masked_pos", ")", "\n", "prediction_scores", ",", "seq_relationship_score", "=", "self", ".", "cls", "(", "\n", "sequence_output_masked", ",", "pooled_output", ",", "task_idx", "=", "task_idx", ")", "\n", "", "return", "prediction_scores", ",", "seq_relationship_score", "\n", "\n", "# masked lm", "\n", "", "sequence_output_masked", "=", "gather_seq_out_by_pos", "(", "\n", "sequence_output", ",", "masked_pos", ")", "\n", "prediction_scores_masked", ",", "seq_relationship_score", "=", "self", ".", "cls", "(", "\n", "sequence_output_masked", ",", "pooled_output", ",", "task_idx", "=", "task_idx", ")", "\n", "if", "self", ".", "crit_mask_lm_smoothed", ":", "\n", "            ", "masked_lm_loss", "=", "self", ".", "crit_mask_lm_smoothed", "(", "\n", "F", ".", "log_softmax", "(", "prediction_scores_masked", ".", "float", "(", ")", ",", "dim", "=", "-", "1", ")", ",", "masked_lm_labels", ")", "\n", "", "else", ":", "\n", "            ", "masked_lm_loss", "=", "self", ".", "crit_mask_lm", "(", "\n", "prediction_scores_masked", ".", "transpose", "(", "1", ",", "2", ")", ".", "float", "(", ")", ",", "masked_lm_labels", ")", "\n", "", "masked_lm_loss", "=", "loss_mask_and_normalize", "(", "\n", "masked_lm_loss", ".", "float", "(", ")", ",", "masked_weights", ")", "\n", "\n", "# next sentence", "\n", "if", "self", ".", "crit_next_sent", "is", "None", "or", "next_sentence_label", "is", "None", ":", "\n", "            ", "next_sentence_loss", "=", "0.0", "\n", "", "else", ":", "\n", "            ", "next_sentence_loss", "=", "self", ".", "crit_next_sent", "(", "\n", "seq_relationship_score", ".", "view", "(", "-", "1", ",", "self", ".", "num_labels", ")", ".", "float", "(", ")", ",", "next_sentence_label", ".", "view", "(", "-", "1", ")", ")", "\n", "\n", "", "if", "self", ".", "cls2", "is", "not", "None", "and", "masked_pos_2", "is", "not", "None", ":", "\n", "            ", "sequence_output_masked_2", "=", "gather_seq_out_by_pos", "(", "\n", "sequence_output", ",", "masked_pos_2", ")", "\n", "prediction_scores_masked_2", ",", "_", "=", "self", ".", "cls2", "(", "\n", "sequence_output_masked_2", ",", "None", ")", "\n", "masked_lm_loss_2", "=", "self", ".", "crit_mask_lm", "(", "\n", "prediction_scores_masked_2", ".", "transpose", "(", "1", ",", "2", ")", ".", "float", "(", ")", ",", "masked_labels_2", ")", "\n", "masked_lm_loss_2", "=", "loss_mask_and_normalize", "(", "\n", "masked_lm_loss_2", ".", "float", "(", ")", ",", "masked_weights_2", ")", "\n", "masked_lm_loss", "=", "masked_lm_loss", "+", "masked_lm_loss_2", "\n", "\n", "", "if", "pair_x", "is", "None", "or", "pair_y", "is", "None", "or", "pair_r", "is", "None", "or", "pair_pos_neg_mask", "is", "None", "or", "pair_loss_mask", "is", "None", ":", "\n", "            ", "return", "masked_lm_loss", ",", "next_sentence_loss", "\n", "\n", "# pair and relation", "\n", "", "if", "pair_x_mask", "is", "None", "or", "pair_y_mask", "is", "None", ":", "\n", "            ", "pair_x_output_masked", "=", "gather_seq_out_by_pos", "(", "\n", "sequence_output", ",", "pair_x", ")", "\n", "pair_y_output_masked", "=", "gather_seq_out_by_pos", "(", "\n", "sequence_output", ",", "pair_y", ")", "\n", "", "else", ":", "\n", "            ", "pair_x_output_masked", "=", "gather_seq_out_by_pos_average", "(", "\n", "sequence_output", ",", "pair_x", ",", "pair_x_mask", ")", "\n", "pair_y_output_masked", "=", "gather_seq_out_by_pos_average", "(", "\n", "sequence_output", ",", "pair_y", ",", "pair_y_mask", ")", "\n", "", "pair_loss", "=", "self", ".", "crit_pair_rel", "(", "\n", "pair_x_output_masked", ",", "pair_y_output_masked", ",", "pair_r", ",", "pair_pos_neg_mask", ")", "\n", "pair_loss", "=", "loss_mask_and_normalize", "(", "\n", "pair_loss", ".", "float", "(", ")", ",", "pair_loss_mask", ")", "\n"]], "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.pytorch_pretrained_bert.modeling.BertForExtractiveSummarization.__init__": [[1362, 1369], ["modeling.PreTrainedBertModel.__init__", "modeling.BertModel", "torch.nn.Embedding", "torch.nn.Embedding", "modeling.BertPreTrainingHeads", "modeling.BertForExtractiveSummarization.apply"], "methods", ["home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.unilm.eval.Eval.__init__"], ["    ", "\"\"\"refer to BertForPreTraining\"\"\"", "\n", "\n", "def", "__init__", "(", "self", ",", "config", ")", ":", "\n", "        ", "super", "(", "BertForExtractiveSummarization", ",", "self", ")", ".", "__init__", "(", "config", ")", "\n", "self", ".", "bert", "=", "BertModel", "(", "config", ")", "\n", "self", ".", "secondary_pred_proj", "=", "nn", ".", "Embedding", "(", "2", ",", "config", ".", "hidden_size", ")", "\n", "self", ".", "cls2", "=", "BertPreTrainingHeads", "(", "\n", "config", ",", "self", ".", "secondary_pred_proj", ".", "weight", ",", "num_labels", "=", "2", ")", "\n"]], "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.pytorch_pretrained_bert.modeling.BertForExtractiveSummarization.forward": [[1370, 1386], ["modeling.BertForExtractiveSummarization.bert", "modeling.BertForExtractiveSummarization.forward.gather_seq_out_by_pos"], "methods", ["None"], ["self", ".", "apply", "(", "self", ".", "init_bert_weights", ")", "\n", "\n", "", "def", "forward", "(", "self", ",", "input_ids", ",", "token_type_ids", "=", "None", ",", "attention_mask", "=", "None", ",", "masked_pos_2", "=", "None", ",", "masked_weights_2", "=", "None", ",", "task_idx", "=", "None", ",", "mask_qkv", "=", "None", ")", ":", "\n", "        ", "sequence_output", ",", "pooled_output", "=", "self", ".", "bert", "(", "input_ids", ",", "token_type_ids", ",", "attention_mask", ",", "\n", "output_all_encoded_layers", "=", "False", ",", "mask_qkv", "=", "mask_qkv", ",", "task_idx", "=", "task_idx", ")", "\n", "\n", "def", "gather_seq_out_by_pos", "(", "seq", ",", "pos", ")", ":", "\n", "            ", "return", "torch", ".", "gather", "(", "seq", ",", "1", ",", "pos", ".", "unsqueeze", "(", "2", ")", ".", "expand", "(", "-", "1", ",", "-", "1", ",", "seq", ".", "size", "(", "-", "1", ")", ")", ")", "\n", "\n", "", "sequence_output_masked_2", "=", "gather_seq_out_by_pos", "(", "\n", "sequence_output", ",", "masked_pos_2", ")", "\n", "prediction_scores_masked_2", ",", "_", "=", "self", ".", "cls2", "(", "\n", "sequence_output_masked_2", ",", "None", ",", "task_idx", "=", "task_idx", ")", "\n", "\n", "predicted_probs", "=", "torch", ".", "nn", ".", "functional", ".", "softmax", "(", "\n", "prediction_scores_masked_2", ",", "dim", "=", "-", "1", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.pytorch_pretrained_bert.modeling.BertForSeq2SeqDecoder.__init__": [[1391, 1419], ["modeling.PreTrainedBertModel.__init__", "modeling.BertModelIncr", "modeling.BertPreTrainingHeads", "modeling.BertForSeq2SeqDecoder.apply", "torch.nn.CrossEntropyLoss", "torch.nn.CrossEntropyLoss", "torch.nn.CrossEntropyLoss", "torch.nn.CrossEntropyLoss", "modeling.BertPreTrainingPairRel"], "methods", ["home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.unilm.eval.Eval.__init__"], ["    ", "\"\"\"refer to BertForPreTraining\"\"\"", "\n", "\n", "def", "__init__", "(", "self", ",", "config", ",", "mask_word_id", "=", "0", ",", "num_labels", "=", "2", ",", "num_rel", "=", "0", ",", "\n", "search_beam_size", "=", "1", ",", "length_penalty", "=", "1.0", ",", "eos_id", "=", "0", ",", "sos_id", "=", "0", ",", "\n", "forbid_duplicate_ngrams", "=", "False", ",", "forbid_ignore_set", "=", "None", ",", "not_predict_set", "=", "None", ",", "ngram_size", "=", "3", ",", "min_len", "=", "0", ",", "mode", "=", "\"s2s\"", ",", "pos_shift", "=", "False", ")", ":", "\n", "        ", "super", "(", "BertForSeq2SeqDecoder", ",", "self", ")", ".", "__init__", "(", "config", ")", "\n", "self", ".", "bert", "=", "BertModelIncr", "(", "config", ")", "\n", "self", ".", "cls", "=", "BertPreTrainingHeads", "(", "\n", "config", ",", "self", ".", "bert", ".", "embeddings", ".", "word_embeddings", ".", "weight", ",", "num_labels", "=", "num_labels", ")", "\n", "self", ".", "apply", "(", "self", ".", "init_bert_weights", ")", "\n", "self", ".", "crit_mask_lm", "=", "nn", ".", "CrossEntropyLoss", "(", "reduction", "=", "'none'", ")", "\n", "self", ".", "crit_next_sent", "=", "nn", ".", "CrossEntropyLoss", "(", "ignore_index", "=", "-", "1", ")", "\n", "self", ".", "mask_word_id", "=", "mask_word_id", "\n", "self", ".", "num_labels", "=", "num_labels", "\n", "self", ".", "num_rel", "=", "num_rel", "\n", "if", "self", ".", "num_rel", ">", "0", ":", "\n", "            ", "self", ".", "crit_pair_rel", "=", "BertPreTrainingPairRel", "(", "\n", "config", ",", "num_rel", "=", "num_rel", ")", "\n", "", "self", ".", "search_beam_size", "=", "search_beam_size", "\n", "self", ".", "length_penalty", "=", "length_penalty", "\n", "self", ".", "eos_id", "=", "eos_id", "\n", "self", ".", "sos_id", "=", "sos_id", "\n", "self", ".", "forbid_duplicate_ngrams", "=", "forbid_duplicate_ngrams", "\n", "self", ".", "forbid_ignore_set", "=", "forbid_ignore_set", "\n", "self", ".", "not_predict_set", "=", "not_predict_set", "\n", "self", ".", "ngram_size", "=", "ngram_size", "\n", "self", ".", "min_len", "=", "min_len", "\n", "assert", "mode", "in", "(", "\"s2s\"", ",", "\"l2r\"", ")", "\n", "self", ".", "mode", "=", "mode", "\n"]], "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.pytorch_pretrained_bert.modeling.BertForSeq2SeqDecoder.forward": [[1420, 1497], ["list", "list", "input_ids.new().fill_", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "modeling.BertForSeq2SeqDecoder.beam_search", "input_ids.size", "token_type_ids.size", "input_ids.new().fill_", "modeling.BertForSeq2SeqDecoder.bert", "modeling.BertForSeq2SeqDecoder.cls", "torch.max", "torch.max", "torch.max", "torch.max", "output_ids.append", "input_ids.new", "list", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "input_ids.new", "curr_ids.size", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "prediction_scores[].fill_", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "zip", "zip"], "methods", ["home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.pytorch_pretrained_bert.modeling.BertForSeq2SeqDecoder.beam_search"], ["self", ".", "pos_shift", "=", "pos_shift", "\n", "\n", "", "def", "forward", "(", "self", ",", "input_ids", ",", "token_type_ids", ",", "position_ids", ",", "attention_mask", ",", "task_idx", "=", "None", ",", "mask_qkv", "=", "None", ")", ":", "\n", "        ", "if", "self", ".", "search_beam_size", ">", "1", ":", "\n", "            ", "return", "self", ".", "beam_search", "(", "input_ids", ",", "token_type_ids", ",", "position_ids", ",", "attention_mask", ",", "task_idx", "=", "task_idx", ",", "mask_qkv", "=", "mask_qkv", ")", "\n", "\n", "", "input_shape", "=", "list", "(", "input_ids", ".", "size", "(", ")", ")", "\n", "batch_size", "=", "input_shape", "[", "0", "]", "\n", "input_length", "=", "input_shape", "[", "1", "]", "\n", "output_shape", "=", "list", "(", "token_type_ids", ".", "size", "(", ")", ")", "\n", "output_length", "=", "output_shape", "[", "1", "]", "\n", "\n", "output_ids", "=", "[", "]", "\n", "prev_embedding", "=", "None", "\n", "prev_encoded_layers", "=", "None", "\n", "curr_ids", "=", "input_ids", "\n", "mask_ids", "=", "input_ids", ".", "new", "(", "batch_size", ",", "1", ")", ".", "fill_", "(", "self", ".", "mask_word_id", ")", "\n", "next_pos", "=", "input_length", "\n", "if", "self", ".", "pos_shift", ":", "\n", "            ", "sos_ids", "=", "input_ids", ".", "new", "(", "batch_size", ",", "1", ")", ".", "fill_", "(", "self", ".", "sos_id", ")", "\n", "\n", "", "while", "next_pos", "<", "output_length", ":", "\n", "            ", "curr_length", "=", "list", "(", "curr_ids", ".", "size", "(", ")", ")", "[", "1", "]", "\n", "\n", "if", "self", ".", "pos_shift", ":", "\n", "                ", "if", "next_pos", "==", "input_length", ":", "\n", "                    ", "x_input_ids", "=", "torch", ".", "cat", "(", "(", "curr_ids", ",", "sos_ids", ")", ",", "dim", "=", "1", ")", "\n", "start_pos", "=", "0", "\n", "", "else", ":", "\n", "                    ", "x_input_ids", "=", "curr_ids", "\n", "start_pos", "=", "next_pos", "\n", "", "", "else", ":", "\n", "                ", "start_pos", "=", "next_pos", "-", "curr_length", "\n", "x_input_ids", "=", "torch", ".", "cat", "(", "(", "curr_ids", ",", "mask_ids", ")", ",", "dim", "=", "1", ")", "\n", "\n", "", "curr_token_type_ids", "=", "token_type_ids", "[", ":", ",", "start_pos", ":", "next_pos", "+", "1", "]", "\n", "curr_attention_mask", "=", "attention_mask", "[", ":", ",", "\n", "start_pos", ":", "next_pos", "+", "1", ",", ":", "next_pos", "+", "1", "]", "\n", "curr_position_ids", "=", "position_ids", "[", ":", ",", "start_pos", ":", "next_pos", "+", "1", "]", "\n", "new_embedding", ",", "new_encoded_layers", ",", "_", "=", "self", ".", "bert", "(", "x_input_ids", ",", "curr_token_type_ids", ",", "curr_position_ids", ",", "curr_attention_mask", ",", "\n", "output_all_encoded_layers", "=", "True", ",", "prev_embedding", "=", "prev_embedding", ",", "prev_encoded_layers", "=", "prev_encoded_layers", ",", "mask_qkv", "=", "mask_qkv", ")", "\n", "\n", "last_hidden", "=", "new_encoded_layers", "[", "-", "1", "]", "[", ":", ",", "-", "1", ":", ",", ":", "]", "\n", "prediction_scores", ",", "_", "=", "self", ".", "cls", "(", "\n", "last_hidden", ",", "None", ",", "task_idx", "=", "task_idx", ")", "\n", "if", "self", ".", "not_predict_set", ":", "\n", "                ", "for", "token_id", "in", "self", ".", "not_predict_set", ":", "\n", "                    ", "prediction_scores", "[", ":", ",", ":", ",", "token_id", "]", ".", "fill_", "(", "-", "10000.0", ")", "\n", "", "", "_", ",", "max_ids", "=", "torch", ".", "max", "(", "prediction_scores", ",", "dim", "=", "-", "1", ")", "\n", "output_ids", ".", "append", "(", "max_ids", ")", "\n", "\n", "if", "self", ".", "pos_shift", ":", "\n", "                ", "if", "prev_embedding", "is", "None", ":", "\n", "                    ", "prev_embedding", "=", "new_embedding", "\n", "", "else", ":", "\n", "                    ", "prev_embedding", "=", "torch", ".", "cat", "(", "\n", "(", "prev_embedding", ",", "new_embedding", ")", ",", "dim", "=", "1", ")", "\n", "", "if", "prev_encoded_layers", "is", "None", ":", "\n", "                    ", "prev_encoded_layers", "=", "[", "x", "for", "x", "in", "new_encoded_layers", "]", "\n", "", "else", ":", "\n", "                    ", "prev_encoded_layers", "=", "[", "torch", ".", "cat", "(", "(", "x", "[", "0", "]", ",", "x", "[", "1", "]", ")", ",", "dim", "=", "1", ")", "for", "x", "in", "zip", "(", "\n", "prev_encoded_layers", ",", "new_encoded_layers", ")", "]", "\n", "", "", "else", ":", "\n", "                ", "if", "prev_embedding", "is", "None", ":", "\n", "                    ", "prev_embedding", "=", "new_embedding", "[", ":", ",", ":", "-", "1", ",", ":", "]", "\n", "", "else", ":", "\n", "                    ", "prev_embedding", "=", "torch", ".", "cat", "(", "\n", "(", "prev_embedding", ",", "new_embedding", "[", ":", ",", ":", "-", "1", ",", ":", "]", ")", ",", "dim", "=", "1", ")", "\n", "", "if", "prev_encoded_layers", "is", "None", ":", "\n", "                    ", "prev_encoded_layers", "=", "[", "x", "[", ":", ",", ":", "-", "1", ",", ":", "]", "\n", "for", "x", "in", "new_encoded_layers", "]", "\n", "", "else", ":", "\n", "                    ", "prev_encoded_layers", "=", "[", "torch", ".", "cat", "(", "(", "x", "[", "0", "]", ",", "x", "[", "1", "]", "[", ":", ",", ":", "-", "1", ",", ":", "]", ")", ",", "dim", "=", "1", ")", "\n", "for", "x", "in", "zip", "(", "prev_encoded_layers", ",", "new_encoded_layers", ")", "]", "\n", "", "", "curr_ids", "=", "max_ids", "\n", "next_pos", "+=", "1", "\n", "\n"]], "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.pytorch_pretrained_bert.modeling.BertForSeq2SeqDecoder.beam_search": [[1498, 1772], ["list", "list", "input_ids.new().fill_", "range", "input_ids.size", "first_expand.size", "input_ids.new().fill_", "modeling.BertForSeq2SeqDecoder.bert", "modeling.BertForSeq2SeqDecoder.cls", "torch.nn.functional.log_softmax", "torch.nn.functional.log_softmax", "torch.nn.functional.log_softmax", "torch.nn.functional.log_softmax", "torch.topk", "torch.topk", "torch.topk", "torch.topk", "step_back_ptrs.append", "step_ids.append", "beam_masks.append", "total_scores.append", "torch.reshape", "torch.reshape", "torch.reshape", "torch.reshape", "torch.reshape.tolist", "torch.reshape.tolist", "torch.reshape.tolist", "torch.reshape.tolist", "torch.reshape.tolist", "torch.reshape.tolist", "traces[].append", "traces[].append", "traces[].append", "enumerate", "range", "sequences[].data.new().fill_", "enumerate", "_pad_sequence().to", "input_ids.new", "list", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "log_scores[].fill_", "len", "torch.reshape", "torch.reshape", "torch.reshape", "torch.reshape", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.reshape", "torch.reshape", "torch.reshape", "torch.reshape", "torch.reshape", "torch.reshape", "torch.reshape", "torch.reshape", "torch.reshape", "torch.reshape", "torch.reshape", "torch.reshape", "torch.reshape", "torch.reshape", "torch.reshape", "torch.reshape", "torch.topk", "torch.topk", "torch.topk", "torch.topk", "torch.div", "torch.div", "torch.div", "torch.div", "torch.reshape", "torch.reshape", "torch.reshape", "torch.reshape", "torch.gather", "torch.gather", "torch.gather", "torch.gather", "torch.eq().float", "torch.eq().float", "torch.eq().float", "torch.eq().float", "list", "torch.reshape", "torch.reshape", "torch.reshape", "torch.reshape", "torch.reshape.repeat", "torch.reshape.repeat", "torch.reshape", "torch.reshape", "torch.reshape", "torch.reshape", "list", "len", "list", "torch.reshape", "torch.reshape", "torch.reshape", "torch.reshape", "torch.gather", "torch.gather", "torch.gather", "torch.gather", "torch.reshape", "torch.reshape", "torch.reshape", "torch.reshape", "modeling.BertForSeq2SeqDecoder.beam_search.first_expand"], "methods", ["None"], ["", "return", "torch", ".", "cat", "(", "output_ids", ",", "dim", "=", "1", ")", "\n", "\n", "", "def", "beam_search", "(", "self", ",", "input_ids", ",", "token_type_ids", ",", "position_ids", ",", "attention_mask", ",", "task_idx", "=", "None", ",", "mask_qkv", "=", "None", ")", ":", "\n", "        ", "input_shape", "=", "list", "(", "input_ids", ".", "size", "(", ")", ")", "\n", "batch_size", "=", "input_shape", "[", "0", "]", "\n", "input_length", "=", "input_shape", "[", "1", "]", "\n", "output_shape", "=", "list", "(", "token_type_ids", ".", "size", "(", ")", ")", "\n", "output_length", "=", "output_shape", "[", "1", "]", "\n", "\n", "output_ids", "=", "[", "]", "\n", "prev_embedding", "=", "None", "\n", "prev_encoded_layers", "=", "None", "\n", "curr_ids", "=", "input_ids", "\n", "mask_ids", "=", "input_ids", ".", "new", "(", "batch_size", ",", "1", ")", ".", "fill_", "(", "self", ".", "mask_word_id", ")", "\n", "next_pos", "=", "input_length", "\n", "if", "self", ".", "pos_shift", ":", "\n", "            ", "sos_ids", "=", "input_ids", ".", "new", "(", "batch_size", ",", "1", ")", ".", "fill_", "(", "self", ".", "sos_id", ")", "\n", "\n", "", "K", "=", "self", ".", "search_beam_size", "\n", "\n", "total_scores", "=", "[", "]", "\n", "beam_masks", "=", "[", "]", "\n", "step_ids", "=", "[", "]", "\n", "step_back_ptrs", "=", "[", "]", "\n", "partial_seqs", "=", "[", "]", "\n", "forbid_word_mask", "=", "None", "\n", "buf_matrix", "=", "None", "\n", "\n", "while", "next_pos", "<", "output_length", ":", "\n", "            ", "curr_length", "=", "list", "(", "curr_ids", ".", "size", "(", ")", ")", "[", "1", "]", "\n", "\n", "if", "self", ".", "pos_shift", ":", "\n", "                ", "if", "next_pos", "==", "input_length", ":", "\n", "                    ", "x_input_ids", "=", "torch", ".", "cat", "(", "(", "curr_ids", ",", "sos_ids", ")", ",", "dim", "=", "1", ")", "\n", "start_pos", "=", "0", "\n", "", "else", ":", "\n", "                    ", "x_input_ids", "=", "curr_ids", "\n", "start_pos", "=", "next_pos", "\n", "", "", "else", ":", "\n", "                ", "start_pos", "=", "next_pos", "-", "curr_length", "\n", "x_input_ids", "=", "torch", ".", "cat", "(", "(", "curr_ids", ",", "mask_ids", ")", ",", "dim", "=", "1", ")", "\n", "\n", "", "curr_token_type_ids", "=", "token_type_ids", "[", ":", ",", "start_pos", ":", "next_pos", "+", "1", "]", "\n", "curr_attention_mask", "=", "attention_mask", "[", ":", ",", "\n", "start_pos", ":", "next_pos", "+", "1", ",", ":", "next_pos", "+", "1", "]", "\n", "curr_position_ids", "=", "position_ids", "[", ":", ",", "start_pos", ":", "next_pos", "+", "1", "]", "\n", "new_embedding", ",", "new_encoded_layers", ",", "_", "=", "self", ".", "bert", "(", "x_input_ids", ",", "curr_token_type_ids", ",", "curr_position_ids", ",", "curr_attention_mask", ",", "\n", "output_all_encoded_layers", "=", "True", ",", "prev_embedding", "=", "prev_embedding", ",", "prev_encoded_layers", "=", "prev_encoded_layers", ",", "mask_qkv", "=", "mask_qkv", ")", "\n", "\n", "last_hidden", "=", "new_encoded_layers", "[", "-", "1", "]", "[", ":", ",", "-", "1", ":", ",", ":", "]", "\n", "prediction_scores", ",", "_", "=", "self", ".", "cls", "(", "\n", "last_hidden", ",", "None", ",", "task_idx", "=", "task_idx", ")", "\n", "log_scores", "=", "torch", ".", "nn", ".", "functional", ".", "log_softmax", "(", "\n", "prediction_scores", ",", "dim", "=", "-", "1", ")", "\n", "if", "forbid_word_mask", "is", "not", "None", ":", "\n", "                ", "log_scores", "+=", "(", "forbid_word_mask", "*", "-", "10000.0", ")", "\n", "", "if", "self", ".", "min_len", "and", "(", "next_pos", "-", "input_length", "+", "1", "<=", "self", ".", "min_len", ")", ":", "\n", "                ", "log_scores", "[", ":", ",", ":", ",", "self", ".", "eos_id", "]", ".", "fill_", "(", "-", "10000.0", ")", "\n", "", "if", "self", ".", "not_predict_set", ":", "\n", "                ", "for", "token_id", "in", "self", ".", "not_predict_set", ":", "\n", "                    ", "log_scores", "[", ":", ",", ":", ",", "token_id", "]", ".", "fill_", "(", "-", "10000.0", ")", "\n", "", "", "kk_scores", ",", "kk_ids", "=", "torch", ".", "topk", "(", "log_scores", ",", "k", "=", "K", ")", "\n", "if", "len", "(", "total_scores", ")", "==", "0", ":", "\n", "                ", "k_ids", "=", "torch", ".", "reshape", "(", "kk_ids", ",", "[", "batch_size", ",", "K", "]", ")", "\n", "back_ptrs", "=", "torch", ".", "zeros", "(", "batch_size", ",", "K", ",", "dtype", "=", "torch", ".", "long", ")", "\n", "k_scores", "=", "torch", ".", "reshape", "(", "kk_scores", ",", "[", "batch_size", ",", "K", "]", ")", "\n", "", "else", ":", "\n", "                ", "last_eos", "=", "torch", ".", "reshape", "(", "\n", "beam_masks", "[", "-", "1", "]", ",", "[", "batch_size", "*", "K", ",", "1", ",", "1", "]", ")", "\n", "last_seq_scores", "=", "torch", ".", "reshape", "(", "\n", "total_scores", "[", "-", "1", "]", ",", "[", "batch_size", "*", "K", ",", "1", ",", "1", "]", ")", "\n", "kk_scores", "+=", "last_eos", "*", "(", "-", "10000.0", ")", "+", "last_seq_scores", "\n", "kk_scores", "=", "torch", ".", "reshape", "(", "kk_scores", ",", "[", "batch_size", ",", "K", "*", "K", "]", ")", "\n", "k_scores", ",", "k_ids", "=", "torch", ".", "topk", "(", "kk_scores", ",", "k", "=", "K", ")", "\n", "back_ptrs", "=", "torch", ".", "div", "(", "k_ids", ",", "K", ")", "\n", "kk_ids", "=", "torch", ".", "reshape", "(", "kk_ids", ",", "[", "batch_size", ",", "K", "*", "K", "]", ")", "\n", "k_ids", "=", "torch", ".", "gather", "(", "kk_ids", ",", "1", ",", "k_ids", ")", "\n", "", "step_back_ptrs", ".", "append", "(", "back_ptrs", ")", "\n", "step_ids", ".", "append", "(", "k_ids", ")", "\n", "beam_masks", ".", "append", "(", "torch", ".", "eq", "(", "k_ids", ",", "self", ".", "eos_id", ")", ".", "float", "(", ")", ")", "\n", "total_scores", ".", "append", "(", "k_scores", ")", "\n", "\n", "def", "first_expand", "(", "x", ")", ":", "\n", "                ", "input_shape", "=", "list", "(", "x", ".", "size", "(", ")", ")", "\n", "expanded_shape", "=", "input_shape", "[", ":", "1", "]", "+", "[", "1", "]", "+", "input_shape", "[", "1", ":", "]", "\n", "x", "=", "torch", ".", "reshape", "(", "x", ",", "expanded_shape", ")", "\n", "repeat_count", "=", "[", "1", ",", "K", "]", "+", "[", "1", "]", "*", "(", "len", "(", "input_shape", ")", "-", "1", ")", "\n", "x", "=", "x", ".", "repeat", "(", "*", "repeat_count", ")", "\n", "x", "=", "torch", ".", "reshape", "(", "x", ",", "[", "input_shape", "[", "0", "]", "*", "K", "]", "+", "input_shape", "[", "1", ":", "]", ")", "\n", "return", "x", "\n", "\n", "", "def", "select_beam_items", "(", "x", ",", "ids", ")", ":", "\n", "                ", "id_shape", "=", "list", "(", "ids", ".", "size", "(", ")", ")", "\n", "id_rank", "=", "len", "(", "id_shape", ")", "\n", "assert", "len", "(", "id_shape", ")", "==", "2", "\n", "x_shape", "=", "list", "(", "x", ".", "size", "(", ")", ")", "\n", "x", "=", "torch", ".", "reshape", "(", "x", ",", "[", "batch_size", ",", "K", "]", "+", "x_shape", "[", "1", ":", "]", ")", "\n", "x_rank", "=", "len", "(", "x_shape", ")", "+", "1", "\n", "assert", "x_rank", ">=", "2", "\n", "if", "id_rank", "<", "x_rank", ":", "\n", "                    ", "ids", "=", "torch", ".", "reshape", "(", "\n", "ids", ",", "id_shape", "+", "[", "1", "]", "*", "(", "x_rank", "-", "id_rank", ")", ")", "\n", "ids", "=", "ids", ".", "expand", "(", "id_shape", "+", "x_shape", "[", "1", ":", "]", ")", "\n", "", "y", "=", "torch", ".", "gather", "(", "x", ",", "1", ",", "ids", ".", "long", "(", ")", ")", "\n", "y", "=", "torch", ".", "reshape", "(", "y", ",", "x_shape", ")", "\n", "return", "y", "\n", "\n", "", "is_first", "=", "(", "prev_embedding", "is", "None", ")", "\n", "\n", "if", "self", ".", "pos_shift", ":", "\n", "                ", "if", "prev_embedding", "is", "None", ":", "\n", "                    ", "prev_embedding", "=", "first_expand", "(", "new_embedding", ")", "\n", "", "else", ":", "\n", "                    ", "prev_embedding", "=", "torch", ".", "cat", "(", "\n", "(", "prev_embedding", ",", "new_embedding", ")", ",", "dim", "=", "1", ")", "\n", "prev_embedding", "=", "select_beam_items", "(", "\n", "prev_embedding", ",", "back_ptrs", ")", "\n", "", "if", "prev_encoded_layers", "is", "None", ":", "\n", "                    ", "prev_encoded_layers", "=", "[", "first_expand", "(", "\n", "x", ")", "for", "x", "in", "new_encoded_layers", "]", "\n", "", "else", ":", "\n", "                    ", "prev_encoded_layers", "=", "[", "torch", ".", "cat", "(", "(", "x", "[", "0", "]", ",", "x", "[", "1", "]", ")", ",", "dim", "=", "1", ")", "for", "x", "in", "zip", "(", "\n", "prev_encoded_layers", ",", "new_encoded_layers", ")", "]", "\n", "prev_encoded_layers", "=", "[", "select_beam_items", "(", "\n", "x", ",", "back_ptrs", ")", "for", "x", "in", "prev_encoded_layers", "]", "\n", "", "", "else", ":", "\n", "                ", "if", "prev_embedding", "is", "None", ":", "\n", "                    ", "prev_embedding", "=", "first_expand", "(", "new_embedding", "[", ":", ",", ":", "-", "1", ",", ":", "]", ")", "\n", "", "else", ":", "\n", "                    ", "prev_embedding", "=", "torch", ".", "cat", "(", "\n", "(", "prev_embedding", ",", "new_embedding", "[", ":", ",", ":", "-", "1", ",", ":", "]", ")", ",", "dim", "=", "1", ")", "\n", "prev_embedding", "=", "select_beam_items", "(", "\n", "prev_embedding", ",", "back_ptrs", ")", "\n", "", "if", "prev_encoded_layers", "is", "None", ":", "\n", "                    ", "prev_encoded_layers", "=", "[", "first_expand", "(", "\n", "x", "[", ":", ",", ":", "-", "1", ",", ":", "]", ")", "for", "x", "in", "new_encoded_layers", "]", "\n", "", "else", ":", "\n", "                    ", "prev_encoded_layers", "=", "[", "torch", ".", "cat", "(", "(", "x", "[", "0", "]", ",", "x", "[", "1", "]", "[", ":", ",", ":", "-", "1", ",", ":", "]", ")", ",", "dim", "=", "1", ")", "\n", "for", "x", "in", "zip", "(", "prev_encoded_layers", ",", "new_encoded_layers", ")", "]", "\n", "prev_encoded_layers", "=", "[", "select_beam_items", "(", "\n", "x", ",", "back_ptrs", ")", "for", "x", "in", "prev_encoded_layers", "]", "\n", "\n", "", "", "curr_ids", "=", "torch", ".", "reshape", "(", "k_ids", ",", "[", "batch_size", "*", "K", ",", "1", "]", ")", "\n", "\n", "if", "is_first", ":", "\n", "                ", "token_type_ids", "=", "first_expand", "(", "token_type_ids", ")", "\n", "position_ids", "=", "first_expand", "(", "position_ids", ")", "\n", "attention_mask", "=", "first_expand", "(", "attention_mask", ")", "\n", "mask_ids", "=", "first_expand", "(", "mask_ids", ")", "\n", "if", "mask_qkv", "is", "not", "None", ":", "\n", "                    ", "mask_qkv", "=", "first_expand", "(", "mask_qkv", ")", "\n", "\n", "", "", "if", "self", ".", "forbid_duplicate_ngrams", ":", "\n", "                ", "wids", "=", "step_ids", "[", "-", "1", "]", ".", "tolist", "(", ")", "\n", "ptrs", "=", "step_back_ptrs", "[", "-", "1", "]", ".", "tolist", "(", ")", "\n", "if", "is_first", ":", "\n", "                    ", "partial_seqs", "=", "[", "]", "\n", "for", "b", "in", "range", "(", "batch_size", ")", ":", "\n", "                        ", "for", "k", "in", "range", "(", "K", ")", ":", "\n", "                            ", "partial_seqs", ".", "append", "(", "[", "wids", "[", "b", "]", "[", "k", "]", "]", ")", "\n", "", "", "", "else", ":", "\n", "                    ", "new_partial_seqs", "=", "[", "]", "\n", "for", "b", "in", "range", "(", "batch_size", ")", ":", "\n", "                        ", "for", "k", "in", "range", "(", "K", ")", ":", "\n", "                            ", "new_partial_seqs", ".", "append", "(", "\n", "partial_seqs", "[", "int", "(", "ptrs", "[", "b", "]", "[", "k", "]", ")", "+", "b", "*", "K", "]", "+", "[", "wids", "[", "b", "]", "[", "k", "]", "]", ")", "\n", "", "", "partial_seqs", "=", "new_partial_seqs", "\n", "\n", "", "def", "get_dup_ngram_candidates", "(", "seq", ",", "n", ")", ":", "\n", "                    ", "cands", "=", "set", "(", ")", "\n", "if", "len", "(", "seq", ")", "<", "n", ":", "\n", "                        ", "return", "[", "]", "\n", "", "tail", "=", "seq", "[", "-", "(", "n", "-", "1", ")", ":", "]", "\n", "if", "self", ".", "forbid_ignore_set", "and", "any", "(", "tk", "in", "self", ".", "forbid_ignore_set", "for", "tk", "in", "tail", ")", ":", "\n", "                        ", "return", "[", "]", "\n", "", "for", "i", "in", "range", "(", "len", "(", "seq", ")", "-", "(", "n", "-", "1", ")", ")", ":", "\n", "                        ", "mismatch", "=", "False", "\n", "for", "j", "in", "range", "(", "n", "-", "1", ")", ":", "\n", "                            ", "if", "tail", "[", "j", "]", "!=", "seq", "[", "i", "+", "j", "]", ":", "\n", "                                ", "mismatch", "=", "True", "\n", "break", "\n", "", "", "if", "(", "not", "mismatch", ")", "and", "not", "(", "self", ".", "forbid_ignore_set", "and", "(", "seq", "[", "i", "+", "n", "-", "1", "]", "in", "self", ".", "forbid_ignore_set", ")", ")", ":", "\n", "                            ", "cands", ".", "add", "(", "seq", "[", "i", "+", "n", "-", "1", "]", ")", "\n", "", "", "return", "list", "(", "sorted", "(", "cands", ")", ")", "\n", "\n", "", "if", "len", "(", "partial_seqs", "[", "0", "]", ")", ">=", "self", ".", "ngram_size", ":", "\n", "                    ", "dup_cands", "=", "[", "]", "\n", "for", "seq", "in", "partial_seqs", ":", "\n", "                        ", "dup_cands", ".", "append", "(", "\n", "get_dup_ngram_candidates", "(", "seq", ",", "self", ".", "ngram_size", ")", ")", "\n", "", "if", "max", "(", "len", "(", "x", ")", "for", "x", "in", "dup_cands", ")", ">", "0", ":", "\n", "                        ", "if", "buf_matrix", "is", "None", ":", "\n", "                            ", "vocab_size", "=", "list", "(", "log_scores", ".", "size", "(", ")", ")", "[", "-", "1", "]", "\n", "buf_matrix", "=", "np", ".", "zeros", "(", "\n", "(", "batch_size", "*", "K", ",", "vocab_size", ")", ",", "dtype", "=", "float", ")", "\n", "", "else", ":", "\n", "                            ", "buf_matrix", ".", "fill", "(", "0", ")", "\n", "", "for", "bk", ",", "cands", "in", "enumerate", "(", "dup_cands", ")", ":", "\n", "                            ", "for", "i", ",", "wid", "in", "enumerate", "(", "cands", ")", ":", "\n", "                                ", "buf_matrix", "[", "bk", ",", "wid", "]", "=", "1.0", "\n", "", "", "forbid_word_mask", "=", "torch", ".", "tensor", "(", "\n", "buf_matrix", ",", "dtype", "=", "log_scores", ".", "dtype", ")", "\n", "forbid_word_mask", "=", "torch", ".", "reshape", "(", "\n", "forbid_word_mask", ",", "[", "batch_size", "*", "K", ",", "1", ",", "vocab_size", "]", ")", ".", "cuda", "(", ")", "\n", "", "else", ":", "\n", "                        ", "forbid_word_mask", "=", "None", "\n", "", "", "", "next_pos", "+=", "1", "\n", "\n", "# [(batch, beam)]", "\n", "", "total_scores", "=", "[", "x", ".", "tolist", "(", ")", "for", "x", "in", "total_scores", "]", "\n", "step_ids", "=", "[", "x", ".", "tolist", "(", ")", "for", "x", "in", "step_ids", "]", "\n", "step_back_ptrs", "=", "[", "x", ".", "tolist", "(", ")", "for", "x", "in", "step_back_ptrs", "]", "\n", "# back tracking", "\n", "traces", "=", "{", "'pred_seq'", ":", "[", "]", ",", "'scores'", ":", "[", "]", ",", "'wids'", ":", "[", "]", ",", "'ptrs'", ":", "[", "]", "}", "\n", "for", "b", "in", "range", "(", "batch_size", ")", ":", "\n", "# [(beam,)]", "\n", "            ", "scores", "=", "[", "x", "[", "b", "]", "for", "x", "in", "total_scores", "]", "\n", "wids_list", "=", "[", "x", "[", "b", "]", "for", "x", "in", "step_ids", "]", "\n", "ptrs", "=", "[", "x", "[", "b", "]", "for", "x", "in", "step_back_ptrs", "]", "\n", "traces", "[", "'scores'", "]", ".", "append", "(", "scores", ")", "\n", "traces", "[", "'wids'", "]", ".", "append", "(", "wids_list", ")", "\n", "traces", "[", "'ptrs'", "]", ".", "append", "(", "ptrs", ")", "\n", "# first we need to find the eos frame where all symbols are eos", "\n", "# any frames after the eos frame are invalid", "\n", "last_frame_id", "=", "len", "(", "scores", ")", "-", "1", "\n", "for", "i", ",", "wids", "in", "enumerate", "(", "wids_list", ")", ":", "\n", "                ", "if", "all", "(", "wid", "==", "self", ".", "eos_id", "for", "wid", "in", "wids", ")", ":", "\n", "                    ", "last_frame_id", "=", "i", "\n", "break", "\n", "", "", "max_score", "=", "-", "math", ".", "inf", "\n", "frame_id", "=", "-", "1", "\n", "pos_in_frame", "=", "-", "1", "\n", "\n", "for", "fid", "in", "range", "(", "last_frame_id", "+", "1", ")", ":", "\n", "                ", "for", "i", ",", "wid", "in", "enumerate", "(", "wids_list", "[", "fid", "]", ")", ":", "\n", "                    ", "if", "wid", "==", "self", ".", "eos_id", "or", "fid", "==", "last_frame_id", ":", "\n", "                        ", "s", "=", "scores", "[", "fid", "]", "[", "i", "]", "\n", "if", "self", ".", "length_penalty", ">", "0", ":", "\n", "                            ", "s", "/=", "math", ".", "pow", "(", "(", "5", "+", "fid", "+", "1", ")", "/", "6.0", ",", "\n", "self", ".", "length_penalty", ")", "\n", "", "if", "s", ">", "max_score", ":", "\n", "                            ", "max_score", "=", "s", "\n", "frame_id", "=", "fid", "\n", "pos_in_frame", "=", "i", "\n", "", "", "", "", "if", "frame_id", "==", "-", "1", ":", "\n", "                ", "traces", "[", "'pred_seq'", "]", ".", "append", "(", "[", "0", "]", ")", "\n", "", "else", ":", "\n", "                ", "seq", "=", "[", "wids_list", "[", "frame_id", "]", "[", "pos_in_frame", "]", "]", "\n", "for", "fid", "in", "range", "(", "frame_id", ",", "0", ",", "-", "1", ")", ":", "\n", "                    ", "pos_in_frame", "=", "ptrs", "[", "fid", "]", "[", "int", "(", "pos_in_frame", ")", "]", "\n", "seq", ".", "append", "(", "wids_list", "[", "fid", "-", "1", "]", "[", "int", "(", "pos_in_frame", ")", "]", ")", "\n", "", "seq", ".", "reverse", "(", ")", "\n", "traces", "[", "'pred_seq'", "]", ".", "append", "(", "seq", ")", "\n", "\n", "", "", "def", "_pad_sequence", "(", "sequences", ",", "max_len", ",", "padding_value", "=", "0", ")", ":", "\n", "            ", "trailing_dims", "=", "sequences", "[", "0", "]", ".", "size", "(", ")", "[", "1", ":", "]", "\n", "out_dims", "=", "(", "len", "(", "sequences", ")", ",", "max_len", ")", "+", "trailing_dims", "\n", "\n", "out_tensor", "=", "sequences", "[", "0", "]", ".", "data", ".", "new", "(", "*", "out_dims", ")", ".", "fill_", "(", "padding_value", ")", "\n", "for", "i", ",", "tensor", "in", "enumerate", "(", "sequences", ")", ":", "\n", "                ", "length", "=", "tensor", ".", "size", "(", "0", ")", "\n", "# use index notation to prevent duplicate references to the tensor", "\n", "out_tensor", "[", "i", ",", ":", "length", ",", "...", "]", "=", "tensor", "\n", "", "return", "out_tensor", "\n", "\n", "# convert to tensors for DataParallel", "\n", "", "for", "k", "in", "(", "'pred_seq'", ",", "'scores'", ",", "'wids'", ",", "'ptrs'", ")", ":", "\n", "            ", "ts_list", "=", "traces", "[", "k", "]", "\n", "if", "not", "isinstance", "(", "ts_list", "[", "0", "]", ",", "torch", ".", "Tensor", ")", ":", "\n", "                ", "dt", "=", "torch", ".", "float", "if", "k", "==", "'scores'", "else", "torch", ".", "long", "\n", "ts_list", "=", "[", "torch", ".", "tensor", "(", "it", ",", "dtype", "=", "dt", ")", "for", "it", "in", "ts_list", "]", "\n", "", "traces", "[", "k", "]", "=", "_pad_sequence", "(", "\n", "ts_list", ",", "output_length", ",", "padding_value", "=", "0", ")", ".", "to", "(", "input_ids", ".", "device", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.pytorch_pretrained_bert.modeling.BertForMaskedLM.__init__": [[1817, 1823], ["modeling.PreTrainedBertModel.__init__", "modeling.BertModel", "modeling.BertOnlyMLMHead", "modeling.BertForMaskedLM.apply"], "methods", ["home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.unilm.eval.Eval.__init__"], ["\n", "\n", "def", "__init__", "(", "self", ",", "config", ")", ":", "\n", "        ", "super", "(", "BertForMaskedLM", ",", "self", ")", ".", "__init__", "(", "config", ")", "\n", "self", ".", "bert", "=", "BertModel", "(", "config", ")", "\n", "self", ".", "cls", "=", "BertOnlyMLMHead", "(", "\n", "config", ",", "self", ".", "bert", ".", "embeddings", ".", "word_embeddings", ".", "weight", ")", "\n"]], "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.pytorch_pretrained_bert.modeling.BertForMaskedLM.forward": [[1824, 1836], ["modeling.BertForMaskedLM.bert", "modeling.BertForMaskedLM.cls", "torch.nn.CrossEntropyLoss", "torch.nn.CrossEntropyLoss", "torch.nn.CrossEntropyLoss.", "torch.nn.CrossEntropyLoss.", "modeling.BertForMaskedLM.view", "masked_lm_labels.view"], "methods", ["None"], ["self", ".", "apply", "(", "self", ".", "init_bert_weights", ")", "\n", "\n", "", "def", "forward", "(", "self", ",", "input_ids", ",", "token_type_ids", "=", "None", ",", "attention_mask", "=", "None", ",", "masked_lm_labels", "=", "None", ",", "mask_qkv", "=", "None", ",", "task_idx", "=", "None", ")", ":", "\n", "        ", "sequence_output", ",", "_", "=", "self", ".", "bert", "(", "input_ids", ",", "token_type_ids", ",", "attention_mask", ",", "\n", "output_all_encoded_layers", "=", "False", ",", "mask_qkv", "=", "mask_qkv", ",", "task_idx", "=", "task_idx", ")", "\n", "prediction_scores", "=", "self", ".", "cls", "(", "sequence_output", ")", "\n", "\n", "if", "masked_lm_labels", "is", "not", "None", ":", "\n", "            ", "loss_fct", "=", "CrossEntropyLoss", "(", "ignore_index", "=", "-", "1", ")", "\n", "masked_lm_loss", "=", "loss_fct", "(", "\n", "prediction_scores", ".", "view", "(", "-", "1", ",", "self", ".", "config", ".", "vocab_size", ")", ",", "masked_lm_labels", ".", "view", "(", "-", "1", ")", ")", "\n", "return", "masked_lm_loss", "\n", "", "else", ":", "\n"]], "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.pytorch_pretrained_bert.modeling.BertForNextSentencePrediction.__init__": [[1882, 1887], ["modeling.PreTrainedBertModel.__init__", "modeling.BertModel", "modeling.BertOnlyNSPHead", "modeling.BertForNextSentencePrediction.apply"], "methods", ["home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.unilm.eval.Eval.__init__"], ["\n", "\n", "def", "__init__", "(", "self", ",", "config", ")", ":", "\n", "        ", "super", "(", "BertForNextSentencePrediction", ",", "self", ")", ".", "__init__", "(", "config", ")", "\n", "self", ".", "bert", "=", "BertModel", "(", "config", ")", "\n", "self", ".", "cls", "=", "BertOnlyNSPHead", "(", "config", ")", "\n"]], "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.pytorch_pretrained_bert.modeling.BertForNextSentencePrediction.forward": [[1888, 1900], ["modeling.BertForNextSentencePrediction.bert", "modeling.BertForNextSentencePrediction.cls", "torch.nn.CrossEntropyLoss", "torch.nn.CrossEntropyLoss", "torch.nn.CrossEntropyLoss.", "torch.nn.CrossEntropyLoss.", "modeling.BertForNextSentencePrediction.view", "next_sentence_label.view"], "methods", ["None"], ["self", ".", "apply", "(", "self", ".", "init_bert_weights", ")", "\n", "\n", "", "def", "forward", "(", "self", ",", "input_ids", ",", "token_type_ids", "=", "None", ",", "attention_mask", "=", "None", ",", "next_sentence_label", "=", "None", ",", "mask_qkv", "=", "None", ",", "task_idx", "=", "None", ")", ":", "\n", "        ", "_", ",", "pooled_output", "=", "self", ".", "bert", "(", "input_ids", ",", "token_type_ids", ",", "attention_mask", ",", "\n", "output_all_encoded_layers", "=", "False", ",", "mask_qkv", "=", "mask_qkv", ",", "task_idx", "=", "task_idx", ")", "\n", "seq_relationship_score", "=", "self", ".", "cls", "(", "pooled_output", ")", "\n", "\n", "if", "next_sentence_label", "is", "not", "None", ":", "\n", "            ", "loss_fct", "=", "CrossEntropyLoss", "(", "ignore_index", "=", "-", "1", ")", "\n", "next_sentence_loss", "=", "loss_fct", "(", "\n", "seq_relationship_score", ".", "view", "(", "-", "1", ",", "2", ")", ",", "next_sentence_label", ".", "view", "(", "-", "1", ")", ")", "\n", "return", "next_sentence_loss", "\n", "", "else", ":", "\n"]], "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.pytorch_pretrained_bert.modeling.BertForSequenceClassification.__init__": [[1948, 1955], ["modeling.PreTrainedBertModel.__init__", "modeling.BertModel", "torch.nn.Dropout", "torch.nn.Dropout", "torch.nn.Linear", "torch.nn.Linear", "modeling.BertForSequenceClassification.apply"], "methods", ["home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.unilm.eval.Eval.__init__"], ["\n", "\n", "def", "__init__", "(", "self", ",", "config", ",", "num_labels", "=", "2", ")", ":", "\n", "        ", "super", "(", "BertForSequenceClassification", ",", "self", ")", ".", "__init__", "(", "config", ")", "\n", "self", ".", "num_labels", "=", "num_labels", "\n", "self", ".", "bert", "=", "BertModel", "(", "config", ")", "\n", "self", ".", "dropout", "=", "nn", ".", "Dropout", "(", "config", ".", "hidden_dropout_prob", ")", "\n", "self", ".", "classifier", "=", "nn", ".", "Linear", "(", "config", ".", "hidden_size", ",", "num_labels", ")", "\n"]], "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.pytorch_pretrained_bert.modeling.BertForSequenceClassification.forward": [[1956, 1976], ["modeling.BertForSequenceClassification.bert", "modeling.BertForSequenceClassification.dropout", "modeling.BertForSequenceClassification.classifier", "torch.nn.CrossEntropyLoss", "torch.nn.CrossEntropyLoss", "torch.nn.MSELoss.", "torch.nn.MSELoss.", "modeling.BertForSequenceClassification.view", "labels.view", "torch.nn.MSELoss", "torch.nn.MSELoss", "torch.nn.MSELoss.", "torch.nn.MSELoss.", "print", "modeling.BertForSequenceClassification.view", "labels.view"], "methods", ["None"], ["self", ".", "apply", "(", "self", ".", "init_bert_weights", ")", "\n", "\n", "", "def", "forward", "(", "self", ",", "input_ids", ",", "token_type_ids", "=", "None", ",", "attention_mask", "=", "None", ",", "labels", "=", "None", ",", "mask_qkv", "=", "None", ",", "task_idx", "=", "None", ")", ":", "\n", "        ", "_", ",", "pooled_output", "=", "self", ".", "bert", "(", "\n", "input_ids", ",", "token_type_ids", ",", "attention_mask", ",", "output_all_encoded_layers", "=", "False", ",", "mask_qkv", "=", "mask_qkv", ",", "task_idx", "=", "task_idx", ")", "\n", "pooled_output", "=", "self", ".", "dropout", "(", "pooled_output", ")", "\n", "logits", "=", "self", ".", "classifier", "(", "pooled_output", ")", "\n", "\n", "if", "labels", "is", "not", "None", ":", "\n", "            ", "if", "labels", ".", "dtype", "==", "torch", ".", "long", ":", "\n", "                ", "loss_fct", "=", "CrossEntropyLoss", "(", ")", "\n", "loss", "=", "loss_fct", "(", "\n", "logits", ".", "view", "(", "-", "1", ",", "self", ".", "num_labels", ")", ",", "labels", ".", "view", "(", "-", "1", ")", ")", "\n", "", "elif", "labels", ".", "dtype", "==", "torch", ".", "half", "or", "labels", ".", "dtype", "==", "torch", ".", "float", ":", "\n", "                ", "loss_fct", "=", "MSELoss", "(", ")", "\n", "loss", "=", "loss_fct", "(", "logits", ".", "view", "(", "-", "1", ")", ",", "labels", ".", "view", "(", "-", "1", ")", ")", "\n", "", "else", ":", "\n", "                ", "print", "(", "'unkown labels.dtype'", ")", "\n", "loss", "=", "None", "\n", "", "return", "loss", "\n", "", "else", ":", "\n"]], "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.pytorch_pretrained_bert.modeling.BertForMultipleChoice.__init__": [[2023, 2030], ["modeling.PreTrainedBertModel.__init__", "modeling.BertModel", "torch.nn.Dropout", "torch.nn.Dropout", "torch.nn.Linear", "torch.nn.Linear", "modeling.BertForMultipleChoice.apply"], "methods", ["home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.unilm.eval.Eval.__init__"], ["\n", "\n", "def", "__init__", "(", "self", ",", "config", ",", "num_choices", "=", "2", ")", ":", "\n", "        ", "super", "(", "BertForMultipleChoice", ",", "self", ")", ".", "__init__", "(", "config", ")", "\n", "self", ".", "num_choices", "=", "num_choices", "\n", "self", ".", "bert", "=", "BertModel", "(", "config", ")", "\n", "self", ".", "dropout", "=", "nn", ".", "Dropout", "(", "config", ".", "hidden_dropout_prob", ")", "\n", "self", ".", "classifier", "=", "nn", ".", "Linear", "(", "config", ".", "hidden_size", ",", "1", ")", "\n"]], "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.pytorch_pretrained_bert.modeling.BertForMultipleChoice.forward": [[2031, 2047], ["input_ids.view", "token_type_ids.view", "attention_mask.view", "modeling.BertForMultipleChoice.bert", "modeling.BertForMultipleChoice.dropout", "modeling.BertForMultipleChoice.classifier", "modeling.BertForMultipleChoice.view", "input_ids.size", "token_type_ids.size", "attention_mask.size", "torch.nn.CrossEntropyLoss", "torch.nn.CrossEntropyLoss", "torch.nn.CrossEntropyLoss.", "torch.nn.CrossEntropyLoss."], "methods", ["None"], ["self", ".", "apply", "(", "self", ".", "init_bert_weights", ")", "\n", "\n", "", "def", "forward", "(", "self", ",", "input_ids", ",", "token_type_ids", "=", "None", ",", "attention_mask", "=", "None", ",", "labels", "=", "None", ",", "mask_qkv", "=", "None", ",", "task_idx", "=", "None", ")", ":", "\n", "        ", "flat_input_ids", "=", "input_ids", ".", "view", "(", "-", "1", ",", "input_ids", ".", "size", "(", "-", "1", ")", ")", "\n", "flat_token_type_ids", "=", "token_type_ids", ".", "view", "(", "-", "1", ",", "token_type_ids", ".", "size", "(", "-", "1", ")", ")", "\n", "flat_attention_mask", "=", "attention_mask", ".", "view", "(", "-", "1", ",", "attention_mask", ".", "size", "(", "-", "1", ")", ")", "\n", "_", ",", "pooled_output", "=", "self", ".", "bert", "(", "\n", "flat_input_ids", ",", "flat_token_type_ids", ",", "flat_attention_mask", ",", "output_all_encoded_layers", "=", "False", ",", "mask_qkv", "=", "mask_qkv", ",", "task_idx", "=", "task_idx", ")", "\n", "pooled_output", "=", "self", ".", "dropout", "(", "pooled_output", ")", "\n", "logits", "=", "self", ".", "classifier", "(", "pooled_output", ")", "\n", "reshaped_logits", "=", "logits", ".", "view", "(", "-", "1", ",", "self", ".", "num_choices", ")", "\n", "\n", "if", "labels", "is", "not", "None", ":", "\n", "            ", "loss_fct", "=", "CrossEntropyLoss", "(", ")", "\n", "loss", "=", "loss_fct", "(", "reshaped_logits", ",", "labels", ")", "\n", "return", "loss", "\n", "", "else", ":", "\n"]], "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.pytorch_pretrained_bert.modeling.BertForTokenClassification.__init__": [[2095, 2102], ["modeling.PreTrainedBertModel.__init__", "modeling.BertModel", "torch.nn.Dropout", "torch.nn.Dropout", "torch.nn.Linear", "torch.nn.Linear", "modeling.BertForTokenClassification.apply"], "methods", ["home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.unilm.eval.Eval.__init__"], ["\n", "\n", "def", "__init__", "(", "self", ",", "config", ",", "num_labels", "=", "2", ")", ":", "\n", "        ", "super", "(", "BertForTokenClassification", ",", "self", ")", ".", "__init__", "(", "config", ")", "\n", "self", ".", "num_labels", "=", "num_labels", "\n", "self", ".", "bert", "=", "BertModel", "(", "config", ")", "\n", "self", ".", "dropout", "=", "nn", ".", "Dropout", "(", "config", ".", "hidden_dropout_prob", ")", "\n", "self", ".", "classifier", "=", "nn", ".", "Linear", "(", "config", ".", "hidden_size", ",", "num_labels", ")", "\n"]], "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.pytorch_pretrained_bert.modeling.BertForTokenClassification.forward": [[2103, 2123], ["modeling.BertForTokenClassification.bert", "modeling.BertForTokenClassification.dropout", "modeling.BertForTokenClassification.classifier", "torch.nn.CrossEntropyLoss", "torch.nn.CrossEntropyLoss", "torch.nn.CrossEntropyLoss.", "torch.nn.CrossEntropyLoss.", "torch.nn.CrossEntropyLoss.", "torch.nn.CrossEntropyLoss.", "attention_mask.view", "modeling.BertForTokenClassification.view", "labels.view", "modeling.BertForTokenClassification.view", "labels.view"], "methods", ["None"], ["self", ".", "apply", "(", "self", ".", "init_bert_weights", ")", "\n", "\n", "", "def", "forward", "(", "self", ",", "input_ids", ",", "token_type_ids", "=", "None", ",", "attention_mask", "=", "None", ",", "labels", "=", "None", ",", "mask_qkv", "=", "None", ",", "task_idx", "=", "None", ")", ":", "\n", "        ", "sequence_output", ",", "_", "=", "self", ".", "bert", "(", "\n", "input_ids", ",", "token_type_ids", ",", "attention_mask", ",", "output_all_encoded_layers", "=", "False", ",", "mask_qkv", "=", "mask_qkv", ",", "task_idx", "=", "task_idx", ")", "\n", "sequence_output", "=", "self", ".", "dropout", "(", "sequence_output", ")", "\n", "logits", "=", "self", ".", "classifier", "(", "sequence_output", ")", "\n", "\n", "if", "labels", "is", "not", "None", ":", "\n", "            ", "loss_fct", "=", "CrossEntropyLoss", "(", ")", "\n", "# Only keep active parts of the loss", "\n", "if", "attention_mask", "is", "not", "None", ":", "\n", "                ", "active_loss", "=", "attention_mask", ".", "view", "(", "-", "1", ")", "==", "1", "\n", "active_logits", "=", "logits", ".", "view", "(", "-", "1", ",", "self", ".", "num_labels", ")", "[", "active_loss", "]", "\n", "active_labels", "=", "labels", ".", "view", "(", "-", "1", ")", "[", "active_loss", "]", "\n", "loss", "=", "loss_fct", "(", "active_logits", ",", "active_labels", ")", "\n", "", "else", ":", "\n", "                ", "loss", "=", "loss_fct", "(", "\n", "logits", ".", "view", "(", "-", "1", ",", "self", ".", "num_labels", ")", ",", "labels", ".", "view", "(", "-", "1", ")", ")", "\n", "", "return", "loss", "\n", "", "else", ":", "\n"]], "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.pytorch_pretrained_bert.modeling.BertForQuestionAnswering.__init__": [[2181, 2187], ["modeling.PreTrainedBertModel.__init__", "modeling.BertModel", "torch.nn.Linear", "torch.nn.Linear", "modeling.BertForQuestionAnswering.apply"], "methods", ["home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.unilm.eval.Eval.__init__"], ["\n", "\n", "def", "__init__", "(", "self", ",", "config", ")", ":", "\n", "        ", "super", "(", "BertForQuestionAnswering", ",", "self", ")", ".", "__init__", "(", "config", ")", "\n", "self", ".", "bert", "=", "BertModel", "(", "config", ")", "\n", "# self.dropout = nn.Dropout(config.hidden_dropout_prob)", "\n", "self", ".", "qa_outputs", "=", "nn", ".", "Linear", "(", "config", ".", "hidden_size", ",", "2", ")", "\n"]], "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.pytorch_pretrained_bert.modeling.BertForQuestionAnswering.forward": [[2188, 2214], ["modeling.BertForQuestionAnswering.bert", "modeling.BertForQuestionAnswering.qa_outputs", "modeling.BertForQuestionAnswering.split", "start_logits.squeeze.squeeze.squeeze", "end_logits.squeeze.squeeze.squeeze", "start_logits.squeeze.squeeze.size", "start_positions.squeeze.squeeze.clamp_", "end_positions.squeeze.squeeze.clamp_", "torch.nn.CrossEntropyLoss", "torch.nn.CrossEntropyLoss", "torch.nn.CrossEntropyLoss.", "torch.nn.CrossEntropyLoss.", "torch.nn.CrossEntropyLoss.", "torch.nn.CrossEntropyLoss.", "len", "start_positions.squeeze.squeeze.squeeze", "len", "end_positions.squeeze.squeeze.squeeze", "start_positions.squeeze.squeeze.size", "end_positions.squeeze.squeeze.size"], "methods", ["home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.preprocess.process_daren.split"], ["self", ".", "apply", "(", "self", ".", "init_bert_weights", ")", "\n", "\n", "", "def", "forward", "(", "self", ",", "input_ids", ",", "token_type_ids", "=", "None", ",", "attention_mask", "=", "None", ",", "start_positions", "=", "None", ",", "end_positions", "=", "None", ",", "task_idx", "=", "None", ")", ":", "\n", "        ", "sequence_output", ",", "_", "=", "self", ".", "bert", "(", "\n", "input_ids", ",", "token_type_ids", ",", "attention_mask", ",", "output_all_encoded_layers", "=", "False", ",", "task_idx", "=", "task_idx", ")", "\n", "logits", "=", "self", ".", "qa_outputs", "(", "sequence_output", ")", "\n", "start_logits", ",", "end_logits", "=", "logits", ".", "split", "(", "1", ",", "dim", "=", "-", "1", ")", "\n", "start_logits", "=", "start_logits", ".", "squeeze", "(", "-", "1", ")", "\n", "end_logits", "=", "end_logits", ".", "squeeze", "(", "-", "1", ")", "\n", "\n", "if", "start_positions", "is", "not", "None", "and", "end_positions", "is", "not", "None", ":", "\n", "# If we are on multi-GPU, split add a dimension", "\n", "            ", "if", "len", "(", "start_positions", ".", "size", "(", ")", ")", ">", "1", ":", "\n", "                ", "start_positions", "=", "start_positions", ".", "squeeze", "(", "-", "1", ")", "\n", "", "if", "len", "(", "end_positions", ".", "size", "(", ")", ")", ">", "1", ":", "\n", "                ", "end_positions", "=", "end_positions", ".", "squeeze", "(", "-", "1", ")", "\n", "# sometimes the start/end positions are outside our model inputs, we ignore these terms", "\n", "", "ignored_index", "=", "start_logits", ".", "size", "(", "1", ")", "\n", "start_positions", ".", "clamp_", "(", "0", ",", "ignored_index", ")", "\n", "end_positions", ".", "clamp_", "(", "0", ",", "ignored_index", ")", "\n", "\n", "loss_fct", "=", "CrossEntropyLoss", "(", "ignore_index", "=", "ignored_index", ")", "\n", "start_loss", "=", "loss_fct", "(", "start_logits", ",", "start_positions", ")", "\n", "end_loss", "=", "loss_fct", "(", "end_logits", ",", "end_positions", ")", "\n", "total_loss", "=", "(", "start_loss", "+", "end_loss", ")", "/", "2", "\n", "return", "total_loss", "\n", "", "else", ":", "\n"]], "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.pytorch_pretrained_bert.modeling.gelu": [[42, 48], ["torch.erf", "torch.erf", "math.sqrt"], "function", ["None"], ["def", "gelu", "(", "x", ")", ":", "\n", "    ", "\"\"\"Implementation of the gelu activation function.\n        For information: OpenAI GPT's gelu is slightly different (and gives slightly different results):\n        0.5 * x * (1 + torch.tanh(math.sqrt(2 / math.pi) * (x + 0.044715 * torch.pow(x, 3))))\n    \"\"\"", "\n", "return", "x", "*", "0.5", "*", "(", "1.0", "+", "torch", ".", "erf", "(", "x", "/", "math", ".", "sqrt", "(", "2.0", ")", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.pytorch_pretrained_bert.modeling.swish": [[50, 52], ["torch.sigmoid", "torch.sigmoid"], "function", ["None"], ["", "def", "swish", "(", "x", ")", ":", "\n", "    ", "return", "x", "*", "torch", ".", "sigmoid", "(", "x", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.pytorch_pretrained_bert.optimization.BertAdam.__init__": [[69, 91], ["dict", "torch.optim.Optimizer.__init__", "ValueError", "ValueError", "ValueError", "ValueError", "ValueError", "ValueError"], "methods", ["home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.unilm.eval.Eval.__init__"], ["def", "__init__", "(", "self", ",", "params", ",", "lr", "=", "required", ",", "warmup", "=", "-", "1", ",", "t_total", "=", "-", "1", ",", "schedule", "=", "'warmup_linear'", ",", "b1", "=", "0.9", ",", "b2", "=", "0.999", ",", "e", "=", "1e-6", ",", "weight_decay", "=", "0.01", ",", "max_grad_norm", "=", "1.0", ")", ":", "\n", "        ", "if", "lr", "is", "not", "required", "and", "lr", "<", "0.0", ":", "\n", "            ", "raise", "ValueError", "(", "\n", "\"Invalid learning rate: {} - should be >= 0.0\"", ".", "format", "(", "lr", ")", ")", "\n", "", "if", "schedule", "not", "in", "SCHEDULES", ":", "\n", "            ", "raise", "ValueError", "(", "\"Invalid schedule parameter: {}\"", ".", "format", "(", "schedule", ")", ")", "\n", "", "if", "not", "0.0", "<=", "warmup", "<", "1.0", "and", "not", "warmup", "==", "-", "1", ":", "\n", "            ", "raise", "ValueError", "(", "\n", "\"Invalid warmup: {} - should be in [0.0, 1.0[ or -1\"", ".", "format", "(", "warmup", ")", ")", "\n", "", "if", "not", "0.0", "<=", "b1", "<", "1.0", ":", "\n", "            ", "raise", "ValueError", "(", "\n", "\"Invalid b1 parameter: {} - should be in [0.0, 1.0[\"", ".", "format", "(", "b1", ")", ")", "\n", "", "if", "not", "0.0", "<=", "b2", "<", "1.0", ":", "\n", "            ", "raise", "ValueError", "(", "\n", "\"Invalid b2 parameter: {} - should be in [0.0, 1.0[\"", ".", "format", "(", "b2", ")", ")", "\n", "", "if", "not", "e", ">=", "0.0", ":", "\n", "            ", "raise", "ValueError", "(", "\n", "\"Invalid epsilon value: {} - should be >= 0.0\"", ".", "format", "(", "e", ")", ")", "\n", "", "defaults", "=", "dict", "(", "lr", "=", "lr", ",", "schedule", "=", "schedule", ",", "warmup", "=", "warmup", ",", "t_total", "=", "t_total", ",", "\n", "b1", "=", "b1", ",", "b2", "=", "b2", ",", "e", "=", "e", ",", "weight_decay", "=", "weight_decay", ",", "\n", "max_grad_norm", "=", "max_grad_norm", ")", "\n", "super", "(", "BertAdam", ",", "self", ")", ".", "__init__", "(", "params", ",", "defaults", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.pytorch_pretrained_bert.optimization.BertAdam.get_lr": [[92, 107], ["lr.append", "len", "schedule_fct"], "methods", ["None"], ["", "def", "get_lr", "(", "self", ")", ":", "\n", "        ", "lr", "=", "[", "]", "\n", "for", "group", "in", "self", ".", "param_groups", ":", "\n", "            ", "for", "p", "in", "group", "[", "'params'", "]", ":", "\n", "                ", "state", "=", "self", ".", "state", "[", "p", "]", "\n", "if", "len", "(", "state", ")", "==", "0", ":", "\n", "                    ", "return", "[", "0", "]", "\n", "", "if", "group", "[", "'t_total'", "]", "!=", "-", "1", ":", "\n", "                    ", "schedule_fct", "=", "SCHEDULES", "[", "group", "[", "'schedule'", "]", "]", "\n", "lr_scheduled", "=", "group", "[", "'lr'", "]", "*", "schedule_fct", "(", "\n", "state", "[", "'step'", "]", "/", "group", "[", "'t_total'", "]", ",", "group", "[", "'warmup'", "]", ")", "\n", "", "else", ":", "\n", "                    ", "lr_scheduled", "=", "group", "[", "'lr'", "]", "\n", "", "lr", ".", "append", "(", "lr_scheduled", ")", "\n", "", "", "return", "lr", "\n", "\n"]], "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.pytorch_pretrained_bert.optimization.BertAdam.step": [[108, 179], ["closure", "next_m.mul_().add_", "next_v.mul_().addcmul_", "p.data.add_", "RuntimeError", "len", "torch.zeros_like", "torch.zeros_like", "torch.nn.utils.clip_grad_norm_", "next_m.mul_", "next_v.mul_", "next_v.sqrt", "schedule_fct"], "methods", ["None"], ["", "def", "step", "(", "self", ",", "closure", "=", "None", ")", ":", "\n", "        ", "\"\"\"Performs a single optimization step.\n\n        Arguments:\n            closure (callable, optional): A closure that reevaluates the model\n                and returns the loss.\n        \"\"\"", "\n", "loss", "=", "None", "\n", "if", "closure", "is", "not", "None", ":", "\n", "            ", "loss", "=", "closure", "(", ")", "\n", "\n", "", "for", "group", "in", "self", ".", "param_groups", ":", "\n", "            ", "for", "p", "in", "group", "[", "'params'", "]", ":", "\n", "                ", "if", "p", ".", "grad", "is", "None", ":", "\n", "                    ", "continue", "\n", "", "grad", "=", "p", ".", "grad", ".", "data", "\n", "if", "grad", ".", "is_sparse", ":", "\n", "                    ", "raise", "RuntimeError", "(", "\n", "'Adam does not support sparse gradients, please consider SparseAdam instead'", ")", "\n", "\n", "", "state", "=", "self", ".", "state", "[", "p", "]", "\n", "\n", "# State initialization", "\n", "if", "len", "(", "state", ")", "==", "0", ":", "\n", "                    ", "state", "[", "'step'", "]", "=", "0", "\n", "# Exponential moving average of gradient values", "\n", "state", "[", "'next_m'", "]", "=", "torch", ".", "zeros_like", "(", "p", ".", "data", ")", "\n", "# Exponential moving average of squared gradient values", "\n", "state", "[", "'next_v'", "]", "=", "torch", ".", "zeros_like", "(", "p", ".", "data", ")", "\n", "\n", "", "next_m", ",", "next_v", "=", "state", "[", "'next_m'", "]", ",", "state", "[", "'next_v'", "]", "\n", "beta1", ",", "beta2", "=", "group", "[", "'b1'", "]", ",", "group", "[", "'b2'", "]", "\n", "\n", "# Add grad clipping", "\n", "if", "group", "[", "'max_grad_norm'", "]", ">", "0", ":", "\n", "                    ", "clip_grad_norm_", "(", "p", ",", "group", "[", "'max_grad_norm'", "]", ")", "\n", "\n", "# Decay the first and second moment running average coefficient", "\n", "# In-place operations to update the averages at the same time", "\n", "", "next_m", ".", "mul_", "(", "beta1", ")", ".", "add_", "(", "1", "-", "beta1", ",", "grad", ")", "\n", "next_v", ".", "mul_", "(", "beta2", ")", ".", "addcmul_", "(", "1", "-", "beta2", ",", "grad", ",", "grad", ")", "\n", "update", "=", "next_m", "/", "(", "next_v", ".", "sqrt", "(", ")", "+", "group", "[", "'e'", "]", ")", "\n", "\n", "# Just adding the square of the weights to the loss function is *not*", "\n", "# the correct way of using L2 regularization/weight decay with Adam,", "\n", "# since that will interact with the m and v parameters in strange ways.", "\n", "#", "\n", "# Instead we want to decay the weights in a manner that doesn't interact", "\n", "# with the m/v parameters. This is equivalent to adding the square", "\n", "# of the weights to the loss with plain (non-momentum) SGD.", "\n", "if", "group", "[", "'weight_decay'", "]", ">", "0.0", ":", "\n", "                    ", "update", "+=", "group", "[", "'weight_decay'", "]", "*", "p", ".", "data", "\n", "\n", "", "if", "group", "[", "'t_total'", "]", "!=", "-", "1", ":", "\n", "                    ", "schedule_fct", "=", "SCHEDULES", "[", "group", "[", "'schedule'", "]", "]", "\n", "lr_scheduled", "=", "group", "[", "'lr'", "]", "*", "schedule_fct", "(", "\n", "state", "[", "'step'", "]", "/", "group", "[", "'t_total'", "]", ",", "group", "[", "'warmup'", "]", ")", "\n", "", "else", ":", "\n", "                    ", "lr_scheduled", "=", "group", "[", "'lr'", "]", "\n", "\n", "", "update_with_lr", "=", "lr_scheduled", "*", "update", "\n", "p", ".", "data", ".", "add_", "(", "-", "update_with_lr", ")", "\n", "\n", "state", "[", "'step'", "]", "+=", "1", "\n", "\n", "# step_size = lr_scheduled * math.sqrt(bias_correction2) / bias_correction1", "\n", "# No bias correction", "\n", "# bias_correction1 = 1 - beta1 ** state['step']", "\n", "# bias_correction2 = 1 - beta2 ** state['step']", "\n", "\n", "", "", "return", "loss", "\n", "\n"]], "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.pytorch_pretrained_bert.optimization.BertAdamFineTune.__init__": [[182, 186], ["optimization.BertAdam.__init__"], "methods", ["home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.unilm.eval.Eval.__init__"], ["    ", "def", "__init__", "(", "self", ",", "params", ",", "lr", "=", "required", ",", "warmup", "=", "-", "1", ",", "t_total", "=", "-", "1", ",", "schedule", "=", "'warmup_linear'", ",", "b1", "=", "0.9", ",", "b2", "=", "0.999", ",", "e", "=", "1e-6", ",", "weight_decay", "=", "0.01", ",", "max_grad_norm", "=", "1.0", ")", ":", "\n", "        ", "self", ".", "init_param_group", "=", "[", "]", "\n", "super", "(", "BertAdamFineTune", ",", "self", ")", ".", "__init__", "(", "params", ",", "lr", ",", "warmup", ",", "\n", "t_total", ",", "schedule", ",", "b1", ",", "b2", ",", "e", ",", "weight_decay", ",", "max_grad_norm", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.pytorch_pretrained_bert.optimization.BertAdamFineTune.save_init_param_group": [[187, 203], ["zip", "zip", "optimization.BertAdamFineTune.init_param_group.append", "optimization.BertAdamFineTune.init_param_group.append", "p.data.clone().detach", "any", "init_p_list.append", "print", "p.data.clone().detach.zero_", "p.data.clone"], "methods", ["None"], ["", "def", "save_init_param_group", "(", "self", ",", "param_groups", ",", "name_groups", ",", "missing_keys", ")", ":", "\n", "        ", "self", ".", "init_param_group", "=", "[", "]", "\n", "for", "group", ",", "name", "in", "zip", "(", "param_groups", ",", "name_groups", ")", ":", "\n", "            ", "if", "group", "[", "'weight_decay'", "]", ">", "0.0", ":", "\n", "                ", "init_p_list", "=", "[", "]", "\n", "for", "p", ",", "n", "in", "zip", "(", "group", "[", "'params'", "]", ",", "name", ")", ":", "\n", "                    ", "init_p", "=", "p", ".", "data", ".", "clone", "(", ")", ".", "detach", "(", ")", "\n", "if", "any", "(", "mk", "in", "n", "for", "mk", "in", "missing_keys", ")", ":", "\n", "                        ", "print", "(", "\"[no finetuning weight decay]\"", ",", "n", ")", "\n", "# should use the original weight decay", "\n", "init_p", ".", "zero_", "(", ")", "\n", "", "init_p_list", ".", "append", "(", "init_p", ")", "\n", "", "self", ".", "init_param_group", ".", "append", "(", "init_p_list", ")", "\n", "", "else", ":", "\n", "# placeholder", "\n", "                ", "self", ".", "init_param_group", ".", "append", "(", "[", "]", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.pytorch_pretrained_bert.optimization.BertAdamFineTune.step": [[204, 280], ["enumerate", "closure", "enumerate", "next_m.mul_().add_", "next_v.mul_().addcmul_", "p.data.add_", "RuntimeError", "len", "torch.zeros_like", "torch.zeros_like", "torch.nn.utils.clip_grad_norm_", "next_m.mul_", "next_v.mul_", "next_v.sqrt", "schedule_fct"], "methods", ["None"], ["", "", "", "def", "step", "(", "self", ",", "closure", "=", "None", ")", ":", "\n", "        ", "\"\"\"Performs a single optimization step.\n\n        Arguments:\n            closure (callable, optional): A closure that reevaluates the model\n                and returns the loss.\n        \"\"\"", "\n", "loss", "=", "None", "\n", "if", "closure", "is", "not", "None", ":", "\n", "            ", "loss", "=", "closure", "(", ")", "\n", "\n", "", "for", "i_group", ",", "group", "in", "enumerate", "(", "self", ".", "param_groups", ")", ":", "\n", "            ", "for", "i_p", ",", "p", "in", "enumerate", "(", "group", "[", "'params'", "]", ")", ":", "\n", "                ", "if", "p", ".", "grad", "is", "None", ":", "\n", "                    ", "continue", "\n", "", "grad", "=", "p", ".", "grad", ".", "data", "\n", "if", "grad", ".", "is_sparse", ":", "\n", "                    ", "raise", "RuntimeError", "(", "\n", "'Adam does not support sparse gradients, please consider SparseAdam instead'", ")", "\n", "\n", "", "state", "=", "self", ".", "state", "[", "p", "]", "\n", "\n", "# State initialization", "\n", "if", "len", "(", "state", ")", "==", "0", ":", "\n", "                    ", "state", "[", "'step'", "]", "=", "0", "\n", "# Exponential moving average of gradient values", "\n", "state", "[", "'next_m'", "]", "=", "torch", ".", "zeros_like", "(", "p", ".", "data", ")", "\n", "# Exponential moving average of squared gradient values", "\n", "state", "[", "'next_v'", "]", "=", "torch", ".", "zeros_like", "(", "p", ".", "data", ")", "\n", "\n", "", "next_m", ",", "next_v", "=", "state", "[", "'next_m'", "]", ",", "state", "[", "'next_v'", "]", "\n", "beta1", ",", "beta2", "=", "group", "[", "'b1'", "]", ",", "group", "[", "'b2'", "]", "\n", "\n", "# Add grad clipping", "\n", "if", "group", "[", "'max_grad_norm'", "]", ">", "0", ":", "\n", "                    ", "clip_grad_norm_", "(", "p", ",", "group", "[", "'max_grad_norm'", "]", ")", "\n", "\n", "# Decay the first and second moment running average coefficient", "\n", "# In-place operations to update the averages at the same time", "\n", "", "next_m", ".", "mul_", "(", "beta1", ")", ".", "add_", "(", "1", "-", "beta1", ",", "grad", ")", "\n", "next_v", ".", "mul_", "(", "beta2", ")", ".", "addcmul_", "(", "1", "-", "beta2", ",", "grad", ",", "grad", ")", "\n", "update", "=", "next_m", "/", "(", "next_v", ".", "sqrt", "(", ")", "+", "group", "[", "'e'", "]", ")", "\n", "\n", "# Just adding the square of the weights to the loss function is *not*", "\n", "# the correct way of using L2 regularization/weight decay with Adam,", "\n", "# since that will interact with the m and v parameters in strange ways.", "\n", "#", "\n", "# Instead we want to decay the weights in a manner that doesn't interact", "\n", "# with the m/v parameters. This is equivalent to adding the square", "\n", "# of the weights to the loss with plain (non-momentum) SGD.", "\n", "if", "group", "[", "'weight_decay'", "]", ">", "0.0", ":", "\n", "                    ", "if", "self", ".", "init_param_group", ":", "\n", "                        ", "update", "+=", "group", "[", "'weight_decay'", "]", "*", "(", "2.0", "*", "p", ".", "data", "-", "\n", "self", ".", "init_param_group", "[", "i_group", "]", "[", "i_p", "]", ")", "\n", "", "else", ":", "\n", "                        ", "update", "+=", "group", "[", "'weight_decay'", "]", "*", "p", ".", "data", "\n", "\n", "", "", "if", "group", "[", "'t_total'", "]", "!=", "-", "1", ":", "\n", "                    ", "schedule_fct", "=", "SCHEDULES", "[", "group", "[", "'schedule'", "]", "]", "\n", "lr_scheduled", "=", "group", "[", "'lr'", "]", "*", "schedule_fct", "(", "\n", "state", "[", "'step'", "]", "/", "group", "[", "'t_total'", "]", ",", "group", "[", "'warmup'", "]", ")", "\n", "", "else", ":", "\n", "                    ", "lr_scheduled", "=", "group", "[", "'lr'", "]", "\n", "\n", "", "update_with_lr", "=", "lr_scheduled", "*", "update", "\n", "p", ".", "data", ".", "add_", "(", "-", "update_with_lr", ")", "\n", "\n", "state", "[", "'step'", "]", "+=", "1", "\n", "\n", "# step_size = lr_scheduled * math.sqrt(bias_correction2) / bias_correction1", "\n", "# No bias correction", "\n", "# bias_correction1 = 1 - beta1 ** state['step']", "\n", "# bias_correction2 = 1 - beta2 ** state['step']", "\n", "\n", "", "", "return", "loss", "\n", "\n"]], "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.pytorch_pretrained_bert.optimization.BertAdamFineTune.load_state_dict_subset_finetune": [[281, 350], ["copy.deepcopy", "any", "collections.defaultdict", "state_dict[].items", "optimization.BertAdamFineTune.__setstate__", "ValueError", "len", "len", "ValueError", "isinstance", "len", "len", "zip", "param.is_floating_point", "value.to.to.to", "isinstance", "optimization.BertAdamFineTune.load_state_dict_subset_finetune.cast"], "methods", ["None"], ["", "def", "load_state_dict_subset_finetune", "(", "self", ",", "state_dict", ",", "num_load_group", ")", ":", "\n", "        ", "r\"\"\"Loads the optimizer state.\n\n        Arguments:\n            state_dict (dict): optimizer state. Should be an object returned\n                from a call to :meth:`state_dict`.\n        \"\"\"", "\n", "# deepcopy, to be consistent with module API", "\n", "state_dict", "=", "deepcopy", "(", "state_dict", ")", "\n", "# Validate the state_dict", "\n", "groups", "=", "self", ".", "param_groups", "\n", "saved_groups", "=", "state_dict", "[", "'param_groups'", "]", "\n", "\n", "if", "len", "(", "groups", ")", "<", "num_load_group", "or", "len", "(", "saved_groups", ")", "<", "num_load_group", ":", "\n", "            ", "raise", "ValueError", "(", "\"loaded state dict has a different number of \"", "\n", "\"parameter groups\"", ")", "\n", "", "param_lens", "=", "(", "len", "(", "g", "[", "'params'", "]", ")", "for", "g", "in", "groups", "[", ":", "num_load_group", "]", ")", "\n", "saved_lens", "=", "(", "len", "(", "g", "[", "'params'", "]", ")", "for", "g", "in", "saved_groups", "[", ":", "num_load_group", "]", ")", "\n", "if", "any", "(", "p_len", "!=", "s_len", "for", "p_len", ",", "s_len", "in", "zip", "(", "param_lens", ",", "saved_lens", ")", ")", ":", "\n", "            ", "raise", "ValueError", "(", "\"loaded state dict contains a parameter group \"", "\n", "\"that doesn't match the size of optimizer's group\"", ")", "\n", "\n", "# Update the state", "\n", "", "id_map", "=", "{", "old_id", ":", "p", "for", "old_id", ",", "p", "in", "\n", "zip", "(", "chain", "(", "*", "(", "g", "[", "'params'", "]", "for", "g", "in", "saved_groups", "[", ":", "num_load_group", "]", ")", ")", ",", "\n", "chain", "(", "*", "(", "g", "[", "'params'", "]", "for", "g", "in", "groups", "[", ":", "num_load_group", "]", ")", ")", ")", "}", "\n", "\n", "def", "cast", "(", "param", ",", "value", ")", ":", "\n", "            ", "r\"\"\"Make a deep copy of value, casting all tensors to device of param.\"\"\"", "\n", "if", "isinstance", "(", "value", ",", "torch", ".", "Tensor", ")", ":", "\n", "# Floating-point types are a bit special here. They are the only ones", "\n", "# that are assumed to always match the type of params.", "\n", "                ", "if", "param", ".", "is_floating_point", "(", ")", ":", "\n", "                    ", "value", "=", "value", ".", "to", "(", "param", ".", "dtype", ")", "\n", "", "value", "=", "value", ".", "to", "(", "param", ".", "device", ")", "\n", "return", "value", "\n", "", "elif", "isinstance", "(", "value", ",", "dict", ")", ":", "\n", "                ", "return", "{", "k", ":", "cast", "(", "param", ",", "v", ")", "for", "k", ",", "v", "in", "value", ".", "items", "(", ")", "}", "\n", "", "elif", "isinstance", "(", "value", ",", "container_abcs", ".", "Iterable", ")", ":", "\n", "                ", "return", "type", "(", "value", ")", "(", "cast", "(", "param", ",", "v", ")", "for", "v", "in", "value", ")", "\n", "", "else", ":", "\n", "                ", "return", "value", "\n", "\n", "# Copy state assigned to params (and cast tensors to appropriate types).", "\n", "# State that is not assigned to params is copied as is (needed for", "\n", "# backward compatibility).", "\n", "", "", "state", "=", "defaultdict", "(", "dict", ")", "\n", "for", "k", ",", "v", "in", "state_dict", "[", "'state'", "]", ".", "items", "(", ")", ":", "\n", "            ", "if", "k", "in", "id_map", ":", "\n", "                ", "param", "=", "id_map", "[", "k", "]", "\n", "state", "[", "param", "]", "=", "cast", "(", "param", ",", "v", ")", "\n", "", "else", ":", "\n", "                ", "state", "[", "k", "]", "=", "v", "\n", "# handle additional params", "\n", "", "", "for", "k", ",", "v", "in", "self", ".", "state", ":", "\n", "            ", "if", "k", "not", "in", "state", ":", "\n", "                ", "state", "[", "k", "]", "=", "v", "\n", "\n", "# do not change groups: {'weight_decay': 0.01, 'lr': 9.995e-06, 'schedule': 'warmup_linear', 'warmup': 0.1, 't_total': 400000, 'b1': 0.9, 'b2': 0.999, 'e': 1e-06, 'max_grad_norm': 1.0, 'params': [...]}", "\n", "# # Update parameter groups, setting their 'params' value", "\n", "# def update_group(group, new_group):", "\n", "#     new_group['params'] = group['params']", "\n", "#     return new_group", "\n", "# param_groups = [", "\n", "#     update_group(g, ng) for g, ng in zip(groups[:num_load_group], saved_groups[:num_load_group])]", "\n", "# # handle additional params", "\n", "# param_groups.extend(groups[num_load_group:])", "\n", "\n", "", "", "self", ".", "__setstate__", "(", "{", "'state'", ":", "state", ",", "'param_groups'", ":", "groups", "}", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.pytorch_pretrained_bert.optimization.warmup_cosine": [[29, 33], ["torch.cos"], "function", ["None"], ["def", "warmup_cosine", "(", "x", ",", "warmup", "=", "0.002", ")", ":", "\n", "    ", "if", "x", "<", "warmup", ":", "\n", "        ", "return", "x", "/", "warmup", "\n", "", "return", "0.5", "*", "(", "1.0", "+", "torch", ".", "cos", "(", "math", ".", "pi", "*", "x", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.pytorch_pretrained_bert.optimization.warmup_constant": [[35, 39], ["None"], "function", ["None"], ["", "def", "warmup_constant", "(", "x", ",", "warmup", "=", "0.002", ")", ":", "\n", "    ", "if", "x", "<", "warmup", ":", "\n", "        ", "return", "x", "/", "warmup", "\n", "", "return", "1.0", "\n", "\n"]], "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.pytorch_pretrained_bert.optimization.warmup_linear": [[41, 45], ["max"], "function", ["None"], ["", "def", "warmup_linear", "(", "x", ",", "warmup", "=", "0.002", ")", ":", "\n", "    ", "if", "x", "<", "warmup", ":", "\n", "        ", "return", "x", "/", "warmup", "\n", "", "return", "max", "(", "(", "x", "-", "1.", ")", "/", "(", "warmup", "-", "1.", ")", ",", "0", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.pytorch_pretrained_bert.optimization.find_state_dict_subset_finetune": [[352, 402], ["set", "zip", "new_state_dict[].items", "zip", "optimization.find_state_dict_subset_finetune._filter_group"], "function", ["None"], ["", "", "def", "find_state_dict_subset_finetune", "(", "org_state_dict", ",", "org_name_list", ",", "no_decay", ",", "param_optimizer", ")", ":", "\n", "# only use the bert encoder and embeddings", "\n", "    ", "want_name_set", "=", "set", "(", ")", "\n", "for", "n", "in", "org_name_list", ":", "\n", "        ", "if", "(", "'bert.encoder'", "in", "n", ")", "or", "(", "'bert.embeddings'", "in", "n", ")", ":", "\n", "            ", "want_name_set", ".", "add", "(", "n", ")", "\n", "# original: name to pid, pid to name", "\n", "", "", "org_grouped_names", "=", "[", "[", "n", "for", "n", "in", "org_name_list", "if", "not", "any", "(", "nd", "in", "n", "for", "nd", "in", "no_decay", ")", "]", ",", "\n", "[", "n", "for", "n", "in", "org_name_list", "if", "any", "(", "nd", "in", "n", "for", "nd", "in", "no_decay", ")", "]", "]", "\n", "org_n2id", ",", "org_id2n", "=", "{", "}", ",", "{", "}", "\n", "for", "ng", ",", "pg", "in", "zip", "(", "org_grouped_names", ",", "org_state_dict", "[", "'param_groups'", "]", ")", ":", "\n", "        ", "for", "n", ",", "pid", "in", "zip", "(", "ng", ",", "pg", "[", "'params'", "]", ")", ":", "\n", "            ", "org_n2id", "[", "n", "]", "=", "pid", "\n", "org_id2n", "[", "pid", "]", "=", "n", "\n", "# group by: whether pretrained; whether weight decay", "\n", "", "", "g_np_list", "=", "[", "\n", "[", "(", "n", ",", "p", ")", "for", "n", ",", "p", "in", "param_optimizer", "if", "n", "in", "want_name_set", "and", "not", "any", "(", "\n", "nd", "in", "n", "for", "nd", "in", "no_decay", ")", "]", ",", "\n", "[", "(", "n", ",", "p", ")", "for", "n", ",", "p", "in", "param_optimizer", "if", "n", "in", "want_name_set", "and", "any", "(", "\n", "nd", "in", "n", "for", "nd", "in", "no_decay", ")", "]", ",", "\n", "[", "(", "n", ",", "p", ")", "for", "n", ",", "p", "in", "param_optimizer", "if", "n", "not", "in", "want_name_set", "and", "not", "any", "(", "\n", "nd", "in", "n", "for", "nd", "in", "no_decay", ")", "]", ",", "\n", "[", "(", "n", ",", "p", ")", "for", "n", ",", "p", "in", "param_optimizer", "if", "n", "not", "in", "want_name_set", "and", "any", "(", "\n", "nd", "in", "n", "for", "nd", "in", "no_decay", ")", "]", ",", "\n", "]", "\n", "optimizer_grouped_parameters", "=", "[", "\n", "{", "'params'", ":", "[", "p", "for", "n", ",", "p", "in", "g_np_list", "[", "0", "]", "]", ",", "'weight_decay'", ":", "0.01", "}", ",", "\n", "{", "'params'", ":", "[", "p", "for", "n", ",", "p", "in", "g_np_list", "[", "1", "]", "]", ",", "'weight_decay'", ":", "0.0", "}", ",", "\n", "{", "'params'", ":", "[", "p", "for", "n", ",", "p", "in", "g_np_list", "[", "2", "]", "]", ",", "'weight_decay'", ":", "0.01", "}", ",", "\n", "{", "'params'", ":", "[", "p", "for", "n", ",", "p", "in", "g_np_list", "[", "3", "]", "]", ",", "'weight_decay'", ":", "0.0", "}", "\n", "]", "\n", "new_state_dict", "=", "{", "}", "\n", "# regroup the original state_dict", "\n", "new_state_dict", "[", "'state'", "]", "=", "{", "pid", ":", "v", "for", "pid", ",", "v", "in", "org_state_dict", "[", "'state'", "]", ".", "items", "(", "\n", ")", "if", "pid", "not", "in", "org_id2n", "or", "org_id2n", "[", "pid", "]", "in", "want_name_set", "}", "\n", "# reset step count to 0", "\n", "for", "pid", ",", "st", "in", "new_state_dict", "[", "'state'", "]", ".", "items", "(", ")", ":", "\n", "        ", "st", "[", "'step'", "]", "=", "0", "\n", "\n", "", "def", "_filter_group", "(", "group", ",", "g_np_list", ",", "i", ",", "org_n2id", ")", ":", "\n", "        ", "packed", "=", "{", "k", ":", "v", "for", "k", ",", "v", "in", "group", ".", "items", "(", ")", "if", "k", "!=", "'params'", "}", "\n", "packed", "[", "'params'", "]", "=", "[", "pid", "for", "pid", "in", "group", "[", "'params'", "]", "\n", "if", "pid", "in", "org_id2n", "and", "org_id2n", "[", "pid", "]", "in", "want_name_set", "]", "\n", "assert", "len", "(", "g_np_list", "[", "i", "]", ")", "==", "len", "(", "packed", "[", "'params'", "]", ")", "\n", "# keep them the same order", "\n", "packed", "[", "'params'", "]", "=", "[", "org_n2id", "[", "n", "]", "for", "n", ",", "p", "in", "g_np_list", "[", "i", "]", "]", "\n", "return", "packed", "\n", "", "new_state_dict", "[", "'param_groups'", "]", "=", "[", "_filter_group", "(", "\n", "g", ",", "g_np_list", ",", "i", ",", "org_n2id", ")", "for", "i", ",", "g", "in", "enumerate", "(", "org_state_dict", "[", "'param_groups'", "]", ")", "]", "\n", "return", "new_state_dict", ",", "optimizer_grouped_parameters", "\n", "", ""]], "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.pytorch_pretrained_bert.file_utils.url_to_filename": [[30, 46], ["url.encode", "hashlib.sha256", "hashlib.sha256.hexdigest", "etag.encode", "hashlib.sha256", "hashlib.sha256.hexdigest"], "function", ["None"], ["def", "url_to_filename", "(", "url", ":", "str", ",", "etag", ":", "str", "=", "None", ")", "->", "str", ":", "\n", "    ", "\"\"\"\n    Convert `url` into a hashed filename in a repeatable way.\n    If `etag` is specified, append its hash to the url's, delimited\n    by a period.\n    \"\"\"", "\n", "url_bytes", "=", "url", ".", "encode", "(", "'utf-8'", ")", "\n", "url_hash", "=", "sha256", "(", "url_bytes", ")", "\n", "filename", "=", "url_hash", ".", "hexdigest", "(", ")", "\n", "\n", "if", "etag", ":", "\n", "        ", "etag_bytes", "=", "etag", ".", "encode", "(", "'utf-8'", ")", "\n", "etag_hash", "=", "sha256", "(", "etag_bytes", ")", "\n", "filename", "+=", "'.'", "+", "etag_hash", ".", "hexdigest", "(", ")", "\n", "\n", "", "return", "filename", "\n", "\n"]], "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.pytorch_pretrained_bert.file_utils.filename_to_url": [[48, 72], ["isinstance", "os.path.join", "str", "os.path.exists", "FileNotFoundError", "os.path.exists", "FileNotFoundError", "open", "json.load"], "function", ["None"], ["", "def", "filename_to_url", "(", "filename", ":", "str", ",", "cache_dir", ":", "Union", "[", "str", ",", "Path", "]", "=", "None", ")", "->", "Tuple", "[", "str", ",", "str", "]", ":", "\n", "    ", "\"\"\"\n    Return the url and etag (which may be ``None``) stored for `filename`.\n    Raise ``FileNotFoundError`` if `filename` or its stored metadata do not exist.\n    \"\"\"", "\n", "if", "cache_dir", "is", "None", ":", "\n", "        ", "cache_dir", "=", "PYTORCH_PRETRAINED_BERT_CACHE", "\n", "", "if", "isinstance", "(", "cache_dir", ",", "Path", ")", ":", "\n", "        ", "cache_dir", "=", "str", "(", "cache_dir", ")", "\n", "\n", "", "cache_path", "=", "os", ".", "path", ".", "join", "(", "cache_dir", ",", "filename", ")", "\n", "if", "not", "os", ".", "path", ".", "exists", "(", "cache_path", ")", ":", "\n", "        ", "raise", "FileNotFoundError", "(", "\"file {} not found\"", ".", "format", "(", "cache_path", ")", ")", "\n", "\n", "", "meta_path", "=", "cache_path", "+", "'.json'", "\n", "if", "not", "os", ".", "path", ".", "exists", "(", "meta_path", ")", ":", "\n", "        ", "raise", "FileNotFoundError", "(", "\"file {} not found\"", ".", "format", "(", "meta_path", ")", ")", "\n", "\n", "", "with", "open", "(", "meta_path", ")", "as", "meta_file", ":", "\n", "        ", "metadata", "=", "json", ".", "load", "(", "meta_file", ")", "\n", "", "url", "=", "metadata", "[", "'url'", "]", "\n", "etag", "=", "metadata", "[", "'etag'", "]", "\n", "\n", "return", "url", ",", "etag", "\n", "\n"]], "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.pytorch_pretrained_bert.file_utils.cached_path": [[74, 102], ["isinstance", "isinstance", "urllib.parse.urlparse", "str", "str", "file_utils.get_from_cache", "os.path.exists", "FileNotFoundError", "ValueError"], "function", ["home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.pytorch_pretrained_bert.file_utils.get_from_cache"], ["", "def", "cached_path", "(", "url_or_filename", ":", "Union", "[", "str", ",", "Path", "]", ",", "cache_dir", ":", "Union", "[", "str", ",", "Path", "]", "=", "None", ")", "->", "str", ":", "\n", "    ", "\"\"\"\n    Given something that might be a URL (or might be a local path),\n    determine which. If it's a URL, download the file and cache it, and\n    return the path to the cached file. If it's already a local path,\n    make sure the file exists and then return the path.\n    \"\"\"", "\n", "if", "cache_dir", "is", "None", ":", "\n", "        ", "cache_dir", "=", "PYTORCH_PRETRAINED_BERT_CACHE", "\n", "", "if", "isinstance", "(", "url_or_filename", ",", "Path", ")", ":", "\n", "        ", "url_or_filename", "=", "str", "(", "url_or_filename", ")", "\n", "", "if", "isinstance", "(", "cache_dir", ",", "Path", ")", ":", "\n", "        ", "cache_dir", "=", "str", "(", "cache_dir", ")", "\n", "\n", "", "parsed", "=", "urlparse", "(", "url_or_filename", ")", "\n", "\n", "if", "parsed", ".", "scheme", "in", "(", "'http'", ",", "'https'", ",", "'s3'", ")", ":", "\n", "# URL, so get it from the cache (downloading if necessary)", "\n", "        ", "return", "get_from_cache", "(", "url_or_filename", ",", "cache_dir", ")", "\n", "", "elif", "os", ".", "path", ".", "exists", "(", "url_or_filename", ")", ":", "\n", "# File, and it exists.", "\n", "        ", "return", "url_or_filename", "\n", "", "elif", "parsed", ".", "scheme", "==", "''", ":", "\n", "# File, but it doesn't exist.", "\n", "        ", "raise", "FileNotFoundError", "(", "\"file {} not found\"", ".", "format", "(", "url_or_filename", ")", ")", "\n", "", "else", ":", "\n", "# Something unknown", "\n", "        ", "raise", "ValueError", "(", "\"unable to parse {} as a URL or as a local path\"", ".", "format", "(", "url_or_filename", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.pytorch_pretrained_bert.file_utils.split_s3_path": [[104, 115], ["urllib.parse.urlparse", "s3_path.startswith", "ValueError"], "function", ["None"], ["", "", "def", "split_s3_path", "(", "url", ":", "str", ")", "->", "Tuple", "[", "str", ",", "str", "]", ":", "\n", "    ", "\"\"\"Split a full s3 path into the bucket name and path.\"\"\"", "\n", "parsed", "=", "urlparse", "(", "url", ")", "\n", "if", "not", "parsed", ".", "netloc", "or", "not", "parsed", ".", "path", ":", "\n", "        ", "raise", "ValueError", "(", "\"bad s3 path {}\"", ".", "format", "(", "url", ")", ")", "\n", "", "bucket_name", "=", "parsed", ".", "netloc", "\n", "s3_path", "=", "parsed", ".", "path", "\n", "# Remove '/' at beginning of path.", "\n", "if", "s3_path", ".", "startswith", "(", "\"/\"", ")", ":", "\n", "        ", "s3_path", "=", "s3_path", "[", "1", ":", "]", "\n", "", "return", "bucket_name", ",", "s3_path", "\n", "\n"]], "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.pytorch_pretrained_bert.file_utils.s3_request": [[117, 134], ["functools.wraps", "func", "int", "FileNotFoundError"], "function", ["None"], ["", "def", "s3_request", "(", "func", ":", "Callable", ")", ":", "\n", "    ", "\"\"\"\n    Wrapper function for s3 requests in order to create more helpful error\n    messages.\n    \"\"\"", "\n", "\n", "@", "wraps", "(", "func", ")", "\n", "def", "wrapper", "(", "url", ":", "str", ",", "*", "args", ",", "**", "kwargs", ")", ":", "\n", "        ", "try", ":", "\n", "            ", "return", "func", "(", "url", ",", "*", "args", ",", "**", "kwargs", ")", "\n", "", "except", "ClientError", "as", "exc", ":", "\n", "            ", "if", "int", "(", "exc", ".", "response", "[", "\"Error\"", "]", "[", "\"Code\"", "]", ")", "==", "404", ":", "\n", "                ", "raise", "FileNotFoundError", "(", "\"file {} not found\"", ".", "format", "(", "url", ")", ")", "\n", "", "else", ":", "\n", "                ", "raise", "\n", "\n", "", "", "", "return", "wrapper", "\n", "\n"]], "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.pytorch_pretrained_bert.file_utils.s3_etag": [[136, 143], ["boto3.resource", "file_utils.split_s3_path", "boto3.resource.Object"], "function", ["home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.pytorch_pretrained_bert.file_utils.split_s3_path"], ["", "@", "s3_request", "\n", "def", "s3_etag", "(", "url", ":", "str", ")", "->", "Optional", "[", "str", "]", ":", "\n", "    ", "\"\"\"Check ETag on S3 object.\"\"\"", "\n", "s3_resource", "=", "boto3", ".", "resource", "(", "\"s3\"", ")", "\n", "bucket_name", ",", "s3_path", "=", "split_s3_path", "(", "url", ")", "\n", "s3_object", "=", "s3_resource", ".", "Object", "(", "bucket_name", ",", "s3_path", ")", "\n", "return", "s3_object", ".", "e_tag", "\n", "\n"]], "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.pytorch_pretrained_bert.file_utils.s3_get": [[145, 151], ["boto3.resource", "file_utils.split_s3_path", "boto3.resource.Bucket().download_fileobj", "boto3.resource.Bucket"], "function", ["home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.pytorch_pretrained_bert.file_utils.split_s3_path"], ["", "@", "s3_request", "\n", "def", "s3_get", "(", "url", ":", "str", ",", "temp_file", ":", "IO", ")", "->", "None", ":", "\n", "    ", "\"\"\"Pull a file directly from S3.\"\"\"", "\n", "s3_resource", "=", "boto3", ".", "resource", "(", "\"s3\"", ")", "\n", "bucket_name", ",", "s3_path", "=", "split_s3_path", "(", "url", ")", "\n", "s3_resource", ".", "Bucket", "(", "bucket_name", ")", ".", "download_fileobj", "(", "s3_path", ",", "temp_file", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.pytorch_pretrained_bert.file_utils.http_get": [[153, 163], ["requests.get", "requests.get.headers.get", "tqdm.tqdm", "requests.get.iter_content", "tqdm.tqdm.close", "int", "tqdm.tqdm.update", "temp_file.write", "len"], "function", ["None"], ["", "def", "http_get", "(", "url", ":", "str", ",", "temp_file", ":", "IO", ")", "->", "None", ":", "\n", "    ", "req", "=", "requests", ".", "get", "(", "url", ",", "stream", "=", "True", ")", "\n", "content_length", "=", "req", ".", "headers", ".", "get", "(", "'Content-Length'", ")", "\n", "total", "=", "int", "(", "content_length", ")", "if", "content_length", "is", "not", "None", "else", "None", "\n", "progress", "=", "tqdm", "(", "unit", "=", "\"B\"", ",", "total", "=", "total", ")", "\n", "for", "chunk", "in", "req", ".", "iter_content", "(", "chunk_size", "=", "1024", ")", ":", "\n", "        ", "if", "chunk", ":", "# filter out keep-alive new chunks", "\n", "            ", "progress", ".", "update", "(", "len", "(", "chunk", ")", ")", "\n", "temp_file", ".", "write", "(", "chunk", ")", "\n", "", "", "progress", ".", "close", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.pytorch_pretrained_bert.file_utils.get_from_cache": [[165, 222], ["isinstance", "os.makedirs", "url.startswith", "file_utils.url_to_filename", "os.path.join", "str", "file_utils.s3_etag", "requests.head", "requests.head.headers.get", "os.path.exists", "IOError", "tempfile.NamedTemporaryFile", "logger.info", "url.startswith", "temp_file.flush", "temp_file.seek", "logger.info", "logger.info", "logger.info", "file_utils.s3_get", "file_utils.http_get", "open", "shutil.copyfileobj", "open", "json.dump"], "function", ["home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.pytorch_pretrained_bert.file_utils.url_to_filename", "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.pytorch_pretrained_bert.file_utils.s3_etag", "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.pytorch_pretrained_bert.file_utils.s3_get", "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.pytorch_pretrained_bert.file_utils.http_get"], ["", "def", "get_from_cache", "(", "url", ":", "str", ",", "cache_dir", ":", "Union", "[", "str", ",", "Path", "]", "=", "None", ")", "->", "str", ":", "\n", "    ", "\"\"\"\n    Given a URL, look for the corresponding dataset in the local cache.\n    If it's not there, download it. Then return the path to the cached file.\n    \"\"\"", "\n", "if", "cache_dir", "is", "None", ":", "\n", "        ", "cache_dir", "=", "PYTORCH_PRETRAINED_BERT_CACHE", "\n", "", "if", "isinstance", "(", "cache_dir", ",", "Path", ")", ":", "\n", "        ", "cache_dir", "=", "str", "(", "cache_dir", ")", "\n", "\n", "", "os", ".", "makedirs", "(", "cache_dir", ",", "exist_ok", "=", "True", ")", "\n", "\n", "# Get eTag to add to filename, if it exists.", "\n", "if", "url", ".", "startswith", "(", "\"s3://\"", ")", ":", "\n", "        ", "etag", "=", "s3_etag", "(", "url", ")", "\n", "", "else", ":", "\n", "        ", "response", "=", "requests", ".", "head", "(", "url", ",", "allow_redirects", "=", "True", ")", "\n", "if", "response", ".", "status_code", "!=", "200", ":", "\n", "            ", "raise", "IOError", "(", "\"HEAD request failed for url {} with status code {}\"", "\n", ".", "format", "(", "url", ",", "response", ".", "status_code", ")", ")", "\n", "", "etag", "=", "response", ".", "headers", ".", "get", "(", "\"ETag\"", ")", "\n", "\n", "", "filename", "=", "url_to_filename", "(", "url", ",", "etag", ")", "\n", "\n", "# get cache path to put the file", "\n", "cache_path", "=", "os", ".", "path", ".", "join", "(", "cache_dir", ",", "filename", ")", "\n", "\n", "if", "not", "os", ".", "path", ".", "exists", "(", "cache_path", ")", ":", "\n", "# Download to temporary file, then copy to cache dir once finished.", "\n", "# Otherwise you get corrupt cache entries if the download gets interrupted.", "\n", "        ", "with", "tempfile", ".", "NamedTemporaryFile", "(", ")", "as", "temp_file", ":", "\n", "            ", "logger", ".", "info", "(", "\"%s not found in cache, downloading to %s\"", ",", "url", ",", "temp_file", ".", "name", ")", "\n", "\n", "# GET file object", "\n", "if", "url", ".", "startswith", "(", "\"s3://\"", ")", ":", "\n", "                ", "s3_get", "(", "url", ",", "temp_file", ")", "\n", "", "else", ":", "\n", "                ", "http_get", "(", "url", ",", "temp_file", ")", "\n", "\n", "# we are copying the file before closing it, so flush to avoid truncation", "\n", "", "temp_file", ".", "flush", "(", ")", "\n", "# shutil.copyfileobj() starts at the current position, so go to the start", "\n", "temp_file", ".", "seek", "(", "0", ")", "\n", "\n", "logger", ".", "info", "(", "\"copying %s to cache at %s\"", ",", "temp_file", ".", "name", ",", "cache_path", ")", "\n", "with", "open", "(", "cache_path", ",", "'wb'", ")", "as", "cache_file", ":", "\n", "                ", "shutil", ".", "copyfileobj", "(", "temp_file", ",", "cache_file", ")", "\n", "\n", "", "logger", ".", "info", "(", "\"creating metadata file for %s\"", ",", "cache_path", ")", "\n", "meta", "=", "{", "'url'", ":", "url", ",", "'etag'", ":", "etag", "}", "\n", "meta_path", "=", "cache_path", "+", "'.json'", "\n", "with", "open", "(", "meta_path", ",", "'w'", ")", "as", "meta_file", ":", "\n", "                ", "json", ".", "dump", "(", "meta", ",", "meta_file", ")", "\n", "\n", "", "logger", ".", "info", "(", "\"removing temp file %s\"", ",", "temp_file", ".", "name", ")", "\n", "\n", "", "", "return", "cache_path", "\n", "\n"]], "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.pytorch_pretrained_bert.file_utils.read_set_from_file": [[224, 234], ["set", "open", "set.add", "line.rstrip"], "function", ["home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.biunilm.loader_utils.TrieTree.add"], ["", "def", "read_set_from_file", "(", "filename", ":", "str", ")", "->", "Set", "[", "str", "]", ":", "\n", "    ", "'''\n    Extract a de-duped collection (set) of text from a file.\n    Expected file format is one item per line.\n    '''", "\n", "collection", "=", "set", "(", ")", "\n", "with", "open", "(", "filename", ",", "'r'", ",", "encoding", "=", "'utf-8'", ")", "as", "file_", ":", "\n", "        ", "for", "line", "in", "file_", ":", "\n", "            ", "collection", ".", "add", "(", "line", ".", "rstrip", "(", ")", ")", "\n", "", "", "return", "collection", "\n", "\n"]], "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.pytorch_pretrained_bert.file_utils.get_file_extension": [[236, 240], ["os.path.splitext", "ext.lower"], "function", ["None"], ["", "def", "get_file_extension", "(", "path", ":", "str", ",", "dot", "=", "True", ",", "lower", ":", "bool", "=", "True", ")", ":", "\n", "    ", "ext", "=", "os", ".", "path", ".", "splitext", "(", "path", ")", "[", "1", "]", "\n", "ext", "=", "ext", "if", "dot", "else", "ext", "[", "1", ":", "]", "\n", "return", "ext", ".", "lower", "(", ")", "if", "lower", "else", "ext", "\n", "", ""]], "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.pytorch_pretrained_bert.tokenization.BertTokenizer.__init__": [[93, 105], ["tokenization.load_vocab", "collections.OrderedDict", "tokenization.BasicTokenizer", "tokenization.WordpieceTokenizer", "os.path.isfile", "ValueError", "int", "tokenization.BertTokenizer.vocab.items"], "methods", ["home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.pytorch_pretrained_bert.tokenization.load_vocab"], ["def", "__init__", "(", "self", ",", "vocab_file", ",", "do_lower_case", "=", "True", ",", "max_len", "=", "None", ",", "never_split", "=", "(", "\"[UNK]\"", ",", "\"[SEP]\"", ",", "\"[X_SEP]\"", ",", "\"[PAD]\"", ",", "\"[CLS]\"", ",", "\"[MASK]\"", ")", ")", ":", "\n", "        ", "if", "not", "os", ".", "path", ".", "isfile", "(", "vocab_file", ")", ":", "\n", "            ", "raise", "ValueError", "(", "\n", "\"Can't find a vocabulary file at path '{}'. To load the vocabulary from a Google pretrained \"", "\n", "\"model use `tokenizer = BertTokenizer.from_pretrained(PRETRAINED_MODEL_NAME)`\"", ".", "format", "(", "vocab_file", ")", ")", "\n", "", "self", ".", "vocab", "=", "load_vocab", "(", "vocab_file", ")", "\n", "self", ".", "ids_to_tokens", "=", "collections", ".", "OrderedDict", "(", "\n", "[", "(", "ids", ",", "tok", ")", "for", "tok", ",", "ids", "in", "self", ".", "vocab", ".", "items", "(", ")", "]", ")", "\n", "self", ".", "basic_tokenizer", "=", "BasicTokenizer", "(", "\n", "do_lower_case", "=", "do_lower_case", ",", "never_split", "=", "never_split", ")", "\n", "self", ".", "wordpiece_tokenizer", "=", "WordpieceTokenizer", "(", "vocab", "=", "self", ".", "vocab", ")", "\n", "self", ".", "max_len", "=", "max_len", "if", "max_len", "is", "not", "None", "else", "int", "(", "1e12", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.pytorch_pretrained_bert.tokenization.BertTokenizer.tokenize": [[106, 112], ["tokenization.BertTokenizer.basic_tokenizer.tokenize", "tokenization.BertTokenizer.wordpiece_tokenizer.tokenize", "split_tokens.append"], "methods", ["home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.preprocess.tokenizing_1342.tokenize", "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.preprocess.tokenizing_1342.tokenize"], ["", "def", "tokenize", "(", "self", ",", "text", ")", ":", "\n", "        ", "split_tokens", "=", "[", "]", "\n", "for", "token", "in", "self", ".", "basic_tokenizer", ".", "tokenize", "(", "text", ")", ":", "\n", "            ", "for", "sub_token", "in", "self", ".", "wordpiece_tokenizer", ".", "tokenize", "(", "token", ")", ":", "\n", "                ", "split_tokens", ".", "append", "(", "sub_token", ")", "\n", "", "", "return", "split_tokens", "\n", "\n"]], "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.pytorch_pretrained_bert.tokenization.BertTokenizer.convert_tokens_to_ids": [[113, 126], ["ids.append", "len", "ValueError", "len"], "methods", ["None"], ["", "def", "convert_tokens_to_ids", "(", "self", ",", "tokens", ")", ":", "\n", "        ", "\"\"\"Converts a sequence of tokens into ids using the vocab.\"\"\"", "\n", "ids", "=", "[", "]", "\n", "for", "token", "in", "tokens", ":", "\n", "            ", "ids", ".", "append", "(", "self", ".", "vocab", "[", "token", "]", ")", "\n", "", "if", "len", "(", "ids", ")", ">", "self", ".", "max_len", ":", "\n", "            ", "raise", "ValueError", "(", "\n", "\"Token indices sequence length is longer than the specified maximum \"", "\n", "\" sequence length for this BERT model ({} > {}). Running this\"", "\n", "\" sequence through BERT will result in indexing errors\"", ".", "format", "(", "\n", "len", "(", "ids", ")", ",", "self", ".", "max_len", ")", "\n", ")", "\n", "", "return", "ids", "\n", "\n"]], "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.pytorch_pretrained_bert.tokenization.BertTokenizer.convert_ids_to_tokens": [[127, 133], ["tokens.append"], "methods", ["None"], ["", "def", "convert_ids_to_tokens", "(", "self", ",", "ids", ")", ":", "\n", "        ", "\"\"\"Converts a sequence of ids in wordpiece tokens using the vocab.\"\"\"", "\n", "tokens", "=", "[", "]", "\n", "for", "i", "in", "ids", ":", "\n", "            ", "tokens", ".", "append", "(", "self", ".", "ids_to_tokens", "[", "i", "]", ")", "\n", "", "return", "tokens", "\n", "\n"]], "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.pytorch_pretrained_bert.tokenization.BertTokenizer.from_pretrained": [[134, 171], ["os.path.isdir", "cls", "os.path.join", "file_utils.cached_path", "logger.info", "logger.info", "min", "logger.error", "kwargs.get", "int", "PRETRAINED_VOCAB_ARCHIVE_MAP.keys"], "methods", ["home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.pytorch_pretrained_bert.file_utils.cached_path"], ["", "@", "classmethod", "\n", "def", "from_pretrained", "(", "cls", ",", "pretrained_model_name", ",", "cache_dir", "=", "None", ",", "*", "inputs", ",", "**", "kwargs", ")", ":", "\n", "        ", "\"\"\"\n        Instantiate a PreTrainedBertModel from a pre-trained model file.\n        Download and cache the pre-trained model file if needed.\n        \"\"\"", "\n", "if", "pretrained_model_name", "in", "PRETRAINED_VOCAB_ARCHIVE_MAP", ":", "\n", "            ", "vocab_file", "=", "PRETRAINED_VOCAB_ARCHIVE_MAP", "[", "pretrained_model_name", "]", "\n", "", "else", ":", "\n", "            ", "vocab_file", "=", "pretrained_model_name", "\n", "", "if", "os", ".", "path", ".", "isdir", "(", "vocab_file", ")", ":", "\n", "            ", "vocab_file", "=", "os", ".", "path", ".", "join", "(", "vocab_file", ",", "VOCAB_NAME", ")", "\n", "# redirect to the cache, if necessary", "\n", "", "try", ":", "\n", "            ", "resolved_vocab_file", "=", "cached_path", "(", "vocab_file", ",", "cache_dir", "=", "cache_dir", ")", "\n", "", "except", "FileNotFoundError", ":", "\n", "            ", "logger", ".", "error", "(", "\n", "\"Model name '{}' was not found in model name list ({}). \"", "\n", "\"We assumed '{}' was a path or url but couldn't find any file \"", "\n", "\"associated to this path or url.\"", ".", "format", "(", "\n", "pretrained_model_name", ",", "\n", "', '", ".", "join", "(", "PRETRAINED_VOCAB_ARCHIVE_MAP", ".", "keys", "(", ")", ")", ",", "\n", "vocab_file", ")", ")", "\n", "return", "None", "\n", "", "if", "resolved_vocab_file", "==", "vocab_file", ":", "\n", "            ", "logger", ".", "info", "(", "\"loading vocabulary file {}\"", ".", "format", "(", "vocab_file", ")", ")", "\n", "", "else", ":", "\n", "            ", "logger", ".", "info", "(", "\"loading vocabulary file {} from cache at {}\"", ".", "format", "(", "\n", "vocab_file", ",", "resolved_vocab_file", ")", ")", "\n", "", "if", "pretrained_model_name", "in", "PRETRAINED_VOCAB_POSITIONAL_EMBEDDINGS_SIZE_MAP", ":", "\n", "# if we're using a pretrained model, ensure the tokenizer wont index sequences longer", "\n", "# than the number of positional embeddings", "\n", "            ", "max_len", "=", "PRETRAINED_VOCAB_POSITIONAL_EMBEDDINGS_SIZE_MAP", "[", "pretrained_model_name", "]", "\n", "kwargs", "[", "'max_len'", "]", "=", "min", "(", "kwargs", ".", "get", "(", "'max_len'", ",", "int", "(", "1e12", ")", ")", ",", "max_len", ")", "\n", "# Instantiate tokenizer.", "\n", "", "tokenizer", "=", "cls", "(", "resolved_vocab_file", ",", "*", "inputs", ",", "**", "kwargs", ")", "\n", "return", "tokenizer", "\n", "\n"]], "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.pytorch_pretrained_bert.tokenization.WhitespaceTokenizer.tokenize": [[174, 176], ["tokenization.whitespace_tokenize"], "methods", ["home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.pytorch_pretrained_bert.tokenization.whitespace_tokenize"], ["    ", "def", "tokenize", "(", "self", ",", "text", ")", ":", "\n", "        ", "return", "whitespace_tokenize", "(", "text", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.pytorch_pretrained_bert.tokenization.BasicTokenizer.__init__": [[181, 189], ["None"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "do_lower_case", "=", "True", ",", "never_split", "=", "(", "\"[UNK]\"", ",", "\"[SEP]\"", ",", "\"[PAD]\"", ",", "\"[CLS]\"", ",", "\"[MASK]\"", ")", ")", ":", "\n", "        ", "\"\"\"Constructs a BasicTokenizer.\n\n        Args:\n          do_lower_case: Whether to lower case the input.\n        \"\"\"", "\n", "self", ".", "do_lower_case", "=", "do_lower_case", "\n", "self", ".", "never_split", "=", "never_split", "\n", "\n"]], "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.pytorch_pretrained_bert.tokenization.BasicTokenizer.tokenize": [[190, 210], ["tokenization.BasicTokenizer._clean_text", "tokenization.BasicTokenizer._tokenize_chinese_chars", "tokenization.whitespace_tokenize", "tokenization.whitespace_tokenize", "split_tokens.extend", "tokenization.BasicTokenizer.lower", "tokenization.BasicTokenizer._run_strip_accents", "tokenization.BasicTokenizer._run_split_on_punc"], "methods", ["home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.pytorch_pretrained_bert.tokenization.BasicTokenizer._clean_text", "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.pytorch_pretrained_bert.tokenization.BasicTokenizer._tokenize_chinese_chars", "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.pytorch_pretrained_bert.tokenization.whitespace_tokenize", "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.pytorch_pretrained_bert.tokenization.whitespace_tokenize", "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.pytorch_pretrained_bert.tokenization.BasicTokenizer._run_strip_accents", "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.pytorch_pretrained_bert.tokenization.BasicTokenizer._run_split_on_punc"], ["", "def", "tokenize", "(", "self", ",", "text", ")", ":", "\n", "        ", "\"\"\"Tokenizes a piece of text.\"\"\"", "\n", "text", "=", "self", ".", "_clean_text", "(", "text", ")", "\n", "# This was added on November 1st, 2018 for the multilingual and Chinese", "\n", "# models. This is also applied to the English models now, but it doesn't", "\n", "# matter since the English models were not trained on any Chinese data", "\n", "# and generally don't have any Chinese data in them (there are Chinese", "\n", "# characters in the vocabulary because Wikipedia does have some Chinese", "\n", "# words in the English Wikipedia.).", "\n", "text", "=", "self", ".", "_tokenize_chinese_chars", "(", "text", ")", "\n", "orig_tokens", "=", "whitespace_tokenize", "(", "text", ")", "\n", "split_tokens", "=", "[", "]", "\n", "for", "token", "in", "orig_tokens", ":", "\n", "            ", "if", "self", ".", "do_lower_case", "and", "token", "not", "in", "self", ".", "never_split", ":", "\n", "                ", "token", "=", "token", ".", "lower", "(", ")", "\n", "token", "=", "self", ".", "_run_strip_accents", "(", "token", ")", "\n", "", "split_tokens", ".", "extend", "(", "self", ".", "_run_split_on_punc", "(", "token", ")", ")", "\n", "\n", "", "output_tokens", "=", "whitespace_tokenize", "(", "\" \"", ".", "join", "(", "split_tokens", ")", ")", "\n", "return", "output_tokens", "\n", "\n"]], "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.pytorch_pretrained_bert.tokenization.BasicTokenizer._run_strip_accents": [[211, 221], ["unicodedata.normalize", "unicodedata.category", "output.append"], "methods", ["None"], ["", "def", "_run_strip_accents", "(", "self", ",", "text", ")", ":", "\n", "        ", "\"\"\"Strips accents from a piece of text.\"\"\"", "\n", "text", "=", "unicodedata", ".", "normalize", "(", "\"NFD\"", ",", "text", ")", "\n", "output", "=", "[", "]", "\n", "for", "char", "in", "text", ":", "\n", "            ", "cat", "=", "unicodedata", ".", "category", "(", "char", ")", "\n", "if", "cat", "==", "\"Mn\"", ":", "\n", "                ", "continue", "\n", "", "output", ".", "append", "(", "char", ")", "\n", "", "return", "\"\"", ".", "join", "(", "output", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.pytorch_pretrained_bert.tokenization.BasicTokenizer._run_split_on_punc": [[222, 243], ["list", "len", "tokenization._is_punctuation", "output.append", "output[].append", "output.append"], "methods", ["home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.pytorch_pretrained_bert.tokenization._is_punctuation"], ["", "def", "_run_split_on_punc", "(", "self", ",", "text", ")", ":", "\n", "        ", "\"\"\"Splits punctuation on a piece of text.\"\"\"", "\n", "if", "text", "in", "self", ".", "never_split", ":", "\n", "            ", "return", "[", "text", "]", "\n", "", "chars", "=", "list", "(", "text", ")", "\n", "i", "=", "0", "\n", "start_new_word", "=", "True", "\n", "output", "=", "[", "]", "\n", "while", "i", "<", "len", "(", "chars", ")", ":", "\n", "            ", "char", "=", "chars", "[", "i", "]", "\n", "if", "_is_punctuation", "(", "char", ")", ":", "\n", "                ", "output", ".", "append", "(", "[", "char", "]", ")", "\n", "start_new_word", "=", "True", "\n", "", "else", ":", "\n", "                ", "if", "start_new_word", ":", "\n", "                    ", "output", ".", "append", "(", "[", "]", ")", "\n", "", "start_new_word", "=", "False", "\n", "output", "[", "-", "1", "]", ".", "append", "(", "char", ")", "\n", "", "i", "+=", "1", "\n", "\n", "", "return", "[", "\"\"", ".", "join", "(", "x", ")", "for", "x", "in", "output", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.pytorch_pretrained_bert.tokenization.BasicTokenizer._tokenize_chinese_chars": [[244, 256], ["ord", "tokenization.BasicTokenizer._is_chinese_char", "output.append", "output.append", "output.append", "output.append"], "methods", ["home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.pytorch_pretrained_bert.tokenization.BasicTokenizer._is_chinese_char"], ["", "def", "_tokenize_chinese_chars", "(", "self", ",", "text", ")", ":", "\n", "        ", "\"\"\"Adds whitespace around any CJK character.\"\"\"", "\n", "output", "=", "[", "]", "\n", "for", "char", "in", "text", ":", "\n", "            ", "cp", "=", "ord", "(", "char", ")", "\n", "if", "self", ".", "_is_chinese_char", "(", "cp", ")", ":", "\n", "                ", "output", ".", "append", "(", "\" \"", ")", "\n", "output", ".", "append", "(", "char", ")", "\n", "output", ".", "append", "(", "\" \"", ")", "\n", "", "else", ":", "\n", "                ", "output", ".", "append", "(", "char", ")", "\n", "", "", "return", "\"\"", ".", "join", "(", "output", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.pytorch_pretrained_bert.tokenization.BasicTokenizer._is_chinese_char": [[257, 278], ["None"], "methods", ["None"], ["", "def", "_is_chinese_char", "(", "self", ",", "cp", ")", ":", "\n", "        ", "\"\"\"Checks whether CP is the codepoint of a CJK character.\"\"\"", "\n", "# This defines a \"chinese character\" as anything in the CJK Unicode block:", "\n", "#   https://en.wikipedia.org/wiki/CJK_Unified_Ideographs_(Unicode_block)", "\n", "#", "\n", "# Note that the CJK Unicode block is NOT all Japanese and Korean characters,", "\n", "# despite its name. The modern Korean Hangul alphabet is a different block,", "\n", "# as is Japanese Hiragana and Katakana. Those alphabets are used to write", "\n", "# space-separated words, so they are not treated specially and handled", "\n", "# like the all of the other languages.", "\n", "if", "(", "(", "cp", ">=", "0x4E00", "and", "cp", "<=", "0x9FFF", ")", "or", "#", "\n", "(", "cp", ">=", "0x3400", "and", "cp", "<=", "0x4DBF", ")", "or", "#", "\n", "(", "cp", ">=", "0x20000", "and", "cp", "<=", "0x2A6DF", ")", "or", "#", "\n", "(", "cp", ">=", "0x2A700", "and", "cp", "<=", "0x2B73F", ")", "or", "#", "\n", "(", "cp", ">=", "0x2B740", "and", "cp", "<=", "0x2B81F", ")", "or", "#", "\n", "(", "cp", ">=", "0x2B820", "and", "cp", "<=", "0x2CEAF", ")", "or", "\n", "(", "cp", ">=", "0xF900", "and", "cp", "<=", "0xFAFF", ")", "or", "#", "\n", "(", "cp", ">=", "0x2F800", "and", "cp", "<=", "0x2FA1F", ")", ")", ":", "#", "\n", "            ", "return", "True", "\n", "\n", "", "return", "False", "\n", "\n"]], "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.pytorch_pretrained_bert.tokenization.BasicTokenizer._clean_text": [[279, 291], ["ord", "tokenization._is_whitespace", "tokenization._is_control", "output.append", "output.append"], "methods", ["home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.pytorch_pretrained_bert.tokenization._is_whitespace", "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.pytorch_pretrained_bert.tokenization._is_control"], ["", "def", "_clean_text", "(", "self", ",", "text", ")", ":", "\n", "        ", "\"\"\"Performs invalid character removal and whitespace cleanup on text.\"\"\"", "\n", "output", "=", "[", "]", "\n", "for", "char", "in", "text", ":", "\n", "            ", "cp", "=", "ord", "(", "char", ")", "\n", "if", "cp", "==", "0", "or", "cp", "==", "0xfffd", "or", "_is_control", "(", "char", ")", ":", "\n", "                ", "continue", "\n", "", "if", "_is_whitespace", "(", "char", ")", ":", "\n", "                ", "output", ".", "append", "(", "\" \"", ")", "\n", "", "else", ":", "\n", "                ", "output", ".", "append", "(", "char", ")", "\n", "", "", "return", "\"\"", ".", "join", "(", "output", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.pytorch_pretrained_bert.tokenization.WordpieceTokenizer.__init__": [[296, 300], ["None"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "vocab", ",", "unk_token", "=", "\"[UNK]\"", ",", "max_input_chars_per_word", "=", "100", ")", ":", "\n", "        ", "self", ".", "vocab", "=", "vocab", "\n", "self", ".", "unk_token", "=", "unk_token", "\n", "self", ".", "max_input_chars_per_word", "=", "max_input_chars_per_word", "\n", "\n"]], "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.pytorch_pretrained_bert.tokenization.WordpieceTokenizer.tokenize": [[301, 351], ["tokenization.whitespace_tokenize", "list", "len", "output_tokens.append", "len", "len", "sub_tokens.append", "output_tokens.append", "output_tokens.extend"], "methods", ["home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.pytorch_pretrained_bert.tokenization.whitespace_tokenize"], ["", "def", "tokenize", "(", "self", ",", "text", ")", ":", "\n", "        ", "\"\"\"Tokenizes a piece of text into its word pieces.\n\n        This uses a greedy longest-match-first algorithm to perform tokenization\n        using the given vocabulary.\n\n        For example:\n          input = \"unaffable\"\n          output = [\"un\", \"##aff\", \"##able\"]\n\n        Args:\n          text: A single token or whitespace separated tokens. This should have\n            already been passed through `BasicTokenizer`.\n\n        Returns:\n          A list of wordpiece tokens.\n        \"\"\"", "\n", "\n", "output_tokens", "=", "[", "]", "\n", "for", "token", "in", "whitespace_tokenize", "(", "text", ")", ":", "\n", "            ", "chars", "=", "list", "(", "token", ")", "\n", "if", "len", "(", "chars", ")", ">", "self", ".", "max_input_chars_per_word", ":", "\n", "                ", "output_tokens", ".", "append", "(", "self", ".", "unk_token", ")", "\n", "continue", "\n", "\n", "", "is_bad", "=", "False", "\n", "start", "=", "0", "\n", "sub_tokens", "=", "[", "]", "\n", "while", "start", "<", "len", "(", "chars", ")", ":", "\n", "                ", "end", "=", "len", "(", "chars", ")", "\n", "cur_substr", "=", "None", "\n", "while", "start", "<", "end", ":", "\n", "                    ", "substr", "=", "\"\"", ".", "join", "(", "chars", "[", "start", ":", "end", "]", ")", "\n", "if", "start", ">", "0", ":", "\n", "                        ", "substr", "=", "\"##\"", "+", "substr", "\n", "", "if", "substr", "in", "self", ".", "vocab", ":", "\n", "                        ", "cur_substr", "=", "substr", "\n", "break", "\n", "", "end", "-=", "1", "\n", "", "if", "cur_substr", "is", "None", ":", "\n", "                    ", "is_bad", "=", "True", "\n", "break", "\n", "", "sub_tokens", ".", "append", "(", "cur_substr", ")", "\n", "start", "=", "end", "\n", "\n", "", "if", "is_bad", ":", "\n", "                ", "output_tokens", ".", "append", "(", "self", ".", "unk_token", ")", "\n", "", "else", ":", "\n", "                ", "output_tokens", ".", "extend", "(", "sub_tokens", ")", "\n", "", "", "return", "output_tokens", "\n", "\n"]], "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.pytorch_pretrained_bert.tokenization.load_vocab": [[51, 79], ["range", "collections.OrderedDict", "open", "reader.readline", "token.strip.strip"], "function", ["None"], ["def", "load_vocab", "(", "vocab_file", ")", ":", "\n", "    ", "\"\"\"Loads a vocabulary file into a dictionary.\"\"\"", "\n", "# mapping unused tokens to special tokens", "\n", "extra_map", "=", "{", "}", "\n", "extra_map", "[", "'[unused1]'", "]", "=", "'[X_SEP]'", "\n", "for", "i", "in", "range", "(", "10", ")", ":", "\n", "        ", "extra_map", "[", "'[unused{}]'", ".", "format", "(", "i", "+", "2", ")", "]", "=", "'[SEP_{}]'", ".", "format", "(", "i", ")", "\n", "", "extra_map", "[", "'[unused12]'", "]", "=", "'[S2S_SEP]'", "\n", "extra_map", "[", "'[unused13]'", "]", "=", "'[S2S_CLS]'", "\n", "extra_map", "[", "'[unused14]'", "]", "=", "'[L2R_SEP]'", "\n", "extra_map", "[", "'[unused15]'", "]", "=", "'[L2R_CLS]'", "\n", "extra_map", "[", "'[unused16]'", "]", "=", "'[R2L_SEP]'", "\n", "extra_map", "[", "'[unused17]'", "]", "=", "'[R2L_CLS]'", "\n", "extra_map", "[", "'[unused18]'", "]", "=", "'[S2S_SOS]'", "\n", "\n", "vocab", "=", "collections", ".", "OrderedDict", "(", ")", "\n", "index", "=", "0", "\n", "with", "open", "(", "vocab_file", ",", "\"r\"", ",", "encoding", "=", "\"utf-8\"", ")", "as", "reader", ":", "\n", "        ", "while", "True", ":", "\n", "            ", "token", "=", "reader", ".", "readline", "(", ")", "\n", "if", "not", "token", ":", "\n", "                ", "break", "\n", "", "token", "=", "token", ".", "strip", "(", ")", "\n", "if", "token", "in", "extra_map", ":", "\n", "                ", "token", "=", "extra_map", "[", "token", "]", "\n", "", "vocab", "[", "token", "]", "=", "index", "\n", "index", "+=", "1", "\n", "", "", "return", "vocab", "\n", "\n"]], "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.pytorch_pretrained_bert.tokenization.whitespace_tokenize": [[81, 88], ["text.strip.strip", "text.strip.split"], "function", ["home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.preprocess.process_daren.split"], ["", "def", "whitespace_tokenize", "(", "text", ")", ":", "\n", "    ", "\"\"\"Runs basic whitespace cleaning and splitting on a peice of text.\"\"\"", "\n", "text", "=", "text", ".", "strip", "(", ")", "\n", "if", "not", "text", ":", "\n", "        ", "return", "[", "]", "\n", "", "tokens", "=", "text", ".", "split", "(", ")", "\n", "return", "tokens", "\n", "\n"]], "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.pytorch_pretrained_bert.tokenization._is_whitespace": [[353, 363], ["unicodedata.category"], "function", ["None"], ["", "", "def", "_is_whitespace", "(", "char", ")", ":", "\n", "    ", "\"\"\"Checks whether `chars` is a whitespace character.\"\"\"", "\n", "# \\t, \\n, and \\r are technically contorl characters but we treat them", "\n", "# as whitespace since they are generally considered as such.", "\n", "if", "char", "==", "\" \"", "or", "char", "==", "\"\\t\"", "or", "char", "==", "\"\\n\"", "or", "char", "==", "\"\\r\"", ":", "\n", "        ", "return", "True", "\n", "", "cat", "=", "unicodedata", ".", "category", "(", "char", ")", "\n", "if", "cat", "==", "\"Zs\"", ":", "\n", "        ", "return", "True", "\n", "", "return", "False", "\n", "\n"]], "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.pytorch_pretrained_bert.tokenization._is_control": [[365, 375], ["unicodedata.category", "unicodedata.category.startswith"], "function", ["None"], ["", "def", "_is_control", "(", "char", ")", ":", "\n", "    ", "\"\"\"Checks whether `chars` is a control character.\"\"\"", "\n", "# These are technically control characters but we count them as whitespace", "\n", "# characters.", "\n", "if", "char", "==", "\"\\t\"", "or", "char", "==", "\"\\n\"", "or", "char", "==", "\"\\r\"", ":", "\n", "        ", "return", "False", "\n", "", "cat", "=", "unicodedata", ".", "category", "(", "char", ")", "\n", "if", "cat", ".", "startswith", "(", "\"C\"", ")", ":", "\n", "        ", "return", "True", "\n", "", "return", "False", "\n", "\n"]], "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.pytorch_pretrained_bert.tokenization._is_punctuation": [[377, 391], ["ord", "unicodedata.category", "unicodedata.category.startswith"], "function", ["None"], ["", "def", "_is_punctuation", "(", "char", ")", ":", "\n", "    ", "\"\"\"Checks whether `chars` is a punctuation character.\"\"\"", "\n", "cp", "=", "ord", "(", "char", ")", "\n", "# We treat all non-letter/number ASCII as punctuation.", "\n", "# Characters such as \"^\", \"$\", and \"`\" are not in the Unicode", "\n", "# Punctuation class but we treat them as punctuation anyways, for", "\n", "# consistency.", "\n", "if", "(", "(", "cp", ">=", "33", "and", "cp", "<=", "47", ")", "or", "(", "cp", ">=", "58", "and", "cp", "<=", "64", ")", "or", "\n", "(", "cp", ">=", "91", "and", "cp", "<=", "96", ")", "or", "(", "cp", ">=", "123", "and", "cp", "<=", "126", ")", ")", ":", "\n", "        ", "return", "True", "\n", "", "cat", "=", "unicodedata", ".", "category", "(", "char", ")", "\n", "if", "cat", ".", "startswith", "(", "\"P\"", ")", ":", "\n", "        ", "return", "True", "\n", "", "return", "False", "\n", "", ""]], "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.pytorch_pretrained_bert.loss.LabelSmoothingLoss.__init__": [[19, 34], ["torch.nn.modules.loss._Loss.__init__", "torch.full", "torch.full", "torch.full", "torch.full", "loss.LabelSmoothingLoss.register_buffer", "torch.full.unsqueeze", "torch.full.unsqueeze"], "methods", ["home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.unilm.eval.Eval.__init__"], ["def", "__init__", "(", "self", ",", "label_smoothing", "=", "0", ",", "tgt_vocab_size", "=", "0", ",", "ignore_index", "=", "0", ",", "size_average", "=", "None", ",", "reduce", "=", "None", ",", "reduction", "=", "'mean'", ")", ":", "\n", "        ", "assert", "0.0", "<", "label_smoothing", "<=", "1.0", "\n", "self", ".", "ignore_index", "=", "ignore_index", "\n", "super", "(", "LabelSmoothingLoss", ",", "self", ")", ".", "__init__", "(", "\n", "size_average", "=", "size_average", ",", "reduce", "=", "reduce", ",", "reduction", "=", "reduction", ")", "\n", "\n", "assert", "label_smoothing", ">", "0", "\n", "assert", "tgt_vocab_size", ">", "0", "\n", "\n", "smoothing_value", "=", "label_smoothing", "/", "(", "tgt_vocab_size", "-", "2", ")", "\n", "one_hot", "=", "torch", ".", "full", "(", "(", "tgt_vocab_size", ",", ")", ",", "smoothing_value", ")", "\n", "one_hot", "[", "self", ".", "ignore_index", "]", "=", "0", "\n", "self", ".", "register_buffer", "(", "'one_hot'", ",", "one_hot", ".", "unsqueeze", "(", "0", ")", ")", "\n", "self", ".", "confidence", "=", "1.0", "-", "label_smoothing", "\n", "self", ".", "tgt_vocab_size", "=", "tgt_vocab_size", "\n", "\n"]], "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.pytorch_pretrained_bert.loss.LabelSmoothingLoss.forward": [[35, 49], ["output.view.view.view", "target.view.view.view", "loss.LabelSmoothingLoss.one_hot.repeat", "loss.LabelSmoothingLoss.scatter_", "loss.LabelSmoothingLoss.masked_fill_", "torch.kl_div().view().sum", "torch.kl_div().view().sum", "output.view.view.size", "target.view.view.size", "target.view.view.size", "target.view.view.size", "target.view.view.unsqueeze", "torch.kl_div().view", "torch.kl_div().view", "torch.kl_div", "torch.kl_div"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "output", ",", "target", ")", ":", "\n", "        ", "\"\"\"\n        output (FloatTensor): batch_size * num_pos * n_classes\n        target (LongTensor): batch_size * num_pos\n        \"\"\"", "\n", "assert", "self", ".", "tgt_vocab_size", "==", "output", ".", "size", "(", "2", ")", "\n", "batch_size", ",", "num_pos", "=", "target", ".", "size", "(", "0", ")", ",", "target", ".", "size", "(", "1", ")", "\n", "output", "=", "output", ".", "view", "(", "-", "1", ",", "self", ".", "tgt_vocab_size", ")", "\n", "target", "=", "target", ".", "view", "(", "-", "1", ")", "\n", "model_prob", "=", "self", ".", "one_hot", ".", "repeat", "(", "target", ".", "size", "(", "0", ")", ",", "1", ")", "\n", "model_prob", ".", "scatter_", "(", "1", ",", "target", ".", "unsqueeze", "(", "1", ")", ",", "self", ".", "confidence", ")", "\n", "model_prob", ".", "masked_fill_", "(", "(", "target", "==", "self", ".", "ignore_index", ")", ".", "unsqueeze", "(", "1", ")", ",", "0", ")", "\n", "\n", "return", "F", ".", "kl_div", "(", "output", ",", "model_prob", ",", "reduction", "=", "'none'", ")", ".", "view", "(", "batch_size", ",", "num_pos", ",", "-", "1", ")", ".", "sum", "(", "2", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.pytorch_pretrained_bert.__main__.main": [[2, 20], ["len", "print", "sys.argv.pop", "sys.argv.pop", "sys.argv.pop", "convert_tf_checkpoint_to_pytorch", "print"], "function", ["None"], ["def", "main", "(", ")", ":", "\n", "    ", "import", "sys", "\n", "try", ":", "\n", "        ", "from", ".", "convert_tf_checkpoint_to_pytorch", "import", "convert_tf_checkpoint_to_pytorch", "\n", "", "except", "ModuleNotFoundError", ":", "\n", "        ", "print", "(", "\"pytorch_pretrained_bert can only be used from the commandline to convert TensorFlow models in PyTorch, \"", "\n", "\"In that case, it requires TensorFlow to be installed. Please see \"", "\n", "\"https://www.tensorflow.org/install/ for installation instructions.\"", ")", "\n", "raise", "\n", "\n", "", "if", "len", "(", "sys", ".", "argv", ")", "!=", "5", ":", "\n", "# pylint: disable=line-too-long", "\n", "        ", "print", "(", "\"Should be used as `pytorch_pretrained_bert convert_tf_checkpoint_to_pytorch TF_CHECKPOINT TF_CONFIG PYTORCH_DUMP_OUTPUT`\"", ")", "\n", "", "else", ":", "\n", "        ", "PYTORCH_DUMP_OUTPUT", "=", "sys", ".", "argv", ".", "pop", "(", ")", "\n", "TF_CONFIG", "=", "sys", ".", "argv", ".", "pop", "(", ")", "\n", "TF_CHECKPOINT", "=", "sys", ".", "argv", ".", "pop", "(", ")", "\n", "convert_tf_checkpoint_to_pytorch", "(", "TF_CHECKPOINT", ",", "TF_CONFIG", ",", "PYTORCH_DUMP_OUTPUT", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.pytorch_pretrained_bert.optimization_fp16.FP16_Optimizer_State.__init__": [[8, 16], ["apex.contrib.optimizers.fp16_optimizer.FP16_Optimizer.__init__"], "methods", ["home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.unilm.eval.Eval.__init__"], ["    ", "def", "__init__", "(", "self", ",", "\n", "init_optimizer", ",", "\n", "static_loss_scale", "=", "1.0", ",", "\n", "dynamic_loss_scale", "=", "False", ",", "\n", "dynamic_loss_args", "=", "None", ",", "\n", "verbose", "=", "True", ")", ":", "\n", "        ", "super", "(", "FP16_Optimizer_State", ",", "self", ")", ".", "__init__", "(", "init_optimizer", ",", "\n", "static_loss_scale", ",", "dynamic_loss_scale", ",", "dynamic_loss_args", ",", "verbose", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.pytorch_pretrained_bert.optimization_fp16.FP16_Optimizer_State.state_dict": [[17, 39], ["optimization_fp16.FP16_Optimizer_State.optimizer.state_dict"], "methods", ["home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.pytorch_pretrained_bert.optimization_fp16.FP16_Optimizer_State.state_dict"], ["", "def", "state_dict", "(", "self", ")", ":", "\n", "        ", "\"\"\"\n        Returns a dict containing the current state of this :class:`FP16_Optimizer` instance.\n        This dict contains attributes of :class:`FP16_Optimizer`, as well as the state_dict\n        of the contained Pytorch optimizer.\n        Example::\n            checkpoint = {}\n            checkpoint['model'] = model.state_dict()\n            checkpoint['optimizer'] = optimizer.state_dict()\n            torch.save(checkpoint, \"saved.pth\")\n        \"\"\"", "\n", "state_dict", "=", "{", "}", "\n", "state_dict", "[", "'dynamic_loss_scale'", "]", "=", "self", ".", "dynamic_loss_scale", "\n", "state_dict", "[", "'cur_scale'", "]", "=", "self", ".", "cur_scale", "\n", "state_dict", "[", "'cur_iter'", "]", "=", "self", ".", "cur_iter", "\n", "if", "state_dict", "[", "'dynamic_loss_scale'", "]", ":", "\n", "            ", "state_dict", "[", "'last_overflow_iter'", "]", "=", "self", ".", "last_overflow_iter", "\n", "state_dict", "[", "'scale_factor'", "]", "=", "self", ".", "scale_factor", "\n", "state_dict", "[", "'scale_window'", "]", "=", "self", ".", "scale_window", "\n", "", "state_dict", "[", "'optimizer_state_dict'", "]", "=", "self", ".", "optimizer", ".", "state_dict", "(", ")", "\n", "state_dict", "[", "'fp32_groups_flat'", "]", "=", "self", ".", "fp32_groups_flat", "\n", "return", "state_dict", "\n", "\n"]], "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.pytorch_pretrained_bert.optimization_fp16.FP16_Optimizer_State.load_state_dict": [[40, 81], ["optimization_fp16.FP16_Optimizer_State.optimizer.load_state_dict", "zip", "current.data.copy_"], "methods", ["home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.pytorch_pretrained_bert.optimization_fp16.FP16_Optimizer_State.load_state_dict"], ["", "def", "load_state_dict", "(", "self", ",", "state_dict", ")", ":", "\n", "        ", "\"\"\"\n        Loads a state_dict created by an earlier call to state_dict(). \n        If ``fp16_optimizer_instance`` was constructed from some ``init_optimizer``, \n        whose parameters in turn came from ``model``, it is expected that the user \n        will call ``model.load_state_dict()`` before\n        ``fp16_optimizer_instance.load_state_dict()`` is called.\n        Example::\n            model = torch.nn.Linear(D_in, D_out).cuda().half()\n            optimizer = torch.optim.SGD(model.parameters(), lr=1e-3)\n            optimizer = FP16_Optimizer(optimizer, static_loss_scale = 128.0)\n            ...\n            checkpoint = torch.load(\"saved.pth\")\n            model.load_state_dict(checkpoint['model'])\n            optimizer.load_state_dict(checkpoint['optimizer'])\n        \"\"\"", "\n", "# I think it should actually be ok to reload the optimizer before the model.", "\n", "self", ".", "dynamic_loss_scale", "=", "state_dict", "[", "'dynamic_loss_scale'", "]", "\n", "self", ".", "cur_scale", "=", "state_dict", "[", "'cur_scale'", "]", "\n", "self", ".", "cur_iter", "=", "state_dict", "[", "'cur_iter'", "]", "\n", "if", "state_dict", "[", "'dynamic_loss_scale'", "]", ":", "\n", "            ", "self", ".", "last_overflow_iter", "=", "state_dict", "[", "'last_overflow_iter'", "]", "\n", "self", ".", "scale_factor", "=", "state_dict", "[", "'scale_factor'", "]", "\n", "self", ".", "scale_window", "=", "state_dict", "[", "'scale_window'", "]", "\n", "", "self", ".", "optimizer", ".", "load_state_dict", "(", "state_dict", "[", "'optimizer_state_dict'", "]", ")", "\n", "# At this point, the optimizer's references to the model's fp32 parameters are up to date.", "\n", "# The optimizer's hyperparameters and internal buffers are also up to date.", "\n", "# However, the fp32 master copies of the model's fp16 params stored by the optimizer are still", "\n", "# out of date.  There are two options.", "\n", "# 1:  Refresh the master params from the model's fp16 params.", "\n", "# This requires less storage but incurs precision loss.", "\n", "# 2:  Save and restore the fp32 master copies separately.", "\n", "# We choose option 2.", "\n", "#", "\n", "# Pytorch Optimizer.load_state_dict casts saved buffers (e.g. momentum) to the type and device", "\n", "# of their associated parameters, because it's possible those buffers might not exist yet in", "\n", "# the current optimizer instance.  In our case, as long as the current FP16_Optimizer has been", "\n", "# constructed in the same way as the one whose state_dict we are loading, the same master params", "\n", "# are guaranteed to exist, so we can just copy_() from the saved master params.", "\n", "for", "current", ",", "saved", "in", "zip", "(", "self", ".", "fp32_groups_flat", ",", "state_dict", "[", "'fp32_groups_flat'", "]", ")", ":", "\n", "            ", "current", ".", "data", ".", "copy_", "(", "saved", ".", "data", ")", "\n", "", "", "", ""]], "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.nn.data_parallel.DataParallelImbalance.__init__": [[74, 101], ["torch.nn.DataParallel.__init__", "list", "torch.cuda._utils._get_device_index", "torch.cuda.is_available", "list", "all", "RuntimeError", "map", "len", "data_parallel.DataParallelImbalance.module.cuda", "range", "torch.cuda.device_count", "torch.cuda._utils._get_device_index", "itertools.chain", "module.parameters", "module.buffers"], "methods", ["home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.unilm.eval.Eval.__init__"], ["    ", "def", "__init__", "(", "self", ",", "module", ",", "device_ids", "=", "None", ",", "output_device", "=", "None", ",", "dim", "=", "0", ")", ":", "\n", "        ", "super", "(", "DataParallelImbalance", ",", "self", ")", ".", "__init__", "(", "\n", "module", ",", "device_ids", ",", "output_device", ",", "dim", ")", "\n", "\n", "if", "not", "torch", ".", "cuda", ".", "is_available", "(", ")", ":", "\n", "            ", "self", ".", "module", "=", "module", "\n", "self", ".", "device_ids", "=", "[", "]", "\n", "return", "\n", "\n", "", "if", "device_ids", "is", "None", ":", "\n", "            ", "device_ids", "=", "list", "(", "range", "(", "torch", ".", "cuda", ".", "device_count", "(", ")", ")", ")", "\n", "", "if", "output_device", "is", "None", ":", "\n", "            ", "output_device", "=", "device_ids", "[", "0", "]", "\n", "\n", "", "if", "not", "all", "(", "t", ".", "is_cuda", "and", "t", ".", "device", ".", "index", "==", "device_ids", "[", "0", "]", "\n", "for", "t", "in", "chain", "(", "module", ".", "parameters", "(", ")", ",", "module", ".", "buffers", "(", ")", ")", ")", ":", "\n", "            ", "raise", "RuntimeError", "(", "\"module must have its parameters and buffers \"", "\n", "\"on device %d (device_ids[0])\"", "%", "device_ids", "[", "0", "]", ")", "\n", "\n", "", "self", ".", "dim", "=", "dim", "\n", "self", ".", "module", "=", "module", "\n", "self", ".", "device_ids", "=", "list", "(", "\n", "map", "(", "lambda", "x", ":", "_get_device_index", "(", "x", ",", "True", ")", ",", "device_ids", ")", ")", "\n", "self", ".", "output_device", "=", "_get_device_index", "(", "output_device", ",", "True", ")", "\n", "\n", "if", "len", "(", "self", ".", "device_ids", ")", "==", "1", ":", "\n", "            ", "self", ".", "module", ".", "cuda", "(", "device_ids", "[", "0", "]", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.nn.data_parallel.DataParallelImbalance.forward": [[102, 112], ["data_parallel.DataParallelImbalance.scatter_imbalance", "data_parallel.DataParallelImbalance.replicate", "data_parallel.DataParallelImbalance.parallel_apply", "data_parallel.DataParallelImbalance.gather", "data_parallel.DataParallelImbalance.module", "len", "data_parallel.DataParallelImbalance.module", "len"], "methods", ["home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.nn.data_parallel.scatter_imbalance"], ["", "", "def", "forward", "(", "self", ",", "*", "inputs", ",", "**", "kwargs", ")", ":", "\n", "        ", "if", "not", "self", ".", "device_ids", ":", "\n", "            ", "return", "self", ".", "module", "(", "*", "inputs", ",", "**", "kwargs", ")", "\n", "", "inputs", ",", "kwargs", "=", "self", ".", "scatter_imbalance", "(", "\n", "inputs", ",", "kwargs", ",", "self", ".", "device_ids", ")", "\n", "if", "len", "(", "self", ".", "device_ids", ")", "==", "1", ":", "\n", "            ", "return", "self", ".", "module", "(", "*", "inputs", "[", "0", "]", ",", "**", "kwargs", "[", "0", "]", ")", "\n", "", "replicas", "=", "self", ".", "replicate", "(", "self", ".", "module", ",", "self", ".", "device_ids", "[", ":", "len", "(", "inputs", ")", "]", ")", "\n", "outputs", "=", "self", ".", "parallel_apply", "(", "replicas", ",", "inputs", ",", "kwargs", ")", "\n", "return", "self", ".", "gather", "(", "outputs", ",", "self", ".", "output_device", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.nn.data_parallel.DataParallelImbalance.scatter_imbalance": [[113, 115], ["data_parallel.scatter_kwargs_imbalance"], "methods", ["home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.nn.data_parallel.scatter_kwargs_imbalance"], ["", "def", "scatter_imbalance", "(", "self", ",", "inputs", ",", "kwargs", ",", "device_ids", ")", ":", "\n", "        ", "return", "scatter_kwargs_imbalance", "(", "inputs", ",", "kwargs", ",", "device_ids", ",", "dim", "=", "self", ".", "dim", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.nn.data_parallel.scatter_imbalance": [[8, 58], ["isinstance", "data_parallel.scatter_imbalance.scatter_map"], "function", ["None"], ["def", "scatter_imbalance", "(", "inputs", ",", "target_gpus", ",", "dim", "=", "0", ")", ":", "\n", "    ", "r\"\"\"\n    Slices tensors into approximately equal chunks and\n    distributes them across given GPUs. Duplicates\n    references to objects that are not tensors.\n    \"\"\"", "\n", "def", "scatter_map", "(", "obj", ")", ":", "\n", "        ", "if", "isinstance", "(", "obj", ",", "torch", ".", "Tensor", ")", ":", "\n", "            ", "if", "(", "len", "(", "target_gpus", ")", "==", "4", ")", "and", "(", "obj", ".", "size", "(", "dim", ")", "==", "22", ")", ":", "\n", "                ", "return", "Scatter", ".", "apply", "(", "target_gpus", ",", "(", "4", ",", "6", ",", "6", ",", "6", ")", ",", "dim", ",", "obj", ")", "\n", "", "if", "(", "len", "(", "target_gpus", ")", "==", "4", ")", "and", "(", "obj", ".", "size", "(", "dim", ")", "==", "60", ")", ":", "\n", "                ", "return", "Scatter", ".", "apply", "(", "target_gpus", ",", "(", "12", ",", "16", ",", "16", ",", "16", ")", ",", "dim", ",", "obj", ")", "\n", "", "elif", "(", "len", "(", "target_gpus", ")", "==", "4", ")", "and", "(", "obj", ".", "size", "(", "dim", ")", "==", "144", ")", ":", "\n", "                ", "return", "Scatter", ".", "apply", "(", "target_gpus", ",", "(", "24", ",", "40", ",", "40", ",", "40", ")", ",", "dim", ",", "obj", ")", "\n", "", "elif", "(", "len", "(", "target_gpus", ")", "==", "8", ")", "and", "(", "obj", ".", "size", "(", "dim", ")", "==", "46", ")", ":", "\n", "                ", "return", "Scatter", ".", "apply", "(", "target_gpus", ",", "(", "4", ",", "6", ",", "6", ",", "6", ",", "6", ",", "6", ",", "6", ",", "6", ")", ",", "dim", ",", "obj", ")", "\n", "", "elif", "(", "len", "(", "target_gpus", ")", "==", "8", ")", "and", "(", "obj", ".", "size", "(", "dim", ")", "==", "62", ")", ":", "\n", "                ", "return", "Scatter", ".", "apply", "(", "target_gpus", ",", "(", "6", ",", "8", ",", "8", ",", "8", ",", "8", ",", "8", ",", "8", ",", "8", ")", ",", "dim", ",", "obj", ")", "\n", "", "elif", "(", "len", "(", "target_gpus", ")", "==", "8", ")", "and", "(", "obj", ".", "size", "(", "dim", ")", "==", "94", ")", ":", "\n", "                ", "return", "Scatter", ".", "apply", "(", "target_gpus", ",", "(", "10", ",", "12", ",", "12", ",", "12", ",", "12", ",", "12", ",", "12", ",", "12", ")", ",", "dim", ",", "obj", ")", "\n", "", "elif", "(", "len", "(", "target_gpus", ")", "==", "8", ")", "and", "(", "obj", ".", "size", "(", "dim", ")", "==", "110", ")", ":", "\n", "                ", "return", "Scatter", ".", "apply", "(", "target_gpus", ",", "(", "12", ",", "14", ",", "14", ",", "14", ",", "14", ",", "14", ",", "14", ",", "14", ")", ",", "dim", ",", "obj", ")", "\n", "", "elif", "(", "len", "(", "target_gpus", ")", "==", "8", ")", "and", "(", "obj", ".", "size", "(", "dim", ")", "==", "118", ")", ":", "\n", "                ", "return", "Scatter", ".", "apply", "(", "target_gpus", ",", "(", "13", ",", "15", ",", "15", ",", "15", ",", "15", ",", "15", ",", "15", ",", "15", ")", ",", "dim", ",", "obj", ")", "\n", "", "elif", "(", "len", "(", "target_gpus", ")", "==", "8", ")", "and", "(", "obj", ".", "size", "(", "dim", ")", "==", "126", ")", ":", "\n", "                ", "return", "Scatter", ".", "apply", "(", "target_gpus", ",", "(", "14", ",", "16", ",", "16", ",", "16", ",", "16", ",", "16", ",", "16", ",", "16", ")", ",", "dim", ",", "obj", ")", "\n", "", "elif", "(", "len", "(", "target_gpus", ")", "==", "8", ")", "and", "(", "obj", ".", "size", "(", "dim", ")", "==", "134", ")", ":", "\n", "                ", "return", "Scatter", ".", "apply", "(", "target_gpus", ",", "(", "15", ",", "17", ",", "17", ",", "17", ",", "17", ",", "17", ",", "17", ",", "17", ")", ",", "dim", ",", "obj", ")", "\n", "", "elif", "(", "len", "(", "target_gpus", ")", "==", "8", ")", "and", "(", "obj", ".", "size", "(", "dim", ")", "==", "142", ")", ":", "\n", "                ", "return", "Scatter", ".", "apply", "(", "target_gpus", ",", "(", "16", ",", "18", ",", "18", ",", "18", ",", "18", ",", "18", ",", "18", ",", "18", ")", ",", "dim", ",", "obj", ")", "\n", "", "elif", "(", "len", "(", "target_gpus", ")", "==", "16", ")", "and", "(", "obj", ".", "size", "(", "dim", ")", "==", "222", ")", ":", "\n", "                ", "return", "Scatter", ".", "apply", "(", "target_gpus", ",", "(", "12", ",", "14", ",", "14", ",", "14", ",", "14", ",", "14", ",", "14", ",", "14", ",", "14", ",", "14", ",", "14", ",", "14", ",", "14", ",", "14", ",", "14", ",", "14", ")", ",", "dim", ",", "obj", ")", "\n", "", "return", "Scatter", ".", "apply", "(", "target_gpus", ",", "None", ",", "dim", ",", "obj", ")", "\n", "", "if", "isinstance", "(", "obj", ",", "tuple", ")", "and", "len", "(", "obj", ")", ">", "0", ":", "\n", "            ", "return", "list", "(", "zip", "(", "*", "map", "(", "scatter_map", ",", "obj", ")", ")", ")", "\n", "", "if", "isinstance", "(", "obj", ",", "list", ")", "and", "len", "(", "obj", ")", ">", "0", ":", "\n", "            ", "return", "list", "(", "map", "(", "list", ",", "zip", "(", "*", "map", "(", "scatter_map", ",", "obj", ")", ")", ")", ")", "\n", "", "if", "isinstance", "(", "obj", ",", "dict", ")", "and", "len", "(", "obj", ")", ">", "0", ":", "\n", "            ", "return", "list", "(", "map", "(", "type", "(", "obj", ")", ",", "zip", "(", "*", "map", "(", "scatter_map", ",", "obj", ".", "items", "(", ")", ")", ")", ")", ")", "\n", "", "return", "[", "obj", "for", "targets", "in", "target_gpus", "]", "\n", "\n", "# After scatter_map is called, a scatter_map cell will exist. This cell", "\n", "# has a reference to the actual function scatter_map, which has references", "\n", "# to a closure that has a reference to the scatter_map cell (because the", "\n", "# fn is recursive). To avoid this reference cycle, we set the function to", "\n", "# None, clearing the cell", "\n", "", "try", ":", "\n", "        ", "return", "scatter_map", "(", "inputs", ")", "\n", "", "finally", ":", "\n", "        ", "scatter_map", "=", "None", "\n", "\n"]], "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.nn.data_parallel.scatter_kwargs_imbalance": [[60, 71], ["tuple", "tuple", "data_parallel.scatter_imbalance", "data_parallel.scatter_imbalance", "len", "len", "tuple.extend", "len", "len", "tuple.extend", "range", "range", "len", "len", "len", "len"], "function", ["home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.nn.data_parallel.scatter_imbalance", "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.nn.data_parallel.scatter_imbalance"], ["", "", "def", "scatter_kwargs_imbalance", "(", "inputs", ",", "kwargs", ",", "target_gpus", ",", "dim", "=", "0", ")", ":", "\n", "    ", "r\"\"\"Scatter with support for kwargs dictionary\"\"\"", "\n", "inputs", "=", "scatter_imbalance", "(", "inputs", ",", "target_gpus", ",", "dim", ")", "if", "inputs", "else", "[", "]", "\n", "kwargs", "=", "scatter_imbalance", "(", "kwargs", ",", "target_gpus", ",", "dim", ")", "if", "kwargs", "else", "[", "]", "\n", "if", "len", "(", "inputs", ")", "<", "len", "(", "kwargs", ")", ":", "\n", "        ", "inputs", ".", "extend", "(", "[", "(", ")", "for", "_", "in", "range", "(", "len", "(", "kwargs", ")", "-", "len", "(", "inputs", ")", ")", "]", ")", "\n", "", "elif", "len", "(", "kwargs", ")", "<", "len", "(", "inputs", ")", ":", "\n", "        ", "kwargs", ".", "extend", "(", "[", "{", "}", "for", "_", "in", "range", "(", "len", "(", "inputs", ")", "-", "len", "(", "kwargs", ")", ")", "]", ")", "\n", "", "inputs", "=", "tuple", "(", "inputs", ")", "\n", "kwargs", "=", "tuple", "(", "kwargs", ")", "\n", "return", "inputs", ",", "kwargs", "\n", "\n"]], "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.cnndm.eval.test_rouge": [[63, 98], ["tempfile.mkdtemp", "len", "time.strftime", "os.path.join", "len", "len", "time.localtime", "os.path.isdir", "os.mkdir", "os.mkdir", "os.mkdir", "range", "cnndm.bs_pyrouge.Rouge155", "cnndm.bs_pyrouge.Rouge155.convert_and_evaluate", "print", "cnndm.bs_pyrouge.Rouge155.output_to_dict", "os.path.isdir", "shutil.rmtree", "len", "open", "f.write", "open", "f.write"], "function", ["None"], []], "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.cnndm.eval.rouge_results_to_str": [[100, 108], ["None"], "function", ["None"], []], "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.cnndm.eval.count_tokens": [[111, 119], ["counter.keys"], "function", ["None"], []], "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.cnndm.eval.get_f1": [[121, 135], ["text_a.lower().split", "text_b.lower().split", "eval.count_tokens", "eval.count_tokens", "count_tokens.keys", "len", "len", "text_a.lower", "text_b.lower", "len", "len", "count_tokens.keys", "min", "len", "len"], "function", ["home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.preprocess.process_daren.split", "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.preprocess.process_daren.split", "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.gigaword.eval.count_tokens", "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.gigaword.eval.count_tokens"], []], "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.cnndm.eval._is_digit": [[142, 147], ["ch.isdigit"], "function", ["None"], []], "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.cnndm.eval.fix_tokenization": [[149, 227], ["text.split", "len", "_tok_dict.keys", "output_tokens.append", "output_tokens.append", "output_tokens.append", "output_tokens[].endswith", "output_tokens.append", "len", "output_tokens.append", "len", "len", "output_tokens.append", "output_tokens.append", "output_tokens.append", "eval._is_digit", "eval._is_digit", "len", "len", "output_tokens[].isdigit", "input_tokens[].isdigit", "len", "len", "output_tokens[].isupper", "input_tokens[].isupper", "len", "len", "len", "len", "len", "len", "input_tokens[].isupper", "output_tokens.append", "output_tokens.append", "len", "output_tokens.append", "len", "len", "output_tokens.append", "len"], "function", ["home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.preprocess.process_daren.split", "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.gigaword.eval._is_digit", "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.gigaword.eval._is_digit"], []], "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.cnndm.eval.remove_duplicate": [[229, 239], ["set", "enumerate", "l.lower().split", "set", "r_list.append", "l.lower", "len", "len"], "function", ["home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.preprocess.process_daren.split"], []], "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.cnndm.eval.process_eval": [[241, 290], ["open", "open", "open", "len", "len", "len", "len", "eval.test_rouge", "evaluator.get_scores", "l.strip().replace", "gold_list.append", "l.strip().split", "pred_list.append", "f_out.write", "f_out.write", "eval.fix_tokenization", "any", "len", "remove_duplicate.append", "eval.remove_duplicate", "l.replace().strip", "len", "l.strip", "l.strip", "fix_tokenization.split", "bit.split", "min", "trunc_list.append", "len", "l.replace", "eval.get_f1"], "function", ["home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.gigaword.eval.test_rouge", "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.preprocess.process_daren.split", "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.gigaword.eval.fix_tokenization", "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.cnndm.eval.remove_duplicate", "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.preprocess.process_daren.split", "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.preprocess.process_daren.split", "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.gigaword.eval.get_f1"], []], "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.cnndm.eval.main": [[292, 359], ["list", "logger.info", "min", "multiprocessing.Pool", "multiprocessing.Pool.imap_unordered", "sorted", "multiprocessing.Pool.close", "multiprocessing.Pool.join", "list", "filter", "set", "len", "print", "rg2_dict.items", "group_dict.items", "glob.glob", "os.path.join", "pathlib.Path().exists", "print", "print", "os.path.join", "pathlib.Path().exists", "glob.glob", "pathlib.Path", "eval.rouge_results_to_str", "open", "f_out.write", "pathlib.Path", "pathlib.Path", "open", "f_in.read().strip().split", "new_eval_fn_list.append", "json.dumps", "pathlib.Path", "pathlib.Path", "pathlib.Path", "open", "float", "open", "f_out.write", "pathlib.Path().exists", "os.path.join.endswith", "os.path.join.endswith", "o_name.split", "pathlib.Path().name.split", "f_in.read().strip", "f_in.read().strip().split", "pathlib.Path", "f_in.read", "pathlib.Path", "f_in.read().strip", "f_in.read"], "function", ["home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.gigaword.eval.rouge_results_to_str", "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.preprocess.process_daren.split", "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.preprocess.process_daren.split", "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.preprocess.process_daren.split", "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.preprocess.process_daren.split"], []], "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.preprocess.tokenizing.read_tsv": [[4, 41], ["open", "f.readlines", "len", "print", "enumerate", "print", "print", "range", "lines[].split", "lines[].strip().split", "print", "len", "ret_input_collect.append", "ret_output_collect.append", "sku_collect.append", "lines[].strip", "lines[].strip().split", "lines[].strip().split", "len", "lines[].strip", "lines[].strip", "lines[].strip"], "function", ["home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.preprocess.process_daren.split", "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.preprocess.process_daren.split", "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.preprocess.process_daren.split", "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.preprocess.process_daren.split"], ["def", "read_tsv", "(", "path", ",", "input_attr", ",", "output_attr", ")", ":", "\n", "    ", "ret_input_collect", "=", "[", "]", "\n", "ret_output_collect", "=", "[", "]", "\n", "with", "open", "(", "path", ",", "\"r\"", ")", "as", "f", ":", "\n", "        ", "lines", "=", "f", ".", "readlines", "(", ")", "\n", "attr_name_cnt", "=", "len", "(", "lines", "[", "0", "]", ".", "split", "(", "\"\\t\"", ")", ")", "\n", "print", "(", "\"attr_cnt\"", ",", "attr_name_cnt", ")", "\n", "input_attr_idx", "=", "0", "\n", "output_attr_idx", "=", "0", "\n", "for", "idx", ",", "i", "in", "enumerate", "(", "lines", "[", "0", "]", ".", "strip", "(", ")", ".", "split", "(", "\"\\t\"", ")", ")", ":", "\n", "            ", "print", "(", "\"*******\"", ",", "i", ",", "idx", ")", "\n", "if", "i", "==", "input_attr", ":", "\n", "                ", "input_attr_idx", "=", "idx", "\n", "", "elif", "i", "==", "output_attr", ":", "\n", "                ", "output_attr_idx", "=", "idx", "\n", "", "", "print", "(", "\"input attr idx\"", ",", "input_attr_idx", ")", "\n", "print", "(", "\"output attr idx\"", ",", "output_attr_idx", ")", "\n", "\n", "for", "idx", "in", "range", "(", "1", ",", "len", "(", "lines", ")", ")", ":", "\n", "            ", "if", "lines", "[", "idx", "]", ".", "strip", "(", ")", "==", "\"\"", ":", "\n", "                ", "continue", "\n", "", "line_attrs", "=", "lines", "[", "idx", "]", ".", "strip", "(", ")", ".", "split", "(", "\"\\t\"", ")", "\n", "assert", "attr_name_cnt", "==", "len", "(", "line_attrs", ")", "\n", "input_attr", "=", "line_attrs", "[", "input_attr_idx", "]", "\n", "output_attr", "=", "line_attrs", "[", "output_attr_idx", "]", "\n", "ret_input_collect", ".", "append", "(", "input_attr", ")", "\n", "ret_output_collect", ".", "append", "(", "output_attr", ")", "\n", "", "", "return", "ret_input_collect", ",", "ret_output_collect", "\n", "\n", "\n", "", "def", "preprocess", "(", "str_list", ")", ":", "\n", "    ", "ret", "=", "[", "]", "\n", "for", "s", "in", "str_list", ":", "\n", "        ", "ret", ".", "append", "(", "s", ".", "lower", "(", ")", ".", "replace", "(", "\"|||\"", ",", "\" [SEP] \"", ")", ")", "\n", "", "return", "ret", "\n", "\n", "", "def", "tokenize", "(", "str_list", ",", "tokenizer", ")", ":", "\n", "    ", "ret", "=", "[", "]", "\n"]], "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.preprocess.tokenizing.preprocess": [[43, 48], ["ret.append", "s.lower().replace", "s.lower"], "function", ["None"], ["        ", "token_list", "=", "tokenizer", ".", "tokenize", "(", "s", ")", "\n", "ret", ".", "append", "(", "token_list", ")", "\n", "", "return", "ret", "\n", "\n", "", "if", "__name__", "==", "\"__main__\"", ":", "\n", "    ", "parser", "=", "argparse", ".", "ArgumentParser", "(", ")", "\n"]], "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.preprocess.tokenizing.tokenize": [[64, 70], ["tqdm.tqdm", "tokenizer.tokenize", "ret.append"], "function", ["home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.preprocess.tokenizing_1342.tokenize"], ["with", "open", "(", "os", ".", "path", ".", "join", "(", "ori_dir", ",", "\"{}.src\"", ".", "format", "(", "args", ".", "split_name", ")", ")", ",", "\"w\"", ")", "as", "f", ":", "\n", "        ", "src_str", "=", "\"\\n\"", ".", "join", "(", "input_collect", ")", "\n", "f", ".", "write", "(", "src_str", ")", "\n", "\n", "", "with", "open", "(", "os", ".", "path", ".", "join", "(", "ori_dir", ",", "\"{}.tgt\"", ".", "format", "(", "args", ".", "split_name", ")", ")", ",", "\"w\"", ")", "as", "f", ":", "\n", "        ", "src_str", "=", "\"\\n\"", ".", "join", "(", "output_collect", ")", "\n", "f", ".", "write", "(", "src_str", ")", "\n"]], "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.preprocess.preprocess.read_tsv": [[4, 32], ["open", "f.readlines", "len", "print", "enumerate", "print", "print", "range", "lines[].split", "lines[].strip().split", "print", "len", "lines[].strip().split", "ret_input_collect.append", "ret_output_collect.append", "lines[].strip", "len", "lines[].strip", "lines[].strip"], "function", ["home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.preprocess.process_daren.split", "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.preprocess.process_daren.split", "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.preprocess.process_daren.split"], ["def", "read_tsv", "(", "path", ",", "input_attr", ",", "output_attr", ")", ":", "\n", "    ", "ret_input_collect", "=", "[", "]", "\n", "ret_output_collect", "=", "[", "]", "\n", "with", "open", "(", "path", ",", "\"r\"", ")", "as", "f", ":", "\n", "        ", "lines", "=", "f", ".", "readlines", "(", ")", "\n", "attr_name_cnt", "=", "len", "(", "lines", "[", "0", "]", ".", "split", "(", "\"\\t\"", ")", ")", "\n", "print", "(", "\"attr_cnt\"", ",", "attr_name_cnt", ")", "\n", "input_attr_idx", "=", "0", "\n", "output_attr_idx", "=", "0", "\n", "for", "idx", ",", "i", "in", "enumerate", "(", "lines", "[", "0", "]", ".", "strip", "(", ")", ".", "split", "(", "\"\\t\"", ")", ")", ":", "\n", "            ", "print", "(", "\"*******\"", ",", "i", ",", "idx", ")", "\n", "if", "i", "==", "input_attr", ":", "\n", "                ", "input_attr_idx", "=", "idx", "\n", "", "elif", "i", "==", "output_attr", ":", "\n", "                ", "output_attr_idx", "=", "idx", "\n", "", "", "print", "(", "\"input attr idx\"", ",", "input_attr_idx", ")", "\n", "print", "(", "\"output attr idx\"", ",", "output_attr_idx", ")", "\n", "\n", "for", "idx", "in", "range", "(", "1", ",", "len", "(", "lines", ")", ")", ":", "\n", "            ", "if", "lines", "[", "idx", "]", ".", "strip", "(", ")", "==", "\"\"", ":", "\n", "                ", "continue", "\n", "", "line_attrs", "=", "lines", "[", "idx", "]", ".", "strip", "(", ")", ".", "split", "(", "\"\\t\"", ")", "\n", "assert", "attr_name_cnt", "==", "len", "(", "line_attrs", ")", "\n", "input_attr", "=", "line_attrs", "[", "input_attr_idx", "]", "\n", "output_attr", "=", "line_attrs", "[", "output_attr_idx", "]", "\n", "ret_input_collect", ".", "append", "(", "input_attr", ")", "\n", "ret_output_collect", ".", "append", "(", "output_attr", ")", "\n", "", "", "return", "ret_input_collect", ",", "ret_output_collect", "\n", "\n"]], "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.preprocess.preprocess.preprocess": [[34, 40], ["ret.append", "s.lower().replace", "s.lower"], "function", ["None"], ["", "def", "preprocess", "(", "str_list", ")", ":", "\n", "    ", "ret", "=", "[", "]", "\n", "for", "s", "in", "str_list", ":", "\n", "        ", "ret", ".", "append", "(", "s", ".", "lower", "(", ")", ".", "replace", "(", "\"|\"", ",", "\",\"", ")", ")", "\n", "# ret.append(s.lower().replace(\"|||\", \" [SEP] \"))", "\n", "", "return", "ret", "\n", "\n"]], "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.preprocess.preprocess.tokenize": [[41, 47], ["tqdm.tqdm", "tokenizer.tokenize", "ret.append"], "function", ["home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.preprocess.tokenizing_1342.tokenize"], ["", "def", "tokenize", "(", "str_list", ",", "tokenizer", ")", ":", "\n", "    ", "ret", "=", "[", "]", "\n", "for", "s", "in", "tqdm", ".", "tqdm", "(", "str_list", ")", ":", "\n", "        ", "token_list", "=", "tokenizer", ".", "tokenize", "(", "s", ")", "\n", "ret", ".", "append", "(", "token_list", ")", "\n", "", "return", "ret", "\n", "\n"]], "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.preprocess.process_daren_1342.remove_useless": [[21, 23], ["d[].split", "d[].split"], "function", ["home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.preprocess.process_daren.split", "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.preprocess.process_daren.split"], ["def", "remove_useless", "(", "d", ")", ":", "\n", "        ", "return", "d", "[", "0", ":", "4", "]", "+", "[", "d", "[", "6", "]", "]", "+", "d", "[", "7", "]", ".", "split", "(", "'|'", ")", "+", "[", "d", "[", "24", "]", ".", "split", "(", "'|||'", ")", "[", "0", "]", "]", "+", "d", "[", "20", ":", "21", "]", "+", "d", "[", "17", ":", "18", "]", "+", "d", "[", "-", "2", ":", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.preprocess.process_daren_1342.unify_pattern": [[24, 34], ["item.split", "pattern.check_topic", "new_d.append", "pattern.check_split", "new_d.append", "items[].split"], "function", ["home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.preprocess.process_daren.split", "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.preprocess.pattern.check_topic", "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.preprocess.pattern.check_split", "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.preprocess.process_daren.split"], ["", "def", "unify_pattern", "(", "d", ")", ":", "\n", "    ", "new_d", "=", "[", "]", "\n", "for", "item", "in", "d", ":", "\n", "       ", "items", "=", "item", ".", "split", "(", "'\\t'", ")", "\n", "if", "check_topic", "(", "items", "[", "-", "1", "]", ")", "is", "True", ":", "\n", "           ", "items", "[", "-", "1", "]", "=", "'\u3002'", ".", "join", "(", "items", "[", "-", "1", "]", ".", "split", "(", "'\u3002'", ")", "[", "1", ":", "]", ")", "\n", "new_d", ".", "append", "(", "'\\t'", ".", "join", "(", "items", ")", ")", "\n", "", "elif", "check_split", "(", "items", "[", "-", "1", "]", ")", "is", "True", ":", "\n", "           ", "new_d", ".", "append", "(", "item", ")", "\n", "", "", "return", "new_d", "\n", "\n"]], "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.preprocess.process_daren_1342.split_": [[36, 48], ["i.split", "process_daren_1342.remove_useless", "re.split", "res.append"], "function", ["home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.preprocess.process_daren.split", "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.produce.preprocess.remove_useless", "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.preprocess.process_daren.split"], ["", "def", "split_", "(", "d", ")", ":", "\n", "    ", "res", "=", "[", "]", "\n", "for", "i", "in", "d", ":", "\n", "        ", "d_list", "=", "i", ".", "split", "(", "'\\t'", ")", "\n", "d_list", "=", "remove_useless", "(", "d_list", ")", "\n", "cw", "=", "d_list", "[", "-", "1", "]", "\n", "cw", "=", "re", ".", "split", "(", "regexPattern", ",", "cw", ")", "\n", "if", "(", "cw", "[", "-", "1", "]", "==", "''", ")", ":", "\n", "            ", "cw", "=", "cw", "[", ":", "-", "1", "]", "\n", "", "d_list", "[", "-", "1", "]", "=", "cw", "\n", "res", ".", "append", "(", "d_list", ")", "\n", "", "return", "res", "\n", "\n"]], "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.preprocess.process_daren_1342.split": [[50, 77], ["utils.load_txt", "process_daren_1342.unify_pattern", "print", "utils.split_data", "utils.multi_process_run", "print", "print", "len", "len", "open", "str", "f.write"], "function", ["home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.postprocess.utils.load_txt", "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.preprocess.process_daren.unify_pattern", "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.postprocess.utils.split_data", "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.postprocess.utils.multi_process_run"], ["", "def", "split", "(", ")", ":", "\n", "    ", "'''\n    split copywriting with \"\uff01\", \"\u3002\" and \"\uff1b\"\n    '''", "\n", "data", "=", "load_txt", "(", "args", ".", "data_path", ")", "\n", "data", "=", "unify_pattern", "(", "data", ")", "\n", "print", "(", "len", "(", "data", ")", ")", "\n", "\n", "if", "args", ".", "debug", ">", "-", "1", ":", "\n", "        ", "data", "=", "data", "[", ":", "args", ".", "debug", "]", "\n", "", "data_list", "=", "split_data", "(", "data", ",", "args", ".", "n", ")", "\n", "data_list", "=", "[", "(", "i", ",", ")", "for", "i", "in", "data_list", "]", "\n", "res", "=", "multi_process_run", "(", "data_list", ",", "split_", ",", "args", ".", "n", ")", "\n", "save_data", "=", "[", "]", "\n", "for", "part_res", "in", "res", ":", "\n", "        ", "save_data", "+=", "part_res", "\n", "\n", "", "print", "(", "len", "(", "save_data", ")", ")", "\n", "c", "=", "0", "\n", "with", "open", "(", "args", ".", "save_path", ",", "\"w\"", ",", "encoding", "=", "\"utf8\"", ")", "as", "f", ":", "\n", "        ", "for", "line", "in", "save_data", ":", "\n", "            ", "for", "item", "in", "line", "[", "-", "1", "]", ":", "\n", "#print(line)", "\n", "#print(item)", "\n", "                ", "c", "+=", "1", "\n", "f", ".", "write", "(", "\"\\t\"", ".", "join", "(", "line", "[", ":", "-", "1", "]", "+", "[", "item", "]", ")", "+", "'\\n'", ")", "\n", "", "", "", "print", "(", "'total item: '", "+", "str", "(", "c", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.preprocess.process_daren_1342.get_atributes": [[78, 96], ["utils.load_txt", "item.split", "list", "open", "elements[].split", "set", "f.write", "elements[].split", "elements[].split", "str"], "function", ["home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.postprocess.utils.load_txt", "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.preprocess.process_daren.split", "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.preprocess.process_daren.split", "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.preprocess.process_daren.split", "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.preprocess.process_daren.split"], ["", "def", "get_atributes", "(", ")", ":", "\n", "   ", "'''\n   get the attributes name for topic selecting\n   '''", "\n", "dic", "=", "{", "}", "\n", "data", "=", "load_txt", "(", "args", ".", "data_path", ")", "\n", "for", "item", "in", "data", ":", "\n", "       ", "elements", "=", "item", ".", "split", "(", "'\\t'", ")", "\n", "attr_name", "=", "elements", "[", "18", "]", ".", "split", "(", "'|||'", ")", "+", "elements", "[", "19", "]", ".", "split", "(", "'|||'", ")", "+", "elements", "[", "22", "]", ".", "split", "(", "'|||'", ")", "\n", "clean_attr", "=", "list", "(", "set", "(", "attr_name", ")", ")", "\n", "for", "word", "in", "clean_attr", ":", "\n", "           ", "if", "word", "in", "dic", ":", "\n", "               ", "dic", "[", "word", "]", "+=", "1", "\n", "", "else", ":", "\n", "               ", "dic", "[", "word", "]", "=", "0", "\n", "", "", "", "with", "open", "(", "args", ".", "save_path", ",", "\"w\"", ",", "encoding", "=", "\"utf8\"", ")", "as", "f", ":", "\n", "        ", "for", "key", "in", "dic", ":", "\n", "            ", "f", ".", "write", "(", "key", "+", "': '", "+", "str", "(", "dic", "[", "key", "]", ")", "+", "'\\n'", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.preprocess.process_daren_1342.extract_attr": [[98, 110], ["set", "set.split", "new_attr.append", "utils.contain_zh", "len"], "function", ["home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.preprocess.process_daren.split", "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.postprocess.utils.contain_zh"], ["", "", "", "def", "extract_attr", "(", "attr", ")", ":", "\n", "    ", "attr", "=", "set", "(", "attr", ".", "split", "(", "'|||'", ")", ")", "\n", "new_attr", "=", "[", "]", "\n", "for", "element", "in", "attr", ":", "\n", "        ", "if", "'\u5b98\u7f51'", "in", "element", "or", "'\u5176\u5b83'", "in", "element", "or", "'\uff08'", "in", "element", "or", "'('", "in", "element", ":", "\n", "           ", "continue", "\n", "", "elif", "not", "contain_zh", "(", "element", ")", ":", "\n", "           ", "continue", "\n", "", "elif", "len", "(", "element", ")", "<", "2", ":", "\n", "           ", "continue", "\n", "", "new_attr", ".", "append", "(", "element", ")", "\n", "", "return", "','", ".", "join", "(", "new_attr", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.preprocess.process_daren_1342.clean_": [[111, 130], ["item.split", "elements[].replace", "len", "prod_desc.replace.replace", "process_daren_1342.extract_attr", "clean_data.append"], "function", ["home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.preprocess.process_daren.split", "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.produce.preprocess.extract_attr"], ["", "def", "clean_", "(", "data", ")", ":", "\n", "    ", "clean_data", "=", "[", "]", "\n", "for", "item", "in", "data", ":", "\n", "        ", "elements", "=", "item", ".", "split", "(", "'\\t'", ")", "\n", "sku", "=", "elements", "[", "0", "]", "\n", "#print(sku)", "\n", "write", "=", "elements", "[", "-", "1", "]", ".", "replace", "(", "'#'", ",", "''", ")", "\n", "#print(write)", "\n", "if", "len", "(", "write", ")", "!=", "0", ":", "\n", "#clean_write=' '.join(jieba.cut(write, HMM=True))", "\n", "            ", "prod_desc", "=", "','", ".", "join", "(", "elements", "[", "1", ":", "-", "5", "]", ")", "#title, memory, color ", "\n", "prod_desc", "=", "prod_desc", ".", "replace", "(", "' '", ",", "','", ")", "\n", "attr", "=", "extract_attr", "(", "elements", "[", "-", "4", "]", "+", "elements", "[", "-", "3", "]", "+", "elements", "[", "-", "5", "]", ")", "\n", "#clean_prod=' '.join(jieba.cut(prod_desc+attribute, HMM=True))  ", "\n", "clean_prod", "=", "prod_desc", "+", "attr", "\n", "\n", "clean_data", ".", "append", "(", "sku", "+", "'|||'", "+", "clean_prod", "+", "'|||'", "+", "write", ")", "#", "\n", "\n", "", "", "return", "clean_data", "\n", "\n"]], "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.preprocess.process_daren_1342.clean": [[133, 152], ["utils.load_txt", "utils.split_data", "utils.multi_process_run", "list", "print", "set", "open", "str", "f.write", "len"], "function", ["home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.postprocess.utils.load_txt", "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.postprocess.utils.split_data", "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.postprocess.utils.multi_process_run"], ["", "def", "clean", "(", ")", ":", "\n", "    ", "'''\n    clean the data for the final use\n    the input data should be the splitted data files\n    '''", "\n", "\n", "data", "=", "load_txt", "(", "args", ".", "data_path", ")", "\n", "\n", "data_list", "=", "split_data", "(", "data", ",", "args", ".", "n", ")", "\n", "data_list", "=", "[", "(", "i", ",", ")", "for", "i", "in", "data_list", "]", "\n", "res", "=", "multi_process_run", "(", "data_list", ",", "clean_", ",", "args", ".", "n", ")", "\n", "save_data", "=", "[", "]", "\n", "for", "part_res", "in", "res", ":", "\n", "        ", "save_data", "+=", "part_res", "\n", "", "save_data", "=", "list", "(", "set", "(", "save_data", ")", ")", "\n", "print", "(", "'total daren writes: '", "+", "str", "(", "len", "(", "save_data", ")", ")", ")", "\n", "with", "open", "(", "args", ".", "save_path", ",", "'w'", ",", "encoding", "=", "'utf-8'", ")", "as", "f", ":", "\n", "        ", "for", "item", "in", "save_data", ":", "\n", "            ", "f", ".", "write", "(", "item", "+", "'\\n'", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.preprocess.process_daren_1342.clean_with_title_": [[153, 174], ["item.split", "elements[].replace", "prod_desc.replace.replace", "process_daren_1342.extract_attr", "clean_data.append", "len", "len", "title.split", "title.split"], "function", ["home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.preprocess.process_daren.split", "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.produce.preprocess.extract_attr", "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.preprocess.process_daren.split", "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.preprocess.process_daren.split"], ["", "", "", "def", "clean_with_title_", "(", "data", ")", ":", "\n", "    ", "clean_data", "=", "[", "]", "\n", "for", "item", "in", "data", ":", "\n", "        ", "elements", "=", "item", ".", "split", "(", "'\\t'", ")", "\n", "sku", "=", "elements", "[", "0", "]", "\n", "#print(sku)", "\n", "write", "=", "elements", "[", "-", "1", "]", ".", "replace", "(", "'#'", ",", "''", ")", "\n", "title", "=", "elements", "[", "-", "2", "]", "\n", "#print(write)", "\n", "if", "len", "(", "write", ")", "!=", "0", "and", "len", "(", "title", ".", "split", "(", "' '", ")", ")", "==", "3", ":", "\n", "#clean_write=' '.join(jieba.cut(write, HMM=True))", "\n", "            ", "prod_desc", "=", "','", ".", "join", "(", "elements", "[", "1", ":", "-", "5", "]", ")", "#title, memory, color ", "\n", "prod_desc", "=", "prod_desc", ".", "replace", "(", "' '", ",", "','", ")", "\n", "attr", "=", "extract_attr", "(", "elements", "[", "-", "4", "]", "+", "elements", "[", "-", "3", "]", "+", "elements", "[", "-", "5", "]", ")", "\n", "#clean_prod=' '.join(jieba.cut(prod_desc+attribute, HMM=True))  ", "\n", "clean_prod", "=", "prod_desc", "+", "attr", "\n", "title", "=", "'+'", ".", "join", "(", "title", ".", "split", "(", "' '", ")", "[", "1", ":", "]", ")", "\n", "write_prod", "=", "write", "+", "'\u3002+'", "+", "clean_prod", "\n", "clean_data", ".", "append", "(", "sku", "+", "'|||'", "+", "write_prod", "+", "'|||'", "+", "title", ")", "#", "\n", "\n", "", "", "return", "clean_data", "\n", "\n"]], "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.preprocess.process_daren_1342.clean_with_title": [[177, 196], ["utils.load_txt", "utils.split_data", "utils.multi_process_run", "list", "print", "set", "open", "str", "f.write", "len"], "function", ["home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.postprocess.utils.load_txt", "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.postprocess.utils.split_data", "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.postprocess.utils.multi_process_run"], ["", "def", "clean_with_title", "(", ")", ":", "\n", "    ", "'''\n    clean the data for the final use\n    the input data should be the splitted data files\n    '''", "\n", "\n", "data", "=", "load_txt", "(", "args", ".", "data_path", ")", "\n", "\n", "data_list", "=", "split_data", "(", "data", ",", "args", ".", "n", ")", "\n", "data_list", "=", "[", "(", "i", ",", ")", "for", "i", "in", "data_list", "]", "\n", "res", "=", "multi_process_run", "(", "data_list", ",", "clean_with_title_", ",", "args", ".", "n", ")", "\n", "save_data", "=", "[", "]", "\n", "for", "part_res", "in", "res", ":", "\n", "        ", "save_data", "+=", "part_res", "\n", "", "save_data", "=", "list", "(", "set", "(", "save_data", ")", ")", "\n", "print", "(", "'total daren writes: '", "+", "str", "(", "len", "(", "save_data", ")", ")", ")", "\n", "with", "open", "(", "args", ".", "save_path", ",", "'w'", ",", "encoding", "=", "'utf-8'", ")", "as", "f", ":", "\n", "        ", "for", "item", "in", "save_data", ":", "\n", "            ", "f", ".", "write", "(", "item", "+", "'\\n'", ")", "\n", "", "", "", "def", "main", "(", ")", ":", "\n"]], "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.preprocess.process_daren_1342.main": [[196, 207], ["process_daren_1342.split", "process_daren_1342.clean", "process_daren_1342.get_atributes", "process_daren_1342.clean_with_title", "NotImplementedError"], "function", ["home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.preprocess.process_daren.split", "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.preprocess.process_daren.clean", "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.produce.preprocess.get_atributes", "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.preprocess.process_daren_1342.clean_with_title"], ["", "", "", "def", "main", "(", ")", ":", "\n", "    ", "if", "args", ".", "task", "==", "\"split\"", ":", "\n", "        ", "split", "(", ")", "\n", "", "elif", "args", ".", "task", "==", "\"clean\"", ":", "\n", "        ", "clean", "(", ")", "\n", "", "elif", "args", ".", "task", "==", "\"get_attr\"", ":", "\n", "        ", "get_atributes", "(", ")", "\n", "", "elif", "args", ".", "task", "==", "\"clean_with_title\"", ":", "\n", "        ", "clean_with_title", "(", ")", "\n", "", "else", ":", "\n", "        ", "raise", "NotImplementedError", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.preprocess.process_daren.remove_useless": [[21, 23], ["None"], "function", ["None"], ["def", "remove_useless", "(", "d", ")", ":", "\n", "    ", "return", "d", "[", "0", ":", "3", "]", "+", "d", "[", "4", ":", "-", "10", "]", "+", "d", "[", "-", "8", ":", "-", "6", "]", "+", "d", "[", "-", "5", ":", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.preprocess.process_daren.unify_pattern": [[24, 34], ["item.split", "pattern.check_topic", "new_d.append", "pattern.check_split", "new_d.append", "items[].split"], "function", ["home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.preprocess.process_daren.split", "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.preprocess.pattern.check_topic", "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.preprocess.pattern.check_split", "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.preprocess.process_daren.split"], ["", "def", "unify_pattern", "(", "d", ")", ":", "\n", "    ", "new_d", "=", "[", "]", "\n", "for", "item", "in", "d", ":", "\n", "       ", "items", "=", "item", ".", "split", "(", "'\\t'", ")", "\n", "if", "check_topic", "(", "items", "[", "2", "]", ")", "is", "True", ":", "\n", "           ", "items", "[", "2", "]", "=", "'\u3002'", ".", "join", "(", "items", "[", "2", "]", ".", "split", "(", "'\u3002'", ")", "[", "1", ":", "]", ")", "\n", "new_d", ".", "append", "(", "'\\t'", ".", "join", "(", "items", ")", ")", "\n", "", "elif", "check_split", "(", "items", "[", "2", "]", ")", "is", "True", ":", "\n", "           ", "new_d", ".", "append", "(", "item", ")", "\n", "", "", "return", "new_d", "\n", "\n"]], "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.preprocess.process_daren.split_": [[36, 48], ["i.split", "process_daren.remove_useless", "re.split", "res.append"], "function", ["home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.preprocess.process_daren.split", "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.produce.preprocess.remove_useless", "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.preprocess.process_daren.split"], ["", "def", "split_", "(", "d", ")", ":", "\n", "    ", "res", "=", "[", "]", "\n", "for", "i", "in", "d", ":", "\n", "        ", "d_list", "=", "i", ".", "split", "(", "'\\t'", ")", "\n", "d_list", "=", "remove_useless", "(", "d_list", ")", "\n", "cw", "=", "d_list", "[", "2", "]", "\n", "cw", "=", "re", ".", "split", "(", "regexPattern", ",", "cw", ")", "\n", "if", "(", "cw", "[", "-", "1", "]", "==", "''", ")", ":", "\n", "            ", "cw", "=", "cw", "[", ":", "-", "1", "]", "\n", "", "d_list", "[", "2", "]", "=", "cw", "\n", "res", ".", "append", "(", "d_list", ")", "\n", "", "return", "res", "\n", "\n"]], "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.preprocess.process_daren.split": [[50, 75], ["utils.load_txt", "process_daren.unify_pattern", "print", "utils.split_data", "utils.multi_process_run", "print", "print", "len", "len", "open", "str", "f.write"], "function", ["home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.postprocess.utils.load_txt", "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.preprocess.process_daren.unify_pattern", "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.postprocess.utils.split_data", "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.postprocess.utils.multi_process_run"], ["", "def", "split", "(", ")", ":", "\n", "    ", "'''\n    split copywriting with \"\uff01\", \"\u3002\" and \"\uff1b\"\n    '''", "\n", "data", "=", "load_txt", "(", "args", ".", "data_path", ")", "\n", "data", "=", "unify_pattern", "(", "data", ")", "\n", "print", "(", "len", "(", "data", ")", ")", "\n", "\n", "if", "args", ".", "debug", ">", "-", "1", ":", "\n", "        ", "data", "=", "data", "[", ":", "args", ".", "debug", "]", "\n", "", "data_list", "=", "split_data", "(", "data", ",", "args", ".", "n", ")", "\n", "data_list", "=", "[", "(", "i", ",", ")", "for", "i", "in", "data_list", "]", "\n", "res", "=", "multi_process_run", "(", "data_list", ",", "split_", ",", "args", ".", "n", ")", "\n", "save_data", "=", "[", "]", "\n", "for", "part_res", "in", "res", ":", "\n", "        ", "save_data", "+=", "part_res", "\n", "\n", "", "print", "(", "len", "(", "save_data", ")", ")", "\n", "c", "=", "0", "\n", "with", "open", "(", "args", ".", "save_path", ",", "\"w\"", ",", "encoding", "=", "\"utf8\"", ")", "as", "f", ":", "\n", "        ", "for", "line", "in", "save_data", ":", "\n", "            ", "for", "item", "in", "line", "[", "2", "]", ":", "\n", "                ", "c", "+=", "1", "\n", "f", ".", "write", "(", "\"\\t\"", ".", "join", "(", "line", "[", ":", "2", "]", "+", "[", "item", "]", "+", "line", "[", "3", ":", "]", ")", "+", "'\\n'", ")", "\n", "", "", "", "print", "(", "'total item: '", "+", "str", "(", "c", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.preprocess.process_daren.get_atributes": [[76, 94], ["utils.load_txt", "item.split", "list", "open", "elements[].split", "set", "f.write", "elements[].split", "elements[].split", "str"], "function", ["home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.postprocess.utils.load_txt", "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.preprocess.process_daren.split", "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.preprocess.process_daren.split", "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.preprocess.process_daren.split", "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.preprocess.process_daren.split"], ["", "def", "get_atributes", "(", ")", ":", "\n", "   ", "'''\n   get the attributes name for topic selecting\n   '''", "\n", "dic", "=", "{", "}", "\n", "data", "=", "load_txt", "(", "args", ".", "data_path", ")", "\n", "for", "item", "in", "data", ":", "\n", "       ", "elements", "=", "item", ".", "split", "(", "'\\t'", ")", "\n", "attr_name", "=", "elements", "[", "18", "]", ".", "split", "(", "'|||'", ")", "+", "elements", "[", "19", "]", ".", "split", "(", "'|||'", ")", "+", "elements", "[", "22", "]", ".", "split", "(", "'|||'", ")", "\n", "clean_attr", "=", "list", "(", "set", "(", "attr_name", ")", ")", "\n", "for", "word", "in", "clean_attr", ":", "\n", "           ", "if", "word", "in", "dic", ":", "\n", "               ", "dic", "[", "word", "]", "+=", "1", "\n", "", "else", ":", "\n", "               ", "dic", "[", "word", "]", "=", "0", "\n", "", "", "", "with", "open", "(", "args", ".", "save_path", ",", "\"w\"", ",", "encoding", "=", "\"utf8\"", ")", "as", "f", ":", "\n", "        ", "for", "key", "in", "dic", ":", "\n", "            ", "f", ".", "write", "(", "key", "+", "': '", "+", "str", "(", "dic", "[", "key", "]", ")", "+", "'\\n'", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.preprocess.process_daren.extract_attr": [[96, 110], ["set", "set.split", "new_attr.append", "utils.contain_zh", "len"], "function", ["home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.preprocess.process_daren.split", "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.postprocess.utils.contain_zh"], ["", "", "", "def", "extract_attr", "(", "attr", ")", ":", "\n", "    ", "attr", "=", "set", "(", "attr", ".", "split", "(", "'|||'", ")", ")", "\n", "new_attr", "=", "[", "]", "\n", "for", "element", "in", "attr", ":", "\n", "        ", "if", "'\u8054\u901a'", "in", "element", "or", "'\u7535\u4fe1'", "in", "element", "or", "'\u79fb\u52a8'", "in", "element", "or", "'WCDMA'", "in", "element", "or", "'GSM'", "in", "element", "or", "'\u5e74'", "in", "element", "or", "'\u5176\u4ed6'", "in", "element", "or", "'MP4'", "in", "element", "or", "'\u4e2a'", "in", "element", "or", "'\u6708'", "in", "element", "or", "'\u8272'", "in", "element", ":", "\n", "           ", "continue", "\n", "", "elif", "'\u5b98\u7f51'", "in", "element", "or", "'\u5176\u5b83'", "in", "element", "or", "'\uff08'", "in", "element", "or", "'('", "in", "element", ":", "\n", "           ", "continue", "\n", "", "elif", "not", "contain_zh", "(", "element", ")", ":", "\n", "           ", "continue", "\n", "", "elif", "len", "(", "element", ")", "<", "2", ":", "\n", "           ", "continue", "\n", "", "new_attr", ".", "append", "(", "element", ")", "\n", "", "return", "','", ".", "join", "(", "new_attr", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.preprocess.process_daren.clean_": [[111, 128], ["item.split", "len", "prod_desc.replace.replace", "process_daren.extract_attr", "clean_data.append"], "function", ["home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.preprocess.process_daren.split", "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.produce.preprocess.extract_attr"], ["", "def", "clean_", "(", "data", ")", ":", "\n", "    ", "clean_data", "=", "[", "]", "\n", "for", "item", "in", "data", ":", "\n", "        ", "elements", "=", "item", ".", "split", "(", "'\\t'", ")", "\n", "sku", "=", "elements", "[", "0", "]", "\n", "#print(sku)", "\n", "write", "=", "elements", "[", "2", "]", "\n", "if", "len", "(", "write", ")", "!=", "0", ":", "\n", "#clean_write=' '.join(jieba.cut(write, HMM=True))", "\n", "            ", "prod_desc", "=", "elements", "[", "3", "]", "+", "elements", "[", "10", "]", "+", "elements", "[", "15", "]", "#title, memory, color ", "\n", "prod_desc", "=", "prod_desc", ".", "replace", "(", "' '", ",", "','", ")", "\n", "attr", "=", "extract_attr", "(", "elements", "[", "17", "]", "+", "elements", "[", "19", "]", ")", "\n", "#clean_prod=' '.join(jieba.cut(prod_desc+attribute, HMM=True))  ", "\n", "clean_prod", "=", "prod_desc", "+", "attr", "\n", "clean_data", ".", "append", "(", "sku", "+", "'|||'", "+", "clean_prod", "+", "'|||'", "+", "write", ")", "#", "\n", "\n", "", "", "return", "clean_data", "\n", "\n"]], "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.preprocess.process_daren.clean": [[129, 148], ["utils.load_txt", "utils.split_data", "utils.multi_process_run", "print", "open", "str", "f.write", "len"], "function", ["home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.postprocess.utils.load_txt", "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.postprocess.utils.split_data", "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.postprocess.utils.multi_process_run"], ["", "def", "clean", "(", ")", ":", "\n", "    ", "'''\n    clean the data for the final use\n    the input data should be the splitted data files\n    '''", "\n", "\n", "data", "=", "load_txt", "(", "args", ".", "data_path", ")", "\n", "\n", "data_list", "=", "split_data", "(", "data", ",", "args", ".", "n", ")", "\n", "data_list", "=", "[", "(", "i", ",", ")", "for", "i", "in", "data_list", "]", "\n", "res", "=", "multi_process_run", "(", "data_list", ",", "clean_", ",", "args", ".", "n", ")", "\n", "save_data", "=", "[", "]", "\n", "for", "part_res", "in", "res", ":", "\n", "        ", "save_data", "+=", "part_res", "\n", "\n", "", "print", "(", "'total daren writes: '", "+", "str", "(", "len", "(", "save_data", ")", ")", ")", "\n", "with", "open", "(", "args", ".", "save_path", ",", "'w'", ",", "encoding", "=", "'utf-8'", ")", "as", "f", ":", "\n", "        ", "for", "item", "in", "save_data", ":", "\n", "            ", "f", ".", "write", "(", "item", "+", "'\\n'", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.preprocess.process_daren.main": [[150, 159], ["process_daren.split", "process_daren.clean", "process_daren.get_atributes", "NotImplementedError"], "function", ["home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.preprocess.process_daren.split", "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.preprocess.process_daren.clean", "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.produce.preprocess.get_atributes"], ["", "", "", "def", "main", "(", ")", ":", "\n", "    ", "if", "args", ".", "task", "==", "\"split\"", ":", "\n", "        ", "split", "(", ")", "\n", "", "elif", "args", ".", "task", "==", "\"clean\"", ":", "\n", "        ", "clean", "(", ")", "\n", "", "elif", "args", ".", "task", "==", "\"get_attr\"", ":", "\n", "        ", "get_atributes", "(", ")", "\n", "", "else", ":", "\n", "        ", "raise", "NotImplementedError", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.preprocess.pattern.check_len": [[4, 14], ["topic.split", "len"], "function", ["home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.preprocess.process_daren.split"], ["def", "check_len", "(", "topic", ")", ":", "\n", "    ", "items", "=", "topic", ".", "split", "(", "'\uff0c'", ")", "\n", "c", "=", "0", "\n", "for", "s", "in", "items", ":", "\n", "        ", "if", "len", "(", "s", ")", ">", "9", ":", "\n", "            ", "c", "+=", "1", "\n", "", "", "if", "c", "==", "0", ":", "\n", "        ", "return", "True", "\n", "", "else", ":", "\n", "        ", "return", "False", "\n", "\n"]], "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.preprocess.pattern.check_topic": [[15, 21], ["write.split", "len", "pattern.check_len", "re.split"], "function", ["home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.preprocess.process_daren.split", "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.preprocess.pattern.check_len", "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.preprocess.process_daren.split"], ["", "", "def", "check_topic", "(", "write", ")", ":", "\n", "        ", "topic", "=", "write", ".", "split", "(", "'\u3002'", ")", "[", "0", "]", "\n", "if", "len", "(", "re", ".", "split", "(", "'\uff0c|\u3001'", ",", "topic", ")", ")", "<", "4", "and", "check_len", "(", "topic", ")", "is", "True", ":", "\n", "            ", "return", "True", "\n", "", "else", ":", "\n", "            ", "return", "False", "\n", "\n"]], "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.preprocess.pattern.check_split": [[22, 27], ["len", "re.split"], "function", ["home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.preprocess.process_daren.split"], ["", "", "def", "check_split", "(", "write", ")", ":", "\n", "    ", "if", "len", "(", "re", ".", "split", "(", "'\uff1b|\u3002|\uff01'", ",", "write", ")", ")", ">", "1", ":", "\n", "        ", "return", "True", "\n", "", "else", ":", "\n", "        ", "return", "False", "\n", "\n"]], "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.preprocess.utils.load_json": [[17, 20], ["open", "json.load"], "function", ["None"], ["def", "classify", "(", "tokens", ",", "lda_model", ",", "dic", ")", ":", "\n", "     ", "c_k", "=", "class_by_keyword", "(", "tokens", ")", "\n", "if", "c_k", ">=", "0", ":", "\n", "        ", "return", "c_k", "\n"]], "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.preprocess.utils.dump_json": [[22, 25], ["open", "json.dump"], "function", ["None"], ["        ", "corpus", "=", "dic", ".", "doc2bow", "(", "tokens", ")", "\n", "row", "=", "lda_model", ".", "get_document_topics", "(", "corpus", ")", "\n", "row", "=", "sorted", "(", "row", ",", "key", "=", "lambda", "x", ":", "(", "x", "[", "1", "]", ")", ",", "reverse", "=", "True", ")", "\n", "orig_class", "=", "row", "[", "0", "]", "[", "0", "]", "\n"]], "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.preprocess.utils.load_pickle": [[27, 30], ["open", "pickle.load"], "function", ["None"], ["                ", "return", "0", "\n", "", "if", "orig_class", "in", "[", "5", ",", "6", ",", "8", "]", ":", "\n", "                ", "return", "3", "\n", "", "if", "orig_class", "in", "[", "2", "]", ":", "\n"]], "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.preprocess.utils.dump_pickle": [[32, 35], ["open", "pickle.dump"], "function", ["None"], ["", "if", "orig_class", "in", "[", "0", ",", "1", ",", "3", ",", "9", "]", ":", "\n", "                ", "return", "4", "\n", "", "if", "orig_class", "in", "[", "4", "]", ":", "\n", "                ", "return", "1", "\n"]], "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.preprocess.utils.load_txt": [[37, 44], ["open", "line.strip.strip", "data.append"], "function", ["None"], ["                ", "return", "5", "\n", "\n", "", "", "", "def", "classify_1343", "(", "tokens", ",", "lda_model", ",", "dic", ")", ":", "\n", "     ", "c_k", "=", "class_by_keyword_1314", "(", "tokens", ")", "\n", "if", "c_k", ">=", "0", ":", "\n", "        ", "return", "c_k", "\n", "", "else", ":", "\n", "        ", "corpus", "=", "dic", ".", "doc2bow", "(", "tokens", ")", "\n"]], "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.preprocess.utils.dump_txt": [[46, 50], ["open", "f.write"], "function", ["None"], ["row", "=", "sorted", "(", "row", ",", "key", "=", "lambda", "x", ":", "(", "x", "[", "1", "]", ")", ",", "reverse", "=", "True", ")", "\n", "orig_class", "=", "row", "[", "0", "]", "[", "0", "]", "\n", "if", "orig_class", "in", "[", "0", ",", "2", ",", "6", ",", "11", "]", ":", "\n", "                ", "return", "0", "\n", "", "if", "orig_class", "in", "[", "1", "]", ":", "\n"]], "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.preprocess.utils.make_dir_path": [[52, 55], ["os.makedirs", "os.path.exists"], "function", ["None"], ["", "if", "orig_class", "in", "[", "7", ",", "8", "]", ":", "\n", "                ", "return", "2", "\n", "", "if", "orig_class", "in", "[", "5", "]", ":", "\n", "                ", "return", "3", "\n"]], "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.preprocess.utils.make_path": [[57, 61], ["os.path.dirname", "utils.make_dir_path"], "function", ["home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.postprocess.utils.make_dir_path"], ["                ", "return", "5", "\n", "", "if", "orig_class", "in", "[", "12", "]", ":", "\n", "                ", "return", "6", "\n", "", "if", "orig_class", "in", "[", "3", ",", "4", ",", "9", "]", ":", "\n", "                ", "return", "8", "\n"]], "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.preprocess.utils.multi_process_run": [[63, 74], ["multiprocessing.Pool", "multiprocessing.Pool.close", "multiprocessing.Pool.join", "multiprocessing.Pool.apply_async", "job.get"], "function", ["None"], ["", "", "", "def", "load_json", "(", "path", ")", ":", "\n", "    ", "with", "open", "(", "path", ",", "\"r\"", ",", "encoding", "=", "\"utf8\"", ")", "as", "f", ":", "\n", "        ", "return", "json", ".", "load", "(", "f", ")", "\n", "\n", "\n", "", "", "def", "dump_json", "(", "path", ",", "data", ",", "indent", "=", "4", ")", ":", "\n", "    ", "with", "open", "(", "path", ",", "\"w\"", ",", "encoding", "=", "\"utf8\"", ")", "as", "f", ":", "\n", "        ", "json", ".", "dump", "(", "data", ",", "f", ",", "indent", "=", "indent", ")", "\n", "\n", "\n", "", "", "def", "load_pickle", "(", "path", ")", ":", "\n", "    ", "with", "open", "(", "path", ",", "\"rb\"", ")", "as", "f", ":", "\n"]], "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.preprocess.utils.split_data": [[76, 86], ["len", "int", "range", "data_list.append"], "function", ["None"], ["\n", "\n", "", "", "def", "dump_pickle", "(", "path", ",", "data", ")", ":", "\n", "    ", "with", "open", "(", "path", ",", "\"wb\"", ")", "as", "f", ":", "\n", "        ", "pickle", ".", "dump", "(", "data", ",", "f", ")", "\n", "\n", "", "", "def", "load_csv", "(", "path", ")", ":", "\n", "    ", "data", "=", "[", "]", "\n", "df", "=", "pd", ".", "read_csv", "(", "path", ")", "\n", "for", "row", "in", "df", ".", "iterrows", "(", ")", ":", "\n", "      ", "data", ".", "append", "(", "row", "[", "1", "]", "[", "'product'", "]", "+", "'|||'", "+", "row", "[", "1", "]", "[", "'pred'", "]", ")", "\n"]], "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.preprocess.utils.contain_zh": [[91, 101], ["zh_pattern.search"], "function", ["None"], ["df", "=", "pd", ".", "read_csv", "(", "path", ")", "\n", "for", "row", "in", "df", ".", "iterrows", "(", ")", ":", "\n", "      ", "data", ".", "append", "(", "row", "[", "1", "]", "[", "'product'", "]", "+", "'|||'", "+", "row", "[", "1", "]", "[", "'pred'", "]", "+", "'|||'", "+", "str", "(", "row", "[", "1", "]", "[", "'sku'", "]", ")", ")", "\n", "", "return", "data", "\n", "\n", "", "def", "load_csv_sku_aspect", "(", "path", ")", ":", "\n", "    ", "data", "=", "[", "]", "\n", "df", "=", "pd", ".", "read_csv", "(", "path", ")", "\n", "for", "row", "in", "df", ".", "iterrows", "(", ")", ":", "\n", "      ", "prod", "=", "row", "[", "1", "]", "[", "'product'", "]", ".", "split", "(", "'|||'", ")", "[", "1", "]", "\n", "aspect", "=", "row", "[", "1", "]", "[", "'product'", "]", ".", "split", "(", "'|||'", ")", "[", "0", "]", "\n"]], "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.preprocess.tokenizing.preprocess2": [[49, 63], ["s.lower().replace", "s.lower().replace.split", "len", "ret.append", "ret.append", "s.lower"], "function", ["home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.preprocess.process_daren.split"], ["parser", ".", "add_argument", "(", "\"--split_name\"", ",", "default", "=", "\"eval\"", ",", "type", "=", "str", ",", "help", "=", "\"Expect in ['train', 'eval']\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--raw_tsv_dir\"", ",", "default", "=", "\"/export/shenkai/data/dapei/data_v1\"", ",", "type", "=", "str", ")", "\n", "parser", ".", "add_argument", "(", "\"--input_attr\"", ",", "type", "=", "str", ",", "default", "=", "\"atitle_add_mergetitle\"", ",", "help", "=", "\"input attribute\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--output_attr\"", ",", "type", "=", "str", ",", "default", "=", "\"match_ad_word\"", ",", "help", "=", "\"output attribute\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--output_dir_path\"", ",", "default", "=", "\"/export/shenkai/data/dapei/data_kai\"", ")", "\n", "args", "=", "parser", ".", "parse_args", "(", ")", "\n", "\n", "\n", "# read raw data, then extract input and output, respectively.", "\n", "raw_tsv_path", "=", "os", ".", "path", ".", "join", "(", "args", ".", "raw_tsv_dir", ",", "\"{}.tsv\"", ".", "format", "(", "args", ".", "split_name", ")", ")", "\n", "input_collect", ",", "output_collect", "=", "read_tsv", "(", "raw_tsv_path", ",", "args", ".", "input_attr", ",", "args", ".", "output_attr", ")", "\n", "\n", "# save original data in ori_dir", "\n", "ori_dir", "=", "os", ".", "path", ".", "join", "(", "args", ".", "output_dir_path", ",", "\"original\"", ")", "\n", "os", ".", "makedirs", "(", "ori_dir", ",", "exist_ok", "=", "True", ")", "\n"]], "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.preprocess.tokenizing_title.read_tsv": [[4, 24], ["pd.read_csv.iterrows", "pandas.read_csv", "pandas.read_csv", "isinstance", "ret_input_collect.append", "sku_collect.append", "ret_output_collect.append"], "function", ["None"], ["def", "read_tsv", "(", "path", ",", "input_name", ",", "output_name", ")", ":", "\n", "    ", "ret_input_collect", "=", "[", "]", "\n", "ret_output_collect", "=", "[", "]", "\n", "sku_collect", "=", "[", "]", "\n", "if", "args", ".", "for_produce", "==", "'produce'", ":", "\n", "        ", "df", "=", "pd", ".", "read_csv", "(", "path", ",", "sep", "=", "','", ")", "\n", "", "else", ":", "\n", "        ", "df", "=", "pd", ".", "read_csv", "(", "path", ",", "sep", "=", "'\\t'", ")", "\n", "", "for", "row", "in", "df", ".", "iterrows", "(", ")", ":", "\n", "            ", "input_attr", "=", "row", "[", "1", "]", "[", "input_name", "]", "\n", "if", "output_name", "==", "''", ":", "\n", "                ", "output_attr", "=", "''", "\n", "", "else", ":", "\n", "              ", "output_attr", "=", "row", "[", "1", "]", "[", "output_name", "]", "\n", "", "sku", "=", "row", "[", "1", "]", "[", "'sku'", "]", "\n", "if", "isinstance", "(", "output_attr", ",", "str", ")", ":", "\n", "                ", "ret_input_collect", ".", "append", "(", "input_attr", ")", "\n", "sku_collect", ".", "append", "(", "sku", ")", "\n", "ret_output_collect", ".", "append", "(", "output_attr", ")", "\n", "", "", "return", "ret_input_collect", ",", "ret_output_collect", ",", "sku_collect", "\n", "\n"]], "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.preprocess.tokenizing_title.preprocess": [[26, 31], ["ret.append", "s.lower().replace", "s.lower"], "function", ["None"], ["", "def", "preprocess", "(", "str_list", ")", ":", "\n", "    ", "ret", "=", "[", "]", "\n", "for", "s", "in", "str_list", ":", "\n", "        ", "ret", ".", "append", "(", "s", ".", "lower", "(", ")", ".", "replace", "(", "\"|||\"", ",", "\" [SEP] \"", ",", "1", ")", ")", "\n", "", "return", "ret", "\n", "\n"]], "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.preprocess.tokenizing_title.preprocess2": [[32, 46], ["s.lower().replace", "s.lower().replace.split", "len", "ret.append", "ret.append", "s.lower"], "function", ["home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.preprocess.process_daren.split"], ["", "def", "preprocess2", "(", "str_list", ")", ":", "\n", "    ", "ret", "=", "[", "]", "\n", "for", "s", "in", "str_list", ":", "\n", "        ", "a", "=", "s", ".", "lower", "(", ")", ".", "replace", "(", "\"|||\"", ",", "\" [SEP] \"", ",", "1", ")", "\n", "b", "=", "a", ".", "split", "(", "\"[SEP]\"", ")", "\n", "if", "len", "(", "b", ")", "==", "1", ":", "\n", "            ", "ret", ".", "append", "(", "a", ")", "\n", "", "else", ":", "\n", "            ", "c", "=", "\"\"", "\n", "for", "i", "in", "b", "[", "0", "]", ":", "\n", "                ", "c", "+=", "i", "+", "\" \"", "\n", "", "b", "[", "0", "]", "=", "c", "\n", "ret", ".", "append", "(", "\"[SEP]\"", ".", "join", "(", "b", ")", ")", "\n", "", "", "return", "ret", "\n", "\n"]], "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.preprocess.tokenizing_title.tokenize": [[47, 53], ["tqdm.tqdm", "tokenizer.tokenize", "ret.append"], "function", ["home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.preprocess.tokenizing_1342.tokenize"], ["", "def", "tokenize", "(", "str_list", ",", "tokenizer", ")", ":", "\n", "    ", "ret", "=", "[", "]", "\n", "for", "s", "in", "tqdm", ".", "tqdm", "(", "str_list", ")", ":", "\n", "        ", "token_list", "=", "tokenizer", ".", "tokenize", "(", "s", ")", "\n", "ret", ".", "append", "(", "token_list", ")", "\n", "", "return", "ret", "\n", "\n"]], "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.preprocess.tokenizing_1343.read_tsv": [[4, 36], ["open", "f.readlines", "len", "print", "enumerate", "print", "print", "range", "lines[].split", "lines[].strip().split", "print", "len", "lines[].strip().split", "ret_input_collect.append", "ret_output_collect.append", "sku_collect.append", "lines[].strip", "len", "lines[].strip", "lines[].strip"], "function", ["home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.preprocess.process_daren.split", "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.preprocess.process_daren.split", "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.preprocess.process_daren.split"], ["def", "read_tsv", "(", "path", ",", "input_attr", ",", "output_attr", ")", ":", "\n", "    ", "ret_input_collect", "=", "[", "]", "\n", "ret_output_collect", "=", "[", "]", "\n", "sku_collect", "=", "[", "]", "\n", "with", "open", "(", "path", ",", "\"r\"", ")", "as", "f", ":", "\n", "        ", "lines", "=", "f", ".", "readlines", "(", ")", "\n", "attr_name_cnt", "=", "len", "(", "lines", "[", "0", "]", ".", "split", "(", "\"\\t\"", ")", ")", "\n", "print", "(", "\"attr_cnt\"", ",", "attr_name_cnt", ")", "\n", "input_attr_idx", "=", "0", "\n", "output_attr_idx", "=", "0", "\n", "for", "idx", ",", "i", "in", "enumerate", "(", "lines", "[", "0", "]", ".", "strip", "(", ")", ".", "split", "(", "\"\\t\"", ")", ")", ":", "\n", "            ", "print", "(", "\"*******\"", ",", "i", ",", "idx", ")", "\n", "if", "i", "==", "input_attr", ":", "\n", "                ", "input_attr_idx", "=", "idx", "\n", "", "elif", "i", "==", "output_attr", ":", "\n", "                ", "output_attr_idx", "=", "idx", "\n", "", "", "print", "(", "\"input attr idx\"", ",", "input_attr_idx", ")", "\n", "print", "(", "\"output attr idx\"", ",", "output_attr_idx", ")", "\n", "\n", "for", "idx", "in", "range", "(", "1", ",", "len", "(", "lines", ")", ")", ":", "\n", "            ", "if", "lines", "[", "idx", "]", ".", "strip", "(", ")", "==", "\"\"", ":", "\n", "                ", "continue", "\n", "", "line_attrs", "=", "lines", "[", "idx", "]", ".", "strip", "(", ")", ".", "split", "(", "\"|||\"", ")", "\n", "#print(line_attrs)", "\n", "assert", "attr_name_cnt", "==", "len", "(", "line_attrs", ")", "\n", "input_attr", "=", "line_attrs", "[", "input_attr_idx", "]", "\n", "output_attr", "=", "line_attrs", "[", "output_attr_idx", "]", "\n", "sku", "=", "line_attrs", "[", "0", "]", "\n", "ret_input_collect", ".", "append", "(", "input_attr", ")", "\n", "ret_output_collect", ".", "append", "(", "output_attr", ")", "\n", "sku_collect", ".", "append", "(", "sku", ")", "\n", "", "", "return", "ret_input_collect", ",", "ret_output_collect", ",", "sku_collect", "\n", "\n"]], "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.preprocess.tokenizing_1343.preprocess": [[38, 43], ["ret.append", "s.lower().replace", "s.lower"], "function", ["None"], ["", "def", "preprocess", "(", "str_list", ")", ":", "\n", "    ", "ret", "=", "[", "]", "\n", "for", "s", "in", "str_list", ":", "\n", "        ", "ret", ".", "append", "(", "s", ".", "lower", "(", ")", ".", "replace", "(", "\"|||\"", ",", "\" [SEP] \"", ",", "1", ")", ")", "\n", "", "return", "ret", "\n", "\n"]], "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.preprocess.tokenizing_1343.preprocess2": [[44, 58], ["s.lower().replace", "s.lower().replace.split", "len", "ret.append", "ret.append", "s.lower"], "function", ["home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.preprocess.process_daren.split"], ["", "def", "preprocess2", "(", "str_list", ")", ":", "\n", "    ", "ret", "=", "[", "]", "\n", "for", "s", "in", "str_list", ":", "\n", "        ", "a", "=", "s", ".", "lower", "(", ")", ".", "replace", "(", "\"|||\"", ",", "\" [SEP] \"", ",", "1", ")", "\n", "b", "=", "a", ".", "split", "(", "\"[SEP]\"", ")", "\n", "if", "len", "(", "b", ")", "==", "1", ":", "\n", "            ", "ret", ".", "append", "(", "a", ")", "\n", "", "else", ":", "\n", "            ", "c", "=", "\"\"", "\n", "for", "i", "in", "b", "[", "0", "]", ":", "\n", "                ", "c", "+=", "i", "+", "\" \"", "\n", "", "b", "[", "0", "]", "=", "c", "\n", "ret", ".", "append", "(", "\"[SEP]\"", ".", "join", "(", "b", ")", ")", "\n", "", "", "return", "ret", "\n", "\n"]], "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.preprocess.tokenizing_1343.tokenize": [[59, 65], ["tqdm.tqdm", "tokenizer.tokenize", "ret.append"], "function", ["home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.preprocess.tokenizing_1342.tokenize"], ["", "def", "tokenize", "(", "str_list", ",", "tokenizer", ")", ":", "\n", "    ", "ret", "=", "[", "]", "\n", "for", "s", "in", "tqdm", ".", "tqdm", "(", "str_list", ")", ":", "\n", "        ", "token_list", "=", "tokenizer", ".", "tokenize", "(", "s", ")", "\n", "ret", ".", "append", "(", "token_list", ")", "\n", "", "return", "ret", "\n", "\n"]], "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.preprocess.tokenizing_1342.read_tsv": [[4, 38], ["open", "f.readlines", "len", "print", "enumerate", "print", "print", "range", "lines[].split", "lines[].strip().split", "print", "len", "lines[].strip().split", "ret_input_collect.append", "ret_output_collect.append", "sku_collect.append", "lines[].strip", "len", "lines[].strip", "lines[].strip", "input_attr.split"], "function", ["home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.preprocess.process_daren.split", "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.preprocess.process_daren.split", "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.preprocess.process_daren.split", "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.preprocess.process_daren.split"], ["def", "read_tsv", "(", "path", ",", "input_attr", ",", "output_attr", ")", ":", "\n", "    ", "ret_input_collect", "=", "[", "]", "\n", "ret_output_collect", "=", "[", "]", "\n", "sku_collect", "=", "[", "]", "\n", "with", "open", "(", "path", ",", "\"r\"", ")", "as", "f", ":", "\n", "        ", "lines", "=", "f", ".", "readlines", "(", ")", "\n", "attr_name_cnt", "=", "len", "(", "lines", "[", "0", "]", ".", "split", "(", "\"\\t\"", ")", ")", "\n", "print", "(", "\"attr_cnt\"", ",", "attr_name_cnt", ")", "\n", "input_attr_idx", "=", "0", "\n", "output_attr_idx", "=", "0", "\n", "for", "idx", ",", "i", "in", "enumerate", "(", "lines", "[", "0", "]", ".", "strip", "(", ")", ".", "split", "(", "\"\\t\"", ")", ")", ":", "\n", "            ", "print", "(", "\"*******\"", ",", "i", ",", "idx", ")", "\n", "if", "i", "==", "input_attr", ":", "\n", "                ", "input_attr_idx", "=", "idx", "\n", "", "elif", "i", "==", "output_attr", ":", "\n", "                ", "output_attr_idx", "=", "idx", "\n", "", "", "print", "(", "\"input attr idx\"", ",", "input_attr_idx", ")", "\n", "print", "(", "\"output attr idx\"", ",", "output_attr_idx", ")", "\n", "\n", "for", "idx", "in", "range", "(", "1", ",", "len", "(", "lines", ")", ")", ":", "\n", "            ", "if", "lines", "[", "idx", "]", ".", "strip", "(", ")", "==", "\"\"", ":", "\n", "                ", "continue", "\n", "", "line_attrs", "=", "lines", "[", "idx", "]", ".", "strip", "(", ")", ".", "split", "(", "\"|||\"", ")", "\n", "#print(line_attrs)", "\n", "assert", "attr_name_cnt", "==", "len", "(", "line_attrs", ")", "\n", "input_attr", "=", "line_attrs", "[", "input_attr_idx", "]", "\n", "if", "'<ahref'", "in", "input_attr", ":", "\n", "                ", "input_attr", "=", "input_attr", ".", "split", "(", "'<ahref'", ")", "[", "0", "]", "\n", "", "output_attr", "=", "line_attrs", "[", "output_attr_idx", "]", "\n", "sku", "=", "line_attrs", "[", "0", "]", "\n", "ret_input_collect", ".", "append", "(", "input_attr", ")", "\n", "ret_output_collect", ".", "append", "(", "output_attr", ")", "\n", "sku_collect", ".", "append", "(", "sku", ")", "\n", "", "", "return", "ret_input_collect", ",", "ret_output_collect", ",", "sku_collect", "\n", "\n"]], "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.preprocess.tokenizing_1342.preprocess": [[40, 45], ["ret.append", "s.lower().replace", "s.lower"], "function", ["None"], ["", "def", "preprocess", "(", "str_list", ")", ":", "\n", "    ", "ret", "=", "[", "]", "\n", "for", "s", "in", "str_list", ":", "\n", "        ", "ret", ".", "append", "(", "s", ".", "lower", "(", ")", ".", "replace", "(", "\"|||\"", ",", "\" [SEP] \"", ",", "1", ")", ")", "\n", "", "return", "ret", "\n", "\n"]], "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.preprocess.tokenizing_1342.preprocess2": [[46, 60], ["s.lower().replace", "s.lower().replace.split", "len", "ret.append", "ret.append", "s.lower"], "function", ["home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.preprocess.process_daren.split"], ["", "def", "preprocess2", "(", "str_list", ")", ":", "\n", "    ", "ret", "=", "[", "]", "\n", "for", "s", "in", "str_list", ":", "\n", "        ", "a", "=", "s", ".", "lower", "(", ")", ".", "replace", "(", "\"|||\"", ",", "\" [SEP] \"", ",", "1", ")", "\n", "b", "=", "a", ".", "split", "(", "\"[SEP]\"", ")", "\n", "if", "len", "(", "b", ")", "==", "1", ":", "\n", "            ", "ret", ".", "append", "(", "a", ")", "\n", "", "else", ":", "\n", "            ", "c", "=", "\"\"", "\n", "for", "i", "in", "b", "[", "0", "]", ":", "\n", "                ", "c", "+=", "i", "+", "\" \"", "\n", "", "b", "[", "0", "]", "=", "c", "\n", "ret", ".", "append", "(", "\"[SEP]\"", ".", "join", "(", "b", ")", ")", "\n", "", "", "return", "ret", "\n", "\n"]], "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.preprocess.tokenizing_1342.tokenize": [[61, 67], ["tqdm.tqdm", "tokenizer.tokenize", "ret.append"], "function", ["home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.preprocess.tokenizing_1342.tokenize"], ["", "def", "tokenize", "(", "str_list", ",", "tokenizer", ")", ":", "\n", "    ", "ret", "=", "[", "]", "\n", "for", "s", "in", "tqdm", ".", "tqdm", "(", "str_list", ")", ":", "\n", "        ", "token_list", "=", "tokenizer", ".", "tokenize", "(", "s", ")", "\n", "ret", ".", "append", "(", "token_list", ")", "\n", "", "return", "ret", "\n", "\n"]], "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.gigaword.eval.test_rouge": [[61, 96], ["tempfile.mkdtemp", "len", "time.strftime", "os.path.join", "len", "len", "time.localtime", "os.path.isdir", "os.mkdir", "os.mkdir", "os.mkdir", "range", "gigaword.bs_pyrouge.Rouge155", "gigaword.bs_pyrouge.Rouge155.convert_and_evaluate", "print", "gigaword.bs_pyrouge.Rouge155.output_to_dict", "os.path.isdir", "shutil.rmtree", "len", "open", "f.write", "open", "f.write"], "function", ["None"], []], "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.gigaword.eval.rouge_results_to_str": [[98, 106], ["None"], "function", ["None"], []], "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.gigaword.eval.count_tokens": [[109, 117], ["counter.keys"], "function", ["None"], []], "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.gigaword.eval.get_f1": [[119, 133], ["text_a.lower().split", "text_b.lower().split", "eval.count_tokens", "eval.count_tokens", "count_tokens.keys", "len", "len", "text_a.lower", "text_b.lower", "len", "len", "count_tokens.keys", "min", "len", "len"], "function", ["home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.preprocess.process_daren.split", "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.preprocess.process_daren.split", "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.gigaword.eval.count_tokens", "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.gigaword.eval.count_tokens"], []], "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.gigaword.eval._is_digit": [[141, 146], ["ch.isdigit"], "function", ["None"], []], "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.gigaword.eval.fix_tokenization": [[148, 226], ["text.split", "len", "_tok_dict.keys", "output_tokens.append", "output_tokens.append", "output_tokens.append", "output_tokens[].endswith", "output_tokens.append", "len", "output_tokens.append", "len", "len", "output_tokens.append", "output_tokens.append", "output_tokens.append", "eval._is_digit", "eval._is_digit", "len", "len", "output_tokens[].isdigit", "input_tokens[].isdigit", "len", "len", "output_tokens[].isupper", "input_tokens[].isupper", "len", "len", "len", "len", "len", "len", "input_tokens[].isupper", "output_tokens.append", "output_tokens.append", "len", "output_tokens.append", "len", "len", "output_tokens.append", "len"], "function", ["home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.preprocess.process_daren.split", "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.gigaword.eval._is_digit", "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.gigaword.eval._is_digit"], []], "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.gigaword.eval.process_eval": [[228, 269], ["open", "open", "open", "len", "len", "len", "len", "eval.test_rouge", "evaluator.get_scores", "l.strip", "gold_list.append", "fix_tokenization().replace", "buf.append", "pred_list.append", "f_out.write", "f_out.write", "l.strip", "len", "eval.fix_tokenization", "bit.split", "min", "trunc_list.append", "l.strip", "len"], "function", ["home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.gigaword.eval.test_rouge", "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.gigaword.eval.fix_tokenization", "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.preprocess.process_daren.split"], []], "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.gigaword.eval.main": [[271, 338], ["list", "logger.info", "min", "multiprocessing.Pool", "multiprocessing.Pool.imap_unordered", "sorted", "multiprocessing.Pool.close", "multiprocessing.Pool.join", "list", "filter", "set", "len", "print", "rg2_dict.items", "group_dict.items", "glob.glob", "os.path.join", "pathlib.Path().exists", "print", "print", "os.path.join", "pathlib.Path().exists", "glob.glob", "pathlib.Path", "eval.rouge_results_to_str", "open", "f_out.write", "pathlib.Path", "pathlib.Path", "open", "f_in.read().strip().split", "new_eval_fn_list.append", "json.dumps", "pathlib.Path", "pathlib.Path", "pathlib.Path", "open", "float", "open", "f_out.write", "pathlib.Path().exists", "os.path.join.endswith", "os.path.join.endswith", "o_name.split", "pathlib.Path().name.split", "f_in.read().strip", "f_in.read().strip().split", "pathlib.Path", "f_in.read", "pathlib.Path", "f_in.read().strip", "f_in.read"], "function", ["home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.gigaword.eval.rouge_results_to_str", "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.preprocess.process_daren.split", "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.preprocess.process_daren.split", "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.preprocess.process_daren.split", "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.preprocess.process_daren.split"], []], "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.data_preprocess.data_preprocess.get_df": [[12, 30], ["pandas.concat", "os.path.join", "open", "f.read", "os.listdir", "pandas.read_csv", "df_all.append", "print", "os.path.join", "f.read.split", "os.path.join", "len", "len"], "function", ["home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.preprocess.process_daren.split"], ["def", "get_df", "(", "data_dir", ",", "folder_name", ",", "n", "=", "2", ")", ":", "\n", "    ", "file_list", "=", "[", "os", ".", "path", ".", "join", "(", "data_dir", ",", "folder_name", ",", "x", ")", "for", "x", "in", "os", ".", "listdir", "(", "os", ".", "path", ".", "join", "(", "data_dir", ",", "folder_name", ")", ")", "if", "'part'", "in", "x", "]", "\n", "if", "n", ">", "0", ":", "\n", "        ", "file_list", "=", "file_list", "[", ":", "n", "]", "\n", "", "df_all", "=", "[", "]", "\n", "for", "filename", "in", "file_list", ":", "\n", "        ", "try", ":", "\n", "            ", "df", "=", "pd", ".", "read_csv", "(", "filename", ",", "sep", "=", "'\\t'", ",", "encoding", "=", "'utf8'", ",", "header", "=", "None", ")", "\n", "df_all", ".", "append", "(", "df", ")", "\n", "print", "(", "len", "(", "df", ")", ")", "\n", "", "except", ":", "\n", "            ", "pass", "\n", "", "", "df_info", "=", "pd", ".", "concat", "(", "df_all", ",", "axis", "=", "0", ")", "\n", "with", "open", "(", "os", ".", "path", ".", "join", "(", "data_dir", ",", "folder_name", ",", "'head.txt'", ")", ",", "'r'", ")", "as", "f", ":", "\n", "        ", "x", "=", "f", ".", "read", "(", ")", "\n", "", "header", "=", "[", "a", "for", "a", "in", "x", ".", "split", "(", "'\\n'", ")", "if", "len", "(", "a", ")", ">", "0", "]", "\n", "df_info", ".", "columns", "=", "header", "\n", "return", "df_info", "\n", "## df_ocr_info.tsv\u751f\u6210\u8fc7\u7a0b\uff0c\u53ef\u5728pretrain\u6570\u636e\u76ee\u5f55\u4e0b\u627e\u5230\u8be5\u6587\u4ef6", "\n"]], "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.data_preprocess.data_preprocess.preprocess_ocr": [[31, 56], ["range", "pandas.DataFrame", "pd.DataFrame.to_csv", "len", "ocr_info.replace.replace", "print", "a.strip", "len", "ocr_info_list.append", "df_ocr[].isna", "len", "pandas.DataFrame", "pd.DataFrame.to_csv", "x[].split", "len", "range", "a.strip", "aa.strip", "len", "[].replace().replace().split", "len", "[].replace().replace", "aa.strip", "[].replace"], "function", ["home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.preprocess.process_daren.split", "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.preprocess.process_daren.split"], ["", "def", "preprocess_ocr", "(", "df_ocr", ")", ":", "\n", "    ", "ocr_info_list", "=", "[", "]", "\n", "df_tmp", "=", "df_ocr", "[", "~", "df_ocr", "[", "'ocr'", "]", ".", "isna", "(", ")", "]", "\n", "for", "i", "in", "range", "(", "len", "(", "df_tmp", ")", ")", ":", "\n", "# for i in range(20):", "\n", "        ", "if", "i", "%", "50000", "==", "1", ":", "\n", "            ", "print", "(", "i", ")", "\n", "if", "len", "(", "ocr_info_list", ")", ">", "0", ":", "\n", "                ", "df_ocr_info", "=", "pd", ".", "DataFrame", "(", "ocr_info_list", ")", "\n", "df_ocr_info", ".", "columns", "=", "[", "'sku'", ",", "'ocr_merge'", "]", "\n", "df_ocr_info", ".", "to_csv", "(", "'df_ocr_info.tsv'", ",", "sep", "=", "'\\t'", ",", "index", "=", "None", ")", "\n", "", "", "sku", "=", "df_tmp", ".", "iloc", "[", "i", "]", "[", "'sku'", "]", "\n", "x", "=", "df_tmp", ".", "iloc", "[", "i", "]", "[", "'ocr'", "]", "\n", "ocr_list", "=", "[", "a", ".", "strip", "(", ")", "for", "a", "in", "x", "[", "2", ":", "-", "2", "]", ".", "split", "(", "'),('", ")", "if", "len", "(", "a", ".", "strip", "(", ")", ")", ">", "0", "]", "\n", "\n", "ocr_info", "=", "'|||'", ".", "join", "(", "[", "''", ".", "join", "(", "[", "aa", ".", "strip", "(", "\"'\"", ")", "for", "aa", "in", "ocr_list", "[", "i", "]", "[", "2", ":", "-", "2", "]", ".", "replace", "(", "')'", ",", "''", ")", ".", "replace", "(", "'('", ",", "''", ")", ".", "split", "(", "\", \"", ")", "if", "len", "(", "aa", ".", "strip", "(", "\"'\"", ")", ")", ">", "0", "]", ")", "for", "i", "in", "range", "(", "len", "(", "ocr_list", ")", ")", "]", ")", "\n", "ocr_info", "=", "ocr_info", ".", "replace", "(", "'None'", ",", "''", ")", "\n", "if", "len", "(", "ocr_info", ")", ">", "0", ":", "\n", "#         print(sku)", "\n", "#         print(ocr_info)", "\n", "            ", "ocr_info_list", ".", "append", "(", "[", "sku", ",", "ocr_info", "]", ")", "\n", "", "", "df_ocr_info", "=", "pd", ".", "DataFrame", "(", "ocr_info_list", ")", "\n", "df_ocr_info", ".", "columns", "=", "[", "'sku'", ",", "'ocr_merge'", "]", "\n", "df_ocr_info", ".", "to_csv", "(", "'df_ocr_info.tsv'", ",", "sep", "=", "'\\t'", ",", "index", "=", "None", ")", "\n", "return", "\n", "\n"]], "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.data_preprocess.data_preprocess.get_item_ad": [[58, 75], ["print", "print", "print", "print", "df_item.drop_duplicates.drop_duplicates", "print", "df_item[].apply", "print", "len", "len", "len", "len", "len", "len", "set", "set", "a.strip", "x[].split", "len", "a.strip"], "function", ["home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.preprocess.process_daren.split"], ["", "def", "get_item_ad", "(", "df_info", ",", "type", "=", "'train'", ")", ":", "\n", "    ", "print", "(", "len", "(", "df_info", ")", ")", "\n", "if", "type", "==", "'train'", ":", "\n", "        ", "split_tag", "=", "0", "\n", "", "else", ":", "\n", "        ", "split_tag", "=", "2", "\n", "", "df_info", "=", "df_info", "[", "df_info", "[", "'split_tag'", "]", "==", "split_tag", "]", "\n", "print", "(", "len", "(", "df_info", ")", ")", "\n", "print", "(", "len", "(", "set", "(", "df_info", "[", "'sku'", "]", ")", ")", ")", "\n", "df_item", "=", "df_info", "[", "[", "'sku'", ",", "'sku_title'", ",", "'attr2_value_name'", ",", "'item_second_cate_name'", "]", "]", "\n", "print", "(", "len", "(", "df_item", ")", ")", "\n", "df_item", "=", "df_item", ".", "drop_duplicates", "(", ")", "\n", "print", "(", "len", "(", "df_item", ")", ")", "\n", "df_item", "[", "'attr_merge'", "]", "=", "df_item", "[", "'attr2_value_name'", "]", ".", "apply", "(", "lambda", "x", ":", "'|'", ".", "join", "(", "set", "(", "[", "a", ".", "strip", "(", ")", "for", "a", "in", "x", "[", "2", ":", "-", "2", "]", ".", "split", "(", "'),('", ")", "if", "len", "(", "a", ".", "strip", "(", ")", ")", ">", "0", "]", ")", ")", ")", "\n", "df_wenan", "=", "df_info", "[", "[", "'id'", ",", "'sku'", ",", "'sku_title'", ",", "'recommend_reason'", "]", "]", "\n", "print", "(", "len", "(", "df_wenan", ")", ")", "\n", "return", "df_wenan", ",", "df_item", "\n", "\n"]], "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.data_preprocess.data_preprocess.get_item_ocr": [[77, 102], ["set", "print", "print", "df_item.drop_duplicates.drop_duplicates", "print", "df_item[].apply", "list", "list", "range", "pandas.DataFrame", "df_wenan.merge.sample().reset_index", "print", "df_wenan.merge.merge", "print", "len", "len", "len", "len", "ocr_list[].split", "len", "len", "len", "result_all.append", "df_wenan.merge.sample", "df_ocr[].isin", "set", "a.strip", "x[].split", "len", "a.strip"], "function", ["home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.preprocess.process_daren.split", "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.preprocess.process_daren.split"], ["", "def", "get_item_ocr", "(", "df_ocr", ",", "df_ocr_info", ")", ":", "\n", "    ", "sku_ids", "=", "set", "(", "df_ocr_info", "[", "'sku'", "]", ")", "\n", "print", "(", "len", "(", "sku_ids", ")", ")", "\n", "df_item", "=", "df_ocr", "[", "df_ocr", "[", "'sku'", "]", ".", "isin", "(", "sku_ids", ")", "]", "[", "[", "'sku'", ",", "'sku_title'", ",", "'attr2_value_name'", ",", "'item_second_cate_name'", "]", "]", "\n", "print", "(", "len", "(", "df_item", ")", ")", "\n", "df_item", "=", "df_item", ".", "drop_duplicates", "(", ")", "\n", "print", "(", "len", "(", "df_item", ")", ")", "\n", "df_item", "[", "'attr_merge'", "]", "=", "df_item", "[", "'attr2_value_name'", "]", ".", "apply", "(", "lambda", "x", ":", "'|'", ".", "join", "(", "set", "(", "[", "a", ".", "strip", "(", ")", "for", "a", "in", "x", "[", "2", ":", "-", "2", "]", ".", "split", "(", "'),('", ")", "if", "len", "(", "a", ".", "strip", "(", ")", ")", ">", "0", "]", ")", ")", ")", "\n", "\n", "sku_list", "=", "list", "(", "df_ocr_info", "[", "'sku'", "]", ")", "\n", "ocr_list", "=", "list", "(", "df_ocr_info", "[", "'ocr_merge'", "]", ")", "\n", "result_all", "=", "[", "]", "\n", "count", "=", "0", "\n", "for", "i", "in", "range", "(", "len", "(", "sku_list", ")", ")", ":", "\n", "        ", "sku", "=", "sku_list", "[", "i", "]", "\n", "for", "ocr", "in", "ocr_list", "[", "i", "]", ".", "split", "(", "'|||'", ")", ":", "\n", "            ", "result_all", ".", "append", "(", "[", "count", ",", "sku", ",", "ocr", "]", ")", "\n", "count", "+=", "1", "\n", "", "", "df_wenan", "=", "pd", ".", "DataFrame", "(", "result_all", ",", "columns", "=", "[", "'id'", ",", "'sku'", ",", "'recommend_reason'", "]", ")", "\n", "df_wenan", "=", "df_wenan", ".", "sample", "(", "frac", "=", "1", ")", ".", "reset_index", "(", "drop", "=", "True", ")", "\n", "print", "(", "len", "(", "df_wenan", ")", ")", "\n", "df_wenan", "=", "df_wenan", ".", "merge", "(", "df_item", "[", "[", "'sku'", ",", "'sku_title'", "]", "]", ",", "how", "=", "'left'", ",", "on", "=", "'sku'", ")", "\n", "df_wenan", "=", "df_wenan", "[", "[", "'id'", ",", "'sku'", ",", "'sku_title'", ",", "'recommend_reason'", "]", "]", "\n", "print", "(", "len", "(", "df_wenan", ")", ",", "len", "(", "df_item", ")", ")", "\n", "return", "df_wenan", ",", "df_item", "\n", "\n"]], "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.data_preprocess.data_preprocess.get_df_unilm": [[104, 213], ["print", "print", "df_w_tmp[].apply", "df_w_tmp[].apply", "df_tmp[].apply", "df_tmp[].apply", "print", "collections.defaultdict", "list", "list", "range", "print", "df_tmp.sample().reset_index.sample().reset_index", "list", "print", "list", "print", "min", "print", "collections.defaultdict", "range", "random.shuffle", "range", "random.shuffle", "range", "pandas.DataFrame", "df_bi.sample().reset_index.sample().reset_index", "print", "df_s2s.merge.merge", "print", "print", "len", "len", "df_wenan.drop_duplicates", "len", "len", "len", "len", "len", "sku_id_dic[].append", "len", "print", "len", "set().difference", "len", "len", "len", "len", "len", "len", "list", "list", "list", "list", "len", "cat_dic[].append", "len", "result_pos.append", "len", "random.choice", "result_neg.append", "len", "len", "len", "len", "len", "df_w_tmp[].apply", "df_tmp[].apply", "df_tmp[].apply", "df_w_tmp[].apply", "df_tmp.sample().reset_index.sample", "len", "len", "set", "len", "attr.split", "random.shuffle", "df_bi.sample().reset_index.sample", "len", "len", "set", "df_w_tmp[].isin", "df_w_tmp[].isin", "set", "set", "len", "len", "len", "len", "len", "len", "len", "len", "str", "str", "str", "str", "str", "str", "str", "str"], "function", ["home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.preprocess.process_daren.split"], ["", "def", "get_df_unilm", "(", "df_wenan", ",", "df_item", ",", "max_len", ",", "n", "=", "15", ",", "seed", "=", "1234", ",", "\n", "wenan_min", "=", "20", ",", "wenan_max", "=", "90", ",", "wenan_cut", "=", "90", ",", "\n", "attr_min", "=", "10", ",", "attr_max", "=", "80", ",", "attr_cut", "=", "80", ",", "\n", "title_min", "=", "5", ",", "title_max", "=", "80", ",", "title_cut", "=", "80", ",", "drop_dup_ad", "=", "True", ")", ":", "\n", "    ", "print", "(", "len", "(", "df_wenan", ")", ",", "len", "(", "df_item", ")", ")", "\n", "df_tmp", "=", "df_item", "[", "[", "'sku'", ",", "'sku_title'", ",", "'attr_merge'", ",", "'item_second_cate_name'", "]", "]", "\n", "if", "drop_dup_ad", ":", "\n", "        ", "df_w_tmp", "=", "df_wenan", ".", "drop_duplicates", "(", "'recommend_reason'", ")", "\n", "", "else", ":", "\n", "        ", "df_w_tmp", "=", "df_wenan", "\n", "", "print", "(", "len", "(", "df_w_tmp", ")", ",", "len", "(", "df_tmp", ")", ")", "\n", "## \u8fc7\u6ee4\u957f\u5ea6\u4e0d\u8fbe\u6807\u6587\u6848", "\n", "df_w_tmp", "=", "df_w_tmp", "[", "df_w_tmp", "[", "'recommend_reason'", "]", ".", "apply", "(", "lambda", "x", ":", "len", "(", "str", "(", "x", ")", ")", ">", "wenan_min", "and", "len", "(", "str", "(", "x", ")", ")", "<", "wenan_max", ")", "]", "\n", "## \u8fc7\u6ee4\u957f\u5ea6\u4e0d\u8fbe\u6807\u5c5e\u6027", "\n", "df_tmp", "=", "df_tmp", "[", "df_tmp", "[", "'attr_merge'", "]", ".", "apply", "(", "lambda", "x", ":", "len", "(", "str", "(", "x", ")", ")", ">", "attr_min", "and", "len", "(", "str", "(", "x", ")", ")", "<", "attr_max", ")", "]", "\n", "## \u8fc7\u6ee4\u957f\u5ea6\u4e0d\u8fbe\u6807\u6807\u9898", "\n", "df_tmp", "=", "df_tmp", "[", "df_tmp", "[", "'sku_title'", "]", ".", "apply", "(", "lambda", "x", ":", "len", "(", "str", "(", "x", ")", ")", ">", "title_min", "and", "len", "(", "str", "(", "x", ")", ")", "<", "title_max", ")", "]", "\n", "df_w_tmp", "=", "df_w_tmp", "[", "df_w_tmp", "[", "'sku_title'", "]", ".", "apply", "(", "lambda", "x", ":", "len", "(", "str", "(", "x", ")", ")", ">", "title_min", "and", "len", "(", "str", "(", "x", ")", ")", "<", "title_max", ")", "]", "\n", "## \u5bf9\u4e8e\u8fc7\u957f\u6807\u9898\u53ca\u6587\u6848\u622a\u65ad", "\n", "df_w_tmp", "[", "'sku_title'", "]", "=", "df_w_tmp", "[", "'sku_title'", "]", ".", "apply", "(", "lambda", "x", ":", "x", "[", ":", "title_cut", "]", ")", "\n", "df_w_tmp", "[", "'recommend_reason'", "]", "=", "df_w_tmp", "[", "'recommend_reason'", "]", ".", "apply", "(", "lambda", "x", ":", "x", "[", ":", "wenan_cut", "]", ")", "\n", "\n", "df_tmp", "[", "'sku_title'", "]", "=", "df_tmp", "[", "'sku_title'", "]", ".", "apply", "(", "lambda", "x", ":", "x", "[", ":", "title_cut", "]", ")", "\n", "df_tmp", "[", "'attr_merge'", "]", "=", "df_tmp", "[", "'attr_merge'", "]", ".", "apply", "(", "lambda", "x", ":", "x", "[", ":", "attr_cut", "]", ")", "\n", "print", "(", "len", "(", "df_w_tmp", ")", ",", "len", "(", "df_tmp", ")", ")", "\n", "\n", "## \u6784\u5efa\u5b57\u5178sku--id, sku\u5bf9\u5e94\u591a\u6761\u6587\u6848\u6570\u636e\u4e2d\uff0c\u6bcf\u6761\u6570\u636e\u6709\u552f\u4e00\u7684id,\u65b9\u4fbf\u540e\u9762\u6784\u5efa\u8d1f\u4f8b", "\n", "sku_id_dic", "=", "defaultdict", "(", "list", ")", "\n", "id_list", "=", "list", "(", "df_w_tmp", "[", "'id'", "]", ")", "\n", "sku_list", "=", "list", "(", "df_w_tmp", "[", "'sku'", "]", ")", "\n", "for", "i", "in", "range", "(", "len", "(", "df_w_tmp", ")", ")", ":", "\n", "        ", "sku_id_dic", "[", "sku_list", "[", "i", "]", "]", ".", "append", "(", "id_list", "[", "i", "]", ")", "\n", "", "print", "(", "len", "(", "sku_id_dic", ")", ")", "\n", "\n", "## \u968f\u673a\u9009\u53d6\u4e00\u534a\u7684sku\u7528\u4f5cbi-\u5546\u54c1\u5c5e\u6027\u4e00\u81f4\u6027\u4efb\u52a1\uff0c\u53e6\u5916\u4e00\u534a\u7528\u4f5cs2s-\u5546\u54c1\u6587\u6848\u4efb\u52a1", "\n", "df_tmp", "=", "df_tmp", ".", "sample", "(", "frac", "=", "1", ",", "random_state", "=", "seed", ")", ".", "reset_index", "(", "drop", "=", "True", ")", "\n", "sku_all", "=", "list", "(", "df_tmp", "[", "'sku'", "]", ")", "\n", "sku_bi", "=", "sku_all", "[", ":", "len", "(", "sku_all", ")", "//", "2", "]", "\n", "sku_s2s", "=", "sku_all", "[", "len", "(", "sku_all", ")", "//", "2", ":", "]", "\n", "## \u901a\u8fc7\u8bbe\u7f6e\u9002\u5f53\u7684n\uff0c\u6784\u5efamax_len\u6570\u91cf\u76ee\u6807\u7684\u6570\u636e\u96c6\uff0c", "\n", "## \u4e00\u4e2asku\u5bf9\u5e94\u591a\u6761\u6587\u6848\u4e14\u6570\u91cf\u4e0d\u786e\u5b9a\uff0c\u76ee\u524d\u65b9\u6cd5\u53ef\u9009\u53d6\u5927\u81f4\u8303\u56f4\u5185\u6570\u91cf\u5408\u9002\u7684\u6570\u636e", "\n", "while", "n", ">", "2", ":", "\n", "        ", "s2s_id", "=", "[", "]", "\n", "for", "sku", "in", "sku_s2s", ":", "\n", "            ", "s2s_id", "+=", "sku_id_dic", "[", "sku", "]", "[", ":", "n", "]", "\n", "", "print", "(", "n", ",", "len", "(", "s2s_id", ")", ")", "\n", "#         if len(s2s_id) > len(df_tmp):", "\n", "if", "len", "(", "s2s_id", ")", ">", "max_len", "*", "3", ":", "\n", "            ", "n", "-=", "1", "\n", "", "else", ":", "\n", "            ", "break", "\n", "", "", "print", "(", "len", "(", "s2s_id", ")", ")", "\n", "## \u4fdd\u8bc1uni-\u6587\u6848\u4efb\u52a1\u4e2d\u4f7f\u7528\u7684\u6587\u6848\u548cs2s\u4efb\u52a1\u4e0d\u91cd\u5408", "\n", "uni_id", "=", "list", "(", "set", "(", "df_w_tmp", "[", "'id'", "]", ")", ".", "difference", "(", "set", "(", "s2s_id", ")", ")", ")", "\n", "print", "(", "len", "(", "uni_id", ")", ")", "\n", "## \u6700\u7ec8\u5e73\u8861\u5404\u4efb\u52a1\u6570\u636e\u91cf\uff0c\u4f7f\u4e09\u4e2a\u4efb\u52a1\u6570\u636e\u91cf\u4e00\u81f4", "\n", "max_len", "=", "min", "(", "len", "(", "uni_id", ")", ",", "len", "(", "s2s_id", ")", ",", "len", "(", "sku_bi", ")", "*", "2", ",", "max_len", ")", "\n", "uni_id", "=", "uni_id", "[", ":", "max_len", "]", "\n", "s2s_id", "=", "s2s_id", "[", ":", "max_len", "]", "\n", "sku_bi", "=", "sku_bi", "[", ":", "max_len", "//", "2", "]", "\n", "print", "(", "len", "(", "uni_id", ")", ",", "len", "(", "s2s_id", ")", ",", "len", "(", "sku_bi", ")", ")", "\n", "\n", "## bi-\u5546\u54c1\u5c5e\u6027\u4e00\u81f4\u6027\u4efb\u52a1, \u6b63\u8d1f\u4f8b\u6784\u5efa", "\n", "cat_dic", "=", "defaultdict", "(", "list", ")", "\n", "sku_cat_dic", "=", "{", "}", "\n", "sku_title_dic", "=", "{", "}", "\n", "sku_attr_dic", "=", "{", "}", "\n", "sku_list", ",", "cat_list", "=", "list", "(", "df_tmp", "[", "'sku'", "]", ")", ",", "list", "(", "df_tmp", "[", "'item_second_cate_name'", "]", ")", "\n", "title_list", ",", "attr_list", "=", "list", "(", "df_tmp", "[", "'sku_title'", "]", ")", ",", "list", "(", "df_tmp", "[", "'attr_merge'", "]", ")", "\n", "for", "i", "in", "range", "(", "len", "(", "sku_list", ")", ")", ":", "\n", "        ", "cat_dic", "[", "cat_list", "[", "i", "]", "]", ".", "append", "(", "sku_list", "[", "i", "]", ")", "\n", "sku_cat_dic", "[", "sku_list", "[", "i", "]", "]", "=", "cat_list", "[", "i", "]", "\n", "sku_title_dic", "[", "sku_list", "[", "i", "]", "]", "=", "title_list", "[", "i", "]", "\n", "sku_attr_dic", "[", "sku_list", "[", "i", "]", "]", "=", "attr_list", "[", "i", "]", "\n", "\n", "", "result_pos", "=", "[", "]", "\n", "random", ".", "shuffle", "(", "sku_bi", ")", "\n", "for", "i", "in", "range", "(", "len", "(", "sku_bi", ")", ")", ":", "\n", "        ", "sku", "=", "sku_bi", "[", "i", "]", "\n", "attr", "=", "sku_attr_dic", "[", "sku", "]", "\n", "if", "i", "%", "2", "==", "0", ":", "\n", "            ", "attr_list", "=", "attr", ".", "split", "(", "'|'", ")", "\n", "random", ".", "shuffle", "(", "attr_list", ")", "\n", "attr", "=", "'|'", ".", "join", "(", "attr_list", ")", "\n", "", "result_pos", ".", "append", "(", "[", "sku_title_dic", "[", "sku", "]", ",", "attr", ",", "0", "]", ")", "\n", "", "result_neg", "=", "[", "]", "\n", "random", ".", "shuffle", "(", "sku_bi", ")", "\n", "for", "i", "in", "range", "(", "len", "(", "sku_bi", ")", ")", ":", "\n", "        ", "sku", "=", "sku_bi", "[", "i", "]", "\n", "if", "i", "%", "2", "==", "0", ":", "\n", "            ", "pool", "=", "cat_dic", "[", "sku_cat_dic", "[", "sku", "]", "]", "\n", "", "else", ":", "\n", "            ", "pool", "=", "sku_list", "\n", "", "sku_pair", "=", "random", ".", "choice", "(", "pool", ")", "\n", "result_neg", ".", "append", "(", "[", "sku_title_dic", "[", "sku", "]", ",", "sku_attr_dic", "[", "sku_pair", "]", ",", "1", "]", ")", "\n", "\n", "", "result_all", "=", "result_pos", "+", "result_neg", "\n", "df_bi", "=", "pd", ".", "DataFrame", "(", "result_all", ")", "\n", "df_bi", ".", "columns", "=", "[", "'title'", ",", "'attr'", ",", "'nsp'", "]", "\n", "df_bi", "=", "df_bi", ".", "sample", "(", "frac", "=", "1", ",", "random_state", "=", "4321", ")", ".", "reset_index", "(", "drop", "=", "True", ")", "\n", "\n", "df_s2s", "=", "df_w_tmp", "[", "df_w_tmp", "[", "'id'", "]", ".", "isin", "(", "set", "(", "s2s_id", ")", ")", "]", "[", "[", "'id'", ",", "'sku'", ",", "'recommend_reason'", "]", "]", "\n", "print", "(", "len", "(", "df_s2s", ")", ")", "\n", "df_s2s", "=", "df_s2s", ".", "merge", "(", "df_tmp", "[", "[", "'sku'", ",", "'sku_title'", ",", "'attr_merge'", "]", "]", ",", "how", "=", "'left'", ",", "on", "=", "'sku'", ")", "\n", "print", "(", "len", "(", "df_s2s", ")", ")", "\n", "df_s2s", "[", "'input'", "]", "=", "df_s2s", "[", "'sku_title'", "]", "+", "df_s2s", "[", "'attr_merge'", "]", "\n", "df_uni", "=", "df_w_tmp", "[", "df_w_tmp", "[", "'id'", "]", ".", "isin", "(", "set", "(", "uni_id", ")", ")", "]", "[", "[", "'id'", ",", "'sku'", ",", "'recommend_reason'", "]", "]", "\n", "print", "(", "len", "(", "df_bi", ")", ",", "len", "(", "df_s2s", ")", ",", "len", "(", "df_uni", ")", ")", "\n", "return", "df_bi", ",", "df_s2s", ",", "df_uni", "\n", "\n"]], "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.data_preprocess.data_preprocess.sentence_reorder": [[214, 218], ["x.strip().split", "random.shuffle", "x.strip"], "function", ["home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.preprocess.process_daren.split"], ["", "def", "sentence_reorder", "(", "x", ")", ":", "\n", "    ", "item_list", "=", "x", ".", "strip", "(", "'\u3002'", ")", ".", "split", "(", "'\u3002'", ")", "\n", "random", ".", "shuffle", "(", "item_list", ")", "\n", "return", "'\u3002'", ".", "join", "(", "item_list", ")", "+", "'\u3002'", "\n", "\n"]], "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.data_preprocess.data_preprocess.get_df_unilm_add2task": [[222, 381], ["print", "print", "df_w_tmp[].apply", "df_w_tmp[].apply", "df_tmp[].apply", "df_tmp[].apply", "print", "collections.defaultdict", "list", "list", "range", "print", "df_tmp.sample().reset_index.sample().reset_index", "list", "print", "list", "print", "min", "print", "collections.defaultdict", "range", "range", "pandas.DataFrame", "df_bi_att.sample().reset_index.sample().reset_index", "print", "merge_df.merge", "print", "df_s2s_order[].apply", "print", "data_preprocess.merge_df", "df_w_tmp[].sample().reset_index().drop_duplicates", "range", "print", "list", "range", "pandas.DataFrame", "df_bi_wenan.sample().reset_index.sample().reset_index", "data_preprocess.merge_df", "print", "len", "len", "df_wenan.drop_duplicates", "len", "len", "len", "len", "len", "sku_id_dic[].append", "len", "print", "len", "set().difference", "len", "len", "len", "len", "len", "len", "len", "len", "list", "list", "list", "list", "len", "cat_dic[].append", "len", "random.random", "len", "len", "len", "list", "list", "len", "int", "list", "len", "list", "sku_wenan_dic.keys", "len", "int", "random.random", "len", "len", "len", "df_w_tmp[].apply", "df_tmp[].apply", "df_tmp[].apply", "df_w_tmp[].apply", "df_tmp.sample().reset_index.sample", "len", "len", "set", "len", "result_all.append", "random.choice", "result_all.append", "df_bi_att.sample().reset_index.sample", "data_preprocess.sentence_reorder", "df_w_tmp[].sample().reset_index", "sku_wenan_dic.items", "set().difference().intersection", "set().intersection", "collections.defaultdict.items", "result_all.append", "random.choice", "result_all.append", "df_bi_wenan.sample().reset_index.sample", "len", "len", "set", "attr.split", "random.shuffle", "df_w_tmp[].isin", "df_w_tmp[].isin", "df_w_tmp[].isin", "str", "sku_wenan_dic.keys", "sku_wenan_dic.keys", "len", "set", "set", "set", "df_w_tmp[].sample", "set().difference", "set", "set", "len", "len", "len", "len", "len", "len", "len", "len", "set", "str", "str", "str", "str", "str", "str", "str", "str"], "function", ["home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.data_preprocess.data_preprocess.merge_df", "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.data_preprocess.data_preprocess.merge_df", "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.data_preprocess.data_preprocess.sentence_reorder", "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.preprocess.process_daren.split"], ["", "def", "get_df_unilm_add2task", "(", "df_wenan", ",", "df_item", ",", "max_len", ",", "n", "=", "15", ",", "seed", "=", "1234", ",", "\n", "wenan_min", "=", "20", ",", "wenan_max", "=", "90", ",", "wenan_cut", "=", "90", ",", "\n", "attr_min", "=", "10", ",", "attr_max", "=", "80", ",", "attr_cut", "=", "80", ",", "\n", "title_min", "=", "5", ",", "title_max", "=", "80", ",", "title_cut", "=", "80", ",", "drop_dup_ad", "=", "True", ")", ":", "\n", "    ", "print", "(", "len", "(", "df_wenan", ")", ",", "len", "(", "df_item", ")", ")", "\n", "df_tmp", "=", "df_item", "[", "[", "'sku'", ",", "'sku_title'", ",", "'attr_merge'", ",", "'item_second_cate_name'", "]", "]", "\n", "if", "drop_dup_ad", ":", "\n", "        ", "df_w_tmp", "=", "df_wenan", ".", "drop_duplicates", "(", "'recommend_reason'", ")", "\n", "", "else", ":", "\n", "        ", "df_w_tmp", "=", "df_wenan", "\n", "", "print", "(", "len", "(", "df_w_tmp", ")", ",", "len", "(", "df_tmp", ")", ")", "\n", "df_w_tmp", "=", "df_w_tmp", "[", "df_w_tmp", "[", "'recommend_reason'", "]", ".", "apply", "(", "lambda", "x", ":", "len", "(", "str", "(", "x", ")", ")", ">", "wenan_min", "and", "len", "(", "str", "(", "x", ")", ")", "<", "wenan_max", ")", "]", "\n", "df_tmp", "=", "df_tmp", "[", "df_tmp", "[", "'attr_merge'", "]", ".", "apply", "(", "lambda", "x", ":", "len", "(", "str", "(", "x", ")", ")", ">", "attr_min", "and", "len", "(", "str", "(", "x", ")", ")", "<", "attr_max", ")", "]", "\n", "df_tmp", "=", "df_tmp", "[", "df_tmp", "[", "'sku_title'", "]", ".", "apply", "(", "lambda", "x", ":", "len", "(", "str", "(", "x", ")", ")", ">", "title_min", "and", "len", "(", "str", "(", "x", ")", ")", "<", "title_max", ")", "]", "\n", "df_w_tmp", "=", "df_w_tmp", "[", "df_w_tmp", "[", "'sku_title'", "]", ".", "apply", "(", "lambda", "x", ":", "len", "(", "str", "(", "x", ")", ")", ">", "title_min", "and", "len", "(", "str", "(", "x", ")", ")", "<", "title_max", ")", "]", "\n", "df_w_tmp", "[", "'sku_title'", "]", "=", "df_w_tmp", "[", "'sku_title'", "]", ".", "apply", "(", "lambda", "x", ":", "x", "[", ":", "title_cut", "]", ")", "\n", "df_w_tmp", "[", "'recommend_reason'", "]", "=", "df_w_tmp", "[", "'recommend_reason'", "]", ".", "apply", "(", "lambda", "x", ":", "x", "[", ":", "wenan_cut", "]", ")", "\n", "\n", "df_tmp", "[", "'sku_title'", "]", "=", "df_tmp", "[", "'sku_title'", "]", ".", "apply", "(", "lambda", "x", ":", "x", "[", ":", "title_cut", "]", ")", "\n", "df_tmp", "[", "'attr_merge'", "]", "=", "df_tmp", "[", "'attr_merge'", "]", ".", "apply", "(", "lambda", "x", ":", "x", "[", ":", "attr_cut", "]", ")", "\n", "print", "(", "len", "(", "df_w_tmp", ")", ",", "len", "(", "df_tmp", ")", ")", "\n", "\n", "sku_id_dic", "=", "defaultdict", "(", "list", ")", "\n", "id_list", "=", "list", "(", "df_w_tmp", "[", "'id'", "]", ")", "\n", "sku_list", "=", "list", "(", "df_w_tmp", "[", "'sku'", "]", ")", "\n", "for", "i", "in", "range", "(", "len", "(", "df_w_tmp", ")", ")", ":", "\n", "        ", "sku_id_dic", "[", "sku_list", "[", "i", "]", "]", ".", "append", "(", "id_list", "[", "i", "]", ")", "\n", "", "print", "(", "len", "(", "sku_id_dic", ")", ")", "\n", "\n", "\n", "df_tmp", "=", "df_tmp", ".", "sample", "(", "frac", "=", "1", ",", "random_state", "=", "seed", ")", ".", "reset_index", "(", "drop", "=", "True", ")", "\n", "sku_all", "=", "list", "(", "df_tmp", "[", "'sku'", "]", ")", "\n", "sku_bi", "=", "sku_all", "[", ":", "len", "(", "sku_all", ")", "//", "2", "]", "\n", "sku_s2s", "=", "sku_all", "[", "len", "(", "sku_all", ")", "//", "2", ":", "]", "\n", "#     n = 50", "\n", "while", "n", ">", "2", ":", "\n", "        ", "s2s_id", "=", "[", "]", "\n", "for", "sku", "in", "sku_s2s", ":", "\n", "            ", "s2s_id", "+=", "sku_id_dic", "[", "sku", "]", "[", ":", "n", "]", "\n", "", "print", "(", "n", ",", "len", "(", "s2s_id", ")", ")", "\n", "#         if len(s2s_id) > len(df_tmp):", "\n", "if", "len", "(", "s2s_id", ")", ">", "max_len", "*", "3", ":", "\n", "            ", "n", "-=", "1", "\n", "", "else", ":", "\n", "            ", "break", "\n", "", "", "print", "(", "len", "(", "s2s_id", ")", ")", "\n", "uni_id", "=", "list", "(", "set", "(", "df_w_tmp", "[", "'id'", "]", ")", ".", "difference", "(", "set", "(", "s2s_id", ")", ")", ")", "\n", "print", "(", "len", "(", "uni_id", ")", ")", "\n", "max_len", "=", "min", "(", "len", "(", "uni_id", ")", ",", "len", "(", "s2s_id", ")", ",", "len", "(", "sku_bi", ")", "*", "2", ",", "max_len", ")", "\n", "uni_id", "=", "uni_id", "[", ":", "max_len", "]", "\n", "s2s_id", "=", "s2s_id", "[", ":", "max_len", "]", "\n", "sku_bi", "=", "sku_bi", "[", ":", "max_len", "//", "2", "]", "\n", "\n", "## \u4eces2s_id\u4e2d\u9009\u62e9\u5404\u4e00\u534a\u7528\u4f5c\u4e24\u4e2a\u4efb\u52a1: s2s-\u5546\u54c1\u6587\u6848\uff0cs2s-\u6253\u4e71\u53e5\u5b50\u987a\u5e8f", "\n", "s2s_id_wenan", "=", "s2s_id", "[", ":", "max_len", "//", "2", "]", "\n", "s2s_id_order", "=", "s2s_id", "[", "max_len", "//", "2", ":", "]", "\n", "\n", "print", "(", "len", "(", "uni_id", ")", ",", "len", "(", "s2s_id", ")", ",", "len", "(", "sku_bi", ")", ",", "len", "(", "s2s_id_wenan", ")", ",", "len", "(", "s2s_id_order", ")", ")", "\n", "\n", "cat_dic", "=", "defaultdict", "(", "list", ")", "\n", "sku_cat_dic", "=", "{", "}", "\n", "sku_title_dic", "=", "{", "}", "\n", "sku_attr_dic", "=", "{", "}", "\n", "sku_list", ",", "cat_list", "=", "list", "(", "df_tmp", "[", "'sku'", "]", ")", ",", "list", "(", "df_tmp", "[", "'item_second_cate_name'", "]", ")", "\n", "title_list", ",", "attr_list", "=", "list", "(", "df_tmp", "[", "'sku_title'", "]", ")", ",", "list", "(", "df_tmp", "[", "'attr_merge'", "]", ")", "\n", "for", "i", "in", "range", "(", "len", "(", "sku_list", ")", ")", ":", "\n", "        ", "cat_dic", "[", "cat_list", "[", "i", "]", "]", ".", "append", "(", "sku_list", "[", "i", "]", ")", "\n", "sku_cat_dic", "[", "sku_list", "[", "i", "]", "]", "=", "cat_list", "[", "i", "]", "\n", "sku_title_dic", "[", "sku_list", "[", "i", "]", "]", "=", "title_list", "[", "i", "]", "\n", "sku_attr_dic", "[", "sku_list", "[", "i", "]", "]", "=", "attr_list", "[", "i", "]", "\n", "\n", "\n", "## get data for task: product / attribute relevance", "\n", "", "result_all", "=", "[", "]", "\n", "for", "i", "in", "range", "(", "len", "(", "sku_bi", ")", ")", ":", "\n", "        ", "sku", "=", "sku_bi", "[", "i", "]", "\n", "random_thres", "=", "random", ".", "random", "(", ")", "\n", "if", "random_thres", ">", "0.5", ":", "\n", "            ", "attr", "=", "sku_attr_dic", "[", "sku", "]", "\n", "if", "random_thres", ">", "0.75", ":", "\n", "                ", "attr_list", "=", "attr", ".", "split", "(", "'|'", ")", "\n", "random", ".", "shuffle", "(", "attr_list", ")", "\n", "attr", "=", "'|'", ".", "join", "(", "attr_list", ")", "\n", "", "result_all", ".", "append", "(", "[", "sku_title_dic", "[", "sku", "]", ",", "attr", ",", "0", "]", ")", "\n", "", "else", ":", "\n", "            ", "if", "random_thres", "<", "0.25", ":", "\n", "                ", "pool", "=", "cat_dic", "[", "sku_cat_dic", "[", "sku", "]", "]", "\n", "", "else", ":", "\n", "                ", "pool", "=", "sku_list", "\n", "", "sku_pair", "=", "random", ".", "choice", "(", "pool", ")", "\n", "result_all", ".", "append", "(", "[", "sku_title_dic", "[", "sku", "]", ",", "sku_attr_dic", "[", "sku_pair", "]", ",", "1", "]", ")", "\n", "\n", "", "", "df_bi_att", "=", "pd", ".", "DataFrame", "(", "result_all", ")", "\n", "df_bi_att", ".", "columns", "=", "[", "'title'", ",", "'attr'", ",", "'nsp'", "]", "\n", "df_bi_att", "=", "df_bi_att", ".", "sample", "(", "frac", "=", "1", ",", "random_state", "=", "4321", ")", ".", "reset_index", "(", "drop", "=", "True", ")", "\n", "\n", "## get data for task s2s", "\n", "df_s2s", "=", "df_w_tmp", "[", "df_w_tmp", "[", "'id'", "]", ".", "isin", "(", "set", "(", "s2s_id_wenan", ")", ")", "]", "[", "[", "'id'", ",", "'sku'", ",", "'recommend_reason'", "]", "]", "\n", "print", "(", "len", "(", "df_s2s", ")", ")", "\n", "df_s2s", "=", "df_s2s", ".", "merge", "(", "df_tmp", "[", "[", "'sku'", ",", "'sku_title'", ",", "'attr_merge'", "]", "]", ",", "how", "=", "'left'", ",", "on", "=", "'sku'", ")", "\n", "print", "(", "len", "(", "df_s2s", ")", ")", "\n", "df_s2s", "[", "'input'", "]", "=", "df_s2s", "[", "'sku_title'", "]", "+", "df_s2s", "[", "'attr_merge'", "]", "\n", "\n", "## get data for task uni-bidirection", "\n", "df_uni", "=", "df_w_tmp", "[", "df_w_tmp", "[", "'id'", "]", ".", "isin", "(", "set", "(", "uni_id", ")", ")", "]", "[", "[", "'id'", ",", "'sku'", ",", "'recommend_reason'", "]", "]", "\n", "\n", "## get data for task sentence reorder", "\n", "df_s2s_order", "=", "df_w_tmp", "[", "df_w_tmp", "[", "'id'", "]", ".", "isin", "(", "set", "(", "s2s_id_order", ")", ")", "]", "[", "[", "'id'", ",", "'sku'", ",", "'recommend_reason'", "]", "]", "\n", "df_s2s_order", "[", "'input'", "]", "=", "df_s2s_order", "[", "'recommend_reason'", "]", ".", "apply", "(", "lambda", "x", ":", "sentence_reorder", "(", "x", ")", ")", "\n", "print", "(", "len", "(", "df_s2s_order", ")", ")", "\n", "\n", "## merge df_s2s", "\n", "df_s2s", "=", "merge_df", "(", "df_s2s", "[", "[", "'id'", ",", "'sku'", ",", "'input'", ",", "'recommend_reason'", "]", "]", ",", "df_s2s_order", "[", "[", "'id'", ",", "'sku'", ",", "'input'", ",", "'recommend_reason'", "]", "]", ")", "\n", "\n", "\n", "## bi-\u5546\u54c1\u6587\u6848\u4e00\u81f4\u6027\u4efb\u52a1, \u6b63\u8d1f\u4f8b\u6784\u5efa", "\n", "df_w_tmp_uniq", "=", "df_w_tmp", "[", "[", "'sku'", ",", "'recommend_reason'", "]", "]", ".", "sample", "(", "frac", "=", "1", ",", "random_state", "=", "seed", ")", ".", "reset_index", "(", "drop", "=", "True", ")", ".", "drop_duplicates", "(", "'sku'", ")", "\n", "sku_wenan_dic", "=", "{", "}", "\n", "sku_w_list", ",", "wenan_w_list", "=", "list", "(", "df_w_tmp_uniq", "[", "'sku'", "]", ")", ",", "list", "(", "df_w_tmp_uniq", "[", "'recommend_reason'", "]", ")", "\n", "for", "i", "in", "range", "(", "len", "(", "sku_w_list", ")", ")", ":", "\n", "        ", "sku_wenan_dic", "[", "str", "(", "sku_w_list", "[", "i", "]", ")", "]", "=", "wenan_w_list", "[", "i", "]", "\n", "\n", "", "sku_wenan_dic", "=", "{", "int", "(", "key", ")", ":", "value", "for", "key", ",", "value", "in", "sku_wenan_dic", ".", "items", "(", ")", "}", "\n", "sku_wenan_relevance", "=", "list", "(", "set", "(", "sku_s2s", ")", ".", "difference", "(", "set", "(", "df_s2s", "[", "'sku'", "]", ")", ")", ".", "intersection", "(", "sku_wenan_dic", ".", "keys", "(", ")", ")", ")", "[", ":", "max_len", "//", "2", "]", "\n", "print", "(", "len", "(", "sku_wenan_relevance", ")", ")", "\n", "cat_wenan_dic", "=", "{", "key", ":", "list", "(", "set", "(", "value", ")", ".", "intersection", "(", "sku_wenan_dic", ".", "keys", "(", ")", ")", ")", "for", "key", ",", "value", "in", "cat_dic", ".", "items", "(", ")", "}", "\n", "\n", "sku_wenan_list", "=", "list", "(", "sku_wenan_dic", ".", "keys", "(", ")", ")", "\n", "\n", "result_all", "=", "[", "]", "\n", "for", "i", "in", "range", "(", "len", "(", "sku_wenan_relevance", ")", ")", ":", "\n", "        ", "sku", "=", "int", "(", "sku_wenan_relevance", "[", "i", "]", ")", "\n", "attr", "=", "sku_attr_dic", "[", "sku", "]", "\n", "random_thres", "=", "random", ".", "random", "(", ")", "\n", "if", "random_thres", ">", "0.5", ":", "\n", "            ", "result_all", ".", "append", "(", "[", "sku_title_dic", "[", "sku", "]", ",", "attr", ",", "sku_wenan_dic", "[", "sku", "]", ",", "0", "]", ")", "\n", "", "else", ":", "\n", "            ", "if", "random_thres", "<", "0.25", ":", "\n", "                ", "pool", "=", "cat_wenan_dic", "[", "sku_cat_dic", "[", "sku", "]", "]", "\n", "", "else", ":", "\n", "                ", "pool", "=", "sku_wenan_list", "\n", "", "if", "len", "(", "pool", ")", "==", "0", ":", "\n", "                ", "pool", "=", "sku_wenan_list", "\n", "", "sku_pair", "=", "random", ".", "choice", "(", "pool", ")", "\n", "result_all", ".", "append", "(", "[", "sku_title_dic", "[", "sku", "]", ",", "attr", ",", "sku_wenan_dic", "[", "sku_pair", "]", ",", "1", "]", ")", "\n", "\n", "", "", "df_bi_wenan", "=", "pd", ".", "DataFrame", "(", "result_all", ")", "\n", "df_bi_wenan", ".", "columns", "=", "[", "'title'", ",", "'attr'", ",", "'wenan'", ",", "'nsp'", "]", "\n", "df_bi_wenan", "[", "'seq1'", "]", "=", "df_bi_wenan", "[", "'title'", "]", "+", "df_bi_wenan", "[", "'attr'", "]", "\n", "df_bi_wenan", "[", "'seq2'", "]", "=", "df_bi_wenan", "[", "'wenan'", "]", "\n", "df_bi_wenan", "=", "df_bi_wenan", ".", "sample", "(", "frac", "=", "1", ",", "random_state", "=", "4321", ")", ".", "reset_index", "(", "drop", "=", "True", ")", "\n", "\n", "## merge df_bi", "\n", "df_bi", "=", "df_bi_att", "[", "[", "'title'", ",", "'attr'", ",", "'nsp'", "]", "]", "\n", "df_bi", ".", "columns", "=", "[", "'seq1'", ",", "'seq2'", ",", "'nsp'", "]", "\n", "df_bi", "=", "merge_df", "(", "df_bi", ",", "df_bi_wenan", "[", "[", "'seq1'", ",", "'seq2'", ",", "'nsp'", "]", "]", ")", "\n", "\n", "print", "(", "len", "(", "df_bi", ")", ",", "len", "(", "df_s2s", ")", ",", "len", "(", "df_uni", ")", ")", "\n", "return", "df_bi", ",", "df_s2s", ",", "df_uni", "\n", "\n"]], "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.data_preprocess.data_preprocess.merge_df": [[384, 388], ["pandas.concat", "df.sample().reset_index.sample().reset_index", "df.sample().reset_index.sample"], "function", ["None"], ["", "def", "merge_df", "(", "df1", ",", "df2", ",", "seed", "=", "1234", ")", ":", "\n", "    ", "df", "=", "pd", ".", "concat", "(", "[", "df1", ",", "df2", "]", ",", "axis", "=", "0", ")", "\n", "df", "=", "df", ".", "sample", "(", "frac", "=", "1", ",", "random_state", "=", "seed", ")", ".", "reset_index", "(", "drop", "=", "True", ")", "\n", "return", "df", "\n", "\n"]], "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.data_preprocess.data_preprocess.get_unilm_v2": [[390, 412], ["data_preprocess.get_df", "data_preprocess.get_df", "pandas.read_csv", "data_preprocess.get_item_ad", "data_preprocess.get_item_ocr", "data_preprocess.get_df_unilm", "data_preprocess.get_df_unilm", "data_preprocess.merge_df", "data_preprocess.merge_df", "data_preprocess.merge_df", "print", "merge_df.to_csv", "merge_df.to_csv", "merge_df.to_csv", "len", "len", "len", "os.path.join", "os.path.join", "os.path.join"], "function", ["home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.data_preprocess.data_preprocess.get_df", "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.data_preprocess.data_preprocess.get_df", "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.data_preprocess.data_preprocess.get_item_ad", "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.data_preprocess.data_preprocess.get_item_ocr", "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.data_preprocess.data_preprocess.get_df_unilm", "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.data_preprocess.data_preprocess.get_df_unilm", "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.data_preprocess.data_preprocess.merge_df", "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.data_preprocess.data_preprocess.merge_df", "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.data_preprocess.data_preprocess.merge_df"], ["", "def", "get_unilm_v2", "(", "data_dir", ",", "\n", "output_dir", "=", "'/export/Data/zhangxueying17/smart_shopping/pretrain/unilm_v2/'", ")", ":", "\n", "    ", "df_info", "=", "get_df", "(", "data_dir", ",", "'pretrain_all_select'", ",", "-", "1", ")", "\n", "df_ocr", "=", "get_df", "(", "data_dir", ",", "'pretrain_ocr'", ",", "-", "1", ")", "\n", "df_ocr_info", "=", "pd", ".", "read_csv", "(", "'df_ocr_info.tsv'", ",", "sep", "=", "'\\t'", ")", "\n", "\n", "df_wenan", ",", "df_item", "=", "get_item_ad", "(", "df_info", ",", "type", "=", "'train'", ")", "\n", "df_wenan_o", ",", "df_item_o", "=", "get_item_ocr", "(", "df_ocr", ",", "df_ocr_info", ")", "\n", "\n", "df_bi_a", ",", "df_s2s_a", ",", "df_uni_a", "=", "get_df_unilm", "(", "df_wenan", ",", "df_item", ",", "1700000", ")", "\n", "df_bi_o", ",", "df_s2s_o", ",", "df_uni_o", "=", "get_df_unilm", "(", "df_wenan_o", ",", "df_item_o", ",", "1700000", ",", "15", ",", "\n", "wenan_max", "=", "1000", ",", "attr_max", "=", "1000", ",", "title_max", "=", "1000", ")", "\n", "\n", "df_bi", "=", "merge_df", "(", "df_bi_a", ",", "df_bi_o", ")", "\n", "df_s2s", "=", "merge_df", "(", "df_s2s_a", ",", "df_s2s_o", ")", "\n", "df_uni", "=", "merge_df", "(", "df_uni_a", ",", "df_uni_o", ")", "\n", "\n", "print", "(", "len", "(", "df_s2s", ")", ",", "len", "(", "df_bi", ")", ",", "len", "(", "df_uni", ")", ")", "\n", "\n", "df_bi", ".", "to_csv", "(", "os", ".", "path", ".", "join", "(", "output_dir", ",", "'bi.tsv'", ")", ",", "sep", "=", "'\\t'", ",", "index", "=", "None", ")", "\n", "df_uni", ".", "to_csv", "(", "os", ".", "path", ".", "join", "(", "output_dir", ",", "'uni.tsv'", ")", ",", "sep", "=", "'\\t'", ",", "index", "=", "None", ")", "\n", "df_s2s", ".", "to_csv", "(", "os", ".", "path", ".", "join", "(", "output_dir", ",", "'s2s.tsv'", ")", ",", "sep", "=", "'\\t'", ",", "index", "=", "None", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.data_preprocess.data_preprocess.get_unilm_v3": [[414, 439], ["data_preprocess.get_df", "data_preprocess.get_df", "pandas.read_csv", "data_preprocess.get_item_ad", "data_preprocess.get_item_ocr", "data_preprocess.get_df_unilm_add2task", "data_preprocess.get_df_unilm", "data_preprocess.merge_df", "data_preprocess.merge_df", "data_preprocess.merge_df", "print", "merge_df.to_csv", "merge_df.to_csv", "merge_df.to_csv", "os.path.join", "len", "len", "len", "os.path.join", "os.path.join", "os.path.join"], "function", ["home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.data_preprocess.data_preprocess.get_df", "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.data_preprocess.data_preprocess.get_df", "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.data_preprocess.data_preprocess.get_item_ad", "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.data_preprocess.data_preprocess.get_item_ocr", "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.data_preprocess.data_preprocess.get_df_unilm_add2task", "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.data_preprocess.data_preprocess.get_df_unilm", "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.data_preprocess.data_preprocess.merge_df", "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.data_preprocess.data_preprocess.merge_df", "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.data_preprocess.data_preprocess.merge_df"], ["", "def", "get_unilm_v3", "(", "data_dir", ",", "\n", "output_dir", "=", "'/export/Data/zhangxueying17/smart_shopping/pretrain/unilm_v3/'", ")", ":", "\n", "\n", "    ", "df_info", "=", "get_df", "(", "data_dir", ",", "'pretrain_all_select'", ",", "-", "1", ")", "\n", "df_ocr", "=", "get_df", "(", "data_dir", ",", "'pretrain_ocr'", ",", "-", "1", ")", "\n", "df_ocr_info", "=", "pd", ".", "read_csv", "(", "os", ".", "path", ".", "join", "(", "data_dir", ",", "'df_ocr_info.tsv'", ")", ",", "sep", "=", "'\\t'", ")", "\n", "\n", "df_wenan", ",", "df_item", "=", "get_item_ad", "(", "df_info", ",", "type", "=", "'train'", ")", "\n", "df_wenan_o", ",", "df_item_o", "=", "get_item_ocr", "(", "df_ocr", ",", "df_ocr_info", ")", "\n", "\n", "df_bi_a", ",", "df_s2s_a", ",", "df_uni_a", "=", "get_df_unilm_add2task", "(", "df_wenan", ",", "df_item", ",", "1700000", ")", "\n", "df_bi_o", ",", "df_s2s_o", ",", "df_uni_o", "=", "get_df_unilm", "(", "df_wenan_o", ",", "df_item_o", ",", "1700000", ",", "15", ",", "\n", "wenan_max", "=", "1000", ",", "attr_max", "=", "1000", ",", "title_max", "=", "1000", ")", "\n", "df_bi_o", ".", "columns", "=", "[", "'seq1'", ",", "'seq2'", ",", "'nsp'", "]", "\n", "df_s2s_o", "=", "df_s2s_o", "[", "[", "'id'", ",", "'sku'", ",", "'input'", ",", "'recommend_reason'", "]", "]", "\n", "\n", "df_bi", "=", "merge_df", "(", "df_bi_a", ",", "df_bi_o", ")", "\n", "df_s2s", "=", "merge_df", "(", "df_s2s_a", ",", "df_s2s_o", ")", "\n", "df_uni", "=", "merge_df", "(", "df_uni_a", ",", "df_uni_o", ")", "\n", "\n", "print", "(", "len", "(", "df_s2s", ")", ",", "len", "(", "df_bi", ")", ",", "len", "(", "df_uni", ")", ")", "\n", "\n", "df_bi", ".", "to_csv", "(", "os", ".", "path", ".", "join", "(", "output_dir", ",", "'bi.tsv'", ")", ",", "sep", "=", "'\\t'", ",", "index", "=", "None", ")", "\n", "df_uni", ".", "to_csv", "(", "os", ".", "path", ".", "join", "(", "output_dir", ",", "'uni.tsv'", ")", ",", "sep", "=", "'\\t'", ",", "index", "=", "None", ")", "\n", "df_s2s", ".", "to_csv", "(", "os", ".", "path", ".", "join", "(", "output_dir", ",", "'s2s.tsv'", ")", ",", "sep", "=", "'\\t'", ",", "index", "=", "None", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.data_preprocess.data_preprocess.get_unilm_v2_eval": [[445, 457], ["data_preprocess.get_df", "data_preprocess.get_item_ad", "data_preprocess.get_df_unilm", "print", "df_bi.to_csv", "df_uni.to_csv", "df_s2s.to_csv", "len", "len", "len", "os.path.join", "os.path.join", "os.path.join"], "function", ["home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.data_preprocess.data_preprocess.get_df", "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.data_preprocess.data_preprocess.get_item_ad", "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.data_preprocess.data_preprocess.get_df_unilm"], ["", "def", "get_unilm_v2_eval", "(", "data_dir", ",", "\n", "output_dir", "=", "'/export/Data/zhangxueying17/smart_shopping/pretrain/unilm_v2_eval/'", ")", ":", "\n", "\n", "    ", "df_info", "=", "get_df", "(", "data_dir", ",", "'pretrain_all_select'", ",", "-", "1", ")", "\n", "df_wenan", ",", "df_item", "=", "get_item_ad", "(", "df_info", ",", "type", "=", "'eval'", ")", "\n", "df_bi", ",", "df_s2s", ",", "df_uni", "=", "get_df_unilm", "(", "df_wenan", ",", "df_item", ",", "50000", ")", "\n", "\n", "print", "(", "len", "(", "df_s2s", ")", ",", "len", "(", "df_bi", ")", ",", "len", "(", "df_uni", ")", ")", "\n", "\n", "df_bi", ".", "to_csv", "(", "os", ".", "path", ".", "join", "(", "output_dir", ",", "'bi.tsv'", ")", ",", "sep", "=", "'\\t'", ",", "index", "=", "None", ")", "\n", "df_uni", ".", "to_csv", "(", "os", ".", "path", ".", "join", "(", "output_dir", ",", "'uni.tsv'", ")", ",", "sep", "=", "'\\t'", ",", "index", "=", "None", ")", "\n", "df_s2s", ".", "to_csv", "(", "os", ".", "path", ".", "join", "(", "output_dir", ",", "'s2s.tsv'", ")", ",", "sep", "=", "'\\t'", ",", "index", "=", "None", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.data_preprocess.data_preprocess.get_unilm_v3_eval": [[459, 473], ["data_preprocess.get_df", "data_preprocess.get_item_ad", "data_preprocess.get_df_unilm_add2task", "print", "df_bi.to_csv", "df_uni.to_csv", "df_s2s.to_csv", "len", "len", "len", "os.path.join", "os.path.join", "os.path.join"], "function", ["home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.data_preprocess.data_preprocess.get_df", "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.data_preprocess.data_preprocess.get_item_ad", "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.data_preprocess.data_preprocess.get_df_unilm_add2task"], ["", "def", "get_unilm_v3_eval", "(", "data_dir", ",", "\n", "output_dir", "=", "'/export/Data/zhangxueying17/smart_shopping/pretrain/unilm_v3_eval/'", ")", ":", "\n", "\n", "    ", "df_info", "=", "get_df", "(", "data_dir", ",", "'pretrain_all_select'", ",", "-", "1", ")", "\n", "\n", "df_wenan", ",", "df_item", "=", "get_item_ad", "(", "df_info", ",", "type", "=", "'eval'", ")", "\n", "\n", "df_bi", ",", "df_s2s", ",", "df_uni", "=", "get_df_unilm_add2task", "(", "df_wenan", ",", "df_item", ",", "50000", ")", "\n", "\n", "print", "(", "len", "(", "df_s2s", ")", ",", "len", "(", "df_bi", ")", ",", "len", "(", "df_uni", ")", ")", "\n", "\n", "df_bi", ".", "to_csv", "(", "os", ".", "path", ".", "join", "(", "output_dir", ",", "'bi.tsv'", ")", ",", "sep", "=", "'\\t'", ",", "index", "=", "None", ")", "\n", "df_uni", ".", "to_csv", "(", "os", ".", "path", ".", "join", "(", "output_dir", ",", "'uni.tsv'", ")", ",", "sep", "=", "'\\t'", ",", "index", "=", "None", ")", "\n", "df_s2s", ".", "to_csv", "(", "os", ".", "path", ".", "join", "(", "output_dir", ",", "'s2s.tsv'", ")", ",", "sep", "=", "'\\t'", ",", "index", "=", "None", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.data_preprocess.data_preprocess.test": [[475, 478], ["print", "data_preprocess.sentence_reorder"], "function", ["home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.data_preprocess.data_preprocess.sentence_reorder"], ["", "def", "test", "(", ")", ":", "\n", "    ", "x", "=", "'\u5957\u88c5\u5408\u8ba18\u518c\uff0c\u7cbe\u9009\u6d77\u8c5a\u7ed8\u672c\u82b1\u56ed\u8bed\u8a00\u542f\u8499\u7c7b\u7ed8\u672c\u3002\u5e7c\u513f\u56ed3-6\u5c81\u5fc5\u5907\u3002\u4ece\u4e94\u5927\u9886\u57df\u4e2d\u7684\u8bed\u8a00\u9886\u57df\u51fa\u53d1\uff0c\u7740\u91cd\u8bed\u8a00\u542f\u8499\u80fd\u529b\u7684\u63d0\u5347\u3002\u5f15\u5bfc\u5b69\u5b50\u80fd\u8bf4\u3001\u6562\u8bf4\u3001\u4f1a\u8bf4\uff0c\u57f9\u517b\u524d\u9605\u8bfb\u548c\u524d\u4e66\u5199\u6280\u80fd\u3002'", "\n", "print", "(", "sentence_reorder", "(", "x", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.data_preprocess.data_preprocess.main": [[479, 486], ["data_preprocess.get_unilm_v3_eval"], "function", ["home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.data_preprocess.data_preprocess.get_unilm_v3_eval"], ["", "def", "main", "(", ")", ":", "\n", "## please change data_dir and output_dir", "\n", "    ", "data_dir", "=", "'/export/Data/zhangxueying17/smart_shopping/pretrain/'", "\n", "# get_unilm_v2(data_dir)", "\n", "# get_unilm_v2_eval(data_dir)", "\n", "# get_unilm_v3(data_dir)", "\n", "get_unilm_v3_eval", "(", "data_dir", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.biunilm.loader_utils.TrieNode.__init__": [[27, 30], ["None"], "methods", ["None"], ["    ", "def", "__init__", "(", "self", ")", ":", "\n", "        ", "self", ".", "children", "=", "{", "}", "\n", "self", ".", "is_leaf", "=", "False", "\n", "\n"]], "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.biunilm.loader_utils.TrieNode.try_get_children": [[31, 35], ["loader_utils.TrieNode"], "methods", ["None"], ["", "def", "try_get_children", "(", "self", ",", "key", ")", ":", "\n", "        ", "if", "key", "not", "in", "self", ".", "children", ":", "\n", "            ", "self", ".", "children", "[", "key", "]", "=", "TrieNode", "(", ")", "\n", "", "return", "self", ".", "children", "[", "key", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.biunilm.loader_utils.TrieTree.__init__": [[38, 40], ["loader_utils.TrieNode"], "methods", ["None"], ["    ", "def", "__init__", "(", "self", ")", ":", "\n", "        ", "self", ".", "root", "=", "TrieNode", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.biunilm.loader_utils.TrieTree.add": [[41, 46], ["r.try_get_children.try_get_children.try_get_children"], "methods", ["home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.biunilm.loader_utils.TrieNode.try_get_children"], ["", "def", "add", "(", "self", ",", "tokens", ")", ":", "\n", "        ", "r", "=", "self", ".", "root", "\n", "for", "token", "in", "tokens", ":", "\n", "            ", "r", "=", "r", ".", "try_get_children", "(", "token", ")", "\n", "", "r", ".", "is_leaf", "=", "True", "\n", "\n"]], "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.biunilm.loader_utils.TrieTree.get_pieces": [[47, 69], ["len", "pieces.append", "len", "list", "range"], "methods", ["None"], ["", "def", "get_pieces", "(", "self", ",", "tokens", ",", "offset", ")", ":", "\n", "        ", "pieces", "=", "[", "]", "\n", "r", "=", "self", ".", "root", "\n", "token_id", "=", "0", "\n", "last_valid", "=", "0", "\n", "match_count", "=", "0", "\n", "while", "last_valid", "<", "len", "(", "tokens", ")", ":", "\n", "            ", "if", "token_id", "<", "len", "(", "tokens", ")", "and", "tokens", "[", "token_id", "]", "in", "r", ".", "children", ":", "\n", "                ", "r", "=", "r", ".", "children", "[", "tokens", "[", "token_id", "]", "]", "\n", "match_count", "+=", "1", "\n", "if", "r", ".", "is_leaf", ":", "\n", "                    ", "last_valid", "=", "token_id", "\n", "", "token_id", "+=", "1", "\n", "", "else", ":", "\n", "                ", "pieces", ".", "append", "(", "\n", "list", "(", "range", "(", "token_id", "-", "match_count", "+", "offset", ",", "last_valid", "+", "1", "+", "offset", ")", ")", ")", "\n", "last_valid", "+=", "1", "\n", "token_id", "=", "last_valid", "\n", "r", "=", "self", ".", "root", "\n", "match_count", "=", "0", "\n", "\n", "", "", "return", "pieces", "\n", "\n"]], "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.biunilm.loader_utils.Pipeline.__init__": [[94, 110], ["super().__init__"], "methods", ["home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.unilm.eval.Eval.__init__"], ["def", "__init__", "(", "self", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "skipgram_prb", "=", "None", "\n", "self", ".", "skipgram_size", "=", "None", "\n", "self", ".", "pre_whole_word", "=", "None", "\n", "self", ".", "mask_whole_word", "=", "None", "\n", "self", ".", "word_subsample_prb", "=", "None", "\n", "self", ".", "sp_prob", "=", "None", "\n", "self", ".", "pieces_dir", "=", "None", "\n", "self", ".", "vocab_words", "=", "None", "\n", "self", ".", "pieces_threshold", "=", "10", "\n", "self", ".", "trie", "=", "None", "\n", "self", ".", "call_count", "=", "0", "\n", "self", ".", "offline_mode", "=", "False", "\n", "self", ".", "skipgram_size_geo_list", "=", "None", "\n", "self", ".", "span_same_mask", "=", "False", "\n", "\n"]], "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.biunilm.loader_utils.Pipeline.init_skipgram_size_geo_list": [[111, 120], ["range", "sum", "g_list.append"], "methods", ["None"], ["", "def", "init_skipgram_size_geo_list", "(", "self", ",", "p", ")", ":", "\n", "        ", "if", "p", ">", "0", ":", "\n", "            ", "g_list", "=", "[", "]", "\n", "t", "=", "p", "\n", "for", "_", "in", "range", "(", "self", ".", "skipgram_size", ")", ":", "\n", "                ", "g_list", ".", "append", "(", "t", ")", "\n", "t", "*=", "(", "1", "-", "p", ")", "\n", "", "s", "=", "sum", "(", "g_list", ")", "\n", "self", ".", "skipgram_size_geo_list", "=", "[", "x", "/", "s", "for", "x", "in", "g_list", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.biunilm.loader_utils.Pipeline.create_trie_tree": [[121, 140], ["print", "print", "loader_utils.TrieTree", "loader_utils.Pipeline.trie.add", "print", "open", "line.split", "loader_utils.Pipeline.trie.add", "int", "tokens.extend", "part.split"], "methods", ["home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.biunilm.loader_utils.TrieTree.add", "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.preprocess.process_daren.split", "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.biunilm.loader_utils.TrieTree.add", "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.preprocess.process_daren.split"], ["", "", "def", "create_trie_tree", "(", "self", ",", "pieces_dir", ")", ":", "\n", "        ", "print", "(", "\"sp_prob = {}\"", ".", "format", "(", "self", ".", "sp_prob", ")", ")", "\n", "print", "(", "\"pieces_threshold = {}\"", ".", "format", "(", "self", ".", "pieces_threshold", ")", ")", "\n", "if", "pieces_dir", "is", "not", "None", ":", "\n", "            ", "self", ".", "trie", "=", "TrieTree", "(", ")", "\n", "pieces_files", "=", "[", "pieces_dir", "]", "\n", "for", "token", "in", "self", ".", "vocab_words", ":", "\n", "                ", "self", ".", "trie", ".", "add", "(", "[", "token", "]", ")", "\n", "", "for", "piece_file", "in", "pieces_files", ":", "\n", "                ", "print", "(", "\"Load piece file: {}\"", ".", "format", "(", "piece_file", ")", ")", "\n", "with", "open", "(", "piece_file", ",", "mode", "=", "'r'", ",", "encoding", "=", "'utf-8'", ")", "as", "reader", ":", "\n", "                    ", "for", "line", "in", "reader", ":", "\n", "                        ", "parts", "=", "line", ".", "split", "(", "'\\t'", ")", "\n", "if", "int", "(", "parts", "[", "-", "1", "]", ")", "<", "self", ".", "pieces_threshold", ":", "\n", "                            ", "pass", "\n", "", "tokens", "=", "[", "]", "\n", "for", "part", "in", "parts", "[", ":", "-", "1", "]", ":", "\n", "                            ", "tokens", ".", "extend", "(", "part", ".", "split", "(", "' '", ")", ")", "\n", "", "self", ".", "trie", ".", "add", "(", "tokens", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.biunilm.loader_utils.Pipeline.__call__": [[141, 143], ["None"], "methods", ["None"], ["", "", "", "", "", "def", "__call__", "(", "self", ",", "instance", ")", ":", "\n", "        ", "raise", "NotImplementedError", "\n", "\n"]], "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.biunilm.loader_utils.Pipeline.get_masked_pos": [[146, 285], ["list", "set", "enumerate", "random.random.shuffle", "set", "list", "loader_utils.Pipeline.create_trie_tree", "list", "zip", "enumerate", "any", "range", "len", "random.random.shuffle", "len", "loader_utils.Pipeline.trie.get_pieces", "list", "_get_word_split_index.append", "loader_utils._get_word_split_index", "range", "loader_utils.Pipeline.trie.get_pieces", "set.add", "len", "range", "loader_utils._expand_whole_word", "set", "enumerate", "range", "len", "len", "tokens[].endswith", "tokens[].endswith", "tokens[].endswith", "cand_pos.append", "loader_utils._get_word_split_index", "list", "len", "list.add", "tokens[].startswith", "new_pieces[].extend", "new_pieces.append", "len", "cand_pos.append", "len", "range", "random.random.random", "min", "zip", "range", "list.add", "len", "numpy.random.choice", "random.random.random", "random.random.randint", "random.random.random", "set.add", "len", "len", "len"], "methods", ["home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.biunilm.loader_utils.Pipeline.create_trie_tree", "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.biunilm.loader_utils.TrieTree.get_pieces", "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.biunilm.loader_utils._get_word_split_index", "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.biunilm.loader_utils.TrieTree.get_pieces", "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.biunilm.loader_utils.TrieTree.add", "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.biunilm.loader_utils._expand_whole_word", "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.biunilm.loader_utils._get_word_split_index", "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.biunilm.loader_utils.TrieTree.add", "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.biunilm.loader_utils.TrieTree.add", "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.biunilm.loader_utils.TrieTree.add"], ["", "def", "get_masked_pos", "(", "self", ",", "tokens", ",", "n_pred", ",", "add_skipgram", "=", "False", ",", "mask_segment", "=", "None", ",", "protect_range", "=", "None", ")", ":", "\n", "        ", "if", "self", ".", "pieces_dir", "is", "not", "None", "and", "self", ".", "trie", "is", "None", ":", "\n", "            ", "self", ".", "create_trie_tree", "(", "self", ".", "pieces_dir", ")", "\n", "", "if", "self", ".", "pre_whole_word", ":", "\n", "            ", "if", "self", ".", "trie", "is", "not", "None", ":", "\n", "                ", "pieces", "=", "self", ".", "trie", ".", "get_pieces", "(", "tokens", ",", "0", ")", "\n", "\n", "new_pieces", "=", "[", "]", "\n", "for", "piece", "in", "pieces", ":", "\n", "                    ", "if", "len", "(", "new_pieces", ")", ">", "0", "and", "tokens", "[", "piece", "[", "0", "]", "]", ".", "startswith", "(", "\"##\"", ")", ":", "\n", "                        ", "new_pieces", "[", "-", "1", "]", ".", "extend", "(", "piece", ")", "\n", "", "else", ":", "\n", "                        ", "new_pieces", ".", "append", "(", "piece", ")", "\n", "", "", "del", "pieces", "\n", "pieces", "=", "new_pieces", "\n", "\n", "pre_word_split", "=", "list", "(", "_", "[", "-", "1", "]", "for", "_", "in", "pieces", ")", "\n", "pre_word_split", ".", "append", "(", "len", "(", "tokens", ")", ")", "\n", "", "else", ":", "\n", "                ", "pre_word_split", "=", "_get_word_split_index", "(", "tokens", ",", "0", ",", "len", "(", "tokens", ")", ")", "\n", "", "index2piece", "=", "None", "\n", "", "else", ":", "\n", "            ", "pre_word_split", "=", "list", "(", "range", "(", "0", ",", "len", "(", "tokens", ")", "+", "1", ")", ")", "\n", "\n", "if", "self", ".", "trie", "is", "not", "None", ":", "\n", "                ", "pieces", "=", "self", ".", "trie", ".", "get_pieces", "(", "tokens", ",", "0", ")", "\n", "\n", "index2piece", "=", "{", "}", "\n", "for", "piece", "in", "pieces", ":", "\n", "                    ", "for", "index", "in", "piece", ":", "\n", "                        ", "index2piece", "[", "index", "]", "=", "(", "piece", "[", "0", "]", ",", "piece", "[", "-", "1", "]", ")", "\n", "", "", "", "else", ":", "\n", "                ", "index2piece", "=", "None", "\n", "\n", "", "", "span_list", "=", "list", "(", "zip", "(", "pre_word_split", "[", ":", "-", "1", "]", ",", "pre_word_split", "[", "1", ":", "]", ")", ")", "\n", "\n", "# candidate positions of masked tokens", "\n", "cand_pos", "=", "[", "]", "\n", "special_pos", "=", "set", "(", ")", "\n", "if", "mask_segment", ":", "\n", "            ", "for", "i", ",", "sp", "in", "enumerate", "(", "span_list", ")", ":", "\n", "                ", "sp_st", ",", "sp_end", "=", "sp", "\n", "if", "(", "sp_end", "-", "sp_st", "==", "1", ")", "and", "tokens", "[", "sp_st", "]", ".", "endswith", "(", "'SEP]'", ")", ":", "\n", "                    ", "segment_index", "=", "i", "\n", "break", "\n", "", "", "", "for", "i", ",", "sp", "in", "enumerate", "(", "span_list", ")", ":", "\n", "            ", "sp_st", ",", "sp_end", "=", "sp", "\n", "if", "(", "sp_end", "-", "sp_st", "==", "1", ")", "and", "(", "tokens", "[", "sp_st", "]", ".", "endswith", "(", "'CLS]'", ")", "or", "tokens", "[", "sp_st", "]", ".", "endswith", "(", "'SEP]'", ")", ")", ":", "\n", "                ", "special_pos", ".", "add", "(", "i", ")", "\n", "", "else", ":", "\n", "                ", "if", "mask_segment", ":", "\n", "                    ", "if", "(", "(", "i", "<", "segment_index", ")", "and", "(", "'a'", "in", "mask_segment", ")", ")", "or", "(", "(", "i", ">", "segment_index", ")", "and", "(", "'b'", "in", "mask_segment", ")", ")", ":", "\n", "                        ", "cand_pos", ".", "append", "(", "i", ")", "\n", "", "", "else", ":", "\n", "                    ", "cand_pos", ".", "append", "(", "i", ")", "\n", "", "", "", "shuffle", "(", "cand_pos", ")", "\n", "\n", "masked_pos", "=", "set", "(", ")", "\n", "for", "i_span", "in", "cand_pos", ":", "\n", "            ", "if", "len", "(", "masked_pos", ")", ">=", "n_pred", ":", "\n", "                ", "break", "\n", "", "cand_st", ",", "cand_end", "=", "span_list", "[", "i_span", "]", "\n", "if", "len", "(", "masked_pos", ")", "+", "cand_end", "-", "cand_st", ">", "n_pred", ":", "\n", "                ", "continue", "\n", "", "if", "any", "(", "p", "in", "masked_pos", "for", "p", "in", "range", "(", "cand_st", ",", "cand_end", ")", ")", ":", "\n", "                ", "continue", "\n", "\n", "", "n_span", "=", "1", "\n", "if", "index2piece", "is", "not", "None", ":", "\n", "                ", "p_start", ",", "p_end", "=", "index2piece", "[", "i_span", "]", "\n", "if", "p_start", "<", "p_end", "and", "(", "rand", "(", ")", "<", "self", ".", "sp_prob", ")", ":", "\n", "# n_span = p_end - p_start + 1", "\n", "                    ", "st_span", ",", "end_span", "=", "p_start", ",", "p_end", "+", "1", "\n", "", "else", ":", "\n", "                    ", "st_span", ",", "end_span", "=", "i_span", ",", "i_span", "+", "1", "\n", "", "", "else", ":", "\n", "                ", "rand_skipgram_size", "=", "0", "\n", "# ngram", "\n", "if", "self", ".", "skipgram_size_geo_list", ":", "\n", "# sampling ngram size from geometric distribution", "\n", "                    ", "rand_skipgram_size", "=", "np", ".", "random", ".", "choice", "(", "\n", "len", "(", "self", ".", "skipgram_size_geo_list", ")", ",", "1", ",", "p", "=", "self", ".", "skipgram_size_geo_list", ")", "[", "0", "]", "+", "1", "\n", "", "else", ":", "\n", "                    ", "if", "add_skipgram", "and", "(", "self", ".", "skipgram_prb", ">", "0", ")", "and", "(", "self", ".", "skipgram_size", ">=", "2", ")", "and", "(", "rand", "(", ")", "<", "self", ".", "skipgram_prb", ")", ":", "\n", "                        ", "rand_skipgram_size", "=", "min", "(", "\n", "randint", "(", "2", ",", "self", ".", "skipgram_size", ")", ",", "len", "(", "span_list", ")", "-", "i_span", ")", "\n", "", "", "for", "n", "in", "range", "(", "2", ",", "rand_skipgram_size", "+", "1", ")", ":", "\n", "                    ", "tail_st", ",", "tail_end", "=", "span_list", "[", "i_span", "+", "n", "-", "1", "]", "\n", "if", "(", "tail_end", "-", "tail_st", "==", "1", ")", "and", "(", "tail_st", "in", "special_pos", ")", ":", "\n", "                        ", "break", "\n", "", "if", "len", "(", "masked_pos", ")", "+", "tail_end", "-", "cand_st", ">", "n_pred", ":", "\n", "                        ", "break", "\n", "", "n_span", "=", "n", "\n", "", "st_span", ",", "end_span", "=", "i_span", ",", "i_span", "+", "n_span", "\n", "\n", "", "if", "self", ".", "mask_whole_word", ":", "\n", "# pre_whole_word==False: position index of span_list is the same as tokens", "\n", "                ", "st_span", ",", "end_span", "=", "_expand_whole_word", "(", "\n", "tokens", ",", "st_span", ",", "end_span", ")", "\n", "\n", "# subsampling according to frequency", "\n", "", "if", "self", ".", "word_subsample_prb", ":", "\n", "                ", "skip_pos", "=", "set", "(", ")", "\n", "if", "self", ".", "pre_whole_word", ":", "\n", "                    ", "w_span_list", "=", "span_list", "[", "st_span", ":", "end_span", "]", "\n", "", "else", ":", "\n", "                    ", "split_idx", "=", "_get_word_split_index", "(", "\n", "tokens", ",", "st_span", ",", "end_span", ")", "\n", "w_span_list", "=", "list", "(", "\n", "zip", "(", "split_idx", "[", ":", "-", "1", "]", ",", "split_idx", "[", "1", ":", "]", ")", ")", "\n", "", "for", "i", ",", "sp", "in", "enumerate", "(", "w_span_list", ")", ":", "\n", "                    ", "sp_st", ",", "sp_end", "=", "sp", "\n", "if", "sp_end", "-", "sp_st", "==", "1", ":", "\n", "                        ", "w_cat", "=", "tokens", "[", "sp_st", "]", "\n", "", "else", ":", "\n", "                        ", "w_cat", "=", "''", ".", "join", "(", "tokens", "[", "sp_st", ":", "sp_end", "]", ")", "\n", "", "if", "(", "w_cat", "in", "self", ".", "word_subsample_prb", ")", "and", "(", "rand", "(", ")", "<", "self", ".", "word_subsample_prb", "[", "w_cat", "]", ")", ":", "\n", "                        ", "for", "k", "in", "range", "(", "sp_st", ",", "sp_end", ")", ":", "\n", "                            ", "skip_pos", ".", "add", "(", "k", ")", "\n", "", "", "", "", "else", ":", "\n", "                ", "skip_pos", "=", "None", "\n", "\n", "", "for", "sp", "in", "range", "(", "st_span", ",", "end_span", ")", ":", "\n", "                ", "for", "mp", "in", "range", "(", "span_list", "[", "sp", "]", "[", "0", "]", ",", "span_list", "[", "sp", "]", "[", "1", "]", ")", ":", "\n", "                    ", "if", "not", "(", "skip_pos", "and", "(", "mp", "in", "skip_pos", ")", ")", "and", "(", "mp", "not", "in", "special_pos", ")", "and", "not", "(", "protect_range", "and", "(", "protect_range", "[", "0", "]", "<=", "mp", "<", "protect_range", "[", "1", "]", ")", ")", ":", "\n", "                        ", "masked_pos", ".", "add", "(", "mp", ")", "\n", "\n", "", "", "", "", "if", "len", "(", "masked_pos", ")", "<", "n_pred", ":", "\n", "            ", "shuffle", "(", "cand_pos", ")", "\n", "for", "pos", "in", "cand_pos", ":", "\n", "                ", "if", "len", "(", "masked_pos", ")", ">=", "n_pred", ":", "\n", "                    ", "break", "\n", "", "if", "pos", "not", "in", "masked_pos", ":", "\n", "                    ", "masked_pos", ".", "add", "(", "pos", ")", "\n", "", "", "", "masked_pos", "=", "list", "(", "masked_pos", ")", "\n", "if", "len", "(", "masked_pos", ")", ">", "n_pred", ":", "\n", "# shuffle(masked_pos)", "\n", "            ", "masked_pos", "=", "masked_pos", "[", ":", "n_pred", "]", "\n", "", "return", "masked_pos", "\n", "\n"]], "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.biunilm.loader_utils.Pipeline.replace_masked_tokens": [[286, 300], ["sorted", "list", "random.random.random", "loader_utils.get_random_word"], "methods", ["home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.biunilm.loader_utils.get_random_word"], ["", "def", "replace_masked_tokens", "(", "self", ",", "tokens", ",", "masked_pos", ")", ":", "\n", "        ", "if", "self", ".", "span_same_mask", ":", "\n", "            ", "masked_pos", "=", "sorted", "(", "list", "(", "masked_pos", ")", ")", "\n", "", "prev_pos", ",", "prev_rand", "=", "None", ",", "None", "\n", "for", "pos", "in", "masked_pos", ":", "\n", "            ", "if", "self", ".", "span_same_mask", "and", "(", "pos", "-", "1", "==", "prev_pos", ")", ":", "\n", "                ", "t_rand", "=", "prev_rand", "\n", "", "else", ":", "\n", "                ", "t_rand", "=", "rand", "(", ")", "\n", "", "if", "t_rand", "<", "0.8", ":", "# 80%", "\n", "                ", "tokens", "[", "pos", "]", "=", "'[MASK]'", "\n", "", "elif", "t_rand", "<", "0.9", ":", "# 10%", "\n", "                ", "tokens", "[", "pos", "]", "=", "get_random_word", "(", "self", ".", "vocab_words", ")", "\n", "", "prev_pos", ",", "prev_rand", "=", "pos", ",", "t_rand", "\n", "", "", "", ""]], "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.biunilm.loader_utils.get_random_word": [[9, 12], ["random.randint", "len"], "function", ["None"], ["def", "get_random_word", "(", "vocab_words", ")", ":", "\n", "    ", "i", "=", "randint", "(", "0", ",", "len", "(", "vocab_words", ")", "-", "1", ")", "\n", "return", "vocab_words", "[", "i", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.biunilm.loader_utils.batch_list_to_batch_tensors": [[14, 24], ["zip", "batch_tensors.append", "isinstance", "batch_tensors.append", "batch_tensors.append", "torch.stack", "torch.stack", "torch.tensor", "torch.tensor"], "function", ["None"], ["", "def", "batch_list_to_batch_tensors", "(", "batch", ")", ":", "\n", "    ", "batch_tensors", "=", "[", "]", "\n", "for", "x", "in", "zip", "(", "*", "batch", ")", ":", "\n", "        ", "if", "x", "[", "0", "]", "is", "None", ":", "\n", "            ", "batch_tensors", ".", "append", "(", "None", ")", "\n", "", "elif", "isinstance", "(", "x", "[", "0", "]", ",", "torch", ".", "Tensor", ")", ":", "\n", "            ", "batch_tensors", ".", "append", "(", "torch", ".", "stack", "(", "x", ")", ")", "\n", "", "else", ":", "\n", "            ", "batch_tensors", ".", "append", "(", "torch", ".", "tensor", "(", "x", ",", "dtype", "=", "torch", ".", "long", ")", ")", "\n", "", "", "return", "batch_tensors", "\n", "\n"]], "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.biunilm.loader_utils._get_word_split_index": [[71, 80], ["split_idx.append", "split_idx.append", "tokens[].startswith"], "function", ["None"], ["", "", "def", "_get_word_split_index", "(", "tokens", ",", "st", ",", "end", ")", ":", "\n", "    ", "split_idx", "=", "[", "]", "\n", "i", "=", "st", "\n", "while", "i", "<", "end", ":", "\n", "        ", "if", "(", "not", "tokens", "[", "i", "]", ".", "startswith", "(", "'##'", ")", ")", "or", "(", "i", "==", "st", ")", ":", "\n", "            ", "split_idx", ".", "append", "(", "i", ")", "\n", "", "i", "+=", "1", "\n", "", "split_idx", ".", "append", "(", "end", ")", "\n", "return", "split_idx", "\n", "\n"]], "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.biunilm.loader_utils._expand_whole_word": [[82, 89], ["tokens[].startswith", "tokens[].startswith", "len"], "function", ["None"], ["", "def", "_expand_whole_word", "(", "tokens", ",", "st", ",", "end", ")", ":", "\n", "    ", "new_st", ",", "new_end", "=", "st", ",", "end", "\n", "while", "(", "new_st", ">=", "0", ")", "and", "tokens", "[", "new_st", "]", ".", "startswith", "(", "'##'", ")", ":", "\n", "        ", "new_st", "-=", "1", "\n", "", "while", "(", "new_end", "<", "len", "(", "tokens", ")", ")", "and", "tokens", "[", "new_end", "]", ".", "startswith", "(", "'##'", ")", ":", "\n", "        ", "new_end", "+=", "1", "\n", "", "return", "new_st", ",", "new_end", "\n", "\n"]], "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.biunilm.seq2seq_loader.Seq2SeqDataset.__init__": [[57, 88], ["super().__init__", "print", "open", "open", "zip", "open", "open", "open", "zip", "len", "tokenizer.tokenize", "tokenizer.tokenize", "seq2seq_loader.Seq2SeqDataset.ex_list.append", "tokenizer.tokenize", "tokenizer.tokenize", "orc.split", "seq2seq_loader.Seq2SeqDataset.ex_list.append", "src.strip", "tgt.strip", "len", "len", "src.strip", "tgt.strip", "int", "int", "s_st.split", "labl.split"], "methods", ["home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.unilm.eval.Eval.__init__", "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.preprocess.tokenizing_1342.tokenize", "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.preprocess.tokenizing_1342.tokenize", "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.preprocess.tokenizing_1342.tokenize", "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.preprocess.tokenizing_1342.tokenize", "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.preprocess.process_daren.split", "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.preprocess.process_daren.split", "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.preprocess.process_daren.split"], ["    ", "\"\"\" Load sentence pair (sequential or random order) from corpus \"\"\"", "\n", "\n", "def", "__init__", "(", "self", ",", "file_src", ",", "file_tgt", ",", "batch_size", ",", "tokenizer", ",", "max_len", ",", "file_oracle", "=", "None", ",", "short_sampling_prob", "=", "0.1", ",", "sent_reverse_order", "=", "False", ",", "bi_uni_pipeline", "=", "[", "]", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "tokenizer", "=", "tokenizer", "# tokenize function", "\n", "self", ".", "max_len", "=", "max_len", "# maximum length of tokens", "\n", "self", ".", "short_sampling_prob", "=", "short_sampling_prob", "\n", "self", ".", "bi_uni_pipeline", "=", "bi_uni_pipeline", "\n", "self", ".", "batch_size", "=", "batch_size", "\n", "self", ".", "sent_reverse_order", "=", "sent_reverse_order", "\n", "\n", "# read the file into memory", "\n", "self", ".", "ex_list", "=", "[", "]", "\n", "if", "file_oracle", "is", "None", ":", "\n", "            ", "with", "open", "(", "file_src", ",", "\"r\"", ",", "encoding", "=", "'utf-8'", ")", "as", "f_src", ",", "open", "(", "file_tgt", ",", "\"r\"", ",", "encoding", "=", "'utf-8'", ")", "as", "f_tgt", ":", "\n", "                ", "for", "src", ",", "tgt", "in", "zip", "(", "f_src", ",", "f_tgt", ")", ":", "\n", "                    ", "src_tk", "=", "tokenizer", ".", "tokenize", "(", "src", ".", "strip", "(", ")", ")", "\n", "tgt_tk", "=", "tokenizer", ".", "tokenize", "(", "tgt", ".", "strip", "(", ")", ")", "\n", "assert", "len", "(", "src_tk", ")", ">", "0", "\n", "assert", "len", "(", "tgt_tk", ")", ">", "0", "\n", "self", ".", "ex_list", ".", "append", "(", "(", "src_tk", ",", "tgt_tk", ")", ")", "\n", "", "", "", "else", ":", "\n", "            ", "with", "open", "(", "file_src", ",", "\"r\"", ",", "encoding", "=", "'utf-8'", ")", "as", "f_src", ",", "open", "(", "file_tgt", ",", "\"r\"", ",", "encoding", "=", "'utf-8'", ")", "as", "f_tgt", ",", "open", "(", "file_oracle", ",", "\"r\"", ",", "encoding", "=", "'utf-8'", ")", "as", "f_orc", ":", "\n", "                ", "for", "src", ",", "tgt", ",", "orc", "in", "zip", "(", "f_src", ",", "f_tgt", ",", "f_orc", ")", ":", "\n", "                    ", "src_tk", "=", "tokenizer", ".", "tokenize", "(", "src", ".", "strip", "(", ")", ")", "\n", "tgt_tk", "=", "tokenizer", ".", "tokenize", "(", "tgt", ".", "strip", "(", ")", ")", "\n", "s_st", ",", "labl", "=", "orc", ".", "split", "(", "'\\t'", ")", "\n", "s_st", "=", "[", "int", "(", "x", ")", "for", "x", "in", "s_st", ".", "split", "(", ")", "]", "\n", "labl", "=", "[", "int", "(", "x", ")", "for", "x", "in", "labl", ".", "split", "(", ")", "]", "\n", "self", ".", "ex_list", ".", "append", "(", "(", "src_tk", ",", "tgt_tk", ",", "s_st", ",", "labl", ")", ")", "\n"]], "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.biunilm.seq2seq_loader.Seq2SeqDataset.__len__": [[89, 91], ["len"], "methods", ["None"], ["", "", "", "print", "(", "'Load {0} documents'", ".", "format", "(", "len", "(", "self", ".", "ex_list", ")", ")", ")", "\n", "\n", "", "def", "__len__", "(", "self", ")", ":", "\n"]], "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.biunilm.seq2seq_loader.Seq2SeqDataset.__getitem__": [[92, 97], ["random.random.choice", "random.random.choice."], "methods", ["None"], ["        ", "return", "len", "(", "self", ".", "ex_list", ")", "\n", "\n", "", "def", "__getitem__", "(", "self", ",", "idx", ")", ":", "\n", "        ", "instance", "=", "self", ".", "ex_list", "[", "idx", "]", "\n", "proc", "=", "choice", "(", "self", ".", "bi_uni_pipeline", ")", "\n", "instance", "=", "proc", "(", "instance", ")", "\n"]], "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.biunilm.seq2seq_loader.Seq2SeqDataset.__iter__": [[98, 106], ["range", "math.ceil", "range", "random.random.randint", "batch.append", "biunilm.loader_utils.batch_list_to_batch_tensors", "len", "float", "seq2seq_loader.Seq2SeqDataset.__getitem__", "len"], "methods", ["home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.biunilm.loader_utils.batch_list_to_batch_tensors", "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.biunilm.bidir_loader.BiDirDataset.__getitem__"], ["return", "instance", "\n", "\n", "", "def", "__iter__", "(", "self", ")", ":", "# iterator to load data", "\n", "        ", "for", "__", "in", "range", "(", "math", ".", "ceil", "(", "len", "(", "self", ".", "ex_list", ")", "/", "float", "(", "self", ".", "batch_size", ")", ")", ")", ":", "\n", "            ", "batch", "=", "[", "]", "\n", "for", "__", "in", "range", "(", "self", ".", "batch_size", ")", ":", "\n", "                ", "idx", "=", "randint", "(", "0", ",", "len", "(", "self", ".", "ex_list", ")", "-", "1", ")", "\n", "batch", ".", "append", "(", "self", ".", "__getitem__", "(", "idx", ")", ")", "\n", "# To Tensor", "\n"]], "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.biunilm.seq2seq_loader.Preprocess4Seq2seq.__init__": [[111, 140], ["biunilm.loader_utils.Pipeline.__init__", "torch.tril", "truncate_config.get", "truncate_config.get", "truncate_config.get", "truncate_config.get", "torch.ones"], "methods", ["home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.unilm.eval.Eval.__init__"], ["    ", "\"\"\" Pre-processing steps for pretraining transformer \"\"\"", "\n", "\n", "def", "__init__", "(", "self", ",", "max_pred", ",", "mask_prob", ",", "vocab_words", ",", "indexer", ",", "max_len", "=", "512", ",", "skipgram_prb", "=", "0", ",", "skipgram_size", "=", "0", ",", "block_mask", "=", "False", ",", "mask_whole_word", "=", "False", ",", "new_segment_ids", "=", "False", ",", "truncate_config", "=", "{", "}", ",", "mask_source_words", "=", "False", ",", "mode", "=", "\"s2s\"", ",", "has_oracle", "=", "False", ",", "num_qkv", "=", "0", ",", "s2s_special_token", "=", "False", ",", "s2s_add_segment", "=", "False", ",", "s2s_share_segment", "=", "False", ",", "pos_shift", "=", "False", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "max_len", "=", "max_len", "\n", "self", ".", "max_pred", "=", "max_pred", "# max tokens of prediction", "\n", "self", ".", "mask_prob", "=", "mask_prob", "# masking probability", "\n", "self", ".", "vocab_words", "=", "vocab_words", "# vocabulary (sub)words", "\n", "self", ".", "indexer", "=", "indexer", "# function from token to token index", "\n", "self", ".", "max_len", "=", "max_len", "\n", "self", ".", "_tril_matrix", "=", "torch", ".", "tril", "(", "torch", ".", "ones", "(", "\n", "(", "max_len", ",", "max_len", ")", ",", "dtype", "=", "torch", ".", "long", ")", ")", "\n", "self", ".", "skipgram_prb", "=", "skipgram_prb", "\n", "self", ".", "skipgram_size", "=", "skipgram_size", "\n", "self", ".", "mask_whole_word", "=", "mask_whole_word", "\n", "self", ".", "new_segment_ids", "=", "new_segment_ids", "\n", "self", ".", "always_truncate_tail", "=", "truncate_config", ".", "get", "(", "\n", "'always_truncate_tail'", ",", "False", ")", "\n", "self", ".", "max_len_a", "=", "truncate_config", ".", "get", "(", "'max_len_a'", ",", "None", ")", "\n", "self", ".", "max_len_b", "=", "truncate_config", ".", "get", "(", "'max_len_b'", ",", "None", ")", "\n", "self", ".", "trunc_seg", "=", "truncate_config", ".", "get", "(", "'trunc_seg'", ",", "None", ")", "\n", "self", ".", "task_idx", "=", "3", "# relax projection layer for different tasks", "\n", "self", ".", "mask_source_words", "=", "mask_source_words", "\n", "assert", "mode", "in", "(", "\"s2s\"", ",", "\"l2r\"", ")", "\n", "self", ".", "mode", "=", "mode", "\n", "self", ".", "has_oracle", "=", "has_oracle", "\n", "self", ".", "num_qkv", "=", "num_qkv", "\n", "self", ".", "s2s_special_token", "=", "s2s_special_token", "\n", "self", ".", "s2s_add_segment", "=", "s2s_add_segment", "\n", "self", ".", "s2s_share_segment", "=", "s2s_share_segment", "\n"]], "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.biunilm.seq2seq_loader.Preprocess4Seq2seq.__call__": [[141, 316], ["seq2seq_loader.truncate_tokens_pair", "seq2seq_loader.Preprocess4Seq2seq.indexer", "seq2seq_loader.Preprocess4Seq2seq.extend", "segment_ids.extend", "torch.zeros", "min", "seq2seq_loader.Preprocess4Seq2seq.indexer", "len", "min", "set", "enumerate", "random.random.shuffle", "set", "max", "list", "seq2seq_loader.Preprocess4Seq2seq.indexer", "len", "mask_qkv.extend", "input_mask[].fill_", "input_mask[].copy_", "input_mask[].copy_", "zip", "len", "len", "max", "range", "len", "random.random.shuffle", "len", "seq2seq_loader.Preprocess4Seq2seq.extend", "list.extend", "masked_weights.extend", "len", "len", "oracle_pos.extend", "oracle_labels.extend", "oracle_weights.extend", "len", "range", "int", "cand_pos.append", "len", "random.random.randint", "random.random.random", "len", "oracle_pos.append", "oracle_labels.append", "len", "len", "len", "len", "len", "round", "cand_pos.append", "set.add", "tokens[].startswith", "tokens[].startswith", "random.random.random", "seq2seq_loader.Preprocess4Seq2seq.__call__._expand_whole_word"], "methods", ["home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.biunilm.bidir_loader.truncate_tokens_pair", "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.biunilm.loader_utils.TrieTree.add", "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.biunilm.loader_utils._expand_whole_word"], ["self", ".", "pos_shift", "=", "pos_shift", "\n", "\n", "", "def", "__call__", "(", "self", ",", "instance", ")", ":", "\n", "        ", "tokens_a", ",", "tokens_b", "=", "instance", "[", ":", "2", "]", "\n", "\n", "if", "self", ".", "pos_shift", ":", "\n", "            ", "tokens_b", "=", "[", "'[S2S_SOS]'", "]", "+", "tokens_b", "\n", "\n", "# -3  for special tokens [CLS], [SEP], [SEP]", "\n", "", "num_truncated_a", ",", "_", ",", "tokens_a", ",", "tokens_b", "=", "truncate_tokens_pair", "(", "tokens_a", ",", "tokens_b", ",", "self", ".", "max_len", "-", "3", ",", "max_len_a", "=", "self", ".", "max_len_a", ",", "\n", "max_len_b", "=", "self", ".", "max_len_b", ",", "trunc_seg", "=", "self", ".", "trunc_seg", ",", "always_truncate_tail", "=", "self", ".", "always_truncate_tail", ")", "\n", "\n", "# Add Special Tokens", "\n", "if", "self", ".", "s2s_special_token", ":", "\n", "            ", "tokens", "=", "[", "'[S2S_CLS]'", "]", "+", "tokens_a", "+", "[", "'[S2S_SEP]'", "]", "+", "tokens_b", "+", "[", "'[SEP]'", "]", "\n", "", "else", ":", "\n", "            ", "tokens", "=", "[", "'[CLS]'", "]", "+", "tokens_a", "+", "[", "'[SEP]'", "]", "+", "tokens_b", "+", "[", "'[SEP]'", "]", "\n", "\n", "", "if", "self", ".", "new_segment_ids", ":", "\n", "            ", "if", "self", ".", "mode", "==", "\"s2s\"", ":", "\n", "                ", "if", "self", ".", "s2s_add_segment", ":", "\n", "                    ", "if", "self", ".", "s2s_share_segment", ":", "\n", "                        ", "segment_ids", "=", "[", "0", "]", "+", "[", "1", "]", "*", "(", "len", "(", "tokens_a", ")", "+", "1", ")", "+", "[", "5", "]", "*", "(", "len", "(", "tokens_b", ")", "+", "1", ")", "\n", "", "else", ":", "\n", "                        ", "segment_ids", "=", "[", "4", "]", "+", "[", "6", "]", "*", "(", "len", "(", "tokens_a", ")", "+", "1", ")", "+", "[", "5", "]", "*", "(", "len", "(", "tokens_b", ")", "+", "1", ")", "\n", "", "", "else", ":", "\n", "                    ", "segment_ids", "=", "[", "4", "]", "*", "(", "len", "(", "tokens_a", ")", "+", "2", ")", "+", "[", "5", "]", "*", "(", "len", "(", "tokens_b", ")", "+", "1", ")", "\n", "", "", "else", ":", "\n", "                ", "segment_ids", "=", "[", "2", "]", "*", "(", "len", "(", "tokens", ")", ")", "\n", "", "", "else", ":", "\n", "            ", "segment_ids", "=", "[", "0", "]", "*", "(", "len", "(", "tokens_a", ")", "+", "2", ")", "+", "[", "1", "]", "*", "(", "len", "(", "tokens_b", ")", "+", "1", ")", "\n", "\n", "", "if", "self", ".", "pos_shift", ":", "\n", "            ", "n_pred", "=", "min", "(", "self", ".", "max_pred", ",", "len", "(", "tokens_b", ")", ")", "\n", "masked_pos", "=", "[", "len", "(", "tokens_a", ")", "+", "2", "+", "i", "for", "i", "in", "range", "(", "len", "(", "tokens_b", ")", ")", "]", "\n", "masked_weights", "=", "[", "1", "]", "*", "n_pred", "\n", "masked_ids", "=", "self", ".", "indexer", "(", "tokens_b", "[", "1", ":", "]", "+", "[", "'[SEP]'", "]", ")", "\n", "", "else", ":", "\n", "# For masked Language Models", "\n", "# the number of prediction is sometimes less than max_pred when sequence is short", "\n", "            ", "effective_length", "=", "len", "(", "tokens_b", ")", "\n", "if", "self", ".", "mask_source_words", ":", "\n", "                ", "effective_length", "+=", "len", "(", "tokens_a", ")", "\n", "", "n_pred", "=", "min", "(", "self", ".", "max_pred", ",", "max", "(", "\n", "1", ",", "int", "(", "round", "(", "effective_length", "*", "self", ".", "mask_prob", ")", ")", ")", ")", "\n", "# candidate positions of masked tokens", "\n", "cand_pos", "=", "[", "]", "\n", "special_pos", "=", "set", "(", ")", "\n", "for", "i", ",", "tk", "in", "enumerate", "(", "tokens", ")", ":", "\n", "# only mask tokens_b (target sequence)", "\n", "# we will mask [SEP] as an ending symbol", "\n", "                ", "if", "(", "i", ">=", "len", "(", "tokens_a", ")", "+", "2", ")", "and", "(", "tk", "!=", "'[CLS]'", ")", ":", "\n", "                    ", "cand_pos", ".", "append", "(", "i", ")", "\n", "", "elif", "self", ".", "mask_source_words", "and", "(", "i", "<", "len", "(", "tokens_a", ")", "+", "2", ")", "and", "(", "tk", "!=", "'[CLS]'", ")", "and", "(", "not", "tk", ".", "startswith", "(", "'[SEP'", ")", ")", ":", "\n", "                    ", "cand_pos", ".", "append", "(", "i", ")", "\n", "", "else", ":", "\n", "                    ", "special_pos", ".", "add", "(", "i", ")", "\n", "", "", "shuffle", "(", "cand_pos", ")", "\n", "\n", "masked_pos", "=", "set", "(", ")", "\n", "max_cand_pos", "=", "max", "(", "cand_pos", ")", "\n", "for", "pos", "in", "cand_pos", ":", "\n", "                ", "if", "len", "(", "masked_pos", ")", ">=", "n_pred", ":", "\n", "                    ", "break", "\n", "", "if", "pos", "in", "masked_pos", ":", "\n", "                    ", "continue", "\n", "\n", "", "def", "_expand_whole_word", "(", "st", ",", "end", ")", ":", "\n", "                    ", "new_st", ",", "new_end", "=", "st", ",", "end", "\n", "while", "(", "new_st", ">=", "0", ")", "and", "tokens", "[", "new_st", "]", ".", "startswith", "(", "'##'", ")", ":", "\n", "                        ", "new_st", "-=", "1", "\n", "", "while", "(", "new_end", "<", "len", "(", "tokens", ")", ")", "and", "tokens", "[", "new_end", "]", ".", "startswith", "(", "'##'", ")", ":", "\n", "                        ", "new_end", "+=", "1", "\n", "", "return", "new_st", ",", "new_end", "\n", "\n", "", "if", "(", "self", ".", "skipgram_prb", ">", "0", ")", "and", "(", "self", ".", "skipgram_size", ">=", "2", ")", "and", "(", "rand", "(", ")", "<", "self", ".", "skipgram_prb", ")", ":", "\n", "# ngram", "\n", "                    ", "cur_skipgram_size", "=", "randint", "(", "2", ",", "self", ".", "skipgram_size", ")", "\n", "if", "self", ".", "mask_whole_word", ":", "\n", "                        ", "st_pos", ",", "end_pos", "=", "_expand_whole_word", "(", "\n", "pos", ",", "pos", "+", "cur_skipgram_size", ")", "\n", "", "else", ":", "\n", "                        ", "st_pos", ",", "end_pos", "=", "pos", ",", "pos", "+", "cur_skipgram_size", "\n", "", "", "else", ":", "\n", "# directly mask", "\n", "                    ", "if", "self", ".", "mask_whole_word", ":", "\n", "                        ", "st_pos", ",", "end_pos", "=", "_expand_whole_word", "(", "pos", ",", "pos", "+", "1", ")", "\n", "", "else", ":", "\n", "                        ", "st_pos", ",", "end_pos", "=", "pos", ",", "pos", "+", "1", "\n", "\n", "", "", "for", "mp", "in", "range", "(", "st_pos", ",", "end_pos", ")", ":", "\n", "                    ", "if", "(", "0", "<", "mp", "<=", "max_cand_pos", ")", "and", "(", "mp", "not", "in", "special_pos", ")", ":", "\n", "                        ", "masked_pos", ".", "add", "(", "mp", ")", "\n", "", "else", ":", "\n", "                        ", "break", "\n", "\n", "", "", "", "masked_pos", "=", "list", "(", "masked_pos", ")", "\n", "if", "len", "(", "masked_pos", ")", ">", "n_pred", ":", "\n", "                ", "shuffle", "(", "masked_pos", ")", "\n", "masked_pos", "=", "masked_pos", "[", ":", "n_pred", "]", "\n", "\n", "", "masked_tokens", "=", "[", "tokens", "[", "pos", "]", "for", "pos", "in", "masked_pos", "]", "\n", "for", "pos", "in", "masked_pos", ":", "\n", "                ", "if", "rand", "(", ")", "<", "0.8", ":", "# 80%", "\n", "                    ", "tokens", "[", "pos", "]", "=", "'[MASK]'", "\n", "", "elif", "rand", "(", ")", "<", "0.5", ":", "# 10%", "\n", "                    ", "tokens", "[", "pos", "]", "=", "get_random_word", "(", "self", ".", "vocab_words", ")", "\n", "# when n_pred < max_pred, we only calculate loss within n_pred", "\n", "", "", "masked_weights", "=", "[", "1", "]", "*", "len", "(", "masked_tokens", ")", "\n", "\n", "# Token Indexing", "\n", "masked_ids", "=", "self", ".", "indexer", "(", "masked_tokens", ")", "\n", "# Token Indexing", "\n", "", "input_ids", "=", "self", ".", "indexer", "(", "tokens", ")", "\n", "\n", "# Zero Padding", "\n", "n_pad", "=", "self", ".", "max_len", "-", "len", "(", "input_ids", ")", "\n", "input_ids", ".", "extend", "(", "[", "0", "]", "*", "n_pad", ")", "\n", "segment_ids", ".", "extend", "(", "[", "0", "]", "*", "n_pad", ")", "\n", "\n", "if", "self", ".", "num_qkv", ">", "1", ":", "\n", "            ", "mask_qkv", "=", "[", "0", "]", "*", "(", "len", "(", "tokens_a", ")", "+", "2", ")", "+", "[", "1", "]", "*", "(", "len", "(", "tokens_b", ")", "+", "1", ")", "\n", "mask_qkv", ".", "extend", "(", "[", "0", "]", "*", "n_pad", ")", "\n", "", "else", ":", "\n", "            ", "mask_qkv", "=", "None", "\n", "\n", "", "input_mask", "=", "torch", ".", "zeros", "(", "self", ".", "max_len", ",", "self", ".", "max_len", ",", "dtype", "=", "torch", ".", "long", ")", "\n", "if", "self", ".", "mode", "==", "\"s2s\"", ":", "\n", "            ", "input_mask", "[", ":", ",", ":", "len", "(", "tokens_a", ")", "+", "2", "]", ".", "fill_", "(", "1", ")", "\n", "second_st", ",", "second_end", "=", "len", "(", "\n", "tokens_a", ")", "+", "2", ",", "len", "(", "tokens_a", ")", "+", "len", "(", "tokens_b", ")", "+", "3", "\n", "input_mask", "[", "second_st", ":", "second_end", ",", "second_st", ":", "second_end", "]", ".", "copy_", "(", "\n", "self", ".", "_tril_matrix", "[", ":", "second_end", "-", "second_st", ",", ":", "second_end", "-", "second_st", "]", ")", "\n", "", "else", ":", "\n", "            ", "st", ",", "end", "=", "0", ",", "len", "(", "tokens_a", ")", "+", "len", "(", "tokens_b", ")", "+", "3", "\n", "input_mask", "[", "st", ":", "end", ",", "st", ":", "end", "]", ".", "copy_", "(", "self", ".", "_tril_matrix", "[", ":", "end", ",", ":", "end", "]", ")", "\n", "\n", "# Zero Padding for masked target", "\n", "", "if", "self", ".", "max_pred", ">", "n_pred", ":", "\n", "            ", "n_pad", "=", "self", ".", "max_pred", "-", "n_pred", "\n", "if", "masked_ids", "is", "not", "None", ":", "\n", "                ", "masked_ids", ".", "extend", "(", "[", "0", "]", "*", "n_pad", ")", "\n", "", "if", "masked_pos", "is", "not", "None", ":", "\n", "                ", "masked_pos", ".", "extend", "(", "[", "0", "]", "*", "n_pad", ")", "\n", "", "if", "masked_weights", "is", "not", "None", ":", "\n", "                ", "masked_weights", ".", "extend", "(", "[", "0", "]", "*", "n_pad", ")", "\n", "\n", "", "", "oracle_pos", "=", "None", "\n", "oracle_weights", "=", "None", "\n", "oracle_labels", "=", "None", "\n", "if", "self", ".", "has_oracle", ":", "\n", "            ", "s_st", ",", "labls", "=", "instance", "[", "2", ":", "]", "\n", "oracle_pos", "=", "[", "]", "\n", "oracle_labels", "=", "[", "]", "\n", "for", "st", ",", "lb", "in", "zip", "(", "s_st", ",", "labls", ")", ":", "\n", "                ", "st", "=", "st", "-", "num_truncated_a", "[", "0", "]", "\n", "if", "st", ">", "0", "and", "st", "<", "len", "(", "tokens_a", ")", ":", "\n", "                    ", "oracle_pos", ".", "append", "(", "st", ")", "\n", "oracle_labels", ".", "append", "(", "lb", ")", "\n", "", "", "oracle_pos", "=", "oracle_pos", "[", ":", "20", "]", "\n", "oracle_labels", "=", "oracle_labels", "[", ":", "20", "]", "\n", "oracle_weights", "=", "[", "1", "]", "*", "len", "(", "oracle_pos", ")", "\n", "if", "len", "(", "oracle_pos", ")", "<", "20", ":", "\n", "                ", "x_pad", "=", "20", "-", "len", "(", "oracle_pos", ")", "\n", "oracle_pos", ".", "extend", "(", "[", "0", "]", "*", "x_pad", ")", "\n", "oracle_labels", ".", "extend", "(", "[", "0", "]", "*", "x_pad", ")", "\n", "oracle_weights", ".", "extend", "(", "[", "0", "]", "*", "x_pad", ")", "\n", "\n", "", "return", "(", "input_ids", ",", "segment_ids", ",", "input_mask", ",", "mask_qkv", ",", "masked_ids", ",", "\n", "masked_pos", ",", "masked_weights", ",", "-", "1", ",", "self", ".", "task_idx", ",", "\n", "oracle_pos", ",", "oracle_weights", ",", "oracle_labels", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.biunilm.seq2seq_loader.Preprocess4Seq2seqDecoder.__init__": [[321, 339], ["biunilm.loader_utils.Pipeline.__init__", "torch.tril", "torch.ones"], "methods", ["home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.unilm.eval.Eval.__init__"], ["", "return", "(", "input_ids", ",", "segment_ids", ",", "input_mask", ",", "mask_qkv", ",", "masked_ids", ",", "masked_pos", ",", "masked_weights", ",", "-", "1", ",", "self", ".", "task_idx", ")", "\n", "\n", "\n", "", "", "class", "Preprocess4Seq2seqDecoder", "(", "Pipeline", ")", ":", "\n", "    ", "\"\"\" Pre-processing steps for pretraining transformer \"\"\"", "\n", "\n", "def", "__init__", "(", "self", ",", "vocab_words", ",", "indexer", ",", "max_len", "=", "512", ",", "max_tgt_length", "=", "128", ",", "new_segment_ids", "=", "False", ",", "mode", "=", "\"s2s\"", ",", "num_qkv", "=", "0", ",", "s2s_special_token", "=", "False", ",", "s2s_add_segment", "=", "False", ",", "s2s_share_segment", "=", "False", ",", "pos_shift", "=", "False", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "max_len", "=", "max_len", "\n", "self", ".", "vocab_words", "=", "vocab_words", "# vocabulary (sub)words", "\n", "self", ".", "indexer", "=", "indexer", "# function from token to token index", "\n", "self", ".", "max_len", "=", "max_len", "\n", "self", ".", "_tril_matrix", "=", "torch", ".", "tril", "(", "torch", ".", "ones", "(", "\n", "(", "max_len", ",", "max_len", ")", ",", "dtype", "=", "torch", ".", "long", ")", ")", "\n", "self", ".", "new_segment_ids", "=", "new_segment_ids", "\n", "self", ".", "task_idx", "=", "3", "# relax projection layer for different tasks", "\n", "assert", "mode", "in", "(", "\"s2s\"", ",", "\"l2r\"", ")", "\n", "self", ".", "mode", "=", "mode", "\n", "self", ".", "max_tgt_length", "=", "max_tgt_length", "\n"]], "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.biunilm.seq2seq_loader.Preprocess4Seq2seqDecoder.__call__": [[340, 408], ["min", "range", "range", "range", "seq2seq_loader.Preprocess4Seq2seqDecoder.indexer", "torch.zeros", "input_mask[].copy_", "len", "len", "len", "position_ids.append", "position_ids.append", "position_ids.append", "input_mask[].fill_", "input_mask[].copy_", "input_mask[].fill_", "len", "len", "len", "len", "len", "len", "len", "len", "len", "len", "len", "len", "len", "len", "len", "len", "len", "len"], "methods", ["None"], ["self", ".", "num_qkv", "=", "num_qkv", "\n", "self", ".", "s2s_special_token", "=", "s2s_special_token", "\n", "self", ".", "s2s_add_segment", "=", "s2s_add_segment", "\n", "self", ".", "s2s_share_segment", "=", "s2s_share_segment", "\n", "self", ".", "pos_shift", "=", "pos_shift", "\n", "\n", "", "def", "__call__", "(", "self", ",", "instance", ")", ":", "\n", "        ", "tokens_a", ",", "max_a_len", "=", "instance", "\n", "\n", "# Add Special Tokens", "\n", "if", "self", ".", "s2s_special_token", ":", "\n", "            ", "padded_tokens_a", "=", "[", "'[S2S_CLS]'", "]", "+", "tokens_a", "+", "[", "'[S2S_SEP]'", "]", "\n", "", "else", ":", "\n", "            ", "padded_tokens_a", "=", "[", "'[CLS]'", "]", "+", "tokens_a", "+", "[", "'[SEP]'", "]", "\n", "", "assert", "len", "(", "padded_tokens_a", ")", "<=", "max_a_len", "+", "2", "\n", "if", "max_a_len", "+", "2", ">", "len", "(", "padded_tokens_a", ")", ":", "\n", "            ", "padded_tokens_a", "+=", "[", "'[PAD]'", "]", "*", "(", "max_a_len", "+", "2", "-", "len", "(", "padded_tokens_a", ")", ")", "\n", "", "assert", "len", "(", "padded_tokens_a", ")", "==", "max_a_len", "+", "2", "\n", "max_len_in_batch", "=", "min", "(", "self", ".", "max_tgt_length", "+", "\n", "max_a_len", "+", "2", ",", "self", ".", "max_len", ")", "\n", "tokens", "=", "padded_tokens_a", "\n", "if", "self", ".", "new_segment_ids", ":", "\n", "            ", "if", "self", ".", "mode", "==", "\"s2s\"", ":", "\n", "                ", "_enc_seg1", "=", "0", "if", "self", ".", "s2s_share_segment", "else", "4", "\n", "if", "self", ".", "s2s_add_segment", ":", "\n", "                    ", "if", "self", ".", "s2s_share_segment", ":", "\n", "                        ", "segment_ids", "=", "[", "\n", "0", "]", "+", "[", "1", "]", "*", "(", "len", "(", "padded_tokens_a", ")", "-", "1", ")", "+", "[", "5", "]", "*", "(", "max_len_in_batch", "-", "len", "(", "padded_tokens_a", ")", ")", "\n", "", "else", ":", "\n", "                        ", "segment_ids", "=", "[", "\n", "4", "]", "+", "[", "6", "]", "*", "(", "len", "(", "padded_tokens_a", ")", "-", "1", ")", "+", "[", "5", "]", "*", "(", "max_len_in_batch", "-", "len", "(", "padded_tokens_a", ")", ")", "\n", "", "", "else", ":", "\n", "                    ", "segment_ids", "=", "[", "4", "]", "*", "(", "len", "(", "padded_tokens_a", ")", ")", "+", "[", "5", "]", "*", "(", "max_len_in_batch", "-", "len", "(", "padded_tokens_a", ")", ")", "\n", "", "", "else", ":", "\n", "                ", "segment_ids", "=", "[", "2", "]", "*", "max_len_in_batch", "\n", "", "", "else", ":", "\n", "            ", "segment_ids", "=", "[", "0", "]", "*", "(", "len", "(", "padded_tokens_a", ")", ")", "+", "[", "1", "]", "*", "(", "max_len_in_batch", "-", "len", "(", "padded_tokens_a", ")", ")", "\n", "\n", "", "if", "self", ".", "num_qkv", ">", "1", ":", "\n", "            ", "mask_qkv", "=", "[", "0", "]", "*", "(", "len", "(", "padded_tokens_a", ")", ")", "+", "[", "1", "]", "*", "(", "max_len_in_batch", "-", "len", "(", "padded_tokens_a", ")", ")", "\n", "", "else", ":", "\n", "            ", "mask_qkv", "=", "None", "\n", "\n", "", "position_ids", "=", "[", "]", "\n", "for", "i", "in", "range", "(", "len", "(", "tokens_a", ")", "+", "2", ")", ":", "\n", "            ", "position_ids", ".", "append", "(", "i", ")", "\n", "", "for", "i", "in", "range", "(", "len", "(", "tokens_a", ")", "+", "2", ",", "max_a_len", "+", "2", ")", ":", "\n", "            ", "position_ids", ".", "append", "(", "0", ")", "\n", "", "for", "i", "in", "range", "(", "max_a_len", "+", "2", ",", "max_len_in_batch", ")", ":", "\n", "            ", "position_ids", ".", "append", "(", "i", "-", "(", "max_a_len", "+", "2", ")", "+", "len", "(", "tokens_a", ")", "+", "2", ")", "\n", "\n", "# Token Indexing", "\n", "", "input_ids", "=", "self", ".", "indexer", "(", "tokens", ")", "\n", "\n", "# Zero Padding", "\n", "input_mask", "=", "torch", ".", "zeros", "(", "\n", "max_len_in_batch", ",", "max_len_in_batch", ",", "dtype", "=", "torch", ".", "long", ")", "\n", "if", "self", ".", "mode", "==", "\"s2s\"", ":", "\n", "            ", "input_mask", "[", ":", ",", ":", "len", "(", "tokens_a", ")", "+", "2", "]", ".", "fill_", "(", "1", ")", "\n", "", "else", ":", "\n", "            ", "st", ",", "end", "=", "0", ",", "len", "(", "tokens_a", ")", "+", "2", "\n", "input_mask", "[", "st", ":", "end", ",", "st", ":", "end", "]", ".", "copy_", "(", "\n", "self", ".", "_tril_matrix", "[", ":", "end", ",", ":", "end", "]", ")", "\n", "input_mask", "[", "end", ":", ",", ":", "len", "(", "tokens_a", ")", "+", "2", "]", ".", "fill_", "(", "1", ")", "\n", "", "second_st", ",", "second_end", "=", "len", "(", "padded_tokens_a", ")", ",", "max_len_in_batch", "\n"]], "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.biunilm.seq2seq_loader.truncate_tokens_pair": [[16, 52], ["trunc_tokens.pop", "len", "len", "len", "random.random", "len", "len", "len"], "function", ["None"], ["def", "truncate_tokens_pair", "(", "tokens_a", ",", "tokens_b", ",", "max_len", ",", "max_len_a", "=", "0", ",", "max_len_b", "=", "0", ",", "trunc_seg", "=", "None", ",", "always_truncate_tail", "=", "False", ")", ":", "\n", "    ", "num_truncated_a", "=", "[", "0", ",", "0", "]", "\n", "num_truncated_b", "=", "[", "0", ",", "0", "]", "\n", "if", "max_len_b", ">", "0", ":", "\n", "        ", "tokens_b", "=", "tokens_b", "[", ":", "max_len_b", "]", "\n", "", "while", "True", ":", "\n", "        ", "if", "len", "(", "tokens_a", ")", "+", "len", "(", "tokens_b", ")", "<=", "max_len", ":", "\n", "            ", "break", "\n", "", "if", "(", "max_len_a", ">", "0", ")", "and", "len", "(", "tokens_a", ")", ">", "max_len_a", ":", "\n", "            ", "trunc_tokens", "=", "tokens_a", "\n", "num_truncated", "=", "num_truncated_a", "\n", "", "elif", "(", "max_len_b", ">", "0", ")", "and", "len", "(", "tokens_b", ")", ">", "max_len_b", ":", "\n", "            ", "trunc_tokens", "=", "tokens_b", "\n", "num_truncated", "=", "num_truncated_b", "\n", "", "elif", "trunc_seg", ":", "\n", "# truncate the specified segment", "\n", "            ", "if", "trunc_seg", "==", "'a'", ":", "\n", "                ", "trunc_tokens", "=", "tokens_a", "\n", "num_truncated", "=", "num_truncated_a", "\n", "", "else", ":", "\n", "                ", "trunc_tokens", "=", "tokens_b", "\n", "num_truncated", "=", "num_truncated_b", "\n", "", "", "else", ":", "\n", "# truncate the longer segment", "\n", "            ", "if", "len", "(", "tokens_a", ")", ">", "len", "(", "tokens_b", ")", ":", "\n", "                ", "trunc_tokens", "=", "tokens_a", "\n", "num_truncated", "=", "num_truncated_a", "\n", "", "else", ":", "\n", "                ", "trunc_tokens", "=", "tokens_b", "\n", "num_truncated", "=", "num_truncated_b", "\n", "# whether always truncate source sequences", "\n", "", "", "if", "(", "not", "always_truncate_tail", ")", "and", "(", "rand", "(", ")", "<", "0.5", ")", ":", "\n", "            ", "del", "trunc_tokens", "[", "0", "]", "\n", "num_truncated", "[", "0", "]", "+=", "1", "\n", "", "else", ":", "\n", "            ", "trunc_tokens", ".", "pop", "(", ")", "\n", "num_truncated", "[", "1", "]", "+=", "1", "\n"]], "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.biunilm.run_seq2seq._get_max_epoch_model": [[38, 49], ["glob.glob", "glob.glob", "os.path.join", "os.path.join", "set", "set", "max", "int", "int", "pathlib.Path().stem.split", "pathlib.Path().stem.split", "pathlib.Path", "pathlib.Path"], "function", ["home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.preprocess.process_daren.split", "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.preprocess.process_daren.split"], ["def", "_get_max_epoch_model", "(", "output_dir", ")", ":", "\n", "    ", "fn_model_list", "=", "glob", ".", "glob", "(", "os", ".", "path", ".", "join", "(", "output_dir", ",", "\"model.*.bin\"", ")", ")", "\n", "fn_optim_list", "=", "glob", ".", "glob", "(", "os", ".", "path", ".", "join", "(", "output_dir", ",", "\"optim.*.bin\"", ")", ")", "\n", "if", "(", "not", "fn_model_list", ")", "or", "(", "not", "fn_optim_list", ")", ":", "\n", "        ", "return", "None", "\n", "", "both_set", "=", "set", "(", "[", "int", "(", "Path", "(", "fn", ")", ".", "stem", ".", "split", "(", "'.'", ")", "[", "-", "1", "]", ")", "for", "fn", "in", "fn_model_list", "]", "\n", ")", "&", "set", "(", "[", "int", "(", "Path", "(", "fn", ")", ".", "stem", ".", "split", "(", "'.'", ")", "[", "-", "1", "]", ")", "for", "fn", "in", "fn_optim_list", "]", ")", "\n", "if", "both_set", ":", "\n", "        ", "return", "max", "(", "both_set", ")", "\n", "", "else", ":", "\n", "        ", "return", "None", "\n", "\n"]], "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.biunilm.run_seq2seq.main": [[51, 489], ["argparse.ArgumentParser", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.parse_args", "parser.parse_args.output_dir.replace", "parser.parse_args.log_dir.replace", "os.makedirs", "os.makedirs", "json.dump", "logger.info", "int", "random.seed", "numpy.random.seed", "torch.manual_seed", "torch.manual_seed", "pytorch_pretrained_bert.tokenization.BertTokenizer.from_pretrained", "int", "run_seq2seq._get_max_epoch_model", "nn.data_parallel.DataParallelImbalance.to", "list", "logger.info", "torch.cuda.empty_cache", "torch.cuda.empty_cache", "os.getenv", "os.getenv", "open", "torch.device", "torch.device", "torch.cuda.device_count", "torch.cuda.device_count", "torch.cuda.set_device", "torch.cuda.set_device", "torch.device", "torch.device", "torch.init_process_group", "ValueError", "torch.cuda.manual_seed_all", "torch.cuda.manual_seed_all", "ValueError", "torch.barrier", "pytorch_pretrained_bert.tokenization.WhitespaceTokenizer", "torch.barrier", "print", "os.path.join", "os.path.join", "biunilm.Seq2SeqDataset", "torch.utils.data.DataLoader", "torch.utils.data.DataLoader", "torch.barrier", "pytorch_pretrained_bert.modeling.BertForPreTrainingLossMask.from_pretrained", "pytorch_pretrained_bert.modeling.BertForPreTrainingLossMask.from_pretrained", "torch.barrier", "nn.data_parallel.DataParallelImbalance.named_parameters", "FusedAdam", "apex.amp.initialize", "logger.info", "pytorch_pretrained_bert.optimization.BertAdam", "DDP", "logger.info", "torch.load", "torch.load", "hasattr", "pytorch_pretrained_bert.optimization.BertAdam.load_state_dict", "logger.info", "logger.info", "logger.info", "nn.data_parallel.DataParallelImbalance.train", "tqdm.trange", "os.path.join", "bool", "biunilm.Preprocess4Seq2seq", "os.path.join", "torch.utils.data.RandomSampler", "torch.utils.data.distributed.DistributedSampler", "logger.info", "torch.load", "torch.load", "math.floor", "nn.data_parallel.DataParallelImbalance.bert.embeddings.word_embeddings.float", "nn.data_parallel.DataParallelImbalance.bert.embeddings.position_embeddings.float", "nn.data_parallel.DataParallelImbalance.bert.embeddings.token_type_embeddings.float", "nn.data_parallel.DataParallelImbalance", "os.path.join", "optim_recover.state_dict.state_dict", "logger.info", "tqdm.tqdm", "enumerate", "list", "torch.get_world_size", "len", "os.path.join", "logger.info", "torch.load", "torch.load", "ImportError", "ImportError", "int", "torch.utils.data.distributed.DistributedSampler.set_epoch", "nn.data_parallel.DataParallelImbalance.", "tqdm.tqdm.set_description", "logger.info", "os.path.join", "torch.save", "torch.save", "os.path.join", "torch.save", "torch.save", "torch.cuda.is_available", "torch.cuda.is_available", "BertTokenizer.from_pretrained.vocab.keys", "any", "masked_lm_loss.mean.mean", "next_sentence_loss.mean.mean", "loss.backward", "pytorch_pretrained_bert.optimization.BertAdam.step", "pytorch_pretrained_bert.optimization.BertAdam.zero_grad", "torch.distributed.get_rank", "torch.distributed.get_rank", "hasattr", "model_to_save.state_dict", "pytorch_pretrained_bert.optimization.BertAdam.state_dict", "any", "t.to", "loss.item", "apex.amp.scale_loss", "scaled_loss.backward", "pytorch_pretrained_bert.optimization.warmup_linear"], "function", ["home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.pytorch_pretrained_bert.tokenization.BertTokenizer.from_pretrained", "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.unilm.run_seq2seq._get_max_epoch_model", "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.pytorch_pretrained_bert.tokenization.BertTokenizer.from_pretrained", "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.pytorch_pretrained_bert.tokenization.BertTokenizer.from_pretrained", "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.pytorch_pretrained_bert.optimization_fp16.FP16_Optimizer_State.load_state_dict", "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.pytorch_pretrained_bert.optimization_fp16.FP16_Optimizer_State.state_dict", "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.pytorch_pretrained_bert.optimization.BertAdamFineTune.step", "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.pytorch_pretrained_bert.optimization_fp16.FP16_Optimizer_State.state_dict", "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.pytorch_pretrained_bert.optimization_fp16.FP16_Optimizer_State.state_dict", "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.pytorch_pretrained_bert.optimization.warmup_linear"], ["", "", "def", "main", "(", ")", ":", "\n", "    ", "parser", "=", "argparse", ".", "ArgumentParser", "(", ")", "\n", "\n", "# Required parameters", "\n", "parser", ".", "add_argument", "(", "\"--data_dir\"", ",", "\n", "default", "=", "None", ",", "\n", "type", "=", "str", ",", "\n", "required", "=", "True", ",", "\n", "help", "=", "\"The input data dir. Should contain the .tsv files (or other data files) for the task.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--src_file\"", ",", "default", "=", "None", ",", "type", "=", "str", ",", "\n", "help", "=", "\"The input data file name.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--tgt_file\"", ",", "default", "=", "None", ",", "type", "=", "str", ",", "\n", "help", "=", "\"The output data file name.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--bert_model\"", ",", "default", "=", "None", ",", "type", "=", "str", ",", "required", "=", "True", ",", "\n", "help", "=", "\"Bert pre-trained model selected in the list: bert-base-uncased, \"", "\n", "\"bert-large-uncased, bert-base-cased, bert-base-multilingual, bert-base-chinese.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--config_path\"", ",", "default", "=", "None", ",", "type", "=", "str", ",", "\n", "help", "=", "\"Bert config file path.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--output_dir\"", ",", "\n", "default", "=", "None", ",", "\n", "type", "=", "str", ",", "\n", "required", "=", "True", ",", "\n", "help", "=", "\"The output directory where the model predictions and checkpoints will be written.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--log_dir\"", ",", "\n", "default", "=", "''", ",", "\n", "type", "=", "str", ",", "\n", "required", "=", "True", ",", "\n", "help", "=", "\"The output directory where the log will be written.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--model_recover_path\"", ",", "\n", "default", "=", "None", ",", "\n", "type", "=", "str", ",", "\n", "help", "=", "\"The file of fine-tuned pretraining model.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--optim_recover_path\"", ",", "\n", "default", "=", "None", ",", "\n", "type", "=", "str", ",", "\n", "help", "=", "\"The file of pretraining optimizer.\"", ")", "\n", "\n", "# Other parameters", "\n", "parser", ".", "add_argument", "(", "\"--max_seq_length\"", ",", "\n", "default", "=", "128", ",", "\n", "type", "=", "int", ",", "\n", "help", "=", "\"The maximum total input sequence length after WordPiece tokenization. \\n\"", "\n", "\"Sequences longer than this will be truncated, and sequences shorter \\n\"", "\n", "\"than this will be padded.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--do_train\"", ",", "\n", "action", "=", "'store_true'", ",", "\n", "help", "=", "\"Whether to run training.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--do_eval\"", ",", "\n", "action", "=", "'store_true'", ",", "\n", "help", "=", "\"Whether to run eval on the dev set.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--do_lower_case\"", ",", "\n", "action", "=", "'store_true'", ",", "\n", "help", "=", "\"Set this flag if you are using an uncased model.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--train_batch_size\"", ",", "\n", "default", "=", "32", ",", "\n", "type", "=", "int", ",", "\n", "help", "=", "\"Total batch size for training.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--eval_batch_size\"", ",", "\n", "default", "=", "64", ",", "\n", "type", "=", "int", ",", "\n", "help", "=", "\"Total batch size for eval.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--learning_rate\"", ",", "default", "=", "5e-5", ",", "type", "=", "float", ",", "\n", "help", "=", "\"The initial learning rate for Adam.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--label_smoothing\"", ",", "default", "=", "0", ",", "type", "=", "float", ",", "\n", "help", "=", "\"The initial learning rate for Adam.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--weight_decay\"", ",", "\n", "default", "=", "0.01", ",", "\n", "type", "=", "float", ",", "\n", "help", "=", "\"The weight decay rate for Adam.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--finetune_decay\"", ",", "\n", "action", "=", "'store_true'", ",", "\n", "help", "=", "\"Weight decay to the original weights.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--num_train_epochs\"", ",", "\n", "default", "=", "3.0", ",", "\n", "type", "=", "float", ",", "\n", "help", "=", "\"Total number of training epochs to perform.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--warmup_proportion\"", ",", "\n", "default", "=", "0.1", ",", "\n", "type", "=", "float", ",", "\n", "help", "=", "\"Proportion of training to perform linear learning rate warmup for. \"", "\n", "\"E.g., 0.1 = 10%% of training.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--hidden_dropout_prob\"", ",", "default", "=", "0.1", ",", "type", "=", "float", ",", "\n", "help", "=", "\"Dropout rate for hidden states.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--attention_probs_dropout_prob\"", ",", "default", "=", "0.1", ",", "type", "=", "float", ",", "\n", "help", "=", "\"Dropout rate for attention probabilities.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--no_cuda\"", ",", "\n", "action", "=", "'store_true'", ",", "\n", "help", "=", "\"Whether not to use CUDA when available\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--local_rank\"", ",", "\n", "type", "=", "int", ",", "\n", "default", "=", "-", "1", ",", "\n", "help", "=", "\"local_rank for distributed training on gpus\"", ")", "\n", "parser", ".", "add_argument", "(", "'--seed'", ",", "\n", "type", "=", "int", ",", "\n", "default", "=", "42", ",", "\n", "help", "=", "\"random seed for initialization\"", ")", "\n", "parser", ".", "add_argument", "(", "'--gradient_accumulation_steps'", ",", "\n", "type", "=", "int", ",", "\n", "default", "=", "1", ",", "\n", "help", "=", "\"Number of updates steps to accumulate before performing a backward/update pass.\"", ")", "\n", "parser", ".", "add_argument", "(", "'--fp16'", ",", "action", "=", "'store_true'", ",", "\n", "help", "=", "\"Whether to use 16-bit float precision instead of 32-bit\"", ")", "\n", "parser", ".", "add_argument", "(", "'--fp32_embedding'", ",", "action", "=", "'store_true'", ",", "\n", "help", "=", "\"Whether to use 32-bit float precision instead of 16-bit for embeddings\"", ")", "\n", "parser", ".", "add_argument", "(", "'--loss_scale'", ",", "type", "=", "float", ",", "default", "=", "0", ",", "\n", "help", "=", "\"Loss scaling to improve fp16 numeric stability. Only used when fp16 set to True.\\n\"", "\n", "\"0 (default value): dynamic loss scaling.\\n\"", "\n", "\"Positive power of 2: static loss scaling value.\\n\"", ")", "\n", "parser", ".", "add_argument", "(", "'--amp'", ",", "action", "=", "'store_true'", ",", "\n", "help", "=", "\"Whether to use amp for fp16\"", ")", "\n", "parser", ".", "add_argument", "(", "'--from_scratch'", ",", "action", "=", "'store_true'", ",", "\n", "help", "=", "\"Initialize parameters with random values (i.e., training from scratch).\"", ")", "\n", "parser", ".", "add_argument", "(", "'--new_segment_ids'", ",", "action", "=", "'store_true'", ",", "\n", "help", "=", "\"Use new segment ids for bi-uni-directional LM.\"", ")", "\n", "parser", ".", "add_argument", "(", "'--new_pos_ids'", ",", "action", "=", "'store_true'", ",", "\n", "help", "=", "\"Use new position ids for LMs.\"", ")", "\n", "parser", ".", "add_argument", "(", "'--tokenized_input'", ",", "action", "=", "'store_true'", ",", "\n", "help", "=", "\"Whether the input is tokenized.\"", ")", "\n", "parser", ".", "add_argument", "(", "'--max_len_a'", ",", "type", "=", "int", ",", "default", "=", "0", ",", "\n", "help", "=", "\"Truncate_config: maximum length of segment A.\"", ")", "\n", "parser", ".", "add_argument", "(", "'--max_len_b'", ",", "type", "=", "int", ",", "default", "=", "0", ",", "\n", "help", "=", "\"Truncate_config: maximum length of segment B.\"", ")", "\n", "parser", ".", "add_argument", "(", "'--trunc_seg'", ",", "default", "=", "''", ",", "\n", "help", "=", "\"Truncate_config: first truncate segment A/B (option: a, b).\"", ")", "\n", "parser", ".", "add_argument", "(", "'--always_truncate_tail'", ",", "action", "=", "'store_true'", ",", "\n", "help", "=", "\"Truncate_config: Whether we should always truncate tail.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--mask_prob\"", ",", "default", "=", "0.15", ",", "type", "=", "float", ",", "\n", "help", "=", "\"Number of prediction is sometimes less than max_pred when sequence is short.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--mask_prob_eos\"", ",", "default", "=", "0", ",", "type", "=", "float", ",", "\n", "help", "=", "\"Number of prediction is sometimes less than max_pred when sequence is short.\"", ")", "\n", "parser", ".", "add_argument", "(", "'--max_pred'", ",", "type", "=", "int", ",", "default", "=", "20", ",", "\n", "help", "=", "\"Max tokens of prediction.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--num_workers\"", ",", "default", "=", "0", ",", "type", "=", "int", ",", "\n", "help", "=", "\"Number of workers for the data loader.\"", ")", "\n", "\n", "parser", ".", "add_argument", "(", "'--mask_source_words'", ",", "action", "=", "'store_true'", ",", "\n", "help", "=", "\"Whether to mask source words for training\"", ")", "\n", "parser", ".", "add_argument", "(", "'--skipgram_prb'", ",", "type", "=", "float", ",", "default", "=", "0.0", ",", "\n", "help", "=", "'prob of ngram mask'", ")", "\n", "parser", ".", "add_argument", "(", "'--skipgram_size'", ",", "type", "=", "int", ",", "default", "=", "1", ",", "\n", "help", "=", "'the max size of ngram mask'", ")", "\n", "parser", ".", "add_argument", "(", "'--mask_whole_word'", ",", "action", "=", "'store_true'", ",", "\n", "help", "=", "\"Whether masking a whole word.\"", ")", "\n", "parser", ".", "add_argument", "(", "'--do_l2r_training'", ",", "action", "=", "'store_true'", ",", "\n", "help", "=", "\"Whether to do left to right training\"", ")", "\n", "parser", ".", "add_argument", "(", "'--has_sentence_oracle'", ",", "action", "=", "'store_true'", ",", "\n", "help", "=", "\"Whether to have sentence level oracle for training. \"", "\n", "\"Only useful for summary generation\"", ")", "\n", "parser", ".", "add_argument", "(", "'--max_position_embeddings'", ",", "type", "=", "int", ",", "default", "=", "None", ",", "\n", "help", "=", "\"max position embeddings\"", ")", "\n", "parser", ".", "add_argument", "(", "'--relax_projection'", ",", "action", "=", "'store_true'", ",", "\n", "help", "=", "\"Use different projection layers for tasks.\"", ")", "\n", "parser", ".", "add_argument", "(", "'--ffn_type'", ",", "default", "=", "0", ",", "type", "=", "int", ",", "\n", "help", "=", "\"0: default mlp; 1: W((Wx+b) elem_prod x);\"", ")", "\n", "parser", ".", "add_argument", "(", "'--num_qkv'", ",", "default", "=", "0", ",", "type", "=", "int", ",", "\n", "help", "=", "\"Number of different <Q,K,V>.\"", ")", "\n", "parser", ".", "add_argument", "(", "'--seg_emb'", ",", "action", "=", "'store_true'", ",", "\n", "help", "=", "\"Using segment embedding for self-attention.\"", ")", "\n", "parser", ".", "add_argument", "(", "'--s2s_special_token'", ",", "action", "=", "'store_true'", ",", "\n", "help", "=", "\"New special tokens ([S2S_SEP]/[S2S_CLS]) of S2S.\"", ")", "\n", "parser", ".", "add_argument", "(", "'--s2s_add_segment'", ",", "action", "=", "'store_true'", ",", "\n", "help", "=", "\"Additional segmental for the encoder of S2S.\"", ")", "\n", "parser", ".", "add_argument", "(", "'--s2s_share_segment'", ",", "action", "=", "'store_true'", ",", "\n", "help", "=", "\"Sharing segment embeddings for the encoder of S2S (used with --s2s_add_segment).\"", ")", "\n", "parser", ".", "add_argument", "(", "'--pos_shift'", ",", "action", "=", "'store_true'", ",", "\n", "help", "=", "\"Using position shift for fine-tuning.\"", ")", "\n", "\n", "args", "=", "parser", ".", "parse_args", "(", ")", "\n", "\n", "# assert Path(args.model_recover_path).exists(", "\n", "# ), \"--model_recover_path doesn't exist\"", "\n", "\n", "args", ".", "output_dir", "=", "args", ".", "output_dir", ".", "replace", "(", "\n", "'[PT_OUTPUT_DIR]'", ",", "os", ".", "getenv", "(", "'PT_OUTPUT_DIR'", ",", "''", ")", ")", "\n", "args", ".", "log_dir", "=", "args", ".", "log_dir", ".", "replace", "(", "\n", "'[PT_OUTPUT_DIR]'", ",", "os", ".", "getenv", "(", "'PT_OUTPUT_DIR'", ",", "''", ")", ")", "\n", "\n", "os", ".", "makedirs", "(", "args", ".", "output_dir", ",", "exist_ok", "=", "True", ")", "\n", "os", ".", "makedirs", "(", "args", ".", "log_dir", ",", "exist_ok", "=", "True", ")", "\n", "json", ".", "dump", "(", "args", ".", "__dict__", ",", "open", "(", "os", ".", "path", ".", "join", "(", "\n", "args", ".", "output_dir", ",", "'opt.json'", ")", ",", "'w'", ")", ",", "sort_keys", "=", "True", ",", "indent", "=", "2", ")", "\n", "\n", "if", "args", ".", "local_rank", "==", "-", "1", "or", "args", ".", "no_cuda", ":", "\n", "        ", "device", "=", "torch", ".", "device", "(", "\n", "\"cuda\"", "if", "torch", ".", "cuda", ".", "is_available", "(", ")", "and", "not", "args", ".", "no_cuda", "else", "\"cpu\"", ")", "\n", "n_gpu", "=", "torch", ".", "cuda", ".", "device_count", "(", ")", "\n", "", "else", ":", "\n", "        ", "torch", ".", "cuda", ".", "set_device", "(", "args", ".", "local_rank", ")", "\n", "device", "=", "torch", ".", "device", "(", "\"cuda\"", ",", "args", ".", "local_rank", ")", "\n", "n_gpu", "=", "1", "\n", "# Initializes the distributed backend which will take care of sychronizing nodes/GPUs", "\n", "dist", ".", "init_process_group", "(", "backend", "=", "'nccl'", ")", "\n", "", "logger", ".", "info", "(", "\"device: {} n_gpu: {}, distributed training: {}, 16-bits training: {}\"", ".", "format", "(", "\n", "device", ",", "n_gpu", ",", "bool", "(", "args", ".", "local_rank", "!=", "-", "1", ")", ",", "args", ".", "fp16", ")", ")", "\n", "\n", "if", "args", ".", "gradient_accumulation_steps", "<", "1", ":", "\n", "        ", "raise", "ValueError", "(", "\"Invalid gradient_accumulation_steps parameter: {}, should be >= 1\"", ".", "format", "(", "\n", "args", ".", "gradient_accumulation_steps", ")", ")", "\n", "\n", "", "args", ".", "train_batch_size", "=", "int", "(", "\n", "args", ".", "train_batch_size", "/", "args", ".", "gradient_accumulation_steps", ")", "\n", "\n", "random", ".", "seed", "(", "args", ".", "seed", ")", "\n", "np", ".", "random", ".", "seed", "(", "args", ".", "seed", ")", "\n", "torch", ".", "manual_seed", "(", "args", ".", "seed", ")", "\n", "if", "n_gpu", ">", "0", ":", "\n", "        ", "torch", ".", "cuda", ".", "manual_seed_all", "(", "args", ".", "seed", ")", "\n", "\n", "", "if", "not", "args", ".", "do_train", "and", "not", "args", ".", "do_eval", ":", "\n", "        ", "raise", "ValueError", "(", "\n", "\"At least one of `do_train` or `do_eval` must be True.\"", ")", "\n", "\n", "", "if", "args", ".", "local_rank", "not", "in", "(", "-", "1", ",", "0", ")", ":", "\n", "# Make sure only the first process in distributed training will download model & vocab", "\n", "        ", "dist", ".", "barrier", "(", ")", "\n", "", "tokenizer", "=", "BertTokenizer", ".", "from_pretrained", "(", "\n", "args", ".", "bert_model", ",", "do_lower_case", "=", "args", ".", "do_lower_case", ")", "\n", "if", "args", ".", "max_position_embeddings", ":", "\n", "        ", "tokenizer", ".", "max_len", "=", "args", ".", "max_position_embeddings", "\n", "", "data_tokenizer", "=", "WhitespaceTokenizer", "(", ")", "if", "args", ".", "tokenized_input", "else", "tokenizer", "\n", "if", "args", ".", "local_rank", "==", "0", ":", "\n", "        ", "dist", ".", "barrier", "(", ")", "\n", "\n", "", "if", "args", ".", "do_train", ":", "\n", "        ", "print", "(", "\"Loading Train Dataset\"", ",", "args", ".", "data_dir", ")", "\n", "bi_uni_pipeline", "=", "[", "seq2seq_loader", ".", "Preprocess4Seq2seq", "(", "args", ".", "max_pred", ",", "args", ".", "mask_prob", ",", "list", "(", "tokenizer", ".", "vocab", ".", "keys", "(", "\n", ")", ")", ",", "tokenizer", ".", "convert_tokens_to_ids", ",", "args", ".", "max_seq_length", ",", "new_segment_ids", "=", "args", ".", "new_segment_ids", ",", "truncate_config", "=", "{", "'max_len_a'", ":", "args", ".", "max_len_a", ",", "'max_len_b'", ":", "args", ".", "max_len_b", ",", "'trunc_seg'", ":", "args", ".", "trunc_seg", ",", "'always_truncate_tail'", ":", "args", ".", "always_truncate_tail", "}", ",", "mask_source_words", "=", "args", ".", "mask_source_words", ",", "skipgram_prb", "=", "args", ".", "skipgram_prb", ",", "skipgram_size", "=", "args", ".", "skipgram_size", ",", "mask_whole_word", "=", "args", ".", "mask_whole_word", ",", "mode", "=", "\"s2s\"", ",", "has_oracle", "=", "args", ".", "has_sentence_oracle", ",", "num_qkv", "=", "args", ".", "num_qkv", ",", "s2s_special_token", "=", "args", ".", "s2s_special_token", ",", "s2s_add_segment", "=", "args", ".", "s2s_add_segment", ",", "s2s_share_segment", "=", "args", ".", "s2s_share_segment", ",", "pos_shift", "=", "args", ".", "pos_shift", ")", "]", "\n", "file_oracle", "=", "None", "\n", "if", "args", ".", "has_sentence_oracle", ":", "\n", "            ", "file_oracle", "=", "os", ".", "path", ".", "join", "(", "args", ".", "data_dir", ",", "'train.oracle'", ")", "\n", "", "fn_src", "=", "os", ".", "path", ".", "join", "(", "\n", "args", ".", "data_dir", ",", "args", ".", "src_file", "if", "args", ".", "src_file", "else", "'train.src'", ")", "\n", "fn_tgt", "=", "os", ".", "path", ".", "join", "(", "\n", "args", ".", "data_dir", ",", "args", ".", "tgt_file", "if", "args", ".", "tgt_file", "else", "'train.tgt'", ")", "\n", "train_dataset", "=", "seq2seq_loader", ".", "Seq2SeqDataset", "(", "\n", "fn_src", ",", "fn_tgt", ",", "args", ".", "train_batch_size", ",", "data_tokenizer", ",", "args", ".", "max_seq_length", ",", "file_oracle", "=", "file_oracle", ",", "bi_uni_pipeline", "=", "bi_uni_pipeline", ")", "\n", "if", "args", ".", "local_rank", "==", "-", "1", ":", "\n", "            ", "train_sampler", "=", "RandomSampler", "(", "train_dataset", ",", "replacement", "=", "False", ")", "\n", "_batch_size", "=", "args", ".", "train_batch_size", "\n", "", "else", ":", "\n", "            ", "train_sampler", "=", "DistributedSampler", "(", "train_dataset", ")", "\n", "_batch_size", "=", "args", ".", "train_batch_size", "//", "dist", ".", "get_world_size", "(", ")", "\n", "", "train_dataloader", "=", "torch", ".", "utils", ".", "data", ".", "DataLoader", "(", "train_dataset", ",", "batch_size", "=", "_batch_size", ",", "sampler", "=", "train_sampler", ",", "\n", "num_workers", "=", "args", ".", "num_workers", ",", "collate_fn", "=", "seq2seq_loader", ".", "batch_list_to_batch_tensors", ",", "pin_memory", "=", "False", ")", "\n", "\n", "# note: args.train_batch_size has been changed to (/= args.gradient_accumulation_steps)", "\n", "# t_total = int(math.ceil(len(train_dataset.ex_list) / args.train_batch_size)", "\n", "", "t_total", "=", "int", "(", "len", "(", "train_dataloader", ")", "*", "args", ".", "num_train_epochs", "/", "\n", "args", ".", "gradient_accumulation_steps", ")", "\n", "\n", "# amp_handle = None", "\n", "# if args.fp16 and args.amp:", "\n", "#     from apex import amp", "\n", "#     amp_handle = amp.init(enable_caching=True)", "\n", "#     logger.info(\"enable fp16 with amp\")", "\n", "\n", "# Prepare model", "\n", "recover_step", "=", "_get_max_epoch_model", "(", "args", ".", "output_dir", ")", "\n", "cls_num_labels", "=", "2", "\n", "type_vocab_size", "=", "6", "+", "(", "1", "if", "args", ".", "s2s_add_segment", "else", "0", ")", "if", "args", ".", "new_segment_ids", "else", "2", "\n", "num_sentlvl_labels", "=", "2", "if", "args", ".", "has_sentence_oracle", "else", "0", "\n", "relax_projection", "=", "4", "if", "args", ".", "relax_projection", "else", "0", "\n", "if", "args", ".", "local_rank", "not", "in", "(", "-", "1", ",", "0", ")", ":", "\n", "# Make sure only the first process in distributed training will download model & vocab", "\n", "        ", "dist", ".", "barrier", "(", ")", "\n", "", "if", "(", "recover_step", "is", "None", ")", "and", "(", "args", ".", "model_recover_path", "is", "None", ")", ":", "\n", "# if _state_dict == {}, the parameters are randomly initialized", "\n", "# if _state_dict == None, the parameters are initialized with bert-init", "\n", "        ", "_state_dict", "=", "{", "}", "if", "args", ".", "from_scratch", "else", "None", "\n", "model", "=", "BertForPreTrainingLossMask", ".", "from_pretrained", "(", "\n", "args", ".", "bert_model", ",", "state_dict", "=", "_state_dict", ",", "num_labels", "=", "cls_num_labels", ",", "num_rel", "=", "0", ",", "type_vocab_size", "=", "type_vocab_size", ",", "config_path", "=", "args", ".", "config_path", ",", "task_idx", "=", "3", ",", "num_sentlvl_labels", "=", "num_sentlvl_labels", ",", "max_position_embeddings", "=", "args", ".", "max_position_embeddings", ",", "label_smoothing", "=", "args", ".", "label_smoothing", ",", "fp32_embedding", "=", "args", ".", "fp32_embedding", ",", "relax_projection", "=", "relax_projection", ",", "new_pos_ids", "=", "args", ".", "new_pos_ids", ",", "ffn_type", "=", "args", ".", "ffn_type", ",", "hidden_dropout_prob", "=", "args", ".", "hidden_dropout_prob", ",", "attention_probs_dropout_prob", "=", "args", ".", "attention_probs_dropout_prob", ",", "num_qkv", "=", "args", ".", "num_qkv", ",", "seg_emb", "=", "args", ".", "seg_emb", ")", "\n", "global_step", "=", "0", "\n", "", "else", ":", "\n", "        ", "if", "recover_step", ":", "\n", "            ", "logger", ".", "info", "(", "\"***** Recover model: %d *****\"", ",", "recover_step", ")", "\n", "model_recover", "=", "torch", ".", "load", "(", "os", ".", "path", ".", "join", "(", "\n", "args", ".", "output_dir", ",", "\"model.{0}.bin\"", ".", "format", "(", "recover_step", ")", ")", ",", "map_location", "=", "'cpu'", ")", "\n", "# recover_step == number of epochs", "\n", "global_step", "=", "math", ".", "floor", "(", "\n", "recover_step", "*", "t_total", "/", "args", ".", "num_train_epochs", ")", "\n", "", "elif", "args", ".", "model_recover_path", ":", "\n", "            ", "logger", ".", "info", "(", "\"***** Recover model: %s *****\"", ",", "\n", "args", ".", "model_recover_path", ")", "\n", "model_recover", "=", "torch", ".", "load", "(", "\n", "args", ".", "model_recover_path", ",", "map_location", "=", "'cpu'", ")", "\n", "global_step", "=", "0", "\n", "", "model", "=", "BertForPreTrainingLossMask", ".", "from_pretrained", "(", "\n", "args", ".", "bert_model", ",", "state_dict", "=", "model_recover", ",", "num_labels", "=", "cls_num_labels", ",", "num_rel", "=", "0", ",", "type_vocab_size", "=", "type_vocab_size", ",", "config_path", "=", "args", ".", "config_path", ",", "task_idx", "=", "3", ",", "num_sentlvl_labels", "=", "num_sentlvl_labels", ",", "max_position_embeddings", "=", "args", ".", "max_position_embeddings", ",", "label_smoothing", "=", "args", ".", "label_smoothing", ",", "fp32_embedding", "=", "args", ".", "fp32_embedding", ",", "relax_projection", "=", "relax_projection", ",", "new_pos_ids", "=", "args", ".", "new_pos_ids", ",", "ffn_type", "=", "args", ".", "ffn_type", ",", "hidden_dropout_prob", "=", "args", ".", "hidden_dropout_prob", ",", "attention_probs_dropout_prob", "=", "args", ".", "attention_probs_dropout_prob", ",", "num_qkv", "=", "args", ".", "num_qkv", ",", "seg_emb", "=", "args", ".", "seg_emb", ")", "\n", "", "if", "args", ".", "local_rank", "==", "0", ":", "\n", "        ", "dist", ".", "barrier", "(", ")", "\n", "\n", "", "if", "args", ".", "fp16", ":", "\n", "# model.half()", "\n", "        ", "if", "args", ".", "fp32_embedding", ":", "\n", "            ", "model", ".", "bert", ".", "embeddings", ".", "word_embeddings", ".", "float", "(", ")", "\n", "model", ".", "bert", ".", "embeddings", ".", "position_embeddings", ".", "float", "(", ")", "\n", "model", ".", "bert", ".", "embeddings", ".", "token_type_embeddings", ".", "float", "(", ")", "\n", "", "", "model", ".", "to", "(", "device", ")", "\n", "\n", "\n", "# Prepare optimizer", "\n", "param_optimizer", "=", "list", "(", "model", ".", "named_parameters", "(", ")", ")", "\n", "no_decay", "=", "[", "'bias'", ",", "'LayerNorm.bias'", ",", "'LayerNorm.weight'", "]", "\n", "optimizer_grouped_parameters", "=", "[", "\n", "{", "'params'", ":", "[", "p", "for", "n", ",", "p", "in", "param_optimizer", "if", "not", "any", "(", "\n", "nd", "in", "n", "for", "nd", "in", "no_decay", ")", "]", ",", "'weight_decay'", ":", "0.01", "}", ",", "\n", "{", "'params'", ":", "[", "p", "for", "n", ",", "p", "in", "param_optimizer", "if", "any", "(", "\n", "nd", "in", "n", "for", "nd", "in", "no_decay", ")", "]", ",", "'weight_decay'", ":", "0.0", "}", "\n", "]", "\n", "if", "args", ".", "fp16", ":", "\n", "        ", "try", ":", "\n", "# from apex.optimizers import FP16_Optimizer", "\n", "            ", "from", "pytorch_pretrained_bert", ".", "optimization_fp16", "import", "FP16_Optimizer_State", "\n", "from", "apex", ".", "optimizers", "import", "FusedAdam", "\n", "", "except", "ImportError", ":", "\n", "            ", "raise", "ImportError", "(", "\n", "\"Please install apex from https://www.github.com/nvidia/apex to use distributed and fp16 training.\"", ")", "\n", "\n", "", "optimizer", "=", "FusedAdam", "(", "optimizer_grouped_parameters", ",", "\n", "lr", "=", "args", ".", "learning_rate", ",", "\n", "bias_correction", "=", "False", ")", "\n", "# max_grad_norm=1.0)", "\n", "# if args.loss_scale == 0:", "\n", "#     optimizer = FP16_Optimizer_State(", "\n", "#         optimizer, dynamic_loss_scale=True)", "\n", "# else:", "\n", "#     optimizer = FP16_Optimizer_State(", "\n", "#         optimizer, static_loss_scale=args.loss_scale)", "\n", "\n", "\n", "\n", "model", ",", "optimizer", "=", "amp", ".", "initialize", "(", "model", ",", "optimizer", ",", "opt_level", "=", "\"O1\"", ",", "loss_scale", "=", "\"dynamic\"", ",", ")", "\n", "# amp_handle = amp.init(enable_caching=True)", "\n", "logger", ".", "info", "(", "\"enable fp16 with amp\"", ")", "\n", "", "else", ":", "\n", "        ", "optimizer", "=", "BertAdam", "(", "optimizer_grouped_parameters", ",", "\n", "lr", "=", "args", ".", "learning_rate", ",", "\n", "warmup", "=", "args", ".", "warmup_proportion", ",", "\n", "t_total", "=", "t_total", ")", "\n", "\n", "", "if", "args", ".", "local_rank", "!=", "-", "1", ":", "\n", "        ", "try", ":", "\n", "            ", "from", "torch", ".", "nn", ".", "parallel", "import", "DistributedDataParallel", "as", "DDP", "\n", "", "except", "ImportError", ":", "\n", "            ", "raise", "ImportError", "(", "\"DistributedDataParallel\"", ")", "\n", "", "model", "=", "DDP", "(", "model", ",", "device_ids", "=", "[", "\n", "args", ".", "local_rank", "]", ",", "output_device", "=", "args", ".", "local_rank", ",", "find_unused_parameters", "=", "True", ")", "\n", "", "elif", "n_gpu", ">", "1", ":", "\n", "# model = torch.nn.DataParallel(model)", "\n", "        ", "model", "=", "DataParallelImbalance", "(", "model", ")", "\n", "\n", "", "if", "recover_step", ":", "\n", "        ", "logger", ".", "info", "(", "\"***** Recover optimizer: %d *****\"", ",", "recover_step", ")", "\n", "optim_recover", "=", "torch", ".", "load", "(", "os", ".", "path", ".", "join", "(", "\n", "args", ".", "output_dir", ",", "\"optim.{0}.bin\"", ".", "format", "(", "recover_step", ")", ")", ",", "map_location", "=", "'cpu'", ")", "\n", "if", "hasattr", "(", "optim_recover", ",", "'state_dict'", ")", ":", "\n", "            ", "optim_recover", "=", "optim_recover", ".", "state_dict", "(", ")", "\n", "", "optimizer", ".", "load_state_dict", "(", "optim_recover", ")", "\n", "if", "args", ".", "loss_scale", "==", "0", ":", "\n", "            ", "logger", ".", "info", "(", "\"***** Recover optimizer: dynamic_loss_scale *****\"", ")", "\n", "optimizer", ".", "dynamic_loss_scale", "=", "True", "\n", "\n", "", "", "logger", ".", "info", "(", "\"***** CUDA.empty_cache() *****\"", ")", "\n", "torch", ".", "cuda", ".", "empty_cache", "(", ")", "\n", "\n", "if", "args", ".", "do_train", ":", "\n", "        ", "logger", ".", "info", "(", "\"***** Running training *****\"", ")", "\n", "logger", ".", "info", "(", "\"  Batch size = %d\"", ",", "args", ".", "train_batch_size", ")", "\n", "logger", ".", "info", "(", "\"  Num steps = %d\"", ",", "t_total", ")", "\n", "\n", "model", ".", "train", "(", ")", "\n", "if", "recover_step", ":", "\n", "            ", "start_epoch", "=", "recover_step", "+", "1", "\n", "", "else", ":", "\n", "            ", "start_epoch", "=", "1", "\n", "", "for", "i_epoch", "in", "trange", "(", "start_epoch", ",", "int", "(", "args", ".", "num_train_epochs", ")", "+", "1", ",", "desc", "=", "\"Epoch\"", ",", "disable", "=", "args", ".", "local_rank", "not", "in", "(", "-", "1", ",", "0", ")", ")", ":", "\n", "            ", "if", "args", ".", "local_rank", "!=", "-", "1", ":", "\n", "                ", "train_sampler", ".", "set_epoch", "(", "i_epoch", ")", "\n", "", "iter_bar", "=", "tqdm", "(", "train_dataloader", ",", "desc", "=", "'Iter (loss=X.XXX)'", ",", "\n", "disable", "=", "args", ".", "local_rank", "not", "in", "(", "-", "1", ",", "0", ")", ")", "\n", "for", "step", ",", "batch", "in", "enumerate", "(", "iter_bar", ")", ":", "\n", "                ", "batch", "=", "[", "\n", "t", ".", "to", "(", "device", ")", "if", "t", "is", "not", "None", "else", "None", "for", "t", "in", "batch", "]", "\n", "if", "args", ".", "has_sentence_oracle", ":", "\n", "                    ", "input_ids", ",", "segment_ids", ",", "input_mask", ",", "mask_qkv", ",", "lm_label_ids", ",", "masked_pos", ",", "masked_weights", ",", "is_next", ",", "task_idx", ",", "oracle_pos", ",", "oracle_weights", ",", "oracle_labels", "=", "batch", "\n", "", "else", ":", "\n", "                    ", "input_ids", ",", "segment_ids", ",", "input_mask", ",", "mask_qkv", ",", "lm_label_ids", ",", "masked_pos", ",", "masked_weights", ",", "is_next", ",", "task_idx", "=", "batch", "\n", "oracle_pos", ",", "oracle_weights", ",", "oracle_labels", "=", "None", ",", "None", ",", "None", "\n", "", "loss_tuple", "=", "model", "(", "input_ids", ",", "segment_ids", ",", "input_mask", ",", "lm_label_ids", ",", "is_next", ",", "masked_pos", "=", "masked_pos", ",", "masked_weights", "=", "masked_weights", ",", "task_idx", "=", "task_idx", ",", "masked_pos_2", "=", "oracle_pos", ",", "masked_weights_2", "=", "oracle_weights", ",", "\n", "masked_labels_2", "=", "oracle_labels", ",", "mask_qkv", "=", "mask_qkv", ")", "\n", "masked_lm_loss", ",", "next_sentence_loss", "=", "loss_tuple", "\n", "if", "n_gpu", ">", "1", ":", "# mean() to average on multi-gpu.", "\n", "# loss = loss.mean()", "\n", "                    ", "masked_lm_loss", "=", "masked_lm_loss", ".", "mean", "(", ")", "\n", "next_sentence_loss", "=", "next_sentence_loss", ".", "mean", "(", ")", "\n", "", "loss", "=", "masked_lm_loss", "+", "next_sentence_loss", "\n", "\n", "# logging for each step (i.e., before normalization by args.gradient_accumulation_steps)", "\n", "iter_bar", ".", "set_description", "(", "'Iter (loss=%5.3f)'", "%", "loss", ".", "item", "(", ")", ")", "\n", "\n", "# ensure that accumlated gradients are normalized", "\n", "if", "args", ".", "gradient_accumulation_steps", ">", "1", ":", "\n", "                    ", "loss", "=", "loss", "/", "args", ".", "gradient_accumulation_steps", "\n", "\n", "", "if", "args", ".", "fp16", ":", "\n", "                    ", "with", "amp", ".", "scale_loss", "(", "loss", ",", "optimizer", ")", "as", "scaled_loss", ":", "\n", "                        ", "scaled_loss", ".", "backward", "(", ")", "\n", "\n", "# optimizer.backward(loss)", "\n", "# if amp_handle:", "\n", "#     amp_handle._clear_cache()", "\n", "", "", "else", ":", "\n", "                    ", "loss", ".", "backward", "(", ")", "\n", "", "if", "(", "step", "+", "1", ")", "%", "args", ".", "gradient_accumulation_steps", "==", "0", ":", "\n", "                    ", "lr_this_step", "=", "args", ".", "learning_rate", "*", "warmup_linear", "(", "global_step", "/", "t_total", ",", "\n", "args", ".", "warmup_proportion", ")", "\n", "if", "args", ".", "fp16", ":", "\n", "# modify learning rate with special warm up BERT uses", "\n", "                        ", "for", "param_group", "in", "optimizer", ".", "param_groups", ":", "\n", "                            ", "param_group", "[", "'lr'", "]", "=", "lr_this_step", "\n", "", "", "optimizer", ".", "step", "(", ")", "\n", "optimizer", ".", "zero_grad", "(", ")", "\n", "global_step", "+=", "1", "\n", "\n", "# Save a trained model", "\n", "", "", "if", "(", "args", ".", "local_rank", "==", "-", "1", "or", "torch", ".", "distributed", ".", "get_rank", "(", ")", "==", "0", ")", ":", "\n", "                ", "logger", ".", "info", "(", "\n", "\"** ** * Saving fine-tuned model and optimizer ** ** * \"", ")", "\n", "model_to_save", "=", "model", ".", "module", "if", "hasattr", "(", "\n", "model", ",", "'module'", ")", "else", "model", "# Only save the model it-self", "\n", "output_model_file", "=", "os", ".", "path", ".", "join", "(", "\n", "args", ".", "output_dir", ",", "\"model.{0}.bin\"", ".", "format", "(", "i_epoch", ")", ")", "\n", "torch", ".", "save", "(", "model_to_save", ".", "state_dict", "(", ")", ",", "output_model_file", ")", "\n", "output_optim_file", "=", "os", ".", "path", ".", "join", "(", "\n", "args", ".", "output_dir", ",", "\"optim.{0}.bin\"", ".", "format", "(", "i_epoch", ")", ")", "\n", "torch", ".", "save", "(", "optimizer", ".", "state_dict", "(", ")", ",", "output_optim_file", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.biunilm.bi_uni_loader.Bi_Uni_Dataset.__init__": [[21, 76], ["super().__init__", "print", "open", "open", "zip", "open", "min", "tokenizer.tokenize", "tokenizer.tokenize", "bi_uni_loader.Bi_Uni_Dataset.ex_list_s2s.append", "open", "open", "zip", "open", "open", "open", "zip", "tokenizer.tokenize", "bi_uni_loader.Bi_Uni_Dataset.ex_list_uni.append", "len", "len", "len", "src.strip", "tgt.strip", "len", "len", "tokenizer.tokenize", "tokenizer.tokenize", "bi_uni_loader.Bi_Uni_Dataset.ex_list_bi.append", "tokenizer.tokenize", "tokenizer.tokenize", "int", "bi_uni_loader.Bi_Uni_Dataset.ex_list_bi.append", "tgt.strip", "len", "src.strip", "tgt.strip", "len", "len", "src.strip", "tgt.strip", "nsp.strip", "len", "len"], "methods", ["home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.unilm.eval.Eval.__init__", "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.preprocess.tokenizing_1342.tokenize", "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.preprocess.tokenizing_1342.tokenize", "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.preprocess.tokenizing_1342.tokenize", "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.preprocess.tokenizing_1342.tokenize", "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.preprocess.tokenizing_1342.tokenize", "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.preprocess.tokenizing_1342.tokenize", "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.preprocess.tokenizing_1342.tokenize"], ["def", "__init__", "(", "self", ",", "file_src_bi", ",", "file_tgt_bi", ",", "file_tgt_uni", ",", "file_src_s2s", ",", "file_tgt_s2s", ",", "\n", "batch_size", ",", "tokenizer", ",", "max_len", ",", "file_nsp", "=", "None", ",", "\n", "short_sampling_prob", "=", "0.1", ",", "sent_reverse_order", "=", "False", ",", "bi_uni_pipeline", "=", "[", "]", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "tokenizer", "=", "tokenizer", "# tokenize function", "\n", "self", ".", "max_len", "=", "max_len", "# maximum length of tokens", "\n", "self", ".", "short_sampling_prob", "=", "short_sampling_prob", "\n", "self", ".", "bi_uni_pipeline", "=", "bi_uni_pipeline", "\n", "self", ".", "batch_size", "=", "batch_size", "\n", "self", ".", "sent_reverse_order", "=", "sent_reverse_order", "\n", "\n", "# read the file into memory", "\n", "self", ".", "ex_list", "=", "[", "]", "\n", "self", ".", "ex_list_bi", "=", "[", "]", "\n", "self", ".", "ex_list_uni", "=", "[", "]", "\n", "self", ".", "ex_list_s2s", "=", "[", "]", "\n", "\n", "with", "open", "(", "file_src_s2s", ",", "\"r\"", ",", "encoding", "=", "'utf-8'", ")", "as", "f_src", ",", "open", "(", "file_tgt_s2s", ",", "\"r\"", ",", "encoding", "=", "'utf-8'", ")", "as", "f_tgt", ":", "\n", "            ", "for", "src", ",", "tgt", "in", "zip", "(", "f_src", ",", "f_tgt", ")", ":", "\n", "                ", "src_tk", "=", "tokenizer", ".", "tokenize", "(", "src", ".", "strip", "(", ")", ")", "\n", "tgt_tk", "=", "tokenizer", ".", "tokenize", "(", "tgt", ".", "strip", "(", ")", ")", "\n", "assert", "len", "(", "src_tk", ")", ">", "0", "\n", "assert", "len", "(", "tgt_tk", ")", ">", "0", "\n", "self", ".", "ex_list_s2s", ".", "append", "(", "(", "src_tk", ",", "tgt_tk", ")", ")", "\n", "\n", "", "", "if", "file_nsp", "is", "None", ":", "\n", "            ", "with", "open", "(", "file_src_bi", ",", "\"r\"", ",", "encoding", "=", "'utf-8'", ")", "as", "f_src", ",", "open", "(", "file_tgt_bi", ",", "\"r\"", ",", "encoding", "=", "'utf-8'", ")", "as", "f_tgt", ":", "\n", "                ", "for", "src", ",", "tgt", "in", "zip", "(", "f_src", ",", "f_tgt", ")", ":", "\n", "                    ", "src_tk", "=", "tokenizer", ".", "tokenize", "(", "src", ".", "strip", "(", ")", ")", "\n", "tgt_tk", "=", "tokenizer", ".", "tokenize", "(", "tgt", ".", "strip", "(", ")", ")", "\n", "assert", "len", "(", "src_tk", ")", ">", "0", "\n", "assert", "len", "(", "tgt_tk", ")", ">", "0", "\n", "self", ".", "ex_list_bi", ".", "append", "(", "(", "src_tk", ",", "tgt_tk", ",", "-", "1", ")", ")", "\n", "", "", "", "else", ":", "\n", "            ", "with", "open", "(", "file_src_bi", ",", "\"r\"", ",", "encoding", "=", "'utf-8'", ")", "as", "f_src", ",", "open", "(", "file_tgt_bi", ",", "\"r\"", ",", "encoding", "=", "'utf-8'", ")", "as", "f_tgt", ",", "open", "(", "file_nsp", ",", "\"r\"", ",", "encoding", "=", "'utf-8'", ")", "as", "f_nsp", ":", "\n", "                ", "for", "src", ",", "tgt", ",", "nsp", "in", "zip", "(", "f_src", ",", "f_tgt", ",", "f_nsp", ")", ":", "\n", "                    ", "src_tk", "=", "tokenizer", ".", "tokenize", "(", "src", ".", "strip", "(", ")", ")", "\n", "tgt_tk", "=", "tokenizer", ".", "tokenize", "(", "tgt", ".", "strip", "(", ")", ")", "\n", "is_next", "=", "int", "(", "nsp", ".", "strip", "(", ")", ")", "\n", "assert", "len", "(", "src_tk", ")", ">", "0", "\n", "assert", "len", "(", "tgt_tk", ")", ">", "0", "\n", "assert", "is_next", "in", "[", "0", ",", "1", "]", "\n", "self", ".", "ex_list_bi", ".", "append", "(", "(", "src_tk", ",", "tgt_tk", ",", "is_next", ")", ")", "\n", "\n", "", "", "", "with", "open", "(", "file_tgt_uni", ",", "\"r\"", ",", "encoding", "=", "'utf-8'", ")", "as", "f_tgt", ":", "\n", "            ", "for", "tgt", "in", "f_tgt", ":", "\n", "                ", "tgt_tk", "=", "tokenizer", ".", "tokenize", "(", "tgt", ".", "strip", "(", ")", ")", "\n", "assert", "len", "(", "tgt_tk", ")", ">", "0", "\n", "self", ".", "ex_list_uni", ".", "append", "(", "tgt_tk", ")", "\n", "\n", "", "", "self", ".", "total_len", "=", "3", "*", "min", "(", "len", "(", "self", ".", "ex_list_bi", ")", ",", "len", "(", "self", ".", "ex_list_s2s", ")", ",", "len", "(", "self", ".", "ex_list_uni", ")", ")", "\n", "\n", "\n", "\n", "print", "(", "'Load {0} documents'", ".", "format", "(", "self", ".", "total_len", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.biunilm.bi_uni_loader.Bi_Uni_Dataset.__len__": [[77, 79], ["None"], "methods", ["None"], ["", "def", "__len__", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "total_len", "\n", "\n"]], "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.biunilm.bi_uni_loader.Bi_Uni_Dataset.__getitem__": [[80, 94], ["proc", "proc", "proc"], "methods", ["None"], ["", "def", "__getitem__", "(", "self", ",", "idx", ")", ":", "\n", "        ", "if", "idx", "%", "3", "==", "0", ":", "\n", "            ", "instance", "=", "self", ".", "ex_list_bi", "[", "idx", "//", "3", "]", "\n", "proc", "=", "self", ".", "bi_uni_pipeline", "[", "0", "]", "\n", "instance", "=", "proc", "(", "instance", ")", "\n", "", "if", "idx", "%", "3", "==", "1", ":", "\n", "            ", "instance", "=", "self", ".", "ex_list_uni", "[", "idx", "//", "3", "]", "\n", "proc", "=", "self", ".", "bi_uni_pipeline", "[", "1", "]", "\n", "instance", "=", "proc", "(", "instance", ")", "\n", "", "if", "idx", "%", "3", "==", "2", ":", "\n", "            ", "instance", "=", "self", ".", "ex_list_s2s", "[", "idx", "//", "3", "]", "\n", "proc", "=", "self", ".", "bi_uni_pipeline", "[", "2", "]", "\n", "instance", "=", "proc", "(", "instance", ")", "\n", "", "return", "instance", "\n", "\n"]], "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.biunilm.bi_uni_loader.Bi_Uni_Dataset.__iter__": [[95, 103], ["range", "math.ceil", "range", "random.random.randint", "batch.append", "biunilm.loader_utils.batch_list_to_batch_tensors", "float", "bi_uni_loader.Bi_Uni_Dataset.__getitem__"], "methods", ["home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.biunilm.loader_utils.batch_list_to_batch_tensors", "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.biunilm.bidir_loader.BiDirDataset.__getitem__"], ["", "def", "__iter__", "(", "self", ")", ":", "# iterator to load data", "\n", "        ", "for", "__", "in", "range", "(", "math", ".", "ceil", "(", "self", ".", "total_len", "/", "float", "(", "self", ".", "batch_size", ")", ")", ")", ":", "\n", "            ", "batch", "=", "[", "]", "\n", "for", "__", "in", "range", "(", "self", ".", "batch_size", ")", ":", "\n", "                ", "idx", "=", "randint", "(", "0", ",", "self", ".", "total_len", "-", "1", ")", "\n", "batch", ".", "append", "(", "self", ".", "__getitem__", "(", "idx", ")", ")", "\n", "# To Tensor", "\n", "", "yield", "batch_list_to_batch_tensors", "(", "batch", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.biunilm.decode_seq2seq.detokenize": [[34, 42], ["tk.startswith", "r_list.append", "len"], "function", ["None"], ["def", "detokenize", "(", "tk_list", ")", ":", "\n", "    ", "r_list", "=", "[", "]", "\n", "for", "tk", "in", "tk_list", ":", "\n", "        ", "if", "tk", ".", "startswith", "(", "'##'", ")", "and", "len", "(", "r_list", ")", ">", "0", ":", "\n", "            ", "r_list", "[", "-", "1", "]", "=", "r_list", "[", "-", "1", "]", "+", "tk", "[", "2", ":", "]", "\n", "", "else", ":", "\n", "            ", "r_list", ".", "append", "(", "tk", ")", "\n", "", "", "return", "r_list", "\n", "\n"]], "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.biunilm.decode_seq2seq.ascii_print": [[44, 47], ["text.encode.encode", "print"], "function", ["None"], ["", "def", "ascii_print", "(", "text", ")", ":", "\n", "    ", "text", "=", "text", ".", "encode", "(", "\"ascii\"", ",", "\"ignore\"", ")", "\n", "print", "(", "text", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.biunilm.decode_seq2seq.main": [[49, 259], ["argparse.ArgumentParser", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.parse_args", "torch.device", "torch.cuda.device_count", "random.seed", "numpy.random.seed", "torch.manual_seed", "pytorch_pretrained_bert.tokenization.BertTokenizer.from_pretrained", "bi_uni_pipeline.append", "BertTokenizer.from_pretrained.convert_tokens_to_ids", "decode_seq2seq.main._get_token_id_set"], "function", ["home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.pytorch_pretrained_bert.tokenization.BertTokenizer.from_pretrained", "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.pytorch_pretrained_bert.tokenization.BertTokenizer.convert_tokens_to_ids"], ["", "def", "main", "(", ")", ":", "\n", "    ", "parser", "=", "argparse", ".", "ArgumentParser", "(", ")", "\n", "\n", "# Required parameters", "\n", "parser", ".", "add_argument", "(", "\"--bert_model\"", ",", "default", "=", "None", ",", "type", "=", "str", ",", "required", "=", "True", ",", "\n", "help", "=", "\"Bert pre-trained model selected in the list: bert-base-uncased, \"", "\n", "\"bert-large-uncased, bert-base-cased, bert-base-multilingual, bert-base-chinese.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--model_recover_path\"", ",", "default", "=", "None", ",", "type", "=", "str", ",", "\n", "help", "=", "\"The file of fine-tuned pretraining model.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--max_seq_length\"", ",", "default", "=", "512", ",", "type", "=", "int", ",", "\n", "help", "=", "\"The maximum total input sequence length after WordPiece tokenization. \\n\"", "\n", "\"Sequences longer than this will be truncated, and sequences shorter \\n\"", "\n", "\"than this will be padded.\"", ")", "\n", "parser", ".", "add_argument", "(", "'--ffn_type'", ",", "default", "=", "0", ",", "type", "=", "int", ",", "\n", "help", "=", "\"0: default mlp; 1: W((Wx+b) elem_prod x);\"", ")", "\n", "parser", ".", "add_argument", "(", "'--num_qkv'", ",", "default", "=", "0", ",", "type", "=", "int", ",", "\n", "help", "=", "\"Number of different <Q,K,V>.\"", ")", "\n", "parser", ".", "add_argument", "(", "'--seg_emb'", ",", "action", "=", "'store_true'", ",", "\n", "help", "=", "\"Using segment embedding for self-attention.\"", ")", "\n", "\n", "# decoding parameters", "\n", "parser", ".", "add_argument", "(", "'--fp16'", ",", "action", "=", "'store_true'", ",", "\n", "help", "=", "\"Whether to use 16-bit float precision instead of 32-bit\"", ")", "\n", "parser", ".", "add_argument", "(", "'--amp'", ",", "action", "=", "'store_true'", ",", "\n", "help", "=", "\"Whether to use amp for fp16\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--input_file\"", ",", "type", "=", "str", ",", "help", "=", "\"Input file\"", ")", "\n", "parser", ".", "add_argument", "(", "'--subset'", ",", "type", "=", "int", ",", "default", "=", "0", ",", "\n", "help", "=", "\"Decode a subset of the input dataset.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--output_file\"", ",", "type", "=", "str", ",", "help", "=", "\"output file\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--split\"", ",", "type", "=", "str", ",", "default", "=", "\"\"", ",", "\n", "help", "=", "\"Data split (train/val/test).\"", ")", "\n", "parser", ".", "add_argument", "(", "'--tokenized_input'", ",", "action", "=", "'store_true'", ",", "\n", "help", "=", "\"Whether the input is tokenized.\"", ")", "\n", "parser", ".", "add_argument", "(", "'--seed'", ",", "type", "=", "int", ",", "default", "=", "123", ",", "\n", "help", "=", "\"random seed for initialization\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--do_lower_case\"", ",", "action", "=", "'store_true'", ",", "\n", "help", "=", "\"Set this flag if you are using an uncased model.\"", ")", "\n", "parser", ".", "add_argument", "(", "'--new_segment_ids'", ",", "action", "=", "'store_true'", ",", "\n", "help", "=", "\"Use new segment ids for bi-uni-directional LM.\"", ")", "\n", "parser", ".", "add_argument", "(", "'--new_pos_ids'", ",", "action", "=", "'store_true'", ",", "\n", "help", "=", "\"Use new position ids for LMs.\"", ")", "\n", "parser", ".", "add_argument", "(", "'--batch_size'", ",", "type", "=", "int", ",", "default", "=", "4", ",", "\n", "help", "=", "\"Batch size for decoding.\"", ")", "\n", "parser", ".", "add_argument", "(", "'--beam_size'", ",", "type", "=", "int", ",", "default", "=", "1", ",", "\n", "help", "=", "\"Beam size for searching\"", ")", "\n", "parser", ".", "add_argument", "(", "'--length_penalty'", ",", "type", "=", "float", ",", "default", "=", "0", ",", "\n", "help", "=", "\"Length penalty for beam search\"", ")", "\n", "\n", "parser", ".", "add_argument", "(", "'--forbid_duplicate_ngrams'", ",", "action", "=", "'store_true'", ")", "\n", "parser", ".", "add_argument", "(", "'--forbid_ignore_word'", ",", "type", "=", "str", ",", "default", "=", "None", ",", "\n", "help", "=", "\"Ignore the word during forbid_duplicate_ngrams\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--min_len\"", ",", "default", "=", "None", ",", "type", "=", "int", ")", "\n", "parser", ".", "add_argument", "(", "'--need_score_traces'", ",", "action", "=", "'store_true'", ")", "\n", "parser", ".", "add_argument", "(", "'--ngram_size'", ",", "type", "=", "int", ",", "default", "=", "3", ")", "\n", "parser", ".", "add_argument", "(", "'--mode'", ",", "default", "=", "\"s2s\"", ",", "\n", "choices", "=", "[", "\"s2s\"", ",", "\"l2r\"", ",", "\"both\"", "]", ")", "\n", "parser", ".", "add_argument", "(", "'--max_tgt_length'", ",", "type", "=", "int", ",", "default", "=", "128", ",", "\n", "help", "=", "\"maximum length of target sequence\"", ")", "\n", "parser", ".", "add_argument", "(", "'--s2s_special_token'", ",", "action", "=", "'store_true'", ",", "\n", "help", "=", "\"New special tokens ([S2S_SEP]/[S2S_CLS]) of S2S.\"", ")", "\n", "parser", ".", "add_argument", "(", "'--s2s_add_segment'", ",", "action", "=", "'store_true'", ",", "\n", "help", "=", "\"Additional segmental for the encoder of S2S.\"", ")", "\n", "parser", ".", "add_argument", "(", "'--s2s_share_segment'", ",", "action", "=", "'store_true'", ",", "\n", "help", "=", "\"Sharing segment embeddings for the encoder of S2S (used with --s2s_add_segment).\"", ")", "\n", "parser", ".", "add_argument", "(", "'--pos_shift'", ",", "action", "=", "'store_true'", ",", "\n", "help", "=", "\"Using position shift for fine-tuning.\"", ")", "\n", "parser", ".", "add_argument", "(", "'--not_predict_token'", ",", "type", "=", "str", ",", "default", "=", "None", ",", "\n", "help", "=", "\"Do not predict the tokens during decoding.\"", ")", "\n", "\n", "args", "=", "parser", ".", "parse_args", "(", ")", "\n", "\n", "if", "args", ".", "need_score_traces", "and", "args", ".", "beam_size", "<=", "1", ":", "\n", "        ", "raise", "ValueError", "(", "\n", "\"Score trace is only available for beam search with beam size > 1.\"", ")", "\n", "", "if", "args", ".", "max_tgt_length", ">=", "args", ".", "max_seq_length", "-", "2", ":", "\n", "        ", "raise", "ValueError", "(", "\"Maximum tgt length exceeds max seq length - 2.\"", ")", "\n", "\n", "", "device", "=", "torch", ".", "device", "(", "\n", "\"cuda\"", "if", "torch", ".", "cuda", ".", "is_available", "(", ")", "else", "\"cpu\"", ")", "\n", "n_gpu", "=", "torch", ".", "cuda", ".", "device_count", "(", ")", "\n", "\n", "random", ".", "seed", "(", "args", ".", "seed", ")", "\n", "np", ".", "random", ".", "seed", "(", "args", ".", "seed", ")", "\n", "torch", ".", "manual_seed", "(", "args", ".", "seed", ")", "\n", "if", "n_gpu", ">", "0", ":", "\n", "        ", "torch", ".", "cuda", ".", "manual_seed_all", "(", "args", ".", "seed", ")", "\n", "\n", "", "tokenizer", "=", "BertTokenizer", ".", "from_pretrained", "(", "\n", "args", ".", "bert_model", ",", "do_lower_case", "=", "args", ".", "do_lower_case", ")", "\n", "\n", "tokenizer", ".", "max_len", "=", "args", ".", "max_seq_length", "\n", "\n", "pair_num_relation", "=", "0", "\n", "bi_uni_pipeline", "=", "[", "]", "\n", "bi_uni_pipeline", ".", "append", "(", "seq2seq_loader", ".", "Preprocess4Seq2seqDecoder", "(", "list", "(", "tokenizer", ".", "vocab", ".", "keys", "(", ")", ")", ",", "tokenizer", ".", "convert_tokens_to_ids", ",", "args", ".", "max_seq_length", ",", "max_tgt_length", "=", "args", ".", "max_tgt_length", ",", "new_segment_ids", "=", "args", ".", "new_segment_ids", ",", "\n", "mode", "=", "\"s2s\"", ",", "num_qkv", "=", "args", ".", "num_qkv", ",", "s2s_special_token", "=", "args", ".", "s2s_special_token", ",", "s2s_add_segment", "=", "args", ".", "s2s_add_segment", ",", "s2s_share_segment", "=", "args", ".", "s2s_share_segment", ",", "pos_shift", "=", "args", ".", "pos_shift", ")", ")", "\n", "\n", "amp_handle", "=", "None", "\n", "if", "args", ".", "fp16", "and", "args", ".", "amp", ":", "\n", "        ", "from", "apex", "import", "amp", "\n", "amp_handle", "=", "amp", ".", "init", "(", "enable_caching", "=", "True", ")", "\n", "logger", ".", "info", "(", "\"enable fp16 with amp\"", ")", "\n", "\n", "# Prepare model", "\n", "", "cls_num_labels", "=", "2", "\n", "type_vocab_size", "=", "6", "+", "(", "1", "if", "args", ".", "s2s_add_segment", "else", "0", ")", "if", "args", ".", "new_segment_ids", "else", "2", "\n", "mask_word_id", ",", "eos_word_ids", ",", "sos_word_id", "=", "tokenizer", ".", "convert_tokens_to_ids", "(", "\n", "[", "\"[MASK]\"", ",", "\"[SEP]\"", ",", "\"[S2S_SOS]\"", "]", ")", "\n", "\n", "def", "_get_token_id_set", "(", "s", ")", ":", "\n", "        ", "r", "=", "None", "\n", "if", "s", ":", "\n", "            ", "w_list", "=", "[", "]", "\n", "for", "w", "in", "s", ".", "split", "(", "'|'", ")", ":", "\n", "                ", "if", "w", ".", "startswith", "(", "'['", ")", "and", "w", ".", "endswith", "(", "']'", ")", ":", "\n", "                    ", "w_list", ".", "append", "(", "w", ".", "upper", "(", ")", ")", "\n", "", "else", ":", "\n", "                    ", "w_list", ".", "append", "(", "w", ")", "\n", "", "", "r", "=", "set", "(", "tokenizer", ".", "convert_tokens_to_ids", "(", "w_list", ")", ")", "\n", "", "return", "r", "\n", "\n", "", "forbid_ignore_set", "=", "_get_token_id_set", "(", "args", ".", "forbid_ignore_word", ")", "\n", "not_predict_set", "=", "_get_token_id_set", "(", "args", ".", "not_predict_token", ")", "\n", "print", "(", "args", ".", "model_recover_path", ")", "\n", "for", "model_recover_path", "in", "glob", ".", "glob", "(", "args", ".", "model_recover_path", ".", "strip", "(", ")", ")", ":", "\n", "        ", "logger", ".", "info", "(", "\"***** Recover model: %s *****\"", ",", "model_recover_path", ")", "\n", "model_recover", "=", "torch", ".", "load", "(", "model_recover_path", ")", "\n", "model", "=", "BertForSeq2SeqDecoder", ".", "from_pretrained", "(", "args", ".", "bert_model", ",", "state_dict", "=", "model_recover", ",", "num_labels", "=", "cls_num_labels", ",", "num_rel", "=", "pair_num_relation", ",", "type_vocab_size", "=", "type_vocab_size", ",", "task_idx", "=", "3", ",", "mask_word_id", "=", "mask_word_id", ",", "search_beam_size", "=", "args", ".", "beam_size", ",", "\n", "length_penalty", "=", "args", ".", "length_penalty", ",", "eos_id", "=", "eos_word_ids", ",", "sos_id", "=", "sos_word_id", ",", "forbid_duplicate_ngrams", "=", "args", ".", "forbid_duplicate_ngrams", ",", "forbid_ignore_set", "=", "forbid_ignore_set", ",", "not_predict_set", "=", "not_predict_set", ",", "ngram_size", "=", "args", ".", "ngram_size", ",", "min_len", "=", "args", ".", "min_len", ",", "mode", "=", "args", ".", "mode", ",", "max_position_embeddings", "=", "args", ".", "max_seq_length", ",", "ffn_type", "=", "args", ".", "ffn_type", ",", "num_qkv", "=", "args", ".", "num_qkv", ",", "seg_emb", "=", "args", ".", "seg_emb", ",", "pos_shift", "=", "args", ".", "pos_shift", ")", "\n", "del", "model_recover", "\n", "\n", "if", "args", ".", "fp16", ":", "\n", "            ", "model", ".", "half", "(", ")", "\n", "", "model", ".", "to", "(", "device", ")", "\n", "if", "n_gpu", ">", "1", ":", "\n", "            ", "model", "=", "torch", ".", "nn", ".", "DataParallel", "(", "model", ")", "\n", "\n", "", "torch", ".", "cuda", ".", "empty_cache", "(", ")", "\n", "model", ".", "eval", "(", ")", "\n", "next_i", "=", "0", "\n", "max_src_length", "=", "args", ".", "max_seq_length", "-", "2", "-", "args", ".", "max_tgt_length", "\n", "\n", "with", "open", "(", "args", ".", "input_file", ",", "encoding", "=", "\"utf-8\"", ")", "as", "fin", ":", "\n", "            ", "input_lines", "=", "[", "x", ".", "strip", "(", ")", "for", "x", "in", "fin", ".", "readlines", "(", ")", "]", "\n", "if", "args", ".", "subset", ">", "0", ":", "\n", "                ", "logger", ".", "info", "(", "\"Decoding subset: %d\"", ",", "args", ".", "subset", ")", "\n", "input_lines", "=", "input_lines", "[", ":", "args", ".", "subset", "]", "\n", "", "", "data_tokenizer", "=", "WhitespaceTokenizer", "(", ")", "if", "args", ".", "tokenized_input", "else", "tokenizer", "\n", "input_lines", "=", "[", "data_tokenizer", ".", "tokenize", "(", "\n", "x", ")", "[", ":", "max_src_length", "]", "for", "x", "in", "input_lines", "]", "\n", "input_lines", "=", "sorted", "(", "list", "(", "enumerate", "(", "input_lines", ")", ")", ",", "\n", "key", "=", "lambda", "x", ":", "-", "len", "(", "x", "[", "1", "]", ")", ")", "\n", "output_lines", "=", "[", "\"\"", "]", "*", "len", "(", "input_lines", ")", "\n", "score_trace_list", "=", "[", "None", "]", "*", "len", "(", "input_lines", ")", "\n", "total_batch", "=", "math", ".", "ceil", "(", "len", "(", "input_lines", ")", "/", "args", ".", "batch_size", ")", "\n", "\n", "with", "tqdm", "(", "total", "=", "total_batch", ")", "as", "pbar", ":", "\n", "            ", "while", "next_i", "<", "len", "(", "input_lines", ")", ":", "\n", "                ", "_chunk", "=", "input_lines", "[", "next_i", ":", "next_i", "+", "args", ".", "batch_size", "]", "\n", "buf_id", "=", "[", "x", "[", "0", "]", "for", "x", "in", "_chunk", "]", "\n", "buf", "=", "[", "x", "[", "1", "]", "for", "x", "in", "_chunk", "]", "\n", "next_i", "+=", "args", ".", "batch_size", "\n", "max_a_len", "=", "max", "(", "[", "len", "(", "x", ")", "for", "x", "in", "buf", "]", ")", "\n", "instances", "=", "[", "]", "\n", "for", "instance", "in", "[", "(", "x", ",", "max_a_len", ")", "for", "x", "in", "buf", "]", ":", "\n", "                    ", "for", "proc", "in", "bi_uni_pipeline", ":", "\n", "                        ", "instances", ".", "append", "(", "proc", "(", "instance", ")", ")", "\n", "", "", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "                    ", "batch", "=", "seq2seq_loader", ".", "batch_list_to_batch_tensors", "(", "\n", "instances", ")", "\n", "batch", "=", "[", "\n", "t", ".", "to", "(", "device", ")", "if", "t", "is", "not", "None", "else", "None", "for", "t", "in", "batch", "]", "\n", "input_ids", ",", "token_type_ids", ",", "position_ids", ",", "input_mask", ",", "mask_qkv", ",", "task_idx", "=", "batch", "\n", "traces", "=", "model", "(", "input_ids", ",", "token_type_ids", ",", "\n", "position_ids", ",", "input_mask", ",", "task_idx", "=", "task_idx", ",", "mask_qkv", "=", "mask_qkv", ")", "\n", "if", "args", ".", "beam_size", ">", "1", ":", "\n", "                        ", "traces", "=", "{", "k", ":", "v", ".", "tolist", "(", ")", "for", "k", ",", "v", "in", "traces", ".", "items", "(", ")", "}", "\n", "output_ids", "=", "traces", "[", "'pred_seq'", "]", "\n", "", "else", ":", "\n", "                        ", "output_ids", "=", "traces", ".", "tolist", "(", ")", "\n", "", "for", "i", "in", "range", "(", "len", "(", "buf", ")", ")", ":", "\n", "                        ", "w_ids", "=", "output_ids", "[", "i", "]", "\n", "output_buf", "=", "tokenizer", ".", "convert_ids_to_tokens", "(", "w_ids", ")", "\n", "output_tokens", "=", "[", "]", "\n", "for", "t", "in", "output_buf", ":", "\n", "                            ", "if", "t", "in", "(", "\"[SEP]\"", ",", "\"[PAD]\"", ")", ":", "\n", "                                ", "break", "\n", "", "output_tokens", ".", "append", "(", "t", ")", "\n", "", "output_sequence", "=", "' '", ".", "join", "(", "detokenize", "(", "output_tokens", ")", ")", "\n", "output_lines", "[", "buf_id", "[", "i", "]", "]", "=", "output_sequence", "\n", "if", "args", ".", "need_score_traces", ":", "\n", "                            ", "score_trace_list", "[", "buf_id", "[", "i", "]", "]", "=", "{", "\n", "'scores'", ":", "traces", "[", "'scores'", "]", "[", "i", "]", ",", "'wids'", ":", "traces", "[", "'wids'", "]", "[", "i", "]", ",", "'ptrs'", ":", "traces", "[", "'ptrs'", "]", "[", "i", "]", "}", "\n", "", "", "", "pbar", ".", "update", "(", "1", ")", "\n", "", "", "if", "args", ".", "output_file", ":", "\n", "            ", "fn_out", "=", "args", ".", "output_file", "\n", "", "else", ":", "\n", "            ", "fn_out", "=", "model_recover_path", "+", "'.'", "+", "args", ".", "split", "\n", "", "with", "open", "(", "fn_out", ",", "\"w\"", ",", "encoding", "=", "\"utf-8\"", ")", "as", "fout", ":", "\n", "            ", "for", "l", "in", "output_lines", ":", "\n", "                ", "fout", ".", "write", "(", "l", ")", "\n", "fout", ".", "write", "(", "\"\\n\"", ")", "\n", "\n", "", "", "if", "args", ".", "need_score_traces", ":", "\n", "            ", "with", "open", "(", "fn_out", "+", "\".trace.pickle\"", ",", "\"wb\"", ")", "as", "fout_trace", ":", "\n", "                ", "pickle", ".", "dump", "(", "\n", "{", "\"version\"", ":", "0.0", ",", "\"num_samples\"", ":", "len", "(", "input_lines", ")", "}", ",", "fout_trace", ")", "\n", "for", "x", "in", "score_trace_list", ":", "\n", "                    ", "pickle", ".", "dump", "(", "x", ",", "fout_trace", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.biunilm.unidir_loader.UniDirDataset.__init__": [[35, 64], ["super().__init__", "print", "open", "len", "tokenizer.tokenize", "unidir_loader.UniDirDataset.ex_list.append", "tgt.strip", "len"], "methods", ["home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.unilm.eval.Eval.__init__", "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.preprocess.tokenizing_1342.tokenize"], ["def", "__init__", "(", "self", ",", "file_tgt", ",", "batch_size", ",", "tokenizer", ",", "max_len", ",", "file_oracle", "=", "None", ",", "short_sampling_prob", "=", "0.1", ",", "sent_reverse_order", "=", "False", ",", "uni_pipeline", "=", "[", "]", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "tokenizer", "=", "tokenizer", "# tokenize function", "\n", "self", ".", "max_len", "=", "max_len", "# maximum length of tokens", "\n", "self", ".", "short_sampling_prob", "=", "short_sampling_prob", "\n", "self", ".", "uni_pipeline", "=", "uni_pipeline", "\n", "self", ".", "batch_size", "=", "batch_size", "\n", "self", ".", "sent_reverse_order", "=", "sent_reverse_order", "\n", "\n", "# read the file into memory", "\n", "self", ".", "ex_list", "=", "[", "]", "\n", "if", "file_oracle", "is", "None", ":", "\n", "            ", "with", "open", "(", "file_tgt", ",", "\"r\"", ",", "encoding", "=", "'utf-8'", ")", "as", "f_tgt", ":", "\n", "                ", "for", "tgt", "in", "f_tgt", ":", "\n", "                    ", "tgt_tk", "=", "tokenizer", ".", "tokenize", "(", "tgt", ".", "strip", "(", ")", ")", "\n", "assert", "len", "(", "tgt_tk", ")", ">", "0", "\n", "self", ".", "ex_list", ".", "append", "(", "tgt_tk", ")", "\n", "# else:", "\n", "#     with open(file_src, \"r\", encoding='utf-8') as f_src, \\", "\n", "#             open(file_tgt, \"r\", encoding='utf-8') as f_tgt, \\", "\n", "#             open(file_oracle, \"r\", encoding='utf-8') as f_orc:", "\n", "#         for src, tgt, orc in zip(f_src, f_tgt, f_orc):", "\n", "#             src_tk = tokenizer.tokenize(src.strip())", "\n", "#             tgt_tk = tokenizer.tokenize(tgt.strip())", "\n", "#             s_st, labl = orc.split('\\t')", "\n", "#             s_st = [int(x) for x in s_st.split()]", "\n", "#             labl = [int(x) for x in labl.split()]", "\n", "#             self.ex_list.append((src_tk, tgt_tk, s_st, labl))", "\n", "", "", "", "print", "(", "'Load {0} documents'", ".", "format", "(", "len", "(", "self", ".", "ex_list", ")", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.biunilm.unidir_loader.UniDirDataset.__len__": [[65, 67], ["len"], "methods", ["None"], ["", "def", "__len__", "(", "self", ")", ":", "\n", "        ", "return", "len", "(", "self", ".", "ex_list", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.biunilm.unidir_loader.UniDirDataset.__getitem__": [[68, 73], ["random.random.choice", "random.random.choice."], "methods", ["None"], ["", "def", "__getitem__", "(", "self", ",", "idx", ")", ":", "\n", "        ", "instance", "=", "self", ".", "ex_list", "[", "idx", "]", "\n", "proc", "=", "choice", "(", "self", ".", "uni_pipeline", ")", "\n", "instance", "=", "proc", "(", "instance", ")", "\n", "return", "instance", "\n", "\n"]], "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.biunilm.unidir_loader.UniDirDataset.__iter__": [[74, 82], ["range", "math.ceil", "range", "random.random.randint", "batch.append", "biunilm.loader_utils.batch_list_to_batch_tensors", "len", "float", "unidir_loader.UniDirDataset.__getitem__", "len"], "methods", ["home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.biunilm.loader_utils.batch_list_to_batch_tensors", "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.biunilm.bidir_loader.BiDirDataset.__getitem__"], ["", "def", "__iter__", "(", "self", ")", ":", "# iterator to load data", "\n", "        ", "for", "__", "in", "range", "(", "math", ".", "ceil", "(", "len", "(", "self", ".", "ex_list", ")", "/", "float", "(", "self", ".", "batch_size", ")", ")", ")", ":", "\n", "            ", "batch", "=", "[", "]", "\n", "for", "__", "in", "range", "(", "self", ".", "batch_size", ")", ":", "\n", "                ", "idx", "=", "randint", "(", "0", ",", "len", "(", "self", ".", "ex_list", ")", "-", "1", ")", "\n", "batch", ".", "append", "(", "self", ".", "__getitem__", "(", "idx", ")", ")", "\n", "# To Tensor", "\n", "", "yield", "batch_list_to_batch_tensors", "(", "batch", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.biunilm.unidir_loader.Preprocess4UniDirection.__init__": [[87, 120], ["biunilm.loader_utils.Pipeline.__init__", "torch.tril", "torch.transpose", "truncate_config.get", "torch.ones", "torch.tril", "torch.ones"], "methods", ["home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.unilm.eval.Eval.__init__"], ["def", "__init__", "(", "self", ",", "max_pred", ",", "mask_prob", ",", "vocab_words", ",", "indexer", ",", "max_len", "=", "512", ",", "skipgram_prb", "=", "0", ",", "skipgram_size", "=", "0", ",", "block_mask", "=", "False", ",", "mask_whole_word", "=", "False", ",", "new_segment_ids", "=", "False", ",", "truncate_config", "=", "{", "}", ",", "mask_source_words", "=", "False", ",", "mode", "=", "\"l2r\"", ",", "has_oracle", "=", "False", ",", "num_qkv", "=", "0", ",", "s2s_special_token", "=", "False", ",", "s2s_add_segment", "=", "False", ",", "s2s_share_segment", "=", "False", ",", "pos_shift", "=", "False", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "max_len", "=", "max_len", "\n", "self", ".", "max_pred", "=", "max_pred", "# max tokens of prediction", "\n", "self", ".", "mask_prob", "=", "mask_prob", "# masking probability", "\n", "self", ".", "vocab_words", "=", "vocab_words", "# vocabulary (sub)words", "\n", "self", ".", "indexer", "=", "indexer", "# function from token to token index", "\n", "self", ".", "max_len", "=", "max_len", "\n", "self", ".", "_tril_matrix", "=", "torch", ".", "tril", "(", "torch", ".", "ones", "(", "\n", "(", "max_len", ",", "max_len", ")", ",", "dtype", "=", "torch", ".", "long", ")", ")", "## lower triangle matrix as l2r input mask", "\n", "\n", "self", ".", "_tril_matrix_upper", "=", "torch", ".", "transpose", "(", "torch", ".", "tril", "(", "torch", ".", "ones", "(", "\n", "(", "max_len", ",", "max_len", ")", ",", "dtype", "=", "torch", ".", "long", ")", ")", ",", "0", ",", "1", ")", "## upper triangle matrix as r2l input mask", "\n", "\n", "self", ".", "skipgram_prb", "=", "skipgram_prb", "\n", "self", ".", "skipgram_size", "=", "skipgram_size", "\n", "self", ".", "mask_whole_word", "=", "mask_whole_word", "\n", "self", ".", "new_segment_ids", "=", "new_segment_ids", "\n", "self", ".", "always_truncate_tail", "=", "truncate_config", ".", "get", "(", "\n", "'always_truncate_tail'", ",", "False", ")", "\n", "# self.max_len_a = truncate_config.get('max_len_a', None)", "\n", "# self.max_len_b = truncate_config.get('max_len_b', None)", "\n", "# self.trunc_seg = truncate_config.get('trunc_seg', None)", "\n", "self", ".", "task_idx", "=", "3", "# relax projection layer for different tasks", "\n", "self", ".", "mask_source_words", "=", "mask_source_words", "\n", "assert", "mode", "in", "(", "\"l2r\"", ",", "\"r2l\"", ")", "\n", "self", ".", "mode", "=", "mode", "\n", "self", ".", "has_oracle", "=", "has_oracle", "\n", "self", ".", "num_qkv", "=", "num_qkv", "\n", "self", ".", "s2s_special_token", "=", "s2s_special_token", "\n", "# self.s2s_add_segment = s2s_add_segment", "\n", "# self.s2s_share_segment = s2s_share_segment", "\n", "self", ".", "pos_shift", "=", "pos_shift", "\n", "\n"]], "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.biunilm.unidir_loader.Preprocess4UniDirection.__call__": [[121, 285], ["unidir_loader.truncate_tokens", "unidir_loader.Preprocess4UniDirection.indexer", "unidir_loader.Preprocess4UniDirection.extend", "segment_ids.extend", "torch.zeros", "min", "unidir_loader.Preprocess4UniDirection.indexer", "len", "min", "set", "enumerate", "random.random.shuffle", "set", "max", "list", "unidir_loader.Preprocess4UniDirection.indexer", "len", "mask_qkv.extend", "input_mask[].copy_", "input_mask[].copy_", "len", "max", "range", "len", "random.random.shuffle", "len", "unidir_loader.Preprocess4UniDirection.extend", "list.extend", "masked_weights.extend", "len", "len", "len", "range", "int", "cand_pos.append", "set.add", "len", "random.random.randint", "random.random.random", "len", "len", "len", "round", "tokens[].startswith", "tokens[].startswith", "random.random.random", "unidir_loader.Preprocess4UniDirection.__call__._expand_whole_word"], "methods", ["home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.biunilm.unidir_loader.truncate_tokens", "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.biunilm.loader_utils.TrieTree.add", "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.biunilm.loader_utils._expand_whole_word"], ["", "def", "__call__", "(", "self", ",", "instance", ")", ":", "\n", "        ", "tokens_b", "=", "instance", "\n", "\n", "# if self.pos_shift and self.mode == 'l2r':", "\n", "#     tokens_b = ['[CLS]'] + tokens_b", "\n", "\n", "# -2  for special tokens [CLS], [SEP],", "\n", "num_truncated_a", ",", "_", "=", "truncate_tokens", "(", "tokens_b", ",", "self", ".", "max_len", "-", "2", ",", "always_truncate_tail", "=", "self", ".", "always_truncate_tail", ")", "\n", "\n", "# Add Special Tokens", "\n", "if", "self", ".", "s2s_special_token", ":", "\n", "            ", "if", "self", ".", "mode", "==", "'l2r'", ":", "\n", "                ", "tokens", "=", "[", "'[L2R_CLS]'", "]", "+", "tokens_b", "+", "[", "'[L2R_SEP]'", "]", "\n", "", "else", ":", "\n", "                ", "tokens", "=", "[", "'[R2L_CLS]'", "]", "+", "tokens_b", "+", "[", "'[R2L_SEP]'", "]", "\n", "", "", "else", ":", "\n", "            ", "tokens", "=", "[", "'[CLS]'", "]", "+", "tokens_b", "+", "[", "'[SEP]'", "]", "\n", "\n", "", "if", "self", ".", "new_segment_ids", ":", "\n", "            ", "if", "self", ".", "mode", "==", "\"l2r\"", ":", "\n", "                ", "segment_ids", "=", "[", "2", "]", "*", "(", "len", "(", "tokens", ")", ")", "\n", "", "else", ":", "\n", "                ", "segment_ids", "=", "[", "3", "]", "*", "(", "len", "(", "tokens", ")", ")", "\n", "", "", "else", ":", "\n", "            ", "segment_ids", "=", "[", "0", "]", "*", "(", "len", "(", "tokens", ")", ")", "\n", "\n", "", "if", "self", ".", "pos_shift", ":", "\n", "            ", "n_pred", "=", "min", "(", "self", ".", "max_pred", ",", "len", "(", "tokens_b", ")", "+", "1", ")", "\n", "masked_pos", "=", "[", "i", "for", "i", "in", "range", "(", "len", "(", "tokens_b", ")", "+", "1", ")", "]", "## predict end token in shift mode", "\n", "masked_weights", "=", "[", "1", "]", "*", "n_pred", "\n", "masked_ids", "=", "self", ".", "indexer", "(", "tokens_b", "+", "[", "'[SEP]'", "]", ")", "\n", "", "else", ":", "\n", "# For masked Language Models", "\n", "# the number of prediction is sometimes less than max_pred when sequence is short", "\n", "            ", "effective_length", "=", "len", "(", "tokens_b", ")", "\n", "# if self.mask_source_words:", "\n", "#     effective_length += len(tokens_a)", "\n", "n_pred", "=", "min", "(", "self", ".", "max_pred", ",", "max", "(", "\n", "1", ",", "int", "(", "round", "(", "effective_length", "*", "self", ".", "mask_prob", ")", ")", ")", ")", "\n", "# candidate positions of masked tokens", "\n", "cand_pos", "=", "[", "]", "\n", "special_pos", "=", "set", "(", ")", "\n", "for", "i", ",", "tk", "in", "enumerate", "(", "tokens", ")", ":", "\n", "# only mask tokens_b (target sequence)", "\n", "# we will mask [SEP] as an ending symbol ? \u9884\u8bad\u7ec3\u662f\u5426mask[SEP]", "\n", "                ", "if", "(", "tk", "!=", "'[CLS]'", ")", ":", "\n", "                    ", "cand_pos", ".", "append", "(", "i", ")", "\n", "# elif self.mask_source_words and (i < len(tokens_a)+2) and (tk != '[CLS]') and (not tk.startswith('[SEP')):", "\n", "#     cand_pos.append(i)", "\n", "", "else", ":", "\n", "                    ", "special_pos", ".", "add", "(", "i", ")", "\n", "", "", "shuffle", "(", "cand_pos", ")", "\n", "\n", "masked_pos", "=", "set", "(", ")", "\n", "max_cand_pos", "=", "max", "(", "cand_pos", ")", "\n", "for", "pos", "in", "cand_pos", ":", "\n", "                ", "if", "len", "(", "masked_pos", ")", ">=", "n_pred", ":", "\n", "                    ", "break", "\n", "", "if", "pos", "in", "masked_pos", ":", "\n", "                    ", "continue", "\n", "\n", "", "def", "_expand_whole_word", "(", "st", ",", "end", ")", ":", "\n", "                    ", "new_st", ",", "new_end", "=", "st", ",", "end", "\n", "while", "(", "new_st", ">=", "0", ")", "and", "tokens", "[", "new_st", "]", ".", "startswith", "(", "'##'", ")", ":", "\n", "                        ", "new_st", "-=", "1", "\n", "", "while", "(", "new_end", "<", "len", "(", "tokens", ")", ")", "and", "tokens", "[", "new_end", "]", ".", "startswith", "(", "'##'", ")", ":", "\n", "                        ", "new_end", "+=", "1", "\n", "", "return", "new_st", ",", "new_end", "\n", "\n", "", "if", "(", "self", ".", "skipgram_prb", ">", "0", ")", "and", "(", "self", ".", "skipgram_size", ">=", "2", ")", "and", "(", "rand", "(", ")", "<", "self", ".", "skipgram_prb", ")", ":", "\n", "# ngram", "\n", "                    ", "cur_skipgram_size", "=", "randint", "(", "2", ",", "self", ".", "skipgram_size", ")", "\n", "if", "self", ".", "mask_whole_word", ":", "\n", "                        ", "st_pos", ",", "end_pos", "=", "_expand_whole_word", "(", "\n", "pos", ",", "pos", "+", "cur_skipgram_size", ")", "\n", "", "else", ":", "\n", "                        ", "st_pos", ",", "end_pos", "=", "pos", ",", "pos", "+", "cur_skipgram_size", "\n", "", "", "else", ":", "\n", "# directly mask", "\n", "                    ", "if", "self", ".", "mask_whole_word", ":", "\n", "                        ", "st_pos", ",", "end_pos", "=", "_expand_whole_word", "(", "pos", ",", "pos", "+", "1", ")", "\n", "", "else", ":", "\n", "                        ", "st_pos", ",", "end_pos", "=", "pos", ",", "pos", "+", "1", "\n", "\n", "", "", "for", "mp", "in", "range", "(", "st_pos", ",", "end_pos", ")", ":", "\n", "                    ", "if", "(", "0", "<", "mp", "<=", "max_cand_pos", ")", "and", "(", "mp", "not", "in", "special_pos", ")", ":", "\n", "                        ", "masked_pos", ".", "add", "(", "mp", ")", "\n", "", "else", ":", "\n", "                        ", "break", "\n", "\n", "", "", "", "masked_pos", "=", "list", "(", "masked_pos", ")", "\n", "if", "len", "(", "masked_pos", ")", ">", "n_pred", ":", "\n", "                ", "shuffle", "(", "masked_pos", ")", "\n", "masked_pos", "=", "masked_pos", "[", ":", "n_pred", "]", "\n", "\n", "", "masked_tokens", "=", "[", "tokens", "[", "pos", "]", "for", "pos", "in", "masked_pos", "]", "\n", "for", "pos", "in", "masked_pos", ":", "\n", "                ", "if", "rand", "(", ")", "<", "0.8", ":", "# 80%", "\n", "                    ", "tokens", "[", "pos", "]", "=", "'[MASK]'", "\n", "", "elif", "rand", "(", ")", "<", "0.5", ":", "# 10%", "\n", "                    ", "tokens", "[", "pos", "]", "=", "get_random_word", "(", "self", ".", "vocab_words", ")", "\n", "# when n_pred < max_pred, we only calculate loss within n_pred", "\n", "", "", "masked_weights", "=", "[", "1", "]", "*", "len", "(", "masked_tokens", ")", "\n", "\n", "# Token Indexing", "\n", "masked_ids", "=", "self", ".", "indexer", "(", "masked_tokens", ")", "\n", "# Token Indexing", "\n", "", "input_ids", "=", "self", ".", "indexer", "(", "tokens", ")", "\n", "\n", "# Zero Padding", "\n", "n_pad", "=", "self", ".", "max_len", "-", "len", "(", "input_ids", ")", "\n", "input_ids", ".", "extend", "(", "[", "0", "]", "*", "n_pad", ")", "\n", "segment_ids", ".", "extend", "(", "[", "0", "]", "*", "n_pad", ")", "\n", "\n", "if", "self", ".", "num_qkv", ">", "1", ":", "\n", "            ", "mask_qkv", "=", "[", "1", "]", "*", "(", "len", "(", "tokens_b", ")", "+", "2", ")", "\n", "mask_qkv", ".", "extend", "(", "[", "0", "]", "*", "n_pad", ")", "\n", "", "else", ":", "\n", "            ", "mask_qkv", "=", "None", "\n", "\n", "", "input_mask", "=", "torch", ".", "zeros", "(", "self", ".", "max_len", ",", "self", ".", "max_len", ",", "dtype", "=", "torch", ".", "long", ")", "\n", "if", "self", ".", "mode", "==", "\"l2r\"", ":", "\n", "            ", "st", ",", "end", "=", "0", ",", "len", "(", "tokens_b", ")", "+", "2", "\n", "input_mask", "[", "st", ":", "end", ",", "st", ":", "end", "]", ".", "copy_", "(", "self", ".", "_tril_matrix", "[", ":", "end", ",", ":", "end", "]", ")", "\n", "", "else", ":", "\n", "            ", "st", ",", "end", "=", "0", ",", "len", "(", "tokens_b", ")", "+", "2", "\n", "input_mask", "[", "st", ":", "end", ",", "st", ":", "end", "]", ".", "copy_", "(", "self", ".", "_tril_matrix_upper", "[", ":", "end", ",", ":", "end", "]", ")", "\n", "\n", "# Zero Padding for masked target", "\n", "", "if", "self", ".", "max_pred", ">", "n_pred", ":", "\n", "            ", "n_pad", "=", "self", ".", "max_pred", "-", "n_pred", "\n", "if", "masked_ids", "is", "not", "None", ":", "\n", "                ", "masked_ids", ".", "extend", "(", "[", "0", "]", "*", "n_pad", ")", "\n", "", "if", "masked_pos", "is", "not", "None", ":", "\n", "                ", "masked_pos", ".", "extend", "(", "[", "0", "]", "*", "n_pad", ")", "\n", "", "if", "masked_weights", "is", "not", "None", ":", "\n", "                ", "masked_weights", ".", "extend", "(", "[", "0", "]", "*", "n_pad", ")", "\n", "\n", "", "", "oracle_pos", "=", "None", "\n", "oracle_weights", "=", "None", "\n", "oracle_labels", "=", "None", "\n", "# if self.has_oracle:", "\n", "#     s_st, labls = instance[2:]", "\n", "#     oracle_pos = []", "\n", "#     oracle_labels = []", "\n", "#     for st, lb in zip(s_st, labls):", "\n", "#         st = st - num_truncated_a[0]", "\n", "#         if st > 0 and st < len(tokens_a):", "\n", "#             oracle_pos.append(st)", "\n", "#             oracle_labels.append(lb)", "\n", "#     oracle_pos = oracle_pos[:20]", "\n", "#     oracle_labels = oracle_labels[:20]", "\n", "#     oracle_weights = [1] * len(oracle_pos)", "\n", "#     if len(oracle_pos) < 20:", "\n", "#         x_pad = 20 - len(oracle_pos)", "\n", "#         oracle_pos.extend([0] * x_pad)", "\n", "#         oracle_labels.extend([0] * x_pad)", "\n", "#         oracle_weights.extend([0] * x_pad)", "\n", "\n", "#     return (input_ids, segment_ids, input_mask, mask_qkv, masked_ids,", "\n", "#             masked_pos, masked_weights, -1, self.task_idx,", "\n", "#             oracle_pos, oracle_weights, oracle_labels)", "\n", "\n", "return", "(", "input_ids", ",", "segment_ids", ",", "input_mask", ",", "mask_qkv", ",", "masked_ids", ",", "masked_pos", ",", "masked_weights", ",", "-", "1", ",", "self", ".", "task_idx", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.biunilm.unidir_loader.truncate_tokens": [[16, 30], ["len", "trunc_tokens.pop", "random.random"], "function", ["None"], ["def", "truncate_tokens", "(", "tokens", ",", "max_len", ",", "always_truncate_tail", "=", "False", ")", ":", "\n", "    ", "num_truncated", "=", "[", "0", ",", "0", "]", "\n", "while", "True", ":", "\n", "        ", "if", "len", "(", "tokens", ")", "<=", "max_len", ":", "\n", "            ", "break", "\n", "", "trunc_tokens", "=", "tokens", "\n", "# whether always truncate source sequences", "\n", "if", "(", "not", "always_truncate_tail", ")", "and", "(", "rand", "(", ")", "<", "0.5", ")", ":", "\n", "            ", "del", "trunc_tokens", "[", "0", "]", "\n", "num_truncated", "[", "0", "]", "+=", "1", "\n", "", "else", ":", "\n", "            ", "trunc_tokens", ".", "pop", "(", ")", "\n", "num_truncated", "[", "1", "]", "+=", "1", "\n", "", "", "return", "num_truncated", "\n", "\n"]], "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.biunilm.bidir_loader.BiDirDataset.__init__": [[57, 99], ["super().__init__", "print", "len", "open", "open", "zip", "open", "open", "open", "zip", "tokenizer.tokenize", "tokenizer.tokenize", "bidir_loader.BiDirDataset.ex_list.append", "tokenizer.tokenize", "tokenizer.tokenize", "int", "bidir_loader.BiDirDataset.ex_list.append", "src.strip", "tgt.strip", "len", "len", "src.strip", "tgt.strip", "nsp.strip", "len", "len"], "methods", ["home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.unilm.eval.Eval.__init__", "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.preprocess.tokenizing_1342.tokenize", "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.preprocess.tokenizing_1342.tokenize", "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.preprocess.tokenizing_1342.tokenize", "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.preprocess.tokenizing_1342.tokenize"], ["def", "__init__", "(", "self", ",", "file_src", ",", "file_tgt", ",", "batch_size", ",", "tokenizer", ",", "max_len", ",", "file_nsp", "=", "None", ",", "file_oracle", "=", "None", ",", "short_sampling_prob", "=", "0.1", ",", "sent_reverse_order", "=", "False", ",", "bi_pipeline", "=", "[", "]", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "tokenizer", "=", "tokenizer", "# tokenize function", "\n", "self", ".", "max_len", "=", "max_len", "# maximum length of tokens", "\n", "self", ".", "short_sampling_prob", "=", "short_sampling_prob", "\n", "self", ".", "bi_pipeline", "=", "bi_pipeline", "\n", "self", ".", "batch_size", "=", "batch_size", "\n", "self", ".", "sent_reverse_order", "=", "sent_reverse_order", "\n", "\n", "# read the file into memory", "\n", "self", ".", "ex_list", "=", "[", "]", "\n", "if", "file_oracle", "is", "None", ":", "\n", "            ", "if", "file_nsp", "is", "None", ":", "\n", "                ", "with", "open", "(", "file_src", ",", "\"r\"", ",", "encoding", "=", "'utf-8'", ")", "as", "f_src", ",", "open", "(", "file_tgt", ",", "\"r\"", ",", "encoding", "=", "'utf-8'", ")", "as", "f_tgt", ":", "\n", "                    ", "for", "src", ",", "tgt", "in", "zip", "(", "f_src", ",", "f_tgt", ")", ":", "\n", "                        ", "src_tk", "=", "tokenizer", ".", "tokenize", "(", "src", ".", "strip", "(", ")", ")", "\n", "tgt_tk", "=", "tokenizer", ".", "tokenize", "(", "tgt", ".", "strip", "(", ")", ")", "\n", "assert", "len", "(", "src_tk", ")", ">", "0", "\n", "assert", "len", "(", "tgt_tk", ")", ">", "0", "\n", "self", ".", "ex_list", ".", "append", "(", "(", "src_tk", ",", "tgt_tk", ",", "-", "1", ")", ")", "\n", "", "", "", "else", ":", "\n", "                ", "with", "open", "(", "file_src", ",", "\"r\"", ",", "encoding", "=", "'utf-8'", ")", "as", "f_src", ",", "open", "(", "file_tgt", ",", "\"r\"", ",", "encoding", "=", "'utf-8'", ")", "as", "f_tgt", ",", "open", "(", "file_nsp", ",", "\"r\"", ",", "encoding", "=", "'utf-8'", ")", "as", "f_nsp", ":", "\n", "                    ", "for", "src", ",", "tgt", ",", "nsp", "in", "zip", "(", "f_src", ",", "f_tgt", ",", "f_nsp", ")", ":", "\n", "                        ", "src_tk", "=", "tokenizer", ".", "tokenize", "(", "src", ".", "strip", "(", ")", ")", "\n", "tgt_tk", "=", "tokenizer", ".", "tokenize", "(", "tgt", ".", "strip", "(", ")", ")", "\n", "is_next", "=", "int", "(", "nsp", ".", "strip", "(", ")", ")", "\n", "assert", "len", "(", "src_tk", ")", ">", "0", "\n", "assert", "len", "(", "tgt_tk", ")", ">", "0", "\n", "assert", "is_next", "in", "[", "0", ",", "1", "]", "\n", "self", ".", "ex_list", ".", "append", "(", "(", "src_tk", ",", "tgt_tk", ",", "is_next", ")", ")", "\n", "# else:", "\n", "#     with open(file_src, \"r\", encoding='utf-8') as f_src, \\", "\n", "#             open(file_tgt, \"r\", encoding='utf-8') as f_tgt, \\", "\n", "#             open(file_oracle, \"r\", encoding='utf-8') as f_orc:", "\n", "#         for src, tgt, orc in zip(f_src, f_tgt, f_orc):", "\n", "#             src_tk = tokenizer.tokenize(src.strip())", "\n", "#             tgt_tk = tokenizer.tokenize(tgt.strip())", "\n", "#             s_st, labl = orc.split('\\t')", "\n", "#             s_st = [int(x) for x in s_st.split()]", "\n", "#             labl = [int(x) for x in labl.split()]", "\n", "#             self.ex_list.append((src_tk, tgt_tk, s_st, labl))", "\n", "", "", "", "", "print", "(", "'Load {0} documents'", ".", "format", "(", "len", "(", "self", ".", "ex_list", ")", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.biunilm.bidir_loader.BiDirDataset.__len__": [[100, 102], ["len"], "methods", ["None"], ["", "def", "__len__", "(", "self", ")", ":", "\n", "        ", "return", "len", "(", "self", ".", "ex_list", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.biunilm.bidir_loader.BiDirDataset.__getitem__": [[103, 108], ["random.random.choice", "random.random.choice."], "methods", ["None"], ["", "def", "__getitem__", "(", "self", ",", "idx", ")", ":", "\n", "        ", "instance", "=", "self", ".", "ex_list", "[", "idx", "]", "\n", "proc", "=", "choice", "(", "self", ".", "bi_pipeline", ")", "\n", "instance", "=", "proc", "(", "instance", ")", "\n", "return", "instance", "\n", "\n"]], "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.biunilm.bidir_loader.BiDirDataset.__iter__": [[109, 117], ["range", "math.ceil", "range", "random.random.randint", "batch.append", "biunilm.loader_utils.batch_list_to_batch_tensors", "len", "float", "bidir_loader.BiDirDataset.__getitem__", "len"], "methods", ["home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.biunilm.loader_utils.batch_list_to_batch_tensors", "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.biunilm.bidir_loader.BiDirDataset.__getitem__"], ["", "def", "__iter__", "(", "self", ")", ":", "# iterator to load data", "\n", "        ", "for", "__", "in", "range", "(", "math", ".", "ceil", "(", "len", "(", "self", ".", "ex_list", ")", "/", "float", "(", "self", ".", "batch_size", ")", ")", ")", ":", "\n", "            ", "batch", "=", "[", "]", "\n", "for", "__", "in", "range", "(", "self", ".", "batch_size", ")", ":", "\n", "                ", "idx", "=", "randint", "(", "0", ",", "len", "(", "self", ".", "ex_list", ")", "-", "1", ")", "\n", "batch", ".", "append", "(", "self", ".", "__getitem__", "(", "idx", ")", ")", "\n", "# To Tensor", "\n", "", "yield", "batch_list_to_batch_tensors", "(", "batch", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.biunilm.bidir_loader.Preprocess4BiDirection.__init__": [[122, 151], ["biunilm.loader_utils.Pipeline.__init__", "torch.tril", "truncate_config.get", "truncate_config.get", "truncate_config.get", "truncate_config.get", "torch.ones"], "methods", ["home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.unilm.eval.Eval.__init__"], ["def", "__init__", "(", "self", ",", "max_pred", ",", "mask_prob", ",", "vocab_words", ",", "indexer", ",", "max_len", "=", "512", ",", "skipgram_prb", "=", "0", ",", "skipgram_size", "=", "0", ",", "block_mask", "=", "False", ",", "mask_whole_word", "=", "False", ",", "new_segment_ids", "=", "False", ",", "truncate_config", "=", "{", "}", ",", "mask_source_words", "=", "False", ",", "mode", "=", "\"\"", ",", "has_oracle", "=", "False", ",", "num_qkv", "=", "0", ",", "s2s_special_token", "=", "False", ",", "s2s_add_segment", "=", "False", ",", "s2s_share_segment", "=", "False", ",", "pos_shift", "=", "False", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "max_len", "=", "max_len", "\n", "self", ".", "max_pred", "=", "max_pred", "# max tokens of prediction", "\n", "self", ".", "mask_prob", "=", "mask_prob", "# masking probability", "\n", "self", ".", "vocab_words", "=", "vocab_words", "# vocabulary (sub)words", "\n", "self", ".", "indexer", "=", "indexer", "# function from token to token index", "\n", "self", ".", "max_len", "=", "max_len", "\n", "self", ".", "_tril_matrix", "=", "torch", ".", "tril", "(", "torch", ".", "ones", "(", "\n", "(", "max_len", ",", "max_len", ")", ",", "dtype", "=", "torch", ".", "long", ")", ")", "\n", "self", ".", "skipgram_prb", "=", "skipgram_prb", "\n", "self", ".", "skipgram_size", "=", "skipgram_size", "\n", "self", ".", "mask_whole_word", "=", "mask_whole_word", "\n", "self", ".", "new_segment_ids", "=", "new_segment_ids", "\n", "self", ".", "always_truncate_tail", "=", "truncate_config", ".", "get", "(", "\n", "'always_truncate_tail'", ",", "False", ")", "\n", "self", ".", "max_len_a", "=", "truncate_config", ".", "get", "(", "'max_len_a'", ",", "None", ")", "\n", "self", ".", "max_len_b", "=", "truncate_config", ".", "get", "(", "'max_len_b'", ",", "None", ")", "\n", "self", ".", "trunc_seg", "=", "truncate_config", ".", "get", "(", "'trunc_seg'", ",", "None", ")", "\n", "self", ".", "task_idx", "=", "3", "# relax projection layer for different tasks", "\n", "self", ".", "mask_source_words", "=", "mask_source_words", "\n", "# assert mode in (\"s2s\", \"l2r\")", "\n", "self", ".", "mode", "=", "mode", "\n", "self", ".", "has_oracle", "=", "has_oracle", "\n", "self", ".", "num_qkv", "=", "num_qkv", "\n", "self", ".", "s2s_special_token", "=", "s2s_special_token", "\n", "self", ".", "s2s_add_segment", "=", "s2s_add_segment", "\n", "self", ".", "s2s_share_segment", "=", "s2s_share_segment", "\n", "self", ".", "pos_shift", "=", "pos_shift", "\n", "\n"]], "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.biunilm.bidir_loader.Preprocess4BiDirection.__call__": [[152, 301], ["bidir_loader.truncate_tokens_pair", "bidir_loader.Preprocess4BiDirection.indexer", "bidir_loader.Preprocess4BiDirection.extend", "segment_ids.extend", "torch.zeros", "input_mask[].fill_", "min", "set", "enumerate", "random.random.shuffle", "set", "max", "list", "bidir_loader.Preprocess4BiDirection.indexer", "len", "mask_qkv.extend", "zip", "len", "len", "max", "range", "len", "random.random.shuffle", "len", "bidir_loader.Preprocess4BiDirection.extend", "list.extend", "masked_weights.extend", "len", "len", "oracle_pos.extend", "oracle_labels.extend", "oracle_weights.extend", "len", "len", "int", "cand_pos.append", "set.add", "len", "random.random.randint", "random.random.random", "len", "len", "oracle_pos.append", "oracle_labels.append", "len", "round", "tk.startswith", "tokens[].startswith", "tokens[].startswith", "random.random.random", "bidir_loader.Preprocess4BiDirection.__call__._expand_whole_word"], "methods", ["home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.biunilm.bidir_loader.truncate_tokens_pair", "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.biunilm.loader_utils.TrieTree.add", "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.biunilm.loader_utils._expand_whole_word"], ["", "def", "__call__", "(", "self", ",", "instance", ")", ":", "\n", "        ", "tokens_a", ",", "tokens_b", ",", "is_next", "=", "instance", "[", ":", "3", "]", "\n", "\n", "# if self.pos_shift:", "\n", "#     tokens_b = ['[S2S_SOS]'] + tokens_b", "\n", "\n", "# -3  for special tokens [CLS], [SEP], [SEP]", "\n", "num_truncated_a", ",", "_", "=", "truncate_tokens_pair", "(", "tokens_a", ",", "tokens_b", ",", "self", ".", "max_len", "-", "3", ",", "max_len_a", "=", "self", ".", "max_len_a", ",", "\n", "max_len_b", "=", "self", ".", "max_len_b", ",", "trunc_seg", "=", "self", ".", "trunc_seg", ",", "always_truncate_tail", "=", "self", ".", "always_truncate_tail", ")", "\n", "\n", "\n", "tokens", "=", "[", "'[CLS]'", "]", "+", "tokens_a", "+", "[", "'[SEP]'", "]", "+", "tokens_b", "+", "[", "'[SEP]'", "]", "\n", "\n", "\n", "segment_ids", "=", "[", "0", "]", "*", "(", "len", "(", "tokens_a", ")", "+", "2", ")", "+", "[", "1", "]", "*", "(", "len", "(", "tokens_b", ")", "+", "1", ")", "\n", "\n", "# if self.pos_shift:", "\n", "# n_pred = min(self.max_pred, len(tokens_b))", "\n", "# masked_pos = [len(tokens_a)+2+i for i in range(len(tokens_b))]", "\n", "# masked_weights = [1]*n_pred", "\n", "# masked_ids = self.indexer(tokens_b[1:]+['[SEP]'])", "\n", "if", "True", ":", "\n", "# For masked Language Models", "\n", "# the number of prediction is sometimes less than max_pred when sequence is short", "\n", "            ", "effective_length", "=", "len", "(", "tokens_a", ")", "+", "len", "(", "tokens_b", ")", "\n", "# if self.mask_source_words:", "\n", "#     effective_length += len(tokens_a)", "\n", "n_pred", "=", "min", "(", "self", ".", "max_pred", ",", "max", "(", "\n", "1", ",", "int", "(", "round", "(", "effective_length", "*", "self", ".", "mask_prob", ")", ")", ")", ")", "\n", "# candidate positions of masked tokens", "\n", "cand_pos", "=", "[", "]", "\n", "special_pos", "=", "set", "(", ")", "\n", "for", "i", ",", "tk", "in", "enumerate", "(", "tokens", ")", ":", "\n", "# mask tokens_a and tokens_b", "\n", "# we will not mask [SEP]", "\n", "                ", "if", "(", "not", "tk", ".", "startswith", "(", "'[SEP'", ")", ")", "and", "(", "tk", "!=", "'[CLS]'", ")", ":", "\n", "                    ", "cand_pos", ".", "append", "(", "i", ")", "\n", "", "else", ":", "\n", "                    ", "special_pos", ".", "add", "(", "i", ")", "\n", "", "", "shuffle", "(", "cand_pos", ")", "\n", "\n", "masked_pos", "=", "set", "(", ")", "\n", "max_cand_pos", "=", "max", "(", "cand_pos", ")", "\n", "for", "pos", "in", "cand_pos", ":", "\n", "                ", "if", "len", "(", "masked_pos", ")", ">=", "n_pred", ":", "\n", "                    ", "break", "\n", "", "if", "pos", "in", "masked_pos", ":", "\n", "                    ", "continue", "\n", "\n", "", "def", "_expand_whole_word", "(", "st", ",", "end", ")", ":", "\n", "                    ", "new_st", ",", "new_end", "=", "st", ",", "end", "\n", "while", "(", "new_st", ">=", "0", ")", "and", "tokens", "[", "new_st", "]", ".", "startswith", "(", "'##'", ")", ":", "\n", "                        ", "new_st", "-=", "1", "\n", "", "while", "(", "new_end", "<", "len", "(", "tokens", ")", ")", "and", "tokens", "[", "new_end", "]", ".", "startswith", "(", "'##'", ")", ":", "\n", "                        ", "new_end", "+=", "1", "\n", "", "return", "new_st", ",", "new_end", "\n", "\n", "", "if", "(", "self", ".", "skipgram_prb", ">", "0", ")", "and", "(", "self", ".", "skipgram_size", ">=", "2", ")", "and", "(", "rand", "(", ")", "<", "self", ".", "skipgram_prb", ")", ":", "\n", "# ngram", "\n", "                    ", "cur_skipgram_size", "=", "randint", "(", "2", ",", "self", ".", "skipgram_size", ")", "\n", "if", "self", ".", "mask_whole_word", ":", "\n", "                        ", "st_pos", ",", "end_pos", "=", "_expand_whole_word", "(", "\n", "pos", ",", "pos", "+", "cur_skipgram_size", ")", "\n", "", "else", ":", "\n", "                        ", "st_pos", ",", "end_pos", "=", "pos", ",", "pos", "+", "cur_skipgram_size", "\n", "", "", "else", ":", "\n", "# directly mask", "\n", "                    ", "if", "self", ".", "mask_whole_word", ":", "\n", "                        ", "st_pos", ",", "end_pos", "=", "_expand_whole_word", "(", "pos", ",", "pos", "+", "1", ")", "\n", "", "else", ":", "\n", "                        ", "st_pos", ",", "end_pos", "=", "pos", ",", "pos", "+", "1", "\n", "\n", "", "", "for", "mp", "in", "range", "(", "st_pos", ",", "end_pos", ")", ":", "\n", "                    ", "if", "(", "0", "<", "mp", "<=", "max_cand_pos", ")", "and", "(", "mp", "not", "in", "special_pos", ")", ":", "\n", "                        ", "masked_pos", ".", "add", "(", "mp", ")", "\n", "", "else", ":", "\n", "                        ", "break", "\n", "\n", "", "", "", "masked_pos", "=", "list", "(", "masked_pos", ")", "\n", "if", "len", "(", "masked_pos", ")", ">", "n_pred", ":", "\n", "                ", "shuffle", "(", "masked_pos", ")", "\n", "masked_pos", "=", "masked_pos", "[", ":", "n_pred", "]", "\n", "\n", "", "masked_tokens", "=", "[", "tokens", "[", "pos", "]", "for", "pos", "in", "masked_pos", "]", "\n", "for", "pos", "in", "masked_pos", ":", "\n", "                ", "if", "rand", "(", ")", "<", "0.8", ":", "# 80%", "\n", "                    ", "tokens", "[", "pos", "]", "=", "'[MASK]'", "\n", "", "elif", "rand", "(", ")", "<", "0.5", ":", "# 10%", "\n", "                    ", "tokens", "[", "pos", "]", "=", "get_random_word", "(", "self", ".", "vocab_words", ")", "\n", "# when n_pred < max_pred, we only calculate loss within n_pred", "\n", "", "", "masked_weights", "=", "[", "1", "]", "*", "len", "(", "masked_tokens", ")", "\n", "\n", "# Token Indexing", "\n", "masked_ids", "=", "self", ".", "indexer", "(", "masked_tokens", ")", "\n", "# Token Indexing", "\n", "", "input_ids", "=", "self", ".", "indexer", "(", "tokens", ")", "\n", "\n", "# Zero Padding", "\n", "n_pad", "=", "self", ".", "max_len", "-", "len", "(", "input_ids", ")", "\n", "input_ids", ".", "extend", "(", "[", "0", "]", "*", "n_pad", ")", "\n", "segment_ids", ".", "extend", "(", "[", "0", "]", "*", "n_pad", ")", "\n", "\n", "if", "self", ".", "num_qkv", ">", "1", ":", "\n", "            ", "mask_qkv", "=", "[", "0", "]", "*", "(", "len", "(", "tokens_a", ")", "+", "2", ")", "+", "[", "1", "]", "*", "(", "len", "(", "tokens_b", ")", "+", "1", ")", "\n", "mask_qkv", ".", "extend", "(", "[", "0", "]", "*", "n_pad", ")", "\n", "", "else", ":", "\n", "            ", "mask_qkv", "=", "None", "\n", "\n", "", "input_mask", "=", "torch", ".", "zeros", "(", "self", ".", "max_len", ",", "self", ".", "max_len", ",", "dtype", "=", "torch", ".", "long", ")", "\n", "st", ",", "end", "=", "0", ",", "len", "(", "tokens_a", ")", "+", "len", "(", "tokens_b", ")", "+", "3", "\n", "input_mask", "[", "st", ":", "end", ",", "st", ":", "end", "]", ".", "fill_", "(", "1", ")", "\n", "\n", "\n", "# Zero Padding for masked target", "\n", "if", "self", ".", "max_pred", ">", "n_pred", ":", "\n", "            ", "n_pad", "=", "self", ".", "max_pred", "-", "n_pred", "\n", "if", "masked_ids", "is", "not", "None", ":", "\n", "                ", "masked_ids", ".", "extend", "(", "[", "0", "]", "*", "n_pad", ")", "\n", "", "if", "masked_pos", "is", "not", "None", ":", "\n", "                ", "masked_pos", ".", "extend", "(", "[", "0", "]", "*", "n_pad", ")", "\n", "", "if", "masked_weights", "is", "not", "None", ":", "\n", "                ", "masked_weights", ".", "extend", "(", "[", "0", "]", "*", "n_pad", ")", "\n", "\n", "", "", "oracle_pos", "=", "None", "\n", "oracle_weights", "=", "None", "\n", "oracle_labels", "=", "None", "\n", "if", "self", ".", "has_oracle", ":", "\n", "            ", "s_st", ",", "labls", "=", "instance", "[", "2", ":", "]", "\n", "oracle_pos", "=", "[", "]", "\n", "oracle_labels", "=", "[", "]", "\n", "for", "st", ",", "lb", "in", "zip", "(", "s_st", ",", "labls", ")", ":", "\n", "                ", "st", "=", "st", "-", "num_truncated_a", "[", "0", "]", "\n", "if", "st", ">", "0", "and", "st", "<", "len", "(", "tokens_a", ")", ":", "\n", "                    ", "oracle_pos", ".", "append", "(", "st", ")", "\n", "oracle_labels", ".", "append", "(", "lb", ")", "\n", "", "", "oracle_pos", "=", "oracle_pos", "[", ":", "20", "]", "\n", "oracle_labels", "=", "oracle_labels", "[", ":", "20", "]", "\n", "oracle_weights", "=", "[", "1", "]", "*", "len", "(", "oracle_pos", ")", "\n", "if", "len", "(", "oracle_pos", ")", "<", "20", ":", "\n", "                ", "x_pad", "=", "20", "-", "len", "(", "oracle_pos", ")", "\n", "oracle_pos", ".", "extend", "(", "[", "0", "]", "*", "x_pad", ")", "\n", "oracle_labels", ".", "extend", "(", "[", "0", "]", "*", "x_pad", ")", "\n", "oracle_weights", ".", "extend", "(", "[", "0", "]", "*", "x_pad", ")", "\n", "\n", "", "return", "(", "input_ids", ",", "segment_ids", ",", "input_mask", ",", "mask_qkv", ",", "masked_ids", ",", "\n", "masked_pos", ",", "masked_weights", ",", "is_next", ",", "self", ".", "task_idx", ",", "\n", "oracle_pos", ",", "oracle_weights", ",", "oracle_labels", ")", "\n", "\n", "", "return", "(", "input_ids", ",", "segment_ids", ",", "input_mask", ",", "mask_qkv", ",", "masked_ids", ",", "masked_pos", ",", "masked_weights", ",", "is_next", ",", "self", ".", "task_idx", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.biunilm.bidir_loader.truncate_tokens_pair": [[16, 52], ["trunc_tokens.pop", "len", "len", "len", "random.random", "len", "len", "len"], "function", ["None"], ["def", "truncate_tokens_pair", "(", "tokens_a", ",", "tokens_b", ",", "max_len", ",", "max_len_a", "=", "0", ",", "max_len_b", "=", "0", ",", "trunc_seg", "=", "None", ",", "always_truncate_tail", "=", "False", ")", ":", "\n", "    ", "num_truncated_a", "=", "[", "0", ",", "0", "]", "\n", "num_truncated_b", "=", "[", "0", ",", "0", "]", "\n", "while", "True", ":", "\n", "        ", "if", "len", "(", "tokens_a", ")", "+", "len", "(", "tokens_b", ")", "<=", "max_len", ":", "\n", "            ", "break", "\n", "", "if", "(", "max_len_a", ">", "0", ")", "and", "len", "(", "tokens_a", ")", ">", "max_len_a", ":", "\n", "            ", "trunc_tokens", "=", "tokens_a", "\n", "num_truncated", "=", "num_truncated_a", "\n", "", "elif", "(", "max_len_b", ">", "0", ")", "and", "len", "(", "tokens_b", ")", ">", "max_len_b", ":", "\n", "            ", "trunc_tokens", "=", "tokens_b", "\n", "num_truncated", "=", "num_truncated_b", "\n", "", "elif", "trunc_seg", ":", "\n", "# truncate the specified segment", "\n", "            ", "if", "trunc_seg", "==", "'a'", ":", "\n", "                ", "trunc_tokens", "=", "tokens_a", "\n", "num_truncated", "=", "num_truncated_a", "\n", "", "else", ":", "\n", "                ", "trunc_tokens", "=", "tokens_b", "\n", "num_truncated", "=", "num_truncated_b", "\n", "", "", "else", ":", "\n", "# truncate the longer segment", "\n", "            ", "if", "len", "(", "tokens_a", ")", ">", "len", "(", "tokens_b", ")", ":", "\n", "                ", "trunc_tokens", "=", "tokens_a", "\n", "num_truncated", "=", "num_truncated_a", "\n", "", "else", ":", "\n", "                ", "trunc_tokens", "=", "tokens_b", "\n", "num_truncated", "=", "num_truncated_b", "\n", "# whether always truncate source sequences", "\n", "", "", "if", "(", "not", "always_truncate_tail", ")", "and", "(", "rand", "(", ")", "<", "0.5", ")", ":", "\n", "            ", "del", "trunc_tokens", "[", "0", "]", "\n", "num_truncated", "[", "0", "]", "+=", "1", "\n", "", "else", ":", "\n", "            ", "trunc_tokens", ".", "pop", "(", ")", "\n", "num_truncated", "[", "1", "]", "+=", "1", "\n", "", "", "return", "num_truncated_a", ",", "num_truncated_b", "\n", "\n"]], "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.biunilm.eval_merge._get_max_epoch_model": [[44, 55], ["glob.glob", "glob.glob", "os.path.join", "os.path.join", "set", "set", "max", "int", "int", "pathlib.Path().stem.split", "pathlib.Path().stem.split", "pathlib.Path", "pathlib.Path"], "function", ["home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.preprocess.process_daren.split", "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.preprocess.process_daren.split"], ["def", "_get_max_epoch_model", "(", "output_dir", ")", ":", "\n", "    ", "fn_model_list", "=", "glob", ".", "glob", "(", "os", ".", "path", ".", "join", "(", "output_dir", ",", "\"model.*.bin\"", ")", ")", "\n", "fn_optim_list", "=", "glob", ".", "glob", "(", "os", ".", "path", ".", "join", "(", "output_dir", ",", "\"optim.*.bin\"", ")", ")", "\n", "if", "(", "not", "fn_model_list", ")", "or", "(", "not", "fn_optim_list", ")", ":", "\n", "        ", "return", "None", "\n", "", "both_set", "=", "set", "(", "[", "int", "(", "Path", "(", "fn", ")", ".", "stem", ".", "split", "(", "'.'", ")", "[", "-", "1", "]", ")", "for", "fn", "in", "fn_model_list", "]", "\n", ")", "&", "set", "(", "[", "int", "(", "Path", "(", "fn", ")", ".", "stem", ".", "split", "(", "'.'", ")", "[", "-", "1", "]", ")", "for", "fn", "in", "fn_optim_list", "]", ")", "\n", "if", "both_set", ":", "\n", "        ", "return", "max", "(", "both_set", ")", "\n", "", "else", ":", "\n", "        ", "return", "None", "\n", "\n"]], "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.biunilm.eval_merge.main": [[57, 458], ["argparse.ArgumentParser", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.parse_args", "parser.parse_args.output_dir.replace", "parser.parse_args.log_dir.replace", "os.makedirs", "os.makedirs", "json.dump", "logger.info", "int", "random.seed", "numpy.random.seed", "torch.manual_seed", "torch.manual_seed", "pytorch_pretrained_bert.tokenization.BertTokenizer.from_pretrained", "int", "glob.glob", "os.getenv", "os.getenv", "open", "torch.device", "torch.device", "torch.cuda.device_count", "torch.cuda.device_count", "torch.cuda.set_device", "torch.cuda.set_device", "torch.device", "torch.device", "torch.init_process_group", "ValueError", "torch.cuda.manual_seed_all", "torch.cuda.manual_seed_all", "ValueError", "torch.barrier", "pytorch_pretrained_bert.tokenization.WhitespaceTokenizer", "torch.barrier", "print", "biunilm.Preprocess4Seq2seq", "biunilm.Preprocess4UniDirection", "biunilm.Preprocess4UniDirection", "biunilm.Preprocess4BiDirection", "os.path.join", "os.path.join", "os.path.join", "os.path.join", "os.path.join", "os.path.join", "torch.utils.data.DataLoader", "torch.utils.data.DataLoader", "os.path.join", "os.path.exists", "logger.info", "torch.load", "torch.load", "pytorch_pretrained_bert.modeling.BertForPreTrainingLossMask.from_pretrained", "BertForPreTrainingLossMask.from_pretrained.to", "logger.info", "torch.cuda.empty_cache", "torch.cuda.empty_cache", "os.path.join", "bool", "list", "list", "list", "list", "os.path.join", "biunilm.Seq2SeqDataset", "biunilm.BiDirDataset", "biunilm.UniDirDataset", "biunilm.Bi_Uni_Dataset", "torch.utils.data.RandomSampler", "torch.utils.data.distributed.DistributedSampler", "torch.barrier", "BertForPreTrainingLossMask.from_pretrained.eval", "tqdm.tqdm", "enumerate", "print", "BertTokenizer.from_pretrained.vocab.keys", "BertTokenizer.from_pretrained.vocab.keys", "BertTokenizer.from_pretrained.vocab.keys", "BertTokenizer.from_pretrained.vocab.keys", "torch.get_world_size", "len", "BertForPreTrainingLossMask.from_pretrained.bert.embeddings.word_embeddings.float", "BertForPreTrainingLossMask.from_pretrained.bert.embeddings.position_embeddings.float", "BertForPreTrainingLossMask.from_pretrained.bert.embeddings.token_type_embeddings.float", "tqdm.tqdm.set_description", "open", "f.write", "torch.cuda.is_available", "torch.cuda.is_available", "torch.no_grad", "torch.no_grad", "BertForPreTrainingLossMask.from_pretrained.", "masked_lm_loss.mean().item", "next_sentence_loss.mean().item", "t.to", "loss.item", "str", "masked_lm_loss.mean", "next_sentence_loss.mean", "str"], "function", ["home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.pytorch_pretrained_bert.tokenization.BertTokenizer.from_pretrained", "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.pytorch_pretrained_bert.tokenization.BertTokenizer.from_pretrained"], ["", "", "def", "main", "(", ")", ":", "\n", "    ", "parser", "=", "argparse", ".", "ArgumentParser", "(", ")", "\n", "\n", "# Required parameters", "\n", "parser", ".", "add_argument", "(", "\"--data_dir\"", ",", "\n", "default", "=", "None", ",", "\n", "type", "=", "str", ",", "\n", "required", "=", "True", ",", "\n", "help", "=", "\"The input data dir. Should contain the .tsv files (or other data files) for the task.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--src_file\"", ",", "default", "=", "None", ",", "type", "=", "str", ",", "\n", "help", "=", "\"The input data file name.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--tgt_file\"", ",", "default", "=", "None", ",", "type", "=", "str", ",", "\n", "help", "=", "\"The output data file name.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--bert_model\"", ",", "default", "=", "None", ",", "type", "=", "str", ",", "required", "=", "True", ",", "\n", "help", "=", "\"Bert pre-trained model selected in the list: bert-base-uncased, \"", "\n", "\"bert-large-uncased, bert-base-cased, bert-base-multilingual, bert-base-chinese.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--config_path\"", ",", "default", "=", "None", ",", "type", "=", "str", ",", "\n", "help", "=", "\"Bert config file path.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--output_dir\"", ",", "\n", "default", "=", "None", ",", "\n", "type", "=", "str", ",", "\n", "required", "=", "True", ",", "\n", "help", "=", "\"The output directory where the model predictions and checkpoints will be written.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--log_dir\"", ",", "\n", "default", "=", "''", ",", "\n", "type", "=", "str", ",", "\n", "required", "=", "True", ",", "\n", "help", "=", "\"The output directory where the log will be written.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--model_recover_path\"", ",", "\n", "default", "=", "None", ",", "\n", "type", "=", "str", ",", "\n", "help", "=", "\"The file of fine-tuned pretraining model.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--optim_recover_path\"", ",", "\n", "default", "=", "None", ",", "\n", "type", "=", "str", ",", "\n", "help", "=", "\"The file of pretraining optimizer.\"", ")", "\n", "\n", "# Other parameters", "\n", "parser", ".", "add_argument", "(", "\"--max_seq_length\"", ",", "\n", "default", "=", "128", ",", "\n", "type", "=", "int", ",", "\n", "help", "=", "\"The maximum total input sequence length after WordPiece tokenization. \\n\"", "\n", "\"Sequences longer than this will be truncated, and sequences shorter \\n\"", "\n", "\"than this will be padded.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--do_train\"", ",", "\n", "action", "=", "'store_true'", ",", "\n", "help", "=", "\"Whether to run training.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--do_eval\"", ",", "\n", "action", "=", "'store_true'", ",", "\n", "help", "=", "\"Whether to run eval on the dev set.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--do_lower_case\"", ",", "\n", "action", "=", "'store_true'", ",", "\n", "help", "=", "\"Set this flag if you are using an uncased model.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--train_batch_size\"", ",", "\n", "default", "=", "32", ",", "\n", "type", "=", "int", ",", "\n", "help", "=", "\"Total batch size for training.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--eval_batch_size\"", ",", "\n", "default", "=", "64", ",", "\n", "type", "=", "int", ",", "\n", "help", "=", "\"Total batch size for eval.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--learning_rate\"", ",", "default", "=", "5e-5", ",", "type", "=", "float", ",", "\n", "help", "=", "\"The initial learning rate for Adam.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--label_smoothing\"", ",", "default", "=", "0", ",", "type", "=", "float", ",", "\n", "help", "=", "\"The initial learning rate for Adam.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--weight_decay\"", ",", "\n", "default", "=", "0.01", ",", "\n", "type", "=", "float", ",", "\n", "help", "=", "\"The weight decay rate for Adam.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--finetune_decay\"", ",", "\n", "action", "=", "'store_true'", ",", "\n", "help", "=", "\"Weight decay to the original weights.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--num_train_epochs\"", ",", "\n", "default", "=", "3.0", ",", "\n", "type", "=", "float", ",", "\n", "help", "=", "\"Total number of training epochs to perform.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--warmup_proportion\"", ",", "\n", "default", "=", "0.1", ",", "\n", "type", "=", "float", ",", "\n", "help", "=", "\"Proportion of training to perform linear learning rate warmup for. \"", "\n", "\"E.g., 0.1 = 10%% of training.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--hidden_dropout_prob\"", ",", "default", "=", "0.1", ",", "type", "=", "float", ",", "\n", "help", "=", "\"Dropout rate for hidden states.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--attention_probs_dropout_prob\"", ",", "default", "=", "0.1", ",", "type", "=", "float", ",", "\n", "help", "=", "\"Dropout rate for attention probabilities.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--no_cuda\"", ",", "\n", "action", "=", "'store_true'", ",", "\n", "help", "=", "\"Whether not to use CUDA when available\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--local_rank\"", ",", "\n", "type", "=", "int", ",", "\n", "default", "=", "-", "1", ",", "\n", "help", "=", "\"local_rank for distributed training on gpus\"", ")", "\n", "parser", ".", "add_argument", "(", "'--seed'", ",", "\n", "type", "=", "int", ",", "\n", "default", "=", "42", ",", "\n", "help", "=", "\"random seed for initialization\"", ")", "\n", "parser", ".", "add_argument", "(", "'--gradient_accumulation_steps'", ",", "\n", "type", "=", "int", ",", "\n", "default", "=", "1", ",", "\n", "help", "=", "\"Number of updates steps to accumulate before performing a backward/update pass.\"", ")", "\n", "parser", ".", "add_argument", "(", "'--fp16'", ",", "action", "=", "'store_true'", ",", "\n", "help", "=", "\"Whether to use 16-bit float precision instead of 32-bit\"", ")", "\n", "parser", ".", "add_argument", "(", "'--fp32_embedding'", ",", "action", "=", "'store_true'", ",", "\n", "help", "=", "\"Whether to use 32-bit float precision instead of 16-bit for embeddings\"", ")", "\n", "parser", ".", "add_argument", "(", "'--loss_scale'", ",", "type", "=", "float", ",", "default", "=", "0", ",", "\n", "help", "=", "\"Loss scaling to improve fp16 numeric stability. Only used when fp16 set to True.\\n\"", "\n", "\"0 (default value): dynamic loss scaling.\\n\"", "\n", "\"Positive power of 2: static loss scaling value.\\n\"", ")", "\n", "parser", ".", "add_argument", "(", "'--amp'", ",", "action", "=", "'store_true'", ",", "\n", "help", "=", "\"Whether to use amp for fp16\"", ")", "\n", "parser", ".", "add_argument", "(", "'--from_scratch'", ",", "action", "=", "'store_true'", ",", "\n", "help", "=", "\"Initialize parameters with random values (i.e., training from scratch).\"", ")", "\n", "parser", ".", "add_argument", "(", "'--new_segment_ids'", ",", "action", "=", "'store_true'", ",", "\n", "help", "=", "\"Use new segment ids for bi-uni-directional LM.\"", ")", "\n", "parser", ".", "add_argument", "(", "'--new_pos_ids'", ",", "action", "=", "'store_true'", ",", "\n", "help", "=", "\"Use new position ids for LMs.\"", ")", "\n", "parser", ".", "add_argument", "(", "'--tokenized_input'", ",", "action", "=", "'store_true'", ",", "\n", "help", "=", "\"Whether the input is tokenized.\"", ")", "\n", "parser", ".", "add_argument", "(", "'--max_len_a'", ",", "type", "=", "int", ",", "default", "=", "0", ",", "\n", "help", "=", "\"Truncate_config: maximum length of segment A.\"", ")", "\n", "parser", ".", "add_argument", "(", "'--max_len_b'", ",", "type", "=", "int", ",", "default", "=", "0", ",", "\n", "help", "=", "\"Truncate_config: maximum length of segment B.\"", ")", "\n", "parser", ".", "add_argument", "(", "'--trunc_seg'", ",", "default", "=", "''", ",", "\n", "help", "=", "\"Truncate_config: first truncate segment A/B (option: a, b).\"", ")", "\n", "parser", ".", "add_argument", "(", "'--always_truncate_tail'", ",", "action", "=", "'store_true'", ",", "\n", "help", "=", "\"Truncate_config: Whether we should always truncate tail.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--mask_prob\"", ",", "default", "=", "0.15", ",", "type", "=", "float", ",", "\n", "help", "=", "\"Number of prediction is sometimes less than max_pred when sequence is short.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--mask_prob_eos\"", ",", "default", "=", "0", ",", "type", "=", "float", ",", "\n", "help", "=", "\"Number of prediction is sometimes less than max_pred when sequence is short.\"", ")", "\n", "parser", ".", "add_argument", "(", "'--max_pred'", ",", "type", "=", "int", ",", "default", "=", "20", ",", "\n", "help", "=", "\"Max tokens of prediction.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--num_workers\"", ",", "default", "=", "0", ",", "type", "=", "int", ",", "\n", "help", "=", "\"Number of workers for the data loader.\"", ")", "\n", "\n", "parser", ".", "add_argument", "(", "'--mask_source_words'", ",", "action", "=", "'store_true'", ",", "\n", "help", "=", "\"Whether to mask source words for training\"", ")", "\n", "parser", ".", "add_argument", "(", "'--skipgram_prb'", ",", "type", "=", "float", ",", "default", "=", "0.0", ",", "\n", "help", "=", "'prob of ngram mask'", ")", "\n", "parser", ".", "add_argument", "(", "'--skipgram_size'", ",", "type", "=", "int", ",", "default", "=", "1", ",", "\n", "help", "=", "'the max size of ngram mask'", ")", "\n", "parser", ".", "add_argument", "(", "'--mask_whole_word'", ",", "action", "=", "'store_true'", ",", "\n", "help", "=", "\"Whether masking a whole word.\"", ")", "\n", "parser", ".", "add_argument", "(", "'--do_l2r_training'", ",", "action", "=", "'store_true'", ",", "\n", "help", "=", "\"Whether to do left to right training\"", ")", "\n", "parser", ".", "add_argument", "(", "'--has_sentence_oracle'", ",", "action", "=", "'store_true'", ",", "\n", "help", "=", "\"Whether to have sentence level oracle for training. \"", "\n", "\"Only useful for summary generation\"", ")", "\n", "parser", ".", "add_argument", "(", "'--max_position_embeddings'", ",", "type", "=", "int", ",", "default", "=", "None", ",", "\n", "help", "=", "\"max position embeddings\"", ")", "\n", "parser", ".", "add_argument", "(", "'--relax_projection'", ",", "action", "=", "'store_true'", ",", "\n", "help", "=", "\"Use different projection layers for tasks.\"", ")", "\n", "parser", ".", "add_argument", "(", "'--ffn_type'", ",", "default", "=", "0", ",", "type", "=", "int", ",", "\n", "help", "=", "\"0: default mlp; 1: W((Wx+b) elem_prod x);\"", ")", "\n", "parser", ".", "add_argument", "(", "'--num_qkv'", ",", "default", "=", "0", ",", "type", "=", "int", ",", "\n", "help", "=", "\"Number of different <Q,K,V>.\"", ")", "\n", "parser", ".", "add_argument", "(", "'--seg_emb'", ",", "action", "=", "'store_true'", ",", "\n", "help", "=", "\"Using segment embedding for self-attention.\"", ")", "\n", "parser", ".", "add_argument", "(", "'--s2s_special_token'", ",", "action", "=", "'store_true'", ",", "\n", "help", "=", "\"New special tokens ([S2S_SEP]/[S2S_CLS]) of S2S.\"", ")", "\n", "parser", ".", "add_argument", "(", "'--s2s_add_segment'", ",", "action", "=", "'store_true'", ",", "\n", "help", "=", "\"Additional segmental for the encoder of S2S.\"", ")", "\n", "parser", ".", "add_argument", "(", "'--s2s_share_segment'", ",", "action", "=", "'store_true'", ",", "\n", "help", "=", "\"Sharing segment embeddings for the encoder of S2S (used with --s2s_add_segment).\"", ")", "\n", "parser", ".", "add_argument", "(", "'--pos_shift'", ",", "action", "=", "'store_true'", ",", "\n", "help", "=", "\"Using position shift for fine-tuning.\"", ")", "\n", "parser", ".", "add_argument", "(", "'--eval_data_type'", ",", "type", "=", "str", ",", "default", "=", "'merge'", ",", "\n", "help", "=", "\"bi, uni, l2r, merge\"", ")", "\n", "\n", "args", "=", "parser", ".", "parse_args", "(", ")", "\n", "\n", "# assert Path(args.model_recover_path).exists(", "\n", "# ), \"--model_recover_path doesn't exist\"", "\n", "\n", "args", ".", "output_dir", "=", "args", ".", "output_dir", ".", "replace", "(", "\n", "'[PT_OUTPUT_DIR]'", ",", "os", ".", "getenv", "(", "'PT_OUTPUT_DIR'", ",", "''", ")", ")", "\n", "args", ".", "log_dir", "=", "args", ".", "log_dir", ".", "replace", "(", "\n", "'[PT_OUTPUT_DIR]'", ",", "os", ".", "getenv", "(", "'PT_OUTPUT_DIR'", ",", "''", ")", ")", "\n", "\n", "os", ".", "makedirs", "(", "args", ".", "output_dir", ",", "exist_ok", "=", "True", ")", "\n", "os", ".", "makedirs", "(", "args", ".", "log_dir", ",", "exist_ok", "=", "True", ")", "\n", "json", ".", "dump", "(", "args", ".", "__dict__", ",", "open", "(", "os", ".", "path", ".", "join", "(", "\n", "args", ".", "output_dir", ",", "'opt.json'", ")", ",", "'w'", ")", ",", "sort_keys", "=", "True", ",", "indent", "=", "2", ")", "\n", "\n", "if", "args", ".", "local_rank", "==", "-", "1", "or", "args", ".", "no_cuda", ":", "\n", "        ", "device", "=", "torch", ".", "device", "(", "\n", "\"cuda\"", "if", "torch", ".", "cuda", ".", "is_available", "(", ")", "and", "not", "args", ".", "no_cuda", "else", "\"cpu\"", ")", "\n", "n_gpu", "=", "torch", ".", "cuda", ".", "device_count", "(", ")", "\n", "", "else", ":", "\n", "        ", "torch", ".", "cuda", ".", "set_device", "(", "args", ".", "local_rank", ")", "\n", "device", "=", "torch", ".", "device", "(", "\"cuda\"", ",", "args", ".", "local_rank", ")", "\n", "n_gpu", "=", "1", "\n", "# Initializes the distributed backend which will take care of sychronizing nodes/GPUs", "\n", "dist", ".", "init_process_group", "(", "backend", "=", "'nccl'", ")", "\n", "", "logger", ".", "info", "(", "\"device: {} n_gpu: {}, distributed training: {}, 16-bits training: {}\"", ".", "format", "(", "\n", "device", ",", "n_gpu", ",", "bool", "(", "args", ".", "local_rank", "!=", "-", "1", ")", ",", "args", ".", "fp16", ")", ")", "\n", "\n", "if", "args", ".", "gradient_accumulation_steps", "<", "1", ":", "\n", "        ", "raise", "ValueError", "(", "\"Invalid gradient_accumulation_steps parameter: {}, should be >= 1\"", ".", "format", "(", "\n", "args", ".", "gradient_accumulation_steps", ")", ")", "\n", "\n", "", "args", ".", "train_batch_size", "=", "int", "(", "\n", "args", ".", "train_batch_size", "/", "args", ".", "gradient_accumulation_steps", ")", "\n", "\n", "random", ".", "seed", "(", "args", ".", "seed", ")", "\n", "np", ".", "random", ".", "seed", "(", "args", ".", "seed", ")", "\n", "torch", ".", "manual_seed", "(", "args", ".", "seed", ")", "\n", "if", "n_gpu", ">", "0", ":", "\n", "        ", "torch", ".", "cuda", ".", "manual_seed_all", "(", "args", ".", "seed", ")", "\n", "\n", "", "if", "not", "args", ".", "do_train", "and", "not", "args", ".", "do_eval", ":", "\n", "        ", "raise", "ValueError", "(", "\n", "\"At least one of `do_train` or `do_eval` must be True.\"", ")", "\n", "\n", "", "if", "args", ".", "local_rank", "not", "in", "(", "-", "1", ",", "0", ")", ":", "\n", "# Make sure only the first process in distributed training will download model & vocab", "\n", "        ", "dist", ".", "barrier", "(", ")", "\n", "", "tokenizer", "=", "BertTokenizer", ".", "from_pretrained", "(", "\n", "args", ".", "bert_model", ",", "do_lower_case", "=", "args", ".", "do_lower_case", ")", "\n", "if", "args", ".", "max_position_embeddings", ":", "\n", "        ", "tokenizer", ".", "max_len", "=", "args", ".", "max_position_embeddings", "\n", "", "data_tokenizer", "=", "WhitespaceTokenizer", "(", ")", "if", "args", ".", "tokenized_input", "else", "tokenizer", "\n", "if", "args", ".", "local_rank", "==", "0", ":", "\n", "        ", "dist", ".", "barrier", "(", ")", "\n", "\n", "", "if", "args", ".", "do_eval", ":", "\n", "        ", "print", "(", "\"Loading Train Dataset\"", ",", "args", ".", "data_dir", ")", "\n", "# bi_uni_pipeline = [seq2seq_loader.Preprocess4Seq2seq(args.max_pred, args.mask_prob, list(tokenizer.vocab.keys(", "\n", "# )), tokenizer.convert_tokens_to_ids, args.max_seq_length, new_segment_ids=args.new_segment_ids, truncate_config={'max_len_a': args.max_len_a, 'max_len_b': args.max_len_b, 'trunc_seg': args.trunc_seg, 'always_truncate_tail': args.always_truncate_tail}, mask_source_words=args.mask_source_words, skipgram_prb=args.skipgram_prb, skipgram_size=args.skipgram_size, mask_whole_word=args.mask_whole_word, mode=\"s2s\", has_oracle=args.has_sentence_oracle, num_qkv=args.num_qkv, s2s_special_token=args.s2s_special_token, s2s_add_segment=args.s2s_add_segment, s2s_share_segment=args.s2s_share_segment, pos_shift=args.pos_shift)]", "\n", "s2s_pipeline", "=", "seq2seq_loader", ".", "Preprocess4Seq2seq", "(", "args", ".", "max_pred", ",", "args", ".", "mask_prob", ",", "list", "(", "tokenizer", ".", "vocab", ".", "keys", "(", "\n", ")", ")", ",", "tokenizer", ".", "convert_tokens_to_ids", ",", "args", ".", "max_seq_length", ",", "new_segment_ids", "=", "args", ".", "new_segment_ids", ",", "\n", "truncate_config", "=", "{", "'max_len_a'", ":", "args", ".", "max_len_a", ",", "'max_len_b'", ":", "args", ".", "max_len_b", ",", "'trunc_seg'", ":", "args", ".", "trunc_seg", ",", "'always_truncate_tail'", ":", "args", ".", "always_truncate_tail", "}", ",", "\n", "mask_source_words", "=", "args", ".", "mask_source_words", ",", "skipgram_prb", "=", "args", ".", "skipgram_prb", ",", "skipgram_size", "=", "args", ".", "skipgram_size", ",", "mask_whole_word", "=", "args", ".", "mask_whole_word", ",", "\n", "mode", "=", "'s2s'", ",", "has_oracle", "=", "args", ".", "has_sentence_oracle", ",", "num_qkv", "=", "args", ".", "num_qkv", ",", "s2s_special_token", "=", "args", ".", "s2s_special_token", ",", "s2s_add_segment", "=", "args", ".", "s2s_add_segment", ",", "\n", "s2s_share_segment", "=", "args", ".", "s2s_share_segment", ",", "pos_shift", "=", "args", ".", "pos_shift", ")", "\n", "\n", "l2r_pipeline", "=", "unidir_loader", ".", "Preprocess4UniDirection", "(", "args", ".", "max_pred", ",", "args", ".", "mask_prob", ",", "list", "(", "tokenizer", ".", "vocab", ".", "keys", "(", "\n", ")", ")", ",", "tokenizer", ".", "convert_tokens_to_ids", ",", "args", ".", "max_seq_length", ",", "new_segment_ids", "=", "args", ".", "new_segment_ids", ",", "\n", "truncate_config", "=", "{", "'max_len_a'", ":", "args", ".", "max_len_a", ",", "'max_len_b'", ":", "args", ".", "max_len_b", ",", "'trunc_seg'", ":", "args", ".", "trunc_seg", ",", "'always_truncate_tail'", ":", "args", ".", "always_truncate_tail", "}", ",", "\n", "mask_source_words", "=", "args", ".", "mask_source_words", ",", "skipgram_prb", "=", "args", ".", "skipgram_prb", ",", "skipgram_size", "=", "args", ".", "skipgram_size", ",", "mask_whole_word", "=", "args", ".", "mask_whole_word", ",", "\n", "mode", "=", "'l2r'", ",", "has_oracle", "=", "args", ".", "has_sentence_oracle", ",", "num_qkv", "=", "args", ".", "num_qkv", ",", "s2s_special_token", "=", "args", ".", "s2s_special_token", ",", "s2s_add_segment", "=", "args", ".", "s2s_add_segment", ",", "\n", "s2s_share_segment", "=", "args", ".", "s2s_share_segment", ",", "pos_shift", "=", "args", ".", "pos_shift", ")", "\n", "\n", "r2l_pipeline", "=", "unidir_loader", ".", "Preprocess4UniDirection", "(", "args", ".", "max_pred", ",", "args", ".", "mask_prob", ",", "list", "(", "tokenizer", ".", "vocab", ".", "keys", "(", "\n", ")", ")", ",", "tokenizer", ".", "convert_tokens_to_ids", ",", "args", ".", "max_seq_length", ",", "new_segment_ids", "=", "args", ".", "new_segment_ids", ",", "\n", "truncate_config", "=", "{", "'max_len_a'", ":", "args", ".", "max_len_a", ",", "'max_len_b'", ":", "args", ".", "max_len_b", ",", "'trunc_seg'", ":", "args", ".", "trunc_seg", ",", "'always_truncate_tail'", ":", "args", ".", "always_truncate_tail", "}", ",", "\n", "mask_source_words", "=", "args", ".", "mask_source_words", ",", "skipgram_prb", "=", "args", ".", "skipgram_prb", ",", "skipgram_size", "=", "args", ".", "skipgram_size", ",", "mask_whole_word", "=", "args", ".", "mask_whole_word", ",", "\n", "mode", "=", "'r2l'", ",", "has_oracle", "=", "args", ".", "has_sentence_oracle", ",", "num_qkv", "=", "args", ".", "num_qkv", ",", "s2s_special_token", "=", "args", ".", "s2s_special_token", ",", "s2s_add_segment", "=", "args", ".", "s2s_add_segment", ",", "\n", "s2s_share_segment", "=", "args", ".", "s2s_share_segment", ",", "pos_shift", "=", "args", ".", "pos_shift", ")", "\n", "\n", "bi_pipeline", "=", "bidir_loader", ".", "Preprocess4BiDirection", "(", "args", ".", "max_pred", ",", "args", ".", "mask_prob", ",", "list", "(", "tokenizer", ".", "vocab", ".", "keys", "(", "\n", ")", ")", ",", "tokenizer", ".", "convert_tokens_to_ids", ",", "args", ".", "max_seq_length", ",", "new_segment_ids", "=", "args", ".", "new_segment_ids", ",", "\n", "truncate_config", "=", "{", "'max_len_a'", ":", "args", ".", "max_len_a", ",", "'max_len_b'", ":", "args", ".", "max_len_b", ",", "'trunc_seg'", ":", "args", ".", "trunc_seg", ",", "'always_truncate_tail'", ":", "args", ".", "always_truncate_tail", "}", ",", "\n", "mask_source_words", "=", "args", ".", "mask_source_words", ",", "skipgram_prb", "=", "args", ".", "skipgram_prb", ",", "skipgram_size", "=", "args", ".", "skipgram_size", ",", "mask_whole_word", "=", "args", ".", "mask_whole_word", ",", "\n", "mode", "=", "''", ",", "has_oracle", "=", "args", ".", "has_sentence_oracle", ",", "num_qkv", "=", "args", ".", "num_qkv", ",", "s2s_special_token", "=", "args", ".", "s2s_special_token", ",", "s2s_add_segment", "=", "args", ".", "s2s_add_segment", ",", "\n", "s2s_share_segment", "=", "args", ".", "s2s_share_segment", ",", "pos_shift", "=", "args", ".", "pos_shift", ")", "\n", "\n", "\n", "file_oracle", "=", "None", "\n", "if", "args", ".", "has_sentence_oracle", ":", "\n", "            ", "file_oracle", "=", "os", ".", "path", ".", "join", "(", "args", ".", "data_dir", ",", "'train.oracle'", ")", "\n", "\n", "", "bi_src", "=", "os", ".", "path", ".", "join", "(", "args", ".", "data_dir", ",", "'bi.src'", ")", "\n", "bi_tgt", "=", "os", ".", "path", ".", "join", "(", "args", ".", "data_dir", ",", "'bi.tgt'", ")", "\n", "bi_nsp", "=", "os", ".", "path", ".", "join", "(", "args", ".", "data_dir", ",", "'bi.nsp'", ")", "\n", "uni_tgt", "=", "os", ".", "path", ".", "join", "(", "args", ".", "data_dir", ",", "'uni.tgt'", ")", "\n", "s2s_src", "=", "os", ".", "path", ".", "join", "(", "args", ".", "data_dir", ",", "'s2s.src'", ")", "\n", "s2s_tgt", "=", "os", ".", "path", ".", "join", "(", "args", ".", "data_dir", ",", "'s2s.tgt'", ")", "\n", "\n", "\n", "if", "args", ".", "eval_data_type", "==", "'s2s'", ":", "\n", "            ", "train_dataset", "=", "seq2seq_loader", ".", "Seq2SeqDataset", "(", "\n", "s2s_src", ",", "s2s_tgt", ",", "\n", "args", ".", "train_batch_size", ",", "data_tokenizer", ",", "args", ".", "max_seq_length", ",", "\n", "bi_uni_pipeline", "=", "[", "s2s_pipeline", "]", ")", "\n", "\n", "", "if", "args", ".", "eval_data_type", "==", "'bi'", ":", "\n", "            ", "train_dataset", "=", "bidir_loader", ".", "BiDirDataset", "(", "\n", "bi_src", ",", "bi_tgt", ",", "\n", "args", ".", "train_batch_size", ",", "data_tokenizer", ",", "args", ".", "max_seq_length", ",", "file_nsp", "=", "bi_nsp", ",", "\n", "bi_pipeline", "=", "[", "bi_pipeline", "]", ")", "\n", "\n", "", "if", "args", ".", "eval_data_type", "==", "'uni'", ":", "\n", "            ", "train_dataset", "=", "unidir_loader", ".", "UniDirDataset", "(", "\n", "uni_tgt", ",", "\n", "args", ".", "train_batch_size", ",", "data_tokenizer", ",", "args", ".", "max_seq_length", ",", "\n", "uni_pipeline", "=", "[", "l2r_pipeline", "]", ")", "\n", "\n", "", "if", "args", ".", "eval_data_type", "==", "'merge'", ":", "\n", "            ", "train_dataset", "=", "bi_uni_loader", ".", "Bi_Uni_Dataset", "(", "\n", "bi_src", ",", "bi_tgt", ",", "uni_tgt", ",", "s2s_src", ",", "s2s_tgt", ",", "\n", "args", ".", "train_batch_size", ",", "data_tokenizer", ",", "args", ".", "max_seq_length", ",", "file_nsp", "=", "bi_nsp", ",", "\n", "bi_uni_pipeline", "=", "[", "bi_pipeline", ",", "l2r_pipeline", ",", "s2s_pipeline", "]", ")", "\n", "\n", "", "if", "args", ".", "local_rank", "==", "-", "1", ":", "\n", "            ", "train_sampler", "=", "RandomSampler", "(", "train_dataset", ",", "replacement", "=", "False", ")", "\n", "_batch_size", "=", "args", ".", "train_batch_size", "\n", "", "else", ":", "\n", "            ", "train_sampler", "=", "DistributedSampler", "(", "train_dataset", ")", "\n", "_batch_size", "=", "args", ".", "train_batch_size", "//", "dist", ".", "get_world_size", "(", ")", "\n", "", "train_dataloader", "=", "torch", ".", "utils", ".", "data", ".", "DataLoader", "(", "train_dataset", ",", "batch_size", "=", "_batch_size", ",", "sampler", "=", "train_sampler", ",", "\n", "num_workers", "=", "args", ".", "num_workers", ",", "collate_fn", "=", "seq2seq_loader", ".", "batch_list_to_batch_tensors", ",", "pin_memory", "=", "False", ")", "\n", "\n", "# note: args.train_batch_size has been changed to (/= args.gradient_accumulation_steps)", "\n", "# t_total = int(math.ceil(len(train_dataset.ex_list) / args.train_batch_size)", "\n", "", "t_total", "=", "int", "(", "len", "(", "train_dataloader", ")", "*", "args", ".", "num_train_epochs", "/", "\n", "args", ".", "gradient_accumulation_steps", ")", "\n", "\n", "# amp_handle = None", "\n", "# if args.fp16 and args.amp:", "\n", "#     from apex import amp", "\n", "#     amp_handle = amp.init(enable_caching=True)", "\n", "#     logger.info(\"enable fp16 with amp\")", "\n", "\n", "# Prepare model", "\n", "# recover_step = _get_max_epoch_model(args.output_dir)", "\n", "cls_num_labels", "=", "2", "\n", "type_vocab_size", "=", "6", "+", "(", "1", "if", "args", ".", "s2s_add_segment", "else", "0", ")", "if", "args", ".", "new_segment_ids", "else", "2", "\n", "num_sentlvl_labels", "=", "2", "if", "args", ".", "has_sentence_oracle", "else", "0", "\n", "relax_projection", "=", "4", "if", "args", ".", "relax_projection", "else", "0", "\n", "# if args.local_rank not in (-1, 0):", "\n", "#     # Make sure only the first process in distributed training will download model & vocab", "\n", "#     dist.barrier()", "\n", "# if (recover_step is None) and (args.model_recover_path is None):", "\n", "#     # if _state_dict == {}, the parameters are randomly initialized", "\n", "#     # if _state_dict == None, the parameters are initialized with bert-init", "\n", "#     _state_dict = {} if args.from_scratch else None", "\n", "#     model = BertForPreTrainingLossMask.from_pretrained(", "\n", "#         args.bert_model, state_dict=_state_dict, num_labels=cls_num_labels, num_rel=0, type_vocab_size=type_vocab_size, config_path=args.config_path, task_idx=3, num_sentlvl_labels=num_sentlvl_labels, max_position_embeddings=args.max_position_embeddings, label_smoothing=args.label_smoothing, fp32_embedding=args.fp32_embedding, relax_projection=relax_projection, new_pos_ids=args.new_pos_ids, ffn_type=args.ffn_type, hidden_dropout_prob=args.hidden_dropout_prob, attention_probs_dropout_prob=args.attention_probs_dropout_prob, num_qkv=args.num_qkv, seg_emb=args.seg_emb)", "\n", "#     global_step = 0", "\n", "# else:", "\n", "#     if recover_step:", "\n", "#         logger.info(\"***** Recover model: %d *****\", recover_step)", "\n", "#         model_recover = torch.load(os.path.join(", "\n", "#             args.output_dir, \"model.{0}.bin\".format(recover_step)), map_location='cpu')", "\n", "#         # recover_step == number of epochs", "\n", "#         global_step = math.floor(", "\n", "#             recover_step * t_total / args.num_train_epochs)", "\n", "#     elif args.model_recover_path:", "\n", "\n", "for", "model_recover_path", "in", "glob", ".", "glob", "(", "os", ".", "path", ".", "join", "(", "args", ".", "model_recover_path", ",", "\"model.*.bin\"", ")", ")", ":", "\n", "        ", "output_filename", "=", "model_recover_path", "+", "'.eval_'", "+", "args", ".", "eval_data_type", "\n", "if", "os", ".", "path", ".", "exists", "(", "output_filename", ")", ":", "\n", "            ", "continue", "\n", "", "logger", ".", "info", "(", "\"***** Recover model: %s *****\"", ",", "model_recover_path", ")", "\n", "model_recover", "=", "torch", ".", "load", "(", "model_recover_path", ",", "map_location", "=", "'cpu'", ")", "\n", "model", "=", "BertForPreTrainingLossMask", ".", "from_pretrained", "(", "\n", "args", ".", "bert_model", ",", "state_dict", "=", "model_recover", ",", "num_labels", "=", "cls_num_labels", ",", "num_rel", "=", "0", ",", "type_vocab_size", "=", "type_vocab_size", ",", "config_path", "=", "args", ".", "config_path", ",", "task_idx", "=", "3", ",", "num_sentlvl_labels", "=", "num_sentlvl_labels", ",", "max_position_embeddings", "=", "args", ".", "max_position_embeddings", ",", "label_smoothing", "=", "args", ".", "label_smoothing", ",", "fp32_embedding", "=", "args", ".", "fp32_embedding", ",", "relax_projection", "=", "relax_projection", ",", "new_pos_ids", "=", "args", ".", "new_pos_ids", ",", "ffn_type", "=", "args", ".", "ffn_type", ",", "hidden_dropout_prob", "=", "args", ".", "hidden_dropout_prob", ",", "attention_probs_dropout_prob", "=", "args", ".", "attention_probs_dropout_prob", ",", "num_qkv", "=", "args", ".", "num_qkv", ",", "seg_emb", "=", "args", ".", "seg_emb", ")", "\n", "\n", "if", "args", ".", "local_rank", "==", "0", ":", "\n", "            ", "dist", ".", "barrier", "(", ")", "\n", "\n", "", "if", "args", ".", "fp16", ":", "\n", "# model.half()", "\n", "            ", "if", "args", ".", "fp32_embedding", ":", "\n", "                ", "model", ".", "bert", ".", "embeddings", ".", "word_embeddings", ".", "float", "(", ")", "\n", "model", ".", "bert", ".", "embeddings", ".", "position_embeddings", ".", "float", "(", ")", "\n", "model", ".", "bert", ".", "embeddings", ".", "token_type_embeddings", ".", "float", "(", ")", "\n", "", "", "model", ".", "to", "(", "device", ")", "\n", "logger", ".", "info", "(", "\"***** CUDA.empty_cache() *****\"", ")", "\n", "torch", ".", "cuda", ".", "empty_cache", "(", ")", "\n", "\n", "eval_loss", "=", "0.0", "\n", "eval_loss_2", "=", "0.0", "\n", "nb_eval_steps", "=", "0", "\n", "if", "args", ".", "do_eval", ":", "\n", "            ", "model", ".", "eval", "(", ")", "\n", "iter_bar", "=", "tqdm", "(", "train_dataloader", ",", "desc", "=", "'Iter (loss=X.XXX)'", ",", "\n", "disable", "=", "args", ".", "local_rank", "not", "in", "(", "-", "1", ",", "0", ")", ")", "\n", "for", "step", ",", "batch", "in", "enumerate", "(", "iter_bar", ")", ":", "\n", "                ", "batch", "=", "[", "\n", "t", ".", "to", "(", "device", ")", "if", "t", "is", "not", "None", "else", "None", "for", "t", "in", "batch", "]", "\n", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "                    ", "if", "args", ".", "has_sentence_oracle", ":", "\n", "                        ", "input_ids", ",", "segment_ids", ",", "input_mask", ",", "mask_qkv", ",", "lm_label_ids", ",", "masked_pos", ",", "masked_weights", ",", "is_next", ",", "task_idx", ",", "oracle_pos", ",", "oracle_weights", ",", "oracle_labels", "=", "batch", "\n", "", "else", ":", "\n", "                        ", "input_ids", ",", "segment_ids", ",", "input_mask", ",", "mask_qkv", ",", "lm_label_ids", ",", "masked_pos", ",", "masked_weights", ",", "is_next", ",", "task_idx", "=", "batch", "\n", "oracle_pos", ",", "oracle_weights", ",", "oracle_labels", "=", "None", ",", "None", ",", "None", "\n", "\n", "", "loss_tuple", "=", "model", "(", "input_ids", ",", "segment_ids", ",", "input_mask", ",", "lm_label_ids", ",", "is_next", ",", "masked_pos", "=", "masked_pos", ",", "masked_weights", "=", "masked_weights", ",", "task_idx", "=", "task_idx", ",", "masked_pos_2", "=", "oracle_pos", ",", "masked_weights_2", "=", "oracle_weights", ",", "\n", "masked_labels_2", "=", "oracle_labels", ",", "mask_qkv", "=", "mask_qkv", ")", "\n", "masked_lm_loss", ",", "next_sentence_loss", "=", "loss_tuple", "\n", "loss", "=", "masked_lm_loss", "+", "next_sentence_loss", "\n", "eval_loss", "+=", "masked_lm_loss", ".", "mean", "(", ")", ".", "item", "(", ")", "\n", "eval_loss_2", "+=", "next_sentence_loss", ".", "mean", "(", ")", ".", "item", "(", ")", "\n", "", "nb_eval_steps", "+=", "1", "\n", "\n", "# logging for each step (i.e., before normalization by args.gradient_accumulation_steps)", "\n", "iter_bar", ".", "set_description", "(", "'Iter (loss=%5.3f)'", "%", "loss", ".", "item", "(", ")", ")", "\n", "\n", "# ensure that accumlated gradients are normalized", "\n", "if", "args", ".", "gradient_accumulation_steps", ">", "1", ":", "\n", "                    ", "loss", "=", "loss", "/", "args", ".", "gradient_accumulation_steps", "\n", "# global_step += 1", "\n", "\n", "", "", "eval_loss", "=", "eval_loss", "/", "nb_eval_steps", "\n", "eval_loss_2", "=", "eval_loss_2", "/", "nb_eval_steps", "\n", "print", "(", "eval_loss", ",", "eval_loss_2", ")", "\n", "with", "open", "(", "output_filename", ",", "'w'", ")", "as", "f", ":", "\n", "                ", "f", ".", "write", "(", "str", "(", "eval_loss", ")", "+", "'\\t'", "+", "str", "(", "eval_loss_2", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.biunilm.run_seq2seq_merge._get_max_epoch_model": [[44, 55], ["glob.glob", "glob.glob", "os.path.join", "os.path.join", "set", "set", "max", "int", "int", "pathlib.Path().stem.split", "pathlib.Path().stem.split", "pathlib.Path", "pathlib.Path"], "function", ["home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.preprocess.process_daren.split", "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.preprocess.process_daren.split"], ["def", "_get_max_epoch_model", "(", "output_dir", ")", ":", "\n", "    ", "fn_model_list", "=", "glob", ".", "glob", "(", "os", ".", "path", ".", "join", "(", "output_dir", ",", "\"model.*.bin\"", ")", ")", "\n", "fn_optim_list", "=", "glob", ".", "glob", "(", "os", ".", "path", ".", "join", "(", "output_dir", ",", "\"optim.*.bin\"", ")", ")", "\n", "if", "(", "not", "fn_model_list", ")", "or", "(", "not", "fn_optim_list", ")", ":", "\n", "        ", "return", "None", "\n", "", "both_set", "=", "set", "(", "[", "int", "(", "Path", "(", "fn", ")", ".", "stem", ".", "split", "(", "'.'", ")", "[", "-", "1", "]", ")", "for", "fn", "in", "fn_model_list", "]", "\n", ")", "&", "set", "(", "[", "int", "(", "Path", "(", "fn", ")", ".", "stem", ".", "split", "(", "'.'", ")", "[", "-", "1", "]", ")", "for", "fn", "in", "fn_optim_list", "]", ")", "\n", "if", "both_set", ":", "\n", "        ", "return", "max", "(", "both_set", ")", "\n", "", "else", ":", "\n", "        ", "return", "None", "\n", "\n"]], "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.biunilm.run_seq2seq_merge.main": [[57, 539], ["argparse.ArgumentParser", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.parse_args", "parser.parse_args.output_dir.replace", "parser.parse_args.log_dir.replace", "os.makedirs", "os.makedirs", "json.dump", "logger.info", "int", "random.seed", "numpy.random.seed", "torch.manual_seed", "torch.manual_seed", "pytorch_pretrained_bert.tokenization.BertTokenizer.from_pretrained", "int", "run_seq2seq_merge._get_max_epoch_model", "nn.data_parallel.DataParallelImbalance.to", "list", "logger.info", "torch.cuda.empty_cache", "torch.cuda.empty_cache", "os.getenv", "os.getenv", "open", "torch.device", "torch.device", "torch.cuda.device_count", "torch.cuda.device_count", "torch.cuda.set_device", "torch.cuda.set_device", "torch.device", "torch.device", "torch.init_process_group", "ValueError", "torch.cuda.manual_seed_all", "torch.cuda.manual_seed_all", "ValueError", "torch.barrier", "pytorch_pretrained_bert.tokenization.WhitespaceTokenizer", "torch.barrier", "print", "biunilm.Preprocess4Seq2seq", "biunilm.Preprocess4UniDirection", "biunilm.Preprocess4UniDirection", "biunilm.Preprocess4BiDirection", "os.path.join", "os.path.join", "os.path.join", "os.path.join", "os.path.join", "os.path.join", "biunilm.Bi_Uni_Dataset", "torch.utils.data.DataLoader", "torch.utils.data.DataLoader", "torch.barrier", "pytorch_pretrained_bert.modeling.BertForPreTrainingLossMask.from_pretrained", "pytorch_pretrained_bert.modeling.BertForPreTrainingLossMask.from_pretrained", "torch.barrier", "nn.data_parallel.DataParallelImbalance.named_parameters", "FusedAdam", "apex.amp.initialize", "logger.info", "pytorch_pretrained_bert.optimization.BertAdam", "DDP", "logger.info", "torch.load", "torch.load", "hasattr", "pytorch_pretrained_bert.optimization.BertAdam.load_state_dict", "logger.info", "logger.info", "logger.info", "nn.data_parallel.DataParallelImbalance.train", "tqdm.trange", "os.path.join", "bool", "list", "list", "list", "list", "os.path.join", "torch.utils.data.RandomSampler", "torch.utils.data.distributed.DistributedSampler", "logger.info", "torch.load", "torch.load", "math.floor", "nn.data_parallel.DataParallelImbalance.bert.embeddings.word_embeddings.float", "nn.data_parallel.DataParallelImbalance.bert.embeddings.position_embeddings.float", "nn.data_parallel.DataParallelImbalance.bert.embeddings.token_type_embeddings.float", "nn.data_parallel.DataParallelImbalance", "os.path.join", "optim_recover.state_dict.state_dict", "logger.info", "tqdm.tqdm", "enumerate", "BertTokenizer.from_pretrained.vocab.keys", "BertTokenizer.from_pretrained.vocab.keys", "BertTokenizer.from_pretrained.vocab.keys", "BertTokenizer.from_pretrained.vocab.keys", "torch.get_world_size", "len", "os.path.join", "logger.info", "torch.load", "torch.load", "ImportError", "ImportError", "int", "torch.utils.data.distributed.DistributedSampler.set_epoch", "nn.data_parallel.DataParallelImbalance.", "tqdm.tqdm.set_description", "logger.info", "os.path.join", "torch.save", "torch.save", "os.path.join", "torch.save", "torch.save", "torch.cuda.is_available", "torch.cuda.is_available", "any", "masked_lm_loss.mean.mean", "next_sentence_loss.mean.mean", "loss.backward", "pytorch_pretrained_bert.optimization.BertAdam.step", "pytorch_pretrained_bert.optimization.BertAdam.zero_grad", "torch.distributed.get_rank", "torch.distributed.get_rank", "hasattr", "model_to_save.state_dict", "pytorch_pretrained_bert.optimization.BertAdam.state_dict", "any", "t.to", "loss.item", "apex.amp.scale_loss", "scaled_loss.backward", "pytorch_pretrained_bert.optimization.warmup_linear"], "function", ["home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.pytorch_pretrained_bert.tokenization.BertTokenizer.from_pretrained", "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.unilm.run_seq2seq._get_max_epoch_model", "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.pytorch_pretrained_bert.tokenization.BertTokenizer.from_pretrained", "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.pytorch_pretrained_bert.tokenization.BertTokenizer.from_pretrained", "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.pytorch_pretrained_bert.optimization_fp16.FP16_Optimizer_State.load_state_dict", "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.pytorch_pretrained_bert.optimization_fp16.FP16_Optimizer_State.state_dict", "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.pytorch_pretrained_bert.optimization.BertAdamFineTune.step", "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.pytorch_pretrained_bert.optimization_fp16.FP16_Optimizer_State.state_dict", "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.pytorch_pretrained_bert.optimization_fp16.FP16_Optimizer_State.state_dict", "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.pytorch_pretrained_bert.optimization.warmup_linear"], ["", "", "def", "main", "(", ")", ":", "\n", "    ", "parser", "=", "argparse", ".", "ArgumentParser", "(", ")", "\n", "\n", "# Required parameters", "\n", "parser", ".", "add_argument", "(", "\"--data_dir\"", ",", "\n", "default", "=", "None", ",", "\n", "type", "=", "str", ",", "\n", "required", "=", "True", ",", "\n", "help", "=", "\"The input data dir. Should contain the .tsv files (or other data files) for the task.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--src_file\"", ",", "default", "=", "None", ",", "type", "=", "str", ",", "\n", "help", "=", "\"The input data file name.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--tgt_file\"", ",", "default", "=", "None", ",", "type", "=", "str", ",", "\n", "help", "=", "\"The output data file name.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--bert_model\"", ",", "default", "=", "None", ",", "type", "=", "str", ",", "required", "=", "True", ",", "\n", "help", "=", "\"Bert pre-trained model selected in the list: bert-base-uncased, \"", "\n", "\"bert-large-uncased, bert-base-cased, bert-base-multilingual, bert-base-chinese.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--config_path\"", ",", "default", "=", "None", ",", "type", "=", "str", ",", "\n", "help", "=", "\"Bert config file path.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--output_dir\"", ",", "\n", "default", "=", "None", ",", "\n", "type", "=", "str", ",", "\n", "required", "=", "True", ",", "\n", "help", "=", "\"The output directory where the model predictions and checkpoints will be written.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--log_dir\"", ",", "\n", "default", "=", "''", ",", "\n", "type", "=", "str", ",", "\n", "required", "=", "True", ",", "\n", "help", "=", "\"The output directory where the log will be written.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--model_recover_path\"", ",", "\n", "default", "=", "None", ",", "\n", "type", "=", "str", ",", "\n", "help", "=", "\"The file of fine-tuned pretraining model.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--optim_recover_path\"", ",", "\n", "default", "=", "None", ",", "\n", "type", "=", "str", ",", "\n", "help", "=", "\"The file of pretraining optimizer.\"", ")", "\n", "\n", "# Other parameters", "\n", "parser", ".", "add_argument", "(", "\"--max_seq_length\"", ",", "\n", "default", "=", "128", ",", "\n", "type", "=", "int", ",", "\n", "help", "=", "\"The maximum total input sequence length after WordPiece tokenization. \\n\"", "\n", "\"Sequences longer than this will be truncated, and sequences shorter \\n\"", "\n", "\"than this will be padded.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--do_train\"", ",", "\n", "action", "=", "'store_true'", ",", "\n", "help", "=", "\"Whether to run training.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--do_eval\"", ",", "\n", "action", "=", "'store_true'", ",", "\n", "help", "=", "\"Whether to run eval on the dev set.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--do_lower_case\"", ",", "\n", "action", "=", "'store_true'", ",", "\n", "help", "=", "\"Set this flag if you are using an uncased model.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--train_batch_size\"", ",", "\n", "default", "=", "32", ",", "\n", "type", "=", "int", ",", "\n", "help", "=", "\"Total batch size for training.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--eval_batch_size\"", ",", "\n", "default", "=", "64", ",", "\n", "type", "=", "int", ",", "\n", "help", "=", "\"Total batch size for eval.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--learning_rate\"", ",", "default", "=", "5e-5", ",", "type", "=", "float", ",", "\n", "help", "=", "\"The initial learning rate for Adam.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--label_smoothing\"", ",", "default", "=", "0", ",", "type", "=", "float", ",", "\n", "help", "=", "\"The initial learning rate for Adam.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--weight_decay\"", ",", "\n", "default", "=", "0.01", ",", "\n", "type", "=", "float", ",", "\n", "help", "=", "\"The weight decay rate for Adam.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--finetune_decay\"", ",", "\n", "action", "=", "'store_true'", ",", "\n", "help", "=", "\"Weight decay to the original weights.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--num_train_epochs\"", ",", "\n", "default", "=", "3.0", ",", "\n", "type", "=", "float", ",", "\n", "help", "=", "\"Total number of training epochs to perform.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--warmup_proportion\"", ",", "\n", "default", "=", "0.1", ",", "\n", "type", "=", "float", ",", "\n", "help", "=", "\"Proportion of training to perform linear learning rate warmup for. \"", "\n", "\"E.g., 0.1 = 10%% of training.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--hidden_dropout_prob\"", ",", "default", "=", "0.1", ",", "type", "=", "float", ",", "\n", "help", "=", "\"Dropout rate for hidden states.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--attention_probs_dropout_prob\"", ",", "default", "=", "0.1", ",", "type", "=", "float", ",", "\n", "help", "=", "\"Dropout rate for attention probabilities.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--no_cuda\"", ",", "\n", "action", "=", "'store_true'", ",", "\n", "help", "=", "\"Whether not to use CUDA when available\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--local_rank\"", ",", "\n", "type", "=", "int", ",", "\n", "default", "=", "-", "1", ",", "\n", "help", "=", "\"local_rank for distributed training on gpus\"", ")", "\n", "parser", ".", "add_argument", "(", "'--seed'", ",", "\n", "type", "=", "int", ",", "\n", "default", "=", "42", ",", "\n", "help", "=", "\"random seed for initialization\"", ")", "\n", "parser", ".", "add_argument", "(", "'--gradient_accumulation_steps'", ",", "\n", "type", "=", "int", ",", "\n", "default", "=", "1", ",", "\n", "help", "=", "\"Number of updates steps to accumulate before performing a backward/update pass.\"", ")", "\n", "parser", ".", "add_argument", "(", "'--fp16'", ",", "action", "=", "'store_true'", ",", "\n", "help", "=", "\"Whether to use 16-bit float precision instead of 32-bit\"", ")", "\n", "parser", ".", "add_argument", "(", "'--fp32_embedding'", ",", "action", "=", "'store_true'", ",", "\n", "help", "=", "\"Whether to use 32-bit float precision instead of 16-bit for embeddings\"", ")", "\n", "parser", ".", "add_argument", "(", "'--loss_scale'", ",", "type", "=", "float", ",", "default", "=", "0", ",", "\n", "help", "=", "\"Loss scaling to improve fp16 numeric stability. Only used when fp16 set to True.\\n\"", "\n", "\"0 (default value): dynamic loss scaling.\\n\"", "\n", "\"Positive power of 2: static loss scaling value.\\n\"", ")", "\n", "parser", ".", "add_argument", "(", "'--amp'", ",", "action", "=", "'store_true'", ",", "\n", "help", "=", "\"Whether to use amp for fp16\"", ")", "\n", "parser", ".", "add_argument", "(", "'--from_scratch'", ",", "action", "=", "'store_true'", ",", "\n", "help", "=", "\"Initialize parameters with random values (i.e., training from scratch).\"", ")", "\n", "parser", ".", "add_argument", "(", "'--new_segment_ids'", ",", "action", "=", "'store_true'", ",", "\n", "help", "=", "\"Use new segment ids for bi-uni-directional LM.\"", ")", "\n", "parser", ".", "add_argument", "(", "'--new_pos_ids'", ",", "action", "=", "'store_true'", ",", "\n", "help", "=", "\"Use new position ids for LMs.\"", ")", "\n", "parser", ".", "add_argument", "(", "'--tokenized_input'", ",", "action", "=", "'store_true'", ",", "\n", "help", "=", "\"Whether the input is tokenized.\"", ")", "\n", "parser", ".", "add_argument", "(", "'--max_len_a'", ",", "type", "=", "int", ",", "default", "=", "0", ",", "\n", "help", "=", "\"Truncate_config: maximum length of segment A.\"", ")", "\n", "parser", ".", "add_argument", "(", "'--max_len_b'", ",", "type", "=", "int", ",", "default", "=", "0", ",", "\n", "help", "=", "\"Truncate_config: maximum length of segment B.\"", ")", "\n", "parser", ".", "add_argument", "(", "'--trunc_seg'", ",", "default", "=", "''", ",", "\n", "help", "=", "\"Truncate_config: first truncate segment A/B (option: a, b).\"", ")", "\n", "parser", ".", "add_argument", "(", "'--always_truncate_tail'", ",", "action", "=", "'store_true'", ",", "\n", "help", "=", "\"Truncate_config: Whether we should always truncate tail.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--mask_prob\"", ",", "default", "=", "0.15", ",", "type", "=", "float", ",", "\n", "help", "=", "\"Number of prediction is sometimes less than max_pred when sequence is short.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--mask_prob_eos\"", ",", "default", "=", "0", ",", "type", "=", "float", ",", "\n", "help", "=", "\"Number of prediction is sometimes less than max_pred when sequence is short.\"", ")", "\n", "parser", ".", "add_argument", "(", "'--max_pred'", ",", "type", "=", "int", ",", "default", "=", "20", ",", "\n", "help", "=", "\"Max tokens of prediction.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--num_workers\"", ",", "default", "=", "0", ",", "type", "=", "int", ",", "\n", "help", "=", "\"Number of workers for the data loader.\"", ")", "\n", "\n", "parser", ".", "add_argument", "(", "'--mask_source_words'", ",", "action", "=", "'store_true'", ",", "\n", "help", "=", "\"Whether to mask source words for training\"", ")", "\n", "parser", ".", "add_argument", "(", "'--skipgram_prb'", ",", "type", "=", "float", ",", "default", "=", "0.0", ",", "\n", "help", "=", "'prob of ngram mask'", ")", "\n", "parser", ".", "add_argument", "(", "'--skipgram_size'", ",", "type", "=", "int", ",", "default", "=", "1", ",", "\n", "help", "=", "'the max size of ngram mask'", ")", "\n", "parser", ".", "add_argument", "(", "'--mask_whole_word'", ",", "action", "=", "'store_true'", ",", "\n", "help", "=", "\"Whether masking a whole word.\"", ")", "\n", "parser", ".", "add_argument", "(", "'--do_l2r_training'", ",", "action", "=", "'store_true'", ",", "\n", "help", "=", "\"Whether to do left to right training\"", ")", "\n", "parser", ".", "add_argument", "(", "'--has_sentence_oracle'", ",", "action", "=", "'store_true'", ",", "\n", "help", "=", "\"Whether to have sentence level oracle for training. \"", "\n", "\"Only useful for summary generation\"", ")", "\n", "parser", ".", "add_argument", "(", "'--max_position_embeddings'", ",", "type", "=", "int", ",", "default", "=", "None", ",", "\n", "help", "=", "\"max position embeddings\"", ")", "\n", "parser", ".", "add_argument", "(", "'--relax_projection'", ",", "action", "=", "'store_true'", ",", "\n", "help", "=", "\"Use different projection layers for tasks.\"", ")", "\n", "parser", ".", "add_argument", "(", "'--ffn_type'", ",", "default", "=", "0", ",", "type", "=", "int", ",", "\n", "help", "=", "\"0: default mlp; 1: W((Wx+b) elem_prod x);\"", ")", "\n", "parser", ".", "add_argument", "(", "'--num_qkv'", ",", "default", "=", "0", ",", "type", "=", "int", ",", "\n", "help", "=", "\"Number of different <Q,K,V>.\"", ")", "\n", "parser", ".", "add_argument", "(", "'--seg_emb'", ",", "action", "=", "'store_true'", ",", "\n", "help", "=", "\"Using segment embedding for self-attention.\"", ")", "\n", "parser", ".", "add_argument", "(", "'--s2s_special_token'", ",", "action", "=", "'store_true'", ",", "\n", "help", "=", "\"New special tokens ([S2S_SEP]/[S2S_CLS]) of S2S.\"", ")", "\n", "parser", ".", "add_argument", "(", "'--s2s_add_segment'", ",", "action", "=", "'store_true'", ",", "\n", "help", "=", "\"Additional segmental for the encoder of S2S.\"", ")", "\n", "parser", ".", "add_argument", "(", "'--s2s_share_segment'", ",", "action", "=", "'store_true'", ",", "\n", "help", "=", "\"Sharing segment embeddings for the encoder of S2S (used with --s2s_add_segment).\"", ")", "\n", "parser", ".", "add_argument", "(", "'--pos_shift'", ",", "action", "=", "'store_true'", ",", "\n", "help", "=", "\"Using position shift for fine-tuning.\"", ")", "\n", "\n", "args", "=", "parser", ".", "parse_args", "(", ")", "\n", "\n", "# assert Path(args.model_recover_path).exists(", "\n", "# ), \"--model_recover_path doesn't exist\"", "\n", "\n", "args", ".", "output_dir", "=", "args", ".", "output_dir", ".", "replace", "(", "\n", "'[PT_OUTPUT_DIR]'", ",", "os", ".", "getenv", "(", "'PT_OUTPUT_DIR'", ",", "''", ")", ")", "\n", "args", ".", "log_dir", "=", "args", ".", "log_dir", ".", "replace", "(", "\n", "'[PT_OUTPUT_DIR]'", ",", "os", ".", "getenv", "(", "'PT_OUTPUT_DIR'", ",", "''", ")", ")", "\n", "\n", "os", ".", "makedirs", "(", "args", ".", "output_dir", ",", "exist_ok", "=", "True", ")", "\n", "os", ".", "makedirs", "(", "args", ".", "log_dir", ",", "exist_ok", "=", "True", ")", "\n", "json", ".", "dump", "(", "args", ".", "__dict__", ",", "open", "(", "os", ".", "path", ".", "join", "(", "\n", "args", ".", "output_dir", ",", "'opt.json'", ")", ",", "'w'", ")", ",", "sort_keys", "=", "True", ",", "indent", "=", "2", ")", "\n", "\n", "if", "args", ".", "local_rank", "==", "-", "1", "or", "args", ".", "no_cuda", ":", "\n", "        ", "device", "=", "torch", ".", "device", "(", "\n", "\"cuda\"", "if", "torch", ".", "cuda", ".", "is_available", "(", ")", "and", "not", "args", ".", "no_cuda", "else", "\"cpu\"", ")", "\n", "n_gpu", "=", "torch", ".", "cuda", ".", "device_count", "(", ")", "\n", "", "else", ":", "\n", "        ", "torch", ".", "cuda", ".", "set_device", "(", "args", ".", "local_rank", ")", "\n", "device", "=", "torch", ".", "device", "(", "\"cuda\"", ",", "args", ".", "local_rank", ")", "\n", "n_gpu", "=", "1", "\n", "# Initializes the distributed backend which will take care of sychronizing nodes/GPUs", "\n", "dist", ".", "init_process_group", "(", "backend", "=", "'nccl'", ")", "\n", "", "logger", ".", "info", "(", "\"device: {} n_gpu: {}, distributed training: {}, 16-bits training: {}\"", ".", "format", "(", "\n", "device", ",", "n_gpu", ",", "bool", "(", "args", ".", "local_rank", "!=", "-", "1", ")", ",", "args", ".", "fp16", ")", ")", "\n", "\n", "if", "args", ".", "gradient_accumulation_steps", "<", "1", ":", "\n", "        ", "raise", "ValueError", "(", "\"Invalid gradient_accumulation_steps parameter: {}, should be >= 1\"", ".", "format", "(", "\n", "args", ".", "gradient_accumulation_steps", ")", ")", "\n", "\n", "", "args", ".", "train_batch_size", "=", "int", "(", "\n", "args", ".", "train_batch_size", "/", "args", ".", "gradient_accumulation_steps", ")", "\n", "\n", "random", ".", "seed", "(", "args", ".", "seed", ")", "\n", "np", ".", "random", ".", "seed", "(", "args", ".", "seed", ")", "\n", "torch", ".", "manual_seed", "(", "args", ".", "seed", ")", "\n", "if", "n_gpu", ">", "0", ":", "\n", "        ", "torch", ".", "cuda", ".", "manual_seed_all", "(", "args", ".", "seed", ")", "\n", "\n", "", "if", "not", "args", ".", "do_train", "and", "not", "args", ".", "do_eval", ":", "\n", "        ", "raise", "ValueError", "(", "\n", "\"At least one of `do_train` or `do_eval` must be True.\"", ")", "\n", "\n", "", "if", "args", ".", "local_rank", "not", "in", "(", "-", "1", ",", "0", ")", ":", "\n", "# Make sure only the first process in distributed training will download model & vocab", "\n", "        ", "dist", ".", "barrier", "(", ")", "\n", "", "tokenizer", "=", "BertTokenizer", ".", "from_pretrained", "(", "\n", "args", ".", "bert_model", ",", "do_lower_case", "=", "args", ".", "do_lower_case", ")", "\n", "if", "args", ".", "max_position_embeddings", ":", "\n", "        ", "tokenizer", ".", "max_len", "=", "args", ".", "max_position_embeddings", "\n", "", "data_tokenizer", "=", "WhitespaceTokenizer", "(", ")", "if", "args", ".", "tokenized_input", "else", "tokenizer", "\n", "if", "args", ".", "local_rank", "==", "0", ":", "\n", "        ", "dist", ".", "barrier", "(", ")", "\n", "\n", "", "if", "args", ".", "do_train", ":", "\n", "        ", "print", "(", "\"Loading Train Dataset\"", ",", "args", ".", "data_dir", ")", "\n", "# bi_uni_pipeline = [seq2seq_loader.Preprocess4Seq2seq(args.max_pred, args.mask_prob, list(tokenizer.vocab.keys(", "\n", "# )), tokenizer.convert_tokens_to_ids, args.max_seq_length, new_segment_ids=args.new_segment_ids, truncate_config={'max_len_a': args.max_len_a, 'max_len_b': args.max_len_b, 'trunc_seg': args.trunc_seg, 'always_truncate_tail': args.always_truncate_tail}, mask_source_words=args.mask_source_words, skipgram_prb=args.skipgram_prb, skipgram_size=args.skipgram_size, mask_whole_word=args.mask_whole_word, mode=\"s2s\", has_oracle=args.has_sentence_oracle, num_qkv=args.num_qkv, s2s_special_token=args.s2s_special_token, s2s_add_segment=args.s2s_add_segment, s2s_share_segment=args.s2s_share_segment, pos_shift=args.pos_shift)]", "\n", "\n", "## \u6784\u5efas2s\u4efb\u52a1 dataloader", "\n", "s2s_pipeline", "=", "seq2seq_loader", ".", "Preprocess4Seq2seq", "(", "args", ".", "max_pred", ",", "args", ".", "mask_prob", ",", "list", "(", "tokenizer", ".", "vocab", ".", "keys", "(", "\n", ")", ")", ",", "tokenizer", ".", "convert_tokens_to_ids", ",", "args", ".", "max_seq_length", ",", "new_segment_ids", "=", "args", ".", "new_segment_ids", ",", "\n", "truncate_config", "=", "{", "'max_len_a'", ":", "args", ".", "max_len_a", ",", "'max_len_b'", ":", "args", ".", "max_len_b", ",", "'trunc_seg'", ":", "args", ".", "trunc_seg", ",", "'always_truncate_tail'", ":", "args", ".", "always_truncate_tail", "}", ",", "\n", "mask_source_words", "=", "args", ".", "mask_source_words", ",", "skipgram_prb", "=", "args", ".", "skipgram_prb", ",", "skipgram_size", "=", "args", ".", "skipgram_size", ",", "mask_whole_word", "=", "args", ".", "mask_whole_word", ",", "\n", "mode", "=", "'s2s'", ",", "has_oracle", "=", "args", ".", "has_sentence_oracle", ",", "num_qkv", "=", "args", ".", "num_qkv", ",", "s2s_special_token", "=", "args", ".", "s2s_special_token", ",", "s2s_add_segment", "=", "args", ".", "s2s_add_segment", ",", "\n", "s2s_share_segment", "=", "args", ".", "s2s_share_segment", ",", "pos_shift", "=", "args", ".", "pos_shift", ")", "\n", "\n", "\n", "## \u6784\u5efaleft to right\u4efb\u52a1 dataloader", "\n", "l2r_pipeline", "=", "unidir_loader", ".", "Preprocess4UniDirection", "(", "args", ".", "max_pred", ",", "args", ".", "mask_prob", ",", "list", "(", "tokenizer", ".", "vocab", ".", "keys", "(", "\n", ")", ")", ",", "tokenizer", ".", "convert_tokens_to_ids", ",", "args", ".", "max_seq_length", ",", "new_segment_ids", "=", "args", ".", "new_segment_ids", ",", "\n", "truncate_config", "=", "{", "'max_len_a'", ":", "args", ".", "max_len_a", ",", "'max_len_b'", ":", "args", ".", "max_len_b", ",", "'trunc_seg'", ":", "args", ".", "trunc_seg", ",", "'always_truncate_tail'", ":", "args", ".", "always_truncate_tail", "}", ",", "\n", "mask_source_words", "=", "args", ".", "mask_source_words", ",", "skipgram_prb", "=", "args", ".", "skipgram_prb", ",", "skipgram_size", "=", "args", ".", "skipgram_size", ",", "mask_whole_word", "=", "args", ".", "mask_whole_word", ",", "\n", "mode", "=", "'l2r'", ",", "has_oracle", "=", "args", ".", "has_sentence_oracle", ",", "num_qkv", "=", "args", ".", "num_qkv", ",", "s2s_special_token", "=", "args", ".", "s2s_special_token", ",", "s2s_add_segment", "=", "args", ".", "s2s_add_segment", ",", "\n", "s2s_share_segment", "=", "args", ".", "s2s_share_segment", ",", "pos_shift", "=", "args", ".", "pos_shift", ")", "\n", "\n", "## \u6784\u5efaright to left\u4efb\u52a1 dataloader, \u5f53\u524d\u7248\u672c\u6ca1\u6709\u4f7f\u7528", "\n", "r2l_pipeline", "=", "unidir_loader", ".", "Preprocess4UniDirection", "(", "args", ".", "max_pred", ",", "args", ".", "mask_prob", ",", "list", "(", "tokenizer", ".", "vocab", ".", "keys", "(", "\n", ")", ")", ",", "tokenizer", ".", "convert_tokens_to_ids", ",", "args", ".", "max_seq_length", ",", "new_segment_ids", "=", "args", ".", "new_segment_ids", ",", "\n", "truncate_config", "=", "{", "'max_len_a'", ":", "args", ".", "max_len_a", ",", "'max_len_b'", ":", "args", ".", "max_len_b", ",", "'trunc_seg'", ":", "args", ".", "trunc_seg", ",", "'always_truncate_tail'", ":", "args", ".", "always_truncate_tail", "}", ",", "\n", "mask_source_words", "=", "args", ".", "mask_source_words", ",", "skipgram_prb", "=", "args", ".", "skipgram_prb", ",", "skipgram_size", "=", "args", ".", "skipgram_size", ",", "mask_whole_word", "=", "args", ".", "mask_whole_word", ",", "\n", "mode", "=", "'r2l'", ",", "has_oracle", "=", "args", ".", "has_sentence_oracle", ",", "num_qkv", "=", "args", ".", "num_qkv", ",", "s2s_special_token", "=", "args", ".", "s2s_special_token", ",", "s2s_add_segment", "=", "args", ".", "s2s_add_segment", ",", "\n", "s2s_share_segment", "=", "args", ".", "s2s_share_segment", ",", "pos_shift", "=", "args", ".", "pos_shift", ")", "\n", "\n", "## \u6784\u5efabi-direction\u4efb\u52a1 dataloader, \u5305\u62ecmlm \u548c nsp", "\n", "bi_pipeline", "=", "bidir_loader", ".", "Preprocess4BiDirection", "(", "args", ".", "max_pred", ",", "args", ".", "mask_prob", ",", "list", "(", "tokenizer", ".", "vocab", ".", "keys", "(", "\n", ")", ")", ",", "tokenizer", ".", "convert_tokens_to_ids", ",", "args", ".", "max_seq_length", ",", "new_segment_ids", "=", "args", ".", "new_segment_ids", ",", "\n", "truncate_config", "=", "{", "'max_len_a'", ":", "args", ".", "max_len_a", ",", "'max_len_b'", ":", "args", ".", "max_len_b", ",", "'trunc_seg'", ":", "args", ".", "trunc_seg", ",", "'always_truncate_tail'", ":", "args", ".", "always_truncate_tail", "}", ",", "\n", "mask_source_words", "=", "args", ".", "mask_source_words", ",", "skipgram_prb", "=", "args", ".", "skipgram_prb", ",", "skipgram_size", "=", "args", ".", "skipgram_size", ",", "mask_whole_word", "=", "args", ".", "mask_whole_word", ",", "\n", "mode", "=", "''", ",", "has_oracle", "=", "args", ".", "has_sentence_oracle", ",", "num_qkv", "=", "args", ".", "num_qkv", ",", "s2s_special_token", "=", "args", ".", "s2s_special_token", ",", "s2s_add_segment", "=", "args", ".", "s2s_add_segment", ",", "\n", "s2s_share_segment", "=", "args", ".", "s2s_share_segment", ",", "pos_shift", "=", "args", ".", "pos_shift", ")", "\n", "\n", "file_oracle", "=", "None", "\n", "if", "args", ".", "has_sentence_oracle", ":", "\n", "            ", "file_oracle", "=", "os", ".", "path", ".", "join", "(", "args", ".", "data_dir", ",", "'train.oracle'", ")", "\n", "\n", "## \u7531\u4e8e\u591a\u4efb\u52a1\uff0c\u76ee\u524d\u56fa\u5b9a\u4e86\u6587\u4ef6\u540d\u79f0", "\n", "", "bi_src", "=", "os", ".", "path", ".", "join", "(", "args", ".", "data_dir", ",", "'bi.src'", ")", "\n", "bi_tgt", "=", "os", ".", "path", ".", "join", "(", "args", ".", "data_dir", ",", "'bi.tgt'", ")", "\n", "bi_nsp", "=", "os", ".", "path", ".", "join", "(", "args", ".", "data_dir", ",", "'bi.nsp'", ")", "\n", "uni_tgt", "=", "os", ".", "path", ".", "join", "(", "args", ".", "data_dir", ",", "'uni.tgt'", ")", "\n", "s2s_src", "=", "os", ".", "path", ".", "join", "(", "args", ".", "data_dir", ",", "'s2s.src'", ")", "\n", "s2s_tgt", "=", "os", ".", "path", ".", "join", "(", "args", ".", "data_dir", ",", "'s2s.tgt'", ")", "\n", "\n", "\n", "## Bi_Uni_Dataset, \u5728\u6bcf\u4e2abatch\u4e2d\u5404\u4efb\u52a1\u6570\u636e\u5404\u5360\u4e09\u5206\u4e4b\u4e00, \u5982\u9700\u8c03\u6574\u6216\u65b0\u589e\u4efb\u52a1, \u53ef\u5728bi_uni_loader.py\u4e2d\u66f4\u6539", "\n", "train_dataset", "=", "bi_uni_loader", ".", "Bi_Uni_Dataset", "(", "\n", "bi_src", ",", "bi_tgt", ",", "uni_tgt", ",", "s2s_src", ",", "s2s_tgt", ",", "\n", "args", ".", "train_batch_size", ",", "data_tokenizer", ",", "args", ".", "max_seq_length", ",", "file_nsp", "=", "bi_nsp", ",", "\n", "bi_uni_pipeline", "=", "[", "bi_pipeline", ",", "l2r_pipeline", ",", "s2s_pipeline", "]", ")", "\n", "\n", "if", "args", ".", "local_rank", "==", "-", "1", ":", "\n", "            ", "train_sampler", "=", "RandomSampler", "(", "train_dataset", ",", "replacement", "=", "False", ")", "\n", "_batch_size", "=", "args", ".", "train_batch_size", "\n", "", "else", ":", "\n", "            ", "train_sampler", "=", "DistributedSampler", "(", "train_dataset", ")", "\n", "_batch_size", "=", "args", ".", "train_batch_size", "//", "dist", ".", "get_world_size", "(", ")", "\n", "", "train_dataloader", "=", "torch", ".", "utils", ".", "data", ".", "DataLoader", "(", "train_dataset", ",", "batch_size", "=", "_batch_size", ",", "sampler", "=", "train_sampler", ",", "\n", "num_workers", "=", "args", ".", "num_workers", ",", "collate_fn", "=", "seq2seq_loader", ".", "batch_list_to_batch_tensors", ",", "pin_memory", "=", "False", ")", "\n", "\n", "# note: args.train_batch_size has been changed to (/= args.gradient_accumulation_steps)", "\n", "# t_total = int(math.ceil(len(train_dataset.ex_list) / args.train_batch_size)", "\n", "", "t_total", "=", "int", "(", "len", "(", "train_dataloader", ")", "*", "args", ".", "num_train_epochs", "/", "\n", "args", ".", "gradient_accumulation_steps", ")", "\n", "\n", "# amp_handle = None", "\n", "# if args.fp16 and args.amp:", "\n", "#     from apex import amp", "\n", "#     amp_handle = amp.init(enable_caching=True)", "\n", "#     logger.info(\"enable fp16 with amp\")", "\n", "\n", "# Prepare model", "\n", "recover_step", "=", "_get_max_epoch_model", "(", "args", ".", "output_dir", ")", "\n", "cls_num_labels", "=", "2", "\n", "type_vocab_size", "=", "6", "+", "(", "1", "if", "args", ".", "s2s_add_segment", "else", "0", ")", "if", "args", ".", "new_segment_ids", "else", "2", "\n", "num_sentlvl_labels", "=", "2", "if", "args", ".", "has_sentence_oracle", "else", "0", "\n", "relax_projection", "=", "4", "if", "args", ".", "relax_projection", "else", "0", "\n", "if", "args", ".", "local_rank", "not", "in", "(", "-", "1", ",", "0", ")", ":", "\n", "# Make sure only the first process in distributed training will download model & vocab", "\n", "        ", "dist", ".", "barrier", "(", ")", "\n", "", "if", "(", "recover_step", "is", "None", ")", "and", "(", "args", ".", "model_recover_path", "is", "None", ")", ":", "\n", "# if _state_dict == {}, the parameters are randomly initialized", "\n", "# if _state_dict == None, the parameters are initialized with bert-init", "\n", "        ", "_state_dict", "=", "{", "}", "if", "args", ".", "from_scratch", "else", "None", "\n", "model", "=", "BertForPreTrainingLossMask", ".", "from_pretrained", "(", "\n", "args", ".", "bert_model", ",", "state_dict", "=", "_state_dict", ",", "num_labels", "=", "cls_num_labels", ",", "num_rel", "=", "0", ",", "type_vocab_size", "=", "type_vocab_size", ",", "config_path", "=", "args", ".", "config_path", ",", "task_idx", "=", "3", ",", "num_sentlvl_labels", "=", "num_sentlvl_labels", ",", "max_position_embeddings", "=", "args", ".", "max_position_embeddings", ",", "label_smoothing", "=", "args", ".", "label_smoothing", ",", "fp32_embedding", "=", "args", ".", "fp32_embedding", ",", "relax_projection", "=", "relax_projection", ",", "new_pos_ids", "=", "args", ".", "new_pos_ids", ",", "ffn_type", "=", "args", ".", "ffn_type", ",", "hidden_dropout_prob", "=", "args", ".", "hidden_dropout_prob", ",", "attention_probs_dropout_prob", "=", "args", ".", "attention_probs_dropout_prob", ",", "num_qkv", "=", "args", ".", "num_qkv", ",", "seg_emb", "=", "args", ".", "seg_emb", ")", "\n", "global_step", "=", "0", "\n", "", "else", ":", "\n", "        ", "if", "recover_step", ":", "\n", "            ", "logger", ".", "info", "(", "\"***** Recover model: %d *****\"", ",", "recover_step", ")", "\n", "model_recover", "=", "torch", ".", "load", "(", "os", ".", "path", ".", "join", "(", "\n", "args", ".", "output_dir", ",", "\"model.{0}.bin\"", ".", "format", "(", "recover_step", ")", ")", ",", "map_location", "=", "'cpu'", ")", "\n", "# recover_step == number of epochs", "\n", "global_step", "=", "math", ".", "floor", "(", "\n", "recover_step", "*", "t_total", "/", "args", ".", "num_train_epochs", ")", "\n", "", "elif", "args", ".", "model_recover_path", ":", "\n", "            ", "logger", ".", "info", "(", "\"***** Recover model: %s *****\"", ",", "\n", "args", ".", "model_recover_path", ")", "\n", "model_recover", "=", "torch", ".", "load", "(", "\n", "args", ".", "model_recover_path", ",", "map_location", "=", "'cpu'", ")", "\n", "global_step", "=", "0", "\n", "", "model", "=", "BertForPreTrainingLossMask", ".", "from_pretrained", "(", "\n", "args", ".", "bert_model", ",", "state_dict", "=", "model_recover", ",", "num_labels", "=", "cls_num_labels", ",", "num_rel", "=", "0", ",", "type_vocab_size", "=", "type_vocab_size", ",", "config_path", "=", "args", ".", "config_path", ",", "task_idx", "=", "3", ",", "num_sentlvl_labels", "=", "num_sentlvl_labels", ",", "max_position_embeddings", "=", "args", ".", "max_position_embeddings", ",", "label_smoothing", "=", "args", ".", "label_smoothing", ",", "fp32_embedding", "=", "args", ".", "fp32_embedding", ",", "relax_projection", "=", "relax_projection", ",", "new_pos_ids", "=", "args", ".", "new_pos_ids", ",", "ffn_type", "=", "args", ".", "ffn_type", ",", "hidden_dropout_prob", "=", "args", ".", "hidden_dropout_prob", ",", "attention_probs_dropout_prob", "=", "args", ".", "attention_probs_dropout_prob", ",", "num_qkv", "=", "args", ".", "num_qkv", ",", "seg_emb", "=", "args", ".", "seg_emb", ")", "\n", "", "if", "args", ".", "local_rank", "==", "0", ":", "\n", "        ", "dist", ".", "barrier", "(", ")", "\n", "\n", "", "if", "args", ".", "fp16", ":", "\n", "# model.half()", "\n", "        ", "if", "args", ".", "fp32_embedding", ":", "\n", "            ", "model", ".", "bert", ".", "embeddings", ".", "word_embeddings", ".", "float", "(", ")", "\n", "model", ".", "bert", ".", "embeddings", ".", "position_embeddings", ".", "float", "(", ")", "\n", "model", ".", "bert", ".", "embeddings", ".", "token_type_embeddings", ".", "float", "(", ")", "\n", "", "", "model", ".", "to", "(", "device", ")", "\n", "\n", "\n", "# Prepare optimizer", "\n", "param_optimizer", "=", "list", "(", "model", ".", "named_parameters", "(", ")", ")", "\n", "no_decay", "=", "[", "'bias'", ",", "'LayerNorm.bias'", ",", "'LayerNorm.weight'", "]", "\n", "optimizer_grouped_parameters", "=", "[", "\n", "{", "'params'", ":", "[", "p", "for", "n", ",", "p", "in", "param_optimizer", "if", "not", "any", "(", "\n", "nd", "in", "n", "for", "nd", "in", "no_decay", ")", "]", ",", "'weight_decay'", ":", "0.01", "}", ",", "\n", "{", "'params'", ":", "[", "p", "for", "n", ",", "p", "in", "param_optimizer", "if", "any", "(", "\n", "nd", "in", "n", "for", "nd", "in", "no_decay", ")", "]", ",", "'weight_decay'", ":", "0.0", "}", "\n", "]", "\n", "if", "args", ".", "fp16", ":", "\n", "        ", "try", ":", "\n", "# from apex.optimizers import FP16_Optimizer", "\n", "            ", "from", "pytorch_pretrained_bert", ".", "optimization_fp16", "import", "FP16_Optimizer_State", "\n", "from", "apex", ".", "optimizers", "import", "FusedAdam", "\n", "", "except", "ImportError", ":", "\n", "            ", "raise", "ImportError", "(", "\n", "\"Please install apex from https://www.github.com/nvidia/apex to use distributed and fp16 training.\"", ")", "\n", "\n", "", "optimizer", "=", "FusedAdam", "(", "optimizer_grouped_parameters", ",", "\n", "lr", "=", "args", ".", "learning_rate", ",", "\n", "bias_correction", "=", "False", ")", "\n", "# max_grad_norm=1.0)", "\n", "# if args.loss_scale == 0:", "\n", "#     optimizer = FP16_Optimizer_State(", "\n", "#         optimizer, dynamic_loss_scale=True)", "\n", "# else:", "\n", "#     optimizer = FP16_Optimizer_State(", "\n", "#         optimizer, static_loss_scale=args.loss_scale)", "\n", "\n", "\n", "\n", "model", ",", "optimizer", "=", "amp", ".", "initialize", "(", "model", ",", "optimizer", ",", "opt_level", "=", "\"O1\"", ",", "loss_scale", "=", "\"dynamic\"", ",", ")", "\n", "# amp_handle = amp.init(enable_caching=True)", "\n", "logger", ".", "info", "(", "\"enable fp16 with amp\"", ")", "\n", "", "else", ":", "\n", "        ", "optimizer", "=", "BertAdam", "(", "optimizer_grouped_parameters", ",", "\n", "lr", "=", "args", ".", "learning_rate", ",", "\n", "warmup", "=", "args", ".", "warmup_proportion", ",", "\n", "t_total", "=", "t_total", ")", "\n", "\n", "", "if", "args", ".", "local_rank", "!=", "-", "1", ":", "\n", "        ", "try", ":", "\n", "            ", "from", "torch", ".", "nn", ".", "parallel", "import", "DistributedDataParallel", "as", "DDP", "\n", "", "except", "ImportError", ":", "\n", "            ", "raise", "ImportError", "(", "\"DistributedDataParallel\"", ")", "\n", "", "model", "=", "DDP", "(", "model", ",", "device_ids", "=", "[", "\n", "args", ".", "local_rank", "]", ",", "output_device", "=", "args", ".", "local_rank", ",", "find_unused_parameters", "=", "True", ")", "\n", "", "elif", "n_gpu", ">", "1", ":", "\n", "# model = torch.nn.DataParallel(model)", "\n", "        ", "model", "=", "DataParallelImbalance", "(", "model", ")", "\n", "\n", "", "if", "recover_step", ":", "\n", "        ", "logger", ".", "info", "(", "\"***** Recover optimizer: %d *****\"", ",", "recover_step", ")", "\n", "optim_recover", "=", "torch", ".", "load", "(", "os", ".", "path", ".", "join", "(", "\n", "args", ".", "output_dir", ",", "\"optim.{0}.bin\"", ".", "format", "(", "recover_step", ")", ")", ",", "map_location", "=", "'cpu'", ")", "\n", "if", "hasattr", "(", "optim_recover", ",", "'state_dict'", ")", ":", "\n", "            ", "optim_recover", "=", "optim_recover", ".", "state_dict", "(", ")", "\n", "", "optimizer", ".", "load_state_dict", "(", "optim_recover", ")", "\n", "if", "args", ".", "loss_scale", "==", "0", ":", "\n", "            ", "logger", ".", "info", "(", "\"***** Recover optimizer: dynamic_loss_scale *****\"", ")", "\n", "optimizer", ".", "dynamic_loss_scale", "=", "True", "\n", "\n", "", "", "logger", ".", "info", "(", "\"***** CUDA.empty_cache() *****\"", ")", "\n", "torch", ".", "cuda", ".", "empty_cache", "(", ")", "\n", "\n", "if", "args", ".", "do_train", ":", "\n", "        ", "logger", ".", "info", "(", "\"***** Running training *****\"", ")", "\n", "logger", ".", "info", "(", "\"  Batch size = %d\"", ",", "args", ".", "train_batch_size", ")", "\n", "logger", ".", "info", "(", "\"  Num steps = %d\"", ",", "t_total", ")", "\n", "\n", "model", ".", "train", "(", ")", "\n", "if", "recover_step", ":", "\n", "            ", "start_epoch", "=", "recover_step", "+", "1", "\n", "", "else", ":", "\n", "            ", "start_epoch", "=", "1", "\n", "", "for", "i_epoch", "in", "trange", "(", "start_epoch", ",", "int", "(", "args", ".", "num_train_epochs", ")", "+", "1", ",", "desc", "=", "\"Epoch\"", ",", "disable", "=", "args", ".", "local_rank", "not", "in", "(", "-", "1", ",", "0", ")", ")", ":", "\n", "            ", "if", "args", ".", "local_rank", "!=", "-", "1", ":", "\n", "                ", "train_sampler", ".", "set_epoch", "(", "i_epoch", ")", "\n", "", "iter_bar", "=", "tqdm", "(", "train_dataloader", ",", "desc", "=", "'Iter (loss=X.XXX)'", ",", "\n", "disable", "=", "args", ".", "local_rank", "not", "in", "(", "-", "1", ",", "0", ")", ")", "\n", "for", "step", ",", "batch", "in", "enumerate", "(", "iter_bar", ")", ":", "\n", "                ", "batch", "=", "[", "\n", "t", ".", "to", "(", "device", ")", "if", "t", "is", "not", "None", "else", "None", "for", "t", "in", "batch", "]", "\n", "if", "args", ".", "has_sentence_oracle", ":", "\n", "                    ", "input_ids", ",", "segment_ids", ",", "input_mask", ",", "mask_qkv", ",", "lm_label_ids", ",", "masked_pos", ",", "masked_weights", ",", "is_next", ",", "task_idx", ",", "oracle_pos", ",", "oracle_weights", ",", "oracle_labels", "=", "batch", "\n", "", "else", ":", "\n", "                    ", "input_ids", ",", "segment_ids", ",", "input_mask", ",", "mask_qkv", ",", "lm_label_ids", ",", "masked_pos", ",", "masked_weights", ",", "is_next", ",", "task_idx", "=", "batch", "\n", "oracle_pos", ",", "oracle_weights", ",", "oracle_labels", "=", "None", ",", "None", ",", "None", "\n", "", "loss_tuple", "=", "model", "(", "input_ids", ",", "segment_ids", ",", "input_mask", ",", "lm_label_ids", ",", "is_next", ",", "masked_pos", "=", "masked_pos", ",", "masked_weights", "=", "masked_weights", ",", "task_idx", "=", "task_idx", ",", "masked_pos_2", "=", "oracle_pos", ",", "masked_weights_2", "=", "oracle_weights", ",", "\n", "masked_labels_2", "=", "oracle_labels", ",", "mask_qkv", "=", "mask_qkv", ")", "\n", "masked_lm_loss", ",", "next_sentence_loss", "=", "loss_tuple", "\n", "if", "n_gpu", ">", "1", ":", "# mean() to average on multi-gpu.", "\n", "# loss = loss.mean()", "\n", "                    ", "masked_lm_loss", "=", "masked_lm_loss", ".", "mean", "(", ")", "\n", "next_sentence_loss", "=", "next_sentence_loss", ".", "mean", "(", ")", "\n", "", "loss", "=", "masked_lm_loss", "+", "next_sentence_loss", "\n", "\n", "# logging for each step (i.e., before normalization by args.gradient_accumulation_steps)", "\n", "iter_bar", ".", "set_description", "(", "'Iter (loss=%5.3f)'", "%", "loss", ".", "item", "(", ")", ")", "\n", "\n", "# ensure that accumlated gradients are normalized", "\n", "if", "args", ".", "gradient_accumulation_steps", ">", "1", ":", "\n", "                    ", "loss", "=", "loss", "/", "args", ".", "gradient_accumulation_steps", "\n", "\n", "", "if", "args", ".", "fp16", ":", "\n", "                    ", "with", "amp", ".", "scale_loss", "(", "loss", ",", "optimizer", ")", "as", "scaled_loss", ":", "\n", "                        ", "scaled_loss", ".", "backward", "(", ")", "\n", "\n", "# optimizer.backward(loss)", "\n", "# if amp_handle:", "\n", "#     amp_handle._clear_cache()", "\n", "", "", "else", ":", "\n", "                    ", "loss", ".", "backward", "(", ")", "\n", "", "if", "(", "step", "+", "1", ")", "%", "args", ".", "gradient_accumulation_steps", "==", "0", ":", "\n", "                    ", "lr_this_step", "=", "args", ".", "learning_rate", "*", "warmup_linear", "(", "global_step", "/", "t_total", ",", "\n", "args", ".", "warmup_proportion", ")", "\n", "if", "args", ".", "fp16", ":", "\n", "# modify learning rate with special warm up BERT uses", "\n", "                        ", "for", "param_group", "in", "optimizer", ".", "param_groups", ":", "\n", "                            ", "param_group", "[", "'lr'", "]", "=", "lr_this_step", "\n", "", "", "optimizer", ".", "step", "(", ")", "\n", "optimizer", ".", "zero_grad", "(", ")", "\n", "global_step", "+=", "1", "\n", "\n", "# Save a trained model", "\n", "", "", "if", "(", "args", ".", "local_rank", "==", "-", "1", "or", "torch", ".", "distributed", ".", "get_rank", "(", ")", "==", "0", ")", ":", "\n", "                ", "logger", ".", "info", "(", "\n", "\"** ** * Saving fine-tuned model and optimizer ** ** * \"", ")", "\n", "model_to_save", "=", "model", ".", "module", "if", "hasattr", "(", "\n", "model", ",", "'module'", ")", "else", "model", "# Only save the model it-self", "\n", "output_model_file", "=", "os", ".", "path", ".", "join", "(", "\n", "args", ".", "output_dir", ",", "\"model.{0}.bin\"", ".", "format", "(", "i_epoch", ")", ")", "\n", "torch", ".", "save", "(", "model_to_save", ".", "state_dict", "(", ")", ",", "output_model_file", ")", "\n", "output_optim_file", "=", "os", ".", "path", ".", "join", "(", "\n", "args", ".", "output_dir", ",", "\"optim.{0}.bin\"", ".", "format", "(", "i_epoch", ")", ")", "\n", "torch", ".", "save", "(", "optimizer", ".", "state_dict", "(", ")", ",", "output_optim_file", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.biunilm.gen_seq_from_trace.read_traces_from_file": [[12, 20], ["open", "pickle.load", "range", "samples.append", "pickle.load"], "function", ["None"], ["def", "read_traces_from_file", "(", "file_name", ")", ":", "\n", "    ", "with", "open", "(", "file_name", ",", "\"rb\"", ")", "as", "fin", ":", "\n", "        ", "meta", "=", "pickle", ".", "load", "(", "fin", ")", "\n", "num_samples", "=", "meta", "[", "\"num_samples\"", "]", "\n", "samples", "=", "[", "]", "\n", "for", "_", "in", "range", "(", "num_samples", ")", ":", "\n", "            ", "samples", ".", "append", "(", "pickle", ".", "load", "(", "fin", ")", ")", "\n", "", "", "return", "samples", "\n", "\n"]], "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.biunilm.gen_seq_from_trace.get_best_sequence": [[22, 71], ["enumerate", "all", "range", "len", "all", "enumerate", "range", "seq.reverse", "seq.append", "math.fabs", "math.pow"], "function", ["None"], ["", "def", "get_best_sequence", "(", "sample", ",", "eos_id", ",", "pad_id", ",", "length_penalty", "=", "None", ",", "alpha", "=", "None", ",", "expect", "=", "None", ",", "min_len", "=", "None", ")", ":", "\n", "# if not any((length_penalty, alpha, expect, min_len)):", "\n", "#     raise ValueError(", "\n", "#         \"You can only specify length penalty or alpha, but not both.\")", "\n", "    ", "scores", "=", "sample", "[", "\"scores\"", "]", "\n", "wids_list", "=", "sample", "[", "\"wids\"", "]", "\n", "ptrs", "=", "sample", "[", "\"ptrs\"", "]", "\n", "\n", "last_frame_id", "=", "len", "(", "scores", ")", "-", "1", "\n", "for", "i", ",", "wids", "in", "enumerate", "(", "wids_list", ")", ":", "\n", "        ", "if", "all", "(", "wid", "in", "(", "eos_id", ",", "pad_id", ")", "for", "wid", "in", "wids", ")", ":", "\n", "            ", "last_frame_id", "=", "i", "\n", "break", "\n", "", "", "while", "all", "(", "wid", "==", "pad_id", "for", "wid", "in", "wids_list", "[", "last_frame_id", "]", ")", ":", "\n", "        ", "last_frame_id", "-=", "1", "\n", "\n", "", "max_score", "=", "-", "math", ".", "inf", "\n", "frame_id", "=", "-", "1", "\n", "pos_in_frame", "=", "-", "1", "\n", "\n", "for", "fid", "in", "range", "(", "last_frame_id", "+", "1", ")", ":", "\n", "        ", "for", "i", ",", "wid", "in", "enumerate", "(", "wids_list", "[", "fid", "]", ")", ":", "\n", "            ", "if", "fid", "<=", "last_frame_id", "and", "scores", "[", "fid", "]", "[", "i", "]", ">=", "0", ":", "\n", "# skip paddings", "\n", "                ", "continue", "\n", "", "if", "(", "wid", "in", "(", "eos_id", ",", "pad_id", ")", ")", "or", "fid", "==", "last_frame_id", ":", "\n", "                ", "s", "=", "scores", "[", "fid", "]", "[", "i", "]", "\n", "if", "length_penalty", ":", "\n", "                    ", "if", "expect", ":", "\n", "                        ", "s", "-=", "length_penalty", "*", "math", ".", "fabs", "(", "fid", "+", "1", "-", "expect", ")", "\n", "", "else", ":", "\n", "                        ", "s", "+=", "length_penalty", "*", "(", "fid", "+", "1", ")", "\n", "", "", "elif", "alpha", ":", "\n", "                    ", "s", "=", "s", "/", "math", ".", "pow", "(", "(", "5", "+", "fid", "+", "1", ")", "/", "6.0", ",", "alpha", ")", "\n", "", "if", "s", ">", "max_score", ":", "\n", "# if (frame_id != -1) and min_len and (fid+1 < min_len):", "\n", "#     continue", "\n", "                    ", "max_score", "=", "s", "\n", "frame_id", "=", "fid", "\n", "pos_in_frame", "=", "i", "\n", "", "", "", "", "if", "frame_id", "==", "-", "1", ":", "\n", "        ", "seq", "=", "[", "]", "\n", "", "else", ":", "\n", "        ", "seq", "=", "[", "wids_list", "[", "frame_id", "]", "[", "pos_in_frame", "]", "]", "\n", "for", "fid", "in", "range", "(", "frame_id", ",", "0", ",", "-", "1", ")", ":", "\n", "            ", "pos_in_frame", "=", "ptrs", "[", "fid", "]", "[", "pos_in_frame", "]", "\n", "seq", ".", "append", "(", "wids_list", "[", "fid", "-", "1", "]", "[", "pos_in_frame", "]", ")", "\n", "", "seq", ".", "reverse", "(", ")", "\n", "", "return", "seq", "\n", "\n"]], "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.biunilm.gen_seq_from_trace.detokenize": [[73, 81], ["tk.startswith", "r_list.append", "len"], "function", ["None"], ["", "def", "detokenize", "(", "tk_list", ")", ":", "\n", "    ", "r_list", "=", "[", "]", "\n", "for", "tk", "in", "tk_list", ":", "\n", "        ", "if", "tk", ".", "startswith", "(", "'##'", ")", "and", "len", "(", "r_list", ")", ">", "0", ":", "\n", "            ", "r_list", "[", "-", "1", "]", "=", "r_list", "[", "-", "1", "]", "+", "tk", "[", "2", ":", "]", "\n", "", "else", ":", "\n", "            ", "r_list", ".", "append", "(", "tk", ")", "\n", "", "", "return", "r_list", "\n", "\n"]], "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.biunilm.gen_seq_from_trace.simple_postprocess": [[83, 88], ["unicodedata.category().startswith", "all", "len", "len", "unicodedata.category"], "function", ["None"], ["", "def", "simple_postprocess", "(", "tk_list", ")", ":", "\n", "# truncate duplicate punctuations", "\n", "    ", "while", "tk_list", "and", "len", "(", "tk_list", ")", ">", "4", "and", "len", "(", "tk_list", "[", "-", "1", "]", ")", "==", "1", "and", "unicodedata", ".", "category", "(", "tk_list", "[", "-", "1", "]", ")", ".", "startswith", "(", "'P'", ")", "and", "all", "(", "it", "==", "tk_list", "[", "-", "1", "]", "for", "it", "in", "tk_list", "[", "-", "4", ":", "]", ")", ":", "\n", "        ", "tk_list", "=", "tk_list", "[", ":", "-", "3", "]", "\n", "", "return", "tk_list", "\n", "\n"]], "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.biunilm.gen_seq_from_trace.main": [[90, 128], ["pytorch_pretrained_bert.tokenization.BertTokenizer.from_pretrained", "set", "tqdm.tqdm", "BertTokenizer.from_pretrained.convert_tokens_to_ids", "glob.glob", "print", "gen_seq_from_trace.read_traces_from_file", "pathlib.Path().exists", "gen_seq_from_trace.get_best_sequence", "BertTokenizer.from_pretrained.convert_ids_to_tokens", "results.append", "open", "str", "str", "str", "str", "fout.write", "fout.write", "pathlib.Path", "buf.append", "gen_seq_from_trace.simple_postprocess", "gen_seq_from_trace.detokenize"], "function", ["home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.pytorch_pretrained_bert.tokenization.BertTokenizer.from_pretrained", "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.pytorch_pretrained_bert.tokenization.BertTokenizer.convert_tokens_to_ids", "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.biunilm.gen_seq_from_trace.read_traces_from_file", "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.biunilm.gen_seq_from_trace.get_best_sequence", "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.pytorch_pretrained_bert.tokenization.BertTokenizer.convert_ids_to_tokens", "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.biunilm.gen_seq_from_trace.simple_postprocess", "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.unilm.decode_seq2seq.detokenize"], ["", "def", "main", "(", "args", ")", ":", "\n", "    ", "tokenizer", "=", "BertTokenizer", ".", "from_pretrained", "(", "\n", "args", ".", "bert_model", ",", "do_lower_case", "=", "args", ".", "do_lower_case", ")", "\n", "\n", "eos_id", ",", "pad_id", "=", "set", "(", "tokenizer", ".", "convert_tokens_to_ids", "(", "[", "\"[SEP]\"", ",", "\"[PAD]\"", "]", ")", ")", "\n", "for", "input_file", "in", "tqdm", "(", "glob", ".", "glob", "(", "args", ".", "input", ")", ")", ":", "\n", "        ", "if", "not", "Path", "(", "input_file", "+", "'.trace.pickle'", ")", ".", "exists", "(", ")", ":", "\n", "            ", "continue", "\n", "", "print", "(", "input_file", ")", "\n", "samples", "=", "read_traces_from_file", "(", "input_file", "+", "'.trace.pickle'", ")", "\n", "\n", "results", "=", "[", "]", "\n", "\n", "for", "s", "in", "samples", ":", "\n", "            ", "word_ids", "=", "get_best_sequence", "(", "s", ",", "eos_id", ",", "pad_id", ",", "alpha", "=", "args", ".", "alpha", ",", "\n", "length_penalty", "=", "args", ".", "length_penalty", ",", "expect", "=", "args", ".", "expect", ",", "min_len", "=", "args", ".", "min_len", ")", "\n", "tokens", "=", "tokenizer", ".", "convert_ids_to_tokens", "(", "word_ids", ")", "\n", "buf", "=", "[", "]", "\n", "for", "t", "in", "tokens", ":", "\n", "                ", "if", "t", "in", "(", "\"[SEP]\"", ",", "\"[PAD]\"", ")", ":", "\n", "                    ", "break", "\n", "", "else", ":", "\n", "                    ", "buf", ".", "append", "(", "t", ")", "\n", "", "", "results", ".", "append", "(", "\" \"", ".", "join", "(", "simple_postprocess", "(", "detokenize", "(", "buf", ")", ")", ")", ")", "\n", "\n", "", "fn_out", "=", "input_file", "+", "'.'", "\n", "if", "args", ".", "length_penalty", ":", "\n", "            ", "fn_out", "+=", "'lenp'", "+", "str", "(", "args", ".", "length_penalty", ")", "\n", "", "if", "args", ".", "expect", ":", "\n", "            ", "fn_out", "+=", "'exp'", "+", "str", "(", "args", ".", "expect", ")", "\n", "", "if", "args", ".", "alpha", ":", "\n", "            ", "fn_out", "+=", "'alp'", "+", "str", "(", "args", ".", "alpha", ")", "\n", "", "if", "args", ".", "min_len", ":", "\n", "            ", "fn_out", "+=", "'minl'", "+", "str", "(", "args", ".", "min_len", ")", "\n", "", "with", "open", "(", "fn_out", ",", "\"w\"", ",", "encoding", "=", "\"utf-8\"", ")", "as", "fout", ":", "\n", "            ", "for", "line", "in", "results", ":", "\n", "                ", "fout", ".", "write", "(", "line", ")", "\n", "fout", ".", "write", "(", "\"\\n\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.biunilm.run_eval._get_max_epoch_model": [[38, 49], ["glob.glob", "glob.glob", "os.path.join", "os.path.join", "set", "set", "max", "int", "int", "pathlib.Path().stem.split", "pathlib.Path().stem.split", "pathlib.Path", "pathlib.Path"], "function", ["home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.preprocess.process_daren.split", "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.preprocess.process_daren.split"], ["def", "_get_max_epoch_model", "(", "output_dir", ")", ":", "\n", "    ", "fn_model_list", "=", "glob", ".", "glob", "(", "os", ".", "path", ".", "join", "(", "output_dir", ",", "\"model.*.bin\"", ")", ")", "\n", "fn_optim_list", "=", "glob", ".", "glob", "(", "os", ".", "path", ".", "join", "(", "output_dir", ",", "\"optim.*.bin\"", ")", ")", "\n", "if", "(", "not", "fn_model_list", ")", "or", "(", "not", "fn_optim_list", ")", ":", "\n", "        ", "return", "None", "\n", "", "both_set", "=", "set", "(", "[", "int", "(", "Path", "(", "fn", ")", ".", "stem", ".", "split", "(", "'.'", ")", "[", "-", "1", "]", ")", "for", "fn", "in", "fn_model_list", "]", "\n", ")", "&", "set", "(", "[", "int", "(", "Path", "(", "fn", ")", ".", "stem", ".", "split", "(", "'.'", ")", "[", "-", "1", "]", ")", "for", "fn", "in", "fn_optim_list", "]", ")", "\n", "if", "both_set", ":", "\n", "        ", "return", "max", "(", "both_set", ")", "\n", "", "else", ":", "\n", "        ", "return", "None", "\n", "\n"]], "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.biunilm.run_eval.main": [[51, 409], ["argparse.ArgumentParser", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.parse_args", "parser.parse_args.output_dir.replace", "parser.parse_args.log_dir.replace", "os.makedirs", "os.makedirs", "json.dump", "logger.info", "int", "random.seed", "numpy.random.seed", "torch.manual_seed", "torch.manual_seed", "pytorch_pretrained_bert.tokenization.BertTokenizer.from_pretrained", "int", "nn.data_parallel.DataParallelImbalance.to", "torch.cuda.empty_cache", "torch.cuda.empty_cache", "os.getenv", "os.getenv", "open", "torch.device", "torch.device", "torch.cuda.device_count", "torch.cuda.device_count", "torch.cuda.set_device", "torch.cuda.set_device", "torch.device", "torch.device", "torch.init_process_group", "ValueError", "torch.cuda.manual_seed_all", "torch.cuda.manual_seed_all", "ValueError", "torch.barrier", "pytorch_pretrained_bert.tokenization.WhitespaceTokenizer", "torch.barrier", "print", "os.path.join", "os.path.join", "biunilm.Seq2SeqDataset", "torch.utils.data.DataLoader", "torch.utils.data.DataLoader", "torch.barrier", "pytorch_pretrained_bert.modeling.BertForPreTrainingLossMask.from_pretrained", "pytorch_pretrained_bert.modeling.BertForPreTrainingLossMask.from_pretrained", "torch.barrier", "DDP", "nn.data_parallel.DataParallelImbalance.eval", "tqdm.trange", "print", "os.path.join", "bool", "biunilm.Preprocess4Seq2seq", "os.path.join", "torch.utils.data.RandomSampler", "torch.utils.data.distributed.DistributedSampler", "logger.info", "torch.load", "torch.load", "math.floor", "nn.data_parallel.DataParallelImbalance.bert.embeddings.word_embeddings.float", "nn.data_parallel.DataParallelImbalance.bert.embeddings.position_embeddings.float", "nn.data_parallel.DataParallelImbalance.bert.embeddings.token_type_embeddings.float", "nn.data_parallel.DataParallelImbalance", "tqdm.tqdm", "enumerate", "list", "torch.get_world_size", "len", "os.path.join", "logger.info", "torch.load", "torch.load", "ImportError", "torch.utils.data.distributed.DistributedSampler.set_epoch", "nn.data_parallel.DataParallelImbalance.", "tqdm.tqdm.set_description", "loss.item", "len", "torch.cuda.is_available", "torch.cuda.is_available", "BertTokenizer.from_pretrained.vocab.keys", "masked_lm_loss.mean.mean", "next_sentence_loss.mean.mean", "t.to", "loss.item"], "function", ["home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.pytorch_pretrained_bert.tokenization.BertTokenizer.from_pretrained", "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.pytorch_pretrained_bert.tokenization.BertTokenizer.from_pretrained", "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.pytorch_pretrained_bert.tokenization.BertTokenizer.from_pretrained"], ["", "", "def", "main", "(", ")", ":", "\n", "    ", "parser", "=", "argparse", ".", "ArgumentParser", "(", ")", "\n", "\n", "# Required parameters", "\n", "parser", ".", "add_argument", "(", "\"--data_dir\"", ",", "\n", "default", "=", "None", ",", "\n", "type", "=", "str", ",", "\n", "required", "=", "True", ",", "\n", "help", "=", "\"The input data dir. Should contain the .tsv files (or other data files) for the task.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--src_file\"", ",", "default", "=", "None", ",", "type", "=", "str", ",", "\n", "help", "=", "\"The input data file name.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--tgt_file\"", ",", "default", "=", "None", ",", "type", "=", "str", ",", "\n", "help", "=", "\"The output data file name.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--bert_model\"", ",", "default", "=", "None", ",", "type", "=", "str", ",", "required", "=", "True", ",", "\n", "help", "=", "\"Bert pre-trained model selected in the list: bert-base-uncased, \"", "\n", "\"bert-large-uncased, bert-base-cased, bert-base-multilingual, bert-base-chinese.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--vocab_path\"", ",", "default", "=", "None", ",", "type", "=", "str", ")", "\n", "parser", ".", "add_argument", "(", "\"--config_path\"", ",", "default", "=", "None", ",", "type", "=", "str", ",", "\n", "help", "=", "\"Bert config file path.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--output_dir\"", ",", "\n", "default", "=", "None", ",", "\n", "type", "=", "str", ",", "\n", "required", "=", "True", ",", "\n", "help", "=", "\"The output directory where the model predictions and checkpoints will be written.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--log_dir\"", ",", "\n", "default", "=", "''", ",", "\n", "type", "=", "str", ",", "\n", "required", "=", "True", ",", "\n", "help", "=", "\"The output directory where the log will be written.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--model_recover_path\"", ",", "\n", "default", "=", "None", ",", "\n", "type", "=", "str", ",", "\n", "help", "=", "\"The file of fine-tuned pretraining model.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--optim_recover_path\"", ",", "\n", "default", "=", "None", ",", "\n", "type", "=", "str", ",", "\n", "help", "=", "\"The file of pretraining optimizer.\"", ")", "\n", "\n", "# Other parameters", "\n", "parser", ".", "add_argument", "(", "\"--max_seq_length\"", ",", "\n", "default", "=", "128", ",", "\n", "type", "=", "int", ",", "\n", "help", "=", "\"The maximum total input sequence length after WordPiece tokenization. \\n\"", "\n", "\"Sequences longer than this will be truncated, and sequences shorter \\n\"", "\n", "\"than this will be padded.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--do_train\"", ",", "\n", "action", "=", "'store_true'", ",", "\n", "help", "=", "\"Whether to run training.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--do_eval\"", ",", "\n", "action", "=", "'store_true'", ",", "\n", "help", "=", "\"Whether to run eval on the dev set.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--do_lower_case\"", ",", "\n", "action", "=", "'store_true'", ",", "\n", "help", "=", "\"Set this flag if you are using an uncased model.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--train_batch_size\"", ",", "\n", "default", "=", "32", ",", "\n", "type", "=", "int", ",", "\n", "help", "=", "\"Total batch size for training.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--eval_batch_size\"", ",", "\n", "default", "=", "64", ",", "\n", "type", "=", "int", ",", "\n", "help", "=", "\"Total batch size for eval.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--learning_rate\"", ",", "default", "=", "5e-5", ",", "type", "=", "float", ",", "\n", "help", "=", "\"The initial learning rate for Adam.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--label_smoothing\"", ",", "default", "=", "0", ",", "type", "=", "float", ",", "\n", "help", "=", "\"The initial learning rate for Adam.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--weight_decay\"", ",", "\n", "default", "=", "0.01", ",", "\n", "type", "=", "float", ",", "\n", "help", "=", "\"The weight decay rate for Adam.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--finetune_decay\"", ",", "\n", "action", "=", "'store_true'", ",", "\n", "help", "=", "\"Weight decay to the original weights.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--num_train_epochs\"", ",", "\n", "default", "=", "3.0", ",", "\n", "type", "=", "float", ",", "\n", "help", "=", "\"Total number of training epochs to perform.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--warmup_proportion\"", ",", "\n", "default", "=", "0.1", ",", "\n", "type", "=", "float", ",", "\n", "help", "=", "\"Proportion of training to perform linear learning rate warmup for. \"", "\n", "\"E.g., 0.1 = 10%% of training.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--hidden_dropout_prob\"", ",", "default", "=", "0.1", ",", "type", "=", "float", ",", "\n", "help", "=", "\"Dropout rate for hidden states.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--attention_probs_dropout_prob\"", ",", "default", "=", "0.1", ",", "type", "=", "float", ",", "\n", "help", "=", "\"Dropout rate for attention probabilities.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--no_cuda\"", ",", "\n", "action", "=", "'store_true'", ",", "\n", "help", "=", "\"Whether not to use CUDA when available\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--local_rank\"", ",", "\n", "type", "=", "int", ",", "\n", "default", "=", "-", "1", ",", "\n", "help", "=", "\"local_rank for distributed training on gpus\"", ")", "\n", "parser", ".", "add_argument", "(", "'--seed'", ",", "\n", "type", "=", "int", ",", "\n", "default", "=", "42", ",", "\n", "help", "=", "\"random seed for initialization\"", ")", "\n", "parser", ".", "add_argument", "(", "'--gradient_accumulation_steps'", ",", "\n", "type", "=", "int", ",", "\n", "default", "=", "1", ",", "\n", "help", "=", "\"Number of updates steps to accumulate before performing a backward/update pass.\"", ")", "\n", "parser", ".", "add_argument", "(", "'--recover_step'", ",", "\n", "type", "=", "int", ",", "\n", "default", "=", "1", ")", "\n", "parser", ".", "add_argument", "(", "'--fp16'", ",", "action", "=", "'store_true'", ",", "\n", "help", "=", "\"Whether to use 16-bit float precision instead of 32-bit\"", ")", "\n", "parser", ".", "add_argument", "(", "'--fp32_embedding'", ",", "action", "=", "'store_true'", ",", "\n", "help", "=", "\"Whether to use 32-bit float precision instead of 16-bit for embeddings\"", ")", "\n", "parser", ".", "add_argument", "(", "'--loss_scale'", ",", "type", "=", "float", ",", "default", "=", "0", ",", "\n", "help", "=", "\"Loss scaling to improve fp16 numeric stability. Only used when fp16 set to True.\\n\"", "\n", "\"0 (default value): dynamic loss scaling.\\n\"", "\n", "\"Positive power of 2: static loss scaling value.\\n\"", ")", "\n", "parser", ".", "add_argument", "(", "'--amp'", ",", "action", "=", "'store_true'", ",", "\n", "help", "=", "\"Whether to use amp for fp16\"", ")", "\n", "parser", ".", "add_argument", "(", "'--from_scratch'", ",", "action", "=", "'store_true'", ",", "\n", "help", "=", "\"Initialize parameters with random values (i.e., training from scratch).\"", ")", "\n", "parser", ".", "add_argument", "(", "'--new_segment_ids'", ",", "action", "=", "'store_true'", ",", "\n", "help", "=", "\"Use new segment ids for bi-uni-directional LM.\"", ")", "\n", "parser", ".", "add_argument", "(", "'--new_pos_ids'", ",", "action", "=", "'store_true'", ",", "\n", "help", "=", "\"Use new position ids for LMs.\"", ")", "\n", "parser", ".", "add_argument", "(", "'--tokenized_input'", ",", "action", "=", "'store_true'", ",", "\n", "help", "=", "\"Whether the input is tokenized.\"", ")", "\n", "parser", ".", "add_argument", "(", "'--max_len_a'", ",", "type", "=", "int", ",", "default", "=", "0", ",", "\n", "help", "=", "\"Truncate_config: maximum length of segment A.\"", ")", "\n", "parser", ".", "add_argument", "(", "'--max_len_b'", ",", "type", "=", "int", ",", "default", "=", "0", ",", "\n", "help", "=", "\"Truncate_config: maximum length of segment B.\"", ")", "\n", "parser", ".", "add_argument", "(", "'--trunc_seg'", ",", "default", "=", "''", ",", "\n", "help", "=", "\"Truncate_config: first truncate segment A/B (option: a, b).\"", ")", "\n", "parser", ".", "add_argument", "(", "'--always_truncate_tail'", ",", "action", "=", "'store_true'", ",", "\n", "help", "=", "\"Truncate_config: Whether we should always truncate tail.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--mask_prob\"", ",", "default", "=", "0.15", ",", "type", "=", "float", ",", "\n", "help", "=", "\"Number of prediction is sometimes less than max_pred when sequence is short.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--mask_prob_eos\"", ",", "default", "=", "0", ",", "type", "=", "float", ",", "\n", "help", "=", "\"Number of prediction is sometimes less than max_pred when sequence is short.\"", ")", "\n", "parser", ".", "add_argument", "(", "'--max_pred'", ",", "type", "=", "int", ",", "default", "=", "20", ",", "\n", "help", "=", "\"Max tokens of prediction.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--num_workers\"", ",", "default", "=", "0", ",", "type", "=", "int", ",", "\n", "help", "=", "\"Number of workers for the data loader.\"", ")", "\n", "\n", "parser", ".", "add_argument", "(", "'--mask_source_words'", ",", "action", "=", "'store_true'", ",", "\n", "help", "=", "\"Whether to mask source words for training\"", ")", "\n", "parser", ".", "add_argument", "(", "'--skipgram_prb'", ",", "type", "=", "float", ",", "default", "=", "0.0", ",", "\n", "help", "=", "'prob of ngram mask'", ")", "\n", "parser", ".", "add_argument", "(", "'--skipgram_size'", ",", "type", "=", "int", ",", "default", "=", "1", ",", "\n", "help", "=", "'the max size of ngram mask'", ")", "\n", "parser", ".", "add_argument", "(", "'--mask_whole_word'", ",", "action", "=", "'store_true'", ",", "\n", "help", "=", "\"Whether masking a whole word.\"", ")", "\n", "parser", ".", "add_argument", "(", "'--do_l2r_training'", ",", "action", "=", "'store_true'", ",", "\n", "help", "=", "\"Whether to do left to right training\"", ")", "\n", "parser", ".", "add_argument", "(", "'--has_sentence_oracle'", ",", "action", "=", "'store_true'", ",", "\n", "help", "=", "\"Whether to have sentence level oracle for training. \"", "\n", "\"Only useful for summary generation\"", ")", "\n", "parser", ".", "add_argument", "(", "'--max_position_embeddings'", ",", "type", "=", "int", ",", "default", "=", "None", ",", "\n", "help", "=", "\"max position embeddings\"", ")", "\n", "parser", ".", "add_argument", "(", "'--relax_projection'", ",", "action", "=", "'store_true'", ",", "\n", "help", "=", "\"Use different projection layers for tasks.\"", ")", "\n", "parser", ".", "add_argument", "(", "'--ffn_type'", ",", "default", "=", "0", ",", "type", "=", "int", ",", "\n", "help", "=", "\"0: default mlp; 1: W((Wx+b) elem_prod x);\"", ")", "\n", "parser", ".", "add_argument", "(", "'--num_qkv'", ",", "default", "=", "0", ",", "type", "=", "int", ",", "\n", "help", "=", "\"Number of different <Q,K,V>.\"", ")", "\n", "parser", ".", "add_argument", "(", "'--seg_emb'", ",", "action", "=", "'store_true'", ",", "\n", "help", "=", "\"Using segment embedding for self-attention.\"", ")", "\n", "parser", ".", "add_argument", "(", "'--s2s_special_token'", ",", "action", "=", "'store_true'", ",", "\n", "help", "=", "\"New special tokens ([S2S_SEP]/[S2S_CLS]) of S2S.\"", ")", "\n", "parser", ".", "add_argument", "(", "'--s2s_add_segment'", ",", "action", "=", "'store_true'", ",", "\n", "help", "=", "\"Additional segmental for the encoder of S2S.\"", ")", "\n", "parser", ".", "add_argument", "(", "'--s2s_share_segment'", ",", "action", "=", "'store_true'", ",", "\n", "help", "=", "\"Sharing segment embeddings for the encoder of S2S (used with --s2s_add_segment).\"", ")", "\n", "parser", ".", "add_argument", "(", "'--pos_shift'", ",", "action", "=", "'store_true'", ",", "\n", "help", "=", "\"Using position shift for fine-tuning.\"", ")", "\n", "\n", "args", "=", "parser", ".", "parse_args", "(", ")", "\n", "\n", "if", "args", ".", "vocab_path", "is", "None", ":", "\n", "        ", "args", ".", "vocab_path", "=", "args", ".", "bert_model", "\n", "\n", "# assert Path(args.model_recover_path).exists(", "\n", "# ), \"--model_recover_path doesn't exist\"", "\n", "\n", "", "args", ".", "output_dir", "=", "args", ".", "output_dir", ".", "replace", "(", "\n", "'[PT_OUTPUT_DIR]'", ",", "os", ".", "getenv", "(", "'PT_OUTPUT_DIR'", ",", "''", ")", ")", "\n", "args", ".", "log_dir", "=", "args", ".", "log_dir", ".", "replace", "(", "\n", "'[PT_OUTPUT_DIR]'", ",", "os", ".", "getenv", "(", "'PT_OUTPUT_DIR'", ",", "''", ")", ")", "\n", "\n", "os", ".", "makedirs", "(", "args", ".", "output_dir", ",", "exist_ok", "=", "True", ")", "\n", "os", ".", "makedirs", "(", "args", ".", "log_dir", ",", "exist_ok", "=", "True", ")", "\n", "json", ".", "dump", "(", "args", ".", "__dict__", ",", "open", "(", "os", ".", "path", ".", "join", "(", "\n", "args", ".", "output_dir", ",", "'opt.json'", ")", ",", "'w'", ")", ",", "sort_keys", "=", "True", ",", "indent", "=", "2", ")", "\n", "\n", "if", "args", ".", "local_rank", "==", "-", "1", "or", "args", ".", "no_cuda", ":", "\n", "        ", "device", "=", "torch", ".", "device", "(", "\n", "\"cuda\"", "if", "torch", ".", "cuda", ".", "is_available", "(", ")", "and", "not", "args", ".", "no_cuda", "else", "\"cpu\"", ")", "\n", "n_gpu", "=", "torch", ".", "cuda", ".", "device_count", "(", ")", "\n", "", "else", ":", "\n", "        ", "torch", ".", "cuda", ".", "set_device", "(", "args", ".", "local_rank", ")", "\n", "device", "=", "torch", ".", "device", "(", "\"cuda\"", ",", "args", ".", "local_rank", ")", "\n", "n_gpu", "=", "1", "\n", "# Initializes the distributed backend which will take care of sychronizing nodes/GPUs", "\n", "dist", ".", "init_process_group", "(", "backend", "=", "'nccl'", ")", "\n", "", "logger", ".", "info", "(", "\"device: {} n_gpu: {}, distributed training: {}, 16-bits training: {}\"", ".", "format", "(", "\n", "device", ",", "n_gpu", ",", "bool", "(", "args", ".", "local_rank", "!=", "-", "1", ")", ",", "args", ".", "fp16", ")", ")", "\n", "\n", "if", "args", ".", "gradient_accumulation_steps", "<", "1", ":", "\n", "        ", "raise", "ValueError", "(", "\"Invalid gradient_accumulation_steps parameter: {}, should be >= 1\"", ".", "format", "(", "\n", "args", ".", "gradient_accumulation_steps", ")", ")", "\n", "\n", "", "args", ".", "train_batch_size", "=", "int", "(", "\n", "args", ".", "train_batch_size", "/", "args", ".", "gradient_accumulation_steps", ")", "\n", "\n", "random", ".", "seed", "(", "args", ".", "seed", ")", "\n", "np", ".", "random", ".", "seed", "(", "args", ".", "seed", ")", "\n", "torch", ".", "manual_seed", "(", "args", ".", "seed", ")", "\n", "if", "n_gpu", ">", "0", ":", "\n", "        ", "torch", ".", "cuda", ".", "manual_seed_all", "(", "args", ".", "seed", ")", "\n", "\n", "", "if", "not", "args", ".", "do_train", "and", "not", "args", ".", "do_eval", ":", "\n", "        ", "raise", "ValueError", "(", "\n", "\"At least one of `do_train` or `do_eval` must be True.\"", ")", "\n", "\n", "", "if", "args", ".", "local_rank", "not", "in", "(", "-", "1", ",", "0", ")", ":", "\n", "# Make sure only the first process in distributed training will download model & vocab", "\n", "        ", "dist", ".", "barrier", "(", ")", "\n", "", "tokenizer", "=", "BertTokenizer", ".", "from_pretrained", "(", "\n", "args", ".", "vocab_path", ",", "do_lower_case", "=", "args", ".", "do_lower_case", ")", "\n", "if", "args", ".", "max_position_embeddings", ":", "\n", "        ", "tokenizer", ".", "max_len", "=", "args", ".", "max_position_embeddings", "\n", "", "data_tokenizer", "=", "WhitespaceTokenizer", "(", ")", "if", "args", ".", "tokenized_input", "else", "tokenizer", "\n", "if", "args", ".", "local_rank", "==", "0", ":", "\n", "        ", "dist", ".", "barrier", "(", ")", "\n", "\n", "", "if", "args", ".", "do_train", ":", "\n", "        ", "print", "(", "\"Loading Train Dataset\"", ",", "args", ".", "data_dir", ")", "\n", "bi_uni_pipeline", "=", "[", "seq2seq_loader", ".", "Preprocess4Seq2seq", "(", "args", ".", "max_pred", ",", "args", ".", "mask_prob", ",", "list", "(", "tokenizer", ".", "vocab", ".", "keys", "(", "\n", ")", ")", ",", "tokenizer", ".", "convert_tokens_to_ids", ",", "args", ".", "max_seq_length", ",", "new_segment_ids", "=", "args", ".", "new_segment_ids", ",", "truncate_config", "=", "{", "'max_len_a'", ":", "args", ".", "max_len_a", ",", "'max_len_b'", ":", "args", ".", "max_len_b", ",", "'trunc_seg'", ":", "args", ".", "trunc_seg", ",", "'always_truncate_tail'", ":", "args", ".", "always_truncate_tail", "}", ",", "mask_source_words", "=", "args", ".", "mask_source_words", ",", "skipgram_prb", "=", "args", ".", "skipgram_prb", ",", "skipgram_size", "=", "args", ".", "skipgram_size", ",", "mask_whole_word", "=", "args", ".", "mask_whole_word", ",", "mode", "=", "\"s2s\"", ",", "has_oracle", "=", "args", ".", "has_sentence_oracle", ",", "num_qkv", "=", "args", ".", "num_qkv", ",", "s2s_special_token", "=", "args", ".", "s2s_special_token", ",", "s2s_add_segment", "=", "args", ".", "s2s_add_segment", ",", "s2s_share_segment", "=", "args", ".", "s2s_share_segment", ",", "pos_shift", "=", "args", ".", "pos_shift", ")", "]", "\n", "file_oracle", "=", "None", "\n", "if", "args", ".", "has_sentence_oracle", ":", "\n", "            ", "file_oracle", "=", "os", ".", "path", ".", "join", "(", "args", ".", "data_dir", ",", "'train.oracle'", ")", "\n", "", "fn_src", "=", "os", ".", "path", ".", "join", "(", "\n", "args", ".", "data_dir", ",", "args", ".", "src_file", "if", "args", ".", "src_file", "else", "'train.src'", ")", "\n", "fn_tgt", "=", "os", ".", "path", ".", "join", "(", "\n", "args", ".", "data_dir", ",", "args", ".", "tgt_file", "if", "args", ".", "tgt_file", "else", "'train.tgt'", ")", "\n", "train_dataset", "=", "seq2seq_loader", ".", "Seq2SeqDataset", "(", "\n", "fn_src", ",", "fn_tgt", ",", "args", ".", "train_batch_size", ",", "data_tokenizer", ",", "args", ".", "max_seq_length", ",", "file_oracle", "=", "file_oracle", ",", "bi_uni_pipeline", "=", "bi_uni_pipeline", ")", "\n", "if", "args", ".", "local_rank", "==", "-", "1", ":", "\n", "            ", "train_sampler", "=", "RandomSampler", "(", "train_dataset", ",", "replacement", "=", "False", ")", "\n", "_batch_size", "=", "args", ".", "train_batch_size", "\n", "", "else", ":", "\n", "            ", "train_sampler", "=", "DistributedSampler", "(", "train_dataset", ")", "\n", "_batch_size", "=", "args", ".", "train_batch_size", "//", "dist", ".", "get_world_size", "(", ")", "\n", "", "train_dataloader", "=", "torch", ".", "utils", ".", "data", ".", "DataLoader", "(", "train_dataset", ",", "batch_size", "=", "_batch_size", ",", "sampler", "=", "train_sampler", ",", "\n", "num_workers", "=", "args", ".", "num_workers", ",", "collate_fn", "=", "seq2seq_loader", ".", "batch_list_to_batch_tensors", ",", "pin_memory", "=", "False", ")", "\n", "\n", "# note: args.train_batch_size has been changed to (/= args.gradient_accumulation_steps)", "\n", "# t_total = int(math.ceil(len(train_dataset.ex_list) / args.train_batch_size)", "\n", "", "t_total", "=", "int", "(", "len", "(", "train_dataloader", ")", "*", "args", ".", "num_train_epochs", "/", "\n", "args", ".", "gradient_accumulation_steps", ")", "\n", "\n", "# amp_handle = None", "\n", "# if args.fp16 and args.amp:", "\n", "#     from apex import amp", "\n", "#     amp_handle = amp.init(enable_caching=True)", "\n", "#     logger.info(\"enable fp16 with amp\")", "\n", "\n", "# Prepare model", "\n", "# recover_step = _get_max_epoch_model(args.output_dir)", "\n", "recover_step", "=", "args", ".", "recover_step", "\n", "cls_num_labels", "=", "2", "\n", "type_vocab_size", "=", "6", "+", "(", "1", "if", "args", ".", "s2s_add_segment", "else", "0", ")", "if", "args", ".", "new_segment_ids", "else", "2", "\n", "num_sentlvl_labels", "=", "2", "if", "args", ".", "has_sentence_oracle", "else", "0", "\n", "relax_projection", "=", "4", "if", "args", ".", "relax_projection", "else", "0", "\n", "if", "args", ".", "local_rank", "not", "in", "(", "-", "1", ",", "0", ")", ":", "\n", "# Make sure only the first process in distributed training will download model & vocab", "\n", "        ", "dist", ".", "barrier", "(", ")", "\n", "", "if", "(", "recover_step", "is", "None", ")", "and", "(", "args", ".", "model_recover_path", "is", "None", ")", ":", "\n", "# if _state_dict == {}, the parameters are randomly initialized", "\n", "# if _state_dict == None, the parameters are initialized with bert-init", "\n", "        ", "_state_dict", "=", "{", "}", "if", "args", ".", "from_scratch", "else", "None", "\n", "model", "=", "BertForPreTrainingLossMask", ".", "from_pretrained", "(", "\n", "args", ".", "bert_model", ",", "state_dict", "=", "_state_dict", ",", "num_labels", "=", "cls_num_labels", ",", "num_rel", "=", "0", ",", "type_vocab_size", "=", "type_vocab_size", ",", "config_path", "=", "args", ".", "config_path", ",", "task_idx", "=", "3", ",", "num_sentlvl_labels", "=", "num_sentlvl_labels", ",", "max_position_embeddings", "=", "args", ".", "max_position_embeddings", ",", "label_smoothing", "=", "args", ".", "label_smoothing", ",", "fp32_embedding", "=", "args", ".", "fp32_embedding", ",", "relax_projection", "=", "relax_projection", ",", "new_pos_ids", "=", "args", ".", "new_pos_ids", ",", "ffn_type", "=", "args", ".", "ffn_type", ",", "hidden_dropout_prob", "=", "args", ".", "hidden_dropout_prob", ",", "attention_probs_dropout_prob", "=", "args", ".", "attention_probs_dropout_prob", ",", "num_qkv", "=", "args", ".", "num_qkv", ",", "seg_emb", "=", "args", ".", "seg_emb", ")", "\n", "global_step", "=", "0", "\n", "", "else", ":", "\n", "        ", "if", "recover_step", ":", "\n", "            ", "logger", ".", "info", "(", "\"***** Recover model: %d *****\"", ",", "recover_step", ")", "\n", "model_recover", "=", "torch", ".", "load", "(", "os", ".", "path", ".", "join", "(", "\n", "args", ".", "output_dir", ",", "\"model.{0}.bin\"", ".", "format", "(", "recover_step", ")", ")", ",", "map_location", "=", "'cpu'", ")", "\n", "# recover_step == number of epochs", "\n", "global_step", "=", "math", ".", "floor", "(", "\n", "recover_step", "*", "t_total", "/", "args", ".", "num_train_epochs", ")", "\n", "", "elif", "args", ".", "model_recover_path", ":", "\n", "            ", "logger", ".", "info", "(", "\"***** Recover model: %s *****\"", ",", "\n", "args", ".", "model_recover_path", ")", "\n", "model_recover", "=", "torch", ".", "load", "(", "\n", "args", ".", "model_recover_path", ",", "map_location", "=", "'cpu'", ")", "\n", "global_step", "=", "0", "\n", "", "model", "=", "BertForPreTrainingLossMask", ".", "from_pretrained", "(", "\n", "args", ".", "bert_model", ",", "state_dict", "=", "model_recover", ",", "num_labels", "=", "cls_num_labels", ",", "num_rel", "=", "0", ",", "type_vocab_size", "=", "type_vocab_size", ",", "config_path", "=", "args", ".", "config_path", ",", "task_idx", "=", "3", ",", "num_sentlvl_labels", "=", "num_sentlvl_labels", ",", "max_position_embeddings", "=", "args", ".", "max_position_embeddings", ",", "label_smoothing", "=", "args", ".", "label_smoothing", ",", "fp32_embedding", "=", "args", ".", "fp32_embedding", ",", "relax_projection", "=", "relax_projection", ",", "new_pos_ids", "=", "args", ".", "new_pos_ids", ",", "ffn_type", "=", "args", ".", "ffn_type", ",", "hidden_dropout_prob", "=", "args", ".", "hidden_dropout_prob", ",", "attention_probs_dropout_prob", "=", "args", ".", "attention_probs_dropout_prob", ",", "num_qkv", "=", "args", ".", "num_qkv", ",", "seg_emb", "=", "args", ".", "seg_emb", ")", "\n", "", "if", "args", ".", "local_rank", "==", "0", ":", "\n", "        ", "dist", ".", "barrier", "(", ")", "\n", "\n", "", "if", "args", ".", "fp16", ":", "\n", "# model.half()", "\n", "        ", "if", "args", ".", "fp32_embedding", ":", "\n", "            ", "model", ".", "bert", ".", "embeddings", ".", "word_embeddings", ".", "float", "(", ")", "\n", "model", ".", "bert", ".", "embeddings", ".", "position_embeddings", ".", "float", "(", ")", "\n", "model", ".", "bert", ".", "embeddings", ".", "token_type_embeddings", ".", "float", "(", ")", "\n", "", "", "model", ".", "to", "(", "device", ")", "\n", "\n", "\n", "if", "args", ".", "local_rank", "!=", "-", "1", ":", "\n", "        ", "try", ":", "\n", "            ", "from", "torch", ".", "nn", ".", "parallel", "import", "DistributedDataParallel", "as", "DDP", "\n", "", "except", "ImportError", ":", "\n", "            ", "raise", "ImportError", "(", "\"DistributedDataParallel\"", ")", "\n", "", "model", "=", "DDP", "(", "model", ",", "device_ids", "=", "[", "\n", "args", ".", "local_rank", "]", ",", "output_device", "=", "args", ".", "local_rank", ",", "find_unused_parameters", "=", "True", ")", "\n", "", "elif", "n_gpu", ">", "1", ":", "\n", "# model = torch.nn.DataParallel(model)", "\n", "        ", "model", "=", "DataParallelImbalance", "(", "model", ")", "\n", "\n", "", "torch", ".", "cuda", ".", "empty_cache", "(", ")", "\n", "\n", "if", "args", ".", "do_train", ":", "\n", "\n", "        ", "model", ".", "eval", "(", ")", "\n", "if", "recover_step", ":", "\n", "            ", "start_epoch", "=", "recover_step", "+", "1", "\n", "", "else", ":", "\n", "            ", "start_epoch", "=", "1", "\n", "", "total_loss", "=", "0", "\n", "for", "i_epoch", "in", "trange", "(", "start_epoch", ",", "49", ",", "desc", "=", "\"Epoch\"", ",", "disable", "=", "args", ".", "local_rank", "not", "in", "(", "-", "1", ",", "0", ")", ")", ":", "\n", "            ", "if", "args", ".", "local_rank", "!=", "-", "1", ":", "\n", "                ", "train_sampler", ".", "set_epoch", "(", "i_epoch", ")", "\n", "", "iter_bar", "=", "tqdm", "(", "train_dataloader", ",", "desc", "=", "'Iter (loss=X.XXX)'", ",", "\n", "disable", "=", "args", ".", "local_rank", "not", "in", "(", "-", "1", ",", "0", ")", ")", "\n", "for", "step", ",", "batch", "in", "enumerate", "(", "iter_bar", ")", ":", "\n", "                ", "batch", "=", "[", "\n", "t", ".", "to", "(", "device", ")", "if", "t", "is", "not", "None", "else", "None", "for", "t", "in", "batch", "]", "\n", "if", "args", ".", "has_sentence_oracle", ":", "\n", "                    ", "input_ids", ",", "segment_ids", ",", "input_mask", ",", "mask_qkv", ",", "lm_label_ids", ",", "masked_pos", ",", "masked_weights", ",", "is_next", ",", "task_idx", ",", "oracle_pos", ",", "oracle_weights", ",", "oracle_labels", "=", "batch", "\n", "", "else", ":", "\n", "                    ", "input_ids", ",", "segment_ids", ",", "input_mask", ",", "mask_qkv", ",", "lm_label_ids", ",", "masked_pos", ",", "masked_weights", ",", "is_next", ",", "task_idx", "=", "batch", "\n", "oracle_pos", ",", "oracle_weights", ",", "oracle_labels", "=", "None", ",", "None", ",", "None", "\n", "", "loss_tuple", "=", "model", "(", "input_ids", ",", "segment_ids", ",", "input_mask", ",", "lm_label_ids", ",", "is_next", ",", "masked_pos", "=", "masked_pos", ",", "masked_weights", "=", "masked_weights", ",", "task_idx", "=", "task_idx", ",", "masked_pos_2", "=", "oracle_pos", ",", "masked_weights_2", "=", "oracle_weights", ",", "\n", "masked_labels_2", "=", "oracle_labels", ",", "mask_qkv", "=", "mask_qkv", ")", "\n", "masked_lm_loss", ",", "next_sentence_loss", "=", "loss_tuple", "\n", "if", "n_gpu", ">", "1", ":", "# mean() to average on multi-gpu.", "\n", "# loss = loss.mean()", "\n", "                    ", "masked_lm_loss", "=", "masked_lm_loss", ".", "mean", "(", ")", "\n", "next_sentence_loss", "=", "next_sentence_loss", ".", "mean", "(", ")", "\n", "", "loss", "=", "masked_lm_loss", "+", "next_sentence_loss", "\n", "\n", "# logging for each step (i.e., before normalization by args.gradient_accumulation_steps)", "\n", "iter_bar", ".", "set_description", "(", "'Iter (loss=%5.3f)'", "%", "loss", ".", "item", "(", ")", ")", "\n", "\n", "total_loss", "+=", "loss", ".", "item", "(", ")", "\n", "", "break", "\n", "", "print", "(", "total_loss", "/", "len", "(", "train_dataloader", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.topic_modeling.classify_1343.check_multi": [[18, 32], ["None"], "function", ["None"], ["def", "check_multi", "(", "tokens", ")", ":", "\n", "    ", "topic_words", "=", "[", "[", "'\u8eab\u6750'", ",", "'\u5bbd\u677e'", ",", "'\u6bd4\u4f8b'", ",", "'\u4fee\u8eab'", ",", "'\u7248\u578b'", "]", ",", "\n", "[", "'\u767e\u642d'", ",", "'\u642d'", ",", "'\u7a7f\u642d'", ",", "'\u642d\u914d'", "]", ",", "\n", "[", "'\u9762\u6599'", ",", "'\u5e03\u6599'", ",", "'\u7eaf\u68c9'", ",", "'\u4eb2\u80a4'", ",", "'\u6750\u8d28'", ",", "'\u624b\u611f'", ",", "'\u900f\u6c14\u6027'", ",", "'\u7f8a\u6bdb'", ",", "'\u6bdb\u5462'", ",", "'\u9e2d\u7ed2'", ",", "'\u4fdd\u6696'", ",", "'\u7fbd\u7ed2'", ",", "'\u9e45\u7ed2'", ",", "'\u586b\u5145'", "]", ",", "\n", "[", "'\u683c\u7eb9'", ",", "'\u5343\u9e1f\u683c'", ",", "'\u8272'", ",", "'\u989c\u8272'", ",", "'\u6761\u7eb9'", ",", "'\u56fe\u6848'", ",", "'\u5370\u82b1'", ",", "'\u649e\u8272'", ",", "'\u5b57\u6bcd'", ",", "'\u7ad6\u7eb9'", "]", ",", "\n", "[", "'\u7acb\u9886'", ",", "'\u5706\u9886'", ",", "'\u8863\u9886'", ",", "'\u9886'", ",", "'\u7ffb\u9886'", ",", "'\u9886\u53e3'", ",", "'\u5b57\u9886'", ",", "'\u9888\u90e8'", ",", "'\u9ad8\u9886'", ",", "'\u9888'", ",", "'\u5c16\u9886'", ",", "'\u8fde\u5e3d'", ",", "'\u5e3d'", "]", ",", "\n", "[", "'\u53e3\u888b'", ",", "'\u63d2\u624b'", ",", "'\u63d2\u888b'", ",", "'\u5f00\u888b'", ",", "'\u8d34\u888b'", ",", "'\u63d2\u515c'", ",", "'\u7269\u54c1'", "]", "]", "\n", "count", "=", "0", "\n", "for", "topic", "in", "topic_words", ":", "\n", "        ", "for", "word", "in", "tokens", ":", "\n", "            ", "if", "word", "in", "topic", ":", "\n", "                ", "count", "+=", "1", "\n", "break", "\n", "", "", "", "return", "count", "\n", "\n"]], "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.topic_modeling.classify_1343.class_by_keyword": [[34, 56], ["len", "len", "set", "set", "len", "set", "set", "len", "set", "set", "len", "set", "set", "len", "set", "set", "len", "set", "set", "len", "set", "set", "len", "set", "set", "set", "set"], "function", ["None"], ["", "def", "class_by_keyword", "(", "tokens", ")", ":", "\n", "    ", "if", "len", "(", "set", "(", "[", "'\u8eab\u6750'", ",", "'\u6bd4\u4f8b'", ",", "'\u4fee\u8eab'", ",", "'\u5bbd\u677e'", ",", "'\u7248\u578b'", "]", ")", "&", "set", "(", "tokens", ")", ")", ">", "0", ":", "\n", "        ", "a", "=", "0", "\n", "", "elif", "len", "(", "set", "(", "[", "'\u767e\u642d'", ",", "'\u914d\u642d'", ",", "'\u642d'", ",", "'\u7a7f\u642d'", "]", ")", "&", "set", "(", "tokens", ")", ")", ">", "0", ":", "\n", "        ", "a", "=", "1", "\n", "", "elif", "len", "(", "set", "(", "[", "'\u683c\u7eb9'", ",", "'\u5343\u9e1f\u683c'", ",", "'\u8272'", ",", "'\u989c\u8272'", ",", "'\u6761\u7eb9'", ",", "'\u56fe\u6848'", ",", "'\u5370\u82b1'", ",", "'\u649e\u8272'", ",", "'\u5b57\u6bcd'", ",", "'\u7ad6\u7eb9'", "]", ")", "&", "set", "(", "tokens", ")", ")", ">", "0", ":", "\n", "        ", "a", "=", "2", "\n", "", "elif", "len", "(", "set", "(", "[", "'\u9762\u6599'", ",", "'\u5e03\u6599'", ",", "'\u7eaf\u68c9'", ",", "'\u4eb2\u80a4'", ",", "'\u6750\u8d28'", ",", "'\u624b\u611f'", ",", "'\u900f\u6c14\u6027'", ",", "'\u7f8a\u6bdb'", ",", "'\u6bdb\u5462'", ",", "'\u9e2d\u7ed2'", ",", "'\u4fdd\u6696'", ",", "'\u7fbd\u7ed2'", ",", "'\u9e45\u7ed2'", ",", "'\u586b\u5145'", "]", ")", "&", "set", "(", "tokens", ")", ")", ">", "0", ":", "\n", "        ", "a", "=", "3", "\n", "", "elif", "len", "(", "set", "(", "[", "'\u53e3\u888b'", ",", "'\u63d2\u624b'", ",", "'\u63d2\u888b'", ",", "'\u5f00\u888b'", ",", "'\u8d34\u888b'", ",", "'\u63d2\u515c'", ",", "'\u7269\u54c1'", "]", ")", "&", "set", "(", "tokens", ")", ")", ">", "0", ":", "\n", "        ", "a", "=", "4", "\n", "", "elif", "len", "(", "set", "(", "[", "'\u7acb\u9886'", ",", "'\u5706\u9886'", ",", "'\u8863\u9886'", ",", "'\u9886'", ",", "'\u7ffb\u9886'", ",", "'\u9886\u53e3'", ",", "'\u5b57\u9886'", ",", "'\u9888\u90e8'", ",", "'\u9ad8\u9886'", ",", "'\u9888'", ",", "'\u5c16\u9886'", ",", "'\u8fde\u5e3d'", ",", "'\u5e3d'", "]", ")", "&", "set", "(", "tokens", ")", ")", ">", "0", ":", "\n", "        ", "a", "=", "5", "\n", "", "elif", "len", "(", "set", "(", "[", "'\u88d9\u6446'", ",", "'\u6446'", ",", "'\u767e\u8936'", ",", "'\u8377\u53f6'", ",", "'\u86cb\u7cd5'", ",", "'\u9c7c\u5c3e'", ",", "'\u5f00\u53c9'", ",", "'\u4e0b\u6446'", "]", ")", "&", "set", "(", "tokens", ")", ")", ">", "0", ":", "\n", "        ", "a", "=", "6", "\n", "", "elif", "len", "(", "set", "(", "[", "'\u8896\u53e3'", ",", "'\u8896'", "]", ")", "&", "set", "(", "tokens", ")", ")", ">", "0", ":", "\n", "        ", "a", "=", "7", "\n", "", "elif", "len", "(", "set", "(", "[", "'\u751c\u7f8e'", ",", "'\u4f18\u96c5'", ",", "'\u77e5\u6027'", ",", "'\u590d\u53e4'", ",", "'\u65f6\u5c1a'", ",", "'\u4fcf\u76ae'", ",", "'\u5c11\u5973'", ",", "'\u4e2a\u6027'", ",", "'\u65f6\u9ae6'", ",", "'\u6027\u611f'", ",", "'\u4f11\u95f2'", ",", "'\u6f6e\u9177'", "]", ")", "&", "set", "(", "tokens", ")", ")", ">", "0", ":", "\n", "        ", "a", "=", "8", "\n", "", "else", ":", "\n", "        ", "a", "=", "-", "1", "\n", "", "return", "a", "\n", "\n"]], "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.topic_modeling.classify_653.check_multi": [[18, 33], ["None"], "function", ["None"], ["def", "check_multi", "(", "tokens", ")", ":", "\n", "    ", "topic_words", "=", "[", "[", "'\u62cd\u6444'", ",", "'\u955c\u5934'", ",", "'\u540e\u7f6e'", ",", "'\u62cd\u7167'", ",", "'\u4e09\u6444'", ",", "'\u56db\u6444'", ",", "'\u4eba\u50cf'", ",", "'\u5355\u6444'", ",", "'\u6563\u70ed'", ",", "'\u6db2\u51b7'", "]", ",", "\n", "[", "'\u8fd0\u884c'", ",", "'\u9e92\u9e9f'", ",", "'\u5904\u7406\u5668'", ",", "'\u9a81\u9f99'", ",", "'\u9ad8\u901a'", ",", "'\u5927\u5bb9\u91cf'", "]", ",", "\n", "[", "'\u7535\u6c60'", ",", "'\u7eed\u822a'", ",", "'\u6301\u4e45'", ",", "'\u5feb\u5145'", ",", "'\u7535\u91cf'", ",", "'\u5145\u7535'", ",", "'\u95ea\u5145'", "]", ",", "\n", "[", "'\u89e3\u9501'", ",", "'\u6307\u7eb9'", ",", "'\u9690\u79c1'", "]", ",", "\n", "[", "'\u5c4f'", ",", "'\u89c6\u91ce'", ",", "'\u82f1\u5bf8'", ",", "'\u5c4f\u5e55'", ",", "'\u5168\u9762'", ",", "'\u663e\u793a\u5c4f'", ",", "'\u663e\u793a'", "]", ",", "\n", "[", "'5G'", ",", "'\u53cc\u6a21'", ",", "'\u53cc\u5361'", "]", ",", "\n", "[", "'\u6750\u6599'", ",", "'\u9632\u6c34'", ",", "'\u8fdb\u6c34'", ",", "'\u5916\u89c2'", ",", "'\u673a\u8eab'", "]", "]", "\n", "count", "=", "0", "\n", "for", "topic", "in", "topic_words", ":", "\n", "        ", "for", "word", "in", "tokens", ":", "\n", "            ", "if", "word", "in", "topic", ":", "\n", "                ", "count", "+=", "1", "\n", "break", "\n", "", "", "", "return", "count", "\n", "\n"]], "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.topic_modeling.classify_653.class_by_keyword": [[35, 53], ["len", "len", "set", "set", "len", "set", "set", "len", "set", "set", "len", "set", "set", "len", "set", "set", "len", "set", "set", "set", "set"], "function", ["None"], ["", "def", "class_by_keyword", "(", "tokens", ")", ":", "\n", "    ", "if", "len", "(", "set", "(", "[", "'\u673a\u8eab'", ",", "'\u5916\u89c2'", ",", "'\u5916\u58f3'", "]", ")", "&", "set", "(", "tokens", ")", ")", ">", "0", ":", "\n", "        ", "a", "=", "0", "\n", "", "elif", "len", "(", "set", "(", "[", "'\u5c4f'", ",", "'\u5c4f\u5e55'", ",", "'\u82f1\u5bf8'", "]", ")", "&", "set", "(", "tokens", ")", ")", ">", "0", ":", "\n", "        ", "a", "=", "1", "\n", "", "elif", "len", "(", "set", "(", "[", "'\u7f51\u7edc'", ",", "'5G'", ",", "'\u53cc\u6a21'", "]", ")", "&", "set", "(", "tokens", ")", ")", ">", "0", ":", "\n", "        ", "a", "=", "2", "\n", "", "elif", "len", "(", "set", "(", "[", "'\u6444\u50cf\u5934'", ",", "'\u4e09\u6444'", ",", "'\u56db\u6444'", ",", "'\u5e7f\u89d2'", ",", "'\u53d8\u7126'", ",", "'\u957f\u7126'", ",", "'\u955c\u5934'", ",", "'\u7f8e\u989c'", ",", "'\u62cd\u6444'", ",", "'\u955c\u5934'", ",", "'\u540e\u7f6e'", ",", "'\u62cd\u7167'", ",", "'\u4eba\u50cf'", ",", "'\u5355\u6444'", "]", ")", "&", "set", "(", "tokens", ")", ")", ">", "0", ":", "\n", "        ", "a", "=", "3", "\n", "", "elif", "len", "(", "set", "(", "[", "'\u5904\u7406\u5668'", ",", "'\u5b58\u50a8'", ",", "'\u6db2\u51b7'", ",", "'\u6563\u70ed'", ",", "'\u5185\u5b58'", ",", "'\u82af\u7247'", "]", ")", "&", "set", "(", "tokens", ")", ")", ">", "0", ":", "\n", "        ", "a", "=", "4", "\n", "", "elif", "len", "(", "set", "(", "[", "'\u5145\u7535'", ",", "'\u7eed\u822a'", ",", "'\u5feb\u5145'", ",", "'\u7535\u6c60'", "]", ")", "&", "set", "(", "tokens", ")", ")", ">", "0", ":", "\n", "        ", "a", "=", "5", "\n", "", "elif", "len", "(", "set", "(", "[", "'\u9762\u5bb9'", ",", "'\u89e3\u9501'", ",", "'\u6307\u7eb9'", ",", "'\u4eba\u8138\u8bc6\u522b'", ",", "'\u9762\u5bb9ID'", "]", ")", "&", "set", "(", "tokens", ")", ")", ">", "0", ":", "\n", "        ", "a", "=", "6", "\n", "", "else", ":", "\n", "        ", "a", "=", "-", "1", "\n", "", "return", "a", "\n", "\n"]], "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.topic_modeling.classify_1342.check_multi": [[18, 33], ["None"], "function", ["None"], ["def", "check_multi", "(", "tokens", ")", ":", "\n", "    ", "topic_words", "=", "[", "[", "'\u8eab\u6750'", ",", "'\u5bbd\u677e'", ",", "'\u6bd4\u4f8b'", ",", "'\u4fee\u8eab'", ",", "'\u7248\u578b'", ",", "'\u526a\u88c1'", "]", ",", "\n", "[", "'\u767e\u642d'", ",", "'\u642d'", ",", "'\u7a7f\u642d'", ",", "'\u642d\u914d'", "]", ",", "\n", "[", "'\u9762\u6599'", ",", "'\u5e03\u6599'", ",", "'\u7eaf\u68c9'", ",", "'\u4eb2\u80a4'", ",", "'\u6750\u8d28'", ",", "'\u624b\u611f'", ",", "'\u900f\u6c14\u6027'", ",", "'\u7f8a\u6bdb'", ",", "'\u6bdb\u5462'", ",", "'\u9e2d\u7ed2'", ",", "'\u4fdd\u6696'", ",", "'\u7fbd\u7ed2'", ",", "'\u9e45\u7ed2'", ",", "'\u586b\u5145'", "]", ",", "\n", "[", "'\u683c\u7eb9'", ",", "'\u5343\u9e1f\u683c'", ",", "'\u8272'", ",", "'\u989c\u8272'", ",", "'\u6761\u7eb9'", ",", "'\u56fe\u6848'", ",", "'\u5370\u82b1'", ",", "'\u649e\u8272'", ",", "'\u5b57\u6bcd'", ",", "'\u7ad6\u7eb9'", ",", "'\u523a\u7ee3'", "]", ",", "\n", "[", "'\u7acb\u9886'", ",", "'\u5706\u9886'", ",", "'\u8863\u9886'", ",", "'\u9886'", ",", "'\u7ffb\u9886'", ",", "'\u9886\u53e3'", ",", "'\u5b57\u9886'", ",", "'\u9888\u90e8'", ",", "'\u9ad8\u9886'", ",", "'\u9888'", ",", "'\u5c16\u9886'", ",", "'\u8fde\u5e3d'", ",", "'\u5e3d'", "]", ",", "\n", "[", "'\u53e3\u888b'", ",", "'\u63d2\u624b'", ",", "'\u63d2\u888b'", ",", "'\u5f00\u888b'", ",", "'\u8d34\u888b'", ",", "'\u63d2\u515c'", ",", "'\u7269\u54c1'", ",", "'\u888b'", "]", ",", "\n", "[", "'\u8896'", ",", "'\u8896\u53e3'", "]", "]", "\n", "count", "=", "0", "\n", "for", "topic", "in", "topic_words", ":", "\n", "        ", "for", "word", "in", "tokens", ":", "\n", "            ", "if", "word", "in", "topic", ":", "\n", "                ", "count", "+=", "1", "\n", "break", "\n", "", "", "", "return", "count", "\n", "\n"]], "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.topic_modeling.classify_1342.class_by_keyword": [[35, 56], ["len", "len", "set", "set", "len", "set", "set", "len", "set", "set", "len", "set", "set", "len", "set", "set", "len", "set", "set", "len", "set", "set", "set", "set"], "function", ["None"], ["", "def", "class_by_keyword", "(", "tokens", ")", ":", "\n", "    ", "if", "len", "(", "set", "(", "[", "'\u9762\u6599'", ",", "'\u5e03\u6599'", ",", "'\u52a0\u7ed2'", ",", "'\u7eaf\u68c9'", ",", "'\u4eb2\u80a4'", ",", "'\u6750\u8d28'", ",", "'\u624b\u611f'", ",", "'\u900f\u6c14\u6027'", ",", "'\u7f8a\u6bdb'", ",", "'\u6bdb\u5462'", ",", "'\u9e2d\u7ed2'", ",", "'\u4fdd\u6696'", ",", "'\u7fbd\u7ed2'", ",", "'\u9e45\u7ed2'", ",", "'\u586b\u5145'", ",", "\n", "'\u6da4\u7eb6'", "]", ")", "&", "set", "(", "tokens", ")", ")", ">", "0", ":", "\n", "        ", "a", "=", "0", "\n", "", "elif", "len", "(", "set", "(", "[", "'\u8eab\u6750'", ",", "'\u6bd4\u4f8b'", ",", "'\u4fee\u8eab'", ",", "'\u5bbd\u677e'", ",", "'\u7248\u578b'", "]", ")", "&", "set", "(", "tokens", ")", ")", ">", "0", ":", "\n", "        ", "a", "=", "1", "\n", "", "elif", "len", "(", "set", "(", "[", "'\u683c\u7eb9'", ",", "'\u5343\u9e1f\u683c'", ",", "'\u8272'", ",", "'\u989c\u8272'", ",", "'\u6761\u7eb9'", ",", "'\u56fe\u6848'", ",", "'\u5370\u82b1'", ",", "'\u649e\u8272'", ",", "'\u5b57\u6bcd'", ",", "'\u7ad6\u7eb9'", ",", "'\u523a\u7ee3'", "]", ")", "&", "set", "(", "tokens", ")", ")", ">", "0", ":", "\n", "        ", "a", "=", "2", "\n", "", "elif", "len", "(", "set", "(", "[", "'\u53e3\u888b'", ",", "'\u63d2\u624b'", ",", "'\u63d2\u888b'", ",", "'\u5f00\u888b'", ",", "'\u8d34\u888b'", ",", "'\u63d2\u515c'", ",", "'\u7269\u54c1'", "]", ")", "&", "set", "(", "tokens", ")", ")", ">", "0", ":", "\n", "        ", "a", "=", "3", "\n", "", "elif", "len", "(", "set", "(", "[", "'\u7acb\u9886'", ",", "'\u5706\u9886'", ",", "'\u8863\u9886'", ",", "'\u9886'", ",", "'\u7ffb\u9886'", ",", "'\u9886\u53e3'", ",", "'\u5b57\u9886'", ",", "'\u9888\u90e8'", ",", "'\u9ad8\u9886'", ",", "'\u9888'", ",", "'\u5c16\u9886'", ",", "'\u8fde\u5e3d'", ",", "'\u5e3d'", "]", ")", "&", "set", "(", "tokens", ")", ")", ">", "0", ":", "\n", "        ", "a", "=", "4", "\n", "", "elif", "len", "(", "set", "(", "[", "'\u8896\u53e3'", ",", "'\u8896'", "]", ")", "&", "set", "(", "tokens", ")", ")", ">", "0", ":", "\n", "        ", "a", "=", "5", "\n", "", "elif", "len", "(", "set", "(", "[", "'\u65f6\u5c1a'", ",", "'\u590d\u53e4'", ",", "'\u6f6e'", ",", "'\u4e2a\u6027'", ",", "'\u5e05\u6c14'", ",", "'\u4e0d\u7f81'", ",", "'\u6f6e\u6d41'", ",", "'\u65f6\u9ae6'", ",", "'\u6027\u611f'", ",", "'\u4f11\u95f2'", ",", "'\u6f6e\u9177'", "]", ")", "&", "set", "(", "tokens", ")", ")", ">", "0", ":", "\n", "        ", "a", "=", "6", "\n", "", "elif", "len", "(", "set", "(", "[", "'\u767e\u642d'", ",", "'\u914d\u642d'", ",", "'\u642d'", ",", "'\u7a7f\u642d'", "]", ")", "&", "set", "(", "tokens", ")", ")", ">", "0", ":", "\n", "        ", "a", "=", "7", "\n", "", "else", ":", "\n", "        ", "a", "=", "-", "1", "\n", "", "return", "a", "\n", "\n"]], "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.topic_modeling.prepare_human_label.write_excel": [[17, 43], ["openpyxl.Workbook", "range", "openpyxl.Workbook.save", "len", "worksheet.cell", "item.split.split", "openpyxl.cell.cell.ILLEGAL_CHARACTERS_RE.sub", "worksheet.cell", "worksheet.cell", "worksheet.cell"], "function", ["home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.preprocess.process_daren.split"], ["def", "write_excel", "(", "filename", ",", "data", ")", ":", "\n", "        ", "workbook", "=", "openpyxl", ".", "Workbook", "(", ")", "\n", "worksheet", "=", "workbook", ".", "active", "\n", "\n", "title", "=", "[", "'\u7c7b\u522b'", ",", "'\u6587\u6848'", ",", "'\u5546\u54c1\u4fe1\u606f'", ",", "'sku'", "]", "\n", "for", "i", "in", "range", "(", "len", "(", "title", ")", ")", ":", "\n", "            ", "worksheet", ".", "cell", "(", "1", ",", "i", "+", "1", ",", "title", "[", "i", "]", ")", "\n", "\n", "", "total_c", "=", "0", "\n", "for", "item", "in", "data", ":", "\n", "            ", "item", "=", "item", ".", "split", "(", "'|||'", ")", "\n", "prod", "=", "item", "[", "1", "]", "\n", "prod", "=", "ILLEGAL_CHARACTERS_RE", ".", "sub", "(", "r''", ",", "prod", ")", "\n", "write", "=", "item", "[", "2", "]", "\n", "sku", "=", "item", "[", "0", "]", "\n", "#worksheet.cell(total_c+2,1,'0\u673a\u8eab\u5916\u89c2')", "\n", "worksheet", ".", "cell", "(", "total_c", "+", "2", ",", "2", ",", "write", ")", "\n", "worksheet", ".", "cell", "(", "total_c", "+", "2", ",", "3", ",", "prod", ")", "\n", "worksheet", ".", "cell", "(", "total_c", "+", "2", ",", "4", ",", "sku", ")", "\n", "total_c", "+=", "1", "\n", "\n", "#dv=DataValidation(type='list', formula1='\"0\u673a\u8eab\u5916\u89c2\",\"1\u5c4f\u5e55\u97f3\u6548\",\"2\u7f51\u7edc\u53cc\u6a215G\",\"3\u7167\u76f8\u7f8e\u989c\",\"4\u6027\u80fd\",\"5\u7535\u6c60\",\"6\u89e3\u9501\"',allow_blank=True)", "\n", "#worksheet.add_data_validation(dv) ", "\n", "#dv.add('A2:A301')", "\n", "\n", "", "workbook", ".", "save", "(", "filename", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.topic_modeling.classify_1381.check_multi": [[18, 33], ["None"], "function", ["None"], ["def", "check_multi", "(", "tokens", ")", ":", "\n", "    ", "topic_words", "=", "[", "[", "'\u9632\u6652'", ",", "'\u7d2b\u5916\u7ebf'", ",", "'\u70c8\u65e5'", ",", "'\u6652'", ",", "'\u9ad8\u500d'", ",", "'UVB'", ",", "'UVA'", "]", ",", "\n", "[", "'\u7ec6\u7eb9'", ",", "'\u76b1\u7eb9'", ",", "'\u6297\u76b1'", ",", "'\u7d27\u81f4'", ",", "'\u6de1\u7eb9'", ",", "'\u63d0\u62c9'", ",", "'\u8f6e\u5ed3'", "]", ",", "\n", "[", "'\u6e05\u6d01'", ",", "'\u9ed1\u5934'", ",", "'\u7c97\u5927'", ",", "'\u63a7\u6cb9'", ",", "'\u51c0\u5316'", ",", "'\u6c61\u57a2'", ",", "'\u51c0\u900f'", ",", "'\u7c89\u523a'", ",", "'\u6d01\u51c0'", ",", "'\u795b\u75d8'", ",", "'\u6d01\u9762'", "]", ",", "\n", "[", "'\u7f8e\u767d'", ",", "'\u795b\u6591'", ",", "'\u9ed1\u8272\u7d20'", ",", "'\u8272\u6591'", ",", "'\u80a4\u8272'", ",", "'\u63d0\u4eae'", ",", "'\u7115\u767d'", ",", "'\u4eae\u767d'", ",", "'\u6de1\u6591'", "]", ",", "\n", "[", "'\u8865\u6c34'", ",", "'\u5e72\u71e5'", ",", "'\u559d\u9971\u6c34'", ",", "'\u562d'", ",", "'\u6c34\u6da6'", ",", "'\u5e72\u76ae'", ",", "'\u6c34\u5ae9'", ",", "'\u4fdd\u6e7f'", ",", "'\u9501\u6c34'", ",", "'\u542b\u6c34\u91cf'", ",", "'\u7f3a\u6c34'", "]", ",", "\n", "[", "'\u4fee\u62a4'", ",", "'\u71ac\u591c'", ",", "'\u8106\u5f31'", ",", "'\u5f3a\u97e7'", ",", "'\u7ef4\u7a33'", ",", "'\u8212\u7f13'", ",", "'\u654f\u611f'", ",", "'\u53d7\u635f'", ",", "'\u5c4f\u969c'", "]", ",", "\n", "[", "'\u8d28\u5730'", ",", "'\u8f7b\u8584'", ",", "'\u523a\u6fc0'", ",", "'\u5473\u9053'", ",", "'\u5ef6\u5c55\u6027'", ",", "'\u7c98\u817b'", ",", "'\u6d41\u52a8\u6027'", ",", "'\u8f7b\u76c8'", "]", "]", "\n", "count", "=", "0", "\n", "for", "topic", "in", "topic_words", ":", "\n", "        ", "for", "word", "in", "tokens", ":", "\n", "            ", "if", "word", "in", "topic", ":", "\n", "                ", "count", "+=", "1", "\n", "break", "\n", "", "", "", "return", "count", "\n", "\n"]], "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.topic_modeling.classify_1381.class_by_keyword": [[35, 55], ["len", "len", "set", "set", "len", "set", "set", "len", "set", "set", "len", "set", "set", "len", "set", "set", "len", "set", "set", "len", "set", "set", "set", "set"], "function", ["None"], ["", "def", "class_by_keyword", "(", "tokens", ")", ":", "\n", "    ", "if", "len", "(", "set", "(", "[", "'\u9632\u6652'", ",", "'\u7d2b\u5916\u7ebf'", ",", "'\u70c8\u65e5'", ",", "'\u6652'", ",", "'\u9ad8\u500d'", ",", "'UVB'", ",", "'UVA'", "]", ")", "&", "set", "(", "tokens", ")", ")", ">", "0", ":", "\n", "        ", "a", "=", "0", "\n", "", "elif", "len", "(", "set", "(", "[", "'\u7ec6\u7eb9'", ",", "'\u76b1\u7eb9'", ",", "'\u6297\u76b1'", ",", "'\u7d27\u81f4'", ",", "'\u6de1\u7eb9'", ",", "'\u63d0\u62c9'", ",", "'\u8f6e\u5ed3'", ",", "'\u5507\u7eb9'", ",", "'\u6cd5\u4ee4\u7eb9'", ",", "'\u6297\u8001'", ",", "'\u521d\u8001'", "]", ")", "&", "set", "(", "tokens", ")", ")", ">", "0", ":", "\n", "        ", "a", "=", "1", "\n", "", "elif", "len", "(", "set", "(", "[", "'\u6e05\u6d01'", ",", "'\u9ed1\u5934'", ",", "'\u7c97\u5927'", ",", "'\u63a7\u6cb9'", ",", "'\u51c0\u5316'", ",", "'\u6c61\u57a2'", ",", "'\u51c0\u900f'", ",", "'\u7c89\u523a'", ",", "'\u6d01\u51c0'", ",", "'\u795b\u75d8'", ",", "'\u6d01\u9762'", ",", "'\u5378\u5986'", "]", ")", "&", "set", "(", "tokens", ")", ")", ">", "0", ":", "\n", "        ", "a", "=", "2", "\n", "", "elif", "len", "(", "set", "(", "[", "'\u7f8e\u767d'", ",", "'\u795b\u6591'", ",", "'\u9ed1\u8272\u7d20'", ",", "'\u8272\u6591'", ",", "'\u80a4\u8272'", ",", "'\u63d0\u4eae'", ",", "'\u7115\u767d'", ",", "'\u4eae\u767d'", ",", "'\u6de1\u6591'", ",", "'\u900f\u4eae'", ",", "'\u767d\u5ae9'", ",", "'\u6697\u6c89'", ",", "'\u4eae\u5ea6'", ",", "'\u8272\u7d20'", ",", "'\u767d\u7699'", "]", ")", "&", "set", "(", "tokens", ")", ")", ">", "0", ":", "\n", "        ", "a", "=", "3", "\n", "", "elif", "len", "(", "set", "(", "[", "'\u8865\u6c34'", ",", "'\u5e72\u71e5'", ",", "'\u559d\u9971\u6c34'", ",", "'\u562d'", ",", "'\u6c34\u6da6'", ",", "'\u5e72\u76ae'", ",", "'\u6c34\u5ae9'", ",", "'\u4fdd\u6e7f'", ",", "'\u9501\u6c34'", ",", "'\u542b\u6c34\u91cf'", ",", "'\u7f3a\u6c34'", ",", "'\u6c34\u4efd'", ",", "'\u6c34\u5206'", "]", ")", "&", "set", "(", "tokens", ")", ")", ">", "0", ":", "\n", "        ", "a", "=", "4", "\n", "", "elif", "len", "(", "set", "(", "[", "'\u4fee\u62a4'", ",", "'\u71ac\u591c'", ",", "'\u8106\u5f31'", ",", "'\u5f3a\u97e7'", ",", "'\u7ef4\u7a33'", ",", "'\u8212\u7f13'", ",", "'\u654f\u611f'", ",", "'\u53d7\u635f'", ",", "'\u5c4f\u969c'", ",", "'\u4fee\u590d'", ",", "'\u5e73\u8861'", "]", ")", "&", "set", "(", "tokens", ")", ")", ">", "0", ":", "\n", "        ", "a", "=", "5", "\n", "", "elif", "len", "(", "set", "(", "[", "'\u8d28\u5730'", ",", "'\u8f7b\u8584'", ",", "'\u523a\u6fc0'", ",", "'\u5473\u9053'", ",", "'\u5ef6\u5c55\u6027'", ",", "'\u7c98\u817b'", ",", "'\u6d41\u52a8\u6027'", ",", "'\u8f7b\u76c8'", ",", "'\u6210\u5206'", ",", "'\u690d\u7269'", ",", "'\u8403\u53d6'", ",", "'\u914d\u65b9'", ",", "'\u8574\u542b'", "]", ")", "&", "set", "(", "tokens", ")", ")", ">", "0", ":", "\n", "        ", "a", "=", "6", "\n", "", "elif", "len", "(", "set", "(", "[", "'\u5305\u88c5'", ",", "'\u74f6\u8eab'", ",", "'\u538b\u5934'", ",", "'\u55b7\u5934'", ",", "'\u8bbe\u8ba1'", ",", "'\u643a\u5e26'", ",", "'\u5916\u89c2'", "]", ")", "&", "set", "(", "tokens", ")", ")", ">", "0", ":", "\n", "        ", "a", "=", "7", "\n", "", "else", ":", "\n", "        ", "a", "=", "-", "1", "\n", "", "return", "a", "\n", "\n"]], "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.topic_modeling.classify_671.check_multi": [[19, 37], ["None"], "function", ["None"], ["def", "check_multi", "(", "tokens", ")", ":", "\n", "    ", "topic_words", "=", "[", "\n", "[", "'\u663e\u5361'", ",", "'\u72ec\u7acb'", ",", "'\u72ec\u663e'", "]", ",", "\n", "[", "'\u4f53\u578b'", ",", "'\u5c0f\u5de7'", ",", "'\u6750\u8d28'", ",", "'\u673a\u8eab'", ",", "'\u5916\u5f62'", ",", "'\u5916\u89c2'", ",", "'\u91d1\u5c5e'", ",", "'\u4f53\u79ef'", "]", ",", "\n", "[", "'\u5b58\u50a8'", ",", "'\u786c\u76d8'", ",", "'\u5185\u5b58'", ",", "'\u56fa\u6001'", ",", "'\u50a8\u5b58'", "]", ",", "\n", "[", "'\u98ce\u6247'", ",", "'\u6563\u70ed'", ",", "'\u964d\u6e29'", ",", "'\u70ed\u91cf'", "]", ",", "\n", "[", "'\u6307\u7eb9'", ",", "'\u89e3\u9501'", ",", "'\u9690\u79c1'", ",", "'\u5b89\u5168'", ",", "'\u5bc6\u7801'", ",", "'\u6307\u7eb9\u8bc6\u522b'", ",", "'\u52a0\u5bc6'", "]", ",", "\n", "[", "'\u89e6\u63a7'", ",", "'\u63a5\u53e3'", ",", "'\u6a21\u5f0f'", ",", "'\u70b9\u89e6'", ",", "'\u8f6c\u8f74'", ",", "'\u65cb\u8f6c'", ",", "'\u7ffb\u8f6c'", ",", "'\u534f\u540c'", "]", ",", "\n", "[", "'\u7535\u6c60'", ",", "'\u7eed\u822a'", ",", "'\u7535\u91cf'", ",", "'\u5145\u7535'", ",", "'\u6301\u4e45'", ",", "'\u5f85\u673a'", "]", ",", "\n", "[", "'\u952e\u76d8'", ",", "'\u6309\u952e'", ",", "'\u624b\u611f'", ",", "'\u56de\u5f39'", ",", "'\u97f3\u6548'", ",", "'\u58f0\u97f3'", ",", "'\u675c\u6bd4'", ",", "'\u58f0\u97f3'", ",", "'\u53d1\u58f0'", "]", ",", "\n", "[", "'\u5c4f\u5e55'", ",", "'\u9ad8\u6e05'", ",", "'\u5237\u65b0\u7387'", ",", "'\u5206\u8fa8\u7387'", ",", "'\u5c4f'", ",", "'\u62a4\u773c'", ",", "'\u84dd\u5149'", ",", "'\u663e\u793a\u5c4f'", ",", "'\u50cf\u7d20'", ",", "'\u6444\u50cf\u5934'", "]", "]", "\n", "count", "=", "0", "\n", "for", "topic", "in", "topic_words", ":", "\n", "        ", "for", "word", "in", "tokens", ":", "\n", "            ", "if", "word", "in", "topic", ":", "\n", "                ", "count", "+=", "1", "\n", "break", "\n", "", "", "", "return", "count", "\n", "\n"]], "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.topic_modeling.classify_671.class_by_keyword": [[39, 61], ["len", "len", "set", "set", "len", "set", "set", "len", "set", "set", "len", "set", "set", "len", "set", "set", "len", "set", "set", "len", "set", "set", "len", "set", "set", "set", "set"], "function", ["None"], ["", "def", "class_by_keyword", "(", "tokens", ")", ":", "\n", "    ", "if", "len", "(", "set", "(", "[", "'\u663e\u5361'", ",", "'\u72ec\u7acb'", ",", "'\u72ec\u663e'", "]", ")", "&", "set", "(", "tokens", ")", ")", ">", "0", ":", "\n", "        ", "a", "=", "0", "\n", "", "elif", "len", "(", "set", "(", "[", "'\u4f53\u578b'", ",", "'\u5c0f\u5de7'", ",", "'\u6750\u8d28'", ",", "'\u673a\u8eab'", ",", "'\u5916\u5f62'", ",", "'\u5916\u89c2'", ",", "'\u91d1\u5c5e'", ",", "'\u4f53\u79ef'", "]", ")", "&", "set", "(", "tokens", ")", ")", ">", "0", ":", "\n", "        ", "a", "=", "1", "\n", "", "elif", "len", "(", "set", "(", "[", "'\u5b58\u50a8'", ",", "'\u786c\u76d8'", ",", "'\u5185\u5b58'", ",", "'\u56fa\u6001'", ",", "'\u50a8\u5b58'", ",", "'\u5904\u7406\u5668'", ",", "'\u82af\u7247'", "]", ")", "&", "set", "(", "tokens", ")", ")", ">", "0", ":", "\n", "        ", "a", "=", "2", "\n", "", "elif", "len", "(", "set", "(", "[", "'\u98ce\u6247'", ",", "'\u6563\u70ed'", ",", "'\u964d\u6e29'", ",", "'\u70ed\u91cf'", "]", ")", "&", "set", "(", "tokens", ")", ")", ">", "0", ":", "\n", "        ", "a", "=", "3", "\n", "", "elif", "len", "(", "set", "(", "[", "'\u6307\u7eb9'", ",", "'\u89e3\u9501'", ",", "'\u9690\u79c1'", ",", "'\u5b89\u5168'", ",", "'\u5bc6\u7801'", ",", "'\u6307\u7eb9\u8bc6\u522b'", ",", "'\u52a0\u5bc6'", "]", ")", "&", "set", "(", "tokens", ")", ")", ">", "0", ":", "\n", "        ", "a", "=", "4", "\n", "", "elif", "len", "(", "set", "(", "[", "'\u89e6\u63a7'", ",", "'\u63a5\u53e3'", ",", "'\u6a21\u5f0f'", ",", "'\u70b9\u89e6'", ",", "'\u8f6c\u8f74'", ",", "'\u65cb\u8f6c'", ",", "'\u7ffb\u8f6c'", ",", "'\u534f\u540c'", "]", ")", "&", "set", "(", "tokens", ")", ")", ">", "0", ":", "\n", "        ", "a", "=", "5", "\n", "", "elif", "len", "(", "set", "(", "[", "'\u7535\u6c60'", ",", "'\u7eed\u822a'", ",", "'\u7535\u91cf'", ",", "'\u5145\u7535'", ",", "'\u5f85\u673a'", "]", ")", "&", "set", "(", "tokens", ")", ")", ">", "0", ":", "\n", "        ", "a", "=", "6", "\n", "", "elif", "len", "(", "set", "(", "[", "'\u952e\u76d8'", ",", "'\u6309\u952e'", ",", "'\u624b\u611f'", ",", "'\u56de\u5f39'", ",", "'\u97f3\u6548'", ",", "'\u58f0\u97f3'", ",", "'\u675c\u6bd4'", ",", "'\u58f0\u97f3'", ",", "'\u53d1\u58f0'", "]", ")", "&", "set", "(", "tokens", ")", ")", ">", "0", ":", "\n", "        ", "a", "=", "7", "\n", "", "elif", "len", "(", "set", "(", "[", "'\u5c4f\u5e55'", ",", "'\u9ad8\u6e05'", ",", "'\u5237\u65b0\u7387'", ",", "'\u5206\u8fa8\u7387'", ",", "'\u5c4f'", ",", "'\u62a4\u773c'", ",", "'\u84dd\u5149'", ",", "'\u663e\u793a\u5c4f'", ",", "'\u50cf\u7d20'", ",", "'\u6444\u50cf\u5934'", "]", ")", "&", "set", "(", "tokens", ")", ")", ">", "0", ":", "\n", "        ", "a", "=", "8", "\n", "", "else", ":", "\n", "        ", "a", "=", "-", "1", "\n", "", "return", "a", "\n", "\n"]], "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.topic_modeling.topic_model.stopwordslist": [[25, 29], ["stopwords.extend", "line.strip", "open().readlines", "open"], "function", ["None"], ["def", "stopwordslist", "(", ")", ":", "\n", "    ", "stopwords", "=", "[", "line", ".", "strip", "(", ")", "for", "line", "in", "open", "(", "'./stop_words.txt'", ",", "'r'", ",", "encoding", "=", "'UTF-8'", ")", ".", "readlines", "(", ")", "]", "\n", "stopwords", ".", "extend", "(", "[", "'\u8bbe\u8ba1'", ",", "'#'", ",", "'\u7a7f'", ",", "'\u808c\u80a4'", ",", "'\u6548\u679c'", ",", "'\u6210\u5206'", ",", "'\u9762\u819c'", "]", ")", "\n", "return", "stopwords", "\n", "\n"]], "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.topic_modeling.topic_model.seg_depart": [[30, 40], ["jieba.cut", "topic_model.stopwordslist", "sentence.strip"], "function", ["home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.unilm.data.cut", "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.evaluation.utils.stopwordslist"], ["", "def", "seg_depart", "(", "sentence", ")", ":", "\n", "    ", "sentence_depart", "=", "jieba", ".", "cut", "(", "sentence", ".", "strip", "(", ")", ")", "\n", "stopwords", "=", "stopwordslist", "(", ")", "\n", "outstr", "=", "''", "\n", "for", "word", "in", "sentence_depart", ":", "\n", "        ", "if", "word", "not", "in", "stopwords", ":", "\n", "            ", "if", "word", "!=", "'\\t'", ":", "\n", "                ", "outstr", "+=", "word", "\n", "outstr", "+=", "\" \"", "\n", "", "", "", "return", "outstr", "\n", "\n"]], "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.topic_modeling.topic_model.format_topics_sentences": [[41, 60], ["pandas.DataFrame", "enumerate", "pandas.Series", "pandas.concat", "ldamodel.get_document_topics", "sorted", "enumerate", "ldamodel.show_topic", "sent_topics_df.append.append", "pandas.Series", "int", "round"], "function", ["None"], ["", "def", "format_topics_sentences", "(", "ldamodel", ",", "corpus", ",", "texts", ")", ":", "\n", "# Init output", "\n", "    ", "sent_topics_df", "=", "pd", ".", "DataFrame", "(", ")", "\n", "# Get main topic in each document", "\n", "for", "i", ",", "row", "in", "enumerate", "(", "ldamodel", ".", "get_document_topics", "(", "corpus", ")", ")", ":", "\n", "        ", "row", "=", "sorted", "(", "row", ",", "key", "=", "lambda", "x", ":", "(", "x", "[", "1", "]", ")", ",", "reverse", "=", "True", ")", "\n", "# Get the Dominant topic, Perc Contribution and Keywords for each document", "\n", "for", "j", ",", "(", "topic_num", ",", "prop_topic", ")", "in", "enumerate", "(", "row", ")", ":", "\n", "            ", "if", "j", "==", "0", ":", "# => dominant topic", "\n", "                ", "wp", "=", "ldamodel", ".", "show_topic", "(", "topic_num", ")", "\n", "topic_keywords", "=", "\", \"", ".", "join", "(", "[", "word", "for", "word", ",", "prop", "in", "wp", "]", ")", "\n", "sent_topics_df", "=", "sent_topics_df", ".", "append", "(", "pd", ".", "Series", "(", "[", "int", "(", "topic_num", ")", ",", "round", "(", "prop_topic", ",", "4", ")", ",", "topic_keywords", "]", ")", ",", "ignore_index", "=", "True", ")", "\n", "", "else", ":", "\n", "                ", "break", "\n", "\n", "", "", "", "sent_topics_df", ".", "columns", "=", "[", "'Dominant_Topic'", ",", "'Perc_Contribution'", ",", "'Topic_Keywords'", "]", "\n", "contents", "=", "pd", ".", "Series", "(", "texts", ")", "\n", "sent_topics_df", "=", "pd", ".", "concat", "(", "[", "sent_topics_df", ",", "contents", "]", ",", "axis", "=", "1", ")", "\n", "return", "(", "sent_topics_df", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.topic_modeling.check_multi.check_multi": [[7, 23], ["list", "jieba.cut"], "function", ["home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.unilm.data.cut"], ["def", "check_multi", "(", "write", ")", ":", "\n", "    ", "topic_words", "=", "[", "[", "'\u62cd\u6444'", ",", "'\u955c\u5934'", ",", "'\u540e\u7f6e'", ",", "'\u62cd\u7167'", ",", "'\u4e09\u6444'", ",", "'\u56db\u6444'", ",", "'\u4eba\u50cf'", ",", "'\u5355\u6444'", "]", ",", "\n", "[", "'\u8fd0\u884c'", ",", "'\u9e92\u9e9f'", ",", "'\u5904\u7406\u5668'", ",", "'\u9a81\u9f99'", ",", "'\u9ad8\u901a'", "]", ",", "\n", "[", "'\u7535\u6c60'", ",", "'\u7eed\u822a'", ",", "'\u5927\u5bb9\u91cf'", ",", "'\u6301\u4e45'", ",", "'\u5feb\u5145'", ",", "'\u7535\u91cf'", ",", "'\u5145\u7535'", ",", "'\u95ea\u5145'", "]", ",", "\n", "[", "'\u89e3\u9501'", ",", "'\u6307\u7eb9'", ",", "'\u9690\u79c1'", "]", ",", "\n", "[", "'\u5c4f'", ",", "'\u673a\u8eab'", ",", "'\u89c6\u91ce'", ",", "'\u82f1\u5bf8'", ",", "'\u753b\u9762'", ",", "'\u5c4f\u5e55'", ",", "'\u5168\u9762'", ",", "'\u663e\u793a\u5c4f'", ",", "'\u663e\u793a'", ",", "'\u6750\u6599'", ",", "'\u9632\u6c34'", ",", "'\u8fdb\u6c34'", ",", "'\u5916\u89c2'", "]", ",", "\n", "[", "'5G'", ",", "'\u53cc\u6a21'", ",", "'\u53cc\u5361'", "]", ",", "\n", "[", "'\u6563\u70ed'", ",", "'\u6db2\u51b7'", "]", "]", "\n", "count", "=", "0", "\n", "write", "=", "list", "(", "jieba", ".", "cut", "(", "write", ",", "HMM", "=", "True", ")", ")", "\n", "for", "topic", "in", "topic_words", ":", "\n", "        ", "for", "word", "in", "write", ":", "\n", "            ", "if", "word", "in", "topic", ":", "\n", "                ", "count", "+=", "1", "\n", "break", "\n", "", "", "", "return", "count", "\n", "\n"]], "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.topic_modeling.classify_794.check_multi": [[18, 36], ["None"], "function", ["None"], ["def", "check_multi", "(", "tokens", ")", ":", "\n", "    ", "topic_words", "=", "[", "[", "'\u6297\u83cc'", ",", "'\u9632\u9709'", ",", "'\u6740\u83cc'", ",", "'\u7ec6\u83cc'", ",", "'\u6291\u83cc'", ",", "'\u87a8\u866b'", ",", "'\u51c0\u5473'", ",", "'\u9664\u83cc'", ",", "'\u81ea\u6d01'", ",", "'\u64e6\u6d17'", "]", ",", "\n", "[", "'\u5b58\u50a8\u7a7a\u95f4'", ",", "'\u5bb9\u91cf\u5927'", ",", "'\u5168\u5bb6\u4eba'", ",", "'\u591a\u4eba'", ",", "'\u5168\u5bb6'", ",", "'\u5bbd\u655e'", ",", "'\u5bb9\u91cf'", ",", "'\u5206\u533a'", ",", "'\u7a7a\u95f4'", "]", ",", "\n", "[", "'\u6c61\u6e0d'", ",", "'\u6d17\u6da4'", ",", "'\u70d8\u5e72'", ",", "'\u70d8'", "]", ",", "\n", "[", "'\u53d8\u8d28'", ",", "'\u4fdd\u9c9c'", ",", "'\u98ce\u51b7'", ",", "'\u65e0\u971c'", ",", "'\u65b0\u9c9c'", ",", "'\u50a8\u9c9c'", ",", "'\u51fa\u51b0'", ",", "'\u7ed3\u971c'", "]", ",", "\n", "[", "'\u753b\u9762'", ",", "'\u753b\u8d28'", ",", "'\u5c4f\u5e55'", ",", "'\u5206\u8fa8\u7387'", ",", "'\u8d85\u9ad8'", ",", "'\u56fe\u50cf'", ",", "'\u8272\u5f69'", ",", "'\u9ad8\u6e05'", "]", ",", "\n", "[", "'\u4eba\u4f53'", ",", "'\u9001\u98ce'", ",", "'\u6c14\u6d41'", ",", "'\u65e0\u98ce\u611f'", ",", "'\u5934\u75db'", ",", "'\u76f4\u5439'", ",", "'\u98ce\u611f'", ",", "'\u96f6\u98ce\u611f'", "]", ",", "\n", "[", "'\u6027\u80fd'", ",", "'\u5904\u7406\u5668'", ",", "'\u82af\u7247'", ",", "'\u642d\u8f7d'", ",", "'\u5f3a\u52b2'", ",", "'\u9ad8\u6548'", ",", "'\u52a8\u529b'", "]", ",", "\n", "[", "'\u8bed\u97f3'", ",", "'\u4eba\u5de5\u667a\u80fd'", ",", "'AI'", ",", "'\u58f0\u63a7'", ",", "'\u7f16\u7a0b'", ",", "'\u63a7\u6e29'", "]", ",", "\n", "[", "'\u673a\u8eab'", ",", "'\u5916\u5f62'", ",", "'\u5916\u89c2'", ",", "'\u7ea4\u8584'", ",", "'\u5927\u6c14'", ",", "'\u65f6\u5c1a'", ",", "'\u5de5\u827a'", ",", "'\u8d28\u611f'", ",", "'\u94dd\u677f'", "]", ",", "\n", "[", "'\u8282\u80fd'", ",", "'\u7535'", ",", "'\u566a\u97f3'", ",", "'\u73af\u4fdd'", ",", "'\u7eff\u8272'", ",", "'\u9759\u97f3'", ",", "'\u80fd\u8017'", ",", "'\u6d6a\u8d39'", ",", "'\u4f4e\u8017'", ",", "'\u7701'", "]", "]", "\n", "count", "=", "0", "\n", "for", "topic", "in", "topic_words", ":", "\n", "        ", "for", "word", "in", "tokens", ":", "\n", "            ", "if", "word", "in", "topic", ":", "\n", "                ", "count", "+=", "1", "\n", "break", "\n", "", "", "", "return", "count", "\n", "\n"]], "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.topic_modeling.classify_794.class_by_keyword": [[38, 62], ["len", "len", "set", "set", "len", "set", "set", "len", "set", "set", "len", "set", "set", "len", "set", "set", "len", "set", "set", "len", "set", "set", "len", "set", "set", "len", "set", "set", "set", "set"], "function", ["None"], ["", "def", "class_by_keyword", "(", "tokens", ")", ":", "\n", "    ", "if", "len", "(", "set", "(", "[", "'\u6297\u83cc'", ",", "'\u9632\u9709'", ",", "'\u6740\u83cc'", ",", "'\u7ec6\u83cc'", ",", "'\u6291\u83cc'", ",", "'\u87a8\u866b'", ",", "'\u51c0\u5473'", ",", "'\u9664\u83cc'", ",", "'\u81ea\u6d01'", ",", "'\u64e6\u6d17'", ",", "'\u5f02\u5473'", "]", ")", "&", "set", "(", "tokens", ")", ")", ">", "0", ":", "\n", "        ", "a", "=", "0", "\n", "", "elif", "len", "(", "set", "(", "[", "'\u5b58\u50a8\u7a7a\u95f4'", ",", "'\u5bb9\u91cf\u5927'", ",", "'\u5168\u5bb6\u4eba'", ",", "'\u591a\u4eba'", ",", "'\u5bbd\u655e'", ",", "'\u5bb9\u91cf'", ",", "'\u5206\u533a'", ",", "'\u7a7a\u95f4'", ",", "'\u5927\u5bb9\u91cf'", "]", ")", "&", "set", "(", "tokens", ")", ")", ">", "0", ":", "\n", "        ", "a", "=", "1", "\n", "", "elif", "len", "(", "set", "(", "[", "'\u6c61\u6e0d'", ",", "'\u6d17\u6da4'", ",", "'\u70d8\u5e72'", ",", "'\u70d8'", "]", ")", "&", "set", "(", "tokens", ")", ")", ">", "0", ":", "\n", "        ", "a", "=", "2", "\n", "", "elif", "len", "(", "set", "(", "[", "'\u53d8\u8d28'", ",", "'\u4fdd\u9c9c'", ",", "'\u98ce\u51b7'", ",", "'\u65e0\u971c'", ",", "'\u65b0\u9c9c'", ",", "'\u50a8\u9c9c'", ",", "'\u51fa\u51b0'", ",", "'\u7ed3\u971c'", "]", ")", "&", "set", "(", "tokens", ")", ")", ">", "0", ":", "\n", "        ", "a", "=", "3", "\n", "", "elif", "len", "(", "set", "(", "[", "'\u753b\u9762'", ",", "'\u753b\u8d28'", ",", "'\u5c4f\u5e55'", ",", "'\u5206\u8fa8\u7387'", ",", "'\u8d85\u9ad8'", ",", "'\u56fe\u50cf'", ",", "'\u8272\u5f69'", ",", "'\u9ad8\u6e05'", ",", "'\u97f3\u6548'", ",", "'\u5c4f'", "]", ")", "&", "set", "(", "tokens", ")", ")", ">", "0", ":", "\n", "        ", "a", "=", "4", "\n", "", "elif", "len", "(", "set", "(", "[", "'\u4eba\u4f53'", ",", "'\u9001\u98ce'", ",", "'\u6c14\u6d41'", ",", "'\u65e0\u98ce\u611f'", ",", "'\u5934\u75db'", ",", "'\u76f4\u5439'", ",", "'\u98ce\u611f'", ",", "'\u96f6\u98ce\u611f'", "]", ")", "&", "set", "(", "tokens", ")", ")", ">", "0", ":", "\n", "        ", "a", "=", "5", "\n", "", "elif", "len", "(", "set", "(", "[", "'\u6027\u80fd'", ",", "'\u5904\u7406\u5668'", ",", "'\u82af\u7247'", ",", "'\u5f3a\u52b2'", ",", "'\u9ad8\u6548'", ",", "'\u52a8\u529b'", "]", ")", "&", "set", "(", "tokens", ")", ")", ">", "0", ":", "\n", "        ", "a", "=", "6", "\n", "", "elif", "len", "(", "set", "(", "[", "'\u8bed\u97f3'", ",", "'\u4eba\u5de5\u667a\u80fd'", ",", "'AI'", ",", "'\u58f0\u63a7'", ",", "'\u7f16\u7a0b'", ",", "'\u63a7\u6e29'", "]", ")", "&", "set", "(", "tokens", ")", ")", ">", "0", ":", "\n", "        ", "a", "=", "7", "\n", "", "elif", "len", "(", "set", "(", "[", "'\u673a\u8eab'", ",", "'\u5916\u5f62'", ",", "'\u5916\u89c2'", ",", "'\u7ea4\u8584'", ",", "'\u5927\u6c14'", ",", "'\u65f6\u5c1a'", ",", "'\u5de5\u827a'", ",", "'\u8d28\u611f'", ",", "'\u94dd\u677f'", "]", ")", "&", "set", "(", "tokens", ")", ")", ">", "0", ":", "\n", "        ", "a", "=", "8", "\n", "", "elif", "len", "(", "set", "(", "[", "'\u8282\u80fd'", ",", "'\u7535'", ",", "'\u566a\u97f3'", ",", "'\u73af\u4fdd'", ",", "'\u7eff\u8272'", ",", "'\u9759\u97f3'", ",", "'\u80fd\u8017'", ",", "'\u6d6a\u8d39'", ",", "'\u4f4e\u8017'", ",", "'\u7701'", ",", "'\u7535\u8d39'", "]", ")", "&", "set", "(", "tokens", ")", ")", ">", "0", ":", "\n", "        ", "a", "=", "9", "\n", "", "else", ":", "\n", "        ", "a", "=", "-", "1", "\n", "", "return", "a", "\n", "\n"]], "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.produce.title_rules.refine_feature_1343": [[4, 43], ["pw.replace.replace", "pw.replace.replace", "feature.split", "feature.split", "len", "len", "feature.split", "len", "feature.split"], "function", ["home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.preprocess.process_daren.split", "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.preprocess.process_daren.split", "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.preprocess.process_daren.split", "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.preprocess.process_daren.split"], ["def", "refine_feature_1343", "(", "prod", ",", "feature", ")", ":", "\n", "#\u53bb\u6389\u6ca1\u6709\u751f\u6210\u5b8c\u6574\u7684", "\n", "    ", "if", "'('", "in", "feature", ":", "\n", "      ", "feature", "=", "feature", ".", "split", "(", "'('", ")", "[", "1", "]", "\n", "", "if", "\"+\"", "not", "in", "feature", "or", "len", "(", "feature", ".", "split", "(", "\"+\"", ")", ")", ">", "2", ":", "\n", "      ", "return", "''", "\n", "", "if", "'[UNK]'", "in", "feature", ":", "\n", "      ", "return", "''", "\n", "", "sensitive_words", "=", "[", "'\u65f6\u5c1a'", ",", "'\u767e\u642d'", ",", "'\u978b'", ",", "'\u3011'", ",", "'\u3010'", ",", "'\uff09'", ",", "'\uff08'", "]", "\n", "for", "word", "in", "sensitive_words", ":", "\n", "      ", "if", "word", "in", "feature", ":", "\n", "         ", "return", "''", "\n", "\n", "#\u68c0\u67e5\u5546\u54c1\u8bcd", "\n", "", "", "pw", "=", "feature", ".", "split", "(", "'+'", ")", "[", "1", "]", "\n", "if", "pw", "in", "[", "'\u8fde\u8863'", ",", "'\u725b\u4ed4'", ",", "'\u534a\u8eab'", "]", ":", "\n", "       ", "pw", "=", "pw", "+", "'\u88d9'", "\n", "", "pw", "=", "pw", ".", "replace", "(", "'\u7fa4'", ",", "'\u88d9'", ")", "\n", "pw", "=", "pw", ".", "replace", "(", "'\u8896'", ",", "'\u88d9'", ")", "\n", "if", "'\u4ef6\u5957'", "in", "prod", "or", "'\u5957\u88c5'", "in", "prod", ":", "\n", "       ", "if", "(", "'\u5957'", "not", "in", "pw", "or", "'\u4e24\u4ef6'", "not", "in", "pw", "or", "'\u4e09\u4ef6'", "not", "in", "pw", ")", ":", "\n", "           ", "pw", "=", "pw", "+", "'\u5957\u88c5'", "\n", "\n", "", "", "core_word", "=", "[", "'\u88e4'", ",", "'\u88d9'", ",", "'\u8fde\u8863\u88d9'", ",", "'\u886c\u886b'", "]", "\n", "for", "w", "in", "core_word", ":", "\n", "     ", "if", "w", "in", "pw", "and", "w", "not", "in", "prod", ":", "\n", "       ", "return", "''", "\n", "\n", "#\u68c0\u67e5\u4fee\u9970\u8bcd", "\n", "", "", "subj", "=", "feature", ".", "split", "(", "'+'", ")", "[", "0", "]", "\n", "if", "len", "(", "subj", ")", ">", "7", ":", "\n", "      ", "return", "''", "\n", "", "if", "subj", "==", "pw", "or", "subj", "in", "pw", ":", "\n", "      ", "return", "''", "\n", "\n", "", "if", "len", "(", "subj", ")", ">", "1", ":", "\n", "      ", "return", "subj", "+", "' '", "+", "pw", "\n", "", "else", ":", "\n", "      ", "return", "''", "\n", "\n"]], "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.produce.title_rules.refine_feature_653": [[45, 108], ["re.findall", "re.findall", "feature.split", "feature.split", "len", "len", "len", "subj_get.extend", "prod_get.extend", "subj.replace.replace", "subj.replace.replace", "re.findall", "re.findall", "len", "len", "random.randint", "len"], "function", ["home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.preprocess.process_daren.split", "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.preprocess.process_daren.split"], ["", "", "def", "refine_feature_653", "(", "prod", ",", "feature", ",", "write", ")", ":", "\n", "\n", "#\u68c0\u67e5\u4e2d\u5fc3\u8bcd\uff0c \u662f\u5426\u662f5g", "\n", "    ", "pw", "=", "feature", ".", "split", "(", "'+'", ")", "[", "1", "]", "\n", "if", "pw", "==", "'5g\u624b\u673a'", "and", "'\u4e0d\u652f\u63015g'", "in", "prod", ":", "\n", "       ", "pw", "=", "'\u624b\u673a'", "\n", "#\u68c0\u67e5\u662f\u5426\u4e3a\u8001\u4eba\u673a", "\n", "", "if", "'\u8001\u4eba\u624b\u673a'", "in", "prod", ":", "\n", "       ", "pw", "=", "'\u8001\u4eba\u624b\u673a'", "\n", "#\u68c0\u67e5\u662f\u5426\u4e3a\u5957\u88c5", "\n", "", "if", "'\u5957\u88c5'", "in", "prod", ":", "\n", "      ", "pw", "=", "'\u624b\u673a\u5957\u88c5'", "\n", "\n", "#\u68c0\u67e5\u4fee\u9970\u8bcd", "\n", "", "subj", "=", "feature", ".", "split", "(", "'+'", ")", "[", "0", "]", "\n", "#\u7ea0\u6b63\u4fee\u9970\u8bcd", "\n", "pattern_list", "=", "[", "[", "'[\u4e09|\u53cc|\u5355\uff5c\u56db]\u6444'", "]", ",", "[", "'[\u66f2\uff5c\u5168]\u9762\u5c4f'", "]", "]", "\n", "for", "pattern_l", "in", "pattern_list", ":", "\n", "        ", "subj_get", "=", "[", "]", "\n", "prod_get", "=", "[", "]", "\n", "for", "pattern", "in", "pattern_l", ":", "\n", "                  ", "subj_get", ".", "extend", "(", "re", ".", "findall", "(", "pattern", ",", "subj", ")", ")", "\n", "prod_get", ".", "extend", "(", "re", ".", "findall", "(", "pattern", ",", "prod", ")", ")", "\n", "", "if", "len", "(", "subj_get", ")", ">", "0", "and", "len", "(", "prod_get", ")", ">", "0", ":", "\n", "                  ", "subj", "=", "subj", ".", "replace", "(", "subj_get", "[", "0", "]", ",", "prod_get", "[", "0", "]", ")", "\n", "#\u5904\u7406\u542b\u6709\u53c2\u6570\u7684\u4fee\u9970\u8bcd", "\n", "#number_pattern_list = [['\\d+\\.?\\d*nm'], ['\\d+\\.?\\d*\u4e07'], ['\\d+\\.?\\d*\u82f1\u5bf8'], ['\\d+\\.?\\d*[w|W|\u74e6]'],['\\d+\\.?\\d*[h|H][Z|z]'],['\\d+\\.?\\d*[m|M][a|A][h|H]', '\\d+\\.?\\d*\u6beb\u5b89']]", "\n", "", "", "num_pattern", "=", "re", ".", "findall", "(", "'\\d+'", ",", "subj", ")", "\n", "if", "len", "(", "num_pattern", ")", ">", "0", ":", "\n", "            ", "subj", "=", "''", "\n", "#num_list=['\u5355','\u53cc','\u4e09','\u56db']", "\n", "#for num in num_list:", "\n", "#   if num in subj:", "\n", "#       subj =''", "\n", "\n", "#\u5220\u6389\u5b57\u6bcd\u6570\u5b57\u7ec4\u5408\u5982\u679c\u4ea7\u54c1\u91cc\u9762\u6ca1\u6709", "\n", "", "word", "=", "re", ".", "findall", "(", "'^[A-Za-z][A-Za-z\\d]{0,11}'", ",", "subj", ")", "\n", "if", "len", "(", "word", ")", ">", "0", ":", "\n", "      ", "if", "word", "[", "0", "]", "not", "in", "prod", "and", "'oled'", "not", "in", "word", "[", "0", "]", ":", "\n", "         ", "subj", "=", "subj", ".", "replace", "(", "word", "[", "0", "]", ",", "''", ")", "\n", "\n", "", "", "if", "pw", "==", "'\u8001\u4eba\u624b\u673a'", ":", "\n", "      ", "subj", "=", "'\u667a\u80fd\u65b9\u4fbf'", "\n", "\n", "", "candidate_subj", "=", "[", "'\u9ad8\u6e05\u62cd\u6444'", ",", "'\u6301\u4e45\u7eed\u822a'", ",", "'\u95ea\u5145'", ",", "'\u8f7b\u8584'", ",", "'\u95ea\u5145'", ",", "'\u9ad8\u6027\u80fd'", ",", "'\u6307\u7eb9\u89e3\u9501'", ",", "'\u8f7b\u8584'", "]", "\n", "if", "len", "(", "subj", ")", ">", "1", ":", "\n", "      ", "return", "subj", "+", "' '", "+", "pw", "\n", "", "else", ":", "#\u5904\u7406\u5356\u70b9\u7f3a\u5931\u95ee\u9898", "\n", "      ", "if", "'\u7535'", "in", "write", ":", "\n", "          ", "return", "'\u95ea\u5145'", "+", "' '", "+", "pw", "\n", "", "elif", "'\u7eed\u822a'", "in", "write", ":", "\n", "          ", "return", "'\u6301\u4e45\u7eed\u822a'", "+", "' '", "+", "pw", "\n", "", "elif", "'\u9ad8\u6e05'", "in", "write", ":", "\n", "          ", "return", "'\u9ad8\u6e05\u62cd\u6444'", "+", "' '", "+", "pw", "\n", "", "elif", "'\u5c4f'", "in", "write", ":", "\n", "          ", "return", "'\u9ad8\u6e05\u5c4f'", "+", "' '", "+", "pw", "\n", "", "elif", "'\u50cf\u7d20'", "in", "write", ":", "\n", "          ", "return", "'\u9ad8\u50cf\u7d20'", "+", "' '", "+", "pw", "\n", "", "elif", "'\u5f95\u5361'", "in", "write", ":", "\n", "          ", "return", "'\u611f\u5149\u5f95\u5361'", "+", "' '", "+", "pw", "\n", "", "else", ":", "\n", "          ", "index", "=", "random", ".", "randint", "(", "0", ",", "len", "(", "candidate_subj", ")", "-", "1", ")", "\n", "return", "candidate_subj", "[", "index", "]", "+", "' '", "+", "pw", "", "", "", "", ""]], "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.produce.postprocess_title.get_brand": [[8, 14], ["re.sub", "re.sub.split", "len"], "function", ["home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.preprocess.process_daren.split"], ["def", "get_brand", "(", "prod", ")", ":", "\n", "  ", "prod", "=", "re", ".", "sub", "(", "u'\\\\\uff08.*?\uff09|\\\\\u3010.*?\u3011'", ",", "\"\"", ",", "prod", ")", "\n", "brand", "=", "prod", ".", "split", "(", "','", ")", "[", "0", "]", "\n", "if", "len", "(", "brand", ")", ">", "6", ":", "\n", "    ", "brand", "=", "brand", "[", ":", "2", "]", "\n", "", "return", "brand", "\n", "\n"]], "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.produce.preprocess.multi_process_run": [[20, 31], ["multiprocessing.Pool", "multiprocessing.Pool.close", "multiprocessing.Pool.join", "multiprocessing.Pool.apply_async", "job.get", "preprocess.clean_"], "function", ["home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.produce.preprocess.clean_"], ["print", "(", "\"output attr idx\"", ",", "output_attr_idx", ")", "\n", "\n", "for", "idx", "in", "range", "(", "1", ",", "len", "(", "lines", ")", ")", ":", "\n", "            ", "if", "lines", "[", "idx", "]", ".", "strip", "(", ")", "==", "\"\"", ":", "\n", "                ", "continue", "\n", "", "line_attrs", "=", "lines", "[", "idx", "]", ".", "strip", "(", ")", ".", "split", "(", "\"\\t\"", ")", "\n", "assert", "attr_name_cnt", "==", "len", "(", "line_attrs", ")", "\n", "input_attr", "=", "line_attrs", "[", "input_attr_idx", "]", "\n", "output_attr", "=", "line_attrs", "[", "output_attr_idx", "]", "\n", "ret_input_collect", ".", "append", "(", "input_attr", ")", "\n", "ret_output_collect", ".", "append", "(", "output_attr", ")", "\n", "", "", "return", "ret_input_collect", ",", "ret_output_collect", "\n"]], "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.produce.preprocess.split_data": [[33, 43], ["len", "int", "range", "data_list.append"], "function", ["None"], ["\n", "", "def", "preprocess", "(", "str_list", ")", ":", "\n", "    ", "ret", "=", "[", "]", "\n", "for", "s", "in", "str_list", ":", "\n", "        ", "ret", ".", "append", "(", "s", ".", "lower", "(", ")", ".", "replace", "(", "\"|\"", ",", "\",\"", ")", ")", "\n", "# ret.append(s.lower().replace(\"|||\", \" [SEP] \"))", "\n", "", "return", "ret", "\n", "\n", "", "def", "tokenize", "(", "str_list", ",", "tokenizer", ")", ":", "\n", "    ", "ret", "=", "[", "]", "\n", "for", "s", "in", "tqdm", ".", "tqdm", "(", "str_list", ")", ":", "\n"]], "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.produce.preprocess.contain_zh": [[45, 55], ["zh_pattern.search"], "function", ["None"], ["ret", ".", "append", "(", "token_list", ")", "\n", "", "return", "ret", "\n", "\n", "", "if", "__name__", "==", "\"__main__\"", ":", "\n", "    ", "parser", "=", "argparse", ".", "ArgumentParser", "(", ")", "\n", "# parser.add_argument(\"--split_name\", default=\"eval\", type=str, help=\"Expect in ['train', 'eval']\")", "\n", "parser", ".", "add_argument", "(", "\"--raw_tsv_dir\"", ",", "default", "=", "\"/export/shenkai/data/dapei/data_v1\"", ",", "type", "=", "str", ")", "\n", "parser", ".", "add_argument", "(", "\"--input_bi\"", ",", "type", "=", "str", ",", "default", "=", "\"title\"", ",", "help", "=", "\"input_bi\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--output_bi\"", ",", "type", "=", "str", ",", "default", "=", "\"attr\"", ",", "help", "=", "\"output_bi\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--nsp_bi\"", ",", "type", "=", "str", ",", "default", "=", "\"nsp\"", ",", "help", "=", "\"nsp_bi\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--output_uni\"", ",", "type", "=", "str", ",", "default", "=", "\"recommend_reason\"", ",", "help", "=", "\"output_uni\"", ")", "\n"]], "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.produce.preprocess.remove_useless": [[56, 65], ["d[].split", "d[].split", "d[].split", "d[].split"], "function", ["home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.preprocess.process_daren.split", "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.preprocess.process_daren.split", "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.preprocess.process_daren.split", "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.preprocess.process_daren.split"], ["parser", ".", "add_argument", "(", "\"--input_s2s\"", ",", "type", "=", "str", ",", "default", "=", "\"input\"", ",", "help", "=", "\"input_s2s\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--output_s2s\"", ",", "type", "=", "str", ",", "default", "=", "\"recommend_reason\"", ",", "help", "=", "\"output_s2s\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--output_dir_path\"", ",", "default", "=", "\"/export/shenkai/data/dapei/data_kai\"", ")", "\n", "args", "=", "parser", ".", "parse_args", "(", ")", "\n", "\n", "## bi: bi.src, bi.tgt, bi.nsp", "\n", "## uni: uni.tgt", "\n", "## s2s: s2s.src, s2s.tgt", "\n", "bi_tsv_path", "=", "os", ".", "path", ".", "join", "(", "args", ".", "raw_tsv_dir", ",", "'bi.tsv'", ")", "\n", "uni_tsv_path", "=", "os", ".", "path", ".", "join", "(", "args", ".", "raw_tsv_dir", ",", "'uni.tsv'", ")", "\n"]], "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.produce.preprocess.get_atributes": [[67, 85], ["load_txt", "item.split", "list", "open", "elements[].split", "set", "f.write", "elements[].split", "elements[].split", "str"], "function", ["home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.postprocess.utils.load_txt", "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.preprocess.process_daren.split", "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.preprocess.process_daren.split", "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.preprocess.process_daren.split", "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.preprocess.process_daren.split"], ["df_bi", "=", "pd", ".", "read_csv", "(", "bi_tsv_path", ",", "sep", "=", "'\\t'", ")", "\n", "df_uni", "=", "pd", ".", "read_csv", "(", "uni_tsv_path", ",", "sep", "=", "'\\t'", ")", "\n", "df_s2s", "=", "pd", ".", "read_csv", "(", "s2s_tsv_path", ",", "sep", "=", "'\\t'", ")", "\n", "bi_src", "=", "list", "(", "df_bi", "[", "args", ".", "input_bi", "]", ")", "\n", "bi_tgt", "=", "list", "(", "df_bi", "[", "args", ".", "output_bi", "]", ")", "\n", "bi_nsp", "=", "list", "(", "df_bi", "[", "args", ".", "nsp_bi", "]", ")", "\n", "uni_tgt", "=", "list", "(", "df_uni", "[", "args", ".", "output_uni", "]", ")", "\n", "s2s_src", "=", "list", "(", "df_s2s", "[", "args", ".", "input_s2s", "]", ")", "\n", "s2s_tgt", "=", "list", "(", "df_s2s", "[", "args", ".", "output_s2s", "]", ")", "\n", "\n", "\n", "# save original data in ori_dir", "\n", "ori_dir", "=", "os", ".", "path", ".", "join", "(", "args", ".", "output_dir_path", ",", "\"original\"", ")", "\n", "os", ".", "makedirs", "(", "ori_dir", ",", "exist_ok", "=", "True", ")", "\n", "with", "open", "(", "os", ".", "path", ".", "join", "(", "ori_dir", ",", "\"bi.src\"", ")", ",", "\"w\"", ")", "as", "f", ":", "\n", "        ", "x_str", "=", "\"\\n\"", ".", "join", "(", "bi_src", ")", "\n", "f", ".", "write", "(", "x_str", ")", "\n", "", "with", "open", "(", "os", ".", "path", ".", "join", "(", "ori_dir", ",", "\"bi.tgt\"", ")", ",", "\"w\"", ")", "as", "f", ":", "\n", "        ", "x_str", "=", "\"\\n\"", ".", "join", "(", "bi_tgt", ")", "\n"]], "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.produce.preprocess.extract_attr": [[87, 101], ["set", "set.split", "new_attr.append", "preprocess.contain_zh", "len"], "function", ["home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.preprocess.process_daren.split", "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.postprocess.utils.contain_zh"], ["", "with", "open", "(", "os", ".", "path", ".", "join", "(", "ori_dir", ",", "\"bi.nsp\"", ")", ",", "\"w\"", ")", "as", "f", ":", "\n", "        ", "x_str", "=", "\"\\n\"", ".", "join", "(", "[", "str", "(", "a", ")", "for", "a", "in", "bi_nsp", "]", ")", "\n", "f", ".", "write", "(", "x_str", ")", "\n", "", "with", "open", "(", "os", ".", "path", ".", "join", "(", "ori_dir", ",", "\"uni.tgt\"", ")", ",", "\"w\"", ")", "as", "f", ":", "\n", "        ", "x_str", "=", "\"\\n\"", ".", "join", "(", "uni_tgt", ")", "\n", "f", ".", "write", "(", "x_str", ")", "\n", "", "with", "open", "(", "os", ".", "path", ".", "join", "(", "ori_dir", ",", "\"s2s.src\"", ")", ",", "\"w\"", ")", "as", "f", ":", "\n", "        ", "x_str", "=", "\"\\n\"", ".", "join", "(", "s2s_src", ")", "\n", "f", ".", "write", "(", "x_str", ")", "\n", "", "with", "open", "(", "os", ".", "path", ".", "join", "(", "ori_dir", ",", "\"s2s.tgt\"", ")", ",", "\"w\"", ")", "as", "f", ":", "\n", "        ", "x_str", "=", "\"\\n\"", ".", "join", "(", "s2s_tgt", ")", "\n", "f", ".", "write", "(", "x_str", ")", "\n", "\n", "\n", "\n"]], "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.produce.preprocess.clean_": [[102, 122], ["preprocess.remove_useless", "prod_desc.replace.replace", "re.sub", "clean_prod.replace.replace", "clean_data.append", "item.split", "preprocess.extract_attr", "preprocess.extract_attr", "extract_attr.replace", "print"], "function", ["home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.produce.preprocess.remove_useless", "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.preprocess.process_daren.split", "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.produce.preprocess.extract_attr", "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.produce.preprocess.extract_attr"], ["\n", "", "bi_src", "=", "preprocess", "(", "bi_src", ")", "\n", "bi_tgt", "=", "preprocess", "(", "bi_tgt", ")", "\n", "uni_tgt", "=", "preprocess", "(", "uni_tgt", ")", "\n", "s2s_src", "=", "preprocess", "(", "s2s_src", ")", "\n", "s2s_tgt", "=", "preprocess", "(", "s2s_tgt", ")", "\n", "\n", "\n", "# then apply tokenize", "\n", "\n", "from", "transformers", "import", "BertTokenizer", "\n", "\n", "tokenizer", "=", "BertTokenizer", ".", "from_pretrained", "(", "\"bert-base-chinese\"", ")", "\n", "\n", "processed_dir", "=", "os", ".", "path", ".", "join", "(", "args", ".", "output_dir_path", ",", "\"processed\"", ")", "\n", "os", ".", "makedirs", "(", "processed_dir", ",", "exist_ok", "=", "True", ")", "\n", "\n", "\n", "print", "(", "\"Tokenizing bi src\"", ")", "\n", "bi_src_tokenized", "=", "tokenize", "(", "bi_src", ",", "tokenizer", ")", "\n", "with", "open", "(", "os", ".", "path", ".", "join", "(", "processed_dir", ",", "\"bi.src\"", ")", ",", "\"w\"", ")", "as", "f", ":", "\n"]], "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.produce.preprocess.main": [[123, 145], ["preprocess.split_data", "preprocess.multi_process_run", "list", "print", "open", "f.readlines", "set", "open", "f.write", "str", "f.write", "len"], "function", ["home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.postprocess.utils.split_data", "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.postprocess.utils.multi_process_run"], ["        ", "tokenized_str", "=", "\"\\n\"", ".", "join", "(", "[", "\" \"", ".", "join", "(", "tokens", ")", "for", "tokens", "in", "bi_src_tokenized", "]", ")", "\n", "f", ".", "write", "(", "tokenized_str", ")", "\n", "\n", "", "print", "(", "\"Tokenizing bi tgt\"", ")", "\n", "bi_tgt_tokenized", "=", "tokenize", "(", "bi_tgt", ",", "tokenizer", ")", "\n", "with", "open", "(", "os", ".", "path", ".", "join", "(", "processed_dir", ",", "\"bi.tgt\"", ")", ",", "\"w\"", ")", "as", "f", ":", "\n", "        ", "tokenized_str", "=", "\"\\n\"", ".", "join", "(", "[", "\" \"", ".", "join", "(", "tokens", ")", "for", "tokens", "in", "bi_tgt_tokenized", "]", ")", "\n", "f", ".", "write", "(", "tokenized_str", ")", "\n", "\n", "", "with", "open", "(", "os", ".", "path", ".", "join", "(", "processed_dir", ",", "\"bi.nsp\"", ")", ",", "\"w\"", ")", "as", "f", ":", "\n", "        ", "x_str", "=", "\"\\n\"", ".", "join", "(", "[", "str", "(", "a", ")", "for", "a", "in", "bi_nsp", "]", ")", "\n", "f", ".", "write", "(", "x_str", ")", "\n", "\n", "", "print", "(", "\"Tokenizing uni tgt\"", ")", "\n", "uni_tgt_tokenized", "=", "tokenize", "(", "uni_tgt", ",", "tokenizer", ")", "\n", "with", "open", "(", "os", ".", "path", ".", "join", "(", "processed_dir", ",", "\"uni.tgt\"", ")", ",", "\"w\"", ")", "as", "f", ":", "\n", "        ", "tokenized_str", "=", "\"\\n\"", ".", "join", "(", "[", "\" \"", ".", "join", "(", "tokens", ")", "for", "tokens", "in", "uni_tgt_tokenized", "]", ")", "\n", "f", ".", "write", "(", "tokenized_str", ")", "\n", "\n", "", "print", "(", "\"Tokenizing s2s src\"", ")", "\n", "s2s_src_tokenized", "=", "tokenize", "(", "s2s_src", ",", "tokenizer", ")", "\n", "with", "open", "(", "os", ".", "path", ".", "join", "(", "processed_dir", ",", "\"s2s.src\"", ")", ",", "\"w\"", ")", "as", "f", ":", "\n", "        ", "tokenized_str", "=", "\"\\n\"", ".", "join", "(", "[", "\" \"", ".", "join", "(", "tokens", ")", "for", "tokens", "in", "s2s_src_tokenized", "]", ")", "\n"]], "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.postprocess.filter_aggregate.if_match": [[24, 30], ["product.lower"], "function", ["None"], ["def", "if_match", "(", "number", ",", "product", ")", ":", "\n", "    ", "prod", "=", "product", ".", "lower", "(", ")", "\n", "for", "num", "in", "number", ":", "\n", "      ", "if", "num", "not", "in", "prod", ":", "\n", "         ", "return", "False", "\n", "", "", "return", "True", "\n", "\n"]], "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.postprocess.filter_aggregate.filter_part": [[32, 60], ["utils.load_csv_sku_aspect", "print", "utils.split_data", "print", "utils.multi_process_run", "utils.multi_process_run", "utils.multi_process_run", "utils.multi_process_run", "utils.multi_process_run", "utils.multi_process_run", "open", "str", "str", "f.write", "len", "len"], "function", ["home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.postprocess.utils.load_csv_sku_aspect", "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.postprocess.utils.split_data", "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.postprocess.utils.multi_process_run", "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.postprocess.utils.multi_process_run", "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.postprocess.utils.multi_process_run", "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.postprocess.utils.multi_process_run", "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.postprocess.utils.multi_process_run", "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.postprocess.utils.multi_process_run"], ["", "def", "filter_part", "(", ")", ":", "\n", "    ", "'''detect and filter the generated write for each aspect of each product\n    require the input is .csv or txt with format of \"sku|||product|||pred'''", "\n", "data", "=", "load_csv_sku_aspect", "(", "args", ".", "data_path", ")", "\n", "print", "(", "str", "(", "len", "(", "data", ")", ")", "+", "' data is loaded!'", ")", "\n", "data_list", "=", "split_data", "(", "data", ",", "args", ".", "n", ")", "\n", "data_list", "=", "[", "(", "i", ",", ")", "for", "i", "in", "data_list", "]", "\n", "if", "args", ".", "sec_id", "==", "'653'", ":", "\n", "         ", "res", "=", "multi_process_run", "(", "data_list", ",", "filter_part_653", ",", "args", ".", "n", ")", "\n", "", "if", "args", ".", "sec_id", "==", "'1343'", ":", "\n", "         ", "res", "=", "multi_process_run", "(", "data_list", ",", "filter_part_1343", ",", "args", ".", "n", ")", "\n", "", "if", "args", ".", "sec_id", "==", "'1342'", ":", "\n", "         ", "res", "=", "multi_process_run", "(", "data_list", ",", "filter_part_1342", ",", "args", ".", "n", ")", "\n", "", "if", "args", ".", "sec_id", "==", "'671'", ":", "\n", "         ", "res", "=", "multi_process_run", "(", "data_list", ",", "filter_part_671", ",", "args", ".", "n", ")", "\n", "", "if", "args", ".", "sec_id", "==", "'1381'", ":", "\n", "         ", "res", "=", "multi_process_run", "(", "data_list", ",", "filter_part_1381", ",", "args", ".", "n", ")", "\n", "", "if", "args", ".", "sec_id", "==", "'794'", ":", "\n", "         ", "res", "=", "multi_process_run", "(", "data_list", ",", "filter_part_794", ",", "args", ".", "n", ")", "\n", "#print(len(res))", "\n", "", "save_data", "=", "[", "]", "\n", "for", "part_res", "in", "res", ":", "\n", "        ", "save_data", "+=", "part_res", "\n", "\n", "", "print", "(", "'total qualified writes: '", "+", "str", "(", "len", "(", "save_data", ")", ")", ")", "\n", "with", "open", "(", "args", ".", "save_path", ",", "'w'", ",", "encoding", "=", "'utf-8'", ")", "as", "f", ":", "\n", "        ", "for", "item", "in", "save_data", ":", "\n", "            ", "f", ".", "write", "(", "item", "+", "'\\n'", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.postprocess.filter_aggregate.group_data": [[64, 81], ["new_data.append", "current_write.append", "current_write.append", "item.split", "len", "new_data.append", "item.split", "item.split", "item.split", "item.split"], "function", ["home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.preprocess.process_daren.split", "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.preprocess.process_daren.split", "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.preprocess.process_daren.split", "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.preprocess.process_daren.split", "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.preprocess.process_daren.split"], ["", "", "", "def", "group_data", "(", "data", ")", ":", "\n", "    ", "new_data", "=", "[", "]", "\n", "current_prod", "=", "''", "\n", "current_write", "=", "[", "]", "\n", "current_sku", "=", "''", "\n", "for", "item", "in", "data", ":", "\n", "        ", "if", "item", ".", "split", "(", "'|||'", ")", "[", "2", "]", "==", "current_prod", ":", "\n", "            ", "current_write", ".", "append", "(", "item", ".", "split", "(", "'|||'", ")", "[", "3", "]", ")", "\n", "", "else", ":", "\n", "            ", "if", "len", "(", "current_prod", ")", ">", "0", ":", "\n", "                ", "new_data", ".", "append", "(", "[", "current_sku", ",", "current_prod", ",", "current_write", "]", ")", "\n", "", "current_prod", "=", "item", ".", "split", "(", "'|||'", ")", "[", "2", "]", "\n", "current_sku", "=", "item", ".", "split", "(", "'|||'", ")", "[", "0", "]", "\n", "current_write", "=", "[", "]", "\n", "current_write", ".", "append", "(", "item", ".", "split", "(", "'|||'", ")", "[", "3", "]", ")", "\n", "", "", "new_data", ".", "append", "(", "[", "current_sku", ",", "current_prod", ",", "current_write", "]", ")", "#put the last one into new data ", "\n", "return", "new_data", "\n", "\n"]], "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.postprocess.filter_aggregate.remove_deplicate": [[83, 95], ["range", "len", "sents[].split", "new_sents.append", "new_sent_l.append", "record.append"], "function", ["home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.preprocess.process_daren.split"], ["", "def", "remove_deplicate", "(", "sents", ")", ":", "\n", "    ", "record", "=", "[", "]", "\n", "new_sents", "=", "[", "]", "\n", "for", "i", "in", "range", "(", "len", "(", "sents", ")", ")", ":", "\n", "        ", "sent_l", "=", "sents", "[", "i", "]", ".", "split", "(", "'\uff0c'", ")", "\n", "new_sent_l", "=", "[", "]", "\n", "for", "sent", "in", "sent_l", ":", "\n", "            ", "if", "sent", "not", "in", "record", ":", "\n", "                ", "new_sent_l", ".", "append", "(", "sent", ")", "\n", "record", ".", "append", "(", "sent", ")", "\n", "", "", "new_sents", ".", "append", "(", "'\uff0c'", ".", "join", "(", "new_sent_l", ")", ")", "\n", "", "return", "new_sents", "\n", "\n"]], "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.postprocess.filter_aggregate.aggregate_": [[97, 178], ["numpy.random.seed", "numpy.array", "range", "len", "numpy.random.choice", "len", "filter_aggregate.remove_deplicate", "np.array.ravel", "np.random.choice.join", "len", "new_data.append", "len", "len", "inner_sent.append", "others.append", "len", "np.random.choice.join", "len", "len", "new_data.append", "re.findall", "len", "first_sent.append", "re.sub", "others.append", "len", "np.random.choice.join", "len", "len", "len"], "function", ["home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.postprocess.filter_aggregate.remove_deplicate"], ["", "def", "aggregate_", "(", "data", ")", ":", "\n", "    ", "np", ".", "random", ".", "seed", "(", "0", ")", "\n", "p", "=", "np", ".", "array", "(", "[", "0.9", ",", "0.06", ",", "0.04", "]", ")", "#the probability of each connector", "\n", "connector_l", "=", "[", "'\u3002'", ",", "'\uff0c'", ",", "'\uff1b'", "]", "# the list of possible connector", "\n", "new_data", "=", "[", "]", "\n", "\n", "for", "i", "in", "range", "(", "len", "(", "data", ")", ")", ":", "\n", "        ", "prod", "=", "data", "[", "i", "]", "[", "1", "]", "\n", "write", "=", "data", "[", "i", "]", "[", "2", "]", "\n", "sku", "=", "data", "[", "i", "]", "[", "0", "]", "\n", "#choose a connecter:", "\n", "connector", "=", "np", ".", "random", ".", "choice", "(", "connector_l", ",", "p", "=", "p", ".", "ravel", "(", ")", ")", "\n", "#find the first sentence", "\n", "if", "args", ".", "sec_id", "==", "'653'", ":", "\n", "              ", "first_patterns", "=", "[", "\"\\w+\u51fa\u54c1\\w+\uff0c\"", ",", "\"^\u8fd9\u6b3e\u624b\u673a\"", ",", "\"^\u8fd9\u6b3e\u4ea7\u54c1\"", ",", "\"^\u624b\u673a\"", ",", "\"\u8be5\u624b\u673a\"", "]", "\n", "", "elif", "args", ".", "sec_id", "==", "'1343'", ":", "\n", "              ", "first_patterns", "=", "[", "\"^\u8fd9\u6b3e\\w+[\u91c7\uff5c\u9009]\u7528\"", "]", "\n", "", "elif", "args", ".", "sec_id", "==", "'1342'", ":", "\n", "              ", "first_patterns", "=", "[", "\"^\u8fd9\u6b3e\\w+\"", "]", "\n", "", "elif", "args", ".", "sec_id", "==", "'671'", ":", "\n", "              ", "first_patterns", "=", "[", "\"\\w+\u51fa\u54c1\\w+\uff0c\"", ",", "\"^\u8fd9\u6b3e\"", ",", "\"^\u7b14\u8bb0\u672c\"", ",", "\"^\u8be5\"", ",", "\"^\u6b64\u6b3e\"", "]", "\n", "", "elif", "args", ".", "sec_id", "==", "'1381'", ":", "\n", "              ", "first_patterns", "=", "[", "\"^\u8fd9\u6b3e\"", ",", "\"^\u8be5\"", ",", "\"^\u6b64\u6b3e\"", ",", "\"^\u8fd9\u4e2a\"", "]", "\n", "", "elif", "args", ".", "sec_id", "==", "'794'", ":", "\n", "              ", "first_patterns", "=", "[", "\"^\u8fd9\u6b3e\"", ",", "\"^\u8be5\"", "]", "\n", "", "first_sent", "=", "[", "]", "\n", "inner_sent", "=", "[", "]", "\n", "others", "=", "[", "]", "\n", "\n", "for", "sent", "in", "write", ":", "\n", "            ", "key", "=", "0", "\n", "#check if first sentence", "\n", "for", "pattern", "in", "first_patterns", ":", "\n", "#print(re.findall(p, sent))", "\n", "                ", "if", "len", "(", "re", ".", "findall", "(", "pattern", ",", "sent", ")", ")", ">", "0", ":", "\n", "                    ", "key", "=", "1", "#indicate this sentence has been dealt with", "\n", "\n", "if", "len", "(", "first_sent", ")", "==", "0", ":", "\n", "                        ", "first_sent", ".", "append", "(", "sent", ")", "\n", "", "else", ":", "\n", "                        ", "sent", "=", "re", ".", "sub", "(", "pattern", ",", "\"\"", ",", "sent", ")", "\n", "others", ".", "append", "(", "sent", ")", "\n", "#break", "\n", "#check if the inner sent", "\n", "", "", "", "if", "key", "==", "0", ":", "\n", "                ", "if", "\"\u6b64\u5916\"", "in", "sent", "or", "\"\u8fd8\u6709\"", "in", "sent", "or", "\"\u540c\u65f6\"", "in", "sent", "or", "\"\u53e6\u5916\"", "in", "sent", "or", "'\u8fd8'", "in", "sent", "or", "'\u53ef\u4ee5'", "in", "sent", ":", "\n", "                    ", "inner_sent", ".", "append", "(", "sent", ")", "\n", "", "else", ":", "\n", "                    ", "others", ".", "append", "(", "sent", ")", "\n", "#\u7ed3\u5408\u51e0\u4e2a\u53e5\u5b50", "\n", "", "", "", "all_sent", "=", "first_sent", "+", "others", "+", "inner_sent", "\n", "#print(all_sent)", "\n", "if", "len", "(", "all_sent", ")", "<", "3", ":", "\n", "                    ", "all_sent", "=", "remove_deplicate", "(", "all_sent", ")", "\n", "new_write", "=", "connector", ".", "join", "(", "all_sent", ")", "+", "'\u3002'", "\n", "if", "len", "(", "new_write", ")", ">", "55", ":", "\n", "                        ", "new_data", ".", "append", "(", "sku", "+", "'|||'", "+", "new_write", "+", "'|||'", "+", "prod", ")", "\n", "", "", "else", ":", "\n", "            ", "if", "len", "(", "all_sent", ")", "==", "3", ":", "\n", "                     ", "option2", "=", "[", "(", "0", ",", "1", ",", "2", ")", "]", "\n", "", "elif", "len", "(", "all_sent", ")", "==", "4", ":", "\n", "                     ", "option2", "=", "[", "(", "0", ",", "1", ",", "3", ")", ",", "(", "1", ",", "2", ",", "3", ")", "]", "\n", "", "elif", "len", "(", "all_sent", ")", "==", "5", ":", "\n", "                     ", "option2", "=", "[", "(", "0", ",", "2", ",", "4", ")", ",", "(", "1", ",", "3", ",", "4", ")", "]", "\n", "", "elif", "len", "(", "all_sent", ")", "==", "6", ":", "\n", "                     ", "option2", "=", "[", "(", "0", ",", "3", ",", "5", ")", ",", "(", "1", ",", "2", ",", "4", ")", "]", "\n", "", "elif", "len", "(", "all_sent", ")", "==", "7", ":", "\n", "                     ", "option2", "=", "[", "(", "0", ",", "1", ",", "5", ")", ",", "(", "1", ",", "2", ",", "4", ")", ",", "(", "2", ",", "5", ",", "4", ")", "]", "\n", "", "elif", "len", "(", "all_sent", ")", "==", "8", ":", "\n", "                     ", "option2", "=", "[", "(", "0", ",", "2", ",", "6", ")", ",", "(", "1", ",", "4", ",", "7", ")", ",", "(", "1", ",", "3", ",", "5", ")", "]", "\n", "\n", "", "for", "option", "in", "option2", ":", "\n", "                ", "selected_sent", "=", "[", "all_sent", "[", "option", "[", "0", "]", "]", ",", "all_sent", "[", "option", "[", "1", "]", "]", ",", "all_sent", "[", "option", "[", "2", "]", "]", "]", "\n", "new_write", "=", "connector", ".", "join", "(", "[", "selected_sent", "[", "0", "]", ",", "selected_sent", "[", "1", "]", ",", "selected_sent", "[", "2", "]", "]", ")", "+", "'\u3002'", "\n", "if", "len", "(", "new_write", ")", ">", "100", ":", "\n", "                    ", "new_write", "=", "connector", ".", "join", "(", "[", "selected_sent", "[", "0", "]", ",", "selected_sent", "[", "1", "]", "]", ")", "+", "'\u3002'", "\n", "", "if", "len", "(", "new_write", ")", ">", "55", ":", "\n", "                    ", "new_data", ".", "append", "(", "sku", "+", "'|||'", "+", "new_write", "+", "'|||'", "+", "prod", ")", "\n", "\n", "", "", "", "", "return", "new_data", "\n", "\n"]], "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.postprocess.filter_aggregate.aggregate": [[180, 205], ["utils.load_txt", "filter_aggregate.group_data", "utils.split_data", "utils.multi_process_run", "print", "pandas.DataFrame", "range", "result.append.to_csv", "len", "save_data[].split", "result.append.append", "str", "pandas.DataFrame", "len"], "function", ["home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.postprocess.utils.load_txt", "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.postprocess.filter_aggregate.group_data", "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.postprocess.utils.split_data", "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.postprocess.utils.multi_process_run", "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.preprocess.process_daren.split"], ["", "def", "aggregate", "(", ")", ":", "\n", "    ", "'''put the write of the different aspect from the same input into one sentence\n    require the input document is .csv with format of \"sku|||aspect|||product|||pred\"\n    '''", "\n", "\n", "data", "=", "load_txt", "(", "args", ".", "data_path", ")", "\n", "data", "=", "group_data", "(", "data", ")", "#group the write for the same product into one data", "\n", "data_list", "=", "split_data", "(", "data", ",", "args", ".", "n", ")", "\n", "data_list", "=", "[", "(", "i", ",", ")", "for", "i", "in", "data_list", "]", "\n", "res", "=", "multi_process_run", "(", "data_list", ",", "aggregate_", ",", "args", ".", "n", ")", "\n", "save_data", "=", "[", "]", "\n", "for", "part_res", "in", "res", ":", "\n", "        ", "save_data", "+=", "part_res", "\n", "\n", "", "print", "(", "'total qualified writes: '", "+", "str", "(", "len", "(", "save_data", ")", ")", ")", "\n", "\n", "result", "=", "pd", ".", "DataFrame", "(", "columns", "=", "(", "'product'", ",", "'write'", ",", "'sku'", ")", ")", "\n", "for", "i", "in", "range", "(", "len", "(", "save_data", ")", ")", ":", "\n", "        ", "item", "=", "save_data", "[", "i", "]", ".", "split", "(", "'|||'", ")", "\n", "sku", "=", "item", "[", "0", "]", "\n", "product", "=", "item", "[", "2", "]", "\n", "write", "=", "item", "[", "1", "]", "\n", "sku_title_write", "=", "write", "+", "'+'", "+", "product", "\n", "result", "=", "result", ".", "append", "(", "pd", ".", "DataFrame", "(", "{", "'sku'", ":", "[", "sku", "]", ",", "'product'", ":", "[", "product", "]", ",", "'write'", ":", "[", "write", "]", ",", "'sku_title_write'", ":", "[", "sku_title_write", "]", "}", ")", ")", "\n", "", "result", ".", "to_csv", "(", "args", ".", "save_path", ",", "sep", "=", "','", ",", "index", "=", "False", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.postprocess.filter_aggregate.main": [[214, 221], ["filter_aggregate.filter_part", "filter_aggregate.aggregate", "NotImplementedError"], "function", ["home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.postprocess.filter_aggregate.filter_part", "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.postprocess.filter_aggregate.aggregate"], ["", "def", "main", "(", ")", ":", "\n", "    ", "if", "args", ".", "task", "==", "\"filter_part\"", ":", "\n", "        ", "filter_part", "(", ")", "\n", "", "elif", "args", ".", "task", "==", "\"aggregate\"", ":", "\n", "        ", "aggregate", "(", ")", "\n", "", "else", ":", "\n", "        ", "raise", "NotImplementedError", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.postprocess.utils.classify": [[17, 38], ["utils.class_by_keyword", "dic.doc2bow", "lda_model.get_document_topics", "sorted"], "function", ["home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.evaluation.utils.class_by_keyword"], ["def", "classify", "(", "tokens", ",", "lda_model", ",", "dic", ")", ":", "\n", "     ", "c_k", "=", "class_by_keyword", "(", "tokens", ")", "\n", "if", "c_k", ">=", "0", ":", "\n", "        ", "return", "c_k", "\n", "", "else", ":", "\n", "        ", "corpus", "=", "dic", ".", "doc2bow", "(", "tokens", ")", "\n", "row", "=", "lda_model", ".", "get_document_topics", "(", "corpus", ")", "\n", "row", "=", "sorted", "(", "row", ",", "key", "=", "lambda", "x", ":", "(", "x", "[", "1", "]", ")", ",", "reverse", "=", "True", ")", "\n", "orig_class", "=", "row", "[", "0", "]", "[", "0", "]", "\n", "if", "orig_class", "in", "[", "11", "]", ":", "\n", "                ", "return", "0", "\n", "", "if", "orig_class", "in", "[", "5", ",", "6", ",", "8", "]", ":", "\n", "                ", "return", "3", "\n", "", "if", "orig_class", "in", "[", "2", "]", ":", "\n", "                ", "return", "2", "\n", "", "if", "orig_class", "in", "[", "0", ",", "1", ",", "3", ",", "9", "]", ":", "\n", "                ", "return", "4", "\n", "", "if", "orig_class", "in", "[", "4", "]", ":", "\n", "                ", "return", "1", "\n", "", "if", "orig_class", "in", "[", "10", ",", "7", "]", ":", "\n", "                ", "return", "5", "\n", "\n"]], "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.postprocess.utils.classify_1343": [[39, 62], ["utils.class_by_keyword_1314", "dic.doc2bow", "lda_model.get_document_topics", "sorted"], "function", ["home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.postprocess.utils.class_by_keyword_1314"], ["", "", "", "def", "classify_1343", "(", "tokens", ",", "lda_model", ",", "dic", ")", ":", "\n", "     ", "c_k", "=", "class_by_keyword_1314", "(", "tokens", ")", "\n", "if", "c_k", ">=", "0", ":", "\n", "        ", "return", "c_k", "\n", "", "else", ":", "\n", "        ", "corpus", "=", "dic", ".", "doc2bow", "(", "tokens", ")", "\n", "row", "=", "lda_model", ".", "get_document_topics", "(", "corpus", ")", "\n", "row", "=", "sorted", "(", "row", ",", "key", "=", "lambda", "x", ":", "(", "x", "[", "1", "]", ")", ",", "reverse", "=", "True", ")", "\n", "orig_class", "=", "row", "[", "0", "]", "[", "0", "]", "\n", "if", "orig_class", "in", "[", "0", ",", "2", ",", "6", ",", "11", "]", ":", "\n", "                ", "return", "0", "\n", "", "if", "orig_class", "in", "[", "1", "]", ":", "\n", "                ", "return", "1", "\n", "", "if", "orig_class", "in", "[", "7", ",", "8", "]", ":", "\n", "                ", "return", "2", "\n", "", "if", "orig_class", "in", "[", "5", "]", ":", "\n", "                ", "return", "3", "\n", "", "if", "orig_class", "in", "[", "10", "]", ":", "\n", "                ", "return", "5", "\n", "", "if", "orig_class", "in", "[", "12", "]", ":", "\n", "                ", "return", "6", "\n", "", "if", "orig_class", "in", "[", "3", ",", "4", ",", "9", "]", ":", "\n", "                ", "return", "8", "\n", "\n"]], "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.postprocess.utils.load_json": [[63, 66], ["open", "json.load"], "function", ["None"], ["", "", "", "def", "load_json", "(", "path", ")", ":", "\n", "    ", "with", "open", "(", "path", ",", "\"r\"", ",", "encoding", "=", "\"utf8\"", ")", "as", "f", ":", "\n", "        ", "return", "json", ".", "load", "(", "f", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.postprocess.utils.dump_json": [[68, 71], ["open", "json.dump"], "function", ["None"], ["", "", "def", "dump_json", "(", "path", ",", "data", ",", "indent", "=", "4", ")", ":", "\n", "    ", "with", "open", "(", "path", ",", "\"w\"", ",", "encoding", "=", "\"utf8\"", ")", "as", "f", ":", "\n", "        ", "json", ".", "dump", "(", "data", ",", "f", ",", "indent", "=", "indent", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.postprocess.utils.load_pickle": [[73, 76], ["open", "pickle.load"], "function", ["None"], ["", "", "def", "load_pickle", "(", "path", ")", ":", "\n", "    ", "with", "open", "(", "path", ",", "\"rb\"", ")", "as", "f", ":", "\n", "        ", "return", "pickle", ".", "load", "(", "f", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.postprocess.utils.dump_pickle": [[78, 81], ["open", "pickle.dump"], "function", ["None"], ["", "", "def", "dump_pickle", "(", "path", ",", "data", ")", ":", "\n", "    ", "with", "open", "(", "path", ",", "\"wb\"", ")", "as", "f", ":", "\n", "        ", "pickle", ".", "dump", "(", "data", ",", "f", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.postprocess.utils.load_csv": [[82, 88], ["pandas.read_csv", "pd.read_csv.iterrows", "data.append"], "function", ["None"], ["", "", "def", "load_csv", "(", "path", ")", ":", "\n", "    ", "data", "=", "[", "]", "\n", "df", "=", "pd", ".", "read_csv", "(", "path", ")", "\n", "for", "row", "in", "df", ".", "iterrows", "(", ")", ":", "\n", "      ", "data", ".", "append", "(", "row", "[", "1", "]", "[", "'product'", "]", "+", "'|||'", "+", "row", "[", "1", "]", "[", "'pred'", "]", ")", "\n", "", "return", "data", "\n", "\n"]], "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.postprocess.utils.load_csv_sku": [[89, 95], ["pandas.read_csv", "pd.read_csv.iterrows", "data.append", "str"], "function", ["None"], ["", "def", "load_csv_sku", "(", "path", ")", ":", "\n", "    ", "data", "=", "[", "]", "\n", "df", "=", "pd", ".", "read_csv", "(", "path", ")", "\n", "for", "row", "in", "df", ".", "iterrows", "(", ")", ":", "\n", "      ", "data", ".", "append", "(", "row", "[", "1", "]", "[", "'product'", "]", "+", "'|||'", "+", "row", "[", "1", "]", "[", "'pred'", "]", "+", "'|||'", "+", "str", "(", "row", "[", "1", "]", "[", "'sku'", "]", ")", ")", "\n", "", "return", "data", "\n", "\n"]], "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.postprocess.utils.load_csv_sku_aspect": [[96, 107], ["pandas.read_csv", "pd.read_csv.iterrows", "[].split", "[].split", "data.append", "print", "str"], "function", ["home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.preprocess.process_daren.split", "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.preprocess.process_daren.split"], ["", "def", "load_csv_sku_aspect", "(", "path", ")", ":", "\n", "    ", "data", "=", "[", "]", "\n", "df", "=", "pd", ".", "read_csv", "(", "path", ")", "\n", "for", "row", "in", "df", ".", "iterrows", "(", ")", ":", "\n", "      ", "prod", "=", "row", "[", "1", "]", "[", "'product'", "]", ".", "split", "(", "'|||'", ")", "[", "1", "]", "\n", "aspect", "=", "row", "[", "1", "]", "[", "'product'", "]", ".", "split", "(", "'|||'", ")", "[", "0", "]", "\n", "try", ":", "\n", "          ", "data", ".", "append", "(", "aspect", "+", "'|||'", "+", "prod", "+", "'|||'", "+", "row", "[", "1", "]", "[", "'pred'", "]", "+", "'|||'", "+", "str", "(", "row", "[", "1", "]", "[", "'sku'", "]", ")", ")", "\n", "", "except", ":", "\n", "          ", "print", "(", "'failed generation!'", ")", "\n", "", "", "return", "data", "\n", "\n"]], "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.postprocess.utils.load_txt": [[108, 115], ["open", "line.strip.strip", "data.append"], "function", ["None"], ["", "def", "load_txt", "(", "path", ")", ":", "\n", "    ", "data", "=", "[", "]", "\n", "with", "open", "(", "path", ",", "\"r\"", ",", "encoding", "=", "\"utf8\"", ")", "as", "f", ":", "\n", "        ", "for", "line", "in", "f", ":", "\n", "            ", "line", "=", "line", ".", "strip", "(", ")", "\n", "data", ".", "append", "(", "line", ")", "\n", "", "", "return", "data", "\n", "\n"]], "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.postprocess.utils.dump_txt": [[117, 121], ["open", "f.write"], "function", ["None"], ["", "def", "dump_txt", "(", "path", ",", "data", ")", ":", "\n", "    ", "with", "open", "(", "path", ",", "\"w\"", ",", "encoding", "=", "\"utf8\"", ")", "as", "f", ":", "\n", "        ", "for", "line", "in", "data", ":", "\n", "            ", "f", ".", "write", "(", "line", "+", "\"\\n\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.postprocess.utils.make_dir_path": [[123, 126], ["os.makedirs", "os.path.exists"], "function", ["None"], ["", "", "", "def", "make_dir_path", "(", "dir_path", ")", ":", "\n", "    ", "if", "dir_path", "and", "not", "os", ".", "path", ".", "exists", "(", "dir_path", ")", ":", "\n", "        ", "os", ".", "makedirs", "(", "dir_path", ",", "exist_ok", "=", "True", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.postprocess.utils.make_path": [[128, 132], ["os.path.dirname", "utils.make_dir_path"], "function", ["home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.postprocess.utils.make_dir_path"], ["", "", "def", "make_path", "(", "f", ")", ":", "\n", "    ", "'f should be a file path instead of dir'", "\n", "d", "=", "os", ".", "path", ".", "dirname", "(", "f", ")", "\n", "make_dir_path", "(", "d", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.postprocess.utils.multi_process_run": [[134, 145], ["multiprocessing.Pool", "multiprocessing.Pool.close", "multiprocessing.Pool.join", "multiprocessing.Pool.apply_async", "job.get"], "function", ["None"], ["", "def", "multi_process_run", "(", "arg_list", ",", "fun", ",", "n_process", "=", "4", ")", ":", "\n", "    ", "pool", "=", "Pool", "(", "processes", "=", "n_process", ")", "\n", "jobs", "=", "[", "pool", ".", "apply_async", "(", "fun", ",", "args", "=", "n", ")", "for", "n", "in", "arg_list", "]", "\n", "\n", "pool", ".", "close", "(", ")", "\n", "pool", ".", "join", "(", ")", "\n", "\n", "try", ":", "# the fun doesn't have a non-None return value", "\n", "        ", "return", "[", "job", ".", "get", "(", ")", "for", "job", "in", "jobs", "]", "\n", "", "except", ":", "\n", "        ", "return", "[", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.postprocess.utils.split_data": [[147, 157], ["len", "int", "range", "data_list.append"], "function", ["None"], ["", "", "def", "split_data", "(", "data", ",", "n", ")", ":", "\n", "    ", "l", "=", "len", "(", "data", ")", "\n", "sl", "=", "int", "(", "l", "/", "n", ")", "\n", "if", "sl", "*", "n", "!=", "l", ":", "\n", "        ", "sl", "+=", "1", "\n", "", "data_list", "=", "[", "]", "\n", "\n", "for", "i", "in", "range", "(", "0", ",", "l", ",", "sl", ")", ":", "\n", "        ", "data_list", ".", "append", "(", "data", "[", "i", ":", "i", "+", "sl", "]", ")", "\n", "", "return", "data_list", "\n", "\n"]], "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.postprocess.utils.contain_zh": [[162, 172], ["zh_pattern.search"], "function", ["None"], ["def", "contain_zh", "(", "word", ")", ":", "\n", "    ", "'''\n    \u5224\u65ad\u4f20\u5165\u5b57\u7b26\u4e32\u662f\u5426\u5305\u542b\u4e2d\u6587\n    :param word: \u5f85\u5224\u65ad\u5b57\u7b26\u4e32\n    :return: True:\u5305\u542b\u4e2d\u6587  False:\u4e0d\u5305\u542b\u4e2d\u6587\n    '''", "\n", "global", "zh_pattern", "\n", "match", "=", "zh_pattern", ".", "search", "(", "word", ")", "\n", "\n", "return", "match", "\n", "\n"]], "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.postprocess.utils.class_by_keyword": [[173, 191], ["len", "len", "set", "set", "len", "set", "set", "len", "set", "set", "len", "set", "set", "len", "set", "set", "len", "set", "set", "set", "set"], "function", ["None"], ["", "def", "class_by_keyword", "(", "tokens", ")", ":", "\n", "    ", "if", "len", "(", "set", "(", "[", "'\u673a\u8eab'", ",", "'\u5916\u89c2'", ",", "'\u5916\u58f3'", "]", ")", "&", "set", "(", "tokens", ")", ")", ">", "0", ":", "\n", "        ", "a", "=", "0", "\n", "", "elif", "len", "(", "set", "(", "[", "'\u9762\u5bb9'", ",", "'\u89e3\u9501'", ",", "'\u6307\u7eb9'", ",", "'\u4eba\u8138\u8bc6\u522b'", ",", "'\u9762\u5bb9ID'", "]", ")", "&", "set", "(", "tokens", ")", ")", ">", "0", ":", "\n", "        ", "a", "=", "6", "\n", "", "elif", "len", "(", "set", "(", "[", "'\u5c4f'", ",", "'\u5c4f\u5e55'", ",", "'\u82f1\u5bf8'", "]", ")", "&", "set", "(", "tokens", ")", ")", ">", "0", ":", "\n", "        ", "a", "=", "1", "\n", "", "elif", "len", "(", "set", "(", "[", "'\u7f51\u7edc'", ",", "'5G'", ",", "'\u53cc\u6a21'", "]", ")", "&", "set", "(", "tokens", ")", ")", ">", "0", ":", "\n", "        ", "a", "=", "2", "\n", "", "elif", "len", "(", "set", "(", "[", "'\u6444\u50cf\u5934'", ",", "'\u4e09\u6444'", ",", "'\u56db\u6444'", ",", "'\u5e7f\u89d2'", ",", "'\u53d8\u7126'", ",", "'\u957f\u7126'", ",", "'\u955c\u5934'", ",", "'\u7f8e\u989c'", ",", "'\u62cd\u6444'", ",", "'\u955c\u5934'", ",", "'\u540e\u7f6e'", ",", "'\u62cd\u7167'", ",", "'\u4eba\u50cf'", ",", "'\u5355\u6444'", "]", ")", "&", "set", "(", "tokens", ")", ")", ">", "0", ":", "\n", "        ", "a", "=", "3", "\n", "", "elif", "len", "(", "set", "(", "[", "'\u5904\u7406\u5668'", ",", "'\u5b58\u50a8'", ",", "'\u6db2\u51b7'", ",", "'\u6563\u70ed'", ",", "'\u5185\u5b58'", ",", "'\u82af\u7247'", "]", ")", "&", "set", "(", "tokens", ")", ")", ">", "0", ":", "\n", "        ", "a", "=", "4", "\n", "", "elif", "len", "(", "set", "(", "[", "'\u5145\u7535'", ",", "'\u7eed\u822a'", ",", "'\u5feb\u5145'", ",", "'\u7535\u6c60'", "]", ")", "&", "set", "(", "tokens", ")", ")", ">", "0", ":", "\n", "        ", "a", "=", "5", "\n", "", "else", ":", "\n", "        ", "a", "=", "-", "1", "\n", "", "return", "a", "\n", "\n"]], "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.postprocess.utils.class_by_keyword_1314": [[193, 215], ["len", "len", "set", "set", "len", "set", "set", "len", "set", "set", "len", "set", "set", "len", "set", "set", "len", "set", "set", "len", "set", "set", "len", "set", "set", "set", "set"], "function", ["None"], ["", "def", "class_by_keyword_1314", "(", "tokens", ")", ":", "\n", "    ", "if", "len", "(", "set", "(", "[", "'\u8eab\u6750'", ",", "'\u6bd4\u4f8b'", ",", "'\u4fee\u8eab'", ",", "'\u5bbd\u677e'", ",", "'\u7248\u578b'", "]", ")", "&", "set", "(", "tokens", ")", ")", ">", "0", ":", "\n", "        ", "a", "=", "0", "\n", "", "elif", "len", "(", "set", "(", "[", "'\u767e\u642d'", ",", "'\u914d\u642d'", ",", "'\u642d'", ",", "'\u7a7f\u642d'", "]", ")", "&", "set", "(", "tokens", ")", ")", ">", "0", ":", "\n", "        ", "a", "=", "1", "\n", "", "elif", "len", "(", "set", "(", "[", "'\u683c\u7eb9'", ",", "'\u5343\u9e1f\u683c'", ",", "'\u8272'", ",", "'\u989c\u8272'", ",", "'\u6761\u7eb9'", ",", "'\u56fe\u6848'", ",", "'\u5370\u82b1'", ",", "'\u649e\u8272'", ",", "'\u5b57\u6bcd'", ",", "'\u7ad6\u7eb9'", "]", ")", "&", "set", "(", "tokens", ")", ")", ">", "0", ":", "\n", "        ", "a", "=", "2", "\n", "", "elif", "len", "(", "set", "(", "[", "'\u9762\u6599'", ",", "'\u5e03\u6599'", ",", "'\u7eaf\u68c9'", ",", "'\u4eb2\u80a4'", ",", "'\u6750\u8d28'", ",", "'\u624b\u611f'", ",", "'\u900f\u6c14\u6027'", ",", "'\u7f8a\u6bdb'", ",", "'\u6bdb\u5462'", ",", "'\u9e2d\u7ed2'", ",", "'\u4fdd\u6696'", ",", "'\u7fbd\u7ed2'", ",", "'\u9e45\u7ed2'", ",", "'\u586b\u5145'", "]", ")", "&", "set", "(", "tokens", ")", ")", ">", "0", ":", "\n", "        ", "a", "=", "3", "\n", "", "elif", "len", "(", "set", "(", "[", "'\u53e3\u888b'", ",", "'\u63d2\u624b'", ",", "'\u63d2\u888b'", ",", "'\u5f00\u888b'", ",", "'\u8d34\u888b'", ",", "'\u63d2\u515c'", ",", "'\u7269\u54c1'", "]", ")", "&", "set", "(", "tokens", ")", ")", ">", "0", ":", "\n", "        ", "a", "=", "4", "\n", "", "elif", "len", "(", "set", "(", "[", "'\u7acb\u9886'", ",", "'\u5706\u9886'", ",", "'\u8863\u9886'", ",", "'\u9886'", ",", "'\u7ffb\u9886'", ",", "'\u9886\u53e3'", ",", "'\u5b57\u9886'", ",", "'\u9888\u90e8'", ",", "'\u9ad8\u9886'", ",", "'\u9888'", ",", "'\u5c16\u9886'", ",", "'\u8fde\u5e3d'", ",", "'\u5e3d'", "]", ")", "&", "set", "(", "tokens", ")", ")", ">", "0", ":", "\n", "        ", "a", "=", "5", "\n", "", "elif", "len", "(", "set", "(", "[", "'\u88d9\u6446'", ",", "'\u6446'", ",", "'\u767e\u8936'", ",", "'\u8377\u53f6'", ",", "'\u86cb\u7cd5'", ",", "'\u9c7c\u5c3e'", ",", "'\u5f00\u53c9'", ",", "'\u4e0b\u6446'", "]", ")", "&", "set", "(", "tokens", ")", ")", ">", "0", ":", "\n", "        ", "a", "=", "6", "\n", "", "elif", "len", "(", "set", "(", "[", "'\u8896\u53e3'", ",", "'\u8896'", "]", ")", "&", "set", "(", "tokens", ")", ")", ">", "0", ":", "\n", "        ", "a", "=", "7", "\n", "", "elif", "len", "(", "set", "(", "[", "'\u751c\u7f8e'", ",", "'\u4f18\u96c5'", ",", "'\u77e5\u6027'", ",", "'\u590d\u53e4'", ",", "'\u65f6\u5c1a'", ",", "'\u4fcf\u76ae'", ",", "'\u5c11\u5973'", ",", "'\u4e2a\u6027'", ",", "'\u65f6\u9ae6'", ",", "'\u6027\u611f'", ",", "'\u4f11\u95f2'", ",", "'\u6f6e\u9177'", "]", ")", "&", "set", "(", "tokens", ")", ")", ">", "0", ":", "\n", "        ", "a", "=", "8", "\n", "", "else", ":", "\n", "        ", "a", "=", "-", "1", "\n", "", "return", "a", "\n", "\n"]], "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.postprocess.utils.stopwordslist": [[218, 222], ["stopwords.extend", "line.strip", "open().readlines", "open"], "function", ["None"], ["", "def", "stopwordslist", "(", ")", ":", "\n", "    ", "stopwords", "=", "[", "line", ".", "strip", "(", ")", "for", "line", "in", "open", "(", "'/home/xiaojie.guo/stylized_product_copywriting/topic_modeling/stop_words.txt'", ",", "'r'", ",", "encoding", "=", "'UTF-8'", ")", ".", "readlines", "(", ")", "]", "\n", "stopwords", ".", "extend", "(", "[", "]", ")", "\n", "return", "stopwords", "\n", "\n"]], "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.postprocess.utils.seg_depart": [[223, 233], ["jieba.cut", "utils.stopwordslist", "sentence.strip"], "function", ["home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.unilm.data.cut", "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.evaluation.utils.stopwordslist"], ["", "def", "seg_depart", "(", "sentence", ")", ":", "\n", "    ", "sentence_depart", "=", "jieba", ".", "cut", "(", "sentence", ".", "strip", "(", ")", ")", "\n", "stopwords", "=", "stopwordslist", "(", ")", "\n", "outstr", "=", "''", "\n", "for", "word", "in", "sentence_depart", ":", "\n", "        ", "if", "word", "not", "in", "stopwords", ":", "\n", "            ", "if", "word", "!=", "'\\t'", ":", "\n", "                ", "outstr", "+=", "word", "\n", "outstr", "+=", "\" \"", "\n", "", "", "", "return", "outstr", "\n", "\n"]], "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.postprocess.utils.correct": [[234, 238], ["MacBertCorrector", "MacBertCorrector.macbert_correct"], "function", ["home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.evaluation.utils.diagnose.macbert_correct"], ["", "def", "correct", "(", "sent", ")", ":", "\n", "    ", "m", "=", "MacBertCorrector", "(", ")", "\n", "correct_sent", ",", "err", "=", "m", ".", "macbert_correct", "(", "sent", ")", "\n", "return", "correct_sent", "\n", "", ""]], "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.postprocess.filter_rules.if_match": [[13, 19], ["product.lower"], "function", ["None"], ["def", "if_match", "(", "number", ",", "product", ")", ":", "\n", "    ", "prod", "=", "product", ".", "lower", "(", ")", "\n", "for", "num", "in", "number", ":", "\n", "      ", "if", "num", "not", "in", "prod", ":", "\n", "         ", "return", "False", "\n", "", "", "return", "True", "\n", "\n"]], "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.postprocess.filter_rules.filter_part_671": [[20, 71], ["re.sub", "re.sub", "re.findall", "re.findall", "clean_data.append", "item.split", "item.split", "item.split", "item.split", "len", "write.replace.lower", "write.replace.replace", "write.replace.lower", "write.replace.replace", "write.replace.lower", "write_get.extend", "prod_get.extend", "write.replace.replace", "len", "filter_rules.if_match", "write.replace.lower", "write.replace.lower", "product.lower", "product.lower", "write.replace.lower", "product.lower", "re.findall", "re.findall", "re.findall", "re.findall", "len", "len"], "function", ["home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.preprocess.process_daren.split", "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.preprocess.process_daren.split", "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.preprocess.process_daren.split", "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.preprocess.process_daren.split", "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.postprocess.filter_rules.if_match"], ["", "def", "filter_part_671", "(", "data", ")", ":", "\n", "    ", "clean_data", "=", "[", "]", "\n", "\n", "for", "item", "in", "data", ":", "\n", "        ", "write", "=", "item", ".", "split", "(", "'|||'", ")", "[", "2", "]", "\n", "aspect", "=", "item", ".", "split", "(", "'|||'", ")", "[", "0", "]", "#code_dic[item.split('|||')[0]]", "\n", "product", "=", "item", ".", "split", "(", "'|||'", ")", "[", "1", "]", "\n", "sku", "=", "item", ".", "split", "(", "'|||'", ")", "[", "3", "]", "\n", "#print(sku)", "\n", "#delete \u3010**\u3011", "\n", "write", "=", "re", ".", "sub", "(", "u\"\\\\\uff08.*?\uff09|\\\\{.*?}|\\\\[.*?]|\\\\\u3010.*?\u3011[\uff0c|\u3002|\uff1b|\uff01]*\"", ",", "\"\"", ",", "write", ")", "\n", "if", "len", "(", "write", ")", ">", "30", ":", "\n", "            ", "continue", "\n", "\n", "\n", "#check if match with input", "\n", "#check 5G", "\n", "", "if", "(", "'\u72ec\u7acb\u663e\u5361'", "in", "write", ".", "lower", "(", ")", "or", "'\u72ec\u663e'", "in", "write", ".", "lower", "(", ")", ")", "and", "(", "'\u72ec\u7acb\u663e\u5361'", "not", "in", "product", ".", "lower", "(", ")", "or", "'\u72ec\u663e'", "not", "in", "product", ".", "lower", "(", ")", ")", ":", "\n", "            ", "continue", "\n", "#check\u65e0\u7ebf\u5145\u7535", "\n", "", "for", "term", "in", "[", "'i3'", ",", "'i5'", ",", "'i7'", ",", "'i9'", "]", ":", "\n", "          ", "if", "term", "in", "write", ".", "lower", "(", ")", "and", "term", "not", "in", "product", ".", "lower", "(", ")", ":", "\n", "              ", "continue", "\n", "\n", "\n", "", "", "if", "'\u6838\u5fc3'", "in", "write", ".", "lower", "(", ")", ":", "\n", "            ", "write", ".", "replace", "(", "'\u6838\u5fc3'", ",", "'\u6838'", ")", "\n", "", "if", "'whr'", "in", "write", ".", "lower", "(", ")", ":", "\n", "            ", "write", ".", "replace", "(", "'whr'", ",", "'wh'", ")", "\n", "\n", "#check \u6444\u5f71 \u5c4f\u5e55\u9891\u7387 \u7535\u6c60\u7535\u91cf  \u5145\u7535\u74e6\u6570 \u5c4f\u5e55\u5c3a\u5bf8 \u50cf\u7d20 \u673a\u8eab\u539a\u5ea6", "\n", "", "pattern_list", "=", "[", "[", "'[\u5355\uff5c\u53cc\uff5c\u516d\uff5c\u516b\uff5c\u56db]\u6838'", ",", "'\\d+\\.?\\d*\u6838'", "]", ",", "[", "'\\d+\\.?\\d*nm'", "]", ",", "[", "'\\d+\\.?\\d*\u7ea7\u538b'", "]", ",", "[", "'\\d+\\.?\\d*\u4e07'", "]", ",", "[", "'\\d+\\.?\\d*\u82f1\u5bf8'", "]", ",", "[", "'\\d+\\.?\\d*[w|W|\u74e6]'", "]", ",", "[", "'\\d+\\.?\\d*[h|H][Z|z]'", "]", ",", "[", "'\\d+\\.?\\d*[m|M][a|A][h|H]'", ",", "'\\d+\\.?\\d*\u6beb\u5b89'", "]", ",", "[", "'\\d+\\.?\\d*k'", "]", ",", "[", "'\\d+\\.?\\d*g[h|H][Z|z]'", "]", ",", "[", "'\\d+\\.?\\d*kg'", ",", "'\\d+\\.?\\d*\u5343\u514b'", "]", ",", "[", "'\\d+\\.?\\d*[W|w][H|h]'", "]", ",", "[", "'\\d+\\.?\\d*\u6beb\u7c73'", "]", ",", "[", "'[g|G|R|r][T|t][X|x]\\d+\\.?\\d*'", "]", "]", "\n", "for", "pattern_l", "in", "pattern_list", ":", "\n", "            ", "write_get", "=", "[", "]", "\n", "prod_get", "=", "[", "]", "\n", "for", "pattern", "in", "pattern_l", ":", "\n", "                ", "write_get", ".", "extend", "(", "re", ".", "findall", "(", "pattern", ",", "write", ")", ")", "\n", "prod_get", ".", "extend", "(", "re", ".", "findall", "(", "pattern", ",", "product", ")", ")", "\n", "", "if", "len", "(", "write_get", ")", ">", "0", "and", "len", "(", "prod_get", ")", ">", "0", ":", "\n", "                ", "write", "=", "write", ".", "replace", "(", "write_get", "[", "0", "]", ",", "prod_get", "[", "0", "]", ")", "\n", "\n", "\n", "#check number", "\n", "", "", "number", "=", "re", ".", "findall", "(", "r\"\\d+\\.?\\d*[a-z]*\"", ",", "write", ".", "lower", "(", ")", ")", "\n", "if", "len", "(", "number", ")", ">", "0", "and", "not", "if_match", "(", "number", ",", "product", ")", ":", "\n", "            ", "continue", "\n", "\n", "", "clean_data", ".", "append", "(", "sku", "+", "\"|||\"", "+", "aspect", "+", "\"|||\"", "+", "product", "+", "\"|||\"", "+", "write", ")", "\n", "", "return", "clean_data", "\n", "\n"]], "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.postprocess.filter_rules.filter_part_653": [[73, 114], ["re.sub", "re.sub", "re.findall", "re.findall", "clean_data.append", "item.split", "item.split", "item.split", "item.split", "write.replace.lower", "write.replace.lower", "product.lower", "write.replace.lower", "product.lower", "write_get.extend", "prod_get.extend", "write.replace.replace", "len", "filter_rules.if_match", "re.findall", "re.findall", "re.findall", "re.findall", "len", "len"], "function", ["home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.preprocess.process_daren.split", "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.preprocess.process_daren.split", "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.preprocess.process_daren.split", "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.preprocess.process_daren.split", "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.postprocess.filter_rules.if_match"], ["", "def", "filter_part_653", "(", "data", ")", ":", "\n", "    ", "clean_data", "=", "[", "]", "\n", "for", "item", "in", "data", ":", "\n", "\n", "        ", "write", "=", "item", ".", "split", "(", "'|||'", ")", "[", "2", "]", "\n", "aspect", "=", "item", ".", "split", "(", "'|||'", ")", "[", "0", "]", "#code_dic[item.split('|||')[0]]", "\n", "product", "=", "item", ".", "split", "(", "'|||'", ")", "[", "1", "]", "\n", "sku", "=", "item", ".", "split", "(", "'|||'", ")", "[", "3", "]", "\n", "\n", "\n", "#delete \u3010**\u3011", "\n", "write", "=", "re", ".", "sub", "(", "u\"\\\\\uff08.*?\uff09|\\\\{.*?}|\\\\[.*?]|\\\\\u3010.*?\u3011[\uff0c|\u3002|\uff1b|\uff01]*\"", ",", "\"\"", ",", "write", ")", "\n", "\n", "\n", "#check if match with input", "\n", "#check 5G", "\n", "if", "'5g'", "in", "write", ".", "lower", "(", ")", "and", "'\u4e0d\u652f\u63015g'", "in", "product", ".", "lower", "(", ")", ":", "\n", "            ", "continue", "\n", "#check\u65e0\u7ebf\u5145\u7535", "\n", "", "if", "'\u65e0\u7ebf'", "in", "write", ".", "lower", "(", ")", "and", "'\u4e0d\u652f\u6301\u65e0\u7ebf\u5145\u7535'", "in", "product", ".", "lower", "(", ")", ":", "\n", "            ", "continue", "\n", "\n", "#check \u6444\u5f71 \u5c4f\u5e55\u9891\u7387 \u7535\u6c60\u7535\u91cf  \u5145\u7535\u74e6\u6570 \u5c4f\u5e55\u5c3a\u5bf8 \u50cf\u7d20 \u673a\u8eab\u539a\u5ea6", "\n", "", "pattern_list", "=", "[", "[", "'[\u4e09|\u53cc|\u5355\uff5c\u56db]\u6444'", "]", ",", "[", "'\\d+\\.?\\d*nm'", "]", ",", "[", "'\\d+\\.?\\d*\u4e07'", "]", ",", "[", "'\\d+\\.?\\d*\u82f1\u5bf8'", "]", ",", "[", "'\\d+\\.?\\d*[w|W|\u74e6]'", "]", ",", "[", "'\\d+\\.?\\d*[h|H][Z|z]'", "]", ",", "[", "'\\d+\\.?\\d*[m|M][a|A][h|H]'", ",", "'\\d+\\.?\\d*\u6beb\u5b89'", "]", "]", "\n", "for", "pattern_l", "in", "pattern_list", ":", "\n", "            ", "write_get", "=", "[", "]", "\n", "prod_get", "=", "[", "]", "\n", "for", "pattern", "in", "pattern_l", ":", "\n", "                ", "write_get", ".", "extend", "(", "re", ".", "findall", "(", "pattern", ",", "write", ")", ")", "\n", "prod_get", ".", "extend", "(", "re", ".", "findall", "(", "pattern", ",", "product", ")", ")", "\n", "", "if", "len", "(", "write_get", ")", ">", "0", "and", "len", "(", "prod_get", ")", ">", "0", ":", "\n", "                ", "write", "=", "write", ".", "replace", "(", "write_get", "[", "0", "]", ",", "prod_get", "[", "0", "]", ")", "\n", "\n", "\n", "#check number", "\n", "", "", "number", "=", "re", ".", "findall", "(", "r\"\\d+\\.?\\d*[a-z]*\"", ",", "write", ".", "lower", "(", ")", ")", "\n", "if", "len", "(", "number", ")", ">", "0", "and", "not", "if_match", "(", "number", ",", "product", ")", ":", "\n", "            ", "continue", "\n", "\n", "", "clean_data", ".", "append", "(", "sku", "+", "\"|||\"", "+", "aspect", "+", "\"|||\"", "+", "product", "+", "\"|||\"", "+", "write", ")", "\n", "", "return", "clean_data", "\n", "\n"]], "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.postprocess.filter_rules.filter_part_1343": [[116, 171], ["re.sub", "re.sub", "clean_data.append", "item.split", "item.split", "item.split", "item.split", "len", "range", "re.sub.replace", "product.lower", "product.lower", "write_l.append", "prod_l.append", "len", "len", "len", "len", "len", "len", "len", "product.lower", "product.lower", "re.sub.replace"], "function", ["home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.preprocess.process_daren.split", "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.preprocess.process_daren.split", "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.preprocess.process_daren.split", "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.preprocess.process_daren.split"], ["", "def", "filter_part_1343", "(", "data", ")", ":", "\n", "    ", "clean_data", "=", "[", "]", "\n", "for", "item", "in", "data", ":", "\n", "        ", "write", "=", "item", ".", "split", "(", "'|||'", ")", "[", "2", "]", "\n", "aspect", "=", "item", ".", "split", "(", "'|||'", ")", "[", "0", "]", "#code_dic[item.split('|||')[0]]", "\n", "product", "=", "item", ".", "split", "(", "'|||'", ")", "[", "1", "]", "\n", "sku", "=", "item", ".", "split", "(", "'|||'", ")", "[", "3", "]", "\n", "\n", "#delete \u3010**\u3011", "\n", "write", "=", "re", ".", "sub", "(", "u\"\\\\\uff08.*?\uff09|\\\\{.*?}|\\\\[.*?]|\\\\\u3010.*?\u3011[\uff0c|\u3002|\uff1b|\uff01]*\"", ",", "\"\"", ",", "write", ")", "\n", "if", "len", "(", "write", ")", ">", "30", ":", "\n", "            ", "continue", "\n", "\n", "#check if the product support the aspect", "\n", "#check \u8896\u53e3", "\n", "", "if", "aspect", "==", "'\u7ec6\u8282\uff08\u8896\u53e3)'", ":", "\n", "            ", "continue", "\n", "#check \u53e3\u888b", "\n", "", "if", "aspect", "==", "'\u7ec6\u8282\uff08\u53e3\u888b)'", "and", "'\u888b'", "not", "in", "product", ".", "lower", "(", ")", "and", "'\u515c'", "not", "in", "product", ".", "lower", "(", ")", ":", "\n", "            ", "continue", "\n", "#check \u9886\u53e3", "\n", "", "if", "aspect", "==", "'\u7ec6\u8282\uff08\u9886\u53e3)'", "and", "(", "'\u88d9'", "in", "product", ".", "lower", "(", ")", "or", "'\u88e4'", "in", "product", ".", "lower", "(", ")", ")", ":", "\n", "            ", "continue", "\n", "\n", "#check if content match", "\n", "#check \u56fe\u6848\u5370\u82b1", "\n", "", "if", "\"a\u5b57\"", "in", "write", "and", "(", "\"A\u5b57\"", "not", "in", "product", "or", "'A\u7248'", "not", "in", "product", ")", ":", "\n", "            ", "continue", "\n", "#check \u6750\u8d28", "\n", "", "if", "\"\u68c9\"", "in", "write", "and", "\"\u68c9\"", "not", "in", "product", ":", "\n", "            ", "continue", "\n", "#check \u56fe\u6848\u82b1\u7eb9", "\n", "", "if", "\"\u7ffb\u9886\"", "in", "write", "and", "\"\u7ffb\u9886\"", "not", "in", "product", ":", "\n", "            ", "continue", "\n", "\n", "#print(aspect)", "\n", "", "write_l", "=", "[", "]", "\n", "prod_l", "=", "[", "]", "\n", "pattern", "=", "[", "'\u649e\u8272'", ",", "'\u6761\u7eb9'", ",", "'\u5b57\u6bcd'", ",", "'\u5370\u82b1'", ",", "'\u62fc\u63a5'", ",", "'\u683c\u7eb9'", ",", "'\u5343\u9e1f\u683c'", "]", "\n", "for", "word", "in", "pattern", ":", "\n", "            ", "if", "word", "in", "write", ":", "\n", "                ", "write_l", ".", "append", "(", "word", ")", "\n", "", "if", "word", "in", "product", ":", "\n", "                ", "prod_l", ".", "append", "(", "word", ")", "\n", "", "", "if", "len", "(", "write_l", ")", ">", "1", "and", "len", "(", "prod_l", ")", ">", "0", ":", "\n", "            ", "for", "i", "in", "range", "(", "len", "(", "write_l", ")", ")", ":", "\n", "                  ", "if", "write_l", "[", "i", "]", "not", "in", "prod_l", ":", "\n", "                     ", "write", ".", "replace", "(", "write_l", "[", "i", "]", ",", "''", ")", "\n", "", "", "", "if", "len", "(", "write_l", ")", "==", "1", "and", "len", "(", "prod_l", ")", ">", "0", "and", "write_l", "[", "0", "]", "not", "in", "prod_l", ":", "\n", "                     ", "write", ".", "replace", "(", "write_l", "[", "0", "]", ",", "prod_l", "[", "0", "]", ")", "\n", "", "if", "len", "(", "write_l", ")", "==", "1", "and", "len", "(", "prod_l", ")", "==", "0", ":", "\n", "                     ", "continue", "\n", "\n", "", "clean_data", ".", "append", "(", "sku", "+", "\"|||\"", "+", "aspect", "+", "\"|||\"", "+", "product", "+", "\"|||\"", "+", "write", ")", "\n", "", "return", "clean_data", "\n", "\n"]], "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.postprocess.filter_rules.filter_part_1342": [[173, 257], ["print", "re.sub", "re.sub", "clean_data.append", "item.split", "item.split", "item.split", "item.split", "len", "range", "write.replace.replace", "product.lower", "product.lower", "write_l.append", "prod_l.append", "len", "len", "len", "len", "len", "len", "len", "write_get.extend", "prod_get.extend", "write.replace.replace", "product.lower", "product.lower", "product.lower", "product.lower", "product.lower", "product.lower", "product.lower", "write.replace.replace", "re.findall", "re.findall", "re.findall", "re.findall", "len", "len", "len", "len"], "function", ["home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.preprocess.process_daren.split", "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.preprocess.process_daren.split", "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.preprocess.process_daren.split", "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.preprocess.process_daren.split"], ["", "def", "filter_part_1342", "(", "data", ")", ":", "\n", "    ", "clean_data", "=", "[", "]", "\n", "\n", "for", "item", "in", "data", ":", "\n", "        ", "write", "=", "item", ".", "split", "(", "'|||'", ")", "[", "2", "]", "\n", "aspect", "=", "item", ".", "split", "(", "'|||'", ")", "[", "0", "]", "#code_dic[item.split('|||')[0]]", "\n", "product", "=", "item", ".", "split", "(", "'|||'", ")", "[", "1", "]", "\n", "sku", "=", "item", ".", "split", "(", "'|||'", ")", "[", "3", "]", "\n", "print", "(", "sku", ")", "\n", "#delete \u3010**\u3011", "\n", "write", "=", "re", ".", "sub", "(", "u\"\\\\\uff08.*?\uff09|\\\\{.*?}|\\\\[.*?]|\\\\\u3010.*?\u3011[\uff0c|\u3002|\uff1b|\uff01]*\"", ",", "\"\"", ",", "write", ")", "\n", "if", "len", "(", "write", ")", ">", "30", ":", "\n", "            ", "continue", "\n", "\n", "#check if the product support the aspect", "\n", "#check \u8896\u53e3", "\n", "", "if", "aspect", "==", "'\u7ec6\u8282\uff08\u8896\u53e3)'", ":", "\n", "            ", "continue", "\n", "#check \u53e3\u888b", "\n", "", "if", "aspect", "==", "'\u7ec6\u8282\uff08\u53e3\u888b)'", "and", "'\u888b'", "not", "in", "product", ".", "lower", "(", ")", "and", "'\u515c'", "not", "in", "product", ".", "lower", "(", ")", ":", "\n", "            ", "continue", "\n", "#check \u8863\u6446", "\n", "", "if", "aspect", "==", "'\u7ec6\u8282\uff08\u8863\u6446)'", "and", "(", "'\u77ed\u8896'", "in", "product", ".", "lower", "(", ")", "or", "'\u957f\u8896'", "in", "product", ".", "lower", "(", ")", "or", "'\u957f\u8896'", "in", "product", ".", "lower", "(", ")", "or", "'t\u6064'", "in", "product", ".", "lower", "(", ")", "or", "'\u88e4'", "in", "product", ".", "lower", "(", ")", ")", ":", "\n", "            ", "continue", "\n", "#check \u9886\u53e3", "\n", "", "if", "aspect", "==", "'\u7ec6\u8282\uff08\u9886\u53e3)'", "and", "(", "'\u7fa4'", "in", "product", ".", "lower", "(", ")", "or", "'\u88e4'", "in", "product", ".", "lower", "(", ")", ")", ":", "\n", "            ", "continue", "\n", "\n", "#check if content match", "\n", "#check \u56fe\u6848\u5370\u82b1", "\n", "", "if", "\"a\u5b57\"", "in", "write", "and", "\"a\u5b57\"", "not", "in", "product", ":", "\n", "            ", "continue", "\n", "#check \u6750\u8d28", "\n", "", "if", "\"\u68c9\"", "in", "write", "and", "\"\u68c9\"", "not", "in", "product", ":", "\n", "            ", "continue", "\n", "#check \u56fe\u6848\u82b1\u7eb9", "\n", "", "if", "\"\u7ffb\u9886\"", "in", "write", "and", "\"\u7ffb\u9886\"", "not", "in", "product", ":", "\n", "            ", "continue", "\n", "", "if", "\"\u7eaf\u8272\"", "in", "write", "and", "(", "'\u5b57\u6bcd'", "in", "product", "or", "'\u5370\u82b1'", "in", "product", "or", "'\u649e\u8272'", "in", "product", ")", ":", "\n", "            ", "continue", "\n", "", "if", "\"\u8fde\u5e3d\"", "in", "write", "and", "\"\u8fde\u5e3d\"", "not", "in", "product", ":", "\n", "            ", "continue", "\n", "", "if", "\"\u62bd\u7ef3\"", "in", "write", "and", "\"\u62bd\u7ef3\"", "not", "in", "product", ":", "\n", "            ", "continue", "\n", "", "if", "\"\u8fde\u5e3d\"", "in", "write", "and", "\"\u8fde\u5e3d\"", "not", "in", "product", ":", "\n", "            ", "continue", "\n", "", "if", "\"\u7ee3\u82b1\"", "in", "write", "and", "\"\u7ee3\u82b1\"", "not", "in", "product", ":", "\n", "            ", "continue", "\n", "", "if", "\"\u523a\u7ee3\"", "in", "write", "and", "\"\u523a\u7ee3\"", "not", "in", "product", ":", "\n", "            ", "continue", "\n", "\n", "", "write_l", "=", "[", "]", "\n", "prod_l", "=", "[", "]", "\n", "pattern", "=", "[", "'\u649e\u8272'", ",", "'\u6761\u7eb9'", ",", "'\u5b57\u6bcd'", ",", "'\u5370\u82b1'", ",", "'\u62fc\u63a5'", ",", "'\u683c\u7eb9'", ",", "'\u5343\u9e1f\u683c'", ",", "'logo'", ",", "'LOGO'", "]", "\n", "for", "word", "in", "pattern", ":", "\n", "            ", "if", "word", "in", "write", ":", "\n", "                ", "write_l", ".", "append", "(", "word", ")", "\n", "", "if", "word", "in", "product", ":", "\n", "                ", "prod_l", ".", "append", "(", "word", ")", "\n", "", "", "if", "len", "(", "write_l", ")", ">", "1", "and", "len", "(", "prod_l", ")", ">", "0", ":", "\n", "            ", "for", "i", "in", "range", "(", "len", "(", "write_l", ")", ")", ":", "\n", "                  ", "if", "write_l", "[", "i", "]", "not", "in", "prod_l", ":", "\n", "                     ", "write", ".", "replace", "(", "write_l", "[", "i", "]", ",", "''", ")", "\n", "", "", "", "if", "len", "(", "write_l", ")", "==", "1", "and", "len", "(", "prod_l", ")", ">", "0", "and", "write_l", "[", "0", "]", "not", "in", "prod_l", ":", "\n", "                     ", "write", ".", "replace", "(", "write_l", "[", "0", "]", ",", "prod_l", "[", "0", "]", ")", "\n", "", "if", "len", "(", "write_l", ")", ">=", "1", "and", "len", "(", "prod_l", ")", "==", "0", ":", "\n", "                     ", "continue", "\n", "\n", "#check \u6444\u5f71 \u5c4f\u5e55\u9891\u7387 \u7535\u6c60\u7535\u91cf  \u5145\u7535\u74e6\u6570 \u5c4f\u5e55\u5c3a\u5bf8 \u50cf\u7d20 \u673a\u8eab\u539a\u5ea6", "\n", "", "pattern_list", "=", "[", "[", "'[\u9ed1|\u767d|\u84dd|\u7ea2|\u9ec4|\u7eaf|\u4eae|\u7eff|\u7c89|\u7070|\u7d2b]\u8272'", "]", "]", "\n", "for", "pattern_l", "in", "pattern_list", ":", "\n", "            ", "write_get", "=", "[", "]", "\n", "prod_get", "=", "[", "]", "\n", "for", "pattern", "in", "pattern_l", ":", "\n", "                ", "write_get", ".", "extend", "(", "re", ".", "findall", "(", "pattern", ",", "write", ")", ")", "\n", "prod_get", ".", "extend", "(", "re", ".", "findall", "(", "pattern", ",", "product", ")", ")", "\n", "", "if", "len", "(", "write_get", ")", ">", "0", "and", "len", "(", "prod_get", ")", ">", "0", ":", "\n", "                ", "write", "=", "write", ".", "replace", "(", "write_get", "[", "0", "]", ",", "prod_get", "[", "0", "]", ")", "\n", "", "if", "len", "(", "write_get", ")", ">=", "1", "and", "len", "(", "prod_get", ")", "==", "0", ":", "\n", "                ", "continue", "\n", "\n", "", "", "clean_data", ".", "append", "(", "sku", "+", "\"|||\"", "+", "aspect", "+", "\"|||\"", "+", "product", "+", "\"|||\"", "+", "write", ")", "\n", "", "return", "clean_data", "\n", "\n"]], "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.postprocess.filter_rules.filter_part_1381": [[259, 304], ["re.sub", "re.sub", "clean_data.append", "item.split", "item.split", "item.split", "item.split", "len", "re.sub.split"], "function", ["home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.preprocess.process_daren.split", "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.preprocess.process_daren.split", "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.preprocess.process_daren.split", "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.preprocess.process_daren.split", "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.preprocess.process_daren.split"], ["", "def", "filter_part_1381", "(", "data", ")", ":", "\n", "    ", "clean_data", "=", "[", "]", "\n", "\n", "for", "item", "in", "data", ":", "\n", "        ", "write", "=", "item", ".", "split", "(", "'|||'", ")", "[", "2", "]", "\n", "aspect", "=", "item", ".", "split", "(", "'|||'", ")", "[", "0", "]", "#code_dic[item.split('|||')[0]]", "\n", "product", "=", "item", ".", "split", "(", "'|||'", ")", "[", "1", "]", "\n", "sku", "=", "item", ".", "split", "(", "'|||'", ")", "[", "3", "]", "\n", "\n", "if", "'[UNK]'", "in", "write", ":", "\n", "            ", "continue", "\n", "", "if", "\"\u8fd9\u6b3e\"", "in", "write", ":", "\n", "            ", "write", "=", "'\u8fd9\u6b3e'", "+", "write", ".", "split", "(", "'\u8fd9\u6b3e'", ")", "[", "1", "]", "\n", "#delete \u3010**\u3011", "\n", "", "write", "=", "re", ".", "sub", "(", "u\"\\\\\uff08.*?\uff09|\\\\{.*?}|\\\\[.*?]|\\\\\u3010.*?\u3011[\uff0c|\u3002|\uff1b|\uff01]*\"", ",", "\"\"", ",", "write", ")", "\n", "if", "len", "(", "write", ")", ">", "30", ":", "\n", "            ", "continue", "\n", "\n", "#check if the product support the aspect", "\n", "#check \u8896\u53e3", "\n", "", "if", "aspect", "==", "'\u9632\u6652\u9694\u79bb'", ":", "\n", "            ", "if", "'\u9632\u6652'", "in", "write", "and", "'\u9632\u6652'", "not", "in", "product", ":", "\n", "               ", "continue", "\n", "", "", "if", "aspect", "==", "'\u6e05\u6d01\u63a7\u6cb9'", ":", "\n", "            ", "if", "'\u9632\u6652'", "in", "product", ":", "\n", "                ", "continue", "\n", "", "", "if", "aspect", "==", "'\u8865\u6c34\u4fdd\u6e7f'", ":", "\n", "            ", "if", "'\u9632\u6652'", "in", "product", ":", "\n", "                ", "continue", "\n", "", "", "if", "aspect", "!=", "'\u5305\u88c5\u8bbe\u8ba1'", "and", "'\u8bbe\u8ba1'", "in", "write", ":", "\n", "                ", "continue", "\n", "", "if", "aspect", "==", "'\u6297\u76b1\u7d27\u81f4'", ":", "\n", "            ", "if", "'\u6d01\u9762'", "in", "product", ":", "\n", "                ", "continue", "\n", "", "", "if", "aspect", "==", "'\u4fee\u62a4\u8212\u7f13'", ":", "\n", "            ", "if", "'\u6d01\u9762'", "in", "product", ":", "\n", "                ", "continue", "\n", "#check if content match", "\n", "#check \u56fe\u6848\u5370\u82b1", "\n", "", "", "if", "(", "\"\u6ce1\u6cab\"", "in", "write", "or", "'\u6d17\u9762\u5976'", "in", "write", ")", "and", "\"\u6d01\u9762\"", "not", "in", "product", ":", "\n", "            ", "continue", "\n", "\n", "\n", "", "clean_data", ".", "append", "(", "sku", "+", "\"|||\"", "+", "aspect", "+", "\"|||\"", "+", "product", "+", "\"|||\"", "+", "write", ")", "\n", "", "return", "clean_data", "\n", "\n"]], "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.postprocess.filter_rules.filter_part_794": [[306, 368], ["print", "print", "len", "re.sub", "re.sub", "re.findall", "re.findall", "clean_data.append", "str", "item.split", "item.split", "item.split", "item.split", "len", "re.findall", "re.findall", "write.replace.replace", "write.replace.lower", "write.replace.lower", "product.lower", "write_get.extend", "prod_get.extend", "write.replace.replace", "len", "filter_rules.if_match", "re.findall", "re.findall", "re.findall", "re.findall", "len", "len", "str"], "function", ["home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.preprocess.process_daren.split", "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.preprocess.process_daren.split", "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.preprocess.process_daren.split", "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.preprocess.process_daren.split", "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.postprocess.filter_rules.if_match"], ["", "def", "filter_part_794", "(", "data", ")", ":", "\n", "    ", "clean_data", "=", "[", "]", "\n", "error_c", "=", "0", "\n", "for", "item", "in", "data", ":", "\n", "      ", "try", ":", "\n", "        ", "write", "=", "item", ".", "split", "(", "'|||'", ")", "[", "2", "]", "\n", "aspect", "=", "item", ".", "split", "(", "'|||'", ")", "[", "0", "]", "#code_dic[item.split('|||')[0]]", "\n", "product", "=", "item", ".", "split", "(", "'|||'", ")", "[", "1", "]", "\n", "sku", "=", "item", ".", "split", "(", "'|||'", ")", "[", "3", "]", "\n", "#print(aspect)", "\n", "#delete \u3010**\u3011", "\n", "write", "=", "re", ".", "sub", "(", "u\"\\\\\uff08.*?\uff09|\\\\{.*?}|\\\\[.*?]|\\\\\u3010.*?\u3011[\uff0c|\u3002|\uff1b|\uff01]*\"", ",", "\"\"", ",", "write", ")", "\n", "if", "len", "(", "write", ")", ">", "30", ":", "\n", "            ", "continue", "\n", "\n", "#check if aspect match with product", "\n", "", "if", "aspect", "==", "'\u9664\u83cc\u81ea\u6d01'", "or", "aspect", "==", "'\u5bb9\u91cf\u591a\u6863'", ":", "\n", "            ", "if", "'\u51b0\u7bb1'", "not", "in", "product", "and", "'\u6d17\u8863\u673a'", "not", "in", "product", "and", "'\u51b0\u67dc'", "not", "in", "product", ":", "\n", "               ", "continue", "\n", "", "", "if", "aspect", "==", "'\u53bb\u6c61\u70d8\u5e72'", ":", "\n", "            ", "if", "'\u6d17\u8863\u673a'", "not", "in", "product", ":", "\n", "               ", "continue", "\n", "", "", "if", "aspect", "==", "'\u4fdd\u9c9c\u9664\u971c'", ":", "\n", "            ", "if", "'\u51b0\u7bb1'", "not", "in", "product", "and", "'\u51b0\u67dc'", "not", "in", "product", ":", "\n", "               ", "continue", "\n", "", "", "if", "aspect", "==", "'\u753b\u8d28\u97f3\u6548'", ":", "\n", "            ", "if", "'\u7535\u89c6\u673a'", "not", "in", "product", ":", "\n", "               ", "continue", "\n", "", "", "if", "aspect", "==", "'\u5236\u51b7\u9001\u98ce'", ":", "\n", "            ", "if", "'\u51b0\u7bb1'", "not", "in", "product", "and", "'\u51b0\u67dc'", "not", "in", "product", "and", "'\u7a7a\u8c03'", "not", "in", "product", ":", "\n", "               ", "continue", "\n", "\n", "\n", "#check if match with input", "\n", "", "", "if", "'\u2103'", "in", "write", ".", "lower", "(", ")", "and", "'\u2103'", "not", "in", "product", ".", "lower", "(", ")", ":", "\n", "            ", "p", "=", "re", ".", "findall", "(", "'\\d+\\.?\\d*\u2103'", ",", "write", ")", "\n", "write", "=", "write", ".", "replace", "(", "p", ",", "''", ")", "\n", "\n", "\n", "#check \u6444\u5f71 \u5c4f\u5e55\u9891\u7387 \u7535\u6c60\u7535\u91cf  \u5145\u7535\u74e6\u6570 \u5c4f\u5e55\u5c3a\u5bf8 \u50cf\u7d20 \u673a\u8eab\u539a\u5ea6", "\n", "", "pattern_list", "=", "[", "[", "'\\d+\\.?\\d*\u516c\u65a4'", "]", ",", "[", "'\\d+\\.?\\d*\u2103'", "]", ",", "[", "'\\d+\\.?\\d*\u5347'", ",", "'\\d+\\.?\\d*L'", ",", "'\\d+\\.?\\d*l'", "]", ",", "[", "'\\d+\\.?\\d*\u6beb\u7c73'", "]", "]", "\n", "for", "pattern_l", "in", "pattern_list", ":", "\n", "            ", "write_get", "=", "[", "]", "\n", "prod_get", "=", "[", "]", "\n", "for", "pattern", "in", "pattern_l", ":", "\n", "                ", "write_get", ".", "extend", "(", "re", ".", "findall", "(", "pattern", ",", "write", ")", ")", "\n", "prod_get", ".", "extend", "(", "re", ".", "findall", "(", "pattern", ",", "product", ")", ")", "\n", "", "if", "len", "(", "write_get", ")", ">", "0", "and", "len", "(", "prod_get", ")", ">", "0", ":", "\n", "                ", "write", "=", "write", ".", "replace", "(", "write_get", "[", "0", "]", ",", "prod_get", "[", "0", "]", ")", "\n", "\n", "\n", "#check number", "\n", "", "", "number", "=", "re", ".", "findall", "(", "r\"\\d+\\.?\\d*[a-z]*\"", ",", "write", ".", "lower", "(", ")", ")", "\n", "if", "len", "(", "number", ")", ">", "0", "and", "not", "if_match", "(", "number", ",", "product", ")", ":", "\n", "            ", "continue", "\n", "\n", "", "clean_data", ".", "append", "(", "str", "(", "sku", ")", "+", "\"|||\"", "+", "aspect", "+", "\"|||\"", "+", "product", "+", "\"|||\"", "+", "write", ")", "\n", "#print(sku+\"|||\"+aspect+\"|||\"+product+\"|||\"+write)", "\n", "", "except", ":", "error_c", "+=", "1", "\n", "", "print", "(", "'error:'", "+", "str", "(", "error_c", ")", ")", "\n", "print", "(", "len", "(", "clean_data", ")", ")", "\n", "return", "clean_data", "\n", "", ""]], "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.title_generation.generate_feature_1343.refine_feature": [[30, 67], ["pw.replace.replace", "pw.replace.replace", "feature.split", "feature.split", "len", "len", "feature.split", "len", "feature.split"], "function", ["home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.preprocess.process_daren.split", "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.preprocess.process_daren.split", "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.preprocess.process_daren.split", "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.preprocess.process_daren.split"], ["", "", "def", "refine_feature", "(", "prod", ",", "feature", ")", ":", "\n", "#\u53bb\u6389\u6ca1\u6709\u751f\u6210\u5b8c\u6574\u7684", "\n", "    ", "if", "'('", "in", "feature", ":", "\n", "      ", "feature", "=", "feature", ".", "split", "(", "'('", ")", "[", "1", "]", "\n", "", "if", "\"+\"", "not", "in", "feature", "or", "len", "(", "feature", ".", "split", "(", "\"+\"", ")", ")", ">", "2", ":", "\n", "      ", "return", "''", "\n", "", "if", "'[UNK]'", "in", "feature", ":", "\n", "      ", "return", "''", "\n", "\n", "\n", "#\u68c0\u67e5\u5546\u54c1\u8bcd", "\n", "", "pw", "=", "feature", ".", "split", "(", "'+'", ")", "[", "1", "]", "\n", "if", "'\u978b'", "in", "pw", ":", "\n", "      ", "return", "''", "\n", "", "if", "pw", "in", "[", "'\u8fde\u8863'", ",", "'\u725b\u4ed4'", ",", "'\u534a\u8eab'", "]", ":", "\n", "       ", "pw", "=", "pw", "+", "'\u88d9'", "\n", "", "pw", "=", "pw", ".", "replace", "(", "'\u7fa4'", ",", "'\u88d9'", ")", "\n", "pw", "=", "pw", ".", "replace", "(", "'\u8896'", ",", "'\u88d9'", ")", "\n", "core_word", "=", "[", "'\u88e4'", ",", "'\u88d9'", ",", "'\u8fde\u8863\u88d9'", ",", "'\u886c\u886b'", "]", "\n", "for", "w", "in", "core_word", ":", "\n", "     ", "if", "w", "in", "pw", "and", "w", "not", "in", "prod", ":", "\n", "       ", "return", "''", "\n", "\n", "\n", "#\u68c0\u67e5\u4fee\u9970\u8bcd", "\n", "", "", "subj", "=", "feature", ".", "split", "(", "'+'", ")", "[", "0", "]", "\n", "if", "len", "(", "subj", ")", ">", "7", ":", "\n", "      ", "subj", "=", "''", "\n", "", "check", "=", "[", "'\u3011'", ",", "'\u3010'", ",", "'\uff09'", ",", "'\uff08'", "]", "\n", "for", "c", "in", "check", ":", "\n", "       ", "if", "c", "in", "subj", ":", "\n", "         ", "subj", "=", "''", "\n", "\n", "", "", "if", "len", "(", "subj", ")", ">", "0", ":", "\n", "      ", "return", "subj", "+", "' '", "+", "pw", "\n", "", "else", ":", "\n", "      ", "return", "pw", "\n", "\n"]], "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.title_generation.split_title.build_data": [[13, 30], ["open", "csv.writer", "csv.writer.writerow", "item.split.split", "item[].replace", "re.sub", "re.sub", "csv.writer.writerow", "len", "print"], "function", ["home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.preprocess.process_daren.split"], ["def", "build_data", "(", "data", ",", "path", ")", ":", "\n", "        ", "with", "open", "(", "path", ",", "'w'", ",", "encoding", "=", "'utf-8'", ")", "as", "f", ":", "\n", "            ", "csv_write", "=", "csv", ".", "writer", "(", "f", ",", "delimiter", "=", "'\\t'", ")", "\n", "csv_head", "=", "[", "\"article_id\"", ",", "\"sku\"", ",", "\"write_product\"", ",", "\"title\"", "]", "\n", "csv_write", ".", "writerow", "(", "csv_head", ")", "\n", "id", "=", "0", "\n", "for", "item", "in", "data", ":", "\n", "               ", "item", "=", "item", ".", "split", "(", "'|||'", ")", "\n", "sku_id", "=", "item", "[", "0", "]", "\n", "write_product", "=", "item", "[", "1", "]", ".", "replace", "(", "' '", ",", "''", ")", "\n", "title", "=", "item", "[", "2", "]", "[", ":", "-", "1", "]", "\n", "write_product", "=", "re", ".", "sub", "(", "u\"\\\\\uff08.*?\uff09|\\\\{.*?}|\\\\[.*?]|\\\\\u3010.*?\u3011[\uff0c|\u3002|\uff1b|\uff01]*\"", ",", "\"\"", ",", "write_product", ")", "\n", "if", "len", "(", "write_product", ")", "==", "0", ":", "\n", "                   ", "print", "(", "id", ")", "\n", "", "item_list", "=", "[", "id", ",", "sku_id", ",", "write_product", ",", "title", "]", "\n", "csv_write", ".", "writerow", "(", "item_list", ")", "\n", "id", "+=", "1", "\n", "\n"]], "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.title_generation.generate_feature_653.get_brand": [[21, 27], ["re.sub", "re.sub.split", "len"], "function", ["home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.preprocess.process_daren.split"], ["", "", "def", "get_brand", "(", "prod", ")", ":", "\n", "  ", "prod", "=", "re", ".", "sub", "(", "u'\\\\\uff08.*?\uff09|\\\\\u3010.*?\u3011'", ",", "\"\"", ",", "prod", ")", "\n", "brand", "=", "prod", ".", "split", "(", "','", ")", "[", "0", "]", "\n", "if", "len", "(", "brand", ")", ">", "6", ":", "\n", "    ", "brand", "=", "brand", "[", ":", "2", "]", "\n", "", "return", "brand", "\n", "\n"]], "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.title_generation.generate_feature_653.refine_feature": [[28, 55], ["re.findall", "feature.split", "feature.split", "len", "len", "subj_get.extend", "prod_get.extend", "subj.replace.replace", "subj.replace.replace", "re.findall", "re.findall", "len", "len"], "function", ["home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.preprocess.process_daren.split", "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.preprocess.process_daren.split"], ["", "def", "refine_feature", "(", "prod", ",", "feature", ")", ":", "\n", "#\u68c0\u67e5\u4e2d\u5fc3\u8bcd\uff0c \u662f\u5426\u662f5g", "\n", "    ", "pw", "=", "feature", ".", "split", "(", "'+'", ")", "[", "1", "]", "\n", "if", "pw", "==", "'5g\u624b\u673a'", "and", "'\u4e0d\u652f\u63015g'", "in", "prod", ":", "\n", "       ", "pw", "=", "'\u624b\u673a'", "\n", "\n", "#\u68c0\u67e5\u4fee\u9970\u8bcd", "\n", "", "subj", "=", "feature", ".", "split", "(", "'+'", ")", "[", "0", "]", "\n", "pattern_list", "=", "[", "[", "'[\u4e09|\u53cc|\u5355\uff5c\u56db]\u6444'", "]", ",", "[", "'[\u66f2\uff5c\u5168]\u9762\u5c4f'", "]", ",", "[", "'\\d+\\.?\\d*nm'", "]", ",", "[", "'\\d+\\.?\\d*\u4e07'", "]", ",", "[", "'\\d+\\.?\\d*\u82f1\u5bf8'", "]", ",", "[", "'\\d+\\.?\\d*[w|W|\u74e6]'", "]", ",", "[", "'\\d+\\.?\\d*[h|H][Z|z]'", "]", ",", "[", "'\\d+\\.?\\d*[m|M][a|A][h|H]'", ",", "'\\d+\\.?\\d*\u6beb\u5b89'", "]", "]", "\n", "for", "pattern_l", "in", "pattern_list", ":", "\n", "        ", "subj_get", "=", "[", "]", "\n", "prod_get", "=", "[", "]", "\n", "for", "pattern", "in", "pattern_l", ":", "\n", "                  ", "subj_get", ".", "extend", "(", "re", ".", "findall", "(", "pattern", ",", "subj", ")", ")", "\n", "prod_get", ".", "extend", "(", "re", ".", "findall", "(", "pattern", ",", "prod", ")", ")", "\n", "", "if", "len", "(", "subj_get", ")", ">", "0", "and", "len", "(", "prod_get", ")", ">", "0", ":", "\n", "                  ", "subj", "=", "subj", ".", "replace", "(", "subj_get", "[", "0", "]", ",", "prod_get", "[", "0", "]", ")", "\n", "#\u5220\u6389\u5b57\u6bcd\u6570\u5b57\u7ec4\u5408\u5982\u679c\u4ea7\u54c1\u91cc\u9762\u6ca1\u6709", "\n", "", "", "word", "=", "re", ".", "findall", "(", "'^[A-Za-z][A-Za-z\\d]{0,11}'", ",", "subj", ")", "\n", "if", "len", "(", "word", ")", ">", "0", ":", "\n", "      ", "if", "word", "[", "0", "]", "not", "in", "prod", "and", "'oled'", "not", "in", "word", "[", "0", "]", ":", "\n", "         ", "subj", "=", "subj", ".", "replace", "(", "word", "[", "0", "]", ",", "''", ")", "\n", "\n", "", "", "if", "len", "(", "subj", ")", ">", "1", ":", "\n", "      ", "return", "subj", "+", "' '", "+", "pw", "\n", "", "else", ":", "\n", "      ", "return", "pw", "\n", "\n"]], "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.evaluation.eval.Eval.__init__": [[18, 27], ["nlgeval.NLGEval", "sumeval.metrics.bleu.BLEUCalculator", "sumeval.metrics.rouge.RougeCalculator", "warnings.warn"], "methods", ["None"], ["gt_lines", "=", "remove_blank", "(", "load_file", "(", "gt_file", ")", ")", "\n", "\n", "assert", "len", "(", "predicted_lines", ")", "==", "len", "(", "gt_lines", ")", "\n", "\n", "\n", "results", "=", "tool", ".", "calculate", "(", "predicted_lines", ",", "gt_lines", ")", "\n", "print", "(", "results", ")", "# ", "\n", ""]], "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.evaluation.eval.Eval.calculate": [[28, 87], ["zip", "eval.Eval.nlgeval.compute_metrics", "eval.Eval.bleu_scorer.bleu", "sacrebleu_list.append", "rouge_1_list.append", "rouge_2_list.append", "rouge_L_list.append", "numpy.mean", "numpy.mean", "numpy.mean", "numpy.mean", "eval.Eval.rouge_scorer.rouge_1", "eval.Eval.rouge_scorer.rouge_2", "eval.Eval.rouge_scorer.rouge_l", "pred.lower().replace", "target.lower().replace", "pred.lower", "target.lower"], "methods", ["None"], []], "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.evaluation.eval.load_file": [[91, 95], ["open", "f.readlines"], "function", ["None"], []], "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.evaluation.eval.remove_blank": [[96, 98], ["l.replace"], "function", ["None"], []], "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.evaluation.utils.diagnose.__init__": [[59, 65], ["None"], "methods", ["None"], ["                ", "return", "6", "\n", "", "if", "orig_class", "in", "[", "3", ",", "4", ",", "9", "]", ":", "\n", "                ", "return", "8", "\n", "\n", "", "", "", "def", "load_json", "(", "path", ")", ":", "\n", "    ", "with", "open", "(", "path", ",", "\"r\"", ",", "encoding", "=", "\"utf8\"", ")", "as", "f", ":", "\n", "        ", "return", "json", ".", "load", "(", "f", ")", "\n"]], "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.evaluation.utils.diagnose.macbert_correct": [[66, 69], ["MacBertCorrector", "MacBertCorrector.macbert_correct"], "methods", ["home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.evaluation.utils.diagnose.macbert_correct"], ["\n", "\n", "", "", "def", "dump_json", "(", "path", ",", "data", ",", "indent", "=", "4", ")", ":", "\n", "    ", "with", "open", "(", "path", ",", "\"w\"", ",", "encoding", "=", "\"utf8\"", ")", "as", "f", ":", "\n"]], "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.evaluation.utils.diagnose.if_aspect": [[70, 79], ["LdaModel.load", "Dictionary.load_from_text", "seg_depart().split", "datapath", "classify", "utils.seg_depart"], "methods", ["home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.preprocess.process_daren.split", "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.postprocess.utils.classify", "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.evaluation.utils.seg_depart"], ["        ", "json", ".", "dump", "(", "data", ",", "f", ",", "indent", "=", "indent", ")", "\n", "\n", "\n", "", "", "def", "load_pickle", "(", "path", ")", ":", "\n", "    ", "with", "open", "(", "path", ",", "\"rb\"", ")", "as", "f", ":", "\n", "        ", "return", "pickle", ".", "load", "(", "f", ")", "\n", "\n", "\n", "", "", "def", "dump_pickle", "(", "path", ",", "data", ")", ":", "\n", "    ", "with", "open", "(", "path", ",", "\"wb\"", ")", "as", "f", ":", "\n"]], "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.evaluation.utils.class_by_keyword": [[4, 22], ["len", "len", "set", "set", "len", "set", "set", "len", "set", "set", "len", "set", "set", "len", "set", "set", "len", "set", "set", "set", "set"], "function", ["None"], ["\n", "#from pycorrector.macbert.macbert_corrector import MacBertCorrector", "\n", "import", "os", "\n", "import", "json", "\n", "import", "pickle", "\n", "import", "numpy", "as", "np", "\n", "from", "multiprocessing", "import", "Pool", "\n", "from", "collections", "import", "OrderedDict", "\n", "import", "pandas", "as", "pd", "\n", "import", "jieba", "\n", "\n", "def", "classify", "(", "tokens", ",", "lda_model", ",", "dic", ")", ":", "\n", "     ", "c_k", "=", "class_by_keyword", "(", "tokens", ")", "\n", "if", "c_k", ">=", "0", ":", "\n", "        ", "return", "c_k", "\n", "", "else", ":", "\n", "        ", "corpus", "=", "dic", ".", "doc2bow", "(", "tokens", ")", "\n"]], "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.evaluation.utils.stopwordslist": [[25, 29], ["stopwords.extend", "line.strip", "open().readlines", "open"], "function", ["None"], ["orig_class", "=", "row", "[", "0", "]", "[", "0", "]", "\n", "if", "orig_class", "in", "[", "11", "]", ":", "\n", "                ", "return", "0", "\n", "", "if", "orig_class", "in", "[", "5", ",", "6", ",", "8", "]", ":", "\n", "                ", "return", "3", "\n"]], "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.evaluation.utils.seg_depart": [[30, 40], ["jieba.cut", "utils.stopwordslist", "sentence.strip"], "function", ["home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.unilm.data.cut", "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.evaluation.utils.stopwordslist"], ["", "if", "orig_class", "in", "[", "2", "]", ":", "\n", "                ", "return", "2", "\n", "", "if", "orig_class", "in", "[", "0", ",", "1", ",", "3", ",", "9", "]", ":", "\n", "                ", "return", "4", "\n", "", "if", "orig_class", "in", "[", "4", "]", ":", "\n", "                ", "return", "1", "\n", "", "if", "orig_class", "in", "[", "10", ",", "7", "]", ":", "\n", "                ", "return", "5", "\n", "\n", "", "", "", "def", "classify_1343", "(", "tokens", ",", "lda_model", ",", "dic", ")", ":", "\n", "     ", "c_k", "=", "class_by_keyword_1314", "(", "tokens", ")", "\n"]], "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.evaluation.utils.check_multi": [[41, 56], ["None"], "function", ["None"], ["if", "c_k", ">=", "0", ":", "\n", "        ", "return", "c_k", "\n", "", "else", ":", "\n", "        ", "corpus", "=", "dic", ".", "doc2bow", "(", "tokens", ")", "\n", "row", "=", "lda_model", ".", "get_document_topics", "(", "corpus", ")", "\n", "row", "=", "sorted", "(", "row", ",", "key", "=", "lambda", "x", ":", "(", "x", "[", "1", "]", ")", ",", "reverse", "=", "True", ")", "\n", "orig_class", "=", "row", "[", "0", "]", "[", "0", "]", "\n", "if", "orig_class", "in", "[", "0", ",", "2", ",", "6", ",", "11", "]", ":", "\n", "                ", "return", "0", "\n", "", "if", "orig_class", "in", "[", "1", "]", ":", "\n", "                ", "return", "1", "\n", "", "if", "orig_class", "in", "[", "7", ",", "8", "]", ":", "\n", "                ", "return", "2", "\n", "", "if", "orig_class", "in", "[", "5", "]", ":", "\n", "                ", "return", "3", "\n", "", "if", "orig_class", "in", "[", "10", "]", ":", "\n"]], "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.evaluation.utils.class_by_keyword_1342": [[80, 101], ["len", "len", "set", "set", "len", "set", "set", "len", "set", "set", "len", "set", "set", "len", "set", "set", "len", "set", "set", "len", "set", "set", "set", "set"], "function", ["None"], ["        ", "pickle", ".", "dump", "(", "data", ",", "f", ")", "\n", "\n", "", "", "def", "load_csv", "(", "path", ")", ":", "\n", "    ", "data", "=", "[", "]", "\n", "df", "=", "pd", ".", "read_csv", "(", "path", ")", "\n", "for", "row", "in", "df", ".", "iterrows", "(", ")", ":", "\n", "      ", "data", ".", "append", "(", "row", "[", "1", "]", "[", "'product'", "]", "+", "'|||'", "+", "row", "[", "1", "]", "[", "'pred'", "]", ")", "\n", "", "return", "data", "\n", "\n", "", "def", "load_csv_sku", "(", "path", ")", ":", "\n", "    ", "data", "=", "[", "]", "\n", "df", "=", "pd", ".", "read_csv", "(", "path", ")", "\n", "for", "row", "in", "df", ".", "iterrows", "(", ")", ":", "\n", "      ", "data", ".", "append", "(", "row", "[", "1", "]", "[", "'product'", "]", "+", "'|||'", "+", "row", "[", "1", "]", "[", "'pred'", "]", "+", "'|||'", "+", "str", "(", "row", "[", "1", "]", "[", "'sku'", "]", ")", ")", "\n", "", "return", "data", "\n", "\n", "", "def", "load_csv_sku_aspect", "(", "path", ")", ":", "\n", "    ", "data", "=", "[", "]", "\n", "df", "=", "pd", ".", "read_csv", "(", "path", ")", "\n", "for", "row", "in", "df", ".", "iterrows", "(", ")", ":", "\n", "      ", "prod", "=", "row", "[", "1", "]", "[", "'product'", "]", ".", "split", "(", "'|||'", ")", "[", "1", "]", "\n", "aspect", "=", "row", "[", "1", "]", "[", "'product'", "]", ".", "split", "(", "'|||'", ")", "[", "0", "]", "\n"]], "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.unilm.data_tmp.get_sku": [[5, 37], ["collections.defaultdict", "tqdm.tqdm", "print", "open", "csv.reader", "list", "new_data[].add", "open", "csv.reader", "list", "open", "csv.writer", "csv.writer.writerow", "tqdm.tqdm", "set", "a[].split", "csv.writer.writerow", "list", "line[].lower"], "function", ["home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.biunilm.loader_utils.TrieTree.add", "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.preprocess.process_daren.split"], ["def", "get_sku", "(", ")", ":", "\n", "    ", "path", "=", "\"data/raw/processed_daren653_sku.csv\"", "\n", "# path = \"data/raw/train.csv\"", "\n", "with", "open", "(", "path", ",", "\"r\"", ")", "as", "f", ":", "\n", "        ", "reader", "=", "csv", ".", "reader", "(", "f", ")", "\n", "data", "=", "list", "(", "reader", ")", "\n", "", "new_data", "=", "defaultdict", "(", "lambda", ":", "set", "(", ")", ")", "\n", "for", "line", "in", "tqdm", "(", "data", "[", "1", ":", "]", ")", ":", "\n", "        ", "new_data", "[", "line", "[", "0", "]", ".", "lower", "(", ")", "]", ".", "add", "(", "line", "[", "-", "1", "]", ")", "\n", "\n", "# for key in tqdm(new_data):", "\n", "#     if len(new_data[key]) > 1:", "\n", "#         print(key)", "\n", "#         print(new_data[key])", "\n", "\n", "", "path", "=", "\"data/feat/v3/processed/output2.csv\"", "\n", "with", "open", "(", "path", ",", "\"r\"", ")", "as", "f", ":", "\n", "        ", "reader", "=", "csv", ".", "reader", "(", "f", ")", "\n", "data", "=", "list", "(", "reader", ")", "\n", "", "save", "=", "\"data/feat/v3/processed/output2_sku.csv\"", "\n", "cnt", "=", "0", "\n", "with", "open", "(", "save", ",", "\"w\"", ")", "as", "f", ":", "\n", "        ", "cf", "=", "csv", ".", "writer", "(", "f", ")", "\n", "cf", ".", "writerow", "(", "data", "[", "0", "]", "+", "[", "\"sku\"", "]", ")", "\n", "for", "a", "in", "tqdm", "(", "data", "[", "1", ":", "]", ")", ":", "\n", "            ", "attr", "=", "a", "[", "1", "]", ".", "split", "(", "\"|||\"", ")", "[", "-", "1", "]", "\n", "if", "attr", "in", "new_data", ":", "\n", "                ", "sku", "=", "list", "(", "new_data", "[", "attr", "]", ")", "[", "0", "]", "\n", "cf", ".", "writerow", "(", "a", "+", "[", "sku", "]", ")", "\n", "", "else", ":", "\n", "                ", "cnt", "+=", "1", "\n", "", "", "", "print", "(", "cnt", ")", "\n", "", "get_sku", "(", ")", ""]], "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.unilm.run_seq2seq._get_max_epoch_model": [[38, 49], ["glob.glob", "glob.glob", "os.path.join", "os.path.join", "set", "set", "max", "int", "int", "pathlib.Path().stem.split", "pathlib.Path().stem.split", "pathlib.Path", "pathlib.Path"], "function", ["home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.preprocess.process_daren.split", "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.preprocess.process_daren.split"], ["def", "_get_max_epoch_model", "(", "output_dir", ")", ":", "\n", "    ", "fn_model_list", "=", "glob", ".", "glob", "(", "os", ".", "path", ".", "join", "(", "output_dir", ",", "\"model.*.bin\"", ")", ")", "\n", "fn_optim_list", "=", "glob", ".", "glob", "(", "os", ".", "path", ".", "join", "(", "output_dir", ",", "\"optim.*.bin\"", ")", ")", "\n", "if", "(", "not", "fn_model_list", ")", "or", "(", "not", "fn_optim_list", ")", ":", "\n", "        ", "return", "None", "\n", "", "both_set", "=", "set", "(", "[", "int", "(", "Path", "(", "fn", ")", ".", "stem", ".", "split", "(", "'.'", ")", "[", "-", "1", "]", ")", "for", "fn", "in", "fn_model_list", "]", "\n", ")", "&", "set", "(", "[", "int", "(", "Path", "(", "fn", ")", ".", "stem", ".", "split", "(", "'.'", ")", "[", "-", "1", "]", ")", "for", "fn", "in", "fn_optim_list", "]", ")", "\n", "if", "both_set", ":", "\n", "        ", "return", "max", "(", "both_set", ")", "\n", "", "else", ":", "\n", "        ", "return", "None", "\n", "\n"]], "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.unilm.run_seq2seq.main": [[51, 493], ["argparse.ArgumentParser", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.parse_args", "parser.parse_args.output_dir.replace", "parser.parse_args.log_dir.replace", "os.makedirs", "os.makedirs", "json.dump", "logger.info", "int", "random.seed", "numpy.random.seed", "torch.manual_seed", "torch.manual_seed", "pytorch_pretrained_bert.tokenization.BertTokenizer.from_pretrained", "int", "run_seq2seq._get_max_epoch_model", "nn.data_parallel.DataParallelImbalance.to", "list", "logger.info", "torch.cuda.empty_cache", "torch.cuda.empty_cache", "os.getenv", "os.getenv", "open", "torch.device", "torch.device", "torch.cuda.device_count", "torch.cuda.device_count", "torch.cuda.set_device", "torch.cuda.set_device", "torch.device", "torch.device", "torch.init_process_group", "ValueError", "torch.cuda.manual_seed_all", "torch.cuda.manual_seed_all", "ValueError", "torch.barrier", "pytorch_pretrained_bert.tokenization.WhitespaceTokenizer", "torch.barrier", "print", "os.path.join", "os.path.join", "biunilm.Seq2SeqDataset", "torch.utils.data.DataLoader", "torch.utils.data.DataLoader", "torch.barrier", "pytorch_pretrained_bert.modeling.BertForPreTrainingLossMask.from_pretrained", "pytorch_pretrained_bert.modeling.BertForPreTrainingLossMask.from_pretrained", "torch.barrier", "nn.data_parallel.DataParallelImbalance.named_parameters", "FusedAdam", "apex.amp.initialize", "logger.info", "pytorch_pretrained_bert.optimization.BertAdam", "DDP", "logger.info", "torch.load", "torch.load", "hasattr", "pytorch_pretrained_bert.optimization.BertAdam.load_state_dict", "logger.info", "logger.info", "logger.info", "nn.data_parallel.DataParallelImbalance.train", "tqdm.trange", "os.path.join", "bool", "biunilm.Preprocess4Seq2seq", "os.path.join", "torch.utils.data.RandomSampler", "torch.utils.data.distributed.DistributedSampler", "logger.info", "torch.load", "torch.load", "math.floor", "nn.data_parallel.DataParallelImbalance.bert.embeddings.word_embeddings.float", "nn.data_parallel.DataParallelImbalance.bert.embeddings.position_embeddings.float", "nn.data_parallel.DataParallelImbalance.bert.embeddings.token_type_embeddings.float", "nn.data_parallel.DataParallelImbalance", "os.path.join", "optim_recover.state_dict.state_dict", "logger.info", "tqdm.tqdm", "enumerate", "list", "torch.get_world_size", "len", "os.path.join", "logger.info", "torch.load", "torch.load", "ImportError", "ImportError", "int", "torch.utils.data.distributed.DistributedSampler.set_epoch", "nn.data_parallel.DataParallelImbalance.", "tqdm.tqdm.set_description", "logger.info", "os.path.join", "torch.save", "torch.save", "os.path.join", "torch.save", "torch.save", "torch.cuda.is_available", "torch.cuda.is_available", "BertTokenizer.from_pretrained.vocab.keys", "any", "masked_lm_loss.mean.mean", "next_sentence_loss.mean.mean", "loss.backward", "pytorch_pretrained_bert.optimization.BertAdam.step", "pytorch_pretrained_bert.optimization.BertAdam.zero_grad", "torch.distributed.get_rank", "torch.distributed.get_rank", "hasattr", "model_to_save.state_dict", "pytorch_pretrained_bert.optimization.BertAdam.state_dict", "any", "t.to", "loss.item", "apex.amp.scale_loss", "scaled_loss.backward", "pytorch_pretrained_bert.optimization.warmup_linear"], "function", ["home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.pytorch_pretrained_bert.tokenization.BertTokenizer.from_pretrained", "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.unilm.run_seq2seq._get_max_epoch_model", "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.pytorch_pretrained_bert.tokenization.BertTokenizer.from_pretrained", "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.pytorch_pretrained_bert.tokenization.BertTokenizer.from_pretrained", "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.pytorch_pretrained_bert.optimization_fp16.FP16_Optimizer_State.load_state_dict", "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.pytorch_pretrained_bert.optimization_fp16.FP16_Optimizer_State.state_dict", "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.pytorch_pretrained_bert.optimization.BertAdamFineTune.step", "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.pytorch_pretrained_bert.optimization_fp16.FP16_Optimizer_State.state_dict", "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.pytorch_pretrained_bert.optimization_fp16.FP16_Optimizer_State.state_dict", "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.pytorch_pretrained_bert.optimization.warmup_linear"], ["", "", "def", "main", "(", ")", ":", "\n", "    ", "parser", "=", "argparse", ".", "ArgumentParser", "(", ")", "\n", "\n", "# Required parameters", "\n", "parser", ".", "add_argument", "(", "\"--data_dir\"", ",", "\n", "default", "=", "None", ",", "\n", "type", "=", "str", ",", "\n", "required", "=", "True", ",", "\n", "help", "=", "\"The input data dir. Should contain the .tsv files (or other data files) for the task.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--src_file\"", ",", "default", "=", "None", ",", "type", "=", "str", ",", "\n", "help", "=", "\"The input data file name.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--tgt_file\"", ",", "default", "=", "None", ",", "type", "=", "str", ",", "\n", "help", "=", "\"The output data file name.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--bert_model\"", ",", "default", "=", "None", ",", "type", "=", "str", ",", "required", "=", "True", ",", "\n", "help", "=", "\"Bert pre-trained model selected in the list: bert-base-uncased, \"", "\n", "\"bert-large-uncased, bert-base-cased, bert-base-multilingual, bert-base-chinese.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--config_path\"", ",", "default", "=", "None", ",", "type", "=", "str", ",", "\n", "help", "=", "\"Bert config file path.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--output_dir\"", ",", "\n", "default", "=", "None", ",", "\n", "type", "=", "str", ",", "\n", "required", "=", "True", ",", "\n", "help", "=", "\"The output directory where the model predictions and checkpoints will be written.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--log_dir\"", ",", "\n", "default", "=", "''", ",", "\n", "type", "=", "str", ",", "\n", "required", "=", "True", ",", "\n", "help", "=", "\"The output directory where the log will be written.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--model_recover_path\"", ",", "\n", "default", "=", "None", ",", "\n", "type", "=", "str", ",", "\n", "help", "=", "\"The file of fine-tuned pretraining model.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--optim_recover_path\"", ",", "\n", "default", "=", "None", ",", "\n", "type", "=", "str", ",", "\n", "help", "=", "\"The file of pretraining optimizer.\"", ")", "\n", "\n", "# Other parameters", "\n", "parser", ".", "add_argument", "(", "\"--max_seq_length\"", ",", "\n", "default", "=", "128", ",", "\n", "type", "=", "int", ",", "\n", "help", "=", "\"The maximum total input sequence length after WordPiece tokenization. \\n\"", "\n", "\"Sequences longer than this will be truncated, and sequences shorter \\n\"", "\n", "\"than this will be padded.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--do_train\"", ",", "\n", "action", "=", "'store_true'", ",", "\n", "help", "=", "\"Whether to run training.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--do_eval\"", ",", "\n", "action", "=", "'store_true'", ",", "\n", "help", "=", "\"Whether to run eval on the dev set.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--do_lower_case\"", ",", "\n", "action", "=", "'store_true'", ",", "\n", "help", "=", "\"Set this flag if you are using an uncased model.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--train_batch_size\"", ",", "\n", "default", "=", "32", ",", "\n", "type", "=", "int", ",", "\n", "help", "=", "\"Total batch size for training.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--eval_batch_size\"", ",", "\n", "default", "=", "64", ",", "\n", "type", "=", "int", ",", "\n", "help", "=", "\"Total batch size for eval.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--learning_rate\"", ",", "default", "=", "5e-5", ",", "type", "=", "float", ",", "\n", "help", "=", "\"The initial learning rate for Adam.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--label_smoothing\"", ",", "default", "=", "0", ",", "type", "=", "float", ",", "\n", "help", "=", "\"The initial learning rate for Adam.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--weight_decay\"", ",", "\n", "default", "=", "0.01", ",", "\n", "type", "=", "float", ",", "\n", "help", "=", "\"The weight decay rate for Adam.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--finetune_decay\"", ",", "\n", "action", "=", "'store_true'", ",", "\n", "help", "=", "\"Weight decay to the original weights.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--num_train_epochs\"", ",", "\n", "default", "=", "3.0", ",", "\n", "type", "=", "float", ",", "\n", "help", "=", "\"Total number of training epochs to perform.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--warmup_proportion\"", ",", "\n", "default", "=", "0.1", ",", "\n", "type", "=", "float", ",", "\n", "help", "=", "\"Proportion of training to perform linear learning rate warmup for. \"", "\n", "\"E.g., 0.1 = 10%% of training.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--hidden_dropout_prob\"", ",", "default", "=", "0.1", ",", "type", "=", "float", ",", "\n", "help", "=", "\"Dropout rate for hidden states.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--attention_probs_dropout_prob\"", ",", "default", "=", "0.1", ",", "type", "=", "float", ",", "\n", "help", "=", "\"Dropout rate for attention probabilities.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--no_cuda\"", ",", "\n", "action", "=", "'store_true'", ",", "\n", "help", "=", "\"Whether not to use CUDA when available\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--local_rank\"", ",", "\n", "type", "=", "int", ",", "\n", "default", "=", "-", "1", ",", "\n", "help", "=", "\"local_rank for distributed training on gpus\"", ")", "\n", "parser", ".", "add_argument", "(", "'--seed'", ",", "\n", "type", "=", "int", ",", "\n", "default", "=", "42", ",", "\n", "help", "=", "\"random seed for initialization\"", ")", "\n", "parser", ".", "add_argument", "(", "'--gradient_accumulation_steps'", ",", "\n", "type", "=", "int", ",", "\n", "default", "=", "1", ",", "\n", "help", "=", "\"Number of updates steps to accumulate before performing a backward/update pass.\"", ")", "\n", "parser", ".", "add_argument", "(", "'--fp16'", ",", "action", "=", "'store_true'", ",", "\n", "help", "=", "\"Whether to use 16-bit float precision instead of 32-bit\"", ")", "\n", "parser", ".", "add_argument", "(", "'--fp32_embedding'", ",", "action", "=", "'store_true'", ",", "\n", "help", "=", "\"Whether to use 32-bit float precision instead of 16-bit for embeddings\"", ")", "\n", "parser", ".", "add_argument", "(", "'--loss_scale'", ",", "type", "=", "float", ",", "default", "=", "0", ",", "\n", "help", "=", "\"Loss scaling to improve fp16 numeric stability. Only used when fp16 set to True.\\n\"", "\n", "\"0 (default value): dynamic loss scaling.\\n\"", "\n", "\"Positive power of 2: static loss scaling value.\\n\"", ")", "\n", "parser", ".", "add_argument", "(", "'--amp'", ",", "action", "=", "'store_true'", ",", "\n", "help", "=", "\"Whether to use amp for fp16\"", ")", "\n", "parser", ".", "add_argument", "(", "'--from_scratch'", ",", "action", "=", "'store_true'", ",", "\n", "help", "=", "\"Initialize parameters with random values (i.e., training from scratch).\"", ")", "\n", "parser", ".", "add_argument", "(", "'--new_segment_ids'", ",", "action", "=", "'store_true'", ",", "\n", "help", "=", "\"Use new segment ids for bi-uni-directional LM.\"", ")", "\n", "parser", ".", "add_argument", "(", "'--new_pos_ids'", ",", "action", "=", "'store_true'", ",", "\n", "help", "=", "\"Use new position ids for LMs.\"", ")", "\n", "parser", ".", "add_argument", "(", "'--tokenized_input'", ",", "action", "=", "'store_true'", ",", "\n", "help", "=", "\"Whether the input is tokenized.\"", ")", "\n", "parser", ".", "add_argument", "(", "'--max_len_a'", ",", "type", "=", "int", ",", "default", "=", "0", ",", "\n", "help", "=", "\"Truncate_config: maximum length of segment A.\"", ")", "\n", "parser", ".", "add_argument", "(", "'--max_len_b'", ",", "type", "=", "int", ",", "default", "=", "0", ",", "\n", "help", "=", "\"Truncate_config: maximum length of segment B.\"", ")", "\n", "parser", ".", "add_argument", "(", "'--trunc_seg'", ",", "default", "=", "''", ",", "\n", "help", "=", "\"Truncate_config: first truncate segment A/B (option: a, b).\"", ")", "\n", "parser", ".", "add_argument", "(", "'--always_truncate_tail'", ",", "action", "=", "'store_true'", ",", "\n", "help", "=", "\"Truncate_config: Whether we should always truncate tail.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--mask_prob\"", ",", "default", "=", "0.15", ",", "type", "=", "float", ",", "\n", "help", "=", "\"Number of prediction is sometimes less than max_pred when sequence is short.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--mask_prob_eos\"", ",", "default", "=", "0", ",", "type", "=", "float", ",", "\n", "help", "=", "\"Number of prediction is sometimes less than max_pred when sequence is short.\"", ")", "\n", "parser", ".", "add_argument", "(", "'--max_pred'", ",", "type", "=", "int", ",", "default", "=", "20", ",", "\n", "help", "=", "\"Max tokens of prediction.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--num_workers\"", ",", "default", "=", "0", ",", "type", "=", "int", ",", "\n", "help", "=", "\"Number of workers for the data loader.\"", ")", "\n", "\n", "parser", ".", "add_argument", "(", "'--mask_source_words'", ",", "action", "=", "'store_true'", ",", "\n", "help", "=", "\"Whether to mask source words for training\"", ")", "\n", "parser", ".", "add_argument", "(", "'--skipgram_prb'", ",", "type", "=", "float", ",", "default", "=", "0.0", ",", "\n", "help", "=", "'prob of ngram mask'", ")", "\n", "parser", ".", "add_argument", "(", "'--skipgram_size'", ",", "type", "=", "int", ",", "default", "=", "1", ",", "\n", "help", "=", "'the max size of ngram mask'", ")", "\n", "parser", ".", "add_argument", "(", "'--mask_whole_word'", ",", "action", "=", "'store_true'", ",", "\n", "help", "=", "\"Whether masking a whole word.\"", ")", "\n", "parser", ".", "add_argument", "(", "'--do_l2r_training'", ",", "action", "=", "'store_true'", ",", "\n", "help", "=", "\"Whether to do left to right training\"", ")", "\n", "parser", ".", "add_argument", "(", "'--has_sentence_oracle'", ",", "action", "=", "'store_true'", ",", "\n", "help", "=", "\"Whether to have sentence level oracle for training. \"", "\n", "\"Only useful for summary generation\"", ")", "\n", "parser", ".", "add_argument", "(", "'--max_position_embeddings'", ",", "type", "=", "int", ",", "default", "=", "None", ",", "\n", "help", "=", "\"max position embeddings\"", ")", "\n", "parser", ".", "add_argument", "(", "'--relax_projection'", ",", "action", "=", "'store_true'", ",", "\n", "help", "=", "\"Use different projection layers for tasks.\"", ")", "\n", "parser", ".", "add_argument", "(", "'--ffn_type'", ",", "default", "=", "0", ",", "type", "=", "int", ",", "\n", "help", "=", "\"0: default mlp; 1: W((Wx+b) elem_prod x);\"", ")", "\n", "parser", ".", "add_argument", "(", "'--num_qkv'", ",", "default", "=", "0", ",", "type", "=", "int", ",", "\n", "help", "=", "\"Number of different <Q,K,V>.\"", ")", "\n", "parser", ".", "add_argument", "(", "'--seg_emb'", ",", "action", "=", "'store_true'", ",", "\n", "help", "=", "\"Using segment embedding for self-attention.\"", ")", "\n", "parser", ".", "add_argument", "(", "'--s2s_special_token'", ",", "action", "=", "'store_true'", ",", "\n", "help", "=", "\"New special tokens ([S2S_SEP]/[S2S_CLS]) of S2S.\"", ")", "\n", "parser", ".", "add_argument", "(", "'--s2s_add_segment'", ",", "action", "=", "'store_true'", ",", "\n", "help", "=", "\"Additional segmental for the encoder of S2S.\"", ")", "\n", "parser", ".", "add_argument", "(", "'--s2s_share_segment'", ",", "action", "=", "'store_true'", ",", "\n", "help", "=", "\"Sharing segment embeddings for the encoder of S2S (used with --s2s_add_segment).\"", ")", "\n", "parser", ".", "add_argument", "(", "'--pos_shift'", ",", "action", "=", "'store_true'", ",", "\n", "help", "=", "\"Using position shift for fine-tuning.\"", ")", "\n", "\n", "args", "=", "parser", ".", "parse_args", "(", ")", "\n", "\n", "# assert Path(args.model_recover_path).exists(", "\n", "# ), \"--model_recover_path doesn't exist\"", "\n", "\n", "args", ".", "output_dir", "=", "args", ".", "output_dir", ".", "replace", "(", "\n", "'[PT_OUTPUT_DIR]'", ",", "os", ".", "getenv", "(", "'PT_OUTPUT_DIR'", ",", "''", ")", ")", "\n", "args", ".", "log_dir", "=", "args", ".", "log_dir", ".", "replace", "(", "\n", "'[PT_OUTPUT_DIR]'", ",", "os", ".", "getenv", "(", "'PT_OUTPUT_DIR'", ",", "''", ")", ")", "\n", "\n", "os", ".", "makedirs", "(", "args", ".", "output_dir", ",", "exist_ok", "=", "True", ")", "\n", "os", ".", "makedirs", "(", "args", ".", "log_dir", ",", "exist_ok", "=", "True", ")", "\n", "json", ".", "dump", "(", "args", ".", "__dict__", ",", "open", "(", "os", ".", "path", ".", "join", "(", "\n", "args", ".", "output_dir", ",", "'opt.json'", ")", ",", "'w'", ")", ",", "sort_keys", "=", "True", ",", "indent", "=", "2", ")", "\n", "\n", "if", "args", ".", "local_rank", "==", "-", "1", "or", "args", ".", "no_cuda", ":", "\n", "        ", "device", "=", "torch", ".", "device", "(", "\n", "\"cuda\"", "if", "torch", ".", "cuda", ".", "is_available", "(", ")", "and", "not", "args", ".", "no_cuda", "else", "\"cpu\"", ")", "\n", "n_gpu", "=", "torch", ".", "cuda", ".", "device_count", "(", ")", "\n", "", "else", ":", "\n", "        ", "torch", ".", "cuda", ".", "set_device", "(", "args", ".", "local_rank", ")", "\n", "device", "=", "torch", ".", "device", "(", "\"cuda\"", ",", "args", ".", "local_rank", ")", "\n", "n_gpu", "=", "1", "\n", "# Initializes the distributed backend which will take care of sychronizing nodes/GPUs", "\n", "dist", ".", "init_process_group", "(", "backend", "=", "'nccl'", ")", "\n", "", "logger", ".", "info", "(", "\"device: {} n_gpu: {}, distributed training: {}, 16-bits training: {}\"", ".", "format", "(", "\n", "device", ",", "n_gpu", ",", "bool", "(", "args", ".", "local_rank", "!=", "-", "1", ")", ",", "args", ".", "fp16", ")", ")", "\n", "\n", "if", "args", ".", "gradient_accumulation_steps", "<", "1", ":", "\n", "        ", "raise", "ValueError", "(", "\"Invalid gradient_accumulation_steps parameter: {}, should be >= 1\"", ".", "format", "(", "\n", "args", ".", "gradient_accumulation_steps", ")", ")", "\n", "\n", "", "args", ".", "train_batch_size", "=", "int", "(", "\n", "args", ".", "train_batch_size", "/", "args", ".", "gradient_accumulation_steps", ")", "\n", "\n", "random", ".", "seed", "(", "args", ".", "seed", ")", "\n", "np", ".", "random", ".", "seed", "(", "args", ".", "seed", ")", "\n", "torch", ".", "manual_seed", "(", "args", ".", "seed", ")", "\n", "if", "n_gpu", ">", "0", ":", "\n", "        ", "torch", ".", "cuda", ".", "manual_seed_all", "(", "args", ".", "seed", ")", "\n", "\n", "", "if", "not", "args", ".", "do_train", "and", "not", "args", ".", "do_eval", ":", "\n", "        ", "raise", "ValueError", "(", "\n", "\"At least one of `do_train` or `do_eval` must be True.\"", ")", "\n", "\n", "", "if", "args", ".", "local_rank", "not", "in", "(", "-", "1", ",", "0", ")", ":", "\n", "# Make sure only the first process in distributed training will download model & vocab", "\n", "        ", "dist", ".", "barrier", "(", ")", "\n", "", "tokenizer", "=", "BertTokenizer", ".", "from_pretrained", "(", "\n", "args", ".", "bert_model", ",", "do_lower_case", "=", "args", ".", "do_lower_case", ")", "\n", "if", "args", ".", "max_position_embeddings", ":", "\n", "        ", "tokenizer", ".", "max_len", "=", "args", ".", "max_position_embeddings", "\n", "", "data_tokenizer", "=", "WhitespaceTokenizer", "(", ")", "if", "args", ".", "tokenized_input", "else", "tokenizer", "\n", "if", "args", ".", "local_rank", "==", "0", ":", "\n", "        ", "dist", ".", "barrier", "(", ")", "\n", "\n", "", "if", "args", ".", "do_train", ":", "\n", "        ", "print", "(", "\"Loading Train Dataset\"", ",", "args", ".", "data_dir", ")", "\n", "bi_uni_pipeline", "=", "[", "seq2seq_loader", ".", "Preprocess4Seq2seq", "(", "args", ".", "max_pred", ",", "args", ".", "mask_prob", ",", "list", "(", "tokenizer", ".", "vocab", ".", "keys", "(", "\n", ")", ")", ",", "tokenizer", ".", "convert_tokens_to_ids", ",", "args", ".", "max_seq_length", ",", "new_segment_ids", "=", "args", ".", "new_segment_ids", ",", "truncate_config", "=", "{", "'max_len_a'", ":", "args", ".", "max_len_a", ",", "'max_len_b'", ":", "args", ".", "max_len_b", ",", "'trunc_seg'", ":", "args", ".", "trunc_seg", ",", "'always_truncate_tail'", ":", "args", ".", "always_truncate_tail", "}", ",", "mask_source_words", "=", "args", ".", "mask_source_words", ",", "skipgram_prb", "=", "args", ".", "skipgram_prb", ",", "skipgram_size", "=", "args", ".", "skipgram_size", ",", "mask_whole_word", "=", "args", ".", "mask_whole_word", ",", "mode", "=", "\"s2s\"", ",", "has_oracle", "=", "args", ".", "has_sentence_oracle", ",", "num_qkv", "=", "args", ".", "num_qkv", ",", "s2s_special_token", "=", "args", ".", "s2s_special_token", ",", "s2s_add_segment", "=", "args", ".", "s2s_add_segment", ",", "s2s_share_segment", "=", "args", ".", "s2s_share_segment", ",", "pos_shift", "=", "args", ".", "pos_shift", ")", "]", "\n", "file_oracle", "=", "None", "\n", "if", "args", ".", "has_sentence_oracle", ":", "\n", "            ", "file_oracle", "=", "os", ".", "path", ".", "join", "(", "args", ".", "data_dir", ",", "'train.oracle'", ")", "\n", "", "fn_src", "=", "os", ".", "path", ".", "join", "(", "\n", "args", ".", "data_dir", ",", "args", ".", "src_file", "if", "args", ".", "src_file", "else", "'train.src'", ")", "\n", "fn_tgt", "=", "os", ".", "path", ".", "join", "(", "\n", "args", ".", "data_dir", ",", "args", ".", "tgt_file", "if", "args", ".", "tgt_file", "else", "'train.tgt'", ")", "\n", "train_dataset", "=", "seq2seq_loader", ".", "Seq2SeqDataset", "(", "\n", "fn_src", ",", "fn_tgt", ",", "args", ".", "train_batch_size", ",", "data_tokenizer", ",", "args", ".", "max_seq_length", ",", "file_oracle", "=", "file_oracle", ",", "bi_uni_pipeline", "=", "bi_uni_pipeline", ")", "\n", "if", "args", ".", "local_rank", "==", "-", "1", ":", "\n", "            ", "train_sampler", "=", "RandomSampler", "(", "train_dataset", ",", "replacement", "=", "False", ")", "\n", "_batch_size", "=", "args", ".", "train_batch_size", "\n", "", "else", ":", "\n", "            ", "train_sampler", "=", "DistributedSampler", "(", "train_dataset", ")", "\n", "_batch_size", "=", "args", ".", "train_batch_size", "//", "dist", ".", "get_world_size", "(", ")", "\n", "", "train_dataloader", "=", "torch", ".", "utils", ".", "data", ".", "DataLoader", "(", "train_dataset", ",", "batch_size", "=", "_batch_size", ",", "sampler", "=", "train_sampler", ",", "\n", "num_workers", "=", "args", ".", "num_workers", ",", "collate_fn", "=", "seq2seq_loader", ".", "batch_list_to_batch_tensors", ",", "pin_memory", "=", "False", ")", "\n", "\n", "# note: args.train_batch_size has been changed to (/= args.gradient_accumulation_steps)", "\n", "# t_total = int(math.ceil(len(train_dataset.ex_list) / args.train_batch_size)", "\n", "", "t_total", "=", "int", "(", "len", "(", "train_dataloader", ")", "*", "args", ".", "num_train_epochs", "/", "\n", "args", ".", "gradient_accumulation_steps", ")", "\n", "\n", "# amp_handle = None", "\n", "# if args.fp16 and args.amp:", "\n", "#     from apex import amp", "\n", "#     amp_handle = amp.init(enable_caching=True)", "\n", "#     logger.info(\"enable fp16 with amp\")", "\n", "\n", "# Prepare model", "\n", "recover_step", "=", "_get_max_epoch_model", "(", "args", ".", "output_dir", ")", "\n", "cls_num_labels", "=", "2", "\n", "type_vocab_size", "=", "6", "+", "(", "1", "if", "args", ".", "s2s_add_segment", "else", "0", ")", "if", "args", ".", "new_segment_ids", "else", "2", "\n", "num_sentlvl_labels", "=", "2", "if", "args", ".", "has_sentence_oracle", "else", "0", "\n", "relax_projection", "=", "4", "if", "args", ".", "relax_projection", "else", "0", "\n", "if", "args", ".", "local_rank", "not", "in", "(", "-", "1", ",", "0", ")", ":", "\n", "# Make sure only the first process in distributed training will download model & vocab", "\n", "        ", "dist", ".", "barrier", "(", ")", "\n", "", "if", "(", "recover_step", "is", "None", ")", "and", "(", "args", ".", "model_recover_path", "is", "None", ")", ":", "\n", "# if _state_dict == {}, the parameters are randomly initialized", "\n", "# if _state_dict == None, the parameters are initialized with bert-init", "\n", "        ", "_state_dict", "=", "{", "}", "if", "args", ".", "from_scratch", "else", "None", "\n", "model", "=", "BertForPreTrainingLossMask", ".", "from_pretrained", "(", "\n", "args", ".", "bert_model", ",", "state_dict", "=", "_state_dict", ",", "num_labels", "=", "cls_num_labels", ",", "num_rel", "=", "0", ",", "type_vocab_size", "=", "type_vocab_size", ",", "config_path", "=", "args", ".", "config_path", ",", "task_idx", "=", "3", ",", "num_sentlvl_labels", "=", "num_sentlvl_labels", ",", "max_position_embeddings", "=", "args", ".", "max_position_embeddings", ",", "label_smoothing", "=", "args", ".", "label_smoothing", ",", "fp32_embedding", "=", "args", ".", "fp32_embedding", ",", "relax_projection", "=", "relax_projection", ",", "new_pos_ids", "=", "args", ".", "new_pos_ids", ",", "ffn_type", "=", "args", ".", "ffn_type", ",", "hidden_dropout_prob", "=", "args", ".", "hidden_dropout_prob", ",", "attention_probs_dropout_prob", "=", "args", ".", "attention_probs_dropout_prob", ",", "num_qkv", "=", "args", ".", "num_qkv", ",", "seg_emb", "=", "args", ".", "seg_emb", ")", "\n", "global_step", "=", "0", "\n", "", "else", ":", "\n", "        ", "if", "recover_step", ":", "\n", "            ", "logger", ".", "info", "(", "\"***** Recover model: %d *****\"", ",", "recover_step", ")", "\n", "model_recover", "=", "torch", ".", "load", "(", "os", ".", "path", ".", "join", "(", "\n", "args", ".", "output_dir", ",", "\"model.{0}.bin\"", ".", "format", "(", "recover_step", ")", ")", ",", "map_location", "=", "'cpu'", ")", "\n", "# recover_step == number of epochs", "\n", "global_step", "=", "math", ".", "floor", "(", "\n", "recover_step", "*", "t_total", "/", "args", ".", "num_train_epochs", ")", "\n", "", "elif", "args", ".", "model_recover_path", ":", "\n", "            ", "logger", ".", "info", "(", "\"***** Recover model: %s *****\"", ",", "\n", "args", ".", "model_recover_path", ")", "\n", "model_recover", "=", "torch", ".", "load", "(", "\n", "args", ".", "model_recover_path", ",", "map_location", "=", "'cpu'", ")", "\n", "global_step", "=", "0", "\n", "", "model", "=", "BertForPreTrainingLossMask", ".", "from_pretrained", "(", "\n", "args", ".", "bert_model", ",", "state_dict", "=", "model_recover", ",", "num_labels", "=", "cls_num_labels", ",", "num_rel", "=", "0", ",", "type_vocab_size", "=", "type_vocab_size", ",", "config_path", "=", "args", ".", "config_path", ",", "task_idx", "=", "3", ",", "num_sentlvl_labels", "=", "num_sentlvl_labels", ",", "max_position_embeddings", "=", "args", ".", "max_position_embeddings", ",", "label_smoothing", "=", "args", ".", "label_smoothing", ",", "fp32_embedding", "=", "args", ".", "fp32_embedding", ",", "relax_projection", "=", "relax_projection", ",", "new_pos_ids", "=", "args", ".", "new_pos_ids", ",", "ffn_type", "=", "args", ".", "ffn_type", ",", "hidden_dropout_prob", "=", "args", ".", "hidden_dropout_prob", ",", "attention_probs_dropout_prob", "=", "args", ".", "attention_probs_dropout_prob", ",", "num_qkv", "=", "args", ".", "num_qkv", ",", "seg_emb", "=", "args", ".", "seg_emb", ")", "\n", "", "if", "args", ".", "local_rank", "==", "0", ":", "\n", "        ", "dist", ".", "barrier", "(", ")", "\n", "\n", "", "if", "args", ".", "fp16", ":", "\n", "# model.half()", "\n", "        ", "if", "args", ".", "fp32_embedding", ":", "\n", "            ", "model", ".", "bert", ".", "embeddings", ".", "word_embeddings", ".", "float", "(", ")", "\n", "model", ".", "bert", ".", "embeddings", ".", "position_embeddings", ".", "float", "(", ")", "\n", "model", ".", "bert", ".", "embeddings", ".", "token_type_embeddings", ".", "float", "(", ")", "\n", "", "", "model", ".", "to", "(", "device", ")", "\n", "\n", "\n", "# Prepare optimizer", "\n", "param_optimizer", "=", "list", "(", "model", ".", "named_parameters", "(", ")", ")", "\n", "no_decay", "=", "[", "'bias'", ",", "'LayerNorm.bias'", ",", "'LayerNorm.weight'", "]", "\n", "optimizer_grouped_parameters", "=", "[", "\n", "{", "'params'", ":", "[", "p", "for", "n", ",", "p", "in", "param_optimizer", "if", "not", "any", "(", "\n", "nd", "in", "n", "for", "nd", "in", "no_decay", ")", "]", ",", "'weight_decay'", ":", "0.01", "}", ",", "\n", "{", "'params'", ":", "[", "p", "for", "n", ",", "p", "in", "param_optimizer", "if", "any", "(", "\n", "nd", "in", "n", "for", "nd", "in", "no_decay", ")", "]", ",", "'weight_decay'", ":", "0.0", "}", "\n", "]", "\n", "if", "args", ".", "fp16", ":", "\n", "        ", "try", ":", "\n", "# from apex.optimizers import FP16_Optimizer", "\n", "            ", "from", "pytorch_pretrained_bert", ".", "optimization_fp16", "import", "FP16_Optimizer_State", "\n", "from", "apex", ".", "optimizers", "import", "FusedAdam", "\n", "", "except", "ImportError", ":", "\n", "            ", "raise", "ImportError", "(", "\n", "\"Please install apex from https://www.github.com/nvidia/apex to use distributed and fp16 training.\"", ")", "\n", "\n", "", "optimizer", "=", "FusedAdam", "(", "optimizer_grouped_parameters", ",", "\n", "lr", "=", "args", ".", "learning_rate", ",", "\n", "bias_correction", "=", "False", ")", "\n", "# max_grad_norm=1.0)", "\n", "# if args.loss_scale == 0:", "\n", "#     optimizer = FP16_Optimizer_State(", "\n", "#         optimizer, dynamic_loss_scale=True)", "\n", "# else:", "\n", "#     optimizer = FP16_Optimizer_State(", "\n", "#         optimizer, static_loss_scale=args.loss_scale)", "\n", "\n", "\n", "\n", "model", ",", "optimizer", "=", "amp", ".", "initialize", "(", "model", ",", "optimizer", ",", "opt_level", "=", "\"O1\"", ",", "loss_scale", "=", "\"dynamic\"", ",", ")", "\n", "# amp_handle = amp.init(enable_caching=True)", "\n", "logger", ".", "info", "(", "\"enable fp16 with amp\"", ")", "\n", "", "else", ":", "\n", "        ", "optimizer", "=", "BertAdam", "(", "optimizer_grouped_parameters", ",", "\n", "lr", "=", "args", ".", "learning_rate", ",", "\n", "warmup", "=", "args", ".", "warmup_proportion", ",", "\n", "t_total", "=", "t_total", ")", "\n", "\n", "", "if", "args", ".", "local_rank", "!=", "-", "1", ":", "\n", "        ", "try", ":", "\n", "            ", "from", "torch", ".", "nn", ".", "parallel", "import", "DistributedDataParallel", "as", "DDP", "\n", "", "except", "ImportError", ":", "\n", "            ", "raise", "ImportError", "(", "\"DistributedDataParallel\"", ")", "\n", "", "model", "=", "DDP", "(", "model", ",", "device_ids", "=", "[", "\n", "args", ".", "local_rank", "]", ",", "output_device", "=", "args", ".", "local_rank", ",", "find_unused_parameters", "=", "True", ")", "\n", "", "elif", "n_gpu", ">", "1", ":", "\n", "# model = torch.nn.DataParallel(model)", "\n", "        ", "model", "=", "DataParallelImbalance", "(", "model", ")", "\n", "\n", "", "if", "recover_step", ":", "\n", "        ", "logger", ".", "info", "(", "\"***** Recover optimizer: %d *****\"", ",", "recover_step", ")", "\n", "optim_recover", "=", "torch", ".", "load", "(", "os", ".", "path", ".", "join", "(", "\n", "args", ".", "output_dir", ",", "\"optim.{0}.bin\"", ".", "format", "(", "recover_step", ")", ")", ",", "map_location", "=", "'cpu'", ")", "\n", "if", "hasattr", "(", "optim_recover", ",", "'state_dict'", ")", ":", "\n", "            ", "optim_recover", "=", "optim_recover", ".", "state_dict", "(", ")", "\n", "", "optimizer", ".", "load_state_dict", "(", "optim_recover", ")", "\n", "if", "args", ".", "loss_scale", "==", "0", ":", "\n", "            ", "logger", ".", "info", "(", "\"***** Recover optimizer: dynamic_loss_scale *****\"", ")", "\n", "optimizer", ".", "dynamic_loss_scale", "=", "True", "\n", "\n", "", "", "logger", ".", "info", "(", "\"***** CUDA.empty_cache() *****\"", ")", "\n", "torch", ".", "cuda", ".", "empty_cache", "(", ")", "\n", "\n", "if", "args", ".", "do_train", ":", "\n", "        ", "logger", ".", "info", "(", "\"***** Running training *****\"", ")", "\n", "logger", ".", "info", "(", "\"  Batch size = %d\"", ",", "args", ".", "train_batch_size", ")", "\n", "logger", ".", "info", "(", "\"  Num steps = %d\"", ",", "t_total", ")", "\n", "\n", "model", ".", "train", "(", ")", "\n", "if", "recover_step", ":", "\n", "            ", "start_epoch", "=", "recover_step", "+", "1", "\n", "", "else", ":", "\n", "            ", "start_epoch", "=", "1", "\n", "", "for", "i_epoch", "in", "trange", "(", "start_epoch", ",", "int", "(", "args", ".", "num_train_epochs", ")", "+", "1", ",", "desc", "=", "\"Epoch\"", ",", "disable", "=", "args", ".", "local_rank", "not", "in", "(", "-", "1", ",", "0", ")", ")", ":", "\n", "            ", "if", "args", ".", "local_rank", "!=", "-", "1", ":", "\n", "                ", "train_sampler", ".", "set_epoch", "(", "i_epoch", ")", "\n", "", "iter_bar", "=", "tqdm", "(", "train_dataloader", ",", "desc", "=", "'Iter (loss=X.XXX)'", ",", "\n", "disable", "=", "args", ".", "local_rank", "not", "in", "(", "-", "1", ",", "0", ")", ")", "\n", "for", "step", ",", "batch", "in", "enumerate", "(", "iter_bar", ")", ":", "\n", "                ", "batch", "=", "[", "\n", "t", ".", "to", "(", "device", ")", "if", "t", "is", "not", "None", "else", "None", "for", "t", "in", "batch", "]", "\n", "if", "args", ".", "has_sentence_oracle", ":", "\n", "                    ", "input_ids", ",", "segment_ids", ",", "input_mask", ",", "mask_qkv", ",", "lm_label_ids", ",", "masked_pos", ",", "masked_weights", ",", "is_next", ",", "task_idx", ",", "oracle_pos", ",", "oracle_weights", ",", "oracle_labels", "=", "batch", "\n", "", "else", ":", "\n", "                    ", "input_ids", ",", "segment_ids", ",", "input_mask", ",", "mask_qkv", ",", "lm_label_ids", ",", "masked_pos", ",", "masked_weights", ",", "is_next", ",", "task_idx", "=", "batch", "\n", "oracle_pos", ",", "oracle_weights", ",", "oracle_labels", "=", "None", ",", "None", ",", "None", "\n", "", "loss_tuple", "=", "model", "(", "input_ids", ",", "segment_ids", ",", "input_mask", ",", "lm_label_ids", ",", "is_next", ",", "masked_pos", "=", "masked_pos", ",", "masked_weights", "=", "masked_weights", ",", "task_idx", "=", "task_idx", ",", "masked_pos_2", "=", "oracle_pos", ",", "masked_weights_2", "=", "oracle_weights", ",", "\n", "masked_labels_2", "=", "oracle_labels", ",", "mask_qkv", "=", "mask_qkv", ")", "\n", "masked_lm_loss", ",", "next_sentence_loss", "=", "loss_tuple", "\n", "if", "n_gpu", ">", "1", ":", "# mean() to average on multi-gpu.", "\n", "# loss = loss.mean()", "\n", "                    ", "masked_lm_loss", "=", "masked_lm_loss", ".", "mean", "(", ")", "\n", "next_sentence_loss", "=", "next_sentence_loss", ".", "mean", "(", ")", "\n", "", "loss", "=", "masked_lm_loss", "+", "next_sentence_loss", "\n", "\n", "# logging for each step (i.e., before normalization by args.gradient_accumulation_steps)", "\n", "iter_bar", ".", "set_description", "(", "'Iter (loss=%5.3f)'", "%", "loss", ".", "item", "(", ")", ")", "\n", "\n", "# ensure that accumlated gradients are normalized", "\n", "if", "args", ".", "gradient_accumulation_steps", ">", "1", ":", "\n", "                    ", "loss", "=", "loss", "/", "args", ".", "gradient_accumulation_steps", "\n", "\n", "", "if", "args", ".", "fp16", ":", "\n", "                    ", "with", "amp", ".", "scale_loss", "(", "loss", ",", "optimizer", ")", "as", "scaled_loss", ":", "\n", "                        ", "scaled_loss", ".", "backward", "(", ")", "\n", "\n", "# optimizer.backward(loss)", "\n", "# if amp_handle:", "\n", "#     amp_handle._clear_cache()", "\n", "", "", "else", ":", "\n", "                    ", "loss", ".", "backward", "(", ")", "\n", "", "if", "(", "step", "+", "1", ")", "%", "args", ".", "gradient_accumulation_steps", "==", "0", ":", "\n", "                    ", "lr_this_step", "=", "args", ".", "learning_rate", "*", "warmup_linear", "(", "global_step", "/", "t_total", ",", "\n", "args", ".", "warmup_proportion", ")", "\n", "if", "args", ".", "fp16", ":", "\n", "# modify learning rate with special warm up BERT uses", "\n", "                        ", "for", "param_group", "in", "optimizer", ".", "param_groups", ":", "\n", "                            ", "param_group", "[", "'lr'", "]", "=", "lr_this_step", "\n", "", "", "optimizer", ".", "step", "(", ")", "\n", "optimizer", ".", "zero_grad", "(", ")", "\n", "global_step", "+=", "1", "\n", "\n", "# Save a trained model", "\n", "", "", "if", "(", "args", ".", "local_rank", "==", "-", "1", "or", "torch", ".", "distributed", ".", "get_rank", "(", ")", "==", "0", ")", ":", "\n", "                ", "logger", ".", "info", "(", "\n", "\"** ** * Saving fine-tuned model and optimizer ** ** * \"", ")", "\n", "model_to_save", "=", "model", ".", "module", "if", "hasattr", "(", "\n", "model", ",", "'module'", ")", "else", "model", "# Only save the model it-self", "\n", "output_model_file", "=", "os", ".", "path", ".", "join", "(", "\n", "args", ".", "output_dir", ",", "\"model.{0}.bin\"", ".", "format", "(", "i_epoch", ")", ")", "\n", "torch", ".", "save", "(", "model_to_save", ".", "state_dict", "(", ")", ",", "output_model_file", ")", "\n", "output_optim_file", "=", "os", ".", "path", ".", "join", "(", "\n", "args", ".", "output_dir", ",", "\"optim.{0}.bin\"", ".", "format", "(", "i_epoch", ")", ")", "\n", "torch", ".", "save", "(", "optimizer", ".", "state_dict", "(", ")", ",", "output_optim_file", ")", "\n", "\n", "# logger.info(\"***** CUDA.empty_cache() *****\")", "\n", "# torch.cuda.empty_cache()", "\n", "\n", "\n"]], "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.unilm.eval.Eval.__init__": [[10, 19], ["nlgeval.NLGEval", "sumeval.metrics.bleu.BLEUCalculator", "sumeval.metrics.rouge.RougeCalculator", "warnings.warn"], "methods", ["None"], ["    ", "with", "open", "(", "path", ",", "\"r\"", ",", "encoding", "=", "\"utf-8\"", ")", "as", "f", ":", "\n", "        ", "content", "=", "f", ".", "readlines", "(", ")", "\n", "", "return", "content", "\n", "\n", "", "def", "remove_blank", "(", "lines", ")", ":", "\n", "    ", "return", "[", "l", ".", "replace", "(", "\" \"", ",", "\"\"", ")", "for", "l", "in", "lines", "]", "\n", "\n", "", "predicted_lines", "=", "remove_blank", "(", "load_file", "(", "predicted_file", ")", ")", "\n", "gt_lines", "=", "remove_blank", "(", "load_file", "(", "gt_file", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.unilm.eval.Eval.calculate": [[20, 80], ["zip", "eval.Eval.nlgeval.compute_metrics", "calculate_aspect", "eval.Eval.bleu_scorer.bleu", "sacrebleu_list.append", "rouge_1_list.append", "rouge_2_list.append", "rouge_L_list.append", "numpy.mean", "numpy.mean", "numpy.mean", "numpy.mean", "eval.Eval.rouge_scorer.rouge_1", "eval.Eval.rouge_scorer.rouge_2", "eval.Eval.rouge_scorer.rouge_l", "pred.lower().replace", "target.lower().replace", "pred.lower", "target.lower"], "methods", ["None"], ["assert", "len", "(", "predicted_lines", ")", "==", "len", "(", "gt_lines", ")", "\n", "\n", "\n", "results", "=", "tool", ".", "calculate", "(", "predicted_lines", ",", "gt_lines", ")", "\n", "print", "(", "results", ")", "# ", "\n", ""]], "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.unilm.eval.load_file": [[87, 91], ["open", "f.readlines"], "function", ["None"], []], "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.unilm.eval.remove_blank": [[92, 94], ["l.replace"], "function", ["None"], []], "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.unilm.data.process_line": [[5, 8], ["range", "line[].replace"], "function", ["None"], ["def", "process_line", "(", "line", ")", ":", "\n", "    ", "for", "i", "in", "range", "(", "3", ")", ":", "\n", "        ", "line", "[", "i", "]", "=", "line", "[", "i", "]", ".", "replace", "(", "\",\"", ",", "\" \"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.unilm.data.process_file": [[10, 19], ["open", "csv.reader", "list", "open", "f.write", "data.process_line", "f.write"], "function", ["home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.unilm.data.process_line"], ["", "", "def", "process_file", "(", "src_path", ",", "save_path", ")", ":", "\n", "    ", "with", "open", "(", "src_path", ",", "\"r\"", ")", "as", "f", ":", "\n", "        ", "reader", "=", "csv", ".", "reader", "(", "f", ")", "\n", "data", "=", "list", "(", "reader", ")", "\n", "", "with", "open", "(", "save_path", ",", "\"w\"", ")", "as", "f", ":", "\n", "        ", "f", ".", "write", "(", "\"\\t\"", ".", "join", "(", "data", "[", "0", "]", ")", "+", "\"\\n\"", ")", "\n", "for", "line", "in", "data", "[", "1", ":", "]", ":", "\n", "            ", "process_line", "(", "line", ")", "\n", "f", ".", "write", "(", "\"\\t\"", ".", "join", "(", "line", ")", "+", "\"\\n\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.unilm.data.process_file2": [[22, 35], ["open", "csv.reader", "list", "open", "f.write", "data.process_file2.process"], "function", ["None"], ["", "", "", "def", "process_file2", "(", "src_path", ",", "save_path", ")", ":", "\n", "    ", "def", "process", "(", "line", ")", ":", "\n", "        ", "line", "[", "0", "]", "=", "\",\"", ".", "join", "(", "line", "[", "0", "]", ".", "split", "(", "\",\"", ")", "[", ":", "3", "]", ")", "\n", "line", "[", "1", "]", "=", "line", "[", "1", "]", ".", "split", "(", "\"|||\"", ")", "[", "0", "]", "+", "\"|||\"", "+", "line", "[", "0", "]", "\n", "line", "[", "2", "]", "=", "line", "[", "2", "]", ".", "split", "(", "\"|||\"", ")", "[", "0", "]", "+", "\"|||\"", "+", "line", "[", "0", "]", "\n", "", "with", "open", "(", "src_path", ",", "\"r\"", ")", "as", "f", ":", "\n", "        ", "reader", "=", "csv", ".", "reader", "(", "f", ")", "\n", "data", "=", "list", "(", "reader", ")", "\n", "", "with", "open", "(", "save_path", ",", "\"w\"", ")", "as", "f", ":", "\n", "        ", "f", ".", "write", "(", "\"\\t\"", ".", "join", "(", "data", "[", "0", "]", ")", "+", "\"\\n\"", ")", "\n", "for", "line", "in", "data", "[", "1", ":", "]", ":", "\n", "            ", "process", "(", "line", ")", "\n", "f", ".", "write", "(", "\"\\t\"", ".", "join", "(", "line", ")", "+", "\"\\n\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.unilm.data.process_file3": [[38, 48], ["open", "open", "f.write", "data.append", "line.split", "f.write", "line.strip"], "function", ["home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.preprocess.process_daren.split"], ["", "", "", "def", "process_file3", "(", "src_path", ",", "save_path", ")", ":", "\n", "    ", "with", "open", "(", "src_path", ",", "\"r\"", ")", "as", "f", ":", "\n", "        ", "data", "=", "[", "]", "\n", "for", "line", "in", "f", ":", "\n", "            ", "data", ".", "append", "(", "line", ".", "strip", "(", ")", ")", "\n", "", "", "with", "open", "(", "save_path", ",", "\"w\"", ")", "as", "f", ":", "\n", "        ", "f", ".", "write", "(", "\"attr\\taspect\\tcode\\twrite\\tsku\\n\"", ")", "\n", "for", "line", "in", "data", ":", "\n", "            ", "attr", ",", "aspect", ",", "code", ",", "write", ",", "sku", "=", "line", ".", "split", "(", "\"\\t\"", ")", "\n", "f", ".", "write", "(", "attr", "+", "\"\\t\"", "+", "aspect", "+", "\"|||\"", "+", "attr", "+", "\"\\t\"", "+", "code", "+", "\"|||\"", "+", "attr", "+", "\"\\t\"", "+", "write", "+", "\"\\t\"", "+", "sku", "+", "\"\\n\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.unilm.data.remove_": [[52, 62], ["s.split.split", "i.split.split"], "function", ["home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.preprocess.process_daren.split", "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.preprocess.process_daren.split"], ["", "", "", "def", "remove_", "(", "s", ",", "a", ",", "b", ")", ":", "\n", "    ", "res", "=", "\"\"", "\n", "if", "a", "in", "s", ":", "\n", "        ", "s", "=", "s", ".", "split", "(", "a", ")", "\n", "for", "i", "in", "s", ":", "\n", "            ", "i", "=", "i", ".", "split", "(", "b", ")", "\n", "res", "+=", "i", "[", "-", "1", "]", "\n", "", "", "else", ":", "\n", "        ", "res", "=", "s", "\n", "", "return", "res", "\n", "\n"]], "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.unilm.data.remove_s": [[64, 69], ["s[].strip.split", "s[].strip"], "function", ["home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.preprocess.process_daren.split"], ["", "def", "remove_s", "(", "s", ",", "a", ")", ":", "\n", "    ", "if", "a", "in", "s", ":", "\n", "        ", "s", "=", "s", ".", "split", "(", "a", ")", "\n", "s", "=", "s", "[", "1", "]", ".", "strip", "(", "\" \"", ")", "\n", "", "return", "s", "\n", "\n"]], "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.unilm.data._count": [[71, 82], ["print", "open", "f.readlines", "line.split.strip", "line.split.split", "len"], "function", ["home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.preprocess.process_daren.split"], ["", "def", "_count", "(", "path", ")", ":", "\n", "    ", "with", "open", "(", "path", ",", "\"r\"", ")", "as", "f", ":", "\n", "        ", "data", "=", "f", ".", "readlines", "(", ")", "\n", "", "max_l", "=", "0", "\n", "for", "line", "in", "data", ":", "\n", "        ", "line", "=", "line", ".", "strip", "(", ")", "\n", "line", "=", "line", ".", "split", "(", "\" \"", ")", "\n", "a", "=", "len", "(", "line", ")", "\n", "if", "a", ">", "max_l", ":", "\n", "            ", "max_l", "=", "a", "\n", "", "", "print", "(", "max_l", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.unilm.data.count": [[84, 87], ["data._count"], "function", ["home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.unilm.data._count"], ["", "def", "count", "(", ")", ":", "\n", "    ", "path", "=", "\"data/feat/v3/processed/eval.tgt\"", "\n", "_count", "(", "path", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.unilm.data._cut": [[89, 106], ["open", "len", "len", "open", "len", "range", "open", "fs.readlines", "ft.readlines", "src_data[].strip", "tgt_data[].strip", "len", "src_data[].strip.split", "fs.write", "tgt_data[].strip.split"], "function", ["home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.preprocess.process_daren.split", "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.preprocess.process_daren.split"], ["", "def", "_cut", "(", "src_path", ",", "tgt_path", ",", "max_l", "=", "512", "-", "3", ")", ":", "\n", "    ", "with", "open", "(", "src_path", ",", "\"r\"", ")", "as", "fs", ":", "\n", "        ", "with", "open", "(", "tgt_path", ",", "\"r\"", ")", "as", "ft", ":", "\n", "            ", "src_data", "=", "fs", ".", "readlines", "(", ")", "\n", "tgt_data", "=", "ft", ".", "readlines", "(", ")", "\n", "\n", "", "", "assert", "len", "(", "src_data", ")", "==", "len", "(", "tgt_data", ")", "\n", "with", "open", "(", "src_path", ",", "\"w\"", ")", "as", "fs", ":", "\n", "        ", "l", "=", "len", "(", "src_data", ")", "\n", "for", "i", "in", "range", "(", "l", ")", ":", "\n", "            ", "src_line", "=", "src_data", "[", "i", "]", ".", "strip", "(", ")", "\n", "tgt_line", "=", "tgt_data", "[", "i", "]", ".", "strip", "(", ")", "\n", "l_tgt", "=", "len", "(", "tgt_line", ".", "split", "(", "\" \"", ")", ")", "\n", "src_list", "=", "src_line", ".", "split", "(", "\" \"", ")", "\n", "src_list", "=", "src_list", "[", ":", "(", "max_l", "-", "l_tgt", ")", "]", "\n", "src_line", "=", "\" \"", ".", "join", "(", "src_list", ")", "\n", "fs", ".", "write", "(", "src_line", "+", "\"\\n\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.unilm.data.cut": [[108, 112], ["data._cut"], "function", ["home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.unilm.data._cut"], ["", "", "", "def", "cut", "(", ")", ":", "\n", "    ", "src_path", "=", "\"data/feat/v1/processed/train.src\"", "\n", "tgt_path", "=", "\"data/feat/v1/processed/train.tgt\"", "\n", "_cut", "(", "src_path", ",", "tgt_path", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.unilm.data.main": [[114, 118], ["data.process_file3"], "function", ["home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.unilm.data.process_file3"], ["", "def", "main", "(", ")", ":", "\n", "    ", "src_path", "=", "\"data/raw/1343/test.tsv\"", "\n", "save_path", "=", "\"data/processed/1343/v1/test.tsv\"", "\n", "process_file3", "(", "src_path", ",", "save_path", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.unilm.data.token": [[120, 127], ["BertTokenizer.from_pretrained", "BertTokenizer.from_pretrained.tokenize", "print"], "function", ["home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.pytorch_pretrained_bert.tokenization.BertTokenizer.from_pretrained", "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.preprocess.tokenizing_1342.tokenize"], ["", "def", "token", "(", ")", ":", "\n", "    ", "from", "transformers", "import", "BertTokenizer", "\n", "tokenizer", "=", "BertTokenizer", ".", "from_pretrained", "(", "\"config/vocab.txt\"", ")", "\n", "a", "=", "\"\u6613\u767e\u5e74EZ908\u8001\u5e74\u624b\u673a \u56fd\u4ea7770\u82af\u7247 \u524d\u540e\u53cc\u6444 512MB+32GB \u73ab\u7470\u91d1 \u79fb\u52a8\u8054\u901a\u7535\u4fe14G\u624b\u673a \u53cc\u5361\u53cc\u5f85\u5168\u7f51\u901a\uff08512MB 32GB\uff09\u73ab\u7470\u91d1\u652f\u6301\u5b58\u50a8\u5361 3.5\u82f1\u5bf8\u5c4f\u5e55 \u5f55\u97f3\u673a \u4e0d\u652f\u6301\u65e0\u7ebf\u5145\u7535 T9\u4f20\u7edf\u952e\u76d8 \u8d85\u5927\u97f3\u91cf \u4e00\u952e\u62a5\u8b66 \u975e\u89e6\u63a7 \u4e0d\u652f\u63015G 3.5\u82f1\u5bf8 \u5851\u6599\u540e\u76d6 \u524d\u7f6e300\u4e07\u50cf\u7d20 \u540e\u7f6e200\u50cf\u7d20 2000mAh\u5927\u7535\u6c60 120g\u8d85\u8f7b\u673a\u8eab \u53ef\u62c6\u5378\u7535\u6c60 \u540e\u7f6e\u5355\u6444 \u56fd\u4ea7770\"", "\n", "res", "=", "tokenizer", ".", "tokenize", "(", "a", ")", "\n", "a", "=", "\" \"", ".", "join", "(", "res", ")", "\n", "print", "(", "a", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.unilm.data.get_part": [[129, 146], ["open", "csv.reader", "list", "open", "line[].split", "f.write", "len"], "function", ["home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.preprocess.process_daren.split"], ["", "def", "get_part", "(", ")", ":", "\n", "    ", "for", "i", "in", "[", "\"train\"", ",", "\"validation\"", ",", "\"test\"", "]", ":", "\n", "        ", "src_path", "=", "\"data/raw/\"", "+", "i", "+", "\".csv\"", "\n", "save_path", "=", "\"data/processed/v2/\"", "+", "i", "+", "\".txt\"", "\n", "with", "open", "(", "src_path", ",", "\"r\"", ")", "as", "f", ":", "\n", "            ", "reader", "=", "csv", ".", "reader", "(", "f", ")", "\n", "data", "=", "list", "(", "reader", ")", "\n", "", "with", "open", "(", "save_path", ",", "\"w\"", ")", "as", "f", ":", "\n", "            ", "for", "line", "in", "data", "[", "1", ":", "]", ":", "\n", "                ", "a", "=", "line", "[", "0", "]", ".", "split", "(", "\",\"", ")", "\n", "res", "=", "a", "[", "0", "]", "\n", "for", "i", "in", "a", "[", "1", ":", "]", ":", "\n", "                    ", "if", "len", "(", "res", ")", "<", "25", ":", "\n", "                        ", "res", "+=", "\",\"", "+", "i", "\n", "", "else", ":", "\n", "                        ", "break", "\n", "", "", "f", ".", "write", "(", "res", "+", "\"\\n\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.unilm.data.test": [[148, 156], ["remove_.replace", "remove_.replace", "data.remove_", "data.remove_", "data.remove_", "print"], "function", ["home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.unilm.data.remove_", "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.unilm.data.remove_", "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.unilm.data.remove_"], ["", "", "", "", "def", "test", "(", ")", ":", "\n", "    ", "a", "=", "\"\u9b45\u65cf\uff08MEIZU\uff09,PRO7\u6807\u51c6\u7248,\u5168\u7f51\u901a4G\u624b\u673a,\u53cc\u5361\u53cc\u5f85,\u63d0\u9999\u7ea2,(4G,RAM+128G,ROM\uff09(4G,RAM+128G,ROM\uff09\u63d0\u9999\u7ea2\u53cc\u5361\u53cc\u5f85\u5355\u901a,\u91d1\u5c5e\u540e\u76d6,5.2\u82f1\u5bf8,WiFi\u70ed\u70b9,MTKP\u7cfb\u5217,\u91d1\u5c5e\u8fb9\u6846,\u5f55\u97f3,\u84dd\u7259\"", "\n", "a", "=", "a", ".", "replace", "(", "\"\uff08\"", ",", "\"(\"", ")", "\n", "a", "=", "a", ".", "replace", "(", "\"\uff09\"", ",", "\")\"", ")", "\n", "a", "=", "remove_", "(", "a", ",", "\"\u3010\"", ",", "\"\u3011\"", ")", "\n", "a", "=", "remove_", "(", "a", ",", "\"\uff08\"", ",", "\"\uff09\"", ")", "\n", "a", "=", "remove_", "(", "a", ",", "\"(\"", ",", "\")\"", ")", "\n", "print", "(", "a", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.unilm.data.all_aspect": [[158, 175], ["set", "open", "open", "line.strip.strip", "line.strip.split", "set.add", "len", "f.write"], "function", ["home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.preprocess.process_daren.split", "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.biunilm.loader_utils.TrieTree.add"], ["", "def", "all_aspect", "(", ")", ":", "\n", "# required by xiaojie", "\n", "    ", "path", "=", "\"data/feat/v3/processed/test.src\"", "\n", "data", "=", "set", "(", ")", "\n", "with", "open", "(", "path", ",", "\"r\"", ")", "as", "f", ":", "\n", "        ", "for", "line", "in", "f", ":", "\n", "            ", "line", "=", "line", ".", "strip", "(", ")", "\n", "attr", "=", "line", ".", "split", "(", "\" [SEP] \"", ")", "\n", "assert", "len", "(", "attr", ")", "==", "2", "\n", "attr", "=", "attr", "[", "1", "]", "\n", "data", ".", "add", "(", "attr", ")", "\n", "", "", "save", "=", "\"data/feat/v3/processed/test2.src\"", "\n", "with", "open", "(", "save", ",", "\"w\"", ")", "as", "f", ":", "\n", "        ", "for", "line", "in", "data", ":", "\n", "            ", "for", "key", "in", "[", "\"\u673a \u8eab \u5916 \u89c2\"", ",", "\"\u5c4f \u5e55 \u97f3 \u6548\"", ",", "\"\u7f51 \u7edc 5 g\"", ",", "\"\u62cd \u7167 \u6444 \u5f71\"", ",", "\"\u6027 \u80fd \u5b58 \u50a8\"", ",", "\"\u7535 \u6c60 \u5145 \u7535\"", ",", "\"\u89e3 \u9501 \u8bc6 \u522b\"", "]", ":", "\n", "                ", "new_line", "=", "key", "+", "\" [SEP] \"", "+", "line", "+", "\"\\n\"", "\n", "f", ".", "write", "(", "new_line", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.unilm.data.merge_xiaojie": [[177, 202], ["open", "open", "open", "csv.writer", "csv.writer.writerow", "zip", "line.replace.strip", "data.append", "line.replace.strip", "line.replace.replace", "line.replace.replace", "line.replace.replace", "data_.append", "csv.writer.writerow"], "function", ["None"], ["", "", "", "", "def", "merge_xiaojie", "(", ")", ":", "\n", "    ", "a", "=", "\"save/bert_save_v3/model.15.txt2\"", "\n", "with", "open", "(", "a", ",", "\"r\"", ")", "as", "f", ":", "\n", "        ", "data", "=", "[", "]", "\n", "for", "line", "in", "f", ":", "\n", "            ", "line", "=", "line", ".", "strip", "(", ")", "\n", "data", ".", "append", "(", "line", ")", "\n", "\n", "", "", "b", "=", "\"data/feat/v3/processed/test2.src\"", "\n", "with", "open", "(", "b", ",", "\"r\"", ")", "as", "f", ":", "\n", "        ", "data_", "=", "[", "]", "\n", "for", "line", "in", "f", ":", "\n", "            ", "line", "=", "line", ".", "strip", "(", ")", "\n", "line", "=", "line", ".", "replace", "(", "\" ##\"", ",", "\"\"", ")", "\n", "line", "=", "line", ".", "replace", "(", "\" \"", ",", "\"\"", ")", "\n", "line", "=", "line", ".", "replace", "(", "\"[SEP]\"", ",", "\"|||\"", ")", "\n", "data_", ".", "append", "(", "line", ")", "\n", "\n", "", "", "c", "=", "\"data/feat/v3/processed/output2.csv\"", "\n", "with", "open", "(", "c", ",", "\"w\"", ")", "as", "f", ":", "\n", "        ", "cf", "=", "csv", ".", "writer", "(", "f", ")", "\n", "cf", ".", "writerow", "(", "[", "\"pred\"", ",", "\"product\"", "]", ")", "\n", "for", "a", ",", "b", "in", "zip", "(", "data", ",", "data_", ")", ":", "\n", "            ", "line", "=", "[", "a", ",", "b", "]", "\n", "cf", ".", "writerow", "(", "line", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.unilm.decode_seq2seq.detokenize": [[34, 42], ["tk.startswith", "r_list.append", "len"], "function", ["None"], ["def", "detokenize", "(", "tk_list", ")", ":", "\n", "    ", "r_list", "=", "[", "]", "\n", "for", "tk", "in", "tk_list", ":", "\n", "        ", "if", "tk", ".", "startswith", "(", "'##'", ")", "and", "len", "(", "r_list", ")", ">", "0", ":", "\n", "            ", "r_list", "[", "-", "1", "]", "=", "r_list", "[", "-", "1", "]", "+", "tk", "[", "2", ":", "]", "\n", "", "else", ":", "\n", "            ", "r_list", ".", "append", "(", "tk", ")", "\n", "", "", "return", "r_list", "\n", "\n"]], "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.unilm.decode_seq2seq.ascii_print": [[44, 47], ["text.encode.encode", "print"], "function", ["None"], ["", "def", "ascii_print", "(", "text", ")", ":", "\n", "    ", "text", "=", "text", ".", "encode", "(", "\"ascii\"", ",", "\"ignore\"", ")", "\n", "print", "(", "text", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.unilm.decode_seq2seq.main": [[49, 264], ["argparse.ArgumentParser", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.parse_args", "torch.device", "torch.cuda.device_count", "random.seed", "numpy.random.seed", "torch.manual_seed", "pytorch_pretrained_bert.tokenization.BertTokenizer.from_pretrained", "bi_uni_pipeline.append", "BertTokenizer.from_pretrained.convert_tokens_to_ids", "decode_seq2seq.main._get_token_id_set"], "function", ["home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.pytorch_pretrained_bert.tokenization.BertTokenizer.from_pretrained", "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.pytorch_pretrained_bert.tokenization.BertTokenizer.convert_tokens_to_ids"], ["", "def", "main", "(", ")", ":", "\n", "    ", "parser", "=", "argparse", ".", "ArgumentParser", "(", ")", "\n", "\n", "# Required parameters", "\n", "parser", ".", "add_argument", "(", "\"--bert_model\"", ",", "default", "=", "None", ",", "type", "=", "str", ",", "required", "=", "True", ",", "\n", "help", "=", "\"Bert pre-trained model selected in the list: bert-base-uncased, \"", "\n", "\"bert-large-uncased, bert-base-cased, bert-base-multilingual, bert-base-chinese.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--model_recover_path\"", ",", "default", "=", "None", ",", "type", "=", "str", ",", "\n", "help", "=", "\"The file of fine-tuned pretraining model.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--max_seq_length\"", ",", "default", "=", "512", ",", "type", "=", "int", ",", "\n", "help", "=", "\"The maximum total input sequence length after WordPiece tokenization. \\n\"", "\n", "\"Sequences longer than this will be truncated, and sequences shorter \\n\"", "\n", "\"than this will be padded.\"", ")", "\n", "parser", ".", "add_argument", "(", "'--ffn_type'", ",", "default", "=", "0", ",", "type", "=", "int", ",", "\n", "help", "=", "\"0: default mlp; 1: W((Wx+b) elem_prod x);\"", ")", "\n", "parser", ".", "add_argument", "(", "'--num_qkv'", ",", "default", "=", "0", ",", "type", "=", "int", ",", "\n", "help", "=", "\"Number of different <Q,K,V>.\"", ")", "\n", "parser", ".", "add_argument", "(", "'--seg_emb'", ",", "action", "=", "'store_true'", ",", "\n", "help", "=", "\"Using segment embedding for self-attention.\"", ")", "\n", "\n", "# decoding parameters", "\n", "parser", ".", "add_argument", "(", "'--fp16'", ",", "action", "=", "'store_true'", ",", "\n", "help", "=", "\"Whether to use 16-bit float precision instead of 32-bit\"", ")", "\n", "parser", ".", "add_argument", "(", "'--amp'", ",", "action", "=", "'store_true'", ",", "\n", "help", "=", "\"Whether to use amp for fp16\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--input_file\"", ",", "type", "=", "str", ",", "help", "=", "\"Input file\"", ")", "\n", "parser", ".", "add_argument", "(", "'--subset'", ",", "type", "=", "int", ",", "default", "=", "0", ",", "\n", "help", "=", "\"Decode a subset of the input dataset.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--output_file\"", ",", "type", "=", "str", ",", "help", "=", "\"output file\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--split\"", ",", "type", "=", "str", ",", "default", "=", "\"\"", ",", "\n", "help", "=", "\"Data split (train/val/test).\"", ")", "\n", "parser", ".", "add_argument", "(", "'--tokenized_input'", ",", "action", "=", "'store_true'", ",", "\n", "help", "=", "\"Whether the input is tokenized.\"", ")", "\n", "parser", ".", "add_argument", "(", "'--seed'", ",", "type", "=", "int", ",", "default", "=", "123", ",", "\n", "help", "=", "\"random seed for initialization\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--do_lower_case\"", ",", "action", "=", "'store_true'", ",", "\n", "help", "=", "\"Set this flag if you are using an uncased model.\"", ")", "\n", "parser", ".", "add_argument", "(", "'--new_segment_ids'", ",", "action", "=", "'store_true'", ",", "\n", "help", "=", "\"Use new segment ids for bi-uni-directional LM.\"", ")", "\n", "parser", ".", "add_argument", "(", "'--new_pos_ids'", ",", "action", "=", "'store_true'", ",", "\n", "help", "=", "\"Use new position ids for LMs.\"", ")", "\n", "parser", ".", "add_argument", "(", "'--batch_size'", ",", "type", "=", "int", ",", "default", "=", "4", ",", "\n", "help", "=", "\"Batch size for decoding.\"", ")", "\n", "parser", ".", "add_argument", "(", "'--beam_size'", ",", "type", "=", "int", ",", "default", "=", "1", ",", "\n", "help", "=", "\"Beam size for searching\"", ")", "\n", "parser", ".", "add_argument", "(", "'--length_penalty'", ",", "type", "=", "float", ",", "default", "=", "0", ",", "\n", "help", "=", "\"Length penalty for beam search\"", ")", "\n", "\n", "parser", ".", "add_argument", "(", "'--forbid_duplicate_ngrams'", ",", "action", "=", "'store_true'", ")", "\n", "parser", ".", "add_argument", "(", "'--forbid_ignore_word'", ",", "type", "=", "str", ",", "default", "=", "None", ",", "\n", "help", "=", "\"Ignore the word during forbid_duplicate_ngrams\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--min_len\"", ",", "default", "=", "None", ",", "type", "=", "int", ")", "\n", "parser", ".", "add_argument", "(", "'--need_score_traces'", ",", "action", "=", "'store_true'", ")", "\n", "parser", ".", "add_argument", "(", "'--ngram_size'", ",", "type", "=", "int", ",", "default", "=", "3", ")", "\n", "parser", ".", "add_argument", "(", "'--mode'", ",", "default", "=", "\"s2s\"", ",", "\n", "choices", "=", "[", "\"s2s\"", ",", "\"l2r\"", ",", "\"both\"", "]", ")", "\n", "parser", ".", "add_argument", "(", "'--max_tgt_length'", ",", "type", "=", "int", ",", "default", "=", "128", ",", "\n", "help", "=", "\"maximum length of target sequence\"", ")", "\n", "parser", ".", "add_argument", "(", "'--s2s_special_token'", ",", "action", "=", "'store_true'", ",", "\n", "help", "=", "\"New special tokens ([S2S_SEP]/[S2S_CLS]) of S2S.\"", ")", "\n", "parser", ".", "add_argument", "(", "'--s2s_add_segment'", ",", "action", "=", "'store_true'", ",", "\n", "help", "=", "\"Additional segmental for the encoder of S2S.\"", ")", "\n", "parser", ".", "add_argument", "(", "'--s2s_share_segment'", ",", "action", "=", "'store_true'", ",", "\n", "help", "=", "\"Sharing segment embeddings for the encoder of S2S (used with --s2s_add_segment).\"", ")", "\n", "parser", ".", "add_argument", "(", "'--pos_shift'", ",", "action", "=", "'store_true'", ",", "\n", "help", "=", "\"Using position shift for fine-tuning.\"", ")", "\n", "parser", ".", "add_argument", "(", "'--not_predict_token'", ",", "type", "=", "str", ",", "default", "=", "None", ",", "\n", "help", "=", "\"Do not predict the tokens during decoding.\"", ")", "\n", "\n", "args", "=", "parser", ".", "parse_args", "(", ")", "\n", "\n", "if", "args", ".", "need_score_traces", "and", "args", ".", "beam_size", "<=", "1", ":", "\n", "        ", "raise", "ValueError", "(", "\n", "\"Score trace is only available for beam search with beam size > 1.\"", ")", "\n", "", "if", "args", ".", "max_tgt_length", ">=", "args", ".", "max_seq_length", "-", "2", ":", "\n", "        ", "raise", "ValueError", "(", "\"Maximum tgt length exceeds max seq length - 2.\"", ")", "\n", "\n", "", "device", "=", "torch", ".", "device", "(", "\n", "\"cuda\"", "if", "torch", ".", "cuda", ".", "is_available", "(", ")", "else", "\"cpu\"", ")", "\n", "n_gpu", "=", "torch", ".", "cuda", ".", "device_count", "(", ")", "\n", "\n", "random", ".", "seed", "(", "args", ".", "seed", ")", "\n", "np", ".", "random", ".", "seed", "(", "args", ".", "seed", ")", "\n", "torch", ".", "manual_seed", "(", "args", ".", "seed", ")", "\n", "if", "n_gpu", ">", "0", ":", "\n", "        ", "torch", ".", "cuda", ".", "manual_seed_all", "(", "args", ".", "seed", ")", "\n", "\n", "", "tokenizer", "=", "BertTokenizer", ".", "from_pretrained", "(", "\n", "args", ".", "bert_model", ",", "do_lower_case", "=", "args", ".", "do_lower_case", ")", "\n", "\n", "tokenizer", ".", "max_len", "=", "args", ".", "max_seq_length", "\n", "\n", "pair_num_relation", "=", "0", "\n", "bi_uni_pipeline", "=", "[", "]", "\n", "bi_uni_pipeline", ".", "append", "(", "seq2seq_loader", ".", "Preprocess4Seq2seqDecoder", "(", "list", "(", "tokenizer", ".", "vocab", ".", "keys", "(", ")", ")", ",", "tokenizer", ".", "convert_tokens_to_ids", ",", "args", ".", "max_seq_length", ",", "max_tgt_length", "=", "args", ".", "max_tgt_length", ",", "new_segment_ids", "=", "args", ".", "new_segment_ids", ",", "\n", "mode", "=", "\"s2s\"", ",", "num_qkv", "=", "args", ".", "num_qkv", ",", "s2s_special_token", "=", "args", ".", "s2s_special_token", ",", "s2s_add_segment", "=", "args", ".", "s2s_add_segment", ",", "s2s_share_segment", "=", "args", ".", "s2s_share_segment", ",", "pos_shift", "=", "args", ".", "pos_shift", ")", ")", "\n", "\n", "amp_handle", "=", "None", "\n", "if", "args", ".", "fp16", "and", "args", ".", "amp", ":", "\n", "        ", "from", "apex", "import", "amp", "\n", "amp_handle", "=", "amp", ".", "init", "(", "enable_caching", "=", "True", ")", "\n", "logger", ".", "info", "(", "\"enable fp16 with amp\"", ")", "\n", "\n", "# Prepare model", "\n", "", "cls_num_labels", "=", "2", "\n", "type_vocab_size", "=", "6", "+", "(", "1", "if", "args", ".", "s2s_add_segment", "else", "0", ")", "if", "args", ".", "new_segment_ids", "else", "2", "\n", "mask_word_id", ",", "eos_word_ids", ",", "sos_word_id", "=", "tokenizer", ".", "convert_tokens_to_ids", "(", "\n", "[", "\"[MASK]\"", ",", "\"[SEP]\"", ",", "\"[S2S_SOS]\"", "]", ")", "\n", "\n", "def", "_get_token_id_set", "(", "s", ")", ":", "\n", "        ", "r", "=", "None", "\n", "if", "s", ":", "\n", "            ", "w_list", "=", "[", "]", "\n", "for", "w", "in", "s", ".", "split", "(", "'|'", ")", ":", "\n", "                ", "if", "w", ".", "startswith", "(", "'['", ")", "and", "w", ".", "endswith", "(", "']'", ")", ":", "\n", "                    ", "w_list", ".", "append", "(", "w", ".", "upper", "(", ")", ")", "\n", "", "else", ":", "\n", "                    ", "w_list", ".", "append", "(", "w", ")", "\n", "", "", "r", "=", "set", "(", "tokenizer", ".", "convert_tokens_to_ids", "(", "w_list", ")", ")", "\n", "", "return", "r", "\n", "\n", "", "forbid_ignore_set", "=", "_get_token_id_set", "(", "args", ".", "forbid_ignore_word", ")", "\n", "not_predict_set", "=", "_get_token_id_set", "(", "args", ".", "not_predict_token", ")", "\n", "print", "(", "args", ".", "model_recover_path", ")", "\n", "for", "model_recover_path", "in", "glob", ".", "glob", "(", "args", ".", "model_recover_path", ".", "strip", "(", ")", ")", ":", "\n", "        ", "logger", ".", "info", "(", "\"***** Recover model: %s *****\"", ",", "model_recover_path", ")", "\n", "model_recover", "=", "torch", ".", "load", "(", "model_recover_path", ")", "\n", "model", "=", "BertForSeq2SeqDecoder", ".", "from_pretrained", "(", "args", ".", "bert_model", ",", "state_dict", "=", "model_recover", ",", "num_labels", "=", "cls_num_labels", ",", "num_rel", "=", "pair_num_relation", ",", "type_vocab_size", "=", "type_vocab_size", ",", "task_idx", "=", "3", ",", "mask_word_id", "=", "mask_word_id", ",", "search_beam_size", "=", "args", ".", "beam_size", ",", "\n", "length_penalty", "=", "args", ".", "length_penalty", ",", "eos_id", "=", "eos_word_ids", ",", "sos_id", "=", "sos_word_id", ",", "forbid_duplicate_ngrams", "=", "args", ".", "forbid_duplicate_ngrams", ",", "forbid_ignore_set", "=", "forbid_ignore_set", ",", "not_predict_set", "=", "not_predict_set", ",", "ngram_size", "=", "args", ".", "ngram_size", ",", "min_len", "=", "args", ".", "min_len", ",", "mode", "=", "args", ".", "mode", ",", "max_position_embeddings", "=", "args", ".", "max_seq_length", ",", "ffn_type", "=", "args", ".", "ffn_type", ",", "num_qkv", "=", "args", ".", "num_qkv", ",", "seg_emb", "=", "args", ".", "seg_emb", ",", "pos_shift", "=", "args", ".", "pos_shift", ")", "\n", "del", "model_recover", "\n", "\n", "if", "args", ".", "fp16", ":", "\n", "            ", "model", ".", "half", "(", ")", "\n", "", "model", ".", "to", "(", "device", ")", "\n", "if", "n_gpu", ">", "1", ":", "\n", "            ", "model", "=", "torch", ".", "nn", ".", "DataParallel", "(", "model", ")", "\n", "\n", "", "torch", ".", "cuda", ".", "empty_cache", "(", ")", "\n", "model", ".", "eval", "(", ")", "\n", "next_i", "=", "0", "\n", "max_src_length", "=", "args", ".", "max_seq_length", "-", "2", "-", "args", ".", "max_tgt_length", "\n", "\n", "with", "open", "(", "args", ".", "input_file", ",", "encoding", "=", "\"utf-8\"", ")", "as", "fin", ":", "\n", "            ", "input_lines", "=", "[", "x", ".", "strip", "(", ")", "for", "x", "in", "fin", ".", "readlines", "(", ")", "]", "\n", "if", "args", ".", "subset", ">", "0", ":", "\n", "                ", "logger", ".", "info", "(", "\"Decoding subset: %d\"", ",", "args", ".", "subset", ")", "\n", "input_lines", "=", "input_lines", "[", ":", "args", ".", "subset", "]", "\n", "", "", "data_tokenizer", "=", "WhitespaceTokenizer", "(", ")", "if", "args", ".", "tokenized_input", "else", "tokenizer", "\n", "input_lines", "=", "[", "data_tokenizer", ".", "tokenize", "(", "\n", "x", ")", "[", ":", "max_src_length", "]", "for", "x", "in", "input_lines", "]", "\n", "input_lines", "=", "sorted", "(", "list", "(", "enumerate", "(", "input_lines", ")", ")", ",", "\n", "key", "=", "lambda", "x", ":", "-", "len", "(", "x", "[", "1", "]", ")", ")", "\n", "output_lines", "=", "[", "\"\"", "]", "*", "len", "(", "input_lines", ")", "\n", "score_trace_list", "=", "[", "None", "]", "*", "len", "(", "input_lines", ")", "\n", "total_batch", "=", "math", ".", "ceil", "(", "len", "(", "input_lines", ")", "/", "args", ".", "batch_size", ")", "\n", "\n", "with", "tqdm", "(", "total", "=", "total_batch", ")", "as", "pbar", ":", "\n", "            ", "while", "next_i", "<", "len", "(", "input_lines", ")", ":", "\n", "                ", "_chunk", "=", "input_lines", "[", "next_i", ":", "next_i", "+", "args", ".", "batch_size", "]", "\n", "buf_id", "=", "[", "x", "[", "0", "]", "for", "x", "in", "_chunk", "]", "\n", "buf", "=", "[", "x", "[", "1", "]", "for", "x", "in", "_chunk", "]", "\n", "next_i", "+=", "args", ".", "batch_size", "\n", "max_a_len", "=", "max", "(", "[", "len", "(", "x", ")", "for", "x", "in", "buf", "]", ")", "\n", "instances", "=", "[", "]", "\n", "for", "instance", "in", "[", "(", "x", ",", "max_a_len", ")", "for", "x", "in", "buf", "]", ":", "\n", "                    ", "for", "proc", "in", "bi_uni_pipeline", ":", "\n", "                        ", "instances", ".", "append", "(", "proc", "(", "instance", ")", ")", "\n", "", "", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "                    ", "batch", "=", "seq2seq_loader", ".", "batch_list_to_batch_tensors", "(", "\n", "instances", ")", "\n", "batch", "=", "[", "\n", "t", ".", "to", "(", "device", ")", "if", "t", "is", "not", "None", "else", "None", "for", "t", "in", "batch", "]", "\n", "input_ids", ",", "token_type_ids", ",", "position_ids", ",", "input_mask", ",", "mask_qkv", ",", "task_idx", "=", "batch", "\n", "traces", "=", "model", "(", "input_ids", ",", "token_type_ids", ",", "\n", "position_ids", ",", "input_mask", ",", "task_idx", "=", "task_idx", ",", "mask_qkv", "=", "mask_qkv", ")", "\n", "if", "args", ".", "beam_size", ">", "1", ":", "\n", "                        ", "traces", "=", "{", "k", ":", "v", ".", "tolist", "(", ")", "for", "k", ",", "v", "in", "traces", ".", "items", "(", ")", "}", "\n", "output_ids", "=", "traces", "[", "'pred_seq'", "]", "\n", "", "else", ":", "\n", "                        ", "output_ids", "=", "traces", ".", "tolist", "(", ")", "\n", "", "for", "i", "in", "range", "(", "len", "(", "buf", ")", ")", ":", "\n", "                        ", "w_ids", "=", "output_ids", "[", "i", "]", "\n", "output_buf", "=", "tokenizer", ".", "convert_ids_to_tokens", "(", "w_ids", ")", "\n", "output_tokens", "=", "[", "]", "\n", "for", "t", "in", "output_buf", ":", "\n", "                            ", "if", "t", "in", "(", "\"[SEP]\"", ",", "\"[PAD]\"", ")", ":", "\n", "                                ", "break", "\n", "", "output_tokens", ".", "append", "(", "t", ")", "\n", "", "output_sequence", "=", "' '", ".", "join", "(", "detokenize", "(", "output_tokens", ")", ")", "\n", "output_lines", "[", "buf_id", "[", "i", "]", "]", "=", "output_sequence", "\n", "if", "args", ".", "need_score_traces", ":", "\n", "                            ", "score_trace_list", "[", "buf_id", "[", "i", "]", "]", "=", "{", "\n", "'scores'", ":", "traces", "[", "'scores'", "]", "[", "i", "]", ",", "'wids'", ":", "traces", "[", "'wids'", "]", "[", "i", "]", ",", "'ptrs'", ":", "traces", "[", "'ptrs'", "]", "[", "i", "]", "}", "\n", "", "", "", "pbar", ".", "update", "(", "1", ")", "\n", "", "", "if", "args", ".", "output_file", ":", "\n", "            ", "fn_out", "=", "args", ".", "output_file", "\n", "", "else", ":", "\n", "            ", "fn_out", "=", "model_recover_path", "+", "'.'", "+", "args", ".", "split", "\n", "", "with", "open", "(", "fn_out", ",", "\"w\"", ",", "encoding", "=", "\"utf-8\"", ")", "as", "fout", ":", "\n", "            ", "for", "l", "in", "output_lines", ":", "\n", "                ", "fout", ".", "write", "(", "l", ")", "\n", "fout", ".", "write", "(", "\"\\n\"", ")", "\n", "\n", "", "", "if", "args", ".", "need_score_traces", ":", "\n", "            ", "with", "open", "(", "fn_out", "+", "\".trace.pickle\"", ",", "\"wb\"", ")", "as", "fout_trace", ":", "\n", "                ", "pickle", ".", "dump", "(", "\n", "{", "\"version\"", ":", "0.0", ",", "\"num_samples\"", ":", "len", "(", "input_lines", ")", "}", ",", "fout_trace", ")", "\n", "for", "x", "in", "score_trace_list", ":", "\n", "                    ", "pickle", ".", "dump", "(", "x", ",", "fout_trace", ")", "\n", "\n", "\n", "", "", "", "", "", "if", "__name__", "==", "\"__main__\"", ":", "\n", "    ", "main", "(", ")", "\n", "", ""]], "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.title_script.process_title.refine_feature_1342": [[15, 50], ["feature.split", "feature.split", "len", "len", "feature.split", "len", "feature.split"], "function", ["home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.preprocess.process_daren.split", "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.preprocess.process_daren.split", "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.preprocess.process_daren.split", "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.preprocess.process_daren.split"], ["def", "refine_feature_1342", "(", "prod", ",", "feature", ")", ":", "\n", "#\u53bb\u6389\u6ca1\u6709\u751f\u6210\u5b8c\u6574\u7684", "\n", "    ", "if", "'('", "in", "feature", ":", "\n", "      ", "feature", "=", "feature", ".", "split", "(", "'('", ")", "[", "1", "]", "\n", "", "if", "\"+\"", "not", "in", "feature", "or", "len", "(", "feature", ".", "split", "(", "\"+\"", ")", ")", ">", "2", ":", "\n", "      ", "return", "''", "\n", "", "if", "'[UNK]'", "in", "feature", ":", "\n", "      ", "return", "''", "\n", "", "sensitive_words", "=", "[", "'\u65f6\u5c1a'", ",", "'\u767e\u642d'", ",", "'\u978b'", ",", "'\u3011'", ",", "'\u3010'", ",", "'\uff09'", ",", "'\uff08'", "]", "\n", "for", "word", "in", "sensitive_words", ":", "\n", "      ", "if", "word", "in", "feature", ":", "\n", "         ", "return", "''", "\n", "\n", "#\u68c0\u67e5\u5546\u54c1\u8bcd", "\n", "", "", "pw", "=", "feature", ".", "split", "(", "'+'", ")", "[", "1", "]", "\n", "if", "'\u4ef6\u5957'", "in", "prod", "or", "'\u5957\u88c5'", "in", "prod", ":", "\n", "       ", "if", "(", "'\u5957'", "not", "in", "pw", "or", "'\u4e24\u4ef6'", "not", "in", "pw", "or", "'\u4e09\u4ef6'", "not", "in", "pw", ")", ":", "\n", "           ", "pw", "=", "pw", "+", "'\u5957\u88c5'", "\n", "\n", "", "", "core_word", "=", "[", "'\u88e4'", ",", "'\u88d9'", ",", "'\u886c\u886b'", "]", "\n", "for", "w", "in", "core_word", ":", "\n", "     ", "if", "w", "in", "pw", "and", "w", "not", "in", "prod", ":", "\n", "       ", "return", "''", "\n", "\n", "#\u68c0\u67e5\u4fee\u9970\u8bcd", "\n", "", "", "subj", "=", "feature", ".", "split", "(", "'+'", ")", "[", "0", "]", "\n", "if", "len", "(", "subj", ")", ">", "7", ":", "\n", "      ", "return", "''", "\n", "", "if", "subj", "==", "pw", "or", "subj", "in", "pw", ":", "\n", "      ", "return", "''", "\n", "\n", "", "if", "len", "(", "subj", ")", ">", "1", ":", "\n", "      ", "return", "subj", "+", "' '", "+", "pw", "\n", "", "else", ":", "\n", "      ", "return", "''", "\n", "\n"]], "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.title_script.process_title.refine_feature_671": [[52, 93], ["re.findall", "feature.split", "feature.split", "len", "len", "len", "feature.split", "len", "subj_get.extend", "prod_get.extend", "subj.replace.replace", "feature.split", "re.findall", "re.findall", "len", "len"], "function", ["home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.preprocess.process_daren.split", "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.preprocess.process_daren.split", "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.preprocess.process_daren.split", "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.preprocess.process_daren.split"], ["", "", "def", "refine_feature_671", "(", "prod", ",", "feature", ")", ":", "\n", "#\u53bb\u6389\u6ca1\u6709\u751f\u6210\u5b8c\u6574\u7684", "\n", "    ", "if", "'('", "in", "feature", ":", "\n", "      ", "feature", "=", "feature", ".", "split", "(", "'('", ")", "[", "1", "]", "\n", "", "if", "\"+\"", "not", "in", "feature", "or", "len", "(", "feature", ".", "split", "(", "\"+\"", ")", ")", ">", "2", ":", "\n", "      ", "return", "''", "\n", "", "if", "'[UNK]'", "in", "feature", ":", "\n", "      ", "return", "''", "\n", "", "sensitive_words", "=", "[", "'\u3011'", ",", "'\u3010'", ",", "'\uff09'", ",", "'\uff08'", "]", "\n", "for", "word", "in", "sensitive_words", ":", "\n", "      ", "if", "word", "in", "feature", ":", "\n", "         ", "return", "''", "\n", "\n", "#\u68c0\u67e5\u5546\u54c1\u8bcd", "\n", "", "", "pw", "=", "feature", ".", "split", "(", "'+'", ")", "[", "1", "]", "\n", "if", "'\u5957\u88c5'", "in", "prod", ":", "\n", "           ", "pw", "=", "'\u7535\u8111\u5957\u88c5'", "\n", "\n", "#\u68c0\u67e5\u4fee\u9970\u8bcd", "\n", "", "subj", "=", "feature", ".", "split", "(", "'+'", ")", "[", "0", "]", "\n", "if", "len", "(", "subj", ")", ">", "7", ":", "\n", "       ", "return", "''", "\n", "", "if", "subj", "==", "pw", "or", "subj", "in", "pw", ":", "\n", "       ", "return", "''", "\n", "", "num_pattern", "=", "re", ".", "findall", "(", "'\\d+'", ",", "subj", ")", "\n", "if", "len", "(", "num_pattern", ")", ">", "0", ":", "\n", "        ", "subj", "=", "''", "\n", "", "pattern_list", "=", "[", "[", "'[\u5355\uff5c\u53cc\uff5c\u516d\uff5c\u516b\uff5c\u56db]\u6838'", ",", "'\\d+\\.?\\d*\u6838'", "]", "]", "\n", "for", "pattern_l", "in", "pattern_list", ":", "\n", "        ", "subj_get", "=", "[", "]", "\n", "prod_get", "=", "[", "]", "\n", "for", "pattern", "in", "pattern_l", ":", "\n", "                  ", "subj_get", ".", "extend", "(", "re", ".", "findall", "(", "pattern", ",", "subj", ")", ")", "\n", "prod_get", ".", "extend", "(", "re", ".", "findall", "(", "pattern", ",", "prod", ")", ")", "\n", "", "if", "len", "(", "subj_get", ")", ">", "0", "and", "len", "(", "prod_get", ")", ">", "0", ":", "\n", "                  ", "subj", "=", "subj", ".", "replace", "(", "subj_get", "[", "0", "]", ",", "prod_get", "[", "0", "]", ")", "\n", "\n", "", "", "if", "len", "(", "subj", ")", ">", "1", ":", "\n", "      ", "return", "subj", "+", "' '", "+", "pw", "\n", "", "else", ":", "\n", "      ", "return", "''", "\n", "\n"]], "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.title_script.process_title.refine_feature_1381": [[95, 127], ["re.findall", "feature.split", "feature.split", "len", "len", "len", "feature.split", "len", "feature.split"], "function", ["home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.preprocess.process_daren.split", "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.preprocess.process_daren.split", "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.preprocess.process_daren.split", "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.preprocess.process_daren.split"], ["", "", "def", "refine_feature_1381", "(", "prod", ",", "feature", ")", ":", "\n", "#\u53bb\u6389\u6ca1\u6709\u751f\u6210\u5b8c\u6574\u7684", "\n", "    ", "if", "'('", "in", "feature", ":", "\n", "      ", "feature", "=", "feature", ".", "split", "(", "'('", ")", "[", "1", "]", "\n", "", "if", "\"+\"", "not", "in", "feature", "or", "len", "(", "feature", ".", "split", "(", "\"+\"", ")", ")", ">", "2", ":", "\n", "      ", "return", "''", "\n", "", "if", "'[UNK]'", "in", "feature", ":", "\n", "      ", "return", "''", "\n", "", "sensitive_words", "=", "[", "'\u3011'", ",", "'\u3010'", ",", "'\uff09'", ",", "'\uff08'", "]", "\n", "for", "word", "in", "sensitive_words", ":", "\n", "      ", "if", "word", "in", "feature", ":", "\n", "         ", "return", "''", "\n", "\n", "#\u68c0\u67e5\u5546\u54c1\u8bcd", "\n", "", "", "pw", "=", "feature", ".", "split", "(", "'+'", ")", "[", "1", "]", "\n", "if", "'\u5957\u88c5'", "in", "prod", ":", "\n", "           ", "pw", "=", "' \u62a4\u80a4\u5957\u88c5'", "\n", "\n", "#\u68c0\u67e5\u4fee\u9970\u8bcd", "\n", "", "subj", "=", "feature", ".", "split", "(", "'+'", ")", "[", "0", "]", "\n", "if", "len", "(", "subj", ")", ">", "7", ":", "\n", "       ", "return", "''", "\n", "", "if", "subj", "==", "pw", "or", "subj", "in", "pw", ":", "\n", "       ", "return", "''", "\n", "", "num_pattern", "=", "re", ".", "findall", "(", "'\\d+'", ",", "subj", ")", "\n", "if", "len", "(", "num_pattern", ")", ">", "0", ":", "\n", "        ", "subj", "=", "''", "\n", "\n", "", "if", "len", "(", "subj", ")", ">", "1", ":", "\n", "      ", "return", "subj", "+", "' '", "+", "pw", "\n", "", "else", ":", "\n", "      ", "return", "''", "\n", "\n"]], "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.title_script.process_title.refine_feature_794": [[128, 158], ["re.findall", "feature.split", "feature.split", "len", "len", "len", "feature.split", "len", "feature.split"], "function", ["home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.preprocess.process_daren.split", "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.preprocess.process_daren.split", "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.preprocess.process_daren.split", "home.repos.pwc.inspect_result.xguo7_automatic-controllable-product-copywriting-for-e-commerce.preprocess.process_daren.split"], ["", "", "def", "refine_feature_794", "(", "prod", ",", "feature", ")", ":", "\n", "#\u53bb\u6389\u6ca1\u6709\u751f\u6210\u5b8c\u6574\u7684", "\n", "    ", "if", "'('", "in", "feature", ":", "\n", "      ", "feature", "=", "feature", ".", "split", "(", "'('", ")", "[", "1", "]", "\n", "", "if", "\"+\"", "not", "in", "feature", "or", "len", "(", "feature", ".", "split", "(", "\"+\"", ")", ")", ">", "2", ":", "\n", "      ", "return", "''", "\n", "", "if", "'[UNK]'", "in", "feature", ":", "\n", "      ", "return", "''", "\n", "", "sensitive_words", "=", "[", "'\u3011'", ",", "'\u3010'", ",", "'\uff09'", ",", "'\uff08'", "]", "\n", "for", "word", "in", "sensitive_words", ":", "\n", "      ", "if", "word", "in", "feature", ":", "\n", "         ", "return", "''", "\n", "\n", "", "", "pw", "=", "feature", ".", "split", "(", "'+'", ")", "[", "1", "]", "\n", "if", "'\u5957\u88c5'", "in", "prod", ":", "\n", "           ", "pw", "=", "' \u5bb6\u7535\u5957\u88c5'", "\n", "#\u68c0\u67e5\u4fee\u9970\u8bcd", "\n", "", "subj", "=", "feature", ".", "split", "(", "'+'", ")", "[", "0", "]", "\n", "if", "len", "(", "subj", ")", ">", "7", ":", "\n", "       ", "return", "''", "\n", "", "if", "subj", "==", "pw", "or", "subj", "in", "pw", ":", "\n", "       ", "return", "''", "\n", "", "num_pattern", "=", "re", ".", "findall", "(", "'\\d+'", ",", "subj", ")", "\n", "if", "len", "(", "num_pattern", ")", ">", "0", ":", "\n", "        ", "subj", "=", "''", "\n", "\n", "", "if", "len", "(", "subj", ")", ">", "1", ":", "\n", "      ", "return", "subj", "+", "' '", "+", "pw", "\n", "", "else", ":", "\n", "      ", "return", "''", "\n", "\n"]]}