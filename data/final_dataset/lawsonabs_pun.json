{"home.repos.pwc.inspect_result.lawsonabs_pun.None.cv_run_ner.main": [[33, 642], ["argparse.ArgumentParser", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.parse_args", "time.strftime", "logging.basicConfig", "logging.getLogger", "logging.getLogger.info", "vars().items", "logging.getLogger.info", "random.seed", "numpy.random.seed", "torch.manual_seed", "torch.manual_seed", "parser.parse_args.task_name.lower", "processor.get_labels", "len", "pytorch_pretrained_bert.tokenization.BertTokenizer.from_pretrained", "processor.get_train_examples", "numpy.array", "bert_utils.getAllWordSenseEmb", "sklearn.model_selection.KFold", "sklearn.model_selection.KFold.get_n_splits", "AutoTokenizer.from_pretrained", "BertModel.from_pretrained", "BertModel.from_pretrained.to", "sklearn.model_selection.KFold.split", "print", "ptvsd.enable_attach", "ptvsd.wait_for_attach", "os.path.isdir", "os.mkdir", "time.localtime", "logging.getLogger.info", "torch.device", "torch.device", "torch.cuda.device_count", "torch.cuda.device_count", "torch.cuda.set_device", "torch.cuda.set_device", "torch.device", "torch.device", "torch.distributed.init_process_group", "torch.distributed.init_process_group", "ValueError", "ValueError", "os.path.exists", "os.listdir", "logging.getLogger.info", "shutil.rmtree", "os.path.exists", "os.makedirs", "ValueError", "list", "list", "bert_models.BertForTokenPronsClassification_v2.from_pretrained", "torch.nn.DataParallel.to", "list", "bert_utils.embed_load", "bert_utils.convert_examples_to_pron_features", "bert_utils.convert_examples_to_pron_features", "bert_utils.embed_extend", "torch.tensor", "torch.tensor", "torch.nn.Embedding.from_pretrained", "torch.nn.Embedding.from_pretrained", "logging.getLogger.info", "logging.getLogger.info", "logging.getLogger.info", "logging.getLogger.info", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.utils.data.TensorDataset", "torch.utils.data.DataLoader", "logging.getLogger.info", "logging.getLogger.info", "logging.getLogger.info", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.utils.data.TensorDataset", "torch.utils.data.SequentialSampler", "torch.utils.data.DataLoader", "torch.nn.DataParallel.train", "bert_utils.get_word_key_id_2_map", "logging.getLogger.info", "tqdm.trange", "os.path.join", "torch.save", "torch.save", "os.path.join", "json.dump", "vars", "bool", "int", "os.path.join", "torch.nn.DataParallel.half", "DDP", "torch.nn.DataParallel.named_parameters", "FusedAdam", "pytorch_pretrained_bert.optimization.BertAdam", "len", "len", "torch.utils.data.RandomSampler", "torch.utils.data.distributed.DistributedSampler", "len", "int", "logging.getLogger.info", "enumerate", "seqeval.metrics.classification_report", "logging.getLogger.info", "logging.getLogger.info", "logging.getLogger.info", "tqdm.tqdm", "seqeval.metrics.classification_report", "logging.getLogger.info", "seqeval.metrics.f1_score", "hasattr", "model_to_save.state_dict", "open", "f.write", "open", "torch.distributed.get_world_size", "torch.distributed.get_world_size", "str", "torch.nn.DataParallel", "torch.nn.DataParallel", "FP16_Optimizer", "FP16_Optimizer", "enumerate", "tqdm.tqdm", "tuple", "torch.nn.Embedding.from_pretrained.to", "torch.nn.DataParallel.", "loss.mean.item", "input_ids.to.size", "torch.argmax", "torch.argmax", "logits.detach().cpu().numpy.detach().cpu().numpy", "label_ids.to().numpy.to().numpy", "input_mask.to().numpy.to().numpy", "enumerate", "torch.nn.Embedding.from_pretrained.to", "input_ids.to.to", "input_mask.to().numpy.to", "segment_ids.to.to", "label_ids.to().numpy.to", "prons_ids.to.to", "prons_att_mask.to.to", "torch.argmax", "torch.argmax", "logits.detach().cpu().numpy.detach().cpu().numpy", "label_ids.to().numpy.to().numpy", "input_mask.to().numpy.to().numpy", "enumerate", "bert_utils.write_scores", "bert_utils.write_scores", "bert_utils.writeToTxt", "str", "model_to_save.config.to_json_string", "enumerate", "len", "os.path.join", "torch.cuda.is_available", "torch.cuda.is_available", "ImportError", "ImportError", "str", "torch.cat.cuda", "loss.mean.mean", "FP16_Optimizer.backward", "loss.mean.backward", "FP16_Optimizer.step", "FP16_Optimizer.zero_grad", "torch.log_softmax", "enumerate", "torch.cat.cuda", "torch.no_grad", "torch.no_grad", "torch.nn.DataParallel.", "torch.log_softmax", "BertTokenizer.from_pretrained.convert_ids_to_tokens", "logging.getLogger.info", "enumerate", "len", "any", "t.to", "torch.nn.Embedding.from_pretrained.", "AutoTokenizer.from_pretrained.convert_ids_to_tokens", "bert_utils.getPunEmb", "cur_pun_emb.view.view", "logits.detach().cpu().numpy.detach().cpu", "label_ids.to().numpy.to", "input_mask.to().numpy.to", "torch.nn.Embedding.from_pretrained.", "AutoTokenizer.from_pretrained.convert_ids_to_tokens", "all_tokens.append", "bert_utils.getPunEmb", "cur_pun_emb.view.view", "AutoTokenizer.from_pretrained.convert_ids_to_tokens", "all_tokens.append", "logits.detach().cpu().numpy.detach().cpu", "label_ids.to().numpy.to", "input_mask.to().numpy.to", "input_ids[].tolist", "str", "str", "str", "any", "prons_ids.to.detach().cpu", "torch.cat", "torch.cat", "temp_1.append", "temp_2.append", "y_true.append", "y_pred.append", "torch.cat", "torch.cat", "temp_1.append", "temp_2.append", "temp_1.pop", "temp_2.pop", "y_true.append", "y_pred.append", "logits.detach().cpu().numpy.detach", "len", "logits.detach().cpu().numpy.detach", "len", "range", "list", "logging.getLogger.info", "range", "prons_ids.to.detach", "range", "len", "BertTokenizer.from_pretrained.convert_ids_to_tokens", "len", "sorted", "logging.getLogger.info", "len", "ind_val_dict.items", "str", "bert_utils.get_word_key_id_2_map.keys", "logging.getLogger.info", "[].item"], "function", ["home.repos.pwc.inspect_result.lawsonabs_pun.None.bert_utils.ScProcessor.get_labels", "home.repos.pwc.inspect_result.lawsonabs_pun.None.bert_models.BertPreTrainedModel.from_pretrained", "home.repos.pwc.inspect_result.lawsonabs_pun.None.bert_utils.ScProcessor.get_train_examples", "home.repos.pwc.inspect_result.lawsonabs_pun.None.bert_utils.getAllWordSenseEmb", "home.repos.pwc.inspect_result.lawsonabs_pun.None.bert_models.BertPreTrainedModel.from_pretrained", "home.repos.pwc.inspect_result.lawsonabs_pun.None.bert_models.BertPreTrainedModel.from_pretrained", "home.repos.pwc.inspect_result.lawsonabs_pun.None.bert_models.BertPreTrainedModel.from_pretrained", "home.repos.pwc.inspect_result.lawsonabs_pun.None.bert_utils.embed_load", "home.repos.pwc.inspect_result.lawsonabs_pun.tools.utils.convert_examples_to_pron_features", "home.repos.pwc.inspect_result.lawsonabs_pun.tools.utils.convert_examples_to_pron_features", "home.repos.pwc.inspect_result.lawsonabs_pun.None.bert_utils.embed_extend", "home.repos.pwc.inspect_result.lawsonabs_pun.None.bert_models.BertPreTrainedModel.from_pretrained", "home.repos.pwc.inspect_result.lawsonabs_pun.None.bert_models.BertPreTrainedModel.from_pretrained", "home.repos.pwc.inspect_result.lawsonabs_pun.subtask3_v2.gloss.train", "home.repos.pwc.inspect_result.lawsonabs_pun.None.bert_utils.get_word_key_id_2_map", "home.repos.pwc.inspect_result.lawsonabs_pun.None.bert_utils.write_scores", "home.repos.pwc.inspect_result.lawsonabs_pun.None.bert_utils.write_scores", "home.repos.pwc.inspect_result.lawsonabs_pun.None.bert_utils.writeToTxt", "home.repos.pwc.inspect_result.lawsonabs_pun.None.bert_models.BertConfig.to_json_string", "home.repos.pwc.inspect_result.lawsonabs_pun.None.bert_utils.getPunEmb", "home.repos.pwc.inspect_result.lawsonabs_pun.None.bert_utils.getPunEmb"], ["def", "main", "(", ")", ":", "\n", "    ", "parser", "=", "argparse", ".", "ArgumentParser", "(", ")", "\n", "parser", ".", "add_argument", "(", "\"--defi_num\"", ",", "\n", "default", "=", "50", ",", "\n", "type", "=", "int", ",", "\n", "help", "=", "\"The number of definition.\"", ")", "\n", "\n", "\n", "## Required parameters", "\n", "parser", ".", "add_argument", "(", "\"--data_dir\"", ",", "\n", "default", "=", "None", ",", "\n", "type", "=", "str", ",", "\n", "required", "=", "True", ",", "\n", "help", "=", "\"The input data dir. Should contain the .tsv files (or other data files) for the task.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--bert_model\"", ",", "default", "=", "None", ",", "type", "=", "str", ",", "required", "=", "True", ",", "\n", "help", "=", "\"Bert pre-trained model selected in the list: bert-base-uncased, \"", "\n", "\"bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, \"", "\n", "\"bert-base-multilingual-cased, bert-base-chinese.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--task_name\"", ",", "\n", "default", "=", "None", ",", "\n", "type", "=", "str", ",", "\n", "required", "=", "True", ",", "\n", "help", "=", "\"The name of the task to train.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--output_dir\"", ",", "\n", "default", "=", "None", ",", "\n", "type", "=", "str", ",", "\n", "required", "=", "True", ",", "\n", "help", "=", "\"The output directory where the model predictions and checkpoints will be written.\"", ")", "\n", "\n", "## Other parameters", "\n", "parser", ".", "add_argument", "(", "\"--cache_dir\"", ",", "\n", "default", "=", "\"\"", ",", "\n", "type", "=", "str", ",", "\n", "help", "=", "\"Where do you want to store the pre-trained models downloaded from s3\"", ")", "\n", "\n", "# \u6211\u5c06128 \u6539\u4e3a\u4e86 75\u3002 \u56e0\u4e3a\u8c03\u7814\u540e\u7684\u6570\u636e\u53d1\u73b0\u6700\u5927\u7684\u957f\u5ea6\u662f68 + cls + sep \u4e5f\u5c3170", "\n", "# \u800c\u4e14\u8d85\u8fc750 \u7684\u5c31\u53ea\u67091 \u6761\uff0c\u6240\u4ee5\u8fd9\u91cc\u8fd8\u662f\u53ea\u752855\u7684\u957f\u5ea6\uff0c\u5982\u679c\u8d85\u51fa\u4e86\uff0c\u5219\u622a\u65ad", "\n", "parser", ".", "add_argument", "(", "\"--max_seq_length\"", ",", "\n", "default", "=", "55", ",", "\n", "type", "=", "int", ",", "\n", "help", "=", "\"The maximum total input sequence length after WordPiece tokenization. \\n\"", "\n", "\"Sequences longer than this will be truncated, and sequences shorter \\n\"", "\n", "\"than this will be padded.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--max_pron_length\"", ",", "\n", "default", "=", "5", ",", "\n", "type", "=", "int", ",", "\n", "help", "=", "\"The maximum total input sequence length after pronounciation tokenization. \\n\"", "\n", "\"Sequences longer than this will be truncated, and sequences shorter \\n\"", "\n", "\"than this will be padded.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--pron_emb_size\"", ",", "\n", "default", "=", "16", ",", "\n", "type", "=", "int", ",", "\n", "help", "=", "\"The embedding size of pronounciation embedding.\"", ")", "\n", "\n", "'''\n    1.--do_train \u8fd9\u4e00\u9009\u9879\u66f4\u591a\u5730\u662f\u4e00\u4e2a\u6807\u5fd7\uff0c\u800c\u975e\u9700\u8981\u63a5\u53d7\u4e00\u4e2a\u503c\u7684\u4ec0\u4e48\u4e1c\u897f\u3002\n    \u73b0\u5728\u6307\u5b9a\u4e00\u4e2a\u65b0\u7684\u5173\u952e\u8bcd action\uff0c \u5e76\u8d4b\u503c\u4e3a \"store_true\"\u3002 \n    \u8fd9\u610f\u5473\u7740\uff0c\u5f53\u8fd9\u4e00\u9009\u9879\u5b58\u5728\u65f6\uff0c\u4e3a args.do_train \u8d4b\u503c\u4e3a True\u3002\u6ca1\u6709\u6307\u5b9a\u65f6\u5219\u9690\u542b\u5730\u8d4b\u503c\u4e3a False\u3002\n    2.\u5f53\u4f60\u4e3a\u5176\u5236\u5b9a\u4e00\u4e2a\u503c\u65f6\uff0c\u5b83\u4f1a\u62a5\u9519\uff0c\u56e0\u4e3a\u5b83\u5c31\u662f\u4e00\u4e2a\u6807\u5fd7\n    '''", "\n", "parser", ".", "add_argument", "(", "\"--do_train\"", ",", "\n", "action", "=", "'store_true'", ",", "\n", "help", "=", "\"Whether to run training.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--do_eval\"", ",", "\n", "action", "=", "'store_true'", ",", "\n", "help", "=", "\"Whether to run eval on the dev set.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--do_pron\"", ",", "\n", "action", "=", "'store_true'", ",", "\n", "help", "=", "'Whether to use pronunciation as features.'", ")", "\n", "\n", "parser", ".", "add_argument", "(", "\"--use_sense\"", ",", "# \u4f46\u662f\u597d\u50cf\u5728\u540e\u9762\u6ca1\u7528\u5230", "\n", "action", "=", "'store_true'", ",", "\n", "help", "=", "'Whether to use sense as features.'", ")", "\n", "\n", "parser", ".", "add_argument", "(", "\"--do_lower_case\"", ",", "\n", "action", "=", "'store_true'", ",", "\n", "help", "=", "\"Set this flag if you are using an uncased model.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--train_batch_size\"", ",", "\n", "default", "=", "5", ",", "\n", "type", "=", "int", ",", "\n", "help", "=", "\"Total batch size for training.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--eval_batch_size\"", ",", "\n", "default", "=", "2", ",", "\n", "type", "=", "int", ",", "\n", "help", "=", "\"Total batch size for eval.\"", ")", "\n", "\n", "parser", ".", "add_argument", "(", "\"--learning_rate\"", ",", "\n", "default", "=", "5e-5", ",", "\n", "type", "=", "float", ",", "\n", "help", "=", "\"The initial learning rate for Adam.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--num_train_epochs\"", ",", "\n", "default", "=", "3", ",", "\n", "type", "=", "int", ",", "\n", "help", "=", "\"Total number of training epochs to perform.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--warmup_proportion\"", ",", "\n", "default", "=", "0.1", ",", "\n", "type", "=", "float", ",", "\n", "help", "=", "\"Proportion of training to perform linear learning rate warmup for. \"", "\n", "\"E.g., 0.1 = 10%% of training.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--no_cuda\"", ",", "\n", "action", "=", "'store_true'", ",", "\n", "help", "=", "\"Whether not to use CUDA when available\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--local_rank\"", ",", "\n", "type", "=", "int", ",", "\n", "default", "=", "-", "1", ",", "\n", "help", "=", "\"local_rank for distributed training on gpus\"", ")", "\n", "parser", ".", "add_argument", "(", "'--seed'", ",", "\n", "type", "=", "int", ",", "\n", "default", "=", "42", ",", "\n", "help", "=", "\"random seed for initialization\"", ")", "\n", "parser", ".", "add_argument", "(", "'--gradient_accumulation_steps'", ",", "\n", "type", "=", "int", ",", "\n", "default", "=", "1", ",", "\n", "help", "=", "\"Number of updates steps to accumulate before performing a backward/update pass.\"", ")", "\n", "parser", ".", "add_argument", "(", "'--fp16'", ",", "\n", "action", "=", "'store_true'", ",", "\n", "help", "=", "\"Whether to use 16-bit float precision instead of 32-bit\"", ")", "\n", "parser", ".", "add_argument", "(", "'--loss_scale'", ",", "\n", "type", "=", "float", ",", "default", "=", "0", ",", "\n", "help", "=", "\"Loss scaling to improve fp16 numeric stability. Only used when fp16 set to True.\\n\"", "\n", "\"0 (default value): dynamic loss scaling.\\n\"", "\n", "\"Positive power of 2: static loss scaling value.\\n\"", ")", "\n", "parser", ".", "add_argument", "(", "'--server_ip'", ",", "type", "=", "str", ",", "default", "=", "''", ",", "help", "=", "\"Can be used for distant debugging.\"", ")", "\n", "parser", ".", "add_argument", "(", "'--server_port'", ",", "type", "=", "str", ",", "default", "=", "''", ",", "help", "=", "\"Can be used for distant debugging.\"", ")", "\n", "parser", ".", "add_argument", "(", "'--file_suffix'", ",", "type", "=", "str", ",", "default", "=", "0", ",", "\n", "help", "=", "\"The suffix of file\"", ")", "\n", "\n", "parser", ".", "add_argument", "(", "'--use_random'", ",", "\n", "action", "=", "'store_true'", ",", "\n", "default", "=", "False", ",", "# \u9ed8\u8ba4\u503c\u4e3afalse\uff0c\u5373\u4f7f\u7528 zero \u586b\u5145", "\n", "help", "=", "'to judge whether use random number padding sense embedding'", ")", "\n", "\n", "\n", "args", "=", "parser", ".", "parse_args", "(", ")", "\n", "\n", "if", "args", ".", "server_ip", "and", "args", ".", "server_port", ":", "\n", "# Distant debugging - see https://code.visualstudio.com/docs/python/debugging#_attach-to-a-local-script", "\n", "        ", "import", "ptvsd", "\n", "print", "(", "\"Waiting for debugger attach\"", ")", "\n", "ptvsd", ".", "enable_attach", "(", "address", "=", "(", "args", ".", "server_ip", ",", "args", ".", "server_port", ")", ",", "redirect_output", "=", "True", ")", "\n", "ptvsd", ".", "wait_for_attach", "(", ")", "\n", "\n", "", "if", "\"homo\"", "in", "args", ".", "data_dir", ":", "\n", "        ", "mark", "=", "\"homo_\"", "# \u6709\u4ec0\u4e48\u7528\uff1f\u7528\u4e8e\u540e\u9762\u751f\u6210\u6587\u4ef6\u5939\u4f7f\u7528", "\n", "", "else", ":", "\n", "        ", "mark", "=", "\"hete_\"", "\n", "\n", "# \u8fd9\u91cc\u4fee\u6539\u4e00\u4e0b\u6587\u4ef6\u540d", "\n", "", "if", "args", ".", "use_sense", ":", "\n", "        ", "mark", "+=", "\"sense_\"", "\n", "", "if", "args", ".", "do_pron", ":", "\n", "        ", "mark", "+=", "\"pron_\"", "\n", "\n", "", "score_file", "=", "\"scores/\"", "+", "mark", "+", "args", ".", "file_suffix", "+", "'/'", "\n", "if", "not", "os", ".", "path", ".", "isdir", "(", "score_file", ")", ":", "os", ".", "mkdir", "(", "score_file", ")", "\n", "args", ".", "output_dir", "=", "score_file", "+", "args", ".", "output_dir", "\n", "\n", "import", "time", "\n", "curTime", "=", "time", ".", "strftime", "(", "\"%m%d_%H%M%S\"", ",", "time", ".", "localtime", "(", ")", ")", "\n", "log_name", "=", "curTime", "+", "'.log'", "\n", "# \u6839\u636e\u751f\u6210\u7684\u6587\u4ef6\u5939\uff0c\u5c06\u65e5\u5fd7\u5199\u5230\u5176\u4e2d", "\n", "logging", ".", "basicConfig", "(", "format", "=", "'%(asctime)s - %(levelname)s - %(name)s -   %(message)s'", ",", "\n", "datefmt", "=", "'%m/%d/%Y %H:%M:%S'", ",", "\n", "level", "=", "logging", ".", "INFO", ",", "\n", "filename", "=", "\"./\"", "+", "score_file", "+", "log_name", ",", "# \u4ee5\u5f53\u524d\u65f6\u95f4\u4f5c\u4e3alog\u540d\uff0c\u53ef\u4ee5\u6307\u5b9a\u4e00\u4e2a\u6587\u4ef6\u5939", "\n", "filemode", "=", "'w'", ",", "# \u5199\u6a21\u5f0f", "\n", ")", "\n", "logger", "=", "logging", ".", "getLogger", "(", "__name__", ")", "\n", "\n", "logger", ".", "info", "(", "\"the paramers in this model are:\"", ")", "\n", "for", "k", ",", "v", "in", "(", "vars", "(", "args", ")", ".", "items", "(", ")", ")", ":", "\n", "        ", "logger", ".", "info", "(", "f\"{k,v}\"", ")", "\n", "\n", "", "if", "args", ".", "local_rank", "==", "-", "1", "or", "args", ".", "no_cuda", ":", "\n", "        ", "device", "=", "torch", ".", "device", "(", "\"cuda\"", "if", "torch", ".", "cuda", ".", "is_available", "(", ")", "and", "not", "args", ".", "no_cuda", "else", "\"cpu\"", ")", "\n", "n_gpu", "=", "torch", ".", "cuda", ".", "device_count", "(", ")", "\n", "", "else", ":", "\n", "        ", "torch", ".", "cuda", ".", "set_device", "(", "args", ".", "local_rank", ")", "\n", "device", "=", "torch", ".", "device", "(", "\"cuda\"", ",", "args", ".", "local_rank", ")", "\n", "n_gpu", "=", "1", "\n", "# Initializes the distributed backend which will take care of sychronizing nodes/GPUs", "\n", "torch", ".", "distributed", ".", "init_process_group", "(", "backend", "=", "'nccl'", ")", "\n", "\n", "# \u4e0b\u9762\u8fd9\u4e2alogger \u662f\u4ece bert_utils \u4e2d\u5bfc\u5165\u7684", "\n", "", "logger", ".", "info", "(", "\"device: {} n_gpu: {}, distributed training: {}, 16-bits training: {}\"", ".", "format", "(", "\n", "device", ",", "n_gpu", ",", "bool", "(", "args", ".", "local_rank", "!=", "-", "1", ")", ",", "args", ".", "fp16", ")", ")", "\n", "\n", "if", "args", ".", "gradient_accumulation_steps", "<", "1", ":", "\n", "        ", "raise", "ValueError", "(", "\"Invalid gradient_accumulation_steps parameter: {}, should be >= 1\"", ".", "format", "(", "\n", "args", ".", "gradient_accumulation_steps", ")", ")", "\n", "\n", "", "args", ".", "train_batch_size", "=", "args", ".", "train_batch_size", "//", "args", ".", "gradient_accumulation_steps", "\n", "\n", "random", ".", "seed", "(", "args", ".", "seed", ")", "\n", "np", ".", "random", ".", "seed", "(", "args", ".", "seed", ")", "\n", "torch", ".", "manual_seed", "(", "args", ".", "seed", ")", "\n", "\n", "if", "not", "args", ".", "do_train", "and", "not", "args", ".", "do_eval", ":", "\n", "        ", "raise", "ValueError", "(", "\"At least one of `do_train` or `do_eval` must be True.\"", ")", "\n", "", "if", "os", ".", "path", ".", "exists", "(", "args", ".", "output_dir", ")", "and", "os", ".", "listdir", "(", "args", ".", "output_dir", ")", "and", "args", ".", "do_train", ":", "\n", "        ", "logger", ".", "info", "(", "f\"{args.output_dir} already exists. It will be deleted...\"", ")", "\n", "shutil", ".", "rmtree", "(", "args", ".", "output_dir", ")", "# \u5982\u679c ", "\n", "#raise ValueError(\"Output directory ({}) already exists and is not empty.\".format(args.output_dir))", "\n", "", "if", "not", "os", ".", "path", ".", "exists", "(", "args", ".", "output_dir", ")", ":", "\n", "        ", "os", ".", "makedirs", "(", "args", ".", "output_dir", ")", "\n", "\n", "", "task_name", "=", "args", ".", "task_name", ".", "lower", "(", ")", "\n", "''' \u8fd9\u4e2aprocessors \u5e94\u8be5\u662f\u5bf9\u6570\u636e\u96c6\u8fdb\u884c\u6307\u5b9a\u683c\u5f0f\u7684\u5904\u7406\n    '''", "\n", "if", "task_name", "not", "in", "processors", ":", "\n", "        ", "raise", "ValueError", "(", "\"Task not found: %s\"", "%", "(", "task_name", ")", ")", "\n", "\n", "", "processor", "=", "processors", "[", "task_name", "]", "(", ")", "\n", "# \"X\" \u662f\u4ec0\u4e48\u610f\u601d\uff1f  => \u5982\u679c\u4e00\u4e2a\u5355\u8bcd\u88ab\u5206\u6210\u4e86\u4e24\u4e2atoken\uff0c\u90a3\u4e48\u5c31\u7528x\u6807\u8bb0", "\n", "label_list", "=", "processor", ".", "get_labels", "(", ")", "# [\"O\", \"P\", \"X\", \"[CLS]\", \"[SEP]\"]", "\n", "\n", "# bug \u6e90\u7801\u540e\u9762\u6709\u4e00\u4e2a+1\uff0c\u6211\u8fd9\u91cc\u5c06\u5176\u53bb\u6389\u4e86", "\n", "num_labels", "=", "len", "(", "label_list", ")", "\n", "\n", "tokenizer", "=", "BertTokenizer", ".", "from_pretrained", "(", "args", ".", "bert_model", ",", "do_lower_case", "=", "args", ".", "do_lower_case", ")", "\n", "\n", "train_examples", "=", "None", "\n", "num_train_optimization_steps", "=", "None", "\n", "'''\n    01.get_train_examples(args.data_dir) \u8fd9\u4e2a\u662f\u628a \u53d1\u97f3embedding \u4e5f\u5199\u5230\u4e86 data example\u4e2d    \n    02. \u8fd9\u91cc\u7684 arg.data_dir \u5176\u5b9e\u662ftrain.txt \u6587\u4ef6\u3002 \u91cc\u9762\u6bcf\u884c\u7684\u5185\u5bb9\u5982\u4e0b\uff1a\n    \u5355\u8bcd \u6807\u8bb0 [\u53d1\u97f3\u5411\u91cf]  => \u4e0b\u9762\u7ed9\u51fa\u793a\u4f8b    \n    ...\n    to O T,UW1\n    a O AH0\n    sting P S,T,IH1,NG\n    operation O AA2,P,ER0,EY1,SH,AH0,N\n    ? O PUNCTUATION_?\n    ...\n\n    \u5176\u5b9e\u8fd9\u4e2a\u5c06\u5355\u8bcd\u6620\u5c04\u6210\u53d1\u97f3\u5411\u91cf\u7684 \u6b65\u9aa4\u662f\u5f88\u5173\u952e\u7684\u3002\u5426\u5219\u4e0d\u77e5\u9053\u600e\u4e48\u5c06\u5411\u91cf\u62fc\u63a5\u3002\n    '''", "\n", "all_examples", "=", "processor", ".", "get_train_examples", "(", "args", ".", "data_dir", ")", "\n", "all_examples", "=", "np", ".", "array", "(", "all_examples", ")", "\n", "sense_path", "=", "\"./data/defi_emb_30.txt\"", "\n", "wordEmb", "=", "getAllWordSenseEmb", "(", "sense_path", ")", "# \u5f97\u5230\u5355\u8bcdsense \u7684embedding", "\n", "\n", "kf", "=", "KFold", "(", "n_splits", "=", "10", ")", "# \u5206\u527210\u4efd", "\n", "kf", ".", "get_n_splits", "(", "all_examples", ")", "# \uff1f\uff1f\uff1f \u8fd9\u4e2a\u529f\u80fd\u662f\uff1f => \u611f\u89c9\u50cf\u662f\u4ec0\u4e48\u90fd\u6ca1\u6709\u505a", "\n", "\n", "\n", "'''\n    \u4e0b\u9762\u5c31\u662f\u4f7f\u7528\u4ea4\u53c9\u9a8c\u8bc1\u5c06\u6240\u6709\u7684\u6570\u636e\u5206\u6210 train \u548c test\uff0c \u7136\u540e\u8fdb\u884c\u6570\u636e\u5206\u5272\n    '''", "\n", "cv_index", "=", "-", "1", "# \u4ec0\u4e48\u542b\u4e49\uff1f => cross validation index", "\n", "from", "transformers", "import", "BertModel", "\n", "from", "transformers", "import", "AutoTokenizer", "# \u5f15\u5165\u4e00\u4e2a\u5305", "\n", "auto_tokenizer", "=", "AutoTokenizer", ".", "from_pretrained", "(", "args", ".", "bert_model", ")", "\n", "sense_bert", "=", "BertModel", ".", "from_pretrained", "(", "\"/home/lawson/pretrain/bert-base-cased\"", ")", "\n", "sense_bert", ".", "to", "(", "device", ")", "# \u653e\u5165\u5230gpu\u4e2d", "\n", "for", "train_index", ",", "test_index", "in", "kf", ".", "split", "(", "all_examples", ")", ":", "\n", "        ", "cv_index", "+=", "1", "\n", "train_examples", "=", "list", "(", "all_examples", "[", "train_index", "]", ")", "\n", "eval_examples", "=", "list", "(", "all_examples", "[", "test_index", "]", ")", "# \u53ef\u4ee5\u76f4\u63a5\u5728numpy\u6570\u7ec4\u4e2d\u518d\u5957\u4e00\u4e2a\u6570\u7ec4\uff0c\u7136\u540e\u5c31\u5f97\u5230\u6570\u7ec4\u7684\u503c", "\n", "\n", "num_train_optimization_steps", "=", "int", "(", "\n", "len", "(", "train_examples", ")", "/", "args", ".", "train_batch_size", "/", "args", ".", "gradient_accumulation_steps", ")", "*", "args", ".", "num_train_epochs", "\n", "if", "args", ".", "local_rank", "!=", "-", "1", ":", "\n", "            ", "num_train_optimization_steps", "=", "num_train_optimization_steps", "//", "torch", ".", "distributed", ".", "get_world_size", "(", ")", "\n", "\n", "# Prepare model", "\n", "", "cache_dir", "=", "args", ".", "cache_dir", "if", "args", ".", "cache_dir", "else", "os", ".", "path", ".", "join", "(", "str", "(", "PYTORCH_PRETRAINED_BERT_CACHE", ")", ",", "'distributed_{}'", ".", "format", "(", "args", ".", "local_rank", ")", ")", "\n", "model", "=", "BertForTokenPronsClassification_v2", ".", "from_pretrained", "(", "args", ".", "bert_model", ",", "\n", "cache_dir", "=", "cache_dir", ",", "\n", "num_labels", "=", "num_labels", ",", "\n", "max_seq_length", "=", "args", ".", "max_seq_length", ",", "\n", "max_prons_length", "=", "args", ".", "max_pron_length", ",", "\n", "pron_emb_size", "=", "args", ".", "pron_emb_size", ",", "# 16", "\n", "do_pron", "=", "args", ".", "do_pron", ",", "\n", "use_sense", "=", "args", ".", "use_sense", ",", "# \u662f\u5426\u4f7f\u7528sense", "\n", "defi_num", "=", "args", ".", "defi_num", ")", "\n", "\n", "if", "args", ".", "fp16", ":", "\n", "            ", "model", ".", "half", "(", ")", "\n", "", "model", ".", "to", "(", "device", ")", "\n", "if", "args", ".", "local_rank", "!=", "-", "1", ":", "\n", "            ", "try", ":", "\n", "                ", "from", "apex", ".", "parallel", "import", "DistributedDataParallel", "as", "DDP", "\n", "", "except", "ImportError", ":", "\n", "                ", "raise", "ImportError", "(", "\"Please install apex from https://www.github.com/nvidia/apex to use distributed and fp16 training.\"", ")", "\n", "\n", "", "model", "=", "DDP", "(", "model", ")", "\n", "", "elif", "n_gpu", ">", "1", ":", "\n", "            ", "model", "=", "torch", ".", "nn", ".", "DataParallel", "(", "model", ")", "\n", "\n", "", "param_optimizer", "=", "list", "(", "model", ".", "named_parameters", "(", ")", ")", "\n", "no_decay", "=", "[", "'bias'", ",", "'LayerNorm.bias'", ",", "'LayerNorm.weight'", "]", "\n", "optimizer_grouped_parameters", "=", "[", "\n", "{", "'params'", ":", "[", "p", "for", "n", ",", "p", "in", "param_optimizer", "if", "not", "any", "(", "nd", "in", "n", "for", "nd", "in", "no_decay", ")", "]", ",", "'weight_decay'", ":", "0.01", "}", ",", "\n", "{", "'params'", ":", "[", "p", "for", "n", ",", "p", "in", "param_optimizer", "if", "any", "(", "nd", "in", "n", "for", "nd", "in", "no_decay", ")", "]", ",", "'weight_decay'", ":", "0.0", "}", "\n", "]", "\n", "if", "args", ".", "fp16", ":", "\n", "            ", "try", ":", "\n", "                ", "from", "apex", ".", "optimizers", "import", "FP16_Optimizer", "\n", "from", "apex", ".", "optimizers", "import", "FusedAdam", "\n", "", "except", "ImportError", ":", "\n", "                ", "raise", "ImportError", "(", "\"Please install apex from https://www.github.com/nvidia/apex to use distributed and fp16 training.\"", ")", "\n", "\n", "", "optimizer", "=", "FusedAdam", "(", "optimizer_grouped_parameters", ",", "\n", "lr", "=", "args", ".", "learning_rate", ",", "\n", "bias_correction", "=", "False", ",", "\n", "max_grad_norm", "=", "1.0", ")", "\n", "if", "args", ".", "loss_scale", "==", "0", ":", "\n", "                ", "optimizer", "=", "FP16_Optimizer", "(", "optimizer", ",", "dynamic_loss_scale", "=", "True", ")", "\n", "", "else", ":", "\n", "                ", "optimizer", "=", "FP16_Optimizer", "(", "optimizer", ",", "static_loss_scale", "=", "args", ".", "loss_scale", ")", "\n", "\n", "", "", "else", ":", "\n", "            ", "optimizer", "=", "BertAdam", "(", "optimizer_grouped_parameters", ",", "\n", "lr", "=", "args", ".", "learning_rate", ",", "\n", "warmup", "=", "args", ".", "warmup_proportion", ",", "\n", "t_total", "=", "num_train_optimization_steps", ")", "\n", "\n", "", "global_step", "=", "0", "\n", "nb_tr_steps", "=", "0", "\n", "tr_loss", "=", "0", "\n", "\n", "# load pretrained embeddings for phonemes", "\n", "prons_map", "=", "{", "}", "\n", "prons_map", ",", "prons_emb", "=", "embed_load", "(", "'./data/pron.'", "+", "str", "(", "args", ".", "pron_emb_size", ")", "+", "'.vec'", ")", "\n", "\n", "# convert texts to trainable features", "\n", "train_features", ",", "prons_map", "=", "convert_examples_to_pron_features", "(", "\n", "train_examples", ",", "\n", "label_list", ",", "\n", "args", ".", "max_seq_length", ",", "\n", "args", ".", "max_pron_length", ",", "\n", "tokenizer", ",", "\n", "prons_map", ",", "\n", "logger", ")", "\n", "eval_features", ",", "prons_map", "=", "convert_examples_to_pron_features", "(", "\n", "eval_examples", ",", "\n", "label_list", ",", "\n", "args", ".", "max_seq_length", ",", "\n", "args", ".", "max_pron_length", ",", "\n", "tokenizer", ",", "\n", "prons_map", ",", "\n", "logger", ")", "\n", "prons_emb", "=", "embed_extend", "(", "prons_emb", ",", "len", "(", "prons_map", ")", ")", "\n", "prons_emb", "=", "torch", ".", "tensor", "(", "prons_emb", ",", "dtype", "=", "torch", ".", "float", ")", "\n", "# \u6839\u636e tensor \u521b\u5efa\u4e00\u4e2a Embedding \u5b9e\u4f8b", "\n", "prons_embedding", "=", "torch", ".", "nn", ".", "Embedding", ".", "from_pretrained", "(", "prons_emb", ")", "\n", "prons_embedding", ".", "weight", ".", "requires_grad", "=", "False", "\n", "\n", "# build training set", "\n", "logger", ".", "info", "(", "\"***** Training Parameters *****\"", ")", "\n", "logger", ".", "info", "(", "\"  Num examples = %d\"", ",", "len", "(", "train_examples", ")", ")", "\n", "logger", ".", "info", "(", "\"  Batch size = %d\"", ",", "args", ".", "train_batch_size", ")", "\n", "logger", ".", "info", "(", "\"  Num steps = %d\"", ",", "num_train_optimization_steps", ")", "# \u5bf9\u8fd9\u4e2a\u53c2\u6570\u4e0d\u4e86\u89e3", "\n", "\n", "# \u4e3a\u4ec0\u4e48\u8fd9\u91cc\u628a\u6570\u5b57\u5168\u90e8\u90fd\u63d0\u53d6\u51fa\u6765\u4e86\uff1f =>  \u4f7f\u7528TensorDataset \u65b9\u4fbf\u5c01\u88c5 ", "\n", "all_input_ids", "=", "torch", ".", "tensor", "(", "[", "f", ".", "input_ids", "for", "f", "in", "train_features", "]", ",", "dtype", "=", "torch", ".", "long", ")", "# size [1446,max_seq_length]", "\n", "all_input_mask", "=", "torch", ".", "tensor", "(", "[", "f", ".", "input_mask", "for", "f", "in", "train_features", "]", ",", "dtype", "=", "torch", ".", "long", ")", "\n", "all_segment_ids", "=", "torch", ".", "tensor", "(", "[", "f", ".", "segment_ids", "for", "f", "in", "train_features", "]", ",", "dtype", "=", "torch", ".", "long", ")", "\n", "all_label_ids", "=", "torch", ".", "tensor", "(", "[", "f", ".", "label_id", "for", "f", "in", "train_features", "]", ",", "dtype", "=", "torch", ".", "long", ")", "\n", "all_prons_ids", "=", "torch", ".", "tensor", "(", "[", "f", ".", "prons_id", "for", "f", "in", "train_features", "]", ",", "dtype", "=", "torch", ".", "long", ")", "\n", "all_prons_att_mask", "=", "torch", ".", "tensor", "(", "[", "f", ".", "prons_att_mask", "for", "f", "in", "train_features", "]", ",", "dtype", "=", "torch", ".", "long", ")", "\n", "#train_all_sense = getSenseEmbedding(all_input_ids,args.bert_model) # train_sense_emb ", "\n", "train_data", "=", "TensorDataset", "(", "all_input_ids", ",", "\n", "all_input_mask", ",", "\n", "all_segment_ids", ",", "\n", "all_label_ids", ",", "\n", "all_prons_ids", ",", "\n", "all_prons_att_mask", ")", "\n", "\n", "# \u8fd9\u4e2a\u91c7\u6837\u5668\u9700\u8981\u5b66\u4e60\u4e00\u4e0b", "\n", "if", "args", ".", "local_rank", "==", "-", "1", ":", "\n", "            ", "train_sampler", "=", "RandomSampler", "(", "train_data", ")", "\n", "", "else", ":", "\n", "            ", "train_sampler", "=", "DistributedSampler", "(", "train_data", ")", "\n", "", "train_dataloader", "=", "DataLoader", "(", "train_data", ",", "sampler", "=", "train_sampler", ",", "batch_size", "=", "args", ".", "train_batch_size", ")", "\n", "\n", "# build test set", "\n", "logger", ".", "info", "(", "\"***** Evaluation Parameters *****\"", ")", "\n", "logger", ".", "info", "(", "\"  Num examples = %d\"", ",", "len", "(", "eval_examples", ")", ")", "\n", "logger", ".", "info", "(", "\"  Batch size = %d\"", ",", "args", ".", "eval_batch_size", ")", "\n", "all_input_ids", "=", "torch", ".", "tensor", "(", "[", "f", ".", "input_ids", "for", "f", "in", "eval_features", "]", ",", "dtype", "=", "torch", ".", "long", ")", "\n", "all_input_mask", "=", "torch", ".", "tensor", "(", "[", "f", ".", "input_mask", "for", "f", "in", "eval_features", "]", ",", "dtype", "=", "torch", ".", "long", ")", "\n", "all_segment_ids", "=", "torch", ".", "tensor", "(", "[", "f", ".", "segment_ids", "for", "f", "in", "eval_features", "]", ",", "dtype", "=", "torch", ".", "long", ")", "\n", "all_label_ids", "=", "torch", ".", "tensor", "(", "[", "f", ".", "label_id", "for", "f", "in", "eval_features", "]", ",", "dtype", "=", "torch", ".", "long", ")", "\n", "all_prons_ids", "=", "torch", ".", "tensor", "(", "[", "f", ".", "prons_id", "for", "f", "in", "eval_features", "]", ",", "dtype", "=", "torch", ".", "long", ")", "\n", "all_prons_att_mask", "=", "torch", ".", "tensor", "(", "[", "f", ".", "prons_att_mask", "for", "f", "in", "eval_features", "]", ",", "dtype", "=", "torch", ".", "long", ")", "\n", "\n", "# \u6839\u636e all_input_ids \u53d8\u6362\u5f97\u5230 \u5bf9\u5e94\u7684 sense_embedding", "\n", "\n", "eval_data", "=", "TensorDataset", "(", "all_input_ids", ",", "\n", "all_input_mask", ",", "\n", "all_segment_ids", ",", "\n", "all_label_ids", ",", "\n", "all_prons_ids", ",", "\n", "all_prons_att_mask", ",", "\n", ")", "\n", "# Run prediction for full data", "\n", "eval_sampler", "=", "SequentialSampler", "(", "eval_data", ")", "\n", "eval_dataloader", "=", "DataLoader", "(", "eval_data", ",", "sampler", "=", "eval_sampler", ",", "batch_size", "=", "args", ".", "eval_batch_size", ")", "\n", "# \u603b\u7ed3\u4e00\u4e0b\uff1a \u4e0d\u540c\u7684train ,eval \u6b65\u9aa4\u90fd\u662f\u9700\u8981\u4e0d\u540c\u7684TensoDataset \u548c Dataloader ", "\n", "model", ".", "train", "(", ")", "\n", "best_score", "=", "0", "\n", "\n", "'''\u539f\u5148\u662f\u4ece1 \u5f00\u59cb\u7684\u5b57\u5178\uff0c\u56e0\u4e3a\u8fd9\u4e2a\u4f1a\u5bfc\u81f4\u540e\u9762 456\u884c\u7684\u4ee3\u7801\u51fa\u73b0\u9519\u8bef\n        temp_2.append(label_map[logits[i][j]]) \n        \u9519\u8bef\u7684\u539f\u56e0\u662f logits[i][j] \u4f1a\u51fa\u73b00\uff0c\u4ece\u800c\u5bfc\u81f4\u8fd9\u79cd\u9519\u8bef\n        '''", "\n", "label_map", "=", "{", "i", ":", "label", "for", "i", ",", "label", "in", "enumerate", "(", "label_list", ",", "0", ")", "}", "\n", "id_2_key_map", "=", "get_word_key_id_2_map", "(", "keyPath", "=", "\"/home/lawson/program/punLocation/data/key.txt\"", ")", "\n", "# start cross-validation training", "\n", "logger", ".", "info", "(", "\"cv: {}\"", ".", "format", "(", "cv_index", ")", ")", "\n", "for", "index", "in", "trange", "(", "int", "(", "args", ".", "num_train_epochs", ")", ",", "desc", "=", "\"Train Epoch\"", ")", ":", "\n", "            ", "tr_loss", "=", "0", "# train loss", "\n", "nb_tr_examples", ",", "nb_tr_steps", "=", "0", ",", "0", "\n", "y_true", ",", "y_pred", "=", "[", "]", ",", "[", "]", "\n", "logger", ".", "info", "(", "\"\\n\\n----------------------------Start Training ----------------------------\"", ")", "\n", "for", "step", ",", "batch", "in", "enumerate", "(", "tqdm", "(", "train_dataloader", ",", "desc", "=", "\"Train Iteration\"", ")", ")", ":", "\n", "                ", "batch", "=", "tuple", "(", "t", ".", "to", "(", "device", ")", "for", "t", "in", "batch", ")", "\n", "input_ids", ",", "input_mask", ",", "segment_ids", ",", "label_ids", ",", "prons_ids", ",", "prons_att_mask", "=", "batch", "\n", "# print(\"\\n\",input_ids.size()) # torch.size[batch_size,max_seq_length]  ", "\n", "prons_emb", "=", "prons_embedding", "(", "prons_ids", ".", "detach", "(", ")", ".", "cpu", "(", ")", ")", ".", "to", "(", "device", ")", "\n", "\n", "defi_emb", "=", "None", "# \u5b58\u50a8\u4e00\u6279pun\u5f97\u5230\u7684sense embedding", "\n", "if", "args", ".", "use_sense", ":", "# \u5982\u679c\u9700\u8981\u4f7f\u7528sense embedding", "\n", "                    ", "for", "input_id", "in", "input_ids", ":", "\n", "                        ", "tokens", "=", "auto_tokenizer", ".", "convert_ids_to_tokens", "(", "input_id", ")", "\n", "cur_pun_emb", "=", "getPunEmb", "(", "wordEmb", ",", "tokens", ",", "args", ".", "defi_num", ",", "args", ".", "use_random", ")", "\n", "# print(cur_pun_emb.size())", "\n", "# size = [word_num * defi_num, defi_dim]", "\n", "cur_pun_emb", "=", "cur_pun_emb", ".", "view", "(", "args", ".", "max_seq_length", ",", "args", ".", "defi_num", ",", "768", ")", "\n", "if", "defi_emb", "is", "None", ":", "\n", "                            ", "defi_emb", "=", "cur_pun_emb", "\n", "", "else", ":", "\n", "                            ", "defi_emb", "=", "torch", ".", "cat", "(", "(", "defi_emb", ",", "cur_pun_emb", ")", ",", "0", ")", "\n", "# defi_emb \u7684size \u662f [batch_size,max_seq_length,defi_num,768]", "\n", "\n", "", "", "defi_emb", "=", "defi_emb", ".", "cuda", "(", ")", "\n", "", "if", "not", "args", ".", "do_pron", ":", "\n", "                    ", "prons_emb", "=", "None", "\n", "\n", "# \u5f00\u59cb\u6267\u884c model \u7528\u4e8e\u8bad\u7ec3                ", "\n", "", "loss", ",", "logits", "=", "model", "(", "input_ids", ",", "segment_ids", ",", "input_mask", ",", "prons_emb", ",", "prons_att_mask", ",", "label_ids", ",", "defi_emb", ")", "\n", "if", "n_gpu", ">", "1", ":", "\n", "                    ", "loss", "=", "loss", ".", "mean", "(", ")", "# mean() to average on multi-gpu.", "\n", "", "if", "args", ".", "gradient_accumulation_steps", ">", "1", ":", "\n", "                    ", "loss", "=", "loss", "/", "args", ".", "gradient_accumulation_steps", "\n", "\n", "", "if", "args", ".", "fp16", ":", "\n", "                    ", "optimizer", ".", "backward", "(", "loss", ")", "\n", "", "else", ":", "\n", "                    ", "loss", ".", "backward", "(", ")", "\n", "\n", "", "tr_loss", "+=", "loss", ".", "item", "(", ")", "\n", "nb_tr_examples", "+=", "input_ids", ".", "size", "(", "0", ")", "\n", "nb_tr_steps", "+=", "1", "\n", "if", "(", "step", "+", "1", ")", "%", "args", ".", "gradient_accumulation_steps", "==", "0", ":", "# \u8fd9\u91cc\u7adf\u7136\u8fd8\u8bbe\u7f6e\u4e86\u4e00\u4e2a\u5bf9\u68af\u5ea6\u7d2f\u79ef\u66f4\u65b0\u7684\u6b65\u9aa4", "\n", "                    ", "optimizer", ".", "step", "(", ")", "\n", "optimizer", ".", "zero_grad", "(", ")", "\n", "global_step", "+=", "1", "\n", "\n", "", "logits", "=", "torch", ".", "argmax", "(", "F", ".", "log_softmax", "(", "logits", ",", "dim", "=", "2", ")", ",", "dim", "=", "2", ")", "\n", "logits", "=", "logits", ".", "detach", "(", ")", ".", "cpu", "(", ")", ".", "numpy", "(", ")", "\n", "label_ids", "=", "label_ids", ".", "to", "(", "'cpu'", ")", ".", "numpy", "(", ")", "\n", "input_mask", "=", "input_mask", ".", "to", "(", "'cpu'", ")", ".", "numpy", "(", ")", "\n", "for", "i", ",", "mask", "in", "enumerate", "(", "input_mask", ")", ":", "\n", "                    ", "temp_1", "=", "[", "]", "\n", "temp_2", "=", "[", "]", "\n", "for", "j", ",", "m", "in", "enumerate", "(", "mask", ")", ":", "# \u53ea\u505a\u6709\u6587\u672c\u5185\u5bb9\u7684\u90a3\u90e8\u5206", "\n", "                        ", "if", "j", "==", "0", ":", "\n", "                            ", "continue", "\n", "", "else", ":", "\n", "                            ", "temp_1", ".", "append", "(", "label_map", "[", "label_ids", "[", "i", "]", "[", "j", "]", "]", ")", "\n", "temp_2", ".", "append", "(", "label_map", "[", "logits", "[", "i", "]", "[", "j", "]", "]", ")", "\n", "\n", "# \u5982\u679c\u662f\u5230pad\u90e8\u5206 \u6216\u8005 \u6700\u540e\u7684\u90e8\u5206\u4e86\uff0c\u90a3\u4e48\u5c31break", "\n", "# \u6700\u540e\u7684\u90e8\u5206\uff0c\u8bf4\u660e\u53c2\u6570 max_seq_length \u8bbe\u7f6e\u7684\u4e0d\u591f\u5927", "\n", "", "if", "m", "==", "0", "or", "j", "+", "1", "==", "len", "(", "mask", ")", ":", "\n", "                            ", "y_true", ".", "append", "(", "temp_1", ")", "\n", "y_pred", ".", "append", "(", "temp_2", ")", "\n", "break", "\n", "\n", "", "", "", "", "report", "=", "classification_report", "(", "y_true", ",", "y_pred", ",", "digits", "=", "4", ")", "\n", "logger", ".", "info", "(", "\"\\n%s\"", ",", "report", ")", "# \u8fd9\u91cc\u7684\u6362\u884c\u662f\u56e0\u4e3a\uff1a\u5982\u679c\u4e0d\u6362\u884c\uff0c\u5c31\u4f1a\u548c\u4e0a\u9762\u7684tqdm\u8f93\u51fa\u6df7\u4e00\u8d77\u4e86\u3002\u4e0b\u540c", "\n", "logger", ".", "info", "(", "\"loss: {}\"", ".", "format", "(", "tr_loss", "/", "nb_tr_examples", ")", ")", "\n", "\n", "'''\n            evaluation \u7684\u65f6\u5019\uff0c\u5c06\u6e90txt\u6587\u4ef6\u548c label \u4e00\u8d77\u5199\u5165\u5230\u6587\u4ef6\u4e2d\uff0c\u8fd9\u6837\u5c31\u65b9\u4fbf\u5bf9\u7167\u9605\u8bfb\n            '''", "\n", "y_pred", ",", "y_true", "=", "[", "]", ",", "[", "]", "# \u7528\u4e8e\u4fdd\u5b58\u9884\u6d4b\u548c\u771f\u5b9e\u7684 label", "\n", "all_tokens", "=", "[", "]", "# \u7528\u4e8e\u4fdd\u5b58\u6240\u6709\u7684tokens\uff0c\u7136\u540e\u5199\u5165\u5230\u6700\u540e\u7684\u6587\u4ef6\u4e2d", "\n", "logger", ".", "info", "(", "\"\\n\\n----------------------------Start Evaluating----------------------------\"", ")", "\n", "for", "input_ids", ",", "input_mask", ",", "segment_ids", ",", "label_ids", ",", "prons_ids", ",", "prons_att_mask", "in", "tqdm", "(", "eval_dataloader", ",", "desc", "=", "\"Evaluating Iterator\"", ")", ":", "\n", "                ", "prons_emb", "=", "prons_embedding", "(", "prons_ids", ")", ".", "to", "(", "device", ")", "\n", "input_ids", "=", "input_ids", ".", "to", "(", "device", ")", "\n", "input_mask", "=", "input_mask", ".", "to", "(", "device", ")", "\n", "segment_ids", "=", "segment_ids", ".", "to", "(", "device", ")", "\n", "label_ids", "=", "label_ids", ".", "to", "(", "device", ")", "\n", "prons_ids", "=", "prons_ids", ".", "to", "(", "device", ")", "\n", "prons_att_mask", "=", "prons_att_mask", ".", "to", "(", "device", ")", "\n", "\n", "eval_defi_emb", "=", "None", "# \u5b58\u50a8\u4e00\u6279pun\u5f97\u5230\u7684sense embedding          ", "\n", "if", "args", ".", "use_sense", ":", "\n", "                    ", "for", "input_id", "in", "input_ids", ":", "\n", "                        ", "tokens", "=", "auto_tokenizer", ".", "convert_ids_to_tokens", "(", "input_id", ")", "\n", "all_tokens", ".", "append", "(", "tokens", ")", "\n", "cur_pun_emb", "=", "getPunEmb", "(", "wordEmb", ",", "tokens", ",", "args", ".", "defi_num", ",", "args", ".", "use_random", ")", "\n", "#print(cur_pun_emb.size())", "\n", "# size = [word_num * defi_num, defi_dim]", "\n", "cur_pun_emb", "=", "cur_pun_emb", ".", "view", "(", "args", ".", "max_seq_length", ",", "args", ".", "defi_num", ",", "768", ")", "\n", "if", "eval_defi_emb", "is", "None", ":", "\n", "                            ", "eval_defi_emb", "=", "cur_pun_emb", "\n", "", "else", ":", "\n", "                            ", "eval_defi_emb", "=", "torch", ".", "cat", "(", "(", "eval_defi_emb", ",", "cur_pun_emb", ")", ",", "0", ")", "\n", "# defi_emb \u7684size \u662f [batch_size,max_seq_length,defi_num,768]", "\n", "", "", "eval_defi_emb", "=", "eval_defi_emb", ".", "cuda", "(", ")", "\n", "", "else", ":", "# \u4f9d\u7136\u9700\u8981\u627e\u51fa\u539f\u6765\u7684tokens", "\n", "                    ", "for", "input_id", "in", "input_ids", ":", "\n", "                        ", "tokens", "=", "auto_tokenizer", ".", "convert_ids_to_tokens", "(", "input_id", ")", "\n", "all_tokens", ".", "append", "(", "tokens", ")", "\n", "", "", "if", "not", "args", ".", "do_pron", ":", "prons_emb", "=", "None", "\n", "\n", "with", "torch", ".", "no_grad", "(", ")", ":", "# \u4e0d\u8ba1\u7b97\u68af\u5ea6", "\n", "                    ", "logits", ",", "att_pron", ",", "att_defi", "=", "model", "(", "input_ids", ",", "segment_ids", ",", "input_mask", ",", "prons_emb", ",", "prons_att_mask", ",", "defi_emb", "=", "eval_defi_emb", ")", "\n", "\n", "", "logits", "=", "torch", ".", "argmax", "(", "F", ".", "log_softmax", "(", "logits", ",", "dim", "=", "2", ")", ",", "dim", "=", "2", ")", "\n", "logits", "=", "logits", ".", "detach", "(", ")", ".", "cpu", "(", ")", ".", "numpy", "(", ")", "\n", "label_ids", "=", "label_ids", ".", "to", "(", "'cpu'", ")", ".", "numpy", "(", ")", "\n", "input_mask", "=", "input_mask", ".", "to", "(", "'cpu'", ")", ".", "numpy", "(", ")", "\n", "\n", "for", "i", ",", "mask", "in", "enumerate", "(", "input_mask", ")", ":", "\n", "                    ", "tokens", "=", "tokenizer", ".", "convert_ids_to_tokens", "(", "input_ids", "[", "i", "]", ".", "tolist", "(", ")", ")", "\n", "# \u8fc7\u6ee4\u6389[PAD] \u548c [CLS]", "\n", "tokens", "=", "[", "i", "for", "i", "in", "tokens", "if", "i", "!=", "'[PAD]'", "and", "i", "!=", "'[CLS]'", "]", "\n", "logger", ".", "info", "(", "f\"\u5f53\u524d\u6587\u672c\u4fe1\u606f\u662f\uff1a{tokens}\"", ")", "\n", "temp_1", "=", "[", "]", "# \u4f5c\u4e3a\u4e34\u65f6\u7684 true", "\n", "temp_2", "=", "[", "]", "# \u4f5c\u4e3a\u4e34\u65f6\u7684 pred", "\n", "for", "j", ",", "m", "in", "enumerate", "(", "mask", ")", ":", "# \u53ea\u8981 mask \u4e3a1 \u7684\u6570\u636e", "\n", "                        ", "if", "j", "==", "0", ":", "# \u8df3\u8fc7CLS \u6807\u5fd7", "\n", "                            ", "continue", "\n", "", "else", ":", "\n", "# \u5e76\u4e0d\u9700\u8981pop\u64cd\u4f5c\uff0c\u76f4\u63a5\u8bb0\u5f55\u6240\u6709\u7684tokens", "\n", "# temp_1.pop()", "\n", "# temp_2.pop()", "\n", "                            ", "temp_1", ".", "append", "(", "label_map", "[", "label_ids", "[", "i", "]", "[", "j", "]", "]", ")", "\n", "temp_2", ".", "append", "(", "label_map", "[", "logits", "[", "i", "]", "[", "j", "]", "]", ")", "\n", "\n", "", "if", "m", "==", "0", "or", "j", "+", "1", "==", "len", "(", "mask", ")", ":", "\n", "                            ", "temp_1", ".", "pop", "(", ")", "# pop() \u662f\u4e3a\u4e86\u6392\u9664SEP \u4e4b\u540e\u7684O", "\n", "temp_2", ".", "pop", "(", ")", "\n", "y_true", ".", "append", "(", "temp_1", ")", "\n", "y_pred", ".", "append", "(", "temp_2", ")", "\n", "# \u4ecepred \u4e2d\u627e\u51fap\u7684\u6807\u5fd7\u4f4d\uff0c\u540c\u65f6\u53bb\u83b7\u53d6 att_defi \u7684\u503c                            ", "\n", "max_value_index", "=", "[", "i", "for", "i", "in", "range", "(", "len", "(", "temp_2", ")", ")", "if", "temp_2", "[", "i", "]", "==", "'P'", "]", "\n", "\n", "if", "args", ".", "use_sense", "and", "len", "(", "max_value_index", ")", "==", "1", ":", "# \u7ed9\u51fa\u5b8c\u7f8e\u9884\u6d4b\u7684sense score weight", "\n", "                                ", "max_value_index", "=", "max_value_index", "[", "0", "]", "+", "1", "# \u9ed8\u8ba4\u53d6\u7b2c0\u4f4d\uff0c\u56e0\u4e3a\u4e4b\u524d\u6709CLS\u5411\u91cf\uff0c\u6240\u4ee5\u8fd9\u91cc\u6709\u4e2a+1\u64cd\u4f5c", "\n", "pred_pun_word", "=", "tokenizer", ".", "convert_ids_to_tokens", "(", "[", "input_ids", "[", "i", "]", "[", "max_value_index", "]", ".", "item", "(", ")", "]", ")", "[", "0", "]", "# \u5f97\u5230\u9884\u6d4b\u7684pun word", "\n", "# \u627e\u51fa\u8be5\u4f4d\u7684attention\u503c ", "\n", "# size = [defi_num]", "\n", "sense_aware", "=", "att_defi", "[", "i", "]", "[", "max_value_index", "]", "\n", "# \u4ecesense_aware \u4e2d\u627e\u51fa\u4e0e\u5176\u6700\u76f8\u5173\u7684top-k\u4e2a\u503c", "\n", "ind_val_dict", "=", "{", "}", "\n", "for", "k", "in", "range", "(", "len", "(", "sense_aware", ")", ")", ":", "\n", "                                    ", "ind_val_dict", "[", "k", "]", "=", "sense_aware", "[", "k", "]", "\n", "", "re", "=", "list", "(", "sorted", "(", "ind_val_dict", ".", "items", "(", ")", ",", "key", "=", "lambda", "x", ":", "x", "[", "1", "]", ",", "reverse", "=", "True", ")", ")", "\n", "top_k", "=", "5", "# \u53d6\u524d5\u4e2a", "\n", "logger", ".", "info", "(", "f\"\u5f53\u524d\u9884\u6d4b\u5f97\u5230\u7684\u53cc\u5173\u8bcd\u662f:{pred_pun_word}\"", ")", "\n", "\n", "for", "m", "in", "range", "(", "top_k", ")", ":", "\n", "                                    ", "index", ",", "weight", "=", "re", "[", "m", "]", "\n", "logger", ".", "info", "(", "f\"index = {index}, weight = {weight},\"", ")", "# \u540c\u65f6\u6253\u5370\u51fakey \u4fe1\u606f", "\n", "cur_key", "=", "pred_pun_word", "+", "\"_\"", "+", "str", "(", "index", ")", "\n", "if", "cur_key", "in", "id_2_key_map", ".", "keys", "(", ")", ":", "\n", "                                        ", "key_list", "=", "id_2_key_map", "[", "cur_key", "]", "\n", "logger", ".", "info", "(", "key_list", ")", "\n", "", "", "", "break", "\n", "\n", "# \u662f\u6240\u6709\u7684eval data \u5b8c\u4e4b\u540e\uff0c\u624d\u6709\u8fd9\u4e2a\u64cd\u4f5c\uff0c\u8bf4\u660ey_true\u662f\u6240\u6709\u7684batch \u6570\u636e\u5f97\u5230\u7684\u7ed3\u679c", "\n", "", "", "", "", "report", "=", "classification_report", "(", "y_true", ",", "y_pred", ",", "digits", "=", "4", ")", "\n", "logger", ".", "info", "(", "\"\\n%s\"", ",", "report", ")", "\n", "f1_new", "=", "f1_score", "(", "y_true", ",", "y_pred", ")", "\n", "\n", "if", "f1_new", ">", "best_score", ":", "\n", "                ", "best_score", "=", "f1_new", "\n", "# \u8fd9\u91cc\u7684 score_file\u8868\u793a\u7684\u662f\u4e00\u4e2a\u6587\u4ef6\u540d", "\n", "write_scores", "(", "score_file", "+", "'true_'", "+", "str", "(", "cv_index", ")", ",", "y_true", ")", "# \u6700\u540e\u5f97\u5230\u7684\u6587\u4ef6\u7c7b\u578b\u662f pickle ", "\n", "write_scores", "(", "score_file", "+", "'pred_'", "+", "str", "(", "cv_index", ")", ",", "y_pred", ")", "\n", "\n", "'''\u5c06\u6e90\u6587\u4ef6\u548cgolden label\uff0cpred \u5199\u5728\u4e00\u8d77'''", "\n", "writeToTxt", "(", "all_tokens", ",", "y_true", ",", "y_pred", ",", "score_file", "+", "\"all_\"", "+", "str", "(", "cv_index", ")", ")", "\n", "\n", "# save a trained model and the associated configuration", "\n", "", "", "model_to_save", "=", "model", ".", "module", "if", "hasattr", "(", "model", ",", "'module'", ")", "else", "model", "# Only save the model it-self", "\n", "output_model_file", "=", "os", ".", "path", ".", "join", "(", "args", ".", "output_dir", ",", "WEIGHTS_NAME", "+", "'_'", "+", "str", "(", "cv_index", ")", ")", "\n", "torch", ".", "save", "(", "model_to_save", ".", "state_dict", "(", ")", ",", "output_model_file", ")", "\n", "output_config_file", "=", "os", ".", "path", ".", "join", "(", "args", ".", "output_dir", ",", "CONFIG_NAME", ")", "\n", "with", "open", "(", "output_config_file", ",", "'w'", ")", "as", "f", ":", "\n", "            ", "f", ".", "write", "(", "model_to_save", ".", "config", ".", "to_json_string", "(", ")", ")", "\n", "\n", "# \u4e0d\u7406\u89e3\u4e3a\u4ec0\u4e48\u8fd9\u91cc\u8981\u4ece\u4e0b\u68071 \u5f00\u59cb\u6807\u8bb0 map ", "\n", "", "label_map", "=", "{", "i", ":", "label", "for", "i", ",", "label", "in", "enumerate", "(", "label_list", ",", "1", ")", "}", "\n", "model_config", "=", "{", "\"bert_model\"", ":", "args", ".", "bert_model", ",", "\n", "\"do_lower\"", ":", "args", ".", "do_lower_case", ",", "\n", "\"max_seq_length\"", ":", "args", ".", "max_seq_length", ",", "\n", "\"num_labels\"", ":", "len", "(", "label_list", ")", "+", "1", ",", "\n", "\"label_map\"", ":", "label_map", "\n", "}", "\n", "json", ".", "dump", "(", "model_config", ",", "open", "(", "os", ".", "path", ".", "join", "(", "args", ".", "output_dir", ",", "\"model_config.json\"", ")", ",", "\"w\"", ")", ")", "\n", "# load a trained model and config that you have fine-tuned", "\n"]], "home.repos.pwc.inspect_result.lawsonabs_pun.None.cv_eval_ner.main": [[11, 25], ["int", "range", "seqeval.metrics.classification_report", "open", "pickle.load", "preds.extend", "open", "pickle.load", "truths.extend", "len", "len", "str", "str"], "function", ["None"], ["def", "main", "(", "argv", ")", ":", "\n", "    ", "file_dir", "=", "argv", "[", "1", "]", "\n", "#print(file_dir)", "\n", "cv", "=", "int", "(", "argv", "[", "2", "]", ")", "\n", "preds", ",", "truths", "=", "[", "]", ",", "[", "]", "\n", "for", "i", "in", "range", "(", "cv", ")", ":", "\n", "        ", "with", "open", "(", "file_dir", "+", "\"pred_\"", "+", "str", "(", "i", ")", ",", "'rb'", ")", "as", "f", ":", "\n", "            ", "pred", "=", "pickle", ".", "load", "(", "f", ")", "\n", "preds", ".", "extend", "(", "pred", ")", "\n", "", "with", "open", "(", "file_dir", "+", "\"true_\"", "+", "str", "(", "i", ")", ",", "'rb'", ")", "as", "f", ":", "\n", "            ", "true", "=", "pickle", ".", "load", "(", "f", ")", "\n", "truths", ".", "extend", "(", "true", ")", "\n", "", "assert", "(", "len", "(", "preds", ")", "==", "len", "(", "truths", ")", ")", "\n", "", "return", "classification_report", "(", "truths", ",", "preds", ",", "digits", "=", "4", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.lawsonabs_pun.None.bert.Ner.__init__": [[19, 25], ["bert.Ner.load_model", "bert.Ner.model.eval", "int", "bert.Ner.label_map.items"], "methods", ["home.repos.pwc.inspect_result.lawsonabs_pun.None.bert.Ner.load_model"], ["    ", "def", "__init__", "(", "self", ",", "model_dir", ":", "str", ")", ":", "\n", "        ", "self", ".", "model", ",", "self", ".", "tokenizer", ",", "self", ".", "model_config", "=", "self", ".", "load_model", "(", "model_dir", ")", "\n", "self", ".", "label_map", "=", "self", ".", "model_config", "[", "\"label_map\"", "]", "\n", "self", ".", "max_seq_length", "=", "self", ".", "model_config", "[", "\"max_seq_length\"", "]", "\n", "self", ".", "label_map", "=", "{", "int", "(", "k", ")", ":", "v", "for", "k", ",", "v", "in", "self", ".", "label_map", ".", "items", "(", ")", "}", "\n", "self", ".", "model", ".", "eval", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.lawsonabs_pun.None.bert.Ner.load_model": [[26, 36], ["os.path.join", "json.load", "os.path.join", "os.path.join", "pytorch_pretrained_bert.modeling.BertConfig", "pytorch_pretrained_bert.modeling.BertForTokenClassification", "pytorch_pretrained_bert.modeling.BertForTokenClassification.load_state_dict", "pytorch_pretrained_bert.tokenization.BertTokenizer.from_pretrained", "open", "torch.load", "torch.load", "torch.load", "torch.load"], "methods", ["home.repos.pwc.inspect_result.lawsonabs_pun.None.bert_models.BertPreTrainedModel.from_pretrained"], ["", "def", "load_model", "(", "self", ",", "model_dir", ":", "str", ",", "model_config", ":", "str", "=", "\"model_config.json\"", ")", ":", "\n", "        ", "model_config", "=", "os", ".", "path", ".", "join", "(", "model_dir", ",", "model_config", ")", "\n", "model_config", "=", "json", ".", "load", "(", "open", "(", "model_config", ")", ")", "\n", "output_config_file", "=", "os", ".", "path", ".", "join", "(", "model_dir", ",", "CONFIG_NAME", ")", "\n", "output_model_file", "=", "os", ".", "path", ".", "join", "(", "model_dir", ",", "WEIGHTS_NAME", ")", "\n", "config", "=", "BertConfig", "(", "output_config_file", ")", "\n", "model", "=", "BertForTokenClassification", "(", "config", ",", "num_labels", "=", "model_config", "[", "\"num_labels\"", "]", ")", "\n", "model", ".", "load_state_dict", "(", "torch", ".", "load", "(", "output_model_file", ")", ")", "\n", "tokenizer", "=", "BertTokenizer", ".", "from_pretrained", "(", "model_config", "[", "\"bert_model\"", "]", ",", "do_lower_case", "=", "False", ")", "\n", "return", "model", ",", "tokenizer", ",", "model_config", "\n", "\n"]], "home.repos.pwc.inspect_result.lawsonabs_pun.None.bert.Ner.tokenize": [[37, 51], ["nltk.word_tokenize", "enumerate", "bert.Ner.tokenizer.tokenize", "tokens.extend", "range", "len", "valid_positions.append", "valid_positions.append"], "methods", ["home.repos.pwc.inspect_result.lawsonabs_pun.None.bert.Ner.tokenize"], ["", "def", "tokenize", "(", "self", ",", "text", ":", "str", ")", ":", "\n", "        ", "\"\"\" tokenize input\"\"\"", "\n", "words", "=", "word_tokenize", "(", "text", ")", "\n", "tokens", "=", "[", "]", "\n", "valid_positions", "=", "[", "]", "\n", "for", "i", ",", "word", "in", "enumerate", "(", "words", ")", ":", "\n", "            ", "token", "=", "self", ".", "tokenizer", ".", "tokenize", "(", "word", ")", "\n", "tokens", ".", "extend", "(", "token", ")", "\n", "for", "i", "in", "range", "(", "len", "(", "token", ")", ")", ":", "\n", "                ", "if", "i", "==", "0", ":", "\n", "                    ", "valid_positions", ".", "append", "(", "1", ")", "\n", "", "else", ":", "\n", "                    ", "valid_positions", ".", "append", "(", "0", ")", "\n", "", "", "", "return", "tokens", ",", "valid_positions", "\n", "\n"]], "home.repos.pwc.inspect_result.lawsonabs_pun.None.bert.Ner.preprocess": [[52, 69], ["bert.Ner.tokenize", "tokens.insert", "tokens.append", "range", "bert.Ner.tokenizer.convert_tokens_to_ids", "len", "segment_ids.append", "len", "len", "bert.Ner.append", "input_mask.append", "segment_ids.append"], "methods", ["home.repos.pwc.inspect_result.lawsonabs_pun.None.bert.Ner.tokenize"], ["", "def", "preprocess", "(", "self", ",", "text", ":", "str", ")", ":", "\n", "        ", "\"\"\" preprocess \"\"\"", "\n", "tokens", ",", "valid_positions", "=", "self", ".", "tokenize", "(", "text", ")", "\n", "## insert \"[CLS]\"", "\n", "tokens", ".", "insert", "(", "0", ",", "\"[CLS]\"", ")", "\n", "## insert \"[SEP]\"", "\n", "tokens", ".", "append", "(", "\"[SEP]\"", ")", "\n", "segment_ids", "=", "[", "]", "\n", "for", "i", "in", "range", "(", "len", "(", "tokens", ")", ")", ":", "\n", "            ", "segment_ids", ".", "append", "(", "0", ")", "\n", "", "input_ids", "=", "self", ".", "tokenizer", ".", "convert_tokens_to_ids", "(", "tokens", ")", "\n", "input_mask", "=", "[", "1", "]", "*", "len", "(", "input_ids", ")", "\n", "while", "len", "(", "input_ids", ")", "<", "self", ".", "max_seq_length", ":", "\n", "            ", "input_ids", ".", "append", "(", "0", ")", "\n", "input_mask", ".", "append", "(", "0", ")", "\n", "segment_ids", ".", "append", "(", "0", ")", "\n", "", "return", "input_ids", ",", "input_mask", ",", "segment_ids", ",", "valid_positions", "\n", "\n"]], "home.repos.pwc.inspect_result.lawsonabs_pun.None.bert.Ner.predict": [[70, 96], ["bert.Ner.preprocess", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.softmax", "torch.softmax", "torch.argmax", "torch.argmax", "torch.argmax", "torch.argmax", "logits_label.detach().cpu().numpy.detach().cpu().numpy.detach().cpu().numpy", "logits_label.detach().cpu().numpy.detach().cpu().numpy.pop", "logits_label.detach().cpu().numpy.detach().cpu().numpy.pop", "zip", "nltk.word_tokenize", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "bert.Ner.model", "values[].item", "len", "len", "len", "len", "logits_label.detach().cpu().numpy.detach().cpu().numpy.detach().cpu", "zip", "enumerate", "labels.append", "zip", "i.item", "logits_label.detach().cpu().numpy.detach().cpu().numpy.detach"], "methods", ["home.repos.pwc.inspect_result.lawsonabs_pun.None.bert.Ner.preprocess"], ["", "def", "predict", "(", "self", ",", "text", ":", "str", ")", ":", "\n", "        ", "input_ids", ",", "input_mask", ",", "segment_ids", ",", "valid_positions", "=", "self", ".", "preprocess", "(", "text", ")", "\n", "input_ids", "=", "torch", ".", "tensor", "(", "[", "input_ids", "]", ",", "dtype", "=", "torch", ".", "long", ")", "\n", "input_mask", "=", "torch", ".", "tensor", "(", "[", "input_mask", "]", ",", "dtype", "=", "torch", ".", "long", ")", "\n", "segment_ids", "=", "torch", ".", "tensor", "(", "[", "segment_ids", "]", ",", "dtype", "=", "torch", ".", "long", ")", "\n", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "            ", "logits", "=", "self", ".", "model", "(", "input_ids", ",", "segment_ids", ",", "input_mask", ")", "\n", "", "logits", "=", "F", ".", "softmax", "(", "logits", ",", "dim", "=", "2", ")", "\n", "logits_label", "=", "torch", ".", "argmax", "(", "logits", ",", "dim", "=", "2", ")", "\n", "logits_label", "=", "logits_label", ".", "detach", "(", ")", ".", "cpu", "(", ")", ".", "numpy", "(", ")", "\n", "# import ipdb; ipdb.set_trace()", "\n", "logits_confidence", "=", "[", "values", "[", "label", "]", ".", "item", "(", ")", "for", "values", ",", "label", "in", "zip", "(", "logits", "[", "0", "]", ",", "logits_label", "[", "0", "]", ")", "]", "\n", "\n", "logits_label", "=", "[", "logits_label", "[", "0", "]", "[", "index", "]", "for", "index", ",", "i", "in", "enumerate", "(", "input_mask", "[", "0", "]", ")", "if", "i", ".", "item", "(", ")", "==", "1", "]", "\n", "logits_label", ".", "pop", "(", "0", ")", "\n", "logits_label", ".", "pop", "(", ")", "\n", "\n", "assert", "len", "(", "logits_label", ")", "==", "len", "(", "valid_positions", ")", "\n", "labels", "=", "[", "]", "\n", "for", "valid", ",", "label", "in", "zip", "(", "valid_positions", ",", "logits_label", ")", ":", "\n", "            ", "if", "valid", ":", "\n", "                ", "labels", ".", "append", "(", "self", ".", "label_map", "[", "label", "]", ")", "\n", "", "", "words", "=", "word_tokenize", "(", "text", ")", "\n", "assert", "len", "(", "labels", ")", "==", "len", "(", "words", ")", "\n", "output", "=", "{", "word", ":", "{", "\"tag\"", ":", "label", ",", "\"confidence\"", ":", "confidence", "}", "for", "word", ",", "label", ",", "confidence", "in", "zip", "(", "words", ",", "labels", ",", "logits_confidence", ")", "}", "\n", "return", "output", "", "", "", ""]], "home.repos.pwc.inspect_result.lawsonabs_pun.None.run_ner.main": [[32, 408], ["argparse.ArgumentParser", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.parse_args", "logger.info", "random.seed", "numpy.random.seed", "torch.manual_seed", "torch.manual_seed", "parser.parse_args.task_name.lower", "processor.get_labels", "pytorch_pretrained_bert.tokenization.BertTokenizer.from_pretrained", "bert_models.BertForTokenPronsClassification.from_pretrained", "torch.nn.DataParallel.to", "list", "torch.nn.DataParallel.to", "print", "ptvsd.enable_attach", "ptvsd.wait_for_attach", "torch.device", "torch.device", "torch.cuda.device_count", "torch.cuda.device_count", "torch.cuda.set_device", "torch.cuda.set_device", "torch.device", "torch.device", "torch.distributed.init_process_group", "torch.distributed.init_process_group", "ValueError", "ValueError", "os.path.exists", "os.listdir", "ValueError", "os.path.exists", "os.makedirs", "ValueError", "len", "processor.get_train_examples", "os.path.join", "torch.nn.DataParallel.half", "DDP", "torch.nn.DataParallel.named_parameters", "FusedAdam", "pytorch_pretrained_bert.optimization.BertAdam", "bert_utils.embed_load", "bert_utils.convert_examples_to_pron_features", "print", "bert_utils.embed_extend", "torch.tensor", "torch.tensor", "torch.nn.Embedding.from_pretrained", "torch.nn.Embedding.from_pretrained", "logger.info", "logger.info", "logger.info", "logger.info", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.utils.data.TensorDataset", "torch.utils.data.DataLoader", "torch.nn.DataParallel.train", "tqdm.trange", "os.path.join", "torch.save", "torch.save", "os.path.join", "json.dump", "os.path.join", "os.path.join", "bert_models.BertForTokenPronsClassification.from_pretrained", "torch.nn.DataParallel.load_state_dict", "processor.get_dev_examples", "bert_utils.convert_examples_to_pron_features", "logger.info", "logger.info", "logger.info", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.utils.data.TensorDataset", "torch.utils.data.SequentialSampler", "torch.utils.data.DataLoader", "torch.nn.DataParallel.eval", "tqdm.tqdm", "seqeval.metrics.classification_report", "os.path.join", "bool", "int", "str", "torch.nn.DataParallel", "torch.nn.DataParallel", "FP16_Optimizer", "FP16_Optimizer", "len", "len", "torch.utils.data.RandomSampler", "torch.utils.data.distributed.DistributedSampler", "int", "enumerate", "print", "hasattr", "model_to_save.state_dict", "open", "f.write", "open", "torch.load", "torch.load", "len", "torch.nn.Embedding.from_pretrained.to", "input_ids.to.to", "input_mask.to().numpy.to", "segment_ids.to.to", "label_ids.to().numpy.to", "prons_ids.to.to", "prons_att_mask.to.to", "torch.argmax", "torch.argmax", "model.detach().cpu().numpy", "label_ids.to().numpy.to().numpy", "input_mask.to().numpy.to().numpy", "enumerate", "open", "logger.info", "logger.info", "writer.write", "torch.distributed.get_world_size", "torch.distributed.get_world_size", "ImportError", "ImportError", "tqdm.tqdm", "tuple", "torch.nn.Embedding.from_pretrained.to", "torch.nn.DataParallel.", "loss.mean.item", "input_ids.to.size", "model_to_save.config.to_json_string", "enumerate", "len", "os.path.join", "torch.distributed.get_rank", "torch.distributed.get_rank", "enumerate", "torch.no_grad", "torch.no_grad", "torch.nn.DataParallel.", "torch.log_softmax", "enumerate", "torch.cuda.is_available", "torch.cuda.is_available", "any", "str", "loss.mean.mean", "FP16_Optimizer.backward", "loss.mean.backward", "FP16_Optimizer.step", "FP16_Optimizer.zero_grad", "torch.nn.Embedding.from_pretrained.", "model.detach().cpu", "label_ids.to().numpy.to", "input_mask.to().numpy.to", "len", "any", "t.to", "torch.nn.Embedding.from_pretrained.", "temp_1.append", "temp_2.append", "temp_1.pop", "temp_2.pop", "y_true.append", "y_pred.append", "prons_ids.to.detach().cpu", "pytorch_pretrained_bert.optimization.warmup_linear", "model.detach", "prons_ids.to.detach"], "function", ["home.repos.pwc.inspect_result.lawsonabs_pun.None.bert_utils.ScProcessor.get_labels", "home.repos.pwc.inspect_result.lawsonabs_pun.None.bert_models.BertPreTrainedModel.from_pretrained", "home.repos.pwc.inspect_result.lawsonabs_pun.None.bert_models.BertPreTrainedModel.from_pretrained", "home.repos.pwc.inspect_result.lawsonabs_pun.None.bert_utils.ScProcessor.get_train_examples", "home.repos.pwc.inspect_result.lawsonabs_pun.None.bert_utils.embed_load", "home.repos.pwc.inspect_result.lawsonabs_pun.tools.utils.convert_examples_to_pron_features", "home.repos.pwc.inspect_result.lawsonabs_pun.None.bert_utils.embed_extend", "home.repos.pwc.inspect_result.lawsonabs_pun.None.bert_models.BertPreTrainedModel.from_pretrained", "home.repos.pwc.inspect_result.lawsonabs_pun.None.bert_models.BertPreTrainedModel.from_pretrained", "home.repos.pwc.inspect_result.lawsonabs_pun.subtask3_v2.gloss.train", "home.repos.pwc.inspect_result.lawsonabs_pun.None.bert_models.BertPreTrainedModel.from_pretrained", "home.repos.pwc.inspect_result.lawsonabs_pun.None.bert_utils.NerProcessor.get_dev_examples", "home.repos.pwc.inspect_result.lawsonabs_pun.tools.utils.convert_examples_to_pron_features", "home.repos.pwc.inspect_result.lawsonabs_pun.None.bert_models.BertConfig.to_json_string"], ["def", "main", "(", ")", ":", "\n", "    ", "parser", "=", "argparse", ".", "ArgumentParser", "(", ")", "\n", "\n", "## Required parameters", "\n", "parser", ".", "add_argument", "(", "\"--data_dir\"", ",", "\n", "default", "=", "None", ",", "\n", "type", "=", "str", ",", "\n", "required", "=", "True", ",", "\n", "help", "=", "\"The input data dir. Should contain the .tsv files (or other data files) for the task.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--bert_model\"", ",", "default", "=", "None", ",", "type", "=", "str", ",", "required", "=", "True", ",", "\n", "help", "=", "\"Bert pre-trained model selected in the list: bert-base-uncased, \"", "\n", "\"bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, \"", "\n", "\"bert-base-multilingual-cased, bert-base-chinese.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--task_name\"", ",", "\n", "default", "=", "None", ",", "\n", "type", "=", "str", ",", "\n", "required", "=", "True", ",", "\n", "help", "=", "\"The name of the task to train.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--output_dir\"", ",", "\n", "default", "=", "None", ",", "\n", "type", "=", "str", ",", "\n", "required", "=", "True", ",", "\n", "help", "=", "\"The output directory where the model predictions and checkpoints will be written.\"", ")", "\n", "\n", "## Other parameters", "\n", "parser", ".", "add_argument", "(", "\"--cache_dir\"", ",", "\n", "default", "=", "\"\"", ",", "\n", "type", "=", "str", ",", "\n", "help", "=", "\"Where do you want to store the pre-trained models downloaded from s3\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--max_seq_length\"", ",", "\n", "default", "=", "128", ",", "\n", "type", "=", "int", ",", "\n", "help", "=", "\"The maximum total input sequence length after WordPiece tokenization. \\n\"", "\n", "\"Sequences longer than this will be truncated, and sequences shorter \\n\"", "\n", "\"than this will be padded.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--max_pron_length\"", ",", "\n", "default", "=", "5", ",", "\n", "type", "=", "int", ",", "\n", "help", "=", "\"The maximum total input sequence length after pronounciation tokenization. \\n\"", "\n", "\"Sequences longer than this will be truncated, and sequences shorter \\n\"", "\n", "\"than this will be padded.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--pron_emb_size\"", ",", "\n", "default", "=", "16", ",", "\n", "type", "=", "int", ",", "\n", "help", "=", "\"The embedding size of pronounciation embedding.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--do_train\"", ",", "\n", "action", "=", "'store_true'", ",", "\n", "help", "=", "\"Whether to run training.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--do_eval\"", ",", "\n", "action", "=", "'store_true'", ",", "\n", "help", "=", "\"Whether to run eval on the dev set.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--do_lower_case\"", ",", "\n", "action", "=", "'store_true'", ",", "\n", "help", "=", "\"Set this flag if you are using an uncased model.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--train_batch_size\"", ",", "\n", "default", "=", "32", ",", "\n", "type", "=", "int", ",", "\n", "help", "=", "\"Total batch size for training.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--eval_batch_size\"", ",", "\n", "default", "=", "8", ",", "\n", "type", "=", "int", ",", "\n", "help", "=", "\"Total batch size for eval.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--learning_rate\"", ",", "\n", "default", "=", "5e-5", ",", "\n", "type", "=", "float", ",", "\n", "help", "=", "\"The initial learning rate for Adam.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--num_train_epochs\"", ",", "\n", "default", "=", "3.0", ",", "\n", "type", "=", "float", ",", "\n", "help", "=", "\"Total number of training epochs to perform.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--warmup_proportion\"", ",", "\n", "default", "=", "0.1", ",", "\n", "type", "=", "float", ",", "\n", "help", "=", "\"Proportion of training to perform linear learning rate warmup for. \"", "\n", "\"E.g., 0.1 = 10%% of training.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--no_cuda\"", ",", "\n", "action", "=", "'store_true'", ",", "\n", "help", "=", "\"Whether not to use CUDA when available\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--local_rank\"", ",", "\n", "type", "=", "int", ",", "\n", "default", "=", "-", "1", ",", "\n", "help", "=", "\"local_rank for distributed training on gpus\"", ")", "\n", "parser", ".", "add_argument", "(", "'--seed'", ",", "\n", "type", "=", "int", ",", "\n", "default", "=", "42", ",", "\n", "help", "=", "\"random seed for initialization\"", ")", "\n", "parser", ".", "add_argument", "(", "'--gradient_accumulation_steps'", ",", "\n", "type", "=", "int", ",", "\n", "default", "=", "1", ",", "\n", "help", "=", "\"Number of updates steps to accumulate before performing a backward/update pass.\"", ")", "\n", "parser", ".", "add_argument", "(", "'--fp16'", ",", "\n", "action", "=", "'store_true'", ",", "\n", "help", "=", "\"Whether to use 16-bit float precision instead of 32-bit\"", ")", "\n", "parser", ".", "add_argument", "(", "'--loss_scale'", ",", "\n", "type", "=", "float", ",", "default", "=", "0", ",", "\n", "help", "=", "\"Loss scaling to improve fp16 numeric stability. Only used when fp16 set to True.\\n\"", "\n", "\"0 (default value): dynamic loss scaling.\\n\"", "\n", "\"Positive power of 2: static loss scaling value.\\n\"", ")", "\n", "parser", ".", "add_argument", "(", "'--server_ip'", ",", "type", "=", "str", ",", "default", "=", "''", ",", "help", "=", "\"Can be used for distant debugging.\"", ")", "\n", "parser", ".", "add_argument", "(", "'--server_port'", ",", "type", "=", "str", ",", "default", "=", "''", ",", "help", "=", "\"Can be used for distant debugging.\"", ")", "\n", "args", "=", "parser", ".", "parse_args", "(", ")", "\n", "\n", "if", "args", ".", "server_ip", "and", "args", ".", "server_port", ":", "\n", "# Distant debugging - see https://code.visualstudio.com/docs/python/debugging#_attach-to-a-local-script", "\n", "        ", "import", "ptvsd", "\n", "print", "(", "\"Waiting for debugger attach\"", ")", "\n", "ptvsd", ".", "enable_attach", "(", "address", "=", "(", "args", ".", "server_ip", ",", "args", ".", "server_port", ")", ",", "redirect_output", "=", "True", ")", "\n", "ptvsd", ".", "wait_for_attach", "(", ")", "\n", "\n", "", "processors", "=", "{", "\"ner\"", ":", "NerProcessor", "}", "\n", "\n", "if", "args", ".", "local_rank", "==", "-", "1", "or", "args", ".", "no_cuda", ":", "\n", "        ", "device", "=", "torch", ".", "device", "(", "\"cuda\"", "if", "torch", ".", "cuda", ".", "is_available", "(", ")", "and", "not", "args", ".", "no_cuda", "else", "\"cpu\"", ")", "\n", "n_gpu", "=", "torch", ".", "cuda", ".", "device_count", "(", ")", "\n", "", "else", ":", "\n", "        ", "torch", ".", "cuda", ".", "set_device", "(", "args", ".", "local_rank", ")", "\n", "device", "=", "torch", ".", "device", "(", "\"cuda\"", ",", "args", ".", "local_rank", ")", "\n", "n_gpu", "=", "1", "\n", "# Initializes the distributed backend which will take care of sychronizing nodes/GPUs", "\n", "torch", ".", "distributed", ".", "init_process_group", "(", "backend", "=", "'nccl'", ")", "\n", "", "logger", ".", "info", "(", "\"device: {} n_gpu: {}, distributed training: {}, 16-bits training: {}\"", ".", "format", "(", "\n", "device", ",", "n_gpu", ",", "bool", "(", "args", ".", "local_rank", "!=", "-", "1", ")", ",", "args", ".", "fp16", ")", ")", "\n", "\n", "if", "args", ".", "gradient_accumulation_steps", "<", "1", ":", "\n", "        ", "raise", "ValueError", "(", "\"Invalid gradient_accumulation_steps parameter: {}, should be >= 1\"", ".", "format", "(", "\n", "args", ".", "gradient_accumulation_steps", ")", ")", "\n", "\n", "", "args", ".", "train_batch_size", "=", "args", ".", "train_batch_size", "//", "args", ".", "gradient_accumulation_steps", "\n", "\n", "random", ".", "seed", "(", "args", ".", "seed", ")", "\n", "np", ".", "random", ".", "seed", "(", "args", ".", "seed", ")", "\n", "torch", ".", "manual_seed", "(", "args", ".", "seed", ")", "\n", "\n", "if", "not", "args", ".", "do_train", "and", "not", "args", ".", "do_eval", ":", "\n", "        ", "raise", "ValueError", "(", "\"At least one of `do_train` or `do_eval` must be True.\"", ")", "\n", "\n", "", "if", "os", ".", "path", ".", "exists", "(", "args", ".", "output_dir", ")", "and", "os", ".", "listdir", "(", "args", ".", "output_dir", ")", "and", "args", ".", "do_train", ":", "\n", "        ", "raise", "ValueError", "(", "\"Output directory ({}) already exists and is not empty.\"", ".", "format", "(", "args", ".", "output_dir", ")", ")", "\n", "", "if", "not", "os", ".", "path", ".", "exists", "(", "args", ".", "output_dir", ")", ":", "\n", "        ", "os", ".", "makedirs", "(", "args", ".", "output_dir", ")", "\n", "\n", "", "task_name", "=", "args", ".", "task_name", ".", "lower", "(", ")", "\n", "\n", "if", "task_name", "not", "in", "processors", ":", "\n", "        ", "raise", "ValueError", "(", "\"Task not found: %s\"", "%", "(", "task_name", ")", ")", "\n", "\n", "", "processor", "=", "processors", "[", "task_name", "]", "(", ")", "\n", "label_list", "=", "processor", ".", "get_labels", "(", ")", "\n", "num_labels", "=", "len", "(", "label_list", ")", "+", "1", "\n", "\n", "tokenizer", "=", "BertTokenizer", ".", "from_pretrained", "(", "args", ".", "bert_model", ",", "do_lower_case", "=", "args", ".", "do_lower_case", ")", "\n", "\n", "train_examples", "=", "None", "\n", "num_train_optimization_steps", "=", "None", "\n", "if", "args", ".", "do_train", ":", "\n", "        ", "train_examples", "=", "processor", ".", "get_train_examples", "(", "args", ".", "data_dir", ")", "\n", "num_train_optimization_steps", "=", "int", "(", "\n", "len", "(", "train_examples", ")", "/", "args", ".", "train_batch_size", "/", "args", ".", "gradient_accumulation_steps", ")", "*", "args", ".", "num_train_epochs", "\n", "if", "args", ".", "local_rank", "!=", "-", "1", ":", "\n", "            ", "num_train_optimization_steps", "=", "num_train_optimization_steps", "//", "torch", ".", "distributed", ".", "get_world_size", "(", ")", "\n", "\n", "# Prepare model", "\n", "", "", "cache_dir", "=", "args", ".", "cache_dir", "if", "args", ".", "cache_dir", "else", "os", ".", "path", ".", "join", "(", "str", "(", "PYTORCH_PRETRAINED_BERT_CACHE", ")", ",", "'distributed_{}'", ".", "format", "(", "args", ".", "local_rank", ")", ")", "\n", "model", "=", "BertForTokenPronsClassification", ".", "from_pretrained", "(", "args", ".", "bert_model", ",", "\n", "cache_dir", "=", "cache_dir", ",", "\n", "num_labels", "=", "num_labels", ",", "\n", "max_seq_length", "=", "args", ".", "max_seq_length", ",", "\n", "max_prons_length", "=", "args", ".", "max_pron_length", ",", "\n", "pron_emb_size", "=", "args", ".", "pron_emb_size", ")", "\n", "\n", "#print(model.classifier.weight.requires_grad)", "\n", "#print(model.classifier.weight)", "\n", "\n", "if", "args", ".", "fp16", ":", "\n", "        ", "model", ".", "half", "(", ")", "\n", "", "model", ".", "to", "(", "device", ")", "\n", "if", "args", ".", "local_rank", "!=", "-", "1", ":", "\n", "        ", "try", ":", "\n", "            ", "from", "apex", ".", "parallel", "import", "DistributedDataParallel", "as", "DDP", "\n", "", "except", "ImportError", ":", "\n", "            ", "raise", "ImportError", "(", "\"Please install apex from https://www.github.com/nvidia/apex to use distributed and fp16 training.\"", ")", "\n", "\n", "", "model", "=", "DDP", "(", "model", ")", "\n", "", "elif", "n_gpu", ">", "1", ":", "\n", "        ", "model", "=", "torch", ".", "nn", ".", "DataParallel", "(", "model", ")", "\n", "\n", "", "param_optimizer", "=", "list", "(", "model", ".", "named_parameters", "(", ")", ")", "\n", "no_decay", "=", "[", "'bias'", ",", "'LayerNorm.bias'", ",", "'LayerNorm.weight'", "]", "\n", "optimizer_grouped_parameters", "=", "[", "\n", "{", "'params'", ":", "[", "p", "for", "n", ",", "p", "in", "param_optimizer", "if", "not", "any", "(", "nd", "in", "n", "for", "nd", "in", "no_decay", ")", "]", ",", "'weight_decay'", ":", "0.01", "}", ",", "\n", "{", "'params'", ":", "[", "p", "for", "n", ",", "p", "in", "param_optimizer", "if", "any", "(", "nd", "in", "n", "for", "nd", "in", "no_decay", ")", "]", ",", "'weight_decay'", ":", "0.0", "}", "\n", "]", "\n", "if", "args", ".", "fp16", ":", "\n", "        ", "try", ":", "\n", "            ", "from", "apex", ".", "optimizers", "import", "FP16_Optimizer", "\n", "from", "apex", ".", "optimizers", "import", "FusedAdam", "\n", "", "except", "ImportError", ":", "\n", "            ", "raise", "ImportError", "(", "\"Please install apex from https://www.github.com/nvidia/apex to use distributed and fp16 training.\"", ")", "\n", "\n", "", "optimizer", "=", "FusedAdam", "(", "optimizer_grouped_parameters", ",", "\n", "lr", "=", "args", ".", "learning_rate", ",", "\n", "bias_correction", "=", "False", ",", "\n", "max_grad_norm", "=", "1.0", ")", "\n", "if", "args", ".", "loss_scale", "==", "0", ":", "\n", "            ", "optimizer", "=", "FP16_Optimizer", "(", "optimizer", ",", "dynamic_loss_scale", "=", "True", ")", "\n", "", "else", ":", "\n", "            ", "optimizer", "=", "FP16_Optimizer", "(", "optimizer", ",", "static_loss_scale", "=", "args", ".", "loss_scale", ")", "\n", "\n", "", "", "else", ":", "\n", "        ", "optimizer", "=", "BertAdam", "(", "optimizer_grouped_parameters", ",", "\n", "lr", "=", "args", ".", "learning_rate", ",", "\n", "warmup", "=", "args", ".", "warmup_proportion", ",", "\n", "t_total", "=", "num_train_optimization_steps", ")", "\n", "\n", "", "global_step", "=", "0", "\n", "nb_tr_steps", "=", "0", "\n", "tr_loss", "=", "0", "\n", "if", "args", ".", "do_train", ":", "\n", "        ", "prons_map", "=", "{", "}", "\n", "prons_map", ",", "prons_emb", "=", "embed_load", "(", "'./data/pron.'", "+", "str", "(", "args", ".", "pron_emb_size", ")", "+", "'.vec'", ")", "\n", "#print(prons_map)", "\n", "train_features", ",", "prons_map", "=", "convert_examples_to_pron_features", "(", "\n", "train_examples", ",", "label_list", ",", "args", ".", "max_seq_length", ",", "args", ".", "max_pron_length", ",", "tokenizer", ",", "prons_map", ")", "\n", "\n", "print", "(", "prons_map", ")", "\n", "prons_emb", "=", "embed_extend", "(", "prons_emb", ",", "len", "(", "prons_map", ")", ")", "\n", "prons_emb", "=", "torch", ".", "tensor", "(", "prons_emb", ",", "dtype", "=", "torch", ".", "float", ")", "\n", "#sys.exit()", "\n", "#prons_embedding = torch.nn.Embedding(len(prons_map)+1, args.pron_emb_size) #initialize embeddings", "\n", "prons_embedding", "=", "torch", ".", "nn", ".", "Embedding", ".", "from_pretrained", "(", "prons_emb", ")", "\n", "prons_embedding", ".", "weight", ".", "requires_grad", "=", "False", "\n", "\n", "logger", ".", "info", "(", "\"***** Running training *****\"", ")", "\n", "logger", ".", "info", "(", "\"  Num examples = %d\"", ",", "len", "(", "train_examples", ")", ")", "\n", "logger", ".", "info", "(", "\"  Batch size = %d\"", ",", "args", ".", "train_batch_size", ")", "\n", "logger", ".", "info", "(", "\"  Num steps = %d\"", ",", "num_train_optimization_steps", ")", "\n", "all_input_ids", "=", "torch", ".", "tensor", "(", "[", "f", ".", "input_ids", "for", "f", "in", "train_features", "]", ",", "dtype", "=", "torch", ".", "long", ")", "\n", "all_input_mask", "=", "torch", ".", "tensor", "(", "[", "f", ".", "input_mask", "for", "f", "in", "train_features", "]", ",", "dtype", "=", "torch", ".", "long", ")", "\n", "all_segment_ids", "=", "torch", ".", "tensor", "(", "[", "f", ".", "segment_ids", "for", "f", "in", "train_features", "]", ",", "dtype", "=", "torch", ".", "long", ")", "\n", "all_label_ids", "=", "torch", ".", "tensor", "(", "[", "f", ".", "label_id", "for", "f", "in", "train_features", "]", ",", "dtype", "=", "torch", ".", "long", ")", "\n", "all_prons_ids", "=", "torch", ".", "tensor", "(", "[", "f", ".", "prons_id", "for", "f", "in", "train_features", "]", ",", "dtype", "=", "torch", ".", "long", ")", "\n", "all_prons_att_mask", "=", "torch", ".", "tensor", "(", "[", "f", ".", "prons_att_mask", "for", "f", "in", "train_features", "]", ",", "dtype", "=", "torch", ".", "long", ")", "\n", "train_data", "=", "TensorDataset", "(", "all_input_ids", ",", "all_input_mask", ",", "all_segment_ids", ",", "all_label_ids", ",", "all_prons_ids", ",", "all_prons_att_mask", ")", "\n", "if", "args", ".", "local_rank", "==", "-", "1", ":", "\n", "            ", "train_sampler", "=", "RandomSampler", "(", "train_data", ")", "\n", "", "else", ":", "\n", "            ", "train_sampler", "=", "DistributedSampler", "(", "train_data", ")", "\n", "", "train_dataloader", "=", "DataLoader", "(", "train_data", ",", "sampler", "=", "train_sampler", ",", "batch_size", "=", "args", ".", "train_batch_size", ")", "\n", "\n", "model", ".", "train", "(", ")", "\n", "for", "index", "in", "trange", "(", "int", "(", "args", ".", "num_train_epochs", ")", ",", "desc", "=", "\"Epoch\"", ")", ":", "\n", "            ", "tr_loss", "=", "0", "\n", "nb_tr_examples", ",", "nb_tr_steps", "=", "0", ",", "0", "\n", "\n", "#if index == 11:", "\n", "#    for param in model.bert.parameters():", "\n", "#        param.requires_grad = False", "\n", "\n", "for", "step", ",", "batch", "in", "enumerate", "(", "tqdm", "(", "train_dataloader", ",", "desc", "=", "\"Iteration\"", ")", ")", ":", "\n", "                ", "batch", "=", "tuple", "(", "t", ".", "to", "(", "device", ")", "for", "t", "in", "batch", ")", "\n", "input_ids", ",", "input_mask", ",", "segment_ids", ",", "label_ids", ",", "prons_ids", ",", "prons_att_mask", "=", "batch", "\n", "prons_emb", "=", "prons_embedding", "(", "prons_ids", ".", "detach", "(", ")", ".", "cpu", "(", ")", ")", ".", "to", "(", "device", ")", "\n", "\n", "loss", "=", "model", "(", "input_ids", ",", "segment_ids", ",", "input_mask", ",", "prons_emb", ",", "prons_att_mask", ",", "label_ids", ")", "\n", "if", "n_gpu", ">", "1", ":", "\n", "                    ", "loss", "=", "loss", ".", "mean", "(", ")", "# mean() to average on multi-gpu.", "\n", "", "if", "args", ".", "gradient_accumulation_steps", ">", "1", ":", "\n", "                    ", "loss", "=", "loss", "/", "args", ".", "gradient_accumulation_steps", "\n", "\n", "", "if", "args", ".", "fp16", ":", "\n", "                    ", "optimizer", ".", "backward", "(", "loss", ")", "\n", "", "else", ":", "\n", "                    ", "loss", ".", "backward", "(", ")", "\n", "\n", "", "tr_loss", "+=", "loss", ".", "item", "(", ")", "\n", "nb_tr_examples", "+=", "input_ids", ".", "size", "(", "0", ")", "\n", "nb_tr_steps", "+=", "1", "\n", "if", "(", "step", "+", "1", ")", "%", "args", ".", "gradient_accumulation_steps", "==", "0", ":", "\n", "                    ", "if", "args", ".", "fp16", ":", "\n", "# modify learning rate with special warm up BERT uses", "\n", "# if args.fp16 is False, BertAdam is used that handles this automatically", "\n", "                        ", "lr_this_step", "=", "args", ".", "learning_rate", "*", "warmup_linear", "(", "global_step", "/", "num_train_optimization_steps", ",", "args", ".", "warmup_proportion", ")", "\n", "for", "param_group", "in", "optimizer", ".", "param_groups", ":", "\n", "                            ", "param_group", "[", "'lr'", "]", "=", "lr_this_step", "\n", "", "", "optimizer", ".", "step", "(", ")", "\n", "optimizer", ".", "zero_grad", "(", ")", "\n", "global_step", "+=", "1", "\n", "", "", "print", "(", "\"loss: {}\"", ".", "format", "(", "tr_loss", "/", "nb_tr_examples", ")", ")", "\n", "\n", "# Save a trained model and the associated configuration", "\n", "", "model_to_save", "=", "model", ".", "module", "if", "hasattr", "(", "model", ",", "'module'", ")", "else", "model", "# Only save the model it-self", "\n", "output_model_file", "=", "os", ".", "path", ".", "join", "(", "args", ".", "output_dir", ",", "WEIGHTS_NAME", ")", "\n", "torch", ".", "save", "(", "model_to_save", ".", "state_dict", "(", ")", ",", "output_model_file", ")", "\n", "output_config_file", "=", "os", ".", "path", ".", "join", "(", "args", ".", "output_dir", ",", "CONFIG_NAME", ")", "\n", "with", "open", "(", "output_config_file", ",", "'w'", ")", "as", "f", ":", "\n", "            ", "f", ".", "write", "(", "model_to_save", ".", "config", ".", "to_json_string", "(", ")", ")", "\n", "", "label_map", "=", "{", "i", ":", "label", "for", "i", ",", "label", "in", "enumerate", "(", "label_list", ",", "1", ")", "}", "\n", "model_config", "=", "{", "\"bert_model\"", ":", "args", ".", "bert_model", ",", "\"do_lower\"", ":", "args", ".", "do_lower_case", ",", "\"max_seq_length\"", ":", "args", ".", "max_seq_length", ",", "\"num_labels\"", ":", "len", "(", "label_list", ")", "+", "1", ",", "\"label_map\"", ":", "label_map", "}", "\n", "json", ".", "dump", "(", "model_config", ",", "open", "(", "os", ".", "path", ".", "join", "(", "args", ".", "output_dir", ",", "\"model_config.json\"", ")", ",", "\"w\"", ")", ")", "\n", "# Load a trained model and config that you have fine-tuned", "\n", "", "else", ":", "\n", "        ", "output_config_file", "=", "os", ".", "path", ".", "join", "(", "args", ".", "output_dir", ",", "CONFIG_NAME", ")", "\n", "output_model_file", "=", "os", ".", "path", ".", "join", "(", "args", ".", "output_dir", ",", "WEIGHTS_NAME", ")", "\n", "#config = BertConfig(output_config_file)", "\n", "#model = BertForTokenClassification(config, num_labels=num_labels)", "\n", "model", "=", "BertForTokenPronsClassification", ".", "from_pretrained", "(", "args", ".", "bert_model", ",", "\n", "cache_dir", "=", "cache_dir", ",", "\n", "num_labels", "=", "num_labels", ",", "\n", "max_seq_length", "=", "args", ".", "max_seq_length", ",", "\n", "max_prons_length", "=", "args", ".", "max_pron_length", ",", "\n", "pron_emb_size", "=", "args", ".", "pron_emb_size", ")", "\n", "model", ".", "load_state_dict", "(", "torch", ".", "load", "(", "output_model_file", ")", ")", "\n", "\n", "", "model", ".", "to", "(", "device", ")", "\n", "#print(model.classifier.weight)", "\n", "\n", "if", "args", ".", "do_eval", "and", "(", "args", ".", "local_rank", "==", "-", "1", "or", "torch", ".", "distributed", ".", "get_rank", "(", ")", "==", "0", ")", ":", "\n", "        ", "eval_examples", "=", "processor", ".", "get_dev_examples", "(", "args", ".", "data_dir", ")", "\n", "eval_features", ",", "prons_map", "=", "convert_examples_to_pron_features", "(", "\n", "eval_examples", ",", "label_list", ",", "args", ".", "max_seq_length", ",", "args", ".", "max_pron_length", ",", "tokenizer", ",", "prons_map", ")", "\n", "logger", ".", "info", "(", "\"***** Running evaluation *****\"", ")", "\n", "logger", ".", "info", "(", "\"  Num examples = %d\"", ",", "len", "(", "eval_examples", ")", ")", "\n", "logger", ".", "info", "(", "\"  Batch size = %d\"", ",", "args", ".", "eval_batch_size", ")", "\n", "all_input_ids", "=", "torch", ".", "tensor", "(", "[", "f", ".", "input_ids", "for", "f", "in", "eval_features", "]", ",", "dtype", "=", "torch", ".", "long", ")", "\n", "all_input_mask", "=", "torch", ".", "tensor", "(", "[", "f", ".", "input_mask", "for", "f", "in", "eval_features", "]", ",", "dtype", "=", "torch", ".", "long", ")", "\n", "all_segment_ids", "=", "torch", ".", "tensor", "(", "[", "f", ".", "segment_ids", "for", "f", "in", "eval_features", "]", ",", "dtype", "=", "torch", ".", "long", ")", "\n", "all_label_ids", "=", "torch", ".", "tensor", "(", "[", "f", ".", "label_id", "for", "f", "in", "eval_features", "]", ",", "dtype", "=", "torch", ".", "long", ")", "\n", "all_prons_ids", "=", "torch", ".", "tensor", "(", "[", "f", ".", "prons_id", "for", "f", "in", "eval_features", "]", ",", "dtype", "=", "torch", ".", "long", ")", "\n", "all_prons_att_mask", "=", "torch", ".", "tensor", "(", "[", "f", ".", "prons_att_mask", "for", "f", "in", "eval_features", "]", ",", "dtype", "=", "torch", ".", "long", ")", "\n", "eval_data", "=", "TensorDataset", "(", "all_input_ids", ",", "all_input_mask", ",", "all_segment_ids", ",", "all_label_ids", ",", "all_prons_ids", ",", "all_prons_att_mask", ")", "\n", "# Run prediction for full data", "\n", "eval_sampler", "=", "SequentialSampler", "(", "eval_data", ")", "\n", "eval_dataloader", "=", "DataLoader", "(", "eval_data", ",", "sampler", "=", "eval_sampler", ",", "batch_size", "=", "args", ".", "eval_batch_size", ")", "\n", "model", ".", "eval", "(", ")", "\n", "eval_loss", ",", "eval_accuracy", "=", "0", ",", "0", "\n", "nb_eval_steps", ",", "nb_eval_examples", "=", "0", ",", "0", "\n", "y_true", "=", "[", "]", "\n", "y_pred", "=", "[", "]", "\n", "label_map", "=", "{", "i", ":", "label", "for", "i", ",", "label", "in", "enumerate", "(", "label_list", ",", "1", ")", "}", "\n", "for", "input_ids", ",", "input_mask", ",", "segment_ids", ",", "label_ids", ",", "prons_ids", ",", "prons_att_mask", "in", "tqdm", "(", "eval_dataloader", ",", "desc", "=", "\"Evaluating\"", ")", ":", "\n", "            ", "prons_emb", "=", "prons_embedding", "(", "prons_ids", ")", ".", "to", "(", "device", ")", "\n", "input_ids", "=", "input_ids", ".", "to", "(", "device", ")", "\n", "input_mask", "=", "input_mask", ".", "to", "(", "device", ")", "\n", "segment_ids", "=", "segment_ids", ".", "to", "(", "device", ")", "\n", "label_ids", "=", "label_ids", ".", "to", "(", "device", ")", "\n", "prons_ids", "=", "prons_ids", ".", "to", "(", "device", ")", "\n", "prons_att_mask", "=", "prons_att_mask", ".", "to", "(", "device", ")", "\n", "\n", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "                ", "logits", "=", "model", "(", "input_ids", ",", "segment_ids", ",", "input_mask", ",", "prons_emb", ",", "prons_att_mask", ")", "\n", "\n", "", "logits", "=", "torch", ".", "argmax", "(", "F", ".", "log_softmax", "(", "logits", ",", "dim", "=", "2", ")", ",", "dim", "=", "2", ")", "\n", "logits", "=", "logits", ".", "detach", "(", ")", ".", "cpu", "(", ")", ".", "numpy", "(", ")", "\n", "label_ids", "=", "label_ids", ".", "to", "(", "'cpu'", ")", ".", "numpy", "(", ")", "\n", "input_mask", "=", "input_mask", ".", "to", "(", "'cpu'", ")", ".", "numpy", "(", ")", "\n", "for", "i", ",", "mask", "in", "enumerate", "(", "input_mask", ")", ":", "\n", "                ", "temp_1", "=", "[", "]", "\n", "temp_2", "=", "[", "]", "\n", "for", "j", ",", "m", "in", "enumerate", "(", "mask", ")", ":", "\n", "                    ", "if", "j", "==", "0", ":", "\n", "                        ", "continue", "\n", "", "if", "m", "and", "label_map", "[", "label_ids", "[", "i", "]", "[", "j", "]", "]", "!=", "\"X\"", ":", "\n", "                        ", "temp_1", ".", "append", "(", "label_map", "[", "label_ids", "[", "i", "]", "[", "j", "]", "]", ")", "\n", "temp_2", ".", "append", "(", "label_map", "[", "logits", "[", "i", "]", "[", "j", "]", "]", ")", "\n", "", "else", ":", "\n", "                        ", "temp_1", ".", "pop", "(", ")", "\n", "temp_2", ".", "pop", "(", ")", "\n", "y_true", ".", "append", "(", "temp_1", ")", "\n", "y_pred", ".", "append", "(", "temp_2", ")", "\n", "break", "\n", "", "", "", "", "report", "=", "classification_report", "(", "y_true", ",", "y_pred", ",", "digits", "=", "4", ")", "\n", "output_eval_file", "=", "os", ".", "path", ".", "join", "(", "args", ".", "output_dir", ",", "\"eval_results.txt\"", ")", "\n", "with", "open", "(", "output_eval_file", ",", "\"w\"", ")", "as", "writer", ":", "\n", "            ", "logger", ".", "info", "(", "\"***** Eval results *****\"", ")", "\n", "logger", ".", "info", "(", "\"\\n%s\"", ",", "report", ")", "\n", "writer", ".", "write", "(", "report", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.lawsonabs_pun.None.file_utils.url_to_filename": [[39, 55], ["url.encode", "hashlib.sha256", "hashlib.sha256.hexdigest", "etag.encode", "hashlib.sha256", "hashlib.sha256.hexdigest"], "function", ["None"], ["def", "url_to_filename", "(", "url", ",", "etag", "=", "None", ")", ":", "\n", "    ", "\"\"\"\n    Convert `url` into a hashed filename in a repeatable way.\n    If `etag` is specified, append its hash to the url's, delimited\n    by a period.\n    \"\"\"", "\n", "url_bytes", "=", "url", ".", "encode", "(", "'utf-8'", ")", "\n", "url_hash", "=", "sha256", "(", "url_bytes", ")", "\n", "filename", "=", "url_hash", ".", "hexdigest", "(", ")", "\n", "\n", "if", "etag", ":", "\n", "        ", "etag_bytes", "=", "etag", ".", "encode", "(", "'utf-8'", ")", "\n", "etag_hash", "=", "sha256", "(", "etag_bytes", ")", "\n", "filename", "+=", "'.'", "+", "etag_hash", ".", "hexdigest", "(", ")", "\n", "\n", "", "return", "filename", "\n", "\n"]], "home.repos.pwc.inspect_result.lawsonabs_pun.None.file_utils.filename_to_url": [[57, 81], ["os.path.join", "isinstance", "str", "os.path.exists", "EnvironmentError", "os.path.exists", "EnvironmentError", "io.open", "json.load"], "function", ["None"], ["", "def", "filename_to_url", "(", "filename", ",", "cache_dir", "=", "None", ")", ":", "\n", "    ", "\"\"\"\n    Return the url and etag (which may be ``None``) stored for `filename`.\n    Raise ``EnvironmentError`` if `filename` or its stored metadata do not exist.\n    \"\"\"", "\n", "if", "cache_dir", "is", "None", ":", "\n", "        ", "cache_dir", "=", "PYTORCH_PRETRAINED_BERT_CACHE", "\n", "", "if", "sys", ".", "version_info", "[", "0", "]", "==", "3", "and", "isinstance", "(", "cache_dir", ",", "Path", ")", ":", "\n", "        ", "cache_dir", "=", "str", "(", "cache_dir", ")", "\n", "\n", "", "cache_path", "=", "os", ".", "path", ".", "join", "(", "cache_dir", ",", "filename", ")", "\n", "if", "not", "os", ".", "path", ".", "exists", "(", "cache_path", ")", ":", "\n", "        ", "raise", "EnvironmentError", "(", "\"file {} not found\"", ".", "format", "(", "cache_path", ")", ")", "\n", "\n", "", "meta_path", "=", "cache_path", "+", "'.json'", "\n", "if", "not", "os", ".", "path", ".", "exists", "(", "meta_path", ")", ":", "\n", "        ", "raise", "EnvironmentError", "(", "\"file {} not found\"", ".", "format", "(", "meta_path", ")", ")", "\n", "\n", "", "with", "open", "(", "meta_path", ",", "encoding", "=", "\"utf-8\"", ")", "as", "meta_file", ":", "\n", "        ", "metadata", "=", "json", ".", "load", "(", "meta_file", ")", "\n", "", "url", "=", "metadata", "[", "'url'", "]", "\n", "etag", "=", "metadata", "[", "'etag'", "]", "\n", "\n", "return", "url", ",", "etag", "\n", "\n"]], "home.repos.pwc.inspect_result.lawsonabs_pun.None.file_utils.cached_path": [[83, 111], ["urlparse", "isinstance", "str", "isinstance", "str", "file_utils.get_from_cache", "os.path.exists", "EnvironmentError", "ValueError"], "function", ["home.repos.pwc.inspect_result.lawsonabs_pun.None.file_utils.get_from_cache"], ["", "def", "cached_path", "(", "url_or_filename", ",", "cache_dir", "=", "None", ")", ":", "\n", "    ", "\"\"\"\n    Given something that might be a URL (or might be a local path),\n    determine which. If it's a URL, download the file and cache it, and\n    return the path to the cached file. If it's already a local path,\n    make sure the file exists and then return the path.\n    \"\"\"", "\n", "if", "cache_dir", "is", "None", ":", "\n", "        ", "cache_dir", "=", "PYTORCH_PRETRAINED_BERT_CACHE", "\n", "", "if", "sys", ".", "version_info", "[", "0", "]", "==", "3", "and", "isinstance", "(", "url_or_filename", ",", "Path", ")", ":", "\n", "        ", "url_or_filename", "=", "str", "(", "url_or_filename", ")", "\n", "", "if", "sys", ".", "version_info", "[", "0", "]", "==", "3", "and", "isinstance", "(", "cache_dir", ",", "Path", ")", ":", "\n", "        ", "cache_dir", "=", "str", "(", "cache_dir", ")", "\n", "\n", "", "parsed", "=", "urlparse", "(", "url_or_filename", ")", "\n", "\n", "if", "parsed", ".", "scheme", "in", "(", "'http'", ",", "'https'", ",", "'s3'", ")", ":", "\n", "# URL, so get it from the cache (downloading if necessary)", "\n", "        ", "return", "get_from_cache", "(", "url_or_filename", ",", "cache_dir", ")", "\n", "", "elif", "os", ".", "path", ".", "exists", "(", "url_or_filename", ")", ":", "\n", "# File, and it exists.", "\n", "        ", "return", "url_or_filename", "\n", "", "elif", "parsed", ".", "scheme", "==", "''", ":", "\n", "# File, but it doesn't exist.", "\n", "        ", "raise", "EnvironmentError", "(", "\"file {} not found\"", ".", "format", "(", "url_or_filename", ")", ")", "\n", "", "else", ":", "\n", "# Something unknown", "\n", "        ", "raise", "ValueError", "(", "\"unable to parse {} as a URL or as a local path\"", ".", "format", "(", "url_or_filename", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.lawsonabs_pun.None.file_utils.split_s3_path": [[113, 124], ["urlparse", "s3_path.startswith", "ValueError"], "function", ["None"], ["", "", "def", "split_s3_path", "(", "url", ")", ":", "\n", "    ", "\"\"\"Split a full s3 path into the bucket name and path.\"\"\"", "\n", "parsed", "=", "urlparse", "(", "url", ")", "\n", "if", "not", "parsed", ".", "netloc", "or", "not", "parsed", ".", "path", ":", "\n", "        ", "raise", "ValueError", "(", "\"bad s3 path {}\"", ".", "format", "(", "url", ")", ")", "\n", "", "bucket_name", "=", "parsed", ".", "netloc", "\n", "s3_path", "=", "parsed", ".", "path", "\n", "# Remove '/' at beginning of path.", "\n", "if", "s3_path", ".", "startswith", "(", "\"/\"", ")", ":", "\n", "        ", "s3_path", "=", "s3_path", "[", "1", ":", "]", "\n", "", "return", "bucket_name", ",", "s3_path", "\n", "\n"]], "home.repos.pwc.inspect_result.lawsonabs_pun.None.file_utils.s3_request": [[126, 143], ["functools.wraps", "func", "int", "EnvironmentError"], "function", ["None"], ["", "def", "s3_request", "(", "func", ")", ":", "\n", "    ", "\"\"\"\n    Wrapper function for s3 requests in order to create more helpful error\n    messages.\n    \"\"\"", "\n", "\n", "@", "wraps", "(", "func", ")", "\n", "def", "wrapper", "(", "url", ",", "*", "args", ",", "**", "kwargs", ")", ":", "\n", "        ", "try", ":", "\n", "            ", "return", "func", "(", "url", ",", "*", "args", ",", "**", "kwargs", ")", "\n", "", "except", "ClientError", "as", "exc", ":", "\n", "            ", "if", "int", "(", "exc", ".", "response", "[", "\"Error\"", "]", "[", "\"Code\"", "]", ")", "==", "404", ":", "\n", "                ", "raise", "EnvironmentError", "(", "\"file {} not found\"", ".", "format", "(", "url", ")", ")", "\n", "", "else", ":", "\n", "                ", "raise", "\n", "\n", "", "", "", "return", "wrapper", "\n", "\n"]], "home.repos.pwc.inspect_result.lawsonabs_pun.None.file_utils.s3_etag": [[145, 152], ["boto3.resource", "file_utils.split_s3_path", "boto3.resource.Object"], "function", ["home.repos.pwc.inspect_result.lawsonabs_pun.None.file_utils.split_s3_path"], ["", "@", "s3_request", "\n", "def", "s3_etag", "(", "url", ")", ":", "\n", "    ", "\"\"\"Check ETag on S3 object.\"\"\"", "\n", "s3_resource", "=", "boto3", ".", "resource", "(", "\"s3\"", ")", "\n", "bucket_name", ",", "s3_path", "=", "split_s3_path", "(", "url", ")", "\n", "s3_object", "=", "s3_resource", ".", "Object", "(", "bucket_name", ",", "s3_path", ")", "\n", "return", "s3_object", ".", "e_tag", "\n", "\n"]], "home.repos.pwc.inspect_result.lawsonabs_pun.None.file_utils.s3_get": [[154, 160], ["boto3.resource", "file_utils.split_s3_path", "boto3.resource.Bucket().download_fileobj", "boto3.resource.Bucket"], "function", ["home.repos.pwc.inspect_result.lawsonabs_pun.None.file_utils.split_s3_path"], ["", "@", "s3_request", "\n", "def", "s3_get", "(", "url", ",", "temp_file", ")", ":", "\n", "    ", "\"\"\"Pull a file directly from S3.\"\"\"", "\n", "s3_resource", "=", "boto3", ".", "resource", "(", "\"s3\"", ")", "\n", "bucket_name", ",", "s3_path", "=", "split_s3_path", "(", "url", ")", "\n", "s3_resource", ".", "Bucket", "(", "bucket_name", ")", ".", "download_fileobj", "(", "s3_path", ",", "temp_file", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.lawsonabs_pun.None.file_utils.http_get": [[162, 172], ["requests.get", "requests.get.headers.get", "tqdm.tqdm", "requests.get.iter_content", "tqdm.tqdm.close", "int", "tqdm.tqdm.update", "temp_file.write", "len"], "function", ["None"], ["", "def", "http_get", "(", "url", ",", "temp_file", ")", ":", "\n", "    ", "req", "=", "requests", ".", "get", "(", "url", ",", "stream", "=", "True", ")", "\n", "content_length", "=", "req", ".", "headers", ".", "get", "(", "'Content-Length'", ")", "\n", "total", "=", "int", "(", "content_length", ")", "if", "content_length", "is", "not", "None", "else", "None", "\n", "progress", "=", "tqdm", "(", "unit", "=", "\"B\"", ",", "total", "=", "total", ")", "\n", "for", "chunk", "in", "req", ".", "iter_content", "(", "chunk_size", "=", "1024", ")", ":", "\n", "        ", "if", "chunk", ":", "# filter out keep-alive new chunks", "\n", "            ", "progress", ".", "update", "(", "len", "(", "chunk", ")", ")", "\n", "temp_file", ".", "write", "(", "chunk", ")", "\n", "", "", "progress", ".", "close", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.lawsonabs_pun.None.file_utils.get_from_cache": [[174, 232], ["url.startswith", "file_utils.url_to_filename", "os.path.join", "isinstance", "str", "os.path.exists", "os.makedirs", "file_utils.s3_etag", "requests.head", "requests.head.headers.get", "os.path.exists", "IOError", "tempfile.NamedTemporaryFile", "logger.info", "url.startswith", "temp_file.flush", "temp_file.seek", "logger.info", "logger.info", "logger.info", "file_utils.s3_get", "file_utils.http_get", "io.open", "shutil.copyfileobj", "io.open", "json.dump"], "function", ["home.repos.pwc.inspect_result.lawsonabs_pun.None.file_utils.url_to_filename", "home.repos.pwc.inspect_result.lawsonabs_pun.None.file_utils.s3_etag", "home.repos.pwc.inspect_result.lawsonabs_pun.None.file_utils.s3_get", "home.repos.pwc.inspect_result.lawsonabs_pun.None.file_utils.http_get"], ["", "def", "get_from_cache", "(", "url", ",", "cache_dir", "=", "None", ")", ":", "\n", "    ", "\"\"\"\n    Given a URL, look for the corresponding dataset in the local cache.\n    If it's not there, download it. Then return the path to the cached file.\n    \"\"\"", "\n", "if", "cache_dir", "is", "None", ":", "\n", "        ", "cache_dir", "=", "PYTORCH_PRETRAINED_BERT_CACHE", "\n", "", "if", "sys", ".", "version_info", "[", "0", "]", "==", "3", "and", "isinstance", "(", "cache_dir", ",", "Path", ")", ":", "\n", "        ", "cache_dir", "=", "str", "(", "cache_dir", ")", "\n", "\n", "", "if", "not", "os", ".", "path", ".", "exists", "(", "cache_dir", ")", ":", "\n", "        ", "os", ".", "makedirs", "(", "cache_dir", ")", "\n", "\n", "# Get eTag to add to filename, if it exists.", "\n", "", "if", "url", ".", "startswith", "(", "\"s3://\"", ")", ":", "\n", "        ", "etag", "=", "s3_etag", "(", "url", ")", "\n", "", "else", ":", "\n", "        ", "response", "=", "requests", ".", "head", "(", "url", ",", "allow_redirects", "=", "True", ")", "\n", "if", "response", ".", "status_code", "!=", "200", ":", "\n", "            ", "raise", "IOError", "(", "\"HEAD request failed for url {} with status code {}\"", "\n", ".", "format", "(", "url", ",", "response", ".", "status_code", ")", ")", "\n", "", "etag", "=", "response", ".", "headers", ".", "get", "(", "\"ETag\"", ")", "\n", "\n", "", "filename", "=", "url_to_filename", "(", "url", ",", "etag", ")", "\n", "\n", "# get cache path to put the file", "\n", "cache_path", "=", "os", ".", "path", ".", "join", "(", "cache_dir", ",", "filename", ")", "\n", "\n", "if", "not", "os", ".", "path", ".", "exists", "(", "cache_path", ")", ":", "\n", "# Download to temporary file, then copy to cache dir once finished.", "\n", "# Otherwise you get corrupt cache entries if the download gets interrupted.", "\n", "        ", "with", "tempfile", ".", "NamedTemporaryFile", "(", ")", "as", "temp_file", ":", "\n", "            ", "logger", ".", "info", "(", "\"%s not found in cache, downloading to %s\"", ",", "url", ",", "temp_file", ".", "name", ")", "\n", "\n", "# GET file object", "\n", "if", "url", ".", "startswith", "(", "\"s3://\"", ")", ":", "\n", "                ", "s3_get", "(", "url", ",", "temp_file", ")", "\n", "", "else", ":", "\n", "                ", "http_get", "(", "url", ",", "temp_file", ")", "\n", "\n", "# we are copying the file before closing it, so flush to avoid truncation", "\n", "", "temp_file", ".", "flush", "(", ")", "\n", "# shutil.copyfileobj() starts at the current position, so go to the start", "\n", "temp_file", ".", "seek", "(", "0", ")", "\n", "\n", "logger", ".", "info", "(", "\"copying %s to cache at %s\"", ",", "temp_file", ".", "name", ",", "cache_path", ")", "\n", "with", "open", "(", "cache_path", ",", "'wb'", ")", "as", "cache_file", ":", "\n", "                ", "shutil", ".", "copyfileobj", "(", "temp_file", ",", "cache_file", ")", "\n", "\n", "", "logger", ".", "info", "(", "\"creating metadata file for %s\"", ",", "cache_path", ")", "\n", "meta", "=", "{", "'url'", ":", "url", ",", "'etag'", ":", "etag", "}", "\n", "meta_path", "=", "cache_path", "+", "'.json'", "\n", "with", "open", "(", "meta_path", ",", "'w'", ",", "encoding", "=", "\"utf-8\"", ")", "as", "meta_file", ":", "\n", "                ", "json", ".", "dump", "(", "meta", ",", "meta_file", ")", "\n", "\n", "", "logger", ".", "info", "(", "\"removing temp file %s\"", ",", "temp_file", ".", "name", ")", "\n", "\n", "", "", "return", "cache_path", "\n", "\n"]], "home.repos.pwc.inspect_result.lawsonabs_pun.None.file_utils.read_set_from_file": [[234, 244], ["set", "io.open", "set.add", "line.rstrip"], "function", ["None"], ["", "def", "read_set_from_file", "(", "filename", ")", ":", "\n", "    ", "'''\n    Extract a de-duped collection (set) of text from a file.\n    Expected file format is one item per line.\n    '''", "\n", "collection", "=", "set", "(", ")", "\n", "with", "open", "(", "filename", ",", "'r'", ",", "encoding", "=", "'utf-8'", ")", "as", "file_", ":", "\n", "        ", "for", "line", "in", "file_", ":", "\n", "            ", "collection", ".", "add", "(", "line", ".", "rstrip", "(", ")", ")", "\n", "", "", "return", "collection", "\n", "\n"]], "home.repos.pwc.inspect_result.lawsonabs_pun.None.file_utils.get_file_extension": [[246, 250], ["os.path.splitext", "ext.lower"], "function", ["None"], ["", "def", "get_file_extension", "(", "path", ",", "dot", "=", "True", ",", "lower", "=", "True", ")", ":", "\n", "    ", "ext", "=", "os", ".", "path", ".", "splitext", "(", "path", ")", "[", "1", "]", "\n", "ext", "=", "ext", "if", "dot", "else", "ext", "[", "1", ":", "]", "\n", "return", "ext", ".", "lower", "(", ")", "if", "lower", "else", "ext", "", "", ""]], "home.repos.pwc.inspect_result.lawsonabs_pun.None.local_attention.Local_attention.__init__": [[16, 24], ["torch.Module.__init__", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Softmax", "torch.Softmax", "torch.Tanh", "torch.Tanh"], "methods", ["home.repos.pwc.inspect_result.lawsonabs_pun.subtask3_v2.gloss.EvalDataset.__init__"], ["    ", "def", "__init__", "(", "self", ",", "dim_in", ",", "dim_out", ")", ":", "\n", "        ", "super", "(", "Local_attention", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "dim_in", "=", "dim_in", "\n", "self", ".", "dim_out", "=", "dim_out", "\n", "self", ".", "linear_in", "=", "nn", ".", "Linear", "(", "dim_in", ",", "dim_out", ",", "bias", "=", "False", ")", "\n", "self", ".", "linear_out", "=", "nn", ".", "Linear", "(", "dim_out", ",", "dim_in", ",", "bias", "=", "False", ")", "\n", "self", ".", "softmax", "=", "nn", ".", "Softmax", "(", "dim", "=", "1", ")", "\n", "self", ".", "tanh", "=", "nn", ".", "Tanh", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.lawsonabs_pun.None.local_attention.Local_attention.masked_softmax": [[26, 29], ["local_attention.Local_attention.softmax"], "methods", ["None"], ["", "def", "masked_softmax", "(", "self", ",", "T", ")", ":", "\n", "        ", "T", "[", "T", "==", "0", "]", "=", "-", "10", "^", "20", "\n", "return", "self", ".", "softmax", "(", "T", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.lawsonabs_pun.None.local_attention.Local_attention.forward": [[30, 62], ["context.view.view.size", "context.view.view.view", "local_attention.Local_attention.linear_in", "context.view.view.view", "context.view.view.matmul", "local_attention.Local_attention.masked_softmax", "torch.bmm", "torch.bmm", "torch.bmm", "torch.bmm", "mix.view.view.view", "attention_weights.view.view.view", "local_attention.Local_attention.linear_out", "local_attention.Local_attention.tanh", "attention_weights.view.view.transpose().contiguous", "attention_weights.view.view.transpose"], "methods", ["home.repos.pwc.inspect_result.lawsonabs_pun.None.attention.Attention.masked_softmax"], ["", "def", "forward", "(", "self", ",", "context", ",", "att_vec", ")", ":", "# att_vec size = [pron_emb_size * 2, 1]", "\n", "        ", "\"\"\"\n        Args:\n            context (:class:`torch.FloatTensor` [batch size, query length, dimensions]): Data\n                overwhich to apply the attention mechanism.  \u4f1a\u88ab\u5e94\u7528 attention \u65b9\u6cd5 \u7684\u6570\u636e\n                \u5728\u53d1\u97f3\u6a21\u5757\u4e2d\uff0c\u8fd9\u4e2acontext  \u5c31\u662f\u6574\u4e2a\u53d1\u97f3\u7684\u6570\u636e\uff1b\u5728sense\u6a21\u5757\u4e2d\uff0c\u8fd9\u4e2acontext \u5c31\u662f\u6240\u6709\u7684sense\u5b9a\u4e49\u6570\u636e\n        Returns:\n            :class:`tuple` with `output` and `weights`:\n            * **output** (:class:`torch.LongTensor` [batch size, output length, dimensions]):\n              Tensor containing the attended features.\n            * **weights** (:class:`torch.FloatTensor` [batch size, output length, query length]):\n              Tensor containing attention weights.\n        \"\"\"", "\n", "batch_size", ",", "pron_len", ",", "dimensions", "=", "context", ".", "size", "(", ")", "# [4096, 5, 16]", "\n", "context", "=", "context", ".", "view", "(", "batch_size", "*", "pron_len", ",", "dimensions", ")", "# [20480,16]", "\n", "context", "=", "self", ".", "linear_in", "(", "context", ")", "\n", "context", "=", "context", ".", "view", "(", "batch_size", ",", "pron_len", ",", "self", ".", "dim_out", ")", "# [4096,5,32]", "\n", "\n", "# \u8ba1\u7b97 attention score => \u7136\u540e\u4f7f\u7528softmax \u5f97\u5230\u5404\u4e2a\u90e8\u5206\u7684\u6743\u91cd", "\n", "attention_scores", "=", "context", ".", "matmul", "(", "att_vec", ")", "# [4096, 5, 1]", "\n", "attention_weights", "=", "self", ".", "masked_softmax", "(", "attention_scores", ")", "\n", "# batch mm()", "\n", "mix", "=", "torch", ".", "bmm", "(", "attention_weights", ".", "transpose", "(", "1", ",", "2", ")", ".", "contiguous", "(", ")", ",", "context", ")", "\n", "mix", "=", "mix", ".", "view", "(", "batch_size", ",", "self", ".", "dim_out", ")", "# [4096,32]", "\n", "attention_weights", "=", "attention_weights", ".", "view", "(", "batch_size", ",", "pron_len", ")", "# [4096,5]", "\n", "\n", "# Apply linear_out on every 2nd dimension of concat", "\n", "# output -> (batch_size, output_len, dimensions)", "\n", "output", "=", "self", ".", "linear_out", "(", "mix", ")", "\n", "output", "=", "self", ".", "tanh", "(", "output", ")", "\n", "\n", "return", "output", ",", "attention_weights", "# [4096,16], [4096,5]", "\n", "", "", ""]], "home.repos.pwc.inspect_result.lawsonabs_pun.None.bert_models.BertConfig.__init__": [[139, 194], ["isinstance", "json.loads.items", "isinstance", "isinstance", "io.open", "json.loads", "ValueError", "reader.read"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "\n", "vocab_size_or_config_json_file", ",", "\n", "hidden_size", "=", "768", ",", "\n", "num_hidden_layers", "=", "12", ",", "\n", "num_attention_heads", "=", "12", ",", "\n", "intermediate_size", "=", "3072", ",", "\n", "hidden_act", "=", "\"gelu\"", ",", "\n", "hidden_dropout_prob", "=", "0.1", ",", "\n", "attention_probs_dropout_prob", "=", "0.1", ",", "\n", "max_position_embeddings", "=", "512", ",", "\n", "type_vocab_size", "=", "2", ",", "\n", "initializer_range", "=", "0.02", ")", ":", "\n", "        ", "\"\"\"Constructs BertConfig.\n        Args:\n            vocab_size_or_config_json_file: Vocabulary size of `inputs_ids` in `BertModel`.\n            hidden_size: Size of the encoder layers and the pooler layer.\n            num_hidden_layers: Number of hidden layers in the Transformer encoder.\n            num_attention_heads: Number of attention heads for each attention layer in\n                the Transformer encoder.\n            intermediate_size: The size of the \"intermediate\" (i.e., feed-forward)\n                layer in the Transformer encoder.\n            hidden_act: The non-linear activation function (function or string) in the\n                encoder and pooler. If string, \"gelu\", \"relu\" and \"swish\" are supported.\n            hidden_dropout_prob: The dropout probabilitiy for all fully connected\n                layers in the embeddings, encoder, and pooler.\n            attention_probs_dropout_prob: The dropout ratio for the attention\n                probabilities.\n            max_position_embeddings: The maximum sequence length that this model might\n                ever be used with. Typically set this to something large just in case\n                (e.g., 512 or 1024 or 2048).\n            type_vocab_size: The vocabulary size of the `token_type_ids` passed into\n                `BertModel`.\n            initializer_range: The sttdev of the truncated_normal_initializer for\n                initializing all weight matrices.\n        \"\"\"", "\n", "if", "isinstance", "(", "vocab_size_or_config_json_file", ",", "str", ")", "or", "(", "sys", ".", "version_info", "[", "0", "]", "==", "2", "\n", "and", "isinstance", "(", "vocab_size_or_config_json_file", ",", "unicode", ")", ")", ":", "\n", "            ", "with", "open", "(", "vocab_size_or_config_json_file", ",", "\"r\"", ",", "encoding", "=", "'utf-8'", ")", "as", "reader", ":", "\n", "                ", "json_config", "=", "json", ".", "loads", "(", "reader", ".", "read", "(", ")", ")", "\n", "", "for", "key", ",", "value", "in", "json_config", ".", "items", "(", ")", ":", "\n", "                ", "self", ".", "__dict__", "[", "key", "]", "=", "value", "\n", "", "", "elif", "isinstance", "(", "vocab_size_or_config_json_file", ",", "int", ")", ":", "\n", "            ", "self", ".", "vocab_size", "=", "vocab_size_or_config_json_file", "\n", "self", ".", "hidden_size", "=", "hidden_size", "\n", "self", ".", "num_hidden_layers", "=", "num_hidden_layers", "\n", "self", ".", "num_attention_heads", "=", "num_attention_heads", "\n", "self", ".", "hidden_act", "=", "hidden_act", "\n", "self", ".", "intermediate_size", "=", "intermediate_size", "\n", "self", ".", "hidden_dropout_prob", "=", "hidden_dropout_prob", "\n", "self", ".", "attention_probs_dropout_prob", "=", "attention_probs_dropout_prob", "\n", "self", ".", "max_position_embeddings", "=", "max_position_embeddings", "\n", "self", ".", "type_vocab_size", "=", "type_vocab_size", "\n", "self", ".", "initializer_range", "=", "initializer_range", "\n", "", "else", ":", "\n", "            ", "raise", "ValueError", "(", "\"First argument must be either a vocabulary size (int)\"", "\n", "\"or the path to a pretrained model config file (str)\"", ")", "\n"]], "home.repos.pwc.inspect_result.lawsonabs_pun.None.bert_models.BertConfig.from_dict": [[196, 203], ["bert_models.BertConfig", "json_object.items"], "methods", ["None"], ["", "", "@", "classmethod", "\n", "def", "from_dict", "(", "cls", ",", "json_object", ")", ":", "\n", "        ", "\"\"\"Constructs a `BertConfig` from a Python dictionary of parameters.\"\"\"", "\n", "config", "=", "BertConfig", "(", "vocab_size_or_config_json_file", "=", "-", "1", ")", "\n", "for", "key", ",", "value", "in", "json_object", ".", "items", "(", ")", ":", "\n", "            ", "config", ".", "__dict__", "[", "key", "]", "=", "value", "\n", "", "return", "config", "\n", "\n"]], "home.repos.pwc.inspect_result.lawsonabs_pun.None.bert_models.BertConfig.from_json_file": [[204, 210], ["cls.from_dict", "io.open", "reader.read", "json.loads"], "methods", ["home.repos.pwc.inspect_result.lawsonabs_pun.None.bert_models.BertConfig.from_dict"], ["", "@", "classmethod", "\n", "def", "from_json_file", "(", "cls", ",", "json_file", ")", ":", "\n", "        ", "\"\"\"Constructs a `BertConfig` from a json file of parameters.\"\"\"", "\n", "with", "open", "(", "json_file", ",", "\"r\"", ",", "encoding", "=", "'utf-8'", ")", "as", "reader", ":", "\n", "            ", "text", "=", "reader", ".", "read", "(", ")", "\n", "", "return", "cls", ".", "from_dict", "(", "json", ".", "loads", "(", "text", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.lawsonabs_pun.None.bert_models.BertConfig.__repr__": [[211, 213], ["str", "bert_models.BertConfig.to_json_string"], "methods", ["home.repos.pwc.inspect_result.lawsonabs_pun.None.bert_models.BertConfig.to_json_string"], ["", "def", "__repr__", "(", "self", ")", ":", "\n", "        ", "return", "str", "(", "self", ".", "to_json_string", "(", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.lawsonabs_pun.None.bert_models.BertConfig.to_dict": [[214, 218], ["copy.deepcopy"], "methods", ["None"], ["", "def", "to_dict", "(", "self", ")", ":", "\n", "        ", "\"\"\"Serializes this instance to a Python dictionary.\"\"\"", "\n", "output", "=", "copy", ".", "deepcopy", "(", "self", ".", "__dict__", ")", "\n", "return", "output", "\n", "\n"]], "home.repos.pwc.inspect_result.lawsonabs_pun.None.bert_models.BertConfig.to_json_string": [[219, 222], ["json.dumps", "bert_models.BertConfig.to_dict"], "methods", ["home.repos.pwc.inspect_result.lawsonabs_pun.None.bert_models.BertConfig.to_dict"], ["", "def", "to_json_string", "(", "self", ")", ":", "\n", "        ", "\"\"\"Serializes this instance to a JSON string.\"\"\"", "\n", "return", "json", ".", "dumps", "(", "self", ".", "to_dict", "(", ")", ",", "indent", "=", "2", ",", "sort_keys", "=", "True", ")", "+", "\"\\n\"", "\n", "\n"]], "home.repos.pwc.inspect_result.lawsonabs_pun.None.bert_models.BertEmbeddings.__init__": [[245, 255], ["torch.nn.Module.__init__", "torch.nn.Embedding", "torch.nn.Embedding", "torch.nn.Embedding", "torch.nn.Embedding", "torch.nn.Embedding", "torch.nn.Embedding", "BertLayerNorm", "torch.nn.Dropout", "torch.nn.Dropout"], "methods", ["home.repos.pwc.inspect_result.lawsonabs_pun.subtask3_v2.gloss.EvalDataset.__init__"], ["def", "__init__", "(", "self", ",", "config", ")", ":", "\n", "        ", "super", "(", "BertEmbeddings", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "word_embeddings", "=", "nn", ".", "Embedding", "(", "config", ".", "vocab_size", ",", "config", ".", "hidden_size", ")", "\n", "self", ".", "position_embeddings", "=", "nn", ".", "Embedding", "(", "config", ".", "max_position_embeddings", ",", "config", ".", "hidden_size", ")", "\n", "self", ".", "token_type_embeddings", "=", "nn", ".", "Embedding", "(", "config", ".", "type_vocab_size", ",", "config", ".", "hidden_size", ")", "\n", "\n", "# self.LayerNorm is not snake-cased to stick with TensorFlow model variable name and be able to load", "\n", "# any TensorFlow checkpoint file", "\n", "self", ".", "LayerNorm", "=", "BertLayerNorm", "(", "config", ".", "hidden_size", ",", "eps", "=", "1e-12", ")", "\n", "self", ".", "dropout", "=", "nn", ".", "Dropout", "(", "config", ".", "hidden_dropout_prob", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.lawsonabs_pun.None.bert_models.BertEmbeddings.forward": [[256, 271], ["input_ids.size", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "position_ids.unsqueeze().expand_as.unsqueeze().expand_as.unsqueeze().expand_as", "bert_models.BertEmbeddings.word_embeddings", "bert_models.BertEmbeddings.position_embeddings", "bert_models.BertEmbeddings.token_type_embeddings", "bert_models.BertEmbeddings.LayerNorm", "bert_models.BertEmbeddings.dropout", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "position_ids.unsqueeze().expand_as.unsqueeze().expand_as.unsqueeze"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "input_ids", ",", "token_type_ids", "=", "None", ")", ":", "\n", "        ", "seq_length", "=", "input_ids", ".", "size", "(", "1", ")", "\n", "position_ids", "=", "torch", ".", "arange", "(", "seq_length", ",", "dtype", "=", "torch", ".", "long", ",", "device", "=", "input_ids", ".", "device", ")", "\n", "position_ids", "=", "position_ids", ".", "unsqueeze", "(", "0", ")", ".", "expand_as", "(", "input_ids", ")", "\n", "if", "token_type_ids", "is", "None", ":", "\n", "            ", "token_type_ids", "=", "torch", ".", "zeros_like", "(", "input_ids", ")", "\n", "\n", "", "words_embeddings", "=", "self", ".", "word_embeddings", "(", "input_ids", ")", "\n", "position_embeddings", "=", "self", ".", "position_embeddings", "(", "position_ids", ")", "\n", "token_type_embeddings", "=", "self", ".", "token_type_embeddings", "(", "token_type_ids", ")", "\n", "\n", "embeddings", "=", "words_embeddings", "+", "position_embeddings", "+", "token_type_embeddings", "\n", "embeddings", "=", "self", ".", "LayerNorm", "(", "embeddings", ")", "\n", "embeddings", "=", "self", ".", "dropout", "(", "embeddings", ")", "\n", "return", "embeddings", "\n", "\n"]], "home.repos.pwc.inspect_result.lawsonabs_pun.None.bert_models.BertSelfAttention.__init__": [[274, 289], ["torch.nn.Module.__init__", "int", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Dropout", "torch.nn.Dropout", "ValueError"], "methods", ["home.repos.pwc.inspect_result.lawsonabs_pun.subtask3_v2.gloss.EvalDataset.__init__"], ["    ", "def", "__init__", "(", "self", ",", "config", ")", ":", "\n", "        ", "super", "(", "BertSelfAttention", ",", "self", ")", ".", "__init__", "(", ")", "\n", "if", "config", ".", "hidden_size", "%", "config", ".", "num_attention_heads", "!=", "0", ":", "\n", "            ", "raise", "ValueError", "(", "\n", "\"The hidden size (%d) is not a multiple of the number of attention \"", "\n", "\"heads (%d)\"", "%", "(", "config", ".", "hidden_size", ",", "config", ".", "num_attention_heads", ")", ")", "\n", "", "self", ".", "num_attention_heads", "=", "config", ".", "num_attention_heads", "\n", "self", ".", "attention_head_size", "=", "int", "(", "config", ".", "hidden_size", "/", "config", ".", "num_attention_heads", ")", "\n", "self", ".", "all_head_size", "=", "self", ".", "num_attention_heads", "*", "self", ".", "attention_head_size", "\n", "\n", "self", ".", "query", "=", "nn", ".", "Linear", "(", "config", ".", "hidden_size", ",", "self", ".", "all_head_size", ")", "\n", "self", ".", "key", "=", "nn", ".", "Linear", "(", "config", ".", "hidden_size", ",", "self", ".", "all_head_size", ")", "\n", "self", ".", "value", "=", "nn", ".", "Linear", "(", "config", ".", "hidden_size", ",", "self", ".", "all_head_size", ")", "\n", "\n", "self", ".", "dropout", "=", "nn", ".", "Dropout", "(", "config", ".", "attention_probs_dropout_prob", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.lawsonabs_pun.None.bert_models.BertSelfAttention.transpose_for_scores": [[290, 294], ["x.view.view.view", "x.view.view.permute", "x.view.view.size"], "methods", ["None"], ["", "def", "transpose_for_scores", "(", "self", ",", "x", ")", ":", "\n", "        ", "new_x_shape", "=", "x", ".", "size", "(", ")", "[", ":", "-", "1", "]", "+", "(", "self", ".", "num_attention_heads", ",", "self", ".", "attention_head_size", ")", "\n", "x", "=", "x", ".", "view", "(", "*", "new_x_shape", ")", "\n", "return", "x", ".", "permute", "(", "0", ",", "2", ",", "1", ",", "3", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.lawsonabs_pun.None.bert_models.BertSelfAttention.forward": [[295, 322], ["bert_models.BertSelfAttention.query", "bert_models.BertSelfAttention.key", "bert_models.BertSelfAttention.value", "bert_models.BertSelfAttention.transpose_for_scores", "bert_models.BertSelfAttention.transpose_for_scores", "bert_models.BertSelfAttention.transpose_for_scores", "torch.matmul", "torch.matmul", "torch.matmul", "torch.matmul", "bert_models.BertSelfAttention.dropout", "torch.matmul", "torch.matmul", "torch.matmul", "torch.matmul", "context_layer.view.view.permute().contiguous", "context_layer.view.view.view", "bert_models.BertSelfAttention.transpose", "math.sqrt", "torch.nn.Softmax", "torch.nn.Softmax", "context_layer.view.view.permute", "context_layer.view.view.size"], "methods", ["home.repos.pwc.inspect_result.lawsonabs_pun.None.bert_models.BertSelfAttention.transpose_for_scores", "home.repos.pwc.inspect_result.lawsonabs_pun.None.bert_models.BertSelfAttention.transpose_for_scores", "home.repos.pwc.inspect_result.lawsonabs_pun.None.bert_models.BertSelfAttention.transpose_for_scores"], ["", "def", "forward", "(", "self", ",", "hidden_states", ",", "attention_mask", ")", ":", "\n", "        ", "mixed_query_layer", "=", "self", ".", "query", "(", "hidden_states", ")", "\n", "mixed_key_layer", "=", "self", ".", "key", "(", "hidden_states", ")", "\n", "mixed_value_layer", "=", "self", ".", "value", "(", "hidden_states", ")", "\n", "\n", "query_layer", "=", "self", ".", "transpose_for_scores", "(", "mixed_query_layer", ")", "\n", "key_layer", "=", "self", ".", "transpose_for_scores", "(", "mixed_key_layer", ")", "\n", "value_layer", "=", "self", ".", "transpose_for_scores", "(", "mixed_value_layer", ")", "\n", "\n", "# Take the dot product between \"query\" and \"key\" to get the raw attention scores.", "\n", "attention_scores", "=", "torch", ".", "matmul", "(", "query_layer", ",", "key_layer", ".", "transpose", "(", "-", "1", ",", "-", "2", ")", ")", "\n", "attention_scores", "=", "attention_scores", "/", "math", ".", "sqrt", "(", "self", ".", "attention_head_size", ")", "\n", "# Apply the attention mask is (precomputed for all layers in BertModel forward() function)", "\n", "attention_scores", "=", "attention_scores", "+", "attention_mask", "\n", "\n", "# Normalize the attention scores to probabilities.", "\n", "attention_probs", "=", "nn", ".", "Softmax", "(", "dim", "=", "-", "1", ")", "(", "attention_scores", ")", "\n", "\n", "# This is actually dropping out entire tokens to attend to, which might", "\n", "# seem a bit unusual, but is taken from the original Transformer paper.", "\n", "attention_probs", "=", "self", ".", "dropout", "(", "attention_probs", ")", "\n", "\n", "context_layer", "=", "torch", ".", "matmul", "(", "attention_probs", ",", "value_layer", ")", "\n", "context_layer", "=", "context_layer", ".", "permute", "(", "0", ",", "2", ",", "1", ",", "3", ")", ".", "contiguous", "(", ")", "\n", "new_context_layer_shape", "=", "context_layer", ".", "size", "(", ")", "[", ":", "-", "2", "]", "+", "(", "self", ".", "all_head_size", ",", ")", "\n", "context_layer", "=", "context_layer", ".", "view", "(", "*", "new_context_layer_shape", ")", "\n", "return", "context_layer", "\n", "\n"]], "home.repos.pwc.inspect_result.lawsonabs_pun.None.bert_models.BertSelfOutput.__init__": [[325, 330], ["torch.nn.Module.__init__", "torch.nn.Linear", "torch.nn.Linear", "BertLayerNorm", "torch.nn.Dropout", "torch.nn.Dropout"], "methods", ["home.repos.pwc.inspect_result.lawsonabs_pun.subtask3_v2.gloss.EvalDataset.__init__"], ["    ", "def", "__init__", "(", "self", ",", "config", ")", ":", "\n", "        ", "super", "(", "BertSelfOutput", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "dense", "=", "nn", ".", "Linear", "(", "config", ".", "hidden_size", ",", "config", ".", "hidden_size", ")", "\n", "self", ".", "LayerNorm", "=", "BertLayerNorm", "(", "config", ".", "hidden_size", ",", "eps", "=", "1e-12", ")", "\n", "self", ".", "dropout", "=", "nn", ".", "Dropout", "(", "config", ".", "hidden_dropout_prob", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.lawsonabs_pun.None.bert_models.BertSelfOutput.forward": [[331, 336], ["bert_models.BertSelfOutput.dense", "bert_models.BertSelfOutput.dropout", "bert_models.BertSelfOutput.LayerNorm"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "hidden_states", ",", "input_tensor", ")", ":", "\n", "        ", "hidden_states", "=", "self", ".", "dense", "(", "hidden_states", ")", "\n", "hidden_states", "=", "self", ".", "dropout", "(", "hidden_states", ")", "\n", "hidden_states", "=", "self", ".", "LayerNorm", "(", "hidden_states", "+", "input_tensor", ")", "\n", "return", "hidden_states", "\n", "\n"]], "home.repos.pwc.inspect_result.lawsonabs_pun.None.bert_models.BertAttention.__init__": [[339, 343], ["torch.nn.Module.__init__", "bert_models.BertSelfAttention", "bert_models.BertSelfOutput"], "methods", ["home.repos.pwc.inspect_result.lawsonabs_pun.subtask3_v2.gloss.EvalDataset.__init__"], ["    ", "def", "__init__", "(", "self", ",", "config", ")", ":", "\n", "        ", "super", "(", "BertAttention", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "self", "=", "BertSelfAttention", "(", "config", ")", "\n", "self", ".", "output", "=", "BertSelfOutput", "(", "config", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.lawsonabs_pun.None.bert_models.BertAttention.forward": [[344, 348], ["bert_models.BertAttention.self", "bert_models.BertAttention.output"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "input_tensor", ",", "attention_mask", ")", ":", "\n", "        ", "self_output", "=", "self", ".", "self", "(", "input_tensor", ",", "attention_mask", ")", "\n", "attention_output", "=", "self", ".", "output", "(", "self_output", ",", "input_tensor", ")", "\n", "return", "attention_output", "\n", "\n"]], "home.repos.pwc.inspect_result.lawsonabs_pun.None.bert_models.BertIntermediate.__init__": [[351, 358], ["torch.nn.Module.__init__", "torch.nn.Linear", "torch.nn.Linear", "isinstance", "isinstance"], "methods", ["home.repos.pwc.inspect_result.lawsonabs_pun.subtask3_v2.gloss.EvalDataset.__init__"], ["    ", "def", "__init__", "(", "self", ",", "config", ")", ":", "\n", "        ", "super", "(", "BertIntermediate", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "dense", "=", "nn", ".", "Linear", "(", "config", ".", "hidden_size", ",", "config", ".", "intermediate_size", ")", "\n", "if", "isinstance", "(", "config", ".", "hidden_act", ",", "str", ")", "or", "(", "sys", ".", "version_info", "[", "0", "]", "==", "2", "and", "isinstance", "(", "config", ".", "hidden_act", ",", "unicode", ")", ")", ":", "\n", "            ", "self", ".", "intermediate_act_fn", "=", "ACT2FN", "[", "config", ".", "hidden_act", "]", "\n", "", "else", ":", "\n", "            ", "self", ".", "intermediate_act_fn", "=", "config", ".", "hidden_act", "\n", "\n"]], "home.repos.pwc.inspect_result.lawsonabs_pun.None.bert_models.BertIntermediate.forward": [[359, 363], ["bert_models.BertIntermediate.dense", "bert_models.BertIntermediate.intermediate_act_fn"], "methods", ["None"], ["", "", "def", "forward", "(", "self", ",", "hidden_states", ")", ":", "\n", "        ", "hidden_states", "=", "self", ".", "dense", "(", "hidden_states", ")", "\n", "hidden_states", "=", "self", ".", "intermediate_act_fn", "(", "hidden_states", ")", "\n", "return", "hidden_states", "\n", "\n"]], "home.repos.pwc.inspect_result.lawsonabs_pun.None.bert_models.BertOutput.__init__": [[366, 371], ["torch.nn.Module.__init__", "torch.nn.Linear", "torch.nn.Linear", "BertLayerNorm", "torch.nn.Dropout", "torch.nn.Dropout"], "methods", ["home.repos.pwc.inspect_result.lawsonabs_pun.subtask3_v2.gloss.EvalDataset.__init__"], ["    ", "def", "__init__", "(", "self", ",", "config", ")", ":", "\n", "        ", "super", "(", "BertOutput", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "dense", "=", "nn", ".", "Linear", "(", "config", ".", "intermediate_size", ",", "config", ".", "hidden_size", ")", "\n", "self", ".", "LayerNorm", "=", "BertLayerNorm", "(", "config", ".", "hidden_size", ",", "eps", "=", "1e-12", ")", "\n", "self", ".", "dropout", "=", "nn", ".", "Dropout", "(", "config", ".", "hidden_dropout_prob", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.lawsonabs_pun.None.bert_models.BertOutput.forward": [[372, 377], ["bert_models.BertOutput.dense", "bert_models.BertOutput.dropout", "bert_models.BertOutput.LayerNorm"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "hidden_states", ",", "input_tensor", ")", ":", "\n", "        ", "hidden_states", "=", "self", ".", "dense", "(", "hidden_states", ")", "\n", "hidden_states", "=", "self", ".", "dropout", "(", "hidden_states", ")", "\n", "hidden_states", "=", "self", ".", "LayerNorm", "(", "hidden_states", "+", "input_tensor", ")", "\n", "return", "hidden_states", "\n", "\n"]], "home.repos.pwc.inspect_result.lawsonabs_pun.None.bert_models.BertLayer.__init__": [[380, 385], ["torch.nn.Module.__init__", "bert_models.BertAttention", "bert_models.BertIntermediate", "bert_models.BertOutput"], "methods", ["home.repos.pwc.inspect_result.lawsonabs_pun.subtask3_v2.gloss.EvalDataset.__init__"], ["    ", "def", "__init__", "(", "self", ",", "config", ")", ":", "\n", "        ", "super", "(", "BertLayer", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "attention", "=", "BertAttention", "(", "config", ")", "\n", "self", ".", "intermediate", "=", "BertIntermediate", "(", "config", ")", "\n", "self", ".", "output", "=", "BertOutput", "(", "config", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.lawsonabs_pun.None.bert_models.BertLayer.forward": [[386, 391], ["bert_models.BertLayer.attention", "bert_models.BertLayer.intermediate", "bert_models.BertLayer.output"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "hidden_states", ",", "attention_mask", ")", ":", "\n", "        ", "attention_output", "=", "self", ".", "attention", "(", "hidden_states", ",", "attention_mask", ")", "\n", "intermediate_output", "=", "self", ".", "intermediate", "(", "attention_output", ")", "\n", "layer_output", "=", "self", ".", "output", "(", "intermediate_output", ",", "attention_output", ")", "\n", "return", "layer_output", "\n", "\n"]], "home.repos.pwc.inspect_result.lawsonabs_pun.None.bert_models.BertEncoder.__init__": [[394, 398], ["torch.nn.Module.__init__", "bert_models.BertLayer", "torch.nn.ModuleList", "torch.nn.ModuleList", "copy.deepcopy", "range"], "methods", ["home.repos.pwc.inspect_result.lawsonabs_pun.subtask3_v2.gloss.EvalDataset.__init__"], ["    ", "def", "__init__", "(", "self", ",", "config", ")", ":", "\n", "        ", "super", "(", "BertEncoder", ",", "self", ")", ".", "__init__", "(", ")", "\n", "layer", "=", "BertLayer", "(", "config", ")", "\n", "self", ".", "layer", "=", "nn", ".", "ModuleList", "(", "[", "copy", ".", "deepcopy", "(", "layer", ")", "for", "_", "in", "range", "(", "config", ".", "num_hidden_layers", ")", "]", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.lawsonabs_pun.None.bert_models.BertEncoder.forward": [[399, 408], ["layer_module", "all_encoder_layers.append", "all_encoder_layers.append"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "hidden_states", ",", "attention_mask", ",", "output_all_encoded_layers", "=", "True", ")", ":", "\n", "        ", "all_encoder_layers", "=", "[", "]", "\n", "for", "layer_module", "in", "self", ".", "layer", ":", "\n", "            ", "hidden_states", "=", "layer_module", "(", "hidden_states", ",", "attention_mask", ")", "\n", "if", "output_all_encoded_layers", ":", "\n", "                ", "all_encoder_layers", ".", "append", "(", "hidden_states", ")", "\n", "", "", "if", "not", "output_all_encoded_layers", ":", "\n", "            ", "all_encoder_layers", ".", "append", "(", "hidden_states", ")", "\n", "", "return", "all_encoder_layers", "\n", "\n"]], "home.repos.pwc.inspect_result.lawsonabs_pun.None.bert_models.BertPooler.__init__": [[411, 415], ["torch.nn.Module.__init__", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Tanh", "torch.nn.Tanh"], "methods", ["home.repos.pwc.inspect_result.lawsonabs_pun.subtask3_v2.gloss.EvalDataset.__init__"], ["    ", "def", "__init__", "(", "self", ",", "config", ")", ":", "\n", "        ", "super", "(", "BertPooler", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "dense", "=", "nn", ".", "Linear", "(", "config", ".", "hidden_size", ",", "config", ".", "hidden_size", ")", "\n", "self", ".", "activation", "=", "nn", ".", "Tanh", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.lawsonabs_pun.None.bert_models.BertPooler.forward": [[416, 423], ["bert_models.BertPooler.dense", "bert_models.BertPooler.activation"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "hidden_states", ")", ":", "\n", "# We \"pool\" the model by simply taking the hidden state corresponding", "\n", "# to the first token.", "\n", "        ", "first_token_tensor", "=", "hidden_states", "[", ":", ",", "0", "]", "\n", "pooled_output", "=", "self", ".", "dense", "(", "first_token_tensor", ")", "\n", "pooled_output", "=", "self", ".", "activation", "(", "pooled_output", ")", "\n", "return", "pooled_output", "\n", "\n"]], "home.repos.pwc.inspect_result.lawsonabs_pun.None.bert_models.BertPredictionHeadTransform.__init__": [[426, 434], ["torch.nn.Module.__init__", "torch.nn.Linear", "torch.nn.Linear", "BertLayerNorm", "isinstance", "isinstance"], "methods", ["home.repos.pwc.inspect_result.lawsonabs_pun.subtask3_v2.gloss.EvalDataset.__init__"], ["    ", "def", "__init__", "(", "self", ",", "config", ")", ":", "\n", "        ", "super", "(", "BertPredictionHeadTransform", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "dense", "=", "nn", ".", "Linear", "(", "config", ".", "hidden_size", ",", "config", ".", "hidden_size", ")", "\n", "if", "isinstance", "(", "config", ".", "hidden_act", ",", "str", ")", "or", "(", "sys", ".", "version_info", "[", "0", "]", "==", "2", "and", "isinstance", "(", "config", ".", "hidden_act", ",", "unicode", ")", ")", ":", "\n", "            ", "self", ".", "transform_act_fn", "=", "ACT2FN", "[", "config", ".", "hidden_act", "]", "\n", "", "else", ":", "\n", "            ", "self", ".", "transform_act_fn", "=", "config", ".", "hidden_act", "\n", "", "self", ".", "LayerNorm", "=", "BertLayerNorm", "(", "config", ".", "hidden_size", ",", "eps", "=", "1e-12", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.lawsonabs_pun.None.bert_models.BertPredictionHeadTransform.forward": [[435, 440], ["bert_models.BertPredictionHeadTransform.dense", "bert_models.BertPredictionHeadTransform.transform_act_fn", "bert_models.BertPredictionHeadTransform.LayerNorm"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "hidden_states", ")", ":", "\n", "        ", "hidden_states", "=", "self", ".", "dense", "(", "hidden_states", ")", "\n", "hidden_states", "=", "self", ".", "transform_act_fn", "(", "hidden_states", ")", "\n", "hidden_states", "=", "self", ".", "LayerNorm", "(", "hidden_states", ")", "\n", "return", "hidden_states", "\n", "\n"]], "home.repos.pwc.inspect_result.lawsonabs_pun.None.bert_models.BertLMPredictionHead.__init__": [[443, 454], ["torch.nn.Module.__init__", "bert_models.BertPredictionHeadTransform", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Parameter", "torch.nn.Parameter", "bert_model_embedding_weights.size", "bert_model_embedding_weights.size", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "bert_model_embedding_weights.size"], "methods", ["home.repos.pwc.inspect_result.lawsonabs_pun.subtask3_v2.gloss.EvalDataset.__init__"], ["    ", "def", "__init__", "(", "self", ",", "config", ",", "bert_model_embedding_weights", ")", ":", "\n", "        ", "super", "(", "BertLMPredictionHead", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "transform", "=", "BertPredictionHeadTransform", "(", "config", ")", "\n", "\n", "# The output weights are the same as the input embeddings, but there is", "\n", "# an output-only bias for each token.", "\n", "self", ".", "decoder", "=", "nn", ".", "Linear", "(", "bert_model_embedding_weights", ".", "size", "(", "1", ")", ",", "\n", "bert_model_embedding_weights", ".", "size", "(", "0", ")", ",", "\n", "bias", "=", "False", ")", "\n", "self", ".", "decoder", ".", "weight", "=", "bert_model_embedding_weights", "\n", "self", ".", "bias", "=", "nn", ".", "Parameter", "(", "torch", ".", "zeros", "(", "bert_model_embedding_weights", ".", "size", "(", "0", ")", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.lawsonabs_pun.None.bert_models.BertLMPredictionHead.forward": [[455, 459], ["bert_models.BertLMPredictionHead.transform", "bert_models.BertLMPredictionHead.decoder"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "hidden_states", ")", ":", "\n", "        ", "hidden_states", "=", "self", ".", "transform", "(", "hidden_states", ")", "\n", "hidden_states", "=", "self", ".", "decoder", "(", "hidden_states", ")", "+", "self", ".", "bias", "\n", "return", "hidden_states", "\n", "\n"]], "home.repos.pwc.inspect_result.lawsonabs_pun.None.bert_models.BertOnlyMLMHead.__init__": [[462, 465], ["torch.nn.Module.__init__", "bert_models.BertLMPredictionHead"], "methods", ["home.repos.pwc.inspect_result.lawsonabs_pun.subtask3_v2.gloss.EvalDataset.__init__"], ["    ", "def", "__init__", "(", "self", ",", "config", ",", "bert_model_embedding_weights", ")", ":", "\n", "        ", "super", "(", "BertOnlyMLMHead", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "predictions", "=", "BertLMPredictionHead", "(", "config", ",", "bert_model_embedding_weights", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.lawsonabs_pun.None.bert_models.BertOnlyMLMHead.forward": [[466, 469], ["bert_models.BertOnlyMLMHead.predictions"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "sequence_output", ")", ":", "\n", "        ", "prediction_scores", "=", "self", ".", "predictions", "(", "sequence_output", ")", "\n", "return", "prediction_scores", "\n", "\n"]], "home.repos.pwc.inspect_result.lawsonabs_pun.None.bert_models.BertOnlyNSPHead.__init__": [[472, 475], ["torch.nn.Module.__init__", "torch.nn.Linear", "torch.nn.Linear"], "methods", ["home.repos.pwc.inspect_result.lawsonabs_pun.subtask3_v2.gloss.EvalDataset.__init__"], ["    ", "def", "__init__", "(", "self", ",", "config", ")", ":", "\n", "        ", "super", "(", "BertOnlyNSPHead", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "seq_relationship", "=", "nn", ".", "Linear", "(", "config", ".", "hidden_size", ",", "2", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.lawsonabs_pun.None.bert_models.BertOnlyNSPHead.forward": [[476, 479], ["bert_models.BertOnlyNSPHead.seq_relationship"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "pooled_output", ")", ":", "\n", "        ", "seq_relationship_score", "=", "self", ".", "seq_relationship", "(", "pooled_output", ")", "\n", "return", "seq_relationship_score", "\n", "\n"]], "home.repos.pwc.inspect_result.lawsonabs_pun.None.bert_models.BertPreTrainingHeads.__init__": [[482, 486], ["torch.nn.Module.__init__", "bert_models.BertLMPredictionHead", "torch.nn.Linear", "torch.nn.Linear"], "methods", ["home.repos.pwc.inspect_result.lawsonabs_pun.subtask3_v2.gloss.EvalDataset.__init__"], ["    ", "def", "__init__", "(", "self", ",", "config", ",", "bert_model_embedding_weights", ")", ":", "\n", "        ", "super", "(", "BertPreTrainingHeads", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "predictions", "=", "BertLMPredictionHead", "(", "config", ",", "bert_model_embedding_weights", ")", "\n", "self", ".", "seq_relationship", "=", "nn", ".", "Linear", "(", "config", ".", "hidden_size", ",", "2", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.lawsonabs_pun.None.bert_models.BertPreTrainingHeads.forward": [[487, 491], ["bert_models.BertPreTrainingHeads.predictions", "bert_models.BertPreTrainingHeads.seq_relationship"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "sequence_output", ",", "pooled_output", ")", ":", "\n", "        ", "prediction_scores", "=", "self", ".", "predictions", "(", "sequence_output", ")", "\n", "seq_relationship_score", "=", "self", ".", "seq_relationship", "(", "pooled_output", ")", "\n", "return", "prediction_scores", ",", "seq_relationship_score", "\n", "\n"]], "home.repos.pwc.inspect_result.lawsonabs_pun.None.bert_models.BertPreTrainedModel.__init__": [[497, 507], ["torch.nn.Module.__init__", "isinstance", "ValueError"], "methods", ["home.repos.pwc.inspect_result.lawsonabs_pun.subtask3_v2.gloss.EvalDataset.__init__"], ["def", "__init__", "(", "self", ",", "config", ",", "*", "inputs", ",", "**", "kwargs", ")", ":", "\n", "        ", "super", "(", "BertPreTrainedModel", ",", "self", ")", ".", "__init__", "(", ")", "\n", "if", "not", "isinstance", "(", "config", ",", "BertConfig", ")", ":", "\n", "            ", "raise", "ValueError", "(", "\n", "\"Parameter config in `{}(config)` should be an instance of class `BertConfig`. \"", "\n", "\"To create a model from a Google pretrained model use \"", "\n", "\"`model = {}.from_pretrained(PRETRAINED_MODEL_NAME)`\"", ".", "format", "(", "\n", "self", ".", "__class__", ".", "__name__", ",", "self", ".", "__class__", ".", "__name__", "\n", ")", ")", "\n", "", "self", ".", "config", "=", "config", "\n", "\n"]], "home.repos.pwc.inspect_result.lawsonabs_pun.None.bert_models.BertPreTrainedModel.init_bert_weights": [[508, 520], ["isinstance", "module.weight.data.normal_", "isinstance", "isinstance", "module.bias.data.zero_", "module.bias.data.zero_", "module.weight.data.fill_"], "methods", ["None"], ["", "def", "init_bert_weights", "(", "self", ",", "module", ")", ":", "\n", "        ", "\"\"\" Initialize the weights.\n        \"\"\"", "\n", "if", "isinstance", "(", "module", ",", "(", "nn", ".", "Linear", ",", "nn", ".", "Embedding", ")", ")", ":", "\n", "# Slightly different from the TF version which uses truncated_normal for initialization", "\n", "# cf https://github.com/pytorch/pytorch/pull/5617", "\n", "            ", "module", ".", "weight", ".", "data", ".", "normal_", "(", "mean", "=", "0.0", ",", "std", "=", "self", ".", "config", ".", "initializer_range", ")", "\n", "", "elif", "isinstance", "(", "module", ",", "BertLayerNorm", ")", ":", "\n", "            ", "module", ".", "bias", ".", "data", ".", "zero_", "(", ")", "\n", "module", ".", "weight", ".", "data", ".", "fill_", "(", "1.0", ")", "\n", "", "if", "isinstance", "(", "module", ",", "nn", ".", "Linear", ")", "and", "module", ".", "bias", "is", "not", "None", ":", "\n", "            ", "module", ".", "bias", ".", "data", ".", "zero_", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.lawsonabs_pun.None.bert_models.BertPreTrainedModel.from_pretrained": [[521, 642], ["os.path.join", "bert_models.BertConfig.from_json_file", "logger.info", "cls", "torch.load.keys", "torch.load.keys", "zip", "getattr", "torch.load.copy", "torch.load.copy", "bert_models.BertPreTrainedModel.from_pretrained.load"], "methods", ["home.repos.pwc.inspect_result.lawsonabs_pun.None.bert_models.BertConfig.from_json_file"], ["", "", "@", "classmethod", "\n", "def", "from_pretrained", "(", "cls", ",", "pretrained_model_name_or_path", ",", "state_dict", "=", "None", ",", "cache_dir", "=", "None", ",", "\n", "from_tf", "=", "False", ",", "*", "inputs", ",", "**", "kwargs", ")", ":", "\n", "        ", "\"\"\"\n        Instantiate a BertPreTrainedModel from a pre-trained model file or a pytorch state dict.\n        Download and cache the pre-trained model file if needed.\n        Params:\n            pretrained_model_name_or_path: either:\n                - a str with the name of a pre-trained model to load selected in the list of:\n                    . `bert-base-uncased`\n                    . `bert-large-uncased`\n                    . `bert-base-cased`\n                    . `bert-large-cased`\n                    . `bert-base-multilingual-uncased`\n                    . `bert-base-multilingual-cased`\n                    . `bert-base-chinese`\n                - a path or url to a pretrained model archive containing:\n                    . `bert_config.json` a configuration file for the model\n                    . `pytorch_model.bin` a PyTorch dump of a BertForPreTraining instance\n                - a path or url to a pretrained model archive containing:\n                    . `bert_config.json` a configuration file for the model\n                    . `model.chkpt` a TensorFlow checkpoint\n            from_tf: should we load the weights from a locally saved TensorFlow checkpoint\n            cache_dir: an optional path to a folder in which the pre-trained models will be cached.\n            state_dict: an optional state dictionnary (collections.OrderedDict object) to use instead of Google pre-trained models\n            *inputs, **kwargs: additional input for the specific Bert class\n                (ex: num_labels for BertForSequenceClassification)\n        \"\"\"", "\n", "if", "pretrained_model_name_or_path", "in", "PRETRAINED_MODEL_ARCHIVE_MAP", ":", "\n", "            ", "archive_file", "=", "PRETRAINED_MODEL_ARCHIVE_MAP", "[", "pretrained_model_name_or_path", "]", "\n", "", "else", ":", "\n", "            ", "archive_file", "=", "pretrained_model_name_or_path", "\n", "# redirect to the cache, if necessary", "\n", "", "try", ":", "\n", "            ", "resolved_archive_file", "=", "cached_path", "(", "archive_file", ",", "cache_dir", "=", "cache_dir", ")", "\n", "", "except", "EnvironmentError", ":", "\n", "            ", "logger", ".", "error", "(", "\n", "\"Model name '{}' was not found in model name list ({}). \"", "\n", "\"We assumed '{}' was a path or url but couldn't find any file \"", "\n", "\"associated to this path or url.\"", ".", "format", "(", "\n", "pretrained_model_name_or_path", ",", "\n", "', '", ".", "join", "(", "PRETRAINED_MODEL_ARCHIVE_MAP", ".", "keys", "(", ")", ")", ",", "\n", "archive_file", ")", ")", "\n", "return", "None", "\n", "", "if", "resolved_archive_file", "==", "archive_file", ":", "\n", "            ", "logger", ".", "info", "(", "\"loading archive file {}\"", ".", "format", "(", "archive_file", ")", ")", "\n", "", "else", ":", "\n", "            ", "logger", ".", "info", "(", "\"loading archive file {} from cache at {}\"", ".", "format", "(", "\n", "archive_file", ",", "resolved_archive_file", ")", ")", "\n", "", "tempdir", "=", "None", "\n", "if", "os", ".", "path", ".", "isdir", "(", "resolved_archive_file", ")", "or", "from_tf", ":", "\n", "            ", "serialization_dir", "=", "resolved_archive_file", "\n", "", "else", ":", "\n", "# Extract archive to temp dir", "\n", "            ", "tempdir", "=", "tempfile", ".", "mkdtemp", "(", ")", "\n", "logger", ".", "info", "(", "\"extracting archive file {} to temp dir {}\"", ".", "format", "(", "\n", "resolved_archive_file", ",", "tempdir", ")", ")", "\n", "with", "tarfile", ".", "open", "(", "resolved_archive_file", ",", "'r:gz'", ")", "as", "archive", ":", "\n", "                ", "archive", ".", "extractall", "(", "tempdir", ")", "\n", "", "serialization_dir", "=", "tempdir", "\n", "# Load config", "\n", "", "config_file", "=", "os", ".", "path", ".", "join", "(", "serialization_dir", ",", "CONFIG_NAME", ")", "\n", "config", "=", "BertConfig", ".", "from_json_file", "(", "config_file", ")", "\n", "logger", ".", "info", "(", "\"Model config {}\"", ".", "format", "(", "config", ")", ")", "# \u8f93\u51fabert\u6a21\u578b\u7684\u914d\u7f6e\u4fe1\u606f", "\n", "# Instantiate model.", "\n", "model", "=", "cls", "(", "config", ",", "*", "inputs", ",", "**", "kwargs", ")", "\n", "if", "state_dict", "is", "None", "and", "not", "from_tf", ":", "\n", "            ", "weights_path", "=", "os", ".", "path", ".", "join", "(", "serialization_dir", ",", "WEIGHTS_NAME", ")", "\n", "state_dict", "=", "torch", ".", "load", "(", "weights_path", ",", "map_location", "=", "'cpu'", "if", "not", "torch", ".", "cuda", ".", "is_available", "(", ")", "else", "None", ")", "\n", "", "if", "tempdir", ":", "\n", "# Clean up temp dir", "\n", "            ", "shutil", ".", "rmtree", "(", "tempdir", ")", "\n", "", "if", "from_tf", ":", "\n", "# Directly load from a TensorFlow checkpoint", "\n", "            ", "weights_path", "=", "os", ".", "path", ".", "join", "(", "serialization_dir", ",", "TF_WEIGHTS_NAME", ")", "\n", "return", "load_tf_weights_in_bert", "(", "model", ",", "weights_path", ")", "\n", "# Load from a PyTorch state_dict", "\n", "", "old_keys", "=", "[", "]", "\n", "new_keys", "=", "[", "]", "\n", "for", "key", "in", "state_dict", ".", "keys", "(", ")", ":", "\n", "            ", "new_key", "=", "None", "\n", "if", "'gamma'", "in", "key", ":", "\n", "                ", "new_key", "=", "key", ".", "replace", "(", "'gamma'", ",", "'weight'", ")", "\n", "", "if", "'beta'", "in", "key", ":", "\n", "                ", "new_key", "=", "key", ".", "replace", "(", "'beta'", ",", "'bias'", ")", "\n", "", "if", "new_key", ":", "\n", "                ", "old_keys", ".", "append", "(", "key", ")", "\n", "new_keys", ".", "append", "(", "new_key", ")", "\n", "", "", "for", "old_key", ",", "new_key", "in", "zip", "(", "old_keys", ",", "new_keys", ")", ":", "\n", "            ", "state_dict", "[", "new_key", "]", "=", "state_dict", ".", "pop", "(", "old_key", ")", "\n", "\n", "", "missing_keys", "=", "[", "]", "\n", "unexpected_keys", "=", "[", "]", "\n", "error_msgs", "=", "[", "]", "\n", "# copy state_dict so _load_from_state_dict can modify it", "\n", "metadata", "=", "getattr", "(", "state_dict", ",", "'_metadata'", ",", "None", ")", "\n", "state_dict", "=", "state_dict", ".", "copy", "(", ")", "\n", "if", "metadata", "is", "not", "None", ":", "\n", "            ", "state_dict", ".", "_metadata", "=", "metadata", "\n", "\n", "", "def", "load", "(", "module", ",", "prefix", "=", "''", ")", ":", "\n", "            ", "local_metadata", "=", "{", "}", "if", "metadata", "is", "None", "else", "metadata", ".", "get", "(", "prefix", "[", ":", "-", "1", "]", ",", "{", "}", ")", "\n", "module", ".", "_load_from_state_dict", "(", "\n", "state_dict", ",", "prefix", ",", "local_metadata", ",", "True", ",", "missing_keys", ",", "unexpected_keys", ",", "error_msgs", ")", "\n", "for", "name", ",", "child", "in", "module", ".", "_modules", ".", "items", "(", ")", ":", "\n", "                ", "if", "child", "is", "not", "None", ":", "\n", "                    ", "load", "(", "child", ",", "prefix", "+", "name", "+", "'.'", ")", "\n", "", "", "", "start_prefix", "=", "''", "\n", "if", "not", "hasattr", "(", "model", ",", "'bert'", ")", "and", "any", "(", "s", ".", "startswith", "(", "'bert.'", ")", "for", "s", "in", "state_dict", ".", "keys", "(", ")", ")", ":", "\n", "            ", "start_prefix", "=", "'bert.'", "\n", "", "load", "(", "model", ",", "prefix", "=", "start_prefix", ")", "\n", "if", "len", "(", "missing_keys", ")", ">", "0", ":", "\n", "            ", "logger", ".", "info", "(", "\"Weights of {} not initialized from pretrained model: {}\"", ".", "format", "(", "\n", "model", ".", "__class__", ".", "__name__", ",", "missing_keys", ")", ")", "\n", "", "if", "len", "(", "unexpected_keys", ")", ">", "0", ":", "\n", "            ", "logger", ".", "info", "(", "\"Weights from pretrained model not used in {}: {}\"", ".", "format", "(", "\n", "model", ".", "__class__", ".", "__name__", ",", "unexpected_keys", ")", ")", "\n", "", "if", "len", "(", "error_msgs", ")", ">", "0", ":", "\n", "            ", "raise", "RuntimeError", "(", "'Error(s) in loading state_dict for {}:\\n\\t{}'", ".", "format", "(", "\n", "model", ".", "__class__", ".", "__name__", ",", "\"\\n\\t\"", ".", "join", "(", "error_msgs", ")", ")", ")", "\n", "", "return", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.lawsonabs_pun.None.bert_models.BertModel.__init__": [[682, 688], ["bert_models.BertPreTrainedModel.__init__", "bert_models.BertEmbeddings", "bert_models.BertEncoder", "bert_models.BertPooler", "bert_models.BertModel.apply"], "methods", ["home.repos.pwc.inspect_result.lawsonabs_pun.subtask3_v2.gloss.EvalDataset.__init__"], ["def", "__init__", "(", "self", ",", "config", ")", ":", "\n", "        ", "super", "(", "BertModel", ",", "self", ")", ".", "__init__", "(", "config", ")", "\n", "self", ".", "embeddings", "=", "BertEmbeddings", "(", "config", ")", "\n", "self", ".", "encoder", "=", "BertEncoder", "(", "config", ")", "\n", "self", ".", "pooler", "=", "BertPooler", "(", "config", ")", "\n", "self", ".", "apply", "(", "self", ".", "init_bert_weights", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.lawsonabs_pun.None.bert_models.BertModel.forward": [[689, 719], ["torch.ones_like.unsqueeze().unsqueeze", "torch.ones_like.unsqueeze().unsqueeze", "extended_attention_mask.to.to.to", "bert_models.BertModel.embeddings", "bert_models.BertModel.encoder", "bert_models.BertModel.pooler", "torch.ones_like", "torch.ones_like", "torch.ones_like", "torch.ones_like", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "torch.ones_like.unsqueeze", "torch.ones_like.unsqueeze", "next", "bert_models.BertModel.parameters"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "input_ids", ",", "token_type_ids", "=", "None", ",", "attention_mask", "=", "None", ",", "output_all_encoded_layers", "=", "True", ")", ":", "\n", "        ", "if", "attention_mask", "is", "None", ":", "\n", "            ", "attention_mask", "=", "torch", ".", "ones_like", "(", "input_ids", ")", "\n", "", "if", "token_type_ids", "is", "None", ":", "\n", "            ", "token_type_ids", "=", "torch", ".", "zeros_like", "(", "input_ids", ")", "\n", "\n", "# We create a 3D attention mask from a 2D tensor mask.", "\n", "# Sizes are [batch_size, 1, 1, to_seq_length]", "\n", "# So we can broadcast to [batch_size, num_heads, from_seq_length, to_seq_length]", "\n", "# this attention mask is more simple than the triangular masking of causal attention", "\n", "# used in OpenAI GPT, we just need to prepare the broadcast dimension here.", "\n", "", "extended_attention_mask", "=", "attention_mask", ".", "unsqueeze", "(", "1", ")", ".", "unsqueeze", "(", "2", ")", "\n", "\n", "# Since attention_mask is 1.0 for positions we want to attend and 0.0 for", "\n", "# masked positions, this operation will create a tensor which is 0.0 for", "\n", "# positions we want to attend and -10000.0 for masked positions.", "\n", "# Since we are adding it to the raw scores before the softmax, this is", "\n", "# effectively the same as removing these entirely.", "\n", "extended_attention_mask", "=", "extended_attention_mask", ".", "to", "(", "dtype", "=", "next", "(", "self", ".", "parameters", "(", ")", ")", ".", "dtype", ")", "# fp16 compatibility", "\n", "extended_attention_mask", "=", "(", "1.0", "-", "extended_attention_mask", ")", "*", "-", "10000.0", "\n", "\n", "embedding_output", "=", "self", ".", "embeddings", "(", "input_ids", ",", "token_type_ids", ")", "\n", "encoded_layers", "=", "self", ".", "encoder", "(", "embedding_output", ",", "\n", "extended_attention_mask", ",", "\n", "output_all_encoded_layers", "=", "output_all_encoded_layers", ")", "\n", "sequence_output", "=", "encoded_layers", "[", "-", "1", "]", "\n", "pooled_output", "=", "self", ".", "pooler", "(", "sequence_output", ")", "\n", "if", "not", "output_all_encoded_layers", ":", "\n", "            ", "encoded_layers", "=", "encoded_layers", "[", "-", "1", "]", "\n", "", "return", "encoded_layers", ",", "pooled_output", "\n", "\n"]], "home.repos.pwc.inspect_result.lawsonabs_pun.None.bert_models.BertForPreTraining.__init__": [[765, 770], ["bert_models.BertPreTrainedModel.__init__", "bert_models.BertModel", "bert_models.BertPreTrainingHeads", "bert_models.BertForPreTraining.apply"], "methods", ["home.repos.pwc.inspect_result.lawsonabs_pun.subtask3_v2.gloss.EvalDataset.__init__"], ["def", "__init__", "(", "self", ",", "config", ")", ":", "\n", "        ", "super", "(", "BertForPreTraining", ",", "self", ")", ".", "__init__", "(", "config", ")", "\n", "self", ".", "bert", "=", "BertModel", "(", "config", ")", "\n", "self", ".", "cls", "=", "BertPreTrainingHeads", "(", "config", ",", "self", ".", "bert", ".", "embeddings", ".", "word_embeddings", ".", "weight", ")", "\n", "self", ".", "apply", "(", "self", ".", "init_bert_weights", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.lawsonabs_pun.None.bert_models.BertForPreTraining.forward": [[771, 784], ["bert_models.BertForPreTraining.bert", "bert_models.BertForPreTraining.cls", "torch.nn.CrossEntropyLoss", "torch.nn.CrossEntropyLoss", "torch.nn.CrossEntropyLoss.", "torch.nn.CrossEntropyLoss.", "torch.nn.CrossEntropyLoss.", "torch.nn.CrossEntropyLoss.", "prediction_scores.view", "masked_lm_labels.view", "seq_relationship_score.view", "next_sentence_label.view"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "input_ids", ",", "token_type_ids", "=", "None", ",", "attention_mask", "=", "None", ",", "masked_lm_labels", "=", "None", ",", "next_sentence_label", "=", "None", ")", ":", "\n", "        ", "sequence_output", ",", "pooled_output", "=", "self", ".", "bert", "(", "input_ids", ",", "token_type_ids", ",", "attention_mask", ",", "\n", "output_all_encoded_layers", "=", "False", ")", "\n", "prediction_scores", ",", "seq_relationship_score", "=", "self", ".", "cls", "(", "sequence_output", ",", "pooled_output", ")", "\n", "\n", "if", "masked_lm_labels", "is", "not", "None", "and", "next_sentence_label", "is", "not", "None", ":", "\n", "            ", "loss_fct", "=", "CrossEntropyLoss", "(", "ignore_index", "=", "-", "1", ")", "\n", "masked_lm_loss", "=", "loss_fct", "(", "prediction_scores", ".", "view", "(", "-", "1", ",", "self", ".", "config", ".", "vocab_size", ")", ",", "masked_lm_labels", ".", "view", "(", "-", "1", ")", ")", "\n", "next_sentence_loss", "=", "loss_fct", "(", "seq_relationship_score", ".", "view", "(", "-", "1", ",", "2", ")", ",", "next_sentence_label", ".", "view", "(", "-", "1", ")", ")", "\n", "total_loss", "=", "masked_lm_loss", "+", "next_sentence_loss", "\n", "return", "total_loss", "\n", "", "else", ":", "\n", "            ", "return", "prediction_scores", ",", "seq_relationship_score", "\n", "\n"]], "home.repos.pwc.inspect_result.lawsonabs_pun.None.bert_models.BertForMaskedLM.__init__": [[822, 827], ["bert_models.BertPreTrainedModel.__init__", "bert_models.BertModel", "bert_models.BertOnlyMLMHead", "bert_models.BertForMaskedLM.apply"], "methods", ["home.repos.pwc.inspect_result.lawsonabs_pun.subtask3_v2.gloss.EvalDataset.__init__"], ["def", "__init__", "(", "self", ",", "config", ")", ":", "\n", "        ", "super", "(", "BertForMaskedLM", ",", "self", ")", ".", "__init__", "(", "config", ")", "\n", "self", ".", "bert", "=", "BertModel", "(", "config", ")", "\n", "self", ".", "cls", "=", "BertOnlyMLMHead", "(", "config", ",", "self", ".", "bert", ".", "embeddings", ".", "word_embeddings", ".", "weight", ")", "\n", "self", ".", "apply", "(", "self", ".", "init_bert_weights", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.lawsonabs_pun.None.bert_models.BertForMaskedLM.forward": [[828, 839], ["bert_models.BertForMaskedLM.bert", "bert_models.BertForMaskedLM.cls", "torch.nn.CrossEntropyLoss", "torch.nn.CrossEntropyLoss", "torch.nn.CrossEntropyLoss.", "torch.nn.CrossEntropyLoss.", "bert_models.BertForMaskedLM.view", "masked_lm_labels.view"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "input_ids", ",", "token_type_ids", "=", "None", ",", "attention_mask", "=", "None", ",", "masked_lm_labels", "=", "None", ")", ":", "\n", "        ", "sequence_output", ",", "_", "=", "self", ".", "bert", "(", "input_ids", ",", "token_type_ids", ",", "attention_mask", ",", "\n", "output_all_encoded_layers", "=", "False", ")", "\n", "prediction_scores", "=", "self", ".", "cls", "(", "sequence_output", ")", "\n", "\n", "if", "masked_lm_labels", "is", "not", "None", ":", "\n", "            ", "loss_fct", "=", "CrossEntropyLoss", "(", "ignore_index", "=", "-", "1", ")", "\n", "masked_lm_loss", "=", "loss_fct", "(", "prediction_scores", ".", "view", "(", "-", "1", ",", "self", ".", "config", ".", "vocab_size", ")", ",", "masked_lm_labels", ".", "view", "(", "-", "1", ")", ")", "\n", "return", "masked_lm_loss", "\n", "", "else", ":", "\n", "            ", "return", "prediction_scores", "\n", "\n"]], "home.repos.pwc.inspect_result.lawsonabs_pun.None.bert_models.BertForNextSentencePrediction.__init__": [[878, 883], ["bert_models.BertPreTrainedModel.__init__", "bert_models.BertModel", "bert_models.BertOnlyNSPHead", "bert_models.BertForNextSentencePrediction.apply"], "methods", ["home.repos.pwc.inspect_result.lawsonabs_pun.subtask3_v2.gloss.EvalDataset.__init__"], ["def", "__init__", "(", "self", ",", "config", ")", ":", "\n", "        ", "super", "(", "BertForNextSentencePrediction", ",", "self", ")", ".", "__init__", "(", "config", ")", "\n", "self", ".", "bert", "=", "BertModel", "(", "config", ")", "\n", "self", ".", "cls", "=", "BertOnlyNSPHead", "(", "config", ")", "\n", "self", ".", "apply", "(", "self", ".", "init_bert_weights", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.lawsonabs_pun.None.bert_models.BertForNextSentencePrediction.forward": [[884, 895], ["bert_models.BertForNextSentencePrediction.bert", "bert_models.BertForNextSentencePrediction.cls", "torch.nn.CrossEntropyLoss", "torch.nn.CrossEntropyLoss", "torch.nn.CrossEntropyLoss.", "torch.nn.CrossEntropyLoss.", "bert_models.BertForNextSentencePrediction.view", "next_sentence_label.view"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "input_ids", ",", "token_type_ids", "=", "None", ",", "attention_mask", "=", "None", ",", "next_sentence_label", "=", "None", ")", ":", "\n", "        ", "_", ",", "pooled_output", "=", "self", ".", "bert", "(", "input_ids", ",", "token_type_ids", ",", "attention_mask", ",", "\n", "output_all_encoded_layers", "=", "False", ")", "\n", "seq_relationship_score", "=", "self", ".", "cls", "(", "pooled_output", ")", "\n", "\n", "if", "next_sentence_label", "is", "not", "None", ":", "\n", "            ", "loss_fct", "=", "CrossEntropyLoss", "(", "ignore_index", "=", "-", "1", ")", "\n", "next_sentence_loss", "=", "loss_fct", "(", "seq_relationship_score", ".", "view", "(", "-", "1", ",", "2", ")", ",", "next_sentence_label", ".", "view", "(", "-", "1", ")", ")", "\n", "return", "next_sentence_loss", "\n", "", "else", ":", "\n", "            ", "return", "seq_relationship_score", "\n", "\n"]], "home.repos.pwc.inspect_result.lawsonabs_pun.None.bert_models.BertForSequenceClassification.__init__": [[935, 942], ["bert_models.BertPreTrainedModel.__init__", "bert_models.BertModel", "torch.nn.Dropout", "torch.nn.Dropout", "torch.nn.Linear", "torch.nn.Linear", "bert_models.BertForSequenceClassification.apply"], "methods", ["home.repos.pwc.inspect_result.lawsonabs_pun.subtask3_v2.gloss.EvalDataset.__init__"], ["def", "__init__", "(", "self", ",", "config", ",", "num_labels", ")", ":", "\n", "        ", "super", "(", "BertForSequenceClassification", ",", "self", ")", ".", "__init__", "(", "config", ")", "\n", "self", ".", "num_labels", "=", "num_labels", "\n", "self", ".", "bert", "=", "BertModel", "(", "config", ")", "\n", "self", ".", "dropout", "=", "nn", ".", "Dropout", "(", "config", ".", "hidden_dropout_prob", ")", "\n", "self", ".", "classifier", "=", "nn", ".", "Linear", "(", "config", ".", "hidden_size", ",", "num_labels", ")", "\n", "self", ".", "apply", "(", "self", ".", "init_bert_weights", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.lawsonabs_pun.None.bert_models.BertForSequenceClassification.forward": [[943, 954], ["bert_models.BertForSequenceClassification.bert", "bert_models.BertForSequenceClassification.dropout", "bert_models.BertForSequenceClassification.classifier", "torch.nn.CrossEntropyLoss", "torch.nn.CrossEntropyLoss", "torch.nn.CrossEntropyLoss.", "torch.nn.CrossEntropyLoss.", "bert_models.BertForSequenceClassification.view", "labels.view"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "input_ids", ",", "token_type_ids", "=", "None", ",", "attention_mask", "=", "None", ",", "labels", "=", "None", ")", ":", "\n", "        ", "_", ",", "pooled_output", "=", "self", ".", "bert", "(", "input_ids", ",", "token_type_ids", ",", "attention_mask", ",", "output_all_encoded_layers", "=", "False", ")", "\n", "pooled_output", "=", "self", ".", "dropout", "(", "pooled_output", ")", "\n", "logits", "=", "self", ".", "classifier", "(", "pooled_output", ")", "\n", "\n", "if", "labels", "is", "not", "None", ":", "\n", "            ", "loss_fct", "=", "CrossEntropyLoss", "(", ")", "\n", "loss", "=", "loss_fct", "(", "logits", ".", "view", "(", "-", "1", ",", "self", ".", "num_labels", ")", ",", "labels", ".", "view", "(", "-", "1", ")", ")", "\n", "return", "loss", "\n", "", "else", ":", "\n", "            ", "return", "logits", "\n", "\n"]], "home.repos.pwc.inspect_result.lawsonabs_pun.None.bert_models.BertForMultipleChoice.__init__": [[994, 1001], ["bert_models.BertPreTrainedModel.__init__", "bert_models.BertModel", "torch.nn.Dropout", "torch.nn.Dropout", "torch.nn.Linear", "torch.nn.Linear", "bert_models.BertForMultipleChoice.apply"], "methods", ["home.repos.pwc.inspect_result.lawsonabs_pun.subtask3_v2.gloss.EvalDataset.__init__"], ["def", "__init__", "(", "self", ",", "config", ",", "num_choices", ")", ":", "\n", "        ", "super", "(", "BertForMultipleChoice", ",", "self", ")", ".", "__init__", "(", "config", ")", "\n", "self", ".", "num_choices", "=", "num_choices", "\n", "self", ".", "bert", "=", "BertModel", "(", "config", ")", "\n", "self", ".", "dropout", "=", "nn", ".", "Dropout", "(", "config", ".", "hidden_dropout_prob", ")", "\n", "self", ".", "classifier", "=", "nn", ".", "Linear", "(", "config", ".", "hidden_size", ",", "1", ")", "\n", "self", ".", "apply", "(", "self", ".", "init_bert_weights", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.lawsonabs_pun.None.bert_models.BertForMultipleChoice.forward": [[1002, 1017], ["input_ids.view", "token_type_ids.view", "attention_mask.view", "bert_models.BertForMultipleChoice.bert", "bert_models.BertForMultipleChoice.dropout", "bert_models.BertForMultipleChoice.classifier", "bert_models.BertForMultipleChoice.view", "input_ids.size", "token_type_ids.size", "attention_mask.size", "torch.nn.CrossEntropyLoss", "torch.nn.CrossEntropyLoss", "torch.nn.CrossEntropyLoss.", "torch.nn.CrossEntropyLoss."], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "input_ids", ",", "token_type_ids", "=", "None", ",", "attention_mask", "=", "None", ",", "labels", "=", "None", ")", ":", "\n", "        ", "flat_input_ids", "=", "input_ids", ".", "view", "(", "-", "1", ",", "input_ids", ".", "size", "(", "-", "1", ")", ")", "\n", "flat_token_type_ids", "=", "token_type_ids", ".", "view", "(", "-", "1", ",", "token_type_ids", ".", "size", "(", "-", "1", ")", ")", "\n", "flat_attention_mask", "=", "attention_mask", ".", "view", "(", "-", "1", ",", "attention_mask", ".", "size", "(", "-", "1", ")", ")", "\n", "_", ",", "pooled_output", "=", "self", ".", "bert", "(", "flat_input_ids", ",", "flat_token_type_ids", ",", "flat_attention_mask", ",", "output_all_encoded_layers", "=", "False", ")", "\n", "pooled_output", "=", "self", ".", "dropout", "(", "pooled_output", ")", "\n", "logits", "=", "self", ".", "classifier", "(", "pooled_output", ")", "\n", "reshaped_logits", "=", "logits", ".", "view", "(", "-", "1", ",", "self", ".", "num_choices", ")", "\n", "\n", "if", "labels", "is", "not", "None", ":", "\n", "            ", "loss_fct", "=", "CrossEntropyLoss", "(", ")", "\n", "loss", "=", "loss_fct", "(", "reshaped_logits", ",", "labels", ")", "\n", "return", "loss", "\n", "", "else", ":", "\n", "            ", "return", "reshaped_logits", "\n", "\n"]], "home.repos.pwc.inspect_result.lawsonabs_pun.None.bert_models.BertForTokenClassification.__init__": [[1057, 1064], ["bert_models.BertPreTrainedModel.__init__", "bert_models.BertModel", "torch.nn.Dropout", "torch.nn.Dropout", "torch.nn.Linear", "torch.nn.Linear", "bert_models.BertForTokenClassification.apply"], "methods", ["home.repos.pwc.inspect_result.lawsonabs_pun.subtask3_v2.gloss.EvalDataset.__init__"], ["def", "__init__", "(", "self", ",", "config", ",", "num_labels", ")", ":", "\n", "        ", "super", "(", "BertForTokenClassification", ",", "self", ")", ".", "__init__", "(", "config", ")", "\n", "self", ".", "num_labels", "=", "num_labels", "\n", "self", ".", "bert", "=", "BertModel", "(", "config", ")", "\n", "self", ".", "dropout", "=", "nn", ".", "Dropout", "(", "config", ".", "hidden_dropout_prob", ")", "\n", "self", ".", "classifier", "=", "nn", ".", "Linear", "(", "config", ".", "hidden_size", ",", "num_labels", ")", "\n", "self", ".", "apply", "(", "self", ".", "init_bert_weights", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.lawsonabs_pun.None.bert_models.BertForTokenClassification.forward": [[1065, 1083], ["bert_models.BertForTokenClassification.bert", "bert_models.BertForTokenClassification.dropout", "bert_models.BertForTokenClassification.classifier", "torch.nn.CrossEntropyLoss", "torch.nn.CrossEntropyLoss", "torch.nn.CrossEntropyLoss.", "torch.nn.CrossEntropyLoss.", "torch.nn.CrossEntropyLoss.", "torch.nn.CrossEntropyLoss.", "attention_mask.view", "bert_models.BertForTokenClassification.view", "labels.view", "bert_models.BertForTokenClassification.view", "labels.view"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "input_ids", ",", "token_type_ids", "=", "None", ",", "attention_mask", "=", "None", ",", "labels", "=", "None", ")", ":", "\n", "        ", "sequence_output", ",", "_", "=", "self", ".", "bert", "(", "input_ids", ",", "token_type_ids", ",", "attention_mask", ",", "output_all_encoded_layers", "=", "False", ")", "\n", "sequence_output", "=", "self", ".", "dropout", "(", "sequence_output", ")", "\n", "logits", "=", "self", ".", "classifier", "(", "sequence_output", ")", "\n", "\n", "if", "labels", "is", "not", "None", ":", "\n", "            ", "loss_fct", "=", "CrossEntropyLoss", "(", ")", "\n", "# Only keep active parts of the loss", "\n", "if", "attention_mask", "is", "not", "None", ":", "\n", "                ", "active_loss", "=", "attention_mask", ".", "view", "(", "-", "1", ")", "==", "1", "\n", "active_logits", "=", "logits", ".", "view", "(", "-", "1", ",", "self", ".", "num_labels", ")", "[", "active_loss", "]", "\n", "active_labels", "=", "labels", ".", "view", "(", "-", "1", ")", "[", "active_loss", "]", "\n", "loss", "=", "loss_fct", "(", "active_logits", ",", "active_labels", ")", "\n", "", "else", ":", "\n", "                ", "loss", "=", "loss_fct", "(", "logits", ".", "view", "(", "-", "1", ",", "self", ".", "num_labels", ")", ",", "labels", ".", "view", "(", "-", "1", ")", ")", "\n", "", "return", "loss", "\n", "", "else", ":", "\n", "            ", "return", "logits", "\n", "\n"]], "home.repos.pwc.inspect_result.lawsonabs_pun.None.bert_models.BertForTokenPronsClassification.__init__": [[1086, 1102], ["bert_models.BertPreTrainedModel.__init__", "bert_models.BertModel", "torch.nn.Dropout", "torch.nn.Dropout", "torch.nn.LSTM", "torch.nn.LSTM", "attention.Attention", "bert_models.BertForTokenPronsClassification.apply", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear"], "methods", ["home.repos.pwc.inspect_result.lawsonabs_pun.subtask3_v2.gloss.EvalDataset.__init__"], ["    ", "def", "__init__", "(", "self", ",", "config", ",", "num_labels", ",", "max_seq_length", ",", "max_prons_length", ",", "pron_emb_size", ",", "do_pron", ")", ":", "\n", "        ", "super", "(", "BertForTokenPronsClassification", ",", "self", ")", ".", "__init__", "(", "config", ")", "\n", "self", ".", "num_labels", "=", "num_labels", "\n", "self", ".", "bert", "=", "BertModel", "(", "config", ")", "\n", "self", ".", "dropout", "=", "nn", ".", "Dropout", "(", "config", ".", "hidden_dropout_prob", ")", "\n", "self", ".", "hidden_size", "=", "pron_emb_size", "\n", "if", "do_pron", ":", "\n", "            ", "self", ".", "classifier", "=", "nn", ".", "Linear", "(", "config", ".", "hidden_size", "+", "self", ".", "hidden_size", ",", "num_labels", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "classifier", "=", "nn", ".", "Linear", "(", "config", ".", "hidden_size", ",", "num_labels", ")", "\n", "", "self", ".", "lstm", "=", "nn", ".", "LSTM", "(", "pron_emb_size", ",", "pron_emb_size", ")", "\n", "self", ".", "attention", "=", "Attention", "(", "self", ".", "hidden_size", ")", "\n", "self", ".", "length_s", "=", "max_seq_length", "\n", "self", ".", "length_p", "=", "max_prons_length", "\n", "self", ".", "emb_p", "=", "pron_emb_size", "\n", "self", ".", "apply", "(", "self", ".", "init_bert_weights", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.lawsonabs_pun.None.bert_models.BertForTokenPronsClassification.forward": [[1103, 1134], ["bert_models.BertForTokenPronsClassification.bert", "bert_models.BertForTokenPronsClassification.dropout", "bert_models.BertForTokenPronsClassification.classifier", "prons.view", "bert_models.BertForTokenPronsClassification.attention", "torch.sum.view", "torch.sum.view", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.nn.CrossEntropyLoss", "torch.nn.CrossEntropyLoss", "torch.nn.CrossEntropyLoss.", "torch.nn.CrossEntropyLoss.", "torch.nn.CrossEntropyLoss.", "torch.nn.CrossEntropyLoss.", "attention_mask.view", "bert_models.BertForTokenPronsClassification.view", "labels.view", "bert_models.BertForTokenPronsClassification.view", "labels.view"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "input_ids", ",", "token_type_ids", "=", "None", ",", "attention_mask", "=", "None", ",", "prons", "=", "None", ",", "prons_mask", "=", "None", ",", "labels", "=", "None", ")", ":", "\n", "\n", "        ", "sequence_output", ",", "_", "=", "self", ".", "bert", "(", "input_ids", ",", "token_type_ids", ",", "attention_mask", ",", "output_all_encoded_layers", "=", "False", ")", "\n", "# sequence_output: (batch_size, sequence_length, config.hidden_size)", "\n", "\n", "if", "prons", "is", "not", "None", ":", "\n", "\n", "            ", "context", "=", "prons", ".", "view", "(", "-", "1", ",", "self", ".", "length_p", ",", "self", ".", "hidden_size", ")", "\n", "#context = pron_output.contiguous()", "\n", "pron_output", ",", "_", "=", "self", ".", "attention", "(", "context", ",", "context", ")", "# self-attention mechanism", "\n", "pron_output", "=", "pron_output", ".", "view", "(", "-", "1", ",", "self", ".", "length_s", ",", "self", ".", "length_p", ",", "self", ".", "hidden_size", ")", "\n", "pron_output", "=", "torch", ".", "sum", "(", "pron_output", ",", "2", ")", "\n", "# pron_output: (batch_size, sequence_length, self.hidden_size)", "\n", "sequence_output", "=", "torch", ".", "cat", "(", "(", "sequence_output", ",", "pron_output", ")", ",", "2", ")", "\n", "\n", "", "sequence_output", "=", "self", ".", "dropout", "(", "sequence_output", ")", "\n", "logits", "=", "self", ".", "classifier", "(", "sequence_output", ")", "\n", "\n", "if", "labels", "is", "not", "None", ":", "\n", "            ", "loss_fct", "=", "CrossEntropyLoss", "(", ")", "\n", "# Only keep active parts of the loss", "\n", "if", "attention_mask", "is", "not", "None", ":", "\n", "                ", "active_loss", "=", "attention_mask", ".", "view", "(", "-", "1", ")", "==", "1", "\n", "active_logits", "=", "logits", ".", "view", "(", "-", "1", ",", "self", ".", "num_labels", ")", "[", "active_loss", "]", "\n", "active_labels", "=", "labels", ".", "view", "(", "-", "1", ")", "[", "active_loss", "]", "\n", "loss", "=", "loss_fct", "(", "active_logits", ",", "active_labels", ")", "\n", "", "else", ":", "\n", "                ", "loss", "=", "loss_fct", "(", "logits", ".", "view", "(", "-", "1", ",", "self", ".", "num_labels", ")", ",", "labels", ".", "view", "(", "-", "1", ")", ")", "\n", "", "return", "loss", ",", "logits", "\n", "", "else", ":", "\n", "            ", "return", "logits", "\n", "\n"]], "home.repos.pwc.inspect_result.lawsonabs_pun.None.bert_models.BertForTokenPronsClassification_v2.__init__": [[1137, 1172], ["bert_models.BertPreTrainedModel.__init__", "bert_models.BertModel", "torch.nn.Dropout", "torch.nn.Dropout", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.parameter.Parameter", "torch.nn.parameter.Parameter", "local_attention.Local_attention", "torch.nn.parameter.Parameter", "torch.nn.parameter.Parameter", "local_attention.Local_attention", "bert_models.BertForTokenPronsClassification_v2.apply", "torch.nn.Linear", "torch.nn.Linear", "torch.rand", "torch.rand", "torch.rand", "torch.rand", "torch.rand", "torch.rand", "torch.rand", "torch.rand"], "methods", ["home.repos.pwc.inspect_result.lawsonabs_pun.subtask3_v2.gloss.EvalDataset.__init__"], ["    ", "def", "__init__", "(", "self", ",", "config", ",", "num_labels", ",", "max_seq_length", ",", "max_prons_length", ",", "pron_emb_size", ",", "do_pron", ",", "use_sense", ",", "defi_num", ")", ":", "\n", "        ", "super", "(", "BertForTokenPronsClassification_v2", ",", "self", ")", ".", "__init__", "(", "config", ")", "\n", "self", ".", "num_labels", "=", "num_labels", "\n", "self", ".", "bert", "=", "BertModel", "(", "config", ")", "\n", "self", ".", "dropout", "=", "nn", ".", "Dropout", "(", "config", ".", "hidden_dropout_prob", ")", "\n", "self", ".", "hidden_size", "=", "pron_emb_size", "\n", "self", ".", "sense_size", "=", "768", "# \u9ed8\u8ba4\u8bbe\u7f6e\u4e3a768\uff0c\u540e\u9762\u518d\u4fee\u6539", "\n", "in_size", "=", "256", "\n", "if", "do_pron", ":", "\n", "            ", "in_size", "+=", "self", ".", "hidden_size", "\n", "", "if", "use_sense", ":", "\n", "            ", "in_size", "+=", "self", ".", "sense_size", "\n", "", "self", ".", "classifier", "=", "nn", ".", "Linear", "(", "in_size", ",", "num_labels", ")", "\n", "\n", "# \u7ed9 pronunciation \u505aattention \u7684\u64cd\u4f5c", "\n", "# \u4e0b\u9762\u8fd9\u4e2a\u5e94\u8be5\u5c31\u662fattention \u4e2d\u968f\u673a\u521d\u59cb\u5316\u7684\u53c2\u6570\uff0c\u8fd9\u4e2a\u7ef4\u5ea6\u662f\u600e\u4e48\u8bbe\u7f6e\u7684\uff1f", "\n", "self", ".", "att_vec", "=", "Parameter", "(", "torch", ".", "rand", "(", "pron_emb_size", "*", "2", ",", "1", ",", "requires_grad", "=", "True", ")", ")", "\n", "self", ".", "attention_1", "=", "Local_attention", "(", "self", ".", "hidden_size", ",", "self", ".", "hidden_size", "*", "2", ")", "\n", "\n", "\n", "# \u7ed9sense \u505a\u7ebf\u6027\u6620\u5c04\u7684 \u77e9\u9635\u53c2\u6570", "\n", "self", ".", "linear_sense", "=", "Parameter", "(", "torch", ".", "rand", "(", "768", "//", "32", ",", "1", ",", "requires_grad", "=", "True", ")", ")", "# \u7ed9sense \u505aattention\u7684\u64cd\u4f5c              ", "\n", "# \u5bf9sense \u505a attention \u64cd\u4f5c", "\n", "# \u8fd9\u91cc\u4f7f\u7528\u7684\u662fattention \u7ef4\u5ea6\u4e0e\u4e4b\u524d\u7684\u4e0d\u540c", "\n", "self", ".", "attention_2", "=", "Local_attention", "(", "768", ",", "768", "//", "32", ")", "\n", "\n", "self", ".", "length_s", "=", "max_seq_length", "\n", "self", ".", "length_p", "=", "max_prons_length", "\n", "self", ".", "emb_p", "=", "pron_emb_size", "\n", "self", ".", "length_defi", "=", "defi_num", "\n", "self", ".", "apply", "(", "self", ".", "init_bert_weights", ")", "\n", "\n", "\n", "# \u6dfb\u52a0\u4e00\u4e2a\u7ed9 sequence_output \u505a\u7ebf\u6027\u6620\u5c04\u7684\u8fc7\u7a0b", "\n", "self", ".", "linear_seqence", "=", "nn", ".", "Linear", "(", "768", ",", "256", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.lawsonabs_pun.None.bert_models.BertForTokenPronsClassification_v2.forward": [[1174, 1223], ["bert_models.BertForTokenPronsClassification_v2.bert", "bert_models.BertForTokenPronsClassification_v2.linear_seqence", "bert_models.BertForTokenPronsClassification_v2.dropout", "bert_models.BertForTokenPronsClassification_v2.classifier", "prons.view", "bert_models.BertForTokenPronsClassification_v2.attention_1", "pron_output.view.view.view", "attention_scores.view", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "defi_emb.view.view.cuda", "defi_emb.view.view.view", "bert_models.BertForTokenPronsClassification_v2.attention_2", "attention_scores_defi.view.view.view", "sense_output.view.view.view", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.nn.CrossEntropyLoss", "torch.nn.CrossEntropyLoss", "torch.nn.CrossEntropyLoss.", "torch.nn.CrossEntropyLoss.", "torch.nn.CrossEntropyLoss.", "torch.nn.CrossEntropyLoss.", "attention_mask.view", "bert_models.BertForTokenPronsClassification_v2.view", "labels.view", "bert_models.BertForTokenPronsClassification_v2.view", "labels.view"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "input_ids", ",", "token_type_ids", "=", "None", ",", "attention_mask", "=", "None", ",", "prons", "=", "None", ",", "prons_mask", "=", "None", ",", "labels", "=", "None", ",", "defi_emb", "=", "None", ")", ":", "\n", "# step1. \u6267\u884cbert\u5f97\u5230\u8f93\u51fa", "\n", "        ", "sequence_output", ",", "_", "=", "self", ".", "bert", "(", "input_ids", ",", "token_type_ids", ",", "attention_mask", ",", "output_all_encoded_layers", "=", "False", ")", "\n", "# sequence_output: (batch_size, sequence_length, config.hidden_size)", "\n", "sequence_output", "=", "self", ".", "linear_seqence", "(", "sequence_output", ")", "# \u5c06768 \u6295\u5f71\u5230 256\u7ef4", "\n", "attention_scores_pron", "=", "None", "\n", "attention_scores_defi", "=", "None", "\n", "# prons:[batch_size,max_seq_length,pron_seq_length,pro_emb_size]", "\n", "if", "prons", "is", "not", "None", ":", "\n", "            ", "pron_context", "=", "prons", ".", "view", "(", "-", "1", ",", "self", ".", "length_p", ",", "self", ".", "hidden_size", ")", "# prons:[batch_size*max_seq_length,pron_seq_length,pro_emb_size]", "\n", "# \u4e3a\u4ec0\u4e48\u53d8\u5316\u7ef4\u5ea6\uff1f", "\n", "# \u53ef\u4ee5\u770b\u5230\u8fd9\u91cc\u662f\u7528attention  \u5f97\u5230\u6700\u540e\u7684 pron embedding.  context size = [4096, 5, 16]", "\n", "pron_output", ",", "attention_scores", "=", "self", ".", "attention_1", "(", "pron_context", ",", "self", ".", "att_vec", ")", "# local attention mechanism", "\n", "pron_output", "=", "pron_output", ".", "view", "(", "-", "1", ",", "self", ".", "length_s", ",", "self", ".", "hidden_size", ")", "\n", "# pron_output: (batch_size, sequence_length, self.hidden_size)", "\n", "\n", "attention_scores_pron", "=", "attention_scores", ".", "view", "(", "-", "1", ",", "self", ".", "length_s", ",", "self", ".", "length_p", ")", "\n", "\n", "# \u5c06bert\u7684\u8f93\u51fa\u548c \u8bad\u7ec3\u5f97\u5230\u7684 pronunciation embedding \u7684attention \u7ed3\u679c\u62fc\u63a5\u5728\u4e00\u8d77", "\n", "sequence_output", "=", "torch", ".", "cat", "(", "(", "sequence_output", ",", "pron_output", ")", ",", "2", ")", "\n", "\n", "# defi_emb \u7684size [batch_size,max_seq_length,defi_num,768]", "\n", "", "if", "defi_emb", "is", "not", "None", ":", "\n", "# \u4ececpu\u79fb\u5230gpu ", "\n", "            ", "defi_emb", "=", "defi_emb", ".", "cuda", "(", ")", "\n", "defi_emb", "=", "defi_emb", ".", "view", "(", "-", "1", ",", "self", ".", "length_defi", ",", "768", ")", "\n", "# \u4f7f\u7528attention \u5904\u7406\u8fd9\u4e2alast_cls_emb", "\n", "sense_output", ",", "attention_scores_defi", "=", "self", ".", "attention_2", "(", "defi_emb", ",", "self", ".", "linear_sense", ")", "\n", "attention_scores_defi", "=", "attention_scores_defi", ".", "view", "(", "-", "1", ",", "self", ".", "length_s", ",", "self", ".", "length_defi", ")", "\n", "sense_output", "=", "sense_output", ".", "view", "(", "-", "1", ",", "self", ".", "length_s", ",", "self", ".", "sense_size", ")", "\n", "# \u76f4\u63a5\u5c06sense\u7684embedding \u62fc\u63a5\u5230\u6bcf\u4e2a\u5355\u8bcd\u7684embedding \u540e\u9762", "\n", "sequence_output", "=", "torch", ".", "cat", "(", "(", "sequence_output", ",", "sense_output", ")", ",", "2", ")", "\n", "\n", "", "sequence_output", "=", "self", ".", "dropout", "(", "sequence_output", ")", "\n", "logits", "=", "self", ".", "classifier", "(", "sequence_output", ")", "\n", "\n", "if", "labels", "is", "not", "None", ":", "\n", "            ", "loss_fct", "=", "CrossEntropyLoss", "(", ")", "\n", "# Only keep active parts of the loss", "\n", "if", "attention_mask", "is", "not", "None", ":", "\n", "                ", "active_loss", "=", "attention_mask", ".", "view", "(", "-", "1", ")", "==", "1", "\n", "active_logits", "=", "logits", ".", "view", "(", "-", "1", ",", "self", ".", "num_labels", ")", "[", "active_loss", "]", "\n", "active_labels", "=", "labels", ".", "view", "(", "-", "1", ")", "[", "active_loss", "]", "\n", "loss", "=", "loss_fct", "(", "active_logits", ",", "active_labels", ")", "\n", "", "else", ":", "\n", "                ", "loss", "=", "loss_fct", "(", "logits", ".", "view", "(", "-", "1", ",", "self", ".", "num_labels", ")", ",", "labels", ".", "view", "(", "-", "1", ")", ")", "\n", "", "return", "loss", ",", "logits", "\n", "", "else", ":", "# \u8bc4\u6d4b\u9636\u6bb5\uff0c\u60f3\u77e5\u9053\u5404\u4e2a\u6743\u91cd\u60c5\u51b5\uff0c\u6240\u4ee5\u8fd9\u91cc\u8fd4\u56de", "\n", "            ", "return", "logits", ",", "attention_scores_pron", ",", "attention_scores_defi", "\n", "# attention_scores_pron size = [train_batch_size,self.length_s,self.length_p]", "\n"]], "home.repos.pwc.inspect_result.lawsonabs_pun.None.bert_models.BertForSequencePronsClassification_v2.__init__": [[1229, 1249], ["bert_models.BertPreTrainedModel.__init__", "bert_models.BertModel", "torch.nn.Dropout", "torch.nn.Dropout", "attention.Attention", "torch.nn.Linear", "torch.nn.Linear", "attention.Attention", "bert_models.BertForSequencePronsClassification_v2.apply"], "methods", ["home.repos.pwc.inspect_result.lawsonabs_pun.subtask3_v2.gloss.EvalDataset.__init__"], ["    ", "def", "__init__", "(", "self", ",", "config", ",", "num_labels", ",", "max_seq_length", ",", "max_prons_length", ",", "pron_emb_size", ",", "do_pron", ")", ":", "\n", "        ", "super", "(", "BertForSequencePronsClassification_v2", ",", "self", ")", ".", "__init__", "(", "config", ")", "\n", "self", ".", "num_labels", "=", "num_labels", "\n", "self", ".", "bert", "=", "BertModel", "(", "config", ")", "\n", "self", ".", "dropout", "=", "nn", ".", "Dropout", "(", "config", ".", "hidden_dropout_prob", ")", "\n", "self", ".", "attention", "=", "Attention", "(", "pron_emb_size", ")", "\n", "\n", "if", "do_pron", ":", "\n", "            ", "self", ".", "hidden_size", "=", "pron_emb_size", "+", "config", ".", "hidden_size", "\n", "", "else", ":", "\n", "            ", "self", ".", "hidden_size", "=", "config", ".", "hidden_size", "\n", "\n", "", "self", ".", "classifier", "=", "nn", ".", "Linear", "(", "config", ".", "hidden_size", "+", "self", ".", "hidden_size", ",", "num_labels", ")", "\n", "self", ".", "attention_2", "=", "Attention", "(", "self", ".", "hidden_size", ")", "\n", "\n", "\n", "self", ".", "length_s", "=", "max_seq_length", "\n", "self", ".", "length_p", "=", "max_prons_length", "\n", "self", ".", "emb_p", "=", "pron_emb_size", "\n", "self", ".", "apply", "(", "self", ".", "init_bert_weights", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.lawsonabs_pun.None.bert_models.BertForSequencePronsClassification_v2.forward": [[1250, 1277], ["bert_models.BertForSequencePronsClassification_v2.bert", "bert_models.BertForSequencePronsClassification_v2.attention_2", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "bert_models.BertForSequencePronsClassification_v2.dropout", "bert_models.BertForSequencePronsClassification_v2.classifier", "prons.view", "bert_models.BertForSequencePronsClassification_v2.attention", "torch.sum.view", "torch.sum.view", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.nn.CrossEntropyLoss", "torch.nn.CrossEntropyLoss", "torch.nn.CrossEntropyLoss.", "torch.nn.CrossEntropyLoss.", "bert_models.BertForSequencePronsClassification_v2.view", "labels.view"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "input_ids", ",", "token_type_ids", "=", "None", ",", "attention_mask", "=", "None", ",", "prons", "=", "None", ",", "prons_mask", "=", "None", ",", "labels", "=", "None", ")", ":", "\n", "\n", "        ", "sequence_output", ",", "pooled_output", "=", "self", ".", "bert", "(", "input_ids", ",", "token_type_ids", ",", "attention_mask", ",", "output_all_encoded_layers", "=", "False", ")", "\n", "\n", "if", "prons", "is", "not", "None", ":", "\n", "\n", "            ", "context", "=", "prons", ".", "view", "(", "-", "1", ",", "self", ".", "length_p", ",", "self", ".", "emb_p", ")", "\n", "pron_output", ",", "_", "=", "self", ".", "attention", "(", "context", ",", "context", ")", "# self-attention mechanism", "\n", "pron_output", "=", "pron_output", ".", "view", "(", "-", "1", ",", "self", ".", "length_s", ",", "self", ".", "length_p", ",", "self", ".", "emb_p", ")", "\n", "pron_output", "=", "torch", ".", "sum", "(", "pron_output", ",", "2", ")", "\n", "sequence_output", "=", "torch", ".", "cat", "(", "(", "sequence_output", ",", "pron_output", ")", ",", "2", ")", "\n", "\n", "", "context", "=", "sequence_output", "\n", "sequence_output", ",", "_", "=", "self", ".", "attention_2", "(", "context", ",", "context", ")", "# self-attention mechanism", "\n", "sequence_output", "=", "torch", ".", "sum", "(", "sequence_output", ",", "1", ")", "\n", "\n", "sequence_output", "=", "self", ".", "dropout", "(", "torch", ".", "cat", "(", "(", "sequence_output", ",", "pooled_output", ")", ",", "-", "1", ")", ")", "\n", "\n", "logits", "=", "self", ".", "classifier", "(", "sequence_output", ")", "\n", "\n", "\n", "if", "labels", "is", "not", "None", ":", "\n", "            ", "loss_fct", "=", "CrossEntropyLoss", "(", ")", "\n", "loss", "=", "loss_fct", "(", "logits", ".", "view", "(", "-", "1", ",", "self", ".", "num_labels", ")", ",", "labels", ".", "view", "(", "-", "1", ")", ")", "\n", "return", "loss", ",", "logits", "\n", "", "else", ":", "\n", "            ", "return", "logits", "\n", "\n"]], "home.repos.pwc.inspect_result.lawsonabs_pun.None.bert_models.BertForSequencePronsClassification_v3.__init__": [[1280, 1301], ["bert_models.BertPreTrainedModel.__init__", "bert_models.BertModel", "torch.nn.Dropout", "torch.nn.Dropout", "local_attention.Local_attention", "torch.nn.Linear", "torch.nn.Linear", "attention.Attention", "torch.nn.parameter.Parameter", "torch.nn.parameter.Parameter", "bert_models.BertForSequencePronsClassification_v3.apply", "torch.rand", "torch.rand", "torch.rand", "torch.rand"], "methods", ["home.repos.pwc.inspect_result.lawsonabs_pun.subtask3_v2.gloss.EvalDataset.__init__"], ["    ", "def", "__init__", "(", "self", ",", "config", ",", "num_labels", ",", "max_seq_length", ",", "max_prons_length", ",", "pron_emb_size", ",", "do_pron", ",", "device", ")", ":", "\n", "        ", "super", "(", "BertForSequencePronsClassification_v3", ",", "self", ")", ".", "__init__", "(", "config", ")", "\n", "self", ".", "num_labels", "=", "num_labels", "\n", "self", ".", "bert", "=", "BertModel", "(", "config", ")", "\n", "self", ".", "dropout", "=", "nn", ".", "Dropout", "(", "config", ".", "hidden_dropout_prob", ")", "\n", "self", ".", "attention", "=", "Local_attention", "(", "pron_emb_size", ")", "\n", "\n", "if", "do_pron", ":", "\n", "            ", "self", ".", "hidden_size", "=", "pron_emb_size", "+", "config", ".", "hidden_size", "\n", "", "else", ":", "\n", "            ", "self", ".", "hidden_size", "=", "config", ".", "hidden_size", "\n", "\n", "", "self", ".", "classifier", "=", "nn", ".", "Linear", "(", "config", ".", "hidden_size", "+", "self", ".", "hidden_size", ",", "num_labels", ")", "\n", "self", ".", "attention_2", "=", "Attention", "(", "self", ".", "hidden_size", ")", "\n", "self", ".", "att_vec", "=", "Parameter", "(", "torch", ".", "rand", "(", "pron_emb_size", "*", "2", ",", "1", ",", "device", "=", "device", ",", "requires_grad", "=", "True", ")", ")", "\n", "\n", "\n", "self", ".", "length_s", "=", "max_seq_length", "\n", "self", ".", "length_p", "=", "max_prons_length", "\n", "self", ".", "emb_p", "=", "pron_emb_size", "\n", "self", ".", "apply", "(", "self", ".", "init_bert_weights", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.lawsonabs_pun.None.bert_models.BertForSequencePronsClassification_v3.forward": [[1302, 1327], ["bert_models.BertForSequencePronsClassification_v3.bert", "bert_models.BertForSequencePronsClassification_v3.attention_2", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "bert_models.BertForSequencePronsClassification_v3.dropout", "bert_models.BertForSequencePronsClassification_v3.classifier", "prons.view", "bert_models.BertForSequencePronsClassification_v3.attention", "pron_output.view.view.view", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.nn.CrossEntropyLoss", "torch.nn.CrossEntropyLoss", "torch.nn.CrossEntropyLoss.", "torch.nn.CrossEntropyLoss.", "bert_models.BertForSequencePronsClassification_v3.view", "labels.view"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "input_ids", ",", "token_type_ids", "=", "None", ",", "attention_mask", "=", "None", ",", "prons", "=", "None", ",", "prons_mask", "=", "None", ",", "labels", "=", "None", ")", ":", "\n", "\n", "        ", "sequence_output", ",", "pooled_output", "=", "self", ".", "bert", "(", "input_ids", ",", "token_type_ids", ",", "attention_mask", ",", "output_all_encoded_layers", "=", "False", ")", "\n", "\n", "if", "prons", "is", "not", "None", ":", "\n", "\n", "            ", "context", "=", "prons", ".", "view", "(", "-", "1", ",", "self", ".", "length_p", ",", "self", ".", "emb_p", ")", "\n", "pron_output", ",", "attention_scores_1", "=", "self", ".", "attention", "(", "context", ",", "self", ".", "att_vec", ")", "# local-attention mechanism", "\n", "pron_output", "=", "pron_output", ".", "view", "(", "-", "1", ",", "self", ".", "length_s", ",", "self", ".", "emb_p", ")", "\n", "sequence_output", "=", "torch", ".", "cat", "(", "(", "sequence_output", ",", "pron_output", ")", ",", "2", ")", "\n", "\n", "", "context", "=", "sequence_output", "\n", "sequence_output", ",", "attention_scores_2", "=", "self", ".", "attention_2", "(", "context", ",", "context", ")", "# self-attention mechanism", "\n", "sequence_output", "=", "torch", ".", "sum", "(", "sequence_output", ",", "1", ")", "\n", "sequence_output", "=", "self", ".", "dropout", "(", "torch", ".", "cat", "(", "(", "sequence_output", ",", "pooled_output", ")", ",", "-", "1", ")", ")", "\n", "\n", "logits", "=", "self", ".", "classifier", "(", "sequence_output", ")", "\n", "\n", "\n", "if", "labels", "is", "not", "None", ":", "\n", "            ", "loss_fct", "=", "CrossEntropyLoss", "(", ")", "\n", "loss", "=", "loss_fct", "(", "logits", ".", "view", "(", "-", "1", ",", "self", ".", "num_labels", ")", ",", "labels", ".", "view", "(", "-", "1", ")", ")", "\n", "return", "loss", ",", "logits", "\n", "", "else", ":", "\n", "            ", "return", "logits", ",", "attention_scores_2", "", "", "", "", ""]], "home.repos.pwc.inspect_result.lawsonabs_pun.None.bert_models.load_tf_weights_in_bert": [[59, 118], ["os.path.abspath", "print", "tf.train.list_variables", "zip", "print", "tf.train.load_variable", "names.append", "arrays.append", "name.split.split", "any", "print", "torch.from_numpy", "torch.from_numpy", "print", "print", "re.fullmatch", "getattr", "re.split", "getattr", "len", "int", "np.transpose", "getattr", "getattr", "getattr"], "function", ["None"], ["def", "load_tf_weights_in_bert", "(", "model", ",", "tf_checkpoint_path", ")", ":", "\n", "    ", "\"\"\" Load tf checkpoints in a pytorch model\n    \"\"\"", "\n", "try", ":", "\n", "        ", "import", "re", "\n", "import", "numpy", "as", "np", "\n", "import", "tensorflow", "as", "tf", "\n", "", "except", "ImportError", ":", "\n", "        ", "print", "(", "\"Loading a TensorFlow models in PyTorch, requires TensorFlow to be installed. Please see \"", "\n", "\"https://www.tensorflow.org/install/ for installation instructions.\"", ")", "\n", "raise", "\n", "", "tf_path", "=", "os", ".", "path", ".", "abspath", "(", "tf_checkpoint_path", ")", "\n", "print", "(", "\"Converting TensorFlow checkpoint from {}\"", ".", "format", "(", "tf_path", ")", ")", "\n", "# Load weights from TF model", "\n", "init_vars", "=", "tf", ".", "train", ".", "list_variables", "(", "tf_path", ")", "\n", "names", "=", "[", "]", "\n", "arrays", "=", "[", "]", "\n", "for", "name", ",", "shape", "in", "init_vars", ":", "\n", "        ", "print", "(", "\"Loading TF weight {} with shape {}\"", ".", "format", "(", "name", ",", "shape", ")", ")", "\n", "array", "=", "tf", ".", "train", ".", "load_variable", "(", "tf_path", ",", "name", ")", "\n", "names", ".", "append", "(", "name", ")", "\n", "arrays", ".", "append", "(", "array", ")", "\n", "\n", "", "for", "name", ",", "array", "in", "zip", "(", "names", ",", "arrays", ")", ":", "\n", "        ", "name", "=", "name", ".", "split", "(", "'/'", ")", "\n", "# adam_v and adam_m are variables used in AdamWeightDecayOptimizer to calculated m and v", "\n", "# which are not required for using pretrained model", "\n", "if", "any", "(", "n", "in", "[", "\"adam_v\"", ",", "\"adam_m\"", "]", "for", "n", "in", "name", ")", ":", "\n", "            ", "print", "(", "\"Skipping {}\"", ".", "format", "(", "\"/\"", ".", "join", "(", "name", ")", ")", ")", "\n", "continue", "\n", "", "pointer", "=", "model", "\n", "for", "m_name", "in", "name", ":", "\n", "            ", "if", "re", ".", "fullmatch", "(", "r'[A-Za-z]+_\\d+'", ",", "m_name", ")", ":", "\n", "                ", "l", "=", "re", ".", "split", "(", "r'_(\\d+)'", ",", "m_name", ")", "\n", "", "else", ":", "\n", "                ", "l", "=", "[", "m_name", "]", "\n", "", "if", "l", "[", "0", "]", "==", "'kernel'", "or", "l", "[", "0", "]", "==", "'gamma'", ":", "\n", "                ", "pointer", "=", "getattr", "(", "pointer", ",", "'weight'", ")", "\n", "", "elif", "l", "[", "0", "]", "==", "'output_bias'", "or", "l", "[", "0", "]", "==", "'beta'", ":", "\n", "                ", "pointer", "=", "getattr", "(", "pointer", ",", "'bias'", ")", "\n", "", "elif", "l", "[", "0", "]", "==", "'output_weights'", ":", "\n", "                ", "pointer", "=", "getattr", "(", "pointer", ",", "'weight'", ")", "\n", "", "else", ":", "\n", "                ", "pointer", "=", "getattr", "(", "pointer", ",", "l", "[", "0", "]", ")", "\n", "", "if", "len", "(", "l", ")", ">=", "2", ":", "\n", "                ", "num", "=", "int", "(", "l", "[", "1", "]", ")", "\n", "pointer", "=", "pointer", "[", "num", "]", "\n", "", "", "if", "m_name", "[", "-", "11", ":", "]", "==", "'_embeddings'", ":", "\n", "            ", "pointer", "=", "getattr", "(", "pointer", ",", "'weight'", ")", "\n", "", "elif", "m_name", "==", "'kernel'", ":", "\n", "            ", "array", "=", "np", ".", "transpose", "(", "array", ")", "\n", "", "try", ":", "\n", "            ", "assert", "pointer", ".", "shape", "==", "array", ".", "shape", "\n", "", "except", "AssertionError", "as", "e", ":", "\n", "            ", "e", ".", "args", "+=", "(", "pointer", ".", "shape", ",", "array", ".", "shape", ")", "\n", "raise", "\n", "", "print", "(", "\"Initialize PyTorch weight {}\"", ".", "format", "(", "name", ")", ")", "\n", "pointer", ".", "data", "=", "torch", ".", "from_numpy", "(", "array", ")", "\n", "", "return", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.lawsonabs_pun.None.bert_models.gelu": [[120, 127], ["torch.erf", "torch.erf", "math.sqrt"], "function", ["None"], ["", "def", "gelu", "(", "x", ")", ":", "\n", "    ", "\"\"\"Implementation of the gelu activation function.\n        For information: OpenAI GPT's gelu is slightly different (and gives slightly different results):\n        0.5 * x * (1 + torch.tanh(math.sqrt(2 / math.pi) * (x + 0.044715 * torch.pow(x, 3))))\n        Also see https://arxiv.org/abs/1606.08415\n    \"\"\"", "\n", "return", "x", "*", "0.5", "*", "(", "1.0", "+", "torch", ".", "erf", "(", "x", "/", "math", ".", "sqrt", "(", "2.0", ")", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.lawsonabs_pun.None.bert_models.swish": [[129, 131], ["torch.sigmoid", "torch.sigmoid"], "function", ["None"], ["", "def", "swish", "(", "x", ")", ":", "\n", "    ", "return", "x", "*", "torch", ".", "sigmoid", "(", "x", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.lawsonabs_pun.None.prob_pi.read_data": [[3, 30], ["open", "line.split", "sentence.append", "label.append", "prons.append", "len", "data.append", "line.startswith", "[].split", "len", "len", "data.append"], "function", ["None"], ["def", "read_data", "(", "filename", ")", ":", "\n", "\n", "    ", "f", "=", "open", "(", "filename", ",", "encoding", "=", "'utf-8'", ")", "\n", "data", "=", "[", "]", "\n", "sentence", "=", "[", "]", "\n", "label", "=", "[", "]", "\n", "prons", "=", "[", "]", "\n", "for", "line", "in", "f", ":", "\n", "        ", "if", "len", "(", "line", ")", "==", "0", "or", "line", ".", "startswith", "(", "'-DOCSTART'", ")", "or", "line", "[", "0", "]", "==", "\"\\n\"", ":", "\n", "            ", "if", "len", "(", "sentence", ")", ">", "0", ":", "\n", "                ", "data", ".", "append", "(", "(", "sentence", ",", "label", ",", "prons", ")", ")", "\n", "sentence", "=", "[", "]", "\n", "label", "=", "[", "]", "\n", "prons", "=", "[", "]", "\n", "", "continue", "\n", "", "splits", "=", "line", ".", "split", "(", "' '", ")", "\n", "sentence", ".", "append", "(", "splits", "[", "0", "]", ")", "\n", "label", ".", "append", "(", "splits", "[", "-", "2", "]", ")", "\n", "prons", ".", "append", "(", "splits", "[", "-", "1", "]", "[", ":", "-", "1", "]", ".", "split", "(", "','", ")", ")", "\n", "\n", "", "if", "len", "(", "sentence", ")", ">", "0", ":", "\n", "        ", "data", ".", "append", "(", "(", "sentence", ",", "label", ",", "prons", ")", ")", "\n", "sentence", "=", "[", "]", "\n", "label", "=", "[", "]", "\n", "prons", "=", "[", "]", "\n", "\n", "", "return", "data", "\n", "\n"]], "home.repos.pwc.inspect_result.lawsonabs_pun.None.prob_pi.main": [[32, 64], ["prob_pi.read_data", "float", "unit[].split", "enumerate", "x.split", "len"], "function", ["home.repos.pwc.inspect_result.lawsonabs_pun.None.prob_pi.read_data"], ["", "def", "main", "(", "argv", ")", ":", "\n", "\t", "filename", "=", "argv", "[", "1", "]", "\n", "pi", "=", "argv", "[", "2", "]", "\n", "\n", "data", "=", "read_data", "(", "filename", ")", "\n", "\n", "num_tokens", "=", "0", "\n", "num_pun_has_pi", "=", "0", "\n", "num_pi", "=", "0", "\n", "num_puns", "=", "0", "\n", "num_prons", "=", "0", "\n", "\n", "for", "sent", "in", "data", ":", "\n", "\n", "\t\t", "for", "unit", "in", "sent", ":", "\n", "\n", "\t\t\t", "label", "=", "unit", "[", "0", "]", "\n", "\n", "if", "label", "==", "P", ":", "num_puns", "+=", "1", "\n", "\n", "token", "=", "unit", "[", "1", "]", "\n", "prons", "=", "unit", "[", "2", "]", ".", "split", "(", "' '", ")", "\n", "prons", "=", "[", "x", ".", "split", "(", "','", ")", "for", "x", "in", "prons", "]", "\n", "\n", "for", "i", ",", "tok", "in", "enumerate", "(", "tokens", ")", ":", "\n", "\t\t\t\t", "num_tokens", "+=", "1", "\n", "if", "pi", "in", "prons", "[", "i", "]", ":", "\n", "\t\t\t\t\t", "num_pi", "+=", "1", "\n", "", "num_prons", "+=", "len", "(", "prons", "[", "i", "]", ">", "0", ")", "\n", "\n", "\n", "", "", "", "p_y", "=", "num_pun", "/", "float", "(", "num_tokens", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.lawsonabs_pun.None.attention.Attention.__init__": [[26, 39], ["torch.Module.__init__", "torch.Linear", "torch.Linear", "torch.Softmax", "torch.Softmax", "torch.Tanh", "torch.Tanh", "ValueError", "torch.Linear", "torch.Linear"], "methods", ["home.repos.pwc.inspect_result.lawsonabs_pun.subtask3_v2.gloss.EvalDataset.__init__"], ["def", "__init__", "(", "self", ",", "dimensions", ",", "attention_type", "=", "'general'", ")", ":", "\n", "        ", "super", "(", "Attention", ",", "self", ")", ".", "__init__", "(", ")", "\n", "\n", "if", "attention_type", "not", "in", "[", "'dot'", ",", "'general'", "]", ":", "\n", "            ", "raise", "ValueError", "(", "'Invalid attention type selected.'", ")", "\n", "\n", "", "self", ".", "attention_type", "=", "attention_type", "\n", "if", "self", ".", "attention_type", "==", "'general'", ":", "\n", "            ", "self", ".", "linear_in", "=", "nn", ".", "Linear", "(", "dimensions", ",", "dimensions", ",", "bias", "=", "False", ")", "\n", "\n", "", "self", ".", "linear_out", "=", "nn", ".", "Linear", "(", "dimensions", "*", "2", ",", "dimensions", ",", "bias", "=", "False", ")", "\n", "self", ".", "softmax", "=", "nn", ".", "Softmax", "(", "dim", "=", "-", "1", ")", "\n", "self", ".", "tanh", "=", "nn", ".", "Tanh", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.lawsonabs_pun.None.attention.Attention.masked_softmax": [[40, 43], ["attention.Attention.softmax"], "methods", ["None"], ["", "def", "masked_softmax", "(", "self", ",", "T", ")", ":", "\n", "        ", "T", "[", "T", "==", "0", "]", "=", "-", "10", "^", "20", "\n", "return", "self", ".", "softmax", "(", "T", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.lawsonabs_pun.None.attention.Attention.forward": [[45, 94], ["query.view.view.size", "context.size", "torch.bmm", "torch.bmm", "torch.bmm", "torch.bmm", "attention_scores.view.view.view", "attention.Attention.masked_softmax", "attention_weights.view.view.view", "torch.bmm", "torch.bmm", "torch.bmm", "torch.bmm", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "combined.view.view.view", "attention.Attention.linear_out().view", "attention.Attention.tanh", "query.view.view.view", "attention.Attention.linear_in", "query.view.view.view", "context.transpose().contiguous", "attention.Attention.linear_out", "context.transpose"], "methods", ["home.repos.pwc.inspect_result.lawsonabs_pun.None.attention.Attention.masked_softmax"], ["", "def", "forward", "(", "self", ",", "query", ",", "context", ")", ":", "\n", "        ", "\"\"\"\n        Args:\n            query (:class:`torch.FloatTensor` [batch size, output length, dimensions]): Sequence of\n                queries to query the context.\n            context (:class:`torch.FloatTensor` [batch size, query length, dimensions]): Data\n                overwhich to apply the attention mechanism.\n        Returns:\n            :class:`tuple` with `output` and `weights`:\n            * **output** (:class:`torch.LongTensor` [batch size, output length, dimensions]):\n              Tensor containing the attended features.\n            * **weights** (:class:`torch.FloatTensor` [batch size, output length, query length]):\n              Tensor containing attention weights.\n        \"\"\"", "\n", "batch_size", ",", "output_len", ",", "dimensions", "=", "query", ".", "size", "(", ")", "\n", "query_len", "=", "context", ".", "size", "(", "1", ")", "\n", "\n", "\n", "if", "self", ".", "attention_type", "==", "\"general\"", ":", "\n", "            ", "query", "=", "query", ".", "view", "(", "batch_size", "*", "output_len", ",", "dimensions", ")", "\n", "query", "=", "self", ".", "linear_in", "(", "query", ")", "\n", "query", "=", "query", ".", "view", "(", "batch_size", ",", "output_len", ",", "dimensions", ")", "\n", "\n", "# TODO: Include mask on PADDING_INDEX?", "\n", "\n", "# (batch_size, output_len, dimensions) * (batch_size, query_len, dimensions) ->", "\n", "# (batch_size, output_len, query_len)", "\n", "", "attention_scores", "=", "torch", ".", "bmm", "(", "query", ",", "context", ".", "transpose", "(", "1", ",", "2", ")", ".", "contiguous", "(", ")", ")", "\n", "#attention_scores = torch.bmm(context.transpose(1, 2).contiguous(), query)", "\n", "\n", "# Compute weights across every context sequence", "\n", "attention_scores", "=", "attention_scores", ".", "view", "(", "batch_size", "*", "output_len", ",", "query_len", ")", "\n", "attention_weights", "=", "self", ".", "masked_softmax", "(", "attention_scores", ")", "\n", "attention_weights", "=", "attention_weights", ".", "view", "(", "batch_size", ",", "output_len", ",", "query_len", ")", "\n", "\n", "# (batch_size, output_len, query_len) * (batch_size, query_len, dimensions) ->", "\n", "# (batch_size, output_len, dimensions)", "\n", "mix", "=", "torch", ".", "bmm", "(", "attention_weights", ",", "context", ")", "\n", "\n", "# concat -> (batch_size * output_len, 2*dimensions)", "\n", "combined", "=", "torch", ".", "cat", "(", "(", "mix", ",", "query", ")", ",", "dim", "=", "2", ")", "\n", "combined", "=", "combined", ".", "view", "(", "batch_size", "*", "output_len", ",", "2", "*", "dimensions", ")", "\n", "\n", "# Apply linear_out on every 2nd dimension of concat", "\n", "# output -> (batch_size, output_len, dimensions)", "\n", "output", "=", "self", ".", "linear_out", "(", "combined", ")", ".", "view", "(", "batch_size", ",", "output_len", ",", "dimensions", ")", "\n", "output", "=", "self", ".", "tanh", "(", "output", ")", "\n", "\n", "return", "output", ",", "attention_weights", "\n", "", "", ""]], "home.repos.pwc.inspect_result.lawsonabs_pun.None.analyse.analyse": [[5, 28], ["open", "f.readline", "f.readline", "f.readline", "line.split.strip", "line.split.split", "print"], "function", ["None"], ["def", "analyse", "(", "path", ")", ":", "\n", "    ", "cur", "=", "\"\"", "\n", "error_flag", "=", "0", "# \u9519\u8bef\u9884\u6d4b\u7684\u6807\u5fd7", "\n", "cnt", "=", "0", "# \u8ba1\u7b97\u6700\u540e\u9884\u6d4b\u5931\u8d25\u7684", "\n", "with", "open", "(", "path", ",", "\"r\"", ")", "as", "f", ":", "\n", "        ", "line", "=", "f", ".", "readline", "(", ")", "\n", "while", "line", ":", "\n", "            ", "if", "line", "==", "'\\n'", ":", "# \u5982\u679c\u662f\u5355\u7eaf\u7684\u6362\u884c", "\n", "                ", "if", "error_flag", ":", "\n", "                    ", "print", "(", "cur", ")", "\n", "error_flag", "=", "0", "\n", "", "cur", "=", "\"\"", "# \u91cd\u7f6e", "\n", "line", "=", "f", ".", "readline", "(", ")", "\n", "continue", "\n", "", "else", ":", "\n", "                ", "line", "=", "line", ".", "strip", "(", ")", "# \u53bb\u6362\u884c", "\n", "line", "=", "line", ".", "split", "(", ")", "# \u751f\u6210\u6570\u7ec4                ", "\n", "cur", "+=", "(", "line", "[", "0", "]", "+", "\" \"", ")", "\n", "if", "(", "line", "[", "1", "]", "==", "'P'", "and", "line", "[", "2", "]", "!=", "'P'", ")", "or", "(", "line", "[", "1", "]", "!=", "'P'", "and", "line", "[", "2", "]", "==", "'P'", ")", ":", "# \u5982\u679c\u4e24\u8005\u7684\u7ed3\u679c\u4e0d\u5339\u914d", "\n", "                    ", "error_flag", "=", "1", "# \u9519\u8bef\u6837\u4f8b", "\n", "cnt", "+=", "1", "\n", "", "", "line", "=", "f", ".", "readline", "(", ")", "\n", "", "", "return", "cnt", "\n", "\n"]], "home.repos.pwc.inspect_result.lawsonabs_pun.None.bert_utils.InputExample.__init__": [[29, 46], ["None"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "guid", ",", "text_a", ",", "text_b", "=", "None", ",", "label", "=", "None", ",", "prons", "=", "None", ")", ":", "\n", "        ", "\"\"\"Constructs a InputExample.\n\n        Args:\n            guid: Unique id for the example.\n            text_a: string. The untokenized text of the first sequence. For single\n            sequence tasks, only this sequence must be specified.\n            text_b: (Optional) string. The untokenized text of the second sequence.\n            Only must be specified for sequence pair tasks.\n            label: (Optional) string. The label of the example. This should be\n            specified for train and dev examples, but not for test examples.\n        \"\"\"", "\n", "self", ".", "guid", "=", "guid", "\n", "self", ".", "text_a", "=", "text_a", "\n", "self", ".", "text_b", "=", "text_b", "\n", "self", ".", "label", "=", "label", "\n", "self", ".", "prons", "=", "prons", "\n", "\n"]], "home.repos.pwc.inspect_result.lawsonabs_pun.None.bert_utils.InputFeatures.__init__": [[50, 55], ["None"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "input_ids", ",", "input_mask", ",", "segment_ids", ",", "label_id", ")", ":", "\n", "        ", "self", ".", "input_ids", "=", "input_ids", "\n", "self", ".", "input_mask", "=", "input_mask", "\n", "self", ".", "segment_ids", "=", "segment_ids", "\n", "self", ".", "label_id", "=", "label_id", "\n", "\n"]], "home.repos.pwc.inspect_result.lawsonabs_pun.None.bert_utils.InputPronFeatures.__init__": [[59, 66], ["None"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "input_ids", ",", "input_mask", ",", "segment_ids", ",", "label_id", ",", "prons_id", ",", "prons_att_mask", ")", ":", "\n", "        ", "self", ".", "input_ids", "=", "input_ids", "\n", "self", ".", "input_mask", "=", "input_mask", "\n", "self", ".", "segment_ids", "=", "segment_ids", "\n", "self", ".", "label_id", "=", "label_id", "\n", "self", ".", "prons_id", "=", "prons_id", "\n", "self", ".", "prons_att_mask", "=", "prons_att_mask", "\n", "\n"]], "home.repos.pwc.inspect_result.lawsonabs_pun.None.bert_utils.DataProcessor.get_train_examples": [[102, 105], ["NotImplementedError"], "methods", ["None"], ["def", "get_train_examples", "(", "self", ",", "data_dir", ")", ":", "\n", "        ", "\"\"\"Gets a collection of `InputExample`s for the train set.\"\"\"", "\n", "raise", "NotImplementedError", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.lawsonabs_pun.None.bert_utils.DataProcessor.get_dev_examples": [[106, 109], ["NotImplementedError"], "methods", ["None"], ["", "def", "get_dev_examples", "(", "self", ",", "data_dir", ")", ":", "\n", "        ", "\"\"\"Gets a collection of `InputExample`s for the dev set.\"\"\"", "\n", "raise", "NotImplementedError", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.lawsonabs_pun.None.bert_utils.DataProcessor.get_labels": [[110, 113], ["NotImplementedError"], "methods", ["None"], ["", "def", "get_labels", "(", "self", ")", ":", "\n", "        ", "\"\"\"Gets the list of labels for this data set.\"\"\"", "\n", "raise", "NotImplementedError", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.lawsonabs_pun.None.bert_utils.DataProcessor._read_csv": [[114, 118], ["bert_utils.readfile"], "methods", ["home.repos.pwc.inspect_result.lawsonabs_pun.None.bert_utils.readfile"], ["", "@", "classmethod", "\n", "def", "_read_csv", "(", "cls", ",", "input_file", ",", "quotechar", "=", "None", ")", ":", "\n", "        ", "\"\"\"Reads a tab separated value file.\"\"\"", "\n", "return", "readfile", "(", "input_file", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.lawsonabs_pun.None.bert_utils.DataProcessor._read_tsv": [[119, 130], ["open", "csv.reader", "lines.append", "list", "unicode"], "methods", ["None"], ["", "@", "classmethod", "\n", "def", "_read_tsv", "(", "cls", ",", "input_file", ",", "quotechar", "=", "None", ")", ":", "\n", "        ", "\"\"\"Reads a tab separated value file.\"\"\"", "\n", "with", "open", "(", "input_file", ",", "\"r\"", ",", "encoding", "=", "'utf-8'", ")", "as", "f", ":", "\n", "            ", "reader", "=", "csv", ".", "reader", "(", "f", ",", "delimiter", "=", "\"\\t\"", ",", "quotechar", "=", "quotechar", ")", "\n", "lines", "=", "[", "]", "\n", "for", "line", "in", "reader", ":", "\n", "                ", "if", "sys", ".", "version_info", "[", "0", "]", "==", "2", ":", "\n", "                    ", "line", "=", "list", "(", "unicode", "(", "cell", ",", "'utf-8'", ")", "for", "cell", "in", "line", ")", "\n", "", "lines", ".", "append", "(", "line", ")", "\n", "", "", "return", "lines", "\n", "\n"]], "home.repos.pwc.inspect_result.lawsonabs_pun.None.bert_utils.NerProcessor.get_train_examples": [[137, 142], ["bert_utils.NerProcessor._create_examples", "bert_utils.NerProcessor._read_csv", "os.path.join"], "methods", ["home.repos.pwc.inspect_result.lawsonabs_pun.None.bert_utils.ScProcessor._create_examples", "home.repos.pwc.inspect_result.lawsonabs_pun.None.bert_utils.DataProcessor._read_csv"], ["def", "get_train_examples", "(", "self", ",", "data_dir", ")", ":", "\n", "        ", "\"\"\"See base class.\"\"\"", "\n", "return", "self", ".", "_create_examples", "(", "\n", "# \u4f7f\u7528\u7ee7\u627f\u7236\u7c7b\u7684\u8bfb\u53d6\u6587\u4ef6\u7684\u65b9\u6cd5", "\n", "self", ".", "_read_csv", "(", "os", ".", "path", ".", "join", "(", "data_dir", ",", "\"train.txt\"", ")", ")", ",", "\"train\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.lawsonabs_pun.None.bert_utils.NerProcessor.get_dev_examples": [[143, 147], ["bert_utils.NerProcessor._create_examples", "bert_utils.NerProcessor._read_csv", "os.path.join"], "methods", ["home.repos.pwc.inspect_result.lawsonabs_pun.None.bert_utils.ScProcessor._create_examples", "home.repos.pwc.inspect_result.lawsonabs_pun.None.bert_utils.DataProcessor._read_csv"], ["", "def", "get_dev_examples", "(", "self", ",", "data_dir", ")", ":", "\n", "        ", "\"\"\"See base class.\"\"\"", "\n", "return", "self", ".", "_create_examples", "(", "\n", "self", ".", "_read_csv", "(", "os", ".", "path", ".", "join", "(", "data_dir", ",", "\"valid.txt\"", ")", ")", ",", "\"dev\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.lawsonabs_pun.None.bert_utils.NerProcessor.get_test_examples": [[148, 152], ["bert_utils.NerProcessor._create_examples", "bert_utils.NerProcessor._read_csv", "os.path.join"], "methods", ["home.repos.pwc.inspect_result.lawsonabs_pun.None.bert_utils.ScProcessor._create_examples", "home.repos.pwc.inspect_result.lawsonabs_pun.None.bert_utils.DataProcessor._read_csv"], ["", "def", "get_test_examples", "(", "self", ",", "data_dir", ")", ":", "\n", "        ", "\"\"\"See base class.\"\"\"", "\n", "return", "self", ".", "_create_examples", "(", "\n", "self", ".", "_read_csv", "(", "os", ".", "path", ".", "join", "(", "data_dir", ",", "\"test.txt\"", ")", ")", ",", "\"test\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.lawsonabs_pun.None.bert_utils.NerProcessor.get_labels": [[153, 159], ["None"], "methods", ["None"], ["", "def", "get_labels", "(", "self", ")", ":", "\n", "#return [\"O\", \"B-MISC\", \"I-MISC\",  \"B-PER\", \"I-PER\", \"B-ORG\", \"I-ORG\", \"B-LOC\", \"I-LOC\", \"X\", \"[CLS]\", \"[SEP]\"]", "\n", "#\u8fd9\u4e2a\u5177\u4f53\u7684\u5404\u4e2a\u683c\u5f0f\u6709\u4ec0\u4e48\u542b\u4e49\uff1f => \u7c7b\u4f3c\u4e8ener\u4e2d\u7684\u4efb\u52a1\uff0c\u505a\u4e00\u4e2a\u6807\u8bb0\u800c\u5df2", "\n", "# \u4f46\u662f\u8fd9\u91cc\u7684X \u662f\u4ec0\u4e48\u610f\u601d\uff1f => bert \u4e2d\u7684 tokenizer \u8fc7\u7a0b\u53ef\u80fd\u4f1a\u5c06\u4e00\u4e2a\u8bcd\u5206\u6210\u591a\u4e2a\uff0c\u8fd9\u91cc\u8bcd\u7684\u540e\u51e0\u90e8\u5206\u5c31\u4f1a\u88ab\u6807\u8bb0\u4e3aX ", "\n", "#return [\"O\", \"P\", \"X\", \"[CLS]\", \"[SEP]\"]", "\n", "        ", "return", "[", "\"O\"", ",", "\"P\"", ",", "\"[CLS]\"", ",", "\"[SEP]\"", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.lawsonabs_pun.None.bert_utils.NerProcessor._create_examples": [[161, 171], ["enumerate", "examples.append", "bert_utils.InputExample"], "methods", ["None"], ["", "def", "_create_examples", "(", "self", ",", "lines", ",", "set_type", ")", ":", "\n", "        ", "examples", "=", "[", "]", "\n", "for", "i", ",", "(", "sentence", ",", "label", ",", "prons", ")", "in", "enumerate", "(", "lines", ")", ":", "\n", "            ", "guid", "=", "\"%s-%s\"", "%", "(", "set_type", ",", "i", ")", "\n", "text_a", "=", "' '", ".", "join", "(", "sentence", ")", "\n", "text_b", "=", "None", "\n", "label", "=", "label", "\n", "prons", "=", "prons", "\n", "examples", ".", "append", "(", "InputExample", "(", "guid", "=", "guid", ",", "text_a", "=", "text_a", ",", "text_b", "=", "text_b", ",", "label", "=", "label", ",", "prons", "=", "prons", ")", ")", "\n", "", "return", "examples", "\n", "\n"]], "home.repos.pwc.inspect_result.lawsonabs_pun.None.bert_utils.ScProcessor.get_train_examples": [[175, 179], ["bert_utils.ScProcessor._create_examples", "bert_utils.ScProcessor._read_tsv", "os.path.join"], "methods", ["home.repos.pwc.inspect_result.lawsonabs_pun.None.bert_utils.ScProcessor._create_examples", "home.repos.pwc.inspect_result.lawsonabs_pun.None.bert_utils.DataProcessor._read_tsv"], ["def", "get_train_examples", "(", "self", ",", "data_dir", ")", ":", "\n", "        ", "\"\"\"See base class.\"\"\"", "\n", "return", "self", ".", "_create_examples", "(", "\n", "self", ".", "_read_tsv", "(", "os", ".", "path", ".", "join", "(", "data_dir", ",", "\"train.tsv\"", ")", ")", ",", "\"train\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.lawsonabs_pun.None.bert_utils.ScProcessor.get_labels": [[180, 183], ["None"], "methods", ["None"], ["", "def", "get_labels", "(", "self", ")", ":", "\n", "        ", "\"\"\"See base class.\"\"\"", "\n", "return", "[", "\"0\"", ",", "\"1\"", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.lawsonabs_pun.None.bert_utils.ScProcessor._create_examples": [[184, 198], ["enumerate", "examples.append", "bert_utils.InputExample"], "methods", ["None"], ["", "def", "_create_examples", "(", "self", ",", "lines", ",", "set_type", ")", ":", "\n", "        ", "\"\"\"Creates examples for the training and dev sets.\"\"\"", "\n", "examples", "=", "[", "]", "\n", "for", "(", "i", ",", "line", ")", "in", "enumerate", "(", "lines", ")", ":", "\n", "            ", "if", "i", "==", "0", ":", "\n", "                ", "continue", "\n", "", "guid", "=", "\"%s-%s\"", "%", "(", "set_type", ",", "i", ")", "\n", "text_a", "=", "line", "[", "1", "]", "\n", "text_b", "=", "None", "\n", "prons", "=", "line", "[", "2", "]", "\n", "label", "=", "line", "[", "0", "]", "\n", "if", "label", "==", "\"-1\"", ":", "label", "=", "\"0\"", "\n", "examples", ".", "append", "(", "InputExample", "(", "guid", "=", "guid", ",", "text_a", "=", "text_a", ",", "text_b", "=", "text_b", ",", "prons", "=", "prons", ",", "label", "=", "label", ")", ")", "\n", "", "return", "examples", "\n", "\n"]], "home.repos.pwc.inspect_result.lawsonabs_pun.None.bert_utils.readfile": [[68, 98], ["open", "line.split", "sentence.append", "label.append", "prons.append", "len", "data.append", "line.startswith", "[].split", "len", "len", "data.append"], "function", ["None"], ["", "", "def", "readfile", "(", "filename", ")", ":", "\n", "    ", "'''\n    read file\n    return format :\n    [ ['EU', 'B-ORG'], ['rejects', 'O'], ['German', 'B-MISC'], ['call', 'O'], ['to', 'O'], ['boycott', 'O'], ['British', 'B-MISC'], ['lamb', 'O'], ['.', 'O'] ]\n    '''", "\n", "f", "=", "open", "(", "filename", ",", "encoding", "=", "'utf-8'", ")", "\n", "data", "=", "[", "]", "\n", "sentence", "=", "[", "]", "\n", "label", "=", "[", "]", "\n", "prons", "=", "[", "]", "\n", "for", "line", "in", "f", ":", "\n", "        ", "if", "len", "(", "line", ")", "==", "0", "or", "line", ".", "startswith", "(", "'-DOCSTART'", ")", "or", "line", "[", "0", "]", "==", "\"\\n\"", ":", "\n", "            ", "if", "len", "(", "sentence", ")", ">", "0", ":", "\n", "                ", "data", ".", "append", "(", "(", "sentence", ",", "label", ",", "prons", ")", ")", "\n", "sentence", "=", "[", "]", "\n", "label", "=", "[", "]", "\n", "prons", "=", "[", "]", "\n", "", "continue", "\n", "", "splits", "=", "line", ".", "split", "(", "' '", ")", "\n", "sentence", ".", "append", "(", "splits", "[", "0", "]", ")", "\n", "label", ".", "append", "(", "splits", "[", "-", "2", "]", ")", "\n", "prons", ".", "append", "(", "splits", "[", "-", "1", "]", "[", ":", "-", "1", "]", ".", "split", "(", "','", ")", ")", "\n", "\n", "", "if", "len", "(", "sentence", ")", ">", "0", ":", "\n", "        ", "data", ".", "append", "(", "(", "sentence", ",", "label", ",", "prons", ")", ")", "\n", "sentence", "=", "[", "]", "\n", "label", "=", "[", "]", "\n", "prons", "=", "[", "]", "\n", "", "return", "data", "\n", "\n"]], "home.repos.pwc.inspect_result.lawsonabs_pun.None.bert_utils.convert_examples_to_features": [[203, 269], ["enumerate", "example.text_a.split", "enumerate", "ntokens.append", "segment_ids.append", "label_ids.append", "enumerate", "ntokens.append", "segment_ids.append", "label_ids.append", "tokenizer.convert_tokens_to_ids", "features.append", "enumerate", "tokenizer.tokenize", "tokens.extend", "range", "len", "ntokens.append", "segment_ids.append", "label_ids.append", "len", "len", "tokenizer.convert_tokens_to_ids.append", "input_mask.append", "segment_ids.append", "label_ids.append", "len", "len", "len", "len", "logger.info", "logger.info", "logger.info", "logger.info", "logger.info", "logger.info", "bert_utils.InputFeatures", "len", "labels.append", "labels.append", "str", "str", "str", "str"], "function", ["home.repos.pwc.inspect_result.lawsonabs_pun.None.bert.Ner.tokenize"], ["def", "convert_examples_to_features", "(", "examples", ",", "label_list", ",", "max_seq_length", ",", "tokenizer", ")", ":", "\n", "    ", "\"\"\"Loads a data file into a list of `InputBatch`s.\"\"\"", "\n", "'''\u540c\u6587\u4ef6 cv_run_ner.py \u4e2d\uff0c\u8fd9\u91cc\u4e5f\u5c06label_map \u4ece0\u5f00\u59cb\n    '''", "\n", "label_map", "=", "{", "label", ":", "i", "for", "i", ",", "label", "in", "enumerate", "(", "label_list", ",", "0", ")", "}", "\n", "\n", "features", "=", "[", "]", "\n", "for", "(", "ex_index", ",", "example", ")", "in", "enumerate", "(", "examples", ")", ":", "\n", "        ", "textlist", "=", "example", ".", "text_a", ".", "split", "(", "' '", ")", "\n", "labellist", "=", "example", ".", "label", "\n", "tokens", "=", "[", "]", "\n", "labels", "=", "[", "]", "\n", "for", "i", ",", "word", "in", "enumerate", "(", "textlist", ")", ":", "\n", "            ", "token", "=", "tokenizer", ".", "tokenize", "(", "word", ")", "\n", "tokens", ".", "extend", "(", "token", ")", "\n", "label_1", "=", "labellist", "[", "i", "]", "\n", "for", "m", "in", "range", "(", "len", "(", "token", ")", ")", ":", "\n", "                ", "if", "m", "==", "0", ":", "\n", "                    ", "labels", ".", "append", "(", "label_1", ")", "\n", "", "else", ":", "\n", "                    ", "labels", ".", "append", "(", "\"X\"", ")", "\n", "", "", "", "if", "len", "(", "tokens", ")", ">=", "max_seq_length", "-", "1", ":", "\n", "            ", "tokens", "=", "tokens", "[", "0", ":", "(", "max_seq_length", "-", "2", ")", "]", "\n", "labels", "=", "labels", "[", "0", ":", "(", "max_seq_length", "-", "2", ")", "]", "\n", "", "ntokens", "=", "[", "]", "\n", "segment_ids", "=", "[", "]", "\n", "label_ids", "=", "[", "]", "\n", "ntokens", ".", "append", "(", "\"[CLS]\"", ")", "\n", "segment_ids", ".", "append", "(", "0", ")", "\n", "label_ids", ".", "append", "(", "label_map", "[", "\"[CLS]\"", "]", ")", "\n", "for", "i", ",", "token", "in", "enumerate", "(", "tokens", ")", ":", "\n", "            ", "ntokens", ".", "append", "(", "token", ")", "\n", "segment_ids", ".", "append", "(", "0", ")", "\n", "label_ids", ".", "append", "(", "label_map", "[", "labels", "[", "i", "]", "]", ")", "\n", "", "ntokens", ".", "append", "(", "\"[SEP]\"", ")", "\n", "segment_ids", ".", "append", "(", "0", ")", "\n", "label_ids", ".", "append", "(", "label_map", "[", "\"[SEP]\"", "]", ")", "\n", "input_ids", "=", "tokenizer", ".", "convert_tokens_to_ids", "(", "ntokens", ")", "\n", "input_mask", "=", "[", "1", "]", "*", "len", "(", "input_ids", ")", "\n", "while", "len", "(", "input_ids", ")", "<", "max_seq_length", ":", "\n", "            ", "input_ids", ".", "append", "(", "0", ")", "\n", "input_mask", ".", "append", "(", "0", ")", "\n", "segment_ids", ".", "append", "(", "0", ")", "\n", "label_ids", ".", "append", "(", "0", ")", "\n", "", "assert", "len", "(", "input_ids", ")", "==", "max_seq_length", "\n", "assert", "len", "(", "input_mask", ")", "==", "max_seq_length", "\n", "assert", "len", "(", "segment_ids", ")", "==", "max_seq_length", "\n", "assert", "len", "(", "label_ids", ")", "==", "max_seq_length", "\n", "\n", "if", "ex_index", "<", "5", ":", "\n", "            ", "logger", ".", "info", "(", "\"*** Example ***\"", ")", "\n", "logger", ".", "info", "(", "\"guid: %s\"", "%", "(", "example", ".", "guid", ")", ")", "\n", "logger", ".", "info", "(", "\"tokens: %s\"", "%", "\" \"", ".", "join", "(", "\n", "[", "str", "(", "x", ")", "for", "x", "in", "tokens", "]", ")", ")", "\n", "logger", ".", "info", "(", "\"input_ids: %s\"", "%", "\" \"", ".", "join", "(", "[", "str", "(", "x", ")", "for", "x", "in", "input_ids", "]", ")", ")", "\n", "logger", ".", "info", "(", "\"input_mask: %s\"", "%", "\" \"", ".", "join", "(", "[", "str", "(", "x", ")", "for", "x", "in", "input_mask", "]", ")", ")", "\n", "logger", ".", "info", "(", "\n", "\"segment_ids: %s\"", "%", "\" \"", ".", "join", "(", "[", "str", "(", "x", ")", "for", "x", "in", "segment_ids", "]", ")", ")", "\n", "# logger.info(\"label: %s (id = %d)\" % (example.label, label_ids))", "\n", "\n", "", "features", ".", "append", "(", "\n", "InputFeatures", "(", "input_ids", "=", "input_ids", ",", "\n", "input_mask", "=", "input_mask", ",", "\n", "segment_ids", "=", "segment_ids", ",", "\n", "label_id", "=", "label_ids", ")", ")", "\n", "", "return", "features", "\n", "\n"]], "home.repos.pwc.inspect_result.lawsonabs_pun.None.bert_utils.convert_examples_to_pron_features": [[273, 385], ["enumerate", "example.text_a.split", "enumerate", "ntokens.append", "segment_ids.append", "label_ids.append", "prons_ids.append", "prons_att_mask.append", "enumerate", "ntokens.append", "segment_ids.append", "label_ids.append", "prons_ids.append", "prons_att_mask.append", "tokenizer.convert_tokens_to_ids", "features.append", "enumerate", "tokenizer.tokenize", "tokens.extend", "range", "range", "len", "ntokens.append", "segment_ids.append", "label_ids.append", "prons_ids.append", "prons_att_mask.append", "len", "len", "tokenizer.convert_tokens_to_ids.append", "input_mask.append", "segment_ids.append", "label_ids.append", "prons_ids.append", "prons_att_mask.append", "len", "len", "len", "len", "len", "len", "logger.info", "logger.info", "logger.info", "logger.info", "logger.info", "logger.info", "logger.info", "logger.info", "logger.info", "logger.info", "bert_utils.InputPronFeatures", "len", "len", "pron_2.append", "len", "len", "len", "labels.append", "prons.append", "prons_mask.append", "labels.append", "prons.append", "prons_mask.append", "str", "len", "len", "str", "str", "str", "str", "str", "str", "str"], "function", ["home.repos.pwc.inspect_result.lawsonabs_pun.None.bert.Ner.tokenize"], ["", "def", "convert_examples_to_pron_features", "(", "examples", ",", "label_list", ",", "max_seq_length", ",", "max_pron_length", ",", "tokenizer", ",", "prons_map", ",", "logger", ")", ":", "\n", "    ", "\"\"\"Loads a data file into a list of `InputBatch`s.\"\"\"", "\n", "# \u6839\u636e\u4f20\u5165\u7684label_list \u751f\u6210\u4e86\u4e00\u4e2a label_map\uff0c\u4e5f\u5c31\u662f\u4e2a\u5b57\u5178", "\n", "label_map", "=", "{", "label", ":", "i", "for", "i", ",", "label", "in", "enumerate", "(", "label_list", ",", "0", ")", "}", "\n", "\n", "features", "=", "[", "]", "\n", "for", "(", "ex_index", ",", "example", ")", "in", "enumerate", "(", "examples", ")", ":", "\n", "        ", "textlist", "=", "example", ".", "text_a", ".", "split", "(", "' '", ")", "\n", "labellist", "=", "example", ".", "label", "\n", "pronslist", "=", "example", ".", "prons", "\n", "tokens", "=", "[", "]", "\n", "labels", "=", "[", "]", "\n", "prons", "=", "[", "]", "# \u662f\u4e2a[[...],[...],...] \u8fd9\u6837\u7684\u6570\u636e\u3002\u56e0\u4e3a\u6bcf\u4e2a\u5355\u8bcd\u90fd\u6709\u597d\u51e0\u4e2a\u97f3\u8282\uff0c\u6240\u4ee5\u5c31\u9700\u8981\u4e00\u6bb5\uff08max_pron_length\uff09\u6765\u5b58\u50a8", "\n", "prons_mask", "=", "[", "]", "\n", "for", "i", ",", "word", "in", "enumerate", "(", "textlist", ")", ":", "\n", "            ", "token", "=", "tokenizer", ".", "tokenize", "(", "word", ")", "# \u5904\u7406\u8be5\u8bcd\uff0c\u5982\u679c\u6539\u8bcd\u627e\u4e0d\u5230\uff0c\u5219\u5206\u5757\u5904\u7406\u3002", "\n", "tokens", ".", "extend", "(", "token", ")", "\n", "label_1", "=", "labellist", "[", "i", "]", "\n", "pron_1", "=", "pronslist", "[", "i", "]", "# the complete prons of a word", "\n", "pron_2", "=", "[", "]", "# save the ids of prons of a word", "\n", "for", "j", "in", "range", "(", "len", "(", "pron_1", ")", ")", ":", "\n", "                ", "index", "=", "len", "(", "prons_map", ")", "# expand the map with new prons", "\n", "if", "pron_1", "[", "j", "]", "not", "in", "prons_map", ":", "\n", "                    ", "prons_map", "[", "pron_1", "[", "j", "]", "]", "=", "index", "+", "1", "\n", "", "pron_2", ".", "append", "(", "prons_map", "[", "pron_1", "[", "j", "]", "]", ")", "\n", "", "pron_mask_2", "=", "[", "1", "]", "*", "len", "(", "pron_2", ")", "# \u8fd9\u4e2amask_2 \u7684\u7528\u5904\uff1f", "\n", "# \u5bf9\u53d1\u97f3\u7684\u5b57\u7b26\u505a\u4e00\u4e2a\u622a\u65ad\u6216\u8005pad\u64cd\u4f5c", "\n", "if", "len", "(", "pron_2", ")", ">=", "max_pron_length", ":", "\n", "                ", "pron_2", "=", "pron_2", "[", "0", ":", "max_pron_length", "]", "# trunk it if too long", "\n", "pron_mask_2", "=", "pron_mask_2", "[", "0", ":", "max_pron_length", "]", "\n", "", "else", ":", "\n", "                ", "pron_2", "+=", "[", "0", "]", "*", "(", "max_pron_length", "-", "len", "(", "pron_2", ")", ")", "# pad it if too short", "\n", "pron_mask_2", "+=", "[", "0", "]", "*", "(", "max_pron_length", "-", "len", "(", "pron_mask_2", ")", ")", "\n", "", "for", "m", "in", "range", "(", "len", "(", "token", ")", ")", ":", "\n", "                ", "if", "m", "==", "0", ":", "\n", "                    ", "labels", ".", "append", "(", "label_1", ")", "\n", "# \u4e5f\u5c31\u662f\u8bf4\uff0c\u5982\u679c\u8fd9\u4e2a\u5355\u8bcd\u88ab\u5207\u6210\u4e86\u591a\u6bb5\uff0c\u90a3\u4e48\u53d1\u97f3\u53ea\u4f1a\u88ab\u8bb0\u5f55\u5230\u7b2c\u4e00\u4e2atoken\u5206\u5757\u4e2d\uff0c\u540e\u9762\u7684\u90fd\u7528\u5176\u5b83\u7684\u586b\u5145", "\n", "prons", ".", "append", "(", "pron_2", ")", "# only send the prons to the first piece_token of a word", "\n", "prons_mask", ".", "append", "(", "pron_mask_2", ")", "\n", "", "else", ":", "# \u5982\u679c\u4e00\u4e2a\u5355\u8bcd\u88ab tokenize \u6210\u4e86\u4e24\u6bb5\uff0c\u5c31\u4f1a\u8fdb\u5165\u5230\u8fd9\u4e2aelse\u4e2d\u3002\u5c31\u4f1a\u88ab\u6807\u5fd7\u4e3a\u4e00\u4e2aX\uff0c\u662f\u5728\u9664\u53bb\u7b2c\u4e00\u4e2apart\u90e8\u5206\u540e\u7684\u6240\u6709\u90e8\u5206\u90fd\u4f1a\u6807\u4e3aX", "\n", "#labels.append(\"X\") # \u6e90\u7801\u4f7f\u7528X \u586b\u5145", "\n", "                    ", "labels", ".", "append", "(", "\"O\"", ")", "# \u4f7f\u7528 O \u586b\u5145", "\n", "prons", ".", "append", "(", "[", "0", "]", "*", "max_pron_length", ")", "# pad other piece_token with 0's", "\n", "prons_mask", ".", "append", "(", "[", "0", "]", "*", "max_pron_length", ")", "\n", "\n", "# \u6839\u636etoken \u7684embedding \u8ba1\u7b97\u76f8\u4f3cembedding", "\n", "\n", "", "", "", "if", "len", "(", "tokens", ")", ">=", "max_seq_length", "-", "1", ":", "# \u5224\u65ad\u6700\u540e\u7684sequence\u662f\u5426\u8d85\u8fc7\u4e86\u6700\u5927\u957f\u5ea6 ", "\n", "            ", "tokens", "=", "tokens", "[", "0", ":", "(", "max_seq_length", "-", "2", ")", "]", "\n", "labels", "=", "labels", "[", "0", ":", "(", "max_seq_length", "-", "2", ")", "]", "\n", "prons", "=", "prons", "[", "0", ":", "(", "max_seq_length", "-", "2", ")", "]", "\n", "prons_mask", "=", "prons_mask", "[", "0", ":", "(", "max_seq_length", "-", "2", ")", "]", "\n", "\n", "", "ntokens", "=", "[", "]", "\n", "segment_ids", "=", "[", "]", "\n", "label_ids", "=", "[", "]", "\n", "prons_ids", "=", "[", "]", "\n", "prons_att_mask", "=", "[", "]", "\n", "ntokens", ".", "append", "(", "\"[CLS]\"", ")", "\n", "segment_ids", ".", "append", "(", "0", ")", "\n", "label_ids", ".", "append", "(", "label_map", "[", "\"[CLS]\"", "]", ")", "\n", "prons_ids", ".", "append", "(", "[", "0", "]", "*", "max_pron_length", ")", "# pad the cls with 0's", "\n", "prons_att_mask", ".", "append", "(", "[", "0", "]", "*", "max_pron_length", ")", "\n", "for", "i", ",", "token", "in", "enumerate", "(", "tokens", ")", ":", "\n", "            ", "ntokens", ".", "append", "(", "token", ")", "\n", "segment_ids", ".", "append", "(", "0", ")", "\n", "label_ids", ".", "append", "(", "label_map", "[", "labels", "[", "i", "]", "]", ")", "\n", "prons_ids", ".", "append", "(", "prons", "[", "i", "]", ")", "\n", "prons_att_mask", ".", "append", "(", "prons_mask", "[", "i", "]", ")", "\n", "", "ntokens", ".", "append", "(", "\"[SEP]\"", ")", "\n", "segment_ids", ".", "append", "(", "0", ")", "\n", "label_ids", ".", "append", "(", "label_map", "[", "\"[SEP]\"", "]", ")", "\n", "prons_ids", ".", "append", "(", "[", "0", "]", "*", "max_pron_length", ")", "# pad the sep with 0's", "\n", "prons_att_mask", ".", "append", "(", "[", "0", "]", "*", "max_pron_length", ")", "\n", "input_ids", "=", "tokenizer", ".", "convert_tokens_to_ids", "(", "ntokens", ")", "\n", "input_mask", "=", "[", "1", "]", "*", "len", "(", "input_ids", ")", "# \u5bf9\u5e94\u6709token\u7684\u5c31\u662f1\uff0c\u6ca1\u6709\u5c31\u662f0", "\n", "while", "len", "(", "input_ids", ")", "<", "max_seq_length", ":", "\n", "            ", "input_ids", ".", "append", "(", "0", ")", "\n", "input_mask", ".", "append", "(", "0", ")", "\n", "segment_ids", ".", "append", "(", "0", ")", "\n", "label_ids", ".", "append", "(", "0", ")", "\n", "prons_ids", ".", "append", "(", "[", "0", "]", "*", "max_pron_length", ")", "\n", "prons_att_mask", ".", "append", "(", "[", "0", "]", "*", "max_pron_length", ")", "\n", "", "assert", "len", "(", "input_ids", ")", "==", "max_seq_length", "\n", "assert", "len", "(", "input_mask", ")", "==", "max_seq_length", "\n", "assert", "len", "(", "segment_ids", ")", "==", "max_seq_length", "\n", "assert", "len", "(", "label_ids", ")", "==", "max_seq_length", "\n", "assert", "len", "(", "prons_ids", ")", "==", "max_seq_length", "\n", "assert", "len", "(", "prons_att_mask", ")", "==", "max_seq_length", "\n", "\n", "if", "ex_index", "<", "0", ":", "\n", "            ", "logger", ".", "info", "(", "\"*** Example ***\"", ")", "\n", "logger", ".", "info", "(", "\"guid: %s\"", "%", "(", "example", ".", "guid", ")", ")", "\n", "logger", ".", "info", "(", "\"tokens: %s\"", "%", "\" \"", ".", "join", "(", "\n", "[", "str", "(", "x", ")", "for", "x", "in", "tokens", "]", ")", ")", "\n", "logger", ".", "info", "(", "\"input_ids: %s\"", "%", "\" \"", ".", "join", "(", "[", "str", "(", "x", ")", "for", "x", "in", "input_ids", "]", ")", ")", "\n", "logger", ".", "info", "(", "\"input_mask: %s\"", "%", "\" \"", ".", "join", "(", "[", "str", "(", "x", ")", "for", "x", "in", "input_mask", "]", ")", ")", "\n", "logger", ".", "info", "(", "\"segment_ids: %s\"", "%", "\" \"", ".", "join", "(", "[", "str", "(", "x", ")", "for", "x", "in", "segment_ids", "]", ")", ")", "\n", "logger", ".", "info", "(", "\"label_ids: %s\"", "%", "\" \"", ".", "join", "(", "[", "str", "(", "x", ")", "for", "x", "in", "label_ids", "]", ")", ")", "\n", "logger", ".", "info", "(", "\"prons_ids: %s\"", "%", "\" \"", ".", "join", "(", "[", "str", "(", "x", ")", "for", "x", "in", "prons_ids", "]", ")", ")", "\n", "logger", ".", "info", "(", "\"prons_att_mask: %s\"", "%", "\" \"", ".", "join", "(", "[", "str", "(", "x", ")", "for", "x", "in", "prons_att_mask", "]", ")", ")", "\n", "logger", ".", "info", "(", "\"prons_map: %s\"", "%", "str", "(", "prons_map", ")", ")", "\n", "# logger.info(\"label: %s (id = %d)\" % (example.label, label_ids))", "\n", "\n", "", "features", ".", "append", "(", "\n", "InputPronFeatures", "(", "input_ids", "=", "input_ids", ",", "\n", "input_mask", "=", "input_mask", ",", "\n", "segment_ids", "=", "segment_ids", ",", "\n", "label_id", "=", "label_ids", ",", "\n", "prons_id", "=", "prons_ids", ",", "\n", "prons_att_mask", "=", "prons_att_mask", ")", ")", "\n", "", "return", "features", ",", "prons_map", "\n", "\n"]], "home.repos.pwc.inspect_result.lawsonabs_pun.None.bert_utils.convert_examples_to_pron_SC_features": [[387, 486], ["print", "enumerate", "example.text_a.split", "enumerate", "ntokens.append", "segment_ids.append", "prons_ids.append", "prons_att_mask.append", "enumerate", "ntokens.append", "segment_ids.append", "prons_ids.append", "prons_att_mask.append", "tokenizer.convert_tokens_to_ids", "features.append", "enumerate", "x.split", "len", "len", "print", "print", "sys.exit", "tokenizer.tokenize", "tokens.extend", "range", "range", "len", "ntokens.append", "segment_ids.append", "prons_ids.append", "prons_att_mask.append", "len", "len", "tokenizer.convert_tokens_to_ids.append", "input_mask.append", "segment_ids.append", "prons_ids.append", "prons_att_mask.append", "len", "len", "len", "len", "len", "logger.info", "logger.info", "logger.info", "logger.info", "logger.info", "logger.info", "logger.info", "bert_utils.InputPronFeatures", "example.prons.split", "len", "len", "pron_2.append", "len", "len", "len", "prons.append", "prons_mask.append", "prons.append", "prons_mask.append", "len", "len", "str", "str", "str", "str"], "function", ["home.repos.pwc.inspect_result.lawsonabs_pun.None.bert.Ner.tokenize"], ["", "def", "convert_examples_to_pron_SC_features", "(", "examples", ",", "label_list", ",", "max_seq_length", ",", "max_pron_length", ",", "tokenizer", ",", "prons_map", ")", ":", "\n", "    ", "\"\"\"Loads a data file into a list of `InputBatch`s.\"\"\"", "\n", "\n", "label_map", "=", "{", "label", ":", "i", "for", "i", ",", "label", "in", "enumerate", "(", "label_list", ")", "}", "\n", "print", "(", "label_map", ")", "\n", "\n", "features", "=", "[", "]", "\n", "for", "(", "ex_index", ",", "example", ")", "in", "enumerate", "(", "examples", ")", ":", "\n", "        ", "textlist", "=", "example", ".", "text_a", ".", "split", "(", "' '", ")", "\n", "label_ids", "=", "label_map", "[", "example", ".", "label", "]", "\n", "pronslist", "=", "[", "x", ".", "split", "(", "','", ")", "for", "x", "in", "example", ".", "prons", ".", "split", "(", "' '", ")", "]", "\n", "#assert(len(textlist) == len(pronslist))", "\n", "if", "len", "(", "textlist", ")", "!=", "len", "(", "pronslist", ")", ":", "\n", "            ", "print", "(", "textlist", ")", "\n", "print", "(", "pronslist", ")", "\n", "sys", ".", "exit", "(", ")", "\n", "", "tokens", "=", "[", "]", "\n", "prons", "=", "[", "]", "\n", "prons_mask", "=", "[", "]", "\n", "for", "i", ",", "word", "in", "enumerate", "(", "textlist", ")", ":", "\n", "            ", "token", "=", "tokenizer", ".", "tokenize", "(", "word", ")", "\n", "tokens", ".", "extend", "(", "token", ")", "\n", "pron_1", "=", "pronslist", "[", "i", "]", "# the complete prons of a word", "\n", "pron_2", "=", "[", "]", "# save the ids of prons of a word", "\n", "for", "j", "in", "range", "(", "len", "(", "pron_1", ")", ")", ":", "\n", "                ", "index", "=", "len", "(", "prons_map", ")", "# expand the map with new prons", "\n", "if", "pron_1", "[", "j", "]", "not", "in", "prons_map", ":", "\n", "                    ", "prons_map", "[", "pron_1", "[", "j", "]", "]", "=", "index", "+", "1", "\n", "", "pron_2", ".", "append", "(", "prons_map", "[", "pron_1", "[", "j", "]", "]", ")", "\n", "", "pron_mask_2", "=", "[", "1", "]", "*", "len", "(", "pron_2", ")", "\n", "\n", "if", "len", "(", "pron_2", ")", ">=", "max_pron_length", ":", "\n", "                ", "pron_2", "=", "pron_2", "[", "0", ":", "max_pron_length", "]", "# trunk it if too long", "\n", "pron_mask_2", "=", "pron_mask_2", "[", "0", ":", "max_pron_length", "]", "\n", "", "else", ":", "\n", "                ", "pron_2", "+=", "[", "0", "]", "*", "(", "max_pron_length", "-", "len", "(", "pron_2", ")", ")", "# pad it if too short", "\n", "pron_mask_2", "+=", "[", "0", "]", "*", "(", "max_pron_length", "-", "len", "(", "pron_mask_2", ")", ")", "\n", "", "for", "m", "in", "range", "(", "len", "(", "token", ")", ")", ":", "\n", "                ", "if", "m", "==", "0", ":", "\n", "                    ", "prons", ".", "append", "(", "pron_2", ")", "# only send the prons to the first piece_token of a word", "\n", "prons_mask", ".", "append", "(", "pron_mask_2", ")", "\n", "", "else", ":", "\n", "                    ", "prons", ".", "append", "(", "[", "0", "]", "*", "max_pron_length", ")", "# pad other piece_token with 0's", "\n", "prons_mask", ".", "append", "(", "[", "0", "]", "*", "max_pron_length", ")", "\n", "", "", "", "if", "len", "(", "tokens", ")", ">=", "max_seq_length", "-", "1", ":", "\n", "            ", "tokens", "=", "tokens", "[", "0", ":", "(", "max_seq_length", "-", "2", ")", "]", "\n", "prons", "=", "prons", "[", "0", ":", "(", "max_seq_length", "-", "2", ")", "]", "\n", "prons_mask", "=", "prons_mask", "[", "0", ":", "(", "max_seq_length", "-", "2", ")", "]", "\n", "\n", "", "ntokens", "=", "[", "]", "\n", "segment_ids", "=", "[", "]", "\n", "prons_ids", "=", "[", "]", "\n", "prons_att_mask", "=", "[", "]", "\n", "ntokens", ".", "append", "(", "\"[CLS]\"", ")", "\n", "segment_ids", ".", "append", "(", "0", ")", "\n", "prons_ids", ".", "append", "(", "[", "0", "]", "*", "max_pron_length", ")", "# pad the cls with 0's", "\n", "prons_att_mask", ".", "append", "(", "[", "0", "]", "*", "max_pron_length", ")", "\n", "for", "i", ",", "token", "in", "enumerate", "(", "tokens", ")", ":", "\n", "            ", "ntokens", ".", "append", "(", "token", ")", "\n", "segment_ids", ".", "append", "(", "0", ")", "\n", "prons_ids", ".", "append", "(", "prons", "[", "i", "]", ")", "\n", "prons_att_mask", ".", "append", "(", "prons_mask", "[", "i", "]", ")", "\n", "", "ntokens", ".", "append", "(", "\"[SEP]\"", ")", "\n", "segment_ids", ".", "append", "(", "0", ")", "\n", "prons_ids", ".", "append", "(", "[", "0", "]", "*", "max_pron_length", ")", "# pad the sep with 0's", "\n", "prons_att_mask", ".", "append", "(", "[", "0", "]", "*", "max_pron_length", ")", "\n", "input_ids", "=", "tokenizer", ".", "convert_tokens_to_ids", "(", "ntokens", ")", "\n", "input_mask", "=", "[", "1", "]", "*", "len", "(", "input_ids", ")", "\n", "while", "len", "(", "input_ids", ")", "<", "max_seq_length", ":", "\n", "            ", "input_ids", ".", "append", "(", "0", ")", "\n", "input_mask", ".", "append", "(", "0", ")", "\n", "segment_ids", ".", "append", "(", "0", ")", "\n", "prons_ids", ".", "append", "(", "[", "0", "]", "*", "max_pron_length", ")", "\n", "prons_att_mask", ".", "append", "(", "[", "0", "]", "*", "max_pron_length", ")", "\n", "", "assert", "len", "(", "input_ids", ")", "==", "max_seq_length", "\n", "assert", "len", "(", "input_mask", ")", "==", "max_seq_length", "\n", "assert", "len", "(", "segment_ids", ")", "==", "max_seq_length", "\n", "assert", "len", "(", "prons_ids", ")", "==", "max_seq_length", "\n", "assert", "len", "(", "prons_att_mask", ")", "==", "max_seq_length", "\n", "\n", "if", "ex_index", "<", "3", ":", "\n", "            ", "logger", ".", "info", "(", "\"*** Example ***\"", ")", "\n", "logger", ".", "info", "(", "\"guid: %s\"", "%", "(", "example", ".", "guid", ")", ")", "\n", "logger", ".", "info", "(", "\"tokens: %s\"", "%", "\" \"", ".", "join", "(", "[", "str", "(", "x", ")", "for", "x", "in", "tokens", "]", ")", ")", "\n", "logger", ".", "info", "(", "\"input_ids: %s\"", "%", "\" \"", ".", "join", "(", "[", "str", "(", "x", ")", "for", "x", "in", "input_ids", "]", ")", ")", "\n", "logger", ".", "info", "(", "\"input_mask: %s\"", "%", "\" \"", ".", "join", "(", "[", "str", "(", "x", ")", "for", "x", "in", "input_mask", "]", ")", ")", "\n", "#logger.info(\"segment_ids: %s\" % \" \".join([str(x) for x in segment_ids]))", "\n", "logger", ".", "info", "(", "\"prons_ids: %s\"", "%", "\" \"", ".", "join", "(", "[", "str", "(", "x", ")", "for", "x", "in", "prons_ids", "]", ")", ")", "\n", "logger", ".", "info", "(", "\"label: %s (id = %d)\"", "%", "(", "example", ".", "label", ",", "label_ids", ")", ")", "\n", "# logger.info(\"label: %s (id = %d)\" % (example.label, label_ids))", "\n", "\n", "", "features", ".", "append", "(", "\n", "InputPronFeatures", "(", "input_ids", "=", "input_ids", ",", "\n", "input_mask", "=", "input_mask", ",", "\n", "segment_ids", "=", "segment_ids", ",", "\n", "label_id", "=", "label_ids", ",", "\n", "prons_id", "=", "prons_ids", ",", "\n", "prons_att_mask", "=", "prons_att_mask", ")", ")", "\n", "", "return", "features", ",", "prons_map", "\n", "\n"]], "home.repos.pwc.inspect_result.lawsonabs_pun.None.bert_utils.embed_load": [[490, 509], ["open", "open.readline", "f.readline.rstrip().split", "open.readline", "f.readline.rstrip().split", "open.readline", "f.readline.rstrip", "int", "embeddings.append", "f.readline.rstrip", "len", "float"], "function", ["None"], ["", "def", "embed_load", "(", "file_input", ")", ":", "# ./data/pron.16.vec  \u4ee3\u8868\u7684\u662f16\u7ef4\u7684\u53d1\u97f3embedding", "\n", "\n", "    ", "f", "=", "open", "(", "file_input", ",", "'r'", ")", "\n", "line", "=", "f", ".", "readline", "(", ")", "\n", "pron_map", "=", "{", "}", "\n", "num", ",", "dim", "=", "line", ".", "rstrip", "(", ")", ".", "split", "(", "' '", ")", "# \u62ff\u5230embedding \u7684\u6570\u76ee\u548c\u7ef4\u5ea6", "\n", "line", "=", "f", ".", "readline", "(", ")", "\n", "embeddings", "=", "[", "[", "0.0", "]", "*", "int", "(", "dim", ")", "]", "\n", "while", "line", "!=", "''", ":", "\n", "        ", "vec", "=", "line", ".", "rstrip", "(", ")", ".", "split", "(", "' '", ")", "\n", "token", "=", "vec", "[", "0", "]", "\n", "emb", "=", "vec", "[", "1", ":", "]", "\n", "if", "token", "not", "in", "pron_map", ":", "\n", "            ", "pron_map", "[", "token", "]", "=", "len", "(", "pron_map", ")", "+", "1", "\n", "embeddings", ".", "append", "(", "[", "float", "(", "x", ")", "for", "x", "in", "emb", "]", ")", "# \u4ecestr \u8f6c\u6210float \u578b", "\n", "\n", "", "line", "=", "f", ".", "readline", "(", ")", "\n", "\n", "", "return", "pron_map", ",", "embeddings", "\n", "\n"]], "home.repos.pwc.inspect_result.lawsonabs_pun.None.bert_utils.embed_extend": [[511, 518], ["len", "range", "embeddings.append", "len", "numpy.random.random"], "function", ["None"], ["", "def", "embed_extend", "(", "embeddings", ",", "length", ")", ":", "\n", "\n", "    ", "dim", "=", "len", "(", "embeddings", "[", "0", "]", ")", "\n", "for", "i", "in", "range", "(", "length", "+", "1", "-", "len", "(", "embeddings", ")", ")", ":", "\n", "        ", "embeddings", ".", "append", "(", "np", ".", "random", ".", "random", "(", "[", "dim", "]", ")", "*", "2", "-", "1", ")", "\n", "\n", "", "return", "embeddings", "\n", "\n"]], "home.repos.pwc.inspect_result.lawsonabs_pun.None.bert_utils.write_scores": [[519, 522], ["open", "pickle.dump"], "function", ["None"], ["", "def", "write_scores", "(", "file_output", ",", "y", ")", ":", "\n", "    ", "with", "open", "(", "file_output", ",", "'wb'", ")", "as", "f", ":", "\n", "        ", "pickle", ".", "dump", "(", "y", ",", "f", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.lawsonabs_pun.None.bert_utils.f1_2d": [[523, 525], ["sklearn.metrics.f1_score", "sklearn.metrics.recall_score", "sklearn.metrics.precision_score"], "function", ["None"], ["", "", "def", "f1_2d", "(", "tmp2", ",", "tmp1", ")", ":", "\n", "    ", "return", "f1_score", "(", "tmp2", ",", "tmp1", ")", ",", "recall_score", "(", "tmp2", ",", "tmp1", ")", ",", "precision_score", "(", "tmp2", ",", "tmp1", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.lawsonabs_pun.None.bert_utils.visualize_local": [[527, 567], ["open", "range", "json.dump", "open.write", "int", "len", "range", "len", "random.random", "tokenizer.convert_ids_to_tokens", "enumerate", "tokenizer.convert_ids_to_tokens.index", "tokenizer.convert_ids_to_tokens", "float", "int"], "function", ["None"], ["", "def", "visualize_local", "(", "logits", ",", "label_ids", ",", "input_ids", ",", "prons_ids", ",", "prons_att_mask", ",", "att", ",", "label_map", ",", "prons_map", ",", "tokenizer", ")", ":", "\n", "    ", "\"\"\"\n    torch.Size([8, 128])\n    torch.Size([8, 128])\n    torch.Size([8, 128])\n    torch.Size([8, 128, 5])\n    torch.Size([8, 128, 5])\n    torch.Size([8, 128, 5])\n    \"\"\"", "\n", "prons_map", "=", "{", "int", "(", "prons_map", "[", "pron", "]", ")", ":", "pron", "for", "pron", "in", "prons_map", "}", "\n", "\n", "f", "=", "open", "(", "'results/pron_viz.json'", ",", "'a'", ")", "\n", "results", "=", "{", "}", "\n", "for", "i", "in", "range", "(", "len", "(", "label_ids", ")", ")", ":", "\n", "\n", "        ", "for", "j", "in", "range", "(", "len", "(", "label_ids", "[", "i", "]", ")", ")", ":", "\n", "\n", "            ", "ran", "=", "random", ".", "random", "(", ")", "\n", "\n", "if", "label_ids", "[", "i", "]", "[", "j", "]", "!=", "0", "and", "label_map", "[", "label_ids", "[", "i", "]", "[", "j", "]", "]", "==", "label_map", "[", "logits", "[", "i", "]", "[", "j", "]", "]", "and", "label_map", "[", "label_ids", "[", "i", "]", "[", "j", "]", "]", "==", "\"P\"", ":", "\n", "                ", "mask", "=", "prons_att_mask", "[", "i", "]", "[", "j", "]", "\n", "score", "=", "att", "[", "i", "]", "[", "j", "]", "\n", "\n", "tmp", "=", "tokenizer", ".", "convert_ids_to_tokens", "(", "input_ids", "[", "i", "]", ")", "\n", "try", ":", "\n", "                    ", "N", "=", "tmp", ".", "index", "(", "'[PAD]'", ")", "\n", "results", "[", "'sent'", "]", "=", "tmp", "[", ":", "N", "]", "\n", "", "except", ":", "\n", "                    ", "result", "[", "'sent'", "]", "=", "tmp", "\n", "\n", "", "results", "[", "'start'", "]", "=", "tokenizer", ".", "convert_ids_to_tokens", "(", "[", "int", "(", "input_ids", "[", "i", "]", "[", "j", "]", ")", "]", ")", "[", "0", "]", "\n", "results", "[", "'pron'", "]", "=", "{", "}", "\n", "\n", "for", "k", ",", "m", "in", "enumerate", "(", "mask", ")", ":", "\n", "                    ", "if", "m", "==", "0", ":", "break", "\n", "results", "[", "'pron'", "]", "[", "prons_map", "[", "prons_ids", "[", "i", "]", "[", "j", "]", "[", "k", "]", "]", "]", "=", "float", "(", "score", "[", "k", "]", ")", "\n", "\n", "", "", "", "", "json", ".", "dump", "(", "results", ",", "f", ")", "\n", "f", ".", "write", "(", "'\\n'", ")", "\n", "return", "\n", "\n"]], "home.repos.pwc.inspect_result.lawsonabs_pun.None.bert_utils.visualize_self": [[569, 600], ["open", "range", "json.dump", "open.write", "len", "tokenizer.convert_ids_to_tokens", "range", "input_mask[].index", "len", "str", "str"], "function", ["None"], ["", "def", "visualize_self", "(", "logits", ",", "label_ids", ",", "input_ids", ",", "input_mask", ",", "att", ",", "tokenizer", ")", ":", "\n", "    ", "\"\"\"\n    torch.Size(8)\n    torch.Size(8)\n    torch.Size([8, 128])\n    torch.Size([8, 128, 128])\n    \"\"\"", "\n", "f", "=", "open", "(", "'results/token_viz.json'", ",", "'a'", ")", "\n", "results", "=", "{", "}", "\n", "for", "i", "in", "range", "(", "len", "(", "input_ids", ")", ")", ":", "\n", "\n", "        ", "if", "label_ids", "[", "i", "]", "==", "logits", "[", "i", "]", "and", "label_ids", "[", "i", "]", "==", "1", ":", "\n", "\n", "            ", "try", ":", "\n", "                ", "N", "=", "input_mask", "[", "i", "]", ".", "index", "(", "0", ")", "\n", "ids", "=", "input_ids", "[", ":", "N", "]", "\n", "", "except", ":", "\n", "                ", "ids", "=", "input_ids", "\n", "\n", "", "tokens", "=", "tokenizer", ".", "convert_ids_to_tokens", "(", "input_ids", "[", "i", "]", ")", "\n", "\n", "results", "[", "'sent_'", "+", "str", "(", "i", ")", "]", "=", "tokens", "\n", "\n", "for", "j", "in", "range", "(", "len", "(", "tokens", ")", ")", ":", "\n", "\n", "                ", "results", "[", "token", "+", "'_'", "+", "str", "(", "j", ")", "]", "=", "att", "[", "i", "]", "[", "j", "]", "\n", "\n", "", "", "", "json", ".", "dump", "(", "results", ",", "f", ")", "\n", "f", ".", "write", "(", "'\\n'", ")", "\n", "\n", "return", "\n", "\n"]], "home.repos.pwc.inspect_result.lawsonabs_pun.None.bert_utils.getSenseEmbedding": [[607, 644], ["BertTokenizerFast.from_pretrained", "batch_input_ids.size", "BertTokenizerFast.from_pretrained.convert_ids_to_tokens", "word.lower.lower", "wn.synsets", "batch_sense.append", "len", "sense_list.append", "len", "random.random", "int", "len", "sense.definition", "sense_list.append", "len"], "function", ["home.repos.pwc.inspect_result.lawsonabs_pun.None.bert_models.BertPreTrainedModel.from_pretrained"], ["def", "getSenseEmbedding", "(", "batch_input_ids", ",", "model_dir", ",", "defi_num", ")", ":", "\n", "# ================ \u4e3b\u8981\u601d\u60f3\uff1a \u8ba1\u7b97\u6bcf\u4e2a token \u5bf9\u5e94\u7684sense embedding  ================", "\n", "        ", "from", "transformers", "import", "BertTokenizerFast", "\n", "from", "nltk", ".", "corpus", "import", "wordnet", "as", "wn", "\n", "sense_tokenizer", "=", "BertTokenizerFast", ".", "from_pretrained", "(", "model_dir", ")", "\n", "black_list", "=", "[", "'is'", ",", "'a'", ",", "'be'", ",", "'did'", "]", "\n", "num", ",", "max_seq_length", "=", "batch_input_ids", ".", "size", "(", ")", "# \u5f97\u5230\u53cc\u5173\u8bed\u603b\u6570\u3001\u6bcf\u53e5\u8bdd\u6700\u5927\u7684\u957f\u5ea6", "\n", "batch_sense", "=", "[", "]", "# \u5b58\u50a8\u6240\u6709\u5355\u8bcd\u7684\u5404\u4e2a\u91ca\u4e49", "\n", "# size = [1446, max_seq_length*10]", "\n", "for", "input_ids", "in", "batch_input_ids", ":", "# \u56e0\u4e3a\u6709\u5f88\u591a\u53e5\u8bdd\uff0c\u6240\u4ee5\u8fd9\u91cc\u904d\u5386\u5176\u4e2d\u7684\u6bcf\u4e00\u4e2a\u53cc\u5173\u8bed            ", "\n", "            ", "words", "=", "sense_tokenizer", ".", "convert_ids_to_tokens", "(", "input_ids", ")", "# \u5c06\u5f97\u5230\u7684token_id \u8f6c\u4e3atoken\uff0c\u4e00\u6b21\u53ef\u4ee5\u8f6c\u6362\u4e00\u53e5\u8bdd            ", "\n", "for", "word", "in", "words", "[", "0", ":", "]", ":", "# \u4ece\u5934\u5230\u5c3e\u5224\u65ad\u4e00\u904d", "\n", "# \u4f7f\u7528 wordnet \u8ba1\u7b97\u5176sense \u5217\u8868", "\n", "                ", "word", "=", "word", ".", "lower", "(", ")", "# \u8f6c\u4e3a\u5c0f\u5199", "\n", "syn", "=", "wn", ".", "synsets", "(", "word", ")", "# \u83b7\u53d6word\u7684\u542b\u4e49\u96c6", "\n", "sense_list", "=", "[", "]", "# \u5f53\u524d\u8fd9\u4e2a\u5355\u8bcd\u768410\u79cdsense", "\n", "if", "word", "not", "in", "black_list", "and", "len", "(", "syn", ")", ">", "0", ":", "\n", "                    ", "for", "sense", "in", "syn", ":", "\n", "                        ", "gross", "=", "sense", ".", "definition", "(", ")", "# \u83b7\u53d6\u5b9a\u4e49", "\n", "sense_list", ".", "append", "(", "gross", ")", "# \u8ffd\u52a0\u5230\u5b9a\u4e49\u96c6\u5408\u4e2d", "\n", "\n", "# \u8fd9\u4e2a\u53c2\u6570\u7684\u8bbe\u7f6e\u8fd8\u662f\u9700\u8981\u8861\u91cf\u4e00\u4e0b", "\n", "# \u5982\u679c\u5355\u8bcd\u7684 sense list \u4e0d\u8db3 defi_num \u4e2a\uff0c\u90a3\u4e48\u5c31 padding \u5230 defi_num \u4e2a", "\n", "", "", "while", "(", "len", "(", "sense_list", ")", "<", "defi_num", ")", ":", "\n", "                    ", "sense_list", ".", "append", "(", "\"\"", ")", "\n", "# \u8003\u8651\u968f\u673a\u5220\u9664\u67d0\u4e2a\u4e0b\u6807\u662f\u5426\u4f1a\u5bfc\u81f4\u540e\u7eed\u7684\u51fa\u73b0\u95ee\u9898", "\n", "# \u5982\u679c\u5355\u8bcd\u7684 sense list \u8d85\u8fc7 defi_num \u4e2a\uff0c\u90a3\u4e48\u5c31\u968f\u673a truncate \u5230 defi_num \u4e2a ", "\n", "", "while", "(", "len", "(", "sense_list", ")", ">", "defi_num", ")", ":", "\n", "                    ", "index", "=", "random", ".", "random", "(", ")", "# \u751f\u6210\u4e00\u4e2a\u968f\u673a\u6570", "\n", "index", "=", "int", "(", "index", "*", "len", "(", "sense_list", ")", ")", "# \u786e\u5b9a\u4e0b\u6807", "\n", "del", "(", "sense_list", "[", "index", "]", ")", "# \u5220\u9664\u4e0b\u6807\u4e3a index \u7684\u503c", "\n", "", "batch_sense", ".", "append", "(", "sense_list", ")", "# \u5f97\u5230\u5f53\u524d\u8fd9\u53e5\u53cc\u5173\u8bed\u7684\u6240\u6709\u5355\u8bcd\u7684senseList         ", "\n", "\n", "# \u5904\u7406\u7ef4\u5ea6", "\n", "# defi_emb = defi_emb.view(-1,defi_num,768)", "\n", "# defi_emb = defi_emb.cuda(device)", "\n", "", "", "return", "batch_sense", "# \u540e\u9762\u4ea4\u7ed9bert\u5904\u7406", "\n", "# len(batch_sense) = 1446. \u4e5f\u5c31\u662f\u53cc\u5173\u8bed\u7684\u4e2a\u6570", "\n"]], "home.repos.pwc.inspect_result.lawsonabs_pun.None.bert_utils.getAllWordSenseEmb": [[670, 691], ["open", "f.readline", "f.readline.split", "f.readline", "[].isalpha", "emb.append", "emb.append", "float", "float"], "function", ["None"], ["def", "getAllWordSenseEmb", "(", "path", ")", ":", "\n", "    ", "wordEmb", "=", "{", "}", "# {str:list}", "\n", "with", "open", "(", "path", ",", "'r'", ")", "as", "f", ":", "\n", "        ", "line", "=", "f", ".", "readline", "(", ")", "\n", "while", "(", "line", ")", ":", "\n", "#print(line)", "\n", "            ", "line", "=", "line", ".", "split", "(", ")", "# \u5148\u6309\u7167\u7a7a\u683c\u5206\u5272", "\n", "if", "line", "[", "-", "1", "]", "==", "\"None\"", ":", "# \u4ee5None\u7ed3\u5c3e", "\n", "                ", "pass", "\n", "", "elif", "line", "[", "0", "]", "[", "0", "]", ".", "isalpha", "(", ")", ":", "# \u5982\u679c\u662f\u5b57\u7b26\u3002\u662f\u4e00\u4e2a\u65b0\u7684\u5f00\u59cb              ", "\n", "                ", "emb", "=", "[", "]", "# \u88c5\u4e0b\u5f53\u4e0b\u5355\u8bcd\u6240\u6709\u7684emb", "\n", "res1", "=", "line", "[", "0", "]", "# \u5f97\u5230\u5355\u8bcd", "\n", "del", "(", "line", "[", "0", "]", ")", "\n", "line", "=", "[", "float", "(", "_", ")", "for", "_", "in", "line", "]", "# \u5168\u90e8\u8f6c\u4e3afloat \u578b", "\n", "wordEmb", "[", "res1", "]", "=", "emb", "\n", "emb", ".", "append", "(", "line", ")", "\n", "", "else", ":", "\n", "                ", "line", "=", "[", "float", "(", "_", ")", "for", "_", "in", "line", "]", "# \u5168\u90e8\u8f6c\u4e3afloat \u578b             ", "\n", "emb", ".", "append", "(", "line", ")", "\n", "", "line", "=", "f", ".", "readline", "(", ")", "\n", "", "", "return", "wordEmb", "\n", "\n"]], "home.repos.pwc.inspect_result.lawsonabs_pun.None.bert_utils.getPunEmb": [[697, 729], ["t.set_printoptions", "t.zeros", "word.lower.lower", "t.randn", "wordEmb.keys", "t.tensor", "t.cat", "t.cat.size", "t.cat", "t.cat.size", "t.cat", "t.cat.size"], "function", ["None"], ["def", "getPunEmb", "(", "wordEmb", ",", "words", ",", "defi_num", ",", "use_random", ")", ":", "\n", "    ", "import", "torch", "as", "t", "\n", "t", ".", "set_printoptions", "(", "profile", "=", "\"full\"", ")", "\n", "# \u8fd9\u91cc\u5b9e\u73b0\u4e24\u79cd\u65b9\u5f0f\u586b\u5145\uff1a", "\n", "# \uff081\uff09\u4f7f\u7528\u96f6\u586b\u5145", "\n", "# \uff082\uff09\u4f7f\u7528\u968f\u673a\u6570\u586b\u5145", "\n", "if", "not", "use_random", ":", "\n", "        ", "pad", "=", "t", ".", "zeros", "(", "1", ",", "768", ")", "# \u7528\u4e8e\u5145\u5f53\u4e00\u4e2a\u8bcd\u7684\u5b9a\u4e49", "\n", "", "pun_sense_emb", "=", "None", "\n", "for", "word", "in", "words", ":", "\n", "        ", "if", "use_random", ":", "\n", "            ", "pad", "=", "t", ".", "randn", "(", "1", ",", "768", ")", "\n", "", "word", "=", "word", ".", "lower", "(", ")", "# \u8f6c\u4e3a\u5c0f\u5199", "\n", "cur_word_emb", "=", "None", "\n", "if", "word", "not", "in", "wordEmb", ".", "keys", "(", ")", ":", "# \u6839\u672c\u627e\u4e0d\u5230\u8fd9\u4e2a\u8bcd\u3002\u9700\u8981\u62fc\u63a5 defi_num \u6b21", "\n", "            ", "if", "cur_word_emb", "is", "None", ":", "\n", "                ", "cur_word_emb", "=", "pad", "\n", "", "while", "(", "cur_word_emb", ".", "size", "(", "0", ")", "<", "defi_num", ")", ":", "\n", "                ", "cur_word_emb", "=", "t", ".", "cat", "(", "(", "cur_word_emb", ",", "pad", ")", ",", "0", ")", "\n", "", "", "else", ":", "\n", "            ", "cur_word_emb", "=", "t", ".", "tensor", "(", "wordEmb", "[", "word", "]", ")", "\n", "while", "(", "cur_word_emb", ".", "size", "(", "0", ")", "<", "defi_num", ")", ":", "# \u5982\u679c\u5c0f\u4e8e defi_num \u4e2a\u5b9a\u4e49\uff0c\u5219\u6269\u5145\u5230\u8fd9\u4e48\u591a", "\n", "# \u5728\u7b2c0\u7ef4\u62fc\u63a5 0\u5411\u91cf", "\n", "                ", "cur_word_emb", "=", "t", ".", "cat", "(", "(", "cur_word_emb", ",", "pad", ")", ",", "0", ")", "\n", "# \u5982\u679ccur_word_emb.size(0) > defi_num  \u65f6\u9700\u8981\u4fee\u6539", "\n", "", "while", "(", "cur_word_emb", ".", "size", "(", "0", ")", ">", "defi_num", ")", ":", "# \u53ea\u53d6\u524d\u9762\u7684defi_num \u4e2a", "\n", "                ", "cur_word_emb", "=", "cur_word_emb", "[", "0", ":", "defi_num", ",", ":", "]", "\n", "", "", "if", "pun_sense_emb", "is", "None", ":", "\n", "            ", "pun_sense_emb", "=", "cur_word_emb", "\n", "", "else", ":", "\n", "            ", "pun_sense_emb", "=", "t", ".", "cat", "(", "(", "pun_sense_emb", ",", "cur_word_emb", ")", ",", "0", ")", "# \u62fc\u63a5\u5f97\u5230\u4e00\u53e5\u8bdd\u4e2d\u6240\u6709\u7684embedding", "\n", "", "", "return", "pun_sense_emb", "\n", "# size [word_num * defi_num, defi_dim]  \u5355\u8bcd\u4e2a\u6570*\u542b\u4e49\u6570\uff0c \u542b\u4e49\u7684\u7ef4\u5ea6", "\n"]], "home.repos.pwc.inspect_result.lawsonabs_pun.None.bert_utils.writeToTxt": [[735, 746], ["open", "range", "len", "range", "f.write", "len", "f.write"], "function", ["None"], ["def", "writeToTxt", "(", "tokens", ",", "true_label", ",", "pred_label", ",", "path", ")", ":", "\n", "    ", "with", "open", "(", "path", ",", "'w'", ")", "as", "f", ":", "\n", "        ", "line", "=", "[", "]", "\n", "for", "i", "in", "range", "(", "len", "(", "tokens", ")", ")", ":", "# \u904d\u5386tokens \u7b2c\u4e00\u7ef4            ", "\n", "            ", "for", "j", "in", "range", "(", "len", "(", "tokens", "[", "i", "]", ")", ")", ":", "# \u904d\u5386 tokens \u7b2c\u4e8c\u7ef4", "\n", "                ", "if", "tokens", "[", "i", "]", "[", "j", "+", "1", "]", "==", "\"[SEP]\"", ":", "\n", "                    ", "break", "\n", "# line = tokens[i][j+1] +\"\\t\"+ true_label[i][j] +\"\\t\"+ pred_label[i][j] + \"\\n\"", "\n", "", "line", "=", "f\"{tokens[i][j+1]:<20}\"", "+", "f\"{true_label[i][j]:<3}\"", "+", "f\"{pred_label[i][j]:<3}\"", "+", "\"\\n\"", "\n", "f", ".", "write", "(", "line", ")", "\n", "", "f", ".", "write", "(", "\"\\n\"", ")", "# \u4e00\u53e5\u8bdd\u5199\u5b8c\u4e4b\u540e\uff0c\u4f7f\u7528\u6362\u884c\u5206\u5272", "\n", "\n"]], "home.repos.pwc.inspect_result.lawsonabs_pun.None.bert_utils.get_word_key_id_2_map": [[752, 772], ["open", "f.readline", "f.readline.strip", "f.readline", "f.readline.strip", "f.readline.split", "f.readline", "str"], "function", ["None"], ["def", "get_word_key_id_2_map", "(", "keyPath", ")", ":", "\n", "    ", "key_id_2_map", "=", "{", "}", "# save_0 => economy%1:04:00::", "\n", "with", "open", "(", "keyPath", ",", "'r'", ")", "as", "f", ":", "\n", "        ", "line", "=", "f", ".", "readline", "(", ")", "\n", "cur_row", "=", "0", "# \u8868\u793a\u5f53\u524d\u884c", "\n", "while", "(", "line", ")", ":", "\n", "            ", "if", "(", "'%'", "not", "in", "line", ")", ":", "# \u5982\u679c\u662fpun word", "\n", "                ", "line", "=", "line", ".", "strip", "(", ")", "\n", "word", "=", "line", "# pun word", "\n", "cur_row", "=", "0", "\n", "line", "=", "f", ".", "readline", "(", ")", "\n", "", "else", ":", "\n", "                ", "line", "=", "line", ".", "strip", "(", ")", "\n", "line", "=", "line", ".", "split", "(", "\";\"", ")", "\n", "for", "key", "in", "line", ":", "\n", "                    ", "key_id_2_map", "[", "word", "+", "\"_\"", "+", "str", "(", "cur_row", ")", "]", "=", "key", "\n", "break", "\n", "", "line", "=", "f", ".", "readline", "(", ")", "\n", "cur_row", "+=", "1", "\n", "", "", "", "return", "key_id_2_map", "\n", "\n"]], "home.repos.pwc.inspect_result.lawsonabs_pun.tools.pureBert.PureModel.__init__": [[13, 19], ["torch.Module.__init__", "transformers.BertModel.from_pretrained", "torch.Linear"], "methods", ["home.repos.pwc.inspect_result.lawsonabs_pun.subtask3_v2.gloss.EvalDataset.__init__", "home.repos.pwc.inspect_result.lawsonabs_pun.None.bert_models.BertPreTrainedModel.from_pretrained"], ["    ", "def", "__init__", "(", "self", ",", "path", ",", "in_fea", ",", "out_fea", "=", "1", ")", ":", "\n", "        ", "super", "(", "PureModel", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "model", "=", "BertModel", ".", "from_pretrained", "(", "path", ")", "\n", "# \u641e\u4e00\u4e2a\u7ebf\u6027\u6620\u5c04", "\n", "# \u56e0\u4e3a\u8fd9\u91cc\u6700\u540e\u8981\u9884\u6d4b\u6210\u4e00\u4e2a\u6570\uff0c\u6240\u4ee5outFea \u76f4\u63a5\u9ed8\u8ba4\u4e3a1", "\n", "self", ".", "linear", "=", "nn", ".", "Linear", "(", "in_fea", ",", "out_fea", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.lawsonabs_pun.tools.pureBert.PureModel.forward": [[20, 33], ["pureBert.PureModel.model", "pureBert.PureModel.linear"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "input_ids", ",", "token_type_ids", ",", "attention_mask", ")", ":", "\n", "        ", "output", "=", "self", ".", "model", "(", "input_ids", ",", "token_type_ids", ",", "attention_mask", ")", "# \u4f9d\u6b21\u4f20\u5165\u4e09\u4e2a\u503c", "\n", "last_hidden_layer", "=", "output", "[", "0", "]", "# \u5f97\u5230\u6700\u540e\u4e00\u5c42\u7684\u8f93\u51fa", "\n", "\n", "# \u53d6\u6bcf\u4e2atoken\u7684embedding\uff0c\u7528\u4e8e\u5224\u65ad\u662f\u5426\u662f\u53cc\u5173\u8bcd", "\n", "# \u53d6\u6bcf\u4e00\u4e2a\u6570\u636e\u4e2d\u7684[CLS] \u5411\u91cf\u7684\u503c \uff0c\u7528\u4e8e\u5224\u65ad\u662f\u5426\u662f\u53cc\u5173\u8bed", "\n", "token_emb", "=", "last_hidden_layer", "[", ":", ",", "1", ":", ",", ":", "]", "\n", "# size = [batch_size,max_seq_len,768]", "\n", "cls_emb", "=", "last_hidden_layer", "[", ":", ",", "0", ",", ":", "]", "\n", "# size =[batch_size,1,768]", "\n", "\n", "out", "=", "self", ".", "linear", "(", "last_hidden_layer", ")", "# \u7ebf\u6027\u6620\u5c04", "\n", "return", "out", "", "", "", ""]], "home.repos.pwc.inspect_result.lawsonabs_pun.tools.generateSenseEmb.simpleReadXml": [[12, 49], ["dom.parse", "root.getElementsByTagName", "open", "f.readline", "text.getAttribute", "f.readline.strip", "f.readline.split", "allHomo.append", "allLabel.append", "f.readline", "text.getElementsByTagName", "puns.append", "pun.append", "word.getAttribute", "punWord.append"], "function", ["None"], ["def", "simpleReadXml", "(", "pathData", ",", "pathLabel", ")", ":", "\n", "    ", "import", "xml", ".", "dom", ".", "minidom", "as", "dom", "\n", "# step1.\u5148\u4ecepathLabel \u4e2d\u627e\u51fa\u6240\u6709\u662f\u53cc\u5173\u8bed\u7684\u53e5\u5b50", "\n", "allHomo", "=", "[", "]", "# \u6807\u8bb0\u6240\u6709\u662f\u53cc\u5173\u8bed\u7684\u53e5\u5b50\u96c6\u5408 [hom_1,hom_2]", "\n", "allLabel", "=", "[", "]", "# \u627e\u51fa\u6240\u6709\u7684\u53cc\u5173\u8bcd [hom_1_12]", "\n", "punWord", "=", "[", "]", "# \u8868\u793a\u6240\u6709\u7684\u53cc\u5173\u8bcd", "\n", "with", "open", "(", "pathLabel", ",", "'r'", ")", "as", "f", ":", "\n", "        ", "line", "=", "f", ".", "readline", "(", ")", "\n", "while", "(", "line", "!=", "\"\"", ")", ":", "\n", "            ", "line", "=", "line", ".", "strip", "(", ")", "# \u53bb\u884c\u672b\u6362\u884c", "\n", "line", "=", "line", ".", "split", "(", ")", "# \u7a7a\u683c\u5206\u5272            ", "\n", "#print(line[0])", "\n", "allHomo", ".", "append", "(", "line", "[", "0", "]", ")", "\n", "allLabel", ".", "append", "(", "line", "[", "1", "]", ")", "# \u5c06\u53cc\u5173\u8bcd\u7684id\u653e\u5165\u5176\u4e2d", "\n", "line", "=", "f", ".", "readline", "(", ")", "\n", "#print(allHomo)", "\n", "\n", "# step2.\u63a5\u7740\u8bfb\u53d6\u53cc\u5173\u53e5\uff0c\u6210\u4e3a\u4e00\u884c\u6587\u672c", "\n", "#\u6253\u5f00xml\u6587\u6863", "\n", "", "", "dom2", "=", "dom", ".", "parse", "(", "pathData", ")", "\n", "#\u5f97\u5230\u6587\u6863\u5143\u7d20\u5bf9\u8c61", "\n", "root", "=", "dom2", ".", "documentElement", "\n", "texts", "=", "root", ".", "getElementsByTagName", "(", "\"text\"", ")", "# \u5f97\u5230\u6240\u6709\u7684text             ", "\n", "puns", "=", "[", "]", "# \u5b58\u50a8\u53cc\u5173\u8bed\u7684\u5217\u8868", "\n", "for", "text", "in", "texts", ":", "\n", "        ", "name", "=", "text", ".", "getAttribute", "(", "'id'", ")", "\n", "if", "name", "in", "allHomo", ":", "\n", "            ", "words", "=", "text", ".", "getElementsByTagName", "(", "\"word\"", ")", "#\u5f97\u5230word", "\n", "pun", "=", "[", "]", "\n", "for", "word", "in", "words", ":", "\n", "                ", "a", "=", "word", ".", "firstChild", ".", "data", "\n", "pun", ".", "append", "(", "a", ")", "\n", "if", "word", ".", "getAttribute", "(", "'id'", ")", "in", "allLabel", ":", "\n", "                    ", "punWord", ".", "append", "(", "a", ")", "# \u8fd9\u4e2a\u5355\u8bcd\u5c31\u662f\u53cc\u5173\u8bcd", "\n", "", "", "puns", ".", "append", "(", "pun", ")", "\n", "\n", "", "", "return", "puns", ",", "punWord", "# 1607\u6761\u8bed\u4e49\u53cc\u5173\u8bed \u4ee5\u53ca \u5bf9\u5e94\u7684\u53cc\u5173\u8bcd", "\n", "\n"]], "home.repos.pwc.inspect_result.lawsonabs_pun.tools.getSense.getSenesKeyFromUrl": [[4, 41], ["requests.get", "BeautifulSoup", "BeautifulSoup.find_all", "str", "keys.append", "ele.get_text"], "function", ["None"], ["def", "getSenesKeyFromUrl", "(", "url", ",", "sense2Key", ")", ":", "\n", "    ", "import", "requests", "# \u83b7\u53d6\u8bf7\u6c42", "\n", "from", "bs4", "import", "BeautifulSoup", "# \u5f15\u5165\u89e3\u6790\u5305", "\n", "\n", "#url = \"http://wordnetweb.princeton.edu/perl/webwn?s=sting&sub=Search+WordNet&o2=1&o0=1&o8=1&o1=1&o7=1&o5=1&o9=&o6=1&o3=1&o4=1&h=0000000000\"", "\n", "html", "=", "requests", ".", "get", "(", "url", ")", "\n", "soup", "=", "BeautifulSoup", "(", "html", ".", "text", ",", "'lxml'", ")", "#html.parser\u662f\u89e3\u6790\u5668\uff0c\u4e5f\u53ef\u662flxml", "\n", "# print(soup.prettify()) # \u8f93\u51fasoup\u5bf9\u8c61\u7684\u5185\u5bb9", "\n", "''' \u8f93\u51fa\u5185\u5bb9\u5c31\u662f\uff1a\n    <html>\n    <body>\n    <div>\n    ...\n    </div>\n    </body>\n    </html>\n    '''", "\n", "lis", "=", "soup", ".", "find_all", "(", "'li'", ")", "# \u4ecesoup\u5bf9\u8c61\u4e2d\u627e\u51fa\u6240\u6709\u7684 li \u6807\u7b7e", "\n", "for", "line", "in", "lis", ":", "\n", "        ", "''' line \u4e2d\u7684\u5185\u5bb9\u5982\u4e0b\uff0c\u65e0\u6362\u884c\n        <li>(2){14355490} &lt;noun.state&gt;[26] \n        <a href=\"webwn?o2=1&amp;o0=1&amp;o8=1&amp;o1=1&amp;o7=1&amp;o5=1&amp;o9=&amp;o6=1&amp;o3=1&amp;o4=1&amp;s=sting&amp;i=0&amp;h=0000000000#c\">S:</a>\n        <a class=\"pos\"> (n) </a>\n        <b>sting#1 (sting%1:26:00::)</b>, \n        <a href=\"webwn?o2=1&amp;o0=1&amp;o8=1&amp;o1=1&amp;o7=1&amp;o5=1&amp;o9=&amp;o6=1&amp;o3=1&amp;o4=1&amp;s=stinging\">stinging#1 (stinging%1:26:00::)</a> \n        (a kind of pain; something as sudden and painful as being stung) <i>\"the sting of death\"; \"he felt the stinging of nettles\"</i></li>\n        '''", "\n", "\n", "# way 3", "\n", "keys", "=", "[", "]", "\n", "for", "ele", "in", "line", ".", "contents", "[", "3", ":", "-", "2", "]", ":", "# \u627e\u51fa\u5176\u4e2d\u7279\u5b9a\u4e0b\u6807\u7684\u5185\u5bb9", "\n", "            ", "try", ":", "\n", "                ", "keys", ".", "append", "(", "ele", ".", "get_text", "(", ")", ")", "\n", "", "except", ":", "\n", "                ", "pass", "\n", "", "", "sense", "=", "str", "(", "line", ".", "contents", "[", "-", "2", "]", ")", "\n", "sense2Key", "[", "sense", "]", "=", "keys", "\n", "#print(f\"{keys} => {sense}\\n\")  ", "\n"]], "home.repos.pwc.inspect_result.lawsonabs_pun.tools.getSense.simpleReadXml": [[47, 66], ["set", "open", "f.readline", "f.readline.strip", "f.readline.split", "line[].split", "line[].split", "f.readline", "set.add", "set.add"], "function", ["None"], ["def", "simpleReadXml", "(", "pathLabel", ")", ":", "\n", "# step1.\u5148\u4ecepathLabel \u4e2d\u627e\u51fa\u6240\u6709\u662f\u53cc\u5173\u8bed\u7684\u53e5\u5b50    ", "\n", "    ", "allLabel", "=", "set", "(", ")", "# \u627e\u51fa\u6240\u6709\u7684\u53cc\u5173\u8bcd\u7684\u5177\u4f53\u91ca\u4e49\u9879 [hom_1_12]    ", "\n", "with", "open", "(", "pathLabel", ",", "'r'", ")", "as", "f", ":", "\n", "        ", "line", "=", "f", ".", "readline", "(", ")", "\n", "while", "(", "line", "!=", "\"\"", ")", ":", "\n", "            ", "line", "=", "line", ".", "strip", "(", ")", "# \u53bb\u884c\u672b\u6362\u884c", "\n", "line", "=", "line", ".", "split", "(", ")", "# \u7a7a\u683c\u5206\u5272            ", "\n", "#print(line[0])", "\n", "# \u5c06\u4e00\u4e32key\u5206\u5272\u6210\u4e00\u4e2a\u5217\u8868\uff0c\u7136\u540e\u904d\u5386\u5217\u8868\u5f97\u5230\u7ed3\u679c", "\n", "left_sense", "=", "line", "[", "1", "]", ".", "split", "(", "\";\"", ")", "\n", "right_sense", "=", "line", "[", "2", "]", ".", "split", "(", "\";\"", ")", "\n", "for", "sense", "in", "left_sense", ":", "\n", "                ", "allLabel", ".", "add", "(", "sense", ")", "# \u5c06\u53cc\u5173\u8bcd\u7684\u91ca\u4e49\u4fe1\u606f\u653e\u5165\u5176\u4e2d", "\n", "", "for", "sense", "in", "right_sense", ":", "\n", "                ", "allLabel", ".", "add", "(", "sense", ")", "\n", "", "line", "=", "f", ".", "readline", "(", ")", "\n", "\n", "", "", "return", "allLabel", "# 1607\u6761\u8bed\u4e49\u53cc\u5173\u8bed\u4e2d\u7684\u53cc\u5173\u8bcd", "\n", "\n"]], "home.repos.pwc.inspect_result.lawsonabs_pun.tools.getSense.getEmb": [[68, 108], ["BertModel.from_pretrained", "BertTokenizer.from_pretrained", "word2Sense.items", "print", "BertTokenizer.from_pretrained.", "BertModel.from_pretrained.", "print", "cls_emb.tolist.tolist", "print", "last_layer.size", "open", "f.write", "res.strip", "open", "f.write", "str"], "function", ["home.repos.pwc.inspect_result.lawsonabs_pun.None.bert_models.BertPreTrainedModel.from_pretrained", "home.repos.pwc.inspect_result.lawsonabs_pun.None.bert_models.BertPreTrainedModel.from_pretrained"], ["", "def", "getEmb", "(", "word2Sense", ",", "path", ")", ":", "\n", "    ", "from", "transformers", "import", "BertModel", ",", "BertTokenizer", "\n", "model", "=", "BertModel", ".", "from_pretrained", "(", "\"/home/lawson/pretrain/bert-base-cased\"", ")", "\n", "tokenizer", "=", "BertTokenizer", ".", "from_pretrained", "(", "\"/home/lawson/pretrain/bert-base-cased\"", ")", "\n", "\n", "# \u904d\u5386\u6240\u6709\u7684\u53cc\u5173\u8bcd\u53ca\u5176\u542b\u4e49\uff0c\u7136\u540e\u5c06\u5176\u542b\u4e49\u7684embedding \u5199\u5165\u5230\u6587\u4ef6\u4e2d", "\n", "for", "item", "in", "word2Sense", ".", "items", "(", ")", ":", "\n", "        ", "key", ",", "sense_list", "=", "item", "\n", "if", "key", "==", "\"stand\"", ":", "\n", "            ", "print", "(", "\"stand\"", ")", "\n", "", "print", "(", "f\"{key}  -> {sense_list}\"", ")", "\n", "# for senses in sense_list:", "\n", "#     row = senses.split()", "\n", "#     max_sense_len = max(max_sense_len,len(row))", "\n", "inputs", "=", "tokenizer", "(", "sense_list", ",", "\n", "padding", "=", "'max_length'", ",", "\n", "truncation", "=", "True", ",", "\n", "max_length", "=", "55", ",", "\n", "return_tensors", "=", "'pt'", ")", "\n", "out", "=", "model", "(", "**", "inputs", ")", "\n", "last_layer", "=", "out", ".", "last_hidden_state", "\n", "print", "(", "last_layer", ".", "size", "(", ")", ")", "\n", "cls_emb", "=", "last_layer", "[", ":", ",", "0", ",", ":", "]", "# \u53ea\u53d6 CLS \u4e0a\u7684\u503c", "\n", "\n", "# step3.\u5c06\u7ed3\u679c\u5199\u5165\u5230\u6587\u4ef6\u4e2d", "\n", "# \u8fd9\u91cc\u751f\u6210\u7684 sense \u53d1\u73b0\u5177\u6709\u6700\u591a\u542b\u4e49\u7684\u53cc\u5173\u8bcd\u662f bust\uff0c\u67097\u4e2a\u53cc\u5173\u610f\u51fa\u73b0\u5728\u8bed\u6599\u4e2d\u3002", "\n", "# \u6240\u4ee5defi_num \u53c2\u6570\u503c\u8c03\u6574\u4e3a7", "\n", "cls_emb", "=", "cls_emb", ".", "tolist", "(", ")", "\n", "with", "open", "(", "path", ",", "'a'", ")", "as", "f", ":", "# \u5199\u5165\u5355\u8bcd", "\n", "            ", "f", ".", "write", "(", "key", "+", "\" \"", ")", "\n", "\n", "# \u5c06 embedding \u503c\u5199\u5165\u5230\u6587\u4ef6\u4e2d ", "\n", "", "for", "emb", "in", "cls_emb", ":", "\n", "            ", "res", "=", "\"\"", "\n", "for", "data", "in", "emb", ":", "\n", "                ", "res", "+=", "(", "str", "(", "data", ")", ")", "+", "\" \"", "\n", "", "res", ".", "strip", "(", "\" \"", ")", "\n", "res", "+=", "\"\\n\"", "\n", "with", "open", "(", "path", ",", "'a'", ")", "as", "f", ":", "\n", "                ", "f", ".", "write", "(", "res", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.lawsonabs_pun.tools.utils.InputPronFeatures.__init__": [[12, 17], ["None"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "input_ids", ",", "input_mask", ",", "segment_ids", ",", "label_id", ",", "prons_id", ",", "prons_att_mask", ")", ":", "\n", "        ", "self", ".", "input_ids", "=", "input_ids", "\n", "self", ".", "input_mask", "=", "input_mask", "\n", "self", ".", "segment_ids", "=", "segment_ids", "\n", "self", ".", "label_id", "=", "label_id", "\n", "\n"]], "home.repos.pwc.inspect_result.lawsonabs_pun.tools.utils.readXml": [[30, 89], ["dom.parse", "root.getElementsByTagName", "open", "f.readline", "text.getAttribute", "range", "res.append", "f.readline.strip", "f.readline.split", "allHomo.append", "allLabel.append", "f.readline", "text.getElementsByTagName", "puns.append", "len", "pun.append", "len", "word.getAttribute", "punWord.append"], "function", ["None"], ["def", "readXml", "(", "pathData", ",", "pathLabel", ")", ":", "\n", "    ", "import", "xml", ".", "dom", ".", "minidom", "as", "dom", "\n", "# step1.\u5148\u4ecepathLabel \u4e2d\u627e\u51fa\u6240\u6709\u662f\u53cc\u5173\u8bed\u7684\u53e5\u5b50", "\n", "allHomo", "=", "[", "]", "# \u6807\u8bb0\u6240\u6709\u662f\u53cc\u5173\u8bed\u7684\u53e5\u5b50\u96c6\u5408 [hom_1,hom_2]", "\n", "allLabel", "=", "[", "]", "# \u627e\u51fa\u6240\u6709\u7684\u53cc\u5173\u8bcd [hom_1_12]", "\n", "punWord", "=", "[", "]", "# \u8868\u793a\u6240\u6709\u7684\u53cc\u5173\u8bcd", "\n", "with", "open", "(", "pathLabel", ",", "'r'", ")", "as", "f", ":", "\n", "        ", "line", "=", "f", ".", "readline", "(", ")", "\n", "while", "(", "line", "!=", "\"\"", ")", ":", "\n", "            ", "line", "=", "line", ".", "strip", "(", ")", "# \u53bb\u884c\u672b\u6362\u884c", "\n", "line", "=", "line", ".", "split", "(", ")", "# \u7a7a\u683c\u5206\u5272            ", "\n", "#print(line[0])", "\n", "allHomo", ".", "append", "(", "line", "[", "0", "]", ")", "\n", "allLabel", ".", "append", "(", "line", "[", "1", "]", ")", "# \u5c06\u53cc\u5173\u8bcd\u7684id\u653e\u5165\u5176\u4e2d", "\n", "line", "=", "f", ".", "readline", "(", ")", "\n", "#print(allHomo)", "\n", "\n", "# step2.\u63a5\u7740\u8bfb\u53d6\u53cc\u5173\u53e5\uff0c\u6210\u4e3a\u4e00\u884c\u6587\u672c", "\n", "#\u6253\u5f00xml\u6587\u6863", "\n", "", "", "dom2", "=", "dom", ".", "parse", "(", "pathData", ")", "\n", "#\u5f97\u5230\u6587\u6863\u5143\u7d20\u5bf9\u8c61", "\n", "root", "=", "dom2", ".", "documentElement", "\n", "texts", "=", "root", ".", "getElementsByTagName", "(", "\"text\"", ")", "# \u5f97\u5230\u6240\u6709\u7684text             ", "\n", "puns", "=", "[", "]", "# \u5b58\u50a8\u53cc\u5173\u8bed\u7684\u5217\u8868", "\n", "for", "text", "in", "texts", ":", "\n", "        ", "name", "=", "text", ".", "getAttribute", "(", "'id'", ")", "\n", "if", "name", "in", "allHomo", ":", "\n", "            ", "words", "=", "text", ".", "getElementsByTagName", "(", "\"word\"", ")", "#\u5f97\u5230word", "\n", "pun", "=", "[", "]", "\n", "for", "word", "in", "words", ":", "\n", "                ", "a", "=", "word", ".", "firstChild", ".", "data", "\n", "pun", ".", "append", "(", "a", ")", "\n", "if", "word", ".", "getAttribute", "(", "'id'", ")", "in", "allLabel", ":", "\n", "                    ", "punWord", ".", "append", "(", "a", ")", "# \u8fd9\u4e2a\u5355\u8bcd\u5c31\u662f\u53cc\u5173\u8bcd", "\n", "", "", "puns", ".", "append", "(", "pun", ")", "\n", "\n", "", "", "res", "=", "[", "]", "\n", "sym1", "=", "[", "','", ",", "'.'", ",", "'?'", ",", "'!'", ",", "';'", ",", "':'", "]", "# \u82f1\u8bed\u5e38\u7528\u7b26\u53f7\u96c6\u5408", "\n", "sym2", "=", "[", "'-'", ",", "':'", ",", "'\\''", "]", "\n", "for", "pun", "in", "puns", ":", "\n", "        ", "temp", "=", "\"\"", "\n", "for", "i", "in", "range", "(", "0", ",", "len", "(", "pun", ")", ")", ":", "\n", "            ", "cur", "=", "pun", "[", "i", "]", "\n", "if", "i", "!=", "0", ":", "\n", "                ", "pre", "=", "pun", "[", "i", "-", "1", "]", "\n", "", "else", ":", "pre", "=", "\"\"", "# i\u4e3a0\u65f6\uff0cpre\u4e3a\u7a7a", "\n", "\n", "if", "(", "len", "(", "cur", ")", ">", "1", ")", ":", "# \u8bf4\u660e\u662f\u5355\u8bcd\uff0c\u800c\u4e0d\u662f\u5b57\u7b26", "\n", "                ", "if", "pre", "not", "in", "sym2", "and", "pre", "!=", "\"\"", ":", "# \u5982\u679c\u4e0d\u5728sym2\u4e2d", "\n", "                    ", "temp", "=", "temp", "+", "\" \"", "\n", "", "temp", "=", "temp", "+", "cur", "\n", "", "else", ":", "# \u5982\u679c\u662f\u4e2a\u5355\u4e2a\u5b57\u7b26", "\n", "                ", "if", "pre", "in", "sym1", ":", "\n", "                    ", "temp", "=", "temp", "+", "\" \"", "\n", "", "temp", "=", "temp", "+", "cur", "\n", "", "", "res", ".", "append", "(", "temp", ")", "\n", "# for i in range(len(res)):", "\n", "#     print(i+1,res[i])", "\n", "", "return", "res", ",", "punWord", "# 1607\u6761\u8bed\u4e49\u53cc\u5173\u8bed \u4ee5\u53ca \u5bf9\u5e94\u7684\u53cc\u5173\u8bcd", "\n", "\n"]], "home.repos.pwc.inspect_result.lawsonabs_pun.tools.utils.writeToText": [[94, 99], ["open", "range", "len", "f.writelines"], "function", ["None"], ["def", "writeToText", "(", "puns", ",", "path", ")", ":", "\n", "    ", "with", "open", "(", "path", ",", "'w'", ")", "as", "f", ":", "\n", "        ", "for", "i", "in", "range", "(", "len", "(", "puns", ")", ")", ":", "\n", "            ", "line", "=", "puns", "[", "i", "]", "+", "\"\\n\"", "# \u5199\u5165\u6bcf\u884c\u7684\u5185\u5bb9", "\n", "f", ".", "writelines", "(", "line", ")", "# \u5199res \u5230path\u4e2d", "\n", "\n"]], "home.repos.pwc.inspect_result.lawsonabs_pun.tools.utils.simpleReadXml": [[103, 140], ["dom.parse", "root.getElementsByTagName", "open", "f.readline", "text.getAttribute", "f.readline.strip", "f.readline.split", "allHomo.append", "allLabel.append", "f.readline", "text.getElementsByTagName", "puns.append", "pun.append", "word.getAttribute", "punWord.append"], "function", ["None"], ["def", "simpleReadXml", "(", "pathData", ",", "pathLabel", ")", ":", "\n", "    ", "import", "xml", ".", "dom", ".", "minidom", "as", "dom", "\n", "# step1.\u5148\u4ecepathLabel \u4e2d\u627e\u51fa\u6240\u6709\u662f\u53cc\u5173\u8bed\u7684\u53e5\u5b50", "\n", "allHomo", "=", "[", "]", "# \u6807\u8bb0\u6240\u6709\u662f\u53cc\u5173\u8bed\u7684\u53e5\u5b50\u96c6\u5408 [hom_1,hom_2]", "\n", "allLabel", "=", "[", "]", "# \u627e\u51fa\u6240\u6709\u7684\u53cc\u5173\u8bcd [hom_1_12]", "\n", "punWord", "=", "[", "]", "# \u8868\u793a\u6240\u6709\u7684\u53cc\u5173\u8bcd", "\n", "with", "open", "(", "pathLabel", ",", "'r'", ")", "as", "f", ":", "\n", "        ", "line", "=", "f", ".", "readline", "(", ")", "\n", "while", "(", "line", "!=", "\"\"", ")", ":", "\n", "            ", "line", "=", "line", ".", "strip", "(", ")", "# \u53bb\u884c\u672b\u6362\u884c", "\n", "line", "=", "line", ".", "split", "(", ")", "# \u7a7a\u683c\u5206\u5272            ", "\n", "#print(line[0])", "\n", "allHomo", ".", "append", "(", "line", "[", "0", "]", ")", "\n", "allLabel", ".", "append", "(", "line", "[", "1", "]", ")", "# \u5c06\u53cc\u5173\u8bcd\u7684id\u653e\u5165\u5176\u4e2d", "\n", "line", "=", "f", ".", "readline", "(", ")", "\n", "#print(allHomo)", "\n", "\n", "# step2.\u63a5\u7740\u8bfb\u53d6\u53cc\u5173\u53e5\uff0c\u6210\u4e3a\u4e00\u884c\u6587\u672c", "\n", "#\u6253\u5f00xml\u6587\u6863", "\n", "", "", "dom2", "=", "dom", ".", "parse", "(", "pathData", ")", "\n", "#\u5f97\u5230\u6587\u6863\u5143\u7d20\u5bf9\u8c61", "\n", "root", "=", "dom2", ".", "documentElement", "\n", "texts", "=", "root", ".", "getElementsByTagName", "(", "\"text\"", ")", "# \u5f97\u5230\u6240\u6709\u7684text             ", "\n", "puns", "=", "[", "]", "# \u5b58\u50a8\u53cc\u5173\u8bed\u7684\u5217\u8868", "\n", "for", "text", "in", "texts", ":", "\n", "        ", "name", "=", "text", ".", "getAttribute", "(", "'id'", ")", "\n", "if", "name", "in", "allHomo", ":", "\n", "            ", "words", "=", "text", ".", "getElementsByTagName", "(", "\"word\"", ")", "#\u5f97\u5230word", "\n", "pun", "=", "[", "]", "\n", "for", "word", "in", "words", ":", "\n", "                ", "a", "=", "word", ".", "firstChild", ".", "data", "\n", "pun", ".", "append", "(", "a", ")", "\n", "if", "word", ".", "getAttribute", "(", "'id'", ")", "in", "allLabel", ":", "\n", "                    ", "punWord", ".", "append", "(", "a", ")", "# \u8fd9\u4e2a\u5355\u8bcd\u5c31\u662f\u53cc\u5173\u8bcd", "\n", "", "", "puns", ".", "append", "(", "pun", ")", "\n", "\n", "", "", "return", "puns", ",", "punWord", "# 1607\u6761\u8bed\u4e49\u53cc\u5173\u8bed \u4ee5\u53ca \u5bf9\u5e94\u7684\u53cc\u5173\u8bcd", "\n", "\n"]], "home.repos.pwc.inspect_result.lawsonabs_pun.tools.utils.convert_examples_to_pron_features": [[143, 199], ["enumerate", "example.text_a.split", "enumerate", "ntokens.append", "segment_ids.append", "label_ids.append", "enumerate", "ntokens.append", "segment_ids.append", "label_ids.append", "tokenizer.convert_tokens_to_ids", "features.append", "enumerate", "tokenizer.tokenize", "tokens.extend", "range", "len", "ntokens.append", "segment_ids.append", "label_ids.append", "len", "len", "tokenizer.convert_tokens_to_ids.append", "input_mask.append", "segment_ids.append", "label_ids.append", "len", "len", "len", "len", "utils.InputPronFeatures", "len", "labels.append", "labels.append"], "function", ["home.repos.pwc.inspect_result.lawsonabs_pun.None.bert.Ner.tokenize"], ["", "def", "convert_examples_to_pron_features", "(", "examples", ",", "label_list", ",", "max_seq_length", ",", "tokenizer", ")", ":", "\n", "    ", "\"\"\"Loads a data file into a list of `InputBatch`s.\"\"\"", "\n", "# \u6839\u636e\u4f20\u5165\u7684label_list \u751f\u6210\u4e86\u4e00\u4e2a label_map\uff0c\u4e5f\u5c31\u662f\u4e2a\u5b57\u5178", "\n", "label_map", "=", "{", "label", ":", "i", "for", "i", ",", "label", "in", "enumerate", "(", "label_list", ",", "1", ")", "}", "\n", "\n", "features", "=", "[", "]", "\n", "for", "(", "ex_index", ",", "example", ")", "in", "enumerate", "(", "examples", ")", ":", "\n", "        ", "textlist", "=", "example", ".", "text_a", ".", "split", "(", "' '", ")", "\n", "labellist", "=", "example", ".", "label", "\n", "tokens", "=", "[", "]", "\n", "labels", "=", "[", "]", "\n", "for", "i", ",", "word", "in", "enumerate", "(", "textlist", ")", ":", "\n", "            ", "token", "=", "tokenizer", ".", "tokenize", "(", "word", ")", "# \u5904\u7406\u8be5\u8bcd\uff0c\u5982\u679c\u6539\u8bcd\u627e\u4e0d\u5230\uff0c\u5219\u5206\u5757\u5904\u7406\u3002", "\n", "tokens", ".", "extend", "(", "token", ")", "\n", "label_1", "=", "labellist", "[", "i", "]", "\n", "\n", "for", "m", "in", "range", "(", "len", "(", "token", ")", ")", ":", "\n", "                ", "if", "m", "==", "0", ":", "\n", "                    ", "labels", ".", "append", "(", "label_1", ")", "\n", "", "else", ":", "# \u5982\u679c\u4e00\u4e2a\u5355\u8bcd\u88ab tokenize \u6210\u4e86\u4e24\u6bb5\uff0c\u5c31\u4f1a\u8fdb\u5165\u5230\u8fd9\u4e2aelse\u4e2d\u3002\u5c31\u4f1a\u88ab\u6807\u5fd7\u4e3a\u4e00\u4e2aX ", "\n", "                    ", "labels", ".", "append", "(", "\"X\"", ")", "\n", "", "", "", "if", "len", "(", "tokens", ")", ">=", "max_seq_length", "-", "1", ":", "# \u5224\u65ad\u6700\u540e\u7684sequence\u662f\u5426\u8d85\u8fc7\u4e86\u6700\u5927\u957f\u5ea6 ", "\n", "            ", "tokens", "=", "tokens", "[", "0", ":", "(", "max_seq_length", "-", "2", ")", "]", "\n", "labels", "=", "labels", "[", "0", ":", "(", "max_seq_length", "-", "2", ")", "]", "\n", "\n", "", "ntokens", "=", "[", "]", "\n", "segment_ids", "=", "[", "]", "\n", "label_ids", "=", "[", "]", "\n", "ntokens", ".", "append", "(", "\"[CLS]\"", ")", "\n", "segment_ids", ".", "append", "(", "0", ")", "\n", "label_ids", ".", "append", "(", "label_map", "[", "\"[CLS]\"", "]", ")", "\n", "for", "i", ",", "token", "in", "enumerate", "(", "tokens", ")", ":", "\n", "            ", "ntokens", ".", "append", "(", "token", ")", "\n", "segment_ids", ".", "append", "(", "0", ")", "\n", "label_ids", ".", "append", "(", "label_map", "[", "labels", "[", "i", "]", "]", ")", "\n", "", "ntokens", ".", "append", "(", "\"[SEP]\"", ")", "\n", "segment_ids", ".", "append", "(", "0", ")", "\n", "label_ids", ".", "append", "(", "label_map", "[", "\"[SEP]\"", "]", ")", "\n", "input_ids", "=", "tokenizer", ".", "convert_tokens_to_ids", "(", "ntokens", ")", "\n", "input_mask", "=", "[", "1", "]", "*", "len", "(", "input_ids", ")", "\n", "while", "len", "(", "input_ids", ")", "<", "max_seq_length", ":", "\n", "            ", "input_ids", ".", "append", "(", "0", ")", "\n", "input_mask", ".", "append", "(", "0", ")", "\n", "segment_ids", ".", "append", "(", "0", ")", "\n", "label_ids", ".", "append", "(", "0", ")", "\n", "", "assert", "len", "(", "input_ids", ")", "==", "max_seq_length", "\n", "assert", "len", "(", "input_mask", ")", "==", "max_seq_length", "\n", "assert", "len", "(", "segment_ids", ")", "==", "max_seq_length", "\n", "assert", "len", "(", "label_ids", ")", "==", "max_seq_length", "\n", "\n", "features", ".", "append", "(", "\n", "InputPronFeatures", "(", "input_ids", "=", "input_ids", ",", "\n", "input_mask", "=", "input_mask", ",", "\n", "segment_ids", "=", "segment_ids", ",", "\n", "label_id", "=", "label_ids", ")", ")", "\n", "", "return", "features", "\n", "\n"]], "home.repos.pwc.inspect_result.lawsonabs_pun.tools.utils.catEmb": [[203, 224], ["t.set_printoptions", "t.zeros", "wordEmb.keys", "t.tensor", "t.cat", "t.cat.size", "t.cat", "t.cat.size", "t.cat"], "function", ["None"], ["def", "catEmb", "(", "wordEmb", ",", "words", ",", "defi_num", ")", ":", "\n", "    ", "import", "torch", "as", "t", "\n", "t", ".", "set_printoptions", "(", "profile", "=", "\"full\"", ")", "\n", "zero", "=", "t", ".", "zeros", "(", "1", ",", "2", ")", "# \u7528\u4e8e\u5145\u5f53\u4e00\u4e2a\u8bcd\u7684\u5b9a\u4e49", "\n", "pun_sense_emb", "=", "None", "\n", "for", "word", "in", "words", ":", "\n", "        ", "cur_word_emb", "=", "None", "\n", "if", "word", "not", "in", "wordEmb", ".", "keys", "(", ")", ":", "# \u6839\u672c\u627e\u4e0d\u5230\u8fd9\u4e2a\u8bcd\u3002\u9700\u8981\u62fc\u63a5 defi_num \u6b21", "\n", "            ", "if", "cur_word_emb", "is", "None", ":", "\n", "                ", "cur_word_emb", "=", "zero", "\n", "", "while", "(", "cur_word_emb", ".", "size", "(", "0", ")", "<", "defi_num", ")", ":", "\n", "                ", "cur_word_emb", "=", "t", ".", "cat", "(", "(", "cur_word_emb", ",", "zero", ")", ",", "0", ")", "\n", "", "", "else", ":", "\n", "            ", "cur_word_emb", "=", "t", ".", "tensor", "(", "wordEmb", "[", "word", "]", ")", "\n", "while", "(", "cur_word_emb", ".", "size", "(", "0", ")", "<", "defi_num", ")", ":", "# \u5982\u679c\u5c0f\u4e8e defi_num \u4e2a\u5b9a\u4e49\uff0c\u5219\u6269\u5145\u5230\u8fd9\u4e48\u591a", "\n", "                ", "cur_word_emb", "=", "t", ".", "cat", "(", "(", "cur_word_emb", ",", "zero", ")", ",", "0", ")", "\n", "", "", "if", "pun_sense_emb", "is", "None", ":", "\n", "            ", "pun_sense_emb", "=", "cur_word_emb", "\n", "", "else", ":", "\n", "            ", "pun_sense_emb", "=", "t", ".", "cat", "(", "(", "pun_sense_emb", ",", "cur_word_emb", ")", ",", "0", ")", "# \u62fc\u63a5\u5f97\u5230\u4e00\u53e5\u8bdd\u4e2d\u6240\u6709\u7684embedding", "\n", "", "", "return", "pun_sense_emb", "\n", "\n"]], "home.repos.pwc.inspect_result.lawsonabs_pun.subtask3_v2.gloss.MyModel.__init__": [[81, 86], ["torch.Module.__init__", "torch.Linear", "torch.Linear", "transformers.BertModel.from_pretrained", "torch.Dropout", "torch.Dropout"], "methods", ["home.repos.pwc.inspect_result.lawsonabs_pun.subtask3_v2.gloss.EvalDataset.__init__", "home.repos.pwc.inspect_result.lawsonabs_pun.None.bert_models.BertPreTrainedModel.from_pretrained"], ["    ", "def", "__init__", "(", "self", ",", "in_fea", ",", "out_fea", ")", ":", "\n", "        ", "super", "(", "MyModel", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "linear", "=", "nn", ".", "Linear", "(", "in_fea", ",", "out_fea", ")", "\n", "self", ".", "bert", "=", "BertModel", ".", "from_pretrained", "(", "\"/home/lawson/pretrain/bert-base-cased\"", ")", "\n", "self", ".", "dropout", "=", "nn", ".", "Dropout", "(", "0.1", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.lawsonabs_pun.subtask3_v2.gloss.MyModel.forward": [[87, 93], ["gloss.MyModel.bert", "gloss.MyModel.dropout", "gloss.MyModel.linear"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "input_ids", ",", "attention_mask", ",", "token_type_ids", ")", ":", "\n", "        ", "out", "=", "self", ".", "bert", "(", "input_ids", ",", "attention_mask", ",", "token_type_ids", ")", "\n", "last_hidden_states", "=", "out", "[", "'last_hidden_state'", "]", "[", ":", ",", "0", "]", "\n", "last_hidden_states", "=", "self", ".", "dropout", "(", "last_hidden_states", ")", "\n", "logits", "=", "self", ".", "linear", "(", "last_hidden_states", ")", "\n", "return", "logits", "\n", "\n"]], "home.repos.pwc.inspect_result.lawsonabs_pun.subtask3_v2.gloss.MyDataset.__init__": [[96, 100], ["torch.utils.data.Dataset.__init__"], "methods", ["home.repos.pwc.inspect_result.lawsonabs_pun.subtask3_v2.gloss.EvalDataset.__init__"], ["    ", "def", "__init__", "(", "self", ",", "train_puns", ",", "train_labels", ")", ":", "\n", "        ", "super", "(", "MyDataset", ")", ".", "__init__", "(", ")", "\n", "self", ".", "puns", "=", "train_puns", "\n", "self", ".", "labels", "=", "train_labels", "\n", "\n"]], "home.repos.pwc.inspect_result.lawsonabs_pun.subtask3_v2.gloss.MyDataset.__len__": [[101, 103], ["len"], "methods", ["None"], ["", "def", "__len__", "(", "self", ")", ":", "\n", "        ", "return", "len", "(", "self", ".", "puns", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.lawsonabs_pun.subtask3_v2.gloss.MyDataset.__getitem__": [[104, 106], ["None"], "methods", ["None"], ["", "def", "__getitem__", "(", "self", ",", "index", ")", ":", "\n", "        ", "return", "self", ".", "puns", "[", "index", "]", ",", "self", ".", "labels", "[", "index", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.lawsonabs_pun.subtask3_v2.gloss.EvalDataset.__init__": [[111, 115], ["torch.utils.data.Dataset.__init__"], "methods", ["home.repos.pwc.inspect_result.lawsonabs_pun.subtask3_v2.gloss.EvalDataset.__init__"], ["    ", "def", "__init__", "(", "self", ",", "eval_puns", ",", "eval_labels", ")", ":", "\n", "        ", "super", "(", "EvalDataset", ")", ".", "__init__", "(", ")", "\n", "self", ".", "puns", "=", "eval_puns", "\n", "self", ".", "labels", "=", "eval_labels", "\n", "#self.keys = eval_keys", "\n"]], "home.repos.pwc.inspect_result.lawsonabs_pun.subtask3_v2.gloss.EvalDataset.__len__": [[117, 119], ["len"], "methods", ["None"], ["", "def", "__len__", "(", "self", ")", ":", "\n", "        ", "return", "len", "(", "self", ".", "puns", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.lawsonabs_pun.subtask3_v2.gloss.EvalDataset.__getitem__": [[120, 122], ["None"], "methods", ["None"], ["", "def", "__getitem__", "(", "self", ",", "index", ")", ":", "\n", "        ", "return", "self", ".", "puns", "[", "index", "]", ",", "self", ".", "labels", "[", "index", "]", "#,self.keys[index]", "\n", "\n"]], "home.repos.pwc.inspect_result.lawsonabs_pun.subtask3_v2.gloss.setup_seed": [[75, 79], ["torch.manual_seed", "numpy.random.seed", "random.seed", "random.seed"], "function", ["None"], ["def", "setup_seed", "(", "seed", ")", ":", "\n", "    ", "t", ".", "manual_seed", "(", "seed", ")", "\n", "np", ".", "random", ".", "seed", "(", "seed", ")", "\n", "random", ".", "seed", "(", "seed", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.lawsonabs_pun.subtask3_v2.gloss.cal_metric": [[129, 147], ["zip", "len", "len"], "function", ["None"], ["def", "cal_metric", "(", "all_pred_keys", ",", "all_true_keys", ")", ":", "\n", "    ", "correct_num", "=", "0", "\n", "for", "pred", ",", "true", "in", "zip", "(", "all_pred_keys", ",", "all_true_keys", ")", ":", "\n", "        ", "true_left", ",", "true_right", "=", "true", "\n", "if", "len", "(", "pred", ")", "<", "2", ":", "\n", "            ", "continue", "\n", "", "pred_left", ",", "pred_right", "=", "pred", "\n", "\n", "if", "true_left", "in", "pred_left", ":", "\n", "            ", "if", "true_right", "in", "pred_right", ":", "\n", "                ", "correct_num", "+=", "1", "\n", "continue", "\n", "\n", "", "", "elif", "true_left", "in", "pred_right", ":", "\n", "            ", "if", "true_right", "in", "pred_left", ":", "\n", "                ", "correct_num", "+=", "1", "\n", "\n", "", "", "", "return", "correct_num", "/", "len", "(", "all_pred_keys", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.lawsonabs_pun.subtask3_v2.gloss.train": [[150, 358], ["gloss.setup_seed", "getAllPunWords", "getAllPuns", "readTask3Label_2", "list", "numpy.array", "numpy.array", "list", "list", "list", "numpy.array", "sklearn.model_selection.KFold", "sklearn.model_selection.KFold.get_n_splits", "sklearn.model_selection.KFold.split", "getAllPuns.values", "getAllPuns.keys", "readTask3Label_2.keys", "readTask3Label_2.values", "zip", "numpy.array", "numpy.array", "numpy.array", "gloss.MyDataset", "torch.utils.data.DataLoader", "gloss.MyModel", "model.cuda.cuda", "torch.CrossEntropyLoss", "torch.optim.Adam", "tqdm.tqdm", "logger.info", "zip", "model.cuda.parameters", "range", "tqdm.tqdm", "torch.save", "logger.info", "zip", "gloss.cal_metric", "max", "logger.info", "nltk.corpus.wordnet.synsets", "labels.cuda.cuda", "tokenizer", "inputs[].cuda", "inputs[].cuda", "inputs[].cuda", "model.cuda.", "nn.CrossEntropyLoss.", "t.optim.Adam.zero_grad", "criterion.backward", "t.optim.Adam.step", "criterion.item", "eval_true_key.append", "zip", "nltk.corpus.wordnet.synset", "wn.synset.definition", "wn.synset.lemmas", "np.array.append", "viz.line", "nltk.corpus.wordnet.synsets", "len", "torch.no_grad", "tokenizer", "inputs[].cuda", "inputs[].cuda", "inputs[].cuda", "model.cuda.", "nn.CrossEntropyLoss.", "torch.Softmax", "nn.Softmax.", "list", "logger.info", "all_pred_key.append", "synset.name", "lemma.key", "lemma_keys.append", "np.array.append", "np.array.append", "np.array.append", "np.array.append", "nltk.corpus.wordnet.synset", "wn.synset.definition", "wn.synset.lemmas", "cur_eval_key.append", "torch.tensor().cuda", "sorted", "logger.info", "synset.name", "lemma.key", "lemma_keys.append", "eval_pun.append", "eval_label.append", "eval_pun.append", "eval_label.append", "pred_label_index_map.items", "wn.synset.append", "torch.tensor"], "function", ["home.repos.pwc.inspect_result.lawsonabs_pun.subtask3_v2.gloss.setup_seed", "home.repos.pwc.inspect_result.lawsonabs_pun.subtask3_v2.gloss.cal_metric"], ["", "def", "train", "(", ")", ":", "\n", "    ", "setup_seed", "(", "args", ".", "seed", ")", "\n", "pun_words", "=", "getAllPunWords", "(", "dataPath", "=", "\"/home/lawson/program/punLocation/data/puns/test/homo/subtask3-homographic-test.xml\"", ")", "\n", "id_puns_map", "=", "getAllPuns", "(", "dataPath", "=", "\"/home/lawson/program/punLocation/data/puns/test/homo/subtask3-homographic-test.xml\"", ")", "\n", "pun2Label", "=", "readTask3Label_2", "(", "labelPath", "=", "\"/home/lawson/program/punLocation/data/puns/test/homo/subtask3-homographic-test.gold\"", ")", "\n", "# pun_words = getAllPunWords(dataPath=\"/home/lawson/program/punLocation/data/puns/test/homo/test.xml\")", "\n", "# id_puns_map = getAllPuns(dataPath=\"/home/lawson/program/punLocation/data/puns/test/homo/test.xml\")", "\n", "# pun2Label = readTask3Label_2(labelPath=\"/home/lawson/program/punLocation/data/puns/test/homo/test.gold\")", "\n", "puns", "=", "list", "(", "id_puns_map", ".", "values", "(", ")", ")", "\n", "puns", "=", "np", ".", "array", "(", "puns", ")", "# \u5fc5\u987b\u8f6c\u4e3anumpy\uff0c\u8fd9\u6837\u624d\u53ef\u4ee5\u4f7f\u7528\u540e\u9762cv\u4e2d\u7684\u9009\u62e9\u64cd\u4f5c", "\n", "pun_words", "=", "np", ".", "array", "(", "pun_words", ")", "\n", "puns_id_1", "=", "list", "(", "id_puns_map", ".", "keys", "(", ")", ")", "\n", "puns_id_2", "=", "list", "(", "pun2Label", ".", "keys", "(", ")", ")", "\n", "pun_labels", "=", "list", "(", "pun2Label", ".", "values", "(", ")", ")", "\n", "pun_labels", "=", "np", ".", "array", "(", "pun_labels", ")", "\n", "pos_list", "=", "[", "'a'", ",", "'r'", ",", "'n'", ",", "'v'", "]", "# adj ,adv ,n ,verb", "\n", "pos_name", "=", "[", "'adjective'", ",", "'adverb'", ",", "'noun'", ",", "'verb'", "]", "\n", "# \u4f7f\u7528\u4ea4\u53c9\u9a8c\u8bc1", "\n", "kf", "=", "KFold", "(", "n_splits", "=", "10", ")", "# \u5206\u527210\u4efd", "\n", "kf", ".", "get_n_splits", "(", "puns", ")", "\n", "cv_index", "=", "0", "\n", "for", "train_index", ",", "test_index", "in", "kf", ".", "split", "(", "puns", ")", ":", "\n", "        ", "win", "=", "f\"train_loss_{cv_index}\"", "\n", "raw_train_pun", "=", "puns", "[", "train_index", "]", "\n", "raw_train_pun_words", "=", "pun_words", "[", "train_index", "]", "\n", "raw_train_pun_labels", "=", "pun_labels", "[", "train_index", "]", "\n", "\n", "raw_eval_pun", "=", "puns", "[", "test_index", "]", "\n", "raw_eval_pun_words", "=", "pun_words", "[", "test_index", "]", "\n", "raw_eval_pun_labels", "=", "pun_labels", "[", "test_index", "]", "\n", "\n", "# \u62fc\u51d1\u5f97\u5230\u8bad\u7ec3\u6837\u672c", "\n", "train_pun", "=", "[", "]", "\n", "train_label", "=", "[", "]", "\n", "train_key", "=", "[", "]", "# \u4fdd\u5b58\u6bcf\u4e2akey\uff0c\u7528\u4e8e\u89e3\u7801\u65f6\u5f97\u5230\u6807\u7b7e", "\n", "# step1.\u83b7\u53d6\u6240\u6709\u5355\u8bcd", "\n", "for", "word", ",", "cur_label_key", ",", "cur_pun", "in", "zip", "(", "raw_train_pun_words", ",", "raw_train_pun_labels", ",", "raw_train_pun", ")", ":", "\n", "            ", "cur_pun", "=", "\" \"", ".", "join", "(", "cur_pun", "[", "0", ":", "-", "1", "]", ")", "\n", "for", "pos", ",", "name", "in", "zip", "(", "pos_list", ",", "pos_name", ")", ":", "\n", "                ", "synsets", "=", "wn", ".", "synsets", "(", "word", ",", "pos", "=", "pos", ")", "# \u6309pos\u5206\u522b\u627e\u51fa word\u8fd9\u4e2a\u5355\u8bcd\u6240\u6709\u7684\u542b\u4e49                            ", "\n", "for", "synset", "in", "synsets", ":", "\n", "                    ", "lemma_keys", "=", "[", "]", "\n", "temp", "=", "wn", ".", "synset", "(", "synset", ".", "name", "(", ")", ")", "\n", "gloss", "=", "temp", ".", "definition", "(", ")", "\n", "lemmas", "=", "temp", ".", "lemmas", "(", ")", "\n", "\n", "for", "lemma", "in", "lemmas", ":", "\n", "                        ", "lemma_key", "=", "lemma", ".", "key", "(", ")", "\n", "lemma_keys", ".", "append", "(", "lemma_key", ")", "\n", "", "train_key", ".", "append", "(", "lemma_keys", ")", "\n", "\n", "# \u4f7f\u7528flag \u7684\u539f\u56e0\u662f\uff1a\u65e0\u8bba\u6b63\u8d1f\uff0c\u53ea\u6dfb\u52a0\u4e00\u4e2a\u6837\u672c", "\n", "flag", "=", "False", "\n", "for", "lable_key", "in", "cur_label_key", ":", "\n", "                        ", "if", "lable_key", "in", "lemma_keys", ":", "\n", "                            ", "flag", "=", "True", "\n", "\n", "", "", "if", "flag", ":", "\n", "                        ", "temp_text", "=", "cur_pun", "+", "\" [SEP] \"", "+", "name", "+", "\" : \"", "+", "gloss", "\n", "# temp_text = cur_pun+\" [SEP] \" + gloss", "\n", "train_pun", ".", "append", "(", "temp_text", ")", "\n", "train_label", ".", "append", "(", "1", ")", "\n", "", "else", ":", "\n", "                        ", "temp_text", "=", "cur_pun", "+", "\" [SEP] \"", "+", "name", "+", "\" : \"", "+", "gloss", "\n", "# temp_text = cur_pun+\" [SEP] \" + gloss", "\n", "train_pun", ".", "append", "(", "temp_text", ")", "\n", "train_label", ".", "append", "(", "0", ")", "\n", "", "", "", "", "train_pun", "=", "np", ".", "array", "(", "train_pun", ")", "\n", "train_label", "=", "np", ".", "array", "(", "train_label", ")", "\n", "train_key", "=", "np", ".", "array", "(", "train_key", ")", "\n", "\n", "\n", "train_data_set", "=", "MyDataset", "(", "train_pun", ",", "train_label", ")", "\n", "train_data_loader", "=", "DataLoader", "(", "train_data_set", ",", "\n", "batch_size", "=", "args", ".", "train_batch_size", ",", "\n", "shuffle", "=", "True", "# \u56e0\u4e3a\u751f\u6210\u7684\u6570\u636e\u90fd\u5728\u4e00\u5757\uff0c\u5927\u90fd\u6bd4\u8f83\u76f8\u4f3c\uff0c\u6240\u4ee5\u9700\u8981shuffle \u4e00\u4e0b", "\n", ")", "\n", "\n", "model", "=", "MyModel", "(", "in_fea", "=", "768", ",", "out_fea", "=", "2", ")", "# \u56e0\u4e3a\u53ea\u662fyes/no \u5206\u7c7b\uff0c\u6240\u4ee5\u8fd9\u91cc\u7684\u8f93\u51fa\u7ef4\u5ea6\u5c31\u662f2", "\n", "model", "=", "model", ".", "cuda", "(", ")", "\n", "criterion", "=", "nn", ".", "CrossEntropyLoss", "(", ")", "\n", "optimizer", "=", "t", ".", "optim", ".", "Adam", "(", "model", ".", "parameters", "(", ")", ",", "lr", "=", "2e-5", ")", "\n", "global_step", "=", "0", "\n", "loggert_step", "=", "50", "\n", "max_f1", "=", "0", "\n", "for", "epoch", "in", "tqdm", "(", "range", "(", "args", ".", "train_epoch", ")", ")", ":", "\n", "            ", "logger_loss", "=", "0", "\n", "for", "batch", "in", "tqdm", "(", "train_data_loader", ")", ":", "\n", "                ", "x", ",", "labels", "=", "batch", "\n", "labels", "=", "labels", ".", "cuda", "(", ")", "\n", "inputs", "=", "tokenizer", "(", "x", ",", "max_length", "=", "args", ".", "max_length", ",", "\n", "padding", "=", "'max_length'", ",", "\n", "return_tensors", "=", "'pt'", ",", "\n", "truncation", "=", "True", ")", "\n", "input_id", "=", "inputs", "[", "'input_ids'", "]", ".", "cuda", "(", ")", "\n", "attention_mask", "=", "inputs", "[", "'attention_mask'", "]", ".", "cuda", "(", ")", "\n", "token_type_ids", "=", "inputs", "[", "'token_type_ids'", "]", ".", "cuda", "(", ")", "\n", "\n", "logits", "=", "model", "(", "input_id", ",", "attention_mask", ",", "token_type_ids", ")", "\n", "loss", "=", "criterion", "(", "logits", ",", "labels", ")", "\n", "optimizer", ".", "zero_grad", "(", ")", "\n", "loss", ".", "backward", "(", ")", "\n", "optimizer", ".", "step", "(", ")", "\n", "# logger.info(f\"loss={loss}\")", "\n", "if", "global_step", "%", "loggert_step", "==", "0", "and", "global_step", ":", "\n", "                    ", "viz", ".", "line", "(", "[", "logger_loss", "]", ",", "[", "global_step", "]", ",", "win", "=", "win", ",", "update", "=", "\"append\"", ")", "\n", "logger_loss", "=", "0", "\n", "", "global_step", "+=", "1", "\n", "logger_loss", "+=", "loss", ".", "item", "(", ")", "\n", "", "save_path", "=", "f\"/home/lawson/program/punLocation/checkpoints/gloss_model_{epoch}.ckpt_1\"", "\n", "t", ".", "save", "(", "model", ".", "state_dict", ",", "save_path", ")", "\n", "\n", "# ============================================ evaluate ============================================", "\n", "logger", ".", "info", "(", "\"============= start evaluating =============\\n\"", ")", "\n", "# \u8bc4\u6d4b\u65f6\uff0c\u9700\u8981\u9010\u6761\u751f\u6210\u7ed3\u679c\uff0c\u6240\u4ee5\u5c31\u4e0d\u7528batch\u3002\u62fc\u51d1\u5f97\u5230\u8bad\u7ec3\u6837\u672c            ", "\n", "# step1. \u8fb9\u751f\u6210\uff0c\u8fb9\u9884\u6d4b", "\n", "eval_true_key", "=", "[", "]", "# \u62ff\u5230eval\u7684\u771f\u5b9ekey\uff0c\u7136\u540e\u548cpred\u8ba1\u7b97metric \u503c", "\n", "all_pred_key", "=", "[", "]", "# \u5f97\u5230\u6240\u6709\u9884\u6d4b\u7684key            ", "\n", "for", "word", ",", "cur_label_key", ",", "cur_pun", "in", "zip", "(", "raw_eval_pun_words", ",", "raw_eval_pun_labels", ",", "raw_eval_pun", ")", ":", "\n", "                ", "eval_pun", "=", "[", "]", "\n", "eval_label", "=", "[", "]", "\n", "cur_eval_key", "=", "[", "]", "# \u4fdd\u5b58\u5f53\u524d\u8fd9\u4e2a\u6837\u672c\u7684key\uff0c\u7528\u4e8e\u89e3\u7801\u65f6\u5f97\u5230\u6807\u7b7e", "\n", "cur_pun", "=", "\" \"", ".", "join", "(", "cur_pun", "[", "0", ":", "-", "1", "]", ")", "\n", "eval_true_key", ".", "append", "(", "cur_label_key", ")", "\n", "for", "pos", ",", "name", "in", "zip", "(", "pos_list", ",", "pos_name", ")", ":", "\n", "                    ", "synsets", "=", "wn", ".", "synsets", "(", "word", ",", "pos", "=", "pos", ")", "# \u5206\u8bcd\u6027\u7c7b\u522b\uff08pos\uff09\u7684\u627e\u51faword\u8fd9\u4e2a\u5355\u8bcd\u6240\u6709\u7684\u542b\u4e49                    ", "\n", "for", "synset", "in", "synsets", ":", "\n", "                        ", "lemma_keys", "=", "[", "]", "\n", "temp", "=", "wn", ".", "synset", "(", "synset", ".", "name", "(", ")", ")", "\n", "gloss", "=", "temp", ".", "definition", "(", ")", "\n", "lemmas", "=", "temp", ".", "lemmas", "(", ")", "\n", "\n", "for", "lemma", "in", "lemmas", ":", "\n", "                            ", "lemma_key", "=", "lemma", ".", "key", "(", ")", "\n", "lemma_keys", ".", "append", "(", "lemma_key", ")", "\n", "", "cur_eval_key", ".", "append", "(", "lemma_keys", ")", "# \u8fd9\u91cc\u6539\u53d8\u6210np\u578b\u7684\u6570\u7ec4", "\n", "\n", "# \u4f7f\u7528flag \u7684\u539f\u56e0\u662f\uff1a\u65e0\u8bba\u6b63\u8d1f\uff0c\u53ea\u6dfb\u52a0\u4e00\u4e2a\u6837\u672c", "\n", "flag", "=", "False", "\n", "for", "lable_key", "in", "cur_label_key", ":", "\n", "                            ", "if", "lable_key", "in", "lemma_keys", ":", "\n", "                                ", "flag", "=", "True", "\n", "\n", "", "", "if", "flag", ":", "\n", "                            ", "temp_text", "=", "cur_pun", "+", "\" [SEP] \"", "+", "name", "+", "\" : \"", "+", "gloss", "\n", "# temp_text = cur_pun+\" [SEP] \" + gloss", "\n", "eval_pun", ".", "append", "(", "temp_text", ")", "\n", "eval_label", ".", "append", "(", "1", ")", "\n", "", "else", ":", "\n", "                            ", "temp_text", "=", "cur_pun", "+", "\" [SEP] \"", "+", "name", "+", "\" : \"", "+", "gloss", "\n", "# temp_text = cur_pun+\" [SEP] \" + gloss", "\n", "eval_pun", ".", "append", "(", "temp_text", ")", "\n", "eval_label", ".", "append", "(", "0", ")", "\n", "", "", "", "if", "len", "(", "eval_pun", ")", "==", "0", ":", "\n", "                    ", "continue", "\n", "", "with", "t", ".", "no_grad", "(", ")", ":", "\n", "                    ", "inputs", "=", "tokenizer", "(", "eval_pun", ",", "\n", "max_length", "=", "args", ".", "max_length", ",", "\n", "padding", "=", "'max_length'", ",", "\n", "return_tensors", "=", "'pt'", ",", "\n", "truncation", "=", "True", "\n", ")", "\n", "input_id", "=", "inputs", "[", "'input_ids'", "]", ".", "cuda", "(", ")", "\n", "attention_mask", "=", "inputs", "[", "'attention_mask'", "]", ".", "cuda", "(", ")", "\n", "token_type_ids", "=", "inputs", "[", "'token_type_ids'", "]", ".", "cuda", "(", ")", "\n", "\n", "logits", "=", "model", "(", "input_id", ",", "attention_mask", ",", "token_type_ids", ")", "\n", "# size = [eval_batch_size,2]", "\n", "loss", "=", "criterion", "(", "logits", ",", "t", ".", "tensor", "(", "eval_label", ")", ".", "cuda", "(", ")", ")", "\n", "# logger.info(f\"loss={loss}\")", "\n", "\n", "# \u5148softmax\u4e00\u4e0b\uff0c\u662f\u4e3a\u4e86\u66f4\u597d\u7684\u5b9a\u9608\u503c", "\n", "softmax", "=", "nn", ".", "Softmax", "(", "dim", "=", "-", "1", ")", "\n", "logits", "=", "softmax", "(", "logits", ")", "\n", "pred_label", "=", "logits", "[", ":", ",", "1", "]", "# \u5f97\u5230\u9884\u6d4b\u503c\u4e3a1\u7684\u6982\u7387", "\n", "# threshold = 0.5                        ", "\n", "# pred_label_index = [index for index in range(len(pred_label)) if pred_label[index] > threshold]", "\n", "# if len(pred_label_index)!=0:", "\n", "#     pred_key = eval_key[pred_label_index]", "\n", "#     temp = np.array(x) # \u8f6c\u6362\u6210numpy\uff0c\u65b9\u4fbf\u505a\u4e0b\u9762\u7684\u5207\u7247\u64cd\u4f5c", "\n", "#     raw_text = temp[pred_label_index]", "\n", "#     print(f\"pred_key={pred_key},raw_text={raw_text}\")", "\n", "cnt", "=", "0", "\n", "pred_label_index_map", "=", "{", "}", "\n", "for", "pred", "in", "pred_label", ":", "\n", "                        ", "pred_label_index_map", "[", "cnt", "]", "=", "pred", "\n", "cnt", "+=", "1", "\n", "", "pred_label", "=", "list", "(", "sorted", "(", "pred_label_index_map", ".", "items", "(", ")", ",", "key", "=", "lambda", "x", ":", "x", "[", "1", "]", ",", "reverse", "=", "True", ")", ")", "\n", "temp", "=", "[", "]", "\n", "# \u53ea\u53d6top2 \u4f5c\u4e3a\u6700\u540e\u7684\u91ca\u4e49", "\n", "cnt", "=", "0", "\n", "for", "item", "in", "pred_label", "[", "0", ":", ":", "]", ":", "\n", "                        ", "index", ",", "score", "=", "item", "\n", "pred_key", "=", "cur_eval_key", "[", "index", "]", "\n", "raw_text", "=", "eval_pun", "[", "index", "]", "\n", "if", "cnt", "<", "2", ":", "\n", "                            ", "temp", ".", "append", "(", "pred_key", ")", "\n", "", "logger", ".", "info", "(", "f\"score = {score}, pred_key={pred_key},raw_text={raw_text}\"", ")", "\n", "cnt", "+=", "1", "\n", "", "logger", ".", "info", "(", "\"\\n\"", ")", "\n", "all_pred_key", ".", "append", "(", "temp", ")", "\n", "\n", "", "", "f1", "=", "cal_metric", "(", "all_pred_key", ",", "eval_true_key", ")", "\n", "max_f1", "=", "max", "(", "f1", ",", "max_f1", ")", "\n", "logger", ".", "info", "(", "f\"cv_index = {cv_index},epoch = {epoch},f1={f1}\\n\"", ")", "\n", "\n", "", "logger", ".", "info", "(", "f\"max_f1 = {max_f1}\\n\"", ")", "\n", "cv_index", "+=", "1", "\n", "\n"]], "home.repos.pwc.inspect_result.lawsonabs_pun.data_with_pronunciation.parse_xml_pd.read_xml": [[6, 20], ["xml.parse", "ET.parse.getroot", "range", "original_sentences.append", "text_ids.append", "len", "original_sentence.append"], "function", ["None"], ["def", "read_xml", "(", "input_file", ")", ":", "\n", "    ", "tree", "=", "ET", ".", "parse", "(", "input_file", ")", "\n", "root", "=", "tree", ".", "getroot", "(", ")", "\n", "original_sentences", ",", "text_ids", "=", "[", "]", ",", "[", "]", "\n", "\n", "for", "child", "in", "root", ":", "\n", "\n", "        ", "original_sentence", "=", "[", "]", "\n", "text_id", "=", "child", ".", "attrib", "[", "'id'", "]", "\n", "for", "i", "in", "range", "(", "len", "(", "child", ")", ")", ":", "\n", "            ", "original_sentence", ".", "append", "(", "child", "[", "i", "]", ".", "text", ")", "\n", "", "original_sentences", ".", "append", "(", "original_sentence", ")", "\n", "text_ids", ".", "append", "(", "text_id", ")", "\n", "", "return", "original_sentences", ",", "text_ids", "\n", "\n"]], "home.repos.pwc.inspect_result.lawsonabs_pun.data_with_pronunciation.parse_xml_pd.read_labels": [[21, 32], ["open", "f.readlines", "line.strip().split", "label_ids.append", "labels.append", "line.strip"], "function", ["None"], ["", "def", "read_labels", "(", "input_file", ")", ":", "\n", "    ", "labels", ",", "label_ids", "=", "[", "]", ",", "[", "]", "\n", "\n", "with", "open", "(", "input_file", ",", "\"r\"", ")", "as", "f", ":", "\n", "        ", "contents", "=", "f", ".", "readlines", "(", ")", "\n", "for", "line", "in", "contents", ":", "\n", "            ", "vec", "=", "line", ".", "strip", "(", ")", ".", "split", "(", "'\\t'", ")", "\n", "label_ids", ".", "append", "(", "vec", "[", "0", "]", ")", "\n", "labels", ".", "append", "(", "vec", "[", "1", "]", ")", "\n", "\n", "", "", "return", "labels", ",", "label_ids", "\n", "\n"]], "home.repos.pwc.inspect_result.lawsonabs_pun.data_with_pronunciation.parse_xml_pd.main": [[33, 48], ["parse_xml_pd.read_xml", "parse_xml_pd.read_labels", "parse_xml_pd.read_xml", "open", "open", "open", "range", "len", "sent.encode.encode", "open.write", "x.split", "str"], "function", ["home.repos.pwc.inspect_result.lawsonabs_pun.data_with_pronunciation.parse_xml_pl.read_xml", "home.repos.pwc.inspect_result.lawsonabs_pun.data_with_pronunciation.parse_xml_pl.read_labels", "home.repos.pwc.inspect_result.lawsonabs_pun.data_with_pronunciation.parse_xml_pl.read_xml"], ["", "def", "main", "(", "argv", ")", ":", "\n", "    ", "sents", ",", "ids1", "=", "read_xml", "(", "argv", "[", "1", "]", ")", "\n", "labs", ",", "ids2", "=", "read_labels", "(", "argv", "[", "2", "]", ")", "\n", "prons", ",", "ids3", "=", "read_xml", "(", "argv", "[", "3", "]", ")", "\n", "assert", "(", "ids1", "==", "ids2", ")", "\n", "output", "=", "argv", "[", "4", "]", "\n", "\n", "train", "=", "open", "(", "output", "+", "'train.tsv'", ",", "'w'", ")", "\n", "test", "=", "open", "(", "output", "+", "'test.tsv'", ",", "'w'", ")", "\n", "dev", "=", "open", "(", "output", "+", "'dev.tsv'", ",", "'w'", ")", "\n", "for", "i", "in", "range", "(", "len", "(", "sents", ")", ")", ":", "\n", "        ", "pron", "=", "' '", ".", "join", "(", "[", "','", ".", "join", "(", "x", ".", "split", "(", "' '", ")", ")", "for", "x", "in", "prons", "[", "i", "]", "]", ")", "\n", "sent", "=", "str", "(", "labs", "[", "i", "]", ")", "+", "'\\t'", "+", "' '", ".", "join", "(", "sents", "[", "i", "]", ")", "+", "'\\t'", "+", "pron", "+", "'\\n'", "\n", "sent", "=", "sent", ".", "encode", "(", "'utf-8'", ")", "\n", "train", ".", "write", "(", "sent", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.lawsonabs_pun.data_with_pronunciation.parse_xml_pl.read_xml": [[6, 20], ["xml.parse", "ET.parse.getroot", "range", "original_sentences.append", "text_ids.append", "len", "original_sentence.append"], "function", ["None"], ["def", "read_xml", "(", "input_file", ")", ":", "\n", "    ", "tree", "=", "ET", ".", "parse", "(", "input_file", ")", "\n", "root", "=", "tree", ".", "getroot", "(", ")", "\n", "original_sentences", ",", "text_ids", "=", "[", "]", ",", "[", "]", "\n", "\n", "for", "child", "in", "root", ":", "\n", "\n", "        ", "original_sentence", "=", "[", "]", "\n", "text_id", "=", "child", ".", "attrib", "[", "'id'", "]", "\n", "for", "i", "in", "range", "(", "len", "(", "child", ")", ")", ":", "\n", "            ", "original_sentence", ".", "append", "(", "child", "[", "i", "]", ".", "text", ")", "\n", "", "original_sentences", ".", "append", "(", "original_sentence", ")", "\n", "text_ids", ".", "append", "(", "text_id", ")", "\n", "", "return", "original_sentences", ",", "text_ids", "\n", "\n"]], "home.repos.pwc.inspect_result.lawsonabs_pun.data_with_pronunciation.parse_xml_pl.read_labels": [[21, 32], ["open", "f.readlines", "line.strip().split", "label_ids.append", "labels.append", "line.strip"], "function", ["None"], ["", "def", "read_labels", "(", "input_file", ")", ":", "\n", "    ", "labels", ",", "label_ids", "=", "[", "]", ",", "[", "]", "\n", "\n", "with", "open", "(", "input_file", ",", "\"r\"", ")", "as", "f", ":", "\n", "        ", "contents", "=", "f", ".", "readlines", "(", ")", "\n", "for", "line", "in", "contents", ":", "\n", "            ", "vec", "=", "line", ".", "strip", "(", ")", ".", "split", "(", "'\\t'", ")", "\n", "label_ids", ".", "append", "(", "vec", "[", "0", "]", ")", "\n", "labels", ".", "append", "(", "vec", "[", "1", "]", ")", "\n", "\n", "", "", "return", "labels", ",", "label_ids", "\n", "\n"]], "home.repos.pwc.inspect_result.lawsonabs_pun.data_with_pronunciation.parse_xml_pl.write_ner": [[33, 43], ["int", "range", "f.write", "len", "tag.split", "pron[].split", "f.write", "f.write"], "function", ["None"], ["", "def", "write_ner", "(", "sent", ",", "tag", ",", "pron", ",", "f", ")", ":", "\n", "    ", "index", "=", "int", "(", "tag", ".", "split", "(", "'_'", ")", "[", "-", "1", "]", ")", "\n", "for", "i", "in", "range", "(", "len", "(", "sent", ")", ")", ":", "\n", "        ", "prons", "=", "','", ".", "join", "(", "pron", "[", "i", "]", ".", "split", "(", "' '", ")", ")", "#.encode('utf-8')", "\n", "sents", "=", "sent", "[", "i", "]", "#.encode('utf-8')", "\n", "if", "index", "==", "i", "+", "1", ":", "\n", "            ", "f", ".", "write", "(", "sents", "+", "' '", "+", "'P'", "+", "' '", "+", "prons", "+", "'\\n'", ")", "\n", "", "else", ":", "\n", "            ", "f", ".", "write", "(", "sents", "+", "' '", "+", "'O'", "+", "' '", "+", "prons", "+", "'\\n'", ")", "\n", "", "", "f", ".", "write", "(", "'\\n'", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.lawsonabs_pun.data_with_pronunciation.parse_xml_pl.main": [[44, 65], ["parse_xml_pl.read_xml", "parse_xml_pl.read_labels", "parse_xml_pl.read_xml", "open", "open", "open", "range", "len", "print", "parse_xml_pl.write_ner"], "function", ["home.repos.pwc.inspect_result.lawsonabs_pun.data_with_pronunciation.parse_xml_pl.read_xml", "home.repos.pwc.inspect_result.lawsonabs_pun.data_with_pronunciation.parse_xml_pl.read_labels", "home.repos.pwc.inspect_result.lawsonabs_pun.data_with_pronunciation.parse_xml_pl.read_xml", "home.repos.pwc.inspect_result.lawsonabs_pun.data_with_pronunciation.parse_xml_pl.write_ner"], ["", "def", "main", "(", "argv", ")", ":", "\n", "    ", "sents", ",", "ids1", "=", "read_xml", "(", "argv", "[", "1", "]", ")", "\n", "labs", ",", "ids2", "=", "read_labels", "(", "argv", "[", "2", "]", ")", "\n", "prons", ",", "ids3", "=", "read_xml", "(", "argv", "[", "3", "]", ")", "\n", "output", "=", "argv", "[", "4", "]", "\n", "\n", "assert", "(", "ids1", "==", "ids2", ")", "\n", "assert", "(", "ids2", "==", "ids3", ")", "\n", "\n", "#file_name = argv[1].replace('.xml','')", "\n", "#with open(file_name, 'w') as f:", "\n", "#    writer = csv.writer(f, delimiter='\\t')", "\n", "#    for i in range(len(sents)):", "\n", "#        writer.writerow([str(labs[i]),str(' '.join(sents[i]))])", "\n", "\n", "train", "=", "open", "(", "output", "+", "'train.txt'", ",", "'w'", ")", "\n", "test", "=", "open", "(", "output", "+", "'test.txt'", ",", "'w'", ")", "\n", "dev", "=", "open", "(", "output", "+", "'valid.txt'", ",", "'w'", ")", "\n", "for", "i", "in", "range", "(", "len", "(", "sents", ")", ")", ":", "\n", "        ", "print", "(", "sents", "[", "i", "]", ")", "\n", "write_ner", "(", "sents", "[", "i", "]", ",", "labs", "[", "i", "]", ",", "prons", "[", "i", "]", ",", "train", ")", "\n", "#num = random.random()", "\n"]]}