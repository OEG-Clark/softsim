{"home.repos.pwc.inspect_result.Stanford-ILIAD_PantheonRL.None.bctrainer.input_check": [[15, 23], ["bctrainer.EnvException", "bctrainer.EnvException"], "function", ["None"], ["", "def", "input_check", "(", "args", ")", ":", "\n", "# Env checking", "\n", "    ", "if", "args", ".", "env", "==", "'OvercookedMultiEnv-v0'", ":", "\n", "        ", "if", "'layout_name'", "not", "in", "args", ".", "env_config", ":", "\n", "            ", "raise", "EnvException", "(", "f\"layout_name needed for {args.env}\"", ")", "\n", "", "elif", "args", ".", "env_config", "[", "'layout_name'", "]", "not", "in", "LAYOUT_LIST", ":", "\n", "            ", "raise", "EnvException", "(", "\n", "f\"{args.env_config['layout_name']} is not a valid layout\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Stanford-ILIAD_PantheonRL.None.trainer.input_check": [[41, 64], ["trainer.EnvException", "trainer.EnvException", "len", "len", "trainer.EnvException", "trainer.EnvException"], "function", ["None"], ["", "def", "input_check", "(", "args", ")", ":", "\n", "# Env checking", "\n", "    ", "if", "args", ".", "env", "==", "'OvercookedMultiEnv-v0'", ":", "\n", "        ", "if", "'layout_name'", "not", "in", "args", ".", "env_config", ":", "\n", "            ", "raise", "EnvException", "(", "f\"layout_name needed for {args.env}\"", ")", "\n", "", "elif", "args", ".", "env_config", "[", "'layout_name'", "]", "not", "in", "LAYOUT_LIST", ":", "\n", "            ", "raise", "EnvException", "(", "\n", "f\"{args.env_config['layout_name']} is not a valid layout\"", ")", "\n", "\n", "# Construct alt configs", "\n", "", "", "if", "args", ".", "alt_config", "is", "None", ":", "\n", "        ", "args", ".", "alt_config", "=", "[", "{", "}", "for", "_", "in", "args", ".", "alt", "]", "\n", "", "elif", "len", "(", "args", ".", "alt_config", ")", "!=", "len", "(", "args", ".", "alt", ")", ":", "\n", "        ", "raise", "EnvException", "(", "\n", "\"Number of partners is different from number of configs\"", ")", "\n", "\n", "# Construct ego config", "\n", "", "if", "'verbose'", "not", "in", "args", ".", "ego_config", ":", "\n", "        ", "args", ".", "ego_config", "[", "'verbose'", "]", "=", "1", "\n", "\n", "", "if", "(", "args", ".", "tensorboard_log", "is", "not", "None", ")", "!=", "(", "args", ".", "tensorboard_name", "is", "not", "None", ")", ":", "\n", "        ", "raise", "EnvException", "(", "\"Must define log and names for tensorboard\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Stanford-ILIAD_PantheonRL.None.trainer.latent_check": [[66, 90], ["all", "trainer.EnvException", "trainer.EnvException", "trainer.EnvException"], "function", ["None"], ["", "", "def", "latent_check", "(", "args", ")", ":", "\n", "# Check for ADAP", "\n", "    ", "all_adap", "=", "all", "(", "[", "v", "in", "ADAP_TYPES", "for", "v", "in", "args", ".", "alt", "]", ")", "\n", "if", "args", ".", "ego", "not", "in", "ADAP_TYPES", "or", "not", "all_adap", ":", "\n", "        ", "raise", "EnvException", "(", "\n", "\"both agents must be ADAP or ADAP_MULT to share latent spaces\"", ")", "\n", "\n", "", "if", "'context_size'", "not", "in", "args", ".", "ego_config", ":", "\n", "        ", "args", ".", "ego_config", "[", "'context_size'", "]", "=", "3", "\n", "", "if", "'context_sampler'", "not", "in", "args", ".", "ego_config", ":", "\n", "        ", "args", ".", "ego_config", "[", "'context_sampler'", "]", "=", "\"l2\"", "\n", "\n", "", "for", "conf", "in", "args", ".", "alt_config", ":", "\n", "        ", "if", "'context_size'", "not", "in", "conf", ":", "\n", "            ", "conf", "[", "'context_size'", "]", "=", "args", ".", "ego_config", "[", "'context_size'", "]", "\n", "", "elif", "conf", "[", "'context_size'", "]", "!=", "args", ".", "ego_config", "[", "'context_size'", "]", ":", "\n", "            ", "raise", "EnvException", "(", "\"both agents must have similar configs \\\n                                to share latent spaces\"", ")", "\n", "\n", "", "if", "'context_sampler'", "not", "in", "conf", ":", "\n", "            ", "conf", "[", "'context_sampler'", "]", "=", "args", ".", "ego_config", "[", "'context_sampler'", "]", "\n", "", "elif", "conf", "[", "'context_sampler'", "]", "!=", "args", ".", "ego_config", "[", "'context_sampler'", "]", ":", "\n", "            ", "raise", "EnvException", "(", "\"both agents must have similar configs \\\n                                to share latent spaces\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Stanford-ILIAD_PantheonRL.None.trainer.generate_env": [[92, 105], ["gym.make", "pantheonrl.common.wrappers.recorder_wrap.getDummyEnv", "pantheonrl.common.wrappers.frame_wrap", "pantheonrl.common.wrappers.frame_wrap", "pantheonrl.common.wrappers.recorder_wrap"], "function", ["home.repos.pwc.inspect_result.Stanford-ILIAD_PantheonRL.common.multiagentenv.MultiAgentEnv.getDummyEnv", "home.repos.pwc.inspect_result.Stanford-ILIAD_PantheonRL.common.wrappers.frame_wrap", "home.repos.pwc.inspect_result.Stanford-ILIAD_PantheonRL.common.wrappers.frame_wrap", "home.repos.pwc.inspect_result.Stanford-ILIAD_PantheonRL.common.wrappers.recorder_wrap"], ["", "", "", "def", "generate_env", "(", "args", ")", ":", "\n", "    ", "env", "=", "gym", ".", "make", "(", "args", ".", "env", ",", "**", "args", ".", "env_config", ")", "\n", "\n", "altenv", "=", "env", ".", "getDummyEnv", "(", "1", ")", "\n", "\n", "if", "args", ".", "framestack", ">", "1", ":", "\n", "        ", "env", "=", "frame_wrap", "(", "env", ",", "args", ".", "framestack", ")", "\n", "altenv", "=", "frame_wrap", "(", "altenv", ",", "args", ".", "framestack", ")", "\n", "\n", "", "if", "args", ".", "record", "is", "not", "None", ":", "\n", "        ", "env", "=", "recorder_wrap", "(", "env", ")", "\n", "\n", "", "return", "env", ",", "altenv", "\n", "\n"]], "home.repos.pwc.inspect_result.Stanford-ILIAD_PantheonRL.None.trainer.generate_ego": [[107, 138], ["trainer.gen_load", "stable_baselines3.common.vec_env.DummyVecEnv", "gen_load.set_env", "gen_load.policy.do_init_weights", "len", "stable_baselines3.PPO", "pantheonrl.algos.adap.adap_learn.ADAP", "stable_baselines3.common.monitor.Monitor", "pantheonrl.algos.adap.adap_learn.ADAP", "dict", "pantheonrl.algos.modular.learn.ModularAlgorithm", "trainer.EnvException", "len"], "function", ["home.repos.pwc.inspect_result.Stanford-ILIAD_PantheonRL.None.trainer.gen_load", "home.repos.pwc.inspect_result.Stanford-ILIAD_PantheonRL.adap.adap_learn.ADAP.set_env", "home.repos.pwc.inspect_result.Stanford-ILIAD_PantheonRL.modular.policies.ModularPolicy.do_init_weights"], ["", "def", "generate_ego", "(", "env", ",", "args", ")", ":", "\n", "    ", "kwargs", "=", "args", ".", "ego_config", "\n", "kwargs", "[", "'env'", "]", "=", "env", "\n", "kwargs", "[", "'device'", "]", "=", "args", ".", "device", "\n", "if", "args", ".", "seed", "is", "not", "None", ":", "\n", "        ", "kwargs", "[", "'seed'", "]", "=", "args", ".", "seed", "\n", "\n", "", "kwargs", "[", "'tensorboard_log'", "]", "=", "args", ".", "tensorboard_log", "\n", "\n", "if", "args", ".", "ego", "==", "'LOAD'", ":", "\n", "        ", "model", "=", "gen_load", "(", "kwargs", ",", "kwargs", "[", "'type'", "]", ",", "kwargs", "[", "'location'", "]", ")", "\n", "# wrap env in Monitor and VecEnv wrapper", "\n", "vec_env", "=", "DummyVecEnv", "(", "[", "lambda", ":", "Monitor", "(", "env", ")", "]", ")", "\n", "model", ".", "set_env", "(", "vec_env", ")", "\n", "if", "kwargs", "[", "'type'", "]", "==", "'ModularAlgorithm'", ":", "\n", "            ", "model", ".", "policy", ".", "do_init_weights", "(", "init_partner", "=", "True", ")", "\n", "model", ".", "policy", ".", "num_partners", "=", "len", "(", "args", ".", "alt", ")", "\n", "", "return", "model", "\n", "", "elif", "args", ".", "ego", "==", "'PPO'", ":", "\n", "        ", "return", "PPO", "(", "policy", "=", "'MlpPolicy'", ",", "**", "kwargs", ")", "\n", "", "elif", "args", ".", "ego", "==", "'ADAP'", ":", "\n", "        ", "return", "ADAP", "(", "policy", "=", "AdapPolicy", ",", "**", "kwargs", ")", "\n", "", "elif", "args", ".", "ego", "==", "'ADAP_MULT'", ":", "\n", "        ", "return", "ADAP", "(", "policy", "=", "AdapPolicyMult", ",", "**", "kwargs", ")", "\n", "", "elif", "args", ".", "ego", "==", "'ModularAlgorithm'", ":", "\n", "        ", "policy_kwargs", "=", "dict", "(", "num_partners", "=", "len", "(", "args", ".", "alt", ")", ")", "\n", "return", "ModularAlgorithm", "(", "policy", "=", "ModularPolicy", ",", "\n", "policy_kwargs", "=", "policy_kwargs", ",", "\n", "**", "kwargs", ")", "\n", "", "else", ":", "\n", "        ", "raise", "EnvException", "(", "\"Not a valid policy\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Stanford-ILIAD_PantheonRL.None.trainer.gen_load": [[140, 158], ["torch.tensor", "pantheonrl.algos.adap.adap_learn.ADAP.load", "pantheonrl.algos.bc.BCShell.policy.set_context", "trainer.EnvException", "config.pop", "stable_baselines3.PPO.load", "pantheonrl.algos.modular.learn.ModularAlgorithm.load", "pantheonrl.algos.bc.BCShell", "trainer.EnvException", "pantheonrl.algos.bc.reconstruct_policy"], "function", ["home.repos.pwc.inspect_result.Stanford-ILIAD_PantheonRL.adap.policies.AdapPolicy.set_context", "home.repos.pwc.inspect_result.Stanford-ILIAD_PantheonRL.algos.bc.reconstruct_policy"], ["", "", "def", "gen_load", "(", "config", ",", "policy_type", ",", "location", ")", ":", "\n", "    ", "if", "policy_type", "in", "ADAP_TYPES", ":", "\n", "        ", "if", "'latent_val'", "not", "in", "config", ":", "\n", "            ", "raise", "EnvException", "(", "\"latent_val needs to be specified for \\\n                                FIXED ADAP policy\"", ")", "\n", "", "latent_val", "=", "th", ".", "tensor", "(", "config", ".", "pop", "(", "'latent_val'", ")", ")", "\n", "agent", "=", "ADAP", ".", "load", "(", "location", ")", "\n", "agent", ".", "policy", ".", "set_context", "(", "latent_val", ")", "\n", "", "elif", "policy_type", "==", "'PPO'", ":", "\n", "        ", "agent", "=", "PPO", ".", "load", "(", "location", ")", "\n", "", "elif", "policy_type", "==", "'ModularAlgorithm'", ":", "\n", "        ", "agent", "=", "ModularAlgorithm", ".", "load", "(", "location", ")", "\n", "", "elif", "policy_type", "==", "'BC'", ":", "\n", "        ", "agent", "=", "BCShell", "(", "reconstruct_policy", "(", "location", ")", ")", "\n", "", "else", ":", "\n", "        ", "raise", "EnvException", "(", "\"Not a valid FIXED/LOAD policy\"", ")", "\n", "\n", "", "return", "agent", "\n", "\n"]], "home.repos.pwc.inspect_result.Stanford-ILIAD_PantheonRL.None.trainer.gen_fixed": [[160, 163], ["trainer.gen_load", "pantheonrl.common.agents.StaticPolicyAgent"], "function", ["home.repos.pwc.inspect_result.Stanford-ILIAD_PantheonRL.None.trainer.gen_load"], ["", "def", "gen_fixed", "(", "config", ",", "policy_type", ",", "location", ")", ":", "\n", "    ", "agent", "=", "gen_load", "(", "config", ",", "policy_type", ",", "location", ")", "\n", "return", "StaticPolicyAgent", "(", "agent", ".", "policy", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Stanford-ILIAD_PantheonRL.None.trainer.gen_default": [[165, 180], ["isinstance", "pantheonrl.envs.rpsgym.rps.RPSWeightedAgent", "trainer.EnvException", "pantheonrl.envs.blockworldgym.simpleblockworld.SBWDefaultAgent", "pantheonrl.envs.blockworldgym.blockworld.DefaultConstructorAgent", "isinstance", "pantheonrl.envs.liargym.liar.LiarDefaultAgent", "trainer.EnvException"], "function", ["None"], ["", "def", "gen_default", "(", "config", ",", "altenv", ")", ":", "\n", "    ", "if", "isinstance", "(", "altenv", ",", "RPSEnv", ")", ":", "\n", "        ", "return", "RPSWeightedAgent", "(", "**", "config", ")", "\n", "\n", "", "if", "config", ":", "\n", "        ", "raise", "EnvException", "(", "\"No config possible for this default agent\"", ")", "\n", "\n", "", "if", "altenv", "==", "simpleblockworld", ".", "PartnerEnv", ":", "\n", "        ", "return", "simpleblockworld", ".", "SBWDefaultAgent", "(", ")", "\n", "", "elif", "altenv", "==", "blockworld", ".", "PartnerEnv", ":", "\n", "        ", "return", "blockworld", ".", "DefaultConstructorAgent", "(", ")", "\n", "", "elif", "isinstance", "(", "altenv", ",", "LiarEnv", ")", ":", "\n", "        ", "return", "LiarDefaultAgent", "(", ")", "\n", "", "else", ":", "\n", "        ", "raise", "EnvException", "(", "\"No default policy available\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Stanford-ILIAD_PantheonRL.None.trainer.gen_partner": [[182, 214], ["pantheonrl.algos.adap.agent.AdapAgent", "trainer.gen_fixed", "pantheonrl.common.agents.OnPolicyAgent", "pantheonrl.algos.adap.adap_learn.ADAP", "trainer.gen_default", "stable_baselines3.PPO", "pantheonrl.algos.adap.adap_learn.ADAP", "trainer.EnvException", "str"], "function", ["home.repos.pwc.inspect_result.Stanford-ILIAD_PantheonRL.None.trainer.gen_fixed", "home.repos.pwc.inspect_result.Stanford-ILIAD_PantheonRL.None.trainer.gen_default"], ["", "", "def", "gen_partner", "(", "type", ",", "config", ",", "altenv", ",", "ego", ",", "args", ")", ":", "\n", "    ", "if", "type", "==", "'FIXED'", ":", "\n", "        ", "return", "gen_fixed", "(", "config", ",", "config", "[", "'type'", "]", ",", "config", "[", "'location'", "]", ")", "\n", "", "elif", "type", "==", "'DEFAULT'", ":", "\n", "        ", "return", "gen_default", "(", "config", ",", "altenv", ")", "\n", "\n", "", "if", "args", ".", "tensorboard_log", "is", "not", "None", ":", "\n", "        ", "agentarg", "=", "{", "\n", "'tensorboard_log'", ":", "args", ".", "tensorboard_log", ",", "\n", "'tb_log_name'", ":", "args", ".", "tensorboard_name", "+", "'_alt_'", "+", "str", "(", "args", ".", "partner_num", ")", "\n", "}", "\n", "", "else", ":", "\n", "        ", "agentarg", "=", "{", "}", "\n", "\n", "", "config", "[", "'env'", "]", "=", "altenv", "\n", "config", "[", "'device'", "]", "=", "args", ".", "device", "\n", "if", "args", ".", "seed", "is", "not", "None", ":", "\n", "        ", "config", "[", "'seed'", "]", "=", "args", ".", "seed", "\n", "", "config", "[", "'verbose'", "]", "=", "args", ".", "verbose_partner", "\n", "\n", "if", "type", "==", "'PPO'", ":", "\n", "        ", "return", "OnPolicyAgent", "(", "PPO", "(", "policy", "=", "'MlpPolicy'", ",", "**", "config", ")", ",", "**", "agentarg", ")", "\n", "\n", "", "if", "type", "==", "'ADAP'", ":", "\n", "        ", "alt", "=", "ADAP", "(", "policy", "=", "AdapPolicy", ",", "**", "config", ")", "\n", "", "elif", "type", "==", "'ADAP_MULT'", ":", "\n", "        ", "alt", "=", "ADAP", "(", "policy", "=", "AdapPolicyMult", ",", "**", "config", ")", "\n", "", "else", ":", "\n", "        ", "raise", "EnvException", "(", "\"Not a valid policy\"", ")", "\n", "\n", "", "shared", "=", "ego", ".", "policy", "if", "args", ".", "share_latent", "else", "None", "\n", "return", "AdapAgent", "(", "alt", ",", "latent_syncer", "=", "shared", ",", "**", "agentarg", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Stanford-ILIAD_PantheonRL.None.trainer.generate_partners": [[216, 229], ["range", "len", "trainer.gen_partner", "print", "env.add_partner_agent", "partners.append"], "function", ["home.repos.pwc.inspect_result.Stanford-ILIAD_PantheonRL.None.trainer.gen_partner", "home.repos.pwc.inspect_result.Stanford-ILIAD_PantheonRL.common.multiagentenv.MultiAgentEnv.add_partner_agent"], ["", "def", "generate_partners", "(", "altenv", ",", "env", ",", "ego", ",", "args", ")", ":", "\n", "    ", "partners", "=", "[", "]", "\n", "for", "i", "in", "range", "(", "len", "(", "args", ".", "alt", ")", ")", ":", "\n", "        ", "args", ".", "partner_num", "=", "i", "\n", "v", "=", "gen_partner", "(", "args", ".", "alt", "[", "i", "]", ",", "\n", "args", ".", "alt_config", "[", "i", "]", ",", "\n", "altenv", ",", "\n", "ego", ",", "\n", "args", ")", "\n", "print", "(", "f'Partner {i}: {v}'", ")", "\n", "env", ".", "add_partner_agent", "(", "v", ")", "\n", "partners", ".", "append", "(", "v", ")", "\n", "", "return", "partners", "\n", "\n"]], "home.repos.pwc.inspect_result.Stanford-ILIAD_PantheonRL.None.trainer.preset": [[231, 257], ["Exception"], "function", ["None"], ["", "def", "preset", "(", "args", ",", "preset_id", ")", ":", "\n", "    ", "'''\n    helpful defaul configuration settings\n    '''", "\n", "\n", "if", "preset_id", "==", "1", ":", "\n", "        ", "env_name", "=", "args", ".", "env", "\n", "if", "'layout_name'", "in", "args", ".", "env_config", ":", "\n", "            ", "env_name", "=", "\"%s-%s\"", "%", "(", "args", ".", "env", ",", "args", ".", "env_config", "[", "'layout_name'", "]", ")", "\n", "\n", "", "if", "args", ".", "tensorboard_log", "is", "None", ":", "\n", "            ", "args", ".", "tensorboard_log", "=", "'logs'", "\n", "", "if", "args", ".", "tensorboard_name", "is", "None", ":", "\n", "            ", "args", ".", "tensorboard_name", "=", "'%s-%s%s-%d'", "%", "(", "\n", "env_name", ",", "args", ".", "ego", ",", "args", ".", "alt", "[", "0", "]", ",", "args", ".", "seed", ")", "\n", "", "if", "args", ".", "ego_save", "is", "None", ":", "\n", "            ", "args", ".", "ego_save", "=", "'models/%s-%s-ego-%d'", "%", "(", "\n", "env_name", ",", "args", ".", "ego", ",", "args", ".", "seed", ")", "\n", "", "if", "args", ".", "alt_save", "is", "None", ":", "\n", "            ", "args", ".", "alt_save", "=", "'models/%s-%s-alt-%d'", "%", "(", "\n", "env_name", ",", "args", ".", "alt", "[", "0", "]", ",", "args", ".", "seed", ")", "\n", "# if not args.record:", "\n", "#     args.record = 'trajs/%s-%s%s-%d' % (env_name, args.ego, args.alt[0], args.seed)", "\n", "", "", "else", ":", "\n", "        ", "raise", "Exception", "(", "\"Invalid preset id\"", ")", "\n", "", "return", "args", "\n", "\n"]], "home.repos.pwc.inspect_result.Stanford-ILIAD_PantheonRL.None.tester.input_check": [[14, 32], ["trainer.EnvException", "trainer.EnvException", "trainer.EnvException", "trainer.EnvException"], "function", ["None"], ["def", "input_check", "(", "args", ")", ":", "\n", "# Env checking", "\n", "    ", "if", "args", ".", "env", "==", "'OvercookedMultiEnv-v0'", ":", "\n", "        ", "if", "'layout_name'", "not", "in", "args", ".", "env_config", ":", "\n", "            ", "raise", "EnvException", "(", "f\"layout_name needed for {args.env}\"", ")", "\n", "", "elif", "args", ".", "env_config", "[", "'layout_name'", "]", "not", "in", "LAYOUT_LIST", ":", "\n", "            ", "raise", "EnvException", "(", "\n", "f\"{args.env_config['layout_name']} is not a valid layout\"", ")", "\n", "\n", "# Construct ego config", "\n", "", "", "if", "'verbose'", "not", "in", "args", ".", "ego_config", ":", "\n", "        ", "args", ".", "ego_config", "[", "'verbose'", "]", "=", "1", "\n", "\n", "", "if", "args", ".", "ego_load", "is", "None", ":", "\n", "        ", "raise", "EnvException", "(", "\"Need to provide file for ego to load\"", ")", "\n", "\n", "", "if", "(", "args", ".", "alt_load", "is", "None", ")", "!=", "(", "args", ".", "alt", "==", "'DEFAULT'", ")", ":", "\n", "        ", "raise", "EnvException", "(", "\"Load policy if and only if alt is not DEFAULT\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Stanford-ILIAD_PantheonRL.None.tester.generate_agent": [[34, 39], ["trainer.gen_fixed", "trainer.gen_default"], "function", ["home.repos.pwc.inspect_result.Stanford-ILIAD_PantheonRL.None.trainer.gen_fixed", "home.repos.pwc.inspect_result.Stanford-ILIAD_PantheonRL.None.trainer.gen_default"], ["", "", "def", "generate_agent", "(", "env", ",", "policy_type", ",", "config", ",", "location", ")", ":", "\n", "    ", "if", "policy_type", "==", "'DEFAULT'", ":", "\n", "        ", "return", "gen_default", "(", "config", ",", "env", ")", "\n", "\n", "", "return", "gen_fixed", "(", "config", ",", "policy_type", ",", "location", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Stanford-ILIAD_PantheonRL.None.tester.run_test": [[41, 63], ["range", "env.close", "print", "print", "env.reset", "rewards.append", "env.render", "ego.get_action", "env.step", "env.render", "time.sleep", "numpy.std", "sum"], "function", ["home.repos.pwc.inspect_result.Stanford-ILIAD_PantheonRL.common.multiagentenv.MultiAgentEnv.reset", "home.repos.pwc.inspect_result.Stanford-ILIAD_PantheonRL.overcookedgym.overcooked.OvercookedMultiEnv.render", "home.repos.pwc.inspect_result.Stanford-ILIAD_PantheonRL.common.agents.RecordingAgentWrapper.get_action", "home.repos.pwc.inspect_result.Stanford-ILIAD_PantheonRL.common.multiagentenv.MultiAgentEnv.step", "home.repos.pwc.inspect_result.Stanford-ILIAD_PantheonRL.overcookedgym.overcooked.OvercookedMultiEnv.render"], ["", "def", "run_test", "(", "ego", ",", "env", ",", "num_episodes", ",", "render", "=", "False", ")", ":", "\n", "    ", "rewards", "=", "[", "]", "\n", "for", "game", "in", "range", "(", "num_episodes", ")", ":", "\n", "        ", "obs", "=", "env", ".", "reset", "(", ")", "\n", "done", "=", "False", "\n", "reward", "=", "0", "\n", "if", "render", ":", "\n", "            ", "env", ".", "render", "(", ")", "\n", "", "while", "not", "done", ":", "\n", "            ", "action", "=", "ego", ".", "get_action", "(", "obs", ",", "False", ")", "\n", "obs", ",", "newreward", ",", "done", ",", "_", "=", "env", ".", "step", "(", "action", ")", "\n", "reward", "+=", "newreward", "\n", "\n", "if", "render", ":", "\n", "                ", "env", ".", "render", "(", ")", "\n", "sleep", "(", "1", "/", "60", ")", "\n", "\n", "", "", "rewards", ".", "append", "(", "reward", ")", "\n", "\n", "", "env", ".", "close", "(", ")", "\n", "print", "(", "f\"Average Reward: {sum(rewards)/num_episodes}\"", ")", "\n", "print", "(", "f\"Standard Deviation: {np.std(rewards)}\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Stanford-ILIAD_PantheonRL.website.data_processing.savedpartnerpath": [[16, 18], ["None"], "function", ["None"], ["def", "savedpartnerpath", "(", "id", ",", "env", ",", "name", ",", "ptype", ")", ":", "\n", "    ", "return", "f\"./data/user{id}/{env}/fixedpartners/{ptype}/{name}\"", "\n", "\n"]], "home.repos.pwc.inspect_result.Stanford-ILIAD_PantheonRL.website.data_processing.loadpartnerpath": [[19, 21], ["None"], "function", ["None"], ["", "def", "loadpartnerpath", "(", "id", ",", "env", ")", ":", "\n", "    ", "return", "f\"./data/user{id}/{env}/fixedpartners\"", "\n", "\n"]], "home.repos.pwc.inspect_result.Stanford-ILIAD_PantheonRL.website.data_processing.savedegopath": [[22, 24], ["None"], "function", ["None"], ["", "def", "savedegopath", "(", "id", ",", "env", ",", "name", ",", "ptype", ")", ":", "\n", "    ", "return", "f\"./data/user{id}/{env}/fixedego/{ptype}/{name}\"", "\n", "\n"]], "home.repos.pwc.inspect_result.Stanford-ILIAD_PantheonRL.website.data_processing.fixedpartneroptions": [[25, 38], ["data_processing.loadpartnerpath", "os.path.isdir", "os.path.isdir", "os.path.join", "os.path.join", "f.replace", "os.listdir", "os.path.isfile", "os.path.join"], "function", ["home.repos.pwc.inspect_result.Stanford-ILIAD_PantheonRL.website.data_processing.loadpartnerpath"], ["", "def", "fixedpartneroptions", "(", "id", ",", "env", ")", ":", "\n", "    ", "mypath", "=", "loadpartnerpath", "(", "id", ",", "env", ")", "\n", "if", "not", "os", ".", "path", ".", "isdir", "(", "mypath", ")", ":", "\n", "        ", "return", "\"You have not saved any fixed partners for this environment. Please select a different partner type.\"", ",", "[", "]", "\n", "", "else", ":", "\n", "        ", "options", "=", "[", "]", "\n", "for", "ptype", "in", "PARTNER_LIST", ":", "\n", "# TODO: add a better fix later", "\n", "            ", "if", "not", "ptype", "in", "ADAP_TYPES", ":", "\n", "                ", "if", "os", ".", "path", ".", "isdir", "(", "join", "(", "mypath", ",", "ptype", ")", ")", ":", "\n", "                    ", "currpath", "=", "join", "(", "mypath", ",", "ptype", ")", "\n", "options", "+=", "[", "(", "f", ".", "replace", "(", "'.zip'", ",", "''", ")", ",", "ptype", ")", "for", "f", "in", "listdir", "(", "currpath", ")", "if", "isfile", "(", "join", "(", "currpath", ",", "f", ")", ")", "]", "\n", "", "", "", "return", "None", ",", "options", "\n", "\n"]], "home.repos.pwc.inspect_result.Stanford-ILIAD_PantheonRL.website.data_processing.common_env_configs": [[39, 51], ["int", "int.isnumeric"], "function", ["None"], ["", "", "def", "common_env_configs", "(", "args", ",", "id", ")", ":", "\n", "    ", "record", "=", "None", "\n", "if", "\"record\"", "in", "args", ":", "\n", "        ", "record", "=", "f\"./data/user{id}traj.json\"", "\n", "", "error", "=", "None", "\n", "framestack", "=", "args", "[", "\"framestack\"", "]", "\n", "if", "not", "framestack", "or", "not", "framestack", ".", "isnumeric", "(", ")", ":", "\n", "        ", "error", "=", "\"Please enter a valid integer for the number of frames stacked.\"", "\n", "", "else", ":", "\n", "        ", "framestack", "=", "int", "(", "framestack", ")", "\n", "\n", "", "return", "record", ",", "framestack", ",", "error", "\n", "\n"]], "home.repos.pwc.inspect_result.Stanford-ILIAD_PantheonRL.website.data_processing.create_args_object": [[52, 55], ["collections.namedtuple", "collections.namedtuple."], "function", ["None"], ["", "def", "create_args_object", "(", "env_args", ")", ":", "\n", "    ", "Config", "=", "namedtuple", "(", "\"Config\"", ",", "[", "\"env\"", ",", "\"env_config\"", ",", "\"record\"", ",", "\"framestack\"", "]", ")", "\n", "return", "Config", "(", "env_args", "[", "\"env\"", "]", ",", "env_args", "[", "\"env_config\"", "]", ",", "env_args", "[", "\"record\"", "]", ",", "env_args", "[", "\"framestack\"", "]", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Stanford-ILIAD_PantheonRL.website.data_processing.create_args_dict": [[56, 59], ["None"], "function", ["None"], ["", "def", "create_args_dict", "(", "env_name", ",", "env_config", ",", "record", ",", "framestack", ")", ":", "\n", "    ", "args", "=", "{", "\"env\"", ":", "env_name", ",", "\"env_config\"", ":", "env_config", ",", "\"record\"", ":", "record", ",", "\"framestack\"", ":", "framestack", "}", "\n", "return", "args", "\n", "\n"]], "home.repos.pwc.inspect_result.Stanford-ILIAD_PantheonRL.website.data_processing.saveenvs": [[60, 65], ["numpy.savez", "os.path.exists", "os.makedirs"], "function", ["None"], ["", "def", "saveenvs", "(", "env", ",", "alt_env", ",", "id", ")", ":", "\n", "    ", "if", "not", "os", ".", "path", ".", "exists", "(", "'data'", ")", ":", "\n", "        ", "os", ".", "makedirs", "(", "'data'", ")", "\n", "\n", "", "np", ".", "savez", "(", "f\"./data/user{id}.npz\"", ",", "env", "=", "env", ",", "alt_env", "=", "alt_env", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Stanford-ILIAD_PantheonRL.website.data_processing.create_ego_dict": [[66, 82], ["int", "data_processing.savedegopath", "int", "args[].isalnum", "os.path.exists"], "function", ["home.repos.pwc.inspect_result.Stanford-ILIAD_PantheonRL.website.data_processing.savedegopath"], ["", "def", "create_ego_dict", "(", "ego_type", ",", "args", ",", "env", ",", "id", ")", ":", "\n", "    ", "seed", "=", "None", "\n", "error", "=", "None", "\n", "location", "=", "None", "\n", "if", "\"seed\"", "in", "args", "and", "args", "[", "'seed'", "]", "!=", "\"\"", ":", "\n", "        ", "seed", "=", "int", "(", "args", "[", "\"seed\"", "]", ")", "\n", "", "if", "\"egoname\"", "in", "args", "and", "args", "[", "'egoname'", "]", "!=", "\"\"", ":", "\n", "        ", "location", "=", "savedegopath", "(", "id", ",", "ENV_TO_NAME", "[", "env", "]", ",", "args", "[", "'egoname'", "]", ",", "ego_type", ")", "\n", "if", "not", "args", "[", "'egoname'", "]", ".", "isalnum", "(", ")", ":", "\n", "            ", "error", "=", "\"Name to save as must be alphanumeric.\"", "\n", "", "elif", "os", ".", "path", ".", "exists", "(", "f\"{location}.zip\"", ")", ":", "\n", "            ", "error", "=", "\"There already exists an ego agent saved with that name.\"", "\n", "\n", "", "", "ego_dict", "=", "{", "\"type\"", ":", "ego_type", ",", "\"seed\"", ":", "seed", ",", "\"timesteps\"", ":", "int", "(", "args", "[", "\"timesteps\"", "]", ")", ",", "\"location\"", ":", "location", "}", "\n", "\n", "return", "error", ",", "ego_dict", "\n", "\n"]], "home.repos.pwc.inspect_result.Stanford-ILIAD_PantheonRL.website.data_processing.create_partner_dict": [[83, 118], ["int", "int", "int", "data_processing.loadpartnerpath", "args[].isnumeric", "args[].isnumeric", "args[].isnumeric", "os.path.isdir", "os.path.join", "int", "args[].find", "args[].isalnum", "os.path.exists", "data_processing.savedpartnerpath"], "function", ["home.repos.pwc.inspect_result.Stanford-ILIAD_PantheonRL.website.data_processing.loadpartnerpath", "home.repos.pwc.inspect_result.Stanford-ILIAD_PantheonRL.website.data_processing.savedpartnerpath"], ["", "def", "create_partner_dict", "(", "id", ",", "partner_type", ",", "env", ",", "args", ")", ":", "\n", "    ", "error", "=", "None", "\n", "partner_dict", "=", "{", "\"type\"", ":", "partner_type", "}", "\n", "\n", "if", "env", "==", "\"rps\"", "and", "partner_type", "==", "\"DEFAULT\"", ":", "\n", "        ", "if", "not", "(", "'r'", "in", "args", "and", "'p'", "in", "args", "and", "'s'", "in", "args", ")", ":", "\n", "            ", "error", "=", "\"You must enter the probabilities of rock, paper, and scissors for the RPS Default Agent.\"", "\n", "", "if", "not", "(", "args", "[", "'r'", "]", ".", "isnumeric", "(", ")", "and", "args", "[", "'s'", "]", ".", "isnumeric", "(", ")", "and", "args", "[", "'p'", "]", ".", "isnumeric", "(", ")", ")", ":", "\n", "            ", "error", "=", "\"Please enter valid numbers for each probability.\"", "\n", "", "r", ",", "p", ",", "s", "=", "int", "(", "args", "[", "'r'", "]", ")", ",", "int", "(", "args", "[", "'p'", "]", ")", ",", "int", "(", "args", "[", "'s'", "]", ")", "\n", "if", "not", "(", "r", ">=", "0", "and", "s", ">=", "0", "and", "p", ">=", "0", ")", ":", "\n", "            ", "error", "=", "\"All probabilities must be nonnegative.\"", "\n", "", "partner_dict", "[", "'r'", "]", ",", "partner_dict", "[", "'p'", "]", ",", "partner_dict", "[", "'s'", "]", "=", "r", ",", "p", ",", "s", "\n", "", "elif", "partner_type", "==", "\"FIXED\"", ":", "\n", "        ", "mypath", "=", "loadpartnerpath", "(", "id", ",", "ENV_TO_NAME", "[", "env", "]", ")", "\n", "if", "not", "os", ".", "path", ".", "isdir", "(", "mypath", ")", ":", "\n", "            ", "error", "=", "\"You do not have any fixed partners for this environment. Please select a different partner type.\"", "\n", "", "else", ":", "\n", "            ", "partner_dict", "[", "'location'", "]", "=", "join", "(", "mypath", ",", "args", "[", "'fixedtype'", "]", ")", "\n", "partner_dict", "[", "'ptype'", "]", "=", "args", "[", "'fixedtype'", "]", "[", ":", "args", "[", "'fixedtype'", "]", ".", "find", "(", "'/'", ")", "]", "\n", "", "", "elif", "partner_type", "!=", "\"DEFAULT\"", ":", "\n", "        ", "seed", "=", "None", "\n", "if", "\"seed\"", "in", "args", "and", "args", "[", "'seed'", "]", "!=", "\"\"", ":", "\n", "            ", "seed", "=", "int", "(", "args", "[", "\"seed\"", "]", ")", "\n", "", "partner_dict", "[", "'seed'", "]", "=", "seed", "\n", "save", "=", "None", "\n", "if", "\"partnername\"", "in", "args", "and", "args", "[", "'partnername'", "]", "!=", "\"\"", ":", "\n", "            ", "if", "not", "args", "[", "'partnername'", "]", ".", "isalnum", "(", ")", ":", "\n", "                ", "error", "=", "\"Name to save as must be alphanumeric.\"", "\n", "", "elif", "os", ".", "path", ".", "exists", "(", "f\"{savedpartnerpath(id, env, args['partnername'], partner_type)}.zip\"", ")", ":", "\n", "                ", "error", "=", "\"There already exists a partner saved with that name.\"", "\n", "", "save", "=", "args", "[", "'partnername'", "]", "\n", "", "partner_dict", "[", "'save'", "]", "=", "save", "\n", "\n", "", "return", "error", ",", "partner_dict", "\n", "\n"]], "home.repos.pwc.inspect_result.Stanford-ILIAD_PantheonRL.website.data_processing.check_agent_errors": [[119, 142], ["datetime.datetime.now().strftime", "errors.append", "len", "len", "errors.append", "partners.pop", "datetime.datetime.now", "errors.append", "partners.pop", "os.path.isfile", "len", "len"], "function", ["None"], ["", "def", "check_agent_errors", "(", "id", ",", "env", ",", "ego", ",", "partners", ")", ":", "\n", "    ", "errors", "=", "[", "]", "\n", "if", "ego", "is", "None", "or", "len", "(", "partners", ")", "<", "1", ":", "\n", "        ", "errors", ".", "append", "(", "\"Need at least one valid ego agent and partner agent to train with.\"", ")", "\n", "\n", "", "i", "=", "0", "\n", "while", "i", "<", "len", "(", "partners", ")", ":", "\n", "        ", "if", "partners", "[", "i", "]", "[", "\"type\"", "]", "==", "\"FIXED\"", "and", "(", "not", "'location'", "in", "partners", "[", "i", "]", "or", "not", "'ptype'", "in", "partners", "[", "i", "]", ")", ":", "\n", "            ", "errors", ".", "append", "(", "f\"Fixed agents need a filename to load from. Partner {i + len(errors)} will be deleted.\\n\"", ")", "\n", "partners", ".", "pop", "(", "i", ")", "\n", "i", "-=", "1", "\n", "", "elif", "partners", "[", "i", "]", "[", "\"type\"", "]", "==", "\"FIXED\"", "and", "not", "os", ".", "path", ".", "isfile", "(", "partners", "[", "i", "]", "[", "'location'", "]", ")", ":", "\n", "# honestly if it reaches here it's probably my fault and not the users", "\n", "            ", "errors", ".", "append", "(", "f\"Fixed agents need a valid filepath. Partner {i + len(errors)} will be deleted.\\n\"", ")", "\n", "partners", ".", "pop", "(", "i", ")", "\n", "i", "-=", "1", "\n", "", "i", "+=", "1", "\n", "\n", "", "time", "=", "datetime", ".", "datetime", ".", "now", "(", ")", ".", "strftime", "(", "\"%Y%m%d-%H%M%S\"", ")", "\n", "tensorboard_log", "=", "f\"./data/user{id}logs\"", "\n", "tensorboard_name", "=", "f\"user{id}-{time}\"", "\n", "\n", "return", "errors", ",", "partners", ",", "tensorboard_log", ",", "tensorboard_name", ",", "time", "\n", "\n"]], "home.repos.pwc.inspect_result.Stanford-ILIAD_PantheonRL.website.data_processing.create_ego_object": [[143, 149], ["collections.namedtuple", "collections.namedtuple."], "function", ["None"], ["", "def", "create_ego_object", "(", "ego_data", ",", "num_partners", ",", "tensorboard_log", ")", ":", "\n", "    ", "ego_config", "=", "{", "\"verbose\"", ":", "1", "}", "\n", "alt", "=", "[", "0", "]", "*", "num_partners", "\n", "device", "=", "\"auto\"", "\n", "Ego", "=", "namedtuple", "(", "\"Ego\"", ",", "[", "\"ego_config\"", ",", "\"tensorboard_log\"", ",", "\"alt\"", ",", "\"device\"", ",", "\"seed\"", ",", "\"ego\"", "]", ")", "\n", "return", "Ego", "(", "ego_config", ",", "tensorboard_log", ",", "alt", ",", "device", ",", "ego_data", "[", "'seed'", "]", ",", "ego_data", "[", "'type'", "]", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Stanford-ILIAD_PantheonRL.website.data_processing.create_partner_object": [[150, 153], ["collections.namedtuple", "collections.namedtuple."], "function", ["None"], ["", "def", "create_partner_object", "(", "seed", ",", "tensorboard_log", ",", "tensorboard_name", ",", "partner_num", ")", ":", "\n", "    ", "Partner", "=", "namedtuple", "(", "\"Partner\"", ",", "[", "\"seed\"", ",", "\"device\"", ",", "\"share_latent\"", ",", "\"tensorboard_log\"", ",", "\"tensorboard_name\"", ",", "\"partner_num\"", ",", "\"verbose_partner\"", "]", ")", "\n", "return", "Partner", "(", "seed", ",", "\"auto\"", ",", "False", ",", "tensorboard_log", ",", "tensorboard_name", ",", "partner_num", ",", "False", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Stanford-ILIAD_PantheonRL.website.data_processing.start_training": [[154, 201], ["ego_data.pop", "data_processing.create_args_object", "trainer.generate_env", "trainer.generate_ego", "enumerate", "trainer.generate_ego.learn", "mydatabase.execute", "mydatabase.commit", "data_processing.create_ego_object", "partner.pop", "env.add_partner_agent", "env.get_transitions", "env.get_transitions.write_transition", "partner.model.save", "trainer.generate_ego.save", "len", "partner.pop", "partner.pop", "trainer.gen_fixed", "data_processing.create_partner_object", "trainer.gen_partner", "partners_to_save.append", "data_processing.savedpartnerpath", "partner.pop", "partner.pop", "copy.deepcopy"], "function", ["home.repos.pwc.inspect_result.Stanford-ILIAD_PantheonRL.website.data_processing.create_args_object", "home.repos.pwc.inspect_result.Stanford-ILIAD_PantheonRL.None.trainer.generate_env", "home.repos.pwc.inspect_result.Stanford-ILIAD_PantheonRL.None.trainer.generate_ego", "home.repos.pwc.inspect_result.Stanford-ILIAD_PantheonRL.common.agents.OffPolicyAgent.learn", "home.repos.pwc.inspect_result.Stanford-ILIAD_PantheonRL.website.data_processing.create_ego_object", "home.repos.pwc.inspect_result.Stanford-ILIAD_PantheonRL.common.multiagentenv.MultiAgentEnv.add_partner_agent", "home.repos.pwc.inspect_result.Stanford-ILIAD_PantheonRL.common.agents.RecordingAgentWrapper.get_transitions", "home.repos.pwc.inspect_result.Stanford-ILIAD_PantheonRL.common.trajsaver.SimultaneousTransitions.write_transition", "home.repos.pwc.inspect_result.Stanford-ILIAD_PantheonRL.None.trainer.gen_fixed", "home.repos.pwc.inspect_result.Stanford-ILIAD_PantheonRL.website.data_processing.create_partner_object", "home.repos.pwc.inspect_result.Stanford-ILIAD_PantheonRL.None.trainer.gen_partner", "home.repos.pwc.inspect_result.Stanford-ILIAD_PantheonRL.website.data_processing.savedpartnerpath"], ["", "def", "start_training", "(", "id", ",", "env_data", ",", "ego_data", ",", "partners", ",", "tensorboard_log", ",", "tensorboard_name", ",", "mydatabase", ")", ":", "\n", "    ", "ego_save", "=", "ego_data", ".", "pop", "(", "\"location\"", ")", "\n", "env_args", "=", "create_args_object", "(", "env_data", ")", "\n", "env", ",", "alt_env", "=", "generate_env", "(", "env_args", ")", "\n", "\n", "ego_agent", "=", "generate_ego", "(", "env", ",", "create_ego_object", "(", "ego_data", ",", "len", "(", "partners", ")", ",", "tensorboard_log", ")", ")", "\n", "\n", "partners_to_save", "=", "[", "]", "\n", "for", "i", ",", "partner", "in", "enumerate", "(", "partners", ")", ":", "\n", "        ", "ptype", "=", "partner", ".", "pop", "(", "\"type\"", ")", "\n", "seed", "=", "None", "\n", "if", "\"seed\"", "in", "partner", ":", "\n", "            ", "seed", "=", "partner", ".", "pop", "(", "\"seed\"", ")", "\n", "", "save", "=", "None", "\n", "if", "\"save\"", "in", "partner", ":", "\n", "            ", "save", "=", "partner", ".", "pop", "(", "\"save\"", ")", "\n", "#TODO: adap agents can't load, fix this later", "\n", "", "if", "ptype", "==", "\"FIXED\"", ":", "\n", "            ", "current_partner", "=", "gen_fixed", "(", "{", "}", ",", "partner", ".", "pop", "(", "'ptype'", ")", ",", "partner", ".", "pop", "(", "'location'", ")", ")", "\n", "", "else", ":", "\n", "            ", "p_args", "=", "create_partner_object", "(", "seed", ",", "tensorboard_log", ",", "tensorboard_name", ",", "i", ")", "\n", "current_partner", "=", "gen_partner", "(", "ptype", ",", "copy", ".", "deepcopy", "(", "partner", ")", ",", "alt_env", ",", "ego_agent", ",", "p_args", ")", "\n", "", "env", ".", "add_partner_agent", "(", "current_partner", ")", "\n", "if", "save", "is", "not", "None", ":", "\n", "            ", "partners_to_save", ".", "append", "(", "(", "current_partner", ",", "save", ",", "ptype", ")", ")", "\n", "\n", "", "", "learn_config", "=", "{", "'total_timesteps'", ":", "ego_data", "[", "\"timesteps\"", "]", "}", "\n", "if", "tensorboard_log", "is", "not", "None", ":", "\n", "        ", "learn_config", "[", "'tb_log_name'", "]", "=", "tensorboard_name", "\n", "", "ego_agent", ".", "learn", "(", "**", "learn_config", ")", "\n", "\n", "if", "env_data", "[", "\"record\"", "]", "is", "not", "None", ":", "\n", "        ", "transition", "=", "env", ".", "get_transitions", "(", ")", "\n", "transition", ".", "write_transition", "(", "env_data", "[", "\"record\"", "]", ")", "\n", "\n", "", "for", "partner", ",", "name", ",", "ptype", "in", "partners_to_save", ":", "\n", "        ", "partner", ".", "model", ".", "save", "(", "savedpartnerpath", "(", "id", ",", "env_data", "[", "'env'", "]", ",", "name", ",", "ptype", ")", ")", "\n", "\n", "", "if", "ego_save", "is", "not", "None", ":", "\n", "        ", "ego_agent", ".", "save", "(", "ego_save", ")", "\n", "\n", "", "mydatabase", ".", "execute", "(", "\n", "'UPDATE user SET running = ?'", "\n", "' WHERE id = ?'", ",", "\n", "(", "False", ",", "id", ")", "\n", ")", "\n", "mydatabase", ".", "commit", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Stanford-ILIAD_PantheonRL.website.data_processing.check_trained": [[202, 205], ["os.path.isdir"], "function", ["None"], ["", "def", "check_trained", "(", "log", ",", "name", ")", ":", "\n", "    ", "mypath", "=", "f\"{log}/{name}_1\"", "\n", "return", "os", ".", "path", ".", "isdir", "(", "mypath", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Stanford-ILIAD_PantheonRL.website.data_processing.create_dir": [[206, 210], ["os.listdir", "os.path.isfile", "os.path.join"], "function", ["None"], ["", "def", "create_dir", "(", "tensorboard_log", ",", "tensorboard_name", ")", ":", "\n", "    ", "mypath", "=", "f\"{tensorboard_log}/{tensorboard_name}_1\"", "\n", "onlyfiles", "=", "[", "f", "for", "f", "in", "listdir", "(", "mypath", ")", "if", "isfile", "(", "join", "(", "mypath", ",", "f", ")", ")", "]", "\n", "return", "mypath", ",", "onlyfiles", "\n", "\n"]], "home.repos.pwc.inspect_result.Stanford-ILIAD_PantheonRL.website.data_processing.read": [[211, 222], ["tensorflow.compat.v1.train.summary_iterator", "os.path.isdir", "os.path.join", "os.listdir", "os.path.isfile", "os.path.join", "len"], "function", ["None"], ["", "def", "read", "(", "tensorboard_log", ",", "tensorboard_name", ")", ":", "\n", "    ", "mypath", "=", "f\"{tensorboard_log}/{tensorboard_name}_1\"", "\n", "if", "not", "os", ".", "path", ".", "isdir", "(", "mypath", ")", ":", "\n", "        ", "return", "{", "}", ",", "\"Please start training before checking for updates.\"", "\n", "", "onlyfiles", "=", "[", "f", "for", "f", "in", "listdir", "(", "mypath", ")", "if", "isfile", "(", "join", "(", "mypath", ",", "f", ")", ")", "]", "\n", "summaries", "=", "tf", ".", "compat", ".", "v1", ".", "train", ".", "summary_iterator", "(", "join", "(", "mypath", ",", "onlyfiles", "[", "len", "(", "onlyfiles", ")", "-", "1", "]", ")", ")", "\n", "mydict", "=", "{", "}", "\n", "for", "e", "in", "summaries", ":", "\n", "        ", "for", "v", "in", "e", ".", "summary", ".", "value", ":", "\n", "            ", "mydict", "[", "v", ".", "tag", "]", "=", "v", ".", "simple_value", "\n", "", "", "return", "mydict", ",", "None", "\n", "\n"]], "home.repos.pwc.inspect_result.Stanford-ILIAD_PantheonRL.website.data_processing.check_for_updates": [[223, 227], ["None"], "function", ["None"], ["", "def", "check_for_updates", "(", "file", ")", ":", "\n", "# a function that would theoretically check for updates from the learning agent", "\n", "# right now it just returns a test json", "\n", "    ", "return", "{", "\"updates\"", ":", "\"In terms of updates, we have no updates.\"", "}", "\n", "\n"]], "home.repos.pwc.inspect_result.Stanford-ILIAD_PantheonRL.website.data_processing.gen_tensorboard": [[228, 239], ["subprocess.Popen", "os.path.isdir", "subprocess.Popen.communicate", "print"], "function", ["None"], ["", "def", "gen_tensorboard", "(", "tb_log", ",", "tb_name", ")", ":", "\n", "    ", "mypath", "=", "f\"{tb_log}/{tb_name}_1\"", "\n", "if", "not", "os", ".", "path", ".", "isdir", "(", "mypath", ")", ":", "\n", "        ", "return", "None", ",", "\"Please start training before generating the tensorboard.\"", "\n", "", "p", "=", "subprocess", ".", "Popen", "(", "[", "\"tensorboard\"", ",", "\"--logdir\"", ",", "mypath", ",", "\"--bind_all\"", ",", "\"--port\"", ",", "TB_PORT", "]", ",", "stderr", "=", "subprocess", ".", "PIPE", ")", "\n", "try", ":", "\n", "        ", "outs", ",", "errs", "=", "p", ".", "communicate", "(", "timeout", "=", "10", ")", "\n", "", "except", "subprocess", ".", "TimeoutExpired", ":", "\n", "        ", "print", "(", "\"timeout expired\"", ")", "\n", "return", "p", ".", "pid", ",", "None", "\n", "", "return", "None", ",", "f\"Tensorboard could not bind to port {TB_PORT} as it was already in use. Tensorboard not created.\"", "\n", "\n"]], "home.repos.pwc.inspect_result.Stanford-ILIAD_PantheonRL.website.data_processing.stop_tensorboard": [[240, 245], ["os.kill"], "function", ["None"], ["", "def", "stop_tensorboard", "(", "processid", ")", ":", "\n", "    ", "try", ":", "\n", "        ", "os", ".", "kill", "(", "processid", ",", "signal", ".", "SIGINT", ")", "\n", "", "except", "OSError", ":", "\n", "        ", "pass", "\n", "", "", ""]], "home.repos.pwc.inspect_result.Stanford-ILIAD_PantheonRL.website.login.main": [[13, 55], ["bp.route", "flask.render_template", "website.db.get_db", "flask.flash", "website.db.get_db.execute().fetchone", "flask.session.clear", "flask.redirect", "website.db.get_db.execute", "website.db.get_db.commit", "flask.url_for", "website.db.get_db.execute", "werkzeug.security.check_password_hash", "werkzeug.security.generate_password_hash"], "function", ["home.repos.pwc.inspect_result.Stanford-ILIAD_PantheonRL.website.db.get_db"], ["@", "bp", ".", "route", "(", "'/'", ",", "methods", "=", "(", "'GET'", ",", "'POST'", ")", ")", "\n", "def", "main", "(", ")", ":", "\n", "    ", "if", "request", ".", "method", "==", "'POST'", ":", "\n", "        ", "username", "=", "request", ".", "form", "[", "'username'", "]", "\n", "password", "=", "request", ".", "form", "[", "'password'", "]", "\n", "db", "=", "get_db", "(", ")", "\n", "error", "=", "None", "\n", "\n", "if", "not", "username", ":", "\n", "            ", "error", "=", "'Username is required.'", "\n", "", "elif", "not", "password", ":", "\n", "            ", "error", "=", "'Password is required.'", "\n", "\n", "", "if", "error", "is", "None", ":", "\n", "# first tries to insert the user into the database", "\n", "            ", "try", ":", "\n", "                ", "db", ".", "execute", "(", "\n", "\"INSERT INTO user (username, password, running) VALUES (?, ?, ?)\"", ",", "\n", "(", "username", ",", "generate_password_hash", "(", "password", ")", ",", "False", ")", ",", "\n", ")", "\n", "db", ".", "commit", "(", ")", "\n", "", "except", "db", ".", "IntegrityError", ":", "\n", "                ", "pass", "\n", "\n", "# whether or not that works, try to get the user from the database", "\n", "", "user", "=", "db", ".", "execute", "(", "\n", "\"SELECT * FROM user WHERE USERNAME = ?\"", ",", "(", "username", ",", ")", "\n", ")", ".", "fetchone", "(", ")", "\n", "\n", "if", "user", "is", "None", ":", "\n", "                ", "error", "=", "'Incorrect username.'", "\n", "", "elif", "not", "check_password_hash", "(", "user", "[", "'password'", "]", ",", "password", ")", ":", "\n", "                ", "error", "=", "'Incorrect password.'", "\n", "\n", "", "", "if", "error", "is", "None", ":", "\n", "            ", "session", ".", "clear", "(", ")", "\n", "session", "[", "'user_id'", "]", "=", "user", "[", "'id'", "]", "\n", "return", "redirect", "(", "url_for", "(", "'welcome.main'", ")", ")", "\n", "\n", "", "flash", "(", "error", ")", "\n", "\n", "", "return", "render_template", "(", "'login.html'", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Stanford-ILIAD_PantheonRL.website.login.logout": [[56, 60], ["bp.route", "flask.session.clear", "flask.redirect", "flask.url_for"], "function", ["None"], ["", "@", "bp", ".", "route", "(", "'/logout'", ")", "\n", "def", "logout", "(", ")", ":", "\n", "    ", "session", ".", "clear", "(", ")", "\n", "return", "redirect", "(", "url_for", "(", "'login.main'", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Stanford-ILIAD_PantheonRL.website.login.load_logged_in_user": [[61, 70], ["flask.session.get", "website.db.get_db().execute().fetchone", "website.db.get_db().execute", "website.db.get_db"], "function", ["home.repos.pwc.inspect_result.Stanford-ILIAD_PantheonRL.website.db.get_db"], ["", "@", "bp", ".", "before_app_request", "\n", "def", "load_logged_in_user", "(", ")", ":", "\n", "    ", "user_id", "=", "session", ".", "get", "(", "'user_id'", ")", "\n", "\n", "if", "user_id", "is", "None", ":", "\n", "        ", "g", ".", "user", "=", "None", "\n", "", "else", ":", "\n", "        ", "g", ".", "user", "=", "get_db", "(", ")", ".", "execute", "(", "\n", "\"SELECT * FROM user WHERE id = ?\"", ",", "(", "user_id", ",", ")", "\n", ")", ".", "fetchone", "(", ")", "\n"]], "home.repos.pwc.inspect_result.Stanford-ILIAD_PantheonRL.website.login.login_required": [[72, 83], ["functools.wraps", "view", "flask.redirect", "flask.url_for", "flask.redirect", "flask.url_for"], "function", ["None"], ["", "", "def", "login_required", "(", "view", ")", ":", "\n", "    ", "@", "functools", ".", "wraps", "(", "view", ")", "\n", "def", "wrapped_view", "(", "**", "kwargs", ")", ":", "\n", "        ", "if", "g", ".", "user", "is", "None", ":", "\n", "            ", "return", "redirect", "(", "url_for", "(", "'login.main'", ")", ")", "\n", "", "elif", "g", ".", "user", "[", "'running'", "]", ":", "\n", "                ", "return", "redirect", "(", "url_for", "(", "'training.main'", ")", ")", "\n", "", "session", "[", "'tb'", "]", "=", "False", "\n", "return", "view", "(", "**", "kwargs", ")", "\n", "\n", "", "return", "wrapped_view", "\n", "\n"]], "home.repos.pwc.inspect_result.Stanford-ILIAD_PantheonRL.website.login.login_for_training": [[84, 93], ["functools.wraps", "view", "flask.redirect", "flask.url_for"], "function", ["None"], ["", "def", "login_for_training", "(", "view", ")", ":", "\n", "    ", "@", "functools", ".", "wraps", "(", "view", ")", "\n", "def", "wrapped_view", "(", "**", "kwargs", ")", ":", "\n", "        ", "if", "g", ".", "user", "is", "None", ":", "\n", "            ", "return", "redirect", "(", "url_for", "(", "'login.main'", ")", ")", "\n", "\n", "", "return", "view", "(", "**", "kwargs", ")", "\n", "\n", "", "return", "wrapped_view", "", "", ""]], "home.repos.pwc.inspect_result.Stanford-ILIAD_PantheonRL.website.constants.generate_url": [[22, 24], ["None"], "function", ["None"], ["def", "generate_url", "(", "host", ")", ":", "\n", "    ", "return", "f\"http://{host}:{TB_PORT}/\"", "", "", ""]], "home.repos.pwc.inspect_result.Stanford-ILIAD_PantheonRL.website.trainer.main": [[14, 29], ["bp.route", "flask.render_template", "website.data_processing.read", "website.data_processing.check_trained", "flask.flash", "flask.redirect", "website.constants.generate_url", "flask.url_for", "urllib.parse.urlparse"], "function", ["home.repos.pwc.inspect_result.Stanford-ILIAD_PantheonRL.website.data_processing.read", "home.repos.pwc.inspect_result.Stanford-ILIAD_PantheonRL.website.data_processing.check_trained", "home.repos.pwc.inspect_result.Stanford-ILIAD_PantheonRL.website.constants.generate_url"], ["from", "pantheonrl", ".", "algos", ".", "adap", ".", "adap_learn", "import", "ADAP", "\n", "from", "pantheonrl", ".", "algos", ".", "adap", ".", "policies", "import", "AdapPolicyMult", ",", "AdapPolicy", "\n", "from", "pantheonrl", ".", "algos", ".", "adap", ".", "agent", "import", "AdapAgent", "\n", "\n", "from", "pantheonrl", ".", "algos", ".", "modular", ".", "learn", "import", "ModularAlgorithm", "\n", "from", "pantheonrl", ".", "algos", ".", "modular", ".", "policies", "import", "ModularPolicy", "\n", "\n", "from", "pantheonrl", ".", "algos", ".", "bc", "import", "BCShell", ",", "reconstruct_policy", "\n", "\n", "from", "pantheonrl", ".", "envs", ".", "rpsgym", ".", "rps", "import", "RPSEnv", ",", "RPSWeightedAgent", "\n", "from", "pantheonrl", ".", "envs", ".", "blockworldgym", "import", "simpleblockworld", ",", "blockworld", "\n", "from", "pantheonrl", ".", "envs", ".", "liargym", ".", "liar", "import", "LiarEnv", ",", "LiarDefaultAgent", "\n", "\n", "from", "overcookedgym", ".", "overcooked_utils", "import", "LAYOUT_LIST", "\n", "\n", "ENV_LIST", "=", "[", "'RPS-v0'", ",", "'BlockEnv-v0'", ",", "'BlockEnv-v1'", ",", "'LiarsDice-v0'", ",", "\n"]], "home.repos.pwc.inspect_result.Stanford-ILIAD_PantheonRL.website.trainer.learn": [[30, 45], ["bp.route", "flask.redirect", "website.db.get_db", "website.db.get_db.execute", "website.db.get_db.commit", "website.data_processing.start_training", "flask.url_for"], "function", ["home.repos.pwc.inspect_result.Stanford-ILIAD_PantheonRL.website.db.get_db", "home.repos.pwc.inspect_result.Stanford-ILIAD_PantheonRL.website.data_processing.start_training"], ["'OvercookedMultiEnv-v0'", "]", "\n", "\n", "ADAP_TYPES", "=", "[", "'ADAP'", ",", "'ADAP_MULT'", "]", "\n", "EGO_LIST", "=", "[", "'PPO'", ",", "'ModularAlgorithm'", ",", "'LOAD'", "]", "+", "ADAP_TYPES", "\n", "PARTNER_LIST", "=", "[", "'PPO'", ",", "'DEFAULT'", ",", "'FIXED'", "]", "+", "ADAP_TYPES", "\n", "\n", "\n", "class", "EnvException", "(", "Exception", ")", ":", "\n", "    ", "\"\"\" Raise when parameters do not align with environment \"\"\"", "\n", "\n", "\n", "", "def", "input_check", "(", "args", ")", ":", "\n", "# Env checking", "\n", "    ", "if", "args", ".", "env", "==", "'OvercookedMultiEnv-v0'", ":", "\n", "        ", "if", "'layout_name'", "not", "in", "args", ".", "env_config", ":", "\n", "            ", "raise", "EnvException", "(", "f\"layout_name needed for {args.env}\"", ")", "\n"]], "home.repos.pwc.inspect_result.Stanford-ILIAD_PantheonRL.website.trainer.done": [[46, 61], ["bp.route", "flask.render_template", "website.db.get_db", "website.db.get_db.execute", "website.db.get_db.commit", "website.data_processing.stop_tensorboard"], "function", ["home.repos.pwc.inspect_result.Stanford-ILIAD_PantheonRL.website.db.get_db", "home.repos.pwc.inspect_result.Stanford-ILIAD_PantheonRL.website.data_processing.stop_tensorboard"], ["", "elif", "args", ".", "env_config", "[", "'layout_name'", "]", "not", "in", "LAYOUT_LIST", ":", "\n", "            ", "raise", "EnvException", "(", "\n", "f\"{args.env_config['layout_name']} is not a valid layout\"", ")", "\n", "\n", "# Construct alt configs", "\n", "", "", "if", "args", ".", "alt_config", "is", "None", ":", "\n", "        ", "args", ".", "alt_config", "=", "[", "{", "}", "for", "_", "in", "args", ".", "alt", "]", "\n", "", "elif", "len", "(", "args", ".", "alt_config", ")", "!=", "len", "(", "args", ".", "alt", ")", ":", "\n", "        ", "raise", "EnvException", "(", "\n", "\"Number of partners is different from number of configs\"", ")", "\n", "\n", "# Construct ego config", "\n", "", "if", "'verbose'", "not", "in", "args", ".", "ego_config", ":", "\n", "        ", "args", ".", "ego_config", "[", "'verbose'", "]", "=", "1", "\n", "\n", "", "if", "(", "args", ".", "tensorboard_log", "is", "not", "None", ")", "!="]], "home.repos.pwc.inspect_result.Stanford-ILIAD_PantheonRL.website.trainer.tb": [[62, 78], ["bp.route", "flask.redirect", "website.data_processing.gen_tensorboard", "website.data_processing.check_trained", "flask.url_for", "flask.flash", "flask.redirect", "flask.url_for"], "function", ["home.repos.pwc.inspect_result.Stanford-ILIAD_PantheonRL.website.data_processing.gen_tensorboard", "home.repos.pwc.inspect_result.Stanford-ILIAD_PantheonRL.website.data_processing.check_trained"], ["(", "args", ".", "tensorboard_name", "is", "not", "None", ")", ":", "\n", "        ", "raise", "EnvException", "(", "\"Must define log and names for tensorboard\"", ")", "\n", "\n", "\n", "", "", "def", "latent_check", "(", "args", ")", ":", "\n", "# Check for ADAP", "\n", "    ", "all_adap", "=", "all", "(", "[", "v", "in", "ADAP_TYPES", "for", "v", "in", "args", ".", "alt", "]", ")", "\n", "if", "args", ".", "ego", "not", "in", "ADAP_TYPES", "or", "not", "all_adap", ":", "\n", "        ", "raise", "EnvException", "(", "\n", "\"both agents must be ADAP or ADAP_MULT to share latent spaces\"", ")", "\n", "\n", "", "if", "'context_size'", "not", "in", "args", ".", "ego_config", ":", "\n", "        ", "args", ".", "ego_config", "[", "'context_size'", "]", "=", "3", "\n", "", "if", "'context_sampler'", "not", "in", "args", ".", "ego_config", ":", "\n", "        ", "args", ".", "ego_config", "[", "'context_sampler'", "]", "=", "\"l2\"", "\n", "\n", "", "for", "conf", "in", "args", ".", "alt_config", ":", "\n"]], "home.repos.pwc.inspect_result.Stanford-ILIAD_PantheonRL.website.env_selection.main": [[13, 31], ["bp.route", "env_selection.clear_agent_selection", "flask.render_template", "error", "flask.flash", "flask.redirect", "flask.url_for"], "function", ["home.repos.pwc.inspect_result.Stanford-ILIAD_PantheonRL.website.env_selection.clear_agent_selection"], ["@", "bp", ".", "route", "(", "\"/\"", ",", "methods", "=", "(", "'GET'", ",", "'POST'", ")", ")", "\n", "@", "login_required", "\n", "def", "main", "(", ")", ":", "\n", "    ", "clear_agent_selection", "(", ")", "\n", "if", "request", ".", "method", "==", "'POST'", ":", "\n", "        ", "env_name", "=", "request", ".", "form", "[", "'env'", "]", "\n", "error", "=", "None", "\n", "\n", "if", "not", "env_name", ":", "\n", "            ", "error", "(", "'You must select an environment to continue.'", ")", "\n", "\n", "", "if", "error", "is", "not", "None", ":", "\n", "            ", "flash", "(", "error", ")", "\n", "\n", "", "for", "possible_env", "in", "ENV_LIST", ":", "\n", "            ", "if", "env_name", "==", "ENV_LIST", "[", "possible_env", "]", ":", "\n", "                ", "return", "redirect", "(", "url_for", "(", "f\"welcome.{ENV_LIST[possible_env]}\"", ")", ")", "\n", "", "", "", "return", "render_template", "(", "'welcome.html'", ",", "envs", "=", "ENV_LIST", ",", "selected", "=", "False", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Stanford-ILIAD_PantheonRL.website.env_selection.blockworld": [[32, 46], ["bp.route", "flask.render_template", "website.data_processing.common_env_configs", "flask.flash", "website.data_processing.create_args_dict", "flask.redirect", "flask.url_for"], "function", ["home.repos.pwc.inspect_result.Stanford-ILIAD_PantheonRL.website.data_processing.common_env_configs", "home.repos.pwc.inspect_result.Stanford-ILIAD_PantheonRL.website.data_processing.create_args_dict"], ["", "@", "bp", ".", "route", "(", "\"/blockworld\"", ",", "methods", "=", "(", "'GET'", ",", "'POST'", ")", ")", "\n", "@", "login_required", "\n", "def", "blockworld", "(", ")", ":", "\n", "    ", "if", "request", ".", "method", "==", "'POST'", ":", "\n", "        ", "record", ",", "framestack", ",", "error", "=", "common_env_configs", "(", "request", ".", "form", ",", "g", ".", "user", "[", "'id'", "]", ")", "\n", "\n", "if", "error", "is", "not", "None", ":", "\n", "            ", "flash", "(", "error", ")", "\n", "", "else", ":", "\n", "            ", "env_name", "=", "\"BlockEnv-v1\"", "\n", "env_config", "=", "{", "}", "\n", "session", "[", "'env_data'", "]", "=", "create_args_dict", "(", "env_name", ",", "env_config", ",", "record", ",", "framestack", ")", "\n", "return", "redirect", "(", "url_for", "(", "'agents.agents'", ",", "env", "=", "'blockworld'", ")", ")", "\n", "", "", "return", "render_template", "(", "'environments/blockworld.html'", ",", "envs", "=", "ENV_LIST", ",", "selected", "=", "True", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Stanford-ILIAD_PantheonRL.website.env_selection.simpleblockworld": [[47, 61], ["bp.route", "flask.render_template", "website.data_processing.common_env_configs", "flask.flash", "website.data_processing.create_args_dict", "flask.redirect", "flask.url_for"], "function", ["home.repos.pwc.inspect_result.Stanford-ILIAD_PantheonRL.website.data_processing.common_env_configs", "home.repos.pwc.inspect_result.Stanford-ILIAD_PantheonRL.website.data_processing.create_args_dict"], ["", "@", "bp", ".", "route", "(", "\"/simpleblockworld\"", ",", "methods", "=", "(", "'GET'", ",", "'POST'", ")", ")", "\n", "@", "login_required", "\n", "def", "simpleblockworld", "(", ")", ":", "\n", "    ", "if", "request", ".", "method", "==", "'POST'", ":", "\n", "        ", "record", ",", "framestack", ",", "error", "=", "common_env_configs", "(", "request", ".", "form", ",", "g", ".", "user", "[", "'id'", "]", ")", "\n", "\n", "if", "error", "is", "not", "None", ":", "\n", "            ", "flash", "(", "error", ")", "\n", "", "else", ":", "\n", "            ", "env_name", "=", "\"BlockEnv-v0\"", "\n", "env_config", "=", "{", "}", "\n", "session", "[", "'env_data'", "]", "=", "create_args_dict", "(", "env_name", ",", "env_config", ",", "record", ",", "framestack", ")", "\n", "return", "redirect", "(", "url_for", "(", "'agents.agents'", ",", "env", "=", "'simpleblockworld'", ")", ")", "\n", "", "", "return", "render_template", "(", "'environments/simpleblockworld.html'", ",", "envs", "=", "ENV_LIST", ",", "selected", "=", "True", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Stanford-ILIAD_PantheonRL.website.env_selection.overcooked": [[62, 82], ["bp.route", "flask.render_template", "int", "website.data_processing.common_env_configs", "flask.flash", "website.data_processing.create_args_dict", "flask.redirect", "flask.url_for"], "function", ["home.repos.pwc.inspect_result.Stanford-ILIAD_PantheonRL.website.data_processing.common_env_configs", "home.repos.pwc.inspect_result.Stanford-ILIAD_PantheonRL.website.data_processing.create_args_dict"], ["", "@", "bp", ".", "route", "(", "\"/overcooked\"", ",", "methods", "=", "(", "'GET'", ",", "'POST'", ")", ")", "\n", "@", "login_required", "\n", "def", "overcooked", "(", ")", ":", "\n", "    ", "if", "request", ".", "method", "==", "'POST'", ":", "\n", "        ", "layout_name", "=", "request", ".", "form", "[", "'layout_name'", "]", "\n", "ego_agent_idx", "=", "int", "(", "request", ".", "form", "[", "'ego_agent_idx'", "]", ")", "\n", "baselines", "=", "'baselines'", "in", "request", ".", "form", "\n", "\n", "record", ",", "framestack", ",", "error", "=", "common_env_configs", "(", "request", ".", "form", ",", "g", ".", "user", "[", "'id'", "]", ")", "\n", "\n", "if", "error", "is", "not", "None", ":", "\n", "            ", "flash", "(", "error", ")", "\n", "", "else", ":", "\n", "            ", "env_name", "=", "\"OvercookedMultiEnv-v0\"", "\n", "env_config", "=", "{", "\"layout_name\"", ":", "layout_name", ",", "\"ego_agent_idx\"", ":", "ego_agent_idx", ",", "\"baselines\"", ":", "baselines", "}", "\n", "session", "[", "'env_data'", "]", "=", "create_args_dict", "(", "env_name", ",", "env_config", ",", "record", ",", "framestack", ")", "\n", "\n", "return", "redirect", "(", "url_for", "(", "'agents.agents'", ",", "env", "=", "'overcooked'", ")", ")", "\n", "\n", "", "", "return", "render_template", "(", "'environments/overcooked.html'", ",", "envs", "=", "ENV_LIST", ",", "selected", "=", "True", ",", "layouts", "=", "LAYOUT_LIST", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Stanford-ILIAD_PantheonRL.website.env_selection.liar": [[83, 100], ["bp.route", "flask.render_template", "website.data_processing.common_env_configs", "flask.flash", "website.data_processing.create_args_dict", "flask.redirect", "flask.url_for"], "function", ["home.repos.pwc.inspect_result.Stanford-ILIAD_PantheonRL.website.data_processing.common_env_configs", "home.repos.pwc.inspect_result.Stanford-ILIAD_PantheonRL.website.data_processing.create_args_dict"], ["", "@", "bp", ".", "route", "(", "\"/liar\"", ",", "methods", "=", "(", "'GET'", ",", "'POST'", ")", ")", "\n", "def", "liar", "(", ")", ":", "\n", "    ", "if", "request", ".", "method", "==", "'POST'", ":", "\n", "        ", "probegostart", "=", "request", ".", "form", "[", "'probegostart'", "]", "\n", "\n", "record", ",", "framestack", ",", "error", "=", "common_env_configs", "(", "request", ".", "form", ",", "g", ".", "user", "[", "'id'", "]", ")", "\n", "\n", "if", "error", "is", "not", "None", ":", "\n", "            ", "flash", "(", "error", ")", "\n", "", "else", ":", "\n", "            ", "env_name", "=", "\"LiarsDice-v0\"", "\n", "env_config", "=", "{", "\"probegostart\"", ":", "probegostart", "}", "\n", "session", "[", "'env_data'", "]", "=", "create_args_dict", "(", "env_name", ",", "env_config", ",", "record", ",", "framestack", ")", "\n", "\n", "return", "redirect", "(", "url_for", "(", "'agents.agents'", ",", "env", "=", "'liar'", ")", ")", "\n", "\n", "", "", "return", "render_template", "(", "'environments/liar.html'", ",", "envs", "=", "ENV_LIST", ",", "selected", "=", "True", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Stanford-ILIAD_PantheonRL.website.env_selection.rps": [[101, 115], ["bp.route", "flask.render_template", "website.data_processing.common_env_configs", "flask.flash", "website.data_processing.create_args_dict", "flask.redirect", "flask.url_for"], "function", ["home.repos.pwc.inspect_result.Stanford-ILIAD_PantheonRL.website.data_processing.common_env_configs", "home.repos.pwc.inspect_result.Stanford-ILIAD_PantheonRL.website.data_processing.create_args_dict"], ["", "@", "bp", ".", "route", "(", "\"/rps\"", ",", "methods", "=", "(", "'GET'", ",", "'POST'", ")", ")", "\n", "def", "rps", "(", ")", ":", "\n", "    ", "if", "request", ".", "method", "==", "'POST'", ":", "\n", "        ", "record", ",", "framestack", ",", "error", "=", "common_env_configs", "(", "request", ".", "form", ",", "g", ".", "user", "[", "'id'", "]", ")", "\n", "\n", "if", "error", "is", "not", "None", ":", "\n", "            ", "flash", "(", "error", ")", "\n", "", "else", ":", "\n", "            ", "env_name", "=", "\"RPS-v0\"", "\n", "env_config", "=", "{", "}", "\n", "session", "[", "'env_data'", "]", "=", "create_args_dict", "(", "env_name", ",", "env_config", ",", "record", ",", "framestack", ")", "\n", "\n", "return", "redirect", "(", "url_for", "(", "'agents.agents'", ",", "env", "=", "'rps'", ")", ")", "\n", "", "", "return", "render_template", "(", "'environments/rps.html'", ",", "envs", "=", "ENV_LIST", ",", "selected", "=", "True", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Stanford-ILIAD_PantheonRL.website.env_selection.clear_agent_selection": [[116, 118], ["env_selection.delete_from_session"], "function", ["home.repos.pwc.inspect_result.Stanford-ILIAD_PantheonRL.website.env_selection.delete_from_session"], ["", "def", "clear_agent_selection", "(", ")", ":", "\n", "    ", "delete_from_session", "(", "[", "'egotype'", ",", "'partnertype'", ",", "'partners'", ",", "'ego'", ",", "'tb_log'", ",", "'tb_name'", ",", "'updates'", ",", "'processid'", "]", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Stanford-ILIAD_PantheonRL.website.env_selection.delete_from_session": [[119, 123], ["flask.session.pop"], "function", ["None"], ["", "def", "delete_from_session", "(", "vars", ")", ":", "\n", "    ", "for", "var", "in", "vars", ":", "\n", "        ", "if", "var", "in", "session", ":", "\n", "            ", "session", ".", "pop", "(", "var", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Stanford-ILIAD_PantheonRL.website.db.get_db": [[10, 19], ["sqlite3.connect"], "function", ["None"], ["def", "get_db", "(", ")", ":", "\n", "    ", "if", "'db'", "not", "in", "g", ":", "\n", "        ", "g", ".", "db", "=", "sqlite3", ".", "connect", "(", "\n", "current_app", ".", "config", "[", "'DATABASE'", "]", ",", "\n", "detect_types", "=", "sqlite3", ".", "PARSE_DECLTYPES", "\n", ")", "\n", "g", ".", "db", ".", "row_factory", "=", "sqlite3", ".", "Row", "\n", "\n", "", "return", "g", ".", "db", "\n", "\n"]], "home.repos.pwc.inspect_result.Stanford-ILIAD_PantheonRL.website.db.close_db": [[20, 25], ["flask.g.pop", "g.pop.close"], "function", ["None"], ["", "def", "close_db", "(", "e", "=", "None", ")", ":", "\n", "    ", "db", "=", "g", ".", "pop", "(", "'db'", ",", "None", ")", "\n", "\n", "if", "db", "is", "not", "None", ":", "\n", "        ", "db", ".", "close", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Stanford-ILIAD_PantheonRL.website.db.init_db": [[26, 31], ["db.get_db", "flask.current_app.open_resource", "get_db.executescript", "f.read().decode", "f.read"], "function", ["home.repos.pwc.inspect_result.Stanford-ILIAD_PantheonRL.website.db.get_db", "home.repos.pwc.inspect_result.Stanford-ILIAD_PantheonRL.website.data_processing.read"], ["", "", "def", "init_db", "(", ")", ":", "\n", "    ", "db", "=", "get_db", "(", ")", "\n", "\n", "with", "current_app", ".", "open_resource", "(", "'schema.sql'", ")", "as", "f", ":", "\n", "        ", "db", ".", "executescript", "(", "f", ".", "read", "(", ")", ".", "decode", "(", "'utf8'", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Stanford-ILIAD_PantheonRL.website.db.clear_user_data": [[32, 39], ["shutil.rmtree", "print"], "function", ["None"], ["", "", "def", "clear_user_data", "(", ")", ":", "\n", "# clears all user-created files", "\n", "    ", "mydir", "=", "\"./data\"", "\n", "try", ":", "\n", "        ", "shutil", ".", "rmtree", "(", "mydir", ")", "\n", "", "except", "OSError", "as", "e", ":", "\n", "        ", "print", "(", "\"Nothing to erase: %s - %s.\"", "%", "(", "e", ".", "filename", ",", "e", ".", "strerror", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Stanford-ILIAD_PantheonRL.website.db.init_db_command": [[40, 47], ["click.command", "db.init_db", "db.clear_user_data", "click.echo"], "function", ["home.repos.pwc.inspect_result.Stanford-ILIAD_PantheonRL.website.db.init_db", "home.repos.pwc.inspect_result.Stanford-ILIAD_PantheonRL.website.db.clear_user_data"], ["", "", "@", "click", ".", "command", "(", "'init-db'", ")", "\n", "@", "with_appcontext", "\n", "def", "init_db_command", "(", ")", ":", "\n", "    ", "\"\"\"Clear the existing data and create new tables.\"\"\"", "\n", "init_db", "(", ")", "\n", "clear_user_data", "(", ")", "\n", "click", ".", "echo", "(", "'Initialized the database.'", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Stanford-ILIAD_PantheonRL.website.db.init_app": [[48, 51], ["app.teardown_appcontext", "app.cli.add_command"], "function", ["None"], ["", "def", "init_app", "(", "app", ")", ":", "\n", "    ", "app", ".", "teardown_appcontext", "(", "close_db", ")", "\n", "app", ".", "cli", ".", "add_command", "(", "init_db_command", ")", "", "", ""]], "home.repos.pwc.inspect_result.Stanford-ILIAD_PantheonRL.website.__init__.create_app": [[5, 35], ["flask.Flask", "flask.Flask.config.from_mapping", "db.init_app", "flask.Flask.register_blueprint", "flask.Flask.register_blueprint", "flask.Flask.register_blueprint", "flask.Flask.register_blueprint", "flask.Flask.config.from_pyfile", "flask.Flask.config.from_mapping", "os.makedirs", "os.path.join"], "function", ["home.repos.pwc.inspect_result.Stanford-ILIAD_PantheonRL.website.db.init_app"], ["def", "create_app", "(", "test_config", "=", "None", ")", ":", "\n", "# create and configure the app", "\n", "    ", "app", "=", "Flask", "(", "__name__", ",", "instance_relative_config", "=", "True", ")", "\n", "\n", "app", ".", "config", ".", "from_mapping", "(", "\n", "SECRET_KEY", "=", "'dev'", ",", "\n", "DATABASE", "=", "os", ".", "path", ".", "join", "(", "app", ".", "instance_path", ",", "'pantheonrl.sqlite'", ")", "\n", ")", "\n", "if", "test_config", "is", "None", ":", "\n", "# load the instance config, if it exists, when not testing", "\n", "        ", "app", ".", "config", ".", "from_pyfile", "(", "'config.py'", ",", "silent", "=", "True", ")", "\n", "", "else", ":", "\n", "        ", "app", ".", "config", ".", "from_mapping", "(", "test_config", ")", "\n", "\n", "# ensure the instance folder exists", "\n", "", "try", ":", "\n", "        ", "os", ".", "makedirs", "(", "app", ".", "instance_path", ")", "\n", "", "except", "OSError", ":", "\n", "        ", "pass", "\n", "\n", "", "from", ".", "import", "db", "\n", "db", ".", "init_app", "(", "app", ")", "\n", "\n", "from", "website", "import", "env_selection", ",", "agents", ",", "login", ",", "trainer", "\n", "app", ".", "register_blueprint", "(", "login", ".", "bp", ")", "\n", "app", ".", "register_blueprint", "(", "env_selection", ".", "bp", ")", "\n", "app", ".", "register_blueprint", "(", "agents", ".", "bp", ")", "\n", "app", ".", "register_blueprint", "(", "trainer", ".", "bp", ")", "\n", "\n", "return", "app", "\n", "", ""]], "home.repos.pwc.inspect_result.Stanford-ILIAD_PantheonRL.website.agents.agents": [[12, 62], ["bp.route", "flask.render_template", "flask.redirect", "website.data_processing.fixedpartneroptions", "flask.url_for", "flask.flash", "website.data_processing.check_agent_errors", "errors.append", "website.db.get_db", "website.db.get_db.execute", "website.db.get_db.commit", "flask.redirect", "flask.flash", "len", "flask.flash", "flask.url_for", "len"], "function", ["home.repos.pwc.inspect_result.Stanford-ILIAD_PantheonRL.website.data_processing.fixedpartneroptions", "home.repos.pwc.inspect_result.Stanford-ILIAD_PantheonRL.website.data_processing.check_agent_errors", "home.repos.pwc.inspect_result.Stanford-ILIAD_PantheonRL.website.db.get_db"], ["@", "bp", ".", "route", "(", "\"/<string:env>/agents\"", ",", "methods", "=", "(", "'GET'", ",", "'POST'", ")", ")", "\n", "@", "login_required", "\n", "def", "agents", "(", "env", ")", ":", "\n", "    ", "if", "not", "'env_data'", "in", "session", ":", "\n", "        ", "return", "redirect", "(", "url_for", "(", "'welcome.main'", ")", ")", "\n", "\n", "", "if", "request", ".", "method", "==", "'POST'", ":", "\n", "        ", "error", "=", "None", "\n", "if", "not", "'ego'", "in", "session", ":", "\n", "            ", "error", "=", "\"Please add the ego agent before training.\"", "\n", "", "elif", "not", "'partners'", "in", "session", "or", "len", "(", "session", "[", "'partners'", "]", ")", "==", "0", ":", "\n", "            ", "error", "=", "\"Please add at least one partner agent.\"", "\n", "\n", "", "if", "error", "is", "not", "None", ":", "\n", "            ", "flash", "(", "error", ")", "\n", "", "else", ":", "\n", "            ", "errors", ",", "partners", ",", "tb_log", ",", "tb_name", ",", "filedata", "=", "check_agent_errors", "(", "g", ".", "user", "[", "'id'", "]", ",", "env", ",", "session", "[", "'ego'", "]", ",", "session", "[", "'partners'", "]", ")", "\n", "\n", "", "if", "not", "errors", "==", "[", "]", ":", "\n", "            ", "errors", ".", "append", "(", "\"Either add more agents, or press \\'Train\\' again to continue.\"", ")", "\n", "for", "e", "in", "errors", ":", "\n", "                ", "flash", "(", "e", ")", "\n", "", "session", "[", "'partners'", "]", "=", "partners", "\n", "", "else", ":", "\n", "            ", "session", "[", "'tb_log'", "]", "=", "tb_log", "\n", "session", "[", "'tb_name'", "]", "=", "tb_name", "\n", "db", "=", "get_db", "(", ")", "\n", "db", ".", "execute", "(", "\n", "'UPDATE user SET filedata = ?'", "\n", "' WHERE id = ?'", ",", "\n", "(", "filedata", ",", "g", ".", "user", "[", "'id'", "]", ")", "\n", ")", "\n", "db", ".", "commit", "(", ")", "\n", "return", "redirect", "(", "url_for", "(", "'training.main'", ")", ")", "\n", "\n", "# set blank agent parameters in session", "\n", "", "", "if", "not", "'egotype'", "in", "session", ":", "\n", "        ", "session", "[", "'egotype'", "]", "=", "None", "\n", "", "if", "not", "'partnertype'", "in", "session", ":", "\n", "        ", "session", "[", "'partnertype'", "]", "=", "None", "\n", "", "if", "not", "'partners'", "in", "session", ":", "\n", "        ", "session", "[", "'partners'", "]", "=", "[", "]", "\n", "\n", "", "options", "=", "[", "]", "\n", "if", "session", "[", "'partnertype'", "]", "==", "\"FIXED\"", ":", "\n", "        ", "error", ",", "options", "=", "fixedpartneroptions", "(", "g", ".", "user", "[", "'id'", "]", ",", "session", "[", "'env_data'", "]", "[", "'env'", "]", ")", "\n", "if", "error", "is", "not", "None", ":", "\n", "            ", "flash", "(", "error", ")", "\n", "\n", "", "", "return", "render_template", "(", "'agentparams.html'", ",", "partners", "=", "len", "(", "session", "[", "'partners'", "]", ")", ",", "ego_options", "=", "EGO_LIST", ",", "partner_options", "=", "PARTNER_LIST", ",", "env", "=", "env", ",", "env_options", "=", "ENV_LIST", ",", "fixed_options", "=", "options", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Stanford-ILIAD_PantheonRL.website.agents.egotype": [[63, 69], ["bp.route", "flask.redirect", "flask.url_for"], "function", ["None"], ["", "@", "bp", ".", "route", "(", "\"/<string:env>/egotype\"", ",", "methods", "=", "(", "'POST'", ",", ")", ")", "\n", "@", "login_required", "\n", "def", "egotype", "(", "env", ")", ":", "\n", "    ", "if", "request", ".", "method", "==", "'POST'", ":", "\n", "        ", "session", "[", "'egotype'", "]", "=", "request", ".", "form", "[", "'egotype'", "]", "\n", "return", "redirect", "(", "url_for", "(", "'agents.agents'", ",", "env", "=", "env", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Stanford-ILIAD_PantheonRL.website.agents.partnertype": [[70, 79], ["bp.route", "flask.redirect", "flask.url_for"], "function", ["None"], ["", "", "@", "bp", ".", "route", "(", "\"/<string:env>/partnertype\"", ",", "methods", "=", "(", "'POST'", ",", ")", ")", "\n", "@", "login_required", "\n", "def", "partnertype", "(", "env", ")", ":", "\n", "    ", "if", "request", ".", "method", "==", "'POST'", ":", "\n", "        ", "session", "[", "'partnertype'", "]", "=", "request", ".", "form", "[", "'partnertype'", "]", "\n", "# if session['partnertype'] == 'FIXED':", "\n", "#     error = \"Fixed partner agents are not yet implemented. Please pick a different partner type.\"", "\n", "#     flash(error)", "\n", "return", "redirect", "(", "url_for", "(", "'agents.agents'", ",", "env", "=", "env", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Stanford-ILIAD_PantheonRL.website.agents.setego": [[80, 92], ["bp.route", "website.data_processing.create_ego_dict", "flask.redirect", "flask.flash", "flask.url_for"], "function", ["home.repos.pwc.inspect_result.Stanford-ILIAD_PantheonRL.website.data_processing.create_ego_dict"], ["", "", "@", "bp", ".", "route", "(", "\"/<string:env>/setego\"", ",", "methods", "=", "(", "'POST'", ",", ")", ")", "\n", "@", "login_required", "\n", "def", "setego", "(", "env", ")", ":", "\n", "    ", "if", "request", ".", "method", "==", "'POST'", ":", "\n", "# add ego params to the session variable", "\n", "        ", "error", ",", "ego_dict", "=", "create_ego_dict", "(", "session", "[", "'egotype'", "]", ",", "request", ".", "form", ",", "env", ",", "g", ".", "user", "[", "'id'", "]", ")", "\n", "if", "error", "is", "not", "None", ":", "\n", "            ", "flash", "(", "error", ")", "\n", "", "else", ":", "\n", "            ", "session", "[", "'ego'", "]", "=", "ego_dict", "\n", "session", "[", "'egotype'", "]", "=", "None", "\n", "", "return", "redirect", "(", "url_for", "(", "'agents.agents'", ",", "env", "=", "env", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Stanford-ILIAD_PantheonRL.website.agents.setpartner": [[94, 106], ["bp.route", "website.data_processing.create_partner_dict", "flask.redirect", "flask.flash", "session[].append", "flask.url_for"], "function", ["home.repos.pwc.inspect_result.Stanford-ILIAD_PantheonRL.website.data_processing.create_partner_dict"], ["", "", "@", "bp", ".", "route", "(", "\"/<string:env>/setpartner\"", ",", "methods", "=", "(", "'POST'", ",", ")", ")", "\n", "@", "login_required", "\n", "def", "setpartner", "(", "env", ")", ":", "\n", "    ", "if", "request", ".", "method", "==", "'POST'", ":", "\n", "# add ego params to the session variable", "\n", "        ", "error", ",", "partner_dict", "=", "create_partner_dict", "(", "g", ".", "user", "[", "'id'", "]", ",", "session", "[", "'partnertype'", "]", ",", "env", ",", "request", ".", "form", ")", "\n", "if", "error", "is", "not", "None", ":", "\n", "            ", "flash", "(", "error", ")", "\n", "", "else", ":", "\n", "            ", "session", "[", "'partners'", "]", ".", "append", "(", "partner_dict", ")", "\n", "session", "[", "'partnertype'", "]", "=", "None", "\n", "", "return", "redirect", "(", "url_for", "(", "'agents.agents'", ",", "env", "=", "env", ")", ")", "", "", "", ""]], "home.repos.pwc.inspect_result.Stanford-ILIAD_PantheonRL.algos.bc.BCShell.__init__": [[30, 32], ["None"], "methods", ["None"], ["    ", "def", "__init__", "(", "self", ",", "policy", ")", ":", "\n", "        ", "self", ".", "policy", "=", "policy", "\n", "\n"]], "home.repos.pwc.inspect_result.Stanford-ILIAD_PantheonRL.algos.bc.ConstantLRSchedule.__init__": [[53, 59], ["None"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "lr", ":", "float", "=", "1e-3", ")", ":", "\n", "        ", "\"\"\"\n        Args:\n          lr: the constant learning rate that calls to this object will return.\n        \"\"\"", "\n", "self", ".", "lr", "=", "lr", "\n", "\n"]], "home.repos.pwc.inspect_result.Stanford-ILIAD_PantheonRL.algos.bc.ConstantLRSchedule.__call__": [[60, 65], ["None"], "methods", ["None"], ["", "def", "__call__", "(", "self", ",", "_", ")", ":", "\n", "        ", "\"\"\"\n        Returns the constant learning rate.\n        \"\"\"", "\n", "return", "self", ".", "lr", "\n", "\n"]], "home.repos.pwc.inspect_result.Stanford-ILIAD_PantheonRL.algos.bc.EpochOrBatchIteratorWithProgress.__init__": [[68, 107], ["ValueError"], "methods", ["None"], ["    ", "def", "__init__", "(", "\n", "self", ",", "\n", "data_loader", ":", "Iterable", "[", "dict", "]", ",", "\n", "n_epochs", ":", "Optional", "[", "int", "]", "=", "None", ",", "\n", "n_batches", ":", "Optional", "[", "int", "]", "=", "None", ",", "\n", "on_epoch_end", ":", "Optional", "[", "Callable", "[", "[", "]", ",", "None", "]", "]", "=", "None", ",", "\n", "on_batch_end", ":", "Optional", "[", "Callable", "[", "[", "]", ",", "None", "]", "]", "=", "None", ",", "\n", ")", ":", "\n", "        ", "\"\"\"\n        Wraps DataLoader so that all BC batches can be processed in a one\n        for-loop. Also uses `tqdm` to show progress in stdout.\n        Args:\n            data_loader: An iterable over data dicts, as used in `BC`.\n            n_epochs: The number of epochs to iterate through in one call to\n                __iter__. Exactly one of `n_epochs` and `n_batches` should be\n                provided.\n            n_batches: The number of batches to iterate through in one call to\n                __iter__. Exactly one of `n_epochs` and `n_batches` should be\n                provided.\n            on_epoch_end: A callback function without parameters to be called\n                at the end of every epoch.\n            on_batch_end: A callback function without parameters to be called\n                at the end of every batch.\n        \"\"\"", "\n", "if", "n_epochs", "is", "not", "None", "and", "n_batches", "is", "None", ":", "\n", "            ", "self", ".", "use_epochs", "=", "True", "\n", "", "elif", "n_epochs", "is", "None", "and", "n_batches", "is", "not", "None", ":", "\n", "            ", "self", ".", "use_epochs", "=", "False", "\n", "", "else", ":", "\n", "            ", "raise", "ValueError", "(", "\n", "\"Must provide exactly one of `n_epochs` \\\n                and `n_batches` arguments.\"", "\n", ")", "\n", "\n", "", "self", ".", "data_loader", "=", "data_loader", "\n", "self", ".", "n_epochs", "=", "n_epochs", "\n", "self", ".", "n_batches", "=", "n_batches", "\n", "self", ".", "on_epoch_end", "=", "on_epoch_end", "\n", "self", ".", "on_batch_end", "=", "on_batch_end", "\n", "\n"]], "home.repos.pwc.inspect_result.Stanford-ILIAD_PantheonRL.algos.bc.EpochOrBatchIteratorWithProgress.__iter__": [[108, 170], ["tqdm.tqdm", "tqdm.tqdm", "tqdm.tqdm.set_description", "contextlib.closing", "bc.EpochOrBatchIteratorWithProgress.__iter__.update_desc"], "methods", ["None"], ["", "def", "__iter__", "(", "self", ")", "->", "Iterable", "[", "Tuple", "[", "dict", ",", "dict", "]", "]", ":", "\n", "        ", "\"\"\"Yields batches while updating tqdm display to display progress.\"\"\"", "\n", "\n", "samples_so_far", "=", "0", "\n", "epoch_num", "=", "0", "\n", "batch_num", "=", "0", "\n", "batch_suffix", "=", "epoch_suffix", "=", "\"\"", "\n", "if", "self", ".", "use_epochs", ":", "\n", "            ", "display", "=", "tqdm", ".", "tqdm", "(", "total", "=", "self", ".", "n_epochs", ")", "\n", "epoch_suffix", "=", "f\"/{self.n_epochs}\"", "\n", "", "else", ":", "# Use batches.", "\n", "            ", "display", "=", "tqdm", ".", "tqdm", "(", "total", "=", "self", ".", "n_batches", ")", "\n", "batch_suffix", "=", "f\"/{self.n_batches}\"", "\n", "\n", "", "def", "update_desc", "(", ")", ":", "\n", "            ", "display", ".", "set_description", "(", "\n", "f\"batch: {batch_num}{batch_suffix}  \\\n                epoch: {epoch_num}{epoch_suffix}\"", "\n", ")", "\n", "\n", "", "with", "contextlib", ".", "closing", "(", "display", ")", ":", "\n", "            ", "while", "True", ":", "\n", "                ", "update_desc", "(", ")", "\n", "got_data_on_epoch", "=", "False", "\n", "for", "batch", "in", "self", ".", "data_loader", ":", "\n", "                    ", "got_data_on_epoch", "=", "True", "\n", "batch_num", "+=", "1", "\n", "batch_size", "=", "len", "(", "batch", "[", "\"obs\"", "]", ")", "\n", "assert", "batch_size", ">", "0", "\n", "samples_so_far", "+=", "batch_size", "\n", "stats", "=", "dict", "(", "\n", "epoch_num", "=", "epoch_num", ",", "\n", "batch_num", "=", "batch_num", ",", "\n", "samples_so_far", "=", "samples_so_far", ",", "\n", ")", "\n", "yield", "batch", ",", "stats", "\n", "if", "self", ".", "on_batch_end", "is", "not", "None", ":", "\n", "                        ", "self", ".", "on_batch_end", "(", ")", "\n", "", "if", "not", "self", ".", "use_epochs", ":", "\n", "                        ", "update_desc", "(", ")", "\n", "display", ".", "update", "(", "1", ")", "\n", "\n", "assert", "self", ".", "n_batches", "is", "not", "None", "\n", "if", "batch_num", ">=", "self", ".", "n_batches", ":", "\n", "                            ", "return", "\n", "", "", "", "if", "not", "got_data_on_epoch", ":", "\n", "                    ", "raise", "AssertionError", "(", "\n", "f\"Data loader returned no data after \"", "\n", "f\"{batch_num} batches, during epoch \"", "\n", "f\"{epoch_num} -- did it reset correctly?\"", "\n", ")", "\n", "", "epoch_num", "+=", "1", "\n", "if", "self", ".", "on_epoch_end", "is", "not", "None", ":", "\n", "                    ", "self", ".", "on_epoch_end", "(", ")", "\n", "\n", "", "if", "self", ".", "use_epochs", ":", "\n", "                    ", "update_desc", "(", ")", "\n", "display", ".", "update", "(", "1", ")", "\n", "\n", "assert", "self", ".", "n_epochs", "is", "not", "None", "\n", "if", "epoch_num", ">=", "self", ".", "n_epochs", ":", "\n", "                        ", "return", "\n", "\n"]], "home.repos.pwc.inspect_result.Stanford-ILIAD_PantheonRL.algos.bc.BC.__init__": [[180, 244], ["stable_baselines3.common.utils.get_device", "dict", "bc.BC.policy_kwargs.update", "stable_baselines3.common.utils.get_device", "bc.BC.policy_class().to", "optimizer_cls", "bc.BC.policy.parameters", "bc.BC.set_expert_data_loader", "ValueError", "bc.ConstantLRSchedule", "bc.BC.policy_class"], "methods", ["home.repos.pwc.inspect_result.Stanford-ILIAD_PantheonRL.common.agents.RecordingAgentWrapper.update", "home.repos.pwc.inspect_result.Stanford-ILIAD_PantheonRL.algos.bc.BC.set_expert_data_loader"], ["def", "__init__", "(", "\n", "self", ",", "\n", "observation_space", ":", "gym", ".", "Space", ",", "\n", "action_space", ":", "gym", ".", "Space", ",", "\n", "*", ",", "\n", "policy_class", ":", "Type", "[", "policies", ".", "BasePolicy", "]", "=", "FeedForward32Policy", ",", "\n", "policy_kwargs", ":", "Optional", "[", "Mapping", "[", "str", ",", "Any", "]", "]", "=", "None", ",", "\n", "expert_data", ":", "Union", "[", "Iterable", "[", "Mapping", "]", ",", "TransitionsMinimal", ",", "None", "]", "=", "None", ",", "\n", "optimizer_cls", ":", "Type", "[", "Optimizer", "]", "=", "Adam", ",", "\n", "optimizer_kwargs", ":", "Optional", "[", "Dict", "[", "str", ",", "Any", "]", "]", "=", "None", ",", "\n", "ent_weight", ":", "float", "=", "1e-3", ",", "\n", "l2_weight", ":", "float", "=", "0.0", ",", "\n", "device", ":", "Union", "[", "str", ",", "th", ".", "device", "]", "=", "\"auto\"", ",", "\n", ")", ":", "\n", "        ", "\"\"\"Behavioral cloning (BC).\n        Recovers a policy via supervised learning on observation-action Tensor\n        pairs, sampled from a Torch DataLoader or any Iterator that ducktypes\n        `torch.utils.data.DataLoader`.\n        Args:\n            observation_space: the observation space of the environment.\n            action_space: the action space of the environment.\n            policy_class: used to instantiate imitation policy.\n            policy_kwargs: keyword arguments passed to policy's constructor.\n            expert_data: If not None, then immediately call\n                  `self.set_expert_data_loader(expert_data)` during\n                  initialization.\n            optimizer_cls: optimiser to use for supervised training.\n            optimizer_kwargs: keyword arguments, excluding learning rate and\n                  weight decay, for optimiser construction.\n            ent_weight: scaling applied to the policy's entropy regularization.\n            l2_weight: scaling applied to the policy's L2 regularization.\n            device: name/identity of device to place policy on.\n        \"\"\"", "\n", "if", "optimizer_kwargs", ":", "\n", "            ", "if", "\"weight_decay\"", "in", "optimizer_kwargs", ":", "\n", "                ", "raise", "ValueError", "(", "\n", "\"Use the parameter l2_weight instead of weight_decay.\"", ")", "\n", "\n", "", "", "self", ".", "action_space", "=", "action_space", "\n", "self", ".", "observation_space", "=", "observation_space", "\n", "self", ".", "policy_class", "=", "policy_class", "\n", "self", ".", "device", "=", "device", "=", "utils", ".", "get_device", "(", "device", ")", "\n", "self", ".", "policy_kwargs", "=", "dict", "(", "\n", "observation_space", "=", "self", ".", "observation_space", ",", "\n", "action_space", "=", "self", ".", "action_space", ",", "\n", "lr_schedule", "=", "ConstantLRSchedule", "(", ")", ",", "\n", ")", "\n", "self", ".", "policy_kwargs", ".", "update", "(", "policy_kwargs", "or", "{", "}", ")", "\n", "self", ".", "device", "=", "utils", ".", "get_device", "(", "device", ")", "\n", "\n", "self", ".", "policy", "=", "self", ".", "policy_class", "(", "**", "self", ".", "policy_kwargs", ")", ".", "to", "(", "\n", "self", ".", "device", "\n", ")", "# pytype: disable=not-instantiable", "\n", "optimizer_kwargs", "=", "optimizer_kwargs", "or", "{", "}", "\n", "\n", "self", ".", "optimizer", "=", "optimizer_cls", "(", "\n", "self", ".", "policy", ".", "parameters", "(", ")", ",", "**", "optimizer_kwargs", ")", "\n", "\n", "self", ".", "expert_data_loader", ":", "Optional", "[", "Iterable", "[", "Mapping", "]", "]", "=", "None", "\n", "self", ".", "ent_weight", "=", "ent_weight", "\n", "self", ".", "l2_weight", "=", "l2_weight", "\n", "\n", "if", "expert_data", "is", "not", "None", ":", "\n", "            ", "self", ".", "set_expert_data_loader", "(", "expert_data", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Stanford-ILIAD_PantheonRL.algos.bc.BC.set_expert_data_loader": [[245, 269], ["isinstance", "torch.DataLoader", "torch.DataLoader"], "methods", ["None"], ["", "", "def", "set_expert_data_loader", "(", "\n", "self", ",", "\n", "expert_data", ":", "Union", "[", "Iterable", "[", "Mapping", "]", ",", "TransitionsMinimal", "]", ",", "\n", ")", "->", "None", ":", "\n", "        ", "\"\"\"Set the expert data loader, which yields batches of obs-act pairs.\n        Changing the expert data loader on-demand is useful for DAgger and\n        other interactive algorithms.\n        Args:\n             expert_data: Either a Torch `DataLoader`, any other iterator that\n                yields dictionaries containing \"obs\" and \"acts\" Tensors or\n                Numpy arrays, or a `TransitionsMinimal` instance.\n                If this is a `TransitionsMinimal` instance, then it is\n                automatically converted into a shuffled `DataLoader` with batch\n                size `BC.DEFAULT_BATCH_SIZE`.\n        \"\"\"", "\n", "if", "isinstance", "(", "expert_data", ",", "TransitionsMinimal", ")", ":", "\n", "            ", "self", ".", "expert_data_loader", "=", "th_data", ".", "DataLoader", "(", "\n", "expert_data", ",", "\n", "shuffle", "=", "True", ",", "\n", "batch_size", "=", "BC", ".", "DEFAULT_BATCH_SIZE", ",", "\n", "collate_fn", "=", "transitions_collate_fn", ",", "\n", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "expert_data_loader", "=", "expert_data", "\n", "\n"]], "home.repos.pwc.inspect_result.Stanford-ILIAD_PantheonRL.algos.bc.BC._calculate_loss": [[270, 316], ["torch.as_tensor().detach", "torch.as_tensor().detach", "torch.as_tensor().detach", "torch.as_tensor().detach", "bc.BC.policy.evaluate_actions", "torch.exp().mean", "torch.exp().mean", "log_prob.mean.mean.mean", "entropy.mean.mean.mean", "dict", "torch.sum", "torch.sum", "sum", "torch.as_tensor", "torch.as_tensor", "torch.as_tensor", "torch.as_tensor", "torch.exp", "torch.exp", "torch.square", "torch.square", "bc.BC.policy.parameters", "neglogp.item", "loss.item", "entropy.mean.mean.item", "ent_loss.item", "torch.exp().mean.item", "l2_norm.item", "l2_loss.item"], "methods", ["home.repos.pwc.inspect_result.Stanford-ILIAD_PantheonRL.modular.policies.ModularPolicy.evaluate_actions"], ["", "", "def", "_calculate_loss", "(", "\n", "self", ",", "\n", "obs", ":", "Union", "[", "th", ".", "Tensor", ",", "np", ".", "ndarray", "]", ",", "\n", "acts", ":", "Union", "[", "th", ".", "Tensor", ",", "np", ".", "ndarray", "]", ",", "\n", ")", "->", "Tuple", "[", "th", ".", "Tensor", ",", "Dict", "[", "str", ",", "float", "]", "]", ":", "\n", "        ", "\"\"\"\n        Calculate the supervised learning loss used to train the behavioral\n        clone.\n        Args:\n            obs: The observations seen by the expert. If this is a Tensor, then\n                gradients are detached first before loss is calculated.\n            acts: The actions taken by the expert. If this is a Tensor, then\n                its gradients are detached first before loss is calculated.\n        Returns:\n            loss: The supervised learning loss for the behavioral clone to\n                optimize.\n            stats_dict: Statistics about the learning process to be logged.\n        \"\"\"", "\n", "obs", "=", "th", ".", "as_tensor", "(", "obs", ",", "device", "=", "self", ".", "device", ")", ".", "detach", "(", ")", "\n", "acts", "=", "th", ".", "as_tensor", "(", "acts", ",", "device", "=", "self", ".", "device", ")", ".", "detach", "(", ")", "\n", "\n", "_", ",", "log_prob", ",", "entropy", "=", "self", ".", "policy", ".", "evaluate_actions", "(", "obs", ",", "acts", ")", "\n", "prob_true_act", "=", "th", ".", "exp", "(", "log_prob", ")", ".", "mean", "(", ")", "\n", "log_prob", "=", "log_prob", ".", "mean", "(", ")", "\n", "entropy", "=", "entropy", ".", "mean", "(", ")", "\n", "\n", "l2_norms", "=", "[", "th", ".", "sum", "(", "th", ".", "square", "(", "w", ")", ")", "for", "w", "in", "self", ".", "policy", ".", "parameters", "(", ")", "]", "\n", "# divide by 2 to cancel with gradient of square", "\n", "l2_norm", "=", "sum", "(", "l2_norms", ")", "/", "2", "\n", "\n", "ent_loss", "=", "-", "self", ".", "ent_weight", "*", "entropy", "\n", "neglogp", "=", "-", "log_prob", "\n", "l2_loss", "=", "self", ".", "l2_weight", "*", "l2_norm", "\n", "loss", "=", "neglogp", "+", "ent_loss", "+", "l2_loss", "\n", "\n", "stats_dict", "=", "dict", "(", "\n", "neglogp", "=", "neglogp", ".", "item", "(", ")", ",", "\n", "loss", "=", "loss", ".", "item", "(", ")", ",", "\n", "entropy", "=", "entropy", ".", "item", "(", ")", ",", "\n", "ent_loss", "=", "ent_loss", ".", "item", "(", ")", ",", "\n", "prob_true_act", "=", "prob_true_act", ".", "item", "(", ")", ",", "\n", "l2_norm", "=", "l2_norm", ".", "item", "(", ")", ",", "\n", "l2_loss", "=", "l2_loss", ".", "item", "(", ")", ",", "\n", ")", "\n", "\n", "return", "loss", ",", "stats_dict", "\n", "\n"]], "home.repos.pwc.inspect_result.Stanford-ILIAD_PantheonRL.algos.bc.BC.train": [[317, 364], ["bc.EpochOrBatchIteratorWithProgress", "bc.BC._calculate_loss", "bc.BC.optimizer.zero_grad", "loss.backward", "bc.BC.optimizer.step", "log.dump", "stats.items", "log.record"], "methods", ["home.repos.pwc.inspect_result.Stanford-ILIAD_PantheonRL.algos.bc.BC._calculate_loss", "home.repos.pwc.inspect_result.Stanford-ILIAD_PantheonRL.common.multiagentenv.MultiAgentEnv.step"], ["", "def", "train", "(", "\n", "self", ",", "\n", "*", ",", "\n", "n_epochs", ":", "Optional", "[", "int", "]", "=", "None", ",", "\n", "n_batches", ":", "Optional", "[", "int", "]", "=", "None", ",", "\n", "on_epoch_end", ":", "Callable", "[", "[", "]", ",", "None", "]", "=", "None", ",", "\n", "on_batch_end", ":", "Callable", "[", "[", "]", ",", "None", "]", "=", "None", ",", "\n", "log_interval", ":", "int", "=", "100", ",", "\n", ")", ":", "\n", "        ", "\"\"\"Train with supervised learning for some number of epochs.\n        Here an 'epoch' is just a complete pass through the expert data loader,\n        as set by `self.set_expert_data_loader()`.\n        Args:\n            n_epochs: Number of complete passes made through expert data before\n                ending training. Provide exactly one of `n_epochs` and\n                `n_batches`.\n            n_batches: Number of batches loaded from dataset before ending\n                training. Provide exactly one of `n_epochs` and `n_batches`.\n            on_epoch_end: Optional callback with no parameters to run at the\n                end of each epoch.\n            on_batch_end: Optional callback with no parameters to run at the\n                end of each batch.\n            log_interval: Log stats after every log_interval batches.\n        \"\"\"", "\n", "it", "=", "EpochOrBatchIteratorWithProgress", "(", "\n", "self", ".", "expert_data_loader", ",", "\n", "n_epochs", "=", "n_epochs", ",", "\n", "n_batches", "=", "n_batches", ",", "\n", "on_epoch_end", "=", "on_epoch_end", ",", "\n", "on_batch_end", "=", "on_batch_end", ",", "\n", ")", "\n", "\n", "batch_num", "=", "0", "\n", "for", "batch", ",", "stats_dict_it", "in", "it", ":", "\n", "            ", "loss", ",", "stats_dict_loss", "=", "self", ".", "_calculate_loss", "(", "\n", "batch", "[", "\"obs\"", "]", ",", "batch", "[", "\"acts\"", "]", ")", "\n", "\n", "self", ".", "optimizer", ".", "zero_grad", "(", ")", "\n", "loss", ".", "backward", "(", ")", "\n", "self", ".", "optimizer", ".", "step", "(", ")", "\n", "\n", "if", "batch_num", "%", "log_interval", "==", "0", ":", "\n", "                ", "for", "stats", "in", "[", "stats_dict_it", ",", "stats_dict_loss", "]", ":", "\n", "                    ", "for", "k", ",", "v", "in", "stats", ".", "items", "(", ")", ":", "\n", "                        ", "log", ".", "record", "(", "k", ",", "v", ")", "\n", "", "", "log", ".", "dump", "(", "batch_num", ")", "\n", "", "batch_num", "+=", "1", "\n", "\n"]], "home.repos.pwc.inspect_result.Stanford-ILIAD_PantheonRL.algos.bc.BC.save_policy": [[365, 371], ["torch.save", "torch.save"], "methods", ["None"], ["", "", "def", "save_policy", "(", "self", ",", "policy_path", ":", "str", ")", "->", "None", ":", "\n", "        ", "\"\"\"Save policy to a path. Can be reloaded by `.reconstruct_policy()`.\n        Args:\n            policy_path: path to save policy to.\n        \"\"\"", "\n", "th", ".", "save", "(", "self", ".", "policy", ",", "policy_path", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.Stanford-ILIAD_PantheonRL.algos.bc.reconstruct_policy": [[34, 48], ["torch.load", "isinstance", "stable_baselines3.common.utils.get_device"], "function", ["None"], ["", "", "def", "reconstruct_policy", "(", "\n", "policy_path", ":", "str", ",", "\n", "device", ":", "Union", "[", "th", ".", "device", ",", "str", "]", "=", "\"auto\"", ",", "\n", ")", "->", "policies", ".", "BasePolicy", ":", "\n", "    ", "\"\"\"Reconstruct a saved policy.\n    Args:\n        policy_path: path where `.save_policy()` has been run.\n        device: device on which to load the policy.\n    Returns:\n        policy: policy with reloaded weights.\n    \"\"\"", "\n", "policy", "=", "th", ".", "load", "(", "policy_path", ",", "map_location", "=", "utils", ".", "get_device", "(", "device", ")", ")", "\n", "assert", "isinstance", "(", "policy", ",", "policies", ".", "BasePolicy", ")", "\n", "return", "policy", "\n", "\n"]], "home.repos.pwc.inspect_result.Stanford-ILIAD_PantheonRL.adap.policies.AdapPolicy.__init__": [[22, 63], ["stable_baselines3.common.policies.ActorCriticPolicy.__init__"], "methods", ["home.repos.pwc.inspect_result.Stanford-ILIAD_PantheonRL.overcookedgym.overcooked.OvercookedMultiEnv.__init__"], ["    ", "def", "__init__", "(", "\n", "self", ",", "\n", "observation_space", ":", "gym", ".", "spaces", ".", "Space", ",", "\n", "action_space", ":", "gym", ".", "spaces", ".", "Space", ",", "\n", "lr_schedule", ":", "Schedule", ",", "\n", "net_arch", ":", "Optional", "[", "List", "[", "Union", "[", "int", ",", "Dict", "[", "str", ",", "List", "[", "int", "]", "]", "]", "]", "]", "=", "None", ",", "\n", "activation_fn", ":", "Type", "[", "nn", ".", "Module", "]", "=", "nn", ".", "Tanh", ",", "\n", "ortho_init", ":", "bool", "=", "True", ",", "\n", "use_sde", ":", "bool", "=", "False", ",", "\n", "log_std_init", ":", "float", "=", "0.0", ",", "\n", "full_std", ":", "bool", "=", "True", ",", "\n", "sde_net_arch", ":", "Optional", "[", "List", "[", "int", "]", "]", "=", "None", ",", "\n", "use_expln", ":", "bool", "=", "False", ",", "\n", "squash_output", ":", "bool", "=", "False", ",", "\n", "features_extractor_class", ":", "Type", "[", "BaseFeaturesExtractor", "]", "=", "\n", "FlattenExtractor", ",", "\n", "features_extractor_kwargs", ":", "Optional", "[", "Dict", "[", "str", ",", "Any", "]", "]", "=", "None", ",", "\n", "normalize_images", ":", "bool", "=", "True", ",", "\n", "optimizer_class", ":", "Type", "[", "Optimizer", "]", "=", "Adam", ",", "\n", "optimizer_kwargs", ":", "Optional", "[", "Dict", "[", "str", ",", "Any", "]", "]", "=", "None", ",", "\n", "context_size", ":", "int", "=", "3", "\n", ")", ":", "\n", "        ", "self", ".", "context_size", "=", "context_size", "\n", "super", "(", ")", ".", "__init__", "(", "\n", "observation_space", "=", "observation_space", ",", "\n", "action_space", "=", "action_space", ",", "\n", "lr_schedule", "=", "lr_schedule", ",", "\n", "net_arch", "=", "net_arch", ",", "\n", "activation_fn", "=", "activation_fn", ",", "\n", "ortho_init", "=", "ortho_init", ",", "\n", "use_sde", "=", "use_sde", ",", "\n", "log_std_init", "=", "log_std_init", ",", "\n", "full_std", "=", "full_std", ",", "\n", "sde_net_arch", "=", "sde_net_arch", ",", "\n", "use_expln", "=", "use_expln", ",", "\n", "squash_output", "=", "squash_output", ",", "\n", "features_extractor_class", "=", "features_extractor_class", ",", "\n", "features_extractor_kwargs", "=", "features_extractor_kwargs", ",", "\n", "normalize_images", "=", "normalize_images", ",", "\n", "optimizer_class", "=", "optimizer_class", ",", "\n", "optimizer_kwargs", "=", "optimizer_kwargs", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.Stanford-ILIAD_PantheonRL.adap.policies.AdapPolicy.set_context": [[65, 67], ["None"], "methods", ["None"], ["", "def", "set_context", "(", "self", ",", "ctxt", ")", ":", "\n", "        ", "self", ".", "context", "=", "ctxt", "\n", "\n"]], "home.repos.pwc.inspect_result.Stanford-ILIAD_PantheonRL.adap.policies.AdapPolicy.get_context": [[68, 70], ["None"], "methods", ["None"], ["", "def", "get_context", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "context", "\n", "\n"]], "home.repos.pwc.inspect_result.Stanford-ILIAD_PantheonRL.adap.policies.AdapPolicy._build_mlp_extractor": [[71, 84], ["stable_baselines3.common.torch_layers.MlpExtractor"], "methods", ["None"], ["", "def", "_build_mlp_extractor", "(", "self", ")", "->", "None", ":", "\n", "        ", "\"\"\"\n        Create the policy and value networks.\n        Part of the layers can be shared.\n        \"\"\"", "\n", "# Note: If net_arch is None and some features extractor is used,", "\n", "#       net_arch here is an empty list and mlp_extractor does not", "\n", "#       really contain any layers (acts like an identity module).", "\n", "self", ".", "mlp_extractor", "=", "MlpExtractor", "(", "\n", "self", ".", "features_dim", "+", "self", ".", "context_size", ",", "\n", "net_arch", "=", "self", ".", "net_arch", ",", "\n", "activation_fn", "=", "self", ".", "activation_fn", ",", "\n", "device", "=", "self", ".", "device", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.Stanford-ILIAD_PantheonRL.adap.policies.AdapPolicy._get_latent": [[86, 107], ["policies.AdapPolicy.extract_features", "torch.cat", "policies.AdapPolicy.mlp_extractor", "policies.AdapPolicy.sde_features_extractor", "policies.AdapPolicy.context.repeat", "torch.cat.size"], "methods", ["None"], ["", "def", "_get_latent", "(", "self", ",", "\n", "obs", ":", "th", ".", "Tensor", ")", "->", "Tuple", "[", "th", ".", "Tensor", ",", "th", ".", "Tensor", ",", "th", ".", "Tensor", "]", ":", "\n", "        ", "\"\"\"\n        Get the latent code (activations of the last layer of each network)\n        for the different networks.\n        :param obs: Observation\n        :return: Latent codes\n            for the actor, the value function and for gSDE function\n        \"\"\"", "\n", "# Preprocess the observation if needed", "\n", "features", "=", "self", ".", "extract_features", "(", "obs", ")", "\n", "features", "=", "th", ".", "cat", "(", "\n", "(", "features", ",", "self", ".", "context", ".", "repeat", "(", "features", ".", "size", "(", ")", "[", "0", "]", ",", "1", ")", ")", ",", "\n", "dim", "=", "1", ")", "\n", "latent_pi", ",", "latent_vf", "=", "self", ".", "mlp_extractor", "(", "features", ")", "\n", "\n", "# Features for sde", "\n", "latent_sde", "=", "latent_pi", "\n", "if", "self", ".", "sde_features_extractor", "is", "not", "None", ":", "\n", "            ", "latent_sde", "=", "self", ".", "sde_features_extractor", "(", "features", ")", "\n", "", "return", "latent_pi", ",", "latent_vf", ",", "latent_sde", "\n", "\n"]], "home.repos.pwc.inspect_result.Stanford-ILIAD_PantheonRL.adap.policies.AdapPolicy.evaluate_actions": [[108, 134], ["policies.AdapPolicy.extract_features", "torch.cat", "policies.AdapPolicy.mlp_extractor", "policies.AdapPolicy._get_action_dist_from_latent", "policies.AdapPolicy.log_prob", "policies.AdapPolicy.value_net", "policies.AdapPolicy.sde_features_extractor", "policies.AdapPolicy.entropy"], "methods", ["home.repos.pwc.inspect_result.Stanford-ILIAD_PantheonRL.modular.policies.ModularPolicy._get_action_dist_from_latent"], ["", "def", "evaluate_actions", "(", "self", ",", "\n", "obs", ":", "th", ".", "Tensor", ",", "\n", "actions", ":", "th", ".", "Tensor", "\n", ")", "->", "Tuple", "[", "th", ".", "Tensor", ",", "th", ".", "Tensor", ",", "th", ".", "Tensor", "]", ":", "\n", "        ", "\"\"\"\n        Evaluate actions according to the current policy,\n        given the observations.\n        :param obs:\n        :param actions:\n        :return: estimated value, log likelihood of taking those actions\n            and entropy of the action distribution.\n        \"\"\"", "\n", "features", "=", "self", ".", "extract_features", "(", "obs", "[", ":", ",", ":", "-", "self", ".", "context_size", "]", ")", "\n", "features", "=", "th", ".", "cat", "(", "\n", "(", "features", ",", "obs", "[", ":", ",", "-", "self", ".", "context_size", ":", "]", ")", ",", "\n", "dim", "=", "1", ")", "\n", "latent_pi", ",", "latent_vf", "=", "self", ".", "mlp_extractor", "(", "features", ")", "\n", "\n", "# Features for sde", "\n", "latent_sde", "=", "latent_pi", "\n", "if", "self", ".", "sde_features_extractor", "is", "not", "None", ":", "\n", "            ", "latent_sde", "=", "self", ".", "sde_features_extractor", "(", "features", ")", "\n", "", "distribution", "=", "self", ".", "_get_action_dist_from_latent", "(", "latent_pi", ",", "latent_sde", ")", "\n", "log_prob", "=", "distribution", ".", "log_prob", "(", "actions", ")", "\n", "values", "=", "self", ".", "value_net", "(", "latent_vf", ")", "\n", "return", "values", ",", "log_prob", ",", "distribution", ".", "entropy", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Stanford-ILIAD_PantheonRL.adap.policies.MultModel.__init__": [[137, 225], ["torch.nn.Module.__init__", "stable_baselines3.common.utils.get_device", "itertools.zip_longest", "torch.nn.Sequential().to", "torch.nn.Sequential().to", "torch.nn.Sequential().to", "torch.nn.Sequential().to", "torch.nn.Sequential().to", "torch.nn.Sequential().to", "torch.nn.Sequential().to", "isinstance", "shared_net.append", "shared_net.append", "isinstance", "isinstance", "policy_net.append", "policy_net.append", "isinstance", "value_net.append", "value_net.append", "torch.nn.Sequential", "torch.nn.Sequential", "torch.nn.Sequential", "torch.nn.Sequential", "torch.nn.Sequential", "torch.nn.Sequential", "torch.nn.Sequential", "torch.nn.Linear", "activation_fn", "isinstance", "isinstance", "torch.nn.Linear", "activation_fn", "torch.nn.Linear", "activation_fn", "torch.nn.Linear", "activation_fn", "torch.nn.Linear", "activation_fn"], "methods", ["home.repos.pwc.inspect_result.Stanford-ILIAD_PantheonRL.overcookedgym.overcooked.OvercookedMultiEnv.__init__"], ["    ", "def", "__init__", "(", "\n", "self", ",", "\n", "feature_dim", ",", "\n", "net_arch", ",", "\n", "activation_fn", ",", "\n", "device", ",", "\n", "context_size", "\n", ")", ":", "\n", "        ", "nn", ".", "Module", ".", "__init__", "(", "self", ")", "\n", "\n", "self", ".", "obs_space_size", "=", "feature_dim", "+", "context_size", "\n", "self", ".", "context_size", "=", "context_size", "\n", "\n", "device", "=", "get_device", "(", "device", ")", "\n", "shared_net", ",", "policy_net", ",", "value_net", "=", "[", "]", ",", "[", "]", ",", "[", "]", "\n", "# Layer sizes of the network that only belongs to the policy network", "\n", "policy_only_layers", "=", "[", "]", "\n", "# Layer sizes of the network that only belongs to the value network", "\n", "value_only_layers", "=", "[", "]", "\n", "last_layer_dim_shared", "=", "feature_dim", "\n", "\n", "# Iterate through shared layers and build shared parts of the network", "\n", "for", "layer", "in", "net_arch", ":", "\n", "            ", "if", "isinstance", "(", "layer", ",", "int", ")", ":", "# Check that this is a shared layer", "\n", "# TODO: give layer a meaningful name", "\n", "# add linear of size layer", "\n", "                ", "shared_net", ".", "append", "(", "nn", ".", "Linear", "(", "last_layer_dim_shared", ",", "layer", ")", ")", "\n", "shared_net", ".", "append", "(", "activation_fn", "(", ")", ")", "\n", "last_layer_dim_shared", "=", "layer", "\n", "", "else", ":", "\n", "                ", "assert", "isinstance", "(", "layer", ",", "dict", ")", ",", "\"Error: the net_arch list can only contain ints and dicts\"", "\n", "if", "\"pi\"", "in", "layer", ":", "\n", "                    ", "assert", "isinstance", "(", "layer", "[", "\"pi\"", "]", ",", "list", ")", ",", "\"Error: net_arch[-1]['pi'] must \\\n                        contain a list of integers.\"", "\n", "policy_only_layers", "=", "layer", "[", "\"pi\"", "]", "\n", "\n", "", "if", "\"vf\"", "in", "layer", ":", "\n", "                    ", "assert", "isinstance", "(", "layer", "[", "\"vf\"", "]", ",", "list", ")", ",", "\"Error: net_arch[-1]['vf'] must \\\n                        contain a list of integers.\"", "\n", "value_only_layers", "=", "layer", "[", "\"vf\"", "]", "\n", "", "break", "\n", "\n", "", "", "last_layer_dim_pi", "=", "last_layer_dim_shared", "\n", "last_layer_dim_vf", "=", "last_layer_dim_shared", "\n", "\n", "# Build the non-shared part of the network", "\n", "for", "pi_layer_size", ",", "vf_layer_size", "in", "zip_longest", "(", "policy_only_layers", ",", "\n", "value_only_layers", ")", ":", "\n", "            ", "if", "pi_layer_size", "is", "not", "None", ":", "\n", "                ", "assert", "isinstance", "(", "pi_layer_size", ",", "int", ")", ",", "\"Error: net_arch[-1]['pi'] must only contain integers.\"", "\n", "policy_net", ".", "append", "(", "nn", ".", "Linear", "(", "last_layer_dim_pi", ",", "pi_layer_size", ")", ")", "\n", "policy_net", ".", "append", "(", "activation_fn", "(", ")", ")", "\n", "last_layer_dim_pi", "=", "pi_layer_size", "\n", "\n", "", "if", "vf_layer_size", "is", "not", "None", ":", "\n", "                ", "assert", "isinstance", "(", "vf_layer_size", ",", "int", ")", ",", "\"Error: net_arch[-1]['vf'] must only contain integers.\"", "\n", "value_net", ".", "append", "(", "nn", ".", "Linear", "(", "last_layer_dim_vf", ",", "vf_layer_size", ")", ")", "\n", "value_net", ".", "append", "(", "activation_fn", "(", ")", ")", "\n", "last_layer_dim_vf", "=", "vf_layer_size", "\n", "\n", "# Save dim, used to create the distributions", "\n", "", "", "self", ".", "latent_dim_pi", "=", "last_layer_dim_pi", "\n", "self", ".", "latent_dim_vf", "=", "last_layer_dim_vf", "\n", "\n", "# Create networks", "\n", "# If list of layers is empty, the network is an Identity module", "\n", "self", ".", "shared_net", "=", "nn", ".", "Sequential", "(", "*", "shared_net", ")", ".", "to", "(", "device", ")", "\n", "\n", "self", ".", "hidden_dim1", "=", "policy_net", "[", "0", "]", ".", "out_features", "\n", "self", ".", "agent_branch_1", "=", "nn", ".", "Sequential", "(", "*", "policy_net", "[", "0", ":", "2", "]", ")", ".", "to", "(", "device", ")", "\n", "self", ".", "agent_scaling", "=", "nn", ".", "Sequential", "(", "\n", "nn", ".", "Linear", "(", "self", ".", "hidden_dim1", ",", "self", ".", "hidden_dim1", "*", "self", ".", "context_size", ")", ",", "\n", "activation_fn", "(", ")", "\n", ")", ".", "to", "(", "device", ")", "\n", "self", ".", "agent_branch_2", "=", "nn", ".", "Sequential", "(", "*", "policy_net", "[", "2", ":", "]", ")", ".", "to", "(", "device", ")", "\n", "\n", "self", ".", "hidden_dim2", "=", "value_net", "[", "0", "]", ".", "out_features", "\n", "self", ".", "value_branch_1", "=", "nn", ".", "Sequential", "(", "*", "value_net", "[", "0", ":", "2", "]", ")", ".", "to", "(", "device", ")", "\n", "self", ".", "value_scaling", "=", "nn", ".", "Sequential", "(", "\n", "nn", ".", "Linear", "(", "self", ".", "hidden_dim2", ",", "self", ".", "hidden_dim2", "*", "self", ".", "context_size", ")", ",", "\n", "activation_fn", "(", ")", "\n", ")", ".", "to", "(", "device", ")", "\n", "self", ".", "value_branch_2", "=", "nn", ".", "Sequential", "(", "*", "value_net", "[", "2", ":", "]", ")", ".", "to", "(", "device", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Stanford-ILIAD_PantheonRL.adap.policies.MultModel.get_input_size_excluding_ctx": [[226, 228], ["None"], "methods", ["None"], ["", "def", "get_input_size_excluding_ctx", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "obs_space_size", "-", "self", ".", "context_size", "\n", "\n"]], "home.repos.pwc.inspect_result.Stanford-ILIAD_PantheonRL.adap.policies.MultModel.get_input_size_inluding_ctx": [[229, 231], ["None"], "methods", ["None"], ["", "def", "get_input_size_inluding_ctx", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "obs_space_size", "\n", "\n"]], "home.repos.pwc.inspect_result.Stanford-ILIAD_PantheonRL.adap.policies.MultModel.policies": [[232, 244], ["policies.MultModel.agent_branch_1", "policies.MultModel.agent_scaling", "x_a.view.view.view", "torch.matmul().squeeze", "policies.MultModel.agent_branch_2", "torch.matmul", "contexts.unsqueeze"], "methods", ["None"], ["", "def", "policies", "(", "self", ",", "observations", ":", "th", ".", "Tensor", ",", "\n", "contexts", ":", "th", ".", "Tensor", ")", "->", "th", ".", "Tensor", ":", "\n", "\n", "        ", "batch_size", "=", "observations", ".", "shape", "[", "0", "]", "\n", "x", "=", "self", ".", "agent_branch_1", "(", "observations", ")", "\n", "x_a", "=", "self", ".", "agent_scaling", "(", "x", ")", "\n", "# reshape to do context multiplication", "\n", "x_a", "=", "x_a", ".", "view", "(", "(", "batch_size", ",", "self", ".", "hidden_dim1", ",", "self", ".", "context_size", ")", ")", "\n", "x_a_out", "=", "th", ".", "matmul", "(", "x_a", ",", "contexts", ".", "unsqueeze", "(", "-", "1", ")", ")", ".", "squeeze", "(", "-", "1", ")", "\n", "logits", "=", "self", ".", "agent_branch_2", "(", "x", "+", "x_a_out", ")", "\n", "\n", "return", "logits", "\n", "\n"]], "home.repos.pwc.inspect_result.Stanford-ILIAD_PantheonRL.adap.policies.MultModel.values": [[245, 258], ["policies.MultModel.value_branch_1", "policies.MultModel.value_scaling", "x_a.view.view.view", "torch.matmul().squeeze", "policies.MultModel.value_branch_2", "torch.matmul", "contexts.unsqueeze"], "methods", ["None"], ["", "def", "values", "(", "self", ",", "observations", ":", "th", ".", "Tensor", ",", "\n", "contexts", ":", "th", ".", "Tensor", ")", "->", "th", ".", "Tensor", ":", "\n", "\n", "        ", "batch_size", "=", "observations", ".", "shape", "[", "0", "]", "\n", "x", "=", "self", ".", "value_branch_1", "(", "observations", ")", "\n", "x_a", "=", "self", ".", "value_scaling", "(", "x", ")", "\n", "# reshape to do context multiplication", "\n", "x_a", "=", "x_a", ".", "view", "(", "(", "batch_size", ",", "self", ".", "hidden_dim2", ",", "self", ".", "context_size", ")", ")", "\n", "x_a_out", "=", "th", ".", "matmul", "(", "x_a", ",", "contexts", ".", "unsqueeze", "(", "-", "1", ")", ")", ".", "squeeze", "(", "-", "1", ")", "\n", "values", "=", "self", ".", "value_branch_2", "(", "x", "+", "x_a_out", ")", "\n", "# values = self.value_branch_2(x_a_out)", "\n", "\n", "return", "values", "\n", "\n"]], "home.repos.pwc.inspect_result.Stanford-ILIAD_PantheonRL.adap.policies.MultModel.forward": [[259, 265], ["policies.MultModel.shared_net", "policies.MultModel.policies", "policies.MultModel.values"], "methods", ["home.repos.pwc.inspect_result.Stanford-ILIAD_PantheonRL.adap.policies.MultModel.policies", "home.repos.pwc.inspect_result.Stanford-ILIAD_PantheonRL.adap.policies.MultModel.values"], ["", "def", "forward", "(", "self", ",", "features", ":", "th", ".", "Tensor", ")", "->", "Tuple", "[", "th", ".", "Tensor", ",", "th", ".", "Tensor", "]", ":", "\n", "        ", "features", "=", "self", ".", "shared_net", "(", "features", ")", "\n", "observations", "=", "features", "[", ":", ",", ":", "-", "self", ".", "context_size", "]", "\n", "contexts", "=", "features", "[", ":", ",", "-", "self", ".", "context_size", ":", "]", "\n", "return", "self", ".", "policies", "(", "observations", ",", "contexts", ")", ",", "self", ".", "values", "(", "observations", ",", "contexts", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Stanford-ILIAD_PantheonRL.adap.policies.AdapPolicyMult._build_mlp_extractor": [[269, 283], ["policies.MultModel"], "methods", ["None"], ["    ", "def", "_build_mlp_extractor", "(", "self", ")", "->", "None", ":", "\n", "        ", "\"\"\"\n        Create the policy and value networks.\n        Part of the layers can be shared.\n        \"\"\"", "\n", "# Note: If net_arch is None and some features extractor is used,", "\n", "#       net_arch here is an empty list and mlp_extractor does not", "\n", "#       really contain any layers (acts like an identity module).", "\n", "self", ".", "mlp_extractor", "=", "MultModel", "(", "\n", "self", ".", "features_dim", ",", "\n", "net_arch", "=", "self", ".", "net_arch", ",", "\n", "activation_fn", "=", "self", ".", "activation_fn", ",", "\n", "device", "=", "self", ".", "device", ",", "\n", "context_size", "=", "self", ".", "context_size", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.Stanford-ILIAD_PantheonRL.adap.agent.AdapAgent.__init__": [[30, 57], ["torch.empty", "agent.AdapAgent.model.set_logger", "collections.deque", "buf.reset", "stable_baselines3.common.utils.configure_logger"], "methods", ["home.repos.pwc.inspect_result.Stanford-ILIAD_PantheonRL.common.multiagentenv.MultiAgentEnv.reset"], ["def", "__init__", "(", "self", ",", "\n", "model", ":", "ADAP", ",", "\n", "log_interval", "=", "None", ",", "\n", "tensorboard_log", "=", "None", ",", "\n", "tb_log_name", "=", "\"AdapAgent\"", ",", "\n", "latent_syncer", ":", "Optional", "[", "AdapPolicy", "]", "=", "None", ")", ":", "\n", "        ", "self", ".", "model", "=", "model", "\n", "self", ".", "_last_episode_starts", "=", "[", "True", "]", "\n", "self", ".", "n_steps", "=", "0", "\n", "self", ".", "values", ":", "th", ".", "Tensor", "=", "th", ".", "empty", "(", "0", ")", "\n", "\n", "self", ".", "model", ".", "set_logger", "(", "configure_logger", "(", "\n", "self", ".", "model", ".", "verbose", ",", "tensorboard_log", ",", "tb_log_name", ")", ")", "\n", "\n", "self", ".", "name", "=", "tb_log_name", "\n", "self", ".", "num_timesteps", "=", "0", "\n", "self", ".", "log_interval", "=", "log_interval", "or", "(", "1", "if", "model", ".", "verbose", "else", "None", ")", "\n", "self", ".", "iteration", "=", "0", "\n", "self", ".", "model", ".", "ep_info_buffer", "=", "deque", "(", "[", "{", "\"r\"", ":", "0", ",", "\"l\"", ":", "0", "}", "]", ",", "maxlen", "=", "100", ")", "\n", "\n", "self", ".", "latent_syncer", "=", "latent_syncer", "\n", "\n", "buf", "=", "self", ".", "model", ".", "rollout_buffer", "\n", "self", ".", "model", ".", "full_obs_shape", "=", "(", "\n", "buf", ".", "obs_shape", "[", "0", "]", "+", "self", ".", "model", ".", "context_size", ",", ")", "\n", "buf", ".", "obs_shape", "=", "self", ".", "model", ".", "full_obs_shape", "\n", "buf", ".", "reset", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Stanford-ILIAD_PantheonRL.adap.agent.AdapAgent.get_action": [[58, 133], ["pantheonrl.common.util.resample_noise", "pantheonrl.common.util.action_from_policy", "numpy.concatenate", "agent.AdapAgent.model.policy.set_context", "buf.compute_returns_and_advantage", "agent.AdapAgent.model.train", "buf.reset", "buf.add", "pantheonrl.common.util.clip_actions", "agent.AdapAgent.latent_syncer.get_context", "agent.AdapAgent.model.logger.record", "agent.AdapAgent.model.logger.record", "agent.AdapAgent.model.logger.record", "agent.AdapAgent.model.logger.dump", "numpy.reshape", "agent.AdapAgent.model.policy.get_context", "numpy.reshape", "numpy.reshape", "agent.AdapAgent.model.ep_info_buffer.pop", "agent.AdapAgent.model.logger.record", "agent.AdapAgent.model.logger.record", "agent.AdapAgent.model.ep_info_buffer.append", "len", "len", "stable_baselines3.common.utils.safe_mean", "stable_baselines3.common.utils.safe_mean"], "methods", ["home.repos.pwc.inspect_result.Stanford-ILIAD_PantheonRL.common.util.resample_noise", "home.repos.pwc.inspect_result.Stanford-ILIAD_PantheonRL.common.util.action_from_policy", "home.repos.pwc.inspect_result.Stanford-ILIAD_PantheonRL.adap.policies.AdapPolicy.set_context", "home.repos.pwc.inspect_result.Stanford-ILIAD_PantheonRL.modular.learn.ModularAlgorithm.train", "home.repos.pwc.inspect_result.Stanford-ILIAD_PantheonRL.common.multiagentenv.MultiAgentEnv.reset", "home.repos.pwc.inspect_result.Stanford-ILIAD_PantheonRL.common.wrappers.HistoryQueue.add", "home.repos.pwc.inspect_result.Stanford-ILIAD_PantheonRL.common.util.clip_actions", "home.repos.pwc.inspect_result.Stanford-ILIAD_PantheonRL.adap.policies.AdapPolicy.get_context", "home.repos.pwc.inspect_result.Stanford-ILIAD_PantheonRL.adap.policies.AdapPolicy.get_context"], ["", "def", "get_action", "(", "self", ",", "obs", ":", "np", ".", "ndarray", ",", "record", ":", "bool", "=", "True", ")", "->", "np", ".", "ndarray", ":", "\n", "        ", "\"\"\"\n        Return an action given an observation.\n\n        When `record` is True, the agent saves the last transition into its\n        buffer. It also updates the model if the buffer is full.\n\n        :param obs: The observation to use\n        :param record: Whether to record the obs, action (True when training)\n        :returns: The action to take\n        \"\"\"", "\n", "if", "self", ".", "latent_syncer", "is", "not", "None", ":", "\n", "            ", "self", ".", "model", ".", "policy", ".", "set_context", "(", "self", ".", "latent_syncer", ".", "get_context", "(", ")", ")", "\n", "\n", "", "buf", "=", "self", ".", "model", ".", "rollout_buffer", "\n", "\n", "# train the model if the buffer is full", "\n", "if", "record", "and", "self", ".", "n_steps", ">=", "self", ".", "model", ".", "n_steps", ":", "\n", "            ", "buf", ".", "compute_returns_and_advantage", "(", "\n", "last_values", "=", "self", ".", "values", ",", "\n", "dones", "=", "self", ".", "_last_episode_starts", "[", "0", "]", "\n", ")", "\n", "\n", "if", "self", ".", "log_interval", "is", "not", "None", "and", "self", ".", "iteration", "%", "self", ".", "log_interval", "==", "0", ":", "\n", "                ", "self", ".", "model", ".", "logger", ".", "record", "(", "\n", "\"name\"", ",", "self", ".", "name", ",", "exclude", "=", "\"tensorboard\"", ")", "\n", "self", ".", "model", ".", "logger", ".", "record", "(", "\n", "\"time/iterations\"", ",", "self", ".", "iteration", ",", "exclude", "=", "\"tensorboard\"", ")", "\n", "\n", "if", "len", "(", "self", ".", "model", ".", "ep_info_buffer", ")", ">", "0", "and", "len", "(", "self", ".", "model", ".", "ep_info_buffer", "[", "0", "]", ")", ">", "0", ":", "\n", "                    ", "last_exclude", "=", "self", ".", "model", ".", "ep_info_buffer", ".", "pop", "(", ")", "\n", "rews", "=", "[", "ep", "[", "\"r\"", "]", "for", "ep", "in", "self", ".", "model", ".", "ep_info_buffer", "]", "\n", "lens", "=", "[", "ep", "[", "\"l\"", "]", "for", "ep", "in", "self", ".", "model", ".", "ep_info_buffer", "]", "\n", "self", ".", "model", ".", "logger", ".", "record", "(", "\n", "\"rollout/ep_rew_mean\"", ",", "safe_mean", "(", "rews", ")", ")", "\n", "self", ".", "model", ".", "logger", ".", "record", "(", "\n", "\"rollout/ep_len_mean\"", ",", "safe_mean", "(", "lens", ")", ")", "\n", "self", ".", "model", ".", "ep_info_buffer", ".", "append", "(", "last_exclude", ")", "\n", "\n", "", "self", ".", "model", ".", "logger", ".", "record", "(", "\n", "\"time/total_timesteps\"", ",", "self", ".", "num_timesteps", ",", "\n", "exclude", "=", "\"tensorboard\"", ")", "\n", "self", ".", "model", ".", "logger", ".", "dump", "(", "step", "=", "self", ".", "num_timesteps", ")", "\n", "\n", "", "self", ".", "model", ".", "train", "(", ")", "\n", "self", ".", "iteration", "+=", "1", "\n", "buf", ".", "reset", "(", ")", "\n", "self", ".", "n_steps", "=", "0", "\n", "\n", "", "resample_noise", "(", "self", ".", "model", ",", "self", ".", "n_steps", ")", "\n", "\n", "actions", ",", "values", ",", "log_probs", "=", "action_from_policy", "(", "obs", ",", "self", ".", "model", ".", "policy", ")", "\n", "\n", "# modify the rollout buffer with newest info", "\n", "obs", "=", "np", ".", "concatenate", "(", "(", "np", ".", "reshape", "(", "obs", ",", "(", "1", ",", "-", "1", ")", ")", ",", "\n", "self", ".", "model", ".", "policy", ".", "get_context", "(", ")", ")", ",", "\n", "axis", "=", "1", ")", "\n", "if", "record", ":", "\n", "            ", "obs_shape", "=", "self", ".", "model", ".", "policy", ".", "observation_space", ".", "shape", "\n", "act_shape", "=", "self", ".", "model", ".", "policy", ".", "action_space", ".", "shape", "\n", "buf", ".", "add", "(", "\n", "np", ".", "reshape", "(", "obs", ",", "(", "1", ",", ")", "+", "obs_shape", ")", ",", "\n", "np", ".", "reshape", "(", "actions", ",", "(", "1", ",", ")", "+", "act_shape", ")", ",", "\n", "[", "0", "]", ",", "\n", "self", ".", "_last_episode_starts", ",", "\n", "values", ",", "\n", "log_probs", "\n", ")", "\n", "\n", "", "self", ".", "n_steps", "+=", "1", "\n", "self", ".", "num_timesteps", "+=", "1", "\n", "self", ".", "values", "=", "values", "\n", "return", "clip_actions", "(", "actions", ",", "self", ".", "model", ")", "[", "0", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.Stanford-ILIAD_PantheonRL.adap.agent.AdapAgent.update": [[134, 150], ["super().update", "agent.AdapAgent.model.policy.set_context"], "methods", ["home.repos.pwc.inspect_result.Stanford-ILIAD_PantheonRL.common.agents.RecordingAgentWrapper.update", "home.repos.pwc.inspect_result.Stanford-ILIAD_PantheonRL.adap.policies.AdapPolicy.set_context"], ["", "def", "update", "(", "self", ",", "reward", ":", "float", ",", "done", ":", "bool", ")", "->", "None", ":", "\n", "        ", "\"\"\"\n        Add new rewards and done information.\n\n        The rewards are added to buffer entry corresponding to the most recent\n        recorded action.\n\n        :param reward: The reward receieved from the previous action step\n        :param done: Whether the game is done\n        \"\"\"", "\n", "super", "(", "AdapAgent", ",", "self", ")", ".", "update", "(", "reward", ",", "done", ")", "\n", "\n", "if", "done", "and", "self", ".", "latent_syncer", "is", "None", ":", "\n", "            ", "sampled_context", "=", "SAMPLERS", "[", "self", ".", "model", ".", "context_sampler", "]", "(", "\n", "ctx_size", "=", "self", ".", "model", ".", "context_size", ",", "num", "=", "1", ",", "torch", "=", "True", ")", "\n", "self", ".", "model", ".", "policy", ".", "set_context", "(", "sampled_context", ")", "\n", "", "", "", ""]], "home.repos.pwc.inspect_result.Stanford-ILIAD_PantheonRL.adap.adap_learn.ADAP.__init__": [[86, 200], ["stable_baselines3.common.on_policy_algorithm.OnPolicyAlgorithm.__init__", "adap_learn.ADAP._setup_model", "warnings.warn"], "methods", ["home.repos.pwc.inspect_result.Stanford-ILIAD_PantheonRL.overcookedgym.overcooked.OvercookedMultiEnv.__init__", "home.repos.pwc.inspect_result.Stanford-ILIAD_PantheonRL.modular.learn.ModularAlgorithm._setup_model"], ["def", "__init__", "(", "\n", "self", ",", "\n", "policy", ":", "Union", "[", "str", ",", "Type", "[", "AdapPolicy", "]", "]", ",", "\n", "env", ":", "Union", "[", "GymEnv", ",", "str", "]", ",", "\n", "learning_rate", ":", "Union", "[", "float", ",", "Schedule", "]", "=", "3e-4", ",", "\n", "n_steps", ":", "int", "=", "2048", ",", "\n", "batch_size", ":", "int", "=", "64", ",", "\n", "n_epochs", ":", "int", "=", "10", ",", "\n", "gamma", ":", "float", "=", "0.99", ",", "\n", "gae_lambda", ":", "float", "=", "0.95", ",", "\n", "clip_range", ":", "Union", "[", "float", ",", "Schedule", "]", "=", "0.2", ",", "\n", "clip_range_vf", ":", "Union", "[", "None", ",", "float", ",", "Schedule", "]", "=", "None", ",", "\n", "ent_coef", ":", "float", "=", "0.0", ",", "\n", "vf_coef", ":", "float", "=", "0.5", ",", "\n", "max_grad_norm", ":", "float", "=", "0.5", ",", "\n", "use_sde", ":", "bool", "=", "False", ",", "\n", "sde_sample_freq", ":", "int", "=", "-", "1", ",", "\n", "target_kl", ":", "Optional", "[", "float", "]", "=", "None", ",", "\n", "tensorboard_log", ":", "Optional", "[", "str", "]", "=", "None", ",", "\n", "create_eval_env", ":", "bool", "=", "False", ",", "\n", "policy_kwargs", ":", "Optional", "[", "Dict", "[", "str", ",", "Any", "]", "]", "=", "None", ",", "\n", "verbose", ":", "int", "=", "0", ",", "\n", "seed", ":", "Optional", "[", "int", "]", "=", "None", ",", "\n", "device", ":", "Union", "[", "th", ".", "device", ",", "str", "]", "=", "\"auto\"", ",", "\n", "_init_setup_model", ":", "bool", "=", "True", ",", "\n", "context_loss_coeff", ":", "float", "=", "0.1", ",", "\n", "context_size", ":", "int", "=", "3", ",", "\n", "num_context_samples", ":", "int", "=", "5", ",", "\n", "context_sampler", ":", "str", "=", "\"l2\"", ",", "\n", "num_state_samples", ":", "int", "=", "32", "\n", ")", ":", "\n", "        ", "if", "policy_kwargs", "is", "None", ":", "\n", "            ", "policy_kwargs", "=", "{", "}", "\n", "", "policy_kwargs", "[", "'context_size'", "]", "=", "context_size", "\n", "super", "(", "ADAP", ",", "self", ")", ".", "__init__", "(", "\n", "policy", ",", "\n", "env", ",", "\n", "learning_rate", "=", "learning_rate", ",", "\n", "n_steps", "=", "n_steps", ",", "\n", "gamma", "=", "gamma", ",", "\n", "gae_lambda", "=", "gae_lambda", ",", "\n", "ent_coef", "=", "ent_coef", ",", "\n", "vf_coef", "=", "vf_coef", ",", "\n", "max_grad_norm", "=", "max_grad_norm", ",", "\n", "use_sde", "=", "use_sde", ",", "\n", "sde_sample_freq", "=", "sde_sample_freq", ",", "\n", "tensorboard_log", "=", "tensorboard_log", ",", "\n", "policy_kwargs", "=", "policy_kwargs", ",", "\n", "verbose", "=", "verbose", ",", "\n", "device", "=", "device", ",", "\n", "create_eval_env", "=", "create_eval_env", ",", "\n", "seed", "=", "seed", ",", "\n", "_init_setup_model", "=", "False", ",", "\n", "supported_action_spaces", "=", "(", "\n", "spaces", ".", "Box", ",", "\n", "spaces", ".", "Discrete", ",", "\n", "spaces", ".", "MultiDiscrete", ",", "\n", "spaces", ".", "MultiBinary", ",", "\n", ")", ",", "\n", ")", "\n", "\n", "# Sanity check, otherwise it will lead to noisy gradient and NaN", "\n", "# because of the advantage normalization", "\n", "assert", "(", "\n", "batch_size", ">", "1", "\n", ")", ",", "\"`batch_size` must be greater than 1. \\\n            See https://github.com/DLR-RM/stable-baselines3/issues/440\"", "\n", "\n", "if", "self", ".", "env", "is", "not", "None", ":", "\n", "# Check that `n_steps * n_envs > 1` to avoid NaN", "\n", "# when doing advantage normalization", "\n", "\n", "            ", "if", "self", ".", "env", ".", "action_space", "==", "spaces", ".", "Box", ":", "\n", "                ", "self", ".", "action_dist", "=", "'gaussian'", "\n", "", "else", ":", "\n", "                ", "self", ".", "action_dist", "=", "'categorical'", "\n", "", "buffer_size", "=", "self", ".", "env", ".", "num_envs", "*", "self", ".", "n_steps", "\n", "assert", "(", "\n", "buffer_size", ">", "1", "\n", ")", ",", "f\"`n_steps * n_envs` must be greater than 1. Currently n_steps=\\\n                {self.n_steps} and n_envs={self.env.num_envs}\"", "\n", "# Check that rollout buffer size is a multiple of mini-batch size", "\n", "untruncated_batches", "=", "buffer_size", "//", "batch_size", "\n", "if", "buffer_size", "%", "batch_size", ">", "0", ":", "\n", "                ", "warnings", ".", "warn", "(", "\n", "f\"You have specified a mini-batch size of {batch_size},\"", "\n", "f\" but because the `RolloutBuffer` is of size \\\n                    `n_steps * n_envs = {buffer_size}`,\"", "\n", "f\" after every {untruncated_batches} untruncated \\\n                    mini-batches,\"", "\n", "f\" there will be a truncated mini-batch of size \\\n                    {buffer_size % batch_size}\\n\"", "\n", "f\"We recommend using a `batch_size` that is a factor of \\\n                    `n_steps * n_envs`.\\n\"", "\n", "f\"Info: (n_steps={self.n_steps} and \\\n                    n_envs={self.env.num_envs})\"", "\n", ")", "\n", "", "", "self", ".", "batch_size", "=", "batch_size", "\n", "self", ".", "n_epochs", "=", "n_epochs", "\n", "self", ".", "clip_range_raw", "=", "clip_range", "\n", "self", ".", "clip_range_vf_raw", "=", "clip_range_vf", "\n", "self", ".", "target_kl", "=", "target_kl", "\n", "\n", "self", ".", "context_loss_coeff", "=", "context_loss_coeff", "\n", "\n", "self", ".", "num_state_samples", "=", "num_state_samples", "\n", "self", ".", "num_context_samples", "=", "num_context_samples", "\n", "self", ".", "context_sampler", "=", "context_sampler", "\n", "self", ".", "context_size", "=", "context_size", "\n", "\n", "if", "_init_setup_model", ":", "\n", "            ", "self", ".", "_setup_model", "(", ")", "\n", "\n", "", "self", ".", "full_obs_shape", "=", "None", "\n", "\n"]], "home.repos.pwc.inspect_result.Stanford-ILIAD_PantheonRL.adap.adap_learn.ADAP.set_env": [[201, 208], ["super().set_env"], "methods", ["home.repos.pwc.inspect_result.Stanford-ILIAD_PantheonRL.adap.adap_learn.ADAP.set_env"], ["", "def", "set_env", "(", "self", ",", "env", ")", ":", "\n", "        ", "super", "(", "ADAP", ",", "self", ")", ".", "set_env", "(", "env", ")", "\n", "\n", "if", "self", ".", "env", ".", "action_space", "==", "spaces", ".", "Box", ":", "\n", "            ", "self", ".", "action_dist", "=", "'gaussian'", "\n", "", "else", ":", "\n", "            ", "self", ".", "action_dist", "=", "'categorical'", "\n", "\n"]], "home.repos.pwc.inspect_result.Stanford-ILIAD_PantheonRL.adap.adap_learn.ADAP._setup_model": [[209, 228], ["super()._setup_model", "adap_learn.ADAP.policy.set_context", "stable_baselines3.common.utils.get_schedule_fn", "isinstance", "stable_baselines3.common.utils.get_schedule_fn"], "methods", ["home.repos.pwc.inspect_result.Stanford-ILIAD_PantheonRL.modular.learn.ModularAlgorithm._setup_model", "home.repos.pwc.inspect_result.Stanford-ILIAD_PantheonRL.adap.policies.AdapPolicy.set_context"], ["", "", "def", "_setup_model", "(", "self", ")", "->", "None", ":", "\n", "        ", "super", "(", "ADAP", ",", "self", ")", ".", "_setup_model", "(", ")", "\n", "\n", "sampled_context", "=", "SAMPLERS", "[", "self", ".", "context_sampler", "]", "(", "\n", "ctx_size", "=", "self", ".", "context_size", ",", "num", "=", "1", ",", "torch", "=", "True", ")", "\n", "\n", "self", ".", "policy", ".", "set_context", "(", "sampled_context", ")", "\n", "\n", "# Initialize schedules for policy/value clipping", "\n", "self", ".", "clip_range", "=", "get_schedule_fn", "(", "self", ".", "clip_range_raw", ")", "\n", "if", "self", ".", "clip_range_vf_raw", "is", "not", "None", ":", "\n", "            ", "if", "isinstance", "(", "self", ".", "clip_range_vf_raw", ",", "(", "float", ",", "int", ")", ")", ":", "\n", "                ", "assert", "self", ".", "clip_range_vf_raw", ">", "0", ",", "\"`clip_range_vf` must be positive, \"", "\"pass `None` to deactivate vf clipping\"", "\n", "\n", "", "self", ".", "clip_range_vf", "=", "get_schedule_fn", "(", "self", ".", "clip_range_vf_raw", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "clip_range_vf", "=", "self", ".", "clip_range_vf_raw", "\n", "\n"]], "home.repos.pwc.inspect_result.Stanford-ILIAD_PantheonRL.adap.adap_learn.ADAP.train": [[229, 372], ["adap_learn.ADAP._update_learning_rate", "adap_learn.ADAP.clip_range", "range", "stable_baselines3.common.utils.explained_variance", "adap_learn.ADAP.logger.record", "adap_learn.ADAP.logger.record", "adap_learn.ADAP.logger.record", "adap_learn.ADAP.logger.record", "adap_learn.ADAP.logger.record", "adap_learn.ADAP.logger.record", "adap_learn.ADAP.logger.record", "adap_learn.ADAP.logger.record", "hasattr", "adap_learn.ADAP.logger.record", "adap_learn.ADAP.logger.record", "adap_learn.ADAP.clip_range_vf", "adap_learn.ADAP.rollout_buffer.get", "adap_learn.ADAP.rollout_buffer.values.flatten", "adap_learn.ADAP.rollout_buffer.returns.flatten", "numpy.mean", "numpy.mean", "numpy.mean", "numpy.mean", "numpy.mean", "numpy.mean", "loss.item", "adap_learn.ADAP.logger.record", "adap_learn.ADAP.logger.record", "isinstance", "adap_learn.ADAP.policy.evaluate_actions", "values.flatten.flatten.flatten", "torch.exp", "pg_losses.append", "torch.mean().item", "clip_fractions.append", "torch.nn.functional.mse_loss", "value_losses.append", "entropy_losses.append", "util.get_context_kl_loss", "context_kl_divs.append", "adap_learn.ADAP.policy.optimizer.zero_grad", "loss.backward", "torch.nn.utils.clip_grad_norm_", "adap_learn.ADAP.policy.optimizer.step", "torch.exp().mean().item", "rollout_data.actions.long().flatten", "adap_learn.ADAP.policy.reset_noise", "torch.clamp", "torch.min().mean", "policy_loss.item", "torch.nn.functional.mse_loss.item", "entropy_loss.item", "util.get_context_kl_loss.detach().numpy", "torch.no_grad", "torch.mean().cpu().numpy", "approx_kl_divs.append", "adap_learn.ADAP.policy.parameters", "advantages.mean", "advantages.std", "torch.mean", "torch.clamp", "torch.mean", "torch.mean", "print", "torch.exp().mean", "rollout_data.actions.long", "torch.min", "util.get_context_kl_loss.detach", "torch.mean().cpu", "torch.exp", "torch.mean", "torch.abs", "torch.exp"], "methods", ["home.repos.pwc.inspect_result.Stanford-ILIAD_PantheonRL.modular.policies.ModularPolicy.evaluate_actions", "home.repos.pwc.inspect_result.Stanford-ILIAD_PantheonRL.adap.util.get_context_kl_loss", "home.repos.pwc.inspect_result.Stanford-ILIAD_PantheonRL.common.multiagentenv.MultiAgentEnv.step", "home.repos.pwc.inspect_result.Stanford-ILIAD_PantheonRL.modular.policies.ModularPolicy.reset_noise"], ["", "", "def", "train", "(", "self", ")", "->", "None", ":", "\n", "        ", "\"\"\"\n        Update policy using the currently gathered rollout buffer.\n        \"\"\"", "\n", "# Update optimizer learning rate", "\n", "self", ".", "_update_learning_rate", "(", "self", ".", "policy", ".", "optimizer", ")", "\n", "# Compute current clip range", "\n", "clip_range", "=", "self", ".", "clip_range", "(", "self", ".", "_current_progress_remaining", ")", "\n", "# Optional: clip range for the value function", "\n", "if", "self", ".", "clip_range_vf", "is", "not", "None", ":", "\n", "            ", "clip_range_vf", "=", "self", ".", "clip_range_vf", "(", "\n", "self", ".", "_current_progress_remaining", ")", "\n", "\n", "", "entropy_losses", "=", "[", "]", "\n", "pg_losses", ",", "value_losses", "=", "[", "]", ",", "[", "]", "\n", "clip_fractions", "=", "[", "]", "\n", "\n", "continue_training", "=", "True", "\n", "\n", "# train for n_epochs epochs", "\n", "for", "epoch", "in", "range", "(", "self", ".", "n_epochs", ")", ":", "\n", "            ", "approx_kl_divs", "=", "[", "]", "\n", "context_kl_divs", "=", "[", "]", "\n", "# Do a complete pass on the rollout buffer", "\n", "for", "rollout_data", "in", "self", ".", "rollout_buffer", ".", "get", "(", "self", ".", "batch_size", ")", ":", "\n", "                ", "actions", "=", "rollout_data", ".", "actions", "\n", "if", "isinstance", "(", "self", ".", "action_space", ",", "spaces", ".", "Discrete", ")", ":", "\n", "# Convert discrete action from float to long", "\n", "                    ", "actions", "=", "rollout_data", ".", "actions", ".", "long", "(", ")", ".", "flatten", "(", ")", "\n", "\n", "# Re-sample the noise matrix because the log_std has changed", "\n", "# TODO: investigate why there is no issue with the gradient", "\n", "# if that line is commented (as in SAC)", "\n", "", "if", "self", ".", "use_sde", ":", "\n", "                    ", "self", ".", "policy", ".", "reset_noise", "(", "self", ".", "batch_size", ")", "\n", "\n", "", "values", ",", "log_prob", ",", "entropy", "=", "self", ".", "policy", ".", "evaluate_actions", "(", "\n", "rollout_data", ".", "observations", ",", "actions", ")", "\n", "values", "=", "values", ".", "flatten", "(", ")", "\n", "# Normalize advantage", "\n", "advantages", "=", "rollout_data", ".", "advantages", "\n", "advantages", "=", "(", "advantages", "-", "advantages", ".", "mean", "(", ")", ")", "/", "(", "advantages", ".", "std", "(", ")", "+", "1e-8", ")", "\n", "\n", "# ratio between old and new policy", "\n", "ratio", "=", "th", ".", "exp", "(", "log_prob", "-", "rollout_data", ".", "old_log_prob", ")", "\n", "\n", "# clipped surrogate loss", "\n", "policy_loss_1", "=", "advantages", "*", "ratio", "\n", "policy_loss_2", "=", "advantages", "*", "th", ".", "clamp", "(", "ratio", ",", "1", "-", "clip_range", ",", "1", "+", "clip_range", ")", "\n", "policy_loss", "=", "-", "th", ".", "min", "(", "policy_loss_1", ",", "policy_loss_2", ")", ".", "mean", "(", ")", "\n", "\n", "# Logging", "\n", "pg_losses", ".", "append", "(", "policy_loss", ".", "item", "(", ")", ")", "\n", "clip_fraction", "=", "th", ".", "mean", "(", "\n", "(", "th", ".", "abs", "(", "ratio", "-", "1", ")", ">", "clip_range", ")", ".", "float", "(", ")", ")", ".", "item", "(", ")", "\n", "clip_fractions", ".", "append", "(", "clip_fraction", ")", "\n", "\n", "if", "self", ".", "clip_range_vf", "is", "None", ":", "\n", "# No clipping", "\n", "                    ", "values_pred", "=", "values", "\n", "", "else", ":", "\n", "# Clip the different between old and new value", "\n", "# NOTE: this depends on the reward scaling", "\n", "                    ", "values_pred", "=", "rollout_data", ".", "old_values", "+", "th", ".", "clamp", "(", "\n", "values", "-", "rollout_data", ".", "old_values", ",", "\n", "-", "clip_range_vf", ",", "\n", "clip_range_vf", "\n", ")", "\n", "# Value loss using the TD(gae_lambda) target", "\n", "", "value_loss", "=", "F", ".", "mse_loss", "(", "rollout_data", ".", "returns", ",", "values_pred", ")", "\n", "value_losses", ".", "append", "(", "value_loss", ".", "item", "(", ")", ")", "\n", "\n", "# Entropy loss favor exploration", "\n", "if", "entropy", "is", "None", ":", "\n", "# Approximate entropy when no analytical form", "\n", "                    ", "entropy_loss", "=", "-", "th", ".", "mean", "(", "-", "log_prob", ")", "\n", "", "else", ":", "\n", "                    ", "entropy_loss", "=", "-", "th", ".", "mean", "(", "entropy", ")", "\n", "\n", "", "entropy_losses", ".", "append", "(", "entropy_loss", ".", "item", "(", ")", ")", "\n", "\n", "# Context loss for ADAP algorithm", "\n", "context_loss", "=", "get_context_kl_loss", "(", "self", ",", "\n", "self", ".", "policy", ",", "rollout_data", ")", "\n", "\n", "context_kl_divs", ".", "append", "(", "context_loss", ".", "detach", "(", ")", ".", "numpy", "(", ")", ")", "\n", "\n", "loss", "=", "policy_loss", "+", "self", ".", "ent_coef", "*", "entropy_loss", "+", "self", ".", "vf_coef", "*", "value_loss", "+", "self", ".", "context_loss_coeff", "*", "context_loss", "\n", "\n", "# Calculate approximate form of reverse KL Divergence", "\n", "with", "th", ".", "no_grad", "(", ")", ":", "\n", "                    ", "log_ratio", "=", "log_prob", "-", "rollout_data", ".", "old_log_prob", "\n", "approx_kl_div", "=", "th", ".", "mean", "(", "\n", "(", "th", ".", "exp", "(", "log_ratio", ")", "-", "1", ")", "-", "log_ratio", ")", ".", "cpu", "(", ")", ".", "numpy", "(", ")", "\n", "approx_kl_divs", ".", "append", "(", "approx_kl_div", ")", "\n", "\n", "", "if", "self", ".", "target_kl", "is", "not", "None", "and", "approx_kl_div", ">", "1.5", "*", "self", ".", "target_kl", ":", "\n", "                    ", "continue_training", "=", "False", "\n", "if", "self", ".", "verbose", ">=", "1", ":", "\n", "                        ", "print", "(", "\n", "f\"Early stopping at step {epoch} due \\\n                            to reaching max kl: {approx_kl_div: .2f}\"", ")", "\n", "", "break", "\n", "\n", "# Optimization step", "\n", "", "self", ".", "policy", ".", "optimizer", ".", "zero_grad", "(", ")", "\n", "loss", ".", "backward", "(", ")", "\n", "# Clip grad norm", "\n", "th", ".", "nn", ".", "utils", ".", "clip_grad_norm_", "(", "\n", "self", ".", "policy", ".", "parameters", "(", ")", ",", "self", ".", "max_grad_norm", ")", "\n", "self", ".", "policy", ".", "optimizer", ".", "step", "(", ")", "\n", "\n", "", "if", "not", "continue_training", ":", "\n", "                ", "break", "\n", "\n", "", "", "self", ".", "_n_updates", "+=", "self", ".", "n_epochs", "\n", "explained_var", "=", "explained_variance", "(", "\n", "self", ".", "rollout_buffer", ".", "values", ".", "flatten", "(", ")", ",", "\n", "self", ".", "rollout_buffer", ".", "returns", ".", "flatten", "(", ")", ")", "\n", "\n", "# Logs", "\n", "self", ".", "logger", ".", "record", "(", "\"train/entropy_loss\"", ",", "np", ".", "mean", "(", "entropy_losses", ")", ")", "\n", "self", ".", "logger", ".", "record", "(", "\"train/policy_gradient_loss\"", ",", "np", ".", "mean", "(", "pg_losses", ")", ")", "\n", "self", ".", "logger", ".", "record", "(", "\"train/value_loss\"", ",", "np", ".", "mean", "(", "value_losses", ")", ")", "\n", "self", ".", "logger", ".", "record", "(", "\"train/approx_kl\"", ",", "np", ".", "mean", "(", "approx_kl_divs", ")", ")", "\n", "self", ".", "logger", ".", "record", "(", "\"train/context_kl_loss\"", ",", "np", ".", "mean", "(", "context_kl_divs", ")", ")", "\n", "self", ".", "logger", ".", "record", "(", "\"train/clip_fraction\"", ",", "np", ".", "mean", "(", "clip_fractions", ")", ")", "\n", "self", ".", "logger", ".", "record", "(", "\"train/loss\"", ",", "loss", ".", "item", "(", ")", ")", "\n", "self", ".", "logger", ".", "record", "(", "\"train/explained_variance\"", ",", "explained_var", ")", "\n", "if", "hasattr", "(", "self", ".", "policy", ",", "\"log_std\"", ")", ":", "\n", "            ", "self", ".", "logger", ".", "record", "(", "\n", "\"train/std\"", ",", "th", ".", "exp", "(", "self", ".", "policy", ".", "log_std", ")", ".", "mean", "(", ")", ".", "item", "(", ")", ")", "\n", "\n", "", "self", ".", "logger", ".", "record", "(", "\"train/n_updates\"", ",", "\n", "self", ".", "_n_updates", ",", "exclude", "=", "\"tensorboard\"", ")", "\n", "self", ".", "logger", ".", "record", "(", "\"train/clip_range\"", ",", "clip_range", ")", "\n", "if", "self", ".", "clip_range_vf", "is", "not", "None", ":", "\n", "            ", "self", ".", "logger", ".", "record", "(", "\"train/clip_range_vf\"", ",", "clip_range_vf", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Stanford-ILIAD_PantheonRL.adap.adap_learn.ADAP.collect_rollouts": [[377, 474], ["tuple", "rollout_buffer.reset", "callback.on_rollout_start", "rollout_buffer.compute_returns_and_advantage", "callback.on_rollout_end", "adap_learn.ADAP.policy.reset_noise", "actions.reshape.reshape.cpu().numpy", "isinstance", "env.step", "callback.update_locals", "adap_learn.ADAP._update_info_buffer", "isinstance", "rollout_buffer.add", "torch.no_grad", "stable_baselines3.common.utils.obs_as_tensor", "adap_learn.ADAP.policy.forward", "adap_learn.ADAP.policy.reset_noise", "torch.no_grad", "stable_baselines3.common.utils.obs_as_tensor", "adap_learn.ADAP.policy.forward", "numpy.clip", "locals", "callback.on_step", "actions.reshape.reshape.reshape", "numpy.concatenate", "adap_learn.ADAP.policy.set_context", "actions.reshape.reshape.cpu", "adap_learn.ADAP.policy.get_context"], "methods", ["home.repos.pwc.inspect_result.Stanford-ILIAD_PantheonRL.common.multiagentenv.MultiAgentEnv.reset", "home.repos.pwc.inspect_result.Stanford-ILIAD_PantheonRL.modular.policies.ModularPolicy.reset_noise", "home.repos.pwc.inspect_result.Stanford-ILIAD_PantheonRL.common.multiagentenv.MultiAgentEnv.step", "home.repos.pwc.inspect_result.Stanford-ILIAD_PantheonRL.common.wrappers.HistoryQueue.add", "home.repos.pwc.inspect_result.Stanford-ILIAD_PantheonRL.modular.policies.ModularPolicy.forward", "home.repos.pwc.inspect_result.Stanford-ILIAD_PantheonRL.modular.policies.ModularPolicy.reset_noise", "home.repos.pwc.inspect_result.Stanford-ILIAD_PantheonRL.modular.policies.ModularPolicy.forward", "home.repos.pwc.inspect_result.Stanford-ILIAD_PantheonRL.adap.policies.AdapPolicy.set_context", "home.repos.pwc.inspect_result.Stanford-ILIAD_PantheonRL.adap.policies.AdapPolicy.get_context"], ["def", "collect_rollouts", "(", "\n", "self", ",", "\n", "env", ":", "VecEnv", ",", "\n", "callback", ":", "BaseCallback", ",", "\n", "rollout_buffer", ":", "RolloutBuffer", ",", "\n", "n_rollout_steps", ":", "int", ",", "\n", ")", "->", "bool", ":", "\n", "        ", "\"\"\"\n        Nearly identical to OnPolicyAlgorithm's collect_rollouts, but it also\n        resamples the context every episode.\n\n        Collect experiences using the current policy and fill a\n        ``RolloutBuffer``.\n        The term rollout here refers to the model-free notion and should not\n        be used with the concept of rollout used in model-based RL or planning.\n        :param env: The training environment\n        :param callback: Callback that will be called at each step\n            (and at the beginning and end of the rollout)\n        :param rollout_buffer: Buffer to fill with rollouts\n        :param n_steps: Number of experiences to collect per environment\n        :return: True if function returned with at least `n_rollout_steps`\n            collected, False if callback terminated rollout prematurely.\n        \"\"\"", "\n", "assert", "self", ".", "_last_obs", "is", "not", "None", ",", "\"No previous observation provided\"", "\n", "n_steps", "=", "0", "\n", "if", "self", ".", "full_obs_shape", "is", "None", ":", "\n", "            ", "self", ".", "full_obs_shape", "=", "(", "\n", "rollout_buffer", ".", "obs_shape", "[", "0", "]", "+", "self", ".", "context_size", ",", ")", "\n", "\n", "", "rollout_buffer", ".", "obs_shape", "=", "tuple", "(", "self", ".", "full_obs_shape", ")", "\n", "\n", "rollout_buffer", ".", "reset", "(", ")", "\n", "# Sample new weights for the state dependent exploration", "\n", "if", "self", ".", "use_sde", ":", "\n", "            ", "self", ".", "policy", ".", "reset_noise", "(", "env", ".", "num_envs", ")", "\n", "\n", "", "callback", ".", "on_rollout_start", "(", ")", "\n", "\n", "while", "n_steps", "<", "n_rollout_steps", ":", "\n", "            ", "if", "self", ".", "use_sde", "and", "self", ".", "sde_sample_freq", ">", "0", "and", "n_steps", "%", "self", ".", "sde_sample_freq", "==", "0", ":", "\n", "# Sample a new noise matrix", "\n", "                ", "self", ".", "policy", ".", "reset_noise", "(", "env", ".", "num_envs", ")", "\n", "\n", "", "with", "th", ".", "no_grad", "(", ")", ":", "\n", "# Convert to pytorch tensor or to TensorDict", "\n", "                ", "obs_tensor", "=", "obs_as_tensor", "(", "self", ".", "_last_obs", ",", "self", ".", "device", ")", "\n", "actions", ",", "values", ",", "log_probs", "=", "self", ".", "policy", ".", "forward", "(", "obs_tensor", ")", "\n", "", "actions", "=", "actions", ".", "cpu", "(", ")", ".", "numpy", "(", ")", "\n", "\n", "# Rescale and perform action", "\n", "clipped_actions", "=", "actions", "\n", "# Clip the actions to avoid out of bound error", "\n", "if", "isinstance", "(", "self", ".", "action_space", ",", "gym", ".", "spaces", ".", "Box", ")", ":", "\n", "                ", "clipped_actions", "=", "np", ".", "clip", "(", "\n", "actions", ",", "self", ".", "action_space", ".", "low", ",", "self", ".", "action_space", ".", "high", ")", "\n", "\n", "", "new_obs", ",", "rewards", ",", "dones", ",", "infos", "=", "env", ".", "step", "(", "clipped_actions", ")", "\n", "self", ".", "num_timesteps", "+=", "env", ".", "num_envs", "\n", "\n", "# Give access to local variables", "\n", "callback", ".", "update_locals", "(", "locals", "(", ")", ")", "\n", "if", "callback", ".", "on_step", "(", ")", "is", "False", ":", "\n", "                ", "return", "False", "\n", "\n", "", "self", ".", "_update_info_buffer", "(", "infos", ")", "\n", "n_steps", "+=", "1", "\n", "\n", "if", "isinstance", "(", "self", ".", "action_space", ",", "gym", ".", "spaces", ".", "Discrete", ")", ":", "\n", "# Reshape in case of discrete action", "\n", "                ", "actions", "=", "actions", ".", "reshape", "(", "-", "1", ",", "1", ")", "\n", "", "rollout_buffer", ".", "add", "(", "np", ".", "concatenate", "(", "\n", "(", "self", ".", "_last_obs", ",", "\n", "self", ".", "policy", ".", "get_context", "(", ")", ")", ",", "\n", "axis", "=", "None", ")", ",", "\n", "actions", ",", "rewards", ",", "\n", "self", ".", "_last_episode_starts", ",", "values", ",", "log_probs", ")", "\n", "self", ".", "_last_obs", "=", "new_obs", "\n", "self", ".", "_last_episode_starts", "=", "dones", "\n", "\n", "# ADAP CHANGE: resample context", "\n", "if", "dones", "[", "0", "]", ":", "\n", "                ", "sampled_context", "=", "SAMPLERS", "[", "self", ".", "context_sampler", "]", "(", "\n", "ctx_size", "=", "self", ".", "context_size", ",", "num", "=", "1", ",", "torch", "=", "True", ")", "\n", "self", ".", "policy", ".", "set_context", "(", "sampled_context", ")", "\n", "\n", "", "", "with", "th", ".", "no_grad", "(", ")", ":", "\n", "# Compute value for the last timestep", "\n", "            ", "obs_tensor", "=", "obs_as_tensor", "(", "new_obs", ",", "self", ".", "device", ")", "\n", "_", ",", "values", ",", "_", "=", "self", ".", "policy", ".", "forward", "(", "obs_tensor", ")", "\n", "\n", "", "rollout_buffer", ".", "compute_returns_and_advantage", "(", "\n", "last_values", "=", "values", ",", "dones", "=", "dones", ")", "\n", "\n", "callback", ".", "on_rollout_end", "(", ")", "\n", "\n", "return", "True", "\n", "\n"]], "home.repos.pwc.inspect_result.Stanford-ILIAD_PantheonRL.adap.adap_learn.ADAP.learn": [[475, 497], ["super().learn"], "methods", ["home.repos.pwc.inspect_result.Stanford-ILIAD_PantheonRL.common.agents.OffPolicyAgent.learn"], ["", "def", "learn", "(", "\n", "self", ",", "\n", "total_timesteps", ":", "int", ",", "\n", "callback", ":", "MaybeCallback", "=", "None", ",", "\n", "log_interval", ":", "int", "=", "1", ",", "\n", "eval_env", ":", "Optional", "[", "GymEnv", "]", "=", "None", ",", "\n", "eval_freq", ":", "int", "=", "-", "1", ",", "\n", "n_eval_episodes", ":", "int", "=", "5", ",", "\n", "tb_log_name", ":", "str", "=", "\"ADAP\"", ",", "\n", "eval_log_path", ":", "Optional", "[", "str", "]", "=", "None", ",", "\n", "reset_num_timesteps", ":", "bool", "=", "True", ",", "\n", ")", "->", "\"ADAP\"", ":", "\n", "        ", "return", "super", "(", "ADAP", ",", "self", ")", ".", "learn", "(", "\n", "total_timesteps", "=", "total_timesteps", ",", "\n", "callback", "=", "callback", ",", "\n", "log_interval", "=", "log_interval", ",", "\n", "eval_env", "=", "eval_env", ",", "\n", "eval_freq", "=", "eval_freq", ",", "\n", "n_eval_episodes", "=", "n_eval_episodes", ",", "\n", "tb_log_name", "=", "tb_log_name", ",", "\n", "eval_log_path", "=", "eval_log_path", ",", "\n", "reset_num_timesteps", "=", "reset_num_timesteps", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.Stanford-ILIAD_PantheonRL.adap.util.kl_divergence": [[16, 40], ["isinstance", "torch.stack().sum", "torch.distributions.kl.kl_divergence", "torch.stack", "torch.distributions.kl.kl_divergence", "zip"], "function", ["home.repos.pwc.inspect_result.Stanford-ILIAD_PantheonRL.adap.util.kl_divergence", "home.repos.pwc.inspect_result.Stanford-ILIAD_PantheonRL.adap.util.kl_divergence"], ["", "def", "kl_divergence", "(", "dist_true", ":", "distributions", ".", "Distribution", ",", "\n", "dist_pred", ":", "distributions", ".", "Distribution", ")", "->", "th", ".", "Tensor", ":", "\n", "    ", "\"\"\"\n    Wrapper for the PyTorch implementation of the full form KL Divergence\n    :param dist_true: the p distribution\n    :param dist_pred: the q distribution\n    :return: KL(dist_true||dist_pred)\n    \"\"\"", "\n", "# KL Divergence for different distribution types is out of scope", "\n", "assert", "dist_true", ".", "__class__", "==", "dist_pred", ".", "__class__", ",", "\"Error: input distributions should be the same type\"", "\n", "\n", "# MultiCategoricalDistribution is not a PyTorch Distribution subclass", "\n", "# so we need to implement it ourselves!", "\n", "if", "isinstance", "(", "dist_pred", ",", "distributions", ".", "MultiCategoricalDistribution", ")", ":", "\n", "        ", "return", "th", ".", "stack", "(", "\n", "[", "kl", ".", "kl_divergence", "(", "p", ",", "q", ")", "for", "p", ",", "q", "in", "zip", "(", "\n", "dist_true", ".", "distribution", ",", "dist_pred", ".", "distribution", ")", "]", ",", "\n", "dim", "=", "1", ",", "\n", ")", ".", "sum", "(", "dim", "=", "1", ")", "\n", "\n", "# Use the PyTorch kl_divergence implementation", "\n", "", "else", ":", "\n", "        ", "return", "kl", ".", "kl_divergence", "(", "dist_true", ".", "distribution", ",", "dist_pred", ".", "distribution", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Stanford-ILIAD_PantheonRL.adap.util.get_L2_sphere": [[42, 51], ["ctxs.to.to", "torch.rand", "torch.sum().reshape", "numpy.random.rand", "numpy.sum().reshape", "torch.sum", "numpy.sum"], "function", ["None"], ["", "", "def", "get_L2_sphere", "(", "ctx_size", ",", "num", ",", "torch", "=", "False", ")", ":", "\n", "    ", "if", "torch", ":", "\n", "        ", "ctxs", "=", "th", ".", "rand", "(", "num", ",", "ctx_size", ",", "device", "=", "'cpu'", ")", "*", "2", "-", "1", "\n", "ctxs", "=", "ctxs", "/", "(", "th", ".", "sum", "(", "(", "ctxs", ")", "**", "2", ",", "dim", "=", "-", "1", ")", ".", "reshape", "(", "num", ",", "1", ")", ")", "**", "(", "1", "/", "2", ")", "\n", "ctxs", "=", "ctxs", ".", "to", "(", "'cpu'", ")", "\n", "", "else", ":", "\n", "        ", "ctxs", "=", "np", ".", "random", ".", "rand", "(", "num", ",", "ctx_size", ")", "*", "2", "-", "1", "\n", "ctxs", "=", "ctxs", "/", "(", "np", ".", "sum", "(", "(", "ctxs", ")", "**", "2", ",", "axis", "=", "-", "1", ")", ".", "reshape", "(", "num", ",", "1", ")", ")", "**", "(", "1", "/", "2", ")", "\n", "", "return", "ctxs", "\n", "\n"]], "home.repos.pwc.inspect_result.Stanford-ILIAD_PantheonRL.adap.util.get_unit_square": [[53, 59], ["torch.rand", "numpy.random.rand"], "function", ["None"], ["", "def", "get_unit_square", "(", "ctx_size", ",", "num", ",", "torch", "=", "False", ")", ":", "\n", "    ", "if", "torch", ":", "\n", "        ", "ctxs", "=", "th", ".", "rand", "(", "num", ",", "ctx_size", ")", "*", "2", "-", "1", "\n", "", "else", ":", "\n", "        ", "ctxs", "=", "np", ".", "random", ".", "rand", "(", "num", ",", "ctx_size", ")", "*", "2", "-", "1", "\n", "", "return", "ctxs", "\n", "\n"]], "home.repos.pwc.inspect_result.Stanford-ILIAD_PantheonRL.adap.util.get_positive_square": [[61, 67], ["torch.rand", "numpy.random.rand"], "function", ["None"], ["", "def", "get_positive_square", "(", "ctx_size", ",", "num", ",", "torch", "=", "False", ")", ":", "\n", "    ", "if", "torch", ":", "\n", "        ", "ctxs", "=", "th", ".", "rand", "(", "num", ",", "ctx_size", ")", "\n", "", "else", ":", "\n", "        ", "ctxs", "=", "np", ".", "random", ".", "rand", "(", "num", ",", "ctx_size", ")", "\n", "", "return", "ctxs", "\n", "\n"]], "home.repos.pwc.inspect_result.Stanford-ILIAD_PantheonRL.adap.util.get_categorical": [[69, 77], ["torch.zeros", "numpy.zeros", "torch.arange", "torch.randint", "numpy.arange", "numpy.random.randint"], "function", ["None"], ["", "def", "get_categorical", "(", "ctx_size", ",", "num", ",", "torch", "=", "False", ")", ":", "\n", "    ", "if", "torch", ":", "\n", "        ", "ctxs", "=", "th", ".", "zeros", "(", "num", ",", "ctx_size", ")", "\n", "ctxs", "[", "th", ".", "arange", "(", "num", ")", ",", "th", ".", "randint", "(", "0", ",", "ctx_size", ",", "size", "=", "(", "num", ",", ")", ")", "]", "=", "1", "\n", "", "else", ":", "\n", "        ", "ctxs", "=", "np", ".", "zeros", "(", "(", "num", ",", "ctx_size", ")", ")", "\n", "ctxs", "[", "np", ".", "arange", "(", "num", ")", ",", "np", ".", "random", ".", "randint", "(", "0", ",", "ctx_size", ",", "size", "=", "(", "num", ",", ")", ")", "]", "=", "1", "\n", "", "return", "ctxs", "\n", "\n"]], "home.repos.pwc.inspect_result.Stanford-ILIAD_PantheonRL.adap.util.get_natural_number": [[79, 88], ["torch.randint", "numpy.random.randint"], "function", ["None"], ["", "def", "get_natural_number", "(", "ctx_size", ",", "num", ",", "torch", "=", "False", ")", ":", "\n", "    ", "'''\n    Returns context vector of shape (num,1) with numbers in range [0, ctx_size]\n    '''", "\n", "if", "torch", ":", "\n", "        ", "ctxs", "=", "th", ".", "randint", "(", "0", ",", "ctx_size", ",", "size", "=", "(", "num", ",", "1", ")", ")", "\n", "", "else", ":", "\n", "        ", "ctxs", "=", "np", ".", "random", ".", "randint", "(", "0", ",", "ctx_size", ",", "size", "=", "(", "num", ",", "1", ")", ")", "\n", "", "return", "ctxs", "\n", "\n"]], "home.repos.pwc.inspect_result.Stanford-ILIAD_PantheonRL.adap.util.get_context_kl_loss": [[97, 132], ["min", "set", "model.get_context", "range", "model.set_context", "torch.randperm", "set.add", "model.set_context", "model._get_latent", "model._get_action_dist_from_latent", "all_action_dists.append", "torch.mean", "sum", "len", "copy.copy", "torch.exp", "itertools.combinations", "util.kl_divergence"], "function", ["home.repos.pwc.inspect_result.Stanford-ILIAD_PantheonRL.adap.policies.AdapPolicy.get_context", "home.repos.pwc.inspect_result.Stanford-ILIAD_PantheonRL.adap.policies.AdapPolicy.set_context", "home.repos.pwc.inspect_result.Stanford-ILIAD_PantheonRL.common.wrappers.HistoryQueue.add", "home.repos.pwc.inspect_result.Stanford-ILIAD_PantheonRL.adap.policies.AdapPolicy.set_context", "home.repos.pwc.inspect_result.Stanford-ILIAD_PantheonRL.modular.policies.ModularPolicy._get_latent", "home.repos.pwc.inspect_result.Stanford-ILIAD_PantheonRL.modular.policies.ModularPolicy._get_action_dist_from_latent", "home.repos.pwc.inspect_result.Stanford-ILIAD_PantheonRL.adap.util.kl_divergence"], ["def", "get_context_kl_loss", "(", "policy", ":", "'ADAP'", ",", "model", ":", "'AdapPolicy'", ",", "\n", "train_batch", ":", "RolloutBufferSamples", ")", ":", "\n", "\n", "    ", "original_obs", "=", "train_batch", ".", "observations", "[", ":", ",", ":", "-", "policy", ".", "context_size", "]", "\n", "\n", "context_size", "=", "policy", ".", "context_size", "\n", "num_context_samples", "=", "policy", ".", "num_context_samples", "\n", "num_state_samples", "=", "policy", ".", "num_state_samples", "\n", "\n", "indices", "=", "th", ".", "randperm", "(", "original_obs", ".", "shape", "[", "0", "]", ")", "[", ":", "num_state_samples", "]", "\n", "sampled_states", "=", "original_obs", "[", "indices", "]", "\n", "num_state_samples", "=", "min", "(", "num_state_samples", ",", "sampled_states", ".", "shape", "[", "0", "]", ")", "\n", "\n", "all_contexts", "=", "set", "(", ")", "\n", "all_action_dists", "=", "[", "]", "\n", "old_context", "=", "model", ".", "get_context", "(", ")", "\n", "for", "i", "in", "range", "(", "0", ",", "num_context_samples", ")", ":", "# 10 sampled contexts", "\n", "        ", "sampled_context", "=", "SAMPLERS", "[", "policy", ".", "context_sampler", "]", "(", "\n", "ctx_size", "=", "context_size", ",", "num", "=", "1", ",", "torch", "=", "True", ")", "\n", "\n", "if", "sampled_context", "in", "all_contexts", ":", "\n", "            ", "continue", "\n", "\n", "", "all_contexts", ".", "add", "(", "sampled_context", ")", "\n", "model", ".", "set_context", "(", "sampled_context", ")", "\n", "latent_pi", ",", "_", ",", "latent_sde", "=", "model", ".", "_get_latent", "(", "sampled_states", ")", "\n", "context_action_dist", "=", "model", ".", "_get_action_dist_from_latent", "(", "\n", "latent_pi", ",", "latent_sde", ")", "\n", "all_action_dists", ".", "append", "(", "copy", ".", "copy", "(", "context_action_dist", ")", ")", "\n", "\n", "", "model", ".", "set_context", "(", "old_context", ")", "\n", "all_CLs", "=", "[", "th", ".", "mean", "(", "th", ".", "exp", "(", "-", "kl_divergence", "(", "a", ",", "b", ")", ")", ")", "\n", "for", "a", ",", "b", "in", "combinations", "(", "all_action_dists", ",", "2", ")", "]", "\n", "rawans", "=", "sum", "(", "all_CLs", ")", "/", "len", "(", "all_CLs", ")", "\n", "return", "rawans", "\n", "", ""]], "home.repos.pwc.inspect_result.Stanford-ILIAD_PantheonRL.modular.policies.ModularPolicy.__init__": [[57, 147], ["stable_baselines3.common.policies.BasePolicy.__init__", "print", "features_extractor_class", "stable_baselines3.common.distributions.make_proba_distribution", "policies.ModularPolicy._build", "torch.cuda.is_available", "torch.cuda.is_available", "dict", "dict"], "methods", ["home.repos.pwc.inspect_result.Stanford-ILIAD_PantheonRL.overcookedgym.overcooked.OvercookedMultiEnv.__init__", "home.repos.pwc.inspect_result.Stanford-ILIAD_PantheonRL.modular.policies.ModularPolicy._build"], ["squash_output", "=", "squash_output", ",", "\n", "features_extractor_class", "=", "features_extractor_class", ",", "\n", "features_extractor_kwargs", "=", "features_extractor_kwargs", ",", "\n", "normalize_images", "=", "normalize_images", ",", "\n", "optimizer_class", "=", "optimizer_class", ",", "\n", "optimizer_kwargs", "=", "optimizer_kwargs", ",", "\n", ")", "\n", "\n", "", "def", "set_context", "(", "self", ",", "ctxt", ")", ":", "\n", "        ", "self", ".", "context", "=", "ctxt", "\n", "\n", "", "def", "get_context", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "context", "\n", "\n", "", "def", "_build_mlp_extractor", "(", "self", ")", "->", "None", ":", "\n", "        ", "\"\"\"\n        Create the policy and value networks.\n        Part of the layers can be shared.\n        \"\"\"", "\n", "# Note: If net_arch is None and some features extractor is used,", "\n", "#       net_arch here is an empty list and mlp_extractor does not", "\n", "#       really contain any layers (acts like an identity module).", "\n", "self", ".", "mlp_extractor", "=", "MlpExtractor", "(", "\n", "self", ".", "features_dim", "+", "self", ".", "context_size", ",", "\n", "net_arch", "=", "self", ".", "net_arch", ",", "\n", "activation_fn", "=", "self", ".", "activation_fn", ",", "\n", "device", "=", "self", ".", "device", ",", "\n", ")", "\n", "\n", "", "def", "_get_latent", "(", "self", ",", "\n", "obs", ":", "th", ".", "Tensor", ")", "->", "Tuple", "[", "th", ".", "Tensor", ",", "th", ".", "Tensor", ",", "th", ".", "Tensor", "]", ":", "\n", "        ", "\"\"\"\n        Get the latent code (activations of the last layer of each network)\n        for the different networks.\n        :param obs: Observation\n        :return: Latent codes\n            for the actor, the value function and for gSDE function\n        \"\"\"", "\n", "# Preprocess the observation if needed", "\n", "features", "=", "self", ".", "extract_features", "(", "obs", ")", "\n", "features", "=", "th", ".", "cat", "(", "\n", "(", "features", ",", "self", ".", "context", ".", "repeat", "(", "features", ".", "size", "(", ")", "[", "0", "]", ",", "1", ")", ")", ",", "\n", "dim", "=", "1", ")", "\n", "latent_pi", ",", "latent_vf", "=", "self", ".", "mlp_extractor", "(", "features", ")", "\n", "\n", "# Features for sde", "\n", "latent_sde", "=", "latent_pi", "\n", "if", "self", ".", "sde_features_extractor", "is", "not", "None", ":", "\n", "            ", "latent_sde", "=", "self", ".", "sde_features_extractor", "(", "features", ")", "\n", "", "return", "latent_pi", ",", "latent_vf", ",", "latent_sde", "\n", "\n", "", "def", "evaluate_actions", "(", "self", ",", "\n", "obs", ":", "th", ".", "Tensor", ",", "\n", "actions", ":", "th", ".", "Tensor", "\n", ")", "->", "Tuple", "[", "th", ".", "Tensor", ",", "th", ".", "Tensor", ",", "th", ".", "Tensor", "]", ":", "\n", "        ", "\"\"\"\n        Evaluate actions according to the current policy,\n        given the observations.\n        :param obs:\n        :param actions:\n        :return: estimated value, log likelihood of taking those actions\n            and entropy of the action distribution.\n        \"\"\"", "\n", "features", "=", "self", ".", "extract_features", "(", "obs", "[", ":", ",", ":", "-", "self", ".", "context_size", "]", ")", "\n", "features", "=", "th", ".", "cat", "(", "\n", "(", "features", ",", "obs", "[", ":", ",", "-", "self", ".", "context_size", ":", "]", ")", ",", "\n", "dim", "=", "1", ")", "\n", "latent_pi", ",", "latent_vf", "=", "self", ".", "mlp_extractor", "(", "features", ")", "\n", "\n", "# Features for sde", "\n", "latent_sde", "=", "latent_pi", "\n", "if", "self", ".", "sde_features_extractor", "is", "not", "None", ":", "\n", "            ", "latent_sde", "=", "self", ".", "sde_features_extractor", "(", "features", ")", "\n", "", "distribution", "=", "self", ".", "_get_action_dist_from_latent", "(", "latent_pi", ",", "latent_sde", ")", "\n", "log_prob", "=", "distribution", ".", "log_prob", "(", "actions", ")", "\n", "values", "=", "self", ".", "value_net", "(", "latent_vf", ")", "\n", "return", "values", ",", "log_prob", ",", "distribution", ".", "entropy", "(", ")", "\n", "\n", "\n", "", "", "class", "MultModel", "(", "MlpExtractor", ")", ":", "\n", "    ", "def", "__init__", "(", "\n", "self", ",", "\n", "feature_dim", ",", "\n", "net_arch", ",", "\n", "activation_fn", ",", "\n", "device", ",", "\n", "context_size", "\n", ")", ":", "\n", "        ", "nn", ".", "Module", ".", "__init__", "(", "self", ")", "\n", "\n", "self", ".", "obs_space_size", "=", "feature_dim", "+", "context_size", "\n"]], "home.repos.pwc.inspect_result.Stanford-ILIAD_PantheonRL.modular.policies.ModularPolicy.set_freeze_module": [[149, 152], ["module.parameters"], "methods", ["None"], ["\n", "device", "=", "get_device", "(", "device", ")", "\n", "shared_net", ",", "policy_net", ",", "value_net", "=", "[", "]", ",", "[", "]", ",", "[", "]", "\n", "# Layer sizes of the network that only belongs to the policy network", "\n"]], "home.repos.pwc.inspect_result.Stanford-ILIAD_PantheonRL.modular.policies.ModularPolicy.set_freeze_main": [[152, 156], ["policies.ModularPolicy.set_freeze_module", "policies.ModularPolicy.set_freeze_module", "policies.ModularPolicy.set_freeze_module"], "methods", ["home.repos.pwc.inspect_result.Stanford-ILIAD_PantheonRL.modular.policies.ModularPolicy.set_freeze_module", "home.repos.pwc.inspect_result.Stanford-ILIAD_PantheonRL.modular.policies.ModularPolicy.set_freeze_module", "home.repos.pwc.inspect_result.Stanford-ILIAD_PantheonRL.modular.policies.ModularPolicy.set_freeze_module"], ["# Layer sizes of the network that only belongs to the policy network", "\n", "policy_only_layers", "=", "[", "]", "\n", "# Layer sizes of the network that only belongs to the value network", "\n", "value_only_layers", "=", "[", "]", "\n", "last_layer_dim_shared", "=", "feature_dim", "\n"]], "home.repos.pwc.inspect_result.Stanford-ILIAD_PantheonRL.modular.policies.ModularPolicy.set_freeze_partner": [[156, 161], ["range", "policies.ModularPolicy.set_freeze_module", "policies.ModularPolicy.set_freeze_module", "policies.ModularPolicy.set_freeze_module"], "methods", ["home.repos.pwc.inspect_result.Stanford-ILIAD_PantheonRL.modular.policies.ModularPolicy.set_freeze_module", "home.repos.pwc.inspect_result.Stanford-ILIAD_PantheonRL.modular.policies.ModularPolicy.set_freeze_module", "home.repos.pwc.inspect_result.Stanford-ILIAD_PantheonRL.modular.policies.ModularPolicy.set_freeze_module"], ["last_layer_dim_shared", "=", "feature_dim", "\n", "\n", "# Iterate through shared layers and build shared parts of the network", "\n", "for", "layer", "in", "net_arch", ":", "\n", "            ", "if", "isinstance", "(", "layer", ",", "int", ")", ":", "# Check that this is a shared layer", "\n", "# TODO: give layer a meaningful name", "\n"]], "home.repos.pwc.inspect_result.Stanford-ILIAD_PantheonRL.modular.policies.ModularPolicy._get_data": [[162, 184], ["super()._get_data", "super()._get_data.update", "collections.defaultdict", "dict"], "methods", ["home.repos.pwc.inspect_result.Stanford-ILIAD_PantheonRL.modular.policies.ModularPolicy._get_data", "home.repos.pwc.inspect_result.Stanford-ILIAD_PantheonRL.common.agents.RecordingAgentWrapper.update"], ["# add linear of size layer", "\n", "                ", "shared_net", ".", "append", "(", "nn", ".", "Linear", "(", "last_layer_dim_shared", ",", "layer", ")", ")", "\n", "shared_net", ".", "append", "(", "activation_fn", "(", ")", ")", "\n", "last_layer_dim_shared", "=", "layer", "\n", "", "else", ":", "\n", "                ", "assert", "isinstance", "(", "layer", ",", "dict", ")", ",", "\"Error: the net_arch list can only contain ints and dicts\"", "\n", "if", "\"pi\"", "in", "layer", ":", "\n", "                    ", "assert", "isinstance", "(", "layer", "[", "\"pi\"", "]", ",", "list", ")", ",", "\"Error: net_arch[-1]['pi'] must \\\n                        contain a list of integers.\"", "\n", "policy_only_layers", "=", "layer", "[", "\"pi\"", "]", "\n", "\n", "", "if", "\"vf\"", "in", "layer", ":", "\n", "                    ", "assert", "isinstance", "(", "layer", "[", "\"vf\"", "]", ",", "list", ")", ",", "\"Error: net_arch[-1]['vf'] must \\\n                        contain a list of integers.\"", "\n", "value_only_layers", "=", "layer", "[", "\"vf\"", "]", "\n", "", "break", "\n", "\n", "", "", "last_layer_dim_pi", "=", "last_layer_dim_shared", "\n", "last_layer_dim_vf", "=", "last_layer_dim_shared", "\n", "\n"]], "home.repos.pwc.inspect_result.Stanford-ILIAD_PantheonRL.modular.policies.ModularPolicy.reset_noise": [[185, 193], ["isinstance", "policies.ModularPolicy.action_dist.sample_weights"], "methods", ["None"], ["# Build the non-shared part of the network", "\n", "for", "pi_layer_size", ",", "vf_layer_size", "in", "zip_longest", "(", "policy_only_layers", ",", "\n", "value_only_layers", ")", ":", "\n", "            ", "if", "pi_layer_size", "is", "not", "None", ":", "\n", "                ", "assert", "isinstance", "(", "pi_layer_size", ",", "int", ")", ",", "\"Error: net_arch[-1]['pi'] must only contain integers.\"", "\n", "policy_net", ".", "append", "(", "nn", ".", "Linear", "(", "last_layer_dim_pi", ",", "pi_layer_size", ")", ")", "\n", "policy_net", ".", "append", "(", "activation_fn", "(", ")", ")", "\n", "last_layer_dim_pi", "=", "pi_layer_size", "\n"]], "home.repos.pwc.inspect_result.Stanford-ILIAD_PantheonRL.modular.policies.ModularPolicy.make_action_dist_net": [[194, 213], ["isinstance", "policies.ModularPolicy.action_dist.proba_distribution_net", "isinstance", "policies.ModularPolicy.action_dist.proba_distribution_net", "isinstance", "policies.ModularPolicy.action_dist.proba_distribution_net", "isinstance", "policies.ModularPolicy.action_dist.proba_distribution_net", "isinstance", "policies.ModularPolicy.action_dist.proba_distribution_net", "NotImplementedError"], "methods", ["None"], ["\n", "", "if", "vf_layer_size", "is", "not", "None", ":", "\n", "                ", "assert", "isinstance", "(", "vf_layer_size", ",", "int", ")", ",", "\"Error: net_arch[-1]['vf'] must only contain integers.\"", "\n", "value_net", ".", "append", "(", "nn", ".", "Linear", "(", "last_layer_dim_vf", ",", "vf_layer_size", ")", ")", "\n", "value_net", ".", "append", "(", "activation_fn", "(", ")", ")", "\n", "last_layer_dim_vf", "=", "vf_layer_size", "\n", "\n", "# Save dim, used to create the distributions", "\n", "", "", "self", ".", "latent_dim_pi", "=", "last_layer_dim_pi", "\n", "self", ".", "latent_dim_vf", "=", "last_layer_dim_vf", "\n", "\n", "# Create networks", "\n", "# If list of layers is empty, the network is an Identity module", "\n", "self", ".", "shared_net", "=", "nn", ".", "Sequential", "(", "*", "shared_net", ")", ".", "to", "(", "device", ")", "\n", "\n", "self", ".", "hidden_dim1", "=", "policy_net", "[", "0", "]", ".", "out_features", "\n", "self", ".", "agent_branch_1", "=", "nn", ".", "Sequential", "(", "*", "policy_net", "[", "0", ":", "2", "]", ")", ".", "to", "(", "device", ")", "\n", "self", ".", "agent_scaling", "=", "nn", ".", "Sequential", "(", "\n", "nn", ".", "Linear", "(", "self", ".", "hidden_dim1", ",", "self", ".", "hidden_dim1", "*", "self", ".", "context_size", ")", ",", "\n"]], "home.repos.pwc.inspect_result.Stanford-ILIAD_PantheonRL.modular.policies.ModularPolicy.build_mlp_action_value_net": [[214, 220], ["stable_baselines3.common.torch_layers.MlpExtractor", "policies.ModularPolicy.make_action_dist_net", "torch.Linear", "torch.Linear"], "methods", ["home.repos.pwc.inspect_result.Stanford-ILIAD_PantheonRL.modular.policies.ModularPolicy.make_action_dist_net"], ["activation_fn", "(", ")", "\n", ")", ".", "to", "(", "device", ")", "\n", "self", ".", "agent_branch_2", "=", "nn", ".", "Sequential", "(", "*", "policy_net", "[", "2", ":", "]", ")", ".", "to", "(", "device", ")", "\n", "\n", "self", ".", "hidden_dim2", "=", "value_net", "[", "0", "]", ".", "out_features", "\n", "self", ".", "value_branch_1", "=", "nn", ".", "Sequential", "(", "*", "value_net", "[", "0", ":", "2", "]", ")", ".", "to", "(", "device", ")", "\n", "self", ".", "value_scaling", "=", "nn", ".", "Sequential", "(", "\n"]], "home.repos.pwc.inspect_result.Stanford-ILIAD_PantheonRL.modular.policies.ModularPolicy.do_init_weights": [[221, 242], ["module_gains.items", "numpy.sqrt", "numpy.sqrt", "range", "module.apply", "numpy.sqrt", "functools.partial"], "methods", ["None"], ["nn", ".", "Linear", "(", "self", ".", "hidden_dim2", ",", "self", ".", "hidden_dim2", "*", "self", ".", "context_size", ")", ",", "\n", "activation_fn", "(", ")", "\n", ")", ".", "to", "(", "device", ")", "\n", "self", ".", "value_branch_2", "=", "nn", ".", "Sequential", "(", "*", "value_net", "[", "2", ":", "]", ")", ".", "to", "(", "device", ")", "\n", "\n", "", "def", "get_input_size_excluding_ctx", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "obs_space_size", "-", "self", ".", "context_size", "\n", "\n", "", "def", "get_input_size_inluding_ctx", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "obs_space_size", "\n", "\n", "", "def", "policies", "(", "self", ",", "observations", ":", "th", ".", "Tensor", ",", "\n", "contexts", ":", "th", ".", "Tensor", ")", "->", "th", ".", "Tensor", ":", "\n", "\n", "        ", "batch_size", "=", "observations", ".", "shape", "[", "0", "]", "\n", "x", "=", "self", ".", "agent_branch_1", "(", "observations", ")", "\n", "x_a", "=", "self", ".", "agent_scaling", "(", "x", ")", "\n", "# reshape to do context multiplication", "\n", "x_a", "=", "x_a", ".", "view", "(", "(", "batch_size", ",", "self", ".", "hidden_dim1", ",", "self", ".", "context_size", ")", ")", "\n", "x_a_out", "=", "th", ".", "matmul", "(", "x_a", ",", "contexts", ".", "unsqueeze", "(", "-", "1", ")", ")", ".", "squeeze", "(", "-", "1", ")", "\n", "logits", "=", "self", ".", "agent_branch_2", "(", "x", "+", "x_a_out", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Stanford-ILIAD_PantheonRL.modular.policies.ModularPolicy._build": [[243, 268], ["policies.ModularPolicy.build_mlp_action_value_net", "zip", "torch.ModuleList", "torch.ModuleList", "torch.ModuleList", "torch.ModuleList", "torch.ModuleList", "torch.ModuleList", "policies.ModularPolicy.optimizer_class", "policies.ModularPolicy.do_init_weights", "policies.ModularPolicy.build_mlp_action_value_net", "print", "policies.ModularPolicy.parameters", "range", "lr_schedule"], "methods", ["home.repos.pwc.inspect_result.Stanford-ILIAD_PantheonRL.modular.policies.ModularPolicy.build_mlp_action_value_net", "home.repos.pwc.inspect_result.Stanford-ILIAD_PantheonRL.modular.policies.ModularPolicy.do_init_weights", "home.repos.pwc.inspect_result.Stanford-ILIAD_PantheonRL.modular.policies.ModularPolicy.build_mlp_action_value_net"], ["return", "logits", "\n", "\n", "", "def", "values", "(", "self", ",", "observations", ":", "th", ".", "Tensor", ",", "\n", "contexts", ":", "th", ".", "Tensor", ")", "->", "th", ".", "Tensor", ":", "\n", "\n", "        ", "batch_size", "=", "observations", ".", "shape", "[", "0", "]", "\n", "x", "=", "self", ".", "value_branch_1", "(", "observations", ")", "\n", "x_a", "=", "self", ".", "value_scaling", "(", "x", ")", "\n", "# reshape to do context multiplication", "\n", "x_a", "=", "x_a", ".", "view", "(", "(", "batch_size", ",", "self", ".", "hidden_dim2", ",", "self", ".", "context_size", ")", ")", "\n", "x_a_out", "=", "th", ".", "matmul", "(", "x_a", ",", "contexts", ".", "unsqueeze", "(", "-", "1", ")", ")", ".", "squeeze", "(", "-", "1", ")", "\n", "values", "=", "self", ".", "value_branch_2", "(", "x", "+", "x_a_out", ")", "\n", "# values = self.value_branch_2(x_a_out)", "\n", "\n", "return", "values", "\n", "\n", "", "def", "forward", "(", "self", ",", "features", ":", "th", ".", "Tensor", ")", "->", "Tuple", "[", "th", ".", "Tensor", ",", "th", ".", "Tensor", "]", ":", "\n", "        ", "features", "=", "self", ".", "shared_net", "(", "features", ")", "\n", "observations", "=", "features", "[", ":", ",", ":", "-", "self", ".", "context_size", "]", "\n", "contexts", "=", "features", "[", ":", ",", "-", "self", ".", "context_size", ":", "]", "\n", "return", "self", ".", "policies", "(", "observations", ",", "contexts", ")", ",", "self", ".", "values", "(", "observations", ",", "contexts", ")", "\n", "\n", "\n", "", "", "class", "AdapPolicyMult", "(", "AdapPolicy", ")", ":", "\n", "\n"]], "home.repos.pwc.inspect_result.Stanford-ILIAD_PantheonRL.modular.policies.ModularPolicy.overwrite_main": [[269, 272], ["policies.ModularPolicy.optimizer_class", "policies.ModularPolicy.parameters", "policies.ModularPolicy.lr_schedule"], "methods", ["None"], ["    ", "def", "_build_mlp_extractor", "(", "self", ")", "->", "None", ":", "\n", "        "]], "home.repos.pwc.inspect_result.Stanford-ILIAD_PantheonRL.modular.policies.ModularPolicy.forward": [[273, 291], ["policies.ModularPolicy._get_latent", "policies.ModularPolicy._get_action_dist_from_latent", "policies.ModularPolicy.get_actions", "policies.ModularPolicy.log_prob", "policies.ModularPolicy.value_net"], "methods", ["home.repos.pwc.inspect_result.Stanford-ILIAD_PantheonRL.modular.policies.ModularPolicy._get_latent", "home.repos.pwc.inspect_result.Stanford-ILIAD_PantheonRL.modular.policies.ModularPolicy._get_action_dist_from_latent"], ["\n", "# Note: If net_arch is None and some features extractor is used,", "\n", "#       net_arch here is an empty list and mlp_extractor does not", "\n", "#       really contain any layers (acts like an identity module).", "\n", "self", ".", "mlp_extractor", "=", "MultModel", "(", "\n", "self", ".", "features_dim", ",", "\n", "net_arch", "=", "self", ".", "net_arch", ",", "\n", "activation_fn", "=", "self", ".", "activation_fn", ",", "\n", "device", "=", "self", ".", "device", ",", "\n", "context_size", "=", "self", ".", "context_size", "\n", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.Stanford-ILIAD_PantheonRL.modular.policies.ModularPolicy._get_latent": [[292, 310], ["policies.ModularPolicy.extract_features", "policies.ModularPolicy.mlp_extractor", "policies.ModularPolicy.sde_features_extractor"], "methods", ["None"], []], "home.repos.pwc.inspect_result.Stanford-ILIAD_PantheonRL.modular.policies.ModularPolicy._get_action_dist_from_latent": [[311, 353], ["policies.ModularPolicy.action_net", "torch.clamp", "torch.clamp", "isinstance", "action_mask.to.to.to", "policies.ModularPolicy.action_dist.proba_distribution", "isinstance", "policies.ModularPolicy.action_dist.proba_distribution", "isinstance", "policies.ModularPolicy.action_dist.proba_distribution", "isinstance", "policies.ModularPolicy.action_dist.proba_distribution", "isinstance", "policies.ModularPolicy.action_dist.proba_distribution", "ValueError"], "methods", ["None"], []], "home.repos.pwc.inspect_result.Stanford-ILIAD_PantheonRL.modular.policies.ModularPolicy._predict": [[354, 363], ["policies.ModularPolicy.forward"], "methods", ["home.repos.pwc.inspect_result.Stanford-ILIAD_PantheonRL.modular.policies.ModularPolicy.forward"], []], "home.repos.pwc.inspect_result.Stanford-ILIAD_PantheonRL.modular.policies.ModularPolicy.evaluate_actions": [[364, 384], ["policies.ModularPolicy._get_latent", "policies.ModularPolicy._get_action_dist_from_latent", "policies.ModularPolicy.log_prob", "policies.ModularPolicy.value_net", "policies.ModularPolicy.entropy"], "methods", ["home.repos.pwc.inspect_result.Stanford-ILIAD_PantheonRL.modular.policies.ModularPolicy._get_latent", "home.repos.pwc.inspect_result.Stanford-ILIAD_PantheonRL.modular.policies.ModularPolicy._get_action_dist_from_latent"], []], "home.repos.pwc.inspect_result.Stanford-ILIAD_PantheonRL.modular.policies.ModularPolicy.get_action_logits_from_obs": [[385, 397], ["policies.ModularPolicy._get_latent", "policies.ModularPolicy.action_net"], "methods", ["home.repos.pwc.inspect_result.Stanford-ILIAD_PantheonRL.modular.policies.ModularPolicy._get_latent"], []], "home.repos.pwc.inspect_result.Stanford-ILIAD_PantheonRL.modular.learn.ModularAlgorithm.__init__": [[27, 118], ["stable_baselines3.common.on_policy_algorithm.OnPolicyAlgorithm.__init__", "learn.ModularAlgorithm._setup_model", "warnings.warn"], "methods", ["home.repos.pwc.inspect_result.Stanford-ILIAD_PantheonRL.overcookedgym.overcooked.OvercookedMultiEnv.__init__", "home.repos.pwc.inspect_result.Stanford-ILIAD_PantheonRL.modular.learn.ModularAlgorithm._setup_model"], ["def", "__init__", "(", "\n", "self", ",", "\n", "policy", ":", "Union", "[", "str", ",", "Type", "[", "ActorCriticPolicy", "]", "]", ",", "\n", "env", ":", "Union", "[", "GymEnv", ",", "str", "]", ",", "\n", "learning_rate", ":", "Union", "[", "float", ",", "Schedule", "]", "=", "3e-4", ",", "\n", "n_steps", ":", "int", "=", "2048", ",", "\n", "batch_size", ":", "Optional", "[", "int", "]", "=", "64", ",", "\n", "n_epochs", ":", "int", "=", "10", ",", "\n", "gamma", ":", "float", "=", "0.99", ",", "\n", "gae_lambda", ":", "float", "=", "0.95", ",", "\n", "clip_range", ":", "Union", "[", "float", ",", "Schedule", "]", "=", "0.2", ",", "\n", "clip_range_vf", ":", "Union", "[", "None", ",", "float", ",", "Schedule", "]", "=", "None", ",", "\n", "ent_coef", ":", "float", "=", "0.0", ",", "\n", "vf_coef", ":", "float", "=", "0.5", ",", "\n", "max_grad_norm", ":", "float", "=", "0.5", ",", "\n", "use_sde", ":", "bool", "=", "False", ",", "\n", "sde_sample_freq", ":", "int", "=", "-", "1", ",", "\n", "target_kl", ":", "Optional", "[", "float", "]", "=", "None", ",", "\n", "tensorboard_log", ":", "Optional", "[", "str", "]", "=", "None", ",", "\n", "create_eval_env", ":", "bool", "=", "False", ",", "\n", "policy_kwargs", ":", "Optional", "[", "Dict", "[", "str", ",", "Any", "]", "]", "=", "None", ",", "\n", "verbose", ":", "int", "=", "0", ",", "\n", "seed", ":", "Optional", "[", "int", "]", "=", "None", ",", "\n", "device", ":", "Union", "[", "th", ".", "device", ",", "str", "]", "=", "\"auto\"", ",", "\n", "_init_setup_model", ":", "bool", "=", "True", ",", "\n", "\n", "# my additional arguments", "\n", "marginal_reg_coef", ":", "float", "=", "0.0", ",", "\n", ")", ":", "\n", "\n", "        ", "super", "(", "ModularAlgorithm", ",", "self", ")", ".", "__init__", "(", "\n", "policy", ",", "\n", "env", ",", "\n", "learning_rate", "=", "learning_rate", ",", "\n", "n_steps", "=", "n_steps", ",", "\n", "gamma", "=", "gamma", ",", "\n", "gae_lambda", "=", "gae_lambda", ",", "\n", "ent_coef", "=", "ent_coef", ",", "\n", "vf_coef", "=", "vf_coef", ",", "\n", "max_grad_norm", "=", "max_grad_norm", ",", "\n", "use_sde", "=", "use_sde", ",", "\n", "sde_sample_freq", "=", "sde_sample_freq", ",", "\n", "tensorboard_log", "=", "tensorboard_log", ",", "\n", "policy_kwargs", "=", "policy_kwargs", ",", "\n", "verbose", "=", "verbose", ",", "\n", "device", "=", "device", ",", "\n", "create_eval_env", "=", "create_eval_env", ",", "\n", "seed", "=", "seed", ",", "\n", "_init_setup_model", "=", "False", ",", "\n", "supported_action_spaces", "=", "(", "\n", "spaces", ".", "Box", ",", "\n", "spaces", ".", "Discrete", ",", "\n", "spaces", ".", "MultiDiscrete", ",", "\n", "spaces", ".", "MultiBinary", ",", "\n", ")", ",", "\n", ")", "\n", "\n", "self", ".", "marginal_reg_coef", "=", "marginal_reg_coef", "\n", "\n", "# Sanity check, otherwise it will lead to noisy gradient and NaN", "\n", "# because of the advantage normalization", "\n", "assert", "(", "\n", "batch_size", ">", "1", "\n", ")", ",", "\"`batch_size` must be greater than 1. See https://github.com/DLR-RM/stable-baselines3/issues/440\"", "\n", "\n", "if", "self", ".", "env", "is", "not", "None", ":", "\n", "# Check that `n_steps * n_envs > 1` to avoid NaN", "\n", "# when doing advantage normalization", "\n", "            ", "buffer_size", "=", "self", ".", "env", ".", "num_envs", "*", "self", ".", "n_steps", "\n", "assert", "(", "\n", "buffer_size", ">", "1", "\n", ")", ",", "f\"`n_steps * n_envs` must be greater than 1. Currently n_steps={self.n_steps} and n_envs={self.env.num_envs}\"", "\n", "# Check that the rollout buffer size is a multiple of the mini-batch size", "\n", "untruncated_batches", "=", "buffer_size", "//", "batch_size", "\n", "if", "buffer_size", "%", "batch_size", ">", "0", ":", "\n", "                ", "warnings", ".", "warn", "(", "\n", "f\"You have specified a mini-batch size of {batch_size},\"", "\n", "f\" but because the `RolloutBuffer` is of size `n_steps * n_envs = {buffer_size}`,\"", "\n", "f\" after every {untruncated_batches} untruncated mini-batches,\"", "\n", "f\" there will be a truncated mini-batch of size {buffer_size % batch_size}\\n\"", "\n", "f\"We recommend using a `batch_size` that is a factor of `n_steps * n_envs`.\\n\"", "\n", "f\"Info: (n_steps={self.n_steps} and n_envs={self.env.num_envs})\"", "\n", ")", "\n", "", "", "self", ".", "batch_size", "=", "batch_size", "\n", "self", ".", "n_epochs", "=", "n_epochs", "\n", "self", ".", "clip_range", "=", "clip_range", "\n", "self", ".", "clip_range_vf", "=", "clip_range_vf", "\n", "self", ".", "target_kl", "=", "target_kl", "\n", "\n", "if", "_init_setup_model", ":", "\n", "            ", "self", ".", "_setup_model", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Stanford-ILIAD_PantheonRL.modular.learn.ModularAlgorithm._setup_model": [[119, 154], ["learn.ModularAlgorithm._setup_lr_schedule", "learn.ModularAlgorithm.set_random_seed", "learn.ModularAlgorithm.policy_class", "learn.ModularAlgorithm.policy.to", "stable_baselines3.common.utils.get_schedule_fn", "isinstance", "buffer_cls", "isinstance", "stable_baselines3.common.utils.get_schedule_fn", "range"], "methods", ["None"], ["", "", "def", "_setup_model", "(", "self", ")", "->", "None", ":", "\n", "\n", "# OnPolicyAlgorithm's _setup_model", "\n", "        ", "self", ".", "_setup_lr_schedule", "(", ")", "\n", "self", ".", "set_random_seed", "(", "self", ".", "seed", ")", "\n", "\n", "self", ".", "policy", "=", "self", ".", "policy_class", "(", "# pytype:disable=not-instantiable", "\n", "self", ".", "observation_space", ",", "\n", "self", ".", "action_space", ",", "\n", "self", ".", "lr_schedule", ",", "\n", "use_sde", "=", "self", ".", "use_sde", ",", "\n", "**", "self", ".", "policy_kwargs", "# pytype:disable=not-instantiable", "\n", ")", "\n", "self", ".", "policy", "=", "self", ".", "policy", ".", "to", "(", "self", ".", "device", ")", "\n", "\n", "buffer_cls", "=", "DictRolloutBuffer", "if", "isinstance", "(", "self", ".", "observation_space", ",", "gym", ".", "spaces", ".", "Dict", ")", "else", "RolloutBuffer", "\n", "\n", "self", ".", "rollout_buffer", "=", "[", "buffer_cls", "(", "\n", "self", ".", "n_steps", ",", "\n", "self", ".", "observation_space", ",", "\n", "self", ".", "action_space", ",", "\n", "self", ".", "device", ",", "\n", "gamma", "=", "self", ".", "gamma", ",", "\n", "gae_lambda", "=", "self", ".", "gae_lambda", ",", "\n", "n_envs", "=", "self", ".", "n_envs", ",", "\n", ")", "for", "_", "in", "range", "(", "self", ".", "policy", ".", "num_partners", ")", "]", "\n", "\n", "# PPO's _setup_model", "\n", "# Initialize schedules for policy/value clipping", "\n", "self", ".", "clip_range", "=", "get_schedule_fn", "(", "self", ".", "clip_range", ")", "\n", "if", "self", ".", "clip_range_vf", "is", "not", "None", ":", "\n", "            ", "if", "isinstance", "(", "self", ".", "clip_range_vf", ",", "(", "float", ",", "int", ")", ")", ":", "\n", "                ", "assert", "self", ".", "clip_range_vf", ">", "0", ",", "\"`clip_range_vf` must be positive, \"", "\"pass `None` to deactivate vf clipping\"", "\n", "\n", "", "self", ".", "clip_range_vf", "=", "get_schedule_fn", "(", "self", ".", "clip_range_vf", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Stanford-ILIAD_PantheonRL.modular.learn.ModularAlgorithm.collect_rollouts": [[155, 219], ["rollout_buffer.reset", "callback.on_rollout_start", "rollout_buffer.compute_returns_and_advantage", "callback.on_rollout_end", "learn.ModularAlgorithm.policy.reset_noise", "actions.reshape.reshape.cpu().numpy", "isinstance", "env.envs[].set_partnerid", "env.step", "learn.ModularAlgorithm._update_info_buffer", "isinstance", "rollout_buffer.add", "learn.ModularAlgorithm.policy.reset_noise", "torch.no_grad", "torch.as_tensor().to", "learn.ModularAlgorithm.policy.forward", "numpy.clip", "callback.on_step", "actions.reshape.reshape.reshape", "actions.reshape.reshape.cpu", "torch.as_tensor"], "methods", ["home.repos.pwc.inspect_result.Stanford-ILIAD_PantheonRL.common.multiagentenv.MultiAgentEnv.reset", "home.repos.pwc.inspect_result.Stanford-ILIAD_PantheonRL.modular.policies.ModularPolicy.reset_noise", "home.repos.pwc.inspect_result.Stanford-ILIAD_PantheonRL.common.multiagentenv.MultiAgentEnv.set_partnerid", "home.repos.pwc.inspect_result.Stanford-ILIAD_PantheonRL.common.multiagentenv.MultiAgentEnv.step", "home.repos.pwc.inspect_result.Stanford-ILIAD_PantheonRL.common.wrappers.HistoryQueue.add", "home.repos.pwc.inspect_result.Stanford-ILIAD_PantheonRL.modular.policies.ModularPolicy.reset_noise", "home.repos.pwc.inspect_result.Stanford-ILIAD_PantheonRL.modular.policies.ModularPolicy.forward"], ["", "", "def", "collect_rollouts", "(", "\n", "self", ",", "env", ":", "VecEnv", ",", "callback", ":", "BaseCallback", ",", "rollout_buffer", ":", "RolloutBuffer", ",", "n_rollout_steps", ":", "int", ",", "partner_idx", ":", "int", "\n", ")", "->", "bool", ":", "\n", "        ", "\"\"\"\n        Collect rollouts using the current policy and fill a `RolloutBuffer`.\n\n        :param env: (VecEnv) The training environment\n        :param callback: (BaseCallback) Callback that will be called at each step\n            (and at the beginning and end of the rollout)\n        :param rollout_buffer: (RolloutBuffer) Buffer to fill with rollouts\n        :param n_steps: (int) Number of experiences to collect per environment\n        :return: (bool) True if function returned with at least `n_rollout_steps`\n            collected, False if callback terminated rollout prematurely.\n        \"\"\"", "\n", "assert", "self", ".", "_last_obs", "is", "not", "None", ",", "\"No previous observation was provided\"", "\n", "n_steps", "=", "0", "\n", "rollout_buffer", ".", "reset", "(", ")", "\n", "# Sample new weights for the state dependent exploration", "\n", "if", "self", ".", "use_sde", ":", "\n", "            ", "self", ".", "policy", ".", "reset_noise", "(", "env", ".", "num_envs", ")", "\n", "\n", "", "callback", ".", "on_rollout_start", "(", ")", "\n", "\n", "self", ".", "_last_dones", "=", "None", "\n", "while", "n_steps", "<", "n_rollout_steps", ":", "\n", "            ", "if", "self", ".", "use_sde", "and", "self", ".", "sde_sample_freq", ">", "0", "and", "n_steps", "%", "self", ".", "sde_sample_freq", "==", "0", ":", "\n", "# Sample a new noise matrix", "\n", "                ", "self", ".", "policy", ".", "reset_noise", "(", "env", ".", "num_envs", ")", "\n", "\n", "", "with", "th", ".", "no_grad", "(", ")", ":", "\n", "# Convert to pytorch tensor", "\n", "                ", "obs_tensor", "=", "th", ".", "as_tensor", "(", "self", ".", "_last_obs", ")", ".", "to", "(", "self", ".", "device", ")", "\n", "#actions, values, log_probs = self.policy.forward(obs_tensor)", "\n", "actions", ",", "values", ",", "log_probs", "=", "self", ".", "policy", ".", "forward", "(", "obs_tensor", ",", "partner_idx", "=", "partner_idx", ")", "\n", "", "actions", "=", "actions", ".", "cpu", "(", ")", ".", "numpy", "(", ")", "\n", "\n", "# Rescale and perform action", "\n", "clipped_actions", "=", "actions", "\n", "# Clip the actions to avoid out of bound error", "\n", "if", "isinstance", "(", "self", ".", "action_space", ",", "gym", ".", "spaces", ".", "Box", ")", ":", "\n", "                ", "clipped_actions", "=", "np", ".", "clip", "(", "actions", ",", "self", ".", "action_space", ".", "low", ",", "self", ".", "action_space", ".", "high", ")", "\n", "\n", "", "env", ".", "envs", "[", "0", "]", ".", "set_partnerid", "(", "partner_idx", ")", "\n", "new_obs", ",", "rewards", ",", "dones", ",", "infos", "=", "env", ".", "step", "(", "clipped_actions", ")", "\n", "\n", "if", "callback", ".", "on_step", "(", ")", "is", "False", ":", "\n", "                ", "return", "False", "\n", "\n", "", "self", ".", "_update_info_buffer", "(", "infos", ")", "\n", "n_steps", "+=", "1", "\n", "self", ".", "num_timesteps", "+=", "env", ".", "num_envs", "\n", "\n", "if", "isinstance", "(", "self", ".", "action_space", ",", "gym", ".", "spaces", ".", "Discrete", ")", ":", "\n", "# Reshape in case of discrete action", "\n", "                ", "actions", "=", "actions", ".", "reshape", "(", "-", "1", ",", "1", ")", "\n", "", "rollout_buffer", ".", "add", "(", "self", ".", "_last_obs", ",", "actions", ",", "rewards", ",", "self", ".", "_last_dones", ",", "values", ",", "log_probs", ")", "\n", "self", ".", "_last_obs", "=", "new_obs", "\n", "self", ".", "_last_dones", "=", "dones", "\n", "\n", "", "rollout_buffer", ".", "compute_returns_and_advantage", "(", "values", ",", "dones", "=", "dones", ")", "\n", "\n", "callback", ".", "on_rollout_end", "(", ")", "\n", "\n", "return", "True", "\n", "\n"]], "home.repos.pwc.inspect_result.Stanford-ILIAD_PantheonRL.modular.learn.ModularAlgorithm.train": [[221, 341], ["learn.ModularAlgorithm._update_learning_rate", "learn.ModularAlgorithm.clip_range", "range", "learn.ModularAlgorithm.logger.record", "learn.ModularAlgorithm.logger.record", "learn.ModularAlgorithm.logger.record", "learn.ModularAlgorithm.clip_range_vf", "range", "numpy.mean", "numpy.mean", "numpy.mean", "learn.ModularAlgorithm.rollout_buffer[].get", "all_kl_divs.append", "isinstance", "learn.ModularAlgorithm.policy.evaluate_actions", "values.flatten.flatten.flatten", "torch.exp", "pg_losses.append", "torch.mean().item", "clip_fractions.append", "torch.nn.functional.mse_loss", "value_losses.append", "entropy_losses.append", "zip", "torch.stack", "torch.stack", "torch.mean", "torch.mean", "torch.mean", "learn.ModularAlgorithm.policy.optimizer.zero_grad", "loss.backward", "torch.nn.utils.clip_grad_norm_", "learn.ModularAlgorithm.policy.optimizer.step", "approx_kl_divs.append", "numpy.mean", "print", "rollout_data.actions.long().flatten", "learn.ModularAlgorithm.policy.reset_noise", "torch.clamp", "torch.min().mean", "policy_loss.item", "torch.nn.functional.mse_loss.item", "log_prob.mean", "log_prob.mean.item", "torch.exp", "torch.exp", "torch.sum", "learn.ModularAlgorithm.policy.parameters", "torch.mean().detach().cpu().numpy", "numpy.mean", "advantages.mean", "advantages.std", "torch.mean", "torch.clamp", "torch.mean", "torch.abs", "rollout_data.actions.long", "torch.min", "learn.ModularAlgorithm.policy.get_action_logits_from_obs", "torch.stack.logsumexp", "composed_logits.logsumexp", "torch.mean().detach().cpu", "numpy.mean", "range", "torch.mean().detach", "torch.abs", "torch.mean"], "methods", ["home.repos.pwc.inspect_result.Stanford-ILIAD_PantheonRL.modular.policies.ModularPolicy.evaluate_actions", "home.repos.pwc.inspect_result.Stanford-ILIAD_PantheonRL.common.multiagentenv.MultiAgentEnv.step", "home.repos.pwc.inspect_result.Stanford-ILIAD_PantheonRL.modular.policies.ModularPolicy.reset_noise", "home.repos.pwc.inspect_result.Stanford-ILIAD_PantheonRL.modular.policies.ModularPolicy.get_action_logits_from_obs"], ["", "def", "train", "(", "self", ")", "->", "None", ":", "\n", "        ", "\"\"\"\n        Update policy using the currently gathered\n        rollout buffer.\n        \"\"\"", "\n", "# Update optimizer learning rate", "\n", "self", ".", "_update_learning_rate", "(", "self", ".", "policy", ".", "optimizer", ")", "\n", "# Compute current clip range", "\n", "clip_range", "=", "self", ".", "clip_range", "(", "self", ".", "_current_progress_remaining", ")", "\n", "# Optional: clip range for the value function", "\n", "if", "self", ".", "clip_range_vf", "is", "not", "None", ":", "\n", "            ", "clip_range_vf", "=", "self", ".", "clip_range_vf", "(", "self", ".", "_current_progress_remaining", ")", "\n", "\n", "", "entropy_losses", ",", "all_kl_divs", "=", "[", "]", ",", "[", "]", "\n", "pg_losses", ",", "value_losses", "=", "[", "]", ",", "[", "]", "\n", "clip_fractions", "=", "[", "]", "\n", "\n", "for", "partner_idx", "in", "range", "(", "self", ".", "policy", ".", "num_partners", ")", ":", "\n", "# train for gradient_steps epochs", "\n", "            ", "for", "epoch", "in", "range", "(", "self", ".", "n_epochs", ")", ":", "\n", "                ", "approx_kl_divs", "=", "[", "]", "\n", "# Do a complete pass on the rollout buffer", "\n", "# for rollout_data in self.rollout_buffer.get(self.batch_size):", "\n", "for", "rollout_data", "in", "self", ".", "rollout_buffer", "[", "partner_idx", "]", ".", "get", "(", "self", ".", "batch_size", ")", ":", "\n", "                    ", "actions", "=", "rollout_data", ".", "actions", "\n", "if", "isinstance", "(", "self", ".", "action_space", ",", "spaces", ".", "Discrete", ")", ":", "\n", "# Convert discrete action from float to long", "\n", "                        ", "actions", "=", "rollout_data", ".", "actions", ".", "long", "(", ")", ".", "flatten", "(", ")", "\n", "\n", "# Re-sample the noise matrix because the log_std has changed", "\n", "# TODO: investigate why there is no issue with the gradient", "\n", "# if that line is commented (as in SAC)", "\n", "", "if", "self", ".", "use_sde", ":", "\n", "                        ", "self", ".", "policy", ".", "reset_noise", "(", "self", ".", "batch_size", ")", "\n", "\n", "#values, log_prob, entropy = self.policy.evaluate_actions(rollout_data.observations, actions)", "\n", "", "values", ",", "log_prob", ",", "entropy", "=", "self", ".", "policy", ".", "evaluate_actions", "(", "rollout_data", ".", "observations", ",", "actions", ",", "partner_idx", "=", "partner_idx", ")", "\n", "values", "=", "values", ".", "flatten", "(", ")", "\n", "# Normalize advantage", "\n", "advantages", "=", "rollout_data", ".", "advantages", "\n", "advantages", "=", "(", "advantages", "-", "advantages", ".", "mean", "(", ")", ")", "/", "(", "advantages", ".", "std", "(", ")", "+", "1e-8", ")", "\n", "\n", "# ratio between old and new policy, should be one at the first iteration", "\n", "ratio", "=", "th", ".", "exp", "(", "log_prob", "-", "rollout_data", ".", "old_log_prob", ")", "\n", "\n", "# clipped surrogate loss", "\n", "policy_loss_1", "=", "advantages", "*", "ratio", "\n", "policy_loss_2", "=", "advantages", "*", "th", ".", "clamp", "(", "ratio", ",", "1", "-", "clip_range", ",", "1", "+", "clip_range", ")", "\n", "policy_loss", "=", "-", "th", ".", "min", "(", "policy_loss_1", ",", "policy_loss_2", ")", ".", "mean", "(", ")", "\n", "\n", "# Logging", "\n", "pg_losses", ".", "append", "(", "policy_loss", ".", "item", "(", ")", ")", "\n", "clip_fraction", "=", "th", ".", "mean", "(", "(", "th", ".", "abs", "(", "ratio", "-", "1", ")", ">", "clip_range", ")", ".", "float", "(", ")", ")", ".", "item", "(", ")", "\n", "clip_fractions", ".", "append", "(", "clip_fraction", ")", "\n", "\n", "if", "self", ".", "clip_range_vf", "is", "None", ":", "\n", "# No clipping", "\n", "                        ", "values_pred", "=", "values", "\n", "", "else", ":", "\n", "# Clip the different between old and new value", "\n", "# NOTE: this depends on the reward scaling", "\n", "                        ", "values_pred", "=", "rollout_data", ".", "old_values", "+", "th", ".", "clamp", "(", "\n", "values", "-", "rollout_data", ".", "old_values", ",", "-", "clip_range_vf", ",", "clip_range_vf", "\n", ")", "\n", "# Value loss using the TD(gae_lambda) target", "\n", "", "value_loss", "=", "F", ".", "mse_loss", "(", "rollout_data", ".", "returns", ",", "values_pred", ")", "\n", "value_losses", ".", "append", "(", "value_loss", ".", "item", "(", ")", ")", "\n", "\n", "# Entropy loss favor exploration", "\n", "if", "entropy", "is", "None", ":", "\n", "# Approximate entropy when no analytical form", "\n", "                        ", "entropy_loss", "=", "log_prob", ".", "mean", "(", ")", "\n", "", "else", ":", "\n", "                        ", "entropy_loss", "=", "-", "th", ".", "mean", "(", "entropy", ")", "\n", "\n", "", "entropy_losses", ".", "append", "(", "entropy_loss", ".", "item", "(", ")", ")", "\n", "\n", "###########", "\n", "# Marginal Regularization", "\n", "###########", "\n", "# each action_dist is a Distribution object containing self.batch_size observations", "\n", "# dist.distribution.probs returns a tensor of shape (self.batch_size, self.action_space)", "\n", "# dist.sample() returns a tensor of shape (self.batch_size)", "\n", "\n", "# careful: must extract torch distribution object from stable_baseline Distribution object, otherwise old references get overwritten", "\n", "main_logits", ",", "partner_logits", "=", "zip", "(", "*", "[", "self", ".", "policy", ".", "get_action_logits_from_obs", "(", "rollout_data", ".", "observations", ",", "partner_idx", "=", "idx", ")", "for", "idx", "in", "range", "(", "self", ".", "policy", ".", "num_partners", ")", "]", ")", "\n", "main_logits", "=", "th", ".", "stack", "(", "[", "logits", "for", "logits", "in", "main_logits", "]", ")", "# (num_partners, self.batch_size, self.action_space)", "\n", "partner_logits", "=", "th", ".", "stack", "(", "[", "logits", "for", "logits", "in", "partner_logits", "]", ")", "# (num_partners, self.batch_size, self.action_space)", "\n", "composed_logits", "=", "main_logits", "+", "partner_logits", "\n", "\n", "# Regularize main prob to be the marginals", "\n", "# Wasserstein metric with unitary distances (for categorical actions)", "\n", "main_probs", "=", "th", ".", "mean", "(", "th", ".", "exp", "(", "main_logits", "-", "main_logits", ".", "logsumexp", "(", "dim", "=", "-", "1", ",", "keepdim", "=", "True", ")", ")", ",", "dim", "=", "0", ")", "\n", "composed_probs", "=", "th", ".", "mean", "(", "th", ".", "exp", "(", "composed_logits", "-", "composed_logits", ".", "logsumexp", "(", "dim", "=", "-", "1", ",", "keepdim", "=", "True", ")", ")", ",", "dim", "=", "0", ")", "\n", "marginal_regularization_loss", "=", "th", ".", "mean", "(", "th", ".", "sum", "(", "th", ".", "abs", "(", "main_probs", "-", "composed_probs", ")", ",", "dim", "=", "1", ")", ")", "\n", "###########", "\n", "\n", "loss", "=", "policy_loss", "+", "self", ".", "ent_coef", "*", "entropy_loss", "+", "self", ".", "vf_coef", "*", "value_loss", "+", "self", ".", "marginal_reg_coef", "*", "marginal_regularization_loss", "\n", "\n", "# Optimization step", "\n", "self", ".", "policy", ".", "optimizer", ".", "zero_grad", "(", ")", "\n", "loss", ".", "backward", "(", ")", "\n", "# Clip grad norm", "\n", "th", ".", "nn", ".", "utils", ".", "clip_grad_norm_", "(", "self", ".", "policy", ".", "parameters", "(", ")", ",", "self", ".", "max_grad_norm", ")", "\n", "self", ".", "policy", ".", "optimizer", ".", "step", "(", ")", "\n", "approx_kl_divs", ".", "append", "(", "th", ".", "mean", "(", "rollout_data", ".", "old_log_prob", "-", "log_prob", ")", ".", "detach", "(", ")", ".", "cpu", "(", ")", ".", "numpy", "(", ")", ")", "\n", "\n", "", "all_kl_divs", ".", "append", "(", "np", ".", "mean", "(", "approx_kl_divs", ")", ")", "\n", "\n", "if", "self", ".", "target_kl", "is", "not", "None", "and", "np", ".", "mean", "(", "approx_kl_divs", ")", ">", "1.5", "*", "self", ".", "target_kl", ":", "\n", "                    ", "print", "(", "f\"Early stopping at step {epoch} due to reaching max kl: {np.mean(approx_kl_divs):.2f}\"", ")", "\n", "break", "\n", "\n", "", "", "", "self", ".", "_n_updates", "+=", "self", ".", "n_epochs", "\n", "# explained_var = explained_variance(self.rollout_buffer.returns.flatten(), self.rollout_buffer.values.flatten())", "\n", "\n", "# Logs", "\n", "self", ".", "logger", ".", "record", "(", "\"train/entropy_loss\"", ",", "np", ".", "mean", "(", "entropy_losses", ")", ")", "\n", "self", ".", "logger", ".", "record", "(", "\"train/policy_gradient_loss\"", ",", "np", ".", "mean", "(", "pg_losses", ")", ")", "\n", "self", ".", "logger", ".", "record", "(", "\"train/value_loss\"", ",", "np", ".", "mean", "(", "value_losses", ")", ")", "\n", "# self.logger.record(\"train/approx_kl\", np.mean(approx_kl_divs))", "\n"]], "home.repos.pwc.inspect_result.Stanford-ILIAD_PantheonRL.modular.learn.ModularAlgorithm.learn": [[353, 405], ["learn.ModularAlgorithm._setup_learn", "callback.on_training_start", "callback.on_training_end", "locals", "globals", "range", "learn.ModularAlgorithm._update_current_progress_remaining", "learn.ModularAlgorithm.train", "learn.ModularAlgorithm.collect_rollouts", "int", "learn.ModularAlgorithm.logger.record", "learn.ModularAlgorithm.logger.record", "learn.ModularAlgorithm.logger.record", "learn.ModularAlgorithm.logger.record", "learn.ModularAlgorithm.logger.dump", "learn.ModularAlgorithm.env.envs[].set_partnerid", "learn.ModularAlgorithm.logger.record", "learn.ModularAlgorithm.logger.record", "int", "print", "len", "len", "stable_baselines3.common.utils.safe_mean", "stable_baselines3.common.utils.safe_mean", "time.time", "time.time"], "methods", ["home.repos.pwc.inspect_result.Stanford-ILIAD_PantheonRL.modular.learn.ModularAlgorithm.train", "home.repos.pwc.inspect_result.Stanford-ILIAD_PantheonRL.modular.learn.ModularAlgorithm.collect_rollouts", "home.repos.pwc.inspect_result.Stanford-ILIAD_PantheonRL.common.multiagentenv.MultiAgentEnv.set_partnerid"], ["", "def", "learn", "(", "\n", "self", ",", "\n", "total_timesteps", ":", "int", ",", "\n", "callback", ":", "MaybeCallback", "=", "None", ",", "\n", "log_interval", ":", "int", "=", "1", ",", "\n", "eval_env", ":", "Optional", "[", "GymEnv", "]", "=", "None", ",", "\n", "eval_freq", ":", "int", "=", "-", "1", ",", "\n", "n_eval_episodes", ":", "int", "=", "5", ",", "\n", "tb_log_name", ":", "str", "=", "\"OnPolicyAlgorithm\"", ",", "\n", "eval_log_path", ":", "Optional", "[", "str", "]", "=", "None", ",", "\n", "reset_num_timesteps", ":", "bool", "=", "True", ",", "\n", ")", "->", "\"OnPolicyAlgorithm\"", ":", "\n", "        ", "iteration", "=", "0", "\n", "\n", "total_timesteps", ",", "callback", "=", "self", ".", "_setup_learn", "(", "\n", "total_timesteps", ",", "eval_env", ",", "callback", ",", "eval_freq", ",", "n_eval_episodes", ",", "eval_log_path", ",", "reset_num_timesteps", ",", "tb_log_name", "\n", ")", "\n", "\n", "callback", ".", "on_training_start", "(", "locals", "(", ")", ",", "globals", "(", ")", ")", "\n", "\n", "while", "self", ".", "num_timesteps", "<", "total_timesteps", ":", "\n", "\n", "            ", "for", "partner_idx", "in", "range", "(", "self", ".", "policy", ".", "num_partners", ")", ":", "\n", "                ", "try", ":", "self", ".", "env", ".", "envs", "[", "0", "]", ".", "set_partnerid", "(", "partner_idx", ")", "\n", "except", ":", "\n", "                    ", "print", "(", "\"unable to switch\"", ")", "\n", "pass", "\n", "", "continue_training", "=", "self", ".", "collect_rollouts", "(", "self", ".", "env", ",", "callback", ",", "self", ".", "rollout_buffer", "[", "partner_idx", "]", ",", "n_rollout_steps", "=", "self", ".", "n_steps", ",", "partner_idx", "=", "partner_idx", ")", "\n", "\n", "", "if", "continue_training", "is", "False", ":", "\n", "                ", "break", "\n", "\n", "", "iteration", "+=", "1", "\n", "self", ".", "_update_current_progress_remaining", "(", "self", ".", "num_timesteps", ",", "total_timesteps", ")", "\n", "\n", "# Display training infos", "\n", "if", "log_interval", "is", "not", "None", "and", "iteration", "%", "log_interval", "==", "0", ":", "\n", "                ", "fps", "=", "int", "(", "self", ".", "num_timesteps", "/", "(", "time", ".", "time", "(", ")", "-", "self", ".", "start_time", ")", ")", "\n", "self", ".", "logger", ".", "record", "(", "\"time/iterations\"", ",", "iteration", ",", "exclude", "=", "\"tensorboard\"", ")", "\n", "if", "len", "(", "self", ".", "ep_info_buffer", ")", ">", "0", "and", "len", "(", "self", ".", "ep_info_buffer", "[", "0", "]", ")", ">", "0", ":", "\n", "                    ", "self", ".", "logger", ".", "record", "(", "\"rollout/ep_rew_mean\"", ",", "safe_mean", "(", "[", "ep_info", "[", "\"r\"", "]", "for", "ep_info", "in", "self", ".", "ep_info_buffer", "]", ")", ")", "\n", "self", ".", "logger", ".", "record", "(", "\"rollout/ep_len_mean\"", ",", "safe_mean", "(", "[", "ep_info", "[", "\"l\"", "]", "for", "ep_info", "in", "self", ".", "ep_info_buffer", "]", ")", ")", "\n", "", "self", ".", "logger", ".", "record", "(", "\"time/fps\"", ",", "fps", ")", "\n", "self", ".", "logger", ".", "record", "(", "\"time/time_elapsed\"", ",", "int", "(", "time", ".", "time", "(", ")", "-", "self", ".", "start_time", ")", ",", "exclude", "=", "\"tensorboard\"", ")", "\n", "self", ".", "logger", ".", "record", "(", "\"time/total_timesteps\"", ",", "self", ".", "num_timesteps", ",", "exclude", "=", "\"tensorboard\"", ")", "\n", "self", ".", "logger", ".", "dump", "(", "step", "=", "self", ".", "num_timesteps", ")", "\n", "\n", "", "self", ".", "train", "(", ")", "\n", "\n", "", "callback", ".", "on_training_end", "(", ")", "\n", "\n", "return", "self", "", "", "", ""]], "home.repos.pwc.inspect_result.Stanford-ILIAD_PantheonRL.envs.pettingzoo.PettingZooAECWrapper.__init__": [[14, 26], ["pantheonrl.common.multiagentenv.MultiAgentEnv.__init__", "base_env.action_space", "base_env.observation_space", "isinstance"], "methods", ["home.repos.pwc.inspect_result.Stanford-ILIAD_PantheonRL.overcookedgym.overcooked.OvercookedMultiEnv.__init__"], ["def", "__init__", "(", "self", ",", "base_env", ",", "ego_ind", "=", "0", ")", ":", "\n", "        ", "self", ".", "base_env", "=", "base_env", "\n", "super", "(", "PettingZooAECWrapper", ",", "self", ")", ".", "__init__", "(", "\n", "ego_ind", ",", "base_env", ".", "max_num_agents", ")", "\n", "ego_agent", "=", "base_env", ".", "possible_agents", "[", "ego_ind", "]", "\n", "self", ".", "action_space", "=", "base_env", ".", "action_space", "(", "ego_agent", ")", "\n", "\n", "obs_space", "=", "base_env", ".", "observation_space", "(", "ego_agent", ")", "\n", "if", "isinstance", "(", "obs_space", ",", "spaces", ".", "Dict", ")", ":", "\n", "            ", "obs_space", "=", "obs_space", ".", "spaces", "[", "'observation'", "]", "\n", "", "self", ".", "observation_space", "=", "obs_space", "\n", "self", ".", "_action_mask", "=", "None", "\n", "\n"]], "home.repos.pwc.inspect_result.Stanford-ILIAD_PantheonRL.envs.pettingzoo.PettingZooAECWrapper.getDummyEnv": [[27, 34], ["pettingzoo.PettingZooAECWrapper.base_env.observation_space", "isinstance", "pettingzoo.PettingZooAECWrapper.base_env.action_space", "pantheonrl.common.multiagentenv.DummyEnv"], "methods", ["None"], ["", "def", "getDummyEnv", "(", "self", ",", "player_ind", ":", "int", ")", ":", "\n", "        ", "agent", "=", "self", ".", "base_env", ".", "possible_agents", "[", "player_ind", "]", "\n", "ospace", "=", "self", ".", "base_env", ".", "observation_space", "(", "agent", ")", "\n", "if", "isinstance", "(", "ospace", ",", "spaces", ".", "Dict", ")", ":", "\n", "            ", "ospace", "=", "ospace", ".", "spaces", "[", "'observation'", "]", "\n", "", "aspace", "=", "self", ".", "base_env", ".", "action_space", "(", "agent", ")", "\n", "return", "DummyEnv", "(", "ospace", ",", "aspace", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Stanford-ILIAD_PantheonRL.envs.pettingzoo.PettingZooAECWrapper.n_step": [[35, 82], ["pettingzoo.PettingZooAECWrapper.base_env.step", "pettingzoo.PettingZooAECWrapper.base_env.possible_agents.index", "pettingzoo.PettingZooAECWrapper.base_env.observe", "isinstance", "pettingzoo.PettingZooAECWrapper.base_env.rewards.items", "all", "pettingzoo.PettingZooAECWrapper._action_mask.tolist().index", "pettingzoo.PettingZooAECWrapper.base_env.dones.values", "tuple", "pettingzoo.PettingZooAECWrapper._action_mask.tolist", "pettingzoo.PettingZooAECWrapper.base_env.possible_agents.index"], "methods", ["home.repos.pwc.inspect_result.Stanford-ILIAD_PantheonRL.common.multiagentenv.MultiAgentEnv.step", "home.repos.pwc.inspect_result.Stanford-ILIAD_PantheonRL.adap.policies.MultModel.values"], ["", "def", "n_step", "(", "\n", "self", ",", "\n", "actions", ":", "List", "[", "np", ".", "ndarray", "]", ",", "\n", ")", "->", "Tuple", "[", "Tuple", "[", "int", ",", "...", "]", ",", "\n", "Tuple", "[", "Optional", "[", "np", ".", "ndarray", "]", ",", "...", "]", ",", "\n", "Tuple", "[", "float", ",", "...", "]", ",", "\n", "bool", ",", "\n", "Dict", "]", ":", "\n", "        ", "\"\"\"\n        Perform the actions specified by the agents that will move. This\n        function returns a tuple of (next agents, observations, both rewards,\n        done, info).\n\n        This function is called by the `step` function.\n\n        :param actions: List of action provided agents that are acting on this\n        step.\n\n        :returns:\n            agents: Tuple representing the agents to call for the next actions\n            observations: Tuple representing the next observations (ego, alt)\n            rewards: Tuple representing the rewards of all agents\n            done: Whether the episode has ended\n            info: Extra information about the environment\n        \"\"\"", "\n", "agent", "=", "self", ".", "base_env", ".", "agent_selection", "\n", "act", "=", "actions", "[", "0", "]", "\n", "if", "self", ".", "_action_mask", "is", "not", "None", "and", "not", "self", ".", "_action_mask", "[", "act", "]", ":", "\n", "            ", "act", "=", "self", ".", "_action_mask", ".", "tolist", "(", ")", ".", "index", "(", "1", ")", "\n", "\n", "", "self", ".", "base_env", ".", "step", "(", "act", ")", "\n", "\n", "agent", "=", "self", ".", "base_env", ".", "agent_selection", "\n", "agent_idx", "=", "self", ".", "base_env", ".", "possible_agents", ".", "index", "(", "agent", ")", "\n", "obs", "=", "self", ".", "base_env", ".", "observe", "(", "agent", ")", "\n", "\n", "if", "isinstance", "(", "obs", ",", "dict", ")", ":", "\n", "            ", "self", ".", "_action_mask", "=", "obs", "[", "'action_mask'", "]", "\n", "obs", "=", "obs", "[", "'observation'", "]", "\n", "\n", "", "rewards", "=", "[", "0", "]", "*", "self", ".", "n_players", "\n", "for", "key", ",", "val", "in", "self", ".", "base_env", ".", "rewards", ".", "items", "(", ")", ":", "\n", "            ", "rewards", "[", "self", ".", "base_env", ".", "possible_agents", ".", "index", "(", "key", ")", "]", "=", "val", "\n", "\n", "", "done", "=", "all", "(", "self", ".", "base_env", ".", "dones", ".", "values", "(", ")", ")", "\n", "info", "=", "self", ".", "base_env", ".", "infos", "[", "self", ".", "base_env", ".", "possible_agents", "[", "self", ".", "ego_ind", "]", "]", "\n", "return", "(", "agent_idx", ",", ")", ",", "(", "obs", ",", ")", ",", "tuple", "(", "rewards", ")", ",", "done", ",", "info", "\n", "\n"]], "home.repos.pwc.inspect_result.Stanford-ILIAD_PantheonRL.envs.pettingzoo.PettingZooAECWrapper.n_reset": [[83, 106], ["pettingzoo.PettingZooAECWrapper.base_env.reset", "pettingzoo.PettingZooAECWrapper.base_env.possible_agents.index", "pettingzoo.PettingZooAECWrapper.base_env.observe", "isinstance"], "methods", ["home.repos.pwc.inspect_result.Stanford-ILIAD_PantheonRL.common.multiagentenv.MultiAgentEnv.reset"], ["", "def", "n_reset", "(", "self", ")", "->", "Tuple", "[", "Tuple", "[", "int", ",", "...", "]", ",", "\n", "Tuple", "[", "Optional", "[", "np", ".", "ndarray", "]", ",", "...", "]", "]", ":", "\n", "        ", "\"\"\"\n        Reset the environment and return which agents will move first along\n        with their initial observations.\n\n        This function is called by the `reset` function.\n\n        :returns:\n            agents: Tuple representing the agents that will move first\n            observations: Tuple representing the observations of both agents\n        \"\"\"", "\n", "self", ".", "base_env", ".", "reset", "(", ")", "\n", "agent", "=", "self", ".", "base_env", ".", "agent_selection", "\n", "agent_idx", "=", "self", ".", "base_env", ".", "possible_agents", ".", "index", "(", "agent", ")", "\n", "obs", "=", "self", ".", "base_env", ".", "observe", "(", "agent", ")", "\n", "\n", "if", "isinstance", "(", "obs", ",", "dict", ")", ":", "\n", "            ", "self", ".", "_action_mask", "=", "obs", "[", "'action_mask'", "]", "\n", "obs", "=", "obs", "[", "'observation'", "]", "\n", "\n", "", "self", ".", "agent_counts", "=", "[", "0", "]", "*", "self", ".", "n_players", "\n", "return", "(", "agent_idx", ",", ")", ",", "(", "obs", ",", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.Stanford-ILIAD_PantheonRL.liargym.liar.LiarDefaultAgent.get_action": [[31, 39], ["obs.tolist.tolist.tolist", "max", "hand.index", "numpy.array", "numpy.array"], "methods", ["None"], ["    ", "def", "get_action", "(", "self", ",", "obs", ",", "record", "=", "True", ")", ":", "\n", "        ", "obs", "=", "obs", ".", "tolist", "(", ")", "\n", "hand", "=", "obs", "[", ":", "N", "]", "\n", "maxval", "=", "max", "(", "hand", ")", "\n", "count", "=", "hand", ".", "index", "(", "maxval", ")", "\n", "if", "obs", "[", "N", "]", "!=", "N", "and", "obs", "[", "N", "+", "1", "]", ">", "maxval", ":", "\n", "            ", "return", "np", ".", "array", "(", "BLUFF", ")", "\n", "", "return", "np", ".", "array", "(", "[", "count", ",", "maxval", "]", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Stanford-ILIAD_PantheonRL.liargym.liar.LiarDefaultAgent.update": [[40, 42], ["None"], "methods", ["None"], ["", "def", "update", "(", "self", ",", "reward", ",", "done", ")", ":", "\n", "        ", "pass", "\n", "\n"]], "home.repos.pwc.inspect_result.Stanford-ILIAD_PantheonRL.liargym.liar.LiarEnv.__init__": [[46, 51], ["pantheonrl.common.multiagentenv.TurnBasedEnv.__init__"], "methods", ["home.repos.pwc.inspect_result.Stanford-ILIAD_PantheonRL.overcookedgym.overcooked.OvercookedMultiEnv.__init__"], ["    ", "def", "__init__", "(", "self", ",", "probegostart", "=", "0.5", ")", ":", "\n", "        ", "super", "(", "LiarEnv", ",", "self", ")", ".", "__init__", "(", "probegostart", "=", "probegostart", ")", "\n", "self", ".", "history", "=", "[", "]", "\n", "self", ".", "observation_space", "=", "OBS_SPACE", "\n", "self", ".", "action_space", "=", "ACTION_SPACE", "\n", "\n"]], "home.repos.pwc.inspect_result.Stanford-ILIAD_PantheonRL.liargym.liar.LiarEnv.getObs": [[52, 56], ["numpy.array", "len"], "methods", ["None"], ["", "def", "getObs", "(", "self", ",", "isego", ")", ":", "\n", "        ", "prevmove", "=", "self", ".", "history", "+", "DEFAULT", "*", "(", "MAX_MOVES", "-", "len", "(", "self", ".", "history", ")", "//", "2", ")", "\n", "return", "np", ".", "array", "(", "(", "self", ".", "egohand", "if", "isego", "else", "self", ".", "althand", ")", "+", "prevmove", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Stanford-ILIAD_PantheonRL.liargym.liar.LiarEnv.sanitize_action": [[57, 67], ["action.tolist", "len", "len"], "methods", ["None"], ["", "def", "sanitize_action", "(", "self", ",", "action", ")", ":", "\n", "\n", "        ", "if", "len", "(", "self", ".", "history", ")", "!=", "0", "and", "(", "action", "[", "1", "]", "<=", "self", ".", "history", "[", "1", "]", "\n", "or", "action", "[", "0", "]", "==", "N", ")", ":", "\n", "            ", "return", "BLUFF", "\n", "\n", "", "if", "len", "(", "self", ".", "history", ")", "==", "0", "and", "action", "[", "0", "]", "==", "N", ":", "\n", "            ", "return", "[", "0", ",", "0", "]", "\n", "\n", "", "return", "action", ".", "tolist", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Stanford-ILIAD_PantheonRL.liargym.liar.LiarEnv.eval_bluff": [[68, 75], ["len"], "methods", ["None"], ["", "def", "eval_bluff", "(", "self", ")", ":", "\n", "        ", "if", "len", "(", "self", ".", "history", ")", "==", "0", ":", "\n", "            ", "return", "False", "\n", "\n", "", "side", "=", "self", ".", "history", "[", "0", "]", "\n", "trueans", "=", "self", ".", "egohand", "[", "side", "]", "+", "self", ".", "althand", "[", "side", "]", "-", "1", "\n", "return", "self", ".", "history", "[", "1", "]", ">", "trueans", "\n", "\n"]], "home.repos.pwc.inspect_result.Stanford-ILIAD_PantheonRL.liargym.liar.LiarEnv.player_step": [[76, 83], ["liar.LiarEnv.sanitize_action", "liar.LiarEnv.getObs", "liar.LiarEnv.eval_bluff", "liar.LiarEnv.getObs"], "methods", ["home.repos.pwc.inspect_result.Stanford-ILIAD_PantheonRL.liargym.liar.LiarEnv.sanitize_action", "home.repos.pwc.inspect_result.Stanford-ILIAD_PantheonRL.liargym.liar.LiarEnv.getObs", "home.repos.pwc.inspect_result.Stanford-ILIAD_PantheonRL.liargym.liar.LiarEnv.eval_bluff", "home.repos.pwc.inspect_result.Stanford-ILIAD_PantheonRL.liargym.liar.LiarEnv.getObs"], ["", "def", "player_step", "(", "self", ",", "action", ",", "isego", ")", ":", "\n", "        ", "action", "=", "self", ".", "sanitize_action", "(", "action", ")", "\n", "if", "action", "==", "BLUFF", ":", "\n", "            ", "didwin", "=", "(", "self", ".", "eval_bluff", "(", ")", "==", "isego", ")", "\n", "return", "self", ".", "getObs", "(", "not", "isego", ")", ",", "WIN", "if", "didwin", "else", "LOSE", ",", "True", ",", "{", "}", "\n", "", "self", ".", "history", "=", "action", "+", "self", ".", "history", "\n", "return", "self", ".", "getObs", "(", "not", "isego", ")", ",", "(", "0", ",", "0", ")", ",", "False", ",", "{", "}", "\n", "\n"]], "home.repos.pwc.inspect_result.Stanford-ILIAD_PantheonRL.liargym.liar.LiarEnv.ego_step": [[84, 89], ["liar.LiarEnv.player_step"], "methods", ["home.repos.pwc.inspect_result.Stanford-ILIAD_PantheonRL.liargym.liar.LiarEnv.player_step"], ["", "def", "ego_step", "(", "self", ",", "action", ")", ":", "\n", "        ", "\"\"\"\n        Return partner's obs, both rewards, is done, and info\n        \"\"\"", "\n", "return", "self", ".", "player_step", "(", "action", ",", "True", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Stanford-ILIAD_PantheonRL.liargym.liar.LiarEnv.alt_step": [[90, 95], ["liar.LiarEnv.player_step"], "methods", ["home.repos.pwc.inspect_result.Stanford-ILIAD_PantheonRL.liargym.liar.LiarEnv.player_step"], ["", "def", "alt_step", "(", "self", ",", "action", ")", ":", "\n", "        ", "\"\"\"\n        Return ego's obs, both rewards, is done, and info\n        \"\"\"", "\n", "return", "self", ".", "player_step", "(", "action", ",", "False", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Stanford-ILIAD_PantheonRL.liargym.liar.LiarEnv.multi_reset": [[96, 102], ["liar.randRoll", "liar.randRoll", "liar.LiarEnv.getObs"], "methods", ["home.repos.pwc.inspect_result.Stanford-ILIAD_PantheonRL.liargym.liar.randRoll", "home.repos.pwc.inspect_result.Stanford-ILIAD_PantheonRL.liargym.liar.randRoll", "home.repos.pwc.inspect_result.Stanford-ILIAD_PantheonRL.liargym.liar.LiarEnv.getObs"], ["", "def", "multi_reset", "(", "self", ",", "egofirst", ")", ":", "\n", "        ", "self", ".", "history", "=", "[", "]", "\n", "self", ".", "egohand", "=", "randRoll", "(", ")", "\n", "self", ".", "althand", "=", "randRoll", "(", ")", "\n", "\n", "return", "self", ".", "getObs", "(", "egofirst", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.Stanford-ILIAD_PantheonRL.liargym.liar.randRoll": [[22, 27], ["range", "dice.append", "dice.count", "numpy.random.randint", "range"], "function", ["None"], ["def", "randRoll", "(", ")", ":", "\n", "    ", "dice", "=", "[", "]", "\n", "for", "i", "in", "range", "(", "M", ")", ":", "\n", "        ", "dice", ".", "append", "(", "np", ".", "random", ".", "randint", "(", "N", ")", ")", "\n", "", "return", "[", "dice", ".", "count", "(", "i", ")", "for", "i", "in", "range", "(", "N", ")", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.Stanford-ILIAD_PantheonRL.rpsgym.rps.RPSWeightedAgent.__init__": [[15, 24], ["None"], "methods", ["None"], ["    ", "def", "__init__", "(", "self", ",", "r", "=", "1", ",", "p", "=", "1", ",", "s", "=", "1", ",", "np_random", "=", "np", ".", "random", ")", ":", "\n", "        ", "weight", "=", "r", "+", "p", "+", "s", "\n", "if", "weight", "==", "0", ":", "\n", "            ", "self", ".", "c0", "=", "1.", "/", "3", "\n", "self", ".", "c1", "=", "2.", "/", "3", "\n", "", "else", ":", "\n", "            ", "self", ".", "c0", "=", "r", "/", "weight", "\n", "self", ".", "c1", "=", "(", "r", "+", "p", ")", "/", "weight", "\n", "", "self", ".", "np_random", "=", "np_random", "\n", "\n"]], "home.repos.pwc.inspect_result.Stanford-ILIAD_PantheonRL.rpsgym.rps.RPSWeightedAgent.get_action": [[25, 28], ["rps.RPSWeightedAgent.np_random.rand"], "methods", ["None"], ["", "def", "get_action", "(", "self", ",", "obs", ",", "record", "=", "True", ")", ":", "\n", "        ", "roll", "=", "self", ".", "np_random", ".", "rand", "(", ")", "\n", "return", "0", "if", "roll", "<", "self", ".", "c0", "else", "1", "if", "roll", "<", "self", ".", "c1", "else", "2", "\n", "\n"]], "home.repos.pwc.inspect_result.Stanford-ILIAD_PantheonRL.rpsgym.rps.RPSWeightedAgent.update": [[29, 31], ["None"], "methods", ["None"], ["", "def", "update", "(", "self", ",", "reward", ",", "done", ")", ":", "\n", "        ", "pass", "\n", "\n"]], "home.repos.pwc.inspect_result.Stanford-ILIAD_PantheonRL.rpsgym.rps.RPSEnv.__init__": [[35, 40], ["pantheonrl.common.multiagentenv.SimultaneousEnv.__init__"], "methods", ["home.repos.pwc.inspect_result.Stanford-ILIAD_PantheonRL.overcookedgym.overcooked.OvercookedMultiEnv.__init__"], ["    ", "def", "__init__", "(", "self", ")", ":", "\n", "        ", "super", "(", "RPSEnv", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "history", "=", "[", "]", "\n", "self", ".", "observation_space", "=", "OBS_SPACE", "\n", "self", ".", "action_space", "=", "ACTION_SPACE", "\n", "\n"]], "home.repos.pwc.inspect_result.Stanford-ILIAD_PantheonRL.rpsgym.rps.RPSEnv.multi_step": [[41, 46], ["None"], "methods", ["None"], ["", "def", "multi_step", "(", "self", ",", "ego_action", ",", "alt_action", ")", ":", "\n", "        ", "outcome", "=", "(", "ego_action", "-", "alt_action", "+", "3", ")", "%", "3", "\n", "outcome", "=", "-", "1", "if", "outcome", "==", "2", "else", "outcome", "\n", "\n", "return", "(", "NULL_OBS", ",", "NULL_OBS", ")", ",", "(", "outcome", ",", "-", "outcome", ")", ",", "True", ",", "{", "}", "\n", "\n"]], "home.repos.pwc.inspect_result.Stanford-ILIAD_PantheonRL.rpsgym.rps.RPSEnv.multi_reset": [[47, 49], ["None"], "methods", ["None"], ["", "def", "multi_reset", "(", "self", ")", ":", "\n", "        ", "return", "(", "NULL_OBS", ",", "NULL_OBS", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.Stanford-ILIAD_PantheonRL.blockworldgym.blockworld.BlockEnv.__init__": [[35, 43], ["pantheonrl.common.multiagentenv.TurnBasedEnv.__init__"], "methods", ["home.repos.pwc.inspect_result.Stanford-ILIAD_PantheonRL.overcookedgym.overcooked.OvercookedMultiEnv.__init__"], ["    ", "def", "__init__", "(", "self", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "probegostart", "=", "1", ")", "\n", "self", ".", "observation_space", "=", "PLANNER_OBS_SPACE", "\n", "self", ".", "partner_observation_space", "=", "CONSTRUCTOR_OBS_SPACE", "\n", "self", ".", "action_space", "=", "PLANNER_ACTION_SPACE", "\n", "self", ".", "partner_action_space", "=", "CONSTRUCTOR_ACTION_SPACE", "\n", "self", ".", "partner_env", "=", "PartnerEnv", "\n", "self", ".", "viewer", "=", "None", "\n", "\n"]], "home.repos.pwc.inspect_result.Stanford-ILIAD_PantheonRL.blockworldgym.blockworld.BlockEnv.getDummyEnv": [[44, 46], ["None"], "methods", ["None"], ["", "def", "getDummyEnv", "(", "self", ",", "player_ind", ":", "int", ")", ":", "\n", "        ", "return", "PartnerEnv", "if", "player_ind", "else", "self", "\n", "\n"]], "home.repos.pwc.inspect_result.Stanford-ILIAD_PantheonRL.blockworldgym.blockworld.BlockEnv.multi_reset": [[47, 53], ["pantheonrl.envs.blockworldgym.gridutils.generate_random_world", "numpy.zeros", "blockworld.BlockEnv.get_obs"], "methods", ["home.repos.pwc.inspect_result.Stanford-ILIAD_PantheonRL.blockworldgym.gridutils.generate_random_world", "home.repos.pwc.inspect_result.Stanford-ILIAD_PantheonRL.blockworldgym.simpleblockworld.SimpleBlockEnv.get_obs"], ["", "def", "multi_reset", "(", "self", ",", "egofirst", ")", ":", "\n", "        ", "self", ".", "gridworld", "=", "generate_random_world", "(", "GRIDLEN", ",", "NUM_BLOCKS", ",", "NUM_COLORS", ")", "\n", "self", ".", "constructor_obs", "=", "np", ".", "zeros", "(", "(", "GRIDLEN", ",", "GRIDLEN", ")", ")", "\n", "self", ".", "last_token", "=", "0", "\n", "self", ".", "viewer", "=", "None", "\n", "return", "self", ".", "get_obs", "(", "egofirst", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Stanford-ILIAD_PantheonRL.blockworldgym.blockworld.BlockEnv.get_obs": [[54, 60], ["numpy.concatenate", "list", "numpy.array", "blockworld.BlockEnv.constructor_obs.flatten"], "methods", ["None"], ["", "def", "get_obs", "(", "self", ",", "isego", ")", ":", "\n", "        ", "if", "isego", ":", "\n", "            ", "return", "np", ".", "concatenate", "(", "(", "self", ".", "gridworld", ",", "self", ".", "constructor_obs", ")", ",", "axis", "=", "None", ")", "\n", "", "else", ":", "\n", "            ", "observations", "=", "list", "(", "self", ".", "constructor_obs", ".", "flatten", "(", ")", ")", "\n", "return", "np", ".", "array", "(", "(", "[", "self", ".", "last_token", "]", "+", "observations", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Stanford-ILIAD_PantheonRL.blockworldgym.blockworld.BlockEnv.ego_step": [[61, 68], ["blockworld.BlockEnv.get_reward", "blockworld.BlockEnv.get_obs"], "methods", ["home.repos.pwc.inspect_result.Stanford-ILIAD_PantheonRL.blockworldgym.simpleblockworld.SimpleBlockEnv.get_reward", "home.repos.pwc.inspect_result.Stanford-ILIAD_PantheonRL.blockworldgym.simpleblockworld.SimpleBlockEnv.get_obs"], ["", "", "def", "ego_step", "(", "self", ",", "action", ")", ":", "\n", "        ", "self", ".", "last_token", "=", "action", "\n", "done", "=", "action", "==", "NUM_TOKENS", "-", "1", "\n", "reward", "=", "0", "\n", "if", "done", ":", "\n", "            ", "reward", "=", "self", ".", "get_reward", "(", ")", "\n", "", "return", "self", ".", "get_obs", "(", "False", ")", ",", "[", "reward", ",", "reward", "]", ",", "done", ",", "{", "}", "\n", "\n"]], "home.repos.pwc.inspect_result.Stanford-ILIAD_PantheonRL.blockworldgym.blockworld.BlockEnv.alt_step": [[69, 76], ["pantheonrl.envs.blockworldgym.gridutils.gravity", "blockworld.BlockEnv.get_obs", "pantheonrl.envs.blockworldgym.gridutils.place"], "methods", ["home.repos.pwc.inspect_result.Stanford-ILIAD_PantheonRL.blockworldgym.gridutils.gravity", "home.repos.pwc.inspect_result.Stanford-ILIAD_PantheonRL.blockworldgym.simpleblockworld.SimpleBlockEnv.get_obs", "home.repos.pwc.inspect_result.Stanford-ILIAD_PantheonRL.blockworldgym.gridutils.place"], ["", "def", "alt_step", "(", "self", ",", "action", ")", ":", "\n", "        ", "x", ",", "orientation", ",", "color", "=", "action", "[", "0", "]", ",", "action", "[", "1", "]", ",", "action", "[", "2", "]", "+", "1", "\n", "if", "not", "(", "orientation", "==", "HORIZONTAL", "and", "x", "==", "GRIDLEN", "-", "1", ")", ":", "\n", "            ", "y", "=", "gravity", "(", "self", ".", "constructor_obs", ",", "orientation", ",", "x", ")", "\n", "if", "y", "!=", "-", "1", ":", "\n", "                ", "place", "(", "self", ".", "constructor_obs", ",", "x", ",", "y", ",", "color", ",", "orientation", ")", "\n", "", "", "return", "self", ".", "get_obs", "(", "True", ")", ",", "[", "0", ",", "0", "]", ",", "False", ",", "{", "}", "\n", "\n"]], "home.repos.pwc.inspect_result.Stanford-ILIAD_PantheonRL.blockworldgym.blockworld.BlockEnv.get_reward": [[77, 84], ["pantheonrl.envs.blockworldgym.gridutils.matches", "numpy.count_nonzero", "numpy.count_nonzero"], "methods", ["home.repos.pwc.inspect_result.Stanford-ILIAD_PantheonRL.blockworldgym.gridutils.matches"], ["", "def", "get_reward", "(", "self", ")", ":", "\n", "# we use F1 score which is 2 * precision * recall / (precision + recall)", "\n", "# also = 2 * truepos / (selected + relevant)", "\n", "        ", "truepos", "=", "matches", "(", "self", ".", "constructor_obs", ",", "self", ".", "gridworld", ")", "\n", "selected", "=", "np", ".", "count_nonzero", "(", "self", ".", "constructor_obs", ")", "\n", "relevant", "=", "np", ".", "count_nonzero", "(", "self", ".", "gridworld", ")", "\n", "return", "2", "*", "truepos", "/", "(", "selected", "+", "relevant", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Stanford-ILIAD_PantheonRL.blockworldgym.blockworld.BlockEnv.render": [[85, 120], ["range", "blockworld.BlockEnv.viewer.render", "rendering.Viewer", "range", "len", "range", "len", "range", "len", "len", "rendering.PolyLine", "rendering.FilledPolygon.set_linewidth", "blockworld.BlockEnv.viewer.add_geom", "rendering.FilledPolygon", "rendering.FilledPolygon.set_color", "blockworld.BlockEnv.viewer.add_geom", "rendering.FilledPolygon.set_color", "rendering.FilledPolygon.set_color", "rendering.FilledPolygon.set_color", "rendering.FilledPolygon.set_color"], "methods", ["home.repos.pwc.inspect_result.Stanford-ILIAD_PantheonRL.overcookedgym.overcooked.OvercookedMultiEnv.render"], ["", "def", "render", "(", "self", ",", "mode", "=", "\"human\"", ")", ":", "\n", "        ", "from", "gym", ".", "envs", ".", "classic_control", "import", "rendering", "\n", "\n", "screen_width", "=", "700", "\n", "scale", "=", "screen_width", "/", "GRIDLEN", "\n", "if", "self", ".", "viewer", "is", "None", ":", "\n", "            ", "self", ".", "viewer", "=", "rendering", ".", "Viewer", "(", "screen_width", ",", "screen_width", ")", "\n", "for", "i", "in", "range", "(", "len", "(", "self", ".", "gridworld", ")", ")", ":", "\n", "                ", "for", "j", "in", "range", "(", "len", "(", "self", ".", "gridworld", "[", "i", "]", ")", ")", ":", "\n", "                    ", "left", ",", "right", ",", "top", ",", "bottom", "=", "j", "*", "scale", ",", "(", "j", "+", "1", ")", "*", "scale", ",", "(", "GRIDLEN", "-", "i", ")", "*", "scale", ",", "(", "GRIDLEN", "-", "(", "i", "+", "1", ")", ")", "*", "scale", "\n", "newblock", "=", "rendering", ".", "PolyLine", "(", "\n", "[", "(", "left", ",", "bottom", ")", ",", "(", "left", ",", "top", ")", ",", "(", "right", ",", "top", ")", ",", "(", "right", ",", "bottom", ")", "]", ",", "close", "=", "True", ")", "\n", "newblock", ".", "set_linewidth", "(", "10", ")", "\n", "self", ".", "viewer", ".", "add_geom", "(", "newblock", ")", "\n", "if", "self", ".", "gridworld", "[", "i", "]", "[", "j", "]", "==", "RED", ":", "\n", "                        ", "newblock", ".", "set_color", "(", "0.98", ",", "0.02", ",", "0.02", ")", "\n", "", "elif", "self", ".", "gridworld", "[", "i", "]", "[", "j", "]", "==", "BLUE", ":", "\n", "                        ", "newblock", ".", "set_color", "(", "0.02", ",", "0.02", ",", "0.98", ")", "\n", "", "", "", "", "for", "i", "in", "range", "(", "len", "(", "self", ".", "constructor_obs", ")", ")", ":", "\n", "            ", "for", "j", "in", "range", "(", "len", "(", "self", ".", "constructor_obs", "[", "i", "]", ")", ")", ":", "\n", "                ", "if", "not", "self", ".", "constructor_obs", "[", "i", "]", "[", "j", "]", "==", "0", ":", "\n", "                    ", "left", ",", "right", ",", "top", ",", "bottom", "=", "j", "*", "scale", ",", "(", "j", "+", "1", ")", "*", "scale", ",", "(", "GRIDLEN", "-", "i", ")", "*", "scale", ",", "(", "GRIDLEN", "-", "(", "i", "+", "1", ")", ")", "*", "scale", "\n", "newblock", "=", "rendering", ".", "FilledPolygon", "(", "\n", "[", "(", "left", ",", "bottom", ")", ",", "(", "left", ",", "top", ")", ",", "(", "right", ",", "top", ")", ",", "(", "right", ",", "bottom", ")", "]", ")", "\n", "newblock", ".", "set_color", "(", "0.5", ",", "0.5", ",", "0.5", ")", "\n", "self", ".", "viewer", ".", "add_geom", "(", "newblock", ")", "\n", "if", "self", ".", "constructor_obs", "[", "i", "]", "[", "j", "]", "==", "RED", ":", "\n", "                        ", "newblock", ".", "set_color", "(", "0.98", ",", "0.02", ",", "0.02", ")", "\n", "", "elif", "self", ".", "constructor_obs", "[", "i", "]", "[", "j", "]", "==", "BLUE", ":", "\n", "                        ", "newblock", ".", "set_color", "(", "0.02", ",", "0.02", ",", "0.98", ")", "\n", "", "", "", "", "return", "self", ".", "viewer", ".", "render", "(", "return_rgb_array", "=", "mode", "==", "\"rgb_array\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Stanford-ILIAD_PantheonRL.blockworldgym.blockworld.DefaultConstructorAgent.get_action": [[123, 134], ["int"], "methods", ["None"], ["    ", "def", "get_action", "(", "self", ",", "obs", ",", "recording", "=", "True", ")", ":", "\n", "        ", "token", "=", "int", "(", "obs", "[", "0", "]", ")", "\n", "if", "token", "==", "0", "or", "token", "==", "29", ":", "\n", "            ", "return", "[", "GRIDLEN", "-", "1", ",", "VERTICAL", ",", "0", "]", "\n", "", "token", "-=", "1", "\n", "color", "=", "token", "%", "2", "\n", "token", "=", "token", "//", "2", "\n", "orientation", "=", "token", "%", "2", "\n", "token", "=", "token", "//", "2", "\n", "x", "=", "token", "\n", "return", "[", "x", ",", "orientation", ",", "color", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.Stanford-ILIAD_PantheonRL.blockworldgym.blockworld.DefaultConstructorAgent.update": [[135, 137], ["None"], "methods", ["None"], ["", "def", "update", "(", "self", ",", "reward", ",", "done", ")", ":", "\n", "        ", "pass", "\n", "", "", ""]], "home.repos.pwc.inspect_result.Stanford-ILIAD_PantheonRL.blockworldgym.gridutils.generate_random_world": [[8, 15], ["numpy.zeros", "gridutils.drop_random"], "function", ["home.repos.pwc.inspect_result.Stanford-ILIAD_PantheonRL.blockworldgym.gridutils.drop_random"], ["def", "generate_random_world", "(", "width", ",", "num_blocks", ",", "num_colors", ")", ":", "\n", "    ", "gridworld", "=", "np", ".", "zeros", "(", "(", "width", ",", "width", ")", ")", "\n", "blocks_placed", "=", "0", "\n", "while", "blocks_placed", "<", "num_blocks", ":", "\n", "        ", "if", "drop_random", "(", "width", ",", "gridworld", ",", "num_colors", ")", "!=", "-", "1", ":", "\n", "            ", "blocks_placed", "+=", "1", "\n", "", "", "return", "gridworld", "\n", "\n"]], "home.repos.pwc.inspect_result.Stanford-ILIAD_PantheonRL.blockworldgym.gridutils.drop_random": [[16, 29], ["numpy.random.randint", "gridutils.gravity", "numpy.random.randint", "numpy.random.randint", "gridutils.place", "numpy.random.randint"], "function", ["home.repos.pwc.inspect_result.Stanford-ILIAD_PantheonRL.blockworldgym.gridutils.gravity", "home.repos.pwc.inspect_result.Stanford-ILIAD_PantheonRL.blockworldgym.gridutils.place"], ["", "def", "drop_random", "(", "width", ",", "gridworld", ",", "num_colors", ")", ":", "\n", "    ", "orientation", "=", "np", ".", "random", ".", "randint", "(", "2", ")", "\n", "if", "orientation", "==", "HORIZONTAL", ":", "\n", "        ", "x", "=", "np", ".", "random", ".", "randint", "(", "width", "-", "1", ")", "# can't drop at the last coordinate if horizontal", "\n", "", "else", ":", "\n", "        ", "x", "=", "np", ".", "random", ".", "randint", "(", "width", ")", "\n", "", "y", "=", "gravity", "(", "gridworld", ",", "orientation", ",", "x", ")", "\n", "if", "y", "==", "-", "1", ":", "\n", "        ", "return", "-", "1", "# error", "\n", "", "else", ":", "\n", "        ", "color", "=", "np", ".", "random", ".", "randint", "(", "num_colors", ")", "+", "1", "\n", "place", "(", "gridworld", ",", "x", ",", "y", ",", "color", ",", "orientation", ")", "\n", "", "return", "0", "\n", "\n"]], "home.repos.pwc.inspect_result.Stanford-ILIAD_PantheonRL.blockworldgym.gridutils.place": [[30, 36], ["None"], "function", ["None"], ["", "def", "place", "(", "gridworld", ",", "x", ",", "y", ",", "color", ",", "orientation", ")", ":", "\n", "    ", "gridworld", "[", "y", "]", "[", "x", "]", "=", "color", "\n", "if", "orientation", "==", "HORIZONTAL", ":", "\n", "        ", "gridworld", "[", "y", "]", "[", "x", "+", "1", "]", "=", "color", "\n", "", "if", "orientation", "==", "VERTICAL", ":", "\n", "        ", "gridworld", "[", "y", "+", "1", "]", "[", "x", "]", "=", "color", "\n", "\n"]], "home.repos.pwc.inspect_result.Stanford-ILIAD_PantheonRL.blockworldgym.gridutils.gravity": [[37, 56], ["range", "len", "len", "len"], "function", ["None"], ["", "", "def", "gravity", "(", "gridworld", ",", "orientation", ",", "x", ")", ":", "\n", "# check if placeable", "\n", "    ", "if", "gridworld", "[", "0", "]", "[", "x", "]", "!=", "0", ":", "\n", "        ", "return", "-", "1", "\n", "", "if", "(", "orientation", "==", "HORIZONTAL", "and", "gridworld", "[", "0", "]", "[", "x", "+", "1", "]", "!=", "0", ")", "or", "(", "orientation", "==", "VERTICAL", "and", "gridworld", "[", "1", "]", "[", "x", "]", "!=", "0", ")", ":", "\n", "        ", "return", "-", "1", "\n", "", "for", "y", "in", "range", "(", "len", "(", "gridworld", ")", ")", ":", "\n", "# this is the final position if it hits something (there's something or a floor right below it)", "\n", "        ", "if", "orientation", "==", "HORIZONTAL", ":", "\n", "            ", "if", "y", "==", "len", "(", "gridworld", ")", "-", "1", ":", "\n", "                ", "return", "y", "\n", "", "if", "gridworld", "[", "y", "+", "1", "]", "[", "x", "]", "!=", "0", "or", "gridworld", "[", "y", "+", "1", "]", "[", "x", "+", "1", "]", "!=", "0", ":", "\n", "                ", "return", "y", "\n", "", "", "if", "orientation", "==", "VERTICAL", ":", "\n", "            ", "if", "y", "==", "len", "(", "gridworld", ")", "-", "2", ":", "\n", "                ", "return", "y", "\n", "", "if", "gridworld", "[", "y", "+", "2", "]", "[", "x", "]", "!=", "0", ":", "\n", "                ", "return", "y", "\n", "", "", "", "return", "-", "1", "# shouldn't be able to reach here", "\n", "\n"]], "home.repos.pwc.inspect_result.Stanford-ILIAD_PantheonRL.blockworldgym.gridutils.matches": [[57, 65], ["warnings.filterwarnings", "warnings.filterwarnings", "numpy.count_nonzero"], "function", ["None"], ["", "def", "matches", "(", "grid1", ",", "grid2", ")", ":", "\n", "# number of nonzero elements in the same place", "\n", "# we can divide the two, and if two nonzero elements are in the same place the quotient is 1", "\n", "# then count nonzeroes in the array quotient==1", "\n", "# but i'm filtering the divide by zero warning -- should be careful about this later", "\n", "    ", "warnings", ".", "filterwarnings", "(", "'ignore'", ",", "'invalid value encountered in true_divide'", ")", "\n", "warnings", ".", "filterwarnings", "(", "'ignore'", ",", "'divide by zero encountered in true_divide'", ")", "\n", "return", "np", ".", "count_nonzero", "(", "(", "grid1", "/", "grid2", "==", "1", ")", ")", "", "", ""]], "home.repos.pwc.inspect_result.Stanford-ILIAD_PantheonRL.blockworldgym.simpleblockworld.SimpleBlockEnv.__init__": [[81, 89], ["pantheonrl.common.multiagentenv.TurnBasedEnv.__init__"], "methods", ["home.repos.pwc.inspect_result.Stanford-ILIAD_PantheonRL.overcookedgym.overcooked.OvercookedMultiEnv.__init__"], ["    ", "def", "__init__", "(", "self", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "probegostart", "=", "1", ")", "\n", "self", ".", "observation_space", "=", "PLANNER_OBS_SPACE", "\n", "self", ".", "partner_observation_space", "=", "CONSTRUCTOR_OBS_SPACE", "\n", "self", ".", "action_space", "=", "PLANNER_ACTION_SPACE", "\n", "self", ".", "partner_action_space", "=", "CONSTRUCTOR_ACTION_SPACE", "\n", "self", ".", "partner_env", "=", "PartnerEnv", "\n", "self", ".", "viewer", "=", "None", "\n", "\n"]], "home.repos.pwc.inspect_result.Stanford-ILIAD_PantheonRL.blockworldgym.simpleblockworld.SimpleBlockEnv.getDummyEnv": [[90, 92], ["None"], "methods", ["None"], ["", "def", "getDummyEnv", "(", "self", ",", "player_ind", ":", "int", ")", ":", "\n", "        ", "return", "PartnerEnv", "if", "player_ind", "else", "self", "\n", "\n"]], "home.repos.pwc.inspect_result.Stanford-ILIAD_PantheonRL.blockworldgym.simpleblockworld.SimpleBlockEnv.multi_reset": [[93, 100], ["simpleblockworld.generate_grid_world", "simpleblockworld.SimpleBlockEnv.get_obs"], "methods", ["home.repos.pwc.inspect_result.Stanford-ILIAD_PantheonRL.blockworldgym.simpleblockworld.generate_grid_world", "home.repos.pwc.inspect_result.Stanford-ILIAD_PantheonRL.blockworldgym.simpleblockworld.SimpleBlockEnv.get_obs"], ["", "def", "multi_reset", "(", "self", ",", "egofirst", ")", ":", "\n", "        ", "self", ".", "gridworld", "=", "generate_grid_world", "(", ")", "\n", "self", ".", "constructor_obs", "=", "[", "[", "block", "[", "0", "]", ",", "block", "[", "1", "]", ",", "block", "[", "2", "]", ",", "0", "]", "\n", "for", "block", "in", "self", ".", "gridworld", "]", "\n", "self", ".", "last_token", "=", "0", "\n", "self", ".", "viewer", "=", "None", "\n", "return", "self", ".", "get_obs", "(", "egofirst", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Stanford-ILIAD_PantheonRL.blockworldgym.simpleblockworld.SimpleBlockEnv.get_obs": [[101, 109], ["numpy.array().flatten", "numpy.array", "numpy.array"], "methods", ["None"], ["", "def", "get_obs", "(", "self", ",", "isego", ")", ":", "\n", "        ", "if", "isego", ":", "\n", "            ", "return", "np", ".", "array", "(", "[", "self", ".", "gridworld", ",", "self", ".", "constructor_obs", "]", ")", ".", "flatten", "(", ")", "\n", "", "else", ":", "\n", "            ", "observations", "=", "[", "\n", "elem", "for", "block", "in", "self", ".", "constructor_obs", "for", "elem", "in", "block", "]", "\n", "output", "=", "np", ".", "array", "(", "(", "[", "self", ".", "last_token", "]", "+", "observations", ")", ")", "\n", "return", "output", "\n", "\n"]], "home.repos.pwc.inspect_result.Stanford-ILIAD_PantheonRL.blockworldgym.simpleblockworld.SimpleBlockEnv.ego_step": [[110, 118], ["simpleblockworld.SimpleBlockEnv.get_reward", "simpleblockworld.SimpleBlockEnv.get_obs"], "methods", ["home.repos.pwc.inspect_result.Stanford-ILIAD_PantheonRL.blockworldgym.simpleblockworld.SimpleBlockEnv.get_reward", "home.repos.pwc.inspect_result.Stanford-ILIAD_PantheonRL.blockworldgym.simpleblockworld.SimpleBlockEnv.get_obs"], ["", "", "def", "ego_step", "(", "self", ",", "action", ")", ":", "\n", "        ", "self", ".", "last_token", "=", "action", "\n", "# the planner gets to decide when they are done by taking action NUM_TOKENS - 1", "\n", "done", "=", "action", "==", "NUM_TOKENS", "-", "1", "\n", "reward", "=", "[", "0", ",", "0", "]", "\n", "if", "done", ":", "\n", "            ", "reward", "=", "self", ".", "get_reward", "(", ")", "\n", "", "return", "self", ".", "get_obs", "(", "False", ")", ",", "reward", ",", "done", ",", "{", "}", "\n", "\n"]], "home.repos.pwc.inspect_result.Stanford-ILIAD_PantheonRL.blockworldgym.simpleblockworld.SimpleBlockEnv.alt_step": [[119, 122], ["simpleblockworld.SimpleBlockEnv.get_obs"], "methods", ["home.repos.pwc.inspect_result.Stanford-ILIAD_PantheonRL.blockworldgym.simpleblockworld.SimpleBlockEnv.get_obs"], ["", "def", "alt_step", "(", "self", ",", "action", ")", ":", "\n", "        ", "self", ".", "constructor_obs", "[", "action", "[", "0", "]", "]", "[", "3", "]", "=", "action", "[", "1", "]", "\n", "return", "self", ".", "get_obs", "(", "True", ")", ",", "[", "0", ",", "0", "]", ",", "False", ",", "{", "}", "\n", "\n"]], "home.repos.pwc.inspect_result.Stanford-ILIAD_PantheonRL.blockworldgym.simpleblockworld.SimpleBlockEnv.get_reward": [[123, 132], ["range"], "methods", ["None"], ["", "def", "get_reward", "(", "self", ")", ":", "\n", "# for simplified version, 100 * num blocks colored correctly / total blocks", "\n", "# (in the actual one, use F1 score)", "\n", "        ", "correct_blocks", "=", "0", "\n", "for", "i", "in", "range", "(", "NUM_BLOCKS", ")", ":", "\n", "            ", "if", "self", ".", "gridworld", "[", "i", "]", "[", "3", "]", "==", "self", ".", "constructor_obs", "[", "i", "]", "[", "3", "]", ":", "\n", "                ", "correct_blocks", "+=", "1", "\n", "", "", "reward", "=", "100", "*", "correct_blocks", "/", "NUM_BLOCKS", "\n", "return", "[", "reward", ",", "reward", "]", "# since they both get the same reward", "\n", "\n"]], "home.repos.pwc.inspect_result.Stanford-ILIAD_PantheonRL.blockworldgym.simpleblockworld.SBWEasyPartner.get_action": [[174, 186], ["None"], "methods", ["None"], ["    ", "def", "get_action", "(", "self", ",", "obs", ",", "recording", "=", "True", ")", ":", "\n", "        ", "token", "=", "obs", "[", "0", "]", "\n", "if", "token", ">", "10", ":", "\n", "            ", "token", "=", "token", "//", "2", "\n", "# tokens 1 - 5 mean color the block at that index red", "\n", "", "if", "1", "<=", "token", "<=", "5", ":", "\n", "            ", "return", "[", "token", "-", "1", ",", "RED", "]", "\n", "# tokens 6 - 10 mean color the block at that index blue", "\n", "", "if", "6", "<=", "token", "<=", "10", ":", "\n", "            ", "return", "[", "token", "-", "8", ",", "BLUE", "]", "\n", "", "else", ":", "\n", "            ", "return", "[", "0", ",", "obs", "[", "4", "]", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.Stanford-ILIAD_PantheonRL.blockworldgym.simpleblockworld.SBWEasyPartner.update": [[187, 189], ["None"], "methods", ["None"], ["", "", "def", "update", "(", "self", ",", "reward", ",", "done", ")", ":", "\n", "        ", "pass", "\n", "\n"]], "home.repos.pwc.inspect_result.Stanford-ILIAD_PantheonRL.blockworldgym.simpleblockworld.SBWDefaultAgent.get_action": [[192, 211], ["numpy.reshape", "simpleblockworld.SBWDefaultAgent.gridfromobs", "simpleblockworld.SBWDefaultAgent.findfirstuncolored", "simpleblockworld.SBWDefaultAgent.findfirstuncolored"], "methods", ["home.repos.pwc.inspect_result.Stanford-ILIAD_PantheonRL.blockworldgym.simpleblockworld.SBWDefaultAgent.gridfromobs", "home.repos.pwc.inspect_result.Stanford-ILIAD_PantheonRL.blockworldgym.simpleblockworld.SBWDefaultAgent.findfirstuncolored", "home.repos.pwc.inspect_result.Stanford-ILIAD_PantheonRL.blockworldgym.simpleblockworld.SBWDefaultAgent.findfirstuncolored"], ["    ", "def", "get_action", "(", "self", ",", "obs", ",", "recording", "=", "True", ")", ":", "\n", "        ", "token", "=", "obs", "[", "0", "]", "\n", "if", "token", "==", "0", ":", "# do nothing", "\n", "            ", "return", "[", "0", ",", "obs", "[", "4", "]", "]", "\n", "", "else", ":", "\n", "            ", "blocks", "=", "np", ".", "reshape", "(", "obs", "[", "1", ":", "]", ",", "(", "NUM_BLOCKS", ",", "4", ")", ")", "\n", "grid", "=", "self", ".", "gridfromobs", "(", "blocks", ")", "\n", "# tokens 1 - 7 mean find the first uncolored one in that row and color it red", "\n", "if", "token", "<=", "7", ":", "\n", "                ", "index", "=", "self", ".", "findfirstuncolored", "(", "grid", ",", "token", "-", "1", ",", "blocks", ")", "\n", "if", "index", "!=", "-", "1", ":", "\n", "                    ", "return", "[", "index", ",", "RED", "]", "\n", "# tokens 8 - 14 mean find the first uncolored one in that row and color it blue", "\n", "", "", "if", "token", "<=", "14", ":", "\n", "                ", "index", "=", "self", ".", "findfirstuncolored", "(", "grid", ",", "token", "-", "8", ",", "blocks", ")", "\n", "if", "index", "!=", "-", "1", ":", "\n", "                    ", "return", "[", "index", ",", "BLUE", "]", "\n", "# otherwise do nothing", "\n", "", "", "return", "[", "0", ",", "obs", "[", "4", "]", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.Stanford-ILIAD_PantheonRL.blockworldgym.simpleblockworld.SBWDefaultAgent.findfirstuncolored": [[212, 218], ["None"], "methods", ["None"], ["", "", "def", "findfirstuncolored", "(", "self", ",", "grid", ",", "row", ",", "blocks", ")", ":", "\n", "        ", "for", "space", "in", "grid", "[", "row", "]", ":", "\n", "            ", "if", "space", "!=", "-", "1", ":", "\n", "                ", "if", "blocks", "[", "space", "]", "[", "3", "]", "==", "0", ":", "\n", "                    ", "return", "space", "\n", "", "", "", "return", "-", "1", "\n", "\n"]], "home.repos.pwc.inspect_result.Stanford-ILIAD_PantheonRL.blockworldgym.simpleblockworld.SBWDefaultAgent.gridfromobs": [[219, 230], ["numpy.full", "range", "len"], "methods", ["None"], ["", "def", "gridfromobs", "(", "self", ",", "blocks", ")", ":", "\n", "        ", "grid", "=", "np", ".", "full", "(", "(", "GRIDLEN", ",", "GRIDLEN", ")", ",", "-", "1", ")", "\n", "for", "i", "in", "range", "(", "len", "(", "blocks", ")", ")", ":", "\n", "            ", "y", "=", "blocks", "[", "i", "]", "[", "1", "]", "\n", "x", "=", "blocks", "[", "i", "]", "[", "2", "]", "\n", "grid", "[", "y", "]", "[", "x", "]", "=", "i", "\n", "if", "blocks", "[", "i", "]", "[", "0", "]", "==", "0", ":", "# horizontal", "\n", "                ", "grid", "[", "y", "]", "[", "x", "+", "1", "]", "=", "i", "\n", "", "else", ":", "\n", "                ", "grid", "[", "y", "+", "1", "]", "[", "x", "]", "=", "i", "\n", "", "", "return", "grid", "\n", "\n"]], "home.repos.pwc.inspect_result.Stanford-ILIAD_PantheonRL.blockworldgym.simpleblockworld.SBWDefaultAgent.update": [[231, 233], ["None"], "methods", ["None"], ["", "def", "update", "(", "self", ",", "reward", ",", "done", ")", ":", "\n", "        ", "pass", "\n", "", "", ""]], "home.repos.pwc.inspect_result.Stanford-ILIAD_PantheonRL.blockworldgym.simpleblockworld.generate_grid_world": [[36, 61], ["numpy.zeros", "simpleblockworld.random_block", "grid_world.append"], "function", ["home.repos.pwc.inspect_result.Stanford-ILIAD_PantheonRL.blockworldgym.simpleblockworld.random_block"], ["def", "generate_grid_world", "(", ")", ":", "\n", "# generates a random GRIDLEN x GRIDLEN world with NUM_BLOCKS blocks", "\n", "# will be replaced in the true version with their generate gridworld function, which has gravity/var blocks/etc", "\n", "    ", "world", "=", "np", ".", "zeros", "(", "(", "GRIDLEN", ",", "GRIDLEN", ")", ")", "\n", "blocks_so_far", "=", "0", "\n", "grid_world", "=", "[", "]", "\n", "while", "blocks_so_far", "<", "NUM_BLOCKS", ":", "\n", "        ", "new_block", "=", "random_block", "(", ")", "\n", "y", "=", "new_block", "[", "1", "]", "\n", "x", "=", "new_block", "[", "2", "]", "\n", "if", "new_block", "[", "0", "]", "==", "0", ":", "\n", "# horizontal", "\n", "            ", "if", "world", "[", "y", "]", "[", "x", "]", "==", "1", "or", "world", "[", "y", "]", "[", "x", "+", "1", "]", "==", "1", ":", "\n", "                ", "continue", "\n", "", "world", "[", "y", "]", "[", "x", "]", "=", "1", "\n", "world", "[", "y", "]", "[", "x", "+", "1", "]", "=", "1", "\n", "", "else", ":", "\n", "# vertical", "\n", "            ", "if", "world", "[", "y", "]", "[", "x", "]", "==", "1", "or", "world", "[", "y", "+", "1", "]", "[", "x", "]", "==", "1", ":", "\n", "                ", "continue", "\n", "", "world", "[", "y", "]", "[", "x", "]", "=", "1", "\n", "world", "[", "y", "+", "1", "]", "[", "x", "]", "=", "1", "\n", "", "grid_world", ".", "append", "(", "new_block", ")", "\n", "blocks_so_far", "+=", "1", "\n", "", "return", "grid_world", "\n", "\n"]], "home.repos.pwc.inspect_result.Stanford-ILIAD_PantheonRL.blockworldgym.simpleblockworld.random_block": [[63, 78], ["block.append", "block.append", "block.append", "numpy.random.randint", "block.append", "numpy.random.randint", "numpy.random.randint", "block.append", "numpy.random.randint", "numpy.random.randint", "numpy.random.randint"], "function", ["None"], ["", "def", "random_block", "(", ")", ":", "\n", "    ", "block", "=", "[", "]", "\n", "if", "np", ".", "random", ".", "randint", "(", "2", ")", "==", "0", ":", "\n", "# horizontal", "\n", "        ", "block", ".", "append", "(", "0", ")", "\n", "x", "=", "np", ".", "random", ".", "randint", "(", "GRIDLEN", "-", "1", ")", "\n", "y", "=", "np", ".", "random", ".", "randint", "(", "GRIDLEN", ")", "\n", "", "else", ":", "\n", "        ", "block", ".", "append", "(", "1", ")", "\n", "x", "=", "np", ".", "random", ".", "randint", "(", "GRIDLEN", ")", "\n", "y", "=", "np", ".", "random", ".", "randint", "(", "GRIDLEN", "-", "1", ")", "\n", "", "block", ".", "append", "(", "y", ")", "\n", "block", ".", "append", "(", "x", ")", "\n", "block", ".", "append", "(", "np", ".", "random", ".", "randint", "(", "NUM_COLORS", ")", "+", "1", ")", "\n", "return", "block", "\n", "\n"]], "home.repos.pwc.inspect_result.Stanford-ILIAD_PantheonRL.common.wrappers.HistoryQueue.__init__": [[45, 51], ["None"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "defaultelem", ":", "np", ".", "ndarray", ",", "size", ":", "int", ")", ":", "\n", "        ", "self", ".", "defaultelem", "=", "defaultelem", "\n", "self", ".", "size", "=", "size", "\n", "self", ".", "pos", "=", "0", "\n", "\n", "self", ".", "history", ":", "List", "[", "np", ".", "ndarray", "]", "=", "[", "defaultelem", "]", "*", "size", "\n", "\n"]], "home.repos.pwc.inspect_result.Stanford-ILIAD_PantheonRL.common.wrappers.HistoryQueue.add": [[52, 65], ["numpy.array", "range"], "methods", ["None"], ["", "def", "add", "(", "self", ",", "toadd", ":", "np", ".", "ndarray", ")", "->", "np", ".", "ndarray", ":", "\n", "        ", "\"\"\"\n        Add the given value to the queue and return the new representation\n\n        :param toadd: The new value to add. This overrides the oldest value\n        :return: The new queue representation, where the first element is the\n            most recently added element and the last element is the oldest\n        \"\"\"", "\n", "self", ".", "history", "[", "self", ".", "pos", "]", "=", "toadd", "\n", "ans", "=", "np", ".", "array", "(", "[", "val", "for", "ind", "in", "range", "(", "self", ".", "size", ")", "\n", "for", "val", "in", "self", ".", "history", "[", "self", ".", "pos", "-", "ind", "]", "]", ")", "\n", "self", ".", "pos", "=", "(", "self", ".", "pos", "+", "1", ")", "%", "self", ".", "size", "\n", "return", "ans", "\n", "\n"]], "home.repos.pwc.inspect_result.Stanford-ILIAD_PantheonRL.common.wrappers.HistoryQueue.reset": [[66, 72], ["None"], "methods", ["None"], ["", "def", "reset", "(", "self", ")", "->", "None", ":", "\n", "        ", "\"\"\"\n        Reset the queue. This fills the buffer with the defaultelement.\n        \"\"\"", "\n", "self", ".", "history", "=", "[", "self", ".", "defaultelem", "]", "*", "self", ".", "size", "\n", "self", ".", "pos", "=", "0", "\n", "\n"]], "home.repos.pwc.inspect_result.Stanford-ILIAD_PantheonRL.common.wrappers.MultiRecorder.get_transitions": [[77, 80], ["None"], "methods", ["None"], ["@", "abstractmethod", "\n", "def", "get_transitions", "(", "self", ")", "->", "MultiTransitions", ":", "\n", "        ", "\"\"\" Get the transitions that have been recorded \"\"\"", "\n", "\n"]], "home.repos.pwc.inspect_result.Stanford-ILIAD_PantheonRL.common.wrappers.TurnBasedRecorder.__init__": [[89, 101], ["multiagentenv.TurnBasedEnv.__init__"], "methods", ["home.repos.pwc.inspect_result.Stanford-ILIAD_PantheonRL.overcookedgym.overcooked.OvercookedMultiEnv.__init__"], ["def", "__init__", "(", "self", ",", "env", ":", "gym", ".", "Env", ")", ":", "\n", "        ", "super", "(", "TurnBasedRecorder", ",", "self", ")", ".", "__init__", "(", "\n", "probegostart", "=", "env", ".", "probegostart", ",", "partners", "=", "env", ".", "partners", "[", "0", "]", ")", "\n", "self", ".", "env", "=", "env", "\n", "\n", "self", ".", "action_space", "=", "env", ".", "action_space", "\n", "self", ".", "observation_space", "=", "env", ".", "observation_space", "\n", "\n", "self", ".", "allobs", ":", "List", "[", "np", ".", "ndarray", "]", "=", "[", "]", "\n", "self", ".", "allacts", ":", "List", "[", "np", ".", "ndarray", "]", "=", "[", "]", "\n", "self", ".", "flags", ":", "List", "[", "int", "]", "=", "[", "]", "\n", "self", ".", "incomplete", "=", "False", "\n", "\n"]], "home.repos.pwc.inspect_result.Stanford-ILIAD_PantheonRL.common.wrappers.TurnBasedRecorder.ego_step": [[102, 119], ["wrappers.TurnBasedRecorder.env.ego_step", "wrappers.TurnBasedRecorder.allacts.append", "wrappers.TurnBasedRecorder.allobs.append", "wrappers.TurnBasedRecorder.flags.append", "wrappers.TurnBasedRecorder.flags.append"], "methods", ["home.repos.pwc.inspect_result.Stanford-ILIAD_PantheonRL.common.multiagentenv.TurnBasedEnv.ego_step"], ["", "def", "ego_step", "(", "\n", "self", ",", "\n", "action", ":", "np", ".", "ndarray", "\n", ")", "->", "Tuple", "[", "Optional", "[", "np", ".", "ndarray", "]", ",", "Tuple", "[", "float", ",", "float", "]", ",", "bool", ",", "Dict", "]", ":", "\n", "        ", "\"\"\"\n        This function calls the embedded environment's ego_step and records the\n        action and new observation.\n        \"\"\"", "\n", "altobs", ",", "rews", ",", "done", ",", "info", "=", "self", ".", "env", ".", "ego_step", "(", "action", ")", "\n", "self", ".", "allacts", ".", "append", "(", "action", ")", "\n", "if", "not", "done", ":", "\n", "            ", "self", ".", "allobs", ".", "append", "(", "altobs", ")", "\n", "self", ".", "flags", ".", "append", "(", "EGO_NOT_DONE", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "flags", ".", "append", "(", "EGO_DONE", ")", "\n", "self", ".", "incomplete", "=", "False", "\n", "", "return", "altobs", ",", "rews", ",", "done", ",", "info", "\n", "\n"]], "home.repos.pwc.inspect_result.Stanford-ILIAD_PantheonRL.common.wrappers.TurnBasedRecorder.alt_step": [[120, 137], ["wrappers.TurnBasedRecorder.env.alt_step", "wrappers.TurnBasedRecorder.allacts.append", "wrappers.TurnBasedRecorder.allobs.append", "wrappers.TurnBasedRecorder.flags.append", "wrappers.TurnBasedRecorder.flags.append"], "methods", ["home.repos.pwc.inspect_result.Stanford-ILIAD_PantheonRL.common.multiagentenv.TurnBasedEnv.alt_step"], ["", "def", "alt_step", "(", "\n", "self", ",", "\n", "action", ":", "np", ".", "ndarray", "\n", ")", "->", "Tuple", "[", "Optional", "[", "np", ".", "ndarray", "]", ",", "Tuple", "[", "float", ",", "float", "]", ",", "bool", ",", "Dict", "]", ":", "\n", "        ", "\"\"\"\n        This function calls the embedded environment's alt_step and records the\n        action and new observation.\n        \"\"\"", "\n", "egoobs", ",", "rews", ",", "done", ",", "info", "=", "self", ".", "env", ".", "alt_step", "(", "action", ")", "\n", "self", ".", "allacts", ".", "append", "(", "action", ")", "\n", "if", "not", "done", ":", "\n", "            ", "self", ".", "allobs", ".", "append", "(", "egoobs", ")", "\n", "self", ".", "flags", ".", "append", "(", "ALT_NOT_DONE", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "flags", ".", "append", "(", "ALT_DONE", ")", "\n", "self", ".", "incomplete", "=", "False", "\n", "", "return", "egoobs", ",", "rews", ",", "done", ",", "info", "\n", "\n"]], "home.repos.pwc.inspect_result.Stanford-ILIAD_PantheonRL.common.wrappers.TurnBasedRecorder.multi_reset": [[138, 150], ["wrappers.TurnBasedRecorder.env.multi_reset", "wrappers.TurnBasedRecorder.allobs.append"], "methods", ["home.repos.pwc.inspect_result.Stanford-ILIAD_PantheonRL.overcookedgym.overcooked.OvercookedMultiEnv.multi_reset"], ["", "def", "multi_reset", "(", "self", ",", "egofirst", ":", "bool", ")", "->", "np", ".", "ndarray", ":", "\n", "        ", "\"\"\"\n        This function calls the embedded environment's multi_reset and records\n        the new observation.\n        \"\"\"", "\n", "newobs", "=", "self", ".", "env", ".", "multi_reset", "(", "egofirst", ")", "\n", "if", "self", ".", "incomplete", ":", "\n", "            ", "self", ".", "allobs", "[", "-", "1", "]", "=", "newobs", "\n", "", "else", ":", "\n", "            ", "self", ".", "allobs", ".", "append", "(", "newobs", ")", "\n", "", "self", ".", "incomplete", "=", "True", "\n", "return", "newobs", "\n", "\n"]], "home.repos.pwc.inspect_result.Stanford-ILIAD_PantheonRL.common.wrappers.TurnBasedRecorder.get_transitions": [[151, 160], ["numpy.array", "trajsaver.TurnBasedTransitions", "numpy.array", "numpy.array"], "methods", ["None"], ["", "def", "get_transitions", "(", "self", ")", "->", "TurnBasedTransitions", ":", "\n", "        ", "\"\"\" Return the recorded transitions \"\"\"", "\n", "obsarray", "=", "np", ".", "array", "(", "self", ".", "allobs", ")", "\n", "if", "self", ".", "incomplete", ":", "\n", "            ", "obsarray", "=", "obsarray", "[", ":", "-", "1", "]", "\n", "", "return", "TurnBasedTransitions", "(", "\n", "obsarray", ",", "\n", "np", ".", "array", "(", "self", ".", "allacts", ")", ",", "\n", "np", ".", "array", "(", "self", ".", "flags", ")", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.Stanford-ILIAD_PantheonRL.common.wrappers.SimultaneousRecorder.__init__": [[170, 183], ["multiagentenv.SimultaneousEnv.__init__"], "methods", ["home.repos.pwc.inspect_result.Stanford-ILIAD_PantheonRL.overcookedgym.overcooked.OvercookedMultiEnv.__init__"], ["def", "__init__", "(", "self", ",", "env", ")", ":", "\n", "        ", "super", "(", "SimultaneousRecorder", ",", "self", ")", ".", "__init__", "(", "partners", "=", "env", ".", "partners", "[", "0", "]", ")", "\n", "self", ".", "env", "=", "env", "\n", "\n", "self", ".", "action_space", "=", "env", ".", "action_space", "\n", "self", ".", "observation_space", "=", "env", ".", "observation_space", "\n", "\n", "self", ".", "allegoobs", "=", "[", "]", "\n", "self", ".", "allegoacts", "=", "[", "]", "\n", "self", ".", "allaltobs", "=", "[", "]", "\n", "self", ".", "allaltacts", "=", "[", "]", "\n", "self", ".", "allflags", "=", "[", "]", "\n", "self", ".", "incomplete", "=", "False", "\n", "\n"]], "home.repos.pwc.inspect_result.Stanford-ILIAD_PantheonRL.common.wrappers.SimultaneousRecorder.multi_step": [[184, 205], ["wrappers.SimultaneousRecorder.env.multi_step", "wrappers.SimultaneousRecorder.allegoacts.append", "wrappers.SimultaneousRecorder.allaltacts.append", "wrappers.SimultaneousRecorder.allegoobs.append", "wrappers.SimultaneousRecorder.allaltobs.append", "wrappers.SimultaneousRecorder.allflags.append", "wrappers.SimultaneousRecorder.allflags.append"], "methods", ["home.repos.pwc.inspect_result.Stanford-ILIAD_PantheonRL.overcookedgym.overcooked.OvercookedMultiEnv.multi_step"], ["", "def", "multi_step", "(", "\n", "self", ",", "\n", "ego_action", ":", "np", ".", "ndarray", ",", "\n", "alt_action", ":", "np", ".", "ndarray", "\n", ")", "->", "Tuple", "[", "Tuple", "[", "Optional", "[", "np", ".", "ndarray", "]", ",", "Optional", "[", "np", ".", "ndarray", "]", "]", ",", "\n", "Tuple", "[", "float", ",", "float", "]", ",", "bool", ",", "Dict", "]", ":", "\n", "        ", "\"\"\"\n        This function calls the embedded environment's multi_step and records\n        the new actions and observations.\n        \"\"\"", "\n", "obs", ",", "rews", ",", "done", ",", "info", "=", "self", ".", "env", ".", "multi_step", "(", "ego_action", ",", "alt_action", ")", "\n", "self", ".", "allegoacts", ".", "append", "(", "ego_action", ")", "\n", "self", ".", "allaltacts", ".", "append", "(", "alt_action", ")", "\n", "if", "not", "done", ":", "\n", "            ", "self", ".", "allegoobs", ".", "append", "(", "obs", "[", "0", "]", ")", "\n", "self", ".", "allaltobs", ".", "append", "(", "obs", "[", "1", "]", ")", "\n", "self", ".", "allflags", ".", "append", "(", "NOT_DONE", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "allflags", ".", "append", "(", "DONE", ")", "\n", "self", ".", "incomplete", "=", "False", "\n", "", "return", "obs", ",", "rews", ",", "done", ",", "info", "\n", "\n"]], "home.repos.pwc.inspect_result.Stanford-ILIAD_PantheonRL.common.wrappers.SimultaneousRecorder.multi_reset": [[206, 216], ["wrappers.SimultaneousRecorder.env.multi_reset", "wrappers.SimultaneousRecorder.allegoobs.append", "wrappers.SimultaneousRecorder.allaltobs.append"], "methods", ["home.repos.pwc.inspect_result.Stanford-ILIAD_PantheonRL.overcookedgym.overcooked.OvercookedMultiEnv.multi_reset"], ["", "def", "multi_reset", "(", "self", ")", "->", "Tuple", "[", "np", ".", "ndarray", ",", "np", ".", "ndarray", "]", ":", "\n", "        ", "\"\"\"\n        This function calls the embedded environment's multi_reset and records\n        the new observations.\n        \"\"\"", "\n", "obs", "=", "self", ".", "env", ".", "multi_reset", "(", ")", "\n", "self", ".", "allegoobs", ".", "append", "(", "obs", "[", "0", "]", ")", "\n", "self", ".", "allaltobs", ".", "append", "(", "obs", "[", "1", "]", ")", "\n", "self", ".", "incomplete", "=", "True", "\n", "return", "obs", "\n", "\n"]], "home.repos.pwc.inspect_result.Stanford-ILIAD_PantheonRL.common.wrappers.SimultaneousRecorder.get_transitions": [[217, 230], ["numpy.array", "numpy.array", "trajsaver.SimultaneousTransitions", "numpy.array", "numpy.array", "numpy.array"], "methods", ["None"], ["", "def", "get_transitions", "(", "self", ")", "->", "SimultaneousTransitions", ":", "\n", "        ", "\"\"\" Return the recorded transitions \"\"\"", "\n", "egoobsarr", "=", "np", ".", "array", "(", "self", ".", "allegoobs", ")", "\n", "altobsarr", "=", "np", ".", "array", "(", "self", ".", "allaltobs", ")", "\n", "if", "self", ".", "incomplete", ":", "\n", "            ", "egoobsarr", "=", "egoobsarr", "[", ":", "-", "1", "]", "\n", "altobsarr", "=", "altobsarr", "[", ":", "-", "1", "]", "\n", "", "return", "SimultaneousTransitions", "(", "\n", "egoobsarr", ",", "\n", "np", ".", "array", "(", "self", ".", "allegoacts", ")", ",", "\n", "altobsarr", ",", "\n", "np", ".", "array", "(", "self", ".", "allaltacts", ")", ",", "\n", "np", ".", "array", "(", "self", ".", "allflags", ")", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.Stanford-ILIAD_PantheonRL.common.wrappers.TurnBasedFrameStack.__init__": [[247, 279], ["multiagentenv.TurnBasedEnv.__init__", "util.calculate_space", "wrappers.HistoryQueue", "wrappers.HistoryQueue", "util.get_default_obs", "util.get_default_obs"], "methods", ["home.repos.pwc.inspect_result.Stanford-ILIAD_PantheonRL.overcookedgym.overcooked.OvercookedMultiEnv.__init__", "home.repos.pwc.inspect_result.Stanford-ILIAD_PantheonRL.common.util.calculate_space", "home.repos.pwc.inspect_result.Stanford-ILIAD_PantheonRL.common.util.get_default_obs", "home.repos.pwc.inspect_result.Stanford-ILIAD_PantheonRL.common.util.get_default_obs"], ["def", "__init__", "(", "\n", "self", ",", "\n", "env", ":", "gym", ".", "Env", ",", "\n", "numframes", ":", "int", ",", "\n", "defaultobs", ":", "Optional", "[", "np", ".", "ndarray", "]", "=", "None", ",", "\n", "altenv", ":", "Optional", "[", "gym", ".", "Env", "]", "=", "None", ",", "\n", "defaultaltobs", ":", "Optional", "[", "np", ".", "ndarray", "]", "=", "None", "\n", ")", ":", "\n", "        ", "super", "(", "TurnBasedFrameStack", ",", "self", ")", ".", "__init__", "(", "\n", "probegostart", "=", "env", ".", "probegostart", ",", "partners", "=", "env", ".", "partners", "[", "0", "]", ")", "\n", "self", ".", "env", "=", "env", "\n", "self", ".", "numframes", "=", "numframes", "\n", "\n", "self", ".", "action_space", "=", "env", ".", "action_space", "\n", "self", ".", "observation_space", "=", "calculate_space", "(", "\n", "env", ".", "observation_space", ",", "numframes", ")", "\n", "\n", "if", "defaultobs", "is", "not", "None", ":", "\n", "            ", "defobs", "=", "defaultobs", "\n", "", "else", ":", "\n", "            ", "defobs", "=", "get_default_obs", "(", "env", ")", "\n", "\n", "", "if", "altenv", "is", "None", ":", "\n", "            ", "altenv", "=", "env", "\n", "\n", "", "if", "defaultaltobs", "is", "not", "None", ":", "\n", "            ", "defaltobs", "=", "defaultaltobs", "\n", "", "else", ":", "\n", "            ", "defaltobs", "=", "get_default_obs", "(", "altenv", ")", "\n", "\n", "", "self", ".", "egohistory", "=", "HistoryQueue", "(", "defobs", ",", "numframes", ")", "\n", "self", ".", "althistory", "=", "HistoryQueue", "(", "defaltobs", ",", "numframes", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Stanford-ILIAD_PantheonRL.common.wrappers.TurnBasedFrameStack.ego_step": [[280, 286], ["wrappers.TurnBasedFrameStack.env.ego_step", "wrappers.TurnBasedFrameStack.althistory.add"], "methods", ["home.repos.pwc.inspect_result.Stanford-ILIAD_PantheonRL.common.multiagentenv.TurnBasedEnv.ego_step", "home.repos.pwc.inspect_result.Stanford-ILIAD_PantheonRL.common.wrappers.HistoryQueue.add"], ["", "def", "ego_step", "(", "\n", "self", ",", "\n", "action", ":", "np", ".", "ndarray", "\n", ")", "->", "Tuple", "[", "Optional", "[", "np", ".", "ndarray", "]", ",", "Tuple", "[", "float", ",", "float", "]", ",", "bool", ",", "Dict", "]", ":", "\n", "        ", "altobs", ",", "rews", ",", "done", ",", "info", "=", "self", ".", "env", ".", "ego_step", "(", "action", ")", "\n", "return", "self", ".", "althistory", ".", "add", "(", "altobs", ")", ",", "rews", ",", "done", ",", "info", "\n", "\n"]], "home.repos.pwc.inspect_result.Stanford-ILIAD_PantheonRL.common.wrappers.TurnBasedFrameStack.alt_step": [[287, 293], ["wrappers.TurnBasedFrameStack.env.alt_step", "wrappers.TurnBasedFrameStack.egohistory.add"], "methods", ["home.repos.pwc.inspect_result.Stanford-ILIAD_PantheonRL.common.multiagentenv.TurnBasedEnv.alt_step", "home.repos.pwc.inspect_result.Stanford-ILIAD_PantheonRL.common.wrappers.HistoryQueue.add"], ["", "def", "alt_step", "(", "\n", "self", ",", "\n", "action", ":", "np", ".", "ndarray", "\n", ")", "->", "Tuple", "[", "Optional", "[", "np", ".", "ndarray", "]", ",", "Tuple", "[", "float", ",", "float", "]", ",", "bool", ",", "Dict", "]", ":", "\n", "        ", "egoobs", ",", "rews", ",", "done", ",", "info", "=", "self", ".", "env", ".", "alt_step", "(", "action", ")", "\n", "return", "self", ".", "egohistory", ".", "add", "(", "egoobs", ")", ",", "rews", ",", "done", ",", "info", "\n", "\n"]], "home.repos.pwc.inspect_result.Stanford-ILIAD_PantheonRL.common.wrappers.TurnBasedFrameStack.multi_reset": [[294, 303], ["wrappers.TurnBasedFrameStack.env.multi_reset", "wrappers.TurnBasedFrameStack.egohistory.reset", "wrappers.TurnBasedFrameStack.althistory.reset", "wrappers.TurnBasedFrameStack.egohistory.add", "wrappers.TurnBasedFrameStack.althistory.add"], "methods", ["home.repos.pwc.inspect_result.Stanford-ILIAD_PantheonRL.overcookedgym.overcooked.OvercookedMultiEnv.multi_reset", "home.repos.pwc.inspect_result.Stanford-ILIAD_PantheonRL.common.multiagentenv.MultiAgentEnv.reset", "home.repos.pwc.inspect_result.Stanford-ILIAD_PantheonRL.common.multiagentenv.MultiAgentEnv.reset", "home.repos.pwc.inspect_result.Stanford-ILIAD_PantheonRL.common.wrappers.HistoryQueue.add", "home.repos.pwc.inspect_result.Stanford-ILIAD_PantheonRL.common.wrappers.HistoryQueue.add"], ["", "def", "multi_reset", "(", "self", ",", "egofirst", ":", "bool", ")", "->", "np", ".", "ndarray", ":", "\n", "        ", "newobs", "=", "self", ".", "env", ".", "multi_reset", "(", "egofirst", ")", "\n", "self", ".", "egohistory", ".", "reset", "(", ")", "\n", "self", ".", "althistory", ".", "reset", "(", ")", "\n", "\n", "if", "egofirst", ":", "\n", "            ", "return", "self", ".", "egohistory", ".", "add", "(", "newobs", ")", "\n", "", "else", ":", "\n", "            ", "return", "self", ".", "althistory", ".", "add", "(", "newobs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Stanford-ILIAD_PantheonRL.common.wrappers.SimultaneousFrameStack.__init__": [[315, 334], ["multiagentenv.SimultaneousEnv.__init__", "util.calculate_space", "wrappers.HistoryQueue", "wrappers.HistoryQueue", "util.get_default_obs", "list"], "methods", ["home.repos.pwc.inspect_result.Stanford-ILIAD_PantheonRL.overcookedgym.overcooked.OvercookedMultiEnv.__init__", "home.repos.pwc.inspect_result.Stanford-ILIAD_PantheonRL.common.util.calculate_space", "home.repos.pwc.inspect_result.Stanford-ILIAD_PantheonRL.common.util.get_default_obs"], ["def", "__init__", "(", "\n", "self", ",", "\n", "env", ":", "gym", ".", "Env", ",", "\n", "numframes", ":", "int", ",", "\n", "defaultobs", ":", "Optional", "[", "np", ".", "ndarray", "]", "=", "None", "\n", ")", ":", "\n", "        ", "super", "(", "SimultaneousFrameStack", ",", "self", ")", ".", "__init__", "(", "partners", "=", "env", ".", "partners", "[", "0", "]", ")", "\n", "self", ".", "env", "=", "env", "\n", "self", ".", "numframes", "=", "numframes", "\n", "\n", "self", ".", "action_space", "=", "env", ".", "action_space", "\n", "self", ".", "observation_space", "=", "calculate_space", "(", "\n", "env", ".", "observation_space", ",", "numframes", ")", "\n", "\n", "self", ".", "defaultobs", "=", "get_default_obs", "(", "\n", "env", ")", "if", "defaultobs", "is", "None", "else", "list", "(", "defaultobs", ")", "\n", "\n", "self", ".", "egohistory", "=", "HistoryQueue", "(", "self", ".", "defaultobs", ",", "self", ".", "numframes", ")", "\n", "self", ".", "althistory", "=", "HistoryQueue", "(", "self", ".", "defaultobs", ",", "self", ".", "numframes", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Stanford-ILIAD_PantheonRL.common.wrappers.SimultaneousFrameStack.multi_step": [[335, 344], ["wrappers.SimultaneousFrameStack.env.multi_step", "wrappers.SimultaneousFrameStack.egohistory.add", "wrappers.SimultaneousFrameStack.althistory.add"], "methods", ["home.repos.pwc.inspect_result.Stanford-ILIAD_PantheonRL.overcookedgym.overcooked.OvercookedMultiEnv.multi_step", "home.repos.pwc.inspect_result.Stanford-ILIAD_PantheonRL.common.wrappers.HistoryQueue.add", "home.repos.pwc.inspect_result.Stanford-ILIAD_PantheonRL.common.wrappers.HistoryQueue.add"], ["", "def", "multi_step", "(", "\n", "self", ",", "\n", "ego_action", ":", "np", ".", "ndarray", ",", "\n", "alt_action", ":", "np", ".", "ndarray", "\n", ")", "->", "Tuple", "[", "Tuple", "[", "Optional", "[", "np", ".", "ndarray", "]", ",", "Optional", "[", "np", ".", "ndarray", "]", "]", ",", "\n", "Tuple", "[", "float", ",", "float", "]", ",", "bool", ",", "Dict", "]", ":", "\n", "        ", "obs", ",", "rews", ",", "done", ",", "info", "=", "self", ".", "env", ".", "multi_step", "(", "ego_action", ",", "alt_action", ")", "\n", "return", "(", "self", ".", "egohistory", ".", "add", "(", "obs", "[", "0", "]", ")", ",", "\n", "self", ".", "althistory", ".", "add", "(", "obs", "[", "1", "]", ")", ")", ",", "rews", ",", "done", ",", "info", "\n", "\n"]], "home.repos.pwc.inspect_result.Stanford-ILIAD_PantheonRL.common.wrappers.SimultaneousFrameStack.multi_reset": [[345, 350], ["wrappers.SimultaneousFrameStack.env.multi_reset", "wrappers.SimultaneousFrameStack.egohistory.reset", "wrappers.SimultaneousFrameStack.althistory.reset", "wrappers.SimultaneousFrameStack.egohistory.add", "wrappers.SimultaneousFrameStack.althistory.add"], "methods", ["home.repos.pwc.inspect_result.Stanford-ILIAD_PantheonRL.overcookedgym.overcooked.OvercookedMultiEnv.multi_reset", "home.repos.pwc.inspect_result.Stanford-ILIAD_PantheonRL.common.multiagentenv.MultiAgentEnv.reset", "home.repos.pwc.inspect_result.Stanford-ILIAD_PantheonRL.common.multiagentenv.MultiAgentEnv.reset", "home.repos.pwc.inspect_result.Stanford-ILIAD_PantheonRL.common.wrappers.HistoryQueue.add", "home.repos.pwc.inspect_result.Stanford-ILIAD_PantheonRL.common.wrappers.HistoryQueue.add"], ["", "def", "multi_reset", "(", "self", ")", "->", "Tuple", "[", "np", ".", "ndarray", ",", "np", ".", "ndarray", "]", ":", "\n", "        ", "obs", "=", "self", ".", "env", ".", "multi_reset", "(", ")", "\n", "self", ".", "egohistory", ".", "reset", "(", ")", "\n", "self", ".", "althistory", ".", "reset", "(", ")", "\n", "return", "(", "self", ".", "egohistory", ".", "add", "(", "obs", "[", "0", "]", ")", ",", "self", ".", "althistory", ".", "add", "(", "obs", "[", "1", "]", ")", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.Stanford-ILIAD_PantheonRL.common.wrappers.frame_wrap": [[23, 28], ["isinstance", "wrappers.TurnBasedFrameStack", "wrappers.SimultaneousFrameStack"], "function", ["None"], ["def", "frame_wrap", "(", "env", ":", "MultiAgentEnv", ",", "numframes", ":", "int", ")", ":", "\n", "    ", "if", "isinstance", "(", "env", ",", "TurnBasedEnv", ")", ":", "\n", "        ", "return", "TurnBasedFrameStack", "(", "env", ",", "numframes", ")", "\n", "", "else", ":", "\n", "        ", "return", "SimultaneousFrameStack", "(", "env", ",", "numframes", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Stanford-ILIAD_PantheonRL.common.wrappers.recorder_wrap": [[30, 35], ["isinstance", "wrappers.TurnBasedRecorder", "wrappers.SimultaneousRecorder"], "function", ["None"], ["", "", "def", "recorder_wrap", "(", "env", ":", "MultiAgentEnv", ")", ":", "\n", "    ", "if", "isinstance", "(", "env", ",", "TurnBasedEnv", ")", ":", "\n", "        ", "return", "TurnBasedRecorder", "(", "env", ")", "\n", "", "else", ":", "\n", "        ", "return", "SimultaneousRecorder", "(", "env", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Stanford-ILIAD_PantheonRL.common.trajsaver.TransitionsMinimal.__len__": [[82, 85], ["len"], "methods", ["None"], ["def", "__len__", "(", "self", ")", ":", "\n", "        ", "\"\"\"Returns number of transitions. Always positive.\"\"\"", "\n", "return", "len", "(", "self", ".", "obs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Stanford-ILIAD_PantheonRL.common.trajsaver.TransitionsMinimal.__post_init__": [[86, 97], ["vars().values", "isinstance", "len", "len", "ValueError", "vars", "val.setflags", "len", "len"], "methods", ["home.repos.pwc.inspect_result.Stanford-ILIAD_PantheonRL.adap.policies.MultModel.values"], ["", "def", "__post_init__", "(", "self", ")", ":", "\n", "        ", "\"\"\"Performs input validation: check shapes & dtypes match docstring.\n        Also make array values read-only.\n        \"\"\"", "\n", "for", "val", "in", "vars", "(", "self", ")", ".", "values", "(", ")", ":", "\n", "            ", "if", "isinstance", "(", "val", ",", "np", ".", "ndarray", ")", ":", "\n", "                ", "val", ".", "setflags", "(", "write", "=", "False", ")", "\n", "\n", "", "", "if", "len", "(", "self", ".", "obs", ")", "!=", "len", "(", "self", ".", "acts", ")", ":", "\n", "            ", "raise", "ValueError", "(", "\n", "\"obs and acts must have same number of timesteps: \"", "\n", "f\"{len(self.obs)} != {len(self.acts)}\"", "\n"]], "home.repos.pwc.inspect_result.Stanford-ILIAD_PantheonRL.common.trajsaver.TransitionsMinimal.__getitem__": [[108, 129], ["trajsaver.dataclass_quick_asdict", "isinstance", "dataclasses.replace", "isinstance", "dataclass_quick_asdict.items"], "methods", ["home.repos.pwc.inspect_result.Stanford-ILIAD_PantheonRL.common.trajsaver.dataclass_quick_asdict"], ["", "def", "__getitem__", "(", "self", ",", "key", ")", ":", "\n", "        ", "\"\"\"See TransitionsMinimal docstring for indexing and slicing semantics.\n        \"\"\"", "\n", "d", "=", "dataclass_quick_asdict", "(", "self", ")", "\n", "d_item", "=", "{", "k", ":", "v", "[", "key", "]", "for", "k", ",", "v", "in", "d", ".", "items", "(", ")", "}", "\n", "\n", "if", "isinstance", "(", "key", ",", "slice", ")", ":", "\n", "# Return type is the same as this dataclass. Replace field value", "\n", "# with slices.", "\n", "            ", "return", "dataclasses", ".", "replace", "(", "self", ",", "**", "d_item", ")", "\n", "", "else", ":", "\n", "            ", "assert", "isinstance", "(", "key", ",", "int", ")", "\n", "# Return type is a dictionary. Array values have no batch dimension", "\n", "#", "\n", "# Dictionary of np.ndarray values is a convenient", "\n", "# torch.util.data.Dataset return type, as a", "\n", "# torch.util.data.DataLoader taking in this `Dataset` as its first", "\n", "# argument knows how to automatically concatenate several", "\n", "# dictionaries together to make a single dictionary batch with", "\n", "# `torch.Tensor` values.", "\n", "return", "d_item", "\n", "\n"]], "home.repos.pwc.inspect_result.Stanford-ILIAD_PantheonRL.common.trajsaver.TransitionsMinimal.write_transition": [[130, 133], ["numpy.concatenate", "numpy.save"], "methods", ["None"], ["", "", "def", "write_transition", "(", "self", ",", "file", ")", ":", "\n", "        ", "full_list", "=", "np", ".", "concatenate", "(", "(", "self", ".", "obs", ",", "self", ".", "acts", ")", ",", "axis", "=", "1", ")", "\n", "np", ".", "save", "(", "file", ",", "full_list", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Stanford-ILIAD_PantheonRL.common.trajsaver.TransitionsMinimal.read_transition": [[134, 141], ["numpy.load", "util.get_space_size", "trajsaver.TransitionsMinimal"], "methods", ["home.repos.pwc.inspect_result.Stanford-ILIAD_PantheonRL.common.util.get_space_size"], ["", "@", "classmethod", "\n", "def", "read_transition", "(", "cls", ",", "file", ",", "obs_space", ",", "act_space", ")", ":", "\n", "        ", "full_list", "=", "np", ".", "load", "(", "file", ")", "\n", "obs_size", "=", "get_space_size", "(", "obs_space", ")", "\n", "obs", "=", "full_list", "[", ":", ",", ":", "obs_size", "]", "\n", "acts", "=", "full_list", "[", ":", ",", "obs_size", ":", "]", "\n", "return", "TransitionsMinimal", "(", "obs", ",", "acts", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Stanford-ILIAD_PantheonRL.common.trajsaver.MultiTransitions.get_ego_transitions": [[146, 149], ["None"], "methods", ["None"], ["@", "abstractmethod", "\n", "def", "get_ego_transitions", "(", "self", ")", "->", "TransitionsMinimal", ":", "\n", "        ", "\"\"\" Returns the ego's transitions \"\"\"", "\n", "\n"]], "home.repos.pwc.inspect_result.Stanford-ILIAD_PantheonRL.common.trajsaver.MultiTransitions.get_alt_transitions": [[150, 153], ["None"], "methods", ["None"], ["", "@", "abstractmethod", "\n", "def", "get_alt_transitions", "(", "self", ")", "->", "TransitionsMinimal", ":", "\n", "        ", "\"\"\" Returns the partner's transitions \"\"\"", "\n", "\n"]], "home.repos.pwc.inspect_result.Stanford-ILIAD_PantheonRL.common.trajsaver.TurnBasedTransitions.get_ego_transitions": [[161, 165], ["trajsaver.TransitionsMinimal"], "methods", ["None"], ["def", "get_ego_transitions", "(", "self", ")", "->", "TransitionsMinimal", ":", "\n", "        ", "\"\"\" Returns the ego's transitions \"\"\"", "\n", "mask", "=", "(", "self", ".", "flags", "%", "2", "==", "0", ")", "\n", "return", "TransitionsMinimal", "(", "self", ".", "obs", "[", "mask", "]", ",", "self", ".", "acts", "[", "mask", "]", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Stanford-ILIAD_PantheonRL.common.trajsaver.TurnBasedTransitions.get_alt_transitions": [[166, 170], ["trajsaver.TransitionsMinimal"], "methods", ["None"], ["", "def", "get_alt_transitions", "(", "self", ")", "->", "TransitionsMinimal", ":", "\n", "        ", "\"\"\" Returns the partner's transitions \"\"\"", "\n", "mask", "=", "(", "self", ".", "flags", "%", "2", "==", "1", ")", "\n", "return", "TransitionsMinimal", "(", "self", ".", "obs", "[", "mask", "]", ",", "self", ".", "acts", "[", "mask", "]", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Stanford-ILIAD_PantheonRL.common.trajsaver.TurnBasedTransitions.write_transition": [[171, 180], ["numpy.reshape", "numpy.reshape", "numpy.reshape", "numpy.concatenate", "numpy.save"], "methods", ["None"], ["", "def", "write_transition", "(", "self", ",", "file", ")", ":", "\n", "        ", "flags", "=", "np", ".", "reshape", "(", "self", ".", "flags", ",", "(", "-", "1", ",", "1", ")", ")", "\n", "len", "=", "flags", ".", "shape", "[", "0", "]", "\n", "\n", "obs", "=", "np", ".", "reshape", "(", "self", ".", "obs", ",", "(", "len", ",", "-", "1", ")", ")", "\n", "acts", "=", "np", ".", "reshape", "(", "self", ".", "acts", ",", "(", "len", ",", "-", "1", ")", ")", "\n", "\n", "full_list", "=", "np", ".", "concatenate", "(", "(", "obs", ",", "acts", ",", "flags", ")", ",", "axis", "=", "1", ")", "\n", "np", ".", "save", "(", "file", ",", "full_list", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Stanford-ILIAD_PantheonRL.common.trajsaver.TurnBasedTransitions.read_transition": [[181, 190], ["numpy.load", "util.get_space_size", "trajsaver.TurnBasedTransitions"], "methods", ["home.repos.pwc.inspect_result.Stanford-ILIAD_PantheonRL.common.util.get_space_size"], ["", "@", "classmethod", "\n", "def", "read_transition", "(", "cls", ",", "file", ",", "obs_space", ",", "act_space", ")", ":", "\n", "        ", "full_list", "=", "np", ".", "load", "(", "file", ")", "\n", "obs_size", "=", "get_space_size", "(", "obs_space", ")", "\n", "obs", "=", "full_list", "[", ":", ",", ":", "obs_size", "]", "\n", "acts", "=", "full_list", "[", ":", ",", "obs_size", ":", "-", "1", "]", "\n", "flags", "=", "full_list", "[", ":", ",", "-", "1", "]", "\n", "\n", "return", "TurnBasedTransitions", "(", "obs", ",", "acts", ",", "flags", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Stanford-ILIAD_PantheonRL.common.trajsaver.SimultaneousTransitions.get_ego_transitions": [[200, 203], ["trajsaver.TransitionsMinimal"], "methods", ["None"], ["def", "get_ego_transitions", "(", "self", ")", "->", "TransitionsMinimal", ":", "\n", "        ", "\"\"\" Returns the ego's transitions \"\"\"", "\n", "return", "TransitionsMinimal", "(", "self", ".", "egoobs", ",", "self", ".", "egoacts", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Stanford-ILIAD_PantheonRL.common.trajsaver.SimultaneousTransitions.get_alt_transitions": [[204, 207], ["trajsaver.TransitionsMinimal"], "methods", ["None"], ["", "def", "get_alt_transitions", "(", "self", ")", "->", "TransitionsMinimal", ":", "\n", "        ", "\"\"\" Returns the partner's transitions \"\"\"", "\n", "return", "TransitionsMinimal", "(", "self", ".", "altobs", ",", "self", ".", "altacts", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Stanford-ILIAD_PantheonRL.common.trajsaver.SimultaneousTransitions.write_transition": [[208, 220], ["numpy.reshape", "numpy.reshape", "numpy.reshape", "numpy.reshape", "numpy.reshape", "numpy.concatenate", "numpy.save"], "methods", ["None"], ["", "def", "write_transition", "(", "self", ",", "file", ")", ":", "\n", "        ", "flags", "=", "np", ".", "reshape", "(", "self", ".", "flags", ",", "(", "-", "1", ",", "1", ")", ")", "\n", "len", "=", "flags", ".", "shape", "[", "0", "]", "\n", "egoobs", "=", "np", ".", "reshape", "(", "self", ".", "egoobs", ",", "(", "len", ",", "-", "1", ")", ")", "\n", "egoacts", "=", "np", ".", "reshape", "(", "self", ".", "egoacts", ",", "(", "len", ",", "-", "1", ")", ")", "\n", "altobs", "=", "np", ".", "reshape", "(", "self", ".", "altobs", ",", "(", "len", ",", "-", "1", ")", ")", "\n", "altacts", "=", "np", ".", "reshape", "(", "self", ".", "altacts", ",", "(", "len", ",", "-", "1", ")", ")", "\n", "full_list", "=", "np", ".", "concatenate", "(", "\n", "(", "egoobs", ",", "egoacts", ",", "altobs", ",", "altacts", ",", "flags", ")", ",", "\n", "axis", "=", "1", "\n", ")", "\n", "np", ".", "save", "(", "file", ",", "full_list", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Stanford-ILIAD_PantheonRL.common.trajsaver.SimultaneousTransitions.read_transition": [[221, 233], ["numpy.load", "util.get_space_size", "util.get_space_size", "trajsaver.SimultaneousTransitions"], "methods", ["home.repos.pwc.inspect_result.Stanford-ILIAD_PantheonRL.common.util.get_space_size", "home.repos.pwc.inspect_result.Stanford-ILIAD_PantheonRL.common.util.get_space_size"], ["", "@", "classmethod", "\n", "def", "read_transition", "(", "cls", ",", "file", ",", "obs_space", ",", "act_space", ")", ":", "\n", "        ", "full_list", "=", "np", ".", "load", "(", "file", ")", "\n", "obs_size", "=", "get_space_size", "(", "obs_space", ")", "\n", "act_size", "=", "get_space_size", "(", "act_space", ")", "\n", "egoobs", "=", "full_list", "[", ":", ",", ":", "obs_size", "]", "\n", "egoacts", "=", "full_list", "[", ":", ",", "obs_size", ":", "(", "obs_size", "+", "act_size", ")", "]", "\n", "altobs", "=", "full_list", "[", ":", ",", "(", "obs_size", "+", "act_size", ")", ":", "(", "2", "*", "obs_size", "+", "act_size", ")", "]", "\n", "altacts", "=", "full_list", "[", ":", ",", "(", "2", "*", "obs_size", "+", "act_size", ")", ":", "-", "1", "]", "\n", "flags", "=", "full_list", "[", ":", ",", "-", "1", "]", "\n", "\n", "return", "SimultaneousTransitions", "(", "egoobs", ",", "egoacts", ",", "altobs", ",", "altacts", ",", "flags", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.Stanford-ILIAD_PantheonRL.common.trajsaver.transitions_collate_fn": [[16, 35], ["torch.utils.data._utils.collate.default_collate", "isinstance", "sample.items"], "function", ["None"], ["def", "transitions_collate_fn", "(", "\n", "batch", ":", "Sequence", "[", "Mapping", "[", "str", ",", "np", ".", "ndarray", "]", "]", ",", "\n", ")", "->", "Dict", "[", "str", ",", "Union", "[", "np", ".", "ndarray", ",", "th", ".", "Tensor", "]", "]", ":", "\n", "    ", "\"\"\"\n    This function is from HumanCompatibleAI's imitation repo:\n    https://github.com/HumanCompatibleAI/imitation/blob/master/src/imitation/\n    data/types.py\n\n    Custom `torch.utils.data.DataLoader` collate_fn for `TransitionsMinimal`.\n    Use this as the `collate_fn` argument to `DataLoader` if using an instance\n    of `TransitionsMinimal` as the `dataset` argument.\n    \"\"\"", "\n", "batch_no_infos", "=", "[", "\n", "{", "k", ":", "v", "for", "k", ",", "v", "in", "sample", ".", "items", "(", ")", "}", "for", "sample", "in", "batch", "\n", "]", "\n", "\n", "result", "=", "default_collate", "(", "batch_no_infos", ")", "\n", "assert", "isinstance", "(", "result", ",", "dict", ")", "\n", "return", "result", "\n", "\n"]], "home.repos.pwc.inspect_result.Stanford-ILIAD_PantheonRL.common.trajsaver.dataclass_quick_asdict": [[37, 51], ["getattr", "dataclasses.fields"], "function", ["None"], ["", "def", "dataclass_quick_asdict", "(", "dataclass_instance", ")", "->", "dict", ":", "\n", "    ", "\"\"\"\n    This function is from HumanCompatibleAI's imitation repo:\n    https://github.com/HumanCompatibleAI/imitation/blob/master/src/imitation/\n    data/types.py\n\n    Extract dataclass to items using `dataclasses.fields` + dict comprehension.\n    This is a quick alternative to `dataclasses.asdict`, which expensively and\n    undocumentedly deep-copies every numpy array value.\n    See https://stackoverflow.com/a/52229565/1091722.\n    \"\"\"", "\n", "obj", "=", "dataclass_instance", "\n", "d", "=", "{", "f", ".", "name", ":", "getattr", "(", "obj", ",", "f", ".", "name", ")", "for", "f", "in", "dataclasses", ".", "fields", "(", "obj", ")", "}", "\n", "return", "d", "\n", "\n"]], "home.repos.pwc.inspect_result.Stanford-ILIAD_PantheonRL.common.multiagentenv.MultiAgentEnv.__init__": [[35, 65], ["tuple", "tuple", "multiagentenv.MultiAgentEnv.set_resample_policy", "len", "multiagentenv.PlayerException", "multiagentenv.PlayerException", "isinstance"], "methods", ["home.repos.pwc.inspect_result.Stanford-ILIAD_PantheonRL.common.multiagentenv.MultiAgentEnv.set_resample_policy"], ["def", "__init__", "(", "self", ",", "\n", "ego_ind", ":", "int", "=", "0", ",", "\n", "n_players", ":", "int", "=", "2", ",", "\n", "resample_policy", ":", "str", "=", "\"default\"", ",", "\n", "partners", ":", "Optional", "[", "List", "[", "List", "[", "Agent", "]", "]", "]", "=", "None", ")", ":", "\n", "        ", "self", ".", "ego_ind", "=", "ego_ind", "\n", "self", ".", "n_players", "=", "n_players", "\n", "if", "partners", "is", "not", "None", ":", "\n", "            ", "if", "len", "(", "partners", ")", "!=", "n_players", "-", "1", ":", "\n", "                ", "raise", "PlayerException", "(", "\n", "\"The number of partners needs to equal the number \\\n                    of non-ego players\"", ")", "\n", "\n", "", "for", "plist", "in", "partners", ":", "\n", "                ", "if", "not", "isinstance", "(", "plist", ",", "list", ")", "or", "not", "plist", ":", "\n", "                    ", "raise", "PlayerException", "(", "\n", "\"Sublist for each partner must be nonempty list\"", ")", "\n", "\n", "", "", "", "self", ".", "partners", "=", "partners", "or", "[", "[", "]", "]", "*", "(", "n_players", "-", "1", ")", "\n", "self", ".", "partnerids", "=", "[", "0", "]", "*", "(", "n_players", "-", "1", ")", "\n", "\n", "self", ".", "_players", ":", "Tuple", "[", "int", ",", "...", "]", "=", "tuple", "(", ")", "\n", "self", ".", "_obs", ":", "Tuple", "[", "Optional", "[", "np", ".", "ndarray", "]", ",", "...", "]", "=", "tuple", "(", ")", "\n", "self", ".", "_old_ego_obs", ":", "Optional", "[", "np", ".", "ndarray", "]", "=", "None", "\n", "\n", "self", ".", "should_update", "=", "[", "False", "]", "*", "(", "self", ".", "n_players", "-", "1", ")", "\n", "self", ".", "total_rews", "=", "[", "0", "]", "*", "(", "self", ".", "n_players", ")", "\n", "self", ".", "ego_moved", "=", "False", "\n", "\n", "self", ".", "set_resample_policy", "(", "resample_policy", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Stanford-ILIAD_PantheonRL.common.multiagentenv.MultiAgentEnv.getDummyEnv": [[66, 74], ["None"], "methods", ["None"], ["", "def", "getDummyEnv", "(", "self", ",", "player_num", ":", "int", ")", ":", "\n", "        ", "\"\"\"\n        Returns a dummy environment with just an observation and action\n        space that a partner agent can use to construct their policy network.\n\n        :param player_num: the partner number to query\n        \"\"\"", "\n", "return", "self", "\n", "\n"]], "home.repos.pwc.inspect_result.Stanford-ILIAD_PantheonRL.common.multiagentenv.MultiAgentEnv._get_partner_num": [[75, 82], ["multiagentenv.PlayerException"], "methods", ["None"], ["", "def", "_get_partner_num", "(", "self", ",", "player_num", ":", "int", ")", "->", "int", ":", "\n", "        ", "if", "player_num", "==", "self", ".", "ego_ind", ":", "\n", "            ", "raise", "PlayerException", "(", "\n", "\"Ego agent is not set by the environment\"", ")", "\n", "", "elif", "player_num", ">", "self", ".", "ego_ind", ":", "\n", "            ", "return", "player_num", "-", "1", "\n", "", "return", "player_num", "\n", "\n"]], "home.repos.pwc.inspect_result.Stanford-ILIAD_PantheonRL.common.multiagentenv.MultiAgentEnv.add_partner_agent": [[83, 93], ["multiagentenv.MultiAgentEnv.partners[].append", "multiagentenv.MultiAgentEnv._get_partner_num"], "methods", ["home.repos.pwc.inspect_result.Stanford-ILIAD_PantheonRL.common.multiagentenv.MultiAgentEnv._get_partner_num"], ["", "def", "add_partner_agent", "(", "self", ",", "agent", ":", "Agent", ",", "player_num", ":", "int", "=", "1", ")", "->", "None", ":", "\n", "        ", "\"\"\"\n        Add agent to the list of potential partner agents. If there are\n        multiple agents that can be a specific player number, the environment\n        randomly samples from them at the start of every episode.\n\n        :param agent: Agent to add\n        :param player_num: the player number that this new agent can be\n        \"\"\"", "\n", "self", ".", "partners", "[", "self", ".", "_get_partner_num", "(", "player_num", ")", "]", ".", "append", "(", "agent", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Stanford-ILIAD_PantheonRL.common.multiagentenv.MultiAgentEnv.set_partnerid": [[94, 103], ["multiagentenv.MultiAgentEnv._get_partner_num", "len"], "methods", ["home.repos.pwc.inspect_result.Stanford-ILIAD_PantheonRL.common.multiagentenv.MultiAgentEnv._get_partner_num"], ["", "def", "set_partnerid", "(", "self", ",", "agent_id", ":", "int", ",", "player_num", ":", "int", "=", "1", ")", "->", "None", ":", "\n", "        ", "\"\"\"\n        Set the current partner agent to use\n\n        :param agent_id: agent_id to use as current partner\n        \"\"\"", "\n", "partner_num", "=", "self", ".", "_get_partner_num", "(", "player_num", ")", "\n", "assert", "(", "agent_id", ">=", "0", "and", "agent_id", "<", "len", "(", "self", ".", "partners", "[", "partner_num", "]", ")", ")", "\n", "self", ".", "partnerids", "[", "partner_num", "]", "=", "agent_id", "\n", "\n"]], "home.repos.pwc.inspect_result.Stanford-ILIAD_PantheonRL.common.multiagentenv.MultiAgentEnv.resample_random": [[104, 108], ["numpy.random.randint", "len"], "methods", ["None"], ["", "def", "resample_random", "(", "self", ")", "->", "None", ":", "\n", "        ", "\"\"\" Randomly resamples each partner policy \"\"\"", "\n", "self", ".", "partnerids", "=", "[", "np", ".", "random", ".", "randint", "(", "len", "(", "plist", ")", ")", "\n", "for", "plist", "in", "self", ".", "partners", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.Stanford-ILIAD_PantheonRL.common.multiagentenv.MultiAgentEnv.resample_round_robin": [[109, 117], ["len"], "methods", ["None"], ["", "def", "resample_round_robin", "(", "self", ")", "->", "None", ":", "\n", "        ", "\"\"\"\n        Sets the partner policy to the next option on the list for round-robin\n        sampling.\n\n        Note: This function is only valid for 2-player environments\n        \"\"\"", "\n", "self", ".", "partnerids", "=", "[", "(", "self", ".", "partnerids", "[", "0", "]", "+", "1", ")", "%", "len", "(", "self", ".", "partners", "[", "0", "]", ")", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.Stanford-ILIAD_PantheonRL.common.multiagentenv.MultiAgentEnv.set_resample_policy": [[118, 139], ["multiagentenv.PlayerException", "multiagentenv.PlayerException"], "methods", ["None"], ["", "def", "set_resample_policy", "(", "self", ",", "resample_policy", ":", "str", ")", "->", "None", ":", "\n", "        ", "\"\"\"\n        Set the resample_partner method to round \"robin\" or \"random\"\n\n        :param resample_policy: The new resampling policy to use.\n        - Valid values are: \"default\", \"robin\", \"random\"\n        \"\"\"", "\n", "if", "resample_policy", "==", "\"default\"", ":", "\n", "            ", "resample_policy", "=", "\"robin\"", "if", "self", ".", "n_players", "==", "2", "else", "\"random\"", "\n", "\n", "", "if", "resample_policy", "==", "\"robin\"", "and", "self", ".", "n_players", "!=", "2", ":", "\n", "            ", "raise", "PlayerException", "(", "\n", "\"Cannot do round robin resampling for >2 players\"", ")", "\n", "\n", "", "if", "resample_policy", "==", "\"robin\"", ":", "\n", "            ", "self", ".", "resample_partner", "=", "self", ".", "resample_round_robin", "\n", "", "elif", "resample_policy", "==", "\"random\"", ":", "\n", "            ", "self", ".", "resample_partner", "=", "self", ".", "resample_random", "\n", "", "else", ":", "\n", "            ", "raise", "PlayerException", "(", "\n", "f\"Invalid resampling policy: {resample_policy}\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Stanford-ILIAD_PantheonRL.common.multiagentenv.MultiAgentEnv._get_actions": [[140, 153], ["zip", "numpy.array", "actions.append", "multiagentenv.MultiAgentEnv._get_partner_num", "actions.append", "agent.get_action", "agent.update"], "methods", ["home.repos.pwc.inspect_result.Stanford-ILIAD_PantheonRL.common.multiagentenv.MultiAgentEnv._get_partner_num", "home.repos.pwc.inspect_result.Stanford-ILIAD_PantheonRL.common.agents.RecordingAgentWrapper.get_action", "home.repos.pwc.inspect_result.Stanford-ILIAD_PantheonRL.common.agents.RecordingAgentWrapper.update"], ["", "", "def", "_get_actions", "(", "self", ",", "players", ",", "obs", ",", "ego_act", "=", "None", ")", ":", "\n", "        ", "actions", "=", "[", "]", "\n", "for", "player", ",", "ob", "in", "zip", "(", "players", ",", "obs", ")", ":", "\n", "            ", "if", "player", "==", "self", ".", "ego_ind", ":", "\n", "                ", "actions", ".", "append", "(", "ego_act", ")", "\n", "", "else", ":", "\n", "                ", "p", "=", "self", ".", "_get_partner_num", "(", "player", ")", "\n", "agent", "=", "self", ".", "partners", "[", "p", "]", "[", "self", ".", "partnerids", "[", "p", "]", "]", "\n", "actions", ".", "append", "(", "agent", ".", "get_action", "(", "ob", ")", ")", "\n", "if", "not", "self", ".", "should_update", "[", "p", "]", ":", "\n", "                    ", "agent", ".", "update", "(", "self", ".", "total_rews", "[", "player", "]", ",", "False", ")", "\n", "", "self", ".", "should_update", "[", "p", "]", "=", "True", "\n", "", "", "return", "np", ".", "array", "(", "actions", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Stanford-ILIAD_PantheonRL.common.multiagentenv.MultiAgentEnv._update_players": [[154, 162], ["range", "range", "[].update"], "methods", ["home.repos.pwc.inspect_result.Stanford-ILIAD_PantheonRL.common.agents.RecordingAgentWrapper.update"], ["", "def", "_update_players", "(", "self", ",", "rews", ",", "done", ")", ":", "\n", "        ", "for", "i", "in", "range", "(", "self", ".", "n_players", "-", "1", ")", ":", "\n", "            ", "nextrew", "=", "rews", "[", "i", "+", "(", "0", "if", "i", "<", "self", ".", "ego_ind", "else", "1", ")", "]", "\n", "if", "self", ".", "should_update", "[", "i", "]", ":", "\n", "                ", "self", ".", "partners", "[", "i", "]", "[", "self", ".", "partnerids", "[", "i", "]", "]", ".", "update", "(", "nextrew", ",", "done", ")", "\n", "\n", "", "", "for", "i", "in", "range", "(", "self", ".", "n_players", ")", ":", "\n", "            ", "self", ".", "total_rews", "[", "i", "]", "+=", "rews", "[", "i", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.Stanford-ILIAD_PantheonRL.common.multiagentenv.MultiAgentEnv.step": [[163, 206], ["multiagentenv.MultiAgentEnv._get_actions", "multiagentenv.MultiAgentEnv.n_step", "multiagentenv.MultiAgentEnv._update_players", "multiagentenv.MultiAgentEnv._players.index"], "methods", ["home.repos.pwc.inspect_result.Stanford-ILIAD_PantheonRL.common.multiagentenv.MultiAgentEnv._get_actions", "home.repos.pwc.inspect_result.Stanford-ILIAD_PantheonRL.common.multiagentenv.SimultaneousEnv.n_step", "home.repos.pwc.inspect_result.Stanford-ILIAD_PantheonRL.common.multiagentenv.MultiAgentEnv._update_players"], ["", "", "def", "step", "(", "\n", "self", ",", "\n", "action", ":", "np", ".", "ndarray", "\n", ")", "->", "Tuple", "[", "Optional", "[", "np", ".", "ndarray", "]", ",", "float", ",", "bool", ",", "Dict", "]", ":", "\n", "        ", "\"\"\"\n        Run one timestep from the perspective of the ego-agent. This involves\n        calling the ego_step function and the alt_step function to get to the\n        next observation of the ego agent.\n\n        Accepts the ego-agent's action and returns a tuple of (observation,\n        reward, done, info) from the perspective of the ego agent.\n\n        :param action: An action provided by the ego-agent.\n\n        :returns:\n            observation: Ego-agent's next observation\n            reward: Amount of reward returned after previous action\n            done: Whether the episode has ended (need to call reset() if True)\n            info: Extra information about the environment\n        \"\"\"", "\n", "ego_rew", "=", "0.0", "\n", "\n", "while", "True", ":", "\n", "            ", "acts", "=", "self", ".", "_get_actions", "(", "self", ".", "_players", ",", "self", ".", "_obs", ",", "action", ")", "\n", "self", ".", "_players", ",", "self", ".", "_obs", ",", "rews", ",", "done", ",", "info", "=", "self", ".", "n_step", "(", "acts", ")", "\n", "info", "[", "'_partnerid'", "]", "=", "self", ".", "partnerids", "\n", "\n", "self", ".", "_update_players", "(", "rews", ",", "done", ")", "\n", "\n", "ego_rew", "+=", "rews", "[", "self", ".", "ego_ind", "]", "if", "self", ".", "ego_moved", "else", "self", ".", "total_rews", "[", "self", ".", "ego_ind", "]", "\n", "\n", "self", ".", "ego_moved", "=", "True", "\n", "\n", "if", "done", ":", "\n", "                ", "return", "self", ".", "_old_ego_obs", ",", "ego_rew", ",", "done", ",", "info", "\n", "\n", "", "if", "self", ".", "ego_ind", "in", "self", ".", "_players", ":", "\n", "                ", "break", "\n", "\n", "", "", "ego_obs", "=", "self", ".", "_obs", "[", "self", ".", "_players", ".", "index", "(", "self", ".", "ego_ind", ")", "]", "\n", "self", ".", "_old_ego_obs", "=", "ego_obs", "\n", "return", "ego_obs", ",", "ego_rew", ",", "done", ",", "info", "\n", "\n"]], "home.repos.pwc.inspect_result.Stanford-ILIAD_PantheonRL.common.multiagentenv.MultiAgentEnv.reset": [[207, 234], ["multiagentenv.MultiAgentEnv.resample_partner", "multiagentenv.MultiAgentEnv.n_reset", "multiagentenv.MultiAgentEnv._get_actions", "multiagentenv.MultiAgentEnv.n_step", "multiagentenv.MultiAgentEnv._update_players", "multiagentenv.PlayerException", "multiagentenv.MultiAgentEnv._players.index"], "methods", ["home.repos.pwc.inspect_result.Stanford-ILIAD_PantheonRL.common.multiagentenv.SimultaneousEnv.n_reset", "home.repos.pwc.inspect_result.Stanford-ILIAD_PantheonRL.common.multiagentenv.MultiAgentEnv._get_actions", "home.repos.pwc.inspect_result.Stanford-ILIAD_PantheonRL.common.multiagentenv.SimultaneousEnv.n_step", "home.repos.pwc.inspect_result.Stanford-ILIAD_PantheonRL.common.multiagentenv.MultiAgentEnv._update_players"], ["", "def", "reset", "(", "self", ")", "->", "np", ".", "ndarray", ":", "\n", "        ", "\"\"\"\n        Reset environment to an initial state and return the first observation\n        for the ego agent.\n\n        :returns: Ego-agent's first observation\n        \"\"\"", "\n", "self", ".", "resample_partner", "(", ")", "\n", "self", ".", "_players", ",", "self", ".", "_obs", "=", "self", ".", "n_reset", "(", ")", "\n", "self", ".", "should_update", "=", "[", "False", "]", "*", "(", "self", ".", "n_players", "-", "1", ")", "\n", "self", ".", "total_rews", "=", "[", "0", "]", "*", "self", ".", "n_players", "\n", "self", ".", "ego_moved", "=", "False", "\n", "\n", "while", "self", ".", "ego_ind", "not", "in", "self", ".", "_players", ":", "\n", "            ", "acts", "=", "self", ".", "_get_actions", "(", "self", ".", "_players", ",", "self", ".", "_obs", ")", "\n", "self", ".", "_players", ",", "self", ".", "_obs", ",", "rews", ",", "done", ",", "_", "=", "self", ".", "n_step", "(", "acts", ")", "\n", "\n", "if", "done", ":", "\n", "                ", "raise", "PlayerException", "(", "\"Game ended before ego moved\"", ")", "\n", "\n", "", "self", ".", "_update_players", "(", "rews", ",", "done", ")", "\n", "\n", "", "ego_obs", "=", "self", ".", "_obs", "[", "self", ".", "_players", ".", "index", "(", "self", ".", "ego_ind", ")", "]", "\n", "\n", "assert", "ego_obs", "is", "not", "None", "\n", "self", ".", "_old_ego_obs", "=", "ego_obs", "\n", "return", "ego_obs", "\n", "\n"]], "home.repos.pwc.inspect_result.Stanford-ILIAD_PantheonRL.common.multiagentenv.MultiAgentEnv.n_step": [[235, 261], ["None"], "methods", ["None"], ["", "@", "abstractmethod", "\n", "def", "n_step", "(", "\n", "self", ",", "\n", "actions", ":", "List", "[", "np", ".", "ndarray", "]", ",", "\n", ")", "->", "Tuple", "[", "Tuple", "[", "int", ",", "...", "]", ",", "\n", "Tuple", "[", "Optional", "[", "np", ".", "ndarray", "]", ",", "...", "]", ",", "\n", "Tuple", "[", "float", ",", "...", "]", ",", "\n", "bool", ",", "\n", "Dict", "]", ":", "\n", "        ", "\"\"\"\n        Perform the actions specified by the agents that will move. This\n        function returns a tuple of (next agents, observations, both rewards,\n        done, info).\n\n        This function is called by the `step` function.\n\n        :param actions: List of action provided agents that are acting on this\n        step.\n\n        :returns:\n            agents: Tuple representing the agents to call for the next actions\n            observations: Tuple representing the next observations (ego, alt)\n            rewards: Tuple representing the rewards of all agents\n            done: Whether the episode has ended\n            info: Extra information about the environment\n        \"\"\"", "\n", "\n"]], "home.repos.pwc.inspect_result.Stanford-ILIAD_PantheonRL.common.multiagentenv.MultiAgentEnv.n_reset": [[262, 275], ["None"], "methods", ["None"], ["", "@", "abstractmethod", "\n", "def", "n_reset", "(", "self", ")", "->", "Tuple", "[", "Tuple", "[", "int", ",", "...", "]", ",", "\n", "Tuple", "[", "Optional", "[", "np", ".", "ndarray", "]", ",", "...", "]", "]", ":", "\n", "        ", "\"\"\"\n        Reset the environment and return which agents will move first along\n        with their initial observations.\n\n        This function is called by the `reset` function.\n\n        :returns:\n            agents: Tuple representing the agents that will move first\n            observations: Tuple representing the observations of both agents\n        \"\"\"", "\n", "\n"]], "home.repos.pwc.inspect_result.Stanford-ILIAD_PantheonRL.common.multiagentenv.TurnBasedEnv.__init__": [[288, 296], ["multiagentenv.MultiAgentEnv.__init__"], "methods", ["home.repos.pwc.inspect_result.Stanford-ILIAD_PantheonRL.overcookedgym.overcooked.OvercookedMultiEnv.__init__"], ["def", "__init__", "(", "self", ",", "\n", "probegostart", ":", "float", "=", "0.5", ",", "\n", "partners", ":", "Optional", "[", "List", "[", "Agent", "]", "]", "=", "None", ")", ":", "\n", "        ", "partners", "=", "[", "partners", "]", "if", "partners", "else", "None", "\n", "super", "(", "TurnBasedEnv", ",", "self", ")", ".", "__init__", "(", "\n", "ego_ind", "=", "0", ",", "n_players", "=", "2", ",", "partners", "=", "partners", ")", "\n", "self", ".", "probegostart", "=", "probegostart", "\n", "self", ".", "ego_next", "=", "True", "\n", "\n"]], "home.repos.pwc.inspect_result.Stanford-ILIAD_PantheonRL.common.multiagentenv.TurnBasedEnv.n_step": [[297, 312], ["multiagentenv.TurnBasedEnv.ego_step", "multiagentenv.TurnBasedEnv.alt_step"], "methods", ["home.repos.pwc.inspect_result.Stanford-ILIAD_PantheonRL.common.multiagentenv.TurnBasedEnv.ego_step", "home.repos.pwc.inspect_result.Stanford-ILIAD_PantheonRL.common.multiagentenv.TurnBasedEnv.alt_step"], ["", "def", "n_step", "(", "\n", "self", ",", "\n", "actions", ":", "List", "[", "np", ".", "ndarray", "]", ",", "\n", ")", "->", "Tuple", "[", "Tuple", "[", "int", ",", "...", "]", ",", "\n", "Tuple", "[", "Optional", "[", "np", ".", "ndarray", "]", ",", "...", "]", ",", "\n", "Tuple", "[", "float", ",", "...", "]", ",", "\n", "bool", ",", "\n", "Dict", "]", ":", "\n", "        ", "agents", "=", "(", "1", "if", "self", ".", "ego_next", "else", "0", ",", ")", "\n", "obs", ",", "rews", ",", "done", ",", "info", "=", "self", ".", "ego_step", "(", "actions", "[", "0", "]", ")", "if", "self", ".", "ego_next", "else", "self", ".", "alt_step", "(", "actions", "[", "0", "]", ")", "\n", "\n", "self", ".", "ego_next", "=", "not", "self", ".", "ego_next", "\n", "\n", "return", "agents", ",", "(", "obs", ",", ")", ",", "rews", ",", "done", ",", "info", "\n", "\n"]], "home.repos.pwc.inspect_result.Stanford-ILIAD_PantheonRL.common.multiagentenv.TurnBasedEnv.n_reset": [[313, 318], ["numpy.random.rand", "multiagentenv.TurnBasedEnv.multi_reset"], "methods", ["home.repos.pwc.inspect_result.Stanford-ILIAD_PantheonRL.overcookedgym.overcooked.OvercookedMultiEnv.multi_reset"], ["", "def", "n_reset", "(", "self", ")", "->", "Tuple", "[", "Tuple", "[", "int", ",", "...", "]", ",", "\n", "Tuple", "[", "Optional", "[", "np", ".", "ndarray", "]", ",", "...", "]", "]", ":", "\n", "        ", "self", ".", "ego_next", "=", "(", "np", ".", "random", ".", "rand", "(", ")", "<", "self", ".", "probegostart", ")", "\n", "\n", "return", "(", "0", "if", "self", ".", "ego_next", "else", "1", ",", ")", ",", "(", "self", ".", "multi_reset", "(", "self", ".", "ego_next", ")", ",", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Stanford-ILIAD_PantheonRL.common.multiagentenv.TurnBasedEnv.ego_step": [[319, 338], ["None"], "methods", ["None"], ["", "@", "abstractmethod", "\n", "def", "ego_step", "(", "\n", "self", ",", "\n", "action", ":", "np", ".", "ndarray", "\n", ")", "->", "Tuple", "[", "Optional", "[", "np", ".", "ndarray", "]", ",", "Tuple", "[", "float", ",", "float", "]", ",", "bool", ",", "Dict", "]", ":", "\n", "        ", "\"\"\"\n        Perform the ego-agent's action and return a tuple of (partner's\n        observation, both rewards, done, info).\n\n        This function is called by the `step` function along with alt-step.\n\n        :param action: An action provided by the ego-agent.\n\n        :returns:\n            partner observation: Partner's next observation\n            rewards: Tuple representing the rewards of both agents (ego, alt)\n            done: Whether the episode has ended\n            info: Extra information about the environment\n        \"\"\"", "\n", "\n"]], "home.repos.pwc.inspect_result.Stanford-ILIAD_PantheonRL.common.multiagentenv.TurnBasedEnv.alt_step": [[339, 358], ["None"], "methods", ["None"], ["", "@", "abstractmethod", "\n", "def", "alt_step", "(", "\n", "self", ",", "\n", "action", ":", "np", ".", "ndarray", "\n", ")", "->", "Tuple", "[", "Optional", "[", "np", ".", "ndarray", "]", ",", "Tuple", "[", "float", ",", "float", "]", ",", "bool", ",", "Dict", "]", ":", "\n", "        ", "\"\"\"\n        Perform the partner's action and return a tuple of (ego's observation,\n        both rewards, done, info).\n\n        This function is called by the `step` function along with ego-step.\n\n        :param action: An action provided by the partner.\n\n        :returns:\n            ego observation: Ego-agent's next observation\n            rewards: Tuple representing the rewards of both agents (ego, alt)\n            done: Whether the episode has ended\n            info: Extra information about the environment\n        \"\"\"", "\n", "\n"]], "home.repos.pwc.inspect_result.Stanford-ILIAD_PantheonRL.common.multiagentenv.TurnBasedEnv.multi_reset": [[359, 371], ["None"], "methods", ["None"], ["", "@", "abstractmethod", "\n", "def", "multi_reset", "(", "self", ",", "egofirst", ":", "bool", ")", "->", "np", ".", "ndarray", ":", "\n", "        ", "\"\"\"\n        Reset the environment and give the observation of the starting agent\n        (based on the value of `egofirst`).\n\n        This function is called by the `reset` function.\n\n        :param egofirst: True if the ego has the first turn, False otherwise\n        :returns: The observation for the starting agent (ego if `egofirst` is\n            True, and the partner's observation otherwise)\n        \"\"\"", "\n", "\n"]], "home.repos.pwc.inspect_result.Stanford-ILIAD_PantheonRL.common.multiagentenv.SimultaneousEnv.__init__": [[380, 384], ["multiagentenv.MultiAgentEnv.__init__"], "methods", ["home.repos.pwc.inspect_result.Stanford-ILIAD_PantheonRL.overcookedgym.overcooked.OvercookedMultiEnv.__init__"], ["def", "__init__", "(", "self", ",", "partners", ":", "Optional", "[", "List", "[", "Agent", "]", "]", "=", "None", ")", ":", "\n", "        ", "partners", "=", "[", "partners", "]", "if", "partners", "else", "None", "\n", "super", "(", "SimultaneousEnv", ",", "self", ")", ".", "__init__", "(", "\n", "ego_ind", "=", "0", ",", "n_players", "=", "2", ",", "partners", "=", "partners", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Stanford-ILIAD_PantheonRL.common.multiagentenv.SimultaneousEnv.n_step": [[385, 394], ["multiagentenv.SimultaneousEnv.multi_step"], "methods", ["home.repos.pwc.inspect_result.Stanford-ILIAD_PantheonRL.overcookedgym.overcooked.OvercookedMultiEnv.multi_step"], ["", "def", "n_step", "(", "\n", "self", ",", "\n", "actions", ":", "List", "[", "np", ".", "ndarray", "]", ",", "\n", ")", "->", "Tuple", "[", "Tuple", "[", "int", ",", "...", "]", ",", "\n", "Tuple", "[", "Optional", "[", "np", ".", "ndarray", "]", ",", "...", "]", ",", "\n", "Tuple", "[", "float", ",", "...", "]", ",", "\n", "bool", ",", "\n", "Dict", "]", ":", "\n", "        ", "return", "(", "(", "0", ",", "1", ")", ",", ")", "+", "self", ".", "multi_step", "(", "actions", "[", "0", "]", ",", "actions", "[", "1", "]", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Stanford-ILIAD_PantheonRL.common.multiagentenv.SimultaneousEnv.n_reset": [[395, 398], ["multiagentenv.SimultaneousEnv.multi_reset"], "methods", ["home.repos.pwc.inspect_result.Stanford-ILIAD_PantheonRL.overcookedgym.overcooked.OvercookedMultiEnv.multi_reset"], ["", "def", "n_reset", "(", "self", ")", "->", "Tuple", "[", "Tuple", "[", "int", ",", "...", "]", ",", "\n", "Tuple", "[", "Optional", "[", "np", ".", "ndarray", "]", ",", "...", "]", "]", ":", "\n", "        ", "return", "(", "0", ",", "1", ")", ",", "self", ".", "multi_reset", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Stanford-ILIAD_PantheonRL.common.multiagentenv.SimultaneousEnv.multi_step": [[399, 421], ["None"], "methods", ["None"], ["", "@", "abstractmethod", "\n", "def", "multi_step", "(", "\n", "self", ",", "\n", "ego_action", ":", "np", ".", "ndarray", ",", "\n", "alt_action", ":", "np", ".", "ndarray", "\n", ")", "->", "Tuple", "[", "Tuple", "[", "Optional", "[", "np", ".", "ndarray", "]", ",", "Optional", "[", "np", ".", "ndarray", "]", "]", ",", "\n", "Tuple", "[", "float", ",", "float", "]", ",", "bool", ",", "Dict", "]", ":", "\n", "        ", "\"\"\"\n        Perform the ego-agent's and partner's actions. This function returns a\n        tuple of (observations, both rewards, done, info).\n\n        This function is called by the `step` function.\n\n        :param ego_action: An action provided by the ego-agent.\n        :param alt_action: An action provided by the partner.\n\n        :returns:\n            observations: Tuple representing the next observations (ego, alt)\n            rewards: Tuple representing the rewards of both agents (ego, alt)\n            done: Whether the episode has ended\n            info: Extra information about the environment\n        \"\"\"", "\n", "\n"]], "home.repos.pwc.inspect_result.Stanford-ILIAD_PantheonRL.common.multiagentenv.SimultaneousEnv.multi_reset": [[422, 431], ["None"], "methods", ["None"], ["", "@", "abstractmethod", "\n", "def", "multi_reset", "(", "self", ")", "->", "Tuple", "[", "np", ".", "ndarray", ",", "np", ".", "ndarray", "]", ":", "\n", "        ", "\"\"\"\n        Reset the environment and give the observation of both agents.\n\n        This function is called by the `reset` function.\n\n        :returns: The observations of both agents\n        \"\"\"", "\n", "", "", ""]], "home.repos.pwc.inspect_result.Stanford-ILIAD_PantheonRL.common.agents.Agent.get_action": [[28, 37], ["None"], "methods", ["None"], ["            ", "errors", ",", "partners", ",", "tb_log", ",", "tb_name", ",", "filedata", "=", "check_agent_errors", "(", "g", ".", "user", "[", "'id'", "]", ",", "env", ",", "session", "[", "'ego'", "]", ",", "session", "[", "'partners'", "]", ")", "\n", "\n", "", "if", "not", "errors", "==", "[", "]", ":", "\n", "            ", "errors", ".", "append", "(", "\"Either add more agents, or press \\'Train\\' again to continue.\"", ")", "\n", "for", "e", "in", "errors", ":", "\n", "                ", "flash", "(", "e", ")", "\n", "", "session", "[", "'partners'", "]", "=", "partners", "\n", "", "else", ":", "\n", "            ", "session", "[", "'tb_log'", "]", "=", "tb_log", "\n", "session", "[", "'tb_name'", "]", "=", "tb_name", "\n"]], "home.repos.pwc.inspect_result.Stanford-ILIAD_PantheonRL.common.agents.Agent.update": [[38, 51], ["None"], "methods", ["None"], ["db", "=", "get_db", "(", ")", "\n", "db", ".", "execute", "(", "\n", "'UPDATE user SET filedata = ?'", "\n", "' WHERE id = ?'", ",", "\n", "(", "filedata", ",", "g", ".", "user", "[", "'id'", "]", ")", "\n", ")", "\n", "db", ".", "commit", "(", ")", "\n", "return", "redirect", "(", "url_for", "(", "'training.main'", ")", ")", "\n", "\n", "# set blank agent parameters in session", "\n", "", "", "if", "not", "'egotype'", "in", "session", ":", "\n", "        ", "session", "[", "'egotype'", "]", "=", "None", "\n", "", "if", "not", "'partnertype'", "in", "session", ":", "\n", "        ", "session", "[", "'partnertype'", "]", "=", "None", "\n"]], "home.repos.pwc.inspect_result.Stanford-ILIAD_PantheonRL.common.agents.StaticPolicyAgent.__init__": [[60, 62], ["None"], "methods", ["None"], ["\n", "", "", "return", "render_template", "(", "'agentparams.html'", ",", "partners", "=", "len", "(", "session", "[", "'partners'", "]", ")", ",", "ego_options", "=", "EGO_LIST", ",", "partner_options", "=", "PARTNER_LIST", ",", "env", "=", "env", ",", "env_options", "=", "ENV_LIST", ",", "fixed_options", "=", "options", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Stanford-ILIAD_PantheonRL.common.agents.StaticPolicyAgent.get_action": [[63, 73], ["util.action_from_policy", "util.clip_actions"], "methods", ["home.repos.pwc.inspect_result.Stanford-ILIAD_PantheonRL.common.util.action_from_policy", "home.repos.pwc.inspect_result.Stanford-ILIAD_PantheonRL.common.util.clip_actions"], ["", "@", "bp", ".", "route", "(", "\"/<string:env>/egotype\"", ",", "methods", "=", "(", "'POST'", ",", ")", ")", "\n", "@", "login_required", "\n", "def", "egotype", "(", "env", ")", ":", "\n", "    ", "if", "request", ".", "method", "==", "'POST'", ":", "\n", "        ", "session", "[", "'egotype'", "]", "=", "request", ".", "form", "[", "'egotype'", "]", "\n", "return", "redirect", "(", "url_for", "(", "'agents.agents'", ",", "env", "=", "env", ")", ")", "\n", "\n", "", "", "@", "bp", ".", "route", "(", "\"/<string:env>/partnertype\"", ",", "methods", "=", "(", "'POST'", ",", ")", ")", "\n", "@", "login_required", "\n", "def", "partnertype", "(", "env", ")", ":", "\n", "    ", "if", "request", ".", "method", "==", "'POST'", ":", "\n"]], "home.repos.pwc.inspect_result.Stanford-ILIAD_PantheonRL.common.agents.StaticPolicyAgent.update": [[74, 79], ["None"], "methods", ["None"], ["        ", "session", "[", "'partnertype'", "]", "=", "request", ".", "form", "[", "'partnertype'", "]", "\n", "# if session['partnertype'] == 'FIXED':", "\n", "#     error = \"Fixed partner agents are not yet implemented. Please pick a different partner type.\"", "\n", "#     flash(error)", "\n", "return", "redirect", "(", "url_for", "(", "'agents.agents'", ",", "env", "=", "env", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Stanford-ILIAD_PantheonRL.common.agents.OnPolicyAgent.__init__": [[91, 109], ["torch.empty", "agents.OnPolicyAgent.model.set_logger", "collections.deque", "stable_baselines3.common.utils.configure_logger"], "methods", ["None"], ["", "return", "redirect", "(", "url_for", "(", "'agents.agents'", ",", "env", "=", "env", ")", ")", "\n", "\n", "\n", "", "", "@", "bp", ".", "route", "(", "\"/<string:env>/setpartner\"", ",", "methods", "=", "(", "'POST'", ",", ")", ")", "\n", "@", "login_required", "\n", "def", "setpartner", "(", "env", ")", ":", "\n", "    ", "if", "request", ".", "method", "==", "'POST'", ":", "\n", "# add ego params to the session variable", "\n", "        ", "error", ",", "partner_dict", "=", "create_partner_dict", "(", "g", ".", "user", "[", "'id'", "]", ",", "session", "[", "'partnertype'", "]", ",", "env", ",", "request", ".", "form", ")", "\n", "if", "error", "is", "not", "None", ":", "\n", "            ", "flash", "(", "error", ")", "\n", "", "else", ":", "\n", "            ", "session", "[", "'partners'", "]", ".", "append", "(", "partner_dict", ")", "\n", "session", "[", "'partnertype'", "]", "=", "None", "\n", "", "return", "redirect", "(", "url_for", "(", "'agents.agents'", ",", "env", "=", "env", ")", ")", "", "", "", ""]], "home.repos.pwc.inspect_result.Stanford-ILIAD_PantheonRL.common.agents.OnPolicyAgent.get_action": [[110, 183], ["util.resample_noise", "util.action_from_policy", "buf.compute_returns_and_advantage", "agents.OnPolicyAgent.model.train", "buf.reset", "agents.OnPolicyAgent.model.ep_info_buffer.pop", "agents.OnPolicyAgent.model.ep_info_buffer.append", "buf.add", "util.clip_actions", "agents.OnPolicyAgent.model.logger.record", "agents.OnPolicyAgent.model.logger.record", "agents.OnPolicyAgent.model.logger.record", "agents.OnPolicyAgent.model.logger.dump", "numpy.reshape", "numpy.reshape", "agents.OnPolicyAgent.model.ep_info_buffer.pop", "agents.OnPolicyAgent.model.logger.record", "agents.OnPolicyAgent.model.logger.record", "agents.OnPolicyAgent.model.ep_info_buffer.append", "len", "len", "stable_baselines3.common.utils.safe_mean", "stable_baselines3.common.utils.safe_mean"], "methods", ["home.repos.pwc.inspect_result.Stanford-ILIAD_PantheonRL.common.util.resample_noise", "home.repos.pwc.inspect_result.Stanford-ILIAD_PantheonRL.common.util.action_from_policy", "home.repos.pwc.inspect_result.Stanford-ILIAD_PantheonRL.modular.learn.ModularAlgorithm.train", "home.repos.pwc.inspect_result.Stanford-ILIAD_PantheonRL.common.multiagentenv.MultiAgentEnv.reset", "home.repos.pwc.inspect_result.Stanford-ILIAD_PantheonRL.common.wrappers.HistoryQueue.add", "home.repos.pwc.inspect_result.Stanford-ILIAD_PantheonRL.common.util.clip_actions"], []], "home.repos.pwc.inspect_result.Stanford-ILIAD_PantheonRL.common.agents.OnPolicyAgent.update": [[184, 202], ["agents.OnPolicyAgent.model.ep_info_buffer.pop", "agents.OnPolicyAgent.model.ep_info_buffer.append", "agents.OnPolicyAgent.model.ep_info_buffer.append"], "methods", ["None"], []], "home.repos.pwc.inspect_result.Stanford-ILIAD_PantheonRL.common.agents.OnPolicyAgent.learn": [[203, 207], ["agents.OnPolicyAgent.model.learn"], "methods", ["home.repos.pwc.inspect_result.Stanford-ILIAD_PantheonRL.common.agents.OffPolicyAgent.learn"], []], "home.repos.pwc.inspect_result.Stanford-ILIAD_PantheonRL.common.agents.OffPolicyAgent.__init__": [[219, 244], ["time.time", "agents.OffPolicyAgent.model.set_logger", "collections.deque", "stable_baselines3.common.utils.configure_logger"], "methods", ["None"], []], "home.repos.pwc.inspect_result.Stanford-ILIAD_PantheonRL.common.agents.OffPolicyAgent.get_action": [[245, 316], ["util.resample_noise", "obs.reshape.reshape.reshape", "agents.OffPolicyAgent.model._sample_action", "util.clip_actions", "numpy.array().copy", "agents.OffPolicyAgent.model._store_transition", "agents.OffPolicyAgent.episode_rewards.append", "agents.OffPolicyAgent.total_timesteps.append", "agents.OffPolicyAgent.model.action_noise.reset", "agents.OffPolicyAgent.model.logger.record", "agents.OffPolicyAgent.model.logger.record", "agents.OffPolicyAgent.model.logger.record", "agents.OffPolicyAgent.model.logger.dump", "numpy.array", "agents.OffPolicyAgent.model.ep_info_buffer.pop", "agents.OffPolicyAgent.model.logger.record", "agents.OffPolicyAgent.model.logger.record", "agents.OffPolicyAgent.model.ep_info_buffer.append", "len", "len", "stable_baselines3.common.utils.safe_mean", "stable_baselines3.common.utils.safe_mean"], "methods", ["home.repos.pwc.inspect_result.Stanford-ILIAD_PantheonRL.common.util.resample_noise", "home.repos.pwc.inspect_result.Stanford-ILIAD_PantheonRL.common.util.clip_actions", "home.repos.pwc.inspect_result.Stanford-ILIAD_PantheonRL.common.multiagentenv.MultiAgentEnv.reset"], []], "home.repos.pwc.inspect_result.Stanford-ILIAD_PantheonRL.common.agents.OffPolicyAgent.update": [[317, 356], ["agents.OffPolicyAgent.model.ep_info_buffer.pop", "agents.OffPolicyAgent.model.ep_info_buffer.append", "stable_baselines3.common.utils.should_collect_more_steps", "agents.OffPolicyAgent.model.train", "agents.OffPolicyAgent.model.ep_info_buffer.append"], "methods", ["home.repos.pwc.inspect_result.Stanford-ILIAD_PantheonRL.modular.learn.ModularAlgorithm.train"], []], "home.repos.pwc.inspect_result.Stanford-ILIAD_PantheonRL.common.agents.OffPolicyAgent.learn": [[357, 360], ["agents.OffPolicyAgent.model.learn"], "methods", ["home.repos.pwc.inspect_result.Stanford-ILIAD_PantheonRL.common.agents.OffPolicyAgent.learn"], []], "home.repos.pwc.inspect_result.Stanford-ILIAD_PantheonRL.common.agents.RecordingAgentWrapper.__init__": [[372, 376], ["None"], "methods", ["None"], []], "home.repos.pwc.inspect_result.Stanford-ILIAD_PantheonRL.common.agents.RecordingAgentWrapper.get_action": [[377, 392], ["agents.RecordingAgentWrapper.realagent.get_action", "agents.RecordingAgentWrapper.allobs.append", "agents.RecordingAgentWrapper.allacts.append"], "methods", ["home.repos.pwc.inspect_result.Stanford-ILIAD_PantheonRL.common.agents.RecordingAgentWrapper.get_action"], []], "home.repos.pwc.inspect_result.Stanford-ILIAD_PantheonRL.common.agents.RecordingAgentWrapper.update": [[393, 401], ["agents.RecordingAgentWrapper.realagent.update"], "methods", ["home.repos.pwc.inspect_result.Stanford-ILIAD_PantheonRL.common.agents.RecordingAgentWrapper.update"], []], "home.repos.pwc.inspect_result.Stanford-ILIAD_PantheonRL.common.agents.RecordingAgentWrapper.get_transitions": [[402, 411], ["numpy.array", "numpy.array", "trajsaver.TransitionsMinimal"], "methods", ["None"], []], "home.repos.pwc.inspect_result.Stanford-ILIAD_PantheonRL.common.util.FeedForward32Policy.__init__": [[122, 124], ["stable_baselines3.common.policies.ActorCriticPolicy.__init__"], "methods", ["home.repos.pwc.inspect_result.Stanford-ILIAD_PantheonRL.overcookedgym.overcooked.OvercookedMultiEnv.__init__"], ["latent_pi", ",", "_", ",", "latent_sde", "=", "model", ".", "_get_latent", "(", "sampled_states", ")", "\n", "context_action_dist", "=", "model", ".", "_get_action_dist_from_latent", "(", "\n", "latent_pi", ",", "latent_sde", ")", "\n"]], "home.repos.pwc.inspect_result.Stanford-ILIAD_PantheonRL.common.util.get_space_size": [[18, 30], ["isinstance", "len", "isinstance", "isinstance", "isinstance", "len"], "function", ["None"], ["    ", "\"\"\"\n    Wrapper for the PyTorch implementation of the full form KL Divergence\n    :param dist_true: the p distribution\n    :param dist_pred: the q distribution\n    :return: KL(dist_true||dist_pred)\n    \"\"\"", "\n", "# KL Divergence for different distribution types is out of scope", "\n", "assert", "dist_true", ".", "__class__", "==", "dist_pred", ".", "__class__", ",", "\"Error: input distributions should be the same type\"", "\n", "\n", "# MultiCategoricalDistribution is not a PyTorch Distribution subclass", "\n", "# so we need to implement it ourselves!", "\n", "if", "isinstance", "(", "dist_pred", ",", "distributions", ".", "MultiCategoricalDistribution", ")", ":", "\n"]], "home.repos.pwc.inspect_result.Stanford-ILIAD_PantheonRL.common.util.calculate_space": [[32, 46], ["isinstance", "numpy.tile", "numpy.tile", "gym.spaces.Box", "isinstance", "gym.spaces.MultiDiscrete", "isinstance", "gym.spaces.MultiBinary", "isinstance", "gym.spaces.MultiDiscrete", "list"], "function", ["None"], ["[", "kl", ".", "kl_divergence", "(", "p", ",", "q", ")", "for", "p", ",", "q", "in", "zip", "(", "\n", "dist_true", ".", "distribution", ",", "dist_pred", ".", "distribution", ")", "]", ",", "\n", "dim", "=", "1", ",", "\n", ")", ".", "sum", "(", "dim", "=", "1", ")", "\n", "\n", "# Use the PyTorch kl_divergence implementation", "\n", "", "else", ":", "\n", "        ", "return", "kl", ".", "kl_divergence", "(", "dist_true", ".", "distribution", ",", "dist_pred", ".", "distribution", ")", "\n", "\n", "\n", "", "", "def", "get_L2_sphere", "(", "ctx_size", ",", "num", ",", "torch", "=", "False", ")", ":", "\n", "    ", "if", "torch", ":", "\n", "        ", "ctxs", "=", "th", ".", "rand", "(", "num", ",", "ctx_size", ",", "device", "=", "'cpu'", ")", "*", "2", "-", "1", "\n", "ctxs", "=", "ctxs", "/", "(", "th", ".", "sum", "(", "(", "ctxs", ")", "**", "2", ",", "dim", "=", "-", "1", ")", ".", "reshape", "(", "num", ",", "1", ")", ")", "**", "(", "1", "/", "2", ")", "\n", "ctxs", "=", "ctxs", ".", "to", "(", "'cpu'", ")", "\n"]], "home.repos.pwc.inspect_result.Stanford-ILIAD_PantheonRL.common.util.get_default_obs": [[48, 61], ["isinstance", "isinstance", "isinstance", "isinstance", "len"], "function", ["None"], ["        ", "ctxs", "=", "np", ".", "random", ".", "rand", "(", "num", ",", "ctx_size", ")", "*", "2", "-", "1", "\n", "ctxs", "=", "ctxs", "/", "(", "np", ".", "sum", "(", "(", "ctxs", ")", "**", "2", ",", "axis", "=", "-", "1", ")", ".", "reshape", "(", "num", ",", "1", ")", ")", "**", "(", "1", "/", "2", ")", "\n", "", "return", "ctxs", "\n", "\n", "\n", "", "def", "get_unit_square", "(", "ctx_size", ",", "num", ",", "torch", "=", "False", ")", ":", "\n", "    ", "if", "torch", ":", "\n", "        ", "ctxs", "=", "th", ".", "rand", "(", "num", ",", "ctx_size", ")", "*", "2", "-", "1", "\n", "", "else", ":", "\n", "        ", "ctxs", "=", "np", ".", "random", ".", "rand", "(", "num", ",", "ctx_size", ")", "*", "2", "-", "1", "\n", "", "return", "ctxs", "\n", "\n", "\n", "", "def", "get_positive_square", "(", "ctx_size", ",", "num", ",", "torch", "=", "False", ")", ":", "\n"]], "home.repos.pwc.inspect_result.Stanford-ILIAD_PantheonRL.common.util.action_from_policy": [[63, 82], ["obs.reshape.reshape", "torch.no_grad", "stable_baselines3.common.utils.obs_as_tensor", "policy.forward", "actions.cpu().numpy", "actions.cpu"], "function", ["home.repos.pwc.inspect_result.Stanford-ILIAD_PantheonRL.modular.policies.ModularPolicy.forward"], ["        ", "ctxs", "=", "th", ".", "rand", "(", "num", ",", "ctx_size", ")", "\n", "", "else", ":", "\n", "        ", "ctxs", "=", "np", ".", "random", ".", "rand", "(", "num", ",", "ctx_size", ")", "\n", "", "return", "ctxs", "\n", "\n", "\n", "", "def", "get_categorical", "(", "ctx_size", ",", "num", ",", "torch", "=", "False", ")", ":", "\n", "    ", "if", "torch", ":", "\n", "        ", "ctxs", "=", "th", ".", "zeros", "(", "num", ",", "ctx_size", ")", "\n", "ctxs", "[", "th", ".", "arange", "(", "num", ")", ",", "th", ".", "randint", "(", "0", ",", "ctx_size", ",", "size", "=", "(", "num", ",", ")", ")", "]", "=", "1", "\n", "", "else", ":", "\n", "        ", "ctxs", "=", "np", ".", "zeros", "(", "(", "num", ",", "ctx_size", ")", ")", "\n", "ctxs", "[", "np", ".", "arange", "(", "num", ")", ",", "np", ".", "random", ".", "randint", "(", "0", ",", "ctx_size", ",", "size", "=", "(", "num", ",", ")", ")", "]", "=", "1", "\n", "", "return", "ctxs", "\n", "\n", "\n", "", "def", "get_natural_number", "(", "ctx_size", ",", "num", ",", "torch", "=", "False", ")", ":", "\n", "    ", "'''\n    Returns context vector of shape (num,1) with numbers in range [0, ctx_size]\n    '''", "\n"]], "home.repos.pwc.inspect_result.Stanford-ILIAD_PantheonRL.common.util.clip_actions": [[84, 100], ["isinstance", "numpy.clip"], "function", ["None"], ["        ", "ctxs", "=", "th", ".", "randint", "(", "0", ",", "ctx_size", ",", "size", "=", "(", "num", ",", "1", ")", ")", "\n", "", "else", ":", "\n", "        ", "ctxs", "=", "np", ".", "random", ".", "randint", "(", "0", ",", "ctx_size", ",", "size", "=", "(", "num", ",", "1", ")", ")", "\n", "", "return", "ctxs", "\n", "\n", "\n", "", "SAMPLERS", "=", "{", "\"l2\"", ":", "get_L2_sphere", ",", "\n", "\"unit_square\"", ":", "get_unit_square", ",", "\n", "\"positive_square\"", ":", "get_positive_square", ",", "\n", "\"categorical\"", ":", "get_categorical", ",", "\n", "\"natural_numbers\"", ":", "get_natural_number", "}", "\n", "\n", "\n", "def", "get_context_kl_loss", "(", "policy", ":", "'ADAP'", ",", "model", ":", "'AdapPolicy'", ",", "\n", "train_batch", ":", "RolloutBufferSamples", ")", ":", "\n", "\n", "    ", "original_obs", "=", "train_batch", ".", "observations", "[", ":", ",", ":", "-", "policy", ".", "context_size", "]", "\n"]], "home.repos.pwc.inspect_result.Stanford-ILIAD_PantheonRL.common.util.resample_noise": [[102, 112], ["model.policy.reset_noise"], "function", ["home.repos.pwc.inspect_result.Stanford-ILIAD_PantheonRL.modular.policies.ModularPolicy.reset_noise"], ["context_size", "=", "policy", ".", "context_size", "\n", "num_context_samples", "=", "policy", ".", "num_context_samples", "\n", "num_state_samples", "=", "policy", ".", "num_state_samples", "\n", "\n", "indices", "=", "th", ".", "randperm", "(", "original_obs", ".", "shape", "[", "0", "]", ")", "[", ":", "num_state_samples", "]", "\n", "sampled_states", "=", "original_obs", "[", "indices", "]", "\n", "num_state_samples", "=", "min", "(", "num_state_samples", ",", "sampled_states", ".", "shape", "[", "0", "]", ")", "\n", "\n", "all_contexts", "=", "set", "(", ")", "\n", "all_action_dists", "=", "[", "]", "\n", "old_context", "=", "model", ".", "get_context", "(", ")", "\n"]], "home.repos.pwc.inspect_result.Stanford-ILIAD_PantheonRL.overcookedgym.overcooked.OvercookedMultiEnv.__init__": [[11, 43], ["pantheonrl.common.multiagentenv.SimultaneousEnv.__init__", "overcooked_ai_py.mdp.overcooked_mdp.OvercookedGridworld.from_layout_name", "overcooked_ai_py.planning.planners.MediumLevelPlanner.from_pickle_or_compute", "overcooked_ai_py.mdp.overcooked_env.OvercookedEnv", "overcooked.OvercookedMultiEnv._setup_observation_space", "len", "gym.spaces.Discrete", "overcooked.OvercookedMultiEnv.multi_reset", "overcooked.OvercookedMultiEnv.mdp.featurize_state", "numpy.random.seed"], "methods", ["home.repos.pwc.inspect_result.Stanford-ILIAD_PantheonRL.overcookedgym.overcooked.OvercookedMultiEnv.__init__", "home.repos.pwc.inspect_result.Stanford-ILIAD_PantheonRL.overcookedgym.overcooked.OvercookedMultiEnv._setup_observation_space", "home.repos.pwc.inspect_result.Stanford-ILIAD_PantheonRL.overcookedgym.overcooked.OvercookedMultiEnv.multi_reset"], ["    ", "def", "__init__", "(", "self", ",", "layout_name", ",", "ego_agent_idx", "=", "0", ",", "baselines", "=", "False", ")", ":", "\n", "        ", "\"\"\"\n        base_env: OvercookedEnv\n        featurize_fn: what function is used to featurize states returned in the 'both_agent_obs' field\n        \"\"\"", "\n", "super", "(", "OvercookedMultiEnv", ",", "self", ")", ".", "__init__", "(", ")", "\n", "\n", "DEFAULT_ENV_PARAMS", "=", "{", "\n", "\"horizon\"", ":", "400", "\n", "}", "\n", "rew_shaping_params", "=", "{", "\n", "\"PLACEMENT_IN_POT_REW\"", ":", "3", ",", "\n", "\"DISH_PICKUP_REWARD\"", ":", "3", ",", "\n", "\"SOUP_PICKUP_REWARD\"", ":", "5", ",", "\n", "\"DISH_DISP_DISTANCE_REW\"", ":", "0", ",", "\n", "\"POT_DISTANCE_REW\"", ":", "0", ",", "\n", "\"SOUP_DISTANCE_REW\"", ":", "0", ",", "\n", "}", "\n", "\n", "self", ".", "mdp", "=", "OvercookedGridworld", ".", "from_layout_name", "(", "layout_name", "=", "layout_name", ",", "rew_shaping_params", "=", "rew_shaping_params", ")", "\n", "mlp", "=", "MediumLevelPlanner", ".", "from_pickle_or_compute", "(", "self", ".", "mdp", ",", "NO_COUNTERS_PARAMS", ",", "force_compute", "=", "False", ")", "\n", "\n", "self", ".", "base_env", "=", "OvercookedEnv", "(", "self", ".", "mdp", ",", "**", "DEFAULT_ENV_PARAMS", ")", "\n", "self", ".", "featurize_fn", "=", "lambda", "x", ":", "self", ".", "mdp", ".", "featurize_state", "(", "x", ",", "mlp", ")", "\n", "\n", "if", "baselines", ":", "np", ".", "random", ".", "seed", "(", "0", ")", "\n", "\n", "self", ".", "observation_space", "=", "self", ".", "_setup_observation_space", "(", ")", "\n", "self", ".", "lA", "=", "len", "(", "Action", ".", "ALL_ACTIONS", ")", "\n", "self", ".", "action_space", "=", "gym", ".", "spaces", ".", "Discrete", "(", "self", ".", "lA", ")", "\n", "self", ".", "ego_agent_idx", "=", "ego_agent_idx", "\n", "self", ".", "multi_reset", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Stanford-ILIAD_PantheonRL.overcookedgym.overcooked.OvercookedMultiEnv._setup_observation_space": [[44, 50], ["overcooked.OvercookedMultiEnv.mdp.get_standard_start_state", "gym.spaces.Box", "numpy.ones", "max", "overcooked.OvercookedMultiEnv.featurize_fn"], "methods", ["None"], ["", "def", "_setup_observation_space", "(", "self", ")", ":", "\n", "        ", "dummy_state", "=", "self", ".", "mdp", ".", "get_standard_start_state", "(", ")", "\n", "obs_shape", "=", "self", ".", "featurize_fn", "(", "dummy_state", ")", "[", "0", "]", ".", "shape", "\n", "high", "=", "np", ".", "ones", "(", "obs_shape", ",", "dtype", "=", "np", ".", "float32", ")", "*", "max", "(", "self", ".", "mdp", ".", "soup_cooking_time", ",", "self", ".", "mdp", ".", "num_items_for_soup", ",", "5", ")", "\n", "\n", "return", "gym", ".", "spaces", ".", "Box", "(", "high", "*", "0", ",", "high", ",", "dtype", "=", "np", ".", "float32", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Stanford-ILIAD_PantheonRL.overcookedgym.overcooked.OvercookedMultiEnv.multi_step": [[51, 81], ["overcooked.OvercookedMultiEnv.base_env.step", "overcooked.OvercookedMultiEnv.featurize_fn"], "methods", ["home.repos.pwc.inspect_result.Stanford-ILIAD_PantheonRL.common.multiagentenv.MultiAgentEnv.step"], ["", "def", "multi_step", "(", "self", ",", "ego_action", ",", "alt_action", ")", ":", "\n", "        ", "\"\"\"\n        action:\n            (agent with index self.agent_idx action, other agent action)\n            is a tuple with the joint action of the primary and secondary agents in index format\n            encoded as an int\n\n        returns:\n            observation: formatted to be standard input for self.agent_idx's policy\n        \"\"\"", "\n", "ego_action", ",", "alt_action", "=", "Action", ".", "INDEX_TO_ACTION", "[", "ego_action", "]", ",", "Action", ".", "INDEX_TO_ACTION", "[", "alt_action", "]", "\n", "if", "self", ".", "ego_agent_idx", "==", "0", ":", "\n", "            ", "joint_action", "=", "(", "ego_action", ",", "alt_action", ")", "\n", "", "else", ":", "\n", "            ", "joint_action", "=", "(", "alt_action", ",", "ego_action", ")", "\n", "\n", "", "next_state", ",", "reward", ",", "done", ",", "info", "=", "self", ".", "base_env", ".", "step", "(", "joint_action", ")", "\n", "\n", "# reward shaping", "\n", "rew_shape", "=", "info", "[", "'shaped_r'", "]", "\n", "reward", "=", "reward", "+", "rew_shape", "\n", "\n", "#print(self.base_env.mdp.state_string(next_state))", "\n", "ob_p0", ",", "ob_p1", "=", "self", ".", "featurize_fn", "(", "next_state", ")", "\n", "if", "self", ".", "ego_agent_idx", "==", "0", ":", "\n", "            ", "ego_obs", ",", "alt_obs", "=", "ob_p0", ",", "ob_p1", "\n", "", "else", ":", "\n", "            ", "ego_obs", ",", "alt_obs", "=", "ob_p1", ",", "ob_p0", "\n", "\n", "", "return", "(", "ego_obs", ",", "alt_obs", ")", ",", "(", "reward", ",", "reward", ")", ",", "done", ",", "{", "}", "#info", "\n", "\n"]], "home.repos.pwc.inspect_result.Stanford-ILIAD_PantheonRL.overcookedgym.overcooked.OvercookedMultiEnv.multi_reset": [[82, 99], ["overcooked.OvercookedMultiEnv.base_env.reset", "overcooked.OvercookedMultiEnv.featurize_fn"], "methods", ["home.repos.pwc.inspect_result.Stanford-ILIAD_PantheonRL.common.multiagentenv.MultiAgentEnv.reset"], ["", "def", "multi_reset", "(", "self", ")", ":", "\n", "        ", "\"\"\"\n        When training on individual maps, we want to randomize which agent is assigned to which\n        starting location, in order to make sure that the agents are trained to be able to\n        complete the task starting at either of the hardcoded positions.\n\n        NOTE: a nicer way to do this would be to just randomize starting positions, and not\n        have to deal with randomizing indices.\n        \"\"\"", "\n", "self", ".", "base_env", ".", "reset", "(", ")", "\n", "ob_p0", ",", "ob_p1", "=", "self", ".", "featurize_fn", "(", "self", ".", "base_env", ".", "state", ")", "\n", "if", "self", ".", "ego_agent_idx", "==", "0", ":", "\n", "            ", "ego_obs", ",", "alt_obs", "=", "ob_p0", ",", "ob_p1", "\n", "", "else", ":", "\n", "            ", "ego_obs", ",", "alt_obs", "=", "ob_p1", ",", "ob_p0", "\n", "\n", "", "return", "(", "ego_obs", ",", "alt_obs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Stanford-ILIAD_PantheonRL.overcookedgym.overcooked.OvercookedMultiEnv.render": [[100, 102], ["None"], "methods", ["None"], ["", "def", "render", "(", "self", ",", "mode", "=", "'human'", ",", "close", "=", "False", ")", ":", "\n", "        ", "pass", "\n", "", "", ""]], "home.repos.pwc.inspect_result.Stanford-ILIAD_PantheonRL.overcooked-flask.app.get_prediction": [[21, 27], ["torch.tensor().unsqueeze().float", "policy.predict", "int", "torch.tensor().unsqueeze", "torch.tensor"], "function", ["home.repos.pwc.inspect_result.Stanford-ILIAD_PantheonRL.overcooked-flask.app.predict"], ["def", "get_prediction", "(", "s", ",", "policy", ",", "layout_name", ",", "algo", ")", ":", "\n", "    ", "s", "=", "torch", ".", "tensor", "(", "s", ")", ".", "unsqueeze", "(", "0", ")", ".", "float", "(", ")", "\n", "# print(s.size())", "\n", "actions", ",", "states", "=", "policy", ".", "predict", "(", "observation", "=", "s", ")", "\n", "# print(actions)", "\n", "return", "int", "(", "actions", "[", "0", "]", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Stanford-ILIAD_PantheonRL.overcooked-flask.app.process_state": [[29, 51], ["app.process_state.state_from_dict"], "function", ["None"], ["", "def", "process_state", "(", "state_dict", ",", "layout_name", ")", ":", "\n", "    ", "def", "object_from_dict", "(", "object_dict", ")", ":", "\n", "        ", "return", "ObjectState", "(", "**", "object_dict", ")", "\n", "\n", "", "def", "player_from_dict", "(", "player_dict", ")", ":", "\n", "        ", "held_obj", "=", "player_dict", ".", "get", "(", "\"held_object\"", ")", "\n", "if", "held_obj", "is", "not", "None", ":", "\n", "            ", "player_dict", "[", "\"held_object\"", "]", "=", "object_from_dict", "(", "held_obj", ")", "\n", "", "return", "PlayerState", "(", "**", "player_dict", ")", "\n", "\n", "", "def", "state_from_dict", "(", "state_dict", ")", ":", "\n", "        ", "state_dict", "[", "\"players\"", "]", "=", "[", "player_from_dict", "(", "\n", "p", ")", "for", "p", "in", "state_dict", "[", "\"players\"", "]", "]", "\n", "object_list", "=", "[", "object_from_dict", "(", "o", ")", "\n", "for", "_", ",", "o", "in", "state_dict", "[", "\"objects\"", "]", ".", "items", "(", ")", "]", "\n", "state_dict", "[", "\"objects\"", "]", "=", "{", "ob", ".", "position", ":", "ob", "for", "ob", "in", "object_list", "}", "\n", "return", "OvercookedState", "(", "**", "state_dict", ")", "\n", "\n", "", "state", "=", "state_from_dict", "(", "copy", ".", "deepcopy", "(", "state_dict", ")", ")", "\n", "print", "(", "state", ".", "to_dict", "(", ")", ")", "\n", "\n", "return", "MDP", ".", "featurize_state", "(", "state", ",", "MLP", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Stanford-ILIAD_PantheonRL.overcooked-flask.app.convert_traj_to_simultaneous_transitions": [[53, 88], ["numpy.concatenate", "numpy.concatenate", "numpy.concatenate", "numpy.concatenate", "numpy.concatenate", "pantheonrl.common.trajsaver.SimultaneousTransitions", "np.concatenate.append", "np.concatenate.append", "np.concatenate.append", "np.concatenate.append", "np.concatenate.append", "app.process_state", "app.process_state"], "function", ["home.repos.pwc.inspect_result.Stanford-ILIAD_PantheonRL.overcooked-flask.app.process_state", "home.repos.pwc.inspect_result.Stanford-ILIAD_PantheonRL.overcooked-flask.app.process_state"], ["", "def", "convert_traj_to_simultaneous_transitions", "(", "traj_dict", ",", "layout_name", ")", ":", "\n", "\n", "    ", "ego_obs", "=", "[", "]", "\n", "alt_obs", "=", "[", "]", "\n", "ego_act", "=", "[", "]", "\n", "alt_act", "=", "[", "]", "\n", "flags", "=", "[", "]", "\n", "\n", "for", "state_list", "in", "traj_dict", "[", "'ep_states'", "]", ":", "# loop over episodes", "\n", "        ", "ego_obs", ".", "append", "(", "[", "process_state", "(", "state", ",", "layout_name", ")", "[", "0", "]", "\n", "for", "state", "in", "state_list", "]", ")", "\n", "alt_obs", ".", "append", "(", "[", "process_state", "(", "state", ",", "layout_name", ")", "[", "1", "]", "\n", "for", "state", "in", "state_list", "]", ")", "\n", "\n", "# check pantheonrl/common/wrappers.py for flag values", "\n", "flag", "=", "[", "0", "for", "state", "in", "state_list", "]", "\n", "flag", "[", "-", "1", "]", "=", "1", "\n", "flags", ".", "append", "(", "flag", ")", "\n", "\n", "", "for", "action_list", "in", "traj_dict", "[", "'ep_actions'", "]", ":", "# loop over episodes", "\n", "        ", "ego_act", ".", "append", "(", "[", "joint_action", "[", "0", "]", "for", "joint_action", "in", "action_list", "]", ")", "\n", "alt_act", ".", "append", "(", "[", "joint_action", "[", "1", "]", "for", "joint_action", "in", "action_list", "]", ")", "\n", "\n", "", "ego_obs", "=", "np", ".", "concatenate", "(", "ego_obs", ",", "axis", "=", "-", "1", ")", "\n", "alt_obs", "=", "np", ".", "concatenate", "(", "alt_obs", ",", "axis", "=", "-", "1", ")", "\n", "ego_act", "=", "np", ".", "concatenate", "(", "ego_act", ",", "axis", "=", "-", "1", ")", "\n", "alt_act", "=", "np", ".", "concatenate", "(", "alt_act", ",", "axis", "=", "-", "1", ")", "\n", "flags", "=", "np", ".", "concatenate", "(", "flags", ",", "axis", "=", "-", "1", ")", "\n", "\n", "return", "SimultaneousTransitions", "(", "\n", "ego_obs", ",", "\n", "ego_act", ",", "\n", "alt_obs", ",", "\n", "alt_act", ",", "\n", "flags", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.Stanford-ILIAD_PantheonRL.overcooked-flask.app.predict": [[91, 127], ["app.route", "json.loads", "int", "app.process_state", "print", "print", "print", "flask.jsonify", "app.get_prediction", "int", "len", "int", "len"], "function", ["home.repos.pwc.inspect_result.Stanford-ILIAD_PantheonRL.overcooked-flask.app.process_state", "home.repos.pwc.inspect_result.Stanford-ILIAD_PantheonRL.overcooked-flask.app.get_prediction"], ["", "@", "app", ".", "route", "(", "'/predict'", ",", "methods", "=", "[", "'POST'", "]", ")", "\n", "def", "predict", "(", ")", ":", "\n", "    ", "if", "request", ".", "method", "==", "'POST'", ":", "\n", "        ", "data_json", "=", "json", ".", "loads", "(", "request", ".", "data", ")", "\n", "state_dict", ",", "player_id_dict", ",", "server_layout_name", ",", "algo", ",", "timestep", "=", "data_json", "[", "\"state\"", "]", ",", "data_json", "[", "\n", "\"npc_index\"", "]", ",", "data_json", "[", "\"layout_name\"", "]", ",", "data_json", "[", "\"algo\"", "]", ",", "data_json", "[", "\"timestep\"", "]", "\n", "player_id", "=", "int", "(", "player_id_dict", ")", "\n", "layout_name", "=", "NAME_TRANSLATION", "[", "server_layout_name", "]", "\n", "s0", ",", "s1", "=", "process_state", "(", "state_dict", ",", "layout_name", ")", "\n", "\n", "# print(s0.to_dict())", "\n", "# print(s1.to_dict())", "\n", "print", "(", "\"---\\n\"", ")", "\n", "\n", "if", "ARGS", ".", "replay_traj", ":", "\n", "            ", "if", "player_id", "==", "0", ":", "\n", "                ", "a", "=", "int", "(", "EGO_TRANSITIONS", ".", "acts", "[", "timestep", "]", "[", "0", "]", ")", "if", "timestep", "<", "len", "(", "\n", "EGO_TRANSITIONS", ".", "acts", ")", "else", "4", "\n", "", "elif", "player_id", "==", "1", ":", "\n", "                ", "a", "=", "int", "(", "ALT_TRANSITIONS", ".", "acts", "[", "timestep", "]", "[", "0", "]", ")", "if", "timestep", "<", "len", "(", "\n", "EGO_TRANSITIONS", ".", "acts", ")", "else", "4", "\n", "", "else", ":", "\n", "                ", "assert", "(", "False", ")", "\n", "", "", "else", ":", "\n", "            ", "if", "player_id", "==", "0", ":", "\n", "                ", "s", ",", "policy", "=", "s0", ",", "POLICY_P0", "\n", "", "elif", "player_id", "==", "1", ":", "\n", "                ", "s", ",", "policy", "=", "s1", ",", "POLICY_P1", "\n", "", "else", ":", "\n", "                ", "assert", "(", "False", ")", "\n", "", "a", "=", "get_prediction", "(", "s", ",", "policy", ",", "layout_name", ",", "algo", ")", "\n", "\n", "", "print", "(", "a", ")", "\n", "# print(algo)", "\n", "print", "(", "\"sending action \"", ",", "a", ")", "\n", "return", "jsonify", "(", "{", "'action'", ":", "a", "}", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Stanford-ILIAD_PantheonRL.overcooked-flask.app.updatemodel": [[129, 155], ["app.route", "json.loads", "print", "flask.jsonify", "os.makedirs", "app.convert_traj_to_simultaneous_transitions", "convert_traj_to_simultaneous_transitions.write_transition", "os.path.dirname", "open", "json.dump"], "function", ["home.repos.pwc.inspect_result.Stanford-ILIAD_PantheonRL.overcooked-flask.app.convert_traj_to_simultaneous_transitions", "home.repos.pwc.inspect_result.Stanford-ILIAD_PantheonRL.common.trajsaver.SimultaneousTransitions.write_transition"], ["", "", "@", "app", ".", "route", "(", "'/updatemodel'", ",", "methods", "=", "[", "'POST'", "]", ")", "\n", "def", "updatemodel", "(", ")", ":", "\n", "    ", "if", "request", ".", "method", "==", "'POST'", ":", "\n", "        ", "data_json", "=", "json", ".", "loads", "(", "request", ".", "data", ")", "\n", "traj_dict", ",", "traj_id", ",", "server_layout_name", ",", "algo", "=", "data_json", "[", "\"traj\"", "]", ",", "data_json", "[", "\n", "\"traj_id\"", "]", ",", "data_json", "[", "\"layout_name\"", "]", ",", "data_json", "[", "\"algo\"", "]", "\n", "layout_name", "=", "NAME_TRANSLATION", "[", "server_layout_name", "]", "\n", "print", "(", "traj_id", ")", "\n", "\n", "if", "ARGS", ".", "trajs_savepath", ":", "\n", "# Save trajectory (save this to keep reward information)", "\n", "            ", "filename", "=", "\"%s.json\"", "%", "(", "ARGS", ".", "trajs_savepath", ")", "\n", "os", ".", "makedirs", "(", "os", ".", "path", ".", "dirname", "(", "filename", ")", ",", "exist_ok", "=", "True", ")", "\n", "with", "open", "(", "filename", ",", "'w'", ")", "as", "f", ":", "\n", "                ", "json", ".", "dump", "(", "traj_dict", ",", "f", ")", "\n", "\n", "# Save transitions minimal (only state/action/done, no reward)", "\n", "", "simultaneous_transitions", "=", "convert_traj_to_simultaneous_transitions", "(", "\n", "traj_dict", ",", "layout_name", ")", "\n", "simultaneous_transitions", ".", "write_transition", "(", "ARGS", ".", "trajs_savepath", ")", "\n", "\n", "# Finetune model: todo", "\n", "\n", "", "REPLAY_TRAJ_IDX", "=", "0", "\n", "done", "=", "True", "\n", "return", "jsonify", "(", "{", "'status'", ":", "done", "}", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Stanford-ILIAD_PantheonRL.overcooked-flask.app.root": [[157, 160], ["app.route", "app.send_static_file"], "function", ["None"], ["", "", "@", "app", ".", "route", "(", "'/'", ")", "\n", "def", "root", "(", ")", ":", "\n", "    ", "return", "app", ".", "send_static_file", "(", "'index.html'", ")", "\n", "\n"]]}