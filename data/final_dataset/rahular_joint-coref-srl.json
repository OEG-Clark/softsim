{"home.repos.pwc.inspect_result.rahular_joint-coref-srl.None.error-analysis.readFile": [[32, 70], ["logging.debug", "open", "infile.read", "infile.read.split", "enumerate", "doc.split.split", "list_of_document_dicts.append", "i.split", "ast.literal_eval", "logging.debug", "logging.debug", "logging.debug", "len", "logging.debug", "len"], "function", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.scripts.convert_openie_to_conll.read"], ["def", "readFile", "(", "path", ")", ":", "\n", "\t", "\"\"\"\n\tpath: string path to file for a given domain\n\t\"\"\"", "\n", "shitty", "=", "0", "\n", "total", "=", "0", "\n", "list_of_document_dicts", "=", "[", "]", "\n", "with", "open", "(", "path", ",", "\"r\"", ")", "as", "infile", ":", "\n", "\t\t", "txt", "=", "infile", ".", "read", "(", ")", "\n", "documents", "=", "txt", ".", "split", "(", "os", ".", "linesep", "+", "os", ".", "linesep", ")", "\n", "for", "i", ",", "doc", "in", "enumerate", "(", "documents", ")", ":", "\n", "\t\t\t", "try", ":", "\n", "\t\t\t\t", "doc", "=", "doc", ".", "split", "(", "os", ".", "linesep", ")", "\n", "tokenized_doc", "=", "doc", "[", "1", "]", "\n", "missing_mentions", "=", "doc", "[", "2", "]", "\n", "predictions", "=", "doc", "[", "4", ":", "]", "\n", "predictions", "=", "[", "i", ".", "split", "(", "\"\\t\"", ")", "for", "i", "in", "predictions", "]", "\n", "checked_predictions", "=", "[", "p", "for", "p", "in", "predictions", "if", "len", "(", "p", ")", "==", "3", "]", "\n", "for", "p", "in", "predictions", ":", "\n", "\t\t\t\t\t", "total", "+=", "1", "\n", "if", "len", "(", "p", ")", "!=", "3", ":", "\n", "\t\t\t\t\t\t", "logging", ".", "debug", "(", "p", ")", "\n", "shitty", "+=", "1", "\n", "", "", "docdict", "=", "{", "\n", "\"tokenized_doc\"", ":", "ast", ".", "literal_eval", "(", "tokenized_doc", ")", ",", "\n", "\"missing\"", ":", "missing_mentions", ",", "\n", "\"predictions\"", ":", "checked_predictions", ",", "\n", "}", "\n", "list_of_document_dicts", ".", "append", "(", "docdict", ")", "\n", "", "except", ":", "\n", "\t\t\t\t", "logging", ".", "debug", "(", "\n", "f\"* problematic doc #{i}\"", "\n", ")", "# this should only be a blank document at the end of the file", "\n", "logging", ".", "debug", "(", "doc", ")", "\n", "logging", ".", "debug", "(", "\"#####\"", "*", "10", ")", "\n", "pass", "\n", "", "", "", "logging", ".", "debug", "(", "f\"* Bad predictions skipped: {shitty} / {total}\"", ")", "\n", "return", "list_of_document_dicts", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.None.error-analysis.countErrorsCoref": [[72, 162], ["collections.defaultdict", "collections.OrderedDict", "nltk.pos_tag", "zip", "error-analysis.countErrorsCoref.get_bucket_key"], "function", ["None"], ["", "def", "countErrorsCoref", "(", "list_of_document_dicts", ")", ":", "\n", "\t", "total_predictions", "=", "0", "\n", "total_correct", "=", "0", "\n", "total_errors", "=", "0", "\n", "\n", "pronoun_errors", "=", "0", "# PRP, PRP$", "\n", "noun_error", "=", "0", "# NN, NNS", "\n", "pnoun_error", "=", "0", "# NNP, NNPS", "\n", "verb_error", "=", "0", "# VB, VBD, VBG, VBN, VBP, VBZ", "\n", "wh_error", "=", "0", "# WDT, WRB", "\n", "poss_wh_error", "=", "0", "# WP, WP$", "\n", "mwe_error", "=", "0", "# MWE", "\n", "other_error", "=", "0", "\n", "other_error_tags", "=", "[", "]", "# hmmm what are these", "\n", "bad_pred_len_dict", "=", "defaultdict", "(", "int", ")", "\n", "\n", "for", "d", "in", "list_of_document_dicts", ":", "\n", "\t\t", "doc_errors", ",", "doc_correct", "=", "0", ",", "0", "\n", "word_tags", "=", "nltk", ".", "pos_tag", "(", "[", "word", "for", "word", ",", "*", "_", "in", "d", "[", "\"predictions\"", "]", "]", ")", "\n", "for", "(", "word", ",", "gold", ",", "pred", ")", ",", "(", "_", ",", "tag", ")", "in", "zip", "(", "d", "[", "\"predictions\"", "]", ",", "word_tags", ")", ":", "\n", "\t\t\t", "gold", "=", "gold", ".", "strip", "(", ")", "\n", "pred", "=", "pred", ".", "strip", "(", ")", "\n", "try", ":", "\n", "\t\t\t\t", "if", "\" \"", "in", "word", ":", "\n", "\t\t\t\t\t", "tag", "=", "\"MWE\"", "# multi word expression", "\n", "", "total_predictions", "+=", "1", "\n", "\n", "if", "pred", "in", "gold", ":", "\n", "\t\t\t\t\t", "doc_correct", "+=", "1", "\n", "", "else", ":", "\n", "\t\t\t\t\t", "doc_errors", "+=", "1", "\n", "bad_pred_len_dict", "[", "str", "(", "len", "(", "word", ")", ")", "]", "+=", "1", "\n", "\n", "if", "tag", "in", "[", "\"PRP\"", ",", "\"PRP$\"", "]", ":", "\n", "\t\t\t\t\t\t", "pronoun_errors", "+=", "1", "\n", "", "elif", "tag", "in", "[", "\"NN\"", ",", "\"NNS\"", "]", ":", "\n", "\t\t\t\t\t\t", "noun_error", "+=", "1", "\n", "", "elif", "tag", "in", "[", "\"NNP\"", ",", "\"NNPS\"", "]", ":", "\n", "\t\t\t\t\t\t", "pnoun_error", "+=", "1", "\n", "", "elif", "tag", "in", "[", "\"VB\"", ",", "\"VBD\"", ",", "\"VBG\"", ",", "\"VBN\"", ",", "\"VBP\"", ",", "\"VBZ\"", "]", ":", "\n", "\t\t\t\t\t\t", "verb_error", "+=", "1", "\n", "", "elif", "tag", "in", "[", "\"WDT\"", ",", "\"WRB\"", "]", ":", "\n", "\t\t\t\t\t\t", "wh_error", "+=", "1", "\n", "", "elif", "tag", "in", "[", "\"WP\"", ",", "\"WP$\"", "]", ":", "\n", "\t\t\t\t\t\t", "poss_wh_error", "+=", "1", "\n", "", "elif", "tag", "in", "[", "\"MWE\"", "]", ":", "\n", "\t\t\t\t\t\t", "mwe_error", "+=", "1", "\n", "", "else", ":", "\n", "\t\t\t\t\t\t", "other_error", "+=", "1", "\n", "other_error_tags", ".", "append", "(", "tag", ")", "\n", "", "", "", "except", "Exception", ":", "\n", "\t\t\t\t", "logging", ".", "debug", "(", "f\"* stupid problem word {word}\"", ")", "\n", "", "", "total_errors", "+=", "doc_errors", "\n", "total_correct", "+=", "doc_correct", "\n", "\n", "def", "get_bucket_key", "(", "l", ")", ":", "\n", "\t\t\t", "if", "l", "<", "100", ":", "return", "\"0-100\"", "\n", "if", "l", ">=", "100", "and", "l", "<", "200", ":", "return", "\"100-200\"", "\n", "if", "l", ">=", "200", "and", "l", "<", "300", ":", "return", "\"200-300\"", "\n", "if", "l", ">=", "300", "and", "l", "<", "400", ":", "return", "\"300-400\"", "\n", "if", "l", ">=", "400", "and", "l", "<", "500", ":", "return", "\"400-500\"", "\n", "if", "l", ">=", "500", "and", "l", "<", "600", ":", "return", "\"500-600\"", "\n", "if", "l", ">=", "600", "and", "l", "<", "700", ":", "return", "\"600-700\"", "\n", "if", "l", ">=", "700", "and", "l", "<", "800", ":", "return", "\"700-800\"", "\n", "if", "l", ">=", "800", "and", "l", "<", "900", ":", "return", "\"800-900\"", "\n", "if", "l", ">=", "900", "and", "l", "<", "1000", ":", "return", "\"900-1000\"", "\n", "else", ":", "return", "\"1000+\"", "\n", "\n", "", "bucket_key", "=", "get_bucket_key", "(", "len", "(", "d", "[", "\"tokenized_doc\"", "]", ")", ")", "\n", "if", "len", "(", "d", "[", "\"predictions\"", "]", ")", ">=", "5", ":", "\n", "\t\t\t", "len_errors", "[", "bucket_key", "]", ".", "append", "(", "100", "*", "(", "doc_correct", "/", "len", "(", "d", "[", "\"predictions\"", "]", ")", ")", ")", "\n", "\n", "", "", "assert", "total_correct", "+", "total_errors", "==", "total_predictions", "\n", "\n", "# The order is: [\"pronouns\", \"nouns\", \"proper-nouns\", \"verbs\", \"wh\", \"possessive-wh\", \"mwe\", \"other\"]", "\n", "pos_errors", "=", "[", "\n", "pronoun_errors", ",", "\n", "noun_error", ",", "\n", "pnoun_error", ",", "\n", "verb_error", ",", "\n", "wh_error", ",", "\n", "poss_wh_error", ",", "\n", "mwe_error", ",", "\n", "other_error", ",", "\n", "]", "\n", "all_errors", "=", "total_errors", "/", "total_predictions", "\n", "\n", "bad_pred_len_dict", "=", "OrderedDict", "(", "sorted", "(", "bad_pred_len_dict", ".", "items", "(", ")", ",", "key", "=", "lambda", "t", ":", "int", "(", "t", "[", "0", "]", ")", ")", ")", "\n", "\n", "return", "all_errors", ",", "pos_errors", ",", "bad_pred_len_dict", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.None.error-analysis.pretty_print": [[164, 175], ["sorted", "logging.info", "logging.info", "logging.info", "logging.info", "logging.info", "logging.info", "logging.info", "list", "numpy.array", "output.keys", "len_errors.items", "le.items", "numpy.mean", "numpy.std"], "function", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.dataset_readers.multiprocess_dataset_reader.logger.info", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.dataset_readers.multiprocess_dataset_reader.logger.info", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.dataset_readers.multiprocess_dataset_reader.logger.info", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.dataset_readers.multiprocess_dataset_reader.logger.info", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.dataset_readers.multiprocess_dataset_reader.logger.info", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.dataset_readers.multiprocess_dataset_reader.logger.info", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.dataset_readers.multiprocess_dataset_reader.logger.info"], ["", "def", "pretty_print", "(", "output", ")", ":", "\n", "\t", "keys", "=", "sorted", "(", "list", "(", "output", ".", "keys", "(", ")", ")", ")", "\n", "logging", ".", "info", "(", "\"All errors\"", ")", "\n", "logging", ".", "info", "(", "keys", ")", "\n", "logging", ".", "info", "(", "[", "\"{:.2f}\"", ".", "format", "(", "output", "[", "key", "]", "[", "\"all_errors\"", "]", "*", "100", ")", "for", "key", "in", "keys", "]", ")", "\n", "logging", ".", "info", "(", "\"POS errors\"", ")", "\n", "logging", ".", "info", "(", "[", "output", "[", "key", "]", "[", "\"pos_errors\"", "]", "for", "key", "in", "keys", "]", ")", "\n", "logging", ".", "info", "(", "[", "(", "key", ",", "output", "[", "key", "]", "[", "\"span_len_errors\"", "]", ")", "for", "key", "in", "keys", "]", ")", "\n", "le", "=", "{", "k", ":", "np", ".", "array", "(", "v", ")", "for", "k", ",", "v", "in", "len_errors", ".", "items", "(", ")", "}", "\n", "le", "=", "{", "k", ":", "(", "\"{:.2f}\"", ".", "format", "(", "np", ".", "mean", "(", "v", ")", ")", ",", "\"{:.2f}\"", ".", "format", "(", "np", ".", "std", "(", "v", ")", ")", ")", "for", "k", ",", "v", "in", "le", ".", "items", "(", ")", "}", "\n", "logging", ".", "info", "(", "le", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.None.error-analysis.main": [[177, 190], ["dict", "error-analysis.pretty_print", "error-analysis.readFile", "error-analysis.countErrorsCoref", "os.listdir", "os.path.join", "os.path.isfile", "f.startswith", "[].split", "os.path.join", "f.split"], "function", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.None.error-analysis.pretty_print", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.None.error-analysis.readFile", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.None.error-analysis.countErrorsCoref", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.dataset_readers.multiprocess_dataset_reader.QIterable.join", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.dataset_readers.multiprocess_dataset_reader.QIterable.join"], ["", "def", "main", "(", "dir", ")", ":", "\n", "\t", "output", "=", "dict", "(", ")", "\n", "coref_files", "=", "[", "\n", "f", "\n", "for", "f", "in", "os", ".", "listdir", "(", "dir", ")", "\n", "if", "os", ".", "path", ".", "isfile", "(", "os", ".", "path", ".", "join", "(", "dir", ",", "f", ")", ")", "and", "f", ".", "startswith", "(", "\"coref\"", ")", "\n", "]", "\n", "for", "f", "in", "coref_files", ":", "\n", "\t\t", "dataset", "=", "\"_\"", ".", "join", "(", "f", ".", "split", "(", "\".\"", ")", "[", "0", "]", ".", "split", "(", "\"_\"", ")", "[", "4", ":", "]", ")", "\n", "list_docs", "=", "readFile", "(", "os", ".", "path", ".", "join", "(", "dir", ",", "f", ")", ")", "\n", "all_errors", ",", "pos_errors", ",", "span_len_errors", "=", "countErrorsCoref", "(", "list_docs", ")", "\n", "output", "[", "dataset", "]", "=", "{", "\"all_errors\"", ":", "all_errors", ",", "\"pos_errors\"", ":", "pos_errors", ",", "\"span_len_errors\"", ":", "span_len_errors", "}", "\n", "", "pretty_print", "(", "output", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.None.predictor.init_model": [[28, 47], ["allennlp.data.Vocabulary.from_files", "allennlp.common.params.Params.from_file", "print", "allennlp.data.iterators.DataIterator.from_params", "Params.from_file.pop", "allennlp.models.model.Model.from_params", "torch.load", "model.to.load_state_dict", "model.to.to", "os.path.join", "os.path.join", "Params.from_file.get", "Params.from_file.pop", "os.path.join", "params.pop.duplicate"], "function", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.data.vocabulary.Vocabulary.from_files", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.params.Params.from_file", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.tokenizers.tokenizer.Tokenizer.from_params", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.params.Params.pop", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.tokenizers.tokenizer.Tokenizer.from_params", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.token_embedders.bert_token_embedder.PretrainedBertModel.load", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.learning_rate_schedulers.learning_rate_scheduler._PyTorchLearningRateSchedulerWrapper.load_state_dict", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.dataset_readers.multiprocess_dataset_reader.QIterable.join", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.dataset_readers.multiprocess_dataset_reader.QIterable.join", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.params.Params.get", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.params.Params.pop", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.dataset_readers.multiprocess_dataset_reader.QIterable.join", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.params.Params.duplicate"], ["def", "init_model", "(", "args", ")", ":", "\n", "    ", "vocab", "=", "Vocabulary", ".", "from_files", "(", "os", ".", "path", ".", "join", "(", "args", ".", "serialization_dir", ",", "\"./vocabulary\"", ")", ")", "\n", "params", "=", "Params", ".", "from_file", "(", "os", ".", "path", ".", "join", "(", "args", ".", "serialization_dir", ",", "\"./config.json\"", ")", ")", "\n", "print", "(", "params", ".", "get", "(", "\"iterators\"", ")", ")", "\n", "iterator_params", "=", "params", ".", "pop", "(", "\"iterators\"", ")", "[", "\"iterator_{}\"", ".", "format", "(", "args", ".", "task", ")", "]", "\n", "iterator", "=", "DataIterator", ".", "from_params", "(", "iterator_params", ")", "\n", "if", "args", ".", "model_type", "==", "\"pt\"", ":", "\n", "        ", "best_model", "=", "\"best_{}.th\"", ".", "format", "(", "args", ".", "task", ")", "\n", "", "else", ":", "\n", "        ", "best_model", "=", "\"best_finetuned_dgi_{}_{}.th\"", ".", "format", "(", "args", ".", "task", ",", "args", ".", "data", ")", "\n", "\n", "", "model_params", "=", "params", ".", "pop", "(", "\"model\"", ")", "\n", "model", "=", "Model", ".", "from_params", "(", "\n", "vocab", "=", "vocab", ",", "params", "=", "model_params", ".", "duplicate", "(", ")", ",", "regularizer", "=", "None", "\n", ")", "\n", "state", "=", "torch", ".", "load", "(", "os", ".", "path", ".", "join", "(", "args", ".", "serialization_dir", ",", "best_model", ")", ",", "map_location", "=", "device", ")", "\n", "model", ".", "load_state_dict", "(", "state", ")", "\n", "model", "=", "model", ".", "to", "(", "device", ")", "\n", "return", "iterator", ",", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.None.predictor.get_instances": [[49, 83], ["allennlp.common.params.Params.from_file", "[].get", "[].get.pop", "[].from_params", "[].from_params._read", "os.path.join", "instance.index_fields", "instances.append", "Params.from_file.get", "Params.from_file.pop"], "function", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.params.Params.from_file", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.params.Params.get", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.params.Params.pop", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.tokenizers.tokenizer.Tokenizer.from_params", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.coreference_resolution.conll.ConllCorefReader._read", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.dataset_readers.multiprocess_dataset_reader.QIterable.join", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.data.instance.Instance.index_fields", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.params.Params.get", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.params.Params.pop"], ["", "def", "get_instances", "(", "args", ",", "config", ",", "model", ")", ":", "\n", "    ", "reader_classes", "=", "{", "\n", "\"coref\"", ":", "{", "\n", "\"conll12\"", ":", "CorefConllReader", ",", "\n", "\"preco\"", ":", "CorefPrecoReader", ",", "\n", "\"wikicoref\"", ":", "CorefWikicorefReader", ",", "\n", "\"winobias\"", ":", "CorefWinobiasReader", ",", "\n", "\"phrase_detectives_g\"", ":", "CorefConllReader", ",", "\n", "\"phrase_detectives_w\"", ":", "CorefConllReader", ",", "\n", "}", ",", "\n", "\"srl\"", ":", "{", "\n", "\"conll12\"", ":", "SrlReader", ",", "\n", "\"conll05_w\"", ":", "SrlReader05", ",", "\n", "\"conll05_b\"", ":", "SrlReader05", ",", "\n", "\"wikibank\"", ":", "SrlWikibankReader", ",", "\n", "\"ewt\"", ":", "SrlWikibankReader", ",", "\n", "}", ",", "\n", "}", "\n", "params", "=", "Params", ".", "from_file", "(", "os", ".", "path", ".", "join", "(", "args", ".", "serialization_dir", ",", "\"./config.json\"", ")", ")", "\n", "task_name", "=", "\"task_{}\"", ".", "format", "(", "args", ".", "task", ")", "\n", "reader_params", "=", "params", ".", "get", "(", "task_name", ")", "[", "\"data_params\"", "]", ".", "get", "(", "\n", "\"validation_dataset_reader\"", ",", "None", "\n", ")", "\n", "if", "reader_params", "is", "None", ":", "\n", "        ", "reader_params", "=", "params", ".", "pop", "(", "task_name", ")", "[", "\"data_params\"", "]", "[", "\"dataset_reader\"", "]", "\n", "", "reader_params", ".", "pop", "(", "\"type\"", ")", "\n", "reader", "=", "reader_classes", "[", "args", ".", "task", "]", "[", "args", ".", "data", "]", ".", "from_params", "(", "reader_params", ")", "\n", "instances", "=", "[", "]", "\n", "for", "instance", "in", "reader", ".", "_read", "(", "\n", "config", "[", "\"{}_{}_test_data_path\"", ".", "format", "(", "args", ".", "task", ",", "args", ".", "data", ")", "]", "\n", ")", ":", "\n", "        ", "instance", ".", "index_fields", "(", "model", ".", "vocab", ")", "\n", "instances", ".", "append", "(", "instance", ")", "\n", "", "return", "instances", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.None.predictor.get_outputs": [[85, 111], ["iterator", "tqdm.tqdm", "model.forward", "model.decode", "allennlp.nn.util.move_to_device", "outputs.append", "zip", "len", "len", "len", "outputs.append"], "function", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.tqdm.Tqdm.tqdm", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.graph_util.graph_encoder_gat.GAT.forward", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.coreference_resolution.coref.CoreferenceResolver.decode", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.nn.util.move_to_device"], ["", "def", "get_outputs", "(", "args", ",", "instances", ",", "iterator", ",", "model", ")", ":", "\n", "    ", "generator", "=", "iterator", "(", "instances", "=", "instances", ",", "num_epochs", "=", "1", ")", "\n", "outputs", "=", "[", "]", "\n", "for", "batch", "in", "tqdm", "(", "generator", ")", ":", "\n", "        ", "if", "device", ".", "type", "!=", "\"cpu\"", ":", "\n", "            ", "batch", "=", "util", ".", "move_to_device", "(", "batch", ",", "device", ".", "index", ")", "\n", "", "output", "=", "model", ".", "forward", "(", "\n", "tensor_batch", "=", "batch", ",", "task_name", "=", "args", ".", "task", ",", "for_training", "=", "False", ",", "sample", "=", "False", "\n", ")", "\n", "output", "=", "model", ".", "decode", "(", "output", ",", "task_name", "=", "args", ".", "task", ")", "\n", "if", "args", ".", "task", "==", "\"coref\"", ":", "\n", "            ", "outputs", ".", "append", "(", "\n", "{", "\n", "\"document\"", ":", "output", "[", "\"document\"", "]", ",", "\n", "\"gold_clusters\"", ":", "output", "[", "\"gold_clusters\"", "]", ",", "\n", "\"predicted_clusters\"", ":", "output", "[", "\"clusters\"", "]", ",", "\n", "}", "\n", ")", "\n", "", "elif", "args", ".", "task", "==", "\"srl\"", ":", "\n", "            ", "words", ",", "gtags", ",", "ptags", "=", "output", "[", "\"words\"", "]", ",", "output", "[", "\"gold_tags\"", "]", ",", "output", "[", "\"tags\"", "]", "\n", "assert", "len", "(", "words", ")", "==", "len", "(", "gtags", ")", "==", "len", "(", "ptags", ")", "\n", "for", "w", ",", "gt", ",", "pt", "in", "zip", "(", "words", ",", "gtags", ",", "ptags", ")", ":", "\n", "                ", "outputs", ".", "append", "(", "\n", "{", "\"words\"", ":", "w", ",", "\"gold_tags\"", ":", "gt", ",", "\"predicted_tags\"", ":", "pt", "}", "\n", ")", "\n", "", "", "", "return", "outputs", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.None.predictor.process_coref": [[113, 165], ["text.extend", "str", "len", "sorted", "len", "sorted", "not_found.append", "triples.append"], "function", ["None"], ["", "def", "process_coref", "(", "data", ")", ":", "\n", "    ", "text", "=", "[", "]", "\n", "for", "d", "in", "data", ":", "\n", "        ", "doc", "=", "[", "str", "(", "token", ")", "for", "token", "in", "d", "[", "\"document\"", "]", "[", "0", "]", "]", "# just to be safe", "\n", "gcs", "=", "d", "[", "\"gold_clusters\"", "]", "[", "0", "]", "\n", "pcs", "=", "d", "[", "\"predicted_clusters\"", "]", "[", "0", "]", "\n", "gcs", "=", "(", "\n", "[", "sorted", "(", "cluster", ",", "key", "=", "lambda", "x", ":", "(", "x", "[", "0", "]", ",", "x", "[", "1", "]", ")", ")", "for", "cluster", "in", "gcs", "]", "\n", "if", "len", "(", "gcs", ")", ">", "0", "\n", "else", "[", "]", "\n", ")", "\n", "pcs", "=", "(", "\n", "[", "sorted", "(", "cluster", ",", "key", "=", "lambda", "x", ":", "(", "x", "[", "0", "]", ",", "x", "[", "1", "]", ")", ")", "for", "cluster", "in", "pcs", "]", "\n", "if", "len", "(", "pcs", ")", ">", "0", "\n", "else", "[", "]", "\n", ")", "\n", "\n", "# assume the first mention in every gold cluster to be the antecedent", "\n", "# every other mention in that cluster will be the coreferent mention", "\n", "# search for every coreferent mention in the predicted clusters", "\n", "triples", "=", "[", "]", "\n", "not_found", "=", "[", "]", "\n", "for", "gc", "in", "gcs", ":", "\n", "            ", "gant", "=", "gc", "[", "0", "]", "\n", "for", "gmention", "in", "gc", "[", "1", ":", "]", ":", "\n", "                ", "found", "=", "True", "\n", "for", "pc", "in", "pcs", ":", "\n", "                    ", "pant", "=", "pc", "[", "0", "]", "\n", "if", "gmention", "in", "pc", "[", "1", ":", "]", ":", "\n", "                        ", "triples", ".", "append", "(", "\n", "\"\\t\"", ".", "join", "(", "\n", "[", "\n", "\" \"", ".", "join", "(", "doc", "[", "gmention", "[", "0", "]", ":", "gmention", "[", "1", "]", "+", "1", "]", ")", ",", "\n", "\" \"", ".", "join", "(", "doc", "[", "gant", "[", "0", "]", ":", "gant", "[", "1", "]", "+", "1", "]", ")", ",", "\n", "\" \"", ".", "join", "(", "doc", "[", "pant", "[", "0", "]", ":", "pant", "[", "1", "]", "+", "1", "]", ")", ",", "\n", "]", "\n", ")", "\n", ")", "\n", "found", "=", "True", "\n", "break", "\n", "", "", "if", "not", "found", ":", "\n", "                    ", "not_found", ".", "append", "(", "gmention", ")", "\n", "", "", "", "text", ".", "extend", "(", "\n", "[", "\n", "\"'''\\n{}\\nMissing mentions: [{}]\\n'''\\n\"", ".", "format", "(", "\n", "doc", ",", "\", \"", ".", "join", "(", "not_found", ")", "\n", ")", ",", "\n", "\"\\n\"", ".", "join", "(", "triples", ")", ",", "\n", "\"\\n\\n\"", ",", "\n", "]", "\n", ")", "\n", "", "return", "text", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.None.predictor.process_srl": [[167, 179], ["text.append", "zip", "text.append", "text.append"], "function", ["None"], ["", "def", "process_srl", "(", "data", ")", ":", "\n", "    ", "text", "=", "[", "]", "\n", "for", "d", "in", "data", ":", "\n", "        ", "words", "=", "d", "[", "\"words\"", "]", "\n", "gtags", "=", "d", "[", "\"gold_tags\"", "]", "\n", "ptags", "=", "d", "[", "\"predicted_tags\"", "]", "\n", "doc", "=", "\" \"", ".", "join", "(", "words", ")", "\n", "text", ".", "append", "(", "\"'''\\n{}\\n'''\\n\"", ".", "format", "(", "doc", ")", ")", "\n", "for", "w", ",", "g", ",", "p", "in", "zip", "(", "words", ",", "gtags", ",", "ptags", ")", ":", "\n", "            ", "text", ".", "append", "(", "\"\\t\"", ".", "join", "(", "[", "w", ",", "g", ",", "p", "]", ")", "+", "\"\\n\"", ")", "\n", "", "text", ".", "append", "(", "\"\\n\"", ")", "\n", "", "return", "text", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.None.predictor.write": [[181, 190], ["open", "args.serialization_dir.split", "f.writelines", "predictor.process_coref", "f.writelines", "predictor.process_srl"], "function", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.None.predictor.process_coref", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.None.predictor.process_srl"], ["", "def", "write", "(", "args", ",", "outputs", ")", ":", "\n", "    ", "file_name", "=", "\"./outputs/{}/{}_{}_{}.txt\"", ".", "format", "(", "\n", "args", ".", "model_type", ",", "args", ".", "task", ",", "args", ".", "serialization_dir", ".", "split", "(", "\"/\"", ")", "[", "-", "1", "]", ",", "args", ".", "data", "\n", ")", "\n", "with", "open", "(", "file_name", ",", "\"w\"", ",", "encoding", "=", "\"utf-8\"", ")", "as", "f", ":", "\n", "        ", "if", "args", ".", "task", "==", "\"coref\"", ":", "\n", "            ", "f", ".", "writelines", "(", "process_coref", "(", "outputs", ")", ")", "\n", "", "elif", "args", ".", "task", "==", "\"srl\"", ":", "\n", "            ", "f", ".", "writelines", "(", "process_srl", "(", "outputs", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.None.predictor.init_args": [[192, 244], ["argparse.ArgumentParser", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.parse_args"], "function", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.commands.__init__.ArgumentParserWithDefaults.add_argument", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.commands.__init__.ArgumentParserWithDefaults.add_argument", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.commands.__init__.ArgumentParserWithDefaults.add_argument", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.commands.__init__.ArgumentParserWithDefaults.add_argument", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.commands.__init__.ArgumentParserWithDefaults.add_argument"], ["", "", "", "def", "init_args", "(", ")", ":", "\n", "# Parse arguments", "\n", "    ", "parser", "=", "argparse", ".", "ArgumentParser", "(", ")", "\n", "parser", ".", "add_argument", "(", "\n", "\"-c\"", ",", "\n", "\"--config_path\"", ",", "\n", "required", "=", "True", ",", "\n", "help", "=", "\"Fine-tuning configuration file.\"", ",", "\n", "type", "=", "str", ",", "\n", ")", "\n", "parser", ".", "add_argument", "(", "\n", "\"-s\"", ",", "\n", "\"--serialization_dir\"", ",", "\n", "required", "=", "True", ",", "\n", "help", "=", "\"Directory where the model is saved.\"", ",", "\n", "type", "=", "str", ",", "\n", ")", "\n", "parser", ".", "add_argument", "(", "\n", "\"-t\"", ",", "\n", "\"--task\"", ",", "\n", "required", "=", "True", ",", "\n", "help", "=", "\"Name of the task to predict.\"", ",", "\n", "choices", "=", "[", "\"coref\"", ",", "\"srl\"", "]", ",", "\n", "type", "=", "str", ",", "\n", ")", "\n", "parser", ".", "add_argument", "(", "\n", "\"-d\"", ",", "\n", "\"--data\"", ",", "\n", "required", "=", "True", ",", "\n", "help", "=", "\"Name of the dataset to load.\"", ",", "\n", "choices", "=", "[", "\n", "\"conll12\"", ",", "\n", "\"preco\"", ",", "\n", "\"phrase_detectives_g\"", ",", "\n", "\"phrase_detectives_w\"", ",", "\n", "\"wikicoref\"", ",", "\n", "\"winobias\"", ",", "\n", "\"conll05_w\"", ",", "\n", "\"conll05_b\"", ",", "\n", "\"ewt\"", ",", "\n", "]", ",", "\n", "type", "=", "str", ",", "\n", ")", "\n", "parser", ".", "add_argument", "(", "\n", "\"-m\"", ",", "\n", "\"--model_type\"", ",", "\n", "required", "=", "True", ",", "\n", "help", "=", "\"Model type to load (pre-trained or fine-tuned).\"", ",", "\n", "choices", "=", "[", "\"pt\"", ",", "\"ft\"", "]", ",", "\n", "type", "=", "str", ",", "\n", ")", "\n", "return", "parser", ".", "parse_args", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.None.predictor.main": [[246, 253], ["predictor.init_args", "hmtl.utils.set_config", "predictor.init_model", "predictor.get_instances", "predictor.get_outputs", "predictor.write"], "function", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.None.build_dgl_graph.init_args", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.utils.constants.set_config", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.utils.builder.init_model", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.None.predictor.get_instances", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.None.predictor.get_outputs", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.None.predictor.write"], ["", "def", "main", "(", ")", ":", "\n", "    ", "args", "=", "init_args", "(", ")", "\n", "config", "=", "set_config", "(", "args", ".", "config_path", ")", "\n", "iterator", ",", "model", "=", "init_model", "(", "args", ")", "\n", "instances", "=", "get_instances", "(", "args", ",", "config", ",", "model", ")", "\n", "outputs", "=", "get_outputs", "(", "args", ",", "instances", ",", "iterator", ",", "model", ")", "\n", "write", "(", "args", ",", "outputs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.None.evaluate.evaluate": [[56, 127], ["model.to.to", "torch.no_grad", "data_iterator", "logger.info", "tqdm.tqdm", "model.to.get_metrics", "float", "model.to.forward", "loss.item", "model.to.get_metrics", "float", "tqdm.tqdm.set_description", "data_iterator.get_num_batches", "allennlp.nn.util.move_to_device", "infered_dicts.append", "model.to.decode", "model.get_metrics.items"], "function", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.dataset_readers.multiprocess_dataset_reader.logger.info", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.tqdm.Tqdm.tqdm", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.coreference_resolution.coref.CoreferenceResolver.get_metrics", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.graph_util.graph_encoder_gat.GAT.forward", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.coreference_resolution.coref.CoreferenceResolver.get_metrics", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.iterators.data_iterator.DataIterator.get_num_batches", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.nn.util.move_to_device", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.coreference_resolution.coref.CoreferenceResolver.decode"], ["def", "evaluate", "(", "\n", "model", ":", "Model", ",", "\n", "instances", ":", "Iterable", "[", "Instance", "]", ",", "\n", "task_name", ":", "str", ",", "\n", "data_iterator", ":", "DataIterator", ",", "\n", "requires_inf_dict", ":", "bool", "=", "False", ",", "\n", "cuda_device", ":", "int", "=", "-", "1", ",", "\n", ")", "->", "Dict", "[", "str", ",", "Any", "]", ":", "\n", "    ", "\"\"\"\n    Evaluate a model for a particular task (usually after training).\n    \n    Parameters\n    ----------\n    model : ``allennlp.models.model.Model``, required\n        The model to evaluate\n    instances : ``Iterable[Instance]``, required\n        The (usually test) dataset on which to evalute the model.\n    task_name : ``str``, required\n        The name of the task on which evaluate the model.\n    data_iterator : ``DataIterator``\n        Iterator that go through the dataset.\n    cuda_device : ``int``\n        Cuda device to use.\n        \n    Returns\n    -------\n    metrics :  ``Dict[str, Any]``\n        A dictionary containing the metrics on the evaluated dataset.\n    \"\"\"", "\n", "if", "run_on_gpu", ":", "\n", "        ", "model", "=", "model", ".", "to", "(", "cuda_device", ")", "\n", "\n", "", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "        ", "iterator", "=", "data_iterator", "(", "instances", ",", "num_epochs", "=", "1", ",", "shuffle", "=", "False", ")", "\n", "logger", ".", "info", "(", "\"Iterating over dataset\"", ")", "\n", "generator_tqdm", "=", "tqdm", ".", "tqdm", "(", "\n", "iterator", ",", "total", "=", "data_iterator", ".", "get_num_batches", "(", "instances", ")", "\n", ")", "\n", "\n", "eval_loss", "=", "0", "\n", "nb_batches", "=", "0", "\n", "infered_dicts", "=", "[", "]", "\n", "for", "batch", "in", "generator_tqdm", ":", "\n", "            ", "if", "run_on_gpu", ":", "\n", "                ", "batch", "=", "util", ".", "move_to_device", "(", "batch", ",", "cuda_device", ")", "\n", "", "nb_batches", "+=", "1", "\n", "\n", "eval_output_dict", "=", "model", ".", "forward", "(", "\n", "task_name", "=", "task_name", ",", "tensor_batch", "=", "batch", ",", "sample", "=", "False", "\n", ")", "\n", "loss", "=", "eval_output_dict", "[", "\"loss\"", "]", "\n", "eval_loss", "+=", "loss", ".", "item", "(", ")", "\n", "metrics", "=", "model", ".", "get_metrics", "(", "task_name", "=", "task_name", ")", "\n", "metrics", "[", "\"loss\"", "]", "=", "float", "(", "eval_loss", "/", "nb_batches", ")", "\n", "\n", "if", "requires_inf_dict", ":", "\n", "                ", "infered_dicts", ".", "append", "(", "\n", "model", ".", "decode", "(", "eval_output_dict", ",", "task_name", "=", "task_name", ")", "\n", ")", "\n", "\n", "", "description", "=", "(", "\n", "\", \"", ".", "join", "(", "\n", "[", "\"%s: %.2f\"", "%", "(", "name", ",", "value", ")", "for", "name", ",", "value", "in", "metrics", ".", "items", "(", ")", "]", "\n", ")", "\n", "+", "\" ||\"", "\n", ")", "\n", "generator_tqdm", ".", "set_description", "(", "description", ",", "refresh", "=", "False", ")", "\n", "\n", "", "metrics", "=", "model", ".", "get_metrics", "(", "task_name", "=", "task_name", ",", "reset", "=", "True", ",", "full", "=", "True", ")", "\n", "metrics", "[", "\"loss\"", "]", "=", "float", "(", "eval_loss", "/", "nb_batches", ")", "\n", "return", "metrics", ",", "infered_dicts", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.None.train.tasks_and_vocab_from_params": [[55, 117], ["itertools.chain", "datasets_for_vocab_creation.items", "logger.info", "allennlp.data.Vocabulary.from_params", "Vocabulary.from_params.save_to_files", "logger.info", "logger.info", "params.pop", "params.pop.pop", "params.pop.pop", "hmtl.tasks.Task.from_params", "task_list.append", "Task.from_params.load_data_from_params", "itertools.chain", "logger.info", "params.pop", "os.path.join", "os.path.join", "params.keys", "re.search"], "function", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.dataset_readers.multiprocess_dataset_reader.logger.info", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.tokenizers.tokenizer.Tokenizer.from_params", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.data.vocabulary.Vocabulary.save_to_files", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.dataset_readers.multiprocess_dataset_reader.logger.info", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.dataset_readers.multiprocess_dataset_reader.logger.info", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.params.Params.pop", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.params.Params.pop", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.params.Params.pop", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.tokenizers.tokenizer.Tokenizer.from_params", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.tasks.task.Task.load_data_from_params", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.dataset_readers.multiprocess_dataset_reader.logger.info", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.params.Params.pop", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.dataset_readers.multiprocess_dataset_reader.QIterable.join", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.dataset_readers.multiprocess_dataset_reader.QIterable.join", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.nn.beam_search.BeamSearch.search"], ["def", "tasks_and_vocab_from_params", "(", "\n", "params", ":", "Params", ",", "serialization_dir", ":", "str", "\n", ")", "->", "Tuple", "[", "List", "[", "Task", "]", ",", "Vocabulary", "]", ":", "\n", "    ", "\"\"\"\n    Load each of the tasks in the model from the ``params`` file\n    and load the datasets associated with each of these task.\n    Create the vocavulary from ``params`` using the concatenation of the ``datasets_for_vocab_creation``\n    from each of the task specific dataset.\n    \n    Parameters\n    ----------\n    params: ``Params``\n        A parameter object specifing an experiment.\n    serialization_dir: ``str``\n        Directory in which to save the model and its logs.\n    Returns\n    -------\n    task_list: ``List[Task]``\n        A list containing the tasks of the model to train.\n    vocab: ``Vocabulary``\n        The vocabulary fitted on the datasets_for_vocab_creation.\n    \"\"\"", "\n", "### Instantiate the different tasks ###", "\n", "task_list", "=", "[", "]", "\n", "instances_for_vocab_creation", "=", "itertools", ".", "chain", "(", ")", "\n", "datasets_for_vocab_creation", "=", "{", "}", "\n", "task_keys", "=", "[", "key", "for", "key", "in", "params", ".", "keys", "(", ")", "if", "re", ".", "search", "(", "\"^task_\"", ",", "key", ")", "]", "\n", "\n", "for", "key", "in", "task_keys", ":", "\n", "        ", "logger", ".", "info", "(", "\"Creating %s\"", ",", "key", ")", "\n", "task_params", "=", "params", ".", "pop", "(", "key", ")", "\n", "task_description", "=", "task_params", ".", "pop", "(", "\"task_description\"", ")", "\n", "task_data_params", "=", "task_params", ".", "pop", "(", "\"data_params\"", ")", "\n", "\n", "task", "=", "Task", ".", "from_params", "(", "params", "=", "task_description", ")", "\n", "task_list", ".", "append", "(", "task", ")", "\n", "\n", "task_instances_for_vocab", ",", "task_datasets_for_vocab", "=", "task", ".", "load_data_from_params", "(", "\n", "params", "=", "task_data_params", "\n", ")", "\n", "instances_for_vocab_creation", "=", "itertools", ".", "chain", "(", "\n", "instances_for_vocab_creation", ",", "task_instances_for_vocab", "\n", ")", "\n", "datasets_for_vocab_creation", "[", "task", ".", "_name", "]", "=", "task_datasets_for_vocab", "\n", "\n", "### Create and save the vocabulary ###", "\n", "", "for", "task_name", ",", "task_dataset_list", "in", "datasets_for_vocab_creation", ".", "items", "(", ")", ":", "\n", "        ", "logger", ".", "info", "(", "\n", "\"Creating a vocabulary using %s data from %s.\"", ",", "\n", "\", \"", ".", "join", "(", "task_dataset_list", ")", ",", "\n", "task_name", ",", "\n", ")", "\n", "\n", "", "logger", ".", "info", "(", "\"Fitting vocabulary from dataset\"", ")", "\n", "vocab", "=", "Vocabulary", ".", "from_params", "(", "\n", "params", ".", "pop", "(", "\"vocabulary\"", ",", "{", "}", ")", ",", "instances", "=", "instances_for_vocab_creation", "\n", ")", "\n", "\n", "vocab", ".", "save_to_files", "(", "os", ".", "path", ".", "join", "(", "serialization_dir", ",", "\"vocabulary\"", ")", ")", "\n", "logger", ".", "info", "(", "\"Vocabulary saved to %s\"", ",", "os", ".", "path", ".", "join", "(", "serialization_dir", ",", "\"vocabulary\"", ")", ")", "\n", "\n", "return", "task_list", ",", "vocab", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.None.train.train_model": [[119, 206], ["multi_task_trainer.train", "json.dumps", "logger.info", "logger.info", "logger.info", "os.path.join", "torch.load", "best_model.load_state_dict", "copy.deepcopy", "logger.info", "open", "metrics_file.write", "logger.info", "evaluate.evaluate", "test_metrics.items", "os.path.join"], "function", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.training.no_op_trainer.NoOpTrainer.train", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.dataset_readers.multiprocess_dataset_reader.logger.info", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.dataset_readers.multiprocess_dataset_reader.logger.info", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.dataset_readers.multiprocess_dataset_reader.logger.info", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.dataset_readers.multiprocess_dataset_reader.QIterable.join", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.token_embedders.bert_token_embedder.PretrainedBertModel.load", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.learning_rate_schedulers.learning_rate_scheduler._PyTorchLearningRateSchedulerWrapper.load_state_dict", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.dataset_readers.multiprocess_dataset_reader.logger.info", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.None.predictor.write", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.dataset_readers.multiprocess_dataset_reader.logger.info", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.training.util.evaluate", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.dataset_readers.multiprocess_dataset_reader.QIterable.join"], ["", "def", "train_model", "(", "\n", "multi_task_trainer", ":", "MultiTaskTrainer", ",", "recover", ":", "bool", "=", "False", "\n", ")", "->", "Dict", "[", "str", ",", "Any", "]", ":", "\n", "    ", "\"\"\"\n    Launching the training of the multi-task model.\n    \n\u00a0\u00a0\u00a0\u00a0Parameters\n\u00a0\u00a0\u00a0\u00a0----------\n    multi_task_trainer: ``MultiTaskTrainer``\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0A trainer (similar to allennlp.training.trainer.Trainer) that can handle multi-task training.\n    recover : ``bool``, optional (default=False)\n        If ``True``, we will try to recover a training run from an existing serialization\n        directory.  This is only intended for use when something actually crashed during the middle\n        of a run.  For continuing training a model on new data, see the ``fine-tune`` command.\n                \u00a0\u00a0\u00a0\u00a0\n\u00a0\u00a0\u00a0\u00a0Returns\n\u00a0\u00a0\u00a0\u00a0-------\n    metrics: ``Dict[str, Any]\n        The different metrics summarizing the training of the model.\n        It includes the validation and test (if necessary) metrics.\n    \"\"\"", "\n", "### Train the multi-task model ###", "\n", "metrics", "=", "multi_task_trainer", ".", "train", "(", "recover", "=", "recover", ")", "\n", "\n", "task_list", "=", "multi_task_trainer", ".", "_task_list", "\n", "serialization_dir", "=", "multi_task_trainer", ".", "_serialization_dir", "\n", "model", "=", "multi_task_trainer", ".", "_model", "\n", "\n", "### Evaluate the model on test data if necessary ###", "\n", "# This is a multi-task learning framework, the best validation metrics for one task are not necessarily", "\n", "# obtained from the same epoch for all the tasks, one epoch begin equal to N forward+backward passes,", "\n", "# where N is the total number of batches in all the training sets.", "\n", "# We evaluate each of the best model for each task (based on the validation metrics) for all the other tasks (which have a test set).", "\n", "for", "task", "in", "task_list", ":", "\n", "        ", "if", "not", "task", ".", "_evaluate_on_test", ":", "\n", "            ", "continue", "\n", "\n", "", "logger", ".", "info", "(", "\n", "\"Task %s will be evaluated using the best epoch weights.\"", ",", "task", ".", "_name", "\n", ")", "\n", "assert", "(", "\n", "task", ".", "_test_data", "is", "not", "None", "\n", ")", ",", "\"Task {} wants to be evaluated on test dataset but no there is no test data loaded.\"", ".", "format", "(", "\n", "task", ".", "_name", "\n", ")", "\n", "\n", "logger", ".", "info", "(", "\"Loading the best epoch weights for task %s\"", ",", "task", ".", "_name", ")", "\n", "best_model_state_path", "=", "os", ".", "path", ".", "join", "(", "\n", "serialization_dir", ",", "\"best_{}.th\"", ".", "format", "(", "task", ".", "_name", ")", "\n", ")", "\n", "best_model_state", "=", "torch", ".", "load", "(", "best_model_state_path", ")", "\n", "best_model", "=", "model", "\n", "best_model", ".", "load_state_dict", "(", "state_dict", "=", "best_model_state", ")", "\n", "\n", "test_metric_dict", "=", "{", "}", "\n", "\n", "for", "pair_task", "in", "task_list", ":", "\n", "            ", "if", "not", "pair_task", ".", "_evaluate_on_test", ":", "\n", "                ", "continue", "\n", "\n", "", "logger", ".", "info", "(", "\n", "\"Pair task %s is evaluated with the best model for %s\"", ",", "\n", "pair_task", ".", "_name", ",", "\n", "task", ".", "_name", ",", "\n", ")", "\n", "test_metric_dict", "[", "pair_task", ".", "_name", "]", "=", "{", "}", "\n", "test_metrics", ",", "_", "=", "evaluate", "(", "\n", "model", "=", "best_model", ",", "\n", "task_name", "=", "pair_task", ".", "_name", ",", "\n", "instances", "=", "pair_task", ".", "_test_data", ",", "\n", "data_iterator", "=", "pair_task", ".", "_data_iterator", ",", "\n", "cuda_device", "=", "multi_task_trainer", ".", "_cuda_device", ",", "\n", ")", "\n", "\n", "for", "metric_name", ",", "value", "in", "test_metrics", ".", "items", "(", ")", ":", "\n", "                ", "test_metric_dict", "[", "pair_task", ".", "_name", "]", "[", "metric_name", "]", "=", "value", "\n", "\n", "", "", "metrics", "[", "task", ".", "_name", "]", "[", "\"test\"", "]", "=", "deepcopy", "(", "test_metric_dict", ")", "\n", "logger", ".", "info", "(", "\"Finished evaluation of task %s.\"", ",", "task", ".", "_name", ")", "\n", "\n", "### Dump validation and possibly test metrics ###", "\n", "", "metrics_json", "=", "json", ".", "dumps", "(", "metrics", ",", "indent", "=", "2", ")", "\n", "with", "open", "(", "os", ".", "path", ".", "join", "(", "serialization_dir", ",", "\"metrics.json\"", ")", ",", "\"w\"", ")", "as", "metrics_file", ":", "\n", "        ", "metrics_file", ".", "write", "(", "metrics_json", ")", "\n", "", "logger", ".", "info", "(", "\"Metrics: %s\"", ",", "metrics_json", ")", "\n", "\n", "return", "metrics", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.None.dgi.Encoder.__init__": [[171, 186], ["torch.Module.__init__", "torch.PReLU", "torch.PReLU", "torch.Embedding", "torch.Embedding", "torch_geometric.nn.GCNConv", "len", "torch_geometric.nn.RGCNConv"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.graph_util.plot.GraphPlot.__init__"], ["    ", "def", "__init__", "(", "self", ",", "in_channels", ",", "hidden_channels", ",", "node_emb_size", ",", "encoder_type", "=", "\"GCN\"", ")", ":", "\n", "        ", "super", "(", "Encoder", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "enc_type", "=", "encoder_type", "\n", "if", "self", ".", "enc_type", "==", "\"GCN\"", ":", "\n", "            ", "self", ".", "conv", "=", "GCNConv", "(", "in_channels", ",", "hidden_channels", ",", "cached", "=", "False", ")", "\n", "", "elif", "self", ".", "enc_type", "==", "\"RGCN\"", ":", "\n", "            ", "self", ".", "conv", "=", "RGCNConv", "(", "\n", "in_channels", ",", "hidden_channels", ",", "num_relations", "=", "3", ",", "num_bases", "=", "2", "\n", ")", "\n", "", "self", ".", "prelu", "=", "nn", ".", "PReLU", "(", "hidden_channels", ")", "\n", "self", ".", "node_type_embedding_size", "=", "node_emb_size", "\n", "self", ".", "node_type_embeddings", "=", "nn", ".", "Embedding", "(", "\n", "len", "(", "K", ".", "id2node_type", ")", ",", "self", ".", "node_type_embedding_size", "\n", ")", "\n", "self", ".", "node_type_embeddings", ".", "requires_grad", "=", "True", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.None.dgi.Encoder.forward": [[187, 207], ["datapoints_batch.span_reps.to", "datapoints_batch.type_features.to", "datapoints_batch.edge_attr.to", "dgi.Encoder.node_type_embeddings", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "datapoints_batch.edge_index.to", "dgi.Encoder.prelu", "type", "datapoints_batch.type_features.to.long", "dgi.Encoder.conv", "torch.cat.float", "torch.cat.float", "dgi.Encoder.conv", "torch.cat.float", "torch.cat.float"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "datapoints", ",", "*", "args", ",", "**", "kwargs", ")", ":", "\n", "        ", "if", "type", "(", "datapoints", ")", "is", "DataLoader", ":", "\n", "            ", "datapoints_batch", "=", "[", "d", "for", "d", "in", "datapoints", "]", "[", "0", "]", "\n", "", "else", ":", "\n", "            ", "datapoints_batch", "=", "datapoints", "\n", "\n", "", "span_reps", "=", "datapoints_batch", ".", "span_reps", ".", "to", "(", "device", ")", "\n", "node_type_features", "=", "datapoints_batch", ".", "type_features", ".", "to", "(", "device", ")", "\n", "edge_type_features", "=", "datapoints_batch", ".", "edge_attr", ".", "to", "(", "device", ")", "\n", "\n", "# embed node type", "\n", "node_type_embeddings", "=", "self", ".", "node_type_embeddings", "(", "node_type_features", ".", "long", "(", ")", ")", "\n", "xs", "=", "torch", ".", "cat", "(", "[", "span_reps", ",", "node_type_embeddings", "]", ",", "dim", "=", "-", "1", ")", "\n", "edge_indices", "=", "datapoints_batch", ".", "edge_index", ".", "to", "(", "device", ")", "\n", "if", "self", ".", "enc_type", "==", "\"GCN\"", ":", "\n", "            ", "x", "=", "self", ".", "conv", "(", "xs", ".", "float", "(", ")", ",", "edge_indices", ")", "\n", "", "elif", "self", ".", "enc_type", "==", "\"RGCN\"", ":", "\n", "            ", "x", "=", "self", ".", "conv", "(", "xs", ".", "float", "(", ")", ",", "edge_indices", ",", "edge_type_features", ")", "\n", "", "x", "=", "self", ".", "prelu", "(", "x", ")", "\n", "return", "x", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.None.dgi.set_device": [[46, 51], ["None"], "function", ["None"], ["def", "set_device", "(", "d", ")", ":", "\n", "# this is invoked from the fine-tuning code when reward models", "\n", "# need to be moved to the CPU", "\n", "    ", "global", "device", "\n", "device", "=", "d", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.None.dgi.graph2pytg": [[53, 118], ["torch.tensor", "torch.tensor", "zip", "list", "graph.nodes.items", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "graph.get_edges", "torch.tensor().permute", "torch.tensor().permute", "torch.tensor", "torch.tensor", "len", "torch_geometric.data.Data", "numpy.zeros_like", "logging.info", "list.append", "numpy.stack", "numpy.stack", "corrected_indices.append", "len", "torch.tensor", "torch.tensor", "torch.tensor.float", "torch.tensor.float", "torch.tensor", "torch.tensor", "logging.info", "corrected_indices.append", "corrected_indices.append", "len"], "function", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.utils.graph.Graph.get_edges", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.dataset_readers.multiprocess_dataset_reader.logger.info", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.dataset_readers.multiprocess_dataset_reader.logger.info"], ["", "def", "graph2pytg", "(", "graph", ")", ":", "\n", "    ", "invalid_nodes", "=", "0", "\n", "max", "=", "graph", ".", "max_span_width", "\n", "\n", "# TODO: need to deal with root node which has no spans or which have invalid spans", "\n", "# (like those which span across sentence boundaries)", "\n", "dummy_span", "=", "torch", ".", "tensor", "(", "np", ".", "zeros_like", "(", "graph", ".", "span_embeddings", "[", "0", "]", ")", ")", "\n", "span_lengths", "=", "[", "span", "[", "1", "]", "-", "span", "[", "0", "]", "if", "span", "else", "0", "for", "span", "in", "graph", ".", "node_spans", "]", "\n", "corrected_indices", "=", "[", "]", "\n", "for", "span", ",", "span_len", "in", "zip", "(", "graph", ".", "node_spans", ",", "span_lengths", ")", ":", "\n", "        ", "if", "not", "span", "or", "span", "==", "(", "-", "1", ",", "-", "1", ")", ":", "\n", "# control comes here only for the root node and hence", "\n", "# it is not counted as an invalid node", "\n", "            ", "corrected_indices", ".", "append", "(", "(", "0", ",", "0", ")", ")", "\n", "", "else", ":", "\n", "            ", "if", "span_len", "<", "max", ":", "\n", "                ", "start", ",", "end", "=", "span", "[", "0", "]", ",", "span", "[", "1", "]", "\n", "", "else", ":", "\n", "                ", "start", ",", "end", "=", "span", "[", "0", "]", ",", "span", "[", "0", "]", "+", "max", "-", "1", "\n", "", "if", "(", "start", ",", "end", ")", "not", "in", "graph", ".", "all_spans", ":", "\n", "                ", "logging", ".", "info", "(", "\"Invalid span: ({}, {})\"", ".", "format", "(", "start", ",", "end", ")", ")", "\n", "invalid_nodes", "+=", "1", "\n", "corrected_indices", ".", "append", "(", "(", "0", ",", "0", ")", ")", "\n", "", "else", ":", "\n", "                ", "corrected_indices", ".", "append", "(", "(", "start", ",", "end", ")", ")", "\n", "\n", "", "", "", "span_reps", "=", "[", "\n", "graph", ".", "span_embeddings_by_span", "[", "span", "]", "if", "span", "!=", "(", "0", ",", "0", ")", "else", "dummy_span", "\n", "for", "span", "in", "corrected_indices", "\n", "]", "\n", "if", "invalid_nodes", ">", "0", ":", "\n", "        ", "logging", ".", "info", "(", "\n", "\"Found {}/{} invalid nodes.\"", ".", "format", "(", "invalid_nodes", ",", "len", "(", "graph", ".", "node_spans", ")", ")", "\n", ")", "\n", "\n", "# get ys for validation task", "\n", "", "y", "=", "list", "(", ")", "\n", "for", "_", ",", "node", "in", "graph", ".", "nodes", ".", "items", "(", ")", ":", "\n", "        ", "if", "node", ".", "type", "in", "node_types", ":", "\n", "            ", "node_type", "=", "node_types", "[", "node", ".", "type", "]", "\n", "", "else", ":", "\n", "            ", "node_type", "=", "node_types", "[", "node", ".", "type", "]", "=", "len", "(", "node_types", ")", "\n", "", "y", ".", "append", "(", "node_type", ")", "\n", "\n", "# add node types and spans to node init", "\n", "", "type_features", "=", "[", "type", "for", "type", "in", "y", "]", "\n", "span_reps", "=", "torch", ".", "tensor", "(", "np", ".", "stack", "(", "span_reps", ")", ")", "\n", "type_features", "=", "torch", ".", "tensor", "(", "np", ".", "stack", "(", "type_features", ")", ")", "\n", "\n", "# get edges and edge attributes", "\n", "edges", ",", "edge_types", "=", "graph", ".", "get_edges", "(", ")", "\n", "edge_index", "=", "torch", ".", "tensor", "(", "edges", ")", ".", "permute", "(", "1", ",", "0", ")", "\n", "edge_attr", "=", "torch", ".", "tensor", "(", "[", "1", "if", "e", "==", "\"cor\"", "else", "0", "for", "e", "in", "edge_types", "]", ")", "\n", "\n", "# get and set num_nodes", "\n", "num_nodes", "=", "len", "(", "graph", ".", "nodes", ")", "\n", "\n", "return", "Data", "(", "\n", "span_reps", "=", "span_reps", ".", "float", "(", ")", ",", "\n", "type_features", "=", "type_features", ".", "float", "(", ")", ",", "\n", "edge_index", "=", "edge_index", ",", "\n", "edge_attr", "=", "edge_attr", ",", "\n", "y", "=", "torch", ".", "tensor", "(", "y", ")", ",", "\n", "graph", "=", "graph", ",", "\n", "num_nodes", "=", "num_nodes", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.None.dgi.perturb": [[121, 138], ["graph.metadata.get", "graph.perturb", "len", "int", "ratio.sum", "numpy.ceil", "numpy.array", "pow"], "function", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.params.Params.get", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.utils.graph.Graph.perturb"], ["", "def", "perturb", "(", "graph", ",", "*", "args", ",", "**", "kwargs", ")", ":", "\n", "    ", "perturbation_type", "=", "kwargs", "[", "\"perturbation_type\"", "]", "\n", "decay_factor", "=", "kwargs", "[", "\"decay_factor\"", "]", "*", "3", "\n", "ratio", "=", "(", "\n", "K", ".", "perturbation_ratio", "\n", "if", "perturbation_type", "==", "\"all\"", "\n", "else", "(", "np", ".", "array", "(", "K", ".", "perturbation_types", ")", "==", "perturbation_type", ")", ".", "astype", "(", "float", ")", "\n", ")", "# 1 for chosen, 0 for others", "\n", "\n", "# i.i.e perturbation probability for each node: on average we want k per sentence,", "\n", "# decayed as training progresses", "\n", "num_sent", "=", "graph", ".", "metadata", ".", "get", "(", "\"num_sent\"", ",", "len", "(", "graph", ".", "nodes", ")", ")", "\n", "graph", ".", "perturb", "(", "\n", "perturbation_probabilities", "=", "ratio", "/", "ratio", ".", "sum", "(", ")", ",", "\n", "no_perturb_actions", "=", "int", "(", "np", ".", "ceil", "(", "pow", "(", "0.8", ",", "decay_factor", ")", "*", "num_sent", ")", ")", ",", "\n", ")", "\n", "return", "graph", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.None.dgi.read_data": [[140, 168], ["hmtl.utils.get_config", "hmtl.utils.init_span_embedder", "hmtl.utils.get_reps", "open", "json.load", "hmtl.utils.Graph.build_graph", "torch_geometric.data.DataLoader", "os.path.join", "len", "data_list.append", "dgi.graph2pytg"], "function", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.utils.constants.get_config", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.utils.builder.init_span_embedder", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.utils.embs.get_reps", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.token_embedders.bert_token_embedder.PretrainedBertModel.load", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.utils.graph.Graph.build_graph", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.dataset_readers.multiprocess_dataset_reader.QIterable.join", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.None.dgi.graph2pytg"], ["", "def", "read_data", "(", "pretrained_serialization_dir", ",", "set_name", ")", ":", "\n", "    ", "config", "=", "get_config", "(", ")", "\n", "combined_path", "=", "\"graphs/gold_combined_{}.json\"", ".", "format", "(", "set_name", ")", "\n", "combined_graph_path", "=", "\"graphs/graphs_gold_combined_{}.json\"", ".", "format", "(", "set_name", ")", "\n", "with", "open", "(", "\n", "os", ".", "path", ".", "join", "(", "pretrained_serialization_dir", ",", "combined_graph_path", ")", ",", "\"r\"", "\n", ")", "as", "f", ":", "\n", "        ", "graph_data", "=", "json", ".", "load", "(", "f", ")", "\n", "\n", "", "init_span_embedder", "(", "pretrained_serialization_dir", ",", "combined_path", ")", "\n", "reps", "=", "get_reps", "(", "graph_data", ",", "set_name", ")", "\n", "\n", "data_list", "=", "[", "]", "\n", "for", "dockey", ",", "gdata", ",", "all_spans", ",", "span_embeddings", ",", "original_text", "in", "reps", ":", "\n", "        ", "graph", ",", "_", "=", "Graph", ".", "build_graph", "(", "\n", "dockey", ",", "\n", "gdata", ",", "\n", "max_span_width", "=", "config", "[", "\"max_span_width\"", "]", ",", "\n", "all_spans", "=", "all_spans", "[", "0", "]", ",", "\n", "span_embeddings", "=", "span_embeddings", "[", "0", "]", ",", "\n", "original_text", "=", "original_text", ",", "\n", ")", "\n", "if", "len", "(", "graph", ".", "nodes", ")", ">", "0", ":", "\n", "            ", "data_list", ".", "append", "(", "graph2pytg", "(", "graph", ")", ")", "\n", "", "num_features", "=", "(", "\n", "span_embeddings", "[", "0", "]", "[", "0", "]", ".", "shape", "[", "0", "]", "+", "config", "[", "\"node_type_embedding_size\"", "]", "\n", ")", "\n", "", "return", "DataLoader", "(", "data_list", ",", "batch_size", "=", "1", ")", ",", "num_features", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.None.dgi.corruption": [[209, 219], ["torch_geometric.data.DataLoader", "isinstance", "dgi.perturb", "dgi.graph2pytg", "pytg_perturbed_graphs.append"], "function", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.utils.graph.Graph.perturb", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.None.dgi.graph2pytg"], ["", "", "def", "corruption", "(", "datapoints", ",", "*", "args", ",", "**", "kwargs", ")", ":", "\n", "    ", "pytg_perturbed_graphs", "=", "[", "]", "\n", "graphs", "=", "datapoints", ".", "graph", "\n", "if", "not", "isinstance", "(", "graphs", ",", "list", ")", ":", "\n", "        ", "graphs", "=", "[", "graphs", "]", "\n", "", "for", "graph", "in", "graphs", ":", "\n", "        ", "perturbed_graph", "=", "perturb", "(", "graph", ",", "*", "args", ",", "**", "kwargs", ")", "\n", "pytg_perturbed_graph", "=", "graph2pytg", "(", "perturbed_graph", ")", "\n", "pytg_perturbed_graphs", ".", "append", "(", "pytg_perturbed_graph", ")", "\n", "", "return", "DataLoader", "(", "pytg_perturbed_graphs", ",", "batch_size", "=", "1", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.None.dgi.summary": [[221, 223], ["torch.sigmoid", "torch.sigmoid", "z.mean"], "function", ["None"], ["", "def", "summary", "(", "z", ",", "*", "args", ",", "**", "kwargs", ")", ":", "\n", "    ", "return", "torch", ".", "sigmoid", "(", "z", ".", "mean", "(", "dim", "=", "0", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.None.dgi.train": [[225, 240], ["model.float.train", "model.float.float", "enumerate", "data.to.to", "optimizer.zero_grad", "model.float.", "model.float.loss", "model.loss.backward", "optimizer.step", "model.loss.item", "len"], "function", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.training.no_op_trainer.NoOpTrainer.train", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.learning_rate_schedulers.slanted_triangular.SlantedTriangular.step"], ["", "def", "train", "(", "loader", ",", "model", ",", "optimizer", ",", "perturbation_type", "=", "None", ",", "decay_factor", "=", "-", "1", ")", ":", "\n", "    ", "model", "=", "model", ".", "train", "(", ")", "\n", "model", "=", "model", ".", "float", "(", ")", "\n", "cum_loss", "=", "0.0", "\n", "for", "_", ",", "data", "in", "enumerate", "(", "loader", ")", ":", "\n", "        ", "data", "=", "data", ".", "to", "(", "device", ")", "\n", "optimizer", ".", "zero_grad", "(", ")", "\n", "pos_z", ",", "neg_z", ",", "summary", "=", "model", "(", "\n", "data", ",", "perturbation_type", "=", "perturbation_type", ",", "decay_factor", "=", "epoch", "\n", ")", "\n", "loss", "=", "model", ".", "loss", "(", "pos_z", ",", "neg_z", ",", "summary", ")", "\n", "loss", ".", "backward", "(", ")", "\n", "optimizer", ".", "step", "(", ")", "\n", "cum_loss", "+=", "loss", ".", "item", "(", ")", "\n", "", "return", "cum_loss", "/", "len", "(", "loader", ".", "dataset", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.None.dgi.classify": [[242, 260], ["sklearn.linear_model.LogisticRegression().fit", "LogisticRegression().fit.predict", "sklearn.metrics.accuracy_score", "sklearn.metrics.f1_score", "train_z.detach().cpu().numpy", "train_y.detach().cpu().numpy", "test_z.detach().cpu().numpy", "test_y.detach().cpu().numpy", "test_y.detach().cpu().numpy", "sklearn.linear_model.LogisticRegression", "train_z.detach().cpu", "train_y.detach().cpu", "test_z.detach().cpu", "test_y.detach().cpu", "test_y.detach().cpu", "train_z.detach", "train_y.detach", "test_z.detach", "test_y.detach", "test_y.detach"], "function", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.predictors.constituency_parser.ConstituencyParserPredictor.predict"], ["", "def", "classify", "(", "\n", "train_z", ",", "\n", "train_y", ",", "\n", "test_z", ",", "\n", "test_y", ",", "\n", "f1_average", "=", "\"binary\"", ",", "\n", "solver", "=", "\"lbfgs\"", ",", "\n", "multi_class", "=", "\"auto\"", ",", "\n", "*", "args", ",", "\n", "**", "kwargs", "\n", ")", ":", "\n", "    ", "clf", "=", "LogisticRegression", "(", "\n", "solver", "=", "solver", ",", "multi_class", "=", "multi_class", ",", "*", "args", ",", "**", "kwargs", "\n", ")", ".", "fit", "(", "train_z", ".", "detach", "(", ")", ".", "cpu", "(", ")", ".", "numpy", "(", ")", ",", "train_y", ".", "detach", "(", ")", ".", "cpu", "(", ")", ".", "numpy", "(", ")", ")", "\n", "preds", "=", "clf", ".", "predict", "(", "test_z", ".", "detach", "(", ")", ".", "cpu", "(", ")", ".", "numpy", "(", ")", ")", "\n", "acc", "=", "accuracy_score", "(", "test_y", ".", "detach", "(", ")", ".", "cpu", "(", ")", ".", "numpy", "(", ")", ",", "preds", ")", "\n", "f1", "=", "f1_score", "(", "test_y", ".", "detach", "(", ")", ".", "cpu", "(", ")", ".", "numpy", "(", ")", ",", "preds", ",", "average", "=", "f1_average", ")", "\n", "return", "clf", ",", "acc", ",", "f1", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.None.dgi.test_nodes": [[262, 284], ["model.eval.eval", "enumerate", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "dgi.classify", "data.to.to", "model.eval.", "torch.cat.append", "torch.cat.append", "data.to.to", "model.eval.", "torch.cat.append", "torch.cat.append"], "function", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.None.dgi.classify"], ["", "def", "test_nodes", "(", "\n", "train_loader", ",", "dev_loader", ",", "model", ",", "perturbation_type", "=", "None", ",", "decay_factor", "=", "-", "1", "\n", ")", ":", "\n", "    ", "model", "=", "model", ".", "eval", "(", ")", "\n", "train_z", ",", "train_y", "=", "[", "]", ",", "[", "]", "\n", "dev_z", ",", "dev_y", "=", "[", "]", ",", "[", "]", "\n", "for", "_", ",", "data", "in", "enumerate", "(", "train_loader", ")", ":", "\n", "        ", "data", "=", "data", ".", "to", "(", "device", ")", "\n", "z", ",", "_", ",", "_", "=", "model", "(", "data", ",", "perturbation_type", "=", "perturbation_type", ",", "decay_factor", "=", "epoch", ")", "\n", "train_z", ".", "append", "(", "z", ")", "\n", "train_y", ".", "append", "(", "data", ".", "y", ")", "\n", "", "for", "data", "in", "dev_loader", ":", "\n", "        ", "data", "=", "data", ".", "to", "(", "device", ")", "\n", "z", ",", "_", ",", "_", "=", "model", "(", "data", ",", "perturbation_type", "=", "perturbation_type", ",", "decay_factor", "=", "epoch", ")", "\n", "dev_z", ".", "append", "(", "z", ")", "\n", "dev_y", ".", "append", "(", "data", ".", "y", ")", "\n", "", "train_z", "=", "torch", ".", "cat", "(", "train_z", ",", "dim", "=", "0", ")", "\n", "train_y", "=", "torch", ".", "cat", "(", "train_y", ",", "dim", "=", "0", ")", "\n", "dev_z", "=", "torch", ".", "cat", "(", "dev_z", ",", "dim", "=", "0", ")", "\n", "dev_y", "=", "torch", ".", "cat", "(", "dev_y", ",", "dim", "=", "0", ")", "\n", "\n", "return", "classify", "(", "train_z", ",", "train_y", ",", "dev_z", ",", "dev_y", ",", "f1_average", "=", "\"micro\"", ",", "max_iter", "=", "150", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.None.dgi.test_graphs": [[286, 318], ["torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "model.eval.eval", "enumerate", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "dgi.classify", "data.to.to", "model.eval.", "torch.cat.append", "torch.cat.append", "torch.cat.append", "torch.cat.append", "data.to.to", "model.eval.", "torch.cat.append", "torch.cat.append", "torch.cat.append", "torch.cat.append", "sump.unsqueeze", "summary().unsqueeze", "sump.unsqueeze", "summary().unsqueeze", "dgi.summary", "dgi.summary"], "function", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.None.dgi.classify", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.None.dgi.summary", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.None.dgi.summary"], ["", "def", "test_graphs", "(", "\n", "train_loader", ",", "dev_loader", ",", "model", ",", "perturbation_type", "=", "None", ",", "decay_factor", "=", "-", "1", "\n", ")", ":", "\n", "    ", "pos_class", "=", "torch", ".", "tensor", "(", "[", "1", "]", ")", "\n", "neg_class", "=", "torch", ".", "tensor", "(", "[", "0", "]", ")", "\n", "model", "=", "model", ".", "eval", "(", ")", "\n", "train_z", ",", "train_y", "=", "[", "]", ",", "[", "]", "\n", "dev_z", ",", "dev_y", "=", "[", "]", ",", "[", "]", "\n", "for", "_", ",", "data", "in", "enumerate", "(", "train_loader", ")", ":", "\n", "        ", "data", "=", "data", ".", "to", "(", "device", ")", "\n", "_", ",", "zn", ",", "sump", "=", "model", "(", "\n", "data", ",", "perturbation_type", "=", "perturbation_type", ",", "decay_factor", "=", "epoch", "\n", ")", "\n", "train_z", ".", "append", "(", "sump", ".", "unsqueeze", "(", "0", ")", ")", "\n", "train_y", ".", "append", "(", "pos_class", ")", "\n", "train_z", ".", "append", "(", "summary", "(", "zn", ")", ".", "unsqueeze", "(", "0", ")", ")", "\n", "train_y", ".", "append", "(", "neg_class", ")", "\n", "", "for", "data", "in", "dev_loader", ":", "\n", "        ", "data", "=", "data", ".", "to", "(", "device", ")", "\n", "_", ",", "zn", ",", "sump", "=", "model", "(", "\n", "data", ",", "perturbation_type", "=", "perturbation_type", ",", "decay_factor", "=", "epoch", "\n", ")", "\n", "dev_z", ".", "append", "(", "sump", ".", "unsqueeze", "(", "0", ")", ")", "\n", "dev_y", ".", "append", "(", "pos_class", ")", "\n", "dev_z", ".", "append", "(", "summary", "(", "zn", ")", ".", "unsqueeze", "(", "0", ")", ")", "\n", "dev_y", ".", "append", "(", "neg_class", ")", "\n", "", "train_z", "=", "torch", ".", "cat", "(", "train_z", ",", "dim", "=", "0", ")", "\n", "train_y", "=", "torch", ".", "cat", "(", "train_y", ",", "dim", "=", "0", ")", "\n", "dev_z", "=", "torch", ".", "cat", "(", "dev_z", ",", "dim", "=", "0", ")", "\n", "dev_y", "=", "torch", ".", "cat", "(", "dev_y", ",", "dim", "=", "0", ")", "\n", "\n", "return", "classify", "(", "train_z", ",", "train_y", ",", "dev_z", ",", "dev_y", ",", "max_iter", "=", "150", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.None.dgi.save": [[320, 332], ["hmtl.utils.get_config", "os.path.join", "torch.save", "torch.save", "pickle.dumps"], "function", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.utils.constants.get_config", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.dataset_readers.multiprocess_dataset_reader.QIterable.join", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.None.combine_data.save", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.None.combine_data.save"], ["", "def", "save", "(", "encoder_state", ",", "clf", ",", "optimizer_state", ",", "perturbation_type", ")", ":", "\n", "    ", "config", "=", "get_config", "(", ")", "\n", "save_path", "=", "os", ".", "path", ".", "join", "(", "\n", "config", "[", "\"reward_serialization_dir\"", "]", ",", "\n", "\"best_dgi_with_decay_{}_model.th\"", ".", "format", "(", "perturbation_type", ")", ",", "\n", ")", "\n", "blob", "=", "{", "\n", "\"encoder_state\"", ":", "encoder_state", ",", "\n", "\"classifier_state\"", ":", "pickle", ".", "dumps", "(", "clf", ")", ",", "\n", "\"training_state\"", ":", "optimizer_state", ",", "\n", "}", "\n", "torch", ".", "save", "(", "blob", ",", "save_path", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.None.dgi.init_args": [[334, 338], ["argparse.ArgumentParser", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.parse_args"], "function", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.commands.__init__.ArgumentParserWithDefaults.add_argument"], ["", "def", "init_args", "(", ")", ":", "\n", "    ", "parser", "=", "argparse", ".", "ArgumentParser", "(", "description", "=", "\"Train DGI reward models\"", ")", "\n", "parser", ".", "add_argument", "(", "\"config_path\"", ",", "help", "=", "\"Configuration file\"", ")", "\n", "return", "parser", ".", "parse_args", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.None.finetune.read_wiki_articles": [[64, 72], ["logger.info", "dict", "open", "tqdm.tqdm", "f.readlines", "nlp", "line.strip", "str", "uuid.uuid4"], "function", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.dataset_readers.multiprocess_dataset_reader.logger.info", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.tqdm.Tqdm.tqdm"], ["", "def", "read_wiki_articles", "(", "path", ")", ":", "\n", "    ", "logger", ".", "info", "(", "\"Reading wikipedia articles from disk\"", ")", "\n", "data", "=", "dict", "(", ")", "\n", "with", "open", "(", "path", ",", "\"r\"", ",", "encoding", "=", "\"utf-8\"", ")", "as", "f", ":", "\n", "        ", "for", "line", "in", "tqdm", "(", "f", ".", "readlines", "(", ")", ")", ":", "\n", "            ", "doc", "=", "nlp", "(", "line", ".", "strip", "(", ")", ")", "\n", "data", "[", "str", "(", "uuid4", "(", ")", ")", "]", "=", "doc", "\n", "", "", "return", "data", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.None.finetune.get_coref_instances": [[74, 85], ["dict", "logger.info", "tqdm.tqdm", "articles.items", "reader.text_to_instance", "reader.text_to_instance.index_fields"], "function", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.dataset_readers.multiprocess_dataset_reader.logger.info", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.tqdm.Tqdm.tqdm", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.coreference_resolution.conll.ConllCorefReader.text_to_instance", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.data.instance.Instance.index_fields"], ["", "def", "get_coref_instances", "(", "articles", ",", "reader", ",", "model", ")", ":", "\n", "    ", "ft_instances", "=", "dict", "(", ")", "\n", "logger", ".", "info", "(", "\"Making coref instances from wikipedia articles\"", ")", "\n", "for", "doc_key", ",", "doc", "in", "tqdm", "(", "articles", ".", "items", "(", ")", ")", ":", "\n", "        ", "instance", "=", "reader", ".", "text_to_instance", "(", "\n", "[", "[", "tok", ".", "text", "for", "tok", "in", "sent", "]", "for", "sent", "in", "doc", ".", "sents", "]", "\n", ")", "\n", "instance", ".", "fields", "[", "\"metadata\"", "]", ".", "metadata", "[", "\"doc_key\"", "]", "=", "doc_key", "\n", "instance", ".", "index_fields", "(", "model", ".", "vocab", ")", "\n", "ft_instances", "[", "doc_key", "]", "=", "instance", "\n", "", "return", "ft_instances", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.None.finetune.get_srl_instances": [[87, 106], ["collections.defaultdict", "logger.info", "tqdm.tqdm", "articles.items", "enumerate", "reader.text_to_instance", "reader.text_to_instance.index_fields", "ft_instances[].append", "allennlp.data.tokenizers.Token", "len", "str"], "function", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.dataset_readers.multiprocess_dataset_reader.logger.info", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.tqdm.Tqdm.tqdm", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.coreference_resolution.conll.ConllCorefReader.text_to_instance", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.data.instance.Instance.index_fields"], ["", "def", "get_srl_instances", "(", "articles", ",", "reader", ",", "model", ")", ":", "\n", "    ", "ft_instances", "=", "defaultdict", "(", "list", ")", "\n", "logger", ".", "info", "(", "\"Making SRL instances from wikipedia articles\"", ")", "\n", "for", "doc_key", ",", "doc", "in", "tqdm", "(", "articles", ".", "items", "(", ")", ")", ":", "\n", "        ", "for", "idx", ",", "sent", "in", "enumerate", "(", "doc", ".", "sents", ")", ":", "\n", "            ", "verbs", "=", "[", "\n", "(", "tok", ".", "text", ",", "tok", ".", "i", "-", "sent", ".", "start", ")", "for", "tok", "in", "sent", "if", "tok", ".", "pos_", "==", "\"VERB\"", "\n", "]", "\n", "for", "_", ",", "verb_loc", "in", "verbs", ":", "\n", "                ", "tokenized_sent", "=", "[", "Token", "(", "tok", ".", "text", ")", "for", "tok", "in", "sent", "]", "\n", "verb_indicator", "=", "[", "0", "]", "*", "len", "(", "tokenized_sent", ")", "\n", "verb_indicator", "[", "verb_loc", "]", "=", "1", "\n", "instance", "=", "reader", ".", "text_to_instance", "(", "tokenized_sent", ",", "verb_indicator", ")", "\n", "instance", ".", "fields", "[", "\"metadata\"", "]", ".", "metadata", "[", "\"sent_key\"", "]", "=", "\"{}-{}\"", ".", "format", "(", "\n", "doc_key", ",", "str", "(", "idx", ")", "\n", ")", "\n", "instance", ".", "index_fields", "(", "model", ".", "vocab", ")", "\n", "ft_instances", "[", "doc_key", "]", ".", "append", "(", "instance", ")", "\n", "", "", "", "return", "ft_instances", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.None.finetune.get_dev_instances": [[108, 141], ["allennlp.common.params.Params.from_file", "[].get", "[].get.pop", "[].from_params", "[].from_params._read", "os.path.join", "instance.index_fields", "dev_instances.append", "Params.from_file.get", "Params.from_file.pop"], "function", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.params.Params.from_file", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.params.Params.get", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.params.Params.pop", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.tokenizers.tokenizer.Tokenizer.from_params", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.coreference_resolution.conll.ConllCorefReader._read", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.dataset_readers.multiprocess_dataset_reader.QIterable.join", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.data.instance.Instance.index_fields", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.params.Params.get", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.params.Params.pop"], ["", "def", "get_dev_instances", "(", "ft_task", ",", "ft_data", ",", "model", ")", ":", "\n", "    ", "reader_classes", "=", "{", "\n", "\"coref\"", ":", "{", "\n", "\"conll12\"", ":", "CorefConllReader", ",", "\n", "\"preco\"", ":", "CorefPrecoReader", ",", "\n", "\"wikicoref\"", ":", "CorefWikicorefReader", ",", "\n", "\"winobias\"", ":", "CorefWinobiasReader", ",", "\n", "\"phrase_detectives_w\"", ":", "CorefConllReader", ",", "# works just fine", "\n", "\"phrase_detectives_g\"", ":", "CorefConllReader", ",", "# works just fine", "\n", "}", ",", "\n", "\"srl\"", ":", "{", "\n", "\"conll12\"", ":", "SrlReader", ",", "\n", "\"conll05\"", ":", "SrlReader05", ",", "\n", "\"wikibank\"", ":", "SrlWikibankReader", ",", "\n", "\"ewt\"", ":", "SrlWikibankReader", ",", "# works just fine", "\n", "}", ",", "\n", "}", "\n", "params", "=", "Params", ".", "from_file", "(", "os", ".", "path", ".", "join", "(", "args", ".", "serialization_dir", ",", "\"./config.json\"", ")", ")", "\n", "task_name", "=", "\"task_{}\"", ".", "format", "(", "ft_task", ")", "\n", "reader_params", "=", "params", ".", "get", "(", "task_name", ")", "[", "\"data_params\"", "]", ".", "get", "(", "\n", "\"validation_dataset_reader\"", ",", "None", "\n", ")", "\n", "if", "reader_params", "is", "None", ":", "\n", "        ", "reader_params", "=", "params", ".", "pop", "(", "task_name", ")", "[", "\"data_params\"", "]", "[", "\"dataset_reader\"", "]", "\n", "", "reader_params", ".", "pop", "(", "\"type\"", ")", "\n", "reader", "=", "reader_classes", "[", "ft_task", "]", "[", "ft_data", "]", ".", "from_params", "(", "reader_params", ")", "\n", "dev_instances", "=", "[", "]", "\n", "for", "instance", "in", "reader", ".", "_read", "(", "\n", "config", "[", "\"{}_{}_dev_data_path\"", ".", "format", "(", "ft_task", ",", "ft_data", ")", "]", "\n", ")", ":", "\n", "        ", "instance", ".", "index_fields", "(", "model", ".", "vocab", ")", "\n", "dev_instances", ".", "append", "(", "instance", ")", "\n", "", "return", "dev_instances", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.None.finetune.get_logits": [[143, 158], ["iterator", "model.forward", "model.decode", "outputs.append", "allennlp.nn.util.move_to_device", "next", "type", "model.parameters"], "function", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.graph_util.graph_encoder_gat.GAT.forward", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.coreference_resolution.coref.CoreferenceResolver.decode", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.nn.util.move_to_device"], ["", "def", "get_logits", "(", "instances", ",", "iterator", ",", "model", ",", "task", ",", "sample", ")", ":", "\n", "    ", "if", "task", "==", "\"srl\"", ":", "\n", "        ", "instances", "=", "[", "i", "for", "sublist", "in", "instances", "for", "i", "in", "sublist", "]", "\n", "", "generator", "=", "iterator", "(", "instances", "=", "instances", ",", "num_epochs", "=", "1", ")", "\n", "outputs", "=", "[", "]", "\n", "for", "batch", "in", "generator", ":", "\n", "# model is on gpu only if sample is true i.e. model == ft_model", "\n", "        ", "if", "sample", "and", "next", "(", "model", ".", "parameters", "(", ")", ")", ".", "is_cuda", "and", "type", "(", "GPU", ".", "index", ")", "==", "int", ":", "\n", "            ", "batch", "=", "util", ".", "move_to_device", "(", "batch", ",", "GPU", ".", "index", ")", "\n", "", "output", "=", "model", ".", "forward", "(", "\n", "tensor_batch", "=", "batch", ",", "task_name", "=", "task", ",", "for_training", "=", "True", ",", "sample", "=", "sample", "\n", ")", "\n", "output", "=", "model", ".", "decode", "(", "output", ",", "task_name", "=", "task", ")", "\n", "outputs", ".", "append", "(", "output", ")", "\n", "", "return", "outputs", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.None.finetune.combine_outputs": [[160, 194], ["dict", "typing.List", "typing.List", "collections.defaultdict", "zip", "[].append", "[].append", "co.get", "srl_tags[].append", "sent_key.split", "sent_key.split"], "function", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.params.Params.get"], ["", "def", "combine_outputs", "(", "coref_outputs", ",", "srl_outputs", ")", ":", "\n", "    ", "output_dict", "=", "dict", "(", ")", "\n", "for", "co", "in", "coref_outputs", ":", "\n", "        ", "doc_key", "=", "co", "[", "\"doc_key\"", "]", "[", "0", "]", "\n", "output_dict", "[", "doc_key", "]", "=", "{", "\n", "\"document\"", ":", "co", "[", "\"document\"", "]", "[", "0", "]", ",", "\n", "\"clusters\"", ":", "co", "[", "\"clusters\"", "]", ",", "\n", "\"all_spans\"", ":", "co", "[", "\"all_spans\"", "]", ",", "\n", "\"endpoint_span_embeddings\"", ":", "co", "[", "\"endpoint_span_embeddings\"", "]", ",", "\n", "\"attended_span_embeddings\"", ":", "co", "[", "\"attended_span_embeddings\"", "]", ",", "\n", "\"coref_log_likelihood\"", ":", "co", ".", "get", "(", "\"log_likelihood\"", ",", "None", ")", ",", "\n", "}", "\n", "output_dict", "[", "doc_key", "]", "[", "\"sentences\"", "]", ":", "List", "(", "str", ")", "=", "[", "]", "\n", "output_dict", "[", "doc_key", "]", "[", "\"srl_tags\"", "]", ":", "List", "(", "dict", ")", "=", "[", "]", "\n", "output_dict", "[", "doc_key", "]", "[", "\"srl_log_likelihood\"", "]", "=", "0", "\n", "\n", "", "sentences", ",", "srl_tags", "=", "{", "}", ",", "defaultdict", "(", "list", ")", "\n", "for", "so", "in", "srl_outputs", ":", "\n", "        ", "for", "words", ",", "verb", ",", "sent_key", ",", "tags", "in", "zip", "(", "\n", "so", "[", "\"words\"", "]", ",", "so", "[", "\"verb\"", "]", ",", "so", "[", "\"sent_key\"", "]", ",", "so", "[", "\"tags\"", "]", "\n", ")", ":", "\n", "            ", "sentences", "[", "sent_key", "]", "=", "words", "\n", "srl_tags", "[", "sent_key", "]", ".", "append", "(", "{", "\"verb\"", ":", "verb", ",", "\"tags\"", ":", "tags", "}", ")", "\n", "", "if", "\"log_likelihood\"", "in", "so", ":", "\n", "            ", "doc_key", "=", "\"-\"", ".", "join", "(", "sent_key", ".", "split", "(", "\"-\"", ")", "[", ":", "-", "1", "]", ")", "\n", "output_dict", "[", "doc_key", "]", "[", "\"srl_log_likelihood\"", "]", "=", "so", "[", "\"log_likelihood\"", "]", "\n", "\n", "", "", "for", "sent_key", "in", "sentences", ":", "\n", "        ", "sentence", "=", "sentences", "[", "sent_key", "]", "\n", "tags", "=", "srl_tags", "[", "sent_key", "]", "\n", "doc_key", "=", "\"-\"", ".", "join", "(", "sent_key", ".", "split", "(", "\"-\"", ")", "[", ":", "-", "1", "]", ")", "\n", "output_dict", "[", "doc_key", "]", "[", "\"sentences\"", "]", ".", "append", "(", "sentence", ")", "\n", "output_dict", "[", "doc_key", "]", "[", "\"srl_tags\"", "]", ".", "append", "(", "tags", ")", "\n", "", "return", "output_dict", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.None.finetune.init_reward_models": [[196, 219], ["dict", "finetune.init_reward_model", "ValueError", "finetune.init_reward_model", "logger.error", "logger.error"], "function", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.None.finetune.init_reward_model", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.None.finetune.init_reward_model"], ["", "def", "init_reward_models", "(", "\n", "serialization_dir", ",", "ft_task", "=", "\"coref\"", ",", "model_type", "=", "None", ",", "encoder_type", "=", "\"GCN\"", "\n", ")", ":", "\n", "    ", "clfs", "=", "dict", "(", ")", "\n", "if", "model_type", "==", "\"sep\"", ":", "\n", "        ", "ptypes", "=", "(", "\n", "K", ".", "perturbation_types", "\n", "if", "config", "[", "\"use_all_type_rewards\"", "]", "\n", "else", "[", "p", "for", "p", "in", "K", ".", "perturbation_types", "if", "ft_task", "in", "p", "]", "\n", ")", "\n", "for", "p", "in", "ptypes", ":", "\n", "            ", "try", ":", "\n", "                ", "clf", ",", "enc", "=", "init_reward_model", "(", "serialization_dir", ",", "p", ")", "\n", "clfs", "[", "p", "]", "=", "{", "\"clf\"", ":", "clf", ",", "\"enc\"", ":", "enc", "}", "\n", "", "except", "Exception", "as", "e", ":", "\n", "                ", "logger", ".", "error", "(", "\"Model for {} not found.\"", ".", "format", "(", "p", ")", ")", "\n", "logger", ".", "error", "(", "e", ")", "\n", "", "", "", "elif", "model_type", "==", "\"com\"", ":", "\n", "        ", "clf", ",", "enc", "=", "init_reward_model", "(", "serialization_dir", ",", "model_type", ")", "\n", "clfs", "[", "model_type", "]", "=", "{", "\"clf\"", ":", "clf", ",", "\"enc\"", ":", "enc", "}", "\n", "", "else", ":", "\n", "        ", "raise", "ValueError", "(", "\"Invalid `mode_type` option.\"", ")", "\n", "", "return", "clfs", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.None.finetune.init_reward_model": [[221, 239], ["hmtl.utils.get_config", "os.path.join", "logger.info", "torch_geometric.nn.DeepGraphInfomax", "torch.load", "torch_geometric.nn.DeepGraphInfomax.load_state_dict", "pickle.loads", "torch_geometric.nn.DeepGraphInfomax.eval().to", "dgi.Encoder", "torch_geometric.nn.DeepGraphInfomax.eval"], "function", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.utils.constants.get_config", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.dataset_readers.multiprocess_dataset_reader.QIterable.join", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.dataset_readers.multiprocess_dataset_reader.logger.info", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.token_embedders.bert_token_embedder.PretrainedBertModel.load", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.learning_rate_schedulers.learning_rate_scheduler._PyTorchLearningRateSchedulerWrapper.load_state_dict"], ["", "def", "init_reward_model", "(", "serialization_dir", ",", "model_name", ")", ":", "\n", "    ", "config", "=", "get_config", "(", ")", "\n", "mpath", "=", "os", ".", "path", ".", "join", "(", "\n", "serialization_dir", ",", "\"best_dgi_with_decay_{}_model.th\"", ".", "format", "(", "model_name", ")", "\n", ")", "\n", "logger", ".", "info", "(", "\"Loading reward models from {}\"", ".", "format", "(", "mpath", ")", ")", "\n", "\n", "num_features", "=", "config", "[", "\"span_rep_size\"", "]", "+", "config", "[", "\"node_type_embedding_size\"", "]", "\n", "G_encoder", "=", "DeepGraphInfomax", "(", "\n", "hidden_channels", "=", "512", ",", "\n", "encoder", "=", "DGI", ".", "Encoder", "(", "num_features", ",", "512", ",", "config", "[", "\"node_type_embedding_size\"", "]", ")", ",", "\n", "summary", "=", "DGI", ".", "summary", ",", "\n", "corruption", "=", "DGI", ".", "corruption", ",", "\n", ")", "\n", "state_dicts", "=", "torch", ".", "load", "(", "mpath", ",", "map_location", "=", "CPU", ")", "\n", "G_encoder", ".", "load_state_dict", "(", "state_dicts", "[", "\"encoder_state\"", "]", ")", "\n", "clf", "=", "pickle", ".", "loads", "(", "state_dicts", "[", "\"classifier_state\"", "]", ")", "\n", "return", "clf", ",", "G_encoder", ".", "eval", "(", ")", ".", "to", "(", "CPU", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.None.finetune.get_reward": [[241, 252], ["dgi.graph2pytg", "len", "[].predict_proba", "graph_rep.unsqueeze().detach().cpu().numpy", "graph_rep.unsqueeze().detach().cpu", "graph_rep.unsqueeze().detach", "graph_rep.unsqueeze"], "function", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.None.dgi.graph2pytg"], ["", "def", "get_reward", "(", "clfs", ",", "graph", ")", ":", "\n", "    ", "data", "=", "DGI", ".", "graph2pytg", "(", "graph", ")", "\n", "reward", "=", "0.0", "\n", "for", "key", "in", "clfs", ":", "\n", "        ", "_", ",", "_", ",", "graph_rep", "=", "clfs", "[", "key", "]", "[", "\"enc\"", "]", "(", "\n", "data", ",", "perturbation_type", "=", "\"coref_add_ant\"", ",", "decay_factor", "=", "0", "\n", ")", "# perturbation type does not matter. `coref_add_ant` is used because it is the fastest", "\n", "reward", "+=", "clfs", "[", "key", "]", "[", "\"clf\"", "]", ".", "predict_proba", "(", "\n", "graph_rep", ".", "unsqueeze", "(", "0", ")", ".", "detach", "(", ")", ".", "cpu", "(", ")", ".", "numpy", "(", ")", "\n", ")", "[", "0", "]", "[", "1", "]", "\n", "", "return", "reward", "/", "len", "(", "clfs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.None.finetune.get_optimizer": [[254, 266], ["allennlp.common.params.Params.from_file", "allennlp.training.optimizers.Optimizer.from_params", "os.path.join", "model.named_parameters", "copy.deepcopy"], "function", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.params.Params.from_file", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.tokenizers.tokenizer.Tokenizer.from_params", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.dataset_readers.multiprocess_dataset_reader.QIterable.join"], ["", "def", "get_optimizer", "(", "model", ",", "ft_task", ")", ":", "\n", "    ", "params", "=", "Params", ".", "from_file", "(", "os", ".", "path", ".", "join", "(", "args", ".", "serialization_dir", ",", "\"./config.json\"", ")", ")", "\n", "optimizer_params", "=", "params", "[", "\"multi_task_trainer\"", "]", "[", "\"optimizer\"", "]", "\n", "parameters_to_train", "=", "[", "\n", "(", "n", ",", "p", ")", "for", "n", ",", "p", "in", "model", ".", "named_parameters", "(", ")", "if", "p", ".", "requires_grad", "\n", "]", "\n", "if", "ft_task", "==", "\"coref\"", ":", "\n", "        ", "optimizer_params", "[", "\"lr\"", "]", "=", "3e-4", "\n", "", "optimizer", "=", "Optimizer", ".", "from_params", "(", "\n", "model_parameters", "=", "parameters_to_train", ",", "params", "=", "copy", ".", "deepcopy", "(", "optimizer_params", ")", "\n", ")", "\n", "return", "optimizer", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.None.finetune.pg_loop": [[268, 439], ["ft_model.to.state_dict", "finetune.get_optimizer", "list", "finetune.get_coref_instances", "finetune.get_srl_instances", "finetune.get_dev_instances", "collections.deque", "range", "wiki_articles.keys", "random.shuffle", "math.ceil", "range", "coref_model.eval.train", "ft_model.to.to", "coref_model.eval.train().to", "srl_model.train().to.eval", "time.time", "get_optimizer.zero_grad", "min", "finetune.get_logits", "finetune.get_logits", "finetune.combine_outputs", "combine_outputs.items", "loss.backward", "get_optimizer.step", "logger.info", "srl_model.train().to.train", "ValueError", "coref_model.eval.eval", "srl_model.train().to.train().to", "ValueError", "len", "ft_model.to.eval", "evaluate.evaluate", "ft_model.to.train", "logger.info", "len", "len", "torch.cat().squeeze().cpu().detach().numpy", "sum", "coref_model.eval.train", "ft_model.to.to", "ft_model.to.to", "logger.info", "copy.deepcopy", "torch.save", "ft_model.to.load_state_dict", "graph_util.json2graph", "hmtl.utils.Graph.build_graph", "finetune.get_reward", "logger.info", "rewards.append", "collections.deque.append", "log_likelihoods.append", "log_likelihoods.append", "len", "sum", "len", "loss.item", "srl_model.train().to.train", "ft_model.to.state_dict", "os.path.join", "copy.deepcopy", "torch.cat().squeeze().cpu().detach", "len", "logging.info", "torch.Tensor().item", "rewards.append", "collections.deque.append", "sum", "len", "time.time", "zip", "torch.cat().squeeze().cpu", "co[].cpu().detach().numpy", "torch.Tensor", "torch.cat().squeeze", "co[].cpu().detach", "torch.cat", "co[].cpu"], "function", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.learning_rate_schedulers.learning_rate_scheduler._PyTorchLearningRateSchedulerWrapper.state_dict", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.None.finetune.get_optimizer", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.None.finetune.get_coref_instances", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.None.finetune.get_srl_instances", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.None.finetune.get_dev_instances", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.training.no_op_trainer.NoOpTrainer.train", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.None.finetune.get_logits", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.None.finetune.get_logits", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.None.finetune.combine_outputs", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.learning_rate_schedulers.slanted_triangular.SlantedTriangular.step", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.dataset_readers.multiprocess_dataset_reader.logger.info", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.training.no_op_trainer.NoOpTrainer.train", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.training.util.evaluate", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.training.no_op_trainer.NoOpTrainer.train", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.dataset_readers.multiprocess_dataset_reader.logger.info", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.training.no_op_trainer.NoOpTrainer.train", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.dataset_readers.multiprocess_dataset_reader.logger.info", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.None.combine_data.save", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.learning_rate_schedulers.learning_rate_scheduler._PyTorchLearningRateSchedulerWrapper.load_state_dict", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.graph_util.output_to_graph.json2graph", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.utils.graph.Graph.build_graph", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.None.finetune.get_reward", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.dataset_readers.multiprocess_dataset_reader.logger.info", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.training.no_op_trainer.NoOpTrainer.train", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.learning_rate_schedulers.learning_rate_scheduler._PyTorchLearningRateSchedulerWrapper.state_dict", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.dataset_readers.multiprocess_dataset_reader.QIterable.join", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.dataset_readers.multiprocess_dataset_reader.logger.info"], ["", "def", "pg_loop", "(", "\n", "wiki_articles", ",", "\n", "artefacts", ",", "\n", "clfs", ",", "\n", "batch_size", "=", "5", ",", "\n", "epochs", "=", "1000", ",", "\n", "to_finetune", "=", "\"srl\"", ",", "\n", "finetune_dataset", "=", "\"conll12\"", ",", "\n", ")", ":", "\n", "    ", "coref_reader", ",", "coref_iterator", ",", "coref_model", "=", "(", "\n", "artefacts", "[", "\"readers\"", "]", "[", "\"coref\"", "]", ",", "\n", "artefacts", "[", "\"iterators\"", "]", "[", "\"coref\"", "]", ",", "\n", "artefacts", "[", "\"models\"", "]", "[", "\"coref\"", "]", ",", "\n", ")", "\n", "srl_reader", ",", "srl_iterator", ",", "srl_model", "=", "(", "\n", "artefacts", "[", "\"readers\"", "]", "[", "\"srl\"", "]", ",", "\n", "artefacts", "[", "\"iterators\"", "]", "[", "\"srl\"", "]", ",", "\n", "artefacts", "[", "\"models\"", "]", "[", "\"srl\"", "]", ",", "\n", ")", "\n", "if", "IS_MTL", ":", "\n", "        ", "if", "to_finetune", "==", "\"coref\"", ":", "\n", "            ", "ft_model", "=", "coref_model", ".", "train", "(", ")", "\n", "metric_key", "=", "\"coref_f1\"", "\n", "", "elif", "to_finetune", "==", "\"srl\"", ":", "\n", "            ", "ft_model", "=", "srl_model", ".", "train", "(", ")", "\n", "ft_model", ".", "_tagger_srl", ".", "finetuning", "=", "True", "\n", "metric_key", "=", "\"f1-measure-overall\"", "\n", "", "else", ":", "\n", "            ", "raise", "ValueError", "(", "\"Invalid value for parameter `to_finetune`.\"", ")", "\n", "", "if", "not", "IS_BIG_MODEL", ":", "\n", "            ", "ft_model", "=", "ft_model", ".", "to", "(", "GPU", ")", "\n", "", "", "else", ":", "\n", "        ", "if", "to_finetune", "==", "\"coref\"", ":", "\n", "            ", "ft_model", "=", "coref_model", "=", "coref_model", ".", "train", "(", ")", ".", "to", "(", "GPU", ")", "\n", "srl_model", "=", "srl_model", ".", "eval", "(", ")", "\n", "metric_key", "=", "\"coref_f1\"", "\n", "", "elif", "to_finetune", "==", "\"srl\"", ":", "\n", "            ", "coref_model", "=", "coref_model", ".", "eval", "(", ")", "\n", "ft_model", "=", "srl_model", "=", "srl_model", ".", "train", "(", ")", ".", "to", "(", "GPU", ")", "\n", "ft_model", ".", "_tagger_srl", ".", "finetuning", "=", "True", "\n", "metric_key", "=", "\"f1-measure-overall\"", "\n", "", "else", ":", "\n", "            ", "raise", "ValueError", "(", "\"Invalid value for parameter `to_finetune`.\"", ")", "\n", "\n", "", "", "best_state_dict", "=", "ft_model", ".", "state_dict", "(", ")", "\n", "optimizer", "=", "get_optimizer", "(", "ft_model", ",", "to_finetune", ")", "\n", "doc_keys", "=", "list", "(", "wiki_articles", ".", "keys", "(", ")", ")", "\n", "coref_ft_instances", "=", "get_coref_instances", "(", "wiki_articles", ",", "coref_reader", ",", "coref_model", ")", "\n", "srl_ft_instances", "=", "get_srl_instances", "(", "wiki_articles", ",", "srl_reader", ",", "srl_model", ")", "\n", "dev_instances", "=", "get_dev_instances", "(", "to_finetune", ",", "finetune_dataset", ",", "ft_model", ")", "\n", "dev_iterator", "=", "coref_iterator", "if", "to_finetune", "==", "\"coref\"", "else", "srl_iterator", "\n", "best_metric_val", ",", "best_state_dict", "=", "0", ",", "None", "\n", "historical_rewards", "=", "deque", "(", "maxlen", "=", "batch_size", "*", "100", ")", "\n", "for", "epoch", "in", "range", "(", "epochs", ")", ":", "\n", "        ", "shuffle", "(", "doc_keys", ")", "\n", "num_iters", "=", "math", ".", "ceil", "(", "len", "(", "doc_keys", ")", "/", "batch_size", ")", "\n", "for", "num_iter", "in", "range", "(", "num_iters", ")", ":", "\n", "            ", "if", "num_iter", "%", "1", "==", "0", ":", "\n", "                ", "ft_model", "=", "ft_model", ".", "eval", "(", ")", "\n", "if", "IS_BIG_MODEL", ":", "\n", "                    ", "ft_model", "=", "ft_model", ".", "to", "(", "GPU", ")", "\n", "", "metrics", ",", "_", "=", "evaluate", "(", "\n", "ft_model", ",", "\n", "dev_instances", ",", "\n", "to_finetune", ",", "\n", "dev_iterator", ",", "\n", "cuda_device", "=", "GPU", ".", "index", ",", "\n", ")", "\n", "if", "IS_BIG_MODEL", ":", "\n", "                    ", "ft_model", "=", "ft_model", ".", "to", "(", "CPU", ")", "\n", "", "ft_model", "=", "ft_model", ".", "train", "(", ")", "\n", "logger", ".", "info", "(", "\"{}: {}\"", ".", "format", "(", "metric_key", ",", "metrics", "[", "metric_key", "]", ")", ")", "\n", "if", "metrics", "[", "metric_key", "]", ">", "best_metric_val", ":", "\n", "                    ", "logger", ".", "info", "(", "\"New best! Woohoo!\"", ")", "\n", "best_metric_val", "=", "metrics", "[", "metric_key", "]", "\n", "best_state_dict", "=", "copy", ".", "deepcopy", "(", "ft_model", ".", "state_dict", "(", ")", ")", "\n", "torch", ".", "save", "(", "\n", "best_state_dict", ",", "\n", "os", ".", "path", ".", "join", "(", "\n", "args", ".", "serialization_dir", ",", "\n", "\"best_finetuned_dgi_{}_{}.th\"", ".", "format", "(", "\n", "to_finetune", ",", "finetune_dataset", "\n", ")", ",", "\n", ")", ",", "\n", ")", "\n", "", "else", ":", "\n", "                    ", "ft_model", ".", "load_state_dict", "(", "copy", ".", "deepcopy", "(", "best_state_dict", ")", ")", "\n", "", "", "start_time", "=", "time", ".", "time", "(", ")", "\n", "rewards", ",", "log_likelihoods", "=", "[", "]", ",", "[", "]", "\n", "optimizer", ".", "zero_grad", "(", ")", "\n", "no_verb_ctr", "=", "0", "\n", "start_idx", "=", "num_iter", "*", "batch_size", "\n", "end_idx", "=", "min", "(", "(", "num_iter", "+", "1", ")", "*", "batch_size", ",", "len", "(", "doc_keys", ")", ")", "\n", "coref_output", "=", "get_logits", "(", "\n", "[", "coref_ft_instances", "[", "key", "]", "for", "key", "in", "doc_keys", "[", "start_idx", ":", "end_idx", "]", "]", ",", "\n", "coref_iterator", ",", "\n", "coref_model", ",", "\n", "task", "=", "\"coref\"", ",", "\n", "sample", "=", "(", "to_finetune", "==", "\"coref\"", ")", ",", "\n", ")", "\n", "srl_output", "=", "get_logits", "(", "\n", "[", "srl_ft_instances", "[", "key", "]", "for", "key", "in", "doc_keys", "[", "start_idx", ":", "end_idx", "]", "]", ",", "\n", "srl_iterator", ",", "\n", "srl_model", ",", "\n", "task", "=", "\"srl\"", ",", "\n", "sample", "=", "(", "to_finetune", "==", "\"srl\"", ")", ",", "\n", ")", "\n", "combined_output", "=", "combine_outputs", "(", "coref_output", ",", "srl_output", ")", "\n", "assert", "len", "(", "combined_output", ")", "==", "end_idx", "-", "start_idx", "\n", "for", "dockey", ",", "co", "in", "combined_output", ".", "items", "(", ")", ":", "\n", "                ", "span_embeddings", "=", "(", "\n", "torch", ".", "cat", "(", "\n", "[", "\n", "co", "[", "\"endpoint_span_embeddings\"", "]", ",", "\n", "co", "[", "\"attended_span_embeddings\"", "]", ",", "\n", "]", ",", "\n", "dim", "=", "-", "1", ",", "\n", ")", "\n", ".", "squeeze", "(", ")", "\n", ".", "cpu", "(", ")", "\n", ".", "detach", "(", ")", "\n", ".", "numpy", "(", ")", "\n", ")", "\n", "try", ":", "\n", "                    ", "nodes", ",", "edges", ",", "_", "=", "json2graph", "(", "co", ")", "\n", "graph", ",", "_", "=", "Graph", ".", "build_graph", "(", "\n", "dockey", ",", "\n", "{", "\"nodes\"", ":", "nodes", ",", "\"edges\"", ":", "edges", "}", ",", "\n", "max_span_width", "=", "config", "[", "\"max_span_width\"", "]", ",", "\n", "all_spans", "=", "co", "[", "\"all_spans\"", "]", ".", "cpu", "(", ")", ".", "detach", "(", ")", ".", "numpy", "(", ")", "[", "0", "]", ",", "\n", "span_embeddings", "=", "span_embeddings", ",", "\n", "original_text", "=", "co", "[", "\"document\"", "]", ",", "\n", ")", "\n", "if", "len", "(", "graph", ".", "nodes", ")", "==", "0", ":", "\n", "                        ", "logging", ".", "info", "(", "\"Graph has 0 nodes. Skip fine-tuning...\"", ")", "\n", "continue", "\n", "", "reward", "=", "get_reward", "(", "clfs", ",", "graph", ")", "\n", "logger", ".", "info", "(", "reward", ")", "\n", "rewards", ".", "append", "(", "reward", ")", "\n", "historical_rewards", ".", "append", "(", "reward", ")", "\n", "", "except", "NameError", ":", "\n", "                    ", "no_verb_ctr", "+=", "1", "\n", "reward", "=", "torch", ".", "Tensor", "(", "[", "-", "1.0", "]", ")", ".", "item", "(", ")", "\n", "rewards", ".", "append", "(", "reward", ")", "\n", "historical_rewards", ".", "append", "(", "reward", ")", "\n", "", "if", "to_finetune", "==", "\"coref\"", ":", "\n", "                    ", "log_likelihoods", ".", "append", "(", "co", "[", "\"coref_log_likelihood\"", "]", ")", "\n", "", "else", ":", "\n", "                    ", "log_likelihoods", ".", "append", "(", "co", "[", "\"srl_log_likelihood\"", "]", ")", "\n", "", "", "baseline", "=", "(", "\n", "sum", "(", "historical_rewards", ")", "/", "len", "(", "historical_rewards", ")", "\n", "if", "len", "(", "historical_rewards", ")", ">", "batch_size", "\n", "else", "0", "\n", ")", "\n", "loss", "=", "sum", "(", "\n", "-", "1.0", "*", "(", "reward", "-", "baseline", ")", "*", "log_likelihood", "\n", "for", "reward", ",", "log_likelihood", "in", "zip", "(", "rewards", ",", "log_likelihoods", ")", "\n", ")", "/", "(", "end_idx", "-", "start_idx", ")", "\n", "loss", ".", "backward", "(", ")", "\n", "optimizer", ".", "step", "(", ")", "\n", "logger", ".", "info", "(", "\n", "\"Epoch {}({}/{}): no verbs: {}, loss = {:.2f}, avg. reward = {:.2f}, \\\n                baseline = {:.2f}, time: {:.2f}s\"", ".", "format", "(", "\n", "epoch", "+", "1", ",", "\n", "num_iter", "+", "1", ",", "\n", "num_iters", ",", "\n", "no_verb_ctr", ",", "\n", "loss", ".", "item", "(", ")", ",", "\n", "sum", "(", "rewards", ")", "/", "len", "(", "rewards", ")", ",", "\n", "baseline", ",", "\n", "time", ".", "time", "(", ")", "-", "start_time", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.None.finetune.init_args": [[443, 474], ["argparse.ArgumentParser", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.parse_args"], "function", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.commands.__init__.ArgumentParserWithDefaults.add_argument", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.commands.__init__.ArgumentParserWithDefaults.add_argument", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.commands.__init__.ArgumentParserWithDefaults.add_argument", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.commands.__init__.ArgumentParserWithDefaults.add_argument", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.commands.__init__.ArgumentParserWithDefaults.add_argument", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.commands.__init__.ArgumentParserWithDefaults.add_argument"], ["", "", "", "def", "init_args", "(", ")", ":", "\n", "    ", "parser", "=", "argparse", ".", "ArgumentParser", "(", "\n", "description", "=", "\"Finetune pre-trained resolvers with policy gradient\"", "\n", ")", "\n", "parser", ".", "add_argument", "(", "\"config_path\"", ",", "help", "=", "\"Configuration file\"", ")", "\n", "parser", ".", "add_argument", "(", "\n", "\"reward_type\"", ",", "help", "=", "\"Type of reward model to load\"", ",", "choices", "=", "[", "\"sep\"", ",", "\"com\"", "]", "\n", ")", "\n", "parser", ".", "add_argument", "(", "\n", "\"gencoder_type\"", ",", "help", "=", "\"Type of graph encoder\"", ",", "choices", "=", "[", "\"GCN\"", ",", "\"GAT\"", ",", "\"TAGCN\"", "]", "\n", ")", "\n", "parser", ".", "add_argument", "(", "\"ft_task\"", ",", "help", "=", "\"Task to finetune\"", ",", "choices", "=", "[", "\"coref\"", ",", "\"srl\"", "]", ")", "\n", "parser", ".", "add_argument", "(", "\n", "\"ft_data\"", ",", "\n", "help", "=", "\"Dataset to finetune on\"", ",", "\n", "choices", "=", "[", "\n", "\"conll12\"", ",", "\n", "\"preco\"", ",", "\n", "\"phrase_detectives_g\"", ",", "\n", "\"phrase_detectives_w\"", ",", "\n", "\"wikicoref\"", ",", "\n", "\"winobias\"", ",", "\n", "\"conll05\"", ",", "\n", "\"ewt\"", ",", "\n", "]", ",", "\n", ")", "\n", "parser", ".", "add_argument", "(", "\n", "\"serialization_dir\"", ",", "\n", "help", "=", "\"Path of the pre-trained model (finetuned model will be saved here)\"", ",", "\n", ")", "\n", "return", "parser", ".", "parse_args", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.None.finetune.init_single_models": [[476, 501], ["hmtl.utils.init_model", "hmtl.utils.init_model", "hmtl.utils.init_model", "hmtl.utils.init_model", "ValueError"], "function", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.utils.builder.init_model", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.utils.builder.init_model", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.utils.builder.init_model", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.utils.builder.init_model"], ["", "def", "init_single_models", "(", "serialization_dir", ",", "ft_task", ",", "device", ")", ":", "\n", "    ", "best_models_dir", "=", "{", "\n", "\"coref\"", ":", "\"./models/single/coref_conll_base\"", ",", "\n", "\"srl\"", ":", "\"./models/single/srl_conll_base\"", ",", "\n", "}", "\n", "if", "ft_task", "==", "\"coref\"", ":", "\n", "        ", "coref_reader", ",", "coref_iterator", ",", "coref_model", "=", "init_model", "(", "\n", "serialization_dir", ",", "device", ",", "task", "=", "\"coref\"", "\n", ")", "\n", "srl_reader", ",", "srl_iterator", ",", "srl_model", "=", "init_model", "(", "\n", "best_models_dir", "[", "\"srl\"", "]", ",", "device", ",", "task", "=", "\"srl\"", "\n", ")", "\n", "", "elif", "ft_task", "==", "\"srl\"", ":", "\n", "        ", "srl_reader", ",", "srl_iterator", ",", "srl_model", "=", "init_model", "(", "\n", "serialization_dir", ",", "device", ",", "task", "=", "\"srl\"", "\n", ")", "\n", "coref_reader", ",", "coref_iterator", ",", "coref_model", "=", "init_model", "(", "\n", "best_models_dir", "[", "\"coref\"", "]", ",", "device", ",", "task", "=", "\"coref\"", "\n", ")", "\n", "", "else", ":", "\n", "        ", "raise", "ValueError", "(", "\"Invalid ft_task.\"", ")", "\n", "", "return", "{", "\n", "\"readers\"", ":", "{", "\"coref\"", ":", "coref_reader", ",", "\"srl\"", ":", "srl_reader", "}", ",", "\n", "\"iterators\"", ":", "{", "\"coref\"", ":", "coref_iterator", ",", "\"srl\"", ":", "srl_iterator", "}", ",", "\n", "\"models\"", ":", "{", "\"coref\"", ":", "coref_model", ",", "\"srl\"", ":", "srl_model", "}", ",", "\n", "}", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.None.build_dgl_graph.GraphNN.__init__": [[493, 519], ["torch.nn.Module.__init__", "hmtl.utils.get_config", "dgl.DGLGraph", "torch.nn.Embedding", "torch.nn.Embedding", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Embedding", "torch.nn.Embedding", "collections.defaultdict", "len", "len"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.graph_util.plot.GraphPlot.__init__", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.utils.constants.get_config"], ["    ", "def", "__init__", "(", "self", ")", ":", "\n", "        ", "super", "(", "GraphNN", ",", "self", ")", ".", "__init__", "(", ")", "\n", "config", "=", "get_config", "(", ")", "\n", "self", ".", "g", "=", "dgl", ".", "DGLGraph", "(", ")", "\n", "\n", "# embedding and hidden layer sizes", "\n", "self", ".", "text_rep_size", "=", "config", "[", "\"span_rep_size\"", "]", "\n", "self", ".", "node_hidden_size", "=", "config", "[", "\"node_hidden_size\"", "]", "\n", "self", ".", "node_type_embedding_size", "=", "config", "[", "\"node_type_embedding_size\"", "]", "\n", "self", ".", "edge_type_embedding_size", "=", "config", "[", "\"edge_type_embedding_size\"", "]", "\n", "\n", "# embeddings and projection layer", "\n", "self", ".", "node_type_embeddings", "=", "nn", ".", "Embedding", "(", "\n", "len", "(", "K", ".", "id2node_type", ")", ",", "self", ".", "node_type_embedding_size", "\n", ")", "\n", "self", ".", "node_type_embeddings", ".", "requires_grad", "=", "True", "\n", "self", ".", "projection_layer_node_init", "=", "nn", ".", "Linear", "(", "\n", "self", ".", "node_type_embedding_size", "+", "self", ".", "text_rep_size", ",", "self", ".", "node_hidden_size", "\n", ")", "\n", "self", ".", "edge_type_embeddings", "=", "nn", ".", "Embedding", "(", "\n", "len", "(", "K", ".", "id2edge_type", ")", ",", "self", ".", "edge_type_embedding_size", "\n", ")", "\n", "self", ".", "edge_type_embeddings", ".", "requires_grad", "=", "True", "\n", "\n", "# edge list so far", "\n", "self", ".", "edge_tracker", "=", "defaultdict", "(", "list", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.None.build_dgl_graph.GraphNN.add_node": [[520, 545], ["build_dgl_graph.GraphNN.g.add_nodes", "build_dgl_graph.GraphNN.g.number_of_nodes", "torch.LongTensor().to", "torch.LongTensor().to", "torch.LongTensor().to", "torch.LongTensor().to", "build_dgl_graph.GraphNN.node_type_embeddings().to", "torch.Tensor().unsqueeze().to", "torch.Tensor().unsqueeze().to", "torch.Tensor().unsqueeze().to", "torch.Tensor().unsqueeze().to", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "build_dgl_graph.GraphNN.projection_layer_node_init", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "build_dgl_graph.GraphNN.node_type_embeddings", "torch.Tensor().unsqueeze", "torch.Tensor().unsqueeze", "torch.Tensor().unsqueeze", "torch.Tensor().unsqueeze", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor"], "methods", ["None"], ["", "def", "add_node", "(", "self", ",", "node_type", ",", "text_encoding", "=", "None", ")", ":", "\n", "# add node", "\n", "        ", "self", ".", "g", ".", "add_nodes", "(", "1", ")", "\n", "num_nodes", "=", "self", ".", "g", ".", "number_of_nodes", "(", ")", "\n", "\n", "# type embedding", "\n", "node_type_tensor", "=", "torch", ".", "LongTensor", "(", "[", "K", ".", "node_type2id", "[", "node_type", "]", "]", ")", ".", "to", "(", "\n", "device", "=", "device", "\n", ")", "\n", "type_embedding", "=", "self", ".", "node_type_embeddings", "(", "node_type_tensor", ")", ".", "to", "(", "device", "=", "device", ")", "\n", "\n", "if", "node_type", "!=", "\"ROOT\"", ":", "\n", "# convert encoding to tensor", "\n", "            ", "text_encoding", "=", "torch", ".", "Tensor", "(", "text_encoding", ")", ".", "unsqueeze", "(", "0", ")", ".", "to", "(", "device", ")", "\n", "\n", "init_vector", "=", "torch", ".", "cat", "(", "[", "type_embedding", ",", "text_encoding", "]", ",", "-", "1", ")", "\n", "# init_vector = type_embedding", "\n", "init_vector_transformed", "=", "self", ".", "projection_layer_node_init", "(", "init_vector", ")", "\n", "\n", "# initial node rep", "\n", "self", ".", "g", ".", "nodes", "[", "num_nodes", "-", "1", "]", ".", "data", "[", "\"hv\"", "]", "=", "init_vector_transformed", "\n", "", "else", ":", "\n", "# initial node rep", "\n", "            ", "self", ".", "g", ".", "nodes", "[", "num_nodes", "-", "1", "]", ".", "data", "[", "\"hv\"", "]", "=", "type_embedding", "\n", "", "return", "num_nodes", "-", "1", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.None.build_dgl_graph.GraphNN.add_edge": [[546, 642], ["torch.LongTensor().to", "torch.LongTensor().to", "torch.LongTensor().to", "torch.LongTensor().to", "build_dgl_graph.GraphNN.edge_type_embeddings().to", "build_dgl_graph.GraphNN.repeat", "build_dgl_graph.GraphNN.g.add_edges", "build_dgl_graph.GraphNN.edge_tracker[].append", "build_dgl_graph.GraphNN.edge_tracker[].append", "torch.LongTensor().to", "torch.LongTensor().to", "torch.LongTensor().to", "torch.LongTensor().to", "build_dgl_graph.GraphNN.edge_type_embeddings().to", "build_dgl_graph.GraphNN.g.add_edge", "build_dgl_graph.GraphNN.g.add_edge", "build_dgl_graph.GraphNN.edge_tracker[].append", "build_dgl_graph.GraphNN.edge_tracker[].append", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "build_dgl_graph.GraphNN.edge_type_embeddings", "build_dgl_graph.GraphNN.edge_tracker.keys", "build_dgl_graph.GraphNN.edge_tracker.keys", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "build_dgl_graph.GraphNN.edge_type_embeddings", "RuntimeError"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.utils.graph.Graph.add_edge", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.utils.graph.Graph.add_edge"], ["", "def", "add_edge", "(", "\n", "self", ",", "\n", "edge_type", ",", "\n", "source_id", ",", "\n", "destination_id", ",", "\n", "bidirectional", "=", "False", ",", "\n", "reverse_dir", "=", "True", ",", "\n", ")", ":", "\n", "\n", "# for corr edges, bidir makes sense", "\n", "        ", "if", "bidirectional", "or", "edge_type", "==", "\"cor\"", ":", "\n", "            ", "edge_type_tensor", "=", "torch", ".", "LongTensor", "(", "[", "K", ".", "edge_type2id", "[", "edge_type", "]", "]", ")", ".", "to", "(", "\n", "device", "=", "device", "\n", ")", "\n", "edge_type_embedding", "=", "self", ".", "edge_type_embeddings", "(", "edge_type_tensor", ")", ".", "to", "(", "\n", "device", "=", "device", "\n", ")", "\n", "# repeat along first dim because one edge per direction", "\n", "edge_type_embedding_bidir", "=", "edge_type_embedding", ".", "repeat", "(", "2", ",", "1", ")", "\n", "self", ".", "g", ".", "add_edges", "(", "[", "source_id", ",", "destination_id", "]", ",", "[", "destination_id", ",", "source_id", "]", ")", "\n", "# this if is for the strange cases when the cor egde is the same as an exisiting srl edge", "\n", "if", "(", "destination_id", ",", "source_id", ")", "in", "self", ".", "edge_tracker", ".", "keys", "(", ")", "or", "(", "\n", "source_id", ",", "\n", "destination_id", ",", "\n", ")", "in", "self", ".", "edge_tracker", ".", "keys", "(", ")", ":", "\n", "                ", "no_exisitng_edges", "=", "(", "\n", "self", ".", "g", ".", "edges", "[", "\n", "[", "source_id", ",", "destination_id", "]", ",", "[", "destination_id", ",", "source_id", "]", "\n", "]", "\n", ".", "data", "[", "\"he\"", "]", "\n", ".", "shape", "[", "0", "]", "\n", ")", "\n", "self", ".", "g", ".", "edges", "[", "\n", "[", "source_id", ",", "destination_id", "]", ",", "[", "destination_id", ",", "source_id", "]", "\n", "]", ".", "data", "[", "\"he\"", "]", "[", "no_exisitng_edges", "-", "2", ":", "]", "=", "edge_type_embedding_bidir", "\n", "", "else", ":", "\n", "                ", "try", ":", "\n", "                    ", "self", ".", "g", ".", "edges", "[", "\n", "[", "source_id", ",", "destination_id", "]", ",", "[", "destination_id", ",", "source_id", "]", "\n", "]", ".", "data", "[", "\"he\"", "]", "=", "edge_type_embedding_bidir", "\n", "", "except", "DGLError", "as", "e", ":", "\n", "                    ", "raise", "RuntimeError", "(", "\n", "\"Failed adding edge with source_id={}, destination_id={}\"", ".", "format", "(", "\n", "source_id", ",", "destination_id", "\n", ")", "\n", ")", "from", "e", "\n", "\n", "", "", "self", ".", "edge_tracker", "[", "(", "source_id", ",", "destination_id", ")", "]", ".", "append", "(", "edge_type", ")", "\n", "self", ".", "edge_tracker", "[", "(", "destination_id", ",", "source_id", ")", "]", ".", "append", "(", "edge_type", ")", "\n", "\n", "", "else", ":", "\n", "            ", "edge_type_tensor", "=", "torch", ".", "LongTensor", "(", "[", "K", ".", "edge_type2id", "[", "edge_type", "]", "]", ")", ".", "to", "(", "\n", "device", "=", "device", "\n", ")", "\n", "edge_type_embedding", "=", "self", ".", "edge_type_embeddings", "(", "edge_type_tensor", ")", ".", "to", "(", "\n", "device", "=", "device", "\n", ")", "\n", "\n", "# srl and root edges go from V -> and Root -> in original graph", "\n", "if", "reverse_dir", ":", "\n", "                ", "self", ".", "g", ".", "add_edge", "(", "destination_id", ",", "source_id", ")", "\n", "\"\"\"\n                # this is if for the strange cases when the cor egde is the same as an exisiting srl edge\n                if (destination_id, source_id) in self.edge_tracker.keys():\n                    # assume one dir is srl, the second is corr\n                    edge_type_tensor_srl = torch.LongTensor([edge_type2id['srl']]).to(device=device)\n                    edge_type_tensor_cor = torch.LongTensor([edge_type2id['cor']]).to(device=device)\n                    edge_type_embedding_srl = self.edge_type_embeddings(edge_type_tensor_srl).to(device=device)\n                    edge_type_embedding_cor = self.edge_type_embeddings(edge_type_tensor_cor).to(device=device)\n                    edge_type_embedding = torch.cat([edge_type_embedding_srl, edge_type_embedding_cor], 0)\n\n                    self.g.edges[destination_id, source_id].data['he'] = edge_type_embedding\n                else:\n                \"\"\"", "\n", "self", ".", "g", ".", "edges", "[", "destination_id", ",", "source_id", "]", ".", "data", "[", "\"he\"", "]", "=", "edge_type_embedding", "\n", "\n", "", "else", ":", "\n", "                ", "self", ".", "g", ".", "add_edge", "(", "source_id", ",", "destination_id", ")", "\n", "\"\"\"\n                 #this is if for the strange cases when the cor egde is the same as an exisiting srl edge\n                if  (source_id, destination_id) in self.edge_tracker.keys():\n                    #assume one dir is srl, the second is corr\n                    edge_type_tensor_srl = torch.LongTensor([edge_type2id['srl']]).to(device=device)\n                    edge_type_tensor_cor = torch.LongTensor([edge_type2id['cor']]).to(device=device)\n                    edge_type_embedding_srl = self.edge_type_embeddings(edge_type_tensor_srl).to(device=device)\n                    edge_type_embedding_cor = self.edge_type_embeddings(edge_type_tensor_cor).to(device=device)\n                    edge_type_embedding = torch.cat([edge_type_embedding_srl, edge_type_embedding_cor], 0)\n                    self.g.edges[source_id, destination_id].data['he'] = edge_type_embedding\n                else:\n                \"\"\"", "\n", "self", ".", "g", ".", "edges", "[", "source_id", ",", "destination_id", "]", ".", "data", "[", "\"he\"", "]", "=", "edge_type_embedding", "\n", "\n", "", "if", "reverse_dir", ":", "\n", "                ", "self", ".", "edge_tracker", "[", "(", "destination_id", ",", "source_id", ")", "]", ".", "append", "(", "edge_type", ")", "\n", "", "else", ":", "\n", "                ", "self", ".", "edge_tracker", "[", "(", "source_id", ",", "destination_id", ")", "]", ".", "append", "(", "edge_type", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.None.build_dgl_graph.GraphNN.get_encoded_text": [[643, 666], ["get_maximum_included_span.append", "get_maximum_included_span.append", "build_dgl_graph.array_in", "build_dgl_graph.get_maximum_included_span", "numpy.expand_dims", "numpy.array", "enumerate", "list", "list"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.None.build_dgl_graph.array_in", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.None.build_dgl_graph.get_maximum_included_span"], ["", "", "", "def", "get_encoded_text", "(", "self", ",", "doc_indices", ",", "all_spans", ",", "span_embeddings", ")", ":", "\n", "# clamp span len to 10", "\n", "        ", "if", "doc_indices", "[", "1", "]", "-", "doc_indices", "[", "0", "]", ">", "9", ":", "\n", "            ", "new_doc_indices", "=", "[", "]", "\n", "new_doc_indices", ".", "append", "(", "doc_indices", "[", "0", "]", ")", "\n", "new_doc_indices", ".", "append", "(", "doc_indices", "[", "0", "]", "+", "9", ")", "\n", "", "else", ":", "\n", "            ", "new_doc_indices", "=", "doc_indices", "\n", "\n", "", "if", "not", "array_in", "(", "np", ".", "array", "(", "new_doc_indices", ")", ",", "all_spans", "[", "0", "]", ")", ":", "\n", "            ", "new_doc_indices", "=", "get_maximum_included_span", "(", "new_doc_indices", ",", "all_spans", "[", "0", "]", ")", "\n", "\n", "", "index_in_all_spans", "=", "[", "\n", "index", "\n", "for", "index", ",", "item", "in", "enumerate", "(", "all_spans", "[", "0", "]", ")", "\n", "if", "list", "(", "item", ")", "==", "list", "(", "new_doc_indices", ")", "\n", "]", "[", "0", "]", "\n", "\n", "if", "span_embeddings", ".", "ndim", "==", "2", ":", "\n", "            ", "span_embeddings", "=", "np", ".", "expand_dims", "(", "span_embeddings", ",", "axis", "=", "0", ")", "\n", "\n", "", "text_encoded", "=", "span_embeddings", "[", "0", "]", "[", "index_in_all_spans", "]", "\n", "return", "text_encoded", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.None.build_dgl_graph.GraphNN.build_graph": [[667, 683], ["build_dgl_graph.GraphNN.add_edge", "build_dgl_graph.GraphNN.get_encoded_text", "build_dgl_graph.GraphNN.add_node", "build_dgl_graph.GraphNN.add_node"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.utils.graph.Graph.add_edge", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.None.build_dgl_graph.GraphNN.get_encoded_text", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.utils.graph.Graph.add_node", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.utils.graph.Graph.add_node"], ["", "def", "build_graph", "(", "self", ",", "nodes_list", ",", "edges_list", ",", "all_spans", ",", "span_embeddings", ")", ":", "\n", "        ", "for", "node", "in", "nodes_list", ":", "\n", "            ", "if", "node", "[", "\"type\"", "]", "!=", "\"ROOT\"", ":", "\n", "                ", "text_encoded", "=", "self", ".", "get_encoded_text", "(", "\n", "node", "[", "\"doc_indices\"", "]", ",", "all_spans", ",", "span_embeddings", "\n", ")", "\n", "self", ".", "add_node", "(", "node", "[", "\"type\"", "]", ",", "text_encoded", ")", "\n", "", "else", ":", "\n", "                ", "root_idx", "=", "self", ".", "add_node", "(", "node", "[", "\"type\"", "]", ")", "\n", "\n", "", "", "for", "edge", "in", "edges_list", ":", "\n", "            ", "source_id", "=", "edge", "[", "\"nids\"", "]", "[", "0", "]", "\n", "destination_id", "=", "edge", "[", "\"nids\"", "]", "[", "1", "]", "\n", "self", ".", "add_edge", "(", "edge", "[", "\"type\"", "]", ",", "source_id", ",", "destination_id", ")", "\n", "\n", "", "return", "root_idx", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.None.build_dgl_graph.ClfHead.__init__": [[686, 690], ["torch.nn.Module.__init__", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "int", "int"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.graph_util.plot.GraphPlot.__init__"], ["    ", "def", "__init__", "(", "self", ",", "input_dim", "=", "200", ")", ":", "\n", "        ", "super", "(", "ClfHead", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "lin1", "=", "nn", ".", "Linear", "(", "input_dim", ",", "int", "(", "input_dim", "/", "2", ")", ")", "\n", "self", ".", "lin2", "=", "nn", ".", "Linear", "(", "int", "(", "input_dim", "/", "2", ")", ",", "1", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.None.build_dgl_graph.ClfHead.forward": [[691, 693], ["build_dgl_graph.ClfHead.lin2", "torch.relu", "torch.relu", "build_dgl_graph.ClfHead.lin1"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "return", "self", ".", "lin2", "(", "F", ".", "relu", "(", "self", ".", "lin1", "(", "x", ")", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.None.build_dgl_graph.CombinedDataset.__init__": [[696, 710], ["list", "zip"], "methods", ["None"], ["    ", "def", "__init__", "(", "\n", "self", ",", "G_encoder", ",", "graph_tuples", ",", "neg_class", ",", "decay_factor", ",", "init_num_perturb_actions", "\n", ")", ":", "\n", "        ", "self", ".", "G_encoder", "=", "G_encoder", "\n", "(", "\n", "self", ".", "dockeys", ",", "\n", "self", ".", "graphs", ",", "\n", "self", ".", "all_spans", ",", "\n", "self", ".", "span_embeddings", ",", "\n", "self", ".", "original_text", ",", "\n", ")", "=", "list", "(", "zip", "(", "*", "graph_tuples", ")", ")", "\n", "self", ".", "neg_class", "=", "neg_class", "\n", "self", ".", "decay_factor", "=", "decay_factor", "\n", "self", ".", "init_num_perturb_actions", "=", "init_num_perturb_actions", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.None.build_dgl_graph.CombinedDataset.__getitem__": [[711, 747], ["int", "enumerate", "build_dgl_graph.get_graph_reps", "graph_x.unsqueeze.unsqueeze.unsqueeze", "build_dgl_graph.perturb", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.None.build_dgl_graph.get_graph_reps", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.utils.graph.Graph.perturb"], ["", "def", "__getitem__", "(", "self", ",", "idx", ")", ":", "\n", "        ", "real_idx", "=", "int", "(", "idx", "/", "2", ")", "\n", "graph", "=", "self", ".", "graphs", "[", "real_idx", "]", "\n", "# add not perturbed as default", "\n", "for", "id", ",", "node", "in", "enumerate", "(", "graph", "[", "\"nodes\"", "]", ")", ":", "\n", "            ", "graph", "[", "\"nodes\"", "]", "[", "id", "]", "[", "\"perturbed\"", "]", "=", "False", "\n", "", "all_spans", "=", "self", ".", "all_spans", "[", "real_idx", "]", "[", "0", "]", "\n", "span_embeddings", "=", "self", ".", "span_embeddings", "[", "real_idx", "]", "[", "0", "]", "\n", "original_text", "=", "self", ".", "original_text", "[", "real_idx", "]", "\n", "if", "idx", "%", "2", "==", "0", ":", "\n", "            ", "graph_y", "=", "1", "\n", "", "else", ":", "\n", "            ", "graph", "=", "perturb", "(", "\n", "self", ".", "dockeys", "[", "real_idx", "]", ",", "\n", "graph", ",", "\n", "all_spans", ",", "\n", "span_embeddings", ",", "\n", "original_text", ",", "\n", "self", ".", "decay_factor", ",", "\n", "self", ".", "init_num_perturb_actions", ",", "\n", ")", "\n", "graph_y", "=", "self", ".", "neg_class", "\n", "", "graph_x", ",", "nodes_X", "=", "get_graph_reps", "(", "\n", "self", ".", "G_encoder", ",", "graph", ",", "self", ".", "all_spans", "[", "real_idx", "]", ",", "span_embeddings", "\n", ")", "\n", "# get per node perturbation labels", "\n", "nodes", "=", "graph", "[", "\"nodes\"", "]", "\n", "if", "self", ".", "neg_class", "==", "-", "1", ":", "\n", "            ", "nodes_Y", "=", "torch", ".", "Tensor", "(", "[", "-", "1", "if", "n", "[", "\"perturbed\"", "]", "else", "1", "for", "n", "in", "nodes", "]", ")", "\n", "", "elif", "self", ".", "neg_class", "==", "0", ":", "\n", "            ", "nodes_Y", "=", "torch", ".", "Tensor", "(", "[", "0", "if", "n", "[", "\"perturbed\"", "]", "else", "1", "for", "n", "in", "nodes", "]", ")", "\n", "", "nodes_X", "=", "nodes_X", "\n", "# expand first dim of G", "\n", "graph_x", "=", "graph_x", ".", "unsqueeze", "(", "0", ")", "\n", "graph_y", "=", "graph_y", "\n", "return", "graph_x", ",", "graph_y", ",", "nodes_X", ",", "nodes_Y", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.None.build_dgl_graph.CombinedDataset.__len__": [[748, 750], ["len"], "methods", ["None"], ["", "def", "__len__", "(", "self", ")", ":", "\n", "        ", "return", "len", "(", "self", ".", "graphs", ")", "*", "2", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.None.build_dgl_graph.GraphDataset.__init__": [[753, 768], ["list", "zip"], "methods", ["None"], ["    ", "def", "__init__", "(", "\n", "self", ",", "G_encoder", ",", "graph_tuples", ",", "neg_class", ",", "decay_factor", ",", "init_num_perturb_actions", "\n", ")", ":", "\n", "        ", "self", ".", "G_encoder", "=", "G_encoder", "\n", "\n", "(", "\n", "self", ".", "dockeys", ",", "\n", "self", ".", "graphs", ",", "\n", "self", ".", "all_spans", ",", "\n", "self", ".", "span_embeddings", ",", "\n", "self", ".", "original_text", ",", "\n", ")", "=", "list", "(", "zip", "(", "*", "graph_tuples", ")", ")", "\n", "self", ".", "neg_class", "=", "neg_class", "\n", "self", ".", "decay_factor", "=", "decay_factor", "\n", "self", ".", "init_num_perturb_actions", "=", "init_num_perturb_actions", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.None.build_dgl_graph.GraphDataset.__getitem__": [[769, 798], ["int", "enumerate", "build_dgl_graph.get_graph_reps", "build_dgl_graph.perturb"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.None.build_dgl_graph.get_graph_reps", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.utils.graph.Graph.perturb"], ["", "def", "__getitem__", "(", "self", ",", "idx", ")", ":", "\n", "        ", "real_idx", "=", "int", "(", "idx", "/", "2", ")", "\n", "graph", "=", "self", ".", "graphs", "[", "real_idx", "]", "\n", "# add not perturbed as default", "\n", "for", "id", ",", "node", "in", "enumerate", "(", "graph", "[", "\"nodes\"", "]", ")", ":", "\n", "            ", "graph", "[", "\"nodes\"", "]", "[", "id", "]", "[", "\"perturbed\"", "]", "=", "False", "\n", "", "all_spans", "=", "self", ".", "all_spans", "[", "real_idx", "]", "[", "0", "]", "\n", "span_embeddings", "=", "self", ".", "span_embeddings", "[", "real_idx", "]", "[", "0", "]", "\n", "original_text", "=", "self", ".", "original_text", "[", "real_idx", "]", "\n", "if", "idx", "%", "2", "==", "0", ":", "\n", "            ", "graph_y", "=", "1", "\n", "", "else", ":", "\n", "            ", "graph", "=", "perturb", "(", "\n", "self", ".", "dockeys", "[", "real_idx", "]", ",", "\n", "graph", ",", "\n", "all_spans", ",", "\n", "span_embeddings", ",", "\n", "original_text", ",", "\n", "self", ".", "decay_factor", ",", "\n", "self", ".", "init_num_perturb_actions", ",", "\n", ")", "\n", "graph_y", "=", "self", ".", "neg_class", "\n", "\n", "", "graph_x", ",", "_", "=", "get_graph_reps", "(", "\n", "self", ".", "G_encoder", ",", "graph", ",", "self", ".", "all_spans", "[", "real_idx", "]", ",", "span_embeddings", "\n", ")", "\n", "graph_y", "=", "graph_y", "\n", "\n", "return", "graph_x", ",", "graph_y", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.None.build_dgl_graph.GraphDataset.__len__": [[799, 801], ["len"], "methods", ["None"], ["", "def", "__len__", "(", "self", ")", ":", "\n", "        ", "return", "len", "(", "self", ".", "graphs", ")", "*", "2", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.None.build_dgl_graph.NodesDataset.__init__": [[804, 818], ["list", "zip"], "methods", ["None"], ["    ", "def", "__init__", "(", "\n", "self", ",", "G_encoder", ",", "graph_tuples", ",", "neg_class", ",", "decay_factor", ",", "init_num_perturb_actions", "\n", ")", ":", "\n", "        ", "self", ".", "G_encoder", "=", "G_encoder", "\n", "(", "\n", "self", ".", "dockeys", ",", "\n", "self", ".", "graphs", ",", "\n", "self", ".", "all_spans", ",", "\n", "self", ".", "span_embeddings", ",", "\n", "self", ".", "original_text", ",", "\n", ")", "=", "list", "(", "zip", "(", "*", "graph_tuples", ")", ")", "\n", "self", ".", "neg_class", "=", "neg_class", "\n", "self", ".", "decay_factor", "=", "decay_factor", "\n", "self", ".", "init_num_perturb_actions", "=", "init_num_perturb_actions", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.None.build_dgl_graph.NodesDataset.__getitem__": [[819, 855], ["int", "enumerate", "build_dgl_graph.get_graph_reps", "build_dgl_graph.perturb", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.None.build_dgl_graph.get_graph_reps", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.utils.graph.Graph.perturb"], ["", "def", "__getitem__", "(", "self", ",", "idx", ")", ":", "\n", "        ", "real_idx", "=", "int", "(", "idx", "/", "2", ")", "\n", "graph", "=", "self", ".", "graphs", "[", "real_idx", "]", "\n", "# add not perturbed as default", "\n", "for", "id", ",", "node", "in", "enumerate", "(", "graph", "[", "\"nodes\"", "]", ")", ":", "\n", "            ", "graph", "[", "\"nodes\"", "]", "[", "id", "]", "[", "\"perturbed\"", "]", "=", "False", "\n", "\n", "", "all_spans", "=", "self", ".", "all_spans", "[", "real_idx", "]", "[", "0", "]", "\n", "span_embeddings", "=", "self", ".", "span_embeddings", "[", "real_idx", "]", "[", "0", "]", "\n", "original_text", "=", "self", ".", "original_text", "[", "real_idx", "]", "\n", "if", "idx", "%", "2", "==", "0", ":", "\n", "            ", "graph_y", "=", "1", "\n", "", "else", ":", "\n", "            ", "graph", "=", "perturb", "(", "\n", "self", ".", "dockeys", "[", "real_idx", "]", ",", "\n", "graph", ",", "\n", "all_spans", ",", "\n", "span_embeddings", ",", "\n", "original_text", ",", "\n", "self", ".", "decay_factor", ",", "\n", "self", ".", "init_num_perturb_actions", ",", "\n", ")", "\n", "graph_y", "=", "self", ".", "neg_class", "\n", "", "graph_x", ",", "nodes_X", "=", "get_graph_reps", "(", "\n", "self", ".", "G_encoder", ",", "graph", ",", "self", ".", "all_spans", "[", "real_idx", "]", ",", "span_embeddings", "\n", ")", "\n", "\n", "# get per node perturbation labels", "\n", "nodes", "=", "graph", "[", "\"nodes\"", "]", "\n", "\n", "if", "self", ".", "neg_class", "==", "-", "1", ":", "\n", "            ", "nodes_Y", "=", "torch", ".", "Tensor", "(", "[", "-", "1", "if", "n", "[", "\"perturbed\"", "]", "else", "1", "for", "n", "in", "nodes", "]", ")", "\n", "", "elif", "self", ".", "neg_class", "==", "0", ":", "\n", "            ", "nodes_Y", "=", "torch", ".", "Tensor", "(", "[", "0", "if", "n", "[", "\"perturbed\"", "]", "else", "1", "for", "n", "in", "nodes", "]", ")", "\n", "", "nodes_X", "=", "nodes_X", "\n", "return", "nodes_X", ",", "nodes_Y", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.None.build_dgl_graph.NodesDataset.__len__": [[856, 858], ["len"], "methods", ["None"], ["", "def", "__len__", "(", "self", ")", ":", "\n", "        ", "return", "len", "(", "self", ".", "graphs", ")", "*", "2", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.None.build_dgl_graph.VanillaDataset.__init__": [[861, 873], ["list", "sklearn.preprocessing.Normalizer", "zip"], "methods", ["None"], ["    ", "def", "__init__", "(", "self", ",", "G_encoder", ",", "x", ",", "y", ")", ":", "\n", "        ", "(", "\n", "self", ".", "dockeys", ",", "\n", "self", ".", "graphs", ",", "\n", "self", ".", "all_spans", ",", "\n", "self", ".", "span_embeddings", ",", "\n", "self", ".", "original_text", ",", "\n", ")", "=", "list", "(", "zip", "(", "*", "x", ")", ")", "\n", "self", ".", "G_encoder", "=", "G_encoder", "\n", "self", ".", "normalizer", "=", "Normalizer", "(", ")", "\n", "self", ".", "x", "=", "x", "\n", "self", ".", "y", "=", "y", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.None.build_dgl_graph.VanillaDataset.__getitem__": [[874, 884], ["build_dgl_graph.get_graph_reps", "numpy.squeeze", "build_dgl_graph.VanillaDataset.normalizer.transform", "graph_x.view().detach().numpy", "graph_x.view().detach", "graph_x.view"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.None.build_dgl_graph.get_graph_reps"], ["", "def", "__getitem__", "(", "self", ",", "idx", ")", ":", "\n", "        ", "graph_x", ",", "_", "=", "get_graph_reps", "(", "\n", "self", ".", "G_encoder", ",", "\n", "self", ".", "graphs", "[", "idx", "]", ",", "\n", "self", ".", "all_spans", "[", "idx", "]", ",", "\n", "self", ".", "span_embeddings", "[", "idx", "]", "[", "0", "]", ",", "\n", ")", "\n", "return", "(", "\n", "np", ".", "squeeze", "(", "self", ".", "normalizer", ".", "transform", "(", "graph_x", ".", "view", "(", "-", "1", ",", "1", ")", ".", "detach", "(", ")", ".", "numpy", "(", ")", ")", ")", ",", "\n", "self", ".", "y", "[", "idx", "]", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.None.build_dgl_graph.VanillaDataset.__len__": [[886, 888], ["len"], "methods", ["None"], ["", "def", "__len__", "(", "self", ")", ":", "\n", "        ", "return", "len", "(", "self", ".", "x", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.None.build_dgl_graph.set_device": [[53, 58], ["None"], "function", ["None"], ["def", "set_device", "(", "d", ")", ":", "\n", "# explicitly change `device`", "\n", "# can be used to setting device from other programs", "\n", "    ", "global", "device", "\n", "device", "=", "d", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.None.build_dgl_graph.my_collate": [[60, 103], ["type", "isinstance", "TypeError", "torch.cat", "torch.cat", "default_collate_err_msg_format.format", "torch.utils.data.get_worker_info", "torch.utils.data.get_worker_info", "sum", "elem.storage()._new_shared", "elem.new", "isinstance", "build_dgl_graph.my_collate", "torch.tensor", "torch.tensor", "isinstance", "x.numel", "elem.storage", "np_str_obj_array_pattern.search", "TypeError", "torch.as_tensor", "torch.as_tensor", "torch.tensor", "torch.tensor", "isinstance", "default_collate_err_msg_format.format", "torch.as_tensor", "torch.as_tensor", "isinstance", "build_dgl_graph.my_collate", "isinstance", "hasattr", "type.", "isinstance", "zip", "build_dgl_graph.my_collate", "build_dgl_graph.my_collate", "zip"], "function", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.None.build_dgl_graph.my_collate", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.nn.beam_search.BeamSearch.search", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.fields.text_field.TextField.as_tensor", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.fields.text_field.TextField.as_tensor", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.fields.text_field.TextField.as_tensor", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.fields.text_field.TextField.as_tensor", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.None.build_dgl_graph.my_collate", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.None.build_dgl_graph.my_collate", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.None.build_dgl_graph.my_collate"], ["", "def", "my_collate", "(", "batch", ")", ":", "\n", "    ", "r\"\"\"Puts each data field into a tensor with outer dimension batch size\"\"\"", "\n", "\n", "elem", "=", "batch", "[", "0", "]", "\n", "elem_type", "=", "type", "(", "elem", ")", "\n", "if", "isinstance", "(", "elem", ",", "torch", ".", "Tensor", ")", ":", "\n", "        ", "out", "=", "None", "\n", "if", "torch", ".", "utils", ".", "data", ".", "get_worker_info", "(", ")", "is", "not", "None", ":", "\n", "# If we're in a background process, concatenate directly into a", "\n", "# shared memory tensor to avoid an extra copy", "\n", "            ", "numel", "=", "sum", "(", "[", "x", ".", "numel", "(", ")", "for", "x", "in", "batch", "]", ")", "\n", "storage", "=", "elem", ".", "storage", "(", ")", ".", "_new_shared", "(", "numel", ")", "\n", "out", "=", "elem", ".", "new", "(", "storage", ")", "\n", "", "return", "torch", ".", "cat", "(", "batch", ",", "0", ",", "out", "=", "out", ")", "\n", "", "elif", "(", "\n", "elem_type", ".", "__module__", "==", "\"numpy\"", "\n", "and", "elem_type", ".", "__name__", "!=", "\"str_\"", "\n", "and", "elem_type", ".", "__name__", "!=", "\"string_\"", "\n", ")", ":", "\n", "        ", "elem", "=", "batch", "[", "0", "]", "\n", "if", "elem_type", ".", "__name__", "==", "\"ndarray\"", ":", "\n", "# array of string classes and object", "\n", "            ", "if", "np_str_obj_array_pattern", ".", "search", "(", "elem", ".", "dtype", ".", "str", ")", "is", "not", "None", ":", "\n", "                ", "raise", "TypeError", "(", "default_collate_err_msg_format", ".", "format", "(", "elem", ".", "dtype", ")", ")", "\n", "\n", "", "return", "my_collate", "(", "[", "torch", ".", "as_tensor", "(", "b", ")", "for", "b", "in", "batch", "]", ")", "\n", "", "elif", "elem", ".", "shape", "==", "(", ")", ":", "# scalars", "\n", "            ", "return", "torch", ".", "as_tensor", "(", "batch", ")", "\n", "", "", "elif", "isinstance", "(", "elem", ",", "float", ")", ":", "\n", "        ", "return", "torch", ".", "tensor", "(", "batch", ",", "dtype", "=", "torch", ".", "float64", ")", "\n", "", "elif", "isinstance", "(", "elem", ",", "int_classes", ")", ":", "\n", "        ", "return", "torch", ".", "tensor", "(", "batch", ")", "\n", "", "elif", "isinstance", "(", "elem", ",", "string_classes", ")", ":", "\n", "        ", "return", "batch", "\n", "", "elif", "isinstance", "(", "elem", ",", "container_abcs", ".", "Mapping", ")", ":", "\n", "        ", "return", "{", "key", ":", "my_collate", "(", "[", "d", "[", "key", "]", "for", "d", "in", "batch", "]", ")", "for", "key", "in", "elem", "}", "\n", "", "elif", "isinstance", "(", "elem", ",", "tuple", ")", "and", "hasattr", "(", "elem", ",", "\"_fields\"", ")", ":", "# namedtuple", "\n", "        ", "return", "elem_type", "(", "*", "(", "my_collate", "(", "samples", ")", "for", "samples", "in", "zip", "(", "*", "batch", ")", ")", ")", "\n", "", "elif", "isinstance", "(", "elem", ",", "container_abcs", ".", "Sequence", ")", ":", "\n", "        ", "transposed", "=", "zip", "(", "*", "batch", ")", "\n", "return", "[", "my_collate", "(", "samples", ")", "for", "samples", "in", "transposed", "]", "\n", "\n", "", "raise", "TypeError", "(", "default_collate_err_msg_format", ".", "format", "(", "elem_type", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.None.build_dgl_graph.array_in": [[105, 110], ["None"], "function", ["None"], ["", "def", "array_in", "(", "arr", ",", "list_of_arr", ")", ":", "\n", "    ", "for", "elem", "in", "list_of_arr", ":", "\n", "        ", "if", "(", "arr", "==", "elem", ")", ".", "all", "(", ")", ":", "\n", "            ", "return", "True", "\n", "", "", "return", "False", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.None.build_dgl_graph.get_maximum_included_span": [[112, 119], ["range", "build_dgl_graph.array_in", "numpy.array"], "function", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.None.build_dgl_graph.array_in"], ["", "def", "get_maximum_included_span", "(", "span", ",", "all_spans", ")", ":", "\n", "    ", "start", "=", "span", "[", "0", "]", "\n", "for", "increment", "in", "range", "(", "1", ",", "10", ")", ":", "\n", "        ", "end", "=", "span", "[", "1", "]", "-", "increment", "\n", "new_span", "=", "[", "start", ",", "end", "]", "\n", "if", "array_in", "(", "np", ".", "array", "(", "new_span", ")", ",", "all_spans", ")", ":", "\n", "            ", "return", "new_span", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.None.build_dgl_graph.save_model": [[121, 132], ["hmtl.utils.get_config", "torch.save", "torch.save", "os.path.join", "clf.state_dict", "clfNodes.state_dict", "G_encoder.state_dict"], "function", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.utils.constants.get_config", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.None.combine_data.save", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.None.combine_data.save", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.dataset_readers.multiprocess_dataset_reader.QIterable.join", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.learning_rate_schedulers.learning_rate_scheduler._PyTorchLearningRateSchedulerWrapper.state_dict", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.learning_rate_schedulers.learning_rate_scheduler._PyTorchLearningRateSchedulerWrapper.state_dict", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.learning_rate_schedulers.learning_rate_scheduler._PyTorchLearningRateSchedulerWrapper.state_dict"], ["", "", "", "def", "save_model", "(", "clf", ",", "G_encoder", ",", "clfNodes", "=", "None", ")", ":", "\n", "    ", "config", "=", "get_config", "(", ")", "\n", "torch", ".", "save", "(", "\n", "{", "\n", "\"clf_state_dict\"", ":", "clf", ".", "state_dict", "(", ")", ",", "\n", "\"clfNodes\"", ":", "clfNodes", ".", "state_dict", "(", ")", ",", "\n", "\"G_encoder_state_dict\"", ":", "G_encoder", ".", "state_dict", "(", ")", ",", "\n", "}", ",", "\n", "os", ".", "path", ".", "join", "(", "\n", "config", "[", "\"reward_serialization_dir\"", "]", ",", "\n", "\"{}_{}.th\"", ".", "format", "(", "args", ".", "model_name", ",", "args", ".", "encoder_type", ")", ",", "\n", ")", ",", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.None.build_dgl_graph.train_classifier": [[136, 317], ["torch.nn.L1Loss", "torch.optim.Adam", "torch.optim.lr_scheduler.StepLR", "build_dgl_graph.train_classifier.get_val_loaders"], "function", ["None"], ["", "def", "train_classifier", "(", "\n", "clf", ",", "\n", "clfNodes", ",", "\n", "G_encoder", ",", "\n", "train_data", ",", "\n", "val_data", ",", "\n", "neg_class", ",", "\n", "criterion", "=", "nn", ".", "L1Loss", "(", ")", ",", "\n", "batch_size", "=", "32", ",", "\n", "epochs", "=", "100", ",", "\n", "patience", "=", "sys", ".", "maxsize", ",", "\n", "init_num_perturb_actions", "=", "1", ",", "\n", "clf_dataset_type", "=", "\"\"", ",", "\n", ")", ":", "\n", "\n", "    ", "optimizer", "=", "Adam", "(", "itertools", ".", "chain", "(", "G_encoder", ".", "parameters", "(", ")", ",", "clf", ".", "parameters", "(", ")", ")", ")", "\n", "scheduler", "=", "StepLR", "(", "optimizer", ",", "step_size", "=", "10", ",", "gamma", "=", "0.7", ")", "\n", "best_f1", ",", "best_acc", ",", "best_epoch", ",", "patience_ctr", "=", "0", ",", "0", ",", "0", ",", "-", "1", "\n", "\n", "def", "get_train_loader", "(", "epoch", ")", ":", "\n", "        ", "if", "args", ".", "perturbation_type", "==", "\"epoch\"", ":", "\n", "            ", "return", "DataLoader", "(", "train_data", ",", "batch_size", "=", "batch_size", ",", "shuffle", "=", "True", ")", "\n", "", "else", ":", "\n", "            ", "if", "clf_dataset_type", "==", "\"combined\"", ":", "\n", "                ", "train_dataset", "=", "CombinedDataset", "(", "\n", "G_encoder", ",", "train_data", ",", "neg_class", ",", "epoch", ",", "init_num_perturb_actions", "\n", ")", "\n", "return", "DataLoader", "(", "\n", "train_dataset", ",", "\n", "batch_size", "=", "batch_size", ",", "\n", "shuffle", "=", "True", ",", "\n", "collate_fn", "=", "my_collate", ",", "\n", ")", "\n", "", "else", ":", "\n", "                ", "train_dataset", "=", "GraphDataset", "(", "\n", "G_encoder", ",", "train_data", ",", "neg_class", ",", "epoch", ",", "init_num_perturb_actions", "\n", ")", "\n", "return", "DataLoader", "(", "train_dataset", ",", "batch_size", "=", "batch_size", ",", "shuffle", "=", "True", ")", "\n", "\n", "", "", "", "def", "get_val_loaders", "(", ")", ":", "\n", "        ", "if", "args", ".", "perturbation_type", "==", "\"epoch\"", ":", "\n", "            ", "return", "{", "\n", "\"val_loader_epoch\"", ":", "DataLoader", "(", "\n", "val_data", ",", "batch_size", "=", "batch_size", ",", "shuffle", "=", "False", "\n", ")", "\n", "}", "\n", "", "else", ":", "\n", "            ", "if", "clf_dataset_type", "==", "\"combined\"", ":", "\n", "                ", "return", "{", "\n", "n", ":", "DataLoader", "(", "\n", "CombinedDataset", "(", "\n", "G_encoder", ",", "\n", "val_data", ",", "\n", "neg_class", ",", "\n", "decay_factor", ",", "\n", "init_num_perturb_actions", ",", "\n", ")", ",", "\n", "batch_size", "=", "batch_size", ",", "\n", "shuffle", "=", "True", ",", "\n", "collate_fn", "=", "my_collate", ",", "\n", ")", "\n", "for", "n", ",", "decay_factor", "in", "(", "\n", "(", "\"val_loader_03\"", ",", "0", ")", ",", "\n", "(", "\"val_loader_02\"", ",", "50", ")", ",", "\n", "(", "\"val_loader_01\"", ",", "100", ")", ",", "\n", ")", "\n", "}", "\n", "", "else", ":", "\n", "                ", "return", "{", "\n", "n", ":", "DataLoader", "(", "\n", "GraphDataset", "(", "\n", "G_encoder", ",", "\n", "val_data", ",", "\n", "neg_class", ",", "\n", "decay_factor", ",", "\n", "init_num_perturb_actions", ",", "\n", ")", ",", "\n", "batch_size", "=", "batch_size", ",", "\n", "shuffle", "=", "True", ",", "\n", ")", "\n", "for", "n", ",", "decay_factor", "in", "(", "\n", "(", "\"val_loader_03\"", ",", "0", ")", ",", "\n", "(", "\"val_loader_02\"", ",", "50", ")", ",", "\n", "(", "\"val_loader_01\"", ",", "100", ")", ",", "\n", ")", "\n", "}", "\n", "\n", "", "", "", "val_loaders", "=", "get_val_loaders", "(", ")", "\n", "train_loader", "=", "get_train_loader", "(", "50", ")", "\n", "for", "epoch", "in", "range", "(", "epochs", ")", ":", "\n", "# train_loader = get_train_loader(epoch)", "\n", "        ", "clf", ".", "train", "(", ")", "\n", "clfNodes", ".", "train", "(", ")", "\n", "G_encoder", ".", "train", "(", ")", "\n", "running_loss", "=", "0.0", "\n", "for", "data_item", "in", "train_loader", ":", "\n", "            ", "if", "clf_dataset_type", "==", "\"combined\"", ":", "\n", "                ", "graph_x", ",", "graph_y", ",", "nodes_x", ",", "nodes_y", "=", "data_item", "\n", "", "else", ":", "\n", "                ", "graph_x", ",", "graph_y", "=", "data_item", "\n", "\n", "", "optimizer", ".", "zero_grad", "(", ")", "\n", "graph_x", "=", "graph_x", ".", "to", "(", "device", ")", "\n", "graph_y", "=", "graph_y", ".", "to", "(", "device", ")", "\n", "logits_graph", "=", "clf", "(", "graph_x", ")", ".", "squeeze", "(", ")", "\n", "loss", "=", "criterion", "(", "logits_graph", ",", "graph_y", ".", "float", "(", ")", ")", "\n", "\n", "if", "clf_dataset_type", "==", "\"combined\"", ":", "\n", "                ", "nodes_x", "=", "nodes_x", ".", "to", "(", "device", ")", "\n", "nodes_y", "=", "nodes_y", ".", "to", "(", "device", ")", "\n", "logits_nodes", "=", "clfNodes", "(", "nodes_x", ")", ".", "squeeze", "(", ")", "\n", "loss_nodes", "=", "criterion", "(", "logits_nodes", ",", "nodes_y", ".", "float", "(", ")", ")", "\n", "loss", "=", "loss", "+", "loss_nodes", ".", "mean", "(", ")", "\n", "\n", "", "loss", ".", "backward", "(", ")", "\n", "optimizer", ".", "step", "(", ")", "\n", "running_loss", "+=", "loss", ".", "item", "(", ")", "\n", "\n", "", "graph_avg_val_f1", ",", "graph_avg_val_acc", "=", "0", ",", "0", "\n", "nodes_avg_val_f1", ",", "nodes_avg_val_acc", "=", "0", ",", "0", "\n", "\n", "for", "name", ",", "val_loader", "in", "val_loaders", ".", "items", "(", ")", ":", "\n", "            ", "results", "=", "validate_classifier", "(", "\n", "clf", ",", "\n", "clfNodes", ",", "\n", "G_encoder", ",", "\n", "val_loader", ",", "\n", "neg_class", "=", "neg_class", ",", "\n", "clf_dataset_type", "=", "clf_dataset_type", ",", "\n", ")", "\n", "graph_acc", "=", "results", ".", "get", "(", "\"graph_acc\"", ",", "-", "1", ")", "\n", "graph_f1", "=", "results", ".", "get", "(", "\"graph_f1\"", ",", "-", "1", ")", "\n", "nodes_acc", "=", "results", ".", "get", "(", "\"nodes_acc\"", ",", "-", "1", ")", "\n", "nodes_f1", "=", "results", ".", "get", "(", "\"nodes_f1\"", ",", "-", "1", ")", "\n", "if", "clf_dataset_type", "==", "\"combined\"", ":", "\n", "                ", "logging", ".", "info", "(", "\n", "\"After epoch {}: Validating {}... graph_val_acc: {}, graph_val_f1: {}, nodes_val_acc: {}, nodes_val_f1: {}\"", ".", "format", "(", "\n", "epoch", ",", "name", ",", "graph_acc", ",", "graph_f1", ",", "nodes_acc", ",", "nodes_f1", "\n", ")", "\n", ")", "\n", "nodes_avg_val_acc", "+=", "nodes_acc", "\n", "nodes_avg_val_f1", "+=", "nodes_f1", "\n", "", "else", ":", "\n", "                ", "logging", ".", "info", "(", "\n", "\"After epoch {}: Validating {}... graph_val_acc: {}, graph_val_f1: {}\"", ".", "format", "(", "\n", "epoch", ",", "name", ",", "graph_acc", ",", "graph_f1", "\n", ")", "\n", ")", "\n", "", "graph_avg_val_acc", "+=", "graph_acc", "\n", "graph_avg_val_f1", "+=", "graph_f1", "\n", "\n", "", "avg_val_acc", "=", "graph_avg_val_acc", "/", "len", "(", "val_loaders", ")", "\n", "avg_val_f1", "=", "graph_avg_val_f1", "/", "len", "(", "val_loaders", ")", "\n", "if", "clf_dataset_type", "==", "\"combined\"", ":", "\n", "            ", "nodes_avg_val_acc", "=", "nodes_avg_val_acc", "/", "len", "(", "val_loaders", ")", "\n", "nodes_avg_val_f1", "=", "nodes_avg_val_f1", "/", "len", "(", "val_loaders", ")", "\n", "avg_val_acc", "=", "(", "avg_val_acc", "+", "nodes_avg_val_acc", ")", "/", "2", "\n", "avg_val_f1", "=", "(", "avg_val_f1", "+", "nodes_avg_val_f1", ")", "/", "2", "\n", "\n", "", "scheduler", ".", "step", "(", ")", "\n", "if", "best_f1", "<", "avg_val_f1", ":", "\n", "            ", "save_model", "(", "clf", ",", "G_encoder", ",", "clfNodes", ")", "\n", "best_f1", "=", "avg_val_f1", "\n", "best_acc", "=", "avg_val_acc", "\n", "best_epoch", "=", "epoch", "\n", "patience_ctr", "=", "0", "\n", "", "else", ":", "\n", "            ", "patience_ctr", "+=", "1", "\n", "", "if", "patience_ctr", ">=", "patience", ":", "\n", "            ", "logging", ".", "info", "(", "\"Ran out of patience. Stopping training...\"", ")", "\n", "break", "\n", "", "", "if", "clf_dataset_type", "==", "\"combined\"", ":", "\n", "        ", "logging", ".", "info", "(", "\n", "\"Best epoch: {}, (global/node) Best accuracy: {}/{}, Best f1: {}/{}\"", ".", "format", "(", "\n", "best_epoch", ",", "avg_val_acc", ",", "nodes_avg_val_acc", ",", "avg_val_f1", ",", "nodes_avg_val_f1", "\n", ")", "\n", ")", "\n", "", "else", ":", "\n", "        ", "logging", ".", "info", "(", "\n", "\"Best epoch: {}, Best accuracy: {}, Best f1: {}\"", ".", "format", "(", "\n", "best_epoch", ",", "best_acc", ",", "best_f1", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.None.build_dgl_graph.validate_classifier": [[321, 393], ["clf.eval", "clfNodes.eval", "G_encoder.eval", "graph_x.to.to", "graph_y.to.to", "clf().squeeze", "torch.eq().sum().item", "torch.eq().sum().item", "graph_y.to.size", "sklearn.metrics.f1_score", "graph_y.to.detach().float().cpu().numpy", "clf().squeeze.detach().cpu().numpy", "nodes_x.to.to", "nodes_y.to.to", "clf().squeeze", "torch.eq().sum().item", "torch.eq().sum().item", "nodes_y.to.size", "sklearn.metrics.f1_score", "clf", "ValueError", "torch.eq().sum", "torch.eq().sum", "nodes_y.to.detach().float().cpu().numpy", "clf().squeeze.detach().cpu().numpy", "graph_y.to.detach().float().cpu", "clf().squeeze.detach().cpu", "clf", "ValueError", "torch.eq().sum", "torch.eq().sum", "torch.eq", "torch.eq", "nodes_y.to.detach().float().cpu", "clf().squeeze.detach().cpu", "clf().squeeze.detach().cpu", "graph_y.to.detach().float().cpu", "graph_y.to.detach().float", "clf().squeeze.detach", "torch.eq", "torch.eq", "clf().squeeze.detach().cpu", "nodes_y.to.detach().float().cpu", "nodes_y.to.detach().float", "clf().squeeze.detach", "clf().squeeze.detach", "graph_y.to.detach().float", "graph_y.to.detach", "clf().squeeze.detach", "nodes_y.to.detach().float", "nodes_y.to.detach", "graph_y.to.detach", "nodes_y.to.detach"], "function", ["None"], ["", "", "def", "validate_classifier", "(", "\n", "clf", ",", "clfNodes", ",", "G_encoder", ",", "loader", ",", "neg_class", "=", "0", ",", "batch_size", "=", "32", ",", "clf_dataset_type", "=", "\"\"", "\n", ")", ":", "\n", "    ", "clf", ".", "eval", "(", ")", "\n", "clfNodes", ".", "eval", "(", ")", "\n", "G_encoder", ".", "eval", "(", ")", "\n", "\n", "acc_total", ",", "acc", ",", "f1_total", ",", "f1", ",", "=", "0", ",", "0", ",", "0", ",", "0", "\n", "acc_total_nodes", ",", "acc_nodes", ",", "f1_total_nodes", ",", "f1_nodes", ",", "=", "0", ",", "0", ",", "0", ",", "0", "\n", "\n", "for", "data_item", "in", "loader", ":", "\n", "        ", "if", "clf_dataset_type", "==", "\"combined\"", ":", "\n", "            ", "graph_x", ",", "graph_y", ",", "nodes_x", ",", "nodes_y", "=", "data_item", "\n", "", "else", ":", "\n", "            ", "graph_x", ",", "graph_y", "=", "data_item", "\n", "\n", "", "graph_x", "=", "graph_x", ".", "to", "(", "device", ")", "\n", "graph_y", "=", "graph_y", ".", "to", "(", "device", ")", "\n", "graph_logits", "=", "clf", "(", "graph_x", ")", ".", "squeeze", "(", ")", "\n", "\n", "if", "neg_class", "==", "-", "1", ":", "\n", "            ", "graph_logits", "[", "graph_logits", ">=", "0", "]", "=", "1", "\n", "graph_logits", "[", "graph_logits", "<", "0", "]", "=", "-", "1", "\n", "", "elif", "neg_class", "==", "0", ":", "\n", "            ", "graph_logits", "[", "graph_logits", ">=", "0.5", "]", "=", "1", "\n", "graph_logits", "[", "graph_logits", "<", "0.5", "]", "=", "0", "\n", "", "else", ":", "\n", "            ", "raise", "ValueError", "(", "\"Invalid `neg_class`.\"", ")", "\n", "", "acc", "+=", "(", "\n", "torch", ".", "eq", "(", "graph_logits", ".", "detach", "(", ")", ".", "cpu", "(", ")", ",", "graph_y", ".", "detach", "(", ")", ".", "float", "(", ")", ".", "cpu", "(", ")", ")", "\n", ".", "sum", "(", ")", "\n", ".", "item", "(", ")", "\n", ")", "\n", "acc_total", "+=", "graph_y", ".", "size", "(", "0", ")", "\n", "f1", "+=", "f1_score", "(", "\n", "graph_y", ".", "detach", "(", ")", ".", "float", "(", ")", ".", "cpu", "(", ")", ".", "numpy", "(", ")", ",", "\n", "graph_logits", ".", "detach", "(", ")", ".", "cpu", "(", ")", ".", "numpy", "(", ")", ",", "\n", "average", "=", "\"binary\"", ",", "\n", ")", "\n", "f1_total", "+=", "1", "\n", "\n", "if", "clf_dataset_type", "==", "\"combined\"", ":", "\n", "            ", "nodes_x", "=", "nodes_x", ".", "to", "(", "device", ")", "\n", "nodes_y", "=", "nodes_y", ".", "to", "(", "device", ")", "\n", "nodes_logits", "=", "clf", "(", "nodes_x", ")", ".", "squeeze", "(", ")", "\n", "\n", "if", "neg_class", "==", "-", "1", ":", "\n", "                ", "nodes_logits", "[", "nodes_logits", ">=", "0", "]", "=", "1", "\n", "nodes_logits", "[", "nodes_logits", "<", "0", "]", "=", "-", "1", "\n", "", "elif", "neg_class", "==", "0", ":", "\n", "                ", "nodes_logits", "[", "nodes_logits", ">=", "0.5", "]", "=", "1", "\n", "nodes_logits", "[", "nodes_logits", "<", "0.5", "]", "=", "0", "\n", "", "else", ":", "\n", "                ", "raise", "ValueError", "(", "\"Invalid `neg_class`.\"", ")", "\n", "", "acc_nodes", "+=", "(", "\n", "torch", ".", "eq", "(", "nodes_logits", ".", "detach", "(", ")", ".", "cpu", "(", ")", ",", "nodes_y", ".", "detach", "(", ")", ".", "float", "(", ")", ".", "cpu", "(", ")", ")", "\n", ".", "sum", "(", ")", "\n", ".", "item", "(", ")", "\n", ")", "\n", "acc_total_nodes", "+=", "nodes_y", ".", "size", "(", "0", ")", "\n", "f1_nodes", "+=", "f1_score", "(", "\n", "nodes_y", ".", "detach", "(", ")", ".", "float", "(", ")", ".", "cpu", "(", ")", ".", "numpy", "(", ")", ",", "\n", "nodes_logits", ".", "detach", "(", ")", ".", "cpu", "(", ")", ".", "numpy", "(", ")", ",", "\n", "average", "=", "\"binary\"", ",", "\n", ")", "\n", "f1_total_nodes", "+=", "1", "\n", "\n", "", "", "results", "=", "{", "\"graph_acc\"", ":", "acc", "/", "acc_total", ",", "\"graph_f1\"", ":", "f1", "/", "f1_total", "}", "\n", "if", "clf_dataset_type", "==", "\"combined\"", ":", "\n", "        ", "results", "[", "\"nodes_acc\"", "]", "=", "acc_nodes", "/", "acc_total_nodes", "\n", "results", "[", "\"nodes_f1\"", "]", "=", "f1_nodes", "/", "f1_total_nodes", "\n", "", "return", "results", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.None.build_dgl_graph.get_graph_reps": [[395, 418], ["hmtl.utils.get_config", "build_dgl_graph.GraphNN", "G.to.to", "G.to.build_graph", "G_encoder"], "function", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.utils.constants.get_config", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.utils.graph.Graph.build_graph"], ["", "def", "get_graph_reps", "(", "G_encoder", ",", "graph", ",", "all_spans", ",", "span_embeddings", ")", ":", "\n", "    ", "config", "=", "get_config", "(", ")", "\n", "\n", "G", "=", "GraphNN", "(", ")", "\n", "G", "=", "G", ".", "to", "(", "device", ")", "\n", "nodes", "=", "graph", "[", "\"nodes\"", "]", "\n", "edges", "=", "graph", "[", "\"edges\"", "]", "\n", "# verb_indices = [node['node_id'] for node in nodes if node['type'] == 'V']", "\n", "root_idx", "=", "G", ".", "build_graph", "(", "nodes", ",", "edges", ",", "all_spans", ",", "span_embeddings", ")", "\n", "feats", "=", "G", ".", "g", ".", "ndata", "[", "\"hv\"", "]", "\n", "graph_rep", ",", "node_reps", "=", "G_encoder", "(", "G", ".", "g", ",", "graph", ",", "feats", ")", "\n", "return", "graph_rep", ",", "node_reps", "\n", "\n", "\"\"\"\n    verb_reps = nodes_matrix_rep[verb_indices]\n    verb_reps_aggregated = verb_reps.sum(dim=0)\n    if encoding_type == 'root':\n        return nodes_matrix_rep[root_idx]\n    elif encoding_type == 'verb_nodes':\n        return verb_reps_aggregated\n    elif encoding_type == 'root_and_verbs':\n        return torch.cat((nodes_matrix_rep[root_idx], verb_reps_aggregated))\n    \"\"\"", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.None.build_dgl_graph.perturb": [[420, 454], ["hmtl.utils.get_config", "hmtl.utils.Graph.build_graph", "graph.metadata.get", "graph.perturb", "graph.to_json", "len", "int", "ratio.sum", "numpy.ceil", "numpy.array", "pow"], "function", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.utils.constants.get_config", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.utils.graph.Graph.build_graph", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.params.Params.get", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.utils.graph.Graph.perturb", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.utils.graph.Graph.to_json"], ["", "def", "perturb", "(", "\n", "dockey", ",", "\n", "graph_json", ",", "\n", "all_spans", ",", "\n", "span_embeddings", ",", "\n", "original_text", ",", "\n", "decay_factor", ",", "\n", "init_num_perturb_actions", ",", "\n", ")", ":", "\n", "    ", "config", "=", "get_config", "(", ")", "\n", "\n", "ratio", "=", "(", "\n", "K", ".", "perturbation_ratio", "\n", "if", "args", ".", "perturbation_type", "==", "\"all\"", "\n", "else", "(", "np", ".", "array", "(", "K", ".", "perturbation_types", ")", "==", "args", ".", "perturbation_type", ")", ".", "astype", "(", "float", ")", "\n", ")", "# 1 for chosen, 0 for others", "\n", "graph", ",", "_", "=", "Graph", ".", "build_graph", "(", "\n", "dockey", ",", "\n", "graph_json", ",", "\n", "max_span_width", "=", "config", "[", "\"max_span_width\"", "]", ",", "\n", "all_spans", "=", "all_spans", ",", "\n", "span_embeddings", "=", "span_embeddings", ",", "\n", "original_text", "=", "original_text", ",", "\n", ")", "\n", "# i.i.e perturbation probability for each node: on average we want k per sentence, decayed as training progresses", "\n", "num_sent", "=", "graph", ".", "metadata", ".", "get", "(", "\"num_sent\"", ",", "len", "(", "graph", ".", "nodes", ")", ")", "\n", "graph", ".", "perturb", "(", "\n", "perturbation_probabilities", "=", "ratio", "/", "ratio", ".", "sum", "(", ")", ",", "\n", "no_perturb_actions", "=", "int", "(", "\n", "np", ".", "ceil", "(", "init_num_perturb_actions", "*", "pow", "(", "0.99", ",", "decay_factor", ")", "*", "num_sent", ")", "\n", ")", ",", "\n", ")", "\n", "\n", "return", "graph", ".", "to_json", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.None.build_dgl_graph.init_args": [[456, 489], ["argparse.ArgumentParser", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.parse_args"], "function", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.commands.__init__.ArgumentParserWithDefaults.add_argument", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.commands.__init__.ArgumentParserWithDefaults.add_argument", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.commands.__init__.ArgumentParserWithDefaults.add_argument", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.commands.__init__.ArgumentParserWithDefaults.add_argument", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.commands.__init__.ArgumentParserWithDefaults.add_argument", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.commands.__init__.ArgumentParserWithDefaults.add_argument", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.commands.__init__.ArgumentParserWithDefaults.add_argument", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.commands.__init__.ArgumentParserWithDefaults.add_argument"], ["", "def", "init_args", "(", ")", ":", "\n", "    ", "parser", "=", "argparse", ".", "ArgumentParser", "(", "\n", "description", "=", "\"Build node representation matrices for documents\"", "\n", ")", "\n", "parser", ".", "add_argument", "(", "\"config_path\"", ",", "help", "=", "\"Configuration file\"", ")", "\n", "parser", ".", "add_argument", "(", "\"encoder_type\"", ",", "help", "=", "\"Type of graph encoder\"", ")", "\n", "parser", ".", "add_argument", "(", "\"perturbation_type\"", ",", "help", "=", "\"Type of perturbation to perform\"", ")", "\n", "parser", ".", "add_argument", "(", "\"model_name\"", ",", "help", "=", "\"Name for the model to be saved\"", ")", "\n", "parser", ".", "add_argument", "(", "\n", "\"train_path\"", ",", "\n", "nargs", "=", "\"?\"", ",", "\n", "default", "=", "\"./models/coref_srl_conll_bert/graphs/graphs_gold_combined_train.json\"", ",", "\n", "help", "=", "\"JSON file containing training graphs\"", ",", "\n", ")", "\n", "parser", ".", "add_argument", "(", "\n", "\"dev_path\"", ",", "\n", "nargs", "=", "\"?\"", ",", "\n", "default", "=", "\"./models/coref_srl_conll_bert/graphs/graphs_gold_combined_dev.json\"", ",", "\n", "help", "=", "\"JSON file containing validation graphs\"", ",", "\n", ")", "\n", "parser", ".", "add_argument", "(", "\n", "\"neg_train_path\"", ",", "\n", "nargs", "=", "\"?\"", ",", "\n", "default", "=", "\"./models/coref_srl_conll_bert/graphs/graphs_epoch_1_combined_train.json\"", ",", "\n", "help", "=", "\"JSON file containing training graphs\"", ",", "\n", ")", "\n", "parser", ".", "add_argument", "(", "\n", "\"neg_dev_path\"", ",", "\n", "nargs", "=", "\"?\"", ",", "\n", "default", "=", "\"./models/coref_srl_conll_bert/graphs/graphs_epoch_1_combined_dev.json\"", ",", "\n", "help", "=", "\"JSON file containing validation graphs\"", ",", "\n", ")", "\n", "return", "parser", ".", "parse_args", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.None.combine_data._normalize_word": [[20, 25], ["None"], "function", ["None"], ["def", "_normalize_word", "(", "word", ")", ":", "\n", "    ", "if", "word", "==", "\"/.\"", "or", "word", "==", "\"/?\"", ":", "\n", "        ", "return", "word", "[", "1", ":", "]", "\n", "", "else", ":", "\n", "        ", "return", "word", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.None.combine_data._find_index": [[27, 32], ["flattened_ontonotes.index"], "function", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.fields.text_field.TextField.index"], ["", "", "def", "_find_index", "(", "doc", ",", "flattened_ontonotes", ")", ":", "\n", "    ", "try", ":", "\n", "        ", "return", "flattened_ontonotes", ".", "index", "(", "doc", ")", "\n", "", "except", "ValueError", ":", "\n", "        ", "return", "-", "1", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.None.combine_data.read_raw_ontonotes_docs": [[34, 46], ["allennlp.data.dataset_readers.dataset_utils.ontonotes.Ontonotes", "allennlp.data.dataset_readers.dataset_utils.ontonotes.Ontonotes.dataset_document_iterator", "ontonotes.append", "flattened_ontonotes.append", "len", "len", "combine_data._normalize_word"], "function", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.dataset_utils.ontonotes.Ontonotes.dataset_document_iterator", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.coreference_resolution.conll.ConllCorefReader._normalize_word"], ["", "", "def", "read_raw_ontonotes_docs", "(", "path", ")", ":", "\n", "    ", "ontonotes", ",", "flattened_ontonotes", "=", "[", "]", ",", "[", "]", "\n", "ontonotes_reader", "=", "Ontonotes", "(", ")", "\n", "for", "sentences", "in", "ontonotes_reader", ".", "dataset_document_iterator", "(", "path", ")", ":", "\n", "        ", "ontonotes", ".", "append", "(", "[", "s", ".", "words", "for", "s", "in", "sentences", "]", ")", "\n", "", "for", "doc", "in", "ontonotes", ":", "\n", "        ", "flattened_ontonotes", ".", "append", "(", "\n", "[", "_normalize_word", "(", "word", ")", "for", "sentence", "in", "doc", "for", "word", "in", "sentence", "]", "\n", ")", "\n", "\n", "", "assert", "len", "(", "ontonotes", ")", "==", "len", "(", "flattened_ontonotes", ")", "\n", "return", "ontonotes", ",", "flattened_ontonotes", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.None.combine_data.read_coref_ontonotes": [[48, 65], ["allennlp.data.dataset_readers.coreference_resolution.conll.ConllCorefReader", "allennlp.data.dataset_readers.coreference_resolution.conll.ConllCorefReader.read", "allennlp.data.token_indexers.single_id_token_indexer.SingleIdTokenIndexer", "allennlp.data.token_indexers.token_characters_indexer.TokenCharactersIndexer", "cdata.append"], "function", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.scripts.convert_openie_to_conll.read"], ["", "def", "read_coref_ontonotes", "(", "path", ")", ":", "\n", "    ", "indexers", "=", "{", "\n", "\"tokens\"", ":", "SingleIdTokenIndexer", "(", "lowercase_tokens", "=", "True", ")", ",", "\n", "\"token_characters\"", ":", "TokenCharactersIndexer", "(", ")", ",", "\n", "}", "\n", "reader", "=", "ConllCorefReader", "(", "max_span_width", "=", "10", ",", "token_indexers", "=", "indexers", ")", "\n", "data", "=", "reader", ".", "read", "(", "path", ")", "\n", "\n", "cdata", "=", "[", "]", "\n", "for", "dp", "in", "data", ":", "\n", "        ", "obj", "=", "dp", ".", "fields", "[", "\"metadata\"", "]", ".", "metadata", "\n", "# reformat to be consistent with the predictions", "\n", "obj", "[", "\"document\"", "]", "=", "[", "obj", "[", "\"original_text\"", "]", "]", "\n", "obj", "[", "\"clusters\"", "]", "=", "[", "obj", "[", "\"clusters\"", "]", "]", "\n", "del", "obj", "[", "\"original_text\"", "]", "\n", "cdata", ".", "append", "(", "obj", ")", "\n", "", "return", "cdata", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.None.combine_data.read_srl_ontonotes": [[67, 88], ["allennlp.data.dataset_readers.semantic_role_labeling.SrlReader", "allennlp.data.dataset_readers.semantic_role_labeling.SrlReader.read", "allennlp.data.token_indexers.single_id_token_indexer.SingleIdTokenIndexer", "allennlp.data.token_indexers.token_characters_indexer.TokenCharactersIndexer", "sdata.append"], "function", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.scripts.convert_openie_to_conll.read"], ["", "def", "read_srl_ontonotes", "(", "path", ")", ":", "\n", "    ", "indexers", "=", "{", "\n", "\"tokens\"", ":", "SingleIdTokenIndexer", "(", "lowercase_tokens", "=", "True", ")", ",", "\n", "\"token_characters\"", ":", "TokenCharactersIndexer", "(", ")", ",", "\n", "}", "\n", "reader", "=", "SrlReader", "(", "token_indexers", "=", "indexers", ")", "\n", "data", "=", "reader", ".", "read", "(", "path", ")", "\n", "\n", "sdata", "=", "[", "]", "\n", "no_verb", "=", "0", "\n", "for", "dp", "in", "data", ":", "\n", "        ", "obj", "=", "dp", ".", "fields", "[", "\"metadata\"", "]", ".", "metadata", "\n", "if", "not", "obj", "[", "\"verb\"", "]", ":", "\n", "            ", "no_verb", "+=", "1", "\n", "continue", "\n", "# reformat to be consistent with the predictions", "\n", "", "obj", "[", "\"tags\"", "]", "=", "[", "dp", ".", "fields", "[", "\"tags\"", "]", ".", "labels", "]", "\n", "obj", "[", "\"words\"", "]", "=", "[", "obj", "[", "\"words\"", "]", "]", "\n", "obj", "[", "\"verb\"", "]", "=", "[", "obj", "[", "\"verb\"", "]", "]", "\n", "sdata", ".", "append", "(", "obj", ")", "\n", "", "return", "sdata", ",", "no_verb", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.None.combine_data.read_pkl": [[90, 93], ["open", "pickle.load"], "function", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.token_embedders.bert_token_embedder.PretrainedBertModel.load"], ["", "def", "read_pkl", "(", "path", ")", ":", "\n", "    ", "with", "open", "(", "path", ",", "\"rb\"", ")", "as", "f", ":", "\n", "        ", "return", "pkl", ".", "load", "(", "f", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.None.combine_data.write_pkl": [[95, 98], ["open", "pickle.dump"], "function", ["None"], ["", "", "def", "write_pkl", "(", "data", ",", "path", ")", ":", "\n", "    ", "with", "open", "(", "path", ",", "\"wb\"", ")", "as", "f", ":", "\n", "        ", "pkl", ".", "dump", "(", "data", ",", "f", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.None.combine_data.make_srl_dict": [[100, 106], ["collections.defaultdict", "zip", "srl_dict[].append"], "function", ["None"], ["", "", "def", "make_srl_dict", "(", "sdata", ")", ":", "\n", "    ", "srl_dict", "=", "defaultdict", "(", "list", ")", "\n", "for", "s", "in", "sdata", ":", "\n", "        ", "for", "words", ",", "verb", ",", "tags", "in", "zip", "(", "s", "[", "\"words\"", "]", ",", "s", "[", "\"verb\"", "]", ",", "s", "[", "\"tags\"", "]", ")", ":", "\n", "            ", "srl_dict", "[", "\" \"", ".", "join", "(", "words", ")", "]", ".", "append", "(", "{", "\"verb\"", ":", "verb", ",", "\"tags\"", ":", "tags", "}", ")", "\n", "", "", "return", "srl_dict", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.None.combine_data.combine": [[108, 139], ["combine_data.read_raw_ontonotes_docs", "enumerate", "print", "combine_data._find_index", "srl_dict.get", "obj[].append", "obj[].append", "str", "uuid.uuid4"], "function", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.None.combine_data.read_raw_ontonotes_docs", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.None.combine_data._find_index", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.params.Params.get"], ["", "def", "combine", "(", "cdata", ",", "srl_dict", ",", "part", "=", "\"test\"", ")", ":", "\n", "    ", "output", "=", "{", "}", "\n", "total", ",", "total_missed", "=", "0", ",", "0", "\n", "\n", "# read original Ontonotes with sentence demarcations", "\n", "ontonotes", ",", "flattened_ontonotes", "=", "read_raw_ontonotes_docs", "(", "\n", "\"./data/conll-2012_single_file/{}.english.gold_conll\"", ".", "format", "(", "part", ")", "\n", ")", "\n", "\n", "for", "dnum", ",", "c", "in", "enumerate", "(", "cdata", ")", ":", "\n", "        ", "num_sents", ",", "missed", "=", "0", ",", "0", "\n", "obj", "=", "{", "}", "\n", "doc_idx", "=", "_find_index", "(", "c", "[", "\"document\"", "]", "[", "0", "]", ",", "flattened_ontonotes", ")", "\n", "if", "doc_idx", ">", "-", "1", ":", "\n", "            ", "obj", "[", "\"document\"", "]", "=", "flattened_ontonotes", "[", "doc_idx", "]", "\n", "obj", "[", "\"clusters\"", "]", "=", "c", "[", "\"clusters\"", "]", "\n", "obj", "[", "\"sentences\"", "]", "=", "[", "]", "\n", "obj", "[", "\"srl_tags\"", "]", "=", "[", "]", "\n", "for", "sent", "in", "ontonotes", "[", "doc_idx", "]", ":", "\n", "                ", "srl_obj", "=", "srl_dict", ".", "get", "(", "\" \"", ".", "join", "(", "sent", ")", ",", "None", ")", "\n", "obj", "[", "\"sentences\"", "]", ".", "append", "(", "sent", ")", "\n", "obj", "[", "\"srl_tags\"", "]", ".", "append", "(", "srl_obj", ")", "\n", "if", "not", "srl_obj", ":", "\n", "                    ", "missed", "+=", "1", "\n", "", "num_sents", "+=", "1", "\n", "", "output", "[", "str", "(", "uuid4", "(", ")", ")", "]", "=", "obj", "\n", "# print('Doc {} missed {}/{} sentences...'.format(dnum+1, missed, num_sents))", "\n", "", "total", "+=", "num_sents", "\n", "total_missed", "+=", "missed", "\n", "", "print", "(", "\"Total missed sentences: {}/{}\"", ".", "format", "(", "total_missed", ",", "total", ")", ")", "\n", "return", "output", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.None.combine_data.save": [[141, 144], ["open", "json.dump"], "function", ["None"], ["", "def", "save", "(", "output", ",", "path", ")", ":", "\n", "    ", "with", "open", "(", "path", ",", "\"w\"", ",", "encoding", "=", "\"utf-8\"", ")", "as", "f", ":", "\n", "        ", "json", ".", "dump", "(", "output", ",", "f", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.None.combine_data.index_span_pkl": [[146, 157], ["data.items", "len"], "function", ["None"], ["", "", "def", "index_span_pkl", "(", "data", ",", "sdata", ")", ":", "\n", "    ", "found", "=", "0", "\n", "new_sdata", "=", "{", "}", "\n", "for", "key", ",", "val", "in", "data", ".", "items", "(", ")", ":", "\n", "        ", "for", "s", "in", "sdata", ":", "\n", "            ", "if", "s", "[", "\"document\"", "]", "[", "0", "]", "==", "val", "[", "\"document\"", "]", ":", "\n", "                ", "new_sdata", "[", "key", "]", "=", "s", "\n", "found", "+=", "1", "\n", "break", "\n", "", "", "", "assert", "found", "==", "len", "(", "sdata", ")", "\n", "return", "new_sdata", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.None.combine_data.main": [[159, 177], ["print", "combine_data.read_coref_ontonotes", "combine_data.read_srl_ontonotes", "combine_data.make_srl_dict", "combine_data.combine", "combine_data.save", "print", "print", "combine_data.read_coref_ontonotes", "combine_data.read_srl_ontonotes", "combine_data.make_srl_dict", "combine_data.combine", "combine_data.save", "print", "len", "len", "len", "len"], "function", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.None.combine_data.read_coref_ontonotes", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.None.combine_data.read_srl_ontonotes", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.None.combine_data.make_srl_dict", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.None.combine_data.combine", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.None.combine_data.save", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.None.combine_data.read_coref_ontonotes", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.None.combine_data.read_srl_ontonotes", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.None.combine_data.make_srl_dict", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.None.combine_data.combine", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.None.combine_data.save"], ["", "def", "main", "(", ")", ":", "\n", "    ", "print", "(", "'Combining training data...'", ")", "\n", "cdata", "=", "read_coref_ontonotes", "(", "'./data/conll-2012_single_file/train.english.gold_conll'", ")", "\n", "sdata", ",", "no_verb", "=", "read_srl_ontonotes", "(", "'./data/conll-2012/v4/data/train/'", ")", "\n", "srl_dict", "=", "make_srl_dict", "(", "sdata", ")", "\n", "output", "=", "combine", "(", "cdata", ",", "srl_dict", ",", "part", "=", "'train'", ")", "\n", "assert", "len", "(", "output", ")", "==", "len", "(", "cdata", ")", "\n", "save", "(", "output", ",", "'./graphs/combined_train.json'", ")", "\n", "print", "(", "'Sentences without verbs: {}'", ".", "format", "(", "no_verb", ")", ")", "\n", "\n", "print", "(", "'Combining validation data...'", ")", "\n", "cdata", "=", "read_coref_ontonotes", "(", "'./data/conll-2012_single_file/dev.english.gold_conll'", ")", "\n", "sdata", ",", "no_verb", "=", "read_srl_ontonotes", "(", "'./data/conll-2012/v4/data/development/'", ")", "\n", "srl_dict", "=", "make_srl_dict", "(", "sdata", ")", "\n", "output", "=", "combine", "(", "cdata", ",", "srl_dict", ",", "part", "=", "'dev'", ")", "\n", "assert", "len", "(", "output", ")", "==", "len", "(", "cdata", ")", "\n", "save", "(", "output", ",", "'./graphs/combined_dev.json'", ")", "\n", "print", "(", "'Sentences without verbs: {}'", ".", "format", "(", "no_verb", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.utils.embs.get_reps": [[9, 37], ["tqdm.tqdm", "hmtl.utils.get_config", "graphs.items", "tqdm.tqdm.set_postfix", "len", "hmtl.utils.get_span_encoding", "reps[].cpu().detach().numpy", "hmtl.utils.get_span_encoding.get", "hmtl.utils.get_span_encoding.get", "torch.cat().cpu().detach().numpy", "all_reps.append", "reps[].cpu().detach", "torch.cat().cpu().detach", "hmtl.utils.get_config.get", "reps[].cpu", "torch.cat().cpu", "torch.cat"], "function", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.tqdm.Tqdm.tqdm", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.utils.constants.get_config", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.utils.builder.get_span_encoding", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.params.Params.get", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.params.Params.get", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.params.Params.get"], ["def", "get_reps", "(", "graphs", ",", "desc", ")", ":", "\n", "    ", "all_reps", "=", "[", "]", "\n", "t", "=", "tqdm", "(", "\n", "graphs", ".", "items", "(", ")", ",", "\n", "unit", "=", "\" graphs\"", ",", "\n", "desc", "=", "\"Collecting {} graph representations\"", ".", "format", "(", "desc", ")", ",", "\n", ")", "\n", "config", "=", "get_config", "(", ")", "\n", "for", "dockey", ",", "graph", "in", "t", ":", "\n", "        ", "t", ".", "set_postfix", "(", "{", "\"Doc Index\"", ":", "dockey", "}", ")", "\n", "if", "len", "(", "graph", "[", "\"nodes\"", "]", ")", "<", "config", "[", "\"max_num_nodes\"", "]", ":", "\n", "            ", "reps", "=", "get_span_encoding", "(", "\n", "dockey", ",", "\n", "zero_span_rep", "=", "config", "[", "\"span_rep_size\"", "]", "if", "config", ".", "get", "(", "\"debug\"", ")", "else", "None", ",", "\n", ")", "\n", "all_spans", "=", "reps", "[", "\"all_spans\"", "]", ".", "cpu", "(", ")", ".", "detach", "(", ")", ".", "numpy", "(", ")", "\n", "attended_span_embeddings", "=", "reps", ".", "get", "(", "\"attended_span_embeddings\"", ")", "\n", "endpoint_span_embeddings", "=", "reps", ".", "get", "(", "\"endpoint_span_embeddings\"", ")", "\n", "span_embeddings", "=", "(", "\n", "torch", ".", "cat", "(", "[", "endpoint_span_embeddings", ",", "attended_span_embeddings", "]", ",", "dim", "=", "-", "1", ")", "\n", ".", "cpu", "(", ")", "\n", ".", "detach", "(", ")", "\n", ".", "numpy", "(", ")", "\n", ")", "\n", "all_reps", ".", "append", "(", "\n", "(", "dockey", ",", "graph", ",", "all_spans", ",", "span_embeddings", ",", "reps", "[", "\"original_text\"", "]", ")", "\n", ")", "\n", "", "", "return", "all_reps", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.utils.embs.get_roberta_reps": [[39, 61], ["tqdm.tqdm", "hmtl.utils.get_config", "graphs.items", "tqdm.tqdm.set_postfix", "len", "hmtl.utils.get_span_encoding", "reps[].cpu().detach().numpy", "hmtl.utils.get_span_encoding.get", "reps.get.cpu().detach().numpy", "all_reps.append", "reps[].cpu().detach", "reps.get.cpu().detach", "hmtl.utils.get_config.get", "reps[].cpu", "reps.get.cpu"], "function", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.tqdm.Tqdm.tqdm", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.utils.constants.get_config", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.utils.builder.get_span_encoding", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.params.Params.get", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.params.Params.get"], ["", "def", "get_roberta_reps", "(", "graphs", ",", "desc", ")", ":", "\n", "    ", "all_reps", "=", "[", "]", "\n", "t", "=", "tqdm", "(", "\n", "graphs", ".", "items", "(", ")", ",", "\n", "unit", "=", "\" graphs\"", ",", "\n", "desc", "=", "\"Collecting {} graph representations\"", ".", "format", "(", "desc", ")", ",", "\n", ")", "\n", "config", "=", "get_config", "(", ")", "\n", "for", "dockey", ",", "graph", "in", "t", ":", "\n", "        ", "t", ".", "set_postfix", "(", "{", "\"Doc Index\"", ":", "dockey", "}", ")", "\n", "if", "len", "(", "graph", "[", "\"nodes\"", "]", ")", "<", "config", "[", "\"max_num_nodes\"", "]", ":", "\n", "            ", "reps", "=", "get_span_encoding", "(", "\n", "dockey", ",", "\n", "zero_span_rep", "=", "config", "[", "\"span_rep_size\"", "]", "if", "config", ".", "get", "(", "\"debug\"", ")", "else", "None", ",", "\n", ")", "\n", "all_spans", "=", "reps", "[", "\"all_spans\"", "]", ".", "cpu", "(", ")", ".", "detach", "(", ")", ".", "numpy", "(", ")", "\n", "roberta_embeddings", "=", "reps", ".", "get", "(", "\"roberta_embeddings\"", ")", "\n", "span_embeddings", "=", "roberta_embeddings", ".", "cpu", "(", ")", ".", "detach", "(", ")", ".", "numpy", "(", ")", "\n", "all_reps", ".", "append", "(", "\n", "(", "dockey", ",", "graph", ",", "all_spans", ",", "span_embeddings", ",", "reps", "[", "\"original_text\"", "]", ")", "\n", ")", "\n", "", "", "return", "all_reps", "\n", "", ""]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.utils.graph.Node.__init__": [[18, 29], ["tuple"], "methods", ["None"], ["    ", "def", "__init__", "(", "self", ",", "node_id", ",", "node_type", ",", "doc_indices", ",", "text", ")", ":", "\n", "        ", "self", ".", "node_id", "=", "node_id", "\n", "self", ".", "type", "=", "node_type", "\n", "self", ".", "doc_indices", ":", "Optional", "[", "Tuple", "[", "int", "]", "]", "=", "None", "if", "doc_indices", "is", "None", "else", "tuple", "(", "\n", "doc_indices", "\n", ")", "\n", "self", ".", "text", "=", "text", "\n", "self", ".", "perturbed", "=", "False", "\n", "\n", "self", ".", "links", "=", "[", "]", "\n", "self", ".", "link_types", "=", "[", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.utils.graph.Node.__eq__": [[30, 32], ["None"], "methods", ["None"], ["", "def", "__eq__", "(", "self", ",", "other", ")", ":", "\n", "        ", "return", "self", ".", "node_id", "==", "other", ".", "node_id", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.utils.graph.Node.__hash__": [[33, 35], ["None"], "methods", ["None"], ["", "def", "__hash__", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "node_id", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.utils.graph.Node.__str__": [[36, 44], ["None"], "methods", ["None"], ["", "def", "__str__", "(", "self", ")", ":", "\n", "        ", "return", "\"node_id: {} node_type: {} doc_incides: {} text: {} links: {} link_types: {}\"", ".", "format", "(", "\n", "self", ".", "node_id", ",", "\n", "self", ".", "type", ",", "\n", "self", ".", "doc_indices", ",", "\n", "self", ".", "text", ",", "\n", "self", ".", "links", ",", "\n", "self", ".", "link_types", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.utils.graph.Node.__repr__": [[46, 49], ["None"], "methods", ["None"], ["", "def", "__repr__", "(", "self", ")", ":", "\n", "        ", "return", "\"{}({}, {}, {}, {})\"", ".", "format", "(", "\n", "Node", ".", "__name__", ",", "self", ".", "node_id", ",", "self", ".", "type", ",", "self", ".", "doc_indices", ",", "self", ".", "text", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.utils.graph.Graph.__init__": [[56, 81], ["dict", "str", "tuple", "dict", "uuid.uuid4", "zip"], "methods", ["None"], ["def", "__init__", "(", "\n", "self", ",", "\n", "graph_id", "=", "None", ",", "\n", "metadata", "=", "None", ",", "\n", "max_span_width", "=", "10", ",", "\n", "all_spans", "=", "None", ",", "\n", "span_embeddings", "=", "None", ",", "\n", "original_text", "=", "None", ",", "\n", ")", ":", "\n", "        ", "self", ".", "id", "=", "graph_id", "or", "str", "(", "uuid4", "(", ")", ")", "\n", "self", ".", "nodes", ":", "Dict", "[", "int", ",", "Node", "]", "=", "dict", "(", ")", "\n", "self", ".", "root", "=", "None", "\n", "self", ".", "max_span_width", "=", "max_span_width", "\n", "self", ".", "all_spans", "=", "[", "tuple", "(", "span", ")", "for", "span", "in", "all_spans", "]", "\n", "self", ".", "span_embeddings", "=", "span_embeddings", "\n", "self", ".", "span_embeddings_by_span", "=", "(", "\n", "None", "\n", "if", "self", ".", "all_spans", "is", "None", "or", "self", ".", "span_embeddings", "is", "None", "\n", "else", "dict", "(", "zip", "(", "self", ".", "all_spans", ",", "self", ".", "span_embeddings", ")", ")", "\n", ")", "\n", "self", ".", "original_text", "=", "original_text", "\n", "self", ".", "node_spans", "=", "[", "]", "\n", "self", ".", "node_texts", "=", "[", "]", "\n", "self", ".", "metadata", "=", "metadata", "or", "{", "}", "\n", "self", ".", "perturbations_applied", "=", "[", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.utils.graph.Graph.get_edges": [[82, 90], ["graph.Graph.nodes.items", "zip", "edges.append", "edge_types.append"], "methods", ["None"], ["", "def", "get_edges", "(", "self", ")", ":", "\n", "        ", "edges", "=", "[", "]", "\n", "edge_types", "=", "[", "]", "\n", "for", "src_id", ",", "node", "in", "self", ".", "nodes", ".", "items", "(", ")", ":", "\n", "            ", "for", "dest_node", ",", "link_type", "in", "zip", "(", "node", ".", "links", ",", "node", ".", "link_types", ")", ":", "\n", "                ", "edges", ".", "append", "(", "(", "src_id", ",", "dest_node", ".", "node_id", ")", ")", "\n", "edge_types", ".", "append", "(", "link_type", ")", "\n", "", "", "return", "edges", ",", "edge_types", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.utils.graph.Graph.__len__": [[91, 93], ["len"], "methods", ["None"], ["", "def", "__len__", "(", "self", ")", ":", "\n", "        ", "return", "len", "(", "self", ".", "nodes", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.utils.graph.Graph.add_node": [[94, 114], ["ValueError", "graph.Graph.node_spans.append", "graph.Graph.node_texts.append", "ValueError", "graph.Graph.node_spans.append", "graph.Graph.node_texts.append"], "methods", ["None"], ["", "def", "add_node", "(", "self", ",", "node", ")", ":", "\n", "        ", "idx", "=", "node", ".", "node_id", "\n", "if", "idx", "in", "self", ".", "nodes", ":", "\n", "            ", "raise", "ValueError", "(", "\"`node_id` must be unique.\"", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "nodes", "[", "idx", "]", "=", "node", "\n", "\n", "", "if", "node", ".", "type", "==", "\"ROOT\"", ":", "\n", "            ", "if", "self", ".", "root", ":", "\n", "                ", "raise", "ValueError", "(", "\"There can be only one ROOT per document\"", ")", "\n", "", "else", ":", "\n", "                ", "self", ".", "root", "=", "node", "\n", "self", ".", "root", ".", "text", "=", "[", "\"$ROOT$\"", "]", "\n", "self", ".", "node_spans", ".", "append", "(", "None", ")", "\n", "self", ".", "node_texts", ".", "append", "(", "\"ROOT\"", ")", "\n", "\n", "", "", "else", ":", "\n", "# add span to all spans for sampling later", "\n", "            ", "self", ".", "node_spans", ".", "append", "(", "node", ".", "doc_indices", ")", "\n", "self", ".", "node_texts", ".", "append", "(", "node", ".", "text", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.utils.graph.Graph.del_node": [[115, 150], ["dict", "graph.Graph.nodes.items", "graph.Graph.node_spans.remove", "graph.Graph.node_texts.remove", "graph.Graph.del_edge", "graph.Graph.del_edge", "graph.Graph.nodes.values", "graph.Graph.nodes.values"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.utils.graph.Graph.del_edge", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.utils.graph.Graph.del_edge"], ["", "", "def", "del_node", "(", "self", ",", "node_id_del", ")", ":", "\n", "        ", "node_del", "=", "self", ".", "nodes", "[", "node_id_del", "]", "\n", "\n", "# find nodes connected to node chosen for deletion, remove edges", "\n", "linked_src_nodes", "=", "[", "\n", "node", ".", "node_id", "for", "node", "in", "self", ".", "nodes", ".", "values", "(", ")", "if", "node_del", "in", "node", ".", "links", "\n", "]", "\n", "linked_dest_nodes", "=", "[", "\n", "node", ".", "node_id", "for", "node", "in", "self", ".", "nodes", ".", "values", "(", ")", "if", "node", "in", "node_del", ".", "links", "\n", "]", "\n", "for", "src_node_id", "in", "linked_src_nodes", ":", "\n", "# print(src_node_id, \"src_node_id\")", "\n", "# print([link.node_id for link in self.nodes[src_node_id].links],", "\n", "#      'link.node_id for link in self.src_node_id.links')", "\n", "            ", "self", ".", "del_edge", "(", "src", "=", "src_node_id", ",", "dest", "=", "node_del", ".", "node_id", ")", "\n", "", "for", "dest_node_id", "in", "linked_dest_nodes", ":", "\n", "# print(dest_node_id, \"dest_node_id\")", "\n", "# print([link.node_id for link in self.nodes[sampled_node_id].links],", "\n", "#      'link.node_id for link in self.nodes[sampled_node_id].links')", "\n", "            ", "self", ".", "del_edge", "(", "src", "=", "node_del", ".", "node_id", ",", "dest", "=", "dest_node_id", ")", "\n", "\n", "# delete the node from the list", "\n", "", "del", "self", ".", "nodes", "[", "node_id_del", "]", "\n", "\n", "# shift ids to fill gap", "\n", "copy_nodes", "=", "dict", "(", ")", "\n", "for", "node_key", ",", "node", "in", "self", ".", "nodes", ".", "items", "(", ")", ":", "\n", "            ", "if", "node", ".", "node_id", ">", "node_id_del", ":", "\n", "                ", "node", ".", "node_id", "-=", "1", "\n", "", "copy_nodes", "[", "node", ".", "node_id", "]", "=", "node", "\n", "", "self", ".", "nodes", "=", "copy_nodes", "\n", "\n", "# remove span and text from graph", "\n", "self", ".", "node_spans", ".", "remove", "(", "node_del", ".", "doc_indices", ")", "\n", "self", ".", "node_texts", ".", "remove", "(", "node_del", ".", "text", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.utils.graph.Graph.add_edge": [[151, 163], ["src_node.links.append", "src_node.link_types.append", "ValueError"], "methods", ["None"], ["", "def", "add_edge", "(", "self", ",", "src", ",", "dest", ",", "edge_type", ")", ":", "\n", "# if src or dest nodes does not exist", "\n", "        ", "if", "src", "not", "in", "self", ".", "nodes", "or", "dest", "not", "in", "self", ".", "nodes", ":", "\n", "            ", "raise", "ValueError", "(", "\"Nodes should exist before linking.\"", ")", "\n", "# elif type != 'cor':", "\n", "", "src_node", "=", "self", ".", "nodes", "[", "src", "]", "\n", "dest_node", "=", "self", ".", "nodes", "[", "dest", "]", "\n", "src_node", ".", "links", ".", "append", "(", "dest_node", ")", "\n", "src_node", ".", "link_types", ".", "append", "(", "edge_type", ")", "\n", "\n", "Graph", ".", "_node_freqs", "[", "src_node", ".", "type", "]", "+=", "1", "\n", "Graph", ".", "_node_freqs", "[", "dest_node", ".", "type", "]", "+=", "1", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.utils.graph.Graph.del_edge": [[164, 177], ["src_node.links.index", "src_node.links.pop", "src_node.link_types.pop", "ValueError"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.fields.text_field.TextField.index", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.params.Params.pop", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.params.Params.pop"], ["", "def", "del_edge", "(", "self", ",", "src", ",", "dest", ")", ":", "\n", "# if src or dest nodes does not exist", "\n", "        ", "if", "src", "not", "in", "self", ".", "nodes", "or", "dest", "not", "in", "self", ".", "nodes", ":", "\n", "            ", "raise", "ValueError", "(", "\"Nodes should exist before deleting.\"", ")", "\n", "# elif type != 'cor':", "\n", "", "src_node", "=", "self", ".", "nodes", "[", "src", "]", "\n", "dest_node", "=", "self", ".", "nodes", "[", "dest", "]", "\n", "\n", "# get dest index in src_node's edges to be able to del", "\n", "dest_index", "=", "src_node", ".", "links", ".", "index", "(", "dest_node", ")", "\n", "\n", "src_node", ".", "links", ".", "pop", "(", "dest_index", ")", "\n", "src_node", ".", "link_types", ".", "pop", "(", "dest_index", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.utils.graph.Graph.is_empty": [[178, 180], ["None"], "methods", ["None"], ["", "def", "is_empty", "(", "self", ")", ":", "\n", "        ", "return", "not", "self", ".", "nodes", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.utils.graph.Graph.topsort": [[181, 217], ["set", "set.add", "stack.append", "graph.Graph.is_empty", "stack.append", "graph.Graph.topsort.recursive_helper"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.utils.graph.Graph.is_empty"], ["", "def", "topsort", "(", "self", ")", ":", "\n", "        ", "stack", "=", "[", "]", "\n", "visited", "=", "set", "(", ")", "\n", "\n", "def", "recursive_helper", "(", "helper_node", ")", ":", "\n", "            ", "visited", ".", "add", "(", "helper_node", ")", "\n", "for", "neighbor", "in", "helper_node", ".", "links", ":", "\n", "                ", "if", "neighbor", "not", "in", "visited", ":", "\n", "# if neighbor is a verb (head node), start a new recursion", "\n", "                    ", "if", "neighbor", ".", "type", "==", "\"V\"", ":", "\n", "                        ", "stack", ".", "append", "(", "[", "\"$START$\"", ",", "\"MT\"", "]", ")", "\n", "", "recursive_helper", "(", "neighbor", ")", "\n", "if", "neighbor", ".", "type", "==", "\"V\"", ":", "\n", "                        ", "stack", ".", "append", "(", "[", "\"$END$\"", ",", "\"MT\"", "]", ")", "\n", "", "", "", "stack", ".", "append", "(", "[", "\" \"", ".", "join", "(", "helper_node", ".", "text", ")", ",", "helper_node", ".", "type", "]", ")", "\n", "\n", "", "if", "not", "self", ".", "is_empty", "(", ")", ":", "\n", "# start with the ROOT node", "\n", "            ", "stack", ".", "append", "(", "[", "\"$START$\"", ",", "\"MT\"", "]", ")", "\n", "recursive_helper", "(", "self", ".", "root", ")", "\n", "stack", ".", "append", "(", "[", "\"$END$\"", ",", "\"MT\"", "]", ")", "\n", "\n", "for", "node", "in", "self", ".", "nodes", ".", "values", "(", ")", ":", "\n", "# continue with head nodes", "\n", "                ", "if", "node", "not", "in", "visited", "and", "len", "(", "node", ".", "links", ")", ">", "0", ":", "\n", "                    ", "stack", ".", "append", "(", "[", "\"$START$\"", ",", "\"MT\"", "]", ")", "\n", "recursive_helper", "(", "node", ")", "\n", "stack", ".", "append", "(", "[", "\"$END$\"", ",", "\"MT\"", "]", ")", "\n", "\n", "", "", "for", "node", "in", "self", ".", "nodes", ".", "values", "(", ")", ":", "\n", "# end with singletons", "\n", "                ", "if", "node", "not", "in", "visited", ":", "\n", "                    ", "stack", ".", "append", "(", "[", "\"$START$\"", ",", "\"MT\"", "]", ")", "\n", "recursive_helper", "(", "node", ")", "\n", "stack", ".", "append", "(", "[", "\"$END$\"", ",", "\"MT\"", "]", ")", "\n", "", "", "", "return", "stack", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.utils.graph.Graph._make_probs": [[218, 222], ["numpy.array", "scipy.special.softmax", "numpy.max"], "methods", ["None"], ["", "@", "staticmethod", "\n", "def", "_make_probs", "(", "types", ")", ":", "\n", "        ", "frequencies", "=", "np", ".", "array", "(", "[", "Graph", ".", "_node_freqs", "[", "t", "]", "for", "t", "in", "types", "]", ")", "\n", "return", "softmax", "(", "frequencies", "/", "np", ".", "max", "(", "frequencies", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.utils.graph.Graph.sample_node": [[223, 233], ["numpy.random.choice", "graph.Graph.nodes.values", "graph.Graph._make_probs", "predicate"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.utils.graph.Graph._make_probs"], ["", "def", "sample_node", "(", "self", ",", "predicate", "=", "None", ")", "->", "Optional", "[", "Node", "]", ":", "\n", "        ", "sampling_pool", "=", "[", "\n", "node", "\n", "for", "node", "in", "self", ".", "nodes", ".", "values", "(", ")", "\n", "if", "node", ".", "type", "not", "in", "(", "\"V\"", ",", "\"ROOT\"", ")", "and", "(", "predicate", "is", "None", "or", "predicate", "(", "node", ")", ")", "\n", "]", "\n", "if", "not", "sampling_pool", ":", "\n", "            ", "return", "None", "\n", "", "sampling_types", "=", "[", "node", ".", "type", "for", "node", "in", "sampling_pool", "]", "\n", "return", "np", ".", "random", ".", "choice", "(", "sampling_pool", ",", "p", "=", "self", ".", "_make_probs", "(", "sampling_types", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.utils.graph.Graph.set_span": [[234, 241], ["graph.Graph.node_spans.remove", "graph.Graph.node_texts.remove", "graph.Graph.node_spans.append", "graph.Graph.node_texts.append"], "methods", ["None"], ["", "def", "set_span", "(", "self", ",", "node", ",", "span", ")", ":", "\n", "        ", "self", ".", "node_spans", ".", "remove", "(", "node", ".", "doc_indices", ")", "\n", "self", ".", "node_texts", ".", "remove", "(", "node", ".", "text", ")", "\n", "node", ".", "doc_indices", "=", "span", "\n", "node", ".", "text", "=", "self", ".", "original_text", "[", "span", "[", "0", "]", ":", "span", "[", "1", "]", "+", "1", "]", "\n", "self", ".", "node_spans", ".", "append", "(", "node", ".", "doc_indices", ")", "\n", "self", ".", "node_texts", ".", "append", "(", "node", ".", "text", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.utils.graph.Graph.perturb": [[242, 273], ["graph.Graph.is_empty", "numpy.random.choice", "len", "len", "ValueError", "range", "getattr", "ValueError", "getattr.", "graph.Graph.perturbations_applied.append", "print"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.utils.graph.Graph.is_empty"], ["", "def", "perturb", "(", "self", ",", "perturbation_probabilities", ",", "no_perturb_actions", "=", "1", ")", ":", "\n", "        ", "\"\"\" apply number of perturbations equal to the number of sentences\n        input: probability of each perturbation type in K.perturbation_types given we decided to perturb\"\"\"", "\n", "\n", "assert", "len", "(", "K", ".", "perturbation_types", ")", "==", "len", "(", "\n", "perturbation_probabilities", "\n", ")", ",", "\"List of perturbations must be of equal size to list of proportions\"", "\n", "\n", "if", "self", ".", "is_empty", "(", ")", ":", "\n", "            ", "raise", "ValueError", "(", "\"Empty Graph\"", ")", "\n", "\n", "", "for", "perturbation", "in", "np", ".", "random", ".", "choice", "(", "\n", "K", ".", "perturbation_types", ",", "no_perturb_actions", ",", "p", "=", "perturbation_probabilities", "\n", ")", ":", "\n", "            ", "try", ":", "\n", "                ", "perturbation_function", "=", "getattr", "(", "self", ",", "perturbation", ")", "\n", "", "except", "AttributeError", "as", "e", ":", "\n", "                ", "raise", "ValueError", "(", "\n", "\"Perturbation '{}' is not supported\"", ".", "format", "(", "perturbation", ")", "\n", ")", "from", "e", "\n", "", "for", "_", "in", "range", "(", "\n", "MAX_PERTURB_ATTEMPTS", "\n", ")", ":", "# retry perturbation until success or until max attempts", "\n", "                ", "result", "=", "None", "\n", "try", ":", "\n", "                    ", "result", "=", "perturbation_function", "(", ")", "\n", "", "except", "ValueError", "as", "e", ":", "\n", "                    ", "print", "(", "\"Failed applying perturbation {}:\\n{}\"", ".", "format", "(", "perturbation", ",", "e", ")", ",", "file", "=", "sys", ".", "stderr", ")", "\n", "", "if", "result", ":", "# succeeded to apply perturbation, move on", "\n", "                    ", "self", ".", "perturbations_applied", ".", "append", "(", "perturbation", "+", "\" \"", "+", "result", ")", "\n", "break", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.utils.graph.Graph.srl_change_label": [[274, 305], ["zip", "numpy.array", "numpy.array.sum", "numpy.random.choice", "target_label_prob.sum", "numpy.random.choice", "numpy.random.choice", "existing_labels.intersection", "graph.Graph.nodes.values", "graph.Graph.nodes.values", "zip", "hmtl.utils.K.arg_confusion.sum"], "methods", ["None"], ["", "", "", "", "def", "srl_change_label", "(", "self", ")", ":", "\n", "# sample gold label to change based on the overall probability of error on that label", "\n", "        ", "existing_labels", "=", "{", "node", ".", "type", "for", "node", "in", "self", ".", "nodes", ".", "values", "(", ")", "}", "\n", "if", "not", "existing_labels", ".", "intersection", "(", "K", ".", "id2arg_label", ")", ":", "# There are no nodes", "\n", "            ", "return", "False", "\n", "", "labels", ",", "source_label_prob", "=", "zip", "(", "\n", "*", "[", "\n", "(", "label", ",", "weight", ")", "\n", "for", "label", ",", "weight", "in", "zip", "(", "K", ".", "id2arg_label", ",", "K", ".", "arg_confusion", ".", "sum", "(", "axis", "=", "0", ")", ")", "\n", "if", "label", "in", "existing_labels", "\n", "]", "\n", ")", "\n", "source_label_prob", "=", "np", ".", "array", "(", "source_label_prob", ",", "dtype", "=", "float", ")", "\n", "source_label_prob", "/=", "source_label_prob", ".", "sum", "(", ")", "\n", "sampled_source_label", "=", "np", ".", "random", ".", "choice", "(", "labels", ",", "p", "=", "source_label_prob", ")", "\n", "# sample replacement label conditioned on the source label", "\n", "target_label_prob", "=", "K", ".", "arg_confusion", "[", "K", ".", "arg_label2id", "[", "sampled_source_label", "]", "]", "\n", "target_label_prob", "/=", "target_label_prob", ".", "sum", "(", ")", "\n", "sampled_target_label", "=", "np", ".", "random", ".", "choice", "(", "K", ".", "id2arg_label", ",", "p", "=", "target_label_prob", ")", "\n", "# sample node id uniformly among nodes with the source label", "\n", "sampling_pool", "=", "[", "\n", "node", ".", "node_id", "\n", "for", "node", "in", "self", ".", "nodes", ".", "values", "(", ")", "\n", "if", "node", ".", "type", "==", "sampled_source_label", "\n", "]", "\n", "sampled_node_id", "=", "np", ".", "random", ".", "choice", "(", "sampling_pool", ")", "\n", "# replace label", "\n", "sampled_node", "=", "self", ".", "nodes", "[", "sampled_node_id", "]", "\n", "sampled_node", ".", "type", "=", "sampled_target_label", "\n", "sampled_node", ".", "perturbed", "=", "True", "\n", "return", "\"{} type to {}\"", ".", "format", "(", "sampled_node_id", ",", "sampled_target_label", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.utils.graph.Graph.srl_move_arg": [[306, 344], ["graph.Graph.sample_node", "numpy.exp", "numpy.sum", "numpy.random.choice", "graph.Graph.set_span", "len", "numpy.dot", "numpy.linalg.norm", "numpy.max", "set", "set"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.utils.graph.Graph.sample_node", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.utils.graph.Graph.set_span"], ["", "def", "srl_move_arg", "(", "self", ")", ":", "\n", "# sample argument to move", "\n", "        ", "sampled_node", "=", "self", ".", "sample_node", "(", "\n", "lambda", "node", ":", "node", ".", "doc_indices", "[", "1", "]", "-", "node", ".", "doc_indices", "[", "0", "]", "<", "self", ".", "max_span_width", "\n", ")", "\n", "if", "sampled_node", "is", "None", ":", "# no nodes found", "\n", "            ", "return", "False", "\n", "# sample target position among spans of the same length that do not intersect with the source one", "\n", "", "span_embedding", "=", "self", ".", "span_embeddings_by_span", "[", "sampled_node", ".", "doc_indices", "]", "\n", "all_non_intersecting_same_len_spans", "=", "[", "\n", "span", "\n", "for", "span", "in", "self", ".", "all_spans", "\n", "if", "set", "(", "span", ")", "!=", "set", "(", "sampled_node", ".", "doc_indices", ")", "\n", "and", "span", "[", "1", "]", "-", "span", "[", "0", "]", "\n", "==", "sampled_node", ".", "doc_indices", "[", "1", "]", "-", "sampled_node", ".", "doc_indices", "[", "0", "]", "\n", "and", "(", "\n", "span", "[", "1", "]", "<", "sampled_node", ".", "doc_indices", "[", "0", "]", "\n", "or", "span", "[", "0", "]", ">", "sampled_node", ".", "doc_indices", "[", "1", "]", "\n", ")", "\n", "]", "\n", "if", "not", "all_non_intersecting_same_len_spans", ":", "\n", "            ", "return", "False", "\n", "", "embeddings", "=", "[", "\n", "self", ".", "span_embeddings_by_span", "[", "span", "]", "\n", "for", "span", "in", "all_non_intersecting_same_len_spans", "\n", "]", "\n", "similarities", "=", "[", "\n", "np", ".", "dot", "(", "embedding", ",", "span_embedding", ")", "/", "np", ".", "linalg", ".", "norm", "(", "embedding", ")", "\n", "for", "embedding", "in", "embeddings", "\n", "]", "\n", "probabilities", "=", "np", ".", "exp", "(", "similarities", "-", "np", ".", "max", "(", "similarities", ")", ")", "\n", "probabilities", "/=", "np", ".", "sum", "(", "probabilities", ")", "\n", "i", "=", "np", ".", "random", ".", "choice", "(", "len", "(", "all_non_intersecting_same_len_spans", ")", ",", "p", "=", "probabilities", ")", "\n", "sampled_span", "=", "all_non_intersecting_same_len_spans", "[", "i", "]", "\n", "# set new span and text", "\n", "self", ".", "set_span", "(", "sampled_node", ",", "sampled_span", ")", "\n", "sampled_node", ".", "perturbed", "=", "True", "\n", "return", "\"{} span to {}\"", ".", "format", "(", "sampled_node", ".", "node_id", ",", "sampled_span", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.utils.graph.Graph.srl_split_spans": [[345, 392], ["graph.Graph.sample_node", "numpy.random.randint", "graph.Node", "graph.Graph.add_node", "graph.Graph.set_span", "numpy.random.randint", "max", "graph.Graph.add_edge", "graph.Graph.nodes.values"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.utils.graph.Graph.sample_node", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.utils.graph.Graph.add_node", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.utils.graph.Graph.set_span", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.utils.graph.Graph.add_edge"], ["", "def", "srl_split_spans", "(", "self", ")", ":", "\n", "# sample argument to split", "\n", "        ", "sampled_node", "=", "self", ".", "sample_node", "(", "\n", "lambda", "node", ":", "node", ".", "doc_indices", "[", "1", "]", "-", "node", ".", "doc_indices", "[", "0", "]", ">", "2", "\n", ")", "\n", "if", "sampled_node", "is", "None", ":", "# no nodes found or span too short", "\n", "            ", "return", "False", "\n", "# sample splitting point uniformly: first half ends at this point", "\n", "", "first_end", "=", "np", ".", "random", ".", "randint", "(", "\n", "sampled_node", ".", "doc_indices", "[", "0", "]", "+", "1", ",", "sampled_node", ".", "doc_indices", "[", "1", "]", "\n", ")", "\n", "second_start", "=", "first_end", "+", "1", "\n", "if", "sampled_node", ".", "doc_indices", "[", "1", "]", "-", "sampled_node", ".", "doc_indices", "[", "0", "]", ">=", "3", ":", "\n", "            ", "second_start", "+=", "np", ".", "random", ".", "randint", "(", "\n", "0", ",", "1", "\n", ")", "# sometimes separate the splits by one word", "\n", "", "if", "(", "\n", "(", "sampled_node", ".", "doc_indices", "[", "0", "]", ",", "first_end", ")", "not", "in", "self", ".", "span_embeddings_by_span", "\n", "or", "(", "second_start", ",", "sampled_node", ".", "doc_indices", "[", "1", "]", ")", "\n", "not", "in", "self", ".", "span_embeddings_by_span", "\n", ")", ":", "\n", "            ", "return", "False", "# For some reason one of the spans has no embedding (maybe crosses sentences)", "\n", "# create new node of the same argument type for second half", "\n", "", "new_node_id", "=", "max", "(", "self", ".", "nodes", ")", "+", "1", "\n", "new_node", "=", "Node", "(", "\n", "new_node_id", ",", "\n", "sampled_node", ".", "type", ",", "\n", "(", "second_start", ",", "sampled_node", ".", "doc_indices", "[", "1", "]", ")", ",", "\n", "sampled_node", ".", "text", "[", "second_start", "-", "sampled_node", ".", "doc_indices", "[", "0", "]", ":", "]", ",", "\n", ")", "\n", "self", ".", "add_node", "(", "new_node", ")", "\n", "# trim old node to first half", "\n", "self", ".", "set_span", "(", "sampled_node", ",", "(", "sampled_node", ".", "doc_indices", "[", "0", "]", ",", "first_end", ")", ")", "\n", "# copy link from Verb nodes", "\n", "linked_src_verb_node_ids", "=", "[", "\n", "node", ".", "node_id", "\n", "for", "node", "in", "self", ".", "nodes", ".", "values", "(", ")", "\n", "if", "node", ".", "type", "==", "\"V\"", "and", "sampled_node", "in", "node", ".", "links", "\n", "]", "\n", "for", "verb_node_id", "in", "linked_src_verb_node_ids", ":", "\n", "            ", "self", ".", "add_edge", "(", "src", "=", "verb_node_id", ",", "dest", "=", "new_node_id", ",", "edge_type", "=", "\"srl\"", ")", "\n", "", "sampled_node", ".", "perturbed", "=", "new_node", ".", "perturbed", "=", "True", "\n", "return", "\"{} to {} and {} ({})\"", ".", "format", "(", "\n", "sampled_node", ".", "node_id", ",", "\n", "sampled_node", ".", "doc_indices", ",", "\n", "new_node", ".", "doc_indices", ",", "\n", "new_node_id", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.utils.graph.Graph.srl_merge_spans": [[394, 421], ["graph.Graph.set_span", "graph.Graph.del_node", "random.sample", "graph.Graph.nodes.values"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.utils.graph.Graph.set_span", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.utils.graph.Graph.del_node"], ["", "def", "srl_merge_spans", "(", "self", ")", ":", "\n", "# find pairs of argument nodes with at most one separating token", "\n", "        ", "argument_nodes", "=", "[", "\n", "node", "for", "node", "in", "self", ".", "nodes", ".", "values", "(", ")", "if", "node", ".", "type", "not", "in", "(", "\"V\"", ",", "\"ROOT\"", ")", "\n", "]", "\n", "adjacent_pairs", "=", "[", "\n", "(", "node1", ",", "node2", ")", "\n", "for", "node1", "in", "argument_nodes", "\n", "for", "node2", "in", "argument_nodes", "\n", "if", "1", "<=", "node2", ".", "doc_indices", "[", "0", "]", "-", "node1", ".", "doc_indices", "[", "1", "]", "<=", "2", "\n", "and", "node1", ".", "doc_indices", "[", "1", "]", "-", "node1", ".", "doc_indices", "[", "0", "]", "<", "self", ".", "max_span_width", "\n", "and", "(", "node1", ".", "doc_indices", "[", "0", "]", ",", "node2", ".", "doc_indices", "[", "1", "]", ")", "\n", "in", "self", ".", "span_embeddings_by_span", "\n", "]", "\n", "if", "not", "adjacent_pairs", ":", "\n", "            ", "return", "False", "\n", "# sample a pair uniformly among them", "\n", "", "sampled_node1", ",", "sampled_node2", "=", "random", ".", "sample", "(", "adjacent_pairs", ",", "1", ")", "[", "0", "]", "\n", "# merge them by expanding the first and dropping the second", "\n", "self", ".", "set_span", "(", "\n", "sampled_node1", ",", "(", "sampled_node1", ".", "doc_indices", "[", "0", "]", ",", "sampled_node2", ".", "doc_indices", "[", "1", "]", ")", "\n", ")", "\n", "# drop second node including all edges", "\n", "self", ".", "del_node", "(", "sampled_node2", ".", "node_id", ")", "\n", "sampled_node1", ".", "perturbed", "=", "True", "\n", "return", "\"{} and {} to {}\"", ".", "format", "(", "\n", "sampled_node1", ".", "node_id", ",", "sampled_node2", ".", "node_id", ",", "sampled_node1", ".", "doc_indices", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.utils.graph.Graph.srl_change_boundary": [[423, 451], ["graph.Graph.sample_node", "random.randint", "random.randint", "graph.Graph.set_span", "max", "min", "min", "len", "len"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.utils.graph.Graph.sample_node", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.utils.graph.Graph.set_span"], ["", "def", "srl_change_boundary", "(", "self", ")", ":", "\n", "# sample argument to change the boundaries of", "\n", "        ", "sampled_node", "=", "self", ".", "sample_node", "(", ")", "\n", "if", "sampled_node", "is", "None", ":", "# no nodes found", "\n", "            ", "return", "False", "\n", "# sample shifted boundaries", "\n", "", "start", "=", "random", ".", "randint", "(", "\n", "max", "(", "(", "0", ",", "sampled_node", ".", "doc_indices", "[", "0", "]", "-", "5", ")", ")", ",", "\n", "min", "(", "sampled_node", ".", "doc_indices", "[", "1", "]", "+", "1", ",", "len", "(", "self", ".", "original_text", ")", ")", ",", "\n", ")", "\n", "end", "=", "random", ".", "randint", "(", "\n", "start", ",", "\n", "min", "(", "\n", "(", "\n", "sampled_node", ".", "doc_indices", "[", "1", "]", "+", "5", ",", "\n", "start", "+", "self", ".", "max_span_width", ",", "\n", "len", "(", "self", ".", "original_text", ")", "+", "1", ",", "\n", ")", "\n", ")", ",", "\n", ")", "\n", "if", "(", "start", ",", "end", ")", "==", "sampled_node", ".", "doc_indices", "or", "(", "\n", "start", ",", "\n", "end", ",", "\n", ")", "not", "in", "self", ".", "span_embeddings_by_span", ":", "\n", "            ", "return", "False", "# not changed", "\n", "", "self", ".", "set_span", "(", "sampled_node", ",", "(", "start", ",", "end", ")", ")", "\n", "sampled_node", ".", "perturbed", "=", "True", "\n", "return", "\"{} to {}\"", ".", "format", "(", "sampled_node", ".", "node_id", ",", "sampled_node", ".", "doc_indices", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.utils.graph.Graph.srl_add_arg": [[452, 498], ["numpy.random.choice", "set", "numpy.random.choice", "graph.Node", "graph.Graph.add_node", "numpy.random.choice", "graph.Graph.add_edge", "list", "list", "random.sample", "max", "graph.Graph._make_probs", "set.intersection", "zip", "graph.Graph.nodes.values", "range"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.utils.graph.Graph.add_node", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.utils.graph.Graph.add_edge", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.utils.graph.Graph._make_probs"], ["", "def", "srl_add_arg", "(", "self", ")", ":", "\n", "# sample node type", "\n", "        ", "sampling_types", "=", "[", "t", "for", "t", "in", "K", ".", "id2node_type", "if", "t", "not", "in", "(", "\"V\"", ",", "\"ROOT\"", ")", "]", "\n", "sampled_ntype", "=", "np", ".", "random", ".", "choice", "(", "\n", "sampling_types", ",", "p", "=", "self", ".", "_make_probs", "(", "sampling_types", ")", "\n", ")", "\n", "# sample node text", "\n", "doc_span_lengths", "=", "set", "(", "list", "(", "span", "[", "1", "]", "-", "span", "[", "0", "]", "for", "span", "in", "self", ".", "node_spans", "if", "span", ")", ")", "\n", "#doc_span_lengths_all = list(span[1] - span[0] for span in self.node_spans if span)", "\n", "#print(doc_span_lengths, \"doc_span_lengths\")", "\n", "#print(doc_span_lengths_all, 'doc_span_lengths_all')", "\n", "sampled_span_length", "=", "np", ".", "random", ".", "choice", "(", "\n", "list", "(", "doc_span_lengths", ".", "intersection", "(", "range", "(", "self", ".", "max_span_width", ")", ")", ")", "\n", ")", "\n", "#print(sampled_span_length, \"sampled_span_length\")", "\n", "#print(len(self.node_spans), len(self.node_texts), \"len(self.node_spans), len(self.node_texts)\")", "\n", "\n", "all_spans_filtered", "=", "[", "\n", "(", "span", ",", "text", ")", "\n", "for", "span", ",", "text", "in", "zip", "(", "self", ".", "node_spans", ",", "self", ".", "node_texts", ")", "\n", "if", "span", "\n", "]", "\n", "#print(all_spans_filtered, \"all_spans_filtered\")", "\n", "\n", "\n", "\n", "all_spans_filtered_by_len", "=", "[", "\n", "(", "span_text", "[", "0", "]", ",", "span_text", "[", "1", "]", ")", "\n", "for", "span_text", "in", "all_spans_filtered", "\n", "if", "span_text", "[", "0", "]", "[", "1", "]", "-", "span_text", "[", "0", "]", "[", "0", "]", "==", "sampled_span_length", "\n", "]", "\n", "#print(all_spans_filtered_by_len, \"all_spans_filtered_by_len\")", "\n", "\n", "sampled_span", ",", "sampled_text", "=", "random", ".", "sample", "(", "all_spans_filtered_by_len", ",", "1", ")", "[", "0", "]", "\n", "# get node id", "\n", "node_id", "=", "max", "(", "self", ".", "nodes", ")", "+", "1", "\n", "node", "=", "Node", "(", "node_id", ",", "sampled_ntype", ",", "sampled_span", ",", "sampled_text", ")", "\n", "self", ".", "add_node", "(", "node", ")", "\n", "# sample Verb node to connect new node to", "\n", "sampling_pool_predicates", "=", "[", "\n", "node", ".", "node_id", "for", "node", "in", "self", ".", "nodes", ".", "values", "(", ")", "if", "node", ".", "type", "==", "\"V\"", "\n", "]", "\n", "sampled_verb_node_id", "=", "np", ".", "random", ".", "choice", "(", "sampling_pool_predicates", ")", "\n", "self", ".", "add_edge", "(", "src", "=", "sampled_verb_node_id", ",", "dest", "=", "node_id", ",", "edge_type", "=", "\"srl\"", ")", "\n", "node", ".", "perturbed", "=", "True", "\n", "return", "\"{} ({})\"", ".", "format", "(", "node", ".", "doc_indices", ",", "node", ".", "node_id", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.utils.graph.Graph.srl_drop_arg": [[499, 507], ["graph.Graph.sample_node", "graph.Graph.del_node"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.utils.graph.Graph.sample_node", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.utils.graph.Graph.del_node"], ["", "def", "srl_drop_arg", "(", "self", ")", ":", "\n", "# sample argument to drop", "\n", "        ", "sampled_node", "=", "self", ".", "sample_node", "(", ")", "\n", "if", "sampled_node", "is", "None", ":", "# no nodes found", "\n", "            ", "return", "False", "\n", "# delete node", "\n", "", "self", ".", "del_node", "(", "sampled_node", ".", "node_id", ")", "\n", "return", "\"{} ({})\"", ".", "format", "(", "sampled_node", ".", "doc_indices", ",", "sampled_node", ".", "node_id", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.utils.graph.Graph.coref_add_ant": [[508, 523], ["random.sample", "graph.Graph.add_edge", "len", "graph.Graph.nodes.values"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.utils.graph.Graph.add_edge"], ["", "def", "coref_add_ant", "(", "self", ")", ":", "\n", "# sample node ids", "\n", "        ", "sampling_pool", "=", "[", "\n", "node", ".", "node_id", "\n", "for", "node", "in", "self", ".", "nodes", ".", "values", "(", ")", "\n", "if", "node", ".", "type", "not", "in", "(", "\"V\"", ",", "\"ROOT\"", ")", "\n", "]", "\n", "if", "len", "(", "sampling_pool", ")", "<", "2", ":", "\n", "            ", "return", "False", "\n", "", "sampled_node_id_1", ",", "sampled_node_id_2", "=", "random", ".", "sample", "(", "sampling_pool", ",", "2", ")", "\n", "self", ".", "add_edge", "(", "src", "=", "sampled_node_id_1", ",", "dest", "=", "sampled_node_id_2", ",", "edge_type", "=", "\"cor\"", ")", "\n", "self", ".", "nodes", "[", "sampled_node_id_1", "]", ".", "perturbed", "=", "self", ".", "nodes", "[", "\n", "sampled_node_id_2", "\n", "]", ".", "perturbed", "=", "True", "\n", "return", "\"{} and {}\"", ".", "format", "(", "sampled_node_id_1", ",", "sampled_node_id_2", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.utils.graph.Graph.coref_drop_ant": [[524, 550], ["graph.Graph.del_edge", "random.sample", "graph.Graph.nodes.values", "all_coref_edges.append", "enumerate"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.utils.graph.Graph.del_edge"], ["", "def", "coref_drop_ant", "(", "self", ")", ":", "\n", "# sample node ids", "\n", "        ", "all_srl_arg_nodes", "=", "[", "\n", "node", ".", "node_id", "\n", "for", "node", "in", "self", ".", "nodes", ".", "values", "(", ")", "\n", "if", "node", ".", "type", "not", "in", "(", "\"V\"", ",", "\"ROOT\"", ")", "\n", "]", "\n", "all_coref_edges", "=", "[", "]", "\n", "for", "srl_arg_node_id", "in", "all_srl_arg_nodes", ":", "\n", "            ", "srl_arg_node", "=", "self", ".", "nodes", "[", "srl_arg_node_id", "]", "\n", "coref_edges_indices", "=", "[", "\n", "idx", "\n", "for", "idx", ",", "edge_type", "in", "enumerate", "(", "srl_arg_node", ".", "link_types", ")", "\n", "if", "edge_type", "==", "\"cor\"", "\n", "]", "\n", "for", "coref_edge_idx", "in", "coref_edges_indices", ":", "\n", "                ", "all_coref_edges", ".", "append", "(", "(", "srl_arg_node_id", ",", "coref_edge_idx", ")", ")", "\n", "", "", "if", "not", "all_coref_edges", ":", "\n", "            ", "return", "False", "\n", "", "src_id", ",", "dest_index", "=", "random", ".", "sample", "(", "all_coref_edges", ",", "1", ")", "[", "0", "]", "\n", "src_node", "=", "self", ".", "nodes", "[", "src_id", "]", "\n", "dest_id", "=", "src_node", ".", "links", "[", "dest_index", "]", ".", "node_id", "\n", "self", ".", "del_edge", "(", "src_id", ",", "dest_id", ")", "\n", "dest_node", "=", "self", ".", "nodes", "[", "dest_id", "]", "\n", "src_node", ".", "perturbed", "=", "dest_node", ".", "perturbed", "=", "True", "\n", "return", "\"{} and {}\"", ".", "format", "(", "src_id", ",", "dest_id", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.utils.graph.Graph.handle_coref_chains": [[551, 564], ["set", "set.add", "set.add", "node.append", "node.append"], "methods", ["None"], ["", "@", "staticmethod", "\n", "def", "handle_coref_chains", "(", "g", ",", "ling", ")", ":", "\n", "        ", "mentions", "=", "set", "(", ")", "\n", "for", "e", "in", "g", "[", "\"edges\"", "]", ":", "\n", "            ", "if", "e", "[", "\"type\"", "]", "==", "\"cor\"", ":", "\n", "                ", "mentions", ".", "add", "(", "e", "[", "\"nids\"", "]", "[", "0", "]", ")", "\n", "mentions", ".", "add", "(", "e", "[", "\"nids\"", "]", "[", "1", "]", ")", "\n", "", "", "for", "node", "in", "ling", ":", "\n", "            ", "if", "node", "[", "0", "]", "in", "mentions", ":", "\n", "                ", "node", ".", "append", "(", "True", ")", "\n", "", "else", ":", "\n", "                ", "node", ".", "append", "(", "False", ")", "\n", "", "", "return", "ling", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.utils.graph.Graph.build_graph": [[565, 590], ["Graph.Graph", "Graph.Graph.topsort", "Graph.Graph.handle_coref_chains", "Graph.Node", "Graph.Graph.add_node", "Graph.Graph.add_edge", "g.get"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.utils.graph.Graph.topsort", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.utils.graph.Graph.handle_coref_chains", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.utils.graph.Graph.add_node", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.utils.graph.Graph.add_edge", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.params.Params.get"], ["", "@", "staticmethod", "\n", "def", "build_graph", "(", "\n", "dockey", ",", "\n", "g", ",", "\n", "max_span_width", "=", "10", ",", "\n", "all_spans", "=", "None", ",", "\n", "span_embeddings", "=", "None", ",", "\n", "original_text", "=", "None", ",", "\n", ")", ":", "\n", "        ", "graph", "=", "Graph", "(", "\n", "dockey", ",", "\n", "metadata", "=", "g", ".", "get", "(", "\"metadata\"", ")", ",", "\n", "max_span_width", "=", "max_span_width", ",", "\n", "all_spans", "=", "all_spans", ",", "\n", "span_embeddings", "=", "span_embeddings", ",", "\n", "original_text", "=", "original_text", ",", "\n", ")", "\n", "for", "n", "in", "g", "[", "\"nodes\"", "]", ":", "\n", "            ", "node", "=", "Node", "(", "n", "[", "\"node_id\"", "]", ",", "n", "[", "\"type\"", "]", ",", "n", "[", "\"doc_indices\"", "]", ",", "n", "[", "\"text\"", "]", ")", "\n", "graph", ".", "add_node", "(", "node", ")", "\n", "", "for", "e", "in", "g", "[", "\"edges\"", "]", ":", "\n", "            ", "graph", ".", "add_edge", "(", "e", "[", "\"nids\"", "]", "[", "0", "]", ",", "e", "[", "\"nids\"", "]", "[", "1", "]", ",", "e", "[", "\"type\"", "]", ")", "\n", "", "ling", "=", "graph", ".", "topsort", "(", ")", "\n", "ling", "=", "Graph", ".", "handle_coref_chains", "(", "g", ",", "ling", ")", "\n", "return", "graph", ",", "ling", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.utils.graph.Graph.to_json": [[591, 607], ["graph.Graph.nodes.values", "nodes.append", "zip", "edges.append"], "methods", ["None"], ["", "def", "to_json", "(", "self", ")", ":", "\n", "        ", "nodes", "=", "[", "]", "\n", "edges", "=", "[", "]", "\n", "for", "node", "in", "self", ".", "nodes", ".", "values", "(", ")", ":", "\n", "            ", "nodes", ".", "append", "(", "\n", "{", "\n", "\"node_id\"", ":", "node", ".", "node_id", ",", "\n", "\"type\"", ":", "node", ".", "type", ",", "\n", "\"doc_indices\"", ":", "node", ".", "doc_indices", ",", "\n", "\"text\"", ":", "node", ".", "text", ",", "\n", "\"perturbed\"", ":", "node", ".", "perturbed", ",", "\n", "}", "\n", ")", "\n", "for", "dest", ",", "etype", "in", "zip", "(", "node", ".", "links", ",", "node", ".", "link_types", ")", ":", "\n", "                ", "edges", ".", "append", "(", "{", "\"nids\"", ":", "[", "node", ".", "node_id", ",", "dest", ".", "node_id", "]", ",", "\"type\"", ":", "etype", "}", ")", "\n", "", "", "return", "{", "\"nodes\"", ":", "nodes", ",", "\"edges\"", ":", "edges", "}", "\n", "", "", ""]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.utils.constants.r": [[8, 10], ["enumerate"], "function", ["None"], ["def", "r", "(", "d", ")", ":", "\n", "    ", "return", "{", "w", ":", "i", "for", "i", ",", "w", "in", "enumerate", "(", "d", ")", "}", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.utils.constants.set_config": [[12, 20], ["ValueError", "open", "json.load"], "function", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.token_embedders.bert_token_embedder.PretrainedBertModel.load"], ["", "def", "set_config", "(", "path", ")", ":", "\n", "    ", "global", "_CONFIG", "\n", "if", "_CONFIG", ":", "\n", "        ", "raise", "ValueError", "(", "\"Configuration already set.\"", ")", "\n", "", "else", ":", "\n", "        ", "with", "open", "(", "path", ",", "\"r\"", ")", "as", "f", ":", "\n", "            ", "_CONFIG", "=", "json", ".", "load", "(", "f", ")", "\n", "", "", "return", "_CONFIG", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.utils.constants.get_config": [[22, 27], ["ValueError"], "function", ["None"], ["", "def", "get_config", "(", ")", ":", "\n", "    ", "if", "not", "_CONFIG", ":", "\n", "        ", "raise", "ValueError", "(", "\"Configuration not set.\"", ")", "\n", "", "else", ":", "\n", "        ", "return", "_CONFIG", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.utils.builder.get_map_loc": [[24, 33], ["torch.cuda.is_available", "torch.device", "torch.device"], "function", ["None"], ["def", "get_map_loc", "(", ")", ":", "\n", "    ", "\"\"\"\n    Chooses device for tensors to be mapped based on the\n    presence/absence of GPUs on a machine\n    \"\"\"", "\n", "if", "torch", ".", "cuda", ".", "is_available", "(", ")", ":", "\n", "        ", "return", "torch", ".", "device", "(", "\"cuda:0\"", ")", "\n", "", "else", ":", "\n", "        ", "return", "torch", ".", "device", "(", "\"cpu\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.utils.builder.init_model": [[35, 78], ["allennlp.data.Vocabulary.from_files", "allennlp.common.params.Params.from_file", "builder.init_model.get_reader_params"], "function", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.data.vocabulary.Vocabulary.from_files", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.params.Params.from_file"], ["", "", "def", "init_model", "(", "serialization_dir", ",", "device", ",", "epoch", "=", "-", "1", ",", "task", "=", "\"coref\"", ")", ":", "\n", "    ", "\"\"\"\n    Load the best model from the `serialization_dir`\n    If `epoch` is not -1, this method loads the model weights after the i-th epoch.\n    \"\"\"", "\n", "vocab", "=", "Vocabulary", ".", "from_files", "(", "os", ".", "path", ".", "join", "(", "serialization_dir", ",", "\"./vocabulary\"", ")", ")", "\n", "params", "=", "Params", ".", "from_file", "(", "os", ".", "path", ".", "join", "(", "serialization_dir", ",", "\"./config.json\"", ")", ")", "\n", "\n", "def", "get_reader_params", "(", "task_name", ")", ":", "\n", "        ", "task_name", "=", "\"task_{}\"", ".", "format", "(", "task_name", ")", "\n", "reader_params", "=", "params", ".", "get", "(", "task_name", ")", "[", "\"data_params\"", "]", ".", "get", "(", "\n", "\"validation_dataset_reader\"", ",", "None", "\n", ")", "\n", "if", "reader_params", "is", "None", ":", "\n", "            ", "reader_params", "=", "params", ".", "pop", "(", "task_name", ")", "[", "\"data_params\"", "]", "[", "\"dataset_reader\"", "]", "\n", "", "return", "reader_params", "\n", "\n", "", "reader_params", "=", "get_reader_params", "(", "task", ")", "\n", "if", "task", "==", "\"coref\"", ":", "\n", "        ", "reader_params", ".", "pop", "(", "\"type\"", ")", "\n", "reader", "=", "CorefConllReader", ".", "from_params", "(", "reader_params", ")", "\n", "iterator_params", "=", "params", ".", "pop", "(", "\"iterators\"", ")", "[", "\"iterator_coref\"", "]", "\n", "iterator", "=", "DataIterator", ".", "from_params", "(", "iterator_params", ")", "\n", "best_model", "=", "(", "\n", "\"best_coref.th\"", "if", "epoch", "==", "-", "1", "else", "\"model_state_{}.th\"", ".", "format", "(", "epoch", ")", "\n", ")", "\n", "", "elif", "task", "==", "\"srl\"", ":", "\n", "        ", "reader_params", ".", "pop", "(", "\"type\"", ")", "\n", "reader", "=", "SrlReader", ".", "from_params", "(", "reader_params", ")", "\n", "iterator_params", "=", "params", ".", "pop", "(", "\"iterators\"", ")", "[", "\"iterator_srl\"", "]", "\n", "iterator", "=", "DataIterator", ".", "from_params", "(", "iterator_params", ")", "\n", "best_model", "=", "\"best_srl.th\"", "if", "epoch", "==", "-", "1", "else", "\"model_state_{}.th\"", ".", "format", "(", "epoch", ")", "\n", "", "else", ":", "\n", "        ", "raise", "ValueError", "(", "\"Undefined task.\"", ")", "\n", "\n", "", "model_params", "=", "params", ".", "pop", "(", "\"model\"", ")", "\n", "model", "=", "Model", ".", "from_params", "(", "\n", "vocab", "=", "vocab", ",", "params", "=", "model_params", ".", "duplicate", "(", ")", ",", "regularizer", "=", "None", "\n", ")", "\n", "state", "=", "torch", ".", "load", "(", "os", ".", "path", ".", "join", "(", "serialization_dir", ",", "best_model", ")", ",", "map_location", "=", "device", ")", "\n", "model", ".", "load_state_dict", "(", "state", ")", "\n", "model", "=", "model", ".", "to", "(", "device", ")", "\n", "return", "reader", ",", "iterator", ",", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.utils.builder.get_span_encoding": [[80, 117], ["reader.text_to_instance", "reader.text_to_instance.index_fields", "iterator", "next", "model.forward", "type", "allennlp.nn.util.move_to_device", "list", "int", "torch.zeros", "builder.get_bert_reps", "builder.get_map_loc", "builder.get_map_loc"], "function", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.coreference_resolution.conll.ConllCorefReader.text_to_instance", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.data.instance.Instance.index_fields", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.graph_util.graph_encoder_gat.GAT.forward", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.nn.util.move_to_device", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.utils.builder.get_bert_reps", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.utils.builder.get_map_loc", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.utils.builder.get_map_loc"], ["", "def", "get_span_encoding", "(", "key", ",", "zero_span_rep", "=", "None", ")", ":", "\n", "    ", "\"\"\"\n    Input: document key\n    Output: all possible span tuples and their encodings\n    \"\"\"", "\n", "instance", "=", "reader", ".", "text_to_instance", "(", "combined_json", "[", "key", "]", "[", "\"sentences\"", "]", ")", "\n", "instance", ".", "index_fields", "(", "model", ".", "vocab", ")", "\n", "generator", "=", "iterator", "(", "instances", "=", "[", "instance", "]", ")", "\n", "batch", "=", "next", "(", "generator", ")", "\n", "if", "type", "(", "get_map_loc", "(", ")", ".", "index", ")", "==", "int", ":", "\n", "        ", "batch", "=", "move_to_device", "(", "batch", ",", "get_map_loc", "(", ")", ".", "index", ")", "\n", "", "if", "zero_span_rep", "is", "not", "None", ":", "# for debugging", "\n", "        ", "assert", "(", "\n", "zero_span_rep", "%", "2", "==", "0", "\n", ")", ",", "\"zero_span_rep must be even as it corresponds to concat(endpoint, attended)\"", "\n", "shape", "=", "list", "(", "batch", "[", "\"spans\"", "]", ".", "shape", ")", "\n", "shape", "[", "-", "1", "]", "=", "int", "(", "zero_span_rep", "/", "2", ")", "\n", "zeros", "=", "torch", ".", "zeros", "(", "shape", ")", "\n", "return", "{", "\n", "\"original_text\"", ":", "batch", "[", "\"metadata\"", "]", "[", "0", "]", "[", "\"original_text\"", "]", ",", "\n", "\"all_spans\"", ":", "batch", "[", "\"spans\"", "]", ",", "\n", "\"endpoint_span_embeddings\"", ":", "zeros", ",", "\n", "\"attended_span_embeddings\"", ":", "zeros", ",", "\n", "\"roberta_embeddings\"", ":", "zeros", ",", "\n", "}", "\n", "", "output", "=", "model", ".", "forward", "(", "tensor_batch", "=", "batch", ",", "task_name", "=", "\"coref\"", ",", "for_training", "=", "False", ")", "\n", "reps", "=", "{", "\n", "\"original_text\"", ":", "batch", "[", "\"metadata\"", "]", "[", "0", "]", "[", "\"original_text\"", "]", ",", "\n", "\"all_spans\"", ":", "output", "[", "\"all_spans\"", "]", ",", "\n", "\"endpoint_span_embeddings\"", ":", "output", "[", "\"endpoint_span_embeddings\"", "]", ",", "\n", "\"attended_span_embeddings\"", ":", "output", "[", "\"attended_span_embeddings\"", "]", ",", "\n", "}", "\n", "if", "include_bert", ":", "\n", "        ", "reps", "[", "\"roberta_embeddings\"", "]", "=", "get_bert_reps", "(", "\n", "combined_json", "[", "key", "]", "[", "\"sentences\"", "]", ",", "output", "[", "\"all_spans\"", "]", "[", "0", "]", "\n", ")", "\n", "", "return", "reps", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.utils.builder.get_bert_reps": [[119, 138], ["roberta_tokenizer.encode", "torch.stack().unsqueeze", "len", "torch.tensor().view", "torch.tensor().unsqueeze", "torch.no_grad", "last_hidden_states.view.view", "torch.max", "reps.append", "len", "roberta_model", "torch.stack", "math.ceil", "torch.tensor", "torch.tensor", "len"], "function", ["None"], ["", "def", "get_bert_reps", "(", "sentences", ",", "all_spans", ")", ":", "\n", "    ", "reps", "=", "[", "]", "\n", "input_ids", "=", "roberta_tokenizer", ".", "encode", "(", "\n", "[", "token", "for", "sentence", "in", "sentences", "for", "token", "in", "sentence", "]", ",", "\n", "add_special_tokens", "=", "False", ",", "\n", ")", "\n", "if", "len", "(", "input_ids", ")", ">", "512", ":", "\n", "        ", "to_fill", "=", "math", ".", "ceil", "(", "len", "(", "input_ids", ")", "/", "512", ")", "*", "512", "-", "len", "(", "input_ids", ")", "\n", "input_ids", "=", "input_ids", "+", "[", "roberta_tokenizer", ".", "pad_token_id", "]", "*", "to_fill", "\n", "input_ids", "=", "torch", ".", "tensor", "(", "input_ids", ")", ".", "view", "(", "-", "1", ",", "512", ")", "\n", "", "else", ":", "\n", "        ", "input_ids", "=", "torch", ".", "tensor", "(", "input_ids", ")", ".", "unsqueeze", "(", "0", ")", "\n", "", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "        ", "last_hidden_states", "=", "roberta_model", "(", "input_ids", ")", "[", "0", "]", "\n", "last_hidden_states", "=", "last_hidden_states", ".", "view", "(", "1", ",", "-", "1", ",", "768", ")", "\n", "", "for", "(", "start", ",", "end", ")", "in", "all_spans", ":", "\n", "        ", "pooled", ",", "_", "=", "torch", ".", "max", "(", "last_hidden_states", "[", "0", "]", "[", "start", ":", "end", "+", "1", "]", ",", "dim", "=", "0", ")", "\n", "reps", ".", "append", "(", "pooled", ")", "\n", "", "return", "torch", ".", "stack", "(", "reps", ")", ".", "unsqueeze", "(", "0", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.utils.builder.read_json": [[140, 143], ["open", "json.load"], "function", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.token_embedders.bert_token_embedder.PretrainedBertModel.load"], ["", "def", "read_json", "(", "path", ")", ":", "\n", "    ", "with", "open", "(", "path", ",", "\"r\"", ",", "encoding", "=", "\"utf-8\"", ")", "as", "f", ":", "\n", "        ", "return", "json", ".", "load", "(", "f", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.utils.builder.init_span_embedder": [[145, 155], ["builder.read_json", "builder.init_model", "model.eval", "os.path.join", "builder.get_map_loc", "transformers.RobertaTokenizer.from_pretrained", "transformers.RobertaModel.from_pretrained"], "function", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.utils.builder.read_json", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.utils.builder.init_model", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.dataset_readers.multiprocess_dataset_reader.QIterable.join", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.utils.builder.get_map_loc"], ["", "", "def", "init_span_embedder", "(", "serialization_dir", ",", "combined_json_path", ")", ":", "\n", "    ", "global", "combined_json", ",", "reader", ",", "iterator", ",", "model", "\n", "global", "roberta_tokenizer", ",", "roberta_model", "\n", "\n", "combined_json", "=", "read_json", "(", "os", ".", "path", ".", "join", "(", "serialization_dir", ",", "combined_json_path", ")", ")", "\n", "reader", ",", "iterator", ",", "model", "=", "init_model", "(", "serialization_dir", ",", "get_map_loc", "(", ")", ")", "\n", "model", ".", "eval", "(", ")", "\n", "if", "include_bert", ":", "\n", "        ", "roberta_tokenizer", "=", "RobertaTokenizer", ".", "from_pretrained", "(", "\"roberta-base\"", ")", "\n", "roberta_model", "=", "RobertaModel", ".", "from_pretrained", "(", "\"roberta-base\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.utils.builder.init_pretrained_model": [[157, 168], ["builder.init_model", "builder.init_model"], "function", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.utils.builder.init_model", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.utils.builder.init_model"], ["", "", "def", "init_pretrained_model", "(", "serialization_dir", ",", "device", ")", ":", "\n", "    ", "coref_reader", ",", "coref_iterator", ",", "coref_model", "=", "init_model", "(", "\n", "serialization_dir", ",", "device", ",", "task", "=", "\"coref\"", "\n", ")", "\n", "srl_reader", ",", "srl_iterator", ",", "srl_model", "=", "init_model", "(", "\n", "serialization_dir", ",", "device", ",", "task", "=", "\"srl\"", "\n", ")", "\n", "return", "{", "\n", "\"readers\"", ":", "{", "\"coref\"", ":", "coref_reader", ",", "\"srl\"", ":", "srl_reader", "}", ",", "\n", "\"iterators\"", ":", "{", "\"coref\"", ":", "coref_iterator", ",", "\"srl\"", ":", "srl_iterator", "}", ",", "\n", "\"models\"", ":", "{", "\"coref\"", ":", "coref_model", ",", "\"srl\"", ":", "srl_model", "}", ",", "\n", "}", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.training.multi_task_trainer.MultiTaskTrainer.__init__": [[36, 144], ["len", "allennlp.training.tensorboard_writer.TensorboardWriter", "allennlp.training.optimizers.Optimizer.from_params", "allennlp.training.learning_rate_schedulers.LearningRateScheduler.from_params", "allennlp.common.checks.check_for_gpu", "multi_task_trainer.MultiTaskTrainer._model.cuda", "multi_task_trainer.MultiTaskTrainer._model.named_parameters", "copy.deepcopy", "copy.deepcopy"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.tokenizers.tokenizer.Tokenizer.from_params", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.tokenizers.tokenizer.Tokenizer.from_params", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.checks.check_for_gpu"], ["    ", "def", "__init__", "(", "\n", "self", ",", "\n", "model", ":", "Model", ",", "\n", "task_list", ":", "List", "[", "Task", "]", ",", "\n", "optimizer_params", ":", "Params", ",", "\n", "lr_scheduler_params", ":", "Params", ",", "\n", "patience", ":", "Optional", "[", "int", "]", "=", "None", ",", "\n", "num_epochs", ":", "int", "=", "20", ",", "\n", "serialization_dir", ":", "str", "=", "None", ",", "\n", "cuda_device", ":", "int", "=", "-", "1", ",", "\n", "grad_norm", ":", "Optional", "[", "float", "]", "=", "None", ",", "\n", "grad_clipping", ":", "Optional", "[", "float", "]", "=", "None", ",", "\n", "min_lr", ":", "float", "=", "0.00001", ",", "\n", "no_tqdm", ":", "bool", "=", "False", ",", "\n", "summary_interval", ":", "int", "=", "50", ",", "\n", "log_parameter_statistics", ":", "bool", "=", "False", ",", "\n", "log_gradient_statistics", ":", "bool", "=", "False", ",", "\n", "log_learning_rate", ":", "bool", "=", "True", ",", "\n", ")", ":", "\n", "        ", "\"\"\" \n        Parameters\n        ----------\n        model: ``Model``, required.\n            An AllenNLP model to be optimized. Pytorch Modules can also be optimized if\n            their ``forward`` method returns a dictionary with a \"loss\" key, containing a\n            scalar tensor representing the loss function to be optimized.\n        iterator: ``DataIterator``, required.\n            A method for iterating over a ``Dataset``, yielding padded indexed batches.\n        patience: Optional[int] > 0, optional (default=None)\n            Number of epochs to be patient before early stopping: the training is stopped\n            after ``patience`` epochs with no improvement. If given, it must be ``> 0``.\n            If None, early stopping is disabled.\n        num_epochs: int, optional (default = 20)\n            Number of training epochs.\n        serialization_dir: str, optional (default=None)\n            Path to directory for saving and loading model files. Models will not be saved if\n            this parameter is not passed.\n        cuda_device: int, optional (default = -1)\n            An integer specifying the CUDA device to use. If -1, the CPU is used.\n            Multi-gpu training is not currently supported, but will be once the\n            Pytorch DataParallel API stabilises.\n        grad_norm: float, optional, (default = None).\n            If provided, gradient norms will be rescaled to have a maximum of this value.\n        grad_clipping : float, optional (default = None).\n            If provided, gradients will be clipped `during the backward pass` to have an (absolute)\n            maximum of this value.  If you are getting ``NaNs`` in your gradients during training\n            that are not solved by using ``grad_norm``, you may need this.\n        no_tqdm : bool, optional (default=False)\n            We use ``tqdm`` for logging, which will print a nice progress bar that updates in place\n            after every batch.  This is nice if you're running training on a local shell, but can\n            cause problems with log files from, e.g., a docker image running on kubernetes.  If\n            ``no_tqdm`` is ``True``, we will not use tqdm, and instead log batch statistics using\n            ``logger.info``.\n        \"\"\"", "\n", "self", ".", "_model", "=", "model", "\n", "parameters_to_train", "=", "[", "\n", "(", "n", ",", "p", ")", "for", "n", ",", "p", "in", "self", ".", "_model", ".", "named_parameters", "(", ")", "if", "p", ".", "requires_grad", "\n", "]", "\n", "\n", "self", ".", "_task_list", "=", "task_list", "\n", "self", ".", "_n_tasks", "=", "len", "(", "self", ".", "_task_list", ")", "\n", "\n", "self", ".", "_optimizer_params", "=", "optimizer_params", "\n", "self", ".", "_optimizers", "=", "{", "}", "\n", "self", ".", "_lr_scheduler_params", "=", "lr_scheduler_params", "\n", "self", ".", "_schedulers", "=", "{", "}", "\n", "for", "task", "in", "self", ".", "_task_list", ":", "\n", "            ", "task_name", "=", "task", ".", "_name", "\n", "self", ".", "_optimizers", "[", "task_name", "]", "=", "Optimizer", ".", "from_params", "(", "\n", "model_parameters", "=", "parameters_to_train", ",", "params", "=", "deepcopy", "(", "optimizer_params", ")", "\n", ")", "\n", "self", ".", "_schedulers", "[", "task_name", "]", "=", "LearningRateScheduler", ".", "from_params", "(", "\n", "optimizer", "=", "self", ".", "_optimizers", "[", "task_name", "]", ",", "\n", "params", "=", "deepcopy", "(", "lr_scheduler_params", ")", ",", "\n", ")", "\n", "\n", "", "self", ".", "_serialization_dir", "=", "serialization_dir", "\n", "\n", "self", ".", "_patience", "=", "patience", "\n", "self", ".", "_num_epochs", "=", "num_epochs", "\n", "self", ".", "_cuda_device", "=", "cuda_device", "\n", "if", "self", ".", "_cuda_device", ">=", "0", ":", "\n", "            ", "check_for_gpu", "(", "self", ".", "_cuda_device", ")", "\n", "self", ".", "_model", "=", "self", ".", "_model", ".", "cuda", "(", "self", ".", "_cuda_device", ")", "\n", "", "self", ".", "_grad_norm", "=", "grad_norm", "\n", "self", ".", "_grad_clipping", "=", "grad_clipping", "\n", "self", ".", "_min_lr", "=", "min_lr", "\n", "\n", "self", ".", "_task_infos", "=", "None", "\n", "self", ".", "_metric_infos", "=", "None", "\n", "\n", "self", ".", "_tr_generators", "=", "None", "\n", "self", ".", "_no_tqdm", "=", "no_tqdm", "\n", "\n", "self", ".", "_summary_interval", "=", "(", "\n", "summary_interval", "# num batches between logging to tensorboard", "\n", ")", "\n", "self", ".", "_log_parameter_statistics", "=", "log_parameter_statistics", "\n", "self", ".", "_log_gradient_statistics", "=", "log_gradient_statistics", "\n", "\n", "self", ".", "_global_step", "=", "0", "\n", "\n", "self", ".", "_tensorboard", "=", "TensorboardWriter", "(", "\n", "get_batch_num_total", "=", "lambda", ":", "self", ".", "_global_step", ",", "\n", "serialization_dir", "=", "serialization_dir", ",", "\n", "summary_interval", "=", "summary_interval", ",", "\n", "should_log_parameter_statistics", "=", "log_parameter_statistics", ",", "\n", "should_log_learning_rate", "=", "log_learning_rate", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.training.multi_task_trainer.MultiTaskTrainer.train": [[146, 153], ["None"], "methods", ["None"], ["", "def", "train", "(", "\n", "self", ",", "\n", "# tasks: List[Task],", "\n", "# params: Params,", "\n", "recover", ":", "bool", "=", "False", ",", "\n", ")", ":", "\n", "        ", "raise", "NotImplementedError", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.training.multi_task_trainer.MultiTaskTrainer._check_history": [[154, 199], ["best_fn", "len", "print", "metric_history.index", "len", "max", "min"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.fields.text_field.TextField.index"], ["", "def", "_check_history", "(", "\n", "self", ",", "\n", "metric_history", ":", "List", "[", "float", "]", ",", "\n", "cur_score", ":", "float", ",", "\n", "should_decrease", ":", "bool", "=", "False", ",", "\n", ")", ":", "\n", "        ", "\"\"\"\n        Given a task, the history of the performance on that task,\n        and the current score, check if current score is\n        best so far and if out of patience.\n        \n        Parameters\n        ----------\n        metric_history: List[float], required\n        cur_score: float, required\n        should_decrease: bool, default = False\n            Wheter or not the validation metric should increase while training.\n            For instance, the bigger the f1 score is, the better it is -> should_decrease = False\n            \n        Returns\n        -------\n        best_so_far: bool\n            Whether or not the current epoch is the best so far in terms of the speicified validation metric.\n        out_of_patience: bool\n            Whether or not the training for this specific task should stop (patience parameter).\n        \"\"\"", "\n", "patience", "=", "self", ".", "_patience", "+", "1", "\n", "best_fn", "=", "min", "if", "should_decrease", "else", "max", "\n", "best_score", "=", "best_fn", "(", "metric_history", ")", "\n", "if", "best_score", "==", "cur_score", ":", "\n", "            ", "best_so_far", "=", "metric_history", ".", "index", "(", "best_score", ")", "==", "len", "(", "metric_history", ")", "-", "1", "\n", "", "else", ":", "\n", "            ", "best_so_far", "=", "False", "\n", "\n", "", "out_of_patience", "=", "False", "\n", "if", "len", "(", "metric_history", ")", ">", "patience", ":", "\n", "            ", "if", "should_decrease", ":", "\n", "                ", "out_of_patience", "=", "max", "(", "metric_history", "[", "-", "patience", ":", "]", ")", "<=", "cur_score", "\n", "", "else", ":", "\n", "                ", "out_of_patience", "=", "min", "(", "metric_history", "[", "-", "patience", ":", "]", ")", ">=", "cur_score", "\n", "\n", "", "", "if", "best_so_far", "and", "out_of_patience", ":", "# then something is up", "\n", "            ", "print", "(", "\"Something is up\"", ")", "\n", "\n", "", "return", "best_so_far", ",", "out_of_patience", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.training.multi_task_trainer.MultiTaskTrainer._forward": [[200, 222], ["allennlp.nn.util.move_to_device", "multi_task_trainer.MultiTaskTrainer._model.forward", "allennlp.common.checks.ConfigurationError", "multi_task_trainer.MultiTaskTrainer._model.get_regularization_penalty", "RuntimeError"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.nn.util.move_to_device", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.graph_util.graph_encoder_gat.GAT.forward", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.models.model.Model.get_regularization_penalty"], ["", "def", "_forward", "(", "\n", "self", ",", "tensor_batch", ":", "torch", ".", "Tensor", ",", "for_training", ":", "bool", "=", "False", ",", "task", ":", "Task", "=", "None", "\n", ")", ":", "\n", "        ", "if", "task", "is", "not", "None", ":", "\n", "            ", "tensor_batch", "=", "move_to_device", "(", "tensor_batch", ",", "self", ".", "_cuda_device", ")", "\n", "output_dict", "=", "self", ".", "_model", ".", "forward", "(", "\n", "task_name", "=", "task", ".", "_name", ",", "\n", "tensor_batch", "=", "tensor_batch", ",", "\n", "for_training", "=", "for_training", ",", "\n", ")", "\n", "if", "for_training", ":", "\n", "                ", "try", ":", "\n", "                    ", "loss", "=", "output_dict", "[", "\"loss\"", "]", "\n", "loss", "+=", "self", ".", "_model", ".", "get_regularization_penalty", "(", ")", "\n", "", "except", "KeyError", ":", "\n", "                    ", "raise", "RuntimeError", "(", "\n", "\"The model you are trying to optimize does not contain a\"", "\n", "\" `loss` key in the output of model.forward(inputs).\"", "\n", ")", "\n", "", "", "return", "output_dict", "\n", "", "else", ":", "\n", "            ", "raise", "ConfigurationError", "(", "\"Cannot call forward through task `None`\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.training.multi_task_trainer.MultiTaskTrainer._get_metrics": [[223, 226], ["getattr", "getattr.get_metrics"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.coreference_resolution.coref.CoreferenceResolver.get_metrics"], ["", "", "def", "_get_metrics", "(", "self", ",", "task", ":", "Task", ",", "reset", ":", "bool", "=", "False", ")", ":", "\n", "        ", "task_tagger", "=", "getattr", "(", "self", ".", "_model", ",", "\"_tagger_\"", "+", "task", ".", "_name", ")", "\n", "return", "task_tagger", ".", "get_metrics", "(", "reset", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.training.multi_task_trainer.MultiTaskTrainer._description_from_metrics": [[227, 232], ["metrics.items"], "methods", ["None"], ["", "def", "_description_from_metrics", "(", "self", ",", "metrics", ":", "Dict", "[", "str", ",", "float", "]", ")", ":", "\n", "# pylint: disable=no-self-use", "\n", "        ", "return", "(", "\n", "\", \"", ".", "join", "(", "[", "\"%s: %.4f\"", "%", "(", "name", ",", "value", ")", "for", "name", ",", "value", "in", "metrics", ".", "items", "(", ")", "]", ")", "\n", "+", "\" ||\"", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.training.multi_task_trainer.MultiTaskTrainer._rescale_gradients": [[234, 244], ["allennlp.training.util.sparse_clip_norm", "multi_task_trainer.MultiTaskTrainer._model.parameters"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.training.util.sparse_clip_norm"], ["", "def", "_rescale_gradients", "(", "self", ")", "->", "Optional", "[", "float", "]", ":", "\n", "        ", "\"\"\"\n        Performs gradient rescaling. Is a no-op if gradient rescaling is not enabled.\n        \"\"\"", "\n", "if", "self", ".", "_grad_norm", ":", "\n", "            ", "parameters_to_clip", "=", "[", "\n", "p", "for", "p", "in", "self", ".", "_model", ".", "parameters", "(", ")", "if", "p", ".", "grad", "is", "not", "None", "\n", "]", "\n", "return", "sparse_clip_norm", "(", "parameters_to_clip", ",", "self", ".", "_grad_norm", ")", "\n", "", "return", "None", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.training.multi_task_trainer.MultiTaskTrainer._enable_gradient_clipping": [[245, 255], ["multi_task_trainer.MultiTaskTrainer._model.parameters", "grad.clamp", "parameter.register_hook"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.metrics.f1_score.F1.clamp"], ["", "def", "_enable_gradient_clipping", "(", "self", ")", "->", "None", ":", "\n", "        ", "if", "self", ".", "_grad_clipping", "is", "not", "None", ":", "\n", "# Pylint is unable to tell that we're in the case that _grad_clipping is not None...", "\n", "# pylint: disable=invalid-unary-operand-type", "\n", "            ", "clip_function", "=", "lambda", "grad", ":", "grad", ".", "clamp", "(", "\n", "-", "self", ".", "_grad_clipping", ",", "self", ".", "_grad_clipping", "\n", ")", "\n", "for", "parameter", "in", "self", ".", "_model", ".", "parameters", "(", ")", ":", "\n", "                ", "if", "parameter", ".", "requires_grad", ":", "\n", "                    ", "parameter", ".", "register_hook", "(", "clip_function", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.training.multi_task_trainer.MultiTaskTrainer._save_checkpoint": [[256, 315], ["os.path.join", "torch.save", "torch.save", "torch.save", "torch.save", "shutil.copyfile", "logger.info", "os.path.join", "multi_task_trainer.MultiTaskTrainer._model.state_dict", "torch.save", "torch.save", "torch.save", "torch.save", "shutil.copyfile", "logger.info", "multi_task_trainer.MultiTaskTrainer._metric_infos.items", "multi_task_trainer.MultiTaskTrainer._optimizers.items", "multi_task_trainer.MultiTaskTrainer._schedulers.items", "os.path.join", "os.path.join", "optimizer.state_dict", "scheduler.state_dict", "logger.info", "logger.info", "shutil.copyfile", "os.path.join"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.dataset_readers.multiprocess_dataset_reader.QIterable.join", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.None.combine_data.save", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.None.combine_data.save", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.None.combine_data.save", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.None.combine_data.save", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.dataset_readers.multiprocess_dataset_reader.logger.info", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.dataset_readers.multiprocess_dataset_reader.QIterable.join", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.learning_rate_schedulers.learning_rate_scheduler._PyTorchLearningRateSchedulerWrapper.state_dict", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.None.combine_data.save", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.None.combine_data.save", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.None.combine_data.save", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.None.combine_data.save", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.dataset_readers.multiprocess_dataset_reader.logger.info", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.dataset_readers.multiprocess_dataset_reader.QIterable.join", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.dataset_readers.multiprocess_dataset_reader.QIterable.join", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.learning_rate_schedulers.learning_rate_scheduler._PyTorchLearningRateSchedulerWrapper.state_dict", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.learning_rate_schedulers.learning_rate_scheduler._PyTorchLearningRateSchedulerWrapper.state_dict", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.dataset_readers.multiprocess_dataset_reader.logger.info", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.dataset_readers.multiprocess_dataset_reader.logger.info", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.dataset_readers.multiprocess_dataset_reader.QIterable.join"], ["", "", "", "", "def", "_save_checkpoint", "(", "self", ",", "epoch", ":", "int", ",", "should_stop", ":", "bool", ")", "->", "None", ":", "\n", "        ", "\"\"\"\n        Save the current states (model, training, optimizers, metrics and tasks).\n        \n        Parameters\n        ----------\n        epoch: int, required.\n            The epoch of training.\n        should_stop: bool, required\n            Wheter or not the training is finished.\n        should_save_model: bool, optional (default = True)\n            Whether or not the model state should be saved.\n        \"\"\"", "\n", "### Saving training state ###", "\n", "training_state", "=", "{", "\n", "\"epoch\"", ":", "epoch", ",", "\n", "\"should_stop\"", ":", "should_stop", ",", "\n", "\"metric_infos\"", ":", "self", ".", "_metric_infos", ",", "\n", "\"task_infos\"", ":", "self", ".", "_task_infos", ",", "\n", "\"schedulers\"", ":", "{", "}", ",", "\n", "\"optimizers\"", ":", "{", "}", ",", "\n", "}", "\n", "\n", "if", "self", ".", "_optimizers", "is", "not", "None", ":", "\n", "            ", "for", "task_name", ",", "optimizer", "in", "self", ".", "_optimizers", ".", "items", "(", ")", ":", "\n", "                ", "training_state", "[", "\"optimizers\"", "]", "[", "task_name", "]", "=", "optimizer", ".", "state_dict", "(", ")", "\n", "", "", "if", "self", ".", "_schedulers", "is", "not", "None", ":", "\n", "            ", "for", "task_name", ",", "scheduler", "in", "self", ".", "_schedulers", ".", "items", "(", ")", ":", "\n", "                ", "training_state", "[", "\"schedulers\"", "]", "[", "task_name", "]", "=", "scheduler", ".", "state_dict", "(", ")", "\n", "\n", "", "", "training_path", "=", "os", ".", "path", ".", "join", "(", "self", ".", "_serialization_dir", ",", "\"training_state.th\"", ")", "\n", "torch", ".", "save", "(", "training_state", ",", "training_path", ")", "\n", "shutil", ".", "copyfile", "(", "training_path", ",", "os", ".", "path", ".", "join", "(", "self", ".", "_serialization_dir", ",", "\"training_state_{}.th\"", ".", "format", "(", "epoch", ")", ")", ")", "\n", "logger", ".", "info", "(", "\"Checkpoint - Saved training state to %s\"", ",", "training_path", ")", "\n", "\n", "### Saving model state ###", "\n", "model_path", "=", "os", ".", "path", ".", "join", "(", "self", ".", "_serialization_dir", ",", "\"model_state.th\"", ")", "\n", "model_state", "=", "self", ".", "_model", ".", "state_dict", "(", ")", "\n", "torch", ".", "save", "(", "model_state", ",", "model_path", ")", "\n", "shutil", ".", "copyfile", "(", "model_path", ",", "os", ".", "path", ".", "join", "(", "self", ".", "_serialization_dir", ",", "\"model_state_{}.th\"", ".", "format", "(", "epoch", ")", ")", ")", "\n", "logger", ".", "info", "(", "\"Checkpoint - Saved model state to %s\"", ",", "model_path", ")", "\n", "\n", "### Saving best models for each task ###", "\n", "for", "task_name", ",", "infos", "in", "self", ".", "_metric_infos", ".", "items", "(", ")", ":", "\n", "            ", "best_epoch", ",", "_", "=", "infos", "[", "\"best\"", "]", "\n", "if", "best_epoch", "==", "epoch", ":", "\n", "                ", "logger", ".", "info", "(", "\n", "\"Checkpoint - Best validation performance so far for %s task\"", ",", "\n", "task_name", ",", "\n", ")", "\n", "logger", ".", "info", "(", "\n", "\"Checkpoint - Copying weights to '%s/best_%s.th'.\"", ",", "\n", "self", ".", "_serialization_dir", ",", "\n", "task_name", ",", "\n", ")", "\n", "shutil", ".", "copyfile", "(", "\n", "model_path", ",", "\n", "os", ".", "path", ".", "join", "(", "\n", "self", ".", "_serialization_dir", ",", "\"best_{}.th\"", ".", "format", "(", "task_name", ")", "\n", ")", ",", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.training.multi_task_trainer.MultiTaskTrainer.find_latest_checkpoint": [[318, 336], ["os.path.join", "os.path.join", "any", "any", "os.listdir", "os.listdir"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.dataset_readers.multiprocess_dataset_reader.QIterable.join", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.dataset_readers.multiprocess_dataset_reader.QIterable.join"], ["", "", "", "def", "find_latest_checkpoint", "(", "self", ")", "->", "Tuple", "[", "str", ",", "str", "]", ":", "\n", "        ", "\"\"\"\n        Return the location of the latest model and training state files.\n        If there isn't a valid checkpoint then return None.\n        \"\"\"", "\n", "have_checkpoint", "=", "(", "\n", "self", ".", "_serialization_dir", "is", "not", "None", "\n", "and", "any", "(", "\"model_state\"", "in", "x", "for", "x", "in", "os", ".", "listdir", "(", "self", ".", "_serialization_dir", ")", ")", "\n", "and", "any", "(", "\"training_state\"", "in", "x", "for", "x", "in", "os", ".", "listdir", "(", "self", ".", "_serialization_dir", ")", ")", "\n", ")", "\n", "\n", "if", "not", "have_checkpoint", ":", "\n", "            ", "return", "None", "\n", "\n", "", "model_path", "=", "os", ".", "path", ".", "join", "(", "self", ".", "_serialization_dir", ",", "\"model_state.th\"", ")", "\n", "training_state_path", "=", "os", ".", "path", ".", "join", "(", "self", ".", "_serialization_dir", ",", "\"training_state.th\"", ")", "\n", "\n", "return", "(", "model_path", ",", "training_state_path", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.training.multi_task_trainer.MultiTaskTrainer._restore_checkpoint": [[337, 403], ["multi_task_trainer.MultiTaskTrainer.find_latest_checkpoint", "torch.load", "torch.load", "torch.load", "torch.load", "torch.load", "torch.load", "torch.load", "torch.load", "multi_task_trainer.MultiTaskTrainer._model.load_state_dict", "logger.info", "training_state[].items", "logger.info", "training_state[].items", "logger.info", "logger.info", "logger.info", "allennlp.common.checks.ConfigurationError", "allennlp.common.checks.ConfigurationError", "multi_task_trainer.MultiTaskTrainer._optimizers[].load_state_dict", "multi_task_trainer.MultiTaskTrainer._schedulers[].lr_scheduler.load_state_dict", "allennlp.nn.util.device_mapping", "allennlp.nn.util.device_mapping"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.training.checkpointer.Checkpointer.find_latest_checkpoint", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.token_embedders.bert_token_embedder.PretrainedBertModel.load", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.token_embedders.bert_token_embedder.PretrainedBertModel.load", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.token_embedders.bert_token_embedder.PretrainedBertModel.load", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.token_embedders.bert_token_embedder.PretrainedBertModel.load", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.token_embedders.bert_token_embedder.PretrainedBertModel.load", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.token_embedders.bert_token_embedder.PretrainedBertModel.load", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.token_embedders.bert_token_embedder.PretrainedBertModel.load", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.token_embedders.bert_token_embedder.PretrainedBertModel.load", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.learning_rate_schedulers.learning_rate_scheduler._PyTorchLearningRateSchedulerWrapper.load_state_dict", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.dataset_readers.multiprocess_dataset_reader.logger.info", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.dataset_readers.multiprocess_dataset_reader.logger.info", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.dataset_readers.multiprocess_dataset_reader.logger.info", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.dataset_readers.multiprocess_dataset_reader.logger.info", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.dataset_readers.multiprocess_dataset_reader.logger.info", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.learning_rate_schedulers.learning_rate_scheduler._PyTorchLearningRateSchedulerWrapper.load_state_dict", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.learning_rate_schedulers.learning_rate_scheduler._PyTorchLearningRateSchedulerWrapper.load_state_dict", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.nn.util.device_mapping", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.nn.util.device_mapping"], ["", "def", "_restore_checkpoint", "(", "self", ")", ":", "\n", "        ", "\"\"\"\n        Restores a model from a serialization_dir to the last saved checkpoint.\n        This includes an epoch count, optimizer state, a model state, a task state and\n        a metric state. All are of which are serialized separately. \n        This function should only be used to continue training -\n        if you wish to load a model for inference/load parts of a model into a new\n        computation graph, you should use the native Pytorch functions:\n        `` model.load_state_dict(torch.load(\"/path/to/model/weights.th\"))``\n\n        Returns\n        -------\n        epoch: int, \n            The epoch at which to resume training.\n        should_stop: bool\n            Whether or not the training should already by stopped.\n        \"\"\"", "\n", "\n", "latest_checkpoint", "=", "self", ".", "find_latest_checkpoint", "(", ")", "\n", "\n", "if", "not", "self", ".", "_serialization_dir", ":", "\n", "            ", "raise", "ConfigurationError", "(", "\n", "\"`serialization_dir` not specified - cannot \"", "\n", "\"restore a model without a directory path.\"", "\n", ")", "\n", "", "if", "latest_checkpoint", "is", "None", ":", "\n", "            ", "raise", "ConfigurationError", "(", "\n", "\"Cannot restore model because one of\"", "\n", "\"`model_state.th` or `training_state.th` is not in directory path.\"", "\n", ")", "\n", "\n", "", "model_path", ",", "training_state_path", "=", "latest_checkpoint", "\n", "\n", "# Load the parameters onto CPU, then transfer to GPU.", "\n", "# This avoids potential OOM on GPU for large models that", "\n", "# load parameters onto GPU then make a new GPU copy into the parameter", "\n", "# buffer. The GPU transfer happens implicitly in load_state_dict.", "\n", "model_state", "=", "torch", ".", "load", "(", "model_path", ",", "map_location", "=", "device_mapping", "(", "-", "1", ")", ")", "\n", "training_state", "=", "torch", ".", "load", "(", "\n", "training_state_path", ",", "map_location", "=", "device_mapping", "(", "-", "1", ")", "\n", ")", "\n", "\n", "# Load model", "\n", "self", ".", "_model", ".", "load_state_dict", "(", "model_state", ")", "\n", "logger", ".", "info", "(", "\"Checkpoint - Model loaded from %s\"", ",", "model_path", ")", "\n", "\n", "# Load optimizers", "\n", "for", "task_name", ",", "optimizers_state", "in", "training_state", "[", "\"optimizers\"", "]", ".", "items", "(", ")", ":", "\n", "            ", "self", ".", "_optimizers", "[", "task_name", "]", ".", "load_state_dict", "(", "optimizers_state", ")", "\n", "", "logger", ".", "info", "(", "\"Checkpoint - Optimizers loaded from %s\"", ",", "training_state_path", ")", "\n", "\n", "# Load schedulers", "\n", "for", "task_name", ",", "scheduler_state", "in", "training_state", "[", "\"schedulers\"", "]", ".", "items", "(", ")", ":", "\n", "            ", "self", ".", "_schedulers", "[", "task_name", "]", ".", "lr_scheduler", ".", "load_state_dict", "(", "scheduler_state", ")", "\n", "", "logger", ".", "info", "(", "\n", "\"Checkpoint - Learning rate schedulers loaded from %s\"", ",", "training_state_path", "\n", ")", "\n", "\n", "self", ".", "_metric_infos", "=", "training_state", "[", "\"metric_infos\"", "]", "\n", "self", ".", "_task_infos", "=", "training_state", "[", "\"task_infos\"", "]", "\n", "logger", ".", "info", "(", "\"Checkpoint - Task infos loaded from %s\"", ",", "training_state_path", ")", "\n", "logger", ".", "info", "(", "\"Checkpoint - Metric infos loaded from %s\"", ",", "training_state_path", ")", "\n", "\n", "n_epoch", ",", "should_stop", "=", "training_state", "[", "\"epoch\"", "]", ",", "training_state", "[", "\"should_stop\"", "]", "\n", "\n", "return", "n_epoch", "+", "1", ",", "should_stop", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.training.multi_task_trainer.MultiTaskTrainer.from_params": [[404, 417], ["params.pop_choice", "cls.by_name().from_params", "cls.list_available", "cls.by_name"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.params.pop_choice", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.tokenizers.tokenizer.Tokenizer.from_params", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.registrable.Registrable.list_available", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.registrable.Registrable.by_name"], ["", "@", "classmethod", "\n", "def", "from_params", "(", "\n", "cls", ",", "model", ":", "Model", ",", "task_list", ":", "List", "[", "Task", "]", ",", "serialization_dir", ":", "str", ",", "params", ":", "Params", "\n", ")", "->", "\"MultiTaskTrainer\"", ":", "\n", "        ", "\"\"\"\n        Static method that constructs the multi task trainer described by ``params``.\n        \"\"\"", "\n", "choice", "=", "params", ".", "pop_choice", "(", "\"type\"", ",", "cls", ".", "list_available", "(", ")", ")", "\n", "return", "cls", ".", "by_name", "(", "choice", ")", ".", "from_params", "(", "\n", "model", "=", "model", ",", "\n", "task_list", "=", "task_list", ",", "\n", "serialization_dir", "=", "serialization_dir", ",", "\n", "params", "=", "params", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.training.sampler_multi_task_trainer.SamplerMultiTaskTrainer.__init__": [[25, 68], ["hmtl.training.multi_task_trainer.MultiTaskTrainer.__init__", "allennlp.common.checks.ConfigurationError"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.graph_util.plot.GraphPlot.__init__"], ["    ", "def", "__init__", "(", "\n", "self", ",", "\n", "model", ":", "Model", ",", "\n", "task_list", ":", "List", "[", "Task", "]", ",", "\n", "optimizer_params", ":", "Params", ",", "\n", "lr_scheduler_params", ":", "Params", ",", "\n", "patience", ":", "Optional", "[", "int", "]", "=", "None", ",", "\n", "num_epochs", ":", "int", "=", "20", ",", "\n", "serialization_dir", ":", "str", "=", "None", ",", "\n", "cuda_device", ":", "int", "=", "-", "1", ",", "\n", "grad_norm", ":", "Optional", "[", "float", "]", "=", "None", ",", "\n", "grad_clipping", ":", "Optional", "[", "float", "]", "=", "None", ",", "\n", "min_lr", ":", "float", "=", "0.00001", ",", "\n", "no_tqdm", ":", "bool", "=", "False", ",", "\n", "summary_interval", ":", "int", "=", "50", ",", "\n", "log_parameter_statistics", ":", "bool", "=", "False", ",", "\n", "log_gradient_statistics", ":", "bool", "=", "False", ",", "\n", "sampling_method", ":", "str", "=", "\"proportional\"", ",", "\n", ")", ":", "\n", "\n", "        ", "if", "sampling_method", "not", "in", "[", "\"uniform\"", ",", "\"proportional\"", "]", ":", "\n", "            ", "raise", "ConfigurationError", "(", "\n", "f\"Sampling method ({sampling_method}) must be `uniform` or `proportional`.\"", "\n", ")", "\n", "\n", "", "self", ".", "_sampling_method", "=", "sampling_method", "\n", "\n", "super", "(", "SamplerMultiTaskTrainer", ",", "self", ")", ".", "__init__", "(", "\n", "model", "=", "model", ",", "\n", "task_list", "=", "task_list", ",", "\n", "optimizer_params", "=", "optimizer_params", ",", "\n", "lr_scheduler_params", "=", "lr_scheduler_params", ",", "\n", "patience", "=", "patience", ",", "\n", "num_epochs", "=", "num_epochs", ",", "\n", "serialization_dir", "=", "serialization_dir", ",", "\n", "cuda_device", "=", "cuda_device", ",", "\n", "grad_norm", "=", "grad_norm", ",", "\n", "grad_clipping", "=", "grad_clipping", ",", "\n", "min_lr", "=", "min_lr", ",", "\n", "no_tqdm", "=", "no_tqdm", ",", "\n", "summary_interval", "=", "summary_interval", ",", "\n", "log_parameter_statistics", "=", "log_parameter_statistics", ",", "\n", "log_gradient_statistics", "=", "log_gradient_statistics", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.training.sampler_multi_task_trainer.SamplerMultiTaskTrainer.train": [[70, 486], ["time.time", "sampler_multi_task_trainer.SamplerMultiTaskTrainer._task_infos.items", "sampler_multi_task_trainer.SamplerMultiTaskTrainer._enable_gradient_clipping", "logger.info", "logging.info", "logging.info", "sampler_multi_task_trainer.SamplerMultiTaskTrainer._task_infos.items", "time.strftime", "enumerate", "logger.info", "logger.info", "logger.info", "data_iterator", "sampler_multi_task_trainer.SamplerMultiTaskTrainer._model.train", "logger.info", "logger.info", "logger.info", "allennlp.common.util.gpu_memory_mb().items", "logger.info", "sampler_multi_task_trainer.SamplerMultiTaskTrainer._task_infos.items", "tqdm.trange", "logger.info", "logger.info", "sampler_multi_task_trainer.SamplerMultiTaskTrainer._model.eval", "enumerate", "logger.info", "logger.info", "logger.info", "sampler_multi_task_trainer.SamplerMultiTaskTrainer._save_checkpoint", "int", "logging.info", "time.time", "time.gmtime", "sampler_multi_task_trainer.SamplerMultiTaskTrainer._restore_checkpoint", "logger.info", "data_iterator.get_num_batches", "data_iterator.get_num_batches", "time.time", "logger.info", "numpy.argmax", "next", "optimizer.zero_grad", "sampler_multi_task_trainer.SamplerMultiTaskTrainer._forward", "loss.backward", "loss.item", "sampler_multi_task_trainer.SamplerMultiTaskTrainer._rescale_gradients", "optimizer.step", "sampler_multi_task_trainer.SamplerMultiTaskTrainer._get_metrics", "float", "sampler_multi_task_trainer.SamplerMultiTaskTrainer._description_from_metrics", "tqdm.trange.set_description", "time.time", "sampler_multi_task_trainer.SamplerMultiTaskTrainer._get_metrics", "sampler_multi_task_trainer.SamplerMultiTaskTrainer.items", "float", "sampler_multi_task_trainer.SamplerMultiTaskTrainer._tensorboard.add_train_scalar", "all_tr_metrics[].items", "logger.info", "data_iterator", "tqdm.tqdm", "sampler_multi_task_trainer.SamplerMultiTaskTrainer._get_metrics", "sampler_multi_task_trainer.SamplerMultiTaskTrainer.items", "float", "all_val_metrics[].items", "metric_history.append", "sampler_multi_task_trainer.SamplerMultiTaskTrainer._check_history", "scheduler.step", "logger.info", "logger.info", "all_tr_metrics[].items", "all_val_metrics[].items", "logging.info", "logging.info", "logging.info", "allennlp.common.checks.ConfigurationError", "float", "float", "allennlp.common.util.gpu_memory_mb", "numpy.random.multinomial", "sampler_multi_task_trainer.SamplerMultiTaskTrainer.items", "sampler_multi_task_trainer.SamplerMultiTaskTrainer._model.named_parameters", "sampler_multi_task_trainer.SamplerMultiTaskTrainer._tensorboard.add_train_scalar", "sampler_multi_task_trainer.SamplerMultiTaskTrainer._forward", "loss.item", "sampler_multi_task_trainer.SamplerMultiTaskTrainer._get_metrics", "float", "sampler_multi_task_trainer.SamplerMultiTaskTrainer._description_from_metrics", "tqdm.tqdm.set_description", "sampler_multi_task_trainer.SamplerMultiTaskTrainer._tensorboard.add_validation_scalar", "logger.info", "logger.info", "logger.info", "logger.info", "logger.info", "logger.info", "sampler_multi_task_trainer.SamplerMultiTaskTrainer._task_infos.values", "allennlp.common.util.peak_memory_mb", "sampler_multi_task_trainer.SamplerMultiTaskTrainer._tensorboard.add_train_scalar", "sampler_multi_task_trainer.SamplerMultiTaskTrainer._tensorboard.add_train_scalar", "sampler_multi_task_trainer.SamplerMultiTaskTrainer._tensorboard.add_train_scalar", "sampler_multi_task_trainer.SamplerMultiTaskTrainer._tensorboard.add_train_scalar", "sampler_multi_task_trainer.SamplerMultiTaskTrainer._tensorboard.add_train_scalar", "param.data.mean", "param.data.std", "param.grad.data.mean", "param.grad.data.std"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.training.multi_task_trainer.MultiTaskTrainer._enable_gradient_clipping", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.dataset_readers.multiprocess_dataset_reader.logger.info", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.dataset_readers.multiprocess_dataset_reader.logger.info", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.dataset_readers.multiprocess_dataset_reader.logger.info", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.dataset_readers.multiprocess_dataset_reader.logger.info", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.dataset_readers.multiprocess_dataset_reader.logger.info", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.dataset_readers.multiprocess_dataset_reader.logger.info", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.training.no_op_trainer.NoOpTrainer.train", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.dataset_readers.multiprocess_dataset_reader.logger.info", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.dataset_readers.multiprocess_dataset_reader.logger.info", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.dataset_readers.multiprocess_dataset_reader.logger.info", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.dataset_readers.multiprocess_dataset_reader.logger.info", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.dataset_readers.multiprocess_dataset_reader.logger.info", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.dataset_readers.multiprocess_dataset_reader.logger.info", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.dataset_readers.multiprocess_dataset_reader.logger.info", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.dataset_readers.multiprocess_dataset_reader.logger.info", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.dataset_readers.multiprocess_dataset_reader.logger.info", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.callbacks.checkpoint.Checkpoint._save_checkpoint", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.dataset_readers.multiprocess_dataset_reader.logger.info", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.training.trainer.Trainer._restore_checkpoint", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.dataset_readers.multiprocess_dataset_reader.logger.info", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.iterators.data_iterator.DataIterator.get_num_batches", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.iterators.data_iterator.DataIterator.get_num_batches", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.dataset_readers.multiprocess_dataset_reader.logger.info", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.training.multi_task_trainer.MultiTaskTrainer._forward", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.training.multi_task_trainer.MultiTaskTrainer._rescale_gradients", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.learning_rate_schedulers.slanted_triangular.SlantedTriangular.step", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.training.multi_task_trainer.MultiTaskTrainer._get_metrics", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.training.multi_task_trainer.MultiTaskTrainer._description_from_metrics", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.training.multi_task_trainer.MultiTaskTrainer._get_metrics", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.training.tensorboard_writer.TensorboardWriter.add_train_scalar", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.dataset_readers.multiprocess_dataset_reader.logger.info", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.tqdm.Tqdm.tqdm", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.training.multi_task_trainer.MultiTaskTrainer._get_metrics", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.training.multi_task_trainer.MultiTaskTrainer._check_history", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.learning_rate_schedulers.slanted_triangular.SlantedTriangular.step", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.dataset_readers.multiprocess_dataset_reader.logger.info", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.dataset_readers.multiprocess_dataset_reader.logger.info", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.dataset_readers.multiprocess_dataset_reader.logger.info", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.dataset_readers.multiprocess_dataset_reader.logger.info", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.dataset_readers.multiprocess_dataset_reader.logger.info", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.util.gpu_memory_mb", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.training.tensorboard_writer.TensorboardWriter.add_train_scalar", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.training.multi_task_trainer.MultiTaskTrainer._forward", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.training.multi_task_trainer.MultiTaskTrainer._get_metrics", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.training.multi_task_trainer.MultiTaskTrainer._description_from_metrics", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.training.tensorboard_writer.TensorboardWriter.add_validation_scalar", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.dataset_readers.multiprocess_dataset_reader.logger.info", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.dataset_readers.multiprocess_dataset_reader.logger.info", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.dataset_readers.multiprocess_dataset_reader.logger.info", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.dataset_readers.multiprocess_dataset_reader.logger.info", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.dataset_readers.multiprocess_dataset_reader.logger.info", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.dataset_readers.multiprocess_dataset_reader.logger.info", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.util.peak_memory_mb", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.training.tensorboard_writer.TensorboardWriter.add_train_scalar", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.training.tensorboard_writer.TensorboardWriter.add_train_scalar", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.training.tensorboard_writer.TensorboardWriter.add_train_scalar", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.training.tensorboard_writer.TensorboardWriter.add_train_scalar", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.training.tensorboard_writer.TensorboardWriter.add_train_scalar"], ["", "@", "overrides", "\n", "def", "train", "(", "self", ",", "recover", ":", "bool", "=", "False", ")", ":", "\n", "        ", "\"\"\"\n        Train the different task_list, save the different checkpoints and metrics,\n        and save the model at the end of training while logging the training details.\n        \n        The metrics through the training are stored in dictionaries with the following structure:\n        \n        all_metrics - Dict[str, str]\n            task_name: val_metric\n\n        metric_infos (Dict[])\n            task_name (Dict[str, diverse]\n                val_metric (str): name (str)\n                hist (str): history_of_the_val_metric (List[float])\n                stopped (str): training_is_stopped (bool)\n                best (str): best_epoch_for_val_metric (Tuple(int, Dict))  \n\n        all_tr_metrics (Dict[str, Dict[str, float]])\n            task_name (Dict[str, float])\n                metric_name (str): value (float)\n                loss: value (float)\t\t\n\n        all_val_metrics (Dict[str, Dict[str, float]])\n            task_name (Dict[str, float])\n                metric_name (str): value (float)\n                loss (str): value (float)\n        \n        Parameters\n        ----------\n        task_list: List[Task], required\n            A list containing the tasks to train.\n        params: Params, required\n            Training parameters\n        recover: bool, required\n            Whether or not training should be recovered from a previous training.\n\n        Returns\n        -------\n        return_dict: Dict\n            A dictionary summarizing the training and the metrics for the best epochs for each task.\n        \"\"\"", "\n", "training_start_time", "=", "time", ".", "time", "(", ")", "\n", "\n", "if", "recover", ":", "\n", "            ", "try", ":", "\n", "                ", "n_epoch", ",", "should_stop", "=", "self", ".", "_restore_checkpoint", "(", ")", "\n", "logger", ".", "info", "(", "\n", "\"Loaded model from checkpoint. Starting at epoch %d\"", ",", "n_epoch", "\n", ")", "\n", "", "except", "RuntimeError", ":", "\n", "                ", "raise", "ConfigurationError", "(", "\n", "\"Could not recover training from the checkpoint.  Did you mean to output to \"", "\n", "\"a different serialization directory or delete the existing serialization \"", "\n", "\"directory?\"", "\n", ")", "\n", "", "", "else", ":", "\n", "            ", "n_epoch", ",", "should_stop", "=", "0", ",", "False", "\n", "\n", "### Store all the necessary informations and attributes about the tasks ###", "\n", "task_infos", "=", "{", "task", ".", "_name", ":", "{", "}", "for", "task", "in", "self", ".", "_task_list", "}", "\n", "for", "task_idx", ",", "task", "in", "enumerate", "(", "self", ".", "_task_list", ")", ":", "\n", "                ", "task_info", "=", "task_infos", "[", "task", ".", "_name", "]", "\n", "\n", "# Store statistiscs on training and validation batches", "\n", "data_iterator", "=", "task", ".", "_data_iterator", "\n", "n_tr_batches", "=", "data_iterator", ".", "get_num_batches", "(", "task", ".", "_train_data", ")", "\n", "n_val_batches", "=", "data_iterator", ".", "get_num_batches", "(", "task", ".", "_validation_data", ")", "\n", "task_info", "[", "\"n_tr_batches\"", "]", "=", "n_tr_batches", "\n", "task_info", "[", "\"n_val_batches\"", "]", "=", "n_val_batches", "\n", "\n", "# Create counter for number of batches trained during the whole", "\n", "# training for this specific task", "\n", "task_info", "[", "\"total_n_batches_trained\"", "]", "=", "0", "\n", "\n", "task_info", "[", "\"last_log\"", "]", "=", "time", ".", "time", "(", ")", "# Time of last logging", "\n", "", "self", ".", "_task_infos", "=", "task_infos", "\n", "\n", "### Bookkeeping the validation metrics ###", "\n", "metric_infos", "=", "{", "\n", "task", ".", "_name", ":", "{", "\n", "\"val_metric\"", ":", "task", ".", "_val_metric", ",", "\n", "\"hist\"", ":", "[", "]", ",", "\n", "\"is_out_of_patience\"", ":", "False", ",", "\n", "\"min_lr_hit\"", ":", "False", ",", "\n", "\"best\"", ":", "(", "-", "1", ",", "{", "}", ")", ",", "\n", "}", "\n", "for", "task", "in", "self", ".", "_task_list", "\n", "}", "\n", "self", ".", "_metric_infos", "=", "metric_infos", "\n", "\n", "### Write log ###", "\n", "", "total_n_tr_batches", "=", "(", "\n", "0", "# The total number of training batches across all the datasets.", "\n", ")", "\n", "for", "task_name", ",", "info", "in", "self", ".", "_task_infos", ".", "items", "(", ")", ":", "\n", "            ", "total_n_tr_batches", "+=", "info", "[", "\"n_tr_batches\"", "]", "\n", "logger", ".", "info", "(", "\"Task %s:\"", ",", "task_name", ")", "\n", "logger", ".", "info", "(", "\"\\t%d training batches\"", ",", "info", "[", "\"n_tr_batches\"", "]", ")", "\n", "logger", ".", "info", "(", "\"\\t%d validation batches\"", ",", "info", "[", "\"n_val_batches\"", "]", ")", "\n", "\n", "### Create the training generators/iterators tqdm ###", "\n", "", "self", ".", "_tr_generators", "=", "{", "}", "\n", "for", "task", "in", "self", ".", "_task_list", ":", "\n", "            ", "data_iterator", "=", "task", ".", "_data_iterator", "\n", "tr_generator", "=", "data_iterator", "(", "task", ".", "_train_data", ",", "num_epochs", "=", "None", ")", "\n", "self", ".", "_tr_generators", "[", "task", ".", "_name", "]", "=", "tr_generator", "\n", "\n", "### Create sampling probability distribution ###", "\n", "", "if", "self", ".", "_sampling_method", "==", "\"uniform\"", ":", "\n", "            ", "sampling_prob", "=", "[", "float", "(", "1", "/", "self", ".", "_n_tasks", ")", "]", "*", "self", ".", "_n_tasks", "\n", "", "elif", "self", ".", "_sampling_method", "==", "\"proportional\"", ":", "\n", "            ", "sampling_prob", "=", "[", "\n", "float", "(", "info", "[", "\"n_tr_batches\"", "]", "/", "total_n_tr_batches", ")", "\n", "for", "info", "in", "self", ".", "_task_infos", ".", "values", "(", ")", "\n", "]", "\n", "\n", "### Enable gradient clipping ###", "\n", "# Only if self._grad_clipping is specified", "\n", "", "self", ".", "_enable_gradient_clipping", "(", ")", "\n", "\n", "### Setup is ready. Training of the model can begin ###", "\n", "logger", ".", "info", "(", "\"Set up ready. Beginning training/validation.\"", ")", "\n", "\n", "### Begin Training of the model ###", "\n", "while", "not", "should_stop", ":", "\n", "# Train one epoch (training pass + validation pass)", "\n", "\n", "            ", "self", ".", "_model", ".", "train", "(", ")", "# Set the model to \"train\" mode.", "\n", "\n", "### Log Infos: current epoch count and CPU/GPU usage ###", "\n", "logger", ".", "info", "(", "\"\"", ")", "\n", "logger", ".", "info", "(", "\"Epoch %d/%d - Begin\"", ",", "n_epoch", ",", "self", ".", "_num_epochs", "-", "1", ")", "\n", "logger", ".", "info", "(", "f\"Peak CPU memory usage MB: {peak_memory_mb()}\"", ")", "\n", "for", "gpu", ",", "memory", "in", "gpu_memory_mb", "(", ")", ".", "items", "(", ")", ":", "\n", "                ", "logger", ".", "info", "(", "f\"GPU {gpu} memory usage MB: {memory}\"", ")", "\n", "\n", "", "logger", ".", "info", "(", "\"Training - Begin\"", ")", "\n", "\n", "### Reset training and trained batches counter before new training epoch ###", "\n", "for", "_", ",", "task_info", "in", "self", ".", "_task_infos", ".", "items", "(", ")", ":", "\n", "                ", "task_info", "[", "\"tr_loss_cum\"", "]", "=", "0.0", "\n", "task_info", "[", "\"n_batches_trained_this_epoch\"", "]", "=", "0", "\n", "", "all_tr_metrics", "=", "{", "}", "# BUG TO COMPLETE COMMENT TO MAKE IT MORE CLEAR", "\n", "\n", "### Start training epoch ###", "\n", "epoch_tqdm", "=", "tqdm", ".", "trange", "(", "total_n_tr_batches", ")", "\n", "for", "_", "in", "epoch_tqdm", ":", "\n", "                ", "task_idx", "=", "np", ".", "argmax", "(", "np", ".", "random", ".", "multinomial", "(", "1", ",", "sampling_prob", ")", ")", "\n", "task", "=", "self", ".", "_task_list", "[", "task_idx", "]", "\n", "task_info", "=", "self", ".", "_task_infos", "[", "task", ".", "_name", "]", "\n", "\n", "### One forward + backward pass ###", "\n", "\n", "# Call next batch to train", "\n", "batch", "=", "next", "(", "self", ".", "_tr_generators", "[", "task", ".", "_name", "]", ")", "\n", "task_info", "[", "\"n_batches_trained_this_epoch\"", "]", "+=", "1", "\n", "\n", "# Load optimizer", "\n", "optimizer", "=", "self", ".", "_optimizers", "[", "task", ".", "_name", "]", "\n", "optimizer", ".", "zero_grad", "(", ")", "\n", "\n", "# Get the loss for this batch", "\n", "output_dict", "=", "self", ".", "_forward", "(", "\n", "tensor_batch", "=", "batch", ",", "task", "=", "task", ",", "for_training", "=", "True", "\n", ")", "\n", "assert", "(", "\n", "\"loss\"", "in", "output_dict", "\n", ")", ",", "\"Model must return a dict containing a 'loss' key\"", "\n", "loss", "=", "output_dict", "[", "\"loss\"", "]", "\n", "loss", ".", "backward", "(", ")", "\n", "task_info", "[", "\"tr_loss_cum\"", "]", "+=", "loss", ".", "item", "(", ")", "\n", "\n", "# Gradient rescaling if self._grad_norm is specified", "\n", "self", ".", "_rescale_gradients", "(", ")", "\n", "\n", "# Take an optimization step", "\n", "optimizer", ".", "step", "(", ")", "\n", "\n", "### Get metrics for all progress so far, update tqdm, display description ###", "\n", "task_metrics", "=", "self", ".", "_get_metrics", "(", "task", "=", "task", ")", "\n", "task_metrics", "[", "\"loss\"", "]", "=", "float", "(", "\n", "task_info", "[", "\"tr_loss_cum\"", "]", "\n", "/", "(", "task_info", "[", "\"n_batches_trained_this_epoch\"", "]", "+", "0.000_001", ")", "\n", ")", "\n", "description", "=", "self", ".", "_description_from_metrics", "(", "task_metrics", ")", "\n", "epoch_tqdm", ".", "set_description", "(", "task", ".", "_name", "+", "\", \"", "+", "description", ")", "\n", "\n", "### Tensorboard logging: Training detailled metrics, parameters and gradients ###", "\n", "if", "self", ".", "_global_step", "%", "self", ".", "_summary_interval", "==", "0", ":", "\n", "# Metrics", "\n", "                    ", "for", "metric_name", ",", "value", "in", "task_metrics", ".", "items", "(", ")", ":", "\n", "                        ", "self", ".", "_tensorboard", ".", "add_train_scalar", "(", "\n", "name", "=", "\"training_details/\"", "+", "task", ".", "_name", "+", "\"/\"", "+", "metric_name", ",", "\n", "value", "=", "value", ",", "\n", ")", "\n", "# Parameters and Gradients", "\n", "", "for", "param_name", ",", "param", "in", "self", ".", "_model", ".", "named_parameters", "(", ")", ":", "\n", "                        ", "if", "self", ".", "_log_parameter_statistics", ":", "\n", "                            ", "self", ".", "_tensorboard", ".", "add_train_scalar", "(", "\n", "name", "=", "\"parameter_mean/\"", "+", "param_name", ",", "\n", "value", "=", "param", ".", "data", ".", "mean", "(", ")", ",", "\n", ")", "\n", "self", ".", "_tensorboard", ".", "add_train_scalar", "(", "\n", "name", "=", "\"parameter_std/\"", "+", "param_name", ",", "\n", "value", "=", "param", ".", "data", ".", "std", "(", ")", ",", "\n", ")", "\n", "", "if", "param", ".", "grad", "is", "None", ":", "\n", "                            ", "continue", "\n", "", "if", "self", ".", "_log_gradient_statistics", ":", "\n", "                            ", "self", ".", "_tensorboard", ".", "add_train_scalar", "(", "\n", "name", "=", "\"grad_mean/\"", "+", "param_name", ",", "\n", "value", "=", "param", ".", "grad", ".", "data", ".", "mean", "(", ")", ",", "\n", ")", "\n", "self", ".", "_tensorboard", ".", "add_train_scalar", "(", "\n", "name", "=", "\"grad_std/\"", "+", "param_name", ",", "\n", "value", "=", "param", ".", "grad", ".", "data", ".", "std", "(", ")", ",", "\n", ")", "\n", "", "", "", "self", ".", "_global_step", "+=", "1", "\n", "\n", "### Bookkeeping all the training metrics for all the tasks on the training epoch that just finished ###", "\n", "", "for", "task", "in", "self", ".", "_task_list", ":", "\n", "                ", "task_info", "=", "self", ".", "_task_infos", "[", "task", ".", "_name", "]", "\n", "\n", "task_info", "[", "\"total_n_batches_trained\"", "]", "+=", "task_info", "[", "\n", "\"n_batches_trained_this_epoch\"", "\n", "]", "\n", "task_info", "[", "\"last_log\"", "]", "=", "time", ".", "time", "(", ")", "\n", "\n", "task_metrics", "=", "self", ".", "_get_metrics", "(", "task", "=", "task", ",", "reset", "=", "True", ")", "\n", "if", "task", ".", "_name", "not", "in", "all_tr_metrics", ":", "\n", "                    ", "all_tr_metrics", "[", "task", ".", "_name", "]", "=", "{", "}", "\n", "", "for", "name", ",", "value", "in", "task_metrics", ".", "items", "(", ")", ":", "\n", "                    ", "all_tr_metrics", "[", "task", ".", "_name", "]", "[", "name", "]", "=", "value", "\n", "", "all_tr_metrics", "[", "task", ".", "_name", "]", "[", "\"loss\"", "]", "=", "float", "(", "\n", "task_info", "[", "\"tr_loss_cum\"", "]", "\n", "/", "(", "task_info", "[", "\"n_batches_trained_this_epoch\"", "]", "+", "0.000_000_01", ")", "\n", ")", "\n", "\n", "# Tensorboard - Training metrics for this epoch", "\n", "self", ".", "_tensorboard", ".", "add_train_scalar", "(", "\n", "name", "=", "\"training_proportions/\"", "+", "task", ".", "_name", ",", "\n", "value", "=", "task_info", "[", "\"n_batches_trained_this_epoch\"", "]", ",", "\n", ")", "\n", "for", "metric_name", ",", "value", "in", "all_tr_metrics", "[", "task", ".", "_name", "]", ".", "items", "(", ")", ":", "\n", "                    ", "self", ".", "_tensorboard", ".", "add_train_scalar", "(", "\n", "name", "=", "\"task_\"", "+", "task", ".", "_name", "+", "\"/\"", "+", "metric_name", ",", "value", "=", "value", "\n", ")", "\n", "\n", "", "", "logger", ".", "info", "(", "\"Train - End\"", ")", "\n", "\n", "### Begin validation of the model ###", "\n", "logger", ".", "info", "(", "\"Validation - Begin\"", ")", "\n", "all_val_metrics", "=", "{", "}", "\n", "\n", "self", ".", "_model", ".", "eval", "(", ")", "# Set the model into evaluation mode", "\n", "\n", "for", "task_idx", ",", "task", "in", "enumerate", "(", "self", ".", "_task_list", ")", ":", "\n", "                ", "logger", ".", "info", "(", "\n", "\"Validation - Task %d/%d: %s\"", ",", "\n", "task_idx", "+", "1", ",", "\n", "self", ".", "_n_tasks", ",", "\n", "task", ".", "_name", ",", "\n", ")", "\n", "\n", "val_loss", "=", "0.0", "\n", "n_batches_val_this_epoch_this_task", "=", "0", "\n", "n_val_batches", "=", "self", ".", "_task_infos", "[", "task", ".", "_name", "]", "[", "\"n_val_batches\"", "]", "\n", "scheduler", "=", "self", ".", "_schedulers", "[", "task", ".", "_name", "]", "\n", "\n", "# Create tqdm generator for current task's validation", "\n", "data_iterator", "=", "task", ".", "_data_iterator", "\n", "val_generator", "=", "data_iterator", "(", "\n", "task", ".", "_validation_data", ",", "num_epochs", "=", "1", ",", "shuffle", "=", "False", "\n", ")", "\n", "val_generator_tqdm", "=", "tqdm", ".", "tqdm", "(", "val_generator", ",", "total", "=", "n_val_batches", ")", "\n", "\n", "# Iterate over each validation batch for this task", "\n", "for", "batch", "in", "val_generator_tqdm", ":", "\n", "                    ", "n_batches_val_this_epoch_this_task", "+=", "1", "\n", "\n", "# Get the loss", "\n", "val_output_dict", "=", "self", ".", "_forward", "(", "\n", "batch", ",", "task", "=", "task", ",", "for_training", "=", "False", "\n", ")", "\n", "loss", "=", "val_output_dict", "[", "\"loss\"", "]", "\n", "val_loss", "+=", "loss", ".", "item", "(", ")", "\n", "\n", "# Get metrics for all progress so far, update tqdm, display description", "\n", "task_metrics", "=", "self", ".", "_get_metrics", "(", "task", "=", "task", ")", "\n", "task_metrics", "[", "\"loss\"", "]", "=", "float", "(", "\n", "val_loss", "/", "n_batches_val_this_epoch_this_task", "\n", ")", "\n", "description", "=", "self", ".", "_description_from_metrics", "(", "task_metrics", ")", "\n", "val_generator_tqdm", ".", "set_description", "(", "description", ")", "\n", "\n", "# Get task validation metrics and store them in all_val_metrics", "\n", "", "task_metrics", "=", "self", ".", "_get_metrics", "(", "task", "=", "task", ",", "reset", "=", "True", ")", "\n", "if", "task", ".", "_name", "not", "in", "all_val_metrics", ":", "\n", "                    ", "all_val_metrics", "[", "task", ".", "_name", "]", "=", "{", "}", "\n", "", "for", "name", ",", "value", "in", "task_metrics", ".", "items", "(", ")", ":", "\n", "                    ", "all_val_metrics", "[", "task", ".", "_name", "]", "[", "name", "]", "=", "value", "\n", "", "all_val_metrics", "[", "task", ".", "_name", "]", "[", "\"loss\"", "]", "=", "float", "(", "\n", "val_loss", "/", "n_batches_val_this_epoch_this_task", "\n", ")", "\n", "\n", "# Tensorboard - Validation metrics for this epoch", "\n", "for", "metric_name", ",", "value", "in", "all_val_metrics", "[", "task", ".", "_name", "]", ".", "items", "(", ")", ":", "\n", "                    ", "self", ".", "_tensorboard", ".", "add_validation_scalar", "(", "\n", "name", "=", "\"task_\"", "+", "task", ".", "_name", "+", "\"/\"", "+", "metric_name", ",", "value", "=", "value", "\n", ")", "\n", "\n", "### Perform a patience check and update the history of validation metric for this task ###", "\n", "", "this_epoch_val_metric", "=", "all_val_metrics", "[", "task", ".", "_name", "]", "[", "task", ".", "_val_metric", "]", "\n", "metric_history", "=", "self", ".", "_metric_infos", "[", "task", ".", "_name", "]", "[", "\"hist\"", "]", "\n", "\n", "metric_history", ".", "append", "(", "this_epoch_val_metric", ")", "\n", "is_best_so_far", ",", "out_of_patience", "=", "self", ".", "_check_history", "(", "\n", "metric_history", "=", "metric_history", ",", "\n", "cur_score", "=", "this_epoch_val_metric", ",", "\n", "should_decrease", "=", "task", ".", "_val_metric_decreases", ",", "\n", ")", "\n", "\n", "if", "is_best_so_far", ":", "\n", "                    ", "logger", ".", "info", "(", "\"Best model found for %s.\"", ",", "task", ".", "_name", ")", "\n", "self", ".", "_metric_infos", "[", "task", ".", "_name", "]", "[", "\"best\"", "]", "=", "(", "n_epoch", ",", "all_val_metrics", ")", "\n", "", "if", "(", "\n", "out_of_patience", "\n", "and", "not", "self", ".", "_metric_infos", "[", "task", ".", "_name", "]", "[", "\"is_out_of_patience\"", "]", "\n", ")", ":", "\n", "                    ", "self", ".", "_metric_infos", "[", "task", ".", "_name", "]", "[", "\"is_out_of_patience\"", "]", "=", "True", "\n", "logger", ".", "info", "(", "\n", "\"Task %s is out of patience and vote to stop the training.\"", ",", "\n", "task", ".", "_name", ",", "\n", ")", "\n", "\n", "# The LRScheduler API is agnostic to whether your schedule requires a validation metric -", "\n", "# if it doesn't, the validation metric passed here is ignored.", "\n", "", "scheduler", ".", "step", "(", "this_epoch_val_metric", ",", "n_epoch", ")", "\n", "\n", "", "logger", ".", "info", "(", "\"Validation - End\"", ")", "\n", "\n", "### Print all training and validation metrics for this epoch ###", "\n", "logger", ".", "info", "(", "\n", "\"***** Epoch %d/%d Statistics *****\"", ",", "n_epoch", ",", "self", ".", "_num_epochs", "-", "1", "\n", ")", "\n", "for", "task", "in", "self", ".", "_task_list", ":", "\n", "                ", "logger", ".", "info", "(", "\"Statistic: %s\"", ",", "task", ".", "_name", ")", "\n", "logger", ".", "info", "(", "\n", "\"\\tTraining - %s: %3d\"", ",", "\n", "\"Nb batches trained\"", ",", "\n", "self", ".", "_task_infos", "[", "task", ".", "_name", "]", "[", "\"n_batches_trained_this_epoch\"", "]", ",", "\n", ")", "\n", "for", "metric_name", ",", "value", "in", "all_tr_metrics", "[", "task", ".", "_name", "]", ".", "items", "(", ")", ":", "\n", "                    ", "logger", ".", "info", "(", "\"\\tTraining - %s: %3f\"", ",", "metric_name", ",", "value", ")", "\n", "", "for", "metric_name", ",", "value", "in", "all_val_metrics", "[", "task", ".", "_name", "]", ".", "items", "(", ")", ":", "\n", "                    ", "logger", ".", "info", "(", "\"\\tValidation - %s: %3f\"", ",", "metric_name", ",", "value", ")", "\n", "", "", "logger", ".", "info", "(", "\"**********\"", ")", "\n", "\n", "### Check to see if should stop ###", "\n", "stop_tr", ",", "stop_val", "=", "True", ",", "True", "\n", "\n", "for", "task", "in", "self", ".", "_task_list", ":", "\n", "# task_info = self._task_infos[task._name]", "\n", "                ", "if", "self", ".", "_optimizers", "[", "task", ".", "_name", "]", ".", "param_groups", "[", "0", "]", "[", "\"lr\"", "]", "<", "self", ".", "_min_lr", ":", "\n", "                    ", "logger", ".", "info", "(", "\"Minimum lr hit on %s.\"", ",", "task", ".", "_name", ")", "\n", "logger", ".", "info", "(", "\"Task %s vote to stop training.\"", ",", "task", ".", "_name", ")", "\n", "self", ".", "_metric_infos", "[", "task", ".", "_name", "]", "[", "\"min_lr_hit\"", "]", "=", "True", "\n", "", "stop_tr", "=", "stop_tr", "and", "self", ".", "_metric_infos", "[", "task", ".", "_name", "]", "[", "\"min_lr_hit\"", "]", "\n", "stop_val", "=", "(", "\n", "stop_val", "and", "self", ".", "_metric_infos", "[", "task", ".", "_name", "]", "[", "\"is_out_of_patience\"", "]", "\n", ")", "\n", "\n", "", "if", "stop_tr", ":", "\n", "                ", "should_stop", "=", "True", "\n", "logging", ".", "info", "(", "\"All tasks hit minimum lr. Stopping training.\"", ")", "\n", "", "if", "stop_val", ":", "\n", "                ", "should_stop", "=", "True", "\n", "logging", ".", "info", "(", "\"All metrics ran out of patience. Stopping training.\"", ")", "\n", "", "if", "n_epoch", ">=", "self", ".", "_num_epochs", "-", "1", ":", "\n", "                ", "should_stop", "=", "True", "\n", "logging", ".", "info", "(", "\"Maximum number of epoch hit. Stopping training.\"", ")", "\n", "\n", "", "self", ".", "_save_checkpoint", "(", "n_epoch", ",", "should_stop", ")", "\n", "\n", "### Update n_epoch ###", "\n", "# One epoch = doing N (forward + backward) pass where N is the total number of training batches.", "\n", "n_epoch", "+=", "1", "\n", "\n", "### Summarize training at the end ###", "\n", "", "logging", ".", "info", "(", "\"***** Training is finished *****\"", ")", "\n", "logging", ".", "info", "(", "\"Stopped training after %d epochs\"", ",", "n_epoch", ")", "\n", "return_metrics", "=", "{", "}", "\n", "for", "task_name", ",", "task_info", "in", "self", ".", "_task_infos", ".", "items", "(", ")", ":", "\n", "            ", "nb_epoch_trained", "=", "int", "(", "\n", "task_info", "[", "\"total_n_batches_trained\"", "]", "/", "task_info", "[", "\"n_tr_batches\"", "]", "\n", ")", "\n", "logging", ".", "info", "(", "\n", "\"Trained %s for %d batches ~= %d epochs\"", ",", "\n", "task_name", ",", "\n", "task_info", "[", "\"total_n_batches_trained\"", "]", ",", "\n", "nb_epoch_trained", ",", "\n", ")", "\n", "return_metrics", "[", "task_name", "]", "=", "{", "\n", "\"best_epoch\"", ":", "self", ".", "_metric_infos", "[", "task_name", "]", "[", "\"best\"", "]", "[", "0", "]", ",", "\n", "\"nb_epoch_trained\"", ":", "nb_epoch_trained", ",", "\n", "\"best_epoch_val_metrics\"", ":", "self", ".", "_metric_infos", "[", "task_name", "]", "[", "\"best\"", "]", "[", "1", "]", ",", "\n", "}", "\n", "\n", "", "training_elapsed_time", "=", "time", ".", "time", "(", ")", "-", "training_start_time", "\n", "return_metrics", "[", "\"training_duration\"", "]", "=", "time", ".", "strftime", "(", "\n", "\"%d:%H:%M:%S\"", ",", "time", ".", "gmtime", "(", "training_elapsed_time", ")", "\n", ")", "\n", "return_metrics", "[", "\"nb_epoch_trained\"", "]", "=", "n_epoch", "\n", "\n", "return", "return_metrics", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.training.sampler_multi_task_trainer.SamplerMultiTaskTrainer.from_params": [[487, 525], ["params.pop", "params.pop", "params.pop_int", "params.pop_int", "params.pop_int", "params.pop_float", "params.pop_float", "params.pop_float", "params.pop_bool", "params.pop", "params.pop", "params.pop", "params.pop", "params.assert_empty", "sampler_multi_task_trainer.SamplerMultiTaskTrainer"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.params.Params.pop", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.params.Params.pop", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.params.Params.pop_int", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.params.Params.pop_int", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.params.Params.pop_int", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.params.Params.pop_float", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.params.Params.pop_float", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.params.Params.pop_float", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.params.Params.pop_bool", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.params.Params.pop", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.params.Params.pop", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.params.Params.pop", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.params.Params.pop", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.params.Params.assert_empty"], ["", "@", "classmethod", "\n", "def", "from_params", "(", "\n", "cls", ",", "model", ":", "Model", ",", "task_list", ":", "List", "[", "Task", "]", ",", "serialization_dir", ":", "str", ",", "params", ":", "Params", "\n", ")", "->", "\"SamplerMultiTaskTrainer\"", ":", "\n", "        ", "\"\"\" Generator multi-task trainer from parameters.  \"\"\"", "\n", "\n", "optimizer_params", "=", "params", ".", "pop", "(", "\"optimizer\"", ")", "\n", "lr_scheduler_params", "=", "params", ".", "pop", "(", "\"scheduler\"", ")", "\n", "patience", "=", "params", ".", "pop_int", "(", "\"patience\"", ",", "2", ")", "\n", "num_epochs", "=", "params", ".", "pop_int", "(", "\"num_epochs\"", ",", "20", ")", "\n", "cuda_device", "=", "params", ".", "pop_int", "(", "\"cuda_device\"", ",", "-", "1", ")", "\n", "grad_norm", "=", "params", ".", "pop_float", "(", "\"grad_norm\"", ",", "None", ")", "\n", "grad_clipping", "=", "params", ".", "pop_float", "(", "\"grad_clipping\"", ",", "None", ")", "\n", "min_lr", "=", "params", ".", "pop_float", "(", "\"min_lr\"", ",", "0.0", ")", "\n", "no_tqdm", "=", "params", ".", "pop_bool", "(", "\"no_tqdm\"", ",", "False", ")", "\n", "summary_interval", "=", "params", ".", "pop", "(", "\"sumarry_interval\"", ",", "50", ")", "\n", "log_parameter_statistics", "=", "params", ".", "pop", "(", "\"log_parameter_statistics\"", ",", "False", ")", "\n", "log_gradient_statistics", "=", "params", ".", "pop", "(", "\"log_gradient_statistics\"", ",", "False", ")", "\n", "sampling_method", "=", "params", ".", "pop", "(", "\"sampling_method\"", ",", "\"proportional\"", ")", "\n", "\n", "params", ".", "assert_empty", "(", "cls", ".", "__name__", ")", "\n", "return", "SamplerMultiTaskTrainer", "(", "\n", "model", "=", "model", ",", "\n", "task_list", "=", "task_list", ",", "\n", "optimizer_params", "=", "optimizer_params", ",", "\n", "lr_scheduler_params", "=", "lr_scheduler_params", ",", "\n", "patience", "=", "patience", ",", "\n", "num_epochs", "=", "num_epochs", ",", "\n", "serialization_dir", "=", "serialization_dir", ",", "\n", "cuda_device", "=", "cuda_device", ",", "\n", "grad_norm", "=", "grad_norm", ",", "\n", "grad_clipping", "=", "grad_clipping", ",", "\n", "min_lr", "=", "min_lr", ",", "\n", "no_tqdm", "=", "no_tqdm", ",", "\n", "summary_interval", "=", "summary_interval", ",", "\n", "log_parameter_statistics", "=", "log_parameter_statistics", ",", "\n", "log_gradient_statistics", "=", "log_gradient_statistics", ",", "\n", "sampling_method", "=", "sampling_method", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.training.metric_tracker.MetricTracker.__init__": [[32, 66], ["allennlp.common.checks.ConfigurationError", "allennlp.common.checks.ConfigurationError", "allennlp.common.checks.ConfigurationError"], "methods", ["None"], ["def", "__init__", "(", "\n", "self", ",", "\n", "patience", ":", "Optional", "[", "int", "]", "=", "None", ",", "\n", "metric_name", ":", "str", "=", "None", ",", "\n", "should_decrease", ":", "bool", "=", "None", ",", "\n", ")", "->", "None", ":", "\n", "        ", "self", ".", "_best_so_far", ":", "float", "=", "None", "\n", "self", ".", "_patience", "=", "patience", "\n", "self", ".", "_epochs_with_no_improvement", "=", "0", "\n", "self", ".", "_is_best_so_far", "=", "True", "\n", "self", ".", "best_epoch_metrics", ":", "Dict", "[", "str", ",", "float", "]", "=", "{", "}", "\n", "self", ".", "_epoch_number", "=", "0", "\n", "self", ".", "best_epoch", ":", "int", "=", "None", "\n", "\n", "# If the metric name starts with \"+\", we want it to increase.", "\n", "# If the metric name starts with \"-\", we want it to decrease.", "\n", "# We also allow you to not specify a metric name and just set `should_decrease` directly.", "\n", "if", "should_decrease", "is", "None", "and", "metric_name", "is", "None", ":", "\n", "            ", "raise", "ConfigurationError", "(", "\n", "\"must specify either `should_decrease` or `metric_name` (but not both)\"", "\n", ")", "\n", "", "elif", "should_decrease", "is", "not", "None", "and", "metric_name", "is", "not", "None", ":", "\n", "            ", "raise", "ConfigurationError", "(", "\n", "\"must specify either `should_decrease` or `metric_name` (but not both)\"", "\n", ")", "\n", "", "elif", "metric_name", "is", "not", "None", ":", "\n", "            ", "if", "metric_name", "[", "0", "]", "==", "\"-\"", ":", "\n", "                ", "self", ".", "_should_decrease", "=", "True", "\n", "", "elif", "metric_name", "[", "0", "]", "==", "\"+\"", ":", "\n", "                ", "self", ".", "_should_decrease", "=", "False", "\n", "", "else", ":", "\n", "                ", "raise", "ConfigurationError", "(", "\"metric_name must start with + or -\"", ")", "\n", "", "", "else", ":", "\n", "            ", "self", ".", "_should_decrease", "=", "should_decrease", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.training.metric_tracker.MetricTracker.clear": [[67, 76], ["None"], "methods", ["None"], ["", "", "def", "clear", "(", "self", ")", "->", "None", ":", "\n", "        ", "\"\"\"\n        Clears out the tracked metrics, but keeps the patience and should_decrease settings.\n        \"\"\"", "\n", "self", ".", "_best_so_far", "=", "None", "\n", "self", ".", "_epochs_with_no_improvement", "=", "0", "\n", "self", ".", "_is_best_so_far", "=", "True", "\n", "self", ".", "_epoch_number", "=", "0", "\n", "self", ".", "best_epoch", "=", "None", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.training.metric_tracker.MetricTracker.state_dict": [[77, 90], ["None"], "methods", ["None"], ["", "def", "state_dict", "(", "self", ")", "->", "Dict", "[", "str", ",", "Any", "]", ":", "\n", "        ", "\"\"\"\n        A `Trainer` can use this to serialize the state of the metric tracker.\n        \"\"\"", "\n", "return", "{", "\n", "\"best_so_far\"", ":", "self", ".", "_best_so_far", ",", "\n", "\"patience\"", ":", "self", ".", "_patience", ",", "\n", "\"epochs_with_no_improvement\"", ":", "self", ".", "_epochs_with_no_improvement", ",", "\n", "\"is_best_so_far\"", ":", "self", ".", "_is_best_so_far", ",", "\n", "\"should_decrease\"", ":", "self", ".", "_should_decrease", ",", "\n", "\"best_epoch_metrics\"", ":", "self", ".", "best_epoch_metrics", ",", "\n", "\"epoch_number\"", ":", "self", ".", "_epoch_number", ",", "\n", "\"best_epoch\"", ":", "self", ".", "best_epoch", ",", "\n", "}", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.training.metric_tracker.MetricTracker.load_state_dict": [[92, 104], ["None"], "methods", ["None"], ["", "def", "load_state_dict", "(", "self", ",", "state_dict", ":", "Dict", "[", "str", ",", "Any", "]", ")", "->", "None", ":", "\n", "        ", "\"\"\"\n        A `Trainer` can use this to hydrate a metric tracker from a serialized state.\n        \"\"\"", "\n", "self", ".", "_best_so_far", "=", "state_dict", "[", "\"best_so_far\"", "]", "\n", "self", ".", "_patience", "=", "state_dict", "[", "\"patience\"", "]", "\n", "self", ".", "_epochs_with_no_improvement", "=", "state_dict", "[", "\"epochs_with_no_improvement\"", "]", "\n", "self", ".", "_is_best_so_far", "=", "state_dict", "[", "\"is_best_so_far\"", "]", "\n", "self", ".", "_should_decrease", "=", "state_dict", "[", "\"should_decrease\"", "]", "\n", "self", ".", "best_epoch_metrics", "=", "state_dict", "[", "\"best_epoch_metrics\"", "]", "\n", "self", ".", "_epoch_number", "=", "state_dict", "[", "\"epoch_number\"", "]", "\n", "self", ".", "best_epoch", "=", "state_dict", "[", "\"best_epoch\"", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.training.metric_tracker.MetricTracker.add_metric": [[105, 124], ["None"], "methods", ["None"], ["", "def", "add_metric", "(", "self", ",", "metric", ":", "float", ")", "->", "None", ":", "\n", "        ", "\"\"\"\n        Record a new value of the metric and update the various things that depend on it.\n        \"\"\"", "\n", "new_best", "=", "(", "\n", "(", "self", ".", "_best_so_far", "is", "None", ")", "\n", "or", "(", "self", ".", "_should_decrease", "and", "metric", "<", "self", ".", "_best_so_far", ")", "\n", "or", "(", "not", "self", ".", "_should_decrease", "and", "metric", ">", "self", ".", "_best_so_far", ")", "\n", ")", "\n", "\n", "if", "new_best", ":", "\n", "            ", "self", ".", "best_epoch", "=", "self", ".", "_epoch_number", "\n", "self", ".", "_is_best_so_far", "=", "True", "\n", "self", ".", "_best_so_far", "=", "metric", "\n", "self", ".", "_epochs_with_no_improvement", "=", "0", "\n", "", "else", ":", "\n", "            ", "self", ".", "_is_best_so_far", "=", "False", "\n", "self", ".", "_epochs_with_no_improvement", "+=", "1", "\n", "", "self", ".", "_epoch_number", "+=", "1", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.training.metric_tracker.MetricTracker.add_metrics": [[125, 131], ["metric_tracker.MetricTracker.add_metric"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.training.metric_tracker.MetricTracker.add_metric"], ["", "def", "add_metrics", "(", "self", ",", "metrics", ":", "Iterable", "[", "float", "]", ")", "->", "None", ":", "\n", "        ", "\"\"\"\n        Helper to add multiple metrics at once.\n        \"\"\"", "\n", "for", "metric", "in", "metrics", ":", "\n", "            ", "self", ".", "add_metric", "(", "metric", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.training.metric_tracker.MetricTracker.is_best_so_far": [[132, 137], ["None"], "methods", ["None"], ["", "", "def", "is_best_so_far", "(", "self", ")", "->", "bool", ":", "\n", "        ", "\"\"\"\n        Returns true if the most recent value of the metric is the best so far.\n        \"\"\"", "\n", "return", "self", ".", "_is_best_so_far", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.training.metric_tracker.MetricTracker.should_stop_early": [[138, 146], ["None"], "methods", ["None"], ["", "def", "should_stop_early", "(", "self", ")", "->", "bool", ":", "\n", "        ", "\"\"\"\n        Returns true if improvement has stopped for long enough.\n        \"\"\"", "\n", "if", "self", ".", "_patience", "is", "None", ":", "\n", "            ", "return", "False", "\n", "", "else", ":", "\n", "            ", "return", "self", ".", "_epochs_with_no_improvement", ">=", "self", ".", "_patience", "\n", "", "", "", ""]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.training.moving_average.MovingAverage.__init__": [[17, 24], ["list", "parameter.data.clone", "parameter.data.clone"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.nn.util.clone", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.nn.util.clone"], ["def", "__init__", "(", "self", ",", "parameters", ":", "Iterable", "[", "NamedParameter", "]", ")", "->", "None", ":", "\n", "        ", "self", ".", "_parameters", "=", "list", "(", "parameters", ")", "\n", "self", ".", "_shadows", "=", "{", "\n", "name", ":", "parameter", ".", "data", ".", "clone", "(", ")", "for", "name", ",", "parameter", "in", "self", ".", "_parameters", "\n", "}", "\n", "self", ".", "_backups", "=", "{", "\n", "name", ":", "parameter", ".", "data", ".", "clone", "(", ")", "for", "name", ",", "parameter", "in", "self", ".", "_parameters", "\n", "}", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.training.moving_average.MovingAverage.apply": [[26, 31], ["None"], "methods", ["None"], ["", "def", "apply", "(", "self", ",", "num_updates", ":", "Optional", "[", "int", "]", "=", "None", ")", ":", "\n", "        ", "\"\"\"\n        Update the moving averages based on the latest values of the parameters.\n        \"\"\"", "\n", "raise", "NotImplementedError", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.training.moving_average.MovingAverage.assign_average_value": [[32, 40], ["moving_average.MovingAverage._backups[].copy_", "parameter.data.copy_"], "methods", ["None"], ["", "def", "assign_average_value", "(", "self", ")", "->", "None", ":", "\n", "        ", "\"\"\"\n        Replace all the parameter values with the averages.\n        Save the current parameter values to restore later.\n        \"\"\"", "\n", "for", "name", ",", "parameter", "in", "self", ".", "_parameters", ":", "\n", "            ", "self", ".", "_backups", "[", "name", "]", ".", "copy_", "(", "parameter", ".", "data", ")", "\n", "parameter", ".", "data", ".", "copy_", "(", "self", ".", "_shadows", "[", "name", "]", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.training.moving_average.MovingAverage.restore": [[41, 47], ["parameter.data.copy_"], "methods", ["None"], ["", "", "def", "restore", "(", "self", ")", "->", "None", ":", "\n", "        ", "\"\"\"\n        Restore the backed-up (non-average) parameter values.\n        \"\"\"", "\n", "for", "name", ",", "parameter", "in", "self", ".", "_parameters", ":", "\n", "            ", "parameter", ".", "data", ".", "copy_", "(", "self", ".", "_backups", "[", "name", "]", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.training.moving_average.ExponentialMovingAverage.__init__": [[67, 78], ["moving_average.MovingAverage.__init__"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.graph_util.plot.GraphPlot.__init__"], ["def", "__init__", "(", "\n", "self", ",", "\n", "parameters", ":", "Iterable", "[", "NamedParameter", "]", ",", "\n", "decay", ":", "float", "=", "0.9999", ",", "\n", "numerator", ":", "float", "=", "1.0", ",", "\n", "denominator", ":", "float", "=", "10.0", ",", "\n", ")", "->", "None", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "parameters", ")", "\n", "self", ".", "_decay", "=", "decay", "\n", "self", ".", "_numerator", "=", "numerator", "\n", "self", ".", "_denominator", "=", "denominator", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.training.moving_average.ExponentialMovingAverage.apply": [[79, 102], ["min", "moving_average.ExponentialMovingAverage._shadows[].mul_().add_", "moving_average.ExponentialMovingAverage._shadows[].mul_"], "methods", ["None"], ["", "def", "apply", "(", "self", ",", "num_updates", ":", "Optional", "[", "int", "]", "=", "None", ")", "->", "None", ":", "\n", "        ", "\"\"\"\n        Apply exponential moving average to `named_parameters` if specified,\n        or we will apply this to all the trainable parameters of the model.\n\n        The optional `num_updates` parameter allows one to tweak the decay rate\n        dynamically. If passed, the actual decay rate used is:\n\n            `min(decay, (numerator + num_updates) / (denominator + num_updates))`\n\n        (This logic is based on the Tensorflow exponential moving average\n         https://www.tensorflow.org/api_docs/python/tf/train/ExponentialMovingAverage)\n        \"\"\"", "\n", "if", "num_updates", "is", "not", "None", ":", "\n", "            ", "decay", "=", "min", "(", "\n", "self", ".", "_decay", ",", "\n", "(", "self", ".", "_numerator", "+", "num_updates", ")", "/", "(", "self", ".", "_denominator", "+", "num_updates", ")", ",", "\n", ")", "\n", "", "else", ":", "\n", "            ", "decay", "=", "self", ".", "_decay", "\n", "\n", "", "for", "name", ",", "parameter", "in", "self", ".", "_parameters", ":", "\n", "            ", "self", ".", "_shadows", "[", "name", "]", ".", "mul_", "(", "decay", ")", ".", "add_", "(", "(", "1", "-", "decay", ")", "*", "parameter", ".", "data", ")", "\n", "", "", "", ""]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.training.trainer_base.TrainerBase.__init__": [[30, 66], ["allennlp.common.checks.check_for_gpu", "isinstance", "allennlp.common.checks.ConfigurationError", "isinstance", "allennlp.common.checks.ConfigurationError", "allennlp.common.checks.ConfigurationError"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.checks.check_for_gpu"], ["def", "__init__", "(", "\n", "self", ",", "\n", "serialization_dir", ":", "str", ",", "\n", "cuda_device", ":", "int", "=", "-", "1", ",", "\n", "distributed", ":", "bool", "=", "False", ",", "\n", "local_rank", ":", "int", "=", "0", ",", "\n", "world_size", ":", "int", "=", "1", ",", "\n", ")", "->", "None", ":", "\n", "\n", "        ", "check_for_gpu", "(", "cuda_device", ")", "\n", "self", ".", "_serialization_dir", "=", "serialization_dir", "\n", "\n", "if", "isinstance", "(", "cuda_device", ",", "list", ")", ":", "\n", "            ", "raise", "ConfigurationError", "(", "\n", "\"In allennlp 1.0, the Trainer can only be assigned a single `cuda_device`. \"", "\n", "\"Instead, we use torch's DistributedDataParallel at the command level, meaning \"", "\n", "\"our Trainer always uses a single GPU per process.\"", "\n", ")", "\n", "\n", "", "if", "not", "isinstance", "(", "cuda_device", ",", "int", ")", ":", "\n", "            ", "raise", "ConfigurationError", "(", "\n", "\"Expected an int for cuda_device, got {}\"", ".", "format", "(", "cuda_device", ")", "\n", ")", "\n", "\n", "", "if", "distributed", "and", "world_size", "<=", "1", ":", "\n", "            ", "raise", "ConfigurationError", "(", "\n", "\"Distributed training can be performed only with more than 1 GPU device. Check \"", "\n", "\"`cuda_device` key in the experiment configuration.\"", "\n", ")", "\n", "\n", "", "self", ".", "cuda_device", "=", "cuda_device", "\n", "\n", "self", ".", "_distributed", "=", "distributed", "\n", "self", ".", "_rank", "=", "local_rank", "\n", "self", ".", "_master", "=", "self", ".", "_rank", "==", "0", "\n", "self", ".", "_world_size", "=", "world_size", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.training.trainer_base.TrainerBase._move_to_gpu": [[67, 72], ["model.cuda"], "methods", ["None"], ["", "def", "_move_to_gpu", "(", "self", ",", "model", ":", "Model", ")", "->", "Model", ":", "\n", "        ", "if", "self", ".", "cuda_device", "!=", "-", "1", ":", "\n", "            ", "return", "model", ".", "cuda", "(", "self", ".", "cuda_device", ")", "\n", "", "else", ":", "\n", "            ", "return", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.training.trainer_base.TrainerBase.train": [[73, 78], ["None"], "methods", ["None"], ["", "", "def", "train", "(", "self", ")", "->", "Dict", "[", "str", ",", "Any", "]", ":", "\n", "        ", "\"\"\"\n        Train a model and return the results.\n        \"\"\"", "\n", "raise", "NotImplementedError", "\n", "", "", ""]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.training.callback_trainer.CallbackTrainer.__init__": [[45, 162], ["allennlp.training.trainer_base.TrainerBase.__init__", "logger.warning", "allennlp.training.callbacks.callback_handler.CallbackHandler", "torch.nn.parallel.DistributedDataParallel"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.graph_util.plot.GraphPlot.__init__"], ["    ", "def", "__init__", "(", "\n", "self", ",", "\n", "model", ":", "Model", ",", "\n", "training_data", ":", "Iterable", "[", "Instance", "]", ",", "\n", "iterator", ":", "DataIterator", ",", "\n", "optimizer", ":", "torch", ".", "optim", ".", "Optimizer", ",", "\n", "num_epochs", ":", "int", "=", "20", ",", "\n", "shuffle", ":", "bool", "=", "True", ",", "\n", "serialization_dir", ":", "Optional", "[", "str", "]", "=", "None", ",", "\n", "cuda_device", ":", "int", "=", "-", "1", ",", "\n", "callbacks", ":", "List", "[", "Callback", "]", "=", "None", ",", "\n", "distributed", ":", "bool", "=", "False", ",", "\n", "rank", ":", "int", "=", "0", ",", "\n", "world_size", ":", "int", "=", "1", ",", "\n", ")", "->", "None", ":", "\n", "        ", "\"\"\"\n        A trainer for doing supervised learning. It just takes a labeled dataset\n        and a `DataIterator`, and uses the supplied `Optimizer` to learn the weights\n        for your model over some fixed number of epochs. It uses callbacks to handle various\n        things ancillary to training, like tracking metrics, validation, early stopping,\n        logging to tensorboard, and so on.\n\n        It's easy to create your own callbacks; for example, if you wanted to get a Slack\n        notification when training finishes. For more complicated variations, you might have\n        to create your own subclass, in which case make sure to fire off all the training events.\n\n        # Parameters\n\n        model : `Model`, required.\n            An AllenNLP model to be optimized. Pytorch Modules can also be optimized if\n            their `forward` method returns a dictionary with a \"loss\" key, containing a\n            scalar tensor representing the loss function to be optimized.\n\n            If you are training your model using GPUs, your model should already be\n            on the correct device. (If you use `Trainer.from_params` this will be\n            handled for you.)\n        training_data : `Iterable[Instance]`, required\n            The instances that you want to train your model on.\n        iterator : `DataIterator`, required\n            The iterator for batching / epoch-ing the instances.\n        optimizer : `torch.nn.Optimizer`, required.\n            An instance of a Pytorch Optimizer, instantiated with the parameters of the\n            model to be optimized.\n        num_epochs : int, optional (default=20)\n            Number of training epochs.\n        shuffle : bool, optional (default=True)\n            Whether to shuffle the instances each epoch.\n        serialization_dir : str, optional (default=None)\n            Path to directory for saving and loading model files. Models will not be saved if\n            this parameter is not passed.\n        cuda_device : `int`, optional (default=-1)\n            An integer or list of integers specifying the CUDA device(s) to use. If -1, the CPU is used.\n            Data parallelism is controlled at the allennlp train level, so each trainer will have a single\n            GPU.\n        callbacks : `List[Callback]`, optional (default=None)\n            A list of callbacks that will be called based on training events.\n        \"\"\"", "\n", "super", "(", ")", ".", "__init__", "(", "serialization_dir", ",", "cuda_device", ",", "distributed", ",", "rank", ",", "world_size", ")", "\n", "\n", "logger", ".", "warning", "(", "\n", "\"The CallbackTrainer should be considered 'experimental' code, \"", "\n", "\"and its behavior may change as we use it more and iterate on it.\"", "\n", ")", "\n", "\n", "# This is all state that the callbacks might want:", "\n", "# I am not calling move_to_gpu here, because if the model is", "\n", "# not already on the GPU then the optimizer is going to be wrong.", "\n", "self", ".", "model", "=", "model", "\n", "self", ".", "optimizer", "=", "optimizer", "\n", "self", ".", "validate", "=", "False", "\n", "\n", "# For capturing mid / end-of-epoch metrics", "\n", "self", ".", "train_metrics", ":", "Dict", "[", "str", ",", "float", "]", "=", "{", "}", "\n", "self", ".", "val_metrics", ":", "Dict", "[", "str", ",", "float", "]", "=", "{", "}", "\n", "self", ".", "latest_val_metric", "=", "0.0", "\n", "self", ".", "train_loss", "=", "0.0", "\n", "\n", "# For capturing overall metrics", "\n", "self", ".", "metrics", ":", "Dict", "[", "str", ",", "Any", "]", "=", "{", "}", "\n", "\n", "self", ".", "batch_num_total", "=", "0", "\n", "self", ".", "batch", ":", "TensorDict", "=", "None", "\n", "self", ".", "batches_this_epoch", "=", "0", "\n", "\n", "self", ".", "training_batches", ":", "Iterable", "[", "List", "[", "TensorDict", "]", "]", "=", "(", ")", "\n", "self", ".", "num_training_batches", "=", "0", "\n", "\n", "self", ".", "should_stop_early", "=", "False", "\n", "self", ".", "num_epochs", "=", "num_epochs", "\n", "\n", "self", ".", "training_start_time", "=", "0.0", "\n", "\n", "self", ".", "last_log", "=", "0.0", "\n", "self", ".", "epoch_number", "=", "0", "\n", "self", ".", "batch_grad_norm", ":", "Optional", "[", "float", "]", "=", "None", "\n", "\n", "self", ".", "training_data", "=", "training_data", "\n", "self", ".", "iterator", "=", "iterator", "\n", "self", ".", "shuffle", "=", "shuffle", "\n", "self", ".", "handler", "=", "CallbackHandler", "(", "callbacks", ",", "self", ")", "\n", "\n", "# For capturing errors that occur during the train loop.", "\n", "self", ".", "exception", ":", "Optional", "[", "Exception", "]", "=", "None", "\n", "\n", "# Using `DistributedDataParallel`(ddp) brings in a quirk wrt AllenNLP's `Model` interface and its", "\n", "# usage. A `Model` object is wrapped by `ddp`, but assigning the wrapped model to `self.model`", "\n", "# will break the usages such as `Model.get_regularization_penalty`, `Model.get_metrics`, etc.", "\n", "#", "\n", "# Hence a reference to Pytorch's object is maintained in the case of distributed training and in the", "\n", "# normal case, reference to `Model` is retained. This reference is only used in", "\n", "# these places: `model.__call__`, `model.train` and `model.eval`.", "\n", "if", "self", ".", "_distributed", ":", "\n", "            ", "self", ".", "_pytorch_model", "=", "DistributedDataParallel", "(", "\n", "self", ".", "model", ",", "device_ids", "=", "[", "self", ".", "_rank", "]", "\n", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "_pytorch_model", "=", "self", ".", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.training.callback_trainer.CallbackTrainer.generate_training_batches": [[163, 173], ["callback_trainer.CallbackTrainer.iterator", "callback_trainer.CallbackTrainer.iterator.get_num_batches"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.iterators.data_iterator.DataIterator.get_num_batches"], ["", "", "def", "generate_training_batches", "(", "self", ")", ":", "\n", "        ", "\"\"\"\n        Generates one epoch worth of training data. Stores it in trainer instance variables\n        so that callbacks can access it.\n        \"\"\"", "\n", "train_generator", "=", "self", ".", "iterator", "(", "\n", "self", ".", "training_data", ",", "num_epochs", "=", "1", ",", "shuffle", "=", "self", ".", "shuffle", "\n", ")", "\n", "self", ".", "training_batches", "=", "train_generator", "\n", "self", ".", "num_training_batches", "=", "self", ".", "iterator", ".", "get_num_batches", "(", "self", ".", "training_data", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.training.callback_trainer.CallbackTrainer.batch_loss": [[174, 198], ["allennlp.nn.util.move_to_device", "callback_trainer.CallbackTrainer._pytorch_model", "callback_trainer.CallbackTrainer.model.get_regularization_penalty", "RuntimeError"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.nn.util.move_to_device", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.models.model.Model.get_regularization_penalty"], ["", "def", "batch_loss", "(", "self", ",", "batch", ":", "TensorDict", ",", "for_training", ":", "bool", ")", "->", "torch", ".", "Tensor", ":", "\n", "        ", "\"\"\"\n        Does a forward pass on the given batches and returns the `loss` value in the result.\n        If `for_training` is `True` also applies regularization penalty.\n\n        This is a method on the trainer so that it can be used both in training and validation\n        (which are handled separately).\n        \"\"\"", "\n", "batch", "=", "nn_util", ".", "move_to_device", "(", "batch", ",", "self", ".", "cuda_device", ")", "\n", "output_dict", "=", "self", ".", "_pytorch_model", "(", "**", "batch", ")", "\n", "\n", "try", ":", "\n", "            ", "loss", "=", "output_dict", "[", "\"loss\"", "]", "\n", "if", "for_training", ":", "\n", "                ", "loss", "+=", "self", ".", "model", ".", "get_regularization_penalty", "(", ")", "\n", "", "", "except", "KeyError", ":", "\n", "            ", "if", "for_training", ":", "\n", "                ", "raise", "RuntimeError", "(", "\n", "\"The model you are trying to optimize does not contain a\"", "\n", "\" 'loss' key in the output of model.forward(inputs).\"", "\n", ")", "\n", "", "loss", "=", "None", "\n", "\n", "", "return", "loss", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.training.callback_trainer.CallbackTrainer.train_one_batch": [[199, 231], ["callback_trainer.CallbackTrainer.handler.fire_event", "callback_trainer.CallbackTrainer.optimizer.zero_grad", "callback_trainer.CallbackTrainer.handler.fire_event", "callback_trainer.CallbackTrainer.batch_loss", "torch.isnan", "callback_trainer.CallbackTrainer.backward", "callback_trainer.CallbackTrainer.item", "callback_trainer.CallbackTrainer.handler.fire_event", "callback_trainer.CallbackTrainer.optimizer.step", "allennlp.training.util.get_metrics", "callback_trainer.CallbackTrainer.handler.fire_event", "allennlp.training.util.description_from_metrics", "ValueError"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.callbacks.callback_handler.CallbackHandler.fire_event", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.callbacks.callback_handler.CallbackHandler.fire_event", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.training.trainer.Trainer.batch_loss", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.callbacks.callback_handler.CallbackHandler.fire_event", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.learning_rate_schedulers.slanted_triangular.SlantedTriangular.step", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.coreference_resolution.coref.CoreferenceResolver.get_metrics", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.callbacks.callback_handler.CallbackHandler.fire_event", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.training.util.description_from_metrics"], ["", "def", "train_one_batch", "(", "self", ",", "batch", ":", "TensorDict", ")", "->", "str", ":", "\n", "        ", "\"\"\"\n        Handles the training for a single batch group.\n        Fires off the events BATCH_START, FORWARD, BACKWARD, and BATCH_END.\n        \"\"\"", "\n", "self", ".", "handler", ".", "fire_event", "(", "Events", ".", "BATCH_START", ")", "\n", "self", ".", "optimizer", ".", "zero_grad", "(", ")", "\n", "\n", "self", ".", "batches_this_epoch", "+=", "1", "\n", "self", ".", "batch_num_total", "+=", "1", "\n", "\n", "self", ".", "handler", ".", "fire_event", "(", "Events", ".", "FORWARD", ")", "\n", "loss", "=", "self", ".", "batch_loss", "(", "batch", ",", "for_training", "=", "True", ")", "\n", "\n", "if", "torch", ".", "isnan", "(", "loss", ")", ":", "\n", "            ", "raise", "ValueError", "(", "\"nan loss encountered\"", ")", "\n", "\n", "", "loss", ".", "backward", "(", ")", "\n", "self", ".", "train_loss", "+=", "loss", ".", "item", "(", ")", "\n", "\n", "self", ".", "handler", ".", "fire_event", "(", "Events", ".", "BACKWARD", ")", "\n", "\n", "self", ".", "optimizer", ".", "step", "(", ")", "\n", "\n", "# Update the description with the latest metrics", "\n", "self", ".", "train_metrics", "=", "training_util", ".", "get_metrics", "(", "\n", "self", ".", "model", ",", "self", ".", "train_loss", ",", "self", ".", "batches_this_epoch", "\n", ")", "\n", "\n", "self", ".", "handler", ".", "fire_event", "(", "Events", ".", "BATCH_END", ")", "\n", "\n", "return", "training_util", ".", "description_from_metrics", "(", "self", ".", "train_metrics", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.training.callback_trainer.CallbackTrainer.train_one_epoch": [[232, 257], ["callback_trainer.CallbackTrainer.handler.fire_event", "callback_trainer.CallbackTrainer._pytorch_model.train", "time.time", "logger.info", "allennlp.common.Tqdm.tqdm", "callback_trainer.CallbackTrainer.handler.fire_event", "callback_trainer.CallbackTrainer.handler.fire_event", "callback_trainer.CallbackTrainer.train_one_batch", "allennlp.common.Tqdm.tqdm.set_description"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.callbacks.callback_handler.CallbackHandler.fire_event", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.training.no_op_trainer.NoOpTrainer.train", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.dataset_readers.multiprocess_dataset_reader.logger.info", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.tqdm.Tqdm.tqdm", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.callbacks.callback_handler.CallbackHandler.fire_event", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.callbacks.callback_handler.CallbackHandler.fire_event", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.training.callback_trainer.CallbackTrainer.train_one_batch"], ["", "def", "train_one_epoch", "(", "self", ")", "->", "None", ":", "\n", "        ", "\"\"\"\n        Trains the model for a single epoch.\n        Fires off the events EPOCH_START and EPOCH_END,\n        and repeatedly calls self.train_one_batch().\n        \"\"\"", "\n", "self", ".", "handler", ".", "fire_event", "(", "Events", ".", "EPOCH_START", ")", "\n", "\n", "self", ".", "train_loss", "=", "0.0", "\n", "# Set the model to \"train\" mode.", "\n", "self", ".", "_pytorch_model", ".", "train", "(", ")", "\n", "\n", "self", ".", "last_log", "=", "time", ".", "time", "(", ")", "\n", "\n", "logger", ".", "info", "(", "\"Training\"", ")", "\n", "self", ".", "batches_this_epoch", "=", "0", "\n", "\n", "batches_tqdm", "=", "Tqdm", ".", "tqdm", "(", "self", ".", "training_batches", ",", "total", "=", "self", ".", "num_training_batches", ")", "\n", "\n", "for", "self", ".", "batch", "in", "batches_tqdm", ":", "\n", "            ", "description", "=", "self", ".", "train_one_batch", "(", "self", ".", "batch", ")", "\n", "batches_tqdm", ".", "set_description", "(", "description", ",", "refresh", "=", "False", ")", "\n", "\n", "", "self", ".", "handler", ".", "fire_event", "(", "Events", ".", "VALIDATE", ")", "\n", "self", ".", "handler", ".", "fire_event", "(", "Events", ".", "EPOCH_END", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.training.callback_trainer.CallbackTrainer.train": [[258, 301], ["logger.info", "callback_trainer.CallbackTrainer.handler.fire_event", "time.time", "range", "callback_trainer.CallbackTrainer.handler.fire_event", "time.time", "callback_trainer.CallbackTrainer.generate_training_batches", "callback_trainer.CallbackTrainer.train_one_epoch", "logger.info", "time.time", "datetime.timedelta", "str", "logger.info", "logger.info", "time.time", "datetime.timedelta", "float", "int"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.dataset_readers.multiprocess_dataset_reader.logger.info", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.callbacks.callback_handler.CallbackHandler.fire_event", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.callbacks.callback_handler.CallbackHandler.fire_event", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.training.callback_trainer.CallbackTrainer.generate_training_batches", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.training.callback_trainer.CallbackTrainer.train_one_epoch", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.dataset_readers.multiprocess_dataset_reader.logger.info", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.dataset_readers.multiprocess_dataset_reader.logger.info", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.dataset_readers.multiprocess_dataset_reader.logger.info"], ["", "@", "handle_errors", "\n", "def", "train", "(", "self", ")", "->", "Dict", "[", "str", ",", "Any", "]", ":", "\n", "        ", "\"\"\"\n        Trains the supplied model with the supplied parameters.\n        Fires off the events TRAINING_START and TRAINING END,\n        and repeatedly calls `self.train_one_epoch()`.\n        \"\"\"", "\n", "logger", ".", "info", "(", "\"Beginning training.\"", ")", "\n", "self", ".", "handler", ".", "fire_event", "(", "Events", ".", "TRAINING_START", ")", "\n", "\n", "self", ".", "training_start_time", "=", "time", ".", "time", "(", ")", "\n", "starting_epoch", "=", "self", ".", "epoch_number", "\n", "\n", "for", "self", ".", "epoch_number", "in", "range", "(", "self", ".", "epoch_number", ",", "self", ".", "num_epochs", ")", ":", "\n", "            ", "epoch_start_time", "=", "time", ".", "time", "(", ")", "\n", "\n", "self", ".", "generate_training_batches", "(", ")", "\n", "self", ".", "train_one_epoch", "(", ")", "\n", "\n", "epoch_elapsed_time", "=", "time", ".", "time", "(", ")", "-", "epoch_start_time", "\n", "logger", ".", "info", "(", "\n", "\"Epoch duration: %s\"", ",", "datetime", ".", "timedelta", "(", "seconds", "=", "epoch_elapsed_time", ")", "\n", ")", "\n", "\n", "if", "self", ".", "epoch_number", "<", "self", ".", "num_epochs", "-", "1", ":", "\n", "                ", "training_elapsed_time", "=", "time", ".", "time", "(", ")", "-", "self", ".", "training_start_time", "\n", "estimated_time_remaining", "=", "training_elapsed_time", "*", "(", "\n", "(", "self", ".", "num_epochs", "-", "starting_epoch", ")", "\n", "/", "float", "(", "self", ".", "epoch_number", "-", "starting_epoch", "+", "1", ")", "\n", "-", "1", "\n", ")", "\n", "formatted_time", "=", "str", "(", "\n", "datetime", ".", "timedelta", "(", "seconds", "=", "int", "(", "estimated_time_remaining", ")", ")", "\n", ")", "\n", "logger", ".", "info", "(", "\"Estimated training time remaining: %s\"", ",", "formatted_time", ")", "\n", "\n", "", "if", "self", ".", "should_stop_early", ":", "\n", "                ", "logger", ".", "info", "(", "\"Ran out of patience.  Stopping training.\"", ")", "\n", "break", "\n", "\n", "", "", "self", ".", "handler", ".", "fire_event", "(", "Events", ".", "TRAINING_END", ")", "\n", "\n", "return", "self", ".", "metrics", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.training.callback_trainer.CallbackTrainer.from_partial_objects": [[302, 363], ["allennlp.common.checks.check_for_gpu", "optimizer.construct", "cls", "model.cuda.cuda.cuda", "model.cuda.cuda.named_parameters", "callback.construct", "constructed_callbacks.append"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.checks.check_for_gpu", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.lazy.Lazy.construct", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.lazy.Lazy.construct"], ["", "@", "classmethod", "\n", "def", "from_partial_objects", "(", "\n", "cls", ",", "\n", "model", ":", "Model", ",", "\n", "serialization_dir", ":", "str", ",", "\n", "iterator", ":", "DataIterator", ",", "\n", "train_data", ":", "Iterable", "[", "Instance", "]", ",", "\n", "validation_iterator", ":", "DataIterator", "=", "None", ",", "\n", "validation_data", ":", "Iterable", "[", "Instance", "]", "=", "None", ",", "\n", "callbacks", ":", "List", "[", "Lazy", "[", "Callback", "]", "]", "=", "None", ",", "\n", "local_rank", ":", "int", "=", "0", ",", "\n", "patience", ":", "int", "=", "None", ",", "\n", "validation_metric", ":", "str", "=", "\"-loss\"", ",", "\n", "shuffle", ":", "bool", "=", "True", ",", "\n", "num_epochs", ":", "int", "=", "20", ",", "\n", "cuda_device", ":", "int", "=", "-", "1", ",", "\n", "distributed", ":", "bool", "=", "False", ",", "\n", "world_size", ":", "int", "=", "1", ",", "\n", "optimizer", ":", "Lazy", "[", "Optimizer", "]", "=", "None", ",", "\n", ")", "->", "\"CallbackTrainer\"", ":", "\n", "\n", "        ", "check_for_gpu", "(", "cuda_device", ")", "\n", "if", "cuda_device", ">=", "0", ":", "\n", "# Moving model to GPU here so that the optimizer state gets constructed on", "\n", "# the right device.", "\n", "            ", "model", "=", "model", ".", "cuda", "(", "cuda_device", ")", "\n", "\n", "", "parameters", "=", "[", "[", "n", ",", "p", "]", "for", "n", ",", "p", "in", "model", ".", "named_parameters", "(", ")", "if", "p", ".", "requires_grad", "]", "\n", "optimizer_", "=", "optimizer", ".", "construct", "(", "model_parameters", "=", "parameters", ")", "\n", "\n", "if", "not", "callbacks", ":", "\n", "            ", "callbacks", "=", "[", "]", "\n", "", "else", ":", "\n", "            ", "constructed_callbacks", "=", "[", "]", "\n", "for", "callback", "in", "callbacks", ":", "\n", "# We only need to pass here the things that weren't already passed to", "\n", "# CallbackTrainer.from_partial_objects; FromParams will automatically pass those", "\n", "# things through to the callback constructor.", "\n", "                ", "callback_", "=", "callback", ".", "construct", "(", "\n", "optimizer", "=", "optimizer", ",", "instances", "=", "train_data", "\n", ")", "\n", "constructed_callbacks", ".", "append", "(", "callback_", ")", "\n", "\n", "", "", "if", "distributed", ":", "\n", "            ", "rank", "=", "cuda_device", "\n", "", "else", ":", "\n", "            ", "rank", "=", "0", "\n", "\n", "", "return", "cls", "(", "\n", "model", ",", "\n", "train_data", ",", "\n", "iterator", ",", "\n", "optimizer_", ",", "\n", "num_epochs", "=", "num_epochs", ",", "\n", "shuffle", "=", "shuffle", ",", "\n", "serialization_dir", "=", "serialization_dir", ",", "\n", "cuda_device", "=", "cuda_device", ",", "\n", "callbacks", "=", "constructed_callbacks", ",", "\n", "distributed", "=", "distributed", ",", "\n", "rank", "=", "rank", ",", "\n", "world_size", "=", "world_size", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.training.callback_trainer.handle_errors": [[30, 41], ["functools.wraps", "method", "callback_trainer..handler.fire_event"], "function", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.callbacks.callback_handler.CallbackHandler.fire_event"], ["def", "handle_errors", "(", "method", ")", ":", "\n", "    ", "@", "functools", ".", "wraps", "(", "method", ")", "\n", "def", "train_and_handle_errors", "(", "self", ":", "\"CallbackTrainer\"", ")", "->", "Dict", "[", "str", ",", "Any", "]", ":", "\n", "        ", "try", ":", "\n", "            ", "return", "method", "(", "self", ")", "\n", "", "except", "Exception", "as", "exc", ":", "\n", "            ", "self", ".", "exception", "=", "exc", "\n", "self", ".", "handler", ".", "fire_event", "(", "Events", ".", "ERROR", ")", "\n", "raise", "\n", "\n", "", "", "return", "train_and_handle_errors", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.training.trainer.Trainer.__init__": [[37, 294], ["allennlp.training.trainer_base.TrainerBase.__init__", "allennlp.training.metric_tracker.MetricTracker", "allennlp.training.tensorboard_writer.TensorboardWriter", "allennlp.training.checkpointer.Checkpointer", "trainer.Trainer._tensorboard.enable_activation_logging", "torch.nn.parallel.DistributedDataParallel", "torch.nn.parallel.DistributedDataParallel", "torch.nn.parallel.DistributedDataParallel", "logger.warning", "allennlp.common.checks.ConfigurationError", "allennlp.common.checks.ConfigurationError", "isinstance"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.graph_util.plot.GraphPlot.__init__", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.training.tensorboard_writer.TensorboardWriter.enable_activation_logging"], ["    ", "def", "__init__", "(", "\n", "self", ",", "\n", "model", ":", "Model", ",", "\n", "optimizer", ":", "torch", ".", "optim", ".", "Optimizer", ",", "\n", "iterator", ":", "DataIterator", ",", "\n", "train_dataset", ":", "Iterable", "[", "Instance", "]", ",", "\n", "validation_dataset", ":", "Optional", "[", "Iterable", "[", "Instance", "]", "]", "=", "None", ",", "\n", "patience", ":", "Optional", "[", "int", "]", "=", "None", ",", "\n", "validation_metric", ":", "str", "=", "\"-loss\"", ",", "\n", "validation_iterator", ":", "DataIterator", "=", "None", ",", "\n", "shuffle", ":", "bool", "=", "True", ",", "\n", "num_epochs", ":", "int", "=", "20", ",", "\n", "serialization_dir", ":", "Optional", "[", "str", "]", "=", "None", ",", "\n", "num_serialized_models_to_keep", ":", "int", "=", "20", ",", "\n", "keep_serialized_model_every_num_seconds", ":", "int", "=", "None", ",", "\n", "checkpointer", ":", "Checkpointer", "=", "None", ",", "\n", "model_save_interval", ":", "float", "=", "None", ",", "\n", "cuda_device", ":", "int", "=", "-", "1", ",", "\n", "grad_norm", ":", "Optional", "[", "float", "]", "=", "None", ",", "\n", "grad_clipping", ":", "Optional", "[", "float", "]", "=", "None", ",", "\n", "learning_rate_scheduler", ":", "Optional", "[", "LearningRateScheduler", "]", "=", "None", ",", "\n", "momentum_scheduler", ":", "Optional", "[", "MomentumScheduler", "]", "=", "None", ",", "\n", "summary_interval", ":", "int", "=", "100", ",", "\n", "histogram_interval", ":", "int", "=", "None", ",", "\n", "should_log_parameter_statistics", ":", "bool", "=", "True", ",", "\n", "should_log_learning_rate", ":", "bool", "=", "False", ",", "\n", "log_batch_size_period", ":", "Optional", "[", "int", "]", "=", "None", ",", "\n", "moving_average", ":", "Optional", "[", "MovingAverage", "]", "=", "None", ",", "\n", "distributed", ":", "bool", "=", "False", ",", "\n", "local_rank", ":", "int", "=", "0", ",", "\n", "world_size", ":", "int", "=", "1", ",", "\n", "num_gradient_accumulation_steps", ":", "int", "=", "1", ",", "\n", ")", "->", "None", ":", "\n", "        ", "\"\"\"\n        A trainer for doing supervised learning. It just takes a labeled dataset\n        and a `DataIterator`, and uses the supplied `Optimizer` to learn the weights\n        for your model over some fixed number of epochs. You can also pass in a validation\n        dataset and enable early stopping. There are many other bells and whistles as well.\n\n        # Parameters\n\n        model : `Model`, required.\n            An AllenNLP model to be optimized. Pytorch Modules can also be optimized if\n            their `forward` method returns a dictionary with a \"loss\" key, containing a\n            scalar tensor representing the loss function to be optimized.\n\n            If you are training your model using GPUs, your model should already be\n            on the correct device. (If you use `Trainer.from_params` this will be\n            handled for you.)\n        optimizer : `torch.nn.Optimizer`, required.\n            An instance of a Pytorch Optimizer, instantiated with the parameters of the\n            model to be optimized.\n        iterator : `DataIterator`, required.\n            A method for iterating over a `Dataset`, yielding padded indexed batches.\n        train_dataset : `Dataset`, required.\n            A `Dataset` to train on. The dataset should have already been indexed.\n        validation_dataset : `Dataset`, optional, (default = None).\n            A `Dataset` to evaluate on. The dataset should have already been indexed.\n        patience : Optional[int] > 0, optional (default=None)\n            Number of epochs to be patient before early stopping: the training is stopped\n            after `patience` epochs with no improvement. If given, it must be `> 0`.\n            If None, early stopping is disabled.\n        validation_metric : str, optional (default=\"loss\")\n            Validation metric to measure for whether to stop training using patience\n            and whether to serialize an `is_best` model each epoch. The metric name\n            must be prepended with either \"+\" or \"-\", which specifies whether the metric\n            is an increasing or decreasing function.\n        validation_iterator : `DataIterator`, optional (default=None)\n            An iterator to use for the validation set.  If `None`, then\n            use the training `iterator`.\n        shuffle : `bool`, optional (default=True)\n            Whether to shuffle the instances in the iterator or not.\n        num_epochs : int, optional (default = 20)\n            Number of training epochs.\n        serialization_dir : str, optional (default=None)\n            Path to directory for saving and loading model files. Models will not be saved if\n            this parameter is not passed.\n        num_serialized_models_to_keep : `int`, optional (default=20)\n            Number of previous model checkpoints to retain.  Default is to keep 20 checkpoints.\n            A value of None or -1 means all checkpoints will be kept.\n        keep_serialized_model_every_num_seconds : `int`, optional (default=None)\n            If num_serialized_models_to_keep is not None, then occasionally it's useful to\n            save models at a given interval in addition to the last num_serialized_models_to_keep.\n            To do so, specify keep_serialized_model_every_num_seconds as the number of seconds\n            between permanently saved checkpoints.  Note that this option is only used if\n            num_serialized_models_to_keep is not None, otherwise all checkpoints are kept.\n        checkpointer : `Checkpointer`, optional (default=None)\n            An instance of class Checkpointer to use instead of the default. If a checkpointer is specified,\n            the arguments num_serialized_models_to_keep and keep_serialized_model_every_num_seconds should\n            not be specified. The caller is responsible for initializing the checkpointer so that it is\n            consistent with serialization_dir.\n        model_save_interval : `float`, optional (default=None)\n            If provided, then serialize models every `model_save_interval`\n            seconds within single epochs.  In all cases, models are also saved\n            at the end of every epoch if `serialization_dir` is provided.\n        cuda_device : `int`, optional (default = -1)\n            An integer specifying the CUDA device(s) to use for this process. If -1, the CPU is used.\n            Data parallelism is controlled at the allennlp train level, so each trainer will have a single\n            GPU.\n        grad_norm : `float`, optional, (default = None).\n            If provided, gradient norms will be rescaled to have a maximum of this value.\n        grad_clipping : `float`, optional (default = `None`).\n            If provided, gradients will be clipped `during the backward pass` to have an (absolute)\n            maximum of this value.  If you are getting `NaNs` in your gradients during training\n            that are not solved by using `grad_norm`, you may need this.\n        learning_rate_scheduler : `LearningRateScheduler`, optional (default = None)\n            If specified, the learning rate will be decayed with respect to\n            this schedule at the end of each epoch (or batch, if the scheduler implements\n            the `step_batch` method). If you use `torch.optim.lr_scheduler.ReduceLROnPlateau`,\n            this will use the `validation_metric` provided to determine if learning has plateaued.\n            To support updating the learning rate on every batch, this can optionally implement\n            `step_batch(batch_num_total)` which updates the learning rate given the batch number.\n        momentum_scheduler : `MomentumScheduler`, optional (default = None)\n            If specified, the momentum will be updated at the end of each batch or epoch\n            according to the schedule.\n        summary_interval : `int`, optional, (default = 100)\n            Number of batches between logging scalars to tensorboard\n        histogram_interval : `int`, optional, (default = `None`)\n            If not None, then log histograms to tensorboard every `histogram_interval` batches.\n            When this parameter is specified, the following additional logging is enabled:\n                * Histograms of model parameters\n                * The ratio of parameter update norm to parameter norm\n                * Histogram of layer activations\n            We log histograms of the parameters returned by\n            `model.get_parameters_for_histogram_tensorboard_logging`.\n            The layer activations are logged for any modules in the `Model` that have\n            the attribute `should_log_activations` set to `True`.  Logging\n            histograms requires a number of GPU-CPU copies during training and is typically\n            slow, so we recommend logging histograms relatively infrequently.\n            Note: only Modules that return tensors, tuples of tensors or dicts\n            with tensors as values currently support activation logging.\n        should_log_parameter_statistics : `bool`, optional, (default = True)\n            Whether to send parameter statistics (mean and standard deviation\n            of parameters and gradients) to tensorboard.\n        should_log_learning_rate : `bool`, optional, (default = False)\n            Whether to send parameter specific learning rate to tensorboard.\n        log_batch_size_period : `int`, optional, (default = `None`)\n            If defined, how often to log the average batch size.\n        moving_average : `MovingAverage`, optional, (default = None)\n            If provided, we will maintain moving averages for all parameters. During training, we\n            employ a shadow variable for each parameter, which maintains the moving average. During\n            evaluation, we backup the original parameters and assign the moving averages to corresponding\n            parameters. Be careful that when saving the checkpoint, we will save the moving averages of\n            parameters. This is necessary because we want the saved model to perform as well as the validated\n            model if we load it later. But this may cause problems if you restart the training from checkpoint.\n        distributed : `bool`, optional, (default = False)\n            If set, PyTorch's `DistributedDataParallel` is used to train the model in multiple GPUs. This also\n            requires `world_size` to be greater than 1.\n        local_rank : `int`, optional, (default = 0)\n            This is the unique identifier of the `Trainer` in a distributed process group. The GPU device id is\n            used as the rank.\n        world_size : `int`, (default = 1)\n            The number of `Trainer` workers participating in the distributed training.\n        num_gradient_accumulation_steps : `int`, optional, (default = 1)\n            Gradients are accumulated for the given number of steps before doing an optimizer step. This can\n            be useful to accommodate batches that are larger than the RAM size. Refer Thomas Wolf's\n            [post](https://tinyurl.com/y5mv44fw) for details on Gradient Accumulation.\n        \"\"\"", "\n", "super", "(", ")", ".", "__init__", "(", "\n", "serialization_dir", ",", "cuda_device", ",", "distributed", ",", "local_rank", ",", "world_size", "\n", ")", "\n", "\n", "# I am not calling move_to_gpu here, because if the model is", "\n", "# not already on the GPU then the optimizer is going to be wrong.", "\n", "self", ".", "model", "=", "model", "\n", "\n", "self", ".", "iterator", "=", "iterator", "\n", "self", ".", "_validation_iterator", "=", "validation_iterator", "\n", "self", ".", "shuffle", "=", "shuffle", "\n", "self", ".", "optimizer", "=", "optimizer", "\n", "self", ".", "train_data", "=", "train_dataset", "\n", "self", ".", "_validation_data", "=", "validation_dataset", "\n", "\n", "if", "patience", "is", "None", ":", "# no early stopping", "\n", "            ", "if", "validation_dataset", ":", "\n", "                ", "logger", ".", "warning", "(", "\n", "\"You provided a validation dataset but patience was set to None, \"", "\n", "\"meaning that early stopping is disabled\"", "\n", ")", "\n", "", "", "elif", "(", "not", "isinstance", "(", "patience", ",", "int", ")", ")", "or", "patience", "<=", "0", ":", "\n", "            ", "raise", "ConfigurationError", "(", "\n", "'{} is an invalid value for \"patience\": it must be a positive integer '", "\n", "\"or None (if you want to disable early stopping)\"", ".", "format", "(", "patience", ")", "\n", ")", "\n", "\n", "# For tracking is_best_so_far and should_stop_early", "\n", "", "self", ".", "_metric_tracker", "=", "MetricTracker", "(", "patience", ",", "validation_metric", ")", "\n", "# Get rid of + or -", "\n", "self", ".", "_validation_metric", "=", "validation_metric", "[", "1", ":", "]", "\n", "\n", "self", ".", "_num_epochs", "=", "num_epochs", "\n", "\n", "if", "checkpointer", "is", "not", "None", ":", "\n", "# We can't easily check if these parameters were passed in, so check against their default values.", "\n", "# We don't check against serialization_dir since it is also used by the parent class.", "\n", "            ", "if", "(", "\n", "num_serialized_models_to_keep", "!=", "20", "\n", "or", "keep_serialized_model_every_num_seconds", "is", "not", "None", "\n", ")", ":", "\n", "                ", "raise", "ConfigurationError", "(", "\n", "\"When passing a custom Checkpointer, you may not also pass in separate checkpointer \"", "\n", "\"args 'num_serialized_models_to_keep' or 'keep_serialized_model_every_num_seconds'.\"", "\n", ")", "\n", "", "self", ".", "_checkpointer", "=", "checkpointer", "\n", "", "else", ":", "\n", "            ", "self", ".", "_checkpointer", "=", "Checkpointer", "(", "\n", "serialization_dir", ",", "\n", "keep_serialized_model_every_num_seconds", ",", "\n", "num_serialized_models_to_keep", ",", "\n", ")", "\n", "\n", "", "self", ".", "_model_save_interval", "=", "model_save_interval", "\n", "\n", "self", ".", "_grad_norm", "=", "grad_norm", "\n", "self", ".", "_grad_clipping", "=", "grad_clipping", "\n", "\n", "self", ".", "_learning_rate_scheduler", "=", "learning_rate_scheduler", "\n", "self", ".", "_momentum_scheduler", "=", "momentum_scheduler", "\n", "self", ".", "_moving_average", "=", "moving_average", "\n", "\n", "# We keep the total batch number as an instance variable because it", "\n", "# is used inside a closure for the hook which logs activations in", "\n", "# `_enable_activation_logging`.", "\n", "self", ".", "_batch_num_total", "=", "0", "\n", "\n", "self", ".", "_tensorboard", "=", "TensorboardWriter", "(", "\n", "get_batch_num_total", "=", "lambda", ":", "self", ".", "_batch_num_total", ",", "\n", "serialization_dir", "=", "serialization_dir", ",", "\n", "summary_interval", "=", "summary_interval", ",", "\n", "histogram_interval", "=", "histogram_interval", ",", "\n", "should_log_parameter_statistics", "=", "should_log_parameter_statistics", ",", "\n", "should_log_learning_rate", "=", "should_log_learning_rate", ",", "\n", ")", "\n", "\n", "self", ".", "_log_batch_size_period", "=", "log_batch_size_period", "\n", "\n", "self", ".", "_last_log", "=", "0.0", "# time of last logging", "\n", "\n", "self", ".", "_num_gradient_accumulation_steps", "=", "num_gradient_accumulation_steps", "\n", "\n", "# Enable activation logging.", "\n", "if", "histogram_interval", "is", "not", "None", ":", "\n", "            ", "self", ".", "_tensorboard", ".", "enable_activation_logging", "(", "self", ".", "model", ")", "\n", "\n", "# Using `DistributedDataParallel`(ddp) brings in a quirk wrt AllenNLP's `Model` interface and its", "\n", "# usage. A `Model` object is wrapped by `ddp`, but assigning the wrapped model to `self.model`", "\n", "# will break the usages such as `Model.get_regularization_penalty`, `Model.get_metrics`, etc.", "\n", "#", "\n", "# Hence a reference to Pytorch's object is maintained in the case of distributed training and in the", "\n", "# normal case, reference to `Model` is retained. This reference is only used in", "\n", "# these places: `model.__call__`, `model.train` and `model.eval`.", "\n", "", "if", "self", ".", "_distributed", ":", "\n", "            ", "self", ".", "_pytorch_model", "=", "DistributedDataParallel", "(", "\n", "self", ".", "model", ",", "device_ids", "=", "[", "self", ".", "cuda_device", "]", ",", "find_unused_parameters", "=", "True", "\n", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "_pytorch_model", "=", "self", ".", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.training.trainer.Trainer.rescale_gradients": [[295, 297], ["allennlp.training.util.rescale_gradients"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.callbacks.gradient_norm_and_clip.GradientNormAndClip.rescale_gradients"], ["", "", "def", "rescale_gradients", "(", "self", ")", "->", "Optional", "[", "float", "]", ":", "\n", "        ", "return", "training_util", ".", "rescale_gradients", "(", "self", ".", "model", ",", "self", ".", "_grad_norm", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.training.trainer.Trainer.batch_loss": [[298, 319], ["allennlp.nn.util.move_to_device", "trainer.Trainer._pytorch_model", "trainer.Trainer.model.get_regularization_penalty", "RuntimeError"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.nn.util.move_to_device", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.models.model.Model.get_regularization_penalty"], ["", "def", "batch_loss", "(", "self", ",", "batch", ":", "TensorDict", ",", "for_training", ":", "bool", ")", "->", "torch", ".", "Tensor", ":", "\n", "        ", "\"\"\"\n        Does a forward pass on the given batches and returns the `loss` value in the result.\n        If `for_training` is `True` also applies regularization penalty.\n        \"\"\"", "\n", "batch", "=", "nn_util", ".", "move_to_device", "(", "batch", ",", "self", ".", "cuda_device", ")", "\n", "output_dict", "=", "self", ".", "_pytorch_model", "(", "**", "batch", ")", "\n", "\n", "try", ":", "\n", "            ", "loss", "=", "output_dict", "[", "\"loss\"", "]", "\n", "if", "for_training", ":", "\n", "                ", "loss", "+=", "self", ".", "model", ".", "get_regularization_penalty", "(", ")", "\n", "", "", "except", "KeyError", ":", "\n", "            ", "if", "for_training", ":", "\n", "                ", "raise", "RuntimeError", "(", "\n", "\"The model you are trying to optimize does not contain a\"", "\n", "\" 'loss' key in the output of model.forward(inputs).\"", "\n", ")", "\n", "", "loss", "=", "None", "\n", "\n", "", "return", "loss", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.training.trainer.Trainer._train_epoch": [[320, 491], ["logger.info", "allennlp.common.util.peak_memory_mb", "logger.info", "allennlp.common.util.gpu_memory_mb().items", "trainer.Trainer._pytorch_model.train", "trainer.Trainer.iterator", "allennlp.common.util.lazy_groups_of", "math.ceil", "time.time", "time.time", "set", "logger.info", "allennlp.training.util.get_metrics", "gpu_usage.append", "logger.info", "allennlp.common.Tqdm.tqdm", "trainer.Trainer.model.get_parameters_for_histogram_tensorboard_logging", "trainer.Trainer.optimizer.zero_grad", "trainer.Trainer.rescale_gradients", "allennlp.training.util.get_metrics", "torch.barrier", "torch.barrier", "torch.barrier", "allennlp.common.util.gpu_memory_mb", "trainer.Trainer.iterator.get_num_batches", "trainer.Trainer.batch_loss", "torch.isnan", "torch.isnan", "torch.isnan", "torch.isnan", "torch.isnan", "torch.isnan", "torch.isnan", "torch.isnan", "torch.isnan", "trainer.Trainer.backward", "trainer.Trainer.item", "trainer.Trainer._learning_rate_scheduler.step_batch", "trainer.Trainer._momentum_scheduler.step_batch", "trainer.Trainer._tensorboard.should_log_histograms_this_batch", "trainer.Trainer.optimizer.step", "trainer.Trainer.model.named_parameters", "trainer.Trainer.optimizer.step", "trainer.Trainer._moving_average.apply", "allennlp.training.util.description_from_metrics", "allennlp.common.Tqdm.tqdm.set_description", "trainer.Trainer._tensorboard.should_log_this_batch", "trainer.Trainer._tensorboard.log_parameter_and_gradient_statistics", "trainer.Trainer._tensorboard.log_learning_rates", "trainer.Trainer._tensorboard.add_train_scalar", "trainer.Trainer._tensorboard.log_metrics", "trainer.Trainer._tensorboard.should_log_histograms_this_batch", "trainer.Trainer._tensorboard.log_histograms", "sum", "time.time", "trainer.Trainer._save_checkpoint", "ValueError", "len", "param.detach().cpu().clone", "param_updates[].sub_", "torch.norm", "torch.norm", "torch.norm", "torch.norm", "torch.norm", "torch.norm", "torch.norm", "torch.norm", "torch.norm", "torch.norm().cpu", "torch.norm().cpu", "torch.norm().cpu", "torch.norm().cpu", "torch.norm().cpu", "torch.norm().cpu", "torch.norm().cpu", "torch.norm().cpu", "torch.norm().cpu", "trainer.Trainer._tensorboard.add_train_scalar", "logger.info", "trainer.Trainer._tensorboard.add_train_scalar", "trainer.Trainer._tensorboard.add_train_scalar", "trainer.Trainer.model.named_parameters", "param.detach().cpu", "param_updates[].view", "allennlp.training.util.get_batch_size", "time.time", "allennlp.training.util.time_to_str", "param.detach().cpu", "torch.norm", "torch.norm", "torch.norm", "torch.norm", "torch.norm", "torch.norm", "torch.norm", "torch.norm", "torch.norm", "allennlp.training.util.get_metrics.items", "int", "str", "param.detach", "param.view", "param.detach"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.dataset_readers.multiprocess_dataset_reader.logger.info", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.util.peak_memory_mb", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.dataset_readers.multiprocess_dataset_reader.logger.info", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.training.no_op_trainer.NoOpTrainer.train", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.util.lazy_groups_of", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.dataset_readers.multiprocess_dataset_reader.logger.info", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.coreference_resolution.coref.CoreferenceResolver.get_metrics", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.dataset_readers.multiprocess_dataset_reader.logger.info", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.tqdm.Tqdm.tqdm", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.models.model.Model.get_parameters_for_histogram_tensorboard_logging", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.callbacks.gradient_norm_and_clip.GradientNormAndClip.rescale_gradients", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.coreference_resolution.coref.CoreferenceResolver.get_metrics", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.util.gpu_memory_mb", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.iterators.data_iterator.DataIterator.get_num_batches", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.training.trainer.Trainer.batch_loss", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.learning_rate_schedulers.slanted_triangular.SlantedTriangular.step_batch", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.learning_rate_schedulers.slanted_triangular.SlantedTriangular.step_batch", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.training.tensorboard_writer.TensorboardWriter.should_log_histograms_this_batch", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.learning_rate_schedulers.slanted_triangular.SlantedTriangular.step", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.learning_rate_schedulers.slanted_triangular.SlantedTriangular.step", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.training.moving_average.ExponentialMovingAverage.apply", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.training.util.description_from_metrics", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.training.tensorboard_writer.TensorboardWriter.should_log_this_batch", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.training.tensorboard_writer.TensorboardWriter.log_parameter_and_gradient_statistics", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.training.tensorboard_writer.TensorboardWriter.log_learning_rates", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.training.tensorboard_writer.TensorboardWriter.add_train_scalar", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.training.tensorboard_writer.TensorboardWriter.log_metrics", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.training.tensorboard_writer.TensorboardWriter.should_log_histograms_this_batch", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.training.tensorboard_writer.TensorboardWriter.log_histograms", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.callbacks.checkpoint.Checkpoint._save_checkpoint", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.nn.util.clone", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.training.tensorboard_writer.TensorboardWriter.add_train_scalar", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.dataset_readers.multiprocess_dataset_reader.logger.info", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.training.tensorboard_writer.TensorboardWriter.add_train_scalar", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.training.tensorboard_writer.TensorboardWriter.add_train_scalar", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.training.util.get_batch_size", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.training.util.time_to_str"], ["", "def", "_train_epoch", "(", "self", ",", "epoch", ":", "int", ")", "->", "Dict", "[", "str", ",", "float", "]", ":", "\n", "        ", "\"\"\"\n        Trains one epoch and returns metrics.\n        \"\"\"", "\n", "logger", ".", "info", "(", "\"Epoch %d/%d\"", ",", "epoch", ",", "self", ".", "_num_epochs", "-", "1", ")", "\n", "peak_cpu_usage", "=", "common_util", ".", "peak_memory_mb", "(", ")", "\n", "logger", ".", "info", "(", "f\"Peak CPU memory usage MB: {peak_cpu_usage}\"", ")", "\n", "gpu_usage", "=", "[", "]", "\n", "for", "gpu", ",", "memory", "in", "common_util", ".", "gpu_memory_mb", "(", ")", ".", "items", "(", ")", ":", "\n", "            ", "gpu_usage", ".", "append", "(", "(", "gpu", ",", "memory", ")", ")", "\n", "logger", ".", "info", "(", "f\"GPU {gpu} memory usage MB: {memory}\"", ")", "\n", "\n", "", "train_loss", "=", "0.0", "\n", "# Set the model to \"train\" mode.", "\n", "self", ".", "_pytorch_model", ".", "train", "(", ")", "\n", "\n", "# Get tqdm for the training batches", "\n", "batch_generator", "=", "self", ".", "iterator", "(", "\n", "self", ".", "train_data", ",", "num_epochs", "=", "1", ",", "shuffle", "=", "self", ".", "shuffle", "\n", ")", "\n", "batch_group_generator", "=", "common_util", ".", "lazy_groups_of", "(", "\n", "batch_generator", ",", "self", ".", "_num_gradient_accumulation_steps", "\n", ")", "\n", "num_training_batches", "=", "math", ".", "ceil", "(", "\n", "self", ".", "iterator", ".", "get_num_batches", "(", "self", ".", "train_data", ")", "\n", "/", "self", ".", "_num_gradient_accumulation_steps", "\n", ")", "\n", "# Having multiple tqdm bars in case of distributed training will be a mess. Hence only the master's", "\n", "# progress is shown", "\n", "if", "self", ".", "_master", ":", "\n", "            ", "batch_group_generator_tqdm", "=", "Tqdm", ".", "tqdm", "(", "\n", "batch_group_generator", ",", "total", "=", "num_training_batches", "\n", ")", "\n", "", "else", ":", "\n", "            ", "batch_group_generator_tqdm", "=", "batch_group_generator", "\n", "\n", "", "self", ".", "_last_log", "=", "time", ".", "time", "(", ")", "\n", "last_save_time", "=", "time", ".", "time", "(", ")", "\n", "\n", "batches_this_epoch", "=", "0", "\n", "if", "self", ".", "_batch_num_total", "is", "None", ":", "\n", "            ", "self", ".", "_batch_num_total", "=", "0", "\n", "\n", "", "histogram_parameters", "=", "set", "(", "\n", "self", ".", "model", ".", "get_parameters_for_histogram_tensorboard_logging", "(", ")", "\n", ")", "\n", "\n", "logger", ".", "info", "(", "\"Training\"", ")", "\n", "\n", "cumulative_batch_group_size", "=", "0", "\n", "for", "batch_group", "in", "batch_group_generator_tqdm", ":", "\n", "            ", "batches_this_epoch", "+=", "1", "\n", "self", ".", "_batch_num_total", "+=", "1", "\n", "batch_num_total", "=", "self", ".", "_batch_num_total", "\n", "\n", "self", ".", "optimizer", ".", "zero_grad", "(", ")", "\n", "\n", "for", "batch", "in", "batch_group", ":", "\n", "                ", "loss", "=", "self", ".", "batch_loss", "(", "batch", ",", "for_training", "=", "True", ")", "\n", "if", "torch", ".", "isnan", "(", "loss", ")", ":", "\n", "                    ", "raise", "ValueError", "(", "\"nan loss encountered\"", ")", "\n", "", "loss", "=", "loss", "/", "len", "(", "batch_group", ")", "\n", "loss", ".", "backward", "(", ")", "\n", "train_loss", "+=", "loss", ".", "item", "(", ")", "\n", "\n", "", "batch_grad_norm", "=", "self", ".", "rescale_gradients", "(", ")", "\n", "\n", "# This does nothing if batch_num_total is None or you are using a", "\n", "# scheduler which doesn't update per batch.", "\n", "if", "self", ".", "_learning_rate_scheduler", ":", "\n", "                ", "self", ".", "_learning_rate_scheduler", ".", "step_batch", "(", "batch_num_total", ")", "\n", "", "if", "self", ".", "_momentum_scheduler", ":", "\n", "                ", "self", ".", "_momentum_scheduler", ".", "step_batch", "(", "batch_num_total", ")", "\n", "\n", "", "if", "self", ".", "_tensorboard", ".", "should_log_histograms_this_batch", "(", ")", "and", "self", ".", "_master", ":", "\n", "# get the magnitude of parameter updates for logging", "\n", "# We need a copy of current parameters to compute magnitude of updates,", "\n", "# and copy them to CPU so large models won't go OOM on the GPU.", "\n", "                ", "param_updates", "=", "{", "\n", "name", ":", "param", ".", "detach", "(", ")", ".", "cpu", "(", ")", ".", "clone", "(", ")", "\n", "for", "name", ",", "param", "in", "self", ".", "model", ".", "named_parameters", "(", ")", "\n", "}", "\n", "self", ".", "optimizer", ".", "step", "(", ")", "\n", "for", "name", ",", "param", "in", "self", ".", "model", ".", "named_parameters", "(", ")", ":", "\n", "                    ", "param_updates", "[", "name", "]", ".", "sub_", "(", "param", ".", "detach", "(", ")", ".", "cpu", "(", ")", ")", "\n", "update_norm", "=", "torch", ".", "norm", "(", "param_updates", "[", "name", "]", ".", "view", "(", "-", "1", ")", ")", "\n", "param_norm", "=", "torch", ".", "norm", "(", "param", ".", "view", "(", "-", "1", ")", ")", ".", "cpu", "(", ")", "\n", "self", ".", "_tensorboard", ".", "add_train_scalar", "(", "\n", "\"gradient_update/\"", "+", "name", ",", "update_norm", "/", "(", "param_norm", "+", "1e-7", ")", "\n", ")", "\n", "", "", "else", ":", "\n", "                ", "self", ".", "optimizer", ".", "step", "(", ")", "\n", "\n", "# Update moving averages", "\n", "", "if", "self", ".", "_moving_average", "is", "not", "None", ":", "\n", "                ", "self", ".", "_moving_average", ".", "apply", "(", "batch_num_total", ")", "\n", "\n", "# Update the description with the latest metrics", "\n", "", "metrics", "=", "training_util", ".", "get_metrics", "(", "\n", "self", ".", "model", ",", "\n", "train_loss", ",", "\n", "batches_this_epoch", ",", "\n", "world_size", "=", "self", ".", "_world_size", ",", "\n", "cuda_device", "=", "[", "self", ".", "cuda_device", "]", ",", "\n", ")", "\n", "\n", "# Updating tqdm only for the master as the trainers wouldn't have one", "\n", "if", "self", ".", "_master", ":", "\n", "                ", "description", "=", "training_util", ".", "description_from_metrics", "(", "metrics", ")", "\n", "batch_group_generator_tqdm", ".", "set_description", "(", "description", ",", "refresh", "=", "False", ")", "\n", "\n", "# Log parameter values to Tensorboard (only from the master)", "\n", "", "if", "self", ".", "_tensorboard", ".", "should_log_this_batch", "(", ")", "and", "self", ".", "_master", ":", "\n", "                ", "self", ".", "_tensorboard", ".", "log_parameter_and_gradient_statistics", "(", "\n", "self", ".", "model", ",", "batch_grad_norm", "\n", ")", "\n", "self", ".", "_tensorboard", ".", "log_learning_rates", "(", "self", ".", "model", ",", "self", ".", "optimizer", ")", "\n", "\n", "self", ".", "_tensorboard", ".", "add_train_scalar", "(", "\"loss/loss_train\"", ",", "metrics", "[", "\"loss\"", "]", ")", "\n", "self", ".", "_tensorboard", ".", "log_metrics", "(", "\n", "{", "\"epoch_metrics/\"", "+", "k", ":", "v", "for", "k", ",", "v", "in", "metrics", ".", "items", "(", ")", "}", "\n", ")", "\n", "\n", "", "if", "self", ".", "_tensorboard", ".", "should_log_histograms_this_batch", "(", ")", "and", "self", ".", "_master", ":", "\n", "                ", "self", ".", "_tensorboard", ".", "log_histograms", "(", "self", ".", "model", ",", "histogram_parameters", ")", "\n", "\n", "", "if", "self", ".", "_log_batch_size_period", ":", "\n", "                ", "batch_group_size", "=", "sum", "(", "\n", "training_util", ".", "get_batch_size", "(", "batch", ")", "for", "batch", "in", "batch_group", "\n", ")", "\n", "cumulative_batch_group_size", "+=", "batch_group_size", "\n", "if", "(", "batches_this_epoch", "-", "1", ")", "%", "self", ".", "_log_batch_size_period", "==", "0", ":", "\n", "                    ", "average", "=", "cumulative_batch_group_size", "/", "batches_this_epoch", "\n", "logger", ".", "info", "(", "\n", "f\"current batch size: {batch_group_size} mean batch size: {average}\"", "\n", ")", "\n", "self", ".", "_tensorboard", ".", "add_train_scalar", "(", "\n", "\"current_batch_size\"", ",", "batch_group_size", "\n", ")", "\n", "self", ".", "_tensorboard", ".", "add_train_scalar", "(", "\"mean_batch_size\"", ",", "average", ")", "\n", "\n", "# Save model if needed.", "\n", "", "", "if", "(", "\n", "self", ".", "_model_save_interval", "is", "not", "None", "\n", "and", "(", "time", ".", "time", "(", ")", "-", "last_save_time", ">", "self", ".", "_model_save_interval", ")", "\n", "and", "self", ".", "_master", "\n", ")", ":", "\n", "                ", "last_save_time", "=", "time", ".", "time", "(", ")", "\n", "self", ".", "_save_checkpoint", "(", "\n", "\"{0}.{1}\"", ".", "format", "(", "\n", "epoch", ",", "training_util", ".", "time_to_str", "(", "int", "(", "last_save_time", ")", ")", "\n", ")", "\n", ")", "\n", "\n", "# Let all workers finish their epoch before computing", "\n", "# the final statistics for the epoch.", "\n", "", "", "if", "self", ".", "_distributed", ":", "\n", "            ", "dist", ".", "barrier", "(", ")", "\n", "\n", "", "metrics", "=", "training_util", ".", "get_metrics", "(", "\n", "self", ".", "model", ",", "\n", "train_loss", ",", "\n", "batches_this_epoch", ",", "\n", "reset", "=", "True", ",", "\n", "world_size", "=", "self", ".", "_world_size", ",", "\n", "cuda_device", "=", "[", "self", ".", "cuda_device", "]", ",", "\n", ")", "\n", "metrics", "[", "\"cpu_memory_MB\"", "]", "=", "peak_cpu_usage", "\n", "for", "(", "gpu_num", ",", "memory", ")", "in", "gpu_usage", ":", "\n", "            ", "metrics", "[", "\"gpu_\"", "+", "str", "(", "gpu_num", ")", "+", "\"_memory_MB\"", "]", "=", "memory", "\n", "", "return", "metrics", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.training.trainer.Trainer._validation_loss": [[492, 542], ["logger.info", "trainer.Trainer._pytorch_model.eval", "val_iterator", "val_iterator.get_num_batches", "allennlp.common.Tqdm.tqdm", "trainer.Trainer._moving_average.assign_average_value", "trainer.Trainer.batch_loss", "allennlp.training.util.get_metrics", "allennlp.training.util.description_from_metrics", "allennlp.common.Tqdm.tqdm.set_description", "trainer.Trainer._moving_average.restore", "trainer.Trainer.detach().cpu().numpy", "trainer.Trainer.detach().cpu", "trainer.Trainer.detach"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.dataset_readers.multiprocess_dataset_reader.logger.info", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.iterators.data_iterator.DataIterator.get_num_batches", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.tqdm.Tqdm.tqdm", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.training.moving_average.MovingAverage.assign_average_value", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.training.trainer.Trainer.batch_loss", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.coreference_resolution.coref.CoreferenceResolver.get_metrics", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.training.util.description_from_metrics", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.training.moving_average.MovingAverage.restore"], ["", "def", "_validation_loss", "(", "self", ")", "->", "Tuple", "[", "float", ",", "int", "]", ":", "\n", "        ", "\"\"\"\n        Computes the validation loss. Returns it and the number of batches.\n        \"\"\"", "\n", "logger", ".", "info", "(", "\"Validating\"", ")", "\n", "\n", "self", ".", "_pytorch_model", ".", "eval", "(", ")", "\n", "\n", "# Replace parameter values with the shadow values from the moving averages.", "\n", "if", "self", ".", "_moving_average", "is", "not", "None", ":", "\n", "            ", "self", ".", "_moving_average", ".", "assign_average_value", "(", ")", "\n", "\n", "", "if", "self", ".", "_validation_iterator", "is", "not", "None", ":", "\n", "            ", "val_iterator", "=", "self", ".", "_validation_iterator", "\n", "", "else", ":", "\n", "            ", "val_iterator", "=", "self", ".", "iterator", "\n", "\n", "", "val_generator", "=", "val_iterator", "(", "self", ".", "_validation_data", ",", "num_epochs", "=", "1", ",", "shuffle", "=", "False", ")", "\n", "num_validation_batches", "=", "val_iterator", ".", "get_num_batches", "(", "self", ".", "_validation_data", ")", "\n", "val_generator_tqdm", "=", "Tqdm", ".", "tqdm", "(", "val_generator", ",", "total", "=", "num_validation_batches", ")", "\n", "batches_this_epoch", "=", "0", "\n", "val_loss", "=", "0", "\n", "for", "batch", "in", "val_generator_tqdm", ":", "\n", "\n", "            ", "loss", "=", "self", ".", "batch_loss", "(", "batch", ",", "for_training", "=", "False", ")", "\n", "if", "loss", "is", "not", "None", ":", "\n", "# You shouldn't necessarily have to compute a loss for validation, so we allow for", "\n", "# `loss` to be None.  We need to be careful, though - `batches_this_epoch` is", "\n", "# currently only used as the divisor for the loss function, so we can safely only", "\n", "# count those batches for which we actually have a loss.  If this variable ever", "\n", "# gets used for something else, we might need to change things around a bit.", "\n", "                ", "batches_this_epoch", "+=", "1", "\n", "val_loss", "+=", "loss", ".", "detach", "(", ")", ".", "cpu", "(", ")", ".", "numpy", "(", ")", "\n", "\n", "# Update the description with the latest metrics", "\n", "", "val_metrics", "=", "training_util", ".", "get_metrics", "(", "\n", "self", ".", "model", ",", "\n", "val_loss", ",", "\n", "batches_this_epoch", ",", "\n", "world_size", "=", "self", ".", "_world_size", ",", "\n", "cuda_device", "=", "[", "self", ".", "cuda_device", "]", ",", "\n", ")", "\n", "description", "=", "training_util", ".", "description_from_metrics", "(", "val_metrics", ")", "\n", "val_generator_tqdm", ".", "set_description", "(", "description", ",", "refresh", "=", "False", ")", "\n", "\n", "# Now restore the original parameter values.", "\n", "", "if", "self", ".", "_moving_average", "is", "not", "None", ":", "\n", "            ", "self", ".", "_moving_average", ".", "restore", "(", ")", "\n", "\n", "", "return", "val_loss", ",", "batches_this_epoch", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.training.trainer.Trainer.train": [[543, 693], ["allennlp.training.util.enable_gradient_clipping", "logger.info", "time.time", "trainer.Trainer._metric_tracker.best_epoch_metrics.items", "range", "trainer.Trainer._tensorboard.close", "trainer.Trainer._checkpointer.best_model_state", "trainer.Trainer._restore_checkpoint", "time.time", "trainer.Trainer._train_epoch", "trainer.Trainer.items", "str", "trainer.Trainer.items", "allennlp.training.util.get_metrics.items", "trainer.Trainer._metric_tracker.is_best_so_far", "logger.info", "trainer.Trainer.model.load_state_dict", "traceback.print_exc", "allennlp.common.checks.ConfigurationError", "max", "key.startswith", "trainer.Trainer._tensorboard.log_metrics", "time.time", "datetime.timedelta", "allennlp.training.util.get_metrics.items", "allennlp.common.util.dump_metrics", "trainer.Trainer._learning_rate_scheduler.step", "trainer.Trainer._momentum_scheduler.step", "trainer.Trainer._save_checkpoint", "torch.barrier", "torch.barrier", "torch.barrier", "time.time", "datetime.timedelta", "str", "logger.info", "metrics.get", "max", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "trainer.Trainer._validation_loss", "allennlp.training.util.get_metrics", "trainer.Trainer._metric_tracker.add_metric", "trainer.Trainer._metric_tracker.should_stop_early", "os.path.join", "time.time", "datetime.timedelta", "metrics.get", "torch.barrier", "torch.barrier", "torch.barrier", "logger.info", "float", "int"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.callbacks.gradient_norm_and_clip.GradientNormAndClip.enable_gradient_clipping", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.dataset_readers.multiprocess_dataset_reader.logger.info", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.token_embedders.embedding.EmbeddingsTextFile.close", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.training.checkpointer.Checkpointer.best_model_state", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.training.trainer.Trainer._restore_checkpoint", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.training.trainer.Trainer._train_epoch", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.training.metric_tracker.MetricTracker.is_best_so_far", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.dataset_readers.multiprocess_dataset_reader.logger.info", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.learning_rate_schedulers.learning_rate_scheduler._PyTorchLearningRateSchedulerWrapper.load_state_dict", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.training.tensorboard_writer.TensorboardWriter.log_metrics", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.util.dump_metrics", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.learning_rate_schedulers.slanted_triangular.SlantedTriangular.step", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.learning_rate_schedulers.slanted_triangular.SlantedTriangular.step", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.callbacks.checkpoint.Checkpoint._save_checkpoint", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.dataset_readers.multiprocess_dataset_reader.logger.info", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.params.Params.get", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.training.trainer.Trainer._validation_loss", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.coreference_resolution.coref.CoreferenceResolver.get_metrics", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.training.metric_tracker.MetricTracker.add_metric", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.training.metric_tracker.MetricTracker.should_stop_early", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.dataset_readers.multiprocess_dataset_reader.QIterable.join", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.params.Params.get", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.dataset_readers.multiprocess_dataset_reader.logger.info"], ["", "def", "train", "(", "self", ")", "->", "Dict", "[", "str", ",", "Any", "]", ":", "\n", "        ", "\"\"\"\n        Trains the supplied model with the supplied parameters.\n        \"\"\"", "\n", "try", ":", "\n", "            ", "epoch_counter", "=", "self", ".", "_restore_checkpoint", "(", ")", "\n", "", "except", "RuntimeError", ":", "\n", "            ", "traceback", ".", "print_exc", "(", ")", "\n", "raise", "ConfigurationError", "(", "\n", "\"Could not recover training from the checkpoint.  Did you mean to output to \"", "\n", "\"a different serialization directory or delete the existing serialization \"", "\n", "\"directory?\"", "\n", ")", "\n", "\n", "", "training_util", ".", "enable_gradient_clipping", "(", "self", ".", "model", ",", "self", ".", "_grad_clipping", ")", "\n", "\n", "logger", ".", "info", "(", "\"Beginning training.\"", ")", "\n", "\n", "train_metrics", ":", "Dict", "[", "str", ",", "float", "]", "=", "{", "}", "\n", "val_metrics", ":", "Dict", "[", "str", ",", "float", "]", "=", "{", "}", "\n", "this_epoch_val_metric", ":", "float", "=", "None", "\n", "metrics", ":", "Dict", "[", "str", ",", "Any", "]", "=", "{", "}", "\n", "epochs_trained", "=", "0", "\n", "training_start_time", "=", "time", ".", "time", "(", ")", "\n", "\n", "metrics", "[", "\"best_epoch\"", "]", "=", "self", ".", "_metric_tracker", ".", "best_epoch", "\n", "for", "key", ",", "value", "in", "self", ".", "_metric_tracker", ".", "best_epoch_metrics", ".", "items", "(", ")", ":", "\n", "            ", "metrics", "[", "\"best_validation_\"", "+", "key", "]", "=", "value", "\n", "\n", "", "for", "epoch", "in", "range", "(", "epoch_counter", ",", "self", ".", "_num_epochs", ")", ":", "\n", "            ", "epoch_start_time", "=", "time", ".", "time", "(", ")", "\n", "train_metrics", "=", "self", ".", "_train_epoch", "(", "epoch", ")", "\n", "\n", "# get peak of memory usage", "\n", "if", "\"cpu_memory_MB\"", "in", "train_metrics", ":", "\n", "                ", "metrics", "[", "\"peak_cpu_memory_MB\"", "]", "=", "max", "(", "\n", "metrics", ".", "get", "(", "\"peak_cpu_memory_MB\"", ",", "0", ")", ",", "train_metrics", "[", "\"cpu_memory_MB\"", "]", "\n", ")", "\n", "", "for", "key", ",", "value", "in", "train_metrics", ".", "items", "(", ")", ":", "\n", "                ", "if", "key", ".", "startswith", "(", "\"gpu_\"", ")", ":", "\n", "                    ", "metrics", "[", "\"peak_\"", "+", "key", "]", "=", "max", "(", "metrics", ".", "get", "(", "\"peak_\"", "+", "key", ",", "0", ")", ",", "value", ")", "\n", "\n", "", "", "if", "self", ".", "_validation_data", "is", "not", "None", ":", "\n", "                ", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "# We have a validation set, so compute all the metrics on it.", "\n", "                    ", "val_loss", ",", "num_batches", "=", "self", ".", "_validation_loss", "(", ")", "\n", "\n", "# It is safe again to wait till the validation is done. This is", "\n", "# important to get the metrics right.", "\n", "if", "self", ".", "_distributed", ":", "\n", "                        ", "dist", ".", "barrier", "(", ")", "\n", "\n", "", "val_metrics", "=", "training_util", ".", "get_metrics", "(", "\n", "self", ".", "model", ",", "\n", "val_loss", ",", "\n", "num_batches", ",", "\n", "reset", "=", "True", ",", "\n", "world_size", "=", "self", ".", "_world_size", ",", "\n", "cuda_device", "=", "[", "self", ".", "cuda_device", "]", ",", "\n", ")", "\n", "\n", "# Check validation metric for early stopping", "\n", "this_epoch_val_metric", "=", "val_metrics", "[", "self", ".", "_validation_metric", "]", "\n", "self", ".", "_metric_tracker", ".", "add_metric", "(", "this_epoch_val_metric", ")", "\n", "\n", "if", "self", ".", "_metric_tracker", ".", "should_stop_early", "(", ")", ":", "\n", "                        ", "logger", ".", "info", "(", "\"Ran out of patience.  Stopping training.\"", ")", "\n", "break", "\n", "\n", "", "", "", "if", "self", ".", "_master", ":", "\n", "                ", "self", ".", "_tensorboard", ".", "log_metrics", "(", "\n", "train_metrics", ",", "\n", "val_metrics", "=", "val_metrics", ",", "\n", "log_to_console", "=", "True", ",", "\n", "epoch", "=", "epoch", "+", "1", ",", "\n", ")", "# +1 because tensorboard doesn't like 0", "\n", "\n", "# Create overall metrics dict", "\n", "", "training_elapsed_time", "=", "time", ".", "time", "(", ")", "-", "training_start_time", "\n", "metrics", "[", "\"training_duration\"", "]", "=", "str", "(", "\n", "datetime", ".", "timedelta", "(", "seconds", "=", "training_elapsed_time", ")", "\n", ")", "\n", "metrics", "[", "\"training_start_epoch\"", "]", "=", "epoch_counter", "\n", "metrics", "[", "\"training_epochs\"", "]", "=", "epochs_trained", "\n", "metrics", "[", "\"epoch\"", "]", "=", "epoch", "\n", "\n", "for", "key", ",", "value", "in", "train_metrics", ".", "items", "(", ")", ":", "\n", "                ", "metrics", "[", "\"training_\"", "+", "key", "]", "=", "value", "\n", "", "for", "key", ",", "value", "in", "val_metrics", ".", "items", "(", ")", ":", "\n", "                ", "metrics", "[", "\"validation_\"", "+", "key", "]", "=", "value", "\n", "\n", "", "if", "self", ".", "_metric_tracker", ".", "is_best_so_far", "(", ")", ":", "\n", "# Update all the best_ metrics.", "\n", "# (Otherwise they just stay the same as they were.)", "\n", "                ", "metrics", "[", "\"best_epoch\"", "]", "=", "epoch", "\n", "for", "key", ",", "value", "in", "val_metrics", ".", "items", "(", ")", ":", "\n", "                    ", "metrics", "[", "\"best_validation_\"", "+", "key", "]", "=", "value", "\n", "\n", "", "self", ".", "_metric_tracker", ".", "best_epoch_metrics", "=", "val_metrics", "\n", "\n", "", "if", "self", ".", "_serialization_dir", "and", "self", ".", "_master", ":", "\n", "                ", "common_util", ".", "dump_metrics", "(", "\n", "os", ".", "path", ".", "join", "(", "\n", "self", ".", "_serialization_dir", ",", "f\"metrics_epoch_{epoch}.json\"", "\n", ")", ",", "\n", "metrics", ",", "\n", ")", "\n", "\n", "# The Scheduler API is agnostic to whether your schedule requires a validation metric -", "\n", "# if it doesn't, the validation metric passed here is ignored.", "\n", "", "if", "self", ".", "_learning_rate_scheduler", ":", "\n", "                ", "self", ".", "_learning_rate_scheduler", ".", "step", "(", "this_epoch_val_metric", ",", "epoch", ")", "\n", "", "if", "self", ".", "_momentum_scheduler", ":", "\n", "                ", "self", ".", "_momentum_scheduler", ".", "step", "(", "this_epoch_val_metric", ",", "epoch", ")", "\n", "\n", "", "if", "self", ".", "_master", ":", "\n", "                ", "self", ".", "_save_checkpoint", "(", "epoch", ")", "\n", "\n", "# Wait for the master to finish saving the checkpoint", "\n", "", "if", "self", ".", "_distributed", ":", "\n", "                ", "dist", ".", "barrier", "(", ")", "\n", "\n", "", "epoch_elapsed_time", "=", "time", ".", "time", "(", ")", "-", "epoch_start_time", "\n", "logger", ".", "info", "(", "\n", "\"Epoch duration: %s\"", ",", "datetime", ".", "timedelta", "(", "seconds", "=", "epoch_elapsed_time", ")", "\n", ")", "\n", "\n", "if", "epoch", "<", "self", ".", "_num_epochs", "-", "1", ":", "\n", "                ", "training_elapsed_time", "=", "time", ".", "time", "(", ")", "-", "training_start_time", "\n", "estimated_time_remaining", "=", "training_elapsed_time", "*", "(", "\n", "(", "self", ".", "_num_epochs", "-", "epoch_counter", ")", "\n", "/", "float", "(", "epoch", "-", "epoch_counter", "+", "1", ")", "\n", "-", "1", "\n", ")", "\n", "formatted_time", "=", "str", "(", "\n", "datetime", ".", "timedelta", "(", "seconds", "=", "int", "(", "estimated_time_remaining", ")", ")", "\n", ")", "\n", "logger", ".", "info", "(", "\"Estimated training time remaining: %s\"", ",", "formatted_time", ")", "\n", "\n", "", "epochs_trained", "+=", "1", "\n", "\n", "# make sure pending events are flushed to disk and files are closed properly", "\n", "", "self", ".", "_tensorboard", ".", "close", "(", ")", "\n", "\n", "# Load the best model state before returning", "\n", "best_model_state", "=", "self", ".", "_checkpointer", ".", "best_model_state", "(", ")", "\n", "if", "best_model_state", ":", "\n", "            ", "self", ".", "model", ".", "load_state_dict", "(", "best_model_state", ")", "\n", "\n", "", "return", "metrics", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.training.trainer.Trainer._save_checkpoint": [[694, 737], ["trainer.Trainer._checkpointer.save_checkpoint", "trainer.Trainer._moving_average.assign_average_value", "trainer.Trainer._metric_tracker.state_dict", "trainer.Trainer.optimizer.state_dict", "trainer.Trainer._learning_rate_scheduler.state_dict", "trainer.Trainer._momentum_scheduler.state_dict", "trainer.Trainer._moving_average.restore", "trainer.Trainer.model.state_dict", "trainer.Trainer._metric_tracker.is_best_so_far"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.training.checkpointer.Checkpointer.save_checkpoint", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.training.moving_average.MovingAverage.assign_average_value", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.learning_rate_schedulers.learning_rate_scheduler._PyTorchLearningRateSchedulerWrapper.state_dict", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.learning_rate_schedulers.learning_rate_scheduler._PyTorchLearningRateSchedulerWrapper.state_dict", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.learning_rate_schedulers.learning_rate_scheduler._PyTorchLearningRateSchedulerWrapper.state_dict", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.learning_rate_schedulers.learning_rate_scheduler._PyTorchLearningRateSchedulerWrapper.state_dict", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.training.moving_average.MovingAverage.restore", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.learning_rate_schedulers.learning_rate_scheduler._PyTorchLearningRateSchedulerWrapper.state_dict", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.training.metric_tracker.MetricTracker.is_best_so_far"], ["", "def", "_save_checkpoint", "(", "self", ",", "epoch", ":", "Union", "[", "int", ",", "str", "]", ")", "->", "None", ":", "\n", "        ", "\"\"\"\n        Saves a checkpoint of the model to self._serialization_dir.\n        Is a no-op if self._serialization_dir is None.\n\n        # Parameters\n\n        epoch : Union[int, str], required.\n            The epoch of training.  If the checkpoint is saved in the middle\n            of an epoch, the parameter is a string with the epoch and timestamp.\n        \"\"\"", "\n", "# If moving averages are used for parameters, we save", "\n", "# the moving average values into checkpoint, instead of the current values.", "\n", "if", "self", ".", "_moving_average", "is", "not", "None", ":", "\n", "            ", "self", ".", "_moving_average", ".", "assign_average_value", "(", ")", "\n", "\n", "# These are the training states we need to persist.", "\n", "", "training_states", "=", "{", "\n", "\"metric_tracker\"", ":", "self", ".", "_metric_tracker", ".", "state_dict", "(", ")", ",", "\n", "\"optimizer\"", ":", "self", ".", "optimizer", ".", "state_dict", "(", ")", ",", "\n", "\"batch_num_total\"", ":", "self", ".", "_batch_num_total", ",", "\n", "}", "\n", "\n", "# If we have a learning rate or momentum scheduler, we should persist them too.", "\n", "if", "self", ".", "_learning_rate_scheduler", "is", "not", "None", ":", "\n", "            ", "training_states", "[", "\n", "\"learning_rate_scheduler\"", "\n", "]", "=", "self", ".", "_learning_rate_scheduler", ".", "state_dict", "(", ")", "\n", "", "if", "self", ".", "_momentum_scheduler", "is", "not", "None", ":", "\n", "            ", "training_states", "[", "\n", "\"momentum_scheduler\"", "\n", "]", "=", "self", ".", "_momentum_scheduler", ".", "state_dict", "(", ")", "\n", "\n", "", "self", ".", "_checkpointer", ".", "save_checkpoint", "(", "\n", "model_state", "=", "self", ".", "model", ".", "state_dict", "(", ")", ",", "\n", "epoch", "=", "epoch", ",", "\n", "training_states", "=", "training_states", ",", "\n", "is_best_so_far", "=", "self", ".", "_metric_tracker", ".", "is_best_so_far", "(", ")", ",", "\n", ")", "\n", "\n", "# Restore the original values for parameters so that training will not be affected.", "\n", "if", "self", ".", "_moving_average", "is", "not", "None", ":", "\n", "            ", "self", ".", "_moving_average", ".", "restore", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.training.trainer.Trainer._restore_checkpoint": [[738, 803], ["trainer.Trainer._checkpointer.restore_checkpoint", "trainer.Trainer.model.load_state_dict", "trainer.Trainer.optimizer.load_state_dict", "allennlp.training.util.move_optimizer_to_cuda", "isinstance", "training_state.get", "trainer.Trainer._learning_rate_scheduler.load_state_dict", "trainer.Trainer._momentum_scheduler.load_state_dict", "trainer.Trainer._metric_tracker.load_state_dict", "trainer.Trainer._metric_tracker.clear", "trainer.Trainer._metric_tracker.add_metrics", "trainer.Trainer._metric_tracker.clear", "int", "training_state[].split"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.callbacks.checkpoint.Checkpoint.restore_checkpoint", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.learning_rate_schedulers.learning_rate_scheduler._PyTorchLearningRateSchedulerWrapper.load_state_dict", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.learning_rate_schedulers.learning_rate_scheduler._PyTorchLearningRateSchedulerWrapper.load_state_dict", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.training.util.move_optimizer_to_cuda", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.params.Params.get", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.learning_rate_schedulers.learning_rate_scheduler._PyTorchLearningRateSchedulerWrapper.load_state_dict", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.learning_rate_schedulers.learning_rate_scheduler._PyTorchLearningRateSchedulerWrapper.load_state_dict", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.learning_rate_schedulers.learning_rate_scheduler._PyTorchLearningRateSchedulerWrapper.load_state_dict", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.training.metric_tracker.MetricTracker.clear", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.training.metric_tracker.MetricTracker.add_metrics", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.training.metric_tracker.MetricTracker.clear"], ["", "", "def", "_restore_checkpoint", "(", "self", ")", "->", "int", ":", "\n", "        ", "\"\"\"\n        Restores the model and training state from the last saved checkpoint.\n        This includes an epoch count and optimizer state, which is serialized separately\n        from model parameters. This function should only be used to continue training -\n        if you wish to load a model for inference/load parts of a model into a new\n        computation graph, you should use the native Pytorch functions:\n        ` model.load_state_dict(torch.load(\"/path/to/model/weights.th\"))`\n\n        If `self._serialization_dir` does not exist or does not contain any checkpointed weights,\n        this function will do nothing and return 0.\n\n        # Returns\n\n        epoch: int\n            The epoch at which to resume training, which should be one after the epoch\n            in the saved training state.\n        \"\"\"", "\n", "model_state", ",", "training_state", "=", "self", ".", "_checkpointer", ".", "restore_checkpoint", "(", ")", "\n", "\n", "if", "not", "training_state", ":", "\n", "# No checkpoint to restore, start at 0", "\n", "            ", "return", "0", "\n", "\n", "", "self", ".", "model", ".", "load_state_dict", "(", "model_state", ")", "\n", "self", ".", "optimizer", ".", "load_state_dict", "(", "training_state", "[", "\"optimizer\"", "]", ")", "\n", "if", "(", "\n", "self", ".", "_learning_rate_scheduler", "is", "not", "None", "\n", "and", "\"learning_rate_scheduler\"", "in", "training_state", "\n", ")", ":", "\n", "            ", "self", ".", "_learning_rate_scheduler", ".", "load_state_dict", "(", "\n", "training_state", "[", "\"learning_rate_scheduler\"", "]", "\n", ")", "\n", "", "if", "(", "\n", "self", ".", "_momentum_scheduler", "is", "not", "None", "\n", "and", "\"momentum_scheduler\"", "in", "training_state", "\n", ")", ":", "\n", "            ", "self", ".", "_momentum_scheduler", ".", "load_state_dict", "(", "\n", "training_state", "[", "\"momentum_scheduler\"", "]", "\n", ")", "\n", "", "training_util", ".", "move_optimizer_to_cuda", "(", "self", ".", "optimizer", ")", "\n", "\n", "# Currently the `training_state` contains a serialized `MetricTracker`.", "\n", "if", "\"metric_tracker\"", "in", "training_state", ":", "\n", "            ", "self", ".", "_metric_tracker", ".", "load_state_dict", "(", "training_state", "[", "\"metric_tracker\"", "]", ")", "\n", "# It used to be the case that we tracked `val_metric_per_epoch`.", "\n", "", "elif", "\"val_metric_per_epoch\"", "in", "training_state", ":", "\n", "            ", "self", ".", "_metric_tracker", ".", "clear", "(", ")", "\n", "self", ".", "_metric_tracker", ".", "add_metrics", "(", "training_state", "[", "\"val_metric_per_epoch\"", "]", ")", "\n", "# And before that we didn't track anything.", "\n", "", "else", ":", "\n", "            ", "self", ".", "_metric_tracker", ".", "clear", "(", ")", "\n", "\n", "", "if", "isinstance", "(", "training_state", "[", "\"epoch\"", "]", ",", "int", ")", ":", "\n", "            ", "epoch_to_return", "=", "training_state", "[", "\"epoch\"", "]", "+", "1", "\n", "", "else", ":", "\n", "            ", "epoch_to_return", "=", "int", "(", "training_state", "[", "\"epoch\"", "]", ".", "split", "(", "\".\"", ")", "[", "0", "]", ")", "+", "1", "\n", "\n", "# For older checkpoints with batch_num_total missing, default to old behavior where", "\n", "# it is unchanged.", "\n", "", "batch_num_total", "=", "training_state", ".", "get", "(", "\"batch_num_total\"", ")", "\n", "if", "batch_num_total", "is", "not", "None", ":", "\n", "            ", "self", ".", "_batch_num_total", "=", "batch_num_total", "\n", "\n", "", "return", "epoch_to_return", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.training.trainer.Trainer.from_partial_objects": [[804, 909], ["allennlp.common.checks.check_for_gpu", "allennlp.common.util.log_frozen_and_tunable_parameter_names", "optimizer.construct", "moving_average.construct", "learning_rate_scheduler.construct", "momentum_scheduler.construct", "checkpointer.construct", "cls", "model.cuda.cuda.cuda", "model.cuda.cuda.named_parameters", "allennlp.training.optimizers.Optimizer.default", "allennlp.training.checkpointer.Checkpointer", "any", "model.cuda.cuda.named_parameters", "parameter.requires_grad_", "re.search"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.checks.check_for_gpu", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.util.log_frozen_and_tunable_parameter_names", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.lazy.Lazy.construct", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.lazy.Lazy.construct", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.lazy.Lazy.construct", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.lazy.Lazy.construct", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.lazy.Lazy.construct", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.training.optimizers.Optimizer.default", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.nn.beam_search.BeamSearch.search"], ["", "@", "classmethod", "\n", "def", "from_partial_objects", "(", "\n", "cls", ",", "\n", "model", ":", "Model", ",", "\n", "serialization_dir", ":", "str", ",", "\n", "iterator", ":", "DataIterator", ",", "\n", "train_data", ":", "Iterable", "[", "Instance", "]", ",", "\n", "validation_iterator", ":", "DataIterator", "=", "None", ",", "\n", "validation_data", ":", "Iterable", "[", "Instance", "]", "=", "None", ",", "\n", "local_rank", ":", "int", "=", "0", ",", "\n", "patience", ":", "int", "=", "None", ",", "\n", "validation_metric", ":", "str", "=", "\"-loss\"", ",", "\n", "shuffle", ":", "bool", "=", "True", ",", "\n", "num_epochs", ":", "int", "=", "20", ",", "\n", "cuda_device", ":", "int", "=", "-", "1", ",", "\n", "grad_norm", ":", "float", "=", "None", ",", "\n", "grad_clipping", ":", "float", "=", "None", ",", "\n", "model_save_interval", ":", "float", "=", "None", ",", "\n", "summary_interval", ":", "int", "=", "100", ",", "\n", "histogram_interval", ":", "int", "=", "None", ",", "\n", "should_log_parameter_statistics", ":", "bool", "=", "True", ",", "\n", "should_log_learning_rate", ":", "bool", "=", "False", ",", "\n", "log_batch_size_period", ":", "int", "=", "None", ",", "\n", "distributed", ":", "bool", "=", "None", ",", "\n", "world_size", ":", "int", "=", "1", ",", "\n", "num_gradient_accumulation_steps", ":", "int", "=", "1", ",", "\n", "no_grad", ":", "List", "[", "str", "]", "=", "None", ",", "\n", "optimizer", ":", "Lazy", "[", "Optimizer", "]", "=", "None", ",", "\n", "learning_rate_scheduler", ":", "Lazy", "[", "LearningRateScheduler", "]", "=", "None", ",", "\n", "momentum_scheduler", ":", "Lazy", "[", "MomentumScheduler", "]", "=", "None", ",", "\n", "moving_average", ":", "Lazy", "[", "MovingAverage", "]", "=", "None", ",", "\n", "checkpointer", ":", "Lazy", "[", "Checkpointer", "]", "=", "None", ",", "\n", ")", "->", "\"Trainer\"", ":", "\n", "        ", "\"\"\"\n        This method exists so that we can have a documented method to construct this class using\n        `FromParams`. If you are not using `FromParams` or config files, you can safely ignore this\n        method.\n\n        The reason we can't just use `__init__` with `FromParams` here is because there are\n        sequential dependencies to this class's arguments.  Anything that has a `Lazy[]` type\n        annotation needs something from one of the non-`Lazy` arguments.  The `Optimizer` needs to\n        have the parameters from the `Model` before it's constructed, and the `Schedulers` need to\n        have the `Optimizer`. Because of this, the typical way we construct things `FromParams`\n        doesn't work, so we use `Lazy` to allow for constructing the objects sequentially.\n\n        If you're not using `FromParams`, you can just construct these arguments in the right order\n        yourself in your code and call the constructor directly.\n        \"\"\"", "\n", "\n", "check_for_gpu", "(", "cuda_device", ")", "\n", "if", "cuda_device", ">=", "0", ":", "\n", "# Moving model to GPU here so that the optimizer state gets constructed on", "\n", "# the right device.", "\n", "            ", "model", "=", "model", ".", "cuda", "(", "cuda_device", ")", "\n", "\n", "", "if", "no_grad", ":", "\n", "            ", "for", "name", ",", "parameter", "in", "model", ".", "named_parameters", "(", ")", ":", "\n", "                ", "if", "any", "(", "re", ".", "search", "(", "regex", ",", "name", ")", "for", "regex", "in", "no_grad", ")", ":", "\n", "                    ", "parameter", ".", "requires_grad_", "(", "False", ")", "\n", "\n", "", "", "", "common_util", ".", "log_frozen_and_tunable_parameter_names", "(", "model", ")", "\n", "\n", "parameters", "=", "[", "[", "n", ",", "p", "]", "for", "n", ",", "p", "in", "model", ".", "named_parameters", "(", ")", "if", "p", ".", "requires_grad", "]", "\n", "optimizer_", "=", "optimizer", ".", "construct", "(", "model_parameters", "=", "parameters", ")", "\n", "if", "not", "optimizer_", ":", "\n", "            ", "optimizer_", "=", "Optimizer", ".", "default", "(", "parameters", ")", "\n", "\n", "", "moving_average_", "=", "moving_average", ".", "construct", "(", "parameters", "=", "parameters", ")", "\n", "learning_rate_scheduler_", "=", "learning_rate_scheduler", ".", "construct", "(", "\n", "optimizer", "=", "optimizer_", "\n", ")", "\n", "momentum_scheduler_", "=", "momentum_scheduler", ".", "construct", "(", "optimizer", "=", "optimizer_", ")", "\n", "\n", "checkpointer_", "=", "checkpointer", ".", "construct", "(", ")", "\n", "if", "not", "checkpointer_", ":", "\n", "            ", "checkpointer_", "=", "Checkpointer", "(", "serialization_dir", ")", "\n", "", "return", "cls", "(", "\n", "model", ",", "\n", "optimizer_", ",", "\n", "iterator", ",", "\n", "train_data", ",", "\n", "validation_data", ",", "\n", "patience", "=", "patience", ",", "\n", "validation_metric", "=", "validation_metric", ",", "\n", "validation_iterator", "=", "validation_iterator", ",", "\n", "shuffle", "=", "shuffle", ",", "\n", "num_epochs", "=", "num_epochs", ",", "\n", "serialization_dir", "=", "serialization_dir", ",", "\n", "cuda_device", "=", "cuda_device", ",", "\n", "grad_norm", "=", "grad_norm", ",", "\n", "grad_clipping", "=", "grad_clipping", ",", "\n", "learning_rate_scheduler", "=", "learning_rate_scheduler_", ",", "\n", "momentum_scheduler", "=", "momentum_scheduler_", ",", "\n", "checkpointer", "=", "checkpointer_", ",", "\n", "model_save_interval", "=", "model_save_interval", ",", "\n", "summary_interval", "=", "summary_interval", ",", "\n", "histogram_interval", "=", "histogram_interval", ",", "\n", "should_log_parameter_statistics", "=", "should_log_parameter_statistics", ",", "\n", "should_log_learning_rate", "=", "should_log_learning_rate", ",", "\n", "log_batch_size_period", "=", "log_batch_size_period", ",", "\n", "moving_average", "=", "moving_average_", ",", "\n", "distributed", "=", "distributed", ",", "\n", "local_rank", "=", "local_rank", ",", "\n", "world_size", "=", "world_size", ",", "\n", "num_gradient_accumulation_steps", "=", "num_gradient_accumulation_steps", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.training.scheduler.Scheduler.__init__": [[29, 59], ["scheduler.Scheduler.step", "enumerate", "enumerate", "group.setdefault", "KeyError", "KeyError"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.learning_rate_schedulers.slanted_triangular.SlantedTriangular.step"], ["def", "__init__", "(", "\n", "self", ",", "\n", "optimizer", ":", "torch", ".", "optim", ".", "Optimizer", ",", "\n", "param_group_field", ":", "str", ",", "\n", "last_epoch", ":", "int", "=", "-", "1", ",", "\n", ")", "->", "None", ":", "\n", "        ", "self", ".", "optimizer", "=", "optimizer", "\n", "self", ".", "param_group_field", "=", "param_group_field", "\n", "self", ".", "_initial_param_group_field", "=", "f\"initial_{param_group_field}\"", "\n", "if", "last_epoch", "==", "-", "1", ":", "\n", "            ", "for", "i", ",", "group", "in", "enumerate", "(", "self", ".", "optimizer", ".", "param_groups", ")", ":", "\n", "                ", "if", "param_group_field", "not", "in", "group", ":", "\n", "                    ", "raise", "KeyError", "(", "\n", "f\"{param_group_field} missing from param_groups[{i}]\"", "\n", ")", "\n", "", "group", ".", "setdefault", "(", "\n", "self", ".", "_initial_param_group_field", ",", "group", "[", "param_group_field", "]", "\n", ")", "\n", "", "", "else", ":", "\n", "            ", "for", "i", ",", "group", "in", "enumerate", "(", "self", ".", "optimizer", ".", "param_groups", ")", ":", "\n", "                ", "if", "self", ".", "_initial_param_group_field", "not", "in", "group", ":", "\n", "                    ", "raise", "KeyError", "(", "\n", "f\"{self._initial_param_group_field} missing from param_groups[{i}]\"", "\n", ")", "\n", "", "", "", "self", ".", "base_values", "=", "[", "\n", "group", "[", "self", ".", "_initial_param_group_field", "]", "\n", "for", "group", "in", "self", ".", "optimizer", ".", "param_groups", "\n", "]", "\n", "self", ".", "step", "(", "epoch", "=", "last_epoch", ")", "\n", "self", ".", "last_epoch", "=", "last_epoch", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.training.scheduler.Scheduler.state_dict": [[60, 66], ["scheduler.Scheduler.__dict__.items"], "methods", ["None"], ["", "def", "state_dict", "(", "self", ")", "->", "Dict", "[", "str", ",", "Any", "]", ":", "\n", "        ", "\"\"\"\n        Returns the state of the scheduler as a `dict`.\n        \"\"\"", "\n", "return", "{", "\n", "key", ":", "value", "for", "key", ",", "value", "in", "self", ".", "__dict__", ".", "items", "(", ")", "if", "key", "!=", "\"optimizer\"", "\n", "}", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.training.scheduler.Scheduler.load_state_dict": [[68, 78], ["scheduler.Scheduler.__dict__.update"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.metrics.conll_coref_scores.Scorer.update"], ["", "def", "load_state_dict", "(", "self", ",", "state_dict", ":", "Dict", "[", "str", ",", "Any", "]", ")", "->", "None", ":", "\n", "        ", "\"\"\"\n        Load the schedulers state.\n\n        # Parameters\n\n        state_dict : `Dict[str, Any]`\n            Scheduler state. Should be an object returned from a call to `state_dict`.\n        \"\"\"", "\n", "self", ".", "__dict__", ".", "update", "(", "state_dict", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.training.scheduler.Scheduler.get_values": [[79, 81], ["None"], "methods", ["None"], ["", "def", "get_values", "(", "self", ")", ":", "\n", "        ", "raise", "NotImplementedError", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.training.scheduler.Scheduler.step": [[82, 90], ["zip", "scheduler.Scheduler.get_values"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.learning_rate_schedulers.slanted_triangular.SlantedTriangular.get_values"], ["", "def", "step", "(", "self", ",", "metric", ":", "float", "=", "None", ",", "epoch", ":", "int", "=", "None", ")", "->", "None", ":", "\n", "        ", "if", "epoch", "is", "None", ":", "\n", "            ", "self", ".", "last_epoch", "+=", "1", "# type: ignore", "\n", "", "else", ":", "\n", "            ", "self", ".", "last_epoch", "=", "epoch", "\n", "", "self", ".", "metric", "=", "metric", "\n", "for", "param_group", ",", "value", "in", "zip", "(", "self", ".", "optimizer", ".", "param_groups", ",", "self", ".", "get_values", "(", ")", ")", ":", "\n", "            ", "param_group", "[", "self", ".", "param_group_field", "]", "=", "value", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.training.scheduler.Scheduler.step_batch": [[91, 98], ["None"], "methods", ["None"], ["", "", "def", "step_batch", "(", "self", ",", "batch_num_total", ":", "int", "=", "None", ")", "->", "None", ":", "\n", "        ", "\"\"\"\n        By default, a scheduler is assumed to only update every epoch, not every batch.\n        So this does nothing unless it's overriden.\n        \"\"\"", "\n", "\n", "return", "\n", "", "", ""]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.training.tensorboard_writer.TensorboardWriter.__init__": [[36, 60], ["tensorboardX.SummaryWriter", "tensorboardX.SummaryWriter", "os.path.join", "os.path.join"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.dataset_readers.multiprocess_dataset_reader.QIterable.join", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.dataset_readers.multiprocess_dataset_reader.QIterable.join"], ["def", "__init__", "(", "\n", "self", ",", "\n", "get_batch_num_total", ":", "Callable", "[", "[", "]", ",", "int", "]", ",", "\n", "serialization_dir", ":", "Optional", "[", "str", "]", "=", "None", ",", "\n", "summary_interval", ":", "int", "=", "100", ",", "\n", "histogram_interval", ":", "int", "=", "None", ",", "\n", "should_log_parameter_statistics", ":", "bool", "=", "True", ",", "\n", "should_log_learning_rate", ":", "bool", "=", "False", ",", "\n", ")", "->", "None", ":", "\n", "        ", "if", "serialization_dir", "is", "not", "None", ":", "\n", "            ", "self", ".", "_train_log", "=", "SummaryWriter", "(", "\n", "os", ".", "path", ".", "join", "(", "serialization_dir", ",", "\"log\"", ",", "\"train\"", ")", "\n", ")", "\n", "self", ".", "_validation_log", "=", "SummaryWriter", "(", "\n", "os", ".", "path", ".", "join", "(", "serialization_dir", ",", "\"log\"", ",", "\"validation\"", ")", "\n", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "_train_log", "=", "self", ".", "_validation_log", "=", "None", "\n", "\n", "", "self", ".", "_summary_interval", "=", "summary_interval", "\n", "self", ".", "_histogram_interval", "=", "histogram_interval", "\n", "self", ".", "_should_log_parameter_statistics", "=", "should_log_parameter_statistics", "\n", "self", ".", "_should_log_learning_rate", "=", "should_log_learning_rate", "\n", "self", ".", "_get_batch_num_total", "=", "get_batch_num_total", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.training.tensorboard_writer.TensorboardWriter._item": [[61, 68], ["hasattr", "value.item"], "methods", ["None"], ["", "@", "staticmethod", "\n", "def", "_item", "(", "value", ":", "Any", ")", ":", "\n", "        ", "if", "hasattr", "(", "value", ",", "\"item\"", ")", ":", "\n", "            ", "val", "=", "value", ".", "item", "(", ")", "\n", "", "else", ":", "\n", "            ", "val", "=", "value", "\n", "", "return", "val", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.training.tensorboard_writer.TensorboardWriter.should_log_this_batch": [[69, 71], ["tensorboard_writer.TensorboardWriter._get_batch_num_total"], "methods", ["None"], ["", "def", "should_log_this_batch", "(", "self", ")", "->", "bool", ":", "\n", "        ", "return", "self", ".", "_get_batch_num_total", "(", ")", "%", "self", ".", "_summary_interval", "==", "0", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.training.tensorboard_writer.TensorboardWriter.should_log_histograms_this_batch": [[72, 76], ["tensorboard_writer.TensorboardWriter._get_batch_num_total"], "methods", ["None"], ["", "def", "should_log_histograms_this_batch", "(", "self", ")", "->", "bool", ":", "\n", "        ", "return", "(", "\n", "self", ".", "_histogram_interval", "is", "not", "None", "\n", "and", "self", ".", "_get_batch_num_total", "(", ")", "%", "self", ".", "_histogram_interval", "==", "0", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.training.tensorboard_writer.TensorboardWriter.add_train_scalar": [[78, 83], ["tensorboard_writer.TensorboardWriter._get_batch_num_total", "tensorboard_writer.TensorboardWriter._train_log.add_scalar", "tensorboard_writer.TensorboardWriter._item"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.training.tensorboard_writer.TensorboardWriter._item"], ["", "def", "add_train_scalar", "(", "self", ",", "name", ":", "str", ",", "value", ":", "float", ",", "timestep", ":", "int", "=", "None", ")", "->", "None", ":", "\n", "        ", "timestep", "=", "timestep", "or", "self", ".", "_get_batch_num_total", "(", ")", "\n", "# get the scalar", "\n", "if", "self", ".", "_train_log", "is", "not", "None", ":", "\n", "            ", "self", ".", "_train_log", ".", "add_scalar", "(", "name", ",", "self", ".", "_item", "(", "value", ")", ",", "timestep", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.training.tensorboard_writer.TensorboardWriter.add_train_histogram": [[84, 90], ["isinstance", "values.cpu().data.numpy().flatten", "tensorboard_writer.TensorboardWriter._train_log.add_histogram", "tensorboard_writer.TensorboardWriter._get_batch_num_total", "values.cpu().data.numpy", "values.cpu"], "methods", ["None"], ["", "", "def", "add_train_histogram", "(", "self", ",", "name", ":", "str", ",", "values", ":", "torch", ".", "Tensor", ")", "->", "None", ":", "\n", "        ", "if", "self", ".", "_train_log", "is", "not", "None", ":", "\n", "            ", "if", "isinstance", "(", "values", ",", "torch", ".", "Tensor", ")", ":", "\n", "                ", "values_to_write", "=", "values", ".", "cpu", "(", ")", ".", "data", ".", "numpy", "(", ")", ".", "flatten", "(", ")", "\n", "self", ".", "_train_log", ".", "add_histogram", "(", "\n", "name", ",", "values_to_write", ",", "self", ".", "_get_batch_num_total", "(", ")", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.training.tensorboard_writer.TensorboardWriter.add_validation_scalar": [[92, 98], ["tensorboard_writer.TensorboardWriter._get_batch_num_total", "tensorboard_writer.TensorboardWriter._validation_log.add_scalar", "tensorboard_writer.TensorboardWriter._item"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.training.tensorboard_writer.TensorboardWriter._item"], ["", "", "", "def", "add_validation_scalar", "(", "\n", "self", ",", "name", ":", "str", ",", "value", ":", "float", ",", "timestep", ":", "int", "=", "None", "\n", ")", "->", "None", ":", "\n", "        ", "timestep", "=", "timestep", "or", "self", ".", "_get_batch_num_total", "(", ")", "\n", "if", "self", ".", "_validation_log", "is", "not", "None", ":", "\n", "            ", "self", ".", "_validation_log", ".", "add_scalar", "(", "name", ",", "self", ".", "_item", "(", "value", ")", ",", "timestep", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.training.tensorboard_writer.TensorboardWriter.log_parameter_and_gradient_statistics": [[99, 135], ["model.named_parameters", "tensorboard_writer.TensorboardWriter.add_train_scalar", "param.data.numel", "tensorboard_writer.TensorboardWriter.add_train_scalar", "param.data.numel", "tensorboard_writer.TensorboardWriter.add_train_scalar", "param.data.mean", "param.data.std", "param.grad.data._values", "torch.prod().item", "tensorboard_writer.TensorboardWriter.add_train_scalar", "logger.info", "param.grad.data._values.mean", "param.grad.data._values.numel", "tensorboard_writer.TensorboardWriter.add_train_scalar", "torch.prod", "param.grad.data._values.std", "torch.tensor"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.training.tensorboard_writer.TensorboardWriter.add_train_scalar", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.training.tensorboard_writer.TensorboardWriter.add_train_scalar", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.training.tensorboard_writer.TensorboardWriter.add_train_scalar", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.training.tensorboard_writer.TensorboardWriter.add_train_scalar", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.dataset_readers.multiprocess_dataset_reader.logger.info", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.training.tensorboard_writer.TensorboardWriter.add_train_scalar"], ["", "", "def", "log_parameter_and_gradient_statistics", "(", "\n", "self", ",", "model", ":", "Model", ",", "batch_grad_norm", ":", "float", "\n", ")", "->", "None", ":", "\n", "        ", "\"\"\"\n        Send the mean and std of all parameters and gradients to tensorboard, as well\n        as logging the average gradient norm.\n        \"\"\"", "\n", "if", "self", ".", "_should_log_parameter_statistics", ":", "\n", "# Log parameter values to Tensorboard", "\n", "            ", "for", "name", ",", "param", "in", "model", ".", "named_parameters", "(", ")", ":", "\n", "                ", "if", "param", ".", "data", ".", "numel", "(", ")", ">", "0", ":", "\n", "                    ", "self", ".", "add_train_scalar", "(", "\"parameter_mean/\"", "+", "name", ",", "param", ".", "data", ".", "mean", "(", ")", ")", "\n", "", "if", "param", ".", "data", ".", "numel", "(", ")", ">", "1", ":", "\n", "                    ", "self", ".", "add_train_scalar", "(", "\"parameter_std/\"", "+", "name", ",", "param", ".", "data", ".", "std", "(", ")", ")", "\n", "", "if", "param", ".", "grad", "is", "not", "None", ":", "\n", "                    ", "if", "param", ".", "grad", ".", "is_sparse", ":", "\n", "\n", "                        ", "grad_data", "=", "param", ".", "grad", ".", "data", ".", "_values", "(", ")", "\n", "", "else", ":", "\n", "                        ", "grad_data", "=", "param", ".", "grad", ".", "data", "\n", "\n", "# skip empty gradients", "\n", "", "if", "torch", ".", "prod", "(", "torch", ".", "tensor", "(", "grad_data", ".", "shape", ")", ")", ".", "item", "(", ")", ">", "0", ":", "\n", "                        ", "self", ".", "add_train_scalar", "(", "\"gradient_mean/\"", "+", "name", ",", "grad_data", ".", "mean", "(", ")", ")", "\n", "if", "grad_data", ".", "numel", "(", ")", ">", "1", ":", "\n", "                            ", "self", ".", "add_train_scalar", "(", "\n", "\"gradient_std/\"", "+", "name", ",", "grad_data", ".", "std", "(", ")", "\n", ")", "\n", "", "", "else", ":", "\n", "# no gradient for a parameter with sparse gradients", "\n", "                        ", "logger", ".", "info", "(", "\n", "\"No gradient for %s, skipping tensorboard logging.\"", ",", "name", "\n", ")", "\n", "# norm of gradients", "\n", "", "", "", "if", "batch_grad_norm", "is", "not", "None", ":", "\n", "                ", "self", ".", "add_train_scalar", "(", "\"gradient_norm\"", ",", "batch_grad_norm", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.training.tensorboard_writer.TensorboardWriter.log_learning_rates": [[136, 153], ["model.named_parameters", "tensorboard_writer.TensorboardWriter.add_train_scalar", "float"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.training.tensorboard_writer.TensorboardWriter.add_train_scalar"], ["", "", "", "def", "log_learning_rates", "(", "self", ",", "model", ":", "Model", ",", "optimizer", ":", "torch", ".", "optim", ".", "Optimizer", ")", ":", "\n", "        ", "\"\"\"\n        Send current parameter specific learning rates to tensorboard\n        \"\"\"", "\n", "if", "self", ".", "_should_log_learning_rate", ":", "\n", "# optimizer stores lr info keyed by parameter tensor", "\n", "# we want to log with parameter name", "\n", "            ", "names", "=", "{", "param", ":", "name", "for", "name", ",", "param", "in", "model", ".", "named_parameters", "(", ")", "}", "\n", "for", "group", "in", "optimizer", ".", "param_groups", ":", "\n", "                ", "if", "\"lr\"", "not", "in", "group", ":", "\n", "                    ", "continue", "\n", "", "rate", "=", "group", "[", "\"lr\"", "]", "\n", "for", "param", "in", "group", "[", "\"params\"", "]", ":", "\n", "# check whether params has requires grad or not", "\n", "                    ", "effective_rate", "=", "rate", "*", "float", "(", "param", ".", "requires_grad", ")", "\n", "self", ".", "add_train_scalar", "(", "\n", "\"learning_rate/\"", "+", "names", "[", "param", "]", ",", "effective_rate", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.training.tensorboard_writer.TensorboardWriter.log_histograms": [[155, 162], ["model.named_parameters", "tensorboard_writer.TensorboardWriter.add_train_histogram"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.training.tensorboard_writer.TensorboardWriter.add_train_histogram"], ["", "", "", "", "def", "log_histograms", "(", "self", ",", "model", ":", "Model", ",", "histogram_parameters", ":", "Set", "[", "str", "]", ")", "->", "None", ":", "\n", "        ", "\"\"\"\n        Send histograms of parameters to tensorboard.\n        \"\"\"", "\n", "for", "name", ",", "param", "in", "model", ".", "named_parameters", "(", ")", ":", "\n", "            ", "if", "name", "in", "histogram_parameters", ":", "\n", "                ", "self", ".", "add_train_histogram", "(", "\"parameter_histogram/\"", "+", "name", ",", "param", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.training.tensorboard_writer.TensorboardWriter.log_metrics": [[163, 219], ["set", "train_metrics.keys", "set.update", "max", "logger.info", "train_metrics.get", "val_metrics.get", "val_metrics.keys", "tensorboard_writer.TensorboardWriter.add_train_scalar", "tensorboard_writer.TensorboardWriter.add_validation_scalar", "logger.info", "len", "name.ljust", "logger.info", "name.ljust", "logger.info", "name.ljust"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.metrics.conll_coref_scores.Scorer.update", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.dataset_readers.multiprocess_dataset_reader.logger.info", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.params.Params.get", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.params.Params.get", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.training.tensorboard_writer.TensorboardWriter.add_train_scalar", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.training.tensorboard_writer.TensorboardWriter.add_validation_scalar", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.dataset_readers.multiprocess_dataset_reader.logger.info", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.dataset_readers.multiprocess_dataset_reader.logger.info", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.dataset_readers.multiprocess_dataset_reader.logger.info"], ["", "", "", "def", "log_metrics", "(", "\n", "self", ",", "\n", "train_metrics", ":", "dict", ",", "\n", "val_metrics", ":", "dict", "=", "None", ",", "\n", "epoch", ":", "int", "=", "None", ",", "\n", "log_to_console", ":", "bool", "=", "False", ",", "\n", ")", "->", "None", ":", "\n", "        ", "\"\"\"\n        Sends all of the train metrics (and validation metrics, if provided) to tensorboard.\n        \"\"\"", "\n", "metric_names", "=", "set", "(", "train_metrics", ".", "keys", "(", ")", ")", "\n", "if", "val_metrics", "is", "not", "None", ":", "\n", "            ", "metric_names", ".", "update", "(", "val_metrics", ".", "keys", "(", ")", ")", "\n", "", "val_metrics", "=", "val_metrics", "or", "{", "}", "\n", "\n", "# For logging to the console", "\n", "if", "log_to_console", ":", "\n", "            ", "dual_message_template", "=", "\"%s |  %8.3f  |  %8.3f\"", "\n", "no_val_message_template", "=", "\"%s |  %8.3f  |  %8s\"", "\n", "no_train_message_template", "=", "\"%s |  %8s  |  %8.3f\"", "\n", "header_template", "=", "\"%s |  %-10s\"", "\n", "name_length", "=", "max", "(", "[", "len", "(", "x", ")", "for", "x", "in", "metric_names", "]", ")", "\n", "logger", ".", "info", "(", "\n", "header_template", ",", "\"Training\"", ".", "rjust", "(", "name_length", "+", "13", ")", ",", "\"Validation\"", "\n", ")", "\n", "\n", "", "for", "name", "in", "metric_names", ":", "\n", "# Log to tensorboard", "\n", "            ", "train_metric", "=", "train_metrics", ".", "get", "(", "name", ")", "\n", "if", "train_metric", "is", "not", "None", ":", "\n", "                ", "self", ".", "add_train_scalar", "(", "name", ",", "train_metric", ",", "timestep", "=", "epoch", ")", "\n", "", "val_metric", "=", "val_metrics", ".", "get", "(", "name", ")", "\n", "if", "val_metric", "is", "not", "None", ":", "\n", "                ", "self", ".", "add_validation_scalar", "(", "name", ",", "val_metric", ",", "timestep", "=", "epoch", ")", "\n", "\n", "# And maybe log to console", "\n", "", "if", "log_to_console", "and", "val_metric", "is", "not", "None", "and", "train_metric", "is", "not", "None", ":", "\n", "                ", "logger", ".", "info", "(", "\n", "dual_message_template", ",", "\n", "name", ".", "ljust", "(", "name_length", ")", ",", "\n", "train_metric", ",", "\n", "val_metric", ",", "\n", ")", "\n", "", "elif", "log_to_console", "and", "val_metric", "is", "not", "None", ":", "\n", "                ", "logger", ".", "info", "(", "\n", "no_train_message_template", ",", "\n", "name", ".", "ljust", "(", "name_length", ")", ",", "\n", "\"N/A\"", ",", "\n", "val_metric", ",", "\n", ")", "\n", "", "elif", "log_to_console", "and", "train_metric", "is", "not", "None", ":", "\n", "                ", "logger", ".", "info", "(", "\n", "no_val_message_template", ",", "\n", "name", ".", "ljust", "(", "name_length", ")", ",", "\n", "train_metric", ",", "\n", "\"N/A\"", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.training.tensorboard_writer.TensorboardWriter.enable_activation_logging": [[221, 239], ["model.named_modules", "module.register_forward_hook", "getattr", "tensorboard_writer.TensorboardWriter.should_log_histograms_this_batch", "tensorboard_writer.TensorboardWriter.log_activation_histogram"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.training.tensorboard_writer.TensorboardWriter.should_log_histograms_this_batch", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.training.tensorboard_writer.TensorboardWriter.log_activation_histogram"], ["", "", "", "def", "enable_activation_logging", "(", "self", ",", "model", ":", "Model", ")", "->", "None", ":", "\n", "        ", "if", "self", ".", "_histogram_interval", "is", "not", "None", ":", "\n", "# To log activation histograms to the forward pass, we register", "\n", "# a hook on forward to capture the output tensors.", "\n", "# This uses a closure to determine whether to log the activations,", "\n", "# since we don't want them on every call.", "\n", "            ", "for", "_", ",", "module", "in", "model", ".", "named_modules", "(", ")", ":", "\n", "                ", "if", "not", "getattr", "(", "module", ",", "\"should_log_activations\"", ",", "False", ")", ":", "\n", "# skip it", "\n", "                    ", "continue", "\n", "\n", "", "def", "hook", "(", "module_", ",", "inputs", ",", "outputs", ")", ":", "\n", "\n", "                    ", "log_prefix", "=", "\"activation_histogram/{0}\"", ".", "format", "(", "module_", ".", "__class__", ")", "\n", "if", "self", ".", "should_log_histograms_this_batch", "(", ")", ":", "\n", "                        ", "self", ".", "log_activation_histogram", "(", "outputs", ",", "log_prefix", ")", "\n", "\n", "", "", "module", ".", "register_forward_hook", "(", "hook", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.training.tensorboard_writer.TensorboardWriter.log_activation_histogram": [[240, 255], ["isinstance", "tensorboard_writer.TensorboardWriter.add_train_histogram", "isinstance", "enumerate", "isinstance", "tensorboard_writer.TensorboardWriter.add_train_histogram", "outputs.items", "tensorboard_writer.TensorboardWriter.add_train_histogram"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.training.tensorboard_writer.TensorboardWriter.add_train_histogram", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.training.tensorboard_writer.TensorboardWriter.add_train_histogram", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.training.tensorboard_writer.TensorboardWriter.add_train_histogram"], ["", "", "", "def", "log_activation_histogram", "(", "self", ",", "outputs", ",", "log_prefix", ":", "str", ")", "->", "None", ":", "\n", "        ", "if", "isinstance", "(", "outputs", ",", "torch", ".", "Tensor", ")", ":", "\n", "            ", "log_name", "=", "log_prefix", "\n", "self", ".", "add_train_histogram", "(", "log_name", ",", "outputs", ")", "\n", "", "elif", "isinstance", "(", "outputs", ",", "(", "list", ",", "tuple", ")", ")", ":", "\n", "            ", "for", "i", ",", "output", "in", "enumerate", "(", "outputs", ")", ":", "\n", "                ", "log_name", "=", "\"{0}_{1}\"", ".", "format", "(", "log_prefix", ",", "i", ")", "\n", "self", ".", "add_train_histogram", "(", "log_name", ",", "output", ")", "\n", "", "", "elif", "isinstance", "(", "outputs", ",", "dict", ")", ":", "\n", "            ", "for", "k", ",", "tensor", "in", "outputs", ".", "items", "(", ")", ":", "\n", "                ", "log_name", "=", "\"{0}_{1}\"", ".", "format", "(", "log_prefix", ",", "k", ")", "\n", "self", ".", "add_train_histogram", "(", "log_name", ",", "tensor", ")", "\n", "", "", "else", ":", "\n", "# skip it", "\n", "            ", "pass", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.training.tensorboard_writer.TensorboardWriter.close": [[256, 265], ["tensorboard_writer.TensorboardWriter._train_log.close", "tensorboard_writer.TensorboardWriter._validation_log.close"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.token_embedders.embedding.EmbeddingsTextFile.close", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.token_embedders.embedding.EmbeddingsTextFile.close"], ["", "", "def", "close", "(", "self", ")", "->", "None", ":", "\n", "        ", "\"\"\"\n        Calls the `close` method of the `SummaryWriter` s which makes sure that pending\n        scalars are flushed to disk and the tensorboard event files are closed properly.\n        \"\"\"", "\n", "if", "self", ".", "_train_log", "is", "not", "None", ":", "\n", "            ", "self", ".", "_train_log", ".", "close", "(", ")", "\n", "", "if", "self", ".", "_validation_log", "is", "not", "None", ":", "\n", "            ", "self", ".", "_validation_log", ".", "close", "(", ")", "\n", "", "", "", ""]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.training.no_op_trainer.NoOpTrainer.__init__": [[11, 19], ["allennlp.training.trainer_base.TrainerBase.__init__"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.graph_util.plot.GraphPlot.__init__"], ["    ", "def", "__init__", "(", "self", ",", "serialization_dir", ":", "str", ",", "model", ":", "Model", ")", "->", "None", ":", "\n", "        ", "\"\"\"\n        A trivial trainer to assist in making model archives for models that do not actually\n        require training. For instance, a majority class baseline.\n        \"\"\"", "\n", "\n", "super", "(", ")", ".", "__init__", "(", "serialization_dir", ",", "cuda_device", "=", "-", "1", ")", "\n", "self", ".", "model", "=", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.training.no_op_trainer.NoOpTrainer.train": [[20, 33], ["no_op_trainer.NoOpTrainer.model.vocab.save_to_files", "allennlp.training.checkpointer.Checkpointer", "allennlp.training.checkpointer.Checkpointer.save_checkpoint", "os.path.join", "no_op_trainer.NoOpTrainer.model.state_dict"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.data.vocabulary.Vocabulary.save_to_files", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.training.checkpointer.Checkpointer.save_checkpoint", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.dataset_readers.multiprocess_dataset_reader.QIterable.join", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.learning_rate_schedulers.learning_rate_scheduler._PyTorchLearningRateSchedulerWrapper.state_dict"], ["", "def", "train", "(", "self", ")", "->", "Dict", "[", "str", ",", "Any", "]", ":", "\n", "        ", "self", ".", "model", ".", "vocab", ".", "save_to_files", "(", "\n", "os", ".", "path", ".", "join", "(", "self", ".", "_serialization_dir", ",", "\"vocabulary\"", ")", "\n", ")", "\n", "\n", "checkpointer", "=", "Checkpointer", "(", "self", ".", "_serialization_dir", ")", "\n", "checkpointer", ".", "save_checkpoint", "(", "\n", "epoch", "=", "0", ",", "\n", "model_state", "=", "self", ".", "model", ".", "state_dict", "(", ")", ",", "\n", "training_states", "=", "{", "}", ",", "\n", "is_best_so_far", "=", "True", ",", "\n", ")", "\n", "return", "{", "}", "\n", "", "", ""]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.training.util.sparse_clip_norm": [[32, 76], ["list", "float", "float", "filter", "float", "max", "p.grad.data.abs().max", "p.grad.data.coalesce", "p.grad.data.coalesce._values().norm", "p.grad.data.norm", "p.grad.data._values().mul_", "p.grad.data.mul_", "p.grad.data.abs", "p.grad.data.coalesce._values", "p.grad.data._values"], "function", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.util.WorkerLogFilter.filter"], ["\n", "### Charge default iterator ###", "\n", "iterators_params", "=", "params", ".", "pop", "(", "\"iterators\"", ")", "\n", "\n", "default_iterator_params", "=", "iterators_params", ".", "pop", "(", "\"iterator\"", ")", "\n", "default_iterator", "=", "DataIterator", ".", "from_params", "(", "default_iterator_params", ")", "\n", "default_iterator", ".", "index_with", "(", "vocab", ")", "\n", "\n", "### Charge dataset specific iterators ###", "\n", "for", "task", "in", "task_list", ":", "\n", "        ", "specific_iterator_params", "=", "iterators_params", ".", "pop", "(", "\"iterator_\"", "+", "task", ".", "_name", ",", "None", ")", "\n", "if", "specific_iterator_params", "is", "not", "None", ":", "\n", "            ", "specific_iterator", "=", "DataIterator", ".", "from_params", "(", "specific_iterator_params", ")", "\n", "specific_iterator", ".", "index_with", "(", "vocab", ")", "\n", "task", ".", "set_data_iterator", "(", "specific_iterator", ")", "\n", "", "else", ":", "\n", "            ", "task", ".", "set_data_iterator", "(", "default_iterator", ")", "\n", "\n", "", "", "return", "task_list", "\n", "", ""]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.training.util.move_optimizer_to_cuda": [[78, 91], ["param_state.keys", "isinstance", "param_state[].cuda", "param.get_device"], "function", ["None"], []], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.training.util.get_batch_size": [[93, 104], ["isinstance", "batch.size", "isinstance", "util.get_batch_size", "next", "iter", "batch.values"], "function", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.training.util.get_batch_size"], []], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.training.util.time_to_str": [[106, 118], ["datetime.datetime.fromtimestamp"], "function", ["None"], []], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.training.util.str_to_time": [[121, 127], ["datetime.datetime", "int", "time_str.split"], "function", ["None"], []], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.training.util.read_all_datasets": [[129, 160], ["logger.info", "dataset_reader.read", "logger.info", "validation_dataset_reader.read", "logger.info", "validation_dataset_reader.read"], "function", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.dataset_readers.multiprocess_dataset_reader.logger.info", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.scripts.convert_openie_to_conll.read", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.dataset_readers.multiprocess_dataset_reader.logger.info", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.scripts.convert_openie_to_conll.read", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.dataset_readers.multiprocess_dataset_reader.logger.info", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.scripts.convert_openie_to_conll.read"], []], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.training.util.datasets_from_params": [[162, 221], ["params.pop", "params.pop", "allennlp.data.dataset_readers.DatasetReader.from_params", "params.pop", "logger.info", "DatasetReader.from_params.read", "params.pop", "params.pop", "logger.info", "allennlp.data.dataset_readers.DatasetReader.from_params", "logger.info", "DatasetReader.from_params.read", "logger.info", "DatasetReader.from_params.read"], "function", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.params.Params.pop", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.params.Params.pop", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.tokenizers.tokenizer.Tokenizer.from_params", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.params.Params.pop", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.dataset_readers.multiprocess_dataset_reader.logger.info", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.scripts.convert_openie_to_conll.read", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.params.Params.pop", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.params.Params.pop", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.dataset_readers.multiprocess_dataset_reader.logger.info", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.tokenizers.tokenizer.Tokenizer.from_params", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.dataset_readers.multiprocess_dataset_reader.logger.info", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.scripts.convert_openie_to_conll.read", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.dataset_readers.multiprocess_dataset_reader.logger.info", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.scripts.convert_openie_to_conll.read"], []], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.training.util.create_serialization_dir": [[223, 301], ["allennlp.common.checks.ConfigurationError", "os.path.exists", "shutil.rmtree", "os.path.exists", "os.listdir", "logger.info", "os.path.join", "allennlp.common.params.Params.from_file", "params.as_flat_dict", "Params.from_file.as_flat_dict", "params.as_flat_dict.keys", "os.makedirs", "allennlp.common.checks.ConfigurationError", "os.path.exists", "allennlp.common.checks.ConfigurationError", "params.as_flat_dict.keys", "loaded_params.as_flat_dict.keys", "logger.error", "loaded_params.as_flat_dict.keys", "params.as_flat_dict.keys", "logger.error", "allennlp.common.checks.ConfigurationError", "allennlp.common.checks.ConfigurationError", "params.as_flat_dict.get", "loaded_params.as_flat_dict.get", "logger.error"], "function", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.dataset_readers.multiprocess_dataset_reader.logger.info", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.dataset_readers.multiprocess_dataset_reader.QIterable.join", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.params.Params.from_file", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.params.Params.as_flat_dict", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.params.Params.as_flat_dict", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.params.Params.get", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.params.Params.get"], []], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.training.util.enable_gradient_clipping": [[303, 310], ["model.parameters", "parameter.register_hook", "allennlp.nn.util.clamp_tensor"], "function", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.nn.util.clamp_tensor"], []], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.training.util.rescale_gradients": [[314, 324], ["util.sparse_clip_norm", "model.parameters"], "function", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.training.util.sparse_clip_norm"], []], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.training.util.get_metrics": [[326, 358], ["model.get_metrics", "float", "model.get_metrics.items", "isinstance", "torch.all_reduce", "torch.tensor().to", "torch.tensor().to", "torch.tensor().to", "torch.tensor().to", "torch.tensor().to.item", "torch.device", "torch.device", "torch.device", "torch.device", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor"], "function", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.coreference_resolution.coref.CoreferenceResolver.get_metrics"], []], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.training.util.evaluate": [[360, 437], ["allennlp.common.checks.check_for_gpu", "torch.no_grad", "torch.no_grad", "model.eval", "data_iterator", "logger.info", "allennlp.common.tqdm.Tqdm.tqdm", "model.get_metrics", "allennlp.nn.util.move_to_device", "model", "model.get", "model.get_metrics", "Tqdm.tqdm.set_description", "data_iterator.get_num_batches", "any", "logger.warning", "RuntimeError", "output_dict[].item", "output_dict.get.item", "metric_name.startswith", "model.get_metrics.items", "name.startswith"], "function", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.checks.check_for_gpu", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.dataset_readers.multiprocess_dataset_reader.logger.info", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.tqdm.Tqdm.tqdm", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.coreference_resolution.coref.CoreferenceResolver.get_metrics", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.nn.util.move_to_device", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.params.Params.get", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.coreference_resolution.coref.CoreferenceResolver.get_metrics", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.iterators.data_iterator.DataIterator.get_num_batches"], []], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.training.util.description_from_metrics": [[439, 457], ["any", "logger.warning", "metric_name.startswith", "metrics.items", "name.startswith"], "function", ["None"], []], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.training.util.make_vocab_from_params": [[460, 499], ["params.pop", "os.makedirs", "os.path.join", "util.datasets_from_params", "set", "logger.info", "allennlp.data.Vocabulary.from_params", "logger.info", "Vocabulary.from_params.save_to_files", "logger.info", "os.path.isdir", "allennlp.common.checks.ConfigurationError", "params.pop", "os.listdir", "allennlp.common.checks.ConfigurationError", "datasets_from_params.items"], "function", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.params.Params.pop", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.dataset_readers.multiprocess_dataset_reader.QIterable.join", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.training.util.datasets_from_params", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.dataset_readers.multiprocess_dataset_reader.logger.info", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.tokenizers.tokenizer.Tokenizer.from_params", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.dataset_readers.multiprocess_dataset_reader.logger.info", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.data.vocabulary.Vocabulary.save_to_files", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.dataset_readers.multiprocess_dataset_reader.logger.info", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.params.Params.pop"], []], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.training.checkpointer.Checkpointer.__init__": [[26, 40], ["time.time"], "methods", ["None"], ["def", "__init__", "(", "\n", "self", ",", "\n", "serialization_dir", ":", "str", "=", "None", ",", "\n", "keep_serialized_model_every_num_seconds", ":", "int", "=", "None", ",", "\n", "num_serialized_models_to_keep", ":", "int", "=", "20", ",", "\n", ")", "->", "None", ":", "\n", "        ", "self", ".", "_serialization_dir", "=", "serialization_dir", "\n", "self", ".", "_keep_serialized_model_every_num_seconds", "=", "(", "\n", "keep_serialized_model_every_num_seconds", "\n", ")", "\n", "self", ".", "_num_serialized_models_to_keep", "=", "num_serialized_models_to_keep", "\n", "\n", "self", ".", "_last_permanent_saved_checkpoint_time", "=", "time", ".", "time", "(", ")", "\n", "self", ".", "_serialized_paths", ":", "List", "[", "Tuple", "[", "float", ",", "str", ",", "str", "]", "]", "=", "[", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.training.checkpointer.Checkpointer.save_checkpoint": [[41, 94], ["os.path.join", "torch.save", "os.path.join", "torch.save", "logger.info", "shutil.copyfile", "checkpointer.Checkpointer._serialized_paths.append", "os.path.join", "len", "checkpointer.Checkpointer._serialized_paths.pop", "time.time", "os.path.isfile", "os.remove"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.dataset_readers.multiprocess_dataset_reader.QIterable.join", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.None.combine_data.save", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.dataset_readers.multiprocess_dataset_reader.QIterable.join", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.None.combine_data.save", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.dataset_readers.multiprocess_dataset_reader.logger.info", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.dataset_readers.multiprocess_dataset_reader.QIterable.join", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.params.Params.pop"], ["", "def", "save_checkpoint", "(", "\n", "self", ",", "\n", "epoch", ":", "Union", "[", "int", ",", "str", "]", ",", "\n", "model_state", ":", "Dict", "[", "str", ",", "Any", "]", ",", "\n", "training_states", ":", "Dict", "[", "str", ",", "Any", "]", ",", "\n", "is_best_so_far", ":", "bool", ",", "\n", ")", "->", "None", ":", "\n", "        ", "if", "self", ".", "_serialization_dir", "is", "not", "None", ":", "\n", "            ", "model_path", "=", "os", ".", "path", ".", "join", "(", "\n", "self", ".", "_serialization_dir", ",", "\"model_state_epoch_{}.th\"", ".", "format", "(", "epoch", ")", "\n", ")", "\n", "torch", ".", "save", "(", "model_state", ",", "model_path", ")", "\n", "training_path", "=", "os", ".", "path", ".", "join", "(", "\n", "self", ".", "_serialization_dir", ",", "\"training_state_epoch_{}.th\"", ".", "format", "(", "epoch", ")", "\n", ")", "\n", "torch", ".", "save", "(", "{", "**", "training_states", ",", "\"epoch\"", ":", "epoch", "}", ",", "training_path", ")", "\n", "\n", "if", "is_best_so_far", ":", "\n", "                ", "logger", ".", "info", "(", "\n", "\"Best validation performance so far. Copying weights to '%s/best.th'.\"", ",", "\n", "self", ".", "_serialization_dir", ",", "\n", ")", "\n", "shutil", ".", "copyfile", "(", "\n", "model_path", ",", "os", ".", "path", ".", "join", "(", "self", ".", "_serialization_dir", ",", "\"best.th\"", ")", "\n", ")", "\n", "\n", "", "if", "(", "\n", "self", ".", "_num_serialized_models_to_keep", "is", "not", "None", "\n", "and", "self", ".", "_num_serialized_models_to_keep", ">=", "0", "\n", ")", ":", "\n", "                ", "self", ".", "_serialized_paths", ".", "append", "(", "(", "time", ".", "time", "(", ")", ",", "model_path", ",", "training_path", ")", ")", "\n", "if", "len", "(", "self", ".", "_serialized_paths", ")", ">", "self", ".", "_num_serialized_models_to_keep", ":", "\n", "                    ", "paths_to_remove", "=", "self", ".", "_serialized_paths", ".", "pop", "(", "0", ")", "\n", "# Check to see if we should keep this checkpoint, if it has been longer", "\n", "# then self._keep_serialized_model_every_num_seconds since the last", "\n", "# kept checkpoint.", "\n", "remove_path", "=", "True", "\n", "if", "self", ".", "_keep_serialized_model_every_num_seconds", "is", "not", "None", ":", "\n", "                        ", "save_time", "=", "paths_to_remove", "[", "0", "]", "\n", "time_since_checkpoint_kept", "=", "(", "\n", "save_time", "-", "self", ".", "_last_permanent_saved_checkpoint_time", "\n", ")", "\n", "if", "(", "\n", "time_since_checkpoint_kept", "\n", ">", "self", ".", "_keep_serialized_model_every_num_seconds", "\n", ")", ":", "\n", "# We want to keep this checkpoint.", "\n", "                            ", "remove_path", "=", "False", "\n", "self", ".", "_last_permanent_saved_checkpoint_time", "=", "save_time", "\n", "", "", "if", "remove_path", ":", "\n", "                        ", "for", "fname", "in", "paths_to_remove", "[", "1", ":", "]", ":", "\n", "                            ", "if", "os", ".", "path", ".", "isfile", "(", "fname", ")", ":", "\n", "                                ", "os", ".", "remove", "(", "fname", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.training.checkpointer.Checkpointer.find_latest_checkpoint": [[95, 139], ["os.listdir", "os.path.join", "os.path.join", "any", "re.search().group", "epoch.split", "sorted", "str", "len", "int_epochs.append", "int_epochs.append", "re.search", "os.listdir", "int", "int"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.dataset_readers.multiprocess_dataset_reader.QIterable.join", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.dataset_readers.multiprocess_dataset_reader.QIterable.join", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.nn.beam_search.BeamSearch.search"], ["", "", "", "", "", "", "", "def", "find_latest_checkpoint", "(", "self", ")", "->", "Tuple", "[", "str", ",", "str", "]", ":", "\n", "        ", "\"\"\"\n        Return the location of the latest model and training state files.\n        If there isn't a valid checkpoint then return None.\n        \"\"\"", "\n", "have_checkpoint", "=", "self", ".", "_serialization_dir", "is", "not", "None", "and", "any", "(", "\n", "\"model_state_epoch_\"", "in", "x", "for", "x", "in", "os", ".", "listdir", "(", "self", ".", "_serialization_dir", ")", "\n", ")", "\n", "\n", "if", "not", "have_checkpoint", ":", "\n", "            ", "return", "None", "\n", "\n", "", "serialization_files", "=", "os", ".", "listdir", "(", "self", ".", "_serialization_dir", ")", "\n", "model_checkpoints", "=", "[", "x", "for", "x", "in", "serialization_files", "if", "\"model_state_epoch\"", "in", "x", "]", "\n", "# Get the last checkpoint file.  Epochs are specified as either an", "\n", "# int (for end of epoch files) or with epoch and timestamp for", "\n", "# within epoch checkpoints, e.g. 5.2018-02-02-15-33-42", "\n", "found_epochs", "=", "[", "\n", "re", ".", "search", "(", "r\"model_state_epoch_([0-9\\.\\-]+)\\.th\"", ",", "x", ")", ".", "group", "(", "1", ")", "\n", "for", "x", "in", "model_checkpoints", "\n", "]", "\n", "int_epochs", ":", "Any", "=", "[", "]", "\n", "for", "epoch", "in", "found_epochs", ":", "\n", "            ", "pieces", "=", "epoch", ".", "split", "(", "\".\"", ")", "\n", "if", "len", "(", "pieces", ")", "==", "1", ":", "\n", "# Just a single epoch without timestamp", "\n", "                ", "int_epochs", ".", "append", "(", "[", "int", "(", "pieces", "[", "0", "]", ")", ",", "\"0\"", "]", ")", "\n", "", "else", ":", "\n", "# has a timestamp", "\n", "                ", "int_epochs", ".", "append", "(", "[", "int", "(", "pieces", "[", "0", "]", ")", ",", "pieces", "[", "1", "]", "]", ")", "\n", "", "", "last_epoch", "=", "sorted", "(", "int_epochs", ",", "reverse", "=", "True", ")", "[", "0", "]", "\n", "if", "last_epoch", "[", "1", "]", "==", "\"0\"", ":", "\n", "            ", "epoch_to_load", "=", "str", "(", "last_epoch", "[", "0", "]", ")", "\n", "", "else", ":", "\n", "            ", "epoch_to_load", "=", "\"{0}.{1}\"", ".", "format", "(", "last_epoch", "[", "0", "]", ",", "last_epoch", "[", "1", "]", ")", "\n", "\n", "", "model_path", "=", "os", ".", "path", ".", "join", "(", "\n", "self", ".", "_serialization_dir", ",", "\"model_state_epoch_{}.th\"", ".", "format", "(", "epoch_to_load", ")", "\n", ")", "\n", "training_state_path", "=", "os", ".", "path", ".", "join", "(", "\n", "self", ".", "_serialization_dir", ",", "\"training_state_epoch_{}.th\"", ".", "format", "(", "epoch_to_load", ")", "\n", ")", "\n", "\n", "return", "(", "model_path", ",", "training_state_path", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.training.checkpointer.Checkpointer.restore_checkpoint": [[140, 174], ["checkpointer.Checkpointer.find_latest_checkpoint", "torch.load", "torch.load", "allennlp.nn.util.device_mapping", "allennlp.nn.util.device_mapping"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.training.checkpointer.Checkpointer.find_latest_checkpoint", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.token_embedders.bert_token_embedder.PretrainedBertModel.load", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.token_embedders.bert_token_embedder.PretrainedBertModel.load", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.nn.util.device_mapping", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.nn.util.device_mapping"], ["", "def", "restore_checkpoint", "(", "self", ")", "->", "Tuple", "[", "Dict", "[", "str", ",", "Any", "]", ",", "Dict", "[", "str", ",", "Any", "]", "]", ":", "\n", "        ", "\"\"\"\n        Restores a model from a serialization_dir to the last saved checkpoint.\n        This includes a training state (typically consisting of an epoch count and optimizer state),\n        which is serialized separately from  model parameters. This function should only be used to\n        continue training - if you wish to load a model for inference/load parts of a model into a new\n        computation graph, you should use the native Pytorch functions:\n        ` model.load_state_dict(torch.load(\"/path/to/model/weights.th\"))`\n\n        If `self._serialization_dir` does not exist or does not contain any checkpointed weights,\n        this function will do nothing and return empty dicts.\n\n        # Returns\n\n        states: Tuple[Dict[str, Any], Dict[str, Any]]\n            The model state and the training state.\n        \"\"\"", "\n", "latest_checkpoint", "=", "self", ".", "find_latest_checkpoint", "(", ")", "\n", "\n", "if", "latest_checkpoint", "is", "None", ":", "\n", "# No checkpoint to restore, start at 0", "\n", "            ", "return", "{", "}", ",", "{", "}", "\n", "\n", "", "model_path", ",", "training_state_path", "=", "latest_checkpoint", "\n", "\n", "# Load the parameters onto CPU, then transfer to GPU.", "\n", "# This avoids potential OOM on GPU for large models that", "\n", "# load parameters onto GPU then make a new GPU copy into the parameter", "\n", "# buffer. The GPU transfer happens implicitly in load_state_dict.", "\n", "model_state", "=", "torch", ".", "load", "(", "model_path", ",", "map_location", "=", "nn_util", ".", "device_mapping", "(", "-", "1", ")", ")", "\n", "training_state", "=", "torch", ".", "load", "(", "\n", "training_state_path", ",", "map_location", "=", "nn_util", ".", "device_mapping", "(", "-", "1", ")", "\n", ")", "\n", "return", "model_state", ",", "training_state", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.training.checkpointer.Checkpointer.best_model_state": [[175, 188], ["logger.info", "os.path.join", "torch.load", "logger.info", "allennlp.nn.util.device_mapping"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.dataset_readers.multiprocess_dataset_reader.logger.info", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.dataset_readers.multiprocess_dataset_reader.QIterable.join", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.token_embedders.bert_token_embedder.PretrainedBertModel.load", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.dataset_readers.multiprocess_dataset_reader.logger.info", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.nn.util.device_mapping"], ["", "def", "best_model_state", "(", "self", ")", "->", "Dict", "[", "str", ",", "Any", "]", ":", "\n", "        ", "if", "self", ".", "_serialization_dir", ":", "\n", "            ", "logger", ".", "info", "(", "\"loading best weights\"", ")", "\n", "best_model_state_path", "=", "os", ".", "path", ".", "join", "(", "self", ".", "_serialization_dir", ",", "\"best.th\"", ")", "\n", "return", "torch", ".", "load", "(", "\n", "best_model_state_path", ",", "map_location", "=", "nn_util", ".", "device_mapping", "(", "-", "1", ")", "\n", ")", "\n", "", "else", ":", "\n", "            ", "logger", ".", "info", "(", "\n", "\"cannot load best weights without `serialization_dir`, \"", "\n", "\"so you're just getting the last weights\"", "\n", ")", "\n", "return", "{", "}", "\n", "", "", "", ""]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.training.optimizers.Optimizer.default": [[156, 160], ["Optimizer.from_params", "allennlp.common.Params"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.tokenizers.tokenizer.Tokenizer.from_params"], ["@", "staticmethod", "\n", "def", "default", "(", "model_parameters", ":", "List", ")", "->", "\"Optimizer\"", ":", "\n", "        ", "return", "Optimizer", ".", "from_params", "(", "\n", "model_parameters", "=", "model_parameters", ",", "params", "=", "Params", "(", "{", "}", ")", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.training.optimizers.AdamOptimizer.__init__": [[165, 182], ["super().__init__", "optimizers.make_parameter_groups"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.graph_util.plot.GraphPlot.__init__", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.training.optimizers.make_parameter_groups"], ["    ", "def", "__init__", "(", "\n", "self", ",", "\n", "model_parameters", ":", "List", "[", "Tuple", "[", "str", ",", "torch", ".", "nn", ".", "Parameter", "]", "]", ",", "\n", "parameter_groups", ":", "List", "[", "Tuple", "[", "List", "[", "str", "]", ",", "Dict", "[", "str", ",", "Any", "]", "]", "]", "=", "None", ",", "\n", "lr", ":", "float", "=", "0.001", ",", "\n", "betas", ":", "Tuple", "[", "float", ",", "float", "]", "=", "(", "0.9", ",", "0.999", ")", ",", "\n", "eps", ":", "float", "=", "1e-08", ",", "\n", "weight_decay", ":", "float", "=", "0.0", ",", "\n", "amsgrad", ":", "bool", "=", "False", ",", "\n", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "\n", "params", "=", "make_parameter_groups", "(", "model_parameters", ",", "parameter_groups", ")", ",", "\n", "lr", "=", "lr", ",", "\n", "betas", "=", "betas", ",", "\n", "eps", "=", "eps", ",", "\n", "weight_decay", "=", "weight_decay", ",", "\n", "amsgrad", "=", "amsgrad", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.training.optimizers.SparseAdamOptimizer.__init__": [[187, 200], ["super().__init__", "optimizers.make_parameter_groups"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.graph_util.plot.GraphPlot.__init__", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.training.optimizers.make_parameter_groups"], ["    ", "def", "__init__", "(", "\n", "self", ",", "\n", "model_parameters", ":", "List", "[", "Tuple", "[", "str", ",", "torch", ".", "nn", ".", "Parameter", "]", "]", ",", "\n", "parameter_groups", ":", "List", "[", "Tuple", "[", "List", "[", "str", "]", ",", "Dict", "[", "str", ",", "Any", "]", "]", "]", "=", "None", ",", "\n", "lr", ":", "float", "=", "0.001", ",", "\n", "betas", ":", "Tuple", "[", "float", ",", "float", "]", "=", "(", "0.9", ",", "0.999", ")", ",", "\n", "eps", ":", "float", "=", "1e-08", ",", "\n", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "\n", "params", "=", "make_parameter_groups", "(", "model_parameters", ",", "parameter_groups", ")", ",", "\n", "lr", "=", "lr", ",", "\n", "betas", "=", "betas", ",", "\n", "eps", "=", "eps", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.training.optimizers.AdamaxOptimizer.__init__": [[205, 220], ["super().__init__", "optimizers.make_parameter_groups"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.graph_util.plot.GraphPlot.__init__", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.training.optimizers.make_parameter_groups"], ["    ", "def", "__init__", "(", "\n", "self", ",", "\n", "model_parameters", ":", "List", "[", "Tuple", "[", "str", ",", "torch", ".", "nn", ".", "Parameter", "]", "]", ",", "\n", "parameter_groups", ":", "List", "[", "Tuple", "[", "List", "[", "str", "]", ",", "Dict", "[", "str", ",", "Any", "]", "]", "]", "=", "None", ",", "\n", "lr", ":", "float", "=", "0.002", ",", "\n", "betas", ":", "Tuple", "[", "float", ",", "float", "]", "=", "(", "0.9", ",", "0.999", ")", ",", "\n", "eps", ":", "float", "=", "1e-08", ",", "\n", "weight_decay", ":", "float", "=", "0.0", ",", "\n", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "\n", "params", "=", "make_parameter_groups", "(", "model_parameters", ",", "parameter_groups", ")", ",", "\n", "lr", "=", "lr", ",", "\n", "betas", "=", "betas", ",", "\n", "eps", "=", "eps", ",", "\n", "weight_decay", "=", "weight_decay", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.training.optimizers.AdamWOptimizer.__init__": [[225, 242], ["super().__init__", "optimizers.make_parameter_groups"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.graph_util.plot.GraphPlot.__init__", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.training.optimizers.make_parameter_groups"], ["    ", "def", "__init__", "(", "\n", "self", ",", "\n", "model_parameters", ":", "List", "[", "Tuple", "[", "str", ",", "torch", ".", "nn", ".", "Parameter", "]", "]", ",", "\n", "parameter_groups", ":", "List", "[", "Tuple", "[", "List", "[", "str", "]", ",", "Dict", "[", "str", ",", "Any", "]", "]", "]", "=", "None", ",", "\n", "lr", ":", "float", "=", "0.001", ",", "\n", "betas", ":", "Tuple", "[", "float", ",", "float", "]", "=", "(", "0.9", ",", "0.999", ")", ",", "\n", "eps", ":", "float", "=", "1e-08", ",", "\n", "weight_decay", ":", "float", "=", "0.01", ",", "\n", "amsgrad", ":", "bool", "=", "False", ",", "\n", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "\n", "params", "=", "make_parameter_groups", "(", "model_parameters", ",", "parameter_groups", ")", ",", "\n", "lr", "=", "lr", ",", "\n", "betas", "=", "betas", ",", "\n", "eps", "=", "eps", ",", "\n", "weight_decay", "=", "weight_decay", ",", "\n", "amsgrad", "=", "amsgrad", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.training.optimizers.HuggingfaceAdamWOptimizer.__init__": [[247, 264], ["super().__init__", "optimizers.make_parameter_groups"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.graph_util.plot.GraphPlot.__init__", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.training.optimizers.make_parameter_groups"], ["    ", "def", "__init__", "(", "\n", "self", ",", "\n", "model_parameters", ":", "List", "[", "Tuple", "[", "str", ",", "torch", ".", "nn", ".", "Parameter", "]", "]", ",", "\n", "parameter_groups", ":", "List", "[", "Tuple", "[", "List", "[", "str", "]", ",", "Dict", "[", "str", ",", "Any", "]", "]", "]", "=", "None", ",", "\n", "lr", ":", "float", "=", "0.001", ",", "\n", "betas", ":", "Tuple", "[", "float", ",", "float", "]", "=", "(", "0.9", ",", "0.999", ")", ",", "\n", "eps", ":", "float", "=", "1e-06", ",", "\n", "weight_decay", ":", "float", "=", "0.0", ",", "\n", "correct_bias", ":", "bool", "=", "False", ",", "\n", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "\n", "params", "=", "make_parameter_groups", "(", "model_parameters", ",", "parameter_groups", ")", ",", "\n", "lr", "=", "lr", ",", "\n", "betas", "=", "betas", ",", "\n", "eps", "=", "eps", ",", "\n", "weight_decay", "=", "weight_decay", ",", "\n", "correct_bias", "=", "correct_bias", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.training.optimizers.AdagradOptimizer.__init__": [[269, 286], ["super().__init__", "optimizers.make_parameter_groups"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.graph_util.plot.GraphPlot.__init__", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.training.optimizers.make_parameter_groups"], ["    ", "def", "__init__", "(", "\n", "self", ",", "\n", "model_parameters", ":", "List", "[", "Tuple", "[", "str", ",", "torch", ".", "nn", ".", "Parameter", "]", "]", ",", "\n", "parameter_groups", ":", "List", "[", "Tuple", "[", "List", "[", "str", "]", ",", "Dict", "[", "str", ",", "Any", "]", "]", "]", "=", "None", ",", "\n", "lr", ":", "float", "=", "0.01", ",", "\n", "lr_decay", ":", "float", "=", "0.0", ",", "\n", "weight_decay", ":", "float", "=", "0.0", ",", "\n", "initial_accumulator_value", ":", "float", "=", "0.0", ",", "\n", "eps", ":", "float", "=", "1e-10", ",", "\n", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "\n", "params", "=", "make_parameter_groups", "(", "model_parameters", ",", "parameter_groups", ")", ",", "\n", "lr", "=", "lr", ",", "\n", "lr_decay", "=", "lr_decay", ",", "\n", "weight_decay", "=", "weight_decay", ",", "\n", "initial_accumulator_value", "=", "initial_accumulator_value", ",", "\n", "eps", "=", "eps", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.training.optimizers.AdadeltaOptimizer.__init__": [[291, 306], ["super().__init__", "optimizers.make_parameter_groups"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.graph_util.plot.GraphPlot.__init__", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.training.optimizers.make_parameter_groups"], ["    ", "def", "__init__", "(", "\n", "self", ",", "\n", "model_parameters", ":", "List", "[", "Tuple", "[", "str", ",", "torch", ".", "nn", ".", "Parameter", "]", "]", ",", "\n", "parameter_groups", ":", "List", "[", "Tuple", "[", "List", "[", "str", "]", ",", "Dict", "[", "str", ",", "Any", "]", "]", "]", "=", "None", ",", "\n", "lr", ":", "float", "=", "1.0", ",", "\n", "rho", ":", "float", "=", "0.9", ",", "\n", "eps", ":", "float", "=", "1e-06", ",", "\n", "weight_decay", ":", "float", "=", "0.0", ",", "\n", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "\n", "params", "=", "make_parameter_groups", "(", "model_parameters", ",", "parameter_groups", ")", ",", "\n", "lr", "=", "lr", ",", "\n", "rho", "=", "rho", ",", "\n", "eps", "=", "eps", ",", "\n", "weight_decay", "=", "weight_decay", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.training.optimizers.SgdOptimizer.__init__": [[311, 328], ["super().__init__", "optimizers.make_parameter_groups"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.graph_util.plot.GraphPlot.__init__", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.training.optimizers.make_parameter_groups"], ["    ", "def", "__init__", "(", "\n", "self", ",", "\n", "model_parameters", ":", "List", "[", "Tuple", "[", "str", ",", "torch", ".", "nn", ".", "Parameter", "]", "]", ",", "\n", "lr", ":", "float", ",", "\n", "parameter_groups", ":", "List", "[", "Tuple", "[", "List", "[", "str", "]", ",", "Dict", "[", "str", ",", "Any", "]", "]", "]", "=", "None", ",", "\n", "momentum", ":", "float", "=", "0.0", ",", "\n", "dampening", ":", "float", "=", "0", ",", "\n", "weight_decay", ":", "float", "=", "0.0", ",", "\n", "nesterov", ":", "bool", "=", "False", ",", "\n", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "\n", "params", "=", "make_parameter_groups", "(", "model_parameters", ",", "parameter_groups", ")", ",", "\n", "lr", "=", "lr", ",", "\n", "momentum", "=", "momentum", ",", "\n", "dampening", "=", "dampening", ",", "\n", "weight_decay", "=", "weight_decay", ",", "\n", "nesterov", "=", "nesterov", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.training.optimizers.RmsPropOptimizer.__init__": [[333, 352], ["super().__init__", "optimizers.make_parameter_groups"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.graph_util.plot.GraphPlot.__init__", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.training.optimizers.make_parameter_groups"], ["    ", "def", "__init__", "(", "\n", "self", ",", "\n", "model_parameters", ":", "List", "[", "Tuple", "[", "str", ",", "torch", ".", "nn", ".", "Parameter", "]", "]", ",", "\n", "parameter_groups", ":", "List", "[", "Tuple", "[", "List", "[", "str", "]", ",", "Dict", "[", "str", ",", "Any", "]", "]", "]", "=", "None", ",", "\n", "lr", ":", "float", "=", "0.01", ",", "\n", "alpha", ":", "float", "=", "0.99", ",", "\n", "eps", ":", "float", "=", "1e-08", ",", "\n", "weight_decay", ":", "float", "=", "0.0", ",", "\n", "momentum", ":", "float", "=", "0.0", ",", "\n", "centered", ":", "bool", "=", "False", ",", "\n", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "\n", "params", "=", "make_parameter_groups", "(", "model_parameters", ",", "parameter_groups", ")", ",", "\n", "lr", "=", "lr", ",", "\n", "alpha", "=", "alpha", ",", "\n", "eps", "=", "eps", ",", "\n", "weight_decay", "=", "weight_decay", ",", "\n", "momentum", "=", "momentum", ",", "\n", "centered", "=", "centered", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.training.optimizers.AveragedSgdOptimizer.__init__": [[357, 374], ["super().__init__", "optimizers.make_parameter_groups"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.graph_util.plot.GraphPlot.__init__", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.training.optimizers.make_parameter_groups"], ["    ", "def", "__init__", "(", "\n", "self", ",", "\n", "model_parameters", ":", "List", "[", "Tuple", "[", "str", ",", "torch", ".", "nn", ".", "Parameter", "]", "]", ",", "\n", "parameter_groups", ":", "List", "[", "Tuple", "[", "List", "[", "str", "]", ",", "Dict", "[", "str", ",", "Any", "]", "]", "]", "=", "None", ",", "\n", "lr", ":", "float", "=", "0.01", ",", "\n", "lambd", ":", "float", "=", "0.0001", ",", "\n", "alpha", ":", "float", "=", "0.75", ",", "\n", "t0", ":", "float", "=", "1000000.0", ",", "\n", "weight_decay", ":", "float", "=", "0.0", ",", "\n", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "\n", "params", "=", "make_parameter_groups", "(", "model_parameters", ",", "parameter_groups", ")", ",", "\n", "lr", "=", "lr", ",", "\n", "lambd", "=", "lambd", ",", "\n", "alpha", "=", "alpha", ",", "\n", "t0", "=", "t0", ",", "\n", "weight_decay", "=", "weight_decay", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.training.optimizers.DenseSparseAdam.__init__": [[399, 418], ["dict", "super().__init__", "ValueError", "ValueError", "ValueError", "ValueError", "optimizers.make_parameter_groups"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.graph_util.plot.GraphPlot.__init__", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.training.optimizers.make_parameter_groups"], ["def", "__init__", "(", "\n", "self", ",", "\n", "model_parameters", ":", "List", "[", "Tuple", "[", "str", ",", "torch", ".", "nn", ".", "Parameter", "]", "]", ",", "\n", "parameter_groups", ":", "List", "[", "Tuple", "[", "List", "[", "str", "]", ",", "Dict", "[", "str", ",", "Any", "]", "]", "]", "=", "None", ",", "\n", "lr", "=", "1e-3", ",", "\n", "betas", "=", "(", "0.9", ",", "0.999", ")", ",", "\n", "eps", "=", "1e-8", ",", "\n", ")", ":", "\n", "        ", "if", "not", "0.0", "<=", "lr", ":", "\n", "            ", "raise", "ValueError", "(", "\"Invalid learning rate: {}\"", ".", "format", "(", "lr", ")", ")", "\n", "", "if", "not", "0.0", "<=", "eps", ":", "\n", "            ", "raise", "ValueError", "(", "\"Invalid epsilon value: {}\"", ".", "format", "(", "eps", ")", ")", "\n", "", "if", "not", "0.0", "<=", "betas", "[", "0", "]", "<", "1.0", ":", "\n", "            ", "raise", "ValueError", "(", "\"Invalid beta parameter at index 0: {}\"", ".", "format", "(", "betas", "[", "0", "]", ")", ")", "\n", "", "if", "not", "0.0", "<=", "betas", "[", "1", "]", "<", "1.0", ":", "\n", "            ", "raise", "ValueError", "(", "\"Invalid beta parameter at index 1: {}\"", ".", "format", "(", "betas", "[", "1", "]", ")", ")", "\n", "", "defaults", "=", "dict", "(", "lr", "=", "lr", ",", "betas", "=", "betas", ",", "eps", "=", "eps", ")", "\n", "super", "(", ")", ".", "__init__", "(", "\n", "make_parameter_groups", "(", "model_parameters", ",", "parameter_groups", ")", ",", "defaults", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.training.optimizers.DenseSparseAdam.step": [[420, 511], ["closure", "len", "torch.zeros_like", "torch.zeros_like", "grad.coalesce.coalesce.coalesce", "grad.coalesce.coalesce._indices", "grad.coalesce.coalesce._values", "grad.coalesce.coalesce.size", "exp_avg.sparse_mask()._values", "grad.coalesce._values.sub().mul_", "exp_avg.add_", "exp_avg_sq.sparse_mask()._values", "grad.coalesce._values.pow().sub_().mul_", "exp_avg_sq.add_", "grad._values.sub().mul_.add_", "grad._values.pow().sub_().mul_.add_", "grad._values.pow().sub_().mul_.sqrt_().add_", "p.data.add_", "exp_avg.mul_().add_", "exp_avg_sq.mul_().addcmul_", "exp_avg_sq.sqrt().add_", "p.data.addcdiv_", "constructor", "optimizers.DenseSparseAdam.step.make_sparse"], "methods", ["None"], ["", "def", "step", "(", "self", ",", "closure", "=", "None", ")", ":", "\n", "        ", "\"\"\"\n        Performs a single optimization step.\n\n        # Parameters\n\n        closure : `callable`, optional.\n            A closure that reevaluates the model and returns the loss.\n        \"\"\"", "\n", "loss", "=", "None", "\n", "if", "closure", "is", "not", "None", ":", "\n", "            ", "loss", "=", "closure", "(", ")", "\n", "\n", "", "for", "group", "in", "self", ".", "param_groups", ":", "\n", "            ", "for", "p", "in", "group", "[", "\"params\"", "]", ":", "\n", "                ", "if", "p", ".", "grad", "is", "None", ":", "\n", "                    ", "continue", "\n", "", "grad", "=", "p", ".", "grad", ".", "data", "\n", "\n", "state", "=", "self", ".", "state", "[", "p", "]", "\n", "\n", "# State initialization", "\n", "if", "len", "(", "state", ")", "==", "0", ":", "\n", "                    ", "state", "[", "\"step\"", "]", "=", "0", "\n", "# Exponential moving average of gradient values", "\n", "state", "[", "\"exp_avg\"", "]", "=", "torch", ".", "zeros_like", "(", "p", ".", "data", ")", "\n", "# Exponential moving average of squared gradient values", "\n", "state", "[", "\"exp_avg_sq\"", "]", "=", "torch", ".", "zeros_like", "(", "p", ".", "data", ")", "\n", "\n", "", "state", "[", "\"step\"", "]", "+=", "1", "\n", "\n", "exp_avg", ",", "exp_avg_sq", "=", "state", "[", "\"exp_avg\"", "]", ",", "state", "[", "\"exp_avg_sq\"", "]", "\n", "beta1", ",", "beta2", "=", "group", "[", "\"betas\"", "]", "\n", "\n", "if", "grad", ".", "is_sparse", ":", "\n", "                    ", "grad", "=", "(", "\n", "grad", ".", "coalesce", "(", ")", "\n", ")", "# the update is non-linear so indices must be unique", "\n", "grad_indices", "=", "grad", ".", "_indices", "(", ")", "\n", "grad_values", "=", "grad", ".", "_values", "(", ")", "\n", "size", "=", "grad", ".", "size", "(", ")", "\n", "\n", "def", "make_sparse", "(", "values", ")", ":", "\n", "                        ", "constructor", "=", "grad", ".", "new", "\n", "if", "grad_indices", ".", "dim", "(", ")", "==", "0", "or", "values", ".", "dim", "(", ")", "==", "0", ":", "\n", "                            ", "return", "constructor", "(", ")", ".", "resize_as_", "(", "grad", ")", "\n", "", "return", "constructor", "(", "grad_indices", ",", "values", ",", "size", ")", "\n", "\n", "# Decay the first and second moment running average coefficient", "\n", "#      old <- b * old + (1 - b) * new", "\n", "# <==> old += (1 - b) * (new - old)", "\n", "", "old_exp_avg_values", "=", "exp_avg", ".", "sparse_mask", "(", "grad", ")", ".", "_values", "(", ")", "\n", "exp_avg_update_values", "=", "grad_values", ".", "sub", "(", "old_exp_avg_values", ")", ".", "mul_", "(", "\n", "1", "-", "beta1", "\n", ")", "\n", "exp_avg", ".", "add_", "(", "make_sparse", "(", "exp_avg_update_values", ")", ")", "\n", "old_exp_avg_sq_values", "=", "exp_avg_sq", ".", "sparse_mask", "(", "grad", ")", ".", "_values", "(", ")", "\n", "exp_avg_sq_update_values", "=", "(", "\n", "grad_values", ".", "pow", "(", "2", ")", ".", "sub_", "(", "old_exp_avg_sq_values", ")", ".", "mul_", "(", "1", "-", "beta2", ")", "\n", ")", "\n", "exp_avg_sq", ".", "add_", "(", "make_sparse", "(", "exp_avg_sq_update_values", ")", ")", "\n", "\n", "# Dense addition again is intended, avoiding another sparse_mask", "\n", "numer", "=", "exp_avg_update_values", ".", "add_", "(", "old_exp_avg_values", ")", "\n", "exp_avg_sq_update_values", ".", "add_", "(", "old_exp_avg_sq_values", ")", "\n", "denom", "=", "exp_avg_sq_update_values", ".", "sqrt_", "(", ")", ".", "add_", "(", "group", "[", "\"eps\"", "]", ")", "\n", "del", "exp_avg_update_values", ",", "exp_avg_sq_update_values", "\n", "\n", "bias_correction1", "=", "1", "-", "beta1", "**", "state", "[", "\"step\"", "]", "\n", "bias_correction2", "=", "1", "-", "beta2", "**", "state", "[", "\"step\"", "]", "\n", "step_size", "=", "(", "\n", "group", "[", "\"lr\"", "]", "*", "math", ".", "sqrt", "(", "bias_correction2", ")", "/", "bias_correction1", "\n", ")", "\n", "\n", "p", ".", "data", ".", "add_", "(", "make_sparse", "(", "-", "step_size", "*", "numer", ".", "div_", "(", "denom", ")", ")", ")", "\n", "\n", "", "else", ":", "\n", "# Decay the first and second moment running average coefficient", "\n", "                    ", "exp_avg", ".", "mul_", "(", "beta1", ")", ".", "add_", "(", "1", "-", "beta1", ",", "grad", ")", "\n", "exp_avg_sq", ".", "mul_", "(", "beta2", ")", ".", "addcmul_", "(", "1", "-", "beta2", ",", "grad", ",", "grad", ")", "\n", "denom", "=", "exp_avg_sq", ".", "sqrt", "(", ")", ".", "add_", "(", "group", "[", "\"eps\"", "]", ")", "\n", "\n", "bias_correction1", "=", "1", "-", "beta1", "**", "state", "[", "\"step\"", "]", "\n", "bias_correction2", "=", "1", "-", "beta2", "**", "state", "[", "\"step\"", "]", "\n", "step_size", "=", "(", "\n", "group", "[", "\"lr\"", "]", "*", "math", ".", "sqrt", "(", "bias_correction2", ")", "/", "bias_correction1", "\n", ")", "\n", "\n", "p", ".", "data", ".", "addcdiv_", "(", "-", "step_size", ",", "exp_avg", ",", "denom", ")", "\n", "\n", "", "", "", "return", "loss", "\n", "", "", ""]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.training.optimizers.make_parameter_groups": [[34, 134], ["logger.info", "range", "logger.info", "range", "regex_use_counts.items", "isinstance", "len", "parameter_groups[].update", "set", "enumerate", "logger.info", "sum", "parameter_group.numel", "range", "range", "[].append", "parameter_group_names[].add", "[].append", "parameter_group_names[].add", "len", "list", "logger.warning", "re.search", "parameter_groups[].items", "parameter.numel", "len", "len", "ValueError"], "function", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.dataset_readers.multiprocess_dataset_reader.logger.info", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.dataset_readers.multiprocess_dataset_reader.logger.info", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.metrics.conll_coref_scores.Scorer.update", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.dataset_readers.multiprocess_dataset_reader.logger.info", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.nn.beam_search.BeamSearch.search"], ["def", "make_parameter_groups", "(", "\n", "model_parameters", ":", "List", "[", "Tuple", "[", "str", ",", "torch", ".", "nn", ".", "Parameter", "]", "]", ",", "\n", "groups", ":", "List", "[", "Tuple", "[", "List", "[", "str", "]", ",", "Dict", "[", "str", ",", "Any", "]", "]", "]", "=", "None", ",", "\n", ")", "->", "Union", "[", "List", "[", "Dict", "[", "str", ",", "Any", "]", "]", ",", "List", "[", "torch", ".", "nn", ".", "Parameter", "]", "]", ":", "\n", "    ", "\"\"\"\n    Takes a list of model parameters with associated names (typically coming from something like\n    `model.parameters`), along with a grouping (as specified below), and prepares them to be passed\n    to the `__init__` function of a `torch.Optimizer`.  This means separating the parameters into\n    groups with the given regexes, and prepping whatever keyword arguments are given for those\n    regexes in `groups`.\n\n    `groups` contains something like:\n\n    ```\n     [\n          ([\"regex1\", \"regex2\"], {\"lr\": 1e-3}),\n          ([\"regex3\"], {\"lr\": 1e-4})\n     ]\n     ```\n\n    The return value in the right format to be passed directly as the `params` argument to a pytorch\n    `Optimizer`.  If there are multiple groups specified, this is list of dictionaries, where each\n    dict contains a \"parameter group\" and groups specific options, e.g., {'params': [list of\n    parameters], 'lr': 1e-3, ...}.  Any config option not specified in the additional options (e.g.\n    for the default group) is inherited from the top level arguments given in the constructor.  See:\n    https://pytorch.org/docs/0.3.0/optim.html?#per-parameter-options.  See also our\n    `test_optimizer_parameter_groups` test for an example of how this works in this code.\n\n    The dictionary's return type is labeled as `Any`, because it can be a `List[torch.nn.Parameter]`\n    (for the \"params\" key), or anything else (typically a float) for the other keys.\n    \"\"\"", "\n", "if", "groups", ":", "\n", "# In addition to any parameters that match group specific regex,", "\n", "# we also need a group for the remaining \"default\" group.", "\n", "# Those will be included in the last entry of parameter_groups.", "\n", "        ", "parameter_groups", ":", "Union", "[", "List", "[", "Dict", "[", "str", ",", "Any", "]", "]", ",", "List", "[", "torch", ".", "nn", ".", "Parameter", "]", "]", "=", "[", "\n", "{", "\"params\"", ":", "[", "]", "}", "for", "_", "in", "range", "(", "len", "(", "groups", ")", "+", "1", ")", "\n", "]", "\n", "# add the group specific kwargs", "\n", "for", "k", "in", "range", "(", "len", "(", "groups", ")", ")", ":", "\n", "            ", "parameter_groups", "[", "k", "]", ".", "update", "(", "groups", "[", "k", "]", "[", "1", "]", ")", "\n", "\n", "", "regex_use_counts", ":", "Dict", "[", "str", ",", "int", "]", "=", "{", "}", "\n", "parameter_group_names", ":", "List", "[", "set", "]", "=", "[", "set", "(", ")", "for", "_", "in", "range", "(", "len", "(", "groups", ")", "+", "1", ")", "]", "\n", "for", "name", ",", "param", "in", "model_parameters", ":", "\n", "# Determine the group for this parameter.", "\n", "            ", "group_index", "=", "None", "\n", "for", "k", ",", "group_regexes", "in", "enumerate", "(", "groups", ")", ":", "\n", "                ", "for", "regex", "in", "group_regexes", "[", "0", "]", ":", "\n", "                    ", "if", "regex", "not", "in", "regex_use_counts", ":", "\n", "                        ", "regex_use_counts", "[", "regex", "]", "=", "0", "\n", "", "if", "re", ".", "search", "(", "regex", ",", "name", ")", ":", "\n", "                        ", "if", "group_index", "is", "not", "None", "and", "group_index", "!=", "k", ":", "\n", "                            ", "raise", "ValueError", "(", "\n", "\"{} was specified in two separate parameter groups\"", ".", "format", "(", "\n", "name", "\n", ")", "\n", ")", "\n", "", "group_index", "=", "k", "\n", "regex_use_counts", "[", "regex", "]", "+=", "1", "\n", "\n", "", "", "", "if", "group_index", "is", "not", "None", ":", "\n", "                ", "parameter_groups", "[", "group_index", "]", "[", "\"params\"", "]", ".", "append", "(", "param", ")", "\n", "parameter_group_names", "[", "group_index", "]", ".", "add", "(", "name", ")", "\n", "", "else", ":", "\n", "# the default group", "\n", "                ", "parameter_groups", "[", "-", "1", "]", "[", "\"params\"", "]", ".", "append", "(", "param", ")", "\n", "parameter_group_names", "[", "-", "1", "]", ".", "add", "(", "name", ")", "\n", "\n", "# log the parameter groups", "\n", "", "", "logger", ".", "info", "(", "\"Done constructing parameter groups.\"", ")", "\n", "for", "k", "in", "range", "(", "len", "(", "groups", ")", "+", "1", ")", ":", "\n", "            ", "group_options", "=", "{", "\n", "key", ":", "val", "for", "key", ",", "val", "in", "parameter_groups", "[", "k", "]", ".", "items", "(", ")", "if", "key", "!=", "\"params\"", "\n", "}", "\n", "logger", ".", "info", "(", "\n", "\"Group %s: %s, %s\"", ",", "k", ",", "list", "(", "parameter_group_names", "[", "k", "]", ")", ",", "group_options", "\n", ")", "\n", "# check for unused regex", "\n", "", "for", "regex", ",", "count", "in", "regex_use_counts", ".", "items", "(", ")", ":", "\n", "            ", "if", "count", "==", "0", ":", "\n", "                ", "logger", ".", "warning", "(", "\n", "\"When constructing parameter groups, %s does not match any parameter name\"", ",", "\n", "regex", ",", "\n", ")", "\n", "\n", "", "", "", "else", ":", "\n", "        ", "parameter_groups", "=", "[", "param", "for", "name", ",", "param", "in", "model_parameters", "]", "\n", "\n", "# Log the number of parameters to optimize", "\n", "", "num_parameters", "=", "0", "\n", "for", "parameter_group", "in", "parameter_groups", ":", "\n", "        ", "if", "isinstance", "(", "parameter_group", ",", "dict", ")", ":", "\n", "            ", "num_parameters", "+=", "sum", "(", "\n", "parameter", ".", "numel", "(", ")", "for", "parameter", "in", "parameter_group", "[", "\"params\"", "]", "\n", ")", "\n", "", "else", ":", "\n", "            ", "num_parameters", "+=", "parameter_group", ".", "numel", "(", ")", "# type: ignore", "\n", "", "", "logger", ".", "info", "(", "\"Number of trainable parameters: %s\"", ",", "num_parameters", ")", "\n", "return", "parameter_groups", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.metrics.conll_coref_full_scores.ConllCorefFullScores.__init__": [[13, 15], ["allennlp.training.metrics.ConllCorefScores.__init__"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.graph_util.plot.GraphPlot.__init__"], ["def", "__init__", "(", "self", ")", "->", "None", ":", "\n", "        ", "super", "(", "ConllCorefFullScores", ",", "self", ")", ".", "__init__", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.metrics.conll_coref_full_scores.ConllCorefFullScores.get_metric": [[16, 46], ["tuple", "conll_coref_full_scores.ConllCorefFullScores.reset", "e.get_precision", "e.get_recall", "e.get_f1", "e.get_precision", "e.get_recall", "e.get_f1", "sum", "len", "metric"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.metrics.sequence_accuracy.SequenceAccuracy.reset", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.metrics.conll_coref_scores.Scorer.get_precision", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.metrics.conll_coref_scores.Scorer.get_recall", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.metrics.conll_coref_scores.Scorer.get_f1", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.metrics.conll_coref_scores.Scorer.get_precision", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.metrics.conll_coref_scores.Scorer.get_recall", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.metrics.conll_coref_scores.Scorer.get_f1"], ["", "@", "overrides", "\n", "def", "get_metric", "(", "self", ",", "reset", ":", "bool", "=", "False", ",", "full", ":", "bool", "=", "False", ")", ":", "\n", "        ", "full_metrics", "=", "{", "}", "\n", "if", "full", ":", "\n", "            ", "for", "e", "in", "self", ".", "scorers", ":", "\n", "                ", "metric_name", "=", "e", ".", "metric", ".", "__name__", "\n", "full_metrics", "[", "metric_name", "]", "=", "{", "\n", "\"precision\"", ":", "e", ".", "get_precision", "(", ")", ",", "\n", "\"recall\"", ":", "e", ".", "get_recall", "(", ")", ",", "\n", "\"f1_score\"", ":", "e", ".", "get_f1", "(", ")", ",", "\n", "}", "\n", "\n", "", "", "metrics", "=", "(", "\n", "lambda", "e", ":", "e", ".", "get_precision", "(", ")", ",", "\n", "lambda", "e", ":", "e", ".", "get_recall", "(", ")", ",", "\n", "lambda", "e", ":", "e", ".", "get_f1", "(", ")", ",", "\n", ")", "\n", "precision", ",", "recall", ",", "f1_score", "=", "tuple", "(", "\n", "sum", "(", "metric", "(", "e", ")", "for", "e", "in", "self", ".", "scorers", ")", "/", "len", "(", "self", ".", "scorers", ")", "\n", "for", "metric", "in", "metrics", "\n", ")", "\n", "\n", "full_metrics", "[", "\"coref_precision\"", "]", "=", "precision", "\n", "full_metrics", "[", "\"coref_recall\"", "]", "=", "recall", "\n", "full_metrics", "[", "\"coref_f1\"", "]", "=", "f1_score", "\n", "\n", "if", "reset", ":", "\n", "            ", "self", ".", "reset", "(", ")", "\n", "\n", "", "return", "full_metrics", "\n", "", "", ""]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.metrics.relation_f1_measure.RelationF1Measure.__init__": [[17, 25], ["None"], "methods", ["None"], ["def", "__init__", "(", "self", ")", "->", "None", ":", "\n", "        ", "\"\"\"\n        A class for computing the metrics specific to relation extraction.\n        We consider a relation correct if we correctly predict the last of the head of the two arguments and the relation type.\n        \"\"\"", "\n", "self", ".", "_true_positives", ":", "int", "=", "0", "\n", "self", ".", "_false_positives", ":", "int", "=", "0", "\n", "self", ".", "_false_negatives", ":", "int", "=", "0", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.metrics.relation_f1_measure.RelationF1Measure.__call__": [[26, 93], ["relation_f1_measure.RelationF1Measure.unwrap_to_tensors", "predictions.size", "torch.ones_like.float", "torch.stack", "squared_mask.unsqueeze().repeat.unsqueeze().repeat.unsqueeze().repeat", "gold_labels.cpu.cpu.cpu", "gold_labels.cpu.cpu.size", "range", "torch.ones_like", "gold_labels.cpu.cpu.size", "predictions.size", "allennlp.common.checks.ConfigurationError", "predictions[].view().nonzero().cpu().numpy", "gold_labels[].view().nonzero().cpu().numpy", "squared_mask.unsqueeze().repeat.unsqueeze().repeat.unsqueeze", "e.view", "predictions[].view().nonzero().cpu", "gold_labels[].view().nonzero().cpu", "predictions[].view().nonzero", "gold_labels[].view().nonzero", "predictions[].view", "gold_labels[].view"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.metrics.metric.Metric.unwrap_to_tensors"], ["", "def", "__call__", "(", "\n", "self", ",", "\n", "predictions", ":", "torch", ".", "Tensor", ",", "\n", "gold_labels", ":", "torch", ".", "Tensor", ",", "\n", "mask", ":", "Optional", "[", "torch", ".", "Tensor", "]", "=", "None", ",", "\n", ")", ":", "\n", "        ", "\"\"\"\n        Update the TP, FP and FN counters.\n        \n        Parameters\n        ----------\n        predictions : ``torch.Tensor``, required.\n            A tensor of predictions of shape (batch_size, sequence_length, num_classes).\n        gold_labels : ``torch.Tensor``, required.\n            A tensor of integer class label of shape (batch_size, sequence_length). It must be the same\n            shape as the ``predictions`` tensor without the ``num_classes`` dimension.\n        mask: ``torch.Tensor``, optional (default = None).\n            A masking tensor the same size as ``gold_labels``.\n        \"\"\"", "\n", "if", "mask", "is", "None", ":", "\n", "            ", "mask", "=", "torch", ".", "ones_like", "(", "gold_labels", ")", "# ones_like(gold_labels)", "\n", "# Get the data from the Variables.", "\n", "", "predictions", ",", "gold_labels", ",", "mask", "=", "self", ".", "unwrap_to_tensors", "(", "\n", "predictions", ",", "gold_labels", ",", "mask", "\n", ")", "\n", "\n", "if", "gold_labels", ".", "size", "(", ")", "!=", "predictions", ".", "size", "(", ")", ":", "\n", "            ", "raise", "ConfigurationError", "(", "\n", "\"Predictions and gold labels don't have the same size.\"", "\n", ")", "\n", "\n", "# Apply mask", "\n", "# Compute the mask before computing the loss", "\n", "# Transform the mask that is at the sentence level (#Size: n_batches x padded_document_length)", "\n", "# to a suitable format for the relation labels level", "\n", "", "_", ",", "padded_document_length", ",", "_", ",", "n_classes", "=", "predictions", ".", "size", "(", ")", "\n", "mask", "=", "mask", ".", "float", "(", ")", "\n", "squared_mask", "=", "torch", ".", "stack", "(", "\n", "[", "e", ".", "view", "(", "padded_document_length", ",", "1", ")", "*", "e", "for", "e", "in", "mask", "]", ",", "dim", "=", "0", "\n", ")", "\n", "squared_mask", "=", "squared_mask", ".", "unsqueeze", "(", "-", "1", ")", ".", "repeat", "(", "\n", "1", ",", "1", ",", "1", ",", "n_classes", "\n", ")", "# Size: n_batches x padded_document_length x padded_document_length x n_classes", "\n", "\n", "gold_labels", "=", "gold_labels", ".", "cpu", "(", ")", "\n", "\n", "predictions", "=", "(", "\n", "predictions", "*", "squared_mask", "\n", ")", "# Size: n_batches x padded_document_length x padded_document_length x n_classes", "\n", "gold_labels", "=", "(", "\n", "gold_labels", "*", "squared_mask", "\n", ")", "# Size: n_batches x padded_document_length x padded_document_length x n_classes", "\n", "\n", "# Iterate over timesteps in batch.", "\n", "batch_size", "=", "gold_labels", ".", "size", "(", "0", ")", "\n", "for", "i", "in", "range", "(", "batch_size", ")", ":", "\n", "            ", "flattened_predictions", "=", "predictions", "[", "i", "]", ".", "view", "(", "-", "1", ")", ".", "nonzero", "(", ")", ".", "cpu", "(", ")", ".", "numpy", "(", ")", "\n", "flattened_gold_labels", "=", "gold_labels", "[", "i", "]", ".", "view", "(", "-", "1", ")", ".", "nonzero", "(", ")", ".", "cpu", "(", ")", ".", "numpy", "(", ")", "\n", "\n", "for", "prediction", "in", "flattened_predictions", ":", "\n", "                ", "if", "prediction", "in", "flattened_gold_labels", ":", "\n", "                    ", "self", ".", "_true_positives", "+=", "1", "\n", "", "else", ":", "\n", "                    ", "self", ".", "_false_positives", "+=", "1", "\n", "", "", "for", "gold", "in", "flattened_gold_labels", ":", "\n", "                ", "if", "gold", "not", "in", "flattened_predictions", ":", "\n", "                    ", "self", ".", "_false_negatives", "+=", "1", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.metrics.relation_f1_measure.RelationF1Measure.get_metric": [[94, 110], ["relation_f1_measure.RelationF1Measure._compute_metrics", "relation_f1_measure.RelationF1Measure.reset"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.metrics.span_based_f1_measure.SpanBasedF1Measure._compute_metrics", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.metrics.sequence_accuracy.SequenceAccuracy.reset"], ["", "", "", "", "def", "get_metric", "(", "self", ",", "reset", ":", "bool", "=", "False", ")", ":", "\n", "        ", "\"\"\"\n        Get the metrics and reset the counters if necessary.\n        \"\"\"", "\n", "all_metrics", "=", "{", "}", "\n", "\n", "# Compute the precision, recall and f1 for all spans jointly.", "\n", "precision", ",", "recall", ",", "f1_measure", "=", "self", ".", "_compute_metrics", "(", "\n", "self", ".", "_true_positives", ",", "self", ".", "_false_positives", ",", "self", ".", "_false_negatives", "\n", ")", "\n", "all_metrics", "[", "\"precision-overall\"", "]", "=", "precision", "\n", "all_metrics", "[", "\"recall-overall\"", "]", "=", "recall", "\n", "all_metrics", "[", "\"f1-measure-overall\"", "]", "=", "f1_measure", "\n", "if", "reset", ":", "\n", "            ", "self", ".", "reset", "(", ")", "\n", "", "return", "all_metrics", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.metrics.relation_f1_measure.RelationF1Measure._compute_metrics": [[111, 121], ["float", "float", "float", "float"], "methods", ["None"], ["", "@", "staticmethod", "\n", "def", "_compute_metrics", "(", "\n", "true_positives", ":", "int", ",", "false_positives", ":", "int", ",", "false_negatives", ":", "int", "\n", ")", ":", "\n", "        ", "precision", "=", "float", "(", "true_positives", ")", "/", "float", "(", "\n", "true_positives", "+", "false_positives", "+", "1e-13", "\n", ")", "\n", "recall", "=", "float", "(", "true_positives", ")", "/", "float", "(", "true_positives", "+", "false_negatives", "+", "1e-13", ")", "\n", "f1_measure", "=", "2.0", "*", "(", "(", "precision", "*", "recall", ")", "/", "(", "precision", "+", "recall", "+", "1e-13", ")", ")", "\n", "return", "precision", ",", "recall", ",", "f1_measure", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.metrics.relation_f1_measure.RelationF1Measure.reset": [[122, 126], ["None"], "methods", ["None"], ["", "def", "reset", "(", "self", ")", ":", "\n", "        ", "self", ".", "_true_positives", "=", "0", "\n", "self", ".", "_false_positives", "=", "0", "\n", "self", ".", "_false_negatives", "=", "0", "\n", "", "", ""]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.metrics.accuracy.Accuracy.__init__": [[9, 12], ["None"], "methods", ["None"], ["    ", "def", "__init__", "(", "self", ")", "->", "None", ":", "\n", "        ", "self", ".", "correct", "=", "0", "\n", "self", ".", "total", "=", "0", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.metrics.accuracy.Accuracy.clamp": [[13, 17], ["None"], "methods", ["None"], ["", "def", "clamp", "(", "self", ",", "logits", ")", ":", "\n", "        ", "logits", "[", "logits", ">=", "0", "]", "=", "1", "\n", "logits", "[", "logits", "<", "0", "]", "=", "-", "1", "\n", "return", "logits", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.metrics.accuracy.Accuracy.__call__": [[18, 26], ["accuracy.Accuracy.detach", "labels.detach.detach.detach", "accuracy.Accuracy.clamp", "torch.eq().sum().item", "labels.detach.detach.size", "list", "accuracy.Accuracy.unwrap_to_tensors", "torch.eq().sum", "torch.eq", "accuracy.Accuracy.cpu", "labels.detach.detach.cpu"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.metrics.f1_score.F1.clamp", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.metrics.metric.Metric.unwrap_to_tensors"], ["", "@", "overrides", "\n", "def", "__call__", "(", "self", ",", "logits", ",", "labels", ")", ":", "\n", "        ", "logits", "=", "list", "(", "self", ".", "unwrap_to_tensors", "(", "logits", ")", ")", "[", "0", "]", "\n", "logits", "=", "logits", ".", "detach", "(", ")", "\n", "labels", "=", "labels", ".", "detach", "(", ")", "\n", "logits", "=", "self", ".", "clamp", "(", "logits", ")", "\n", "self", ".", "correct", "+=", "torch", ".", "eq", "(", "logits", ".", "cpu", "(", ")", ",", "labels", ".", "cpu", "(", ")", ")", ".", "sum", "(", ")", ".", "item", "(", ")", "\n", "self", ".", "total", "+=", "labels", ".", "size", "(", "0", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.metrics.accuracy.Accuracy.get_metric": [[27, 33], ["accuracy.Accuracy.reset"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.metrics.sequence_accuracy.SequenceAccuracy.reset"], ["", "@", "overrides", "\n", "def", "get_metric", "(", "self", ",", "reset", ":", "bool", "=", "False", ")", ":", "\n", "        ", "accuracy_value", "=", "self", ".", "correct", "/", "self", ".", "total", "if", "self", ".", "total", ">", "0", "else", "0", "\n", "if", "reset", ":", "\n", "            ", "self", ".", "reset", "(", ")", "\n", "", "return", "accuracy_value", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.metrics.accuracy.Accuracy.reset": [[34, 38], ["None"], "methods", ["None"], ["", "@", "overrides", "\n", "def", "reset", "(", "self", ")", ":", "\n", "        ", "self", ".", "correct", "=", "0", "\n", "self", ".", "total", "=", "0", "\n", "", "", ""]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.metrics.f1_score.F1.__init__": [[10, 13], ["None"], "methods", ["None"], ["    ", "def", "__init__", "(", "self", ")", "->", "None", ":", "\n", "        ", "self", ".", "f1", "=", "0", "\n", "self", ".", "total", "=", "0", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.metrics.f1_score.F1.clamp": [[14, 18], ["None"], "methods", ["None"], ["", "def", "clamp", "(", "self", ",", "logits", ")", ":", "\n", "        ", "logits", "[", "logits", ">=", "0", "]", "=", "1", "\n", "logits", "[", "logits", "<", "0", "]", "=", "-", "1", "\n", "return", "logits", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.metrics.f1_score.F1.__call__": [[19, 29], ["sklearn.metrics.f1_score.F1.detach", "labels.detach.detach.detach", "sklearn.metrics.f1_score.F1.clamp", "sklearn.metrics.f1_score", "list", "labels.detach.detach.cpu().numpy", "sklearn.metrics.f1_score.F1.cpu().numpy", "sklearn.metrics.f1_score.F1.unwrap_to_tensors", "labels.detach.detach.cpu", "sklearn.metrics.f1_score.F1.cpu"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.metrics.f1_score.F1.clamp", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.metrics.metric.Metric.unwrap_to_tensors"], ["", "@", "overrides", "\n", "def", "__call__", "(", "self", ",", "logits", ",", "labels", ")", ":", "\n", "        ", "logits", "=", "list", "(", "self", ".", "unwrap_to_tensors", "(", "logits", ")", ")", "[", "0", "]", "\n", "logits", "=", "logits", ".", "detach", "(", ")", "\n", "labels", "=", "labels", ".", "detach", "(", ")", "\n", "logits", "=", "self", ".", "clamp", "(", "logits", ")", "\n", "self", ".", "f1", "+=", "f1_score", "(", "\n", "labels", ".", "cpu", "(", ")", ".", "numpy", "(", ")", ",", "logits", ".", "cpu", "(", ")", ".", "numpy", "(", ")", ",", "average", "=", "\"binary\"", "\n", ")", "\n", "self", ".", "total", "+=", "1", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.metrics.f1_score.F1.get_metric": [[30, 36], ["sklearn.metrics.f1_score.F1.reset"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.metrics.sequence_accuracy.SequenceAccuracy.reset"], ["", "@", "overrides", "\n", "def", "get_metric", "(", "self", ",", "reset", ":", "bool", "=", "False", ")", ":", "\n", "        ", "f1_value", "=", "self", ".", "f1", "/", "self", ".", "total", "if", "self", ".", "total", ">", "0", "else", "0", "\n", "if", "reset", ":", "\n", "            ", "self", ".", "reset", "(", ")", "\n", "", "return", "f1_value", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.metrics.f1_score.F1.reset": [[37, 41], ["None"], "methods", ["None"], ["", "@", "overrides", "\n", "def", "reset", "(", "self", ")", ":", "\n", "        ", "self", ".", "f1", "=", "0", "\n", "self", ".", "total", "=", "0", "\n", "", "", ""]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.metrics.evalb_bracketing_scorer.EvalbBracketingScorer.__init__": [[58, 89], ["os.path.join", "os.path.join"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.dataset_readers.multiprocess_dataset_reader.QIterable.join", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.dataset_readers.multiprocess_dataset_reader.QIterable.join"], ["def", "__init__", "(", "\n", "self", ",", "\n", "evalb_directory_path", ":", "str", "=", "DEFAULT_EVALB_DIR", ",", "\n", "evalb_param_filename", ":", "str", "=", "\"COLLINS.prm\"", ",", "\n", "evalb_num_errors_to_kill", ":", "int", "=", "10", ",", "\n", ")", "->", "None", ":", "\n", "        ", "self", ".", "_evalb_directory_path", "=", "evalb_directory_path", "\n", "self", ".", "_evalb_program_path", "=", "os", ".", "path", ".", "join", "(", "evalb_directory_path", ",", "\"evalb\"", ")", "\n", "self", ".", "_evalb_param_path", "=", "os", ".", "path", ".", "join", "(", "\n", "evalb_directory_path", ",", "evalb_param_filename", "\n", ")", "\n", "self", ".", "_evalb_num_errors_to_kill", "=", "evalb_num_errors_to_kill", "\n", "\n", "self", ".", "_header_line", "=", "[", "\n", "\"ID\"", ",", "\n", "\"Len.\"", ",", "\n", "\"Stat.\"", ",", "\n", "\"Recal\"", ",", "\n", "\"Prec.\"", ",", "\n", "\"Bracket\"", ",", "\n", "\"gold\"", ",", "\n", "\"test\"", ",", "\n", "\"Bracket\"", ",", "\n", "\"Words\"", ",", "\n", "\"Tags\"", ",", "\n", "\"Accracy\"", ",", "\n", "]", "\n", "\n", "self", ".", "_correct_predicted_brackets", "=", "0.0", "\n", "self", ".", "_gold_brackets", "=", "0.0", "\n", "self", ".", "_predicted_brackets", "=", "0.0", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.metrics.evalb_bracketing_scorer.EvalbBracketingScorer.__call__": [[90, 153], ["tempfile.mkdtemp", "os.path.join", "os.path.join", "subprocess.run", "subprocess.run.stdout.split", "shutil.rmtree", "os.path.exists", "logger.warning", "evalb_bracketing_scorer.EvalbBracketingScorer.compile_evalb", "open", "open", "str", "line.strip().split", "os.path.exists", "allennlp.common.checks.ConfigurationError", "gold_file.write", "predicted_file.write", "line.strip", "len", "float", "tree.pformat", "tree.pformat"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.dataset_readers.multiprocess_dataset_reader.QIterable.join", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.dataset_readers.multiprocess_dataset_reader.QIterable.join", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.commands.train.TrainModel.run", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.metrics.evalb_bracketing_scorer.EvalbBracketingScorer.compile_evalb", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.None.predictor.write", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.None.predictor.write"], ["", "@", "overrides", "\n", "def", "__call__", "(", "self", ",", "predicted_trees", ":", "List", "[", "Tree", "]", ",", "gold_trees", ":", "List", "[", "Tree", "]", ")", "->", "None", ":", "# type: ignore", "\n", "        ", "\"\"\"\n        # Parameters\n\n        predicted_trees : `List[Tree]`\n            A list of predicted NLTK Trees to compute score for.\n        gold_trees : `List[Tree]`\n            A list of gold NLTK Trees to use as a reference.\n        \"\"\"", "\n", "if", "not", "os", ".", "path", ".", "exists", "(", "self", ".", "_evalb_program_path", ")", ":", "\n", "            ", "logger", ".", "warning", "(", "\n", "f\"EVALB not found at {self._evalb_program_path}.  Attempting to compile it.\"", "\n", ")", "\n", "EvalbBracketingScorer", ".", "compile_evalb", "(", "self", ".", "_evalb_directory_path", ")", "\n", "\n", "# If EVALB executable still doesn't exist, raise an error.", "\n", "if", "not", "os", ".", "path", ".", "exists", "(", "self", ".", "_evalb_program_path", ")", ":", "\n", "                ", "compile_command", "=", "(", "\n", "f\"python -c 'from allennlp.training.metrics import EvalbBracketingScorer; \"", "\n", "f'EvalbBracketingScorer.compile_evalb(\"{self._evalb_directory_path}\")\\''", "\n", ")", "\n", "raise", "ConfigurationError", "(", "\n", "f\"EVALB still not found at {self._evalb_program_path}. \"", "\n", "\"You must compile the EVALB scorer before using it.\"", "\n", "\" Run 'make' in the '{}' directory or run: {}\"", ".", "format", "(", "\n", "self", ".", "_evalb_program_path", ",", "compile_command", "\n", ")", "\n", ")", "\n", "", "", "tempdir", "=", "tempfile", ".", "mkdtemp", "(", ")", "\n", "gold_path", "=", "os", ".", "path", ".", "join", "(", "tempdir", ",", "\"gold.txt\"", ")", "\n", "predicted_path", "=", "os", ".", "path", ".", "join", "(", "tempdir", ",", "\"predicted.txt\"", ")", "\n", "with", "open", "(", "gold_path", ",", "\"w\"", ")", "as", "gold_file", ":", "\n", "            ", "for", "tree", "in", "gold_trees", ":", "\n", "                ", "gold_file", ".", "write", "(", "f\"{tree.pformat(margin=1000000)}\\n\"", ")", "\n", "\n", "", "", "with", "open", "(", "predicted_path", ",", "\"w\"", ")", "as", "predicted_file", ":", "\n", "            ", "for", "tree", "in", "predicted_trees", ":", "\n", "                ", "predicted_file", ".", "write", "(", "f\"{tree.pformat(margin=1000000)}\\n\"", ")", "\n", "\n", "", "", "command", "=", "[", "\n", "self", ".", "_evalb_program_path", ",", "\n", "\"-p\"", ",", "\n", "self", ".", "_evalb_param_path", ",", "\n", "\"-e\"", ",", "\n", "str", "(", "self", ".", "_evalb_num_errors_to_kill", ")", ",", "\n", "gold_path", ",", "\n", "predicted_path", ",", "\n", "]", "\n", "completed_process", "=", "subprocess", ".", "run", "(", "\n", "command", ",", "stdout", "=", "subprocess", ".", "PIPE", ",", "universal_newlines", "=", "True", ",", "check", "=", "True", "\n", ")", "\n", "\n", "for", "line", "in", "completed_process", ".", "stdout", ".", "split", "(", "\"\\n\"", ")", ":", "\n", "            ", "stripped", "=", "line", ".", "strip", "(", ")", ".", "split", "(", ")", "\n", "if", "len", "(", "stripped", ")", "==", "12", "and", "stripped", "!=", "self", ".", "_header_line", ":", "\n", "# This line contains results for a single tree.", "\n", "                ", "numeric_line", "=", "[", "float", "(", "x", ")", "for", "x", "in", "stripped", "]", "\n", "self", ".", "_correct_predicted_brackets", "+=", "numeric_line", "[", "5", "]", "\n", "self", ".", "_gold_brackets", "+=", "numeric_line", "[", "6", "]", "\n", "self", ".", "_predicted_brackets", "+=", "numeric_line", "[", "7", "]", "\n", "\n", "", "", "shutil", ".", "rmtree", "(", "tempdir", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.metrics.evalb_bracketing_scorer.EvalbBracketingScorer.get_metric": [[154, 183], ["evalb_bracketing_scorer.EvalbBracketingScorer.reset"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.metrics.sequence_accuracy.SequenceAccuracy.reset"], ["", "@", "overrides", "\n", "def", "get_metric", "(", "self", ",", "reset", ":", "bool", "=", "False", ")", ":", "\n", "        ", "\"\"\"\n        # Returns\n\n        The average precision, recall and f1.\n        \"\"\"", "\n", "recall", "=", "(", "\n", "self", ".", "_correct_predicted_brackets", "/", "self", ".", "_gold_brackets", "\n", "if", "self", ".", "_gold_brackets", ">", "0", "\n", "else", "0.0", "\n", ")", "\n", "precision", "=", "(", "\n", "self", ".", "_correct_predicted_brackets", "/", "self", ".", "_predicted_brackets", "\n", "if", "self", ".", "_gold_brackets", ">", "0", "\n", "else", "0.0", "\n", ")", "\n", "f1_measure", "=", "(", "\n", "2", "*", "(", "precision", "*", "recall", ")", "/", "(", "precision", "+", "recall", ")", "\n", "if", "precision", "+", "recall", ">", "0", "\n", "else", "0", "\n", ")", "\n", "\n", "if", "reset", ":", "\n", "            ", "self", ".", "reset", "(", ")", "\n", "", "return", "{", "\n", "\"evalb_recall\"", ":", "recall", ",", "\n", "\"evalb_precision\"", ":", "precision", ",", "\n", "\"evalb_f1_measure\"", ":", "f1_measure", ",", "\n", "}", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.metrics.evalb_bracketing_scorer.EvalbBracketingScorer.reset": [[185, 190], ["None"], "methods", ["None"], ["", "@", "overrides", "\n", "def", "reset", "(", "self", ")", ":", "\n", "        ", "self", ".", "_correct_predicted_brackets", "=", "0.0", "\n", "self", ".", "_gold_brackets", "=", "0.0", "\n", "self", ".", "_predicted_brackets", "=", "0.0", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.metrics.evalb_bracketing_scorer.EvalbBracketingScorer.compile_evalb": [[191, 195], ["logger.info", "os.system"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.dataset_readers.multiprocess_dataset_reader.logger.info"], ["", "@", "staticmethod", "\n", "def", "compile_evalb", "(", "evalb_directory_path", ":", "str", "=", "DEFAULT_EVALB_DIR", ")", ":", "\n", "        ", "logger", ".", "info", "(", "f\"Compiling EVALB by running make in {evalb_directory_path}.\"", ")", "\n", "os", ".", "system", "(", "\"cd {} && make && cd ../../../\"", ".", "format", "(", "evalb_directory_path", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.metrics.evalb_bracketing_scorer.EvalbBracketingScorer.clean_evalb": [[196, 199], ["os.system", "os.path.join"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.dataset_readers.multiprocess_dataset_reader.QIterable.join"], ["", "@", "staticmethod", "\n", "def", "clean_evalb", "(", "evalb_directory_path", ":", "str", "=", "DEFAULT_EVALB_DIR", ")", ":", "\n", "        ", "os", ".", "system", "(", "\"rm {}\"", ".", "format", "(", "os", ".", "path", ".", "join", "(", "evalb_directory_path", ",", "\"evalb\"", ")", ")", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.metrics.mean_absolute_error.MeanAbsoluteError.__init__": [[15, 18], ["None"], "methods", ["None"], ["def", "__init__", "(", "self", ")", "->", "None", ":", "\n", "        ", "self", ".", "_absolute_error", "=", "0.0", "\n", "self", ".", "_total_count", "=", "0.0", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.metrics.mean_absolute_error.MeanAbsoluteError.__call__": [[19, 46], ["mean_absolute_error.MeanAbsoluteError.unwrap_to_tensors", "torch.abs", "torch.sum", "torch.sum", "gold_labels.numel"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.metrics.metric.Metric.unwrap_to_tensors"], ["", "def", "__call__", "(", "\n", "self", ",", "\n", "predictions", ":", "torch", ".", "Tensor", ",", "\n", "gold_labels", ":", "torch", ".", "Tensor", ",", "\n", "mask", ":", "Optional", "[", "torch", ".", "Tensor", "]", "=", "None", ",", "\n", ")", ":", "\n", "        ", "\"\"\"\n        # Parameters\n\n        predictions : `torch.Tensor`, required.\n            A tensor of predictions of shape (batch_size, ...).\n        gold_labels : `torch.Tensor`, required.\n            A tensor of the same shape as `predictions`.\n        mask : `torch.Tensor`, optional (default = None).\n            A tensor of the same shape as `predictions`.\n        \"\"\"", "\n", "predictions", ",", "gold_labels", ",", "mask", "=", "self", ".", "unwrap_to_tensors", "(", "\n", "predictions", ",", "gold_labels", ",", "mask", "\n", ")", "\n", "\n", "absolute_errors", "=", "torch", ".", "abs", "(", "predictions", "-", "gold_labels", ")", "\n", "if", "mask", "is", "not", "None", ":", "\n", "            ", "absolute_errors", "*=", "mask", "\n", "self", ".", "_total_count", "+=", "torch", ".", "sum", "(", "mask", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "_total_count", "+=", "gold_labels", ".", "numel", "(", ")", "\n", "", "self", ".", "_absolute_error", "+=", "torch", ".", "sum", "(", "absolute_errors", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.metrics.mean_absolute_error.MeanAbsoluteError.get_metric": [[47, 57], ["float", "float", "mean_absolute_error.MeanAbsoluteError.reset"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.metrics.sequence_accuracy.SequenceAccuracy.reset"], ["", "def", "get_metric", "(", "self", ",", "reset", ":", "bool", "=", "False", ")", ":", "\n", "        ", "\"\"\"\n        # Returns\n\n        The accumulated mean absolute error.\n        \"\"\"", "\n", "mean_absolute_error", "=", "float", "(", "self", ".", "_absolute_error", ")", "/", "float", "(", "self", ".", "_total_count", ")", "\n", "if", "reset", ":", "\n", "            ", "self", ".", "reset", "(", ")", "\n", "", "return", "mean_absolute_error", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.metrics.mean_absolute_error.MeanAbsoluteError.reset": [[58, 62], ["None"], "methods", ["None"], ["", "@", "overrides", "\n", "def", "reset", "(", "self", ")", ":", "\n", "        ", "self", ".", "_absolute_error", "=", "0.0", "\n", "self", ".", "_total_count", "=", "0.0", "\n", "", "", ""]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.metrics.average.Average.__init__": [[15, 18], ["None"], "methods", ["None"], ["def", "__init__", "(", "self", ")", "->", "None", ":", "\n", "        ", "self", ".", "_total_value", "=", "0.0", "\n", "self", ".", "_count", "=", "0", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.metrics.average.Average.__call__": [[19, 29], ["list", "average.Average.unwrap_to_tensors"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.metrics.metric.Metric.unwrap_to_tensors"], ["", "@", "overrides", "\n", "def", "__call__", "(", "self", ",", "value", ")", ":", "\n", "        ", "\"\"\"\n        # Parameters\n\n        value : `float`\n            The value to average.\n        \"\"\"", "\n", "self", ".", "_total_value", "+=", "list", "(", "self", ".", "unwrap_to_tensors", "(", "value", ")", ")", "[", "0", "]", "\n", "self", ".", "_count", "+=", "1", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.metrics.average.Average.get_metric": [[30, 41], ["average.Average.reset"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.metrics.sequence_accuracy.SequenceAccuracy.reset"], ["", "@", "overrides", "\n", "def", "get_metric", "(", "self", ",", "reset", ":", "bool", "=", "False", ")", ":", "\n", "        ", "\"\"\"\n        # Returns\n\n        The average of all values that were passed to `__call__`.\n        \"\"\"", "\n", "average_value", "=", "self", ".", "_total_value", "/", "self", ".", "_count", "if", "self", ".", "_count", ">", "0", "else", "0", "\n", "if", "reset", ":", "\n", "            ", "self", ".", "reset", "(", ")", "\n", "", "return", "average_value", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.metrics.average.Average.reset": [[42, 46], ["None"], "methods", ["None"], ["", "@", "overrides", "\n", "def", "reset", "(", "self", ")", ":", "\n", "        ", "self", ".", "_total_value", "=", "0.0", "\n", "self", ".", "_count", "=", "0", "\n", "", "", ""]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.metrics.categorical_accuracy.CategoricalAccuracy.__init__": [[19, 30], ["allennlp.common.checks.ConfigurationError", "allennlp.common.checks.ConfigurationError"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "top_k", ":", "int", "=", "1", ",", "tie_break", ":", "bool", "=", "False", ")", "->", "None", ":", "\n", "        ", "if", "top_k", ">", "1", "and", "tie_break", ":", "\n", "            ", "raise", "ConfigurationError", "(", "\n", "\"Tie break in Categorical Accuracy can be done only for maximum (top_k = 1)\"", "\n", ")", "\n", "", "if", "top_k", "<=", "0", ":", "\n", "            ", "raise", "ConfigurationError", "(", "\"top_k passed to Categorical Accuracy must be > 0\"", ")", "\n", "", "self", ".", "_top_k", "=", "top_k", "\n", "self", ".", "_tie_break", "=", "tie_break", "\n", "self", ".", "correct_count", "=", "0.0", "\n", "self", ".", "total_count", "=", "0.0", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.metrics.categorical_accuracy.CategoricalAccuracy.__call__": [[31, 97], ["categorical_accuracy.CategoricalAccuracy.unwrap_to_tensors", "predictions.view.view.size", "predictions.view.view.view", "gold_labels.view().long.view().long.view().long", "max_predictions_mask[].float.sum", "gold_labels.view().long.view().long.dim", "allennlp.common.checks.ConfigurationError", "allennlp.common.checks.ConfigurationError", "[].unsqueeze.eq().float", "predictions.view.view.eq", "max_predictions_mask[].float", "predictions.view.eq.sum", "predictions.eq.sum.float", "max_predictions_mask[].float.unsqueeze_", "mask.view().float", "mask.sum", "gold_labels.view().long.view().long.numel", "predictions.view.view.dim", "gold_labels.view().long.view().long.view", "[].unsqueeze", "predictions.view.view.max", "max_predictions.unsqueeze", "predictions.view.view.size", "predictions.view.view.topk", "[].unsqueeze.eq", "mask.view", "min", "gold_labels.view().long.view().long.unsqueeze", "predictions.view.view.max", "torch.arange().long", "torch.arange", "gold_labels.view().long.view().long.numel"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.metrics.metric.Metric.unwrap_to_tensors"], ["", "def", "__call__", "(", "\n", "self", ",", "\n", "predictions", ":", "torch", ".", "Tensor", ",", "\n", "gold_labels", ":", "torch", ".", "Tensor", ",", "\n", "mask", ":", "Optional", "[", "torch", ".", "Tensor", "]", "=", "None", ",", "\n", ")", ":", "\n", "        ", "\"\"\"\n        # Parameters\n\n        predictions : `torch.Tensor`, required.\n            A tensor of predictions of shape (batch_size, ..., num_classes).\n        gold_labels : `torch.Tensor`, required.\n            A tensor of integer class label of shape (batch_size, ...). It must be the same\n            shape as the `predictions` tensor without the `num_classes` dimension.\n        mask : `torch.Tensor`, optional (default = None).\n            A masking tensor the same size as `gold_labels`.\n        \"\"\"", "\n", "predictions", ",", "gold_labels", ",", "mask", "=", "self", ".", "unwrap_to_tensors", "(", "\n", "predictions", ",", "gold_labels", ",", "mask", "\n", ")", "\n", "\n", "# Some sanity checks.", "\n", "num_classes", "=", "predictions", ".", "size", "(", "-", "1", ")", "\n", "if", "gold_labels", ".", "dim", "(", ")", "!=", "predictions", ".", "dim", "(", ")", "-", "1", ":", "\n", "            ", "raise", "ConfigurationError", "(", "\n", "\"gold_labels must have dimension == predictions.size() - 1 but \"", "\n", "\"found tensor of shape: {}\"", ".", "format", "(", "predictions", ".", "size", "(", ")", ")", "\n", ")", "\n", "", "if", "(", "gold_labels", ">=", "num_classes", ")", ".", "any", "(", ")", ":", "\n", "            ", "raise", "ConfigurationError", "(", "\n", "\"A gold label passed to Categorical Accuracy contains an id >= {}, \"", "\n", "\"the number of classes.\"", ".", "format", "(", "num_classes", ")", "\n", ")", "\n", "\n", "", "predictions", "=", "predictions", ".", "view", "(", "(", "-", "1", ",", "num_classes", ")", ")", "\n", "gold_labels", "=", "gold_labels", ".", "view", "(", "-", "1", ")", ".", "long", "(", ")", "\n", "if", "not", "self", ".", "_tie_break", ":", "\n", "# Top K indexes of the predictions (or fewer, if there aren't K of them).", "\n", "# Special case topk == 1, because it's common and .max() is much faster than .topk().", "\n", "            ", "if", "self", ".", "_top_k", "==", "1", ":", "\n", "                ", "top_k", "=", "predictions", ".", "max", "(", "-", "1", ")", "[", "1", "]", ".", "unsqueeze", "(", "-", "1", ")", "\n", "", "else", ":", "\n", "                ", "top_k", "=", "predictions", ".", "topk", "(", "min", "(", "self", ".", "_top_k", ",", "predictions", ".", "shape", "[", "-", "1", "]", ")", ",", "-", "1", ")", "[", "1", "]", "\n", "\n", "# This is of shape (batch_size, ..., top_k).", "\n", "", "correct", "=", "top_k", ".", "eq", "(", "gold_labels", ".", "unsqueeze", "(", "-", "1", ")", ")", ".", "float", "(", ")", "\n", "", "else", ":", "\n", "# prediction is correct if gold label falls on any of the max scores. distribute score by tie_counts", "\n", "            ", "max_predictions", "=", "predictions", ".", "max", "(", "-", "1", ")", "[", "0", "]", "\n", "max_predictions_mask", "=", "predictions", ".", "eq", "(", "max_predictions", ".", "unsqueeze", "(", "-", "1", ")", ")", "\n", "# max_predictions_mask is (rows X num_classes) and gold_labels is (batch_size)", "\n", "# ith entry in gold_labels points to index (0-num_classes) for ith row in max_predictions", "\n", "# For each row check if index pointed by gold_label is was 1 or not (among max scored classes)", "\n", "correct", "=", "max_predictions_mask", "[", "\n", "torch", ".", "arange", "(", "gold_labels", ".", "numel", "(", ")", ")", ".", "long", "(", ")", ",", "gold_labels", "\n", "]", ".", "float", "(", ")", "\n", "tie_counts", "=", "max_predictions_mask", ".", "sum", "(", "-", "1", ")", "\n", "correct", "/=", "tie_counts", ".", "float", "(", ")", "\n", "correct", ".", "unsqueeze_", "(", "-", "1", ")", "\n", "\n", "", "if", "mask", "is", "not", "None", ":", "\n", "            ", "correct", "*=", "mask", ".", "view", "(", "-", "1", ",", "1", ")", ".", "float", "(", ")", "\n", "self", ".", "total_count", "+=", "mask", ".", "sum", "(", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "total_count", "+=", "gold_labels", ".", "numel", "(", ")", "\n", "", "self", ".", "correct_count", "+=", "correct", ".", "sum", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.metrics.categorical_accuracy.CategoricalAccuracy.get_metric": [[98, 111], ["categorical_accuracy.CategoricalAccuracy.reset", "float", "float"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.metrics.sequence_accuracy.SequenceAccuracy.reset"], ["", "def", "get_metric", "(", "self", ",", "reset", ":", "bool", "=", "False", ")", ":", "\n", "        ", "\"\"\"\n        # Returns\n\n        The accumulated accuracy.\n        \"\"\"", "\n", "if", "self", ".", "total_count", ">", "1e-12", ":", "\n", "            ", "accuracy", "=", "float", "(", "self", ".", "correct_count", ")", "/", "float", "(", "self", ".", "total_count", ")", "\n", "", "else", ":", "\n", "            ", "accuracy", "=", "0.0", "\n", "", "if", "reset", ":", "\n", "            ", "self", ".", "reset", "(", ")", "\n", "", "return", "accuracy", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.metrics.categorical_accuracy.CategoricalAccuracy.reset": [[112, 116], ["None"], "methods", ["None"], ["", "@", "overrides", "\n", "def", "reset", "(", "self", ")", ":", "\n", "        ", "self", ".", "correct_count", "=", "0.0", "\n", "self", ".", "total_count", "=", "0.0", "\n", "", "", ""]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.metrics.pearson_correlation.PearsonCorrelation.__init__": [[37, 41], ["allennlp.training.metrics.covariance.Covariance", "allennlp.training.metrics.covariance.Covariance", "allennlp.training.metrics.covariance.Covariance"], "methods", ["None"], ["def", "__init__", "(", "self", ")", "->", "None", ":", "\n", "        ", "self", ".", "_predictions_labels_covariance", "=", "Covariance", "(", ")", "\n", "self", ".", "_predictions_variance", "=", "Covariance", "(", ")", "\n", "self", ".", "_labels_variance", "=", "Covariance", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.metrics.pearson_correlation.PearsonCorrelation.__call__": [[42, 64], ["pearson_correlation.PearsonCorrelation.unwrap_to_tensors", "pearson_correlation.PearsonCorrelation._predictions_labels_covariance", "pearson_correlation.PearsonCorrelation._predictions_variance", "pearson_correlation.PearsonCorrelation._labels_variance"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.metrics.metric.Metric.unwrap_to_tensors"], ["", "def", "__call__", "(", "\n", "self", ",", "\n", "predictions", ":", "torch", ".", "Tensor", ",", "\n", "gold_labels", ":", "torch", ".", "Tensor", ",", "\n", "mask", ":", "Optional", "[", "torch", ".", "Tensor", "]", "=", "None", ",", "\n", ")", ":", "\n", "        ", "\"\"\"\n        # Parameters\n\n        predictions : `torch.Tensor`, required.\n            A tensor of predictions of shape (batch_size, ...).\n        gold_labels : `torch.Tensor`, required.\n            A tensor of the same shape as `predictions`.\n        mask : `torch.Tensor`, optional (default = None).\n            A tensor of the same shape as `predictions`.\n        \"\"\"", "\n", "predictions", ",", "gold_labels", ",", "mask", "=", "self", ".", "unwrap_to_tensors", "(", "\n", "predictions", ",", "gold_labels", ",", "mask", "\n", ")", "\n", "self", ".", "_predictions_labels_covariance", "(", "predictions", ",", "gold_labels", ",", "mask", ")", "\n", "self", ".", "_predictions_variance", "(", "predictions", ",", "predictions", ",", "mask", ")", "\n", "self", ".", "_labels_variance", "(", "gold_labels", ",", "gold_labels", ",", "mask", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.metrics.pearson_correlation.PearsonCorrelation.get_metric": [[65, 82], ["pearson_correlation.PearsonCorrelation._predictions_labels_covariance.get_metric", "pearson_correlation.PearsonCorrelation._predictions_variance.get_metric", "pearson_correlation.PearsonCorrelation._labels_variance.get_metric", "pearson_correlation.PearsonCorrelation.reset", "math.sqrt", "math.sqrt", "numpy.around"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.metrics.sequence_accuracy.SequenceAccuracy.get_metric", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.metrics.sequence_accuracy.SequenceAccuracy.get_metric", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.metrics.sequence_accuracy.SequenceAccuracy.get_metric", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.metrics.sequence_accuracy.SequenceAccuracy.reset"], ["", "def", "get_metric", "(", "self", ",", "reset", ":", "bool", "=", "False", ")", ":", "\n", "        ", "\"\"\"\n        # Returns\n\n        The accumulated sample Pearson correlation.\n        \"\"\"", "\n", "covariance", "=", "self", ".", "_predictions_labels_covariance", ".", "get_metric", "(", "reset", "=", "reset", ")", "\n", "predictions_variance", "=", "self", ".", "_predictions_variance", ".", "get_metric", "(", "reset", "=", "reset", ")", "\n", "labels_variance", "=", "self", ".", "_labels_variance", ".", "get_metric", "(", "reset", "=", "reset", ")", "\n", "if", "reset", ":", "\n", "            ", "self", ".", "reset", "(", ")", "\n", "", "denominator", "=", "math", ".", "sqrt", "(", "predictions_variance", ")", "*", "math", ".", "sqrt", "(", "labels_variance", ")", "\n", "if", "np", ".", "around", "(", "denominator", ",", "decimals", "=", "5", ")", "==", "0", ":", "\n", "            ", "pearson_r", "=", "0", "\n", "", "else", ":", "\n", "            ", "pearson_r", "=", "covariance", "/", "denominator", "\n", "", "return", "pearson_r", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.metrics.pearson_correlation.PearsonCorrelation.reset": [[83, 88], ["pearson_correlation.PearsonCorrelation._predictions_labels_covariance.reset", "pearson_correlation.PearsonCorrelation._predictions_variance.reset", "pearson_correlation.PearsonCorrelation._labels_variance.reset"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.metrics.sequence_accuracy.SequenceAccuracy.reset", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.metrics.sequence_accuracy.SequenceAccuracy.reset", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.metrics.sequence_accuracy.SequenceAccuracy.reset"], ["", "@", "overrides", "\n", "def", "reset", "(", "self", ")", ":", "\n", "        ", "self", ".", "_predictions_labels_covariance", ".", "reset", "(", ")", "\n", "self", ".", "_predictions_variance", ".", "reset", "(", ")", "\n", "self", ".", "_labels_variance", ".", "reset", "(", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.metrics.covariance.Covariance.__init__": [[29, 34], ["None"], "methods", ["None"], ["def", "__init__", "(", "self", ")", "->", "None", ":", "\n", "        ", "self", ".", "_total_prediction_mean", "=", "0.0", "\n", "self", ".", "_total_label_mean", "=", "0.0", "\n", "self", ".", "_total_co_moment", "=", "0.0", "\n", "self", ".", "_total_count", "=", "0.0", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.metrics.covariance.Covariance.__call__": [[35, 104], ["covariance.Covariance.unwrap_to_tensors", "predictions.view.view.view", "gold_labels.view.view.view", "delta_mean_prediction.item", "delta_mean_label.item", "delta_co_moment.item", "mask.view.view.view", "torch.sum().item", "gold_labels.view.view.numel", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.metrics.metric.Metric.unwrap_to_tensors"], ["", "def", "__call__", "(", "\n", "self", ",", "\n", "predictions", ":", "torch", ".", "Tensor", ",", "\n", "gold_labels", ":", "torch", ".", "Tensor", ",", "\n", "mask", ":", "Optional", "[", "torch", ".", "Tensor", "]", "=", "None", ",", "\n", ")", ":", "\n", "        ", "\"\"\"\n        # Parameters\n\n        predictions : `torch.Tensor`, required.\n            A tensor of predictions of shape (batch_size, ...).\n        gold_labels : `torch.Tensor`, required.\n            A tensor of the same shape as `predictions`.\n        mask : `torch.Tensor`, optional (default = None).\n            A tensor of the same shape as `predictions`.\n        \"\"\"", "\n", "predictions", ",", "gold_labels", ",", "mask", "=", "self", ".", "unwrap_to_tensors", "(", "\n", "predictions", ",", "gold_labels", ",", "mask", "\n", ")", "\n", "# Flatten predictions, gold_labels, and mask. We calculate the covariance between", "\n", "# the vectors, since each element in the predictions and gold_labels tensor is assumed", "\n", "# to be a separate observation.", "\n", "predictions", "=", "predictions", ".", "view", "(", "-", "1", ")", "\n", "gold_labels", "=", "gold_labels", ".", "view", "(", "-", "1", ")", "\n", "\n", "if", "mask", "is", "not", "None", ":", "\n", "            ", "mask", "=", "mask", ".", "view", "(", "-", "1", ")", "\n", "predictions", "=", "predictions", "*", "mask", "\n", "gold_labels", "=", "gold_labels", "*", "mask", "\n", "num_batch_items", "=", "torch", ".", "sum", "(", "mask", ")", ".", "item", "(", ")", "\n", "", "else", ":", "\n", "            ", "num_batch_items", "=", "gold_labels", ".", "numel", "(", ")", "\n", "\n", "# Note that self._total_count must be a float or int at all times", "\n", "# If it is a 1-dimension Tensor, the previous count will equal the updated_count.", "\n", "# The sampe applies for previous_total_prediction_mean and", "\n", "# previous_total_label_mean below -- we handle this in the code by", "\n", "# calling .item() judiciously.", "\n", "", "previous_count", "=", "self", ".", "_total_count", "\n", "updated_count", "=", "self", ".", "_total_count", "+", "num_batch_items", "\n", "\n", "batch_mean_prediction", "=", "torch", ".", "sum", "(", "predictions", ")", "/", "num_batch_items", "\n", "delta_mean_prediction", "=", "(", "\n", "(", "batch_mean_prediction", "-", "self", ".", "_total_prediction_mean", ")", "*", "num_batch_items", "\n", ")", "/", "updated_count", "\n", "previous_total_prediction_mean", "=", "self", ".", "_total_prediction_mean", "\n", "self", ".", "_total_prediction_mean", "+=", "delta_mean_prediction", ".", "item", "(", ")", "\n", "\n", "batch_mean_label", "=", "torch", ".", "sum", "(", "gold_labels", ")", "/", "num_batch_items", "\n", "delta_mean_label", "=", "(", "\n", "(", "batch_mean_label", "-", "self", ".", "_total_label_mean", ")", "*", "num_batch_items", "\n", ")", "/", "updated_count", "\n", "previous_total_label_mean", "=", "self", ".", "_total_label_mean", "\n", "self", ".", "_total_label_mean", "+=", "delta_mean_label", ".", "item", "(", ")", "\n", "\n", "batch_coresiduals", "=", "(", "predictions", "-", "batch_mean_prediction", ")", "*", "(", "\n", "gold_labels", "-", "batch_mean_label", "\n", ")", "\n", "if", "mask", "is", "not", "None", ":", "\n", "            ", "batch_co_moment", "=", "torch", ".", "sum", "(", "batch_coresiduals", "*", "mask", ")", "\n", "", "else", ":", "\n", "            ", "batch_co_moment", "=", "torch", ".", "sum", "(", "batch_coresiduals", ")", "\n", "", "delta_co_moment", "=", "batch_co_moment", "+", "(", "\n", "previous_total_prediction_mean", "-", "batch_mean_prediction", "\n", ")", "*", "(", "previous_total_label_mean", "-", "batch_mean_label", ")", "*", "(", "\n", "previous_count", "*", "num_batch_items", "/", "updated_count", "\n", ")", "\n", "self", ".", "_total_co_moment", "+=", "delta_co_moment", ".", "item", "(", ")", "\n", "self", ".", "_total_count", "=", "updated_count", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.metrics.covariance.Covariance.get_metric": [[105, 115], ["covariance.Covariance.reset"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.metrics.sequence_accuracy.SequenceAccuracy.reset"], ["", "def", "get_metric", "(", "self", ",", "reset", ":", "bool", "=", "False", ")", ":", "\n", "        ", "\"\"\"\n        # Returns\n\n        The accumulated covariance.\n        \"\"\"", "\n", "covariance", "=", "self", ".", "_total_co_moment", "/", "(", "self", ".", "_total_count", "-", "1", ")", "\n", "if", "reset", ":", "\n", "            ", "self", ".", "reset", "(", ")", "\n", "", "return", "covariance", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.metrics.covariance.Covariance.reset": [[116, 122], ["None"], "methods", ["None"], ["", "@", "overrides", "\n", "def", "reset", "(", "self", ")", ":", "\n", "        ", "self", ".", "_total_prediction_mean", "=", "0.0", "\n", "self", ".", "_total_label_mean", "=", "0.0", "\n", "self", ".", "_total_co_moment", "=", "0.0", "\n", "self", ".", "_total_count", "=", "0.0", "\n", "", "", ""]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.metrics.mention_recall.MentionRecall.__init__": [[11, 14], ["None"], "methods", ["None"], ["    ", "def", "__init__", "(", "self", ")", "->", "None", ":", "\n", "        ", "self", ".", "_num_gold_mentions", "=", "0", "\n", "self", ".", "_num_recalled_mentions", "=", "0", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.metrics.mention_recall.MentionRecall.__call__": [[15, 33], ["zip", "batched_top_spans.data.tolist", "len", "len"], "methods", ["None"], ["", "@", "overrides", "\n", "def", "__call__", "(", "\n", "self", ",", "# type: ignore", "\n", "batched_top_spans", ":", "torch", ".", "Tensor", ",", "\n", "batched_metadata", ":", "List", "[", "Dict", "[", "str", ",", "Any", "]", "]", ",", "\n", ")", ":", "\n", "        ", "for", "top_spans", ",", "metadata", "in", "zip", "(", "\n", "batched_top_spans", ".", "data", ".", "tolist", "(", ")", ",", "batched_metadata", "\n", ")", ":", "\n", "\n", "            ", "gold_mentions", ":", "Set", "[", "Tuple", "[", "int", ",", "int", "]", "]", "=", "{", "\n", "mention", "for", "cluster", "in", "metadata", "[", "\"clusters\"", "]", "for", "mention", "in", "cluster", "\n", "}", "\n", "predicted_spans", ":", "Set", "[", "Tuple", "[", "int", ",", "int", "]", "]", "=", "{", "\n", "(", "span", "[", "0", "]", ",", "span", "[", "1", "]", ")", "for", "span", "in", "top_spans", "\n", "}", "\n", "self", ".", "_num_gold_mentions", "+=", "len", "(", "gold_mentions", ")", "\n", "self", ".", "_num_recalled_mentions", "+=", "len", "(", "gold_mentions", "&", "predicted_spans", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.metrics.mention_recall.MentionRecall.get_metric": [[34, 43], ["mention_recall.MentionRecall.reset", "float"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.metrics.sequence_accuracy.SequenceAccuracy.reset"], ["", "", "@", "overrides", "\n", "def", "get_metric", "(", "self", ",", "reset", ":", "bool", "=", "False", ")", "->", "float", ":", "\n", "        ", "if", "self", ".", "_num_gold_mentions", "==", "0", ":", "\n", "            ", "recall", "=", "0.0", "\n", "", "else", ":", "\n", "            ", "recall", "=", "self", ".", "_num_recalled_mentions", "/", "float", "(", "self", ".", "_num_gold_mentions", ")", "\n", "", "if", "reset", ":", "\n", "            ", "self", ".", "reset", "(", ")", "\n", "", "return", "recall", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.metrics.mention_recall.MentionRecall.reset": [[44, 48], ["None"], "methods", ["None"], ["", "@", "overrides", "\n", "def", "reset", "(", "self", ")", ":", "\n", "        ", "self", ".", "_num_gold_mentions", "=", "0", "\n", "self", ".", "_num_recalled_mentions", "=", "0", "\n", "", "", ""]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.metrics.srl_eval_scorer.SrlEvalScorer.__init__": [[48, 59], ["set", "collections.defaultdict", "collections.defaultdict", "collections.defaultdict"], "methods", ["None"], ["def", "__init__", "(", "\n", "self", ",", "\n", "srl_eval_path", ":", "str", "=", "DEFAULT_SRL_EVAL_PATH", ",", "\n", "ignore_classes", ":", "List", "[", "str", "]", "=", "None", ",", "\n", ")", "->", "None", ":", "\n", "        ", "self", ".", "_srl_eval_path", "=", "srl_eval_path", "\n", "self", ".", "_ignore_classes", "=", "set", "(", "ignore_classes", ")", "\n", "# These will hold per label span counts.", "\n", "self", ".", "_true_positives", ":", "Dict", "[", "str", ",", "int", "]", "=", "defaultdict", "(", "int", ")", "\n", "self", ".", "_false_positives", ":", "Dict", "[", "str", ",", "int", "]", "=", "defaultdict", "(", "int", ")", "\n", "self", ".", "_false_negatives", ":", "Dict", "[", "str", ",", "int", "]", "=", "defaultdict", "(", "int", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.metrics.srl_eval_scorer.SrlEvalScorer.__call__": [[60, 134], ["tempfile.mkdtemp", "os.path.join", "os.path.join", "subprocess.run", "subprocess.run.stdout.split", "shutil.rmtree", "os.path.exists", "allennlp.common.checks.ConfigurationError", "open", "open", "zip", "line.strip().split", "allennlp.models.srl_util.write_conll_formatted_tags_to_file", "len", "int", "int", "int", "line.strip"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.dataset_readers.multiprocess_dataset_reader.QIterable.join", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.dataset_readers.multiprocess_dataset_reader.QIterable.join", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.commands.train.TrainModel.run", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.models.srl_util.write_conll_formatted_tags_to_file"], ["", "@", "overrides", "\n", "def", "__call__", "(", "\n", "self", ",", "# type: ignore", "\n", "batch_verb_indices", ":", "List", "[", "Optional", "[", "int", "]", "]", ",", "\n", "batch_sentences", ":", "List", "[", "List", "[", "str", "]", "]", ",", "\n", "batch_conll_formatted_predicted_tags", ":", "List", "[", "List", "[", "str", "]", "]", ",", "\n", "batch_conll_formatted_gold_tags", ":", "List", "[", "List", "[", "str", "]", "]", ",", "\n", ")", "->", "None", ":", "\n", "\n", "        ", "\"\"\"\n        # Parameters\n\n        batch_verb_indices : `List[Optional[int]]`, required.\n            The indices of the verbal predicate in the sentences which\n            the gold labels are the arguments for, or None if the sentence\n            contains no verbal predicate.\n        batch_sentences : `List[List[str]]`, required.\n            The word tokens for each instance in the batch.\n        batch_conll_formatted_predicted_tags : `List[List[str]]`, required.\n            A list of predicted CoNLL-formatted SRL tags (itself a list) to compute score for.\n            Use allennlp.models.semantic_role_labeler.convert_bio_tags_to_conll_format\n            to convert from BIO to CoNLL format before passing the tags into the metric,\n            if applicable.\n        batch_conll_formatted_gold_tags : `List[List[str]]`, required.\n            A list of gold CoNLL-formatted SRL tags (itself a list) to use as a reference.\n            Use allennlp.models.semantic_role_labeler.convert_bio_tags_to_conll_format\n            to convert from BIO to CoNLL format before passing the\n            tags into the metric, if applicable.\n        \"\"\"", "\n", "if", "not", "os", ".", "path", ".", "exists", "(", "self", ".", "_srl_eval_path", ")", ":", "\n", "            ", "raise", "ConfigurationError", "(", "f\"srl-eval.pl not found at {self._srl_eval_path}.\"", ")", "\n", "", "tempdir", "=", "tempfile", ".", "mkdtemp", "(", ")", "\n", "gold_path", "=", "os", ".", "path", ".", "join", "(", "tempdir", ",", "\"gold.txt\"", ")", "\n", "predicted_path", "=", "os", ".", "path", ".", "join", "(", "tempdir", ",", "\"predicted.txt\"", ")", "\n", "\n", "with", "open", "(", "predicted_path", ",", "\"w\"", ",", "encoding", "=", "\"utf-8\"", ")", "as", "predicted_file", ",", "open", "(", "\n", "gold_path", ",", "\"w\"", ",", "encoding", "=", "\"utf-8\"", "\n", ")", "as", "gold_file", ":", "\n", "            ", "for", "verb_index", ",", "sentence", ",", "predicted_tag_sequence", ",", "gold_tag_sequence", "in", "zip", "(", "\n", "batch_verb_indices", ",", "\n", "batch_sentences", ",", "\n", "batch_conll_formatted_predicted_tags", ",", "\n", "batch_conll_formatted_gold_tags", ",", "\n", ")", ":", "\n", "                ", "write_conll_formatted_tags_to_file", "(", "\n", "predicted_file", ",", "\n", "gold_file", ",", "\n", "verb_index", ",", "\n", "sentence", ",", "\n", "predicted_tag_sequence", ",", "\n", "gold_tag_sequence", ",", "\n", ")", "\n", "", "", "perl_script_command", "=", "[", "\"perl\"", ",", "self", ".", "_srl_eval_path", ",", "gold_path", ",", "predicted_path", "]", "\n", "completed_process", "=", "subprocess", ".", "run", "(", "\n", "perl_script_command", ",", "\n", "stdout", "=", "subprocess", ".", "PIPE", ",", "\n", "universal_newlines", "=", "True", ",", "\n", "check", "=", "True", ",", "\n", ")", "\n", "for", "line", "in", "completed_process", ".", "stdout", ".", "split", "(", "\"\\n\"", ")", ":", "\n", "            ", "stripped", "=", "line", ".", "strip", "(", ")", ".", "split", "(", ")", "\n", "if", "len", "(", "stripped", ")", "==", "7", ":", "\n", "                ", "tag", "=", "stripped", "[", "0", "]", "\n", "# Overall metrics are calculated in get_metric, skip them here.", "\n", "if", "tag", "==", "\"Overall\"", "or", "tag", "in", "self", ".", "_ignore_classes", ":", "\n", "                    ", "continue", "\n", "# This line contains results for a span", "\n", "", "num_correct", "=", "int", "(", "stripped", "[", "1", "]", ")", "\n", "num_excess", "=", "int", "(", "stripped", "[", "2", "]", ")", "\n", "num_missed", "=", "int", "(", "stripped", "[", "3", "]", ")", "\n", "self", ".", "_true_positives", "[", "tag", "]", "+=", "num_correct", "\n", "self", ".", "_false_positives", "[", "tag", "]", "+=", "num_excess", "\n", "self", ".", "_false_negatives", "[", "tag", "]", "+=", "num_missed", "\n", "", "", "shutil", ".", "rmtree", "(", "tempdir", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.metrics.srl_eval_scorer.SrlEvalScorer.get_metric": [[135, 182], ["set", "all_tags.update", "all_tags.update", "all_tags.update", "srl_eval_scorer.SrlEvalScorer._compute_metrics", "srl_eval_scorer.SrlEvalScorer._true_positives.keys", "srl_eval_scorer.SrlEvalScorer._false_positives.keys", "srl_eval_scorer.SrlEvalScorer._false_negatives.keys", "srl_eval_scorer.SrlEvalScorer._compute_metrics", "sum", "sum", "sum", "srl_eval_scorer.SrlEvalScorer.reset", "ValueError", "srl_eval_scorer.SrlEvalScorer._true_positives.values", "srl_eval_scorer.SrlEvalScorer._false_positives.values", "srl_eval_scorer.SrlEvalScorer._false_negatives.values"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.metrics.conll_coref_scores.Scorer.update", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.metrics.conll_coref_scores.Scorer.update", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.metrics.conll_coref_scores.Scorer.update", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.metrics.span_based_f1_measure.SpanBasedF1Measure._compute_metrics", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.metrics.span_based_f1_measure.SpanBasedF1Measure._compute_metrics", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.metrics.sequence_accuracy.SequenceAccuracy.reset"], ["", "def", "get_metric", "(", "self", ",", "reset", ":", "bool", "=", "False", ")", ":", "\n", "        ", "\"\"\"\n        # Returns\n\n        A Dict per label containing following the span based metrics:\n        precision : float\n        recall : float\n        f1-measure : float\n\n        Additionally, an `overall` key is included, which provides the precision,\n        recall and f1-measure for all spans.\n        \"\"\"", "\n", "all_tags", ":", "Set", "[", "str", "]", "=", "set", "(", ")", "\n", "all_tags", ".", "update", "(", "self", ".", "_true_positives", ".", "keys", "(", ")", ")", "\n", "all_tags", ".", "update", "(", "self", ".", "_false_positives", ".", "keys", "(", ")", ")", "\n", "all_tags", ".", "update", "(", "self", ".", "_false_negatives", ".", "keys", "(", ")", ")", "\n", "all_metrics", "=", "{", "}", "\n", "for", "tag", "in", "all_tags", ":", "\n", "            ", "if", "tag", "==", "\"overall\"", ":", "\n", "                ", "raise", "ValueError", "(", "\n", "\"'overall' is disallowed as a tag type, \"", "\n", "\"rename the tag type to something else if necessary.\"", "\n", ")", "\n", "", "precision", ",", "recall", ",", "f1_measure", "=", "self", ".", "_compute_metrics", "(", "\n", "self", ".", "_true_positives", "[", "tag", "]", ",", "\n", "self", ".", "_false_positives", "[", "tag", "]", ",", "\n", "self", ".", "_false_negatives", "[", "tag", "]", ",", "\n", ")", "\n", "precision_key", "=", "\"precision\"", "+", "\"-\"", "+", "tag", "\n", "recall_key", "=", "\"recall\"", "+", "\"-\"", "+", "tag", "\n", "f1_key", "=", "\"f1-measure\"", "+", "\"-\"", "+", "tag", "\n", "all_metrics", "[", "precision_key", "]", "=", "precision", "\n", "all_metrics", "[", "recall_key", "]", "=", "recall", "\n", "all_metrics", "[", "f1_key", "]", "=", "f1_measure", "\n", "\n", "# Compute the precision, recall and f1 for all spans jointly.", "\n", "", "precision", ",", "recall", ",", "f1_measure", "=", "self", ".", "_compute_metrics", "(", "\n", "sum", "(", "self", ".", "_true_positives", ".", "values", "(", ")", ")", ",", "\n", "sum", "(", "self", ".", "_false_positives", ".", "values", "(", ")", ")", ",", "\n", "sum", "(", "self", ".", "_false_negatives", ".", "values", "(", ")", ")", ",", "\n", ")", "\n", "all_metrics", "[", "\"precision-overall\"", "]", "=", "precision", "\n", "all_metrics", "[", "\"recall-overall\"", "]", "=", "recall", "\n", "all_metrics", "[", "\"f1-measure-overall\"", "]", "=", "f1_measure", "\n", "if", "reset", ":", "\n", "            ", "self", ".", "reset", "(", ")", "\n", "", "return", "all_metrics", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.metrics.srl_eval_scorer.SrlEvalScorer._compute_metrics": [[183, 193], ["float", "float", "float", "float"], "methods", ["None"], ["", "@", "staticmethod", "\n", "def", "_compute_metrics", "(", "\n", "true_positives", ":", "int", ",", "false_positives", ":", "int", ",", "false_negatives", ":", "int", "\n", ")", ":", "\n", "        ", "precision", "=", "float", "(", "true_positives", ")", "/", "float", "(", "\n", "true_positives", "+", "false_positives", "+", "1e-13", "\n", ")", "\n", "recall", "=", "float", "(", "true_positives", ")", "/", "float", "(", "true_positives", "+", "false_negatives", "+", "1e-13", ")", "\n", "f1_measure", "=", "2.0", "*", "(", "(", "precision", "*", "recall", ")", "/", "(", "precision", "+", "recall", "+", "1e-13", ")", ")", "\n", "return", "precision", ",", "recall", ",", "f1_measure", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.metrics.srl_eval_scorer.SrlEvalScorer.reset": [[194, 198], ["collections.defaultdict", "collections.defaultdict", "collections.defaultdict"], "methods", ["None"], ["", "def", "reset", "(", "self", ")", ":", "\n", "        ", "self", ".", "_true_positives", "=", "defaultdict", "(", "int", ")", "\n", "self", ".", "_false_positives", "=", "defaultdict", "(", "int", ")", "\n", "self", ".", "_false_negatives", "=", "defaultdict", "(", "int", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.metrics.unigram_recall.UnigramRecall.__init__": [[20, 23], ["None"], "methods", ["None"], ["def", "__init__", "(", "self", ")", "->", "None", ":", "\n", "        ", "self", ".", "correct_count", "=", "0.0", "\n", "self", ".", "total_count", "=", "0.0", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.metrics.unigram_recall.UnigramRecall.__call__": [[24, 83], ["unigram_recall.UnigramRecall.unwrap_to_tensors", "range", "gold_labels.dim", "allennlp.common.checks.ConfigurationError", "allennlp.common.checks.ConfigurationError", "predictions.size", "predictions.size", "predictions.dim", "mask.size", "gold_labels.size", "gold_labels.size", "mask.size", "float", "len"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.metrics.metric.Metric.unwrap_to_tensors"], ["", "def", "__call__", "(", "\n", "self", ",", "\n", "predictions", ":", "torch", ".", "Tensor", ",", "\n", "gold_labels", ":", "torch", ".", "Tensor", ",", "\n", "mask", ":", "Optional", "[", "torch", ".", "Tensor", "]", "=", "None", ",", "\n", "end_index", ":", "int", "=", "sys", ".", "maxsize", ",", "\n", ")", ":", "\n", "        ", "\"\"\"\n        # Parameters\n\n        predictions : `torch.Tensor`, required.\n            A tensor of predictions of shape (batch_size, k, sequence_length).\n        gold_labels : `torch.Tensor`, required.\n            A tensor of integer class label of shape (batch_size, sequence_length).\n        mask : `torch.Tensor`, optional (default = None).\n            A masking tensor the same size as `gold_labels`.\n        \"\"\"", "\n", "predictions", ",", "gold_labels", ",", "mask", "=", "self", ".", "unwrap_to_tensors", "(", "\n", "predictions", ",", "gold_labels", ",", "mask", "\n", ")", "\n", "\n", "# Some sanity checks.", "\n", "if", "gold_labels", ".", "dim", "(", ")", "!=", "predictions", ".", "dim", "(", ")", "-", "1", ":", "\n", "            ", "raise", "ConfigurationError", "(", "\n", "\"gold_labels must have dimension == predictions.dim() - 1 but \"", "\n", "\"found tensor of shape: {}\"", ".", "format", "(", "gold_labels", ".", "size", "(", ")", ")", "\n", ")", "\n", "", "if", "mask", "is", "not", "None", "and", "mask", ".", "size", "(", ")", "!=", "gold_labels", ".", "size", "(", ")", ":", "\n", "            ", "raise", "ConfigurationError", "(", "\n", "\"mask must have the same size as predictions but \"", "\n", "\"found tensor of shape: {}\"", ".", "format", "(", "mask", ".", "size", "(", ")", ")", "\n", ")", "\n", "\n", "", "batch_size", "=", "predictions", ".", "size", "(", ")", "[", "0", "]", "\n", "correct", "=", "0.0", "\n", "for", "i", "in", "range", "(", "batch_size", ")", ":", "\n", "            ", "beams", "=", "predictions", "[", "i", "]", "\n", "cur_gold", "=", "gold_labels", "[", "i", "]", "\n", "\n", "if", "mask", "is", "not", "None", ":", "\n", "                ", "masked_gold", "=", "cur_gold", "*", "mask", "[", "i", "]", "\n", "", "else", ":", "\n", "                ", "masked_gold", "=", "cur_gold", "\n", "", "cleaned_gold", "=", "[", "x", "for", "x", "in", "masked_gold", "if", "x", "not", "in", "(", "0", ",", "end_index", ")", "]", "\n", "\n", "retval", "=", "0.0", "\n", "for", "word", "in", "cleaned_gold", ":", "\n", "                ", "stillsearch", "=", "True", "\n", "for", "beam", "in", "beams", ":", "\n", "# word is from cleaned gold which doesn't have 0 or", "\n", "# end_index, so we don't need to explicitly remove those", "\n", "# from beam.", "\n", "                    ", "if", "stillsearch", "and", "(", "word", "in", "beam", ")", ":", "\n", "                        ", "retval", "+=", "1.0", "/", "float", "(", "len", "(", "cleaned_gold", ")", ")", "\n", "stillsearch", "=", "False", "\n", "", "", "", "correct", "+=", "retval", "\n", "\n", "", "self", ".", "correct_count", "+=", "correct", "\n", "self", ".", "total_count", "+=", "predictions", ".", "size", "(", ")", "[", "0", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.metrics.unigram_recall.UnigramRecall.get_metric": [[84, 98], ["unigram_recall.UnigramRecall.reset", "float", "float"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.metrics.sequence_accuracy.SequenceAccuracy.reset"], ["", "def", "get_metric", "(", "self", ",", "reset", ":", "bool", "=", "False", ")", ":", "\n", "        ", "\"\"\"\n        # Returns\n\n        The accumulated recall.\n        \"\"\"", "\n", "recall", "=", "(", "\n", "float", "(", "self", ".", "correct_count", ")", "/", "float", "(", "self", ".", "total_count", ")", "\n", "if", "self", ".", "total_count", ">", "0", "\n", "else", "0", "\n", ")", "\n", "if", "reset", ":", "\n", "            ", "self", ".", "reset", "(", ")", "\n", "", "return", "recall", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.metrics.unigram_recall.UnigramRecall.reset": [[99, 103], ["None"], "methods", ["None"], ["", "@", "overrides", "\n", "def", "reset", "(", "self", ")", ":", "\n", "        ", "self", ".", "correct_count", "=", "0.0", "\n", "self", ".", "total_count", "=", "0.0", "\n", "", "", ""]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.metrics.boolean_accuracy.BooleanAccuracy.__init__": [[26, 29], ["None"], "methods", ["None"], ["def", "__init__", "(", "self", ")", "->", "None", ":", "\n", "        ", "self", ".", "_correct_count", "=", "0.0", "\n", "self", ".", "_total_count", "=", "0.0", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.metrics.boolean_accuracy.BooleanAccuracy.__call__": [[30, 89], ["boolean_accuracy.BooleanAccuracy.unwrap_to_tensors", "predictions.view.view.size", "predictions.view.view.view", "gold_labels.view.view.view", "predictions.view.view.eq().prod().float", "torch.ones().float.sum", "gold_labels.view.view.size", "predictions.view.view.size", "ValueError", "ValueError", "[].float", "torch.ones().float", "mask.size", "predictions.view.view.size", "predictions.view.view.eq().prod", "torch.ones", "gold_labels.view.view.size", "mask.size", "mask.view().max", "predictions.view.view.eq", "mask.view"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.metrics.metric.Metric.unwrap_to_tensors"], ["", "def", "__call__", "(", "\n", "self", ",", "\n", "predictions", ":", "torch", ".", "Tensor", ",", "\n", "gold_labels", ":", "torch", ".", "Tensor", ",", "\n", "mask", ":", "Optional", "[", "torch", ".", "Tensor", "]", "=", "None", ",", "\n", ")", ":", "\n", "        ", "\"\"\"\n        # Parameters\n\n        predictions : `torch.Tensor`, required.\n            A tensor of predictions of shape (batch_size, ...).\n        gold_labels : `torch.Tensor`, required.\n            A tensor of the same shape as `predictions`.\n        mask : `torch.Tensor`, optional (default = None).\n            A tensor of the same shape as `predictions`.\n        \"\"\"", "\n", "predictions", ",", "gold_labels", ",", "mask", "=", "self", ".", "unwrap_to_tensors", "(", "\n", "predictions", ",", "gold_labels", ",", "mask", "\n", ")", "\n", "\n", "# Some sanity checks.", "\n", "if", "gold_labels", ".", "size", "(", ")", "!=", "predictions", ".", "size", "(", ")", ":", "\n", "            ", "raise", "ValueError", "(", "\n", "f\"gold_labels must have shape == predictions.size() but \"", "\n", "f\"found tensor of shape: {gold_labels.size()}\"", "\n", ")", "\n", "", "if", "mask", "is", "not", "None", "and", "mask", ".", "size", "(", ")", "!=", "predictions", ".", "size", "(", ")", ":", "\n", "            ", "raise", "ValueError", "(", "\n", "f\"mask must have shape == predictions.size() but \"", "\n", "f\"found tensor of shape: {mask.size()}\"", "\n", ")", "\n", "\n", "", "batch_size", "=", "predictions", ".", "size", "(", "0", ")", "\n", "\n", "if", "mask", "is", "not", "None", ":", "\n", "# We can multiply by the mask up front, because we're just checking equality below, and", "\n", "# this way everything that's masked will be equal.", "\n", "            ", "predictions", "=", "predictions", "*", "mask", "\n", "gold_labels", "=", "gold_labels", "*", "mask", "\n", "\n", "# We want to skip predictions that are completely masked;", "\n", "# so we'll keep predictions that aren't.", "\n", "keep", "=", "mask", ".", "view", "(", "batch_size", ",", "-", "1", ")", ".", "max", "(", "dim", "=", "1", ")", "[", "0", "]", ".", "float", "(", ")", "\n", "", "else", ":", "\n", "            ", "keep", "=", "torch", ".", "ones", "(", "batch_size", ")", ".", "float", "(", ")", "\n", "\n", "", "predictions", "=", "predictions", ".", "view", "(", "batch_size", ",", "-", "1", ")", "\n", "gold_labels", "=", "gold_labels", ".", "view", "(", "batch_size", ",", "-", "1", ")", "\n", "\n", "# At this point, predictions is (batch_size, rest_of_dims_combined),", "\n", "# so .eq -> .prod will be 1 if every element of the instance prediction is correct", "\n", "# and 0 if at least one element of the instance prediction is wrong.", "\n", "# Because of how we're handling masking, masked positions are automatically \"correct\".", "\n", "correct", "=", "predictions", ".", "eq", "(", "gold_labels", ")", ".", "prod", "(", "dim", "=", "1", ")", ".", "float", "(", ")", "\n", "\n", "# Since masked positions are correct, we need to explicitly exclude instance predictions", "\n", "# where the entire prediction is masked (because they look \"correct\").", "\n", "self", ".", "_correct_count", "+=", "(", "correct", "*", "keep", ")", ".", "sum", "(", ")", "\n", "self", ".", "_total_count", "+=", "keep", ".", "sum", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.metrics.boolean_accuracy.BooleanAccuracy.get_metric": [[90, 103], ["boolean_accuracy.BooleanAccuracy.reset", "float", "float"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.metrics.sequence_accuracy.SequenceAccuracy.reset"], ["", "def", "get_metric", "(", "self", ",", "reset", ":", "bool", "=", "False", ")", ":", "\n", "        ", "\"\"\"\n        # Returns\n\n        The accumulated accuracy.\n        \"\"\"", "\n", "if", "self", ".", "_total_count", ">", "0", ":", "\n", "            ", "accuracy", "=", "float", "(", "self", ".", "_correct_count", ")", "/", "float", "(", "self", ".", "_total_count", ")", "\n", "", "else", ":", "\n", "            ", "accuracy", "=", "0.0", "\n", "", "if", "reset", ":", "\n", "            ", "self", ".", "reset", "(", ")", "\n", "", "return", "accuracy", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.metrics.boolean_accuracy.BooleanAccuracy.reset": [[104, 108], ["None"], "methods", ["None"], ["", "@", "overrides", "\n", "def", "reset", "(", "self", ")", ":", "\n", "        ", "self", ".", "_correct_count", "=", "0.0", "\n", "self", ".", "_total_count", "=", "0.0", "\n", "", "", ""]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.metrics.f1_measure.F1Measure.__init__": [[16, 19], ["allennlp.training.metrics.fbeta_measure.FBetaMeasure.__init__"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.graph_util.plot.GraphPlot.__init__"], ["def", "__init__", "(", "self", ",", "positive_label", ":", "int", ")", "->", "None", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "beta", "=", "1", ",", "labels", "=", "[", "positive_label", "]", ")", "\n", "self", ".", "_positive_label", "=", "positive_label", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.metrics.f1_measure.F1Measure.get_metric": [[20, 36], ["super().get_metric"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.metrics.sequence_accuracy.SequenceAccuracy.get_metric"], ["", "def", "get_metric", "(", "self", ",", "reset", ":", "bool", "=", "False", ")", "->", "Tuple", "[", "float", ",", "float", ",", "float", "]", ":", "\n", "        ", "\"\"\"\n        # Returns\n\n        A tuple of the following metrics based on the accumulated count statistics:\n        precision : float\n        recall : float\n        f1-measure : float\n        \"\"\"", "\n", "metric", "=", "super", "(", ")", ".", "get_metric", "(", "reset", "=", "reset", ")", "\n", "# Because we just care about the class `positive_label`", "\n", "# there is just one item in `precision`, `recall`, `fscore`", "\n", "precision", "=", "metric", "[", "\"precision\"", "]", "[", "0", "]", "\n", "recall", "=", "metric", "[", "\"recall\"", "]", "[", "0", "]", "\n", "fscore", "=", "metric", "[", "\"fscore\"", "]", "[", "0", "]", "\n", "return", "precision", ",", "recall", ",", "fscore", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.metrics.f1_measure.F1Measure._true_positives": [[37, 45], ["None"], "methods", ["None"], ["", "@", "property", "\n", "def", "_true_positives", "(", "self", ")", ":", "\n", "# When this metric is never called, `self._true_positive_sum` is None,", "\n", "# under which case we return 0.0 for backward compatibility.", "\n", "        ", "if", "self", ".", "_true_positive_sum", "is", "None", ":", "\n", "            ", "return", "0.0", "\n", "", "else", ":", "\n", "            ", "return", "self", ".", "_true_positive_sum", "[", "self", ".", "_positive_label", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.metrics.f1_measure.F1Measure._true_negatives": [[46, 54], ["None"], "methods", ["None"], ["", "", "@", "property", "\n", "def", "_true_negatives", "(", "self", ")", ":", "\n", "# When this metric is never called, `self._true_negative_sum` is None,", "\n", "# under which case we return 0.0 for backward compatibility.", "\n", "        ", "if", "self", ".", "_true_negative_sum", "is", "None", ":", "\n", "            ", "return", "0.0", "\n", "", "else", ":", "\n", "            ", "return", "self", ".", "_true_negative_sum", "[", "self", ".", "_positive_label", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.metrics.f1_measure.F1Measure._false_positives": [[55, 65], ["None"], "methods", ["None"], ["", "", "@", "property", "\n", "def", "_false_positives", "(", "self", ")", ":", "\n", "# When this metric is never called, `self._pred_sum` is None,", "\n", "# under which case we return 0.0 for backward compatibility.", "\n", "        ", "if", "self", ".", "_pred_sum", "is", "None", ":", "\n", "            ", "return", "0.0", "\n", "", "else", ":", "\n", "# `self._pred_sum` is the total number of instances under each _predicted_ class,", "\n", "# including true positives and false positives.", "\n", "            ", "return", "self", ".", "_pred_sum", "[", "self", ".", "_positive_label", "]", "-", "self", ".", "_true_positives", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.metrics.f1_measure.F1Measure._false_negatives": [[66, 76], ["None"], "methods", ["None"], ["", "", "@", "property", "\n", "def", "_false_negatives", "(", "self", ")", ":", "\n", "# When this metric is never called, `self._true_sum` is None,", "\n", "# under which case we return 0.0 for backward compatibility.", "\n", "        ", "if", "self", ".", "_true_sum", "is", "None", ":", "\n", "            ", "return", "0.0", "\n", "", "else", ":", "\n", "# `self._true_sum` is the total number of instances under each _true_ class,", "\n", "# including true positives and false negatives.", "\n", "            ", "return", "self", ".", "_true_sum", "[", "self", ".", "_positive_label", "]", "-", "self", ".", "_true_positives", "\n", "", "", "", ""]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.metrics.bleu.BLEU.__init__": [[42, 53], ["collections.Counter", "collections.Counter", "set"], "methods", ["None"], ["def", "__init__", "(", "\n", "self", ",", "\n", "ngram_weights", ":", "Iterable", "[", "float", "]", "=", "(", "0.25", ",", "0.25", ",", "0.25", ",", "0.25", ")", ",", "\n", "exclude_indices", ":", "Set", "[", "int", "]", "=", "None", ",", "\n", ")", "->", "None", ":", "\n", "        ", "self", ".", "_ngram_weights", "=", "ngram_weights", "\n", "self", ".", "_exclude_indices", "=", "exclude_indices", "or", "set", "(", ")", "\n", "self", ".", "_precision_matches", ":", "Dict", "[", "int", ",", "int", "]", "=", "Counter", "(", ")", "\n", "self", ".", "_precision_totals", ":", "Dict", "[", "int", ",", "int", "]", "=", "Counter", "(", ")", "\n", "self", ".", "_prediction_lengths", "=", "0", "\n", "self", ".", "_reference_lengths", "=", "0", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.metrics.bleu.BLEU.reset": [[54, 60], ["collections.Counter", "collections.Counter"], "methods", ["None"], ["", "@", "overrides", "\n", "def", "reset", "(", "self", ")", "->", "None", ":", "\n", "        ", "self", ".", "_precision_matches", "=", "Counter", "(", ")", "\n", "self", ".", "_precision_totals", "=", "Counter", "(", ")", "\n", "self", ".", "_prediction_lengths", "=", "0", "\n", "self", ".", "_reference_lengths", "=", "0", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.metrics.bleu.BLEU._ngrams": [[61, 76], ["collections.Counter", "range", "tensor.size", "tensor[].split", "tuple", "any", "tensor_slice.size", "x.item"], "methods", ["None"], ["", "def", "_ngrams", "(", "\n", "self", ",", "tensor", ":", "torch", ".", "LongTensor", ",", "ngram_size", ":", "int", "\n", ")", "->", "Dict", "[", "Tuple", "[", "int", ",", "...", "]", ",", "int", "]", ":", "\n", "        ", "ngram_counts", ":", "Dict", "[", "Tuple", "[", "int", ",", "...", "]", ",", "int", "]", "=", "Counter", "(", ")", "\n", "if", "ngram_size", ">", "tensor", ".", "size", "(", "-", "1", ")", ":", "\n", "            ", "return", "ngram_counts", "\n", "", "for", "start_position", "in", "range", "(", "ngram_size", ")", ":", "\n", "            ", "for", "tensor_slice", "in", "tensor", "[", "start_position", ":", "]", ".", "split", "(", "ngram_size", ",", "dim", "=", "-", "1", ")", ":", "\n", "                ", "if", "tensor_slice", ".", "size", "(", "-", "1", ")", "<", "ngram_size", ":", "\n", "                    ", "break", "\n", "", "ngram", "=", "tuple", "(", "x", ".", "item", "(", ")", "for", "x", "in", "tensor_slice", ")", "\n", "if", "any", "(", "x", "in", "self", ".", "_exclude_indices", "for", "x", "in", "ngram", ")", ":", "\n", "                    ", "continue", "\n", "", "ngram_counts", "[", "ngram", "]", "+=", "1", "\n", "", "", "return", "ngram_counts", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.metrics.bleu.BLEU._get_modified_precision_counts": [[77, 104], ["range", "predicted_tokens.size", "bleu.BLEU._ngrams", "bleu.BLEU._ngrams", "bleu.BLEU.items", "min"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.metrics.bleu.BLEU._ngrams", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.metrics.bleu.BLEU._ngrams"], ["", "def", "_get_modified_precision_counts", "(", "\n", "self", ",", "\n", "predicted_tokens", ":", "torch", ".", "LongTensor", ",", "\n", "reference_tokens", ":", "torch", ".", "LongTensor", ",", "\n", "ngram_size", ":", "int", ",", "\n", ")", "->", "Tuple", "[", "int", ",", "int", "]", ":", "\n", "        ", "\"\"\"\n        Compare the predicted tokens to the reference (gold) tokens at the desired\n        ngram size and calculate the numerator and denominator for a modified\n        form of precision.\n\n        The numerator is the number of ngrams in the predicted sentences that match\n        with an ngram in the corresponding reference sentence, clipped by the total\n        count of that ngram in the reference sentence. The denominator is just\n        the total count of predicted ngrams.\n        \"\"\"", "\n", "clipped_matches", "=", "0", "\n", "total_predicted", "=", "0", "\n", "for", "batch_num", "in", "range", "(", "predicted_tokens", ".", "size", "(", "0", ")", ")", ":", "\n", "            ", "predicted_row", "=", "predicted_tokens", "[", "batch_num", ",", ":", "]", "\n", "reference_row", "=", "reference_tokens", "[", "batch_num", ",", ":", "]", "\n", "predicted_ngram_counts", "=", "self", ".", "_ngrams", "(", "predicted_row", ",", "ngram_size", ")", "\n", "reference_ngram_counts", "=", "self", ".", "_ngrams", "(", "reference_row", ",", "ngram_size", ")", "\n", "for", "ngram", ",", "count", "in", "predicted_ngram_counts", ".", "items", "(", ")", ":", "\n", "                ", "clipped_matches", "+=", "min", "(", "count", ",", "reference_ngram_counts", "[", "ngram", "]", ")", "\n", "total_predicted", "+=", "count", "\n", "", "", "return", "clipped_matches", ",", "total_predicted", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.metrics.bleu.BLEU._get_brevity_penalty": [[105, 111], ["math.exp"], "methods", ["None"], ["", "def", "_get_brevity_penalty", "(", "self", ")", "->", "float", ":", "\n", "        ", "if", "self", ".", "_prediction_lengths", ">", "self", ".", "_reference_lengths", ":", "\n", "            ", "return", "1.0", "\n", "", "if", "self", ".", "_reference_lengths", "==", "0", "or", "self", ".", "_prediction_lengths", "==", "0", ":", "\n", "            ", "return", "0.0", "\n", "", "return", "math", ".", "exp", "(", "1.0", "-", "self", ".", "_reference_lengths", "/", "self", ".", "_prediction_lengths", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.metrics.bleu.BLEU._get_valid_tokens_mask": [[112, 117], ["torch.ones", "tensor.size"], "methods", ["None"], ["", "def", "_get_valid_tokens_mask", "(", "self", ",", "tensor", ":", "torch", ".", "LongTensor", ")", "->", "torch", ".", "ByteTensor", ":", "\n", "        ", "valid_tokens_mask", "=", "torch", ".", "ones", "(", "tensor", ".", "size", "(", ")", ",", "dtype", "=", "torch", ".", "bool", ")", "\n", "for", "index", "in", "self", ".", "_exclude_indices", ":", "\n", "            ", "valid_tokens_mask", "=", "valid_tokens_mask", "&", "(", "tensor", "!=", "index", ")", "\n", "", "return", "valid_tokens_mask", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.metrics.bleu.BLEU.__call__": [[118, 153], ["bleu.BLEU.unwrap_to_tensors", "enumerate", "bleu.BLEU._get_modified_precision_counts", "bleu.BLEU._get_valid_tokens_mask", "bleu.BLEU.sum().item", "bleu.BLEU._get_valid_tokens_mask", "bleu.BLEU.sum().item", "predictions.size", "predictions.size", "gold_targets.size", "gold_targets.size", "bleu.BLEU.sum", "bleu.BLEU.sum"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.metrics.metric.Metric.unwrap_to_tensors", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.metrics.bleu.BLEU._get_modified_precision_counts", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.metrics.bleu.BLEU._get_valid_tokens_mask", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.metrics.bleu.BLEU._get_valid_tokens_mask"], ["", "@", "overrides", "\n", "def", "__call__", "(", "\n", "self", ",", "# type: ignore", "\n", "predictions", ":", "torch", ".", "LongTensor", ",", "\n", "gold_targets", ":", "torch", ".", "LongTensor", ",", "\n", ")", "->", "None", ":", "\n", "        ", "\"\"\"\n        Update precision counts.\n\n        # Parameters\n\n        predictions : `torch.LongTensor`, required\n            Batched predicted tokens of shape `(batch_size, max_sequence_length)`.\n        references : `torch.LongTensor`, required\n            Batched reference (gold) translations with shape `(batch_size, max_gold_sequence_length)`.\n\n        # Returns\n\n        None\n        \"\"\"", "\n", "predictions", ",", "gold_targets", "=", "self", ".", "unwrap_to_tensors", "(", "predictions", ",", "gold_targets", ")", "\n", "for", "ngram_size", ",", "_", "in", "enumerate", "(", "self", ".", "_ngram_weights", ",", "start", "=", "1", ")", ":", "\n", "            ", "precision_matches", ",", "precision_totals", "=", "self", ".", "_get_modified_precision_counts", "(", "\n", "predictions", ",", "gold_targets", ",", "ngram_size", "\n", ")", "\n", "self", ".", "_precision_matches", "[", "ngram_size", "]", "+=", "precision_matches", "\n", "self", ".", "_precision_totals", "[", "ngram_size", "]", "+=", "precision_totals", "\n", "", "if", "not", "self", ".", "_exclude_indices", ":", "\n", "            ", "self", ".", "_prediction_lengths", "+=", "predictions", ".", "size", "(", "0", ")", "*", "predictions", ".", "size", "(", "1", ")", "\n", "self", ".", "_reference_lengths", "+=", "gold_targets", ".", "size", "(", "0", ")", "*", "gold_targets", ".", "size", "(", "1", ")", "\n", "", "else", ":", "\n", "            ", "valid_predictions_mask", "=", "self", ".", "_get_valid_tokens_mask", "(", "predictions", ")", "\n", "self", ".", "_prediction_lengths", "+=", "valid_predictions_mask", ".", "sum", "(", ")", ".", "item", "(", ")", "\n", "valid_gold_targets_mask", "=", "self", ".", "_get_valid_tokens_mask", "(", "gold_targets", ")", "\n", "self", ".", "_reference_lengths", "+=", "valid_gold_targets_mask", ".", "sum", "(", ")", ".", "item", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.metrics.bleu.BLEU.get_metric": [[154, 169], ["bleu.BLEU._get_brevity_penalty", "math.exp", "bleu.BLEU.reset", "enumerate", "sum", "math.log", "math.log"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.metrics.bleu.BLEU._get_brevity_penalty", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.metrics.sequence_accuracy.SequenceAccuracy.reset"], ["", "", "@", "overrides", "\n", "def", "get_metric", "(", "self", ",", "reset", ":", "bool", "=", "False", ")", "->", "Dict", "[", "str", ",", "float", "]", ":", "\n", "        ", "brevity_penalty", "=", "self", ".", "_get_brevity_penalty", "(", ")", "\n", "ngram_scores", "=", "(", "\n", "weight", "\n", "*", "(", "\n", "math", ".", "log", "(", "self", ".", "_precision_matches", "[", "n", "]", "+", "1e-13", ")", "\n", "-", "math", ".", "log", "(", "self", ".", "_precision_totals", "[", "n", "]", "+", "1e-13", ")", "\n", ")", "\n", "for", "n", ",", "weight", "in", "enumerate", "(", "self", ".", "_ngram_weights", ",", "start", "=", "1", ")", "\n", ")", "\n", "bleu", "=", "brevity_penalty", "*", "math", ".", "exp", "(", "sum", "(", "ngram_scores", ")", ")", "\n", "if", "reset", ":", "\n", "            ", "self", ".", "reset", "(", ")", "\n", "", "return", "{", "\"BLEU\"", ":", "bleu", "}", "\n", "", "", ""]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.metrics.conll_coref_scores.ConllCorefScores.__init__": [[14, 16], ["conll_coref_scores.Scorer"], "methods", ["None"], ["    ", "def", "__init__", "(", "self", ")", "->", "None", ":", "\n", "        ", "self", ".", "scorers", "=", "[", "Scorer", "(", "m", ")", "for", "m", "in", "(", "Scorer", ".", "muc", ",", "Scorer", ".", "b_cubed", ",", "Scorer", ".", "ceafe", ")", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.metrics.conll_coref_scores.ConllCorefScores.__call__": [[17, 59], ["conll_coref_scores.ConllCorefScores.unwrap_to_tensors", "enumerate", "conll_coref_scores.ConllCorefScores.get_gold_clusters", "conll_coref_scores.ConllCorefScores.get_predicted_clusters", "scorer.update"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.metrics.metric.Metric.unwrap_to_tensors", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.metrics.conll_coref_scores.ConllCorefScores.get_gold_clusters", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.metrics.conll_coref_scores.ConllCorefScores.get_predicted_clusters", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.metrics.conll_coref_scores.Scorer.update"], ["", "@", "overrides", "\n", "def", "__call__", "(", "\n", "self", ",", "# type: ignore", "\n", "top_spans", ":", "torch", ".", "Tensor", ",", "\n", "antecedent_indices", ":", "torch", ".", "Tensor", ",", "\n", "predicted_antecedents", ":", "torch", ".", "Tensor", ",", "\n", "metadata_list", ":", "List", "[", "Dict", "[", "str", ",", "Any", "]", "]", ",", "\n", ")", ":", "\n", "        ", "\"\"\"\n        # Parameters\n\n        top_spans : `torch.Tensor`\n            (start, end) indices for all spans kept after span pruning in the model.\n            Expected shape: (batch_size, num_spans, 2)\n        antecedent_indices : `torch.Tensor`\n            For each span, the indices of all allowed antecedents for that span.  This is\n            independent of the batch dimension, as it's just based on order in the document.\n            Expected shape: (num_spans, num_antecedents)\n        predicted_antecedents : `torch.Tensor`\n            For each span, this contains the index (into antecedent_indices) of the most likely\n            antecedent for that span.\n            Expected shape: (batch_size, num_spans)\n        metadata_list : `List[Dict[str, Any]]`\n            A metadata dictionary for each instance in the batch.  We use the \"clusters\" key from\n            this dictionary, which has the annotated gold coreference clusters for that instance.\n        \"\"\"", "\n", "top_spans", ",", "antecedent_indices", ",", "predicted_antecedents", "=", "self", ".", "unwrap_to_tensors", "(", "\n", "top_spans", ",", "antecedent_indices", ",", "predicted_antecedents", "\n", ")", "\n", "for", "i", ",", "metadata", "in", "enumerate", "(", "metadata_list", ")", ":", "\n", "            ", "gold_clusters", ",", "mention_to_gold", "=", "self", ".", "get_gold_clusters", "(", "\n", "metadata", "[", "\"clusters\"", "]", "\n", ")", "\n", "predicted_clusters", ",", "mention_to_predicted", "=", "self", ".", "get_predicted_clusters", "(", "\n", "top_spans", "[", "i", "]", ",", "antecedent_indices", ",", "predicted_antecedents", "[", "i", "]", "\n", ")", "\n", "for", "scorer", "in", "self", ".", "scorers", ":", "\n", "                ", "scorer", ".", "update", "(", "\n", "predicted_clusters", ",", "\n", "gold_clusters", ",", "\n", "mention_to_predicted", ",", "\n", "mention_to_gold", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.metrics.conll_coref_scores.ConllCorefScores.get_metric": [[61, 75], ["tuple", "conll_coref_scores.ConllCorefScores.reset", "e.get_precision", "e.get_recall", "e.get_f1", "sum", "len", "metric"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.metrics.sequence_accuracy.SequenceAccuracy.reset", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.metrics.conll_coref_scores.Scorer.get_precision", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.metrics.conll_coref_scores.Scorer.get_recall", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.metrics.conll_coref_scores.Scorer.get_f1"], ["", "", "", "@", "overrides", "\n", "def", "get_metric", "(", "self", ",", "reset", ":", "bool", "=", "False", ")", "->", "Tuple", "[", "float", ",", "float", ",", "float", "]", ":", "\n", "        ", "metrics", "=", "(", "\n", "lambda", "e", ":", "e", ".", "get_precision", "(", ")", ",", "\n", "lambda", "e", ":", "e", ".", "get_recall", "(", ")", ",", "\n", "lambda", "e", ":", "e", ".", "get_f1", "(", ")", ",", "\n", ")", "\n", "precision", ",", "recall", ",", "f1_score", "=", "tuple", "(", "\n", "sum", "(", "metric", "(", "e", ")", "for", "e", "in", "self", ".", "scorers", ")", "/", "len", "(", "self", ".", "scorers", ")", "\n", "for", "metric", "in", "metrics", "\n", ")", "\n", "if", "reset", ":", "\n", "            ", "self", ".", "reset", "(", ")", "\n", "", "return", "precision", ",", "recall", ",", "f1_score", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.metrics.conll_coref_scores.ConllCorefScores.reset": [[76, 80], ["conll_coref_scores.Scorer"], "methods", ["None"], ["", "@", "overrides", "\n", "def", "reset", "(", "self", ")", ":", "\n", "        ", "self", ".", "scorers", "=", "[", "\n", "Scorer", "(", "metric", ")", "for", "metric", "in", "(", "Scorer", ".", "muc", ",", "Scorer", ".", "b_cubed", ",", "Scorer", ".", "ceafe", ")", "\n", "]", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.metrics.conll_coref_scores.ConllCorefScores.get_gold_clusters": [[82, 90], ["tuple", "tuple"], "methods", ["None"], ["", "@", "staticmethod", "\n", "def", "get_gold_clusters", "(", "gold_clusters", ")", ":", "\n", "        ", "gold_clusters", "=", "[", "tuple", "(", "tuple", "(", "m", ")", "for", "m", "in", "gc", ")", "for", "gc", "in", "gold_clusters", "]", "\n", "mention_to_gold", "=", "{", "}", "\n", "for", "gold_cluster", "in", "gold_clusters", ":", "\n", "            ", "for", "mention", "in", "gold_cluster", ":", "\n", "                ", "mention_to_gold", "[", "mention", "]", "=", "gold_cluster", "\n", "", "", "return", "gold_clusters", ",", "mention_to_gold", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.metrics.conll_coref_scores.ConllCorefScores.get_predicted_clusters": [[91, 140], ["top_spans.numpy.numpy.numpy", "antecedent_indices.numpy.numpy.numpy", "predicted_antecedents.numpy.numpy.numpy", "enumerate", "tuple", "tuple", "clusters[].append", "tuple", "predicted_clusters_to_ids.keys", "len", "clusters.append", "predicted_clusters_to_ids.items"], "methods", ["None"], ["", "@", "staticmethod", "\n", "def", "get_predicted_clusters", "(", "\n", "top_spans", ":", "torch", ".", "Tensor", ",", "\n", "antecedent_indices", ":", "torch", ".", "Tensor", ",", "\n", "predicted_antecedents", ":", "torch", ".", "Tensor", ",", "\n", ")", "->", "Tuple", "[", "\n", "List", "[", "Tuple", "[", "Tuple", "[", "int", ",", "int", "]", ",", "...", "]", "]", ",", "\n", "Dict", "[", "Tuple", "[", "int", ",", "int", "]", ",", "Tuple", "[", "Tuple", "[", "int", ",", "int", "]", ",", "...", "]", "]", ",", "\n", "]", ":", "\n", "# Pytorch 0.4 introduced scalar tensors, so our calls to tuple() and such below don't", "\n", "# actually give ints unless we convert to numpy first.  So we do that here.", "\n", "        ", "top_spans", "=", "top_spans", ".", "numpy", "(", ")", "# (num_spans, 2)", "\n", "antecedent_indices", "=", "antecedent_indices", ".", "numpy", "(", ")", "# (num_spans, num_antecedents)", "\n", "predicted_antecedents", "=", "predicted_antecedents", ".", "numpy", "(", ")", "# (num_spans,)", "\n", "\n", "predicted_clusters_to_ids", ":", "Dict", "[", "Tuple", "[", "int", ",", "int", "]", ",", "int", "]", "=", "{", "}", "\n", "clusters", ":", "List", "[", "List", "[", "Tuple", "[", "int", ",", "int", "]", "]", "]", "=", "[", "]", "\n", "for", "i", ",", "predicted_antecedent", "in", "enumerate", "(", "predicted_antecedents", ")", ":", "\n", "            ", "if", "predicted_antecedent", "<", "0", ":", "\n", "                ", "continue", "\n", "\n", "# Find predicted index in the antecedent spans.", "\n", "", "predicted_index", "=", "antecedent_indices", "[", "i", ",", "predicted_antecedent", "]", "\n", "# Must be a previous span.", "\n", "assert", "i", ">", "predicted_index", "\n", "antecedent_span", ":", "Tuple", "[", "int", ",", "int", "]", "=", "tuple", "(", "top_spans", "[", "predicted_index", "]", ")", "# type: ignore", "\n", "\n", "# Check if we've seen the span before.", "\n", "if", "antecedent_span", "in", "predicted_clusters_to_ids", ".", "keys", "(", ")", ":", "\n", "                ", "predicted_cluster_id", ":", "int", "=", "predicted_clusters_to_ids", "[", "antecedent_span", "]", "\n", "", "else", ":", "\n", "# We start a new cluster.", "\n", "                ", "predicted_cluster_id", "=", "len", "(", "clusters", ")", "\n", "clusters", ".", "append", "(", "[", "antecedent_span", "]", ")", "\n", "predicted_clusters_to_ids", "[", "antecedent_span", "]", "=", "predicted_cluster_id", "\n", "\n", "", "mention", ":", "Tuple", "[", "int", ",", "int", "]", "=", "tuple", "(", "top_spans", "[", "i", "]", ")", "# type: ignore", "\n", "clusters", "[", "predicted_cluster_id", "]", ".", "append", "(", "mention", ")", "\n", "predicted_clusters_to_ids", "[", "mention", "]", "=", "predicted_cluster_id", "\n", "\n", "# finalise the spans and clusters.", "\n", "", "final_clusters", "=", "[", "tuple", "(", "cluster", ")", "for", "cluster", "in", "clusters", "]", "\n", "# Return a mapping of each mention to the cluster containing it.", "\n", "mention_to_cluster", ":", "Dict", "[", "Tuple", "[", "int", ",", "int", "]", ",", "Tuple", "[", "Tuple", "[", "int", ",", "int", "]", ",", "...", "]", "]", "=", "{", "\n", "mention", ":", "final_clusters", "[", "cluster_id", "]", "\n", "for", "mention", ",", "cluster_id", "in", "predicted_clusters_to_ids", ".", "items", "(", ")", "\n", "}", "\n", "\n", "return", "final_clusters", ",", "mention_to_cluster", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.metrics.conll_coref_scores.Scorer.__init__": [[147, 153], ["None"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "metric", ")", ":", "\n", "        ", "self", ".", "precision_numerator", "=", "0", "\n", "self", ".", "precision_denominator", "=", "0", "\n", "self", ".", "recall_numerator", "=", "0", "\n", "self", ".", "recall_denominator", "=", "0", "\n", "self", ".", "metric", "=", "metric", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.metrics.conll_coref_scores.Scorer.update": [[154, 164], ["conll_coref_scores.Scorer.metric", "conll_coref_scores.Scorer.metric", "conll_coref_scores.Scorer.metric"], "methods", ["None"], ["", "def", "update", "(", "self", ",", "predicted", ",", "gold", ",", "mention_to_predicted", ",", "mention_to_gold", ")", ":", "\n", "        ", "if", "self", ".", "metric", "==", "self", ".", "ceafe", ":", "\n", "            ", "p_num", ",", "p_den", ",", "r_num", ",", "r_den", "=", "self", ".", "metric", "(", "predicted", ",", "gold", ")", "\n", "", "else", ":", "\n", "            ", "p_num", ",", "p_den", "=", "self", ".", "metric", "(", "predicted", ",", "mention_to_gold", ")", "\n", "r_num", ",", "r_den", "=", "self", ".", "metric", "(", "gold", ",", "mention_to_predicted", ")", "\n", "", "self", ".", "precision_numerator", "+=", "p_num", "\n", "self", ".", "precision_denominator", "+=", "p_den", "\n", "self", ".", "recall_numerator", "+=", "r_num", "\n", "self", ".", "recall_denominator", "+=", "r_den", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.metrics.conll_coref_scores.Scorer.get_f1": [[165, 180], ["float", "float"], "methods", ["None"], ["", "def", "get_f1", "(", "self", ")", ":", "\n", "        ", "precision", "=", "(", "\n", "0", "\n", "if", "self", ".", "precision_denominator", "==", "0", "\n", "else", "self", ".", "precision_numerator", "/", "float", "(", "self", ".", "precision_denominator", ")", "\n", ")", "\n", "recall", "=", "(", "\n", "0", "\n", "if", "self", ".", "recall_denominator", "==", "0", "\n", "else", "self", ".", "recall_numerator", "/", "float", "(", "self", ".", "recall_denominator", ")", "\n", ")", "\n", "return", "(", "\n", "0", "\n", "if", "precision", "+", "recall", "==", "0", "\n", "else", "2", "*", "precision", "*", "recall", "/", "(", "precision", "+", "recall", ")", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.metrics.conll_coref_scores.Scorer.get_recall": [[182, 187], ["float"], "methods", ["None"], ["", "def", "get_recall", "(", "self", ")", ":", "\n", "        ", "if", "self", ".", "recall_numerator", "==", "0", ":", "\n", "            ", "return", "0", "\n", "", "else", ":", "\n", "            ", "return", "self", ".", "recall_numerator", "/", "float", "(", "self", ".", "recall_denominator", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.metrics.conll_coref_scores.Scorer.get_precision": [[188, 193], ["float"], "methods", ["None"], ["", "", "def", "get_precision", "(", "self", ")", ":", "\n", "        ", "if", "self", ".", "precision_numerator", "==", "0", ":", "\n", "            ", "return", "0", "\n", "", "else", ":", "\n", "            ", "return", "self", ".", "precision_numerator", "/", "float", "(", "self", ".", "precision_denominator", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.metrics.conll_coref_scores.Scorer.get_prf": [[194, 196], ["conll_coref_scores.Scorer.get_precision", "conll_coref_scores.Scorer.get_recall", "conll_coref_scores.Scorer.get_f1"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.metrics.conll_coref_scores.Scorer.get_precision", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.metrics.conll_coref_scores.Scorer.get_recall", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.metrics.conll_coref_scores.Scorer.get_f1"], ["", "", "def", "get_prf", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "get_precision", "(", ")", ",", "self", ".", "get_recall", "(", ")", ",", "self", ".", "get_f1", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.metrics.conll_coref_scores.Scorer.b_cubed": [[197, 218], ["collections.Counter", "collections.Counter.items", "len", "len", "float", "len", "len", "tuple"], "methods", ["None"], ["", "@", "staticmethod", "\n", "def", "b_cubed", "(", "clusters", ",", "mention_to_gold", ")", ":", "\n", "        ", "\"\"\"\n        Averaged per-mention precision and recall.\n        <https://pdfs.semanticscholar.org/cfe3/c24695f1c14b78a5b8e95bcbd1c666140fd1.pdf>\n        \"\"\"", "\n", "numerator", ",", "denominator", "=", "0", ",", "0", "\n", "for", "cluster", "in", "clusters", ":", "\n", "            ", "if", "len", "(", "cluster", ")", "==", "1", ":", "\n", "                ", "continue", "\n", "", "gold_counts", "=", "Counter", "(", ")", "\n", "correct", "=", "0", "\n", "for", "mention", "in", "cluster", ":", "\n", "                ", "if", "mention", "in", "mention_to_gold", ":", "\n", "                    ", "gold_counts", "[", "tuple", "(", "mention_to_gold", "[", "mention", "]", ")", "]", "+=", "1", "\n", "", "", "for", "cluster2", ",", "count", "in", "gold_counts", ".", "items", "(", ")", ":", "\n", "                ", "if", "len", "(", "cluster2", ")", "!=", "1", ":", "\n", "                    ", "correct", "+=", "count", "*", "count", "\n", "", "", "numerator", "+=", "correct", "/", "float", "(", "len", "(", "cluster", ")", ")", "\n", "denominator", "+=", "len", "(", "cluster", ")", "\n", "", "return", "numerator", ",", "denominator", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.metrics.conll_coref_scores.Scorer.muc": [[219, 238], ["len", "set", "len", "len", "set.add"], "methods", ["None"], ["", "@", "staticmethod", "\n", "def", "muc", "(", "clusters", ",", "mention_to_gold", ")", ":", "\n", "        ", "\"\"\"\n        Counts the mentions in each predicted cluster which need to be re-allocated in\n        order for each predicted cluster to be contained by the respective gold cluster.\n        <https://aclweb.org/anthology/M/M95/M95-1005.pdf>\n        \"\"\"", "\n", "true_p", ",", "all_p", "=", "0", ",", "0", "\n", "for", "cluster", "in", "clusters", ":", "\n", "            ", "all_p", "+=", "len", "(", "cluster", ")", "-", "1", "\n", "true_p", "+=", "len", "(", "cluster", ")", "\n", "linked", "=", "set", "(", ")", "\n", "for", "mention", "in", "cluster", ":", "\n", "                ", "if", "mention", "in", "mention_to_gold", ":", "\n", "                    ", "linked", ".", "add", "(", "mention_to_gold", "[", "mention", "]", ")", "\n", "", "else", ":", "\n", "                    ", "true_p", "-=", "1", "\n", "", "", "true_p", "-=", "len", "(", "linked", ")", "\n", "", "return", "true_p", ",", "all_p", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.metrics.conll_coref_scores.Scorer.phi4": [[239, 255], ["float", "len", "len", "len"], "methods", ["None"], ["", "@", "staticmethod", "\n", "def", "phi4", "(", "gold_clustering", ",", "predicted_clustering", ")", ":", "\n", "        ", "\"\"\"\n        Subroutine for ceafe. Computes the mention F measure between gold and\n        predicted mentions in a cluster.\n        \"\"\"", "\n", "return", "(", "\n", "2", "\n", "*", "len", "(", "\n", "[", "\n", "mention", "\n", "for", "mention", "in", "gold_clustering", "\n", "if", "mention", "in", "predicted_clustering", "\n", "]", "\n", ")", "\n", "/", "float", "(", "len", "(", "gold_clustering", ")", "+", "len", "(", "predicted_clustering", ")", ")", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.metrics.conll_coref_scores.Scorer.ceafe": [[257, 274], ["numpy.zeros", "enumerate", "scipy.optimize.linear_sum_assignment", "sum", "enumerate", "len", "len", "len", "len", "conll_coref_scores.Scorer.phi4", "len"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.metrics.conll_coref_scores.Scorer.phi4"], ["", "@", "staticmethod", "\n", "def", "ceafe", "(", "clusters", ",", "gold_clusters", ")", ":", "\n", "        ", "\"\"\"\n        Computes the  Constrained EntityAlignment F-Measure (CEAF) for evaluating coreference.\n        Gold and predicted mentions are aligned into clusterings which maximise a metric - in\n        this case, the F measure between gold and predicted clusters.\n\n        <https://www.semanticscholar.org/paper/On-Coreference-Resolution-Performance-Metrics-Luo/de133c1f22d0dfe12539e25dda70f28672459b99>\n        \"\"\"", "\n", "clusters", "=", "[", "cluster", "for", "cluster", "in", "clusters", "if", "len", "(", "cluster", ")", "!=", "1", "]", "\n", "scores", "=", "np", ".", "zeros", "(", "(", "len", "(", "gold_clusters", ")", ",", "len", "(", "clusters", ")", ")", ")", "\n", "for", "i", ",", "gold_cluster", "in", "enumerate", "(", "gold_clusters", ")", ":", "\n", "            ", "for", "j", ",", "cluster", "in", "enumerate", "(", "clusters", ")", ":", "\n", "                ", "scores", "[", "i", ",", "j", "]", "=", "Scorer", ".", "phi4", "(", "gold_cluster", ",", "cluster", ")", "\n", "", "", "row", ",", "col", "=", "linear_sum_assignment", "(", "-", "scores", ")", "\n", "similarity", "=", "sum", "(", "scores", "[", "row", ",", "col", "]", ")", "\n", "return", "similarity", ",", "len", "(", "clusters", ")", ",", "similarity", ",", "len", "(", "gold_clusters", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.metrics.entropy.Entropy.__init__": [[11, 14], ["None"], "methods", ["None"], ["    ", "def", "__init__", "(", "self", ")", "->", "None", ":", "\n", "        ", "self", ".", "_entropy", "=", "0.0", "\n", "self", ".", "_count", "=", "0", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.metrics.entropy.Entropy.__call__": [[15, 41], ["weighted_negative_likelihood.sum.Entropy.unwrap_to_tensors", "torch.nn.functional.log_softmax", "weighted_negative_likelihood.sum", "torch.ones", "torch.exp", "torch.ones.unsqueeze", "weighted_negative_likelihood.sum.sum", "torch.ones.sum", "logits.size"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.metrics.metric.Metric.unwrap_to_tensors"], ["", "@", "overrides", "\n", "def", "__call__", "(", "\n", "self", ",", "# type: ignore", "\n", "logits", ":", "torch", ".", "Tensor", ",", "\n", "mask", ":", "Optional", "[", "torch", ".", "Tensor", "]", "=", "None", ",", "\n", ")", ":", "\n", "        ", "\"\"\"\n        # Parameters\n\n        logits : `torch.Tensor`, required.\n            A tensor of unnormalized log probabilities of shape (batch_size, ..., num_classes).\n        mask : `torch.Tensor`, optional (default = None).\n            A masking tensor of shape (batch_size, ...).\n        \"\"\"", "\n", "logits", ",", "mask", "=", "self", ".", "unwrap_to_tensors", "(", "logits", ",", "mask", ")", "\n", "\n", "if", "mask", "is", "None", ":", "\n", "            ", "mask", "=", "torch", ".", "ones", "(", "logits", ".", "size", "(", ")", "[", ":", "-", "1", "]", ")", "\n", "\n", "", "log_probs", "=", "torch", ".", "nn", ".", "functional", ".", "log_softmax", "(", "logits", ",", "dim", "=", "-", "1", ")", "\n", "probabilities", "=", "torch", ".", "exp", "(", "log_probs", ")", "*", "mask", ".", "unsqueeze", "(", "-", "1", ")", "\n", "weighted_negative_likelihood", "=", "-", "log_probs", "*", "probabilities", "\n", "entropy", "=", "weighted_negative_likelihood", ".", "sum", "(", "-", "1", ")", "\n", "\n", "self", ".", "_entropy", "+=", "entropy", ".", "sum", "(", ")", "/", "mask", ".", "sum", "(", ")", "\n", "self", ".", "_count", "+=", "1", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.metrics.entropy.Entropy.get_metric": [[42, 53], ["entropy.Entropy.reset"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.metrics.sequence_accuracy.SequenceAccuracy.reset"], ["", "@", "overrides", "\n", "def", "get_metric", "(", "self", ",", "reset", ":", "bool", "=", "False", ")", ":", "\n", "        ", "\"\"\"\n        # Returns\n\n        The scalar average entropy.\n        \"\"\"", "\n", "average_value", "=", "self", ".", "_entropy", "/", "self", ".", "_count", "if", "self", ".", "_count", ">", "0", "else", "0", "\n", "if", "reset", ":", "\n", "            ", "self", ".", "reset", "(", ")", "\n", "", "return", "average_value", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.metrics.entropy.Entropy.reset": [[54, 58], ["None"], "methods", ["None"], ["", "@", "overrides", "\n", "def", "reset", "(", "self", ")", ":", "\n", "        ", "self", ".", "_entropy", "=", "0.0", "\n", "self", ".", "_count", "=", "0", "\n", "", "", ""]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.metrics.span_based_f1_measure.SpanBasedF1Measure.__init__": [[40, 100], ["vocabulary.get_index_to_token_vocabulary", "collections.defaultdict", "collections.defaultdict", "collections.defaultdict", "allennlp.common.checks.ConfigurationError", "allennlp.common.checks.ConfigurationError", "allennlp.common.checks.ConfigurationError"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.data.vocabulary.Vocabulary.get_index_to_token_vocabulary"], ["def", "__init__", "(", "\n", "self", ",", "\n", "vocabulary", ":", "Vocabulary", ",", "\n", "tag_namespace", ":", "str", "=", "\"tags\"", ",", "\n", "ignore_classes", ":", "List", "[", "str", "]", "=", "None", ",", "\n", "label_encoding", ":", "Optional", "[", "str", "]", "=", "\"BIO\"", ",", "\n", "tags_to_spans_function", ":", "Optional", "[", "TAGS_TO_SPANS_FUNCTION_TYPE", "]", "=", "None", ",", "\n", ")", "->", "None", ":", "\n", "        ", "\"\"\"\n        # Parameters\n\n        vocabulary : `Vocabulary`, required.\n            A vocabulary containing the tag namespace.\n        tag_namespace : str, required.\n            This metric assumes that a BIO format is used in which the\n            labels are of the format: [\"B-LABEL\", \"I-LABEL\"].\n        ignore_classes : List[str], optional.\n            Span labels which will be ignored when computing span metrics.\n            A \"span label\" is the part that comes after the BIO label, so it\n            would be \"ARG1\" for the tag \"B-ARG1\". For example by passing:\n\n             `ignore_classes=[\"V\"]`\n            the following sequence would not consider the \"V\" span at index (2, 3)\n            when computing the precision, recall and F1 metrics.\n\n            [\"O\", \"O\", \"B-V\", \"I-V\", \"B-ARG1\", \"I-ARG1\"]\n\n            This is helpful for instance, to avoid computing metrics for \"V\"\n            spans in a BIO tagging scheme which are typically not included.\n        label_encoding : `str`, optional (default = \"BIO\")\n            The encoding used to specify label span endpoints in the sequence.\n            Valid options are \"BIO\", \"IOB1\", \"BIOUL\" or \"BMES\".\n        tags_to_spans_function : `Callable`, optional (default = `None`)\n            If `label_encoding` is `None`, `tags_to_spans_function` will be\n            used to generate spans.\n        \"\"\"", "\n", "if", "label_encoding", "and", "tags_to_spans_function", ":", "\n", "            ", "raise", "ConfigurationError", "(", "\n", "\"Both label_encoding and tags_to_spans_function are provided. \"", "\n", "'Set \"label_encoding=None\" explicitly to enable tags_to_spans_function.'", "\n", ")", "\n", "", "if", "label_encoding", ":", "\n", "            ", "if", "label_encoding", "not", "in", "[", "\"BIO\"", ",", "\"IOB1\"", ",", "\"BIOUL\"", ",", "\"BMES\"", "]", ":", "\n", "                ", "raise", "ConfigurationError", "(", "\n", "\"Unknown label encoding - expected 'BIO', 'IOB1', 'BIOUL', 'BMES'.\"", "\n", ")", "\n", "", "", "elif", "tags_to_spans_function", "is", "None", ":", "\n", "            ", "raise", "ConfigurationError", "(", "\n", "\"At least one of the (label_encoding, tags_to_spans_function) should be provided.\"", "\n", ")", "\n", "\n", "", "self", ".", "_label_encoding", "=", "label_encoding", "\n", "self", ".", "_tags_to_spans_function", "=", "tags_to_spans_function", "\n", "self", ".", "_label_vocabulary", "=", "vocabulary", ".", "get_index_to_token_vocabulary", "(", "tag_namespace", ")", "\n", "self", ".", "_ignore_classes", ":", "List", "[", "str", "]", "=", "ignore_classes", "or", "[", "]", "\n", "\n", "# These will hold per label span counts.", "\n", "self", ".", "_true_positives", ":", "Dict", "[", "str", ",", "int", "]", "=", "defaultdict", "(", "int", ")", "\n", "self", ".", "_false_positives", ":", "Dict", "[", "str", ",", "int", "]", "=", "defaultdict", "(", "int", ")", "\n", "self", ".", "_false_negatives", ":", "Dict", "[", "str", ",", "int", "]", "=", "defaultdict", "(", "int", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.metrics.span_based_f1_measure.SpanBasedF1Measure.__call__": [[101, 204], ["span_based_f1_measure.SpanBasedF1Measure.unwrap_to_tensors", "predictions.size", "allennlp.nn.util.get_lengths_from_binary_sequence_mask", "torch.gather.float", "torch.gather.size", "range", "torch.ones_like", "allennlp.common.checks.ConfigurationError", "predictions.max", "torch.gather", "torch.gather", "tags_to_spans_function", "tags_to_spans_function", "span_based_f1_measure.SpanBasedF1Measure._handle_continued_spans", "span_based_f1_measure.SpanBasedF1Measure._handle_continued_spans", "torch.gather.long", "sequence_prediction[].tolist", "sequence_gold_label[].tolist", "span_based_f1_measure.SpanBasedF1Measure.remove"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.metrics.metric.Metric.unwrap_to_tensors", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.nn.util.get_lengths_from_binary_sequence_mask", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.metrics.span_based_f1_measure.SpanBasedF1Measure._handle_continued_spans", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.metrics.span_based_f1_measure.SpanBasedF1Measure._handle_continued_spans"], ["", "def", "__call__", "(", "\n", "self", ",", "\n", "predictions", ":", "torch", ".", "Tensor", ",", "\n", "gold_labels", ":", "torch", ".", "Tensor", ",", "\n", "mask", ":", "Optional", "[", "torch", ".", "Tensor", "]", "=", "None", ",", "\n", "prediction_map", ":", "Optional", "[", "torch", ".", "Tensor", "]", "=", "None", ",", "\n", ")", ":", "\n", "        ", "\"\"\"\n        # Parameters\n\n        predictions : `torch.Tensor`, required.\n            A tensor of predictions of shape (batch_size, sequence_length, num_classes).\n        gold_labels : `torch.Tensor`, required.\n            A tensor of integer class label of shape (batch_size, sequence_length). It must be the same\n            shape as the `predictions` tensor without the `num_classes` dimension.\n        mask : `torch.Tensor`, optional (default = None).\n            A masking tensor the same size as `gold_labels`.\n        prediction_map : `torch.Tensor`, optional (default = None).\n            A tensor of size (batch_size, num_classes) which provides a mapping from the index of predictions\n            to the indices of the label vocabulary. If provided, the output label at each timestep will be\n            `vocabulary.get_index_to_token_vocabulary(prediction_map[batch, argmax(predictions[batch, t]))`,\n            rather than simply `vocabulary.get_index_to_token_vocabulary(argmax(predictions[batch, t]))`.\n            This is useful in cases where each Instance in the dataset is associated with a different possible\n            subset of labels from a large label-space (IE FrameNet, where each frame has a different set of\n            possible roles associated with it).\n        \"\"\"", "\n", "if", "mask", "is", "None", ":", "\n", "            ", "mask", "=", "torch", ".", "ones_like", "(", "gold_labels", ")", "\n", "\n", "", "predictions", ",", "gold_labels", ",", "mask", ",", "prediction_map", "=", "self", ".", "unwrap_to_tensors", "(", "\n", "predictions", ",", "gold_labels", ",", "mask", ",", "prediction_map", "\n", ")", "\n", "\n", "num_classes", "=", "predictions", ".", "size", "(", "-", "1", ")", "\n", "if", "(", "gold_labels", ">=", "num_classes", ")", ".", "any", "(", ")", ":", "\n", "            ", "raise", "ConfigurationError", "(", "\n", "\"A gold label passed to SpanBasedF1Measure contains an \"", "\n", "\"id >= {}, the number of classes.\"", ".", "format", "(", "num_classes", ")", "\n", ")", "\n", "\n", "", "sequence_lengths", "=", "get_lengths_from_binary_sequence_mask", "(", "mask", ")", "\n", "argmax_predictions", "=", "predictions", ".", "max", "(", "-", "1", ")", "[", "1", "]", "\n", "\n", "if", "prediction_map", "is", "not", "None", ":", "\n", "            ", "argmax_predictions", "=", "torch", ".", "gather", "(", "prediction_map", ",", "1", ",", "argmax_predictions", ")", "\n", "gold_labels", "=", "torch", ".", "gather", "(", "prediction_map", ",", "1", ",", "gold_labels", ".", "long", "(", ")", ")", "\n", "\n", "", "argmax_predictions", "=", "argmax_predictions", ".", "float", "(", ")", "\n", "\n", "# Iterate over timesteps in batch.", "\n", "batch_size", "=", "gold_labels", ".", "size", "(", "0", ")", "\n", "for", "i", "in", "range", "(", "batch_size", ")", ":", "\n", "            ", "sequence_prediction", "=", "argmax_predictions", "[", "i", ",", ":", "]", "\n", "sequence_gold_label", "=", "gold_labels", "[", "i", ",", ":", "]", "\n", "length", "=", "sequence_lengths", "[", "i", "]", "\n", "\n", "if", "length", "==", "0", ":", "\n", "# It is possible to call this metric with sequences which are", "\n", "# completely padded. These contribute nothing, so we skip these rows.", "\n", "                ", "continue", "\n", "\n", "", "predicted_string_labels", "=", "[", "\n", "self", ".", "_label_vocabulary", "[", "label_id", "]", "\n", "for", "label_id", "in", "sequence_prediction", "[", ":", "length", "]", ".", "tolist", "(", ")", "\n", "]", "\n", "gold_string_labels", "=", "[", "\n", "self", ".", "_label_vocabulary", "[", "label_id", "]", "\n", "for", "label_id", "in", "sequence_gold_label", "[", ":", "length", "]", ".", "tolist", "(", ")", "\n", "]", "\n", "\n", "tags_to_spans_function", "=", "None", "\n", "# `label_encoding` is empty and `tags_to_spans_function` is provided.", "\n", "if", "self", ".", "_label_encoding", "is", "None", "and", "self", ".", "_tags_to_spans_function", ":", "\n", "                ", "tags_to_spans_function", "=", "self", ".", "_tags_to_spans_function", "\n", "# Search by `label_encoding`.", "\n", "", "elif", "self", ".", "_label_encoding", "==", "\"BIO\"", ":", "\n", "                ", "tags_to_spans_function", "=", "bio_tags_to_spans", "\n", "", "elif", "self", ".", "_label_encoding", "==", "\"IOB1\"", ":", "\n", "                ", "tags_to_spans_function", "=", "iob1_tags_to_spans", "\n", "", "elif", "self", ".", "_label_encoding", "==", "\"BIOUL\"", ":", "\n", "                ", "tags_to_spans_function", "=", "bioul_tags_to_spans", "\n", "", "elif", "self", ".", "_label_encoding", "==", "\"BMES\"", ":", "\n", "                ", "tags_to_spans_function", "=", "bmes_tags_to_spans", "\n", "\n", "", "predicted_spans", "=", "tags_to_spans_function", "(", "\n", "predicted_string_labels", ",", "self", ".", "_ignore_classes", "\n", ")", "\n", "gold_spans", "=", "tags_to_spans_function", "(", "\n", "gold_string_labels", ",", "self", ".", "_ignore_classes", "\n", ")", "\n", "\n", "predicted_spans", "=", "self", ".", "_handle_continued_spans", "(", "predicted_spans", ")", "\n", "gold_spans", "=", "self", ".", "_handle_continued_spans", "(", "gold_spans", ")", "\n", "\n", "for", "span", "in", "predicted_spans", ":", "\n", "                ", "if", "span", "in", "gold_spans", ":", "\n", "                    ", "self", ".", "_true_positives", "[", "span", "[", "0", "]", "]", "+=", "1", "\n", "gold_spans", ".", "remove", "(", "span", ")", "\n", "", "else", ":", "\n", "                    ", "self", ".", "_false_positives", "[", "span", "[", "0", "]", "]", "+=", "1", "\n", "# These spans weren't predicted.", "\n", "", "", "for", "span", "in", "gold_spans", ":", "\n", "                ", "self", ".", "_false_negatives", "[", "span", "[", "0", "]", "]", "+=", "1", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.metrics.span_based_f1_measure.SpanBasedF1Measure._handle_continued_spans": [[205, 242], ["set", "list", "min", "max", "span_set.difference_update", "span_set.add", "label.startswith"], "methods", ["None"], ["", "", "", "@", "staticmethod", "\n", "def", "_handle_continued_spans", "(", "spans", ":", "List", "[", "TypedStringSpan", "]", ")", "->", "List", "[", "TypedStringSpan", "]", ":", "\n", "        ", "\"\"\"\n        The official CONLL 2012 evaluation script for SRL treats continued spans (i.e spans which\n        have a `C-` prepended to another valid tag) as part of the span that they are continuing.\n        This is basically a massive hack to allow SRL models which produce a linear sequence of\n        predictions to do something close to structured prediction. However, this means that to\n        compute the metric, these continuation spans need to be merged into the span to which\n        they refer. The way this is done is to simply consider the span for the continued argument\n        to start at the start index of the first occurrence of the span and end at the end index\n        of the last occurrence of the span. Handling this is important, because predicting continued\n        spans is difficult and typically will effect overall average F1 score by ~ 2 points.\n\n        # Parameters\n\n        spans : `List[TypedStringSpan]`, required.\n            A list of (label, (start, end)) spans.\n\n        # Returns\n\n        A `List[TypedStringSpan]` with continued arguments replaced with a single span.\n        \"\"\"", "\n", "span_set", ":", "Set", "[", "TypedStringSpan", "]", "=", "set", "(", "spans", ")", "\n", "continued_labels", ":", "List", "[", "str", "]", "=", "[", "\n", "label", "[", "2", ":", "]", "for", "(", "label", ",", "span", ")", "in", "span_set", "if", "label", ".", "startswith", "(", "\"C-\"", ")", "\n", "]", "\n", "for", "label", "in", "continued_labels", ":", "\n", "            ", "continued_spans", "=", "{", "span", "for", "span", "in", "span_set", "if", "label", "in", "span", "[", "0", "]", "}", "\n", "\n", "span_start", "=", "min", "(", "span", "[", "1", "]", "[", "0", "]", "for", "span", "in", "continued_spans", ")", "\n", "span_end", "=", "max", "(", "span", "[", "1", "]", "[", "1", "]", "for", "span", "in", "continued_spans", ")", "\n", "replacement_span", ":", "TypedStringSpan", "=", "(", "label", ",", "(", "span_start", ",", "span_end", ")", ")", "\n", "\n", "span_set", ".", "difference_update", "(", "continued_spans", ")", "\n", "span_set", ".", "add", "(", "replacement_span", ")", "\n", "\n", "", "return", "list", "(", "span_set", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.metrics.span_based_f1_measure.SpanBasedF1Measure.get_metric": [[243, 285], ["set", "all_tags.update", "all_tags.update", "all_tags.update", "span_based_f1_measure.SpanBasedF1Measure._compute_metrics", "span_based_f1_measure.SpanBasedF1Measure._true_positives.keys", "span_based_f1_measure.SpanBasedF1Measure._false_positives.keys", "span_based_f1_measure.SpanBasedF1Measure._false_negatives.keys", "span_based_f1_measure.SpanBasedF1Measure._compute_metrics", "sum", "sum", "sum", "span_based_f1_measure.SpanBasedF1Measure.reset", "span_based_f1_measure.SpanBasedF1Measure._true_positives.values", "span_based_f1_measure.SpanBasedF1Measure._false_positives.values", "span_based_f1_measure.SpanBasedF1Measure._false_negatives.values"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.metrics.conll_coref_scores.Scorer.update", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.metrics.conll_coref_scores.Scorer.update", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.metrics.conll_coref_scores.Scorer.update", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.metrics.span_based_f1_measure.SpanBasedF1Measure._compute_metrics", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.metrics.span_based_f1_measure.SpanBasedF1Measure._compute_metrics", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.metrics.sequence_accuracy.SequenceAccuracy.reset"], ["", "def", "get_metric", "(", "self", ",", "reset", ":", "bool", "=", "False", ")", ":", "\n", "        ", "\"\"\"\n        # Returns\n\n        A Dict per label containing following the span based metrics:\n        precision : float\n        recall : float\n        f1-measure : float\n\n        Additionally, an `overall` key is included, which provides the precision,\n        recall and f1-measure for all spans.\n        \"\"\"", "\n", "all_tags", ":", "Set", "[", "str", "]", "=", "set", "(", ")", "\n", "all_tags", ".", "update", "(", "self", ".", "_true_positives", ".", "keys", "(", ")", ")", "\n", "all_tags", ".", "update", "(", "self", ".", "_false_positives", ".", "keys", "(", ")", ")", "\n", "all_tags", ".", "update", "(", "self", ".", "_false_negatives", ".", "keys", "(", ")", ")", "\n", "all_metrics", "=", "{", "}", "\n", "for", "tag", "in", "all_tags", ":", "\n", "            ", "precision", ",", "recall", ",", "f1_measure", "=", "self", ".", "_compute_metrics", "(", "\n", "self", ".", "_true_positives", "[", "tag", "]", ",", "\n", "self", ".", "_false_positives", "[", "tag", "]", ",", "\n", "self", ".", "_false_negatives", "[", "tag", "]", ",", "\n", ")", "\n", "precision_key", "=", "\"precision\"", "+", "\"-\"", "+", "tag", "\n", "recall_key", "=", "\"recall\"", "+", "\"-\"", "+", "tag", "\n", "f1_key", "=", "\"f1-measure\"", "+", "\"-\"", "+", "tag", "\n", "all_metrics", "[", "precision_key", "]", "=", "precision", "\n", "all_metrics", "[", "recall_key", "]", "=", "recall", "\n", "all_metrics", "[", "f1_key", "]", "=", "f1_measure", "\n", "\n", "# Compute the precision, recall and f1 for all spans jointly.", "\n", "", "precision", ",", "recall", ",", "f1_measure", "=", "self", ".", "_compute_metrics", "(", "\n", "sum", "(", "self", ".", "_true_positives", ".", "values", "(", ")", ")", ",", "\n", "sum", "(", "self", ".", "_false_positives", ".", "values", "(", ")", ")", ",", "\n", "sum", "(", "self", ".", "_false_negatives", ".", "values", "(", ")", ")", ",", "\n", ")", "\n", "all_metrics", "[", "\"precision-overall\"", "]", "=", "precision", "\n", "all_metrics", "[", "\"recall-overall\"", "]", "=", "recall", "\n", "all_metrics", "[", "\"f1-measure-overall\"", "]", "=", "f1_measure", "\n", "if", "reset", ":", "\n", "            ", "self", ".", "reset", "(", ")", "\n", "", "return", "all_metrics", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.metrics.span_based_f1_measure.SpanBasedF1Measure._compute_metrics": [[286, 296], ["float", "float", "float", "float"], "methods", ["None"], ["", "@", "staticmethod", "\n", "def", "_compute_metrics", "(", "\n", "true_positives", ":", "int", ",", "false_positives", ":", "int", ",", "false_negatives", ":", "int", "\n", ")", ":", "\n", "        ", "precision", "=", "float", "(", "true_positives", ")", "/", "float", "(", "\n", "true_positives", "+", "false_positives", "+", "1e-13", "\n", ")", "\n", "recall", "=", "float", "(", "true_positives", ")", "/", "float", "(", "true_positives", "+", "false_negatives", "+", "1e-13", ")", "\n", "f1_measure", "=", "2.0", "*", "(", "(", "precision", "*", "recall", ")", "/", "(", "precision", "+", "recall", "+", "1e-13", ")", ")", "\n", "return", "precision", ",", "recall", ",", "f1_measure", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.metrics.span_based_f1_measure.SpanBasedF1Measure.reset": [[297, 301], ["collections.defaultdict", "collections.defaultdict", "collections.defaultdict"], "methods", ["None"], ["", "def", "reset", "(", "self", ")", ":", "\n", "        ", "self", ".", "_true_positives", "=", "defaultdict", "(", "int", ")", "\n", "self", ".", "_false_positives", "=", "defaultdict", "(", "int", ")", "\n", "self", ".", "_false_negatives", "=", "defaultdict", "(", "int", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.metrics.spearman_correlation.SpearmanCorrelation.__init__": [[22, 26], ["allennlp.training.metrics.metric.Metric.__init__", "torch.zeros", "torch.zeros"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.graph_util.plot.GraphPlot.__init__"], ["def", "__init__", "(", "self", ")", "->", "None", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "total_predictions", "=", "torch", ".", "zeros", "(", "0", ")", "\n", "self", ".", "total_gold_labels", "=", "torch", ".", "zeros", "(", "0", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.metrics.spearman_correlation.SpearmanCorrelation.__call__": [[27, 63], ["spearman_correlation.SpearmanCorrelation.unwrap_to_tensors", "predictions.view.view.view", "gold_labels.view.view.view", "mask.view.view.view", "torch.cat", "torch.cat", "torch.cat", "torch.cat"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.metrics.metric.Metric.unwrap_to_tensors"], ["", "def", "__call__", "(", "\n", "self", ",", "\n", "predictions", ":", "torch", ".", "Tensor", ",", "\n", "gold_labels", ":", "torch", ".", "Tensor", ",", "\n", "mask", ":", "Optional", "[", "torch", ".", "Tensor", "]", "=", "None", ",", "\n", ")", ":", "\n", "        ", "\"\"\"\n        # Parameters\n\n        predictions : `torch.Tensor`, required.\n            A tensor of predictions of shape (batch_size, ...).\n        gold_labels : `torch.Tensor`, required.\n            A tensor of the same shape as `predictions`.\n        mask : `torch.Tensor`, optional (default = None).\n            A tensor of the same shape as `predictions`.\n        \"\"\"", "\n", "predictions", ",", "gold_labels", ",", "mask", "=", "self", ".", "unwrap_to_tensors", "(", "\n", "predictions", ",", "gold_labels", ",", "mask", "\n", ")", "\n", "# Flatten predictions, gold_labels, and mask. We calculate the spearman correlation between", "\n", "# the vectors, since each element in the predictions and gold_labels tensor is assumed", "\n", "# to be a separate observation.", "\n", "predictions", "=", "predictions", ".", "view", "(", "-", "1", ")", "\n", "gold_labels", "=", "gold_labels", ".", "view", "(", "-", "1", ")", "\n", "\n", "if", "mask", "is", "not", "None", ":", "\n", "            ", "mask", "=", "mask", ".", "view", "(", "-", "1", ")", "\n", "self", ".", "total_predictions", "=", "torch", ".", "cat", "(", "\n", "(", "self", ".", "total_predictions", ",", "predictions", "*", "mask", ")", ",", "0", "\n", ")", "\n", "self", ".", "total_gold_labels", "=", "torch", ".", "cat", "(", "\n", "(", "self", ".", "total_gold_labels", ",", "gold_labels", "*", "mask", ")", ",", "0", "\n", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "total_predictions", "=", "torch", ".", "cat", "(", "(", "self", ".", "total_predictions", ",", "predictions", ")", ",", "0", ")", "\n", "self", ".", "total_gold_labels", "=", "torch", ".", "cat", "(", "(", "self", ".", "total_gold_labels", ",", "gold_labels", ")", ",", "0", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.metrics.spearman_correlation.SpearmanCorrelation.get_metric": [[64, 79], ["scipy.spearmanr", "stats.spearmanr.SpearmanCorrelation.total_predictions.numpy", "stats.spearmanr.SpearmanCorrelation.total_gold_labels.numpy", "stats.spearmanr.SpearmanCorrelation.reset"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.metrics.sequence_accuracy.SequenceAccuracy.reset"], ["", "", "@", "overrides", "\n", "def", "get_metric", "(", "self", ",", "reset", ":", "bool", "=", "False", ")", ":", "\n", "        ", "\"\"\"\n        # Returns\n\n        The accumulated sample Spearman correlation.\n        \"\"\"", "\n", "spearman_correlation", "=", "stats", ".", "spearmanr", "(", "\n", "self", ".", "total_predictions", ".", "numpy", "(", ")", ",", "self", ".", "total_gold_labels", ".", "numpy", "(", ")", "\n", ")", "\n", "\n", "if", "reset", ":", "\n", "            ", "self", ".", "reset", "(", ")", "\n", "\n", "", "return", "spearman_correlation", "[", "0", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.metrics.spearman_correlation.SpearmanCorrelation.reset": [[80, 84], ["torch.zeros", "torch.zeros"], "methods", ["None"], ["", "@", "overrides", "\n", "def", "reset", "(", "self", ")", ":", "\n", "        ", "self", ".", "total_predictions", "=", "torch", ".", "zeros", "(", "0", ")", "\n", "self", ".", "total_gold_labels", "=", "torch", ".", "zeros", "(", "0", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.metrics.perplexity.Perplexity.get_metric": [[20, 33], ["super().get_metric", "float", "torch.exp"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.metrics.sequence_accuracy.SequenceAccuracy.get_metric"], ["@", "overrides", "\n", "def", "get_metric", "(", "self", ",", "reset", ":", "bool", "=", "False", ")", "->", "float", ":", "\n", "        ", "\"\"\"\n        # Returns\n\n        The accumulated perplexity.\n        \"\"\"", "\n", "average_loss", "=", "super", "(", ")", ".", "get_metric", "(", "reset", ")", "\n", "if", "average_loss", "==", "0", ":", "\n", "            ", "return", "0.0", "\n", "\n", "# Exponentiate the loss to compute perplexity", "\n", "", "return", "float", "(", "torch", ".", "exp", "(", "average_loss", ")", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.metrics.metric.Metric.__call__": [[13, 31], ["None"], "methods", ["None"], ["def", "__call__", "(", "\n", "self", ",", "\n", "predictions", ":", "torch", ".", "Tensor", ",", "\n", "gold_labels", ":", "torch", ".", "Tensor", ",", "\n", "mask", ":", "Optional", "[", "torch", ".", "Tensor", "]", ",", "\n", ")", ":", "\n", "        ", "\"\"\"\n        # Parameters\n\n        predictions : `torch.Tensor`, required.\n            A tensor of predictions.\n        gold_labels : `torch.Tensor`, required.\n            A tensor corresponding to some gold label to evaluate against.\n        mask : `torch.Tensor`, optional (default = None).\n            A mask can be passed, in order to deal with metrics which are\n            computed over potentially padded elements, such as sequence labels.\n        \"\"\"", "\n", "raise", "NotImplementedError", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.metrics.metric.Metric.get_metric": [[32, 39], ["None"], "methods", ["None"], ["", "def", "get_metric", "(", "\n", "self", ",", "reset", ":", "bool", "\n", ")", "->", "Union", "[", "float", ",", "Tuple", "[", "float", ",", "...", "]", ",", "Dict", "[", "str", ",", "float", "]", ",", "Dict", "[", "str", ",", "List", "[", "float", "]", "]", "]", ":", "\n", "        ", "\"\"\"\n        Compute and return the metric. Optionally also call `self.reset`.\n        \"\"\"", "\n", "raise", "NotImplementedError", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.metrics.metric.Metric.reset": [[40, 45], ["None"], "methods", ["None"], ["", "def", "reset", "(", "self", ")", "->", "None", ":", "\n", "        ", "\"\"\"\n        Reset any accumulators or internal state.\n        \"\"\"", "\n", "raise", "NotImplementedError", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.metrics.metric.Metric.unwrap_to_tensors": [[46, 55], ["isinstance", "x.detach().cpu", "x.detach"], "methods", ["None"], ["", "@", "staticmethod", "\n", "def", "unwrap_to_tensors", "(", "*", "tensors", ":", "torch", ".", "Tensor", ")", ":", "\n", "        ", "\"\"\"\n        If you actually passed gradient-tracking Tensors to a Metric, there will be\n        a huge memory leak, because it will prevent garbage collection for the computation\n        graph. This method ensures that you're using tensors directly and that they are on\n        the CPU.\n        \"\"\"", "\n", "return", "(", "x", ".", "detach", "(", ")", ".", "cpu", "(", ")", "if", "isinstance", "(", "x", ",", "torch", ".", "Tensor", ")", "else", "x", "for", "x", "in", "tensors", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.metrics.attachment_scores.AttachmentScores.__init__": [[24, 33], ["None"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "ignore_classes", ":", "List", "[", "int", "]", "=", "None", ")", "->", "None", ":", "\n", "        ", "self", ".", "_labeled_correct", "=", "0.0", "\n", "self", ".", "_unlabeled_correct", "=", "0.0", "\n", "self", ".", "_exact_labeled_correct", "=", "0.0", "\n", "self", ".", "_exact_unlabeled_correct", "=", "0.0", "\n", "self", ".", "_total_words", "=", "0.0", "\n", "self", ".", "_total_sentences", "=", "0.0", "\n", "\n", "self", ".", "_ignore_classes", ":", "List", "[", "int", "]", "=", "ignore_classes", "or", "[", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.metrics.attachment_scores.AttachmentScores.__call__": [[34, 85], ["attachment_scores.AttachmentScores.unwrap_to_tensors", "mask.long.long.long", "predicted_indices.long.long.long", "predicted_labels.long.long.long", "gold_indices.long.long.long", "gold_labels.long.long.long", "correct_indices.sum", "unlabeled_exact_match.sum", "correct_labels_and_indices.sum", "labeled_exact_match.sum", "correct_indices.size", "gold_labels.long.long.eq", "predicted_indices.long.long.eq().long", "predicted_labels.long.long.eq().long", "correct_indices.numel", "predicted_indices.long.long.eq", "predicted_labels.long.long.eq"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.metrics.metric.Metric.unwrap_to_tensors"], ["", "def", "__call__", "(", "# type: ignore", "\n", "self", ",", "\n", "predicted_indices", ":", "torch", ".", "Tensor", ",", "\n", "predicted_labels", ":", "torch", ".", "Tensor", ",", "\n", "gold_indices", ":", "torch", ".", "Tensor", ",", "\n", "gold_labels", ":", "torch", ".", "Tensor", ",", "\n", "mask", ":", "Optional", "[", "torch", ".", "Tensor", "]", "=", "None", ",", "\n", ")", ":", "\n", "        ", "\"\"\"\n        # Parameters\n\n        predicted_indices : `torch.Tensor`, required.\n            A tensor of head index predictions of shape (batch_size, timesteps).\n        predicted_labels : `torch.Tensor`, required.\n            A tensor of arc label predictions of shape (batch_size, timesteps).\n        gold_indices : `torch.Tensor`, required.\n            A tensor of the same shape as `predicted_indices`.\n        gold_labels : `torch.Tensor`, required.\n            A tensor of the same shape as `predicted_labels`.\n        mask : `torch.Tensor`, optional (default = None).\n            A tensor of the same shape as `predicted_indices`.\n        \"\"\"", "\n", "unwrapped", "=", "self", ".", "unwrap_to_tensors", "(", "\n", "predicted_indices", ",", "predicted_labels", ",", "gold_indices", ",", "gold_labels", ",", "mask", "\n", ")", "\n", "predicted_indices", ",", "predicted_labels", ",", "gold_indices", ",", "gold_labels", ",", "mask", "=", "unwrapped", "\n", "\n", "mask", "=", "mask", ".", "long", "(", ")", "\n", "predicted_indices", "=", "predicted_indices", ".", "long", "(", ")", "\n", "predicted_labels", "=", "predicted_labels", ".", "long", "(", ")", "\n", "gold_indices", "=", "gold_indices", ".", "long", "(", ")", "\n", "gold_labels", "=", "gold_labels", ".", "long", "(", ")", "\n", "\n", "# Multiply by a mask denoting locations of", "\n", "# gold labels which we should ignore.", "\n", "for", "label", "in", "self", ".", "_ignore_classes", ":", "\n", "            ", "label_mask", "=", "gold_labels", ".", "eq", "(", "label", ")", "\n", "mask", "=", "mask", "*", "(", "~", "label_mask", ")", ".", "long", "(", ")", "\n", "\n", "", "correct_indices", "=", "predicted_indices", ".", "eq", "(", "gold_indices", ")", ".", "long", "(", ")", "*", "mask", "\n", "unlabeled_exact_match", "=", "(", "correct_indices", "+", "(", "1", "-", "mask", ")", ")", ".", "prod", "(", "dim", "=", "-", "1", ")", "\n", "correct_labels", "=", "predicted_labels", ".", "eq", "(", "gold_labels", ")", ".", "long", "(", ")", "*", "mask", "\n", "correct_labels_and_indices", "=", "correct_indices", "*", "correct_labels", "\n", "labeled_exact_match", "=", "(", "correct_labels_and_indices", "+", "(", "1", "-", "mask", ")", ")", ".", "prod", "(", "dim", "=", "-", "1", ")", "\n", "\n", "self", ".", "_unlabeled_correct", "+=", "correct_indices", ".", "sum", "(", ")", "\n", "self", ".", "_exact_unlabeled_correct", "+=", "unlabeled_exact_match", ".", "sum", "(", ")", "\n", "self", ".", "_labeled_correct", "+=", "correct_labels_and_indices", ".", "sum", "(", ")", "\n", "self", ".", "_exact_labeled_correct", "+=", "labeled_exact_match", ".", "sum", "(", ")", "\n", "self", ".", "_total_sentences", "+=", "correct_indices", ".", "size", "(", "0", ")", "\n", "self", ".", "_total_words", "+=", "correct_indices", ".", "numel", "(", ")", "-", "(", "1", "-", "mask", ")", ".", "sum", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.metrics.attachment_scores.AttachmentScores.get_metric": [[86, 117], ["attachment_scores.AttachmentScores.reset", "float", "float", "float", "float", "float", "float", "float", "float"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.metrics.sequence_accuracy.SequenceAccuracy.reset"], ["", "def", "get_metric", "(", "self", ",", "reset", ":", "bool", "=", "False", ")", ":", "\n", "        ", "\"\"\"\n        # Returns\n\n        The accumulated metrics as a dictionary.\n        \"\"\"", "\n", "unlabeled_attachment_score", "=", "0.0", "\n", "labeled_attachment_score", "=", "0.0", "\n", "unlabeled_exact_match", "=", "0.0", "\n", "labeled_exact_match", "=", "0.0", "\n", "if", "self", ".", "_total_words", ">", "0.0", ":", "\n", "            ", "unlabeled_attachment_score", "=", "float", "(", "self", ".", "_unlabeled_correct", ")", "/", "float", "(", "\n", "self", ".", "_total_words", "\n", ")", "\n", "labeled_attachment_score", "=", "float", "(", "self", ".", "_labeled_correct", ")", "/", "float", "(", "\n", "self", ".", "_total_words", "\n", ")", "\n", "", "if", "self", ".", "_total_sentences", ">", "0", ":", "\n", "            ", "unlabeled_exact_match", "=", "float", "(", "self", ".", "_exact_unlabeled_correct", ")", "/", "float", "(", "\n", "self", ".", "_total_sentences", "\n", ")", "\n", "labeled_exact_match", "=", "float", "(", "self", ".", "_exact_labeled_correct", ")", "/", "float", "(", "\n", "self", ".", "_total_sentences", "\n", ")", "\n", "", "if", "reset", ":", "\n", "            ", "self", ".", "reset", "(", ")", "\n", "", "return", "{", "\n", "\"UAS\"", ":", "unlabeled_attachment_score", ",", "\n", "\"LAS\"", ":", "labeled_attachment_score", ",", "\n", "\"UEM\"", ":", "unlabeled_exact_match", ",", "\n", "\"LEM\"", ":", "labeled_exact_match", ",", "\n", "}", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.metrics.attachment_scores.AttachmentScores.reset": [[119, 127], ["None"], "methods", ["None"], ["", "@", "overrides", "\n", "def", "reset", "(", "self", ")", ":", "\n", "        ", "self", ".", "_labeled_correct", "=", "0.0", "\n", "self", ".", "_unlabeled_correct", "=", "0.0", "\n", "self", ".", "_exact_labeled_correct", "=", "0.0", "\n", "self", ".", "_exact_unlabeled_correct", "=", "0.0", "\n", "self", ".", "_total_words", "=", "0.0", "\n", "self", ".", "_total_sentences", "=", "0.0", "\n", "", "", ""]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.metrics.auc.Auc.__init__": [[18, 23], ["allennlp.training.metrics.metric.Metric.__init__", "torch.FloatTensor", "torch.LongTensor"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.graph_util.plot.GraphPlot.__init__"], ["def", "__init__", "(", "self", ",", "positive_label", "=", "1", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "_positive_label", "=", "positive_label", "\n", "self", ".", "_all_predictions", "=", "torch", ".", "FloatTensor", "(", ")", "\n", "self", ".", "_all_gold_labels", "=", "torch", ".", "LongTensor", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.metrics.auc.Auc.__call__": [[24, 85], ["auc.Auc.unwrap_to_tensors", "torch.unique", "torch.ones.to", "torch.cat", "torch.cat", "gold_labels.dim", "allennlp.common.checks.ConfigurationError", "predictions.dim", "allennlp.common.checks.ConfigurationError", "torch.unique.numel", "allennlp.common.checks.ConfigurationError", "set", "allennlp.common.checks.ConfigurationError", "torch.ones", "torch.unique.tolist", "torch.masked_select().float", "torch.masked_select().long", "gold_labels.size", "predictions.size", "torch.unique.numel", "torch.masked_select", "torch.masked_select"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.metrics.metric.Metric.unwrap_to_tensors"], ["", "def", "__call__", "(", "\n", "self", ",", "\n", "predictions", ":", "torch", ".", "Tensor", ",", "\n", "gold_labels", ":", "torch", ".", "Tensor", ",", "\n", "mask", ":", "Optional", "[", "torch", ".", "Tensor", "]", "=", "None", ",", "\n", ")", ":", "\n", "        ", "\"\"\"\n        # Parameters\n\n        predictions : `torch.Tensor`, required.\n            A one-dimensional tensor of prediction scores of shape (batch_size).\n        gold_labels : `torch.Tensor`, required.\n            A one-dimensional label tensor of shape (batch_size), with {1, 0}\n            entries for positive and negative class. If it's not binary,\n            `positive_label` should be passed in the initialization.\n        mask : `torch.Tensor`, optional (default = None).\n            A one-dimensional label tensor of shape (batch_size).\n        \"\"\"", "\n", "\n", "predictions", ",", "gold_labels", ",", "mask", "=", "self", ".", "unwrap_to_tensors", "(", "\n", "predictions", ",", "gold_labels", ",", "mask", "\n", ")", "\n", "\n", "# Sanity checks.", "\n", "if", "gold_labels", ".", "dim", "(", ")", "!=", "1", ":", "\n", "            ", "raise", "ConfigurationError", "(", "\n", "\"gold_labels must be one-dimensional, \"", "\n", "\"but found tensor of shape: {}\"", ".", "format", "(", "gold_labels", ".", "size", "(", ")", ")", "\n", ")", "\n", "", "if", "predictions", ".", "dim", "(", ")", "!=", "1", ":", "\n", "            ", "raise", "ConfigurationError", "(", "\n", "\"predictions must be one-dimensional, \"", "\n", "\"but found tensor of shape: {}\"", ".", "format", "(", "predictions", ".", "size", "(", ")", ")", "\n", ")", "\n", "\n", "", "unique_gold_labels", "=", "torch", ".", "unique", "(", "gold_labels", ")", "\n", "if", "unique_gold_labels", ".", "numel", "(", ")", ">", "2", ":", "\n", "            ", "raise", "ConfigurationError", "(", "\n", "\"AUC can be used for binary tasks only. gold_labels has {} unique labels, \"", "\n", "\"expected at maximum 2.\"", ".", "format", "(", "unique_gold_labels", ".", "numel", "(", ")", ")", "\n", ")", "\n", "\n", "", "gold_labels_is_binary", "=", "set", "(", "unique_gold_labels", ".", "tolist", "(", ")", ")", "<=", "{", "0", ",", "1", "}", "\n", "if", "not", "gold_labels_is_binary", "and", "self", ".", "_positive_label", "not", "in", "unique_gold_labels", ":", "\n", "            ", "raise", "ConfigurationError", "(", "\n", "\"gold_labels should be binary with 0 and 1 or initialized positive_label \"", "\n", "\"{} should be present in gold_labels\"", ".", "format", "(", "self", ".", "_positive_label", ")", "\n", ")", "\n", "\n", "", "if", "mask", "is", "None", ":", "\n", "            ", "batch_size", "=", "gold_labels", ".", "shape", "[", "0", "]", "\n", "mask", "=", "torch", ".", "ones", "(", "batch_size", ")", "\n", "", "mask", "=", "mask", ".", "to", "(", "dtype", "=", "torch", ".", "bool", ")", "\n", "\n", "self", ".", "_all_predictions", "=", "torch", ".", "cat", "(", "\n", "[", "self", ".", "_all_predictions", ",", "torch", ".", "masked_select", "(", "predictions", ",", "mask", ")", ".", "float", "(", ")", "]", ",", "\n", "dim", "=", "0", ",", "\n", ")", "\n", "self", ".", "_all_gold_labels", "=", "torch", ".", "cat", "(", "\n", "[", "self", ".", "_all_gold_labels", ",", "torch", ".", "masked_select", "(", "gold_labels", ",", "mask", ")", ".", "long", "(", ")", "]", ",", "\n", "dim", "=", "0", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.metrics.auc.Auc.get_metric": [[87, 99], ["sklearn.metrics.roc_curve", "sklearn.metrics.auc", "metrics.auc.Auc._all_gold_labels.numpy", "metrics.auc.Auc._all_predictions.numpy", "metrics.auc.Auc.reset"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.metrics.sequence_accuracy.SequenceAccuracy.reset"], ["", "def", "get_metric", "(", "self", ",", "reset", ":", "bool", "=", "False", ")", ":", "\n", "        ", "if", "self", ".", "_all_gold_labels", ".", "shape", "[", "0", "]", "==", "0", ":", "\n", "            ", "return", "0.5", "\n", "", "false_positive_rates", ",", "true_positive_rates", ",", "_", "=", "metrics", ".", "roc_curve", "(", "\n", "self", ".", "_all_gold_labels", ".", "numpy", "(", ")", ",", "\n", "self", ".", "_all_predictions", ".", "numpy", "(", ")", ",", "\n", "pos_label", "=", "self", ".", "_positive_label", ",", "\n", ")", "\n", "auc", "=", "metrics", ".", "auc", "(", "false_positive_rates", ",", "true_positive_rates", ")", "\n", "if", "reset", ":", "\n", "            ", "self", ".", "reset", "(", ")", "\n", "", "return", "auc", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.metrics.auc.Auc.reset": [[100, 104], ["torch.FloatTensor", "torch.LongTensor"], "methods", ["None"], ["", "@", "overrides", "\n", "def", "reset", "(", "self", ")", ":", "\n", "        ", "self", ".", "_all_predictions", "=", "torch", ".", "FloatTensor", "(", ")", "\n", "self", ".", "_all_gold_labels", "=", "torch", ".", "LongTensor", "(", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.metrics.fbeta_measure.FBetaMeasure.__init__": [[59, 88], ["allennlp.common.checks.ConfigurationError", "allennlp.common.checks.ConfigurationError", "allennlp.common.checks.ConfigurationError", "len"], "methods", ["None"], ["def", "__init__", "(", "\n", "self", ",", "beta", ":", "float", "=", "1.0", ",", "average", ":", "str", "=", "None", ",", "labels", ":", "List", "[", "int", "]", "=", "None", "\n", ")", "->", "None", ":", "\n", "        ", "average_options", "=", "(", "None", ",", "\"micro\"", ",", "\"macro\"", ")", "\n", "if", "average", "not", "in", "average_options", ":", "\n", "            ", "raise", "ConfigurationError", "(", "f\"`average` has to be one of {average_options}.\"", ")", "\n", "", "if", "beta", "<=", "0", ":", "\n", "            ", "raise", "ConfigurationError", "(", "\"`beta` should be >0 in the F-beta score.\"", ")", "\n", "", "if", "labels", "is", "not", "None", "and", "len", "(", "labels", ")", "==", "0", ":", "\n", "            ", "raise", "ConfigurationError", "(", "\"`labels` cannot be an empty list \"", ")", "\n", "", "self", ".", "_beta", "=", "beta", "\n", "self", ".", "_average", "=", "average", "\n", "self", ".", "_labels", "=", "labels", "\n", "\n", "# statistics", "\n", "# the total number of true positive instances under each class", "\n", "# Shape: (num_classes, )", "\n", "self", ".", "_true_positive_sum", ":", "Union", "[", "None", ",", "torch", ".", "Tensor", "]", "=", "None", "\n", "# the total number of instances", "\n", "# Shape: (num_classes, )", "\n", "self", ".", "_total_sum", ":", "Union", "[", "None", ",", "torch", ".", "Tensor", "]", "=", "None", "\n", "# the total number of instances under each _predicted_ class,", "\n", "# including true positives and false positives", "\n", "# Shape: (num_classes, )", "\n", "self", ".", "_pred_sum", ":", "Union", "[", "None", ",", "torch", ".", "Tensor", "]", "=", "None", "\n", "# the total number of instances under each _true_ class,", "\n", "# including true positives and false negatives", "\n", "# Shape: (num_classes, )", "\n", "self", ".", "_true_sum", ":", "Union", "[", "None", ",", "torch", ".", "Tensor", "]", "=", "None", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.metrics.fbeta_measure.FBetaMeasure.__call__": [[89, 163], ["fbeta_measure.FBetaMeasure.unwrap_to_tensors", "predictions.size", "torch.ones_like.to", "gold_labels.float.float.float", "[].float", "argmax_predictions[].long", "gold_labels[].long", "torch.ones_like.sum().to", "allennlp.common.checks.ConfigurationError", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.ones_like", "torch.zeros", "torch.bincount().float", "torch.bincount().float", "torch.zeros", "torch.bincount().float", "torch.zeros", "torch.ones_like.sum", "predictions.max", "torch.bincount", "torch.bincount", "torch.bincount", "true_positives_bins.long"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.metrics.metric.Metric.unwrap_to_tensors"], ["", "@", "overrides", "\n", "def", "__call__", "(", "\n", "self", ",", "\n", "predictions", ":", "torch", ".", "Tensor", ",", "\n", "gold_labels", ":", "torch", ".", "Tensor", ",", "\n", "mask", ":", "Optional", "[", "torch", ".", "Tensor", "]", "=", "None", ",", "\n", ")", ":", "\n", "        ", "\"\"\"\n        # Parameters\n\n        predictions : `torch.Tensor`, required.\n            A tensor of predictions of shape (batch_size, ..., num_classes).\n        gold_labels : `torch.Tensor`, required.\n            A tensor of integer class label of shape (batch_size, ...). It must be the same\n            shape as the `predictions` tensor without the `num_classes` dimension.\n        mask : `torch.Tensor`, optional (default = None).\n            A masking tensor the same size as `gold_labels`.\n        \"\"\"", "\n", "predictions", ",", "gold_labels", ",", "mask", "=", "self", ".", "unwrap_to_tensors", "(", "\n", "predictions", ",", "gold_labels", ",", "mask", "\n", ")", "\n", "\n", "# Calculate true_positive_sum, true_negative_sum, pred_sum, true_sum", "\n", "num_classes", "=", "predictions", ".", "size", "(", "-", "1", ")", "\n", "if", "(", "gold_labels", ">=", "num_classes", ")", ".", "any", "(", ")", ":", "\n", "            ", "raise", "ConfigurationError", "(", "\n", "\"A gold label passed to FBetaMeasure contains \"", "\n", "f\"an id >= {num_classes}, the number of classes.\"", "\n", ")", "\n", "\n", "# It means we call this metric at the first time", "\n", "# when `self._true_positive_sum` is None.", "\n", "", "if", "self", ".", "_true_positive_sum", "is", "None", ":", "\n", "            ", "self", ".", "_true_positive_sum", "=", "torch", ".", "zeros", "(", "num_classes", ")", "\n", "self", ".", "_true_sum", "=", "torch", ".", "zeros", "(", "num_classes", ")", "\n", "self", ".", "_pred_sum", "=", "torch", ".", "zeros", "(", "num_classes", ")", "\n", "self", ".", "_total_sum", "=", "torch", ".", "zeros", "(", "num_classes", ")", "\n", "\n", "", "if", "mask", "is", "None", ":", "\n", "            ", "mask", "=", "torch", ".", "ones_like", "(", "gold_labels", ")", "\n", "", "mask", "=", "mask", ".", "to", "(", "dtype", "=", "torch", ".", "bool", ")", "\n", "gold_labels", "=", "gold_labels", ".", "float", "(", ")", "\n", "\n", "argmax_predictions", "=", "predictions", ".", "max", "(", "dim", "=", "-", "1", ")", "[", "1", "]", ".", "float", "(", ")", "\n", "true_positives", "=", "(", "gold_labels", "==", "argmax_predictions", ")", "*", "mask", "\n", "true_positives_bins", "=", "gold_labels", "[", "true_positives", "]", "\n", "\n", "# Watch it:", "\n", "# The total numbers of true positives under all _predicted_ classes are zeros.", "\n", "if", "true_positives_bins", ".", "shape", "[", "0", "]", "==", "0", ":", "\n", "            ", "true_positive_sum", "=", "torch", ".", "zeros", "(", "num_classes", ")", "\n", "", "else", ":", "\n", "            ", "true_positive_sum", "=", "torch", ".", "bincount", "(", "\n", "true_positives_bins", ".", "long", "(", ")", ",", "minlength", "=", "num_classes", "\n", ")", ".", "float", "(", ")", "\n", "\n", "", "pred_bins", "=", "argmax_predictions", "[", "mask", "]", ".", "long", "(", ")", "\n", "# Watch it:", "\n", "# When the `mask` is all 0, we will get an _empty_ tensor.", "\n", "if", "pred_bins", ".", "shape", "[", "0", "]", "!=", "0", ":", "\n", "            ", "pred_sum", "=", "torch", ".", "bincount", "(", "pred_bins", ",", "minlength", "=", "num_classes", ")", ".", "float", "(", ")", "\n", "", "else", ":", "\n", "            ", "pred_sum", "=", "torch", ".", "zeros", "(", "num_classes", ")", "\n", "\n", "", "gold_labels_bins", "=", "gold_labels", "[", "mask", "]", ".", "long", "(", ")", "\n", "if", "gold_labels", ".", "shape", "[", "0", "]", "!=", "0", ":", "\n", "            ", "true_sum", "=", "torch", ".", "bincount", "(", "gold_labels_bins", ",", "minlength", "=", "num_classes", ")", ".", "float", "(", ")", "\n", "", "else", ":", "\n", "            ", "true_sum", "=", "torch", ".", "zeros", "(", "num_classes", ")", "\n", "\n", "", "self", ".", "_true_positive_sum", "+=", "true_positive_sum", "\n", "self", ".", "_pred_sum", "+=", "pred_sum", "\n", "self", ".", "_true_sum", "+=", "true_sum", "\n", "self", ".", "_total_sum", "+=", "mask", ".", "sum", "(", ")", ".", "to", "(", "torch", ".", "float", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.metrics.fbeta_measure.FBetaMeasure.get_metric": [[164, 220], ["fbeta_measure._prf_divide", "fbeta_measure._prf_divide", "RuntimeError", "tp_sum.sum.sum.sum", "pred_sum.sum.sum.sum", "true_sum.sum.sum.sum", "precision.mean.mean.mean", "recall.mean.mean.mean", "fscore.mean.mean.mean", "fbeta_measure.FBetaMeasure.reset", "precision.mean.mean.tolist", "recall.mean.mean.tolist", "fscore.mean.mean.tolist", "precision.mean.mean.item", "recall.mean.mean.item", "fscore.mean.mean.item"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.metrics.fbeta_measure._prf_divide", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.metrics.fbeta_measure._prf_divide", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.metrics.sequence_accuracy.SequenceAccuracy.reset"], ["", "@", "overrides", "\n", "def", "get_metric", "(", "self", ",", "reset", ":", "bool", "=", "False", ")", ":", "\n", "        ", "\"\"\"\n        # Returns\n\n        A tuple of the following metrics based on the accumulated count statistics:\n        precisions : List[float]\n        recalls : List[float]\n        f1-measures : List[float]\n\n        If `self.average` is not `None`, you will get `float` instead of `List[float]`.\n        \"\"\"", "\n", "if", "self", ".", "_true_positive_sum", "is", "None", ":", "\n", "            ", "raise", "RuntimeError", "(", "\"You never call this metric before.\"", ")", "\n", "\n", "", "tp_sum", "=", "self", ".", "_true_positive_sum", "\n", "pred_sum", "=", "self", ".", "_pred_sum", "\n", "true_sum", "=", "self", ".", "_true_sum", "\n", "\n", "if", "self", ".", "_labels", "is", "not", "None", ":", "\n", "# Retain only selected labels and order them", "\n", "            ", "tp_sum", "=", "tp_sum", "[", "self", ".", "_labels", "]", "\n", "pred_sum", "=", "pred_sum", "[", "self", ".", "_labels", "]", "\n", "true_sum", "=", "true_sum", "[", "self", ".", "_labels", "]", "\n", "\n", "", "if", "self", ".", "_average", "==", "\"micro\"", ":", "\n", "            ", "tp_sum", "=", "tp_sum", ".", "sum", "(", ")", "\n", "pred_sum", "=", "pred_sum", ".", "sum", "(", ")", "\n", "true_sum", "=", "true_sum", ".", "sum", "(", ")", "\n", "\n", "", "beta2", "=", "self", ".", "_beta", "**", "2", "\n", "# Finally, we have all our sufficient statistics.", "\n", "precision", "=", "_prf_divide", "(", "tp_sum", ",", "pred_sum", ")", "\n", "recall", "=", "_prf_divide", "(", "tp_sum", ",", "true_sum", ")", "\n", "fscore", "=", "(", "1", "+", "beta2", ")", "*", "precision", "*", "recall", "/", "(", "beta2", "*", "precision", "+", "recall", ")", "\n", "fscore", "[", "tp_sum", "==", "0", "]", "=", "0.0", "\n", "\n", "if", "self", ".", "_average", "==", "\"macro\"", ":", "\n", "            ", "precision", "=", "precision", ".", "mean", "(", ")", "\n", "recall", "=", "recall", ".", "mean", "(", ")", "\n", "fscore", "=", "fscore", ".", "mean", "(", ")", "\n", "\n", "", "if", "reset", ":", "\n", "            ", "self", ".", "reset", "(", ")", "\n", "\n", "", "if", "self", ".", "_average", "is", "None", ":", "\n", "            ", "return", "{", "\n", "\"precision\"", ":", "precision", ".", "tolist", "(", ")", ",", "\n", "\"recall\"", ":", "recall", ".", "tolist", "(", ")", ",", "\n", "\"fscore\"", ":", "fscore", ".", "tolist", "(", ")", ",", "\n", "}", "\n", "", "else", ":", "\n", "            ", "return", "{", "\n", "\"precision\"", ":", "precision", ".", "item", "(", ")", ",", "\n", "\"recall\"", ":", "recall", ".", "item", "(", ")", ",", "\n", "\"fscore\"", ":", "fscore", ".", "item", "(", ")", ",", "\n", "}", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.metrics.fbeta_measure.FBetaMeasure.reset": [[222, 228], ["None"], "methods", ["None"], ["", "", "@", "overrides", "\n", "def", "reset", "(", "self", ")", "->", "None", ":", "\n", "        ", "self", ".", "_true_positive_sum", "=", "None", "\n", "self", ".", "_pred_sum", "=", "None", "\n", "self", ".", "_true_sum", "=", "None", "\n", "self", ".", "_total_sum", "=", "None", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.metrics.fbeta_measure.FBetaMeasure._true_negative_sum": [[229, 241], ["None"], "methods", ["None"], ["", "@", "property", "\n", "def", "_true_negative_sum", "(", "self", ")", ":", "\n", "        ", "if", "self", ".", "_total_sum", "is", "None", ":", "\n", "            ", "return", "None", "\n", "", "else", ":", "\n", "            ", "true_negative_sum", "=", "(", "\n", "self", ".", "_total_sum", "\n", "-", "self", ".", "_pred_sum", "\n", "-", "self", ".", "_true_sum", "\n", "+", "self", ".", "_true_positive_sum", "\n", ")", "\n", "return", "true_negative_sum", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.metrics.fbeta_measure._prf_divide": [[243, 256], ["mask.any"], "function", ["None"], ["", "", "", "def", "_prf_divide", "(", "numerator", ",", "denominator", ")", ":", "\n", "    ", "\"\"\"Performs division and handles divide-by-zero.\n\n    On zero-division, sets the corresponding result elements to zero.\n    \"\"\"", "\n", "result", "=", "numerator", "/", "denominator", "\n", "mask", "=", "denominator", "==", "0.0", "\n", "if", "not", "mask", ".", "any", "(", ")", ":", "\n", "        ", "return", "result", "\n", "\n", "# remove nan", "\n", "", "result", "[", "mask", "]", "=", "0.0", "\n", "return", "result", "\n", "", ""]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.metrics.sequence_accuracy.SequenceAccuracy.__init__": [[17, 20], ["None"], "methods", ["None"], ["def", "__init__", "(", "self", ")", "->", "None", ":", "\n", "        ", "self", ".", "correct_count", "=", "0.0", "\n", "self", ".", "total_count", "=", "0.0", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.metrics.sequence_accuracy.SequenceAccuracy.__call__": [[21, 73], ["sequence_accuracy.SequenceAccuracy.unwrap_to_tensors", "list", "list.insert", "gold_labels.unsqueeze().expand", "masked_gold.eq", "some_match.sum().item", "gold_labels.dim", "allennlp.common.checks.ConfigurationError", "allennlp.common.checks.ConfigurationError", "predictions.size", "gold_labels.size", "mask.unsqueeze().expand", "masked_gold.eq.min", "matches_per_question.max", "predictions.size", "predictions.dim", "mask.size", "gold_labels.size", "gold_labels.unsqueeze", "some_match.sum", "gold_labels.size", "mask.size", "mask.unsqueeze"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.metrics.metric.Metric.unwrap_to_tensors"], ["", "def", "__call__", "(", "\n", "self", ",", "\n", "predictions", ":", "torch", ".", "Tensor", ",", "\n", "gold_labels", ":", "torch", ".", "Tensor", ",", "\n", "mask", ":", "Optional", "[", "torch", ".", "Tensor", "]", "=", "None", ",", "\n", ")", ":", "\n", "        ", "\"\"\"\n        # Parameters\n\n        predictions : `torch.Tensor`, required.\n            A tensor of predictions of shape (batch_size, k, sequence_length).\n        gold_labels : `torch.Tensor`, required.\n            A tensor of integer class label of shape (batch_size, sequence_length).\n        mask : `torch.Tensor`, optional (default = None).\n            A masking tensor the same size as `gold_labels`.\n        \"\"\"", "\n", "predictions", ",", "gold_labels", ",", "mask", "=", "self", ".", "unwrap_to_tensors", "(", "\n", "predictions", ",", "gold_labels", ",", "mask", "\n", ")", "\n", "\n", "# Some sanity checks.", "\n", "if", "gold_labels", ".", "dim", "(", ")", "!=", "predictions", ".", "dim", "(", ")", "-", "1", ":", "\n", "            ", "raise", "ConfigurationError", "(", "\n", "\"gold_labels must have dimension == predictions.dim() - 1 but \"", "\n", "\"found tensor of shape: {}\"", ".", "format", "(", "gold_labels", ".", "size", "(", ")", ")", "\n", ")", "\n", "", "if", "mask", "is", "not", "None", "and", "mask", ".", "size", "(", ")", "!=", "gold_labels", ".", "size", "(", ")", ":", "\n", "            ", "raise", "ConfigurationError", "(", "\n", "\"mask must have the same size as predictions but \"", "\n", "\"found tensor of shape: {}\"", ".", "format", "(", "mask", ".", "size", "(", ")", ")", "\n", ")", "\n", "\n", "", "k", "=", "predictions", ".", "size", "(", ")", "[", "1", "]", "\n", "expanded_size", "=", "list", "(", "gold_labels", ".", "size", "(", ")", ")", "\n", "expanded_size", ".", "insert", "(", "1", ",", "k", ")", "\n", "expanded_gold", "=", "gold_labels", ".", "unsqueeze", "(", "1", ")", ".", "expand", "(", "expanded_size", ")", "\n", "\n", "if", "mask", "is", "not", "None", ":", "\n", "            ", "expanded_mask", "=", "mask", ".", "unsqueeze", "(", "1", ")", ".", "expand", "(", "expanded_size", ")", "\n", "masked_gold", "=", "expanded_mask", "*", "expanded_gold", "\n", "masked_predictions", "=", "expanded_mask", "*", "predictions", "\n", "", "else", ":", "\n", "            ", "masked_gold", "=", "expanded_gold", "\n", "masked_predictions", "=", "predictions", "\n", "\n", "", "eqs", "=", "masked_gold", ".", "eq", "(", "masked_predictions", ")", "\n", "matches_per_question", "=", "eqs", ".", "min", "(", "dim", "=", "2", ")", "[", "0", "]", "\n", "some_match", "=", "matches_per_question", ".", "max", "(", "dim", "=", "1", ")", "[", "0", "]", "\n", "correct", "=", "some_match", ".", "sum", "(", ")", ".", "item", "(", ")", "\n", "\n", "self", ".", "total_count", "+=", "predictions", ".", "size", "(", ")", "[", "0", "]", "\n", "self", ".", "correct_count", "+=", "correct", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.metrics.sequence_accuracy.SequenceAccuracy.get_metric": [[74, 88], ["sequence_accuracy.SequenceAccuracy.reset", "float", "float"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.metrics.sequence_accuracy.SequenceAccuracy.reset"], ["", "def", "get_metric", "(", "self", ",", "reset", ":", "bool", "=", "False", ")", ":", "\n", "        ", "\"\"\"\n        # Returns\n\n        The accumulated accuracy.\n        \"\"\"", "\n", "if", "self", ".", "total_count", ">", "0", ":", "\n", "            ", "accuracy", "=", "float", "(", "self", ".", "correct_count", ")", "/", "float", "(", "self", ".", "total_count", ")", "\n", "", "else", ":", "\n", "            ", "accuracy", "=", "0", "\n", "\n", "", "if", "reset", ":", "\n", "            ", "self", ".", "reset", "(", ")", "\n", "", "return", "accuracy", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.metrics.sequence_accuracy.SequenceAccuracy.reset": [[89, 93], ["None"], "methods", ["None"], ["", "@", "overrides", "\n", "def", "reset", "(", "self", ")", ":", "\n", "        ", "self", ".", "correct_count", "=", "0.0", "\n", "self", ".", "total_count", "=", "0.0", "\n", "", "", ""]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.loss.hinge_loss.HingeLoss.__init__": [[6, 9], ["torch.Module.__init__"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.graph_util.plot.GraphPlot.__init__"], ["    ", "def", "__init__", "(", "self", ",", "margin", "=", "1.0", ")", ":", "\n", "        ", "super", "(", "HingeLoss", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "margin", "=", "margin", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.loss.hinge_loss.HingeLoss.forward": [[10, 14], ["torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.mul", "torch.mul", "torch.mul", "torch.mul"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "output", ",", "target", ")", ":", "\n", "        ", "hinge_loss", "=", "self", ".", "margin", "-", "torch", ".", "mul", "(", "output", ",", "target", ")", "\n", "hinge_loss", "[", "hinge_loss", "<", "0", "]", "=", "0", "\n", "return", "torch", ".", "sum", "(", "hinge_loss", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.modules.pruner.Pruner.__init__": [[22, 25], ["super().__init__"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.graph_util.plot.GraphPlot.__init__"], ["def", "__init__", "(", "self", ",", "scorer", ":", "torch", ".", "nn", ".", "Module", ")", "->", "None", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "_scorer", "=", "scorer", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.modules.pruner.Pruner.forward": [[26, 144], ["isinstance", "num_items_to_keep.max", "mask.unsqueeze.unsqueeze.unsqueeze", "embeddings.size", "pruner.Pruner._scorer", "allennlp.nn.util.replace_masked_values", "allennlp.nn.util.replace_masked_values.topk", "allennlp.nn.util.get_mask_from_sequence_lengths", "top_indices_mask.byte.byte.byte", "torch.where.squeeze", "torch.where.max", "fill_value.unsqueeze.unsqueeze.unsqueeze", "torch.where", "torch.sort", "allennlp.nn.util.flatten_and_batch_shift_indices", "allennlp.nn.util.batched_index_select", "allennlp.nn.util.batched_index_select", "sequence_mask.squeeze().byte.squeeze().byte.squeeze().byte", "top_mask.long.long.long", "allennlp.nn.util.batched_index_select", "mask.unsqueeze.unsqueeze.size", "ValueError", "torch.ones", "allennlp.nn.util.replace_masked_values.size", "allennlp.nn.util.replace_masked_values.dim", "sequence_mask.squeeze().byte.squeeze().byte.squeeze", "allennlp.nn.util.replace_masked_values.size"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.nn.util.replace_masked_values", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.nn.util.get_mask_from_sequence_lengths", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.nn.util.flatten_and_batch_shift_indices", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.nn.util.batched_index_select", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.nn.util.batched_index_select", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.nn.util.batched_index_select"], ["", "@", "overrides", "\n", "def", "forward", "(", "\n", "self", ",", "\n", "embeddings", ":", "torch", ".", "FloatTensor", ",", "\n", "mask", ":", "torch", ".", "LongTensor", ",", "\n", "num_items_to_keep", ":", "Union", "[", "int", ",", "torch", ".", "LongTensor", "]", ",", "\n", ")", "->", "Tuple", "[", "\n", "torch", ".", "FloatTensor", ",", "torch", ".", "LongTensor", ",", "torch", ".", "LongTensor", ",", "torch", ".", "FloatTensor", "\n", "]", ":", "\n", "        ", "\"\"\"\n        Extracts the top-k scoring items with respect to the scorer. We additionally return\n        the indices of the top-k in their original order, not ordered by score, so that downstream\n        components can rely on the original ordering (e.g., for knowing what spans are valid\n        antecedents in a coreference resolution model). May use the same k for all sentences in\n        minibatch, or different k for each.\n\n        # Parameters\n\n        embeddings : `torch.FloatTensor`, required.\n            A tensor of shape (batch_size, num_items, embedding_size), containing an embedding for\n            each item in the list that we want to prune.\n        mask : `torch.LongTensor`, required.\n            A tensor of shape (batch_size, num_items), denoting unpadded elements of\n            `embeddings`.\n        num_items_to_keep : `Union[int, torch.LongTensor]`, required.\n            If a tensor of shape (batch_size), specifies the number of items to keep for each\n            individual sentence in minibatch.\n            If an int, keep the same number of items for all sentences.\n\n        # Returns\n\n        top_embeddings : `torch.FloatTensor`\n            The representations of the top-k scoring items.\n            Has shape (batch_size, max_num_items_to_keep, embedding_size).\n        top_mask : `torch.LongTensor`\n            The corresponding mask for `top_embeddings`.\n            Has shape (batch_size, max_num_items_to_keep).\n        top_indices : `torch.IntTensor`\n            The indices of the top-k scoring items into the original `embeddings`\n            tensor. This is returned because it can be useful to retain pointers to\n            the original items, if each item is being scored by multiple distinct\n            scorers, for instance. Has shape (batch_size, max_num_items_to_keep).\n        top_item_scores : `torch.FloatTensor`\n            The values of the top-k scoring items.\n            Has shape (batch_size, max_num_items_to_keep, 1).\n        \"\"\"", "\n", "# If an int was given for number of items to keep, construct tensor by repeating the value.", "\n", "if", "isinstance", "(", "num_items_to_keep", ",", "int", ")", ":", "\n", "            ", "batch_size", "=", "mask", ".", "size", "(", "0", ")", "\n", "# Put the tensor on same device as the mask.", "\n", "num_items_to_keep", "=", "num_items_to_keep", "*", "torch", ".", "ones", "(", "\n", "[", "batch_size", "]", ",", "dtype", "=", "torch", ".", "long", ",", "device", "=", "mask", ".", "device", "\n", ")", "\n", "\n", "", "max_items_to_keep", "=", "num_items_to_keep", ".", "max", "(", ")", "\n", "\n", "mask", "=", "mask", ".", "unsqueeze", "(", "-", "1", ")", "\n", "num_items", "=", "embeddings", ".", "size", "(", "1", ")", "\n", "# Shape: (batch_size, num_items, 1)", "\n", "scores", "=", "self", ".", "_scorer", "(", "embeddings", ")", "\n", "\n", "if", "scores", ".", "size", "(", "-", "1", ")", "!=", "1", "or", "scores", ".", "dim", "(", ")", "!=", "3", ":", "\n", "            ", "raise", "ValueError", "(", "\n", "f\"The scorer passed to Pruner must produce a tensor of shape\"", "\n", "f\"(batch_size, num_items, 1), but found shape {scores.size()}\"", "\n", ")", "\n", "# Make sure that we don't select any masked items by setting their scores to be very", "\n", "# negative.  These are logits, typically, so -1e20 should be plenty negative.", "\n", "", "scores", "=", "util", ".", "replace_masked_values", "(", "scores", ",", "mask", ",", "-", "1e20", ")", "\n", "\n", "# Shape: (batch_size, max_num_items_to_keep, 1)", "\n", "_", ",", "top_indices", "=", "scores", ".", "topk", "(", "max_items_to_keep", ",", "1", ")", "\n", "\n", "# Mask based on number of items to keep for each sentence.", "\n", "# Shape: (batch_size, max_num_items_to_keep)", "\n", "top_indices_mask", "=", "util", ".", "get_mask_from_sequence_lengths", "(", "\n", "num_items_to_keep", ",", "max_items_to_keep", "\n", ")", "\n", "top_indices_mask", "=", "top_indices_mask", ".", "byte", "(", ")", "\n", "\n", "# Shape: (batch_size, max_num_items_to_keep)", "\n", "top_indices", "=", "top_indices", ".", "squeeze", "(", "-", "1", ")", "\n", "\n", "# Fill all masked indices with largest \"top\" index for that sentence, so that all masked", "\n", "# indices will be sorted to the end.", "\n", "# Shape: (batch_size, 1)", "\n", "fill_value", ",", "_", "=", "top_indices", ".", "max", "(", "dim", "=", "1", ")", "\n", "fill_value", "=", "fill_value", ".", "unsqueeze", "(", "-", "1", ")", "\n", "# Shape: (batch_size, max_num_items_to_keep)", "\n", "top_indices", "=", "torch", ".", "where", "(", "top_indices_mask", ",", "top_indices", ",", "fill_value", ")", "\n", "\n", "# Now we order the selected indices in increasing order with", "\n", "# respect to their indices (and hence, with respect to the", "\n", "# order they originally appeared in the `embeddings` tensor).", "\n", "top_indices", ",", "_", "=", "torch", ".", "sort", "(", "top_indices", ",", "1", ")", "\n", "\n", "# Shape: (batch_size * max_num_items_to_keep)", "\n", "# torch.index_select only accepts 1D indices, but here", "\n", "# we need to select items for each element in the batch.", "\n", "flat_top_indices", "=", "util", ".", "flatten_and_batch_shift_indices", "(", "top_indices", ",", "num_items", ")", "\n", "\n", "# Shape: (batch_size, max_num_items_to_keep, embedding_size)", "\n", "top_embeddings", "=", "util", ".", "batched_index_select", "(", "\n", "embeddings", ",", "top_indices", ",", "flat_top_indices", "\n", ")", "\n", "\n", "# Combine the masks on spans that are out-of-bounds, and the mask on spans that are outside", "\n", "# the top k for each sentence.", "\n", "# Shape: (batch_size, max_num_items_to_keep)", "\n", "sequence_mask", "=", "util", ".", "batched_index_select", "(", "mask", ",", "top_indices", ",", "flat_top_indices", ")", "\n", "sequence_mask", "=", "sequence_mask", ".", "squeeze", "(", "-", "1", ")", ".", "byte", "(", ")", "\n", "top_mask", "=", "top_indices_mask", "&", "sequence_mask", "\n", "top_mask", "=", "top_mask", ".", "long", "(", ")", "\n", "\n", "# Shape: (batch_size, max_num_items_to_keep, 1)", "\n", "top_scores", "=", "util", ".", "batched_index_select", "(", "scores", ",", "top_indices", ",", "flat_top_indices", ")", "\n", "\n", "return", "top_embeddings", ",", "top_mask", ",", "top_indices", ",", "top_scores", "\n", "", "", ""]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.modules.masked_layer_norm.MaskedLayerNorm.__init__": [[11, 17], ["super().__init__", "torch.nn.Parameter", "torch.nn.Parameter", "torch.zeros", "torch.ones"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.graph_util.plot.GraphPlot.__init__"], ["def", "__init__", "(", "self", ",", "size", ":", "int", ",", "gamma0", ":", "float", "=", "0.1", ",", "eps", ":", "float", "=", "1e-6", ")", "->", "None", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "gamma", "=", "torch", ".", "nn", ".", "Parameter", "(", "torch", ".", "ones", "(", "1", ",", "1", ",", "size", ")", "*", "gamma0", ")", "\n", "self", ".", "beta", "=", "torch", ".", "nn", ".", "Parameter", "(", "torch", ".", "zeros", "(", "1", ",", "1", ",", "size", ")", ")", "\n", "self", ".", "size", "=", "size", "\n", "self", ".", "eps", "=", "eps", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.modules.masked_layer_norm.MaskedLayerNorm.forward": [[18, 28], ["mask.unsqueeze().float", "torch.sqrt", "mask.unsqueeze().float.sum", "mask.unsqueeze"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "tensor", ":", "torch", ".", "Tensor", ",", "mask", ":", "torch", ".", "Tensor", ")", "->", "torch", ".", "Tensor", ":", "\n", "\n", "        ", "broadcast_mask", "=", "mask", ".", "unsqueeze", "(", "-", "1", ")", ".", "float", "(", ")", "\n", "num_elements", "=", "broadcast_mask", ".", "sum", "(", ")", "*", "self", ".", "size", "\n", "mean", "=", "(", "tensor", "*", "broadcast_mask", ")", ".", "sum", "(", ")", "/", "num_elements", "\n", "masked_centered", "=", "(", "tensor", "-", "mean", ")", "*", "broadcast_mask", "\n", "std", "=", "torch", ".", "sqrt", "(", "\n", "(", "masked_centered", "*", "masked_centered", ")", ".", "sum", "(", ")", "/", "num_elements", "+", "self", ".", "eps", "\n", ")", "\n", "return", "self", ".", "gamma", "*", "(", "tensor", "-", "mean", ")", "/", "(", "std", "+", "self", ".", "eps", ")", "+", "self", ".", "beta", "\n", "", "", ""]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.modules.bimpm_matching.BiMpmMatching.__init__": [[138, 213], ["super().__init__", "allennlp.common.checks.ConfigurationError", "torch.Parameter", "torch.Parameter", "torch.Parameter", "torch.nn.init.kaiming_normal_", "torch.nn.init.kaiming_normal_", "torch.nn.init.kaiming_normal_", "torch.nn.init.kaiming_normal_", "torch.nn.init.kaiming_normal_", "torch.nn.init.kaiming_normal_", "torch.nn.init.kaiming_normal_", "torch.nn.init.kaiming_normal_", "torch.nn.init.kaiming_normal_", "bimpm_matching.BiMpmMatching.__init__.create_parameter"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.graph_util.plot.GraphPlot.__init__"], ["def", "__init__", "(", "\n", "self", ",", "\n", "hidden_dim", ":", "int", "=", "100", ",", "\n", "num_perspectives", ":", "int", "=", "20", ",", "\n", "share_weights_between_directions", ":", "bool", "=", "True", ",", "\n", "is_forward", ":", "bool", "=", "None", ",", "\n", "with_full_match", ":", "bool", "=", "True", ",", "\n", "with_maxpool_match", ":", "bool", "=", "True", ",", "\n", "with_attentive_match", ":", "bool", "=", "True", ",", "\n", "with_max_attentive_match", ":", "bool", "=", "True", ",", "\n", ")", "->", "None", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "\n", "self", ".", "hidden_dim", "=", "hidden_dim", "\n", "self", ".", "num_perspectives", "=", "num_perspectives", "\n", "self", ".", "is_forward", "=", "is_forward", "\n", "\n", "self", ".", "with_full_match", "=", "with_full_match", "\n", "self", ".", "with_maxpool_match", "=", "with_maxpool_match", "\n", "self", ".", "with_attentive_match", "=", "with_attentive_match", "\n", "self", ".", "with_max_attentive_match", "=", "with_max_attentive_match", "\n", "\n", "if", "not", "(", "\n", "with_full_match", "\n", "or", "with_maxpool_match", "\n", "or", "with_attentive_match", "\n", "or", "with_max_attentive_match", "\n", ")", ":", "\n", "            ", "raise", "ConfigurationError", "(", "\n", "\"At least one of the matching method should be enabled\"", "\n", ")", "\n", "\n", "", "def", "create_parameter", "(", ")", ":", "# utility function to create and initialize a parameter", "\n", "            ", "param", "=", "nn", ".", "Parameter", "(", "torch", ".", "zeros", "(", "num_perspectives", ",", "hidden_dim", ")", ")", "\n", "torch", ".", "nn", ".", "init", ".", "kaiming_normal_", "(", "param", ")", "\n", "return", "param", "\n", "\n", "", "def", "share_or_create", "(", "\n", "weights_to_share", ",", "\n", ")", ":", "# utility function to create or share the weights", "\n", "            ", "return", "(", "\n", "weights_to_share", "\n", "if", "share_weights_between_directions", "\n", "else", "create_parameter", "(", ")", "\n", ")", "\n", "\n", "", "output_dim", "=", "2", "# used to calculate total output dimension, 2 is for cosine max and cosine min", "\n", "if", "with_full_match", ":", "\n", "            ", "if", "is_forward", "is", "None", ":", "\n", "                ", "raise", "ConfigurationError", "(", "\n", "\"Must specify is_forward to enable full matching\"", "\n", ")", "\n", "", "self", ".", "full_match_weights", "=", "create_parameter", "(", ")", "\n", "self", ".", "full_match_weights_reversed", "=", "share_or_create", "(", "self", ".", "full_match_weights", ")", "\n", "output_dim", "+=", "num_perspectives", "+", "1", "\n", "\n", "", "if", "with_maxpool_match", ":", "\n", "            ", "self", ".", "maxpool_match_weights", "=", "create_parameter", "(", ")", "\n", "output_dim", "+=", "num_perspectives", "*", "2", "\n", "\n", "", "if", "with_attentive_match", ":", "\n", "            ", "self", ".", "attentive_match_weights", "=", "create_parameter", "(", ")", "\n", "self", ".", "attentive_match_weights_reversed", "=", "share_or_create", "(", "\n", "self", ".", "attentive_match_weights", "\n", ")", "\n", "output_dim", "+=", "num_perspectives", "+", "1", "\n", "\n", "", "if", "with_max_attentive_match", ":", "\n", "            ", "self", ".", "max_attentive_match_weights", "=", "create_parameter", "(", ")", "\n", "self", ".", "max_attentive_match_weights_reversed", "=", "share_or_create", "(", "\n", "self", ".", "max_attentive_match_weights", "\n", ")", "\n", "output_dim", "+=", "num_perspectives", "+", "1", "\n", "\n", "", "self", ".", "output_dim", "=", "output_dim", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.modules.bimpm_matching.BiMpmMatching.get_output_dim": [[214, 216], ["None"], "methods", ["None"], ["", "def", "get_output_dim", "(", "self", ")", "->", "int", ":", "\n", "        ", "return", "self", ".", "output_dim", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.modules.bimpm_matching.BiMpmMatching.forward": [[217, 410], ["allennlp.nn.util.get_lengths_from_binary_sequence_mask", "allennlp.nn.util.get_lengths_from_binary_sequence_mask", "torch.cosine_similarity", "torch.cosine_similarity", "torch.cosine_similarity", "allennlp.nn.util.masked_max", "allennlp.nn.util.masked_mean", "allennlp.nn.util.masked_max", "allennlp.nn.util.masked_mean", "matching_vector_1.extend", "matching_vector_2.extend", "context_1.size", "context_2.size", "mask_1.float", "mask_2.float", "mask_1.unsqueeze", "mask_2.unsqueeze", "context_1.unsqueeze", "context_2.unsqueeze", "mask_2.unsqueeze", "mask_2.unsqueeze", "torch.cosine_similarity.permute", "mask_1.unsqueeze", "torch.cosine_similarity.permute", "mask_1.unsqueeze", "bimpm_matching.multi_perspective_match", "bimpm_matching.multi_perspective_match", "matching_vector_1.extend", "matching_vector_2.extend", "bimpm_matching.multi_perspective_match_pairwise", "allennlp.nn.util.masked_max", "allennlp.nn.util.masked_mean", "allennlp.nn.util.masked_max", "allennlp.nn.util.masked_mean", "matching_vector_1.extend", "matching_vector_2.extend", "context_2.unsqueeze", "torch.cosine_similarity.unsqueeze", "context_1.unsqueeze", "torch.cosine_similarity.unsqueeze", "allennlp.nn.util.masked_softmax", "allennlp.nn.util.masked_softmax", "bimpm_matching.multi_perspective_match", "bimpm_matching.multi_perspective_match", "matching_vector_1.extend", "matching_vector_2.extend", "allennlp.nn.util.masked_max", "allennlp.nn.util.masked_max", "bimpm_matching.multi_perspective_match", "bimpm_matching.multi_perspective_match", "matching_vector_1.extend", "matching_vector_2.extend", "last_position_1.view().expand.view().expand.view().expand", "last_position_2.view().expand.view().expand.view().expand", "context_1.gather", "context_2.gather", "mask_2.unsqueeze().unsqueeze", "mask_2.unsqueeze().unsqueeze", "multi_perspective_match_pairwise.permute", "mask_1.unsqueeze().unsqueeze", "multi_perspective_match_pairwise.permute", "mask_1.unsqueeze().unsqueeze", "att_2.sum", "mask_1.unsqueeze", "att_1.sum", "mask_2.unsqueeze", "mask_2.unsqueeze().unsqueeze", "att_1.permute", "mask_1.unsqueeze().unsqueeze", "last_position_1.view().expand.view().expand.view", "last_position_2.view().expand.view().expand.view", "mask_2.unsqueeze", "mask_2.unsqueeze", "mask_1.unsqueeze", "mask_1.unsqueeze", "mask_2.unsqueeze", "mask_1.unsqueeze"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.nn.util.get_lengths_from_binary_sequence_mask", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.nn.util.get_lengths_from_binary_sequence_mask", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.nn.util.masked_max", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.nn.util.masked_mean", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.nn.util.masked_max", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.nn.util.masked_mean", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.modules.bimpm_matching.multi_perspective_match", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.modules.bimpm_matching.multi_perspective_match", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.modules.bimpm_matching.multi_perspective_match_pairwise", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.nn.util.masked_max", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.nn.util.masked_mean", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.nn.util.masked_max", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.nn.util.masked_mean", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.nn.util.masked_softmax", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.nn.util.masked_softmax", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.modules.bimpm_matching.multi_perspective_match", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.modules.bimpm_matching.multi_perspective_match", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.nn.util.masked_max", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.nn.util.masked_max", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.modules.bimpm_matching.multi_perspective_match", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.modules.bimpm_matching.multi_perspective_match"], ["", "def", "forward", "(", "\n", "self", ",", "\n", "context_1", ":", "torch", ".", "Tensor", ",", "\n", "mask_1", ":", "torch", ".", "Tensor", ",", "\n", "context_2", ":", "torch", ".", "Tensor", ",", "\n", "mask_2", ":", "torch", ".", "Tensor", ",", "\n", ")", "->", "Tuple", "[", "List", "[", "torch", ".", "Tensor", "]", ",", "List", "[", "torch", ".", "Tensor", "]", "]", ":", "\n", "\n", "        ", "\"\"\"\n        Given the forward (or backward) representations of sentence1 and sentence2, apply four bilateral\n        matching functions between them in one direction.\n\n        # Parameters\n\n        context_1 : `torch.Tensor`\n            Tensor of shape (batch_size, seq_len1, hidden_dim) representing the encoding of the first sentence.\n        mask_1 : `torch.Tensor`\n            Binary Tensor of shape (batch_size, seq_len1), indicating which\n            positions in the first sentence are padding (0) and which are not (1).\n        context_2 : `torch.Tensor`\n            Tensor of shape (batch_size, seq_len2, hidden_dim) representing the encoding of the second sentence.\n        mask_2 : `torch.Tensor`\n            Binary Tensor of shape (batch_size, seq_len2), indicating which\n            positions in the second sentence are padding (0) and which are not (1).\n\n        # Returns\n\n        A tuple of matching vectors for the two sentences. Each of which is a list of\n        matching vectors of shape (batch, seq_len, num_perspectives or 1)\n        \"\"\"", "\n", "assert", "(", "not", "mask_2", ".", "requires_grad", ")", "and", "(", "not", "mask_1", ".", "requires_grad", ")", "\n", "assert", "context_1", ".", "size", "(", "-", "1", ")", "==", "context_2", ".", "size", "(", "-", "1", ")", "==", "self", ".", "hidden_dim", "\n", "\n", "# (batch,)", "\n", "len_1", "=", "get_lengths_from_binary_sequence_mask", "(", "mask_1", ")", "\n", "len_2", "=", "get_lengths_from_binary_sequence_mask", "(", "mask_2", ")", "\n", "\n", "# (batch, seq_len*)", "\n", "mask_1", ",", "mask_2", "=", "mask_1", ".", "float", "(", ")", ",", "mask_2", ".", "float", "(", ")", "\n", "\n", "# explicitly set masked weights to zero", "\n", "# (batch_size, seq_len*, hidden_dim)", "\n", "context_1", "=", "context_1", "*", "mask_1", ".", "unsqueeze", "(", "-", "1", ")", "\n", "context_2", "=", "context_2", "*", "mask_2", ".", "unsqueeze", "(", "-", "1", ")", "\n", "\n", "# array to keep the matching vectors for the two sentences", "\n", "matching_vector_1", ":", "List", "[", "torch", ".", "Tensor", "]", "=", "[", "]", "\n", "matching_vector_2", ":", "List", "[", "torch", ".", "Tensor", "]", "=", "[", "]", "\n", "\n", "# Step 0. unweighted cosine", "\n", "# First calculate the cosine similarities between each forward", "\n", "# (or backward) contextual embedding and every forward (or backward)", "\n", "# contextual embedding of the other sentence.", "\n", "\n", "# (batch, seq_len1, seq_len2)", "\n", "cosine_sim", "=", "F", ".", "cosine_similarity", "(", "\n", "context_1", ".", "unsqueeze", "(", "-", "2", ")", ",", "context_2", ".", "unsqueeze", "(", "-", "3", ")", ",", "dim", "=", "3", "\n", ")", "\n", "\n", "# (batch, seq_len*, 1)", "\n", "cosine_max_1", "=", "masked_max", "(", "cosine_sim", ",", "mask_2", ".", "unsqueeze", "(", "-", "2", ")", ",", "dim", "=", "2", ",", "keepdim", "=", "True", ")", "\n", "cosine_mean_1", "=", "masked_mean", "(", "\n", "cosine_sim", ",", "mask_2", ".", "unsqueeze", "(", "-", "2", ")", ",", "dim", "=", "2", ",", "keepdim", "=", "True", "\n", ")", "\n", "cosine_max_2", "=", "masked_max", "(", "\n", "cosine_sim", ".", "permute", "(", "0", ",", "2", ",", "1", ")", ",", "mask_1", ".", "unsqueeze", "(", "-", "2", ")", ",", "dim", "=", "2", ",", "keepdim", "=", "True", "\n", ")", "\n", "cosine_mean_2", "=", "masked_mean", "(", "\n", "cosine_sim", ".", "permute", "(", "0", ",", "2", ",", "1", ")", ",", "mask_1", ".", "unsqueeze", "(", "-", "2", ")", ",", "dim", "=", "2", ",", "keepdim", "=", "True", "\n", ")", "\n", "\n", "matching_vector_1", ".", "extend", "(", "[", "cosine_max_1", ",", "cosine_mean_1", "]", ")", "\n", "matching_vector_2", ".", "extend", "(", "[", "cosine_max_2", ",", "cosine_mean_2", "]", ")", "\n", "\n", "# Step 1. Full-Matching", "\n", "# Each time step of forward (or backward) contextual embedding of one sentence", "\n", "# is compared with the last time step of the forward (or backward)", "\n", "# contextual embedding of the other sentence", "\n", "if", "self", ".", "with_full_match", ":", "\n", "\n", "# (batch, 1, hidden_dim)", "\n", "            ", "if", "self", ".", "is_forward", ":", "\n", "# (batch, 1, hidden_dim)", "\n", "                ", "last_position_1", "=", "(", "len_1", "-", "1", ")", ".", "clamp", "(", "min", "=", "0", ")", "\n", "last_position_1", "=", "last_position_1", ".", "view", "(", "-", "1", ",", "1", ",", "1", ")", ".", "expand", "(", "\n", "-", "1", ",", "1", ",", "self", ".", "hidden_dim", "\n", ")", "\n", "last_position_2", "=", "(", "len_2", "-", "1", ")", ".", "clamp", "(", "min", "=", "0", ")", "\n", "last_position_2", "=", "last_position_2", ".", "view", "(", "-", "1", ",", "1", ",", "1", ")", ".", "expand", "(", "\n", "-", "1", ",", "1", ",", "self", ".", "hidden_dim", "\n", ")", "\n", "\n", "context_1_last", "=", "context_1", ".", "gather", "(", "1", ",", "last_position_1", ")", "\n", "context_2_last", "=", "context_2", ".", "gather", "(", "1", ",", "last_position_2", ")", "\n", "", "else", ":", "\n", "                ", "context_1_last", "=", "context_1", "[", ":", ",", "0", ":", "1", ",", ":", "]", "\n", "context_2_last", "=", "context_2", "[", ":", ",", "0", ":", "1", ",", ":", "]", "\n", "\n", "# (batch, seq_len*, num_perspectives)", "\n", "", "matching_vector_1_full", "=", "multi_perspective_match", "(", "\n", "context_1", ",", "context_2_last", ",", "self", ".", "full_match_weights", "\n", ")", "\n", "matching_vector_2_full", "=", "multi_perspective_match", "(", "\n", "context_2", ",", "context_1_last", ",", "self", ".", "full_match_weights_reversed", "\n", ")", "\n", "\n", "matching_vector_1", ".", "extend", "(", "matching_vector_1_full", ")", "\n", "matching_vector_2", ".", "extend", "(", "matching_vector_2_full", ")", "\n", "\n", "# Step 2. Maxpooling-Matching", "\n", "# Each time step of forward (or backward) contextual embedding of one sentence", "\n", "# is compared with every time step of the forward (or backward)", "\n", "# contextual embedding of the other sentence, and only the max value of each", "\n", "# dimension is retained.", "\n", "", "if", "self", ".", "with_maxpool_match", ":", "\n", "# (batch, seq_len1, seq_len2, num_perspectives)", "\n", "            ", "matching_vector_max", "=", "multi_perspective_match_pairwise", "(", "\n", "context_1", ",", "context_2", ",", "self", ".", "maxpool_match_weights", "\n", ")", "\n", "\n", "# (batch, seq_len*, num_perspectives)", "\n", "matching_vector_1_max", "=", "masked_max", "(", "\n", "matching_vector_max", ",", "mask_2", ".", "unsqueeze", "(", "-", "2", ")", ".", "unsqueeze", "(", "-", "1", ")", ",", "dim", "=", "2", "\n", ")", "\n", "matching_vector_1_mean", "=", "masked_mean", "(", "\n", "matching_vector_max", ",", "mask_2", ".", "unsqueeze", "(", "-", "2", ")", ".", "unsqueeze", "(", "-", "1", ")", ",", "dim", "=", "2", "\n", ")", "\n", "matching_vector_2_max", "=", "masked_max", "(", "\n", "matching_vector_max", ".", "permute", "(", "0", ",", "2", ",", "1", ",", "3", ")", ",", "\n", "mask_1", ".", "unsqueeze", "(", "-", "2", ")", ".", "unsqueeze", "(", "-", "1", ")", ",", "\n", "dim", "=", "2", ",", "\n", ")", "\n", "matching_vector_2_mean", "=", "masked_mean", "(", "\n", "matching_vector_max", ".", "permute", "(", "0", ",", "2", ",", "1", ",", "3", ")", ",", "\n", "mask_1", ".", "unsqueeze", "(", "-", "2", ")", ".", "unsqueeze", "(", "-", "1", ")", ",", "\n", "dim", "=", "2", ",", "\n", ")", "\n", "\n", "matching_vector_1", ".", "extend", "(", "[", "matching_vector_1_max", ",", "matching_vector_1_mean", "]", ")", "\n", "matching_vector_2", ".", "extend", "(", "[", "matching_vector_2_max", ",", "matching_vector_2_mean", "]", ")", "\n", "\n", "# Step 3. Attentive-Matching", "\n", "# Each forward (or backward) similarity is taken as the weight", "\n", "# of the forward (or backward) contextual embedding, and calculate an", "\n", "# attentive vector for the sentence by weighted summing all its", "\n", "# contextual embeddings.", "\n", "# Finally match each forward (or backward) contextual embedding", "\n", "# with its corresponding attentive vector.", "\n", "\n", "# (batch, seq_len1, seq_len2, hidden_dim)", "\n", "", "att_2", "=", "context_2", ".", "unsqueeze", "(", "-", "3", ")", "*", "cosine_sim", ".", "unsqueeze", "(", "-", "1", ")", "\n", "\n", "# (batch, seq_len1, seq_len2, hidden_dim)", "\n", "att_1", "=", "context_1", ".", "unsqueeze", "(", "-", "2", ")", "*", "cosine_sim", ".", "unsqueeze", "(", "-", "1", ")", "\n", "\n", "if", "self", ".", "with_attentive_match", ":", "\n", "# (batch, seq_len*, hidden_dim)", "\n", "            ", "att_mean_2", "=", "masked_softmax", "(", "att_2", ".", "sum", "(", "dim", "=", "2", ")", ",", "mask_1", ".", "unsqueeze", "(", "-", "1", ")", ")", "\n", "att_mean_1", "=", "masked_softmax", "(", "att_1", ".", "sum", "(", "dim", "=", "1", ")", ",", "mask_2", ".", "unsqueeze", "(", "-", "1", ")", ")", "\n", "\n", "# (batch, seq_len*, num_perspectives)", "\n", "matching_vector_1_att_mean", "=", "multi_perspective_match", "(", "\n", "context_1", ",", "att_mean_2", ",", "self", ".", "attentive_match_weights", "\n", ")", "\n", "matching_vector_2_att_mean", "=", "multi_perspective_match", "(", "\n", "context_2", ",", "att_mean_1", ",", "self", ".", "attentive_match_weights_reversed", "\n", ")", "\n", "matching_vector_1", ".", "extend", "(", "matching_vector_1_att_mean", ")", "\n", "matching_vector_2", ".", "extend", "(", "matching_vector_2_att_mean", ")", "\n", "\n", "# Step 4. Max-Attentive-Matching", "\n", "# Pick the contextual embeddings with the highest cosine similarity as the attentive", "\n", "# vector, and match each forward (or backward) contextual embedding with its", "\n", "# corresponding attentive vector.", "\n", "", "if", "self", ".", "with_max_attentive_match", ":", "\n", "# (batch, seq_len*, hidden_dim)", "\n", "            ", "att_max_2", "=", "masked_max", "(", "att_2", ",", "mask_2", ".", "unsqueeze", "(", "-", "2", ")", ".", "unsqueeze", "(", "-", "1", ")", ",", "dim", "=", "2", ")", "\n", "att_max_1", "=", "masked_max", "(", "\n", "att_1", ".", "permute", "(", "0", ",", "2", ",", "1", ",", "3", ")", ",", "mask_1", ".", "unsqueeze", "(", "-", "2", ")", ".", "unsqueeze", "(", "-", "1", ")", ",", "dim", "=", "2", "\n", ")", "\n", "\n", "# (batch, seq_len*, num_perspectives)", "\n", "matching_vector_1_att_max", "=", "multi_perspective_match", "(", "\n", "context_1", ",", "att_max_2", ",", "self", ".", "max_attentive_match_weights", "\n", ")", "\n", "matching_vector_2_att_max", "=", "multi_perspective_match", "(", "\n", "context_2", ",", "att_max_1", ",", "self", ".", "max_attentive_match_weights_reversed", "\n", ")", "\n", "\n", "matching_vector_1", ".", "extend", "(", "matching_vector_1_att_max", ")", "\n", "matching_vector_2", ".", "extend", "(", "matching_vector_2_att_max", ")", "\n", "\n", "", "return", "matching_vector_1", ",", "matching_vector_2", "\n", "", "", ""]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.modules.bimpm_matching.multi_perspective_match": [[21, 59], ["torch.cosine_similarity().unsqueeze", "weight.unsqueeze().unsqueeze.unsqueeze().unsqueeze", "torch.cosine_similarity", "vector1.size", "vector2.size", "weight.unsqueeze().unsqueeze.size", "vector1.size", "vector1.size", "vector1.unsqueeze", "vector2.unsqueeze", "torch.cosine_similarity", "weight.unsqueeze().unsqueeze.unsqueeze"], "function", ["None"], ["def", "multi_perspective_match", "(", "\n", "vector1", ":", "torch", ".", "Tensor", ",", "vector2", ":", "torch", ".", "Tensor", ",", "weight", ":", "torch", ".", "Tensor", "\n", ")", "->", "Tuple", "[", "torch", ".", "Tensor", ",", "torch", ".", "Tensor", "]", ":", "\n", "    ", "\"\"\"\n    Calculate multi-perspective cosine matching between time-steps of vectors\n    of the same length.\n\n    # Parameters\n\n    vector1 : `torch.Tensor`\n        A tensor of shape `(batch, seq_len, hidden_size)`\n    vector2 : `torch.Tensor`\n        A tensor of shape `(batch, seq_len or 1, hidden_size)`\n    weight : `torch.Tensor`\n        A tensor of shape `(num_perspectives, hidden_size)`\n\n    # Returns\n\n    A tuple of two tensors consisting multi-perspective matching results.\n    The first one is of the shape (batch, seq_len, 1), the second one is of shape\n    (batch, seq_len, num_perspectives)\n    \"\"\"", "\n", "assert", "vector1", ".", "size", "(", "0", ")", "==", "vector2", ".", "size", "(", "0", ")", "\n", "assert", "weight", ".", "size", "(", "1", ")", "==", "vector1", ".", "size", "(", "2", ")", "==", "vector1", ".", "size", "(", "2", ")", "\n", "\n", "# (batch, seq_len, 1)", "\n", "similarity_single", "=", "F", ".", "cosine_similarity", "(", "vector1", ",", "vector2", ",", "2", ")", ".", "unsqueeze", "(", "2", ")", "\n", "\n", "# (1, 1, num_perspectives, hidden_size)", "\n", "weight", "=", "weight", ".", "unsqueeze", "(", "0", ")", ".", "unsqueeze", "(", "0", ")", "\n", "\n", "# (batch, seq_len, num_perspectives, hidden_size)", "\n", "vector1", "=", "weight", "*", "vector1", ".", "unsqueeze", "(", "2", ")", "\n", "vector2", "=", "weight", "*", "vector2", ".", "unsqueeze", "(", "2", ")", "\n", "\n", "similarity_multi", "=", "F", ".", "cosine_similarity", "(", "vector1", ",", "vector2", ",", "dim", "=", "3", ")", "\n", "\n", "return", "similarity_single", ",", "similarity_multi", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.modules.bimpm_matching.multi_perspective_match_pairwise": [[61, 106], ["weight.unsqueeze().unsqueeze.size", "weight.unsqueeze().unsqueeze.unsqueeze().unsqueeze", "vector1.norm", "vector2.norm", "torch.matmul", "torch.matmul", "torch.matmul", "vector1.unsqueeze().expand", "vector2.unsqueeze().expand", "vector2.transpose", "vector2.norm.transpose", "weight.unsqueeze().unsqueeze.unsqueeze", "vector1.unsqueeze", "vector2.unsqueeze", "norm_value.clamp"], "function", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.metrics.f1_score.F1.clamp"], ["", "def", "multi_perspective_match_pairwise", "(", "\n", "vector1", ":", "torch", ".", "Tensor", ",", "\n", "vector2", ":", "torch", ".", "Tensor", ",", "\n", "weight", ":", "torch", ".", "Tensor", ",", "\n", "eps", ":", "float", "=", "1e-8", ",", "\n", ")", "->", "torch", ".", "Tensor", ":", "\n", "    ", "\"\"\"\n    Calculate multi-perspective cosine matching between each time step of\n    one vector and each time step of another vector.\n\n    # Parameters\n\n    vector1 : `torch.Tensor`\n        A tensor of shape `(batch, seq_len1, hidden_size)`\n    vector2 : `torch.Tensor`\n        A tensor of shape `(batch, seq_len2, hidden_size)`\n    weight : `torch.Tensor`\n        A tensor of shape `(num_perspectives, hidden_size)`\n    eps : `float` optional, (default = 1e-8)\n        A small value to avoid zero division problem\n\n    # Returns\n\n    A tensor of shape (batch, seq_len1, seq_len2, num_perspectives) consisting\n    multi-perspective matching results\n    \"\"\"", "\n", "num_perspectives", "=", "weight", ".", "size", "(", "0", ")", "\n", "\n", "# (1, num_perspectives, 1, hidden_size)", "\n", "weight", "=", "weight", ".", "unsqueeze", "(", "0", ")", ".", "unsqueeze", "(", "2", ")", "\n", "\n", "# (batch, num_perspectives, seq_len*, hidden_size)", "\n", "vector1", "=", "weight", "*", "vector1", ".", "unsqueeze", "(", "1", ")", ".", "expand", "(", "-", "1", ",", "num_perspectives", ",", "-", "1", ",", "-", "1", ")", "\n", "vector2", "=", "weight", "*", "vector2", ".", "unsqueeze", "(", "1", ")", ".", "expand", "(", "-", "1", ",", "num_perspectives", ",", "-", "1", ",", "-", "1", ")", "\n", "\n", "# (batch, num_perspectives, seq_len*, 1)", "\n", "vector1_norm", "=", "vector1", ".", "norm", "(", "p", "=", "2", ",", "dim", "=", "3", ",", "keepdim", "=", "True", ")", "\n", "vector2_norm", "=", "vector2", ".", "norm", "(", "p", "=", "2", ",", "dim", "=", "3", ",", "keepdim", "=", "True", ")", "\n", "\n", "# (batch, num_perspectives, seq_len1, seq_len2)", "\n", "mul_result", "=", "torch", ".", "matmul", "(", "vector1", ",", "vector2", ".", "transpose", "(", "2", ",", "3", ")", ")", "\n", "norm_value", "=", "vector1_norm", "*", "vector2_norm", ".", "transpose", "(", "2", ",", "3", ")", "\n", "\n", "# (batch, seq_len1, seq_len2, num_perspectives)", "\n", "return", "(", "mul_result", "/", "norm_value", ".", "clamp", "(", "min", "=", "eps", ")", ")", ".", "permute", "(", "0", ",", "2", ",", "3", ",", "1", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.modules.augmented_lstm.AugmentedLstm.__init__": [[61, 97], ["super().__init__", "augmented_lstm.AugmentedLstm.reset_parameters", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.graph_util.plot.GraphPlot.__init__", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.similarity_functions.linear.LinearSimilarity.reset_parameters"], ["def", "__init__", "(", "\n", "self", ",", "\n", "input_size", ":", "int", ",", "\n", "hidden_size", ":", "int", ",", "\n", "go_forward", ":", "bool", "=", "True", ",", "\n", "recurrent_dropout_probability", ":", "float", "=", "0.0", ",", "\n", "use_highway", ":", "bool", "=", "True", ",", "\n", "use_input_projection_bias", ":", "bool", "=", "True", ",", "\n", ")", "->", "None", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "# Required to be wrapped with a `PytorchSeq2SeqWrapper`.", "\n", "self", ".", "input_size", "=", "input_size", "\n", "self", ".", "hidden_size", "=", "hidden_size", "\n", "\n", "self", ".", "go_forward", "=", "go_forward", "\n", "self", ".", "use_highway", "=", "use_highway", "\n", "self", ".", "recurrent_dropout_probability", "=", "recurrent_dropout_probability", "\n", "\n", "# We do the projections for all the gates all at once, so if we are", "\n", "# using highway layers, we need some extra projections, which is", "\n", "# why the sizes of the Linear layers change here depending on this flag.", "\n", "if", "use_highway", ":", "\n", "            ", "self", ".", "input_linearity", "=", "torch", ".", "nn", ".", "Linear", "(", "\n", "input_size", ",", "6", "*", "hidden_size", ",", "bias", "=", "use_input_projection_bias", "\n", ")", "\n", "self", ".", "state_linearity", "=", "torch", ".", "nn", ".", "Linear", "(", "\n", "hidden_size", ",", "5", "*", "hidden_size", ",", "bias", "=", "True", "\n", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "input_linearity", "=", "torch", ".", "nn", ".", "Linear", "(", "\n", "input_size", ",", "4", "*", "hidden_size", ",", "bias", "=", "use_input_projection_bias", "\n", ")", "\n", "self", ".", "state_linearity", "=", "torch", ".", "nn", ".", "Linear", "(", "\n", "hidden_size", ",", "4", "*", "hidden_size", ",", "bias", "=", "True", "\n", ")", "\n", "", "self", ".", "reset_parameters", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.modules.augmented_lstm.AugmentedLstm.reset_parameters": [[98, 112], ["allennlp.nn.initializers.block_orthogonal", "allennlp.nn.initializers.block_orthogonal", "augmented_lstm.AugmentedLstm.state_linearity.bias.data.fill_", "augmented_lstm.AugmentedLstm.state_linearity.bias.data[].fill_"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.nn.initializers.block_orthogonal", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.nn.initializers.block_orthogonal"], ["", "def", "reset_parameters", "(", "self", ")", ":", "\n", "# Use sensible default initializations for parameters.", "\n", "        ", "block_orthogonal", "(", "\n", "self", ".", "input_linearity", ".", "weight", ".", "data", ",", "[", "self", ".", "hidden_size", ",", "self", ".", "input_size", "]", "\n", ")", "\n", "block_orthogonal", "(", "\n", "self", ".", "state_linearity", ".", "weight", ".", "data", ",", "[", "self", ".", "hidden_size", ",", "self", ".", "hidden_size", "]", "\n", ")", "\n", "\n", "self", ".", "state_linearity", ".", "bias", ".", "data", ".", "fill_", "(", "0.0", ")", "\n", "# Initialize forget gate biases to 1.0 as per An Empirical", "\n", "# Exploration of Recurrent Network Architectures, (Jozefowicz, 2015).", "\n", "self", ".", "state_linearity", ".", "bias", ".", "data", "[", "self", ".", "hidden_size", ":", "2", "*", "self", ".", "hidden_size", "]", ".", "fill_", "(", "\n", "1.0", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.modules.augmented_lstm.AugmentedLstm.forward": [[114, 270], ["torch.nn.utils.rnn.pad_packed_sequence", "sequence_tensor.new_zeros", "range", "torch.nn.utils.rnn.pack_padded_sequence", "isinstance", "allennlp.common.checks.ConfigurationError", "sequence_tensor.size", "sequence_tensor.size", "sequence_tensor.new_zeros", "sequence_tensor.new_zeros", "initial_state[].squeeze", "initial_state[].squeeze", "allennlp.nn.util.get_dropout_mask", "full_batch_previous_memory[].clone", "full_batch_previous_state[].clone", "augmented_lstm.AugmentedLstm.input_linearity", "augmented_lstm.AugmentedLstm.state_linearity", "torch.sigmoid", "torch.sigmoid", "torch.tanh", "torch.sigmoid", "full_batch_previous_memory.clone.clone.clone", "full_batch_previous_state.clone.clone.clone", "full_batch_previous_state.clone.clone.unsqueeze", "full_batch_previous_memory.clone.clone.unsqueeze", "torch.tanh", "torch.sigmoid", "type", "len"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.nn.util.get_dropout_mask", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.nn.util.clone", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.nn.util.clone", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.nn.util.clone", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.nn.util.clone"], ["", "def", "forward", "(", "\n", "self", ",", "\n", "inputs", ":", "PackedSequence", ",", "\n", "initial_state", ":", "Optional", "[", "Tuple", "[", "torch", ".", "Tensor", ",", "torch", ".", "Tensor", "]", "]", "=", "None", ",", "\n", ")", ":", "\n", "        ", "\"\"\"\n        # Parameters\n\n        inputs : `PackedSequence`, required.\n            A tensor of shape (batch_size, num_timesteps, input_size)\n            to apply the LSTM over.\n\n        initial_state : `Tuple[torch.Tensor, torch.Tensor]`, optional, (default = None)\n            A tuple (state, memory) representing the initial hidden state and memory\n            of the LSTM. Each tensor has shape (1, batch_size, output_dimension).\n\n        # Returns\n\n        A PackedSequence containing a torch.FloatTensor of shape\n        (batch_size, num_timesteps, output_dimension) representing\n        the outputs of the LSTM per timestep and a tuple containing\n        the LSTM state, with shape (1, batch_size, hidden_size) to\n        match the Pytorch API.\n        \"\"\"", "\n", "if", "not", "isinstance", "(", "inputs", ",", "PackedSequence", ")", ":", "\n", "            ", "raise", "ConfigurationError", "(", "\n", "\"inputs must be PackedSequence but got %s\"", "%", "(", "type", "(", "inputs", ")", ")", "\n", ")", "\n", "\n", "", "sequence_tensor", ",", "batch_lengths", "=", "pad_packed_sequence", "(", "inputs", ",", "batch_first", "=", "True", ")", "\n", "batch_size", "=", "sequence_tensor", ".", "size", "(", ")", "[", "0", "]", "\n", "total_timesteps", "=", "sequence_tensor", ".", "size", "(", ")", "[", "1", "]", "\n", "\n", "output_accumulator", "=", "sequence_tensor", ".", "new_zeros", "(", "\n", "batch_size", ",", "total_timesteps", ",", "self", ".", "hidden_size", "\n", ")", "\n", "if", "initial_state", "is", "None", ":", "\n", "            ", "full_batch_previous_memory", "=", "sequence_tensor", ".", "new_zeros", "(", "\n", "batch_size", ",", "self", ".", "hidden_size", "\n", ")", "\n", "full_batch_previous_state", "=", "sequence_tensor", ".", "new_zeros", "(", "\n", "batch_size", ",", "self", ".", "hidden_size", "\n", ")", "\n", "", "else", ":", "\n", "            ", "full_batch_previous_state", "=", "initial_state", "[", "0", "]", ".", "squeeze", "(", "0", ")", "\n", "full_batch_previous_memory", "=", "initial_state", "[", "1", "]", ".", "squeeze", "(", "0", ")", "\n", "\n", "", "current_length_index", "=", "batch_size", "-", "1", "if", "self", ".", "go_forward", "else", "0", "\n", "if", "self", ".", "recurrent_dropout_probability", ">", "0.0", ":", "\n", "            ", "dropout_mask", "=", "get_dropout_mask", "(", "\n", "self", ".", "recurrent_dropout_probability", ",", "full_batch_previous_memory", "\n", ")", "\n", "", "else", ":", "\n", "            ", "dropout_mask", "=", "None", "\n", "\n", "", "for", "timestep", "in", "range", "(", "total_timesteps", ")", ":", "\n", "# The index depends on which end we start.", "\n", "            ", "index", "=", "timestep", "if", "self", ".", "go_forward", "else", "total_timesteps", "-", "timestep", "-", "1", "\n", "\n", "# What we are doing here is finding the index into the batch dimension", "\n", "# which we need to use for this timestep, because the sequences have", "\n", "# variable length, so once the index is greater than the length of this", "\n", "# particular batch sequence, we no longer need to do the computation for", "\n", "# this sequence. The key thing to recognise here is that the batch inputs", "\n", "# must be _ordered_ by length from longest (first in batch) to shortest", "\n", "# (last) so initially, we are going forwards with every sequence and as we", "\n", "# pass the index at which the shortest elements of the batch finish,", "\n", "# we stop picking them up for the computation.", "\n", "if", "self", ".", "go_forward", ":", "\n", "                ", "while", "batch_lengths", "[", "current_length_index", "]", "<=", "index", ":", "\n", "                    ", "current_length_index", "-=", "1", "\n", "# If we're going backwards, we are _picking up_ more indices.", "\n", "", "", "else", ":", "\n", "# First conditional: Are we already at the maximum number of elements in the batch?", "\n", "# Second conditional: Does the next shortest sequence beyond the current batch", "\n", "# index require computation use this timestep?", "\n", "                ", "while", "(", "\n", "current_length_index", "<", "(", "len", "(", "batch_lengths", ")", "-", "1", ")", "\n", "and", "batch_lengths", "[", "current_length_index", "+", "1", "]", ">", "index", "\n", ")", ":", "\n", "                    ", "current_length_index", "+=", "1", "\n", "\n", "# Actually get the slices of the batch which we need for the computation at this timestep.", "\n", "", "", "previous_memory", "=", "full_batch_previous_memory", "[", "\n", "0", ":", "current_length_index", "+", "1", "\n", "]", ".", "clone", "(", ")", "\n", "previous_state", "=", "full_batch_previous_state", "[", "\n", "0", ":", "current_length_index", "+", "1", "\n", "]", ".", "clone", "(", ")", "\n", "# Only do recurrent dropout if the dropout prob is > 0.0 and we are in training mode.", "\n", "if", "dropout_mask", "is", "not", "None", "and", "self", ".", "training", ":", "\n", "                ", "previous_state", "=", "(", "\n", "previous_state", "*", "dropout_mask", "[", "0", ":", "current_length_index", "+", "1", "]", "\n", ")", "\n", "", "timestep_input", "=", "sequence_tensor", "[", "0", ":", "current_length_index", "+", "1", ",", "index", "]", "\n", "\n", "# Do the projections for all the gates all at once.", "\n", "projected_input", "=", "self", ".", "input_linearity", "(", "timestep_input", ")", "\n", "projected_state", "=", "self", ".", "state_linearity", "(", "previous_state", ")", "\n", "\n", "# Main LSTM equations using relevant chunks of the big linear", "\n", "# projections of the hidden state and inputs.", "\n", "input_gate", "=", "torch", ".", "sigmoid", "(", "\n", "projected_input", "[", ":", ",", "0", "*", "self", ".", "hidden_size", ":", "1", "*", "self", ".", "hidden_size", "]", "\n", "+", "projected_state", "[", ":", ",", "0", "*", "self", ".", "hidden_size", ":", "1", "*", "self", ".", "hidden_size", "]", "\n", ")", "\n", "forget_gate", "=", "torch", ".", "sigmoid", "(", "\n", "projected_input", "[", ":", ",", "1", "*", "self", ".", "hidden_size", ":", "2", "*", "self", ".", "hidden_size", "]", "\n", "+", "projected_state", "[", ":", ",", "1", "*", "self", ".", "hidden_size", ":", "2", "*", "self", ".", "hidden_size", "]", "\n", ")", "\n", "memory_init", "=", "torch", ".", "tanh", "(", "\n", "projected_input", "[", ":", ",", "2", "*", "self", ".", "hidden_size", ":", "3", "*", "self", ".", "hidden_size", "]", "\n", "+", "projected_state", "[", ":", ",", "2", "*", "self", ".", "hidden_size", ":", "3", "*", "self", ".", "hidden_size", "]", "\n", ")", "\n", "output_gate", "=", "torch", ".", "sigmoid", "(", "\n", "projected_input", "[", ":", ",", "3", "*", "self", ".", "hidden_size", ":", "4", "*", "self", ".", "hidden_size", "]", "\n", "+", "projected_state", "[", ":", ",", "3", "*", "self", ".", "hidden_size", ":", "4", "*", "self", ".", "hidden_size", "]", "\n", ")", "\n", "memory", "=", "input_gate", "*", "memory_init", "+", "forget_gate", "*", "previous_memory", "\n", "timestep_output", "=", "output_gate", "*", "torch", ".", "tanh", "(", "memory", ")", "\n", "\n", "if", "self", ".", "use_highway", ":", "\n", "                ", "highway_gate", "=", "torch", ".", "sigmoid", "(", "\n", "projected_input", "[", ":", ",", "4", "*", "self", ".", "hidden_size", ":", "5", "*", "self", ".", "hidden_size", "]", "\n", "+", "projected_state", "[", ":", ",", "4", "*", "self", ".", "hidden_size", ":", "5", "*", "self", ".", "hidden_size", "]", "\n", ")", "\n", "highway_input_projection", "=", "projected_input", "[", "\n", ":", ",", "5", "*", "self", ".", "hidden_size", ":", "6", "*", "self", ".", "hidden_size", "\n", "]", "\n", "timestep_output", "=", "(", "\n", "highway_gate", "*", "timestep_output", "\n", "+", "(", "1", "-", "highway_gate", ")", "*", "highway_input_projection", "\n", ")", "\n", "\n", "# We've been doing computation with less than the full batch, so here we create a new", "\n", "# variable for the the whole batch at this timestep and insert the result for the", "\n", "# relevant elements of the batch into it.", "\n", "", "full_batch_previous_memory", "=", "full_batch_previous_memory", ".", "clone", "(", ")", "\n", "full_batch_previous_state", "=", "full_batch_previous_state", ".", "clone", "(", ")", "\n", "full_batch_previous_memory", "[", "0", ":", "current_length_index", "+", "1", "]", "=", "memory", "\n", "full_batch_previous_state", "[", "0", ":", "current_length_index", "+", "1", "]", "=", "timestep_output", "\n", "output_accumulator", "[", "0", ":", "current_length_index", "+", "1", ",", "index", "]", "=", "timestep_output", "\n", "\n", "", "output_accumulator", "=", "pack_padded_sequence", "(", "\n", "output_accumulator", ",", "batch_lengths", ",", "batch_first", "=", "True", "\n", ")", "\n", "\n", "# Mimic the pytorch API by returning state in the following shape:", "\n", "# (num_layers * num_directions, batch_size, hidden_size). As this", "\n", "# LSTM cannot be stacked, the first dimension here is just 1.", "\n", "final_state", "=", "(", "\n", "full_batch_previous_state", ".", "unsqueeze", "(", "0", ")", ",", "\n", "full_batch_previous_memory", ".", "unsqueeze", "(", "0", ")", ",", "\n", ")", "\n", "\n", "return", "output_accumulator", ",", "final_state", "\n", "", "", ""]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.modules.stacked_bidirectional_lstm.StackedBidirectionalLstm.__init__": [[45, 89], ["super().__init__", "range", "allennlp.modules.input_variational_dropout.InputVariationalDropout", "allennlp.modules.augmented_lstm.AugmentedLstm", "allennlp.modules.augmented_lstm.AugmentedLstm", "stacked_bidirectional_lstm.StackedBidirectionalLstm.add_module", "stacked_bidirectional_lstm.StackedBidirectionalLstm.add_module", "layers.append"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.graph_util.plot.GraphPlot.__init__"], ["def", "__init__", "(", "\n", "self", ",", "\n", "input_size", ":", "int", ",", "\n", "hidden_size", ":", "int", ",", "\n", "num_layers", ":", "int", ",", "\n", "recurrent_dropout_probability", ":", "float", "=", "0.0", ",", "\n", "layer_dropout_probability", ":", "float", "=", "0.0", ",", "\n", "use_highway", ":", "bool", "=", "True", ",", "\n", ")", "->", "None", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "\n", "# Required to be wrapped with a `PytorchSeq2SeqWrapper`.", "\n", "self", ".", "input_size", "=", "input_size", "\n", "self", ".", "hidden_size", "=", "hidden_size", "\n", "self", ".", "num_layers", "=", "num_layers", "\n", "self", ".", "bidirectional", "=", "True", "\n", "\n", "layers", "=", "[", "]", "\n", "lstm_input_size", "=", "input_size", "\n", "for", "layer_index", "in", "range", "(", "num_layers", ")", ":", "\n", "\n", "            ", "forward_layer", "=", "AugmentedLstm", "(", "\n", "lstm_input_size", ",", "\n", "hidden_size", ",", "\n", "go_forward", "=", "True", ",", "\n", "recurrent_dropout_probability", "=", "recurrent_dropout_probability", ",", "\n", "use_highway", "=", "use_highway", ",", "\n", "use_input_projection_bias", "=", "False", ",", "\n", ")", "\n", "backward_layer", "=", "AugmentedLstm", "(", "\n", "lstm_input_size", ",", "\n", "hidden_size", ",", "\n", "go_forward", "=", "False", ",", "\n", "recurrent_dropout_probability", "=", "recurrent_dropout_probability", ",", "\n", "use_highway", "=", "use_highway", ",", "\n", "use_input_projection_bias", "=", "False", ",", "\n", ")", "\n", "\n", "lstm_input_size", "=", "hidden_size", "*", "2", "\n", "self", ".", "add_module", "(", "\"forward_layer_{}\"", ".", "format", "(", "layer_index", ")", ",", "forward_layer", ")", "\n", "self", ".", "add_module", "(", "\"backward_layer_{}\"", ".", "format", "(", "layer_index", ")", ",", "backward_layer", ")", "\n", "layers", ".", "append", "(", "[", "forward_layer", ",", "backward_layer", "]", ")", "\n", "", "self", ".", "lstm_layers", "=", "layers", "\n", "self", ".", "layer_dropout", "=", "InputVariationalDropout", "(", "layer_dropout_probability", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.modules.stacked_bidirectional_lstm.StackedBidirectionalLstm.forward": [[90, 155], ["enumerate", "torch.cat", "torch.cat", "getattr", "getattr", "getattr.", "getattr.", "torch.nn.utils.rnn.pad_packed_sequence", "torch.nn.utils.rnn.pad_packed_sequence", "torch.cat", "torch.nn.utils.rnn.pack_padded_sequence", "torch.cat.extend", "torch.cat.extend", "len", "len", "allennlp.common.checks.ConfigurationError", "list", "stacked_bidirectional_lstm.StackedBidirectionalLstm.layer_dropout", "initial_state[].size", "zip", "initial_state[].split", "initial_state[].split"], "methods", ["None"], ["", "def", "forward", "(", "\n", "self", ",", "inputs", ":", "PackedSequence", ",", "initial_state", ":", "Optional", "[", "TensorPair", "]", "=", "None", "\n", ")", "->", "Tuple", "[", "PackedSequence", ",", "TensorPair", "]", ":", "\n", "        ", "\"\"\"\n        # Parameters\n\n        inputs : `PackedSequence`, required.\n            A batch first `PackedSequence` to run the stacked LSTM over.\n        initial_state : Tuple[torch.Tensor, torch.Tensor], optional, (default = None)\n            A tuple (state, memory) representing the initial hidden state and memory\n            of the LSTM. Each tensor has shape (num_layers, batch_size, output_dimension * 2).\n\n        # Returns\n\n        output_sequence : PackedSequence\n            The encoded sequence of shape (batch_size, sequence_length, hidden_size * 2)\n        final_states: torch.Tensor\n            The per-layer final (state, memory) states of the LSTM, each with shape\n            (num_layers * 2, batch_size, hidden_size * 2).\n        \"\"\"", "\n", "if", "initial_state", "is", "None", ":", "\n", "            ", "hidden_states", ":", "List", "[", "Optional", "[", "TensorPair", "]", "]", "=", "[", "None", "]", "*", "len", "(", "self", ".", "lstm_layers", ")", "\n", "", "elif", "initial_state", "[", "0", "]", ".", "size", "(", ")", "[", "0", "]", "!=", "len", "(", "self", ".", "lstm_layers", ")", ":", "\n", "            ", "raise", "ConfigurationError", "(", "\n", "\"Initial states were passed to forward() but the number of \"", "\n", "\"initial states does not match the number of layers.\"", "\n", ")", "\n", "", "else", ":", "\n", "            ", "hidden_states", "=", "list", "(", "\n", "zip", "(", "initial_state", "[", "0", "]", ".", "split", "(", "1", ",", "0", ")", ",", "initial_state", "[", "1", "]", ".", "split", "(", "1", ",", "0", ")", ")", "\n", ")", "\n", "\n", "", "output_sequence", "=", "inputs", "\n", "final_h", "=", "[", "]", "\n", "final_c", "=", "[", "]", "\n", "for", "i", ",", "state", "in", "enumerate", "(", "hidden_states", ")", ":", "\n", "            ", "forward_layer", "=", "getattr", "(", "self", ",", "\"forward_layer_{}\"", ".", "format", "(", "i", ")", ")", "\n", "backward_layer", "=", "getattr", "(", "self", ",", "\"backward_layer_{}\"", ".", "format", "(", "i", ")", ")", "\n", "# The state is duplicated to mirror the Pytorch API for LSTMs.", "\n", "forward_output", ",", "final_forward_state", "=", "forward_layer", "(", "output_sequence", ",", "state", ")", "\n", "backward_output", ",", "final_backward_state", "=", "backward_layer", "(", "\n", "output_sequence", ",", "state", "\n", ")", "\n", "\n", "forward_output", ",", "lengths", "=", "pad_packed_sequence", "(", "\n", "forward_output", ",", "batch_first", "=", "True", "\n", ")", "\n", "backward_output", ",", "_", "=", "pad_packed_sequence", "(", "backward_output", ",", "batch_first", "=", "True", ")", "\n", "\n", "output_sequence", "=", "torch", ".", "cat", "(", "[", "forward_output", ",", "backward_output", "]", ",", "-", "1", ")", "\n", "# Apply layer wise dropout on each output sequence apart from the", "\n", "# first (input) and last", "\n", "if", "i", "<", "(", "self", ".", "num_layers", "-", "1", ")", ":", "\n", "                ", "output_sequence", "=", "self", ".", "layer_dropout", "(", "output_sequence", ")", "\n", "", "output_sequence", "=", "pack_padded_sequence", "(", "\n", "output_sequence", ",", "lengths", ",", "batch_first", "=", "True", "\n", ")", "\n", "\n", "final_h", ".", "extend", "(", "[", "final_forward_state", "[", "0", "]", ",", "final_backward_state", "[", "0", "]", "]", ")", "\n", "final_c", ".", "extend", "(", "[", "final_forward_state", "[", "1", "]", ",", "final_backward_state", "[", "1", "]", "]", ")", "\n", "\n", "", "final_h", "=", "torch", ".", "cat", "(", "final_h", ",", "dim", "=", "0", ")", "\n", "final_c", "=", "torch", ".", "cat", "(", "final_c", ",", "dim", "=", "0", ")", "\n", "final_state_tuple", "=", "(", "final_h", ",", "final_c", ")", "\n", "return", "output_sequence", ",", "final_state_tuple", "\n", "", "", ""]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.modules.elmo.Elmo.__init__": [[96, 138], ["super().__init__", "logger.info", "torch.nn.modules.Dropout", "range", "elmo._ElmoBiLm", "allennlp.modules.scalar_mix.ScalarMix", "elmo.Elmo.add_module", "elmo.Elmo._scalar_mixes.append", "allennlp.common.checks.ConfigurationError"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.graph_util.plot.GraphPlot.__init__", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.dataset_readers.multiprocess_dataset_reader.logger.info"], ["def", "__init__", "(", "\n", "self", ",", "\n", "options_file", ":", "str", ",", "\n", "weight_file", ":", "str", ",", "\n", "num_output_representations", ":", "int", ",", "\n", "requires_grad", ":", "bool", "=", "False", ",", "\n", "do_layer_norm", ":", "bool", "=", "False", ",", "\n", "dropout", ":", "float", "=", "0.5", ",", "\n", "vocab_to_cache", ":", "List", "[", "str", "]", "=", "None", ",", "\n", "keep_sentence_boundaries", ":", "bool", "=", "False", ",", "\n", "scalar_mix_parameters", ":", "List", "[", "float", "]", "=", "None", ",", "\n", "module", ":", "torch", ".", "nn", ".", "Module", "=", "None", ",", "\n", ")", "->", "None", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "\n", "logger", ".", "info", "(", "\"Initializing ELMo\"", ")", "\n", "if", "module", "is", "not", "None", ":", "\n", "            ", "if", "options_file", "is", "not", "None", "or", "weight_file", "is", "not", "None", ":", "\n", "                ", "raise", "ConfigurationError", "(", "\n", "\"Don't provide options_file or weight_file with module\"", "\n", ")", "\n", "", "self", ".", "_elmo_lstm", "=", "module", "\n", "", "else", ":", "\n", "            ", "self", ".", "_elmo_lstm", "=", "_ElmoBiLm", "(", "\n", "options_file", ",", "\n", "weight_file", ",", "\n", "requires_grad", "=", "requires_grad", ",", "\n", "vocab_to_cache", "=", "vocab_to_cache", ",", "\n", ")", "\n", "", "self", ".", "_has_cached_vocab", "=", "vocab_to_cache", "is", "not", "None", "\n", "self", ".", "_keep_sentence_boundaries", "=", "keep_sentence_boundaries", "\n", "self", ".", "_dropout", "=", "Dropout", "(", "p", "=", "dropout", ")", "\n", "self", ".", "_scalar_mixes", ":", "Any", "=", "[", "]", "\n", "for", "k", "in", "range", "(", "num_output_representations", ")", ":", "\n", "            ", "scalar_mix", "=", "ScalarMix", "(", "\n", "self", ".", "_elmo_lstm", ".", "num_layers", ",", "\n", "do_layer_norm", "=", "do_layer_norm", ",", "\n", "initial_scalar_parameters", "=", "scalar_mix_parameters", ",", "\n", "trainable", "=", "scalar_mix_parameters", "is", "None", ",", "\n", ")", "\n", "self", ".", "add_module", "(", "\"scalar_mix_{}\"", ".", "format", "(", "k", ")", ",", "scalar_mix", ")", "\n", "self", ".", "_scalar_mixes", ".", "append", "(", "scalar_mix", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.modules.elmo.Elmo.get_output_dim": [[139, 141], ["elmo.Elmo._elmo_lstm.get_output_dim"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.seq2seq_decoders.auto_regressive_seq_decoder.AutoRegressiveSeqDecoder.get_output_dim"], ["", "", "def", "get_output_dim", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "_elmo_lstm", ".", "get_output_dim", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.modules.elmo.Elmo.forward": [[142, 229], ["inputs.size", "elmo.Elmo._elmo_lstm", "range", "len", "inputs.view", "word_inputs.size", "len", "getattr", "getattr.", "representations.append", "processed_mask.view", "word_inputs.view", "allennlp.nn.util.remove_sentence_boundaries", "elmo.Elmo._dropout", "len", "representation.view", "len", "processed_mask.view", "len", "logger.warning", "representation.view"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.nn.util.remove_sentence_boundaries"], ["", "def", "forward", "(", "\n", "self", ",", "inputs", ":", "torch", ".", "Tensor", ",", "word_inputs", ":", "torch", ".", "Tensor", "=", "None", "\n", ")", "->", "Dict", "[", "str", ",", "Union", "[", "torch", ".", "Tensor", ",", "List", "[", "torch", ".", "Tensor", "]", "]", "]", ":", "\n", "        ", "\"\"\"\n        # Parameters\n\n        inputs : `torch.Tensor`, required.\n        Shape `(batch_size, timesteps, 50)` of character ids representing the current batch.\n        word_inputs : `torch.Tensor`, required.\n            If you passed a cached vocab, you can in addition pass a tensor of shape\n            `(batch_size, timesteps)`, which represent word ids which have been pre-cached.\n\n        # Returns\n\n        Dict with keys:\n        `'elmo_representations'` : `List[torch.Tensor]`\n            A `num_output_representations` list of ELMo representations for the input sequence.\n            Each representation is shape `(batch_size, timesteps, embedding_dim)`\n        `'mask'`:  `torch.Tensor`\n            Shape `(batch_size, timesteps)` long tensor with sequence mask.\n        \"\"\"", "\n", "# reshape the input if needed", "\n", "original_shape", "=", "inputs", ".", "size", "(", ")", "\n", "if", "len", "(", "original_shape", ")", ">", "3", ":", "\n", "            ", "timesteps", ",", "num_characters", "=", "original_shape", "[", "-", "2", ":", "]", "\n", "reshaped_inputs", "=", "inputs", ".", "view", "(", "-", "1", ",", "timesteps", ",", "num_characters", ")", "\n", "", "else", ":", "\n", "            ", "reshaped_inputs", "=", "inputs", "\n", "\n", "", "if", "word_inputs", "is", "not", "None", ":", "\n", "            ", "original_word_size", "=", "word_inputs", ".", "size", "(", ")", "\n", "if", "self", ".", "_has_cached_vocab", "and", "len", "(", "original_word_size", ")", ">", "2", ":", "\n", "                ", "reshaped_word_inputs", "=", "word_inputs", ".", "view", "(", "-", "1", ",", "original_word_size", "[", "-", "1", "]", ")", "\n", "", "elif", "not", "self", ".", "_has_cached_vocab", ":", "\n", "                ", "logger", ".", "warning", "(", "\n", "\"Word inputs were passed to ELMo but it does not have a cached vocab.\"", "\n", ")", "\n", "reshaped_word_inputs", "=", "None", "\n", "", "else", ":", "\n", "                ", "reshaped_word_inputs", "=", "word_inputs", "\n", "", "", "else", ":", "\n", "            ", "reshaped_word_inputs", "=", "word_inputs", "\n", "\n", "# run the biLM", "\n", "", "bilm_output", "=", "self", ".", "_elmo_lstm", "(", "reshaped_inputs", ",", "reshaped_word_inputs", ")", "\n", "layer_activations", "=", "bilm_output", "[", "\"activations\"", "]", "\n", "mask_with_bos_eos", "=", "bilm_output", "[", "\"mask\"", "]", "\n", "\n", "# compute the elmo representations", "\n", "representations", "=", "[", "]", "\n", "for", "i", "in", "range", "(", "len", "(", "self", ".", "_scalar_mixes", ")", ")", ":", "\n", "            ", "scalar_mix", "=", "getattr", "(", "self", ",", "\"scalar_mix_{}\"", ".", "format", "(", "i", ")", ")", "\n", "representation_with_bos_eos", "=", "scalar_mix", "(", "\n", "layer_activations", ",", "mask_with_bos_eos", "\n", ")", "\n", "if", "self", ".", "_keep_sentence_boundaries", ":", "\n", "                ", "processed_representation", "=", "representation_with_bos_eos", "\n", "processed_mask", "=", "mask_with_bos_eos", "\n", "", "else", ":", "\n", "                ", "(", "\n", "representation_without_bos_eos", ",", "\n", "mask_without_bos_eos", ",", "\n", ")", "=", "remove_sentence_boundaries", "(", "\n", "representation_with_bos_eos", ",", "mask_with_bos_eos", "\n", ")", "\n", "processed_representation", "=", "representation_without_bos_eos", "\n", "processed_mask", "=", "mask_without_bos_eos", "\n", "", "representations", ".", "append", "(", "self", ".", "_dropout", "(", "processed_representation", ")", ")", "\n", "\n", "# reshape if necessary", "\n", "", "if", "word_inputs", "is", "not", "None", "and", "len", "(", "original_word_size", ")", ">", "2", ":", "\n", "            ", "mask", "=", "processed_mask", ".", "view", "(", "original_word_size", ")", "\n", "elmo_representations", "=", "[", "\n", "representation", ".", "view", "(", "original_word_size", "+", "(", "-", "1", ",", ")", ")", "\n", "for", "representation", "in", "representations", "\n", "]", "\n", "", "elif", "len", "(", "original_shape", ")", ">", "3", ":", "\n", "            ", "mask", "=", "processed_mask", ".", "view", "(", "original_shape", "[", ":", "-", "1", "]", ")", "\n", "elmo_representations", "=", "[", "\n", "representation", ".", "view", "(", "original_shape", "[", ":", "-", "1", "]", "+", "(", "-", "1", ",", ")", ")", "\n", "for", "representation", "in", "representations", "\n", "]", "\n", "", "else", ":", "\n", "            ", "mask", "=", "processed_mask", "\n", "elmo_representations", "=", "representations", "\n", "\n", "", "return", "{", "\"elmo_representations\"", ":", "elmo_representations", ",", "\"mask\"", ":", "mask", "}", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.modules.elmo._ElmoCharacterEncoder.__init__": [[298, 318], ["super().__init__", "elmo._ElmoCharacterEncoder._load_weights", "torch.from_numpy", "torch.from_numpy", "open", "json.load", "allennlp.common.file_utils.cached_path", "numpy.array", "numpy.array"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.graph_util.plot.GraphPlot.__init__", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.modules.elmo._ElmoCharacterEncoder._load_weights", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.token_embedders.bert_token_embedder.PretrainedBertModel.load", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.file_utils.cached_path"], ["def", "__init__", "(", "\n", "self", ",", "options_file", ":", "str", ",", "weight_file", ":", "str", ",", "requires_grad", ":", "bool", "=", "False", "\n", ")", "->", "None", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "\n", "with", "open", "(", "cached_path", "(", "options_file", ")", ",", "\"r\"", ")", "as", "fin", ":", "\n", "            ", "self", ".", "_options", "=", "json", ".", "load", "(", "fin", ")", "\n", "", "self", ".", "_weight_file", "=", "weight_file", "\n", "\n", "self", ".", "output_dim", "=", "self", ".", "_options", "[", "\"lstm\"", "]", "[", "\"projection_dim\"", "]", "\n", "self", ".", "requires_grad", "=", "requires_grad", "\n", "\n", "self", ".", "_load_weights", "(", ")", "\n", "\n", "# Cache the arrays for use in forward -- +1 due to masking.", "\n", "self", ".", "_beginning_of_sentence_characters", "=", "torch", ".", "from_numpy", "(", "\n", "numpy", ".", "array", "(", "ELMoCharacterMapper", ".", "beginning_of_sentence_characters", ")", "+", "1", "\n", ")", "\n", "self", ".", "_end_of_sentence_characters", "=", "torch", ".", "from_numpy", "(", "\n", "numpy", ".", "array", "(", "ELMoCharacterMapper", ".", "end_of_sentence_characters", ")", "+", "1", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.modules.elmo._ElmoCharacterEncoder.get_output_dim": [[320, 322], ["None"], "methods", ["None"], ["", "def", "get_output_dim", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "output_dim", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.modules.elmo._ElmoCharacterEncoder.forward": [[323, 395], ["allennlp.nn.util.add_sentence_boundary_token_ids", "torch.nn.functional.embedding", "torch.transpose", "range", "torch.cat", "elmo._ElmoCharacterEncoder._highways", "elmo._ElmoCharacterEncoder._projection", "character_ids_with_bos_eos.size", "character_ids_with_bos_eos.view", "len", "getattr", "getattr.", "torch.max", "activation", "convs.append", "elmo._ElmoCharacterEncoder.view", "allennlp.common.checks.ConfigurationError"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.nn.util.add_sentence_boundary_token_ids"], ["", "@", "overrides", "\n", "def", "forward", "(", "self", ",", "inputs", ":", "torch", ".", "Tensor", ")", "->", "Dict", "[", "str", ",", "torch", ".", "Tensor", "]", ":", "\n", "        ", "\"\"\"\n        Compute context insensitive token embeddings for ELMo representations.\n\n        # Parameters\n\n        inputs : `torch.Tensor`\n            Shape `(batch_size, sequence_length, 50)` of character ids representing the\n            current batch.\n\n        # Returns\n\n        Dict with keys:\n        `'token_embedding'` : `torch.Tensor`\n            Shape `(batch_size, sequence_length + 2, embedding_dim)` tensor with context\n            insensitive token representations.\n        `'mask'`:  `torch.Tensor`\n            Shape `(batch_size, sequence_length + 2)` long tensor with sequence mask.\n        \"\"\"", "\n", "# Add BOS/EOS", "\n", "mask", "=", "(", "(", "inputs", ">", "0", ")", ".", "long", "(", ")", ".", "sum", "(", "dim", "=", "-", "1", ")", ">", "0", ")", ".", "long", "(", ")", "\n", "character_ids_with_bos_eos", ",", "mask_with_bos_eos", "=", "add_sentence_boundary_token_ids", "(", "\n", "inputs", ",", "\n", "mask", ",", "\n", "self", ".", "_beginning_of_sentence_characters", ",", "\n", "self", ".", "_end_of_sentence_characters", ",", "\n", ")", "\n", "\n", "# the character id embedding", "\n", "max_chars_per_token", "=", "self", ".", "_options", "[", "\"char_cnn\"", "]", "[", "\"max_characters_per_token\"", "]", "\n", "# (batch_size * sequence_length, max_chars_per_token, embed_dim)", "\n", "character_embedding", "=", "torch", ".", "nn", ".", "functional", ".", "embedding", "(", "\n", "character_ids_with_bos_eos", ".", "view", "(", "-", "1", ",", "max_chars_per_token", ")", ",", "\n", "self", ".", "_char_embedding_weights", ",", "\n", ")", "\n", "\n", "# run convolutions", "\n", "cnn_options", "=", "self", ".", "_options", "[", "\"char_cnn\"", "]", "\n", "if", "cnn_options", "[", "\"activation\"", "]", "==", "\"tanh\"", ":", "\n", "            ", "activation", "=", "torch", ".", "tanh", "\n", "", "elif", "cnn_options", "[", "\"activation\"", "]", "==", "\"relu\"", ":", "\n", "            ", "activation", "=", "torch", ".", "nn", ".", "functional", ".", "relu", "\n", "", "else", ":", "\n", "            ", "raise", "ConfigurationError", "(", "\"Unknown activation\"", ")", "\n", "\n", "# (batch_size * sequence_length, embed_dim, max_chars_per_token)", "\n", "", "character_embedding", "=", "torch", ".", "transpose", "(", "character_embedding", ",", "1", ",", "2", ")", "\n", "convs", "=", "[", "]", "\n", "for", "i", "in", "range", "(", "len", "(", "self", ".", "_convolutions", ")", ")", ":", "\n", "            ", "conv", "=", "getattr", "(", "self", ",", "\"char_conv_{}\"", ".", "format", "(", "i", ")", ")", "\n", "convolved", "=", "conv", "(", "character_embedding", ")", "\n", "# (batch_size * sequence_length, n_filters for this width)", "\n", "convolved", ",", "_", "=", "torch", ".", "max", "(", "convolved", ",", "dim", "=", "-", "1", ")", "\n", "convolved", "=", "activation", "(", "convolved", ")", "\n", "convs", ".", "append", "(", "convolved", ")", "\n", "\n", "# (batch_size * sequence_length, n_filters)", "\n", "", "token_embedding", "=", "torch", ".", "cat", "(", "convs", ",", "dim", "=", "-", "1", ")", "\n", "\n", "# apply the highway layers (batch_size * sequence_length, n_filters)", "\n", "token_embedding", "=", "self", ".", "_highways", "(", "token_embedding", ")", "\n", "\n", "# final projection  (batch_size * sequence_length, embedding_dim)", "\n", "token_embedding", "=", "self", ".", "_projection", "(", "token_embedding", ")", "\n", "\n", "# reshape to (batch_size, sequence_length, embedding_dim)", "\n", "batch_size", ",", "sequence_length", ",", "_", "=", "character_ids_with_bos_eos", ".", "size", "(", ")", "\n", "\n", "return", "{", "\n", "\"mask\"", ":", "mask_with_bos_eos", ",", "\n", "\"token_embedding\"", ":", "token_embedding", ".", "view", "(", "batch_size", ",", "sequence_length", ",", "-", "1", ")", ",", "\n", "}", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.modules.elmo._ElmoCharacterEncoder._load_weights": [[397, 402], ["elmo._ElmoCharacterEncoder._load_char_embedding", "elmo._ElmoCharacterEncoder._load_cnn_weights", "elmo._ElmoCharacterEncoder._load_highway", "elmo._ElmoCharacterEncoder._load_projection"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.modules.elmo._ElmoCharacterEncoder._load_char_embedding", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.modules.elmo._ElmoCharacterEncoder._load_cnn_weights", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.modules.elmo._ElmoCharacterEncoder._load_highway", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.modules.elmo._ElmoCharacterEncoder._load_projection"], ["", "def", "_load_weights", "(", "self", ")", ":", "\n", "        ", "self", ".", "_load_char_embedding", "(", ")", "\n", "self", ".", "_load_cnn_weights", "(", ")", "\n", "self", ".", "_load_highway", "(", ")", "\n", "self", ".", "_load_projection", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.modules.elmo._ElmoCharacterEncoder._load_char_embedding": [[403, 415], ["numpy.zeros", "torch.nn.Parameter", "h5py.File", "torch.FloatTensor", "allennlp.common.file_utils.cached_path"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.file_utils.cached_path"], ["", "def", "_load_char_embedding", "(", "self", ")", ":", "\n", "        ", "with", "h5py", ".", "File", "(", "cached_path", "(", "self", ".", "_weight_file", ")", ",", "\"r\"", ")", "as", "fin", ":", "\n", "            ", "char_embed_weights", "=", "fin", "[", "\"char_embed\"", "]", "[", "...", "]", "\n", "\n", "", "weights", "=", "numpy", ".", "zeros", "(", "\n", "(", "char_embed_weights", ".", "shape", "[", "0", "]", "+", "1", ",", "char_embed_weights", ".", "shape", "[", "1", "]", ")", ",", "\n", "dtype", "=", "\"float32\"", ",", "\n", ")", "\n", "weights", "[", "1", ":", ",", ":", "]", "=", "char_embed_weights", "\n", "\n", "self", ".", "_char_embedding_weights", "=", "torch", ".", "nn", ".", "Parameter", "(", "\n", "torch", ".", "FloatTensor", "(", "weights", ")", ",", "requires_grad", "=", "self", ".", "requires_grad", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.modules.elmo._ElmoCharacterEncoder._load_cnn_weights": [[417, 448], ["enumerate", "torch.nn.Conv1d", "numpy.transpose", "torch.nn.Conv1d.weight.data.copy_", "torch.nn.Conv1d.bias.data.copy_", "convolutions.append", "elmo._ElmoCharacterEncoder.add_module", "h5py.File", "weight.squeeze", "tuple", "ValueError", "torch.FloatTensor", "torch.FloatTensor", "allennlp.common.file_utils.cached_path"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.file_utils.cached_path"], ["", "def", "_load_cnn_weights", "(", "self", ")", ":", "\n", "        ", "cnn_options", "=", "self", ".", "_options", "[", "\"char_cnn\"", "]", "\n", "filters", "=", "cnn_options", "[", "\"filters\"", "]", "\n", "char_embed_dim", "=", "cnn_options", "[", "\"embedding\"", "]", "[", "\"dim\"", "]", "\n", "\n", "convolutions", "=", "[", "]", "\n", "for", "i", ",", "(", "width", ",", "num", ")", "in", "enumerate", "(", "filters", ")", ":", "\n", "            ", "conv", "=", "torch", ".", "nn", ".", "Conv1d", "(", "\n", "in_channels", "=", "char_embed_dim", ",", "\n", "out_channels", "=", "num", ",", "\n", "kernel_size", "=", "width", ",", "\n", "bias", "=", "True", ",", "\n", ")", "\n", "# load the weights", "\n", "with", "h5py", ".", "File", "(", "cached_path", "(", "self", ".", "_weight_file", ")", ",", "\"r\"", ")", "as", "fin", ":", "\n", "                ", "weight", "=", "fin", "[", "\"CNN\"", "]", "[", "\"W_cnn_{}\"", ".", "format", "(", "i", ")", "]", "[", "...", "]", "\n", "bias", "=", "fin", "[", "\"CNN\"", "]", "[", "\"b_cnn_{}\"", ".", "format", "(", "i", ")", "]", "[", "...", "]", "\n", "\n", "", "w_reshaped", "=", "numpy", ".", "transpose", "(", "weight", ".", "squeeze", "(", "axis", "=", "0", ")", ",", "axes", "=", "(", "2", ",", "1", ",", "0", ")", ")", "\n", "if", "w_reshaped", ".", "shape", "!=", "tuple", "(", "conv", ".", "weight", ".", "data", ".", "shape", ")", ":", "\n", "                ", "raise", "ValueError", "(", "\"Invalid weight file\"", ")", "\n", "", "conv", ".", "weight", ".", "data", ".", "copy_", "(", "torch", ".", "FloatTensor", "(", "w_reshaped", ")", ")", "\n", "conv", ".", "bias", ".", "data", ".", "copy_", "(", "torch", ".", "FloatTensor", "(", "bias", ")", ")", "\n", "\n", "conv", ".", "weight", ".", "requires_grad", "=", "self", ".", "requires_grad", "\n", "conv", ".", "bias", ".", "requires_grad", "=", "self", ".", "requires_grad", "\n", "\n", "convolutions", ".", "append", "(", "conv", ")", "\n", "self", ".", "add_module", "(", "\"char_conv_{}\"", ".", "format", "(", "i", ")", ",", "conv", ")", "\n", "\n", "", "self", ".", "_convolutions", "=", "convolutions", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.modules.elmo._ElmoCharacterEncoder._load_highway": [[449, 483], ["sum", "allennlp.modules.highway.Highway", "range", "h5py.File", "numpy.transpose", "numpy.concatenate", "elmo._ElmoCharacterEncoder._highways._layers[].weight.data.copy_", "numpy.concatenate", "elmo._ElmoCharacterEncoder._highways._layers[].bias.data.copy_", "allennlp.common.file_utils.cached_path", "numpy.transpose", "torch.FloatTensor", "torch.FloatTensor"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.file_utils.cached_path"], ["", "def", "_load_highway", "(", "self", ")", ":", "\n", "\n", "# the highway layers have same dimensionality as the number of cnn filters", "\n", "        ", "cnn_options", "=", "self", ".", "_options", "[", "\"char_cnn\"", "]", "\n", "filters", "=", "cnn_options", "[", "\"filters\"", "]", "\n", "n_filters", "=", "sum", "(", "f", "[", "1", "]", "for", "f", "in", "filters", ")", "\n", "n_highway", "=", "cnn_options", "[", "\"n_highway\"", "]", "\n", "\n", "# create the layers, and load the weights", "\n", "self", ".", "_highways", "=", "Highway", "(", "\n", "n_filters", ",", "n_highway", ",", "activation", "=", "torch", ".", "nn", ".", "functional", ".", "relu", "\n", ")", "\n", "for", "k", "in", "range", "(", "n_highway", ")", ":", "\n", "# The AllenNLP highway is one matrix multplication with concatenation of", "\n", "# transform and carry weights.", "\n", "            ", "with", "h5py", ".", "File", "(", "cached_path", "(", "self", ".", "_weight_file", ")", ",", "\"r\"", ")", "as", "fin", ":", "\n", "# The weights are transposed due to multiplication order assumptions in tf", "\n", "# vs pytorch (tf.matmul(X, W) vs pytorch.matmul(W, X))", "\n", "                ", "w_transform", "=", "numpy", ".", "transpose", "(", "\n", "fin", "[", "\"CNN_high_{}\"", ".", "format", "(", "k", ")", "]", "[", "\"W_transform\"", "]", "[", "...", "]", "\n", ")", "\n", "# -1.0 since AllenNLP is g * x + (1 - g) * f(x) but tf is (1 - g) * x + g * f(x)", "\n", "w_carry", "=", "-", "1.0", "*", "numpy", ".", "transpose", "(", "\n", "fin", "[", "\"CNN_high_{}\"", ".", "format", "(", "k", ")", "]", "[", "\"W_carry\"", "]", "[", "...", "]", "\n", ")", "\n", "weight", "=", "numpy", ".", "concatenate", "(", "[", "w_transform", ",", "w_carry", "]", ",", "axis", "=", "0", ")", "\n", "self", ".", "_highways", ".", "_layers", "[", "k", "]", ".", "weight", ".", "data", ".", "copy_", "(", "torch", ".", "FloatTensor", "(", "weight", ")", ")", "\n", "self", ".", "_highways", ".", "_layers", "[", "k", "]", ".", "weight", ".", "requires_grad", "=", "self", ".", "requires_grad", "\n", "\n", "b_transform", "=", "fin", "[", "\"CNN_high_{}\"", ".", "format", "(", "k", ")", "]", "[", "\"b_transform\"", "]", "[", "...", "]", "\n", "b_carry", "=", "-", "1.0", "*", "fin", "[", "\"CNN_high_{}\"", ".", "format", "(", "k", ")", "]", "[", "\"b_carry\"", "]", "[", "...", "]", "\n", "bias", "=", "numpy", ".", "concatenate", "(", "[", "b_transform", ",", "b_carry", "]", ",", "axis", "=", "0", ")", "\n", "self", ".", "_highways", ".", "_layers", "[", "k", "]", ".", "bias", ".", "data", ".", "copy_", "(", "torch", ".", "FloatTensor", "(", "bias", ")", ")", "\n", "self", ".", "_highways", ".", "_layers", "[", "k", "]", ".", "bias", ".", "requires_grad", "=", "self", ".", "requires_grad", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.modules.elmo._ElmoCharacterEncoder._load_projection": [[484, 500], ["sum", "torch.nn.Linear", "h5py.File", "elmo._ElmoCharacterEncoder._projection.weight.data.copy_", "elmo._ElmoCharacterEncoder._projection.bias.data.copy_", "allennlp.common.file_utils.cached_path", "torch.FloatTensor", "torch.FloatTensor", "numpy.transpose"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.file_utils.cached_path"], ["", "", "", "def", "_load_projection", "(", "self", ")", ":", "\n", "        ", "cnn_options", "=", "self", ".", "_options", "[", "\"char_cnn\"", "]", "\n", "filters", "=", "cnn_options", "[", "\"filters\"", "]", "\n", "n_filters", "=", "sum", "(", "f", "[", "1", "]", "for", "f", "in", "filters", ")", "\n", "\n", "self", ".", "_projection", "=", "torch", ".", "nn", ".", "Linear", "(", "n_filters", ",", "self", ".", "output_dim", ",", "bias", "=", "True", ")", "\n", "with", "h5py", ".", "File", "(", "cached_path", "(", "self", ".", "_weight_file", ")", ",", "\"r\"", ")", "as", "fin", ":", "\n", "            ", "weight", "=", "fin", "[", "\"CNN_proj\"", "]", "[", "\"W_proj\"", "]", "[", "...", "]", "\n", "bias", "=", "fin", "[", "\"CNN_proj\"", "]", "[", "\"b_proj\"", "]", "[", "...", "]", "\n", "self", ".", "_projection", ".", "weight", ".", "data", ".", "copy_", "(", "\n", "torch", ".", "FloatTensor", "(", "numpy", ".", "transpose", "(", "weight", ")", ")", "\n", ")", "\n", "self", ".", "_projection", ".", "bias", ".", "data", ".", "copy_", "(", "torch", ".", "FloatTensor", "(", "bias", ")", ")", "\n", "\n", "self", ".", "_projection", ".", "weight", ".", "requires_grad", "=", "self", ".", "requires_grad", "\n", "self", ".", "_projection", ".", "bias", ".", "requires_grad", "=", "self", ".", "requires_grad", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.modules.elmo._ElmoBiLm.__init__": [[525, 575], ["super().__init__", "elmo._ElmoCharacterEncoder", "allennlp.modules.elmo_lstm.ElmoLstm", "elmo._ElmoBiLm._elmo_lstm.load_weights", "logging.warning", "logging.info", "elmo._ElmoBiLm.create_cached_cnn_embeddings", "open", "json.load", "options[].get", "allennlp.common.checks.ConfigurationError", "allennlp.common.file_utils.cached_path"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.graph_util.plot.GraphPlot.__init__", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.modules.elmo_lstm.ElmoLstm.load_weights", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.dataset_readers.multiprocess_dataset_reader.logger.info", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.modules.elmo._ElmoBiLm.create_cached_cnn_embeddings", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.token_embedders.bert_token_embedder.PretrainedBertModel.load", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.params.Params.get", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.file_utils.cached_path"], ["def", "__init__", "(", "\n", "self", ",", "\n", "options_file", ":", "str", ",", "\n", "weight_file", ":", "str", ",", "\n", "requires_grad", ":", "bool", "=", "False", ",", "\n", "vocab_to_cache", ":", "List", "[", "str", "]", "=", "None", ",", "\n", ")", "->", "None", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "\n", "self", ".", "_token_embedder", "=", "_ElmoCharacterEncoder", "(", "\n", "options_file", ",", "weight_file", ",", "requires_grad", "=", "requires_grad", "\n", ")", "\n", "\n", "self", ".", "_requires_grad", "=", "requires_grad", "\n", "if", "requires_grad", "and", "vocab_to_cache", ":", "\n", "            ", "logging", ".", "warning", "(", "\n", "\"You are fine tuning ELMo and caching char CNN word vectors. \"", "\n", "\"This behaviour is not guaranteed to be well defined, particularly. \"", "\n", "\"if not all of your inputs will occur in the vocabulary cache.\"", "\n", ")", "\n", "# This is an embedding, used to look up cached", "\n", "# word vectors built from character level cnn embeddings.", "\n", "", "self", ".", "_word_embedding", "=", "None", "\n", "self", ".", "_bos_embedding", ":", "torch", ".", "Tensor", "=", "None", "\n", "self", ".", "_eos_embedding", ":", "torch", ".", "Tensor", "=", "None", "\n", "if", "vocab_to_cache", ":", "\n", "            ", "logging", ".", "info", "(", "\"Caching character cnn layers for words in vocabulary.\"", ")", "\n", "# This sets 3 attributes, _word_embedding, _bos_embedding and _eos_embedding.", "\n", "# They are set in the method so they can be accessed from outside the", "\n", "# constructor.", "\n", "self", ".", "create_cached_cnn_embeddings", "(", "vocab_to_cache", ")", "\n", "\n", "", "with", "open", "(", "cached_path", "(", "options_file", ")", ",", "\"r\"", ")", "as", "fin", ":", "\n", "            ", "options", "=", "json", ".", "load", "(", "fin", ")", "\n", "", "if", "not", "options", "[", "\"lstm\"", "]", ".", "get", "(", "\"use_skip_connections\"", ")", ":", "\n", "            ", "raise", "ConfigurationError", "(", "\n", "\"We only support pretrained biLMs with residual connections\"", "\n", ")", "\n", "", "self", ".", "_elmo_lstm", "=", "ElmoLstm", "(", "\n", "input_size", "=", "options", "[", "\"lstm\"", "]", "[", "\"projection_dim\"", "]", ",", "\n", "hidden_size", "=", "options", "[", "\"lstm\"", "]", "[", "\"projection_dim\"", "]", ",", "\n", "cell_size", "=", "options", "[", "\"lstm\"", "]", "[", "\"dim\"", "]", ",", "\n", "num_layers", "=", "options", "[", "\"lstm\"", "]", "[", "\"n_layers\"", "]", ",", "\n", "memory_cell_clip_value", "=", "options", "[", "\"lstm\"", "]", "[", "\"cell_clip\"", "]", ",", "\n", "state_projection_clip_value", "=", "options", "[", "\"lstm\"", "]", "[", "\"proj_clip\"", "]", ",", "\n", "requires_grad", "=", "requires_grad", ",", "\n", ")", "\n", "self", ".", "_elmo_lstm", ".", "load_weights", "(", "weight_file", ")", "\n", "# Number of representation layers including context independent layer", "\n", "self", ".", "num_layers", "=", "options", "[", "\"lstm\"", "]", "[", "\"n_layers\"", "]", "+", "1", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.modules.elmo._ElmoBiLm.get_output_dim": [[576, 578], ["elmo._ElmoBiLm._token_embedder.get_output_dim"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.seq2seq_decoders.auto_regressive_seq_decoder.AutoRegressiveSeqDecoder.get_output_dim"], ["", "def", "get_output_dim", "(", "self", ")", ":", "\n", "        ", "return", "2", "*", "self", ".", "_token_embedder", ".", "get_output_dim", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.modules.elmo._ElmoBiLm.forward": [[579, 642], ["elmo._ElmoBiLm._elmo_lstm", "torch.chunk", "elmo._ElmoBiLm._token_embedder", "elmo._ElmoBiLm.size", "output_tensors.append", "elmo._ElmoBiLm._word_embedding", "allennlp.nn.util.add_sentence_boundary_token_ids", "torch.cat", "mask.float().unsqueeze", "layer_activations.squeeze", "elmo._ElmoBiLm._token_embedder", "mask.float"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.nn.util.add_sentence_boundary_token_ids"], ["", "def", "forward", "(", "\n", "self", ",", "inputs", ":", "torch", ".", "Tensor", ",", "word_inputs", ":", "torch", ".", "Tensor", "=", "None", "\n", ")", "->", "Dict", "[", "str", ",", "Union", "[", "torch", ".", "Tensor", ",", "List", "[", "torch", ".", "Tensor", "]", "]", "]", ":", "\n", "        ", "\"\"\"\n        # Parameters\n\n        inputs : `torch.Tensor`, required.\n            Shape `(batch_size, timesteps, 50)` of character ids representing the current batch.\n        word_inputs : `torch.Tensor`, required.\n            If you passed a cached vocab, you can in addition pass a tensor of shape `(batch_size, timesteps)`,\n            which represent word ids which have been pre-cached.\n\n        # Returns\n\n        Dict with keys:\n\n        `'activations'` : `List[torch.Tensor]`\n            A list of activations at each layer of the network, each of shape\n            `(batch_size, timesteps + 2, embedding_dim)`\n        `'mask'`:  `torch.Tensor`\n            Shape `(batch_size, timesteps + 2)` long tensor with sequence mask.\n\n        Note that the output tensors all include additional special begin and end of sequence\n        markers.\n        \"\"\"", "\n", "if", "self", ".", "_word_embedding", "is", "not", "None", "and", "word_inputs", "is", "not", "None", ":", "\n", "            ", "try", ":", "\n", "                ", "mask_without_bos_eos", "=", "(", "word_inputs", ">", "0", ")", ".", "long", "(", ")", "\n", "# The character cnn part is cached - just look it up.", "\n", "embedded_inputs", "=", "self", ".", "_word_embedding", "(", "word_inputs", ")", "# type: ignore", "\n", "# shape (batch_size, timesteps + 2, embedding_dim)", "\n", "type_representation", ",", "mask", "=", "add_sentence_boundary_token_ids", "(", "\n", "embedded_inputs", ",", "\n", "mask_without_bos_eos", ",", "\n", "self", ".", "_bos_embedding", ",", "\n", "self", ".", "_eos_embedding", ",", "\n", ")", "\n", "", "except", "RuntimeError", ":", "\n", "# Back off to running the character convolutions,", "\n", "# as we might not have the words in the cache.", "\n", "                ", "token_embedding", "=", "self", ".", "_token_embedder", "(", "inputs", ")", "\n", "mask", "=", "token_embedding", "[", "\"mask\"", "]", "\n", "type_representation", "=", "token_embedding", "[", "\"token_embedding\"", "]", "\n", "", "", "else", ":", "\n", "            ", "token_embedding", "=", "self", ".", "_token_embedder", "(", "inputs", ")", "\n", "mask", "=", "token_embedding", "[", "\"mask\"", "]", "\n", "type_representation", "=", "token_embedding", "[", "\"token_embedding\"", "]", "\n", "", "lstm_outputs", "=", "self", ".", "_elmo_lstm", "(", "type_representation", ",", "mask", ")", "\n", "\n", "# Prepare the output.  The first layer is duplicated.", "\n", "# Because of minor differences in how masking is applied depending", "\n", "# on whether the char cnn layers are cached, we'll be defensive and", "\n", "# multiply by the mask here. It's not strictly necessary, as the", "\n", "# mask passed on is correct, but the values in the padded areas", "\n", "# of the char cnn representations can change.", "\n", "output_tensors", "=", "[", "\n", "torch", ".", "cat", "(", "[", "type_representation", ",", "type_representation", "]", ",", "dim", "=", "-", "1", ")", "\n", "*", "mask", ".", "float", "(", ")", ".", "unsqueeze", "(", "-", "1", ")", "\n", "]", "\n", "for", "layer_activations", "in", "torch", ".", "chunk", "(", "lstm_outputs", ",", "lstm_outputs", ".", "size", "(", "0", ")", ",", "dim", "=", "0", ")", ":", "\n", "            ", "output_tensors", ".", "append", "(", "layer_activations", ".", "squeeze", "(", "0", ")", ")", "\n", "\n", "", "return", "{", "\"activations\"", ":", "output_tensors", ",", "\"mask\"", ":", "mask", "}", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.modules.elmo._ElmoBiLm.create_cached_cnn_embeddings": [[643, 704], ["allennlp.common.util.lazy_groups_of", "allennlp.nn.util.get_device_of", "allennlp.common.util.lazy_groups_of", "torch.cat", "list", "Embedding", "iter", "next", "elmo.batch_to_ids", "elmo._ElmoBiLm._token_embedder", "allennlp.nn.util.remove_sentence_boundaries", "all_embeddings.append", "embedding.size", "elmo._ElmoBiLm.parameters", "batched_tensor.cuda.cuda.cuda", "token_embedding.view", "token_embedding.size", "len", "len"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.util.lazy_groups_of", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.nn.util.get_device_of", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.util.lazy_groups_of", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.modules.elmo.batch_to_ids", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.nn.util.remove_sentence_boundaries"], ["", "def", "create_cached_cnn_embeddings", "(", "self", ",", "tokens", ":", "List", "[", "str", "]", ")", "->", "None", ":", "\n", "        ", "\"\"\"\n        Given a list of tokens, this method precomputes word representations\n        by running just the character convolutions and highway layers of elmo,\n        essentially creating uncontextual word vectors. On subsequent forward passes,\n        the word ids are looked up from an embedding, rather than being computed on\n        the fly via the CNN encoder.\n\n        This function sets 3 attributes:\n\n        _word_embedding : `torch.Tensor`\n            The word embedding for each word in the tokens passed to this method.\n        _bos_embedding : `torch.Tensor`\n            The embedding for the BOS token.\n        _eos_embedding : `torch.Tensor`\n            The embedding for the EOS token.\n\n        # Parameters\n\n        tokens : `List[str]`, required.\n            A list of tokens to precompute character convolutions for.\n        \"\"\"", "\n", "tokens", "=", "[", "ELMoCharacterMapper", ".", "bos_token", ",", "ELMoCharacterMapper", ".", "eos_token", "]", "+", "tokens", "\n", "timesteps", "=", "32", "\n", "batch_size", "=", "32", "\n", "chunked_tokens", "=", "lazy_groups_of", "(", "iter", "(", "tokens", ")", ",", "timesteps", ")", "\n", "\n", "all_embeddings", "=", "[", "]", "\n", "device", "=", "get_device_of", "(", "next", "(", "self", ".", "parameters", "(", ")", ")", ")", "\n", "for", "batch", "in", "lazy_groups_of", "(", "chunked_tokens", ",", "batch_size", ")", ":", "\n", "# Shape (batch_size, timesteps, 50)", "\n", "            ", "batched_tensor", "=", "batch_to_ids", "(", "batch", ")", "\n", "# NOTE: This device check is for when a user calls this method having", "\n", "# already placed the model on a device. If this is called in the", "\n", "# constructor, it will probably happen on the CPU. This isn't too bad,", "\n", "# because it's only a few convolutions and will likely be very fast.", "\n", "if", "device", ">=", "0", ":", "\n", "                ", "batched_tensor", "=", "batched_tensor", ".", "cuda", "(", "device", ")", "\n", "", "output", "=", "self", ".", "_token_embedder", "(", "batched_tensor", ")", "\n", "token_embedding", "=", "output", "[", "\"token_embedding\"", "]", "\n", "mask", "=", "output", "[", "\"mask\"", "]", "\n", "token_embedding", ",", "_", "=", "remove_sentence_boundaries", "(", "token_embedding", ",", "mask", ")", "\n", "all_embeddings", ".", "append", "(", "token_embedding", ".", "view", "(", "-", "1", ",", "token_embedding", ".", "size", "(", "-", "1", ")", ")", ")", "\n", "", "full_embedding", "=", "torch", ".", "cat", "(", "all_embeddings", ",", "0", ")", "\n", "\n", "# We might have some trailing embeddings from padding in the batch, so", "\n", "# we clip the embedding and lookup to the right size.", "\n", "full_embedding", "=", "full_embedding", "[", ":", "len", "(", "tokens", ")", ",", ":", "]", "\n", "embedding", "=", "full_embedding", "[", "2", ":", "len", "(", "tokens", ")", ",", ":", "]", "\n", "vocab_size", ",", "embedding_dim", "=", "list", "(", "embedding", ".", "size", "(", ")", ")", "\n", "\n", "from", "allennlp", ".", "modules", ".", "token_embedders", "import", "Embedding", "# type: ignore", "\n", "\n", "self", ".", "_bos_embedding", "=", "full_embedding", "[", "0", ",", ":", "]", "\n", "self", ".", "_eos_embedding", "=", "full_embedding", "[", "1", ",", ":", "]", "\n", "self", ".", "_word_embedding", "=", "Embedding", "(", "# type: ignore", "\n", "vocab_size", ",", "\n", "embedding_dim", ",", "\n", "weight", "=", "embedding", ".", "data", ",", "\n", "trainable", "=", "self", ".", "_requires_grad", ",", "\n", "padding_index", "=", "0", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.modules.elmo.batch_to_ids": [[231, 257], ["allennlp.data.token_indexers.elmo_indexer.ELMoTokenCharactersIndexer", "allennlp.data.batch.Batch", "allennlp.data.Vocabulary", "allennlp.data.batch.Batch.index_instances", "allennlp.data.fields.TextField", "allennlp.data.Instance", "instances.append", "allennlp.data.Token", "allennlp.data.batch.Batch.as_tensor_dict"], "function", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.data.batch.Batch.index_instances", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.data.instance.Instance.as_tensor_dict"], ["", "", "def", "batch_to_ids", "(", "batch", ":", "List", "[", "List", "[", "str", "]", "]", ")", "->", "torch", ".", "Tensor", ":", "\n", "    ", "\"\"\"\n    Converts a batch of tokenized sentences to a tensor representing the sentences with encoded characters\n    (len(batch), max sentence length, max word length).\n\n    # Parameters\n\n    batch : `List[List[str]]`, required\n        A list of tokenized sentences.\n\n    # Returns\n\n        A tensor of padded character ids.\n    \"\"\"", "\n", "instances", "=", "[", "]", "\n", "indexer", "=", "ELMoTokenCharactersIndexer", "(", ")", "\n", "for", "sentence", "in", "batch", ":", "\n", "        ", "tokens", "=", "[", "Token", "(", "token", ")", "for", "token", "in", "sentence", "]", "\n", "field", "=", "TextField", "(", "tokens", ",", "{", "\"character_ids\"", ":", "indexer", "}", ")", "\n", "instance", "=", "Instance", "(", "{", "\"elmo\"", ":", "field", "}", ")", "\n", "instances", ".", "append", "(", "instance", ")", "\n", "\n", "", "dataset", "=", "Batch", "(", "instances", ")", "\n", "vocab", "=", "Vocabulary", "(", ")", "\n", "dataset", ".", "index_instances", "(", "vocab", ")", "\n", "return", "dataset", ".", "as_tensor_dict", "(", ")", "[", "\"elmo\"", "]", "[", "\"character_ids\"", "]", "[", "\"tokens\"", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.modules.time_distributed.TimeDistributed.__init__": [[26, 29], ["super().__init__"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.graph_util.plot.GraphPlot.__init__"], ["def", "__init__", "(", "self", ",", "module", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "_module", "=", "module", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.modules.time_distributed.TimeDistributed.forward": [[30, 65], ["kwargs.items", "time_distributed.TimeDistributed._module", "time_distributed.TimeDistributed.contiguous().view", "time_distributed.TimeDistributed._reshape_tensor", "RuntimeError", "isinstance", "time_distributed.TimeDistributed._reshape_tensor", "some_input.size", "time_distributed.TimeDistributed.size", "time_distributed.TimeDistributed.contiguous"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.modules.time_distributed.TimeDistributed._reshape_tensor", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.modules.time_distributed.TimeDistributed._reshape_tensor"], ["", "@", "overrides", "\n", "def", "forward", "(", "self", ",", "*", "inputs", ",", "pass_through", ":", "List", "[", "str", "]", "=", "None", ",", "**", "kwargs", ")", ":", "\n", "\n", "        ", "pass_through", "=", "pass_through", "or", "[", "]", "\n", "\n", "reshaped_inputs", "=", "[", "\n", "self", ".", "_reshape_tensor", "(", "input_tensor", ")", "for", "input_tensor", "in", "inputs", "\n", "]", "\n", "\n", "# Need some input to then get the batch_size and time_steps.", "\n", "some_input", "=", "None", "\n", "if", "inputs", ":", "\n", "            ", "some_input", "=", "inputs", "[", "-", "1", "]", "\n", "\n", "", "reshaped_kwargs", "=", "{", "}", "\n", "for", "key", ",", "value", "in", "kwargs", ".", "items", "(", ")", ":", "\n", "            ", "if", "isinstance", "(", "value", ",", "torch", ".", "Tensor", ")", "and", "key", "not", "in", "pass_through", ":", "\n", "                ", "if", "some_input", "is", "None", ":", "\n", "                    ", "some_input", "=", "value", "\n", "\n", "", "value", "=", "self", ".", "_reshape_tensor", "(", "value", ")", "\n", "\n", "", "reshaped_kwargs", "[", "key", "]", "=", "value", "\n", "\n", "", "reshaped_outputs", "=", "self", ".", "_module", "(", "*", "reshaped_inputs", ",", "**", "reshaped_kwargs", ")", "\n", "\n", "if", "some_input", "is", "None", ":", "\n", "            ", "raise", "RuntimeError", "(", "\"No input tensor to time-distribute\"", ")", "\n", "\n", "# Now get the output back into the right shape.", "\n", "# (batch_size, time_steps, **output_size)", "\n", "", "new_size", "=", "some_input", ".", "size", "(", ")", "[", ":", "2", "]", "+", "reshaped_outputs", ".", "size", "(", ")", "[", "1", ":", "]", "\n", "outputs", "=", "reshaped_outputs", ".", "contiguous", "(", ")", ".", "view", "(", "new_size", ")", "\n", "\n", "return", "outputs", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.modules.time_distributed.TimeDistributed._reshape_tensor": [[66, 75], ["input_tensor.size", "input_tensor.contiguous().view", "len", "RuntimeError", "list", "input_tensor.contiguous"], "methods", ["None"], ["", "@", "staticmethod", "\n", "def", "_reshape_tensor", "(", "input_tensor", ")", ":", "\n", "        ", "input_size", "=", "input_tensor", ".", "size", "(", ")", "\n", "if", "len", "(", "input_size", ")", "<=", "2", ":", "\n", "            ", "raise", "RuntimeError", "(", "f\"No dimension to distribute: {input_size}\"", ")", "\n", "# Squash batch_size and time_steps into a single axis; result has shape", "\n", "# (batch_size * time_steps, **input_size).", "\n", "", "squashed_shape", "=", "[", "-", "1", "]", "+", "list", "(", "input_size", "[", "2", ":", "]", ")", "\n", "return", "input_tensor", ".", "contiguous", "(", ")", ".", "view", "(", "*", "squashed_shape", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.modules.input_variational_dropout.InputVariationalDropout.forward": [[15, 39], ["input_tensor.data.new_ones", "torch.nn.functional.dropout", "torch.nn.functional.dropout.unsqueeze", "torch.nn.functional.dropout.unsqueeze"], "methods", ["None"], ["def", "forward", "(", "self", ",", "input_tensor", ")", ":", "\n", "\n", "        ", "\"\"\"\n        Apply dropout to input tensor.\n\n        # Parameters\n\n        input_tensor : `torch.FloatTensor`\n            A tensor of shape `(batch_size, num_timesteps, embedding_dim)`\n\n        # Returns\n\n        output : `torch.FloatTensor`\n            A tensor of shape `(batch_size, num_timesteps, embedding_dim)` with dropout applied.\n        \"\"\"", "\n", "ones", "=", "input_tensor", ".", "data", ".", "new_ones", "(", "input_tensor", ".", "shape", "[", "0", "]", ",", "input_tensor", ".", "shape", "[", "-", "1", "]", ")", "\n", "dropout_mask", "=", "torch", ".", "nn", ".", "functional", ".", "dropout", "(", "\n", "ones", ",", "self", ".", "p", ",", "self", ".", "training", ",", "inplace", "=", "False", "\n", ")", "\n", "if", "self", ".", "inplace", ":", "\n", "            ", "input_tensor", "*=", "dropout_mask", ".", "unsqueeze", "(", "1", ")", "\n", "return", "None", "\n", "", "else", ":", "\n", "            ", "return", "dropout_mask", ".", "unsqueeze", "(", "1", ")", "*", "input_tensor", "\n", "", "", "", ""]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.modules.layer_norm.LayerNorm.__init__": [[28, 33], ["super().__init__", "torch.nn.Parameter", "torch.nn.Parameter", "torch.ones", "torch.zeros"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.graph_util.plot.GraphPlot.__init__"], ["def", "__init__", "(", "self", ",", "dimension", ":", "int", ",", "eps", ":", "float", "=", "1e-6", ")", "->", "None", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "gamma", "=", "torch", ".", "nn", ".", "Parameter", "(", "torch", ".", "ones", "(", "dimension", ")", ")", "\n", "self", ".", "beta", "=", "torch", ".", "nn", ".", "Parameter", "(", "torch", ".", "zeros", "(", "dimension", ")", ")", "\n", "self", ".", "eps", "=", "eps", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.modules.layer_norm.LayerNorm.forward": [[34, 38], ["tensor.mean", "tensor.std"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "tensor", ":", "torch", ".", "Tensor", ")", ":", "\n", "        ", "mean", "=", "tensor", ".", "mean", "(", "-", "1", ",", "keepdim", "=", "True", ")", "\n", "std", "=", "tensor", ".", "std", "(", "-", "1", ",", "unbiased", "=", "False", ",", "keepdim", "=", "True", ")", "\n", "return", "self", ".", "gamma", "*", "(", "tensor", "-", "mean", ")", "/", "(", "std", "+", "self", ".", "eps", ")", "+", "self", ".", "beta", "\n", "", "", ""]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.modules.feedforward.FeedForward.__init__": [[37, 77], ["super().__init__", "zip", "torch.nn.ModuleList", "torch.nn.ModuleList", "isinstance", "isinstance", "isinstance", "len", "allennlp.common.checks.ConfigurationError", "len", "allennlp.common.checks.ConfigurationError", "len", "allennlp.common.checks.ConfigurationError", "linear_layers.append", "torch.nn.Dropout", "torch.nn.Linear", "len", "len", "len"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.graph_util.plot.GraphPlot.__init__"], ["def", "__init__", "(", "\n", "self", ",", "\n", "input_dim", ":", "int", ",", "\n", "num_layers", ":", "int", ",", "\n", "hidden_dims", ":", "Union", "[", "int", ",", "List", "[", "int", "]", "]", ",", "\n", "activations", ":", "Union", "[", "Activation", ",", "List", "[", "Activation", "]", "]", ",", "\n", "dropout", ":", "Union", "[", "float", ",", "List", "[", "float", "]", "]", "=", "0.0", ",", "\n", ")", "->", "None", ":", "\n", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "if", "not", "isinstance", "(", "hidden_dims", ",", "list", ")", ":", "\n", "            ", "hidden_dims", "=", "[", "hidden_dims", "]", "*", "num_layers", "# type: ignore", "\n", "", "if", "not", "isinstance", "(", "activations", ",", "list", ")", ":", "\n", "            ", "activations", "=", "[", "activations", "]", "*", "num_layers", "# type: ignore", "\n", "", "if", "not", "isinstance", "(", "dropout", ",", "list", ")", ":", "\n", "            ", "dropout", "=", "[", "dropout", "]", "*", "num_layers", "# type: ignore", "\n", "", "if", "len", "(", "hidden_dims", ")", "!=", "num_layers", ":", "\n", "            ", "raise", "ConfigurationError", "(", "\n", "\"len(hidden_dims) (%d) != num_layers (%d)\"", "\n", "%", "(", "len", "(", "hidden_dims", ")", ",", "num_layers", ")", "\n", ")", "\n", "", "if", "len", "(", "activations", ")", "!=", "num_layers", ":", "\n", "            ", "raise", "ConfigurationError", "(", "\n", "\"len(activations) (%d) != num_layers (%d)\"", "\n", "%", "(", "len", "(", "activations", ")", ",", "num_layers", ")", "\n", ")", "\n", "", "if", "len", "(", "dropout", ")", "!=", "num_layers", ":", "\n", "            ", "raise", "ConfigurationError", "(", "\n", "\"len(dropout) (%d) != num_layers (%d)\"", "%", "(", "len", "(", "dropout", ")", ",", "num_layers", ")", "\n", ")", "\n", "", "self", ".", "_activations", "=", "activations", "\n", "input_dims", "=", "[", "input_dim", "]", "+", "hidden_dims", "[", ":", "-", "1", "]", "\n", "linear_layers", "=", "[", "]", "\n", "for", "layer_input_dim", ",", "layer_output_dim", "in", "zip", "(", "input_dims", ",", "hidden_dims", ")", ":", "\n", "            ", "linear_layers", ".", "append", "(", "torch", ".", "nn", ".", "Linear", "(", "layer_input_dim", ",", "layer_output_dim", ")", ")", "\n", "", "self", ".", "_linear_layers", "=", "torch", ".", "nn", ".", "ModuleList", "(", "linear_layers", ")", "\n", "dropout_layers", "=", "[", "torch", ".", "nn", ".", "Dropout", "(", "p", "=", "value", ")", "for", "value", "in", "dropout", "]", "\n", "self", ".", "_dropout", "=", "torch", ".", "nn", ".", "ModuleList", "(", "dropout_layers", ")", "\n", "self", ".", "_output_dim", "=", "hidden_dims", "[", "-", "1", "]", "\n", "self", ".", "input_dim", "=", "input_dim", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.modules.feedforward.FeedForward.get_output_dim": [[78, 80], ["None"], "methods", ["None"], ["", "def", "get_output_dim", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "_output_dim", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.modules.feedforward.FeedForward.get_input_dim": [[81, 83], ["None"], "methods", ["None"], ["", "def", "get_input_dim", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "input_dim", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.modules.feedforward.FeedForward.forward": [[84, 92], ["zip", "dropout", "activation", "layer"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "inputs", ":", "torch", ".", "Tensor", ")", "->", "torch", ".", "Tensor", ":", "\n", "\n", "        ", "output", "=", "inputs", "\n", "for", "layer", ",", "activation", ",", "dropout", "in", "zip", "(", "\n", "self", ".", "_linear_layers", ",", "self", ".", "_activations", ",", "self", ".", "_dropout", "\n", ")", ":", "\n", "            ", "output", "=", "dropout", "(", "activation", "(", "layer", "(", "output", ")", ")", ")", "\n", "", "return", "output", "\n", "", "", ""]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.modules.lstm_cell_with_projection.LstmCellWithProjection.__init__": [[57, 85], ["super().__init__", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "lstm_cell_with_projection.LstmCellWithProjection.reset_parameters"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.graph_util.plot.GraphPlot.__init__", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.similarity_functions.linear.LinearSimilarity.reset_parameters"], ["def", "__init__", "(", "\n", "self", ",", "\n", "input_size", ":", "int", ",", "\n", "hidden_size", ":", "int", ",", "\n", "cell_size", ":", "int", ",", "\n", "go_forward", ":", "bool", "=", "True", ",", "\n", "recurrent_dropout_probability", ":", "float", "=", "0.0", ",", "\n", "memory_cell_clip_value", ":", "Optional", "[", "float", "]", "=", "None", ",", "\n", "state_projection_clip_value", ":", "Optional", "[", "float", "]", "=", "None", ",", "\n", ")", "->", "None", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "# Required to be wrapped with a `PytorchSeq2SeqWrapper`.", "\n", "self", ".", "input_size", "=", "input_size", "\n", "self", ".", "hidden_size", "=", "hidden_size", "\n", "self", ".", "cell_size", "=", "cell_size", "\n", "\n", "self", ".", "go_forward", "=", "go_forward", "\n", "self", ".", "state_projection_clip_value", "=", "state_projection_clip_value", "\n", "self", ".", "memory_cell_clip_value", "=", "memory_cell_clip_value", "\n", "self", ".", "recurrent_dropout_probability", "=", "recurrent_dropout_probability", "\n", "\n", "# We do the projections for all the gates all at once.", "\n", "self", ".", "input_linearity", "=", "torch", ".", "nn", ".", "Linear", "(", "input_size", ",", "4", "*", "cell_size", ",", "bias", "=", "False", ")", "\n", "self", ".", "state_linearity", "=", "torch", ".", "nn", ".", "Linear", "(", "hidden_size", ",", "4", "*", "cell_size", ",", "bias", "=", "True", ")", "\n", "\n", "# Additional projection matrix for making the hidden state smaller.", "\n", "self", ".", "state_projection", "=", "torch", ".", "nn", ".", "Linear", "(", "cell_size", ",", "hidden_size", ",", "bias", "=", "False", ")", "\n", "self", ".", "reset_parameters", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.modules.lstm_cell_with_projection.LstmCellWithProjection.reset_parameters": [[86, 99], ["allennlp.nn.initializers.block_orthogonal", "allennlp.nn.initializers.block_orthogonal", "lstm_cell_with_projection.LstmCellWithProjection.state_linearity.bias.data.fill_", "lstm_cell_with_projection.LstmCellWithProjection.state_linearity.bias.data[].fill_"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.nn.initializers.block_orthogonal", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.nn.initializers.block_orthogonal"], ["", "def", "reset_parameters", "(", "self", ")", ":", "\n", "# Use sensible default initializations for parameters.", "\n", "        ", "block_orthogonal", "(", "\n", "self", ".", "input_linearity", ".", "weight", ".", "data", ",", "[", "self", ".", "cell_size", ",", "self", ".", "input_size", "]", "\n", ")", "\n", "block_orthogonal", "(", "\n", "self", ".", "state_linearity", ".", "weight", ".", "data", ",", "[", "self", ".", "cell_size", ",", "self", ".", "hidden_size", "]", "\n", ")", "\n", "\n", "self", ".", "state_linearity", ".", "bias", ".", "data", ".", "fill_", "(", "0.0", ")", "\n", "# Initialize forget gate biases to 1.0 as per An Empirical", "\n", "# Exploration of Recurrent Network Architectures, (Jozefowicz, 2015).", "\n", "self", ".", "state_linearity", ".", "bias", ".", "data", "[", "self", ".", "cell_size", ":", "2", "*", "self", ".", "cell_size", "]", ".", "fill_", "(", "1.0", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.modules.lstm_cell_with_projection.LstmCellWithProjection.forward": [[100, 265], ["inputs.new_zeros", "range", "inputs.size", "inputs.size", "inputs.new_zeros", "inputs.new_zeros", "initial_state[].squeeze", "initial_state[].squeeze", "allennlp.nn.util.get_dropout_mask", "full_batch_previous_memory[].clone", "full_batch_previous_state[].clone", "lstm_cell_with_projection.LstmCellWithProjection.input_linearity", "lstm_cell_with_projection.LstmCellWithProjection.state_linearity", "torch.sigmoid", "torch.sigmoid", "torch.tanh", "torch.sigmoid", "lstm_cell_with_projection.LstmCellWithProjection.state_projection", "full_batch_previous_memory.clone.clone.clone", "full_batch_previous_state.clone.clone.clone", "full_batch_previous_state.clone.clone.unsqueeze", "full_batch_previous_memory.clone.clone.unsqueeze", "torch.clamp", "torch.tanh", "torch.clamp", "len"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.nn.util.get_dropout_mask", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.nn.util.clone", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.nn.util.clone", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.nn.util.clone", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.nn.util.clone", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.metrics.f1_score.F1.clamp", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.metrics.f1_score.F1.clamp"], ["", "def", "forward", "(", "\n", "self", ",", "\n", "inputs", ":", "torch", ".", "FloatTensor", ",", "\n", "batch_lengths", ":", "List", "[", "int", "]", ",", "\n", "initial_state", ":", "Optional", "[", "Tuple", "[", "torch", ".", "Tensor", ",", "torch", ".", "Tensor", "]", "]", "=", "None", ",", "\n", ")", ":", "\n", "        ", "\"\"\"\n        # Parameters\n\n        inputs : `torch.FloatTensor`, required.\n            A tensor of shape (batch_size, num_timesteps, input_size)\n            to apply the LSTM over.\n        batch_lengths : `List[int]`, required.\n            A list of length batch_size containing the lengths of the sequences in batch.\n        initial_state : `Tuple[torch.Tensor, torch.Tensor]`, optional, (default = None)\n            A tuple (state, memory) representing the initial hidden state and memory\n            of the LSTM. The `state` has shape (1, batch_size, hidden_size) and the\n            `memory` has shape (1, batch_size, cell_size).\n\n        # Returns\n\n        output_accumulator : `torch.FloatTensor`\n            The outputs of the LSTM for each timestep. A tensor of shape\n            (batch_size, max_timesteps, hidden_size) where for a given batch\n            element, all outputs past the sequence length for that batch are\n            zero tensors.\n        final_state : `Tuple[`torch.FloatTensor, torch.FloatTensor]`\n            A tuple (state, memory) representing the initial hidden state and memory\n            of the LSTM. The `state` has shape (1, batch_size, hidden_size) and the\n            `memory` has shape (1, batch_size, cell_size).\n        \"\"\"", "\n", "batch_size", "=", "inputs", ".", "size", "(", ")", "[", "0", "]", "\n", "total_timesteps", "=", "inputs", ".", "size", "(", ")", "[", "1", "]", "\n", "\n", "output_accumulator", "=", "inputs", ".", "new_zeros", "(", "\n", "batch_size", ",", "total_timesteps", ",", "self", ".", "hidden_size", "\n", ")", "\n", "\n", "if", "initial_state", "is", "None", ":", "\n", "            ", "full_batch_previous_memory", "=", "inputs", ".", "new_zeros", "(", "batch_size", ",", "self", ".", "cell_size", ")", "\n", "full_batch_previous_state", "=", "inputs", ".", "new_zeros", "(", "batch_size", ",", "self", ".", "hidden_size", ")", "\n", "", "else", ":", "\n", "            ", "full_batch_previous_state", "=", "initial_state", "[", "0", "]", ".", "squeeze", "(", "0", ")", "\n", "full_batch_previous_memory", "=", "initial_state", "[", "1", "]", ".", "squeeze", "(", "0", ")", "\n", "\n", "", "current_length_index", "=", "batch_size", "-", "1", "if", "self", ".", "go_forward", "else", "0", "\n", "if", "self", ".", "recurrent_dropout_probability", ">", "0.0", "and", "self", ".", "training", ":", "\n", "            ", "dropout_mask", "=", "get_dropout_mask", "(", "\n", "self", ".", "recurrent_dropout_probability", ",", "full_batch_previous_state", "\n", ")", "\n", "", "else", ":", "\n", "            ", "dropout_mask", "=", "None", "\n", "\n", "", "for", "timestep", "in", "range", "(", "total_timesteps", ")", ":", "\n", "# The index depends on which end we start.", "\n", "            ", "index", "=", "timestep", "if", "self", ".", "go_forward", "else", "total_timesteps", "-", "timestep", "-", "1", "\n", "\n", "# What we are doing here is finding the index into the batch dimension", "\n", "# which we need to use for this timestep, because the sequences have", "\n", "# variable length, so once the index is greater than the length of this", "\n", "# particular batch sequence, we no longer need to do the computation for", "\n", "# this sequence. The key thing to recognise here is that the batch inputs", "\n", "# must be _ordered_ by length from longest (first in batch) to shortest", "\n", "# (last) so initially, we are going forwards with every sequence and as we", "\n", "# pass the index at which the shortest elements of the batch finish,", "\n", "# we stop picking them up for the computation.", "\n", "if", "self", ".", "go_forward", ":", "\n", "                ", "while", "batch_lengths", "[", "current_length_index", "]", "<=", "index", ":", "\n", "                    ", "current_length_index", "-=", "1", "\n", "# If we're going backwards, we are _picking up_ more indices.", "\n", "", "", "else", ":", "\n", "# First conditional: Are we already at the maximum number of elements in the batch?", "\n", "# Second conditional: Does the next shortest sequence beyond the current batch", "\n", "# index require computation use this timestep?", "\n", "                ", "while", "(", "\n", "current_length_index", "<", "(", "len", "(", "batch_lengths", ")", "-", "1", ")", "\n", "and", "batch_lengths", "[", "current_length_index", "+", "1", "]", ">", "index", "\n", ")", ":", "\n", "                    ", "current_length_index", "+=", "1", "\n", "\n", "# Actually get the slices of the batch which we", "\n", "# need for the computation at this timestep.", "\n", "# shape (batch_size, cell_size)", "\n", "", "", "previous_memory", "=", "full_batch_previous_memory", "[", "\n", "0", ":", "current_length_index", "+", "1", "\n", "]", ".", "clone", "(", ")", "\n", "# Shape (batch_size, hidden_size)", "\n", "previous_state", "=", "full_batch_previous_state", "[", "\n", "0", ":", "current_length_index", "+", "1", "\n", "]", ".", "clone", "(", ")", "\n", "# Shape (batch_size, input_size)", "\n", "timestep_input", "=", "inputs", "[", "0", ":", "current_length_index", "+", "1", ",", "index", "]", "\n", "\n", "# Do the projections for all the gates all at once.", "\n", "# Both have shape (batch_size, 4 * cell_size)", "\n", "projected_input", "=", "self", ".", "input_linearity", "(", "timestep_input", ")", "\n", "projected_state", "=", "self", ".", "state_linearity", "(", "previous_state", ")", "\n", "\n", "# Main LSTM equations using relevant chunks of the big linear", "\n", "# projections of the hidden state and inputs.", "\n", "input_gate", "=", "torch", ".", "sigmoid", "(", "\n", "projected_input", "[", ":", ",", "(", "0", "*", "self", ".", "cell_size", ")", ":", "(", "1", "*", "self", ".", "cell_size", ")", "]", "\n", "+", "projected_state", "[", ":", ",", "(", "0", "*", "self", ".", "cell_size", ")", ":", "(", "1", "*", "self", ".", "cell_size", ")", "]", "\n", ")", "\n", "forget_gate", "=", "torch", ".", "sigmoid", "(", "\n", "projected_input", "[", ":", ",", "(", "1", "*", "self", ".", "cell_size", ")", ":", "(", "2", "*", "self", ".", "cell_size", ")", "]", "\n", "+", "projected_state", "[", ":", ",", "(", "1", "*", "self", ".", "cell_size", ")", ":", "(", "2", "*", "self", ".", "cell_size", ")", "]", "\n", ")", "\n", "memory_init", "=", "torch", ".", "tanh", "(", "\n", "projected_input", "[", ":", ",", "(", "2", "*", "self", ".", "cell_size", ")", ":", "(", "3", "*", "self", ".", "cell_size", ")", "]", "\n", "+", "projected_state", "[", ":", ",", "(", "2", "*", "self", ".", "cell_size", ")", ":", "(", "3", "*", "self", ".", "cell_size", ")", "]", "\n", ")", "\n", "output_gate", "=", "torch", ".", "sigmoid", "(", "\n", "projected_input", "[", ":", ",", "(", "3", "*", "self", ".", "cell_size", ")", ":", "(", "4", "*", "self", ".", "cell_size", ")", "]", "\n", "+", "projected_state", "[", ":", ",", "(", "3", "*", "self", ".", "cell_size", ")", ":", "(", "4", "*", "self", ".", "cell_size", ")", "]", "\n", ")", "\n", "memory", "=", "input_gate", "*", "memory_init", "+", "forget_gate", "*", "previous_memory", "\n", "\n", "# Here is the non-standard part of this LSTM cell; first, we clip the", "\n", "# memory cell, then we project the output of the timestep to a smaller size", "\n", "# and again clip it.", "\n", "\n", "if", "self", ".", "memory_cell_clip_value", ":", "\n", "\n", "                ", "memory", "=", "torch", ".", "clamp", "(", "\n", "memory", ",", "-", "self", ".", "memory_cell_clip_value", ",", "self", ".", "memory_cell_clip_value", "\n", ")", "\n", "\n", "# shape (current_length_index, cell_size)", "\n", "", "pre_projection_timestep_output", "=", "output_gate", "*", "torch", ".", "tanh", "(", "memory", ")", "\n", "\n", "# shape (current_length_index, hidden_size)", "\n", "timestep_output", "=", "self", ".", "state_projection", "(", "pre_projection_timestep_output", ")", "\n", "if", "self", ".", "state_projection_clip_value", ":", "\n", "\n", "                ", "timestep_output", "=", "torch", ".", "clamp", "(", "\n", "timestep_output", ",", "\n", "-", "self", ".", "state_projection_clip_value", ",", "\n", "self", ".", "state_projection_clip_value", ",", "\n", ")", "\n", "\n", "# Only do dropout if the dropout prob is > 0.0 and we are in training mode.", "\n", "", "if", "dropout_mask", "is", "not", "None", ":", "\n", "                ", "timestep_output", "=", "(", "\n", "timestep_output", "*", "dropout_mask", "[", "0", ":", "current_length_index", "+", "1", "]", "\n", ")", "\n", "\n", "# We've been doing computation with less than the full batch, so here we create a new", "\n", "# variable for the the whole batch at this timestep and insert the result for the", "\n", "# relevant elements of the batch into it.", "\n", "", "full_batch_previous_memory", "=", "full_batch_previous_memory", ".", "clone", "(", ")", "\n", "full_batch_previous_state", "=", "full_batch_previous_state", ".", "clone", "(", ")", "\n", "full_batch_previous_memory", "[", "0", ":", "current_length_index", "+", "1", "]", "=", "memory", "\n", "full_batch_previous_state", "[", "0", ":", "current_length_index", "+", "1", "]", "=", "timestep_output", "\n", "output_accumulator", "[", "0", ":", "current_length_index", "+", "1", ",", "index", "]", "=", "timestep_output", "\n", "\n", "# Mimic the pytorch API by returning state in the following shape:", "\n", "# (num_layers * num_directions, batch_size, ...). As this", "\n", "# LSTM cell cannot be stacked, the first dimension here is just 1.", "\n", "", "final_state", "=", "(", "\n", "full_batch_previous_state", ".", "unsqueeze", "(", "0", ")", ",", "\n", "full_batch_previous_memory", ".", "unsqueeze", "(", "0", ")", ",", "\n", ")", "\n", "\n", "return", "output_accumulator", ",", "final_state", "\n", "", "", ""]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.modules.highway.Highway.__init__": [[33, 51], ["super().__init__", "torch.nn.ModuleList", "layer.bias[].data.fill_", "torch.nn.Linear", "range"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.graph_util.plot.GraphPlot.__init__"], ["def", "__init__", "(", "\n", "self", ",", "\n", "input_dim", ":", "int", ",", "\n", "num_layers", ":", "int", "=", "1", ",", "\n", "activation", ":", "Callable", "[", "[", "torch", ".", "Tensor", "]", ",", "torch", ".", "Tensor", "]", "=", "torch", ".", "nn", ".", "functional", ".", "relu", ",", "\n", ")", "->", "None", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "_input_dim", "=", "input_dim", "\n", "self", ".", "_layers", "=", "torch", ".", "nn", ".", "ModuleList", "(", "\n", "[", "torch", ".", "nn", ".", "Linear", "(", "input_dim", ",", "input_dim", "*", "2", ")", "for", "_", "in", "range", "(", "num_layers", ")", "]", "\n", ")", "\n", "self", ".", "_activation", "=", "activation", "\n", "for", "layer", "in", "self", ".", "_layers", ":", "\n", "# We should bias the highway layer to just carry its input forward.  We do that by", "\n", "# setting the bias on `B(x)` to be positive, because that means `g` will be biased to", "\n", "# be high, so we will carry the input forward.  The bias on `B(x)` is the second half", "\n", "# of the bias vector in each Linear layer.", "\n", "            ", "layer", ".", "bias", "[", "input_dim", ":", "]", ".", "data", ".", "fill_", "(", "1", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.modules.highway.Highway.forward": [[52, 65], ["layer", "layer.chunk", "highway.Highway._activation", "torch.sigmoid"], "methods", ["None"], ["", "", "@", "overrides", "\n", "def", "forward", "(", "self", ",", "inputs", ":", "torch", ".", "Tensor", ")", "->", "torch", ".", "Tensor", ":", "\n", "        ", "current_input", "=", "inputs", "\n", "for", "layer", "in", "self", ".", "_layers", ":", "\n", "            ", "projected_input", "=", "layer", "(", "current_input", ")", "\n", "linear_part", "=", "current_input", "\n", "# NOTE: if you modify this, think about whether you should modify the initialization", "\n", "# above, too.", "\n", "nonlinear_part", ",", "gate", "=", "projected_input", ".", "chunk", "(", "2", ",", "dim", "=", "-", "1", ")", "\n", "nonlinear_part", "=", "self", ".", "_activation", "(", "nonlinear_part", ")", "\n", "gate", "=", "torch", ".", "sigmoid", "(", "gate", ")", "\n", "current_input", "=", "gate", "*", "linear_part", "+", "(", "1", "-", "gate", ")", "*", "nonlinear_part", "\n", "", "return", "current_input", "\n", "", "", ""]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.modules.stacked_alternating_lstm.StackedAlternatingLstm.__init__": [[48, 80], ["super().__init__", "range", "allennlp.modules.augmented_lstm.AugmentedLstm", "stacked_alternating_lstm.StackedAlternatingLstm.add_module", "layers.append"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.graph_util.plot.GraphPlot.__init__"], ["def", "__init__", "(", "\n", "self", ",", "\n", "input_size", ":", "int", ",", "\n", "hidden_size", ":", "int", ",", "\n", "num_layers", ":", "int", ",", "\n", "recurrent_dropout_probability", ":", "float", "=", "0.0", ",", "\n", "use_highway", ":", "bool", "=", "True", ",", "\n", "use_input_projection_bias", ":", "bool", "=", "True", ",", "\n", ")", "->", "None", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "\n", "# Required to be wrapped with a `PytorchSeq2SeqWrapper`.", "\n", "self", ".", "input_size", "=", "input_size", "\n", "self", ".", "hidden_size", "=", "hidden_size", "\n", "self", ".", "num_layers", "=", "num_layers", "\n", "\n", "layers", "=", "[", "]", "\n", "lstm_input_size", "=", "input_size", "\n", "for", "layer_index", "in", "range", "(", "num_layers", ")", ":", "\n", "            ", "go_forward", "=", "layer_index", "%", "2", "==", "0", "\n", "layer", "=", "AugmentedLstm", "(", "\n", "lstm_input_size", ",", "\n", "hidden_size", ",", "\n", "go_forward", ",", "\n", "recurrent_dropout_probability", "=", "recurrent_dropout_probability", ",", "\n", "use_highway", "=", "use_highway", ",", "\n", "use_input_projection_bias", "=", "use_input_projection_bias", ",", "\n", ")", "\n", "lstm_input_size", "=", "hidden_size", "\n", "self", ".", "add_module", "(", "\"layer_{}\"", ".", "format", "(", "layer_index", ")", ",", "layer", ")", "\n", "layers", ".", "append", "(", "layer", ")", "\n", "", "self", ".", "lstm_layers", "=", "layers", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.modules.stacked_alternating_lstm.StackedAlternatingLstm.forward": [[81, 125], ["enumerate", "tuple", "getattr", "getattr.", "final_states.append", "len", "len", "allennlp.common.checks.ConfigurationError", "list", "torch.cat", "initial_state[].size", "zip", "zip", "initial_state[].split", "initial_state[].split"], "methods", ["None"], ["", "def", "forward", "(", "\n", "self", ",", "inputs", ":", "PackedSequence", ",", "initial_state", ":", "Optional", "[", "TensorPair", "]", "=", "None", "\n", ")", "->", "Tuple", "[", "Union", "[", "torch", ".", "Tensor", ",", "PackedSequence", "]", ",", "TensorPair", "]", ":", "\n", "        ", "\"\"\"\n        # Parameters\n\n        inputs : `PackedSequence`, required.\n            A batch first `PackedSequence` to run the stacked LSTM over.\n        initial_state : Tuple[torch.Tensor, torch.Tensor], optional, (default = None)\n            A tuple (state, memory) representing the initial hidden state and memory\n            of the LSTM. Each tensor has shape (1, batch_size, output_dimension).\n\n        # Returns\n\n        output_sequence : PackedSequence\n            The encoded sequence of shape (batch_size, sequence_length, hidden_size)\n        final_states: Tuple[torch.Tensor, torch.Tensor]\n            The per-layer final (state, memory) states of the LSTM, each with shape\n            (num_layers, batch_size, hidden_size).\n        \"\"\"", "\n", "if", "not", "initial_state", ":", "\n", "            ", "hidden_states", ":", "List", "[", "Optional", "[", "TensorPair", "]", "]", "=", "[", "None", "]", "*", "len", "(", "self", ".", "lstm_layers", ")", "\n", "", "elif", "initial_state", "[", "0", "]", ".", "size", "(", ")", "[", "0", "]", "!=", "len", "(", "self", ".", "lstm_layers", ")", ":", "\n", "            ", "raise", "ConfigurationError", "(", "\n", "\"Initial states were passed to forward() but the number of \"", "\n", "\"initial states does not match the number of layers.\"", "\n", ")", "\n", "", "else", ":", "\n", "            ", "hidden_states", "=", "list", "(", "\n", "zip", "(", "initial_state", "[", "0", "]", ".", "split", "(", "1", ",", "0", ")", ",", "initial_state", "[", "1", "]", ".", "split", "(", "1", ",", "0", ")", ")", "\n", ")", "\n", "\n", "", "output_sequence", "=", "inputs", "\n", "final_states", "=", "[", "]", "\n", "for", "i", ",", "state", "in", "enumerate", "(", "hidden_states", ")", ":", "\n", "            ", "layer", "=", "getattr", "(", "self", ",", "\"layer_{}\"", ".", "format", "(", "i", ")", ")", "\n", "# The state is duplicated to mirror the Pytorch API for LSTMs.", "\n", "output_sequence", ",", "final_state", "=", "layer", "(", "output_sequence", ",", "state", ")", "\n", "final_states", ".", "append", "(", "final_state", ")", "\n", "\n", "", "final_hidden_state", ",", "final_cell_state", "=", "tuple", "(", "\n", "torch", ".", "cat", "(", "state_list", ",", "0", ")", "for", "state_list", "in", "zip", "(", "*", "final_states", ")", "\n", ")", "\n", "return", "output_sequence", ",", "(", "final_hidden_state", ",", "final_cell_state", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.modules.residual_with_layer_dropout.ResidualWithLayerDropout.__init__": [[15, 23], ["super().__init__", "ValueError"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.graph_util.plot.GraphPlot.__init__"], ["def", "__init__", "(", "self", ",", "undecayed_dropout_prob", ":", "float", "=", "0.5", ")", "->", "None", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "if", "undecayed_dropout_prob", "<", "0", "or", "undecayed_dropout_prob", ">", "1", ":", "\n", "            ", "raise", "ValueError", "(", "\n", "f\"undecayed dropout probability has to be between 0 and 1, \"", "\n", "f\"but got {undecayed_dropout_prob}\"", "\n", ")", "\n", "", "self", ".", "undecayed_dropout_prob", "=", "undecayed_dropout_prob", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.modules.residual_with_layer_dropout.ResidualWithLayerDropout.forward": [[24, 67], ["torch.rand"], "methods", ["None"], ["", "def", "forward", "(", "\n", "self", ",", "# type: ignore", "\n", "layer_input", ":", "torch", ".", "Tensor", ",", "\n", "layer_output", ":", "torch", ".", "Tensor", ",", "\n", "layer_index", ":", "int", "=", "None", ",", "\n", "total_layers", ":", "int", "=", "None", ",", "\n", ")", "->", "torch", ".", "Tensor", ":", "\n", "\n", "        ", "\"\"\"\n        Apply dropout to this layer, for this whole mini-batch.\n        dropout_prob = layer_index / total_layers * undecayed_dropout_prob if layer_idx and\n        total_layers is specified, else it will use the undecayed_dropout_prob directly.\n\n        # Parameters\n\n        layer_input `torch.FloatTensor` required\n            The input tensor of this layer.\n        layer_output `torch.FloatTensor` required\n            The output tensor of this layer, with the same shape as the layer_input.\n        layer_index `int`\n            The layer index, starting from 1. This is used to calcuate the dropout prob\n            together with the `total_layers` parameter.\n        total_layers `int`\n            The total number of layers.\n\n        # Returns\n\n        output : `torch.FloatTensor`\n            A tensor with the same shape as `layer_input` and `layer_output`.\n        \"\"\"", "\n", "if", "layer_index", "is", "not", "None", "and", "total_layers", "is", "not", "None", ":", "\n", "            ", "dropout_prob", "=", "(", "\n", "1.0", "*", "self", ".", "undecayed_dropout_prob", "*", "layer_index", "/", "total_layers", "\n", ")", "\n", "", "else", ":", "\n", "            ", "dropout_prob", "=", "1.0", "*", "self", ".", "undecayed_dropout_prob", "\n", "", "if", "self", ".", "training", ":", "\n", "            ", "if", "torch", ".", "rand", "(", "1", ")", "<", "dropout_prob", ":", "\n", "                ", "return", "layer_input", "\n", "", "else", ":", "\n", "                ", "return", "layer_output", "+", "layer_input", "\n", "", "", "else", ":", "\n", "            ", "return", "(", "1", "-", "dropout_prob", ")", "*", "layer_output", "+", "layer_input", "\n", "", "", "", ""]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.modules.scalar_mix.ScalarMix.__init__": [[18, 47], ["super().__init__", "torch.nn.ParameterList", "torch.nn.Parameter", "torch.FloatTensor", "len", "allennlp.common.checks.ConfigurationError", "torch.nn.Parameter", "torch.FloatTensor", "range"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.graph_util.plot.GraphPlot.__init__"], ["def", "__init__", "(", "\n", "self", ",", "\n", "mixture_size", ":", "int", ",", "\n", "do_layer_norm", ":", "bool", "=", "False", ",", "\n", "initial_scalar_parameters", ":", "List", "[", "float", "]", "=", "None", ",", "\n", "trainable", ":", "bool", "=", "True", ",", "\n", ")", "->", "None", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "mixture_size", "=", "mixture_size", "\n", "self", ".", "do_layer_norm", "=", "do_layer_norm", "\n", "\n", "if", "initial_scalar_parameters", "is", "None", ":", "\n", "            ", "initial_scalar_parameters", "=", "[", "0.0", "]", "*", "mixture_size", "\n", "", "elif", "len", "(", "initial_scalar_parameters", ")", "!=", "mixture_size", ":", "\n", "            ", "raise", "ConfigurationError", "(", "\n", "\"Length of initial_scalar_parameters {} differs \"", "\n", "\"from mixture_size {}\"", ".", "format", "(", "initial_scalar_parameters", ",", "mixture_size", ")", "\n", ")", "\n", "\n", "", "self", ".", "scalar_parameters", "=", "ParameterList", "(", "\n", "[", "\n", "Parameter", "(", "\n", "torch", ".", "FloatTensor", "(", "[", "initial_scalar_parameters", "[", "i", "]", "]", ")", ",", "\n", "requires_grad", "=", "trainable", ",", "\n", ")", "\n", "for", "i", "in", "range", "(", "mixture_size", ")", "\n", "]", "\n", ")", "\n", "self", ".", "gamma", "=", "Parameter", "(", "torch", ".", "FloatTensor", "(", "[", "1.0", "]", ")", ",", "requires_grad", "=", "trainable", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.modules.scalar_mix.ScalarMix.forward": [[48, 101], ["torch.nn.functional.softmax", "torch.split", "len", "allennlp.common.checks.ConfigurationError", "torch.cat", "zip", "mask.float", "mask.float.unsqueeze", "tensors[].size", "zip", "torch.sum", "torch.sum", "torch.sqrt", "pieces.append", "sum", "torch.sum", "pieces.append", "sum", "len", "scalar_mix.ScalarMix.forward._do_layer_norm"], "methods", ["None"], ["", "def", "forward", "(", "\n", "self", ",", "tensors", ":", "List", "[", "torch", ".", "Tensor", "]", ",", "mask", ":", "torch", ".", "Tensor", "=", "None", "\n", ")", "->", "torch", ".", "Tensor", ":", "\n", "        ", "\"\"\"\n        Compute a weighted average of the `tensors`.  The input tensors an be any shape\n        with at least two dimensions, but must all be the same shape.\n\n        When `do_layer_norm=True`, the `mask` is required input.  If the `tensors` are\n        dimensioned  `(dim_0, ..., dim_{n-1}, dim_n)`, then the `mask` is dimensioned\n        `(dim_0, ..., dim_{n-1})`, as in the typical case with `tensors` of shape\n        `(batch_size, timesteps, dim)` and `mask` of shape `(batch_size, timesteps)`.\n\n        When `do_layer_norm=False` the `mask` is ignored.\n        \"\"\"", "\n", "if", "len", "(", "tensors", ")", "!=", "self", ".", "mixture_size", ":", "\n", "            ", "raise", "ConfigurationError", "(", "\n", "\"{} tensors were passed, but the module was initialized to \"", "\n", "\"mix {} tensors.\"", ".", "format", "(", "len", "(", "tensors", ")", ",", "self", ".", "mixture_size", ")", "\n", ")", "\n", "\n", "", "def", "_do_layer_norm", "(", "tensor", ",", "broadcast_mask", ",", "num_elements_not_masked", ")", ":", "\n", "            ", "tensor_masked", "=", "tensor", "*", "broadcast_mask", "\n", "mean", "=", "torch", ".", "sum", "(", "tensor_masked", ")", "/", "num_elements_not_masked", "\n", "variance", "=", "(", "\n", "torch", ".", "sum", "(", "(", "(", "tensor_masked", "-", "mean", ")", "*", "broadcast_mask", ")", "**", "2", ")", "\n", "/", "num_elements_not_masked", "\n", ")", "\n", "return", "(", "tensor", "-", "mean", ")", "/", "torch", ".", "sqrt", "(", "variance", "+", "1e-12", ")", "\n", "\n", "", "normed_weights", "=", "torch", ".", "nn", ".", "functional", ".", "softmax", "(", "\n", "torch", ".", "cat", "(", "[", "parameter", "for", "parameter", "in", "self", ".", "scalar_parameters", "]", ")", ",", "dim", "=", "0", "\n", ")", "\n", "normed_weights", "=", "torch", ".", "split", "(", "normed_weights", ",", "split_size_or_sections", "=", "1", ")", "\n", "\n", "if", "not", "self", ".", "do_layer_norm", ":", "\n", "            ", "pieces", "=", "[", "]", "\n", "for", "weight", ",", "tensor", "in", "zip", "(", "normed_weights", ",", "tensors", ")", ":", "\n", "                ", "pieces", ".", "append", "(", "weight", "*", "tensor", ")", "\n", "", "return", "self", ".", "gamma", "*", "sum", "(", "pieces", ")", "\n", "\n", "", "else", ":", "\n", "            ", "mask_float", "=", "mask", ".", "float", "(", ")", "\n", "broadcast_mask", "=", "mask_float", ".", "unsqueeze", "(", "-", "1", ")", "\n", "input_dim", "=", "tensors", "[", "0", "]", ".", "size", "(", "-", "1", ")", "\n", "num_elements_not_masked", "=", "torch", ".", "sum", "(", "mask_float", ")", "*", "input_dim", "\n", "\n", "pieces", "=", "[", "]", "\n", "for", "weight", ",", "tensor", "in", "zip", "(", "normed_weights", ",", "tensors", ")", ":", "\n", "                ", "pieces", ".", "append", "(", "\n", "weight", "\n", "*", "_do_layer_norm", "(", "tensor", ",", "broadcast_mask", ",", "num_elements_not_masked", ")", "\n", ")", "\n", "", "return", "self", ".", "gamma", "*", "sum", "(", "pieces", ")", "\n", "", "", "", ""]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.modules.maxout.Maxout.__init__": [[35, 80], ["super().__init__", "zip", "torch.nn.ModuleList", "torch.nn.ModuleList", "isinstance", "isinstance", "isinstance", "len", "allennlp.common.checks.ConfigurationError", "len", "allennlp.common.checks.ConfigurationError", "len", "allennlp.common.checks.ConfigurationError", "linear_layers.append", "torch.nn.Dropout", "torch.nn.Linear", "len", "len", "len"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.graph_util.plot.GraphPlot.__init__"], ["def", "__init__", "(", "\n", "self", ",", "\n", "input_dim", ":", "int", ",", "\n", "num_layers", ":", "int", ",", "\n", "output_dims", ":", "Union", "[", "int", ",", "Sequence", "[", "int", "]", "]", ",", "\n", "pool_sizes", ":", "Union", "[", "int", ",", "Sequence", "[", "int", "]", "]", ",", "\n", "dropout", ":", "Union", "[", "float", ",", "Sequence", "[", "float", "]", "]", "=", "0.0", ",", "\n", ")", "->", "None", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "if", "not", "isinstance", "(", "output_dims", ",", "list", ")", ":", "\n", "            ", "output_dims", "=", "[", "output_dims", "]", "*", "num_layers", "# type: ignore", "\n", "", "if", "not", "isinstance", "(", "pool_sizes", ",", "list", ")", ":", "\n", "            ", "pool_sizes", "=", "[", "pool_sizes", "]", "*", "num_layers", "# type: ignore", "\n", "", "if", "not", "isinstance", "(", "dropout", ",", "list", ")", ":", "\n", "            ", "dropout", "=", "[", "dropout", "]", "*", "num_layers", "# type: ignore", "\n", "", "if", "len", "(", "output_dims", ")", "!=", "num_layers", ":", "\n", "            ", "raise", "ConfigurationError", "(", "\n", "\"len(output_dims) (%d) != num_layers (%d)\"", "\n", "%", "(", "len", "(", "output_dims", ")", ",", "num_layers", ")", "\n", ")", "\n", "", "if", "len", "(", "pool_sizes", ")", "!=", "num_layers", ":", "\n", "            ", "raise", "ConfigurationError", "(", "\n", "\"len(pool_sizes) (%d) != num_layers (%d)\"", "\n", "%", "(", "len", "(", "pool_sizes", ")", ",", "num_layers", ")", "\n", ")", "\n", "", "if", "len", "(", "dropout", ")", "!=", "num_layers", ":", "\n", "            ", "raise", "ConfigurationError", "(", "\n", "\"len(dropout) (%d) != num_layers (%d)\"", "%", "(", "len", "(", "dropout", ")", ",", "num_layers", ")", "\n", ")", "\n", "\n", "", "self", ".", "_pool_sizes", "=", "pool_sizes", "\n", "input_dims", "=", "[", "input_dim", "]", "+", "output_dims", "[", ":", "-", "1", "]", "\n", "linear_layers", "=", "[", "]", "\n", "for", "layer_input_dim", ",", "layer_output_dim", ",", "pool_size", "in", "zip", "(", "\n", "input_dims", ",", "output_dims", ",", "pool_sizes", "\n", ")", ":", "\n", "            ", "linear_layers", ".", "append", "(", "\n", "torch", ".", "nn", ".", "Linear", "(", "layer_input_dim", ",", "layer_output_dim", "*", "pool_size", ")", "\n", ")", "\n", "", "self", ".", "_linear_layers", "=", "torch", ".", "nn", ".", "ModuleList", "(", "linear_layers", ")", "\n", "dropout_layers", "=", "[", "torch", ".", "nn", ".", "Dropout", "(", "p", "=", "value", ")", "for", "value", "in", "dropout", "]", "\n", "self", ".", "_dropout", "=", "torch", ".", "nn", ".", "ModuleList", "(", "dropout_layers", ")", "\n", "self", ".", "_output_dims", "=", "output_dims", "\n", "self", ".", "_output_dim", "=", "output_dims", "[", "-", "1", "]", "\n", "self", ".", "_input_dim", "=", "input_dim", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.modules.maxout.Maxout.get_output_dim": [[81, 83], ["None"], "methods", ["None"], ["", "def", "get_output_dim", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "_output_dim", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.modules.maxout.Maxout.get_input_dim": [[84, 86], ["None"], "methods", ["None"], ["", "def", "get_input_dim", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "_input_dim", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.modules.maxout.Maxout.forward": [[87, 103], ["zip", "layer", "list", "list.append", "dropout", "inputs.size", "torch.max", "layer.view"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "inputs", ":", "torch", ".", "Tensor", ")", "->", "torch", ".", "Tensor", ":", "\n", "\n", "        ", "output", "=", "inputs", "\n", "for", "layer", ",", "layer_output_dim", ",", "dropout", ",", "pool_size", "in", "zip", "(", "\n", "self", ".", "_linear_layers", ",", "self", ".", "_output_dims", ",", "self", ".", "_dropout", ",", "self", ".", "_pool_sizes", "\n", ")", ":", "\n", "            ", "affine_output", "=", "layer", "(", "output", ")", "\n", "# Compute and apply the proper shape for the max.", "\n", "shape", "=", "list", "(", "inputs", ".", "size", "(", ")", ")", "\n", "shape", "[", "-", "1", "]", "=", "layer_output_dim", "\n", "shape", ".", "append", "(", "pool_size", ")", "\n", "\n", "maxed_output", "=", "torch", ".", "max", "(", "affine_output", ".", "view", "(", "*", "shape", ")", ",", "dim", "=", "-", "1", ")", "[", "0", "]", "\n", "dropped_output", "=", "dropout", "(", "maxed_output", ")", "\n", "output", "=", "dropped_output", "\n", "", "return", "output", "\n", "", "", ""]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.modules.encoder_base._EncoderBase.__init__": [[28, 32], ["super().__init__"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.graph_util.plot.GraphPlot.__init__"], ["def", "__init__", "(", "self", ",", "stateful", ":", "bool", "=", "False", ")", "->", "None", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "stateful", "=", "stateful", "\n", "self", ".", "_states", ":", "Optional", "[", "RnnStateStorage", "]", "=", "None", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.modules.encoder_base._EncoderBase.sort_and_run_forward": [[33, 138], ["mask.size", "torch.sum().int().item", "allennlp.nn.util.get_lengths_from_binary_sequence_mask", "allennlp.nn.util.sort_batch_by_length", "torch.nn.utils.rnn.pack_padded_sequence", "module", "sorted_sequence_lengths[].data.tolist", "encoder_base._EncoderBase._get_initial_states", "torch.sum().int", "isinstance", "[].contiguous", "torch.sum", "[].contiguous", "hidden_state.index_select", "state.index_select"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.nn.util.get_lengths_from_binary_sequence_mask", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.nn.util.sort_batch_by_length", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.modules.encoder_base._EncoderBase._get_initial_states"], ["", "def", "sort_and_run_forward", "(", "\n", "self", ",", "\n", "module", ":", "Callable", "[", "\n", "[", "PackedSequence", ",", "Optional", "[", "RnnState", "]", "]", ",", "\n", "Tuple", "[", "Union", "[", "PackedSequence", ",", "torch", ".", "Tensor", "]", ",", "RnnState", "]", ",", "\n", "]", ",", "\n", "inputs", ":", "torch", ".", "Tensor", ",", "\n", "mask", ":", "torch", ".", "Tensor", ",", "\n", "hidden_state", ":", "Optional", "[", "RnnState", "]", "=", "None", ",", "\n", ")", ":", "\n", "        ", "\"\"\"\n        This function exists because Pytorch RNNs require that their inputs be sorted\n        before being passed as input. As all of our Seq2xxxEncoders use this functionality,\n        it is provided in a base class. This method can be called on any module which\n        takes as input a `PackedSequence` and some `hidden_state`, which can either be a\n        tuple of tensors or a tensor.\n\n        As all of our Seq2xxxEncoders have different return types, we return `sorted`\n        outputs from the module, which is called directly. Additionally, we return the\n        indices into the batch dimension required to restore the tensor to it's correct,\n        unsorted order and the number of valid batch elements (i.e the number of elements\n        in the batch which are not completely masked). This un-sorting and re-padding\n        of the module outputs is left to the subclasses because their outputs have different\n        types and handling them smoothly here is difficult.\n\n        # Parameters\n\n        module : `Callable[[PackedSequence, Optional[RnnState]],\n                            Tuple[Union[PackedSequence, torch.Tensor], RnnState]]`, required.\n            A function to run on the inputs. In most cases, this is a `torch.nn.Module`.\n        inputs : `torch.Tensor`, required.\n            A tensor of shape `(batch_size, sequence_length, embedding_size)` representing\n            the inputs to the Encoder.\n        mask : `torch.Tensor`, required.\n            A tensor of shape `(batch_size, sequence_length)`, representing masked and\n            non-masked elements of the sequence for each element in the batch.\n        hidden_state : `Optional[RnnState]`, (default = None).\n            A single tensor of shape (num_layers, batch_size, hidden_size) representing the\n            state of an RNN with or a tuple of\n            tensors of shapes (num_layers, batch_size, hidden_size) and\n            (num_layers, batch_size, memory_size), representing the hidden state and memory\n            state of an LSTM-like RNN.\n\n        # Returns\n\n        module_output : `Union[torch.Tensor, PackedSequence]`.\n            A Tensor or PackedSequence representing the output of the Pytorch Module.\n            The batch size dimension will be equal to `num_valid`, as sequences of zero\n            length are clipped off before the module is called, as Pytorch cannot handle\n            zero length sequences.\n        final_states : `Optional[RnnState]`\n            A Tensor representing the hidden state of the Pytorch Module. This can either\n            be a single tensor of shape (num_layers, num_valid, hidden_size), for instance in\n            the case of a GRU, or a tuple of tensors, such as those required for an LSTM.\n        restoration_indices : `torch.LongTensor`\n            A tensor of shape `(batch_size,)`, describing the re-indexing required to transform\n            the outputs back to their original batch order.\n        \"\"\"", "\n", "# In some circumstances you may have sequences of zero length. `pack_padded_sequence`", "\n", "# requires all sequence lengths to be > 0, so remove sequences of zero length before", "\n", "# calling self._module, then fill with zeros.", "\n", "\n", "# First count how many sequences are empty.", "\n", "batch_size", "=", "mask", ".", "size", "(", "0", ")", "\n", "num_valid", "=", "torch", ".", "sum", "(", "mask", "[", ":", ",", "0", "]", ")", ".", "int", "(", ")", ".", "item", "(", ")", "\n", "\n", "sequence_lengths", "=", "get_lengths_from_binary_sequence_mask", "(", "mask", ")", "\n", "(", "\n", "sorted_inputs", ",", "\n", "sorted_sequence_lengths", ",", "\n", "restoration_indices", ",", "\n", "sorting_indices", ",", "\n", ")", "=", "sort_batch_by_length", "(", "inputs", ",", "sequence_lengths", ")", "\n", "\n", "# Now create a PackedSequence with only the non-empty, sorted sequences.", "\n", "packed_sequence_input", "=", "pack_padded_sequence", "(", "\n", "sorted_inputs", "[", ":", "num_valid", ",", ":", ",", ":", "]", ",", "\n", "sorted_sequence_lengths", "[", ":", "num_valid", "]", ".", "data", ".", "tolist", "(", ")", ",", "\n", "batch_first", "=", "True", ",", "\n", ")", "\n", "# Prepare the initial states.", "\n", "if", "not", "self", ".", "stateful", ":", "\n", "            ", "if", "hidden_state", "is", "None", ":", "\n", "                ", "initial_states", ":", "Any", "=", "hidden_state", "\n", "", "elif", "isinstance", "(", "hidden_state", ",", "tuple", ")", ":", "\n", "                ", "initial_states", "=", "[", "\n", "state", ".", "index_select", "(", "1", ",", "sorting_indices", ")", "[", "\n", ":", ",", ":", "num_valid", ",", ":", "\n", "]", ".", "contiguous", "(", ")", "\n", "for", "state", "in", "hidden_state", "\n", "]", "\n", "", "else", ":", "\n", "                ", "initial_states", "=", "hidden_state", ".", "index_select", "(", "1", ",", "sorting_indices", ")", "[", "\n", ":", ",", ":", "num_valid", ",", ":", "\n", "]", ".", "contiguous", "(", ")", "\n", "\n", "", "", "else", ":", "\n", "            ", "initial_states", "=", "self", ".", "_get_initial_states", "(", "\n", "batch_size", ",", "num_valid", ",", "sorting_indices", "\n", ")", "\n", "\n", "# Actually call the module on the sorted PackedSequence.", "\n", "", "module_output", ",", "final_states", "=", "module", "(", "packed_sequence_input", ",", "initial_states", ")", "\n", "\n", "return", "module_output", ",", "final_states", ",", "restoration_indices", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.modules.encoder_base._EncoderBase._get_initial_states": [[139, 229], ["encoder_base._EncoderBase._states[].size", "tuple", "len", "correctly_shaped_state.index_select", "sorted_state[].contiguous", "tuple", "encoder_base._EncoderBase._states[].size", "state.new_zeros", "resized_states.append", "encoder_base._EncoderBase._states[].size", "tuple", "state.index_select", "state.size", "state.size", "torch.cat", "state[].contiguous"], "methods", ["None"], ["", "def", "_get_initial_states", "(", "\n", "self", ",", "batch_size", ":", "int", ",", "num_valid", ":", "int", ",", "sorting_indices", ":", "torch", ".", "LongTensor", "\n", ")", "->", "Optional", "[", "RnnState", "]", ":", "\n", "        ", "\"\"\"\n        Returns an initial state for use in an RNN. Additionally, this method handles\n        the batch size changing across calls by mutating the state to append initial states\n        for new elements in the batch. Finally, it also handles sorting the states\n        with respect to the sequence lengths of elements in the batch and removing rows\n        which are completely padded. Importantly, this `mutates` the state if the\n        current batch size is larger than when it was previously called.\n\n        # Parameters\n\n        batch_size : `int`, required.\n            The batch size can change size across calls to stateful RNNs, so we need\n            to know if we need to expand or shrink the states before returning them.\n            Expanded states will be set to zero.\n        num_valid : `int`, required.\n            The batch may contain completely padded sequences which get removed before\n            the sequence is passed through the encoder. We also need to clip these off\n            of the state too.\n        sorting_indices `torch.LongTensor`, required.\n            Pytorch RNNs take sequences sorted by length. When we return the states to be\n            used for a given call to `module.forward`, we need the states to match up to\n            the sorted sequences, so before returning them, we sort the states using the\n            same indices used to sort the sequences.\n\n        # Returns\n\n        This method has a complex return type because it has to deal with the first time it\n        is called, when it has no state, and the fact that types of RNN have heterogeneous\n        states.\n\n        If it is the first time the module has been called, it returns `None`, regardless\n        of the type of the `Module`.\n\n        Otherwise, for LSTMs, it returns a tuple of `torch.Tensors` with shape\n        `(num_layers, num_valid, state_size)` and `(num_layers, num_valid, memory_size)`\n        respectively, or for GRUs, it returns a single `torch.Tensor` of shape\n        `(num_layers, num_valid, state_size)`.\n        \"\"\"", "\n", "# We don't know the state sizes the first time calling forward,", "\n", "# so we let the module define what it's initial hidden state looks like.", "\n", "if", "self", ".", "_states", "is", "None", ":", "\n", "            ", "return", "None", "\n", "\n", "# Otherwise, we have some previous states.", "\n", "", "if", "batch_size", ">", "self", ".", "_states", "[", "0", "]", ".", "size", "(", "1", ")", ":", "\n", "# This batch is larger than the all previous states.", "\n", "# If so, resize the states.", "\n", "            ", "num_states_to_concat", "=", "batch_size", "-", "self", ".", "_states", "[", "0", "]", ".", "size", "(", "1", ")", "\n", "resized_states", "=", "[", "]", "\n", "# state has shape (num_layers, batch_size, hidden_size)", "\n", "for", "state", "in", "self", ".", "_states", ":", "\n", "# This _must_ be inside the loop because some", "\n", "# RNNs have states with different last dimension sizes.", "\n", "                ", "zeros", "=", "state", ".", "new_zeros", "(", "\n", "state", ".", "size", "(", "0", ")", ",", "num_states_to_concat", ",", "state", ".", "size", "(", "2", ")", "\n", ")", "\n", "resized_states", ".", "append", "(", "torch", ".", "cat", "(", "[", "state", ",", "zeros", "]", ",", "1", ")", ")", "\n", "", "self", ".", "_states", "=", "tuple", "(", "resized_states", ")", "\n", "correctly_shaped_states", "=", "self", ".", "_states", "\n", "\n", "", "elif", "batch_size", "<", "self", ".", "_states", "[", "0", "]", ".", "size", "(", "1", ")", ":", "\n", "# This batch is smaller than the previous one.", "\n", "            ", "correctly_shaped_states", "=", "tuple", "(", "\n", "state", "[", ":", ",", ":", "batch_size", ",", ":", "]", "for", "state", "in", "self", ".", "_states", "\n", ")", "\n", "", "else", ":", "\n", "            ", "correctly_shaped_states", "=", "self", ".", "_states", "\n", "\n", "# At this point, our states are of shape (num_layers, batch_size, hidden_size).", "\n", "# However, the encoder uses sorted sequences and additionally removes elements", "\n", "# of the batch which are fully padded. We need the states to match up to these", "\n", "# sorted and filtered sequences, so we do that in the next two blocks before", "\n", "# returning the state/s.", "\n", "", "if", "len", "(", "self", ".", "_states", ")", "==", "1", ":", "\n", "# GRUs only have a single state. This `unpacks` it from the", "\n", "# tuple and returns the tensor directly.", "\n", "            ", "correctly_shaped_state", "=", "correctly_shaped_states", "[", "0", "]", "\n", "sorted_state", "=", "correctly_shaped_state", ".", "index_select", "(", "1", ",", "sorting_indices", ")", "\n", "return", "sorted_state", "[", ":", ",", ":", "num_valid", ",", ":", "]", ".", "contiguous", "(", ")", "\n", "", "else", ":", "\n", "# LSTMs have a state tuple of (state, memory).", "\n", "            ", "sorted_states", "=", "[", "\n", "state", ".", "index_select", "(", "1", ",", "sorting_indices", ")", "\n", "for", "state", "in", "correctly_shaped_states", "\n", "]", "\n", "return", "tuple", "(", "\n", "state", "[", ":", ",", ":", "num_valid", ",", ":", "]", ".", "contiguous", "(", ")", "for", "state", "in", "sorted_states", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.modules.encoder_base._EncoderBase._update_states": [[231, 313], ["state.index_select", "tuple", "encoder_base._EncoderBase._states[].size", "final_states[].size", "tuple", "zip", "zip", "new_states.append", "new_states.append", "old_state.detach", "new_state.detach", "state[].sum"], "methods", ["None"], ["", "", "def", "_update_states", "(", "\n", "self", ",", "final_states", ":", "RnnStateStorage", ",", "restoration_indices", ":", "torch", ".", "LongTensor", "\n", ")", "->", "None", ":", "\n", "        ", "\"\"\"\n        After the RNN has run forward, the states need to be updated.\n        This method just sets the state to the updated new state, performing\n        several pieces of book-keeping along the way - namely, unsorting the\n        states and ensuring that the states of completely padded sequences are\n        not updated. Finally, it also detaches the state variable from the\n        computational graph, such that the graph can be garbage collected after\n        each batch iteration.\n\n        # Parameters\n\n        final_states : `RnnStateStorage`, required.\n            The hidden states returned as output from the RNN.\n        restoration_indices : `torch.LongTensor`, required.\n            The indices that invert the sorting used in `sort_and_run_forward`\n            to order the states with respect to the lengths of the sequences in\n            the batch.\n        \"\"\"", "\n", "# TODO(Mark): seems weird to sort here, but append zeros in the subclasses.", "\n", "# which way around is best?", "\n", "new_unsorted_states", "=", "[", "\n", "state", ".", "index_select", "(", "1", ",", "restoration_indices", ")", "for", "state", "in", "final_states", "\n", "]", "\n", "\n", "if", "self", ".", "_states", "is", "None", ":", "\n", "# We don't already have states, so just set the", "\n", "# ones we receive to be the current state.", "\n", "            ", "self", ".", "_states", "=", "tuple", "(", "state", ".", "data", "for", "state", "in", "new_unsorted_states", ")", "\n", "", "else", ":", "\n", "# Now we've sorted the states back so that they correspond to the original", "\n", "# indices, we need to figure out what states we need to update, because if we", "\n", "# didn't use a state for a particular row, we want to preserve its state.", "\n", "# Thankfully, the rows which are all zero in the state correspond exactly", "\n", "# to those which aren't used, so we create masks of shape (new_batch_size,),", "\n", "# denoting which states were used in the RNN computation.", "\n", "            ", "current_state_batch_size", "=", "self", ".", "_states", "[", "0", "]", ".", "size", "(", "1", ")", "\n", "new_state_batch_size", "=", "final_states", "[", "0", "]", ".", "size", "(", "1", ")", "\n", "# Masks for the unused states of shape (1, new_batch_size, 1)", "\n", "used_new_rows_mask", "=", "[", "\n", "(", "state", "[", "0", ",", ":", ",", ":", "]", ".", "sum", "(", "-", "1", ")", "!=", "0.0", ")", ".", "float", "(", ")", ".", "view", "(", "1", ",", "new_state_batch_size", ",", "1", ")", "\n", "for", "state", "in", "new_unsorted_states", "\n", "]", "\n", "new_states", "=", "[", "]", "\n", "if", "current_state_batch_size", ">", "new_state_batch_size", ":", "\n", "# The new state is smaller than the old one,", "\n", "# so just update the indices which we used.", "\n", "                ", "for", "old_state", ",", "new_state", ",", "used_mask", "in", "zip", "(", "\n", "self", ".", "_states", ",", "new_unsorted_states", ",", "used_new_rows_mask", "\n", ")", ":", "\n", "# zero out all rows in the previous state", "\n", "# which _were_ used in the current state.", "\n", "                    ", "masked_old_state", "=", "old_state", "[", ":", ",", ":", "new_state_batch_size", ",", ":", "]", "*", "(", "\n", "1", "-", "used_mask", "\n", ")", "\n", "# The old state is larger, so update the relevant parts of it.", "\n", "old_state", "[", ":", ",", ":", "new_state_batch_size", ",", ":", "]", "=", "(", "\n", "new_state", "+", "masked_old_state", "\n", ")", "\n", "new_states", ".", "append", "(", "old_state", ".", "detach", "(", ")", ")", "\n", "", "", "else", ":", "\n", "# The states are the same size, so we just have to", "\n", "# deal with the possibility that some rows weren't used.", "\n", "                ", "new_states", "=", "[", "]", "\n", "for", "old_state", ",", "new_state", ",", "used_mask", "in", "zip", "(", "\n", "self", ".", "_states", ",", "new_unsorted_states", ",", "used_new_rows_mask", "\n", ")", ":", "\n", "# zero out all rows which _were_ used in the current state.", "\n", "                    ", "masked_old_state", "=", "old_state", "*", "(", "1", "-", "used_mask", ")", "\n", "# The old state is larger, so update the relevant parts of it.", "\n", "new_state", "+=", "masked_old_state", "\n", "new_states", ".", "append", "(", "new_state", ".", "detach", "(", ")", ")", "\n", "\n", "# It looks like there should be another case handled here - when", "\n", "# the current_state_batch_size < new_state_batch_size. However,", "\n", "# this never happens, because the states themeselves are mutated", "\n", "# by appending zeros when calling _get_inital_states, meaning that", "\n", "# the new states are either of equal size, or smaller, in the case", "\n", "# that there are some unused elements (zero-length) for the RNN computation.", "\n", "", "", "self", ".", "_states", "=", "tuple", "(", "new_states", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.modules.encoder_base._EncoderBase.reset_states": [[314, 344], ["mask.float().view.float().view.size", "mask.float().view.float().view.float().view", "tuple", "old_state.size", "new_states.append", "mask.float().view.float().view.float", "ValueError", "new_state.detach"], "methods", ["None"], ["", "", "def", "reset_states", "(", "self", ",", "mask", ":", "torch", ".", "Tensor", "=", "None", ")", "->", "None", ":", "\n", "        ", "\"\"\"\n        Resets the internal states of a stateful encoder.\n\n        # Parameters\n\n        mask : `torch.Tensor`, optional.\n            A tensor of shape `(batch_size,)` indicating which states should\n            be reset. If not provided, all states will be reset.\n        \"\"\"", "\n", "if", "mask", "is", "None", ":", "\n", "            ", "self", ".", "_states", "=", "None", "\n", "", "else", ":", "\n", "# state has shape (num_layers, batch_size, hidden_size). We reshape", "\n", "# mask to have shape (1, batch_size, 1) so that operations", "\n", "# broadcast properly.", "\n", "            ", "mask_batch_size", "=", "mask", ".", "size", "(", "0", ")", "\n", "mask", "=", "mask", ".", "float", "(", ")", ".", "view", "(", "1", ",", "mask_batch_size", ",", "1", ")", "\n", "new_states", "=", "[", "]", "\n", "for", "old_state", "in", "self", ".", "_states", ":", "\n", "                ", "old_state_batch_size", "=", "old_state", ".", "size", "(", "1", ")", "\n", "if", "old_state_batch_size", "!=", "mask_batch_size", ":", "\n", "                    ", "raise", "ValueError", "(", "\n", "f\"Trying to reset states using mask with incorrect batch size. \"", "\n", "f\"Expected batch size: {old_state_batch_size}. \"", "\n", "f\"Provided batch size: {mask_batch_size}.\"", "\n", ")", "\n", "", "new_state", "=", "(", "1", "-", "mask", ")", "*", "old_state", "\n", "new_states", ".", "append", "(", "new_state", ".", "detach", "(", ")", ")", "\n", "", "self", ".", "_states", "=", "tuple", "(", "new_states", ")", "\n", "", "", "", ""]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.modules.gated_sum.GatedSum.__init__": [[20, 27], ["torch.nn.Sigmoid", "super().__init__", "torch.nn.Linear"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.graph_util.plot.GraphPlot.__init__"], ["def", "__init__", "(", "\n", "self", ",", "input_dim", ":", "int", ",", "activation", ":", "Activation", "=", "torch", ".", "nn", ".", "Sigmoid", "(", ")", "\n", ")", "->", "None", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "input_dim", "=", "input_dim", "\n", "self", ".", "_gate", "=", "torch", ".", "nn", ".", "Linear", "(", "input_dim", "*", "2", ",", "1", ")", "\n", "self", ".", "_activation", "=", "activation", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.modules.gated_sum.GatedSum.get_input_dim": [[28, 30], ["None"], "methods", ["None"], ["", "def", "get_input_dim", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "input_dim", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.modules.gated_sum.GatedSum.get_output_dim": [[31, 33], ["None"], "methods", ["None"], ["", "def", "get_output_dim", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "input_dim", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.modules.gated_sum.GatedSum.forward": [[34, 41], ["gated_sum.GatedSum._activation", "input_a.size", "input_b.size", "ValueError", "input_a.size", "ValueError", "gated_sum.GatedSum._gate", "torch.cat"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "input_a", ":", "torch", ".", "Tensor", ",", "input_b", ":", "torch", ".", "Tensor", ")", "->", "torch", ".", "Tensor", ":", "\n", "        ", "if", "input_a", ".", "size", "(", ")", "!=", "input_b", ".", "size", "(", ")", ":", "\n", "            ", "raise", "ValueError", "(", "\"The input must have the same size.\"", ")", "\n", "", "if", "input_a", ".", "size", "(", "-", "1", ")", "!=", "self", ".", "input_dim", ":", "\n", "            ", "raise", "ValueError", "(", "\"Input size must match `input_dim`.\"", ")", "\n", "", "gate_value", "=", "self", ".", "_activation", "(", "self", ".", "_gate", "(", "torch", ".", "cat", "(", "[", "input_a", ",", "input_b", "]", ",", "-", "1", ")", ")", ")", "\n", "return", "gate_value", "*", "input_a", "+", "(", "1", "-", "gate_value", ")", "*", "input_b", "\n", "", "", ""]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.modules.elmo_lstm.ElmoLstm.__init__": [[58, 110], ["allennlp.modules.encoder_base._EncoderBase.__init__", "range", "allennlp.modules.lstm_cell_with_projection.LstmCellWithProjection", "allennlp.modules.lstm_cell_with_projection.LstmCellWithProjection", "elmo_lstm.ElmoLstm.add_module", "elmo_lstm.ElmoLstm.add_module", "forward_layers.append", "backward_layers.append"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.graph_util.plot.GraphPlot.__init__"], ["def", "__init__", "(", "\n", "self", ",", "\n", "input_size", ":", "int", ",", "\n", "hidden_size", ":", "int", ",", "\n", "cell_size", ":", "int", ",", "\n", "num_layers", ":", "int", ",", "\n", "requires_grad", ":", "bool", "=", "False", ",", "\n", "recurrent_dropout_probability", ":", "float", "=", "0.0", ",", "\n", "memory_cell_clip_value", ":", "Optional", "[", "float", "]", "=", "None", ",", "\n", "state_projection_clip_value", ":", "Optional", "[", "float", "]", "=", "None", ",", "\n", ")", "->", "None", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "stateful", "=", "True", ")", "\n", "\n", "# Required to be wrapped with a `PytorchSeq2SeqWrapper`.", "\n", "self", ".", "input_size", "=", "input_size", "\n", "self", ".", "hidden_size", "=", "hidden_size", "\n", "self", ".", "num_layers", "=", "num_layers", "\n", "self", ".", "cell_size", "=", "cell_size", "\n", "self", ".", "requires_grad", "=", "requires_grad", "\n", "\n", "forward_layers", "=", "[", "]", "\n", "backward_layers", "=", "[", "]", "\n", "\n", "lstm_input_size", "=", "input_size", "\n", "go_forward", "=", "True", "\n", "for", "layer_index", "in", "range", "(", "num_layers", ")", ":", "\n", "            ", "forward_layer", "=", "LstmCellWithProjection", "(", "\n", "lstm_input_size", ",", "\n", "hidden_size", ",", "\n", "cell_size", ",", "\n", "go_forward", ",", "\n", "recurrent_dropout_probability", ",", "\n", "memory_cell_clip_value", ",", "\n", "state_projection_clip_value", ",", "\n", ")", "\n", "backward_layer", "=", "LstmCellWithProjection", "(", "\n", "lstm_input_size", ",", "\n", "hidden_size", ",", "\n", "cell_size", ",", "\n", "not", "go_forward", ",", "\n", "recurrent_dropout_probability", ",", "\n", "memory_cell_clip_value", ",", "\n", "state_projection_clip_value", ",", "\n", ")", "\n", "lstm_input_size", "=", "hidden_size", "\n", "\n", "self", ".", "add_module", "(", "\"forward_layer_{}\"", ".", "format", "(", "layer_index", ")", ",", "forward_layer", ")", "\n", "self", ".", "add_module", "(", "\"backward_layer_{}\"", ".", "format", "(", "layer_index", ")", ",", "backward_layer", ")", "\n", "forward_layers", ".", "append", "(", "forward_layer", ")", "\n", "backward_layers", ".", "append", "(", "backward_layer", ")", "\n", "", "self", ".", "forward_layers", "=", "forward_layers", "\n", "self", ".", "backward_layers", "=", "backward_layers", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.modules.elmo_lstm.ElmoLstm.forward": [[111, 173], ["mask.size", "elmo_lstm.ElmoLstm.sort_and_run_forward", "torch.cat.size", "elmo_lstm.ElmoLstm._update_states", "torch.cat.index_select", "torch.cat.new_zeros", "torch.cat", "torch.cat.new_zeros", "torch.cat", "state.size", "state.new_zeros", "new_states.append", "stacked_sequence_output[].size", "torch.cat"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.modules.encoder_base._EncoderBase.sort_and_run_forward", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.modules.encoder_base._EncoderBase._update_states"], ["", "def", "forward", "(", "self", ",", "inputs", ":", "torch", ".", "Tensor", ",", "mask", ":", "torch", ".", "LongTensor", ")", "->", "torch", ".", "Tensor", ":", "\n", "        ", "\"\"\"\n        # Parameters\n\n        inputs : `torch.Tensor`, required.\n            A Tensor of shape `(batch_size, sequence_length, hidden_size)`.\n        mask : `torch.LongTensor`, required.\n            A binary mask of shape `(batch_size, sequence_length)` representing the\n            non-padded elements in each sequence in the batch.\n\n        # Returns\n\n        A `torch.Tensor` of shape (num_layers, batch_size, sequence_length, hidden_size),\n        where the num_layers dimension represents the LSTM output from that layer.\n        \"\"\"", "\n", "batch_size", ",", "total_sequence_length", "=", "mask", ".", "size", "(", ")", "\n", "(", "\n", "stacked_sequence_output", ",", "\n", "final_states", ",", "\n", "restoration_indices", ",", "\n", ")", "=", "self", ".", "sort_and_run_forward", "(", "self", ".", "_lstm_forward", ",", "inputs", ",", "mask", ")", "\n", "\n", "(", "\n", "num_layers", ",", "\n", "num_valid", ",", "\n", "returned_timesteps", ",", "\n", "encoder_dim", ",", "\n", ")", "=", "stacked_sequence_output", ".", "size", "(", ")", "\n", "# Add back invalid rows which were removed in the call to sort_and_run_forward.", "\n", "if", "num_valid", "<", "batch_size", ":", "\n", "            ", "zeros", "=", "stacked_sequence_output", ".", "new_zeros", "(", "\n", "num_layers", ",", "batch_size", "-", "num_valid", ",", "returned_timesteps", ",", "encoder_dim", "\n", ")", "\n", "stacked_sequence_output", "=", "torch", ".", "cat", "(", "[", "stacked_sequence_output", ",", "zeros", "]", ",", "1", ")", "\n", "\n", "# The states also need to have invalid rows added back.", "\n", "new_states", "=", "[", "]", "\n", "for", "state", "in", "final_states", ":", "\n", "                ", "state_dim", "=", "state", ".", "size", "(", "-", "1", ")", "\n", "zeros", "=", "state", ".", "new_zeros", "(", "num_layers", ",", "batch_size", "-", "num_valid", ",", "state_dim", ")", "\n", "new_states", ".", "append", "(", "torch", ".", "cat", "(", "[", "state", ",", "zeros", "]", ",", "1", ")", ")", "\n", "", "final_states", "=", "new_states", "\n", "\n", "# It's possible to need to pass sequences which are padded to longer than the", "\n", "# max length of the sequence to a Seq2StackEncoder. However, packing and unpacking", "\n", "# the sequences mean that the returned tensor won't include these dimensions, because", "\n", "# the RNN did not need to process them. We add them back on in the form of zeros here.", "\n", "", "sequence_length_difference", "=", "total_sequence_length", "-", "returned_timesteps", "\n", "if", "sequence_length_difference", ">", "0", ":", "\n", "            ", "zeros", "=", "stacked_sequence_output", ".", "new_zeros", "(", "\n", "num_layers", ",", "\n", "batch_size", ",", "\n", "sequence_length_difference", ",", "\n", "stacked_sequence_output", "[", "0", "]", ".", "size", "(", "-", "1", ")", ",", "\n", ")", "\n", "stacked_sequence_output", "=", "torch", ".", "cat", "(", "[", "stacked_sequence_output", ",", "zeros", "]", ",", "2", ")", "\n", "\n", "", "self", ".", "_update_states", "(", "final_states", ",", "restoration_indices", ")", "\n", "\n", "# Restore the original indices and return the sequence.", "\n", "# Has shape (num_layers, batch_size, sequence_length, hidden_size)", "\n", "return", "stacked_sequence_output", ".", "index_select", "(", "1", ",", "restoration_indices", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.modules.elmo_lstm.ElmoLstm._lstm_forward": [[174, 272], ["torch.nn.utils.rnn.pad_packed_sequence", "enumerate", "torch.stack", "zip", "getattr", "getattr", "getattr.", "getattr.", "sequence_outputs.append", "final_states.append", "torch.cat", "torch.cat", "len", "len", "allennlp.common.checks.ConfigurationError", "list", "state[].split", "state[].split", "torch.cat", "initial_state[].size", "zip", "torch.cat", "torch.cat", "initial_state[].split", "initial_state[].split"], "methods", ["None"], ["", "def", "_lstm_forward", "(", "\n", "self", ",", "\n", "inputs", ":", "PackedSequence", ",", "\n", "initial_state", ":", "Optional", "[", "Tuple", "[", "torch", ".", "Tensor", ",", "torch", ".", "Tensor", "]", "]", "=", "None", ",", "\n", ")", "->", "Tuple", "[", "torch", ".", "Tensor", ",", "Tuple", "[", "torch", ".", "Tensor", ",", "torch", ".", "Tensor", "]", "]", ":", "\n", "        ", "\"\"\"\n        # Parameters\n\n        inputs : `PackedSequence`, required.\n            A batch first `PackedSequence` to run the stacked LSTM over.\n        initial_state : `Tuple[torch.Tensor, torch.Tensor]`, optional, (default = None)\n            A tuple (state, memory) representing the initial hidden state and memory\n            of the LSTM, with shape (num_layers, batch_size, 2 * hidden_size) and\n            (num_layers, batch_size, 2 * cell_size) respectively.\n\n        # Returns\n\n        output_sequence : `torch.FloatTensor`\n            The encoded sequence of shape (num_layers, batch_size, sequence_length, hidden_size)\n        final_states : `Tuple[torch.FloatTensor, torch.FloatTensor]`\n            The per-layer final (state, memory) states of the LSTM, with shape\n            (num_layers, batch_size, 2 * hidden_size) and  (num_layers, batch_size, 2 * cell_size)\n            respectively. The last dimension is duplicated because it contains the state/memory\n            for both the forward and backward layers.\n        \"\"\"", "\n", "if", "initial_state", "is", "None", ":", "\n", "            ", "hidden_states", ":", "List", "[", "Optional", "[", "Tuple", "[", "torch", ".", "Tensor", ",", "torch", ".", "Tensor", "]", "]", "]", "=", "[", "\n", "None", "\n", "]", "*", "len", "(", "self", ".", "forward_layers", ")", "\n", "", "elif", "initial_state", "[", "0", "]", ".", "size", "(", ")", "[", "0", "]", "!=", "len", "(", "self", ".", "forward_layers", ")", ":", "\n", "            ", "raise", "ConfigurationError", "(", "\n", "\"Initial states were passed to forward() but the number of \"", "\n", "\"initial states does not match the number of layers.\"", "\n", ")", "\n", "", "else", ":", "\n", "            ", "hidden_states", "=", "list", "(", "\n", "zip", "(", "initial_state", "[", "0", "]", ".", "split", "(", "1", ",", "0", ")", ",", "initial_state", "[", "1", "]", ".", "split", "(", "1", ",", "0", ")", ")", "\n", ")", "\n", "\n", "", "inputs", ",", "batch_lengths", "=", "pad_packed_sequence", "(", "inputs", ",", "batch_first", "=", "True", ")", "\n", "forward_output_sequence", "=", "inputs", "\n", "backward_output_sequence", "=", "inputs", "\n", "\n", "final_states", "=", "[", "]", "\n", "sequence_outputs", "=", "[", "]", "\n", "for", "layer_index", ",", "state", "in", "enumerate", "(", "hidden_states", ")", ":", "\n", "            ", "forward_layer", "=", "getattr", "(", "self", ",", "\"forward_layer_{}\"", ".", "format", "(", "layer_index", ")", ")", "\n", "backward_layer", "=", "getattr", "(", "self", ",", "\"backward_layer_{}\"", ".", "format", "(", "layer_index", ")", ")", "\n", "\n", "forward_cache", "=", "forward_output_sequence", "\n", "backward_cache", "=", "backward_output_sequence", "\n", "\n", "if", "state", "is", "not", "None", ":", "\n", "                ", "forward_hidden_state", ",", "backward_hidden_state", "=", "state", "[", "0", "]", ".", "split", "(", "\n", "self", ".", "hidden_size", ",", "2", "\n", ")", "\n", "forward_memory_state", ",", "backward_memory_state", "=", "state", "[", "1", "]", ".", "split", "(", "\n", "self", ".", "cell_size", ",", "2", "\n", ")", "\n", "forward_state", "=", "(", "forward_hidden_state", ",", "forward_memory_state", ")", "\n", "backward_state", "=", "(", "backward_hidden_state", ",", "backward_memory_state", ")", "\n", "", "else", ":", "\n", "                ", "forward_state", "=", "None", "\n", "backward_state", "=", "None", "\n", "\n", "", "forward_output_sequence", ",", "forward_state", "=", "forward_layer", "(", "\n", "forward_output_sequence", ",", "batch_lengths", ",", "forward_state", "\n", ")", "\n", "backward_output_sequence", ",", "backward_state", "=", "backward_layer", "(", "\n", "backward_output_sequence", ",", "batch_lengths", ",", "backward_state", "\n", ")", "\n", "# Skip connections, just adding the input to the output.", "\n", "if", "layer_index", "!=", "0", ":", "\n", "                ", "forward_output_sequence", "+=", "forward_cache", "\n", "backward_output_sequence", "+=", "backward_cache", "\n", "\n", "", "sequence_outputs", ".", "append", "(", "\n", "torch", ".", "cat", "(", "[", "forward_output_sequence", ",", "backward_output_sequence", "]", ",", "-", "1", ")", "\n", ")", "\n", "# Append the state tuples in a list, so that we can return", "\n", "# the final states for all the layers.", "\n", "final_states", ".", "append", "(", "\n", "(", "\n", "torch", ".", "cat", "(", "[", "forward_state", "[", "0", "]", ",", "backward_state", "[", "0", "]", "]", ",", "-", "1", ")", ",", "\n", "torch", ".", "cat", "(", "[", "forward_state", "[", "1", "]", ",", "backward_state", "[", "1", "]", "]", ",", "-", "1", ")", ",", "\n", ")", "\n", ")", "\n", "\n", "", "stacked_sequence_outputs", ":", "torch", ".", "FloatTensor", "=", "torch", ".", "stack", "(", "sequence_outputs", ")", "\n", "# Stack the hidden state and memory for each layer into 2 tensors of shape", "\n", "# (num_layers, batch_size, hidden_size) and (num_layers, batch_size, cell_size)", "\n", "# respectively.", "\n", "final_hidden_states", ",", "final_memory_states", "=", "zip", "(", "*", "final_states", ")", "\n", "final_state_tuple", ":", "Tuple", "[", "torch", ".", "FloatTensor", ",", "torch", ".", "FloatTensor", "]", "=", "(", "\n", "torch", ".", "cat", "(", "final_hidden_states", ",", "0", ")", ",", "\n", "torch", ".", "cat", "(", "final_memory_states", ",", "0", ")", ",", "\n", ")", "\n", "return", "stacked_sequence_outputs", ",", "final_state_tuple", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.modules.elmo_lstm.ElmoLstm.load_weights": [[273, 347], ["h5py.File", "enumerate", "allennlp.common.file_utils.cached_path", "zip", "enumerate", "numpy.transpose", "numpy.transpose.copy", "lstm.input_linearity.weight.data.copy_", "lstm.state_linearity.weight.data.copy_", "tf_bias.copy", "lstm.state_linearity.bias.data.copy_", "numpy.transpose", "lstm.state_projection.weight.data.copy_", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.file_utils.cached_path"], ["", "def", "load_weights", "(", "self", ",", "weight_file", ":", "str", ")", "->", "None", ":", "\n", "        ", "\"\"\"\n        Load the pre-trained weights from the file.\n        \"\"\"", "\n", "requires_grad", "=", "self", ".", "requires_grad", "\n", "\n", "with", "h5py", ".", "File", "(", "cached_path", "(", "weight_file", ")", ",", "\"r\"", ")", "as", "fin", ":", "\n", "            ", "for", "i_layer", ",", "lstms", "in", "enumerate", "(", "\n", "zip", "(", "self", ".", "forward_layers", ",", "self", ".", "backward_layers", ")", "\n", ")", ":", "\n", "                ", "for", "j_direction", ",", "lstm", "in", "enumerate", "(", "lstms", ")", ":", "\n", "# lstm is an instance of LSTMCellWithProjection", "\n", "                    ", "cell_size", "=", "lstm", ".", "cell_size", "\n", "\n", "dataset", "=", "fin", "[", "\"RNN_%s\"", "%", "j_direction", "]", "[", "\"RNN\"", "]", "[", "\"MultiRNNCell\"", "]", "[", "\n", "\"Cell%s\"", "%", "i_layer", "\n", "]", "[", "\"LSTMCell\"", "]", "\n", "\n", "# tensorflow packs together both W and U matrices into one matrix,", "\n", "# but pytorch maintains individual matrices.  In addition, tensorflow", "\n", "# packs the gates as input, memory, forget, output but pytorch", "\n", "# uses input, forget, memory, output.  So we need to modify the weights.", "\n", "tf_weights", "=", "numpy", ".", "transpose", "(", "dataset", "[", "\"W_0\"", "]", "[", "...", "]", ")", "\n", "torch_weights", "=", "tf_weights", ".", "copy", "(", ")", "\n", "\n", "# split the W from U matrices", "\n", "input_size", "=", "lstm", ".", "input_size", "\n", "input_weights", "=", "torch_weights", "[", ":", ",", ":", "input_size", "]", "\n", "recurrent_weights", "=", "torch_weights", "[", ":", ",", "input_size", ":", "]", "\n", "tf_input_weights", "=", "tf_weights", "[", ":", ",", ":", "input_size", "]", "\n", "tf_recurrent_weights", "=", "tf_weights", "[", ":", ",", "input_size", ":", "]", "\n", "\n", "# handle the different gate order convention", "\n", "for", "torch_w", ",", "tf_w", "in", "[", "\n", "[", "input_weights", ",", "tf_input_weights", "]", ",", "\n", "[", "recurrent_weights", ",", "tf_recurrent_weights", "]", ",", "\n", "]", ":", "\n", "                        ", "torch_w", "[", "(", "1", "*", "cell_size", ")", ":", "(", "2", "*", "cell_size", ")", ",", ":", "]", "=", "tf_w", "[", "\n", "(", "2", "*", "cell_size", ")", ":", "(", "3", "*", "cell_size", ")", ",", ":", "\n", "]", "\n", "torch_w", "[", "(", "2", "*", "cell_size", ")", ":", "(", "3", "*", "cell_size", ")", ",", ":", "]", "=", "tf_w", "[", "\n", "(", "1", "*", "cell_size", ")", ":", "(", "2", "*", "cell_size", ")", ",", ":", "\n", "]", "\n", "\n", "", "lstm", ".", "input_linearity", ".", "weight", ".", "data", ".", "copy_", "(", "\n", "torch", ".", "FloatTensor", "(", "input_weights", ")", "\n", ")", "\n", "lstm", ".", "state_linearity", ".", "weight", ".", "data", ".", "copy_", "(", "\n", "torch", ".", "FloatTensor", "(", "recurrent_weights", ")", "\n", ")", "\n", "lstm", ".", "input_linearity", ".", "weight", ".", "requires_grad", "=", "requires_grad", "\n", "lstm", ".", "state_linearity", ".", "weight", ".", "requires_grad", "=", "requires_grad", "\n", "\n", "# the bias weights", "\n", "tf_bias", "=", "dataset", "[", "\"B\"", "]", "[", "...", "]", "\n", "# tensorflow adds 1.0 to forget gate bias instead of modifying the", "\n", "# parameters...", "\n", "tf_bias", "[", "(", "2", "*", "cell_size", ")", ":", "(", "3", "*", "cell_size", ")", "]", "+=", "1", "\n", "torch_bias", "=", "tf_bias", ".", "copy", "(", ")", "\n", "torch_bias", "[", "(", "1", "*", "cell_size", ")", ":", "(", "2", "*", "cell_size", ")", "]", "=", "tf_bias", "[", "\n", "(", "2", "*", "cell_size", ")", ":", "(", "3", "*", "cell_size", ")", "\n", "]", "\n", "torch_bias", "[", "(", "2", "*", "cell_size", ")", ":", "(", "3", "*", "cell_size", ")", "]", "=", "tf_bias", "[", "\n", "(", "1", "*", "cell_size", ")", ":", "(", "2", "*", "cell_size", ")", "\n", "]", "\n", "lstm", ".", "state_linearity", ".", "bias", ".", "data", ".", "copy_", "(", "torch", ".", "FloatTensor", "(", "torch_bias", ")", ")", "\n", "lstm", ".", "state_linearity", ".", "bias", ".", "requires_grad", "=", "requires_grad", "\n", "\n", "# the projection weights", "\n", "proj_weights", "=", "numpy", ".", "transpose", "(", "dataset", "[", "\"W_P_0\"", "]", "[", "...", "]", ")", "\n", "lstm", ".", "state_projection", ".", "weight", ".", "data", ".", "copy_", "(", "\n", "torch", ".", "FloatTensor", "(", "proj_weights", ")", "\n", ")", "\n", "lstm", ".", "state_projection", ".", "weight", ".", "requires_grad", "=", "requires_grad", "\n", "", "", "", "", "", ""]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.modules.sampled_softmax_loss.SampledSoftmaxLoss.__init__": [[74, 123], ["super().__init__", "sampled_softmax_loss.SampledSoftmaxLoss.initialize_num_words", "allennlp.common.checks.ConfigurationError", "torch.nn.Embedding", "sampled_softmax_loss.SampledSoftmaxLoss.softmax_w.weight.data.normal_", "torch.nn.Embedding", "sampled_softmax_loss.SampledSoftmaxLoss.softmax_b.weight.data.fill_", "torch.nn.Parameter", "torch.nn.Parameter", "torch.zeros", "torch.randn", "numpy.sqrt", "numpy.sqrt"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.graph_util.plot.GraphPlot.__init__", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.modules.sampled_softmax_loss.SampledSoftmaxLoss.initialize_num_words"], ["def", "__init__", "(", "\n", "self", ",", "\n", "num_words", ":", "int", ",", "\n", "embedding_dim", ":", "int", ",", "\n", "num_samples", ":", "int", ",", "\n", "sparse", ":", "bool", "=", "False", ",", "\n", "unk_id", ":", "int", "=", "None", ",", "\n", "use_character_inputs", ":", "bool", "=", "True", ",", "\n", "use_fast_sampler", ":", "bool", "=", "False", ",", "\n", ")", "->", "None", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "\n", "# TODO(joelgrus): implement tie_embeddings (maybe)", "\n", "self", ".", "tie_embeddings", "=", "False", "\n", "\n", "assert", "num_samples", "<", "num_words", "\n", "\n", "if", "use_fast_sampler", ":", "\n", "            ", "raise", "ConfigurationError", "(", "\"fast sampler is not implemented\"", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "choice_func", "=", "_choice", "\n", "\n", "# Glorit init (std=(1.0 / sqrt(fan_in))", "\n", "", "if", "sparse", ":", "\n", "# create our own sparse embedding", "\n", "            ", "self", ".", "softmax_w", "=", "torch", ".", "nn", ".", "Embedding", "(", "num_words", ",", "embedding_dim", ",", "sparse", "=", "True", ")", "\n", "self", ".", "softmax_w", ".", "weight", ".", "data", ".", "normal_", "(", "\n", "mean", "=", "0.0", ",", "std", "=", "1.0", "/", "np", ".", "sqrt", "(", "embedding_dim", ")", "\n", ")", "\n", "self", ".", "softmax_b", "=", "torch", ".", "nn", ".", "Embedding", "(", "num_words", ",", "1", ",", "sparse", "=", "True", ")", "\n", "self", ".", "softmax_b", ".", "weight", ".", "data", ".", "fill_", "(", "0.0", ")", "\n", "", "else", ":", "\n", "# just create tensors to use as the embeddings", "\n", "# Glorit init (std=(1.0 / sqrt(fan_in))", "\n", "            ", "self", ".", "softmax_w", "=", "torch", ".", "nn", ".", "Parameter", "(", "\n", "torch", ".", "randn", "(", "num_words", ",", "embedding_dim", ")", "/", "np", ".", "sqrt", "(", "embedding_dim", ")", "\n", ")", "\n", "self", ".", "softmax_b", "=", "torch", ".", "nn", ".", "Parameter", "(", "torch", ".", "zeros", "(", "num_words", ")", ")", "\n", "\n", "", "self", ".", "sparse", "=", "sparse", "\n", "self", ".", "use_character_inputs", "=", "use_character_inputs", "\n", "\n", "if", "use_character_inputs", ":", "\n", "            ", "self", ".", "_unk_id", "=", "unk_id", "\n", "\n", "", "self", ".", "_num_samples", "=", "num_samples", "\n", "self", ".", "_embedding_dim", "=", "embedding_dim", "\n", "self", ".", "_num_words", "=", "num_words", "\n", "self", ".", "initialize_num_words", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.modules.sampled_softmax_loss.SampledSoftmaxLoss.initialize_num_words": [[124, 137], ["numpy.log", "sampled_softmax_loss.SampledSoftmaxLoss.softmax_w.weight.size", "sampled_softmax_loss.SampledSoftmaxLoss.softmax_w.size", "numpy.log", "numpy.log", "numpy.arange", "numpy.arange"], "methods", ["None"], ["", "def", "initialize_num_words", "(", "self", ")", ":", "\n", "        ", "if", "self", ".", "sparse", ":", "\n", "            ", "num_words", "=", "self", ".", "softmax_w", ".", "weight", ".", "size", "(", "0", ")", "\n", "", "else", ":", "\n", "            ", "num_words", "=", "self", ".", "softmax_w", ".", "size", "(", "0", ")", "\n", "\n", "", "self", ".", "_num_words", "=", "num_words", "\n", "self", ".", "_log_num_words_p1", "=", "np", ".", "log", "(", "num_words", "+", "1", ")", "\n", "\n", "# compute the probability of each sampled id", "\n", "self", ".", "_probs", "=", "(", "\n", "np", ".", "log", "(", "np", ".", "arange", "(", "num_words", ")", "+", "2", ")", "-", "np", ".", "log", "(", "np", ".", "arange", "(", "num_words", ")", "+", "1", ")", "\n", ")", "/", "self", ".", "_log_num_words_p1", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.modules.sampled_softmax_loss.SampledSoftmaxLoss.forward": [[138, 159], ["torch.tensor().to", "sampled_softmax_loss.SampledSoftmaxLoss._forward_eval", "sampled_softmax_loss.SampledSoftmaxLoss._forward_train", "torch.tensor"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.modules.sampled_softmax_loss.SampledSoftmaxLoss._forward_eval", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.modules.sampled_softmax_loss.SampledSoftmaxLoss._forward_train"], ["", "def", "forward", "(", "\n", "self", ",", "\n", "embeddings", ":", "torch", ".", "Tensor", ",", "\n", "targets", ":", "torch", ".", "Tensor", ",", "\n", "target_token_embedding", ":", "torch", ".", "Tensor", "=", "None", ",", "\n", ")", "->", "torch", ".", "Tensor", ":", "\n", "# embeddings is size (n, embedding_dim)", "\n", "# targets is (n_words, ) with the index of the actual target", "\n", "# when tieing weights, target_token_embedding is required.", "\n", "# it is size (n_words, embedding_dim)", "\n", "# returns log likelihood loss (batch_size, )", "\n", "# Does not do any count normalization / divide by batch size", "\n", "\n", "        ", "if", "embeddings", ".", "shape", "[", "0", "]", "==", "0", ":", "\n", "# empty batch", "\n", "            ", "return", "torch", ".", "tensor", "(", "0.0", ")", ".", "to", "(", "embeddings", ".", "device", ")", "\n", "\n", "", "if", "not", "self", ".", "training", ":", "\n", "            ", "return", "self", ".", "_forward_eval", "(", "embeddings", ",", "targets", ")", "\n", "", "else", ":", "\n", "            ", "return", "self", ".", "_forward_train", "(", "embeddings", ",", "targets", ",", "target_token_embedding", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.modules.sampled_softmax_loss.SampledSoftmaxLoss._forward_train": [[160, 242], ["sampled_softmax_loss.SampledSoftmaxLoss.log_uniform_candidate_sampler", "targets.long", "targets.long.requires_grad_", "torch.cat", "targets.long.size", "sampled_logits.masked_fill", "torch.cat", "torch.nn.functional.log_softmax", "torch.cat.unsqueeze", "sampled_softmax_loss.SampledSoftmaxLoss.softmax_w().squeeze", "sampled_softmax_loss.SampledSoftmaxLoss.softmax_b().squeeze().squeeze", "torch.nn.functional.embedding", "torch.nn.functional.embedding().squeeze", "torch.log", "torch.log", "targets.long.unsqueeze", "log_softmax[].sum", "torch.matmul", "true_logits.unsqueeze", "sampled_softmax_loss.SampledSoftmaxLoss.softmax_w", "sampled_softmax_loss.SampledSoftmaxLoss.softmax_b().squeeze", "torch.nn.functional.embedding", "sampled_w.t", "sampled_softmax_loss.SampledSoftmaxLoss.softmax_b.unsqueeze", "sampled_softmax_loss.SampledSoftmaxLoss.softmax_b"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.modules.sampled_softmax_loss.SampledSoftmaxLoss.log_uniform_candidate_sampler"], ["", "", "def", "_forward_train", "(", "\n", "self", ",", "\n", "embeddings", ":", "torch", ".", "Tensor", ",", "\n", "targets", ":", "torch", ".", "Tensor", ",", "\n", "target_token_embedding", ":", "torch", ".", "Tensor", ",", "\n", ")", "->", "torch", ".", "Tensor", ":", "\n", "\n", "# (target_token_embedding is only used in the tie_embeddings case,", "\n", "#  which is not implemented)", "\n", "\n", "# want to compute (n, n_samples + 1) array with the log", "\n", "# probabilities where the first index is the true target", "\n", "# and the remaining ones are the the negative samples.", "\n", "# then we can just select the first column", "\n", "\n", "# NOTE: targets input has padding removed (so 0 == the first id, NOT the padding id)", "\n", "\n", "        ", "(", "\n", "sampled_ids", ",", "\n", "target_expected_count", ",", "\n", "sampled_expected_count", ",", "\n", ")", "=", "self", ".", "log_uniform_candidate_sampler", "(", "targets", ",", "choice_func", "=", "self", ".", "choice_func", ")", "\n", "\n", "long_targets", "=", "targets", ".", "long", "(", ")", "\n", "long_targets", ".", "requires_grad_", "(", "False", ")", "\n", "\n", "# Get the softmax weights (so we can compute logits)", "\n", "# shape (batch_size * max_sequence_length + num_samples)", "\n", "all_ids", "=", "torch", ".", "cat", "(", "[", "long_targets", ",", "sampled_ids", "]", ",", "dim", "=", "0", ")", "\n", "\n", "if", "self", ".", "sparse", ":", "\n", "            ", "all_ids_1", "=", "all_ids", ".", "unsqueeze", "(", "1", ")", "\n", "all_w", "=", "self", ".", "softmax_w", "(", "all_ids_1", ")", ".", "squeeze", "(", "1", ")", "\n", "all_b", "=", "self", ".", "softmax_b", "(", "all_ids_1", ")", ".", "squeeze", "(", "2", ")", ".", "squeeze", "(", "1", ")", "\n", "", "else", ":", "\n", "            ", "all_w", "=", "torch", ".", "nn", ".", "functional", ".", "embedding", "(", "all_ids", ",", "self", ".", "softmax_w", ")", "\n", "# the unsqueeze / squeeze works around an issue with 1 dim", "\n", "# embeddings", "\n", "all_b", "=", "torch", ".", "nn", ".", "functional", ".", "embedding", "(", "\n", "all_ids", ",", "self", ".", "softmax_b", ".", "unsqueeze", "(", "1", ")", "\n", ")", ".", "squeeze", "(", "1", ")", "\n", "\n", "", "batch_size", "=", "long_targets", ".", "size", "(", "0", ")", "\n", "true_w", "=", "all_w", "[", ":", "batch_size", ",", ":", "]", "\n", "sampled_w", "=", "all_w", "[", "batch_size", ":", ",", ":", "]", "\n", "true_b", "=", "all_b", "[", ":", "batch_size", "]", "\n", "sampled_b", "=", "all_b", "[", "batch_size", ":", "]", "\n", "\n", "# compute the logits and remove log expected counts", "\n", "# [batch_size, ]", "\n", "true_logits", "=", "(", "\n", "(", "true_w", "*", "embeddings", ")", ".", "sum", "(", "dim", "=", "1", ")", "\n", "+", "true_b", "\n", "-", "torch", ".", "log", "(", "target_expected_count", "+", "1e-7", ")", "\n", ")", "\n", "# [batch_size, n_samples]", "\n", "sampled_logits", "=", "(", "\n", "torch", ".", "matmul", "(", "embeddings", ",", "sampled_w", ".", "t", "(", ")", ")", "\n", "+", "sampled_b", "\n", "-", "torch", ".", "log", "(", "sampled_expected_count", "+", "1e-7", ")", "\n", ")", "\n", "\n", "# remove true labels -- we will take", "\n", "# softmax, so set the sampled logits of true values to a large", "\n", "# negative number", "\n", "# [batch_size, n_samples]", "\n", "true_in_sample_mask", "=", "sampled_ids", "==", "long_targets", ".", "unsqueeze", "(", "1", ")", "\n", "masked_sampled_logits", "=", "sampled_logits", ".", "masked_fill", "(", "\n", "true_in_sample_mask", ",", "-", "10000.0", "\n", ")", "\n", "# now concat the true logits as index 0", "\n", "# [batch_size, n_samples + 1]", "\n", "logits", "=", "torch", ".", "cat", "(", "[", "true_logits", ".", "unsqueeze", "(", "1", ")", ",", "masked_sampled_logits", "]", ",", "dim", "=", "1", ")", "\n", "\n", "# finally take log_softmax", "\n", "log_softmax", "=", "torch", ".", "nn", ".", "functional", ".", "log_softmax", "(", "logits", ",", "dim", "=", "1", ")", "\n", "# true log likelihood is index 0, loss = -1.0 * sum over batch", "\n", "# the likelihood loss can become very large if the corresponding", "\n", "# true logit is very small, so we apply a per-target cap here", "\n", "# so that a single logit for a very rare word won't dominate the batch.", "\n", "nll_loss", "=", "-", "1.0", "*", "log_softmax", "[", ":", ",", "0", "]", ".", "sum", "(", ")", "\n", "return", "nll_loss", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.modules.sampled_softmax_loss.SampledSoftmaxLoss._forward_eval": [[243, 264], ["torch.nn.functional.log_softmax", "torch.nn.functional.nll_loss", "sampled_softmax_loss.SampledSoftmaxLoss.softmax_b.weight.squeeze", "targets_.long", "torch.matmul", "w.t"], "methods", ["None"], ["", "def", "_forward_eval", "(", "\n", "self", ",", "embeddings", ":", "torch", ".", "Tensor", ",", "targets", ":", "torch", ".", "Tensor", "\n", ")", "->", "torch", ".", "Tensor", ":", "\n", "\n", "# evaluation mode, use full softmax", "\n", "        ", "if", "self", ".", "sparse", ":", "\n", "            ", "w", "=", "self", ".", "softmax_w", ".", "weight", "\n", "b", "=", "self", ".", "softmax_b", ".", "weight", ".", "squeeze", "(", "1", ")", "\n", "", "else", ":", "\n", "            ", "w", "=", "self", ".", "softmax_w", "\n", "b", "=", "self", ".", "softmax_b", "\n", "\n", "", "log_softmax", "=", "torch", ".", "nn", ".", "functional", ".", "log_softmax", "(", "\n", "torch", ".", "matmul", "(", "embeddings", ",", "w", ".", "t", "(", ")", ")", "+", "b", ",", "dim", "=", "-", "1", "\n", ")", "\n", "if", "self", ".", "tie_embeddings", "and", "not", "self", ".", "use_character_inputs", ":", "\n", "            ", "targets_", "=", "targets", "+", "1", "\n", "", "else", ":", "\n", "            ", "targets_", "=", "targets", "\n", "", "return", "torch", ".", "nn", ".", "functional", ".", "nll_loss", "(", "\n", "log_softmax", ",", "targets_", ".", "long", "(", ")", ",", "reduction", "=", "\"sum\"", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.modules.sampled_softmax_loss.SampledSoftmaxLoss.log_uniform_candidate_sampler": [[266, 308], ["choice_func", "torch.from_numpy().to", "torch.from_numpy().to.requires_grad_", "target_expected_count.requires_grad_", "sampled_expected_count.requires_grad_", "torch.log", "torch.log", "torch.from_numpy", "torch.exp", "torch.exp", "targets.float", "targets.float", "torch.log1p", "torch.from_numpy().to.float", "torch.from_numpy().to.float", "torch.log1p"], "methods", ["None"], ["", "def", "log_uniform_candidate_sampler", "(", "self", ",", "targets", ",", "choice_func", "=", "_choice", ")", ":", "\n", "# returns sampled, true_expected_count, sampled_expected_count", "\n", "# targets = (batch_size, )", "\n", "#", "\n", "#  samples = (n_samples, )", "\n", "#  true_expected_count = (batch_size, )", "\n", "#  sampled_expected_count = (n_samples, )", "\n", "\n", "# see: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/kernels/range_sampler.h", "\n", "# https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/kernels/range_sampler.cc", "\n", "\n", "# algorithm: keep track of number of tries when doing sampling,", "\n", "#   then expected count is", "\n", "#   -expm1(num_tries * log1p(-p))", "\n", "# = (1 - (1-p)^num_tries) where p is self._probs[id]", "\n", "\n", "        ", "np_sampled_ids", ",", "num_tries", "=", "choice_func", "(", "self", ".", "_num_words", ",", "self", ".", "_num_samples", ")", "\n", "\n", "sampled_ids", "=", "torch", ".", "from_numpy", "(", "np_sampled_ids", ")", ".", "to", "(", "targets", ".", "device", ")", "\n", "\n", "# Compute expected count = (1 - (1-p)^num_tries) = -expm1(num_tries * log1p(-p))", "\n", "# P(class) = (log(class + 2) - log(class + 1)) / log(range_max + 1)", "\n", "target_probs", "=", "(", "\n", "torch", ".", "log", "(", "(", "targets", ".", "float", "(", ")", "+", "2.0", ")", "/", "(", "targets", ".", "float", "(", ")", "+", "1.0", ")", ")", "\n", "/", "self", ".", "_log_num_words_p1", "\n", ")", "\n", "target_expected_count", "=", "-", "1.0", "*", "(", "\n", "torch", ".", "exp", "(", "num_tries", "*", "torch", ".", "log1p", "(", "-", "target_probs", ")", ")", "-", "1.0", "\n", ")", "\n", "sampled_probs", "=", "(", "\n", "torch", ".", "log", "(", "(", "sampled_ids", ".", "float", "(", ")", "+", "2.0", ")", "/", "(", "sampled_ids", ".", "float", "(", ")", "+", "1.0", ")", ")", "\n", "/", "self", ".", "_log_num_words_p1", "\n", ")", "\n", "sampled_expected_count", "=", "-", "1.0", "*", "(", "\n", "torch", ".", "exp", "(", "num_tries", "*", "torch", ".", "log1p", "(", "-", "sampled_probs", ")", ")", "-", "1.0", "\n", ")", "\n", "\n", "sampled_ids", ".", "requires_grad_", "(", "False", ")", "\n", "target_expected_count", ".", "requires_grad_", "(", "False", ")", "\n", "sampled_expected_count", ".", "requires_grad_", "(", "False", ")", "\n", "\n", "return", "sampled_ids", ",", "target_expected_count", ",", "sampled_expected_count", "\n", "", "", ""]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.modules.sampled_softmax_loss._choice": [[11, 43], ["sampled_softmax_loss._choice.get_buffer"], "function", ["None"], ["def", "_choice", "(", "num_words", ":", "int", ",", "num_samples", ":", "int", ")", "->", "Tuple", "[", "np", ".", "ndarray", ",", "int", "]", ":", "\n", "    ", "\"\"\"\n    Chooses `num_samples` samples without replacement from [0, ..., num_words).\n    Returns a tuple (samples, num_tries).\n    \"\"\"", "\n", "num_tries", "=", "0", "\n", "num_chosen", "=", "0", "\n", "\n", "def", "get_buffer", "(", ")", "->", "np", ".", "ndarray", ":", "\n", "        ", "log_samples", "=", "np", ".", "random", ".", "rand", "(", "num_samples", ")", "*", "np", ".", "log", "(", "num_words", "+", "1", ")", "\n", "samples", "=", "np", ".", "exp", "(", "log_samples", ")", ".", "astype", "(", "\"int64\"", ")", "-", "1", "\n", "return", "np", ".", "clip", "(", "samples", ",", "a_min", "=", "0", ",", "a_max", "=", "num_words", "-", "1", ")", "\n", "\n", "", "sample_buffer", "=", "get_buffer", "(", ")", "\n", "buffer_index", "=", "0", "\n", "samples", ":", "Set", "[", "int", "]", "=", "set", "(", ")", "\n", "\n", "while", "num_chosen", "<", "num_samples", ":", "\n", "        ", "num_tries", "+=", "1", "\n", "# choose sample", "\n", "sample_id", "=", "sample_buffer", "[", "buffer_index", "]", "\n", "if", "sample_id", "not", "in", "samples", ":", "\n", "            ", "samples", ".", "add", "(", "sample_id", ")", "\n", "num_chosen", "+=", "1", "\n", "\n", "", "buffer_index", "+=", "1", "\n", "if", "buffer_index", "==", "num_samples", ":", "\n", "# Reset the buffer", "\n", "            ", "sample_buffer", "=", "get_buffer", "(", ")", "\n", "buffer_index", "=", "0", "\n", "\n", "", "", "return", "np", ".", "array", "(", "list", "(", "samples", ")", ")", ",", "num_tries", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.modules.conditional_random_field.ConditionalRandomField.__init__": [[188, 219], ["super().__init__", "torch.nn.Parameter", "torch.nn.Parameter", "conditional_random_field.ConditionalRandomField.reset_parameters", "torch.Tensor", "torch.Tensor().fill_", "torch.Tensor().fill_", "torch.nn.Parameter", "torch.nn.Parameter", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.graph_util.plot.GraphPlot.__init__", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.similarity_functions.linear.LinearSimilarity.reset_parameters"], ["def", "__init__", "(", "\n", "self", ",", "\n", "num_tags", ":", "int", ",", "\n", "constraints", ":", "List", "[", "Tuple", "[", "int", ",", "int", "]", "]", "=", "None", ",", "\n", "include_start_end_transitions", ":", "bool", "=", "True", ",", "\n", ")", "->", "None", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "num_tags", "=", "num_tags", "\n", "\n", "# transitions[i, j] is the logit for transitioning from state i to state j.", "\n", "self", ".", "transitions", "=", "torch", ".", "nn", ".", "Parameter", "(", "torch", ".", "Tensor", "(", "num_tags", ",", "num_tags", ")", ")", "\n", "\n", "# _constraint_mask indicates valid transitions (based on supplied constraints).", "\n", "# Include special start of sequence (num_tags + 1) and end of sequence tags (num_tags + 2)", "\n", "if", "constraints", "is", "None", ":", "\n", "# All transitions are valid.", "\n", "            ", "constraint_mask", "=", "torch", ".", "Tensor", "(", "num_tags", "+", "2", ",", "num_tags", "+", "2", ")", ".", "fill_", "(", "1.0", ")", "\n", "", "else", ":", "\n", "            ", "constraint_mask", "=", "torch", ".", "Tensor", "(", "num_tags", "+", "2", ",", "num_tags", "+", "2", ")", ".", "fill_", "(", "0.0", ")", "\n", "for", "i", ",", "j", "in", "constraints", ":", "\n", "                ", "constraint_mask", "[", "i", ",", "j", "]", "=", "1.0", "\n", "\n", "", "", "self", ".", "_constraint_mask", "=", "torch", ".", "nn", ".", "Parameter", "(", "constraint_mask", ",", "requires_grad", "=", "False", ")", "\n", "\n", "# Also need logits for transitioning from \"start\" state and to \"end\" state.", "\n", "self", ".", "include_start_end_transitions", "=", "include_start_end_transitions", "\n", "if", "include_start_end_transitions", ":", "\n", "            ", "self", ".", "start_transitions", "=", "torch", ".", "nn", ".", "Parameter", "(", "torch", ".", "Tensor", "(", "num_tags", ")", ")", "\n", "self", ".", "end_transitions", "=", "torch", ".", "nn", ".", "Parameter", "(", "torch", ".", "Tensor", "(", "num_tags", ")", ")", "\n", "\n", "", "self", ".", "reset_parameters", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.modules.conditional_random_field.ConditionalRandomField.reset_parameters": [[220, 225], ["torch.nn.init.xavier_normal_", "torch.nn.init.normal_", "torch.nn.init.normal_"], "methods", ["None"], ["", "def", "reset_parameters", "(", "self", ")", ":", "\n", "        ", "torch", ".", "nn", ".", "init", ".", "xavier_normal_", "(", "self", ".", "transitions", ")", "\n", "if", "self", ".", "include_start_end_transitions", ":", "\n", "            ", "torch", ".", "nn", ".", "init", ".", "normal_", "(", "self", ".", "start_transitions", ")", "\n", "torch", ".", "nn", ".", "init", ".", "normal_", "(", "self", ".", "end_transitions", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.modules.conditional_random_field.ConditionalRandomField._input_likelihood": [[226, 274], ["logits.transpose().contiguous.transpose().contiguous.size", "mask.float().transpose().contiguous.float().transpose().contiguous.float().transpose().contiguous", "logits.transpose().contiguous.transpose().contiguous.transpose().contiguous", "range", "allennlp.logsumexp", "logits[].view", "conditional_random_field.ConditionalRandomField.transitions.view", "alpha.view", "mask.float().transpose().contiguous.float().transpose().contiguous.float().transpose", "logits.transpose().contiguous.transpose().contiguous.transpose", "conditional_random_field.ConditionalRandomField.start_transitions.view", "conditional_random_field.ConditionalRandomField.end_transitions.view", "allennlp.logsumexp", "mask[].view", "mask.float().transpose().contiguous.float().transpose().contiguous.float"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.nn.util.logsumexp", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.nn.util.logsumexp"], ["", "", "def", "_input_likelihood", "(", "\n", "self", ",", "logits", ":", "torch", ".", "Tensor", ",", "mask", ":", "torch", ".", "Tensor", "\n", ")", "->", "torch", ".", "Tensor", ":", "\n", "        ", "\"\"\"\n        Computes the (batch_size,) denominator term for the log-likelihood, which is the\n        sum of the likelihoods across all possible state sequences.\n        \"\"\"", "\n", "batch_size", ",", "sequence_length", ",", "num_tags", "=", "logits", ".", "size", "(", ")", "\n", "\n", "# Transpose batch size and sequence dimensions", "\n", "mask", "=", "mask", ".", "float", "(", ")", ".", "transpose", "(", "0", ",", "1", ")", ".", "contiguous", "(", ")", "\n", "logits", "=", "logits", ".", "transpose", "(", "0", ",", "1", ")", ".", "contiguous", "(", ")", "\n", "\n", "# Initial alpha is the (batch_size, num_tags) tensor of likelihoods combining the", "\n", "# transitions to the initial states and the logits for the first timestep.", "\n", "if", "self", ".", "include_start_end_transitions", ":", "\n", "            ", "alpha", "=", "self", ".", "start_transitions", ".", "view", "(", "1", ",", "num_tags", ")", "+", "logits", "[", "0", "]", "\n", "", "else", ":", "\n", "            ", "alpha", "=", "logits", "[", "0", "]", "\n", "\n", "# For each i we compute logits for the transitions from timestep i-1 to timestep i.", "\n", "# We do so in a (batch_size, num_tags, num_tags) tensor where the axes are", "\n", "# (instance, current_tag, next_tag)", "\n", "", "for", "i", "in", "range", "(", "1", ",", "sequence_length", ")", ":", "\n", "# The emit scores are for time i (\"next_tag\") so we broadcast along the current_tag axis.", "\n", "            ", "emit_scores", "=", "logits", "[", "i", "]", ".", "view", "(", "batch_size", ",", "1", ",", "num_tags", ")", "\n", "# Transition scores are (current_tag, next_tag) so we broadcast along the instance axis.", "\n", "transition_scores", "=", "self", ".", "transitions", ".", "view", "(", "1", ",", "num_tags", ",", "num_tags", ")", "\n", "# Alpha is for the current_tag, so we broadcast along the next_tag axis.", "\n", "broadcast_alpha", "=", "alpha", ".", "view", "(", "batch_size", ",", "num_tags", ",", "1", ")", "\n", "\n", "# Add all the scores together and logexp over the current_tag axis.", "\n", "inner", "=", "broadcast_alpha", "+", "emit_scores", "+", "transition_scores", "\n", "\n", "# In valid positions (mask == 1) we want to take the logsumexp over the current_tag dimension", "\n", "# of `inner`. Otherwise (mask == 0) we want to retain the previous alpha.", "\n", "alpha", "=", "util", ".", "logsumexp", "(", "inner", ",", "1", ")", "*", "mask", "[", "i", "]", ".", "view", "(", "batch_size", ",", "1", ")", "+", "alpha", "*", "(", "\n", "1", "-", "mask", "[", "i", "]", "\n", ")", ".", "view", "(", "batch_size", ",", "1", ")", "\n", "\n", "# Every sequence needs to end with a transition to the stop_tag.", "\n", "", "if", "self", ".", "include_start_end_transitions", ":", "\n", "            ", "stops", "=", "alpha", "+", "self", ".", "end_transitions", ".", "view", "(", "1", ",", "num_tags", ")", "\n", "", "else", ":", "\n", "            ", "stops", "=", "alpha", "\n", "\n", "# Finally we log_sum_exp along the num_tags dim, result is (batch_size,)", "\n", "", "return", "util", ".", "logsumexp", "(", "stops", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.modules.conditional_random_field.ConditionalRandomField._joint_likelihood": [[275, 330], ["logits.transpose().contiguous.transpose().contiguous.transpose().contiguous", "mask.float().transpose().contiguous.float().transpose().contiguous.float().transpose().contiguous", "tags.transpose().contiguous.transpose().contiguous.transpose().contiguous", "range", "tags.transpose().contiguous.transpose().contiguous.gather().squeeze", "last_inputs.gather", "last_input_score.squeeze.squeeze.squeeze", "conditional_random_field.ConditionalRandomField.start_transitions.index_select", "logits[].gather().squeeze", "mask.float().transpose().contiguous.float().transpose().contiguous.sum().long", "conditional_random_field.ConditionalRandomField.end_transitions.index_select", "tags.transpose().contiguous.gather().squeeze.view", "logits.transpose().contiguous.transpose().contiguous.transpose", "mask.float().transpose().contiguous.float().transpose().contiguous.float().transpose", "tags.transpose().contiguous.transpose().contiguous.transpose", "tags.transpose().contiguous.transpose().contiguous.gather", "logits[].gather", "mask.float().transpose().contiguous.float().transpose().contiguous.sum", "last_tag_index.view", "mask.float().transpose().contiguous.float().transpose().contiguous.float", "current_tag.view", "next_tag.view", "current_tag.view"], "methods", ["None"], ["", "def", "_joint_likelihood", "(", "\n", "self", ",", "logits", ":", "torch", ".", "Tensor", ",", "tags", ":", "torch", ".", "Tensor", ",", "mask", ":", "torch", ".", "LongTensor", "\n", ")", "->", "torch", ".", "Tensor", ":", "\n", "        ", "\"\"\"\n        Computes the numerator term for the log-likelihood, which is just score(inputs, tags)\n        \"\"\"", "\n", "batch_size", ",", "sequence_length", ",", "_", "=", "logits", ".", "data", ".", "shape", "\n", "\n", "# Transpose batch size and sequence dimensions:", "\n", "logits", "=", "logits", ".", "transpose", "(", "0", ",", "1", ")", ".", "contiguous", "(", ")", "\n", "mask", "=", "mask", ".", "float", "(", ")", ".", "transpose", "(", "0", ",", "1", ")", ".", "contiguous", "(", ")", "\n", "tags", "=", "tags", ".", "transpose", "(", "0", ",", "1", ")", ".", "contiguous", "(", ")", "\n", "\n", "# Start with the transition scores from start_tag to the first tag in each input", "\n", "if", "self", ".", "include_start_end_transitions", ":", "\n", "            ", "score", "=", "self", ".", "start_transitions", ".", "index_select", "(", "0", ",", "tags", "[", "0", "]", ")", "\n", "", "else", ":", "\n", "            ", "score", "=", "0.0", "\n", "\n", "# Add up the scores for the observed transitions and all the inputs but the last", "\n", "", "for", "i", "in", "range", "(", "sequence_length", "-", "1", ")", ":", "\n", "# Each is shape (batch_size,)", "\n", "            ", "current_tag", ",", "next_tag", "=", "tags", "[", "i", "]", ",", "tags", "[", "i", "+", "1", "]", "\n", "\n", "# The scores for transitioning from current_tag to next_tag", "\n", "transition_score", "=", "self", ".", "transitions", "[", "current_tag", ".", "view", "(", "-", "1", ")", ",", "next_tag", ".", "view", "(", "-", "1", ")", "]", "\n", "\n", "# The score for using current_tag", "\n", "emit_score", "=", "logits", "[", "i", "]", ".", "gather", "(", "1", ",", "current_tag", ".", "view", "(", "batch_size", ",", "1", ")", ")", ".", "squeeze", "(", "1", ")", "\n", "\n", "# Include transition score if next element is unmasked,", "\n", "# input_score if this element is unmasked.", "\n", "score", "=", "score", "+", "transition_score", "*", "mask", "[", "i", "+", "1", "]", "+", "emit_score", "*", "mask", "[", "i", "]", "\n", "\n", "# Transition from last state to \"stop\" state. To start with, we need to find the last tag", "\n", "# for each instance.", "\n", "", "last_tag_index", "=", "mask", ".", "sum", "(", "0", ")", ".", "long", "(", ")", "-", "1", "\n", "last_tags", "=", "tags", ".", "gather", "(", "0", ",", "last_tag_index", ".", "view", "(", "1", ",", "batch_size", ")", ")", ".", "squeeze", "(", "0", ")", "\n", "\n", "# Compute score of transitioning to `stop_tag` from each \"last tag\".", "\n", "if", "self", ".", "include_start_end_transitions", ":", "\n", "            ", "last_transition_score", "=", "self", ".", "end_transitions", ".", "index_select", "(", "0", ",", "last_tags", ")", "\n", "", "else", ":", "\n", "            ", "last_transition_score", "=", "0.0", "\n", "\n", "# Add the last input if it's not masked.", "\n", "", "last_inputs", "=", "logits", "[", "-", "1", "]", "# (batch_size, num_tags)", "\n", "last_input_score", "=", "last_inputs", ".", "gather", "(", "\n", "1", ",", "last_tags", ".", "view", "(", "-", "1", ",", "1", ")", "\n", ")", "# (batch_size, 1)", "\n", "last_input_score", "=", "last_input_score", ".", "squeeze", "(", ")", "# (batch_size,)", "\n", "\n", "score", "=", "score", "+", "last_transition_score", "+", "last_input_score", "*", "mask", "[", "-", "1", "]", "\n", "\n", "return", "score", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.modules.conditional_random_field.ConditionalRandomField.forward": [[331, 345], ["conditional_random_field.ConditionalRandomField._input_likelihood", "conditional_random_field.ConditionalRandomField._joint_likelihood", "torch.sum", "torch.ones", "tags.size"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.modules.conditional_random_field.ConditionalRandomField._input_likelihood", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.modules.conditional_random_field.ConditionalRandomField._joint_likelihood"], ["", "def", "forward", "(", "\n", "self", ",", "inputs", ":", "torch", ".", "Tensor", ",", "tags", ":", "torch", ".", "Tensor", ",", "mask", ":", "torch", ".", "ByteTensor", "=", "None", "\n", ")", "->", "torch", ".", "Tensor", ":", "\n", "        ", "\"\"\"\n        Computes the log likelihood.\n        \"\"\"", "\n", "\n", "if", "mask", "is", "None", ":", "\n", "            ", "mask", "=", "torch", ".", "ones", "(", "*", "tags", ".", "size", "(", ")", ",", "dtype", "=", "torch", ".", "long", ")", "\n", "\n", "", "log_denominator", "=", "self", ".", "_input_likelihood", "(", "inputs", ",", "mask", ")", "\n", "log_numerator", "=", "self", ".", "_joint_likelihood", "(", "inputs", ",", "tags", ",", "mask", ")", "\n", "\n", "return", "torch", ".", "sum", "(", "log_numerator", "-", "log_denominator", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.modules.conditional_random_field.ConditionalRandomField.viterbi_tags": [[346, 443], ["logits.size", "torch.Tensor().fill_", "torch.Tensor", "zip", "torch.ones", "prediction_mask.nonzero().squeeze", "torch.index_select", "torch.Tensor.fill_", "allennlp.viterbi_decode", "zip", "best_paths.append", "torch.Tensor", "top_k_paths.append", "conditional_random_field.ConditionalRandomField.start_transitions.detach", "conditional_random_field.ConditionalRandomField.end_transitions.detach", "conditional_random_field.ConditionalRandomField._constraint_mask[].detach", "conditional_random_field.ConditionalRandomField._constraint_mask[].detach", "prediction_mask.nonzero", "conditional_random_field.ConditionalRandomField._constraint_mask[].detach", "conditional_random_field.ConditionalRandomField._constraint_mask[].detach", "viterbi_score.item"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.nn.util.viterbi_decode"], ["", "def", "viterbi_tags", "(", "\n", "self", ",", "logits", ":", "torch", ".", "Tensor", ",", "mask", ":", "torch", ".", "Tensor", "=", "None", ",", "top_k", ":", "int", "=", "None", "\n", ")", "->", "Union", "[", "List", "[", "VITERBI_DECODING", "]", ",", "List", "[", "List", "[", "VITERBI_DECODING", "]", "]", "]", ":", "\n", "        ", "\"\"\"\n        Uses viterbi algorithm to find most likely tags for the given inputs.\n        If constraints are applied, disallows all other transitions.\n\n        Returns a list of results, of the same size as the batch (one result per batch member)\n        Each result is a List of length top_k, containing the top K viterbi decodings\n        Each decoding is a tuple  (tag_sequence, viterbi_score)\n\n        For backwards compatibility, if top_k is None, then instead returns a flat list of\n        tag sequences (the top tag sequence for each batch item).\n        \"\"\"", "\n", "if", "mask", "is", "None", ":", "\n", "            ", "mask", "=", "torch", ".", "ones", "(", "*", "logits", ".", "shape", "[", ":", "2", "]", ",", "dtype", "=", "torch", ".", "long", ",", "device", "=", "logits", ".", "device", ")", "\n", "\n", "", "if", "top_k", "is", "None", ":", "\n", "            ", "top_k", "=", "1", "\n", "flatten_output", "=", "True", "\n", "", "else", ":", "\n", "            ", "flatten_output", "=", "False", "\n", "\n", "", "_", ",", "max_seq_length", ",", "num_tags", "=", "logits", ".", "size", "(", ")", "\n", "\n", "# Get the tensors out of the variables", "\n", "logits", ",", "mask", "=", "logits", ".", "data", ",", "mask", ".", "data", "\n", "\n", "# Augment transitions matrix with start and end transitions", "\n", "start_tag", "=", "num_tags", "\n", "end_tag", "=", "num_tags", "+", "1", "\n", "transitions", "=", "torch", ".", "Tensor", "(", "num_tags", "+", "2", ",", "num_tags", "+", "2", ")", ".", "fill_", "(", "-", "10000.0", ")", "\n", "\n", "# Apply transition constraints", "\n", "constrained_transitions", "=", "self", ".", "transitions", "*", "self", ".", "_constraint_mask", "[", "\n", ":", "num_tags", ",", ":", "num_tags", "\n", "]", "+", "-", "10000.0", "*", "(", "1", "-", "self", ".", "_constraint_mask", "[", ":", "num_tags", ",", ":", "num_tags", "]", ")", "\n", "transitions", "[", ":", "num_tags", ",", ":", "num_tags", "]", "=", "constrained_transitions", ".", "data", "\n", "\n", "if", "self", ".", "include_start_end_transitions", ":", "\n", "            ", "transitions", "[", "\n", "start_tag", ",", ":", "num_tags", "\n", "]", "=", "self", ".", "start_transitions", ".", "detach", "(", ")", "*", "self", ".", "_constraint_mask", "[", "\n", "start_tag", ",", ":", "num_tags", "\n", "]", ".", "data", "+", "-", "10000.0", "*", "(", "\n", "1", "-", "self", ".", "_constraint_mask", "[", "start_tag", ",", ":", "num_tags", "]", ".", "detach", "(", ")", "\n", ")", "\n", "transitions", "[", "\n", ":", "num_tags", ",", "end_tag", "\n", "]", "=", "self", ".", "end_transitions", ".", "detach", "(", ")", "*", "self", ".", "_constraint_mask", "[", "\n", ":", "num_tags", ",", "end_tag", "\n", "]", ".", "data", "+", "-", "10000.0", "*", "(", "\n", "1", "-", "self", ".", "_constraint_mask", "[", ":", "num_tags", ",", "end_tag", "]", ".", "detach", "(", ")", "\n", ")", "\n", "", "else", ":", "\n", "            ", "transitions", "[", "start_tag", ",", ":", "num_tags", "]", "=", "-", "10000.0", "*", "(", "\n", "1", "-", "self", ".", "_constraint_mask", "[", "start_tag", ",", ":", "num_tags", "]", ".", "detach", "(", ")", "\n", ")", "\n", "transitions", "[", ":", "num_tags", ",", "end_tag", "]", "=", "-", "10000.0", "*", "(", "\n", "1", "-", "self", ".", "_constraint_mask", "[", ":", "num_tags", ",", "end_tag", "]", ".", "detach", "(", ")", "\n", ")", "\n", "\n", "", "best_paths", "=", "[", "]", "\n", "# Pad the max sequence length by 2 to account for start_tag + end_tag.", "\n", "tag_sequence", "=", "torch", ".", "Tensor", "(", "max_seq_length", "+", "2", ",", "num_tags", "+", "2", ")", "\n", "\n", "for", "prediction", ",", "prediction_mask", "in", "zip", "(", "logits", ",", "mask", ")", ":", "\n", "            ", "mask_indices", "=", "prediction_mask", ".", "nonzero", "(", ")", ".", "squeeze", "(", ")", "\n", "masked_prediction", "=", "torch", ".", "index_select", "(", "prediction", ",", "0", ",", "mask_indices", ")", "\n", "sequence_length", "=", "masked_prediction", ".", "shape", "[", "0", "]", "\n", "\n", "# Start with everything totally unlikely", "\n", "tag_sequence", ".", "fill_", "(", "-", "10000.0", ")", "\n", "# At timestep 0 we must have the START_TAG", "\n", "tag_sequence", "[", "0", ",", "start_tag", "]", "=", "0.0", "\n", "# At steps 1, ..., sequence_length we just use the incoming prediction", "\n", "tag_sequence", "[", "1", ":", "(", "sequence_length", "+", "1", ")", ",", ":", "num_tags", "]", "=", "masked_prediction", "\n", "# And at the last timestep we must have the END_TAG", "\n", "tag_sequence", "[", "sequence_length", "+", "1", ",", "end_tag", "]", "=", "0.0", "\n", "\n", "# We pass the tags and the transitions to `viterbi_decode`.", "\n", "viterbi_paths", ",", "viterbi_scores", "=", "util", ".", "viterbi_decode", "(", "\n", "tag_sequence", "=", "tag_sequence", "[", ":", "(", "sequence_length", "+", "2", ")", "]", ",", "\n", "transition_matrix", "=", "transitions", ",", "\n", "top_k", "=", "top_k", ",", "\n", ")", "\n", "top_k_paths", "=", "[", "]", "\n", "for", "viterbi_path", ",", "viterbi_score", "in", "zip", "(", "viterbi_paths", ",", "viterbi_scores", ")", ":", "\n", "# Get rid of START and END sentinels and append.", "\n", "                ", "viterbi_path", "=", "viterbi_path", "[", "1", ":", "-", "1", "]", "\n", "top_k_paths", ".", "append", "(", "(", "viterbi_path", ",", "viterbi_score", ".", "item", "(", ")", ")", ")", "\n", "", "best_paths", ".", "append", "(", "top_k_paths", ")", "\n", "\n", "", "if", "flatten_output", ":", "\n", "            ", "return", "[", "top_k_paths", "[", "0", "]", "for", "top_k_paths", "in", "best_paths", "]", "\n", "\n", "", "return", "best_paths", "\n", "", "", ""]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.modules.conditional_random_field.allowed_transitions": [[14, 64], ["len", "list", "labels.items", "conditional_random_field.is_transition_allowed", "allowed.append"], "function", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.modules.conditional_random_field.is_transition_allowed"], ["def", "allowed_transitions", "(", "\n", "constraint_type", ":", "str", ",", "labels", ":", "Dict", "[", "int", ",", "str", "]", "\n", ")", "->", "List", "[", "Tuple", "[", "int", ",", "int", "]", "]", ":", "\n", "    ", "\"\"\"\n    Given labels and a constraint type, returns the allowed transitions. It will\n    additionally include transitions for the start and end states, which are used\n    by the conditional random field.\n\n    # Parameters\n\n    constraint_type : `str`, required\n        Indicates which constraint to apply. Current choices are\n        \"BIO\", \"IOB1\", \"BIOUL\", and \"BMES\".\n    labels : `Dict[int, str]`, required\n        A mapping {label_id -> label}. Most commonly this would be the value from\n        Vocabulary.get_index_to_token_vocabulary()\n\n    # Returns\n\n    `List[Tuple[int, int]]`\n        The allowed transitions (from_label_id, to_label_id).\n    \"\"\"", "\n", "num_labels", "=", "len", "(", "labels", ")", "\n", "start_tag", "=", "num_labels", "\n", "end_tag", "=", "num_labels", "+", "1", "\n", "labels_with_boundaries", "=", "list", "(", "labels", ".", "items", "(", ")", ")", "+", "[", "\n", "(", "start_tag", ",", "\"START\"", ")", ",", "\n", "(", "end_tag", ",", "\"END\"", ")", ",", "\n", "]", "\n", "\n", "allowed", "=", "[", "]", "\n", "for", "from_label_index", ",", "from_label", "in", "labels_with_boundaries", ":", "\n", "        ", "if", "from_label", "in", "(", "\"START\"", ",", "\"END\"", ")", ":", "\n", "            ", "from_tag", "=", "from_label", "\n", "from_entity", "=", "\"\"", "\n", "", "else", ":", "\n", "            ", "from_tag", "=", "from_label", "[", "0", "]", "\n", "from_entity", "=", "from_label", "[", "1", ":", "]", "\n", "", "for", "to_label_index", ",", "to_label", "in", "labels_with_boundaries", ":", "\n", "            ", "if", "to_label", "in", "(", "\"START\"", ",", "\"END\"", ")", ":", "\n", "                ", "to_tag", "=", "to_label", "\n", "to_entity", "=", "\"\"", "\n", "", "else", ":", "\n", "                ", "to_tag", "=", "to_label", "[", "0", "]", "\n", "to_entity", "=", "to_label", "[", "1", ":", "]", "\n", "", "if", "is_transition_allowed", "(", "\n", "constraint_type", ",", "from_tag", ",", "from_entity", ",", "to_tag", ",", "to_entity", "\n", ")", ":", "\n", "                ", "allowed", ".", "append", "(", "(", "from_label_index", ",", "to_label_index", ")", ")", "\n", "", "", "", "return", "allowed", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.modules.conditional_random_field.is_transition_allowed": [[66, 166], ["any", "any", "any", "any", "allennlp.common.checks.ConfigurationError"], "function", ["None"], ["", "def", "is_transition_allowed", "(", "\n", "constraint_type", ":", "str", ",", "from_tag", ":", "str", ",", "from_entity", ":", "str", ",", "to_tag", ":", "str", ",", "to_entity", ":", "str", "\n", ")", ":", "\n", "    ", "\"\"\"\n    Given a constraint type and strings `from_tag` and `to_tag` that\n    represent the origin and destination of the transition, return whether\n    the transition is allowed under the given constraint type.\n\n    # Parameters\n\n    constraint_type : `str`, required\n        Indicates which constraint to apply. Current choices are\n        \"BIO\", \"IOB1\", \"BIOUL\", and \"BMES\".\n    from_tag : `str`, required\n        The tag that the transition originates from. For example, if the\n        label is `I-PER`, the `from_tag` is `I`.\n    from_entity : `str`, required\n        The entity corresponding to the `from_tag`. For example, if the\n        label is `I-PER`, the `from_entity` is `PER`.\n    to_tag : `str`, required\n        The tag that the transition leads to. For example, if the\n        label is `I-PER`, the `to_tag` is `I`.\n    to_entity : `str`, required\n        The entity corresponding to the `to_tag`. For example, if the\n        label is `I-PER`, the `to_entity` is `PER`.\n\n    # Returns\n\n    `bool`\n        Whether the transition is allowed under the given `constraint_type`.\n    \"\"\"", "\n", "\n", "if", "to_tag", "==", "\"START\"", "or", "from_tag", "==", "\"END\"", ":", "\n", "# Cannot transition into START or from END", "\n", "        ", "return", "False", "\n", "\n", "", "if", "constraint_type", "==", "\"BIOUL\"", ":", "\n", "        ", "if", "from_tag", "==", "\"START\"", ":", "\n", "            ", "return", "to_tag", "in", "(", "\"O\"", ",", "\"B\"", ",", "\"U\"", ")", "\n", "", "if", "to_tag", "==", "\"END\"", ":", "\n", "            ", "return", "from_tag", "in", "(", "\"O\"", ",", "\"L\"", ",", "\"U\"", ")", "\n", "", "return", "any", "(", "\n", "[", "\n", "# O can transition to O, B-* or U-*", "\n", "# L-x can transition to O, B-*, or U-*", "\n", "# U-x can transition to O, B-*, or U-*", "\n", "from_tag", "in", "(", "\"O\"", ",", "\"L\"", ",", "\"U\"", ")", "and", "to_tag", "in", "(", "\"O\"", ",", "\"B\"", ",", "\"U\"", ")", ",", "\n", "# B-x can only transition to I-x or L-x", "\n", "# I-x can only transition to I-x or L-x", "\n", "from_tag", "in", "(", "\"B\"", ",", "\"I\"", ")", "\n", "and", "to_tag", "in", "(", "\"I\"", ",", "\"L\"", ")", "\n", "and", "from_entity", "==", "to_entity", ",", "\n", "]", "\n", ")", "\n", "", "elif", "constraint_type", "==", "\"BIO\"", ":", "\n", "        ", "if", "from_tag", "==", "\"START\"", ":", "\n", "            ", "return", "to_tag", "in", "(", "\"O\"", ",", "\"B\"", ")", "\n", "", "if", "to_tag", "==", "\"END\"", ":", "\n", "            ", "return", "from_tag", "in", "(", "\"O\"", ",", "\"B\"", ",", "\"I\"", ")", "\n", "", "return", "any", "(", "\n", "[", "\n", "# Can always transition to O or B-x", "\n", "to_tag", "in", "(", "\"O\"", ",", "\"B\"", ")", ",", "\n", "# Can only transition to I-x from B-x or I-x", "\n", "to_tag", "==", "\"I\"", "and", "from_tag", "in", "(", "\"B\"", ",", "\"I\"", ")", "and", "from_entity", "==", "to_entity", ",", "\n", "]", "\n", ")", "\n", "", "elif", "constraint_type", "==", "\"IOB1\"", ":", "\n", "        ", "if", "from_tag", "==", "\"START\"", ":", "\n", "            ", "return", "to_tag", "in", "(", "\"O\"", ",", "\"I\"", ")", "\n", "", "if", "to_tag", "==", "\"END\"", ":", "\n", "            ", "return", "from_tag", "in", "(", "\"O\"", ",", "\"B\"", ",", "\"I\"", ")", "\n", "", "return", "any", "(", "\n", "[", "\n", "# Can always transition to O or I-x", "\n", "to_tag", "in", "(", "\"O\"", ",", "\"I\"", ")", ",", "\n", "# Can only transition to B-x from B-x or I-x, where", "\n", "# x is the same tag.", "\n", "to_tag", "==", "\"B\"", "and", "from_tag", "in", "(", "\"B\"", ",", "\"I\"", ")", "and", "from_entity", "==", "to_entity", ",", "\n", "]", "\n", ")", "\n", "", "elif", "constraint_type", "==", "\"BMES\"", ":", "\n", "        ", "if", "from_tag", "==", "\"START\"", ":", "\n", "            ", "return", "to_tag", "in", "(", "\"B\"", ",", "\"S\"", ")", "\n", "", "if", "to_tag", "==", "\"END\"", ":", "\n", "            ", "return", "from_tag", "in", "(", "\"E\"", ",", "\"S\"", ")", "\n", "", "return", "any", "(", "\n", "[", "\n", "# Can only transition to B or S from E or S.", "\n", "to_tag", "in", "(", "\"B\"", ",", "\"S\"", ")", "and", "from_tag", "in", "(", "\"E\"", ",", "\"S\"", ")", ",", "\n", "# Can only transition to M-x from B-x, where", "\n", "# x is the same tag.", "\n", "to_tag", "==", "\"M\"", "and", "from_tag", "in", "(", "\"B\"", ",", "\"M\"", ")", "and", "from_entity", "==", "to_entity", ",", "\n", "# Can only transition to E-x from B-x or M-x, where", "\n", "# x is the same tag.", "\n", "to_tag", "==", "\"E\"", "and", "from_tag", "in", "(", "\"B\"", ",", "\"M\"", ")", "and", "from_entity", "==", "to_entity", ",", "\n", "]", "\n", ")", "\n", "", "else", ":", "\n", "        ", "raise", "ConfigurationError", "(", "f\"Unknown constraint type: {constraint_type}\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.text_field_embedders.shortcut_connect_text_field_embedder.ShortcutConnectTextFieldEmbedder.__init__": [[35, 43], ["allennlp.modules.text_field_embedders.text_field_embedder.TextFieldEmbedder.__init__"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.graph_util.plot.GraphPlot.__init__"], ["def", "__init__", "(", "\n", "self", ",", "\n", "base_text_field_embedder", ":", "TextFieldEmbedder", ",", "\n", "previous_encoders", ":", "List", "[", "Seq2SeqEncoder", "]", ",", "\n", ")", "->", "None", ":", "\n", "        ", "super", "(", "ShortcutConnectTextFieldEmbedder", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "_base_text_field_embedder", "=", "base_text_field_embedder", "\n", "self", ".", "_previous_encoders", "=", "previous_encoders", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.text_field_embedders.shortcut_connect_text_field_embedder.ShortcutConnectTextFieldEmbedder.get_output_dim": [[44, 51], ["shortcut_connect_text_field_embedder.ShortcutConnectTextFieldEmbedder._base_text_field_embedder.get_output_dim", "shortcut_connect_text_field_embedder.ShortcutConnectTextFieldEmbedder._previous_encoders[].get_output_dim"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.seq2seq_decoders.auto_regressive_seq_decoder.AutoRegressiveSeqDecoder.get_output_dim", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.seq2seq_decoders.auto_regressive_seq_decoder.AutoRegressiveSeqDecoder.get_output_dim"], ["", "@", "overrides", "\n", "def", "get_output_dim", "(", "self", ")", "->", "int", ":", "\n", "        ", "output_dim", "=", "0", "\n", "output_dim", "+=", "self", ".", "_base_text_field_embedder", ".", "get_output_dim", "(", ")", "\n", "output_dim", "+=", "self", ".", "_previous_encoders", "[", "-", "1", "]", ".", "get_output_dim", "(", ")", "\n", "\n", "return", "output_dim", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.text_field_embedders.shortcut_connect_text_field_embedder.ShortcutConnectTextFieldEmbedder.forward": [[52, 69], ["shortcut_connect_text_field_embedder.ShortcutConnectTextFieldEmbedder._base_text_field_embedder.forward", "allennlp.get_text_field_mask", "torch.cat", "encoder", "torch.cat"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.graph_util.graph_encoder_gat.GAT.forward", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.nn.util.get_text_field_mask"], ["", "@", "overrides", "\n", "def", "forward", "(", "\n", "self", ",", "text_field_input", ":", "Dict", "[", "str", ",", "torch", ".", "Tensor", "]", ",", "num_wrapping_dims", ":", "int", "=", "0", "\n", ")", "->", "torch", ".", "Tensor", ":", "\n", "        ", "text_field_embeddings", "=", "self", ".", "_base_text_field_embedder", ".", "forward", "(", "\n", "text_field_input", ",", "num_wrapping_dims", "\n", ")", "\n", "base_representation", "=", "text_field_embeddings", "\n", "mask", "=", "util", ".", "get_text_field_mask", "(", "text_field_input", ")", "\n", "\n", "for", "encoder", "in", "self", ".", "_previous_encoders", ":", "\n", "            ", "text_field_embeddings", "=", "encoder", "(", "text_field_embeddings", ",", "mask", ")", "\n", "text_field_embeddings", "=", "torch", ".", "cat", "(", "\n", "[", "base_representation", ",", "text_field_embeddings", "]", ",", "dim", "=", "-", "1", "\n", ")", "\n", "\n", "", "return", "torch", ".", "cat", "(", "[", "text_field_embeddings", "]", ",", "dim", "=", "-", "1", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.text_field_embedders.text_field_embedder.TextFieldEmbedder.forward": [[26, 43], ["None"], "methods", ["None"], ["def", "forward", "(", "\n", "self", ",", "text_field_input", ":", "TextFieldTensors", ",", "num_wrapping_dims", ":", "int", "=", "0", ",", "**", "kwargs", "\n", ")", "->", "torch", ".", "Tensor", ":", "\n", "        ", "\"\"\"\n        # Parameters\n\n        text_field_input : `TextFieldTensors`\n            A dictionary that was the output of a call to `TextField.as_tensor`.  Each tensor in\n            here is assumed to have a shape roughly similar to `(batch_size, sequence_length)`\n            (perhaps with an extra trailing dimension for the characters in each token).\n        num_wrapping_dims : `int`, optional (default=0)\n            If you have a `ListField[TextField]` that created the `text_field_input`, you'll\n            end up with tensors of shape `(batch_size, wrapping_dim1, wrapping_dim2, ...,\n            sequence_length)`.  This parameter tells us how many wrapping dimensions there are, so\n            that we can correctly `TimeDistribute` the embedding of each named representation.\n        \"\"\"", "\n", "raise", "NotImplementedError", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.text_field_embedders.text_field_embedder.TextFieldEmbedder.get_output_dim": [[44, 51], ["None"], "methods", ["None"], ["", "def", "get_output_dim", "(", "self", ")", "->", "int", ":", "\n", "        ", "\"\"\"\n        Returns the dimension of the vector representing each token in the output of this\n        `TextFieldEmbedder`.  This is _not_ the shape of the returned tensor, but the last element\n        of that shape.\n        \"\"\"", "\n", "raise", "NotImplementedError", "\n", "", "", ""]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.text_field_embedders.basic_text_field_embedder.BasicTextFieldEmbedder.__init__": [[34, 44], ["allennlp.modules.text_field_embedders.text_field_embedder.TextFieldEmbedder.__init__", "token_embedders.items", "sorted", "basic_text_field_embedder.BasicTextFieldEmbedder.add_module", "basic_text_field_embedder.BasicTextFieldEmbedder._token_embedders.keys"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.graph_util.plot.GraphPlot.__init__"], ["def", "__init__", "(", "self", ",", "token_embedders", ":", "Dict", "[", "str", ",", "TokenEmbedder", "]", ")", "->", "None", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "# NOTE(mattg): I'd prefer to just use ModuleDict(token_embedders) here, but that changes", "\n", "# weight locations in torch state dictionaries and invalidates all prior models, just for a", "\n", "# cosmetic change in the code.", "\n", "self", ".", "_token_embedders", "=", "token_embedders", "\n", "for", "key", ",", "embedder", "in", "token_embedders", ".", "items", "(", ")", ":", "\n", "            ", "name", "=", "\"token_embedder_%s\"", "%", "key", "\n", "self", ".", "add_module", "(", "name", ",", "embedder", ")", "\n", "", "self", ".", "_ordered_embedder_keys", "=", "sorted", "(", "self", ".", "_token_embedders", ".", "keys", "(", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.text_field_embedders.basic_text_field_embedder.BasicTextFieldEmbedder.get_output_dim": [[45, 51], ["basic_text_field_embedder.BasicTextFieldEmbedder._token_embedders.values", "embedder.get_output_dim"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.seq2seq_decoders.auto_regressive_seq_decoder.AutoRegressiveSeqDecoder.get_output_dim"], ["", "@", "overrides", "\n", "def", "get_output_dim", "(", "self", ")", "->", "int", ":", "\n", "        ", "output_dim", "=", "0", "\n", "for", "embedder", "in", "self", ".", "_token_embedders", ".", "values", "(", ")", ":", "\n", "            ", "output_dim", "+=", "embedder", ".", "get_output_dim", "(", ")", "\n", "", "return", "output_dim", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.text_field_embedders.basic_text_field_embedder.BasicTextFieldEmbedder.forward": [[52, 95], ["torch.cat", "basic_text_field_embedder.BasicTextFieldEmbedder._token_embedders.keys", "text_field_input.keys", "allennlp.common.checks.ConfigurationError", "getattr", "set", "forward_params.keys", "range", "inspect.signature", "allennlp.modules.time_distributed.TimeDistributed", "allennlp.modules.time_distributed.TimeDistributed.", "allennlp.modules.time_distributed.TimeDistributed.", "embedded_representations.append", "str", "str", "set.add", "len", "len", "basic_text_field_embedder.BasicTextFieldEmbedder._token_embedders.keys", "text_field_input.keys", "list", "tensors.values"], "methods", ["None"], ["", "def", "forward", "(", "\n", "self", ",", "text_field_input", ":", "TextFieldTensors", ",", "num_wrapping_dims", ":", "int", "=", "0", ",", "**", "kwargs", "\n", ")", "->", "torch", ".", "Tensor", ":", "\n", "        ", "if", "self", ".", "_token_embedders", ".", "keys", "(", ")", "!=", "text_field_input", ".", "keys", "(", ")", ":", "\n", "            ", "message", "=", "\"Mismatched token keys: %s and %s\"", "%", "(", "\n", "str", "(", "self", ".", "_token_embedders", ".", "keys", "(", ")", ")", ",", "\n", "str", "(", "text_field_input", ".", "keys", "(", ")", ")", ",", "\n", ")", "\n", "raise", "ConfigurationError", "(", "message", ")", "\n", "\n", "", "embedded_representations", "=", "[", "]", "\n", "for", "key", "in", "self", ".", "_ordered_embedder_keys", ":", "\n", "# Note: need to use getattr here so that the pytorch voodoo", "\n", "# with submodules works with multiple GPUs.", "\n", "            ", "embedder", "=", "getattr", "(", "self", ",", "\"token_embedder_{}\"", ".", "format", "(", "key", ")", ")", "\n", "forward_params", "=", "inspect", ".", "signature", "(", "embedder", ".", "forward", ")", ".", "parameters", "\n", "forward_params_values", "=", "{", "}", "\n", "missing_tensor_args", "=", "set", "(", ")", "\n", "for", "param", "in", "forward_params", ".", "keys", "(", ")", ":", "\n", "                ", "if", "param", "in", "kwargs", ":", "\n", "                    ", "forward_params_values", "[", "param", "]", "=", "kwargs", "[", "param", "]", "\n", "", "else", ":", "\n", "                    ", "missing_tensor_args", ".", "add", "(", "param", ")", "\n", "\n", "", "", "for", "_", "in", "range", "(", "num_wrapping_dims", ")", ":", "\n", "                ", "embedder", "=", "TimeDistributed", "(", "embedder", ")", "\n", "\n", "", "tensors", ":", "Dict", "[", "str", ",", "torch", ".", "Tensor", "]", "=", "text_field_input", "[", "key", "]", "\n", "if", "len", "(", "tensors", ")", "==", "1", "and", "len", "(", "missing_tensor_args", ")", "==", "1", ":", "\n", "# If there's only one tensor argument to the embedder, and we just have one tensor to", "\n", "# embed, we can just pass in that tensor, without requiring a name match.", "\n", "                ", "token_vectors", "=", "embedder", "(", "\n", "list", "(", "tensors", ".", "values", "(", ")", ")", "[", "0", "]", ",", "**", "forward_params_values", "\n", ")", "\n", "", "else", ":", "\n", "# If there are multiple tensor arguments, we have to require matching names from the", "\n", "# TokenIndexer.  I don't think there's an easy way around that.", "\n", "                ", "token_vectors", "=", "embedder", "(", "**", "tensors", ",", "**", "forward_params_values", ")", "\n", "", "if", "token_vectors", "is", "not", "None", ":", "\n", "# To handle some very rare use cases, we allow the return value of the embedder to", "\n", "# be None; we just skip it in that case.", "\n", "                ", "embedded_representations", ".", "append", "(", "token_vectors", ")", "\n", "", "", "return", "torch", ".", "cat", "(", "embedded_representations", ",", "dim", "=", "-", "1", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.seq2seq_encoders.stacked_gru.StackedGRU.__init__": [[41, 85], ["allennlp.modules.seq2seq_encoders.seq2seq_encoder.Seq2SeqEncoder.__init__", "range", "len", "ValueError", "len", "ValueError", "torch.nn.GRU", "stacked_gru.StackedGRU.add_module", "stacked_gru.StackedGRU._gru_layers.append", "len", "len"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.graph_util.plot.GraphPlot.__init__"], ["def", "__init__", "(", "\n", "self", ",", "\n", "input_dim", ":", "int", ",", "\n", "hidden_sizes", ":", "List", "[", "int", "]", ",", "\n", "num_layers", ":", "int", ",", "\n", "bidirectional", ":", "bool", ",", "\n", "dropouts", ":", "List", "[", "float", "]", "=", "None", ",", "\n", ")", "->", "None", ":", "\n", "        ", "super", "(", "StackedGRU", ",", "self", ")", ".", "__init__", "(", ")", "\n", "\n", "self", ".", "_input_dim", "=", "input_dim", "\n", "self", ".", "_hidden_sizes", "=", "hidden_sizes", "\n", "self", ".", "_num_layers", "=", "num_layers", "\n", "self", ".", "_bidirectional", "=", "bidirectional", "\n", "self", ".", "_dropouts", "=", "[", "0.0", "]", "*", "num_layers", "if", "dropouts", "is", "None", "else", "dropouts", "\n", "\n", "if", "len", "(", "self", ".", "_hidden_sizes", ")", "!=", "self", ".", "_num_layers", ":", "\n", "            ", "raise", "ValueError", "(", "\n", "f\"Number of layers ({self._num_layers}) must be equal to the length of hidden state size list ({len(self._hidden_sizes)})\"", "\n", ")", "\n", "", "if", "len", "(", "self", ".", "_dropouts", ")", "!=", "self", ".", "_num_layers", ":", "\n", "            ", "raise", "ValueError", "(", "\n", "f\"Number of layers ({self._num_layers}) must be equal to the legnth of drouput rates list ({len(self._dropouts)})\"", "\n", ")", "\n", "\n", "", "self", ".", "_output_dim", "=", "hidden_sizes", "[", "-", "1", "]", "\n", "if", "self", ".", "_bidirectional", ":", "\n", "            ", "self", ".", "_output_dim", "*=", "2", "\n", "\n", "", "self", ".", "_gru_layers", ":", "List", "[", "GRU", "]", "=", "[", "]", "\n", "for", "k", "in", "range", "(", "self", ".", "_num_layers", ")", ":", "\n", "            ", "input_size", "=", "self", ".", "_input_dim", "if", "k", "==", "0", "else", "self", ".", "_hidden_sizes", "[", "k", "-", "1", "]", "\n", "if", "self", ".", "_bidirectional", "and", "(", "k", "!=", "0", ")", ":", "\n", "                ", "input_size", "*=", "2", "\n", "\n", "", "gru_layer", "=", "GRU", "(", "\n", "input_size", "=", "input_size", ",", "\n", "hidden_size", "=", "self", ".", "_hidden_sizes", "[", "k", "]", ",", "\n", "dropout", "=", "self", ".", "_dropouts", "[", "k", "]", ",", "\n", "num_layers", "=", "1", ",", "\n", "bidirectional", "=", "self", ".", "_bidirectional", ",", "\n", ")", "\n", "self", ".", "add_module", "(", "f\"gru_{k}\"", ",", "gru_layer", ")", "\n", "self", ".", "_gru_layers", ".", "append", "(", "gru_layer", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.seq2seq_encoders.stacked_gru.StackedGRU.get_input_dim": [[86, 88], ["None"], "methods", ["None"], ["", "", "def", "get_input_dim", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "_input_dim", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.seq2seq_encoders.stacked_gru.StackedGRU.get_output_dim": [[89, 91], ["None"], "methods", ["None"], ["", "def", "get_output_dim", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "_output_dim", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.seq2seq_encoders.stacked_gru.StackedGRU.is_bidirectional": [[92, 95], ["None"], "methods", ["None"], ["", "@", "overrides", "\n", "def", "is_bidirectional", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "_bidirectional", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.seq2seq_encoders.stacked_gru.StackedGRU.forward": [[96, 124], ["gru", "range", "gru"], "methods", ["None"], ["", "@", "overrides", "\n", "def", "forward", "(", "\n", "self", ",", "\n", "inputs", ":", "torch", ".", "Tensor", ",", "\n", "mask", ":", "torch", ".", "LongTensor", "=", "None", ",", "# pylint: disable=arguments-differ", "\n", ")", "->", "torch", ".", "FloatTensor", ":", "\n", "        ", "\"\"\"\n        Parameters\n        ----------\n        inputs : ``torch.FloatTensor``, required.\n            A tensor of shape (batch_size, timesteps, input_dim)\n        mask : ``torch.FloatTensor``, optional (default = None).\n            A tensor of shape (batch_size, timesteps).\n\n        Returns\n        -------\n        A tensor of shape (batch_size, timesteps, output_projection_dim),\n        where output_projection_dim = input_dim by default.\n        \"\"\"", "\n", "gru", "=", "self", ".", "_gru_layers", "[", "0", "]", "\n", "outputs", ",", "_", "=", "gru", "(", "inputs", ")", "\n", "\n", "for", "k", "in", "range", "(", "1", ",", "self", ".", "_num_layers", ")", ":", "\n", "            ", "gru", "=", "self", ".", "_gru_layers", "[", "k", "]", "\n", "next_outputs", ",", "_", "=", "gru", "(", "outputs", ")", "\n", "outputs", "=", "next_outputs", "\n", "\n", "", "return", "outputs", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.seq2seq_encoders.stacked_gru.StackedGRU.from_params": [[125, 140], ["params.pop_int", "params.pop", "params.pop", "params.pop_int", "params.pop_bool", "params.assert_empty", "cls"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.params.Params.pop_int", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.params.Params.pop", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.params.Params.pop", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.params.Params.pop_int", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.params.Params.pop_bool", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.params.Params.assert_empty"], ["", "@", "classmethod", "\n", "def", "from_params", "(", "cls", ",", "params", ":", "Params", ")", "->", "\"StackedGRU\"", ":", "\n", "        ", "input_dim", "=", "params", ".", "pop_int", "(", "\"input_dim\"", ")", "\n", "hidden_sizes", "=", "params", ".", "pop", "(", "\"hidden_sizes\"", ")", "\n", "dropouts", "=", "params", ".", "pop", "(", "\"dropouts\"", ",", "None", ")", "\n", "num_layers", "=", "params", ".", "pop_int", "(", "\"num_layers\"", ")", "\n", "bidirectional", "=", "params", ".", "pop_bool", "(", "\"bidirectional\"", ")", "\n", "params", ".", "assert_empty", "(", "cls", ".", "__name__", ")", "\n", "\n", "return", "cls", "(", "\n", "input_dim", "=", "input_dim", ",", "\n", "hidden_sizes", "=", "hidden_sizes", ",", "\n", "num_layers", "=", "num_layers", ",", "\n", "bidirectional", "=", "bidirectional", ",", "\n", "dropouts", "=", "dropouts", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.seq2seq_encoders.intra_sentence_attention.IntraSentenceAttentionEncoder.__init__": [[57, 99], ["allennlp.modules.similarity_functions.DotProductSimilarity", "allennlp.modules.seq2seq_encoders.seq2seq_encoder.Seq2SeqEncoder.__init__", "allennlp.modules.matrix_attention.legacy_matrix_attention.LegacyMatrixAttention", "isinstance", "allennlp.nn.util.get_combined_dim", "torch.nn.Linear", "torch.nn.Linear", "allennlp.common.checks.ConfigurationError", "allennlp.common.checks.ConfigurationError", "allennlp.common.checks.ConfigurationError"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.graph_util.plot.GraphPlot.__init__", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.nn.util.get_combined_dim"], ["def", "__init__", "(", "\n", "self", ",", "\n", "input_dim", ":", "int", ",", "\n", "projection_dim", ":", "int", "=", "None", ",", "\n", "similarity_function", ":", "SimilarityFunction", "=", "DotProductSimilarity", "(", ")", ",", "\n", "num_attention_heads", ":", "int", "=", "1", ",", "\n", "combination", ":", "str", "=", "\"1,2\"", ",", "\n", "output_dim", ":", "int", "=", "None", ",", "\n", ")", "->", "None", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "_input_dim", "=", "input_dim", "\n", "if", "projection_dim", ":", "\n", "            ", "self", ".", "_projection", "=", "torch", ".", "nn", ".", "Linear", "(", "input_dim", ",", "projection_dim", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "_projection", "=", "lambda", "x", ":", "x", "\n", "projection_dim", "=", "input_dim", "\n", "", "self", ".", "_matrix_attention", "=", "LegacyMatrixAttention", "(", "similarity_function", ")", "\n", "self", ".", "_num_attention_heads", "=", "num_attention_heads", "\n", "if", "isinstance", "(", "similarity_function", ",", "MultiHeadedSimilarity", ")", ":", "\n", "            ", "if", "num_attention_heads", "==", "1", ":", "\n", "                ", "raise", "ConfigurationError", "(", "\n", "\"Similarity function has multiple heads but encoder doesn't\"", "\n", ")", "\n", "", "if", "num_attention_heads", "!=", "similarity_function", ".", "num_heads", ":", "\n", "                ", "raise", "ConfigurationError", "(", "\n", "\"Number of heads don't match between similarity function \"", "\n", "\"and encoder: %d, %d\"", "\n", "%", "(", "num_attention_heads", ",", "similarity_function", ".", "num_heads", ")", "\n", ")", "\n", "", "", "elif", "num_attention_heads", ">", "1", ":", "\n", "            ", "raise", "ConfigurationError", "(", "\n", "\"Encoder has multiple heads but similarity function doesn't\"", "\n", ")", "\n", "", "self", ".", "_combination", "=", "combination", "\n", "\n", "combined_dim", "=", "util", ".", "get_combined_dim", "(", "combination", ",", "[", "input_dim", ",", "projection_dim", "]", ")", "\n", "if", "output_dim", ":", "\n", "            ", "self", ".", "_output_projection", "=", "Linear", "(", "combined_dim", ",", "output_dim", ")", "\n", "self", ".", "_output_dim", "=", "output_dim", "\n", "", "else", ":", "\n", "            ", "self", ".", "_output_projection", "=", "lambda", "x", ":", "x", "\n", "self", ".", "_output_dim", "=", "combined_dim", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.seq2seq_encoders.intra_sentence_attention.IntraSentenceAttentionEncoder.get_input_dim": [[100, 103], ["None"], "methods", ["None"], ["", "", "@", "overrides", "\n", "def", "get_input_dim", "(", "self", ")", "->", "int", ":", "\n", "        ", "return", "self", ".", "_input_dim", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.seq2seq_encoders.intra_sentence_attention.IntraSentenceAttentionEncoder.get_output_dim": [[104, 107], ["None"], "methods", ["None"], ["", "@", "overrides", "\n", "def", "get_output_dim", "(", "self", ")", "->", "int", ":", "\n", "        ", "return", "self", ".", "_output_dim", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.seq2seq_encoders.intra_sentence_attention.IntraSentenceAttentionEncoder.is_bidirectional": [[108, 111], ["None"], "methods", ["None"], ["", "@", "overrides", "\n", "def", "is_bidirectional", "(", "self", ")", ":", "\n", "        ", "return", "False", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.seq2seq_encoders.intra_sentence_attention.IntraSentenceAttentionEncoder.forward": [[112, 162], ["tokens.size", "util.masked_softmax.IntraSentenceAttentionEncoder._matrix_attention", "allennlp.nn.util.masked_softmax", "util.masked_softmax.IntraSentenceAttentionEncoder._projection", "allennlp.nn.util.weighted_sum", "allennlp.nn.util.combine_tensors", "util.masked_softmax.IntraSentenceAttentionEncoder._output_projection", "similarity_matrix.permute.permute.permute", "similarity_matrix.permute.permute.contiguous", "list", "output_token_representation.permute.permute.view", "output_token_representation.permute.permute.permute", "attended_sentence.view.view.view", "output_token_representation.permute.permute.size"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.nn.util.masked_softmax", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.nn.util.weighted_sum", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.nn.util.combine_tensors"], ["", "@", "overrides", "\n", "def", "forward", "(", "self", ",", "tokens", ":", "torch", ".", "Tensor", ",", "mask", ":", "torch", ".", "Tensor", ")", ":", "\n", "        ", "batch_size", ",", "sequence_length", ",", "_", "=", "tokens", ".", "size", "(", ")", "\n", "# Shape: (batch_size, sequence_length, sequence_length)", "\n", "similarity_matrix", "=", "self", ".", "_matrix_attention", "(", "tokens", ",", "tokens", ")", "\n", "\n", "if", "self", ".", "_num_attention_heads", ">", "1", ":", "\n", "# In this case, the similarity matrix actually has shape", "\n", "# (batch_size, sequence_length, sequence_length, num_heads).  To make the rest of the", "\n", "# logic below easier, we'll permute this to", "\n", "# (batch_size, sequence_length, num_heads, sequence_length).", "\n", "            ", "similarity_matrix", "=", "similarity_matrix", ".", "permute", "(", "0", ",", "1", ",", "3", ",", "2", ")", "\n", "\n", "# Shape: (batch_size, sequence_length, [num_heads,] sequence_length)", "\n", "", "intra_sentence_attention", "=", "util", ".", "masked_softmax", "(", "\n", "similarity_matrix", ".", "contiguous", "(", ")", ",", "mask", "\n", ")", "\n", "\n", "# Shape: (batch_size, sequence_length, projection_dim)", "\n", "output_token_representation", "=", "self", ".", "_projection", "(", "tokens", ")", "\n", "\n", "if", "self", ".", "_num_attention_heads", ">", "1", ":", "\n", "# We need to split and permute the output representation to be", "\n", "# (batch_size, num_heads, sequence_length, projection_dim / num_heads), so that we can", "\n", "# do a proper weighted sum with `intra_sentence_attention`.", "\n", "            ", "shape", "=", "list", "(", "output_token_representation", ".", "size", "(", ")", ")", "\n", "new_shape", "=", "shape", "[", ":", "-", "1", "]", "+", "[", "self", ".", "_num_attention_heads", ",", "-", "1", "]", "\n", "# Shape: (batch_size, sequence_length, num_heads, projection_dim / num_heads)", "\n", "output_token_representation", "=", "output_token_representation", ".", "view", "(", "*", "new_shape", ")", "\n", "# Shape: (batch_size, num_heads, sequence_length, projection_dim / num_heads)", "\n", "output_token_representation", "=", "output_token_representation", ".", "permute", "(", "\n", "0", ",", "2", ",", "1", ",", "3", "\n", ")", "\n", "\n", "# Shape: (batch_size, sequence_length, [num_heads,] projection_dim [/ num_heads])", "\n", "", "attended_sentence", "=", "util", ".", "weighted_sum", "(", "\n", "output_token_representation", ",", "intra_sentence_attention", "\n", ")", "\n", "\n", "if", "self", ".", "_num_attention_heads", ">", "1", ":", "\n", "# Here we concatenate the weighted representation for each head.  We'll accomplish this", "\n", "# just with a resize.", "\n", "# Shape: (batch_size, sequence_length, projection_dim)", "\n", "            ", "attended_sentence", "=", "attended_sentence", ".", "view", "(", "batch_size", ",", "sequence_length", ",", "-", "1", ")", "\n", "\n", "# Shape: (batch_size, sequence_length, combination_dim)", "\n", "", "combined_tensors", "=", "util", ".", "combine_tensors", "(", "\n", "self", ".", "_combination", ",", "[", "tokens", ",", "attended_sentence", "]", "\n", ")", "\n", "return", "self", ".", "_output_projection", "(", "combined_tensors", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.seq2seq_encoders.multi_head_self_attention.MultiHeadSelfAttention.__init__": [[41, 75], ["allennlp.modules.seq2seq_encoders.seq2seq_encoder.Seq2SeqEncoder.__init__", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Dropout", "ValueError", "ValueError"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.graph_util.plot.GraphPlot.__init__"], ["def", "__init__", "(", "\n", "self", ",", "\n", "num_heads", ":", "int", ",", "\n", "input_dim", ":", "int", ",", "\n", "attention_dim", ":", "int", ",", "\n", "values_dim", ":", "int", ",", "\n", "output_projection_dim", ":", "int", "=", "None", ",", "\n", "attention_dropout_prob", ":", "float", "=", "0.1", ",", "\n", ")", "->", "None", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "\n", "self", ".", "_num_heads", "=", "num_heads", "\n", "self", ".", "_input_dim", "=", "input_dim", "\n", "self", ".", "_output_dim", "=", "output_projection_dim", "or", "input_dim", "\n", "self", ".", "_attention_dim", "=", "attention_dim", "\n", "self", ".", "_values_dim", "=", "values_dim", "\n", "\n", "if", "attention_dim", "%", "num_heads", "!=", "0", ":", "\n", "            ", "raise", "ValueError", "(", "\n", "f\"Key size ({attention_dim}) must be divisible by the number of \"", "\n", "f\"attention heads ({num_heads}).\"", "\n", ")", "\n", "\n", "", "if", "values_dim", "%", "num_heads", "!=", "0", ":", "\n", "            ", "raise", "ValueError", "(", "\n", "f\"Value size ({values_dim}) must be divisible by the number of \"", "\n", "f\"attention heads ({num_heads}).\"", "\n", ")", "\n", "\n", "", "self", ".", "_combined_projection", "=", "Linear", "(", "input_dim", ",", "2", "*", "attention_dim", "+", "values_dim", ")", "\n", "\n", "self", ".", "_scale", "=", "(", "input_dim", "//", "num_heads", ")", "**", "0.5", "\n", "self", ".", "_output_projection", "=", "Linear", "(", "values_dim", ",", "self", ".", "_output_dim", ")", "\n", "self", ".", "_attention_dropout", "=", "Dropout", "(", "attention_dropout_prob", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.seq2seq_encoders.multi_head_self_attention.MultiHeadSelfAttention.get_input_dim": [[76, 78], ["None"], "methods", ["None"], ["", "def", "get_input_dim", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "_input_dim", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.seq2seq_encoders.multi_head_self_attention.MultiHeadSelfAttention.get_output_dim": [[79, 81], ["None"], "methods", ["None"], ["", "def", "get_output_dim", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "_output_dim", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.seq2seq_encoders.multi_head_self_attention.MultiHeadSelfAttention.is_bidirectional": [[82, 85], ["None"], "methods", ["None"], ["", "@", "overrides", "\n", "def", "is_bidirectional", "(", "self", ")", ":", "\n", "        ", "return", "False", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.seq2seq_encoders.multi_head_self_attention.MultiHeadSelfAttention.forward": [[86, 178], ["inputs.size", "multi_head_self_attention.MultiHeadSelfAttention.MultiHeadSelfAttention._combined_projection", "multi_head_self_attention.MultiHeadSelfAttention.MultiHeadSelfAttention.split", "queries.contiguous.contiguous.contiguous", "keys.contiguous.contiguous.contiguous", "torch.cat().contiguous", "torch.cat().contiguous.view", "values_per_head.view.view.transpose().contiguous", "values_per_head.view.view.view", "queries.contiguous.contiguous.view", "queries_per_head.view.view.transpose().contiguous", "queries_per_head.view.view.view", "keys.contiguous.contiguous.view", "keys_per_head.view.view.transpose().contiguous", "keys_per_head.view.view.view", "torch.bmm", "allennlp.nn.util.masked_softmax", "multi_head_self_attention.MultiHeadSelfAttention.MultiHeadSelfAttention._attention_dropout", "allennlp.nn.util.weighted_sum", "multi_head_self_attention.MultiHeadSelfAttention.MultiHeadSelfAttention.view", "multi_head_self_attention.MultiHeadSelfAttention.MultiHeadSelfAttention.transpose().contiguous", "multi_head_self_attention.MultiHeadSelfAttention.MultiHeadSelfAttention.view", "multi_head_self_attention.MultiHeadSelfAttention.MultiHeadSelfAttention._output_projection", "inputs.new_ones", "int", "int", "int", "int", "int", "int", "keys_per_head.view.view.transpose", "inputs.new_ones.repeat().view", "int", "torch.cat", "values_per_head.view.view.transpose", "queries_per_head.view.view.transpose", "keys_per_head.view.view.transpose", "multi_head_self_attention.MultiHeadSelfAttention.MultiHeadSelfAttention.transpose", "inputs.new_ones.repeat"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.nn.util.masked_softmax", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.nn.util.weighted_sum"], ["", "@", "overrides", "\n", "def", "forward", "(", "\n", "self", ",", "inputs", ":", "torch", ".", "Tensor", ",", "mask", ":", "torch", ".", "LongTensor", "=", "None", "\n", ")", "->", "torch", ".", "FloatTensor", ":", "\n", "        ", "\"\"\"\n        # Parameters\n\n        inputs : `torch.FloatTensor`, required.\n            A tensor of shape (batch_size, timesteps, input_dim)\n        mask : `torch.FloatTensor`, optional (default = None).\n            A tensor of shape (batch_size, timesteps).\n\n        # Returns\n\n        A tensor of shape (batch_size, timesteps, output_projection_dim),\n        where output_projection_dim = input_dim by default.\n        \"\"\"", "\n", "num_heads", "=", "self", ".", "_num_heads", "\n", "\n", "batch_size", ",", "timesteps", ",", "_", "=", "inputs", ".", "size", "(", ")", "\n", "if", "mask", "is", "None", ":", "\n", "            ", "mask", "=", "inputs", ".", "new_ones", "(", "batch_size", ",", "timesteps", ")", "\n", "\n", "# Shape (batch_size, timesteps, 2 * attention_dim + values_dim)", "\n", "", "combined_projection", "=", "self", ".", "_combined_projection", "(", "inputs", ")", "\n", "# split by attention dim - if values_dim > attention_dim, we will get more", "\n", "# than 3 elements returned. All of the rest are the values vector, so we", "\n", "# just concatenate them back together again below.", "\n", "queries", ",", "keys", ",", "*", "values", "=", "combined_projection", ".", "split", "(", "self", ".", "_attention_dim", ",", "-", "1", ")", "\n", "queries", "=", "queries", ".", "contiguous", "(", ")", "\n", "keys", "=", "keys", ".", "contiguous", "(", ")", "\n", "values", "=", "torch", ".", "cat", "(", "values", ",", "-", "1", ")", ".", "contiguous", "(", ")", "\n", "# Shape (num_heads * batch_size, timesteps, values_dim / num_heads)", "\n", "values_per_head", "=", "values", ".", "view", "(", "\n", "batch_size", ",", "timesteps", ",", "num_heads", ",", "int", "(", "self", ".", "_values_dim", "/", "num_heads", ")", "\n", ")", "\n", "values_per_head", "=", "values_per_head", ".", "transpose", "(", "1", ",", "2", ")", ".", "contiguous", "(", ")", "\n", "values_per_head", "=", "values_per_head", ".", "view", "(", "\n", "batch_size", "*", "num_heads", ",", "timesteps", ",", "int", "(", "self", ".", "_values_dim", "/", "num_heads", ")", "\n", ")", "\n", "\n", "# Shape (num_heads * batch_size, timesteps, attention_dim / num_heads)", "\n", "queries_per_head", "=", "queries", ".", "view", "(", "\n", "batch_size", ",", "timesteps", ",", "num_heads", ",", "int", "(", "self", ".", "_attention_dim", "/", "num_heads", ")", "\n", ")", "\n", "queries_per_head", "=", "queries_per_head", ".", "transpose", "(", "1", ",", "2", ")", ".", "contiguous", "(", ")", "\n", "queries_per_head", "=", "queries_per_head", ".", "view", "(", "\n", "batch_size", "*", "num_heads", ",", "timesteps", ",", "int", "(", "self", ".", "_attention_dim", "/", "num_heads", ")", "\n", ")", "\n", "\n", "# Shape (num_heads * batch_size, timesteps, attention_dim / num_heads)", "\n", "keys_per_head", "=", "keys", ".", "view", "(", "\n", "batch_size", ",", "timesteps", ",", "num_heads", ",", "int", "(", "self", ".", "_attention_dim", "/", "num_heads", ")", "\n", ")", "\n", "keys_per_head", "=", "keys_per_head", ".", "transpose", "(", "1", ",", "2", ")", ".", "contiguous", "(", ")", "\n", "keys_per_head", "=", "keys_per_head", ".", "view", "(", "\n", "batch_size", "*", "num_heads", ",", "timesteps", ",", "int", "(", "self", ".", "_attention_dim", "/", "num_heads", ")", "\n", ")", "\n", "\n", "# shape (num_heads * batch_size, timesteps, timesteps)", "\n", "scaled_similarities", "=", "torch", ".", "bmm", "(", "\n", "queries_per_head", "/", "self", ".", "_scale", ",", "keys_per_head", ".", "transpose", "(", "1", ",", "2", ")", "\n", ")", "\n", "\n", "# shape (num_heads * batch_size, timesteps, timesteps)", "\n", "# Normalise the distributions, using the same mask for all heads.", "\n", "attention", "=", "masked_softmax", "(", "\n", "scaled_similarities", ",", "\n", "mask", ".", "repeat", "(", "1", ",", "num_heads", ")", ".", "view", "(", "batch_size", "*", "num_heads", ",", "timesteps", ")", ",", "\n", "memory_efficient", "=", "True", ",", "\n", ")", "\n", "attention", "=", "self", ".", "_attention_dropout", "(", "attention", ")", "\n", "\n", "# Take a weighted sum of the values with respect to the attention", "\n", "# distributions for each element in the num_heads * batch_size dimension.", "\n", "# shape (num_heads * batch_size, timesteps, values_dim/num_heads)", "\n", "outputs", "=", "weighted_sum", "(", "values_per_head", ",", "attention", ")", "\n", "\n", "# Reshape back to original shape (batch_size, timesteps, values_dim)", "\n", "# shape (batch_size, num_heads, timesteps, values_dim/num_heads)", "\n", "outputs", "=", "outputs", ".", "view", "(", "\n", "batch_size", ",", "num_heads", ",", "timesteps", ",", "int", "(", "self", ".", "_values_dim", "/", "num_heads", ")", "\n", ")", "\n", "# shape (batch_size, timesteps, num_heads, values_dim/num_heads)", "\n", "outputs", "=", "outputs", ".", "transpose", "(", "1", ",", "2", ")", ".", "contiguous", "(", ")", "\n", "# shape (batch_size, timesteps, values_dim)", "\n", "outputs", "=", "outputs", ".", "view", "(", "batch_size", ",", "timesteps", ",", "self", ".", "_values_dim", ")", "\n", "\n", "# Project back to original input size.", "\n", "# shape (batch_size, timesteps, input_size)", "\n", "outputs", "=", "self", ".", "_output_projection", "(", "outputs", ")", "\n", "return", "outputs", "\n", "", "", ""]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.seq2seq_encoders.stacked_self_attention.StackedSelfAttentionEncoder.__init__": [[64, 127], ["allennlp.modules.seq2seq_encoders.seq2seq_encoder.Seq2SeqEncoder.__init__", "range", "torch.nn.Dropout", "stacked_self_attention.StackedSelfAttentionEncoder.StackedSelfAttentionEncoder._attention_layers[].get_output_dim", "allennlp.modules.feedforward.FeedForward", "stacked_self_attention.StackedSelfAttentionEncoder.StackedSelfAttentionEncoder.add_module", "stacked_self_attention.StackedSelfAttentionEncoder.StackedSelfAttentionEncoder._feedfoward_layers.append", "allennlp.modules.layer_norm.LayerNorm", "stacked_self_attention.StackedSelfAttentionEncoder.StackedSelfAttentionEncoder.add_module", "stacked_self_attention.StackedSelfAttentionEncoder.StackedSelfAttentionEncoder._feed_forward_layer_norm_layers.append", "allennlp.modules.seq2seq_encoders.multi_head_self_attention.MultiHeadSelfAttention", "stacked_self_attention.StackedSelfAttentionEncoder.StackedSelfAttentionEncoder.add_module", "stacked_self_attention.StackedSelfAttentionEncoder.StackedSelfAttentionEncoder._attention_layers.append", "allennlp.modules.layer_norm.LayerNorm", "stacked_self_attention.StackedSelfAttentionEncoder.StackedSelfAttentionEncoder.add_module", "stacked_self_attention.StackedSelfAttentionEncoder.StackedSelfAttentionEncoder._layer_norm_layers.append", "allennlp.modules.feedforward.FeedForward.get_output_dim", "allennlp.modules.seq2seq_encoders.multi_head_self_attention.MultiHeadSelfAttention.get_output_dim", "allennlp.nn.activations.Activation.by_name", "allennlp.nn.activations.Activation.by_name"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.graph_util.plot.GraphPlot.__init__", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.seq2seq_decoders.auto_regressive_seq_decoder.AutoRegressiveSeqDecoder.get_output_dim", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.seq2seq_decoders.auto_regressive_seq_decoder.AutoRegressiveSeqDecoder.get_output_dim", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.seq2seq_decoders.auto_regressive_seq_decoder.AutoRegressiveSeqDecoder.get_output_dim", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.registrable.Registrable.by_name", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.registrable.Registrable.by_name"], ["def", "__init__", "(", "\n", "self", ",", "\n", "input_dim", ":", "int", ",", "\n", "hidden_dim", ":", "int", ",", "\n", "projection_dim", ":", "int", ",", "\n", "feedforward_hidden_dim", ":", "int", ",", "\n", "num_layers", ":", "int", ",", "\n", "num_attention_heads", ":", "int", ",", "\n", "use_positional_encoding", ":", "bool", "=", "True", ",", "\n", "dropout_prob", ":", "float", "=", "0.1", ",", "\n", "residual_dropout_prob", ":", "float", "=", "0.2", ",", "\n", "attention_dropout_prob", ":", "float", "=", "0.1", ",", "\n", ")", "->", "None", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "\n", "self", ".", "_use_positional_encoding", "=", "use_positional_encoding", "\n", "self", ".", "_attention_layers", ":", "List", "[", "MultiHeadSelfAttention", "]", "=", "[", "]", "\n", "self", ".", "_feedfoward_layers", ":", "List", "[", "FeedForward", "]", "=", "[", "]", "\n", "self", ".", "_layer_norm_layers", ":", "List", "[", "LayerNorm", "]", "=", "[", "]", "\n", "self", ".", "_feed_forward_layer_norm_layers", ":", "List", "[", "LayerNorm", "]", "=", "[", "]", "\n", "\n", "feedfoward_input_dim", "=", "input_dim", "\n", "for", "i", "in", "range", "(", "num_layers", ")", ":", "\n", "            ", "feedfoward", "=", "FeedForward", "(", "\n", "feedfoward_input_dim", ",", "\n", "activations", "=", "[", "\n", "Activation", ".", "by_name", "(", "\"relu\"", ")", "(", ")", ",", "\n", "Activation", ".", "by_name", "(", "\"linear\"", ")", "(", ")", ",", "\n", "]", ",", "\n", "hidden_dims", "=", "[", "feedforward_hidden_dim", ",", "hidden_dim", "]", ",", "\n", "num_layers", "=", "2", ",", "\n", "dropout", "=", "dropout_prob", ",", "\n", ")", "\n", "\n", "# Note: Please use `ModuleList` in new code. It provides better", "\n", "# support for running on multiple GPUs. We've kept `add_module` here", "\n", "# solely for backwards compatibility with existing serialized models.", "\n", "self", ".", "add_module", "(", "f\"feedforward_{i}\"", ",", "feedfoward", ")", "\n", "self", ".", "_feedfoward_layers", ".", "append", "(", "feedfoward", ")", "\n", "\n", "feedforward_layer_norm", "=", "LayerNorm", "(", "feedfoward", ".", "get_output_dim", "(", ")", ")", "\n", "self", ".", "add_module", "(", "f\"feedforward_layer_norm_{i}\"", ",", "feedforward_layer_norm", ")", "\n", "self", ".", "_feed_forward_layer_norm_layers", ".", "append", "(", "feedforward_layer_norm", ")", "\n", "\n", "self_attention", "=", "MultiHeadSelfAttention", "(", "\n", "num_heads", "=", "num_attention_heads", ",", "\n", "input_dim", "=", "hidden_dim", ",", "\n", "attention_dim", "=", "projection_dim", ",", "\n", "values_dim", "=", "projection_dim", ",", "\n", "attention_dropout_prob", "=", "attention_dropout_prob", ",", "\n", ")", "\n", "self", ".", "add_module", "(", "f\"self_attention_{i}\"", ",", "self_attention", ")", "\n", "self", ".", "_attention_layers", ".", "append", "(", "self_attention", ")", "\n", "\n", "layer_norm", "=", "LayerNorm", "(", "self_attention", ".", "get_output_dim", "(", ")", ")", "\n", "self", ".", "add_module", "(", "f\"layer_norm_{i}\"", ",", "layer_norm", ")", "\n", "self", ".", "_layer_norm_layers", ".", "append", "(", "layer_norm", ")", "\n", "\n", "feedfoward_input_dim", "=", "hidden_dim", "\n", "\n", "", "self", ".", "dropout", "=", "Dropout", "(", "residual_dropout_prob", ")", "\n", "self", ".", "_input_dim", "=", "input_dim", "\n", "self", ".", "_output_dim", "=", "self", ".", "_attention_layers", "[", "-", "1", "]", ".", "get_output_dim", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.seq2seq_encoders.stacked_self_attention.StackedSelfAttentionEncoder.get_input_dim": [[128, 131], ["None"], "methods", ["None"], ["", "@", "overrides", "\n", "def", "get_input_dim", "(", "self", ")", "->", "int", ":", "\n", "        ", "return", "self", ".", "_input_dim", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.seq2seq_encoders.stacked_self_attention.StackedSelfAttentionEncoder.get_output_dim": [[132, 135], ["None"], "methods", ["None"], ["", "@", "overrides", "\n", "def", "get_output_dim", "(", "self", ")", "->", "int", ":", "\n", "        ", "return", "self", ".", "_output_dim", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.seq2seq_encoders.stacked_self_attention.StackedSelfAttentionEncoder.is_bidirectional": [[136, 139], ["None"], "methods", ["None"], ["", "@", "overrides", "\n", "def", "is_bidirectional", "(", "self", ")", ":", "\n", "        ", "return", "False", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.seq2seq_encoders.stacked_self_attention.StackedSelfAttentionEncoder.forward": [[140, 174], ["range", "allennlp.nn.util.add_positional_features", "len", "getattr", "getattr", "getattr", "getattr", "getattr.", "stacked_self_attention.StackedSelfAttentionEncoder.StackedSelfAttentionEncoder.dropout", "getattr.", "getattr.", "getattr.size", "cached_input.size", "getattr.", "stacked_self_attention.StackedSelfAttentionEncoder.StackedSelfAttentionEncoder.dropout"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.nn.util.add_positional_features"], ["", "@", "overrides", "\n", "def", "forward", "(", "self", ",", "inputs", ":", "torch", ".", "Tensor", ",", "mask", ":", "torch", ".", "Tensor", ")", ":", "\n", "        ", "if", "self", ".", "_use_positional_encoding", ":", "\n", "            ", "output", "=", "add_positional_features", "(", "inputs", ")", "\n", "", "else", ":", "\n", "            ", "output", "=", "inputs", "\n", "", "for", "i", "in", "range", "(", "len", "(", "self", ".", "_attention_layers", ")", ")", ":", "\n", "# It's necessary to use `getattr` here because the elements stored", "\n", "# in the lists are not replicated by torch.nn.parallel.replicate", "\n", "# when running on multiple GPUs. Please use `ModuleList` in new", "\n", "# code. It handles this issue transparently. We've kept `add_module`", "\n", "# (in conjunction with `getattr`) solely for backwards compatibility", "\n", "# with existing serialized models.", "\n", "            ", "attention", "=", "getattr", "(", "self", ",", "f\"self_attention_{i}\"", ")", "\n", "feedforward", "=", "getattr", "(", "self", ",", "f\"feedforward_{i}\"", ")", "\n", "feedforward_layer_norm", "=", "getattr", "(", "self", ",", "f\"feedforward_layer_norm_{i}\"", ")", "\n", "layer_norm", "=", "getattr", "(", "self", ",", "f\"layer_norm_{i}\"", ")", "\n", "cached_input", "=", "output", "\n", "# Project output of attention encoder through a feedforward", "\n", "# network and back to the input size for the next layer.", "\n", "# shape (batch_size, timesteps, input_size)", "\n", "feedforward_output", "=", "feedforward", "(", "output", ")", "\n", "feedforward_output", "=", "self", ".", "dropout", "(", "feedforward_output", ")", "\n", "if", "feedforward_output", ".", "size", "(", ")", "==", "cached_input", ".", "size", "(", ")", ":", "\n", "# First layer might have the wrong size for highway", "\n", "# layers, so we exclude it here.", "\n", "                ", "feedforward_output", "=", "feedforward_layer_norm", "(", "\n", "feedforward_output", "+", "cached_input", "\n", ")", "\n", "# shape (batch_size, sequence_length, hidden_dim)", "\n", "", "attention_output", "=", "attention", "(", "feedforward_output", ",", "mask", ")", "\n", "output", "=", "layer_norm", "(", "self", ".", "dropout", "(", "attention_output", ")", "+", "feedforward_output", ")", "\n", "\n", "", "return", "output", "\n", "", "", ""]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.seq2seq_encoders.seq2seq_encoder.Seq2SeqEncoder.get_input_dim": [[16, 23], ["None"], "methods", ["None"], ["def", "get_input_dim", "(", "self", ")", "->", "int", ":", "\n", "        ", "\"\"\"\n        Returns the dimension of the vector input for each element in the sequence input\n        to a `Seq2SeqEncoder`. This is `not` the shape of the input tensor, but the\n        last element of that shape.\n        \"\"\"", "\n", "raise", "NotImplementedError", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.seq2seq_encoders.seq2seq_encoder.Seq2SeqEncoder.get_output_dim": [[24, 30], ["None"], "methods", ["None"], ["", "def", "get_output_dim", "(", "self", ")", "->", "int", ":", "\n", "        ", "\"\"\"\n        Returns the dimension of each vector in the sequence output by this `Seq2SeqEncoder`.\n        This is `not` the shape of the returned tensor, but the last element of that shape.\n        \"\"\"", "\n", "raise", "NotImplementedError", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.seq2seq_encoders.seq2seq_encoder.Seq2SeqEncoder.is_bidirectional": [[31, 38], ["None"], "methods", ["None"], ["", "def", "is_bidirectional", "(", "self", ")", "->", "bool", ":", "\n", "        ", "\"\"\"\n        Returns `True` if this encoder is bidirectional.  If so, we assume the forward direction\n        of the encoder is the first half of the final dimension, and the backward direction is the\n        second half.\n        \"\"\"", "\n", "raise", "NotImplementedError", "\n", "", "", ""]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.seq2seq_encoders.feedforward_encoder.FeedForwardEncoder.__init__": [[14, 17], ["allennlp.modules.seq2seq_encoders.seq2seq_encoder.Seq2SeqEncoder.__init__"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.graph_util.plot.GraphPlot.__init__"], ["def", "__init__", "(", "self", ",", "feedforward", ":", "FeedForward", ")", "->", "None", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "_feedforward", "=", "feedforward", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.seq2seq_encoders.feedforward_encoder.FeedForwardEncoder.get_input_dim": [[18, 21], ["feedforward_encoder.FeedForwardEncoder._feedforward.get_input_dim"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.seq2vec_encoders.cnn_highway_encoder.CnnHighwayEncoder.get_input_dim"], ["", "@", "overrides", "\n", "def", "get_input_dim", "(", "self", ")", "->", "int", ":", "\n", "        ", "return", "self", ".", "_feedforward", ".", "get_input_dim", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.seq2seq_encoders.feedforward_encoder.FeedForwardEncoder.get_output_dim": [[22, 25], ["feedforward_encoder.FeedForwardEncoder._feedforward.get_output_dim"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.seq2seq_decoders.auto_regressive_seq_decoder.AutoRegressiveSeqDecoder.get_output_dim"], ["", "@", "overrides", "\n", "def", "get_output_dim", "(", "self", ")", "->", "int", ":", "\n", "        ", "return", "self", ".", "_feedforward", ".", "get_output_dim", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.seq2seq_encoders.feedforward_encoder.FeedForwardEncoder.is_bidirectional": [[26, 29], ["None"], "methods", ["None"], ["", "@", "overrides", "\n", "def", "is_bidirectional", "(", "self", ")", "->", "bool", ":", "\n", "        ", "return", "False", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.seq2seq_encoders.feedforward_encoder.FeedForwardEncoder.forward": [[30, 51], ["feedforward_encoder.FeedForwardEncoder._feedforward", "feedforward_encoder.FeedForwardEncoder._feedforward", "mask.unsqueeze().float", "mask.unsqueeze"], "methods", ["None"], ["", "@", "overrides", "\n", "def", "forward", "(", "\n", "self", ",", "inputs", ":", "torch", ".", "Tensor", ",", "mask", ":", "torch", ".", "LongTensor", "=", "None", "\n", ")", "->", "torch", ".", "Tensor", ":", "\n", "        ", "\"\"\"\n        # Parameters\n\n        inputs : `torch.Tensor`, required.\n            A tensor of shape (batch_size, timesteps, input_dim)\n        mask : `torch.LongTensor`, optional (default = None).\n            A tensor of shape (batch_size, timesteps).\n\n        # Returns\n\n        A tensor of shape (batch_size, timesteps, output_dim).\n        \"\"\"", "\n", "if", "mask", "is", "None", ":", "\n", "            ", "return", "self", ".", "_feedforward", "(", "inputs", ")", "\n", "", "else", ":", "\n", "            ", "outputs", "=", "self", ".", "_feedforward", "(", "inputs", ")", "\n", "return", "outputs", "*", "mask", ".", "unsqueeze", "(", "dim", "=", "-", "1", ")", ".", "float", "(", ")", "\n", "", "", "", ""]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.seq2seq_encoders.compose_encoder.ComposeEncoder.__init__": [[21, 46], ["allennlp.modules.seq2seq_encoders.seq2seq_encoder.Seq2SeqEncoder.__init__", "enumerate", "all", "any", "compose_encoder.ComposeEncoder.add_module", "ValueError", "len", "ValueError", "encoder.is_bidirectional", "encoder.is_bidirectional", "ValueError", "last_enc.get_output_dim", "enc.get_input_dim"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.graph_util.plot.GraphPlot.__init__", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.seq2seq_encoders.qanet_encoder.QaNetEncoderBlock.is_bidirectional", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.seq2seq_encoders.qanet_encoder.QaNetEncoderBlock.is_bidirectional", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.seq2seq_decoders.auto_regressive_seq_decoder.AutoRegressiveSeqDecoder.get_output_dim", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.seq2vec_encoders.cnn_highway_encoder.CnnHighwayEncoder.get_input_dim"], ["def", "__init__", "(", "self", ",", "encoders", ":", "List", "[", "Seq2SeqEncoder", "]", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "encoders", "=", "encoders", "\n", "for", "idx", ",", "encoder", "in", "enumerate", "(", "encoders", ")", ":", "\n", "            ", "self", ".", "add_module", "(", "\"encoder%d\"", "%", "idx", ",", "encoder", ")", "\n", "\n", "# Compute bidirectionality.", "\n", "", "all_bidirectional", "=", "all", "(", "encoder", ".", "is_bidirectional", "(", ")", "for", "encoder", "in", "encoders", ")", "\n", "any_bidirectional", "=", "any", "(", "encoder", ".", "is_bidirectional", "(", ")", "for", "encoder", "in", "encoders", ")", "\n", "self", ".", "bidirectional", "=", "all_bidirectional", "\n", "\n", "if", "all_bidirectional", "!=", "any_bidirectional", ":", "\n", "            ", "raise", "ValueError", "(", "\"All encoders need to match in bidirectionality.\"", ")", "\n", "\n", "", "if", "len", "(", "self", ".", "encoders", ")", "<", "1", ":", "\n", "            ", "raise", "ValueError", "(", "\"Need at least one encoder.\"", ")", "\n", "\n", "", "last_enc", "=", "None", "\n", "for", "enc", "in", "encoders", ":", "\n", "            ", "if", "(", "\n", "last_enc", "is", "not", "None", "\n", "and", "last_enc", ".", "get_output_dim", "(", ")", "!=", "enc", ".", "get_input_dim", "(", ")", "\n", ")", ":", "\n", "                ", "raise", "ValueError", "(", "\"Encoder input and output dimensions don't match.\"", ")", "\n", "", "last_enc", "=", "enc", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.seq2seq_encoders.compose_encoder.ComposeEncoder.forward": [[47, 68], ["encoder"], "methods", ["None"], ["", "", "@", "overrides", "\n", "def", "forward", "(", "\n", "self", ",", "# pylint: disable=arguments-differ", "\n", "inputs", ":", "torch", ".", "Tensor", ",", "\n", "mask", ":", "torch", ".", "LongTensor", "=", "None", ",", "\n", ")", "->", "torch", ".", "Tensor", ":", "\n", "        ", "\"\"\"\n        # Parameters\n\n        inputs : `torch.Tensor`, required.\n            A tensor of shape (batch_size, timesteps, input_dim)\n        mask : `torch.LongTensor`, optional (default = None).\n            A tensor of shape (batch_size, timesteps).\n\n        # Returns\n\n        A tensor computed by composing the sequence of encoders.\n        \"\"\"", "\n", "for", "encoder", "in", "self", ".", "encoders", ":", "\n", "            ", "inputs", "=", "encoder", "(", "inputs", ",", "mask", ")", "\n", "", "return", "inputs", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.seq2seq_encoders.compose_encoder.ComposeEncoder.get_input_dim": [[69, 72], ["compose_encoder.ComposeEncoder.encoders[].get_input_dim"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.seq2vec_encoders.cnn_highway_encoder.CnnHighwayEncoder.get_input_dim"], ["", "@", "overrides", "\n", "def", "get_input_dim", "(", "self", ")", "->", "int", ":", "\n", "        ", "return", "self", ".", "encoders", "[", "0", "]", ".", "get_input_dim", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.seq2seq_encoders.compose_encoder.ComposeEncoder.get_output_dim": [[73, 76], ["compose_encoder.ComposeEncoder.encoders[].get_output_dim"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.seq2seq_decoders.auto_regressive_seq_decoder.AutoRegressiveSeqDecoder.get_output_dim"], ["", "@", "overrides", "\n", "def", "get_output_dim", "(", "self", ")", "->", "int", ":", "\n", "        ", "return", "self", ".", "encoders", "[", "-", "1", "]", ".", "get_output_dim", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.seq2seq_encoders.compose_encoder.ComposeEncoder.is_bidirectional": [[77, 80], ["None"], "methods", ["None"], ["", "@", "overrides", "\n", "def", "is_bidirectional", "(", "self", ")", "->", "bool", ":", "\n", "        ", "return", "self", ".", "bidirectional", "\n", "", "", ""]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.seq2seq_encoders.pytorch_seq2seq_wrapper.PytorchSeq2SeqWrapper.__init__": [[37, 56], ["allennlp.modules.seq2seq_encoders.seq2seq_encoder.Seq2SeqEncoder.__init__", "allennlp.common.checks.ConfigurationError"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.graph_util.plot.GraphPlot.__init__"], ["def", "__init__", "(", "self", ",", "module", ":", "torch", ".", "nn", ".", "Module", ",", "stateful", ":", "bool", "=", "False", ")", "->", "None", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "stateful", ")", "\n", "self", ".", "_module", "=", "module", "\n", "try", ":", "\n", "            ", "if", "not", "self", ".", "_module", ".", "batch_first", ":", "\n", "                ", "raise", "ConfigurationError", "(", "\n", "\"Our encoder semantics assumes batch is always first!\"", "\n", ")", "\n", "", "", "except", "AttributeError", ":", "\n", "            ", "pass", "\n", "\n", "", "try", ":", "\n", "            ", "self", ".", "_is_bidirectional", "=", "self", ".", "_module", ".", "bidirectional", "\n", "", "except", "AttributeError", ":", "\n", "            ", "self", ".", "_is_bidirectional", "=", "False", "\n", "", "if", "self", ".", "_is_bidirectional", ":", "\n", "            ", "self", ".", "_num_directions", "=", "2", "\n", "", "else", ":", "\n", "            ", "self", ".", "_num_directions", "=", "1", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.seq2seq_encoders.pytorch_seq2seq_wrapper.PytorchSeq2SeqWrapper.get_input_dim": [[57, 60], ["None"], "methods", ["None"], ["", "", "@", "overrides", "\n", "def", "get_input_dim", "(", "self", ")", "->", "int", ":", "\n", "        ", "return", "self", ".", "_module", ".", "input_size", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.seq2seq_encoders.pytorch_seq2seq_wrapper.PytorchSeq2SeqWrapper.get_output_dim": [[61, 64], ["None"], "methods", ["None"], ["", "@", "overrides", "\n", "def", "get_output_dim", "(", "self", ")", "->", "int", ":", "\n", "        ", "return", "self", ".", "_module", ".", "hidden_size", "*", "self", ".", "_num_directions", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.seq2seq_encoders.pytorch_seq2seq_wrapper.PytorchSeq2SeqWrapper.is_bidirectional": [[65, 68], ["None"], "methods", ["None"], ["", "@", "overrides", "\n", "def", "is_bidirectional", "(", "self", ")", "->", "bool", ":", "\n", "        ", "return", "self", ".", "_is_bidirectional", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.seq2seq_encoders.pytorch_seq2seq_wrapper.PytorchSeq2SeqWrapper.forward": [[69, 142], ["mask.size", "pytorch_seq2seq_wrapper.PytorchSeq2SeqWrapper.sort_and_run_forward", "torch.nn.utils.rnn.pad_packed_sequence", "torch.cat.size", "torch.cat.index_select", "ValueError", "ValueError", "torch.cat.size", "torch.cat.new_zeros", "torch.cat", "torch.cat.size", "torch.cat.new_zeros", "torch.cat", "pytorch_seq2seq_wrapper.PytorchSeq2SeqWrapper._update_states", "pytorch_seq2seq_wrapper.PytorchSeq2SeqWrapper._module", "isinstance", "torch.cat.size", "state.size", "state.new_zeros", "new_states.append", "torch.cat"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.modules.encoder_base._EncoderBase.sort_and_run_forward", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.modules.encoder_base._EncoderBase._update_states"], ["", "@", "overrides", "\n", "def", "forward", "(", "\n", "self", ",", "\n", "inputs", ":", "torch", ".", "Tensor", ",", "\n", "mask", ":", "torch", ".", "Tensor", ",", "\n", "hidden_state", ":", "torch", ".", "Tensor", "=", "None", ",", "\n", ")", "->", "torch", ".", "Tensor", ":", "\n", "\n", "        ", "if", "self", ".", "stateful", "and", "mask", "is", "None", ":", "\n", "            ", "raise", "ValueError", "(", "\"Always pass a mask with stateful RNNs.\"", ")", "\n", "", "if", "self", ".", "stateful", "and", "hidden_state", "is", "not", "None", ":", "\n", "            ", "raise", "ValueError", "(", "\"Stateful RNNs provide their own initial hidden_state.\"", ")", "\n", "\n", "", "if", "mask", "is", "None", ":", "\n", "            ", "return", "self", ".", "_module", "(", "inputs", ",", "hidden_state", ")", "[", "0", "]", "\n", "\n", "", "batch_size", ",", "total_sequence_length", "=", "mask", ".", "size", "(", ")", "\n", "\n", "(", "\n", "packed_sequence_output", ",", "\n", "final_states", ",", "\n", "restoration_indices", ",", "\n", ")", "=", "self", ".", "sort_and_run_forward", "(", "self", ".", "_module", ",", "inputs", ",", "mask", ",", "hidden_state", ")", "\n", "\n", "unpacked_sequence_tensor", ",", "_", "=", "pad_packed_sequence", "(", "\n", "packed_sequence_output", ",", "batch_first", "=", "True", "\n", ")", "\n", "\n", "num_valid", "=", "unpacked_sequence_tensor", ".", "size", "(", "0", ")", "\n", "# Some RNNs (GRUs) only return one state as a Tensor.  Others (LSTMs) return two.", "\n", "# If one state, use a single element list to handle in a consistent manner below.", "\n", "if", "not", "isinstance", "(", "final_states", ",", "(", "list", ",", "tuple", ")", ")", "and", "self", ".", "stateful", ":", "\n", "            ", "final_states", "=", "[", "final_states", "]", "\n", "\n", "# Add back invalid rows.", "\n", "", "if", "num_valid", "<", "batch_size", ":", "\n", "            ", "_", ",", "length", ",", "output_dim", "=", "unpacked_sequence_tensor", ".", "size", "(", ")", "\n", "zeros", "=", "unpacked_sequence_tensor", ".", "new_zeros", "(", "\n", "batch_size", "-", "num_valid", ",", "length", ",", "output_dim", "\n", ")", "\n", "unpacked_sequence_tensor", "=", "torch", ".", "cat", "(", "[", "unpacked_sequence_tensor", ",", "zeros", "]", ",", "0", ")", "\n", "\n", "# The states also need to have invalid rows added back.", "\n", "if", "self", ".", "stateful", ":", "\n", "                ", "new_states", "=", "[", "]", "\n", "for", "state", "in", "final_states", ":", "\n", "                    ", "num_layers", ",", "_", ",", "state_dim", "=", "state", ".", "size", "(", ")", "\n", "zeros", "=", "state", ".", "new_zeros", "(", "\n", "num_layers", ",", "batch_size", "-", "num_valid", ",", "state_dim", "\n", ")", "\n", "new_states", ".", "append", "(", "torch", ".", "cat", "(", "[", "state", ",", "zeros", "]", ",", "1", ")", ")", "\n", "", "final_states", "=", "new_states", "\n", "\n", "# It's possible to need to pass sequences which are padded to longer than the", "\n", "# max length of the sequence to a Seq2SeqEncoder. However, packing and unpacking", "\n", "# the sequences mean that the returned tensor won't include these dimensions, because", "\n", "# the RNN did not need to process them. We add them back on in the form of zeros here.", "\n", "", "", "sequence_length_difference", "=", "(", "\n", "total_sequence_length", "-", "unpacked_sequence_tensor", ".", "size", "(", "1", ")", "\n", ")", "\n", "if", "sequence_length_difference", ">", "0", ":", "\n", "            ", "zeros", "=", "unpacked_sequence_tensor", ".", "new_zeros", "(", "\n", "batch_size", ",", "\n", "sequence_length_difference", ",", "\n", "unpacked_sequence_tensor", ".", "size", "(", "-", "1", ")", ",", "\n", ")", "\n", "unpacked_sequence_tensor", "=", "torch", ".", "cat", "(", "[", "unpacked_sequence_tensor", ",", "zeros", "]", ",", "1", ")", "\n", "\n", "", "if", "self", ".", "stateful", ":", "\n", "            ", "self", ".", "_update_states", "(", "final_states", ",", "restoration_indices", ")", "\n", "\n", "# Restore the original indices and return the sequence.", "\n", "", "return", "unpacked_sequence_tensor", ".", "index_select", "(", "0", ",", "restoration_indices", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.seq2seq_encoders.gated_cnn_encoder.ResidualBlock.__init__": [[11, 80], ["super().__init__", "torch.nn.ModuleList", "enumerate", "math.sqrt", "torch.nn.Conv1d.weight.data.normal_", "torch.nn.Conv1d.bias.data.zero_", "gated_cnn_encoder.ResidualBlock._convolutions.append", "allennlp.common.checks.ConfigurationError", "len", "torch.nn.Conv1d", "torch.nn.utils.weight_norm", "len", "torch.nn.Conv1d", "ValueError"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.graph_util.plot.GraphPlot.__init__"], ["    ", "def", "__init__", "(", "\n", "self", ",", "\n", "input_dim", ":", "int", ",", "\n", "layers", ":", "Sequence", "[", "Sequence", "[", "int", "]", "]", ",", "\n", "direction", ":", "str", ",", "\n", "do_weight_norm", ":", "bool", "=", "True", ",", "\n", "dropout", ":", "float", "=", "0.0", ",", "\n", ")", "->", "None", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "\n", "self", ".", "dropout", "=", "dropout", "\n", "self", ".", "_convolutions", "=", "torch", ".", "nn", ".", "ModuleList", "(", ")", "\n", "last_dim", "=", "input_dim", "\n", "for", "k", ",", "layer", "in", "enumerate", "(", "layers", ")", ":", "\n", "# We run two convolutions for each block -- one for the", "\n", "# output and one for the gates -- do them at once, and", "\n", "# we'll worry about slicing them in forward", "\n", "            ", "if", "len", "(", "layer", ")", "==", "2", ":", "\n", "# no dilation", "\n", "                ", "conv", "=", "torch", ".", "nn", ".", "Conv1d", "(", "\n", "last_dim", ",", "\n", "layer", "[", "1", "]", "*", "2", ",", "\n", "layer", "[", "0", "]", ",", "\n", "stride", "=", "1", ",", "\n", "padding", "=", "layer", "[", "0", "]", "-", "1", ",", "\n", "bias", "=", "True", ",", "\n", ")", "\n", "", "elif", "len", "(", "layer", ")", "==", "3", ":", "\n", "# a dilation", "\n", "                ", "assert", "layer", "[", "0", "]", "==", "2", ",", "\"only support kernel = 2 for now\"", "\n", "conv", "=", "torch", ".", "nn", ".", "Conv1d", "(", "\n", "last_dim", ",", "\n", "layer", "[", "1", "]", "*", "2", ",", "\n", "layer", "[", "0", "]", ",", "\n", "stride", "=", "1", ",", "\n", "padding", "=", "layer", "[", "2", "]", ",", "\n", "dilation", "=", "layer", "[", "2", "]", ",", "\n", "bias", "=", "True", ",", "\n", ")", "\n", "", "else", ":", "\n", "                ", "raise", "ValueError", "(", "\"each layer must have length 2 or 3\"", ")", "\n", "\n", "# from Convolutional Sequence to Sequence Learning", "\n", "", "if", "k", "==", "0", ":", "\n", "                ", "conv_dropout", "=", "dropout", "\n", "", "else", ":", "\n", "# no dropout", "\n", "                ", "conv_dropout", "=", "0.0", "\n", "", "std", "=", "math", ".", "sqrt", "(", "(", "4", "*", "(", "1.0", "-", "conv_dropout", ")", ")", "/", "(", "layer", "[", "0", "]", "*", "last_dim", ")", ")", "\n", "\n", "conv", ".", "weight", ".", "data", ".", "normal_", "(", "0", ",", "std", "=", "std", ")", "\n", "conv", ".", "bias", ".", "data", ".", "zero_", "(", ")", "\n", "\n", "if", "do_weight_norm", ":", "\n", "# conv.weight.shape == (out_channels, in_channels, kernel width)", "\n", "# in fairseq, conv.weight.shape == ([width, in, out])", "\n", "#   for ConvTBC.  In ConvTBC, weight norm is applied as", "\n", "#   nn.utils.weight_norm(m, dim=2) over the output dimension.", "\n", "# so for regular 1D convs we need to apply over dimension=0", "\n", "                ", "conv", "=", "torch", ".", "nn", ".", "utils", ".", "weight_norm", "(", "conv", ",", "name", "=", "\"weight\"", ",", "dim", "=", "0", ")", "\n", "\n", "", "self", ".", "_convolutions", ".", "append", "(", "conv", ")", "\n", "last_dim", "=", "layer", "[", "1", "]", "\n", "\n", "", "assert", "last_dim", "==", "input_dim", "\n", "\n", "if", "direction", "not", "in", "(", "\"forward\"", ",", "\"backward\"", ")", ":", "\n", "            ", "raise", "ConfigurationError", "(", "f\"invalid direction: {direction}\"", ")", "\n", "", "self", ".", "_direction", "=", "direction", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.seq2seq_encoders.gated_cnn_encoder.ResidualBlock.forward": [[81, 109], ["x.size", "enumerate", "convolution", "torch.nn.functional.glu", "math.sqrt", "torch.nn.functional.dropout", "conv_out.narrow.narrow.size", "conv_out.narrow.narrow.narrow", "conv_out.narrow.narrow.narrow"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "x", ":", "torch", ".", "Tensor", ")", "->", "torch", ".", "Tensor", ":", "\n", "\n", "# x = (batch_size, dim, timesteps)", "\n", "# outputs: (batch_size, dim, timesteps) = f(x) + x", "\n", "        ", "out", "=", "x", "\n", "timesteps", "=", "x", ".", "size", "(", "2", ")", "\n", "for", "k", ",", "convolution", "in", "enumerate", "(", "self", ".", "_convolutions", ")", ":", "\n", "            ", "if", "k", "==", "0", "and", "self", ".", "dropout", ">", "0", ":", "\n", "# apply dropout to the input", "\n", "                ", "out", "=", "torch", ".", "nn", ".", "functional", ".", "dropout", "(", "out", ",", "self", ".", "dropout", ",", "self", ".", "training", ")", "\n", "\n", "", "conv_out", "=", "convolution", "(", "out", ")", "\n", "\n", "# remove the padding indices", "\n", "# x is padded by convolution width - 1 in each direction", "\n", "dims_to_remove", "=", "conv_out", ".", "size", "(", "2", ")", "-", "timesteps", "\n", "if", "dims_to_remove", ">", "0", ":", "\n", "                ", "if", "self", ".", "_direction", "==", "\"forward\"", ":", "\n", "# remove from the end of the sequence", "\n", "                    ", "conv_out", "=", "conv_out", ".", "narrow", "(", "2", ",", "0", ",", "timesteps", ")", "\n", "", "else", ":", "\n", "# remove from the beginning of the sequence", "\n", "                    ", "conv_out", "=", "conv_out", ".", "narrow", "(", "2", ",", "dims_to_remove", ",", "timesteps", ")", "\n", "\n", "", "", "out", "=", "torch", ".", "nn", ".", "functional", ".", "glu", "(", "conv_out", ",", "dim", "=", "1", ")", "\n", "\n", "# see Convolutional Sequence to Sequence Learning", "\n", "", "return", "(", "out", "+", "x", ")", "*", "math", ".", "sqrt", "(", "0.5", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.seq2seq_encoders.gated_cnn_encoder.GatedCnnEncoder.__init__": [[159, 182], ["allennlp.modules.seq2seq_encoders.seq2seq_encoder.Seq2SeqEncoder.__init__", "torch.nn.ModuleList", "torch.nn.ModuleList", "gated_cnn_encoder.GatedCnnEncoder._forward_residual_blocks.append", "gated_cnn_encoder.GatedCnnEncoder._backward_residual_blocks.append", "gated_cnn_encoder.ResidualBlock", "gated_cnn_encoder.ResidualBlock"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.graph_util.plot.GraphPlot.__init__"], ["def", "__init__", "(", "\n", "self", ",", "\n", "input_dim", ":", "int", ",", "\n", "layers", ":", "Sequence", "[", "Sequence", "[", "Sequence", "[", "int", "]", "]", "]", ",", "\n", "dropout", ":", "float", "=", "0.0", ",", "\n", "return_all_layers", ":", "bool", "=", "False", ",", "\n", ")", "->", "None", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "\n", "self", ".", "_forward_residual_blocks", "=", "torch", ".", "nn", ".", "ModuleList", "(", ")", "\n", "self", ".", "_backward_residual_blocks", "=", "torch", ".", "nn", ".", "ModuleList", "(", ")", "\n", "self", ".", "_input_dim", "=", "input_dim", "\n", "self", ".", "_output_dim", "=", "input_dim", "*", "2", "\n", "\n", "for", "layer", "in", "layers", ":", "\n", "            ", "self", ".", "_forward_residual_blocks", ".", "append", "(", "\n", "ResidualBlock", "(", "input_dim", ",", "layer", ",", "\"forward\"", ",", "dropout", "=", "dropout", ")", "\n", ")", "\n", "self", ".", "_backward_residual_blocks", ".", "append", "(", "\n", "ResidualBlock", "(", "input_dim", ",", "layer", ",", "\"backward\"", ",", "dropout", "=", "dropout", ")", "\n", ")", "\n", "\n", "", "self", ".", "_return_all_layers", "=", "return_all_layers", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.seq2seq_encoders.gated_cnn_encoder.GatedCnnEncoder.forward": [[183, 221], ["torch.transpose", "enumerate", "torch.cat().transpose", "block", "outputs.append", "torch.cat().transpose", "block.masked_fill", "layer_outputs[].append", "zip", "torch.cat", "torch.cat"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "token_embeddings", ":", "torch", ".", "Tensor", ",", "mask", ":", "torch", ".", "Tensor", ")", ":", "\n", "\n", "# Convolutions need transposed input", "\n", "        ", "transposed_embeddings", "=", "torch", ".", "transpose", "(", "token_embeddings", ",", "1", ",", "2", ")", "\n", "\n", "# We need to broadcast the mask to feature dimension,", "\n", "# and to use masked_fill_ we need the inverse of the mask.", "\n", "mask_for_fill", "=", "(", "1", "-", "mask", ")", ".", "unsqueeze", "(", "1", ")", ".", "to", "(", "dtype", "=", "torch", ".", "bool", ")", "\n", "\n", "if", "self", ".", "_return_all_layers", ":", "\n", "# outputs will be [[all forward layers], [all backward layers]]", "\n", "            ", "layer_outputs", ":", "List", "[", "List", "[", "torch", ".", "Tensor", "]", "]", "=", "[", "[", "]", ",", "[", "]", "]", "\n", "", "else", ":", "\n", "# outputs will be [forward final layer, backward final layer]", "\n", "            ", "outputs", ":", "List", "[", "torch", ".", "Tensor", "]", "=", "[", "]", "\n", "\n", "", "for", "k", ",", "blocks", "in", "enumerate", "(", "\n", "[", "self", ".", "_forward_residual_blocks", ",", "self", ".", "_backward_residual_blocks", "]", "\n", ")", ":", "\n", "            ", "out", "=", "transposed_embeddings", "\n", "# Due to zero padding for backward sequences, we need", "\n", "# to ensure that the input has zeros everywhere where", "\n", "# there isn't a mask.", "\n", "for", "block", "in", "blocks", ":", "\n", "                ", "out", "=", "block", "(", "out", ".", "masked_fill", "(", "mask_for_fill", ",", "0.0", ")", ")", "\n", "if", "self", ".", "_return_all_layers", ":", "\n", "                    ", "layer_outputs", "[", "k", "]", ".", "append", "(", "out", ")", "\n", "", "", "if", "not", "self", ".", "_return_all_layers", ":", "\n", "                ", "outputs", ".", "append", "(", "out", ")", "\n", "\n", "", "", "if", "self", ".", "_return_all_layers", ":", "\n", "            ", "return", "[", "\n", "torch", ".", "cat", "(", "[", "fwd", ",", "bwd", "]", ",", "dim", "=", "1", ")", ".", "transpose", "(", "1", ",", "2", ")", "\n", "for", "fwd", ",", "bwd", "in", "zip", "(", "*", "layer_outputs", ")", "\n", "]", "\n", "", "else", ":", "\n", "# Concatenate forward and backward, then transpose back", "\n", "            ", "return", "torch", ".", "cat", "(", "outputs", ",", "dim", "=", "1", ")", ".", "transpose", "(", "1", ",", "2", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.seq2seq_encoders.gated_cnn_encoder.GatedCnnEncoder.get_input_dim": [[222, 224], ["None"], "methods", ["None"], ["", "", "def", "get_input_dim", "(", "self", ")", "->", "int", ":", "\n", "        ", "return", "self", ".", "_input_dim", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.seq2seq_encoders.gated_cnn_encoder.GatedCnnEncoder.get_output_dim": [[225, 227], ["None"], "methods", ["None"], ["", "def", "get_output_dim", "(", "self", ")", "->", "int", ":", "\n", "        ", "return", "self", ".", "_output_dim", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.seq2seq_encoders.gated_cnn_encoder.GatedCnnEncoder.is_bidirectional": [[228, 230], ["None"], "methods", ["None"], ["", "def", "is_bidirectional", "(", "self", ")", "->", "bool", ":", "\n", "        ", "return", "True", "\n", "", "", ""]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.seq2seq_encoders.pass_through_encoder.PassThroughEncoder.__init__": [[15, 18], ["allennlp.modules.seq2seq_encoders.seq2seq_encoder.Seq2SeqEncoder.__init__"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.graph_util.plot.GraphPlot.__init__"], ["def", "__init__", "(", "self", ",", "input_dim", ":", "int", ")", "->", "None", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "_input_dim", "=", "input_dim", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.seq2seq_encoders.pass_through_encoder.PassThroughEncoder.get_input_dim": [[19, 22], ["None"], "methods", ["None"], ["", "@", "overrides", "\n", "def", "get_input_dim", "(", "self", ")", "->", "int", ":", "\n", "        ", "return", "self", ".", "_input_dim", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.seq2seq_encoders.pass_through_encoder.PassThroughEncoder.get_output_dim": [[23, 26], ["None"], "methods", ["None"], ["", "@", "overrides", "\n", "def", "get_output_dim", "(", "self", ")", "->", "int", ":", "\n", "        ", "return", "self", ".", "_input_dim", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.seq2seq_encoders.pass_through_encoder.PassThroughEncoder.is_bidirectional": [[27, 30], ["None"], "methods", ["None"], ["", "@", "overrides", "\n", "def", "is_bidirectional", "(", "self", ")", ":", "\n", "        ", "return", "False", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.seq2seq_encoders.pass_through_encoder.PassThroughEncoder.forward": [[31, 54], ["mask.unsqueeze().float", "mask.unsqueeze"], "methods", ["None"], ["", "@", "overrides", "\n", "def", "forward", "(", "\n", "self", ",", "inputs", ":", "torch", ".", "Tensor", ",", "mask", ":", "torch", ".", "LongTensor", "=", "None", "\n", ")", "->", "torch", ".", "Tensor", ":", "\n", "        ", "\"\"\"\n        # Parameters\n\n        inputs : `torch.Tensor`, required.\n            A tensor of shape (batch_size, timesteps, input_dim)\n        mask : `torch.LongTensor`, optional (default = None).\n            A tensor of shape (batch_size, timesteps).\n\n        # Returns\n\n        A tensor of shape (batch_size, timesteps, output_dim),\n        where output_dim = input_dim.\n        \"\"\"", "\n", "if", "mask", "is", "None", ":", "\n", "            ", "return", "inputs", "\n", "", "else", ":", "\n", "# We should mask out the output instead of the input.", "\n", "# But here, output = input, so we directly mask out the input.", "\n", "            ", "return", "inputs", "*", "mask", ".", "unsqueeze", "(", "dim", "=", "-", "1", ")", ".", "float", "(", ")", "\n", "", "", "", ""]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.seq2seq_encoders.bidirectional_language_model_transformer.PositionalEncoding.__init__": [[53, 66], ["super().__init__", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.arange().unsqueeze().float", "torch.arange().unsqueeze().float", "torch.arange().unsqueeze().float", "torch.arange().unsqueeze().float", "torch.exp", "torch.exp", "torch.exp", "torch.exp", "torch.sin", "torch.sin", "torch.sin", "torch.sin", "torch.cos", "torch.cos", "torch.cos", "torch.cos", "positional_encoding.unsqueeze.unsqueeze.unsqueeze", "bidirectional_language_model_transformer.PositionalEncoding.register_buffer", "torch.arange().unsqueeze", "torch.arange().unsqueeze", "torch.arange().unsqueeze", "torch.arange().unsqueeze", "torch.arange().float", "torch.arange().float", "torch.arange().float", "torch.arange().float", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "math.log"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.graph_util.plot.GraphPlot.__init__"], ["def", "__init__", "(", "self", ",", "input_dim", ":", "int", ",", "max_len", ":", "int", "=", "5000", ")", "->", "None", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "\n", "# Compute the positional encodings once in log space.", "\n", "positional_encoding", "=", "torch", ".", "zeros", "(", "max_len", ",", "input_dim", ",", "requires_grad", "=", "False", ")", "\n", "position", "=", "torch", ".", "arange", "(", "0", ",", "max_len", ")", ".", "unsqueeze", "(", "1", ")", ".", "float", "(", ")", "\n", "div_term", "=", "torch", ".", "exp", "(", "\n", "torch", ".", "arange", "(", "0", ",", "input_dim", ",", "2", ")", ".", "float", "(", ")", "*", "-", "(", "math", ".", "log", "(", "10000.0", ")", "/", "input_dim", ")", "\n", ")", "\n", "positional_encoding", "[", ":", ",", "0", ":", ":", "2", "]", "=", "torch", ".", "sin", "(", "position", "*", "div_term", ")", "\n", "positional_encoding", "[", ":", ",", "1", ":", ":", "2", "]", "=", "torch", ".", "cos", "(", "position", "*", "div_term", ")", "\n", "positional_encoding", "=", "positional_encoding", ".", "unsqueeze", "(", "0", ")", "\n", "self", ".", "register_buffer", "(", "\"positional_encoding\"", ",", "positional_encoding", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.seq2seq_encoders.bidirectional_language_model_transformer.PositionalEncoding.forward": [[67, 70], ["x.size"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "x", ":", "torch", ".", "Tensor", ")", "->", "torch", ".", "Tensor", ":", "\n", "\n", "        ", "return", "x", "+", "self", ".", "positional_encoding", "[", ":", ",", ":", "x", ".", "size", "(", "1", ")", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.seq2seq_encoders.bidirectional_language_model_transformer.PositionwiseFeedForward.__init__": [[75, 80], ["super().__init__", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Dropout", "torch.nn.Dropout", "torch.nn.Dropout", "torch.nn.Dropout"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.graph_util.plot.GraphPlot.__init__"], ["def", "__init__", "(", "self", ",", "input_dim", ":", "int", ",", "ff_dim", ":", "int", ",", "dropout", ":", "float", "=", "0.1", ")", "->", "None", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "w_1", "=", "torch", ".", "nn", ".", "Linear", "(", "input_dim", ",", "ff_dim", ")", "\n", "self", ".", "w_2", "=", "torch", ".", "nn", ".", "Linear", "(", "ff_dim", ",", "input_dim", ")", "\n", "self", ".", "dropout", "=", "torch", ".", "nn", ".", "Dropout", "(", "dropout", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.seq2seq_encoders.bidirectional_language_model_transformer.PositionwiseFeedForward.forward": [[81, 84], ["bidirectional_language_model_transformer.PositionwiseFeedForward.w_2", "bidirectional_language_model_transformer.PositionwiseFeedForward.dropout", "torch.relu", "torch.relu", "bidirectional_language_model_transformer.PositionwiseFeedForward.w_1"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "x", ":", "torch", ".", "Tensor", ")", "->", "torch", ".", "Tensor", ":", "\n", "\n", "        ", "return", "self", ".", "w_2", "(", "self", ".", "dropout", "(", "F", ".", "relu", "(", "self", ".", "w_1", "(", "x", ")", ")", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.seq2seq_encoders.bidirectional_language_model_transformer.TransformerEncoder.__init__": [[89, 96], ["super().__init__", "allennlp.nn.util.clone", "allennlp.modules.layer_norm.LayerNorm"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.graph_util.plot.GraphPlot.__init__", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.nn.util.clone"], ["def", "__init__", "(", "\n", "self", ",", "layer", ":", "torch", ".", "nn", ".", "Module", ",", "num_layers", ":", "int", ",", "return_all_layers", ":", "bool", "=", "False", "\n", ")", "->", "None", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "layers", "=", "util", ".", "clone", "(", "layer", ",", "num_layers", ")", "\n", "self", ".", "norm", "=", "LayerNorm", "(", "layer", ".", "size", ")", "\n", "self", ".", "return_all_layers", "=", "return_all_layers", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.seq2seq_encoders.bidirectional_language_model_transformer.TransformerEncoder.forward": [[97, 109], ["bidirectional_language_model_transformer.TransformerEncoder.norm", "layer", "bidirectional_language_model_transformer.TransformerEncoder.norm", "all_layers.append"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "x", ",", "mask", ")", ":", "\n", "        ", "\"\"\"Pass the input (and mask) through each layer in turn.\"\"\"", "\n", "all_layers", "=", "[", "]", "\n", "for", "layer", "in", "self", ".", "layers", ":", "\n", "            ", "x", "=", "layer", "(", "x", ",", "mask", ")", "\n", "if", "self", ".", "return_all_layers", ":", "\n", "                ", "all_layers", ".", "append", "(", "x", ")", "\n", "\n", "", "", "if", "self", ".", "return_all_layers", ":", "\n", "            ", "all_layers", "[", "-", "1", "]", "=", "self", ".", "norm", "(", "all_layers", "[", "-", "1", "]", ")", "\n", "return", "all_layers", "\n", "", "return", "self", ".", "norm", "(", "x", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.seq2seq_encoders.bidirectional_language_model_transformer.SublayerConnection.__init__": [[117, 121], ["super().__init__", "allennlp.modules.layer_norm.LayerNorm", "torch.nn.Dropout", "torch.nn.Dropout", "torch.nn.Dropout", "torch.nn.Dropout"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.graph_util.plot.GraphPlot.__init__"], ["def", "__init__", "(", "self", ",", "size", ":", "int", ",", "dropout", ":", "float", ")", "->", "None", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "norm", "=", "LayerNorm", "(", "size", ")", "\n", "self", ".", "dropout", "=", "torch", ".", "nn", ".", "Dropout", "(", "dropout", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.seq2seq_encoders.bidirectional_language_model_transformer.SublayerConnection.forward": [[122, 127], ["bidirectional_language_model_transformer.SublayerConnection.dropout", "sublayer", "bidirectional_language_model_transformer.SublayerConnection.norm"], "methods", ["None"], ["", "def", "forward", "(", "\n", "self", ",", "x", ":", "torch", ".", "Tensor", ",", "sublayer", ":", "Callable", "[", "[", "torch", ".", "Tensor", "]", ",", "torch", ".", "Tensor", "]", "\n", ")", "->", "torch", ".", "Tensor", ":", "\n", "        ", "\"\"\"Apply residual connection to any sublayer with the same size.\"\"\"", "\n", "return", "x", "+", "self", ".", "dropout", "(", "sublayer", "(", "self", ".", "norm", "(", "x", ")", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.seq2seq_encoders.bidirectional_language_model_transformer.EncoderLayer.__init__": [[132, 144], ["super().__init__", "allennlp.nn.util.clone", "bidirectional_language_model_transformer.SublayerConnection"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.graph_util.plot.GraphPlot.__init__", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.nn.util.clone"], ["def", "__init__", "(", "\n", "self", ",", "\n", "size", ":", "int", ",", "\n", "self_attn", ":", "torch", ".", "nn", ".", "Module", ",", "\n", "feed_forward", ":", "torch", ".", "nn", ".", "Module", ",", "\n", "dropout", ":", "float", ",", "\n", ")", "->", "None", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "self_attn", "=", "self_attn", "\n", "self", ".", "feed_forward", "=", "feed_forward", "\n", "self", ".", "sublayer", "=", "util", ".", "clone", "(", "SublayerConnection", "(", "size", ",", "dropout", ")", ",", "2", ")", "\n", "self", ".", "size", "=", "size", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.seq2seq_encoders.bidirectional_language_model_transformer.EncoderLayer.forward": [[145, 149], ["bidirectional_language_model_transformer.EncoderLayer.self_attn"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "x", ":", "torch", ".", "Tensor", ",", "mask", ":", "torch", ".", "Tensor", ")", "->", "torch", ".", "Tensor", ":", "\n", "        ", "\"\"\"Follow Figure 1 (left) for connections.\"\"\"", "\n", "x", "=", "self", ".", "sublayer", "[", "0", "]", "(", "x", ",", "lambda", "x", ":", "self", ".", "self_attn", "(", "x", ",", "x", ",", "x", ",", "mask", ")", ")", "\n", "return", "self", ".", "sublayer", "[", "1", "]", "(", "x", ",", "self", ".", "feed_forward", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.seq2seq_encoders.bidirectional_language_model_transformer.MultiHeadedAttention.__init__": [[152, 162], ["super().__init__", "allennlp.nn.util.clone", "torch.nn.Dropout", "torch.nn.Dropout", "torch.nn.Dropout", "torch.nn.Dropout", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.graph_util.plot.GraphPlot.__init__", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.nn.util.clone"], ["    ", "def", "__init__", "(", "self", ",", "num_heads", ":", "int", ",", "input_dim", ":", "int", ",", "dropout", ":", "float", "=", "0.1", ")", "->", "None", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "assert", "input_dim", "%", "num_heads", "==", "0", ",", "\"input_dim must be a multiple of num_heads\"", "\n", "# We assume d_v always equals d_k", "\n", "self", ".", "d_k", "=", "input_dim", "//", "num_heads", "\n", "self", ".", "num_heads", "=", "num_heads", "\n", "# These linear layers are", "\n", "#  [query_projection, key_projection, value_projection, concatenated_heads_projection]", "\n", "self", ".", "linears", "=", "util", ".", "clone", "(", "torch", ".", "nn", ".", "Linear", "(", "input_dim", ",", "input_dim", ")", ",", "4", ")", "\n", "self", ".", "dropout", "=", "torch", ".", "nn", ".", "Dropout", "(", "p", "=", "dropout", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.seq2seq_encoders.bidirectional_language_model_transformer.MultiHeadedAttention.forward": [[163, 189], ["query.size", "bidirectional_language_model_transformer.attention", "x.transpose().contiguous().view.transpose().contiguous().view.transpose().contiguous().view", "mask.unsqueeze().expand.unsqueeze().expand.unsqueeze().expand", "layer().view().transpose", "zip", "x.transpose().contiguous().view.transpose().contiguous().view.transpose().contiguous", "mask.unsqueeze().expand.unsqueeze().expand.unsqueeze", "layer().view", "x.transpose().contiguous().view.transpose().contiguous().view.transpose", "layer"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.seq2seq_encoders.bidirectional_language_model_transformer.attention"], ["", "def", "forward", "(", "\n", "self", ",", "\n", "query", ":", "torch", ".", "Tensor", ",", "\n", "key", ":", "torch", ".", "Tensor", ",", "\n", "value", ":", "torch", ".", "Tensor", ",", "\n", "mask", ":", "torch", ".", "Tensor", "=", "None", ",", "\n", ")", "->", "torch", ".", "Tensor", ":", "\n", "        ", "if", "mask", "is", "not", "None", ":", "\n", "# Same mask applied to all h heads.", "\n", "# Shape (batch_size, num_heads, timesteps, timesteps)", "\n", "            ", "mask", "=", "mask", ".", "unsqueeze", "(", "1", ")", ".", "expand", "(", "[", "-", "1", ",", "self", ".", "num_heads", ",", "-", "1", ",", "-", "1", "]", ")", "\n", "\n", "", "nbatches", "=", "query", ".", "size", "(", "0", ")", "\n", "\n", "# 1) Do all the linear projections in batch from d_model => h x d_k", "\n", "query", ",", "key", ",", "value", "=", "[", "\n", "layer", "(", "x", ")", ".", "view", "(", "nbatches", ",", "-", "1", ",", "self", ".", "num_heads", ",", "self", ".", "d_k", ")", ".", "transpose", "(", "1", ",", "2", ")", "\n", "for", "layer", ",", "x", "in", "zip", "(", "self", ".", "linears", ",", "(", "query", ",", "key", ",", "value", ")", ")", "\n", "]", "\n", "\n", "# 2) Apply attention on all the projected vectors in batch.", "\n", "x", ",", "_", "=", "attention", "(", "query", ",", "key", ",", "value", ",", "mask", "=", "mask", ",", "dropout", "=", "self", ".", "dropout", ")", "\n", "\n", "# 3) \"Concat\" using a view and apply a final linear.", "\n", "x", "=", "x", ".", "transpose", "(", "1", ",", "2", ")", ".", "contiguous", "(", ")", ".", "view", "(", "nbatches", ",", "-", "1", ",", "self", ".", "num_heads", "*", "self", ".", "d_k", ")", "\n", "return", "self", ".", "linears", "[", "-", "1", "]", "(", "x", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.seq2seq_encoders.bidirectional_language_model_transformer.BidirectionalLanguageModelTransformer.__init__": [[217, 265], ["warnings.warn", "allennlp.modules.seq2seq_encoders.seq2seq_encoder.Seq2SeqEncoder.__init__", "bidirectional_language_model_transformer.make_model", "bidirectional_language_model_transformer.make_model", "bidirectional_language_model_transformer.PositionalEncoding", "torch.nn.Dropout", "torch.nn.Dropout", "torch.nn.Dropout", "torch.nn.Dropout"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.graph_util.plot.GraphPlot.__init__", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.seq2seq_encoders.bidirectional_language_model_transformer.make_model", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.seq2seq_encoders.bidirectional_language_model_transformer.make_model"], ["    ", "def", "__init__", "(", "\n", "self", ",", "\n", "input_dim", ":", "int", ",", "\n", "hidden_dim", ":", "int", ",", "\n", "num_layers", ":", "int", ",", "\n", "dropout", ":", "float", "=", "0.1", ",", "\n", "input_dropout", ":", "float", "=", "None", ",", "\n", "return_all_layers", ":", "bool", "=", "False", ",", "\n", ")", "->", "None", ":", "\n", "\n", "        ", "warnings", ".", "warn", "(", "\n", "\"This particular transformer implementation is a provisional feature \"", "\n", "\"that's intended for AI2 internal use and might be deleted at any time. \"", "\n", "\"If you use it, consider yourself warned!\"", ",", "\n", "ExperimentalFeatureWarning", ",", "\n", ")", "\n", "\n", "super", "(", ")", ".", "__init__", "(", ")", "\n", "\n", "self", ".", "_return_all_layers", "=", "return_all_layers", "\n", "self", ".", "transformer_layers", "=", "num_layers", "\n", "self", ".", "num_layers", "=", "num_layers", "\n", "\n", "self", ".", "_forward_transformer", "=", "make_model", "(", "\n", "input_size", "=", "input_dim", ",", "\n", "hidden_size", "=", "hidden_dim", ",", "\n", "num_layers", "=", "num_layers", ",", "\n", "dropout", "=", "dropout", ",", "\n", "return_all_layers", "=", "return_all_layers", ",", "\n", ")", "\n", "self", ".", "_backward_transformer", "=", "make_model", "(", "\n", "input_size", "=", "input_dim", ",", "\n", "hidden_size", "=", "hidden_dim", ",", "\n", "num_layers", "=", "num_layers", ",", "\n", "dropout", "=", "dropout", ",", "\n", "return_all_layers", "=", "return_all_layers", ",", "\n", ")", "\n", "self", ".", "_position", "=", "PositionalEncoding", "(", "input_dim", ")", "\n", "\n", "self", ".", "input_dim", "=", "input_dim", "\n", "self", ".", "output_dim", "=", "2", "*", "input_dim", "\n", "\n", "if", "input_dropout", ":", "\n", "            ", "self", ".", "_dropout", "=", "torch", ".", "nn", ".", "Dropout", "(", "input_dropout", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "_dropout", "=", "lambda", "x", ":", "x", "\n", "\n", "", "self", ".", "should_log_activations", "=", "False", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.seq2seq_encoders.bidirectional_language_model_transformer.BidirectionalLanguageModelTransformer.get_attention_masks": [[266, 288], ["mask.size", "bidirectional_language_model_transformer.subsequent_mask", "forward_mask.transpose", "mask.unsqueeze"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.seq2seq_encoders.bidirectional_language_model_transformer.subsequent_mask"], ["", "def", "get_attention_masks", "(", "\n", "self", ",", "mask", ":", "torch", ".", "Tensor", "\n", ")", "->", "Tuple", "[", "torch", ".", "Tensor", ",", "torch", ".", "Tensor", "]", ":", "\n", "        ", "\"\"\"\n        Returns 2 masks of shape (batch_size, timesteps, timesteps) representing\n        1) non-padded elements, and\n        2) elements of the sequence which are permitted to be involved in attention at a given timestep.\n        \"\"\"", "\n", "device", "=", "mask", ".", "device", "\n", "# Forward case:", "\n", "timesteps", "=", "mask", ".", "size", "(", "1", ")", "\n", "# Shape (1, timesteps, timesteps)", "\n", "subsequent", "=", "subsequent_mask", "(", "timesteps", ",", "device", ")", "\n", "# Broadcasted logical and - we want zero", "\n", "# elements where either we have padding from the mask,", "\n", "# or we aren't allowed to use the timesteps.", "\n", "# Shape (batch_size, timesteps, timesteps)", "\n", "forward_mask", "=", "mask", ".", "unsqueeze", "(", "-", "1", ")", "&", "subsequent", "\n", "# Backward case - exactly the same, but transposed.", "\n", "backward_mask", "=", "forward_mask", ".", "transpose", "(", "1", ",", "2", ")", "\n", "\n", "return", "forward_mask", ",", "backward_mask", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.seq2seq_encoders.bidirectional_language_model_transformer.BidirectionalLanguageModelTransformer.forward": [[289, 305], ["bidirectional_language_model_transformer.BidirectionalLanguageModelTransformer.get_attention_masks", "bidirectional_language_model_transformer.BidirectionalLanguageModelTransformer._position", "bidirectional_language_model_transformer.BidirectionalLanguageModelTransformer._dropout", "bidirectional_language_model_transformer.BidirectionalLanguageModelTransformer._forward_transformer", "bidirectional_language_model_transformer.BidirectionalLanguageModelTransformer._backward_transformer", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "mask.int", "zip", "to_return.append", "torch.cat", "torch.cat", "torch.cat", "torch.cat"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.seq2seq_encoders.bidirectional_language_model_transformer.BidirectionalLanguageModelTransformer.get_attention_masks"], ["", "def", "forward", "(", "\n", "self", ",", "token_embeddings", ":", "torch", ".", "Tensor", ",", "mask", ":", "torch", ".", "Tensor", "\n", ")", "->", "torch", ".", "Tensor", ":", "\n", "        ", "forward_mask", ",", "backward_mask", "=", "self", ".", "get_attention_masks", "(", "mask", ".", "int", "(", ")", ")", "\n", "token_embeddings", "=", "self", ".", "_position", "(", "token_embeddings", ")", "\n", "token_embeddings", "=", "self", ".", "_dropout", "(", "token_embeddings", ")", "\n", "forward_output", "=", "self", ".", "_forward_transformer", "(", "token_embeddings", ",", "forward_mask", ")", "\n", "backward_output", "=", "self", ".", "_backward_transformer", "(", "token_embeddings", ",", "backward_mask", ")", "\n", "\n", "if", "self", ".", "_return_all_layers", ":", "\n", "            ", "to_return", "=", "[", "]", "\n", "for", "forward", ",", "backward", "in", "zip", "(", "forward_output", ",", "backward_output", ")", ":", "\n", "                ", "to_return", ".", "append", "(", "torch", ".", "cat", "(", "[", "forward", ",", "backward", "]", ",", "-", "1", ")", ")", "\n", "", "return", "to_return", "\n", "\n", "", "return", "torch", ".", "cat", "(", "[", "forward_output", ",", "backward_output", "]", ",", "-", "1", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.seq2seq_encoders.bidirectional_language_model_transformer.BidirectionalLanguageModelTransformer.get_regularization_penalty": [[306, 308], ["None"], "methods", ["None"], ["", "def", "get_regularization_penalty", "(", "self", ")", ":", "\n", "        ", "return", "0.0", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.seq2seq_encoders.bidirectional_language_model_transformer.BidirectionalLanguageModelTransformer.get_input_dim": [[309, 311], ["None"], "methods", ["None"], ["", "def", "get_input_dim", "(", "self", ")", "->", "int", ":", "\n", "        ", "return", "self", ".", "input_dim", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.seq2seq_encoders.bidirectional_language_model_transformer.BidirectionalLanguageModelTransformer.get_output_dim": [[312, 314], ["None"], "methods", ["None"], ["", "def", "get_output_dim", "(", "self", ")", "->", "int", ":", "\n", "        ", "return", "self", ".", "output_dim", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.seq2seq_encoders.bidirectional_language_model_transformer.BidirectionalLanguageModelTransformer.is_bidirectional": [[315, 317], ["None"], "methods", ["None"], ["", "def", "is_bidirectional", "(", "self", ")", "->", "bool", ":", "\n", "        ", "return", "True", "\n", "", "", ""]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.seq2seq_encoders.bidirectional_language_model_transformer.attention": [[24, 40], ["query.size", "torch.softmax", "torch.matmul", "torch.matmul", "math.sqrt", "scores.masked_fill.masked_fill", "dropout", "torch.matmul", "torch.matmul", "key.transpose"], "function", ["None"], ["def", "attention", "(", "\n", "query", ":", "torch", ".", "Tensor", ",", "\n", "key", ":", "torch", ".", "Tensor", ",", "\n", "value", ":", "torch", ".", "Tensor", ",", "\n", "mask", ":", "torch", ".", "Tensor", "=", "None", ",", "\n", "dropout", ":", "Callable", "=", "None", ",", "\n", ")", "->", "Tuple", "[", "torch", ".", "Tensor", ",", "torch", ".", "Tensor", "]", ":", "\n", "    ", "\"\"\"Compute 'Scaled Dot Product Attention'\"\"\"", "\n", "d_k", "=", "query", ".", "size", "(", "-", "1", ")", "\n", "scores", "=", "torch", ".", "matmul", "(", "query", ",", "key", ".", "transpose", "(", "-", "2", ",", "-", "1", ")", ")", "/", "math", ".", "sqrt", "(", "d_k", ")", "\n", "if", "mask", "is", "not", "None", ":", "\n", "        ", "scores", "=", "scores", ".", "masked_fill", "(", "mask", "==", "0", ",", "-", "1e9", ")", "\n", "", "p_attn", "=", "F", ".", "softmax", "(", "scores", ",", "dim", "=", "-", "1", ")", "\n", "if", "dropout", "is", "not", "None", ":", "\n", "        ", "p_attn", "=", "dropout", "(", "p_attn", ")", "\n", "", "return", "torch", ".", "matmul", "(", "p_attn", ",", "value", ")", ",", "p_attn", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.seq2seq_encoders.bidirectional_language_model_transformer.subsequent_mask": [[42, 48], ["torch.tril().unsqueeze", "torch.tril().unsqueeze", "torch.tril", "torch.tril", "torch.ones", "torch.ones"], "function", ["None"], ["", "def", "subsequent_mask", "(", "size", ":", "int", ",", "device", ":", "str", "=", "\"cpu\"", ")", "->", "torch", ".", "Tensor", ":", "\n", "    ", "\"\"\"Mask out subsequent positions.\"\"\"", "\n", "mask", "=", "torch", ".", "tril", "(", "\n", "torch", ".", "ones", "(", "size", ",", "size", ",", "device", "=", "device", ",", "dtype", "=", "torch", ".", "int32", ")", "\n", ")", ".", "unsqueeze", "(", "0", ")", "\n", "return", "mask", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.seq2seq_encoders.bidirectional_language_model_transformer.make_model": [[191, 213], ["bidirectional_language_model_transformer.MultiHeadedAttention", "bidirectional_language_model_transformer.PositionwiseFeedForward", "bidirectional_language_model_transformer.TransformerEncoder", "TransformerEncoder.parameters", "bidirectional_language_model_transformer.EncoderLayer", "p.dim", "torch.nn.init.xavier_uniform_", "torch.nn.init.xavier_uniform_"], "function", ["None"], ["", "", "def", "make_model", "(", "\n", "num_layers", ":", "int", "=", "6", ",", "\n", "input_size", ":", "int", "=", "512", ",", "# Attention size", "\n", "hidden_size", ":", "int", "=", "2048", ",", "# FF layer size", "\n", "heads", ":", "int", "=", "8", ",", "\n", "dropout", ":", "float", "=", "0.1", ",", "\n", "return_all_layers", ":", "bool", "=", "False", ",", "\n", ")", "->", "TransformerEncoder", ":", "\n", "    ", "\"\"\"Helper: Construct a model from hyperparameters.\"\"\"", "\n", "attn", "=", "MultiHeadedAttention", "(", "heads", ",", "input_size", ",", "dropout", ")", "\n", "ff", "=", "PositionwiseFeedForward", "(", "input_size", ",", "hidden_size", ",", "dropout", ")", "\n", "model", "=", "TransformerEncoder", "(", "\n", "EncoderLayer", "(", "input_size", ",", "attn", ",", "ff", ",", "dropout", ")", ",", "\n", "num_layers", ",", "\n", "return_all_layers", "=", "return_all_layers", ",", "\n", ")", "\n", "\n", "# Initialize parameters with Glorot / fan_avg.", "\n", "for", "p", "in", "model", ".", "parameters", "(", ")", ":", "\n", "        ", "if", "p", ".", "dim", "(", ")", ">", "1", ":", "\n", "            ", "torch", ".", "nn", ".", "init", ".", "xavier_uniform_", "(", "p", ")", "\n", "", "", "return", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.seq2seq_encoders.__init__._Seq2SeqWrapper.__init__": [[86, 88], ["None"], "methods", ["None"], []], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.seq2seq_encoders.__init__._Seq2SeqWrapper.__call__": [[89, 91], ["__init__._Seq2SeqWrapper.from_params", "allennlp.common.Params"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.tokenizers.tokenizer.Tokenizer.from_params"], []], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.seq2seq_encoders.__init__._Seq2SeqWrapper.from_params": [[93, 103], ["params.pop_bool", "__init__._Seq2SeqWrapper._module_class", "allennlp.modules.seq2seq_encoders.pytorch_seq2seq_wrapper.PytorchSeq2SeqWrapper", "params.pop_bool", "allennlp.common.checks.ConfigurationError", "params.as_dict"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.params.Params.pop_bool", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.params.Params.pop_bool", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.params.Params.as_dict"], []], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.seq2seq_encoders.qanet_encoder.QaNetEncoder.__init__": [[57, 100], ["allennlp.modules.seq2seq_encoders.seq2seq_encoder.Seq2SeqEncoder.__init__", "torch.nn.ModuleList", "range", "torch.nn.Linear", "qanet_encoder.QaNetEncoderBlock", "qanet_encoder.QaNetEncoder._encoder_blocks.append"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.graph_util.plot.GraphPlot.__init__"], ["def", "__init__", "(", "\n", "self", ",", "\n", "input_dim", ":", "int", ",", "\n", "hidden_dim", ":", "int", ",", "\n", "attention_projection_dim", ":", "int", ",", "\n", "feedforward_hidden_dim", ":", "int", ",", "\n", "num_blocks", ":", "int", ",", "\n", "num_convs_per_block", ":", "int", ",", "\n", "conv_kernel_size", ":", "int", ",", "\n", "num_attention_heads", ":", "int", ",", "\n", "use_positional_encoding", ":", "bool", "=", "True", ",", "\n", "dropout_prob", ":", "float", "=", "0.1", ",", "\n", "layer_dropout_undecayed_prob", ":", "float", "=", "0.1", ",", "\n", "attention_dropout_prob", ":", "float", "=", "0", ",", "\n", ")", "->", "None", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "\n", "self", ".", "_input_projection_layer", "=", "None", "\n", "\n", "if", "input_dim", "!=", "hidden_dim", ":", "\n", "            ", "self", ".", "_input_projection_layer", "=", "torch", ".", "nn", ".", "Linear", "(", "input_dim", ",", "hidden_dim", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "_input_projection_layer", "=", "lambda", "x", ":", "x", "\n", "\n", "", "self", ".", "_encoder_blocks", "=", "ModuleList", "(", "[", "]", ")", "\n", "for", "_", "in", "range", "(", "num_blocks", ")", ":", "\n", "            ", "encoder_block", "=", "QaNetEncoderBlock", "(", "\n", "hidden_dim", ",", "\n", "hidden_dim", ",", "\n", "attention_projection_dim", ",", "\n", "feedforward_hidden_dim", ",", "\n", "num_convs_per_block", ",", "\n", "conv_kernel_size", ",", "\n", "num_attention_heads", ",", "\n", "use_positional_encoding", ",", "\n", "dropout_prob", ",", "\n", "layer_dropout_undecayed_prob", ",", "\n", "attention_dropout_prob", ",", "\n", ")", "\n", "self", ".", "_encoder_blocks", ".", "append", "(", "encoder_block", ")", "\n", "\n", "", "self", ".", "_input_dim", "=", "input_dim", "\n", "self", ".", "_output_dim", "=", "hidden_dim", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.seq2seq_encoders.qanet_encoder.QaNetEncoder.get_input_dim": [[101, 104], ["None"], "methods", ["None"], ["", "@", "overrides", "\n", "def", "get_input_dim", "(", "self", ")", "->", "int", ":", "\n", "        ", "return", "self", ".", "_input_dim", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.seq2seq_encoders.qanet_encoder.QaNetEncoder.get_output_dim": [[105, 108], ["None"], "methods", ["None"], ["", "@", "overrides", "\n", "def", "get_output_dim", "(", "self", ")", "->", "int", ":", "\n", "        ", "return", "self", ".", "_output_dim", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.seq2seq_encoders.qanet_encoder.QaNetEncoder.is_bidirectional": [[109, 112], ["None"], "methods", ["None"], ["", "@", "overrides", "\n", "def", "is_bidirectional", "(", "self", ")", "->", "bool", ":", "\n", "        ", "return", "False", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.seq2seq_encoders.qanet_encoder.QaNetEncoder.forward": [[113, 120], ["qanet_encoder.QaNetEncoder._input_projection_layer", "encoder_block"], "methods", ["None"], ["", "@", "overrides", "\n", "def", "forward", "(", "self", ",", "inputs", ":", "torch", ".", "Tensor", ",", "mask", ":", "torch", ".", "Tensor", "=", "None", ")", "->", "torch", ".", "Tensor", ":", "\n", "        ", "inputs", "=", "self", ".", "_input_projection_layer", "(", "inputs", ")", "\n", "output", "=", "inputs", "\n", "for", "encoder_block", "in", "self", ".", "_encoder_blocks", ":", "\n", "            ", "output", "=", "encoder_block", "(", "output", ",", "mask", ")", "\n", "", "return", "output", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.seq2seq_encoders.qanet_encoder.QaNetEncoderBlock.__init__": [[170, 234], ["allennlp.modules.seq2seq_encoders.seq2seq_encoder.Seq2SeqEncoder.__init__", "allennlp.common.checks.check_dimensions_match", "torch.nn.ModuleList", "torch.nn.ModuleList", "range", "torch.nn.LayerNorm", "allennlp.modules.seq2seq_encoders.multi_head_self_attention.MultiHeadSelfAttention", "torch.nn.LayerNorm", "allennlp.modules.feedforward.FeedForward", "torch.nn.Dropout", "allennlp.modules.residual_with_layer_dropout.ResidualWithLayerDropout", "torch.nn.ConstantPad1d", "torch.nn.Conv1d", "torch.nn.Conv1d", "qanet_encoder.QaNetEncoderBlock._conv_layers.append", "torch.nn.LayerNorm", "torch.nn.Sequential", "range", "allennlp.nn.activations.Activation.by_name", "allennlp.nn.activations.Activation.by_name", "allennlp.nn.activations.Activation.by_name"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.graph_util.plot.GraphPlot.__init__", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.checks.check_dimensions_match", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.registrable.Registrable.by_name", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.registrable.Registrable.by_name", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.registrable.Registrable.by_name"], ["def", "__init__", "(", "\n", "self", ",", "\n", "input_dim", ":", "int", ",", "\n", "hidden_dim", ":", "int", ",", "\n", "attention_projection_dim", ":", "int", ",", "\n", "feedforward_hidden_dim", ":", "int", ",", "\n", "num_convs", ":", "int", ",", "\n", "conv_kernel_size", ":", "int", ",", "\n", "num_attention_heads", ":", "int", ",", "\n", "use_positional_encoding", ":", "bool", "=", "True", ",", "\n", "dropout_prob", ":", "float", "=", "0.1", ",", "\n", "layer_dropout_undecayed_prob", ":", "float", "=", "0.1", ",", "\n", "attention_dropout_prob", ":", "float", "=", "0", ",", "\n", ")", "->", "None", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "\n", "check_dimensions_match", "(", "input_dim", ",", "hidden_dim", ",", "\"input_dim\"", ",", "\"hidden_dim\"", ")", "\n", "\n", "self", ".", "_use_positional_encoding", "=", "use_positional_encoding", "\n", "\n", "self", ".", "_conv_norm_layers", "=", "torch", ".", "nn", ".", "ModuleList", "(", "\n", "[", "LayerNorm", "(", "hidden_dim", ")", "for", "_", "in", "range", "(", "num_convs", ")", "]", "\n", ")", "\n", "self", ".", "_conv_layers", "=", "torch", ".", "nn", ".", "ModuleList", "(", ")", "\n", "for", "_", "in", "range", "(", "num_convs", ")", ":", "\n", "            ", "padding", "=", "torch", ".", "nn", ".", "ConstantPad1d", "(", "\n", "(", "conv_kernel_size", "//", "2", ",", "(", "conv_kernel_size", "-", "1", ")", "//", "2", ")", ",", "0", "\n", ")", "\n", "depthwise_conv", "=", "torch", ".", "nn", ".", "Conv1d", "(", "\n", "hidden_dim", ",", "hidden_dim", ",", "conv_kernel_size", ",", "groups", "=", "hidden_dim", "\n", ")", "\n", "pointwise_conv", "=", "torch", ".", "nn", ".", "Conv1d", "(", "hidden_dim", ",", "hidden_dim", ",", "1", ")", "\n", "self", ".", "_conv_layers", ".", "append", "(", "\n", "torch", ".", "nn", ".", "Sequential", "(", "\n", "padding", ",", "\n", "depthwise_conv", ",", "\n", "pointwise_conv", ",", "\n", "Activation", ".", "by_name", "(", "\"relu\"", ")", "(", ")", ",", "\n", ")", "\n", ")", "\n", "\n", "", "self", ".", "attention_norm_layer", "=", "LayerNorm", "(", "hidden_dim", ")", "\n", "self", ".", "attention_layer", "=", "MultiHeadSelfAttention", "(", "\n", "num_heads", "=", "num_attention_heads", ",", "\n", "input_dim", "=", "hidden_dim", ",", "\n", "attention_dim", "=", "attention_projection_dim", ",", "\n", "values_dim", "=", "attention_projection_dim", ",", "\n", "attention_dropout_prob", "=", "attention_dropout_prob", ",", "\n", ")", "\n", "self", ".", "feedforward_norm_layer", "=", "LayerNorm", "(", "hidden_dim", ")", "\n", "self", ".", "feedforward", "=", "FeedForward", "(", "\n", "hidden_dim", ",", "\n", "activations", "=", "[", "Activation", ".", "by_name", "(", "\"relu\"", ")", "(", ")", ",", "Activation", ".", "by_name", "(", "\"linear\"", ")", "(", ")", "]", ",", "\n", "hidden_dims", "=", "[", "feedforward_hidden_dim", ",", "hidden_dim", "]", ",", "\n", "num_layers", "=", "2", ",", "\n", "dropout", "=", "dropout_prob", ",", "\n", ")", "\n", "\n", "self", ".", "dropout", "=", "Dropout", "(", "dropout_prob", ")", "\n", "self", ".", "residual_with_layer_dropout", "=", "ResidualWithLayerDropout", "(", "\n", "layer_dropout_undecayed_prob", "\n", ")", "\n", "self", ".", "_input_dim", "=", "input_dim", "\n", "self", ".", "_output_dim", "=", "hidden_dim", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.seq2seq_encoders.qanet_encoder.QaNetEncoderBlock.get_input_dim": [[235, 238], ["None"], "methods", ["None"], ["", "@", "overrides", "\n", "def", "get_input_dim", "(", "self", ")", "->", "int", ":", "\n", "        ", "return", "self", ".", "_input_dim", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.seq2seq_encoders.qanet_encoder.QaNetEncoderBlock.get_output_dim": [[239, 242], ["None"], "methods", ["None"], ["", "@", "overrides", "\n", "def", "get_output_dim", "(", "self", ")", "->", "int", ":", "\n", "        ", "return", "self", ".", "_output_dim", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.seq2seq_encoders.qanet_encoder.QaNetEncoderBlock.is_bidirectional": [[243, 246], ["None"], "methods", ["None"], ["", "@", "overrides", "\n", "def", "is_bidirectional", "(", "self", ")", ":", "\n", "        ", "return", "False", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.seq2seq_encoders.qanet_encoder.QaNetEncoderBlock.forward": [[247, 283], ["zip", "qanet_encoder.QaNetEncoderBlock.dropout", "qanet_encoder.QaNetEncoderBlock.dropout", "qanet_encoder.QaNetEncoderBlock.residual_with_layer_dropout", "qanet_encoder.QaNetEncoderBlock.dropout", "qanet_encoder.QaNetEncoderBlock.dropout", "qanet_encoder.QaNetEncoderBlock.residual_with_layer_dropout", "allennlp.nn.util.add_positional_features", "len", "qanet_encoder.QaNetEncoderBlock.dropout", "qanet_encoder.QaNetEncoderBlock.dropout", "qanet_encoder.QaNetEncoderBlock.residual_with_layer_dropout", "qanet_encoder.QaNetEncoderBlock.attention_norm_layer", "qanet_encoder.QaNetEncoderBlock.attention_layer", "qanet_encoder.QaNetEncoderBlock.feedforward_norm_layer", "qanet_encoder.QaNetEncoderBlock.feedforward", "conv_norm_layer", "conv_layer().transpose_", "conv_layer", "qanet_encoder.QaNetEncoderBlock.transpose_"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.nn.util.add_positional_features"], ["", "@", "overrides", "\n", "def", "forward", "(", "self", ",", "inputs", ":", "torch", ".", "Tensor", ",", "mask", ":", "torch", ".", "Tensor", "=", "None", ")", "->", "torch", ".", "Tensor", ":", "\n", "        ", "if", "self", ".", "_use_positional_encoding", ":", "\n", "            ", "output", "=", "add_positional_features", "(", "inputs", ")", "\n", "", "else", ":", "\n", "            ", "output", "=", "inputs", "\n", "\n", "", "total_sublayers", "=", "len", "(", "self", ".", "_conv_layers", ")", "+", "2", "\n", "sublayer_count", "=", "0", "\n", "\n", "for", "conv_norm_layer", ",", "conv_layer", "in", "zip", "(", "\n", "self", ".", "_conv_norm_layers", ",", "self", ".", "_conv_layers", "\n", ")", ":", "\n", "            ", "conv_norm_out", "=", "self", ".", "dropout", "(", "conv_norm_layer", "(", "output", ")", ")", "\n", "conv_out", "=", "self", ".", "dropout", "(", "\n", "conv_layer", "(", "conv_norm_out", ".", "transpose_", "(", "1", ",", "2", ")", ")", ".", "transpose_", "(", "1", ",", "2", ")", "\n", ")", "\n", "sublayer_count", "+=", "1", "\n", "output", "=", "self", ".", "residual_with_layer_dropout", "(", "\n", "output", ",", "conv_out", ",", "sublayer_count", ",", "total_sublayers", "\n", ")", "\n", "\n", "", "attention_norm_out", "=", "self", ".", "dropout", "(", "self", ".", "attention_norm_layer", "(", "output", ")", ")", "\n", "attention_out", "=", "self", ".", "dropout", "(", "self", ".", "attention_layer", "(", "attention_norm_out", ",", "mask", ")", ")", "\n", "sublayer_count", "+=", "1", "\n", "output", "=", "self", ".", "residual_with_layer_dropout", "(", "\n", "output", ",", "attention_out", ",", "sublayer_count", ",", "total_sublayers", "\n", ")", "\n", "\n", "feedforward_norm_out", "=", "self", ".", "dropout", "(", "self", ".", "feedforward_norm_layer", "(", "output", ")", ")", "\n", "feedforward_out", "=", "self", ".", "dropout", "(", "self", ".", "feedforward", "(", "feedforward_norm_out", ")", ")", "\n", "sublayer_count", "+=", "1", "\n", "output", "=", "self", ".", "residual_with_layer_dropout", "(", "\n", "output", ",", "feedforward_out", ",", "sublayer_count", ",", "total_sublayers", "\n", ")", "\n", "return", "output", "\n", "", "", ""]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.tasks.task.Task.__init__": [[28, 46], ["None"], "methods", ["None"], ["def", "__init__", "(", "\n", "self", ",", "\n", "name", ":", "str", ",", "\n", "validation_metric_name", ":", "str", ",", "\n", "validation_metric_decreases", ":", "bool", ",", "\n", "evaluate_on_test", ":", "bool", "=", "False", ",", "\n", ")", "->", "None", ":", "\n", "        ", "self", ".", "_name", "=", "name", "\n", "\n", "self", ".", "_train_data", "=", "None", "\n", "self", ".", "_validation_data", "=", "None", "\n", "self", ".", "_test_data", "=", "None", "\n", "self", ".", "_evaluate_on_test", "=", "evaluate_on_test", "\n", "\n", "self", ".", "_val_metric", "=", "validation_metric_name", "\n", "self", ".", "_val_metric_decreases", "=", "validation_metric_decreases", "\n", "\n", "self", ".", "_data_iterator", "=", "None", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.tasks.task.Task.set_data_iterator": [[47, 53], ["allennlp.common.checks.ConfigurationError"], "methods", ["None"], ["", "def", "set_data_iterator", "(", "self", ",", "data_iterator", ":", "DataIterator", ")", ":", "\n", "        ", "if", "data_iterator", "is", "not", "None", ":", "\n", "            ", "self", ".", "_data_iterator", "=", "data_iterator", "\n", "", "else", ":", "\n", "            ", "ConfigurationError", "(", "\n", "f\"data_iterator cannot be None in set_iterator - Task name: {self._name}\"", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.tasks.task.Task.load_data_from_params": [[55, 99], ["allennlp.training.util.datasets_from_params", "set", "params.pop", "allennlp.training.util.datasets_from_params.keys", "sum", "allennlp.training.util.datasets_from_params.keys", "sum", "allennlp.training.util.datasets_from_params.keys", "sum", "allennlp.common.checks.ConfigurationError", "allennlp.training.util.datasets_from_params.items"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.training.util.datasets_from_params", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.params.Params.pop"], ["", "", "def", "load_data_from_params", "(", "self", ",", "params", ":", "Params", ")", ":", "\n", "        ", "all_datasets", "=", "datasets_from_params", "(", "params", ")", "\n", "datasets_for_vocab_creation", "=", "set", "(", "\n", "params", ".", "pop", "(", "\"datasets_for_vocab_creation\"", ",", "all_datasets", ")", "\n", ")", "\n", "\n", "for", "dataset", "in", "datasets_for_vocab_creation", ":", "\n", "            ", "if", "dataset", "not", "in", "all_datasets", ":", "\n", "                ", "raise", "ConfigurationError", "(", "\n", "f\"invalid 'dataset_for_vocab_creation' {dataset}\"", "\n", ")", "\n", "\n", "", "", "instances_for_vocab_creation", "=", "(", "\n", "instance", "\n", "for", "key", ",", "dataset", "in", "all_datasets", ".", "items", "(", ")", "\n", "for", "instance", "in", "dataset", "\n", "if", "key", "in", "datasets_for_vocab_creation", "\n", ")", "\n", "\n", "self", ".", "_instances_for_vocab_creation", "=", "instances_for_vocab_creation", "\n", "self", ".", "_datasets_for_vocab_creation", "=", "datasets_for_vocab_creation", "\n", "\n", "if", "\"train\"", "in", "all_datasets", ".", "keys", "(", ")", ":", "\n", "            ", "self", ".", "_train_data", "=", "all_datasets", "[", "\"train\"", "]", "\n", "self", ".", "_tr_instances", "=", "sum", "(", "\n", "1", "for", "e", "in", "self", ".", "_train_data", "\n", ")", "# This is horrible if lazy iterator (Iterable)", "\n", "", "if", "\"validation\"", "in", "all_datasets", ".", "keys", "(", ")", ":", "\n", "            ", "self", ".", "_validation_data", "=", "all_datasets", "[", "\"validation\"", "]", "\n", "self", ".", "_val_instances", "=", "sum", "(", "\n", "1", "for", "e", "in", "self", ".", "_validation_data", "\n", ")", "# This is horrible if lazy iterator (Iterable)", "\n", "", "if", "\"test\"", "in", "all_datasets", ".", "keys", "(", ")", ":", "\n", "            ", "self", ".", "_test_data", "=", "all_datasets", "[", "\"test\"", "]", "\n", "self", ".", "_test_instances", "=", "sum", "(", "\n", "1", "for", "e", "in", "self", ".", "_test_data", "\n", ")", "# This is horrible if lazy iterator (Iterable)", "\n", "\n", "# If trying to evaluate on test set, make sure the dataset is loaded", "\n", "", "if", "self", ".", "_evaluate_on_test", ":", "\n", "            ", "assert", "self", ".", "_test_data", "is", "not", "None", "\n", "\n", "# return instances_for_vocab_creation, datasets_for_vocab_creation, all_datasets", "\n", "", "return", "instances_for_vocab_creation", ",", "datasets_for_vocab_creation", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.tasks.task.Task.from_params": [[100, 117], ["params.pop", "params.pop", "params.pop_bool", "params.pop_bool", "params.assert_empty", "cls"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.params.Params.pop", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.params.Params.pop", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.params.Params.pop_bool", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.params.Params.pop_bool", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.params.Params.assert_empty"], ["", "@", "classmethod", "\n", "def", "from_params", "(", "cls", ",", "params", ":", "Params", ")", "->", "\"Task\"", ":", "\n", "        ", "task_name", "=", "params", ".", "pop", "(", "\"task_name\"", ",", "\"ner\"", ")", "\n", "validation_metric_name", "=", "params", ".", "pop", "(", "\n", "\"validation_metric_name\"", ",", "\"f1-measure-overall\"", "\n", ")", "\n", "validation_metric_decreases", "=", "params", ".", "pop_bool", "(", "\n", "\"validation_metric_decreases\"", ",", "False", "\n", ")", "\n", "evaluate_on_test", "=", "params", ".", "pop_bool", "(", "\"evaluate_on_test\"", ",", "False", ")", "\n", "\n", "params", ".", "assert_empty", "(", "cls", ".", "__name__", ")", "\n", "return", "cls", "(", "\n", "name", "=", "task_name", ",", "\n", "validation_metric_name", "=", "validation_metric_name", ",", "\n", "validation_metric_decreases", "=", "validation_metric_decreases", ",", "\n", "evaluate_on_test", "=", "evaluate_on_test", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.dataset_readers.srl_conll_2005.SrlReader05.__init__": [[131, 148], ["allennlp.data.dataset_readers.dataset_reader.DatasetReader.__init__", "transformers.tokenization_bert.BertTokenizer.from_pretrained", "allennlp.data.token_indexers.SingleIdTokenIndexer"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.graph_util.plot.GraphPlot.__init__"], ["def", "__init__", "(", "\n", "self", ",", "\n", "token_indexers", ":", "Dict", "[", "str", ",", "TokenIndexer", "]", "=", "None", ",", "\n", "bert_model_name", ":", "str", "=", "None", ",", "\n", "subset_size", ":", "int", "=", "sys", ".", "maxsize", ",", "\n", "**", "kwargs", ",", "\n", ")", "->", "None", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "**", "kwargs", ")", "\n", "self", ".", "_token_indexers", "=", "token_indexers", "or", "{", "\"tokens\"", ":", "SingleIdTokenIndexer", "(", ")", "}", "\n", "self", ".", "_to_yield", "=", "subset_size", "\n", "\n", "if", "bert_model_name", "is", "not", "None", ":", "\n", "            ", "self", ".", "bert_tokenizer", "=", "BertTokenizer", ".", "from_pretrained", "(", "bert_model_name", ")", "\n", "self", ".", "lowercase_input", "=", "\"uncased\"", "in", "bert_model_name", "\n", "", "else", ":", "\n", "            ", "self", ".", "bert_tokenizer", "=", "None", "\n", "self", ".", "lowercase_input", "=", "False", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.dataset_readers.srl_conll_2005.SrlReader05._wordpiece_tokenize_input": [[149, 201], ["srl_conll_2005.SrlReader05.bert_tokenizer.wordpiece_tokenizer.tokenize", "start_offsets.append", "len", "end_offsets.append", "word_piece_tokens.extend", "token.lower.lower.lower"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.tokenizers.whitespace_tokenizer.WhitespaceTokenizer.tokenize"], ["", "", "def", "_wordpiece_tokenize_input", "(", "\n", "self", ",", "tokens", ":", "List", "[", "str", "]", "\n", ")", "->", "Tuple", "[", "List", "[", "str", "]", ",", "List", "[", "int", "]", ",", "List", "[", "int", "]", "]", ":", "\n", "        ", "\"\"\"\n        Convert a list of tokens to wordpiece tokens and offsets, as well as adding\n        BERT CLS and SEP tokens to the begining and end of the sentence.\n\n        A slight oddity with this function is that it also returns the wordpiece offsets\n        corresponding to the _start_ of words as well as the end.\n\n        We need both of these offsets (or at least, it's easiest to use both), because we need\n        to convert the labels to tags using the end_offsets. However, when we are decoding a\n        BIO sequence inside the SRL model itself, it's important that we use the start_offsets,\n        because otherwise we might select an ill-formed BIO sequence from the BIO sequence on top of\n        wordpieces (this happens in the case that a word is split into multiple word pieces,\n        and then we take the last tag of the word, which might correspond to, e.g, I-V, which\n        would not be allowed as it is not preceeded by a B tag).\n\n        For example:\n\n        `annotate` will be bert tokenized as [\"anno\", \"##tate\"].\n        If this is tagged as [B-V, I-V] as it should be, we need to select the\n        _first_ wordpiece label to be the label for the token, because otherwise\n        we may end up with invalid tag sequences (we cannot start a new tag with an I).\n\n        # Returns\n\n        wordpieces : List[str]\n            The BERT wordpieces from the words in the sentence.\n        end_offsets : List[int]\n            Indices into wordpieces such that `[wordpieces[i] for i in end_offsets]`\n            results in the end wordpiece of each word being chosen.\n        start_offsets : List[int]\n            Indices into wordpieces such that `[wordpieces[i] for i in start_offsets]`\n            results in the start wordpiece of each word being chosen.\n        \"\"\"", "\n", "word_piece_tokens", ":", "List", "[", "str", "]", "=", "[", "]", "\n", "end_offsets", "=", "[", "]", "\n", "start_offsets", "=", "[", "]", "\n", "cumulative", "=", "0", "\n", "for", "token", "in", "tokens", ":", "\n", "            ", "if", "self", ".", "lowercase_input", ":", "\n", "                ", "token", "=", "token", ".", "lower", "(", ")", "\n", "", "word_pieces", "=", "self", ".", "bert_tokenizer", ".", "wordpiece_tokenizer", ".", "tokenize", "(", "token", ")", "\n", "start_offsets", ".", "append", "(", "cumulative", "+", "1", ")", "\n", "cumulative", "+=", "len", "(", "word_pieces", ")", "\n", "end_offsets", ".", "append", "(", "cumulative", ")", "\n", "word_piece_tokens", ".", "extend", "(", "word_pieces", ")", "\n", "\n", "", "wordpieces", "=", "[", "\"[CLS]\"", "]", "+", "word_piece_tokens", "+", "[", "\"[SEP]\"", "]", "\n", "\n", "return", "wordpieces", ",", "end_offsets", ",", "start_offsets", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.dataset_readers.srl_conll_2005.SrlReader05._read": [[202, 250], ["allennlp.common.file_utils.cached_path", "allennlp.data.dataset_readers.dataset_utils.Ontonotes", "open", "line.strip.strip.strip", "line.strip.strip.split", "conll_rows.append", "len", "line.strip.split.append", "allennlp.data.dataset_readers.dataset_utils.Ontonotes._conll_rows_to_sentence", "allennlp.data.tokenizers.Token", "srl_conll_2005.SrlReader05.text_to_instance", "srl_conll_2005.SrlReader05.text_to_instance"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.file_utils.cached_path", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.dataset_utils.ontonotes.Ontonotes._conll_rows_to_sentence", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.coreference_resolution.conll.ConllCorefReader.text_to_instance", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.coreference_resolution.conll.ConllCorefReader.text_to_instance"], ["", "@", "overrides", "\n", "def", "_read", "(", "self", ",", "file_path", ":", "str", ")", ":", "\n", "        ", "file_path", "=", "cached_path", "(", "file_path", ")", "\n", "ontonotes_helper", "=", "Ontonotes", "(", ")", "\n", "with", "open", "(", "file_path", ",", "\"r\"", ",", "encoding", "=", "\"utf-8\"", ")", "as", "open_file", ":", "\n", "            ", "conll_rows", "=", "[", "]", "\n", "for", "line", "in", "open_file", ":", "\n", "                ", "line", "=", "line", ".", "strip", "(", ")", "\n", "if", "line", "!=", "\"\"", ":", "\n", "# Non-empty line. Collect the annotation in Ontonotes format", "\n", "                    ", "items", "=", "line", ".", "split", "(", ")", "\n", "if", "len", "(", "items", ")", "<", "7", ":", "\n", "                        ", "items", ".", "append", "(", "\"-\"", ")", "# for sentences which have no SRL annotations", "\n", "", "conll_rows", ".", "append", "(", "\n", "\"\\t\"", ".", "join", "(", "\n", "[", "\"DocID\"", ",", "\"0\"", ",", "\"-\"", ",", "items", "[", "0", "]", ",", "\"XX\"", ",", "\"-\"", ",", "\"-\"", ",", "\"-\"", ",", "\"-\"", ",", "\"-\"", "]", "\n", "+", "items", "[", "6", ":", "]", "\n", "+", "[", "\"-\"", "]", "\n", ")", "\n", ")", "\n", "", "else", ":", "\n", "                    ", "if", "self", ".", "_to_yield", "==", "0", ":", "\n", "                        ", "break", "\n", "", "elif", "conll_rows", ":", "\n", "                        ", "sentence", "=", "ontonotes_helper", ".", "_conll_rows_to_sentence", "(", "conll_rows", ")", "\n", "conll_rows", "=", "[", "]", "\n", "\n", "tokens", "=", "[", "Token", "(", "t", ")", "for", "t", "in", "sentence", ".", "words", "]", "\n", "if", "not", "sentence", ".", "srl_frames", ":", "\n", "# Sentence contains no predicates.", "\n", "                            ", "tags", "=", "[", "\"O\"", "for", "_", "in", "tokens", "]", "\n", "verb_label", "=", "[", "0", "for", "_", "in", "tokens", "]", "\n", "yield", "self", ".", "text_to_instance", "(", "tokens", ",", "verb_label", ",", "tags", ")", "\n", "", "else", ":", "\n", "                            ", "for", "(", "_", ",", "tags", ")", "in", "sentence", ".", "srl_frames", ":", "\n", "                                ", "if", "not", "self", ".", "bert_tokenizer", ":", "\n", "# Ontonotes does not have 2 token verbs and if there is no", "\n", "# bert tokenizer to split a one-word verb into word pieces,", "\n", "# the model would have never seen I-V during training.", "\n", "# So we just take the first token as the verb.", "\n", "                                    ", "tags", "=", "[", "tag", "if", "tag", "!=", "\"I-V\"", "else", "\"O\"", "for", "tag", "in", "tags", "]", "\n", "", "verb_indicator", "=", "[", "\n", "1", "if", "label", "[", "-", "2", ":", "]", "==", "\"-V\"", "else", "0", "for", "label", "in", "tags", "\n", "]", "\n", "yield", "self", ".", "text_to_instance", "(", "\n", "tokens", ",", "verb_indicator", ",", "tags", "\n", ")", "\n", "", "", "self", ".", "_to_yield", "-=", "1", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.dataset_readers.srl_conll_2005.SrlReader05.text_to_instance": [[251, 304], ["all", "allennlp.data.fields.MetadataField", "allennlp.data.instance.Instance", "srl_conll_2005.SrlReader05._wordpiece_tokenize_input", "srl_conll_2005._convert_verb_indices_to_wordpiece_indices", "allennlp.data.fields.TextField", "allennlp.data.fields.SequenceLabelField", "allennlp.data.fields.TextField", "allennlp.data.fields.SequenceLabelField", "verb_label.index", "srl_conll_2005._convert_tags_to_wordpiece_tags", "allennlp.data.fields.SequenceLabelField", "allennlp.data.fields.SequenceLabelField", "allennlp.data.tokenizers.Token"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.dataset_readers.semantic_role_labeling.SrlReader._wordpiece_tokenize_input", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.dataset_readers.semantic_role_labeling._convert_verb_indices_to_wordpiece_indices", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.fields.text_field.TextField.index", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.dataset_readers.semantic_role_labeling._convert_tags_to_wordpiece_tags"], ["", "", "", "", "", "def", "text_to_instance", "(", "# type: ignore", "\n", "self", ",", "tokens", ":", "List", "[", "Token", "]", ",", "verb_label", ":", "List", "[", "int", "]", ",", "tags", ":", "List", "[", "str", "]", "=", "None", "\n", ")", "->", "Instance", ":", "\n", "        ", "\"\"\"\n        We take `pre-tokenized` input here, along with a verb label.  The verb label should be a\n        one-hot binary vector, the same length as the tokens, indicating the position of the verb\n        to find arguments for.\n        \"\"\"", "\n", "\n", "metadata_dict", ":", "Dict", "[", "str", ",", "Any", "]", "=", "{", "}", "\n", "if", "self", ".", "bert_tokenizer", "is", "not", "None", ":", "\n", "            ", "wordpieces", ",", "offsets", ",", "start_offsets", "=", "self", ".", "_wordpiece_tokenize_input", "(", "\n", "[", "t", ".", "text", "for", "t", "in", "tokens", "]", "\n", ")", "\n", "new_verbs", "=", "_convert_verb_indices_to_wordpiece_indices", "(", "verb_label", ",", "offsets", ")", "\n", "metadata_dict", "[", "\"offsets\"", "]", "=", "start_offsets", "\n", "# In order to override the indexing mechanism, we need to set the `text_id`", "\n", "# attribute directly. This causes the indexing to use this id.", "\n", "text_field", "=", "TextField", "(", "\n", "[", "Token", "(", "t", ",", "text_id", "=", "self", ".", "bert_tokenizer", ".", "vocab", "[", "t", "]", ")", "for", "t", "in", "wordpieces", "]", ",", "\n", "token_indexers", "=", "self", ".", "_token_indexers", ",", "\n", ")", "\n", "verb_indicator", "=", "SequenceLabelField", "(", "new_verbs", ",", "text_field", ")", "\n", "\n", "", "else", ":", "\n", "            ", "text_field", "=", "TextField", "(", "tokens", ",", "token_indexers", "=", "self", ".", "_token_indexers", ")", "\n", "verb_indicator", "=", "SequenceLabelField", "(", "verb_label", ",", "text_field", ")", "\n", "\n", "", "fields", ":", "Dict", "[", "str", ",", "Field", "]", "=", "{", "}", "\n", "fields", "[", "\"tokens\"", "]", "=", "text_field", "\n", "fields", "[", "\"verb_indicator\"", "]", "=", "verb_indicator", "\n", "\n", "if", "all", "(", "[", "x", "==", "0", "for", "x", "in", "verb_label", "]", ")", ":", "\n", "            ", "verb", "=", "None", "\n", "verb_index", "=", "None", "\n", "", "else", ":", "\n", "            ", "verb_index", "=", "verb_label", ".", "index", "(", "1", ")", "\n", "verb", "=", "tokens", "[", "verb_index", "]", ".", "text", "\n", "\n", "", "metadata_dict", "[", "\"words\"", "]", "=", "[", "x", ".", "text", "for", "x", "in", "tokens", "]", "\n", "metadata_dict", "[", "\"verb\"", "]", "=", "verb", "\n", "metadata_dict", "[", "\"verb_index\"", "]", "=", "verb_index", "\n", "\n", "if", "tags", ":", "\n", "            ", "if", "self", ".", "bert_tokenizer", "is", "not", "None", ":", "\n", "                ", "new_tags", "=", "_convert_tags_to_wordpiece_tags", "(", "tags", ",", "offsets", ")", "\n", "fields", "[", "\"tags\"", "]", "=", "SequenceLabelField", "(", "new_tags", ",", "text_field", ")", "\n", "", "else", ":", "\n", "                ", "fields", "[", "\"tags\"", "]", "=", "SequenceLabelField", "(", "tags", ",", "text_field", ")", "\n", "", "metadata_dict", "[", "\"gold_tags\"", "]", "=", "tags", "\n", "\n", "", "fields", "[", "\"metadata\"", "]", "=", "MetadataField", "(", "metadata_dict", ")", "\n", "return", "Instance", "(", "fields", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.dataset_readers.srl_conll_2005._convert_tags_to_wordpiece_tags": [[20, 63], ["enumerate", "new_tags.append", "tag.startswith", "new_tags.append", "tag.startswith", "new_tags.append", "tag.startswith", "tag.split", "new_tags.append"], "function", ["None"], ["def", "_convert_tags_to_wordpiece_tags", "(", "tags", ":", "List", "[", "str", "]", ",", "offsets", ":", "List", "[", "int", "]", ")", "->", "List", "[", "str", "]", ":", "\n", "    ", "\"\"\"\n    Converts a series of BIO tags to account for a wordpiece tokenizer,\n    extending/modifying BIO tags where appropriate to deal with words which\n    are split into multiple wordpieces by the tokenizer.\n\n    This is only used if you pass a `bert_model_name` to the dataset reader below.\n\n    # Parameters\n\n    tags : `List[str]`\n        The BIO formatted tags to convert to BIO tags for wordpieces\n    offsets : `List[int]`\n        The wordpiece offsets.\n\n    # Returns\n\n    The new BIO tags.\n    \"\"\"", "\n", "new_tags", "=", "[", "]", "\n", "j", "=", "0", "\n", "for", "i", ",", "offset", "in", "enumerate", "(", "offsets", ")", ":", "\n", "        ", "tag", "=", "tags", "[", "i", "]", "\n", "is_o", "=", "tag", "==", "\"O\"", "\n", "is_start", "=", "True", "\n", "while", "j", "<", "offset", ":", "\n", "            ", "if", "is_o", ":", "\n", "                ", "new_tags", ".", "append", "(", "\"O\"", ")", "\n", "\n", "", "elif", "tag", ".", "startswith", "(", "\"I\"", ")", ":", "\n", "                ", "new_tags", ".", "append", "(", "tag", ")", "\n", "\n", "", "elif", "is_start", "and", "tag", ".", "startswith", "(", "\"B\"", ")", ":", "\n", "                ", "new_tags", ".", "append", "(", "tag", ")", "\n", "is_start", "=", "False", "\n", "\n", "", "elif", "tag", ".", "startswith", "(", "\"B\"", ")", ":", "\n", "                ", "_", ",", "label", "=", "tag", ".", "split", "(", "\"-\"", ",", "1", ")", "\n", "new_tags", ".", "append", "(", "\"I-\"", "+", "label", ")", "\n", "", "j", "+=", "1", "\n", "\n", "# Add O tags for cls and sep tokens.", "\n", "", "", "return", "[", "\"O\"", "]", "+", "new_tags", "+", "[", "\"O\"", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.dataset_readers.srl_conll_2005._convert_verb_indices_to_wordpiece_indices": [[65, 96], ["enumerate", "new_verb_indices.append"], "function", ["None"], ["", "def", "_convert_verb_indices_to_wordpiece_indices", "(", "\n", "verb_indices", ":", "List", "[", "int", "]", ",", "offsets", ":", "List", "[", "int", "]", "\n", ")", ":", "\n", "    ", "\"\"\"\n    Converts binary verb indicators to account for a wordpiece tokenizer,\n    extending/modifying BIO tags where appropriate to deal with words which\n    are split into multiple wordpieces by the tokenizer.\n\n    This is only used if you pass a `bert_model_name` to the dataset reader below.\n\n    # Parameters\n\n    verb_indices : `List[int]`\n        The binary verb indicators, 0 for not a verb, 1 for verb.\n    offsets : `List[int]`\n        The wordpiece offsets.\n\n    # Returns\n\n    The new verb indices.\n    \"\"\"", "\n", "j", "=", "0", "\n", "new_verb_indices", "=", "[", "]", "\n", "for", "i", ",", "offset", "in", "enumerate", "(", "offsets", ")", ":", "\n", "        ", "indicator", "=", "verb_indices", "[", "i", "]", "\n", "while", "j", "<", "offset", ":", "\n", "            ", "new_verb_indices", ".", "append", "(", "indicator", ")", "\n", "j", "+=", "1", "\n", "\n", "# Add 0 indicators for cls and sep tokens.", "\n", "", "", "return", "[", "0", "]", "+", "new_verb_indices", "+", "[", "0", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.dataset_readers.lm_conll.ConllLanguageModelingReader.__init__": [[48, 68], ["allennlp.data.dataset_readers.dataset_reader.DatasetReader.__init__", "allennlp.data.tokenizers.WordTokenizer", "allennlp.data.tokenizers.WordTokenizer", "allennlp.data.token_indexers.SingleIdTokenIndexer", "allennlp.data.token_indexers.SingleIdTokenIndexer"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.graph_util.plot.GraphPlot.__init__"], ["def", "__init__", "(", "\n", "self", ",", "\n", "source_tokenizer", ":", "Tokenizer", "=", "None", ",", "\n", "target_tokenizer", ":", "Tokenizer", "=", "None", ",", "\n", "source_token_indexers", ":", "Dict", "[", "str", ",", "TokenIndexer", "]", "=", "None", ",", "\n", "target_token_indexers", ":", "Dict", "[", "str", ",", "TokenIndexer", "]", "=", "None", ",", "\n", "max_tokens", ":", "int", "=", "None", ",", "\n", "lazy", ":", "bool", "=", "False", ",", "\n", ")", "->", "None", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "lazy", ")", "\n", "self", ".", "_source_tokenizer", "=", "source_tokenizer", "or", "WordTokenizer", "(", ")", "\n", "self", ".", "_target_tokenizer", "=", "target_tokenizer", "or", "WordTokenizer", "(", ")", "\n", "\n", "self", ".", "_source_token_indexers", "=", "source_token_indexers", "or", "{", "\n", "\"tokens\"", ":", "SingleIdTokenIndexer", "(", ")", "\n", "}", "\n", "self", ".", "_target_token_indexers", "=", "target_token_indexers", "or", "{", "\n", "\"tokens\"", ":", "SingleIdTokenIndexer", "(", ")", "\n", "}", "\n", "self", ".", "_max_tokens", "=", "max_tokens", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.dataset_readers.lm_conll.ConllLanguageModelingReader._normalize_word": [[69, 74], ["None"], "methods", ["None"], ["", "def", "_normalize_word", "(", "self", ",", "word", ")", ":", "\n", "        ", "if", "word", "==", "\"/.\"", "or", "word", "==", "\"/?\"", ":", "\n", "            ", "return", "word", "[", "1", ":", "]", "\n", "", "else", ":", "\n", "            ", "return", "word", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.dataset_readers.lm_conll.ConllLanguageModelingReader.read_ontonotes": [[75, 82], ["allennlp.data.dataset_readers.dataset_utils.ontonotes.Ontonotes", "allennlp.data.dataset_readers.dataset_utils.ontonotes.Ontonotes.dataset_document_iterator"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.dataset_utils.ontonotes.Ontonotes.dataset_document_iterator"], ["", "", "def", "read_ontonotes", "(", "self", ",", "file_path", ")", ":", "\n", "        ", "ontonotes_reader", "=", "Ontonotes", "(", ")", "\n", "for", "doc", "in", "ontonotes_reader", ".", "dataset_document_iterator", "(", "file_path", ")", ":", "\n", "            ", "yield", "[", "START_SYMBOL", "]", "+", "[", "\n", "word", "for", "sentence", "in", "doc", "for", "word", "in", "sentence", ".", "words", "\n", "]", "+", "[", "\n", "END_SYMBOL", "\n", "]", "# List[str]", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.dataset_readers.lm_conll.ConllLanguageModelingReader._read": [[85, 111], ["pickle.load.items", "open", "pickle.load", "allennlp.data.tokenizers.token.Token", "range", "allennlp.data.fields.TextField", "allennlp.data.fields.TextField", "tokenized_text.append", "allennlp.data.instance.Instance", "len", "allennlp.data.fields.MetadataField"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.token_embedders.bert_token_embedder.PretrainedBertModel.load"], ["", "", "@", "overrides", "\n", "def", "_read", "(", "self", ",", "file_path", ":", "str", ")", ":", "\n", "        ", "with", "open", "(", "file_path", ",", "\"rb\"", ")", "as", "f", ":", "\n", "            ", "data", "=", "pkl", ".", "load", "(", "f", ")", "\n", "", "for", "key", ",", "values", "in", "data", ".", "items", "(", ")", ":", "\n", "            ", "document", "=", "[", "START_SYMBOL", "]", "+", "values", "[", "\"document\"", "]", "[", "0", "]", "+", "[", "END_SYMBOL", "]", "\n", "node_repr", "=", "values", "[", "\"node_repr\"", "]", "\n", "# we need not tokenize here because Ontonotes sentences are already tokenized", "\n", "tokenized_doc", "=", "[", "Token", "(", "word", ")", "for", "word", "in", "document", "]", "\n", "if", "self", ".", "_max_tokens", "is", "not", "None", ":", "\n", "                ", "num_tokens", "=", "self", ".", "_max_tokens", "+", "1", "\n", "tokenized_text", "=", "[", "]", "\n", "for", "index", "in", "range", "(", "0", ",", "len", "(", "tokenized_doc", ")", "-", "num_tokens", ",", "num_tokens", "-", "1", ")", ":", "\n", "                    ", "tokenized_text", ".", "append", "(", "tokenized_doc", "[", "index", ":", "(", "index", "+", "num_tokens", ")", "]", ")", "\n", "", "", "else", ":", "\n", "                ", "tokenized_text", "=", "[", "tokenized_doc", "]", "\n", "\n", "", "for", "text", "in", "tokenized_text", ":", "\n", "                ", "input_field", "=", "TextField", "(", "text", "[", ":", "-", "1", "]", ",", "self", ".", "_source_token_indexers", ")", "\n", "output_field", "=", "TextField", "(", "text", "[", "1", ":", "]", ",", "self", ".", "_target_token_indexers", ")", "\n", "yield", "Instance", "(", "\n", "{", "\n", "\"source_tokens\"", ":", "input_field", ",", "\n", "\"target_tokens\"", ":", "output_field", ",", "\n", "\"metadata\"", ":", "MetadataField", "(", "\n", "{", "\"key\"", ":", "key", ",", "\"node_repr\"", ":", "node_repr", ",", "\"document\"", ":", "document", "}", "\n", ")", ",", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.dataset_readers.lm_conll.ConllLanguageModelingReader._readO": [[116, 135], ["allennlp.common.file_utils.cached_path", "lm_conll.ConllLanguageModelingReader.read_ontonotes", "allennlp.data.tokenizers.token.Token", "range", "allennlp.data.fields.TextField", "allennlp.data.fields.TextField", "tokenized_text.append", "allennlp.data.instance.Instance", "len"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.file_utils.cached_path", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.dataset_readers.lm_conll.ConllLanguageModelingReader.read_ontonotes"], ["", "", "", "def", "_readO", "(", "self", ",", "file_path", ":", "str", ")", ":", "\n", "# if `file_path` is a URL, redirect to the cache", "\n", "        ", "file_path", "=", "cached_path", "(", "file_path", ")", "\n", "for", "doc", "in", "self", ".", "read_ontonotes", "(", "file_path", ")", ":", "\n", "# we need not tokenize here because Ontonotes sentences are already tokenized", "\n", "            ", "tokenized_doc", "=", "[", "Token", "(", "word", ")", "for", "word", "in", "doc", "]", "\n", "if", "self", ".", "_max_tokens", "is", "not", "None", ":", "\n", "                ", "num_tokens", "=", "self", ".", "_max_tokens", "+", "1", "\n", "tokenized_text", "=", "[", "]", "\n", "for", "index", "in", "range", "(", "0", ",", "len", "(", "tokenized_doc", ")", "-", "num_tokens", ",", "num_tokens", "-", "1", ")", ":", "\n", "                    ", "tokenized_text", ".", "append", "(", "tokenized_doc", "[", "index", ":", "(", "index", "+", "num_tokens", ")", "]", ")", "\n", "", "", "else", ":", "\n", "                ", "tokenized_text", "=", "[", "tokenized_doc", "]", "\n", "\n", "", "for", "text", "in", "tokenized_text", ":", "\n", "                ", "input_field", "=", "TextField", "(", "text", "[", ":", "-", "1", "]", ",", "self", ".", "_source_token_indexers", ")", "\n", "output_field", "=", "TextField", "(", "text", "[", "1", ":", "]", ",", "self", ".", "_target_indexer", ")", "\n", "yield", "Instance", "(", "\n", "{", "\"source_tokens\"", ":", "input_field", ",", "\"target_tokens\"", ":", "output_field", "}", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.dataset_readers.lm_conll.ConllLanguageModelingReader.text_to_instance": [[137, 144], ["lm_conll.ConllLanguageModelingReader._source_tokenizer.tokenize", "lm_conll.ConllLanguageModelingReader._target_tokenizer.tokenize", "allennlp.data.fields.TextField", "allennlp.data.fields.TextField", "allennlp.data.instance.Instance"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.tokenizers.whitespace_tokenizer.WhitespaceTokenizer.tokenize", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.tokenizers.whitespace_tokenizer.WhitespaceTokenizer.tokenize"], ["", "", "", "@", "overrides", "\n", "def", "text_to_instance", "(", "self", ",", "sentence", ":", "str", ")", "->", "Instance", ":", "# type: ignore", "\n", "        ", "source_text", "=", "self", ".", "_source_tokenizer", ".", "tokenize", "(", "sentence", ")", "\n", "target_text", "=", "self", ".", "_target_tokenizer", ".", "tokenize", "(", "sentence", ")", "\n", "input_field", "=", "TextField", "(", "source_text", "[", ":", "-", "1", "]", ",", "self", ".", "_source_token_indexers", ")", "\n", "output_field", "=", "TextField", "(", "target_text", "[", "1", ":", "]", ",", "self", ".", "_target_token_indexers", ")", "\n", "return", "Instance", "(", "{", "\"source_tokens\"", ":", "input_field", ",", "\"target_tokens\"", ":", "output_field", "}", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.dataset_readers.srl_wikibank.SrlWikibankReader.__init__": [[131, 148], ["allennlp.data.dataset_readers.dataset_reader.DatasetReader.__init__", "transformers.tokenization_bert.BertTokenizer.from_pretrained", "allennlp.data.token_indexers.SingleIdTokenIndexer"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.graph_util.plot.GraphPlot.__init__"], ["def", "__init__", "(", "\n", "self", ",", "\n", "token_indexers", ":", "Dict", "[", "str", ",", "TokenIndexer", "]", "=", "None", ",", "\n", "bert_model_name", ":", "str", "=", "None", ",", "\n", "subset_size", ":", "int", "=", "sys", ".", "maxsize", ",", "\n", "**", "kwargs", ",", "\n", ")", "->", "None", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "**", "kwargs", ")", "\n", "self", ".", "_token_indexers", "=", "token_indexers", "or", "{", "\"tokens\"", ":", "SingleIdTokenIndexer", "(", ")", "}", "\n", "self", ".", "_to_yield", "=", "subset_size", "\n", "\n", "if", "bert_model_name", "is", "not", "None", ":", "\n", "            ", "self", ".", "bert_tokenizer", "=", "BertTokenizer", ".", "from_pretrained", "(", "bert_model_name", ")", "\n", "self", ".", "lowercase_input", "=", "\"uncased\"", "in", "bert_model_name", "\n", "", "else", ":", "\n", "            ", "self", ".", "bert_tokenizer", "=", "None", "\n", "self", ".", "lowercase_input", "=", "False", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.dataset_readers.srl_wikibank.SrlWikibankReader._wordpiece_tokenize_input": [[149, 201], ["srl_wikibank.SrlWikibankReader.bert_tokenizer.wordpiece_tokenizer.tokenize", "start_offsets.append", "len", "end_offsets.append", "word_piece_tokens.extend", "token.lower.lower.lower"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.tokenizers.whitespace_tokenizer.WhitespaceTokenizer.tokenize"], ["", "", "def", "_wordpiece_tokenize_input", "(", "\n", "self", ",", "tokens", ":", "List", "[", "str", "]", "\n", ")", "->", "Tuple", "[", "List", "[", "str", "]", ",", "List", "[", "int", "]", ",", "List", "[", "int", "]", "]", ":", "\n", "        ", "\"\"\"\n        Convert a list of tokens to wordpiece tokens and offsets, as well as adding\n        BERT CLS and SEP tokens to the begining and end of the sentence.\n\n        A slight oddity with this function is that it also returns the wordpiece offsets\n        corresponding to the _start_ of words as well as the end.\n\n        We need both of these offsets (or at least, it's easiest to use both), because we need\n        to convert the labels to tags using the end_offsets. However, when we are decoding a\n        BIO sequence inside the SRL model itself, it's important that we use the start_offsets,\n        because otherwise we might select an ill-formed BIO sequence from the BIO sequence on top of\n        wordpieces (this happens in the case that a word is split into multiple word pieces,\n        and then we take the last tag of the word, which might correspond to, e.g, I-V, which\n        would not be allowed as it is not preceeded by a B tag).\n\n        For example:\n\n        `annotate` will be bert tokenized as [\"anno\", \"##tate\"].\n        If this is tagged as [B-V, I-V] as it should be, we need to select the\n        _first_ wordpiece label to be the label for the token, because otherwise\n        we may end up with invalid tag sequences (we cannot start a new tag with an I).\n\n        # Returns\n\n        wordpieces : List[str]\n            The BERT wordpieces from the words in the sentence.\n        end_offsets : List[int]\n            Indices into wordpieces such that `[wordpieces[i] for i in end_offsets]`\n            results in the end wordpiece of each word being chosen.\n        start_offsets : List[int]\n            Indices into wordpieces such that `[wordpieces[i] for i in start_offsets]`\n            results in the start wordpiece of each word being chosen.\n        \"\"\"", "\n", "word_piece_tokens", ":", "List", "[", "str", "]", "=", "[", "]", "\n", "end_offsets", "=", "[", "]", "\n", "start_offsets", "=", "[", "]", "\n", "cumulative", "=", "0", "\n", "for", "token", "in", "tokens", ":", "\n", "            ", "if", "self", ".", "lowercase_input", ":", "\n", "                ", "token", "=", "token", ".", "lower", "(", ")", "\n", "", "word_pieces", "=", "self", ".", "bert_tokenizer", ".", "wordpiece_tokenizer", ".", "tokenize", "(", "token", ")", "\n", "start_offsets", ".", "append", "(", "cumulative", "+", "1", ")", "\n", "cumulative", "+=", "len", "(", "word_pieces", ")", "\n", "end_offsets", ".", "append", "(", "cumulative", ")", "\n", "word_piece_tokens", ".", "extend", "(", "word_pieces", ")", "\n", "\n", "", "wordpieces", "=", "[", "\"[CLS]\"", "]", "+", "word_piece_tokens", "+", "[", "\"[SEP]\"", "]", "\n", "\n", "return", "wordpieces", ",", "end_offsets", ",", "start_offsets", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.dataset_readers.srl_wikibank.SrlWikibankReader._read": [[202, 229], ["allennlp.common.file_utils.cached_path", "allennlp.data.dataset_readers.dataset_utils.Ontonotes", "allennlp.data.dataset_readers.dataset_utils.Ontonotes.dataset_document_iterator", "allennlp.data.tokenizers.Token", "srl_wikibank.SrlWikibankReader.text_to_instance", "srl_wikibank.SrlWikibankReader.text_to_instance"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.file_utils.cached_path", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.dataset_utils.ontonotes.Ontonotes.dataset_document_iterator", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.coreference_resolution.conll.ConllCorefReader.text_to_instance", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.coreference_resolution.conll.ConllCorefReader.text_to_instance"], ["", "@", "overrides", "\n", "def", "_read", "(", "self", ",", "file_path", ":", "str", ")", ":", "\n", "        ", "file_path", "=", "cached_path", "(", "file_path", ")", "\n", "ontonotes_helper", "=", "Ontonotes", "(", ")", "\n", "for", "doc", "in", "ontonotes_helper", ".", "dataset_document_iterator", "(", "file_path", ")", ":", "\n", "            ", "if", "self", ".", "_to_yield", "==", "0", ":", "\n", "                ", "break", "\n", "", "for", "sentence", "in", "doc", ":", "\n", "                ", "tokens", "=", "[", "Token", "(", "t", ")", "for", "t", "in", "sentence", ".", "words", "]", "\n", "if", "not", "sentence", ".", "srl_frames", ":", "\n", "# Sentence contains no predicates.", "\n", "                    ", "tags", "=", "[", "\"O\"", "for", "_", "in", "tokens", "]", "\n", "verb_label", "=", "[", "0", "for", "_", "in", "tokens", "]", "\n", "yield", "self", ".", "text_to_instance", "(", "tokens", ",", "verb_label", ",", "tags", ")", "\n", "", "else", ":", "\n", "                    ", "for", "(", "_", ",", "tags", ")", "in", "sentence", ".", "srl_frames", ":", "\n", "                        ", "if", "not", "self", ".", "bert_tokenizer", ":", "\n", "# Ontonotes does not have 2 token verbs and if there is no", "\n", "# bert tokenizer to split a one-word verb into word pieces,", "\n", "# the model would have never seen I-V during training.", "\n", "# So we just take the first token as the verb.", "\n", "                            ", "tags", "=", "[", "tag", "if", "tag", "!=", "\"I-V\"", "else", "\"O\"", "for", "tag", "in", "tags", "]", "\n", "", "verb_indicator", "=", "[", "\n", "1", "if", "label", "[", "-", "2", ":", "]", "==", "\"-V\"", "else", "0", "for", "label", "in", "tags", "\n", "]", "\n", "yield", "self", ".", "text_to_instance", "(", "tokens", ",", "verb_indicator", ",", "tags", ")", "\n", "", "", "self", ".", "_to_yield", "-=", "1", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.dataset_readers.srl_wikibank.SrlWikibankReader.text_to_instance": [[230, 283], ["all", "allennlp.data.fields.MetadataField", "allennlp.data.instance.Instance", "srl_wikibank.SrlWikibankReader._wordpiece_tokenize_input", "srl_wikibank._convert_verb_indices_to_wordpiece_indices", "allennlp.data.fields.TextField", "allennlp.data.fields.SequenceLabelField", "allennlp.data.fields.TextField", "allennlp.data.fields.SequenceLabelField", "verb_label.index", "srl_wikibank._convert_tags_to_wordpiece_tags", "allennlp.data.fields.SequenceLabelField", "allennlp.data.fields.SequenceLabelField", "allennlp.data.tokenizers.Token"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.dataset_readers.semantic_role_labeling.SrlReader._wordpiece_tokenize_input", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.dataset_readers.semantic_role_labeling._convert_verb_indices_to_wordpiece_indices", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.fields.text_field.TextField.index", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.dataset_readers.semantic_role_labeling._convert_tags_to_wordpiece_tags"], ["", "", "", "def", "text_to_instance", "(", "# type: ignore", "\n", "self", ",", "tokens", ":", "List", "[", "Token", "]", ",", "verb_label", ":", "List", "[", "int", "]", ",", "tags", ":", "List", "[", "str", "]", "=", "None", "\n", ")", "->", "Instance", ":", "\n", "        ", "\"\"\"\n        We take `pre-tokenized` input here, along with a verb label.  The verb label should be a\n        one-hot binary vector, the same length as the tokens, indicating the position of the verb\n        to find arguments for.\n        \"\"\"", "\n", "\n", "metadata_dict", ":", "Dict", "[", "str", ",", "Any", "]", "=", "{", "}", "\n", "if", "self", ".", "bert_tokenizer", "is", "not", "None", ":", "\n", "            ", "wordpieces", ",", "offsets", ",", "start_offsets", "=", "self", ".", "_wordpiece_tokenize_input", "(", "\n", "[", "t", ".", "text", "for", "t", "in", "tokens", "]", "\n", ")", "\n", "new_verbs", "=", "_convert_verb_indices_to_wordpiece_indices", "(", "verb_label", ",", "offsets", ")", "\n", "metadata_dict", "[", "\"offsets\"", "]", "=", "start_offsets", "\n", "# In order to override the indexing mechanism, we need to set the `text_id`", "\n", "# attribute directly. This causes the indexing to use this id.", "\n", "text_field", "=", "TextField", "(", "\n", "[", "Token", "(", "t", ",", "text_id", "=", "self", ".", "bert_tokenizer", ".", "vocab", "[", "t", "]", ")", "for", "t", "in", "wordpieces", "]", ",", "\n", "token_indexers", "=", "self", ".", "_token_indexers", ",", "\n", ")", "\n", "verb_indicator", "=", "SequenceLabelField", "(", "new_verbs", ",", "text_field", ")", "\n", "\n", "", "else", ":", "\n", "            ", "text_field", "=", "TextField", "(", "tokens", ",", "token_indexers", "=", "self", ".", "_token_indexers", ")", "\n", "verb_indicator", "=", "SequenceLabelField", "(", "verb_label", ",", "text_field", ")", "\n", "\n", "", "fields", ":", "Dict", "[", "str", ",", "Field", "]", "=", "{", "}", "\n", "fields", "[", "\"tokens\"", "]", "=", "text_field", "\n", "fields", "[", "\"verb_indicator\"", "]", "=", "verb_indicator", "\n", "\n", "if", "all", "(", "[", "x", "==", "0", "for", "x", "in", "verb_label", "]", ")", ":", "\n", "            ", "verb", "=", "None", "\n", "verb_index", "=", "None", "\n", "", "else", ":", "\n", "            ", "verb_index", "=", "verb_label", ".", "index", "(", "1", ")", "\n", "verb", "=", "tokens", "[", "verb_index", "]", ".", "text", "\n", "\n", "", "metadata_dict", "[", "\"words\"", "]", "=", "[", "x", ".", "text", "for", "x", "in", "tokens", "]", "\n", "metadata_dict", "[", "\"verb\"", "]", "=", "verb", "\n", "metadata_dict", "[", "\"verb_index\"", "]", "=", "verb_index", "\n", "\n", "if", "tags", ":", "\n", "            ", "if", "self", ".", "bert_tokenizer", "is", "not", "None", ":", "\n", "                ", "new_tags", "=", "_convert_tags_to_wordpiece_tags", "(", "tags", ",", "offsets", ")", "\n", "fields", "[", "\"tags\"", "]", "=", "SequenceLabelField", "(", "new_tags", ",", "text_field", ")", "\n", "", "else", ":", "\n", "                ", "fields", "[", "\"tags\"", "]", "=", "SequenceLabelField", "(", "tags", ",", "text_field", ")", "\n", "", "metadata_dict", "[", "\"gold_tags\"", "]", "=", "tags", "\n", "\n", "", "fields", "[", "\"metadata\"", "]", "=", "MetadataField", "(", "metadata_dict", ")", "\n", "return", "Instance", "(", "fields", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.dataset_readers.srl_wikibank._convert_tags_to_wordpiece_tags": [[20, 63], ["enumerate", "new_tags.append", "tag.startswith", "new_tags.append", "tag.startswith", "new_tags.append", "tag.startswith", "tag.split", "new_tags.append"], "function", ["None"], ["def", "_convert_tags_to_wordpiece_tags", "(", "tags", ":", "List", "[", "str", "]", ",", "offsets", ":", "List", "[", "int", "]", ")", "->", "List", "[", "str", "]", ":", "\n", "    ", "\"\"\"\n    Converts a series of BIO tags to account for a wordpiece tokenizer,\n    extending/modifying BIO tags where appropriate to deal with words which\n    are split into multiple wordpieces by the tokenizer.\n\n    This is only used if you pass a `bert_model_name` to the dataset reader below.\n\n    # Parameters\n\n    tags : `List[str]`\n        The BIO formatted tags to convert to BIO tags for wordpieces\n    offsets : `List[int]`\n        The wordpiece offsets.\n\n    # Returns\n\n    The new BIO tags.\n    \"\"\"", "\n", "new_tags", "=", "[", "]", "\n", "j", "=", "0", "\n", "for", "i", ",", "offset", "in", "enumerate", "(", "offsets", ")", ":", "\n", "        ", "tag", "=", "tags", "[", "i", "]", "\n", "is_o", "=", "tag", "==", "\"O\"", "\n", "is_start", "=", "True", "\n", "while", "j", "<", "offset", ":", "\n", "            ", "if", "is_o", ":", "\n", "                ", "new_tags", ".", "append", "(", "\"O\"", ")", "\n", "\n", "", "elif", "tag", ".", "startswith", "(", "\"I\"", ")", ":", "\n", "                ", "new_tags", ".", "append", "(", "tag", ")", "\n", "\n", "", "elif", "is_start", "and", "tag", ".", "startswith", "(", "\"B\"", ")", ":", "\n", "                ", "new_tags", ".", "append", "(", "tag", ")", "\n", "is_start", "=", "False", "\n", "\n", "", "elif", "tag", ".", "startswith", "(", "\"B\"", ")", ":", "\n", "                ", "_", ",", "label", "=", "tag", ".", "split", "(", "\"-\"", ",", "1", ")", "\n", "new_tags", ".", "append", "(", "\"I-\"", "+", "label", ")", "\n", "", "j", "+=", "1", "\n", "\n", "# Add O tags for cls and sep tokens.", "\n", "", "", "return", "[", "\"O\"", "]", "+", "new_tags", "+", "[", "\"O\"", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.dataset_readers.srl_wikibank._convert_verb_indices_to_wordpiece_indices": [[65, 96], ["enumerate", "new_verb_indices.append"], "function", ["None"], ["", "def", "_convert_verb_indices_to_wordpiece_indices", "(", "\n", "verb_indices", ":", "List", "[", "int", "]", ",", "offsets", ":", "List", "[", "int", "]", "\n", ")", ":", "\n", "    ", "\"\"\"\n    Converts binary verb indicators to account for a wordpiece tokenizer,\n    extending/modifying BIO tags where appropriate to deal with words which\n    are split into multiple wordpieces by the tokenizer.\n\n    This is only used if you pass a `bert_model_name` to the dataset reader below.\n\n    # Parameters\n\n    verb_indices : `List[int]`\n        The binary verb indicators, 0 for not a verb, 1 for verb.\n    offsets : `List[int]`\n        The wordpiece offsets.\n\n    # Returns\n\n    The new verb indices.\n    \"\"\"", "\n", "j", "=", "0", "\n", "new_verb_indices", "=", "[", "]", "\n", "for", "i", ",", "offset", "in", "enumerate", "(", "offsets", ")", ":", "\n", "        ", "indicator", "=", "verb_indices", "[", "i", "]", "\n", "while", "j", "<", "offset", ":", "\n", "            ", "new_verb_indices", ".", "append", "(", "indicator", ")", "\n", "j", "+=", "1", "\n", "\n", "# Add 0 indicators for cls and sep tokens.", "\n", "", "", "return", "[", "0", "]", "+", "new_verb_indices", "+", "[", "0", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.dataset_readers.coref_winobias.CorefWinobiasReader.__init__": [[60, 71], ["allennlp.data.dataset_readers.dataset_reader.DatasetReader.__init__", "allennlp.data.token_indexers.SingleIdTokenIndexer"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.graph_util.plot.GraphPlot.__init__"], ["def", "__init__", "(", "\n", "self", ",", "\n", "max_span_width", ":", "int", ",", "\n", "token_indexers", ":", "Dict", "[", "str", ",", "TokenIndexer", "]", "=", "None", ",", "\n", "subset_size", ":", "int", "=", "sys", ".", "maxsize", ",", "\n", "**", "kwargs", ",", "\n", ")", "->", "None", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "**", "kwargs", ")", "\n", "self", ".", "_max_span_width", "=", "max_span_width", "\n", "self", ".", "_token_indexers", "=", "token_indexers", "or", "{", "\"tokens\"", ":", "SingleIdTokenIndexer", "(", ")", "}", "\n", "self", ".", "_to_yield", "=", "subset_size", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.dataset_readers.coref_winobias.CorefWinobiasReader._read": [[72, 117], ["open", "allennlp.common.file_utils.cached_path", "sentence.strip().split", "collections.defaultdict", "enumerate", "token.endswith", "coref_winobias.CorefWinobiasReader.text_to_instance", "sentence.strip", "clusters[].append", "clusters[].append", "words.append", "words.append", "words.append", "clusters[].append", "clusters[].append", "token.strip", "token.strip", "allennlp.data.tokenizers.Token", "clusters.values"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.file_utils.cached_path", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.coreference_resolution.conll.ConllCorefReader.text_to_instance"], ["", "@", "overrides", "\n", "def", "_read", "(", "self", ",", "file_path", ":", "str", ")", ":", "\n", "\n", "        ", "for", "sentence", "in", "open", "(", "cached_path", "(", "file_path", ")", ",", "\"r\"", ")", ":", "\n", "# For this dataset, consider a sentence to be a document", "\n", "            ", "if", "self", ".", "_to_yield", "==", "0", ":", "\n", "                ", "break", "\n", "\n", "", "tokens", "=", "sentence", ".", "strip", "(", ")", ".", "split", "(", "\" \"", ")", "\n", "clusters", ":", "DefaultDict", "[", "int", ",", "List", "[", "Tuple", "[", "int", ",", "int", "]", "]", "]", "=", "collections", ".", "defaultdict", "(", "\n", "list", "\n", ")", "\n", "words", "=", "[", "]", "\n", "for", "index", ",", "token", "in", "enumerate", "(", "tokens", ")", ":", "\n", "# Coreference is annotated using [square brackets]", "\n", "# or (round brackets) around coreferent phrases.", "\n", "                ", "if", "\"[\"", "in", "token", "and", "\"]\"", "in", "token", ":", "\n", "                    ", "clusters", "[", "0", "]", ".", "append", "(", "(", "index", ",", "index", ")", ")", "\n", "", "elif", "\"[\"", "in", "token", ":", "\n", "                    ", "clusters", "[", "0", "]", ".", "append", "(", "(", "index", ",", "index", ")", ")", "\n", "", "elif", "\"]\"", "in", "token", ":", "\n", "                    ", "old_span", "=", "clusters", "[", "0", "]", "[", "-", "1", "]", "\n", "clusters", "[", "0", "]", "[", "-", "1", "]", "=", "(", "old_span", "[", "0", "]", ",", "index", ")", "\n", "\n", "", "if", "\"(\"", "in", "token", "and", "\")\"", "in", "token", ":", "\n", "                    ", "clusters", "[", "1", "]", ".", "append", "(", "(", "index", ",", "index", ")", ")", "\n", "", "elif", "\"(\"", "in", "token", ":", "\n", "                    ", "clusters", "[", "1", "]", ".", "append", "(", "(", "index", ",", "index", ")", ")", "\n", "", "elif", "\")\"", "in", "token", ":", "\n", "                    ", "old_span", "=", "clusters", "[", "1", "]", "[", "-", "1", "]", "\n", "clusters", "[", "1", "]", "[", "-", "1", "]", "=", "(", "old_span", "[", "0", "]", ",", "index", ")", "\n", "\n", "", "if", "token", ".", "endswith", "(", "\".\"", ")", ":", "\n", "# Winobias is tokenised, but not for full stops.", "\n", "# We'll just special case them here.", "\n", "                    ", "token", "=", "token", "[", ":", "-", "1", "]", "\n", "words", ".", "append", "(", "token", ".", "strip", "(", "\"[]()\"", ")", ")", "\n", "words", ".", "append", "(", "\".\"", ")", "\n", "", "else", ":", "\n", "                    ", "words", ".", "append", "(", "token", ".", "strip", "(", "\"[]()\"", ")", ")", "\n", "\n", "", "", "yield", "self", ".", "text_to_instance", "(", "\n", "[", "Token", "(", "x", ")", "for", "x", "in", "words", "]", ",", "[", "x", "for", "x", "in", "clusters", ".", "values", "(", ")", "]", "\n", ")", "\n", "self", ".", "_to_yield", "-=", "1", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.dataset_readers.coref_winobias.CorefWinobiasReader.text_to_instance": [[118, 187], ["allennlp.data.fields.TextField", "allennlp.data.dataset_readers.dataset_utils.enumerate_spans", "allennlp.data.fields.ListField", "allennlp.data.fields.MetadataField", "allennlp.data.instance.Instance", "enumerate", "spans.append", "allennlp.data.fields.SequenceLabelField", "allennlp.data.fields.SpanField", "span_labels.append", "span_labels.append", "tuple"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.dataset_utils.span_utils.enumerate_spans"], ["", "", "@", "overrides", "\n", "def", "text_to_instance", "(", "\n", "self", ",", "# type: ignore", "\n", "sentence", ":", "List", "[", "Token", "]", ",", "\n", "gold_clusters", ":", "Optional", "[", "List", "[", "List", "[", "Tuple", "[", "int", ",", "int", "]", "]", "]", "]", "=", "None", ",", "\n", ")", "->", "Instance", ":", "\n", "\n", "        ", "\"\"\"\n        # Parameters\n\n        sentence : `List[Token]`, required.\n            The already tokenised sentence to analyse.\n        gold_clusters : `Optional[List[List[Tuple[int, int]]]]`, optional (default = None)\n            A list of all clusters in the sentence, represented as word spans. Each cluster\n            contains some number of spans, which can be nested and overlap, but will never\n            exactly match between clusters.\n\n        # Returns\n\n        An `Instance` containing the following `Fields`:\n            text : `TextField`\n                The text of the full sentence.\n            spans : `ListField[SpanField]`\n                A ListField containing the spans represented as `SpanFields`\n                with respect to the sentence text.\n            span_labels : `SequenceLabelField`, optional\n                The id of the cluster which each possible span belongs to, or -1 if it does\n                 not belong to a cluster. As these labels have variable length (it depends on\n                 how many spans we are considering), we represent this a as a `SequenceLabelField`\n                 with respect to the `spans `ListField`.\n        \"\"\"", "\n", "metadata", ":", "Dict", "[", "str", ",", "Any", "]", "=", "{", "\"original_text\"", ":", "sentence", "}", "\n", "if", "gold_clusters", "is", "not", "None", ":", "\n", "            ", "metadata", "[", "\"clusters\"", "]", "=", "gold_clusters", "\n", "\n", "", "text_field", "=", "TextField", "(", "sentence", ",", "self", ".", "_token_indexers", ")", "\n", "\n", "cluster_dict", "=", "{", "}", "\n", "if", "gold_clusters", "is", "not", "None", ":", "\n", "            ", "for", "cluster_id", ",", "cluster", "in", "enumerate", "(", "gold_clusters", ")", ":", "\n", "                ", "for", "mention", "in", "cluster", ":", "\n", "                    ", "cluster_dict", "[", "tuple", "(", "mention", ")", "]", "=", "cluster_id", "\n", "\n", "", "", "", "spans", ":", "List", "[", "Field", "]", "=", "[", "]", "\n", "span_labels", ":", "Optional", "[", "List", "[", "int", "]", "]", "=", "[", "]", "if", "gold_clusters", "is", "not", "None", "else", "None", "\n", "\n", "for", "start", ",", "end", "in", "enumerate_spans", "(", "\n", "sentence", ",", "max_span_width", "=", "self", ".", "_max_span_width", "\n", ")", ":", "\n", "            ", "if", "span_labels", "is", "not", "None", ":", "\n", "                ", "if", "(", "start", ",", "end", ")", "in", "cluster_dict", ":", "\n", "                    ", "span_labels", ".", "append", "(", "cluster_dict", "[", "(", "start", ",", "end", ")", "]", ")", "\n", "", "else", ":", "\n", "                    ", "span_labels", ".", "append", "(", "-", "1", ")", "\n", "\n", "", "", "spans", ".", "append", "(", "SpanField", "(", "start", ",", "end", ",", "text_field", ")", ")", "\n", "\n", "", "span_field", "=", "ListField", "(", "spans", ")", "\n", "metadata_field", "=", "MetadataField", "(", "metadata", ")", "\n", "\n", "fields", ":", "Dict", "[", "str", ",", "Field", "]", "=", "{", "\n", "\"text\"", ":", "text_field", ",", "\n", "\"spans\"", ":", "span_field", ",", "\n", "\"metadata\"", ":", "metadata_field", ",", "\n", "}", "\n", "if", "span_labels", "is", "not", "None", ":", "\n", "            ", "fields", "[", "\"span_labels\"", "]", "=", "SequenceLabelField", "(", "span_labels", ",", "span_field", ")", "\n", "\n", "", "return", "Instance", "(", "fields", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.dataset_readers.coref_preco.CorefPrecoReader.__init__": [[50, 63], ["allennlp.data.dataset_readers.dataset_reader.DatasetReader.__init__", "allennlp.data.token_indexers.SingleIdTokenIndexer"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.graph_util.plot.GraphPlot.__init__"], ["def", "__init__", "(", "\n", "self", ",", "\n", "max_span_width", ":", "int", ",", "\n", "token_indexers", ":", "Dict", "[", "str", ",", "TokenIndexer", "]", "=", "None", ",", "\n", "wordpiece_modeling_tokenizer", ":", "Optional", "[", "PretrainedTransformerTokenizer", "]", "=", "None", ",", "\n", "subset_size", ":", "int", "=", "sys", ".", "maxsize", ",", "\n", "lazy", ":", "bool", "=", "False", ",", "\n", ")", "->", "None", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "lazy", ")", "\n", "self", ".", "_max_span_width", "=", "max_span_width", "\n", "self", ".", "_token_indexers", "=", "token_indexers", "or", "{", "\"tokens\"", ":", "SingleIdTokenIndexer", "(", ")", "}", "\n", "self", ".", "_wordpiece_modeling_tokenizer", "=", "wordpiece_modeling_tokenizer", "\n", "self", ".", "_to_yield", "=", "subset_size", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.dataset_readers.coref_preco.CorefPrecoReader.make_clusters": [[64, 81], ["collections.defaultdict", "enumerate", "collections.defaultdict", "range", "offsets.append", "len", "len", "temp_clusters[].append", "final_clusters[].append"], "methods", ["None"], ["", "def", "make_clusters", "(", "self", ",", "sentences", ",", "raw_clusters", ")", ":", "\n", "        ", "temp_clusters", "=", "defaultdict", "(", "list", ")", "\n", "for", "cluster_id", ",", "cluster", "in", "enumerate", "(", "raw_clusters", ")", ":", "\n", "            ", "for", "sent_id", ",", "start", ",", "end", "in", "cluster", ":", "\n", "                ", "temp_clusters", "[", "sent_id", "]", ".", "append", "(", "[", "start", ",", "end", ",", "cluster_id", "]", ")", "\n", "", "", "final_clusters", "=", "defaultdict", "(", "list", ")", "\n", "offset", ",", "offsets", "=", "0", ",", "[", "]", "\n", "for", "sentence", "in", "sentences", ":", "\n", "            ", "offsets", ".", "append", "(", "offset", ")", "\n", "offset", "+=", "len", "(", "sentence", ")", "\n", "", "for", "sent_id", "in", "range", "(", "len", "(", "sentences", ")", ")", ":", "\n", "            ", "for", "start", ",", "end", ",", "cluster_id", "in", "temp_clusters", "[", "sent_id", "]", ":", "\n", "# preco dataset has python slicing index type - so we subtract the end by 1", "\n", "                ", "final_clusters", "[", "cluster_id", "]", ".", "append", "(", "\n", "(", "start", "+", "offsets", "[", "sent_id", "]", ",", "end", "-", "1", "+", "offsets", "[", "sent_id", "]", ")", "\n", ")", "\n", "", "", "return", "final_clusters", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.dataset_readers.coref_preco.CorefPrecoReader._read": [[82, 99], ["allennlp.common.file_utils.cached_path", "open", "f.readlines", "coref_preco.CorefPrecoReader.make_clusters", "data.append", "coref_preco.CorefPrecoReader.text_to_instance", "json.loads", "list", "coref_preco.CorefPrecoReader.values"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.file_utils.cached_path", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.dataset_readers.coref_preco.CorefPrecoReader.make_clusters", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.coreference_resolution.conll.ConllCorefReader.text_to_instance"], ["", "@", "overrides", "\n", "def", "_read", "(", "self", ",", "file_path", ":", "str", ")", ":", "\n", "# if `file_path` is a URL, redirect to the cache", "\n", "        ", "file_path", "=", "cached_path", "(", "file_path", ")", "\n", "data", "=", "[", "]", "\n", "with", "open", "(", "file_path", ",", "\"r\"", ")", "as", "f", ":", "\n", "            ", "for", "line", "in", "f", ".", "readlines", "(", ")", ":", "\n", "                ", "data", ".", "append", "(", "json", ".", "loads", "(", "line", ")", ")", "\n", "\n", "", "", "for", "instance", "in", "data", ":", "\n", "            ", "if", "self", ".", "_to_yield", "==", "0", ":", "\n", "                ", "break", "\n", "", "sentences", "=", "instance", "[", "\"sentences\"", "]", "\n", "raw_clusters", "=", "instance", "[", "\"mention_clusters\"", "]", "\n", "final_clusters", "=", "self", ".", "make_clusters", "(", "sentences", ",", "raw_clusters", ")", "\n", "yield", "self", ".", "text_to_instance", "(", "sentences", ",", "list", "(", "final_clusters", ".", "values", "(", ")", ")", ")", "\n", "self", ".", "_to_yield", "-=", "1", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.dataset_readers.coref_preco.CorefPrecoReader.text_to_instance": [[100, 215], ["allennlp.data.fields.TextField", "allennlp.data.fields.ListField", "allennlp.data.fields.MetadataField", "allennlp.data.instance.Instance", "coref_preco.CorefPrecoReader._normalize_word", "coref_preco.CorefPrecoReader._wordpiece_modeling_tokenizer.intra_word_tokenize", "enumerate", "allennlp.data.dataset_readers.dataset_utils.enumerate_spans", "len", "allennlp.data.fields.SequenceLabelField", "allennlp.data.tokenizers.Token", "spans.append", "enumerate", "allennlp.data.fields.SpanField", "span_labels.append", "span_labels.append", "tuple", "len"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.coreference_resolution.conll.ConllCorefReader._normalize_word", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.tokenizers.pretrained_transformer_tokenizer.PretrainedTransformerTokenizer.intra_word_tokenize", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.dataset_utils.span_utils.enumerate_spans"], ["", "", "@", "overrides", "\n", "def", "text_to_instance", "(", "\n", "self", ",", "# type: ignore", "\n", "sentences", ":", "List", "[", "List", "[", "str", "]", "]", ",", "\n", "gold_clusters", ":", "Optional", "[", "List", "[", "List", "[", "Tuple", "[", "int", ",", "int", "]", "]", "]", "]", "=", "None", ",", "\n", ")", "->", "Instance", ":", "\n", "\n", "        ", "\"\"\"\n        # Parameters\n        sentences : `List[List[str]]`, required.\n            A list of lists representing the tokenised words and sentences in the document.\n        gold_clusters : `Optional[List[List[Tuple[int, int]]]]`, optional (default = None)\n            A list of all clusters in the document, represented as word spans. Each cluster\n            contains some number of spans, which can be nested and overlap, but will never\n            exactly match between clusters.\n        # Returns\n        An `Instance` containing the following `Fields`:\n            text : `TextField`\n                The text of the full document.\n            spans : `ListField[SpanField]`\n                A ListField containing the spans represented as `SpanFields`\n                with respect to the document text.\n            span_labels : `SequenceLabelField`, optional\n                The id of the cluster which each possible span belongs to, or -1 if it does\n                 not belong to a cluster. As these labels have variable length (it depends on\n                 how many spans we are considering), we represent this a as a `SequenceLabelField`\n                 with respect to the `spans `ListField`.\n        \"\"\"", "\n", "flattened_sentences", "=", "[", "\n", "self", ".", "_normalize_word", "(", "word", ")", "for", "sentence", "in", "sentences", "for", "word", "in", "sentence", "\n", "]", "\n", "\n", "if", "self", ".", "_wordpiece_modeling_tokenizer", "is", "not", "None", ":", "\n", "            ", "(", "\n", "flat_sentences_tokens", ",", "\n", "offsets", ",", "\n", ")", "=", "self", ".", "_wordpiece_modeling_tokenizer", ".", "intra_word_tokenize", "(", "\n", "flattened_sentences", "\n", ")", "\n", "flattened_sentences", "=", "[", "t", ".", "text", "for", "t", "in", "flat_sentences_tokens", "]", "\n", "", "else", ":", "\n", "            ", "flat_sentences_tokens", "=", "[", "Token", "(", "word", ")", "for", "word", "in", "flattened_sentences", "]", "\n", "\n", "", "text_field", "=", "TextField", "(", "flat_sentences_tokens", ",", "self", ".", "_token_indexers", ")", "\n", "\n", "cluster_dict", "=", "{", "}", "\n", "if", "gold_clusters", "is", "not", "None", ":", "\n", "            ", "if", "self", ".", "_wordpiece_modeling_tokenizer", "is", "not", "None", ":", "\n", "                ", "for", "cluster", "in", "gold_clusters", ":", "\n", "                    ", "for", "mention_id", ",", "mention", "in", "enumerate", "(", "cluster", ")", ":", "\n", "                        ", "start", "=", "offsets", "[", "mention", "[", "0", "]", "]", "[", "0", "]", "\n", "end", "=", "offsets", "[", "mention", "[", "1", "]", "]", "[", "1", "]", "\n", "cluster", "[", "mention_id", "]", "=", "(", "start", ",", "end", ")", "\n", "\n", "", "", "", "for", "cluster_id", ",", "cluster", "in", "enumerate", "(", "gold_clusters", ")", ":", "\n", "                ", "for", "mention", "in", "cluster", ":", "\n", "                    ", "cluster_dict", "[", "tuple", "(", "mention", ")", "]", "=", "cluster_id", "\n", "\n", "", "", "", "spans", ":", "List", "[", "Field", "]", "=", "[", "]", "\n", "span_labels", ":", "Optional", "[", "List", "[", "int", "]", "]", "=", "[", "]", "if", "gold_clusters", "is", "not", "None", "else", "None", "\n", "\n", "sentence_offset", "=", "0", "\n", "for", "sentence", "in", "sentences", ":", "\n", "            ", "for", "start", ",", "end", "in", "enumerate_spans", "(", "\n", "sentence", ",", "offset", "=", "sentence_offset", ",", "max_span_width", "=", "self", ".", "_max_span_width", "\n", ")", ":", "\n", "                ", "if", "self", ".", "_wordpiece_modeling_tokenizer", "is", "not", "None", ":", "\n", "                    ", "start", "=", "offsets", "[", "start", "]", "[", "0", "]", "\n", "end", "=", "offsets", "[", "end", "]", "[", "1", "]", "\n", "\n", "# `enumerate_spans` uses word-level width limit; here we apply it to wordpieces", "\n", "# We have to do this check here because we use a span width embedding that has", "\n", "# only `self._max_span_width` entries, and since we are doing wordpiece", "\n", "# modeling, the span width embedding operates on wordpiece lengths. So a check", "\n", "# here is necessary or else we wouldn't know how many entries there would be.", "\n", "if", "end", "-", "start", "+", "1", ">", "self", ".", "_max_span_width", ":", "\n", "                        ", "continue", "\n", "# We also don't generate spans that contain special tokens", "\n", "", "if", "(", "\n", "start", "\n", "<", "self", ".", "_wordpiece_modeling_tokenizer", ".", "num_added_start_tokens", "\n", ")", ":", "\n", "                        ", "continue", "\n", "", "if", "(", "\n", "end", "\n", ">=", "len", "(", "flat_sentences_tokens", ")", "\n", "-", "self", ".", "_wordpiece_modeling_tokenizer", ".", "num_added_end_tokens", "\n", ")", ":", "\n", "                        ", "continue", "\n", "\n", "", "", "if", "span_labels", "is", "not", "None", ":", "\n", "                    ", "if", "(", "start", ",", "end", ")", "in", "cluster_dict", ":", "\n", "                        ", "span_labels", ".", "append", "(", "cluster_dict", "[", "(", "start", ",", "end", ")", "]", ")", "\n", "", "else", ":", "\n", "                        ", "span_labels", ".", "append", "(", "-", "1", ")", "\n", "\n", "", "", "spans", ".", "append", "(", "SpanField", "(", "start", ",", "end", ",", "text_field", ")", ")", "\n", "", "sentence_offset", "+=", "len", "(", "sentence", ")", "\n", "\n", "", "span_field", "=", "ListField", "(", "spans", ")", "\n", "\n", "metadata", ":", "Dict", "[", "str", ",", "Any", "]", "=", "{", "\"original_text\"", ":", "flattened_sentences", "}", "\n", "if", "gold_clusters", "is", "not", "None", ":", "\n", "            ", "metadata", "[", "\"clusters\"", "]", "=", "gold_clusters", "\n", "", "metadata_field", "=", "MetadataField", "(", "metadata", ")", "\n", "\n", "fields", ":", "Dict", "[", "str", ",", "Field", "]", "=", "{", "\n", "\"text\"", ":", "text_field", ",", "\n", "\"spans\"", ":", "span_field", ",", "\n", "\"metadata\"", ":", "metadata_field", ",", "\n", "}", "\n", "if", "span_labels", "is", "not", "None", ":", "\n", "            ", "fields", "[", "\"span_labels\"", "]", "=", "SequenceLabelField", "(", "span_labels", ",", "span_field", ")", "\n", "\n", "", "return", "Instance", "(", "fields", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.dataset_readers.coref_preco.CorefPrecoReader._normalize_word": [[216, 222], ["None"], "methods", ["None"], ["", "@", "staticmethod", "\n", "def", "_normalize_word", "(", "word", ")", ":", "\n", "        ", "if", "word", "==", "\"/.\"", "or", "word", "==", "\"/?\"", ":", "\n", "            ", "return", "word", "[", "1", ":", "]", "\n", "", "else", ":", "\n", "            ", "return", "word", "\n", "", "", "", ""]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.dataset_readers.coref_wikicoref.CorefWikicorefReader.__init__": [[50, 63], ["allennlp.data.dataset_readers.dataset_reader.DatasetReader.__init__", "allennlp.data.token_indexers.SingleIdTokenIndexer"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.graph_util.plot.GraphPlot.__init__"], ["def", "__init__", "(", "\n", "self", ",", "\n", "max_span_width", ":", "int", ",", "\n", "token_indexers", ":", "Dict", "[", "str", ",", "TokenIndexer", "]", "=", "None", ",", "\n", "wordpiece_modeling_tokenizer", ":", "Optional", "[", "PretrainedTransformerTokenizer", "]", "=", "None", ",", "\n", "subset_size", ":", "int", "=", "sys", ".", "maxsize", ",", "\n", "lazy", ":", "bool", "=", "False", ",", "\n", ")", "->", "None", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "lazy", ")", "\n", "self", ".", "_max_span_width", "=", "max_span_width", "\n", "self", ".", "_token_indexers", "=", "token_indexers", "or", "{", "\"tokens\"", ":", "SingleIdTokenIndexer", "(", ")", "}", "\n", "self", ".", "_wordpiece_modeling_tokenizer", "=", "wordpiece_modeling_tokenizer", "\n", "self", ".", "_to_yield", "=", "subset_size", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.dataset_readers.coref_wikicoref.CorefWikicorefReader._read": [[64, 82], ["allennlp.common.file_utils.cached_path", "open", "f.readlines", "data.append", "coref_wikicoref.CorefWikicorefReader.text_to_instance", "json.loads", "tuple"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.file_utils.cached_path", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.coreference_resolution.conll.ConllCorefReader.text_to_instance"], ["", "@", "overrides", "\n", "def", "_read", "(", "self", ",", "file_path", ":", "str", ")", ":", "\n", "# if `file_path` is a URL, redirect to the cache", "\n", "        ", "file_path", "=", "cached_path", "(", "file_path", ")", "\n", "data", "=", "[", "]", "\n", "with", "open", "(", "file_path", ",", "\"r\"", ")", "as", "f", ":", "\n", "            ", "for", "line", "in", "f", ".", "readlines", "(", ")", ":", "\n", "                ", "data", ".", "append", "(", "json", ".", "loads", "(", "line", ")", ")", "\n", "\n", "", "", "for", "instance", "in", "data", ":", "\n", "            ", "if", "self", ".", "_to_yield", "==", "0", ":", "\n", "                ", "break", "\n", "", "sentences", "=", "instance", "[", "\"sentences\"", "]", "\n", "clusters", "=", "[", "\n", "[", "tuple", "(", "span", ")", "for", "span", "in", "cluster", "]", "for", "cluster", "in", "instance", "[", "\"clusters\"", "]", "\n", "]", "\n", "yield", "self", ".", "text_to_instance", "(", "sentences", ",", "clusters", ")", "\n", "self", ".", "_to_yield", "-=", "1", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.dataset_readers.coref_wikicoref.CorefWikicorefReader.text_to_instance": [[83, 198], ["allennlp.data.fields.TextField", "allennlp.data.fields.ListField", "allennlp.data.fields.MetadataField", "allennlp.data.instance.Instance", "coref_wikicoref.CorefWikicorefReader._normalize_word", "coref_wikicoref.CorefWikicorefReader._wordpiece_modeling_tokenizer.intra_word_tokenize", "enumerate", "allennlp.data.dataset_readers.dataset_utils.enumerate_spans", "len", "allennlp.data.fields.SequenceLabelField", "allennlp.data.tokenizers.Token", "spans.append", "enumerate", "allennlp.data.fields.SpanField", "span_labels.append", "span_labels.append", "tuple", "len"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.coreference_resolution.conll.ConllCorefReader._normalize_word", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.tokenizers.pretrained_transformer_tokenizer.PretrainedTransformerTokenizer.intra_word_tokenize", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.dataset_utils.span_utils.enumerate_spans"], ["", "", "@", "overrides", "\n", "def", "text_to_instance", "(", "\n", "self", ",", "# type: ignore", "\n", "sentences", ":", "List", "[", "List", "[", "str", "]", "]", ",", "\n", "gold_clusters", ":", "Optional", "[", "List", "[", "List", "[", "Tuple", "[", "int", ",", "int", "]", "]", "]", "]", "=", "None", ",", "\n", ")", "->", "Instance", ":", "\n", "\n", "        ", "\"\"\"\n        # Parameters\n        sentences : `List[List[str]]`, required.\n            A list of lists representing the tokenised words and sentences in the document.\n        gold_clusters : `Optional[List[List[Tuple[int, int]]]]`, optional (default = None)\n            A list of all clusters in the document, represented as word spans. Each cluster\n            contains some number of spans, which can be nested and overlap, but will never\n            exactly match between clusters.\n        # Returns\n        An `Instance` containing the following `Fields`:\n            text : `TextField`\n                The text of the full document.\n            spans : `ListField[SpanField]`\n                A ListField containing the spans represented as `SpanFields`\n                with respect to the document text.\n            span_labels : `SequenceLabelField`, optional\n                The id of the cluster which each possible span belongs to, or -1 if it does\n                 not belong to a cluster. As these labels have variable length (it depends on\n                 how many spans we are considering), we represent this a as a `SequenceLabelField`\n                 with respect to the `spans `ListField`.\n        \"\"\"", "\n", "flattened_sentences", "=", "[", "\n", "self", ".", "_normalize_word", "(", "word", ")", "for", "sentence", "in", "sentences", "for", "word", "in", "sentence", "\n", "]", "\n", "\n", "if", "self", ".", "_wordpiece_modeling_tokenizer", "is", "not", "None", ":", "\n", "            ", "(", "\n", "flat_sentences_tokens", ",", "\n", "offsets", ",", "\n", ")", "=", "self", ".", "_wordpiece_modeling_tokenizer", ".", "intra_word_tokenize", "(", "\n", "flattened_sentences", "\n", ")", "\n", "flattened_sentences", "=", "[", "t", ".", "text", "for", "t", "in", "flat_sentences_tokens", "]", "\n", "", "else", ":", "\n", "            ", "flat_sentences_tokens", "=", "[", "Token", "(", "word", ")", "for", "word", "in", "flattened_sentences", "]", "\n", "\n", "", "text_field", "=", "TextField", "(", "flat_sentences_tokens", ",", "self", ".", "_token_indexers", ")", "\n", "\n", "cluster_dict", "=", "{", "}", "\n", "if", "gold_clusters", "is", "not", "None", ":", "\n", "            ", "if", "self", ".", "_wordpiece_modeling_tokenizer", "is", "not", "None", ":", "\n", "                ", "for", "cluster", "in", "gold_clusters", ":", "\n", "                    ", "for", "mention_id", ",", "mention", "in", "enumerate", "(", "cluster", ")", ":", "\n", "                        ", "start", "=", "offsets", "[", "mention", "[", "0", "]", "]", "[", "0", "]", "\n", "end", "=", "offsets", "[", "mention", "[", "1", "]", "]", "[", "1", "]", "\n", "cluster", "[", "mention_id", "]", "=", "(", "start", ",", "end", ")", "\n", "\n", "", "", "", "for", "cluster_id", ",", "cluster", "in", "enumerate", "(", "gold_clusters", ")", ":", "\n", "                ", "for", "mention", "in", "cluster", ":", "\n", "                    ", "cluster_dict", "[", "tuple", "(", "mention", ")", "]", "=", "cluster_id", "\n", "\n", "", "", "", "spans", ":", "List", "[", "Field", "]", "=", "[", "]", "\n", "span_labels", ":", "Optional", "[", "List", "[", "int", "]", "]", "=", "[", "]", "if", "gold_clusters", "is", "not", "None", "else", "None", "\n", "\n", "sentence_offset", "=", "0", "\n", "for", "sentence", "in", "sentences", ":", "\n", "            ", "for", "start", ",", "end", "in", "enumerate_spans", "(", "\n", "sentence", ",", "offset", "=", "sentence_offset", ",", "max_span_width", "=", "self", ".", "_max_span_width", "\n", ")", ":", "\n", "                ", "if", "self", ".", "_wordpiece_modeling_tokenizer", "is", "not", "None", ":", "\n", "                    ", "start", "=", "offsets", "[", "start", "]", "[", "0", "]", "\n", "end", "=", "offsets", "[", "end", "]", "[", "1", "]", "\n", "\n", "# `enumerate_spans` uses word-level width limit; here we apply it to wordpieces", "\n", "# We have to do this check here because we use a span width embedding that has", "\n", "# only `self._max_span_width` entries, and since we are doing wordpiece", "\n", "# modeling, the span width embedding operates on wordpiece lengths. So a check", "\n", "# here is necessary or else we wouldn't know how many entries there would be.", "\n", "if", "end", "-", "start", "+", "1", ">", "self", ".", "_max_span_width", ":", "\n", "                        ", "continue", "\n", "# We also don't generate spans that contain special tokens", "\n", "", "if", "(", "\n", "start", "\n", "<", "self", ".", "_wordpiece_modeling_tokenizer", ".", "num_added_start_tokens", "\n", ")", ":", "\n", "                        ", "continue", "\n", "", "if", "(", "\n", "end", "\n", ">=", "len", "(", "flat_sentences_tokens", ")", "\n", "-", "self", ".", "_wordpiece_modeling_tokenizer", ".", "num_added_end_tokens", "\n", ")", ":", "\n", "                        ", "continue", "\n", "\n", "", "", "if", "span_labels", "is", "not", "None", ":", "\n", "                    ", "if", "(", "start", ",", "end", ")", "in", "cluster_dict", ":", "\n", "                        ", "span_labels", ".", "append", "(", "cluster_dict", "[", "(", "start", ",", "end", ")", "]", ")", "\n", "", "else", ":", "\n", "                        ", "span_labels", ".", "append", "(", "-", "1", ")", "\n", "\n", "", "", "spans", ".", "append", "(", "SpanField", "(", "start", ",", "end", ",", "text_field", ")", ")", "\n", "", "sentence_offset", "+=", "len", "(", "sentence", ")", "\n", "\n", "", "span_field", "=", "ListField", "(", "spans", ")", "\n", "\n", "metadata", ":", "Dict", "[", "str", ",", "Any", "]", "=", "{", "\"original_text\"", ":", "flattened_sentences", "}", "\n", "if", "gold_clusters", "is", "not", "None", ":", "\n", "            ", "metadata", "[", "\"clusters\"", "]", "=", "gold_clusters", "\n", "", "metadata_field", "=", "MetadataField", "(", "metadata", ")", "\n", "\n", "fields", ":", "Dict", "[", "str", ",", "Field", "]", "=", "{", "\n", "\"text\"", ":", "text_field", ",", "\n", "\"spans\"", ":", "span_field", ",", "\n", "\"metadata\"", ":", "metadata_field", ",", "\n", "}", "\n", "if", "span_labels", "is", "not", "None", ":", "\n", "            ", "fields", "[", "\"span_labels\"", "]", "=", "SequenceLabelField", "(", "span_labels", ",", "span_field", ")", "\n", "\n", "", "return", "Instance", "(", "fields", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.dataset_readers.coref_wikicoref.CorefWikicorefReader._normalize_word": [[199, 205], ["None"], "methods", ["None"], ["", "@", "staticmethod", "\n", "def", "_normalize_word", "(", "word", ")", ":", "\n", "        ", "if", "word", "==", "\"/.\"", "or", "word", "==", "\"/?\"", ":", "\n", "            ", "return", "word", "[", "1", ":", "]", "\n", "", "else", ":", "\n", "            ", "return", "word", "\n", "", "", "", ""]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.dataset_readers.coref_conll.CorefConllReader.__init__": [[88, 101], ["allennlp.data.dataset_readers.dataset_reader.DatasetReader.__init__", "allennlp.data.token_indexers.SingleIdTokenIndexer"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.graph_util.plot.GraphPlot.__init__"], ["def", "__init__", "(", "\n", "self", ",", "\n", "max_span_width", ":", "int", ",", "\n", "token_indexers", ":", "Dict", "[", "str", ",", "TokenIndexer", "]", "=", "None", ",", "\n", "wordpiece_modeling_tokenizer", ":", "Optional", "[", "PretrainedTransformerTokenizer", "]", "=", "None", ",", "\n", "subset_size", ":", "int", "=", "sys", ".", "maxsize", ",", "\n", "**", "kwargs", ",", "\n", ")", "->", "None", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "**", "kwargs", ")", "\n", "self", ".", "_max_span_width", "=", "max_span_width", "\n", "self", ".", "_token_indexers", "=", "token_indexers", "or", "{", "\"tokens\"", ":", "SingleIdTokenIndexer", "(", ")", "}", "\n", "self", ".", "_wordpiece_modeling_tokenizer", "=", "wordpiece_modeling_tokenizer", "\n", "self", ".", "_to_yield", "=", "subset_size", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.dataset_readers.coref_conll.CorefConllReader._read": [[102, 129], ["allennlp.common.file_utils.cached_path", "allennlp.data.dataset_readers.dataset_utils.Ontonotes", "allennlp.data.dataset_readers.dataset_utils.Ontonotes.dataset_document_iterator", "collections.defaultdict", "coref_conll.canonicalize_clusters", "len", "coref_conll.CorefConllReader.text_to_instance", "clusters[].append"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.file_utils.cached_path", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.dataset_utils.ontonotes.Ontonotes.dataset_document_iterator", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.coreference_resolution.conll.canonicalize_clusters", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.coreference_resolution.conll.ConllCorefReader.text_to_instance"], ["", "@", "overrides", "\n", "def", "_read", "(", "self", ",", "file_path", ":", "str", ")", ":", "\n", "# if `file_path` is a URL, redirect to the cache", "\n", "        ", "file_path", "=", "cached_path", "(", "file_path", ")", "\n", "\n", "ontonotes_reader", "=", "Ontonotes", "(", ")", "\n", "for", "sentences", "in", "ontonotes_reader", ".", "dataset_document_iterator", "(", "file_path", ")", ":", "\n", "            ", "clusters", ":", "DefaultDict", "[", "int", ",", "List", "[", "Tuple", "[", "int", ",", "int", "]", "]", "]", "=", "collections", ".", "defaultdict", "(", "\n", "list", "\n", ")", "\n", "if", "self", ".", "_to_yield", "==", "0", ":", "\n", "                ", "break", "\n", "", "total_tokens", "=", "0", "\n", "for", "sentence", "in", "sentences", ":", "\n", "                ", "for", "typed_span", "in", "sentence", ".", "coref_spans", ":", "\n", "# Coref annotations are on a _per sentence_", "\n", "# basis, so we need to adjust them to be relative", "\n", "# to the length of the document.", "\n", "                    ", "span_id", ",", "(", "start", ",", "end", ")", "=", "typed_span", "\n", "clusters", "[", "span_id", "]", ".", "append", "(", "(", "start", "+", "total_tokens", ",", "end", "+", "total_tokens", ")", ")", "\n", "", "total_tokens", "+=", "len", "(", "sentence", ".", "words", ")", "\n", "\n", "", "canonical_clusters", "=", "canonicalize_clusters", "(", "clusters", ")", "\n", "yield", "self", ".", "text_to_instance", "(", "\n", "[", "s", ".", "words", "for", "s", "in", "sentences", "]", ",", "canonical_clusters", "\n", ")", "\n", "self", ".", "_to_yield", "-=", "1", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.dataset_readers.coref_conll.CorefConllReader.text_to_instance": [[130, 245], ["allennlp.data.fields.TextField", "allennlp.data.fields.ListField", "allennlp.data.fields.MetadataField", "allennlp.data.instance.Instance", "coref_conll.CorefConllReader._normalize_word", "coref_conll.CorefConllReader._wordpiece_modeling_tokenizer.intra_word_tokenize", "enumerate", "allennlp.data.dataset_readers.dataset_utils.enumerate_spans", "len", "allennlp.data.fields.SequenceLabelField", "allennlp.data.tokenizers.Token", "spans.append", "enumerate", "allennlp.data.fields.SpanField", "span_labels.append", "span_labels.append", "tuple", "len"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.coreference_resolution.conll.ConllCorefReader._normalize_word", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.tokenizers.pretrained_transformer_tokenizer.PretrainedTransformerTokenizer.intra_word_tokenize", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.dataset_utils.span_utils.enumerate_spans"], ["", "", "@", "overrides", "\n", "def", "text_to_instance", "(", "\n", "self", ",", "# type: ignore", "\n", "sentences", ":", "List", "[", "List", "[", "str", "]", "]", ",", "\n", "gold_clusters", ":", "Optional", "[", "List", "[", "List", "[", "Tuple", "[", "int", ",", "int", "]", "]", "]", "]", "=", "None", ",", "\n", ")", "->", "Instance", ":", "\n", "\n", "        ", "\"\"\"\n        # Parameters\n        sentences : `List[List[str]]`, required.\n            A list of lists representing the tokenised words and sentences in the document.\n        gold_clusters : `Optional[List[List[Tuple[int, int]]]]`, optional (default = None)\n            A list of all clusters in the document, represented as word spans. Each cluster\n            contains some number of spans, which can be nested and overlap, but will never\n            exactly match between clusters.\n        # Returns\n        An `Instance` containing the following `Fields`:\n            text : `TextField`\n                The text of the full document.\n            spans : `ListField[SpanField]`\n                A ListField containing the spans represented as `SpanFields`\n                with respect to the document text.\n            span_labels : `SequenceLabelField`, optional\n                The id of the cluster which each possible span belongs to, or -1 if it does\n                 not belong to a cluster. As these labels have variable length (it depends on\n                 how many spans we are considering), we represent this a as a `SequenceLabelField`\n                 with respect to the `spans `ListField`.\n        \"\"\"", "\n", "flattened_sentences", "=", "[", "\n", "self", ".", "_normalize_word", "(", "word", ")", "for", "sentence", "in", "sentences", "for", "word", "in", "sentence", "\n", "]", "\n", "\n", "if", "self", ".", "_wordpiece_modeling_tokenizer", "is", "not", "None", ":", "\n", "            ", "(", "\n", "flat_sentences_tokens", ",", "\n", "offsets", ",", "\n", ")", "=", "self", ".", "_wordpiece_modeling_tokenizer", ".", "intra_word_tokenize", "(", "\n", "flattened_sentences", "\n", ")", "\n", "flattened_sentences", "=", "[", "t", ".", "text", "for", "t", "in", "flat_sentences_tokens", "]", "\n", "", "else", ":", "\n", "            ", "flat_sentences_tokens", "=", "[", "Token", "(", "word", ")", "for", "word", "in", "flattened_sentences", "]", "\n", "\n", "", "text_field", "=", "TextField", "(", "flat_sentences_tokens", ",", "self", ".", "_token_indexers", ")", "\n", "\n", "cluster_dict", "=", "{", "}", "\n", "if", "gold_clusters", "is", "not", "None", ":", "\n", "            ", "if", "self", ".", "_wordpiece_modeling_tokenizer", "is", "not", "None", ":", "\n", "                ", "for", "cluster", "in", "gold_clusters", ":", "\n", "                    ", "for", "mention_id", ",", "mention", "in", "enumerate", "(", "cluster", ")", ":", "\n", "                        ", "start", "=", "offsets", "[", "mention", "[", "0", "]", "]", "[", "0", "]", "\n", "end", "=", "offsets", "[", "mention", "[", "1", "]", "]", "[", "1", "]", "\n", "cluster", "[", "mention_id", "]", "=", "(", "start", ",", "end", ")", "\n", "\n", "", "", "", "for", "cluster_id", ",", "cluster", "in", "enumerate", "(", "gold_clusters", ")", ":", "\n", "                ", "for", "mention", "in", "cluster", ":", "\n", "                    ", "cluster_dict", "[", "tuple", "(", "mention", ")", "]", "=", "cluster_id", "\n", "\n", "", "", "", "spans", ":", "List", "[", "Field", "]", "=", "[", "]", "\n", "span_labels", ":", "Optional", "[", "List", "[", "int", "]", "]", "=", "[", "]", "if", "gold_clusters", "is", "not", "None", "else", "None", "\n", "\n", "sentence_offset", "=", "0", "\n", "for", "sentence", "in", "sentences", ":", "\n", "            ", "for", "start", ",", "end", "in", "enumerate_spans", "(", "\n", "sentence", ",", "offset", "=", "sentence_offset", ",", "max_span_width", "=", "self", ".", "_max_span_width", "\n", ")", ":", "\n", "                ", "if", "self", ".", "_wordpiece_modeling_tokenizer", "is", "not", "None", ":", "\n", "                    ", "start", "=", "offsets", "[", "start", "]", "[", "0", "]", "\n", "end", "=", "offsets", "[", "end", "]", "[", "1", "]", "\n", "\n", "# `enumerate_spans` uses word-level width limit; here we apply it to wordpieces", "\n", "# We have to do this check here because we use a span width embedding that has", "\n", "# only `self._max_span_width` entries, and since we are doing wordpiece", "\n", "# modeling, the span width embedding operates on wordpiece lengths. So a check", "\n", "# here is necessary or else we wouldn't know how many entries there would be.", "\n", "if", "end", "-", "start", "+", "1", ">", "self", ".", "_max_span_width", ":", "\n", "                        ", "continue", "\n", "# We also don't generate spans that contain special tokens", "\n", "", "if", "(", "\n", "start", "\n", "<", "self", ".", "_wordpiece_modeling_tokenizer", ".", "num_added_start_tokens", "\n", ")", ":", "\n", "                        ", "continue", "\n", "", "if", "(", "\n", "end", "\n", ">=", "len", "(", "flat_sentences_tokens", ")", "\n", "-", "self", ".", "_wordpiece_modeling_tokenizer", ".", "num_added_end_tokens", "\n", ")", ":", "\n", "                        ", "continue", "\n", "\n", "", "", "if", "span_labels", "is", "not", "None", ":", "\n", "                    ", "if", "(", "start", ",", "end", ")", "in", "cluster_dict", ":", "\n", "                        ", "span_labels", ".", "append", "(", "cluster_dict", "[", "(", "start", ",", "end", ")", "]", ")", "\n", "", "else", ":", "\n", "                        ", "span_labels", ".", "append", "(", "-", "1", ")", "\n", "\n", "", "", "spans", ".", "append", "(", "SpanField", "(", "start", ",", "end", ",", "text_field", ")", ")", "\n", "", "sentence_offset", "+=", "len", "(", "sentence", ")", "\n", "\n", "", "span_field", "=", "ListField", "(", "spans", ")", "\n", "\n", "metadata", ":", "Dict", "[", "str", ",", "Any", "]", "=", "{", "\"original_text\"", ":", "flattened_sentences", "}", "\n", "if", "gold_clusters", "is", "not", "None", ":", "\n", "            ", "metadata", "[", "\"clusters\"", "]", "=", "gold_clusters", "\n", "", "metadata_field", "=", "MetadataField", "(", "metadata", ")", "\n", "\n", "fields", ":", "Dict", "[", "str", ",", "Field", "]", "=", "{", "\n", "\"text\"", ":", "text_field", ",", "\n", "\"spans\"", ":", "span_field", ",", "\n", "\"metadata\"", ":", "metadata_field", ",", "\n", "}", "\n", "if", "span_labels", "is", "not", "None", ":", "\n", "            ", "fields", "[", "\"span_labels\"", "]", "=", "SequenceLabelField", "(", "span_labels", ",", "span_field", ")", "\n", "\n", "", "return", "Instance", "(", "fields", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.dataset_readers.coref_conll.CorefConllReader._normalize_word": [[246, 252], ["None"], "methods", ["None"], ["", "@", "staticmethod", "\n", "def", "_normalize_word", "(", "word", ")", ":", "\n", "        ", "if", "word", "in", "(", "\"/.\"", ",", "\"/?\"", ")", ":", "\n", "            ", "return", "word", "[", "1", ":", "]", "\n", "", "else", ":", "\n", "            ", "return", "word", "\n", "", "", "", ""]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.dataset_readers.coref_conll.canonicalize_clusters": [[26, 58], ["clusters.values", "list", "cluster_with_overlapping_mention.update", "merged_clusters.append", "set"], "function", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.metrics.conll_coref_scores.Scorer.update"], ["def", "canonicalize_clusters", "(", "\n", "clusters", ":", "DefaultDict", "[", "int", ",", "List", "[", "Tuple", "[", "int", ",", "int", "]", "]", "]", "\n", ")", "->", "List", "[", "List", "[", "Tuple", "[", "int", ",", "int", "]", "]", "]", ":", "\n", "    ", "\"\"\"\n    The CONLL 2012 data includes 2 annotated spans which are identical,\n    but have different ids. This checks all clusters for spans which are\n    identical, and if it finds any, merges the clusters containing the\n    identical spans.\n    \"\"\"", "\n", "merged_clusters", ":", "List", "[", "Set", "[", "Tuple", "[", "int", ",", "int", "]", "]", "]", "=", "[", "]", "\n", "for", "cluster", "in", "clusters", ".", "values", "(", ")", ":", "\n", "        ", "cluster_with_overlapping_mention", "=", "None", "\n", "for", "mention", "in", "cluster", ":", "\n", "# Look at clusters we have already processed to", "\n", "# see if they contain a mention in the current", "\n", "# cluster for comparison.", "\n", "            ", "for", "cluster2", "in", "merged_clusters", ":", "\n", "                ", "if", "mention", "in", "cluster2", ":", "\n", "# first cluster in merged clusters", "\n", "# which contains this mention.", "\n", "                    ", "cluster_with_overlapping_mention", "=", "cluster2", "\n", "break", "\n", "# Already encountered overlap - no need to keep looking.", "\n", "", "", "if", "cluster_with_overlapping_mention", "is", "not", "None", ":", "\n", "                ", "break", "\n", "", "", "if", "cluster_with_overlapping_mention", "is", "not", "None", ":", "\n", "# Merge cluster we are currently processing into", "\n", "# the cluster in the processed list.", "\n", "            ", "cluster_with_overlapping_mention", ".", "update", "(", "cluster", ")", "\n", "", "else", ":", "\n", "            ", "merged_clusters", ".", "append", "(", "set", "(", "cluster", ")", ")", "\n", "", "", "return", "[", "list", "(", "c", ")", "for", "c", "in", "merged_clusters", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.dataset_readers.srl_conll_2012.SrlReader.__init__": [[131, 150], ["allennlp.data.dataset_readers.dataset_reader.DatasetReader.__init__", "transformers.tokenization_bert.BertTokenizer.from_pretrained", "allennlp.data.token_indexers.SingleIdTokenIndexer"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.graph_util.plot.GraphPlot.__init__"], ["def", "__init__", "(", "\n", "self", ",", "\n", "token_indexers", ":", "Dict", "[", "str", ",", "TokenIndexer", "]", "=", "None", ",", "\n", "domain_identifier", ":", "str", "=", "None", ",", "\n", "bert_model_name", ":", "str", "=", "None", ",", "\n", "subset_size", ":", "int", "=", "sys", ".", "maxsize", ",", "\n", "**", "kwargs", ",", "\n", ")", "->", "None", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "**", "kwargs", ")", "\n", "self", ".", "_token_indexers", "=", "token_indexers", "or", "{", "\"tokens\"", ":", "SingleIdTokenIndexer", "(", ")", "}", "\n", "self", ".", "_domain_identifier", "=", "domain_identifier", "\n", "self", ".", "_to_yield", "=", "subset_size", "\n", "\n", "if", "bert_model_name", "is", "not", "None", ":", "\n", "            ", "self", ".", "bert_tokenizer", "=", "BertTokenizer", ".", "from_pretrained", "(", "bert_model_name", ")", "\n", "self", ".", "lowercase_input", "=", "\"uncased\"", "in", "bert_model_name", "\n", "", "else", ":", "\n", "            ", "self", ".", "bert_tokenizer", "=", "None", "\n", "self", ".", "lowercase_input", "=", "False", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.dataset_readers.srl_conll_2012.SrlReader._wordpiece_tokenize_input": [[151, 203], ["srl_conll_2012.SrlReader.bert_tokenizer.wordpiece_tokenizer.tokenize", "start_offsets.append", "len", "end_offsets.append", "word_piece_tokens.extend", "token.lower.lower.lower"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.tokenizers.whitespace_tokenizer.WhitespaceTokenizer.tokenize"], ["", "", "def", "_wordpiece_tokenize_input", "(", "\n", "self", ",", "tokens", ":", "List", "[", "str", "]", "\n", ")", "->", "Tuple", "[", "List", "[", "str", "]", ",", "List", "[", "int", "]", ",", "List", "[", "int", "]", "]", ":", "\n", "        ", "\"\"\"\n        Convert a list of tokens to wordpiece tokens and offsets, as well as adding\n        BERT CLS and SEP tokens to the begining and end of the sentence.\n\n        A slight oddity with this function is that it also returns the wordpiece offsets\n        corresponding to the _start_ of words as well as the end.\n\n        We need both of these offsets (or at least, it's easiest to use both), because we need\n        to convert the labels to tags using the end_offsets. However, when we are decoding a\n        BIO sequence inside the SRL model itself, it's important that we use the start_offsets,\n        because otherwise we might select an ill-formed BIO sequence from the BIO sequence on top of\n        wordpieces (this happens in the case that a word is split into multiple word pieces,\n        and then we take the last tag of the word, which might correspond to, e.g, I-V, which\n        would not be allowed as it is not preceeded by a B tag).\n\n        For example:\n\n        `annotate` will be bert tokenized as [\"anno\", \"##tate\"].\n        If this is tagged as [B-V, I-V] as it should be, we need to select the\n        _first_ wordpiece label to be the label for the token, because otherwise\n        we may end up with invalid tag sequences (we cannot start a new tag with an I).\n\n        # Returns\n\n        wordpieces : List[str]\n            The BERT wordpieces from the words in the sentence.\n        end_offsets : List[int]\n            Indices into wordpieces such that `[wordpieces[i] for i in end_offsets]`\n            results in the end wordpiece of each word being chosen.\n        start_offsets : List[int]\n            Indices into wordpieces such that `[wordpieces[i] for i in start_offsets]`\n            results in the start wordpiece of each word being chosen.\n        \"\"\"", "\n", "word_piece_tokens", ":", "List", "[", "str", "]", "=", "[", "]", "\n", "end_offsets", "=", "[", "]", "\n", "start_offsets", "=", "[", "]", "\n", "cumulative", "=", "0", "\n", "for", "token", "in", "tokens", ":", "\n", "            ", "if", "self", ".", "lowercase_input", ":", "\n", "                ", "token", "=", "token", ".", "lower", "(", ")", "\n", "", "word_pieces", "=", "self", ".", "bert_tokenizer", ".", "wordpiece_tokenizer", ".", "tokenize", "(", "token", ")", "\n", "start_offsets", ".", "append", "(", "cumulative", "+", "1", ")", "\n", "cumulative", "+=", "len", "(", "word_pieces", ")", "\n", "end_offsets", ".", "append", "(", "cumulative", ")", "\n", "word_piece_tokens", ".", "extend", "(", "word_pieces", ")", "\n", "\n", "", "wordpieces", "=", "[", "\"[CLS]\"", "]", "+", "word_piece_tokens", "+", "[", "\"[SEP]\"", "]", "\n", "\n", "return", "wordpieces", ",", "end_offsets", ",", "start_offsets", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.dataset_readers.srl_conll_2012.SrlReader._read": [[204, 234], ["allennlp.common.file_utils.cached_path", "allennlp.data.dataset_readers.dataset_utils.Ontonotes", "logger.info", "allennlp.data.dataset_readers.dataset_utils.Ontonotes.dataset_path_iterator", "logger.info", "allennlp.data.dataset_readers.dataset_utils.Ontonotes.sentence_iterator", "allennlp.data.tokenizers.Token", "srl_conll_2012.SrlReader.text_to_instance", "srl_conll_2012.SrlReader.text_to_instance"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.file_utils.cached_path", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.dataset_readers.multiprocess_dataset_reader.logger.info", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.dataset_utils.ontonotes.Ontonotes.dataset_path_iterator", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.dataset_readers.multiprocess_dataset_reader.logger.info", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.dataset_utils.ontonotes.Ontonotes.sentence_iterator", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.coreference_resolution.conll.ConllCorefReader.text_to_instance", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.coreference_resolution.conll.ConllCorefReader.text_to_instance"], ["", "@", "overrides", "\n", "def", "_read", "(", "self", ",", "file_path", ":", "str", ")", ":", "\n", "# if `file_path` is a URL, redirect to the cache", "\n", "        ", "file_path", "=", "cached_path", "(", "file_path", ")", "\n", "ontonotes_reader", "=", "Ontonotes", "(", ")", "\n", "logger", ".", "info", "(", "\"Reading SRL instances from dataset files at: %s\"", ",", "file_path", ")", "\n", "if", "self", ".", "_domain_identifier", "is", "not", "None", ":", "\n", "            ", "logger", ".", "info", "(", "\n", "\"Filtering to only include file paths containing the %s domain\"", ",", "\n", "self", ".", "_domain_identifier", ",", "\n", ")", "\n", "", "for", "conll_file", "in", "ontonotes_reader", ".", "dataset_path_iterator", "(", "file_path", ")", ":", "\n", "            ", "if", "self", ".", "_to_yield", ">", "0", "and", "(", "\n", "self", ".", "_domain_identifier", "is", "None", "\n", "or", "f\"/{self._domain_identifier}/\"", "in", "conll_file", "\n", ")", ":", "\n", "                ", "for", "sentence", "in", "ontonotes_reader", ".", "sentence_iterator", "(", "conll_file", ")", ":", "\n", "                    ", "tokens", "=", "[", "Token", "(", "t", ")", "for", "t", "in", "sentence", ".", "words", "]", "\n", "if", "not", "sentence", ".", "srl_frames", ":", "\n", "# Sentence contains no predicates.", "\n", "                        ", "tags", "=", "[", "\"O\"", "for", "_", "in", "tokens", "]", "\n", "verb_label", "=", "[", "0", "for", "_", "in", "tokens", "]", "\n", "yield", "self", ".", "text_to_instance", "(", "tokens", ",", "verb_label", ",", "tags", ")", "\n", "", "else", ":", "\n", "                        ", "for", "(", "_", ",", "tags", ")", "in", "sentence", ".", "srl_frames", ":", "\n", "                            ", "verb_indicator", "=", "[", "\n", "1", "if", "label", "[", "-", "2", ":", "]", "==", "\"-V\"", "else", "0", "for", "label", "in", "tags", "\n", "]", "\n", "yield", "self", ".", "text_to_instance", "(", "tokens", ",", "verb_indicator", ",", "tags", ")", "\n", "", "", "", "self", ".", "_to_yield", "-=", "1", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.dataset_readers.srl_conll_2012.SrlReader.text_to_instance": [[235, 288], ["all", "allennlp.data.fields.MetadataField", "allennlp.data.instance.Instance", "srl_conll_2012.SrlReader._wordpiece_tokenize_input", "srl_conll_2012._convert_verb_indices_to_wordpiece_indices", "allennlp.data.fields.TextField", "allennlp.data.fields.SequenceLabelField", "allennlp.data.fields.TextField", "allennlp.data.fields.SequenceLabelField", "verb_label.index", "srl_conll_2012._convert_tags_to_wordpiece_tags", "allennlp.data.fields.SequenceLabelField", "allennlp.data.fields.SequenceLabelField", "allennlp.data.tokenizers.Token"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.dataset_readers.semantic_role_labeling.SrlReader._wordpiece_tokenize_input", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.dataset_readers.semantic_role_labeling._convert_verb_indices_to_wordpiece_indices", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.fields.text_field.TextField.index", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.dataset_readers.semantic_role_labeling._convert_tags_to_wordpiece_tags"], ["", "", "", "def", "text_to_instance", "(", "# type: ignore", "\n", "self", ",", "tokens", ":", "List", "[", "Token", "]", ",", "verb_label", ":", "List", "[", "int", "]", ",", "tags", ":", "List", "[", "str", "]", "=", "None", "\n", ")", "->", "Instance", ":", "\n", "        ", "\"\"\"\n        We take `pre-tokenized` input here, along with a verb label.  The verb label should be a\n        one-hot binary vector, the same length as the tokens, indicating the position of the verb\n        to find arguments for.\n        \"\"\"", "\n", "\n", "metadata_dict", ":", "Dict", "[", "str", ",", "Any", "]", "=", "{", "}", "\n", "if", "self", ".", "bert_tokenizer", "is", "not", "None", ":", "\n", "            ", "wordpieces", ",", "offsets", ",", "start_offsets", "=", "self", ".", "_wordpiece_tokenize_input", "(", "\n", "[", "t", ".", "text", "for", "t", "in", "tokens", "]", "\n", ")", "\n", "new_verbs", "=", "_convert_verb_indices_to_wordpiece_indices", "(", "verb_label", ",", "offsets", ")", "\n", "metadata_dict", "[", "\"offsets\"", "]", "=", "start_offsets", "\n", "# In order to override the indexing mechanism, we need to set the `text_id`", "\n", "# attribute directly. This causes the indexing to use this id.", "\n", "text_field", "=", "TextField", "(", "\n", "[", "Token", "(", "t", ",", "text_id", "=", "self", ".", "bert_tokenizer", ".", "vocab", "[", "t", "]", ")", "for", "t", "in", "wordpieces", "]", ",", "\n", "token_indexers", "=", "self", ".", "_token_indexers", ",", "\n", ")", "\n", "verb_indicator", "=", "SequenceLabelField", "(", "new_verbs", ",", "text_field", ")", "\n", "\n", "", "else", ":", "\n", "            ", "text_field", "=", "TextField", "(", "tokens", ",", "token_indexers", "=", "self", ".", "_token_indexers", ")", "\n", "verb_indicator", "=", "SequenceLabelField", "(", "verb_label", ",", "text_field", ")", "\n", "\n", "", "fields", ":", "Dict", "[", "str", ",", "Field", "]", "=", "{", "}", "\n", "fields", "[", "\"tokens\"", "]", "=", "text_field", "\n", "fields", "[", "\"verb_indicator\"", "]", "=", "verb_indicator", "\n", "\n", "if", "all", "(", "[", "x", "==", "0", "for", "x", "in", "verb_label", "]", ")", ":", "\n", "            ", "verb", "=", "None", "\n", "verb_index", "=", "None", "\n", "", "else", ":", "\n", "            ", "verb_index", "=", "verb_label", ".", "index", "(", "1", ")", "\n", "verb", "=", "tokens", "[", "verb_index", "]", ".", "text", "\n", "\n", "", "metadata_dict", "[", "\"words\"", "]", "=", "[", "x", ".", "text", "for", "x", "in", "tokens", "]", "\n", "metadata_dict", "[", "\"verb\"", "]", "=", "verb", "\n", "metadata_dict", "[", "\"verb_index\"", "]", "=", "verb_index", "\n", "\n", "if", "tags", ":", "\n", "            ", "if", "self", ".", "bert_tokenizer", "is", "not", "None", ":", "\n", "                ", "new_tags", "=", "_convert_tags_to_wordpiece_tags", "(", "tags", ",", "offsets", ")", "\n", "fields", "[", "\"tags\"", "]", "=", "SequenceLabelField", "(", "new_tags", ",", "text_field", ")", "\n", "", "else", ":", "\n", "                ", "fields", "[", "\"tags\"", "]", "=", "SequenceLabelField", "(", "tags", ",", "text_field", ")", "\n", "", "metadata_dict", "[", "\"gold_tags\"", "]", "=", "tags", "\n", "\n", "", "fields", "[", "\"metadata\"", "]", "=", "MetadataField", "(", "metadata_dict", ")", "\n", "return", "Instance", "(", "fields", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.dataset_readers.srl_conll_2012._convert_tags_to_wordpiece_tags": [[20, 63], ["enumerate", "new_tags.append", "tag.startswith", "new_tags.append", "tag.startswith", "new_tags.append", "tag.startswith", "tag.split", "new_tags.append"], "function", ["None"], ["def", "_convert_tags_to_wordpiece_tags", "(", "tags", ":", "List", "[", "str", "]", ",", "offsets", ":", "List", "[", "int", "]", ")", "->", "List", "[", "str", "]", ":", "\n", "    ", "\"\"\"\n    Converts a series of BIO tags to account for a wordpiece tokenizer,\n    extending/modifying BIO tags where appropriate to deal with words which\n    are split into multiple wordpieces by the tokenizer.\n\n    This is only used if you pass a `bert_model_name` to the dataset reader below.\n\n    # Parameters\n\n    tags : `List[str]`\n        The BIO formatted tags to convert to BIO tags for wordpieces\n    offsets : `List[int]`\n        The wordpiece offsets.\n\n    # Returns\n\n    The new BIO tags.\n    \"\"\"", "\n", "new_tags", "=", "[", "]", "\n", "j", "=", "0", "\n", "for", "i", ",", "offset", "in", "enumerate", "(", "offsets", ")", ":", "\n", "        ", "tag", "=", "tags", "[", "i", "]", "\n", "is_o", "=", "tag", "==", "\"O\"", "\n", "is_start", "=", "True", "\n", "while", "j", "<", "offset", ":", "\n", "            ", "if", "is_o", ":", "\n", "                ", "new_tags", ".", "append", "(", "\"O\"", ")", "\n", "\n", "", "elif", "tag", ".", "startswith", "(", "\"I\"", ")", ":", "\n", "                ", "new_tags", ".", "append", "(", "tag", ")", "\n", "\n", "", "elif", "is_start", "and", "tag", ".", "startswith", "(", "\"B\"", ")", ":", "\n", "                ", "new_tags", ".", "append", "(", "tag", ")", "\n", "is_start", "=", "False", "\n", "\n", "", "elif", "tag", ".", "startswith", "(", "\"B\"", ")", ":", "\n", "                ", "_", ",", "label", "=", "tag", ".", "split", "(", "\"-\"", ",", "1", ")", "\n", "new_tags", ".", "append", "(", "\"I-\"", "+", "label", ")", "\n", "", "j", "+=", "1", "\n", "\n", "# Add O tags for cls and sep tokens.", "\n", "", "", "return", "[", "\"O\"", "]", "+", "new_tags", "+", "[", "\"O\"", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.dataset_readers.srl_conll_2012._convert_verb_indices_to_wordpiece_indices": [[65, 96], ["enumerate", "new_verb_indices.append"], "function", ["None"], ["", "def", "_convert_verb_indices_to_wordpiece_indices", "(", "\n", "verb_indices", ":", "List", "[", "int", "]", ",", "offsets", ":", "List", "[", "int", "]", "\n", ")", ":", "\n", "    ", "\"\"\"\n    Converts binary verb indicators to account for a wordpiece tokenizer,\n    extending/modifying BIO tags where appropriate to deal with words which\n    are split into multiple wordpieces by the tokenizer.\n\n    This is only used if you pass a `bert_model_name` to the dataset reader below.\n\n    # Parameters\n\n    verb_indices : `List[int]`\n        The binary verb indicators, 0 for not a verb, 1 for verb.\n    offsets : `List[int]`\n        The wordpiece offsets.\n\n    # Returns\n\n    The new verb indices.\n    \"\"\"", "\n", "j", "=", "0", "\n", "new_verb_indices", "=", "[", "]", "\n", "for", "i", ",", "offset", "in", "enumerate", "(", "offsets", ")", ":", "\n", "        ", "indicator", "=", "verb_indices", "[", "i", "]", "\n", "while", "j", "<", "offset", ":", "\n", "            ", "new_verb_indices", ".", "append", "(", "indicator", ")", "\n", "j", "+=", "1", "\n", "\n", "# Add 0 indicators for cls and sep tokens.", "\n", "", "", "return", "[", "0", "]", "+", "new_verb_indices", "+", "[", "0", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.dataset_readers.sharded_dataset_reader.ShardedDatasetReader.__init__": [[29, 33], ["allennlp.data.dataset_readers.dataset_reader.DatasetReader.__init__"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.graph_util.plot.GraphPlot.__init__"], ["def", "__init__", "(", "self", ",", "base_reader", ":", "DatasetReader", ",", "**", "kwargs", ",", ")", "->", "None", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "**", "kwargs", ")", "\n", "\n", "self", ".", "reader", "=", "base_reader", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.dataset_readers.sharded_dataset_reader.ShardedDatasetReader.text_to_instance": [[34, 39], ["sharded_dataset_reader.ShardedDatasetReader.reader.text_to_instance"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.coreference_resolution.conll.ConllCorefReader.text_to_instance"], ["", "def", "text_to_instance", "(", "self", ",", "*", "args", ",", "**", "kwargs", ")", "->", "Instance", ":", "\n", "        ", "\"\"\"\n        Just delegate to the base reader text_to_instance.\n        \"\"\"", "\n", "return", "self", ".", "reader", ".", "text_to_instance", "(", "*", "args", ",", "**", "kwargs", ")", "# type: ignore", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.dataset_readers.sharded_dataset_reader.ShardedDatasetReader._read": [[40, 51], ["glob.glob", "glob.glob.sort", "logger.info", "sharded_dataset_reader.ShardedDatasetReader.reader.read"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.dataset_readers.multiprocess_dataset_reader.logger.info", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.scripts.convert_openie_to_conll.read"], ["", "def", "_read", "(", "self", ",", "file_path", ":", "str", ")", "->", "Iterable", "[", "Instance", "]", ":", "\n", "        ", "shards", "=", "glob", ".", "glob", "(", "file_path", ")", "\n", "# Ensure a consistent order.", "\n", "shards", ".", "sort", "(", ")", "\n", "\n", "# TODO(brendanr): Modify such that different shards are used by", "\n", "# different workers in the distributed case.", "\n", "for", "shard", "in", "shards", ":", "\n", "            ", "logger", ".", "info", "(", "f\"reading instances from {shard}\"", ")", "\n", "for", "instance", "in", "self", ".", "reader", ".", "read", "(", "shard", ")", ":", "\n", "                ", "yield", "instance", "\n", "", "", "", "", ""]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.dataset_readers.sequence_tagging.SequenceTaggingDatasetReader.__init__": [[41, 52], ["allennlp.data.dataset_readers.dataset_reader.DatasetReader.__init__", "allennlp.data.token_indexers.SingleIdTokenIndexer"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.graph_util.plot.GraphPlot.__init__"], ["def", "__init__", "(", "\n", "self", ",", "\n", "word_tag_delimiter", ":", "str", "=", "DEFAULT_WORD_TAG_DELIMITER", ",", "\n", "token_delimiter", ":", "str", "=", "None", ",", "\n", "token_indexers", ":", "Dict", "[", "str", ",", "TokenIndexer", "]", "=", "None", ",", "\n", "**", "kwargs", ",", "\n", ")", "->", "None", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "**", "kwargs", ")", "\n", "self", ".", "_token_indexers", "=", "token_indexers", "or", "{", "\"tokens\"", ":", "SingleIdTokenIndexer", "(", ")", "}", "\n", "self", ".", "_word_tag_delimiter", "=", "word_tag_delimiter", "\n", "self", ".", "_token_delimiter", "=", "token_delimiter", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.dataset_readers.sequence_tagging.SequenceTaggingDatasetReader._read": [[53, 75], ["allennlp.common.file_utils.cached_path", "open", "logger.info", "line.strip.strip.strip", "pair.rsplit", "allennlp.data.tokenizers.Token", "sequence_tagging.SequenceTaggingDatasetReader.text_to_instance", "line.strip.strip.split"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.file_utils.cached_path", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.dataset_readers.multiprocess_dataset_reader.logger.info", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.coreference_resolution.conll.ConllCorefReader.text_to_instance"], ["", "@", "overrides", "\n", "def", "_read", "(", "self", ",", "file_path", ")", ":", "\n", "# if `file_path` is a URL, redirect to the cache", "\n", "        ", "file_path", "=", "cached_path", "(", "file_path", ")", "\n", "\n", "with", "open", "(", "file_path", ",", "\"r\"", ")", "as", "data_file", ":", "\n", "\n", "            ", "logger", ".", "info", "(", "\"Reading instances from lines in file at: %s\"", ",", "file_path", ")", "\n", "for", "line", "in", "data_file", ":", "\n", "                ", "line", "=", "line", ".", "strip", "(", "\"\\n\"", ")", "\n", "\n", "# skip blank lines", "\n", "if", "not", "line", ":", "\n", "                    ", "continue", "\n", "\n", "", "tokens_and_tags", "=", "[", "\n", "pair", ".", "rsplit", "(", "self", ".", "_word_tag_delimiter", ",", "1", ")", "\n", "for", "pair", "in", "line", ".", "split", "(", "self", ".", "_token_delimiter", ")", "\n", "]", "\n", "tokens", "=", "[", "Token", "(", "token", ")", "for", "token", ",", "tag", "in", "tokens_and_tags", "]", "\n", "tags", "=", "[", "tag", "for", "token", ",", "tag", "in", "tokens_and_tags", "]", "\n", "yield", "self", ".", "text_to_instance", "(", "tokens", ",", "tags", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.dataset_readers.sequence_tagging.SequenceTaggingDatasetReader.text_to_instance": [[76, 90], ["allennlp.data.fields.TextField", "allennlp.data.fields.MetadataField", "allennlp.data.instance.Instance", "allennlp.data.fields.SequenceLabelField"], "methods", ["None"], ["", "", "", "def", "text_to_instance", "(", "# type: ignore", "\n", "self", ",", "tokens", ":", "List", "[", "Token", "]", ",", "tags", ":", "List", "[", "str", "]", "=", "None", "\n", ")", "->", "Instance", ":", "\n", "        ", "\"\"\"\n        We take `pre-tokenized` input here, because we don't have a tokenizer in this class.\n        \"\"\"", "\n", "\n", "fields", ":", "Dict", "[", "str", ",", "Field", "]", "=", "{", "}", "\n", "sequence", "=", "TextField", "(", "tokens", ",", "self", ".", "_token_indexers", ")", "\n", "fields", "[", "\"tokens\"", "]", "=", "sequence", "\n", "fields", "[", "\"metadata\"", "]", "=", "MetadataField", "(", "{", "\"words\"", ":", "[", "x", ".", "text", "for", "x", "in", "tokens", "]", "}", ")", "\n", "if", "tags", "is", "not", "None", ":", "\n", "            ", "fields", "[", "\"tags\"", "]", "=", "SequenceLabelField", "(", "tags", ",", "sequence", ")", "\n", "", "return", "Instance", "(", "fields", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.dataset_readers.universal_dependencies_multilang.UniversalDependenciesMultiLangDatasetReader.__init__": [[83, 104], ["allennlp.data.dataset_readers.dataset_reader.DatasetReader.__init__", "allennlp.data.token_indexers.SingleIdTokenIndexer"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.graph_util.plot.GraphPlot.__init__"], ["def", "__init__", "(", "\n", "self", ",", "\n", "languages", ":", "List", "[", "str", "]", ",", "\n", "token_indexers", ":", "Dict", "[", "str", ",", "TokenIndexer", "]", "=", "None", ",", "\n", "use_language_specific_pos", ":", "bool", "=", "False", ",", "\n", "alternate", ":", "bool", "=", "True", ",", "\n", "is_first_pass_for_vocab", ":", "bool", "=", "True", ",", "\n", "instances_per_file", ":", "int", "=", "32", ",", "\n", "**", "kwargs", ",", "\n", ")", "->", "None", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "**", "kwargs", ")", "\n", "self", ".", "_languages", "=", "languages", "\n", "self", ".", "_token_indexers", "=", "token_indexers", "or", "{", "\"tokens\"", ":", "SingleIdTokenIndexer", "(", ")", "}", "\n", "self", ".", "_use_language_specific_pos", "=", "use_language_specific_pos", "\n", "\n", "self", ".", "_is_first_pass_for_vocab", "=", "is_first_pass_for_vocab", "\n", "self", ".", "_alternate", "=", "alternate", "\n", "self", ".", "_instances_per_file", "=", "instances_per_file", "\n", "\n", "self", ".", "_is_first_pass", "=", "True", "\n", "self", ".", "_iterators", ":", "List", "[", "Tuple", "[", "str", ",", "Iterator", "[", "Any", "]", "]", "]", "=", "None", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.dataset_readers.universal_dependencies_multilang.UniversalDependenciesMultiLangDatasetReader._read_one_file": [[105, 130], ["open", "logger.info", "conllu.parse_incr", "universal_dependencies_multilang.UniversalDependenciesMultiLangDatasetReader.text_to_instance", "isinstance", "list", "zip"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.dataset_readers.multiprocess_dataset_reader.logger.info", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.coreference_resolution.conll.ConllCorefReader.text_to_instance"], ["", "def", "_read_one_file", "(", "self", ",", "lang", ":", "str", ",", "file_path", ":", "str", ")", ":", "\n", "        ", "with", "open", "(", "file_path", ",", "\"r\"", ")", "as", "conllu_file", ":", "\n", "            ", "logger", ".", "info", "(", "\n", "\"Reading UD instances for %s language from conllu dataset at: %s\"", ",", "\n", "lang", ",", "\n", "file_path", ",", "\n", ")", "\n", "\n", "for", "annotation", "in", "parse_incr", "(", "conllu_file", ")", ":", "\n", "# CoNLLU annotations sometimes add back in words that have been elided", "\n", "# in the original sentence; we remove these, as we're just predicting", "\n", "# dependencies for the original sentence.", "\n", "# We filter by integers here as elided words have a non-integer word id,", "\n", "# as parsed by the conllu python library.", "\n", "                ", "annotation", "=", "[", "x", "for", "x", "in", "annotation", "if", "isinstance", "(", "x", "[", "\"id\"", "]", ",", "int", ")", "]", "\n", "\n", "heads", "=", "[", "x", "[", "\"head\"", "]", "for", "x", "in", "annotation", "]", "\n", "tags", "=", "[", "x", "[", "\"deprel\"", "]", "for", "x", "in", "annotation", "]", "\n", "words", "=", "[", "x", "[", "\"form\"", "]", "for", "x", "in", "annotation", "]", "\n", "if", "self", ".", "_use_language_specific_pos", ":", "\n", "                    ", "pos_tags", "=", "[", "x", "[", "\"xpostag\"", "]", "for", "x", "in", "annotation", "]", "\n", "", "else", ":", "\n", "                    ", "pos_tags", "=", "[", "x", "[", "\"upostag\"", "]", "for", "x", "in", "annotation", "]", "\n", "", "yield", "self", ".", "text_to_instance", "(", "\n", "lang", ",", "words", ",", "pos_tags", ",", "list", "(", "zip", "(", "tags", ",", "heads", ")", ")", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.dataset_readers.universal_dependencies_multilang.UniversalDependenciesMultiLangDatasetReader._read": [[132, 164], ["universal_dependencies_multilang.get_file_paths", "itertools.chain", "len", "iter", "numpy.random.randint", "range", "universal_dependencies_multilang.UniversalDependenciesMultiLangDatasetReader._read_one_file", "iter", "universal_dependencies_multilang.UniversalDependenciesMultiLangDatasetReader._read_one_file", "iter.__next__", "iter", "universal_dependencies_multilang.UniversalDependenciesMultiLangDatasetReader._read_one_file", "iter.__next__"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.dataset_readers.universal_dependencies_multilang.get_file_paths", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.dataset_readers.universal_dependencies_multilang.UniversalDependenciesMultiLangDatasetReader._read_one_file", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.dataset_readers.universal_dependencies_multilang.UniversalDependenciesMultiLangDatasetReader._read_one_file", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.token_embedders.embedding.EmbeddingsTextFile.__next__", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.dataset_readers.universal_dependencies_multilang.UniversalDependenciesMultiLangDatasetReader._read_one_file", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.token_embedders.embedding.EmbeddingsTextFile.__next__"], ["", "", "", "@", "overrides", "\n", "def", "_read", "(", "self", ",", "file_path", ":", "str", ")", ":", "\n", "        ", "file_paths", "=", "get_file_paths", "(", "file_path", ",", "self", ".", "_languages", ")", "\n", "if", "(", "self", ".", "_is_first_pass", "and", "self", ".", "_is_first_pass_for_vocab", ")", "or", "(", "\n", "not", "self", ".", "_alternate", "\n", ")", ":", "\n", "            ", "iterators", "=", "[", "\n", "iter", "(", "self", ".", "_read_one_file", "(", "lang", ",", "file_path", ")", ")", "\n", "for", "(", "lang", ",", "file_path", ")", "in", "file_paths", "\n", "]", "\n", "self", ".", "_is_first_pass", "=", "False", "\n", "for", "inst", "in", "itertools", ".", "chain", "(", "*", "iterators", ")", ":", "\n", "                ", "yield", "inst", "\n", "\n", "", "", "else", ":", "\n", "            ", "if", "self", ".", "_iterators", "is", "None", ":", "\n", "                ", "self", ".", "_iterators", "=", "[", "\n", "(", "lang", ",", "iter", "(", "self", ".", "_read_one_file", "(", "lang", ",", "file_path", ")", ")", ")", "\n", "for", "(", "lang", ",", "file_path", ")", "in", "file_paths", "\n", "]", "\n", "", "num_files", "=", "len", "(", "file_paths", ")", "\n", "while", "True", ":", "\n", "                ", "ind", "=", "np", ".", "random", ".", "randint", "(", "num_files", ")", "\n", "lang", ",", "lang_iter", "=", "self", ".", "_iterators", "[", "ind", "]", "\n", "for", "_", "in", "range", "(", "self", ".", "_instances_per_file", ")", ":", "\n", "                    ", "try", ":", "\n", "                        ", "yield", "lang_iter", ".", "__next__", "(", ")", "\n", "", "except", "StopIteration", ":", "\n", "                        ", "lang", ",", "file_path", "=", "file_paths", "[", "ind", "]", "\n", "lang_iter", "=", "iter", "(", "self", ".", "_read_one_file", "(", "lang", ",", "file_path", ")", ")", "\n", "self", ".", "_iterators", "[", "ind", "]", "=", "(", "lang", ",", "lang_iter", ")", "\n", "yield", "lang_iter", ".", "__next__", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.dataset_readers.universal_dependencies_multilang.UniversalDependenciesMultiLangDatasetReader.text_to_instance": [[165, 216], ["allennlp.data.fields.TextField", "allennlp.data.fields.SequenceLabelField", "allennlp.data.fields.MetadataField", "allennlp.data.instance.Instance", "allennlp.data.fields.SequenceLabelField", "allennlp.data.fields.SequenceLabelField", "allennlp.data.tokenizers.Token", "int"], "methods", ["None"], ["", "", "", "", "", "@", "overrides", "\n", "def", "text_to_instance", "(", "\n", "self", ",", "# type: ignore", "\n", "lang", ":", "str", ",", "\n", "words", ":", "List", "[", "str", "]", ",", "\n", "upos_tags", ":", "List", "[", "str", "]", ",", "\n", "dependencies", ":", "List", "[", "Tuple", "[", "str", ",", "int", "]", "]", "=", "None", ",", "\n", ")", "->", "Instance", ":", "\n", "\n", "        ", "\"\"\"\n        # Parameters\n\n        lang : `str`, required.\n            The language identifier.\n        words : `List[str]`, required.\n            The words in the sentence to be encoded.\n        upos_tags : `List[str]`, required.\n            The universal dependencies POS tags for each word.\n        dependencies `List[Tuple[str, int]]`, optional (default = None)\n            A list of  (head tag, head index) tuples. Indices are 1 indexed,\n            meaning an index of 0 corresponds to that word being the root of\n            the dependency tree.\n\n        # Returns\n\n        An instance containing words, upos tags, dependency head tags and head\n        indices as fields. The language identifier is stored in the metadata.\n        \"\"\"", "\n", "fields", ":", "Dict", "[", "str", ",", "Field", "]", "=", "{", "}", "\n", "\n", "tokens", "=", "TextField", "(", "[", "Token", "(", "w", ")", "for", "w", "in", "words", "]", ",", "self", ".", "_token_indexers", ")", "\n", "fields", "[", "\"words\"", "]", "=", "tokens", "\n", "fields", "[", "\"pos_tags\"", "]", "=", "SequenceLabelField", "(", "\n", "upos_tags", ",", "tokens", ",", "label_namespace", "=", "\"pos\"", "\n", ")", "\n", "if", "dependencies", "is", "not", "None", ":", "\n", "# We don't want to expand the label namespace with an additional dummy token, so we'll", "\n", "# always give the 'ROOT_HEAD' token a label of 'root'.", "\n", "            ", "fields", "[", "\"head_tags\"", "]", "=", "SequenceLabelField", "(", "\n", "[", "x", "[", "0", "]", "for", "x", "in", "dependencies", "]", ",", "tokens", ",", "label_namespace", "=", "\"head_tags\"", "\n", ")", "\n", "fields", "[", "\"head_indices\"", "]", "=", "SequenceLabelField", "(", "\n", "[", "int", "(", "x", "[", "1", "]", ")", "for", "x", "in", "dependencies", "]", ",", "\n", "tokens", ",", "\n", "label_namespace", "=", "\"head_index_tags\"", ",", "\n", ")", "\n", "\n", "", "fields", "[", "\"metadata\"", "]", "=", "MetadataField", "(", "\n", "{", "\"words\"", ":", "words", ",", "\"pos\"", ":", "upos_tags", ",", "\"lang\"", ":", "lang", "}", "\n", ")", "\n", "return", "Instance", "(", "fields", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.dataset_readers.universal_dependencies_multilang.get_file_paths": [[21, 49], ["glob.glob", "allennlp.common.checks.ConfigurationError", "os.path.splitext", "base.split", "paths.append", "os.path.basename"], "function", ["None"], ["def", "get_file_paths", "(", "pathname", ":", "str", ",", "languages", ":", "List", "[", "str", "]", ")", ":", "\n", "    ", "\"\"\"\n    Gets a list of all files by the pathname with the given language ids.\n    Filenames are assumed to have the language identifier followed by a dash\n    as a prefix (e.g. en-universal.conll).\n\n    # Parameters\n\n    pathname :  `str`, required.\n        An absolute or relative pathname (can contain shell-style wildcards)\n    languages : `List[str]`, required\n        The language identifiers to use.\n\n    # Returns\n\n    A list of tuples (language id, file path).\n    \"\"\"", "\n", "paths", "=", "[", "]", "\n", "for", "file_path", "in", "glob", ".", "glob", "(", "pathname", ")", ":", "\n", "        ", "base", "=", "os", ".", "path", ".", "splitext", "(", "os", ".", "path", ".", "basename", "(", "file_path", ")", ")", "[", "0", "]", "\n", "lang_id", "=", "base", ".", "split", "(", "\"-\"", ")", "[", "0", "]", "\n", "if", "lang_id", "in", "languages", ":", "\n", "            ", "paths", ".", "append", "(", "(", "lang_id", ",", "file_path", ")", ")", "\n", "\n", "", "", "if", "not", "paths", ":", "\n", "        ", "raise", "ConfigurationError", "(", "\"No dataset files to read\"", ")", "\n", "\n", "", "return", "paths", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.dataset_readers.copynet_seq2seq.CopyNetDatasetReader.__init__": [[103, 127], ["allennlp.data.dataset_readers.dataset_reader.DatasetReader.__init__", "allennlp.data.tokenizers.SpacyTokenizer", "allennlp.common.checks.ConfigurationError", "allennlp.data.token_indexers.SingleIdTokenIndexer", "allennlp.data.token_indexers.SingleIdTokenIndexer", "isinstance"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.graph_util.plot.GraphPlot.__init__"], [")", "\n", "self", ".", "_end_index", "=", "self", ".", "vocab", ".", "get_token_index", "(", "END_SYMBOL", ",", "self", ".", "_target_namespace", ")", "\n", "self", ".", "_oov_index", "=", "self", ".", "vocab", ".", "get_token_index", "(", "\n", "self", ".", "vocab", ".", "_oov_token", ",", "self", ".", "_target_namespace", "\n", ")", "\n", "self", ".", "_pad_index", "=", "self", ".", "vocab", ".", "get_token_index", "(", "\n", "self", ".", "vocab", ".", "_padding_token", ",", "self", ".", "_target_namespace", "\n", ")", "\n", "self", ".", "_copy_index", "=", "self", ".", "vocab", ".", "add_token_to_namespace", "(", "\n", "copy_token", ",", "self", ".", "_target_namespace", "\n", ")", "\n", "\n", "self", ".", "_tensor_based_metric", "=", "tensor_based_metric", "or", "BLEU", "(", "\n", "exclude_indices", "=", "{", "self", ".", "_pad_index", ",", "self", ".", "_end_index", ",", "self", ".", "_start_index", "}", "\n", ")", "\n", "self", ".", "_token_based_metric", "=", "token_based_metric", "\n", "\n", "self", ".", "_target_vocab_size", "=", "self", ".", "vocab", ".", "get_vocab_size", "(", "self", ".", "_target_namespace", ")", "\n", "\n", "# Encoding modules.", "\n", "self", ".", "_source_embedder", "=", "source_embedder", "\n", "self", ".", "_encoder", "=", "encoder", "\n", "\n", "# Decoder output dim needs to be the same as the encoder output dim since we initialize the", "\n", "# hidden state of the decoder with the final hidden state of the encoder.", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.dataset_readers.copynet_seq2seq.CopyNetDatasetReader._read": [[129, 147], ["open", "logger.info", "enumerate", "allennlp.common.file_utils.cached_path", "line.strip.strip.strip", "line.strip.strip.split", "len", "RuntimeError", "copynet_seq2seq.CopyNetDatasetReader.text_to_instance"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.dataset_readers.multiprocess_dataset_reader.logger.info", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.file_utils.cached_path", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.coreference_resolution.conll.ConllCorefReader.text_to_instance"], ["self", ".", "encoder_output_dim", "=", "self", ".", "_encoder", ".", "get_output_dim", "(", ")", "\n", "self", ".", "decoder_output_dim", "=", "self", ".", "encoder_output_dim", "\n", "self", ".", "decoder_input_dim", "=", "self", ".", "decoder_output_dim", "\n", "\n", "target_vocab_size", "=", "self", ".", "vocab", ".", "get_vocab_size", "(", "self", ".", "_target_namespace", ")", "\n", "\n", "# The decoder input will be a function of the embedding of the previous predicted token,", "\n", "# an attended encoder hidden state called the \"attentive read\", and another", "\n", "# weighted sum of the encoder hidden state called the \"selective read\".", "\n", "# While the weights for the attentive read are calculated by an `Attention` module,", "\n", "# the weights for the selective read are simply the predicted probabilities", "\n", "# corresponding to each token in the source sentence that matches the target", "\n", "# token from the previous timestep.", "\n", "self", ".", "_target_embedder", "=", "Embedding", "(", "target_vocab_size", ",", "target_embedding_dim", ")", "\n", "self", ".", "_attention", "=", "attention", "\n", "self", ".", "_input_projection_layer", "=", "Linear", "(", "\n", "target_embedding_dim", "+", "self", ".", "encoder_output_dim", "*", "2", ",", "self", ".", "decoder_input_dim", "\n", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.dataset_readers.copynet_seq2seq.CopyNetDatasetReader._tokens_to_ids": [[148, 155], ["out.append", "ids.setdefault", "token.text.lower", "len"], "methods", ["None"], ["# We then run the projected decoder input through an LSTM cell to produce", "\n", "# the next hidden state.", "\n", "self", ".", "_decoder_cell", "=", "LSTMCell", "(", "self", ".", "decoder_input_dim", ",", "self", ".", "decoder_output_dim", ")", "\n", "\n", "# We create a \"generation\" score for each token in the target vocab", "\n", "# with a linear projection of the decoder hidden state.", "\n", "self", ".", "_output_generation_layer", "=", "Linear", "(", "\n", "self", ".", "decoder_output_dim", ",", "target_vocab_size", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.dataset_readers.copynet_seq2seq.CopyNetDatasetReader.text_to_instance": [[156, 213], ["copynet_seq2seq.CopyNetDatasetReader._source_tokenizer.tokenize", "copynet_seq2seq.CopyNetDatasetReader.insert", "copynet_seq2seq.CopyNetDatasetReader.append", "allennlp.data.fields.TextField", "allennlp.data.fields.NamespaceSwappingField", "allennlp.data.fields.MetadataField", "allennlp.data.instance.Instance", "allennlp.data.tokenizers.Token", "allennlp.data.tokenizers.Token", "copynet_seq2seq.CopyNetDatasetReader._target_tokenizer.tokenize", "copynet_seq2seq.CopyNetDatasetReader.insert", "copynet_seq2seq.CopyNetDatasetReader.append", "allennlp.data.fields.TextField", "copynet_seq2seq.CopyNetDatasetReader._tokens_to_ids", "allennlp.data.fields.ArrayField", "allennlp.data.fields.ArrayField", "copynet_seq2seq.CopyNetDatasetReader._tokens_to_ids", "allennlp.data.fields.ArrayField", "allennlp.data.tokenizers.Token", "allennlp.data.tokenizers.Token", "numpy.array", "numpy.array", "numpy.array", "len", "len"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.tokenizers.whitespace_tokenizer.WhitespaceTokenizer.tokenize", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.tokenizers.whitespace_tokenizer.WhitespaceTokenizer.tokenize", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.dataset_readers.copynet_seq2seq.CopyNetDatasetReader._tokens_to_ids", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.dataset_readers.copynet_seq2seq.CopyNetDatasetReader._tokens_to_ids"], [")", "\n", "\n", "# We create a \"copying\" score for each source token by applying a non-linearity", "\n", "# (tanh) to a linear projection of the encoded hidden state for that token,", "\n", "# and then taking the dot product of the result with the decoder hidden state.", "\n", "self", ".", "_output_copying_layer", "=", "Linear", "(", "\n", "self", ".", "encoder_output_dim", ",", "self", ".", "decoder_output_dim", "\n", ")", "\n", "\n", "# At prediction time, we'll use a beam search to find the best target sequence.", "\n", "self", ".", "_beam_search", "=", "BeamSearch", "(", "\n", "self", ".", "_end_index", ",", "max_steps", "=", "max_decoding_steps", ",", "beam_size", "=", "beam_size", "\n", ")", "\n", "\n", "initializer", "(", "self", ")", "\n", "\n", "", "@", "overrides", "\n", "def", "forward", "(", "\n", "self", ",", "# type: ignore", "\n", "source_tokens", ":", "TextFieldTensors", ",", "\n", "source_token_ids", ":", "torch", ".", "Tensor", ",", "\n", "source_to_target", ":", "torch", ".", "Tensor", ",", "\n", "metadata", ":", "List", "[", "Dict", "[", "str", ",", "Any", "]", "]", ",", "\n", "target_tokens", ":", "TextFieldTensors", "=", "None", ",", "\n", "target_token_ids", ":", "torch", ".", "Tensor", "=", "None", ",", "\n", ")", "->", "Dict", "[", "str", ",", "torch", ".", "Tensor", "]", ":", "\n", "\n", "        ", "\"\"\"\n        Make foward pass with decoder logic for producing the entire target sequence.\n\n        # Parameters\n\n        source_tokens : `TextFieldTensors`, required\n            The output of `TextField.as_array()` applied on the source `TextField`. This will be\n            passed through a `TextFieldEmbedder` and then through an encoder.\n        source_token_ids : `torch.Tensor`, required\n            Tensor containing IDs that indicate which source tokens match each other.\n            Has shape: `(batch_size, trimmed_source_length)`.\n        source_to_target : `torch.Tensor`, required\n            Tensor containing vocab index of each source token with respect to the\n            target vocab namespace. Shape: `(batch_size, trimmed_source_length)`.\n        metadata : `List[Dict[str, Any]]`, required\n            Metadata field that contains the original source tokens with key 'source_tokens'\n            and any other meta fields. When 'target_tokens' is also passed, the metadata\n            should also contain the original target tokens with key 'target_tokens'.\n        target_tokens : `TextFieldTensors`, optional (default = None)\n            Output of `Textfield.as_array()` applied on target `TextField`. We assume that the\n            target tokens are also represented as a `TextField` which must contain a \"tokens\"\n            key that uses single ids.\n        target_token_ids : `torch.Tensor`, optional (default = None)\n            A tensor of shape `(batch_size, target_sequence_length)` which indicates which\n            tokens in the target sequence match tokens in the source sequence.\n\n        # Returns\n\n        Dict[str, torch.Tensor]\n        \"\"\"", "\n", "state", "=", "self", ".", "_encode", "(", "source_tokens", ")", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.dataset_readers.text_classification_json.TextClassificationJsonReader.__init__": [[45, 62], ["allennlp.data.dataset_readers.dataset_reader.DatasetReader.__init__", "allennlp.data.tokenizers.SpacyTokenizer", "allennlp.data.tokenizers.sentence_splitter.SpacySentenceSplitter", "allennlp.data.token_indexers.SingleIdTokenIndexer"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.graph_util.plot.GraphPlot.__init__"], ["def", "__init__", "(", "\n", "self", ",", "\n", "token_indexers", ":", "Dict", "[", "str", ",", "TokenIndexer", "]", "=", "None", ",", "\n", "tokenizer", ":", "Tokenizer", "=", "None", ",", "\n", "segment_sentences", ":", "bool", "=", "False", ",", "\n", "max_sequence_length", ":", "int", "=", "None", ",", "\n", "skip_label_indexing", ":", "bool", "=", "False", ",", "\n", "**", "kwargs", ",", "\n", ")", "->", "None", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "**", "kwargs", ")", "\n", "self", ".", "_tokenizer", "=", "tokenizer", "or", "SpacyTokenizer", "(", ")", "\n", "self", ".", "_segment_sentences", "=", "segment_sentences", "\n", "self", ".", "_max_sequence_length", "=", "max_sequence_length", "\n", "self", ".", "_skip_label_indexing", "=", "skip_label_indexing", "\n", "self", ".", "_token_indexers", "=", "token_indexers", "or", "{", "\"tokens\"", ":", "SingleIdTokenIndexer", "(", ")", "}", "\n", "if", "self", ".", "_segment_sentences", ":", "\n", "            ", "self", ".", "_sentence_segmenter", "=", "SpacySentenceSplitter", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.dataset_readers.text_classification_json.TextClassificationJsonReader._read": [[63, 85], ["open", "data_file.readlines", "allennlp.common.file_utils.cached_path", "json.loads", "json.loads.get", "text_classification_json.TextClassificationJsonReader.text_to_instance", "str", "int", "ValueError"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.file_utils.cached_path", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.params.Params.get", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.coreference_resolution.conll.ConllCorefReader.text_to_instance"], ["", "", "@", "overrides", "\n", "def", "_read", "(", "self", ",", "file_path", ")", ":", "\n", "        ", "with", "open", "(", "cached_path", "(", "file_path", ")", ",", "\"r\"", ")", "as", "data_file", ":", "\n", "            ", "for", "line", "in", "data_file", ".", "readlines", "(", ")", ":", "\n", "                ", "if", "not", "line", ":", "\n", "                    ", "continue", "\n", "", "items", "=", "json", ".", "loads", "(", "line", ")", "\n", "text", "=", "items", "[", "\"text\"", "]", "\n", "label", "=", "items", ".", "get", "(", "\"label\"", ",", "None", ")", "\n", "if", "label", "is", "not", "None", ":", "\n", "                    ", "if", "self", ".", "_skip_label_indexing", ":", "\n", "                        ", "try", ":", "\n", "                            ", "label", "=", "int", "(", "label", ")", "\n", "", "except", "ValueError", ":", "\n", "                            ", "raise", "ValueError", "(", "\n", "\"Labels must be integers if skip_label_indexing is True.\"", "\n", ")", "\n", "", "", "else", ":", "\n", "                        ", "label", "=", "str", "(", "label", ")", "\n", "", "", "instance", "=", "self", ".", "text_to_instance", "(", "text", "=", "text", ",", "label", "=", "label", ")", "\n", "if", "instance", "is", "not", "None", ":", "\n", "                    ", "yield", "instance", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.dataset_readers.text_classification_json.TextClassificationJsonReader._truncate": [[86, 93], ["len"], "methods", ["None"], ["", "", "", "", "def", "_truncate", "(", "self", ",", "tokens", ")", ":", "\n", "        ", "\"\"\"\n        truncate a set of tokens using the provided sequence length\n        \"\"\"", "\n", "if", "len", "(", "tokens", ")", ">", "self", ".", "_max_sequence_length", ":", "\n", "            ", "tokens", "=", "tokens", "[", ":", "self", ".", "_max_sequence_length", "]", "\n", "", "return", "tokens", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.dataset_readers.text_classification_json.TextClassificationJsonReader.text_to_instance": [[94, 133], ["allennlp.data.instance.Instance", "text_classification_json.TextClassificationJsonReader._sentence_segmenter.split_sentences", "allennlp.data.fields.ListField", "text_classification_json.TextClassificationJsonReader._tokenizer.tokenize", "allennlp.data.fields.TextField", "allennlp.data.fields.LabelField", "text_classification_json.TextClassificationJsonReader._tokenizer.tokenize", "sentences.append", "text_classification_json.TextClassificationJsonReader._truncate", "text_classification_json.TextClassificationJsonReader._truncate", "allennlp.data.fields.TextField"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.tokenizers.sentence_splitter.SpacySentenceSplitter.split_sentences", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.tokenizers.whitespace_tokenizer.WhitespaceTokenizer.tokenize", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.tokenizers.whitespace_tokenizer.WhitespaceTokenizer.tokenize", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.dataset_readers.text_classification_json.TextClassificationJsonReader._truncate", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.dataset_readers.text_classification_json.TextClassificationJsonReader._truncate"], ["", "@", "overrides", "\n", "def", "text_to_instance", "(", "\n", "self", ",", "text", ":", "str", ",", "label", ":", "Union", "[", "str", ",", "int", "]", "=", "None", "\n", ")", "->", "Instance", ":", "# type: ignore", "\n", "        ", "\"\"\"\n        # Parameters\n\n        text : `str`, required.\n            The text to classify\n        label : `str`, optional, (default = None).\n            The label for this text.\n\n        # Returns\n\n        An `Instance` containing the following fields:\n            tokens : `TextField`\n                The tokens in the sentence or phrase.\n            label : `LabelField`\n                The label label of the sentence or phrase.\n        \"\"\"", "\n", "\n", "fields", ":", "Dict", "[", "str", ",", "Field", "]", "=", "{", "}", "\n", "if", "self", ".", "_segment_sentences", ":", "\n", "            ", "sentences", ":", "List", "[", "Field", "]", "=", "[", "]", "\n", "sentence_splits", "=", "self", ".", "_sentence_segmenter", ".", "split_sentences", "(", "text", ")", "\n", "for", "sentence", "in", "sentence_splits", ":", "\n", "                ", "word_tokens", "=", "self", ".", "_tokenizer", ".", "tokenize", "(", "sentence", ")", "\n", "if", "self", ".", "_max_sequence_length", "is", "not", "None", ":", "\n", "                    ", "word_tokens", "=", "self", ".", "_truncate", "(", "word_tokens", ")", "\n", "", "sentences", ".", "append", "(", "TextField", "(", "word_tokens", ",", "self", ".", "_token_indexers", ")", ")", "\n", "", "fields", "[", "\"tokens\"", "]", "=", "ListField", "(", "sentences", ")", "\n", "", "else", ":", "\n", "            ", "tokens", "=", "self", ".", "_tokenizer", ".", "tokenize", "(", "text", ")", "\n", "if", "self", ".", "_max_sequence_length", "is", "not", "None", ":", "\n", "                ", "tokens", "=", "self", ".", "_truncate", "(", "tokens", ")", "\n", "", "fields", "[", "\"tokens\"", "]", "=", "TextField", "(", "tokens", ",", "self", ".", "_token_indexers", ")", "\n", "", "if", "label", "is", "not", "None", ":", "\n", "            ", "fields", "[", "\"label\"", "]", "=", "LabelField", "(", "label", ",", "skip_indexing", "=", "self", ".", "_skip_label_indexing", ")", "\n", "", "return", "Instance", "(", "fields", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.dataset_readers.quora_paraphrase.QuoraParaphraseDatasetReader.__init__": [[39, 48], ["allennlp.data.dataset_readers.dataset_reader.DatasetReader.__init__", "allennlp.data.tokenizers.whitespace_tokenizer.WhitespaceTokenizer", "allennlp.data.token_indexers.SingleIdTokenIndexer"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.graph_util.plot.GraphPlot.__init__"], ["def", "__init__", "(", "\n", "self", ",", "\n", "tokenizer", ":", "Tokenizer", "=", "None", ",", "\n", "token_indexers", ":", "Dict", "[", "str", ",", "TokenIndexer", "]", "=", "None", ",", "\n", "**", "kwargs", ",", "\n", ")", "->", "None", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "**", "kwargs", ")", "\n", "self", ".", "_tokenizer", "=", "tokenizer", "or", "WhitespaceTokenizer", "(", ")", "\n", "self", ".", "_token_indexers", "=", "token_indexers", "or", "{", "\"tokens\"", ":", "SingleIdTokenIndexer", "(", ")", "}", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.dataset_readers.quora_paraphrase.QuoraParaphraseDatasetReader._read": [[49, 58], ["logger.info", "open", "csv.reader", "allennlp.common.file_utils.cached_path", "len", "quora_paraphrase.QuoraParaphraseDatasetReader.text_to_instance"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.dataset_readers.multiprocess_dataset_reader.logger.info", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.file_utils.cached_path", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.coreference_resolution.conll.ConllCorefReader.text_to_instance"], ["", "@", "overrides", "\n", "def", "_read", "(", "self", ",", "file_path", ")", ":", "\n", "        ", "logger", ".", "info", "(", "\"Reading instances from lines in file at: %s\"", ",", "file_path", ")", "\n", "with", "open", "(", "cached_path", "(", "file_path", ")", ",", "\"r\"", ")", "as", "data_file", ":", "\n", "            ", "tsv_in", "=", "csv", ".", "reader", "(", "data_file", ",", "delimiter", "=", "\"\\t\"", ")", "\n", "for", "row", "in", "tsv_in", ":", "\n", "                ", "if", "len", "(", "row", ")", "==", "4", ":", "\n", "                    ", "yield", "self", ".", "text_to_instance", "(", "\n", "premise", "=", "row", "[", "1", "]", ",", "hypothesis", "=", "row", "[", "2", "]", ",", "label", "=", "row", "[", "0", "]", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.dataset_readers.quora_paraphrase.QuoraParaphraseDatasetReader.text_to_instance": [[60, 77], ["quora_paraphrase.QuoraParaphraseDatasetReader._tokenizer.tokenize", "quora_paraphrase.QuoraParaphraseDatasetReader._tokenizer.tokenize", "allennlp.data.fields.TextField", "allennlp.data.fields.TextField", "allennlp.data.instance.Instance", "allennlp.data.fields.LabelField"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.tokenizers.whitespace_tokenizer.WhitespaceTokenizer.tokenize", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.tokenizers.whitespace_tokenizer.WhitespaceTokenizer.tokenize"], ["", "", "", "", "@", "overrides", "\n", "def", "text_to_instance", "(", "\n", "self", ",", "# type: ignore", "\n", "premise", ":", "str", ",", "\n", "hypothesis", ":", "str", ",", "\n", "label", ":", "str", "=", "None", ",", "\n", ")", "->", "Instance", ":", "\n", "\n", "        ", "fields", ":", "Dict", "[", "str", ",", "Field", "]", "=", "{", "}", "\n", "tokenized_premise", "=", "self", ".", "_tokenizer", ".", "tokenize", "(", "premise", ")", "\n", "tokenized_hypothesis", "=", "self", ".", "_tokenizer", ".", "tokenize", "(", "hypothesis", ")", "\n", "fields", "[", "\"premise\"", "]", "=", "TextField", "(", "tokenized_premise", ",", "self", ".", "_token_indexers", ")", "\n", "fields", "[", "\"hypothesis\"", "]", "=", "TextField", "(", "tokenized_hypothesis", ",", "self", ".", "_token_indexers", ")", "\n", "if", "label", "is", "not", "None", ":", "\n", "            ", "fields", "[", "\"label\"", "]", "=", "LabelField", "(", "label", ")", "\n", "\n", "", "return", "Instance", "(", "fields", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.dataset_readers.snli.SnliReader.__init__": [[34, 43], ["allennlp.data.dataset_readers.dataset_reader.DatasetReader.__init__", "allennlp.data.tokenizers.SpacyTokenizer", "allennlp.data.token_indexers.SingleIdTokenIndexer"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.graph_util.plot.GraphPlot.__init__"], ["def", "__init__", "(", "\n", "self", ",", "\n", "tokenizer", ":", "Tokenizer", "=", "None", ",", "\n", "token_indexers", ":", "Dict", "[", "str", ",", "TokenIndexer", "]", "=", "None", ",", "\n", "**", "kwargs", ",", "\n", ")", "->", "None", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "**", "kwargs", ")", "\n", "self", ".", "_tokenizer", "=", "tokenizer", "or", "SpacyTokenizer", "(", ")", "\n", "self", ".", "_token_indexers", "=", "token_indexers", "or", "{", "\"tokens\"", ":", "SingleIdTokenIndexer", "(", ")", "}", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.dataset_readers.snli.SnliReader._read": [[44, 64], ["allennlp.common.file_utils.cached_path", "open", "logger.info", "json.loads", "snli.SnliReader.text_to_instance"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.file_utils.cached_path", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.dataset_readers.multiprocess_dataset_reader.logger.info", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.coreference_resolution.conll.ConllCorefReader.text_to_instance"], ["", "@", "overrides", "\n", "def", "_read", "(", "self", ",", "file_path", ":", "str", ")", ":", "\n", "# if `file_path` is a URL, redirect to the cache", "\n", "        ", "file_path", "=", "cached_path", "(", "file_path", ")", "\n", "\n", "with", "open", "(", "file_path", ",", "\"r\"", ")", "as", "snli_file", ":", "\n", "            ", "logger", ".", "info", "(", "\"Reading SNLI instances from jsonl dataset at: %s\"", ",", "file_path", ")", "\n", "for", "line", "in", "snli_file", ":", "\n", "                ", "example", "=", "json", ".", "loads", "(", "line", ")", "\n", "\n", "label", "=", "example", "[", "\"gold_label\"", "]", "\n", "if", "label", "==", "\"-\"", ":", "\n", "# These were cases where the annotators disagreed; we'll just skip them.  It's", "\n", "# like 800 out of 500k examples in the training data.", "\n", "                    ", "continue", "\n", "\n", "", "premise", "=", "example", "[", "\"sentence1\"", "]", "\n", "hypothesis", "=", "example", "[", "\"sentence2\"", "]", "\n", "\n", "yield", "self", ".", "text_to_instance", "(", "premise", ",", "hypothesis", ",", "label", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.dataset_readers.snli.SnliReader.text_to_instance": [[65, 87], ["snli.SnliReader._tokenizer.tokenize", "snli.SnliReader._tokenizer.tokenize", "allennlp.data.fields.TextField", "allennlp.data.fields.TextField", "allennlp.data.fields.MetadataField", "allennlp.data.instance.Instance", "allennlp.data.fields.LabelField"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.tokenizers.whitespace_tokenizer.WhitespaceTokenizer.tokenize", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.tokenizers.whitespace_tokenizer.WhitespaceTokenizer.tokenize"], ["", "", "", "@", "overrides", "\n", "def", "text_to_instance", "(", "\n", "self", ",", "# type: ignore", "\n", "premise", ":", "str", ",", "\n", "hypothesis", ":", "str", ",", "\n", "label", ":", "str", "=", "None", ",", "\n", ")", "->", "Instance", ":", "\n", "\n", "        ", "fields", ":", "Dict", "[", "str", ",", "Field", "]", "=", "{", "}", "\n", "premise_tokens", "=", "self", ".", "_tokenizer", ".", "tokenize", "(", "premise", ")", "\n", "hypothesis_tokens", "=", "self", ".", "_tokenizer", ".", "tokenize", "(", "hypothesis", ")", "\n", "fields", "[", "\"premise\"", "]", "=", "TextField", "(", "premise_tokens", ",", "self", ".", "_token_indexers", ")", "\n", "fields", "[", "\"hypothesis\"", "]", "=", "TextField", "(", "hypothesis_tokens", ",", "self", ".", "_token_indexers", ")", "\n", "if", "label", ":", "\n", "            ", "fields", "[", "\"label\"", "]", "=", "LabelField", "(", "label", ")", "\n", "\n", "", "metadata", "=", "{", "\n", "\"premise_tokens\"", ":", "[", "x", ".", "text", "for", "x", "in", "premise_tokens", "]", ",", "\n", "\"hypothesis_tokens\"", ":", "[", "x", ".", "text", "for", "x", "in", "hypothesis_tokens", "]", ",", "\n", "}", "\n", "fields", "[", "\"metadata\"", "]", "=", "MetadataField", "(", "metadata", ")", "\n", "return", "Instance", "(", "fields", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.dataset_readers.masked_language_modeling.MaskedLanguageModelingReader.__init__": [[47, 64], ["allennlp.data.dataset_readers.dataset_reader.DatasetReader.__init__", "isinstance", "allennlp.data.tokenizers.whitespace_tokenizer.WhitespaceTokenizer", "copy.copy", "allennlp.data.token_indexers.SingleIdTokenIndexer"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.graph_util.plot.GraphPlot.__init__"], ["def", "__init__", "(", "\n", "self", ",", "\n", "tokenizer", ":", "Tokenizer", "=", "None", ",", "\n", "token_indexers", ":", "Dict", "[", "str", ",", "TokenIndexer", "]", "=", "None", ",", "\n", "**", "kwargs", ",", "\n", ")", "->", "None", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "**", "kwargs", ")", "\n", "self", ".", "_tokenizer", "=", "tokenizer", "or", "WhitespaceTokenizer", "(", ")", "\n", "# temporary hack to not to add special tokens", "\n", "self", ".", "_targets_tokenizer", ":", "Tokenizer", "\n", "if", "isinstance", "(", "self", ".", "_tokenizer", ",", "PretrainedTransformerTokenizer", ")", ":", "\n", "            ", "self", ".", "_targets_tokenizer", "=", "copy", ".", "copy", "(", "self", ".", "_tokenizer", ")", "\n", "self", ".", "_targets_tokenizer", ".", "_add_special_tokens", "=", "False", "\n", "", "else", ":", "\n", "            ", "self", ".", "_targets_tokenizer", "=", "self", ".", "_tokenizer", "\n", "\n", "", "self", ".", "_token_indexers", "=", "token_indexers", "or", "{", "\"tokens\"", ":", "SingleIdTokenIndexer", "(", ")", "}", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.dataset_readers.masked_language_modeling.MaskedLanguageModelingReader._read": [[65, 78], ["logger.error", "open", "masked_language_modeling.MaskedLanguageModelingReader._tokenizer.tokenize", "allennlp.data.tokenizers.Token", "masked_language_modeling.MaskedLanguageModelingReader.text_to_instance"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.tokenizers.whitespace_tokenizer.WhitespaceTokenizer.tokenize", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.coreference_resolution.conll.ConllCorefReader.text_to_instance"], ["", "@", "overrides", "\n", "def", "_read", "(", "self", ",", "file_path", ":", "str", ")", ":", "\n", "        ", "import", "sys", "\n", "\n", "# You can call pytest with either `pytest` or `py.test`.", "\n", "if", "\"test\"", "not", "in", "sys", ".", "argv", "[", "0", "]", ":", "\n", "            ", "logger", ".", "error", "(", "\"_read is only implemented for unit tests at the moment\"", ")", "\n", "", "with", "open", "(", "file_path", ",", "\"r\"", ")", "as", "text_file", ":", "\n", "            ", "for", "sentence", "in", "text_file", ":", "\n", "                ", "tokens", "=", "self", ".", "_tokenizer", ".", "tokenize", "(", "sentence", ")", "\n", "target", "=", "tokens", "[", "0", "]", ".", "text", "\n", "tokens", "[", "0", "]", "=", "Token", "(", "\"[MASK]\"", ")", "\n", "yield", "self", ".", "text_to_instance", "(", "sentence", ",", "tokens", ",", "[", "target", "]", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.dataset_readers.masked_language_modeling.MaskedLanguageModelingReader.text_to_instance": [[79, 140], ["allennlp.data.fields.TextField", "enumerate", "allennlp.data.fields.ListField", "allennlp.data.instance.Instance", "masked_language_modeling.MaskedLanguageModelingReader._tokenizer.tokenize", "ValueError", "ValueError", "zip", "allennlp.data.fields.TextField", "mask_positions.append", "len", "len", "allennlp.data.fields.IndexField", "target_tokens.append", "masked_language_modeling.MaskedLanguageModelingReader._targets_tokenizer.tokenize", "allennlp.data.tokenizers.Token", "len", "len"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.tokenizers.whitespace_tokenizer.WhitespaceTokenizer.tokenize", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.tokenizers.whitespace_tokenizer.WhitespaceTokenizer.tokenize"], ["", "", "", "@", "overrides", "\n", "def", "text_to_instance", "(", "\n", "self", ",", "# type: ignore", "\n", "sentence", ":", "str", "=", "None", ",", "\n", "tokens", ":", "List", "[", "Token", "]", "=", "None", ",", "\n", "targets", ":", "List", "[", "str", "]", "=", "None", ",", "\n", ")", "->", "Instance", ":", "\n", "\n", "        ", "\"\"\"\n        # Parameters\n\n        sentence : `str`, optional\n            A sentence containing [MASK] tokens that should be filled in by the model.  This input\n            is superceded and ignored if `tokens` is given.\n        tokens : `List[Token]`, optional\n            An already-tokenized sentence containing some number of [MASK] tokens to be predicted.\n        targets : `List[str]`, optional\n            Contains the target tokens to be predicted.  The length of this list should be the same\n            as the number of [MASK] tokens in the input.\n        \"\"\"", "\n", "if", "not", "tokens", ":", "\n", "            ", "tokens", "=", "self", ".", "_tokenizer", ".", "tokenize", "(", "sentence", ")", "\n", "", "input_field", "=", "TextField", "(", "tokens", ",", "self", ".", "_token_indexers", ")", "\n", "mask_positions", "=", "[", "]", "\n", "for", "i", ",", "token", "in", "enumerate", "(", "tokens", ")", ":", "\n", "            ", "if", "token", ".", "text", "==", "\"[MASK]\"", ":", "\n", "                ", "mask_positions", ".", "append", "(", "i", ")", "\n", "", "", "if", "not", "mask_positions", ":", "\n", "            ", "raise", "ValueError", "(", "\"No [MASK] tokens found!\"", ")", "\n", "", "if", "targets", "and", "len", "(", "targets", ")", "!=", "len", "(", "mask_positions", ")", ":", "\n", "            ", "raise", "ValueError", "(", "\n", "f\"Found {len(mask_positions)} mask tokens and {len(targets)} targets\"", "\n", ")", "\n", "", "mask_position_field", "=", "ListField", "(", "\n", "[", "IndexField", "(", "i", ",", "input_field", ")", "for", "i", "in", "mask_positions", "]", "\n", ")", "\n", "fields", ":", "Dict", "[", "str", ",", "Field", "]", "=", "{", "\n", "\"tokens\"", ":", "input_field", ",", "\n", "\"mask_positions\"", ":", "mask_position_field", ",", "\n", "}", "\n", "# TODO(mattg): there's a problem if the targets get split into multiple word pieces...", "\n", "# (maksym-del): if we index word that was not split into wordpieces with", "\n", "# PretrainedTransformerTokenizer we will get OOV token ID...", "\n", "# Until this is handeled, let's use first wordpiece id for each token since tokens should contain text_ids", "\n", "# to be indexed with PretrainedTokenIndexer. It also requeires hack to avoid adding special tokens...", "\n", "if", "targets", "is", "not", "None", ":", "\n", "# target_field = TextField([Token(target) for target in targets], self._token_indexers)", "\n", "            ", "first_wordpieces", "=", "[", "\n", "self", ".", "_targets_tokenizer", ".", "tokenize", "(", "target", ")", "[", "0", "]", "for", "target", "in", "targets", "\n", "]", "\n", "target_tokens", "=", "[", "]", "\n", "for", "wordpiece", ",", "target", "in", "zip", "(", "first_wordpieces", ",", "targets", ")", ":", "\n", "                ", "target_tokens", ".", "append", "(", "\n", "Token", "(", "\n", "text", "=", "target", ",", "\n", "text_id", "=", "wordpiece", ".", "text_id", ",", "\n", "type_id", "=", "wordpiece", ".", "type_id", ",", "\n", ")", "\n", ")", "\n", "", "fields", "[", "\"target_ids\"", "]", "=", "TextField", "(", "target_tokens", ",", "self", ".", "_token_indexers", ")", "\n", "", "return", "Instance", "(", "fields", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.dataset_readers.ccgbank.CcgBankDatasetReader.__init__": [[57, 77], ["allennlp.data.dataset_readers.dataset_reader.DatasetReader.__init__", "set", "allennlp.common.checks.ConfigurationError", "allennlp.data.token_indexers.SingleIdTokenIndexer", "allennlp.common.checks.ConfigurationError"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.graph_util.plot.GraphPlot.__init__"], ["def", "__init__", "(", "\n", "self", ",", "\n", "token_indexers", ":", "Dict", "[", "str", ",", "TokenIndexer", "]", "=", "None", ",", "\n", "tag_label", ":", "str", "=", "\"ccg\"", ",", "\n", "feature_labels", ":", "Sequence", "[", "str", "]", "=", "(", ")", ",", "\n", "label_namespace", ":", "str", "=", "\"labels\"", ",", "\n", "**", "kwargs", ",", "\n", ")", "->", "None", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "**", "kwargs", ")", "\n", "self", ".", "_token_indexers", "=", "token_indexers", "or", "{", "\"tokens\"", ":", "SingleIdTokenIndexer", "(", ")", "}", "\n", "self", ".", "tag_label", "=", "tag_label", "\n", "if", "tag_label", "is", "not", "None", "and", "tag_label", "not", "in", "_VALID_LABELS", ":", "\n", "            ", "raise", "ConfigurationError", "(", "\"unknown tag label type: {}\"", ".", "format", "(", "tag_label", ")", ")", "\n", "\n", "", "self", ".", "feature_labels", "=", "set", "(", "feature_labels", ")", "\n", "for", "label", "in", "feature_labels", ":", "\n", "            ", "if", "label", "not", "in", "_VALID_LABELS", ":", "\n", "                ", "raise", "ConfigurationError", "(", "\"unknown feature label type: {}\"", ".", "format", "(", "label", ")", ")", "\n", "\n", "", "", "self", ".", "label_namespace", "=", "label_namespace", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.dataset_readers.ccgbank.CcgBankDatasetReader._read": [[78, 109], ["allennlp.common.file_utils.cached_path", "logger.info", "open", "line.startswith", "re.findall", "zip", "list", "ccgbank.CcgBankDatasetReader.text_to_instance", "leaf.split"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.file_utils.cached_path", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.dataset_readers.multiprocess_dataset_reader.logger.info", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.coreference_resolution.conll.ConllCorefReader.text_to_instance"], ["", "@", "overrides", "\n", "def", "_read", "(", "self", ",", "file_path", ")", ":", "\n", "# if `file_path` is a URL, redirect to the cache", "\n", "        ", "file_path", "=", "cached_path", "(", "file_path", ")", "\n", "logger", ".", "info", "(", "\"Reading instances from lines in file at: %s\"", ",", "file_path", ")", "\n", "\n", "with", "open", "(", "file_path", ")", "as", "input_file", ":", "\n", "            ", "for", "line", "in", "input_file", ":", "\n", "                ", "if", "line", ".", "startswith", "(", "\"(<\"", ")", ":", "\n", "# Each leaf looks like", "\n", "# (<L ccg_category modified_pos original_pos token predicate_arg_category>)", "\n", "                    ", "leaves", "=", "re", ".", "findall", "(", "\"<L (.*?)>\"", ",", "line", ")", "\n", "\n", "# Use magic unzipping trick to split into tuples", "\n", "tuples", "=", "zip", "(", "*", "[", "leaf", ".", "split", "(", ")", "for", "leaf", "in", "leaves", "]", ")", "\n", "\n", "# Convert to lists and assign to variables.", "\n", "(", "\n", "ccg_categories", ",", "\n", "modified_pos_tags", ",", "\n", "original_pos_tags", ",", "\n", "tokens", ",", "\n", "predicate_arg_categories", ",", "\n", ")", "=", "[", "list", "(", "result", ")", "for", "result", "in", "tuples", "]", "\n", "\n", "yield", "self", ".", "text_to_instance", "(", "\n", "tokens", ",", "\n", "ccg_categories", ",", "\n", "original_pos_tags", ",", "\n", "modified_pos_tags", ",", "\n", "predicate_arg_categories", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.dataset_readers.ccgbank.CcgBankDatasetReader.text_to_instance": [[111, 211], ["allennlp.data.fields.TextField", "allennlp.data.instance.Instance", "allennlp.data.fields.SequenceLabelField", "allennlp.data.fields.SequenceLabelField", "allennlp.data.fields.SequenceLabelField", "allennlp.data.fields.SequenceLabelField", "allennlp.data.fields.SequenceLabelField", "allennlp.data.tokenizers.Token", "allennlp.common.checks.ConfigurationError", "allennlp.common.checks.ConfigurationError", "allennlp.common.checks.ConfigurationError", "allennlp.common.checks.ConfigurationError", "allennlp.data.fields.SequenceLabelField", "allennlp.data.fields.SequenceLabelField", "allennlp.data.fields.SequenceLabelField"], "methods", ["None"], ["", "", "", "", "@", "overrides", "\n", "def", "text_to_instance", "(", "\n", "self", ",", "# type: ignore", "\n", "tokens", ":", "List", "[", "str", "]", ",", "\n", "ccg_categories", ":", "List", "[", "str", "]", "=", "None", ",", "\n", "original_pos_tags", ":", "List", "[", "str", "]", "=", "None", ",", "\n", "modified_pos_tags", ":", "List", "[", "str", "]", "=", "None", ",", "\n", "predicate_arg_categories", ":", "List", "[", "str", "]", "=", "None", ",", "\n", ")", "->", "Instance", ":", "\n", "        ", "\"\"\"\n        We take `pre-tokenized` input here, because we don't have a tokenizer in this class.\n\n        # Parameters\n\n        tokens : `List[str]`, required.\n            The tokens in a given sentence.\n        ccg_categories : `List[str]`, optional, (default = None).\n            The CCG categories for the words in the sentence. (e.g. N/N)\n        original_pos_tags : `List[str]`, optional, (default = None).\n            The tag assigned to the word in the Penn Treebank.\n        modified_pos_tags : `List[str]`, optional, (default = None).\n            The POS tag might have changed during the translation to CCG.\n        predicate_arg_categories : `List[str]`, optional, (default = None).\n            Encodes the word-word dependencies in the underlying predicate-\n            argument structure.\n\n        # Returns\n\n        An `Instance` containing the following fields:\n            tokens : `TextField`\n                The tokens in the sentence.\n            tags : `SequenceLabelField`\n                The tags corresponding to the `tag_label` constructor argument.\n            feature_label_tags : `SequenceLabelField`\n                Tags corresponding to each feature_label (if any) specified in the\n                `feature_labels` constructor argument.\n        \"\"\"", "\n", "\n", "text_field", "=", "TextField", "(", "\n", "[", "Token", "(", "x", ")", "for", "x", "in", "tokens", "]", ",", "token_indexers", "=", "self", ".", "_token_indexers", "\n", ")", "\n", "fields", ":", "Dict", "[", "str", ",", "Field", "]", "=", "{", "\"tokens\"", ":", "text_field", "}", "\n", "\n", "# Add \"feature labels\" to instance", "\n", "if", "\"ccg\"", "in", "self", ".", "feature_labels", ":", "\n", "            ", "if", "ccg_categories", "is", "None", ":", "\n", "                ", "raise", "ConfigurationError", "(", "\n", "\"Dataset reader was specified to use CCG categories as \"", "\n", "\"features. Pass them to text_to_instance.\"", "\n", ")", "\n", "", "fields", "[", "\"ccg_tags\"", "]", "=", "SequenceLabelField", "(", "\n", "ccg_categories", ",", "text_field", ",", "\"ccg_tags\"", "\n", ")", "\n", "", "if", "\"original_pos\"", "in", "self", ".", "feature_labels", ":", "\n", "            ", "if", "original_pos_tags", "is", "None", ":", "\n", "                ", "raise", "ConfigurationError", "(", "\n", "\"Dataset reader was specified to use original POS tags as \"", "\n", "\"features. Pass them to text_to_instance.\"", "\n", ")", "\n", "", "fields", "[", "\"original_pos_tags\"", "]", "=", "SequenceLabelField", "(", "\n", "original_pos_tags", ",", "text_field", ",", "\"original_pos_tags\"", "\n", ")", "\n", "", "if", "\"modified_pos\"", "in", "self", ".", "feature_labels", ":", "\n", "            ", "if", "modified_pos_tags", "is", "None", ":", "\n", "                ", "raise", "ConfigurationError", "(", "\n", "\"Dataset reader was specified to use modified POS tags as \"", "\n", "\" features. Pass them to text_to_instance.\"", "\n", ")", "\n", "", "fields", "[", "\"modified_pos_tags\"", "]", "=", "SequenceLabelField", "(", "\n", "modified_pos_tags", ",", "text_field", ",", "\"modified_pos_tags\"", "\n", ")", "\n", "", "if", "\"predicate_arg\"", "in", "self", ".", "feature_labels", ":", "\n", "            ", "if", "predicate_arg_categories", "is", "None", ":", "\n", "                ", "raise", "ConfigurationError", "(", "\n", "\"Dataset reader was specified to use predicate arg tags as \"", "\n", "\" features. Pass them to text_to_instance.\"", "\n", ")", "\n", "", "fields", "[", "\"predicate_arg_tags\"", "]", "=", "SequenceLabelField", "(", "\n", "predicate_arg_categories", ",", "text_field", ",", "\"predicate_arg_tags\"", "\n", ")", "\n", "\n", "# Add \"tag label\" to instance", "\n", "", "if", "self", ".", "tag_label", "==", "\"ccg\"", "and", "ccg_categories", "is", "not", "None", ":", "\n", "            ", "fields", "[", "\"tags\"", "]", "=", "SequenceLabelField", "(", "\n", "ccg_categories", ",", "text_field", ",", "self", ".", "label_namespace", "\n", ")", "\n", "", "elif", "self", ".", "tag_label", "==", "\"original_pos\"", "and", "original_pos_tags", "is", "not", "None", ":", "\n", "            ", "fields", "[", "\"tags\"", "]", "=", "SequenceLabelField", "(", "\n", "original_pos_tags", ",", "text_field", ",", "self", ".", "label_namespace", "\n", ")", "\n", "", "elif", "self", ".", "tag_label", "==", "\"modified_pos\"", "and", "modified_pos_tags", "is", "not", "None", ":", "\n", "            ", "fields", "[", "\"tags\"", "]", "=", "SequenceLabelField", "(", "\n", "modified_pos_tags", ",", "text_field", ",", "self", ".", "label_namespace", "\n", ")", "\n", "", "elif", "self", ".", "tag_label", "==", "\"predicate_arg\"", "and", "predicate_arg_categories", "is", "not", "None", ":", "\n", "            ", "fields", "[", "\"tags\"", "]", "=", "SequenceLabelField", "(", "\n", "predicate_arg_categories", ",", "text_field", ",", "self", ".", "label_namespace", "\n", ")", "\n", "\n", "", "return", "Instance", "(", "fields", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.dataset_readers.multiprocess_dataset_reader.logger.info": [[24, 32], ["cls._logger.info", "torch.multiprocessing.log_to_stderr", "cls._logger.setLevel"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.dataset_readers.multiprocess_dataset_reader.logger.info"], ["@", "classmethod", "\n", "def", "info", "(", "cls", ",", "message", ":", "str", ")", "->", "None", ":", "\n", "\n", "        ", "if", "cls", ".", "_logger", "is", "None", ":", "\n", "            ", "cls", ".", "_logger", "=", "log_to_stderr", "(", ")", "\n", "cls", ".", "_logger", ".", "setLevel", "(", "logging", ".", "INFO", ")", "\n", "\n", "", "cls", ".", "_logger", ".", "info", "(", "message", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.dataset_readers.multiprocess_dataset_reader.QIterable.__init__": [[83, 99], ["torch.multiprocessing.Queue"], "methods", ["None"], ["def", "__init__", "(", "\n", "self", ",", "output_queue_size", ",", "epochs_per_read", ",", "num_workers", ",", "reader", ",", "file_path", "\n", ")", "->", "None", ":", "\n", "        ", "self", ".", "output_queue", "=", "Queue", "(", "output_queue_size", ")", "\n", "self", ".", "epochs_per_read", "=", "epochs_per_read", "\n", "self", ".", "num_workers", "=", "num_workers", "\n", "self", ".", "reader", "=", "reader", "\n", "self", ".", "file_path", "=", "file_path", "\n", "\n", "# Initialized in start.", "\n", "self", ".", "input_queue", ":", "Optional", "[", "Queue", "]", "=", "None", "\n", "self", ".", "processes", ":", "List", "[", "Process", "]", "=", "[", "]", "\n", "# The num_active_workers and num_inflight_items counts in conjunction", "\n", "# determine whether there could be any outstanding instances.", "\n", "self", ".", "num_active_workers", ":", "Optional", "[", "Value", "]", "=", "None", "\n", "self", ".", "num_inflight_items", ":", "Optional", "[", "Value", "]", "=", "None", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.dataset_readers.multiprocess_dataset_reader.QIterable.__iter__": [[100, 120], ["multiprocess_dataset_reader.QIterable.start", "multiprocess_dataset_reader.QIterable.join", "multiprocess_dataset_reader.QIterable.output_queue.get", "multiprocess_dataset_reader.QIterable.num_inflight_items.get_lock"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.dataset_readers.multiprocess_dataset_reader.QIterable.start", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.dataset_readers.multiprocess_dataset_reader.QIterable.join", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.params.Params.get"], ["", "def", "__iter__", "(", "self", ")", "->", "Iterator", "[", "Instance", "]", ":", "\n", "        ", "self", ".", "start", "(", ")", "\n", "\n", "# Keep going as long as not all the workers have finished or there are items in flight.", "\n", "while", "self", ".", "num_active_workers", ".", "value", ">", "0", "or", "self", ".", "num_inflight_items", ".", "value", ">", "0", ":", "\n", "# Inner loop to minimize locking on self.num_active_workers.", "\n", "            ", "while", "True", ":", "\n", "                ", "try", ":", "\n", "# Non-blocking to handle the empty-queue case.", "\n", "                    ", "yield", "self", ".", "output_queue", ".", "get", "(", "block", "=", "False", ",", "timeout", "=", "1.0", ")", "\n", "with", "self", ".", "num_inflight_items", ".", "get_lock", "(", ")", ":", "\n", "                        ", "self", ".", "num_inflight_items", ".", "value", "-=", "1", "\n", "", "", "except", "Empty", ":", "\n", "# The queue could be empty because the workers are", "\n", "# all finished or because they're busy processing.", "\n", "# The outer loop distinguishes between these two", "\n", "# cases.", "\n", "                    ", "break", "\n", "\n", "", "", "", "self", ".", "join", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.dataset_readers.multiprocess_dataset_reader.QIterable.start": [[121, 158], ["glob.glob", "glob.glob.sort", "len", "torch.multiprocessing.Queue", "range", "range", "torch.multiprocessing.Value", "torch.multiprocessing.Value", "range", "numpy.random.shuffle", "multiprocess_dataset_reader.QIterable.input_queue.put", "torch.multiprocessing.Process", "multiprocess_dataset_reader.logger.info", "torch.multiprocessing.Process.start", "multiprocess_dataset_reader.QIterable.processes.append", "multiprocess_dataset_reader.QIterable.input_queue.put"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.dataset_readers.multiprocess_dataset_reader.logger.info", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.dataset_readers.multiprocess_dataset_reader.QIterable.start"], ["", "def", "start", "(", "self", ")", "->", "None", ":", "\n", "        ", "shards", "=", "glob", ".", "glob", "(", "self", ".", "file_path", ")", "\n", "# Ensure a consistent order before shuffling for testing.", "\n", "shards", ".", "sort", "(", ")", "\n", "num_shards", "=", "len", "(", "shards", ")", "\n", "\n", "# If we want multiple epochs per read, put shards in the queue multiple times.", "\n", "self", ".", "input_queue", "=", "Queue", "(", "num_shards", "*", "self", ".", "epochs_per_read", "+", "self", ".", "num_workers", ")", "\n", "for", "_", "in", "range", "(", "self", ".", "epochs_per_read", ")", ":", "\n", "            ", "np", ".", "random", ".", "shuffle", "(", "shards", ")", "\n", "for", "shard", "in", "shards", ":", "\n", "                ", "self", ".", "input_queue", ".", "put", "(", "shard", ")", "\n", "\n", "# Then put a None per worker to signify no more files.", "\n", "", "", "for", "_", "in", "range", "(", "self", ".", "num_workers", ")", ":", "\n", "            ", "self", ".", "input_queue", ".", "put", "(", "None", ")", "\n", "\n", "", "assert", "(", "\n", "not", "self", ".", "processes", "\n", ")", ",", "\"Process list non-empty! You must call QIterable.join() before restarting.\"", "\n", "self", ".", "num_active_workers", "=", "Value", "(", "\"i\"", ",", "self", ".", "num_workers", ")", "\n", "self", ".", "num_inflight_items", "=", "Value", "(", "\"i\"", ",", "0", ")", "\n", "for", "worker_id", "in", "range", "(", "self", ".", "num_workers", ")", ":", "\n", "            ", "process", "=", "Process", "(", "\n", "target", "=", "_worker", ",", "\n", "args", "=", "(", "\n", "self", ".", "reader", ",", "\n", "self", ".", "input_queue", ",", "\n", "self", ".", "output_queue", ",", "\n", "self", ".", "num_active_workers", ",", "\n", "self", ".", "num_inflight_items", ",", "\n", "worker_id", ",", "\n", ")", ",", "\n", ")", "\n", "logger", ".", "info", "(", "f\"starting worker {worker_id}\"", ")", "\n", "process", ".", "start", "(", ")", "\n", "self", ".", "processes", ".", "append", "(", "process", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.dataset_readers.multiprocess_dataset_reader.QIterable.join": [[159, 163], ["multiprocess_dataset_reader.QIterable.processes.clear", "process.join"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.training.metric_tracker.MetricTracker.clear", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.dataset_readers.multiprocess_dataset_reader.QIterable.join"], ["", "", "def", "join", "(", "self", ")", "->", "None", ":", "\n", "        ", "for", "process", "in", "self", ".", "processes", ":", "\n", "            ", "process", ".", "join", "(", ")", "\n", "", "self", ".", "processes", ".", "clear", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.dataset_readers.multiprocess_dataset_reader.QIterable.__del__": [[164, 177], ["process.terminate"], "methods", ["None"], ["", "def", "__del__", "(", "self", ")", "->", "None", ":", "\n", "        ", "\"\"\"\n        Terminate processes if the user hasn't joined. This is necessary as\n        leaving stray processes running can corrupt shared state. In brief,\n        we've observed shared memory counters being reused (when the memory was\n        free from the perspective of the parent process) while the stray\n        workers still held a reference to them.\n\n        For a discussion of using destructors in Python in this manner, see\n        https://eli.thegreenplace.net/2009/06/12/safely-using-destructors-in-python/.\n        \"\"\"", "\n", "for", "process", "in", "self", ".", "processes", ":", "\n", "            ", "process", ".", "terminate", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.dataset_readers.multiprocess_dataset_reader.MultiprocessDatasetReader.__init__": [[207, 223], ["allennlp.data.dataset_readers.dataset_reader.DatasetReader.__init__"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.graph_util.plot.GraphPlot.__init__"], ["def", "__init__", "(", "\n", "self", ",", "\n", "base_reader", ":", "DatasetReader", ",", "\n", "num_workers", ":", "int", ",", "\n", "epochs_per_read", ":", "int", "=", "1", ",", "\n", "output_queue_size", ":", "int", "=", "1000", ",", "\n", "**", "kwargs", ",", "\n", ")", "->", "None", ":", "\n", "# Multiprocess reader is intrinsically lazy.", "\n", "        ", "kwargs", "[", "\"lazy\"", "]", "=", "True", "\n", "super", "(", ")", ".", "__init__", "(", "**", "kwargs", ")", "\n", "\n", "self", ".", "reader", "=", "base_reader", "\n", "self", ".", "num_workers", "=", "num_workers", "\n", "self", ".", "epochs_per_read", "=", "epochs_per_read", "\n", "self", ".", "output_queue_size", "=", "output_queue_size", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.dataset_readers.multiprocess_dataset_reader.MultiprocessDatasetReader.text_to_instance": [[224, 229], ["multiprocess_dataset_reader.MultiprocessDatasetReader.reader.text_to_instance"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.coreference_resolution.conll.ConllCorefReader.text_to_instance"], ["", "def", "text_to_instance", "(", "self", ",", "*", "args", ",", "**", "kwargs", ")", "->", "Instance", ":", "\n", "        ", "\"\"\"\n        Just delegate to the base reader text_to_instance.\n        \"\"\"", "\n", "return", "self", ".", "reader", ".", "text_to_instance", "(", "*", "args", ",", "**", "kwargs", ")", "# type: ignore", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.dataset_readers.multiprocess_dataset_reader.MultiprocessDatasetReader._read": [[230, 232], ["RuntimeError"], "methods", ["None"], ["", "def", "_read", "(", "self", ",", "file_path", ":", "str", ")", "->", "Iterable", "[", "Instance", "]", ":", "\n", "        ", "raise", "RuntimeError", "(", "\"Multiprocess reader implements read() directly.\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.dataset_readers.multiprocess_dataset_reader.MultiprocessDatasetReader.read": [[233, 240], ["multiprocess_dataset_reader.QIterable"], "methods", ["None"], ["", "def", "read", "(", "self", ",", "file_path", ":", "str", ")", "->", "Iterable", "[", "Instance", "]", ":", "\n", "        ", "return", "QIterable", "(", "\n", "output_queue_size", "=", "self", ".", "output_queue_size", ",", "\n", "epochs_per_read", "=", "self", ".", "epochs_per_read", ",", "\n", "num_workers", "=", "self", ".", "num_workers", ",", "\n", "reader", "=", "self", ".", "reader", ",", "\n", "file_path", "=", "file_path", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.dataset_readers.multiprocess_dataset_reader._worker": [[34, 75], ["multiprocess_dataset_reader.logger.info", "input_queue.get", "multiprocess_dataset_reader.logger.info", "reader.read", "output_queue.close", "output_queue.join_thread", "multiprocess_dataset_reader.logger.info", "output_queue.put", "os.getpid", "num_active_workers.get_lock", "num_inflight_items.get_lock"], "function", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.dataset_readers.multiprocess_dataset_reader.logger.info", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.params.Params.get", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.dataset_readers.multiprocess_dataset_reader.logger.info", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.scripts.convert_openie_to_conll.read", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.token_embedders.embedding.EmbeddingsTextFile.close", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.dataset_readers.multiprocess_dataset_reader.logger.info"], ["", "", "def", "_worker", "(", "\n", "reader", ":", "DatasetReader", ",", "\n", "input_queue", ":", "Queue", ",", "\n", "output_queue", ":", "Queue", ",", "\n", "num_active_workers", ":", "Value", ",", "\n", "num_inflight_items", ":", "Value", ",", "\n", "worker_id", ":", "int", ",", "\n", ")", "->", "None", ":", "\n", "    ", "\"\"\"\n    A worker that pulls filenames off the input queue, uses the dataset reader\n    to read them, and places the generated instances on the output queue.  When\n    there are no filenames left on the input queue, it decrements\n    num_active_workers to signal completion.\n    \"\"\"", "\n", "logger", ".", "info", "(", "f\"Reader worker: {worker_id} PID: {os.getpid()}\"", ")", "\n", "# Keep going until you get a file_path that's None.", "\n", "while", "True", ":", "\n", "        ", "file_path", "=", "input_queue", ".", "get", "(", ")", "\n", "if", "file_path", "is", "None", ":", "\n", "# It's important that we close and join the queue here before", "\n", "# decrementing num_active_workers. Otherwise our parent may join us", "\n", "# before the queue's feeder thread has passed all buffered items to", "\n", "# the underlying pipe resulting in a deadlock.", "\n", "#", "\n", "# See:", "\n", "# https://docs.python.org/3.6/library/multiprocessing.html?highlight=process#pipes-and-queues", "\n", "# https://docs.python.org/3.6/library/multiprocessing.html?highlight=process#programming-guidelines", "\n", "            ", "output_queue", ".", "close", "(", ")", "\n", "output_queue", ".", "join_thread", "(", ")", "\n", "# Decrementing is not atomic.", "\n", "# See https://docs.python.org/2/library/multiprocessing.html#multiprocessing.Value.", "\n", "with", "num_active_workers", ".", "get_lock", "(", ")", ":", "\n", "                ", "num_active_workers", ".", "value", "-=", "1", "\n", "", "logger", ".", "info", "(", "f\"Reader worker {worker_id} finished\"", ")", "\n", "break", "\n", "\n", "", "logger", ".", "info", "(", "f\"reading instances from {file_path}\"", ")", "\n", "for", "instance", "in", "reader", ".", "read", "(", "file_path", ")", ":", "\n", "            ", "with", "num_inflight_items", ".", "get_lock", "(", ")", ":", "\n", "                ", "num_inflight_items", ".", "value", "+=", "1", "\n", "", "output_queue", ".", "put", "(", "instance", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.dataset_readers.universal_dependencies.UniversalDependenciesDatasetReader.__init__": [[34, 45], ["allennlp.data.dataset_readers.dataset_reader.DatasetReader.__init__", "allennlp.data.token_indexers.SingleIdTokenIndexer"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.graph_util.plot.GraphPlot.__init__"], ["def", "__init__", "(", "\n", "self", ",", "\n", "token_indexers", ":", "Dict", "[", "str", ",", "TokenIndexer", "]", "=", "None", ",", "\n", "use_language_specific_pos", ":", "bool", "=", "False", ",", "\n", "tokenizer", ":", "Tokenizer", "=", "None", ",", "\n", "**", "kwargs", ",", "\n", ")", "->", "None", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "**", "kwargs", ")", "\n", "self", ".", "_token_indexers", "=", "token_indexers", "or", "{", "\"tokens\"", ":", "SingleIdTokenIndexer", "(", ")", "}", "\n", "self", ".", "use_language_specific_pos", "=", "use_language_specific_pos", "\n", "self", ".", "tokenizer", "=", "tokenizer", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.dataset_readers.universal_dependencies.UniversalDependenciesDatasetReader._read": [[46, 70], ["allennlp.common.file_utils.cached_path", "open", "logger.info", "conllu.parse_incr", "universal_dependencies.UniversalDependenciesDatasetReader.text_to_instance", "isinstance", "list", "zip"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.file_utils.cached_path", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.dataset_readers.multiprocess_dataset_reader.logger.info", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.coreference_resolution.conll.ConllCorefReader.text_to_instance"], ["", "@", "overrides", "\n", "def", "_read", "(", "self", ",", "file_path", ":", "str", ")", ":", "\n", "# if `file_path` is a URL, redirect to the cache", "\n", "        ", "file_path", "=", "cached_path", "(", "file_path", ")", "\n", "\n", "with", "open", "(", "file_path", ",", "\"r\"", ")", "as", "conllu_file", ":", "\n", "            ", "logger", ".", "info", "(", "\"Reading UD instances from conllu dataset at: %s\"", ",", "file_path", ")", "\n", "\n", "for", "annotation", "in", "parse_incr", "(", "conllu_file", ")", ":", "\n", "# CoNLLU annotations sometimes add back in words that have been elided", "\n", "# in the original sentence; we remove these, as we're just predicting", "\n", "# dependencies for the original sentence.", "\n", "# We filter by integers here as elided words have a non-integer word id,", "\n", "# as parsed by the conllu python library.", "\n", "                ", "annotation", "=", "[", "x", "for", "x", "in", "annotation", "if", "isinstance", "(", "x", "[", "\"id\"", "]", ",", "int", ")", "]", "\n", "\n", "heads", "=", "[", "x", "[", "\"head\"", "]", "for", "x", "in", "annotation", "]", "\n", "tags", "=", "[", "x", "[", "\"deprel\"", "]", "for", "x", "in", "annotation", "]", "\n", "words", "=", "[", "x", "[", "\"form\"", "]", "for", "x", "in", "annotation", "]", "\n", "if", "self", ".", "use_language_specific_pos", ":", "\n", "                    ", "pos_tags", "=", "[", "x", "[", "\"xpostag\"", "]", "for", "x", "in", "annotation", "]", "\n", "", "else", ":", "\n", "                    ", "pos_tags", "=", "[", "x", "[", "\"upostag\"", "]", "for", "x", "in", "annotation", "]", "\n", "", "yield", "self", ".", "text_to_instance", "(", "words", ",", "pos_tags", ",", "list", "(", "zip", "(", "tags", ",", "heads", ")", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.dataset_readers.universal_dependencies.UniversalDependenciesDatasetReader.text_to_instance": [[71, 122], ["allennlp.data.fields.TextField", "allennlp.data.fields.SequenceLabelField", "allennlp.data.fields.MetadataField", "allennlp.data.instance.Instance", "universal_dependencies.UniversalDependenciesDatasetReader.tokenizer.tokenize", "allennlp.data.fields.SequenceLabelField", "allennlp.data.fields.SequenceLabelField", "allennlp.data.tokenizers.Token"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.tokenizers.whitespace_tokenizer.WhitespaceTokenizer.tokenize"], ["", "", "", "@", "overrides", "\n", "def", "text_to_instance", "(", "\n", "self", ",", "# type: ignore", "\n", "words", ":", "List", "[", "str", "]", ",", "\n", "upos_tags", ":", "List", "[", "str", "]", ",", "\n", "dependencies", ":", "List", "[", "Tuple", "[", "str", ",", "int", "]", "]", "=", "None", ",", "\n", ")", "->", "Instance", ":", "\n", "\n", "        ", "\"\"\"\n        # Parameters\n\n        words : `List[str]`, required.\n            The words in the sentence to be encoded.\n        upos_tags : `List[str]`, required.\n            The universal dependencies POS tags for each word.\n        dependencies : `List[Tuple[str, int]]`, optional (default = None)\n            A list of  (head tag, head index) tuples. Indices are 1 indexed,\n            meaning an index of 0 corresponds to that word being the root of\n            the dependency tree.\n\n        # Returns\n\n        An instance containing words, upos tags, dependency head tags and head\n        indices as fields.\n        \"\"\"", "\n", "fields", ":", "Dict", "[", "str", ",", "Field", "]", "=", "{", "}", "\n", "\n", "if", "self", ".", "tokenizer", "is", "not", "None", ":", "\n", "            ", "tokens", "=", "self", ".", "tokenizer", ".", "tokenize", "(", "\" \"", ".", "join", "(", "words", ")", ")", "\n", "", "else", ":", "\n", "            ", "tokens", "=", "[", "Token", "(", "t", ")", "for", "t", "in", "words", "]", "\n", "\n", "", "text_field", "=", "TextField", "(", "tokens", ",", "self", ".", "_token_indexers", ")", "\n", "fields", "[", "\"words\"", "]", "=", "text_field", "\n", "fields", "[", "\"pos_tags\"", "]", "=", "SequenceLabelField", "(", "\n", "upos_tags", ",", "text_field", ",", "label_namespace", "=", "\"pos\"", "\n", ")", "\n", "if", "dependencies", "is", "not", "None", ":", "\n", "# We don't want to expand the label namespace with an additional dummy token, so we'll", "\n", "# always give the 'ROOT_HEAD' token a label of 'root'.", "\n", "            ", "fields", "[", "\"head_tags\"", "]", "=", "SequenceLabelField", "(", "\n", "[", "x", "[", "0", "]", "for", "x", "in", "dependencies", "]", ",", "text_field", ",", "label_namespace", "=", "\"head_tags\"", "\n", ")", "\n", "fields", "[", "\"head_indices\"", "]", "=", "SequenceLabelField", "(", "\n", "[", "x", "[", "1", "]", "for", "x", "in", "dependencies", "]", ",", "\n", "text_field", ",", "\n", "label_namespace", "=", "\"head_index_tags\"", ",", "\n", ")", "\n", "\n", "", "fields", "[", "\"metadata\"", "]", "=", "MetadataField", "(", "{", "\"words\"", ":", "words", ",", "\"pos\"", ":", "upos_tags", "}", ")", "\n", "return", "Instance", "(", "fields", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.dataset_readers.stanford_sentiment_tree_bank.StanfordSentimentTreeBankDatasetReader.__init__": [[51, 69], ["allennlp.data.dataset_readers.dataset_reader.DatasetReader.__init__", "allennlp.common.checks.ConfigurationError", "allennlp.data.token_indexers.SingleIdTokenIndexer"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.graph_util.plot.GraphPlot.__init__"], ["def", "__init__", "(", "\n", "self", ",", "\n", "token_indexers", ":", "Dict", "[", "str", ",", "TokenIndexer", "]", "=", "None", ",", "\n", "use_subtrees", ":", "bool", "=", "False", ",", "\n", "granularity", ":", "str", "=", "\"5-class\"", ",", "\n", "**", "kwargs", ",", "\n", ")", "->", "None", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "**", "kwargs", ")", "\n", "self", ".", "_token_indexers", "=", "token_indexers", "or", "{", "\"tokens\"", ":", "SingleIdTokenIndexer", "(", ")", "}", "\n", "self", ".", "_use_subtrees", "=", "use_subtrees", "\n", "allowed_granularities", "=", "[", "\"5-class\"", ",", "\"3-class\"", ",", "\"2-class\"", "]", "\n", "if", "granularity", "not", "in", "allowed_granularities", ":", "\n", "            ", "raise", "ConfigurationError", "(", "\n", "\"granularity is {}, but expected one of: {}\"", ".", "format", "(", "\n", "granularity", ",", "allowed_granularities", "\n", ")", "\n", ")", "\n", "", "self", ".", "_granularity", "=", "granularity", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.dataset_readers.stanford_sentiment_tree_bank.StanfordSentimentTreeBankDatasetReader._read": [[70, 92], ["open", "logger.info", "data_file.readlines", "allennlp.common.file_utils.cached_path", "line.strip.strip.strip", "nltk.tree.Tree.fromstring", "nltk.tree.Tree.fromstring.subtrees", "stanford_sentiment_tree_bank.StanfordSentimentTreeBankDatasetReader.text_to_instance", "stanford_sentiment_tree_bank.StanfordSentimentTreeBankDatasetReader.text_to_instance", "nltk.tree.Tree.fromstring.leaves", "nltk.tree.Tree.fromstring.label", "subtree.leaves", "subtree.label"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.dataset_readers.multiprocess_dataset_reader.logger.info", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.file_utils.cached_path", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.coreference_resolution.conll.ConllCorefReader.text_to_instance", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.coreference_resolution.conll.ConllCorefReader.text_to_instance"], ["", "@", "overrides", "\n", "def", "_read", "(", "self", ",", "file_path", ")", ":", "\n", "        ", "with", "open", "(", "cached_path", "(", "file_path", ")", ",", "\"r\"", ")", "as", "data_file", ":", "\n", "            ", "logger", ".", "info", "(", "\"Reading instances from lines in file at: %s\"", ",", "file_path", ")", "\n", "for", "line", "in", "data_file", ".", "readlines", "(", ")", ":", "\n", "                ", "line", "=", "line", ".", "strip", "(", "\"\\n\"", ")", "\n", "if", "not", "line", ":", "\n", "                    ", "continue", "\n", "", "parsed_line", "=", "Tree", ".", "fromstring", "(", "line", ")", "\n", "if", "self", ".", "_use_subtrees", ":", "\n", "                    ", "for", "subtree", "in", "parsed_line", ".", "subtrees", "(", ")", ":", "\n", "                        ", "instance", "=", "self", ".", "text_to_instance", "(", "\n", "subtree", ".", "leaves", "(", ")", ",", "subtree", ".", "label", "(", ")", "\n", ")", "\n", "if", "instance", "is", "not", "None", ":", "\n", "                            ", "yield", "instance", "\n", "", "", "", "else", ":", "\n", "                    ", "instance", "=", "self", ".", "text_to_instance", "(", "\n", "parsed_line", ".", "leaves", "(", ")", ",", "parsed_line", ".", "label", "(", ")", "\n", ")", "\n", "if", "instance", "is", "not", "None", ":", "\n", "                        ", "yield", "instance", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.dataset_readers.stanford_sentiment_tree_bank.StanfordSentimentTreeBankDatasetReader.text_to_instance": [[93, 143], ["allennlp.data.fields.TextField", "allennlp.data.instance.Instance", "allennlp.data.fields.LabelField", "allennlp.data.tokenizers.Token", "int", "int", "int", "int"], "methods", ["None"], ["", "", "", "", "", "@", "overrides", "\n", "def", "text_to_instance", "(", "\n", "self", ",", "tokens", ":", "List", "[", "str", "]", ",", "sentiment", ":", "str", "=", "None", "\n", ")", "->", "Instance", ":", "# type: ignore", "\n", "        ", "\"\"\"\n        We take `pre-tokenized` input here, because we don't have a tokenizer in this class.\n\n        # Parameters\n\n        tokens : `List[str]`, required.\n            The tokens in a given sentence.\n        sentiment : `str`, optional, (default = None).\n            The sentiment for this sentence.\n\n        # Returns\n\n        An `Instance` containing the following fields:\n            tokens : `TextField`\n                The tokens in the sentence or phrase.\n            label : `LabelField`\n                The sentiment label of the sentence or phrase.\n        \"\"\"", "\n", "\n", "text_field", "=", "TextField", "(", "\n", "[", "Token", "(", "x", ")", "for", "x", "in", "tokens", "]", ",", "token_indexers", "=", "self", ".", "_token_indexers", "\n", ")", "\n", "fields", ":", "Dict", "[", "str", ",", "Field", "]", "=", "{", "\"tokens\"", ":", "text_field", "}", "\n", "if", "sentiment", "is", "not", "None", ":", "\n", "# 0 and 1 are negative sentiment, 2 is neutral, and 3 and 4 are positive sentiment", "\n", "# In 5-class, we use labels as is.", "\n", "# 3-class reduces the granularity, and only asks the model to predict", "\n", "# negative, neutral, or positive.", "\n", "# 2-class further reduces the granularity by only asking the model to", "\n", "# predict whether an instance is negative or positive.", "\n", "            ", "if", "self", ".", "_granularity", "==", "\"3-class\"", ":", "\n", "                ", "if", "int", "(", "sentiment", ")", "<", "2", ":", "\n", "                    ", "sentiment", "=", "\"0\"", "\n", "", "elif", "int", "(", "sentiment", ")", "==", "2", ":", "\n", "                    ", "sentiment", "=", "\"1\"", "\n", "", "else", ":", "\n", "                    ", "sentiment", "=", "\"2\"", "\n", "", "", "elif", "self", ".", "_granularity", "==", "\"2-class\"", ":", "\n", "                ", "if", "int", "(", "sentiment", ")", "<", "2", ":", "\n", "                    ", "sentiment", "=", "\"0\"", "\n", "", "elif", "int", "(", "sentiment", ")", "==", "2", ":", "\n", "                    ", "return", "None", "\n", "", "else", ":", "\n", "                    ", "sentiment", "=", "\"1\"", "\n", "", "", "fields", "[", "\"label\"", "]", "=", "LabelField", "(", "sentiment", ")", "\n", "", "return", "Instance", "(", "fields", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.dataset_readers.semantic_dependency_parsing.SemanticDependenciesDatasetReader.__init__": [[83, 88], ["allennlp.data.dataset_readers.dataset_reader.DatasetReader.__init__", "allennlp.data.token_indexers.SingleIdTokenIndexer"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.graph_util.plot.GraphPlot.__init__"], ["def", "__init__", "(", "\n", "self", ",", "token_indexers", ":", "Dict", "[", "str", ",", "TokenIndexer", "]", "=", "None", ",", "**", "kwargs", "\n", ")", "->", "None", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "**", "kwargs", ")", "\n", "self", ".", "_token_indexers", "=", "token_indexers", "or", "{", "\"tokens\"", ":", "SingleIdTokenIndexer", "(", ")", "}", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.dataset_readers.semantic_dependency_parsing.SemanticDependenciesDatasetReader._read": [[89, 107], ["allennlp.common.file_utils.cached_path", "logger.info", "open", "semantic_dependency_parsing.lazy_parse", "sdp_file.read", "semantic_dependency_parsing.SemanticDependenciesDatasetReader.text_to_instance"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.file_utils.cached_path", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.dataset_readers.multiprocess_dataset_reader.logger.info", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.dataset_readers.semantic_dependency_parsing.lazy_parse", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.scripts.convert_openie_to_conll.read", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.coreference_resolution.conll.ConllCorefReader.text_to_instance"], ["", "@", "overrides", "\n", "def", "_read", "(", "self", ",", "file_path", ":", "str", ")", ":", "\n", "# if `file_path` is a URL, redirect to the cache", "\n", "        ", "file_path", "=", "cached_path", "(", "file_path", ")", "\n", "\n", "logger", ".", "info", "(", "\"Reading semantic dependency parsing data from: %s\"", ",", "file_path", ")", "\n", "\n", "with", "open", "(", "file_path", ")", "as", "sdp_file", ":", "\n", "            ", "for", "annotated_sentence", ",", "directed_arc_indices", ",", "arc_tags", "in", "lazy_parse", "(", "\n", "sdp_file", ".", "read", "(", ")", "\n", ")", ":", "\n", "# If there are no arc indices, skip this instance.", "\n", "                ", "if", "not", "directed_arc_indices", ":", "\n", "                    ", "continue", "\n", "", "tokens", "=", "[", "word", "[", "\"form\"", "]", "for", "word", "in", "annotated_sentence", "]", "\n", "pos_tags", "=", "[", "word", "[", "\"pos\"", "]", "for", "word", "in", "annotated_sentence", "]", "\n", "yield", "self", ".", "text_to_instance", "(", "\n", "tokens", ",", "pos_tags", ",", "directed_arc_indices", ",", "arc_tags", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.dataset_readers.semantic_dependency_parsing.SemanticDependenciesDatasetReader.text_to_instance": [[109, 130], ["allennlp.data.fields.TextField", "allennlp.data.fields.MetadataField", "allennlp.data.instance.Instance", "allennlp.data.fields.SequenceLabelField", "allennlp.data.fields.AdjacencyField", "allennlp.data.tokenizers.Token"], "methods", ["None"], ["", "", "", "@", "overrides", "\n", "def", "text_to_instance", "(", "\n", "self", ",", "# type: ignore", "\n", "tokens", ":", "List", "[", "str", "]", ",", "\n", "pos_tags", ":", "List", "[", "str", "]", "=", "None", ",", "\n", "arc_indices", ":", "List", "[", "Tuple", "[", "int", ",", "int", "]", "]", "=", "None", ",", "\n", "arc_tags", ":", "List", "[", "str", "]", "=", "None", ",", "\n", ")", "->", "Instance", ":", "\n", "\n", "        ", "fields", ":", "Dict", "[", "str", ",", "Field", "]", "=", "{", "}", "\n", "token_field", "=", "TextField", "(", "[", "Token", "(", "t", ")", "for", "t", "in", "tokens", "]", ",", "self", ".", "_token_indexers", ")", "\n", "fields", "[", "\"tokens\"", "]", "=", "token_field", "\n", "fields", "[", "\"metadata\"", "]", "=", "MetadataField", "(", "{", "\"tokens\"", ":", "tokens", "}", ")", "\n", "if", "pos_tags", "is", "not", "None", ":", "\n", "            ", "fields", "[", "\"pos_tags\"", "]", "=", "SequenceLabelField", "(", "\n", "pos_tags", ",", "token_field", ",", "label_namespace", "=", "\"pos\"", "\n", ")", "\n", "", "if", "arc_indices", "is", "not", "None", "and", "arc_tags", "is", "not", "None", ":", "\n", "            ", "fields", "[", "\"arc_tags\"", "]", "=", "AdjacencyField", "(", "arc_indices", ",", "token_field", ",", "arc_tags", ")", "\n", "\n", "", "return", "Instance", "(", "fields", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.dataset_readers.semantic_dependency_parsing.parse_sentence": [[18, 63], ["enumerate", "enumerate", "line.split", "annotated_sentence.append", "enumerate", "sentence_blob.split", "predicates.append", "zip", "arc_indices.append", "arc_tags.append", "line.strip().startswith", "len", "line.strip"], "function", ["None"], ["def", "parse_sentence", "(", "\n", "sentence_blob", ":", "str", ",", "\n", ")", "->", "Tuple", "[", "List", "[", "Dict", "[", "str", ",", "str", "]", "]", ",", "List", "[", "Tuple", "[", "int", ",", "int", "]", "]", ",", "List", "[", "str", "]", "]", ":", "\n", "    ", "\"\"\"\n    Parses a chunk of text in the SemEval SDP format.\n\n    Each word in the sentence is returned as a dictionary with the following\n    format:\n    'id': '1',\n    'form': 'Pierre',\n    'lemma': 'Pierre',\n    'pos': 'NNP',\n    'head': '2',   # Note that this is the `syntactic` head.\n    'deprel': 'nn',\n    'top': '-',\n    'pred': '+',\n    'frame': 'named:x-c'\n\n    Along with a list of arcs and their corresponding tags. Note that\n    in semantic dependency parsing words can have more than one head\n    (it is not a tree), meaning that the list of arcs and tags are\n    not tied to the length of the sentence.\n    \"\"\"", "\n", "annotated_sentence", "=", "[", "]", "\n", "arc_indices", "=", "[", "]", "\n", "arc_tags", "=", "[", "]", "\n", "predicates", "=", "[", "]", "\n", "\n", "lines", "=", "[", "\n", "line", ".", "split", "(", "\"\\t\"", ")", "\n", "for", "line", "in", "sentence_blob", ".", "split", "(", "\"\\n\"", ")", "\n", "if", "line", "and", "not", "line", ".", "strip", "(", ")", ".", "startswith", "(", "\"#\"", ")", "\n", "]", "\n", "for", "line_idx", ",", "line", "in", "enumerate", "(", "lines", ")", ":", "\n", "        ", "annotated_token", "=", "{", "k", ":", "v", "for", "k", ",", "v", "in", "zip", "(", "FIELDS", ",", "line", ")", "}", "\n", "if", "annotated_token", "[", "\"pred\"", "]", "==", "\"+\"", ":", "\n", "            ", "predicates", ".", "append", "(", "line_idx", ")", "\n", "", "annotated_sentence", ".", "append", "(", "annotated_token", ")", "\n", "\n", "", "for", "line_idx", ",", "line", "in", "enumerate", "(", "lines", ")", ":", "\n", "        ", "for", "predicate_idx", ",", "arg", "in", "enumerate", "(", "line", "[", "len", "(", "FIELDS", ")", ":", "]", ")", ":", "\n", "            ", "if", "arg", "!=", "\"_\"", ":", "\n", "                ", "arc_indices", ".", "append", "(", "(", "line_idx", ",", "predicates", "[", "predicate_idx", "]", ")", ")", "\n", "arc_tags", ".", "append", "(", "arg", ")", "\n", "", "", "", "return", "annotated_sentence", ",", "arc_indices", ",", "arc_tags", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.dataset_readers.semantic_dependency_parsing.lazy_parse": [[65, 69], ["text.split", "semantic_dependency_parsing.parse_sentence"], "function", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.dataset_readers.semantic_dependency_parsing.parse_sentence"], ["", "def", "lazy_parse", "(", "text", ":", "str", ")", ":", "\n", "    ", "for", "sentence", "in", "text", ".", "split", "(", "\"\\n\\n\"", ")", ":", "\n", "        ", "if", "sentence", ":", "\n", "            ", "yield", "parse_sentence", "(", "sentence", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.dataset_readers.seq2seq.Seq2SeqDatasetReader.__init__": [[55, 84], ["allennlp.data.dataset_readers.dataset_reader.DatasetReader.__init__", "allennlp.data.tokenizers.SpacyTokenizer", "allennlp.data.token_indexers.SingleIdTokenIndexer"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.graph_util.plot.GraphPlot.__init__"], []], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.dataset_readers.seq2seq.Seq2SeqDatasetReader._read": [[85, 112], ["open", "logger.info", "enumerate", "logger.info", "logger.info", "allennlp.common.file_utils.cached_path", "csv.reader", "len", "allennlp.common.checks.ConfigurationError", "seq2seq.Seq2SeqDatasetReader.text_to_instance"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.dataset_readers.multiprocess_dataset_reader.logger.info", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.dataset_readers.multiprocess_dataset_reader.logger.info", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.dataset_readers.multiprocess_dataset_reader.logger.info", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.file_utils.cached_path", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.coreference_resolution.conll.ConllCorefReader.text_to_instance"], []], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.dataset_readers.seq2seq.Seq2SeqDatasetReader.text_to_instance": [[114, 144], ["seq2seq.Seq2SeqDatasetReader._source_tokenizer.tokenize", "allennlp.data.fields.TextField", "seq2seq.Seq2SeqDatasetReader.insert", "seq2seq.Seq2SeqDatasetReader.append", "seq2seq.Seq2SeqDatasetReader._target_tokenizer.tokenize", "seq2seq.Seq2SeqDatasetReader.insert", "seq2seq.Seq2SeqDatasetReader.append", "allennlp.data.fields.TextField", "allennlp.data.instance.Instance", "allennlp.data.instance.Instance", "len", "allennlp.data.tokenizers.Token", "allennlp.data.tokenizers.Token", "allennlp.data.tokenizers.Token", "allennlp.data.tokenizers.Token", "len"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.tokenizers.whitespace_tokenizer.WhitespaceTokenizer.tokenize", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.tokenizers.whitespace_tokenizer.WhitespaceTokenizer.tokenize"], []], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.dataset_readers.semantic_role_labeling.SrlReader.__init__": [[130, 147], ["allennlp.data.dataset_readers.dataset_reader.DatasetReader.__init__", "transformers.tokenization_bert.BertTokenizer.from_pretrained", "allennlp.data.token_indexers.SingleIdTokenIndexer"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.graph_util.plot.GraphPlot.__init__"], ["def", "__init__", "(", "\n", "self", ",", "\n", "token_indexers", ":", "Dict", "[", "str", ",", "TokenIndexer", "]", "=", "None", ",", "\n", "domain_identifier", ":", "str", "=", "None", ",", "\n", "bert_model_name", ":", "str", "=", "None", ",", "\n", "**", "kwargs", ",", "\n", ")", "->", "None", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "**", "kwargs", ")", "\n", "self", ".", "_token_indexers", "=", "token_indexers", "or", "{", "\"tokens\"", ":", "SingleIdTokenIndexer", "(", ")", "}", "\n", "self", ".", "_domain_identifier", "=", "domain_identifier", "\n", "\n", "if", "bert_model_name", "is", "not", "None", ":", "\n", "            ", "self", ".", "bert_tokenizer", "=", "BertTokenizer", ".", "from_pretrained", "(", "bert_model_name", ")", "\n", "self", ".", "lowercase_input", "=", "\"uncased\"", "in", "bert_model_name", "\n", "", "else", ":", "\n", "            ", "self", ".", "bert_tokenizer", "=", "None", "\n", "self", ".", "lowercase_input", "=", "False", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.dataset_readers.semantic_role_labeling.SrlReader._wordpiece_tokenize_input": [[148, 200], ["semantic_role_labeling.SrlReader.bert_tokenizer.wordpiece_tokenizer.tokenize", "start_offsets.append", "len", "end_offsets.append", "word_piece_tokens.extend", "token.lower.lower.lower"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.tokenizers.whitespace_tokenizer.WhitespaceTokenizer.tokenize"], ["", "", "def", "_wordpiece_tokenize_input", "(", "\n", "self", ",", "tokens", ":", "List", "[", "str", "]", "\n", ")", "->", "Tuple", "[", "List", "[", "str", "]", ",", "List", "[", "int", "]", ",", "List", "[", "int", "]", "]", ":", "\n", "        ", "\"\"\"\n        Convert a list of tokens to wordpiece tokens and offsets, as well as adding\n        BERT CLS and SEP tokens to the begining and end of the sentence.\n\n        A slight oddity with this function is that it also returns the wordpiece offsets\n        corresponding to the _start_ of words as well as the end.\n\n        We need both of these offsets (or at least, it's easiest to use both), because we need\n        to convert the labels to tags using the end_offsets. However, when we are decoding a\n        BIO sequence inside the SRL model itself, it's important that we use the start_offsets,\n        because otherwise we might select an ill-formed BIO sequence from the BIO sequence on top of\n        wordpieces (this happens in the case that a word is split into multiple word pieces,\n        and then we take the last tag of the word, which might correspond to, e.g, I-V, which\n        would not be allowed as it is not preceeded by a B tag).\n\n        For example:\n\n        `annotate` will be bert tokenized as [\"anno\", \"##tate\"].\n        If this is tagged as [B-V, I-V] as it should be, we need to select the\n        _first_ wordpiece label to be the label for the token, because otherwise\n        we may end up with invalid tag sequences (we cannot start a new tag with an I).\n\n        # Returns\n\n        wordpieces : List[str]\n            The BERT wordpieces from the words in the sentence.\n        end_offsets : List[int]\n            Indices into wordpieces such that `[wordpieces[i] for i in end_offsets]`\n            results in the end wordpiece of each word being chosen.\n        start_offsets : List[int]\n            Indices into wordpieces such that `[wordpieces[i] for i in start_offsets]`\n            results in the start wordpiece of each word being chosen.\n        \"\"\"", "\n", "word_piece_tokens", ":", "List", "[", "str", "]", "=", "[", "]", "\n", "end_offsets", "=", "[", "]", "\n", "start_offsets", "=", "[", "]", "\n", "cumulative", "=", "0", "\n", "for", "token", "in", "tokens", ":", "\n", "            ", "if", "self", ".", "lowercase_input", ":", "\n", "                ", "token", "=", "token", ".", "lower", "(", ")", "\n", "", "word_pieces", "=", "self", ".", "bert_tokenizer", ".", "wordpiece_tokenizer", ".", "tokenize", "(", "token", ")", "\n", "start_offsets", ".", "append", "(", "cumulative", "+", "1", ")", "\n", "cumulative", "+=", "len", "(", "word_pieces", ")", "\n", "end_offsets", ".", "append", "(", "cumulative", ")", "\n", "word_piece_tokens", ".", "extend", "(", "word_pieces", ")", "\n", "\n", "", "wordpieces", "=", "[", "\"[CLS]\"", "]", "+", "word_piece_tokens", "+", "[", "\"[SEP]\"", "]", "\n", "\n", "return", "wordpieces", ",", "end_offsets", ",", "start_offsets", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.dataset_readers.semantic_role_labeling.SrlReader._read": [[201, 226], ["allennlp.common.file_utils.cached_path", "allennlp.data.dataset_readers.dataset_utils.Ontonotes", "logger.info", "semantic_role_labeling.SrlReader._ontonotes_subset", "logger.info", "allennlp.data.tokenizers.Token", "semantic_role_labeling.SrlReader.text_to_instance", "semantic_role_labeling.SrlReader.text_to_instance"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.file_utils.cached_path", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.dataset_readers.multiprocess_dataset_reader.logger.info", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.dataset_readers.ontonotes_ner.OntonotesNamedEntityRecognition._ontonotes_subset", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.dataset_readers.multiprocess_dataset_reader.logger.info", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.coreference_resolution.conll.ConllCorefReader.text_to_instance", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.coreference_resolution.conll.ConllCorefReader.text_to_instance"], ["", "@", "overrides", "\n", "def", "_read", "(", "self", ",", "file_path", ":", "str", ")", ":", "\n", "# if `file_path` is a URL, redirect to the cache", "\n", "        ", "file_path", "=", "cached_path", "(", "file_path", ")", "\n", "ontonotes_reader", "=", "Ontonotes", "(", ")", "\n", "logger", ".", "info", "(", "\"Reading SRL instances from dataset files at: %s\"", ",", "file_path", ")", "\n", "if", "self", ".", "_domain_identifier", "is", "not", "None", ":", "\n", "            ", "logger", ".", "info", "(", "\n", "\"Filtering to only include file paths containing the %s domain\"", ",", "\n", "self", ".", "_domain_identifier", ",", "\n", ")", "\n", "\n", "", "for", "sentence", "in", "self", ".", "_ontonotes_subset", "(", "\n", "ontonotes_reader", ",", "file_path", ",", "self", ".", "_domain_identifier", "\n", ")", ":", "\n", "            ", "tokens", "=", "[", "Token", "(", "t", ")", "for", "t", "in", "sentence", ".", "words", "]", "\n", "if", "not", "sentence", ".", "srl_frames", ":", "\n", "# Sentence contains no predicates.", "\n", "                ", "tags", "=", "[", "\"O\"", "for", "_", "in", "tokens", "]", "\n", "verb_label", "=", "[", "0", "for", "_", "in", "tokens", "]", "\n", "yield", "self", ".", "text_to_instance", "(", "tokens", ",", "verb_label", ",", "tags", ")", "\n", "", "else", ":", "\n", "                ", "for", "(", "_", ",", "tags", ")", "in", "sentence", ".", "srl_frames", ":", "\n", "                    ", "verb_indicator", "=", "[", "1", "if", "label", "[", "-", "2", ":", "]", "==", "\"-V\"", "else", "0", "for", "label", "in", "tags", "]", "\n", "yield", "self", ".", "text_to_instance", "(", "tokens", ",", "verb_indicator", ",", "tags", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.dataset_readers.semantic_role_labeling.SrlReader._ontonotes_subset": [[227, 239], ["ontonotes_reader.dataset_path_iterator", "ontonotes_reader.sentence_iterator"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.dataset_utils.ontonotes.Ontonotes.dataset_path_iterator", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.dataset_utils.ontonotes.Ontonotes.sentence_iterator"], ["", "", "", "", "@", "staticmethod", "\n", "def", "_ontonotes_subset", "(", "\n", "ontonotes_reader", ":", "Ontonotes", ",", "file_path", ":", "str", ",", "domain_identifier", ":", "str", "\n", ")", "->", "Iterable", "[", "OntonotesSentence", "]", ":", "\n", "        ", "\"\"\"\n        Iterates over the Ontonotes 5.0 dataset using an optional domain identifier.\n        If the domain identifier is present, only examples which contain the domain\n        identifier in the file path are yielded.\n        \"\"\"", "\n", "for", "conll_file", "in", "ontonotes_reader", ".", "dataset_path_iterator", "(", "file_path", ")", ":", "\n", "            ", "if", "domain_identifier", "is", "None", "or", "f\"/{domain_identifier}/\"", "in", "conll_file", ":", "\n", "                ", "yield", "from", "ontonotes_reader", ".", "sentence_iterator", "(", "conll_file", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.dataset_readers.semantic_role_labeling.SrlReader.text_to_instance": [[240, 293], ["all", "allennlp.data.fields.MetadataField", "allennlp.data.instance.Instance", "semantic_role_labeling.SrlReader._wordpiece_tokenize_input", "semantic_role_labeling._convert_verb_indices_to_wordpiece_indices", "allennlp.data.fields.TextField", "allennlp.data.fields.SequenceLabelField", "allennlp.data.fields.TextField", "allennlp.data.fields.SequenceLabelField", "verb_label.index", "semantic_role_labeling._convert_tags_to_wordpiece_tags", "allennlp.data.fields.SequenceLabelField", "allennlp.data.fields.SequenceLabelField", "allennlp.data.tokenizers.Token"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.dataset_readers.semantic_role_labeling.SrlReader._wordpiece_tokenize_input", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.dataset_readers.semantic_role_labeling._convert_verb_indices_to_wordpiece_indices", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.fields.text_field.TextField.index", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.dataset_readers.semantic_role_labeling._convert_tags_to_wordpiece_tags"], ["", "", "", "def", "text_to_instance", "(", "# type: ignore", "\n", "self", ",", "tokens", ":", "List", "[", "Token", "]", ",", "verb_label", ":", "List", "[", "int", "]", ",", "tags", ":", "List", "[", "str", "]", "=", "None", "\n", ")", "->", "Instance", ":", "\n", "        ", "\"\"\"\n        We take `pre-tokenized` input here, along with a verb label.  The verb label should be a\n        one-hot binary vector, the same length as the tokens, indicating the position of the verb\n        to find arguments for.\n        \"\"\"", "\n", "\n", "metadata_dict", ":", "Dict", "[", "str", ",", "Any", "]", "=", "{", "}", "\n", "if", "self", ".", "bert_tokenizer", "is", "not", "None", ":", "\n", "            ", "wordpieces", ",", "offsets", ",", "start_offsets", "=", "self", ".", "_wordpiece_tokenize_input", "(", "\n", "[", "t", ".", "text", "for", "t", "in", "tokens", "]", "\n", ")", "\n", "new_verbs", "=", "_convert_verb_indices_to_wordpiece_indices", "(", "verb_label", ",", "offsets", ")", "\n", "metadata_dict", "[", "\"offsets\"", "]", "=", "start_offsets", "\n", "# In order to override the indexing mechanism, we need to set the `text_id`", "\n", "# attribute directly. This causes the indexing to use this id.", "\n", "text_field", "=", "TextField", "(", "\n", "[", "Token", "(", "t", ",", "text_id", "=", "self", ".", "bert_tokenizer", ".", "vocab", "[", "t", "]", ")", "for", "t", "in", "wordpieces", "]", ",", "\n", "token_indexers", "=", "self", ".", "_token_indexers", ",", "\n", ")", "\n", "verb_indicator", "=", "SequenceLabelField", "(", "new_verbs", ",", "text_field", ")", "\n", "\n", "", "else", ":", "\n", "            ", "text_field", "=", "TextField", "(", "tokens", ",", "token_indexers", "=", "self", ".", "_token_indexers", ")", "\n", "verb_indicator", "=", "SequenceLabelField", "(", "verb_label", ",", "text_field", ")", "\n", "\n", "", "fields", ":", "Dict", "[", "str", ",", "Field", "]", "=", "{", "}", "\n", "fields", "[", "\"tokens\"", "]", "=", "text_field", "\n", "fields", "[", "\"verb_indicator\"", "]", "=", "verb_indicator", "\n", "\n", "if", "all", "(", "[", "x", "==", "0", "for", "x", "in", "verb_label", "]", ")", ":", "\n", "            ", "verb", "=", "None", "\n", "verb_index", "=", "None", "\n", "", "else", ":", "\n", "            ", "verb_index", "=", "verb_label", ".", "index", "(", "1", ")", "\n", "verb", "=", "tokens", "[", "verb_index", "]", ".", "text", "\n", "\n", "", "metadata_dict", "[", "\"words\"", "]", "=", "[", "x", ".", "text", "for", "x", "in", "tokens", "]", "\n", "metadata_dict", "[", "\"verb\"", "]", "=", "verb", "\n", "metadata_dict", "[", "\"verb_index\"", "]", "=", "verb_index", "\n", "\n", "if", "tags", ":", "\n", "            ", "if", "self", ".", "bert_tokenizer", "is", "not", "None", ":", "\n", "                ", "new_tags", "=", "_convert_tags_to_wordpiece_tags", "(", "tags", ",", "offsets", ")", "\n", "fields", "[", "\"tags\"", "]", "=", "SequenceLabelField", "(", "new_tags", ",", "text_field", ")", "\n", "", "else", ":", "\n", "                ", "fields", "[", "\"tags\"", "]", "=", "SequenceLabelField", "(", "tags", ",", "text_field", ")", "\n", "", "metadata_dict", "[", "\"gold_tags\"", "]", "=", "tags", "\n", "\n", "", "fields", "[", "\"metadata\"", "]", "=", "MetadataField", "(", "metadata_dict", ")", "\n", "return", "Instance", "(", "fields", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.dataset_readers.semantic_role_labeling._convert_tags_to_wordpiece_tags": [[19, 62], ["enumerate", "new_tags.append", "tag.startswith", "new_tags.append", "tag.startswith", "new_tags.append", "tag.startswith", "tag.split", "new_tags.append"], "function", ["None"], ["def", "_convert_tags_to_wordpiece_tags", "(", "tags", ":", "List", "[", "str", "]", ",", "offsets", ":", "List", "[", "int", "]", ")", "->", "List", "[", "str", "]", ":", "\n", "    ", "\"\"\"\n    Converts a series of BIO tags to account for a wordpiece tokenizer,\n    extending/modifying BIO tags where appropriate to deal with words which\n    are split into multiple wordpieces by the tokenizer.\n\n    This is only used if you pass a `bert_model_name` to the dataset reader below.\n\n    # Parameters\n\n    tags : `List[str]`\n        The BIO formatted tags to convert to BIO tags for wordpieces\n    offsets : `List[int]`\n        The wordpiece offsets.\n\n    # Returns\n\n    The new BIO tags.\n    \"\"\"", "\n", "new_tags", "=", "[", "]", "\n", "j", "=", "0", "\n", "for", "i", ",", "offset", "in", "enumerate", "(", "offsets", ")", ":", "\n", "        ", "tag", "=", "tags", "[", "i", "]", "\n", "is_o", "=", "tag", "==", "\"O\"", "\n", "is_start", "=", "True", "\n", "while", "j", "<", "offset", ":", "\n", "            ", "if", "is_o", ":", "\n", "                ", "new_tags", ".", "append", "(", "\"O\"", ")", "\n", "\n", "", "elif", "tag", ".", "startswith", "(", "\"I\"", ")", ":", "\n", "                ", "new_tags", ".", "append", "(", "tag", ")", "\n", "\n", "", "elif", "is_start", "and", "tag", ".", "startswith", "(", "\"B\"", ")", ":", "\n", "                ", "new_tags", ".", "append", "(", "tag", ")", "\n", "is_start", "=", "False", "\n", "\n", "", "elif", "tag", ".", "startswith", "(", "\"B\"", ")", ":", "\n", "                ", "_", ",", "label", "=", "tag", ".", "split", "(", "\"-\"", ",", "1", ")", "\n", "new_tags", ".", "append", "(", "\"I-\"", "+", "label", ")", "\n", "", "j", "+=", "1", "\n", "\n", "# Add O tags for cls and sep tokens.", "\n", "", "", "return", "[", "\"O\"", "]", "+", "new_tags", "+", "[", "\"O\"", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.dataset_readers.semantic_role_labeling._convert_verb_indices_to_wordpiece_indices": [[64, 95], ["enumerate", "new_verb_indices.append"], "function", ["None"], ["", "def", "_convert_verb_indices_to_wordpiece_indices", "(", "\n", "verb_indices", ":", "List", "[", "int", "]", ",", "offsets", ":", "List", "[", "int", "]", "\n", ")", ":", "\n", "    ", "\"\"\"\n    Converts binary verb indicators to account for a wordpiece tokenizer,\n    extending/modifying BIO tags where appropriate to deal with words which\n    are split into multiple wordpieces by the tokenizer.\n\n    This is only used if you pass a `bert_model_name` to the dataset reader below.\n\n    # Parameters\n\n    verb_indices : `List[int]`\n        The binary verb indicators, 0 for not a verb, 1 for verb.\n    offsets : `List[int]`\n        The wordpiece offsets.\n\n    # Returns\n\n    The new verb indices.\n    \"\"\"", "\n", "j", "=", "0", "\n", "new_verb_indices", "=", "[", "]", "\n", "for", "i", ",", "offset", "in", "enumerate", "(", "offsets", ")", ":", "\n", "        ", "indicator", "=", "verb_indices", "[", "i", "]", "\n", "while", "j", "<", "offset", ":", "\n", "            ", "new_verb_indices", ".", "append", "(", "indicator", ")", "\n", "j", "+=", "1", "\n", "\n", "# Add 0 indicators for cls and sep tokens.", "\n", "", "", "return", "[", "0", "]", "+", "new_verb_indices", "+", "[", "0", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.dataset_readers.interleaving_dataset_reader.InterleavingDatasetReader.__init__": [[38, 52], ["allennlp.data.dataset_readers.dataset_reader.DatasetReader.__init__", "allennlp.common.checks.ConfigurationError"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.graph_util.plot.GraphPlot.__init__"], ["def", "__init__", "(", "\n", "self", ",", "\n", "readers", ":", "Dict", "[", "str", ",", "DatasetReader", "]", ",", "\n", "dataset_field_name", ":", "str", "=", "\"dataset\"", ",", "\n", "scheme", ":", "str", "=", "\"round_robin\"", ",", "\n", "**", "kwargs", ",", "\n", ")", "->", "None", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "**", "kwargs", ")", "\n", "self", ".", "_readers", "=", "readers", "\n", "self", ".", "_dataset_field_name", "=", "dataset_field_name", "\n", "\n", "if", "scheme", "not", "in", "_VALID_SCHEMES", ":", "\n", "            ", "raise", "ConfigurationError", "(", "f\"invalid scheme: {scheme}\"", ")", "\n", "", "self", ".", "_scheme", "=", "scheme", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.dataset_readers.interleaving_dataset_reader.InterleavingDatasetReader._read_round_robin": [[53, 68], ["set", "iter", "dataset_iterators.items", "datasets.items", "next", "allennlp.data.fields.MetadataField", "set.remove"], "methods", ["None"], ["", "def", "_read_round_robin", "(", "\n", "self", ",", "datasets", ":", "Dict", "[", "str", ",", "Iterable", "[", "Instance", "]", "]", "\n", ")", "->", "Iterable", "[", "Instance", "]", ":", "\n", "        ", "remaining", "=", "set", "(", "datasets", ")", "\n", "dataset_iterators", "=", "{", "key", ":", "iter", "(", "dataset", ")", "for", "key", ",", "dataset", "in", "datasets", ".", "items", "(", ")", "}", "\n", "\n", "while", "remaining", ":", "\n", "            ", "for", "key", ",", "dataset", "in", "dataset_iterators", ".", "items", "(", ")", ":", "\n", "                ", "if", "key", "in", "remaining", ":", "\n", "                    ", "try", ":", "\n", "                        ", "instance", "=", "next", "(", "dataset", ")", "\n", "instance", ".", "fields", "[", "self", ".", "_dataset_field_name", "]", "=", "MetadataField", "(", "key", ")", "\n", "yield", "instance", "\n", "", "except", "StopIteration", ":", "\n", "                        ", "remaining", ".", "remove", "(", "key", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.dataset_readers.interleaving_dataset_reader.InterleavingDatasetReader._read_all_at_once": [[69, 76], ["datasets.items", "allennlp.data.fields.MetadataField"], "methods", ["None"], ["", "", "", "", "", "def", "_read_all_at_once", "(", "\n", "self", ",", "datasets", ":", "Dict", "[", "str", ",", "Iterable", "[", "Instance", "]", "]", "\n", ")", "->", "Iterable", "[", "Instance", "]", ":", "\n", "        ", "for", "key", ",", "dataset", "in", "datasets", ".", "items", "(", ")", ":", "\n", "            ", "for", "instance", "in", "dataset", ":", "\n", "                ", "instance", ".", "fields", "[", "self", ".", "_dataset_field_name", "]", "=", "MetadataField", "(", "key", ")", "\n", "yield", "instance", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.dataset_readers.interleaving_dataset_reader.InterleavingDatasetReader._read": [[77, 100], ["json.loads", "json.loads.keys", "interleaving_dataset_reader.InterleavingDatasetReader._readers.keys", "allennlp.common.checks.ConfigurationError", "reader.read", "allennlp.common.checks.ConfigurationError", "interleaving_dataset_reader.InterleavingDatasetReader._readers.items", "interleaving_dataset_reader.InterleavingDatasetReader._read_round_robin", "RuntimeError", "interleaving_dataset_reader.InterleavingDatasetReader._read_all_at_once"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.scripts.convert_openie_to_conll.read", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.dataset_readers.interleaving_dataset_reader.InterleavingDatasetReader._read_round_robin", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.dataset_readers.interleaving_dataset_reader.InterleavingDatasetReader._read_all_at_once"], ["", "", "", "def", "_read", "(", "self", ",", "file_path", ":", "str", ")", "->", "Iterable", "[", "Instance", "]", ":", "\n", "        ", "try", ":", "\n", "            ", "file_paths", "=", "json", ".", "loads", "(", "file_path", ")", "\n", "", "except", "json", ".", "JSONDecodeError", ":", "\n", "            ", "raise", "ConfigurationError", "(", "\n", "\"the file_path for the InterleavingDatasetReader \"", "\n", "\"needs to be a JSON-serialized dictionary {reader_name -> file_path}\"", "\n", ")", "\n", "\n", "", "if", "file_paths", ".", "keys", "(", ")", "!=", "self", ".", "_readers", ".", "keys", "(", ")", ":", "\n", "            ", "raise", "ConfigurationError", "(", "\"mismatched keys\"", ")", "\n", "\n", "# Load datasets", "\n", "", "datasets", "=", "{", "\n", "key", ":", "reader", ".", "read", "(", "file_paths", "[", "key", "]", ")", "for", "key", ",", "reader", "in", "self", ".", "_readers", ".", "items", "(", ")", "\n", "}", "\n", "\n", "if", "self", ".", "_scheme", "==", "\"round_robin\"", ":", "\n", "            ", "yield", "from", "self", ".", "_read_round_robin", "(", "datasets", ")", "\n", "", "elif", "self", ".", "_scheme", "==", "\"all_at_once\"", ":", "\n", "            ", "yield", "from", "self", ".", "_read_all_at_once", "(", "datasets", ")", "\n", "", "else", ":", "\n", "            ", "raise", "RuntimeError", "(", "\"impossible to get here\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.dataset_readers.interleaving_dataset_reader.InterleavingDatasetReader.text_to_instance": [[101, 104], ["RuntimeError"], "methods", ["None"], ["", "", "def", "text_to_instance", "(", "self", ")", "->", "Instance", ":", "# type: ignore", "\n", "\n", "        ", "raise", "RuntimeError", "(", "\"text_to_instance doesn't make sense here\"", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.dataset_readers.event2mind.Event2MindDatasetReader.__init__": [[66, 88], ["allennlp.data.dataset_readers.dataset_reader.DatasetReader.__init__", "allennlp.data.tokenizers.SpacyTokenizer", "allennlp.data.token_indexers.SingleIdTokenIndexer"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.graph_util.plot.GraphPlot.__init__"], []], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.dataset_readers.event2mind.Event2MindDatasetReader._read": [[90, 134], ["open", "logger.info", "csv.reader", "next", "enumerate", "allennlp.common.file_utils.cached_path", "json.loads", "json.loads", "json.loads", "len", "allennlp.common.checks.ConfigurationError", "str", "event2mind.Event2MindDatasetReader.text_to_instance", "event2mind.Event2MindDatasetReader.text_to_instance", "event2mind.Event2MindDatasetReader.text_to_instance", "event2mind.Event2MindDatasetReader.text_to_instance"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.dataset_readers.multiprocess_dataset_reader.logger.info", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.file_utils.cached_path", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.coreference_resolution.conll.ConllCorefReader.text_to_instance", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.coreference_resolution.conll.ConllCorefReader.text_to_instance", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.coreference_resolution.conll.ConllCorefReader.text_to_instance", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.coreference_resolution.conll.ConllCorefReader.text_to_instance"], []], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.dataset_readers.event2mind.Event2MindDatasetReader._preprocess_string": [[135, 176], ["tokenizer.tokenize", "string.lower", "string.lower", "range", "retval.append", "retval.append", "words_with_persony.append", "len", "words_with_persony.append", "words_with_persony.append"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.tokenizers.whitespace_tokenizer.WhitespaceTokenizer.tokenize"], []], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.dataset_readers.event2mind.Event2MindDatasetReader._build_target_field": [[177, 183], ["event2mind.Event2MindDatasetReader._preprocess_string", "event2mind.Event2MindDatasetReader._target_tokenizer.tokenize", "event2mind.Event2MindDatasetReader.insert", "event2mind.Event2MindDatasetReader.append", "allennlp.data.fields.TextField", "allennlp.data.tokenizers.Token", "allennlp.data.tokenizers.Token"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.dataset_readers.event2mind.Event2MindDatasetReader._preprocess_string", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.tokenizers.whitespace_tokenizer.WhitespaceTokenizer.tokenize"], []], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.dataset_readers.event2mind.Event2MindDatasetReader.text_to_instance": [[184, 214], ["event2mind.Event2MindDatasetReader._preprocess_string", "event2mind.Event2MindDatasetReader._source_tokenizer.tokenize", "event2mind.Event2MindDatasetReader.append", "allennlp.data.fields.TextField", "event2mind.Event2MindDatasetReader.insert", "allennlp.data.tokenizers.Token", "allennlp.data.instance.Instance", "allennlp.data.instance.Instance", "allennlp.data.tokenizers.Token", "Exception", "Exception", "event2mind.Event2MindDatasetReader._build_target_field", "event2mind.Event2MindDatasetReader._build_target_field", "event2mind.Event2MindDatasetReader._build_target_field"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.dataset_readers.event2mind.Event2MindDatasetReader._preprocess_string", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.tokenizers.whitespace_tokenizer.WhitespaceTokenizer.tokenize", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.dataset_readers.event2mind.Event2MindDatasetReader._build_target_field", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.dataset_readers.event2mind.Event2MindDatasetReader._build_target_field", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.dataset_readers.event2mind.Event2MindDatasetReader._build_target_field"], []], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.dataset_readers.simple_language_modeling.SimpleLanguageModelingDatasetReader.__init__": [[41, 66], ["allennlp.data.dataset_readers.dataset_reader.DatasetReader.__init__", "logger.info", "logger.info", "allennlp.data.tokenizers.SpacyTokenizer", "allennlp.data.tokenizers.Token", "allennlp.data.tokenizers.Token", "allennlp.data.token_indexers.SingleIdTokenIndexer"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.graph_util.plot.GraphPlot.__init__", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.dataset_readers.multiprocess_dataset_reader.logger.info", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.dataset_readers.multiprocess_dataset_reader.logger.info"], ["def", "__init__", "(", "\n", "self", ",", "\n", "tokenizer", ":", "Tokenizer", "=", "None", ",", "\n", "token_indexers", ":", "Dict", "[", "str", ",", "TokenIndexer", "]", "=", "None", ",", "\n", "max_sequence_length", ":", "int", "=", "None", ",", "\n", "start_tokens", ":", "List", "[", "str", "]", "=", "None", ",", "\n", "end_tokens", ":", "List", "[", "str", "]", "=", "None", ",", "\n", "**", "kwargs", ",", "\n", ")", "->", "None", ":", "\n", "        ", "if", "\"lazy\"", "not", "in", "kwargs", ":", "\n", "# We typically want language modeling data to be read lazily.", "\n", "            ", "kwargs", "[", "\"lazy\"", "]", "=", "True", "\n", "", "super", "(", ")", ".", "__init__", "(", "**", "kwargs", ")", "\n", "self", ".", "_tokenizer", "=", "tokenizer", "or", "SpacyTokenizer", "(", ")", "\n", "self", ".", "_token_indexers", "=", "token_indexers", "or", "{", "\"tokens\"", ":", "SingleIdTokenIndexer", "(", ")", "}", "\n", "if", "max_sequence_length", "is", "not", "None", ":", "\n", "            ", "self", ".", "_max_sequence_length", ":", "Union", "[", "float", ",", "Optional", "[", "int", "]", "]", "=", "max_sequence_length", "\n", "", "else", ":", "\n", "            ", "self", ".", "_max_sequence_length", "=", "math", ".", "inf", "\n", "\n", "", "self", ".", "_start_tokens", "=", "[", "Token", "(", "st", ")", "for", "st", "in", "(", "start_tokens", "or", "[", "]", ")", "]", "\n", "self", ".", "_end_tokens", "=", "[", "Token", "(", "et", ")", "for", "et", "in", "(", "end_tokens", "or", "[", "]", ")", "]", "\n", "\n", "logger", ".", "info", "(", "\"Creating SimpleLanguageModelingDatasetReader\"", ")", "\n", "logger", ".", "info", "(", "\"max_sequence_length=%s\"", ",", "max_sequence_length", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.dataset_readers.simple_language_modeling.SimpleLanguageModelingDatasetReader.text_to_instance": [[67, 82], ["simple_language_modeling.SimpleLanguageModelingDatasetReader._tokenizer.tokenize", "tokenized_with_ends.extend", "tokenized_with_ends.extend", "tokenized_with_ends.extend", "allennlp.data.instance.Instance", "allennlp.data.fields.TextField"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.tokenizers.whitespace_tokenizer.WhitespaceTokenizer.tokenize"], ["", "@", "overrides", "\n", "def", "text_to_instance", "(", "\n", "self", ",", "# type: ignore", "\n", "sentence", ":", "str", ",", "\n", ")", "->", "Instance", ":", "\n", "\n", "        ", "tokenized", "=", "self", ".", "_tokenizer", ".", "tokenize", "(", "sentence", ")", "\n", "tokenized_with_ends", "=", "[", "]", "\n", "tokenized_with_ends", ".", "extend", "(", "self", ".", "_start_tokens", ")", "\n", "tokenized_with_ends", ".", "extend", "(", "tokenized", ")", "\n", "tokenized_with_ends", ".", "extend", "(", "self", ".", "_end_tokens", ")", "\n", "return_instance", "=", "Instance", "(", "\n", "{", "\"source\"", ":", "TextField", "(", "tokenized_with_ends", ",", "self", ".", "_token_indexers", ")", "}", "\n", ")", "\n", "return", "return_instance", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.dataset_readers.simple_language_modeling.SimpleLanguageModelingDatasetReader._read": [[83, 103], ["logger.info", "open", "logger.info", "logger.warning", "simple_language_modeling.SimpleLanguageModelingDatasetReader.text_to_instance", "simple_language_modeling.SimpleLanguageModelingDatasetReader.fields[].sequence_length"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.dataset_readers.multiprocess_dataset_reader.logger.info", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.dataset_readers.multiprocess_dataset_reader.logger.info", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.coreference_resolution.conll.ConllCorefReader.text_to_instance", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.fields.text_field.TextField.sequence_length"], ["", "@", "overrides", "\n", "def", "_read", "(", "self", ",", "file_path", ":", "str", ")", "->", "Iterable", "[", "Instance", "]", ":", "\n", "\n", "        ", "logger", ".", "info", "(", "\"Loading data from %s\"", ",", "file_path", ")", "\n", "dropped_instances", "=", "0", "\n", "\n", "with", "open", "(", "file_path", ")", "as", "file", ":", "\n", "            ", "for", "sentence", "in", "file", ":", "\n", "                ", "instance", "=", "self", ".", "text_to_instance", "(", "sentence", ")", "\n", "if", "(", "\n", "instance", ".", "fields", "[", "\"source\"", "]", ".", "sequence_length", "(", ")", "\n", "<=", "self", ".", "_max_sequence_length", "\n", ")", ":", "\n", "                    ", "yield", "instance", "\n", "", "else", ":", "\n", "                    ", "dropped_instances", "+=", "1", "\n", "", "", "", "if", "not", "dropped_instances", ":", "\n", "            ", "logger", ".", "info", "(", "f\"No instances dropped from {file_path}.\"", ")", "\n", "", "else", ":", "\n", "            ", "logger", ".", "warning", "(", "f\"Dropped {dropped_instances} instances from {file_path}.\"", ")", "\n", "", "", "", ""]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.dataset_readers.babi.BabiReader.__init__": [[34, 44], ["allennlp.data.dataset_readers.dataset_reader.DatasetReader.__init__", "allennlp.data.token_indexers.SingleIdTokenIndexer"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.graph_util.plot.GraphPlot.__init__"], ["def", "__init__", "(", "\n", "self", ",", "\n", "keep_sentences", ":", "bool", "=", "False", ",", "\n", "token_indexers", ":", "Dict", "[", "str", ",", "TokenIndexer", "]", "=", "None", ",", "\n", "**", "kwargs", ",", "\n", ")", "->", "None", ":", "\n", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "**", "kwargs", ")", "\n", "self", ".", "_keep_sentences", "=", "keep_sentences", "\n", "self", ".", "_token_indexers", "=", "token_indexers", "or", "{", "\"tokens\"", ":", "SingleIdTokenIndexer", "(", ")", "}", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.dataset_readers.babi.BabiReader._read": [[45, 72], ["allennlp.common.file_utils.cached_path", "logger.info", "logger.info", "open", "dataset_file.readlines", "line.replace().split", "question_str.split", "babi.BabiReader.text_to_instance", "line.replace().split", "context.append", "line.replace", "int", "supports_str.split", "line.replace"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.file_utils.cached_path", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.dataset_readers.multiprocess_dataset_reader.logger.info", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.dataset_readers.multiprocess_dataset_reader.logger.info", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.coreference_resolution.conll.ConllCorefReader.text_to_instance"], ["", "@", "overrides", "\n", "def", "_read", "(", "self", ",", "file_path", ":", "str", ")", ":", "\n", "# if `file_path` is a URL, redirect to the cache", "\n", "        ", "file_path", "=", "cached_path", "(", "file_path", ")", "\n", "\n", "logger", ".", "info", "(", "\"Reading file at %s\"", ",", "file_path", ")", "\n", "\n", "with", "open", "(", "file_path", ")", "as", "dataset_file", ":", "\n", "            ", "dataset", "=", "dataset_file", ".", "readlines", "(", ")", "\n", "\n", "", "logger", ".", "info", "(", "\"Reading the dataset\"", ")", "\n", "\n", "context", ":", "List", "[", "List", "[", "str", "]", "]", "=", "[", "[", "]", "]", "\n", "for", "line", "in", "dataset", ":", "\n", "            ", "if", "\"?\"", "in", "line", ":", "\n", "                ", "question_str", ",", "answer", ",", "supports_str", "=", "line", ".", "replace", "(", "\"?\"", ",", "\" ?\"", ")", ".", "split", "(", "\"\\t\"", ")", "\n", "question", "=", "question_str", ".", "split", "(", ")", "[", "1", ":", "]", "\n", "supports", "=", "[", "int", "(", "support", ")", "-", "1", "for", "support", "in", "supports_str", ".", "split", "(", ")", "]", "\n", "\n", "yield", "self", ".", "text_to_instance", "(", "context", ",", "question", ",", "answer", ",", "supports", ")", "\n", "", "else", ":", "\n", "                ", "new_entry", "=", "line", ".", "replace", "(", "\".\"", ",", "\" .\"", ")", ".", "split", "(", ")", "[", "1", ":", "]", "\n", "\n", "if", "line", "[", "0", "]", "==", "\"1\"", ":", "\n", "                    ", "context", "=", "[", "new_entry", "]", "\n", "", "else", ":", "\n", "                    ", "context", ".", "append", "(", "new_entry", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.dataset_readers.babi.BabiReader.text_to_instance": [[73, 107], ["allennlp.data.fields.TextField", "allennlp.data.fields.TextField", "allennlp.data.instance.Instance", "allennlp.data.fields.ListField", "allennlp.data.fields.ListField", "allennlp.data.fields.TextField", "allennlp.data.tokenizers.Token", "allennlp.data.tokenizers.Token", "allennlp.data.fields.TextField", "allennlp.data.fields.IndexField", "allennlp.data.tokenizers.Token", "allennlp.data.tokenizers.Token"], "methods", ["None"], ["", "", "", "", "@", "overrides", "\n", "def", "text_to_instance", "(", "\n", "self", ",", "# type: ignore", "\n", "context", ":", "List", "[", "List", "[", "str", "]", "]", ",", "\n", "question", ":", "List", "[", "str", "]", ",", "\n", "answer", ":", "str", ",", "\n", "supports", ":", "List", "[", "int", "]", ",", "\n", ")", "->", "Instance", ":", "\n", "\n", "        ", "fields", ":", "Dict", "[", "str", ",", "Field", "]", "=", "{", "}", "\n", "\n", "if", "self", ".", "_keep_sentences", ":", "\n", "            ", "context_field_ks", "=", "ListField", "(", "\n", "[", "\n", "TextField", "(", "[", "Token", "(", "word", ")", "for", "word", "in", "line", "]", ",", "self", ".", "_token_indexers", ")", "\n", "for", "line", "in", "context", "\n", "]", "\n", ")", "\n", "\n", "fields", "[", "\"supports\"", "]", "=", "ListField", "(", "\n", "[", "IndexField", "(", "support", ",", "context_field_ks", ")", "for", "support", "in", "supports", "]", "\n", ")", "\n", "", "else", ":", "\n", "            ", "context_field", "=", "TextField", "(", "\n", "[", "Token", "(", "word", ")", "for", "line", "in", "context", "for", "word", "in", "line", "]", ",", "self", ".", "_token_indexers", "\n", ")", "\n", "\n", "", "fields", "[", "\"context\"", "]", "=", "context_field_ks", "if", "self", ".", "_keep_sentences", "else", "context_field", "\n", "fields", "[", "\"question\"", "]", "=", "TextField", "(", "\n", "[", "Token", "(", "word", ")", "for", "word", "in", "question", "]", ",", "self", ".", "_token_indexers", "\n", ")", "\n", "fields", "[", "\"answer\"", "]", "=", "TextField", "(", "[", "Token", "(", "answer", ")", "]", ",", "self", ".", "_token_indexers", ")", "\n", "\n", "return", "Instance", "(", "fields", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.dataset_readers.conll2000.Conll2000DatasetReader.__init__": [[63, 87], ["allennlp.data.dataset_readers.dataset_reader.DatasetReader.__init__", "set", "allennlp.common.checks.ConfigurationError", "allennlp.common.checks.ConfigurationError", "allennlp.data.token_indexers.SingleIdTokenIndexer", "allennlp.common.checks.ConfigurationError"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.graph_util.plot.GraphPlot.__init__"], ["def", "__init__", "(", "\n", "self", ",", "\n", "token_indexers", ":", "Dict", "[", "str", ",", "TokenIndexer", "]", "=", "None", ",", "\n", "tag_label", ":", "str", "=", "\"chunk\"", ",", "\n", "feature_labels", ":", "Sequence", "[", "str", "]", "=", "(", ")", ",", "\n", "coding_scheme", ":", "str", "=", "\"BIO\"", ",", "\n", "label_namespace", ":", "str", "=", "\"labels\"", ",", "\n", "**", "kwargs", ",", "\n", ")", "->", "None", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "**", "kwargs", ")", "\n", "self", ".", "_token_indexers", "=", "token_indexers", "or", "{", "\"tokens\"", ":", "SingleIdTokenIndexer", "(", ")", "}", "\n", "if", "tag_label", "is", "not", "None", "and", "tag_label", "not", "in", "self", ".", "_VALID_LABELS", ":", "\n", "            ", "raise", "ConfigurationError", "(", "\"unknown tag label type: {}\"", ".", "format", "(", "tag_label", ")", ")", "\n", "", "for", "label", "in", "feature_labels", ":", "\n", "            ", "if", "label", "not", "in", "self", ".", "_VALID_LABELS", ":", "\n", "                ", "raise", "ConfigurationError", "(", "\"unknown feature label type: {}\"", ".", "format", "(", "label", ")", ")", "\n", "", "", "if", "coding_scheme", "not", "in", "(", "\"BIO\"", ",", "\"BIOUL\"", ")", ":", "\n", "            ", "raise", "ConfigurationError", "(", "\"unknown coding_scheme: {}\"", ".", "format", "(", "coding_scheme", ")", ")", "\n", "\n", "", "self", ".", "tag_label", "=", "tag_label", "\n", "self", ".", "feature_labels", "=", "set", "(", "feature_labels", ")", "\n", "self", ".", "coding_scheme", "=", "coding_scheme", "\n", "self", ".", "label_namespace", "=", "label_namespace", "\n", "self", ".", "_original_coding_scheme", "=", "\"BIO\"", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.dataset_readers.conll2000.Conll2000DatasetReader._read": [[88, 109], ["allennlp.common.file_utils.cached_path", "open", "logger.info", "itertools.groupby", "line.strip().split", "list", "allennlp.data.tokenizers.Token", "conll2000.Conll2000DatasetReader.text_to_instance", "zip", "line.strip"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.file_utils.cached_path", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.dataset_readers.multiprocess_dataset_reader.logger.info", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.coreference_resolution.conll.ConllCorefReader.text_to_instance"], ["", "@", "overrides", "\n", "def", "_read", "(", "self", ",", "file_path", ":", "str", ")", "->", "Iterable", "[", "Instance", "]", ":", "\n", "# if `file_path` is a URL, redirect to the cache", "\n", "        ", "file_path", "=", "cached_path", "(", "file_path", ")", "\n", "\n", "with", "open", "(", "file_path", ",", "\"r\"", ")", "as", "data_file", ":", "\n", "            ", "logger", ".", "info", "(", "\"Reading instances from lines in file at: %s\"", ",", "file_path", ")", "\n", "\n", "# Group into alternative divider / sentence chunks.", "\n", "for", "is_divider", ",", "lines", "in", "itertools", ".", "groupby", "(", "data_file", ",", "_is_divider", ")", ":", "\n", "# Ignore the divider chunks, so that `lines` corresponds to the words", "\n", "# of a single sentence.", "\n", "                ", "if", "not", "is_divider", ":", "\n", "                    ", "fields", "=", "[", "line", ".", "strip", "(", ")", ".", "split", "(", ")", "for", "line", "in", "lines", "]", "\n", "# unzipping trick returns tuples, but our Fields need lists", "\n", "fields", "=", "[", "list", "(", "field", ")", "for", "field", "in", "zip", "(", "*", "fields", ")", "]", "\n", "tokens_", ",", "pos_tags", ",", "chunk_tags", "=", "fields", "\n", "# TextField requires `Token` objects", "\n", "tokens", "=", "[", "Token", "(", "token", ")", "for", "token", "in", "tokens_", "]", "\n", "\n", "yield", "self", ".", "text_to_instance", "(", "tokens", ",", "pos_tags", ",", "chunk_tags", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.dataset_readers.conll2000.Conll2000DatasetReader.text_to_instance": [[110, 166], ["allennlp.data.fields.TextField", "allennlp.data.fields.MetadataField", "allennlp.data.instance.Instance", "allennlp.data.fields.SequenceLabelField", "allennlp.data.fields.SequenceLabelField", "allennlp.data.fields.SequenceLabelField", "allennlp.data.dataset_readers.dataset_utils.to_bioul", "allennlp.common.checks.ConfigurationError", "allennlp.common.checks.ConfigurationError", "allennlp.data.fields.SequenceLabelField"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.dataset_utils.span_utils.to_bioul"], ["", "", "", "", "def", "text_to_instance", "(", "# type: ignore", "\n", "self", ",", "\n", "tokens", ":", "List", "[", "Token", "]", ",", "\n", "pos_tags", ":", "List", "[", "str", "]", "=", "None", ",", "\n", "chunk_tags", ":", "List", "[", "str", "]", "=", "None", ",", "\n", ")", "->", "Instance", ":", "\n", "        ", "\"\"\"\n        We take `pre-tokenized` input here, because we don't have a tokenizer in this class.\n        \"\"\"", "\n", "\n", "sequence", "=", "TextField", "(", "tokens", ",", "self", ".", "_token_indexers", ")", "\n", "instance_fields", ":", "Dict", "[", "str", ",", "Field", "]", "=", "{", "\"tokens\"", ":", "sequence", "}", "\n", "instance_fields", "[", "\"metadata\"", "]", "=", "MetadataField", "(", "{", "\"words\"", ":", "[", "x", ".", "text", "for", "x", "in", "tokens", "]", "}", ")", "\n", "\n", "# Recode the labels if necessary.", "\n", "if", "self", ".", "coding_scheme", "==", "\"BIOUL\"", ":", "\n", "            ", "coded_chunks", "=", "(", "\n", "to_bioul", "(", "chunk_tags", ",", "encoding", "=", "self", ".", "_original_coding_scheme", ")", "\n", "if", "chunk_tags", "is", "not", "None", "\n", "else", "None", "\n", ")", "\n", "", "else", ":", "\n", "# the default BIO", "\n", "            ", "coded_chunks", "=", "chunk_tags", "\n", "\n", "# Add \"feature labels\" to instance", "\n", "", "if", "\"pos\"", "in", "self", ".", "feature_labels", ":", "\n", "            ", "if", "pos_tags", "is", "None", ":", "\n", "                ", "raise", "ConfigurationError", "(", "\n", "\"Dataset reader was specified to use pos_tags as \"", "\n", "\"features. Pass them to text_to_instance.\"", "\n", ")", "\n", "", "instance_fields", "[", "\"pos_tags\"", "]", "=", "SequenceLabelField", "(", "\n", "pos_tags", ",", "sequence", ",", "\"pos_tags\"", "\n", ")", "\n", "", "if", "\"chunk\"", "in", "self", ".", "feature_labels", ":", "\n", "            ", "if", "coded_chunks", "is", "None", ":", "\n", "                ", "raise", "ConfigurationError", "(", "\n", "\"Dataset reader was specified to use chunk tags as \"", "\n", "\"features. Pass them to text_to_instance.\"", "\n", ")", "\n", "", "instance_fields", "[", "\"chunk_tags\"", "]", "=", "SequenceLabelField", "(", "\n", "coded_chunks", ",", "sequence", ",", "\"chunk_tags\"", "\n", ")", "\n", "\n", "# Add \"tag label\" to instance", "\n", "", "if", "self", ".", "tag_label", "==", "\"pos\"", "and", "pos_tags", "is", "not", "None", ":", "\n", "            ", "instance_fields", "[", "\"tags\"", "]", "=", "SequenceLabelField", "(", "\n", "pos_tags", ",", "sequence", ",", "self", ".", "label_namespace", "\n", ")", "\n", "", "elif", "self", ".", "tag_label", "==", "\"chunk\"", "and", "coded_chunks", "is", "not", "None", ":", "\n", "            ", "instance_fields", "[", "\"tags\"", "]", "=", "SequenceLabelField", "(", "\n", "coded_chunks", ",", "sequence", ",", "self", ".", "label_namespace", "\n", ")", "\n", "\n", "", "return", "Instance", "(", "instance_fields", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.dataset_readers.conll2000._is_divider": [[19, 21], ["line.strip"], "function", ["None"], ["def", "_is_divider", "(", "line", ":", "str", ")", "->", "bool", ":", "\n", "    ", "return", "line", ".", "strip", "(", ")", "==", "\"\"", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.dataset_readers.ontonotes_ner.OntonotesNamedEntityRecognition.__init__": [[63, 79], ["allennlp.data.dataset_readers.dataset_reader.DatasetReader.__init__", "allennlp.common.checks.ConfigurationError", "allennlp.data.token_indexers.SingleIdTokenIndexer"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.graph_util.plot.GraphPlot.__init__"], ["def", "__init__", "(", "\n", "self", ",", "\n", "token_indexers", ":", "Dict", "[", "str", ",", "TokenIndexer", "]", "=", "None", ",", "\n", "domain_identifier", ":", "str", "=", "None", ",", "\n", "coding_scheme", ":", "str", "=", "\"BIO\"", ",", "\n", "**", "kwargs", ",", "\n", ")", "->", "None", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "**", "kwargs", ")", "\n", "self", ".", "_token_indexers", "=", "token_indexers", "or", "{", "\"tokens\"", ":", "SingleIdTokenIndexer", "(", ")", "}", "\n", "self", ".", "_domain_identifier", "=", "domain_identifier", "\n", "if", "domain_identifier", "==", "\"pt\"", ":", "\n", "            ", "raise", "ConfigurationError", "(", "\n", "\"The Ontonotes 5.0 dataset does not contain annotations for\"", "\n", "\" the old and new testament sections.\"", "\n", ")", "\n", "", "self", ".", "_coding_scheme", "=", "coding_scheme", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.dataset_readers.ontonotes_ner.OntonotesNamedEntityRecognition._read": [[80, 99], ["allennlp.common.file_utils.cached_path", "allennlp.data.dataset_readers.dataset_utils.Ontonotes", "logger.info", "ontonotes_ner.OntonotesNamedEntityRecognition._ontonotes_subset", "logger.info", "allennlp.data.tokenizers.Token", "ontonotes_ner.OntonotesNamedEntityRecognition.text_to_instance", "ontonotes_ner._normalize_word"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.file_utils.cached_path", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.dataset_readers.multiprocess_dataset_reader.logger.info", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.dataset_readers.ontonotes_ner.OntonotesNamedEntityRecognition._ontonotes_subset", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.dataset_readers.multiprocess_dataset_reader.logger.info", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.coreference_resolution.conll.ConllCorefReader.text_to_instance", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.coreference_resolution.conll.ConllCorefReader._normalize_word"], ["", "@", "overrides", "\n", "def", "_read", "(", "self", ",", "file_path", ":", "str", ")", ":", "\n", "# if `file_path` is a URL, redirect to the cache", "\n", "        ", "file_path", "=", "cached_path", "(", "file_path", ")", "\n", "ontonotes_reader", "=", "Ontonotes", "(", ")", "\n", "logger", ".", "info", "(", "\n", "\"Reading Fine-Grained NER instances from dataset files at: %s\"", ",", "file_path", "\n", ")", "\n", "if", "self", ".", "_domain_identifier", "is", "not", "None", ":", "\n", "            ", "logger", ".", "info", "(", "\n", "\"Filtering to only include file paths containing the %s domain\"", ",", "\n", "self", ".", "_domain_identifier", ",", "\n", ")", "\n", "\n", "", "for", "sentence", "in", "self", ".", "_ontonotes_subset", "(", "\n", "ontonotes_reader", ",", "file_path", ",", "self", ".", "_domain_identifier", "\n", ")", ":", "\n", "            ", "tokens", "=", "[", "Token", "(", "_normalize_word", "(", "t", ")", ")", "for", "t", "in", "sentence", ".", "words", "]", "\n", "yield", "self", ".", "text_to_instance", "(", "tokens", ",", "sentence", ".", "named_entities", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.dataset_readers.ontonotes_ner.OntonotesNamedEntityRecognition._ontonotes_subset": [[100, 114], ["ontonotes_reader.dataset_path_iterator", "ontonotes_reader.sentence_iterator"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.dataset_utils.ontonotes.Ontonotes.dataset_path_iterator", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.dataset_utils.ontonotes.Ontonotes.sentence_iterator"], ["", "", "@", "staticmethod", "\n", "def", "_ontonotes_subset", "(", "\n", "ontonotes_reader", ":", "Ontonotes", ",", "file_path", ":", "str", ",", "domain_identifier", ":", "str", "\n", ")", "->", "Iterable", "[", "OntonotesSentence", "]", ":", "\n", "        ", "\"\"\"\n        Iterates over the Ontonotes 5.0 dataset using an optional domain identifier.\n        If the domain identifier is present, only examples which contain the domain\n        identifier in the file path are yielded.\n        \"\"\"", "\n", "for", "conll_file", "in", "ontonotes_reader", ".", "dataset_path_iterator", "(", "file_path", ")", ":", "\n", "            ", "if", "(", "\n", "domain_identifier", "is", "None", "or", "f\"/{domain_identifier}/\"", "in", "conll_file", "\n", ")", "and", "\"/pt/\"", "not", "in", "conll_file", ":", "\n", "                ", "yield", "from", "ontonotes_reader", ".", "sentence_iterator", "(", "conll_file", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.dataset_readers.ontonotes_ner.OntonotesNamedEntityRecognition.text_to_instance": [[115, 134], ["allennlp.data.fields.TextField", "allennlp.data.fields.MetadataField", "allennlp.data.instance.Instance", "allennlp.data.fields.SequenceLabelField", "allennlp.data.dataset_readers.dataset_utils.to_bioul"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.dataset_utils.span_utils.to_bioul"], ["", "", "", "@", "overrides", "\n", "def", "text_to_instance", "(", "\n", "self", ",", "# type: ignore", "\n", "tokens", ":", "List", "[", "Token", "]", ",", "\n", "ner_tags", ":", "List", "[", "str", "]", "=", "None", ",", "\n", ")", "->", "Instance", ":", "\n", "        ", "\"\"\"\n        We take `pre-tokenized` input here, because we don't have a tokenizer in this class.\n        \"\"\"", "\n", "\n", "sequence", "=", "TextField", "(", "tokens", ",", "self", ".", "_token_indexers", ")", "\n", "instance_fields", ":", "Dict", "[", "str", ",", "Field", "]", "=", "{", "\"tokens\"", ":", "sequence", "}", "\n", "instance_fields", "[", "\"metadata\"", "]", "=", "MetadataField", "(", "{", "\"words\"", ":", "[", "x", ".", "text", "for", "x", "in", "tokens", "]", "}", ")", "\n", "# Add \"tag label\" to instance", "\n", "if", "ner_tags", "is", "not", "None", ":", "\n", "            ", "if", "self", ".", "_coding_scheme", "==", "\"BIOUL\"", ":", "\n", "                ", "ner_tags", "=", "to_bioul", "(", "ner_tags", ",", "encoding", "=", "\"BIO\"", ")", "\n", "", "instance_fields", "[", "\"tags\"", "]", "=", "SequenceLabelField", "(", "ner_tags", ",", "sequence", ")", "\n", "", "return", "Instance", "(", "instance_fields", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.dataset_readers.ontonotes_ner._normalize_word": [[23, 28], ["None"], "function", ["None"], ["def", "_normalize_word", "(", "word", ":", "str", ")", ":", "\n", "    ", "if", "word", "in", "(", "\"/.\"", ",", "\"/?\"", ")", ":", "\n", "        ", "return", "word", "[", "1", ":", "]", "\n", "", "else", ":", "\n", "        ", "return", "word", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.dataset_readers.penn_tree_bank.PennTreeBankConstituencySpanDatasetReader.__init__": [[67, 82], ["allennlp.data.dataset_readers.dataset_reader.DatasetReader.__init__", "allennlp.data.token_indexers.SingleIdTokenIndexer"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.graph_util.plot.GraphPlot.__init__"], ["def", "__init__", "(", "\n", "self", ",", "\n", "token_indexers", ":", "Dict", "[", "str", ",", "TokenIndexer", "]", "=", "None", ",", "\n", "use_pos_tags", ":", "bool", "=", "True", ",", "\n", "convert_parentheses", ":", "bool", "=", "False", ",", "\n", "label_namespace_prefix", ":", "str", "=", "\"\"", ",", "\n", "pos_label_namespace", ":", "str", "=", "\"pos\"", ",", "\n", "**", "kwargs", ",", "\n", ")", "->", "None", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "**", "kwargs", ")", "\n", "self", ".", "_token_indexers", "=", "token_indexers", "or", "{", "\"tokens\"", ":", "SingleIdTokenIndexer", "(", ")", "}", "\n", "self", ".", "_use_pos_tags", "=", "use_pos_tags", "\n", "self", ".", "_convert_parentheses", "=", "convert_parentheses", "\n", "self", ".", "_label_namespace_prefix", "=", "label_namespace_prefix", "\n", "self", ".", "_pos_label_namespace", "=", "pos_label_namespace", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.dataset_readers.penn_tree_bank.PennTreeBankConstituencySpanDatasetReader._read": [[83, 100], ["allennlp.common.file_utils.cached_path", "os.path.split", "logger.info", "nltk.corpus.reader.bracket_parse.BracketParseCorpusReader().parsed_sents", "penn_tree_bank.PennTreeBankConstituencySpanDatasetReader._strip_functional_tags", "nltk.corpus.reader.bracket_parse.BracketParseCorpusReader", "penn_tree_bank.PennTreeBankConstituencySpanDatasetReader.text_to_instance", "parse.label", "parse.label", "parse.leaves", "parse.pos"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.file_utils.cached_path", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.dataset_readers.multiprocess_dataset_reader.logger.info", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.dataset_readers.penn_tree_bank.PennTreeBankConstituencySpanDatasetReader._strip_functional_tags", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.coreference_resolution.conll.ConllCorefReader.text_to_instance"], ["", "@", "overrides", "\n", "def", "_read", "(", "self", ",", "file_path", ")", ":", "\n", "# if `file_path` is a URL, redirect to the cache", "\n", "        ", "file_path", "=", "cached_path", "(", "file_path", ")", "\n", "directory", ",", "filename", "=", "os", ".", "path", ".", "split", "(", "file_path", ")", "\n", "logger", ".", "info", "(", "\"Reading instances from lines in file at: %s\"", ",", "file_path", ")", "\n", "for", "parse", "in", "BracketParseCorpusReader", "(", "\n", "root", "=", "directory", ",", "fileids", "=", "[", "filename", "]", "\n", ")", ".", "parsed_sents", "(", ")", ":", "\n", "\n", "            ", "self", ".", "_strip_functional_tags", "(", "parse", ")", "\n", "# This is un-needed and clutters the label space.", "\n", "# All the trees also contain a root S node.", "\n", "if", "parse", ".", "label", "(", ")", "==", "\"VROOT\"", "or", "parse", ".", "label", "(", ")", "==", "\"TOP\"", ":", "\n", "                ", "parse", "=", "parse", "[", "0", "]", "\n", "", "pos_tags", "=", "[", "x", "[", "1", "]", "for", "x", "in", "parse", ".", "pos", "(", ")", "]", "if", "self", ".", "_use_pos_tags", "else", "None", "\n", "yield", "self", ".", "text_to_instance", "(", "parse", ".", "leaves", "(", ")", ",", "pos_tags", ",", "parse", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.dataset_readers.penn_tree_bank.PennTreeBankConstituencySpanDatasetReader.text_to_instance": [[101, 189], ["allennlp.data.fields.TextField", "allennlp.data.dataset_readers.dataset_utils.span_utils.enumerate_spans", "allennlp.data.fields.MetadataField", "allennlp.data.fields.ListField", "allennlp.data.instance.Instance", "allennlp.data.fields.SequenceLabelField", "penn_tree_bank.PennTreeBankConstituencySpanDatasetReader._get_gold_spans", "spans.append", "allennlp.data.fields.SequenceLabelField", "PTB_PARENTHESES.get", "allennlp.data.tokenizers.Token", "allennlp.common.checks.ConfigurationError", "allennlp.data.fields.SpanField", "gold_labels.append", "gold_spans.get"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.dataset_utils.span_utils.enumerate_spans", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.dataset_readers.penn_tree_bank.PennTreeBankConstituencySpanDatasetReader._get_gold_spans", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.params.Params.get", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.params.Params.get"], ["", "", "@", "overrides", "\n", "def", "text_to_instance", "(", "\n", "self", ",", "# type: ignore", "\n", "tokens", ":", "List", "[", "str", "]", ",", "\n", "pos_tags", ":", "List", "[", "str", "]", "=", "None", ",", "\n", "gold_tree", ":", "Tree", "=", "None", ",", "\n", ")", "->", "Instance", ":", "\n", "        ", "\"\"\"\n        We take `pre-tokenized` input here, because we don't have a tokenizer in this class.\n\n        # Parameters\n\n        tokens : `List[str]`, required.\n            The tokens in a given sentence.\n        pos_tags : `List[str]`, optional, (default = None).\n            The POS tags for the words in the sentence.\n        gold_tree : `Tree`, optional (default = None).\n            The gold parse tree to create span labels from.\n\n        # Returns\n\n        An `Instance` containing the following fields:\n            tokens : `TextField`\n                The tokens in the sentence.\n            pos_tags : `SequenceLabelField`\n                The POS tags of the words in the sentence.\n                Only returned if `use_pos_tags` is `True`\n            spans : `ListField[SpanField]`\n                A ListField containing all possible subspans of the\n                sentence.\n            span_labels : `SequenceLabelField`, optional.\n                The constituency tags for each of the possible spans, with\n                respect to a gold parse tree. If a span is not contained\n                within the tree, a span will have a `NO-LABEL` label.\n            gold_tree : `MetadataField(Tree)`\n                The gold NLTK parse tree for use in evaluation.\n        \"\"\"", "\n", "\n", "if", "self", ".", "_convert_parentheses", ":", "\n", "            ", "tokens", "=", "[", "PTB_PARENTHESES", ".", "get", "(", "token", ",", "token", ")", "for", "token", "in", "tokens", "]", "\n", "", "text_field", "=", "TextField", "(", "\n", "[", "Token", "(", "x", ")", "for", "x", "in", "tokens", "]", ",", "token_indexers", "=", "self", ".", "_token_indexers", "\n", ")", "\n", "fields", ":", "Dict", "[", "str", ",", "Field", "]", "=", "{", "\"tokens\"", ":", "text_field", "}", "\n", "\n", "pos_namespace", "=", "self", ".", "_label_namespace_prefix", "+", "self", ".", "_pos_label_namespace", "\n", "if", "self", ".", "_use_pos_tags", "and", "pos_tags", "is", "not", "None", ":", "\n", "            ", "pos_tag_field", "=", "SequenceLabelField", "(", "\n", "pos_tags", ",", "text_field", ",", "label_namespace", "=", "pos_namespace", "\n", ")", "\n", "fields", "[", "\"pos_tags\"", "]", "=", "pos_tag_field", "\n", "", "elif", "self", ".", "_use_pos_tags", ":", "\n", "            ", "raise", "ConfigurationError", "(", "\n", "\"use_pos_tags was set to True but no gold pos\"", "\n", "\" tags were passed to the dataset reader.\"", "\n", ")", "\n", "", "spans", ":", "List", "[", "Field", "]", "=", "[", "]", "\n", "gold_labels", "=", "[", "]", "\n", "\n", "if", "gold_tree", "is", "not", "None", ":", "\n", "            ", "gold_spans", ":", "Dict", "[", "Tuple", "[", "int", ",", "int", "]", ",", "str", "]", "=", "{", "}", "\n", "self", ".", "_get_gold_spans", "(", "gold_tree", ",", "0", ",", "gold_spans", ")", "\n", "\n", "", "else", ":", "\n", "            ", "gold_spans", "=", "None", "\n", "", "for", "start", ",", "end", "in", "enumerate_spans", "(", "tokens", ")", ":", "\n", "            ", "spans", ".", "append", "(", "SpanField", "(", "start", ",", "end", ",", "text_field", ")", ")", "\n", "\n", "if", "gold_spans", "is", "not", "None", ":", "\n", "                ", "gold_labels", ".", "append", "(", "gold_spans", ".", "get", "(", "(", "start", ",", "end", ")", ",", "\"NO-LABEL\"", ")", ")", "\n", "\n", "", "", "metadata", "=", "{", "\"tokens\"", ":", "tokens", "}", "\n", "if", "gold_tree", ":", "\n", "            ", "metadata", "[", "\"gold_tree\"", "]", "=", "gold_tree", "\n", "", "if", "self", ".", "_use_pos_tags", ":", "\n", "            ", "metadata", "[", "\"pos_tags\"", "]", "=", "pos_tags", "\n", "\n", "", "fields", "[", "\"metadata\"", "]", "=", "MetadataField", "(", "metadata", ")", "\n", "\n", "span_list_field", ":", "ListField", "=", "ListField", "(", "spans", ")", "\n", "fields", "[", "\"spans\"", "]", "=", "span_list_field", "\n", "if", "gold_tree", "is", "not", "None", ":", "\n", "            ", "fields", "[", "\"span_labels\"", "]", "=", "SequenceLabelField", "(", "\n", "gold_labels", ",", "\n", "span_list_field", ",", "\n", "label_namespace", "=", "self", ".", "_label_namespace_prefix", "+", "\"labels\"", ",", "\n", ")", "\n", "", "return", "Instance", "(", "fields", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.dataset_readers.penn_tree_bank.PennTreeBankConstituencySpanDatasetReader._strip_functional_tags": [[190, 203], ["tree.set_label", "[].split", "isinstance", "penn_tree_bank.PennTreeBankConstituencySpanDatasetReader._strip_functional_tags", "[].split", "tree.label().split", "tree.label"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.dataset_readers.penn_tree_bank.PennTreeBankConstituencySpanDatasetReader._strip_functional_tags"], ["", "def", "_strip_functional_tags", "(", "self", ",", "tree", ":", "Tree", ")", "->", "None", ":", "\n", "        ", "\"\"\"\n        Removes all functional tags from constituency labels in an NLTK tree.\n        We also strip off anything after a =, - or | character, because these\n        are functional tags which we don't want to use.\n\n        This modification is done in-place.\n        \"\"\"", "\n", "clean_label", "=", "tree", ".", "label", "(", ")", ".", "split", "(", "\"=\"", ")", "[", "0", "]", ".", "split", "(", "\"-\"", ")", "[", "0", "]", ".", "split", "(", "\"|\"", ")", "[", "0", "]", "\n", "tree", ".", "set_label", "(", "clean_label", ")", "\n", "for", "child", "in", "tree", ":", "\n", "            ", "if", "not", "isinstance", "(", "child", "[", "0", "]", ",", "str", ")", ":", "\n", "                ", "self", ".", "_strip_functional_tags", "(", "child", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.dataset_readers.penn_tree_bank.PennTreeBankConstituencySpanDatasetReader._get_gold_spans": [[204, 264], ["isinstance", "typed_spans.get", "len", "penn_tree_bank.PennTreeBankConstituencySpanDatasetReader._get_gold_spans", "tree.label", "tree.label"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.params.Params.get", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.dataset_readers.penn_tree_bank.PennTreeBankConstituencySpanDatasetReader._get_gold_spans"], ["", "", "", "def", "_get_gold_spans", "(", "\n", "self", ",", "tree", ":", "Tree", ",", "index", ":", "int", ",", "typed_spans", ":", "Dict", "[", "Tuple", "[", "int", ",", "int", "]", ",", "str", "]", "\n", ")", "->", "int", ":", "\n", "        ", "\"\"\"\n        Recursively construct the gold spans from an nltk `Tree`.\n        Labels are the constituents, and in the case of nested constituents\n        with the same spans, labels are concatenated in parent-child order.\n        For example, `(S (NP (D the) (N man)))` would have an `S-NP` label\n        for the outer span, as it has both `S` and `NP` label.\n        Spans are inclusive.\n\n        TODO(Mark): If we encounter a gold nested labelling at test time\n        which we haven't encountered, we won't be able to run the model\n        at all.\n\n        # Parameters\n\n        tree : `Tree`, required.\n            An NLTK parse tree to extract spans from.\n        index : `int`, required.\n            The index of the current span in the sentence being considered.\n        typed_spans : `Dict[Tuple[int, int], str]`, required.\n            A dictionary mapping spans to span labels.\n\n        # Returns\n\n        typed_spans : `Dict[Tuple[int, int], str]`.\n            A dictionary mapping all subtree spans in the parse tree\n            to their constituency labels. POS tags are ignored.\n        \"\"\"", "\n", "# NLTK leaves are strings.", "\n", "if", "isinstance", "(", "tree", "[", "0", "]", ",", "str", ")", ":", "\n", "# The \"length\" of a tree is defined by", "\n", "# NLTK as the number of children.", "\n", "# We don't actually want the spans for leaves, because", "\n", "# their labels are POS tags. Instead, we just add the length", "\n", "# of the word to the end index as we iterate through.", "\n", "            ", "end", "=", "index", "+", "len", "(", "tree", ")", "\n", "", "else", ":", "\n", "# otherwise, the tree has children.", "\n", "            ", "child_start", "=", "index", "\n", "for", "child", "in", "tree", ":", "\n", "# typed_spans is being updated inplace.", "\n", "                ", "end", "=", "self", ".", "_get_gold_spans", "(", "child", ",", "child_start", ",", "typed_spans", ")", "\n", "child_start", "=", "end", "\n", "# Set the end index of the current span to", "\n", "# the last appended index - 1, as the span is inclusive.", "\n", "", "span", "=", "(", "index", ",", "end", "-", "1", ")", "\n", "current_span_label", "=", "typed_spans", ".", "get", "(", "span", ")", "\n", "if", "current_span_label", "is", "None", ":", "\n", "# This span doesn't have nested labels, just", "\n", "# use the current node's label.", "\n", "                ", "typed_spans", "[", "span", "]", "=", "tree", ".", "label", "(", ")", "\n", "", "else", ":", "\n", "# This span has already been added, so prepend", "\n", "# this label (as we are traversing the tree from", "\n", "# the bottom up).", "\n", "                ", "typed_spans", "[", "span", "]", "=", "tree", ".", "label", "(", ")", "+", "\"-\"", "+", "current_span_label", "\n", "\n", "", "", "return", "end", "\n", "", "", ""]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.dataset_readers.conll2003.Conll2003DatasetReader.__init__": [[78, 102], ["allennlp.data.dataset_readers.dataset_reader.DatasetReader.__init__", "set", "allennlp.common.checks.ConfigurationError", "allennlp.common.checks.ConfigurationError", "allennlp.data.token_indexers.SingleIdTokenIndexer", "allennlp.common.checks.ConfigurationError"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.graph_util.plot.GraphPlot.__init__"], ["def", "__init__", "(", "\n", "self", ",", "\n", "token_indexers", ":", "Dict", "[", "str", ",", "TokenIndexer", "]", "=", "None", ",", "\n", "tag_label", ":", "str", "=", "\"ner\"", ",", "\n", "feature_labels", ":", "Sequence", "[", "str", "]", "=", "(", ")", ",", "\n", "coding_scheme", ":", "str", "=", "\"IOB1\"", ",", "\n", "label_namespace", ":", "str", "=", "\"labels\"", ",", "\n", "**", "kwargs", ",", "\n", ")", "->", "None", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "**", "kwargs", ")", "\n", "self", ".", "_token_indexers", "=", "token_indexers", "or", "{", "\"tokens\"", ":", "SingleIdTokenIndexer", "(", ")", "}", "\n", "if", "tag_label", "is", "not", "None", "and", "tag_label", "not", "in", "self", ".", "_VALID_LABELS", ":", "\n", "            ", "raise", "ConfigurationError", "(", "\"unknown tag label type: {}\"", ".", "format", "(", "tag_label", ")", ")", "\n", "", "for", "label", "in", "feature_labels", ":", "\n", "            ", "if", "label", "not", "in", "self", ".", "_VALID_LABELS", ":", "\n", "                ", "raise", "ConfigurationError", "(", "\"unknown feature label type: {}\"", ".", "format", "(", "label", ")", ")", "\n", "", "", "if", "coding_scheme", "not", "in", "(", "\"IOB1\"", ",", "\"BIOUL\"", ")", ":", "\n", "            ", "raise", "ConfigurationError", "(", "\"unknown coding_scheme: {}\"", ".", "format", "(", "coding_scheme", ")", ")", "\n", "\n", "", "self", ".", "tag_label", "=", "tag_label", "\n", "self", ".", "feature_labels", "=", "set", "(", "feature_labels", ")", "\n", "self", ".", "coding_scheme", "=", "coding_scheme", "\n", "self", ".", "label_namespace", "=", "label_namespace", "\n", "self", ".", "_original_coding_scheme", "=", "\"IOB1\"", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.dataset_readers.conll2003.Conll2003DatasetReader._read": [[103, 124], ["allennlp.common.file_utils.cached_path", "open", "logger.info", "itertools.groupby", "line.strip().split", "list", "allennlp.data.tokenizers.Token", "conll2003.Conll2003DatasetReader.text_to_instance", "zip", "line.strip"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.file_utils.cached_path", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.dataset_readers.multiprocess_dataset_reader.logger.info", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.coreference_resolution.conll.ConllCorefReader.text_to_instance"], ["", "@", "overrides", "\n", "def", "_read", "(", "self", ",", "file_path", ":", "str", ")", "->", "Iterable", "[", "Instance", "]", ":", "\n", "# if `file_path` is a URL, redirect to the cache", "\n", "        ", "file_path", "=", "cached_path", "(", "file_path", ")", "\n", "\n", "with", "open", "(", "file_path", ",", "\"r\"", ")", "as", "data_file", ":", "\n", "            ", "logger", ".", "info", "(", "\"Reading instances from lines in file at: %s\"", ",", "file_path", ")", "\n", "\n", "# Group into alternative divider / sentence chunks.", "\n", "for", "is_divider", ",", "lines", "in", "itertools", ".", "groupby", "(", "data_file", ",", "_is_divider", ")", ":", "\n", "# Ignore the divider chunks, so that `lines` corresponds to the words", "\n", "# of a single sentence.", "\n", "                ", "if", "not", "is_divider", ":", "\n", "                    ", "fields", "=", "[", "line", ".", "strip", "(", ")", ".", "split", "(", ")", "for", "line", "in", "lines", "]", "\n", "# unzipping trick returns tuples, but our Fields need lists", "\n", "fields", "=", "[", "list", "(", "field", ")", "for", "field", "in", "zip", "(", "*", "fields", ")", "]", "\n", "tokens_", ",", "pos_tags", ",", "chunk_tags", ",", "ner_tags", "=", "fields", "\n", "# TextField requires `Token` objects", "\n", "tokens", "=", "[", "Token", "(", "token", ")", "for", "token", "in", "tokens_", "]", "\n", "\n", "yield", "self", ".", "text_to_instance", "(", "tokens", ",", "pos_tags", ",", "chunk_tags", ",", "ner_tags", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.dataset_readers.conll2003.Conll2003DatasetReader.text_to_instance": [[125, 201], ["allennlp.data.fields.TextField", "allennlp.data.fields.MetadataField", "allennlp.data.instance.Instance", "allennlp.data.fields.SequenceLabelField", "allennlp.data.fields.SequenceLabelField", "allennlp.data.fields.SequenceLabelField", "allennlp.data.fields.SequenceLabelField", "allennlp.data.dataset_readers.dataset_utils.to_bioul", "allennlp.data.dataset_readers.dataset_utils.to_bioul", "allennlp.common.checks.ConfigurationError", "allennlp.common.checks.ConfigurationError", "allennlp.common.checks.ConfigurationError", "allennlp.data.fields.SequenceLabelField", "allennlp.data.fields.SequenceLabelField"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.dataset_utils.span_utils.to_bioul", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.dataset_utils.span_utils.to_bioul"], ["", "", "", "", "def", "text_to_instance", "(", "# type: ignore", "\n", "self", ",", "\n", "tokens", ":", "List", "[", "Token", "]", ",", "\n", "pos_tags", ":", "List", "[", "str", "]", "=", "None", ",", "\n", "chunk_tags", ":", "List", "[", "str", "]", "=", "None", ",", "\n", "ner_tags", ":", "List", "[", "str", "]", "=", "None", ",", "\n", ")", "->", "Instance", ":", "\n", "        ", "\"\"\"\n        We take `pre-tokenized` input here, because we don't have a tokenizer in this class.\n        \"\"\"", "\n", "\n", "sequence", "=", "TextField", "(", "tokens", ",", "self", ".", "_token_indexers", ")", "\n", "instance_fields", ":", "Dict", "[", "str", ",", "Field", "]", "=", "{", "\"tokens\"", ":", "sequence", "}", "\n", "instance_fields", "[", "\"metadata\"", "]", "=", "MetadataField", "(", "{", "\"words\"", ":", "[", "x", ".", "text", "for", "x", "in", "tokens", "]", "}", ")", "\n", "\n", "# Recode the labels if necessary.", "\n", "if", "self", ".", "coding_scheme", "==", "\"BIOUL\"", ":", "\n", "            ", "coded_chunks", "=", "(", "\n", "to_bioul", "(", "chunk_tags", ",", "encoding", "=", "self", ".", "_original_coding_scheme", ")", "\n", "if", "chunk_tags", "is", "not", "None", "\n", "else", "None", "\n", ")", "\n", "coded_ner", "=", "(", "\n", "to_bioul", "(", "ner_tags", ",", "encoding", "=", "self", ".", "_original_coding_scheme", ")", "\n", "if", "ner_tags", "is", "not", "None", "\n", "else", "None", "\n", ")", "\n", "", "else", ":", "\n", "# the default IOB1", "\n", "            ", "coded_chunks", "=", "chunk_tags", "\n", "coded_ner", "=", "ner_tags", "\n", "\n", "# Add \"feature labels\" to instance", "\n", "", "if", "\"pos\"", "in", "self", ".", "feature_labels", ":", "\n", "            ", "if", "pos_tags", "is", "None", ":", "\n", "                ", "raise", "ConfigurationError", "(", "\n", "\"Dataset reader was specified to use pos_tags as \"", "\n", "\"features. Pass them to text_to_instance.\"", "\n", ")", "\n", "", "instance_fields", "[", "\"pos_tags\"", "]", "=", "SequenceLabelField", "(", "\n", "pos_tags", ",", "sequence", ",", "\"pos_tags\"", "\n", ")", "\n", "", "if", "\"chunk\"", "in", "self", ".", "feature_labels", ":", "\n", "            ", "if", "coded_chunks", "is", "None", ":", "\n", "                ", "raise", "ConfigurationError", "(", "\n", "\"Dataset reader was specified to use chunk tags as \"", "\n", "\"features. Pass them to text_to_instance.\"", "\n", ")", "\n", "", "instance_fields", "[", "\"chunk_tags\"", "]", "=", "SequenceLabelField", "(", "\n", "coded_chunks", ",", "sequence", ",", "\"chunk_tags\"", "\n", ")", "\n", "", "if", "\"ner\"", "in", "self", ".", "feature_labels", ":", "\n", "            ", "if", "coded_ner", "is", "None", ":", "\n", "                ", "raise", "ConfigurationError", "(", "\n", "\"Dataset reader was specified to use NER tags as \"", "\n", "\" features. Pass them to text_to_instance.\"", "\n", ")", "\n", "", "instance_fields", "[", "\"ner_tags\"", "]", "=", "SequenceLabelField", "(", "\n", "coded_ner", ",", "sequence", ",", "\"ner_tags\"", "\n", ")", "\n", "\n", "# Add \"tag label\" to instance", "\n", "", "if", "self", ".", "tag_label", "==", "\"ner\"", "and", "coded_ner", "is", "not", "None", ":", "\n", "            ", "instance_fields", "[", "\"tags\"", "]", "=", "SequenceLabelField", "(", "\n", "coded_ner", ",", "sequence", ",", "self", ".", "label_namespace", "\n", ")", "\n", "", "elif", "self", ".", "tag_label", "==", "\"pos\"", "and", "pos_tags", "is", "not", "None", ":", "\n", "            ", "instance_fields", "[", "\"tags\"", "]", "=", "SequenceLabelField", "(", "\n", "pos_tags", ",", "sequence", ",", "self", ".", "label_namespace", "\n", ")", "\n", "", "elif", "self", ".", "tag_label", "==", "\"chunk\"", "and", "coded_chunks", "is", "not", "None", ":", "\n", "            ", "instance_fields", "[", "\"tags\"", "]", "=", "SequenceLabelField", "(", "\n", "coded_chunks", ",", "sequence", ",", "self", ".", "label_namespace", "\n", ")", "\n", "\n", "", "return", "Instance", "(", "instance_fields", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.dataset_readers.conll2003._is_divider": [[19, 29], ["line.strip", "line.split"], "function", ["None"], ["def", "_is_divider", "(", "line", ":", "str", ")", "->", "bool", ":", "\n", "    ", "empty_line", "=", "line", ".", "strip", "(", ")", "==", "\"\"", "\n", "if", "empty_line", ":", "\n", "        ", "return", "True", "\n", "", "else", ":", "\n", "        ", "first_token", "=", "line", ".", "split", "(", ")", "[", "0", "]", "\n", "if", "first_token", "==", "\"-DOCSTART-\"", ":", "\n", "            ", "return", "True", "\n", "", "else", ":", "\n", "            ", "return", "False", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.dataset_readers.next_token_lm.NextTokenLmReader.__init__": [[43, 58], ["allennlp.data.dataset_readers.dataset_reader.DatasetReader.__init__", "isinstance", "allennlp.data.tokenizers.whitespace_tokenizer.WhitespaceTokenizer", "copy.copy", "allennlp.data.token_indexers.SingleIdTokenIndexer"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.graph_util.plot.GraphPlot.__init__"], []], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.dataset_readers.next_token_lm.NextTokenLmReader._read": [[59, 74], ["logger.error", "open", "next_token_lm.NextTokenLmReader._tokenizer.tokenize", "next_token_lm.NextTokenLmReader.text_to_instance"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.tokenizers.whitespace_tokenizer.WhitespaceTokenizer.tokenize", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.coreference_resolution.conll.ConllCorefReader.text_to_instance"], []], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.dataset_readers.next_token_lm.NextTokenLmReader.text_to_instance": [[75, 98], ["allennlp.data.fields.TextField", "allennlp.data.instance.Instance", "next_token_lm.NextTokenLmReader._tokenizer.tokenize", "allennlp.data.tokenizers.Token", "allennlp.data.fields.TextField", "next_token_lm.NextTokenLmReader._targets_tokenizer.tokenize"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.tokenizers.whitespace_tokenizer.WhitespaceTokenizer.tokenize", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.tokenizers.whitespace_tokenizer.WhitespaceTokenizer.tokenize"], []], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.dataset_readers.dataset_reader._LazyInstances.__init__": [[22, 34], ["typing.Iterable.__init__"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.graph_util.plot.GraphPlot.__init__"], ["def", "__init__", "(", "\n", "self", ",", "\n", "instance_generator", ":", "Callable", "[", "[", "]", ",", "Iterable", "[", "Instance", "]", "]", ",", "\n", "cache_file", ":", "str", "=", "None", ",", "\n", "deserialize", ":", "Callable", "[", "[", "str", "]", ",", "Instance", "]", "=", "None", ",", "\n", "serialize", ":", "Callable", "[", "[", "Instance", "]", ",", "str", "]", "=", "None", ",", "\n", ")", "->", "None", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "instance_generator", "=", "instance_generator", "\n", "self", ".", "cache_file", "=", "cache_file", "\n", "self", ".", "deserialize", "=", "deserialize", "\n", "self", ".", "serialize", "=", "serialize", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.dataset_readers.dataset_reader._LazyInstances.__iter__": [[35, 56], ["os.path.exists", "open", "dataset_reader._LazyInstances.instance_generator", "isinstance", "open", "dataset_reader._LazyInstances.instance_generator", "allennlp.common.checks.ConfigurationError", "dataset_reader._LazyInstances.deserialize", "data_file.write", "data_file.write", "dataset_reader._LazyInstances.serialize"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.None.predictor.write", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.None.predictor.write"], ["", "def", "__iter__", "(", "self", ")", "->", "Iterator", "[", "Instance", "]", ":", "\n", "# Case 1: Use cached instances", "\n", "        ", "if", "self", ".", "cache_file", "is", "not", "None", "and", "os", ".", "path", ".", "exists", "(", "self", ".", "cache_file", ")", ":", "\n", "            ", "with", "open", "(", "self", ".", "cache_file", ")", "as", "data_file", ":", "\n", "                ", "for", "line", "in", "data_file", ":", "\n", "                    ", "yield", "self", ".", "deserialize", "(", "line", ")", "\n", "# Case 2: Need to cache instances", "\n", "", "", "", "elif", "self", ".", "cache_file", "is", "not", "None", ":", "\n", "            ", "with", "open", "(", "self", ".", "cache_file", ",", "\"w\"", ")", "as", "data_file", ":", "\n", "                ", "for", "instance", "in", "self", ".", "instance_generator", "(", ")", ":", "\n", "                    ", "data_file", ".", "write", "(", "self", ".", "serialize", "(", "instance", ")", ")", "\n", "data_file", ".", "write", "(", "\"\\n\"", ")", "\n", "yield", "instance", "\n", "# Case 3: No cache", "\n", "", "", "", "else", ":", "\n", "            ", "instances", "=", "self", ".", "instance_generator", "(", ")", "\n", "if", "isinstance", "(", "instances", ",", "list", ")", ":", "\n", "                ", "raise", "ConfigurationError", "(", "\n", "\"For a lazy dataset reader, _read() must return a generator\"", "\n", ")", "\n", "", "yield", "from", "instances", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.dataset_readers.dataset_reader.DatasetReader.__init__": [[87, 94], ["pathlib.Path", "os.makedirs"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "lazy", ":", "bool", "=", "False", ",", "cache_directory", ":", "str", "=", "None", ")", "->", "None", ":", "\n", "        ", "self", ".", "lazy", "=", "lazy", "\n", "if", "cache_directory", ":", "\n", "            ", "self", ".", "_cache_directory", "=", "pathlib", ".", "Path", "(", "cache_directory", ")", "\n", "os", ".", "makedirs", "(", "self", ".", "_cache_directory", ",", "exist_ok", "=", "True", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "_cache_directory", "=", "None", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.dataset_readers.dataset_reader.DatasetReader.read": [[95, 155], ["getattr", "logger.warning", "dataset_reader.DatasetReader._get_cache_location_for_file_path", "dataset_reader._LazyInstances", "os.path.exists", "dataset_reader.DatasetReader._instances_from_cache_file", "dataset_reader.DatasetReader._read", "isinstance", "allennlp.common.checks.ConfigurationError", "logger.info", "dataset_reader.DatasetReader._instances_to_cache_file", "dataset_reader.DatasetReader._read", "os.path.exists", "allennlp.common.Tqdm.tqdm"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.dataset_readers.dataset_reader.DatasetReader._get_cache_location_for_file_path", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.dataset_readers.dataset_reader.DatasetReader._instances_from_cache_file", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.coreference_resolution.conll.ConllCorefReader._read", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.dataset_readers.multiprocess_dataset_reader.logger.info", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.dataset_readers.dataset_reader.DatasetReader._instances_to_cache_file", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.coreference_resolution.conll.ConllCorefReader._read", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.tqdm.Tqdm.tqdm"], ["", "", "def", "read", "(", "self", ",", "file_path", ":", "str", ")", "->", "Iterable", "[", "Instance", "]", ":", "\n", "        ", "\"\"\"\n        Returns an `Iterable` containing all the instances\n        in the specified dataset.\n\n        If `self.lazy` is False, this calls `self._read()`,\n        ensures that the result is a list, then returns the resulting list.\n\n        If `self.lazy` is True, this returns an object whose\n        `__iter__` method calls `self._read()` each iteration.\n        In this case your implementation of `_read()` must also be lazy\n        (that is, not load all instances into memory at once), otherwise\n        you will get a `ConfigurationError`.\n\n        In either case, the returned `Iterable` can be iterated\n        over multiple times. It's unlikely you want to override this function,\n        but if you do your result should likewise be repeatedly iterable.\n        \"\"\"", "\n", "lazy", "=", "getattr", "(", "self", ",", "\"lazy\"", ",", "None", ")", "\n", "\n", "if", "lazy", "is", "None", ":", "\n", "            ", "logger", ".", "warning", "(", "\n", "\"DatasetReader.lazy is not set, \"", "\n", "\"did you forget to call the superclass constructor?\"", "\n", ")", "\n", "\n", "", "if", "self", ".", "_cache_directory", ":", "\n", "            ", "cache_file", "=", "self", ".", "_get_cache_location_for_file_path", "(", "file_path", ")", "\n", "", "else", ":", "\n", "            ", "cache_file", "=", "None", "\n", "\n", "", "if", "lazy", ":", "\n", "            ", "return", "_LazyInstances", "(", "\n", "lambda", ":", "self", ".", "_read", "(", "file_path", ")", ",", "\n", "cache_file", ",", "\n", "self", ".", "deserialize_instance", ",", "\n", "self", ".", "serialize_instance", ",", "\n", ")", "\n", "", "else", ":", "\n", "# First we read the instances, either from a cache or from the original file.", "\n", "            ", "if", "cache_file", "and", "os", ".", "path", ".", "exists", "(", "cache_file", ")", ":", "\n", "                ", "instances", "=", "self", ".", "_instances_from_cache_file", "(", "cache_file", ")", "\n", "", "else", ":", "\n", "                ", "instances", "=", "self", ".", "_read", "(", "file_path", ")", "\n", "\n", "# Then some validation.", "\n", "", "if", "not", "isinstance", "(", "instances", ",", "list", ")", ":", "\n", "                ", "instances", "=", "[", "instance", "for", "instance", "in", "Tqdm", ".", "tqdm", "(", "instances", ")", "]", "\n", "", "if", "not", "instances", ":", "\n", "                ", "raise", "ConfigurationError", "(", "\n", "\"No instances were read from the given filepath {}. \"", "\n", "\"Is the path correct?\"", ".", "format", "(", "file_path", ")", "\n", ")", "\n", "\n", "# And finally we write to the cache if we need to.", "\n", "", "if", "cache_file", "and", "not", "os", ".", "path", ".", "exists", "(", "cache_file", ")", ":", "\n", "                ", "logger", ".", "info", "(", "f\"Caching instances to {cache_file}\"", ")", "\n", "self", ".", "_instances_to_cache_file", "(", "cache_file", ",", "instances", ")", "\n", "\n", "", "return", "instances", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.dataset_readers.dataset_reader.DatasetReader._get_cache_location_for_file_path": [[156, 158], ["str", "allennlp.common.util.flatten_filename", "str"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.util.flatten_filename"], ["", "", "def", "_get_cache_location_for_file_path", "(", "self", ",", "file_path", ":", "str", ")", "->", "str", ":", "\n", "        ", "return", "str", "(", "self", ".", "_cache_directory", "/", "util", ".", "flatten_filename", "(", "str", "(", "file_path", ")", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.dataset_readers.dataset_reader.DatasetReader._read": [[159, 167], ["None"], "methods", ["None"], ["", "def", "_read", "(", "self", ",", "file_path", ":", "str", ")", "->", "Iterable", "[", "Instance", "]", ":", "\n", "        ", "\"\"\"\n        Reads the instances from the given file_path and returns them as an\n        `Iterable` (which could be a list or could be a generator).\n        You are strongly encouraged to use a generator, so that users can\n        read a dataset in a lazy way, if they so choose.\n        \"\"\"", "\n", "raise", "NotImplementedError", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.dataset_readers.dataset_reader.DatasetReader._instances_from_cache_file": [[168, 172], ["open", "dataset_reader.DatasetReader.deserialize_instance", "line.strip"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.dataset_readers.dataset_reader.DatasetReader.deserialize_instance"], ["", "def", "_instances_from_cache_file", "(", "self", ",", "cache_filename", ":", "str", ")", "->", "Iterable", "[", "Instance", "]", ":", "\n", "        ", "with", "open", "(", "cache_filename", ",", "\"r\"", ")", "as", "cache_file", ":", "\n", "            ", "for", "line", "in", "cache_file", ":", "\n", "                ", "yield", "self", ".", "deserialize_instance", "(", "line", ".", "strip", "(", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.dataset_readers.dataset_reader.DatasetReader._instances_to_cache_file": [[173, 177], ["open", "allennlp.common.Tqdm.tqdm", "cache.write", "dataset_reader.DatasetReader.serialize_instance"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.tqdm.Tqdm.tqdm", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.None.predictor.write", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.dataset_readers.dataset_reader.DatasetReader.serialize_instance"], ["", "", "", "def", "_instances_to_cache_file", "(", "self", ",", "cache_filename", ",", "instances", ")", "->", "None", ":", "\n", "        ", "with", "open", "(", "cache_filename", ",", "\"w\"", ")", "as", "cache", ":", "\n", "            ", "for", "instance", "in", "Tqdm", ".", "tqdm", "(", "instances", ")", ":", "\n", "                ", "cache", ".", "write", "(", "self", ".", "serialize_instance", "(", "instance", ")", "+", "\"\\n\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.dataset_readers.dataset_reader.DatasetReader.text_to_instance": [[178, 196], ["None"], "methods", ["None"], ["", "", "", "def", "text_to_instance", "(", "self", ",", "*", "inputs", ")", "->", "Instance", ":", "\n", "        ", "\"\"\"\n        Does whatever tokenization or processing is necessary to go from textual input to an\n        `Instance`.  The primary intended use for this is with a\n        :class:`~allennlp.predictors.predictor.Predictor`, which gets text input as a JSON\n        object and needs to process it to be input to a model.\n\n        The intent here is to share code between :func:`_read` and what happens at\n        model serving time, or any other time you want to make a prediction from new data.  We need\n        to process the data in the same way it was done at training time.  Allowing the\n        `DatasetReader` to process new text lets us accomplish this, as we can just call\n        `DatasetReader.text_to_instance` when serving predictions.\n\n        The input type here is rather vaguely specified, unfortunately.  The `Predictor` will\n        have to make some assumptions about the kind of `DatasetReader` that it's using, in order\n        to pass it the right information.\n        \"\"\"", "\n", "raise", "NotImplementedError", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.dataset_readers.dataset_reader.DatasetReader.serialize_instance": [[197, 206], ["jsonpickle.dumps"], "methods", ["None"], ["", "def", "serialize_instance", "(", "self", ",", "instance", ":", "Instance", ")", "->", "str", ":", "\n", "        ", "\"\"\"\n        Serializes an `Instance` to a string.  We use this for caching the processed data.\n\n        The default implementation is to use `jsonpickle`.  If you would like some other format\n        for your pre-processed data, override this method.\n        \"\"\"", "\n", "\n", "return", "jsonpickle", ".", "dumps", "(", "instance", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.dataset_readers.dataset_reader.DatasetReader.deserialize_instance": [[207, 217], ["jsonpickle.loads"], "methods", ["None"], ["", "def", "deserialize_instance", "(", "self", ",", "string", ":", "str", ")", "->", "Instance", ":", "\n", "        ", "\"\"\"\n        Deserializes an `Instance` from a string.  We use this when reading processed data from a\n        cache.\n\n        The default implementation is to use `jsonpickle`.  If you would like some other format\n        for your pre-processed data, override this method.\n        \"\"\"", "\n", "\n", "return", "jsonpickle", ".", "loads", "(", "string", ")", "# type: ignore", "\n", "", "", ""]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.dataset_utils.ace.ACESentence.__init__": [[39, 52], ["None"], "methods", ["None"], ["def", "__init__", "(", "\n", "self", ",", "\n", "words", ":", "List", "[", "str", "]", ",", "\n", "mention_tags", ":", "List", "[", "str", "]", ",", "\n", "relations", ":", "List", "[", "Tuple", "[", "str", ",", "List", "[", "str", "]", "]", "]", ",", "\n", "last_head_token_relations", ":", "List", "[", "Tuple", "[", "str", ",", "List", "[", "str", "]", "]", "]", ",", "\n", "coref_spans", ":", "Set", "[", "TypedSpan", "]", ",", "\n", ")", ":", "\n", "        ", "self", ".", "words", "=", "words", "\n", "self", ".", "mention_tags", "=", "mention_tags", "\n", "self", ".", "relations", "=", "relations", "\n", "self", ".", "last_head_token_relations", "=", "last_head_token_relations", "\n", "self", ".", "coref_spans", "=", "coref_spans", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.dataset_utils.ace.ACE.dataset_iterator": [[61, 67], ["ace.ACE.dataset_path_iterator", "ace.ACE.sentence_iterator"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.dataset_utils.ontonotes.Ontonotes.dataset_path_iterator", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.dataset_utils.ontonotes.Ontonotes.sentence_iterator"], ["def", "dataset_iterator", "(", "self", ",", "file_path", ":", "str", ")", "->", "Iterator", "[", "ACESentence", "]", ":", "\n", "        ", "\"\"\"\n        An iterator over the entire dataset, yielding all sentences processed.\n        \"\"\"", "\n", "for", "conll_file", "in", "self", ".", "dataset_path_iterator", "(", "file_path", ")", ":", "\n", "            ", "yield", "from", "self", ".", "sentence_iterator", "(", "conll_file", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.dataset_utils.ace.ACE.dataset_path_iterator": [[68, 83], ["logger.info", "list", "os.walk", "data_file.endswith", "os.path.join"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.dataset_readers.multiprocess_dataset_reader.logger.info", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.dataset_readers.multiprocess_dataset_reader.QIterable.join"], ["", "", "@", "staticmethod", "\n", "def", "dataset_path_iterator", "(", "file_path", ":", "str", ")", "->", "Iterator", "[", "str", "]", ":", "\n", "        ", "\"\"\"\n        An iterator returning file_paths in a directory\n        containing CONLL-formatted files.\n        \"\"\"", "\n", "logger", ".", "info", "(", "\n", "\"Reading ACE CONLL-like sentences from dataset files at: %s\"", ",", "file_path", "\n", ")", "\n", "for", "root", ",", "_", ",", "files", "in", "list", "(", "os", ".", "walk", "(", "file_path", ")", ")", ":", "\n", "            ", "for", "data_file", "in", "files", ":", "\n", "                ", "if", "not", "data_file", ".", "endswith", "(", "\"like_conll\"", ")", ":", "\n", "                    ", "continue", "\n", "\n", "", "yield", "os", ".", "path", ".", "join", "(", "root", ",", "data_file", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.dataset_utils.ace.ACE.dataset_document_iterator": [[84, 108], ["codecs.open", "line.strip.strip.strip", "line.strip.strip.startswith", "conll_rows.append", "line.strip.strip.startswith", "document.append", "ace.ACE._conll_rows_to_sentence"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.dataset_utils.ontonotes.Ontonotes._conll_rows_to_sentence"], ["", "", "", "def", "dataset_document_iterator", "(", "self", ",", "file_path", ":", "str", ")", "->", "Iterator", "[", "List", "[", "ACESentence", "]", "]", ":", "\n", "        ", "\"\"\"\n        An iterator over CONLL-formatted files which yields documents, regardless\n        of the number of document annotations in a particular file.\n        \"\"\"", "\n", "with", "codecs", ".", "open", "(", "file_path", ",", "\"r\"", ",", "encoding", "=", "\"utf8\"", ")", "as", "open_file", ":", "\n", "            ", "conll_rows", "=", "[", "]", "\n", "document", ":", "List", "[", "ACESentence", "]", "=", "[", "]", "\n", "for", "line", "in", "open_file", ":", "\n", "                ", "line", "=", "line", ".", "strip", "(", ")", "\n", "if", "line", "!=", "\"\"", "and", "not", "line", ".", "startswith", "(", "\"#\"", ")", ":", "\n", "# Non-empty line. Collect the annotation.", "\n", "                    ", "conll_rows", ".", "append", "(", "line", ")", "\n", "", "else", ":", "\n", "                    ", "if", "conll_rows", ":", "\n", "                        ", "document", ".", "append", "(", "self", ".", "_conll_rows_to_sentence", "(", "conll_rows", ")", ")", "\n", "conll_rows", "=", "[", "]", "\n", "", "", "if", "line", ".", "startswith", "(", "\"#end document\"", ")", ":", "\n", "                    ", "yield", "document", "\n", "document", "=", "[", "]", "\n", "", "", "if", "document", ":", "\n", "# Collect any stragglers or files which might not", "\n", "# have the '#end document' format for the end of the file.", "\n", "                ", "yield", "document", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.dataset_utils.ace.ACE.sentence_iterator": [[109, 116], ["ace.ACE.dataset_document_iterator"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.dataset_utils.ontonotes.Ontonotes.dataset_document_iterator"], ["", "", "", "def", "sentence_iterator", "(", "self", ",", "file_path", ":", "str", ")", "->", "Iterator", "[", "ACESentence", "]", ":", "\n", "        ", "\"\"\"\n        An iterator over the sentences in an individual CONLL formatted file.\n        \"\"\"", "\n", "for", "document", "in", "self", ".", "dataset_document_iterator", "(", "file_path", ")", ":", "\n", "            ", "for", "sentence", "in", "document", ":", "\n", "                ", "yield", "sentence", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.dataset_utils.ace.ACE._conll_rows_to_sentence": [[117, 183], ["collections.defaultdict", "collections.defaultdict", "enumerate", "allennlp.data.dataset_readers.dataset_utils.iob1_to_bioul", "ace.ACESentence", "row.split", "ace.ACE._process_span_annotations_for_word", "ace.ACE._process_coref_span_annotations_for_word", "sentence.append", "allennlp.data.dataset_readers.dataset_utils.iob1_to_bioul", "last_head_token_relations.append", "bioul_relations.append", "clusters.items", "reformatted_frame.append", "reformatted_frame.append"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.dataset_utils.span_utils.iob1_to_bioul", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.dataset_utils.ontonotes.Ontonotes._process_span_annotations_for_word", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.dataset_utils.ontonotes.Ontonotes._process_coref_span_annotations_for_word", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.dataset_utils.span_utils.iob1_to_bioul"], ["", "", "", "def", "_conll_rows_to_sentence", "(", "self", ",", "conll_rows", ":", "List", "[", "str", "]", ")", "->", "ACESentence", ":", "\n", "        ", "sentence", ":", "List", "[", "str", "]", "=", "[", "]", "\n", "mention_tags", ":", "List", "[", "str", "]", "=", "[", "]", "\n", "\n", "span_labels", ":", "List", "[", "List", "[", "str", "]", "]", "=", "[", "]", "\n", "current_span_labels", ":", "List", "[", "str", "]", "=", "[", "]", "\n", "\n", "# Cluster id -> List of (start_index, end_index) spans.", "\n", "clusters", ":", "DefaultDict", "[", "int", ",", "List", "[", "Tuple", "[", "int", ",", "int", "]", "]", "]", "=", "defaultdict", "(", "list", ")", "\n", "# Cluster id -> List of start_indices which are open for this id.", "\n", "coref_stacks", ":", "DefaultDict", "[", "int", ",", "List", "[", "int", "]", "]", "=", "defaultdict", "(", "list", ")", "\n", "\n", "for", "index", ",", "row", "in", "enumerate", "(", "conll_rows", ")", ":", "\n", "            ", "conll_components", "=", "row", ".", "split", "(", ")", "\n", "\n", "word", "=", "conll_components", "[", "1", "]", "\n", "\n", "if", "not", "span_labels", ":", "\n", "                ", "span_labels", "=", "[", "[", "]", "for", "_", "in", "conll_components", "[", "2", ":", "-", "1", "]", "]", "\n", "current_span_labels", "=", "[", "None", "for", "_", "in", "conll_components", "[", "2", ":", "-", "1", "]", "]", "\n", "", "self", ".", "_process_span_annotations_for_word", "(", "\n", "annotations", "=", "conll_components", "[", "2", ":", "-", "1", "]", ",", "\n", "span_labels", "=", "span_labels", ",", "\n", "current_span_labels", "=", "current_span_labels", ",", "\n", ")", "\n", "\n", "# Process coref", "\n", "self", ".", "_process_coref_span_annotations_for_word", "(", "\n", "conll_components", "[", "-", "1", "]", ",", "index", ",", "clusters", ",", "coref_stacks", "\n", ")", "\n", "\n", "sentence", ".", "append", "(", "word", ")", "\n", "\n", "", "mention_tags", "=", "iob1_to_bioul", "(", "span_labels", "[", "0", "]", ")", "\n", "\n", "# Process coref clusters", "\n", "coref_span_tuples", ":", "Set", "[", "TypedSpan", "]", "=", "{", "\n", "(", "cluster_id", ",", "span", ")", "\n", "for", "cluster_id", ",", "span_list", "in", "clusters", ".", "items", "(", ")", "\n", "for", "span", "in", "span_list", "\n", "}", "\n", "\n", "# Reformat the labels to only keep the the last token of the head", "\n", "# Cf paper, we model relation between last tokens of heads.", "\n", "last_head_token_relations", "=", "[", "]", "\n", "bioul_relations", "=", "[", "]", "\n", "\n", "for", "relation_frame", "in", "span_labels", "[", "1", ":", "]", ":", "\n", "            ", "bioul_relation_frame", "=", "iob1_to_bioul", "(", "relation_frame", ")", "\n", "\n", "reformatted_frame", "=", "[", "]", "\n", "for", "annotation", "in", "bioul_relation_frame", ":", "\n", "                ", "if", "annotation", "[", ":", "2", "]", "in", "[", "\"L-\"", ",", "\"U-\"", "]", ":", "\n", "                    ", "reformatted_frame", ".", "append", "(", "annotation", "[", "2", ":", "]", ")", "\n", "", "else", ":", "\n", "                    ", "reformatted_frame", ".", "append", "(", "\"*\"", ")", "\n", "\n", "", "", "last_head_token_relations", ".", "append", "(", "reformatted_frame", ")", "\n", "bioul_relations", ".", "append", "(", "bioul_relation_frame", ")", "\n", "\n", "", "return", "ACESentence", "(", "\n", "sentence", ",", "\n", "mention_tags", ",", "\n", "bioul_relations", ",", "\n", "last_head_token_relations", ",", "\n", "coref_span_tuples", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.dataset_utils.ace.ACE._process_mention_tags": [[185, 205], ["annotation.strip", "labels.append"], "methods", ["None"], ["", "@", "staticmethod", "\n", "def", "_process_mention_tags", "(", "annotations", ":", "List", "[", "str", "]", ")", ":", "\n", "        ", "\"\"\"\n        Read and pre-process the entity mention tags as a formatted in CoNll-NER-style.\n        \"\"\"", "\n", "labels", "=", "[", "]", "\n", "current_span_label", "=", "None", "\n", "for", "annotation", "in", "annotations", ":", "\n", "            ", "label", "=", "annotation", ".", "strip", "(", "\"()*\"", ")", "\n", "if", "\"(\"", "in", "annotation", ":", "\n", "                ", "bio_label", "=", "\"B-\"", "+", "label", "\n", "current_span_label", "=", "label", "\n", "", "elif", "current_span_label", "is", "not", "None", ":", "\n", "                ", "bio_label", "=", "\"I-\"", "+", "current_span_label", "\n", "", "else", ":", "\n", "                ", "bio_label", "=", "\"O\"", "\n", "", "if", "\")\"", "in", "annotation", ":", "\n", "                ", "current_span_label", "=", "None", "\n", "", "labels", ".", "append", "(", "bio_label", ")", "\n", "", "return", "labels", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.dataset_utils.ace.ACE._process_span_annotations_for_word": [[206, 248], ["enumerate", "annotation.strip", "span_labels[].append", "span_labels[].append", "span_labels[].append"], "methods", ["None"], ["", "@", "staticmethod", "\n", "def", "_process_span_annotations_for_word", "(", "\n", "annotations", ":", "List", "[", "str", "]", ",", "\n", "span_labels", ":", "List", "[", "List", "[", "str", "]", "]", ",", "\n", "current_span_labels", ":", "List", "[", "Optional", "[", "str", "]", "]", ",", "\n", ")", "->", "None", ":", "\n", "        ", "\"\"\"\n        Given a sequence of different label types for a single word and the current\n        span label we are inside, compute the BIO tag for each label and append to a list.\n\n        Parameters\n        ----------\n        annotations: ``List[str]``\n            A list of labels to compute BIO tags for.\n        span_labels : ``List[List[str]]``\n            A list of lists, one for each annotation, to incrementally collect\n            the BIO tags for a sequence.\n        current_span_labels : ``List[Optional[str]]``\n            The currently open span per annotation type, or ``None`` if there is no open span.\n        \"\"\"", "\n", "for", "annotation_index", ",", "annotation", "in", "enumerate", "(", "annotations", ")", ":", "\n", "# strip all bracketing information to", "\n", "# get the actual propbank label.", "\n", "            ", "label", "=", "annotation", ".", "strip", "(", "\"()*\"", ")", "\n", "\n", "if", "\"(\"", "in", "annotation", ":", "\n", "# Entering into a span for a particular semantic role label.", "\n", "# We append the label and set the current span for this annotation.", "\n", "                ", "bio_label", "=", "\"B-\"", "+", "label", "\n", "span_labels", "[", "annotation_index", "]", ".", "append", "(", "bio_label", ")", "\n", "current_span_labels", "[", "annotation_index", "]", "=", "label", "\n", "", "elif", "current_span_labels", "[", "annotation_index", "]", "is", "not", "None", ":", "\n", "# If there's no '(' token, but the current_span_label is not None,", "\n", "# then we are inside a span.", "\n", "                ", "bio_label", "=", "\"I-\"", "+", "current_span_labels", "[", "annotation_index", "]", "\n", "span_labels", "[", "annotation_index", "]", ".", "append", "(", "bio_label", ")", "\n", "", "else", ":", "\n", "# We're outside a span.", "\n", "                ", "span_labels", "[", "annotation_index", "]", ".", "append", "(", "\"O\"", ")", "\n", "# Exiting a span, so we reset the current span label for this annotation.", "\n", "", "if", "\")\"", "in", "annotation", ":", "\n", "                ", "current_span_labels", "[", "annotation_index", "]", "=", "None", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.dataset_utils.ace.ACE._process_coref_span_annotations_for_word": [[249, 299], ["label.split", "int", "coref_stacks[].pop", "clusters[].append", "int", "clusters[].append", "int", "coref_stacks[].append"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.params.Params.pop"], ["", "", "", "@", "staticmethod", "\n", "def", "_process_coref_span_annotations_for_word", "(", "\n", "label", ":", "str", ",", "\n", "word_index", ":", "int", ",", "\n", "clusters", ":", "DefaultDict", "[", "int", ",", "List", "[", "Tuple", "[", "int", ",", "int", "]", "]", "]", ",", "\n", "coref_stacks", ":", "DefaultDict", "[", "int", ",", "List", "[", "int", "]", "]", ",", "\n", ")", "->", "None", ":", "\n", "        ", "\"\"\"\n        For a given coref label, add it to a currently open span(s), complete a span(s) or\n        ignore it, if it is outside of all spans. This method mutates the clusters and coref_stacks\n        dictionaries.\n\n        Parameters\n        ----------\n        label : ``str``\n            The coref label for this word.\n        word_index : ``int``\n            The word index into the sentence.\n        clusters : ``DefaultDict[int, List[Tuple[int, int]]]``\n            A dictionary mapping cluster ids to lists of inclusive spans into the\n            sentence.\n        coref_stacks: ``DefaultDict[int, List[int]]``\n            Stacks for each cluster id to hold the start indices of active spans (spans\n            which we are inside of when processing a given word). Spans with the same id\n            can be nested, which is why we collect these opening spans on a stack, e.g:\n\n            [Greg, the baker who referred to [himself]_ID1 as 'the bread man']_ID1\n        \"\"\"", "\n", "if", "label", "!=", "\"-\"", ":", "\n", "            ", "for", "segment", "in", "label", ".", "split", "(", "\"|\"", ")", ":", "\n", "# The conll representation of coref spans allows spans to", "\n", "# overlap. If spans end or begin at the same word, they are", "\n", "# separated by a \"|\".", "\n", "                ", "if", "segment", "[", "0", "]", "==", "\"(\"", ":", "\n", "# The span begins at this word.", "\n", "                    ", "if", "segment", "[", "-", "1", "]", "==", "\")\"", ":", "\n", "# The span begins and ends at this word (single word span).", "\n", "                        ", "cluster_id", "=", "int", "(", "segment", "[", "1", ":", "-", "1", "]", ")", "\n", "clusters", "[", "cluster_id", "]", ".", "append", "(", "(", "word_index", ",", "word_index", ")", ")", "\n", "", "else", ":", "\n", "# The span is starting, so we record the index of the word.", "\n", "                        ", "cluster_id", "=", "int", "(", "segment", "[", "1", ":", "]", ")", "\n", "coref_stacks", "[", "cluster_id", "]", ".", "append", "(", "word_index", ")", "\n", "", "", "else", ":", "\n", "# The span for this id is ending, but didn't start at this word.", "\n", "# Retrieve the start index from the document state and", "\n", "# add the span to the clusters for this id.", "\n", "                    ", "cluster_id", "=", "int", "(", "segment", "[", ":", "-", "1", "]", ")", "\n", "start", "=", "coref_stacks", "[", "cluster_id", "]", ".", "pop", "(", ")", "\n", "clusters", "[", "cluster_id", "]", ".", "append", "(", "(", "start", ",", "word_index", ")", ")", "\n", "", "", "", "", "", ""]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.dataset_utils.span_utils.InvalidTagSequence.__init__": [[10, 13], ["Exception.__init__"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.graph_util.plot.GraphPlot.__init__"], ["    ", "def", "__init__", "(", "self", ",", "tag_sequence", "=", "None", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "tag_sequence", "=", "tag_sequence", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.dataset_utils.span_utils.InvalidTagSequence.__str__": [[14, 16], ["None"], "methods", ["None"], ["", "def", "__str__", "(", "self", ")", ":", "\n", "        ", "return", "\" \"", ".", "join", "(", "self", ".", "tag_sequence", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.dataset_utils.span_utils.enumerate_spans": [[21, 70], ["range", "len", "len", "min", "min", "range", "len", "len", "filter_function", "spans.append", "slice"], "function", ["None"], ["def", "enumerate_spans", "(", "\n", "sentence", ":", "List", "[", "T", "]", ",", "\n", "offset", ":", "int", "=", "0", ",", "\n", "max_span_width", ":", "int", "=", "None", ",", "\n", "min_span_width", ":", "int", "=", "1", ",", "\n", "filter_function", ":", "Callable", "[", "[", "List", "[", "T", "]", "]", ",", "bool", "]", "=", "None", ",", "\n", ")", "->", "List", "[", "Tuple", "[", "int", ",", "int", "]", "]", ":", "\n", "    ", "\"\"\"\n    Given a sentence, return all token spans within the sentence. Spans are `inclusive`.\n    Additionally, you can provide a maximum and minimum span width, which will be used\n    to exclude spans outside of this range.\n\n    Finally, you can provide a function mapping `List[T] -> bool`, which will\n    be applied to every span to decide whether that span should be included. This\n    allows filtering by length, regex matches, pos tags or any Spacy `Token`\n    attributes, for example.\n\n    # Parameters\n\n    sentence : `List[T]`, required.\n        The sentence to generate spans for. The type is generic, as this function\n        can be used with strings, or Spacy `Tokens` or other sequences.\n    offset : `int`, optional (default = 0)\n        A numeric offset to add to all span start and end indices. This is helpful\n        if the sentence is part of a larger structure, such as a document, which\n        the indices need to respect.\n    max_span_width : `int`, optional (default = None)\n        The maximum length of spans which should be included. Defaults to len(sentence).\n    min_span_width : `int`, optional (default = 1)\n        The minimum length of spans which should be included. Defaults to 1.\n    filter_function : `Callable[[List[T]], bool]`, optional (default = None)\n        A function mapping sequences of the passed type T to a boolean value.\n        If `True`, the span is included in the returned spans from the\n        sentence, otherwise it is excluded..\n    \"\"\"", "\n", "max_span_width", "=", "max_span_width", "or", "len", "(", "sentence", ")", "\n", "filter_function", "=", "filter_function", "or", "(", "lambda", "x", ":", "True", ")", "\n", "spans", ":", "List", "[", "Tuple", "[", "int", ",", "int", "]", "]", "=", "[", "]", "\n", "\n", "for", "start_index", "in", "range", "(", "len", "(", "sentence", ")", ")", ":", "\n", "        ", "last_end_index", "=", "min", "(", "start_index", "+", "max_span_width", ",", "len", "(", "sentence", ")", ")", "\n", "first_end_index", "=", "min", "(", "start_index", "+", "min_span_width", "-", "1", ",", "len", "(", "sentence", ")", ")", "\n", "for", "end_index", "in", "range", "(", "first_end_index", ",", "last_end_index", ")", ":", "\n", "            ", "start", "=", "offset", "+", "start_index", "\n", "end", "=", "offset", "+", "end_index", "\n", "# add 1 to end index because span indices are inclusive.", "\n", "if", "filter_function", "(", "sentence", "[", "slice", "(", "start_index", ",", "end_index", "+", "1", ")", "]", ")", ":", "\n", "                ", "spans", ".", "append", "(", "(", "start", ",", "end", ")", ")", "\n", "", "", "", "return", "spans", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.dataset_utils.span_utils.bio_tags_to_spans": [[72, 144], ["set", "enumerate", "list", "spans.add", "span_utils.InvalidTagSequence", "spans.add", "spans.add", "spans.add"], "function", ["None"], ["", "def", "bio_tags_to_spans", "(", "\n", "tag_sequence", ":", "List", "[", "str", "]", ",", "classes_to_ignore", ":", "List", "[", "str", "]", "=", "None", "\n", ")", "->", "List", "[", "TypedStringSpan", "]", ":", "\n", "    ", "\"\"\"\n    Given a sequence corresponding to BIO tags, extracts spans.\n    Spans are inclusive and can be of zero length, representing a single word span.\n    Ill-formed spans are also included (i.e those which do not start with a \"B-LABEL\"),\n    as otherwise it is possible to get a perfect precision score whilst still predicting\n    ill-formed spans in addition to the correct spans. This function works properly when\n    the spans are unlabeled (i.e., your labels are simply \"B\", \"I\", and \"O\").\n\n    # Parameters\n\n    tag_sequence : List[str], required.\n        The integer class labels for a sequence.\n    classes_to_ignore : List[str], optional (default = None).\n        A list of string class labels `excluding` the bio tag\n        which should be ignored when extracting spans.\n\n    # Returns\n\n    spans : List[TypedStringSpan]\n        The typed, extracted spans from the sequence, in the format (label, (span_start, span_end)).\n        Note that the label `does not` contain any BIO tag prefixes.\n    \"\"\"", "\n", "classes_to_ignore", "=", "classes_to_ignore", "or", "[", "]", "\n", "spans", ":", "Set", "[", "Tuple", "[", "str", ",", "Tuple", "[", "int", ",", "int", "]", "]", "]", "=", "set", "(", ")", "\n", "span_start", "=", "0", "\n", "span_end", "=", "0", "\n", "active_conll_tag", "=", "None", "\n", "for", "index", ",", "string_tag", "in", "enumerate", "(", "tag_sequence", ")", ":", "\n", "# Actual BIO tag.", "\n", "        ", "bio_tag", "=", "string_tag", "[", "0", "]", "\n", "if", "bio_tag", "not", "in", "[", "\"B\"", ",", "\"I\"", ",", "\"O\"", "]", ":", "\n", "            ", "raise", "InvalidTagSequence", "(", "tag_sequence", ")", "\n", "", "conll_tag", "=", "string_tag", "[", "2", ":", "]", "\n", "if", "bio_tag", "==", "\"O\"", "or", "conll_tag", "in", "classes_to_ignore", ":", "\n", "# The span has ended.", "\n", "            ", "if", "active_conll_tag", "is", "not", "None", ":", "\n", "                ", "spans", ".", "add", "(", "(", "active_conll_tag", ",", "(", "span_start", ",", "span_end", ")", ")", ")", "\n", "", "active_conll_tag", "=", "None", "\n", "# We don't care about tags we are", "\n", "# told to ignore, so we do nothing.", "\n", "continue", "\n", "", "elif", "bio_tag", "==", "\"B\"", ":", "\n", "# We are entering a new span; reset indices", "\n", "# and active tag to new span.", "\n", "            ", "if", "active_conll_tag", "is", "not", "None", ":", "\n", "                ", "spans", ".", "add", "(", "(", "active_conll_tag", ",", "(", "span_start", ",", "span_end", ")", ")", ")", "\n", "", "active_conll_tag", "=", "conll_tag", "\n", "span_start", "=", "index", "\n", "span_end", "=", "index", "\n", "", "elif", "bio_tag", "==", "\"I\"", "and", "conll_tag", "==", "active_conll_tag", ":", "\n", "# We're inside a span.", "\n", "            ", "span_end", "+=", "1", "\n", "", "else", ":", "\n", "# This is the case the bio label is an \"I\", but either:", "\n", "# 1) the span hasn't started - i.e. an ill formed span.", "\n", "# 2) The span is an I tag for a different conll annotation.", "\n", "# We'll process the previous span if it exists, but also", "\n", "# include this span. This is important, because otherwise,", "\n", "# a model may get a perfect F1 score whilst still including", "\n", "# false positive ill-formed spans.", "\n", "            ", "if", "active_conll_tag", "is", "not", "None", ":", "\n", "                ", "spans", ".", "add", "(", "(", "active_conll_tag", ",", "(", "span_start", ",", "span_end", ")", ")", ")", "\n", "", "active_conll_tag", "=", "conll_tag", "\n", "span_start", "=", "index", "\n", "span_end", "=", "index", "\n", "# Last token might have been a part of a valid span.", "\n", "", "", "if", "active_conll_tag", "is", "not", "None", ":", "\n", "        ", "spans", ".", "add", "(", "(", "active_conll_tag", ",", "(", "span_start", ",", "span_end", ")", ")", ")", "\n", "", "return", "list", "(", "spans", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.dataset_utils.span_utils.iob1_tags_to_spans": [[146, 208], ["set", "enumerate", "list", "spans.add", "span_utils.InvalidTagSequence", "span_utils._iob1_start_of_chunk", "spans.add", "spans.add"], "function", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.dataset_utils.span_utils._iob1_start_of_chunk"], ["", "def", "iob1_tags_to_spans", "(", "\n", "tag_sequence", ":", "List", "[", "str", "]", ",", "classes_to_ignore", ":", "List", "[", "str", "]", "=", "None", "\n", ")", "->", "List", "[", "TypedStringSpan", "]", ":", "\n", "    ", "\"\"\"\n    Given a sequence corresponding to IOB1 tags, extracts spans.\n    Spans are inclusive and can be of zero length, representing a single word span.\n    Ill-formed spans are also included (i.e., those where \"B-LABEL\" is not preceded\n    by \"I-LABEL\" or \"B-LABEL\").\n\n    # Parameters\n\n    tag_sequence : List[str], required.\n        The integer class labels for a sequence.\n    classes_to_ignore : List[str], optional (default = None).\n        A list of string class labels `excluding` the bio tag\n        which should be ignored when extracting spans.\n\n    # Returns\n\n    spans : List[TypedStringSpan]\n        The typed, extracted spans from the sequence, in the format (label, (span_start, span_end)).\n        Note that the label `does not` contain any BIO tag prefixes.\n    \"\"\"", "\n", "classes_to_ignore", "=", "classes_to_ignore", "or", "[", "]", "\n", "spans", ":", "Set", "[", "Tuple", "[", "str", ",", "Tuple", "[", "int", ",", "int", "]", "]", "]", "=", "set", "(", ")", "\n", "span_start", "=", "0", "\n", "span_end", "=", "0", "\n", "active_conll_tag", "=", "None", "\n", "prev_bio_tag", "=", "None", "\n", "prev_conll_tag", "=", "None", "\n", "for", "index", ",", "string_tag", "in", "enumerate", "(", "tag_sequence", ")", ":", "\n", "        ", "curr_bio_tag", "=", "string_tag", "[", "0", "]", "\n", "curr_conll_tag", "=", "string_tag", "[", "2", ":", "]", "\n", "\n", "if", "curr_bio_tag", "not", "in", "[", "\"B\"", ",", "\"I\"", ",", "\"O\"", "]", ":", "\n", "            ", "raise", "InvalidTagSequence", "(", "tag_sequence", ")", "\n", "", "if", "curr_bio_tag", "==", "\"O\"", "or", "curr_conll_tag", "in", "classes_to_ignore", ":", "\n", "# The span has ended.", "\n", "            ", "if", "active_conll_tag", "is", "not", "None", ":", "\n", "                ", "spans", ".", "add", "(", "(", "active_conll_tag", ",", "(", "span_start", ",", "span_end", ")", ")", ")", "\n", "", "active_conll_tag", "=", "None", "\n", "", "elif", "_iob1_start_of_chunk", "(", "\n", "prev_bio_tag", ",", "prev_conll_tag", ",", "curr_bio_tag", ",", "curr_conll_tag", "\n", ")", ":", "\n", "# We are entering a new span; reset indices", "\n", "# and active tag to new span.", "\n", "            ", "if", "active_conll_tag", "is", "not", "None", ":", "\n", "                ", "spans", ".", "add", "(", "(", "active_conll_tag", ",", "(", "span_start", ",", "span_end", ")", ")", ")", "\n", "", "active_conll_tag", "=", "curr_conll_tag", "\n", "span_start", "=", "index", "\n", "span_end", "=", "index", "\n", "", "else", ":", "\n", "# bio_tag == \"I\" and curr_conll_tag == active_conll_tag", "\n", "# We're continuing a span.", "\n", "            ", "span_end", "+=", "1", "\n", "\n", "", "prev_bio_tag", "=", "string_tag", "[", "0", "]", "\n", "prev_conll_tag", "=", "string_tag", "[", "2", ":", "]", "\n", "# Last token might have been a part of a valid span.", "\n", "", "if", "active_conll_tag", "is", "not", "None", ":", "\n", "        ", "spans", ".", "add", "(", "(", "active_conll_tag", ",", "(", "span_start", ",", "span_end", ")", ")", ")", "\n", "", "return", "list", "(", "spans", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.dataset_utils.span_utils._iob1_start_of_chunk": [[210, 223], ["None"], "function", ["None"], ["", "def", "_iob1_start_of_chunk", "(", "\n", "prev_bio_tag", ":", "Optional", "[", "str", "]", ",", "\n", "prev_conll_tag", ":", "Optional", "[", "str", "]", ",", "\n", "curr_bio_tag", ":", "str", ",", "\n", "curr_conll_tag", ":", "str", ",", "\n", ")", "->", "bool", ":", "\n", "    ", "if", "curr_bio_tag", "==", "\"B\"", ":", "\n", "        ", "return", "True", "\n", "", "if", "curr_bio_tag", "==", "\"I\"", "and", "prev_bio_tag", "==", "\"O\"", ":", "\n", "        ", "return", "True", "\n", "", "if", "curr_bio_tag", "!=", "\"O\"", "and", "prev_conll_tag", "!=", "curr_conll_tag", ":", "\n", "        ", "return", "True", "\n", "", "return", "False", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.dataset_utils.span_utils.bioul_tags_to_spans": [[225, 270], ["len", "spans.append", "spans.append", "span_utils.InvalidTagSequence", "label.partition", "len", "span_utils.InvalidTagSequence", "span_utils.InvalidTagSequence", "label.partition"], "function", ["None"], ["", "def", "bioul_tags_to_spans", "(", "\n", "tag_sequence", ":", "List", "[", "str", "]", ",", "classes_to_ignore", ":", "List", "[", "str", "]", "=", "None", "\n", ")", "->", "List", "[", "TypedStringSpan", "]", ":", "\n", "    ", "\"\"\"\n    Given a sequence corresponding to BIOUL tags, extracts spans.\n    Spans are inclusive and can be of zero length, representing a single word span.\n    Ill-formed spans are not allowed and will raise `InvalidTagSequence`.\n    This function works properly when the spans are unlabeled (i.e., your labels are\n    simply \"B\", \"I\", \"O\", \"U\", and \"L\").\n\n    # Parameters\n\n    tag_sequence : `List[str]`, required.\n        The tag sequence encoded in BIOUL, e.g. [\"B-PER\", \"L-PER\", \"O\"].\n    classes_to_ignore : `List[str]`, optional (default = None).\n        A list of string class labels `excluding` the bio tag\n        which should be ignored when extracting spans.\n\n    # Returns\n\n    spans : `List[TypedStringSpan]`\n        The typed, extracted spans from the sequence, in the format (label, (span_start, span_end)).\n    \"\"\"", "\n", "spans", "=", "[", "]", "\n", "classes_to_ignore", "=", "classes_to_ignore", "or", "[", "]", "\n", "index", "=", "0", "\n", "while", "index", "<", "len", "(", "tag_sequence", ")", ":", "\n", "        ", "label", "=", "tag_sequence", "[", "index", "]", "\n", "if", "label", "[", "0", "]", "==", "\"U\"", ":", "\n", "            ", "spans", ".", "append", "(", "(", "label", ".", "partition", "(", "\"-\"", ")", "[", "2", "]", ",", "(", "index", ",", "index", ")", ")", ")", "\n", "", "elif", "label", "[", "0", "]", "==", "\"B\"", ":", "\n", "            ", "start", "=", "index", "\n", "while", "label", "[", "0", "]", "!=", "\"L\"", ":", "\n", "                ", "index", "+=", "1", "\n", "if", "index", ">=", "len", "(", "tag_sequence", ")", ":", "\n", "                    ", "raise", "InvalidTagSequence", "(", "tag_sequence", ")", "\n", "", "label", "=", "tag_sequence", "[", "index", "]", "\n", "if", "not", "(", "label", "[", "0", "]", "==", "\"I\"", "or", "label", "[", "0", "]", "==", "\"L\"", ")", ":", "\n", "                    ", "raise", "InvalidTagSequence", "(", "tag_sequence", ")", "\n", "", "", "spans", ".", "append", "(", "(", "label", ".", "partition", "(", "\"-\"", ")", "[", "2", "]", ",", "(", "start", ",", "index", ")", ")", ")", "\n", "", "else", ":", "\n", "            ", "if", "label", "!=", "\"O\"", ":", "\n", "                ", "raise", "InvalidTagSequence", "(", "tag_sequence", ")", "\n", "", "", "index", "+=", "1", "\n", "", "return", "[", "span", "for", "span", "in", "spans", "if", "span", "[", "0", "]", "not", "in", "classes_to_ignore", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.dataset_utils.span_utils.iob1_to_bioul": [[272, 278], ["warnings.warn", "span_utils.to_bioul"], "function", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.dataset_utils.span_utils.to_bioul"], ["", "def", "iob1_to_bioul", "(", "tag_sequence", ":", "List", "[", "str", "]", ")", "->", "List", "[", "str", "]", ":", "\n", "    ", "warnings", ".", "warn", "(", "\n", "\"iob1_to_bioul has been replaced with 'to_bioul' to allow more encoding options.\"", ",", "\n", "FutureWarning", ",", "\n", ")", "\n", "return", "to_bioul", "(", "tag_sequence", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.dataset_utils.span_utils.to_bioul": [[280, 384], ["allennlp.common.checks.ConfigurationError", "list", "in_stack.pop", "span_utils.to_bioul.replace_label"], "function", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.params.Params.pop"], ["", "def", "to_bioul", "(", "tag_sequence", ":", "List", "[", "str", "]", ",", "encoding", ":", "str", "=", "\"IOB1\"", ")", "->", "List", "[", "str", "]", ":", "\n", "    ", "\"\"\"\n    Given a tag sequence encoded with IOB1 labels, recode to BIOUL.\n\n    In the IOB1 scheme, I is a token inside a span, O is a token outside\n    a span and B is the beginning of span immediately following another\n    span of the same type.\n\n    In the BIO scheme, I is a token inside a span, O is a token outside\n    a span and B is the beginning of a span.\n\n    # Parameters\n\n    tag_sequence : `List[str]`, required.\n        The tag sequence encoded in IOB1, e.g. [\"I-PER\", \"I-PER\", \"O\"].\n    encoding : `str`, optional, (default = `IOB1`).\n        The encoding type to convert from. Must be either \"IOB1\" or \"BIO\".\n\n    # Returns\n\n    bioul_sequence : `List[str]`\n        The tag sequence encoded in IOB1, e.g. [\"B-PER\", \"L-PER\", \"O\"].\n    \"\"\"", "\n", "if", "encoding", "not", "in", "{", "\"IOB1\"", ",", "\"BIO\"", "}", ":", "\n", "        ", "raise", "ConfigurationError", "(", "f\"Invalid encoding {encoding} passed to 'to_bioul'.\"", ")", "\n", "\n", "", "def", "replace_label", "(", "full_label", ",", "new_label", ")", ":", "\n", "# example: full_label = 'I-PER', new_label = 'U', returns 'U-PER'", "\n", "        ", "parts", "=", "list", "(", "full_label", ".", "partition", "(", "\"-\"", ")", ")", "\n", "parts", "[", "0", "]", "=", "new_label", "\n", "return", "\"\"", ".", "join", "(", "parts", ")", "\n", "\n", "", "def", "pop_replace_append", "(", "in_stack", ",", "out_stack", ",", "new_label", ")", ":", "\n", "# pop the last element from in_stack, replace the label, append", "\n", "# to out_stack", "\n", "        ", "tag", "=", "in_stack", ".", "pop", "(", ")", "\n", "new_tag", "=", "replace_label", "(", "tag", ",", "new_label", ")", "\n", "out_stack", ".", "append", "(", "new_tag", ")", "\n", "\n", "", "def", "process_stack", "(", "stack", ",", "out_stack", ")", ":", "\n", "# process a stack of labels, add them to out_stack", "\n", "        ", "if", "len", "(", "stack", ")", "==", "1", ":", "\n", "# just a U token", "\n", "            ", "pop_replace_append", "(", "stack", ",", "out_stack", ",", "\"U\"", ")", "\n", "", "else", ":", "\n", "# need to code as BIL", "\n", "            ", "recoded_stack", "=", "[", "]", "\n", "pop_replace_append", "(", "stack", ",", "recoded_stack", ",", "\"L\"", ")", "\n", "while", "len", "(", "stack", ")", ">=", "2", ":", "\n", "                ", "pop_replace_append", "(", "stack", ",", "recoded_stack", ",", "\"I\"", ")", "\n", "", "pop_replace_append", "(", "stack", ",", "recoded_stack", ",", "\"B\"", ")", "\n", "recoded_stack", ".", "reverse", "(", ")", "\n", "out_stack", ".", "extend", "(", "recoded_stack", ")", "\n", "\n", "# Process the tag_sequence one tag at a time, adding spans to a stack,", "\n", "# then recode them.", "\n", "", "", "bioul_sequence", "=", "[", "]", "\n", "stack", ":", "List", "[", "str", "]", "=", "[", "]", "\n", "\n", "for", "label", "in", "tag_sequence", ":", "\n", "# need to make a dict like", "\n", "# token = {'token': 'Matt', \"labels\": {'conll2003': \"B-PER\"}", "\n", "#                   'gold': 'I-PER'}", "\n", "# where 'gold' is the raw value from the CoNLL data set", "\n", "\n", "        ", "if", "label", "==", "\"O\"", "and", "len", "(", "stack", ")", "==", "0", ":", "\n", "            ", "bioul_sequence", ".", "append", "(", "label", ")", "\n", "", "elif", "label", "==", "\"O\"", "and", "len", "(", "stack", ")", ">", "0", ":", "\n", "# need to process the entries on the stack plus this one", "\n", "            ", "process_stack", "(", "stack", ",", "bioul_sequence", ")", "\n", "bioul_sequence", ".", "append", "(", "label", ")", "\n", "", "elif", "label", "[", "0", "]", "==", "\"I\"", ":", "\n", "# check if the previous type is the same as this one", "\n", "# if it is then append to stack", "\n", "# otherwise this start a new entity if the type", "\n", "# is different", "\n", "            ", "if", "len", "(", "stack", ")", "==", "0", ":", "\n", "                ", "if", "encoding", "==", "\"BIO\"", ":", "\n", "                    ", "raise", "InvalidTagSequence", "(", "tag_sequence", ")", "\n", "", "stack", ".", "append", "(", "label", ")", "\n", "", "else", ":", "\n", "# check if the previous type is the same as this one", "\n", "                ", "this_type", "=", "label", ".", "partition", "(", "\"-\"", ")", "[", "2", "]", "\n", "prev_type", "=", "stack", "[", "-", "1", "]", ".", "partition", "(", "\"-\"", ")", "[", "2", "]", "\n", "if", "this_type", "==", "prev_type", ":", "\n", "                    ", "stack", ".", "append", "(", "label", ")", "\n", "", "else", ":", "\n", "                    ", "if", "encoding", "==", "\"BIO\"", ":", "\n", "                        ", "raise", "InvalidTagSequence", "(", "tag_sequence", ")", "\n", "# a new entity", "\n", "", "process_stack", "(", "stack", ",", "bioul_sequence", ")", "\n", "stack", ".", "append", "(", "label", ")", "\n", "", "", "", "elif", "label", "[", "0", "]", "==", "\"B\"", ":", "\n", "            ", "if", "len", "(", "stack", ")", ">", "0", ":", "\n", "                ", "process_stack", "(", "stack", ",", "bioul_sequence", ")", "\n", "", "stack", ".", "append", "(", "label", ")", "\n", "", "else", ":", "\n", "            ", "raise", "InvalidTagSequence", "(", "tag_sequence", ")", "\n", "\n", "# process the stack", "\n", "", "", "if", "len", "(", "stack", ")", ">", "0", ":", "\n", "        ", "process_stack", "(", "stack", ",", "bioul_sequence", ")", "\n", "\n", "", "return", "bioul_sequence", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.dataset_utils.span_utils.bmes_tags_to_spans": [[386, 446], ["enumerate", "span_utils.bmes_tags_to_spans.extract_bmes_tag_label"], "function", ["None"], ["", "def", "bmes_tags_to_spans", "(", "\n", "tag_sequence", ":", "List", "[", "str", "]", ",", "classes_to_ignore", ":", "List", "[", "str", "]", "=", "None", "\n", ")", "->", "List", "[", "TypedStringSpan", "]", ":", "\n", "    ", "\"\"\"\n    Given a sequence corresponding to BMES tags, extracts spans.\n    Spans are inclusive and can be of zero length, representing a single word span.\n    Ill-formed spans are also included (i.e those which do not start with a \"B-LABEL\"),\n    as otherwise it is possible to get a perfect precision score whilst still predicting\n    ill-formed spans in addition to the correct spans.\n    This function works properly when the spans are unlabeled (i.e., your labels are\n    simply \"B\", \"M\", \"E\" and \"S\").\n\n    # Parameters\n\n    tag_sequence : List[str], required.\n        The integer class labels for a sequence.\n    classes_to_ignore : List[str], optional (default = None).\n        A list of string class labels `excluding` the bio tag\n        which should be ignored when extracting spans.\n\n    # Returns\n\n    spans : List[TypedStringSpan]\n        The typed, extracted spans from the sequence, in the format (label, (span_start, span_end)).\n        Note that the label `does not` contain any BIO tag prefixes.\n    \"\"\"", "\n", "\n", "def", "extract_bmes_tag_label", "(", "text", ")", ":", "\n", "        ", "bmes_tag", "=", "text", "[", "0", "]", "\n", "label", "=", "text", "[", "2", ":", "]", "\n", "return", "bmes_tag", ",", "label", "\n", "\n", "", "spans", ":", "List", "[", "Tuple", "[", "str", ",", "List", "[", "int", "]", "]", "]", "=", "[", "]", "\n", "prev_bmes_tag", ":", "Optional", "[", "str", "]", "=", "None", "\n", "for", "index", ",", "tag", "in", "enumerate", "(", "tag_sequence", ")", ":", "\n", "        ", "bmes_tag", ",", "label", "=", "extract_bmes_tag_label", "(", "tag", ")", "\n", "if", "bmes_tag", "in", "(", "\"B\"", ",", "\"S\"", ")", ":", "\n", "# Regardless of tag, we start a new span when reaching B & S.", "\n", "            ", "spans", ".", "append", "(", "(", "label", ",", "[", "index", ",", "index", "]", ")", ")", "\n", "", "elif", "(", "\n", "bmes_tag", "in", "(", "\"M\"", ",", "\"E\"", ")", "\n", "and", "prev_bmes_tag", "in", "(", "\"B\"", ",", "\"M\"", ")", "\n", "and", "spans", "[", "-", "1", "]", "[", "0", "]", "==", "label", "\n", ")", ":", "\n", "# Only expand the span if", "\n", "# 1. Valid transition: B/M -> M/E.", "\n", "# 2. Matched label.", "\n", "            ", "spans", "[", "-", "1", "]", "[", "1", "]", "[", "1", "]", "=", "index", "\n", "", "else", ":", "\n", "# Best effort split for invalid span.", "\n", "            ", "spans", ".", "append", "(", "(", "label", ",", "[", "index", ",", "index", "]", ")", ")", "\n", "# update previous BMES tag.", "\n", "", "prev_bmes_tag", "=", "bmes_tag", "\n", "\n", "", "classes_to_ignore", "=", "classes_to_ignore", "or", "[", "]", "\n", "return", "[", "\n", "# to tuple.", "\n", "(", "span", "[", "0", "]", ",", "(", "span", "[", "1", "]", "[", "0", "]", ",", "span", "[", "1", "]", "[", "1", "]", ")", ")", "\n", "for", "span", "in", "spans", "\n", "if", "span", "[", "0", "]", "not", "in", "classes_to_ignore", "\n", "]", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.dataset_utils.ontonotes.OntonotesSentence.__init__": [[57, 85], ["None"], "methods", ["None"], ["def", "__init__", "(", "\n", "self", ",", "\n", "document_id", ":", "str", ",", "\n", "sentence_id", ":", "int", ",", "\n", "words", ":", "List", "[", "str", "]", ",", "\n", "pos_tags", ":", "List", "[", "str", "]", ",", "\n", "parse_tree", ":", "Optional", "[", "Tree", "]", ",", "\n", "predicate_lemmas", ":", "List", "[", "Optional", "[", "str", "]", "]", ",", "\n", "predicate_framenet_ids", ":", "List", "[", "Optional", "[", "str", "]", "]", ",", "\n", "word_senses", ":", "List", "[", "Optional", "[", "float", "]", "]", ",", "\n", "speakers", ":", "List", "[", "Optional", "[", "str", "]", "]", ",", "\n", "named_entities", ":", "List", "[", "str", "]", ",", "\n", "srl_frames", ":", "List", "[", "Tuple", "[", "str", ",", "List", "[", "str", "]", "]", "]", ",", "\n", "coref_spans", ":", "Set", "[", "TypedSpan", "]", ",", "\n", ")", "->", "None", ":", "\n", "\n", "        ", "self", ".", "document_id", "=", "document_id", "\n", "self", ".", "sentence_id", "=", "sentence_id", "\n", "self", ".", "words", "=", "words", "\n", "self", ".", "pos_tags", "=", "pos_tags", "\n", "self", ".", "parse_tree", "=", "parse_tree", "\n", "self", ".", "predicate_lemmas", "=", "predicate_lemmas", "\n", "self", ".", "predicate_framenet_ids", "=", "predicate_framenet_ids", "\n", "self", ".", "word_senses", "=", "word_senses", "\n", "self", ".", "speakers", "=", "speakers", "\n", "self", ".", "named_entities", "=", "named_entities", "\n", "self", ".", "srl_frames", "=", "srl_frames", "\n", "self", ".", "coref_spans", "=", "coref_spans", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.dataset_utils.ontonotes.Ontonotes.dataset_iterator": [[181, 187], ["ontonotes.Ontonotes.dataset_path_iterator", "ontonotes.Ontonotes.sentence_iterator"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.dataset_utils.ontonotes.Ontonotes.dataset_path_iterator", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.dataset_utils.ontonotes.Ontonotes.sentence_iterator"], ["def", "dataset_iterator", "(", "self", ",", "file_path", ":", "str", ")", "->", "Iterator", "[", "OntonotesSentence", "]", ":", "\n", "        ", "\"\"\"\n        An iterator over the entire dataset, yielding all sentences processed.\n        \"\"\"", "\n", "for", "conll_file", "in", "self", ".", "dataset_path_iterator", "(", "file_path", ")", ":", "\n", "            ", "yield", "from", "self", ".", "sentence_iterator", "(", "conll_file", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.dataset_utils.ontonotes.Ontonotes.dataset_path_iterator": [[188, 204], ["logger.info", "list", "os.walk", "data_file.endswith", "os.path.join"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.dataset_readers.multiprocess_dataset_reader.logger.info", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.dataset_readers.multiprocess_dataset_reader.QIterable.join"], ["", "", "@", "staticmethod", "\n", "def", "dataset_path_iterator", "(", "file_path", ":", "str", ")", "->", "Iterator", "[", "str", "]", ":", "\n", "        ", "\"\"\"\n        An iterator returning file_paths in a directory\n        containing CONLL-formatted files.\n        \"\"\"", "\n", "logger", ".", "info", "(", "\"Reading CONLL sentences from dataset files at: %s\"", ",", "file_path", ")", "\n", "for", "root", ",", "_", ",", "files", "in", "list", "(", "os", ".", "walk", "(", "file_path", ")", ")", ":", "\n", "            ", "for", "data_file", "in", "files", ":", "\n", "# These are a relic of the dataset pre-processing. Every", "\n", "# file will be duplicated - one file called filename.gold_skel", "\n", "# and one generated from the preprocessing called filename.gold_conll.", "\n", "                ", "if", "not", "data_file", ".", "endswith", "(", "\"gold_conll\"", ")", ":", "\n", "                    ", "continue", "\n", "\n", "", "yield", "os", ".", "path", ".", "join", "(", "root", ",", "data_file", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.dataset_utils.ontonotes.Ontonotes.dataset_document_iterator": [[205, 233], ["codecs.open", "line.strip.strip.strip", "line.strip.strip.startswith", "conll_rows.append", "line.strip.strip.startswith", "document.append", "ontonotes.Ontonotes._conll_rows_to_sentence"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.dataset_utils.ontonotes.Ontonotes._conll_rows_to_sentence"], ["", "", "", "def", "dataset_document_iterator", "(", "\n", "self", ",", "file_path", ":", "str", "\n", ")", "->", "Iterator", "[", "List", "[", "OntonotesSentence", "]", "]", ":", "\n", "        ", "\"\"\"\n        An iterator over CONLL formatted files which yields documents, regardless\n        of the number of document annotations in a particular file. This is useful\n        for conll data which has been preprocessed, such as the preprocessing which\n        takes place for the 2012 CONLL Coreference Resolution task.\n        \"\"\"", "\n", "with", "codecs", ".", "open", "(", "file_path", ",", "\"r\"", ",", "encoding", "=", "\"utf8\"", ")", "as", "open_file", ":", "\n", "            ", "conll_rows", "=", "[", "]", "\n", "document", ":", "List", "[", "OntonotesSentence", "]", "=", "[", "]", "\n", "for", "line", "in", "open_file", ":", "\n", "                ", "line", "=", "line", ".", "strip", "(", ")", "\n", "if", "line", "!=", "\"\"", "and", "not", "line", ".", "startswith", "(", "\"#\"", ")", ":", "\n", "# Non-empty line. Collect the annotation.", "\n", "                    ", "conll_rows", ".", "append", "(", "line", ")", "\n", "", "else", ":", "\n", "                    ", "if", "conll_rows", ":", "\n", "                        ", "document", ".", "append", "(", "self", ".", "_conll_rows_to_sentence", "(", "conll_rows", ")", ")", "\n", "conll_rows", "=", "[", "]", "\n", "", "", "if", "line", ".", "startswith", "(", "\"#end document\"", ")", ":", "\n", "                    ", "yield", "document", "\n", "document", "=", "[", "]", "\n", "", "", "if", "document", ":", "\n", "# Collect any stragglers or files which might not", "\n", "# have the '#end document' format for the end of the file.", "\n", "                ", "yield", "document", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.dataset_utils.ontonotes.Ontonotes.sentence_iterator": [[234, 241], ["ontonotes.Ontonotes.dataset_document_iterator"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.dataset_utils.ontonotes.Ontonotes.dataset_document_iterator"], ["", "", "", "def", "sentence_iterator", "(", "self", ",", "file_path", ":", "str", ")", "->", "Iterator", "[", "OntonotesSentence", "]", ":", "\n", "        ", "\"\"\"\n        An iterator over the sentences in an individual CONLL formatted file.\n        \"\"\"", "\n", "for", "document", "in", "self", ".", "dataset_document_iterator", "(", "file_path", ")", ":", "\n", "            ", "for", "sentence", "in", "document", ":", "\n", "                ", "yield", "sentence", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.dataset_utils.ontonotes.Ontonotes._conll_rows_to_sentence": [[242, 373], ["collections.defaultdict", "collections.defaultdict", "enumerate", "all", "ontonotes.OntonotesSentence", "row.split", "int", "ontonotes.Ontonotes._process_span_annotations_for_word", "any", "ontonotes.Ontonotes._process_coref_span_annotations_for_word", "sentence.append", "pos_tags.append", "parse_pieces.append", "predicate_lemmas.append", "predicate_framenet_ids.append", "word_senses.append", "speakers.append", "nltk.Tree.fromstring", "parse_piece.split", "verbal_predicates.append", "zip", "clusters.items", "right_hand_side.count", "float"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.dataset_utils.ontonotes.Ontonotes._process_span_annotations_for_word", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.dataset_utils.ontonotes.Ontonotes._process_coref_span_annotations_for_word"], ["", "", "", "def", "_conll_rows_to_sentence", "(", "self", ",", "conll_rows", ":", "List", "[", "str", "]", ")", "->", "OntonotesSentence", ":", "\n", "        ", "document_id", ":", "str", "=", "None", "\n", "sentence_id", ":", "int", "=", "None", "\n", "# The words in the sentence.", "\n", "sentence", ":", "List", "[", "str", "]", "=", "[", "]", "\n", "# The pos tags of the words in the sentence.", "\n", "pos_tags", ":", "List", "[", "str", "]", "=", "[", "]", "\n", "# the pieces of the parse tree.", "\n", "parse_pieces", ":", "List", "[", "str", "]", "=", "[", "]", "\n", "# The lemmatised form of the words in the sentence which", "\n", "# have SRL or word sense information.", "\n", "predicate_lemmas", ":", "List", "[", "str", "]", "=", "[", "]", "\n", "# The FrameNet ID of the predicate.", "\n", "predicate_framenet_ids", ":", "List", "[", "str", "]", "=", "[", "]", "\n", "# The sense of the word, if available.", "\n", "word_senses", ":", "List", "[", "float", "]", "=", "[", "]", "\n", "# The current speaker, if available.", "\n", "speakers", ":", "List", "[", "str", "]", "=", "[", "]", "\n", "\n", "verbal_predicates", ":", "List", "[", "str", "]", "=", "[", "]", "\n", "span_labels", ":", "List", "[", "List", "[", "str", "]", "]", "=", "[", "]", "\n", "current_span_labels", ":", "List", "[", "str", "]", "=", "[", "]", "\n", "\n", "# Cluster id -> List of (start_index, end_index) spans.", "\n", "clusters", ":", "DefaultDict", "[", "int", ",", "List", "[", "Tuple", "[", "int", ",", "int", "]", "]", "]", "=", "defaultdict", "(", "list", ")", "\n", "# Cluster id -> List of start_indices which are open for this id.", "\n", "coref_stacks", ":", "DefaultDict", "[", "int", ",", "List", "[", "int", "]", "]", "=", "defaultdict", "(", "list", ")", "\n", "\n", "for", "index", ",", "row", "in", "enumerate", "(", "conll_rows", ")", ":", "\n", "            ", "conll_components", "=", "row", ".", "split", "(", ")", "\n", "\n", "document_id", "=", "conll_components", "[", "0", "]", "\n", "sentence_id", "=", "int", "(", "conll_components", "[", "1", "]", ")", "\n", "word", "=", "conll_components", "[", "3", "]", "\n", "pos_tag", "=", "conll_components", "[", "4", "]", "\n", "parse_piece", "=", "conll_components", "[", "5", "]", "\n", "\n", "# Replace brackets in text and pos tags", "\n", "# with a different token for parse trees.", "\n", "if", "pos_tag", "!=", "\"XX\"", "and", "word", "!=", "\"XX\"", ":", "\n", "                ", "if", "word", "==", "\"(\"", ":", "\n", "                    ", "parse_word", "=", "\"-LRB-\"", "\n", "", "elif", "word", "==", "\")\"", ":", "\n", "                    ", "parse_word", "=", "\"-RRB-\"", "\n", "", "else", ":", "\n", "                    ", "parse_word", "=", "word", "\n", "", "if", "pos_tag", "==", "\"(\"", ":", "\n", "                    ", "pos_tag", "=", "\"-LRB-\"", "\n", "", "if", "pos_tag", "==", "\")\"", ":", "\n", "                    ", "pos_tag", "=", "\"-RRB-\"", "\n", "", "(", "left_brackets", ",", "right_hand_side", ")", "=", "parse_piece", ".", "split", "(", "\"*\"", ")", "\n", "# only keep ')' if there are nested brackets with nothing in them.", "\n", "right_brackets", "=", "right_hand_side", ".", "count", "(", "\")\"", ")", "*", "\")\"", "\n", "parse_piece", "=", "(", "\n", "f\"{left_brackets} ({pos_tag} {parse_word}) {right_brackets}\"", "\n", ")", "\n", "", "else", ":", "\n", "# There are some bad annotations in the CONLL data.", "\n", "# They contain no information, so to make this explicit,", "\n", "# we just set the parse piece to be None which will result", "\n", "# in the overall parse tree being None.", "\n", "                ", "parse_piece", "=", "None", "\n", "\n", "", "lemmatised_word", "=", "conll_components", "[", "6", "]", "\n", "framenet_id", "=", "conll_components", "[", "7", "]", "\n", "word_sense", "=", "conll_components", "[", "8", "]", "\n", "speaker", "=", "conll_components", "[", "9", "]", "\n", "\n", "if", "not", "span_labels", ":", "\n", "# If this is the first word in the sentence, create", "\n", "# empty lists to collect the NER and SRL BIO labels.", "\n", "# We can't do this upfront, because we don't know how many", "\n", "# components we are collecting, as a sentence can have", "\n", "# variable numbers of SRL frames.", "\n", "                ", "span_labels", "=", "[", "[", "]", "for", "_", "in", "conll_components", "[", "10", ":", "-", "1", "]", "]", "\n", "# Create variables representing the current label for each label", "\n", "# sequence we are collecting.", "\n", "current_span_labels", "=", "[", "None", "for", "_", "in", "conll_components", "[", "10", ":", "-", "1", "]", "]", "\n", "\n", "", "self", ".", "_process_span_annotations_for_word", "(", "\n", "conll_components", "[", "10", ":", "-", "1", "]", ",", "span_labels", ",", "current_span_labels", "\n", ")", "\n", "\n", "# If any annotation marks this word as a verb predicate,", "\n", "# we need to record its index. This also has the side effect", "\n", "# of ordering the verbal predicates by their location in the", "\n", "# sentence, automatically aligning them with the annotations.", "\n", "word_is_verbal_predicate", "=", "any", "(", "[", "\"(V\"", "in", "x", "for", "x", "in", "conll_components", "[", "11", ":", "-", "1", "]", "]", ")", "\n", "if", "word_is_verbal_predicate", ":", "\n", "                ", "verbal_predicates", ".", "append", "(", "word", ")", "\n", "\n", "", "self", ".", "_process_coref_span_annotations_for_word", "(", "\n", "conll_components", "[", "-", "1", "]", ",", "index", ",", "clusters", ",", "coref_stacks", "\n", ")", "\n", "\n", "sentence", ".", "append", "(", "word", ")", "\n", "pos_tags", ".", "append", "(", "pos_tag", ")", "\n", "parse_pieces", ".", "append", "(", "parse_piece", ")", "\n", "predicate_lemmas", ".", "append", "(", "lemmatised_word", "if", "lemmatised_word", "!=", "\"-\"", "else", "None", ")", "\n", "predicate_framenet_ids", ".", "append", "(", "framenet_id", "if", "framenet_id", "!=", "\"-\"", "else", "None", ")", "\n", "word_senses", ".", "append", "(", "float", "(", "word_sense", ")", "if", "word_sense", "!=", "\"-\"", "else", "None", ")", "\n", "speakers", ".", "append", "(", "speaker", "if", "speaker", "!=", "\"-\"", "else", "None", ")", "\n", "\n", "", "named_entities", "=", "span_labels", "[", "0", "]", "\n", "srl_frames", "=", "[", "\n", "(", "predicate", ",", "labels", ")", "\n", "for", "predicate", ",", "labels", "in", "zip", "(", "verbal_predicates", ",", "span_labels", "[", "1", ":", "]", ")", "\n", "]", "\n", "\n", "if", "all", "(", "parse_pieces", ")", ":", "\n", "            ", "parse_tree", "=", "Tree", ".", "fromstring", "(", "\"\"", ".", "join", "(", "parse_pieces", ")", ")", "\n", "", "else", ":", "\n", "            ", "parse_tree", "=", "None", "\n", "", "coref_span_tuples", ":", "Set", "[", "TypedSpan", "]", "=", "{", "\n", "(", "cluster_id", ",", "span", ")", "\n", "for", "cluster_id", ",", "span_list", "in", "clusters", ".", "items", "(", ")", "\n", "for", "span", "in", "span_list", "\n", "}", "\n", "return", "OntonotesSentence", "(", "\n", "document_id", ",", "\n", "sentence_id", ",", "\n", "sentence", ",", "\n", "pos_tags", ",", "\n", "parse_tree", ",", "\n", "predicate_lemmas", ",", "\n", "predicate_framenet_ids", ",", "\n", "word_senses", ",", "\n", "speakers", ",", "\n", "named_entities", ",", "\n", "srl_frames", ",", "\n", "coref_span_tuples", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.dataset_utils.ontonotes.Ontonotes._process_coref_span_annotations_for_word": [[375, 425], ["label.split", "int", "coref_stacks[].pop", "clusters[].append", "int", "clusters[].append", "int", "coref_stacks[].append"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.params.Params.pop"], ["", "@", "staticmethod", "\n", "def", "_process_coref_span_annotations_for_word", "(", "\n", "label", ":", "str", ",", "\n", "word_index", ":", "int", ",", "\n", "clusters", ":", "DefaultDict", "[", "int", ",", "List", "[", "Tuple", "[", "int", ",", "int", "]", "]", "]", ",", "\n", "coref_stacks", ":", "DefaultDict", "[", "int", ",", "List", "[", "int", "]", "]", ",", "\n", ")", "->", "None", ":", "\n", "        ", "\"\"\"\n        For a given coref label, add it to a currently open span(s), complete a span(s) or\n        ignore it, if it is outside of all spans. This method mutates the clusters and coref_stacks\n        dictionaries.\n\n        # Parameters\n\n        label : `str`\n            The coref label for this word.\n        word_index : `int`\n            The word index into the sentence.\n        clusters : `DefaultDict[int, List[Tuple[int, int]]]`\n            A dictionary mapping cluster ids to lists of inclusive spans into the\n            sentence.\n        coref_stacks : `DefaultDict[int, List[int]]`\n            Stacks for each cluster id to hold the start indices of active spans (spans\n            which we are inside of when processing a given word). Spans with the same id\n            can be nested, which is why we collect these opening spans on a stack, e.g:\n\n            [Greg, the baker who referred to [himself]_ID1 as 'the bread man']_ID1\n        \"\"\"", "\n", "if", "label", "!=", "\"-\"", ":", "\n", "            ", "for", "segment", "in", "label", ".", "split", "(", "\"|\"", ")", ":", "\n", "# The conll representation of coref spans allows spans to", "\n", "# overlap. If spans end or begin at the same word, they are", "\n", "# separated by a \"|\".", "\n", "                ", "if", "segment", "[", "0", "]", "==", "\"(\"", ":", "\n", "# The span begins at this word.", "\n", "                    ", "if", "segment", "[", "-", "1", "]", "==", "\")\"", ":", "\n", "# The span begins and ends at this word (single word span).", "\n", "                        ", "cluster_id", "=", "int", "(", "segment", "[", "1", ":", "-", "1", "]", ")", "\n", "clusters", "[", "cluster_id", "]", ".", "append", "(", "(", "word_index", ",", "word_index", ")", ")", "\n", "", "else", ":", "\n", "# The span is starting, so we record the index of the word.", "\n", "                        ", "cluster_id", "=", "int", "(", "segment", "[", "1", ":", "]", ")", "\n", "coref_stacks", "[", "cluster_id", "]", ".", "append", "(", "word_index", ")", "\n", "", "", "else", ":", "\n", "# The span for this id is ending, but didn't start at this word.", "\n", "# Retrieve the start index from the document state and", "\n", "# add the span to the clusters for this id.", "\n", "                    ", "cluster_id", "=", "int", "(", "segment", "[", ":", "-", "1", "]", ")", "\n", "start", "=", "coref_stacks", "[", "cluster_id", "]", ".", "pop", "(", ")", "\n", "clusters", "[", "cluster_id", "]", ".", "append", "(", "(", "start", ",", "word_index", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.dataset_utils.ontonotes.Ontonotes._process_span_annotations_for_word": [[426, 468], ["enumerate", "annotation.strip", "span_labels[].append", "span_labels[].append", "span_labels[].append"], "methods", ["None"], ["", "", "", "", "@", "staticmethod", "\n", "def", "_process_span_annotations_for_word", "(", "\n", "annotations", ":", "List", "[", "str", "]", ",", "\n", "span_labels", ":", "List", "[", "List", "[", "str", "]", "]", ",", "\n", "current_span_labels", ":", "List", "[", "Optional", "[", "str", "]", "]", ",", "\n", ")", "->", "None", ":", "\n", "        ", "\"\"\"\n        Given a sequence of different label types for a single word and the current\n        span label we are inside, compute the BIO tag for each label and append to a list.\n\n        # Parameters\n\n        annotations : `List[str]`\n            A list of labels to compute BIO tags for.\n        span_labels : `List[List[str]]`\n            A list of lists, one for each annotation, to incrementally collect\n            the BIO tags for a sequence.\n        current_span_labels : `List[Optional[str]]`\n            The currently open span per annotation type, or `None` if there is no open span.\n        \"\"\"", "\n", "for", "annotation_index", ",", "annotation", "in", "enumerate", "(", "annotations", ")", ":", "\n", "# strip all bracketing information to", "\n", "# get the actual propbank label.", "\n", "            ", "label", "=", "annotation", ".", "strip", "(", "\"()*\"", ")", "\n", "\n", "if", "\"(\"", "in", "annotation", ":", "\n", "# Entering into a span for a particular semantic role label.", "\n", "# We append the label and set the current span for this annotation.", "\n", "                ", "bio_label", "=", "\"B-\"", "+", "label", "\n", "span_labels", "[", "annotation_index", "]", ".", "append", "(", "bio_label", ")", "\n", "current_span_labels", "[", "annotation_index", "]", "=", "label", "\n", "", "elif", "current_span_labels", "[", "annotation_index", "]", "is", "not", "None", ":", "\n", "# If there's no '(' token, but the current_span_label is not None,", "\n", "# then we are inside a span.", "\n", "                ", "bio_label", "=", "\"I-\"", "+", "current_span_labels", "[", "annotation_index", "]", "\n", "span_labels", "[", "annotation_index", "]", ".", "append", "(", "bio_label", ")", "\n", "", "else", ":", "\n", "# We're outside a span.", "\n", "                ", "span_labels", "[", "annotation_index", "]", ".", "append", "(", "\"O\"", ")", "\n", "# Exiting a span, so we reset the current span label for this annotation.", "\n", "", "if", "\")\"", "in", "annotation", ":", "\n", "                ", "current_span_labels", "[", "annotation_index", "]", "=", "None", "\n", "", "", "", "", ""]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.models.coref_base.CoreferenceResolver.__init__": [[64, 119], ["allennlp.nn.InitializerApplicator", "allennlp.models.model.Model.__init__", "allennlp.modules.TimeDistributed", "torch.nn.Sequential", "torch.nn.Sequential", "torch.nn.Sequential", "torch.nn.Sequential", "allennlp.modules.Pruner", "allennlp.modules.TimeDistributed", "allennlp.modules.span_extractors.EndpointSpanExtractor", "allennlp.modules.span_extractors.SelfAttentiveSpanExtractor", "allennlp.modules.token_embedders.Embedding", "allennlp.training.metrics.MentionRecall", "allennlp.training.metrics.ConllCorefScores", "initializer", "allennlp.modules.TimeDistributed", "allennlp.modules.TimeDistributed", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "context_layer.get_output_dim", "torch.nn.Dropout", "torch.nn.Dropout", "torch.nn.Dropout", "torch.nn.Dropout", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "antecedent_feedforward.get_output_dim", "text_field_embedder.get_output_dim", "mention_feedforward.get_output_dim"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.graph_util.plot.GraphPlot.__init__", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.seq2seq_decoders.auto_regressive_seq_decoder.AutoRegressiveSeqDecoder.get_output_dim", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.seq2seq_decoders.auto_regressive_seq_decoder.AutoRegressiveSeqDecoder.get_output_dim", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.seq2seq_decoders.auto_regressive_seq_decoder.AutoRegressiveSeqDecoder.get_output_dim", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.seq2seq_decoders.auto_regressive_seq_decoder.AutoRegressiveSeqDecoder.get_output_dim"], ["def", "__init__", "(", "\n", "self", ",", "\n", "vocab", ":", "Vocabulary", ",", "\n", "text_field_embedder", ":", "TextFieldEmbedder", ",", "\n", "context_layer", ":", "Seq2SeqEncoder", ",", "\n", "mention_feedforward", ":", "FeedForward", ",", "\n", "antecedent_feedforward", ":", "FeedForward", ",", "\n", "feature_size", ":", "int", ",", "\n", "max_span_width", ":", "int", ",", "\n", "spans_per_word", ":", "float", ",", "\n", "max_antecedents", ":", "int", ",", "\n", "lexical_dropout", ":", "float", "=", "0.2", ",", "\n", "initializer", ":", "InitializerApplicator", "=", "InitializerApplicator", "(", ")", ",", "\n", "**", "kwargs", "\n", ")", "->", "None", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "vocab", ",", "**", "kwargs", ")", "\n", "\n", "self", ".", "_text_field_embedder", "=", "text_field_embedder", "\n", "self", ".", "_context_layer", "=", "context_layer", "\n", "self", ".", "_antecedent_feedforward", "=", "TimeDistributed", "(", "antecedent_feedforward", ")", "\n", "feedforward_scorer", "=", "torch", ".", "nn", ".", "Sequential", "(", "\n", "TimeDistributed", "(", "mention_feedforward", ")", ",", "\n", "TimeDistributed", "(", "torch", ".", "nn", ".", "Linear", "(", "mention_feedforward", ".", "get_output_dim", "(", ")", ",", "1", ")", ")", ",", "\n", ")", "\n", "self", ".", "_mention_pruner", "=", "Pruner", "(", "feedforward_scorer", ")", "\n", "self", ".", "_antecedent_scorer", "=", "TimeDistributed", "(", "\n", "torch", ".", "nn", ".", "Linear", "(", "antecedent_feedforward", ".", "get_output_dim", "(", ")", ",", "1", ")", "\n", ")", "\n", "\n", "self", ".", "_endpoint_span_extractor", "=", "EndpointSpanExtractor", "(", "\n", "context_layer", ".", "get_output_dim", "(", ")", ",", "\n", "combination", "=", "\"x,y\"", ",", "\n", "num_width_embeddings", "=", "max_span_width", ",", "\n", "span_width_embedding_dim", "=", "feature_size", ",", "\n", "bucket_widths", "=", "False", ",", "\n", ")", "\n", "self", ".", "_attentive_span_extractor", "=", "SelfAttentiveSpanExtractor", "(", "\n", "input_dim", "=", "text_field_embedder", ".", "get_output_dim", "(", ")", "\n", ")", "\n", "\n", "# 10 possible distance buckets.", "\n", "self", ".", "_num_distance_buckets", "=", "10", "\n", "self", ".", "_distance_embedding", "=", "Embedding", "(", "self", ".", "_num_distance_buckets", ",", "feature_size", ")", "\n", "\n", "self", ".", "_max_span_width", "=", "max_span_width", "\n", "self", ".", "_spans_per_word", "=", "spans_per_word", "\n", "self", ".", "_max_antecedents", "=", "max_antecedents", "\n", "\n", "self", ".", "_mention_recall", "=", "MentionRecall", "(", ")", "\n", "self", ".", "_conll_coref_scores", "=", "ConllCorefScores", "(", ")", "\n", "if", "lexical_dropout", ">", "0", ":", "\n", "            ", "self", ".", "_lexical_dropout", "=", "torch", ".", "nn", ".", "Dropout", "(", "p", "=", "lexical_dropout", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "_lexical_dropout", "=", "lambda", "x", ":", "x", "\n", "", "initializer", "(", "self", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.models.coref_base.CoreferenceResolver.forward": [[120, 340], ["coref_base.CoreferenceResolver._lexical_dropout", "coref_base.CoreferenceResolver.size", "torch.relu().long.size", "allennlp.nn.util.get_text_field_mask().float", "torch.relu().long", "torch.relu().long", "coref_base.CoreferenceResolver._context_layer", "coref_base.CoreferenceResolver._endpoint_span_extractor", "coref_base.CoreferenceResolver._attentive_span_extractor", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "int", "coref_base.CoreferenceResolver._mention_pruner", "top_span_mask.unsqueeze.unsqueeze.unsqueeze", "allennlp.nn.util.flatten_and_batch_shift_indices", "allennlp.nn.util.batched_index_select", "min", "coref_base.CoreferenceResolver._generate_valid_antecedents", "allennlp.nn.util.flattened_index_select", "allennlp.nn.util.flattened_index_select().squeeze", "coref_base.CoreferenceResolver._compute_span_pair_embeddings", "coref_base.CoreferenceResolver._compute_coreference_scores", "coref_base.CoreferenceResolver.max", "coref_base.CoreferenceResolver._text_field_embedder", "math.floor", "allennlp.nn.util.get_device_of", "allennlp.nn.util.batched_index_select", "allennlp.nn.util.flattened_index_select().squeeze", "valid_antecedent_log_mask.long", "coref_base.CoreferenceResolver._compute_antecedent_gold_labels", "allennlp.nn.util.masked_log_softmax", "coref_base.CoreferenceResolver._mention_recall", "coref_base.CoreferenceResolver._conll_coref_scores", "allennlp.nn.util.get_text_field_mask", "torch.relu", "torch.relu", "allennlp.nn.util.flattened_index_select", "span_labels.unsqueeze", "coref_base.CoreferenceResolver.log", "allennlp.nn.util.logsumexp().sum", "torch.relu().long.float", "allennlp.nn.util.flattened_index_select", "allennlp.nn.util.logsumexp"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.nn.util.flatten_and_batch_shift_indices", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.nn.util.batched_index_select", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.coreference_resolution.coref.CoreferenceResolver._generate_valid_antecedents", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.nn.util.flattened_index_select", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.coreference_resolution.coref.CoreferenceResolver._compute_span_pair_embeddings", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.coreference_resolution.coref.CoreferenceResolver._compute_coreference_scores", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.nn.util.get_device_of", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.nn.util.batched_index_select", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.coreference_resolution.coref.CoreferenceResolver._compute_antecedent_gold_labels", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.nn.util.masked_log_softmax", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.nn.util.get_text_field_mask", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.nn.util.flattened_index_select", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.nn.util.flattened_index_select", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.nn.util.logsumexp"], ["", "@", "overrides", "\n", "def", "forward", "(", "\n", "self", ",", "# type: ignore", "\n", "text", ":", "TextFieldTensors", ",", "\n", "spans", ":", "torch", ".", "IntTensor", ",", "\n", "span_labels", ":", "torch", ".", "IntTensor", "=", "None", ",", "\n", "metadata", ":", "List", "[", "Dict", "[", "str", ",", "Any", "]", "]", "=", "None", ",", "\n", ")", "->", "Dict", "[", "str", ",", "torch", ".", "Tensor", "]", ":", "\n", "\n", "        ", "\"\"\"\n        # Parameters\n\n        text : `TextFieldTensors`, required.\n            The output of a `TextField` representing the text of\n            the document.\n        spans : `torch.IntTensor`, required.\n            A tensor of shape (batch_size, num_spans, 2), representing the inclusive start and end\n            indices of candidate spans for mentions. Comes from a `ListField[SpanField]` of\n            indices into the text of the document.\n        span_labels : `torch.IntTensor`, optional (default = None).\n            A tensor of shape (batch_size, num_spans), representing the cluster ids\n            of each span, or -1 for those which do not appear in any clusters.\n        metadata : `List[Dict[str, Any]]`, optional (default = None).\n            A metadata dictionary for each instance in the batch. We use the \"original_text\" and \"clusters\" keys\n            from this dictionary, which respectively have the original text and the annotated gold coreference\n            clusters for that instance.\n\n        # Returns\n\n        An output dictionary consisting of:\n        top_spans : `torch.IntTensor`\n            A tensor of shape `(batch_size, num_spans_to_keep, 2)` representing\n            the start and end word indices of the top spans that survived the pruning stage.\n        antecedent_indices : `torch.IntTensor`\n            A tensor of shape `(num_spans_to_keep, max_antecedents)` representing for each top span\n            the index (with respect to top_spans) of the possible antecedents the model considered.\n        predicted_antecedents : `torch.IntTensor`\n            A tensor of shape `(batch_size, num_spans_to_keep)` representing, for each top span, the\n            index (with respect to antecedent_indices) of the most likely antecedent. -1 means there\n            was no predicted link.\n        loss : `torch.FloatTensor`, optional\n            A scalar loss to be optimised.\n        \"\"\"", "\n", "# Shape: (batch_size, document_length, embedding_size)", "\n", "text_embeddings", "=", "self", ".", "_lexical_dropout", "(", "self", ".", "_text_field_embedder", "(", "text", ")", ")", "\n", "\n", "document_length", "=", "text_embeddings", ".", "size", "(", "1", ")", "\n", "num_spans", "=", "spans", ".", "size", "(", "1", ")", "\n", "\n", "# Shape: (batch_size, document_length)", "\n", "text_mask", "=", "util", ".", "get_text_field_mask", "(", "text", ")", ".", "float", "(", ")", "\n", "\n", "# Shape: (batch_size, num_spans)", "\n", "span_mask", "=", "(", "spans", "[", ":", ",", ":", ",", "0", "]", ">=", "0", ")", ".", "squeeze", "(", "-", "1", ")", ".", "float", "(", ")", "\n", "# SpanFields return -1 when they are used as padding. As we do", "\n", "# some comparisons based on span widths when we attend over the", "\n", "# span representations that we generate from these indices, we", "\n", "# need them to be <= 0. This is only relevant in edge cases where", "\n", "# the number of spans we consider after the pruning stage is >= the", "\n", "# total number of spans, because in this case, it is possible we might", "\n", "# consider a masked span.", "\n", "# Shape: (batch_size, num_spans, 2)", "\n", "spans", "=", "F", ".", "relu", "(", "spans", ".", "float", "(", ")", ")", ".", "long", "(", ")", "\n", "\n", "# Shape: (batch_size, document_length, encoding_dim)", "\n", "contextualized_embeddings", "=", "self", ".", "_context_layer", "(", "text_embeddings", ",", "text_mask", ")", "\n", "# Shape: (batch_size, num_spans, 2 * encoding_dim + feature_size)", "\n", "endpoint_span_embeddings", "=", "self", ".", "_endpoint_span_extractor", "(", "\n", "contextualized_embeddings", ",", "spans", "\n", ")", "\n", "# Shape: (batch_size, num_spans, emebedding_size)", "\n", "attended_span_embeddings", "=", "self", ".", "_attentive_span_extractor", "(", "\n", "text_embeddings", ",", "spans", "\n", ")", "\n", "\n", "# Shape: (batch_size, num_spans, emebedding_size + 2 * encoding_dim + feature_size)", "\n", "span_embeddings", "=", "torch", ".", "cat", "(", "\n", "[", "endpoint_span_embeddings", ",", "attended_span_embeddings", "]", ",", "-", "1", "\n", ")", "\n", "\n", "# Prune based on mention scores.", "\n", "num_spans_to_keep", "=", "int", "(", "math", ".", "floor", "(", "self", ".", "_spans_per_word", "*", "document_length", ")", ")", "\n", "\n", "(", "\n", "top_span_embeddings", ",", "\n", "top_span_mask", ",", "\n", "top_span_indices", ",", "\n", "top_span_mention_scores", ",", "\n", ")", "=", "self", ".", "_mention_pruner", "(", "span_embeddings", ",", "span_mask", ",", "num_spans_to_keep", ")", "\n", "top_span_mask", "=", "top_span_mask", ".", "unsqueeze", "(", "-", "1", ")", "\n", "# Shape: (batch_size * num_spans_to_keep)", "\n", "# torch.index_select only accepts 1D indices, but here", "\n", "# we need to select spans for each element in the batch.", "\n", "# This reformats the indices to take into account their", "\n", "# index into the batch. We precompute this here to make", "\n", "# the multiple calls to util.batched_index_select below more efficient.", "\n", "flat_top_span_indices", "=", "util", ".", "flatten_and_batch_shift_indices", "(", "\n", "top_span_indices", ",", "num_spans", "\n", ")", "\n", "\n", "# Compute final predictions for which spans to consider as mentions.", "\n", "# Shape: (batch_size, num_spans_to_keep, 2)", "\n", "top_spans", "=", "util", ".", "batched_index_select", "(", "\n", "spans", ",", "top_span_indices", ",", "flat_top_span_indices", "\n", ")", "\n", "\n", "# Compute indices for antecedent spans to consider.", "\n", "max_antecedents", "=", "min", "(", "self", ".", "_max_antecedents", ",", "num_spans_to_keep", ")", "\n", "\n", "# Now that we have our variables in terms of num_spans_to_keep, we need to", "\n", "# compare span pairs to decide each span's antecedent. Each span can only", "\n", "# have prior spans as antecedents, and we only consider up to max_antecedents", "\n", "# prior spans. So the first thing we do is construct a matrix mapping a span's", "\n", "#  index to the indices of its allowed antecedents. Note that this is independent", "\n", "#  of the batch dimension - it's just a function of the span's position in", "\n", "# top_spans. The spans are in document order, so we can just use the relative", "\n", "# index of the spans to know which other spans are allowed antecedents.", "\n", "\n", "# Once we have this matrix, we reformat our variables again to get embeddings", "\n", "# for all valid antecedents for each span. This gives us variables with shapes", "\n", "#  like (batch_size, num_spans_to_keep, max_antecedents, embedding_size), which", "\n", "#  we can use to make coreference decisions between valid span pairs.", "\n", "\n", "# Shapes:", "\n", "# (num_spans_to_keep, max_antecedents),", "\n", "# (1, max_antecedents),", "\n", "# (1, num_spans_to_keep, max_antecedents)", "\n", "(", "\n", "valid_antecedent_indices", ",", "\n", "valid_antecedent_offsets", ",", "\n", "valid_antecedent_log_mask", ",", "\n", ")", "=", "self", ".", "_generate_valid_antecedents", "(", "# noqa", "\n", "num_spans_to_keep", ",", "max_antecedents", ",", "util", ".", "get_device_of", "(", "text_mask", ")", "\n", ")", "\n", "# Select tensors relating to the antecedent spans.", "\n", "# Shape: (batch_size, num_spans_to_keep, max_antecedents, embedding_size)", "\n", "candidate_antecedent_embeddings", "=", "util", ".", "flattened_index_select", "(", "\n", "top_span_embeddings", ",", "valid_antecedent_indices", "\n", ")", "\n", "\n", "# Shape: (batch_size, num_spans_to_keep, max_antecedents)", "\n", "candidate_antecedent_mention_scores", "=", "util", ".", "flattened_index_select", "(", "\n", "top_span_mention_scores", ",", "valid_antecedent_indices", "\n", ")", ".", "squeeze", "(", "-", "1", ")", "\n", "# Compute antecedent scores.", "\n", "# Shape: (batch_size, num_spans_to_keep, max_antecedents, embedding_size)", "\n", "span_pair_embeddings", "=", "self", ".", "_compute_span_pair_embeddings", "(", "\n", "top_span_embeddings", ",", "\n", "candidate_antecedent_embeddings", ",", "\n", "valid_antecedent_offsets", ",", "\n", ")", "\n", "# Shape: (batch_size, num_spans_to_keep, 1 + max_antecedents)", "\n", "coreference_scores", "=", "self", ".", "_compute_coreference_scores", "(", "\n", "span_pair_embeddings", ",", "\n", "top_span_mention_scores", ",", "\n", "candidate_antecedent_mention_scores", ",", "\n", "valid_antecedent_log_mask", ",", "\n", ")", "\n", "\n", "# We now have, for each span which survived the pruning stage,", "\n", "# a predicted antecedent. This implies a clustering if we group", "\n", "# mentions which refer to each other in a chain.", "\n", "# Shape: (batch_size, num_spans_to_keep)", "\n", "_", ",", "predicted_antecedents", "=", "coreference_scores", ".", "max", "(", "2", ")", "\n", "# Subtract one here because index 0 is the \"no antecedent\" class,", "\n", "# so this makes the indices line up with actual spans if the prediction", "\n", "# is greater than -1.", "\n", "predicted_antecedents", "-=", "1", "\n", "\n", "output_dict", "=", "{", "\n", "\"top_spans\"", ":", "top_spans", ",", "\n", "\"antecedent_indices\"", ":", "valid_antecedent_indices", ",", "\n", "\"predicted_antecedents\"", ":", "predicted_antecedents", ",", "\n", "}", "\n", "if", "span_labels", "is", "not", "None", ":", "\n", "# Find the gold labels for the spans which we kept.", "\n", "            ", "pruned_gold_labels", "=", "util", ".", "batched_index_select", "(", "\n", "span_labels", ".", "unsqueeze", "(", "-", "1", ")", ",", "top_span_indices", ",", "flat_top_span_indices", "\n", ")", "\n", "\n", "antecedent_labels", "=", "util", ".", "flattened_index_select", "(", "\n", "pruned_gold_labels", ",", "valid_antecedent_indices", "\n", ")", ".", "squeeze", "(", "-", "1", ")", "\n", "antecedent_labels", "+=", "valid_antecedent_log_mask", ".", "long", "(", ")", "\n", "\n", "# Compute labels.", "\n", "# Shape: (batch_size, num_spans_to_keep, max_antecedents + 1)", "\n", "gold_antecedent_labels", "=", "self", ".", "_compute_antecedent_gold_labels", "(", "\n", "pruned_gold_labels", ",", "antecedent_labels", "\n", ")", "\n", "# Now, compute the loss using the negative marginal log-likelihood.", "\n", "# This is equal to the log of the sum of the probabilities of all antecedent predictions", "\n", "# that would be consistent with the data, in the sense that we are minimising, for a", "\n", "# given span, the negative marginal log likelihood of all antecedents which are in the", "\n", "# same gold cluster as the span we are currently considering. Each span i predicts a", "\n", "# single antecedent j, but there might be several prior mentions k in the same", "\n", "# coreference cluster that would be valid antecedents. Our loss is the sum of the", "\n", "# probability assigned to all valid antecedents. This is a valid objective for", "\n", "# clustering as we don't mind which antecedent is predicted, so long as they are in", "\n", "#  the same coreference cluster.", "\n", "coreference_log_probs", "=", "util", ".", "masked_log_softmax", "(", "\n", "coreference_scores", ",", "top_span_mask", "\n", ")", "\n", "correct_antecedent_log_probs", "=", "(", "\n", "coreference_log_probs", "+", "gold_antecedent_labels", ".", "log", "(", ")", "\n", ")", "\n", "negative_marginal_log_likelihood", "=", "-", "util", ".", "logsumexp", "(", "\n", "correct_antecedent_log_probs", "\n", ")", ".", "sum", "(", ")", "\n", "\n", "self", ".", "_mention_recall", "(", "top_spans", ",", "metadata", ")", "\n", "self", ".", "_conll_coref_scores", "(", "\n", "top_spans", ",", "valid_antecedent_indices", ",", "predicted_antecedents", ",", "metadata", "\n", ")", "\n", "\n", "output_dict", "[", "\"loss\"", "]", "=", "negative_marginal_log_likelihood", "\n", "\n", "", "if", "metadata", "is", "not", "None", ":", "\n", "            ", "output_dict", "[", "\"document\"", "]", "=", "[", "x", "[", "\"original_text\"", "]", "for", "x", "in", "metadata", "]", "\n", "", "return", "output_dict", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.models.coref_base.CoreferenceResolver.decode": [[341, 427], ["output_dict[].detach().cpu", "output_dict[].detach().cpu", "output_dict[].detach().cpu", "zip", "enumerate", "batch_clusters.append", "output_dict[].detach", "output_dict[].detach", "output_dict[].detach", "zip", "clusters[].append", "top_spans[].item", "top_spans[].item", "len", "clusters.append", "span[].item", "span[].item"], "methods", ["None"], ["", "@", "overrides", "\n", "def", "decode", "(", "self", ",", "output_dict", ":", "Dict", "[", "str", ",", "torch", ".", "Tensor", "]", ")", ":", "\n", "        ", "\"\"\"\n        Converts the list of spans and predicted antecedent indices into clusters\n        of spans for each element in the batch.\n\n        # Parameters\n\n        output_dict : `Dict[str, torch.Tensor]`, required.\n            The result of calling :func:`forward` on an instance or batch of instances.\n\n        # Returns\n\n        The same output dictionary, but with an additional `clusters` key:\n\n        clusters : `List[List[List[Tuple[int, int]]]]`\n            A nested list, representing, for each instance in the batch, the list of clusters,\n            which are in turn comprised of a list of (start, end) inclusive spans into the\n            original document.\n        \"\"\"", "\n", "\n", "# A tensor of shape (batch_size, num_spans_to_keep, 2), representing", "\n", "# the start and end indices of each span.", "\n", "batch_top_spans", "=", "output_dict", "[", "\"top_spans\"", "]", ".", "detach", "(", ")", ".", "cpu", "(", ")", "\n", "\n", "# A tensor of shape (batch_size, num_spans_to_keep) representing, for each span,", "\n", "# the index into `antecedent_indices` which specifies the antecedent span. Additionally,", "\n", "# the index can be -1, specifying that the span has no predicted antecedent.", "\n", "batch_predicted_antecedents", "=", "(", "\n", "output_dict", "[", "\"predicted_antecedents\"", "]", ".", "detach", "(", ")", ".", "cpu", "(", ")", "\n", ")", "\n", "\n", "# A tensor of shape (num_spans_to_keep, max_antecedents), representing the indices", "\n", "# of the predicted antecedents with respect to the 2nd dimension of `batch_top_spans`", "\n", "# for each antecedent we considered.", "\n", "antecedent_indices", "=", "output_dict", "[", "\"antecedent_indices\"", "]", ".", "detach", "(", ")", ".", "cpu", "(", ")", "\n", "batch_clusters", ":", "List", "[", "List", "[", "List", "[", "Tuple", "[", "int", ",", "int", "]", "]", "]", "]", "=", "[", "]", "\n", "\n", "# Calling zip() on two tensors results in an iterator over their", "\n", "# first dimension. This is iterating over instances in the batch.", "\n", "for", "top_spans", ",", "predicted_antecedents", "in", "zip", "(", "\n", "batch_top_spans", ",", "batch_predicted_antecedents", "\n", ")", ":", "\n", "            ", "spans_to_cluster_ids", ":", "Dict", "[", "Tuple", "[", "int", ",", "int", "]", ",", "int", "]", "=", "{", "}", "\n", "clusters", ":", "List", "[", "List", "[", "Tuple", "[", "int", ",", "int", "]", "]", "]", "=", "[", "]", "\n", "\n", "for", "i", ",", "(", "span", ",", "predicted_antecedent", ")", "in", "enumerate", "(", "\n", "zip", "(", "top_spans", ",", "predicted_antecedents", ")", "\n", ")", ":", "\n", "                ", "if", "predicted_antecedent", "<", "0", ":", "\n", "# We don't care about spans which are", "\n", "# not co-referent with anything.", "\n", "                    ", "continue", "\n", "\n", "# Find the right cluster to update with this span.", "\n", "# To do this, we find the row in `antecedent_indices`", "\n", "# corresponding to this span we are considering.", "\n", "# The predicted antecedent is then an index into this list", "\n", "# of indices, denoting the span from `top_spans` which is the", "\n", "# most likely antecedent.", "\n", "", "predicted_index", "=", "antecedent_indices", "[", "i", ",", "predicted_antecedent", "]", "\n", "\n", "antecedent_span", "=", "(", "\n", "top_spans", "[", "predicted_index", ",", "0", "]", ".", "item", "(", ")", ",", "\n", "top_spans", "[", "predicted_index", ",", "1", "]", ".", "item", "(", ")", ",", "\n", ")", "\n", "\n", "# Check if we've seen the span before.", "\n", "if", "antecedent_span", "in", "spans_to_cluster_ids", ":", "\n", "                    ", "predicted_cluster_id", ":", "int", "=", "spans_to_cluster_ids", "[", "antecedent_span", "]", "\n", "", "else", ":", "\n", "# We start a new cluster.", "\n", "                    ", "predicted_cluster_id", "=", "len", "(", "clusters", ")", "\n", "# Append a new cluster containing only this span.", "\n", "clusters", ".", "append", "(", "[", "antecedent_span", "]", ")", "\n", "# Record the new id of this span.", "\n", "spans_to_cluster_ids", "[", "antecedent_span", "]", "=", "predicted_cluster_id", "\n", "\n", "# Now add the span we are currently considering.", "\n", "", "span_start", ",", "span_end", "=", "span", "[", "0", "]", ".", "item", "(", ")", ",", "span", "[", "1", "]", ".", "item", "(", ")", "\n", "clusters", "[", "predicted_cluster_id", "]", ".", "append", "(", "(", "span_start", ",", "span_end", ")", ")", "\n", "spans_to_cluster_ids", "[", "(", "span_start", ",", "span_end", ")", "]", "=", "predicted_cluster_id", "\n", "", "batch_clusters", ".", "append", "(", "clusters", ")", "\n", "\n", "", "output_dict", "[", "\"clusters\"", "]", "=", "batch_clusters", "\n", "return", "output_dict", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.models.coref_base.CoreferenceResolver.get_metrics": [[428, 440], ["coref_base.CoreferenceResolver._mention_recall.get_metric", "coref_base.CoreferenceResolver._conll_coref_scores.get_metric"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.metrics.sequence_accuracy.SequenceAccuracy.get_metric", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.metrics.sequence_accuracy.SequenceAccuracy.get_metric"], ["", "@", "overrides", "\n", "def", "get_metrics", "(", "self", ",", "reset", ":", "bool", "=", "False", ")", "->", "Dict", "[", "str", ",", "float", "]", ":", "\n", "        ", "mention_recall", "=", "self", ".", "_mention_recall", ".", "get_metric", "(", "reset", ")", "\n", "coref_precision", ",", "coref_recall", ",", "coref_f1", "=", "self", ".", "_conll_coref_scores", ".", "get_metric", "(", "\n", "reset", "\n", ")", "\n", "\n", "return", "{", "\n", "\"coref_precision\"", ":", "coref_precision", ",", "\n", "\"coref_recall\"", ":", "coref_recall", ",", "\n", "\"coref_f1\"", ":", "coref_f1", ",", "\n", "\"mention_recall\"", ":", "mention_recall", ",", "\n", "}", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.models.coref_base.CoreferenceResolver._generate_valid_antecedents": [[442, 509], ["allennlp.nn.util.get_range_vector().unsqueeze", "torch.relu().long", "torch.relu().long", "allennlp.nn.util.get_range_vector", "torch.relu", "torch.relu", "allennlp.nn.util.get_range_vector", "raw_antecedent_indices.float"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.nn.util.get_range_vector", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.nn.util.get_range_vector"], ["", "@", "staticmethod", "\n", "def", "_generate_valid_antecedents", "(", "\n", "num_spans_to_keep", ":", "int", ",", "max_antecedents", ":", "int", ",", "device", ":", "int", "\n", ")", "->", "Tuple", "[", "torch", ".", "IntTensor", ",", "torch", ".", "IntTensor", ",", "torch", ".", "FloatTensor", "]", ":", "\n", "        ", "\"\"\"\n        This method generates possible antecedents per span which survived the pruning\n        stage. This procedure is `generic across the batch`. The reason this is the case is\n        that each span in a batch can be coreferent with any previous span, but here we\n        are computing the possible `indices` of these spans. So, regardless of the batch,\n        the 1st span _cannot_ have any antecedents, because there are none to select from.\n        Similarly, each element can only predict previous spans, so this returns a matrix\n        of shape (num_spans_to_keep, max_antecedents), where the (i,j)-th index is equal to\n        (i - 1) - j if j <= i, or zero otherwise.\n\n        # Parameters\n\n        num_spans_to_keep : `int`, required.\n            The number of spans that were kept while pruning.\n        max_antecedents : `int`, required.\n            The maximum number of antecedent spans to consider for every span.\n        device : `int`, required.\n            The CUDA device to use.\n\n        # Returns\n\n        valid_antecedent_indices : `torch.IntTensor`\n            The indices of every antecedent to consider with respect to the top k spans.\n            Has shape `(num_spans_to_keep, max_antecedents)`.\n        valid_antecedent_offsets : `torch.IntTensor`\n            The distance between the span and each of its antecedents in terms of the number\n            of considered spans (i.e not the word distance between the spans).\n            Has shape `(1, max_antecedents)`.\n        valid_antecedent_log_mask : `torch.FloatTensor`\n            The logged mask representing whether each antecedent span is valid. Required since\n            different spans have different numbers of valid antecedents. For example, the first\n            span in the document should have no valid antecedents.\n            Has shape `(1, num_spans_to_keep, max_antecedents)`.\n        \"\"\"", "\n", "# Shape: (num_spans_to_keep, 1)", "\n", "target_indices", "=", "util", ".", "get_range_vector", "(", "num_spans_to_keep", ",", "device", ")", ".", "unsqueeze", "(", "1", ")", "\n", "\n", "# Shape: (1, max_antecedents)", "\n", "valid_antecedent_offsets", "=", "(", "\n", "util", ".", "get_range_vector", "(", "max_antecedents", ",", "device", ")", "+", "1", "\n", ")", ".", "unsqueeze", "(", "0", ")", "\n", "\n", "# This is a broadcasted subtraction.", "\n", "# Shape: (num_spans_to_keep, max_antecedents)", "\n", "raw_antecedent_indices", "=", "target_indices", "-", "valid_antecedent_offsets", "\n", "\n", "# In our matrix of indices, the upper triangular part will be negative", "\n", "# because the offsets will be > the target indices. We want to mask these,", "\n", "# because these are exactly the indices which we don't want to predict, per span.", "\n", "# We're generating a logspace mask here because we will eventually create a", "\n", "# distribution over these indices, so we need the 0 elements of the mask to be -inf", "\n", "# in order to not mess up the normalisation of the distribution.", "\n", "# Shape: (1, num_spans_to_keep, max_antecedents)", "\n", "valid_antecedent_log_mask", "=", "(", "\n", "(", "raw_antecedent_indices", ">=", "0", ")", ".", "float", "(", ")", ".", "unsqueeze", "(", "0", ")", ".", "log", "(", ")", "\n", ")", "\n", "\n", "# Shape: (num_spans_to_keep, max_antecedents)", "\n", "valid_antecedent_indices", "=", "F", ".", "relu", "(", "raw_antecedent_indices", ".", "float", "(", ")", ")", ".", "long", "(", ")", "\n", "return", "(", "\n", "valid_antecedent_indices", ",", "\n", "valid_antecedent_offsets", ",", "\n", "valid_antecedent_log_mask", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.models.coref_base.CoreferenceResolver._compute_span_pair_embeddings": [[511, 579], ["top_span_embeddings.unsqueeze().expand_as", "coref_base.CoreferenceResolver._distance_embedding", "antecedent_distance_embeddings.expand.expand.unsqueeze", "antecedent_distance_embeddings.expand.expand.expand", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "allennlp.nn.util.bucket_values", "antecedent_embeddings.size", "antecedent_embeddings.size", "antecedent_embeddings.size", "antecedent_distance_embeddings.expand.expand.size", "top_span_embeddings.unsqueeze"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.nn.util.bucket_values"], ["", "def", "_compute_span_pair_embeddings", "(", "\n", "self", ",", "\n", "top_span_embeddings", ":", "torch", ".", "FloatTensor", ",", "\n", "antecedent_embeddings", ":", "torch", ".", "FloatTensor", ",", "\n", "antecedent_offsets", ":", "torch", ".", "FloatTensor", ",", "\n", ")", ":", "\n", "        ", "\"\"\"\n        Computes an embedding representation of pairs of spans for the pairwise scoring function\n        to consider. This includes both the original span representations, the element-wise\n        similarity of the span representations, and an embedding representation of the distance\n        between the two spans.\n\n        # Parameters\n\n        top_span_embeddings : `torch.FloatTensor`, required.\n            Embedding representations of the top spans. Has shape\n            (batch_size, num_spans_to_keep, embedding_size).\n        antecedent_embeddings : `torch.FloatTensor`, required.\n            Embedding representations of the antecedent spans we are considering\n            for each top span. Has shape\n            (batch_size, num_spans_to_keep, max_antecedents, embedding_size).\n        antecedent_offsets : `torch.IntTensor`, required.\n            The offsets between each top span and its antecedent spans in terms\n            of spans we are considering. Has shape (1, max_antecedents).\n\n        # Returns\n\n        span_pair_embeddings : `torch.FloatTensor`\n            Embedding representation of the pair of spans to consider. Has shape\n            (batch_size, num_spans_to_keep, max_antecedents, embedding_size)\n        \"\"\"", "\n", "# Shape: (batch_size, num_spans_to_keep, max_antecedents, embedding_size)", "\n", "target_embeddings", "=", "top_span_embeddings", ".", "unsqueeze", "(", "2", ")", ".", "expand_as", "(", "\n", "antecedent_embeddings", "\n", ")", "\n", "\n", "# Shape: (1, max_antecedents, embedding_size)", "\n", "antecedent_distance_embeddings", "=", "self", ".", "_distance_embedding", "(", "\n", "util", ".", "bucket_values", "(", "\n", "antecedent_offsets", ",", "num_total_buckets", "=", "self", ".", "_num_distance_buckets", "\n", ")", "\n", ")", "\n", "\n", "# Shape: (1, 1, max_antecedents, embedding_size)", "\n", "antecedent_distance_embeddings", "=", "antecedent_distance_embeddings", ".", "unsqueeze", "(", "0", ")", "\n", "\n", "expanded_distance_embeddings_shape", "=", "(", "\n", "antecedent_embeddings", ".", "size", "(", "0", ")", ",", "\n", "antecedent_embeddings", ".", "size", "(", "1", ")", ",", "\n", "antecedent_embeddings", ".", "size", "(", "2", ")", ",", "\n", "antecedent_distance_embeddings", ".", "size", "(", "-", "1", ")", ",", "\n", ")", "\n", "# Shape: (batch_size, num_spans_to_keep, max_antecedents, embedding_size)", "\n", "antecedent_distance_embeddings", "=", "antecedent_distance_embeddings", ".", "expand", "(", "\n", "*", "expanded_distance_embeddings_shape", "\n", ")", "\n", "\n", "# Shape: (batch_size, num_spans_to_keep, max_antecedents, embedding_size)", "\n", "span_pair_embeddings", "=", "torch", ".", "cat", "(", "\n", "[", "\n", "target_embeddings", ",", "\n", "antecedent_embeddings", ",", "\n", "antecedent_embeddings", "*", "target_embeddings", ",", "\n", "antecedent_distance_embeddings", ",", "\n", "]", ",", "\n", "-", "1", ",", "\n", ")", "\n", "return", "span_pair_embeddings", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.models.coref_base.CoreferenceResolver._compute_antecedent_gold_labels": [[580, 622], ["top_span_labels.expand_as", "torch.cat", "torch.cat", "torch.cat", "torch.cat"], "methods", ["None"], ["", "@", "staticmethod", "\n", "def", "_compute_antecedent_gold_labels", "(", "\n", "top_span_labels", ":", "torch", ".", "IntTensor", ",", "antecedent_labels", ":", "torch", ".", "IntTensor", "\n", ")", ":", "\n", "        ", "\"\"\"\n        Generates a binary indicator for every pair of spans. This label is one if and\n        only if the pair of spans belong to the same cluster. The labels are augmented\n        with a dummy antecedent at the zeroth position, which represents the prediction\n        that a span does not have any antecedent.\n\n        # Parameters\n\n        top_span_labels : `torch.IntTensor`, required.\n            The cluster id label for every span. The id is arbitrary,\n            as we just care about the clustering. Has shape (batch_size, num_spans_to_keep).\n        antecedent_labels : `torch.IntTensor`, required.\n            The cluster id label for every antecedent span. The id is arbitrary,\n            as we just care about the clustering. Has shape\n            (batch_size, num_spans_to_keep, max_antecedents).\n\n        # Returns\n\n        pairwise_labels_with_dummy_label : `torch.FloatTensor`\n            A binary tensor representing whether a given pair of spans belong to\n            the same cluster in the gold clustering.\n            Has shape (batch_size, num_spans_to_keep, max_antecedents + 1).\n\n        \"\"\"", "\n", "# Shape: (batch_size, num_spans_to_keep, max_antecedents)", "\n", "target_labels", "=", "top_span_labels", ".", "expand_as", "(", "antecedent_labels", ")", "\n", "same_cluster_indicator", "=", "(", "target_labels", "==", "antecedent_labels", ")", ".", "float", "(", ")", "\n", "non_dummy_indicator", "=", "(", "target_labels", ">=", "0", ")", ".", "float", "(", ")", "\n", "pairwise_labels", "=", "same_cluster_indicator", "*", "non_dummy_indicator", "\n", "\n", "# Shape: (batch_size, num_spans_to_keep, 1)", "\n", "dummy_labels", "=", "(", "1", "-", "pairwise_labels", ")", ".", "prod", "(", "-", "1", ",", "keepdim", "=", "True", ")", "\n", "\n", "# Shape: (batch_size, num_spans_to_keep, max_antecedents + 1)", "\n", "pairwise_labels_with_dummy_label", "=", "torch", ".", "cat", "(", "\n", "[", "dummy_labels", ",", "pairwise_labels", "]", ",", "-", "1", "\n", ")", "\n", "return", "pairwise_labels_with_dummy_label", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.models.coref_base.CoreferenceResolver._compute_coreference_scores": [[623, 674], ["coref_base.CoreferenceResolver._antecedent_scorer().squeeze", "coref_base.CoreferenceResolver.new_zeros", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "coref_base.CoreferenceResolver.size", "coref_base.CoreferenceResolver.size", "coref_base.CoreferenceResolver._antecedent_scorer", "coref_base.CoreferenceResolver._antecedent_feedforward"], "methods", ["None"], ["", "def", "_compute_coreference_scores", "(", "\n", "self", ",", "\n", "pairwise_embeddings", ":", "torch", ".", "FloatTensor", ",", "\n", "top_span_mention_scores", ":", "torch", ".", "FloatTensor", ",", "\n", "antecedent_mention_scores", ":", "torch", ".", "FloatTensor", ",", "\n", "antecedent_log_mask", ":", "torch", ".", "FloatTensor", ",", "\n", ")", "->", "torch", ".", "FloatTensor", ":", "\n", "        ", "\"\"\"\n        Computes scores for every pair of spans. Additionally, a dummy label is included,\n        representing the decision that the span is not coreferent with anything. For the dummy\n        label, the score is always zero. For the true antecedent spans, the score consists of\n        the pairwise antecedent score and the unary mention scores for the span and its\n        antecedent. The factoring allows the model to blame many of the absent links on bad\n        spans, enabling the pruning strategy used in the forward pass.\n\n        # Parameters\n\n        pairwise_embeddings : `torch.FloatTensor`, required.\n            Embedding representations of pairs of spans. Has shape\n            (batch_size, num_spans_to_keep, max_antecedents, encoding_dim)\n        top_span_mention_scores : `torch.FloatTensor`, required.\n            Mention scores for every span. Has shape\n            (batch_size, num_spans_to_keep, max_antecedents).\n        antecedent_mention_scores : `torch.FloatTensor`, required.\n            Mention scores for every antecedent. Has shape\n            (batch_size, num_spans_to_keep, max_antecedents).\n        antecedent_log_mask : `torch.FloatTensor`, required.\n            The log of the mask for valid antecedents.\n\n        # Returns\n\n        coreference_scores : `torch.FloatTensor`\n            A tensor of shape (batch_size, num_spans_to_keep, max_antecedents + 1),\n            representing the unormalised score for each (span, antecedent) pair\n            we considered.\n\n        \"\"\"", "\n", "# Shape: (batch_size, num_spans_to_keep, max_antecedents)", "\n", "antecedent_scores", "=", "self", ".", "_antecedent_scorer", "(", "\n", "self", ".", "_antecedent_feedforward", "(", "pairwise_embeddings", ")", "\n", ")", ".", "squeeze", "(", "-", "1", ")", "\n", "antecedent_scores", "+=", "top_span_mention_scores", "+", "antecedent_mention_scores", "\n", "antecedent_scores", "+=", "antecedent_log_mask", "\n", "\n", "# Shape: (batch_size, num_spans_to_keep, 1)", "\n", "shape", "=", "[", "antecedent_scores", ".", "size", "(", "0", ")", ",", "antecedent_scores", ".", "size", "(", "1", ")", ",", "1", "]", "\n", "dummy_scores", "=", "antecedent_scores", ".", "new_zeros", "(", "*", "shape", ")", "\n", "\n", "# Shape: (batch_size, num_spans_to_keep, max_antecedents + 1)", "\n", "coreference_scores", "=", "torch", ".", "cat", "(", "[", "dummy_scores", ",", "antecedent_scores", "]", ",", "-", "1", ")", "\n", "return", "coreference_scores", "\n", "", "", ""]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.models.srl_custom.SrlCustom.__init__": [[64, 113], ["allennlp.nn.InitializerApplicator", "allennlp.models.model.Model.__init__", "srl_custom.SrlCustom.vocab.get_vocab_size", "allennlp.modules.token_embedders.Embedding", "allennlp.modules.TimeDistributed", "torch.nn.modules.Dropout", "torch.nn.modules.Dropout", "allennlp.common.checks.check_dimensions_match", "allennlp.common.checks.check_dimensions_match", "initializer", "allennlp.training.metrics.srl_eval_scorer.SrlEvalScorer", "torch.nn.modules.Linear", "torch.nn.modules.Linear", "text_field_embedder.get_output_dim", "encoder.get_input_dim", "srl_custom.SrlCustom.secondary_encoder.get_input_dim", "srl_custom.SrlCustom.secondary_encoder.get_output_dim", "encoder.get_output_dim"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.graph_util.plot.GraphPlot.__init__", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.data.vocabulary.Vocabulary.get_vocab_size", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.checks.check_dimensions_match", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.checks.check_dimensions_match", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.seq2seq_decoders.auto_regressive_seq_decoder.AutoRegressiveSeqDecoder.get_output_dim", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.seq2vec_encoders.cnn_highway_encoder.CnnHighwayEncoder.get_input_dim", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.seq2vec_encoders.cnn_highway_encoder.CnnHighwayEncoder.get_input_dim", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.seq2seq_decoders.auto_regressive_seq_decoder.AutoRegressiveSeqDecoder.get_output_dim", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.seq2seq_decoders.auto_regressive_seq_decoder.AutoRegressiveSeqDecoder.get_output_dim"], ["def", "__init__", "(", "\n", "self", ",", "\n", "vocab", ":", "Vocabulary", ",", "\n", "text_field_embedder", ":", "TextFieldEmbedder", ",", "\n", "encoder", ":", "Seq2SeqEncoder", ",", "\n", "secondary_encoder", ":", "Seq2SeqEncoder", ",", "\n", "binary_feature_dim", ":", "int", ",", "\n", "embedding_dropout", ":", "float", "=", "0.0", ",", "\n", "initializer", ":", "InitializerApplicator", "=", "InitializerApplicator", "(", ")", ",", "\n", "regularizer", ":", "Optional", "[", "RegularizerApplicator", "]", "=", "None", ",", "\n", "label_smoothing", ":", "float", "=", "None", ",", "\n", "ignore_span_metric", ":", "bool", "=", "False", ",", "\n", "srl_eval_path", ":", "str", "=", "DEFAULT_SRL_EVAL_PATH", ",", "\n", ")", "->", "None", ":", "\n", "        ", "super", "(", "SrlCustom", ",", "self", ")", ".", "__init__", "(", "vocab", ",", "regularizer", ")", "\n", "self", ".", "finetuning", "=", "False", "# this is set only when performing policy gradient", "\n", "self", ".", "text_field_embedder", "=", "text_field_embedder", "\n", "self", ".", "num_classes", "=", "self", ".", "vocab", ".", "get_vocab_size", "(", "\"labels\"", ")", "\n", "\n", "if", "srl_eval_path", "is", "not", "None", ":", "\n", "# For the span based evaluation, we don't want to consider labels", "\n", "# for verb, because the verb index is provided to the model.", "\n", "            ", "self", ".", "span_metric", "=", "SrlEvalScorer", "(", "srl_eval_path", ",", "ignore_classes", "=", "[", "\"V\"", "]", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "span_metric", "=", "None", "\n", "\n", "", "self", ".", "encoder", "=", "encoder", "\n", "self", ".", "secondary_encoder", "=", "secondary_encoder", "\n", "# There are exactly 2 binary features for the verb predicate embedding.", "\n", "self", ".", "binary_feature_embedding", "=", "Embedding", "(", "2", ",", "binary_feature_dim", ")", "\n", "self", ".", "tag_projection_layer", "=", "TimeDistributed", "(", "\n", "Linear", "(", "self", ".", "secondary_encoder", ".", "get_output_dim", "(", ")", ",", "self", ".", "num_classes", ")", "\n", ")", "\n", "self", ".", "embedding_dropout", "=", "Dropout", "(", "p", "=", "embedding_dropout", ")", "\n", "self", ".", "_label_smoothing", "=", "label_smoothing", "\n", "self", ".", "ignore_span_metric", "=", "ignore_span_metric", "\n", "\n", "check_dimensions_match", "(", "\n", "text_field_embedder", ".", "get_output_dim", "(", ")", ",", "encoder", ".", "get_input_dim", "(", ")", ",", "\n", "\"text embedding dim\"", ",", "\n", "\"encoder input dim\"", ",", "\n", ")", "\n", "check_dimensions_match", "(", "\n", "encoder", ".", "get_output_dim", "(", ")", "+", "binary_feature_dim", ",", "\n", "self", ".", "secondary_encoder", ".", "get_input_dim", "(", ")", ",", "\n", "\"encoder dim + verb indicator embedding dim\"", ",", "\n", "\"secondary encoder dim\"", ",", "\n", ")", "\n", "initializer", "(", "self", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.models.srl_custom.SrlCustom.forward": [[114, 223], ["srl_custom.SrlCustom.embedding_dropout", "allennlp.nn.util.get_text_field_mask", "srl_custom.SrlCustom.binary_feature_embedding", "srl_custom.SrlCustom.encoder", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat.size", "torch.cat.size", "allennlp.nn.util.get_device_of", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "srl_custom.SrlCustom.secondary_encoder", "srl_custom.SrlCustom.tag_projection_layer", "srl_custom.SrlCustom.view", "dict", "torch.softmax().view", "torch.softmax().view", "zip", "list", "list", "list", "list", "srl_custom.SrlCustom.text_field_embedder", "verb_indicator.long", "torch.cat.size", "torch.cat.size", "torch.cat.size", "torch.cat.size", "torch.distributions.Categorical", "torch.distributions.Categorical", "torch.distributions.Categorical.sample", "torch.distributions.Categorical.sample", "torch.distributions.Categorical.log_prob", "torch.distributions.Categorical.log_prob", "torch.ones_like", "torch.ones_like", "torch.ones_like", "torch.ones_like", "allennlp.nn.util.sequence_cross_entropy_with_logits", "torch.softmax", "torch.softmax", "torch.distributions.Categorical.sample.clone", "torch.distributions.Categorical.log_prob.sum", "torch.softmax", "torch.softmax", "srl_custom.SrlCustom.decode().pop", "srl_custom.SrlCustom.span_metric", "allennlp.models.srl_util.convert_bio_tags_to_conll_format", "allennlp.models.srl_util.convert_bio_tags_to_conll_format", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "x.get", "x.get", "srl_custom.SrlCustom.decode", "torch.ones_like.size", "torch.ones_like.size"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.nn.util.get_text_field_mask", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.nn.util.get_device_of", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.nn.util.sequence_cross_entropy_with_logits", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.nn.util.clone", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.params.Params.pop", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.models.srl_util.convert_bio_tags_to_conll_format", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.models.srl_util.convert_bio_tags_to_conll_format", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.params.Params.get", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.params.Params.get", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.coreference_resolution.coref.CoreferenceResolver.decode"], ["", "def", "forward", "(", "\n", "self", ",", "# type: ignore", "\n", "tokens", ":", "TextFieldTensors", ",", "\n", "verb_indicator", ":", "torch", ".", "LongTensor", ",", "\n", "tags", ":", "torch", ".", "LongTensor", "=", "None", ",", "\n", "sample", ":", "bool", "=", "False", ",", "\n", "metadata", ":", "List", "[", "Dict", "[", "str", ",", "Any", "]", "]", "=", "None", ",", "\n", ")", "->", "Dict", "[", "str", ",", "torch", ".", "Tensor", "]", ":", "\n", "# pylint: disable=arguments-differ", "\n", "        ", "embedded_text_input", "=", "self", ".", "embedding_dropout", "(", "self", ".", "text_field_embedder", "(", "tokens", ")", ")", "\n", "mask", "=", "get_text_field_mask", "(", "tokens", ")", "\n", "embedded_verb_indicator", "=", "self", ".", "binary_feature_embedding", "(", "verb_indicator", ".", "long", "(", ")", ")", "\n", "\n", "encoded_text", "=", "self", ".", "encoder", "(", "embedded_text_input", ",", "mask", ")", "\n", "encoded_text_with_verb_indicator", "=", "torch", ".", "cat", "(", "\n", "[", "encoded_text", ",", "embedded_verb_indicator", "]", ",", "-", "1", "\n", ")", "\n", "batch_size", ",", "sequence_length", ",", "_", "=", "encoded_text_with_verb_indicator", ".", "size", "(", ")", "\n", "\n", "# secondary mask: all ones with `secondary_encoded_text`'s shape", "\n", "secondary_mask_size", "=", "[", "\n", "encoded_text_with_verb_indicator", ".", "size", "(", "0", ")", ",", "\n", "encoded_text_with_verb_indicator", ".", "size", "(", "1", ")", ",", "\n", "]", "\n", "d", "=", "get_device_of", "(", "encoded_text_with_verb_indicator", ")", "\n", "secondary_mask", "=", "torch", ".", "ones", "(", "\n", "secondary_mask_size", ",", "\n", "dtype", "=", "torch", ".", "long", ",", "\n", "device", "=", "\"cpu\"", "if", "d", "<", "0", "else", "d", ",", "\n", ")", "\n", "secondary_encoded_text", "=", "self", ".", "secondary_encoder", "(", "\n", "encoded_text_with_verb_indicator", ",", "secondary_mask", "\n", ")", "\n", "\n", "logits", "=", "self", ".", "tag_projection_layer", "(", "secondary_encoded_text", ")", "\n", "reshaped_logits", "=", "logits", ".", "view", "(", "-", "1", ",", "self", ".", "num_classes", ")", "\n", "\n", "output_dict", "=", "dict", "(", ")", "\n", "if", "sample", ":", "\n", "            ", "probs", "=", "Categorical", "(", "F", ".", "softmax", "(", "reshaped_logits", ",", "dim", "=", "-", "1", ")", ")", "\n", "sampled_idx", "=", "probs", ".", "sample", "(", ")", "\n", "log_likelihood", "=", "probs", ".", "log_prob", "(", "\n", "sampled_idx", ".", "clone", "(", ")", "\n", ")", "# to avoid in-place operations", "\n", "scaler", "=", "torch", ".", "ones_like", "(", "reshaped_logits", ")", "\n", "scaler", "[", "torch", ".", "arange", "(", "0", ",", "scaler", ".", "size", "(", "0", ")", ",", "dtype", "=", "torch", ".", "long", ")", ",", "sampled_idx", "]", "*=", "2", "\n", "reshaped_logits", "=", "(", "\n", "reshaped_logits", "*", "scaler", "\n", ")", "# doing this in-place causes error", "\n", "output_dict", "[", "\"log_likelihood\"", "]", "=", "log_likelihood", ".", "sum", "(", ")", "/", "batch_size", "\n", "\n", "", "class_probabilities", "=", "F", ".", "softmax", "(", "reshaped_logits", ",", "dim", "=", "-", "1", ")", ".", "view", "(", "\n", "[", "batch_size", ",", "sequence_length", ",", "self", ".", "num_classes", "]", "\n", ")", "\n", "output_dict", "[", "\"logits\"", "]", "=", "logits", "\n", "output_dict", "[", "\"class_probabilities\"", "]", "=", "class_probabilities", "\n", "output_dict", "[", "\"mask\"", "]", "=", "mask", "\n", "\n", "words", ",", "verbs", ",", "sent_keys", ",", "gold_tags", "=", "zip", "(", "\n", "*", "[", "\n", "(", "x", "[", "\"words\"", "]", ",", "x", "[", "\"verb\"", "]", ",", "x", ".", "get", "(", "\"sent_key\"", ",", "None", ")", ",", "x", ".", "get", "(", "\"gold_tags\"", ",", "None", ")", ")", "\n", "for", "x", "in", "metadata", "\n", "]", "\n", ")", "\n", "output_dict", "[", "\"words\"", "]", "=", "list", "(", "words", ")", "\n", "output_dict", "[", "\"verb\"", "]", "=", "list", "(", "verbs", ")", "\n", "output_dict", "[", "\"sent_key\"", "]", "=", "list", "(", "sent_keys", ")", "\n", "output_dict", "[", "\"gold_tags\"", "]", "=", "list", "(", "gold_tags", ")", "\n", "\n", "if", "tags", "is", "not", "None", ":", "\n", "            ", "loss", "=", "sequence_cross_entropy_with_logits", "(", "\n", "logits", ",", "tags", ",", "mask", ",", "label_smoothing", "=", "self", ".", "_label_smoothing", "\n", ")", "\n", "# remove the last condition if you want metrics during training", "\n", "# NOTE: this will reduce the performance quite a bit", "\n", "if", "self", ".", "finetuning", "or", "(", "\n", "not", "self", ".", "ignore_span_metric", "\n", "and", "self", ".", "span_metric", "is", "not", "None", "\n", "and", "not", "self", ".", "training", "\n", ")", ":", "\n", "                ", "batch_verb_indices", "=", "[", "\n", "example_metadata", "[", "\"verb_index\"", "]", "for", "example_metadata", "in", "metadata", "\n", "]", "\n", "batch_sentences", "=", "[", "\n", "example_metadata", "[", "\"words\"", "]", "for", "example_metadata", "in", "metadata", "\n", "]", "\n", "# Get the BIO tags from decode()", "\n", "# TODO (nfliu): This is kind of a hack, consider splitting out part", "\n", "# of decode() to a separate function.", "\n", "batch_bio_predicted_tags", "=", "self", ".", "decode", "(", "output_dict", ")", ".", "pop", "(", "\"tags\"", ")", "\n", "batch_conll_predicted_tags", "=", "[", "\n", "convert_bio_tags_to_conll_format", "(", "tags", ")", "\n", "for", "tags", "in", "batch_bio_predicted_tags", "\n", "]", "\n", "batch_bio_gold_tags", "=", "[", "\n", "example_metadata", "[", "\"gold_tags\"", "]", "for", "example_metadata", "in", "metadata", "\n", "]", "\n", "batch_conll_gold_tags", "=", "[", "\n", "convert_bio_tags_to_conll_format", "(", "tags", ")", "\n", "for", "tags", "in", "batch_bio_gold_tags", "\n", "]", "\n", "self", ".", "span_metric", "(", "\n", "batch_verb_indices", ",", "\n", "batch_sentences", ",", "\n", "batch_conll_predicted_tags", ",", "\n", "batch_conll_gold_tags", ",", "\n", ")", "\n", "", "output_dict", "[", "\"loss\"", "]", "=", "loss", "\n", "", "return", "output_dict", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.models.srl_custom.SrlCustom.decode": [[224, 259], ["allennlp.nn.util.get_lengths_from_binary_sequence_mask().data.tolist", "srl_custom.SrlCustom.get_viterbi_pairwise_potentials", "srl_custom.SrlCustom.get_start_transitions", "zip", "all_predictions.dim", "allennlp.nn.util.viterbi_decode", "all_tags.append", "all_predictions[].detach().cpu", "srl_custom.SrlCustom.vocab.get_token_from_index", "allennlp.nn.util.get_lengths_from_binary_sequence_mask", "range", "all_predictions[].detach", "all_predictions.size"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.models.srl_bert.SrlBert.get_viterbi_pairwise_potentials", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.models.srl_bert.SrlBert.get_start_transitions", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.nn.util.viterbi_decode", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.data.vocabulary.Vocabulary.get_token_from_index", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.nn.util.get_lengths_from_binary_sequence_mask"], ["", "@", "overrides", "\n", "def", "decode", "(", "self", ",", "output_dict", ":", "Dict", "[", "str", ",", "torch", ".", "Tensor", "]", ")", "->", "Dict", "[", "str", ",", "torch", ".", "Tensor", "]", ":", "\n", "        ", "\"\"\"\n        Does constrained viterbi decoding on class probabilities output in :func:`forward`.  The\n        constraint simply specifies that the output tags must be a valid BIO sequence.  We add a\n        ``\"tags\"`` key to the dictionary with the result.\n        \"\"\"", "\n", "all_predictions", "=", "output_dict", "[", "\"class_probabilities\"", "]", "\n", "sequence_lengths", "=", "get_lengths_from_binary_sequence_mask", "(", "\n", "output_dict", "[", "\"mask\"", "]", "\n", ")", ".", "data", ".", "tolist", "(", ")", "\n", "\n", "if", "all_predictions", ".", "dim", "(", ")", "==", "3", ":", "\n", "            ", "predictions_list", "=", "[", "\n", "all_predictions", "[", "i", "]", ".", "detach", "(", ")", ".", "cpu", "(", ")", "\n", "for", "i", "in", "range", "(", "all_predictions", ".", "size", "(", "0", ")", ")", "\n", "]", "\n", "", "else", ":", "\n", "            ", "predictions_list", "=", "[", "all_predictions", "]", "\n", "", "all_tags", "=", "[", "]", "\n", "transition_matrix", "=", "self", ".", "get_viterbi_pairwise_potentials", "(", ")", "\n", "start_transitions", "=", "self", ".", "get_start_transitions", "(", ")", "\n", "for", "predictions", ",", "length", "in", "zip", "(", "predictions_list", ",", "sequence_lengths", ")", ":", "\n", "            ", "max_likelihood_sequence", ",", "_", "=", "viterbi_decode", "(", "\n", "predictions", "[", ":", "length", "]", ",", "\n", "transition_matrix", ",", "\n", "allowed_start_transitions", "=", "start_transitions", ",", "\n", ")", "\n", "tags", "=", "[", "\n", "self", ".", "vocab", ".", "get_token_from_index", "(", "x", ",", "namespace", "=", "\"labels\"", ")", "\n", "for", "x", "in", "max_likelihood_sequence", "\n", "]", "\n", "all_tags", ".", "append", "(", "tags", ")", "\n", "", "output_dict", "[", "\"tags\"", "]", "=", "all_tags", "\n", "return", "output_dict", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.models.srl_custom.SrlCustom.get_metrics": [[260, 272], ["srl_custom.SrlCustom.span_metric.get_metric", "srl_custom.SrlCustom.items"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.metrics.sequence_accuracy.SequenceAccuracy.get_metric"], ["", "def", "get_metrics", "(", "self", ",", "reset", ":", "bool", "=", "False", ")", ":", "\n", "        ", "if", "self", ".", "ignore_span_metric", ":", "\n", "# Return an empty dictionary if ignoring the", "\n", "# span metric", "\n", "            ", "return", "{", "}", "\n", "\n", "", "else", ":", "\n", "            ", "metric_dict", "=", "self", ".", "span_metric", ".", "get_metric", "(", "reset", "=", "reset", ")", "\n", "\n", "# This can be a lot of metrics, as there are 3 per class.", "\n", "# we only really care about the overall metrics, so we filter for them here.", "\n", "return", "{", "x", ":", "y", "for", "x", ",", "y", "in", "metric_dict", ".", "items", "(", ")", "if", "\"overall\"", "in", "x", "}", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.models.srl_custom.SrlCustom.get_viterbi_pairwise_potentials": [[273, 296], ["srl_custom.SrlCustom.vocab.get_index_to_token_vocabulary", "len", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "srl_custom.SrlCustom.items", "srl_custom.SrlCustom.items", "float"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.data.vocabulary.Vocabulary.get_index_to_token_vocabulary"], ["", "", "def", "get_viterbi_pairwise_potentials", "(", "self", ")", ":", "\n", "        ", "\"\"\"\n        Generate a matrix of pairwise transition potentials for the BIO labels.\n        The only constraint implemented here is that I-XXX labels must be preceded\n        by either an identical I-XXX tag or a B-XXX tag. In order to achieve this\n        constraint, pairs of labels which do not satisfy this constraint have a\n        pairwise potential of -inf.\n        Returns\n        -------\n        transition_matrix : torch.Tensor\n            A (num_labels, num_labels) matrix of pairwise potentials.\n        \"\"\"", "\n", "all_labels", "=", "self", ".", "vocab", ".", "get_index_to_token_vocabulary", "(", "\"labels\"", ")", "\n", "num_labels", "=", "len", "(", "all_labels", ")", "\n", "transition_matrix", "=", "torch", ".", "zeros", "(", "[", "num_labels", ",", "num_labels", "]", ")", "\n", "\n", "for", "i", ",", "previous_label", "in", "all_labels", ".", "items", "(", ")", ":", "\n", "            ", "for", "j", ",", "label", "in", "all_labels", ".", "items", "(", ")", ":", "\n", "# I labels can only be preceded by themselves or", "\n", "# their corresponding B tag.", "\n", "                ", "if", "i", "!=", "j", "and", "label", "[", "0", "]", "==", "\"I\"", "and", "not", "previous_label", "==", "\"B\"", "+", "label", "[", "1", ":", "]", ":", "\n", "                    ", "transition_matrix", "[", "i", ",", "j", "]", "=", "float", "(", "\"-inf\"", ")", "\n", "", "", "", "return", "transition_matrix", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.models.srl_custom.SrlCustom.get_start_transitions": [[297, 317], ["srl_custom.SrlCustom.vocab.get_index_to_token_vocabulary", "len", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "srl_custom.SrlCustom.items", "float"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.data.vocabulary.Vocabulary.get_index_to_token_vocabulary"], ["", "def", "get_start_transitions", "(", "self", ")", ":", "\n", "        ", "\"\"\"\n        In the BIO sequence, we cannot start the sequence with an I-XXX tag.\n        This transition sequence is passed to viterbi_decode to specify this constraint.\n        Returns\n        -------\n        start_transitions : torch.Tensor\n            The pairwise potentials between a START token and\n            the first token of the sequence.\n        \"\"\"", "\n", "all_labels", "=", "self", ".", "vocab", ".", "get_index_to_token_vocabulary", "(", "\"labels\"", ")", "\n", "num_labels", "=", "len", "(", "all_labels", ")", "\n", "\n", "start_transitions", "=", "torch", ".", "zeros", "(", "num_labels", ")", "\n", "\n", "for", "i", ",", "label", "in", "all_labels", ".", "items", "(", ")", ":", "\n", "            ", "if", "label", "[", "0", "]", "==", "\"I\"", ":", "\n", "                ", "start_transitions", "[", "i", "]", "=", "float", "(", "\"-inf\"", ")", "\n", "\n", "", "", "return", "start_transitions", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.models.srl_custom.write_to_conll_eval_file": [[319, 361], ["warnings.warn", "allennlp.models.srl_util.write_bio_formatted_tags_to_file"], "function", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.models.srl_util.write_bio_formatted_tags_to_file"], ["", "", "def", "write_to_conll_eval_file", "(", "\n", "prediction_file", ":", "TextIO", ",", "\n", "gold_file", ":", "TextIO", ",", "\n", "verb_index", ":", "Optional", "[", "int", "]", ",", "\n", "sentence", ":", "List", "[", "str", "]", ",", "\n", "prediction", ":", "List", "[", "str", "]", ",", "\n", "gold_labels", ":", "List", "[", "str", "]", ",", "\n", ")", ":", "\n", "    ", "\"\"\"\n    .. deprecated:: 0.8.4\n       The ``write_to_conll_eval_file`` function was deprecated in favor of the\n       identical ``write_bio_formatted_tags_to_file`` in version 0.8.4.\n    Prints predicate argument predictions and gold labels for a single verbal\n    predicate in a sentence to two provided file references.\n    The CoNLL SRL format is described in\n    `the shared task data README <https://www.lsi.upc.edu/~srlconll/conll05st-release/README>`_ .\n    This function expects IOB2-formatted tags, where the B- tag is used in the beginning\n    of every chunk (i.e. all chunks start with the B- tag).\n    Parameters\n    ----------\n    prediction_file : TextIO, required.\n        A file reference to print predictions to.\n    gold_file : TextIO, required.\n        A file reference to print gold labels to.\n    verb_index : Optional[int], required.\n        The index of the verbal predicate in the sentence which\n        the gold labels are the arguments for, or None if the sentence\n        contains no verbal predicate.\n    sentence : List[str], required.\n        The word tokens.\n    prediction : List[str], required.\n        The predicted BIO labels.\n    gold_labels : List[str], required.\n        The gold BIO labels.\n    \"\"\"", "\n", "warnings", ".", "warn", "(", "\n", "\"The 'write_to_conll_eval_file' function has been deprecated in favor of \"", "\n", "\"the identical 'write_bio_formatted_tags_to_file' function.\"", ",", "\n", "DeprecationWarning", ",", "\n", ")", "\n", "write_bio_formatted_tags_to_file", "(", "\n", "prediction_file", ",", "gold_file", ",", "verb_index", ",", "sentence", ",", "prediction", ",", "gold_labels", "\n", ")", ""]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.models.layerCorefSrl.LayerCorefSrl.__init__": [[41, 140], ["allennlp.models.model.Model.__init__", "params.pop", "allennlp.modules.text_field_embedders.BasicTextFieldEmbedder.from_params", "params.pop", "params.pop.pop", "allennlp.modules.Seq2SeqEncoder.from_params", "params.pop", "params.pop.pop", "params.pop.pop.pop", "params.pop.pop", "allennlp.modules.Seq2SeqEncoder.from_params", "hmtl.models.SrlCustom", "params.pop", "params.pop.pop", "params.pop.pop.pop_bool", "params.pop.pop.pop", "hmtl.models.CoreferenceCustom", "logger.info", "allennlp.nn.InitializerApplicator.from_params", "allennlp.nn.InitializerApplicator", "layerCorefSrl.LayerCorefSrl._encoder.get_output_dim", "params.pop.pop.get", "allennlp.nn.InitializerApplicator.from_params", "allennlp.nn.InitializerApplicator", "params.pop.pop.pop_int", "params.pop.pop.pop_float", "allennlp.modules.FeedForward.from_params", "allennlp.modules.FeedForward.from_params", "params.pop.pop.pop_int", "params.pop.pop.pop_int", "params.pop.pop.pop_float", "params.pop.pop.pop_int", "params.pop.pop.pop_float", "params.pop.pop.pop", "params.pop.pop.pop"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.graph_util.plot.GraphPlot.__init__", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.params.Params.pop", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.tokenizers.tokenizer.Tokenizer.from_params", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.params.Params.pop", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.params.Params.pop", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.tokenizers.tokenizer.Tokenizer.from_params", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.params.Params.pop", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.params.Params.pop", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.params.Params.pop", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.params.Params.pop", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.tokenizers.tokenizer.Tokenizer.from_params", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.params.Params.pop", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.params.Params.pop", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.params.Params.pop_bool", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.params.Params.pop", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.dataset_readers.multiprocess_dataset_reader.logger.info", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.tokenizers.tokenizer.Tokenizer.from_params", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.seq2seq_decoders.auto_regressive_seq_decoder.AutoRegressiveSeqDecoder.get_output_dim", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.params.Params.get", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.tokenizers.tokenizer.Tokenizer.from_params", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.params.Params.pop_int", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.params.Params.pop_float", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.tokenizers.tokenizer.Tokenizer.from_params", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.tokenizers.tokenizer.Tokenizer.from_params", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.params.Params.pop_int", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.params.Params.pop_int", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.params.Params.pop_float", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.params.Params.pop_int", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.params.Params.pop_float", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.params.Params.pop", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.params.Params.pop"], ["def", "__init__", "(", "\n", "self", ",", "\n", "vocab", ":", "Vocabulary", ",", "\n", "params", ":", "Params", ",", "\n", "regularizer", ":", "RegularizerApplicator", "=", "None", ",", "\n", ")", ":", "\n", "\n", "        ", "super", "(", "LayerCorefSrl", ",", "self", ")", ".", "__init__", "(", "vocab", "=", "vocab", ",", "regularizer", "=", "regularizer", ")", "\n", "\n", "# Base text Field Embedder", "\n", "text_field_embedder_params", "=", "params", ".", "pop", "(", "\"text_field_embedder\"", ")", "\n", "text_field_embedder", "=", "BasicTextFieldEmbedder", ".", "from_params", "(", "\n", "vocab", "=", "vocab", ",", "params", "=", "text_field_embedder_params", "\n", ")", "\n", "self", ".", "_text_field_embedder", "=", "text_field_embedder", "\n", "\n", "###############", "\n", "# Common Stuffs", "\n", "###############", "\n", "common_params", "=", "params", ".", "pop", "(", "\"common\"", ")", "\n", "\n", "# Encoder", "\n", "encoder_params", "=", "common_params", ".", "pop", "(", "\"encoder\"", ")", "\n", "encoder", "=", "Seq2SeqEncoder", ".", "from_params", "(", "encoder_params", ")", "\n", "self", ".", "_encoder", "=", "encoder", "\n", "\n", "############", "\n", "# SRL Stuffs", "\n", "############", "\n", "srl_params", "=", "params", ".", "pop", "(", "\"srl\"", ")", "\n", "\n", "# Tagger: SRL", "\n", "tagger_srl_params", "=", "srl_params", ".", "pop", "(", "\"tagger\"", ")", "\n", "init_params", "=", "tagger_srl_params", ".", "pop", "(", "\"initializer\"", ",", "None", ")", "\n", "initializer", "=", "(", "\n", "InitializerApplicator", ".", "from_params", "(", "init_params", ")", "\n", "if", "init_params", "is", "not", "None", "\n", "else", "InitializerApplicator", "(", ")", "\n", ")", "\n", "\n", "# Secondary Encoder", "\n", "secondary_encoder_params", "=", "srl_params", ".", "pop", "(", "\"secondary_encoder\"", ")", "\n", "secondary_encoder_params", "[", "\n", "\"input_size\"", "\n", "]", "=", "self", ".", "_encoder", ".", "get_output_dim", "(", ")", "+", "tagger_srl_params", ".", "get", "(", "\"binary_feature_dim\"", ")", "\n", "secondary_encoder", "=", "Seq2SeqEncoder", ".", "from_params", "(", "secondary_encoder_params", ")", "\n", "self", ".", "_secondary_encoder", "=", "secondary_encoder", "\n", "\n", "tagger_srl", "=", "SrlCustom", "(", "\n", "vocab", "=", "vocab", ",", "\n", "text_field_embedder", "=", "self", ".", "_text_field_embedder", ",", "\n", "encoder", "=", "self", ".", "_encoder", ",", "\n", "secondary_encoder", "=", "self", ".", "_secondary_encoder", ",", "\n", "binary_feature_dim", "=", "tagger_srl_params", ".", "pop_int", "(", "\"binary_feature_dim\"", ",", "100", ")", ",", "\n", "label_smoothing", "=", "tagger_srl_params", ".", "pop_float", "(", "\"lexical_dropout\"", ",", "0.1", ")", ",", "\n", "initializer", "=", "initializer", ",", "\n", ")", "\n", "self", ".", "_tagger_srl", "=", "tagger_srl", "\n", "\n", "##############", "\n", "# Coref Stuffs", "\n", "##############", "\n", "coref_params", "=", "params", ".", "pop", "(", "\"coref\"", ")", "\n", "\n", "# Tagger: Coreference", "\n", "tagger_coref_params", "=", "coref_params", ".", "pop", "(", "\"tagger\"", ")", "\n", "eval_on_gold_mentions", "=", "tagger_coref_params", ".", "pop_bool", "(", "\n", "\"eval_on_gold_mentions\"", ",", "False", "\n", ")", "\n", "init_params", "=", "tagger_coref_params", ".", "pop", "(", "\"initializer\"", ",", "None", ")", "\n", "initializer", "=", "(", "\n", "InitializerApplicator", ".", "from_params", "(", "init_params", ")", "\n", "if", "init_params", "is", "not", "None", "\n", "else", "InitializerApplicator", "(", ")", "\n", ")", "\n", "\n", "tagger_coref", "=", "CoreferenceCustom", "(", "\n", "vocab", "=", "vocab", ",", "\n", "text_field_embedder", "=", "self", ".", "_text_field_embedder", ",", "\n", "context_layer", "=", "self", ".", "_encoder", ",", "\n", "mention_feedforward", "=", "FeedForward", ".", "from_params", "(", "\n", "tagger_coref_params", ".", "pop", "(", "\"mention_feedforward\"", ")", "\n", ")", ",", "\n", "antecedent_feedforward", "=", "FeedForward", ".", "from_params", "(", "\n", "tagger_coref_params", ".", "pop", "(", "\"antecedent_feedforward\"", ")", "\n", ")", ",", "\n", "feature_size", "=", "tagger_coref_params", ".", "pop_int", "(", "\"feature_size\"", ")", ",", "\n", "max_span_width", "=", "tagger_coref_params", ".", "pop_int", "(", "\"max_span_width\"", ")", ",", "\n", "spans_per_word", "=", "tagger_coref_params", ".", "pop_float", "(", "\"spans_per_word\"", ")", ",", "\n", "max_antecedents", "=", "tagger_coref_params", ".", "pop_int", "(", "\"max_antecedents\"", ")", ",", "\n", "lexical_dropout", "=", "tagger_coref_params", ".", "pop_float", "(", "\"lexical_dropout\"", ",", "0.2", ")", ",", "\n", "initializer", "=", "initializer", ",", "\n", "eval_on_gold_mentions", "=", "eval_on_gold_mentions", ",", "\n", ")", "\n", "self", ".", "_tagger_coref", "=", "tagger_coref", "\n", "if", "eval_on_gold_mentions", ":", "\n", "            ", "self", ".", "_tagger_coref", ".", "_eval_on_gold_mentions", "=", "True", "\n", "\n", "", "logger", ".", "info", "(", "\"Multi-Task Learning Model has been instantiated.\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.models.layerCorefSrl.LayerCorefSrl.forward": [[141, 165], ["getattr", "getattr.forward"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.graph_util.graph_encoder_gat.GAT.forward"], ["", "@", "overrides", "\n", "def", "forward", "(", "\n", "self", ",", "\n", "tensor_batch", ",", "\n", "for_training", ":", "bool", "=", "False", ",", "\n", "task_name", ":", "str", "=", "\"srl\"", ",", "\n", "sample", ":", "bool", "=", "False", ",", "\n", ")", "->", "Dict", "[", "str", ",", "torch", ".", "Tensor", "]", ":", "\n", "# pylint: disable=arguments-differ", "\n", "        ", "\"\"\"\n        Special case for forward: for coreference, we can use gold mentions to predict the clusters\n        during evaluation (not during training).\n        \"\"\"", "\n", "\n", "tagger", "=", "getattr", "(", "self", ",", "\"_tagger_%s\"", "%", "task_name", ")", "\n", "\n", "if", "task_name", "==", "\"coref\"", "and", "tagger", ".", "_eval_on_gold_mentions", ":", "\n", "            ", "if", "for_training", ":", "\n", "                ", "tagger", ".", "_use_gold_mentions", "=", "False", "\n", "", "else", ":", "\n", "                ", "tagger", ".", "_use_gold_mentions", "=", "True", "\n", "\n", "", "", "tensor_batch", "[", "\"sample\"", "]", "=", "sample", "\n", "return", "tagger", ".", "forward", "(", "**", "tensor_batch", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.models.layerCorefSrl.LayerCorefSrl.decode": [[166, 170], ["getattr", "getattr.decode"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.coreference_resolution.coref.CoreferenceResolver.decode"], ["", "@", "overrides", "\n", "def", "decode", "(", "self", ",", "output_dict", ":", "Dict", "[", "str", ",", "torch", ".", "Tensor", "]", ",", "task_name", ":", "str", "=", "\"srl\"", ")", ":", "\n", "        ", "tagger", "=", "getattr", "(", "self", ",", "\"_tagger_%s\"", "%", "task_name", ")", "\n", "return", "tagger", ".", "decode", "(", "output_dict", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.models.layerCorefSrl.LayerCorefSrl.get_metrics": [[171, 181], ["getattr", "getattr.get_metrics", "getattr.get_metrics"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.coreference_resolution.coref.CoreferenceResolver.get_metrics", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.coreference_resolution.coref.CoreferenceResolver.get_metrics"], ["", "@", "overrides", "\n", "def", "get_metrics", "(", "\n", "self", ",", "task_name", ":", "str", "=", "\"srl\"", ",", "reset", ":", "bool", "=", "False", ",", "full", ":", "bool", "=", "False", "\n", ")", "->", "Dict", "[", "str", ",", "float", "]", ":", "\n", "\n", "        ", "task_tagger", "=", "getattr", "(", "self", ",", "\"_tagger_\"", "+", "task_name", ")", "\n", "if", "full", "and", "task_name", "==", "\"coref\"", ":", "\n", "            ", "return", "task_tagger", ".", "get_metrics", "(", "reset", "=", "reset", ",", "full", "=", "full", ")", "\n", "", "else", ":", "\n", "            ", "return", "task_tagger", ".", "get_metrics", "(", "reset", "=", "reset", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.models.layerCorefSrl.LayerCorefSrl.from_params": [[182, 191], ["cls"], "methods", ["None"], ["", "", "@", "classmethod", "\n", "def", "from_params", "(", "\n", "cls", ",", "\n", "vocab", ":", "Vocabulary", ",", "\n", "params", ":", "Params", ",", "\n", "regularizer", ":", "RegularizerApplicator", ",", "\n", "**", "kwargs", "\n", ")", "->", "\"LayerCorefSrl\"", ":", "\n", "        ", "return", "cls", "(", "vocab", "=", "vocab", ",", "params", "=", "params", ",", "regularizer", "=", "regularizer", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.models.srl_custom_bert.SrlCustomBert.__init__": [[49, 83], ["allennlp.nn.InitializerApplicator", "allennlp.models.model.Model.__init__", "isinstance", "srl_custom_bert.SrlCustomBert.vocab.get_vocab_size", "torch.nn.modules.Linear", "torch.nn.modules.Linear", "torch.nn.modules.Dropout", "torch.nn.modules.Dropout", "initializer", "transformers.modeling_bert.BertModel.from_pretrained", "allennlp.training.metrics.srl_eval_scorer.SrlEvalScorer"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.graph_util.plot.GraphPlot.__init__", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.data.vocabulary.Vocabulary.get_vocab_size"], ["def", "__init__", "(", "\n", "self", ",", "\n", "vocab", ":", "Vocabulary", ",", "\n", "bert_model", ":", "Union", "[", "str", ",", "BertModel", "]", ",", "\n", "embedding_dropout", ":", "float", "=", "0.0", ",", "\n", "initializer", ":", "InitializerApplicator", "=", "InitializerApplicator", "(", ")", ",", "\n", "label_smoothing", ":", "float", "=", "None", ",", "\n", "ignore_span_metric", ":", "bool", "=", "False", ",", "\n", "srl_eval_path", ":", "str", "=", "DEFAULT_SRL_EVAL_PATH", ",", "\n", "**", "kwargs", ",", "\n", ")", "->", "None", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "vocab", ",", "**", "kwargs", ")", "\n", "\n", "self", ".", "finetuning", "=", "False", "# this is set only when performing policy gradient", "\n", "if", "isinstance", "(", "bert_model", ",", "str", ")", ":", "\n", "            ", "self", ".", "bert_model", "=", "BertModel", ".", "from_pretrained", "(", "bert_model", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "bert_model", "=", "bert_model", "\n", "\n", "", "self", ".", "num_classes", "=", "self", ".", "vocab", ".", "get_vocab_size", "(", "\"labels\"", ")", "\n", "if", "srl_eval_path", "is", "not", "None", ":", "\n", "# For the span based evaluation, we don't want to consider labels", "\n", "# for verb, because the verb index is provided to the model.", "\n", "            ", "self", ".", "span_metric", "=", "SrlEvalScorer", "(", "srl_eval_path", ",", "ignore_classes", "=", "[", "\"V\"", "]", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "span_metric", "=", "None", "\n", "", "self", ".", "tag_projection_layer", "=", "Linear", "(", "\n", "self", ".", "bert_model", ".", "config", ".", "hidden_size", ",", "self", ".", "num_classes", "\n", ")", "\n", "\n", "self", ".", "embedding_dropout", "=", "Dropout", "(", "p", "=", "embedding_dropout", ")", "\n", "self", ".", "_label_smoothing", "=", "label_smoothing", "\n", "self", ".", "ignore_span_metric", "=", "ignore_span_metric", "\n", "initializer", "(", "self", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.models.srl_custom_bert.SrlCustomBert.forward": [[84, 222], ["allennlp.nn.util.get_text_field_mask", "srl_custom_bert.SrlCustomBert.bert_model", "srl_custom_bert.SrlCustomBert.embedding_dropout", "srl_custom_bert.SrlCustomBert.size", "srl_custom_bert.SrlCustomBert.tag_projection_layer", "srl_custom_bert.SrlCustomBert.view", "dict", "torch.softmax().view", "torch.softmax().view", "zip", "list", "list", "list", "list", "list", "nan_mask.sum", "torch.distributions.Categorical", "torch.distributions.Categorical", "torch.distributions.Categorical.sample", "torch.distributions.Categorical.sample", "torch.distributions.Categorical.log_prob", "torch.distributions.Categorical.log_prob", "torch.ones_like", "torch.ones_like", "torch.ones_like", "torch.ones_like", "allennlp.nn.util.sequence_cross_entropy_with_logits", "allennlp.nn.util.get_token_ids_from_text_field_tensors", "torch.softmax", "torch.softmax", "torch.distributions.Categorical.sample.clone", "torch.distributions.Categorical.log_prob.sum", "torch.softmax", "torch.softmax", "srl_custom_bert.SrlCustomBert.decode().pop", "srl_custom_bert.SrlCustomBert.span_metric", "allennlp.models.srl_util.convert_bio_tags_to_conll_format", "allennlp.models.srl_util.convert_bio_tags_to_conll_format", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "x.get", "x.get", "srl_custom_bert.SrlCustomBert.decode", "torch.ones_like.size", "torch.ones_like.size"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.nn.util.get_text_field_mask", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.nn.util.sequence_cross_entropy_with_logits", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.nn.util.get_token_ids_from_text_field_tensors", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.nn.util.clone", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.params.Params.pop", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.models.srl_util.convert_bio_tags_to_conll_format", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.models.srl_util.convert_bio_tags_to_conll_format", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.params.Params.get", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.params.Params.get", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.coreference_resolution.coref.CoreferenceResolver.decode"], ["", "def", "forward", "(", "# type: ignore", "\n", "self", ",", "\n", "tokens", ":", "TextFieldTensors", ",", "\n", "verb_indicator", ":", "torch", ".", "Tensor", ",", "\n", "metadata", ":", "List", "[", "Any", "]", ",", "\n", "tags", ":", "torch", ".", "LongTensor", "=", "None", ",", "\n", "sample", ":", "bool", "=", "False", ",", "\n", ")", ":", "\n", "\n", "        ", "\"\"\"\n        # Parameters\n\n        tokens : TextFieldTensors, required\n            The output of `TextField.as_array()`, which should typically be passed directly to a\n            `TextFieldEmbedder`. For this model, this must be a `SingleIdTokenIndexer` which\n            indexes wordpieces from the BERT vocabulary.\n        verb_indicator: torch.LongTensor, required.\n            An integer `SequenceFeatureField` representation of the position of the verb\n            in the sentence. This should have shape (batch_size, num_tokens) and importantly, can be\n            all zeros, in the case that the sentence has no verbal predicate.\n        tags : torch.LongTensor, optional (default = None)\n            A torch tensor representing the sequence of integer gold class labels\n            of shape `(batch_size, num_tokens)`\n        metadata : `List[Dict[str, Any]]`, optional, (default = None)\n            metadata containg the original words in the sentence, the verb to compute the\n            frame for, and start offsets for converting wordpieces back to a sequence of words,\n            under 'words', 'verb' and 'offsets' keys, respectively.\n\n        # Returns\n\n        An output dictionary consisting of:\n        logits : torch.FloatTensor\n            A tensor of shape `(batch_size, num_tokens, tag_vocab_size)` representing\n            unnormalised log probabilities of the tag classes.\n        class_probabilities : torch.FloatTensor\n            A tensor of shape `(batch_size, num_tokens, tag_vocab_size)` representing\n            a distribution of the tag classes per word.\n        loss : torch.FloatTensor, optional\n            A scalar loss to be optimised.\n        \"\"\"", "\n", "mask", "=", "get_text_field_mask", "(", "tokens", ")", "\n", "bert_embeddings", ",", "_", "=", "self", ".", "bert_model", "(", "\n", "input_ids", "=", "util", ".", "get_token_ids_from_text_field_tensors", "(", "tokens", ")", ",", "\n", "token_type_ids", "=", "verb_indicator", ",", "\n", "attention_mask", "=", "mask", ",", "\n", ")", "\n", "\n", "# This was a bug in the old allennlp. Fixed in the custom version.", "\n", "nan_mask", "=", "bert_embeddings", "!=", "bert_embeddings", "\n", "found_nan", "=", "False", "\n", "if", "nan_mask", ".", "sum", "(", ")", ">", "0", ":", "\n", "            ", "found_nan", "=", "True", "\n", "# logger.info(\"srl: {} values in bert embeddings have NaN.\".format(nan_mask.sum()))", "\n", "bert_embeddings", "[", "nan_mask", "]", "=", "0", "# is this a good value?", "\n", "\n", "", "embedded_text_input", "=", "self", ".", "embedding_dropout", "(", "bert_embeddings", ")", "\n", "batch_size", ",", "sequence_length", ",", "_", "=", "embedded_text_input", ".", "size", "(", ")", "\n", "logits", "=", "self", ".", "tag_projection_layer", "(", "embedded_text_input", ")", "\n", "\n", "reshaped_logits", "=", "logits", ".", "view", "(", "-", "1", ",", "self", ".", "num_classes", ")", "\n", "\n", "output_dict", "=", "dict", "(", ")", "\n", "if", "sample", ":", "\n", "            ", "probs", "=", "Categorical", "(", "F", ".", "softmax", "(", "reshaped_logits", ",", "dim", "=", "-", "1", ")", ")", "\n", "sampled_idx", "=", "probs", ".", "sample", "(", ")", "\n", "log_likelihood", "=", "probs", ".", "log_prob", "(", "\n", "sampled_idx", ".", "clone", "(", ")", "\n", ")", "# to avoid in-place operations", "\n", "scaler", "=", "torch", ".", "ones_like", "(", "reshaped_logits", ")", "\n", "scaler", "[", "\n", "torch", ".", "arange", "(", "0", ",", "scaler", ".", "size", "(", "0", ")", ",", "dtype", "=", "torch", ".", "long", ")", ",", "sampled_idx", "\n", "]", "*=", "2", "\n", "reshaped_logits", "=", "(", "\n", "reshaped_logits", "*", "scaler", "\n", ")", "# doing this in-place causes error", "\n", "output_dict", "[", "\"log_likelihood\"", "]", "=", "log_likelihood", ".", "sum", "(", ")", "/", "batch_size", "\n", "\n", "", "class_probabilities", "=", "F", ".", "softmax", "(", "reshaped_logits", ",", "dim", "=", "-", "1", ")", ".", "view", "(", "\n", "[", "batch_size", ",", "sequence_length", ",", "self", ".", "num_classes", "]", "\n", ")", "\n", "output_dict", "[", "\"logits\"", "]", "=", "logits", "\n", "output_dict", "[", "\"class_probabilities\"", "]", "=", "class_probabilities", "\n", "output_dict", "[", "\"mask\"", "]", "=", "mask", "\n", "\n", "# We add in the offsets here so we can compute the un-wordpieced tags.", "\n", "words", ",", "verbs", ",", "offsets", ",", "sent_keys", ",", "gold_tags", "=", "zip", "(", "\n", "*", "[", "\n", "(", "x", "[", "\"words\"", "]", ",", "x", "[", "\"verb\"", "]", ",", "x", "[", "\"offsets\"", "]", ",", "x", ".", "get", "(", "\"sent_key\"", ",", "None", ")", ",", "x", ".", "get", "(", "\"gold_tags\"", ",", "None", ")", ")", "\n", "for", "x", "in", "metadata", "\n", "]", "\n", ")", "\n", "output_dict", "[", "\"words\"", "]", "=", "list", "(", "words", ")", "\n", "output_dict", "[", "\"verb\"", "]", "=", "list", "(", "verbs", ")", "\n", "output_dict", "[", "\"sent_key\"", "]", "=", "list", "(", "sent_keys", ")", "\n", "output_dict", "[", "\"wordpiece_offsets\"", "]", "=", "list", "(", "offsets", ")", "\n", "output_dict", "[", "\"gold_tags\"", "]", "=", "list", "(", "gold_tags", ")", "\n", "\n", "if", "tags", "is", "not", "None", ":", "\n", "            ", "loss", "=", "sequence_cross_entropy_with_logits", "(", "\n", "logits", ",", "tags", ",", "mask", ",", "label_smoothing", "=", "self", ".", "_label_smoothing", "\n", ")", "\n", "# remove the last condition if you want metrics during training", "\n", "# NOTE: this will reduce the performance quite a bit", "\n", "if", "self", ".", "finetuning", "or", "(", "\n", "not", "self", ".", "ignore_span_metric", "\n", "and", "self", ".", "span_metric", "is", "not", "None", "\n", "and", "not", "self", ".", "training", "\n", ")", ":", "\n", "                ", "batch_verb_indices", "=", "[", "\n", "example_metadata", "[", "\"verb_index\"", "]", "for", "example_metadata", "in", "metadata", "\n", "]", "\n", "batch_sentences", "=", "[", "\n", "example_metadata", "[", "\"words\"", "]", "for", "example_metadata", "in", "metadata", "\n", "]", "\n", "# Get the BIO tags from decode()", "\n", "# TODO (nfliu): This is kind of a hack, consider splitting out part", "\n", "# of decode() to a separate function.", "\n", "batch_bio_predicted_tags", "=", "self", ".", "decode", "(", "output_dict", ")", ".", "pop", "(", "\"tags\"", ")", "\n", "batch_conll_predicted_tags", "=", "[", "\n", "convert_bio_tags_to_conll_format", "(", "tags", ")", "\n", "for", "tags", "in", "batch_bio_predicted_tags", "\n", "]", "\n", "batch_bio_gold_tags", "=", "[", "\n", "example_metadata", "[", "\"gold_tags\"", "]", "for", "example_metadata", "in", "metadata", "\n", "]", "\n", "batch_conll_gold_tags", "=", "[", "\n", "convert_bio_tags_to_conll_format", "(", "tags", ")", "\n", "for", "tags", "in", "batch_bio_gold_tags", "\n", "]", "\n", "self", ".", "span_metric", "(", "\n", "batch_verb_indices", ",", "\n", "batch_sentences", ",", "\n", "batch_conll_predicted_tags", ",", "\n", "batch_conll_gold_tags", ",", "\n", ")", "\n", "", "output_dict", "[", "\"loss\"", "]", "=", "loss", "\n", "output_dict", "[", "\"found_nan\"", "]", "=", "found_nan", "\n", "", "return", "output_dict", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.models.srl_custom_bert.SrlCustomBert.decode": [[223, 278], ["allennlp.nn.util.get_lengths_from_binary_sequence_mask().data.tolist", "srl_custom_bert.SrlCustomBert.get_viterbi_pairwise_potentials", "srl_custom_bert.SrlCustomBert.get_start_transitions", "zip", "all_predictions.dim", "allennlp.nn.util.viterbi_decode", "wordpiece_tags.append", "word_tags.append", "all_predictions[].detach().cpu", "srl_custom_bert.SrlCustomBert.vocab.get_token_from_index", "allennlp.nn.util.get_lengths_from_binary_sequence_mask", "range", "all_predictions[].detach", "all_predictions.size"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.models.srl_bert.SrlBert.get_viterbi_pairwise_potentials", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.models.srl_bert.SrlBert.get_start_transitions", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.nn.util.viterbi_decode", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.data.vocabulary.Vocabulary.get_token_from_index", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.nn.util.get_lengths_from_binary_sequence_mask"], ["", "@", "overrides", "\n", "def", "decode", "(", "self", ",", "output_dict", ":", "Dict", "[", "str", ",", "torch", ".", "Tensor", "]", ")", "->", "Dict", "[", "str", ",", "torch", ".", "Tensor", "]", ":", "\n", "        ", "\"\"\"\n        Does constrained viterbi decoding on class probabilities output in :func:`forward`.  The\n        constraint simply specifies that the output tags must be a valid BIO sequence.  We add a\n        `\"tags\"` key to the dictionary with the result.\n\n        NOTE: First, we decode a BIO sequence on top of the wordpieces. This is important; viterbi\n        decoding produces low quality output if you decode on top of word representations directly,\n        because the model gets confused by the 'missing' positions (which is sensible as it is trained\n        to perform tagging on wordpieces, not words).\n\n        Secondly, it's important that the indices we use to recover words from the wordpieces are the\n        start_offsets (i.e offsets which correspond to using the first wordpiece of words which are\n        tokenized into multiple wordpieces) as otherwise, we might get an ill-formed BIO sequence\n        when we select out the word tags from the wordpiece tags. This happens in the case that a word\n        is split into multiple word pieces, and then we take the last tag of the word, which might\n        correspond to, e.g, I-V, which would not be allowed as it is not preceeded by a B tag.\n        \"\"\"", "\n", "all_predictions", "=", "output_dict", "[", "\"class_probabilities\"", "]", "\n", "sequence_lengths", "=", "get_lengths_from_binary_sequence_mask", "(", "\n", "output_dict", "[", "\"mask\"", "]", "\n", ")", ".", "data", ".", "tolist", "(", ")", "\n", "\n", "if", "all_predictions", ".", "dim", "(", ")", "==", "3", ":", "\n", "            ", "predictions_list", "=", "[", "\n", "all_predictions", "[", "i", "]", ".", "detach", "(", ")", ".", "cpu", "(", ")", "\n", "for", "i", "in", "range", "(", "all_predictions", ".", "size", "(", "0", ")", ")", "\n", "]", "\n", "", "else", ":", "\n", "            ", "predictions_list", "=", "[", "all_predictions", "]", "\n", "", "wordpiece_tags", "=", "[", "]", "\n", "word_tags", "=", "[", "]", "\n", "transition_matrix", "=", "self", ".", "get_viterbi_pairwise_potentials", "(", ")", "\n", "start_transitions", "=", "self", ".", "get_start_transitions", "(", ")", "\n", "# **************** Different ********************", "\n", "# We add in the offsets here so we can compute the un-wordpieced tags.", "\n", "for", "predictions", ",", "length", ",", "offsets", "in", "zip", "(", "\n", "predictions_list", ",", "sequence_lengths", ",", "output_dict", "[", "\"wordpiece_offsets\"", "]", "\n", ")", ":", "\n", "            ", "max_likelihood_sequence", ",", "_", "=", "viterbi_decode", "(", "\n", "predictions", "[", ":", "length", "]", ",", "\n", "transition_matrix", ",", "\n", "allowed_start_transitions", "=", "start_transitions", ",", "\n", ")", "\n", "tags", "=", "[", "\n", "self", ".", "vocab", ".", "get_token_from_index", "(", "x", ",", "namespace", "=", "\"labels\"", ")", "\n", "for", "x", "in", "max_likelihood_sequence", "\n", "]", "\n", "\n", "wordpiece_tags", ".", "append", "(", "tags", ")", "\n", "word_tags", ".", "append", "(", "[", "tags", "[", "i", "]", "for", "i", "in", "offsets", "]", ")", "\n", "", "output_dict", "[", "\"wordpiece_tags\"", "]", "=", "wordpiece_tags", "\n", "output_dict", "[", "\"tags\"", "]", "=", "word_tags", "\n", "return", "output_dict", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.models.srl_custom_bert.SrlCustomBert.get_metrics": [[279, 291], ["srl_custom_bert.SrlCustomBert.span_metric.get_metric", "srl_custom_bert.SrlCustomBert.items"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.metrics.sequence_accuracy.SequenceAccuracy.get_metric"], ["", "def", "get_metrics", "(", "self", ",", "reset", ":", "bool", "=", "False", ")", ":", "\n", "        ", "if", "self", ".", "ignore_span_metric", ":", "\n", "# Return an empty dictionary if ignoring the", "\n", "# span metric", "\n", "            ", "return", "{", "}", "\n", "\n", "", "else", ":", "\n", "            ", "metric_dict", "=", "self", ".", "span_metric", ".", "get_metric", "(", "reset", "=", "reset", ")", "\n", "\n", "# This can be a lot of metrics, as there are 3 per class.", "\n", "# we only really care about the overall metrics, so we filter for them here.", "\n", "return", "{", "x", ":", "y", "for", "x", ",", "y", "in", "metric_dict", ".", "items", "(", ")", "if", "\"overall\"", "in", "x", "}", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.models.srl_custom_bert.SrlCustomBert.get_viterbi_pairwise_potentials": [[292, 316], ["srl_custom_bert.SrlCustomBert.vocab.get_index_to_token_vocabulary", "len", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "srl_custom_bert.SrlCustomBert.items", "srl_custom_bert.SrlCustomBert.items", "float"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.data.vocabulary.Vocabulary.get_index_to_token_vocabulary"], ["", "", "def", "get_viterbi_pairwise_potentials", "(", "self", ")", ":", "\n", "        ", "\"\"\"\n        Generate a matrix of pairwise transition potentials for the BIO labels.\n        The only constraint implemented here is that I-XXX labels must be preceded\n        by either an identical I-XXX tag or a B-XXX tag. In order to achieve this\n        constraint, pairs of labels which do not satisfy this constraint have a\n        pairwise potential of -inf.\n\n        # Returns\n\n        transition_matrix : torch.Tensor\n            A (num_labels, num_labels) matrix of pairwise potentials.\n        \"\"\"", "\n", "all_labels", "=", "self", ".", "vocab", ".", "get_index_to_token_vocabulary", "(", "\"labels\"", ")", "\n", "num_labels", "=", "len", "(", "all_labels", ")", "\n", "transition_matrix", "=", "torch", ".", "zeros", "(", "[", "num_labels", ",", "num_labels", "]", ")", "\n", "\n", "for", "i", ",", "previous_label", "in", "all_labels", ".", "items", "(", ")", ":", "\n", "            ", "for", "j", ",", "label", "in", "all_labels", ".", "items", "(", ")", ":", "\n", "# I labels can only be preceded by themselves or", "\n", "# their corresponding B tag.", "\n", "                ", "if", "i", "!=", "j", "and", "label", "[", "0", "]", "==", "\"I\"", "and", "not", "previous_label", "==", "\"B\"", "+", "label", "[", "1", ":", "]", ":", "\n", "                    ", "transition_matrix", "[", "i", ",", "j", "]", "=", "float", "(", "\"-inf\"", ")", "\n", "", "", "", "return", "transition_matrix", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.models.srl_custom_bert.SrlCustomBert.get_start_transitions": [[317, 338], ["srl_custom_bert.SrlCustomBert.vocab.get_index_to_token_vocabulary", "len", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "srl_custom_bert.SrlCustomBert.items", "float"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.data.vocabulary.Vocabulary.get_index_to_token_vocabulary"], ["", "def", "get_start_transitions", "(", "self", ")", ":", "\n", "        ", "\"\"\"\n        In the BIO sequence, we cannot start the sequence with an I-XXX tag.\n        This transition sequence is passed to viterbi_decode to specify this constraint.\n\n        # Returns\n\n        start_transitions : torch.Tensor\n            The pairwise potentials between a START token and\n            the first token of the sequence.\n        \"\"\"", "\n", "all_labels", "=", "self", ".", "vocab", ".", "get_index_to_token_vocabulary", "(", "\"labels\"", ")", "\n", "num_labels", "=", "len", "(", "all_labels", ")", "\n", "\n", "start_transitions", "=", "torch", ".", "zeros", "(", "num_labels", ")", "\n", "\n", "for", "i", ",", "label", "in", "all_labels", ".", "items", "(", ")", ":", "\n", "            ", "if", "label", "[", "0", "]", "==", "\"I\"", ":", "\n", "                ", "start_transitions", "[", "i", "]", "=", "float", "(", "\"-inf\"", ")", "\n", "\n", "", "", "return", "start_transitions", "\n", "", "", ""]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.models.layerSrl.LayerSrl.__init__": [[48, 116], ["allennlp.models.model.Model.__init__", "params.get", "logger.info", "logger.info", "params.pop", "transformers.AutoModel.from_pretrained", "hmtl.models.SrlCustomBert", "params.pop", "allennlp.modules.text_field_embedders.BasicTextFieldEmbedder.from_params", "params.pop", "params.pop.pop", "allennlp.modules.Seq2SeqEncoder.from_params", "params.pop.pop", "allennlp.modules.Seq2SeqEncoder.from_params", "params.pop.pop", "hmtl.models.SrlCustom", "layerSrl.LayerSrl._encoder.get_output_dim", "params.pop.get", "allennlp.nn.InitializerApplicator.from_params", "allennlp.nn.InitializerApplicator", "params.pop_float", "params.pop_float", "params.pop.pop_int", "params.pop.pop_float"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.graph_util.plot.GraphPlot.__init__", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.params.Params.get", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.dataset_readers.multiprocess_dataset_reader.logger.info", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.dataset_readers.multiprocess_dataset_reader.logger.info", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.params.Params.pop", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.params.Params.pop", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.tokenizers.tokenizer.Tokenizer.from_params", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.params.Params.pop", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.params.Params.pop", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.tokenizers.tokenizer.Tokenizer.from_params", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.params.Params.pop", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.tokenizers.tokenizer.Tokenizer.from_params", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.params.Params.pop", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.seq2seq_decoders.auto_regressive_seq_decoder.AutoRegressiveSeqDecoder.get_output_dim", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.params.Params.get", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.tokenizers.tokenizer.Tokenizer.from_params", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.params.Params.pop_float", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.params.Params.pop_float", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.params.Params.pop_int", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.params.Params.pop_float"], ["def", "__init__", "(", "\n", "self", ",", "\n", "vocab", ":", "Vocabulary", ",", "\n", "params", ":", "Params", ",", "\n", "regularizer", ":", "RegularizerApplicator", "=", "None", ",", "\n", ")", ":", "\n", "\n", "        ", "super", "(", "LayerSrl", ",", "self", ")", ".", "__init__", "(", "vocab", "=", "vocab", ",", "regularizer", "=", "regularizer", ")", "\n", "\n", "if", "params", ".", "get", "(", "\"bert_model\"", ",", "None", ")", ":", "\n", "            ", "logger", ".", "info", "(", "\"Loading BERT as embedder for Srl\"", ")", "\n", "_bert_model_name", "=", "params", ".", "pop", "(", "\"bert_model\"", ")", "\n", "self", ".", "bert_model", "=", "AutoModel", ".", "from_pretrained", "(", "_bert_model_name", ")", "\n", "self", ".", "bert_model", ".", "_name", "=", "_bert_model_name", "\n", "tagger_srl", "=", "SrlCustomBert", "(", "\n", "vocab", "=", "vocab", ",", "\n", "bert_model", "=", "self", ".", "bert_model", ",", "\n", "embedding_dropout", "=", "params", ".", "pop_float", "(", "\"embedding_dropout\"", ")", ",", "\n", "label_smoothing", "=", "params", ".", "pop_float", "(", "\"label_smoothing\"", ")", ",", "\n", ")", "\n", "self", ".", "_tagger_srl", "=", "tagger_srl", "\n", "\n", "", "else", ":", "\n", "# Base text Field Embedder", "\n", "            ", "text_field_embedder_params", "=", "params", ".", "pop", "(", "\"text_field_embedder\"", ")", "\n", "text_field_embedder", "=", "BasicTextFieldEmbedder", ".", "from_params", "(", "\n", "vocab", "=", "vocab", ",", "params", "=", "text_field_embedder_params", "\n", ")", "\n", "self", ".", "_text_field_embedder", "=", "text_field_embedder", "\n", "\n", "##############", "\n", "# SRL Stuffs", "\n", "##############", "\n", "srl_params", "=", "params", ".", "pop", "(", "\"srl\"", ")", "\n", "\n", "# Encoder", "\n", "encoder_params", "=", "srl_params", ".", "pop", "(", "\"encoder\"", ")", "\n", "encoder", "=", "Seq2SeqEncoder", ".", "from_params", "(", "encoder_params", ")", "\n", "self", ".", "_encoder", "=", "encoder", "\n", "\n", "# Secondary Encoder", "\n", "secondary_encoder_params", "=", "srl_params", ".", "pop", "(", "\"secondary_encoder\"", ")", "\n", "secondary_encoder_params", "[", "\n", "\"input_size\"", "\n", "]", "=", "self", ".", "_encoder", ".", "get_output_dim", "(", ")", "+", "srl_params", ".", "get", "(", "\"binary_feature_dim\"", ")", "\n", "secondary_encoder", "=", "Seq2SeqEncoder", ".", "from_params", "(", "secondary_encoder_params", ")", "\n", "self", ".", "_secondary_encoder", "=", "secondary_encoder", "\n", "\n", "# Tagger: SRL", "\n", "init_params", "=", "srl_params", ".", "pop", "(", "\"initializer\"", ",", "None", ")", "\n", "initializer", "=", "(", "\n", "InitializerApplicator", ".", "from_params", "(", "init_params", ")", "\n", "if", "init_params", "is", "not", "None", "\n", "else", "InitializerApplicator", "(", ")", "\n", ")", "\n", "\n", "tagger_srl", "=", "SrlCustom", "(", "\n", "vocab", "=", "vocab", ",", "\n", "text_field_embedder", "=", "self", ".", "_text_field_embedder", ",", "\n", "encoder", "=", "self", ".", "_encoder", ",", "\n", "secondary_encoder", "=", "self", ".", "_secondary_encoder", ",", "\n", "binary_feature_dim", "=", "srl_params", ".", "pop_int", "(", "\"binary_feature_dim\"", ",", "100", ")", ",", "\n", "label_smoothing", "=", "srl_params", ".", "pop_float", "(", "\"label_smoothing\"", ",", "0.1", ")", ",", "\n", "initializer", "=", "initializer", ",", "\n", ")", "\n", "self", ".", "_tagger_srl", "=", "tagger_srl", "\n", "\n", "", "logger", ".", "info", "(", "\"Multi-Task Learning Model has been instantiated.\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.models.layerSrl.LayerSrl.forward": [[117, 129], ["getattr", "getattr.forward"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.graph_util.graph_encoder_gat.GAT.forward"], ["", "@", "overrides", "\n", "def", "forward", "(", "\n", "self", ",", "\n", "tensor_batch", ",", "\n", "for_training", ":", "bool", "=", "False", ",", "\n", "task_name", ":", "str", "=", "\"srl\"", ",", "\n", "sample", ":", "bool", "=", "False", ",", "\n", ")", "->", "Dict", "[", "str", ",", "torch", ".", "Tensor", "]", ":", "\n", "# pylint: disable=arguments-differ", "\n", "        ", "tagger", "=", "getattr", "(", "self", ",", "\"_tagger_%s\"", "%", "task_name", ")", "\n", "tensor_batch", "[", "\"sample\"", "]", "=", "sample", "\n", "return", "tagger", ".", "forward", "(", "**", "tensor_batch", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.models.layerSrl.LayerSrl.decode": [[130, 134], ["getattr", "getattr.decode"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.coreference_resolution.coref.CoreferenceResolver.decode"], ["", "@", "overrides", "\n", "def", "decode", "(", "self", ",", "output_dict", ":", "Dict", "[", "str", ",", "torch", ".", "Tensor", "]", ",", "task_name", ":", "str", "=", "\"srl\"", ")", ":", "\n", "        ", "tagger", "=", "getattr", "(", "self", ",", "\"_tagger_%s\"", "%", "task_name", ")", "\n", "return", "tagger", ".", "decode", "(", "output_dict", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.models.layerSrl.LayerSrl.get_metrics": [[135, 141], ["getattr", "getattr.get_metrics"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.coreference_resolution.coref.CoreferenceResolver.get_metrics"], ["", "@", "overrides", "\n", "def", "get_metrics", "(", "\n", "self", ",", "task_name", ":", "str", "=", "\"srl\"", ",", "reset", ":", "bool", "=", "False", ",", "full", ":", "bool", "=", "False", "\n", ")", "->", "Dict", "[", "str", ",", "float", "]", ":", "\n", "        ", "task_tagger", "=", "getattr", "(", "self", ",", "\"_tagger_\"", "+", "task_name", ")", "\n", "return", "task_tagger", ".", "get_metrics", "(", "reset", "=", "reset", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.models.layerSrl.LayerSrl.from_params": [[142, 151], ["cls"], "methods", ["None"], ["", "@", "classmethod", "\n", "def", "from_params", "(", "\n", "cls", ",", "\n", "vocab", ":", "Vocabulary", ",", "\n", "params", ":", "Params", ",", "\n", "regularizer", ":", "RegularizerApplicator", ",", "\n", "**", "kwargs", "\n", ")", "->", "\"LayerSrl\"", ":", "\n", "        ", "return", "cls", "(", "vocab", "=", "vocab", ",", "params", "=", "params", ",", "regularizer", "=", "regularizer", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.models.layerCorefSrlBert.LayerCorefSrlBert.__init__": [[41, 133], ["allennlp.models.model.Model.__init__", "params.pop", "params.pop", "params.pop.pop", "transformers.AutoModel.from_pretrained", "params.pop", "allennlp.modules.text_field_embedders.BasicTextFieldEmbedder.from_params", "hmtl.models.SrlCustomBert", "params.pop.pop", "allennlp.modules.Seq2SeqEncoder.from_params", "params.pop.pop", "params.pop.pop.pop_bool", "params.pop.pop.pop", "allennlp.modules.span_extractors.EndpointSpanExtractor", "layerCorefSrlBert.LayerCorefSrlBert._text_field_embedder.get_output_dim", "allennlp.modules.span_extractors.SelfAttentiveSpanExtractor", "hmtl.models.CoreferenceCustom", "logger.info", "allennlp.nn.InitializerApplicator.from_params", "allennlp.nn.InitializerApplicator", "layerCorefSrlBert.LayerCorefSrlBert._encoder_coref.get_output_dim", "params.pop_float", "params.pop.pop.get", "params.pop.pop.get", "allennlp.modules.FeedForward.from_params", "allennlp.modules.FeedForward.from_params", "params.pop.pop.pop_int", "params.pop.pop.pop_int", "params.pop.pop.pop_float", "params.pop.pop.pop_int", "params.pop.pop.pop_float", "params.pop.pop.pop", "params.pop.pop.pop"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.graph_util.plot.GraphPlot.__init__", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.params.Params.pop", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.params.Params.pop", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.params.Params.pop", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.params.Params.pop", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.tokenizers.tokenizer.Tokenizer.from_params", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.params.Params.pop", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.tokenizers.tokenizer.Tokenizer.from_params", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.params.Params.pop", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.params.Params.pop_bool", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.params.Params.pop", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.seq2seq_decoders.auto_regressive_seq_decoder.AutoRegressiveSeqDecoder.get_output_dim", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.dataset_readers.multiprocess_dataset_reader.logger.info", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.tokenizers.tokenizer.Tokenizer.from_params", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.seq2seq_decoders.auto_regressive_seq_decoder.AutoRegressiveSeqDecoder.get_output_dim", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.params.Params.pop_float", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.params.Params.get", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.params.Params.get", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.tokenizers.tokenizer.Tokenizer.from_params", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.tokenizers.tokenizer.Tokenizer.from_params", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.params.Params.pop_int", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.params.Params.pop_int", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.params.Params.pop_float", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.params.Params.pop_int", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.params.Params.pop_float", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.params.Params.pop", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.params.Params.pop"], ["def", "__init__", "(", "\n", "self", ",", "\n", "vocab", ":", "Vocabulary", ",", "\n", "params", ":", "Params", ",", "\n", "regularizer", ":", "RegularizerApplicator", "=", "None", ",", "\n", ")", ":", "\n", "        ", "super", "(", "LayerCorefSrlBert", ",", "self", ")", ".", "__init__", "(", "vocab", "=", "vocab", ",", "regularizer", "=", "regularizer", ")", "\n", "\n", "srl_params", "=", "params", ".", "pop", "(", "\"srl\"", ")", "\n", "coref_params", "=", "params", ".", "pop", "(", "\"coref\"", ")", "\n", "\n", "# Base text Field Embedder", "\n", "_bert_model_name", "=", "srl_params", ".", "pop", "(", "\"bert_model\"", ")", "\n", "bert_model", "=", "AutoModel", ".", "from_pretrained", "(", "_bert_model_name", ")", "\n", "bert_model", ".", "_name", "=", "_bert_model_name", "\n", "text_field_embedder_params", "=", "params", ".", "pop", "(", "\"text_field_embedder\"", ")", "\n", "text_field_embedder_params", "[", "\"token_embedders\"", "]", "[", "\"tokens\"", "]", "[", "\n", "\"model_name\"", "\n", "]", "=", "bert_model", "\n", "text_field_embedder", "=", "BasicTextFieldEmbedder", ".", "from_params", "(", "\n", "vocab", "=", "vocab", ",", "params", "=", "text_field_embedder_params", "\n", ")", "\n", "self", ".", "_text_field_embedder", "=", "text_field_embedder", "\n", "\n", "############", "\n", "# SRL Stuffs", "\n", "############", "\n", "tagger_srl", "=", "SrlCustomBert", "(", "\n", "vocab", "=", "vocab", ",", "\n", "bert_model", "=", "bert_model", ",", "\n", "label_smoothing", "=", "params", ".", "pop_float", "(", "\"label_smoothing\"", ",", "0.1", ")", ",", "\n", ")", "\n", "self", ".", "_tagger_srl", "=", "tagger_srl", "\n", "\n", "##############", "\n", "# Coref Stuffs", "\n", "##############", "\n", "\n", "# Encoder", "\n", "encoder_coref_params", "=", "coref_params", ".", "pop", "(", "\"encoder\"", ")", "\n", "encoder_coref", "=", "Seq2SeqEncoder", ".", "from_params", "(", "encoder_coref_params", ")", "\n", "self", ".", "_encoder_coref", "=", "encoder_coref", "\n", "\n", "# Tagger: Coreference", "\n", "tagger_coref_params", "=", "coref_params", ".", "pop", "(", "\"tagger\"", ")", "\n", "eval_on_gold_mentions", "=", "tagger_coref_params", ".", "pop_bool", "(", "\n", "\"eval_on_gold_mentions\"", ",", "False", "\n", ")", "\n", "init_params", "=", "tagger_coref_params", ".", "pop", "(", "\"initializer\"", ",", "None", ")", "\n", "initializer", "=", "(", "\n", "InitializerApplicator", ".", "from_params", "(", "init_params", ")", "\n", "if", "init_params", "is", "not", "None", "\n", "else", "InitializerApplicator", "(", ")", "\n", ")", "\n", "\n", "# Span embedders", "\n", "self", ".", "_endpoint_span_extractor", "=", "EndpointSpanExtractor", "(", "\n", "self", ".", "_encoder_coref", ".", "get_output_dim", "(", ")", ",", "\n", "combination", "=", "\"x,y\"", ",", "\n", "num_width_embeddings", "=", "tagger_coref_params", ".", "get", "(", "\"max_span_width\"", ",", "10", ")", ",", "\n", "span_width_embedding_dim", "=", "tagger_coref_params", ".", "get", "(", "\"feature_size\"", ",", "20", ")", ",", "\n", "bucket_widths", "=", "False", ",", "\n", ")", "\n", "input_embedding_size", "=", "self", ".", "_text_field_embedder", ".", "get_output_dim", "(", ")", "\n", "self", ".", "_attentive_span_extractor", "=", "SelfAttentiveSpanExtractor", "(", "\n", "input_dim", "=", "input_embedding_size", "\n", ")", "\n", "\n", "tagger_coref", "=", "CoreferenceCustom", "(", "\n", "vocab", "=", "vocab", ",", "\n", "text_field_embedder", "=", "self", ".", "_text_field_embedder", ",", "\n", "context_layer", "=", "self", ".", "_encoder_coref", ",", "\n", "mention_feedforward", "=", "FeedForward", ".", "from_params", "(", "\n", "tagger_coref_params", ".", "pop", "(", "\"mention_feedforward\"", ")", "\n", ")", ",", "\n", "antecedent_feedforward", "=", "FeedForward", ".", "from_params", "(", "\n", "tagger_coref_params", ".", "pop", "(", "\"antecedent_feedforward\"", ")", "\n", ")", ",", "\n", "feature_size", "=", "tagger_coref_params", ".", "pop_int", "(", "\"feature_size\"", ")", ",", "\n", "max_span_width", "=", "tagger_coref_params", ".", "pop_int", "(", "\"max_span_width\"", ")", ",", "\n", "spans_per_word", "=", "tagger_coref_params", ".", "pop_float", "(", "\"spans_per_word\"", ")", ",", "\n", "max_antecedents", "=", "tagger_coref_params", ".", "pop_int", "(", "\"max_antecedents\"", ")", ",", "\n", "lexical_dropout", "=", "tagger_coref_params", ".", "pop_float", "(", "\"lexical_dropout\"", ",", "0.2", ")", ",", "\n", "initializer", "=", "initializer", ",", "\n", "eval_on_gold_mentions", "=", "eval_on_gold_mentions", ",", "\n", ")", "\n", "self", ".", "_tagger_coref", "=", "tagger_coref", "\n", "\n", "if", "eval_on_gold_mentions", ":", "\n", "            ", "self", ".", "_tagger_coref", ".", "_eval_on_gold_mentions", "=", "True", "\n", "\n", "", "logger", ".", "info", "(", "\"Multi-Task Learning Model has been instantiated.\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.models.layerCorefSrlBert.LayerCorefSrlBert.forward": [[134, 158], ["getattr", "getattr.forward"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.graph_util.graph_encoder_gat.GAT.forward"], ["", "@", "overrides", "\n", "def", "forward", "(", "\n", "self", ",", "\n", "tensor_batch", ",", "\n", "for_training", ":", "bool", "=", "False", ",", "\n", "task_name", ":", "str", "=", "\"srl\"", ",", "\n", "sample", ":", "bool", "=", "False", ",", "\n", ")", "->", "Dict", "[", "str", ",", "torch", ".", "Tensor", "]", ":", "\n", "# pylint: disable=arguments-differ", "\n", "        ", "\"\"\"\n        Special case for forward: for coreference, we can use gold mentions to predict the clusters\n        during evaluation (not during training).\n        \"\"\"", "\n", "\n", "tagger", "=", "getattr", "(", "self", ",", "\"_tagger_\"", "+", "task_name", ")", "\n", "\n", "if", "task_name", "==", "\"coref\"", "and", "tagger", ".", "_eval_on_gold_mentions", ":", "\n", "            ", "if", "for_training", ":", "\n", "                ", "tagger", ".", "_use_gold_mentions", "=", "False", "\n", "", "else", ":", "\n", "                ", "tagger", ".", "_use_gold_mentions", "=", "True", "\n", "\n", "", "", "tensor_batch", "[", "\"sample\"", "]", "=", "sample", "\n", "return", "tagger", ".", "forward", "(", "**", "tensor_batch", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.models.layerCorefSrlBert.LayerCorefSrlBert.decode": [[159, 163], ["getattr", "getattr.decode"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.coreference_resolution.coref.CoreferenceResolver.decode"], ["", "@", "overrides", "\n", "def", "decode", "(", "self", ",", "output_dict", ":", "Dict", "[", "str", ",", "torch", ".", "Tensor", "]", ",", "task_name", ":", "str", "=", "\"srl\"", ")", ":", "\n", "        ", "tagger", "=", "getattr", "(", "self", ",", "\"_tagger_\"", "+", "task_name", ")", "\n", "return", "tagger", ".", "decode", "(", "output_dict", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.models.layerCorefSrlBert.LayerCorefSrlBert.get_metrics": [[164, 174], ["getattr", "getattr.get_metrics", "getattr.get_metrics"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.coreference_resolution.coref.CoreferenceResolver.get_metrics", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.coreference_resolution.coref.CoreferenceResolver.get_metrics"], ["", "@", "overrides", "\n", "def", "get_metrics", "(", "\n", "self", ",", "task_name", ":", "str", "=", "\"srl\"", ",", "reset", ":", "bool", "=", "False", ",", "full", ":", "bool", "=", "False", "\n", ")", "->", "Dict", "[", "str", ",", "float", "]", ":", "\n", "\n", "        ", "task_tagger", "=", "getattr", "(", "self", ",", "\"_tagger_\"", "+", "task_name", ")", "\n", "if", "full", "and", "task_name", "==", "\"coref\"", ":", "\n", "            ", "return", "task_tagger", ".", "get_metrics", "(", "reset", "=", "reset", ",", "full", "=", "full", ")", "\n", "", "else", ":", "\n", "            ", "return", "task_tagger", ".", "get_metrics", "(", "reset", "=", "reset", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.models.layerCorefSrlBert.LayerCorefSrlBert.from_params": [[175, 184], ["cls"], "methods", ["None"], ["", "", "@", "classmethod", "\n", "def", "from_params", "(", "\n", "cls", ",", "\n", "vocab", ":", "Vocabulary", ",", "\n", "params", ":", "Params", ",", "\n", "regularizer", ":", "RegularizerApplicator", ",", "\n", "**", "kwargs", "\n", ")", "->", "\"LayerCorefSrlBert\"", ":", "\n", "        ", "return", "cls", "(", "vocab", "=", "vocab", ",", "params", "=", "params", ",", "regularizer", "=", "regularizer", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.models.layerCoref.LayerCoref.__init__": [[47, 120], ["allennlp.models.model.Model.__init__", "params.pop", "allennlp.modules.text_field_embedders.BasicTextFieldEmbedder.from_params", "params.pop", "params.pop.pop", "allennlp.modules.Seq2SeqEncoder.from_params", "params.pop.pop", "params.pop.pop.pop_bool", "params.pop.pop.pop", "allennlp.modules.span_extractors.EndpointSpanExtractor", "layerCoref.LayerCoref._text_field_embedder.get_output_dim", "allennlp.modules.span_extractors.SelfAttentiveSpanExtractor", "hmtl.models.CoreferenceCustom", "logger.info", "allennlp.nn.InitializerApplicator.from_params", "allennlp.nn.InitializerApplicator", "layerCoref.LayerCoref._encoder_coref.get_output_dim", "params.pop.pop.get", "params.pop.pop.get", "allennlp.modules.FeedForward.from_params", "allennlp.modules.FeedForward.from_params", "params.pop.pop.pop_int", "params.pop.pop.pop_int", "params.pop.pop.pop_float", "params.pop.pop.pop_int", "params.pop.pop.pop_float", "params.pop.pop.pop", "params.pop.pop.pop"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.graph_util.plot.GraphPlot.__init__", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.params.Params.pop", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.tokenizers.tokenizer.Tokenizer.from_params", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.params.Params.pop", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.params.Params.pop", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.tokenizers.tokenizer.Tokenizer.from_params", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.params.Params.pop", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.params.Params.pop_bool", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.params.Params.pop", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.seq2seq_decoders.auto_regressive_seq_decoder.AutoRegressiveSeqDecoder.get_output_dim", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.dataset_readers.multiprocess_dataset_reader.logger.info", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.tokenizers.tokenizer.Tokenizer.from_params", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.seq2seq_decoders.auto_regressive_seq_decoder.AutoRegressiveSeqDecoder.get_output_dim", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.params.Params.get", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.params.Params.get", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.tokenizers.tokenizer.Tokenizer.from_params", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.tokenizers.tokenizer.Tokenizer.from_params", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.params.Params.pop_int", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.params.Params.pop_int", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.params.Params.pop_float", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.params.Params.pop_int", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.params.Params.pop_float", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.params.Params.pop", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.params.Params.pop"], ["def", "__init__", "(", "\n", "self", ",", "\n", "vocab", ":", "Vocabulary", ",", "\n", "params", ":", "Params", ",", "\n", "regularizer", ":", "RegularizerApplicator", "=", "None", ",", "\n", ")", ":", "\n", "        ", "super", "(", "LayerCoref", ",", "self", ")", ".", "__init__", "(", "vocab", "=", "vocab", ",", "regularizer", "=", "regularizer", ")", "\n", "\n", "# Base text Field Embedder", "\n", "text_field_embedder_params", "=", "params", ".", "pop", "(", "\"text_field_embedder\"", ")", "\n", "text_field_embedder", "=", "BasicTextFieldEmbedder", ".", "from_params", "(", "\n", "vocab", "=", "vocab", ",", "params", "=", "text_field_embedder_params", "\n", ")", "\n", "self", ".", "_text_field_embedder", "=", "text_field_embedder", "\n", "\n", "##############", "\n", "# Coref Stuffs", "\n", "##############", "\n", "coref_params", "=", "params", ".", "pop", "(", "\"coref\"", ")", "\n", "\n", "# Encoder", "\n", "encoder_coref_params", "=", "coref_params", ".", "pop", "(", "\"encoder\"", ")", "\n", "encoder_coref", "=", "Seq2SeqEncoder", ".", "from_params", "(", "encoder_coref_params", ")", "\n", "self", ".", "_encoder_coref", "=", "encoder_coref", "\n", "\n", "# Tagger: Coreference", "\n", "tagger_coref_params", "=", "coref_params", ".", "pop", "(", "\"tagger\"", ")", "\n", "eval_on_gold_mentions", "=", "tagger_coref_params", ".", "pop_bool", "(", "\n", "\"eval_on_gold_mentions\"", ",", "False", "\n", ")", "\n", "init_params", "=", "tagger_coref_params", ".", "pop", "(", "\"initializer\"", ",", "None", ")", "\n", "initializer", "=", "(", "\n", "InitializerApplicator", ".", "from_params", "(", "params", "=", "init_params", ")", "\n", "if", "init_params", "is", "not", "None", "\n", "else", "InitializerApplicator", "(", ")", "\n", ")", "\n", "\n", "# Span embedders", "\n", "self", ".", "_endpoint_span_extractor", "=", "EndpointSpanExtractor", "(", "\n", "self", ".", "_encoder_coref", ".", "get_output_dim", "(", ")", ",", "\n", "combination", "=", "\"x,y\"", ",", "\n", "num_width_embeddings", "=", "tagger_coref_params", ".", "get", "(", "\"max_span_width\"", ",", "10", ")", ",", "\n", "span_width_embedding_dim", "=", "tagger_coref_params", ".", "get", "(", "\"feature_size\"", ",", "20", ")", ",", "\n", "bucket_widths", "=", "False", ",", "\n", ")", "\n", "input_embedding_size", "=", "self", ".", "_text_field_embedder", ".", "get_output_dim", "(", ")", "\n", "self", ".", "_attentive_span_extractor", "=", "SelfAttentiveSpanExtractor", "(", "\n", "input_dim", "=", "input_embedding_size", "\n", ")", "\n", "\n", "tagger_coref", "=", "CoreferenceCustom", "(", "\n", "vocab", "=", "vocab", ",", "\n", "text_field_embedder", "=", "self", ".", "_text_field_embedder", ",", "\n", "context_layer", "=", "self", ".", "_encoder_coref", ",", "\n", "mention_feedforward", "=", "FeedForward", ".", "from_params", "(", "\n", "tagger_coref_params", ".", "pop", "(", "\"mention_feedforward\"", ")", "\n", ")", ",", "\n", "antecedent_feedforward", "=", "FeedForward", ".", "from_params", "(", "\n", "tagger_coref_params", ".", "pop", "(", "\"antecedent_feedforward\"", ")", "\n", ")", ",", "\n", "feature_size", "=", "tagger_coref_params", ".", "pop_int", "(", "\"feature_size\"", ")", ",", "\n", "max_span_width", "=", "tagger_coref_params", ".", "pop_int", "(", "\"max_span_width\"", ")", ",", "\n", "spans_per_word", "=", "tagger_coref_params", ".", "pop_float", "(", "\"spans_per_word\"", ")", ",", "\n", "max_antecedents", "=", "tagger_coref_params", ".", "pop_int", "(", "\"max_antecedents\"", ")", ",", "\n", "lexical_dropout", "=", "tagger_coref_params", ".", "pop_float", "(", "\"lexical_dropout\"", ",", "0.2", ")", ",", "\n", "initializer", "=", "initializer", ",", "\n", "eval_on_gold_mentions", "=", "eval_on_gold_mentions", ",", "\n", ")", "\n", "self", ".", "_tagger_coref", "=", "tagger_coref", "\n", "if", "eval_on_gold_mentions", ":", "\n", "            ", "self", ".", "_tagger_coref", ".", "_eval_on_gold_mentions", "=", "True", "\n", "\n", "", "logger", ".", "info", "(", "\"Multi-Task Learning Model has been instantiated.\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.models.layerCoref.LayerCoref.forward": [[121, 140], ["getattr", "getattr.forward"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.graph_util.graph_encoder_gat.GAT.forward"], ["", "@", "overrides", "\n", "def", "forward", "(", "\n", "self", ",", "\n", "tensor_batch", ",", "\n", "for_training", ":", "bool", "=", "False", ",", "\n", "task_name", ":", "str", "=", "\"coref\"", ",", "\n", "sample", ":", "bool", "=", "False", ",", "\n", ")", "->", "Dict", "[", "str", ",", "torch", ".", "Tensor", "]", ":", "\n", "# pylint: disable=arguments-differ", "\n", "\n", "        ", "tagger", "=", "getattr", "(", "self", ",", "\"_tagger_%s\"", "%", "task_name", ")", "\n", "\n", "if", "task_name", "==", "\"coref\"", "and", "tagger", ".", "_eval_on_gold_mentions", ":", "\n", "            ", "if", "for_training", ":", "\n", "                ", "tagger", ".", "_use_gold_mentions", "=", "False", "\n", "", "else", ":", "\n", "                ", "tagger", ".", "_use_gold_mentions", "=", "True", "\n", "", "", "tensor_batch", "[", "\"sample\"", "]", "=", "sample", "\n", "return", "tagger", ".", "forward", "(", "**", "tensor_batch", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.models.layerCoref.LayerCoref.decode": [[141, 145], ["getattr", "getattr.decode"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.coreference_resolution.coref.CoreferenceResolver.decode"], ["", "@", "overrides", "\n", "def", "decode", "(", "self", ",", "output_dict", ":", "Dict", "[", "str", ",", "torch", ".", "Tensor", "]", ",", "task_name", ":", "str", "=", "\"coref\"", ")", ":", "\n", "        ", "tagger", "=", "getattr", "(", "self", ",", "\"_tagger_%s\"", "%", "task_name", ")", "\n", "return", "tagger", ".", "decode", "(", "output_dict", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.models.layerCoref.LayerCoref.get_metrics": [[146, 156], ["getattr", "getattr.get_metrics", "getattr.get_metrics"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.coreference_resolution.coref.CoreferenceResolver.get_metrics", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.coreference_resolution.coref.CoreferenceResolver.get_metrics"], ["", "@", "overrides", "\n", "def", "get_metrics", "(", "\n", "self", ",", "task_name", ":", "str", ",", "reset", ":", "bool", "=", "False", ",", "full", ":", "bool", "=", "False", "\n", ")", "->", "Dict", "[", "str", ",", "float", "]", ":", "\n", "\n", "        ", "task_tagger", "=", "getattr", "(", "self", ",", "\"_tagger_\"", "+", "task_name", ")", "\n", "if", "full", "and", "task_name", "==", "\"coref\"", ":", "\n", "            ", "return", "task_tagger", ".", "get_metrics", "(", "reset", "=", "reset", ",", "full", "=", "full", ")", "\n", "", "else", ":", "\n", "            ", "return", "task_tagger", ".", "get_metrics", "(", "reset", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.models.layerCoref.LayerCoref.from_params": [[157, 166], ["cls"], "methods", ["None"], ["", "", "@", "classmethod", "\n", "def", "from_params", "(", "\n", "cls", ",", "\n", "vocab", ":", "Vocabulary", ",", "\n", "params", ":", "Params", ",", "\n", "regularizer", ":", "RegularizerApplicator", ",", "\n", "**", "kwargs", "\n", ")", "->", "\"LayerCoref\"", ":", "\n", "        ", "return", "cls", "(", "vocab", "=", "vocab", ",", "params", "=", "params", ",", "regularizer", "=", "regularizer", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.models.coref_custom.CoreferenceCustom.__init__": [[41, 77], ["allennlp.nn.InitializerApplicator", "hmtl.models.coref_base.CoreferenceResolver.__init__", "hmtl.training.metrics.ConllCorefFullScores"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.graph_util.plot.GraphPlot.__init__"], ["def", "__init__", "(", "\n", "self", ",", "\n", "vocab", ":", "Vocabulary", ",", "\n", "text_field_embedder", ":", "TextFieldEmbedder", ",", "\n", "context_layer", ":", "Seq2SeqEncoder", ",", "\n", "mention_feedforward", ":", "FeedForward", ",", "\n", "antecedent_feedforward", ":", "FeedForward", ",", "\n", "feature_size", ":", "int", ",", "\n", "max_span_width", ":", "int", ",", "\n", "spans_per_word", ":", "float", ",", "\n", "max_antecedents", ":", "int", ",", "\n", "lexical_dropout", ":", "float", "=", "0.2", ",", "\n", "initializer", ":", "InitializerApplicator", "=", "InitializerApplicator", "(", ")", ",", "\n", "eval_on_gold_mentions", ":", "bool", "=", "False", ",", "\n", ")", "->", "None", ":", "\n", "        ", "super", "(", "CoreferenceCustom", ",", "self", ")", ".", "__init__", "(", "\n", "vocab", "=", "vocab", ",", "\n", "text_field_embedder", "=", "text_field_embedder", ",", "\n", "context_layer", "=", "context_layer", ",", "\n", "mention_feedforward", "=", "mention_feedforward", ",", "\n", "antecedent_feedforward", "=", "antecedent_feedforward", ",", "\n", "feature_size", "=", "feature_size", ",", "\n", "max_span_width", "=", "max_span_width", ",", "\n", "spans_per_word", "=", "spans_per_word", ",", "\n", "max_antecedents", "=", "max_antecedents", ",", "\n", "lexical_dropout", "=", "lexical_dropout", ",", "\n", "initializer", "=", "initializer", ",", "\n", ")", "\n", "\n", "self", ".", "_conll_coref_scores", "=", "ConllCorefFullScores", "(", ")", "\n", "self", ".", "_eval_on_gold_mentions", "=", "eval_on_gold_mentions", "\n", "\n", "if", "self", ".", "_eval_on_gold_mentions", ":", "\n", "            ", "self", ".", "_use_gold_mentions", "=", "False", "\n", "", "else", ":", "\n", "            ", "self", ".", "_use_gold_mentions", "=", "None", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.models.coref_custom.CoreferenceCustom.get_metrics": [[78, 85], ["coref_custom.CoreferenceCustom._mention_recall.get_metric", "coref_custom.CoreferenceCustom._conll_coref_scores.get_metric"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.metrics.sequence_accuracy.SequenceAccuracy.get_metric", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.metrics.sequence_accuracy.SequenceAccuracy.get_metric"], ["", "", "@", "overrides", "\n", "def", "get_metrics", "(", "self", ",", "reset", ":", "bool", "=", "False", ",", "full", ":", "bool", "=", "False", ")", ":", "\n", "        ", "mention_recall", "=", "self", ".", "_mention_recall", ".", "get_metric", "(", "reset", "=", "reset", ")", "\n", "metrics", "=", "self", ".", "_conll_coref_scores", ".", "get_metric", "(", "reset", "=", "reset", ",", "full", "=", "full", ")", "\n", "metrics", "[", "\"mention_recall\"", "]", "=", "mention_recall", "\n", "\n", "return", "metrics", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.models.coref_custom.CoreferenceCustom.forward": [[86, 276], ["coref_custom.CoreferenceCustom._lexical_dropout", "coref_custom.CoreferenceCustom.size", "allennlp.nn.util.get_text_field_mask().float", "torch.relu().long", "torch.relu().long", "coref_custom.CoreferenceCustom._context_layer", "coref_custom.CoreferenceCustom._endpoint_span_extractor", "coref_custom.CoreferenceCustom._attentive_span_extractor", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "int", "coref_custom.CoreferenceCustom._mention_pruner", "top_span_mask.unsqueeze.unsqueeze.unsqueeze", "allennlp.nn.util.flatten_and_batch_shift_indices", "allennlp.nn.util.batched_index_select", "min", "coref_custom.CoreferenceCustom._generate_valid_antecedents", "allennlp.nn.util.flattened_index_select", "allennlp.nn.util.flattened_index_select().squeeze", "coref_custom.CoreferenceCustom._compute_span_pair_embeddings", "coref_custom.CoreferenceCustom._compute_coreference_scores", "coref_custom.CoreferenceCustom._text_field_embedder", "nan_mask.sum", "logger.info", "torch.stack().unsqueeze().unsqueeze", "torch.stack().unsqueeze().unsqueeze", "torch.stack().unsqueeze().unsqueeze", "torch.stack().unsqueeze().unsqueeze", "span_mask.float.float.sum().item", "span_mask.float.float.float", "torch.relu().long.size", "math.floor", "allennlp.nn.util.get_device_of", "torch.distributions.Categorical", "torch.distributions.Categorical", "torch.distributions.Categorical.sample", "torch.distributions.Categorical.sample", "torch.distributions.Categorical.log_prob().sum", "torch.distributions.Categorical.log_prob().sum", "coref_custom.CoreferenceCustom.max", "allennlp.nn.util.batched_index_select", "allennlp.nn.util.flattened_index_select().squeeze", "valid_antecedent_log_mask.long", "coref_custom.CoreferenceCustom._compute_antecedent_gold_labels", "allennlp.nn.util.masked_log_softmax", "coref_custom.CoreferenceCustom._mention_recall", "coref_custom.CoreferenceCustom._conll_coref_scores", "allennlp.nn.util.get_text_field_mask", "torch.device", "torch.device", "torch.device", "torch.device", "torch.device", "torch.device", "torch.device", "torch.device", "torch.as_tensor", "torch.as_tensor", "torch.as_tensor", "torch.as_tensor", "torch.relu().long.unsqueeze", "torch.relu", "torch.relu", "allennlp.nn.util.flattened_index_select", "torch.softmax", "torch.softmax", "span_labels.unsqueeze", "coref_custom.CoreferenceCustom.log", "allennlp.nn.util.logsumexp().sum", "x.get", "x.get", "nan_mask.sum", "torch.stack().unsqueeze", "torch.stack().unsqueeze", "torch.stack().unsqueeze", "torch.stack().unsqueeze", "span_mask.float.float.sum", "torch.relu().long.float", "torch.distributions.Categorical.log_prob", "torch.distributions.Categorical.log_prob", "allennlp.nn.util.flattened_index_select", "torch.distributions.Categorical.sample.clone", "allennlp.nn.util.logsumexp", "torch.stack", "torch.stack", "torch.stack", "torch.stack"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.nn.util.flatten_and_batch_shift_indices", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.nn.util.batched_index_select", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.coreference_resolution.coref.CoreferenceResolver._generate_valid_antecedents", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.nn.util.flattened_index_select", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.coreference_resolution.coref.CoreferenceResolver._compute_span_pair_embeddings", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.coreference_resolution.coref.CoreferenceResolver._compute_coreference_scores", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.dataset_readers.multiprocess_dataset_reader.logger.info", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.nn.util.get_device_of", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.nn.util.batched_index_select", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.coreference_resolution.coref.CoreferenceResolver._compute_antecedent_gold_labels", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.nn.util.masked_log_softmax", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.nn.util.get_text_field_mask", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.fields.text_field.TextField.as_tensor", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.fields.text_field.TextField.as_tensor", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.fields.text_field.TextField.as_tensor", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.fields.text_field.TextField.as_tensor", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.nn.util.flattened_index_select", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.params.Params.get", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.params.Params.get", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.nn.util.flattened_index_select", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.nn.util.clone", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.nn.util.logsumexp"], ["", "@", "overrides", "\n", "def", "forward", "(", "\n", "self", ",", "# type: ignore", "\n", "text", ":", "TextFieldTensors", ",", "\n", "spans", ":", "torch", ".", "IntTensor", ",", "\n", "span_labels", ":", "torch", ".", "IntTensor", "=", "None", ",", "\n", "metadata", ":", "List", "[", "Dict", "[", "str", ",", "Any", "]", "]", "=", "None", ",", "\n", "sample", ":", "bool", "=", "False", ",", "\n", ")", "->", "Dict", "[", "str", ",", "torch", ".", "Tensor", "]", ":", "\n", "# pylint: disable=arguments-differ", "\n", "\n", "# Shape: (batch_size, document_length, embedding_size)", "\n", "        ", "text_embeddings", "=", "self", ".", "_lexical_dropout", "(", "self", ".", "_text_field_embedder", "(", "text", ")", ")", "\n", "\n", "# This was a bug in the old allennlp. Fixed in the custom version.", "\n", "nan_mask", "=", "text_embeddings", "!=", "text_embeddings", "\n", "found_nan", "=", "False", "\n", "if", "nan_mask", ".", "sum", "(", ")", ">", "0", ":", "\n", "            ", "found_nan", "=", "True", "\n", "logger", ".", "info", "(", "\"coref: {} values in bert embeddings have NaN.\"", ".", "format", "(", "nan_mask", ".", "sum", "(", ")", ")", ")", "\n", "text_embeddings", "[", "nan_mask", "]", "=", "0", "# is this a good value?", "\n", "\n", "", "document_length", "=", "text_embeddings", ".", "size", "(", "1", ")", "\n", "\n", "# Shape: (batch_size, document_length)", "\n", "text_mask", "=", "util", ".", "get_text_field_mask", "(", "text", ")", ".", "float", "(", ")", "\n", "\n", "# Shape: (batch_size, num_spans)", "\n", "if", "self", ".", "_use_gold_mentions", ":", "\n", "            ", "if", "text_embeddings", ".", "is_cuda", ":", "\n", "                ", "device", "=", "torch", ".", "device", "(", "\"cuda\"", ")", "\n", "", "else", ":", "\n", "                ", "device", "=", "torch", ".", "device", "(", "\"cpu\"", ")", "\n", "\n", "", "s", "=", "[", "\n", "torch", ".", "as_tensor", "(", "pair", ",", "dtype", "=", "torch", ".", "long", ",", "device", "=", "device", ")", "\n", "for", "cluster", "in", "metadata", "[", "0", "]", "[", "\"clusters\"", "]", "\n", "for", "pair", "in", "cluster", "\n", "]", "\n", "gm", "=", "torch", ".", "stack", "(", "s", ",", "dim", "=", "0", ")", ".", "unsqueeze", "(", "0", ")", ".", "unsqueeze", "(", "1", ")", "\n", "\n", "span_mask", "=", "spans", ".", "unsqueeze", "(", "2", ")", "-", "gm", "\n", "span_mask", "=", "(", "span_mask", "[", ":", ",", ":", ",", ":", ",", "0", "]", "==", "0", ")", "+", "(", "span_mask", "[", ":", ",", ":", ",", ":", ",", "1", "]", "==", "0", ")", "\n", "span_mask", ",", "_", "=", "(", "span_mask", "==", "2", ")", ".", "max", "(", "-", "1", ")", "\n", "num_spans", "=", "span_mask", ".", "sum", "(", ")", ".", "item", "(", ")", "\n", "span_mask", "=", "span_mask", ".", "float", "(", ")", "\n", "", "else", ":", "\n", "            ", "span_mask", "=", "(", "spans", "[", ":", ",", ":", ",", "0", "]", ">=", "0", ")", ".", "squeeze", "(", "-", "1", ")", ".", "float", "(", ")", "\n", "num_spans", "=", "spans", ".", "size", "(", "1", ")", "\n", "# Shape: (batch_size, num_spans, 2)", "\n", "", "spans", "=", "F", ".", "relu", "(", "spans", ".", "float", "(", ")", ")", ".", "long", "(", ")", "\n", "\n", "# Shape: (batch_size, document_length, encoding_dim)", "\n", "contextualized_embeddings", "=", "self", ".", "_context_layer", "(", "text_embeddings", ",", "text_mask", ")", "\n", "# Shape: (batch_size, num_spans, 2 * encoding_dim + feature_size)", "\n", "endpoint_span_embeddings", "=", "self", ".", "_endpoint_span_extractor", "(", "\n", "contextualized_embeddings", ",", "spans", "\n", ")", "\n", "\n", "# Shape: (batch_size, num_spans, emebedding_size)", "\n", "attended_span_embeddings", "=", "self", ".", "_attentive_span_extractor", "(", "\n", "text_embeddings", ",", "spans", "\n", ")", "# text_embeddings", "\n", "\n", "# Shape: (batch_size, num_spans, emebedding_size + 2 * encoding_dim + feature_size)", "\n", "span_embeddings", "=", "torch", ".", "cat", "(", "\n", "[", "endpoint_span_embeddings", ",", "attended_span_embeddings", "]", ",", "-", "1", "\n", ")", "\n", "\n", "# Prune based on mention scores.", "\n", "num_spans_to_keep", "=", "int", "(", "math", ".", "floor", "(", "self", ".", "_spans_per_word", "*", "document_length", ")", ")", "\n", "\n", "(", "\n", "top_span_embeddings", ",", "\n", "top_span_mask", ",", "\n", "top_span_indices", ",", "\n", "top_span_mention_scores", ",", "\n", ")", "=", "self", ".", "_mention_pruner", "(", "span_embeddings", ",", "span_mask", ",", "num_spans_to_keep", ")", "\n", "top_span_mask", "=", "top_span_mask", ".", "unsqueeze", "(", "-", "1", ")", "\n", "# Shape: (batch_size * num_spans_to_keep)", "\n", "flat_top_span_indices", "=", "util", ".", "flatten_and_batch_shift_indices", "(", "\n", "top_span_indices", ",", "num_spans", "\n", ")", "\n", "\n", "# Compute final predictions for which spans to consider as mentions.", "\n", "# Shape: (batch_size, num_spans_to_keep, 2)", "\n", "top_spans", "=", "util", ".", "batched_index_select", "(", "\n", "spans", ",", "top_span_indices", ",", "flat_top_span_indices", "\n", ")", "\n", "\n", "# Compute indices for antecedent spans to consider.", "\n", "max_antecedents", "=", "min", "(", "self", ".", "_max_antecedents", ",", "num_spans_to_keep", ")", "\n", "\n", "# Shapes:", "\n", "# (num_spans_to_keep, max_antecedents),", "\n", "# (1, max_antecedents),", "\n", "# (1, num_spans_to_keep, max_antecedents)", "\n", "(", "\n", "valid_antecedent_indices", ",", "\n", "valid_antecedent_offsets", ",", "\n", "valid_antecedent_log_mask", ",", "\n", ")", "=", "self", ".", "_generate_valid_antecedents", "(", "\n", "num_spans_to_keep", ",", "max_antecedents", ",", "util", ".", "get_device_of", "(", "text_mask", ")", "\n", ")", "\n", "# Select tensors relating to the antecedent spans.", "\n", "# Shape: (batch_size, num_spans_to_keep, max_antecedents, embedding_size)", "\n", "candidate_antecedent_embeddings", "=", "util", ".", "flattened_index_select", "(", "\n", "top_span_embeddings", ",", "valid_antecedent_indices", "\n", ")", "\n", "\n", "# Shape: (batch_size, num_spans_to_keep, max_antecedents)", "\n", "candidate_antecedent_mention_scores", "=", "util", ".", "flattened_index_select", "(", "\n", "top_span_mention_scores", ",", "valid_antecedent_indices", "\n", ")", ".", "squeeze", "(", "-", "1", ")", "\n", "# Compute antecedent scores.", "\n", "# Shape: (batch_size, num_spans_to_keep, max_antecedents, embedding_size)", "\n", "span_pair_embeddings", "=", "self", ".", "_compute_span_pair_embeddings", "(", "\n", "top_span_embeddings", ",", "\n", "candidate_antecedent_embeddings", ",", "\n", "valid_antecedent_offsets", ",", "\n", ")", "\n", "# Shape: (batch_size, num_spans_to_keep, 1 + max_antecedents)", "\n", "coreference_scores", "=", "self", ".", "_compute_coreference_scores", "(", "\n", "span_pair_embeddings", ",", "\n", "top_span_mention_scores", ",", "\n", "candidate_antecedent_mention_scores", ",", "\n", "valid_antecedent_log_mask", ",", "\n", ")", "\n", "\n", "# Shape: (batch_size, num_spans_to_keep)", "\n", "if", "sample", ":", "\n", "            ", "probs", "=", "Categorical", "(", "F", ".", "softmax", "(", "coreference_scores", ",", "dim", "=", "2", ")", ")", "\n", "predicted_antecedents", "=", "probs", ".", "sample", "(", ")", "\n", "log_likelihood", "=", "probs", ".", "log_prob", "(", "\n", "predicted_antecedents", ".", "clone", "(", ")", "\n", ")", ".", "sum", "(", ")", "# to avoid in-place operations", "\n", "", "else", ":", "\n", "            ", "_", ",", "predicted_antecedents", "=", "coreference_scores", ".", "max", "(", "2", ")", "\n", "", "predicted_antecedents", "-=", "1", "\n", "\n", "output_dict", "=", "{", "\n", "\"top_spans\"", ":", "top_spans", ",", "\n", "\"antecedent_indices\"", ":", "valid_antecedent_indices", ",", "\n", "\"predicted_antecedents\"", ":", "predicted_antecedents", ",", "\n", "\"found_nan\"", ":", "found_nan", "\n", "}", "\n", "if", "sample", ":", "\n", "            ", "output_dict", "[", "\"log_likelihood\"", "]", "=", "log_likelihood", "\n", "\n", "", "if", "span_labels", "is", "not", "None", ":", "\n", "# Find the gold labels for the spans which we kept.", "\n", "            ", "pruned_gold_labels", "=", "util", ".", "batched_index_select", "(", "\n", "span_labels", ".", "unsqueeze", "(", "-", "1", ")", ",", "top_span_indices", ",", "flat_top_span_indices", "\n", ")", "\n", "\n", "antecedent_labels", "=", "util", ".", "flattened_index_select", "(", "\n", "pruned_gold_labels", ",", "valid_antecedent_indices", "\n", ")", ".", "squeeze", "(", "-", "1", ")", "\n", "antecedent_labels", "+=", "valid_antecedent_log_mask", ".", "long", "(", ")", "\n", "\n", "# Compute labels.", "\n", "# Shape: (batch_size, num_spans_to_keep, max_antecedents + 1)", "\n", "gold_antecedent_labels", "=", "self", ".", "_compute_antecedent_gold_labels", "(", "\n", "pruned_gold_labels", ",", "antecedent_labels", "\n", ")", "\n", "coreference_log_probs", "=", "util", ".", "masked_log_softmax", "(", "\n", "coreference_scores", ",", "top_span_mask", "\n", ")", "\n", "correct_antecedent_log_probs", "=", "(", "\n", "coreference_log_probs", "+", "gold_antecedent_labels", ".", "log", "(", ")", "\n", ")", "\n", "negative_marginal_log_likelihood", "=", "-", "util", ".", "logsumexp", "(", "\n", "correct_antecedent_log_probs", "\n", ")", ".", "sum", "(", ")", "\n", "\n", "self", ".", "_mention_recall", "(", "top_spans", ",", "metadata", ")", "\n", "self", ".", "_conll_coref_scores", "(", "\n", "top_spans", ",", "valid_antecedent_indices", ",", "predicted_antecedents", ",", "metadata", "\n", ")", "\n", "output_dict", "[", "\"loss\"", "]", "=", "negative_marginal_log_likelihood", "\n", "", "else", ":", "\n", "            ", "output_dict", "[", "\"all_spans\"", "]", "=", "spans", "\n", "output_dict", "[", "\"endpoint_span_embeddings\"", "]", "=", "endpoint_span_embeddings", "\n", "output_dict", "[", "\"attended_span_embeddings\"", "]", "=", "attended_span_embeddings", "\n", "\n", "", "if", "metadata", "is", "not", "None", ":", "\n", "            ", "output_dict", "[", "\"document\"", "]", "=", "[", "x", "[", "\"original_text\"", "]", "for", "x", "in", "metadata", "]", "\n", "output_dict", "[", "\"doc_key\"", "]", "=", "[", "x", ".", "get", "(", "\"doc_key\"", ",", "None", ")", "for", "x", "in", "metadata", "]", "\n", "output_dict", "[", "\"gold_clusters\"", "]", "=", "[", "x", ".", "get", "(", "\"clusters\"", ",", "None", ")", "for", "x", "in", "metadata", "]", "\n", "", "return", "output_dict", "\n", "", "", ""]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.models.biaffine_dependency_parser_multilang.BiaffineDependencyParserMultiLang.__init__": [[76, 113], ["allennlp.nn.InitializerApplicator", "allennlp.models.biaffine_dependency_parser.BiaffineDependencyParser.__init__", "collections.defaultdict"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.graph_util.plot.GraphPlot.__init__"], ["def", "__init__", "(", "\n", "self", ",", "\n", "vocab", ":", "Vocabulary", ",", "\n", "text_field_embedder", ":", "TextFieldEmbedder", ",", "\n", "encoder", ":", "Seq2SeqEncoder", ",", "\n", "tag_representation_dim", ":", "int", ",", "\n", "arc_representation_dim", ":", "int", ",", "\n", "tag_feedforward", ":", "FeedForward", "=", "None", ",", "\n", "arc_feedforward", ":", "FeedForward", "=", "None", ",", "\n", "pos_tag_embedding", ":", "Embedding", "=", "None", ",", "\n", "use_mst_decoding_for_validation", ":", "bool", "=", "True", ",", "\n", "langs_for_early_stop", ":", "List", "[", "str", "]", "=", "None", ",", "\n", "dropout", ":", "float", "=", "0.0", ",", "\n", "input_dropout", ":", "float", "=", "0.0", ",", "\n", "initializer", ":", "InitializerApplicator", "=", "InitializerApplicator", "(", ")", ",", "\n", "**", "kwargs", ",", "\n", ")", "->", "None", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "\n", "vocab", ",", "\n", "text_field_embedder", ",", "\n", "encoder", ",", "\n", "tag_representation_dim", ",", "\n", "arc_representation_dim", ",", "\n", "tag_feedforward", ",", "\n", "arc_feedforward", ",", "\n", "pos_tag_embedding", ",", "\n", "use_mst_decoding_for_validation", ",", "\n", "dropout", ",", "\n", "input_dropout", ",", "\n", "initializer", ",", "\n", "**", "kwargs", ",", "\n", ")", "\n", "\n", "self", ".", "_langs_for_early_stop", "=", "langs_for_early_stop", "or", "[", "]", "\n", "\n", "self", ".", "_lang_attachment_scores", ":", "Dict", "[", "str", ",", "AttachmentScores", "]", "=", "defaultdict", "(", "\n", "AttachmentScores", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.models.biaffine_dependency_parser_multilang.BiaffineDependencyParserMultiLang.forward": [[115, 186], ["biaffine_dependency_parser_multilang.BiaffineDependencyParserMultiLang.text_field_embedder", "allennlp.nn.util.get_text_field_mask", "biaffine_dependency_parser_multilang.BiaffineDependencyParserMultiLang._parse", "allennlp.common.checks.ConfigurationError", "biaffine_dependency_parser_multilang.BiaffineDependencyParserMultiLang._pos_tag_embedding", "torch.cat", "biaffine_dependency_parser_multilang.BiaffineDependencyParserMultiLang._get_mask_for_eval", "allennlp.common.checks.ConfigurationError", "allennlp.common.checks.ConfigurationError"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.nn.util.get_text_field_mask", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.models.biaffine_dependency_parser.BiaffineDependencyParser._parse", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.models.biaffine_dependency_parser.BiaffineDependencyParser._get_mask_for_eval"], ["", "@", "overrides", "\n", "def", "forward", "(", "\n", "self", ",", "# type: ignore", "\n", "words", ":", "TextFieldTensors", ",", "\n", "pos_tags", ":", "torch", ".", "LongTensor", ",", "\n", "metadata", ":", "List", "[", "Dict", "[", "str", ",", "Any", "]", "]", ",", "\n", "head_tags", ":", "torch", ".", "LongTensor", "=", "None", ",", "\n", "head_indices", ":", "torch", ".", "LongTensor", "=", "None", ",", "\n", ")", "->", "Dict", "[", "str", ",", "torch", ".", "Tensor", "]", ":", "\n", "\n", "        ", "\"\"\"\n        Embedding each language by the corresponding parameters for\n        `TextFieldEmbedder`. Batches should contain only samples from a\n        single language.\n        Metadata should have a `lang` key.\n        \"\"\"", "\n", "if", "\"lang\"", "not", "in", "metadata", "[", "0", "]", ":", "\n", "            ", "raise", "ConfigurationError", "(", "\n", "\"metadata is missing 'lang' key; \"", "\n", "\"Use the universal_dependencies_multilang dataset_reader.\"", "\n", ")", "\n", "\n", "", "batch_lang", "=", "metadata", "[", "0", "]", "[", "\"lang\"", "]", "\n", "for", "entry", "in", "metadata", ":", "\n", "            ", "if", "entry", "[", "\"lang\"", "]", "!=", "batch_lang", ":", "\n", "                ", "raise", "ConfigurationError", "(", "\"Two languages in the same batch.\"", ")", "\n", "\n", "", "", "embedded_text_input", "=", "self", ".", "text_field_embedder", "(", "words", ",", "lang", "=", "batch_lang", ")", "\n", "if", "pos_tags", "is", "not", "None", "and", "self", ".", "_pos_tag_embedding", "is", "not", "None", ":", "\n", "            ", "embedded_pos_tags", "=", "self", ".", "_pos_tag_embedding", "(", "pos_tags", ")", "\n", "embedded_text_input", "=", "torch", ".", "cat", "(", "\n", "[", "embedded_text_input", ",", "embedded_pos_tags", "]", ",", "-", "1", "\n", ")", "\n", "", "elif", "self", ".", "_pos_tag_embedding", "is", "not", "None", ":", "\n", "            ", "raise", "ConfigurationError", "(", "\n", "\"Model uses a POS embedding, but no POS tags were passed.\"", "\n", ")", "\n", "\n", "", "mask", "=", "get_text_field_mask", "(", "words", ")", "\n", "\n", "predicted_heads", ",", "predicted_head_tags", ",", "mask", ",", "arc_nll", ",", "tag_nll", "=", "self", ".", "_parse", "(", "\n", "embedded_text_input", ",", "mask", ",", "head_tags", ",", "head_indices", "\n", ")", "\n", "\n", "loss", "=", "arc_nll", "+", "tag_nll", "\n", "\n", "if", "head_indices", "is", "not", "None", "and", "head_tags", "is", "not", "None", ":", "\n", "            ", "evaluation_mask", "=", "self", ".", "_get_mask_for_eval", "(", "mask", "[", ":", ",", "1", ":", "]", ",", "pos_tags", ")", "\n", "# We calculate attatchment scores for the whole sentence", "\n", "# but excluding the symbolic ROOT token at the start,", "\n", "# which is why we start from the second element in the sequence.", "\n", "self", ".", "_lang_attachment_scores", "[", "batch_lang", "]", "(", "\n", "predicted_heads", "[", ":", ",", "1", ":", "]", ",", "\n", "predicted_head_tags", "[", ":", ",", "1", ":", "]", ",", "\n", "head_indices", ",", "\n", "head_tags", ",", "\n", "evaluation_mask", ",", "\n", ")", "\n", "\n", "", "output_dict", "=", "{", "\n", "\"heads\"", ":", "predicted_heads", ",", "\n", "\"head_tags\"", ":", "predicted_head_tags", ",", "\n", "\"arc_loss\"", ":", "arc_nll", ",", "\n", "\"tag_loss\"", ":", "tag_nll", ",", "\n", "\"loss\"", ":", "loss", ",", "\n", "\"mask\"", ":", "mask", ",", "\n", "\"words\"", ":", "[", "meta", "[", "\"words\"", "]", "for", "meta", "in", "metadata", "]", ",", "\n", "\"pos\"", ":", "[", "meta", "[", "\"pos\"", "]", "for", "meta", "in", "metadata", "]", ",", "\n", "}", "\n", "\n", "return", "output_dict", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.models.biaffine_dependency_parser_multilang.BiaffineDependencyParserMultiLang.get_metrics": [[187, 211], ["biaffine_dependency_parser_multilang.BiaffineDependencyParserMultiLang._lang_attachment_scores.items", "scores.get_metric", "scores.get_metric.keys", "metrics.update", "all_uas.append", "all_las.append", "numpy.mean", "numpy.mean"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.metrics.sequence_accuracy.SequenceAccuracy.get_metric", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.metrics.conll_coref_scores.Scorer.update"], ["", "@", "overrides", "\n", "def", "get_metrics", "(", "self", ",", "reset", ":", "bool", "=", "False", ")", "->", "Dict", "[", "str", ",", "float", "]", ":", "\n", "        ", "metrics", "=", "{", "}", "\n", "all_uas", "=", "[", "]", "\n", "all_las", "=", "[", "]", "\n", "for", "lang", ",", "scores", "in", "self", ".", "_lang_attachment_scores", ".", "items", "(", ")", ":", "\n", "            ", "lang_metrics", "=", "scores", ".", "get_metric", "(", "reset", ")", "\n", "\n", "for", "key", "in", "lang_metrics", ".", "keys", "(", ")", ":", "\n", "# Store only those metrics.", "\n", "                ", "if", "key", "in", "[", "\"UAS\"", ",", "\"LAS\"", ",", "\"loss\"", "]", ":", "\n", "                    ", "metrics", "[", "\"{}_{}\"", ".", "format", "(", "key", ",", "lang", ")", "]", "=", "lang_metrics", "[", "key", "]", "\n", "\n", "# Include in the average only languages that should count for early stopping.", "\n", "", "", "if", "lang", "in", "self", ".", "_langs_for_early_stop", ":", "\n", "                ", "all_uas", ".", "append", "(", "metrics", "[", "\"UAS_{}\"", ".", "format", "(", "lang", ")", "]", ")", "\n", "all_las", ".", "append", "(", "metrics", "[", "\"LAS_{}\"", ".", "format", "(", "lang", ")", "]", ")", "\n", "\n", "", "", "if", "self", ".", "_langs_for_early_stop", ":", "\n", "            ", "metrics", ".", "update", "(", "\n", "{", "\"UAS_AVG\"", ":", "numpy", ".", "mean", "(", "all_uas", ")", ",", "\"LAS_AVG\"", ":", "numpy", ".", "mean", "(", "all_las", ")", "}", "\n", ")", "\n", "\n", "", "return", "metrics", "\n", "", "", ""]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.models.decomposable_attention.DecomposableAttention.__init__": [[69, 111], ["allennlp.nn.InitializerApplicator", "allennlp.models.model.Model.__init__", "allennlp.modules.TimeDistributed", "allennlp.modules.matrix_attention.legacy_matrix_attention.LegacyMatrixAttention", "allennlp.modules.TimeDistributed", "vocab.get_vocab_size", "allennlp.common.checks.check_dimensions_match", "allennlp.common.checks.check_dimensions_match", "allennlp.training.metrics.CategoricalAccuracy", "torch.nn.CrossEntropyLoss", "initializer", "text_field_embedder.get_output_dim", "attend_feedforward.get_input_dim", "aggregate_feedforward.get_output_dim"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.graph_util.plot.GraphPlot.__init__", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.data.vocabulary.Vocabulary.get_vocab_size", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.checks.check_dimensions_match", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.checks.check_dimensions_match", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.seq2seq_decoders.auto_regressive_seq_decoder.AutoRegressiveSeqDecoder.get_output_dim", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.seq2vec_encoders.cnn_highway_encoder.CnnHighwayEncoder.get_input_dim", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.seq2seq_decoders.auto_regressive_seq_decoder.AutoRegressiveSeqDecoder.get_output_dim"], []], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.models.decomposable_attention.DecomposableAttention.forward": [[112, 216], ["decomposable_attention.DecomposableAttention._text_field_embedder", "decomposable_attention.DecomposableAttention._text_field_embedder", "allennlp.nn.util.get_text_field_mask().float", "allennlp.nn.util.get_text_field_mask().float", "decomposable_attention.DecomposableAttention._attend_feedforward", "decomposable_attention.DecomposableAttention._attend_feedforward", "decomposable_attention.DecomposableAttention._matrix_attention", "allennlp.nn.util.masked_softmax", "allennlp.nn.util.weighted_sum", "allennlp.nn.util.masked_softmax", "allennlp.nn.util.weighted_sum", "torch.cat", "torch.cat", "decomposable_attention.DecomposableAttention._compare_feedforward", "compared_premise.sum.sum.sum", "decomposable_attention.DecomposableAttention._compare_feedforward", "compared_hypothesis.sum.sum.sum", "torch.cat", "decomposable_attention.DecomposableAttention._aggregate_feedforward", "torch.nn.functional.softmax", "decomposable_attention.DecomposableAttention._premise_encoder", "decomposable_attention.DecomposableAttention._hypothesis_encoder", "decomposable_attention.DecomposableAttention.transpose().contiguous", "allennlp.nn.util.get_text_field_mask().float.unsqueeze", "allennlp.nn.util.get_text_field_mask().float.unsqueeze", "decomposable_attention.DecomposableAttention._loss", "decomposable_attention.DecomposableAttention._accuracy", "allennlp.nn.util.get_text_field_mask", "allennlp.nn.util.get_text_field_mask", "label.long().view", "decomposable_attention.DecomposableAttention.transpose", "label.long"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.nn.util.masked_softmax", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.nn.util.weighted_sum", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.nn.util.masked_softmax", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.nn.util.weighted_sum", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.nn.util.get_text_field_mask", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.nn.util.get_text_field_mask"], []], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.models.decomposable_attention.DecomposableAttention.get_metrics": [[217, 219], ["decomposable_attention.DecomposableAttention._accuracy.get_metric"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.metrics.sequence_accuracy.SequenceAccuracy.get_metric"], []], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.models.bert_for_classification.BertForClassification.__init__": [[47, 85], ["allennlp.nn.initializers.InitializerApplicator", "allennlp.models.model.Model.__init__", "isinstance", "bert_for_classification.BertForClassification.bert_model.parameters", "torch.nn.Dropout", "torch.nn.Linear", "allennlp.training.metrics.CategoricalAccuracy", "torch.nn.CrossEntropyLoss", "initializer", "allennlp.modules.token_embedders.bert_token_embedder.PretrainedBertModel.load", "vocab.get_vocab_size"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.graph_util.plot.GraphPlot.__init__", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.token_embedders.bert_token_embedder.PretrainedBertModel.load", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.data.vocabulary.Vocabulary.get_vocab_size"], ["def", "__init__", "(", "\n", "self", ",", "\n", "vocab", ":", "Vocabulary", ",", "\n", "bert_model", ":", "Union", "[", "str", ",", "BertModel", "]", ",", "\n", "dropout", ":", "float", "=", "0.0", ",", "\n", "num_labels", ":", "int", "=", "None", ",", "\n", "index", ":", "str", "=", "\"bert\"", ",", "\n", "label_namespace", ":", "str", "=", "\"labels\"", ",", "\n", "trainable", ":", "bool", "=", "True", ",", "\n", "initializer", ":", "InitializerApplicator", "=", "InitializerApplicator", "(", ")", ",", "\n", "**", "kwargs", ",", "\n", ")", "->", "None", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "vocab", ",", "**", "kwargs", ")", "\n", "\n", "if", "isinstance", "(", "bert_model", ",", "str", ")", ":", "\n", "            ", "self", ".", "bert_model", "=", "PretrainedBertModel", ".", "load", "(", "bert_model", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "bert_model", "=", "bert_model", "\n", "\n", "", "for", "param", "in", "self", ".", "bert_model", ".", "parameters", "(", ")", ":", "\n", "            ", "param", ".", "requires_grad", "=", "trainable", "\n", "\n", "", "in_features", "=", "self", ".", "bert_model", ".", "config", ".", "hidden_size", "\n", "\n", "self", ".", "_label_namespace", "=", "label_namespace", "\n", "\n", "if", "num_labels", ":", "\n", "            ", "out_features", "=", "num_labels", "\n", "", "else", ":", "\n", "            ", "out_features", "=", "vocab", ".", "get_vocab_size", "(", "namespace", "=", "self", ".", "_label_namespace", ")", "\n", "\n", "", "self", ".", "_dropout", "=", "torch", ".", "nn", ".", "Dropout", "(", "p", "=", "dropout", ")", "\n", "\n", "self", ".", "_classification_layer", "=", "torch", ".", "nn", ".", "Linear", "(", "in_features", ",", "out_features", ")", "\n", "self", ".", "_accuracy", "=", "CategoricalAccuracy", "(", ")", "\n", "self", ".", "_loss", "=", "torch", ".", "nn", ".", "CrossEntropyLoss", "(", ")", "\n", "self", ".", "_index", "=", "index", "\n", "initializer", "(", "self", ".", "_classification_layer", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.models.bert_for_classification.BertForClassification.forward": [[86, 137], ["bert_for_classification.BertForClassification.bert_model", "bert_for_classification.BertForClassification._dropout", "bert_for_classification.BertForClassification._classification_layer", "torch.nn.functional.softmax", "bert_for_classification.BertForClassification._loss", "bert_for_classification.BertForClassification._accuracy", "label.long().view", "label.long"], "methods", ["None"], ["", "def", "forward", "(", "# type: ignore", "\n", "self", ",", "tokens", ":", "TextFieldTensors", ",", "label", ":", "torch", ".", "IntTensor", "=", "None", "\n", ")", "->", "Dict", "[", "str", ",", "torch", ".", "Tensor", "]", ":", "\n", "\n", "        ", "\"\"\"\n        # Parameters\n\n        tokens : TextFieldTensors\n            From a `TextField` (that has a bert-pretrained token indexer)\n        label : torch.IntTensor, optional (default = None)\n            From a `LabelField`\n\n        # Returns\n\n        An output dictionary consisting of:\n\n        logits : torch.FloatTensor\n            A tensor of shape `(batch_size, num_labels)` representing\n            unnormalized log probabilities of the label.\n        probs : torch.FloatTensor\n            A tensor of shape `(batch_size, num_labels)` representing\n            probabilities of the label.\n        loss : torch.FloatTensor, optional\n            A scalar loss to be optimised.\n        \"\"\"", "\n", "inputs", "=", "tokens", "[", "self", ".", "_index", "]", "\n", "input_ids", "=", "inputs", "[", "\"input_ids\"", "]", "\n", "token_type_ids", "=", "inputs", "[", "\"token_type_ids\"", "]", "\n", "input_mask", "=", "(", "input_ids", "!=", "0", ")", ".", "long", "(", ")", "\n", "\n", "_", ",", "pooled", ",", "*", "_", "=", "self", ".", "bert_model", "(", "\n", "input_ids", "=", "input_ids", ",", "\n", "token_type_ids", "=", "token_type_ids", ",", "\n", "attention_mask", "=", "input_mask", ",", "\n", ")", "\n", "\n", "pooled", "=", "self", ".", "_dropout", "(", "pooled", ")", "\n", "\n", "# apply classification layer", "\n", "logits", "=", "self", ".", "_classification_layer", "(", "pooled", ")", "\n", "\n", "probs", "=", "torch", ".", "nn", ".", "functional", ".", "softmax", "(", "logits", ",", "dim", "=", "-", "1", ")", "\n", "\n", "output_dict", "=", "{", "\"logits\"", ":", "logits", ",", "\"probs\"", ":", "probs", "}", "\n", "\n", "if", "label", "is", "not", "None", ":", "\n", "            ", "loss", "=", "self", ".", "_loss", "(", "logits", ",", "label", ".", "long", "(", ")", ".", "view", "(", "-", "1", ")", ")", "\n", "output_dict", "[", "\"loss\"", "]", "=", "loss", "\n", "self", ".", "_accuracy", "(", "logits", ",", "label", ")", "\n", "\n", "", "return", "output_dict", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.models.bert_for_classification.BertForClassification.decode": [[138, 158], ["predictions.dim", "prediction.argmax().item", "bert_for_classification.BertForClassification.vocab.get_index_to_token_vocabulary().get", "classes.append", "str", "range", "prediction.argmax", "bert_for_classification.BertForClassification.vocab.get_index_to_token_vocabulary"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.params.Params.get", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.data.vocabulary.Vocabulary.get_index_to_token_vocabulary"], ["", "@", "overrides", "\n", "def", "decode", "(", "self", ",", "output_dict", ":", "Dict", "[", "str", ",", "torch", ".", "Tensor", "]", ")", "->", "Dict", "[", "str", ",", "torch", ".", "Tensor", "]", ":", "\n", "        ", "\"\"\"\n        Does a simple argmax over the probabilities, converts index to string label, and\n        add `\"label\"` key to the dictionary with the result.\n        \"\"\"", "\n", "predictions", "=", "output_dict", "[", "\"probs\"", "]", "\n", "if", "predictions", ".", "dim", "(", ")", "==", "2", ":", "\n", "            ", "predictions_list", "=", "[", "predictions", "[", "i", "]", "for", "i", "in", "range", "(", "predictions", ".", "shape", "[", "0", "]", ")", "]", "\n", "", "else", ":", "\n", "            ", "predictions_list", "=", "[", "predictions", "]", "\n", "", "classes", "=", "[", "]", "\n", "for", "prediction", "in", "predictions_list", ":", "\n", "            ", "label_idx", "=", "prediction", ".", "argmax", "(", "dim", "=", "-", "1", ")", ".", "item", "(", ")", "\n", "label_str", "=", "self", ".", "vocab", ".", "get_index_to_token_vocabulary", "(", "\n", "self", ".", "_label_namespace", "\n", ")", ".", "get", "(", "label_idx", ",", "str", "(", "label_idx", ")", ")", "\n", "classes", ".", "append", "(", "label_str", ")", "\n", "", "output_dict", "[", "\"label\"", "]", "=", "classes", "\n", "return", "output_dict", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.models.bert_for_classification.BertForClassification.get_metrics": [[159, 162], ["bert_for_classification.BertForClassification._accuracy.get_metric"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.metrics.sequence_accuracy.SequenceAccuracy.get_metric"], ["", "def", "get_metrics", "(", "self", ",", "reset", ":", "bool", "=", "False", ")", "->", "Dict", "[", "str", ",", "float", "]", ":", "\n", "        ", "metrics", "=", "{", "\"accuracy\"", ":", "self", ".", "_accuracy", ".", "get_metric", "(", "reset", ")", "}", "\n", "return", "metrics", "\n", "", "", ""]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.models.simple_tagger.SimpleTagger.__init__": [[53, 103], ["allennlp.nn.InitializerApplicator", "allennlp.models.model.Model.__init__", "simple_tagger.SimpleTagger.vocab.get_vocab_size", "allennlp.modules.TimeDistributed", "allennlp.common.checks.check_dimensions_match", "initializer", "torch.nn.modules.linear.Linear", "torch.nn.modules.linear.Linear", "text_field_embedder.get_output_dim", "encoder.get_input_dim", "allennlp.common.checks.ConfigurationError", "allennlp.training.metrics.CategoricalAccuracy", "allennlp.training.metrics.CategoricalAccuracy", "allennlp.training.metrics.SpanBasedF1Measure", "simple_tagger.SimpleTagger.encoder.get_output_dim"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.graph_util.plot.GraphPlot.__init__", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.data.vocabulary.Vocabulary.get_vocab_size", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.checks.check_dimensions_match", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.seq2seq_decoders.auto_regressive_seq_decoder.AutoRegressiveSeqDecoder.get_output_dim", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.seq2vec_encoders.cnn_highway_encoder.CnnHighwayEncoder.get_input_dim", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.seq2seq_decoders.auto_regressive_seq_decoder.AutoRegressiveSeqDecoder.get_output_dim"], ["def", "__init__", "(", "\n", "self", ",", "\n", "vocab", ":", "Vocabulary", ",", "\n", "text_field_embedder", ":", "TextFieldEmbedder", ",", "\n", "encoder", ":", "Seq2SeqEncoder", ",", "\n", "calculate_span_f1", ":", "bool", "=", "None", ",", "\n", "label_encoding", ":", "Optional", "[", "str", "]", "=", "None", ",", "\n", "label_namespace", ":", "str", "=", "\"labels\"", ",", "\n", "verbose_metrics", ":", "bool", "=", "False", ",", "\n", "initializer", ":", "InitializerApplicator", "=", "InitializerApplicator", "(", ")", ",", "\n", "**", "kwargs", ",", "\n", ")", "->", "None", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "vocab", ",", "**", "kwargs", ")", "\n", "\n", "self", ".", "label_namespace", "=", "label_namespace", "\n", "self", ".", "text_field_embedder", "=", "text_field_embedder", "\n", "self", ".", "num_classes", "=", "self", ".", "vocab", ".", "get_vocab_size", "(", "label_namespace", ")", "\n", "self", ".", "encoder", "=", "encoder", "\n", "self", ".", "_verbose_metrics", "=", "verbose_metrics", "\n", "self", ".", "tag_projection_layer", "=", "TimeDistributed", "(", "\n", "Linear", "(", "self", ".", "encoder", ".", "get_output_dim", "(", ")", ",", "self", ".", "num_classes", ")", "\n", ")", "\n", "\n", "check_dimensions_match", "(", "\n", "text_field_embedder", ".", "get_output_dim", "(", ")", ",", "\n", "encoder", ".", "get_input_dim", "(", ")", ",", "\n", "\"text field embedding dim\"", ",", "\n", "\"encoder input dim\"", ",", "\n", ")", "\n", "\n", "# We keep calculate_span_f1 as a constructor argument for API consistency with", "\n", "# the CrfTagger, even it is redundant in this class", "\n", "# (label_encoding serves the same purpose).", "\n", "if", "calculate_span_f1", "and", "not", "label_encoding", ":", "\n", "            ", "raise", "ConfigurationError", "(", "\n", "\"calculate_span_f1 is True, but no label_encoding was specified.\"", "\n", ")", "\n", "", "self", ".", "metrics", "=", "{", "\n", "\"accuracy\"", ":", "CategoricalAccuracy", "(", ")", ",", "\n", "\"accuracy3\"", ":", "CategoricalAccuracy", "(", "top_k", "=", "3", ")", ",", "\n", "}", "\n", "\n", "if", "calculate_span_f1", "or", "label_encoding", ":", "\n", "            ", "self", ".", "_f1_metric", "=", "SpanBasedF1Measure", "(", "\n", "vocab", ",", "tag_namespace", "=", "label_namespace", ",", "label_encoding", "=", "label_encoding", "\n", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "_f1_metric", "=", "None", "\n", "\n", "", "initializer", "(", "self", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.models.simple_tagger.SimpleTagger.forward": [[104, 167], ["simple_tagger.SimpleTagger.text_field_embedder", "simple_tagger.SimpleTagger.size", "allennlp.nn.util.get_text_field_mask", "simple_tagger.SimpleTagger.encoder", "simple_tagger.SimpleTagger.tag_projection_layer", "simple_tagger.SimpleTagger.view", "torch.softmax().view", "torch.softmax().view", "allennlp.nn.util.sequence_cross_entropy_with_logits", "simple_tagger.SimpleTagger.metrics.values", "torch.softmax", "torch.softmax", "metric", "simple_tagger.SimpleTagger._f1_metric", "allennlp.nn.util.get_text_field_mask.float", "allennlp.nn.util.get_text_field_mask.float"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.nn.util.get_text_field_mask", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.nn.util.sequence_cross_entropy_with_logits"], ["", "@", "overrides", "\n", "def", "forward", "(", "\n", "self", ",", "# type: ignore", "\n", "tokens", ":", "TextFieldTensors", ",", "\n", "tags", ":", "torch", ".", "LongTensor", "=", "None", ",", "\n", "metadata", ":", "List", "[", "Dict", "[", "str", ",", "Any", "]", "]", "=", "None", ",", "\n", ")", "->", "Dict", "[", "str", ",", "torch", ".", "Tensor", "]", ":", "\n", "\n", "        ", "\"\"\"\n        # Parameters\n\n        tokens : TextFieldTensors, required\n            The output of `TextField.as_array()`, which should typically be passed directly to a\n            `TextFieldEmbedder`. This output is a dictionary mapping keys to `TokenIndexer`\n            tensors.  At its most basic, using a `SingleIdTokenIndexer` this is : `{\"tokens\":\n            Tensor(batch_size, num_tokens)}`. This dictionary will have the same keys as were used\n            for the `TokenIndexers` when you created the `TextField` representing your\n            sequence.  The dictionary is designed to be passed directly to a `TextFieldEmbedder`,\n            which knows how to combine different word representations into a single vector per\n            token in your input.\n        tags : torch.LongTensor, optional (default = None)\n            A torch tensor representing the sequence of integer gold class labels of shape\n            `(batch_size, num_tokens)`.\n        metadata : `List[Dict[str, Any]]`, optional, (default = None)\n            metadata containing the original words in the sentence to be tagged under a 'words' key.\n\n        # Returns\n\n        An output dictionary consisting of:\n        logits : torch.FloatTensor\n            A tensor of shape `(batch_size, num_tokens, tag_vocab_size)` representing\n            unnormalised log probabilities of the tag classes.\n        class_probabilities : torch.FloatTensor\n            A tensor of shape `(batch_size, num_tokens, tag_vocab_size)` representing\n            a distribution of the tag classes per word.\n        loss : torch.FloatTensor, optional\n            A scalar loss to be optimised.\n\n        \"\"\"", "\n", "embedded_text_input", "=", "self", ".", "text_field_embedder", "(", "tokens", ")", "\n", "batch_size", ",", "sequence_length", ",", "_", "=", "embedded_text_input", ".", "size", "(", ")", "\n", "mask", "=", "get_text_field_mask", "(", "tokens", ")", "\n", "encoded_text", "=", "self", ".", "encoder", "(", "embedded_text_input", ",", "mask", ")", "\n", "\n", "logits", "=", "self", ".", "tag_projection_layer", "(", "encoded_text", ")", "\n", "reshaped_log_probs", "=", "logits", ".", "view", "(", "-", "1", ",", "self", ".", "num_classes", ")", "\n", "class_probabilities", "=", "F", ".", "softmax", "(", "reshaped_log_probs", ",", "dim", "=", "-", "1", ")", ".", "view", "(", "\n", "[", "batch_size", ",", "sequence_length", ",", "self", ".", "num_classes", "]", "\n", ")", "\n", "\n", "output_dict", "=", "{", "\"logits\"", ":", "logits", ",", "\"class_probabilities\"", ":", "class_probabilities", "}", "\n", "\n", "if", "tags", "is", "not", "None", ":", "\n", "            ", "loss", "=", "sequence_cross_entropy_with_logits", "(", "logits", ",", "tags", ",", "mask", ")", "\n", "for", "metric", "in", "self", ".", "metrics", ".", "values", "(", ")", ":", "\n", "                ", "metric", "(", "logits", ",", "tags", ",", "mask", ".", "float", "(", ")", ")", "\n", "", "if", "self", ".", "_f1_metric", "is", "not", "None", ":", "\n", "                ", "self", ".", "_f1_metric", "(", "logits", ",", "tags", ",", "mask", ".", "float", "(", ")", ")", "\n", "", "output_dict", "[", "\"loss\"", "]", "=", "loss", "\n", "\n", "", "if", "metadata", "is", "not", "None", ":", "\n", "            ", "output_dict", "[", "\"words\"", "]", "=", "[", "x", "[", "\"words\"", "]", "for", "x", "in", "metadata", "]", "\n", "", "return", "output_dict", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.models.simple_tagger.SimpleTagger.decode": [[168, 192], ["all_predictions.cpu().data.numpy.cpu().data.numpy.cpu().data.numpy", "numpy.argmax", "all_tags.append", "simple_tagger.SimpleTagger.vocab.get_token_from_index", "all_predictions.cpu().data.numpy.cpu().data.numpy.cpu", "range"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.data.vocabulary.Vocabulary.get_token_from_index"], ["", "@", "overrides", "\n", "def", "decode", "(", "self", ",", "output_dict", ":", "Dict", "[", "str", ",", "torch", ".", "Tensor", "]", ")", "->", "Dict", "[", "str", ",", "torch", ".", "Tensor", "]", ":", "\n", "        ", "\"\"\"\n        Does a simple position-wise argmax over each token, converts indices to string labels, and\n        adds a `\"tags\"` key to the dictionary with the result.\n        \"\"\"", "\n", "all_predictions", "=", "output_dict", "[", "\"class_probabilities\"", "]", "\n", "all_predictions", "=", "all_predictions", ".", "cpu", "(", ")", ".", "data", ".", "numpy", "(", ")", "\n", "if", "all_predictions", ".", "ndim", "==", "3", ":", "\n", "            ", "predictions_list", "=", "[", "\n", "all_predictions", "[", "i", "]", "for", "i", "in", "range", "(", "all_predictions", ".", "shape", "[", "0", "]", ")", "\n", "]", "\n", "", "else", ":", "\n", "            ", "predictions_list", "=", "[", "all_predictions", "]", "\n", "", "all_tags", "=", "[", "]", "\n", "for", "predictions", "in", "predictions_list", ":", "\n", "            ", "argmax_indices", "=", "numpy", ".", "argmax", "(", "predictions", ",", "axis", "=", "-", "1", ")", "\n", "tags", "=", "[", "\n", "self", ".", "vocab", ".", "get_token_from_index", "(", "x", ",", "namespace", "=", "self", ".", "label_namespace", ")", "\n", "for", "x", "in", "argmax_indices", "\n", "]", "\n", "all_tags", ".", "append", "(", "tags", ")", "\n", "", "output_dict", "[", "\"tags\"", "]", "=", "all_tags", "\n", "return", "output_dict", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.models.simple_tagger.SimpleTagger.get_metrics": [[193, 209], ["metric.get_metric", "simple_tagger.SimpleTagger._f1_metric.get_metric", "simple_tagger.SimpleTagger.metrics.items", "metrics_to_return.update", "metrics_to_return.update", "simple_tagger.SimpleTagger.items"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.metrics.sequence_accuracy.SequenceAccuracy.get_metric", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.metrics.sequence_accuracy.SequenceAccuracy.get_metric", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.metrics.conll_coref_scores.Scorer.update", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.metrics.conll_coref_scores.Scorer.update"], ["", "@", "overrides", "\n", "def", "get_metrics", "(", "self", ",", "reset", ":", "bool", "=", "False", ")", "->", "Dict", "[", "str", ",", "float", "]", ":", "\n", "        ", "metrics_to_return", "=", "{", "\n", "metric_name", ":", "metric", ".", "get_metric", "(", "reset", ")", "\n", "for", "metric_name", ",", "metric", "in", "self", ".", "metrics", ".", "items", "(", ")", "\n", "}", "\n", "\n", "if", "self", ".", "_f1_metric", "is", "not", "None", ":", "\n", "            ", "f1_dict", "=", "self", ".", "_f1_metric", ".", "get_metric", "(", "reset", "=", "reset", ")", "\n", "if", "self", ".", "_verbose_metrics", ":", "\n", "                ", "metrics_to_return", ".", "update", "(", "f1_dict", ")", "\n", "", "else", ":", "\n", "                ", "metrics_to_return", ".", "update", "(", "\n", "{", "x", ":", "y", "for", "x", ",", "y", "in", "f1_dict", ".", "items", "(", ")", "if", "\"overall\"", "in", "x", "}", "\n", ")", "\n", "", "", "return", "metrics_to_return", "\n", "", "", ""]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.models.semantic_role_labeler.SemanticRoleLabeler.__init__": [[65, 107], ["allennlp.nn.InitializerApplicator", "allennlp.models.model.Model.__init__", "semantic_role_labeler.SemanticRoleLabeler.vocab.get_vocab_size", "allennlp.modules.token_embedders.Embedding", "allennlp.modules.TimeDistributed", "torch.nn.modules.Dropout", "torch.nn.modules.Dropout", "allennlp.common.checks.check_dimensions_match", "initializer", "allennlp.training.metrics.srl_eval_scorer.SrlEvalScorer", "torch.nn.modules.Linear", "torch.nn.modules.Linear", "encoder.get_input_dim", "semantic_role_labeler.SemanticRoleLabeler.encoder.get_output_dim", "text_field_embedder.get_output_dim"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.graph_util.plot.GraphPlot.__init__", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.data.vocabulary.Vocabulary.get_vocab_size", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.checks.check_dimensions_match", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.seq2vec_encoders.cnn_highway_encoder.CnnHighwayEncoder.get_input_dim", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.seq2seq_decoders.auto_regressive_seq_decoder.AutoRegressiveSeqDecoder.get_output_dim", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.seq2seq_decoders.auto_regressive_seq_decoder.AutoRegressiveSeqDecoder.get_output_dim"], ["\n", "spacy_doc", "=", "Doc", "(", "self", ".", "_tokenizer", ".", "spacy", ".", "vocab", ",", "words", "=", "tokenized_sentence", ")", "\n", "for", "pipe", "in", "filter", "(", "None", ",", "self", ".", "_tokenizer", ".", "spacy", ".", "pipeline", ")", ":", "\n", "            ", "pipe", "[", "1", "]", "(", "spacy_doc", ")", "\n", "\n", "", "tokens", "=", "[", "token", "for", "token", "in", "spacy_doc", "]", "\n", "instances", "=", "self", ".", "tokens_to_instances", "(", "tokens", ")", "\n", "\n", "if", "not", "instances", ":", "\n", "            ", "return", "sanitize", "(", "{", "\"verbs\"", ":", "[", "]", ",", "\"words\"", ":", "tokens", "}", ")", "\n", "\n", "", "return", "self", ".", "predict_instances", "(", "instances", ")", "\n", "\n", "", "@", "staticmethod", "\n", "def", "make_srl_string", "(", "words", ":", "List", "[", "str", "]", ",", "tags", ":", "List", "[", "str", "]", ")", "->", "str", ":", "\n", "        ", "frame", "=", "[", "]", "\n", "chunk", "=", "[", "]", "\n", "\n", "for", "(", "token", ",", "tag", ")", "in", "zip", "(", "words", ",", "tags", ")", ":", "\n", "            ", "if", "tag", ".", "startswith", "(", "\"I-\"", ")", ":", "\n", "                ", "chunk", ".", "append", "(", "token", ")", "\n", "", "else", ":", "\n", "                ", "if", "chunk", ":", "\n", "                    ", "frame", ".", "append", "(", "\"[\"", "+", "\" \"", ".", "join", "(", "chunk", ")", "+", "\"]\"", ")", "\n", "chunk", "=", "[", "]", "\n", "\n", "", "if", "tag", ".", "startswith", "(", "\"B-\"", ")", ":", "\n", "                    ", "chunk", ".", "append", "(", "tag", "[", "2", ":", "]", "+", "\": \"", "+", "token", ")", "\n", "", "elif", "tag", "==", "\"O\"", ":", "\n", "                    ", "frame", ".", "append", "(", "token", ")", "\n", "\n", "", "", "", "if", "chunk", ":", "\n", "            ", "frame", ".", "append", "(", "\"[\"", "+", "\" \"", ".", "join", "(", "chunk", ")", "+", "\"]\"", ")", "\n", "\n", "", "return", "\" \"", ".", "join", "(", "frame", ")", "\n", "\n", "", "@", "overrides", "\n", "def", "_json_to_instance", "(", "self", ",", "json_dict", ":", "JsonDict", ")", ":", "\n", "        ", "raise", "NotImplementedError", "(", "\n", "\"The SRL model uses a different API for creating instances.\"", "\n", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.models.semantic_role_labeler.SemanticRoleLabeler.forward": [[108, 218], ["semantic_role_labeler.SemanticRoleLabeler.embedding_dropout", "allennlp.nn.util.get_text_field_mask", "semantic_role_labeler.SemanticRoleLabeler.binary_feature_embedding", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat.size", "torch.cat.size", "semantic_role_labeler.SemanticRoleLabeler.encoder", "semantic_role_labeler.SemanticRoleLabeler.tag_projection_layer", "semantic_role_labeler.SemanticRoleLabeler.view", "torch.softmax().view", "torch.softmax().view", "zip", "semantic_role_labeler.SemanticRoleLabeler.text_field_embedder", "verb_indicator.long", "allennlp.nn.util.sequence_cross_entropy_with_logits", "list", "list", "torch.softmax", "torch.softmax", "semantic_role_labeler.SemanticRoleLabeler.decode().pop", "semantic_role_labeler.SemanticRoleLabeler.span_metric", "allennlp.models.srl_util.convert_bio_tags_to_conll_format", "allennlp.models.srl_util.convert_bio_tags_to_conll_format", "semantic_role_labeler.SemanticRoleLabeler.decode"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.nn.util.get_text_field_mask", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.nn.util.sequence_cross_entropy_with_logits", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.params.Params.pop", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.models.srl_util.convert_bio_tags_to_conll_format", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.models.srl_util.convert_bio_tags_to_conll_format", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.coreference_resolution.coref.CoreferenceResolver.decode"], ["", "def", "tokens_to_instances", "(", "self", ",", "tokens", ")", ":", "\n", "        ", "words", "=", "[", "token", ".", "text", "for", "token", "in", "tokens", "]", "\n", "instances", ":", "List", "[", "Instance", "]", "=", "[", "]", "\n", "for", "i", ",", "word", "in", "enumerate", "(", "tokens", ")", ":", "\n", "            ", "if", "word", ".", "pos_", "==", "\"VERB\"", ":", "\n", "                ", "verb_labels", "=", "[", "0", "for", "_", "in", "words", "]", "\n", "verb_labels", "[", "i", "]", "=", "1", "\n", "instance", "=", "self", ".", "_dataset_reader", ".", "text_to_instance", "(", "tokens", ",", "verb_labels", ")", "\n", "instances", ".", "append", "(", "instance", ")", "\n", "", "", "return", "instances", "\n", "\n", "", "def", "_sentence_to_srl_instances", "(", "self", ",", "json_dict", ":", "JsonDict", ")", "->", "List", "[", "Instance", "]", ":", "\n", "        ", "\"\"\"\n        The SRL model has a slightly different API from other models, as the model is run\n        forward for every verb in the sentence. This means that for a single sentence, we need\n        to generate a `List[Instance]`, where the length of this list corresponds to the number\n        of verbs in the sentence. Additionally, all of these verbs share the same return dictionary\n        after being passed through the model (as really we care about all the frames of the sentence\n        together, rather than separately).\n\n        # Parameters\n\n        json_dict : `JsonDict`, required.\n            JSON that looks like `{\"sentence\": \"...\"}`.\n\n        # Returns\n\n        instances : `List[Instance]`\n            One instance per verb.\n        \"\"\"", "\n", "sentence", "=", "json_dict", "[", "\"sentence\"", "]", "\n", "tokens", "=", "self", ".", "_tokenizer", ".", "tokenize", "(", "sentence", ")", "\n", "return", "self", ".", "tokens_to_instances", "(", "tokens", ")", "\n", "\n", "", "@", "overrides", "\n", "def", "predict_batch_json", "(", "self", ",", "inputs", ":", "List", "[", "JsonDict", "]", ")", "->", "List", "[", "JsonDict", "]", ":", "\n", "        ", "\"\"\"\n        Expects JSON that looks like `[{\"sentence\": \"...\"}, {\"sentence\": \"...\"}, ...]`\n        and returns JSON that looks like\n\n        .. code-block:: js\n\n            [\n                {\"words\": [...],\n                 \"verbs\": [\n                    {\"verb\": \"...\", \"description\": \"...\", \"tags\": [...]},\n                    ...\n                    {\"verb\": \"...\", \"description\": \"...\", \"tags\": [...]},\n                ]},\n                {\"words\": [...],\n                 \"verbs\": [\n                    {\"verb\": \"...\", \"description\": \"...\", \"tags\": [...]},\n                    ...\n                    {\"verb\": \"...\", \"description\": \"...\", \"tags\": [...]},\n                ]}\n            ]\n        \"\"\"", "\n", "# For SRL, we have more instances than sentences, but the user specified", "\n", "# a batch size with respect to the number of sentences passed, so we respect", "\n", "# that here by taking the batch size which we use to be the number of sentences", "\n", "# we are given.", "\n", "batch_size", "=", "len", "(", "inputs", ")", "\n", "instances_per_sentence", "=", "[", "\n", "self", ".", "_sentence_to_srl_instances", "(", "json", ")", "for", "json", "in", "inputs", "\n", "]", "\n", "\n", "flattened_instances", "=", "[", "\n", "instance", "\n", "for", "sentence_instances", "in", "instances_per_sentence", "\n", "for", "instance", "in", "sentence_instances", "\n", "]", "\n", "\n", "if", "not", "flattened_instances", ":", "\n", "            ", "return", "sanitize", "(", "\n", "[", "\n", "{", "\"verbs\"", ":", "[", "]", ",", "\"words\"", ":", "self", ".", "_tokenizer", ".", "tokenize", "(", "x", "[", "\"sentence\"", "]", ")", "}", "\n", "for", "x", "in", "inputs", "\n", "]", "\n", ")", "\n", "\n", "# Make the instances into batches and check the last batch for", "\n", "# padded elements as the number of instances might not be perfectly", "\n", "# divisible by the batch size.", "\n", "", "batched_instances", "=", "group_by_count", "(", "flattened_instances", ",", "batch_size", ",", "None", ")", "\n", "batched_instances", "[", "-", "1", "]", "=", "[", "\n", "instance", "for", "instance", "in", "batched_instances", "[", "-", "1", "]", "if", "instance", "is", "not", "None", "\n", "]", "\n", "# Run the model on the batches.", "\n", "outputs", "=", "[", "]", "\n", "for", "batch", "in", "batched_instances", ":", "\n", "            ", "outputs", ".", "extend", "(", "self", ".", "_model", ".", "forward_on_instances", "(", "batch", ")", ")", "\n", "\n", "", "verbs_per_sentence", "=", "[", "len", "(", "sent", ")", "for", "sent", "in", "instances_per_sentence", "]", "\n", "return_dicts", ":", "List", "[", "JsonDict", "]", "=", "[", "{", "\"verbs\"", ":", "[", "]", "}", "for", "x", "in", "inputs", "]", "\n", "\n", "output_index", "=", "0", "\n", "for", "sentence_index", ",", "verb_count", "in", "enumerate", "(", "verbs_per_sentence", ")", ":", "\n", "            ", "if", "verb_count", "==", "0", ":", "\n", "# We didn't run any predictions for sentences with no verbs,", "\n", "# so we don't have a way to extract the original sentence.", "\n", "# Here we just tokenize the input again.", "\n", "                ", "original_text", "=", "self", ".", "_tokenizer", ".", "tokenize", "(", "\n", "inputs", "[", "sentence_index", "]", "[", "\"sentence\"", "]", "\n", ")", "\n", "return_dicts", "[", "sentence_index", "]", "[", "\"words\"", "]", "=", "original_text", "\n", "continue", "\n", "\n", "", "for", "_", "in", "range", "(", "verb_count", ")", ":", "\n", "                ", "output", "=", "outputs", "[", "output_index", "]", "\n", "words", "=", "output", "[", "\"words\"", "]", "\n", "tags", "=", "output", "[", "\"tags\"", "]", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.models.semantic_role_labeler.SemanticRoleLabeler.decode": [[219, 254], ["allennlp.nn.util.get_lengths_from_binary_sequence_mask().data.tolist", "semantic_role_labeler.SemanticRoleLabeler.get_viterbi_pairwise_potentials", "semantic_role_labeler.SemanticRoleLabeler.get_start_transitions", "zip", "all_predictions.dim", "allennlp.nn.util.viterbi_decode", "all_tags.append", "all_predictions[].detach().cpu", "semantic_role_labeler.SemanticRoleLabeler.vocab.get_token_from_index", "allennlp.nn.util.get_lengths_from_binary_sequence_mask", "range", "all_predictions[].detach", "all_predictions.size"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.models.srl_bert.SrlBert.get_viterbi_pairwise_potentials", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.models.srl_bert.SrlBert.get_start_transitions", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.nn.util.viterbi_decode", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.data.vocabulary.Vocabulary.get_token_from_index", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.nn.util.get_lengths_from_binary_sequence_mask"], ["description", "=", "self", ".", "make_srl_string", "(", "words", ",", "tags", ")", "\n", "return_dicts", "[", "sentence_index", "]", "[", "\"words\"", "]", "=", "words", "\n", "return_dicts", "[", "sentence_index", "]", "[", "\"verbs\"", "]", ".", "append", "(", "\n", "{", "\"verb\"", ":", "output", "[", "\"verb\"", "]", ",", "\"description\"", ":", "description", ",", "\"tags\"", ":", "tags", "}", "\n", ")", "\n", "output_index", "+=", "1", "\n", "\n", "", "", "return", "sanitize", "(", "return_dicts", ")", "\n", "\n", "", "def", "predict_instances", "(", "self", ",", "instances", ":", "List", "[", "Instance", "]", ")", "->", "JsonDict", ":", "\n", "        ", "outputs", "=", "self", ".", "_model", ".", "forward_on_instances", "(", "instances", ")", "\n", "\n", "results", "=", "{", "\"verbs\"", ":", "[", "]", ",", "\"words\"", ":", "outputs", "[", "0", "]", "[", "\"words\"", "]", "}", "\n", "for", "output", "in", "outputs", ":", "\n", "            ", "tags", "=", "output", "[", "\"tags\"", "]", "\n", "description", "=", "self", ".", "make_srl_string", "(", "output", "[", "\"words\"", "]", ",", "tags", ")", "\n", "results", "[", "\"verbs\"", "]", ".", "append", "(", "\n", "{", "\"verb\"", ":", "output", "[", "\"verb\"", "]", ",", "\"description\"", ":", "description", ",", "\"tags\"", ":", "tags", "}", "\n", ")", "\n", "\n", "", "return", "sanitize", "(", "results", ")", "\n", "\n", "", "@", "overrides", "\n", "def", "predict_json", "(", "self", ",", "inputs", ":", "JsonDict", ")", "->", "JsonDict", ":", "\n", "        "]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.models.semantic_role_labeler.SemanticRoleLabeler.get_metrics": [[255, 267], ["semantic_role_labeler.SemanticRoleLabeler.span_metric.get_metric", "semantic_role_labeler.SemanticRoleLabeler.items"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.metrics.sequence_accuracy.SequenceAccuracy.get_metric"], ["\n", "instances", "=", "self", ".", "_sentence_to_srl_instances", "(", "inputs", ")", "\n", "\n", "if", "not", "instances", ":", "\n", "            ", "return", "sanitize", "(", "\n", "{", "\"verbs\"", ":", "[", "]", ",", "\"words\"", ":", "self", ".", "_tokenizer", ".", "tokenize", "(", "inputs", "[", "\"sentence\"", "]", ")", "}", "\n", ")", "\n", "\n", "", "return", "self", ".", "predict_instances", "(", "instances", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.models.semantic_role_labeler.SemanticRoleLabeler.get_viterbi_pairwise_potentials": [[268, 292], ["semantic_role_labeler.SemanticRoleLabeler.vocab.get_index_to_token_vocabulary", "len", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "semantic_role_labeler.SemanticRoleLabeler.items", "semantic_role_labeler.SemanticRoleLabeler.items", "float"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.data.vocabulary.Vocabulary.get_index_to_token_vocabulary"], []], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.models.semantic_role_labeler.SemanticRoleLabeler.get_start_transitions": [[293, 314], ["semantic_role_labeler.SemanticRoleLabeler.vocab.get_index_to_token_vocabulary", "len", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "semantic_role_labeler.SemanticRoleLabeler.items", "float"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.data.vocabulary.Vocabulary.get_index_to_token_vocabulary"], []], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.models.semantic_role_labeler.write_to_conll_eval_file": [[316, 362], ["warnings.warn", "allennlp.models.srl_util.write_bio_formatted_tags_to_file"], "function", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.models.srl_util.write_bio_formatted_tags_to_file"], []], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.models.srl_bert.SrlBert.__init__": [[42, 75], ["allennlp.nn.InitializerApplicator", "allennlp.models.model.Model.__init__", "isinstance", "srl_bert.SrlBert.vocab.get_vocab_size", "torch.nn.modules.Linear", "torch.nn.modules.Linear", "torch.nn.modules.Dropout", "torch.nn.modules.Dropout", "initializer", "transformers.modeling_bert.BertModel.from_pretrained", "allennlp.training.metrics.srl_eval_scorer.SrlEvalScorer"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.graph_util.plot.GraphPlot.__init__", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.data.vocabulary.Vocabulary.get_vocab_size"], ["def", "__init__", "(", "\n", "self", ",", "\n", "vocab", ":", "Vocabulary", ",", "\n", "bert_model", ":", "Union", "[", "str", ",", "BertModel", "]", ",", "\n", "embedding_dropout", ":", "float", "=", "0.0", ",", "\n", "initializer", ":", "InitializerApplicator", "=", "InitializerApplicator", "(", ")", ",", "\n", "label_smoothing", ":", "float", "=", "None", ",", "\n", "ignore_span_metric", ":", "bool", "=", "False", ",", "\n", "srl_eval_path", ":", "str", "=", "DEFAULT_SRL_EVAL_PATH", ",", "\n", "**", "kwargs", ",", "\n", ")", "->", "None", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "vocab", ",", "**", "kwargs", ")", "\n", "\n", "if", "isinstance", "(", "bert_model", ",", "str", ")", ":", "\n", "            ", "self", ".", "bert_model", "=", "BertModel", ".", "from_pretrained", "(", "bert_model", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "bert_model", "=", "bert_model", "\n", "\n", "", "self", ".", "num_classes", "=", "self", ".", "vocab", ".", "get_vocab_size", "(", "\"labels\"", ")", "\n", "if", "srl_eval_path", "is", "not", "None", ":", "\n", "# For the span based evaluation, we don't want to consider labels", "\n", "# for verb, because the verb index is provided to the model.", "\n", "            ", "self", ".", "span_metric", "=", "SrlEvalScorer", "(", "srl_eval_path", ",", "ignore_classes", "=", "[", "\"V\"", "]", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "span_metric", "=", "None", "\n", "", "self", ".", "tag_projection_layer", "=", "Linear", "(", "\n", "self", ".", "bert_model", ".", "config", ".", "hidden_size", ",", "self", ".", "num_classes", "\n", ")", "\n", "\n", "self", ".", "embedding_dropout", "=", "Dropout", "(", "p", "=", "embedding_dropout", ")", "\n", "self", ".", "_label_smoothing", "=", "label_smoothing", "\n", "self", ".", "ignore_span_metric", "=", "ignore_span_metric", "\n", "initializer", "(", "self", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.models.srl_bert.SrlBert.forward": [[76, 181], ["allennlp.nn.util.get_text_field_mask", "srl_bert.SrlBert.bert_model", "srl_bert.SrlBert.embedding_dropout", "srl_bert.SrlBert.size", "srl_bert.SrlBert.tag_projection_layer", "srl_bert.SrlBert.view", "torch.softmax().view", "torch.softmax().view", "zip", "list", "list", "list", "allennlp.nn.util.sequence_cross_entropy_with_logits", "allennlp.nn.util.get_token_ids_from_text_field_tensors", "torch.softmax", "torch.softmax", "srl_bert.SrlBert.decode().pop", "srl_bert.SrlBert.span_metric", "allennlp.models.srl_util.convert_bio_tags_to_conll_format", "allennlp.models.srl_util.convert_bio_tags_to_conll_format", "srl_bert.SrlBert.decode"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.nn.util.get_text_field_mask", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.nn.util.sequence_cross_entropy_with_logits", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.nn.util.get_token_ids_from_text_field_tensors", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.params.Params.pop", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.models.srl_util.convert_bio_tags_to_conll_format", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.models.srl_util.convert_bio_tags_to_conll_format", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.coreference_resolution.coref.CoreferenceResolver.decode"], ["", "def", "forward", "(", "# type: ignore", "\n", "self", ",", "\n", "tokens", ":", "TextFieldTensors", ",", "\n", "verb_indicator", ":", "torch", ".", "Tensor", ",", "\n", "metadata", ":", "List", "[", "Any", "]", ",", "\n", "tags", ":", "torch", ".", "LongTensor", "=", "None", ",", "\n", ")", ":", "\n", "\n", "        ", "\"\"\"\n        # Parameters\n\n        tokens : TextFieldTensors, required\n            The output of `TextField.as_array()`, which should typically be passed directly to a\n            `TextFieldEmbedder`. For this model, this must be a `SingleIdTokenIndexer` which\n            indexes wordpieces from the BERT vocabulary.\n        verb_indicator: torch.LongTensor, required.\n            An integer `SequenceFeatureField` representation of the position of the verb\n            in the sentence. This should have shape (batch_size, num_tokens) and importantly, can be\n            all zeros, in the case that the sentence has no verbal predicate.\n        tags : torch.LongTensor, optional (default = None)\n            A torch tensor representing the sequence of integer gold class labels\n            of shape `(batch_size, num_tokens)`\n        metadata : `List[Dict[str, Any]]`, optional, (default = None)\n            metadata containg the original words in the sentence, the verb to compute the\n            frame for, and start offsets for converting wordpieces back to a sequence of words,\n            under 'words', 'verb' and 'offsets' keys, respectively.\n\n        # Returns\n\n        An output dictionary consisting of:\n        logits : torch.FloatTensor\n            A tensor of shape `(batch_size, num_tokens, tag_vocab_size)` representing\n            unnormalised log probabilities of the tag classes.\n        class_probabilities : torch.FloatTensor\n            A tensor of shape `(batch_size, num_tokens, tag_vocab_size)` representing\n            a distribution of the tag classes per word.\n        loss : torch.FloatTensor, optional\n            A scalar loss to be optimised.\n        \"\"\"", "\n", "mask", "=", "get_text_field_mask", "(", "tokens", ")", "\n", "bert_embeddings", ",", "_", "=", "self", ".", "bert_model", "(", "\n", "input_ids", "=", "util", ".", "get_token_ids_from_text_field_tensors", "(", "tokens", ")", ",", "\n", "token_type_ids", "=", "verb_indicator", ",", "\n", "attention_mask", "=", "mask", ",", "\n", ")", "\n", "\n", "embedded_text_input", "=", "self", ".", "embedding_dropout", "(", "bert_embeddings", ")", "\n", "batch_size", ",", "sequence_length", ",", "_", "=", "embedded_text_input", ".", "size", "(", ")", "\n", "logits", "=", "self", ".", "tag_projection_layer", "(", "embedded_text_input", ")", "\n", "\n", "reshaped_log_probs", "=", "logits", ".", "view", "(", "-", "1", ",", "self", ".", "num_classes", ")", "\n", "class_probabilities", "=", "F", ".", "softmax", "(", "reshaped_log_probs", ",", "dim", "=", "-", "1", ")", ".", "view", "(", "\n", "[", "batch_size", ",", "sequence_length", ",", "self", ".", "num_classes", "]", "\n", ")", "\n", "output_dict", "=", "{", "\"logits\"", ":", "logits", ",", "\"class_probabilities\"", ":", "class_probabilities", "}", "\n", "# We need to retain the mask in the output dictionary", "\n", "# so that we can crop the sequences to remove padding", "\n", "# when we do viterbi inference in self.decode.", "\n", "output_dict", "[", "\"mask\"", "]", "=", "mask", "\n", "# We add in the offsets here so we can compute the un-wordpieced tags.", "\n", "words", ",", "verbs", ",", "offsets", "=", "zip", "(", "\n", "*", "[", "(", "x", "[", "\"words\"", "]", ",", "x", "[", "\"verb\"", "]", ",", "x", "[", "\"offsets\"", "]", ")", "for", "x", "in", "metadata", "]", "\n", ")", "\n", "output_dict", "[", "\"words\"", "]", "=", "list", "(", "words", ")", "\n", "output_dict", "[", "\"verb\"", "]", "=", "list", "(", "verbs", ")", "\n", "output_dict", "[", "\"wordpiece_offsets\"", "]", "=", "list", "(", "offsets", ")", "\n", "\n", "if", "tags", "is", "not", "None", ":", "\n", "            ", "loss", "=", "sequence_cross_entropy_with_logits", "(", "\n", "logits", ",", "tags", ",", "mask", ",", "label_smoothing", "=", "self", ".", "_label_smoothing", "\n", ")", "\n", "if", "(", "\n", "not", "self", ".", "ignore_span_metric", "\n", "and", "self", ".", "span_metric", "is", "not", "None", "\n", "and", "not", "self", ".", "training", "\n", ")", ":", "\n", "                ", "batch_verb_indices", "=", "[", "\n", "example_metadata", "[", "\"verb_index\"", "]", "for", "example_metadata", "in", "metadata", "\n", "]", "\n", "batch_sentences", "=", "[", "\n", "example_metadata", "[", "\"words\"", "]", "for", "example_metadata", "in", "metadata", "\n", "]", "\n", "# Get the BIO tags from decode()", "\n", "# TODO (nfliu): This is kind of a hack, consider splitting out part", "\n", "# of decode() to a separate function.", "\n", "batch_bio_predicted_tags", "=", "self", ".", "decode", "(", "output_dict", ")", ".", "pop", "(", "\"tags\"", ")", "\n", "batch_conll_predicted_tags", "=", "[", "\n", "convert_bio_tags_to_conll_format", "(", "tags", ")", "\n", "for", "tags", "in", "batch_bio_predicted_tags", "\n", "]", "\n", "batch_bio_gold_tags", "=", "[", "\n", "example_metadata", "[", "\"gold_tags\"", "]", "for", "example_metadata", "in", "metadata", "\n", "]", "\n", "batch_conll_gold_tags", "=", "[", "\n", "convert_bio_tags_to_conll_format", "(", "tags", ")", "\n", "for", "tags", "in", "batch_bio_gold_tags", "\n", "]", "\n", "self", ".", "span_metric", "(", "\n", "batch_verb_indices", ",", "\n", "batch_sentences", ",", "\n", "batch_conll_predicted_tags", ",", "\n", "batch_conll_gold_tags", ",", "\n", ")", "\n", "", "output_dict", "[", "\"loss\"", "]", "=", "loss", "\n", "", "return", "output_dict", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.models.srl_bert.SrlBert.decode": [[182, 237], ["allennlp.nn.util.get_lengths_from_binary_sequence_mask().data.tolist", "srl_bert.SrlBert.get_viterbi_pairwise_potentials", "srl_bert.SrlBert.get_start_transitions", "zip", "all_predictions.dim", "allennlp.nn.util.viterbi_decode", "wordpiece_tags.append", "word_tags.append", "all_predictions[].detach().cpu", "srl_bert.SrlBert.vocab.get_token_from_index", "allennlp.nn.util.get_lengths_from_binary_sequence_mask", "range", "all_predictions[].detach", "all_predictions.size"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.models.srl_bert.SrlBert.get_viterbi_pairwise_potentials", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.models.srl_bert.SrlBert.get_start_transitions", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.nn.util.viterbi_decode", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.data.vocabulary.Vocabulary.get_token_from_index", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.nn.util.get_lengths_from_binary_sequence_mask"], ["", "@", "overrides", "\n", "def", "decode", "(", "self", ",", "output_dict", ":", "Dict", "[", "str", ",", "torch", ".", "Tensor", "]", ")", "->", "Dict", "[", "str", ",", "torch", ".", "Tensor", "]", ":", "\n", "        ", "\"\"\"\n        Does constrained viterbi decoding on class probabilities output in :func:`forward`.  The\n        constraint simply specifies that the output tags must be a valid BIO sequence.  We add a\n        `\"tags\"` key to the dictionary with the result.\n\n        NOTE: First, we decode a BIO sequence on top of the wordpieces. This is important; viterbi\n        decoding produces low quality output if you decode on top of word representations directly,\n        because the model gets confused by the 'missing' positions (which is sensible as it is trained\n        to perform tagging on wordpieces, not words).\n\n        Secondly, it's important that the indices we use to recover words from the wordpieces are the\n        start_offsets (i.e offsets which correspond to using the first wordpiece of words which are\n        tokenized into multiple wordpieces) as otherwise, we might get an ill-formed BIO sequence\n        when we select out the word tags from the wordpiece tags. This happens in the case that a word\n        is split into multiple word pieces, and then we take the last tag of the word, which might\n        correspond to, e.g, I-V, which would not be allowed as it is not preceeded by a B tag.\n        \"\"\"", "\n", "all_predictions", "=", "output_dict", "[", "\"class_probabilities\"", "]", "\n", "sequence_lengths", "=", "get_lengths_from_binary_sequence_mask", "(", "\n", "output_dict", "[", "\"mask\"", "]", "\n", ")", ".", "data", ".", "tolist", "(", ")", "\n", "\n", "if", "all_predictions", ".", "dim", "(", ")", "==", "3", ":", "\n", "            ", "predictions_list", "=", "[", "\n", "all_predictions", "[", "i", "]", ".", "detach", "(", ")", ".", "cpu", "(", ")", "\n", "for", "i", "in", "range", "(", "all_predictions", ".", "size", "(", "0", ")", ")", "\n", "]", "\n", "", "else", ":", "\n", "            ", "predictions_list", "=", "[", "all_predictions", "]", "\n", "", "wordpiece_tags", "=", "[", "]", "\n", "word_tags", "=", "[", "]", "\n", "transition_matrix", "=", "self", ".", "get_viterbi_pairwise_potentials", "(", ")", "\n", "start_transitions", "=", "self", ".", "get_start_transitions", "(", ")", "\n", "# **************** Different ********************", "\n", "# We add in the offsets here so we can compute the un-wordpieced tags.", "\n", "for", "predictions", ",", "length", ",", "offsets", "in", "zip", "(", "\n", "predictions_list", ",", "sequence_lengths", ",", "output_dict", "[", "\"wordpiece_offsets\"", "]", "\n", ")", ":", "\n", "            ", "max_likelihood_sequence", ",", "_", "=", "viterbi_decode", "(", "\n", "predictions", "[", ":", "length", "]", ",", "\n", "transition_matrix", ",", "\n", "allowed_start_transitions", "=", "start_transitions", ",", "\n", ")", "\n", "tags", "=", "[", "\n", "self", ".", "vocab", ".", "get_token_from_index", "(", "x", ",", "namespace", "=", "\"labels\"", ")", "\n", "for", "x", "in", "max_likelihood_sequence", "\n", "]", "\n", "\n", "wordpiece_tags", ".", "append", "(", "tags", ")", "\n", "word_tags", ".", "append", "(", "[", "tags", "[", "i", "]", "for", "i", "in", "offsets", "]", ")", "\n", "", "output_dict", "[", "\"wordpiece_tags\"", "]", "=", "wordpiece_tags", "\n", "output_dict", "[", "\"tags\"", "]", "=", "word_tags", "\n", "return", "output_dict", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.models.srl_bert.SrlBert.get_metrics": [[238, 250], ["srl_bert.SrlBert.span_metric.get_metric", "srl_bert.SrlBert.items"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.metrics.sequence_accuracy.SequenceAccuracy.get_metric"], ["", "def", "get_metrics", "(", "self", ",", "reset", ":", "bool", "=", "False", ")", ":", "\n", "        ", "if", "self", ".", "ignore_span_metric", ":", "\n", "# Return an empty dictionary if ignoring the", "\n", "# span metric", "\n", "            ", "return", "{", "}", "\n", "\n", "", "else", ":", "\n", "            ", "metric_dict", "=", "self", ".", "span_metric", ".", "get_metric", "(", "reset", "=", "reset", ")", "\n", "\n", "# This can be a lot of metrics, as there are 3 per class.", "\n", "# we only really care about the overall metrics, so we filter for them here.", "\n", "return", "{", "x", ":", "y", "for", "x", ",", "y", "in", "metric_dict", ".", "items", "(", ")", "if", "\"overall\"", "in", "x", "}", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.models.srl_bert.SrlBert.get_viterbi_pairwise_potentials": [[251, 275], ["srl_bert.SrlBert.vocab.get_index_to_token_vocabulary", "len", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "srl_bert.SrlBert.items", "srl_bert.SrlBert.items", "float"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.data.vocabulary.Vocabulary.get_index_to_token_vocabulary"], ["", "", "def", "get_viterbi_pairwise_potentials", "(", "self", ")", ":", "\n", "        ", "\"\"\"\n        Generate a matrix of pairwise transition potentials for the BIO labels.\n        The only constraint implemented here is that I-XXX labels must be preceded\n        by either an identical I-XXX tag or a B-XXX tag. In order to achieve this\n        constraint, pairs of labels which do not satisfy this constraint have a\n        pairwise potential of -inf.\n\n        # Returns\n\n        transition_matrix : torch.Tensor\n            A (num_labels, num_labels) matrix of pairwise potentials.\n        \"\"\"", "\n", "all_labels", "=", "self", ".", "vocab", ".", "get_index_to_token_vocabulary", "(", "\"labels\"", ")", "\n", "num_labels", "=", "len", "(", "all_labels", ")", "\n", "transition_matrix", "=", "torch", ".", "zeros", "(", "[", "num_labels", ",", "num_labels", "]", ")", "\n", "\n", "for", "i", ",", "previous_label", "in", "all_labels", ".", "items", "(", ")", ":", "\n", "            ", "for", "j", ",", "label", "in", "all_labels", ".", "items", "(", ")", ":", "\n", "# I labels can only be preceded by themselves or", "\n", "# their corresponding B tag.", "\n", "                ", "if", "i", "!=", "j", "and", "label", "[", "0", "]", "==", "\"I\"", "and", "not", "previous_label", "==", "\"B\"", "+", "label", "[", "1", ":", "]", ":", "\n", "                    ", "transition_matrix", "[", "i", ",", "j", "]", "=", "float", "(", "\"-inf\"", ")", "\n", "", "", "", "return", "transition_matrix", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.models.srl_bert.SrlBert.get_start_transitions": [[276, 297], ["srl_bert.SrlBert.vocab.get_index_to_token_vocabulary", "len", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "srl_bert.SrlBert.items", "float"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.data.vocabulary.Vocabulary.get_index_to_token_vocabulary"], ["", "def", "get_start_transitions", "(", "self", ")", ":", "\n", "        ", "\"\"\"\n        In the BIO sequence, we cannot start the sequence with an I-XXX tag.\n        This transition sequence is passed to viterbi_decode to specify this constraint.\n\n        # Returns\n\n        start_transitions : torch.Tensor\n            The pairwise potentials between a START token and\n            the first token of the sequence.\n        \"\"\"", "\n", "all_labels", "=", "self", ".", "vocab", ".", "get_index_to_token_vocabulary", "(", "\"labels\"", ")", "\n", "num_labels", "=", "len", "(", "all_labels", ")", "\n", "\n", "start_transitions", "=", "torch", ".", "zeros", "(", "num_labels", ")", "\n", "\n", "for", "i", ",", "label", "in", "all_labels", ".", "items", "(", ")", ":", "\n", "            ", "if", "label", "[", "0", "]", "==", "\"I\"", ":", "\n", "                ", "start_transitions", "[", "i", "]", "=", "float", "(", "\"-inf\"", ")", "\n", "\n", "", "", "return", "start_transitions", "\n", "", "", ""]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.models.crf_tagger.CrfTagger.__init__": [[70, 161], ["allennlp.nn.InitializerApplicator", "allennlp.models.model.Model.__init__", "crf_tagger.CrfTagger.vocab.get_vocab_size", "allennlp.modules.TimeDistributed", "allennlp.modules.ConditionalRandomField", "allennlp.common.checks.check_dimensions_match", "initializer", "torch.nn.Dropout", "feedforward.get_output_dim", "crf_tagger.CrfTagger.encoder.get_output_dim", "torch.nn.modules.linear.Linear", "crf_tagger.CrfTagger.vocab.get_index_to_token_vocabulary", "allennlp.modules.conditional_random_field.allowed_transitions", "allennlp.training.metrics.CategoricalAccuracy", "allennlp.training.metrics.CategoricalAccuracy", "allennlp.training.metrics.SpanBasedF1Measure", "text_field_embedder.get_output_dim", "encoder.get_input_dim", "allennlp.common.checks.check_dimensions_match", "allennlp.common.checks.ConfigurationError", "allennlp.common.checks.ConfigurationError", "encoder.get_output_dim", "feedforward.get_input_dim"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.graph_util.plot.GraphPlot.__init__", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.data.vocabulary.Vocabulary.get_vocab_size", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.checks.check_dimensions_match", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.seq2seq_decoders.auto_regressive_seq_decoder.AutoRegressiveSeqDecoder.get_output_dim", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.seq2seq_decoders.auto_regressive_seq_decoder.AutoRegressiveSeqDecoder.get_output_dim", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.data.vocabulary.Vocabulary.get_index_to_token_vocabulary", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.modules.conditional_random_field.allowed_transitions", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.seq2seq_decoders.auto_regressive_seq_decoder.AutoRegressiveSeqDecoder.get_output_dim", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.seq2vec_encoders.cnn_highway_encoder.CnnHighwayEncoder.get_input_dim", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.checks.check_dimensions_match", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.seq2seq_decoders.auto_regressive_seq_decoder.AutoRegressiveSeqDecoder.get_output_dim", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.seq2vec_encoders.cnn_highway_encoder.CnnHighwayEncoder.get_input_dim"], ["def", "__init__", "(", "\n", "self", ",", "\n", "vocab", ":", "Vocabulary", ",", "\n", "text_field_embedder", ":", "TextFieldEmbedder", ",", "\n", "encoder", ":", "Seq2SeqEncoder", ",", "\n", "label_namespace", ":", "str", "=", "\"labels\"", ",", "\n", "feedforward", ":", "Optional", "[", "FeedForward", "]", "=", "None", ",", "\n", "label_encoding", ":", "Optional", "[", "str", "]", "=", "None", ",", "\n", "include_start_end_transitions", ":", "bool", "=", "True", ",", "\n", "constrain_crf_decoding", ":", "bool", "=", "None", ",", "\n", "calculate_span_f1", ":", "bool", "=", "None", ",", "\n", "dropout", ":", "Optional", "[", "float", "]", "=", "None", ",", "\n", "verbose_metrics", ":", "bool", "=", "False", ",", "\n", "initializer", ":", "InitializerApplicator", "=", "InitializerApplicator", "(", ")", ",", "\n", "top_k", ":", "int", "=", "1", ",", "\n", "**", "kwargs", ",", "\n", ")", "->", "None", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "vocab", ",", "**", "kwargs", ")", "\n", "\n", "self", ".", "label_namespace", "=", "label_namespace", "\n", "self", ".", "text_field_embedder", "=", "text_field_embedder", "\n", "self", ".", "num_tags", "=", "self", ".", "vocab", ".", "get_vocab_size", "(", "label_namespace", ")", "\n", "self", ".", "encoder", "=", "encoder", "\n", "self", ".", "top_k", "=", "top_k", "\n", "self", ".", "_verbose_metrics", "=", "verbose_metrics", "\n", "if", "dropout", ":", "\n", "            ", "self", ".", "dropout", "=", "torch", ".", "nn", ".", "Dropout", "(", "dropout", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "dropout", "=", "None", "\n", "", "self", ".", "_feedforward", "=", "feedforward", "\n", "\n", "if", "feedforward", "is", "not", "None", ":", "\n", "            ", "output_dim", "=", "feedforward", ".", "get_output_dim", "(", ")", "\n", "", "else", ":", "\n", "            ", "output_dim", "=", "self", ".", "encoder", ".", "get_output_dim", "(", ")", "\n", "", "self", ".", "tag_projection_layer", "=", "TimeDistributed", "(", "Linear", "(", "output_dim", ",", "self", ".", "num_tags", ")", ")", "\n", "\n", "# if  constrain_crf_decoding and calculate_span_f1 are not", "\n", "# provided, (i.e., they're None), set them to True", "\n", "# if label_encoding is provided and False if it isn't.", "\n", "if", "constrain_crf_decoding", "is", "None", ":", "\n", "            ", "constrain_crf_decoding", "=", "label_encoding", "is", "not", "None", "\n", "", "if", "calculate_span_f1", "is", "None", ":", "\n", "            ", "calculate_span_f1", "=", "label_encoding", "is", "not", "None", "\n", "\n", "", "self", ".", "label_encoding", "=", "label_encoding", "\n", "if", "constrain_crf_decoding", ":", "\n", "            ", "if", "not", "label_encoding", ":", "\n", "                ", "raise", "ConfigurationError", "(", "\n", "\"constrain_crf_decoding is True, but no label_encoding was specified.\"", "\n", ")", "\n", "", "labels", "=", "self", ".", "vocab", ".", "get_index_to_token_vocabulary", "(", "label_namespace", ")", "\n", "constraints", "=", "allowed_transitions", "(", "label_encoding", ",", "labels", ")", "\n", "", "else", ":", "\n", "            ", "constraints", "=", "None", "\n", "\n", "", "self", ".", "include_start_end_transitions", "=", "include_start_end_transitions", "\n", "self", ".", "crf", "=", "ConditionalRandomField", "(", "\n", "self", ".", "num_tags", ",", "\n", "constraints", ",", "\n", "include_start_end_transitions", "=", "include_start_end_transitions", ",", "\n", ")", "\n", "\n", "self", ".", "metrics", "=", "{", "\n", "\"accuracy\"", ":", "CategoricalAccuracy", "(", ")", ",", "\n", "\"accuracy3\"", ":", "CategoricalAccuracy", "(", "top_k", "=", "3", ")", ",", "\n", "}", "\n", "self", ".", "calculate_span_f1", "=", "calculate_span_f1", "\n", "if", "calculate_span_f1", ":", "\n", "            ", "if", "not", "label_encoding", ":", "\n", "                ", "raise", "ConfigurationError", "(", "\n", "\"calculate_span_f1 is True, but no label_encoding was specified.\"", "\n", ")", "\n", "", "self", ".", "_f1_metric", "=", "SpanBasedF1Measure", "(", "\n", "vocab", ",", "tag_namespace", "=", "label_namespace", ",", "label_encoding", "=", "label_encoding", "\n", ")", "\n", "\n", "", "check_dimensions_match", "(", "\n", "text_field_embedder", ".", "get_output_dim", "(", ")", ",", "\n", "encoder", ".", "get_input_dim", "(", ")", ",", "\n", "\"text field embedding dim\"", ",", "\n", "\"encoder input dim\"", ",", "\n", ")", "\n", "if", "feedforward", "is", "not", "None", ":", "\n", "            ", "check_dimensions_match", "(", "\n", "encoder", ".", "get_output_dim", "(", ")", ",", "\n", "feedforward", ".", "get_input_dim", "(", ")", ",", "\n", "\"encoder output dim\"", ",", "\n", "\"feedforward input dim\"", ",", "\n", ")", "\n", "", "initializer", "(", "self", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.models.crf_tagger.CrfTagger.forward": [[162, 247], ["crf_tagger.CrfTagger.text_field_embedder", "allennlp.get_text_field_mask", "crf_tagger.CrfTagger.encoder", "crf_tagger.CrfTagger.tag_projection_layer", "crf_tagger.CrfTagger.crf.viterbi_tags", "typing.cast", "crf_tagger.CrfTagger.dropout", "crf_tagger.CrfTagger.dropout", "crf_tagger.CrfTagger._feedforward", "crf_tagger.CrfTagger.crf", "enumerate", "crf_tagger.CrfTagger.metrics.values", "enumerate", "metric", "crf_tagger.CrfTagger._f1_metric", "allennlp.get_text_field_mask.float", "allennlp.get_text_field_mask.float"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.nn.util.get_text_field_mask", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.modules.conditional_random_field.ConditionalRandomField.viterbi_tags"], ["", "@", "overrides", "\n", "def", "forward", "(", "\n", "self", ",", "# type: ignore", "\n", "tokens", ":", "TextFieldTensors", ",", "\n", "tags", ":", "torch", ".", "LongTensor", "=", "None", ",", "\n", "metadata", ":", "List", "[", "Dict", "[", "str", ",", "Any", "]", "]", "=", "None", ",", "\n", "**", "kwargs", ",", "\n", ")", "->", "Dict", "[", "str", ",", "torch", ".", "Tensor", "]", ":", "\n", "\n", "        ", "\"\"\"\n        # Parameters\n\n        tokens : `TextFieldTensors`, required\n            The output of `TextField.as_array()`, which should typically be passed directly to a\n            `TextFieldEmbedder`. This output is a dictionary mapping keys to `TokenIndexer`\n            tensors.  At its most basic, using a `SingleIdTokenIndexer` this is : `{\"tokens\":\n            Tensor(batch_size, num_tokens)}`. This dictionary will have the same keys as were used\n            for the `TokenIndexers` when you created the `TextField` representing your\n            sequence.  The dictionary is designed to be passed directly to a `TextFieldEmbedder`,\n            which knows how to combine different word representations into a single vector per\n            token in your input.\n        tags : `torch.LongTensor`, optional (default = `None`)\n            A torch tensor representing the sequence of integer gold class labels of shape\n            `(batch_size, num_tokens)`.\n        metadata : `List[Dict[str, Any]]`, optional, (default = None)\n            metadata containg the original words in the sentence to be tagged under a 'words' key.\n\n        # Returns\n\n        An output dictionary consisting of:\n\n        logits : `torch.FloatTensor`\n            The logits that are the output of the `tag_projection_layer`\n        mask : `torch.LongTensor`\n            The text field mask for the input tokens\n        tags : `List[List[int]]`\n            The predicted tags using the Viterbi algorithm.\n        loss : `torch.FloatTensor`, optional\n            A scalar loss to be optimised. Only computed if gold label `tags` are provided.\n        \"\"\"", "\n", "embedded_text_input", "=", "self", ".", "text_field_embedder", "(", "tokens", ")", "\n", "mask", "=", "util", ".", "get_text_field_mask", "(", "tokens", ")", "\n", "\n", "if", "self", ".", "dropout", ":", "\n", "            ", "embedded_text_input", "=", "self", ".", "dropout", "(", "embedded_text_input", ")", "\n", "\n", "", "encoded_text", "=", "self", ".", "encoder", "(", "embedded_text_input", ",", "mask", ")", "\n", "\n", "if", "self", ".", "dropout", ":", "\n", "            ", "encoded_text", "=", "self", ".", "dropout", "(", "encoded_text", ")", "\n", "\n", "", "if", "self", ".", "_feedforward", "is", "not", "None", ":", "\n", "            ", "encoded_text", "=", "self", ".", "_feedforward", "(", "encoded_text", ")", "\n", "\n", "", "logits", "=", "self", ".", "tag_projection_layer", "(", "encoded_text", ")", "\n", "best_paths", "=", "self", ".", "crf", ".", "viterbi_tags", "(", "logits", ",", "mask", ",", "top_k", "=", "self", ".", "top_k", ")", "\n", "\n", "# Just get the top tags and ignore the scores.", "\n", "predicted_tags", "=", "cast", "(", "List", "[", "List", "[", "int", "]", "]", ",", "[", "x", "[", "0", "]", "[", "0", "]", "for", "x", "in", "best_paths", "]", ")", "\n", "\n", "output", "=", "{", "\"logits\"", ":", "logits", ",", "\"mask\"", ":", "mask", ",", "\"tags\"", ":", "predicted_tags", "}", "\n", "\n", "if", "self", ".", "top_k", ">", "1", ":", "\n", "            ", "output", "[", "\"top_k_tags\"", "]", "=", "best_paths", "\n", "\n", "", "if", "tags", "is", "not", "None", ":", "\n", "# Add negative log-likelihood as loss", "\n", "            ", "log_likelihood", "=", "self", ".", "crf", "(", "logits", ",", "tags", ",", "mask", ")", "\n", "\n", "output", "[", "\"loss\"", "]", "=", "-", "log_likelihood", "\n", "\n", "# Represent viterbi tags as \"class probabilities\" that we can", "\n", "# feed into the metrics", "\n", "class_probabilities", "=", "logits", "*", "0.0", "\n", "for", "i", ",", "instance_tags", "in", "enumerate", "(", "predicted_tags", ")", ":", "\n", "                ", "for", "j", ",", "tag_id", "in", "enumerate", "(", "instance_tags", ")", ":", "\n", "                    ", "class_probabilities", "[", "i", ",", "j", ",", "tag_id", "]", "=", "1", "\n", "\n", "", "", "for", "metric", "in", "self", ".", "metrics", ".", "values", "(", ")", ":", "\n", "                ", "metric", "(", "class_probabilities", ",", "tags", ",", "mask", ".", "float", "(", ")", ")", "\n", "", "if", "self", ".", "calculate_span_f1", ":", "\n", "                ", "self", ".", "_f1_metric", "(", "class_probabilities", ",", "tags", ",", "mask", ".", "float", "(", ")", ")", "\n", "", "", "if", "metadata", "is", "not", "None", ":", "\n", "            ", "output", "[", "\"words\"", "]", "=", "[", "x", "[", "\"words\"", "]", "for", "x", "in", "metadata", "]", "\n", "", "return", "output", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.models.crf_tagger.CrfTagger.decode": [[248, 276], ["crf_tagger.CrfTagger.decode.decode_tags"], "methods", ["None"], ["", "@", "overrides", "\n", "def", "decode", "(", "self", ",", "output_dict", ":", "Dict", "[", "str", ",", "torch", ".", "Tensor", "]", ")", "->", "Dict", "[", "str", ",", "torch", ".", "Tensor", "]", ":", "\n", "        ", "\"\"\"\n        Converts the tag ids to the actual tags.\n        `output_dict[\"tags\"]` is a list of lists of tag_ids,\n        so we use an ugly nested list comprehension.\n        \"\"\"", "\n", "\n", "def", "decode_tags", "(", "tags", ")", ":", "\n", "            ", "return", "[", "\n", "self", ".", "vocab", ".", "get_token_from_index", "(", "tag", ",", "namespace", "=", "self", ".", "label_namespace", ")", "\n", "for", "tag", "in", "tags", "\n", "]", "\n", "\n", "", "def", "decode_top_k_tags", "(", "top_k_tags", ")", ":", "\n", "            ", "return", "[", "\n", "{", "\"tags\"", ":", "decode_tags", "(", "scored_path", "[", "0", "]", ")", ",", "\"score\"", ":", "scored_path", "[", "1", "]", "}", "\n", "for", "scored_path", "in", "top_k_tags", "\n", "]", "\n", "\n", "", "output_dict", "[", "\"tags\"", "]", "=", "[", "decode_tags", "(", "t", ")", "for", "t", "in", "output_dict", "[", "\"tags\"", "]", "]", "\n", "\n", "if", "\"top_k_tags\"", "in", "output_dict", ":", "\n", "            ", "output_dict", "[", "\"top_k_tags\"", "]", "=", "[", "\n", "decode_top_k_tags", "(", "t", ")", "for", "t", "in", "output_dict", "[", "\"top_k_tags\"", "]", "\n", "]", "\n", "\n", "", "return", "output_dict", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.models.crf_tagger.CrfTagger.get_metrics": [[277, 293], ["metric.get_metric", "crf_tagger.CrfTagger._f1_metric.get_metric", "crf_tagger.CrfTagger.metrics.items", "metrics_to_return.update", "metrics_to_return.update", "crf_tagger.CrfTagger.items"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.metrics.sequence_accuracy.SequenceAccuracy.get_metric", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.metrics.sequence_accuracy.SequenceAccuracy.get_metric", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.metrics.conll_coref_scores.Scorer.update", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.metrics.conll_coref_scores.Scorer.update"], ["", "@", "overrides", "\n", "def", "get_metrics", "(", "self", ",", "reset", ":", "bool", "=", "False", ")", "->", "Dict", "[", "str", ",", "float", "]", ":", "\n", "        ", "metrics_to_return", "=", "{", "\n", "metric_name", ":", "metric", ".", "get_metric", "(", "reset", ")", "\n", "for", "metric_name", ",", "metric", "in", "self", ".", "metrics", ".", "items", "(", ")", "\n", "}", "\n", "\n", "if", "self", ".", "calculate_span_f1", ":", "\n", "            ", "f1_dict", "=", "self", ".", "_f1_metric", ".", "get_metric", "(", "reset", "=", "reset", ")", "\n", "if", "self", ".", "_verbose_metrics", ":", "\n", "                ", "metrics_to_return", ".", "update", "(", "f1_dict", ")", "\n", "", "else", ":", "\n", "                ", "metrics_to_return", ".", "update", "(", "\n", "{", "x", ":", "y", "for", "x", ",", "y", "in", "f1_dict", ".", "items", "(", ")", "if", "\"overall\"", "in", "x", "}", "\n", ")", "\n", "", "", "return", "metrics_to_return", "\n", "", "", ""]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.models.graph_parser.GraphParser.__init__": [[68, 147], ["allennlp.nn.InitializerApplicator", "allennlp.models.model.Model.__init__", "encoder.get_output_dim", "copy.deepcopy", "allennlp.modules.matrix_attention.bilinear_matrix_attention.BilinearMatrixAttention", "graph_parser.GraphParser.vocab.get_vocab_size", "copy.deepcopy", "allennlp.modules.matrix_attention.bilinear_matrix_attention.BilinearMatrixAttention", "allennlp.modules.InputVariationalDropout", "torch.nn.modules.Dropout", "text_field_embedder.get_output_dim", "allennlp.common.checks.check_dimensions_match", "allennlp.common.checks.check_dimensions_match", "allennlp.common.checks.check_dimensions_match", "allennlp.training.metrics.F1Measure", "torch.nn.BCEWithLogitsLoss", "torch.nn.CrossEntropyLoss", "initializer", "allennlp.common.checks.ConfigurationError", "allennlp.modules.FeedForward", "allennlp.modules.FeedForward", "pos_tag_embedding.get_output_dim", "encoder.get_input_dim", "graph_parser.GraphParser.head_tag_feedforward.get_output_dim", "graph_parser.GraphParser.head_arc_feedforward.get_output_dim", "allennlp.nn.Activation.by_name", "allennlp.nn.Activation.by_name"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.graph_util.plot.GraphPlot.__init__", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.seq2seq_decoders.auto_regressive_seq_decoder.AutoRegressiveSeqDecoder.get_output_dim", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.data.vocabulary.Vocabulary.get_vocab_size", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.seq2seq_decoders.auto_regressive_seq_decoder.AutoRegressiveSeqDecoder.get_output_dim", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.checks.check_dimensions_match", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.checks.check_dimensions_match", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.checks.check_dimensions_match", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.seq2seq_decoders.auto_regressive_seq_decoder.AutoRegressiveSeqDecoder.get_output_dim", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.seq2vec_encoders.cnn_highway_encoder.CnnHighwayEncoder.get_input_dim", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.seq2seq_decoders.auto_regressive_seq_decoder.AutoRegressiveSeqDecoder.get_output_dim", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.seq2seq_decoders.auto_regressive_seq_decoder.AutoRegressiveSeqDecoder.get_output_dim", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.registrable.Registrable.by_name", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.registrable.Registrable.by_name"], ["def", "__init__", "(", "\n", "self", ",", "\n", "vocab", ":", "Vocabulary", ",", "\n", "text_field_embedder", ":", "TextFieldEmbedder", ",", "\n", "encoder", ":", "Seq2SeqEncoder", ",", "\n", "tag_representation_dim", ":", "int", ",", "\n", "arc_representation_dim", ":", "int", ",", "\n", "tag_feedforward", ":", "FeedForward", "=", "None", ",", "\n", "arc_feedforward", ":", "FeedForward", "=", "None", ",", "\n", "pos_tag_embedding", ":", "Embedding", "=", "None", ",", "\n", "dropout", ":", "float", "=", "0.0", ",", "\n", "input_dropout", ":", "float", "=", "0.0", ",", "\n", "edge_prediction_threshold", ":", "float", "=", "0.5", ",", "\n", "initializer", ":", "InitializerApplicator", "=", "InitializerApplicator", "(", ")", ",", "\n", "**", "kwargs", ",", "\n", ")", "->", "None", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "vocab", ",", "**", "kwargs", ")", "\n", "\n", "self", ".", "text_field_embedder", "=", "text_field_embedder", "\n", "self", ".", "encoder", "=", "encoder", "\n", "self", ".", "edge_prediction_threshold", "=", "edge_prediction_threshold", "\n", "if", "not", "0", "<", "edge_prediction_threshold", "<", "1", ":", "\n", "            ", "raise", "ConfigurationError", "(", "\n", "f\"edge_prediction_threshold must be between \"", "\n", "f\"0 and 1 (exclusive) but found {edge_prediction_threshold}.\"", "\n", ")", "\n", "\n", "", "encoder_dim", "=", "encoder", ".", "get_output_dim", "(", ")", "\n", "\n", "self", ".", "head_arc_feedforward", "=", "arc_feedforward", "or", "FeedForward", "(", "\n", "encoder_dim", ",", "1", ",", "arc_representation_dim", ",", "Activation", ".", "by_name", "(", "\"elu\"", ")", "(", ")", "\n", ")", "\n", "self", ".", "child_arc_feedforward", "=", "copy", ".", "deepcopy", "(", "self", ".", "head_arc_feedforward", ")", "\n", "\n", "self", ".", "arc_attention", "=", "BilinearMatrixAttention", "(", "\n", "arc_representation_dim", ",", "arc_representation_dim", ",", "use_input_biases", "=", "True", "\n", ")", "\n", "\n", "num_labels", "=", "self", ".", "vocab", ".", "get_vocab_size", "(", "\"labels\"", ")", "\n", "self", ".", "head_tag_feedforward", "=", "tag_feedforward", "or", "FeedForward", "(", "\n", "encoder_dim", ",", "1", ",", "tag_representation_dim", ",", "Activation", ".", "by_name", "(", "\"elu\"", ")", "(", ")", "\n", ")", "\n", "self", ".", "child_tag_feedforward", "=", "copy", ".", "deepcopy", "(", "self", ".", "head_tag_feedforward", ")", "\n", "\n", "self", ".", "tag_bilinear", "=", "BilinearMatrixAttention", "(", "\n", "tag_representation_dim", ",", "tag_representation_dim", ",", "label_dim", "=", "num_labels", "\n", ")", "\n", "\n", "self", ".", "_pos_tag_embedding", "=", "pos_tag_embedding", "or", "None", "\n", "self", ".", "_dropout", "=", "InputVariationalDropout", "(", "dropout", ")", "\n", "self", ".", "_input_dropout", "=", "Dropout", "(", "input_dropout", ")", "\n", "\n", "representation_dim", "=", "text_field_embedder", ".", "get_output_dim", "(", ")", "\n", "if", "pos_tag_embedding", "is", "not", "None", ":", "\n", "            ", "representation_dim", "+=", "pos_tag_embedding", ".", "get_output_dim", "(", ")", "\n", "\n", "", "check_dimensions_match", "(", "\n", "representation_dim", ",", "\n", "encoder", ".", "get_input_dim", "(", ")", ",", "\n", "\"text field embedding dim\"", ",", "\n", "\"encoder input dim\"", ",", "\n", ")", "\n", "check_dimensions_match", "(", "\n", "tag_representation_dim", ",", "\n", "self", ".", "head_tag_feedforward", ".", "get_output_dim", "(", ")", ",", "\n", "\"tag representation dim\"", ",", "\n", "\"tag feedforward output dim\"", ",", "\n", ")", "\n", "check_dimensions_match", "(", "\n", "arc_representation_dim", ",", "\n", "self", ".", "head_arc_feedforward", ".", "get_output_dim", "(", ")", ",", "\n", "\"arc representation dim\"", ",", "\n", "\"arc feedforward output dim\"", ",", "\n", ")", "\n", "\n", "self", ".", "_unlabelled_f1", "=", "F1Measure", "(", "positive_label", "=", "1", ")", "\n", "self", ".", "_arc_loss", "=", "torch", ".", "nn", ".", "BCEWithLogitsLoss", "(", "reduction", "=", "\"none\"", ")", "\n", "self", ".", "_tag_loss", "=", "torch", ".", "nn", ".", "CrossEntropyLoss", "(", "reduction", "=", "\"none\"", ")", "\n", "initializer", "(", "self", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.models.graph_parser.GraphParser.forward": [[148, 254], ["graph_parser.GraphParser.text_field_embedder", "allennlp.nn.util.get_text_field_mask", "graph_parser.GraphParser._input_dropout", "graph_parser.GraphParser.encoder", "allennlp.nn.util.get_text_field_mask.float", "graph_parser.GraphParser._dropout", "graph_parser.GraphParser._dropout", "graph_parser.GraphParser._dropout", "graph_parser.GraphParser._dropout", "graph_parser.GraphParser._dropout", "graph_parser.GraphParser.arc_attention", "graph_parser.GraphParser.tag_bilinear", "arc_tag_logits.permute().contiguous.permute().contiguous.permute().contiguous", "graph_parser.GraphParser._greedy_decode", "graph_parser.GraphParser._pos_tag_embedding", "torch.cat", "graph_parser.GraphParser.head_arc_feedforward", "graph_parser.GraphParser.child_arc_feedforward", "graph_parser.GraphParser.head_tag_feedforward", "graph_parser.GraphParser.child_tag_feedforward", "minus_mask.unsqueeze", "graph_parser.GraphParser._construct_loss", "graph_parser.GraphParser._unlabelled_f1", "allennlp.common.checks.ConfigurationError", "arc_tag_logits.permute().contiguous.permute().contiguous.permute", "minus_mask.unsqueeze", "allennlp.nn.util.get_text_field_mask.float.unsqueeze", "allennlp.nn.util.get_text_field_mask.float.unsqueeze", "torch.stack"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.nn.util.get_text_field_mask", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.models.biaffine_dependency_parser.BiaffineDependencyParser._greedy_decode", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.models.biaffine_dependency_parser.BiaffineDependencyParser._construct_loss"], ["", "@", "overrides", "\n", "def", "forward", "(", "\n", "self", ",", "# type: ignore", "\n", "tokens", ":", "TextFieldTensors", ",", "\n", "pos_tags", ":", "torch", ".", "LongTensor", "=", "None", ",", "\n", "metadata", ":", "List", "[", "Dict", "[", "str", ",", "Any", "]", "]", "=", "None", ",", "\n", "arc_tags", ":", "torch", ".", "LongTensor", "=", "None", ",", "\n", ")", "->", "Dict", "[", "str", ",", "torch", ".", "Tensor", "]", ":", "\n", "\n", "        ", "\"\"\"\n        # Parameters\n\n        tokens : TextFieldTensors, required\n            The output of `TextField.as_array()`.\n        pos_tags : torch.LongTensor, optional (default = None)\n            The output of a `SequenceLabelField` containing POS tags.\n        metadata : List[Dict[str, Any]], optional (default = None)\n            A dictionary of metadata for each batch element which has keys:\n                tokens : `List[str]`, required.\n                    The original string tokens in the sentence.\n        arc_tags : torch.LongTensor, optional (default = None)\n            A torch tensor representing the sequence of integer indices denoting the parent of every\n            word in the dependency parse. Has shape `(batch_size, sequence_length, sequence_length)`.\n\n        # Returns\n\n        An output dictionary.\n        \"\"\"", "\n", "embedded_text_input", "=", "self", ".", "text_field_embedder", "(", "tokens", ")", "\n", "if", "pos_tags", "is", "not", "None", "and", "self", ".", "_pos_tag_embedding", "is", "not", "None", ":", "\n", "            ", "embedded_pos_tags", "=", "self", ".", "_pos_tag_embedding", "(", "pos_tags", ")", "\n", "embedded_text_input", "=", "torch", ".", "cat", "(", "\n", "[", "embedded_text_input", ",", "embedded_pos_tags", "]", ",", "-", "1", "\n", ")", "\n", "", "elif", "self", ".", "_pos_tag_embedding", "is", "not", "None", ":", "\n", "            ", "raise", "ConfigurationError", "(", "\n", "\"Model uses a POS embedding, but no POS tags were passed.\"", "\n", ")", "\n", "\n", "", "mask", "=", "get_text_field_mask", "(", "tokens", ")", "\n", "embedded_text_input", "=", "self", ".", "_input_dropout", "(", "embedded_text_input", ")", "\n", "encoded_text", "=", "self", ".", "encoder", "(", "embedded_text_input", ",", "mask", ")", "\n", "\n", "float_mask", "=", "mask", ".", "float", "(", ")", "\n", "encoded_text", "=", "self", ".", "_dropout", "(", "encoded_text", ")", "\n", "\n", "# shape (batch_size, sequence_length, arc_representation_dim)", "\n", "head_arc_representation", "=", "self", ".", "_dropout", "(", "self", ".", "head_arc_feedforward", "(", "encoded_text", ")", ")", "\n", "child_arc_representation", "=", "self", ".", "_dropout", "(", "\n", "self", ".", "child_arc_feedforward", "(", "encoded_text", ")", "\n", ")", "\n", "\n", "# shape (batch_size, sequence_length, tag_representation_dim)", "\n", "head_tag_representation", "=", "self", ".", "_dropout", "(", "self", ".", "head_tag_feedforward", "(", "encoded_text", ")", ")", "\n", "child_tag_representation", "=", "self", ".", "_dropout", "(", "\n", "self", ".", "child_tag_feedforward", "(", "encoded_text", ")", "\n", ")", "\n", "# shape (batch_size, sequence_length, sequence_length)", "\n", "arc_scores", "=", "self", ".", "arc_attention", "(", "\n", "head_arc_representation", ",", "child_arc_representation", "\n", ")", "\n", "# shape (batch_size, num_tags, sequence_length, sequence_length)", "\n", "arc_tag_logits", "=", "self", ".", "tag_bilinear", "(", "\n", "head_tag_representation", ",", "child_tag_representation", "\n", ")", "\n", "# Switch to (batch_size, sequence_length, sequence_length, num_tags)", "\n", "arc_tag_logits", "=", "arc_tag_logits", ".", "permute", "(", "0", ",", "2", ",", "3", ",", "1", ")", ".", "contiguous", "(", ")", "\n", "\n", "minus_inf", "=", "-", "1e8", "\n", "minus_mask", "=", "(", "1", "-", "float_mask", ")", "*", "minus_inf", "\n", "arc_scores", "=", "arc_scores", "+", "minus_mask", ".", "unsqueeze", "(", "2", ")", "+", "minus_mask", ".", "unsqueeze", "(", "1", ")", "\n", "\n", "arc_probs", ",", "arc_tag_probs", "=", "self", ".", "_greedy_decode", "(", "arc_scores", ",", "arc_tag_logits", ",", "mask", ")", "\n", "\n", "output_dict", "=", "{", "\n", "\"arc_probs\"", ":", "arc_probs", ",", "\n", "\"arc_tag_probs\"", ":", "arc_tag_probs", ",", "\n", "\"mask\"", ":", "mask", ",", "\n", "}", "\n", "\n", "if", "metadata", ":", "\n", "            ", "output_dict", "[", "\"tokens\"", "]", "=", "[", "meta", "[", "\"tokens\"", "]", "for", "meta", "in", "metadata", "]", "\n", "\n", "", "if", "arc_tags", "is", "not", "None", ":", "\n", "            ", "arc_nll", ",", "tag_nll", "=", "self", ".", "_construct_loss", "(", "\n", "arc_scores", "=", "arc_scores", ",", "\n", "arc_tag_logits", "=", "arc_tag_logits", ",", "\n", "arc_tags", "=", "arc_tags", ",", "\n", "mask", "=", "mask", ",", "\n", ")", "\n", "output_dict", "[", "\"loss\"", "]", "=", "arc_nll", "+", "tag_nll", "\n", "output_dict", "[", "\"arc_loss\"", "]", "=", "arc_nll", "\n", "output_dict", "[", "\"tag_loss\"", "]", "=", "tag_nll", "\n", "\n", "# Make the arc tags not have negative values anywhere", "\n", "# (by default, no edge is indicated with -1).", "\n", "arc_indices", "=", "(", "arc_tags", "!=", "-", "1", ")", ".", "float", "(", ")", "\n", "tag_mask", "=", "float_mask", ".", "unsqueeze", "(", "1", ")", "*", "float_mask", ".", "unsqueeze", "(", "2", ")", "\n", "one_minus_arc_probs", "=", "1", "-", "arc_probs", "\n", "# We stack scores here because the f1 measure expects a", "\n", "# distribution, rather than a single value.", "\n", "self", ".", "_unlabelled_f1", "(", "\n", "torch", ".", "stack", "(", "[", "one_minus_arc_probs", ",", "arc_probs", "]", ",", "-", "1", ")", ",", "arc_indices", ",", "tag_mask", "\n", ")", "\n", "\n", "", "return", "output_dict", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.models.graph_parser.GraphParser.decode": [[255, 282], ["output_dict[].cpu().detach().numpy", "output_dict[].cpu().detach().numpy", "allennlp.nn.util.get_lengths_from_binary_sequence_mask", "zip", "range", "arcs.append", "arc_tags.append", "output_dict[].cpu().detach", "output_dict[].cpu().detach", "range", "output_dict[].cpu", "output_dict[].cpu", "edges.append", "instance_arc_tag_probs[].argmax", "edge_tags.append", "graph_parser.GraphParser.vocab.get_token_from_index"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.nn.util.get_lengths_from_binary_sequence_mask", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.data.vocabulary.Vocabulary.get_token_from_index"], ["", "@", "overrides", "\n", "def", "decode", "(", "self", ",", "output_dict", ":", "Dict", "[", "str", ",", "torch", ".", "Tensor", "]", ")", "->", "Dict", "[", "str", ",", "torch", ".", "Tensor", "]", ":", "\n", "        ", "arc_tag_probs", "=", "output_dict", "[", "\"arc_tag_probs\"", "]", ".", "cpu", "(", ")", ".", "detach", "(", ")", ".", "numpy", "(", ")", "\n", "arc_probs", "=", "output_dict", "[", "\"arc_probs\"", "]", ".", "cpu", "(", ")", ".", "detach", "(", ")", ".", "numpy", "(", ")", "\n", "mask", "=", "output_dict", "[", "\"mask\"", "]", "\n", "lengths", "=", "get_lengths_from_binary_sequence_mask", "(", "mask", ")", "\n", "arcs", "=", "[", "]", "\n", "arc_tags", "=", "[", "]", "\n", "for", "instance_arc_probs", ",", "instance_arc_tag_probs", ",", "length", "in", "zip", "(", "\n", "arc_probs", ",", "arc_tag_probs", ",", "lengths", "\n", ")", ":", "\n", "\n", "            ", "arc_matrix", "=", "instance_arc_probs", ">", "self", ".", "edge_prediction_threshold", "\n", "edges", "=", "[", "]", "\n", "edge_tags", "=", "[", "]", "\n", "for", "i", "in", "range", "(", "length", ")", ":", "\n", "                ", "for", "j", "in", "range", "(", "length", ")", ":", "\n", "                    ", "if", "arc_matrix", "[", "i", ",", "j", "]", "==", "1", ":", "\n", "                        ", "edges", ".", "append", "(", "(", "i", ",", "j", ")", ")", "\n", "tag", "=", "instance_arc_tag_probs", "[", "i", ",", "j", "]", ".", "argmax", "(", "-", "1", ")", "\n", "edge_tags", ".", "append", "(", "self", ".", "vocab", ".", "get_token_from_index", "(", "tag", ",", "\"labels\"", ")", ")", "\n", "", "", "", "arcs", ".", "append", "(", "edges", ")", "\n", "arc_tags", ".", "append", "(", "edge_tags", ")", "\n", "\n", "", "output_dict", "[", "\"arcs\"", "]", "=", "arcs", "\n", "output_dict", "[", "\"arc_tags\"", "]", "=", "arc_tags", "\n", "return", "output_dict", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.models.graph_parser.GraphParser._construct_loss": [[283, 343], ["mask.float", "arc_tag_logits.size", "arc_tag_logits.view", "arc_tags.view", "tag_mask.sum", "mask.float.unsqueeze", "graph_parser.GraphParser._tag_loss().view", "arc_nll.sum", "tag_mask.sum.float", "tag_nll.sum", "tag_mask.sum.float", "graph_parser.GraphParser._arc_loss", "mask.float.unsqueeze", "mask.float.unsqueeze", "mask.float.unsqueeze", "graph_parser.GraphParser._tag_loss", "arc_tags.view.long"], "methods", ["None"], ["", "def", "_construct_loss", "(", "\n", "self", ",", "\n", "arc_scores", ":", "torch", ".", "Tensor", ",", "\n", "arc_tag_logits", ":", "torch", ".", "Tensor", ",", "\n", "arc_tags", ":", "torch", ".", "Tensor", ",", "\n", "mask", ":", "torch", ".", "Tensor", ",", "\n", ")", "->", "Tuple", "[", "torch", ".", "Tensor", ",", "torch", ".", "Tensor", "]", ":", "\n", "        ", "\"\"\"\n        Computes the arc and tag loss for an adjacency matrix.\n\n        # Parameters\n\n        arc_scores : `torch.Tensor`, required.\n            A tensor of shape (batch_size, sequence_length, sequence_length) used to generate a\n            binary classification decision for whether an edge is present between two words.\n        arc_tag_logits : `torch.Tensor`, required.\n            A tensor of shape (batch_size, sequence_length, sequence_length, num_tags) used to generate\n            a distribution over edge tags for a given edge.\n        arc_tags : `torch.Tensor`, required.\n            A tensor of shape (batch_size, sequence_length, sequence_length).\n            The labels for every arc.\n        mask : `torch.Tensor`, required.\n            A mask of shape (batch_size, sequence_length), denoting unpadded\n            elements in the sequence.\n\n        # Returns\n\n        arc_nll : `torch.Tensor`, required.\n            The negative log likelihood from the arc loss.\n        tag_nll : `torch.Tensor`, required.\n            The negative log likelihood from the arc tag loss.\n        \"\"\"", "\n", "float_mask", "=", "mask", ".", "float", "(", ")", "\n", "arc_indices", "=", "(", "arc_tags", "!=", "-", "1", ")", ".", "float", "(", ")", "\n", "# Make the arc tags not have negative values anywhere", "\n", "# (by default, no edge is indicated with -1).", "\n", "arc_tags", "=", "arc_tags", "*", "arc_indices", "\n", "arc_nll", "=", "(", "\n", "self", ".", "_arc_loss", "(", "arc_scores", ",", "arc_indices", ")", "\n", "*", "float_mask", ".", "unsqueeze", "(", "1", ")", "\n", "*", "float_mask", ".", "unsqueeze", "(", "2", ")", "\n", ")", "\n", "# We want the mask for the tags to only include the unmasked words", "\n", "# and we only care about the loss with respect to the gold arcs.", "\n", "tag_mask", "=", "float_mask", ".", "unsqueeze", "(", "1", ")", "*", "float_mask", ".", "unsqueeze", "(", "2", ")", "*", "arc_indices", "\n", "\n", "batch_size", ",", "sequence_length", ",", "_", ",", "num_tags", "=", "arc_tag_logits", ".", "size", "(", ")", "\n", "original_shape", "=", "[", "batch_size", ",", "sequence_length", ",", "sequence_length", "]", "\n", "reshaped_logits", "=", "arc_tag_logits", ".", "view", "(", "-", "1", ",", "num_tags", ")", "\n", "reshaped_tags", "=", "arc_tags", ".", "view", "(", "-", "1", ")", "\n", "tag_nll", "=", "(", "\n", "self", ".", "_tag_loss", "(", "reshaped_logits", ",", "reshaped_tags", ".", "long", "(", ")", ")", ".", "view", "(", "original_shape", ")", "\n", "*", "tag_mask", "\n", ")", "\n", "\n", "valid_positions", "=", "tag_mask", ".", "sum", "(", ")", "\n", "\n", "arc_nll", "=", "arc_nll", ".", "sum", "(", ")", "/", "valid_positions", ".", "float", "(", ")", "\n", "tag_nll", "=", "tag_nll", ".", "sum", "(", ")", "/", "valid_positions", ".", "float", "(", ")", "\n", "return", "arc_nll", ",", "tag_nll", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.models.graph_parser.GraphParser._greedy_decode": [[344, 387], ["torch.diag", "arc_scores.masked_fill_", "arc_tag_logits.masked_fill_", "arc_scores.sigmoid", "torch.nn.functional.softmax", "arc_scores.new().fill_", "torch.diag.unsqueeze().unsqueeze", "minus_mask.unsqueeze", "arc_scores.new", "torch.diag.unsqueeze", "mask.size"], "methods", ["None"], ["", "@", "staticmethod", "\n", "def", "_greedy_decode", "(", "\n", "arc_scores", ":", "torch", ".", "Tensor", ",", "arc_tag_logits", ":", "torch", ".", "Tensor", ",", "mask", ":", "torch", ".", "Tensor", "\n", ")", "->", "Tuple", "[", "torch", ".", "Tensor", ",", "torch", ".", "Tensor", "]", ":", "\n", "        ", "\"\"\"\n        Decodes the head and head tag predictions by decoding the unlabeled arcs\n        independently for each word and then again, predicting the head tags of\n        these greedily chosen arcs independently.\n\n        # Parameters\n\n        arc_scores : `torch.Tensor`, required.\n            A tensor of shape (batch_size, sequence_length, sequence_length) used to generate\n            a distribution over attachments of a given word to all other words.\n        arc_tag_logits : `torch.Tensor`, required.\n            A tensor of shape (batch_size, sequence_length, sequence_length, num_tags) used to\n            generate a distribution over tags for each arc.\n        mask : `torch.Tensor`, required.\n            A mask of shape (batch_size, sequence_length).\n\n        # Returns\n\n        arc_probs : `torch.Tensor`\n            A tensor of shape (batch_size, sequence_length, sequence_length) representing the\n            probability of an arc being present for this edge.\n        arc_tag_probs : `torch.Tensor`\n            A tensor of shape (batch_size, sequence_length, sequence_length, sequence_length)\n            representing the distribution over edge tags for a given edge.\n        \"\"\"", "\n", "# Mask the diagonal, because we don't self edges.", "\n", "inf_diagonal_mask", "=", "torch", ".", "diag", "(", "arc_scores", ".", "new", "(", "mask", ".", "size", "(", "1", ")", ")", ".", "fill_", "(", "-", "numpy", ".", "inf", ")", ")", "\n", "arc_scores", "=", "arc_scores", "+", "inf_diagonal_mask", "\n", "# shape (batch_size, sequence_length, sequence_length, num_tags)", "\n", "arc_tag_logits", "=", "arc_tag_logits", "+", "inf_diagonal_mask", ".", "unsqueeze", "(", "0", ")", ".", "unsqueeze", "(", "-", "1", ")", "\n", "# Mask padded tokens, because we only want to consider actual word -> word edges.", "\n", "minus_mask", "=", "(", "1", "-", "mask", ")", ".", "to", "(", "dtype", "=", "torch", ".", "bool", ")", ".", "unsqueeze", "(", "2", ")", "\n", "arc_scores", ".", "masked_fill_", "(", "minus_mask", ",", "-", "numpy", ".", "inf", ")", "\n", "arc_tag_logits", ".", "masked_fill_", "(", "minus_mask", ".", "unsqueeze", "(", "-", "1", ")", ",", "-", "numpy", ".", "inf", ")", "\n", "# shape (batch_size, sequence_length, sequence_length)", "\n", "arc_probs", "=", "arc_scores", ".", "sigmoid", "(", ")", "\n", "# shape (batch_size, sequence_length, sequence_length, num_tags)", "\n", "arc_tag_probs", "=", "torch", ".", "nn", ".", "functional", ".", "softmax", "(", "arc_tag_logits", ",", "dim", "=", "-", "1", ")", "\n", "return", "arc_probs", ",", "arc_tag_probs", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.models.graph_parser.GraphParser.get_metrics": [[388, 396], ["graph_parser.GraphParser._unlabelled_f1.get_metric"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.metrics.sequence_accuracy.SequenceAccuracy.get_metric"], ["", "@", "overrides", "\n", "def", "get_metrics", "(", "self", ",", "reset", ":", "bool", "=", "False", ")", "->", "Dict", "[", "str", ",", "float", "]", ":", "\n", "        ", "metrics", "=", "{", "}", "\n", "precision", ",", "recall", ",", "f1_measure", "=", "self", ".", "_unlabelled_f1", ".", "get_metric", "(", "reset", ")", "\n", "metrics", "[", "\"precision\"", "]", "=", "precision", "\n", "metrics", "[", "\"recall\"", "]", "=", "recall", "\n", "metrics", "[", "\"f1\"", "]", "=", "f1_measure", "\n", "return", "metrics", "\n", "", "", ""]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.models.bimpm.BiMpm.__init__": [[65, 122], ["allennlp.nn.InitializerApplicator", "allennlp.models.model.Model.__init__", "allennlp.common.checks.check_dimensions_match", "torch.nn.Dropout", "torch.nn.CrossEntropyLoss", "initializer", "bimpm.BiMpm.matcher_backward2.get_output_dim", "bimpm.BiMpm.aggregator.get_input_dim", "allennlp.training.metrics.CategoricalAccuracy", "bimpm.BiMpm.matcher_forward2.get_output_dim", "bimpm.BiMpm.matcher_backward1.get_output_dim", "bimpm.BiMpm.matcher_word.get_output_dim", "bimpm.BiMpm.matcher_forward1.get_output_dim"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.graph_util.plot.GraphPlot.__init__", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.checks.check_dimensions_match", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.seq2seq_decoders.auto_regressive_seq_decoder.AutoRegressiveSeqDecoder.get_output_dim", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.seq2vec_encoders.cnn_highway_encoder.CnnHighwayEncoder.get_input_dim", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.seq2seq_decoders.auto_regressive_seq_decoder.AutoRegressiveSeqDecoder.get_output_dim", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.seq2seq_decoders.auto_regressive_seq_decoder.AutoRegressiveSeqDecoder.get_output_dim", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.seq2seq_decoders.auto_regressive_seq_decoder.AutoRegressiveSeqDecoder.get_output_dim", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.seq2seq_decoders.auto_regressive_seq_decoder.AutoRegressiveSeqDecoder.get_output_dim"], ["def", "__init__", "(", "\n", "self", ",", "\n", "vocab", ":", "Vocabulary", ",", "\n", "text_field_embedder", ":", "TextFieldEmbedder", ",", "\n", "matcher_word", ":", "BiMpmMatching", ",", "\n", "encoder1", ":", "Seq2SeqEncoder", ",", "\n", "matcher_forward1", ":", "BiMpmMatching", ",", "\n", "matcher_backward1", ":", "BiMpmMatching", ",", "\n", "encoder2", ":", "Seq2SeqEncoder", ",", "\n", "matcher_forward2", ":", "BiMpmMatching", ",", "\n", "matcher_backward2", ":", "BiMpmMatching", ",", "\n", "aggregator", ":", "Seq2VecEncoder", ",", "\n", "classifier_feedforward", ":", "FeedForward", ",", "\n", "dropout", ":", "float", "=", "0.1", ",", "\n", "initializer", ":", "InitializerApplicator", "=", "InitializerApplicator", "(", ")", ",", "\n", "**", "kwargs", ",", "\n", ")", "->", "None", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "vocab", ",", "**", "kwargs", ")", "\n", "\n", "self", ".", "text_field_embedder", "=", "text_field_embedder", "\n", "\n", "self", ".", "matcher_word", "=", "matcher_word", "\n", "\n", "self", ".", "encoder1", "=", "encoder1", "\n", "self", ".", "matcher_forward1", "=", "matcher_forward1", "\n", "self", ".", "matcher_backward1", "=", "matcher_backward1", "\n", "\n", "self", ".", "encoder2", "=", "encoder2", "\n", "self", ".", "matcher_forward2", "=", "matcher_forward2", "\n", "self", ".", "matcher_backward2", "=", "matcher_backward2", "\n", "\n", "self", ".", "aggregator", "=", "aggregator", "\n", "\n", "matching_dim", "=", "(", "\n", "self", ".", "matcher_word", ".", "get_output_dim", "(", ")", "\n", "+", "self", ".", "matcher_forward1", ".", "get_output_dim", "(", ")", "\n", "+", "self", ".", "matcher_backward1", ".", "get_output_dim", "(", ")", "\n", "+", "self", ".", "matcher_forward2", ".", "get_output_dim", "(", ")", "\n", "+", "self", ".", "matcher_backward2", ".", "get_output_dim", "(", ")", "\n", ")", "\n", "\n", "check_dimensions_match", "(", "\n", "matching_dim", ",", "\n", "self", ".", "aggregator", ".", "get_input_dim", "(", ")", ",", "\n", "\"sum of dim of all matching layers\"", ",", "\n", "\"aggregator input dim\"", ",", "\n", ")", "\n", "\n", "self", ".", "classifier_feedforward", "=", "classifier_feedforward", "\n", "\n", "self", ".", "dropout", "=", "torch", ".", "nn", ".", "Dropout", "(", "dropout", ")", "\n", "\n", "self", ".", "metrics", "=", "{", "\"accuracy\"", ":", "CategoricalAccuracy", "(", ")", "}", "\n", "\n", "self", ".", "loss", "=", "torch", ".", "nn", ".", "CrossEntropyLoss", "(", ")", "\n", "\n", "initializer", "(", "self", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.models.bimpm.BiMpm.forward": [[123, 238], ["allennlp.nn.util.get_text_field_mask", "allennlp.nn.util.get_text_field_mask", "bimpm.BiMpm.dropout", "bimpm.BiMpm.dropout", "bimpm.BiMpm.dropout", "bimpm.BiMpm.dropout", "bimpm.BiMpm.dropout", "bimpm.BiMpm.dropout", "bimpm.BiMpm.forward.add_matching_result"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.nn.util.get_text_field_mask", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.nn.util.get_text_field_mask"], ["", "@", "overrides", "\n", "def", "forward", "(", "\n", "self", ",", "# type: ignore", "\n", "premise", ":", "TextFieldTensors", ",", "\n", "hypothesis", ":", "TextFieldTensors", ",", "\n", "label", ":", "torch", ".", "LongTensor", "=", "None", ",", "\n", "metadata", ":", "List", "[", "Dict", "[", "str", ",", "Any", "]", "]", "=", "None", ",", "\n", ")", "->", "Dict", "[", "str", ",", "torch", ".", "Tensor", "]", ":", "\n", "        ", "\"\"\"\n\n        # Parameters\n\n        premise : TextFieldTensors\n            The premise from a `TextField`\n        hypothesis : TextFieldTensors\n            The hypothesis from a `TextField`\n        label : torch.LongTensor, optional (default = None)\n            The label for the pair of the premise and the hypothesis\n        metadata : `List[Dict[str, Any]]`, optional, (default = None)\n            Additional information about the pair\n        # Returns\n\n        An output dictionary consisting of:\n\n        logits : torch.FloatTensor\n            A tensor of shape `(batch_size, num_labels)` representing unnormalised log\n            probabilities of the entailment label.\n        loss : torch.FloatTensor, optional\n            A scalar loss to be optimised.\n        \"\"\"", "\n", "\n", "mask_premise", "=", "util", ".", "get_text_field_mask", "(", "premise", ")", "\n", "mask_hypothesis", "=", "util", ".", "get_text_field_mask", "(", "hypothesis", ")", "\n", "\n", "# embedding and encoding of the premise", "\n", "embedded_premise", "=", "self", ".", "dropout", "(", "self", ".", "text_field_embedder", "(", "premise", ")", ")", "\n", "encoded_premise1", "=", "self", ".", "dropout", "(", "self", ".", "encoder1", "(", "embedded_premise", ",", "mask_premise", ")", ")", "\n", "encoded_premise2", "=", "self", ".", "dropout", "(", "self", ".", "encoder2", "(", "encoded_premise1", ",", "mask_premise", ")", ")", "\n", "\n", "# embedding and encoding of the hypothesis", "\n", "embedded_hypothesis", "=", "self", ".", "dropout", "(", "self", ".", "text_field_embedder", "(", "hypothesis", ")", ")", "\n", "encoded_hypothesis1", "=", "self", ".", "dropout", "(", "\n", "self", ".", "encoder1", "(", "embedded_hypothesis", ",", "mask_hypothesis", ")", "\n", ")", "\n", "encoded_hypothesis2", "=", "self", ".", "dropout", "(", "\n", "self", ".", "encoder2", "(", "encoded_hypothesis1", ",", "mask_hypothesis", ")", "\n", ")", "\n", "\n", "matching_vector_premise", ":", "List", "[", "torch", ".", "Tensor", "]", "=", "[", "]", "\n", "matching_vector_hypothesis", ":", "List", "[", "torch", ".", "Tensor", "]", "=", "[", "]", "\n", "\n", "def", "add_matching_result", "(", "matcher", ",", "encoded_premise", ",", "encoded_hypothesis", ")", ":", "\n", "# utility function to get matching result and add to the result list", "\n", "            ", "matching_result", "=", "matcher", "(", "\n", "encoded_premise", ",", "mask_premise", ",", "encoded_hypothesis", ",", "mask_hypothesis", "\n", ")", "\n", "matching_vector_premise", ".", "extend", "(", "matching_result", "[", "0", "]", ")", "\n", "matching_vector_hypothesis", ".", "extend", "(", "matching_result", "[", "1", "]", ")", "\n", "\n", "# calculate matching vectors from word embedding, first layer encoding, and second layer encoding", "\n", "", "add_matching_result", "(", "self", ".", "matcher_word", ",", "embedded_premise", ",", "embedded_hypothesis", ")", "\n", "half_hidden_size_1", "=", "self", ".", "encoder1", ".", "get_output_dim", "(", ")", "//", "2", "\n", "add_matching_result", "(", "\n", "self", ".", "matcher_forward1", ",", "\n", "encoded_premise1", "[", ":", ",", ":", ",", ":", "half_hidden_size_1", "]", ",", "\n", "encoded_hypothesis1", "[", ":", ",", ":", ",", ":", "half_hidden_size_1", "]", ",", "\n", ")", "\n", "add_matching_result", "(", "\n", "self", ".", "matcher_backward1", ",", "\n", "encoded_premise1", "[", ":", ",", ":", ",", "half_hidden_size_1", ":", "]", ",", "\n", "encoded_hypothesis1", "[", ":", ",", ":", ",", "half_hidden_size_1", ":", "]", ",", "\n", ")", "\n", "\n", "half_hidden_size_2", "=", "self", ".", "encoder2", ".", "get_output_dim", "(", ")", "//", "2", "\n", "add_matching_result", "(", "\n", "self", ".", "matcher_forward2", ",", "\n", "encoded_premise2", "[", ":", ",", ":", ",", ":", "half_hidden_size_2", "]", ",", "\n", "encoded_hypothesis2", "[", ":", ",", ":", ",", ":", "half_hidden_size_2", "]", ",", "\n", ")", "\n", "add_matching_result", "(", "\n", "self", ".", "matcher_backward2", ",", "\n", "encoded_premise2", "[", ":", ",", ":", ",", "half_hidden_size_2", ":", "]", ",", "\n", "encoded_hypothesis2", "[", ":", ",", ":", ",", "half_hidden_size_2", ":", "]", ",", "\n", ")", "\n", "\n", "# concat the matching vectors", "\n", "matching_vector_cat_premise", "=", "self", ".", "dropout", "(", "\n", "torch", ".", "cat", "(", "matching_vector_premise", ",", "dim", "=", "2", ")", "\n", ")", "\n", "matching_vector_cat_hypothesis", "=", "self", ".", "dropout", "(", "\n", "torch", ".", "cat", "(", "matching_vector_hypothesis", ",", "dim", "=", "2", ")", "\n", ")", "\n", "\n", "# aggregate the matching vectors", "\n", "aggregated_premise", "=", "self", ".", "dropout", "(", "\n", "self", ".", "aggregator", "(", "matching_vector_cat_premise", ",", "mask_premise", ")", "\n", ")", "\n", "aggregated_hypothesis", "=", "self", ".", "dropout", "(", "\n", "self", ".", "aggregator", "(", "matching_vector_cat_hypothesis", ",", "mask_hypothesis", ")", "\n", ")", "\n", "\n", "# the final forward layer", "\n", "logits", "=", "self", ".", "classifier_feedforward", "(", "\n", "torch", ".", "cat", "(", "[", "aggregated_premise", ",", "aggregated_hypothesis", "]", ",", "dim", "=", "-", "1", ")", "\n", ")", "\n", "probs", "=", "torch", ".", "nn", ".", "functional", ".", "softmax", "(", "logits", ",", "dim", "=", "-", "1", ")", "\n", "\n", "output_dict", "=", "{", "\"logits\"", ":", "logits", ",", "\"probs\"", ":", "probs", "}", "\n", "if", "label", "is", "not", "None", ":", "\n", "            ", "loss", "=", "self", ".", "loss", "(", "logits", ",", "label", ")", "\n", "for", "metric", "in", "self", ".", "metrics", ".", "values", "(", ")", ":", "\n", "                ", "metric", "(", "logits", ",", "label", ")", "\n", "", "output_dict", "[", "\"loss\"", "]", "=", "loss", "\n", "\n", "", "return", "output_dict", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.models.bimpm.BiMpm.decode": [[239, 252], ["output_dict[].cpu().data.numpy", "numpy.argmax", "bimpm.BiMpm.vocab.get_token_from_index", "output_dict[].cpu"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.data.vocabulary.Vocabulary.get_token_from_index"], ["", "@", "overrides", "\n", "def", "decode", "(", "self", ",", "output_dict", ":", "Dict", "[", "str", ",", "torch", ".", "Tensor", "]", ")", "->", "Dict", "[", "str", ",", "torch", ".", "Tensor", "]", ":", "\n", "        ", "\"\"\"\n        Converts indices to string labels, and adds a `\"label\"` key to the result.\n        \"\"\"", "\n", "predictions", "=", "output_dict", "[", "\"probs\"", "]", ".", "cpu", "(", ")", ".", "data", ".", "numpy", "(", ")", "\n", "argmax_indices", "=", "numpy", ".", "argmax", "(", "predictions", ",", "axis", "=", "-", "1", ")", "\n", "labels", "=", "[", "\n", "self", ".", "vocab", ".", "get_token_from_index", "(", "x", ",", "namespace", "=", "\"labels\"", ")", "\n", "for", "x", "in", "argmax_indices", "\n", "]", "\n", "output_dict", "[", "\"label\"", "]", "=", "labels", "\n", "return", "output_dict", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.models.bimpm.BiMpm.get_metrics": [[253, 258], ["metric.get_metric", "bimpm.BiMpm.metrics.items"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.metrics.sequence_accuracy.SequenceAccuracy.get_metric"], ["", "@", "overrides", "\n", "def", "get_metrics", "(", "self", ",", "reset", ":", "bool", "=", "False", ")", "->", "Dict", "[", "str", ",", "float", "]", ":", "\n", "        ", "return", "{", "\n", "metric_name", ":", "metric", ".", "get_metric", "(", "reset", ")", "\n", "for", "metric_name", ",", "metric", "in", "self", ".", "metrics", ".", "items", "(", ")", "\n", "}", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.models.event2mind.Event2Mind.__init__": [[60, 112], ["allennlp.models.model.Model.__init__", "torch.nn.Dropout", "torch.nn.Dropout", "event2mind.Event2Mind.vocab.get_token_index", "event2mind.Event2Mind.vocab.get_token_index", "event2mind.Event2Mind.vocab.get_vocab_size", "event2mind.Event2Mind._encoder.get_output_dim", "torch.nn.ModuleDict", "torch.nn.ModuleDict", "allennlp.nn.beam_search.BeamSearch", "event2mind.Event2Mind._source_embedder.get_output_dim", "event2mind.StateDecoder"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.graph_util.plot.GraphPlot.__init__", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.data.vocabulary.Vocabulary.get_token_index", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.data.vocabulary.Vocabulary.get_token_index", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.data.vocabulary.Vocabulary.get_vocab_size", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.seq2seq_decoders.auto_regressive_seq_decoder.AutoRegressiveSeqDecoder.get_output_dim", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.seq2seq_decoders.auto_regressive_seq_decoder.AutoRegressiveSeqDecoder.get_output_dim"], []], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.models.event2mind.Event2Mind._update_recall": [[114, 128], ["allennlp.nn.util.get_text_field_mask", "targets[].contiguous", "target_mask[].contiguous", "target_recall"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.nn.util.get_text_field_mask"], []], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.models.event2mind.Event2Mind._get_num_decoding_steps": [[130, 141], ["targets.size"], "methods", ["None"], []], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.models.event2mind.Event2Mind.forward": [[142, 216], ["event2mind.Event2Mind._embedding_dropout", "allennlp.nn.util.get_text_field_mask", "event2mind.Event2Mind._encoder", "event2mind.Event2Mind._source_embedder", "event2mind.Event2Mind._states.items", "event2mind.Event2Mind._states.items", "target_tokens.keys", "event2mind.Event2Mind._states.keys", "Exception", "event2mind.Event2Mind.greedy_search", "len", "event2mind.Event2Mind.size", "event2mind.Event2Mind.new_full", "event2mind.Event2Mind._beam_search.search", "target_tokens.keys", "event2mind.Event2Mind._states.keys", "event2mind.Event2Mind._states.keys", "target_tokens.keys", "event2mind.Event2Mind._update_recall"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.nn.util.get_text_field_mask", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.models.event2mind.Event2Mind.greedy_search", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.nn.beam_search.BeamSearch.search", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.models.event2mind.Event2Mind._update_recall"], []], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.models.event2mind.Event2Mind.greedy_search": [[217, 259], ["event2mind.Event2Mind._get_num_decoding_steps", "range", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "allennlp.nn.util.get_text_field_mask", "event2mind.Event2Mind._get_loss", "target_embedder", "decoder_cell", "output_projection_layer", "step_logits.append", "output_projection_layer.unsqueeze"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.models.event2mind.Event2Mind._get_num_decoding_steps", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.nn.util.get_text_field_mask", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.encoder_decoders.simple_seq2seq.SimpleSeq2Seq._get_loss"], []], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.models.event2mind.Event2Mind.greedy_predict": [[260, 302], ["range", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "final_encoder_output.size", "final_encoder_output.new_full", "target_embedder", "decoder_cell", "output_projection_layer", "torch.softmax", "torch.softmax", "torch.max", "torch.max", "torch.max", "torch.max", "predictions.append", "ps.unsqueeze"], "methods", ["None"], []], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.models.event2mind.Event2Mind._get_loss": [[303, 342], ["targets[].contiguous", "target_mask[].contiguous", "allennlp.nn.util.sequence_cross_entropy_with_logits"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.nn.util.sequence_cross_entropy_with_logits"], []], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.models.event2mind.Event2Mind.decode_all": [[343, 358], ["isinstance", "predicted_indices.detach().cpu().numpy.detach().cpu().numpy.detach().cpu().numpy", "list", "all_predicted_tokens.append", "event2mind.Event2Mind.vocab.get_token_from_index", "predicted_indices.detach().cpu().numpy.detach().cpu().numpy.detach().cpu", "list.index", "predicted_indices.detach().cpu().numpy.detach().cpu().numpy.detach"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.data.vocabulary.Vocabulary.get_token_from_index", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.fields.text_field.TextField.index"], []], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.models.event2mind.Event2Mind.decode": [[359, 378], ["event2mind.Event2Mind.decode_all"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.models.event2mind.Event2Mind.decode_all"], []], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.models.event2mind.Event2Mind.get_metrics": [[379, 387], ["event2mind.Event2Mind._states.items", "state.recall.get_metric"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.metrics.sequence_accuracy.SequenceAccuracy.get_metric"], []], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.models.event2mind.StateDecoder.__init__": [[395, 401], ["torch.nn.Module.__init__", "allennlp.modules.token_embedders.Embedding", "torch.nn.modules.rnn.GRUCell", "torch.nn.modules.rnn.GRUCell", "torch.nn.modules.linear.Linear", "torch.nn.modules.linear.Linear", "allennlp.training.metrics.UnigramRecall"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.graph_util.plot.GraphPlot.__init__"], []], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.models.event2mind.StateDecoder.take_step": [[402, 412], ["event2mind.StateDecoder.embedder", "event2mind.StateDecoder.decoder_cell", "event2mind.StateDecoder.output_projection_layer", "torch.log_softmax", "torch.log_softmax"], "methods", ["None"], []], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.models.model.Model.__init__": [[62, 68], ["super().__init__"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.graph_util.plot.GraphPlot.__init__"], ["def", "__init__", "(", "\n", "self", ",", "vocab", ":", "Vocabulary", ",", "regularizer", ":", "RegularizerApplicator", "=", "None", "\n", ")", "->", "None", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "vocab", "=", "vocab", "\n", "self", ".", "_regularizer", "=", "regularizer", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.models.model.Model.get_regularization_penalty": [[69, 78], ["model.Model._regularizer"], "methods", ["None"], ["", "def", "get_regularization_penalty", "(", "self", ")", "->", "Union", "[", "float", ",", "torch", ".", "Tensor", "]", ":", "\n", "        ", "\"\"\"\n        Computes the regularization penalty for the model.\n        Returns 0 if the model was not configured to use regularization.\n        \"\"\"", "\n", "if", "self", ".", "_regularizer", "is", "None", ":", "\n", "            ", "return", "0.0", "\n", "", "else", ":", "\n", "            ", "return", "self", ".", "_regularizer", "(", "self", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.models.model.Model.get_parameters_for_histogram_tensorboard_logging": [[79, 84], ["model.Model.named_parameters"], "methods", ["None"], ["", "", "def", "get_parameters_for_histogram_tensorboard_logging", "(", "self", ")", "->", "List", "[", "str", "]", ":", "\n", "        ", "\"\"\"\n        Returns the name of model parameters used for logging histograms to tensorboard.\n        \"\"\"", "\n", "return", "[", "name", "for", "name", ",", "_", "in", "self", ".", "named_parameters", "(", ")", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.models.model.Model.forward": [[85, 125], ["None"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "*", "inputs", ")", "->", "Dict", "[", "str", ",", "torch", ".", "Tensor", "]", ":", "\n", "        ", "\"\"\"\n        Defines the forward pass of the model. In addition, to facilitate easy training,\n        this method is designed to compute a loss function defined by a user.\n\n        The input is comprised of everything required to perform a\n        training update, `including` labels - you define the signature here!\n        It is down to the user to ensure that inference can be performed\n        without the presence of these labels. Hence, any inputs not available at\n        inference time should only be used inside a conditional block.\n\n        The intended sketch of this method is as follows::\n\n            def forward(self, input1, input2, targets=None):\n                ....\n                ....\n                output1 = self.layer1(input1)\n                output2 = self.layer2(input2)\n                output_dict = {\"output1\": output1, \"output2\": output2}\n                if targets is not None:\n                    # Function returning a scalar torch.Tensor, defined by the user.\n                    loss = self._compute_loss(output1, output2, targets)\n                    output_dict[\"loss\"] = loss\n                return output_dict\n\n        # Parameters\n\n        inputs:\n            Tensors comprising everything needed to perform a training update, `including` labels,\n            which should be optional (i.e have a default value of `None`).  At inference time,\n            simply pass the relevant inputs, not including the labels.\n\n        # Returns\n\n        output_dict : `Dict[str, torch.Tensor]`\n            The outputs from the model. In order to train a model using the\n            `Trainer` api, you must provide a \"loss\" key pointing to a\n            scalar `torch.Tensor` representing the loss to be optimized.\n        \"\"\"", "\n", "raise", "NotImplementedError", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.models.model.Model.forward_on_instance": [[126, 135], ["model.Model.forward_on_instances"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.models.model.Model.forward_on_instances"], ["", "def", "forward_on_instance", "(", "self", ",", "instance", ":", "Instance", ")", "->", "Dict", "[", "str", ",", "numpy", ".", "ndarray", "]", ":", "\n", "        ", "\"\"\"\n        Takes an [`Instance`](../data/instance.md), which typically has raw text in it,\n        converts that text into arrays using this model's [`Vocabulary`](../data/vocabulary.md),\n        passes those arrays through `self.forward()` and `self.decode()` (which by default does nothing)\n        and returns the result.  Before returning the result, we convert any\n        `torch.Tensors` into numpy arrays and remove the batch dimension.\n        \"\"\"", "\n", "return", "self", ".", "forward_on_instances", "(", "[", "instance", "]", ")", "[", "0", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.models.model.Model.forward_on_instances": [[136, 188], ["len", "torch.no_grad", "model.Model._get_prediction_device", "allennlp.data.batch.Batch", "allennlp.data.batch.Batch.index_instances", "allennlp.nn.util.move_to_device", "model.Model.decode", "list", "allennlp.data.batch.Batch.as_tensor_dict", "model.Model.", "model.Model.items", "isinstance", "zip", "output.unsqueeze.unsqueeze.detach().cpu().numpy", "output.unsqueeze.unsqueeze.dim", "output.unsqueeze.unsqueeze.unsqueeze", "output.unsqueeze.unsqueeze.size", "model.Model._maybe_warn_for_unseparable_batches", "len", "model.Model._maybe_warn_for_unseparable_batches", "output.unsqueeze.unsqueeze.detach().cpu", "output.unsqueeze.unsqueeze.detach"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.models.model.Model._get_prediction_device", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.data.batch.Batch.index_instances", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.nn.util.move_to_device", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.coreference_resolution.coref.CoreferenceResolver.decode", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.data.instance.Instance.as_tensor_dict", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.models.model.Model._maybe_warn_for_unseparable_batches", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.models.model.Model._maybe_warn_for_unseparable_batches"], ["", "def", "forward_on_instances", "(", "\n", "self", ",", "instances", ":", "List", "[", "Instance", "]", "\n", ")", "->", "List", "[", "Dict", "[", "str", ",", "numpy", ".", "ndarray", "]", "]", ":", "\n", "        ", "\"\"\"\n        Takes a list of `Instances`, converts that text into\n        arrays using this model's `Vocabulary`, passes those arrays through\n        `self.forward()` and `self.decode()` (which by default does nothing)\n        and returns the result.  Before returning the result, we convert any\n        `torch.Tensors` into numpy arrays and separate the\n        batched output into a list of individual dicts per instance. Note that typically\n        this will be faster on a GPU (and conditionally, on a CPU) than repeated calls to\n        `forward_on_instance`.\n\n        # Parameters\n\n        instances : List[Instance], required\n            The instances to run the model on.\n\n        # Returns\n\n        A list of the models output for each instance.\n        \"\"\"", "\n", "batch_size", "=", "len", "(", "instances", ")", "\n", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "            ", "cuda_device", "=", "self", ".", "_get_prediction_device", "(", ")", "\n", "dataset", "=", "Batch", "(", "instances", ")", "\n", "dataset", ".", "index_instances", "(", "self", ".", "vocab", ")", "\n", "model_input", "=", "util", ".", "move_to_device", "(", "dataset", ".", "as_tensor_dict", "(", ")", ",", "cuda_device", ")", "\n", "outputs", "=", "self", ".", "decode", "(", "self", "(", "**", "model_input", ")", ")", "\n", "\n", "instance_separated_output", ":", "List", "[", "Dict", "[", "str", ",", "numpy", ".", "ndarray", "]", "]", "=", "[", "\n", "{", "}", "for", "_", "in", "dataset", ".", "instances", "\n", "]", "\n", "for", "name", ",", "output", "in", "list", "(", "outputs", ".", "items", "(", ")", ")", ":", "\n", "                ", "if", "isinstance", "(", "output", ",", "torch", ".", "Tensor", ")", ":", "\n", "# NOTE(markn): This is a hack because 0-dim pytorch tensors are not iterable.", "\n", "# This occurs with batch size 1, because we still want to include the loss in that case.", "\n", "                    ", "if", "output", ".", "dim", "(", ")", "==", "0", ":", "\n", "                        ", "output", "=", "output", ".", "unsqueeze", "(", "0", ")", "\n", "\n", "", "if", "output", ".", "size", "(", "0", ")", "!=", "batch_size", ":", "\n", "                        ", "self", ".", "_maybe_warn_for_unseparable_batches", "(", "name", ")", "\n", "continue", "\n", "", "output", "=", "output", ".", "detach", "(", ")", ".", "cpu", "(", ")", ".", "numpy", "(", ")", "\n", "", "elif", "len", "(", "output", ")", "!=", "batch_size", ":", "\n", "                    ", "self", ".", "_maybe_warn_for_unseparable_batches", "(", "name", ")", "\n", "continue", "\n", "", "for", "instance_output", ",", "batch_element", "in", "zip", "(", "\n", "instance_separated_output", ",", "output", "\n", ")", ":", "\n", "                    ", "instance_output", "[", "name", "]", "=", "batch_element", "\n", "", "", "return", "instance_separated_output", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.models.model.Model.decode": [[189, 205], ["None"], "methods", ["None"], ["", "", "def", "decode", "(", "self", ",", "output_dict", ":", "Dict", "[", "str", ",", "torch", ".", "Tensor", "]", ")", "->", "Dict", "[", "str", ",", "torch", ".", "Tensor", "]", ":", "\n", "        ", "\"\"\"\n        Takes the result of `forward` and runs inference / decoding / whatever\n        post-processing you need to do your model.  The intent is that `model.forward()` should\n        produce potentials or probabilities, and then `model.decode()` can take those results and\n        run some kind of beam search or constrained inference or whatever is necessary.  This does\n        not handle all possible decoding use cases, but it at least handles simple kinds of\n        decoding.\n\n        This method `modifies` the input dictionary, and also `returns` the same dictionary.\n\n        By default in the base class we do nothing.  If your model has some special decoding step,\n        override this method.\n        \"\"\"", "\n", "\n", "return", "output_dict", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.models.model.Model.get_metrics": [[206, 219], ["None"], "methods", ["None"], ["", "def", "get_metrics", "(", "self", ",", "reset", ":", "bool", "=", "False", ")", "->", "Dict", "[", "str", ",", "float", "]", ":", "\n", "        ", "\"\"\"\n        Returns a dictionary of metrics. This method will be called by\n        `allennlp.training.Trainer` in order to compute and use model metrics for early\n        stopping and model serialization.  We return an empty dictionary here rather than raising\n        as it is not required to implement metrics for a new model.  A boolean `reset` parameter is\n        passed, as frequently a metric accumulator will have some state which should be reset\n        between epochs. This is also compatible with [`Metric`s](../training/metrics/metric.md). Metrics\n        should be populated during the call to `forward`, with the `Metric` handling the accumulation of\n        the metric until this method is called.\n        \"\"\"", "\n", "\n", "return", "{", "}", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.models.model.Model._get_prediction_device": [[220, 240], ["allennlp.nn.util.get_device_of", "len", "allennlp.common.checks.ConfigurationError", "model.Model.parameters", "len", "devices.pop", "str"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.nn.util.get_device_of", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.params.Params.pop"], ["", "def", "_get_prediction_device", "(", "self", ")", "->", "int", ":", "\n", "        ", "\"\"\"\n        This method checks the device of the model parameters to determine the cuda_device\n        this model should be run on for predictions.  If there are no parameters, it returns -1.\n\n        # Returns\n\n        The cuda device this model should run on for predictions.\n        \"\"\"", "\n", "devices", "=", "{", "util", ".", "get_device_of", "(", "param", ")", "for", "param", "in", "self", ".", "parameters", "(", ")", "}", "\n", "\n", "if", "len", "(", "devices", ")", ">", "1", ":", "\n", "            ", "devices_string", "=", "\", \"", ".", "join", "(", "str", "(", "x", ")", "for", "x", "in", "devices", ")", "\n", "raise", "ConfigurationError", "(", "\n", "f\"Parameters have mismatching cuda_devices: {devices_string}\"", "\n", ")", "\n", "", "elif", "len", "(", "devices", ")", "==", "1", ":", "\n", "            ", "return", "devices", ".", "pop", "(", ")", "\n", "", "else", ":", "\n", "            ", "return", "-", "1", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.models.model.Model._maybe_warn_for_unseparable_batches": [[241, 256], ["logger.warning", "model.Model._warn_for_unseparable_batches.add"], "methods", ["None"], ["", "", "def", "_maybe_warn_for_unseparable_batches", "(", "self", ",", "output_key", ":", "str", ")", ":", "\n", "        ", "\"\"\"\n        This method warns once if a user implements a model which returns a dictionary with\n        values which we are unable to split back up into elements of the batch. This is controlled\n        by a class attribute `_warn_for_unseperable_batches` because it would be extremely verbose\n        otherwise.\n        \"\"\"", "\n", "if", "output_key", "not", "in", "self", ".", "_warn_for_unseparable_batches", ":", "\n", "            ", "logger", ".", "warning", "(", "\n", "f\"Encountered the {output_key} key in the model's return dictionary which \"", "\n", "\"couldn't be split by the batch size. Key will be ignored.\"", "\n", ")", "\n", "# We only want to warn once for this key,", "\n", "# so we set this to false so we don't warn again.", "\n", "self", ".", "_warn_for_unseparable_batches", ".", "add", "(", "output_key", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.models.model.Model._load": [[257, 315], ["os.path.join", "config.get", "config.get.pop_choice", "allennlp.data.Vocabulary.resolve_class_name", "vocab_class.from_files", "config.get", "model.remove_pretrained_embedding_params", "Model.from_params", "Model.from_params.extend_embedder_vocab", "torch.load", "Model.from_params.load_state_dict", "os.path.join", "allennlp.common.params.Params", "allennlp.data.Vocabulary.list_available", "config.get.get", "config.get.get", "Model.from_params.cuda", "Model.from_params.cpu", "allennlp.nn.util.device_mapping"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.dataset_readers.multiprocess_dataset_reader.QIterable.join", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.params.Params.get", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.params.pop_choice", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.registrable.Registrable.resolve_class_name", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.data.vocabulary.Vocabulary.from_files", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.params.Params.get", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.models.model.remove_pretrained_embedding_params", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.tokenizers.tokenizer.Tokenizer.from_params", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.models.model.Model.extend_embedder_vocab", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.token_embedders.bert_token_embedder.PretrainedBertModel.load", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.learning_rate_schedulers.learning_rate_scheduler._PyTorchLearningRateSchedulerWrapper.load_state_dict", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.dataset_readers.multiprocess_dataset_reader.QIterable.join", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.registrable.Registrable.list_available", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.params.Params.get", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.params.Params.get", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.nn.util.device_mapping"], ["", "", "@", "classmethod", "\n", "def", "_load", "(", "\n", "cls", ",", "\n", "config", ":", "Params", ",", "\n", "serialization_dir", ":", "str", ",", "\n", "weights_file", ":", "str", "=", "None", ",", "\n", "cuda_device", ":", "int", "=", "-", "1", ",", "\n", ")", "->", "\"Model\"", ":", "\n", "        ", "\"\"\"\n        Instantiates an already-trained model, based on the experiment\n        configuration and some optional overrides.\n        \"\"\"", "\n", "weights_file", "=", "weights_file", "or", "os", ".", "path", ".", "join", "(", "serialization_dir", ",", "_DEFAULT_WEIGHTS", ")", "\n", "\n", "# Load vocabulary from file", "\n", "vocab_dir", "=", "os", ".", "path", ".", "join", "(", "serialization_dir", ",", "\"vocabulary\"", ")", "\n", "# If the config specifies a vocabulary subclass, we need to use it.", "\n", "vocab_params", "=", "config", ".", "get", "(", "\"vocabulary\"", ",", "Params", "(", "{", "}", ")", ")", "\n", "vocab_choice", "=", "vocab_params", ".", "pop_choice", "(", "\n", "\"type\"", ",", "Vocabulary", ".", "list_available", "(", ")", ",", "True", "\n", ")", "\n", "vocab_class", ",", "_", "=", "Vocabulary", ".", "resolve_class_name", "(", "vocab_choice", ")", "\n", "vocab", "=", "vocab_class", ".", "from_files", "(", "\n", "vocab_dir", ",", "\n", "vocab_params", ".", "get", "(", "\"padding_token\"", ",", "None", ")", ",", "\n", "vocab_params", ".", "get", "(", "\"oov_token\"", ",", "None", ")", ",", "\n", ")", "\n", "\n", "model_params", "=", "config", ".", "get", "(", "\"model\"", ")", "\n", "\n", "# The experiment config tells us how to _train_ a model, including where to get pre-trained", "\n", "# embeddings from.  We're now _loading_ the model, so those embeddings will already be", "\n", "# stored in our weights.  We don't need any pretrained weight file anymore, and we don't", "\n", "# want the code to look for it, so we remove it from the parameters here.", "\n", "remove_pretrained_embedding_params", "(", "model_params", ")", "\n", "model", "=", "Model", ".", "from_params", "(", "vocab", "=", "vocab", ",", "params", "=", "model_params", ")", "\n", "\n", "# If vocab+embedding extension was done, the model initialized from from_params", "\n", "# and one defined by state dict in weights_file might not have same embedding shapes.", "\n", "# Eg. when model embedder module was transferred along with vocab extension, the", "\n", "# initialized embedding weight shape would be smaller than one in the state_dict.", "\n", "# So calling model embedding extension is required before load_state_dict.", "\n", "# If vocab and model embeddings are in sync, following would be just a no-op.", "\n", "model", ".", "extend_embedder_vocab", "(", ")", "\n", "\n", "model_state", "=", "torch", ".", "load", "(", "\n", "weights_file", ",", "map_location", "=", "util", ".", "device_mapping", "(", "cuda_device", ")", "\n", ")", "\n", "model", ".", "load_state_dict", "(", "model_state", ")", "\n", "\n", "# Force model to cpu or gpu, as appropriate, to make sure that the embeddings are", "\n", "# in sync with the weights", "\n", "if", "cuda_device", ">=", "0", ":", "\n", "            ", "model", ".", "cuda", "(", "cuda_device", ")", "\n", "", "else", ":", "\n", "            ", "model", ".", "cpu", "(", ")", "\n", "\n", "", "return", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.models.model.Model.load": [[316, 364], ["cls.by_name", "model_class._load", "isinstance"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.registrable.Registrable.by_name", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.models.ensemble.Ensemble._load"], ["", "@", "classmethod", "\n", "def", "load", "(", "\n", "cls", ",", "\n", "config", ":", "Params", ",", "\n", "serialization_dir", ":", "str", ",", "\n", "weights_file", ":", "str", "=", "None", ",", "\n", "cuda_device", ":", "int", "=", "-", "1", ",", "\n", ")", "->", "\"Model\"", ":", "\n", "        ", "\"\"\"\n        Instantiates an already-trained model, based on the experiment\n        configuration and some optional overrides.\n\n        # Parameters\n\n        config: Params\n            The configuration that was used to train the model. It should definitely\n            have a `model` section, and should probably have a `trainer` section\n            as well.\n        serialization_dir: str = None\n            The directory containing the serialized weights, parameters, and vocabulary\n            of the model.\n        weights_file: str = None\n            By default we load the weights from `best.th` in the serialization\n            directory, but you can override that value here.\n        cuda_device: int = -1\n            By default we load the model on the CPU, but if you want to load it\n            for GPU usage you can specify the id of your GPU here\n\n\n        # Returns\n\n        model: Model\n            The model specified in the configuration, loaded with the serialized\n            vocabulary and the trained weights.\n        \"\"\"", "\n", "\n", "# Peak at the class of the model.", "\n", "model_type", "=", "(", "\n", "config", "[", "\"model\"", "]", "\n", "if", "isinstance", "(", "config", "[", "\"model\"", "]", ",", "str", ")", "\n", "else", "config", "[", "\"model\"", "]", "[", "\"type\"", "]", "\n", ")", "\n", "\n", "# Load using an overridable _load method.", "\n", "# This allows subclasses of Model to override _load.", "\n", "\n", "model_class", ":", "Type", "[", "Model", "]", "=", "cls", ".", "by_name", "(", "model_type", ")", "# type: ignore", "\n", "return", "model_class", ".", "_load", "(", "config", ",", "serialization_dir", ",", "weights_file", ",", "cuda_device", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.models.model.Model.extend_embedder_vocab": [[365, 394], ["model.Model.named_modules", "hasattr", "embedding_sources_mapping.get", "module.extend_vocab"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.params.Params.get", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.token_embedders.embedding.Embedding.extend_vocab"], ["", "def", "extend_embedder_vocab", "(", "\n", "self", ",", "embedding_sources_mapping", ":", "Dict", "[", "str", ",", "str", "]", "=", "None", "\n", ")", "->", "None", ":", "\n", "        ", "\"\"\"\n        Iterates through all embedding modules in the model and assures it can embed\n        with the extended vocab. This is required in fine-tuning or transfer learning\n        scenarios where model was trained with original vocabulary but during\n        fine-tuning/transfer-learning, it will have it work with extended vocabulary\n        (original + new-data vocabulary).\n\n        # Parameters\n\n        embedding_sources_mapping : Dict[str, str], (optional, default=None)\n            Mapping from model_path to pretrained-file path of the embedding\n            modules. If pretrained-file used at time of embedding initialization\n            isn't available now, user should pass this mapping. Model path is\n            path traversing the model attributes upto this embedding module.\n            Eg. \"_text_field_embedder.token_embedder_tokens\".\n        \"\"\"", "\n", "# self.named_modules() gives all sub-modules (including nested children)", "\n", "# The path nesting is already separated by \".\": eg. parent_module_name.child_module_name", "\n", "embedding_sources_mapping", "=", "embedding_sources_mapping", "or", "{", "}", "\n", "for", "model_path", ",", "module", "in", "self", ".", "named_modules", "(", ")", ":", "\n", "            ", "if", "hasattr", "(", "module", ",", "\"extend_vocab\"", ")", ":", "\n", "                ", "pretrained_file", "=", "embedding_sources_mapping", ".", "get", "(", "model_path", ",", "None", ")", "\n", "module", ".", "extend_vocab", "(", "\n", "self", ".", "vocab", ",", "\n", "extension_pretrained_file", "=", "pretrained_file", ",", "\n", "model_path", "=", "model_path", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.models.model.remove_pretrained_embedding_params": [[397, 405], ["isinstance", "params.keys", "params.values", "isinstance", "model.remove_pretrained_embedding_params"], "function", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.models.model.remove_pretrained_embedding_params"], ["", "", "", "", "def", "remove_pretrained_embedding_params", "(", "params", ":", "Params", ")", ":", "\n", "    ", "if", "isinstance", "(", "params", ",", "Params", ")", ":", "# The model could possible be a string, for example.", "\n", "        ", "keys", "=", "params", ".", "keys", "(", ")", "\n", "if", "\"pretrained_file\"", "in", "keys", ":", "\n", "            ", "del", "params", "[", "\"pretrained_file\"", "]", "\n", "", "for", "value", "in", "params", ".", "values", "(", ")", ":", "\n", "            ", "if", "isinstance", "(", "value", ",", "Params", ")", ":", "\n", "                ", "remove_pretrained_embedding_params", "(", "value", ")", "\n", "", "", "", "", ""]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.models.biaffine_dependency_parser.BiaffineDependencyParser.__init__": [[87, 175], ["allennlp.nn.InitializerApplicator", "allennlp.models.model.Model.__init__", "encoder.get_output_dim", "copy.deepcopy", "allennlp.modules.matrix_attention.bilinear_matrix_attention.BilinearMatrixAttention", "biaffine_dependency_parser.BiaffineDependencyParser.vocab.get_vocab_size", "copy.deepcopy", "torch.nn.modules.Bilinear", "torch.nn.modules.Bilinear", "torch.nn.modules.Bilinear", "torch.nn.modules.Bilinear", "allennlp.modules.InputVariationalDropout", "torch.nn.modules.Dropout", "torch.nn.modules.Dropout", "torch.nn.Parameter", "torch.nn.Parameter", "torch.nn.Parameter", "torch.nn.Parameter", "text_field_embedder.get_output_dim", "allennlp.common.checks.check_dimensions_match", "allennlp.common.checks.check_dimensions_match", "allennlp.common.checks.check_dimensions_match", "biaffine_dependency_parser.BiaffineDependencyParser.vocab.get_token_to_index_vocabulary", "set", "logger.info", "allennlp.training.metrics.AttachmentScores", "initializer", "allennlp.modules.FeedForward", "allennlp.modules.FeedForward", "torch.randn", "torch.randn", "torch.randn", "torch.randn", "pos_tag_embedding.get_output_dim", "encoder.get_input_dim", "biaffine_dependency_parser.BiaffineDependencyParser.head_tag_feedforward.get_output_dim", "biaffine_dependency_parser.BiaffineDependencyParser.head_arc_feedforward.get_output_dim", "punctuation_tag_indices.values", "biaffine_dependency_parser.BiaffineDependencyParser.items", "allennlp.nn.Activation.by_name", "allennlp.nn.Activation.by_name", "encoder.get_output_dim"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.graph_util.plot.GraphPlot.__init__", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.seq2seq_decoders.auto_regressive_seq_decoder.AutoRegressiveSeqDecoder.get_output_dim", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.data.vocabulary.Vocabulary.get_vocab_size", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.seq2seq_decoders.auto_regressive_seq_decoder.AutoRegressiveSeqDecoder.get_output_dim", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.checks.check_dimensions_match", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.checks.check_dimensions_match", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.checks.check_dimensions_match", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.data.vocabulary.Vocabulary.get_token_to_index_vocabulary", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.dataset_readers.multiprocess_dataset_reader.logger.info", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.seq2seq_decoders.auto_regressive_seq_decoder.AutoRegressiveSeqDecoder.get_output_dim", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.seq2vec_encoders.cnn_highway_encoder.CnnHighwayEncoder.get_input_dim", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.seq2seq_decoders.auto_regressive_seq_decoder.AutoRegressiveSeqDecoder.get_output_dim", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.seq2seq_decoders.auto_regressive_seq_decoder.AutoRegressiveSeqDecoder.get_output_dim", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.registrable.Registrable.by_name", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.registrable.Registrable.by_name", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.seq2seq_decoders.auto_regressive_seq_decoder.AutoRegressiveSeqDecoder.get_output_dim"], ["model", ":", "Model", ",", "\n", "dataset_reader", ":", "DatasetReader", ",", "\n", "language", ":", "str", "=", "\"en_core_web_sm\"", ",", "\n", ")", "->", "None", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "model", ",", "dataset_reader", ")", "\n", "# TODO(Mark) Make the language configurable and based on a model attribute.", "\n", "self", ".", "_tokenizer", "=", "SpacyTokenizer", "(", "language", "=", "language", ",", "pos_tags", "=", "True", ")", "\n", "\n", "", "def", "predict", "(", "self", ",", "sentence", ":", "str", ")", "->", "JsonDict", ":", "\n", "        ", "\"\"\"\n        Predict a dependency parse for the given sentence.\n        # Parameters\n\n        sentence The sentence to parse.\n\n        # Returns\n\n        A dictionary representation of the dependency tree.\n        \"\"\"", "\n", "return", "self", ".", "predict_json", "(", "{", "\"sentence\"", ":", "sentence", "}", ")", "\n", "\n", "", "@", "overrides", "\n", "def", "_json_to_instance", "(", "self", ",", "json_dict", ":", "JsonDict", ")", "->", "Instance", ":", "\n", "        ", "\"\"\"\n        Expects JSON that looks like `{\"sentence\": \"...\"}`.\n        \"\"\"", "\n", "spacy_tokens", "=", "self", ".", "_tokenizer", ".", "tokenize", "(", "json_dict", "[", "\"sentence\"", "]", ")", "\n", "sentence_text", "=", "[", "token", ".", "text", "for", "token", "in", "spacy_tokens", "]", "\n", "if", "self", ".", "_dataset_reader", ".", "use_language_specific_pos", ":", "# type: ignore", "\n", "# fine-grained part of speech", "\n", "            ", "pos_tags", "=", "[", "token", ".", "tag_", "for", "token", "in", "spacy_tokens", "]", "\n", "", "else", ":", "\n", "# coarse-grained part of speech (Universal Depdendencies format)", "\n", "            ", "pos_tags", "=", "[", "token", ".", "pos_", "for", "token", "in", "spacy_tokens", "]", "\n", "", "return", "self", ".", "_dataset_reader", ".", "text_to_instance", "(", "sentence_text", ",", "pos_tags", ")", "\n", "\n", "", "@", "overrides", "\n", "def", "predict_instance", "(", "self", ",", "instance", ":", "Instance", ")", "->", "JsonDict", ":", "\n", "        ", "outputs", "=", "self", ".", "_model", ".", "forward_on_instance", "(", "instance", ")", "\n", "\n", "words", "=", "outputs", "[", "\"words\"", "]", "\n", "pos", "=", "outputs", "[", "\"pos\"", "]", "\n", "heads", "=", "outputs", "[", "\"predicted_heads\"", "]", "\n", "tags", "=", "outputs", "[", "\"predicted_dependencies\"", "]", "\n", "outputs", "[", "\"hierplane_tree\"", "]", "=", "self", ".", "_build_hierplane_tree", "(", "words", ",", "heads", ",", "tags", ",", "pos", ")", "\n", "return", "sanitize", "(", "outputs", ")", "\n", "\n", "", "@", "overrides", "\n", "def", "predict_batch_instance", "(", "self", ",", "instances", ":", "List", "[", "Instance", "]", ")", "->", "List", "[", "JsonDict", "]", ":", "\n", "        ", "outputs", "=", "self", ".", "_model", ".", "forward_on_instances", "(", "instances", ")", "\n", "for", "output", "in", "outputs", ":", "\n", "            ", "words", "=", "output", "[", "\"words\"", "]", "\n", "pos", "=", "output", "[", "\"pos\"", "]", "\n", "heads", "=", "output", "[", "\"predicted_heads\"", "]", "\n", "tags", "=", "output", "[", "\"predicted_dependencies\"", "]", "\n", "output", "[", "\"hierplane_tree\"", "]", "=", "self", ".", "_build_hierplane_tree", "(", "\n", "words", ",", "heads", ",", "tags", ",", "pos", "\n", ")", "\n", "", "return", "sanitize", "(", "outputs", ")", "\n", "\n", "", "@", "staticmethod", "\n", "def", "_build_hierplane_tree", "(", "\n", "words", ":", "List", "[", "str", "]", ",", "heads", ":", "List", "[", "int", "]", ",", "tags", ":", "List", "[", "str", "]", ",", "pos", ":", "List", "[", "str", "]", "\n", ")", "->", "Dict", "[", "str", ",", "Any", "]", ":", "\n", "        ", "\"\"\"\n        # Returns\n\n        A JSON dictionary render-able by Hierplane for the given tree.\n        \"\"\"", "\n", "\n", "word_index_to_cumulative_indices", ":", "Dict", "[", "int", ",", "Tuple", "[", "int", ",", "int", "]", "]", "=", "{", "}", "\n", "cumulative_index", "=", "0", "\n", "for", "i", ",", "word", "in", "enumerate", "(", "words", ")", ":", "\n", "            ", "word_length", "=", "len", "(", "word", ")", "+", "1", "\n", "word_index_to_cumulative_indices", "[", "i", "]", "=", "(", "\n", "cumulative_index", ",", "\n", "cumulative_index", "+", "word_length", ",", "\n", ")", "\n", "cumulative_index", "+=", "word_length", "\n", "\n", "", "def", "node_constuctor", "(", "index", ":", "int", ")", ":", "\n", "            ", "children", "=", "[", "]", "\n", "for", "next_index", ",", "child", "in", "enumerate", "(", "heads", ")", ":", "\n", "                ", "if", "child", "==", "index", "+", "1", ":", "\n", "                    ", "children", ".", "append", "(", "node_constuctor", "(", "next_index", ")", ")", "\n", "\n", "# These are the icons which show up in the bottom right", "\n", "# corner of the node.", "\n", "", "", "attributes", "=", "[", "pos", "[", "index", "]", "]", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.models.biaffine_dependency_parser.BiaffineDependencyParser.forward": [[176, 279], ["biaffine_dependency_parser.BiaffineDependencyParser.text_field_embedder", "allennlp.nn.util.get_text_field_mask", "biaffine_dependency_parser.BiaffineDependencyParser._parse", "biaffine_dependency_parser.BiaffineDependencyParser._pos_tag_embedding", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "biaffine_dependency_parser.BiaffineDependencyParser._get_mask_for_eval", "biaffine_dependency_parser.BiaffineDependencyParser._attachment_scores", "allennlp.common.checks.ConfigurationError"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.nn.util.get_text_field_mask", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.models.biaffine_dependency_parser.BiaffineDependencyParser._parse", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.models.biaffine_dependency_parser.BiaffineDependencyParser._get_mask_for_eval"], ["start", ",", "end", "=", "word_index_to_cumulative_indices", "[", "index", "]", "\n", "\n", "hierplane_node", "=", "{", "\n", "\"word\"", ":", "words", "[", "index", "]", ",", "\n", "# The type of the node - all nodes with the same", "\n", "# type have a unified colour.", "\n", "\"nodeType\"", ":", "tags", "[", "index", "]", ",", "\n", "# Attributes of the node.", "\n", "\"attributes\"", ":", "attributes", ",", "\n", "# The link between  the node and it's parent.", "\n", "\"link\"", ":", "tags", "[", "index", "]", ",", "\n", "\"spans\"", ":", "[", "{", "\"start\"", ":", "start", ",", "\"end\"", ":", "end", "}", "]", ",", "\n", "}", "\n", "if", "children", ":", "\n", "                ", "hierplane_node", "[", "\"children\"", "]", "=", "children", "\n", "", "return", "hierplane_node", "\n", "\n", "# We are guaranteed that there is a single word pointing to", "\n", "# the root index, so we can find it just by searching for 0 in the list.", "\n", "", "root_index", "=", "heads", ".", "index", "(", "0", ")", "\n", "hierplane_tree", "=", "{", "\n", "\"text\"", ":", "\" \"", ".", "join", "(", "words", ")", ",", "\n", "\"root\"", ":", "node_constuctor", "(", "root_index", ")", ",", "\n", "\"nodeTypeToStyle\"", ":", "NODE_TYPE_TO_STYLE", ",", "\n", "\"linkToPosition\"", ":", "LINK_TO_POSITION", ",", "\n", "}", "\n", "return", "hierplane_tree", "\n", "", "", ""]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.models.biaffine_dependency_parser.BiaffineDependencyParser.decode": [[280, 302], ["output_dict.pop().cpu().detach().numpy", "output_dict.pop().cpu().detach().numpy", "output_dict.pop", "allennlp.nn.util.get_lengths_from_binary_sequence_mask", "zip", "list", "head_tag_labels.append", "head_indices.append", "output_dict.pop().cpu().detach", "output_dict.pop().cpu().detach", "biaffine_dependency_parser.BiaffineDependencyParser.vocab.get_token_from_index", "output_dict.pop().cpu", "output_dict.pop().cpu", "output_dict.pop", "output_dict.pop"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.params.Params.pop", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.nn.util.get_lengths_from_binary_sequence_mask", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.data.vocabulary.Vocabulary.get_token_from_index", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.params.Params.pop", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.params.Params.pop"], []], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.models.biaffine_dependency_parser.BiaffineDependencyParser._parse": [[303, 380], ["biaffine_dependency_parser.BiaffineDependencyParser._input_dropout", "biaffine_dependency_parser.BiaffineDependencyParser.encoder", "biaffine_dependency_parser.BiaffineDependencyParser.size", "biaffine_dependency_parser.BiaffineDependencyParser._head_sentinel.expand", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat.float", "torch.cat.float", "biaffine_dependency_parser.BiaffineDependencyParser._dropout", "biaffine_dependency_parser.BiaffineDependencyParser._dropout", "biaffine_dependency_parser.BiaffineDependencyParser._dropout", "biaffine_dependency_parser.BiaffineDependencyParser._dropout", "biaffine_dependency_parser.BiaffineDependencyParser._dropout", "biaffine_dependency_parser.BiaffineDependencyParser.arc_attention", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "biaffine_dependency_parser.BiaffineDependencyParser.head_arc_feedforward", "biaffine_dependency_parser.BiaffineDependencyParser.child_arc_feedforward", "biaffine_dependency_parser.BiaffineDependencyParser.head_tag_feedforward", "biaffine_dependency_parser.BiaffineDependencyParser.child_tag_feedforward", "minus_mask.unsqueeze", "biaffine_dependency_parser.BiaffineDependencyParser._greedy_decode", "biaffine_dependency_parser.BiaffineDependencyParser._mst_decode", "biaffine_dependency_parser.BiaffineDependencyParser._construct_loss", "biaffine_dependency_parser.BiaffineDependencyParser._construct_loss", "torch.cat.new_ones", "torch.cat.new_ones", "minus_mask.unsqueeze", "torch.cat.new_zeros", "torch.cat.new_zeros", "torch.cat.new_zeros", "torch.cat.new_zeros", "predicted_heads.long", "predicted_head_tags.long"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.models.biaffine_dependency_parser.BiaffineDependencyParser._greedy_decode", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.models.biaffine_dependency_parser.BiaffineDependencyParser._mst_decode", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.models.biaffine_dependency_parser.BiaffineDependencyParser._construct_loss", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.models.biaffine_dependency_parser.BiaffineDependencyParser._construct_loss"], []], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.models.biaffine_dependency_parser.BiaffineDependencyParser._construct_loss": [[381, 465], ["mask.float", "attended_arcs.size", "allennlp.nn.util.get_range_vector().unsqueeze", "biaffine_dependency_parser.BiaffineDependencyParser._get_head_tags", "allennlp.nn.util.get_range_vector", "allennlp.nn.util.get_range_vector.view().expand().long", "mask.float.unsqueeze", "allennlp.nn.util.masked_log_softmax", "mask.float.unsqueeze", "allennlp.nn.util.get_device_of", "mask.sum", "valid_positions.float", "valid_positions.float", "allennlp.nn.util.get_range_vector", "allennlp.nn.util.masked_log_softmax", "mask.float.unsqueeze", "mask.unsqueeze", "allennlp.nn.util.get_range_vector.view().expand", "arc_loss.sum", "tag_loss.sum", "allennlp.nn.util.get_device_of", "allennlp.nn.util.get_range_vector.view"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.models.biaffine_dependency_parser.BiaffineDependencyParser._get_head_tags", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.nn.util.get_range_vector", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.nn.util.masked_log_softmax", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.nn.util.get_device_of", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.nn.util.get_range_vector", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.nn.util.masked_log_softmax", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.nn.util.get_device_of"], []], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.models.biaffine_dependency_parser.BiaffineDependencyParser._greedy_decode": [[466, 523], ["attended_arcs.max", "biaffine_dependency_parser.BiaffineDependencyParser._get_head_tags", "biaffine_dependency_parser.BiaffineDependencyParser.max", "torch.diag", "torch.diag", "torch.diag", "torch.diag", "attended_arcs.masked_fill_", "attended_arcs.new().fill_", "attended_arcs.new", "mask.size"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.models.biaffine_dependency_parser.BiaffineDependencyParser._get_head_tags"], []], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.models.biaffine_dependency_parser.BiaffineDependencyParser._mst_decode": [[524, 614], ["head_tag_representation.expand().contiguous.expand().contiguous.size", "mask.data.sum().long().cpu().numpy", "head_tag_representation.expand().contiguous.expand().contiguous.unsqueeze", "head_tag_representation.expand().contiguous.expand().contiguous.expand().contiguous", "child_tag_representation.expand().contiguous.expand().contiguous.unsqueeze", "child_tag_representation.expand().contiguous.expand().contiguous.expand().contiguous", "biaffine_dependency_parser.BiaffineDependencyParser.tag_bilinear", "torch.log_softmax().permute", "torch.log_softmax().permute", "torch.log_softmax().transpose", "torch.log_softmax().transpose", "torch.exp", "torch.exp", "torch.exp", "torch.exp", "biaffine_dependency_parser.BiaffineDependencyParser._run_mst_decoding", "minus_mask.unsqueeze", "mask.data.sum().long().cpu", "head_tag_representation.expand().contiguous.expand().contiguous.expand", "child_tag_representation.expand().contiguous.expand().contiguous.expand", "torch.log_softmax", "torch.log_softmax", "mask.float", "minus_mask.unsqueeze", "torch.log_softmax", "torch.log_softmax", "torch.log_softmax().transpose.unsqueeze", "mask.data.sum().long", "mask.data.sum"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.models.biaffine_dependency_parser.BiaffineDependencyParser._run_mst_decoding"], []], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.models.biaffine_dependency_parser.BiaffineDependencyParser._run_mst_decoding": [[615, 646], ["zip", "batch_energy.detach().cpu", "energy.max", "allennlp.nn.chu_liu_edmonds.decode_mst", "enumerate", "heads.append", "head_tags.append", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "scores.numpy", "instance_head_tags.append", "numpy.stack", "numpy.stack", "batch_energy.detach", "tag_ids[].item"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.nn.chu_liu_edmonds.decode_mst"], []], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.models.biaffine_dependency_parser.BiaffineDependencyParser._get_head_tags": [[648, 705], ["head_tag_representation.size", "allennlp.nn.util.get_range_vector().unsqueeze", "selected_head_tag_representations.contiguous.contiguous.contiguous", "biaffine_dependency_parser.BiaffineDependencyParser.tag_bilinear", "allennlp.nn.util.get_range_vector", "allennlp.nn.util.get_device_of"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.nn.util.get_range_vector", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.nn.util.get_device_of"], []], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.models.biaffine_dependency_parser.BiaffineDependencyParser._get_mask_for_eval": [[706, 731], ["mask.detach", "pos_tags.eq().long", "pos_tags.eq"], "methods", ["None"], []], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.models.biaffine_dependency_parser.BiaffineDependencyParser.get_metrics": [[732, 735], ["biaffine_dependency_parser.BiaffineDependencyParser._attachment_scores.get_metric"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.metrics.sequence_accuracy.SequenceAccuracy.get_metric"], []], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.models.language_model._SoftmaxLoss.__init__": [[24, 34], ["super().__init__", "torch.nn.Parameter", "torch.nn.Parameter", "torch.zeros", "torch.randn", "numpy.sqrt"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.graph_util.plot.GraphPlot.__init__"], ["def", "__init__", "(", "self", ",", "num_words", ":", "int", ",", "embedding_dim", ":", "int", ")", "->", "None", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "\n", "# TODO(joelgrus): implement tie_embeddings (maybe)", "\n", "self", ".", "tie_embeddings", "=", "False", "\n", "\n", "self", ".", "softmax_w", "=", "torch", ".", "nn", ".", "Parameter", "(", "\n", "torch", ".", "randn", "(", "embedding_dim", ",", "num_words", ")", "/", "np", ".", "sqrt", "(", "embedding_dim", ")", "\n", ")", "\n", "self", ".", "softmax_b", "=", "torch", ".", "nn", ".", "Parameter", "(", "torch", ".", "zeros", "(", "num_words", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.models.language_model._SoftmaxLoss.forward": [[35, 45], ["torch.nn.functional.log_softmax", "torch.nn.functional.nll_loss", "targets.long", "torch.matmul"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "embeddings", ":", "torch", ".", "Tensor", ",", "targets", ":", "torch", ".", "Tensor", ")", "->", "torch", ".", "Tensor", ":", "\n", "\n", "# embeddings is size (n, embedding_dim)", "\n", "# targets is (batch_size, ) with the correct class id", "\n", "# Does not do any count normalization / divide by batch size", "\n", "        ", "probs", "=", "torch", ".", "nn", ".", "functional", ".", "log_softmax", "(", "\n", "torch", ".", "matmul", "(", "embeddings", ",", "self", ".", "softmax_w", ")", "+", "self", ".", "softmax_b", ",", "dim", "=", "-", "1", "\n", ")", "\n", "\n", "return", "torch", ".", "nn", ".", "functional", ".", "nll_loss", "(", "probs", ",", "targets", ".", "long", "(", ")", ",", "reduction", "=", "\"sum\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.models.language_model.LanguageModel.__init__": [[91, 149], ["allennlp.models.model.Model.__init__", "language_model.LanguageModel.register_buffer", "allennlp.training.metrics.Perplexity", "contextualizer.is_bidirectional", "allennlp.common.checks.ConfigurationError", "contextualizer.get_output_dim", "allennlp.modules.sampled_softmax_loss.SampledSoftmaxLoss", "language_model._SoftmaxLoss", "torch.zeros", "torch.nn.Dropout", "initializer", "contextualizer.get_output_dim", "vocab.get_vocab_size", "vocab.get_vocab_size", "contextualizer.is_bidirectional"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.graph_util.plot.GraphPlot.__init__", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.seq2seq_encoders.qanet_encoder.QaNetEncoderBlock.is_bidirectional", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.seq2seq_decoders.auto_regressive_seq_decoder.AutoRegressiveSeqDecoder.get_output_dim", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.seq2seq_decoders.auto_regressive_seq_decoder.AutoRegressiveSeqDecoder.get_output_dim", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.data.vocabulary.Vocabulary.get_vocab_size", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.data.vocabulary.Vocabulary.get_vocab_size", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.seq2seq_encoders.qanet_encoder.QaNetEncoderBlock.is_bidirectional"], ["def", "__init__", "(", "\n", "self", ",", "\n", "vocab", ":", "Vocabulary", ",", "\n", "text_field_embedder", ":", "TextFieldEmbedder", ",", "\n", "contextualizer", ":", "Seq2SeqEncoder", ",", "\n", "dropout", ":", "float", "=", "None", ",", "\n", "num_samples", ":", "int", "=", "None", ",", "\n", "sparse_embeddings", ":", "bool", "=", "False", ",", "\n", "bidirectional", ":", "bool", "=", "False", ",", "\n", "initializer", ":", "InitializerApplicator", "=", "None", ",", "\n", "**", "kwargs", ",", "\n", ")", "->", "None", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "vocab", ",", "**", "kwargs", ")", "\n", "self", ".", "_text_field_embedder", "=", "text_field_embedder", "\n", "\n", "if", "contextualizer", ".", "is_bidirectional", "(", ")", "is", "not", "bidirectional", ":", "\n", "            ", "raise", "ConfigurationError", "(", "\n", "\"Bidirectionality of contextualizer must match bidirectionality of \"", "\n", "\"language model. \"", "\n", "f\"Contextualizer bidirectional: {contextualizer.is_bidirectional()}, \"", "\n", "f\"language model bidirectional: {bidirectional}\"", "\n", ")", "\n", "\n", "", "self", ".", "_contextualizer", "=", "contextualizer", "\n", "self", ".", "_bidirectional", "=", "bidirectional", "\n", "\n", "# The dimension for making predictions just in the forward", "\n", "# (or backward) direction.", "\n", "if", "self", ".", "_bidirectional", ":", "\n", "            ", "self", ".", "_forward_dim", "=", "contextualizer", ".", "get_output_dim", "(", ")", "//", "2", "\n", "", "else", ":", "\n", "            ", "self", ".", "_forward_dim", "=", "contextualizer", ".", "get_output_dim", "(", ")", "\n", "\n", "# TODO(joelgrus): more sampled softmax configuration options, as needed.", "\n", "", "if", "num_samples", "is", "not", "None", ":", "\n", "            ", "self", ".", "_softmax_loss", "=", "SampledSoftmaxLoss", "(", "\n", "num_words", "=", "vocab", ".", "get_vocab_size", "(", ")", ",", "\n", "embedding_dim", "=", "self", ".", "_forward_dim", ",", "\n", "num_samples", "=", "num_samples", ",", "\n", "sparse", "=", "sparse_embeddings", ",", "\n", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "_softmax_loss", "=", "_SoftmaxLoss", "(", "\n", "num_words", "=", "vocab", ".", "get_vocab_size", "(", ")", ",", "embedding_dim", "=", "self", ".", "_forward_dim", "\n", ")", "\n", "\n", "# This buffer is now unused and exists only for backwards compatibility reasons.", "\n", "", "self", ".", "register_buffer", "(", "\"_last_average_loss\"", ",", "torch", ".", "zeros", "(", "1", ")", ")", "\n", "\n", "self", ".", "_perplexity", "=", "Perplexity", "(", ")", "\n", "\n", "if", "dropout", ":", "\n", "            ", "self", ".", "_dropout", "=", "torch", ".", "nn", ".", "Dropout", "(", "dropout", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "_dropout", "=", "lambda", "x", ":", "x", "\n", "\n", "", "if", "initializer", "is", "not", "None", ":", "\n", "            ", "initializer", "(", "self", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.models.language_model.LanguageModel._get_target_token_embeddings": [[150, 162], ["token_embeddings.new_zeros().to", "token_embeddings.masked_select().view", "torch.cat", "torch.cat", "token_embeddings.new_zeros", "token_embeddings.masked_select", "mask.size", "torch.cat.unsqueeze"], "methods", ["None"], ["", "", "def", "_get_target_token_embeddings", "(", "\n", "self", ",", "token_embeddings", ":", "torch", ".", "Tensor", ",", "mask", ":", "torch", ".", "Tensor", ",", "direction", ":", "int", "\n", ")", "->", "torch", ".", "Tensor", ":", "\n", "# Need to shift the mask in the correct direction", "\n", "        ", "zero_col", "=", "token_embeddings", ".", "new_zeros", "(", "mask", ".", "size", "(", "0", ")", ",", "1", ")", ".", "to", "(", "dtype", "=", "torch", ".", "bool", ")", "\n", "if", "direction", "==", "0", ":", "\n", "# forward direction, get token to right", "\n", "            ", "shifted_mask", "=", "torch", ".", "cat", "(", "[", "zero_col", ",", "mask", "[", ":", ",", "0", ":", "-", "1", "]", "]", ",", "dim", "=", "1", ")", "\n", "", "else", ":", "\n", "            ", "shifted_mask", "=", "torch", ".", "cat", "(", "[", "mask", "[", ":", ",", "1", ":", "]", ",", "zero_col", "]", ",", "dim", "=", "1", ")", "\n", "", "return", "token_embeddings", ".", "masked_select", "(", "shifted_mask", ".", "unsqueeze", "(", "-", "1", ")", ")", ".", "view", "(", "\n", "-", "1", ",", "self", ".", "_forward_dim", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.models.language_model.LanguageModel._compute_loss": [[164, 188], ["language_model.LanguageModel._loss_helper", "lm_embeddings.chunk", "language_model.LanguageModel._loss_helper"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.models.language_model.LanguageModel._loss_helper", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.models.language_model.LanguageModel._loss_helper"], ["", "def", "_compute_loss", "(", "\n", "self", ",", "\n", "lm_embeddings", ":", "torch", ".", "Tensor", ",", "\n", "token_embeddings", ":", "torch", ".", "Tensor", ",", "\n", "forward_targets", ":", "torch", ".", "Tensor", ",", "\n", "backward_targets", ":", "torch", ".", "Tensor", "=", "None", ",", "\n", ")", "->", "Tuple", "[", "torch", ".", "Tensor", ",", "torch", ".", "Tensor", "]", ":", "\n", "# If bidirectional, lm_embeddings is shape (batch_size, timesteps, dim * 2)", "\n", "# If unidirectional, lm_embeddings is shape (batch_size, timesteps, dim)", "\n", "# forward_targets, backward_targets (None in the unidirectional case) are", "\n", "# shape (batch_size, timesteps) masked with 0", "\n", "        ", "if", "self", ".", "_bidirectional", ":", "\n", "            ", "forward_embeddings", ",", "backward_embeddings", "=", "lm_embeddings", ".", "chunk", "(", "2", ",", "-", "1", ")", "\n", "backward_loss", "=", "self", ".", "_loss_helper", "(", "\n", "1", ",", "backward_embeddings", ",", "backward_targets", ",", "token_embeddings", "\n", ")", "\n", "", "else", ":", "\n", "            ", "forward_embeddings", "=", "lm_embeddings", "\n", "backward_loss", "=", "None", "\n", "\n", "", "forward_loss", "=", "self", ".", "_loss_helper", "(", "\n", "0", ",", "forward_embeddings", ",", "forward_targets", ",", "token_embeddings", "\n", ")", "\n", "return", "forward_loss", ",", "backward_loss", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.models.language_model.LanguageModel._loss_helper": [[189, 226], ["direction_embeddings.masked_select().view", "direction_targets.masked_select", "language_model.LanguageModel._softmax_loss", "NotImplementedError", "language_model.LanguageModel._get_target_token_embeddings", "language_model.LanguageModel._softmax", "direction_embeddings.masked_select", "mask.unsqueeze"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.models.language_model.LanguageModel._get_target_token_embeddings"], ["", "def", "_loss_helper", "(", "\n", "self", ",", "\n", "direction", ":", "int", ",", "\n", "direction_embeddings", ":", "torch", ".", "Tensor", ",", "\n", "direction_targets", ":", "torch", ".", "Tensor", ",", "\n", "token_embeddings", ":", "torch", ".", "Tensor", ",", "\n", ")", "->", "Tuple", "[", "int", ",", "int", "]", ":", "\n", "        ", "mask", "=", "direction_targets", ">", "0", "\n", "# we need to subtract 1 to undo the padding id since the softmax", "\n", "# does not include a padding dimension", "\n", "\n", "# shape (batch_size * timesteps, )", "\n", "non_masked_targets", "=", "direction_targets", ".", "masked_select", "(", "mask", ")", "-", "1", "\n", "\n", "# shape (batch_size * timesteps, embedding_dim)", "\n", "non_masked_embeddings", "=", "direction_embeddings", ".", "masked_select", "(", "\n", "mask", ".", "unsqueeze", "(", "-", "1", ")", "\n", ")", ".", "view", "(", "-", "1", ",", "self", ".", "_forward_dim", ")", "\n", "# note: need to return average loss across forward and backward", "\n", "# directions, but total sum loss across all batches.", "\n", "# Assuming batches include full sentences, forward and backward", "\n", "# directions have the same number of samples, so sum up loss", "\n", "# here then divide by 2 just below", "\n", "if", "not", "self", ".", "_softmax_loss", ".", "tie_embeddings", "or", "not", "self", ".", "_use_character_inputs", ":", "\n", "            ", "return", "self", ".", "_softmax_loss", "(", "non_masked_embeddings", ",", "non_masked_targets", ")", "\n", "", "else", ":", "\n", "# we also need the token embeddings corresponding to the", "\n", "# the targets", "\n", "            ", "raise", "NotImplementedError", "(", "\n", "\"This requires SampledSoftmaxLoss, which isn't implemented yet.\"", "\n", ")", "\n", "\n", "non_masked_token_embeddings", "=", "self", ".", "_get_target_token_embeddings", "(", "\n", "token_embeddings", ",", "mask", ",", "direction", "\n", ")", "\n", "return", "self", ".", "_softmax", "(", "\n", "non_masked_embeddings", ",", "non_masked_targets", ",", "non_masked_token_embeddings", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.models.language_model.LanguageModel.delete_softmax": [[228, 234], ["None"], "methods", ["None"], ["", "", "def", "delete_softmax", "(", "self", ")", "->", "None", ":", "\n", "        ", "\"\"\"\n        Remove the softmax weights. Useful for saving memory when calculating the loss\n        is not necessary, e.g. in an embedder.\n        \"\"\"", "\n", "self", ".", "_softmax_loss", "=", "None", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.models.language_model.LanguageModel.num_layers": [[235, 246], ["hasattr", "NotImplementedError", "type"], "methods", ["None"], ["", "def", "num_layers", "(", "self", ")", "->", "int", ":", "\n", "        ", "\"\"\"\n        Returns the depth of this LM. That is, how many layers the contextualizer has plus one for\n        the non-contextual layer.\n        \"\"\"", "\n", "if", "hasattr", "(", "self", ".", "_contextualizer", ",", "\"num_layers\"", ")", ":", "\n", "            ", "return", "self", ".", "_contextualizer", ".", "num_layers", "+", "1", "\n", "", "else", ":", "\n", "            ", "raise", "NotImplementedError", "(", "\n", "f\"Contextualizer of type {type(self._contextualizer)} \"", "\n", "+", "\"does not report how many layers it has.\"", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.models.language_model.LanguageModel.forward": [[248, 362], ["allennlp.nn.util.get_text_field_mask", "language_model.LanguageModel._text_field_embedder", "language_model.LanguageModel._contextualizer", "source.get", "return_dict.update", "isinstance", "torch.zeros_like", "language_model.LanguageModel._dropout", "language_model.LanguageModel._compute_loss", "torch.sum", "language_model.LanguageModel._perplexity", "torch.zeros_like", "torch.tensor().to", "return_dict.update", "return_dict.update", "torch.sum.float", "torch.sum.float", "torch.tensor", "torch.sum.float", "torch.sum.float", "torch.sum.float"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.nn.util.get_text_field_mask", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.params.Params.get", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.metrics.conll_coref_scores.Scorer.update", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.models.language_model.LanguageModel._compute_loss", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.metrics.conll_coref_scores.Scorer.update", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.metrics.conll_coref_scores.Scorer.update"], ["", "", "def", "forward", "(", "# type: ignore", "\n", "self", ",", "source", ":", "TextFieldTensors", "\n", ")", "->", "Dict", "[", "str", ",", "torch", ".", "Tensor", "]", ":", "\n", "        ", "\"\"\"\n        Computes the averaged forward (and backward, if language model is bidirectional)\n        LM loss from the batch.\n\n        # Parameters\n\n        source : `TextFieldTensors`, required.\n            The output of `Batch.as_tensor_dict()` for a batch of sentences. By convention,\n            it's required to have at least a `\"tokens\"` entry that's the output of a\n            `SingleIdTokenIndexer`, which is used to compute the language model targets.\n\n        # Returns\n\n        Dict with keys:\n\n        `'loss'` : `torch.Tensor`\n            forward negative log likelihood, or the average of forward/backward\n            if language model is bidirectional\n        `'forward_loss'` : `torch.Tensor`\n            forward direction negative log likelihood\n        `'backward_loss'` : `torch.Tensor` or `None`\n            backward direction negative log likelihood. If language model is not\n            bidirectional, this is `None`.\n        `'lm_embeddings'` : `Union[torch.Tensor, List[torch.Tensor]]`\n            (batch_size, timesteps, embed_dim) tensor of top layer contextual representations or\n            list of all layers. No dropout applied.\n        `'noncontextual_token_embeddings'` : `torch.Tensor`\n            (batch_size, timesteps, token_embed_dim) tensor of bottom layer noncontextual\n            representations\n        `'mask'` : `torch.Tensor`\n            (batch_size, timesteps) mask for the embeddings\n        \"\"\"", "\n", "\n", "mask", "=", "get_text_field_mask", "(", "source", ")", "\n", "\n", "# shape (batch_size, timesteps, embedding_size)", "\n", "embeddings", "=", "self", ".", "_text_field_embedder", "(", "source", ")", "\n", "\n", "# Either the top layer or all layers.", "\n", "contextual_embeddings", ":", "Union", "[", "\n", "torch", ".", "Tensor", ",", "List", "[", "torch", ".", "Tensor", "]", "\n", "]", "=", "self", ".", "_contextualizer", "(", "embeddings", ",", "mask", ")", "\n", "\n", "return_dict", "=", "{", "}", "\n", "\n", "# If we have target tokens, calculate the loss.", "\n", "token_id_dict", "=", "source", ".", "get", "(", "\"tokens\"", ")", "\n", "if", "token_id_dict", "is", "not", "None", ":", "\n", "            ", "token_ids", "=", "token_id_dict", "[", "\"tokens\"", "]", "\n", "assert", "isinstance", "(", "contextual_embeddings", ",", "torch", ".", "Tensor", ")", "\n", "\n", "# Use token_ids to compute targets", "\n", "forward_targets", "=", "torch", ".", "zeros_like", "(", "token_ids", ")", "\n", "forward_targets", "[", ":", ",", "0", ":", "-", "1", "]", "=", "token_ids", "[", ":", ",", "1", ":", "]", "\n", "\n", "if", "self", ".", "_bidirectional", ":", "\n", "                ", "backward_targets", "=", "torch", ".", "zeros_like", "(", "token_ids", ")", "\n", "backward_targets", "[", ":", ",", "1", ":", "]", "=", "token_ids", "[", ":", ",", "0", ":", "-", "1", "]", "\n", "", "else", ":", "\n", "                ", "backward_targets", "=", "None", "\n", "\n", "# add dropout", "\n", "", "contextual_embeddings_with_dropout", "=", "self", ".", "_dropout", "(", "contextual_embeddings", ")", "\n", "\n", "# compute softmax loss", "\n", "forward_loss", ",", "backward_loss", "=", "self", ".", "_compute_loss", "(", "\n", "contextual_embeddings_with_dropout", ",", "\n", "embeddings", ",", "\n", "forward_targets", ",", "\n", "backward_targets", ",", "\n", ")", "\n", "\n", "num_targets", "=", "torch", ".", "sum", "(", "(", "forward_targets", ">", "0", ")", ".", "long", "(", ")", ")", "\n", "if", "num_targets", ">", "0", ":", "\n", "                ", "if", "self", ".", "_bidirectional", ":", "\n", "                    ", "average_loss", "=", "(", "\n", "0.5", "*", "(", "forward_loss", "+", "backward_loss", ")", "/", "num_targets", ".", "float", "(", ")", "\n", ")", "\n", "", "else", ":", "\n", "                    ", "average_loss", "=", "forward_loss", "/", "num_targets", ".", "float", "(", ")", "\n", "", "", "else", ":", "\n", "                ", "average_loss", "=", "torch", ".", "tensor", "(", "0.0", ")", ".", "to", "(", "forward_targets", ".", "device", ")", "\n", "\n", "", "self", ".", "_perplexity", "(", "average_loss", ")", "\n", "\n", "if", "num_targets", ">", "0", ":", "\n", "                ", "return_dict", ".", "update", "(", "\n", "{", "\n", "\"loss\"", ":", "average_loss", ",", "\n", "\"forward_loss\"", ":", "forward_loss", "/", "num_targets", ".", "float", "(", ")", ",", "\n", "\"batch_weight\"", ":", "num_targets", ".", "float", "(", ")", ",", "\n", "}", "\n", ")", "\n", "if", "backward_loss", "is", "not", "None", ":", "\n", "                    ", "return_dict", "[", "\"backward_loss\"", "]", "=", "backward_loss", "/", "num_targets", ".", "float", "(", ")", "\n", "", "", "else", ":", "\n", "# average_loss zero tensor, return it for all", "\n", "                ", "return_dict", ".", "update", "(", "{", "\"loss\"", ":", "average_loss", ",", "\"forward_loss\"", ":", "average_loss", "}", ")", "\n", "if", "backward_loss", "is", "not", "None", ":", "\n", "                    ", "return_dict", "[", "\"backward_loss\"", "]", "=", "average_loss", "\n", "\n", "", "", "", "return_dict", ".", "update", "(", "\n", "{", "\n", "# Note: These embeddings do not have dropout applied.", "\n", "\"lm_embeddings\"", ":", "contextual_embeddings", ",", "\n", "\"noncontextual_token_embeddings\"", ":", "embeddings", ",", "\n", "\"mask\"", ":", "mask", ",", "\n", "}", "\n", ")", "\n", "\n", "return", "return_dict", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.models.language_model.LanguageModel.get_metrics": [[363, 365], ["language_model.LanguageModel._perplexity.get_metric"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.metrics.sequence_accuracy.SequenceAccuracy.get_metric"], ["", "def", "get_metrics", "(", "self", ",", "reset", ":", "bool", "=", "False", ")", ":", "\n", "        ", "return", "{", "\"perplexity\"", ":", "self", ".", "_perplexity", ".", "get_metric", "(", "reset", "=", "reset", ")", "}", "\n", "", "", ""]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.models.biattentive_classification_network.BiattentiveClassificationNetwork.__init__": [[77, 211], ["allennlp.nn.InitializerApplicator", "allennlp.models.model.Model.__init__", "torch.nn.Dropout", "torch.nn.Dropout", "biattentive_classification_network.BiattentiveClassificationNetwork.vocab.get_vocab_size", "torch.nn.Dropout", "torch.nn.Dropout", "torch.nn.Linear", "torch.nn.Linear", "allennlp.common.checks.check_dimensions_match", "allennlp.common.checks.check_dimensions_match", "allennlp.common.checks.check_dimensions_match", "torch.nn.CrossEntropyLoss", "torch.nn.CrossEntropyLoss", "torch.nn.CrossEntropyLoss", "torch.nn.CrossEntropyLoss", "initializer", "biattentive_classification_network.BiattentiveClassificationNetwork._text_field_embedder._token_embedders.keys", "allennlp.common.checks.ConfigurationError", "int", "int", "allennlp.common.checks.ConfigurationError", "biattentive_classification_network.BiattentiveClassificationNetwork._integrator.get_output_dim", "allennlp.common.checks.check_dimensions_match", "allennlp.common.checks.check_dimensions_match", "biattentive_classification_network.BiattentiveClassificationNetwork._pre_encode_feedforward.get_output_dim", "biattentive_classification_network.BiattentiveClassificationNetwork._encoder.get_input_dim", "biattentive_classification_network.BiattentiveClassificationNetwork._integrator.get_input_dim", "allennlp.common.checks.check_dimensions_match", "allennlp.common.checks.check_dimensions_match", "biattentive_classification_network.BiattentiveClassificationNetwork._output_layer.get_output_dim", "allennlp.training.metrics.CategoricalAccuracy", "allennlp.training.metrics.CategoricalAccuracy", "allennlp.common.checks.ConfigurationError", "len", "allennlp.common.checks.ConfigurationError", "biattentive_classification_network.BiattentiveClassificationNetwork._integrator.get_output_dim", "biattentive_classification_network.BiattentiveClassificationNetwork._elmo.get_output_dim", "biattentive_classification_network.BiattentiveClassificationNetwork._pre_encode_feedforward.get_input_dim", "text_field_embedder.get_output_dim", "biattentive_classification_network.BiattentiveClassificationNetwork._pre_encode_feedforward.get_input_dim", "biattentive_classification_network.BiattentiveClassificationNetwork._encoder.get_output_dim", "biattentive_classification_network.BiattentiveClassificationNetwork._output_layer.get_input_dim", "biattentive_classification_network.BiattentiveClassificationNetwork._output_layer.get_input_dim", "text_field_embedder.get_output_dim", "biattentive_classification_network.BiattentiveClassificationNetwork._elmo.get_output_dim", "biattentive_classification_network.BiattentiveClassificationNetwork._integrator.get_output_dim", "len"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.graph_util.plot.GraphPlot.__init__", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.data.vocabulary.Vocabulary.get_vocab_size", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.checks.check_dimensions_match", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.checks.check_dimensions_match", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.checks.check_dimensions_match", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.seq2seq_decoders.auto_regressive_seq_decoder.AutoRegressiveSeqDecoder.get_output_dim", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.checks.check_dimensions_match", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.checks.check_dimensions_match", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.seq2seq_decoders.auto_regressive_seq_decoder.AutoRegressiveSeqDecoder.get_output_dim", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.seq2vec_encoders.cnn_highway_encoder.CnnHighwayEncoder.get_input_dim", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.seq2vec_encoders.cnn_highway_encoder.CnnHighwayEncoder.get_input_dim", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.checks.check_dimensions_match", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.checks.check_dimensions_match", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.seq2seq_decoders.auto_regressive_seq_decoder.AutoRegressiveSeqDecoder.get_output_dim", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.seq2seq_decoders.auto_regressive_seq_decoder.AutoRegressiveSeqDecoder.get_output_dim", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.seq2seq_decoders.auto_regressive_seq_decoder.AutoRegressiveSeqDecoder.get_output_dim", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.seq2vec_encoders.cnn_highway_encoder.CnnHighwayEncoder.get_input_dim", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.seq2seq_decoders.auto_regressive_seq_decoder.AutoRegressiveSeqDecoder.get_output_dim", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.seq2vec_encoders.cnn_highway_encoder.CnnHighwayEncoder.get_input_dim", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.seq2seq_decoders.auto_regressive_seq_decoder.AutoRegressiveSeqDecoder.get_output_dim", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.seq2vec_encoders.cnn_highway_encoder.CnnHighwayEncoder.get_input_dim", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.seq2vec_encoders.cnn_highway_encoder.CnnHighwayEncoder.get_input_dim", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.seq2seq_decoders.auto_regressive_seq_decoder.AutoRegressiveSeqDecoder.get_output_dim", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.seq2seq_decoders.auto_regressive_seq_decoder.AutoRegressiveSeqDecoder.get_output_dim", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.seq2seq_decoders.auto_regressive_seq_decoder.AutoRegressiveSeqDecoder.get_output_dim"], ["def", "__init__", "(", "\n", "self", ",", "\n", "vocab", ":", "Vocabulary", ",", "\n", "text_field_embedder", ":", "TextFieldEmbedder", ",", "\n", "embedding_dropout", ":", "float", ",", "\n", "pre_encode_feedforward", ":", "FeedForward", ",", "\n", "encoder", ":", "Seq2SeqEncoder", ",", "\n", "integrator", ":", "Seq2SeqEncoder", ",", "\n", "integrator_dropout", ":", "float", ",", "\n", "output_layer", ":", "Union", "[", "FeedForward", ",", "Maxout", "]", ",", "\n", "elmo", ":", "Elmo", "=", "None", ",", "\n", "use_input_elmo", ":", "bool", "=", "False", ",", "\n", "use_integrator_output_elmo", ":", "bool", "=", "False", ",", "\n", "initializer", ":", "InitializerApplicator", "=", "InitializerApplicator", "(", ")", ",", "\n", "**", "kwargs", ",", "\n", ")", "->", "None", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "vocab", ",", "**", "kwargs", ")", "\n", "\n", "self", ".", "_text_field_embedder", "=", "text_field_embedder", "\n", "if", "\"elmo\"", "in", "self", ".", "_text_field_embedder", ".", "_token_embedders", ".", "keys", "(", ")", ":", "\n", "            ", "raise", "ConfigurationError", "(", "\n", "\"To use ELMo in the BiattentiveClassificationNetwork input, \"", "\n", "\"remove elmo from the text_field_embedder and pass an \"", "\n", "\"Elmo object to the BiattentiveClassificationNetwork and set the \"", "\n", "\"'use_input_elmo' and 'use_integrator_output_elmo' flags accordingly.\"", "\n", ")", "\n", "", "self", ".", "_embedding_dropout", "=", "nn", ".", "Dropout", "(", "embedding_dropout", ")", "\n", "self", ".", "_num_classes", "=", "self", ".", "vocab", ".", "get_vocab_size", "(", "\"labels\"", ")", "\n", "\n", "self", ".", "_pre_encode_feedforward", "=", "pre_encode_feedforward", "\n", "self", ".", "_encoder", "=", "encoder", "\n", "self", ".", "_integrator", "=", "integrator", "\n", "self", ".", "_integrator_dropout", "=", "nn", ".", "Dropout", "(", "integrator_dropout", ")", "\n", "\n", "self", ".", "_elmo", "=", "elmo", "\n", "self", ".", "_use_input_elmo", "=", "use_input_elmo", "\n", "self", ".", "_use_integrator_output_elmo", "=", "use_integrator_output_elmo", "\n", "self", ".", "_num_elmo_layers", "=", "int", "(", "self", ".", "_use_input_elmo", ")", "+", "int", "(", "\n", "self", ".", "_use_integrator_output_elmo", "\n", ")", "\n", "# Check that, if elmo is None, none of the elmo flags are set.", "\n", "if", "self", ".", "_elmo", "is", "None", "and", "self", ".", "_num_elmo_layers", "!=", "0", ":", "\n", "            ", "raise", "ConfigurationError", "(", "\n", "\"One of 'use_input_elmo' or 'use_integrator_output_elmo' is True, \"", "\n", "\"but no Elmo object was provided upon construction. Pass in an Elmo \"", "\n", "\"object to use Elmo.\"", "\n", ")", "\n", "\n", "", "if", "self", ".", "_elmo", "is", "not", "None", ":", "\n", "# Check that, if elmo is not None, we use it somewhere.", "\n", "            ", "if", "self", ".", "_num_elmo_layers", "==", "0", ":", "\n", "                ", "raise", "ConfigurationError", "(", "\n", "\"Elmo object provided upon construction, but both 'use_input_elmo' \"", "\n", "\"and 'use_integrator_output_elmo' are 'False'. Set one of them to \"", "\n", "\"'True' to use Elmo, or do not provide an Elmo object upon construction.\"", "\n", ")", "\n", "# Check that the number of flags set is equal to the num_output_representations of the Elmo object", "\n", "\n", "", "if", "len", "(", "self", ".", "_elmo", ".", "_scalar_mixes", ")", "!=", "self", ".", "_num_elmo_layers", ":", "\n", "                ", "raise", "ConfigurationError", "(", "\n", "f\"Elmo object has num_output_representations={len(self._elmo._scalar_mixes)}, but this \"", "\n", "f\"does not match the number of use_*_elmo flags set to true. use_input_elmo \"", "\n", "f\"is {self._use_input_elmo}, and use_integrator_output_elmo \"", "\n", "f\"is {self._use_integrator_output_elmo}\"", "\n", ")", "\n", "\n", "# Calculate combined integrator output dim, taking into account elmo", "\n", "", "", "if", "self", ".", "_use_integrator_output_elmo", ":", "\n", "            ", "self", ".", "_combined_integrator_output_dim", "=", "(", "\n", "self", ".", "_integrator", ".", "get_output_dim", "(", ")", "+", "self", ".", "_elmo", ".", "get_output_dim", "(", ")", "\n", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "_combined_integrator_output_dim", "=", "self", ".", "_integrator", ".", "get_output_dim", "(", ")", "\n", "\n", "", "self", ".", "_self_attentive_pooling_projection", "=", "nn", ".", "Linear", "(", "\n", "self", ".", "_combined_integrator_output_dim", ",", "1", "\n", ")", "\n", "self", ".", "_output_layer", "=", "output_layer", "\n", "\n", "if", "self", ".", "_use_input_elmo", ":", "\n", "            ", "check_dimensions_match", "(", "\n", "text_field_embedder", ".", "get_output_dim", "(", ")", "+", "self", ".", "_elmo", ".", "get_output_dim", "(", ")", ",", "\n", "self", ".", "_pre_encode_feedforward", ".", "get_input_dim", "(", ")", ",", "\n", "\"text field embedder output dim + ELMo output dim\"", ",", "\n", "\"Pre-encoder feedforward input dim\"", ",", "\n", ")", "\n", "", "else", ":", "\n", "            ", "check_dimensions_match", "(", "\n", "text_field_embedder", ".", "get_output_dim", "(", ")", ",", "\n", "self", ".", "_pre_encode_feedforward", ".", "get_input_dim", "(", ")", ",", "\n", "\"text field embedder output dim\"", ",", "\n", "\"Pre-encoder feedforward input dim\"", ",", "\n", ")", "\n", "\n", "", "check_dimensions_match", "(", "\n", "self", ".", "_pre_encode_feedforward", ".", "get_output_dim", "(", ")", ",", "\n", "self", ".", "_encoder", ".", "get_input_dim", "(", ")", ",", "\n", "\"Pre-encoder feedforward output dim\"", ",", "\n", "\"Encoder input dim\"", ",", "\n", ")", "\n", "check_dimensions_match", "(", "\n", "self", ".", "_encoder", ".", "get_output_dim", "(", ")", "*", "3", ",", "\n", "self", ".", "_integrator", ".", "get_input_dim", "(", ")", ",", "\n", "\"Encoder output dim * 3\"", ",", "\n", "\"Integrator input dim\"", ",", "\n", ")", "\n", "if", "self", ".", "_use_integrator_output_elmo", ":", "\n", "            ", "check_dimensions_match", "(", "\n", "self", ".", "_combined_integrator_output_dim", "*", "4", ",", "\n", "self", ".", "_output_layer", ".", "get_input_dim", "(", ")", ",", "\n", "\"(Integrator output dim + ELMo output dim) * 4\"", ",", "\n", "\"Output layer input dim\"", ",", "\n", ")", "\n", "", "else", ":", "\n", "            ", "check_dimensions_match", "(", "\n", "self", ".", "_integrator", ".", "get_output_dim", "(", ")", "*", "4", ",", "\n", "self", ".", "_output_layer", ".", "get_input_dim", "(", ")", ",", "\n", "\"Integrator output dim * 4\"", ",", "\n", "\"Output layer input dim\"", ",", "\n", ")", "\n", "\n", "", "check_dimensions_match", "(", "\n", "self", ".", "_output_layer", ".", "get_output_dim", "(", ")", ",", "\n", "self", ".", "_num_classes", ",", "\n", "\"Output layer output dim\"", ",", "\n", "\"Number of classes.\"", ",", "\n", ")", "\n", "\n", "self", ".", "metrics", "=", "{", "\n", "\"accuracy\"", ":", "CategoricalAccuracy", "(", ")", ",", "\n", "\"accuracy3\"", ":", "CategoricalAccuracy", "(", "top_k", "=", "3", ")", ",", "\n", "}", "\n", "self", ".", "loss", "=", "torch", ".", "nn", ".", "CrossEntropyLoss", "(", ")", "\n", "initializer", "(", "self", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.models.biattentive_classification_network.BiattentiveClassificationNetwork.forward": [[212, 340], ["allennlp.nn.util.get_text_field_mask().float", "tokens.pop", "biattentive_classification_network.BiattentiveClassificationNetwork._embedding_dropout", "biattentive_classification_network.BiattentiveClassificationNetwork._pre_encode_feedforward", "biattentive_classification_network.BiattentiveClassificationNetwork._encoder", "biattentive_classification_network.BiattentiveClassificationNetwork.bmm", "allennlp.nn.util.masked_softmax", "allennlp.nn.util.weighted_sum", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "biattentive_classification_network.BiattentiveClassificationNetwork._integrator", "allennlp.nn.util.replace_masked_values", "allennlp.nn.util.replace_masked_values", "biattentive_classification_network.BiattentiveClassificationNetwork._self_attentive_pooling_projection().squeeze", "allennlp.nn.util.masked_softmax", "allennlp.nn.util.weighted_sum", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "biattentive_classification_network.BiattentiveClassificationNetwork._integrator_dropout", "biattentive_classification_network.BiattentiveClassificationNetwork._output_layer", "torch.softmax", "torch.softmax", "biattentive_classification_network.BiattentiveClassificationNetwork._text_field_embedder", "biattentive_classification_network.BiattentiveClassificationNetwork.permute().contiguous", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "allennlp.nn.util.get_text_field_mask().float.unsqueeze", "torch.max", "torch.max", "torch.max", "torch.max", "allennlp.nn.util.get_text_field_mask().float.unsqueeze", "torch.min", "torch.min", "torch.min", "torch.min", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "biattentive_classification_network.BiattentiveClassificationNetwork.loss", "biattentive_classification_network.BiattentiveClassificationNetwork.metrics.values", "allennlp.nn.util.get_text_field_mask", "allennlp.common.checks.ConfigurationError", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "biattentive_classification_network.BiattentiveClassificationNetwork._self_attentive_pooling_projection", "metric", "biattentive_classification_network.BiattentiveClassificationNetwork._elmo", "elmo_representations.pop", "elmo_representations.pop", "biattentive_classification_network.BiattentiveClassificationNetwork.permute"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.params.Params.pop", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.nn.util.masked_softmax", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.nn.util.weighted_sum", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.nn.util.replace_masked_values", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.nn.util.replace_masked_values", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.nn.util.masked_softmax", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.nn.util.weighted_sum", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.nn.util.get_text_field_mask", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.params.Params.pop", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.params.Params.pop"], ["", "@", "overrides", "\n", "def", "forward", "(", "\n", "self", ",", "# type: ignore", "\n", "tokens", ":", "TextFieldTensors", ",", "\n", "label", ":", "torch", ".", "LongTensor", "=", "None", ",", "\n", ")", "->", "Dict", "[", "str", ",", "torch", ".", "Tensor", "]", ":", "\n", "\n", "        ", "\"\"\"\n        # Parameters\n\n        tokens : TextFieldTensors, required\n            The output of `TextField.as_array()`.\n        label : torch.LongTensor, optional (default = None)\n            A variable representing the label for each instance in the batch.\n        # Returns\n\n        An output dictionary consisting of:\n        class_probabilities : torch.FloatTensor\n            A tensor of shape `(batch_size, num_classes)` representing a\n            distribution over the label classes for each instance.\n        loss : torch.FloatTensor, optional\n            A scalar loss to be optimised.\n        \"\"\"", "\n", "text_mask", "=", "util", ".", "get_text_field_mask", "(", "tokens", ")", ".", "float", "(", ")", "\n", "# Pop elmo tokens, since elmo embedder should not be present.", "\n", "elmo_tokens", "=", "tokens", ".", "pop", "(", "\"elmo\"", ",", "None", ")", "\n", "if", "tokens", ":", "\n", "            ", "embedded_text", "=", "self", ".", "_text_field_embedder", "(", "tokens", ")", "\n", "", "else", ":", "\n", "# only using \"elmo\" for input", "\n", "            ", "embedded_text", "=", "None", "\n", "\n", "# Add the \"elmo\" key back to \"tokens\" if not None, since the tests and the", "\n", "# subsequent training epochs rely not being modified during forward()", "\n", "", "if", "elmo_tokens", "is", "not", "None", ":", "\n", "            ", "tokens", "[", "\"elmo\"", "]", "=", "elmo_tokens", "\n", "\n", "# Create ELMo embeddings if applicable", "\n", "", "if", "self", ".", "_elmo", ":", "\n", "            ", "if", "elmo_tokens", "is", "not", "None", ":", "\n", "                ", "elmo_representations", "=", "self", ".", "_elmo", "(", "elmo_tokens", "[", "\"tokens\"", "]", ")", "[", "\n", "\"elmo_representations\"", "\n", "]", "\n", "# Pop from the end is more performant with list", "\n", "if", "self", ".", "_use_integrator_output_elmo", ":", "\n", "                    ", "integrator_output_elmo", "=", "elmo_representations", ".", "pop", "(", ")", "\n", "", "if", "self", ".", "_use_input_elmo", ":", "\n", "                    ", "input_elmo", "=", "elmo_representations", ".", "pop", "(", ")", "\n", "", "assert", "not", "elmo_representations", "\n", "", "else", ":", "\n", "                ", "raise", "ConfigurationError", "(", "\n", "\"Model was built to use Elmo, but input text is not tokenized for Elmo.\"", "\n", ")", "\n", "\n", "", "", "if", "self", ".", "_use_input_elmo", ":", "\n", "            ", "if", "embedded_text", "is", "not", "None", ":", "\n", "                ", "embedded_text", "=", "torch", ".", "cat", "(", "[", "embedded_text", ",", "input_elmo", "]", ",", "dim", "=", "-", "1", ")", "\n", "", "else", ":", "\n", "                ", "embedded_text", "=", "input_elmo", "\n", "\n", "", "", "dropped_embedded_text", "=", "self", ".", "_embedding_dropout", "(", "embedded_text", ")", "\n", "pre_encoded_text", "=", "self", ".", "_pre_encode_feedforward", "(", "dropped_embedded_text", ")", "\n", "encoded_tokens", "=", "self", ".", "_encoder", "(", "pre_encoded_text", ",", "text_mask", ")", "\n", "\n", "# Compute biattention. This is a special case since the inputs are the same.", "\n", "attention_logits", "=", "encoded_tokens", ".", "bmm", "(", "\n", "encoded_tokens", ".", "permute", "(", "0", ",", "2", ",", "1", ")", ".", "contiguous", "(", ")", "\n", ")", "\n", "attention_weights", "=", "util", ".", "masked_softmax", "(", "attention_logits", ",", "text_mask", ")", "\n", "encoded_text", "=", "util", ".", "weighted_sum", "(", "encoded_tokens", ",", "attention_weights", ")", "\n", "\n", "# Build the input to the integrator", "\n", "integrator_input", "=", "torch", ".", "cat", "(", "\n", "[", "\n", "encoded_tokens", ",", "\n", "encoded_tokens", "-", "encoded_text", ",", "\n", "encoded_tokens", "*", "encoded_text", ",", "\n", "]", ",", "\n", "2", ",", "\n", ")", "\n", "integrated_encodings", "=", "self", ".", "_integrator", "(", "integrator_input", ",", "text_mask", ")", "\n", "\n", "# Concatenate ELMo representations to integrated_encodings if specified", "\n", "if", "self", ".", "_use_integrator_output_elmo", ":", "\n", "            ", "integrated_encodings", "=", "torch", ".", "cat", "(", "\n", "[", "integrated_encodings", ",", "integrator_output_elmo", "]", ",", "dim", "=", "-", "1", "\n", ")", "\n", "\n", "# Simple Pooling layers", "\n", "", "max_masked_integrated_encodings", "=", "util", ".", "replace_masked_values", "(", "\n", "integrated_encodings", ",", "text_mask", ".", "unsqueeze", "(", "2", ")", ",", "-", "1e7", "\n", ")", "\n", "max_pool", "=", "torch", ".", "max", "(", "max_masked_integrated_encodings", ",", "1", ")", "[", "0", "]", "\n", "min_masked_integrated_encodings", "=", "util", ".", "replace_masked_values", "(", "\n", "integrated_encodings", ",", "text_mask", ".", "unsqueeze", "(", "2", ")", ",", "+", "1e7", "\n", ")", "\n", "min_pool", "=", "torch", ".", "min", "(", "min_masked_integrated_encodings", ",", "1", ")", "[", "0", "]", "\n", "mean_pool", "=", "torch", ".", "sum", "(", "integrated_encodings", ",", "1", ")", "/", "torch", ".", "sum", "(", "\n", "text_mask", ",", "1", ",", "keepdim", "=", "True", "\n", ")", "\n", "\n", "# Self-attentive pooling layer", "\n", "# Run through linear projection. Shape: (batch_size, sequence length, 1)", "\n", "# Then remove the last dimension to get the proper attention shape (batch_size, sequence length).", "\n", "self_attentive_logits", "=", "self", ".", "_self_attentive_pooling_projection", "(", "\n", "integrated_encodings", "\n", ")", ".", "squeeze", "(", "2", ")", "\n", "self_weights", "=", "util", ".", "masked_softmax", "(", "self_attentive_logits", ",", "text_mask", ")", "\n", "self_attentive_pool", "=", "util", ".", "weighted_sum", "(", "integrated_encodings", ",", "self_weights", ")", "\n", "\n", "pooled_representations", "=", "torch", ".", "cat", "(", "\n", "[", "max_pool", ",", "min_pool", ",", "mean_pool", ",", "self_attentive_pool", "]", ",", "1", "\n", ")", "\n", "pooled_representations_dropped", "=", "self", ".", "_integrator_dropout", "(", "\n", "pooled_representations", "\n", ")", "\n", "\n", "logits", "=", "self", ".", "_output_layer", "(", "pooled_representations_dropped", ")", "\n", "class_probabilities", "=", "F", ".", "softmax", "(", "logits", ",", "dim", "=", "-", "1", ")", "\n", "\n", "output_dict", "=", "{", "\"logits\"", ":", "logits", ",", "\"class_probabilities\"", ":", "class_probabilities", "}", "\n", "if", "label", "is", "not", "None", ":", "\n", "            ", "loss", "=", "self", ".", "loss", "(", "logits", ",", "label", ")", "\n", "for", "metric", "in", "self", ".", "metrics", ".", "values", "(", ")", ":", "\n", "                ", "metric", "(", "logits", ",", "label", ")", "\n", "", "output_dict", "[", "\"loss\"", "]", "=", "loss", "\n", "\n", "", "return", "output_dict", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.models.biattentive_classification_network.BiattentiveClassificationNetwork.decode": [[341, 355], ["output_dict[].cpu().data.numpy", "numpy.argmax", "biattentive_classification_network.BiattentiveClassificationNetwork.vocab.get_token_from_index", "output_dict[].cpu"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.data.vocabulary.Vocabulary.get_token_from_index"], ["", "@", "overrides", "\n", "def", "decode", "(", "self", ",", "output_dict", ":", "Dict", "[", "str", ",", "torch", ".", "Tensor", "]", ")", "->", "Dict", "[", "str", ",", "torch", ".", "Tensor", "]", ":", "\n", "        ", "\"\"\"\n        Does a simple argmax over the class probabilities, converts indices to string labels, and\n        adds a `\"label\"` key to the dictionary with the result.\n        \"\"\"", "\n", "predictions", "=", "output_dict", "[", "\"class_probabilities\"", "]", ".", "cpu", "(", ")", ".", "data", ".", "numpy", "(", ")", "\n", "argmax_indices", "=", "numpy", ".", "argmax", "(", "predictions", ",", "axis", "=", "-", "1", ")", "\n", "labels", "=", "[", "\n", "self", ".", "vocab", ".", "get_token_from_index", "(", "x", ",", "namespace", "=", "\"labels\"", ")", "\n", "for", "x", "in", "argmax_indices", "\n", "]", "\n", "output_dict", "[", "\"label\"", "]", "=", "labels", "\n", "return", "output_dict", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.models.biattentive_classification_network.BiattentiveClassificationNetwork.get_metrics": [[356, 361], ["metric.get_metric", "biattentive_classification_network.BiattentiveClassificationNetwork.metrics.items"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.metrics.sequence_accuracy.SequenceAccuracy.get_metric"], ["", "@", "overrides", "\n", "def", "get_metrics", "(", "self", ",", "reset", ":", "bool", "=", "False", ")", "->", "Dict", "[", "str", ",", "float", "]", ":", "\n", "        ", "return", "{", "\n", "metric_name", ":", "metric", ".", "get_metric", "(", "reset", ")", "\n", "for", "metric_name", ",", "metric", "in", "self", ".", "metrics", ".", "items", "(", ")", "\n", "}", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.models.basic_classifier.BasicClassifier.__init__": [[53, 98], ["allennlp.nn.InitializerApplicator", "allennlp.models.model.Model.__init__", "torch.nn.Linear", "allennlp.training.metrics.CategoricalAccuracy", "torch.nn.CrossEntropyLoss", "initializer", "basic_classifier.BasicClassifier._feedforward.get_output_dim", "basic_classifier.BasicClassifier._seq2vec_encoder.get_output_dim", "torch.nn.Dropout", "vocab.get_vocab_size"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.graph_util.plot.GraphPlot.__init__", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.seq2seq_decoders.auto_regressive_seq_decoder.AutoRegressiveSeqDecoder.get_output_dim", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.seq2seq_decoders.auto_regressive_seq_decoder.AutoRegressiveSeqDecoder.get_output_dim", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.data.vocabulary.Vocabulary.get_vocab_size"], ["def", "__init__", "(", "\n", "self", ",", "\n", "vocab", ":", "Vocabulary", ",", "\n", "text_field_embedder", ":", "TextFieldEmbedder", ",", "\n", "seq2vec_encoder", ":", "Seq2VecEncoder", ",", "\n", "seq2seq_encoder", ":", "Seq2SeqEncoder", "=", "None", ",", "\n", "feedforward", ":", "Optional", "[", "FeedForward", "]", "=", "None", ",", "\n", "dropout", ":", "float", "=", "None", ",", "\n", "num_labels", ":", "int", "=", "None", ",", "\n", "label_namespace", ":", "str", "=", "\"labels\"", ",", "\n", "initializer", ":", "InitializerApplicator", "=", "InitializerApplicator", "(", ")", ",", "\n", "**", "kwargs", ",", "\n", ")", "->", "None", ":", "\n", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "vocab", ",", "**", "kwargs", ")", "\n", "self", ".", "_text_field_embedder", "=", "text_field_embedder", "\n", "\n", "if", "seq2seq_encoder", ":", "\n", "            ", "self", ".", "_seq2seq_encoder", "=", "seq2seq_encoder", "\n", "", "else", ":", "\n", "            ", "self", ".", "_seq2seq_encoder", "=", "None", "\n", "\n", "", "self", ".", "_seq2vec_encoder", "=", "seq2vec_encoder", "\n", "self", ".", "_feedforward", "=", "feedforward", "\n", "if", "feedforward", "is", "not", "None", ":", "\n", "            ", "self", ".", "_classifier_input_dim", "=", "self", ".", "_feedforward", ".", "get_output_dim", "(", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "_classifier_input_dim", "=", "self", ".", "_seq2vec_encoder", ".", "get_output_dim", "(", ")", "\n", "\n", "", "if", "dropout", ":", "\n", "            ", "self", ".", "_dropout", "=", "torch", ".", "nn", ".", "Dropout", "(", "dropout", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "_dropout", "=", "None", "\n", "", "self", ".", "_label_namespace", "=", "label_namespace", "\n", "\n", "if", "num_labels", ":", "\n", "            ", "self", ".", "_num_labels", "=", "num_labels", "\n", "", "else", ":", "\n", "            ", "self", ".", "_num_labels", "=", "vocab", ".", "get_vocab_size", "(", "namespace", "=", "self", ".", "_label_namespace", ")", "\n", "", "self", ".", "_classification_layer", "=", "torch", ".", "nn", ".", "Linear", "(", "\n", "self", ".", "_classifier_input_dim", ",", "self", ".", "_num_labels", "\n", ")", "\n", "self", ".", "_accuracy", "=", "CategoricalAccuracy", "(", ")", "\n", "self", ".", "_loss", "=", "torch", ".", "nn", ".", "CrossEntropyLoss", "(", ")", "\n", "initializer", "(", "self", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.models.basic_classifier.BasicClassifier.forward": [[99, 149], ["basic_classifier.BasicClassifier._text_field_embedder", "allennlp.nn.util.get_text_field_mask().float", "basic_classifier.BasicClassifier._seq2vec_encoder", "basic_classifier.BasicClassifier._classification_layer", "torch.nn.functional.softmax", "basic_classifier.BasicClassifier._seq2seq_encoder", "basic_classifier.BasicClassifier._dropout", "basic_classifier.BasicClassifier._feedforward", "basic_classifier.BasicClassifier._loss", "basic_classifier.BasicClassifier._accuracy", "allennlp.nn.util.get_text_field_mask", "label.long().view", "label.long"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.nn.util.get_text_field_mask"], ["", "def", "forward", "(", "# type: ignore", "\n", "self", ",", "tokens", ":", "TextFieldTensors", ",", "label", ":", "torch", ".", "IntTensor", "=", "None", "\n", ")", "->", "Dict", "[", "str", ",", "torch", ".", "Tensor", "]", ":", "\n", "\n", "        ", "\"\"\"\n        # Parameters\n\n        tokens : TextFieldTensors\n            From a `TextField`\n        label : torch.IntTensor, optional (default = None)\n            From a `LabelField`\n\n        # Returns\n\n        An output dictionary consisting of:\n\n        logits : torch.FloatTensor\n            A tensor of shape `(batch_size, num_labels)` representing\n            unnormalized log probabilities of the label.\n        probs : torch.FloatTensor\n            A tensor of shape `(batch_size, num_labels)` representing\n            probabilities of the label.\n        loss : torch.FloatTensor, optional\n            A scalar loss to be optimised.\n        \"\"\"", "\n", "embedded_text", "=", "self", ".", "_text_field_embedder", "(", "tokens", ")", "\n", "mask", "=", "get_text_field_mask", "(", "tokens", ")", ".", "float", "(", ")", "\n", "\n", "if", "self", ".", "_seq2seq_encoder", ":", "\n", "            ", "embedded_text", "=", "self", ".", "_seq2seq_encoder", "(", "embedded_text", ",", "mask", "=", "mask", ")", "\n", "\n", "", "embedded_text", "=", "self", ".", "_seq2vec_encoder", "(", "embedded_text", ",", "mask", "=", "mask", ")", "\n", "\n", "if", "self", ".", "_dropout", ":", "\n", "            ", "embedded_text", "=", "self", ".", "_dropout", "(", "embedded_text", ")", "\n", "\n", "", "if", "self", ".", "_feedforward", "is", "not", "None", ":", "\n", "            ", "embedded_text", "=", "self", ".", "_feedforward", "(", "embedded_text", ")", "\n", "\n", "", "logits", "=", "self", ".", "_classification_layer", "(", "embedded_text", ")", "\n", "probs", "=", "torch", ".", "nn", ".", "functional", ".", "softmax", "(", "logits", ",", "dim", "=", "-", "1", ")", "\n", "\n", "output_dict", "=", "{", "\"logits\"", ":", "logits", ",", "\"probs\"", ":", "probs", "}", "\n", "\n", "if", "label", "is", "not", "None", ":", "\n", "            ", "loss", "=", "self", ".", "_loss", "(", "logits", ",", "label", ".", "long", "(", ")", ".", "view", "(", "-", "1", ")", ")", "\n", "output_dict", "[", "\"loss\"", "]", "=", "loss", "\n", "self", ".", "_accuracy", "(", "logits", ",", "label", ")", "\n", "\n", "", "return", "output_dict", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.models.basic_classifier.BasicClassifier.decode": [[150, 170], ["predictions.dim", "prediction.argmax().item", "basic_classifier.BasicClassifier.vocab.get_index_to_token_vocabulary().get", "classes.append", "str", "range", "prediction.argmax", "basic_classifier.BasicClassifier.vocab.get_index_to_token_vocabulary"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.params.Params.get", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.data.vocabulary.Vocabulary.get_index_to_token_vocabulary"], ["", "@", "overrides", "\n", "def", "decode", "(", "self", ",", "output_dict", ":", "Dict", "[", "str", ",", "torch", ".", "Tensor", "]", ")", "->", "Dict", "[", "str", ",", "torch", ".", "Tensor", "]", ":", "\n", "        ", "\"\"\"\n        Does a simple argmax over the probabilities, converts index to string label, and\n        add `\"label\"` key to the dictionary with the result.\n        \"\"\"", "\n", "predictions", "=", "output_dict", "[", "\"probs\"", "]", "\n", "if", "predictions", ".", "dim", "(", ")", "==", "2", ":", "\n", "            ", "predictions_list", "=", "[", "predictions", "[", "i", "]", "for", "i", "in", "range", "(", "predictions", ".", "shape", "[", "0", "]", ")", "]", "\n", "", "else", ":", "\n", "            ", "predictions_list", "=", "[", "predictions", "]", "\n", "", "classes", "=", "[", "]", "\n", "for", "prediction", "in", "predictions_list", ":", "\n", "            ", "label_idx", "=", "prediction", ".", "argmax", "(", "dim", "=", "-", "1", ")", ".", "item", "(", ")", "\n", "label_str", "=", "self", ".", "vocab", ".", "get_index_to_token_vocabulary", "(", "\n", "self", ".", "_label_namespace", "\n", ")", ".", "get", "(", "label_idx", ",", "str", "(", "label_idx", ")", ")", "\n", "classes", ".", "append", "(", "label_str", ")", "\n", "", "output_dict", "[", "\"label\"", "]", "=", "classes", "\n", "return", "output_dict", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.models.basic_classifier.BasicClassifier.get_metrics": [[171, 174], ["basic_classifier.BasicClassifier._accuracy.get_metric"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.metrics.sequence_accuracy.SequenceAccuracy.get_metric"], ["", "def", "get_metrics", "(", "self", ",", "reset", ":", "bool", "=", "False", ")", "->", "Dict", "[", "str", ",", "float", "]", ":", "\n", "        ", "metrics", "=", "{", "\"accuracy\"", ":", "self", ".", "_accuracy", ".", "get_metric", "(", "reset", ")", "}", "\n", "return", "metrics", "\n", "", "", ""]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.models.ensemble.Ensemble.__init__": [[21, 32], ["allennlp.models.model.Model.__init__", "torch.nn.ModuleList", "allennlp.common.checks.ConfigurationError"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.graph_util.plot.GraphPlot.__init__"], ["def", "__init__", "(", "self", ",", "submodels", ":", "List", "[", "Model", "]", ")", "->", "None", ":", "\n", "        ", "vocab", "=", "submodels", "[", "0", "]", ".", "vocab", "\n", "for", "submodel", "in", "submodels", ":", "\n", "            ", "if", "submodel", ".", "vocab", "!=", "vocab", ":", "\n", "                ", "raise", "ConfigurationError", "(", "\"Vocabularies in ensemble differ\"", ")", "\n", "\n", "", "", "super", "(", ")", ".", "__init__", "(", "vocab", ",", "None", ")", "\n", "\n", "# Using ModuleList propagates calls to .eval() so dropout is disabled on the submodels in evaluation", "\n", "# and prediction.", "\n", "self", ".", "submodels", "=", "torch", ".", "nn", ".", "ModuleList", "(", "submodels", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.models.ensemble.Ensemble._load": [[33, 61], ["config.get", "allennlp.models.model.remove_pretrained_embedding_params", "allennlp.models.model.Model.from_params", "allennlp.models.model.Model.from_params.cuda", "allennlp.models.model.Model.from_params.cpu"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.params.Params.get", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.models.model.remove_pretrained_embedding_params", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.tokenizers.tokenizer.Tokenizer.from_params"], ["", "@", "classmethod", "\n", "def", "_load", "(", "\n", "cls", ",", "\n", "config", ":", "Params", ",", "\n", "serialization_dir", ":", "str", ",", "\n", "weights_file", ":", "str", "=", "None", ",", "\n", "cuda_device", ":", "int", "=", "-", "1", ",", "\n", ")", "->", "\"Model\"", ":", "\n", "        ", "\"\"\"\n        Ensembles don't have vocabularies or weights of their own, so they override _load.\n        \"\"\"", "\n", "model_params", "=", "config", ".", "get", "(", "\"model\"", ")", "\n", "\n", "# The experiment config tells us how to _train_ a model, including where to get pre-trained", "\n", "# embeddings from.  We're now _loading_ the model, so those embeddings will already be", "\n", "# stored in our weights.  We don't need any pretrained weight file anymore, and we don't", "\n", "# want the code to look for it, so we remove it from the parameters here.", "\n", "remove_pretrained_embedding_params", "(", "model_params", ")", "\n", "model", "=", "Model", ".", "from_params", "(", "vocab", "=", "None", ",", "params", "=", "model_params", ")", "\n", "\n", "# Force model to cpu or gpu, as appropriate, to make sure that the embeddings are", "\n", "# in sync with the weights", "\n", "if", "cuda_device", ">=", "0", ":", "\n", "            ", "model", ".", "cuda", "(", "cuda_device", ")", "\n", "", "else", ":", "\n", "            ", "model", ".", "cpu", "(", ")", "\n", "\n", "", "return", "model", "\n", "", "", ""]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.models.bidirectional_lm.BidirectionalLanguageModel.__init__": [[42, 63], ["allennlp.models.language_model.LanguageModel.__init__"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.graph_util.plot.GraphPlot.__init__"], ["def", "__init__", "(", "\n", "self", ",", "\n", "vocab", ":", "Vocabulary", ",", "\n", "text_field_embedder", ":", "TextFieldEmbedder", ",", "\n", "contextualizer", ":", "Seq2SeqEncoder", ",", "\n", "dropout", ":", "float", "=", "None", ",", "\n", "num_samples", ":", "int", "=", "None", ",", "\n", "sparse_embeddings", ":", "bool", "=", "False", ",", "\n", "initializer", ":", "InitializerApplicator", "=", "None", ",", "\n", "**", "kwargs", ",", "\n", ")", "->", "None", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "\n", "vocab", "=", "vocab", ",", "\n", "text_field_embedder", "=", "text_field_embedder", ",", "\n", "contextualizer", "=", "contextualizer", ",", "\n", "dropout", "=", "dropout", ",", "\n", "num_samples", "=", "num_samples", ",", "\n", "sparse_embeddings", "=", "sparse_embeddings", ",", "\n", "bidirectional", "=", "True", ",", "\n", "initializer", "=", "initializer", ",", "\n", "**", "kwargs", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.models.next_token_lm.NextTokenLM.__init__": [[46, 74], ["allennlp.models.model.Model.__init__", "allennlp.training.metrics.Perplexity", "torch.nn.Dropout", "allennlp.common.checks.check_dimensions_match", "initializer", "text_field_embedder.get_output_dim", "contextualizer.get_input_dim"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.graph_util.plot.GraphPlot.__init__", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.checks.check_dimensions_match", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.seq2seq_decoders.auto_regressive_seq_decoder.AutoRegressiveSeqDecoder.get_output_dim", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.seq2vec_encoders.cnn_highway_encoder.CnnHighwayEncoder.get_input_dim"], []], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.models.next_token_lm.NextTokenLM.forward": [[75, 114], ["next_token_lm.NextTokenLM._text_field_embedder", "next_token_lm.NextTokenLM.size", "next_token_lm.NextTokenLM._language_model_head", "target_logits.view.view.size", "torch.nn.functional.softmax", "min", "torch.nn.functional.softmax.topk", "allennlp.nn.util.get_token_ids_from_text_field_tensors", "allennlp.nn.util.get_text_field_mask", "next_token_lm.NextTokenLM._contextualizer", "allennlp.nn.util.get_final_encoder_states", "next_token_lm.NextTokenLM._dropout", "allennlp.nn.util.get_token_ids_from_text_field_tensors().view", "target_logits.view.view.view", "torch.nn.functional.cross_entropy", "next_token_lm.NextTokenLM._perplexity", "allennlp.nn.util.get_token_ids_from_text_field_tensors"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.nn.util.get_token_ids_from_text_field_tensors", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.nn.util.get_text_field_mask", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.nn.util.get_final_encoder_states", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.nn.util.get_token_ids_from_text_field_tensors"], []], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.models.next_token_lm.NextTokenLM.get_metrics": [[115, 117], ["next_token_lm.NextTokenLM._perplexity.get_metric"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.metrics.sequence_accuracy.SequenceAccuracy.get_metric"], []], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.models.next_token_lm.NextTokenLM.decode": [[118, 147], ["print", "top_words.append", "tokens.append", "next_token_lm.NextTokenLM.vocab.get_token_from_index", "next_token_lm.NextTokenLM.vocab.get_token_from_index", "token_id.item", "index.item"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.data.vocabulary.Vocabulary.get_token_from_index", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.data.vocabulary.Vocabulary.get_token_from_index"], []], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.models.masked_language_model.MaskedLanguageModel.__init__": [[45, 73], ["allennlp.models.model.Model.__init__", "allennlp.training.metrics.Perplexity", "torch.nn.Dropout", "allennlp.common.checks.check_dimensions_match", "initializer", "text_field_embedder.get_output_dim", "contextualizer.get_input_dim"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.graph_util.plot.GraphPlot.__init__", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.checks.check_dimensions_match", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.seq2seq_decoders.auto_regressive_seq_decoder.AutoRegressiveSeqDecoder.get_output_dim", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.seq2vec_encoders.cnn_highway_encoder.CnnHighwayEncoder.get_input_dim"], []], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.models.masked_language_model.MaskedLanguageModel.forward": [[74, 145], ["mask_positions.squeeze.squeeze.squeeze", "mask_positions.squeeze.squeeze.size", "masked_language_model.MaskedLanguageModel._text_field_embedder", "torch.arange().long().unsqueeze", "masked_language_model.MaskedLanguageModel._language_model_head", "target_logits.view.view.size", "torch.nn.functional.softmax", "min", "torch.nn.functional.softmax.topk", "allennlp.nn.util.get_token_ids_from_text_field_tensors", "ValueError", "allennlp.nn.util.get_text_field_mask", "masked_language_model.MaskedLanguageModel._contextualizer", "masked_language_model.MaskedLanguageModel._dropout", "target_logits.view.view.view", "targets.view.view.view", "torch.nn.functional.cross_entropy", "masked_language_model.MaskedLanguageModel._perplexity", "len", "targets.view.view.size", "mask_positions.squeeze.squeeze.size", "torch.arange().long", "list", "targets.view.view.size", "mask_positions.squeeze.squeeze.size", "torch.arange", "target_ids.values"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.nn.util.get_token_ids_from_text_field_tensors", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.nn.util.get_text_field_mask"], []], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.models.masked_language_model.MaskedLanguageModel.get_metrics": [[146, 148], ["masked_language_model.MaskedLanguageModel._perplexity.get_metric"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.metrics.sequence_accuracy.SequenceAccuracy.get_metric"], []], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.models.masked_language_model.MaskedLanguageModel.decode": [[149, 178], ["top_words.append", "tokens.append", "masked_language_model.MaskedLanguageModel.vocab.get_token_from_index", "masked_language_model.MaskedLanguageModel.vocab.get_token_from_index", "token_id.item", "index.item"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.data.vocabulary.Vocabulary.get_token_from_index", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.data.vocabulary.Vocabulary.get_token_from_index"], []], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.models.archival.Archive.extract_module": [[29, 82], ["modules_dict.get", "modules_dict.get.parameters", "allennlp.common.checks.ConfigurationError", "isinstance", "allennlp.common.checks.ConfigurationError", "parameter.requires_grad_", "archival.Archive.model.named_modules", "type", "type"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.params.Params.get"], ["def", "extract_module", "(", "self", ",", "path", ":", "str", ",", "freeze", ":", "bool", "=", "True", ")", "->", "Module", ":", "\n", "        ", "\"\"\"\n        This method can be used to load a module from the pretrained model archive.\n\n        It is also used implicitly in FromParams based construction. So instead of using standard\n        params to construct a module, you can instead load a pretrained module from the model\n        archive directly. For eg, instead of using params like {\"type\": \"module_type\", ...}, you\n        can use the following template::\n\n            {\n                \"_pretrained\": {\n                    \"archive_file\": \"../path/to/model.tar.gz\",\n                    \"path\": \"path.to.module.in.model\",\n                    \"freeze\": False\n                }\n            }\n\n        If you use this feature with FromParams, take care of the following caveat: Call to\n        initializer(self) at end of model initializer can potentially wipe the transferred parameters\n        by reinitializing them. This can happen if you have setup initializer regex that also\n        matches parameters of the transferred module. To safe-guard against this, you can either\n        update your initializer regex to prevent conflicting match or add extra initializer::\n\n            [\n                [\".*transferred_module_name.*\", \"prevent\"]]\n            ]\n\n        # Parameters\n\n        path : `str`, required\n            Path of target module to be loaded from the model.\n            Eg. \"_textfield_embedder.token_embedder_tokens\"\n        freeze : `bool`, optional (default=True)\n            Whether to freeze the module parameters or not.\n\n        \"\"\"", "\n", "modules_dict", "=", "{", "path", ":", "module", "for", "path", ",", "module", "in", "self", ".", "model", ".", "named_modules", "(", ")", "}", "\n", "module", "=", "modules_dict", ".", "get", "(", "path", ",", "None", ")", "\n", "\n", "if", "not", "module", ":", "\n", "            ", "raise", "ConfigurationError", "(", "\n", "f\"You asked to transfer module at path {path} from \"", "\n", "f\"the model {type(self.model)}. But it's not present.\"", "\n", ")", "\n", "", "if", "not", "isinstance", "(", "module", ",", "Module", ")", ":", "\n", "            ", "raise", "ConfigurationError", "(", "\n", "f\"The transferred object from model {type(self.model)} at path \"", "\n", "f\"{path} is not a PyTorch Module.\"", "\n", ")", "\n", "\n", "", "for", "parameter", "in", "module", ".", "parameters", "(", ")", ":", "# type: ignore", "\n", "            ", "parameter", ".", "requires_grad_", "(", "not", "freeze", ")", "\n", "", "return", "module", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.models.archival.archive_model": [[91, 132], ["os.path.join", "os.path.join", "logger.info", "os.path.exists", "logger.error", "os.path.exists", "logger.error", "os.path.isdir", "os.path.join", "tarfile.open", "archive.add", "archive.add", "archive.add", "os.path.join", "os.path.join"], "function", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.dataset_readers.multiprocess_dataset_reader.QIterable.join", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.dataset_readers.multiprocess_dataset_reader.QIterable.join", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.dataset_readers.multiprocess_dataset_reader.logger.info", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.dataset_readers.multiprocess_dataset_reader.QIterable.join", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.dataset_readers.multiprocess_dataset_reader.QIterable.join", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.dataset_readers.multiprocess_dataset_reader.QIterable.join"], ["def", "archive_model", "(", "\n", "serialization_dir", ":", "str", ",", "weights", ":", "str", "=", "_DEFAULT_WEIGHTS", ",", "archive_path", ":", "str", "=", "None", ",", "\n", ")", "->", "None", ":", "\n", "    ", "\"\"\"\n    Archive the model weights, its training configuration, and its vocabulary to `model.tar.gz`.\n\n    # Parameters\n\n    serialization_dir : `str`\n        The directory where the weights and vocabulary are written out.\n    weights : `str`, optional (default=_DEFAULT_WEIGHTS)\n        Which weights file to include in the archive. The default is `best.th`.\n    archive_path : `str`, optional, (default = None)\n        A full path to serialize the model to. The default is \"model.tar.gz\" inside the\n        serialization_dir. If you pass a directory here, we'll serialize the model\n        to \"model.tar.gz\" inside the directory.\n    \"\"\"", "\n", "weights_file", "=", "os", ".", "path", ".", "join", "(", "serialization_dir", ",", "weights", ")", "\n", "if", "not", "os", ".", "path", ".", "exists", "(", "weights_file", ")", ":", "\n", "        ", "logger", ".", "error", "(", "\n", "\"weights file %s does not exist, unable to archive model\"", ",", "weights_file", "\n", ")", "\n", "return", "\n", "\n", "", "config_file", "=", "os", ".", "path", ".", "join", "(", "serialization_dir", ",", "CONFIG_NAME", ")", "\n", "if", "not", "os", ".", "path", ".", "exists", "(", "config_file", ")", ":", "\n", "        ", "logger", ".", "error", "(", "\n", "\"config file %s does not exist, unable to archive model\"", ",", "config_file", "\n", ")", "\n", "\n", "", "if", "archive_path", "is", "not", "None", ":", "\n", "        ", "archive_file", "=", "archive_path", "\n", "if", "os", ".", "path", ".", "isdir", "(", "archive_file", ")", ":", "\n", "            ", "archive_file", "=", "os", ".", "path", ".", "join", "(", "archive_file", ",", "\"model.tar.gz\"", ")", "\n", "", "", "else", ":", "\n", "        ", "archive_file", "=", "os", ".", "path", ".", "join", "(", "serialization_dir", ",", "\"model.tar.gz\"", ")", "\n", "", "logger", ".", "info", "(", "\"archiving weights and vocabulary to %s\"", ",", "archive_file", ")", "\n", "with", "tarfile", ".", "open", "(", "archive_file", ",", "\"w:gz\"", ")", "as", "archive", ":", "\n", "        ", "archive", ".", "add", "(", "config_file", ",", "arcname", "=", "CONFIG_NAME", ")", "\n", "archive", ".", "add", "(", "weights_file", ",", "arcname", "=", "_WEIGHTS_NAME", ")", "\n", "archive", ".", "add", "(", "os", ".", "path", ".", "join", "(", "serialization_dir", ",", "\"vocabulary\"", ")", ",", "arcname", "=", "\"vocabulary\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.models.archival.load_archive": [[134, 201], ["allennlp.common.file_utils.cached_path", "os.path.isdir", "allennlp.common.params.Params.from_file", "allennlp.models.model.Model.load", "archival.Archive", "logger.info", "logger.info", "tempfile.mkdtemp", "logger.info", "atexit.register", "os.path.join", "os.path.join", "Params.from_file.duplicate", "tarfile.open", "archive.extractall", "os.path.exists", "os.path.join"], "function", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.file_utils.cached_path", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.params.Params.from_file", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.token_embedders.bert_token_embedder.PretrainedBertModel.load", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.dataset_readers.multiprocess_dataset_reader.logger.info", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.dataset_readers.multiprocess_dataset_reader.logger.info", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.dataset_readers.multiprocess_dataset_reader.logger.info", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.commands.subcommand.Subcommand.register", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.dataset_readers.multiprocess_dataset_reader.QIterable.join", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.dataset_readers.multiprocess_dataset_reader.QIterable.join", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.params.Params.duplicate", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.dataset_readers.multiprocess_dataset_reader.QIterable.join"], ["", "", "def", "load_archive", "(", "\n", "archive_file", ":", "str", ",", "\n", "cuda_device", ":", "int", "=", "-", "1", ",", "\n", "overrides", ":", "str", "=", "\"\"", ",", "\n", "weights_file", ":", "str", "=", "None", ",", "\n", ")", "->", "Archive", ":", "\n", "    ", "\"\"\"\n    Instantiates an Archive from an archived `tar.gz` file.\n\n    # Parameters\n\n    archive_file : `str`\n        The archive file to load the model from.\n    weights_file : `str`, optional (default = None)\n        The weights file to use.  If unspecified, weights.th in the archive_file will be used.\n    cuda_device : `int`, optional (default = -1)\n        If `cuda_device` is >= 0, the model will be loaded onto the\n        corresponding GPU. Otherwise it will be loaded onto the CPU.\n    overrides : `str`, optional (default = \"\")\n        JSON overrides to apply to the unarchived `Params` object.\n    \"\"\"", "\n", "# redirect to the cache, if necessary", "\n", "resolved_archive_file", "=", "cached_path", "(", "archive_file", ")", "\n", "\n", "if", "resolved_archive_file", "==", "archive_file", ":", "\n", "        ", "logger", ".", "info", "(", "f\"loading archive file {archive_file}\"", ")", "\n", "", "else", ":", "\n", "        ", "logger", ".", "info", "(", "\n", "f\"loading archive file {archive_file} from cache at {resolved_archive_file}\"", "\n", ")", "\n", "\n", "", "if", "os", ".", "path", ".", "isdir", "(", "resolved_archive_file", ")", ":", "\n", "        ", "serialization_dir", "=", "resolved_archive_file", "\n", "", "else", ":", "\n", "# Extract archive to temp dir", "\n", "        ", "tempdir", "=", "tempfile", ".", "mkdtemp", "(", ")", "\n", "logger", ".", "info", "(", "\n", "f\"extracting archive file {resolved_archive_file} to temp dir {tempdir}\"", "\n", ")", "\n", "with", "tarfile", ".", "open", "(", "resolved_archive_file", ",", "\"r:gz\"", ")", "as", "archive", ":", "\n", "            ", "archive", ".", "extractall", "(", "tempdir", ")", "\n", "# Postpone cleanup until exit in case the unarchived contents are needed outside", "\n", "# this function.", "\n", "", "atexit", ".", "register", "(", "_cleanup_archive_dir", ",", "tempdir", ")", "\n", "\n", "serialization_dir", "=", "tempdir", "\n", "\n", "# Load config", "\n", "", "config", "=", "Params", ".", "from_file", "(", "os", ".", "path", ".", "join", "(", "serialization_dir", ",", "CONFIG_NAME", ")", ",", "overrides", ")", "\n", "\n", "if", "weights_file", ":", "\n", "        ", "weights_path", "=", "weights_file", "\n", "", "else", ":", "\n", "        ", "weights_path", "=", "os", ".", "path", ".", "join", "(", "serialization_dir", ",", "_WEIGHTS_NAME", ")", "\n", "# Fallback for serialization directories.", "\n", "if", "not", "os", ".", "path", ".", "exists", "(", "weights_path", ")", ":", "\n", "            ", "weights_path", "=", "os", ".", "path", ".", "join", "(", "serialization_dir", ",", "_DEFAULT_WEIGHTS", ")", "\n", "\n", "# Instantiate model. Use a duplicate of the config, as it will get consumed.", "\n", "", "", "model", "=", "Model", ".", "load", "(", "\n", "config", ".", "duplicate", "(", ")", ",", "\n", "weights_file", "=", "weights_path", ",", "\n", "serialization_dir", "=", "serialization_dir", ",", "\n", "cuda_device", "=", "cuda_device", ",", "\n", ")", "\n", "\n", "return", "Archive", "(", "model", "=", "model", ",", "config", "=", "config", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.models.archival._cleanup_archive_dir": [[203, 207], ["os.path.exists", "logger.info", "shutil.rmtree"], "function", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.dataset_readers.multiprocess_dataset_reader.logger.info"], ["", "def", "_cleanup_archive_dir", "(", "path", ":", "str", ")", ":", "\n", "    ", "if", "os", ".", "path", ".", "exists", "(", "path", ")", ":", "\n", "        ", "logger", ".", "info", "(", "\"removing temporary unarchived model dir at %s\"", ",", "path", ")", "\n", "shutil", ".", "rmtree", "(", "path", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.models.constituency_parser.SpanConstituencyParser.__init__": [[84, 143], ["allennlp.nn.InitializerApplicator", "allennlp.models.model.Model.__init__", "constituency_parser.SpanConstituencyParser.vocab.get_vocab_size", "allennlp.modules.TimeDistributed", "text_field_embedder.get_output_dim", "allennlp.common.checks.check_dimensions_match", "allennlp.common.checks.check_dimensions_match", "allennlp.training.metrics.CategoricalAccuracy", "initializer", "allennlp.modules.TimeDistributed", "feedforward.get_output_dim", "span_extractor.get_output_dim", "torch.nn.modules.linear.Linear", "pos_tag_embedding.get_output_dim", "encoder.get_input_dim", "encoder.get_output_dim", "span_extractor.get_input_dim", "allennlp.common.checks.check_dimensions_match", "allennlp.training.metrics.EvalbBracketingScorer", "span_extractor.get_output_dim", "feedforward.get_input_dim"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.graph_util.plot.GraphPlot.__init__", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.data.vocabulary.Vocabulary.get_vocab_size", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.seq2seq_decoders.auto_regressive_seq_decoder.AutoRegressiveSeqDecoder.get_output_dim", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.checks.check_dimensions_match", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.checks.check_dimensions_match", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.seq2seq_decoders.auto_regressive_seq_decoder.AutoRegressiveSeqDecoder.get_output_dim", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.seq2seq_decoders.auto_regressive_seq_decoder.AutoRegressiveSeqDecoder.get_output_dim", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.seq2seq_decoders.auto_regressive_seq_decoder.AutoRegressiveSeqDecoder.get_output_dim", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.seq2vec_encoders.cnn_highway_encoder.CnnHighwayEncoder.get_input_dim", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.seq2seq_decoders.auto_regressive_seq_decoder.AutoRegressiveSeqDecoder.get_output_dim", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.seq2vec_encoders.cnn_highway_encoder.CnnHighwayEncoder.get_input_dim", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.checks.check_dimensions_match", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.seq2seq_decoders.auto_regressive_seq_decoder.AutoRegressiveSeqDecoder.get_output_dim", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.seq2vec_encoders.cnn_highway_encoder.CnnHighwayEncoder.get_input_dim"], ["\n", "return", "self", ".", "predict_json", "(", "{", "\"sentence\"", ":", "sentence", "}", ")", "\n", "\n", "", "@", "overrides", "\n", "def", "_json_to_instance", "(", "self", ",", "json_dict", ":", "JsonDict", ")", "->", "Instance", ":", "\n", "        ", "\"\"\"\n        Expects JSON that looks like `{\"sentence\": \"...\"}`.\n        \"\"\"", "\n", "spacy_tokens", "=", "self", ".", "_tokenizer", ".", "tokenize", "(", "json_dict", "[", "\"sentence\"", "]", ")", "\n", "sentence_text", "=", "[", "token", ".", "text", "for", "token", "in", "spacy_tokens", "]", "\n", "pos_tags", "=", "[", "token", ".", "tag_", "for", "token", "in", "spacy_tokens", "]", "\n", "return", "self", ".", "_dataset_reader", ".", "text_to_instance", "(", "sentence_text", ",", "pos_tags", ")", "\n", "\n", "", "@", "overrides", "\n", "def", "predict_instance", "(", "self", ",", "instance", ":", "Instance", ")", "->", "JsonDict", ":", "\n", "        ", "outputs", "=", "self", ".", "_model", ".", "forward_on_instance", "(", "instance", ")", "\n", "\n", "# format the NLTK tree as a string on a single line.", "\n", "tree", "=", "outputs", ".", "pop", "(", "\"trees\"", ")", "\n", "outputs", "[", "\"hierplane_tree\"", "]", "=", "self", ".", "_build_hierplane_tree", "(", "tree", ",", "0", ",", "is_root", "=", "True", ")", "\n", "outputs", "[", "\"trees\"", "]", "=", "tree", ".", "pformat", "(", "margin", "=", "1000000", ")", "\n", "return", "sanitize", "(", "outputs", ")", "\n", "\n", "", "@", "overrides", "\n", "def", "predict_batch_instance", "(", "self", ",", "instances", ":", "List", "[", "Instance", "]", ")", "->", "List", "[", "JsonDict", "]", ":", "\n", "        ", "outputs", "=", "self", ".", "_model", ".", "forward_on_instances", "(", "instances", ")", "\n", "for", "output", "in", "outputs", ":", "\n", "# format the NLTK tree as a string on a single line.", "\n", "            ", "tree", "=", "output", ".", "pop", "(", "\"trees\"", ")", "\n", "output", "[", "\"hierplane_tree\"", "]", "=", "self", ".", "_build_hierplane_tree", "(", "tree", ",", "0", ",", "is_root", "=", "True", ")", "\n", "output", "[", "\"trees\"", "]", "=", "tree", ".", "pformat", "(", "margin", "=", "1000000", ")", "\n", "", "return", "sanitize", "(", "outputs", ")", "\n", "\n", "", "def", "_build_hierplane_tree", "(", "self", ",", "tree", ":", "Tree", ",", "index", ":", "int", ",", "is_root", ":", "bool", ")", "->", "JsonDict", ":", "\n", "        ", "\"\"\"\n        Recursively builds a JSON dictionary from an NLTK `Tree` suitable for\n        rendering trees using the `Hierplane library<https://allenai.github.io/hierplane/>`.\n\n        # Parameters\n\n        tree : `Tree`, required.\n            The tree to convert into Hierplane JSON.\n        index : int, required.\n            The character index into the tree, used for creating spans.\n        is_root : bool\n            An indicator which allows us to add the outer Hierplane JSON which\n            is required for rendering.\n\n        # Returns\n\n        A JSON dictionary render-able by Hierplane for the given tree.\n        \"\"\"", "\n", "children", "=", "[", "]", "\n", "for", "child", "in", "tree", ":", "\n", "            ", "if", "isinstance", "(", "child", ",", "Tree", ")", ":", "\n", "# If the child is a tree, it has children,", "\n", "# as NLTK leaves are just strings.", "\n", "                ", "children", ".", "append", "(", "self", ".", "_build_hierplane_tree", "(", "child", ",", "index", ",", "is_root", "=", "False", ")", ")", "\n", "", "else", ":", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.models.constituency_parser.SpanConstituencyParser.forward": [[144, 270], ["constituency_parser.SpanConstituencyParser.text_field_embedder", "allennlp.nn.util.get_text_field_mask", "allennlp.nn.util.get_lengths_from_binary_sequence_mask", "constituency_parser.SpanConstituencyParser.encoder", "constituency_parser.SpanConstituencyParser.span_extractor", "constituency_parser.SpanConstituencyParser.tag_projection_layer", "allennlp.nn.util.masked_softmax", "constituency_parser.SpanConstituencyParser.pos_tag_embedding", "torch.cat", "span_mask.unsqueeze.unsqueeze.dim", "span_mask.unsqueeze.unsqueeze.unsqueeze", "span_labels.unsqueeze.unsqueeze.unsqueeze", "constituency_parser.SpanConstituencyParser.feedforward_layer", "span_mask.unsqueeze.unsqueeze.unsqueeze", "allennlp.nn.util.sequence_cross_entropy_with_logits", "constituency_parser.SpanConstituencyParser.tag_accuracy", "meta.get", "all", "constituency_parser.SpanConstituencyParser.construct_trees", "constituency_parser.SpanConstituencyParser._evalb_score", "allennlp.common.checks.ConfigurationError", "span_labels.unsqueeze.unsqueeze.dim", "meta.get", "list", "allennlp.nn.util.masked_softmax.cpu", "spans.cpu", "zip", "tree.pos"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.nn.util.get_text_field_mask", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.nn.util.get_lengths_from_binary_sequence_mask", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.nn.util.masked_softmax", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.nn.util.sequence_cross_entropy_with_logits", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.params.Params.get", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.models.constituency_parser.SpanConstituencyParser.construct_trees", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.params.Params.get"], ["# We're at a leaf, so add the length of", "\n", "# the word to the character index.", "\n", "                ", "index", "+=", "len", "(", "child", ")", "\n", "\n", "", "", "label", "=", "tree", ".", "label", "(", ")", "\n", "span", "=", "\" \"", ".", "join", "(", "tree", ".", "leaves", "(", ")", ")", "\n", "hierplane_node", "=", "{", "\n", "\"word\"", ":", "span", ",", "\n", "\"nodeType\"", ":", "label", ",", "\n", "\"attributes\"", ":", "[", "label", "]", ",", "\n", "\"link\"", ":", "label", ",", "\n", "}", "\n", "if", "children", ":", "\n", "            ", "hierplane_node", "[", "\"children\"", "]", "=", "children", "\n", "# TODO(Mark): Figure out how to span highlighting to the leaves.", "\n", "", "if", "is_root", ":", "\n", "            ", "hierplane_node", "=", "{", "\n", "\"linkNameToLabel\"", ":", "LINK_TO_LABEL", ",", "\n", "\"nodeTypeToStyle\"", ":", "NODE_TYPE_TO_STYLE", ",", "\n", "\"text\"", ":", "span", ",", "\n", "\"root\"", ":", "hierplane_node", ",", "\n", "}", "\n", "", "return", "hierplane_node", "\n", "", "", ""]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.models.constituency_parser.SpanConstituencyParser.decode": [[271, 300], ["constituency_parser.SpanConstituencyParser.construct_trees", "all_predictions.size", "output_dict[].cpu", "output_dict[].cpu", "all", "range", "range"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.models.constituency_parser.SpanConstituencyParser.construct_trees"], []], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.models.constituency_parser.SpanConstituencyParser.construct_trees": [[301, 383], ["all_spans.clone", "constituency_parser.SpanConstituencyParser.vocab.get_token_index", "enumerate", "zip", "zip", "constituency_parser.SpanConstituencyParser.resolve_overlap_conflicts_greedily", "trees.append", "torch.max", "constituency_parser.SpanConstituencyParser.vocab.get_token_from_index", "constituency_parser.SpanConstituencyParser.construct_tree_from_spans", "selected_spans.append", "int", "constituency_parser.SpanInformation", "len", "int", "int", "float", "float", "int"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.nn.util.clone", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.data.vocabulary.Vocabulary.get_token_index", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.models.constituency_parser.SpanConstituencyParser.resolve_overlap_conflicts_greedily", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.data.vocabulary.Vocabulary.get_token_from_index", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.models.constituency_parser.SpanConstituencyParser.construct_tree_from_spans"], []], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.models.constituency_parser.SpanConstituencyParser.resolve_overlap_conflicts_greedily": [[384, 443], ["enumerate", "list", "enumerate", "spans.pop", "spans.pop"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.params.Params.pop", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.params.Params.pop"], []], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.models.constituency_parser.SpanConstituencyParser.construct_tree_from_spans": [[444, 511], ["constituency_parser.SpanConstituencyParser.construct_tree_from_spans.assemble_subtree"], "methods", ["None"], []], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.models.constituency_parser.SpanConstituencyParser.get_metrics": [[512, 520], ["constituency_parser.SpanConstituencyParser.tag_accuracy.get_metric", "constituency_parser.SpanConstituencyParser._evalb_score.get_metric", "all_metrics.update"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.metrics.sequence_accuracy.SequenceAccuracy.get_metric", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.metrics.sequence_accuracy.SequenceAccuracy.get_metric", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.metrics.conll_coref_scores.Scorer.update"], []], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.models.srl_util.write_bio_formatted_tags_to_file": [[4, 48], ["srl_util.convert_bio_tags_to_conll_format", "srl_util.convert_bio_tags_to_conll_format", "srl_util.write_conll_formatted_tags_to_file"], "function", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.models.srl_util.convert_bio_tags_to_conll_format", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.models.srl_util.convert_bio_tags_to_conll_format", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.models.srl_util.write_conll_formatted_tags_to_file"], ["def", "write_bio_formatted_tags_to_file", "(", "\n", "prediction_file", ":", "TextIO", ",", "\n", "gold_file", ":", "TextIO", ",", "\n", "verb_index", ":", "Optional", "[", "int", "]", ",", "\n", "sentence", ":", "List", "[", "str", "]", ",", "\n", "prediction", ":", "List", "[", "str", "]", ",", "\n", "gold_labels", ":", "List", "[", "str", "]", ",", "\n", ")", ":", "\n", "    ", "\"\"\"\n    Prints predicate argument predictions and gold labels for a single verbal\n    predicate in a sentence to two provided file references.\n\n    The CoNLL SRL format is described in\n    [the shared task data README](https://www.lsi.upc.edu/~srlconll/conll05st-release/README).\n\n    This function expects IOB2-formatted tags, where the B- tag is used in the beginning\n    of every chunk (i.e. all chunks start with the B- tag).\n\n    # Parameters\n\n    prediction_file : TextIO, required.\n        A file reference to print predictions to.\n    gold_file : TextIO, required.\n        A file reference to print gold labels to.\n    verb_index : Optional[int], required.\n        The index of the verbal predicate in the sentence which\n        the gold labels are the arguments for, or None if the sentence\n        contains no verbal predicate.\n    sentence : List[str], required.\n        The word tokens.\n    prediction : List[str], required.\n        The predicted BIO labels.\n    gold_labels : List[str], required.\n        The gold BIO labels.\n    \"\"\"", "\n", "conll_formatted_predictions", "=", "convert_bio_tags_to_conll_format", "(", "prediction", ")", "\n", "conll_formatted_gold_labels", "=", "convert_bio_tags_to_conll_format", "(", "gold_labels", ")", "\n", "write_conll_formatted_tags_to_file", "(", "\n", "prediction_file", ",", "\n", "gold_file", ",", "\n", "verb_index", ",", "\n", "sentence", ",", "\n", "conll_formatted_predictions", ",", "\n", "conll_formatted_gold_labels", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.models.srl_util.write_conll_formatted_tags_to_file": [[51, 99], ["zip", "prediction_file.write", "gold_file.write", "len", "prediction_file.write", "prediction_file.write", "gold_file.write", "gold_file.write", "word.ljust", "word.ljust", "predicted.rjust", "gold.rjust"], "function", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.None.predictor.write", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.None.predictor.write", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.None.predictor.write", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.None.predictor.write", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.None.predictor.write", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.None.predictor.write"], ["", "def", "write_conll_formatted_tags_to_file", "(", "\n", "prediction_file", ":", "TextIO", ",", "\n", "gold_file", ":", "TextIO", ",", "\n", "verb_index", ":", "Optional", "[", "int", "]", ",", "\n", "sentence", ":", "List", "[", "str", "]", ",", "\n", "conll_formatted_predictions", ":", "List", "[", "str", "]", ",", "\n", "conll_formatted_gold_labels", ":", "List", "[", "str", "]", ",", "\n", ")", ":", "\n", "    ", "\"\"\"\n    Prints predicate argument predictions and gold labels for a single verbal\n    predicate in a sentence to two provided file references.\n\n    The CoNLL SRL format is described in\n    [the shared task data README](https://www.lsi.upc.edu/~srlconll/conll05st-release/README).\n\n    This function expects IOB2-formatted tags, where the B- tag is used in the beginning\n    of every chunk (i.e. all chunks start with the B- tag).\n\n    # Parameters\n\n    prediction_file : TextIO, required.\n        A file reference to print predictions to.\n    gold_file : TextIO, required.\n        A file reference to print gold labels to.\n    verb_index : Optional[int], required.\n        The index of the verbal predicate in the sentence which\n        the gold labels are the arguments for, or None if the sentence\n        contains no verbal predicate.\n    sentence : List[str], required.\n        The word tokens.\n    conll_formatted_predictions : List[str], required.\n        The predicted CoNLL-formatted labels.\n    conll_formatted_gold_labels : List[str], required.\n        The gold CoNLL-formatted labels.\n    \"\"\"", "\n", "verb_only_sentence", "=", "[", "\"-\"", "]", "*", "len", "(", "sentence", ")", "\n", "if", "verb_index", "is", "not", "None", ":", "\n", "        ", "verb_only_sentence", "[", "verb_index", "]", "=", "sentence", "[", "verb_index", "]", "\n", "\n", "", "for", "word", ",", "predicted", ",", "gold", "in", "zip", "(", "\n", "verb_only_sentence", ",", "conll_formatted_predictions", ",", "conll_formatted_gold_labels", "\n", ")", ":", "\n", "        ", "prediction_file", ".", "write", "(", "word", ".", "ljust", "(", "15", ")", ")", "\n", "prediction_file", ".", "write", "(", "predicted", ".", "rjust", "(", "15", ")", "+", "\"\\n\"", ")", "\n", "gold_file", ".", "write", "(", "word", ".", "ljust", "(", "15", ")", ")", "\n", "gold_file", ".", "write", "(", "gold", ".", "rjust", "(", "15", ")", "+", "\"\\n\"", ")", "\n", "", "prediction_file", ".", "write", "(", "\"\\n\"", ")", "\n", "gold_file", ".", "write", "(", "\"\\n\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.models.srl_util.convert_bio_tags_to_conll_format": [[101, 145], ["len", "enumerate", "conll_labels.append", "conll_labels.append"], "function", ["None"], ["", "def", "convert_bio_tags_to_conll_format", "(", "labels", ":", "List", "[", "str", "]", ")", ":", "\n", "    ", "\"\"\"\n    Converts BIO formatted SRL tags to the format required for evaluation with the\n    official CONLL 2005 perl script. Spans are represented by bracketed labels,\n    with the labels of words inside spans being the same as those outside spans.\n    Beginning spans always have a opening bracket and a closing asterisk (e.g. \"(ARG-1*\" )\n    and closing spans always have a closing bracket (e.g. \"*)\" ). This applies even for\n    length 1 spans, (e.g \"(ARG-0*)\").\n\n    A full example of the conversion performed:\n\n    [B-ARG-1, I-ARG-1, I-ARG-1, I-ARG-1, I-ARG-1, O]\n    [ \"(ARG-1*\", \"*\", \"*\", \"*\", \"*)\", \"*\"]\n\n    # Parameters\n\n    labels : List[str], required.\n        A list of BIO tags to convert to the CONLL span based format.\n\n    # Returns\n\n    A list of labels in the CONLL span based format.\n    \"\"\"", "\n", "sentence_length", "=", "len", "(", "labels", ")", "\n", "conll_labels", "=", "[", "]", "\n", "for", "i", ",", "label", "in", "enumerate", "(", "labels", ")", ":", "\n", "        ", "if", "label", "==", "\"O\"", ":", "\n", "            ", "conll_labels", ".", "append", "(", "\"*\"", ")", "\n", "continue", "\n", "", "new_label", "=", "\"*\"", "\n", "# Are we at the beginning of a new span, at the first word in the sentence,", "\n", "# or is the label different from the previous one? If so, we are seeing a new label.", "\n", "if", "label", "[", "0", "]", "==", "\"B\"", "or", "i", "==", "0", "or", "label", "[", "1", ":", "]", "!=", "labels", "[", "i", "-", "1", "]", "[", "1", ":", "]", ":", "\n", "            ", "new_label", "=", "\"(\"", "+", "label", "[", "2", ":", "]", "+", "new_label", "\n", "# Are we at the end of the sentence, is the next word a new span, or is the next", "\n", "# word not in a span? If so, we need to close the label span.", "\n", "", "if", "(", "\n", "i", "==", "sentence_length", "-", "1", "\n", "or", "labels", "[", "i", "+", "1", "]", "[", "0", "]", "==", "\"B\"", "\n", "or", "label", "[", "1", ":", "]", "!=", "labels", "[", "i", "+", "1", "]", "[", "1", ":", "]", "\n", ")", ":", "\n", "            ", "new_label", "=", "new_label", "+", "\")\"", "\n", "", "conll_labels", ".", "append", "(", "new_label", ")", "\n", "", "return", "conll_labels", "\n", "", ""]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.models.esim.ESIM.__init__": [[55, 114], ["allennlp.nn.InitializerApplicator", "allennlp.models.model.Model.__init__", "allennlp.modules.matrix_attention.legacy_matrix_attention.LegacyMatrixAttention", "vocab.get_vocab_size", "allennlp.common.checks.check_dimensions_match", "allennlp.common.checks.check_dimensions_match", "allennlp.common.checks.check_dimensions_match", "allennlp.training.metrics.CategoricalAccuracy", "torch.nn.CrossEntropyLoss", "initializer", "torch.nn.Dropout", "allennlp.modules.InputVariationalDropout", "text_field_embedder.get_output_dim", "encoder.get_input_dim", "projection_feedforward.get_input_dim", "projection_feedforward.get_output_dim", "inference_encoder.get_input_dim", "encoder.get_output_dim"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.graph_util.plot.GraphPlot.__init__", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.data.vocabulary.Vocabulary.get_vocab_size", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.checks.check_dimensions_match", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.checks.check_dimensions_match", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.checks.check_dimensions_match", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.seq2seq_decoders.auto_regressive_seq_decoder.AutoRegressiveSeqDecoder.get_output_dim", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.seq2vec_encoders.cnn_highway_encoder.CnnHighwayEncoder.get_input_dim", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.seq2vec_encoders.cnn_highway_encoder.CnnHighwayEncoder.get_input_dim", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.seq2seq_decoders.auto_regressive_seq_decoder.AutoRegressiveSeqDecoder.get_output_dim", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.seq2vec_encoders.cnn_highway_encoder.CnnHighwayEncoder.get_input_dim", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.seq2seq_decoders.auto_regressive_seq_decoder.AutoRegressiveSeqDecoder.get_output_dim"], ["def", "__init__", "(", "\n", "self", ",", "\n", "vocab", ":", "Vocabulary", ",", "\n", "text_field_embedder", ":", "TextFieldEmbedder", ",", "\n", "encoder", ":", "Seq2SeqEncoder", ",", "\n", "similarity_function", ":", "SimilarityFunction", ",", "\n", "projection_feedforward", ":", "FeedForward", ",", "\n", "inference_encoder", ":", "Seq2SeqEncoder", ",", "\n", "output_feedforward", ":", "FeedForward", ",", "\n", "output_logit", ":", "FeedForward", ",", "\n", "dropout", ":", "float", "=", "0.5", ",", "\n", "initializer", ":", "InitializerApplicator", "=", "InitializerApplicator", "(", ")", ",", "\n", "**", "kwargs", ",", "\n", ")", "->", "None", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "vocab", ",", "**", "kwargs", ")", "\n", "\n", "self", ".", "_text_field_embedder", "=", "text_field_embedder", "\n", "self", ".", "_encoder", "=", "encoder", "\n", "\n", "self", ".", "_matrix_attention", "=", "LegacyMatrixAttention", "(", "similarity_function", ")", "\n", "self", ".", "_projection_feedforward", "=", "projection_feedforward", "\n", "\n", "self", ".", "_inference_encoder", "=", "inference_encoder", "\n", "\n", "if", "dropout", ":", "\n", "            ", "self", ".", "dropout", "=", "torch", ".", "nn", ".", "Dropout", "(", "dropout", ")", "\n", "self", ".", "rnn_input_dropout", "=", "InputVariationalDropout", "(", "dropout", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "dropout", "=", "None", "\n", "self", ".", "rnn_input_dropout", "=", "None", "\n", "\n", "", "self", ".", "_output_feedforward", "=", "output_feedforward", "\n", "self", ".", "_output_logit", "=", "output_logit", "\n", "\n", "self", ".", "_num_labels", "=", "vocab", ".", "get_vocab_size", "(", "namespace", "=", "\"labels\"", ")", "\n", "\n", "check_dimensions_match", "(", "\n", "text_field_embedder", ".", "get_output_dim", "(", ")", ",", "\n", "encoder", ".", "get_input_dim", "(", ")", ",", "\n", "\"text field embedding dim\"", ",", "\n", "\"encoder input dim\"", ",", "\n", ")", "\n", "check_dimensions_match", "(", "\n", "encoder", ".", "get_output_dim", "(", ")", "*", "4", ",", "\n", "projection_feedforward", ".", "get_input_dim", "(", ")", ",", "\n", "\"encoder output dim\"", ",", "\n", "\"projection feedforward input\"", ",", "\n", ")", "\n", "check_dimensions_match", "(", "\n", "projection_feedforward", ".", "get_output_dim", "(", ")", ",", "\n", "inference_encoder", ".", "get_input_dim", "(", ")", ",", "\n", "\"proj feedforward output dim\"", ",", "\n", "\"inference lstm input dim\"", ",", "\n", ")", "\n", "\n", "self", ".", "_accuracy", "=", "CategoricalAccuracy", "(", ")", "\n", "self", ".", "_loss", "=", "torch", ".", "nn", ".", "CrossEntropyLoss", "(", ")", "\n", "\n", "initializer", "(", "self", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.models.esim.ESIM.forward": [[115, 252], ["esim.ESIM._text_field_embedder", "esim.ESIM._text_field_embedder", "allennlp.nn.util.get_text_field_mask().float", "allennlp.nn.util.get_text_field_mask().float", "esim.ESIM._encoder", "esim.ESIM._encoder", "esim.ESIM._matrix_attention", "allennlp.nn.util.masked_softmax", "allennlp.nn.util.weighted_sum", "allennlp.nn.util.masked_softmax", "allennlp.nn.util.weighted_sum", "torch.cat", "torch.cat", "esim.ESIM._projection_feedforward", "esim.ESIM._projection_feedforward", "esim.ESIM._inference_encoder", "esim.ESIM._inference_encoder", "allennlp.nn.util.replace_masked_values().max", "allennlp.nn.util.replace_masked_values().max", "torch.cat", "esim.ESIM._output_feedforward", "esim.ESIM._output_logit", "torch.nn.functional.softmax", "esim.ESIM.rnn_input_dropout", "esim.ESIM.rnn_input_dropout", "esim.ESIM.transpose().contiguous", "esim.ESIM.rnn_input_dropout", "esim.ESIM.rnn_input_dropout", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "esim.ESIM.dropout", "esim.ESIM._loss", "esim.ESIM._accuracy", "allennlp.nn.util.get_text_field_mask", "allennlp.nn.util.get_text_field_mask", "allennlp.nn.util.replace_masked_values", "allennlp.nn.util.replace_masked_values", "label.long().view", "esim.ESIM.transpose", "allennlp.nn.util.get_text_field_mask().float.unsqueeze", "allennlp.nn.util.get_text_field_mask().float.unsqueeze", "allennlp.nn.util.get_text_field_mask().float.unsqueeze", "allennlp.nn.util.get_text_field_mask().float.unsqueeze", "label.long"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.nn.util.masked_softmax", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.nn.util.weighted_sum", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.nn.util.masked_softmax", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.nn.util.weighted_sum", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.nn.util.get_text_field_mask", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.nn.util.get_text_field_mask", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.nn.util.replace_masked_values", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.nn.util.replace_masked_values"], ["", "def", "forward", "(", "# type: ignore", "\n", "self", ",", "\n", "premise", ":", "TextFieldTensors", ",", "\n", "hypothesis", ":", "TextFieldTensors", ",", "\n", "label", ":", "torch", ".", "IntTensor", "=", "None", ",", "\n", "metadata", ":", "List", "[", "Dict", "[", "str", ",", "Any", "]", "]", "=", "None", ",", "\n", ")", "->", "Dict", "[", "str", ",", "torch", ".", "Tensor", "]", ":", "\n", "\n", "        ", "\"\"\"\n        # Parameters\n\n        premise : TextFieldTensors\n            From a `TextField`\n        hypothesis : TextFieldTensors\n            From a `TextField`\n        label : torch.IntTensor, optional (default = None)\n            From a `LabelField`\n        metadata : `List[Dict[str, Any]]`, optional, (default = None)\n            Metadata containing the original tokenization of the premise and\n            hypothesis with 'premise_tokens' and 'hypothesis_tokens' keys respectively.\n\n        # Returns\n\n        An output dictionary consisting of:\n\n        label_logits : torch.FloatTensor\n            A tensor of shape `(batch_size, num_labels)` representing unnormalised log\n            probabilities of the entailment label.\n        label_probs : torch.FloatTensor\n            A tensor of shape `(batch_size, num_labels)` representing probabilities of the\n            entailment label.\n        loss : torch.FloatTensor, optional\n            A scalar loss to be optimised.\n        \"\"\"", "\n", "embedded_premise", "=", "self", ".", "_text_field_embedder", "(", "premise", ")", "\n", "embedded_hypothesis", "=", "self", ".", "_text_field_embedder", "(", "hypothesis", ")", "\n", "premise_mask", "=", "get_text_field_mask", "(", "premise", ")", ".", "float", "(", ")", "\n", "hypothesis_mask", "=", "get_text_field_mask", "(", "hypothesis", ")", ".", "float", "(", ")", "\n", "\n", "# apply dropout for LSTM", "\n", "if", "self", ".", "rnn_input_dropout", ":", "\n", "            ", "embedded_premise", "=", "self", ".", "rnn_input_dropout", "(", "embedded_premise", ")", "\n", "embedded_hypothesis", "=", "self", ".", "rnn_input_dropout", "(", "embedded_hypothesis", ")", "\n", "\n", "# encode premise and hypothesis", "\n", "", "encoded_premise", "=", "self", ".", "_encoder", "(", "embedded_premise", ",", "premise_mask", ")", "\n", "encoded_hypothesis", "=", "self", ".", "_encoder", "(", "embedded_hypothesis", ",", "hypothesis_mask", ")", "\n", "\n", "# Shape: (batch_size, premise_length, hypothesis_length)", "\n", "similarity_matrix", "=", "self", ".", "_matrix_attention", "(", "encoded_premise", ",", "encoded_hypothesis", ")", "\n", "\n", "# Shape: (batch_size, premise_length, hypothesis_length)", "\n", "p2h_attention", "=", "masked_softmax", "(", "similarity_matrix", ",", "hypothesis_mask", ")", "\n", "# Shape: (batch_size, premise_length, embedding_dim)", "\n", "attended_hypothesis", "=", "weighted_sum", "(", "encoded_hypothesis", ",", "p2h_attention", ")", "\n", "\n", "# Shape: (batch_size, hypothesis_length, premise_length)", "\n", "h2p_attention", "=", "masked_softmax", "(", "\n", "similarity_matrix", ".", "transpose", "(", "1", ",", "2", ")", ".", "contiguous", "(", ")", ",", "premise_mask", "\n", ")", "\n", "# Shape: (batch_size, hypothesis_length, embedding_dim)", "\n", "attended_premise", "=", "weighted_sum", "(", "encoded_premise", ",", "h2p_attention", ")", "\n", "\n", "# the \"enhancement\" layer", "\n", "premise_enhanced", "=", "torch", ".", "cat", "(", "\n", "[", "\n", "encoded_premise", ",", "\n", "attended_hypothesis", ",", "\n", "encoded_premise", "-", "attended_hypothesis", ",", "\n", "encoded_premise", "*", "attended_hypothesis", ",", "\n", "]", ",", "\n", "dim", "=", "-", "1", ",", "\n", ")", "\n", "hypothesis_enhanced", "=", "torch", ".", "cat", "(", "\n", "[", "\n", "encoded_hypothesis", ",", "\n", "attended_premise", ",", "\n", "encoded_hypothesis", "-", "attended_premise", ",", "\n", "encoded_hypothesis", "*", "attended_premise", ",", "\n", "]", ",", "\n", "dim", "=", "-", "1", ",", "\n", ")", "\n", "\n", "# The projection layer down to the model dimension.  Dropout is not applied before", "\n", "# projection.", "\n", "projected_enhanced_premise", "=", "self", ".", "_projection_feedforward", "(", "premise_enhanced", ")", "\n", "projected_enhanced_hypothesis", "=", "self", ".", "_projection_feedforward", "(", "\n", "hypothesis_enhanced", "\n", ")", "\n", "\n", "# Run the inference layer", "\n", "if", "self", ".", "rnn_input_dropout", ":", "\n", "            ", "projected_enhanced_premise", "=", "self", ".", "rnn_input_dropout", "(", "\n", "projected_enhanced_premise", "\n", ")", "\n", "projected_enhanced_hypothesis", "=", "self", ".", "rnn_input_dropout", "(", "\n", "projected_enhanced_hypothesis", "\n", ")", "\n", "", "v_ai", "=", "self", ".", "_inference_encoder", "(", "projected_enhanced_premise", ",", "premise_mask", ")", "\n", "v_bi", "=", "self", ".", "_inference_encoder", "(", "projected_enhanced_hypothesis", ",", "hypothesis_mask", ")", "\n", "\n", "# The pooling layer -- max and avg pooling.", "\n", "# (batch_size, model_dim)", "\n", "v_a_max", ",", "_", "=", "replace_masked_values", "(", "v_ai", ",", "premise_mask", ".", "unsqueeze", "(", "-", "1", ")", ",", "-", "1e7", ")", ".", "max", "(", "\n", "dim", "=", "1", "\n", ")", "\n", "v_b_max", ",", "_", "=", "replace_masked_values", "(", "\n", "v_bi", ",", "hypothesis_mask", ".", "unsqueeze", "(", "-", "1", ")", ",", "-", "1e7", "\n", ")", ".", "max", "(", "dim", "=", "1", ")", "\n", "\n", "v_a_avg", "=", "torch", ".", "sum", "(", "v_ai", "*", "premise_mask", ".", "unsqueeze", "(", "-", "1", ")", ",", "dim", "=", "1", ")", "/", "torch", ".", "sum", "(", "\n", "premise_mask", ",", "1", ",", "keepdim", "=", "True", "\n", ")", "\n", "v_b_avg", "=", "torch", ".", "sum", "(", "v_bi", "*", "hypothesis_mask", ".", "unsqueeze", "(", "-", "1", ")", ",", "dim", "=", "1", ")", "/", "torch", ".", "sum", "(", "\n", "hypothesis_mask", ",", "1", ",", "keepdim", "=", "True", "\n", ")", "\n", "\n", "# Now concat", "\n", "# (batch_size, model_dim * 2 * 4)", "\n", "v_all", "=", "torch", ".", "cat", "(", "[", "v_a_avg", ",", "v_a_max", ",", "v_b_avg", ",", "v_b_max", "]", ",", "dim", "=", "1", ")", "\n", "\n", "# the final MLP -- apply dropout to input, and MLP applies to output & hidden", "\n", "if", "self", ".", "dropout", ":", "\n", "            ", "v_all", "=", "self", ".", "dropout", "(", "v_all", ")", "\n", "\n", "", "output_hidden", "=", "self", ".", "_output_feedforward", "(", "v_all", ")", "\n", "label_logits", "=", "self", ".", "_output_logit", "(", "output_hidden", ")", "\n", "label_probs", "=", "torch", ".", "nn", ".", "functional", ".", "softmax", "(", "label_logits", ",", "dim", "=", "-", "1", ")", "\n", "\n", "output_dict", "=", "{", "\"label_logits\"", ":", "label_logits", ",", "\"label_probs\"", ":", "label_probs", "}", "\n", "\n", "if", "label", "is", "not", "None", ":", "\n", "            ", "loss", "=", "self", ".", "_loss", "(", "label_logits", ",", "label", ".", "long", "(", ")", ".", "view", "(", "-", "1", ")", ")", "\n", "self", ".", "_accuracy", "(", "label_logits", ",", "label", ")", "\n", "output_dict", "[", "\"loss\"", "]", "=", "loss", "\n", "\n", "", "return", "output_dict", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.models.esim.ESIM.get_metrics": [[253, 255], ["esim.ESIM._accuracy.get_metric"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.metrics.sequence_accuracy.SequenceAccuracy.get_metric"], ["", "def", "get_metrics", "(", "self", ",", "reset", ":", "bool", "=", "False", ")", "->", "Dict", "[", "str", ",", "float", "]", ":", "\n", "        ", "return", "{", "\"accuracy\"", ":", "self", ".", "_accuracy", ".", "get_metric", "(", "reset", ")", "}", "\n", "", "", ""]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.util.create_and_set_iterators": [[16, 53], ["params.pop", "params.pop.pop", "allennlp.data.iterators.DataIterator.from_params", "DataIterator.from_params.index_with", "params.pop.pop", "allennlp.data.iterators.DataIterator.from_params", "DataIterator.from_params.index_with", "task.set_data_iterator", "task.set_data_iterator"], "function", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.params.Params.pop", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.params.Params.pop", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.tokenizers.tokenizer.Tokenizer.from_params", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.iterators.data_iterator.DataIterator.index_with", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.params.Params.pop", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.tokenizers.tokenizer.Tokenizer.from_params", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.iterators.data_iterator.DataIterator.index_with", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.tasks.task.Task.set_data_iterator", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.tasks.task.Task.set_data_iterator"], ["def", "create_and_set_iterators", "(", "\n", "params", ":", "Params", ",", "task_list", ":", "List", "[", "Task", "]", ",", "vocab", ":", "Vocabulary", "\n", ")", "->", "List", "[", "Task", "]", ":", "\n", "    ", "\"\"\"\n    Each task/dataset can have its own specific data iterator. If not precised,\n    we use a shared/common data iterator.\n    \n    Parameters\n    ----------\n    params: ``Params``\n        A parameter object specifing an experiment.\n    task_list: ``List[Task]``\n        A list containing the tasks of the model to train.\n        \n    Returns\n    -------\n    task_list: ``List[Task]``\n        The list containing the tasks of the model to train, where each task has a new attribute: the data iterator.\n    \"\"\"", "\n", "### Charge default iterator ###", "\n", "iterators_params", "=", "params", ".", "pop", "(", "\"iterators\"", ")", "\n", "\n", "default_iterator_params", "=", "iterators_params", ".", "pop", "(", "\"iterator\"", ")", "\n", "default_iterator", "=", "DataIterator", ".", "from_params", "(", "default_iterator_params", ")", "\n", "default_iterator", ".", "index_with", "(", "vocab", ")", "\n", "\n", "### Charge dataset specific iterators ###", "\n", "for", "task", "in", "task_list", ":", "\n", "        ", "specific_iterator_params", "=", "iterators_params", ".", "pop", "(", "\"iterator_\"", "+", "task", ".", "_name", ",", "None", ")", "\n", "if", "specific_iterator_params", "is", "not", "None", ":", "\n", "            ", "specific_iterator", "=", "DataIterator", ".", "from_params", "(", "specific_iterator_params", ")", "\n", "specific_iterator", ".", "index_with", "(", "vocab", ")", "\n", "task", ".", "set_data_iterator", "(", "specific_iterator", ")", "\n", "", "else", ":", "\n", "            ", "task", ".", "set_data_iterator", "(", "default_iterator", ")", "\n", "\n", "", "", "return", "task_list", "\n", "", ""]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.plugins.discover_namespace_plugins": [[30, 49], ["importlib.import_module", "pkgutil.iter_modules", "importlib.reload"], "function", ["None"], ["def", "discover_namespace_plugins", "(", "\n", "namespace_name", ":", "str", "=", "\"allennlp_plugins\"", ",", "\n", ")", "->", "Iterable", "[", "pkgutil", ".", "ModuleInfo", "]", ":", "\n", "    ", "\"\"\"\n    Returns an iterable of the plugins found, declared within the namespace package `namespace_name`.\n    \"\"\"", "\n", "try", ":", "\n", "        ", "reload", "=", "namespace_name", "in", "sys", ".", "modules", "\n", "\n", "namespace_module", "=", "importlib", ".", "import_module", "(", "namespace_name", ")", "\n", "\n", "if", "reload", ":", "\n", "            ", "importlib", ".", "reload", "(", "namespace_module", ")", "\n", "\n", "", "return", "pkgutil", ".", "iter_modules", "(", "\n", "namespace_module", ".", "__path__", ",", "namespace_module", ".", "__name__", "+", "\".\"", "# type: ignore", "\n", ")", "\n", "", "except", "ModuleNotFoundError", ":", "\n", "        ", "return", "[", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.plugins.discover_file_plugins": [[51, 63], ["os.path.isfile", "open", "file_.readlines", "module_name.strip.strip"], "function", ["None"], ["", "", "def", "discover_file_plugins", "(", "plugins_filename", ":", "str", "=", "\".allennlp_plugins\"", ")", "->", "Iterable", "[", "str", "]", ":", "\n", "    ", "\"\"\"\n    Returns an iterable of the plugins found, declared within a file whose path is `plugins_filename`.\n    \"\"\"", "\n", "if", "os", ".", "path", ".", "isfile", "(", "plugins_filename", ")", ":", "\n", "        ", "with", "open", "(", "plugins_filename", ")", "as", "file_", ":", "\n", "            ", "for", "module_name", "in", "file_", ".", "readlines", "(", ")", ":", "\n", "                ", "module_name", "=", "module_name", ".", "strip", "(", ")", "\n", "if", "module_name", ":", "\n", "                    ", "yield", "module_name", "\n", "", "", "", "", "else", ":", "\n", "        ", "return", "[", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.plugins.discover_plugins": [[65, 72], ["plugins.discover_namespace_plugins", "plugins.discover_file_plugins"], "function", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.plugins.discover_namespace_plugins", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.plugins.discover_file_plugins"], ["", "", "def", "discover_plugins", "(", ")", "->", "Iterable", "[", "str", "]", ":", "\n", "    ", "\"\"\"\n    Returns an iterable of the plugins found.\n    \"\"\"", "\n", "for", "module_info", "in", "discover_namespace_plugins", "(", ")", ":", "\n", "        ", "yield", "module_info", ".", "name", "\n", "", "yield", "from", "discover_file_plugins", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.plugins.import_plugins": [[74, 83], ["plugins.discover_plugins", "importlib.import_module", "logger.error"], "function", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.plugins.discover_plugins"], ["", "def", "import_plugins", "(", ")", "->", "None", ":", "\n", "    ", "\"\"\"\n    Imports the plugins found with `discover_plugins()`.\n    \"\"\"", "\n", "for", "module_name", "in", "discover_plugins", "(", ")", ":", "\n", "        ", "try", ":", "\n", "            ", "importlib", ".", "import_module", "(", "module_name", ")", "\n", "", "except", "ModuleNotFoundError", "as", "e", ":", "\n", "            ", "logger", ".", "error", "(", "f\"Plugin {module_name} could not be loaded: {e}\"", ")", "\n", "", "", "", ""]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.file_utils.url_to_filename": [[44, 60], ["url.encode", "hashlib.sha256", "hashlib.sha256.hexdigest", "etag.encode", "hashlib.sha256", "hashlib.sha256.hexdigest"], "function", ["None"], ["", "def", "url_to_filename", "(", "url", ":", "str", ",", "etag", ":", "str", "=", "None", ")", "->", "str", ":", "\n", "    ", "\"\"\"\n    Convert `url` into a hashed filename in a repeatable way.\n    If `etag` is specified, append its hash to the url's, delimited\n    by a period.\n    \"\"\"", "\n", "url_bytes", "=", "url", ".", "encode", "(", "\"utf-8\"", ")", "\n", "url_hash", "=", "sha256", "(", "url_bytes", ")", "\n", "filename", "=", "url_hash", ".", "hexdigest", "(", ")", "\n", "\n", "if", "etag", ":", "\n", "        ", "etag_bytes", "=", "etag", ".", "encode", "(", "\"utf-8\"", ")", "\n", "etag_hash", "=", "sha256", "(", "etag_bytes", ")", "\n", "filename", "+=", "\".\"", "+", "etag_hash", ".", "hexdigest", "(", ")", "\n", "\n", "", "return", "filename", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.file_utils.filename_to_url": [[62, 84], ["os.path.join", "os.path.exists", "FileNotFoundError", "os.path.exists", "FileNotFoundError", "open", "json.load"], "function", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.dataset_readers.multiprocess_dataset_reader.QIterable.join", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.token_embedders.bert_token_embedder.PretrainedBertModel.load"], ["", "def", "filename_to_url", "(", "filename", ":", "str", ",", "cache_dir", ":", "str", "=", "None", ")", "->", "Tuple", "[", "str", ",", "str", "]", ":", "\n", "    ", "\"\"\"\n    Return the url and etag (which may be `None`) stored for `filename`.\n    Raise `FileNotFoundError` if `filename` or its stored metadata do not exist.\n    \"\"\"", "\n", "if", "cache_dir", "is", "None", ":", "\n", "        ", "cache_dir", "=", "CACHE_DIRECTORY", "\n", "\n", "", "cache_path", "=", "os", ".", "path", ".", "join", "(", "cache_dir", ",", "filename", ")", "\n", "if", "not", "os", ".", "path", ".", "exists", "(", "cache_path", ")", ":", "\n", "        ", "raise", "FileNotFoundError", "(", "\"file {} not found\"", ".", "format", "(", "cache_path", ")", ")", "\n", "\n", "", "meta_path", "=", "cache_path", "+", "\".json\"", "\n", "if", "not", "os", ".", "path", ".", "exists", "(", "meta_path", ")", ":", "\n", "        ", "raise", "FileNotFoundError", "(", "\"file {} not found\"", ".", "format", "(", "meta_path", ")", ")", "\n", "\n", "", "with", "open", "(", "meta_path", ")", "as", "meta_file", ":", "\n", "        ", "metadata", "=", "json", ".", "load", "(", "meta_file", ")", "\n", "", "url", "=", "metadata", "[", "\"url\"", "]", "\n", "etag", "=", "metadata", "[", "\"etag\"", "]", "\n", "\n", "return", "url", ",", "etag", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.file_utils.cached_path": [[86, 114], ["isinstance", "os.path.expanduser", "urllib.parse.urlparse", "str", "file_utils.get_from_cache", "os.path.exists", "FileNotFoundError", "ValueError"], "function", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.file_utils.get_from_cache"], ["", "def", "cached_path", "(", "url_or_filename", ":", "Union", "[", "str", ",", "Path", "]", ",", "cache_dir", ":", "str", "=", "None", ")", "->", "str", ":", "\n", "    ", "\"\"\"\n    Given something that might be a URL (or might be a local path),\n    determine which. If it's a URL, download the file and cache it, and\n    return the path to the cached file. If it's already a local path,\n    make sure the file exists and then return the path.\n    \"\"\"", "\n", "if", "cache_dir", "is", "None", ":", "\n", "        ", "cache_dir", "=", "CACHE_DIRECTORY", "\n", "", "if", "isinstance", "(", "url_or_filename", ",", "Path", ")", ":", "\n", "        ", "url_or_filename", "=", "str", "(", "url_or_filename", ")", "\n", "\n", "", "url_or_filename", "=", "os", ".", "path", ".", "expanduser", "(", "url_or_filename", ")", "\n", "parsed", "=", "urlparse", "(", "url_or_filename", ")", "\n", "\n", "if", "parsed", ".", "scheme", "in", "(", "\"http\"", ",", "\"https\"", ",", "\"s3\"", ")", ":", "\n", "# URL, so get it from the cache (downloading if necessary)", "\n", "        ", "return", "get_from_cache", "(", "url_or_filename", ",", "cache_dir", ")", "\n", "", "elif", "os", ".", "path", ".", "exists", "(", "url_or_filename", ")", ":", "\n", "# File, and it exists.", "\n", "        ", "return", "url_or_filename", "\n", "", "elif", "parsed", ".", "scheme", "==", "\"\"", ":", "\n", "# File, but it doesn't exist.", "\n", "        ", "raise", "FileNotFoundError", "(", "\"file {} not found\"", ".", "format", "(", "url_or_filename", ")", ")", "\n", "", "else", ":", "\n", "# Something unknown", "\n", "        ", "raise", "ValueError", "(", "\n", "\"unable to parse {} as a URL or as a local path\"", ".", "format", "(", "url_or_filename", ")", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.file_utils.is_url_or_existing_file": [[117, 127], ["os.path.expanduser", "urllib.parse.urlparse", "str", "os.path.exists"], "function", ["None"], ["", "", "def", "is_url_or_existing_file", "(", "url_or_filename", ":", "Union", "[", "str", ",", "Path", ",", "None", "]", ")", "->", "bool", ":", "\n", "    ", "\"\"\"\n    Given something that might be a URL (or might be a local path),\n    determine check if it's url or an existing file path.\n    \"\"\"", "\n", "if", "url_or_filename", "is", "None", ":", "\n", "        ", "return", "False", "\n", "", "url_or_filename", "=", "os", ".", "path", ".", "expanduser", "(", "str", "(", "url_or_filename", ")", ")", "\n", "parsed", "=", "urlparse", "(", "url_or_filename", ")", "\n", "return", "parsed", ".", "scheme", "in", "(", "\"http\"", ",", "\"https\"", ",", "\"s3\"", ")", "or", "os", ".", "path", ".", "exists", "(", "url_or_filename", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.file_utils.split_s3_path": [[129, 140], ["urllib.parse.urlparse", "s3_path.startswith", "ValueError"], "function", ["None"], ["", "def", "split_s3_path", "(", "url", ":", "str", ")", "->", "Tuple", "[", "str", ",", "str", "]", ":", "\n", "    ", "\"\"\"Split a full s3 path into the bucket name and path.\"\"\"", "\n", "parsed", "=", "urlparse", "(", "url", ")", "\n", "if", "not", "parsed", ".", "netloc", "or", "not", "parsed", ".", "path", ":", "\n", "        ", "raise", "ValueError", "(", "\"bad s3 path {}\"", ".", "format", "(", "url", ")", ")", "\n", "", "bucket_name", "=", "parsed", ".", "netloc", "\n", "s3_path", "=", "parsed", ".", "path", "\n", "# Remove '/' at beginning of path.", "\n", "if", "s3_path", ".", "startswith", "(", "\"/\"", ")", ":", "\n", "        ", "s3_path", "=", "s3_path", "[", "1", ":", "]", "\n", "", "return", "bucket_name", ",", "s3_path", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.file_utils.s3_request": [[142, 159], ["functools.wraps", "func", "int", "FileNotFoundError"], "function", ["None"], ["", "def", "s3_request", "(", "func", ":", "Callable", ")", ":", "\n", "    ", "\"\"\"\n    Wrapper function for s3 requests in order to create more helpful error\n    messages.\n    \"\"\"", "\n", "\n", "@", "wraps", "(", "func", ")", "\n", "def", "wrapper", "(", "url", ":", "str", ",", "*", "args", ",", "**", "kwargs", ")", ":", "\n", "        ", "try", ":", "\n", "            ", "return", "func", "(", "url", ",", "*", "args", ",", "**", "kwargs", ")", "\n", "", "except", "ClientError", "as", "exc", ":", "\n", "            ", "if", "int", "(", "exc", ".", "response", "[", "\"Error\"", "]", "[", "\"Code\"", "]", ")", "==", "404", ":", "\n", "                ", "raise", "FileNotFoundError", "(", "\"file {} not found\"", ".", "format", "(", "url", ")", ")", "\n", "", "else", ":", "\n", "                ", "raise", "\n", "\n", "", "", "", "return", "wrapper", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.file_utils.get_s3_resource": [[161, 171], ["boto3.session.Session", "boto3.session.Session.get_credentials", "boto3.session.Session.resource", "boto3.session.Session.resource", "botocore.client.Config"], "function", ["None"], ["", "def", "get_s3_resource", "(", ")", ":", "\n", "    ", "session", "=", "boto3", ".", "session", ".", "Session", "(", ")", "\n", "if", "session", ".", "get_credentials", "(", ")", "is", "None", ":", "\n", "# Use unsigned requests.", "\n", "        ", "s3_resource", "=", "session", ".", "resource", "(", "\n", "\"s3\"", ",", "config", "=", "botocore", ".", "client", ".", "Config", "(", "signature_version", "=", "botocore", ".", "UNSIGNED", ")", "\n", ")", "\n", "", "else", ":", "\n", "        ", "s3_resource", "=", "session", ".", "resource", "(", "\"s3\"", ")", "\n", "", "return", "s3_resource", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.file_utils.s3_etag": [[173, 180], ["file_utils.get_s3_resource", "file_utils.split_s3_path", "get_s3_resource.Object"], "function", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.file_utils.get_s3_resource", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.file_utils.split_s3_path"], ["", "@", "s3_request", "\n", "def", "s3_etag", "(", "url", ":", "str", ")", "->", "Optional", "[", "str", "]", ":", "\n", "    ", "\"\"\"Check ETag on S3 object.\"\"\"", "\n", "s3_resource", "=", "get_s3_resource", "(", ")", "\n", "bucket_name", ",", "s3_path", "=", "split_s3_path", "(", "url", ")", "\n", "s3_object", "=", "s3_resource", ".", "Object", "(", "bucket_name", ",", "s3_path", ")", "\n", "return", "s3_object", ".", "e_tag", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.file_utils.s3_get": [[182, 188], ["file_utils.get_s3_resource", "file_utils.split_s3_path", "get_s3_resource.Bucket().download_fileobj", "get_s3_resource.Bucket"], "function", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.file_utils.get_s3_resource", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.file_utils.split_s3_path"], ["", "@", "s3_request", "\n", "def", "s3_get", "(", "url", ":", "str", ",", "temp_file", ":", "IO", ")", "->", "None", ":", "\n", "    ", "\"\"\"Pull a file directly from S3.\"\"\"", "\n", "s3_resource", "=", "get_s3_resource", "(", ")", "\n", "bucket_name", ",", "s3_path", "=", "split_s3_path", "(", "url", ")", "\n", "s3_resource", ".", "Bucket", "(", "bucket_name", ")", ".", "download_fileobj", "(", "s3_path", ",", "temp_file", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.file_utils.session_with_backoff": [[190, 204], ["requests.Session", "requests.packages.urllib3.util.retry.Retry", "requests.Session.mount", "requests.Session.mount", "requests.adapters.HTTPAdapter", "requests.adapters.HTTPAdapter"], "function", ["None"], ["", "def", "session_with_backoff", "(", ")", "->", "requests", ".", "Session", ":", "\n", "    ", "\"\"\"\n    We ran into an issue where http requests to s3 were timing out,\n    possibly because we were making too many requests too quickly.\n    This helper function returns a requests session that has retry-with-backoff\n    built in.\n    see stackoverflow.com/questions/23267409/how-to-implement-retry-mechanism-into-python-requests-library\n    \"\"\"", "\n", "session", "=", "requests", ".", "Session", "(", ")", "\n", "retries", "=", "Retry", "(", "total", "=", "5", ",", "backoff_factor", "=", "1", ",", "status_forcelist", "=", "[", "502", ",", "503", ",", "504", "]", ")", "\n", "session", ".", "mount", "(", "\"http://\"", ",", "HTTPAdapter", "(", "max_retries", "=", "retries", ")", ")", "\n", "session", ".", "mount", "(", "\"https://\"", ",", "HTTPAdapter", "(", "max_retries", "=", "retries", ")", ")", "\n", "\n", "return", "session", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.file_utils.http_get": [[206, 217], ["file_utils.session_with_backoff", "session.get", "session.get.headers.get", "allennlp.common.tqdm.Tqdm.tqdm", "session.get.iter_content", "Tqdm.tqdm.close", "int", "Tqdm.tqdm.update", "temp_file.write", "len"], "function", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.file_utils.session_with_backoff", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.params.Params.get", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.params.Params.get", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.tqdm.Tqdm.tqdm", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.token_embedders.embedding.EmbeddingsTextFile.close", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.metrics.conll_coref_scores.Scorer.update", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.None.predictor.write"], ["", "def", "http_get", "(", "url", ":", "str", ",", "temp_file", ":", "IO", ")", "->", "None", ":", "\n", "    ", "with", "session_with_backoff", "(", ")", "as", "session", ":", "\n", "        ", "req", "=", "session", ".", "get", "(", "url", ",", "stream", "=", "True", ")", "\n", "content_length", "=", "req", ".", "headers", ".", "get", "(", "\"Content-Length\"", ")", "\n", "total", "=", "int", "(", "content_length", ")", "if", "content_length", "is", "not", "None", "else", "None", "\n", "progress", "=", "Tqdm", ".", "tqdm", "(", "unit", "=", "\"B\"", ",", "total", "=", "total", ")", "\n", "for", "chunk", "in", "req", ".", "iter_content", "(", "chunk_size", "=", "1024", ")", ":", "\n", "            ", "if", "chunk", ":", "# filter out keep-alive new chunks", "\n", "                ", "progress", ".", "update", "(", "len", "(", "chunk", ")", ")", "\n", "temp_file", ".", "write", "(", "chunk", ")", "\n", "", "", "progress", ".", "close", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.file_utils.get_from_cache": [[220, 279], ["os.makedirs", "url.startswith", "file_utils.url_to_filename", "os.path.join", "file_utils.s3_etag", "session.head.headers.get", "os.path.exists", "file_utils.session_with_backoff", "session.head", "IOError", "tempfile.NamedTemporaryFile", "logger.info", "url.startswith", "temp_file.flush", "temp_file.seek", "logger.info", "logger.info", "logger.info", "file_utils.s3_get", "file_utils.http_get", "open", "shutil.copyfileobj", "open", "json.dump"], "function", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.file_utils.url_to_filename", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.dataset_readers.multiprocess_dataset_reader.QIterable.join", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.file_utils.s3_etag", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.params.Params.get", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.file_utils.session_with_backoff", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.dataset_readers.multiprocess_dataset_reader.logger.info", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.dataset_readers.multiprocess_dataset_reader.logger.info", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.dataset_readers.multiprocess_dataset_reader.logger.info", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.dataset_readers.multiprocess_dataset_reader.logger.info", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.file_utils.s3_get", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.file_utils.http_get"], ["", "", "def", "get_from_cache", "(", "url", ":", "str", ",", "cache_dir", ":", "str", "=", "None", ")", "->", "str", ":", "\n", "    ", "\"\"\"\n    Given a URL, look for the corresponding dataset in the local cache.\n    If it's not there, download it. Then return the path to the cached file.\n    \"\"\"", "\n", "if", "cache_dir", "is", "None", ":", "\n", "        ", "cache_dir", "=", "CACHE_DIRECTORY", "\n", "\n", "", "os", ".", "makedirs", "(", "cache_dir", ",", "exist_ok", "=", "True", ")", "\n", "\n", "# Get eTag to add to filename, if it exists.", "\n", "if", "url", ".", "startswith", "(", "\"s3://\"", ")", ":", "\n", "        ", "etag", "=", "s3_etag", "(", "url", ")", "\n", "", "else", ":", "\n", "        ", "with", "session_with_backoff", "(", ")", "as", "session", ":", "\n", "            ", "response", "=", "session", ".", "head", "(", "url", ",", "allow_redirects", "=", "True", ")", "\n", "", "if", "response", ".", "status_code", "!=", "200", ":", "\n", "            ", "raise", "IOError", "(", "\n", "\"HEAD request failed for url {} with status code {}\"", ".", "format", "(", "\n", "url", ",", "response", ".", "status_code", "\n", ")", "\n", ")", "\n", "", "etag", "=", "response", ".", "headers", ".", "get", "(", "\"ETag\"", ")", "\n", "\n", "", "filename", "=", "url_to_filename", "(", "url", ",", "etag", ")", "\n", "\n", "# get cache path to put the file", "\n", "cache_path", "=", "os", ".", "path", ".", "join", "(", "cache_dir", ",", "filename", ")", "\n", "\n", "if", "not", "os", ".", "path", ".", "exists", "(", "cache_path", ")", ":", "\n", "# Download to temporary file, then copy to cache dir once finished.", "\n", "# Otherwise you get corrupt cache entries if the download gets interrupted.", "\n", "        ", "with", "tempfile", ".", "NamedTemporaryFile", "(", ")", "as", "temp_file", ":", "\n", "            ", "logger", ".", "info", "(", "\"%s not found in cache, downloading to %s\"", ",", "url", ",", "temp_file", ".", "name", ")", "\n", "\n", "# GET file object", "\n", "if", "url", ".", "startswith", "(", "\"s3://\"", ")", ":", "\n", "                ", "s3_get", "(", "url", ",", "temp_file", ")", "\n", "", "else", ":", "\n", "                ", "http_get", "(", "url", ",", "temp_file", ")", "\n", "\n", "# we are copying the file before closing it, so flush to avoid truncation", "\n", "", "temp_file", ".", "flush", "(", ")", "\n", "# shutil.copyfileobj() starts at the current position, so go to the start", "\n", "temp_file", ".", "seek", "(", "0", ")", "\n", "\n", "logger", ".", "info", "(", "\"copying %s to cache at %s\"", ",", "temp_file", ".", "name", ",", "cache_path", ")", "\n", "with", "open", "(", "cache_path", ",", "\"wb\"", ")", "as", "cache_file", ":", "\n", "                ", "shutil", ".", "copyfileobj", "(", "temp_file", ",", "cache_file", ")", "\n", "\n", "", "logger", ".", "info", "(", "\"creating metadata file for %s\"", ",", "cache_path", ")", "\n", "meta", "=", "{", "\"url\"", ":", "url", ",", "\"etag\"", ":", "etag", "}", "\n", "meta_path", "=", "cache_path", "+", "\".json\"", "\n", "with", "open", "(", "meta_path", ",", "\"w\"", ")", "as", "meta_file", ":", "\n", "                ", "json", ".", "dump", "(", "meta", ",", "meta_file", ")", "\n", "\n", "", "logger", ".", "info", "(", "\"removing temp file %s\"", ",", "temp_file", ".", "name", ")", "\n", "\n", "", "", "return", "cache_path", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.file_utils.read_set_from_file": [[281, 291], ["set", "open", "set.add", "line.rstrip"], "function", ["None"], ["", "def", "read_set_from_file", "(", "filename", ":", "str", ")", "->", "Set", "[", "str", "]", ":", "\n", "    ", "\"\"\"\n    Extract a de-duped collection (set) of text from a file.\n    Expected file format is one item per line.\n    \"\"\"", "\n", "collection", "=", "set", "(", ")", "\n", "with", "open", "(", "filename", ",", "\"r\"", ")", "as", "file_", ":", "\n", "        ", "for", "line", "in", "file_", ":", "\n", "            ", "collection", ".", "add", "(", "line", ".", "rstrip", "(", ")", ")", "\n", "", "", "return", "collection", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.file_utils.get_file_extension": [[293, 297], ["os.path.splitext", "ext.lower"], "function", ["None"], ["", "def", "get_file_extension", "(", "path", ":", "str", ",", "dot", "=", "True", ",", "lower", ":", "bool", "=", "True", ")", ":", "\n", "    ", "ext", "=", "os", ".", "path", ".", "splitext", "(", "path", ")", "[", "1", "]", "\n", "ext", "=", "ext", "if", "dot", "else", "ext", "[", "1", ":", "]", "\n", "return", "ext", ".", "lower", "(", ")", "if", "lower", "else", "ext", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.file_utils.open_compressed": [[299, 318], ["isinstance", "str.endswith", "open_fn", "str", "str.endswith"], "function", ["None"], ["", "def", "open_compressed", "(", "\n", "filename", ":", "Union", "[", "str", ",", "Path", "]", ",", "\n", "mode", ":", "str", "=", "\"rt\"", ",", "\n", "encoding", ":", "Optional", "[", "str", "]", "=", "\"UTF-8\"", ",", "\n", "**", "kwargs", ",", "\n", ")", ":", "\n", "    ", "if", "isinstance", "(", "filename", ",", "Path", ")", ":", "\n", "        ", "filename", "=", "str", "(", "filename", ")", "\n", "", "open_fn", ":", "Callable", "=", "open", "\n", "\n", "if", "filename", ".", "endswith", "(", "\".gz\"", ")", ":", "\n", "        ", "import", "gzip", "\n", "\n", "open_fn", "=", "gzip", ".", "open", "\n", "", "elif", "filename", ".", "endswith", "(", "\".bz2\"", ")", ":", "\n", "        ", "import", "bz2", "\n", "\n", "open_fn", "=", "bz2", ".", "open", "\n", "", "return", "open_fn", "(", "filename", ",", "mode", "=", "mode", ",", "encoding", "=", "encoding", ",", "**", "kwargs", ")", "\n", "", ""]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.checks.ConfigurationError.__init__": [[22, 25], ["Exception.__init__"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.graph_util.plot.GraphPlot.__init__"], ["def", "__init__", "(", "self", ",", "message", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "message", "=", "message", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.checks.ConfigurationError.__str__": [[26, 30], ["None"], "methods", ["None"], ["", "def", "__str__", "(", "self", ")", ":", "\n", "# TODO(brendanr): Is there some reason why we need repr here? It", "\n", "# produces horrible output for simple multi-line error messages.", "\n", "        ", "return", "self", ".", "message", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.checks.log_pytorch_version_info": [[41, 45], ["logger.info"], "function", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.dataset_readers.multiprocess_dataset_reader.logger.info"], ["", "def", "log_pytorch_version_info", "(", ")", ":", "\n", "    ", "import", "torch", "\n", "\n", "logger", ".", "info", "(", "\"Pytorch version: %s\"", ",", "torch", ".", "__version__", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.checks.check_dimensions_match": [[47, 53], ["checks.ConfigurationError"], "function", ["None"], ["", "def", "check_dimensions_match", "(", "\n", "dimension_1", ":", "int", ",", "dimension_2", ":", "int", ",", "dim_1_name", ":", "str", ",", "dim_2_name", ":", "str", "\n", ")", "->", "None", ":", "\n", "    ", "if", "dimension_1", "!=", "dimension_2", ":", "\n", "        ", "raise", "ConfigurationError", "(", "\n", "f\"{dim_1_name} must match {dim_2_name}, but got {dimension_1} \"", "\n", "f\"and {dimension_2} instead\"", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.checks.parse_cuda_device": [[57, 101], ["isinstance", "checks.parse_cuda_device.from_list"], "function", ["None"], ["", "", "def", "parse_cuda_device", "(", "cuda_device", ":", "Union", "[", "str", ",", "int", ",", "List", "[", "int", "]", "]", ")", "->", "int", ":", "\n", "    ", "\"\"\"\n    Disambiguates single GPU and multiple GPU settings for cuda_device param.\n    \"\"\"", "\n", "\n", "message", "=", "\"\"\"\n    In allennlp 1.0, the Trainer cannot be passed multiple cuda devices.\n    Instead, use the faster Distributed Data Parallel. For instance, if you previously had config like:\n        {\n          \"trainer\": {\n            \"cuda_device\": [0, 1, 2, 3],\n            \"num_epochs\": 20,\n            ...\n          }\n        }\n        simply change it to:\n        {\n          \"distributed\": {\n            \"cuda_devices\": [0, 1, 2, 3],\n          },\n          \"trainer\": {\n            \"num_epochs\": 20,\n            ...\n          }\n        }\n        \"\"\"", "\n", "\n", "def", "from_list", "(", "strings", ")", ":", "\n", "        ", "if", "len", "(", "strings", ")", ">", "1", ":", "\n", "            ", "raise", "ConfigurationError", "(", "message", ")", "\n", "", "elif", "len", "(", "strings", ")", "==", "1", ":", "\n", "            ", "return", "int", "(", "strings", "[", "0", "]", ")", "\n", "", "else", ":", "\n", "            ", "return", "-", "1", "\n", "\n", "", "", "if", "isinstance", "(", "cuda_device", ",", "str", ")", ":", "\n", "        ", "return", "from_list", "(", "re", ".", "split", "(", "r\",\\s*\"", ",", "cuda_device", ")", ")", "\n", "", "elif", "isinstance", "(", "cuda_device", ",", "int", ")", ":", "\n", "        ", "return", "cuda_device", "\n", "", "elif", "isinstance", "(", "cuda_device", ",", "list", ")", ":", "\n", "        ", "return", "from_list", "(", "cuda_device", ")", "\n", "", "else", ":", "\n", "# TODO(brendanr): Determine why mypy can't tell that this matches the Union.", "\n", "        ", "return", "int", "(", "cuda_device", ")", "# type: ignore", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.checks.check_for_gpu": [[103, 118], ["isinstance", "checks.check_for_gpu", "torch.cuda.device_count", "checks.ConfigurationError", "checks.ConfigurationError"], "function", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.checks.check_for_gpu"], ["", "", "def", "check_for_gpu", "(", "device_id", ":", "Union", "[", "int", ",", "List", "[", "int", "]", "]", ")", ":", "\n", "    ", "if", "isinstance", "(", "device_id", ",", "list", ")", ":", "\n", "        ", "for", "did", "in", "device_id", ":", "\n", "            ", "check_for_gpu", "(", "did", ")", "\n", "", "", "elif", "device_id", "is", "not", "None", "and", "device_id", ">=", "0", ":", "\n", "        ", "num_devices_available", "=", "cuda", ".", "device_count", "(", ")", "\n", "if", "num_devices_available", "==", "0", ":", "\n", "            ", "raise", "ConfigurationError", "(", "\n", "\"Experiment specified a GPU but none is available;\"", "\n", "\" if you want to run on CPU use the override\"", "\n", "\" 'trainer.cuda_device=-1' in the json config file.\"", "\n", ")", "\n", "", "elif", "device_id", ">=", "num_devices_available", ":", "\n", "            ", "raise", "ConfigurationError", "(", "\n", "f\"Experiment specified GPU device {device_id}\"", "\n", "f\" but there are only {num_devices_available} devices \"", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.checks.check_for_java": [[123, 131], ["subprocess.check_output", "subprocess.check_output.decode"], "function", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.coreference_resolution.coref.CoreferenceResolver.decode"], ["", "", "", "def", "check_for_java", "(", ")", "->", "bool", ":", "\n", "    ", "try", ":", "\n", "        ", "java_version", "=", "subprocess", ".", "check_output", "(", "\n", "[", "\"java\"", ",", "\"-version\"", "]", ",", "stderr", "=", "subprocess", ".", "STDOUT", "\n", ")", "\n", "return", "\"version\"", "in", "java_version", ".", "decode", "(", ")", "\n", "", "except", "FileNotFoundError", ":", "\n", "        ", "return", "False", "\n", "", "", ""]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.from_params.FromParams.from_params": [[485, 591], ["logger.info", "isinstance", "Registrable._registry.get", "allennlp.common.params.Params", "typing.cast", "allennlp.common.params.Params.pop_choice", "typing.cast.resolve_class_name", "hasattr", "constructor_to_call", "getattr", "from_params.create_extras", "typing.cast", "typing.cast.from_params", "from_params.create_extras", "subclass", "from_params.create_kwargs", "getattr", "set", "typing.cast.list_available", "create_extras.keys"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.dataset_readers.multiprocess_dataset_reader.logger.info", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.params.Params.get", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.params.pop_choice", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.registrable.Registrable.resolve_class_name", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.from_params.create_extras", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.tokenizers.tokenizer.Tokenizer.from_params", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.from_params.create_extras", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.from_params.create_kwargs", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.registrable.Registrable.list_available"], ["@", "classmethod", "\n", "def", "from_params", "(", "\n", "cls", ":", "Type", "[", "T", "]", ",", "\n", "params", ":", "Params", ",", "\n", "constructor_to_call", ":", "Callable", "[", "...", ",", "T", "]", "=", "None", ",", "\n", "constructor_to_inspect", ":", "Callable", "[", "...", ",", "T", "]", "=", "None", ",", "\n", "**", "extras", ",", "\n", ")", "->", "T", ":", "\n", "        ", "\"\"\"\n        This is the automatic implementation of `from_params`. Any class that subclasses\n        `FromParams` (or `Registrable`, which itself subclasses `FromParams`) gets this\n        implementation for free.  If you want your class to be instantiated from params in the\n        \"obvious\" way -- pop off parameters and hand them to your constructor with the same names --\n        this provides that functionality.\n\n        If you need more complex logic in your from `from_params` method, you'll have to implement\n        your own method that overrides this one.\n\n        The `constructor_to_call` and `constructor_to_inspect` arguments deal with a bit of\n        redirection that we do.  We allow you to register particular `@classmethods` on a class as\n        the constructor to use for a registered name.  This lets you, e.g., have a single\n        `Vocabulary` class that can be constructed in two different ways, with different names\n        registered to each constructor.  In order to handle this, we need to know not just the class\n        we're trying to construct (`cls`), but also what method we should inspect to find its\n        arguments (`constructor_to_inspect`), and what method to call when we're done constructing\n        arguments (`constructor_to_call`).  These two methods are the same when you've used a\n        `@classmethod` as your constructor, but they are `different` when you use the default\n        constructor (because you inspect `__init__`, but call `cls()`).\n        \"\"\"", "\n", "\n", "from", "allennlp", ".", "common", ".", "registrable", "import", "(", "\n", "Registrable", ",", "\n", ")", "# import here to avoid circular imports", "\n", "\n", "logger", ".", "info", "(", "\n", "f\"instantiating class {cls} from params {getattr(params, 'params', params)} \"", "\n", "f\"and extras {set(extras.keys())}\"", "\n", ")", "\n", "\n", "if", "params", "is", "None", ":", "\n", "            ", "return", "None", "\n", "\n", "", "if", "isinstance", "(", "params", ",", "str", ")", ":", "\n", "            ", "params", "=", "Params", "(", "{", "\"type\"", ":", "params", "}", ")", "\n", "\n", "", "registered_subclasses", "=", "Registrable", ".", "_registry", ".", "get", "(", "cls", ")", "\n", "\n", "if", "registered_subclasses", "is", "not", "None", "and", "not", "constructor_to_call", ":", "\n", "# We know `cls` inherits from Registrable, so we'll use a cast to make mypy happy.", "\n", "\n", "            ", "as_registrable", "=", "cast", "(", "Type", "[", "Registrable", "]", ",", "cls", ")", "\n", "default_to_first_choice", "=", "as_registrable", ".", "default_implementation", "is", "not", "None", "\n", "choice", "=", "params", ".", "pop_choice", "(", "\n", "\"type\"", ",", "\n", "choices", "=", "as_registrable", ".", "list_available", "(", ")", ",", "\n", "default_to_first_choice", "=", "default_to_first_choice", ",", "\n", ")", "\n", "subclass", ",", "constructor_name", "=", "as_registrable", ".", "resolve_class_name", "(", "choice", ")", "\n", "# See the docstring for an explanation of what's going on here.", "\n", "if", "not", "constructor_name", ":", "\n", "                ", "constructor_to_inspect", "=", "subclass", ".", "__init__", "\n", "constructor_to_call", "=", "subclass", "# type: ignore", "\n", "", "else", ":", "\n", "                ", "constructor_to_inspect", "=", "getattr", "(", "subclass", ",", "constructor_name", ")", "\n", "constructor_to_call", "=", "constructor_to_inspect", "\n", "\n", "", "if", "hasattr", "(", "subclass", ",", "\"from_params\"", ")", ":", "\n", "# We want to call subclass.from_params.", "\n", "                ", "extras", "=", "create_extras", "(", "subclass", ",", "extras", ")", "\n", "# mypy can't follow the typing redirection that we do, so we explicitly cast here.", "\n", "retyped_subclass", "=", "cast", "(", "Type", "[", "T", "]", ",", "subclass", ")", "\n", "return", "retyped_subclass", ".", "from_params", "(", "\n", "params", "=", "params", ",", "\n", "constructor_to_call", "=", "constructor_to_call", ",", "\n", "constructor_to_inspect", "=", "constructor_to_inspect", ",", "\n", "**", "extras", ",", "\n", ")", "\n", "", "else", ":", "\n", "# In some rare cases, we get a registered subclass that does _not_ have a", "\n", "# from_params method (this happens with Activations, for instance, where we", "\n", "# register pytorch modules directly).  This is a bit of a hack to make those work,", "\n", "# instead of adding a `from_params` method for them somehow.  We just trust that", "\n", "# you've done the right thing in passing your parameters, and nothing else needs to", "\n", "# be recursively constructed.", "\n", "                ", "extras", "=", "create_extras", "(", "subclass", ",", "extras", ")", "\n", "constructor_args", "=", "{", "**", "params", ",", "**", "extras", "}", "\n", "return", "subclass", "(", "**", "constructor_args", ")", "# type: ignore", "\n", "", "", "else", ":", "\n", "# This is not a base class, so convert our params and extras into a dict of kwargs.", "\n", "\n", "# See the docstring for an explanation of what's going on here.", "\n", "            ", "if", "not", "constructor_to_inspect", ":", "\n", "                ", "constructor_to_inspect", "=", "cls", ".", "__init__", "\n", "", "if", "not", "constructor_to_call", ":", "\n", "                ", "constructor_to_call", "=", "cls", "\n", "\n", "", "if", "constructor_to_inspect", "==", "object", ".", "__init__", ":", "\n", "# This class does not have an explicit constructor, so don't give it any kwargs.", "\n", "# Without this logic, create_kwargs will look at object.__init__ and see that", "\n", "# it takes *args and **kwargs and look for those.", "\n", "                ", "kwargs", ":", "Dict", "[", "str", ",", "Any", "]", "=", "{", "}", "\n", "", "else", ":", "\n", "# This class has a constructor, so create kwargs for it.", "\n", "                ", "kwargs", "=", "create_kwargs", "(", "constructor_to_inspect", ",", "cls", ",", "params", ",", "**", "extras", ")", "\n", "\n", "", "return", "constructor_to_call", "(", "**", "kwargs", ")", "# type: ignore", "\n", "", "", "", ""]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.from_params.takes_arg": [[61, 75], ["inspect.isclass", "inspect.signature", "inspect.ismethod", "inspect.isfunction", "inspect.signature", "allennlp.common.checks.ConfigurationError"], "function", ["None"], ["def", "takes_arg", "(", "obj", ",", "arg", ":", "str", ")", "->", "bool", ":", "\n", "    ", "\"\"\"\n    Checks whether the provided obj takes a certain arg.\n    If it's a class, we're really checking whether its constructor does.\n    If it's a function or method, we're checking the object itself.\n    Otherwise, we raise an error.\n    \"\"\"", "\n", "if", "inspect", ".", "isclass", "(", "obj", ")", ":", "\n", "        ", "signature", "=", "inspect", ".", "signature", "(", "obj", ".", "__init__", ")", "\n", "", "elif", "inspect", ".", "ismethod", "(", "obj", ")", "or", "inspect", ".", "isfunction", "(", "obj", ")", ":", "\n", "        ", "signature", "=", "inspect", ".", "signature", "(", "obj", ")", "\n", "", "else", ":", "\n", "        ", "raise", "ConfigurationError", "(", "f\"object {obj} is not callable\"", ")", "\n", "", "return", "arg", "in", "signature", ".", "parameters", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.from_params.takes_kwargs": [[77, 95], ["inspect.isclass", "bool", "inspect.signature", "any", "inspect.ismethod", "inspect.isfunction", "inspect.signature", "allennlp.common.checks.ConfigurationError", "inspect.signature.parameters.values"], "function", ["None"], ["", "def", "takes_kwargs", "(", "obj", ")", "->", "bool", ":", "\n", "    ", "\"\"\"\n    Checks whether a provided object takes in any positional arguments.\n    Similar to takes_arg, we do this for both the __init__ function of\n    the class or a function / method\n    Otherwise, we raise an error\n    \"\"\"", "\n", "if", "inspect", ".", "isclass", "(", "obj", ")", ":", "\n", "        ", "signature", "=", "inspect", ".", "signature", "(", "obj", ".", "__init__", ")", "\n", "", "elif", "inspect", ".", "ismethod", "(", "obj", ")", "or", "inspect", ".", "isfunction", "(", "obj", ")", ":", "\n", "        ", "signature", "=", "inspect", ".", "signature", "(", "obj", ")", "\n", "", "else", ":", "\n", "        ", "raise", "ConfigurationError", "(", "f\"object {obj} is not callable\"", ")", "\n", "", "return", "bool", "(", "\n", "any", "(", "\n", "[", "\n", "p", ".", "kind", "==", "inspect", ".", "Parameter", ".", "VAR_KEYWORD", "# type: ignore", "\n", "for", "p", "in", "signature", ".", "parameters", ".", "values", "(", ")", "\n", "]", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.from_params.can_construct_from_params": [[100, 112], ["getattr", "hasattr", "hasattr", "getattr", "all", "from_params.can_construct_from_params"], "function", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.from_params.can_construct_from_params"], ["", "def", "can_construct_from_params", "(", "type_", ":", "Type", ")", "->", "bool", ":", "\n", "    ", "if", "type_", "in", "[", "str", ",", "int", ",", "float", ",", "bool", "]", ":", "\n", "        ", "return", "True", "\n", "", "origin", "=", "getattr", "(", "type_", ",", "\"__origin__\"", ",", "None", ")", "\n", "if", "origin", "==", "Lazy", ":", "\n", "        ", "return", "True", "\n", "", "elif", "origin", ":", "\n", "        ", "if", "hasattr", "(", "type_", ",", "\"from_params\"", ")", ":", "\n", "            ", "return", "True", "\n", "", "args", "=", "getattr", "(", "type_", ",", "\"__args__\"", ")", "\n", "return", "all", "(", "[", "can_construct_from_params", "(", "arg", ")", "for", "arg", "in", "args", "]", ")", "\n", "", "return", "hasattr", "(", "type_", ",", "\"from_params\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.from_params.remove_optional": [[114, 126], ["getattr", "getattr", "len", "type"], "function", ["None"], ["", "def", "remove_optional", "(", "annotation", ":", "type", ")", ":", "\n", "    ", "\"\"\"\n    Optional[X] annotations are actually represented as Union[X, NoneType].\n    For our purposes, the \"Optional\" part is not interesting, so here we\n    throw it away.\n    \"\"\"", "\n", "origin", "=", "getattr", "(", "annotation", ",", "\"__origin__\"", ",", "None", ")", "\n", "args", "=", "getattr", "(", "annotation", ",", "\"__args__\"", ",", "(", ")", ")", "\n", "if", "origin", "==", "Union", "and", "len", "(", "args", ")", "==", "2", "and", "args", "[", "1", "]", "==", "type", "(", "None", ")", ":", "# noqa", "\n", "        ", "return", "args", "[", "0", "]", "\n", "", "else", ":", "\n", "        ", "return", "annotation", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.from_params.infer_params": [[128, 154], ["inspect.signature", "dict", "dict.values", "from_params.infer_params", "cls.mro"], "function", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.from_params.infer_params"], ["", "", "def", "infer_params", "(", "cls", ":", "Type", "[", "T", "]", ",", "constructor", ":", "Callable", "[", "...", ",", "T", "]", "=", "None", ")", ":", "\n", "    ", "if", "constructor", "is", "None", ":", "\n", "        ", "constructor", "=", "cls", ".", "__init__", "\n", "\n", "", "signature", "=", "inspect", ".", "signature", "(", "constructor", ")", "\n", "parameters", "=", "dict", "(", "signature", ".", "parameters", ")", "\n", "\n", "has_kwargs", "=", "False", "\n", "for", "param", "in", "parameters", ".", "values", "(", ")", ":", "\n", "        ", "if", "param", ".", "kind", "==", "param", ".", "VAR_KEYWORD", ":", "\n", "            ", "has_kwargs", "=", "True", "\n", "\n", "", "", "if", "not", "has_kwargs", ":", "\n", "        ", "return", "parameters", "\n", "\n", "# \"mro\" is \"method resolution order\".  The first one is the current class, the next is the", "\n", "# first superclass, and so on.  Taking the first superclass should work in almost all cases that", "\n", "# we're looking for here.  This could fail, though, if you are using multiple inheritance and we", "\n", "# pick the wrong superclass.  We'll worry about how to fix that when we run into an actual", "\n", "# problem because of it.", "\n", "", "super_class", "=", "cls", ".", "mro", "(", ")", "[", "1", "]", "\n", "super_parameters", "=", "infer_params", "(", "super_class", ")", "\n", "\n", "return", "{", "\n", "**", "super_parameters", ",", "\n", "**", "parameters", ",", "\n", "}", "# Subclass parameters overwrite superclass ones", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.from_params.create_kwargs": [[157, 207], ["from_params.infer_params", "infer_params.items", "params.assert_empty", "from_params.remove_optional", "from_params.pop_and_construct_arg"], "function", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.from_params.infer_params", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.params.Params.assert_empty", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.from_params.remove_optional", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.from_params.pop_and_construct_arg"], ["", "def", "create_kwargs", "(", "\n", "constructor", ":", "Callable", "[", "...", ",", "T", "]", ",", "cls", ":", "Type", "[", "T", "]", ",", "params", ":", "Params", ",", "**", "extras", "\n", ")", "->", "Dict", "[", "str", ",", "Any", "]", ":", "\n", "    ", "\"\"\"\n    Given some class, a `Params` object, and potentially other keyword arguments,\n    create a dict of keyword args suitable for passing to the class's constructor.\n\n    The function does this by finding the class's constructor, matching the constructor\n    arguments to entries in the `params` object, and instantiating values for the parameters\n    using the type annotation and possibly a from_params method.\n\n    Any values that are provided in the `extras` will just be used as is.\n    For instance, you might provide an existing `Vocabulary` this way.\n    \"\"\"", "\n", "# Get the signature of the constructor.", "\n", "\n", "kwargs", ":", "Dict", "[", "str", ",", "Any", "]", "=", "{", "}", "\n", "\n", "parameters", "=", "infer_params", "(", "cls", ",", "constructor", ")", "\n", "\n", "# Iterate over all the constructor parameters and their annotations.", "\n", "for", "param_name", ",", "param", "in", "parameters", ".", "items", "(", ")", ":", "\n", "# Skip \"self\". You're not *required* to call the first parameter \"self\",", "\n", "# so in theory this logic is fragile, but if you don't call the self parameter", "\n", "# \"self\" you kind of deserve what happens.", "\n", "        ", "if", "param_name", "==", "\"self\"", ":", "\n", "            ", "continue", "\n", "# Also skip **kwargs parameters; we handled them above.", "\n", "", "if", "param", ".", "kind", "==", "param", ".", "VAR_KEYWORD", ":", "\n", "            ", "continue", "\n", "\n", "# If the annotation is a compound type like typing.Dict[str, int],", "\n", "# it will have an __origin__ field indicating `typing.Dict`", "\n", "# and an __args__ field indicating `(str, int)`. We capture both.", "\n", "", "annotation", "=", "remove_optional", "(", "param", ".", "annotation", ")", "\n", "\n", "constructed_arg", "=", "pop_and_construct_arg", "(", "\n", "cls", ".", "__name__", ",", "param_name", ",", "annotation", ",", "param", ".", "default", ",", "params", ",", "**", "extras", "\n", ")", "\n", "\n", "# If we just ended up constructing the default value for the parameter, we can just omit it.", "\n", "# Leaving it in can cause issues with **kwargs in some corner cases, where you might end up", "\n", "# with multiple values for a single parameter (e.g., the default value gives you lazy=False", "\n", "# for a dataset reader inside **kwargs, but a particular dataset reader actually hard-codes", "\n", "# lazy=True - the superclass sees both lazy=True and lazy=False in its constructor).", "\n", "if", "constructed_arg", "is", "not", "param", ".", "default", ":", "\n", "            ", "kwargs", "[", "param_name", "]", "=", "constructed_arg", "\n", "\n", "", "", "params", ".", "assert_empty", "(", "cls", ".", "__name__", ")", "\n", "return", "kwargs", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.from_params.create_extras": [[209, 237], ["hasattr", "from_params.takes_kwargs", "extras.items", "from_params.takes_arg"], "function", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.from_params.takes_kwargs", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.from_params.takes_arg"], ["", "def", "create_extras", "(", "cls", ":", "Type", "[", "T", "]", ",", "extras", ":", "Dict", "[", "str", ",", "Any", "]", ")", "->", "Dict", "[", "str", ",", "Any", "]", ":", "\n", "    ", "\"\"\"\n    Given a dictionary of extra arguments, returns a dictionary of\n    kwargs that actually are a part of the signature of the cls.from_params\n    (or cls) method.\n    \"\"\"", "\n", "subextras", ":", "Dict", "[", "str", ",", "Any", "]", "=", "{", "}", "\n", "if", "hasattr", "(", "cls", ",", "\"from_params\"", ")", ":", "\n", "        ", "from_params_method", "=", "cls", ".", "from_params", "# type: ignore", "\n", "", "else", ":", "\n", "# In some rare cases, we get a registered subclass that does _not_ have a", "\n", "# from_params method (this happens with Activations, for instance, where we", "\n", "# register pytorch modules directly).  This is a bit of a hack to make those work,", "\n", "# instead of adding a `from_params` method for them somehow. Then the extras", "\n", "# in the class constructor are what we are looking for, to pass on.", "\n", "        ", "from_params_method", "=", "cls", "\n", "", "if", "takes_kwargs", "(", "from_params_method", ")", ":", "\n", "# If annotation.params accepts **kwargs, we need to pass them all along.", "\n", "# For example, `BasicTextFieldEmbedder.from_params` requires a Vocabulary", "\n", "# object, but `TextFieldEmbedder.from_params` does not.", "\n", "        ", "subextras", "=", "extras", "\n", "", "else", ":", "\n", "# Otherwise, only supply the ones that are actual args; any additional ones", "\n", "# will cause a TypeError.", "\n", "        ", "subextras", "=", "{", "\n", "k", ":", "v", "for", "k", ",", "v", "in", "extras", ".", "items", "(", ")", "if", "takes_arg", "(", "from_params_method", ",", "k", ")", "\n", "}", "\n", "", "return", "subextras", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.from_params.pop_and_construct_arg": [[239, 303], ["from_params.construct_arg", "params.pop", "params.pop", "getattr", "isinstance", "params.pop().pop", "params.pop().pop.pop", "params.pop().pop.pop", "params.pop().pop.pop", "load_archive", "load_archive.extract_module", "allennlp.common.lazy.Lazy", "params.get", "params.get", "isinstance", "allennlp.common.checks.ConfigurationError", "params.pop", "type"], "function", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.from_params.construct_arg", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.params.Params.pop", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.params.Params.pop", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.params.Params.pop", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.params.Params.pop", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.params.Params.pop", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.params.Params.pop", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.models.archival.load_archive", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.models.archival.Archive.extract_module", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.params.Params.get", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.params.Params.get", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.params.Params.pop"], ["", "def", "pop_and_construct_arg", "(", "\n", "class_name", ":", "str", ",", "\n", "argument_name", ":", "str", ",", "\n", "annotation", ":", "Type", ",", "\n", "default", ":", "Any", ",", "\n", "params", ":", "Params", ",", "\n", "**", "extras", ",", "\n", ")", "->", "Any", ":", "\n", "    ", "\"\"\"\n    Does the work of actually constructing an individual argument for\n    [`create_kwargs`](./from_params#create_kwargs).\n\n    Here we're in the inner loop of iterating over the parameters to a particular constructor,\n    trying to construct just one of them.  The information we get for that parameter is its name,\n    its type annotation, and its default value; we also get the full set of `Params` for\n    constructing the object (which we may mutate), and any `extras` that the constructor might\n    need.\n\n    We take the type annotation and default value here separately, instead of using an\n    `inspect.Parameter` object directly, so that we can handle `Union` types using recursion on\n    this method, trying the different annotation types in the union in turn.\n    \"\"\"", "\n", "from", "allennlp", ".", "models", ".", "archival", "import", "(", "\n", "load_archive", ",", "\n", ")", "# import here to avoid circular imports", "\n", "\n", "# We used `argument_name` as the method argument to avoid conflicts with 'name' being a key in", "\n", "# `extras`, which isn't _that_ unlikely.  Now that we are inside the method, we can switch back", "\n", "# to using `name`.", "\n", "name", "=", "argument_name", "\n", "\n", "# Some constructors expect extra non-parameter items, e.g. vocab: Vocabulary.", "\n", "# We check the provided `extras` for these and just use them if they exist.", "\n", "if", "name", "in", "extras", ":", "\n", "        ", "return", "extras", "[", "name", "]", "\n", "# Next case is when argument should be loaded from pretrained archive.", "\n", "", "elif", "(", "\n", "name", "in", "params", "\n", "and", "isinstance", "(", "params", ".", "get", "(", "name", ")", ",", "Params", ")", "\n", "and", "\"_pretrained\"", "in", "params", ".", "get", "(", "name", ")", "\n", ")", ":", "\n", "        ", "load_module_params", "=", "params", ".", "pop", "(", "name", ")", ".", "pop", "(", "\"_pretrained\"", ")", "\n", "archive_file", "=", "load_module_params", ".", "pop", "(", "\"archive_file\"", ")", "\n", "module_path", "=", "load_module_params", ".", "pop", "(", "\"module_path\"", ")", "\n", "freeze", "=", "load_module_params", ".", "pop", "(", "\"freeze\"", ",", "True", ")", "\n", "archive", "=", "load_archive", "(", "archive_file", ")", "\n", "result", "=", "archive", ".", "extract_module", "(", "module_path", ",", "freeze", ")", "\n", "if", "not", "isinstance", "(", "result", ",", "annotation", ")", ":", "\n", "            ", "raise", "ConfigurationError", "(", "\n", "f\"The module from model at {archive_file} at path {module_path} \"", "\n", "f\"was expected of type {annotation} but is of type {type(result)}\"", "\n", ")", "\n", "", "return", "result", "\n", "\n", "", "popped_params", "=", "(", "\n", "params", ".", "pop", "(", "name", ",", "default", ")", "if", "default", "!=", "_NO_DEFAULT", "else", "params", ".", "pop", "(", "name", ")", "\n", ")", "\n", "if", "popped_params", "is", "None", ":", "\n", "        ", "origin", "=", "getattr", "(", "annotation", ",", "\"__origin__\"", ",", "None", ")", "\n", "if", "origin", "==", "Lazy", ":", "\n", "            ", "return", "Lazy", "(", "lambda", "**", "kwargs", ":", "None", ")", "\n", "", "return", "None", "\n", "\n", "", "return", "construct_arg", "(", "class_name", ",", "name", ",", "popped_params", ",", "annotation", ",", "default", ",", "**", "extras", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.from_params.construct_arg": [[305, 477], ["getattr", "getattr", "hasattr", "from_params.create_extras", "isinstance", "int", "isinstance", "annotation.from_params", "allennlp.common.checks.ConfigurationError", "bool", "annotation.by_name", "allennlp.common.params.Params", "float", "from_params.can_construct_from_params", "copy.deepcopy.items", "len", "from_params.construct_arg", "from_params.can_construct_from_params", "enumerate", "str", "len", "from_params.construct_arg", "value_list.append", "all", "enumerate", "tuple", "str", "zip", "from_params.construct_arg", "value_list.append", "from_params.can_construct_from_params", "set", "enumerate", "from_params.can_construct_from_params", "str", "len", "from_params.construct_arg", "set.add", "copy.deepcopy", "allennlp.common.checks.ConfigurationError", "str", "from_params.create_extras", "allennlp.common.lazy.Lazy", "isinstance", "from_params.construct_arg", "allennlp.common.lazy.Lazy", "value_cls.from_params", "copy.deepcopy.as_dict", "str", "copy.deepcopy"], "function", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.from_params.create_extras", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.tokenizers.tokenizer.Tokenizer.from_params", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.registrable.Registrable.by_name", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.from_params.can_construct_from_params", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.from_params.construct_arg", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.from_params.can_construct_from_params", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.from_params.construct_arg", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.from_params.construct_arg", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.from_params.can_construct_from_params", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.from_params.can_construct_from_params", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.from_params.construct_arg", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.from_params.create_extras", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.from_params.construct_arg", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.tokenizers.tokenizer.Tokenizer.from_params", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.params.Params.as_dict"], ["", "def", "construct_arg", "(", "\n", "class_name", ":", "str", ",", "\n", "argument_name", ":", "str", ",", "\n", "popped_params", ":", "Params", ",", "\n", "annotation", ":", "Type", ",", "\n", "default", ":", "Any", ",", "\n", "**", "extras", ",", "\n", ")", "->", "Any", ":", "\n", "    ", "\"\"\"\n    The first two parameters here are only used for logging if we encounter an error.\n    \"\"\"", "\n", "origin", "=", "getattr", "(", "annotation", ",", "\"__origin__\"", ",", "None", ")", "\n", "args", "=", "getattr", "(", "annotation", ",", "\"__args__\"", ",", "[", "]", ")", "\n", "\n", "# The parameter is optional if its default value is not the \"no default\" sentinel.", "\n", "optional", "=", "default", "!=", "_NO_DEFAULT", "\n", "\n", "if", "hasattr", "(", "annotation", ",", "\"from_params\"", ")", ":", "\n", "        ", "if", "popped_params", "is", "default", ":", "\n", "            ", "return", "default", "\n", "", "elif", "popped_params", "is", "not", "None", ":", "\n", "# Our params have an entry for this, so we use that.", "\n", "\n", "            ", "subextras", "=", "create_extras", "(", "annotation", ",", "extras", ")", "\n", "\n", "# In some cases we allow a string instead of a param dict, so", "\n", "# we need to handle that case separately.", "\n", "if", "isinstance", "(", "popped_params", ",", "str", ")", ":", "\n", "                ", "return", "annotation", ".", "by_name", "(", "popped_params", ")", "(", ")", "\n", "", "else", ":", "\n", "                ", "if", "isinstance", "(", "popped_params", ",", "dict", ")", ":", "\n", "                    ", "popped_params", "=", "Params", "(", "popped_params", ")", "\n", "", "return", "annotation", ".", "from_params", "(", "params", "=", "popped_params", ",", "**", "subextras", ")", "\n", "", "", "elif", "not", "optional", ":", "\n", "# Not optional and not supplied, that's an error!", "\n", "            ", "raise", "ConfigurationError", "(", "f\"expected key {argument_name} for {class_name}\"", ")", "\n", "", "else", ":", "\n", "            ", "return", "default", "\n", "\n", "# If the parameter type is a Python primitive, just pop it off", "\n", "# using the correct casting pop_xyz operation.", "\n", "", "", "elif", "annotation", "==", "str", ":", "\n", "        ", "return", "popped_params", "\n", "", "elif", "annotation", "==", "int", ":", "\n", "        ", "return", "int", "(", "popped_params", ")", "# type: ignore", "\n", "", "elif", "annotation", "==", "bool", ":", "\n", "        ", "return", "bool", "(", "popped_params", ")", "\n", "", "elif", "annotation", "==", "float", ":", "\n", "        ", "return", "float", "(", "popped_params", ")", "# type: ignore", "\n", "\n", "# This is special logic for handling types like Dict[str, TokenIndexer],", "\n", "# List[TokenIndexer], Tuple[TokenIndexer, Tokenizer], and Set[TokenIndexer],", "\n", "# which it creates by instantiating each value from_params and returning the resulting structure.", "\n", "", "elif", "(", "\n", "origin", "in", "(", "Dict", ",", "dict", ")", "\n", "and", "len", "(", "args", ")", "==", "2", "\n", "and", "can_construct_from_params", "(", "args", "[", "-", "1", "]", ")", "\n", ")", ":", "\n", "        ", "value_cls", "=", "annotation", ".", "__args__", "[", "-", "1", "]", "\n", "\n", "value_dict", "=", "{", "}", "\n", "\n", "for", "key", ",", "value_params", "in", "popped_params", ".", "items", "(", ")", ":", "\n", "            ", "value_dict", "[", "key", "]", "=", "construct_arg", "(", "\n", "str", "(", "value_cls", ")", ",", "\n", "argument_name", "+", "\".\"", "+", "key", ",", "\n", "value_params", ",", "\n", "value_cls", ",", "\n", "_NO_DEFAULT", ",", "\n", "**", "extras", ",", "\n", ")", "\n", "\n", "", "return", "value_dict", "\n", "\n", "", "elif", "(", "\n", "origin", "in", "(", "List", ",", "list", ")", "and", "len", "(", "args", ")", "==", "1", "and", "can_construct_from_params", "(", "args", "[", "0", "]", ")", "\n", ")", ":", "\n", "        ", "value_cls", "=", "annotation", ".", "__args__", "[", "0", "]", "\n", "\n", "value_list", "=", "[", "]", "\n", "\n", "for", "i", ",", "value_params", "in", "enumerate", "(", "popped_params", ")", ":", "\n", "            ", "value", "=", "construct_arg", "(", "\n", "str", "(", "value_cls", ")", ",", "\n", "argument_name", "+", "f\".{i}\"", ",", "\n", "value_params", ",", "\n", "value_cls", ",", "\n", "_NO_DEFAULT", ",", "\n", "**", "extras", ",", "\n", ")", "\n", "value_list", ".", "append", "(", "value", ")", "\n", "\n", "", "return", "value_list", "\n", "\n", "", "elif", "origin", "in", "(", "Tuple", ",", "tuple", ")", "and", "all", "(", "\n", "can_construct_from_params", "(", "arg", ")", "for", "arg", "in", "args", "\n", ")", ":", "\n", "        ", "value_list", "=", "[", "]", "\n", "\n", "for", "i", ",", "(", "value_cls", ",", "value_params", ")", "in", "enumerate", "(", "\n", "zip", "(", "annotation", ".", "__args__", ",", "popped_params", ")", "\n", ")", ":", "\n", "            ", "value", "=", "construct_arg", "(", "\n", "str", "(", "value_cls", ")", ",", "\n", "argument_name", "+", "f\".{i}\"", ",", "\n", "value_params", ",", "\n", "value_cls", ",", "\n", "_NO_DEFAULT", ",", "\n", "**", "extras", ",", "\n", ")", "\n", "value_list", ".", "append", "(", "value", ")", "\n", "\n", "", "return", "tuple", "(", "value_list", ")", "\n", "\n", "", "elif", "origin", "in", "(", "Set", ",", "set", ")", "and", "len", "(", "args", ")", "==", "1", "and", "can_construct_from_params", "(", "args", "[", "0", "]", ")", ":", "\n", "        ", "value_cls", "=", "annotation", ".", "__args__", "[", "0", "]", "\n", "\n", "value_set", "=", "set", "(", ")", "\n", "\n", "for", "i", ",", "value_params", "in", "enumerate", "(", "popped_params", ")", ":", "\n", "            ", "value", "=", "construct_arg", "(", "\n", "str", "(", "value_cls", ")", ",", "\n", "argument_name", "+", "f\".{i}\"", ",", "\n", "value_params", ",", "\n", "value_cls", ",", "\n", "_NO_DEFAULT", ",", "\n", "**", "extras", ",", "\n", ")", "\n", "value_set", ".", "add", "(", "value", ")", "\n", "\n", "", "return", "value_set", "\n", "\n", "", "elif", "origin", "==", "Union", ":", "\n", "# Storing this so we can recover it later if we need to.", "\n", "        ", "backup_params", "=", "deepcopy", "(", "popped_params", ")", "\n", "\n", "# We'll try each of the given types in the union sequentially, returning the first one that", "\n", "# succeeds.", "\n", "for", "arg_annotation", "in", "args", ":", "\n", "            ", "try", ":", "\n", "                ", "return", "construct_arg", "(", "\n", "str", "(", "arg_annotation", ")", ",", "\n", "argument_name", ",", "\n", "popped_params", ",", "\n", "arg_annotation", ",", "\n", "default", ",", "\n", "**", "extras", ",", "\n", ")", "\n", "", "except", "(", "ValueError", ",", "TypeError", ",", "ConfigurationError", ",", "AttributeError", ")", ":", "\n", "# Our attempt to construct the argument may have modified popped_params, so we", "\n", "# restore it here.", "\n", "                ", "popped_params", "=", "deepcopy", "(", "backup_params", ")", "\n", "\n", "# If none of them succeeded, we crash.", "\n", "", "", "raise", "ConfigurationError", "(", "\n", "f\"Failed to construct argument {argument_name} with type {annotation}\"", "\n", ")", "\n", "", "elif", "origin", "==", "Lazy", ":", "\n", "        ", "if", "popped_params", "is", "default", ":", "\n", "            ", "return", "Lazy", "(", "lambda", "**", "kwargs", ":", "default", ")", "\n", "", "value_cls", "=", "args", "[", "0", "]", "\n", "subextras", "=", "create_extras", "(", "value_cls", ",", "extras", ")", "\n", "\n", "def", "constructor", "(", "**", "kwargs", ")", ":", "\n", "            ", "return", "value_cls", ".", "from_params", "(", "params", "=", "popped_params", ",", "**", "kwargs", ",", "**", "subextras", ")", "\n", "\n", "", "return", "Lazy", "(", "constructor", ")", "# type: ignore", "\n", "", "else", ":", "\n", "# Pass it on as is and hope for the best.   \u00af\\_(\u30c4)_/\u00af", "\n", "        ", "if", "isinstance", "(", "popped_params", ",", "Params", ")", ":", "\n", "            ", "return", "popped_params", ".", "as_dict", "(", "quiet", "=", "True", ")", "\n", "", "return", "popped_params", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.params.Params.__init__": [[230, 233], ["params._replace_none"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.params._replace_none"], ["def", "__init__", "(", "self", ",", "params", ":", "Dict", "[", "str", ",", "Any", "]", ",", "history", ":", "str", "=", "\"\"", ")", "->", "None", ":", "\n", "        ", "self", ".", "params", "=", "_replace_none", "(", "params", ")", "\n", "self", ".", "history", "=", "history", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.params.Params.pop": [[234, 260], ["params.Params.params.pop", "params._is_dict_free", "logger.info", "params.Params._check_is_dict", "params.Params.params.pop", "allennlp.common.checks.ConfigurationError"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.params.Params.pop", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.params._is_dict_free", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.dataset_readers.multiprocess_dataset_reader.logger.info", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.params.Params._check_is_dict", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.params.Params.pop"], ["", "@", "overrides", "\n", "def", "pop", "(", "self", ",", "key", ":", "str", ",", "default", ":", "Any", "=", "DEFAULT", ",", "keep_as_dict", ":", "bool", "=", "False", ")", "->", "Any", ":", "\n", "\n", "        ", "\"\"\"\n        Performs the functionality associated with dict.pop(key), along with checking for\n        returned dictionaries, replacing them with Param objects with an updated history\n        (unless keep_as_dict is True, in which case we leave them as dictionaries).\n\n        If `key` is not present in the dictionary, and no default was specified, we raise a\n        `ConfigurationError`, instead of the typical `KeyError`.\n        \"\"\"", "\n", "if", "default", "is", "self", ".", "DEFAULT", ":", "\n", "            ", "try", ":", "\n", "                ", "value", "=", "self", ".", "params", ".", "pop", "(", "key", ")", "\n", "", "except", "KeyError", ":", "\n", "                ", "raise", "ConfigurationError", "(", "\n", "'key \"{}\" is required at location \"{}\"'", ".", "format", "(", "key", ",", "self", ".", "history", ")", "\n", ")", "\n", "", "", "else", ":", "\n", "            ", "value", "=", "self", ".", "params", ".", "pop", "(", "key", ",", "default", ")", "\n", "\n", "", "if", "keep_as_dict", "or", "_is_dict_free", "(", "value", ")", ":", "\n", "            ", "logger", ".", "info", "(", "f\"{self.history}{key} = {value}\"", ")", "\n", "return", "value", "\n", "", "else", ":", "\n", "            ", "return", "self", ".", "_check_is_dict", "(", "key", ",", "value", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.params.Params.pop_int": [[261, 270], ["params.Params.pop", "int"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.params.Params.pop"], ["", "", "def", "pop_int", "(", "self", ",", "key", ":", "str", ",", "default", ":", "Any", "=", "DEFAULT", ")", "->", "int", ":", "\n", "        ", "\"\"\"\n        Performs a pop and coerces to an int.\n        \"\"\"", "\n", "value", "=", "self", ".", "pop", "(", "key", ",", "default", ")", "\n", "if", "value", "is", "None", ":", "\n", "            ", "return", "None", "\n", "", "else", ":", "\n", "            ", "return", "int", "(", "value", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.params.Params.pop_float": [[271, 280], ["params.Params.pop", "float"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.params.Params.pop"], ["", "", "def", "pop_float", "(", "self", ",", "key", ":", "str", ",", "default", ":", "Any", "=", "DEFAULT", ")", "->", "float", ":", "\n", "        ", "\"\"\"\n        Performs a pop and coerces to a float.\n        \"\"\"", "\n", "value", "=", "self", ".", "pop", "(", "key", ",", "default", ")", "\n", "if", "value", "is", "None", ":", "\n", "            ", "return", "None", "\n", "", "else", ":", "\n", "            ", "return", "float", "(", "value", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.params.Params.pop_bool": [[281, 296], ["params.Params.pop", "isinstance", "ValueError"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.params.Params.pop"], ["", "", "def", "pop_bool", "(", "self", ",", "key", ":", "str", ",", "default", ":", "Any", "=", "DEFAULT", ")", "->", "bool", ":", "\n", "        ", "\"\"\"\n        Performs a pop and coerces to a bool.\n        \"\"\"", "\n", "value", "=", "self", ".", "pop", "(", "key", ",", "default", ")", "\n", "if", "value", "is", "None", ":", "\n", "            ", "return", "None", "\n", "", "elif", "isinstance", "(", "value", ",", "bool", ")", ":", "\n", "            ", "return", "value", "\n", "", "elif", "value", "==", "\"true\"", ":", "\n", "            ", "return", "True", "\n", "", "elif", "value", "==", "\"false\"", ":", "\n", "            ", "return", "False", "\n", "", "else", ":", "\n", "            ", "raise", "ValueError", "(", "\"Cannot convert variable to bool: \"", "+", "value", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.params.Params.get": [[297, 313], ["params.Params._check_is_dict", "params.Params.params.get", "params.Params.params.get", "allennlp.common.checks.ConfigurationError"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.params.Params._check_is_dict", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.params.Params.get", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.params.Params.get"], ["", "", "@", "overrides", "\n", "def", "get", "(", "self", ",", "key", ":", "str", ",", "default", ":", "Any", "=", "DEFAULT", ")", ":", "\n", "        ", "\"\"\"\n        Performs the functionality associated with dict.get(key) but also checks for returned\n        dicts and returns a Params object in their place with an updated history.\n        \"\"\"", "\n", "if", "default", "is", "self", ".", "DEFAULT", ":", "\n", "            ", "try", ":", "\n", "                ", "value", "=", "self", ".", "params", ".", "get", "(", "key", ")", "\n", "", "except", "KeyError", ":", "\n", "                ", "raise", "ConfigurationError", "(", "\n", "'key \"{}\" is required at location \"{}\"'", ".", "format", "(", "key", ",", "self", ".", "history", ")", "\n", ")", "\n", "", "", "else", ":", "\n", "            ", "value", "=", "self", ".", "params", ".", "get", "(", "key", ",", "default", ")", "\n", "", "return", "self", ".", "_check_is_dict", "(", "key", ",", "value", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.params.Params.pop_choice": [[314, 362], ["params.Params.pop", "allennlp.common.checks.ConfigurationError"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.params.Params.pop"], ["", "def", "pop_choice", "(", "\n", "self", ",", "\n", "key", ":", "str", ",", "\n", "choices", ":", "List", "[", "Any", "]", ",", "\n", "default_to_first_choice", ":", "bool", "=", "False", ",", "\n", "allow_class_names", ":", "bool", "=", "True", ",", "\n", ")", "->", "Any", ":", "\n", "        ", "\"\"\"\n        Gets the value of `key` in the `params` dictionary, ensuring that the value is one of\n        the given choices. Note that this `pops` the key from params, modifying the dictionary,\n        consistent with how parameters are processed in this codebase.\n\n        # Parameters\n\n        key: str\n            Key to get the value from in the param dictionary\n        choices: List[Any]\n            A list of valid options for values corresponding to `key`.  For example, if you're\n            specifying the type of encoder to use for some part of your model, the choices might be\n            the list of encoder classes we know about and can instantiate.  If the value we find in\n            the param dictionary is not in `choices`, we raise a `ConfigurationError`, because\n            the user specified an invalid value in their parameter file.\n        default_to_first_choice: bool, optional (default=False)\n            If this is `True`, we allow the `key` to not be present in the parameter\n            dictionary.  If the key is not present, we will use the return as the value the first\n            choice in the `choices` list.  If this is `False`, we raise a\n            `ConfigurationError`, because specifying the `key` is required (e.g., you `have` to\n            specify your model class when running an experiment, but you can feel free to use\n            default settings for encoders if you want).\n        allow_class_names : bool, optional (default = True)\n            If this is `True`, then we allow unknown choices that look like fully-qualified class names.\n            This is to allow e.g. specifying a model type as my_library.my_model.MyModel\n            and importing it on the fly. Our check for \"looks like\" is extremely lenient\n            and consists of checking that the value contains a '.'.\n        \"\"\"", "\n", "default", "=", "choices", "[", "0", "]", "if", "default_to_first_choice", "else", "self", ".", "DEFAULT", "\n", "value", "=", "self", ".", "pop", "(", "key", ",", "default", ")", "\n", "ok_because_class_name", "=", "allow_class_names", "and", "\".\"", "in", "value", "\n", "if", "value", "not", "in", "choices", "and", "not", "ok_because_class_name", ":", "\n", "            ", "key_str", "=", "self", ".", "history", "+", "key", "\n", "message", "=", "(", "\n", "f\"{value} not in acceptable choices for {key_str}: {choices}. \"", "\n", "\"You should either use the --include-package flag to make sure the correct module \"", "\n", "\"is loaded, or use a fully qualified class name in your config file like \"", "\n", "\"\"\"{\"model\": \"my_module.models.MyModel\"} to have it imported automatically.\"\"\"", "\n", ")", "\n", "raise", "ConfigurationError", "(", "message", ")", "\n", "", "return", "value", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.params.Params.as_dict": [[363, 399], ["logger.info", "logger.info", "params.Params.as_dict.log_recursively"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.dataset_readers.multiprocess_dataset_reader.logger.info", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.dataset_readers.multiprocess_dataset_reader.logger.info"], ["", "def", "as_dict", "(", "self", ",", "quiet", ":", "bool", "=", "False", ",", "infer_type_and_cast", ":", "bool", "=", "False", ")", ":", "\n", "        ", "\"\"\"\n        Sometimes we need to just represent the parameters as a dict, for instance when we pass\n        them to PyTorch code.\n\n        # Parameters\n\n        quiet: bool, optional (default = False)\n            Whether to log the parameters before returning them as a dict.\n        infer_type_and_cast : bool, optional (default = False)\n            If True, we infer types and cast (e.g. things that look like floats to floats).\n        \"\"\"", "\n", "if", "infer_type_and_cast", ":", "\n", "            ", "params_as_dict", "=", "infer_and_cast", "(", "self", ".", "params", ")", "\n", "", "else", ":", "\n", "            ", "params_as_dict", "=", "self", ".", "params", "\n", "\n", "", "if", "quiet", ":", "\n", "            ", "return", "params_as_dict", "\n", "\n", "", "def", "log_recursively", "(", "parameters", ",", "history", ")", ":", "\n", "            ", "for", "key", ",", "value", "in", "parameters", ".", "items", "(", ")", ":", "\n", "                ", "if", "isinstance", "(", "value", ",", "dict", ")", ":", "\n", "                    ", "new_local_history", "=", "history", "+", "key", "+", "\".\"", "\n", "log_recursively", "(", "value", ",", "new_local_history", ")", "\n", "", "else", ":", "\n", "                    ", "logger", ".", "info", "(", "f\"{history}{key} = {value}\"", ")", "\n", "\n", "", "", "", "logger", ".", "info", "(", "\n", "\"Converting Params object to dict; logging of default \"", "\n", "\"values will not occur when dictionary parameters are \"", "\n", "\"used subsequently.\"", "\n", ")", "\n", "logger", ".", "info", "(", "\"CURRENTLY DEFINED PARAMETERS: \"", ")", "\n", "log_recursively", "(", "self", ".", "params", ",", "self", ".", "history", ")", "\n", "return", "params_as_dict", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.params.Params.as_flat_dict": [[400, 417], ["params.Params.as_flat_dict.recurse"], "methods", ["None"], ["", "def", "as_flat_dict", "(", "self", ")", ":", "\n", "        ", "\"\"\"\n        Returns the parameters of a flat dictionary from keys to values.\n        Nested structure is collapsed with periods.\n        \"\"\"", "\n", "flat_params", "=", "{", "}", "\n", "\n", "def", "recurse", "(", "parameters", ",", "path", ")", ":", "\n", "            ", "for", "key", ",", "value", "in", "parameters", ".", "items", "(", ")", ":", "\n", "                ", "newpath", "=", "path", "+", "[", "key", "]", "\n", "if", "isinstance", "(", "value", ",", "dict", ")", ":", "\n", "                    ", "recurse", "(", "value", ",", "newpath", ")", "\n", "", "else", ":", "\n", "                    ", "flat_params", "[", "\".\"", ".", "join", "(", "newpath", ")", "]", "=", "value", "\n", "\n", "", "", "", "recurse", "(", "self", ".", "params", ",", "[", "]", ")", "\n", "return", "flat_params", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.params.Params.duplicate": [[418, 424], ["copy.deepcopy"], "methods", ["None"], ["", "def", "duplicate", "(", "self", ")", "->", "\"Params\"", ":", "\n", "        ", "\"\"\"\n        Uses `copy.deepcopy()` to create a duplicate (but fully distinct)\n        copy of these Params.\n        \"\"\"", "\n", "return", "copy", ".", "deepcopy", "(", "self", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.params.Params.assert_empty": [[425, 435], ["allennlp.common.checks.ConfigurationError"], "methods", ["None"], ["", "def", "assert_empty", "(", "self", ",", "class_name", ":", "str", ")", ":", "\n", "        ", "\"\"\"\n        Raises a `ConfigurationError` if `self.params` is not empty.  We take `class_name` as\n        an argument so that the error message gives some idea of where an error happened, if there\n        was one.  `class_name` should be the name of the `calling` class, the one that got extra\n        parameters (if there are any).\n        \"\"\"", "\n", "if", "self", ".", "params", ":", "\n", "            ", "raise", "ConfigurationError", "(", "\n", "\"Extra parameters passed to {}: {}\"", ".", "format", "(", "class_name", ",", "self", ".", "params", ")", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.params.Params.__getitem__": [[437, 442], ["params.Params._check_is_dict"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.params.Params._check_is_dict"], ["", "", "def", "__getitem__", "(", "self", ",", "key", ")", ":", "\n", "        ", "if", "key", "in", "self", ".", "params", ":", "\n", "            ", "return", "self", ".", "_check_is_dict", "(", "key", ",", "self", ".", "params", "[", "key", "]", ")", "\n", "", "else", ":", "\n", "            ", "raise", "KeyError", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.params.Params.__setitem__": [[443, 445], ["None"], "methods", ["None"], ["", "", "def", "__setitem__", "(", "self", ",", "key", ",", "value", ")", ":", "\n", "        ", "self", ".", "params", "[", "key", "]", "=", "value", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.params.Params.__delitem__": [[446, 448], ["None"], "methods", ["None"], ["", "def", "__delitem__", "(", "self", ",", "key", ")", ":", "\n", "        ", "del", "self", ".", "params", "[", "key", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.params.Params.__iter__": [[449, 451], ["iter"], "methods", ["None"], ["", "def", "__iter__", "(", "self", ")", ":", "\n", "        ", "return", "iter", "(", "self", ".", "params", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.params.Params.__len__": [[452, 454], ["len"], "methods", ["None"], ["", "def", "__len__", "(", "self", ")", ":", "\n", "        ", "return", "len", "(", "self", ".", "params", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.params.Params._check_is_dict": [[455, 465], ["isinstance", "isinstance", "params.Params", "params.Params._check_is_dict", "enumerate"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.params.Params._check_is_dict"], ["", "def", "_check_is_dict", "(", "self", ",", "new_history", ",", "value", ")", ":", "\n", "        ", "if", "isinstance", "(", "value", ",", "dict", ")", ":", "\n", "            ", "new_history", "=", "self", ".", "history", "+", "new_history", "+", "\".\"", "\n", "return", "Params", "(", "value", ",", "history", "=", "new_history", ",", ")", "\n", "", "if", "isinstance", "(", "value", ",", "list", ")", ":", "\n", "            ", "value", "=", "[", "\n", "self", ".", "_check_is_dict", "(", "f\"{new_history}.{i}\"", ",", "v", ")", "\n", "for", "i", ",", "v", "in", "enumerate", "(", "value", ")", "\n", "]", "\n", "", "return", "value", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.params.Params.from_file": [[466, 500], ["allennlp.common.file_utils.cached_path", "json.loads", "params.parse_overrides", "params.with_fallback", "cls", "params._environment_variables", "evaluate_file"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.file_utils.cached_path", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.params.parse_overrides", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.params.with_fallback", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.params._environment_variables"], ["", "@", "classmethod", "\n", "def", "from_file", "(", "\n", "cls", ",", "params_file", ":", "str", ",", "params_overrides", ":", "str", "=", "\"\"", ",", "ext_vars", ":", "dict", "=", "None", "\n", ")", "->", "\"Params\"", ":", "\n", "        ", "\"\"\"\n        Load a `Params` object from a configuration file.\n\n        # Parameters\n\n        params_file : `str`\n            The path to the configuration file to load.\n        params_overrides : `str`, optional\n            A dict of overrides that can be applied to final object.\n            e.g. {\"model.embedding_dim\": 10}\n        ext_vars : `dict`, optional\n            Our config files are Jsonnet, which allows specifying external variables\n            for later substitution. Typically we substitute these using environment\n            variables; however, you can also specify them here, in which case they\n            take priority over environment variables.\n            e.g. {\"HOME_DIR\": \"/Users/allennlp/home\"}\n        \"\"\"", "\n", "if", "ext_vars", "is", "None", ":", "\n", "            ", "ext_vars", "=", "{", "}", "\n", "\n", "# redirect to cache, if necessary", "\n", "", "params_file", "=", "cached_path", "(", "params_file", ")", "\n", "ext_vars", "=", "{", "**", "_environment_variables", "(", ")", ",", "**", "ext_vars", "}", "\n", "\n", "file_dict", "=", "json", ".", "loads", "(", "evaluate_file", "(", "params_file", ",", "ext_vars", "=", "ext_vars", ")", ")", "\n", "\n", "overrides_dict", "=", "parse_overrides", "(", "params_overrides", ")", "\n", "param_dict", "=", "with_fallback", "(", "preferred", "=", "overrides_dict", ",", "fallback", "=", "file_dict", ")", "\n", "\n", "return", "cls", "(", "param_dict", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.params.Params.to_file": [[501, 506], ["open", "json.dump", "params.Params.as_ordered_dict"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.params.Params.as_ordered_dict"], ["", "def", "to_file", "(", "\n", "self", ",", "params_file", ":", "str", ",", "preference_orders", ":", "List", "[", "List", "[", "str", "]", "]", "=", "None", "\n", ")", "->", "None", ":", "\n", "        ", "with", "open", "(", "params_file", ",", "\"w\"", ")", "as", "handle", ":", "\n", "            ", "json", ".", "dump", "(", "self", ".", "as_ordered_dict", "(", "preference_orders", ")", ",", "handle", ",", "indent", "=", "4", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.params.Params.as_ordered_dict": [[507, 559], ["params.Params.as_dict", "params.Params.as_ordered_dict.order_dict"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.params.Params.as_dict"], ["", "", "def", "as_ordered_dict", "(", "self", ",", "preference_orders", ":", "List", "[", "List", "[", "str", "]", "]", "=", "None", ")", "->", "OrderedDict", ":", "\n", "        ", "\"\"\"\n        Returns Ordered Dict of Params from list of partial order preferences.\n\n        # Parameters\n\n        preference_orders: List[List[str]], optional\n            `preference_orders` is list of partial preference orders. [\"A\", \"B\", \"C\"] means\n            \"A\" > \"B\" > \"C\". For multiple preference_orders first will be considered first.\n            Keys not found, will have last but alphabetical preference. Default Preferences:\n            `[[\"dataset_reader\", \"iterator\", \"model\", \"train_data_path\", \"validation_data_path\",\n            \"test_data_path\", \"trainer\", \"vocabulary\"], [\"type\"]]`\n        \"\"\"", "\n", "params_dict", "=", "self", ".", "as_dict", "(", "quiet", "=", "True", ")", "\n", "if", "not", "preference_orders", ":", "\n", "            ", "preference_orders", "=", "[", "]", "\n", "preference_orders", ".", "append", "(", "\n", "[", "\n", "\"dataset_reader\"", ",", "\n", "\"iterator\"", ",", "\n", "\"model\"", ",", "\n", "\"train_data_path\"", ",", "\n", "\"validation_data_path\"", ",", "\n", "\"test_data_path\"", ",", "\n", "\"trainer\"", ",", "\n", "\"vocabulary\"", ",", "\n", "]", "\n", ")", "\n", "preference_orders", ".", "append", "(", "[", "\"type\"", "]", ")", "\n", "\n", "", "def", "order_func", "(", "key", ")", ":", "\n", "# Makes a tuple to use for ordering.  The tuple is an index into each of the `preference_orders`,", "\n", "# followed by the key itself.  This gives us integer sorting if you have a key in one of the", "\n", "# `preference_orders`, followed by alphabetical ordering if not.", "\n", "            ", "order_tuple", "=", "[", "\n", "order", ".", "index", "(", "key", ")", "if", "key", "in", "order", "else", "len", "(", "order", ")", "\n", "for", "order", "in", "preference_orders", "\n", "]", "\n", "return", "order_tuple", "+", "[", "key", "]", "\n", "\n", "", "def", "order_dict", "(", "dictionary", ",", "order_func", ")", ":", "\n", "# Recursively orders dictionary according to scoring order_func", "\n", "            ", "result", "=", "OrderedDict", "(", ")", "\n", "for", "key", ",", "val", "in", "sorted", "(", "\n", "dictionary", ".", "items", "(", ")", ",", "key", "=", "lambda", "item", ":", "order_func", "(", "item", "[", "0", "]", ")", "\n", ")", ":", "\n", "                ", "result", "[", "key", "]", "=", "(", "\n", "order_dict", "(", "val", ",", "order_func", ")", "if", "isinstance", "(", "val", ",", "dict", ")", "else", "val", "\n", ")", "\n", "", "return", "result", "\n", "\n", "", "return", "order_dict", "(", "params_dict", ",", "order_func", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.params.Params.get_hash": [[560, 572], ["json.dumps", "zlib.adler32", "str", "json.dumps.encode"], "methods", ["None"], ["", "def", "get_hash", "(", "self", ")", "->", "str", ":", "\n", "        ", "\"\"\"\n        Returns a hash code representing the current state of this `Params` object.  We don't\n        want to implement `__hash__` because that has deeper python implications (and this is a\n        mutable object), but this will give you a representation of the current state.\n        We use `zlib.adler32` instead of Python's builtin `hash` because the random seed for the\n        latter is reset on each new program invocation, as discussed here:\n        https://stackoverflow.com/questions/27954892/deterministic-hashing-in-python-3.\n        \"\"\"", "\n", "dumped", "=", "json", ".", "dumps", "(", "self", ".", "params", ",", "sort_keys", "=", "True", ")", "\n", "hashed", "=", "zlib", ".", "adler32", "(", "dumped", ".", "encode", "(", ")", ")", "\n", "return", "str", "(", "hashed", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.params.Params.__str__": [[573, 575], ["None"], "methods", ["None"], ["", "def", "__str__", "(", "self", ")", "->", "str", ":", "\n", "        ", "return", "f\"{self.history}Params({self.params})\"", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.params.infer_and_cast": [[43, 81], ["isinstance", "isinstance", "isinstance", "params.infer_and_cast", "isinstance", "params.infer_and_cast", "ValueError", "value.items", "value.lower", "value.lower", "int", "float"], "function", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.params.infer_and_cast", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.params.infer_and_cast"], ["def", "infer_and_cast", "(", "value", ":", "Any", ")", ":", "\n", "    ", "\"\"\"\n    In some cases we'll be feeding params dicts to functions we don't own;\n    for example, PyTorch optimizers. In that case we can't use `pop_int`\n    or similar to force casts (which means you can't specify `int` parameters\n    using environment variables). This function takes something that looks JSON-like\n    and recursively casts things that look like (bool, int, float) to (bool, int, float).\n    \"\"\"", "\n", "\n", "if", "isinstance", "(", "value", ",", "(", "int", ",", "float", ",", "bool", ")", ")", ":", "\n", "# Already one of our desired types, so leave as is.", "\n", "        ", "return", "value", "\n", "", "elif", "isinstance", "(", "value", ",", "list", ")", ":", "\n", "# Recursively call on each list element.", "\n", "        ", "return", "[", "infer_and_cast", "(", "item", ")", "for", "item", "in", "value", "]", "\n", "", "elif", "isinstance", "(", "value", ",", "dict", ")", ":", "\n", "# Recursively call on each dict value.", "\n", "        ", "return", "{", "key", ":", "infer_and_cast", "(", "item", ")", "for", "key", ",", "item", "in", "value", ".", "items", "(", ")", "}", "\n", "", "elif", "isinstance", "(", "value", ",", "str", ")", ":", "\n", "# If it looks like a bool, make it a bool.", "\n", "        ", "if", "value", ".", "lower", "(", ")", "==", "\"true\"", ":", "\n", "            ", "return", "True", "\n", "", "elif", "value", ".", "lower", "(", ")", "==", "\"false\"", ":", "\n", "            ", "return", "False", "\n", "", "else", ":", "\n", "# See if it could be an int.", "\n", "            ", "try", ":", "\n", "                ", "return", "int", "(", "value", ")", "\n", "", "except", "ValueError", ":", "\n", "                ", "pass", "\n", "# See if it could be a float.", "\n", "", "try", ":", "\n", "                ", "return", "float", "(", "value", ")", "\n", "", "except", "ValueError", ":", "\n", "# Just return it as a string.", "\n", "                ", "return", "value", "\n", "", "", "", "else", ":", "\n", "        ", "raise", "ValueError", "(", "f\"cannot infer type of {value}\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.params._is_encodable": [[83, 92], ["value.encode"], "function", ["None"], ["", "", "def", "_is_encodable", "(", "value", ":", "str", ")", "->", "bool", ":", "\n", "    ", "\"\"\"\n    We need to filter out environment variables that can't\n    be unicode-encoded to avoid a \"surrogates not allowed\"\n    error in jsonnet.\n    \"\"\"", "\n", "# Idiomatically you'd like to not check the != b\"\"", "\n", "# but mypy doesn't like that.", "\n", "return", "(", "value", "==", "\"\"", ")", "or", "(", "value", ".", "encode", "(", "\"utf-8\"", ",", "\"ignore\"", ")", "!=", "b\"\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.params._environment_variables": [[94, 99], ["os.environ.items", "params._is_encodable"], "function", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.params._is_encodable"], ["", "def", "_environment_variables", "(", ")", "->", "Dict", "[", "str", ",", "str", "]", ":", "\n", "    ", "\"\"\"\n    Wraps `os.environ` to filter out non-encodable values.\n    \"\"\"", "\n", "return", "{", "key", ":", "value", "for", "key", ",", "value", "in", "os", ".", "environ", ".", "items", "(", ")", "if", "_is_encodable", "(", "value", ")", "}", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.params.unflatten": [[101, 127], ["flat_dict.items", "compound_key.split", "curr_dict.get", "allennlp.common.checks.ConfigurationError", "isinstance", "isinstance", "allennlp.common.checks.ConfigurationError"], "function", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.params.Params.get"], ["", "def", "unflatten", "(", "flat_dict", ":", "Dict", "[", "str", ",", "Any", "]", ")", "->", "Dict", "[", "str", ",", "Any", "]", ":", "\n", "    ", "\"\"\"\n    Given a \"flattened\" dict with compound keys, e.g.\n        {\"a.b\": 0}\n    unflatten it:\n        {\"a\": {\"b\": 0}}\n    \"\"\"", "\n", "unflat", ":", "Dict", "[", "str", ",", "Any", "]", "=", "{", "}", "\n", "\n", "for", "compound_key", ",", "value", "in", "flat_dict", ".", "items", "(", ")", ":", "\n", "        ", "curr_dict", "=", "unflat", "\n", "parts", "=", "compound_key", ".", "split", "(", "\".\"", ")", "\n", "for", "key", "in", "parts", "[", ":", "-", "1", "]", ":", "\n", "            ", "curr_value", "=", "curr_dict", ".", "get", "(", "key", ")", "\n", "if", "key", "not", "in", "curr_dict", ":", "\n", "                ", "curr_dict", "[", "key", "]", "=", "{", "}", "\n", "curr_dict", "=", "curr_dict", "[", "key", "]", "\n", "", "elif", "isinstance", "(", "curr_value", ",", "dict", ")", ":", "\n", "                ", "curr_dict", "=", "curr_value", "\n", "", "else", ":", "\n", "                ", "raise", "ConfigurationError", "(", "\"flattened dictionary is invalid\"", ")", "\n", "", "", "if", "not", "isinstance", "(", "curr_dict", ",", "dict", ")", "or", "parts", "[", "-", "1", "]", "in", "curr_dict", ":", "\n", "            ", "raise", "ConfigurationError", "(", "\"flattened dictionary is invalid\"", ")", "\n", "", "curr_dict", "[", "parts", "[", "-", "1", "]", "]", "=", "value", "\n", "\n", "", "return", "unflat", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.params.with_fallback": [[129, 177], ["set", "set", "preferred.keys", "fallback.keys", "copy.deepcopy", "copy.deepcopy", "params.with_fallback.merge"], "function", ["None"], ["", "def", "with_fallback", "(", "\n", "preferred", ":", "Dict", "[", "str", ",", "Any", "]", ",", "fallback", ":", "Dict", "[", "str", ",", "Any", "]", "\n", ")", "->", "Dict", "[", "str", ",", "Any", "]", ":", "\n", "    ", "\"\"\"\n    Deep merge two dicts, preferring values from `preferred`.\n    \"\"\"", "\n", "\n", "def", "merge", "(", "preferred_value", ":", "Any", ",", "fallback_value", ":", "Any", ")", "->", "Any", ":", "\n", "        ", "if", "isinstance", "(", "preferred_value", ",", "dict", ")", "and", "isinstance", "(", "fallback_value", ",", "dict", ")", ":", "\n", "            ", "return", "with_fallback", "(", "preferred_value", ",", "fallback_value", ")", "\n", "", "elif", "isinstance", "(", "preferred_value", ",", "dict", ")", "and", "isinstance", "(", "fallback_value", ",", "list", ")", ":", "\n", "# treat preferred_value as a sparse list, where each key is an index to be overridden", "\n", "            ", "merged_list", "=", "fallback_value", "\n", "for", "elem_key", ",", "preferred_element", "in", "preferred_value", ".", "items", "(", ")", ":", "\n", "                ", "try", ":", "\n", "                    ", "index", "=", "int", "(", "elem_key", ")", "\n", "merged_list", "[", "index", "]", "=", "merge", "(", "preferred_element", ",", "fallback_value", "[", "index", "]", ")", "\n", "", "except", "ValueError", ":", "\n", "                    ", "raise", "ConfigurationError", "(", "\n", "\"could not merge dicts - the preferred dict contains \"", "\n", "f\"invalid keys (key {elem_key} is not a valid list index)\"", "\n", ")", "\n", "", "except", "IndexError", ":", "\n", "                    ", "raise", "ConfigurationError", "(", "\n", "\"could not merge dicts - the preferred dict contains \"", "\n", "f\"invalid keys (key {index} is out of bounds)\"", "\n", ")", "\n", "", "", "return", "merged_list", "\n", "", "else", ":", "\n", "            ", "return", "copy", ".", "deepcopy", "(", "preferred_value", ")", "\n", "\n", "", "", "preferred_keys", "=", "set", "(", "preferred", ".", "keys", "(", ")", ")", "\n", "fallback_keys", "=", "set", "(", "fallback", ".", "keys", "(", ")", ")", "\n", "common_keys", "=", "preferred_keys", "&", "fallback_keys", "\n", "\n", "merged", ":", "Dict", "[", "str", ",", "Any", "]", "=", "{", "}", "\n", "\n", "for", "key", "in", "preferred_keys", "-", "fallback_keys", ":", "\n", "        ", "merged", "[", "key", "]", "=", "copy", ".", "deepcopy", "(", "preferred", "[", "key", "]", ")", "\n", "", "for", "key", "in", "fallback_keys", "-", "preferred_keys", ":", "\n", "        ", "merged", "[", "key", "]", "=", "copy", ".", "deepcopy", "(", "fallback", "[", "key", "]", ")", "\n", "\n", "", "for", "key", "in", "common_keys", ":", "\n", "        ", "preferred_value", "=", "preferred", "[", "key", "]", "\n", "fallback_value", "=", "fallback", "[", "key", "]", "\n", "\n", "merged", "[", "key", "]", "=", "merge", "(", "preferred_value", ",", "fallback_value", ")", "\n", "", "return", "merged", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.params.parse_overrides": [[179, 188], ["params._environment_variables", "params.unflatten", "json.loads", "evaluate_snippet"], "function", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.params._environment_variables", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.params.unflatten"], ["", "def", "parse_overrides", "(", "serialized_overrides", ":", "str", ")", "->", "Dict", "[", "str", ",", "Any", "]", ":", "\n", "    ", "if", "serialized_overrides", ":", "\n", "        ", "ext_vars", "=", "_environment_variables", "(", ")", "\n", "\n", "return", "unflatten", "(", "\n", "json", ".", "loads", "(", "evaluate_snippet", "(", "\"\"", ",", "serialized_overrides", ",", "ext_vars", "=", "ext_vars", ")", ")", "\n", ")", "\n", "", "else", ":", "\n", "        ", "return", "{", "}", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.params._is_dict_free": [[190, 200], ["isinstance", "isinstance", "all", "params._is_dict_free"], "function", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.params._is_dict_free"], ["", "", "def", "_is_dict_free", "(", "obj", ":", "Any", ")", "->", "bool", ":", "\n", "    ", "\"\"\"\n    Returns False if obj is a dict, or if it's a list with an element that _has_dict.\n    \"\"\"", "\n", "if", "isinstance", "(", "obj", ",", "dict", ")", ":", "\n", "        ", "return", "False", "\n", "", "elif", "isinstance", "(", "obj", ",", "list", ")", ":", "\n", "        ", "return", "all", "(", "_is_dict_free", "(", "item", ")", "for", "item", "in", "obj", ")", "\n", "", "else", ":", "\n", "        ", "return", "True", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.params.pop_choice": [[577, 599], ["params.Params.pop_choice", "params.Params"], "function", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.params.pop_choice"], ["", "", "def", "pop_choice", "(", "\n", "params", ":", "Dict", "[", "str", ",", "Any", "]", ",", "\n", "key", ":", "str", ",", "\n", "choices", ":", "List", "[", "Any", "]", ",", "\n", "default_to_first_choice", ":", "bool", "=", "False", ",", "\n", "history", ":", "str", "=", "\"?.\"", ",", "\n", "allow_class_names", ":", "bool", "=", "True", ",", "\n", ")", "->", "Any", ":", "\n", "    ", "\"\"\"\n    Performs the same function as `Params.pop_choice`, but is required in order to deal with\n    places that the Params object is not welcome, such as inside Keras layers.  See the docstring\n    of that method for more detail on how this function works.\n\n    This method adds a `history` parameter, in the off-chance that you know it, so that we can\n    reproduce `Params.pop_choice` exactly.  We default to using \"?.\" if you don't know the\n    history, so you'll have to fix that in the log if you want to actually recover the logged\n    parameters.\n    \"\"\"", "\n", "value", "=", "Params", "(", "params", ",", "history", ")", ".", "pop_choice", "(", "\n", "key", ",", "choices", ",", "default_to_first_choice", ",", "allow_class_names", "=", "allow_class_names", "\n", ")", "\n", "return", "value", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.params._replace_none": [[601, 611], ["isinstance", "params.items", "isinstance", "params._replace_none", "params._replace_none"], "function", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.params._replace_none", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.params._replace_none"], ["", "def", "_replace_none", "(", "params", ":", "Any", ")", "->", "Any", ":", "\n", "    ", "if", "params", "==", "\"None\"", ":", "\n", "        ", "return", "None", "\n", "", "elif", "isinstance", "(", "params", ",", "dict", ")", ":", "\n", "        ", "for", "key", ",", "value", "in", "params", ".", "items", "(", ")", ":", "\n", "            ", "params", "[", "key", "]", "=", "_replace_none", "(", "value", ")", "\n", "", "return", "params", "\n", "", "elif", "isinstance", "(", "params", ",", "list", ")", ":", "\n", "        ", "return", "[", "_replace_none", "(", "value", ")", "for", "value", "in", "params", "]", "\n", "", "return", "params", "\n", "", ""]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.registrable.Registrable.register": [[44, 130], ["logger.info", "allennlp.common.checks.ConfigurationError"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.dataset_readers.multiprocess_dataset_reader.logger.info"], ["@", "classmethod", "\n", "def", "register", "(", "\n", "cls", ":", "Type", "[", "T", "]", ",", "name", ":", "str", ",", "constructor", ":", "str", "=", "None", ",", "exist_ok", ":", "bool", "=", "False", "\n", ")", ":", "\n", "        ", "\"\"\"\n        Register a class under a particular name.\n\n        # Parameters\n\n        name : `str`\n            The name to register the class under.\n        constructor : `str`, optional (default=None)\n            The name of the method to use on the class to construct the object.  If this is given,\n            we will use this method (which must be a `@classmethod`) instead of the default\n            constructor.\n        exist_ok : `bool`, optional (default=False)\n            If True, overwrites any existing models registered under `name`. Else,\n            throws an error if a model is already registered under `name`.\n\n        # Examples\n\n        To use this class, you would typically have a base class that inherits from `Registrable`:\n\n        ```python\n        class Vocabulary(Registrable):\n            ...\n        ```\n\n        Then, if you want to register a subclass, you decorate it like this:\n\n        ```python\n        @Vocabulary.register(\"my-vocabulary\")\n        class MyVocabulary(Vocabulary):\n            def __init__(self, param1: int, param2: str):\n                ...\n        ```\n\n        Registering a class like this will let you instantiate a class from a config file, where you\n        give `\"type\": \"my-vocabulary\"`, and keys corresponding to the parameters of the `__init__`\n        method (note that for this to work, those parameters must have type annotations).\n\n        If you want to have the instantiation from a config file call a method other than the\n        constructor, either because you have several different construction paths that could be\n        taken for the same object (as we do in `Vocabulary`) or because you have logic you want to\n        happen before you get to the constructor (as we do in `Embedding`), you can register a\n        specific `@classmethod` as the constructor to use, like this:\n\n        ```python\n        @Vocabulary.register(\"my-vocabulary-from-instances\", constructor=\"from_instances\")\n        @Vocabulary.register(\"my-vocabulary-from-files\", constructor=\"from_files\")\n        class MyVocabulary(Vocabulary):\n            def __init__(self, some_params):\n                ...\n\n            @classmethod\n            def from_instances(cls, some_other_params) -> MyVocabulary:\n                ...  # construct some_params from instances\n                return cls(some_params)\n\n            @classmethod\n            def from_files(cls, still_other_params) -> MyVocabulary:\n                ...  # construct some_params from files\n                return cls(some_params)\n        ```\n        \"\"\"", "\n", "registry", "=", "Registrable", ".", "_registry", "[", "cls", "]", "\n", "\n", "def", "add_subclass_to_registry", "(", "subclass", ":", "Type", "[", "T", "]", ")", ":", "\n", "# Add to registry, raise an error if key has already been used.", "\n", "            ", "if", "name", "in", "registry", ":", "\n", "                ", "if", "exist_ok", ":", "\n", "                    ", "message", "=", "(", "\n", "f\"{name} has already been registered as {registry[name][0].__name__}, but \"", "\n", "f\"exist_ok=True, so overwriting with {cls.__name__}\"", "\n", ")", "\n", "logger", ".", "info", "(", "message", ")", "\n", "", "else", ":", "\n", "                    ", "message", "=", "(", "\n", "f\"Cannot register {name} as {cls.__name__}; \"", "\n", "f\"name already in use for {registry[name][0].__name__}\"", "\n", ")", "\n", "raise", "ConfigurationError", "(", "message", ")", "\n", "", "", "registry", "[", "name", "]", "=", "(", "subclass", ",", "constructor", ")", "\n", "return", "subclass", "\n", "\n", "", "return", "add_subclass_to_registry", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.registrable.Registrable.by_name": [[131, 144], ["logger.debug", "cls.resolve_class_name", "getattr"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.registrable.Registrable.resolve_class_name"], ["", "@", "classmethod", "\n", "def", "by_name", "(", "cls", ":", "Type", "[", "T", "]", ",", "name", ":", "str", ")", "->", "Callable", "[", "...", ",", "T", "]", ":", "\n", "        ", "\"\"\"\n        Returns a callable function that constructs an argument of the registered class.  Because\n        you can register particular functions as constructors for specific names, this isn't\n        necessarily the `__init__` method of some class.\n        \"\"\"", "\n", "logger", ".", "debug", "(", "f\"instantiating registered subclass {name} of {cls}\"", ")", "\n", "subclass", ",", "constructor", "=", "cls", ".", "resolve_class_name", "(", "name", ")", "\n", "if", "not", "constructor", ":", "\n", "            ", "return", "subclass", "\n", "", "else", ":", "\n", "            ", "return", "getattr", "(", "subclass", ",", "constructor", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.registrable.Registrable.resolve_class_name": [[145, 188], ["Registrable._registry[].get", "name.split", "allennlp.common.checks.ConfigurationError", "importlib.import_module", "getattr", "allennlp.common.checks.ConfigurationError", "allennlp.common.checks.ConfigurationError"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.params.Params.get"], ["", "", "@", "classmethod", "\n", "def", "resolve_class_name", "(", "cls", ":", "Type", "[", "T", "]", ",", "name", ":", "str", ")", "->", "Tuple", "[", "Type", "[", "T", "]", ",", "Optional", "[", "str", "]", "]", ":", "\n", "        ", "\"\"\"\n        Returns the subclass that corresponds to the given `name`, along with the name of the\n        method that was registered as a constructor for that `name`, if any.\n\n        This method also allows `name` to be a fully-specified module name, instead of a name that\n        was already added to the `Registry`.  In that case, you cannot use a separate function as\n        a constructor (as you need to call `cls.register()` in order to tell us what separate\n        function to use).\n        \"\"\"", "\n", "if", "name", "in", "Registrable", ".", "_registry", "[", "cls", "]", ":", "\n", "            ", "subclass", ",", "constructor", "=", "Registrable", ".", "_registry", "[", "cls", "]", ".", "get", "(", "name", ")", "\n", "return", "subclass", ",", "constructor", "\n", "", "elif", "\".\"", "in", "name", ":", "\n", "# This might be a fully qualified class name, so we'll try importing its \"module\"", "\n", "# and finding it there.", "\n", "            ", "parts", "=", "name", ".", "split", "(", "\".\"", ")", "\n", "submodule", "=", "\".\"", ".", "join", "(", "parts", "[", ":", "-", "1", "]", ")", "\n", "class_name", "=", "parts", "[", "-", "1", "]", "\n", "\n", "try", ":", "\n", "                ", "module", "=", "importlib", ".", "import_module", "(", "submodule", ")", "\n", "", "except", "ModuleNotFoundError", ":", "\n", "                ", "raise", "ConfigurationError", "(", "\n", "f\"tried to interpret {name} as a path to a class \"", "\n", "f\"but unable to import module {submodule}\"", "\n", ")", "\n", "\n", "", "try", ":", "\n", "                ", "subclass", "=", "getattr", "(", "module", ",", "class_name", ")", "\n", "constructor", "=", "None", "\n", "return", "subclass", ",", "constructor", "\n", "", "except", "AttributeError", ":", "\n", "                ", "raise", "ConfigurationError", "(", "\n", "f\"tried to interpret {name} as a path to a class \"", "\n", "f\"but unable to find class {class_name} in {submodule}\"", "\n", ")", "\n", "\n", "", "", "else", ":", "\n", "# is not a qualified class name", "\n", "            ", "raise", "ConfigurationError", "(", "\n", "f\"{name} is not a registered name for {cls.__name__}. \"", "\n", "\"You probably need to use the --include-package flag \"", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.registrable.Registrable.list_available": [[194, 208], ["list", "Registrable._registry[].keys", "allennlp.common.checks.ConfigurationError"], "methods", ["None"], ["", "", "@", "classmethod", "\n", "def", "list_available", "(", "cls", ")", "->", "List", "[", "str", "]", ":", "\n", "        ", "\"\"\"List default first if it exists\"\"\"", "\n", "keys", "=", "list", "(", "Registrable", ".", "_registry", "[", "cls", "]", ".", "keys", "(", ")", ")", "\n", "default", "=", "cls", ".", "default_implementation", "\n", "\n", "if", "default", "is", "None", ":", "\n", "            ", "return", "keys", "\n", "", "elif", "default", "not", "in", "keys", ":", "\n", "            ", "raise", "ConfigurationError", "(", "\n", "f\"Default implementation {default} is not registered\"", "\n", ")", "\n", "", "else", ":", "\n", "            ", "return", "[", "default", "]", "+", "[", "k", "for", "k", "in", "keys", "if", "k", "!=", "default", "]", "\n", "", "", "", ""]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.tqdm.Tqdm.set_default_mininterval": [[29, 32], ["None"], "methods", ["None"], ["@", "staticmethod", "\n", "def", "set_default_mininterval", "(", "value", ":", "float", ")", "->", "None", ":", "\n", "        ", "Tqdm", ".", "default_mininterval", "=", "value", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.tqdm.Tqdm.set_slower_interval": [[33, 45], ["None"], "methods", ["None"], ["", "@", "staticmethod", "\n", "def", "set_slower_interval", "(", "use_slower_interval", ":", "bool", ")", "->", "None", ":", "\n", "        ", "\"\"\"\n        If `use_slower_interval` is `True`, we will dramatically slow down `tqdm's` default\n        output rate.  `tqdm's` default output rate is great for interactively watching progress,\n        but it is not great for log files.  You might want to set this if you are primarily going\n        to be looking at output through log files, not the terminal.\n        \"\"\"", "\n", "if", "use_slower_interval", ":", "\n", "            ", "Tqdm", ".", "default_mininterval", "=", "10.0", "\n", "", "else", ":", "\n", "            ", "Tqdm", ".", "default_mininterval", "=", "0.1", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.tqdm.Tqdm.tqdm": [[46, 51], ["_tqdm"], "methods", ["None"], ["", "", "@", "staticmethod", "\n", "def", "tqdm", "(", "*", "args", ",", "**", "kwargs", ")", ":", "\n", "        ", "new_kwargs", "=", "{", "\"mininterval\"", ":", "Tqdm", ".", "default_mininterval", ",", "**", "kwargs", "}", "\n", "\n", "return", "_tqdm", "(", "*", "args", ",", "**", "new_kwargs", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.util.FileFriendlyLogFilter.filter": [[259, 265], ["record.msg.replace"], "methods", ["None"], []], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.util.WorkerLogFilter.__init__": [[268, 271], ["logging.Filter.__init__"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.graph_util.plot.GraphPlot.__init__"], []], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.util.WorkerLogFilter.filter": [[272, 276], ["None"], "methods", ["None"], []], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.util.sanitize": [[65, 104], ["isinstance", "isinstance", "x.cpu().tolist", "isinstance", "x.tolist", "isinstance", "x.cpu", "x.item", "isinstance", "isinstance", "util.sanitize", "bool", "isinstance", "x.items", "isinstance", "util.sanitize", "hasattr", "x.to_json", "ValueError", "type"], "function", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.util.sanitize", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.util.sanitize", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.utils.graph.Graph.to_json"], []], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.util.group_by_count": [[109, 125], ["list", "itertools.zip_longest", "iter"], "function", ["None"], []], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.util.lazy_groups_of": [[131, 143], ["iter", "list", "itertools.islice", "len"], "function", ["None"], []], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.util.pad_sequence_to_length": [[145, 189], ["range", "len", "padded_sequence.append", "padded_sequence.insert", "default_value", "default_value"], "function", ["None"], []], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.util.add_noise_to_dict_values": [[191, 205], ["dictionary.items", "random.uniform"], "function", ["None"], []], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.util.namespace_match": [[207, 218], ["namespace.endswith"], "function", ["None"], []], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.util.prepare_environment": [[220, 250], ["params.pop_int", "params.pop_int", "params.pop_int", "allennlp.common.checks.log_pytorch_version_info", "random.seed", "numpy.random.seed", "torch.manual_seed", "torch.manual_seed", "torch.cuda.is_available", "torch.cuda.is_available", "torch.cuda.manual_seed_all", "torch.cuda.manual_seed_all"], "function", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.params.Params.pop_int", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.params.Params.pop_int", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.params.Params.pop_int", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.checks.log_pytorch_version_info"], []], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.util.prepare_global_logging": [[278, 370], ["allennlp.common.tqdm.Tqdm.set_slower_interval", "logging.StreamHandler", "logging.StreamHandler", "logging.Formatter", "logging.getLogger", "util.FileFriendlyLogFilter", "os.environ.get", "logging.FileHandler.addFilter", "logging.FileHandler.addFilter", "logging.FileHandler.setFormatter", "logging.FileHandler.setFormatter", "logging.FileHandler.setLevel", "logging.FileHandler.setLevel", "logging.getLogger.addHandler", "logging.getLogger.addHandler", "logging.getLogger.setLevel", "sys.stdout.isatty", "logging.FileHandler", "logging.FileHandler", "logging.FileHandler", "logging.FileHandler", "util.WorkerLogFilter", "logging.FileHandler.addFilter", "logging.FileHandler.addFilter", "len", "logging.StreamHandler.setFormatter", "logging.StreamHandler.setFormatter", "logging.StreamHandler.setLevel", "logging.StreamHandler.setLevel", "logging.getLogger.addHandler", "logging.getLogger.addHandler", "logging.getLogger.removeHandler", "logging.StreamHandler.addFilter", "logging.StreamHandler.addFilter", "os.path.join", "os.path.join", "os.path.join", "os.path.join"], "function", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.tqdm.Tqdm.set_slower_interval", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.params.Params.get", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.dataset_readers.multiprocess_dataset_reader.QIterable.join", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.dataset_readers.multiprocess_dataset_reader.QIterable.join", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.dataset_readers.multiprocess_dataset_reader.QIterable.join", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.dataset_readers.multiprocess_dataset_reader.QIterable.join"], []], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.util.get_spacy_model": [[375, 407], ["disable.append", "disable.append", "disable.append", "spacy.load", "logger.warning", "spacy.cli.download.download", "__import__", "__import__.load"], "function", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.token_embedders.bert_token_embedder.PretrainedBertModel.load", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.token_embedders.bert_token_embedder.PretrainedBertModel.load"], []], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.util.pushd": [[409, 429], ["os.getcwd", "os.chdir", "logger.info", "os.chdir", "logger.info"], "function", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.dataset_readers.multiprocess_dataset_reader.logger.info", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.dataset_readers.multiprocess_dataset_reader.logger.info"], []], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.util.push_python_path": [[431, 448], ["pathlib.Path().resolve", "str", "sys.path.insert", "sys.path.remove", "pathlib.Path"], "function", ["None"], []], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.util.import_submodules": [[450, 476], ["importlib.invalidate_caches", "util.push_python_path", "importlib.import_module", "getattr", "pkgutil.walk_packages", "util.import_submodules"], "function", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.util.push_python_path", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.util.import_submodules"], []], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.util.peak_memory_mb": [[478, 502], ["resource.getrusage"], "function", ["None"], []], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.util.gpu_memory_mb": [[504, 534], ["subprocess.check_output", "int", "logger.warning", "subprocess.check_output.strip().split", "enumerate", "subprocess.check_output.strip"], "function", ["None"], []], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.util.ensure_list": [[536, 545], ["isinstance", "list"], "function", ["None"], []], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.util.is_lazy": [[547, 553], ["isinstance"], "function", ["None"], []], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.util.log_frozen_and_tunable_parameter_names": [[555, 568], ["util.get_frozen_and_tunable_parameter_names", "logger.info", "logger.info", "logger.info", "logger.info"], "function", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.util.get_frozen_and_tunable_parameter_names", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.dataset_readers.multiprocess_dataset_reader.logger.info", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.dataset_readers.multiprocess_dataset_reader.logger.info", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.dataset_readers.multiprocess_dataset_reader.logger.info", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.dataset_readers.multiprocess_dataset_reader.logger.info"], []], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.util.get_frozen_and_tunable_parameter_names": [[570, 582], ["model.named_parameters", "model.named_parameters"], "function", ["None"], []], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.util.dump_metrics": [[584, 593], ["json.dumps", "logger.info", "open", "metrics_file.write"], "function", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.dataset_readers.multiprocess_dataset_reader.logger.info", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.None.predictor.write"], []], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.util.flatten_filename": [[595, 597], ["file_path.replace"], "function", ["None"], []], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.util.is_master": [[599, 638], ["torch.is_available", "torch.is_initialized", "torch.get_rank", "torch.get_world_size", "int", "os.environ.get"], "function", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.params.Params.get"], []], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.util.is_distributed": [[640, 645], ["torch.is_available", "torch.is_initialized"], "function", ["None"], []], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.util.sanitize_wordpiece": [[647, 657], ["wordpiece.startswith", "wordpiece.startswith"], "function", ["None"], []], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.lazy.Lazy.__init__": [[25, 27], ["None"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "constructor", ":", "Callable", "[", "...", ",", "T", "]", ")", ":", "\n", "        ", "self", ".", "_constructor", "=", "constructor", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.lazy.Lazy.construct": [[28, 30], ["lazy.Lazy._constructor"], "methods", ["None"], ["", "def", "construct", "(", "self", ",", "**", "kwargs", ")", "->", "T", ":", "\n", "        ", "return", "self", ".", "_constructor", "(", "**", "kwargs", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.allennlp.run.run": [[19, 21], ["allennlp.commands.main"], "function", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.ai2_internal.run_with_beaker.main"], ["def", "run", "(", ")", ":", "\n", "    ", "main", "(", "prog", "=", "\"allennlp\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.nn.beam_search.BeamSearch.__init__": [[34, 45], ["None"], "methods", ["None"], ["def", "__init__", "(", "\n", "self", ",", "\n", "end_index", ":", "int", ",", "\n", "max_steps", ":", "int", "=", "50", ",", "\n", "beam_size", ":", "int", "=", "10", ",", "\n", "per_node_beam_size", ":", "int", "=", "None", ",", "\n", ")", "->", "None", ":", "\n", "        ", "self", ".", "_end_index", "=", "end_index", "\n", "self", ".", "max_steps", "=", "max_steps", "\n", "self", ".", "beam_size", "=", "beam_size", "\n", "self", ".", "per_node_beam_size", "=", "per_node_beam_size", "or", "beam_size", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.nn.beam_search.BeamSearch.search": [[46, 299], ["step", "start_class_log_probabilities.topk", "predictions.append", "start_class_log_probabilities.new_full", "state.items", "range", "range", "predictions[].gather().unsqueeze", "reconstructed_predictions.append", "torch.cat", "start_predictions.size", "start_class_log_probabilities.size", "allennlp.common.checks.ConfigurationError", "warnings.warn", "float", "state_tensor.size", "state_tensor.unsqueeze().expand().reshape", "predictions[].reshape", "step", "predictions[].reshape.unsqueeze().expand", "torch.where", "torch.where.topk", "last_log_probabilities.unsqueeze().expand().reshape", "summed_top_log_probabilities.reshape", "predicted_classes.reshape", "summed_top_log_probabilities.reshape.topk", "predicted_classes.reshape.gather", "predictions.append", "backpointers.append", "state.items", "torch.isfinite().all", "warnings.warn", "predictions[].unsqueeze", "predictions[].gather().unsqueeze", "reconstructed_predictions.append", "backpointers[].gather", "list", "start_predicted_classes.unsqueeze", "state_tensor.size", "backpointer.view().expand", "state_tensor.reshape().gather().reshape", "len", "predictions[].gather", "reversed", "state_tensor.unsqueeze().expand", "predictions[].reshape.unsqueeze", "last_log_probabilities.unsqueeze().expand", "torch.isfinite", "predictions[].gather", "backpointer.view", "state_tensor.reshape().gather", "state_tensor.unsqueeze", "last_log_probabilities.unsqueeze", "state_tensor.reshape", "len"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.learning_rate_schedulers.slanted_triangular.SlantedTriangular.step", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.learning_rate_schedulers.slanted_triangular.SlantedTriangular.step"], ["", "def", "search", "(", "\n", "self", ",", "\n", "start_predictions", ":", "torch", ".", "Tensor", ",", "\n", "start_state", ":", "StateType", ",", "\n", "step", ":", "StepFunctionType", ",", "\n", ")", "->", "Tuple", "[", "torch", ".", "Tensor", ",", "torch", ".", "Tensor", "]", ":", "\n", "        ", "\"\"\"\n        Given a starting state and a step function, apply beam search to find the\n        most likely target sequences.\n\n        Notes\n        -----\n        If your step function returns `-inf` for some log probabilities\n        (like if you're using a masked log-softmax) then some of the \"best\"\n        sequences returned may also have `-inf` log probability. Specifically\n        this happens when the beam size is smaller than the number of actions\n        with finite log probability (non-zero probability) returned by the step function.\n        Therefore if you're using a mask you may want to check the results from `search`\n        and potentially discard sequences with non-finite log probability.\n\n        # Parameters\n\n        start_predictions : `torch.Tensor`\n            A tensor containing the initial predictions with shape `(batch_size,)`.\n            Usually the initial predictions are just the index of the \"start\" token\n            in the target vocabulary.\n        start_state : `StateType`\n            The initial state passed to the `step` function. Each value of the state dict\n            should be a tensor of shape `(batch_size, *)`, where `*` means any other\n            number of dimensions.\n        step : `StepFunctionType`\n            A function that is responsible for computing the next most likely tokens,\n            given the current state and the predictions from the last time step.\n            The function should accept two arguments. The first being a tensor\n            of shape `(group_size,)`, representing the index of the predicted\n            tokens from the last time step, and the second being the current state.\n            The `group_size` will be `batch_size * beam_size`, except in the initial\n            step, for which it will just be `batch_size`.\n            The function is expected to return a tuple, where the first element\n            is a tensor of shape `(group_size, target_vocab_size)` containing\n            the log probabilities of the tokens for the next step, and the second\n            element is the updated state. The tensor in the state should have shape\n            `(group_size, *)`, where `*` means any other number of dimensions.\n\n        # Returns\n\n        Tuple[torch.Tensor, torch.Tensor]\n            Tuple of `(predictions, log_probabilities)`, where `predictions`\n            has shape `(batch_size, beam_size, max_steps)` and `log_probabilities`\n            has shape `(batch_size, beam_size)`.\n        \"\"\"", "\n", "batch_size", "=", "start_predictions", ".", "size", "(", ")", "[", "0", "]", "\n", "\n", "# List of (batch_size, beam_size) tensors. One for each time step. Does not", "\n", "# include the start symbols, which are implicit.", "\n", "predictions", ":", "List", "[", "torch", ".", "Tensor", "]", "=", "[", "]", "\n", "\n", "# List of (batch_size, beam_size) tensors. One for each time step. None for", "\n", "# the first.  Stores the index n for the parent prediction, i.e.", "\n", "# predictions[t-1][i][n], that it came from.", "\n", "backpointers", ":", "List", "[", "torch", ".", "Tensor", "]", "=", "[", "]", "\n", "\n", "# Calculate the first timestep. This is done outside the main loop", "\n", "# because we are going from a single decoder input (the output from the", "\n", "# encoder) to the top `beam_size` decoder outputs. On the other hand,", "\n", "# within the main loop we are going from the `beam_size` elements of the", "\n", "# beam to `beam_size`^2 candidates from which we will select the top", "\n", "# `beam_size` elements for the next iteration.", "\n", "# shape: (batch_size, num_classes)", "\n", "start_class_log_probabilities", ",", "state", "=", "step", "(", "start_predictions", ",", "start_state", ")", "\n", "\n", "num_classes", "=", "start_class_log_probabilities", ".", "size", "(", ")", "[", "1", "]", "\n", "\n", "# Make sure `per_node_beam_size` is not larger than `num_classes`.", "\n", "if", "self", ".", "per_node_beam_size", ">", "num_classes", ":", "\n", "            ", "raise", "ConfigurationError", "(", "\n", "f\"Target vocab size ({num_classes:d}) too small \"", "\n", "f\"relative to per_node_beam_size ({self.per_node_beam_size:d}).\\n\"", "\n", "f\"Please decrease beam_size or per_node_beam_size.\"", "\n", ")", "\n", "\n", "# shape: (batch_size, beam_size), (batch_size, beam_size)", "\n", "", "(", "\n", "start_top_log_probabilities", ",", "\n", "start_predicted_classes", ",", "\n", ")", "=", "start_class_log_probabilities", ".", "topk", "(", "self", ".", "beam_size", ")", "\n", "if", "self", ".", "beam_size", "==", "1", "and", "(", "start_predicted_classes", "==", "self", ".", "_end_index", ")", ".", "all", "(", ")", ":", "\n", "            ", "warnings", ".", "warn", "(", "\n", "\"Empty sequences predicted. You may want to increase the beam size or ensure \"", "\n", "\"your step function is working properly.\"", ",", "\n", "RuntimeWarning", ",", "\n", ")", "\n", "return", "start_predicted_classes", ".", "unsqueeze", "(", "-", "1", ")", ",", "start_top_log_probabilities", "\n", "\n", "# The log probabilities for the last time step.", "\n", "# shape: (batch_size, beam_size)", "\n", "", "last_log_probabilities", "=", "start_top_log_probabilities", "\n", "\n", "# shape: [(batch_size, beam_size)]", "\n", "predictions", ".", "append", "(", "start_predicted_classes", ")", "\n", "\n", "# Log probability tensor that mandates that the end token is selected.", "\n", "# shape: (batch_size * beam_size, num_classes)", "\n", "log_probs_after_end", "=", "start_class_log_probabilities", ".", "new_full", "(", "\n", "(", "batch_size", "*", "self", ".", "beam_size", ",", "num_classes", ")", ",", "float", "(", "\"-inf\"", ")", "\n", ")", "\n", "log_probs_after_end", "[", ":", ",", "self", ".", "_end_index", "]", "=", "0.0", "\n", "\n", "# Set the same state for each element in the beam.", "\n", "for", "key", ",", "state_tensor", "in", "state", ".", "items", "(", ")", ":", "\n", "            ", "_", ",", "*", "last_dims", "=", "state_tensor", ".", "size", "(", ")", "\n", "# shape: (batch_size * beam_size, *)", "\n", "state", "[", "key", "]", "=", "(", "\n", "state_tensor", ".", "unsqueeze", "(", "1", ")", "\n", ".", "expand", "(", "batch_size", ",", "self", ".", "beam_size", ",", "*", "last_dims", ")", "\n", ".", "reshape", "(", "batch_size", "*", "self", ".", "beam_size", ",", "*", "last_dims", ")", "\n", ")", "\n", "\n", "", "for", "timestep", "in", "range", "(", "self", ".", "max_steps", "-", "1", ")", ":", "\n", "# shape: (batch_size * beam_size,)", "\n", "            ", "last_predictions", "=", "predictions", "[", "-", "1", "]", ".", "reshape", "(", "batch_size", "*", "self", ".", "beam_size", ")", "\n", "\n", "# If every predicted token from the last step is `self._end_index`,", "\n", "# then we can stop early.", "\n", "if", "(", "last_predictions", "==", "self", ".", "_end_index", ")", ".", "all", "(", ")", ":", "\n", "                ", "break", "\n", "\n", "# Take a step. This get the predicted log probs of the next classes", "\n", "# and updates the state.", "\n", "# shape: (batch_size * beam_size, num_classes)", "\n", "", "class_log_probabilities", ",", "state", "=", "step", "(", "last_predictions", ",", "state", ")", "\n", "\n", "# shape: (batch_size * beam_size, num_classes)", "\n", "last_predictions_expanded", "=", "last_predictions", ".", "unsqueeze", "(", "-", "1", ")", ".", "expand", "(", "\n", "batch_size", "*", "self", ".", "beam_size", ",", "num_classes", "\n", ")", "\n", "\n", "# Here we are finding any beams where we predicted the end token in", "\n", "# the previous timestep and replacing the distribution with a", "\n", "# one-hot distribution, forcing the beam to predict the end token", "\n", "# this timestep as well.", "\n", "# shape: (batch_size * beam_size, num_classes)", "\n", "cleaned_log_probabilities", "=", "torch", ".", "where", "(", "\n", "last_predictions_expanded", "==", "self", ".", "_end_index", ",", "\n", "log_probs_after_end", ",", "\n", "class_log_probabilities", ",", "\n", ")", "\n", "\n", "# shape (both): (batch_size * beam_size, per_node_beam_size)", "\n", "top_log_probabilities", ",", "predicted_classes", "=", "cleaned_log_probabilities", ".", "topk", "(", "\n", "self", ".", "per_node_beam_size", "\n", ")", "\n", "\n", "# Here we expand the last log probabilities to (batch_size * beam_size, per_node_beam_size)", "\n", "# so that we can add them to the current log probs for this timestep.", "\n", "# This lets us maintain the log probability of each element on the beam.", "\n", "# shape: (batch_size * beam_size, per_node_beam_size)", "\n", "expanded_last_log_probabilities", "=", "(", "\n", "last_log_probabilities", ".", "unsqueeze", "(", "2", ")", "\n", ".", "expand", "(", "batch_size", ",", "self", ".", "beam_size", ",", "self", ".", "per_node_beam_size", ")", "\n", ".", "reshape", "(", "batch_size", "*", "self", ".", "beam_size", ",", "self", ".", "per_node_beam_size", ")", "\n", ")", "\n", "\n", "# shape: (batch_size * beam_size, per_node_beam_size)", "\n", "summed_top_log_probabilities", "=", "(", "\n", "top_log_probabilities", "+", "expanded_last_log_probabilities", "\n", ")", "\n", "\n", "# shape: (batch_size, beam_size * per_node_beam_size)", "\n", "reshaped_summed", "=", "summed_top_log_probabilities", ".", "reshape", "(", "\n", "batch_size", ",", "self", ".", "beam_size", "*", "self", ".", "per_node_beam_size", "\n", ")", "\n", "\n", "# shape: (batch_size, beam_size * per_node_beam_size)", "\n", "reshaped_predicted_classes", "=", "predicted_classes", ".", "reshape", "(", "\n", "batch_size", ",", "self", ".", "beam_size", "*", "self", ".", "per_node_beam_size", "\n", ")", "\n", "\n", "# Keep only the top `beam_size` beam indices.", "\n", "# shape: (batch_size, beam_size), (batch_size, beam_size)", "\n", "restricted_beam_log_probs", ",", "restricted_beam_indices", "=", "reshaped_summed", ".", "topk", "(", "\n", "self", ".", "beam_size", "\n", ")", "\n", "\n", "# Use the beam indices to extract the corresponding classes.", "\n", "# shape: (batch_size, beam_size)", "\n", "restricted_predicted_classes", "=", "reshaped_predicted_classes", ".", "gather", "(", "\n", "1", ",", "restricted_beam_indices", "\n", ")", "\n", "\n", "predictions", ".", "append", "(", "restricted_predicted_classes", ")", "\n", "\n", "# shape: (batch_size, beam_size)", "\n", "last_log_probabilities", "=", "restricted_beam_log_probs", "\n", "\n", "# The beam indices come from a `beam_size * per_node_beam_size` dimension where the", "\n", "# indices with a common ancestor are grouped together. Hence", "\n", "# dividing by per_node_beam_size gives the ancestor. (Note that this is integer", "\n", "# division as the tensor is a LongTensor.)", "\n", "# shape: (batch_size, beam_size)", "\n", "backpointer", "=", "restricted_beam_indices", "/", "self", ".", "per_node_beam_size", "\n", "\n", "backpointers", ".", "append", "(", "backpointer", ")", "\n", "\n", "# Keep only the pieces of the state tensors corresponding to the", "\n", "# ancestors created this iteration.", "\n", "for", "key", ",", "state_tensor", "in", "state", ".", "items", "(", ")", ":", "\n", "                ", "_", ",", "*", "last_dims", "=", "state_tensor", ".", "size", "(", ")", "\n", "# shape: (batch_size, beam_size, *)", "\n", "expanded_backpointer", "=", "backpointer", ".", "view", "(", "\n", "batch_size", ",", "self", ".", "beam_size", ",", "*", "(", "[", "1", "]", "*", "len", "(", "last_dims", ")", ")", "\n", ")", ".", "expand", "(", "batch_size", ",", "self", ".", "beam_size", ",", "*", "last_dims", ")", "\n", "\n", "# shape: (batch_size * beam_size, *)", "\n", "state", "[", "key", "]", "=", "(", "\n", "state_tensor", ".", "reshape", "(", "batch_size", ",", "self", ".", "beam_size", ",", "*", "last_dims", ")", "\n", ".", "gather", "(", "1", ",", "expanded_backpointer", ")", "\n", ".", "reshape", "(", "batch_size", "*", "self", ".", "beam_size", ",", "*", "last_dims", ")", "\n", ")", "\n", "\n", "", "", "if", "not", "torch", ".", "isfinite", "(", "last_log_probabilities", ")", ".", "all", "(", ")", ":", "\n", "            ", "warnings", ".", "warn", "(", "\n", "\"Infinite log probabilities encountered. Some final sequences may not make sense. \"", "\n", "\"This can happen when the beam size is larger than the number of valid (non-zero \"", "\n", "\"probability) transitions that the step function produces.\"", ",", "\n", "RuntimeWarning", ",", "\n", ")", "\n", "\n", "# Reconstruct the sequences.", "\n", "# shape: [(batch_size, beam_size, 1)]", "\n", "", "reconstructed_predictions", "=", "[", "predictions", "[", "-", "1", "]", ".", "unsqueeze", "(", "2", ")", "]", "\n", "\n", "# shape: (batch_size, beam_size)", "\n", "cur_backpointers", "=", "backpointers", "[", "-", "1", "]", "\n", "\n", "for", "timestep", "in", "range", "(", "len", "(", "predictions", ")", "-", "2", ",", "0", ",", "-", "1", ")", ":", "\n", "# shape: (batch_size, beam_size, 1)", "\n", "            ", "cur_preds", "=", "predictions", "[", "timestep", "]", ".", "gather", "(", "1", ",", "cur_backpointers", ")", ".", "unsqueeze", "(", "2", ")", "\n", "\n", "reconstructed_predictions", ".", "append", "(", "cur_preds", ")", "\n", "\n", "# shape: (batch_size, beam_size)", "\n", "cur_backpointers", "=", "backpointers", "[", "timestep", "-", "1", "]", ".", "gather", "(", "1", ",", "cur_backpointers", ")", "\n", "\n", "# shape: (batch_size, beam_size, 1)", "\n", "", "final_preds", "=", "predictions", "[", "0", "]", ".", "gather", "(", "1", ",", "cur_backpointers", ")", ".", "unsqueeze", "(", "2", ")", "\n", "\n", "reconstructed_predictions", ".", "append", "(", "final_preds", ")", "\n", "\n", "# shape: (batch_size, beam_size, max_steps)", "\n", "all_predictions", "=", "torch", ".", "cat", "(", "list", "(", "reversed", "(", "reconstructed_predictions", ")", ")", ",", "2", ")", "\n", "\n", "return", "all_predictions", ",", "last_log_probabilities", "\n", "", "", ""]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.nn.initializers.Initializer.__call__": [[50, 58], ["None"], "methods", ["None"], ["def", "__call__", "(", "self", ",", "tensor", ":", "torch", ".", "Tensor", ",", "**", "kwargs", ")", "->", "None", ":", "\n", "        ", "\"\"\"\n        This function is here just to make mypy happy.  We expect initialization functions to\n        follow this API; the builtin pytorch initialization functions follow this just fine, even\n        though they don't subclass `Initialization`.  We're just making it explicit here, so mypy\n        knows that initializers are callable like this.\n        \"\"\"", "\n", "raise", "NotImplementedError", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.nn.initializers.PretrainedModelInitializer.__init__": [[267, 272], ["torch.load", "torch.load", "torch.load", "torch.load"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.token_embedders.bert_token_embedder.PretrainedBertModel.load", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.token_embedders.bert_token_embedder.PretrainedBertModel.load", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.token_embedders.bert_token_embedder.PretrainedBertModel.load", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.token_embedders.bert_token_embedder.PretrainedBertModel.load"], ["def", "__init__", "(", "\n", "self", ",", "weights_file_path", ":", "str", ",", "parameter_name_overrides", ":", "Dict", "[", "str", ",", "str", "]", "=", "None", "\n", ")", "->", "None", ":", "\n", "        ", "self", ".", "weights", ":", "Dict", "[", "str", ",", "torch", ".", "Tensor", "]", "=", "torch", ".", "load", "(", "weights_file_path", ")", "\n", "self", ".", "parameter_name_overrides", "=", "parameter_name_overrides", "or", "{", "}", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.nn.initializers.PretrainedModelInitializer.__call__": [[273, 291], ["tensor.data.size", "source_weights.size", "allennlp.common.checks.ConfigurationError", "tensor.data.size", "source_weights.size"], "methods", ["None"], ["", "@", "overrides", "\n", "def", "__call__", "(", "self", ",", "tensor", ":", "torch", ".", "Tensor", ",", "parameter_name", ":", "str", ",", "**", "kwargs", ")", "->", "None", ":", "# type: ignore", "\n", "# Select the new parameter name if it's being overridden", "\n", "        ", "if", "parameter_name", "in", "self", ".", "parameter_name_overrides", ":", "\n", "            ", "parameter_name", "=", "self", ".", "parameter_name_overrides", "[", "parameter_name", "]", "\n", "\n", "# If the size of the source and destination tensors are not the", "\n", "# same, then we need to raise an error", "\n", "", "source_weights", "=", "self", ".", "weights", "[", "parameter_name", "]", "\n", "if", "tensor", ".", "data", ".", "size", "(", ")", "!=", "source_weights", ".", "size", "(", ")", ":", "\n", "            ", "raise", "ConfigurationError", "(", "\n", "\"Incompatible sizes found for parameter %s. \"", "\n", "\"Found %s and %s\"", "\n", "%", "(", "parameter_name", ",", "tensor", ".", "data", ".", "size", "(", ")", ",", "source_weights", ".", "size", "(", ")", ")", "\n", ")", "\n", "\n", "# Copy the parameters from the source to the destination", "\n", "", "tensor", ".", "data", "[", ":", "]", "=", "source_weights", "[", ":", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.nn.initializers.InitializerApplicator.__init__": [[300, 318], ["None"], "methods", ["None"], ["def", "__init__", "(", "\n", "self", ",", "\n", "initializers", ":", "List", "[", "Tuple", "[", "str", ",", "Initializer", "]", "]", "=", "None", ",", "\n", "prevent_regexes", ":", "List", "[", "str", "]", "=", "None", ",", "\n", ")", "->", "None", ":", "\n", "        ", "\"\"\"\n        # Parameters\n\n        initializers : `List[Tuple[str, Initializer]]`, optional (default = [])\n            A list mapping parameter regexes to initializers.  We will check each parameter against\n            each regex in turn, and apply the initializer paired with the first matching regex, if\n            any. If \"prevent\" is assigned to any regex, then it will override and prevent the matched\n            parameters to be initialzed.\n        \"\"\"", "\n", "self", ".", "_initializers", "=", "initializers", "or", "[", "]", "\n", "self", ".", "_prevent_regex", "=", "None", "\n", "if", "prevent_regexes", ":", "\n", "            ", "self", ".", "_prevent_regex", "=", "\"(\"", "+", "\")|(\"", ".", "join", "(", "prevent_regexes", ")", "+", "\")\"", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.nn.initializers.InitializerApplicator.__call__": [[319, 359], ["logger.info", "set", "module.named_parameters", "logger.info", "list", "list.sort", "logger.warning", "logger.info", "set.add", "re.search", "logger.info", "initializer", "unused_regexes.discard", "bool", "re.search"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.dataset_readers.multiprocess_dataset_reader.logger.info", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.dataset_readers.multiprocess_dataset_reader.logger.info", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.dataset_readers.multiprocess_dataset_reader.logger.info", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.nn.beam_search.BeamSearch.search", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.dataset_readers.multiprocess_dataset_reader.logger.info", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.nn.beam_search.BeamSearch.search"], ["", "", "def", "__call__", "(", "self", ",", "module", ":", "torch", ".", "nn", ".", "Module", ")", "->", "None", ":", "\n", "        ", "\"\"\"\n        Applies an initializer to all parameters in a module that match one of the regexes we were\n        given in this object's constructor.  Does nothing to parameters that do not match.\n\n        # Parameters\n\n        module : torch.nn.Module, required.\n            The Pytorch module to apply the initializers to.\n        \"\"\"", "\n", "logger", ".", "info", "(", "\"Initializing parameters\"", ")", "\n", "unused_regexes", "=", "{", "initializer", "[", "0", "]", "for", "initializer", "in", "self", ".", "_initializers", "}", "\n", "uninitialized_parameters", "=", "set", "(", ")", "\n", "# Store which initialisers were applied to which parameters.", "\n", "for", "name", ",", "parameter", "in", "module", ".", "named_parameters", "(", ")", ":", "\n", "            ", "for", "initializer_regex", ",", "initializer", "in", "self", ".", "_initializers", ":", "\n", "                ", "allow", "=", "self", ".", "_prevent_regex", "is", "None", "or", "not", "bool", "(", "\n", "re", ".", "search", "(", "self", ".", "_prevent_regex", ",", "name", ")", "\n", ")", "\n", "if", "allow", "and", "re", ".", "search", "(", "initializer_regex", ",", "name", ")", ":", "\n", "                    ", "logger", ".", "info", "(", "\n", "\"Initializing %s using %s initializer\"", ",", "name", ",", "initializer_regex", "\n", ")", "\n", "initializer", "(", "parameter", ",", "parameter_name", "=", "name", ")", "\n", "unused_regexes", ".", "discard", "(", "initializer_regex", ")", "\n", "break", "\n", "", "", "else", ":", "# no break", "\n", "                ", "uninitialized_parameters", ".", "add", "(", "name", ")", "\n", "", "", "for", "regex", "in", "unused_regexes", ":", "\n", "            ", "logger", ".", "warning", "(", "\n", "\"Did not use initialization regex that was passed: %s\"", ",", "regex", "\n", ")", "\n", "", "logger", ".", "info", "(", "\n", "\"Done initializing parameters; the following parameters are using their \"", "\n", "\"default initialization from their code\"", "\n", ")", "\n", "uninitialized_parameter_list", "=", "list", "(", "uninitialized_parameters", ")", "\n", "uninitialized_parameter_list", ".", "sort", "(", ")", "\n", "for", "name", "in", "uninitialized_parameter_list", ":", "\n", "            ", "logger", ".", "info", "(", "\"   %s\"", ",", "name", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.nn.initializers.InitializerApplicator.from_params": [[360, 407], ["initializers.InitializerApplicator", "initializers.InitializerApplicator.from_params.is_prevent"], "methods", ["None"], ["", "", "@", "classmethod", "\n", "def", "from_params", "(", "\n", "cls", ",", "params", ":", "List", "[", "Tuple", "[", "str", ",", "Params", "]", "]", "=", "None", "\n", ")", "->", "\"InitializerApplicator\"", ":", "\n", "        ", "\"\"\"\n        Converts a Params object into an InitializerApplicator. The json should\n        be formatted as follows:\n\n        ```\n            [\n                [\"parameter_regex_match1\",\n                    {\n                        \"type\": \"normal\"\n                        \"mean\": 0.01\n                        \"std\": 0.1\n                    }\n                ],\n                [\"parameter_regex_match2\", \"uniform\"]\n                [\"prevent_init_regex\", \"prevent\"]\n            ]\n        ```\n\n        where the first item in each tuple is the regex that matches to parameters, and the second\n        item is a set of parameters that will be passed to `Initialzer.from_params()`.  These\n        values can either be strings, in which case they correspond to the names of initializers,\n        or dictionaries, in which case they must contain the \"type\" key, corresponding to the name\n        of an initializer.  In addition, they may contain auxiliary named parameters which will be\n        fed to the initializer itself. To determine valid auxiliary parameters, please refer to the\n        torch.nn.init documentation. Only \"prevent\" is a special type which does not have corresponding\n        initializer. Any parameter matching its corresponding regex will be overridden to NOT initialize.\n\n        # Returns\n\n        An InitializerApplicator containing the specified initializers.\n        \"\"\"", "\n", "\n", "params", "=", "params", "or", "[", "]", "\n", "\n", "def", "is_prevent", "(", "item", ")", ":", "\n", "            ", "return", "item", "in", "(", "\"prevent\"", ",", "{", "\"type\"", ":", "\"prevent\"", "}", ")", "\n", "\n", "", "prevent_regexes", "=", "[", "param", "[", "0", "]", "for", "param", "in", "params", "if", "is_prevent", "(", "param", "[", "1", "]", ")", "]", "\n", "params", "=", "[", "param", "for", "param", "in", "params", "if", "param", "[", "1", "]", "if", "not", "is_prevent", "(", "param", "[", "1", "]", ")", "]", "\n", "initializers", "=", "[", "\n", "(", "name", ",", "Initializer", ".", "from_params", "(", "init_params", ")", ")", "for", "name", ",", "init_params", "in", "params", "\n", "]", "\n", "return", "InitializerApplicator", "(", "initializers", ",", "prevent_regexes", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.nn.initializers.uniform_unit_scaling": [[60, 98], ["torch.nn.init.calculate_gain", "torch.nn.init.calculate_gain", "tensor.data.uniform_", "list", "math.sqrt", "tensor.size"], "function", ["None"], ["", "", "def", "uniform_unit_scaling", "(", "tensor", ":", "torch", ".", "Tensor", ",", "nonlinearity", ":", "str", "=", "\"linear\"", ")", ":", "\n", "    ", "\"\"\"\n    An initaliser which preserves output variance for approximately gaussian\n    distributed inputs. This boils down to initialising layers using a uniform\n    distribution in the range `(-sqrt(3/dim[0]) * scale, sqrt(3 / dim[0]) * scale)`, where\n    `dim[0]` is equal to the input dimension of the parameter and the `scale`\n    is a constant scaling factor which depends on the non-linearity used.\n\n    See `Random Walk Initialisation for Training Very Deep Feedforward Networks\n    <https://www.semanticscholar.org/paper/Random-Walk-Initialization-for-Training-Very-Deep-Sussillo-Abbott/be9728a0728b6acf7a485225b1e41592176eda0b>`_\n    for more information.\n\n    # Parameters\n\n    tensor : `torch.Tensor`, required.\n        The tensor to initialise.\n    nonlinearity : `str`, optional (default = \"linear\")\n        The non-linearity which is performed after the projection that this\n        tensor is involved in. This must be the name of a function contained\n        in the `torch.nn.functional` package.\n\n    # Returns\n\n    The initialised tensor.\n    \"\"\"", "\n", "size", "=", "1.0", "\n", "# Estimate the input size. This won't work perfectly,", "\n", "# but it covers almost all use cases where this initialiser", "\n", "# would be expected to be useful, i.e in large linear and", "\n", "# convolutional layers, as the last dimension will almost", "\n", "# always be the output size.", "\n", "for", "dimension", "in", "list", "(", "tensor", ".", "size", "(", ")", ")", "[", ":", "-", "1", "]", ":", "\n", "        ", "size", "*=", "dimension", "\n", "\n", "", "activation_scaling", "=", "torch", ".", "nn", ".", "init", ".", "calculate_gain", "(", "nonlinearity", ",", "tensor", ")", "\n", "max_value", "=", "math", ".", "sqrt", "(", "3", "/", "size", ")", "*", "activation_scaling", "\n", "\n", "return", "tensor", ".", "data", ".", "uniform_", "(", "-", "max_value", ",", "max_value", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.nn.initializers.block_orthogonal": [[100, 149], ["list", "any", "itertools.product", "tensor.size", "allennlp.common.checks.ConfigurationError", "list", "zip", "tuple", "torch.nn.init.orthogonal_", "torch.nn.init.orthogonal_", "range", "zip", "tensor[].contiguous", "zip", "slice"], "function", ["None"], ["", "def", "block_orthogonal", "(", "\n", "tensor", ":", "torch", ".", "Tensor", ",", "split_sizes", ":", "List", "[", "int", "]", ",", "gain", ":", "float", "=", "1.0", "\n", ")", "->", "None", ":", "\n", "    ", "\"\"\"\n    An initializer which allows initializing model parameters in \"blocks\". This is helpful\n    in the case of recurrent models which use multiple gates applied to linear projections,\n    which can be computed efficiently if they are concatenated together. However, they are\n    separate parameters which should be initialized independently.\n\n    # Parameters\n\n    tensor : `torch.Tensor`, required.\n        A tensor to initialize.\n    split_sizes : List[int], required.\n        A list of length `tensor.ndim()` specifying the size of the\n        blocks along that particular dimension. E.g. `[10, 20]` would\n        result in the tensor being split into chunks of size 10 along the\n        first dimension and 20 along the second.\n    gain : float, optional (default = 1.0)\n        The gain (scaling) applied to the orthogonal initialization.\n    \"\"\"", "\n", "data", "=", "tensor", ".", "data", "\n", "sizes", "=", "list", "(", "tensor", ".", "size", "(", ")", ")", "\n", "if", "any", "(", "[", "a", "%", "b", "!=", "0", "for", "a", ",", "b", "in", "zip", "(", "sizes", ",", "split_sizes", ")", "]", ")", ":", "\n", "        ", "raise", "ConfigurationError", "(", "\n", "\"tensor dimensions must be divisible by their respective \"", "\n", "\"split_sizes. Found size: {} and split_sizes: {}\"", ".", "format", "(", "sizes", ",", "split_sizes", ")", "\n", ")", "\n", "", "indexes", "=", "[", "\n", "list", "(", "range", "(", "0", ",", "max_size", ",", "split", ")", ")", "for", "max_size", ",", "split", "in", "zip", "(", "sizes", ",", "split_sizes", ")", "\n", "]", "\n", "# Iterate over all possible blocks within the tensor.", "\n", "for", "block_start_indices", "in", "itertools", ".", "product", "(", "*", "indexes", ")", ":", "\n", "# A list of tuples containing the index to start at for this block", "\n", "# and the appropriate step size (i.e split_size[i] for dimension i).", "\n", "        ", "index_and_step_tuples", "=", "zip", "(", "block_start_indices", ",", "split_sizes", ")", "\n", "# This is a tuple of slices corresponding to:", "\n", "# tensor[index: index + step_size, ...]. This is", "\n", "# required because we could have an arbitrary number", "\n", "# of dimensions. The actual slices we need are the", "\n", "# start_index: start_index + step for each dimension in the tensor.", "\n", "block_slice", "=", "tuple", "(", "\n", "[", "\n", "slice", "(", "start_index", ",", "start_index", "+", "step", ")", "\n", "for", "start_index", ",", "step", "in", "index_and_step_tuples", "\n", "]", "\n", ")", "\n", "data", "[", "block_slice", "]", "=", "torch", ".", "nn", ".", "init", ".", "orthogonal_", "(", "\n", "tensor", "[", "block_slice", "]", ".", "contiguous", "(", ")", ",", "gain", "=", "gain", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.nn.initializers.zero": [[152, 154], ["tensor.data.zero_"], "function", ["None"], ["", "", "def", "zero", "(", "tensor", ":", "torch", ".", "Tensor", ")", "->", "None", ":", "\n", "    ", "return", "tensor", ".", "data", ".", "zero_", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.nn.initializers.lstm_hidden_bias": [[156, 165], ["tensor.data.zero_"], "function", ["None"], ["", "def", "lstm_hidden_bias", "(", "tensor", ":", "torch", ".", "Tensor", ")", "->", "None", ":", "\n", "    ", "\"\"\"\n    Initialize the biases of the forget gate to 1, and all other gates to 0,\n    following Jozefowicz et al., An Empirical Exploration of Recurrent Network Architectures\n    \"\"\"", "\n", "# gates are (b_hi|b_hf|b_hg|b_ho) of shape (4*hidden_size)", "\n", "tensor", ".", "data", ".", "zero_", "(", ")", "\n", "hidden_size", "=", "tensor", ".", "shape", "[", "0", "]", "//", "4", "\n", "tensor", ".", "data", "[", "hidden_size", ":", "(", "2", "*", "hidden_size", ")", "]", "=", "1.0", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.nn.initializers._initializer_wrapper": [[167, 186], ["initializers.._init_function", "cls", "params.as_dict"], "function", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.params.Params.as_dict"], ["", "def", "_initializer_wrapper", "(", "init_function", ":", "Callable", "[", "...", ",", "None", "]", ")", "->", "Type", "[", "Initializer", "]", ":", "\n", "    ", "class", "Init", "(", "Initializer", ")", ":", "\n", "        ", "_initializer_wrapper", "=", "True", "\n", "\n", "def", "__init__", "(", "self", ",", "**", "kwargs", ")", ":", "\n", "            ", "self", ".", "_init_function", "=", "init_function", "\n", "self", ".", "_kwargs", "=", "kwargs", "\n", "\n", "", "def", "__call__", "(", "self", ",", "tensor", ":", "torch", ".", "Tensor", ",", "**", "kwargs", ")", "->", "None", ":", "\n", "            ", "self", ".", "_init_function", "(", "tensor", ",", "**", "self", ".", "_kwargs", ")", "\n", "\n", "", "def", "__repr__", "(", "self", ")", ":", "\n", "            ", "return", "\"Init: %s, with params: %s\"", "%", "(", "self", ".", "_init_function", ",", "self", ".", "_kwargs", ")", "\n", "\n", "", "@", "classmethod", "\n", "def", "from_params", "(", "cls", ",", "params", ":", "Params", ",", "**", "extras", ")", ":", "# type: ignore", "\n", "            ", "return", "cls", "(", "**", "params", ".", "as_dict", "(", ")", ")", "\n", "\n", "", "", "return", "Init", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.nn.util.has_tensor": [[22, 35], ["isinstance", "isinstance", "any", "isinstance", "any", "util.has_tensor", "obj.values", "util.has_tensor"], "function", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.nn.util.has_tensor", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.nn.util.has_tensor"], ["\n", "### Charge default iterator ###", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.nn.util.move_to_device": [[37, 58], ["isinstance", "util.has_tensor", "obj.cuda", "isinstance", "isinstance", "util.move_to_device", "obj.items", "util.move_to_device", "isinstance", "hasattr", "obj.__class__", "isinstance", "tuple", "util.move_to_device", "util.move_to_device"], "function", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.nn.util.has_tensor", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.nn.util.move_to_device", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.nn.util.move_to_device", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.nn.util.move_to_device", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.nn.util.move_to_device"], ["\n", "default_iterator_params", "=", "iterators_params", ".", "pop", "(", "\"iterator\"", ")", "\n", "default_iterator", "=", "DataIterator", ".", "from_params", "(", "default_iterator_params", ")", "\n", "default_iterator", ".", "index_with", "(", "vocab", ")", "\n", "\n", "### Charge dataset specific iterators ###", "\n", "for", "task", "in", "task_list", ":", "\n", "        ", "specific_iterator_params", "=", "iterators_params", ".", "pop", "(", "\"iterator_\"", "+", "task", ".", "_name", ",", "None", ")", "\n", "if", "specific_iterator_params", "is", "not", "None", ":", "\n", "            ", "specific_iterator", "=", "DataIterator", ".", "from_params", "(", "specific_iterator_params", ")", "\n", "specific_iterator", ".", "index_with", "(", "vocab", ")", "\n", "task", ".", "set_data_iterator", "(", "specific_iterator", ")", "\n", "", "else", ":", "\n", "            ", "task", ".", "set_data_iterator", "(", "default_iterator", ")", "\n", "\n", "", "", "return", "task_list", "\n", "", ""]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.nn.util.clamp_tensor": [[60, 73], ["tensor.coalesce", "tensor.coalesce._values().clamp_", "tensor.clamp", "tensor.coalesce._values"], "function", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.metrics.f1_score.F1.clamp"], []], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.nn.util.batch_tensor_dicts": [[75, 103], ["collections.defaultdict", "key_to_tensors.items", "tensor_dict.items", "torch.stack", "key_to_tensors[].append", "all", "batched_tensor.squeeze.squeeze", "tensor.size"], "function", ["None"], []], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.nn.util.get_lengths_from_binary_sequence_mask": [[105, 122], ["mask.long().sum", "mask.long"], "function", ["None"], []], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.nn.util.get_mask_from_sequence_lengths": [[124, 141], ["sequence_lengths.new_ones", "sequence_lengths.new_ones.cumsum", "sequence_lengths.size", "sequence_lengths.unsqueeze"], "function", ["None"], []], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.nn.util.sort_batch_by_length": [[143, 191], ["sequence_lengths.sort", "tensor.index_select", "torch.arange", "permutation_index.sort", "torch.arange.index_select", "allennlp.common.checks.ConfigurationError", "len", "isinstance", "isinstance"], "function", ["None"], []], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.nn.util.get_final_encoder_states": [[194, 231], ["encoder_outputs.size", "last_word_indices.view().expand", "encoder_outputs.gather", "torch.cat.squeeze", "mask.sum().long", "torch.cat", "last_word_indices.view", "mask.sum"], "function", ["None"], []], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.nn.util.get_dropout_mask": [[233, 259], ["binary_mask.float().div", "binary_mask.float", "torch.rand", "tensor_for_masking.size"], "function", ["None"], []], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.nn.util.masked_softmax": [[261, 304], ["torch.nn.functional.softmax", "mask.unsqueeze.float", "mask.unsqueeze.dim", "vector.dim", "mask.unsqueeze.unsqueeze", "torch.nn.functional.softmax", "vector.masked_fill", "torch.nn.functional.softmax", "torch.nn.functional.softmax.sum"], "function", ["None"], []], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.nn.util.masked_log_softmax": [[306, 340], ["torch.nn.functional.log_softmax", "mask.unsqueeze.float", "mask.unsqueeze.dim", "vector.dim", "mask.unsqueeze.unsqueeze"], "function", ["None"], []], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.nn.util.masked_max": [[342, 373], ["vector.masked_fill", "vector.masked_fill.max"], "function", ["None"], []], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.nn.util.masked_mean": [[375, 408], ["vector.masked_fill", "torch.sum", "torch.sum", "mask.float", "torch.sum.clamp"], "function", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.metrics.f1_score.F1.clamp"], []], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.nn.util.masked_flip": [[410, 438], ["padded_sequence.size", "torch.flip", "torch.nn.utils.rnn.pad_sequence", "padded_sequence.size", "len", "len", "padded_sequence.size", "enumerate"], "function", ["None"], []], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.nn.util.viterbi_decode": [[440, 619], ["list", "range", "path_scores[].view", "min", "torch.topk", "range", "torch.cat.size", "torch.zeros", "torch.cat", "torch.cat", "torch.zeros", "torch.cat", "torch.cat", "torch.cat.size", "torch.zeros", "path_scores.append", "path_scores.append", "summed_potentials.view.view", "min", "torch.topk", "path_indices.append", "reversed", "viterbi_path.reverse", "viterbi_paths.append", "ValueError", "torch.zeros", "torch.zeros", "len", "allennlp.common.checks.ConfigurationError", "torch.ones", "torch.zeros.unsqueeze", "tag_sequence[].unsqueeze", "path_scores[].unsqueeze", "torch.zeros", "path_scores.append", "path_scores.append", "paths.squeeze", "path_scores[].view.size", "viterbi_path.append", "torch.tensor", "torch.tensor", "range", "summed_potentials.view.size", "logger.warning", "torch.zeros.unsqueeze", "int", "backward_timestep.view"], "function", ["None"], []], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.nn.util.get_text_field_mask": [[621, 683], ["text_field_tensors.items", "tensor_dims.sort", "len", "masks.append", "len", "ValueError", "tensor.dim", "text_field_tensors.values", "indexer_output.values", "ValueError"], "function", ["None"], []], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.nn.util.get_token_ids_from_text_field_tensors": [[686, 702], ["text_field_tensors.items", "NotImplementedError", "indexer_tensors.items"], "function", ["None"], []], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.nn.util.weighted_sum": [[707, 744], ["intermediate.sum", "attention.unsqueeze().bmm().squeeze", "attention.bmm", "attention.dim", "list", "range", "matrix.unsqueeze.expand", "attention.unsqueeze().expand_as", "attention.dim", "matrix.unsqueeze.dim", "attention.dim", "matrix.unsqueeze.dim", "matrix.unsqueeze.dim", "matrix.unsqueeze.size", "matrix.unsqueeze.unsqueeze", "list.insert", "attention.unsqueeze().bmm", "attention.size", "attention.unsqueeze", "attention.dim", "matrix.unsqueeze.dim", "attention.unsqueeze"], "function", ["None"], []], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.nn.util.sequence_cross_entropy_with_logits": [[746, 906], ["weights.float.float", "tuple", "weights.float.sum", "logits.view", "torch.nn.functional.log_softmax", "targets.view().long", "negative_log_likelihood_flat.sum.view", "ValueError", "range", "logits.size", "torch.nn.functional.log_softmax.exp", "torch.gather", "focal_factor.view.view", "isinstance", "torch.gather().view", "logits.size", "torch.zeros_like().scatter_", "negative_log_likelihood_flat.sum.sum", "len", "targets.view", "torch.tensor", "isinstance", "torch.gather", "targets.size", "negative_log_likelihood_flat.view.sum", "per_batch_loss.sum", "targets.size", "torch.tensor", "TypeError", "torch.gather", "targets.size", "torch.zeros_like", "negative_log_likelihood_flat.view.sum", "negative_log_likelihood_flat.view.sum", "float", "torch.cat.size", "torch.cat.view", "torch.cat", "weights.sum.sum", "float", "type", "targets.view().long.view"], "function", ["None"], []], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.nn.util.replace_masked_values": [[908, 925], ["tensor.masked_fill", "tensor.dim", "mask.dim", "allennlp.common.checks.ConfigurationError", "tensor.dim", "mask.dim"], "function", ["None"], []], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.nn.util.tensors_equal": [[927, 967], ["isinstance", "all", "isinstance", "all", "isinstance", "isinstance", "len", "len", "util.tensors_equal", "isinstance", "tensor1.keys", "tensor2.keys", "zip", "util.tensors_equal", "isinstance", "tensor1.size", "tensor2.size", "print", "type", "type"], "function", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.nn.util.tensors_equal", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.nn.util.tensors_equal"], []], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.nn.util.device_mapping": [[969, 983], ["storage.cuda"], "function", ["None"], []], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.nn.util.combine_tensors": [[985, 1016], ["combination.replace().replace.replace().replace", "torch.cat", "len", "allennlp.common.checks.ConfigurationError", "util._get_combination", "combination.replace().replace.replace", "combination.replace().replace.split"], "function", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.nn.util._get_combination"], []], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.nn.util._rindex": [[1018, 1037], ["range", "ValueError", "len"], "function", ["None"], []], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.nn.util._get_combination": [[1039, 1059], ["combination.isdigit", "util._get_combination", "util._get_combination", "int", "len", "allennlp.common.checks.ConfigurationError", "allennlp.common.checks.ConfigurationError"], "function", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.nn.util._get_combination", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.nn.util._get_combination"], []], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.nn.util.combine_tensors_and_multiply": [[1061, 1099], ["combination.replace().replace.replace().replace", "combination.replace().replace.split", "zip", "len", "allennlp.common.checks.ConfigurationError", "tensor.size", "util._get_combination_dim", "to_sum.append", "combination.replace().replace.replace", "util._get_combination_and_multiply"], "function", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.nn.util._get_combination_dim", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.nn.util._get_combination_and_multiply"], []], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.nn.util._get_combination_and_multiply": [[1101, 1153], ["combination.isdigit", "torch.matmul", "util._get_combination", "util._get_combination", "int", "len", "allennlp.common.checks.ConfigurationError", "torch.matmul", "ValueError", "max", "first_tensor.squeeze.dim", "util._rindex", "first_tensor.squeeze.squeeze", "second_tensor.squeeze.dim", "util._rindex", "second_tensor.squeeze.squeeze", "second_tensor.squeeze.transpose", "result.squeeze.dim", "result.squeeze.squeeze", "torch.matmul", "first_tensor.squeeze.dim", "second_tensor.squeeze.dim", "first_tensor.squeeze.dim", "second_tensor.squeeze.dim", "first_tensor.squeeze.size", "second_tensor.squeeze.size", "ValueError", "max", "first_tensor.squeeze.dim", "util._rindex", "first_tensor.squeeze.squeeze", "second_tensor.squeeze.dim", "util._rindex", "second_tensor.squeeze.squeeze", "second_tensor.squeeze.pow().transpose", "result.squeeze.dim", "result.squeeze.squeeze", "first_tensor.squeeze.dim", "second_tensor.squeeze.dim", "first_tensor.squeeze.dim", "second_tensor.squeeze.dim", "first_tensor.squeeze.size", "second_tensor.squeeze.size", "torch.matmul", "torch.matmul", "allennlp.common.checks.ConfigurationError", "second_tensor.squeeze.pow", "torch.matmul", "torch.matmul"], "function", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.nn.util._get_combination", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.nn.util._get_combination", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.nn.util._rindex", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.nn.util._rindex", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.nn.util._rindex", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.nn.util._rindex"], []], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.nn.util.get_combined_dim": [[1155, 1176], ["combination.replace().replace.replace().replace", "sum", "len", "allennlp.common.checks.ConfigurationError", "combination.replace().replace.replace", "util._get_combination_dim", "combination.replace().replace.split"], "function", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.nn.util._get_combination_dim"], []], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.nn.util._get_combination_dim": [[1179, 1194], ["combination.isdigit", "util._get_combination_dim", "util._get_combination_dim", "int", "len", "allennlp.common.checks.ConfigurationError", "allennlp.common.checks.ConfigurationError"], "function", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.nn.util._get_combination_dim", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.nn.util._get_combination_dim"], []], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.nn.util.logsumexp": [[1196, 1219], ["tensor.max", "stable_vec.exp().sum().log", "max_score.unsqueeze", "stable_vec.exp().sum", "stable_vec.exp"], "function", ["None"], []], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.nn.util.get_device_of": [[1221, 1229], ["tensor.get_device"], "function", ["None"], []], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.nn.util.flatten_and_batch_shift_indices": [[1231, 1280], ["range", "offset_indices.view.view", "allennlp.common.checks.ConfigurationError", "util.get_range_vector", "offsets.unsqueeze.unsqueeze", "torch.max", "torch.min", "indices.size", "util.get_device_of", "len", "indices.size", "torch.min", "torch.max"], "function", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.nn.util.get_range_vector", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.nn.util.get_device_of"], []], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.nn.util.batched_index_select": [[1282, 1336], ["target.view", "target.view.index_select", "flattened_target.index_select.view", "util.flatten_and_batch_shift_indices", "target.size", "list", "target.size", "indices.size", "target.size"], "function", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.nn.util.flatten_and_batch_shift_indices"], []], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.nn.util.batched_span_select": [[1338, 1403], ["spans.split", "get_range_vector().view", "torch.nn.functional.relu().long", "util.batched_index_select", "span_widths.max().item", "util.get_range_vector", "torch.nn.functional.relu", "span_widths.max", "util.get_device_of", "raw_span_indices.float"], "function", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.nn.util.batched_index_select", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.nn.util.get_range_vector", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.nn.util.get_device_of"], []], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.nn.util.flattened_index_select": [[1405, 1440], ["target.index_select", "target.index_select.view", "indices.dim", "allennlp.common.checks.ConfigurationError", "indices.view", "target.size", "indices.size", "indices.size", "indices.size"], "function", ["None"], []], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.nn.util.get_range_vector": [[1442, 1451], ["torch.arange", "torch.cuda.LongTensor().fill_().cumsum", "torch.cuda.LongTensor().fill_", "torch.cuda.LongTensor"], "function", ["None"], []], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.nn.util.bucket_values": [[1453, 1493], ["combined_index.clamp", "distances.float().log", "math.log", "distances.float"], "function", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.metrics.f1_score.F1.clamp"], []], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.nn.util.add_sentence_boundary_token_ids": [[1495, 1552], ["mask.sum().detach().cpu().numpy", "list", "list", "tensor.new_zeros", "len", "enumerate", "mask.sum().detach().cpu", "len", "enumerate", "ValueError", "mask.sum().detach", "mask.sum"], "function", ["None"], []], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.nn.util.remove_sentence_boundaries": [[1554, 1596], ["mask.sum().detach().cpu().numpy", "list", "list", "tensor.new_zeros", "tensor.new_zeros", "enumerate", "mask.sum().detach().cpu", "mask.sum().detach", "mask.sum"], "function", ["None"], []], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.nn.util.add_positional_features": [[1598, 1656], ["tensor.size", "get_range_vector().data.float", "get_range_vector().data.float", "torch.cat", "math.log", "float", "torch.exp", "get_range_vector().data.float.unsqueeze", "inverse_timescales.unsqueeze", "torch.cat", "torch.cat.unsqueeze", "torch.sin", "torch.cos", "util.get_range_vector", "util.get_range_vector", "float", "float", "torch.cat.new_zeros", "util.get_device_of", "util.get_device_of"], "function", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.nn.util.get_range_vector", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.nn.util.get_range_vector", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.nn.util.get_device_of", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.nn.util.get_device_of"], []], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.nn.util.clone": [[1658, 1661], ["torch.nn.ModuleList", "copy.deepcopy", "range"], "function", ["None"], []], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.nn.util.combine_initial_dims": [[1663, 1674], ["tensor.dim", "tensor.view", "tensor.size"], "function", ["None"], []], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.nn.util.uncombine_initial_dims": [[1676, 1693], ["len", "tensor.view", "list", "tensor.size"], "function", ["None"], []], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.nn.util.inspect_parameters": [[1695, 1727], ["sorted", "module.named_parameters", "name.split", "print", "json.dumps"], "function", ["None"], []], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.nn.util.find_embedding_layer": [[1729, 1771], ["model.modules", "model.modules", "RuntimeError", "isinstance", "isinstance", "isinstance", "isinstance", "len", "isinstance", "list", "module._token_embedders.values"], "function", ["None"], []], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.nn.activations.Activation.__call__": [[46, 54], ["None"], "methods", ["None"], ["def", "__call__", "(", "self", ",", "tensor", ":", "torch", ".", "Tensor", ")", "->", "torch", ".", "Tensor", ":", "\n", "        ", "\"\"\"\n        This function is here just to make mypy happy.  We expect activation functions to follow\n        this API; the builtin pytorch activation functions follow this just fine, even though they\n        don't subclass `Activation`.  We're just making it explicit here, so mypy knows that\n        activations are callable like this.\n        \"\"\"", "\n", "raise", "NotImplementedError", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.nn.chu_liu_edmonds.decode_mst": [[7, 93], ["numpy.array", "numpy.zeros", "numpy.zeros", "range", "chu_liu_edmonds.chu_liu_edmonds", "numpy.zeros", "final_edges.items", "allennlp.common.checks.ConfigurationError", "energy.max.argmax", "energy.max.max", "representatives.append", "range", "numpy.ones", "allennlp.common.checks.ConfigurationError", "range"], "function", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.nn.chu_liu_edmonds.chu_liu_edmonds"], ["def", "decode_mst", "(", "\n", "energy", ":", "numpy", ".", "ndarray", ",", "length", ":", "int", ",", "has_labels", ":", "bool", "=", "True", "\n", ")", "->", "Tuple", "[", "numpy", ".", "ndarray", ",", "numpy", ".", "ndarray", "]", ":", "\n", "    ", "\"\"\"\n    Note: Counter to typical intuition, this function decodes the _maximum_\n    spanning tree.\n\n    Decode the optimal MST tree with the Chu-Liu-Edmonds algorithm for\n    maximum spanning arborescences on graphs.\n\n    # Parameters\n\n    energy : `numpy.ndarray`, required.\n        A tensor with shape (num_labels, timesteps, timesteps)\n        containing the energy of each edge. If has_labels is `False`,\n        the tensor should have shape (timesteps, timesteps) instead.\n    length : `int`, required.\n        The length of this sequence, as the energy may have come\n        from a padded batch.\n    has_labels : `bool`, optional, (default = True)\n        Whether the graph has labels or not.\n    \"\"\"", "\n", "if", "has_labels", "and", "energy", ".", "ndim", "!=", "3", ":", "\n", "        ", "raise", "ConfigurationError", "(", "\"The dimension of the energy array is not equal to 3.\"", ")", "\n", "", "elif", "not", "has_labels", "and", "energy", ".", "ndim", "!=", "2", ":", "\n", "        ", "raise", "ConfigurationError", "(", "\"The dimension of the energy array is not equal to 2.\"", ")", "\n", "", "input_shape", "=", "energy", ".", "shape", "\n", "max_length", "=", "input_shape", "[", "-", "1", "]", "\n", "\n", "# Our energy matrix might have been batched -", "\n", "# here we clip it to contain only non padded tokens.", "\n", "if", "has_labels", ":", "\n", "        ", "energy", "=", "energy", "[", ":", ",", ":", "length", ",", ":", "length", "]", "\n", "# get best label for each edge.", "\n", "label_id_matrix", "=", "energy", ".", "argmax", "(", "axis", "=", "0", ")", "\n", "energy", "=", "energy", ".", "max", "(", "axis", "=", "0", ")", "\n", "", "else", ":", "\n", "        ", "energy", "=", "energy", "[", ":", "length", ",", ":", "length", "]", "\n", "label_id_matrix", "=", "None", "\n", "# get original score matrix", "\n", "", "original_score_matrix", "=", "energy", "\n", "# initialize score matrix to original score matrix", "\n", "score_matrix", "=", "numpy", ".", "array", "(", "original_score_matrix", ",", "copy", "=", "True", ")", "\n", "\n", "old_input", "=", "numpy", ".", "zeros", "(", "[", "length", ",", "length", "]", ",", "dtype", "=", "numpy", ".", "int32", ")", "\n", "old_output", "=", "numpy", ".", "zeros", "(", "[", "length", ",", "length", "]", ",", "dtype", "=", "numpy", ".", "int32", ")", "\n", "current_nodes", "=", "[", "True", "for", "_", "in", "range", "(", "length", ")", "]", "\n", "representatives", ":", "List", "[", "Set", "[", "int", "]", "]", "=", "[", "]", "\n", "\n", "for", "node1", "in", "range", "(", "length", ")", ":", "\n", "        ", "original_score_matrix", "[", "node1", ",", "node1", "]", "=", "0.0", "\n", "score_matrix", "[", "node1", ",", "node1", "]", "=", "0.0", "\n", "representatives", ".", "append", "(", "{", "node1", "}", ")", "\n", "\n", "for", "node2", "in", "range", "(", "node1", "+", "1", ",", "length", ")", ":", "\n", "            ", "old_input", "[", "node1", ",", "node2", "]", "=", "node1", "\n", "old_output", "[", "node1", ",", "node2", "]", "=", "node2", "\n", "\n", "old_input", "[", "node2", ",", "node1", "]", "=", "node2", "\n", "old_output", "[", "node2", ",", "node1", "]", "=", "node1", "\n", "\n", "", "", "final_edges", ":", "Dict", "[", "int", ",", "int", "]", "=", "{", "}", "\n", "\n", "# The main algorithm operates inplace.", "\n", "chu_liu_edmonds", "(", "\n", "length", ",", "\n", "score_matrix", ",", "\n", "current_nodes", ",", "\n", "final_edges", ",", "\n", "old_input", ",", "\n", "old_output", ",", "\n", "representatives", ",", "\n", ")", "\n", "\n", "heads", "=", "numpy", ".", "zeros", "(", "[", "max_length", "]", ",", "numpy", ".", "int32", ")", "\n", "if", "has_labels", ":", "\n", "        ", "head_type", "=", "numpy", ".", "ones", "(", "[", "max_length", "]", ",", "numpy", ".", "int32", ")", "\n", "", "else", ":", "\n", "        ", "head_type", "=", "None", "\n", "\n", "", "for", "child", ",", "parent", "in", "final_edges", ".", "items", "(", ")", ":", "\n", "        ", "heads", "[", "child", "]", "=", "parent", "\n", "if", "has_labels", ":", "\n", "            ", "head_type", "[", "child", "]", "=", "label_id_matrix", "[", "parent", ",", "child", "]", "\n", "\n", "", "", "return", "heads", ",", "head_type", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.nn.chu_liu_edmonds.chu_liu_edmonds": [[95, 262], ["range", "chu_liu_edmonds._find_cycle", "range", "enumerate", "chu_liu_edmonds.chu_liu_edmonds", "enumerate", "parents.append", "range", "float", "float", "considered_representatives.append", "range", "set", "considered_representatives[].add", "representatives[].add"], "function", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.nn.chu_liu_edmonds._find_cycle", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.nn.chu_liu_edmonds.chu_liu_edmonds"], ["", "def", "chu_liu_edmonds", "(", "\n", "length", ":", "int", ",", "\n", "score_matrix", ":", "numpy", ".", "ndarray", ",", "\n", "current_nodes", ":", "List", "[", "bool", "]", ",", "\n", "final_edges", ":", "Dict", "[", "int", ",", "int", "]", ",", "\n", "old_input", ":", "numpy", ".", "ndarray", ",", "\n", "old_output", ":", "numpy", ".", "ndarray", ",", "\n", "representatives", ":", "List", "[", "Set", "[", "int", "]", "]", ",", "\n", ")", ":", "\n", "    ", "\"\"\"\n    Applies the chu-liu-edmonds algorithm recursively\n    to a graph with edge weights defined by score_matrix.\n\n    Note that this function operates in place, so variables\n    will be modified.\n\n    # Parameters\n\n    length : `int`, required.\n        The number of nodes.\n    score_matrix : `numpy.ndarray`, required.\n        The score matrix representing the scores for pairs\n        of nodes.\n    current_nodes : `List[bool]`, required.\n        The nodes which are representatives in the graph.\n        A representative at it's most basic represents a node,\n        but as the algorithm progresses, individual nodes will\n        represent collapsed cycles in the graph.\n    final_edges : `Dict[int, int]`, required.\n        An empty dictionary which will be populated with the\n        nodes which are connected in the maximum spanning tree.\n    old_input : `numpy.ndarray`, required.\n    old_output : `numpy.ndarray`, required.\n    representatives : `List[Set[int]]`, required.\n        A list containing the nodes that a particular node\n        is representing at this iteration in the graph.\n\n    # Returns\n\n    Nothing - all variables are modified in place.\n\n    \"\"\"", "\n", "# Set the initial graph to be the greedy best one.", "\n", "parents", "=", "[", "-", "1", "]", "\n", "for", "node1", "in", "range", "(", "1", ",", "length", ")", ":", "\n", "        ", "parents", ".", "append", "(", "0", ")", "\n", "if", "current_nodes", "[", "node1", "]", ":", "\n", "            ", "max_score", "=", "score_matrix", "[", "0", ",", "node1", "]", "\n", "for", "node2", "in", "range", "(", "1", ",", "length", ")", ":", "\n", "                ", "if", "node2", "==", "node1", "or", "not", "current_nodes", "[", "node2", "]", ":", "\n", "                    ", "continue", "\n", "\n", "", "new_score", "=", "score_matrix", "[", "node2", ",", "node1", "]", "\n", "if", "new_score", ">", "max_score", ":", "\n", "                    ", "max_score", "=", "new_score", "\n", "parents", "[", "node1", "]", "=", "node2", "\n", "\n", "# Check if this solution has a cycle.", "\n", "", "", "", "", "has_cycle", ",", "cycle", "=", "_find_cycle", "(", "parents", ",", "length", ",", "current_nodes", ")", "\n", "# If there are no cycles, find all edges and return.", "\n", "if", "not", "has_cycle", ":", "\n", "        ", "final_edges", "[", "0", "]", "=", "-", "1", "\n", "for", "node", "in", "range", "(", "1", ",", "length", ")", ":", "\n", "            ", "if", "not", "current_nodes", "[", "node", "]", ":", "\n", "                ", "continue", "\n", "\n", "", "parent", "=", "old_input", "[", "parents", "[", "node", "]", ",", "node", "]", "\n", "child", "=", "old_output", "[", "parents", "[", "node", "]", ",", "node", "]", "\n", "final_edges", "[", "child", "]", "=", "parent", "\n", "", "return", "\n", "\n", "# Otherwise, we have a cycle so we need to remove an edge.", "\n", "# From here until the recursive call is the contraction stage of the algorithm.", "\n", "", "cycle_weight", "=", "0.0", "\n", "# Find the weight of the cycle.", "\n", "index", "=", "0", "\n", "for", "node", "in", "cycle", ":", "\n", "        ", "index", "+=", "1", "\n", "cycle_weight", "+=", "score_matrix", "[", "parents", "[", "node", "]", ",", "node", "]", "\n", "\n", "# For each node in the graph, find the maximum weight incoming", "\n", "# and outgoing edge into the cycle.", "\n", "", "cycle_representative", "=", "cycle", "[", "0", "]", "\n", "for", "node", "in", "range", "(", "length", ")", ":", "\n", "        ", "if", "not", "current_nodes", "[", "node", "]", "or", "node", "in", "cycle", ":", "\n", "            ", "continue", "\n", "\n", "", "in_edge_weight", "=", "float", "(", "\"-inf\"", ")", "\n", "in_edge", "=", "-", "1", "\n", "out_edge_weight", "=", "float", "(", "\"-inf\"", ")", "\n", "out_edge", "=", "-", "1", "\n", "\n", "for", "node_in_cycle", "in", "cycle", ":", "\n", "            ", "if", "score_matrix", "[", "node_in_cycle", ",", "node", "]", ">", "in_edge_weight", ":", "\n", "                ", "in_edge_weight", "=", "score_matrix", "[", "node_in_cycle", ",", "node", "]", "\n", "in_edge", "=", "node_in_cycle", "\n", "\n", "# Add the new edge score to the cycle weight", "\n", "# and subtract the edge we're considering removing.", "\n", "", "score", "=", "(", "\n", "cycle_weight", "\n", "+", "score_matrix", "[", "node", ",", "node_in_cycle", "]", "\n", "-", "score_matrix", "[", "parents", "[", "node_in_cycle", "]", ",", "node_in_cycle", "]", "\n", ")", "\n", "\n", "if", "score", ">", "out_edge_weight", ":", "\n", "                ", "out_edge_weight", "=", "score", "\n", "out_edge", "=", "node_in_cycle", "\n", "\n", "", "", "score_matrix", "[", "cycle_representative", ",", "node", "]", "=", "in_edge_weight", "\n", "old_input", "[", "cycle_representative", ",", "node", "]", "=", "old_input", "[", "in_edge", ",", "node", "]", "\n", "old_output", "[", "cycle_representative", ",", "node", "]", "=", "old_output", "[", "in_edge", ",", "node", "]", "\n", "\n", "score_matrix", "[", "node", ",", "cycle_representative", "]", "=", "out_edge_weight", "\n", "old_output", "[", "node", ",", "cycle_representative", "]", "=", "old_output", "[", "node", ",", "out_edge", "]", "\n", "old_input", "[", "node", ",", "cycle_representative", "]", "=", "old_input", "[", "node", ",", "out_edge", "]", "\n", "\n", "# For the next recursive iteration, we want to consider the cycle as a", "\n", "# single node. Here we collapse the cycle into the first node in the", "\n", "# cycle (first node is arbitrary), set all the other nodes not be", "\n", "# considered in the next iteration. We also keep track of which", "\n", "# representatives we are considering this iteration because we need", "\n", "# them below to check if we're done.", "\n", "", "considered_representatives", ":", "List", "[", "Set", "[", "int", "]", "]", "=", "[", "]", "\n", "for", "i", ",", "node_in_cycle", "in", "enumerate", "(", "cycle", ")", ":", "\n", "        ", "considered_representatives", ".", "append", "(", "set", "(", ")", ")", "\n", "if", "i", ">", "0", ":", "\n", "# We need to consider at least one", "\n", "# node in the cycle, arbitrarily choose", "\n", "# the first.", "\n", "            ", "current_nodes", "[", "node_in_cycle", "]", "=", "False", "\n", "\n", "", "for", "node", "in", "representatives", "[", "node_in_cycle", "]", ":", "\n", "            ", "considered_representatives", "[", "i", "]", ".", "add", "(", "node", ")", "\n", "if", "i", ">", "0", ":", "\n", "                ", "representatives", "[", "cycle_representative", "]", ".", "add", "(", "node", ")", "\n", "\n", "", "", "", "chu_liu_edmonds", "(", "\n", "length", ",", "\n", "score_matrix", ",", "\n", "current_nodes", ",", "\n", "final_edges", ",", "\n", "old_input", ",", "\n", "old_output", ",", "\n", "representatives", ",", "\n", ")", "\n", "\n", "# Expansion stage.", "\n", "# check each node in cycle, if one of its representatives", "\n", "# is a key in the final_edges, it is the one we need.", "\n", "found", "=", "False", "\n", "key_node", "=", "-", "1", "\n", "for", "i", ",", "node", "in", "enumerate", "(", "cycle", ")", ":", "\n", "        ", "for", "cycle_rep", "in", "considered_representatives", "[", "i", "]", ":", "\n", "            ", "if", "cycle_rep", "in", "final_edges", ":", "\n", "                ", "key_node", "=", "node", "\n", "found", "=", "True", "\n", "break", "\n", "", "", "if", "found", ":", "\n", "            ", "break", "\n", "\n", "", "", "previous", "=", "parents", "[", "key_node", "]", "\n", "while", "previous", "!=", "key_node", ":", "\n", "        ", "child", "=", "old_output", "[", "parents", "[", "previous", "]", ",", "previous", "]", "\n", "parent", "=", "old_input", "[", "parents", "[", "previous", "]", ",", "previous", "]", "\n", "final_edges", "[", "child", "]", "=", "parent", "\n", "previous", "=", "parents", "[", "previous", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.nn.chu_liu_edmonds._find_cycle": [[264, 306], ["set", "range", "set", "set.add", "list", "range", "set.add", "set.add", "set.add"], "function", ["None"], ["", "", "def", "_find_cycle", "(", "\n", "parents", ":", "List", "[", "int", "]", ",", "length", ":", "int", ",", "current_nodes", ":", "List", "[", "bool", "]", "\n", ")", "->", "Tuple", "[", "bool", ",", "List", "[", "int", "]", "]", ":", "\n", "\n", "    ", "added", "=", "[", "False", "for", "_", "in", "range", "(", "length", ")", "]", "\n", "added", "[", "0", "]", "=", "True", "\n", "cycle", "=", "set", "(", ")", "\n", "has_cycle", "=", "False", "\n", "for", "i", "in", "range", "(", "1", ",", "length", ")", ":", "\n", "        ", "if", "has_cycle", ":", "\n", "            ", "break", "\n", "# don't redo nodes we've already", "\n", "# visited or aren't considering.", "\n", "", "if", "added", "[", "i", "]", "or", "not", "current_nodes", "[", "i", "]", ":", "\n", "            ", "continue", "\n", "# Initialize a new possible cycle.", "\n", "", "this_cycle", "=", "set", "(", ")", "\n", "this_cycle", ".", "add", "(", "i", ")", "\n", "added", "[", "i", "]", "=", "True", "\n", "has_cycle", "=", "True", "\n", "next_node", "=", "i", "\n", "while", "parents", "[", "next_node", "]", "not", "in", "this_cycle", ":", "\n", "            ", "next_node", "=", "parents", "[", "next_node", "]", "\n", "# If we see a node we've already processed,", "\n", "# we can stop, because the node we are", "\n", "# processing would have been in that cycle.", "\n", "if", "added", "[", "next_node", "]", ":", "\n", "                ", "has_cycle", "=", "False", "\n", "break", "\n", "", "added", "[", "next_node", "]", "=", "True", "\n", "this_cycle", ".", "add", "(", "next_node", ")", "\n", "\n", "", "if", "has_cycle", ":", "\n", "            ", "original", "=", "next_node", "\n", "cycle", ".", "add", "(", "original", ")", "\n", "next_node", "=", "parents", "[", "original", "]", "\n", "while", "next_node", "!=", "original", ":", "\n", "                ", "cycle", ".", "add", "(", "next_node", ")", "\n", "next_node", "=", "parents", "[", "next_node", "]", "\n", "", "break", "\n", "\n", "", "", "return", "has_cycle", ",", "list", "(", "cycle", ")", "\n", "", ""]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.regularizers.regularizer.Regularizer.__call__": [[14, 16], ["None"], "methods", ["None"], ["def", "__call__", "(", "self", ",", "parameter", ":", "torch", ".", "Tensor", ")", "->", "torch", ".", "Tensor", ":", "\n", "        ", "raise", "NotImplementedError", "\n", "", "", ""]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.regularizers.regularizers.L1Regularizer.__init__": [[10, 12], ["None"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "alpha", ":", "float", "=", "0.01", ")", "->", "None", ":", "\n", "        ", "self", ".", "alpha", "=", "alpha", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.regularizers.regularizers.L1Regularizer.__call__": [[13, 15], ["torch.sum", "torch.abs"], "methods", ["None"], ["", "def", "__call__", "(", "self", ",", "parameter", ":", "torch", ".", "Tensor", ")", "->", "torch", ".", "Tensor", ":", "\n", "        ", "return", "self", ".", "alpha", "*", "torch", ".", "sum", "(", "torch", ".", "abs", "(", "parameter", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.regularizers.regularizers.L2Regularizer.__init__": [[21, 23], ["None"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "alpha", ":", "float", "=", "0.01", ")", "->", "None", ":", "\n", "        ", "self", ".", "alpha", "=", "alpha", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.regularizers.regularizers.L2Regularizer.__call__": [[24, 26], ["torch.sum", "torch.pow"], "methods", ["None"], ["", "def", "__call__", "(", "self", ",", "parameter", ":", "torch", ".", "Tensor", ")", "->", "torch", ".", "Tensor", ":", "\n", "        ", "return", "self", ".", "alpha", "*", "torch", ".", "sum", "(", "torch", ".", "pow", "(", "parameter", ",", "2", ")", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.regularizers.regularizer_applicator.RegularizerApplicator.__init__": [[15, 25], ["None"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "regexes", ":", "List", "[", "Tuple", "[", "str", ",", "Regularizer", "]", "]", "=", "None", ")", "->", "None", ":", "\n", "        ", "\"\"\"\n        # Parameters\n\n        regexes : List[Tuple[str, Regularizer]], optional (default = None)\n            A sequence of pairs (regex, Regularizer), where each Regularizer\n            applies to the parameters its regex matches (and that haven't previously\n            been matched).\n        \"\"\"", "\n", "self", ".", "_regularizers", "=", "regexes", "or", "[", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.regularizers.regularizer_applicator.RegularizerApplicator.__call__": [[26, 44], ["module.named_parameters", "re.search", "regularizer"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.nn.beam_search.BeamSearch.search"], ["", "def", "__call__", "(", "self", ",", "module", ":", "torch", ".", "nn", ".", "Module", ")", "->", "torch", ".", "Tensor", ":", "\n", "        ", "\"\"\"\n        # Parameters\n\n        module : torch.nn.Module, required\n            The module to regularize.\n        \"\"\"", "\n", "accumulator", "=", "0.0", "\n", "for", "name", ",", "parameter", "in", "module", ".", "named_parameters", "(", ")", ":", "\n", "# We first check if the parameter needs gradient updates or not", "\n", "            ", "if", "parameter", ".", "requires_grad", ":", "\n", "# For each parameter find the first matching regex.", "\n", "                ", "for", "regex", ",", "regularizer", "in", "self", ".", "_regularizers", ":", "\n", "                    ", "if", "re", ".", "search", "(", "regex", ",", "name", ")", ":", "\n", "                        ", "penalty", "=", "regularizer", "(", "parameter", ")", "\n", "accumulator", "=", "accumulator", "+", "penalty", "\n", "break", "\n", "", "", "", "", "return", "accumulator", "\n", "", "", ""]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.predictors.decomposable_attention.DecomposableAttentionPredictor.predict": [[19, 37], ["decomposable_attention.DecomposableAttentionPredictor.predict_json"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.predictors.semantic_role_labeler.SemanticRoleLabelerPredictor.predict_json"], ["def", "predict", "(", "self", ",", "premise", ":", "str", ",", "hypothesis", ":", "str", ")", "->", "JsonDict", ":", "\n", "        ", "\"\"\"\n        Predicts whether the hypothesis is entailed by the premise text.\n\n        # Parameters\n\n        premise : `str`\n            A passage representing what is assumed to be true.\n\n        hypothesis : `str`\n            A sentence that may be entailed by the premise.\n\n        # Returns\n\n        A dictionary where the key \"label_probs\" determines the probabilities of each of\n        [entailment, contradiction, neutral].\n        \"\"\"", "\n", "return", "self", ".", "predict_json", "(", "{", "\"premise\"", ":", "premise", ",", "\"hypothesis\"", ":", "hypothesis", "}", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.predictors.decomposable_attention.DecomposableAttentionPredictor._json_to_instance": [[38, 46], ["decomposable_attention.DecomposableAttentionPredictor._dataset_reader.text_to_instance"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.coreference_resolution.conll.ConllCorefReader.text_to_instance"], ["", "@", "overrides", "\n", "def", "_json_to_instance", "(", "self", ",", "json_dict", ":", "JsonDict", ")", "->", "Instance", ":", "\n", "        ", "\"\"\"\n        Expects JSON that looks like `{\"premise\": \"...\", \"hypothesis\": \"...\"}`.\n        \"\"\"", "\n", "premise_text", "=", "json_dict", "[", "\"premise\"", "]", "\n", "hypothesis_text", "=", "json_dict", "[", "\"hypothesis\"", "]", "\n", "return", "self", ".", "_dataset_reader", ".", "text_to_instance", "(", "premise_text", ",", "hypothesis_text", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.predictors.decomposable_attention.DecomposableAttentionPredictor.predictions_to_labeled_instances": [[47, 56], ["copy.deepcopy", "numpy.argmax", "copy.deepcopy.add_field", "allennlp.data.fields.LabelField", "int"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.data.instance.Instance.add_field"], ["", "@", "overrides", "\n", "def", "predictions_to_labeled_instances", "(", "\n", "self", ",", "instance", ":", "Instance", ",", "outputs", ":", "Dict", "[", "str", ",", "numpy", ".", "ndarray", "]", "\n", ")", "->", "List", "[", "Instance", "]", ":", "\n", "        ", "new_instance", "=", "deepcopy", "(", "instance", ")", "\n", "label", "=", "numpy", ".", "argmax", "(", "outputs", "[", "\"label_logits\"", "]", ")", "\n", "# Skip indexing, we have integer representations of the strings \"entailment\", etc.", "\n", "new_instance", ".", "add_field", "(", "\"label\"", ",", "LabelField", "(", "int", "(", "label", ")", ",", "skip_indexing", "=", "True", ")", ")", "\n", "return", "[", "new_instance", "]", "\n", "", "", ""]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.predictors.open_information_extraction.OpenIePredictor.__init__": [[191, 194], ["allennlp.predictors.predictor.Predictor.__init__", "allennlp.data.tokenizers.SpacyTokenizer"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.graph_util.plot.GraphPlot.__init__"], ["def", "__init__", "(", "self", ",", "model", ":", "Model", ",", "dataset_reader", ":", "DatasetReader", ")", "->", "None", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "model", ",", "dataset_reader", ")", "\n", "self", ".", "_tokenizer", "=", "SpacyTokenizer", "(", "pos_tags", "=", "True", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.predictors.open_information_extraction.OpenIePredictor._json_to_instance": [[195, 206], ["int", "open_information_extraction.OpenIePredictor._dataset_reader.text_to_instance"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.coreference_resolution.conll.ConllCorefReader.text_to_instance"], ["", "def", "_json_to_instance", "(", "self", ",", "json_dict", ":", "JsonDict", ")", "->", "Instance", ":", "\n", "        ", "\"\"\"\n        Expects JSON that looks like `{\"sentence\": \"...\", \"predicate_index\": \"...\"}`.\n        Assumes sentence is tokenized, and that predicate_index points to a specific\n        predicate (word index) within the sentence, for which to produce Open IE extractions.\n        \"\"\"", "\n", "tokens", "=", "json_dict", "[", "\"sentence\"", "]", "\n", "predicate_index", "=", "int", "(", "json_dict", "[", "\"predicate_index\"", "]", ")", "\n", "verb_labels", "=", "[", "0", "for", "_", "in", "tokens", "]", "\n", "verb_labels", "[", "predicate_index", "]", "=", "1", "\n", "return", "self", ".", "_dataset_reader", ".", "text_to_instance", "(", "tokens", ",", "verb_labels", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.predictors.open_information_extraction.OpenIePredictor.predict_json": [[207, 270], ["open_information_extraction.OpenIePredictor._tokenizer.tokenize", "open_information_extraction.consolidate_predictions", "consolidate_predictions.values", "allennlp.common.util.sanitize", "open_information_extraction.OpenIePredictor._json_to_instance", "open_information_extraction.join_mwp", "open_information_extraction.make_oie_string", "results[].append", "enumerate", "open_information_extraction.sanitize_label", "open_information_extraction.get_predicate_text", "open_information_extraction.OpenIePredictor._model.forward_on_instance"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.tokenizers.whitespace_tokenizer.WhitespaceTokenizer.tokenize", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.predictors.open_information_extraction.consolidate_predictions", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.util.sanitize", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.predictors.constituency_parser.ConstituencyParserPredictor._json_to_instance", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.predictors.open_information_extraction.join_mwp", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.predictors.open_information_extraction.make_oie_string", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.predictors.open_information_extraction.sanitize_label", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.predictors.open_information_extraction.get_predicate_text", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.models.model.Model.forward_on_instance"], ["", "@", "overrides", "\n", "def", "predict_json", "(", "self", ",", "inputs", ":", "JsonDict", ")", "->", "JsonDict", ":", "\n", "        ", "\"\"\"\n        Create instance(s) after predicting the format. One sentence containing multiple verbs\n        will lead to multiple instances.\n\n        Expects JSON that looks like `{\"sentence\": \"...\"}`\n\n        Returns a JSON that looks like\n\n        .. code-block:: js\n\n            {\"tokens\": [...],\n             \"tag_spans\": [{\"ARG0\": \"...\",\n                            \"V\": \"...\",\n                            \"ARG1\": \"...\",\n                             ...}]}\n        \"\"\"", "\n", "sent_tokens", "=", "self", ".", "_tokenizer", ".", "tokenize", "(", "inputs", "[", "\"sentence\"", "]", ")", "\n", "\n", "# Find all verbs in the input sentence", "\n", "pred_ids", "=", "[", "i", "for", "(", "i", ",", "t", ")", "in", "enumerate", "(", "sent_tokens", ")", "if", "t", ".", "pos_", "==", "\"VERB\"", "]", "\n", "\n", "# Create instances", "\n", "instances", "=", "[", "\n", "self", ".", "_json_to_instance", "(", "\n", "{", "\"sentence\"", ":", "sent_tokens", ",", "\"predicate_index\"", ":", "pred_id", "}", "\n", ")", "\n", "for", "pred_id", "in", "pred_ids", "\n", "]", "\n", "\n", "# Run model", "\n", "outputs", "=", "[", "\n", "[", "\n", "sanitize_label", "(", "label", ")", "\n", "for", "label", "in", "self", ".", "_model", ".", "forward_on_instance", "(", "instance", ")", "[", "\"tags\"", "]", "\n", "]", "\n", "for", "instance", "in", "instances", "\n", "]", "\n", "\n", "# Consolidate predictions", "\n", "pred_dict", "=", "consolidate_predictions", "(", "outputs", ",", "sent_tokens", ")", "\n", "\n", "# Build and return output dictionary", "\n", "results", "=", "{", "\"verbs\"", ":", "[", "]", ",", "\"words\"", ":", "sent_tokens", "}", "\n", "\n", "for", "tags", "in", "pred_dict", ".", "values", "(", ")", ":", "\n", "# Join multi-word predicates", "\n", "            ", "tags", "=", "join_mwp", "(", "tags", ")", "\n", "\n", "# Create description text", "\n", "description", "=", "make_oie_string", "(", "sent_tokens", ",", "tags", ")", "\n", "\n", "# Add a predicate prediction to the return dictionary.", "\n", "results", "[", "\"verbs\"", "]", ".", "append", "(", "\n", "{", "\n", "\"verb\"", ":", "get_predicate_text", "(", "sent_tokens", ",", "tags", ")", ",", "\n", "\"description\"", ":", "description", ",", "\n", "\"tags\"", ":", "tags", ",", "\n", "}", "\n", ")", "\n", "\n", "", "return", "sanitize", "(", "results", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.predictors.open_information_extraction.join_mwp": [[13, 34], ["tag.split", "ret.append", "ret.append"], "function", ["None"], ["def", "join_mwp", "(", "tags", ":", "List", "[", "str", "]", ")", "->", "List", "[", "str", "]", ":", "\n", "    ", "\"\"\"\n    Join multi-word predicates to a single\n    predicate ('V') token.\n    \"\"\"", "\n", "ret", "=", "[", "]", "\n", "verb_flag", "=", "False", "\n", "for", "tag", "in", "tags", ":", "\n", "        ", "if", "\"V\"", "in", "tag", ":", "\n", "# Create a continuous 'V' BIO span", "\n", "            ", "prefix", ",", "_", "=", "tag", ".", "split", "(", "\"-\"", ",", "1", ")", "\n", "if", "verb_flag", ":", "\n", "# Continue a verb label across the different predicate parts", "\n", "                ", "prefix", "=", "\"I\"", "\n", "", "ret", ".", "append", "(", "f\"{prefix}-V\"", ")", "\n", "verb_flag", "=", "True", "\n", "", "else", ":", "\n", "            ", "ret", ".", "append", "(", "tag", ")", "\n", "verb_flag", "=", "False", "\n", "\n", "", "", "return", "ret", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.predictors.open_information_extraction.make_oie_string": [[36, 63], ["zip", "tag.startswith", "frame.append", "chunk.append", "tag.startswith", "frame.append", "chunk.append", "frame.append"], "function", ["None"], ["", "def", "make_oie_string", "(", "tokens", ":", "List", "[", "Token", "]", ",", "tags", ":", "List", "[", "str", "]", ")", "->", "str", ":", "\n", "    ", "\"\"\"\n    Converts a list of model outputs (i.e., a list of lists of bio tags, each\n    pertaining to a single word), returns an inline bracket representation of\n    the prediction.\n    \"\"\"", "\n", "frame", "=", "[", "]", "\n", "chunk", "=", "[", "]", "\n", "words", "=", "[", "token", ".", "text", "for", "token", "in", "tokens", "]", "\n", "\n", "for", "(", "token", ",", "tag", ")", "in", "zip", "(", "words", ",", "tags", ")", ":", "\n", "        ", "if", "tag", ".", "startswith", "(", "\"I-\"", ")", ":", "\n", "            ", "chunk", ".", "append", "(", "token", ")", "\n", "", "else", ":", "\n", "            ", "if", "chunk", ":", "\n", "                ", "frame", ".", "append", "(", "\"[\"", "+", "\" \"", ".", "join", "(", "chunk", ")", "+", "\"]\"", ")", "\n", "chunk", "=", "[", "]", "\n", "\n", "", "if", "tag", ".", "startswith", "(", "\"B-\"", ")", ":", "\n", "                ", "chunk", ".", "append", "(", "tag", "[", "2", ":", "]", "+", "\": \"", "+", "token", ")", "\n", "", "elif", "tag", "==", "\"O\"", ":", "\n", "                ", "frame", ".", "append", "(", "token", ")", "\n", "\n", "", "", "", "if", "chunk", ":", "\n", "        ", "frame", ".", "append", "(", "\"[\"", "+", "\" \"", ".", "join", "(", "chunk", ")", "+", "\"]\"", ")", "\n", "\n", "", "return", "\" \"", ".", "join", "(", "frame", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.predictors.open_information_extraction.get_predicate_indices": [[65, 70], ["enumerate"], "function", ["None"], ["", "def", "get_predicate_indices", "(", "tags", ":", "List", "[", "str", "]", ")", "->", "List", "[", "int", "]", ":", "\n", "    ", "\"\"\"\n    Return the word indices of a predicate in BIO tags.\n    \"\"\"", "\n", "return", "[", "ind", "for", "ind", ",", "tag", "in", "enumerate", "(", "tags", ")", "if", "\"V\"", "in", "tag", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.predictors.open_information_extraction.get_predicate_text": [[72, 78], ["open_information_extraction.get_predicate_indices"], "function", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.predictors.open_information_extraction.get_predicate_indices"], ["", "def", "get_predicate_text", "(", "sent_tokens", ":", "List", "[", "Token", "]", ",", "tags", ":", "List", "[", "str", "]", ")", "->", "str", ":", "\n", "    ", "\"\"\"\n    Get the predicate in this prediction.\n    \"\"\"", "\n", "return", "\" \"", ".", "join", "(", "\n", "[", "sent_tokens", "[", "pred_id", "]", ".", "text", "for", "pred_id", "in", "get_predicate_indices", "(", "tags", ")", "]", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.predictors.open_information_extraction.predicates_overlap": [[81, 92], ["open_information_extraction.get_predicate_indices", "open_information_extraction.get_predicate_indices", "any", "set.intersection", "set", "set"], "function", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.predictors.open_information_extraction.get_predicate_indices", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.predictors.open_information_extraction.get_predicate_indices"], ["", "def", "predicates_overlap", "(", "tags1", ":", "List", "[", "str", "]", ",", "tags2", ":", "List", "[", "str", "]", ")", "->", "bool", ":", "\n", "    ", "\"\"\"\n    Tests whether the predicate in BIO tags1 overlap\n    with those of tags2.\n    \"\"\"", "\n", "# Get predicate word indices from both predictions", "\n", "pred_ind1", "=", "get_predicate_indices", "(", "tags1", ")", "\n", "pred_ind2", "=", "get_predicate_indices", "(", "tags2", ")", "\n", "\n", "# Return if pred_ind1 pred_ind2 overlap", "\n", "return", "any", "(", "set", ".", "intersection", "(", "set", "(", "pred_ind1", ")", ",", "set", "(", "pred_ind2", ")", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.predictors.open_information_extraction.get_coherent_next_tag": [[94, 106], ["None"], "function", ["None"], ["", "def", "get_coherent_next_tag", "(", "prev_label", ":", "str", ",", "cur_label", ":", "str", ")", "->", "str", ":", "\n", "    ", "\"\"\"\n    Generate a coherent tag, given previous tag and current label.\n    \"\"\"", "\n", "if", "cur_label", "==", "\"O\"", ":", "\n", "# Don't need to add prefix to an \"O\" label", "\n", "        ", "return", "\"O\"", "\n", "\n", "", "if", "prev_label", "==", "cur_label", ":", "\n", "        ", "return", "f\"I-{cur_label}\"", "\n", "", "else", ":", "\n", "        ", "return", "f\"B-{cur_label}\"", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.predictors.open_information_extraction.merge_overlapping_predictions": [[108, 138], ["zip", "open_information_extraction.get_coherent_next_tag", "ret_sequence.append", "tag1.split", "tag2.split"], "function", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.predictors.open_information_extraction.get_coherent_next_tag"], ["", "", "def", "merge_overlapping_predictions", "(", "tags1", ":", "List", "[", "str", "]", ",", "tags2", ":", "List", "[", "str", "]", ")", "->", "List", "[", "str", "]", ":", "\n", "    ", "\"\"\"\n    Merge two predictions into one. Assumes the predicate in tags1 overlap with\n    the predicate of tags2.\n    \"\"\"", "\n", "ret_sequence", "=", "[", "]", "\n", "prev_label", "=", "\"O\"", "\n", "\n", "# Build a coherent sequence out of two", "\n", "# spans which predicates' overlap", "\n", "\n", "for", "tag1", ",", "tag2", "in", "zip", "(", "tags1", ",", "tags2", ")", ":", "\n", "        ", "label1", "=", "tag1", ".", "split", "(", "\"-\"", ",", "1", ")", "[", "-", "1", "]", "\n", "label2", "=", "tag2", ".", "split", "(", "\"-\"", ",", "1", ")", "[", "-", "1", "]", "\n", "if", "(", "label1", "==", "\"V\"", ")", "or", "(", "label2", "==", "\"V\"", ")", ":", "\n", "# Construct maximal predicate length -", "\n", "# add predicate tag if any of the sequence predict it", "\n", "            ", "cur_label", "=", "\"V\"", "\n", "\n", "# Else - prefer an argument over 'O' label", "\n", "", "elif", "label1", "!=", "\"O\"", ":", "\n", "            ", "cur_label", "=", "label1", "\n", "", "else", ":", "\n", "            ", "cur_label", "=", "label2", "\n", "\n", "# Append cur tag to the returned sequence", "\n", "", "cur_tag", "=", "get_coherent_next_tag", "(", "prev_label", ",", "cur_label", ")", "\n", "prev_label", "=", "cur_label", "\n", "ret_sequence", ".", "append", "(", "cur_tag", ")", "\n", "", "return", "ret_sequence", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.predictors.open_information_extraction.consolidate_predictions": [[140, 168], ["zip", "open_information_extraction.join_mwp", "open_information_extraction.get_predicate_text", "pred_dict.items", "open_information_extraction.predicates_overlap", "open_information_extraction.merge_overlapping_predictions"], "function", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.predictors.open_information_extraction.join_mwp", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.predictors.open_information_extraction.get_predicate_text", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.predictors.open_information_extraction.predicates_overlap", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.predictors.open_information_extraction.merge_overlapping_predictions"], ["", "def", "consolidate_predictions", "(", "\n", "outputs", ":", "List", "[", "List", "[", "str", "]", "]", ",", "sent_tokens", ":", "List", "[", "Token", "]", "\n", ")", "->", "Dict", "[", "str", ",", "List", "[", "str", "]", "]", ":", "\n", "    ", "\"\"\"\n    Identify that certain predicates are part of a multiword predicate\n    (e.g., \"decided to run\") in which case, we don't need to return\n    the embedded predicate (\"run\").\n    \"\"\"", "\n", "pred_dict", ":", "Dict", "[", "str", ",", "List", "[", "str", "]", "]", "=", "{", "}", "\n", "merged_outputs", "=", "[", "join_mwp", "(", "output", ")", "for", "output", "in", "outputs", "]", "\n", "predicate_texts", "=", "[", "get_predicate_text", "(", "sent_tokens", ",", "tags", ")", "for", "tags", "in", "merged_outputs", "]", "\n", "\n", "for", "pred1_text", ",", "tags1", "in", "zip", "(", "predicate_texts", ",", "merged_outputs", ")", ":", "\n", "# A flag indicating whether to add tags1 to predictions", "\n", "        ", "add_to_prediction", "=", "True", "\n", "\n", "#  Check if this predicate overlaps another predicate", "\n", "for", "pred2_text", ",", "tags2", "in", "pred_dict", ".", "items", "(", ")", ":", "\n", "            ", "if", "predicates_overlap", "(", "tags1", ",", "tags2", ")", ":", "\n", "# tags1 overlaps tags2", "\n", "                ", "pred_dict", "[", "pred2_text", "]", "=", "merge_overlapping_predictions", "(", "tags1", ",", "tags2", ")", "\n", "add_to_prediction", "=", "False", "\n", "\n", "# This predicate doesn't overlap - add as a new predicate", "\n", "", "", "if", "add_to_prediction", ":", "\n", "            ", "pred_dict", "[", "pred1_text", "]", "=", "tags1", "\n", "\n", "", "", "return", "pred_dict", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.predictors.open_information_extraction.sanitize_label": [[170, 181], ["label.split", "suffix.split"], "function", ["None"], ["", "def", "sanitize_label", "(", "label", ":", "str", ")", "->", "str", ":", "\n", "    ", "\"\"\"\n    Sanitize a BIO label - this deals with OIE\n    labels sometimes having some noise, as parentheses.\n    \"\"\"", "\n", "if", "\"-\"", "in", "label", ":", "\n", "        ", "prefix", ",", "suffix", "=", "label", ".", "split", "(", "\"-\"", ",", "1", ")", "\n", "suffix", "=", "suffix", ".", "split", "(", "\"(\"", ")", "[", "-", "1", "]", "\n", "return", "f\"{prefix}-{suffix}\"", "\n", "", "else", ":", "\n", "        ", "return", "label", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.predictors.sentence_tagger.SentenceTaggerPredictor.__init__": [[24, 32], ["allennlp.predictors.predictor.Predictor.__init__", "allennlp.data.tokenizers.spacy_tokenizer.SpacyTokenizer"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.graph_util.plot.GraphPlot.__init__"], ["def", "__init__", "(", "\n", "self", ",", "\n", "model", ":", "Model", ",", "\n", "dataset_reader", ":", "DatasetReader", ",", "\n", "language", ":", "str", "=", "\"en_core_web_sm\"", ",", "\n", ")", "->", "None", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "model", ",", "dataset_reader", ")", "\n", "self", ".", "_tokenizer", "=", "SpacyTokenizer", "(", "language", "=", "language", ",", "pos_tags", "=", "True", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.predictors.sentence_tagger.SentenceTaggerPredictor.predict": [[33, 35], ["sentence_tagger.SentenceTaggerPredictor.predict_json"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.predictors.semantic_role_labeler.SemanticRoleLabelerPredictor.predict_json"], ["", "def", "predict", "(", "self", ",", "sentence", ":", "str", ")", "->", "JsonDict", ":", "\n", "        ", "return", "self", ".", "predict_json", "(", "{", "\"sentence\"", ":", "sentence", "}", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.predictors.sentence_tagger.SentenceTaggerPredictor._json_to_instance": [[36, 45], ["sentence_tagger.SentenceTaggerPredictor._tokenizer.tokenize", "sentence_tagger.SentenceTaggerPredictor._dataset_reader.text_to_instance"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.tokenizers.whitespace_tokenizer.WhitespaceTokenizer.tokenize", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.coreference_resolution.conll.ConllCorefReader.text_to_instance"], ["", "@", "overrides", "\n", "def", "_json_to_instance", "(", "self", ",", "json_dict", ":", "JsonDict", ")", "->", "Instance", ":", "\n", "        ", "\"\"\"\n        Expects JSON that looks like `{\"sentence\": \"...\"}`.\n        Runs the underlying model, and adds the `\"words\"` to the output.\n        \"\"\"", "\n", "sentence", "=", "json_dict", "[", "\"sentence\"", "]", "\n", "tokens", "=", "self", ".", "_tokenizer", ".", "tokenize", "(", "sentence", ")", "\n", "return", "self", ".", "_dataset_reader", ".", "text_to_instance", "(", "tokens", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.predictors.sentence_tagger.SentenceTaggerPredictor.predictions_to_labeled_instances": [[46, 110], ["instances.reverse", "len", "copy.deepcopy", "copy.deepcopy.add_field", "instances.append", "predicted_spans.append", "allennlp.data.fields.SequenceLabelField", "predicted_spans.append", "enumerate", "enumerate"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.data.instance.Instance.add_field"], ["", "@", "overrides", "\n", "def", "predictions_to_labeled_instances", "(", "\n", "self", ",", "instance", ":", "Instance", ",", "outputs", ":", "Dict", "[", "str", ",", "numpy", ".", "ndarray", "]", "\n", ")", "->", "List", "[", "Instance", "]", ":", "\n", "        ", "\"\"\"\n        This function currently only handles BIOUL tags.\n\n        Imagine an NER model predicts three named entities (each one with potentially\n        multiple tokens). For each individual entity, we create a new Instance that has\n        the label set to only that entity and the rest of the tokens are labeled as outside.\n        We then return a list of those Instances.\n\n        For example:\n        Mary  went to Seattle to visit Microsoft Research\n        U-Per  O    O   U-Loc  O   O     B-Org     L-Org\n\n        We create three instances.\n        Mary  went to Seattle to visit Microsoft Research\n        U-Per  O    O    O     O   O       O         O\n\n        Mary  went to Seattle to visit Microsoft Research\n        O      O    O   U-LOC  O   O       O         O\n\n        Mary  went to Seattle to visit Microsoft Research\n        O      O    O    O     O   O     B-Org     L-Org\n        \"\"\"", "\n", "predicted_tags", "=", "outputs", "[", "\"tags\"", "]", "\n", "predicted_spans", "=", "[", "]", "\n", "\n", "i", "=", "0", "\n", "while", "i", "<", "len", "(", "predicted_tags", ")", ":", "\n", "            ", "tag", "=", "predicted_tags", "[", "i", "]", "\n", "# if its a U, add it to the list", "\n", "if", "tag", "[", "0", "]", "==", "\"U\"", ":", "\n", "                ", "current_tags", "=", "[", "\n", "t", "if", "idx", "==", "i", "else", "\"O\"", "for", "idx", ",", "t", "in", "enumerate", "(", "predicted_tags", ")", "\n", "]", "\n", "predicted_spans", ".", "append", "(", "current_tags", ")", "\n", "# if its a B, keep going until you hit an L.", "\n", "", "elif", "tag", "[", "0", "]", "==", "\"B\"", ":", "\n", "                ", "begin_idx", "=", "i", "\n", "while", "tag", "[", "0", "]", "!=", "\"L\"", ":", "\n", "                    ", "i", "+=", "1", "\n", "tag", "=", "predicted_tags", "[", "i", "]", "\n", "", "end_idx", "=", "i", "\n", "current_tags", "=", "[", "\n", "t", "if", "begin_idx", "<=", "idx", "<=", "end_idx", "else", "\"O\"", "\n", "for", "idx", ",", "t", "in", "enumerate", "(", "predicted_tags", ")", "\n", "]", "\n", "predicted_spans", ".", "append", "(", "current_tags", ")", "\n", "", "i", "+=", "1", "\n", "\n", "# Creates a new instance for each contiguous tag", "\n", "", "instances", "=", "[", "]", "\n", "for", "labels", "in", "predicted_spans", ":", "\n", "            ", "new_instance", "=", "deepcopy", "(", "instance", ")", "\n", "text_field", ":", "TextField", "=", "instance", "[", "\"tokens\"", "]", "# type: ignore", "\n", "new_instance", ".", "add_field", "(", "\n", "\"tags\"", ",", "SequenceLabelField", "(", "labels", ",", "text_field", ")", ",", "self", ".", "_model", ".", "vocab", "\n", ")", "\n", "instances", ".", "append", "(", "new_instance", ")", "\n", "", "instances", ".", "reverse", "(", ")", "# NER tags are in the opposite order as desired for the interpret UI", "\n", "\n", "return", "instances", "\n", "", "", ""]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.predictors.predictor.Predictor.__init__": [[43, 47], ["[].get_device", "next", "predictor.Predictor._model.named_parameters"], "methods", ["None"], ["state", "=", "torch", ".", "load", "(", "os", ".", "path", ".", "join", "(", "args", ".", "serialization_dir", ",", "best_model", ")", ",", "map_location", "=", "device", ")", "\n", "model", ".", "load_state_dict", "(", "state", ")", "\n", "model", "=", "model", ".", "to", "(", "device", ")", "\n", "return", "iterator", ",", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.predictors.predictor.Predictor.load_line": [[48, 54], ["json.loads"], "methods", ["None"], ["\n", "", "def", "get_instances", "(", "args", ",", "config", ",", "model", ")", ":", "\n", "    ", "reader_classes", "=", "{", "\n", "\"coref\"", ":", "{", "\n", "\"conll12\"", ":", "CorefConllReader", ",", "\n", "\"preco\"", ":", "CorefPrecoReader", ",", "\n", "\"wikicoref\"", ":", "CorefWikicorefReader", ",", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.predictors.predictor.Predictor.dump_line": [[55, 61], ["json.dumps"], "methods", ["None"], ["\"winobias\"", ":", "CorefWinobiasReader", ",", "\n", "\"phrase_detectives_g\"", ":", "CorefConllReader", ",", "\n", "\"phrase_detectives_w\"", ":", "CorefConllReader", ",", "\n", "}", ",", "\n", "\"srl\"", ":", "{", "\n", "\"conll12\"", ":", "SrlReader", ",", "\n", "\"conll05_w\"", ":", "SrlReader05", ",", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.predictors.predictor.Predictor.predict_json": [[62, 65], ["predictor.Predictor._json_to_instance", "predictor.Predictor.predict_instance"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.predictors.constituency_parser.ConstituencyParserPredictor._json_to_instance", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.predictors.constituency_parser.ConstituencyParserPredictor.predict_instance"], ["\"conll05_b\"", ":", "SrlReader05", ",", "\n", "\"wikibank\"", ":", "SrlWikibankReader", ",", "\n", "\"ewt\"", ":", "SrlWikibankReader", ",", "\n", "}", ",", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.predictors.predictor.Predictor.json_to_labeled_instances": [[66, 82], ["predictor.Predictor._json_to_instance", "predictor.Predictor._model.forward_on_instance", "predictor.Predictor.predictions_to_labeled_instances"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.predictors.constituency_parser.ConstituencyParserPredictor._json_to_instance", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.models.model.Model.forward_on_instance", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.predictors.masked_language_model.MaskedLanguageModelPredictor.predictions_to_labeled_instances"], ["}", "\n", "params", "=", "Params", ".", "from_file", "(", "os", ".", "path", ".", "join", "(", "args", ".", "serialization_dir", ",", "\"./config.json\"", ")", ")", "\n", "task_name", "=", "\"task_{}\"", ".", "format", "(", "args", ".", "task", ")", "\n", "reader_params", "=", "params", ".", "get", "(", "task_name", ")", "[", "\"data_params\"", "]", ".", "get", "(", "\n", "\"validation_dataset_reader\"", ",", "None", "\n", ")", "\n", "if", "reader_params", "is", "None", ":", "\n", "        ", "reader_params", "=", "params", ".", "pop", "(", "task_name", ")", "[", "\"data_params\"", "]", "[", "\"dataset_reader\"", "]", "\n", "", "reader_params", ".", "pop", "(", "\"type\"", ")", "\n", "reader", "=", "reader_classes", "[", "args", ".", "task", "]", "[", "args", ".", "data", "]", ".", "from_params", "(", "reader_params", ")", "\n", "instances", "=", "[", "]", "\n", "for", "instance", "in", "reader", ".", "_read", "(", "\n", "config", "[", "\"{}_{}_test_data_path\"", ".", "format", "(", "args", ".", "task", ",", "args", ".", "data", ")", "]", "\n", ")", ":", "\n", "        ", "instance", ".", "index_fields", "(", "model", ".", "vocab", ")", "\n", "instances", ".", "append", "(", "instance", ")", "\n", "", "return", "instances", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.predictors.predictor.Predictor.get_gradients": [[83, 137], ["predictor.Predictor._register_embedding_gradient_hooks", "allennlp.data.batch.Batch", "allennlp.data.batch.Batch.index_instances", "allennlp.nn.util.move_to_device", "dict", "enumerate", "allennlp.data.batch.Batch.as_tensor_dict", "torch.backends.cudnn.flags", "predictor.Predictor._model.decode", "predictor.Predictor._model.zero_grad", "loss.backward", "hook.remove", "grad.detach().cpu().numpy", "predictor.Predictor._model.forward", "str", "grad.detach().cpu", "grad.detach"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.predictors.predictor.Predictor._register_embedding_gradient_hooks", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.data.batch.Batch.index_instances", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.nn.util.move_to_device", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.data.instance.Instance.as_tensor_dict", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.coreference_resolution.coref.CoreferenceResolver.decode", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.graph_util.graph_encoder_gat.GAT.forward"], ["\n", "\n", "", "def", "get_outputs", "(", "args", ",", "instances", ",", "iterator", ",", "model", ")", ":", "\n", "    ", "generator", "=", "iterator", "(", "instances", "=", "instances", ",", "num_epochs", "=", "1", ")", "\n", "outputs", "=", "[", "]", "\n", "for", "batch", "in", "tqdm", "(", "generator", ")", ":", "\n", "        ", "if", "device", ".", "type", "!=", "\"cpu\"", ":", "\n", "            ", "batch", "=", "util", ".", "move_to_device", "(", "batch", ",", "device", ".", "index", ")", "\n", "", "output", "=", "model", ".", "forward", "(", "\n", "tensor_batch", "=", "batch", ",", "task_name", "=", "args", ".", "task", ",", "for_training", "=", "False", ",", "sample", "=", "False", "\n", ")", "\n", "output", "=", "model", ".", "decode", "(", "output", ",", "task_name", "=", "args", ".", "task", ")", "\n", "if", "args", ".", "task", "==", "\"coref\"", ":", "\n", "            ", "outputs", ".", "append", "(", "\n", "{", "\n", "\"document\"", ":", "output", "[", "\"document\"", "]", ",", "\n", "\"gold_clusters\"", ":", "output", "[", "\"gold_clusters\"", "]", ",", "\n", "\"predicted_clusters\"", ":", "output", "[", "\"clusters\"", "]", ",", "\n", "}", "\n", ")", "\n", "", "elif", "args", ".", "task", "==", "\"srl\"", ":", "\n", "            ", "words", ",", "gtags", ",", "ptags", "=", "output", "[", "\"words\"", "]", ",", "output", "[", "\"gold_tags\"", "]", ",", "output", "[", "\"tags\"", "]", "\n", "assert", "len", "(", "words", ")", "==", "len", "(", "gtags", ")", "==", "len", "(", "ptags", ")", "\n", "for", "w", ",", "gt", ",", "pt", "in", "zip", "(", "words", ",", "gtags", ",", "ptags", ")", ":", "\n", "                ", "outputs", ".", "append", "(", "\n", "{", "\"words\"", ":", "w", ",", "\"gold_tags\"", ":", "gt", ",", "\"predicted_tags\"", ":", "pt", "}", "\n", ")", "\n", "", "", "", "return", "outputs", "\n", "\n", "\n", "", "def", "process_coref", "(", "data", ")", ":", "\n", "    ", "text", "=", "[", "]", "\n", "for", "d", "in", "data", ":", "\n", "        ", "doc", "=", "[", "str", "(", "token", ")", "for", "token", "in", "d", "[", "\"document\"", "]", "[", "0", "]", "]", "# just to be safe", "\n", "gcs", "=", "d", "[", "\"gold_clusters\"", "]", "[", "0", "]", "\n", "pcs", "=", "d", "[", "\"predicted_clusters\"", "]", "[", "0", "]", "\n", "gcs", "=", "(", "\n", "[", "sorted", "(", "cluster", ",", "key", "=", "lambda", "x", ":", "(", "x", "[", "0", "]", ",", "x", "[", "1", "]", ")", ")", "for", "cluster", "in", "gcs", "]", "\n", "if", "len", "(", "gcs", ")", ">", "0", "\n", "else", "[", "]", "\n", ")", "\n", "pcs", "=", "(", "\n", "[", "sorted", "(", "cluster", ",", "key", "=", "lambda", "x", ":", "(", "x", "[", "0", "]", ",", "x", "[", "1", "]", ")", ")", "for", "cluster", "in", "pcs", "]", "\n", "if", "len", "(", "pcs", ")", ">", "0", "\n", "else", "[", "]", "\n", ")", "\n", "\n", "# assume the first mention in every gold cluster to be the antecedent", "\n", "# every other mention in that cluster will be the coreferent mention", "\n", "# search for every coreferent mention in the predicted clusters", "\n", "triples", "=", "[", "]", "\n", "not_found", "=", "[", "]", "\n", "for", "gc", "in", "gcs", ":", "\n", "            ", "gant", "=", "gc", "[", "0", "]", "\n", "for", "gmention", "in", "gc", "[", "1", ":", "]", ":", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.predictors.predictor.Predictor._register_embedding_gradient_hooks": [[138, 156], ["allennlp.nn.util.find_embedding_layer", "backward_hooks.append", "embedding_gradients.append", "allennlp.nn.util.find_embedding_layer.register_backward_hook"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.nn.util.find_embedding_layer"], ["                ", "found", "=", "True", "\n", "for", "pc", "in", "pcs", ":", "\n", "                    ", "pant", "=", "pc", "[", "0", "]", "\n", "if", "gmention", "in", "pc", "[", "1", ":", "]", ":", "\n", "                        ", "triples", ".", "append", "(", "\n", "\"\\t\"", ".", "join", "(", "\n", "[", "\n", "\" \"", ".", "join", "(", "doc", "[", "gmention", "[", "0", "]", ":", "gmention", "[", "1", "]", "+", "1", "]", ")", ",", "\n", "\" \"", ".", "join", "(", "doc", "[", "gant", "[", "0", "]", ":", "gant", "[", "1", "]", "+", "1", "]", ")", ",", "\n", "\" \"", ".", "join", "(", "doc", "[", "pant", "[", "0", "]", ":", "pant", "[", "1", "]", "+", "1", "]", ")", ",", "\n", "]", "\n", ")", "\n", ")", "\n", "found", "=", "True", "\n", "break", "\n", "", "", "if", "not", "found", ":", "\n", "                    ", "not_found", ".", "append", "(", "gmention", ")", "\n", "", "", "", "text", ".", "extend", "(", "\n", "[", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.predictors.predictor.Predictor.capture_model_internals": [[157, 191], ["enumerate", "predictor.Predictor._model.modules", "module.register_forward_hook.remove", "module.register_forward_hook", "hooks.append", "str", "allennlp.common.util.sanitize", "predictor.Predictor.capture_model_internals.add_output"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.util.sanitize"], ["\"'''\\n{}\\nMissing mentions: [{}]\\n'''\\n\"", ".", "format", "(", "\n", "doc", ",", "\", \"", ".", "join", "(", "not_found", ")", "\n", ")", ",", "\n", "\"\\n\"", ".", "join", "(", "triples", ")", ",", "\n", "\"\\n\\n\"", ",", "\n", "]", "\n", ")", "\n", "", "return", "text", "\n", "\n", "\n", "", "def", "process_srl", "(", "data", ")", ":", "\n", "    ", "text", "=", "[", "]", "\n", "for", "d", "in", "data", ":", "\n", "        ", "words", "=", "d", "[", "\"words\"", "]", "\n", "gtags", "=", "d", "[", "\"gold_tags\"", "]", "\n", "ptags", "=", "d", "[", "\"predicted_tags\"", "]", "\n", "doc", "=", "\" \"", ".", "join", "(", "words", ")", "\n", "text", ".", "append", "(", "\"'''\\n{}\\n'''\\n\"", ".", "format", "(", "doc", ")", ")", "\n", "for", "w", ",", "g", ",", "p", "in", "zip", "(", "words", ",", "gtags", ",", "ptags", ")", ":", "\n", "            ", "text", ".", "append", "(", "\"\\t\"", ".", "join", "(", "[", "w", ",", "g", ",", "p", "]", ")", "+", "\"\\n\"", ")", "\n", "", "text", ".", "append", "(", "\"\\n\"", ")", "\n", "", "return", "text", "\n", "\n", "\n", "", "def", "write", "(", "args", ",", "outputs", ")", ":", "\n", "    ", "file_name", "=", "\"./outputs/{}/{}_{}_{}.txt\"", ".", "format", "(", "\n", "args", ".", "model_type", ",", "args", ".", "task", ",", "args", ".", "serialization_dir", ".", "split", "(", "\"/\"", ")", "[", "-", "1", "]", ",", "args", ".", "data", "\n", ")", "\n", "with", "open", "(", "file_name", ",", "\"w\"", ",", "encoding", "=", "\"utf-8\"", ")", "as", "f", ":", "\n", "        ", "if", "args", ".", "task", "==", "\"coref\"", ":", "\n", "            ", "f", ".", "writelines", "(", "process_coref", "(", "outputs", ")", ")", "\n", "", "elif", "args", ".", "task", "==", "\"srl\"", ":", "\n", "            ", "f", ".", "writelines", "(", "process_srl", "(", "outputs", ")", ")", "\n", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.predictors.predictor.Predictor.predict_instance": [[192, 195], ["predictor.Predictor._model.forward_on_instance", "allennlp.common.util.sanitize"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.models.model.Model.forward_on_instance", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.util.sanitize"], ["", "", "", "def", "init_args", "(", ")", ":", "\n", "# Parse arguments", "\n", "    ", "parser", "=", "argparse", ".", "ArgumentParser", "(", ")", "\n", "parser", ".", "add_argument", "(", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.predictors.predictor.Predictor.predictions_to_labeled_instances": [[196, 210], ["RuntimeError"], "methods", ["None"], ["\"-c\"", ",", "\n", "\"--config_path\"", ",", "\n", "required", "=", "True", ",", "\n", "help", "=", "\"Fine-tuning configuration file.\"", ",", "\n", "type", "=", "str", ",", "\n", ")", "\n", "parser", ".", "add_argument", "(", "\n", "\"-s\"", ",", "\n", "\"--serialization_dir\"", ",", "\n", "required", "=", "True", ",", "\n", "help", "=", "\"Directory where the model is saved.\"", ",", "\n", "type", "=", "str", ",", "\n", ")", "\n", "parser", ".", "add_argument", "(", "\n", "\"-t\"", ",", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.predictors.predictor.Predictor._json_to_instance": [[211, 218], ["None"], "methods", ["None"], ["\"--task\"", ",", "\n", "required", "=", "True", ",", "\n", "help", "=", "\"Name of the task to predict.\"", ",", "\n", "choices", "=", "[", "\"coref\"", ",", "\"srl\"", "]", ",", "\n", "type", "=", "str", ",", "\n", ")", "\n", "parser", ".", "add_argument", "(", "\n", "\"-d\"", ",", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.predictors.predictor.Predictor.predict_batch_json": [[219, 222], ["predictor.Predictor._batch_json_to_instances", "predictor.Predictor.predict_batch_instance"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.predictors.predictor.Predictor._batch_json_to_instances", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.predictors.constituency_parser.ConstituencyParserPredictor.predict_batch_instance"], ["\"--data\"", ",", "\n", "required", "=", "True", ",", "\n", "help", "=", "\"Name of the dataset to load.\"", ",", "\n", "choices", "=", "[", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.predictors.predictor.Predictor.predict_batch_instance": [[223, 226], ["predictor.Predictor._model.forward_on_instances", "allennlp.common.util.sanitize"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.models.model.Model.forward_on_instances", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.util.sanitize"], ["\"conll12\"", ",", "\n", "\"preco\"", ",", "\n", "\"phrase_detectives_g\"", ",", "\n", "\"phrase_detectives_w\"", ",", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.predictors.predictor.Predictor._batch_json_to_instances": [[227, 240], ["instances.append", "predictor.Predictor._json_to_instance"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.predictors.constituency_parser.ConstituencyParserPredictor._json_to_instance"], ["\"wikicoref\"", ",", "\n", "\"winobias\"", ",", "\n", "\"conll05_w\"", ",", "\n", "\"conll05_b\"", ",", "\n", "\"ewt\"", ",", "\n", "]", ",", "\n", "type", "=", "str", ",", "\n", ")", "\n", "parser", ".", "add_argument", "(", "\n", "\"-m\"", ",", "\n", "\"--model_type\"", ",", "\n", "required", "=", "True", ",", "\n", "help", "=", "\"Model type to load (pre-trained or fine-tuned).\"", ",", "\n", "choices", "=", "[", "\"pt\"", ",", "\"ft\"", "]", ",", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.predictors.predictor.Predictor.from_path": [[241, 277], ["predictor.Predictor.from_archive", "allennlp.models.archival.load_archive"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.predictors.predictor.Predictor.from_archive", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.models.archival.load_archive"], ["type", "=", "str", ",", "\n", ")", "\n", "return", "parser", ".", "parse_args", "(", ")", "\n", "\n", "\n", "", "def", "main", "(", ")", ":", "\n", "    ", "args", "=", "init_args", "(", ")", "\n", "config", "=", "set_config", "(", "args", ".", "config_path", ")", "\n", "iterator", ",", "model", "=", "init_model", "(", "args", ")", "\n", "instances", "=", "get_instances", "(", "args", ",", "config", ",", "model", ")", "\n", "outputs", "=", "get_outputs", "(", "args", ",", "instances", ",", "iterator", ",", "model", ")", "\n", "write", "(", "args", ",", "outputs", ")", "\n", "\n", "\n", "", "if", "__name__", "==", "\"__main__\"", ":", "\n", "    ", "main", "(", ")", "\n", "", ""]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.predictors.predictor.Predictor.from_archive": [[279, 318], ["archive.config.duplicate", "allennlp.data.DatasetReader.from_params", "model.eval", "predictor_class", "archive.config.duplicate.get().get", "Predictor.by_name", "archive.config.duplicate.get"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.params.Params.duplicate", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.tokenizers.tokenizer.Tokenizer.from_params", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.params.Params.get", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.registrable.Registrable.by_name", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.params.Params.get"], []], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.predictors.text_classifier.TextClassifierPredictor.predict": [[22, 24], ["text_classifier.TextClassifierPredictor.predict_json"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.predictors.semantic_role_labeler.SemanticRoleLabelerPredictor.predict_json"], ["def", "predict", "(", "self", ",", "sentence", ":", "str", ")", "->", "JsonDict", ":", "\n", "        ", "return", "self", ".", "predict_json", "(", "{", "\"sentence\"", ":", "sentence", "}", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.predictors.text_classifier.TextClassifierPredictor._json_to_instance": [[25, 38], ["text_classifier.TextClassifierPredictor._dataset_reader.text_to_instance", "allennlp.data.tokenizers.spacy_tokenizer.SpacyTokenizer", "hasattr", "hasattr", "str", "allennlp.data.tokenizers.spacy_tokenizer.SpacyTokenizer.tokenize"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.coreference_resolution.conll.ConllCorefReader.text_to_instance", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.tokenizers.whitespace_tokenizer.WhitespaceTokenizer.tokenize"], ["", "@", "overrides", "\n", "def", "_json_to_instance", "(", "self", ",", "json_dict", ":", "JsonDict", ")", "->", "Instance", ":", "\n", "        ", "\"\"\"\n        Expects JSON that looks like `{\"sentence\": \"...\"}`.\n        Runs the underlying model, and adds the `\"label\"` to the output.\n        \"\"\"", "\n", "sentence", "=", "json_dict", "[", "\"sentence\"", "]", "\n", "if", "not", "hasattr", "(", "self", ".", "_dataset_reader", ",", "\"tokenizer\"", ")", "and", "not", "hasattr", "(", "\n", "self", ".", "_dataset_reader", ",", "\"_tokenizer\"", "\n", ")", ":", "\n", "            ", "tokenizer", "=", "SpacyTokenizer", "(", ")", "\n", "sentence", "=", "[", "str", "(", "t", ")", "for", "t", "in", "tokenizer", ".", "tokenize", "(", "sentence", ")", "]", "\n", "", "return", "self", ".", "_dataset_reader", ".", "text_to_instance", "(", "sentence", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.predictors.text_classifier.TextClassifierPredictor.predictions_to_labeled_instances": [[39, 47], ["copy.deepcopy", "numpy.argmax", "copy.deepcopy.add_field", "allennlp.data.fields.LabelField", "int"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.data.instance.Instance.add_field"], ["", "@", "overrides", "\n", "def", "predictions_to_labeled_instances", "(", "\n", "self", ",", "instance", ":", "Instance", ",", "outputs", ":", "Dict", "[", "str", ",", "numpy", ".", "ndarray", "]", "\n", ")", "->", "List", "[", "Instance", "]", ":", "\n", "        ", "new_instance", "=", "deepcopy", "(", "instance", ")", "\n", "label", "=", "numpy", ".", "argmax", "(", "outputs", "[", "\"probs\"", "]", ")", "\n", "new_instance", ".", "add_field", "(", "\"label\"", ",", "LabelField", "(", "int", "(", "label", ")", ",", "skip_indexing", "=", "True", ")", ")", "\n", "return", "[", "new_instance", "]", "\n", "", "", ""]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.predictors.semantic_role_labeler.SemanticRoleLabelerPredictor.__init__": [[19, 27], ["allennlp.predictors.predictor.Predictor.__init__", "allennlp.data.tokenizers.spacy_tokenizer.SpacyTokenizer"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.graph_util.plot.GraphPlot.__init__"], ["def", "__init__", "(", "\n", "self", ",", "\n", "model", ":", "Model", ",", "\n", "dataset_reader", ":", "DatasetReader", ",", "\n", "language", ":", "str", "=", "\"en_core_web_sm\"", ",", "\n", ")", "->", "None", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "model", ",", "dataset_reader", ")", "\n", "self", ".", "_tokenizer", "=", "SpacyTokenizer", "(", "language", "=", "language", ",", "pos_tags", "=", "True", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.predictors.semantic_role_labeler.SemanticRoleLabelerPredictor.predict": [[28, 52], ["semantic_role_labeler.SemanticRoleLabelerPredictor.predict_json"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.predictors.semantic_role_labeler.SemanticRoleLabelerPredictor.predict_json"], ["", "def", "predict", "(", "self", ",", "sentence", ":", "str", ")", "->", "JsonDict", ":", "\n", "        ", "\"\"\"\n        Predicts the semantic roles of the supplied sentence and returns a dictionary\n        with the results.\n\n        .. code-block:: js\n\n            {\"words\": [...],\n             \"verbs\": [\n                {\"verb\": \"...\", \"description\": \"...\", \"tags\": [...]},\n                ...\n                {\"verb\": \"...\", \"description\": \"...\", \"tags\": [...]},\n            ]}\n\n        # Parameters\n\n        sentence, `str`\n            The sentence to parse via semantic role labeling.\n\n        # Returns\n\n        A dictionary representation of the semantic roles in the sentence.\n        \"\"\"", "\n", "return", "self", ".", "predict_json", "(", "{", "\"sentence\"", ":", "sentence", "}", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.predictors.semantic_role_labeler.SemanticRoleLabelerPredictor.predict_tokenized": [[53, 78], ["spacy.tokens.Doc", "filter", "semantic_role_labeler.SemanticRoleLabelerPredictor.tokens_to_instances", "semantic_role_labeler.SemanticRoleLabelerPredictor.predict_instances", "allennlp.common.util.sanitize"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.util.WorkerLogFilter.filter", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.predictors.semantic_role_labeler.SemanticRoleLabelerPredictor.tokens_to_instances", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.predictors.semantic_role_labeler.SemanticRoleLabelerPredictor.predict_instances", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.util.sanitize"], ["", "def", "predict_tokenized", "(", "self", ",", "tokenized_sentence", ":", "List", "[", "str", "]", ")", "->", "JsonDict", ":", "\n", "        ", "\"\"\"\n        Predicts the semantic roles of the supplied sentence tokens and returns a dictionary\n        with the results.\n\n        # Parameters\n\n        tokenized_sentence, `List[str]`\n            The sentence tokens to parse via semantic role labeling.\n\n        # Returns\n\n        A dictionary representation of the semantic roles in the sentence.\n        \"\"\"", "\n", "spacy_doc", "=", "Doc", "(", "self", ".", "_tokenizer", ".", "spacy", ".", "vocab", ",", "words", "=", "tokenized_sentence", ")", "\n", "for", "pipe", "in", "filter", "(", "None", ",", "self", ".", "_tokenizer", ".", "spacy", ".", "pipeline", ")", ":", "\n", "            ", "pipe", "[", "1", "]", "(", "spacy_doc", ")", "\n", "\n", "", "tokens", "=", "[", "token", "for", "token", "in", "spacy_doc", "]", "\n", "instances", "=", "self", ".", "tokens_to_instances", "(", "tokens", ")", "\n", "\n", "if", "not", "instances", ":", "\n", "            ", "return", "sanitize", "(", "{", "\"verbs\"", ":", "[", "]", ",", "\"words\"", ":", "tokens", "}", ")", "\n", "\n", "", "return", "self", ".", "predict_instances", "(", "instances", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.predictors.semantic_role_labeler.SemanticRoleLabelerPredictor.make_srl_string": [[79, 101], ["zip", "tag.startswith", "frame.append", "chunk.append", "tag.startswith", "frame.append", "chunk.append", "frame.append"], "methods", ["None"], ["", "@", "staticmethod", "\n", "def", "make_srl_string", "(", "words", ":", "List", "[", "str", "]", ",", "tags", ":", "List", "[", "str", "]", ")", "->", "str", ":", "\n", "        ", "frame", "=", "[", "]", "\n", "chunk", "=", "[", "]", "\n", "\n", "for", "(", "token", ",", "tag", ")", "in", "zip", "(", "words", ",", "tags", ")", ":", "\n", "            ", "if", "tag", ".", "startswith", "(", "\"I-\"", ")", ":", "\n", "                ", "chunk", ".", "append", "(", "token", ")", "\n", "", "else", ":", "\n", "                ", "if", "chunk", ":", "\n", "                    ", "frame", ".", "append", "(", "\"[\"", "+", "\" \"", ".", "join", "(", "chunk", ")", "+", "\"]\"", ")", "\n", "chunk", "=", "[", "]", "\n", "\n", "", "if", "tag", ".", "startswith", "(", "\"B-\"", ")", ":", "\n", "                    ", "chunk", ".", "append", "(", "tag", "[", "2", ":", "]", "+", "\": \"", "+", "token", ")", "\n", "", "elif", "tag", "==", "\"O\"", ":", "\n", "                    ", "frame", ".", "append", "(", "token", ")", "\n", "\n", "", "", "", "if", "chunk", ":", "\n", "            ", "frame", ".", "append", "(", "\"[\"", "+", "\" \"", ".", "join", "(", "chunk", ")", "+", "\"]\"", ")", "\n", "\n", "", "return", "\" \"", ".", "join", "(", "frame", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.predictors.semantic_role_labeler.SemanticRoleLabelerPredictor._json_to_instance": [[102, 106], ["NotImplementedError"], "methods", ["None"], ["", "@", "overrides", "\n", "def", "_json_to_instance", "(", "self", ",", "json_dict", ":", "JsonDict", ")", ":", "\n", "        ", "raise", "NotImplementedError", "(", "\n", "\"The SRL model uses a different API for creating instances.\"", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.predictors.semantic_role_labeler.SemanticRoleLabelerPredictor.tokens_to_instances": [[108, 118], ["enumerate", "semantic_role_labeler.SemanticRoleLabelerPredictor._dataset_reader.text_to_instance", "instances.append"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.coreference_resolution.conll.ConllCorefReader.text_to_instance"], ["", "def", "tokens_to_instances", "(", "self", ",", "tokens", ")", ":", "\n", "        ", "words", "=", "[", "token", ".", "text", "for", "token", "in", "tokens", "]", "\n", "instances", ":", "List", "[", "Instance", "]", "=", "[", "]", "\n", "for", "i", ",", "word", "in", "enumerate", "(", "tokens", ")", ":", "\n", "            ", "if", "word", ".", "pos_", "==", "\"VERB\"", ":", "\n", "                ", "verb_labels", "=", "[", "0", "for", "_", "in", "words", "]", "\n", "verb_labels", "[", "i", "]", "=", "1", "\n", "instance", "=", "self", ".", "_dataset_reader", ".", "text_to_instance", "(", "tokens", ",", "verb_labels", ")", "\n", "instances", ".", "append", "(", "instance", ")", "\n", "", "", "return", "instances", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.predictors.semantic_role_labeler.SemanticRoleLabelerPredictor._sentence_to_srl_instances": [[119, 141], ["semantic_role_labeler.SemanticRoleLabelerPredictor._tokenizer.tokenize", "semantic_role_labeler.SemanticRoleLabelerPredictor.tokens_to_instances"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.tokenizers.whitespace_tokenizer.WhitespaceTokenizer.tokenize", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.predictors.semantic_role_labeler.SemanticRoleLabelerPredictor.tokens_to_instances"], ["", "def", "_sentence_to_srl_instances", "(", "self", ",", "json_dict", ":", "JsonDict", ")", "->", "List", "[", "Instance", "]", ":", "\n", "        ", "\"\"\"\n        The SRL model has a slightly different API from other models, as the model is run\n        forward for every verb in the sentence. This means that for a single sentence, we need\n        to generate a `List[Instance]`, where the length of this list corresponds to the number\n        of verbs in the sentence. Additionally, all of these verbs share the same return dictionary\n        after being passed through the model (as really we care about all the frames of the sentence\n        together, rather than separately).\n\n        # Parameters\n\n        json_dict : `JsonDict`, required.\n            JSON that looks like `{\"sentence\": \"...\"}`.\n\n        # Returns\n\n        instances : `List[Instance]`\n            One instance per verb.\n        \"\"\"", "\n", "sentence", "=", "json_dict", "[", "\"sentence\"", "]", "\n", "tokens", "=", "self", ".", "_tokenizer", ".", "tokenize", "(", "sentence", ")", "\n", "return", "self", ".", "tokens_to_instances", "(", "tokens", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.predictors.semantic_role_labeler.SemanticRoleLabelerPredictor.predict_batch_json": [[142, 227], ["len", "allennlp.common.util.group_by_count", "enumerate", "allennlp.common.util.sanitize", "semantic_role_labeler.SemanticRoleLabelerPredictor._sentence_to_srl_instances", "allennlp.common.util.sanitize", "outputs.extend", "len", "range", "semantic_role_labeler.SemanticRoleLabelerPredictor._model.forward_on_instances", "semantic_role_labeler.SemanticRoleLabelerPredictor._tokenizer.tokenize", "semantic_role_labeler.SemanticRoleLabelerPredictor.make_srl_string", "[].append", "semantic_role_labeler.SemanticRoleLabelerPredictor._tokenizer.tokenize"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.util.group_by_count", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.util.sanitize", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.predictors.semantic_role_labeler.SemanticRoleLabelerPredictor._sentence_to_srl_instances", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.util.sanitize", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.models.model.Model.forward_on_instances", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.tokenizers.whitespace_tokenizer.WhitespaceTokenizer.tokenize", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.predictors.semantic_role_labeler.SemanticRoleLabelerPredictor.make_srl_string", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.tokenizers.whitespace_tokenizer.WhitespaceTokenizer.tokenize"], ["", "@", "overrides", "\n", "def", "predict_batch_json", "(", "self", ",", "inputs", ":", "List", "[", "JsonDict", "]", ")", "->", "List", "[", "JsonDict", "]", ":", "\n", "        ", "\"\"\"\n        Expects JSON that looks like `[{\"sentence\": \"...\"}, {\"sentence\": \"...\"}, ...]`\n        and returns JSON that looks like\n\n        .. code-block:: js\n\n            [\n                {\"words\": [...],\n                 \"verbs\": [\n                    {\"verb\": \"...\", \"description\": \"...\", \"tags\": [...]},\n                    ...\n                    {\"verb\": \"...\", \"description\": \"...\", \"tags\": [...]},\n                ]},\n                {\"words\": [...],\n                 \"verbs\": [\n                    {\"verb\": \"...\", \"description\": \"...\", \"tags\": [...]},\n                    ...\n                    {\"verb\": \"...\", \"description\": \"...\", \"tags\": [...]},\n                ]}\n            ]\n        \"\"\"", "\n", "# For SRL, we have more instances than sentences, but the user specified", "\n", "# a batch size with respect to the number of sentences passed, so we respect", "\n", "# that here by taking the batch size which we use to be the number of sentences", "\n", "# we are given.", "\n", "batch_size", "=", "len", "(", "inputs", ")", "\n", "instances_per_sentence", "=", "[", "\n", "self", ".", "_sentence_to_srl_instances", "(", "json", ")", "for", "json", "in", "inputs", "\n", "]", "\n", "\n", "flattened_instances", "=", "[", "\n", "instance", "\n", "for", "sentence_instances", "in", "instances_per_sentence", "\n", "for", "instance", "in", "sentence_instances", "\n", "]", "\n", "\n", "if", "not", "flattened_instances", ":", "\n", "            ", "return", "sanitize", "(", "\n", "[", "\n", "{", "\"verbs\"", ":", "[", "]", ",", "\"words\"", ":", "self", ".", "_tokenizer", ".", "tokenize", "(", "x", "[", "\"sentence\"", "]", ")", "}", "\n", "for", "x", "in", "inputs", "\n", "]", "\n", ")", "\n", "\n", "# Make the instances into batches and check the last batch for", "\n", "# padded elements as the number of instances might not be perfectly", "\n", "# divisible by the batch size.", "\n", "", "batched_instances", "=", "group_by_count", "(", "flattened_instances", ",", "batch_size", ",", "None", ")", "\n", "batched_instances", "[", "-", "1", "]", "=", "[", "\n", "instance", "for", "instance", "in", "batched_instances", "[", "-", "1", "]", "if", "instance", "is", "not", "None", "\n", "]", "\n", "# Run the model on the batches.", "\n", "outputs", "=", "[", "]", "\n", "for", "batch", "in", "batched_instances", ":", "\n", "            ", "outputs", ".", "extend", "(", "self", ".", "_model", ".", "forward_on_instances", "(", "batch", ")", ")", "\n", "\n", "", "verbs_per_sentence", "=", "[", "len", "(", "sent", ")", "for", "sent", "in", "instances_per_sentence", "]", "\n", "return_dicts", ":", "List", "[", "JsonDict", "]", "=", "[", "{", "\"verbs\"", ":", "[", "]", "}", "for", "x", "in", "inputs", "]", "\n", "\n", "output_index", "=", "0", "\n", "for", "sentence_index", ",", "verb_count", "in", "enumerate", "(", "verbs_per_sentence", ")", ":", "\n", "            ", "if", "verb_count", "==", "0", ":", "\n", "# We didn't run any predictions for sentences with no verbs,", "\n", "# so we don't have a way to extract the original sentence.", "\n", "# Here we just tokenize the input again.", "\n", "                ", "original_text", "=", "self", ".", "_tokenizer", ".", "tokenize", "(", "\n", "inputs", "[", "sentence_index", "]", "[", "\"sentence\"", "]", "\n", ")", "\n", "return_dicts", "[", "sentence_index", "]", "[", "\"words\"", "]", "=", "original_text", "\n", "continue", "\n", "\n", "", "for", "_", "in", "range", "(", "verb_count", ")", ":", "\n", "                ", "output", "=", "outputs", "[", "output_index", "]", "\n", "words", "=", "output", "[", "\"words\"", "]", "\n", "tags", "=", "output", "[", "\"tags\"", "]", "\n", "description", "=", "self", ".", "make_srl_string", "(", "words", ",", "tags", ")", "\n", "return_dicts", "[", "sentence_index", "]", "[", "\"words\"", "]", "=", "words", "\n", "return_dicts", "[", "sentence_index", "]", "[", "\"verbs\"", "]", ".", "append", "(", "\n", "{", "\"verb\"", ":", "output", "[", "\"verb\"", "]", ",", "\"description\"", ":", "description", ",", "\"tags\"", ":", "tags", "}", "\n", ")", "\n", "output_index", "+=", "1", "\n", "\n", "", "", "return", "sanitize", "(", "return_dicts", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.predictors.semantic_role_labeler.SemanticRoleLabelerPredictor.predict_instances": [[228, 240], ["semantic_role_labeler.SemanticRoleLabelerPredictor._model.forward_on_instances", "allennlp.common.util.sanitize", "semantic_role_labeler.SemanticRoleLabelerPredictor.make_srl_string", "results[].append"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.models.model.Model.forward_on_instances", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.util.sanitize", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.predictors.semantic_role_labeler.SemanticRoleLabelerPredictor.make_srl_string"], ["", "def", "predict_instances", "(", "self", ",", "instances", ":", "List", "[", "Instance", "]", ")", "->", "JsonDict", ":", "\n", "        ", "outputs", "=", "self", ".", "_model", ".", "forward_on_instances", "(", "instances", ")", "\n", "\n", "results", "=", "{", "\"verbs\"", ":", "[", "]", ",", "\"words\"", ":", "outputs", "[", "0", "]", "[", "\"words\"", "]", "}", "\n", "for", "output", "in", "outputs", ":", "\n", "            ", "tags", "=", "output", "[", "\"tags\"", "]", "\n", "description", "=", "self", ".", "make_srl_string", "(", "output", "[", "\"words\"", "]", ",", "tags", ")", "\n", "results", "[", "\"verbs\"", "]", ".", "append", "(", "\n", "{", "\"verb\"", ":", "output", "[", "\"verb\"", "]", ",", "\"description\"", ":", "description", ",", "\"tags\"", ":", "tags", "}", "\n", ")", "\n", "\n", "", "return", "sanitize", "(", "results", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.predictors.semantic_role_labeler.SemanticRoleLabelerPredictor.predict_json": [[241, 264], ["semantic_role_labeler.SemanticRoleLabelerPredictor._sentence_to_srl_instances", "semantic_role_labeler.SemanticRoleLabelerPredictor.predict_instances", "allennlp.common.util.sanitize", "semantic_role_labeler.SemanticRoleLabelerPredictor._tokenizer.tokenize"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.predictors.semantic_role_labeler.SemanticRoleLabelerPredictor._sentence_to_srl_instances", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.predictors.semantic_role_labeler.SemanticRoleLabelerPredictor.predict_instances", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.util.sanitize", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.tokenizers.whitespace_tokenizer.WhitespaceTokenizer.tokenize"], ["", "@", "overrides", "\n", "def", "predict_json", "(", "self", ",", "inputs", ":", "JsonDict", ")", "->", "JsonDict", ":", "\n", "        ", "\"\"\"\n        Expects JSON that looks like `{\"sentence\": \"...\"}`\n        and returns JSON that looks like\n\n        .. code-block:: js\n\n            {\"words\": [...],\n             \"verbs\": [\n                {\"verb\": \"...\", \"description\": \"...\", \"tags\": [...]},\n                ...\n                {\"verb\": \"...\", \"description\": \"...\", \"tags\": [...]},\n            ]}\n        \"\"\"", "\n", "instances", "=", "self", ".", "_sentence_to_srl_instances", "(", "inputs", ")", "\n", "\n", "if", "not", "instances", ":", "\n", "            ", "return", "sanitize", "(", "\n", "{", "\"verbs\"", ":", "[", "]", ",", "\"words\"", ":", "self", ".", "_tokenizer", ".", "tokenize", "(", "inputs", "[", "\"sentence\"", "]", ")", "}", "\n", ")", "\n", "\n", "", "return", "self", ".", "predict_instances", "(", "instances", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.predictors.coref.CorefPredictor.__init__": [[22, 33], ["allennlp.predictors.predictor.Predictor.__init__", "allennlp.common.util.get_spacy_model"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.graph_util.plot.GraphPlot.__init__", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.util.get_spacy_model"], ["def", "__init__", "(", "\n", "self", ",", "\n", "model", ":", "Model", ",", "\n", "dataset_reader", ":", "DatasetReader", ",", "\n", "language", ":", "str", "=", "\"en_core_web_sm\"", ",", "\n", ")", "->", "None", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "model", ",", "dataset_reader", ")", "\n", "\n", "# We have to use spacy to tokenise our document here, because we need", "\n", "# to also know sentence boundaries to propose valid mentions.", "\n", "self", ".", "_spacy", "=", "get_spacy_model", "(", "language", ",", "pos_tags", "=", "True", ",", "parse", "=", "True", ",", "ner", "=", "False", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.predictors.coref.CorefPredictor.predict": [[34, 67], ["coref.CorefPredictor.predict_json"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.predictors.semantic_role_labeler.SemanticRoleLabelerPredictor.predict_json"], ["", "def", "predict", "(", "self", ",", "document", ":", "str", ")", "->", "JsonDict", ":", "\n", "        ", "\"\"\"\n        Predict the coreference clusters in the given document.\n\n        .. code-block:: js\n\n            {\n            \"document\": [tokenised document text]\n            \"clusters\":\n              [\n                [\n                  [start_index, end_index],\n                  [start_index, end_index]\n                ],\n                [\n                  [start_index, end_index],\n                  [start_index, end_index],\n                  [start_index, end_index],\n                ],\n                ....\n              ]\n            }\n\n        # Parameters\n\n        document : `str`\n            A string representation of a document.\n\n        # Returns\n\n        A dictionary representation of the predicted coreference clusters.\n        \"\"\"", "\n", "return", "self", ".", "predict_json", "(", "{", "\"document\"", ":", "document", "}", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.predictors.coref.CorefPredictor.predict_tokenized": [[68, 83], ["coref.CorefPredictor._words_list_to_instance", "coref.CorefPredictor.predict_instance"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.predictors.coref.CorefPredictor._words_list_to_instance", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.predictors.constituency_parser.ConstituencyParserPredictor.predict_instance"], ["", "def", "predict_tokenized", "(", "self", ",", "tokenized_document", ":", "List", "[", "str", "]", ")", "->", "JsonDict", ":", "\n", "        ", "\"\"\"\n        Predict the coreference clusters in the given document.\n\n        # Parameters\n\n        tokenized_document : `List[str]`\n            A list of words representation of a tokenized document.\n\n        # Returns\n\n        A dictionary representation of the predicted coreference clusters.\n        \"\"\"", "\n", "instance", "=", "self", ".", "_words_list_to_instance", "(", "tokenized_document", ")", "\n", "return", "self", ".", "predict_instance", "(", "instance", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.predictors.coref.CorefPredictor.predictions_to_labeled_instances": [[84, 124], ["copy.deepcopy", "copy.deepcopy.add_field", "instances.append", "copy.deepcopy", "copy.deepcopy.add_field", "instances.append", "allennlp.data.fields.SequenceLabelField", "len", "allennlp.data.fields.SequenceLabelField"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.data.instance.Instance.add_field", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.data.instance.Instance.add_field"], ["", "@", "overrides", "\n", "def", "predictions_to_labeled_instances", "(", "\n", "self", ",", "instance", ":", "Instance", ",", "outputs", ":", "Dict", "[", "str", ",", "numpy", ".", "ndarray", "]", "\n", ")", "->", "List", "[", "Instance", "]", ":", "\n", "        ", "\"\"\"\n        Takes each predicted cluster and makes it into a labeled `Instance` with only that\n        cluster labeled, so we can compute gradients of the loss `on the model's prediction of that\n        cluster`.  This lets us run interpretation methods using those gradients.  See superclass\n        docstring for more info.\n        \"\"\"", "\n", "# Digging into an Instance makes mypy go crazy, because we have all kinds of things where", "\n", "# the type has been lost.  So there are lots of `type: ignore`s here...", "\n", "predicted_clusters", "=", "outputs", "[", "\"clusters\"", "]", "\n", "span_field", ":", "ListField", "=", "instance", "[", "\"spans\"", "]", "# type: ignore", "\n", "instances", "=", "[", "]", "\n", "for", "cluster", "in", "predicted_clusters", ":", "\n", "            ", "new_instance", "=", "deepcopy", "(", "instance", ")", "\n", "span_labels", "=", "[", "\n", "0", "if", "(", "span", ".", "span_start", ",", "span", ".", "span_end", ")", "in", "cluster", "else", "-", "1", "# type: ignore", "\n", "for", "span", "in", "span_field", "\n", "]", "# type: ignore", "\n", "new_instance", ".", "add_field", "(", "\n", "\"span_labels\"", ",", "\n", "SequenceLabelField", "(", "span_labels", ",", "span_field", ")", ",", "\n", "self", ".", "_model", ".", "vocab", ",", "\n", ")", "\n", "new_instance", "[", "\"metadata\"", "]", ".", "metadata", "[", "\"clusters\"", "]", "=", "[", "cluster", "]", "# type: ignore", "\n", "instances", ".", "append", "(", "new_instance", ")", "\n", "", "if", "not", "instances", ":", "\n", "# No predicted clusters; we just give an empty coref prediction.", "\n", "            ", "new_instance", "=", "deepcopy", "(", "instance", ")", "\n", "span_labels", "=", "[", "-", "1", "]", "*", "len", "(", "span_field", ")", "# type: ignore", "\n", "new_instance", ".", "add_field", "(", "\n", "\"span_labels\"", ",", "\n", "SequenceLabelField", "(", "span_labels", ",", "span_field", ")", ",", "\n", "self", ".", "_model", ".", "vocab", ",", "\n", ")", "\n", "new_instance", "[", "\"metadata\"", "]", ".", "metadata", "[", "\"clusters\"", "]", "=", "[", "]", "# type: ignore", "\n", "instances", ".", "append", "(", "new_instance", ")", "\n", "", "return", "instances", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.predictors.coref.CorefPredictor.replace_corefs": [[125, 167], ["list", "range"], "methods", ["None"], ["", "@", "staticmethod", "\n", "def", "replace_corefs", "(", "document", ":", "Doc", ",", "clusters", ":", "List", "[", "List", "[", "List", "[", "int", "]", "]", "]", ")", "->", "str", ":", "\n", "        ", "\"\"\"\n        Uses a list of coreference clusters to convert a spacy document into a\n        string, where each coreference is replaced by its main mention.\n        \"\"\"", "\n", "# Original tokens with correct whitespace", "\n", "resolved", "=", "list", "(", "tok", ".", "text_with_ws", "for", "tok", "in", "document", ")", "\n", "\n", "for", "cluster", "in", "clusters", ":", "\n", "# The main mention is the first item in the cluster", "\n", "            ", "mention_start", ",", "mention_end", "=", "cluster", "[", "0", "]", "[", "0", "]", ",", "cluster", "[", "0", "]", "[", "1", "]", "+", "1", "\n", "mention_span", "=", "document", "[", "mention_start", ":", "mention_end", "]", "\n", "\n", "# The coreferences are all items following the first in the cluster", "\n", "for", "coref", "in", "cluster", "[", "1", ":", "]", ":", "\n", "                ", "final_token", "=", "document", "[", "coref", "[", "1", "]", "]", "\n", "# In both of the following cases, the first token in the coreference", "\n", "# is replaced with the main mention, while all subsequent tokens", "\n", "# are masked out with \"\", so that they can be elimated from", "\n", "# the returned document during \"\".join(resolved).", "\n", "\n", "# The first case attempts to correctly handle possessive coreferences", "\n", "# by inserting \"'s\" between the mention and the final whitespace", "\n", "# These include my, his, her, their, our, etc.", "\n", "\n", "# Disclaimer: Grammar errors can occur when the main mention is plural,", "\n", "# e.g. \"zebras\" becomes \"zebras's\" because this case isn't", "\n", "# being explictly checked and handled.", "\n", "\n", "if", "final_token", ".", "tag_", "in", "[", "\"PRP$\"", ",", "\"POS\"", "]", ":", "\n", "                    ", "resolved", "[", "coref", "[", "0", "]", "]", "=", "(", "\n", "mention_span", ".", "text", "+", "\"'s\"", "+", "final_token", ".", "whitespace_", "\n", ")", "\n", "", "else", ":", "\n", "# If not possessive, then replace first token with main mention directly", "\n", "                    ", "resolved", "[", "coref", "[", "0", "]", "]", "=", "mention_span", ".", "text", "+", "final_token", ".", "whitespace_", "\n", "# Mask out remaining tokens", "\n", "", "for", "i", "in", "range", "(", "coref", "[", "0", "]", "+", "1", ",", "coref", "[", "1", "]", "+", "1", ")", ":", "\n", "                    ", "resolved", "[", "i", "]", "=", "\"\"", "\n", "\n", "", "", "", "return", "\"\"", ".", "join", "(", "resolved", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.predictors.coref.CorefPredictor.coref_resolved": [[168, 190], ["coref.CorefPredictor._spacy", "coref.CorefPredictor.predict().get", "coref.CorefPredictor.replace_corefs", "coref.CorefPredictor.predict"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.params.Params.get", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.predictors.coref.CorefPredictor.replace_corefs", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.predictors.constituency_parser.ConstituencyParserPredictor.predict"], ["", "def", "coref_resolved", "(", "self", ",", "document", ":", "str", ")", "->", "str", ":", "\n", "        ", "\"\"\"\n        Produce a document where each coreference is replaced by the its main mention\n\n        # Parameters\n\n        document : `str`\n            A string representation of a document.\n\n        # Returns\n\n        A string with each coference replaced by its main mention\n        \"\"\"", "\n", "\n", "spacy_document", "=", "self", ".", "_spacy", "(", "document", ")", "\n", "clusters", "=", "self", ".", "predict", "(", "document", ")", ".", "get", "(", "\"clusters\"", ")", "\n", "\n", "# Passing a document with no coreferences returns its original form", "\n", "if", "not", "clusters", ":", "\n", "            ", "return", "document", "\n", "\n", "", "return", "self", ".", "replace_corefs", "(", "spacy_document", ",", "clusters", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.predictors.coref.CorefPredictor._words_list_to_instance": [[191, 205], ["spacy.tokens.Doc", "filter", "coref.CorefPredictor._dataset_reader.text_to_instance"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.util.WorkerLogFilter.filter", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.coreference_resolution.conll.ConllCorefReader.text_to_instance"], ["", "def", "_words_list_to_instance", "(", "self", ",", "words", ":", "List", "[", "str", "]", ")", "->", "Instance", ":", "\n", "        ", "\"\"\"\n        Create an instance from words list represent an already tokenized document,\n        for skipping tokenization when that information already exist for the user\n        \"\"\"", "\n", "spacy_document", "=", "Doc", "(", "self", ".", "_spacy", ".", "vocab", ",", "words", "=", "words", ")", "\n", "for", "pipe", "in", "filter", "(", "None", ",", "self", ".", "_spacy", ".", "pipeline", ")", ":", "\n", "            ", "pipe", "[", "1", "]", "(", "spacy_document", ")", "\n", "\n", "", "sentences", "=", "[", "\n", "[", "token", ".", "text", "for", "token", "in", "sentence", "]", "for", "sentence", "in", "spacy_document", ".", "sents", "\n", "]", "\n", "instance", "=", "self", ".", "_dataset_reader", ".", "text_to_instance", "(", "sentences", ")", "\n", "return", "instance", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.predictors.coref.CorefPredictor._json_to_instance": [[206, 218], ["coref.CorefPredictor._spacy", "coref.CorefPredictor._dataset_reader.text_to_instance"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.coreference_resolution.conll.ConllCorefReader.text_to_instance"], ["", "@", "overrides", "\n", "def", "_json_to_instance", "(", "self", ",", "json_dict", ":", "JsonDict", ")", "->", "Instance", ":", "\n", "        ", "\"\"\"\n        Expects JSON that looks like `{\"document\": \"string of document text\"}`\n        \"\"\"", "\n", "document", "=", "json_dict", "[", "\"document\"", "]", "\n", "spacy_document", "=", "self", ".", "_spacy", "(", "document", ")", "\n", "sentences", "=", "[", "\n", "[", "token", ".", "text", "for", "token", "in", "sentence", "]", "for", "sentence", "in", "spacy_document", ".", "sents", "\n", "]", "\n", "instance", "=", "self", ".", "_dataset_reader", ".", "text_to_instance", "(", "sentences", ")", "\n", "return", "instance", "\n", "", "", ""]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.predictors.seq2seq.Seq2SeqPredictor.predict": [[17, 19], ["seq2seq.Seq2SeqPredictor.predict_json"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.predictors.semantic_role_labeler.SemanticRoleLabelerPredictor.predict_json"], ["def", "predict", "(", "self", ",", "source", ":", "str", ")", "->", "JsonDict", ":", "\n", "        ", "return", "self", ".", "predict_json", "(", "{", "\"source\"", ":", "source", "}", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.predictors.seq2seq.Seq2SeqPredictor._json_to_instance": [[20, 27], ["seq2seq.Seq2SeqPredictor._dataset_reader.text_to_instance"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.coreference_resolution.conll.ConllCorefReader.text_to_instance"], ["", "@", "overrides", "\n", "def", "_json_to_instance", "(", "self", ",", "json_dict", ":", "JsonDict", ")", "->", "Instance", ":", "\n", "        ", "\"\"\"\n        Expects JSON that looks like `{\"source\": \"...\"}`.\n        \"\"\"", "\n", "source", "=", "json_dict", "[", "\"source\"", "]", "\n", "return", "self", ".", "_dataset_reader", ".", "text_to_instance", "(", "source", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.predictors.simple_seq2seq.SimpleSeq2SeqPredictor.__init__": [[15, 21], ["allennlp.predictors.seq2seq.Seq2SeqPredictor.__init__", "warnings.warn"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.graph_util.plot.GraphPlot.__init__"], ["def", "__init__", "(", "self", ",", "model", ":", "Model", ",", "dataset_reader", ":", "DatasetReader", ")", "->", "None", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "model", ",", "dataset_reader", ")", "\n", "warnings", ".", "warn", "(", "\n", "\"The 'simple_seq2seq' predictor has been deprecated in favor of \"", "\n", "\"the 'seq2seq' predictor. This will be removed in version 0.10.\"", ",", "\n", "DeprecationWarning", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.predictors.event2mind.Event2MindPredictor.predict": [[14, 33], ["event2mind.Event2MindPredictor.predict_json"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.predictors.semantic_role_labeler.SemanticRoleLabelerPredictor.predict_json"], ["def", "predict", "(", "self", ",", "source", ":", "str", ")", "->", "JsonDict", ":", "\n", "        ", "\"\"\"\n        Given a source string of some event, returns a JSON dictionary\n        containing, for each target type, the top predicted sequences as\n        indices, as tokens and the log probability of each.\n\n        The JSON dictionary looks like:\n\n        .. code-block:: js\n\n            {\n                `${target_type}_top_k_predictions`: [[1, 2, 3], [4, 5, 6], ...],\n                `${target_type}_top_k_predicted_tokens`: [[\"to\", \"feel\", \"brave\"], ...],\n                `${target_type}_top_k_log_probabilities`: [-0.301, -0.046, ...]\n            }\n\n        By default `target_type` can be xreact, oreact and xintent.\n        \"\"\"", "\n", "return", "self", ".", "predict_json", "(", "{", "\"source\"", ":", "source", "}", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.predictors.event2mind.Event2MindPredictor._json_to_instance": [[34, 41], ["event2mind.Event2MindPredictor._dataset_reader.text_to_instance"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.coreference_resolution.conll.ConllCorefReader.text_to_instance"], ["", "@", "overrides", "\n", "def", "_json_to_instance", "(", "self", ",", "json_dict", ":", "JsonDict", ")", "->", "Instance", ":", "\n", "        ", "\"\"\"\n        Expects JSON that looks like `{\"source\": \"...\"}`.\n        \"\"\"", "\n", "source", "=", "json_dict", "[", "\"source\"", "]", "\n", "return", "self", ".", "_dataset_reader", ".", "text_to_instance", "(", "source", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.predictors.biaffine_dependency_parser.BiaffineDependencyParserPredictor.__init__": [[85, 94], ["allennlp.predictors.predictor.Predictor.__init__", "allennlp.data.tokenizers.spacy_tokenizer.SpacyTokenizer"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.graph_util.plot.GraphPlot.__init__"], ["def", "__init__", "(", "\n", "self", ",", "\n", "model", ":", "Model", ",", "\n", "dataset_reader", ":", "DatasetReader", ",", "\n", "language", ":", "str", "=", "\"en_core_web_sm\"", ",", "\n", ")", "->", "None", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "model", ",", "dataset_reader", ")", "\n", "# TODO(Mark) Make the language configurable and based on a model attribute.", "\n", "self", ".", "_tokenizer", "=", "SpacyTokenizer", "(", "language", "=", "language", ",", "pos_tags", "=", "True", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.predictors.biaffine_dependency_parser.BiaffineDependencyParserPredictor.predict": [[95, 107], ["biaffine_dependency_parser.BiaffineDependencyParserPredictor.predict_json"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.predictors.semantic_role_labeler.SemanticRoleLabelerPredictor.predict_json"], ["", "def", "predict", "(", "self", ",", "sentence", ":", "str", ")", "->", "JsonDict", ":", "\n", "        ", "\"\"\"\n        Predict a dependency parse for the given sentence.\n        # Parameters\n\n        sentence The sentence to parse.\n\n        # Returns\n\n        A dictionary representation of the dependency tree.\n        \"\"\"", "\n", "return", "self", ".", "predict_json", "(", "{", "\"sentence\"", ":", "sentence", "}", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.predictors.biaffine_dependency_parser.BiaffineDependencyParserPredictor._json_to_instance": [[108, 122], ["biaffine_dependency_parser.BiaffineDependencyParserPredictor._tokenizer.tokenize", "biaffine_dependency_parser.BiaffineDependencyParserPredictor._dataset_reader.text_to_instance"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.tokenizers.whitespace_tokenizer.WhitespaceTokenizer.tokenize", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.coreference_resolution.conll.ConllCorefReader.text_to_instance"], ["", "@", "overrides", "\n", "def", "_json_to_instance", "(", "self", ",", "json_dict", ":", "JsonDict", ")", "->", "Instance", ":", "\n", "        ", "\"\"\"\n        Expects JSON that looks like `{\"sentence\": \"...\"}`.\n        \"\"\"", "\n", "spacy_tokens", "=", "self", ".", "_tokenizer", ".", "tokenize", "(", "json_dict", "[", "\"sentence\"", "]", ")", "\n", "sentence_text", "=", "[", "token", ".", "text", "for", "token", "in", "spacy_tokens", "]", "\n", "if", "self", ".", "_dataset_reader", ".", "use_language_specific_pos", ":", "# type: ignore", "\n", "# fine-grained part of speech", "\n", "            ", "pos_tags", "=", "[", "token", ".", "tag_", "for", "token", "in", "spacy_tokens", "]", "\n", "", "else", ":", "\n", "# coarse-grained part of speech (Universal Depdendencies format)", "\n", "            ", "pos_tags", "=", "[", "token", ".", "pos_", "for", "token", "in", "spacy_tokens", "]", "\n", "", "return", "self", ".", "_dataset_reader", ".", "text_to_instance", "(", "sentence_text", ",", "pos_tags", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.predictors.biaffine_dependency_parser.BiaffineDependencyParserPredictor.predict_instance": [[123, 133], ["biaffine_dependency_parser.BiaffineDependencyParserPredictor._model.forward_on_instance", "biaffine_dependency_parser.BiaffineDependencyParserPredictor._build_hierplane_tree", "allennlp.common.util.sanitize"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.models.model.Model.forward_on_instance", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.predictors.constituency_parser.ConstituencyParserPredictor._build_hierplane_tree", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.util.sanitize"], ["", "@", "overrides", "\n", "def", "predict_instance", "(", "self", ",", "instance", ":", "Instance", ")", "->", "JsonDict", ":", "\n", "        ", "outputs", "=", "self", ".", "_model", ".", "forward_on_instance", "(", "instance", ")", "\n", "\n", "words", "=", "outputs", "[", "\"words\"", "]", "\n", "pos", "=", "outputs", "[", "\"pos\"", "]", "\n", "heads", "=", "outputs", "[", "\"predicted_heads\"", "]", "\n", "tags", "=", "outputs", "[", "\"predicted_dependencies\"", "]", "\n", "outputs", "[", "\"hierplane_tree\"", "]", "=", "self", ".", "_build_hierplane_tree", "(", "words", ",", "heads", ",", "tags", ",", "pos", ")", "\n", "return", "sanitize", "(", "outputs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.predictors.biaffine_dependency_parser.BiaffineDependencyParserPredictor.predict_batch_instance": [[134, 146], ["biaffine_dependency_parser.BiaffineDependencyParserPredictor._model.forward_on_instances", "allennlp.common.util.sanitize", "biaffine_dependency_parser.BiaffineDependencyParserPredictor._build_hierplane_tree"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.models.model.Model.forward_on_instances", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.util.sanitize", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.predictors.constituency_parser.ConstituencyParserPredictor._build_hierplane_tree"], ["", "@", "overrides", "\n", "def", "predict_batch_instance", "(", "self", ",", "instances", ":", "List", "[", "Instance", "]", ")", "->", "List", "[", "JsonDict", "]", ":", "\n", "        ", "outputs", "=", "self", ".", "_model", ".", "forward_on_instances", "(", "instances", ")", "\n", "for", "output", "in", "outputs", ":", "\n", "            ", "words", "=", "output", "[", "\"words\"", "]", "\n", "pos", "=", "output", "[", "\"pos\"", "]", "\n", "heads", "=", "output", "[", "\"predicted_heads\"", "]", "\n", "tags", "=", "output", "[", "\"predicted_dependencies\"", "]", "\n", "output", "[", "\"hierplane_tree\"", "]", "=", "self", ".", "_build_hierplane_tree", "(", "\n", "words", ",", "heads", ",", "tags", ",", "pos", "\n", ")", "\n", "", "return", "sanitize", "(", "outputs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.predictors.biaffine_dependency_parser.BiaffineDependencyParserPredictor._build_hierplane_tree": [[147, 203], ["enumerate", "heads.index", "enumerate", "biaffine_dependency_parser.BiaffineDependencyParserPredictor._build_hierplane_tree.node_constuctor"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.fields.text_field.TextField.index"], ["", "@", "staticmethod", "\n", "def", "_build_hierplane_tree", "(", "\n", "words", ":", "List", "[", "str", "]", ",", "heads", ":", "List", "[", "int", "]", ",", "tags", ":", "List", "[", "str", "]", ",", "pos", ":", "List", "[", "str", "]", "\n", ")", "->", "Dict", "[", "str", ",", "Any", "]", ":", "\n", "        ", "\"\"\"\n        # Returns\n\n        A JSON dictionary render-able by Hierplane for the given tree.\n        \"\"\"", "\n", "\n", "word_index_to_cumulative_indices", ":", "Dict", "[", "int", ",", "Tuple", "[", "int", ",", "int", "]", "]", "=", "{", "}", "\n", "cumulative_index", "=", "0", "\n", "for", "i", ",", "word", "in", "enumerate", "(", "words", ")", ":", "\n", "            ", "word_length", "=", "len", "(", "word", ")", "+", "1", "\n", "word_index_to_cumulative_indices", "[", "i", "]", "=", "(", "\n", "cumulative_index", ",", "\n", "cumulative_index", "+", "word_length", ",", "\n", ")", "\n", "cumulative_index", "+=", "word_length", "\n", "\n", "", "def", "node_constuctor", "(", "index", ":", "int", ")", ":", "\n", "            ", "children", "=", "[", "]", "\n", "for", "next_index", ",", "child", "in", "enumerate", "(", "heads", ")", ":", "\n", "                ", "if", "child", "==", "index", "+", "1", ":", "\n", "                    ", "children", ".", "append", "(", "node_constuctor", "(", "next_index", ")", ")", "\n", "\n", "# These are the icons which show up in the bottom right", "\n", "# corner of the node.", "\n", "", "", "attributes", "=", "[", "pos", "[", "index", "]", "]", "\n", "start", ",", "end", "=", "word_index_to_cumulative_indices", "[", "index", "]", "\n", "\n", "hierplane_node", "=", "{", "\n", "\"word\"", ":", "words", "[", "index", "]", ",", "\n", "# The type of the node - all nodes with the same", "\n", "# type have a unified colour.", "\n", "\"nodeType\"", ":", "tags", "[", "index", "]", ",", "\n", "# Attributes of the node.", "\n", "\"attributes\"", ":", "attributes", ",", "\n", "# The link between  the node and it's parent.", "\n", "\"link\"", ":", "tags", "[", "index", "]", ",", "\n", "\"spans\"", ":", "[", "{", "\"start\"", ":", "start", ",", "\"end\"", ":", "end", "}", "]", ",", "\n", "}", "\n", "if", "children", ":", "\n", "                ", "hierplane_node", "[", "\"children\"", "]", "=", "children", "\n", "", "return", "hierplane_node", "\n", "\n", "# We are guaranteed that there is a single word pointing to", "\n", "# the root index, so we can find it just by searching for 0 in the list.", "\n", "", "root_index", "=", "heads", ".", "index", "(", "0", ")", "\n", "hierplane_tree", "=", "{", "\n", "\"text\"", ":", "\" \"", ".", "join", "(", "words", ")", ",", "\n", "\"root\"", ":", "node_constuctor", "(", "root_index", ")", ",", "\n", "\"nodeTypeToStyle\"", ":", "NODE_TYPE_TO_STYLE", ",", "\n", "\"linkToPosition\"", ":", "LINK_TO_POSITION", ",", "\n", "}", "\n", "return", "hierplane_tree", "\n", "", "", ""]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.predictors.next_token_lm.NextTokenLMPredictor.predict": [[15, 17], ["next_token_lm.NextTokenLMPredictor.predict_json"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.predictors.semantic_role_labeler.SemanticRoleLabelerPredictor.predict_json"], ["    ", "def", "predict", "(", "self", ",", "sentence", ":", "str", ")", "->", "JsonDict", ":", "\n", "        ", "return", "self", ".", "predict_json", "(", "{", "\"sentence\"", ":", "sentence", "}", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.predictors.next_token_lm.NextTokenLMPredictor.predictions_to_labeled_instances": [[18, 32], ["copy.deepcopy", "copy.deepcopy.add_field", "allennlp.data.Token", "allennlp.data.fields.TextField"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.data.instance.Instance.add_field"], ["", "@", "overrides", "\n", "def", "predictions_to_labeled_instances", "(", "\n", "self", ",", "instance", ":", "Instance", ",", "outputs", ":", "Dict", "[", "str", ",", "numpy", ".", "ndarray", "]", "\n", ")", ":", "\n", "        ", "new_instance", "=", "deepcopy", "(", "instance", ")", "\n", "token_field", ":", "TextField", "=", "instance", "[", "\"tokens\"", "]", "# type: ignore", "\n", "mask_targets", "=", "[", "Token", "(", "target_top_k", "[", "0", "]", ")", "for", "target_top_k", "in", "outputs", "[", "\"words\"", "]", "]", "\n", "\n", "new_instance", ".", "add_field", "(", "\n", "\"target_ids\"", ",", "\n", "TextField", "(", "mask_targets", ",", "token_field", ".", "_token_indexers", ")", ",", "\n", "vocab", "=", "self", ".", "_model", ".", "vocab", ",", "\n", ")", "\n", "return", "[", "new_instance", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.predictors.next_token_lm.NextTokenLMPredictor._json_to_instance": [[33, 40], ["next_token_lm.NextTokenLMPredictor._dataset_reader.text_to_instance"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.coreference_resolution.conll.ConllCorefReader.text_to_instance"], ["", "@", "overrides", "\n", "def", "_json_to_instance", "(", "self", ",", "json_dict", ":", "JsonDict", ")", "->", "Instance", ":", "\n", "        ", "\"\"\"\n        Expects JSON that looks like `{\"sentence\": \"...\"}`.\n        \"\"\"", "\n", "sentence", "=", "json_dict", "[", "\"sentence\"", "]", "\n", "return", "self", ".", "_dataset_reader", ".", "text_to_instance", "(", "sentence", "=", "sentence", ")", "# type: ignore", "\n", "", "", ""]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.predictors.masked_language_model.MaskedLanguageModelPredictor.predict": [[15, 17], ["masked_language_model.MaskedLanguageModelPredictor.predict_json"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.predictors.semantic_role_labeler.SemanticRoleLabelerPredictor.predict_json"], ["    ", "def", "predict", "(", "self", ",", "sentence_with_masks", ":", "str", ")", "->", "JsonDict", ":", "\n", "        ", "return", "self", ".", "predict_json", "(", "{", "\"sentence\"", ":", "sentence_with_masks", "}", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.predictors.masked_language_model.MaskedLanguageModelPredictor.predictions_to_labeled_instances": [[18, 32], ["copy.deepcopy", "copy.deepcopy.add_field", "allennlp.data.Token", "allennlp.data.fields.TextField"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.data.instance.Instance.add_field"], ["", "@", "overrides", "\n", "def", "predictions_to_labeled_instances", "(", "\n", "self", ",", "instance", ":", "Instance", ",", "outputs", ":", "Dict", "[", "str", ",", "numpy", ".", "ndarray", "]", "\n", ")", ":", "\n", "        ", "new_instance", "=", "deepcopy", "(", "instance", ")", "\n", "token_field", ":", "TextField", "=", "instance", "[", "\"tokens\"", "]", "# type: ignore", "\n", "mask_targets", "=", "[", "Token", "(", "target_top_k", "[", "0", "]", ")", "for", "target_top_k", "in", "outputs", "[", "\"words\"", "]", "]", "\n", "\n", "new_instance", ".", "add_field", "(", "\n", "\"target_ids\"", ",", "\n", "TextField", "(", "mask_targets", ",", "token_field", ".", "_token_indexers", ")", ",", "\n", "vocab", "=", "self", ".", "_model", ".", "vocab", ",", "\n", ")", "\n", "return", "[", "new_instance", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.predictors.masked_language_model.MaskedLanguageModelPredictor._json_to_instance": [[33, 40], ["masked_language_model.MaskedLanguageModelPredictor._dataset_reader.text_to_instance"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.coreference_resolution.conll.ConllCorefReader.text_to_instance"], ["", "@", "overrides", "\n", "def", "_json_to_instance", "(", "self", ",", "json_dict", ":", "JsonDict", ")", "->", "Instance", ":", "\n", "        ", "\"\"\"\n        Expects JSON that looks like `{\"sentence\": \"...\"}`.\n        \"\"\"", "\n", "sentence", "=", "json_dict", "[", "\"sentence\"", "]", "\n", "return", "self", ".", "_dataset_reader", ".", "text_to_instance", "(", "sentence", "=", "sentence", ")", "# type: ignore", "\n", "", "", ""]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.predictors.constituency_parser.ConstituencyParserPredictor.__init__": [[66, 74], ["allennlp.predictors.predictor.Predictor.__init__", "allennlp.data.tokenizers.spacy_tokenizer.SpacyTokenizer"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.graph_util.plot.GraphPlot.__init__"], ["def", "__init__", "(", "\n", "self", ",", "\n", "model", ":", "Model", ",", "\n", "dataset_reader", ":", "DatasetReader", ",", "\n", "language", ":", "str", "=", "\"en_core_web_sm\"", ",", "\n", ")", "->", "None", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "model", ",", "dataset_reader", ")", "\n", "self", ".", "_tokenizer", "=", "SpacyTokenizer", "(", "language", "=", "language", ",", "pos_tags", "=", "True", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.predictors.constituency_parser.ConstituencyParserPredictor.predict": [[75, 87], ["constituency_parser.ConstituencyParserPredictor.predict_json"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.predictors.semantic_role_labeler.SemanticRoleLabelerPredictor.predict_json"], ["", "def", "predict", "(", "self", ",", "sentence", ":", "str", ")", "->", "JsonDict", ":", "\n", "        ", "\"\"\"\n        Predict a constituency parse for the given sentence.\n        # Parameters\n\n        sentence The sentence to parse.\n\n        # Returns\n\n        A dictionary representation of the constituency tree.\n        \"\"\"", "\n", "return", "self", ".", "predict_json", "(", "{", "\"sentence\"", ":", "sentence", "}", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.predictors.constituency_parser.ConstituencyParserPredictor._json_to_instance": [[88, 97], ["constituency_parser.ConstituencyParserPredictor._tokenizer.tokenize", "constituency_parser.ConstituencyParserPredictor._dataset_reader.text_to_instance"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.tokenizers.whitespace_tokenizer.WhitespaceTokenizer.tokenize", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.coreference_resolution.conll.ConllCorefReader.text_to_instance"], ["", "@", "overrides", "\n", "def", "_json_to_instance", "(", "self", ",", "json_dict", ":", "JsonDict", ")", "->", "Instance", ":", "\n", "        ", "\"\"\"\n        Expects JSON that looks like `{\"sentence\": \"...\"}`.\n        \"\"\"", "\n", "spacy_tokens", "=", "self", ".", "_tokenizer", ".", "tokenize", "(", "json_dict", "[", "\"sentence\"", "]", ")", "\n", "sentence_text", "=", "[", "token", ".", "text", "for", "token", "in", "spacy_tokens", "]", "\n", "pos_tags", "=", "[", "token", ".", "tag_", "for", "token", "in", "spacy_tokens", "]", "\n", "return", "self", ".", "_dataset_reader", ".", "text_to_instance", "(", "sentence_text", ",", "pos_tags", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.predictors.constituency_parser.ConstituencyParserPredictor.predict_instance": [[98, 107], ["constituency_parser.ConstituencyParserPredictor._model.forward_on_instance", "constituency_parser.ConstituencyParserPredictor.pop", "constituency_parser.ConstituencyParserPredictor._build_hierplane_tree", "constituency_parser.ConstituencyParserPredictor.pop.pformat", "allennlp.common.util.sanitize"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.models.model.Model.forward_on_instance", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.params.Params.pop", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.predictors.constituency_parser.ConstituencyParserPredictor._build_hierplane_tree", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.util.sanitize"], ["", "@", "overrides", "\n", "def", "predict_instance", "(", "self", ",", "instance", ":", "Instance", ")", "->", "JsonDict", ":", "\n", "        ", "outputs", "=", "self", ".", "_model", ".", "forward_on_instance", "(", "instance", ")", "\n", "\n", "# format the NLTK tree as a string on a single line.", "\n", "tree", "=", "outputs", ".", "pop", "(", "\"trees\"", ")", "\n", "outputs", "[", "\"hierplane_tree\"", "]", "=", "self", ".", "_build_hierplane_tree", "(", "tree", ",", "0", ",", "is_root", "=", "True", ")", "\n", "outputs", "[", "\"trees\"", "]", "=", "tree", ".", "pformat", "(", "margin", "=", "1000000", ")", "\n", "return", "sanitize", "(", "outputs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.predictors.constituency_parser.ConstituencyParserPredictor.predict_batch_instance": [[108, 117], ["constituency_parser.ConstituencyParserPredictor._model.forward_on_instances", "allennlp.common.util.sanitize", "output.pop", "constituency_parser.ConstituencyParserPredictor._build_hierplane_tree", "output.pop.pformat"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.models.model.Model.forward_on_instances", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.util.sanitize", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.params.Params.pop", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.predictors.constituency_parser.ConstituencyParserPredictor._build_hierplane_tree"], ["", "@", "overrides", "\n", "def", "predict_batch_instance", "(", "self", ",", "instances", ":", "List", "[", "Instance", "]", ")", "->", "List", "[", "JsonDict", "]", ":", "\n", "        ", "outputs", "=", "self", ".", "_model", ".", "forward_on_instances", "(", "instances", ")", "\n", "for", "output", "in", "outputs", ":", "\n", "# format the NLTK tree as a string on a single line.", "\n", "            ", "tree", "=", "output", ".", "pop", "(", "\"trees\"", ")", "\n", "output", "[", "\"hierplane_tree\"", "]", "=", "self", ".", "_build_hierplane_tree", "(", "tree", ",", "0", ",", "is_root", "=", "True", ")", "\n", "output", "[", "\"trees\"", "]", "=", "tree", ".", "pformat", "(", "margin", "=", "1000000", ")", "\n", "", "return", "sanitize", "(", "outputs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.predictors.constituency_parser.ConstituencyParserPredictor._build_hierplane_tree": [[118, 167], ["tree.label", "isinstance", "tree.leaves", "children.append", "len", "constituency_parser.ConstituencyParserPredictor._build_hierplane_tree"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.predictors.constituency_parser.ConstituencyParserPredictor._build_hierplane_tree"], ["", "def", "_build_hierplane_tree", "(", "self", ",", "tree", ":", "Tree", ",", "index", ":", "int", ",", "is_root", ":", "bool", ")", "->", "JsonDict", ":", "\n", "        ", "\"\"\"\n        Recursively builds a JSON dictionary from an NLTK `Tree` suitable for\n        rendering trees using the `Hierplane library<https://allenai.github.io/hierplane/>`.\n\n        # Parameters\n\n        tree : `Tree`, required.\n            The tree to convert into Hierplane JSON.\n        index : int, required.\n            The character index into the tree, used for creating spans.\n        is_root : bool\n            An indicator which allows us to add the outer Hierplane JSON which\n            is required for rendering.\n\n        # Returns\n\n        A JSON dictionary render-able by Hierplane for the given tree.\n        \"\"\"", "\n", "children", "=", "[", "]", "\n", "for", "child", "in", "tree", ":", "\n", "            ", "if", "isinstance", "(", "child", ",", "Tree", ")", ":", "\n", "# If the child is a tree, it has children,", "\n", "# as NLTK leaves are just strings.", "\n", "                ", "children", ".", "append", "(", "self", ".", "_build_hierplane_tree", "(", "child", ",", "index", ",", "is_root", "=", "False", ")", ")", "\n", "", "else", ":", "\n", "# We're at a leaf, so add the length of", "\n", "# the word to the character index.", "\n", "                ", "index", "+=", "len", "(", "child", ")", "\n", "\n", "", "", "label", "=", "tree", ".", "label", "(", ")", "\n", "span", "=", "\" \"", ".", "join", "(", "tree", ".", "leaves", "(", ")", ")", "\n", "hierplane_node", "=", "{", "\n", "\"word\"", ":", "span", ",", "\n", "\"nodeType\"", ":", "label", ",", "\n", "\"attributes\"", ":", "[", "label", "]", ",", "\n", "\"link\"", ":", "label", ",", "\n", "}", "\n", "if", "children", ":", "\n", "            ", "hierplane_node", "[", "\"children\"", "]", "=", "children", "\n", "# TODO(Mark): Figure out how to span highlighting to the leaves.", "\n", "", "if", "is_root", ":", "\n", "            ", "hierplane_node", "=", "{", "\n", "\"linkNameToLabel\"", ":", "LINK_TO_LABEL", ",", "\n", "\"nodeTypeToStyle\"", ":", "NODE_TYPE_TO_STYLE", ",", "\n", "\"text\"", ":", "span", ",", "\n", "\"root\"", ":", "hierplane_node", ",", "\n", "}", "\n", "", "return", "hierplane_node", "\n", "", "", ""]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.tools.create_elmo_embeddings_from_vocab.main": [[14, 101], ["allennlp.data.token_indexers.ELMoTokenCharactersIndexer", "range", "range", "torch.cat().cpu().numpy", "os.makedirs", "os.path.split", "open", "vocab_file.read().strip().split", "allennlp.common.checks.ConfigurationError", "allennlp.data.token_indexers.ELMoTokenCharactersIndexer.tokens_to_indices", "sentences.append", "allennlp.modules.elmo._ElmoCharacterEncoder().cuda", "allennlp.modules.elmo._ElmoCharacterEncoder", "torch.stack", "token_embedding[].contiguous().view", "all_embeddings.append", "gzip.open", "enumerate", "open", "allennlp.data.Vocabulary", "allennlp.data.token_indexers.ELMoTokenCharactersIndexer.as_padded_tensor_dict", "len", "batch.cuda.cuda", "token_embedding.size", "torch.cat().cpu", "os.path.join", "embeddings_file.write", "os.path.join", "new_vocab_file.write", "vocab_file.read().strip", "allennlp.data.Token", "len", "allennlp.modules.elmo._ElmoCharacterEncoder", "len", "allennlp.modules.elmo._ElmoCharacterEncoder.", "token_embedding[].contiguous", "torch.cat", "str", "vocab_file.read", "list"], "function", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.token_indexers.token_characters_indexer.TokenCharactersIndexer.tokens_to_indices", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.token_indexers.token_characters_indexer.TokenCharactersIndexer.as_padded_tensor_dict", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.dataset_readers.multiprocess_dataset_reader.QIterable.join", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.None.predictor.write", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.dataset_readers.multiprocess_dataset_reader.QIterable.join", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.None.predictor.write", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.scripts.convert_openie_to_conll.read"], ["def", "main", "(", "\n", "vocab_path", ":", "str", ",", "\n", "elmo_config_path", ":", "str", ",", "\n", "elmo_weights_path", ":", "str", ",", "\n", "output_dir", ":", "str", ",", "\n", "batch_size", ":", "int", ",", "\n", "device", ":", "int", ",", "\n", "use_custom_oov_token", ":", "bool", "=", "False", ",", "\n", ")", ":", "\n", "    ", "\"\"\"\n    Creates ELMo word representations from a vocabulary file. These\n    word representations are _independent_ - they are the result of running\n    the CNN and Highway layers of the ELMo model, but not the Bidirectional LSTM.\n    ELMo requires 2 additional tokens: <S> and </S>. The first token\n    in this file is assumed to be an unknown token.\n\n    This script produces two artifacts: A new vocabulary file\n    with the <S> and </S> tokens inserted and a glove formatted embedding\n    file containing word : vector pairs, one per line, with all values\n    separated by a space.\n    \"\"\"", "\n", "\n", "# Load the vocabulary words and convert to char ids", "\n", "with", "open", "(", "vocab_path", ",", "\"r\"", ")", "as", "vocab_file", ":", "\n", "        ", "tokens", "=", "vocab_file", ".", "read", "(", ")", ".", "strip", "(", ")", ".", "split", "(", "\"\\n\"", ")", "\n", "\n", "# Insert the sentence boundary tokens which elmo uses at positions 1 and 2.", "\n", "", "if", "tokens", "[", "0", "]", "!=", "DEFAULT_OOV_TOKEN", "and", "not", "use_custom_oov_token", ":", "\n", "        ", "raise", "ConfigurationError", "(", "\"ELMo embeddings require the use of a OOV token.\"", ")", "\n", "\n", "", "tokens", "=", "[", "tokens", "[", "0", "]", "]", "+", "[", "\"<S>\"", ",", "\"</S>\"", "]", "+", "tokens", "[", "1", ":", "]", "\n", "\n", "indexer", "=", "ELMoTokenCharactersIndexer", "(", ")", "\n", "indices", "=", "indexer", ".", "tokens_to_indices", "(", "\n", "[", "Token", "(", "token", ")", "for", "token", "in", "tokens", "]", ",", "Vocabulary", "(", ")", "\n", ")", "[", "\"tokens\"", "]", "\n", "sentences", "=", "[", "]", "\n", "for", "k", "in", "range", "(", "(", "len", "(", "indices", ")", "//", "50", ")", "+", "1", ")", ":", "\n", "        ", "sentences", ".", "append", "(", "\n", "indexer", ".", "as_padded_tensor_dict", "(", "\n", "indices", "[", "(", "k", "*", "50", ")", ":", "(", "(", "k", "+", "1", ")", "*", "50", ")", "]", ",", "padding_lengths", "=", "{", "\"tokens\"", ":", "50", "}", "\n", ")", "\n", ")", "\n", "\n", "", "last_batch_remainder", "=", "50", "-", "(", "len", "(", "indices", ")", "%", "50", ")", "\n", "if", "device", "!=", "-", "1", ":", "\n", "        ", "elmo_token_embedder", "=", "_ElmoCharacterEncoder", "(", "\n", "elmo_config_path", ",", "elmo_weights_path", "\n", ")", ".", "cuda", "(", "device", ")", "\n", "", "else", ":", "\n", "        ", "elmo_token_embedder", "=", "_ElmoCharacterEncoder", "(", "elmo_config_path", ",", "elmo_weights_path", ")", "\n", "\n", "", "all_embeddings", "=", "[", "]", "\n", "for", "i", "in", "range", "(", "(", "len", "(", "sentences", ")", "//", "batch_size", ")", "+", "1", ")", ":", "\n", "        ", "batch", "=", "torch", ".", "stack", "(", "sentences", "[", "i", "*", "batch_size", ":", "(", "i", "+", "1", ")", "*", "batch_size", "]", ")", "\n", "if", "device", "!=", "-", "1", ":", "\n", "            ", "batch", "=", "batch", ".", "cuda", "(", "device", ")", "\n", "\n", "", "token_embedding", "=", "elmo_token_embedder", "(", "batch", ")", "[", "\"token_embedding\"", "]", ".", "data", "\n", "\n", "# Reshape back to a list of words of shape (batch_size * 50, encoding_dim)", "\n", "# We also need to remove the <S>, </S> tokens appended by the encoder.", "\n", "per_word_embeddings", "=", "(", "\n", "token_embedding", "[", ":", ",", "1", ":", "-", "1", ",", ":", "]", ".", "contiguous", "(", ")", ".", "view", "(", "-", "1", ",", "token_embedding", ".", "size", "(", "-", "1", ")", ")", "\n", ")", "\n", "\n", "all_embeddings", ".", "append", "(", "per_word_embeddings", ")", "\n", "\n", "# Remove the embeddings associated with padding in the last batch.", "\n", "", "all_embeddings", "[", "-", "1", "]", "=", "all_embeddings", "[", "-", "1", "]", "[", ":", "-", "last_batch_remainder", ",", ":", "]", "\n", "\n", "embedding_weight", "=", "torch", ".", "cat", "(", "all_embeddings", ",", "0", ")", ".", "cpu", "(", ")", ".", "numpy", "(", ")", "\n", "\n", "# Write out the embedding in a glove format.", "\n", "os", ".", "makedirs", "(", "output_dir", ",", "exist_ok", "=", "True", ")", "\n", "with", "gzip", ".", "open", "(", "\n", "os", ".", "path", ".", "join", "(", "output_dir", ",", "\"elmo_embeddings.txt.gz\"", ")", ",", "\"wb\"", "\n", ")", "as", "embeddings_file", ":", "\n", "        ", "for", "i", ",", "word", "in", "enumerate", "(", "tokens", ")", ":", "\n", "            ", "string_array", "=", "\" \"", ".", "join", "(", "[", "str", "(", "x", ")", "for", "x", "in", "list", "(", "embedding_weight", "[", "i", ",", ":", "]", ")", "]", ")", "\n", "embeddings_file", ".", "write", "(", "f\"{word} {string_array}\\n\"", ".", "encode", "(", "\"utf-8\"", ")", ")", "\n", "\n", "# Write out the new vocab with the <S> and </S> tokens.", "\n", "", "", "_", ",", "vocab_file_name", "=", "os", ".", "path", ".", "split", "(", "vocab_path", ")", "\n", "with", "open", "(", "os", ".", "path", ".", "join", "(", "output_dir", ",", "vocab_file_name", ")", ",", "\"w\"", ")", "as", "new_vocab_file", ":", "\n", "        ", "for", "word", "in", "tokens", ":", "\n", "            ", "new_vocab_file", ".", "write", "(", "f\"{word}\\n\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.tools.archive_surgery.main": [[29, 84], ["argparse.ArgumentParser", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_mutually_exclusive_group", "parser.add_mutually_exclusive_group.add_argument", "parser.add_mutually_exclusive_group.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.parse_args", "allennlp.common.file_utils.cached_path", "tempfile.mkdtemp", "atexit.register", "os.path.join", "subprocess.run", "RuntimeError", "os.path.exists", "ValueError", "os.path.exists", "ValueError", "tarfile.open", "archive.extractall", "tarfile.open", "tar.add", "os.environ.get", "shutil.rmtree"], "function", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.commands.__init__.ArgumentParserWithDefaults.add_argument", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.commands.__init__.ArgumentParserWithDefaults.add_argument", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.commands.__init__.ArgumentParserWithDefaults.add_argument", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.commands.__init__.ArgumentParserWithDefaults.add_argument", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.commands.__init__.ArgumentParserWithDefaults.add_argument", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.file_utils.cached_path", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.commands.subcommand.Subcommand.register", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.dataset_readers.multiprocess_dataset_reader.QIterable.join", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.commands.train.TrainModel.run", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.params.Params.get"], ["def", "main", "(", ")", ":", "\n", "    ", "parser", "=", "argparse", ".", "ArgumentParser", "(", "\n", "description", "=", "\"Perform surgery on a model.tar.gz archive\"", ",", "\n", "formatter_class", "=", "argparse", ".", "ArgumentDefaultsHelpFormatter", ",", "\n", ")", "\n", "\n", "parser", ".", "add_argument", "(", "\"--input-file\"", ",", "required", "=", "True", ",", "help", "=", "\"path to input file\"", ")", "\n", "parser", ".", "add_argument", "(", "\n", "\"--editor\"", ",", "\n", "default", "=", "os", ".", "environ", ".", "get", "(", "\"EDITOR\"", ")", ",", "\n", "help", "=", "\"editor to launch, whose default value is `$EDITOR` the environment variable\"", ",", "\n", ")", "\n", "output", "=", "parser", ".", "add_mutually_exclusive_group", "(", ")", "\n", "output", ".", "add_argument", "(", "\"--output-file\"", ",", "help", "=", "\"path to output file\"", ")", "\n", "output", ".", "add_argument", "(", "\n", "\"--inplace\"", ",", "\n", "action", "=", "\"store_true\"", ",", "\n", "help", "=", "\"overwrite the input file with the modified configuration\"", ",", "\n", ")", "\n", "parser", ".", "add_argument", "(", "\n", "\"-f\"", ",", "\n", "\"--force\"", ",", "\n", "action", "=", "\"store_true\"", ",", "\n", "help", "=", "\"overwrite the output file if it exists\"", ",", "\n", ")", "\n", "\n", "args", "=", "parser", ".", "parse_args", "(", ")", "\n", "\n", "if", "args", ".", "editor", "is", "None", ":", "\n", "        ", "raise", "RuntimeError", "(", "\n", "\"please specify an editor or set the $EDITOR environment variable\"", "\n", ")", "\n", "\n", "", "if", "not", "args", ".", "inplace", "and", "os", ".", "path", ".", "exists", "(", "args", ".", "output_file", ")", "and", "not", "args", ".", "force", ":", "\n", "        ", "raise", "ValueError", "(", "\"output file already exists, use --force to override\"", ")", "\n", "\n", "", "archive_file", "=", "cached_path", "(", "args", ".", "input_file", ")", "\n", "if", "not", "os", ".", "path", ".", "exists", "(", "archive_file", ")", ":", "\n", "        ", "raise", "ValueError", "(", "\"input file doesn't exist\"", ")", "\n", "", "if", "args", ".", "inplace", ":", "\n", "        ", "output_file", "=", "archive_file", "\n", "", "else", ":", "\n", "        ", "output_file", "=", "args", ".", "output_file", "\n", "\n", "# Extract archive to temp dir", "\n", "", "tempdir", "=", "tempfile", ".", "mkdtemp", "(", ")", "\n", "with", "tarfile", ".", "open", "(", "archive_file", ",", "\"r:gz\"", ")", "as", "archive", ":", "\n", "        ", "archive", ".", "extractall", "(", "tempdir", ")", "\n", "", "atexit", ".", "register", "(", "lambda", ":", "shutil", ".", "rmtree", "(", "tempdir", ")", ")", "\n", "\n", "config_path", "=", "os", ".", "path", ".", "join", "(", "tempdir", ",", "CONFIG_NAME", ")", "\n", "subprocess", ".", "run", "(", "[", "args", ".", "editor", ",", "config_path", "]", ",", "check", "=", "False", ")", "\n", "\n", "with", "tarfile", ".", "open", "(", "output_file", ",", "\"w:gz\"", ")", "as", "tar", ":", "\n", "        ", "tar", ".", "add", "(", "tempdir", ",", "arcname", "=", "os", ".", "path", ".", "sep", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.tools.inspect_cache.main": [[7, 26], ["print", "os.listdir", "os.path.exists", "print", "print", "print", "print", "filename.endswith", "allennlp.common.file_utils.filename_to_url", "print", "print", "print", "print"], "function", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.file_utils.filename_to_url"], ["def", "main", "(", ")", ":", "\n", "    ", "print", "(", "f\"Looking for datasets in {CACHE_DIRECTORY}...\"", ")", "\n", "if", "not", "os", ".", "path", ".", "exists", "(", "CACHE_DIRECTORY", ")", ":", "\n", "        ", "print", "(", "\"Directory does not exist.\"", ")", "\n", "print", "(", "\"No cached datasets found.\"", ")", "\n", "\n", "", "cached_files", "=", "os", ".", "listdir", "(", "CACHE_DIRECTORY", ")", "\n", "\n", "if", "not", "cached_files", ":", "\n", "        ", "print", "(", "\"Directory is empty.\"", ")", "\n", "print", "(", "\"No cached datasets found.\"", ")", "\n", "\n", "", "for", "filename", "in", "cached_files", ":", "\n", "        ", "if", "not", "filename", ".", "endswith", "(", "\"json\"", ")", ":", "\n", "            ", "url", ",", "etag", "=", "filename_to_url", "(", "filename", ")", "\n", "print", "(", "\"Filename: %s\"", "%", "filename", ")", "\n", "print", "(", "\"Url: %s\"", "%", "url", ")", "\n", "print", "(", "\"ETag: %s\"", "%", "etag", ")", "\n", "print", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.momentum_schedulers.momentum_scheduler.MomentumScheduler.__init__": [[9, 11], ["allennlp.training.scheduler.Scheduler.__init__"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.graph_util.plot.GraphPlot.__init__"], ["    ", "def", "__init__", "(", "self", ",", "optimizer", ":", "torch", ".", "optim", ".", "Optimizer", ",", "last_epoch", ":", "int", "=", "-", "1", ")", "->", "None", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "optimizer", ",", "\"momentum\"", ",", "last_epoch", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.momentum_schedulers.momentum_scheduler.MomentumScheduler.get_values": [[12, 14], ["None"], "methods", ["None"], ["", "def", "get_values", "(", "self", ")", "->", "None", ":", "\n", "        ", "raise", "NotImplementedError", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.momentum_schedulers.momentum_scheduler.MomentumScheduler.from_params": [[16, 24], ["params.pop_choice", "MomentumScheduler.list_available", "MomentumScheduler.by_name", "params.as_dict"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.params.pop_choice", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.registrable.Registrable.list_available", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.registrable.Registrable.by_name", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.params.Params.as_dict"], ["", "@", "classmethod", "\n", "def", "from_params", "(", "cls", ",", "optimizer", ":", "torch", ".", "optim", ".", "Optimizer", ",", "params", ":", "Params", ")", ":", "# type: ignore", "\n", "\n", "        ", "scheduler_type", "=", "params", ".", "pop_choice", "(", "\"type\"", ",", "MomentumScheduler", ".", "list_available", "(", ")", ")", "\n", "scheduler", "=", "MomentumScheduler", ".", "by_name", "(", "scheduler_type", ")", "(", "\n", "optimizer", ",", "**", "params", ".", "as_dict", "(", ")", "\n", ")", "\n", "return", "scheduler", "\n", "", "", ""]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.momentum_schedulers.inverted_triangular.InvertedTriangular.__init__": [[18, 30], ["allennlp.training.momentum_schedulers.momentum_scheduler.MomentumScheduler.__init__"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.graph_util.plot.GraphPlot.__init__"], ["def", "__init__", "(", "\n", "self", ",", "\n", "optimizer", ":", "torch", ".", "optim", ".", "Optimizer", ",", "\n", "cool_down", ":", "int", ",", "\n", "warm_up", ":", "int", ",", "\n", "ratio", ":", "int", "=", "10", ",", "\n", "last_epoch", ":", "int", "=", "-", "1", ",", "\n", ")", "->", "None", ":", "\n", "        ", "self", ".", "cool_down", "=", "cool_down", "\n", "self", ".", "warm_up", "=", "warm_up", "\n", "self", ".", "ratio", "=", "ratio", "\n", "super", "(", ")", ".", "__init__", "(", "optimizer", ",", "last_epoch", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.momentum_schedulers.inverted_triangular.InvertedTriangular.get_values": [[31, 48], ["None"], "methods", ["None"], ["", "def", "get_values", "(", "self", ")", ":", "\n", "        ", "step", "=", "self", ".", "last_epoch", "+", "1", "\n", "if", "step", "<=", "self", ".", "cool_down", ":", "\n", "            ", "values", "=", "[", "\n", "m", "-", "(", "m", "-", "m", "/", "self", ".", "ratio", ")", "*", "(", "step", "/", "self", ".", "cool_down", ")", "\n", "for", "m", "in", "self", ".", "base_values", "\n", "]", "\n", "", "elif", "step", "<=", "self", ".", "cool_down", "+", "self", ".", "warm_up", ":", "\n", "            ", "values", "=", "[", "\n", "(", "m", "/", "self", ".", "ratio", ")", "\n", "+", "(", "m", "-", "m", "/", "self", ".", "ratio", ")", "*", "(", "step", "-", "self", ".", "cool_down", ")", "/", "self", ".", "warm_up", "\n", "for", "m", "in", "self", ".", "base_values", "\n", "]", "\n", "", "else", ":", "\n", "            ", "values", "=", "self", ".", "base_values", "\n", "\n", "", "return", "values", "\n", "", "", ""]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.callbacks.update_moving_average.UpdateMovingAverage.__init__": [[24, 26], ["None"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "moving_average", ":", "MovingAverage", ")", "->", "None", ":", "\n", "        ", "self", ".", "moving_average", "=", "moving_average", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.callbacks.update_moving_average.UpdateMovingAverage.apply_moving_average": [[27, 30], ["allennlp.training.callbacks.callback.handle_event", "update_moving_average.UpdateMovingAverage.moving_average.apply"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.callbacks.callback.handle_event", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.training.moving_average.ExponentialMovingAverage.apply"], ["", "@", "handle_event", "(", "Events", ".", "BATCH_END", ",", "priority", "=", "-", "1000", ")", "\n", "def", "apply_moving_average", "(", "self", ",", "trainer", ":", "\"CallbackTrainer\"", ")", "->", "None", ":", "\n", "        ", "self", ".", "moving_average", ".", "apply", "(", "trainer", ".", "batch_num_total", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.callbacks.update_moving_average.UpdateMovingAverage.from_params": [[31, 47], ["params.pop", "allennlp.training.moving_average.MovingAverage.from_params", "update_moving_average.UpdateMovingAverage", "model.named_parameters"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.params.Params.pop", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.tokenizers.tokenizer.Tokenizer.from_params"], ["", "@", "classmethod", "\n", "def", "from_params", "(", "# type: ignore", "\n", "cls", ",", "params", ":", "Params", ",", "model", ":", "Model", ",", "**", "extras", "\n", ")", "->", "\"UpdateMovingAverage\"", ":", "\n", "\n", "        ", "moving_average_params", "=", "params", ".", "pop", "(", "\"moving_average\"", ")", "\n", "model_parameters", "=", "[", "\n", "[", "name", ",", "param", "]", "\n", "for", "name", ",", "param", "in", "model", ".", "named_parameters", "(", ")", "\n", "if", "param", ".", "requires_grad", "\n", "]", "\n", "moving_average", "=", "MovingAverage", ".", "from_params", "(", "\n", "params", "=", "moving_average_params", ",", "parameters", "=", "model_parameters", "\n", ")", "\n", "\n", "return", "UpdateMovingAverage", "(", "moving_average", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.callbacks.validate.Validate.__init__": [[34, 42], ["None"], "methods", ["None"], ["def", "__init__", "(", "\n", "self", ",", "validation_data", ":", "Iterable", "[", "Instance", "]", ",", "validation_iterator", ":", "DataIterator", "\n", ")", "->", "None", ":", "\n", "        ", "self", ".", "instances", "=", "validation_data", "\n", "self", ".", "iterator", "=", "validation_iterator", "\n", "\n", "# `MovingAverage`s used by the trainer.", "\n", "self", ".", "moving_averages", ":", "List", "[", "MovingAverage", "]", "=", "[", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.callbacks.validate.Validate.set_validate": [[43, 47], ["allennlp.training.callbacks.callback.handle_event"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.callbacks.callback.handle_event"], ["", "@", "handle_event", "(", "Events", ".", "TRAINING_START", ")", "\n", "def", "set_validate", "(", "self", ",", "trainer", ":", "\"CallbackTrainer\"", ")", ":", "\n", "\n", "        ", "trainer", ".", "validate", "=", "True", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.callbacks.validate.Validate.collect_moving_averages": [[48, 54], ["allennlp.training.callbacks.callback.handle_event", "getattr", "trainer.handler.callbacks", "hasattr"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.callbacks.callback.handle_event", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.callbacks.callback_handler.CallbackHandler.callbacks"], ["", "@", "handle_event", "(", "Events", ".", "TRAINING_START", ")", "\n", "def", "collect_moving_averages", "(", "self", ",", "trainer", ":", "\"CallbackTrainer\"", ")", ":", "\n", "        ", "self", ".", "moving_averages", "=", "[", "\n", "getattr", "(", "callback", ",", "\"moving_average\"", ")", "\n", "for", "callback", "in", "trainer", ".", "handler", ".", "callbacks", "(", ")", "\n", "if", "hasattr", "(", "callback", ",", "\"moving_average\"", ")", "\n", "]", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.callbacks.validate.Validate.validate": [[56, 100], ["allennlp.training.callbacks.callback.handle_event", "moving_average.assign_average_value", "torch.no_grad", "logger.info", "trainer.model.eval", "validate.Validate.iterator", "validate.Validate.iterator.get_num_batches", "allennlp.common.tqdm.Tqdm.tqdm", "allennlp.training.util.get_metrics", "moving_average.restore", "trainer.batch_loss", "allennlp.training.util.get_metrics", "allennlp.training.util.description_from_metrics", "allennlp.common.tqdm.Tqdm.tqdm.set_description", "trainer.batch_loss.detach().cpu().numpy", "trainer.batch_loss.detach().cpu", "trainer.batch_loss.detach"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.callbacks.callback.handle_event", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.training.moving_average.MovingAverage.assign_average_value", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.dataset_readers.multiprocess_dataset_reader.logger.info", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.iterators.data_iterator.DataIterator.get_num_batches", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.tqdm.Tqdm.tqdm", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.coreference_resolution.coref.CoreferenceResolver.get_metrics", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.training.moving_average.MovingAverage.restore", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.training.trainer.Trainer.batch_loss", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.coreference_resolution.coref.CoreferenceResolver.get_metrics", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.training.util.description_from_metrics"], ["", "@", "handle_event", "(", "Events", ".", "VALIDATE", ")", "\n", "def", "validate", "(", "self", ",", "trainer", ":", "\"CallbackTrainer\"", ")", ":", "\n", "# If the trainer has MovingAverage objects, use their weights for validation.", "\n", "        ", "for", "moving_average", "in", "self", ".", "moving_averages", ":", "\n", "            ", "moving_average", ".", "assign_average_value", "(", ")", "\n", "\n", "", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "# We have a validation set, so compute all the metrics on it.", "\n", "            ", "logger", ".", "info", "(", "\"Validating\"", ")", "\n", "\n", "trainer", ".", "model", ".", "eval", "(", ")", "\n", "\n", "val_generator", "=", "self", ".", "iterator", "(", "self", ".", "instances", ",", "num_epochs", "=", "1", ",", "shuffle", "=", "False", ")", "\n", "num_validation_batches", "=", "self", ".", "iterator", ".", "get_num_batches", "(", "self", ".", "instances", ")", "\n", "val_generator_tqdm", "=", "Tqdm", ".", "tqdm", "(", "val_generator", ",", "total", "=", "num_validation_batches", ")", "\n", "\n", "batches_this_epoch", "=", "0", "\n", "val_loss", "=", "0", "\n", "for", "batch", "in", "val_generator_tqdm", ":", "\n", "\n", "                ", "loss", "=", "trainer", ".", "batch_loss", "(", "batch", ",", "for_training", "=", "False", ")", "\n", "if", "loss", "is", "not", "None", ":", "\n", "# You shouldn't necessarily have to compute a loss for validation, so we allow for", "\n", "# `loss` to be None.  We need to be careful, though - `batches_this_epoch` is", "\n", "# currently only used as the divisor for the loss function, so we can safely only", "\n", "# count those batches for which we actually have a loss.  If this variable ever", "\n", "# gets used for something else, we might need to change things around a bit.", "\n", "                    ", "batches_this_epoch", "+=", "1", "\n", "val_loss", "+=", "loss", ".", "detach", "(", ")", ".", "cpu", "(", ")", ".", "numpy", "(", ")", "\n", "\n", "# Update the description with the latest metrics", "\n", "", "val_metrics", "=", "training_util", ".", "get_metrics", "(", "\n", "trainer", ".", "model", ",", "val_loss", ",", "batches_this_epoch", "\n", ")", "\n", "description", "=", "training_util", ".", "description_from_metrics", "(", "val_metrics", ")", "\n", "val_generator_tqdm", ".", "set_description", "(", "description", ",", "refresh", "=", "False", ")", "\n", "\n", "", "trainer", ".", "val_metrics", "=", "training_util", ".", "get_metrics", "(", "\n", "trainer", ".", "model", ",", "val_loss", ",", "batches_this_epoch", ",", "reset", "=", "True", "\n", ")", "\n", "\n", "# If the trainer has a moving average, restore", "\n", "", "for", "moving_average", "in", "self", ".", "moving_averages", ":", "\n", "            ", "moving_average", ".", "restore", "(", ")", "\n", "", "", "", ""]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.callbacks.post_to_url.PostToUrl.__init__": [[30, 35], ["None"], "methods", ["None"], ["def", "__init__", "(", "\n", "self", ",", "url", ":", "str", ",", "message", ":", "str", "=", "DEFAULT_MESSAGE", ",", "key", ":", "str", "=", "\"text\"", "\n", ")", "->", "None", ":", "\n", "        ", "self", ".", "url", "=", "url", "\n", "self", ".", "json", "=", "{", "key", ":", "message", "}", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.callbacks.post_to_url.PostToUrl.post_to_url": [[36, 40], ["allennlp.training.callbacks.callback.handle_event", "requests.post"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.callbacks.callback.handle_event"], ["", "@", "handle_event", "(", "Events", ".", "TRAINING_END", ")", "\n", "def", "post_to_url", "(", "self", ",", "trainer", ":", "\"CallbackTrainer\"", ")", ":", "\n", "\n", "        ", "requests", ".", "post", "(", "self", ".", "url", ",", "json", "=", "self", ".", "json", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.callbacks.log_to_tensorboard.LogToTensorboard.__init__": [[33, 44], ["set"], "methods", ["None"], ["def", "__init__", "(", "\n", "self", ",", "tensorboard", ":", "TensorboardWriter", ",", "log_batch_size_period", ":", "int", "=", "None", "\n", ")", "->", "None", ":", "\n", "        ", "self", ".", "log_batch_size_period", "=", "log_batch_size_period", "\n", "self", ".", "tensorboard", "=", "tensorboard", "\n", "\n", "self", ".", "cumulative_batch_size", "=", "0", "\n", "\n", "# For logging histograms", "\n", "self", ".", "histogram_parameters", ":", "Set", "[", "str", "]", "=", "set", "(", ")", "\n", "self", ".", "param_updates", ":", "Dict", "[", "str", ",", "torch", ".", "Tensor", "]", "=", "{", "}", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.callbacks.log_to_tensorboard.LogToTensorboard.training_start": [[45, 60], ["allennlp.training.callbacks.callback.handle_event", "set", "trainer.model.get_parameters_for_histogram_tensorboard_logging", "log_to_tensorboard.LogToTensorboard.tensorboard.enable_activation_logging"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.callbacks.callback.handle_event", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.models.model.Model.get_parameters_for_histogram_tensorboard_logging", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.training.tensorboard_writer.TensorboardWriter.enable_activation_logging"], ["", "@", "handle_event", "(", "Events", ".", "TRAINING_START", ")", "\n", "def", "training_start", "(", "self", ",", "trainer", ":", "\"CallbackTrainer\"", ")", ":", "\n", "# This is an ugly hack to get the tensorboard instance to know about the trainer, because", "\n", "# the callbacks are defined before the trainer.", "\n", "# TODO: figure out a better way to handle this.", "\n", "        ", "self", ".", "tensorboard", ".", "_get_batch_num_total", "=", "lambda", ":", "trainer", ".", "batch_num_total", "\n", "\n", "# Get histogram parameters", "\n", "self", ".", "histogram_parameters", "=", "set", "(", "\n", "trainer", ".", "model", ".", "get_parameters_for_histogram_tensorboard_logging", "(", ")", "\n", ")", "\n", "\n", "# Enable activation logging.", "\n", "if", "self", ".", "tensorboard", ".", "_histogram_interval", "is", "not", "None", ":", "\n", "            ", "self", ".", "tensorboard", ".", "enable_activation_logging", "(", "trainer", ".", "model", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.callbacks.log_to_tensorboard.LogToTensorboard.copy_current_parameters": [[61, 70], ["allennlp.training.callbacks.callback.handle_event", "log_to_tensorboard.LogToTensorboard.tensorboard.should_log_histograms_this_batch", "param.detach().cpu().clone", "trainer.model.named_parameters", "param.detach().cpu", "param.detach"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.callbacks.callback.handle_event", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.training.tensorboard_writer.TensorboardWriter.should_log_histograms_this_batch", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.nn.util.clone"], ["", "", "@", "handle_event", "(", "Events", ".", "BATCH_START", ")", "\n", "def", "copy_current_parameters", "(", "self", ",", "trainer", ":", "\"CallbackTrainer\"", ")", ":", "\n", "        ", "if", "self", ".", "tensorboard", ".", "should_log_histograms_this_batch", "(", ")", ":", "\n", "# Get the magnitude of parameter updates for logging", "\n", "# We need a copy of current parameters to compute magnitude of updates,", "\n", "# and copy them to CPU so large models won't go OOM on the GPU.", "\n", "            ", "self", ".", "param_updates", "=", "{", "\n", "name", ":", "param", ".", "detach", "(", ")", ".", "cpu", "(", ")", ".", "clone", "(", ")", "\n", "for", "name", ",", "param", "in", "trainer", ".", "model", ".", "named_parameters", "(", ")", "\n", "}", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.callbacks.log_to_tensorboard.LogToTensorboard.batch_end_logging": [[72, 109], ["allennlp.training.callbacks.callback.handle_event", "log_to_tensorboard.LogToTensorboard.tensorboard.should_log_this_batch", "log_to_tensorboard.LogToTensorboard.tensorboard.should_log_histograms_this_batch", "log_to_tensorboard.LogToTensorboard.tensorboard.log_parameter_and_gradient_statistics", "log_to_tensorboard.LogToTensorboard.tensorboard.log_learning_rates", "log_to_tensorboard.LogToTensorboard.tensorboard.add_train_scalar", "log_to_tensorboard.LogToTensorboard.tensorboard.log_metrics", "allennlp.training.util.get_batch_size", "trainer.model.named_parameters", "log_to_tensorboard.LogToTensorboard.param_updates.clear", "log_to_tensorboard.LogToTensorboard.tensorboard.log_histograms", "logger.debug", "log_to_tensorboard.LogToTensorboard.tensorboard.add_train_scalar", "log_to_tensorboard.LogToTensorboard.tensorboard.add_train_scalar", "log_to_tensorboard.LogToTensorboard.param_updates[].sub_", "torch.norm", "torch.norm().cpu", "log_to_tensorboard.LogToTensorboard.tensorboard.add_train_scalar", "param.detach().cpu", "log_to_tensorboard.LogToTensorboard.param_updates[].view", "trainer.train_metrics.items", "torch.norm", "param.detach", "param.view"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.callbacks.callback.handle_event", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.training.tensorboard_writer.TensorboardWriter.should_log_this_batch", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.training.tensorboard_writer.TensorboardWriter.should_log_histograms_this_batch", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.training.tensorboard_writer.TensorboardWriter.log_parameter_and_gradient_statistics", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.training.tensorboard_writer.TensorboardWriter.log_learning_rates", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.training.tensorboard_writer.TensorboardWriter.add_train_scalar", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.training.tensorboard_writer.TensorboardWriter.log_metrics", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.training.util.get_batch_size", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.training.metric_tracker.MetricTracker.clear", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.training.tensorboard_writer.TensorboardWriter.log_histograms", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.training.tensorboard_writer.TensorboardWriter.add_train_scalar", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.training.tensorboard_writer.TensorboardWriter.add_train_scalar", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.training.tensorboard_writer.TensorboardWriter.add_train_scalar"], ["", "", "@", "handle_event", "(", "Events", ".", "BATCH_END", ")", "\n", "def", "batch_end_logging", "(", "self", ",", "trainer", ":", "\"CallbackTrainer\"", ")", ":", "\n", "# Log parameter values to tensorboard", "\n", "        ", "if", "self", ".", "tensorboard", ".", "should_log_this_batch", "(", ")", ":", "\n", "            ", "self", ".", "tensorboard", ".", "log_parameter_and_gradient_statistics", "(", "\n", "trainer", ".", "model", ",", "trainer", ".", "batch_grad_norm", "\n", ")", "\n", "self", ".", "tensorboard", ".", "log_learning_rates", "(", "trainer", ".", "model", ",", "trainer", ".", "optimizer", ")", "\n", "\n", "self", ".", "tensorboard", ".", "add_train_scalar", "(", "\n", "\"loss/loss_train\"", ",", "trainer", ".", "train_metrics", "[", "\"loss\"", "]", "\n", ")", "\n", "self", ".", "tensorboard", ".", "log_metrics", "(", "\n", "{", "\"epoch_metrics/\"", "+", "k", ":", "v", "for", "k", ",", "v", "in", "trainer", ".", "train_metrics", ".", "items", "(", ")", "}", "\n", ")", "\n", "\n", "", "if", "self", ".", "log_batch_size_period", ":", "\n", "            ", "cur_batch", "=", "training_util", ".", "get_batch_size", "(", "trainer", ".", "batch", ")", "\n", "self", ".", "cumulative_batch_size", "+=", "cur_batch", "\n", "if", "(", "trainer", ".", "batches_this_epoch", "-", "1", ")", "%", "self", ".", "log_batch_size_period", "==", "0", ":", "\n", "                ", "average", "=", "self", ".", "cumulative_batch_size", "/", "trainer", ".", "batches_this_epoch", "\n", "logger", ".", "debug", "(", "\n", "f\"current batch size: {cur_batch} mean batch size: {average}\"", "\n", ")", "\n", "self", ".", "tensorboard", ".", "add_train_scalar", "(", "\"current_batch_size\"", ",", "cur_batch", ")", "\n", "self", ".", "tensorboard", ".", "add_train_scalar", "(", "\"mean_batch_size\"", ",", "average", ")", "\n", "\n", "", "", "if", "self", ".", "tensorboard", ".", "should_log_histograms_this_batch", "(", ")", ":", "\n", "            ", "for", "name", ",", "param", "in", "trainer", ".", "model", ".", "named_parameters", "(", ")", ":", "\n", "                ", "self", ".", "param_updates", "[", "name", "]", ".", "sub_", "(", "param", ".", "detach", "(", ")", ".", "cpu", "(", ")", ")", "\n", "update_norm", "=", "torch", ".", "norm", "(", "self", ".", "param_updates", "[", "name", "]", ".", "view", "(", "-", "1", ")", ")", "\n", "param_norm", "=", "torch", ".", "norm", "(", "param", ".", "view", "(", "-", "1", ")", ")", ".", "cpu", "(", ")", "\n", "self", ".", "tensorboard", ".", "add_train_scalar", "(", "\n", "\"gradient_update/\"", "+", "name", ",", "update_norm", "/", "(", "param_norm", "+", "1e-7", ")", "\n", ")", "\n", "", "self", ".", "param_updates", ".", "clear", "(", ")", "\n", "self", ".", "tensorboard", ".", "log_histograms", "(", "trainer", ".", "model", ",", "self", ".", "histogram_parameters", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.callbacks.log_to_tensorboard.LogToTensorboard.epoch_end_logging": [[110, 117], ["allennlp.training.callbacks.callback.handle_event", "log_to_tensorboard.LogToTensorboard.tensorboard.log_metrics"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.callbacks.callback.handle_event", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.training.tensorboard_writer.TensorboardWriter.log_metrics"], ["", "", "@", "handle_event", "(", "Events", ".", "EPOCH_END", ")", "\n", "def", "epoch_end_logging", "(", "self", ",", "trainer", ":", "\"CallbackTrainer\"", ")", ":", "\n", "        ", "self", ".", "tensorboard", ".", "log_metrics", "(", "\n", "trainer", ".", "train_metrics", ",", "\n", "val_metrics", "=", "trainer", ".", "val_metrics", ",", "\n", "log_to_console", "=", "True", ",", "\n", "epoch", "=", "trainer", ".", "epoch_number", "+", "1", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.callbacks.log_to_tensorboard.LogToTensorboard.training_end": [[119, 123], ["allennlp.training.callbacks.callback.handle_event", "log_to_tensorboard.LogToTensorboard.tensorboard.close"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.callbacks.callback.handle_event", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.token_embedders.embedding.EmbeddingsTextFile.close"], ["", "@", "handle_event", "(", "Events", ".", "TRAINING_END", ")", "\n", "def", "training_end", "(", "self", ",", "trainer", ":", "\"CallbackTrainer\"", ")", ":", "\n", "\n", "        ", "self", ".", "tensorboard", ".", "close", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.callbacks.log_to_tensorboard.LogToTensorboard.from_params": [[124, 137], ["params.pop_int", "allennlp.training.tensorboard_writer.TensorboardWriter.from_params", "log_to_tensorboard.LogToTensorboard"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.params.Params.pop_int", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.tokenizers.tokenizer.Tokenizer.from_params"], ["", "@", "classmethod", "\n", "def", "from_params", "(", "# type: ignore", "\n", "cls", ",", "serialization_dir", ":", "str", ",", "params", ":", "Params", ",", "**", "extras", "\n", ")", "->", "\"LogToTensorboard\"", ":", "\n", "        ", "log_batch_size_period", "=", "params", ".", "pop_int", "(", "\"log_batch_size_period\"", ",", "None", ")", "\n", "tensorboard", "=", "TensorboardWriter", ".", "from_params", "(", "\n", "params", "=", "params", ",", "\n", "serialization_dir", "=", "serialization_dir", ",", "\n", "get_batch_num_total", "=", "lambda", ":", "None", ",", "\n", ")", "\n", "# TODO(mattg): remove get_batch_num_total from TensorboardWriter, and instead just add a", "\n", "# method / arguments to tell the writer what batch num we're at.", "\n", "return", "LogToTensorboard", "(", "tensorboard", ",", "log_batch_size_period", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.callbacks.track_metrics.TrackMetrics.__init__": [[36, 51], ["allennlp.training.metric_tracker.MetricTracker", "allennlp.common.checks.ConfigurationError", "isinstance"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "patience", ":", "int", "=", "None", ",", "validation_metric", ":", "str", "=", "\"-loss\"", ")", "->", "None", ":", "\n", "        ", "if", "patience", "is", "not", "None", "and", "(", "not", "isinstance", "(", "patience", ",", "int", ")", "or", "patience", "<=", "0", ")", ":", "\n", "            ", "raise", "ConfigurationError", "(", "\n", "f\"patience must be a positive number, but got {patience}.\"", "\n", "f\"To disable early stopping, don't specify it.\"", "\n", ")", "\n", "\n", "", "self", ".", "patience", "=", "patience", "\n", "self", ".", "validation_metric", "=", "validation_metric", "[", "1", ":", "]", "\n", "self", ".", "metric_tracker", "=", "MetricTracker", "(", "patience", ",", "validation_metric", ")", "\n", "self", ".", "starting_epoch", "=", "0", "\n", "\n", "self", ".", "peak_cpu_usage", "=", "0.0", "\n", "# Track pairs (gpu_id, memory usage)", "\n", "self", ".", "gpu_usage", ":", "List", "[", "Tuple", "[", "int", ",", "int", "]", "]", "=", "[", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.callbacks.track_metrics.TrackMetrics.get_training_state": [[52, 57], ["track_metrics.TrackMetrics.metric_tracker.state_dict", "track_metrics.TrackMetrics.metric_tracker.is_best_so_far"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.learning_rate_schedulers.learning_rate_scheduler._PyTorchLearningRateSchedulerWrapper.state_dict", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.training.metric_tracker.MetricTracker.is_best_so_far"], ["", "def", "get_training_state", "(", "self", ")", "->", "dict", ":", "\n", "        ", "return", "{", "\n", "\"metric_tracker\"", ":", "self", ".", "metric_tracker", ".", "state_dict", "(", ")", ",", "\n", "# This is already in the metric_tracker state dict, but it makes our lives easier.", "\n", "\"is_best_so_far\"", ":", "self", ".", "metric_tracker", ".", "is_best_so_far", "(", ")", ",", "\n", "}", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.callbacks.track_metrics.TrackMetrics.restore_training_state": [[59, 64], ["training_state.pop", "track_metrics.TrackMetrics.metric_tracker.load_state_dict"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.params.Params.pop", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.learning_rate_schedulers.learning_rate_scheduler._PyTorchLearningRateSchedulerWrapper.load_state_dict"], ["", "def", "restore_training_state", "(", "self", ",", "training_state", ":", "dict", ")", "->", "None", ":", "\n", "        ", "state_dict", "=", "training_state", ".", "pop", "(", "\"metric_tracker\"", ",", "None", ")", "\n", "\n", "if", "state_dict", ":", "\n", "            ", "self", ".", "metric_tracker", ".", "load_state_dict", "(", "state_dict", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.callbacks.track_metrics.TrackMetrics.set_up_metrics": [[65, 79], ["allennlp.training.callbacks.callback.handle_event", "track_metrics.TrackMetrics.metric_tracker.best_epoch_metrics.items", "logger.warning"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.callbacks.callback.handle_event"], ["", "", "@", "handle_event", "(", "Events", ".", "TRAINING_START", ",", "priority", "=", "100", ")", "\n", "def", "set_up_metrics", "(", "self", ",", "trainer", ":", "\"CallbackTrainer\"", ")", ":", "\n", "# Keep track of starting epoch", "\n", "        ", "self", ".", "starting_epoch", "=", "trainer", ".", "epoch_number", "\n", "\n", "if", "self", ".", "patience", "is", "None", "and", "trainer", ".", "validate", ":", "\n", "            ", "logger", ".", "warning", "(", "\n", "\"You provided a validation dataset but patience was set to None, \"", "\n", "\"meaning that early stopping is disabled\"", "\n", ")", "\n", "\n", "", "trainer", ".", "metrics", "[", "\"best_epoch\"", "]", "=", "self", ".", "metric_tracker", ".", "best_epoch", "or", "0", "\n", "for", "key", ",", "value", "in", "self", ".", "metric_tracker", ".", "best_epoch_metrics", ".", "items", "(", ")", ":", "\n", "            ", "trainer", ".", "metrics", "[", "\"best_validation_\"", "+", "key", "]", "=", "value", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.callbacks.track_metrics.TrackMetrics.measure_cpu_gpu": [[80, 90], ["allennlp.training.callbacks.callback.handle_event", "logger.info", "allennlp.common.util.peak_memory_mb", "logger.info", "track_metrics.TrackMetrics.gpu_usage.clear", "allennlp.common.util.gpu_memory_mb().items", "track_metrics.TrackMetrics.gpu_usage.append", "logger.info", "allennlp.common.util.gpu_memory_mb"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.callbacks.callback.handle_event", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.dataset_readers.multiprocess_dataset_reader.logger.info", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.util.peak_memory_mb", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.dataset_readers.multiprocess_dataset_reader.logger.info", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.training.metric_tracker.MetricTracker.clear", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.dataset_readers.multiprocess_dataset_reader.logger.info", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.util.gpu_memory_mb"], ["", "", "@", "handle_event", "(", "Events", ".", "EPOCH_START", ",", "priority", "=", "100", ")", "\n", "def", "measure_cpu_gpu", "(", "self", ",", "trainer", ":", "\"CallbackTrainer\"", ")", ":", "\n", "# This used to be in train_epoch()", "\n", "        ", "logger", ".", "info", "(", "\"Epoch %d/%d\"", ",", "trainer", ".", "epoch_number", ",", "trainer", ".", "num_epochs", "-", "1", ")", "\n", "self", ".", "peak_cpu_usage", "=", "peak_memory_mb", "(", ")", "\n", "logger", ".", "info", "(", "f\"Peak CPU memory usage MB: {self.peak_cpu_usage}\"", ")", "\n", "self", ".", "gpu_usage", ".", "clear", "(", ")", "\n", "for", "gpu", ",", "memory", "in", "gpu_memory_mb", "(", ")", ".", "items", "(", ")", ":", "\n", "            ", "self", ".", "gpu_usage", ".", "append", "(", "(", "gpu", ",", "memory", ")", ")", "\n", "logger", ".", "info", "(", "f\"GPU {gpu} memory usage MB: {memory}\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.callbacks.track_metrics.TrackMetrics.collect_train_metrics": [[92, 111], ["allennlp.training.callbacks.callback.handle_event", "allennlp.training.util.get_metrics", "trainer.train_metrics.items", "max", "key.startswith", "trainer.metrics.get", "max", "trainer.metrics.get", "str"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.callbacks.callback.handle_event", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.coreference_resolution.coref.CoreferenceResolver.get_metrics", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.params.Params.get", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.params.Params.get"], ["", "", "@", "handle_event", "(", "Events", ".", "VALIDATE", ",", "priority", "=", "-", "100", ")", "\n", "def", "collect_train_metrics", "(", "self", ",", "trainer", ":", "\"CallbackTrainer\"", ")", ":", "\n", "        ", "trainer", ".", "train_metrics", "=", "training_util", ".", "get_metrics", "(", "\n", "trainer", ".", "model", ",", "trainer", ".", "train_loss", ",", "trainer", ".", "batches_this_epoch", ",", "reset", "=", "True", "\n", ")", "\n", "trainer", ".", "train_metrics", "[", "\"cpu_memory_MB\"", "]", "=", "self", ".", "peak_cpu_usage", "\n", "for", "(", "gpu_num", ",", "memory", ")", "in", "self", ".", "gpu_usage", ":", "\n", "            ", "trainer", ".", "train_metrics", "[", "\"gpu_\"", "+", "str", "(", "gpu_num", ")", "+", "\"_memory_MB\"", "]", "=", "memory", "\n", "\n", "# get peak of memory usage", "\n", "", "if", "\"cpu_memory_MB\"", "in", "trainer", ".", "train_metrics", ":", "\n", "            ", "trainer", ".", "metrics", "[", "\"peak_cpu_memory_MB\"", "]", "=", "max", "(", "\n", "trainer", ".", "metrics", ".", "get", "(", "\"peak_cpu_memory_MB\"", ",", "0", ")", ",", "\n", "trainer", ".", "train_metrics", "[", "\"cpu_memory_MB\"", "]", ",", "\n", ")", "\n", "", "for", "key", ",", "value", "in", "trainer", ".", "train_metrics", ".", "items", "(", ")", ":", "\n", "            ", "if", "key", ".", "startswith", "(", "\"gpu_\"", ")", ":", "\n", "                ", "trainer", ".", "metrics", "[", "\"peak_\"", "+", "key", "]", "=", "max", "(", "\n", "trainer", ".", "metrics", ".", "get", "(", "\"peak_\"", "+", "key", ",", "0", ")", ",", "value", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.callbacks.track_metrics.TrackMetrics.collect_val_metrics": [[114, 123], ["allennlp.training.callbacks.callback.handle_event", "track_metrics.TrackMetrics.metric_tracker.add_metric", "track_metrics.TrackMetrics.metric_tracker.should_stop_early"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.callbacks.callback.handle_event", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.training.metric_tracker.MetricTracker.add_metric", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.training.metric_tracker.MetricTracker.should_stop_early"], ["", "", "", "@", "handle_event", "(", "Events", ".", "VALIDATE", ",", "priority", "=", "100", ")", "\n", "def", "collect_val_metrics", "(", "self", ",", "trainer", ":", "\"CallbackTrainer\"", ")", ":", "\n", "        ", "if", "trainer", ".", "validate", ":", "\n", "# Check validation metric for early stopping", "\n", "            ", "trainer", ".", "latest_val_metric", "=", "trainer", ".", "val_metrics", "[", "self", ".", "validation_metric", "]", "\n", "self", ".", "metric_tracker", ".", "add_metric", "(", "trainer", ".", "latest_val_metric", ")", "\n", "\n", "if", "self", ".", "metric_tracker", ".", "should_stop_early", "(", ")", ":", "\n", "                ", "trainer", ".", "should_stop_early", "=", "True", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.callbacks.track_metrics.TrackMetrics.end_of_epoch": [[124, 158], ["allennlp.training.callbacks.callback.handle_event", "str", "trainer.train_metrics.items", "trainer.val_metrics.items", "track_metrics.TrackMetrics.metric_tracker.is_best_so_far", "time.time", "datetime.timedelta", "trainer.val_metrics.items", "copy.deepcopy", "allennlp.common.util.dump_metrics", "os.path.join"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.callbacks.callback.handle_event", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.training.metric_tracker.MetricTracker.is_best_so_far", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.util.dump_metrics", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.dataset_readers.multiprocess_dataset_reader.QIterable.join"], ["", "", "", "@", "handle_event", "(", "Events", ".", "EPOCH_END", ",", "priority", "=", "100", ")", "\n", "def", "end_of_epoch", "(", "self", ",", "trainer", ":", "\"CallbackTrainer\"", ")", ":", "\n", "# Create overall metrics dict", "\n", "        ", "training_elapsed_time", "=", "time", ".", "time", "(", ")", "-", "trainer", ".", "training_start_time", "\n", "trainer", ".", "metrics", "[", "\"training_duration\"", "]", "=", "str", "(", "\n", "datetime", ".", "timedelta", "(", "seconds", "=", "training_elapsed_time", ")", "\n", ")", "\n", "trainer", ".", "metrics", "[", "\"training_start_epoch\"", "]", "=", "self", ".", "starting_epoch", "\n", "trainer", ".", "metrics", "[", "\"training_epochs\"", "]", "=", "(", "\n", "trainer", ".", "epoch_number", "-", "self", ".", "starting_epoch", "+", "1", "\n", ")", "\n", "trainer", ".", "metrics", "[", "\"epoch\"", "]", "=", "trainer", ".", "epoch_number", "\n", "\n", "for", "key", ",", "value", "in", "trainer", ".", "train_metrics", ".", "items", "(", ")", ":", "\n", "            ", "trainer", ".", "metrics", "[", "\"training_\"", "+", "key", "]", "=", "value", "\n", "", "for", "key", ",", "value", "in", "trainer", ".", "val_metrics", ".", "items", "(", ")", ":", "\n", "            ", "trainer", ".", "metrics", "[", "\"validation_\"", "+", "key", "]", "=", "value", "\n", "\n", "", "if", "self", ".", "metric_tracker", ".", "is_best_so_far", "(", ")", ":", "\n", "# Update all the best_ metrics.", "\n", "# (Otherwise they just stay the same as they were.)", "\n", "            ", "trainer", ".", "metrics", "[", "\"best_epoch\"", "]", "=", "trainer", ".", "epoch_number", "\n", "for", "key", ",", "value", "in", "trainer", ".", "val_metrics", ".", "items", "(", ")", ":", "\n", "                ", "trainer", ".", "metrics", "[", "\"best_validation_\"", "+", "key", "]", "=", "value", "\n", "\n", "", "self", ".", "metric_tracker", ".", "best_epoch_metrics", "=", "copy", ".", "deepcopy", "(", "trainer", ".", "val_metrics", ")", "\n", "\n", "", "if", "trainer", ".", "_serialization_dir", ":", "\n", "            ", "dump_metrics", "(", "\n", "os", ".", "path", ".", "join", "(", "\n", "trainer", ".", "_serialization_dir", ",", "\n", "f\"metrics_epoch_{trainer.epoch_number}.json\"", ",", "\n", ")", ",", "\n", "trainer", ".", "metrics", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.callbacks.update_momentum.UpdateMomentum.__init__": [[25, 27], ["None"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "momentum_scheduler", ":", "MomentumScheduler", ")", "->", "None", ":", "\n", "        ", "self", ".", "momentum_scheduler", "=", "momentum_scheduler", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.callbacks.update_momentum.UpdateMomentum.step_batch": [[28, 31], ["allennlp.training.callbacks.callback.handle_event", "update_momentum.UpdateMomentum.momentum_scheduler.step_batch"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.callbacks.callback.handle_event", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.learning_rate_schedulers.slanted_triangular.SlantedTriangular.step_batch"], ["", "@", "handle_event", "(", "Events", ".", "BACKWARD", ",", "priority", "=", "1000", ")", "\n", "def", "step_batch", "(", "self", ",", "trainer", ":", "\"CallbackTrainer\"", ")", ":", "\n", "        ", "self", ".", "momentum_scheduler", ".", "step_batch", "(", "trainer", ".", "batch_num_total", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.callbacks.update_momentum.UpdateMomentum.step": [[32, 35], ["allennlp.training.callbacks.callback.handle_event", "update_momentum.UpdateMomentum.momentum_scheduler.step"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.callbacks.callback.handle_event", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.learning_rate_schedulers.slanted_triangular.SlantedTriangular.step"], ["", "@", "handle_event", "(", "Events", ".", "EPOCH_END", ")", "\n", "def", "step", "(", "self", ",", "trainer", ":", "\"CallbackTrainer\"", ")", ":", "\n", "        ", "self", ".", "momentum_scheduler", ".", "step", "(", "trainer", ".", "latest_val_metric", ",", "trainer", ".", "epoch_number", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.callbacks.update_momentum.UpdateMomentum.get_training_state": [[36, 38], ["update_momentum.UpdateMomentum.momentum_scheduler.state_dict"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.learning_rate_schedulers.learning_rate_scheduler._PyTorchLearningRateSchedulerWrapper.state_dict"], ["", "def", "get_training_state", "(", "self", ")", "->", "dict", ":", "\n", "        ", "return", "{", "\"momentum_scheduler\"", ":", "self", ".", "momentum_scheduler", ".", "state_dict", "(", ")", "}", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.callbacks.update_momentum.UpdateMomentum.restore_training_state": [[39, 44], ["training_state.pop", "update_momentum.UpdateMomentum.momentum_scheduler.load_state_dict"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.params.Params.pop", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.learning_rate_schedulers.learning_rate_scheduler._PyTorchLearningRateSchedulerWrapper.load_state_dict"], ["", "def", "restore_training_state", "(", "self", ",", "training_state", ":", "dict", ")", "->", "None", ":", "\n", "        ", "state_dict", "=", "training_state", ".", "pop", "(", "\"momentum_scheduler\"", ",", "None", ")", "\n", "\n", "if", "state_dict", ":", "\n", "            ", "self", ".", "momentum_scheduler", ".", "load_state_dict", "(", "state_dict", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.callbacks.update_momentum.UpdateMomentum.from_params": [[45, 53], ["cls", "allennlp.training.momentum_schedulers.MomentumScheduler.from_params", "params.pop"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.tokenizers.tokenizer.Tokenizer.from_params", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.params.Params.pop"], ["", "", "@", "classmethod", "\n", "def", "from_params", "(", "# type: ignore", "\n", "cls", ",", "params", ":", "Params", ",", "optimizer", ":", "torch", ".", "optim", ".", "Optimizer", ",", "**", "extras", "\n", ")", "->", "\"UpdateMomentum\"", ":", "\n", "\n", "        ", "return", "cls", "(", "\n", "MomentumScheduler", ".", "from_params", "(", "\n", "params", "=", "params", ".", "pop", "(", "\"momentum_scheduler\"", ")", ",", "optimizer", "=", "optimizer", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.callbacks.callback.Callback.get_training_state": [[40, 51], ["None"], "methods", ["None"], ["def", "get_training_state", "(", "self", ")", "->", "dict", ":", "\n", "        ", "\"\"\"\n        If this callback contains state that should be checkpointed for training,\n        return it here (with a key that's unique to this callback).\n        If the state lives in a pytorch object with a `state_dict`\n        method, this should return the output of `state_dict()`, not the object itself.\n\n        This default implementation suffices when there's no state to checkpoint.\n        \"\"\"", "\n", "\n", "return", "{", "}", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.callbacks.callback.Callback.restore_training_state": [[52, 60], ["None"], "methods", ["None"], ["", "def", "restore_training_state", "(", "self", ",", "training_state", ":", "dict", ")", "->", "None", ":", "\n", "        ", "\"\"\"\n        Given a dict of training state, pull out the relevant parts\n        and rehydrate the state of this callback however is necessary.\n\n        This default implementation suffices when there's no state to restore.\n        \"\"\"", "\n", "pass", "\n", "", "", ""]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.callbacks.callback.handle_event": [[6, 13], ["setattr", "setattr"], "function", ["None"], ["def", "handle_event", "(", "event", ":", "str", ",", "priority", ":", "int", "=", "0", ")", ":", "\n", "    ", "def", "wrapper", "(", "method", ":", "Callable", "[", "[", "]", ",", "None", "]", ")", ":", "\n", "        ", "setattr", "(", "method", ",", "\"_event\"", ",", "event", ")", "\n", "setattr", "(", "method", ",", "\"_priority\"", ",", "priority", ")", "\n", "return", "method", "\n", "\n", "", "return", "wrapper", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.callbacks.checkpoint.Checkpoint.__init__": [[38, 53], ["time.time"], "methods", ["None"], ["def", "__init__", "(", "\n", "self", ",", "\n", "checkpointer", ":", "Checkpointer", ",", "\n", "model_save_interval", ":", "Optional", "[", "float", "]", "=", "None", ",", "\n", "state_dict_attrs", ":", "List", "[", "str", "]", "=", "None", ",", "\n", "other_attrs", ":", "List", "[", "str", "]", "=", "None", ",", "\n", ")", "->", "None", ":", "\n", "        ", "self", ".", "checkpointer", "=", "checkpointer", "\n", "self", ".", "model_save_interval", "=", "model_save_interval", "\n", "self", ".", "state_dict_attrs", "=", "state_dict_attrs", "or", "[", "\"optimizer\"", "]", "\n", "self", ".", "other_attrs", "=", "other_attrs", "or", "[", "\"batch_num_total\"", "]", "\n", "self", ".", "last_save_time", "=", "time", ".", "time", "(", ")", "\n", "\n", "# `MovingAverage`s used by the trainer.", "\n", "self", ".", "moving_averages", ":", "List", "[", "MovingAverage", "]", "=", "[", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.callbacks.checkpoint.Checkpoint._should_save_at_batch_end": [[54, 58], ["time.time"], "methods", ["None"], ["", "def", "_should_save_at_batch_end", "(", "self", ")", "->", "bool", ":", "\n", "        ", "return", "(", "\n", "self", ".", "model_save_interval", "is", "not", "None", "\n", "and", "time", ".", "time", "(", ")", "-", "self", ".", "last_save_time", ">", "self", ".", "model_save_interval", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.callbacks.checkpoint.Checkpoint.collect_moving_averages": [[60, 66], ["allennlp.training.callbacks.callback.handle_event", "getattr", "trainer.handler.callbacks", "hasattr"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.callbacks.callback.handle_event", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.callbacks.callback_handler.CallbackHandler.callbacks"], ["", "@", "handle_event", "(", "Events", ".", "TRAINING_START", ")", "\n", "def", "collect_moving_averages", "(", "self", ",", "trainer", ":", "\"CallbackTrainer\"", ")", ":", "\n", "        ", "self", ".", "moving_averages", "=", "[", "\n", "getattr", "(", "callback", ",", "\"moving_average\"", ")", "\n", "for", "callback", "in", "trainer", ".", "handler", ".", "callbacks", "(", ")", "\n", "if", "hasattr", "(", "callback", ",", "\"moving_average\"", ")", "\n", "]", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.callbacks.checkpoint.Checkpoint.save_model_at_batch_end": [[68, 74], ["allennlp.training.callbacks.callback.handle_event", "checkpoint.Checkpoint._should_save_at_batch_end", "time.time", "checkpoint.Checkpoint._save_checkpoint", "allennlp.training.util.time_to_str", "int"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.callbacks.callback.handle_event", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.callbacks.checkpoint.Checkpoint._should_save_at_batch_end", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.callbacks.checkpoint.Checkpoint._save_checkpoint", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.training.util.time_to_str"], ["", "@", "handle_event", "(", "Events", ".", "BATCH_END", ")", "\n", "def", "save_model_at_batch_end", "(", "self", ",", "trainer", ":", "\"CallbackTrainer\"", ")", ":", "\n", "        ", "if", "self", ".", "_should_save_at_batch_end", "(", ")", ":", "\n", "            ", "self", ".", "last_save_time", "=", "time", ".", "time", "(", ")", "\n", "epoch", "=", "f\"{trainer.epoch_number}.{training_util.time_to_str(int(self.last_save_time))}\"", "\n", "self", ".", "_save_checkpoint", "(", "epoch", ",", "trainer", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.callbacks.checkpoint.Checkpoint.save_model_at_epoch_end": [[75, 78], ["allennlp.training.callbacks.callback.handle_event", "checkpoint.Checkpoint._save_checkpoint"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.callbacks.callback.handle_event", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.callbacks.checkpoint.Checkpoint._save_checkpoint"], ["", "", "@", "handle_event", "(", "Events", ".", "EPOCH_END", ",", "priority", "=", "1000", ")", "\n", "def", "save_model_at_epoch_end", "(", "self", ",", "trainer", ":", "\"CallbackTrainer\"", ")", ":", "\n", "        ", "self", ".", "_save_checkpoint", "(", "f\"{trainer.epoch_number}\"", ",", "trainer", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.callbacks.checkpoint.Checkpoint._save_checkpoint": [[79, 111], ["trainer.handler.callbacks", "training_states.pop", "checkpoint.Checkpoint.checkpointer.save_checkpoint", "moving_average.assign_average_value", "getattr", "getattr", "training_states.update", "moving_average.restore", "getattr.state_dict", "callback.get_training_state", "trainer.model.state_dict"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.callbacks.callback_handler.CallbackHandler.callbacks", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.params.Params.pop", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.training.checkpointer.Checkpointer.save_checkpoint", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.training.moving_average.MovingAverage.assign_average_value", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.metrics.conll_coref_scores.Scorer.update", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.training.moving_average.MovingAverage.restore", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.learning_rate_schedulers.learning_rate_scheduler._PyTorchLearningRateSchedulerWrapper.state_dict", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.callbacks.update_learning_rate.UpdateLearningRate.get_training_state", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.learning_rate_schedulers.learning_rate_scheduler._PyTorchLearningRateSchedulerWrapper.state_dict"], ["", "def", "_save_checkpoint", "(", "self", ",", "epoch", ":", "str", ",", "trainer", ":", "\"CallbackTrainer\"", ")", ":", "\n", "# If the trainer has MovingAverage objects, use their weights for checkpointing.", "\n", "        ", "for", "moving_average", "in", "self", ".", "moving_averages", ":", "\n", "            ", "moving_average", ".", "assign_average_value", "(", ")", "\n", "\n", "", "training_states", "=", "{", "}", "\n", "\n", "# Add state_dict attributes", "\n", "for", "attr", "in", "self", ".", "state_dict_attrs", ":", "\n", "            ", "state_attr", "=", "getattr", "(", "trainer", ",", "attr", ")", "\n", "if", "state_attr", "is", "not", "None", ":", "\n", "                ", "training_states", "[", "attr", "]", "=", "state_attr", ".", "state_dict", "(", ")", "\n", "\n", "# Add other attributes", "\n", "", "", "for", "attr", "in", "self", ".", "other_attrs", ":", "\n", "            ", "training_states", "[", "attr", "]", "=", "getattr", "(", "trainer", ",", "attr", ")", "\n", "\n", "# Get attributes from callbacks", "\n", "", "for", "callback", "in", "trainer", ".", "handler", ".", "callbacks", "(", ")", ":", "\n", "            ", "training_states", ".", "update", "(", "callback", ".", "get_training_state", "(", ")", ")", "\n", "\n", "", "is_best_so_far", "=", "training_states", ".", "pop", "(", "\"is_best_so_far\"", ",", "True", ")", "\n", "self", ".", "checkpointer", ".", "save_checkpoint", "(", "\n", "model_state", "=", "trainer", ".", "model", ".", "state_dict", "(", ")", ",", "\n", "epoch", "=", "epoch", ",", "\n", "training_states", "=", "training_states", ",", "\n", "is_best_so_far", "=", "is_best_so_far", ",", "\n", ")", "\n", "\n", "# If the trainer has a moving average, restore.", "\n", "for", "moving_average", "in", "self", ".", "moving_averages", ":", "\n", "            ", "moving_average", ".", "restore", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.callbacks.checkpoint.Checkpoint.restore_checkpoint": [[112, 158], ["allennlp.training.callbacks.callback.handle_event", "trainer.model.load_state_dict", "trainer.handler.callbacks", "isinstance", "checkpoint.Checkpoint.checkpointer.restore_checkpoint", "getattr", "setattr", "callback.restore_training_state", "traceback.print_exc", "allennlp.common.checks.ConfigurationError", "getattr.load_state_dict", "int", "training_state[].split"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.callbacks.callback.handle_event", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.learning_rate_schedulers.learning_rate_scheduler._PyTorchLearningRateSchedulerWrapper.load_state_dict", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.callbacks.callback_handler.CallbackHandler.callbacks", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.callbacks.checkpoint.Checkpoint.restore_checkpoint", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.callbacks.update_learning_rate.UpdateLearningRate.restore_training_state", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.learning_rate_schedulers.learning_rate_scheduler._PyTorchLearningRateSchedulerWrapper.load_state_dict"], ["", "", "@", "handle_event", "(", "Events", ".", "TRAINING_START", ")", "\n", "def", "restore_checkpoint", "(", "self", ",", "trainer", ":", "\"CallbackTrainer\"", ")", ":", "\n", "# Restores the model and training state from the last saved checkpoint.", "\n", "# This includes an epoch count and optimizer state, which is serialized separately", "\n", "# from model parameters. This function should only be used to continue training -", "\n", "# if you wish to load a model for inference/load parts of a model into a new", "\n", "# computation graph, you should use the native Pytorch functions:", "\n", "# ` model.load_state_dict(torch.load(\"/path/to/model/weights.th\"))`", "\n", "\n", "# If `self._serialization_dir` does not exist or does not contain any checkpointed weights,", "\n", "# this will do nothing.", "\n", "        ", "try", ":", "\n", "            ", "model_state", ",", "training_state", "=", "self", ".", "checkpointer", ".", "restore_checkpoint", "(", ")", "\n", "", "except", "RuntimeError", ":", "\n", "            ", "traceback", ".", "print_exc", "(", ")", "\n", "raise", "ConfigurationError", "(", "\n", "\"Could not recover training from the checkpoint.  \"", "\n", "\"Did you mean to output to a different serialization directory \"", "\n", "\"or delete the existing serialization directory?\"", "\n", ")", "\n", "\n", "", "if", "not", "training_state", ":", "\n", "# No checkpoint to restore, start at 0", "\n", "            ", "trainer", ".", "epoch_number", "=", "0", "\n", "return", "\n", "\n", "", "trainer", ".", "model", ".", "load_state_dict", "(", "model_state", ")", "\n", "\n", "# Restore state_dict attrs", "\n", "for", "attr", "in", "self", ".", "state_dict_attrs", ":", "\n", "            ", "state_attr", "=", "getattr", "(", "trainer", ",", "attr", ")", "\n", "if", "state_attr", "is", "not", "None", ":", "\n", "                ", "state_attr", ".", "load_state_dict", "(", "training_state", "[", "attr", "]", ")", "\n", "\n", "# Restore other attrs", "\n", "", "", "for", "attr", "in", "self", ".", "other_attrs", ":", "\n", "            ", "setattr", "(", "trainer", ",", "attr", ",", "training_state", "[", "attr", "]", ")", "\n", "\n", "# Restore callback attrs", "\n", "", "for", "callback", "in", "trainer", ".", "handler", ".", "callbacks", "(", ")", ":", "\n", "            ", "callback", ".", "restore_training_state", "(", "training_state", ")", "\n", "\n", "", "if", "isinstance", "(", "training_state", "[", "\"epoch\"", "]", ",", "int", ")", ":", "\n", "            ", "trainer", ".", "epoch_number", "=", "training_state", "[", "\"epoch\"", "]", "+", "1", "\n", "", "else", ":", "\n", "            ", "trainer", ".", "epoch_number", "=", "int", "(", "training_state", "[", "\"epoch\"", "]", ".", "split", "(", "\".\"", ")", "[", "0", "]", ")", "+", "1", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.callbacks.checkpoint.Checkpoint.load_best_model_state": [[159, 165], ["allennlp.training.callbacks.callback.handle_event", "checkpoint.Checkpoint.checkpointer.best_model_state", "trainer.model.load_state_dict"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.callbacks.callback.handle_event", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.training.checkpointer.Checkpointer.best_model_state", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.learning_rate_schedulers.learning_rate_scheduler._PyTorchLearningRateSchedulerWrapper.load_state_dict"], ["", "", "@", "handle_event", "(", "Events", ".", "TRAINING_END", ")", "\n", "def", "load_best_model_state", "(", "self", ",", "trainer", ":", "\"CallbackTrainer\"", ")", ":", "\n", "# Load the best model state before returning", "\n", "        ", "best_model_state", "=", "self", ".", "checkpointer", ".", "best_model_state", "(", ")", "\n", "if", "best_model_state", ":", "\n", "            ", "trainer", ".", "model", ".", "load_state_dict", "(", "best_model_state", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.callbacks.checkpoint.Checkpoint.from_params": [[166, 183], ["params.pop", "params.pop", "params.pop", "checkpoint.Checkpoint", "allennlp.training.checkpointer.Checkpointer.from_params", "allennlp.training.checkpointer.Checkpointer"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.params.Params.pop", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.params.Params.pop", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.params.Params.pop", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.tokenizers.tokenizer.Tokenizer.from_params"], ["", "", "@", "classmethod", "\n", "def", "from_params", "(", "# type: ignore", "\n", "cls", ",", "params", ":", "Params", ",", "serialization_dir", ":", "str", ",", "**", "extras", "\n", ")", "->", "\"Checkpoint\"", ":", "\n", "\n", "        ", "checkpointer_params", "=", "params", ".", "pop", "(", "\"checkpointer\"", ",", "None", ")", "\n", "if", "checkpointer_params", ":", "\n", "            ", "checkpointer", "=", "Checkpointer", ".", "from_params", "(", "\n", "checkpointer_params", ",", "serialization_dir", "=", "serialization_dir", "\n", ")", "\n", "", "else", ":", "\n", "            ", "checkpointer", "=", "Checkpointer", "(", "serialization_dir", "=", "serialization_dir", ")", "\n", "\n", "", "state_dict_attrs", "=", "params", ".", "pop", "(", "\"state_dict_attrs\"", ",", "None", ")", "\n", "other_attrs", "=", "params", ".", "pop", "(", "\"other_attrs\"", ",", "None", ")", "\n", "\n", "return", "Checkpoint", "(", "checkpointer", ",", "state_dict_attrs", ",", "other_attrs", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.callbacks.callback_handler.CallbackHandler.__init__": [[48, 61], ["collections.defaultdict", "collections.defaultdict", "callback_handler.CallbackHandler.add_callback"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.callbacks.callback_handler.CallbackHandler.add_callback"], ["def", "__init__", "(", "\n", "self", ",", "callbacks", ":", "Iterable", "[", "Callback", "]", ",", "state", ":", "TrainerBase", ",", "verbose", ":", "bool", "=", "False", "\n", ")", "->", "None", ":", "\n", "# Set up callbacks", "\n", "        ", "self", ".", "_callbacks", ":", "Dict", "[", "str", ",", "List", "[", "EventHandler", "]", "]", "=", "defaultdict", "(", "list", ")", "\n", "\n", "# This is just so we can find specific types of callbacks.", "\n", "self", ".", "_callbacks_by_type", ":", "Dict", "[", "type", ",", "List", "[", "Callback", "]", "]", "=", "defaultdict", "(", "list", ")", "\n", "self", ".", "state", "=", "state", "\n", "self", ".", "verbose", "=", "verbose", "\n", "\n", "for", "callback", "in", "callbacks", ":", "\n", "            ", "self", ".", "add_callback", "(", "callback", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.callbacks.callback_handler.CallbackHandler.callbacks": [[62, 75], ["list", "callback_handler.CallbackHandler._callbacks.values"], "methods", ["None"], ["", "", "def", "callbacks", "(", "self", ")", "->", "List", "[", "Callback", "]", ":", "\n", "\n", "        ", "\"\"\"\n        Returns the callbacks associated with this handler.\n        Each callback may be registered under multiple events,\n        but we make sure to only return it once. If `typ` is specified,\n        only returns callbacks of that type.\n        \"\"\"", "\n", "return", "list", "(", "\n", "{", "\n", "callback", ".", "callback", "\n", "for", "callback_list", "in", "self", ".", "_callbacks", ".", "values", "(", ")", "\n", "for", "callback", "in", "callback_list", "\n", "}", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.callbacks.callback_handler.CallbackHandler.add_callback": [[78, 88], ["inspect.getmembers", "getattr", "getattr", "callback_handler.CallbackHandler._callbacks[].append", "callback_handler.CallbackHandler._callbacks[].sort", "callback_handler.CallbackHandler._callbacks_by_type[].append", "callback_handler.EventHandler", "type"], "methods", ["None"], ["", "def", "add_callback", "(", "self", ",", "callback", ":", "Callback", ")", "->", "None", ":", "\n", "        ", "for", "name", ",", "method", "in", "inspect", ".", "getmembers", "(", "callback", ",", "_is_event_handler", ")", ":", "\n", "            ", "event", "=", "getattr", "(", "method", ",", "\"_event\"", ")", "\n", "priority", "=", "getattr", "(", "method", ",", "\"_priority\"", ")", "\n", "self", ".", "_callbacks", "[", "event", "]", ".", "append", "(", "\n", "EventHandler", "(", "name", ",", "callback", ",", "method", ",", "priority", ")", "\n", ")", "\n", "self", ".", "_callbacks", "[", "event", "]", ".", "sort", "(", "key", "=", "lambda", "eh", ":", "eh", ".", "priority", ")", "\n", "\n", "self", ".", "_callbacks_by_type", "[", "type", "(", "callback", ")", "]", ".", "append", "(", "callback", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.callbacks.callback_handler.CallbackHandler.fire_event": [[89, 98], ["callback_handler.CallbackHandler._callbacks.get", "event_handler.handler", "logger.info"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.params.Params.get", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.dataset_readers.multiprocess_dataset_reader.logger.info"], ["", "", "def", "fire_event", "(", "self", ",", "event", ":", "str", ")", "->", "None", ":", "\n", "        ", "\"\"\"\n        Runs every callback registered for the provided event,\n        ordered by their priorities.\n        \"\"\"", "\n", "for", "event_handler", "in", "self", ".", "_callbacks", ".", "get", "(", "event", ",", "[", "]", ")", ":", "\n", "            ", "if", "self", ".", "verbose", ":", "\n", "                ", "logger", ".", "info", "(", "f\"event {event} -> {event_handler.name}\"", ")", "\n", "", "event_handler", ".", "handler", "(", "self", ".", "state", ")", "\n", "", "", "", ""]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.callbacks.callback_handler._is_event_handler": [[19, 24], ["inspect.ismethod", "hasattr", "hasattr"], "function", ["None"], ["", "def", "_is_event_handler", "(", "member", ")", "->", "bool", ":", "\n", "    ", "return", "(", "\n", "inspect", ".", "ismethod", "(", "member", ")", "\n", "and", "hasattr", "(", "member", ",", "\"_event\"", ")", "\n", "and", "hasattr", "(", "member", ",", "\"_priority\"", ")", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.callbacks.update_learning_rate.UpdateLearningRate.__init__": [[25, 27], ["None"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "learning_rate_scheduler", ":", "LearningRateScheduler", ")", "->", "None", ":", "\n", "        ", "self", ".", "learning_rate_scheduler", "=", "learning_rate_scheduler", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.callbacks.update_learning_rate.UpdateLearningRate.step_batch": [[28, 31], ["allennlp.training.callbacks.callback.handle_event", "update_learning_rate.UpdateLearningRate.learning_rate_scheduler.step_batch"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.callbacks.callback.handle_event", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.learning_rate_schedulers.slanted_triangular.SlantedTriangular.step_batch"], ["", "@", "handle_event", "(", "Events", ".", "BACKWARD", ",", "priority", "=", "1000", ")", "\n", "def", "step_batch", "(", "self", ",", "trainer", ":", "\"CallbackTrainer\"", ")", ":", "\n", "        ", "self", ".", "learning_rate_scheduler", ".", "step_batch", "(", "trainer", ".", "batch_num_total", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.callbacks.update_learning_rate.UpdateLearningRate.step": [[32, 36], ["allennlp.training.callbacks.callback.handle_event", "update_learning_rate.UpdateLearningRate.learning_rate_scheduler.step"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.callbacks.callback.handle_event", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.learning_rate_schedulers.slanted_triangular.SlantedTriangular.step"], ["", "@", "handle_event", "(", "Events", ".", "EPOCH_END", ")", "\n", "def", "step", "(", "self", ",", "trainer", ":", "\"CallbackTrainer\"", ")", ":", "\n", "        ", "self", ".", "learning_rate_scheduler", ".", "step", "(", "\n", "trainer", ".", "latest_val_metric", ",", "trainer", ".", "epoch_number", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.callbacks.update_learning_rate.UpdateLearningRate.get_training_state": [[38, 43], ["update_learning_rate.UpdateLearningRate.learning_rate_scheduler.state_dict"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.learning_rate_schedulers.learning_rate_scheduler._PyTorchLearningRateSchedulerWrapper.state_dict"], ["", "def", "get_training_state", "(", "self", ")", "->", "dict", ":", "\n", "        ", "\"\"\"\n        We need to persist the learning_rate_scheduler state as training state.\n        \"\"\"", "\n", "return", "{", "\"learning_rate_scheduler\"", ":", "self", ".", "learning_rate_scheduler", ".", "state_dict", "(", ")", "}", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.callbacks.update_learning_rate.UpdateLearningRate.restore_training_state": [[44, 49], ["training_state.pop", "update_learning_rate.UpdateLearningRate.learning_rate_scheduler.load_state_dict"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.params.Params.pop", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.learning_rate_schedulers.learning_rate_scheduler._PyTorchLearningRateSchedulerWrapper.load_state_dict"], ["", "def", "restore_training_state", "(", "self", ",", "training_state", ":", "dict", ")", "->", "None", ":", "\n", "        ", "state_dict", "=", "training_state", ".", "pop", "(", "\"learning_rate_scheduler\"", ",", "None", ")", "\n", "\n", "if", "state_dict", ":", "\n", "            ", "self", ".", "learning_rate_scheduler", ".", "load_state_dict", "(", "state_dict", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.callbacks.update_learning_rate.UpdateLearningRate.from_params": [[50, 58], ["cls", "allennlp.training.learning_rate_schedulers.LearningRateScheduler.from_params", "params.pop"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.tokenizers.tokenizer.Tokenizer.from_params", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.params.Params.pop"], ["", "", "@", "classmethod", "\n", "def", "from_params", "(", "# type: ignore", "\n", "cls", ",", "params", ":", "Params", ",", "optimizer", ":", "torch", ".", "optim", ".", "Optimizer", ",", "**", "extras", "\n", ")", "->", "\"UpdateLearningRate\"", ":", "\n", "\n", "        ", "return", "cls", "(", "\n", "LearningRateScheduler", ".", "from_params", "(", "\n", "params", "=", "params", ".", "pop", "(", "\"learning_rate_scheduler\"", ")", ",", "optimizer", "=", "optimizer", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.callbacks.gradient_norm_and_clip.GradientNormAndClip.__init__": [[27, 32], ["None"], "methods", ["None"], ["def", "__init__", "(", "\n", "self", ",", "grad_norm", ":", "Optional", "[", "float", "]", "=", "None", ",", "grad_clipping", ":", "Optional", "[", "float", "]", "=", "None", "\n", ")", "->", "None", ":", "\n", "        ", "self", ".", "grad_norm", "=", "grad_norm", "\n", "self", ".", "grad_clipping", "=", "grad_clipping", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.callbacks.gradient_norm_and_clip.GradientNormAndClip.enable_gradient_clipping": [[33, 36], ["allennlp.training.callbacks.callback.handle_event", "allennlp.training.util.enable_gradient_clipping"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.callbacks.callback.handle_event", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.callbacks.gradient_norm_and_clip.GradientNormAndClip.enable_gradient_clipping"], ["", "@", "handle_event", "(", "Events", ".", "TRAINING_START", ")", "\n", "def", "enable_gradient_clipping", "(", "self", ",", "trainer", ":", "\"CallbackTrainer\"", ")", ":", "\n", "        ", "training_util", ".", "enable_gradient_clipping", "(", "trainer", ".", "model", ",", "self", ".", "grad_clipping", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.callbacks.gradient_norm_and_clip.GradientNormAndClip.rescale_gradients": [[37, 41], ["allennlp.training.callbacks.callback.handle_event", "allennlp.training.util.rescale_gradients"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.callbacks.callback.handle_event", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.callbacks.gradient_norm_and_clip.GradientNormAndClip.rescale_gradients"], ["", "@", "handle_event", "(", "Events", ".", "BACKWARD", ",", "priority", "=", "1000", ")", "\n", "def", "rescale_gradients", "(", "self", ",", "trainer", ":", "\"CallbackTrainer\"", ")", ":", "\n", "        ", "trainer", ".", "batch_grad_norm", "=", "training_util", ".", "rescale_gradients", "(", "\n", "trainer", ".", "model", ",", "self", ".", "grad_norm", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.learning_rate_schedulers.learning_rate_scheduler.LearningRateScheduler.__init__": [[13, 15], ["allennlp.training.scheduler.Scheduler.__init__"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.graph_util.plot.GraphPlot.__init__"], ["    ", "def", "__init__", "(", "self", ",", "optimizer", ":", "torch", ".", "optim", ".", "Optimizer", ",", "last_epoch", ":", "int", "=", "-", "1", ")", "->", "None", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "optimizer", ",", "\"lr\"", ",", "last_epoch", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.learning_rate_schedulers.learning_rate_scheduler.LearningRateScheduler.get_values": [[16, 18], ["None"], "methods", ["None"], ["", "def", "get_values", "(", "self", ")", "->", "None", ":", "\n", "        ", "raise", "NotImplementedError", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.learning_rate_schedulers.learning_rate_scheduler.LearningRateScheduler.from_params": [[20, 35], ["params.pop_choice", "isinstance", "LearningRateScheduler.list_available", "LearningRateScheduler.by_name", "learning_rate_scheduler._PyTorchLearningRateSchedulerWithMetricsWrapper", "isinstance", "params.as_dict", "learning_rate_scheduler._PyTorchLearningRateSchedulerWrapper"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.params.pop_choice", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.registrable.Registrable.list_available", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.registrable.Registrable.by_name", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.params.Params.as_dict"], ["", "@", "classmethod", "\n", "def", "from_params", "(", "cls", ",", "optimizer", ":", "torch", ".", "optim", ".", "Optimizer", ",", "params", ":", "Params", ")", ":", "# type: ignore", "\n", "\n", "        ", "scheduler_type", "=", "params", ".", "pop_choice", "(", "\n", "\"type\"", ",", "LearningRateScheduler", ".", "list_available", "(", ")", "\n", ")", "\n", "scheduler", "=", "LearningRateScheduler", ".", "by_name", "(", "scheduler_type", ")", "(", "\n", "optimizer", ",", "**", "params", ".", "as_dict", "(", ")", "\n", ")", "# type: ignore", "\n", "if", "isinstance", "(", "scheduler", ",", "torch", ".", "optim", ".", "lr_scheduler", ".", "ReduceLROnPlateau", ")", ":", "\n", "            ", "return", "_PyTorchLearningRateSchedulerWithMetricsWrapper", "(", "scheduler", ")", "\n", "", "elif", "isinstance", "(", "scheduler", ",", "torch", ".", "optim", ".", "lr_scheduler", ".", "_LRScheduler", ")", ":", "\n", "            ", "return", "_PyTorchLearningRateSchedulerWrapper", "(", "scheduler", ")", "\n", "", "else", ":", "\n", "            ", "return", "scheduler", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.learning_rate_schedulers.learning_rate_scheduler._PyTorchLearningRateSchedulerWrapper.__init__": [[38, 40], ["None"], "methods", ["None"], ["    ", "def", "__init__", "(", "self", ",", "lr_scheduler", ":", "torch", ".", "optim", ".", "lr_scheduler", ".", "_LRScheduler", ")", "->", "None", ":", "\n", "        ", "self", ".", "lr_scheduler", "=", "lr_scheduler", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.learning_rate_schedulers.learning_rate_scheduler._PyTorchLearningRateSchedulerWrapper.get_values": [[41, 43], ["learning_rate_scheduler._PyTorchLearningRateSchedulerWrapper.lr_scheduler.get_lr"], "methods", ["None"], ["", "def", "get_values", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "lr_scheduler", ".", "get_lr", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.learning_rate_schedulers.learning_rate_scheduler._PyTorchLearningRateSchedulerWrapper.step": [[44, 47], ["learning_rate_scheduler._PyTorchLearningRateSchedulerWrapper.lr_scheduler.step"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.learning_rate_schedulers.slanted_triangular.SlantedTriangular.step"], ["", "@", "overrides", "\n", "def", "step", "(", "self", ",", "metric", ":", "float", "=", "None", ",", "epoch", ":", "int", "=", "None", ")", "->", "None", ":", "\n", "        ", "self", ".", "lr_scheduler", ".", "step", "(", "epoch", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.learning_rate_schedulers.learning_rate_scheduler._PyTorchLearningRateSchedulerWrapper.state_dict": [[48, 51], ["learning_rate_scheduler._PyTorchLearningRateSchedulerWrapper.lr_scheduler.state_dict"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.learning_rate_schedulers.learning_rate_scheduler._PyTorchLearningRateSchedulerWrapper.state_dict"], ["", "@", "overrides", "\n", "def", "state_dict", "(", "self", ")", "->", "Dict", "[", "str", ",", "Any", "]", ":", "\n", "        ", "return", "self", ".", "lr_scheduler", ".", "state_dict", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.learning_rate_schedulers.learning_rate_scheduler._PyTorchLearningRateSchedulerWrapper.load_state_dict": [[52, 55], ["learning_rate_scheduler._PyTorchLearningRateSchedulerWrapper.lr_scheduler.load_state_dict"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.learning_rate_schedulers.learning_rate_scheduler._PyTorchLearningRateSchedulerWrapper.load_state_dict"], ["", "@", "overrides", "\n", "def", "load_state_dict", "(", "self", ",", "state_dict", ":", "Dict", "[", "str", ",", "Any", "]", ")", "->", "None", ":", "\n", "        ", "self", ".", "lr_scheduler", ".", "load_state_dict", "(", "state_dict", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.learning_rate_schedulers.learning_rate_scheduler._PyTorchLearningRateSchedulerWithMetricsWrapper.step": [[60, 69], ["learning_rate_scheduler._PyTorchLearningRateSchedulerWithMetricsWrapper.lr_scheduler.step", "allennlp.common.checks.ConfigurationError"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.learning_rate_schedulers.slanted_triangular.SlantedTriangular.step"], ["    ", "@", "overrides", "\n", "def", "step", "(", "self", ",", "metric", ":", "float", "=", "None", ",", "epoch", ":", "int", "=", "None", ")", "->", "None", ":", "\n", "        ", "if", "metric", "is", "None", ":", "\n", "            ", "raise", "ConfigurationError", "(", "\n", "\"This learning rate scheduler requires \"", "\n", "\"a validation metric to compute the schedule and therefore \"", "\n", "\"must be used with a validation dataset.\"", "\n", ")", "\n", "", "self", ".", "lr_scheduler", ".", "step", "(", "metric", ",", "epoch", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.learning_rate_schedulers.cosine.CosineWithRestarts.__init__": [[39, 64], ["allennlp.training.learning_rate_schedulers.learning_rate_scheduler.LearningRateScheduler.__init__", "logger.warning"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.graph_util.plot.GraphPlot.__init__"], ["def", "__init__", "(", "\n", "self", ",", "\n", "optimizer", ":", "torch", ".", "optim", ".", "Optimizer", ",", "\n", "t_initial", ":", "int", ",", "\n", "t_mul", ":", "float", "=", "1.0", ",", "\n", "eta_min", ":", "float", "=", "0.0", ",", "\n", "eta_mul", ":", "float", "=", "1.0", ",", "\n", "last_epoch", ":", "int", "=", "-", "1", ",", "\n", ")", "->", "None", ":", "\n", "        ", "assert", "t_initial", ">", "0", "\n", "assert", "eta_min", ">=", "0", "\n", "if", "t_initial", "==", "1", "and", "t_mul", "==", "1", "and", "eta_mul", "==", "1", ":", "\n", "            ", "logger", ".", "warning", "(", "\n", "\"Cosine annealing scheduler will have no effect on the learning \"", "\n", "\"rate since t_initial = t_mul = eta_mul = 1.\"", "\n", ")", "\n", "", "self", ".", "t_initial", "=", "t_initial", "\n", "self", ".", "t_mul", "=", "t_mul", "\n", "self", ".", "eta_min", "=", "eta_min", "\n", "self", ".", "eta_mul", "=", "eta_mul", "\n", "self", ".", "_last_restart", ":", "int", "=", "0", "\n", "self", ".", "_cycle_counter", ":", "int", "=", "0", "\n", "self", ".", "_cycle_len", ":", "int", "=", "t_initial", "\n", "self", ".", "_n_restarts", ":", "int", "=", "0", "\n", "super", "(", ")", ".", "__init__", "(", "optimizer", ",", "last_epoch", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.learning_rate_schedulers.cosine.CosineWithRestarts.get_values": [[65, 94], ["int", "numpy.cos"], "methods", ["None"], ["", "def", "get_values", "(", "self", ")", ":", "\n", "        ", "\"\"\"Get updated learning rate.\"\"\"", "\n", "if", "self", ".", "last_epoch", "==", "-", "1", ":", "\n", "            ", "return", "self", ".", "base_values", "\n", "\n", "", "step", "=", "self", ".", "last_epoch", "+", "1", "\n", "self", ".", "_cycle_counter", "=", "step", "-", "self", ".", "_last_restart", "\n", "\n", "if", "self", ".", "_cycle_counter", "%", "self", ".", "_cycle_len", "==", "0", ":", "\n", "            ", "self", ".", "_n_restarts", "+=", "1", "\n", "self", ".", "_cycle_counter", "=", "0", "\n", "self", ".", "_last_restart", "=", "step", "\n", "\n", "", "base_lrs", "=", "[", "lr", "*", "self", ".", "eta_mul", "**", "self", ".", "_n_restarts", "for", "lr", "in", "self", ".", "base_values", "]", "\n", "self", ".", "_cycle_len", "=", "int", "(", "self", ".", "t_initial", "*", "self", ".", "t_mul", "**", "self", ".", "_n_restarts", ")", "\n", "\n", "lrs", "=", "[", "\n", "self", ".", "eta_min", "\n", "+", "(", "(", "lr", "-", "self", ".", "eta_min", ")", "/", "2", ")", "\n", "*", "(", "\n", "np", ".", "cos", "(", "\n", "np", ".", "pi", "*", "(", "self", ".", "_cycle_counter", "%", "self", ".", "_cycle_len", ")", "/", "self", ".", "_cycle_len", "\n", ")", "\n", "+", "1", "\n", ")", "\n", "for", "lr", "in", "base_lrs", "\n", "]", "\n", "\n", "return", "lrs", "\n", "", "", ""]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.learning_rate_schedulers.noam.NoamLR.__init__": [[27, 39], ["allennlp.training.learning_rate_schedulers.learning_rate_scheduler.LearningRateScheduler.__init__"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.graph_util.plot.GraphPlot.__init__"], ["def", "__init__", "(", "\n", "self", ",", "\n", "optimizer", ":", "torch", ".", "optim", ".", "Optimizer", ",", "\n", "model_size", ":", "int", ",", "\n", "warmup_steps", ":", "int", ",", "\n", "factor", ":", "float", "=", "1.0", ",", "\n", "last_epoch", ":", "int", "=", "-", "1", ",", "\n", ")", "->", "None", ":", "\n", "        ", "self", ".", "warmup_steps", "=", "warmup_steps", "\n", "self", ".", "factor", "=", "factor", "\n", "self", ".", "model_size", "=", "model_size", "\n", "super", "(", ")", ".", "__init__", "(", "optimizer", ",", "last_epoch", "=", "last_epoch", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.learning_rate_schedulers.noam.NoamLR.step": [[40, 43], ["None"], "methods", ["None"], ["", "@", "overrides", "\n", "def", "step", "(", "self", ",", "metric", ":", "float", "=", "None", ",", "epoch", ":", "int", "=", "None", ")", "->", "None", ":", "\n", "        ", "pass", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.learning_rate_schedulers.noam.NoamLR.step_batch": [[44, 53], ["zip", "noam.NoamLR.get_values"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.learning_rate_schedulers.slanted_triangular.SlantedTriangular.get_values"], ["", "def", "step_batch", "(", "self", ",", "batch_num_total", ":", "int", "=", "None", ")", "->", "None", ":", "\n", "        ", "if", "batch_num_total", "is", "None", ":", "\n", "            ", "self", ".", "last_epoch", "+=", "1", "# type: ignore", "\n", "", "else", ":", "\n", "            ", "self", ".", "last_epoch", "=", "batch_num_total", "\n", "", "for", "param_group", ",", "learning_rate", "in", "zip", "(", "\n", "self", ".", "optimizer", ".", "param_groups", ",", "self", ".", "get_values", "(", ")", "\n", ")", ":", "\n", "            ", "param_group", "[", "\"lr\"", "]", "=", "learning_rate", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.learning_rate_schedulers.noam.NoamLR.get_values": [[54, 62], ["max", "min", "range", "len"], "methods", ["None"], ["", "", "def", "get_values", "(", "self", ")", ":", "\n", "        ", "step", "=", "max", "(", "self", ".", "last_epoch", ",", "1", ")", "\n", "scale", "=", "self", ".", "factor", "*", "(", "\n", "self", ".", "model_size", "**", "(", "-", "0.5", ")", "\n", "*", "min", "(", "step", "**", "(", "-", "0.5", ")", ",", "step", "*", "self", ".", "warmup_steps", "**", "(", "-", "1.5", ")", ")", "\n", ")", "\n", "\n", "return", "[", "scale", "for", "_", "in", "range", "(", "len", "(", "self", ".", "base_values", ")", ")", "]", "\n", "", "", ""]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.learning_rate_schedulers.slanted_triangular.SlantedTriangular.__init__": [[50, 93], ["allennlp.training.learning_rate_schedulers.learning_rate_scheduler.LearningRateScheduler.__init__", "slanted_triangular.SlantedTriangular.step_batch", "range", "len", "len"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.graph_util.plot.GraphPlot.__init__", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.learning_rate_schedulers.slanted_triangular.SlantedTriangular.step_batch"], ["def", "__init__", "(", "\n", "self", ",", "\n", "optimizer", ":", "torch", ".", "optim", ".", "Optimizer", ",", "\n", "num_epochs", ":", "int", ",", "\n", "num_steps_per_epoch", ":", "int", ",", "\n", "cut_frac", ":", "float", "=", "0.1", ",", "\n", "ratio", ":", "int", "=", "32", ",", "\n", "last_epoch", ":", "int", "=", "-", "1", ",", "\n", "gradual_unfreezing", ":", "bool", "=", "False", ",", "\n", "discriminative_fine_tuning", ":", "bool", "=", "False", ",", "\n", "decay_factor", ":", "float", "=", "0.38", ",", "\n", ")", "->", "None", ":", "\n", "        ", "self", ".", "num_epochs", "=", "num_epochs", "\n", "self", ".", "num_steps_per_epoch", "=", "num_steps_per_epoch", "\n", "self", ".", "cut_frac", "=", "cut_frac", "\n", "self", ".", "ratio", "=", "ratio", "\n", "self", ".", "gradual_unfreezing", "=", "gradual_unfreezing", "\n", "self", ".", "freezing_current", "=", "self", ".", "gradual_unfreezing", "\n", "self", ".", "is_first_epoch", "=", "True", "\n", "# track the actual number of steps for each epoch", "\n", "self", ".", "batch_num_total_epoch_end", ":", "List", "[", "int", "]", "=", "[", "]", "\n", "if", "self", ".", "gradual_unfreezing", ":", "\n", "            ", "assert", "not", "optimizer", ".", "param_groups", "[", "-", "1", "]", "[", "\n", "\"params\"", "\n", "]", ",", "\"The default group should be empty.\"", "\n", "", "if", "self", ".", "gradual_unfreezing", "or", "discriminative_fine_tuning", ":", "\n", "            ", "assert", "len", "(", "optimizer", ".", "param_groups", ")", ">", "2", ",", "(", "\n", "\"There should be at least 3 param_groups (2 + empty default group)\"", "\n", "\" for gradual unfreezing / discriminative fine-tuning to make sense.\"", "\n", ")", "\n", "", "super", "(", ")", ".", "__init__", "(", "optimizer", ",", "last_epoch", ")", "\n", "if", "discriminative_fine_tuning", ":", "\n", "# skip the last param_group if it is has no parameters", "\n", "            ", "exponent", "=", "0", "\n", "for", "i", "in", "range", "(", "len", "(", "self", ".", "base_values", ")", "-", "1", ",", "-", "1", ",", "-", "1", ")", ":", "\n", "                ", "param_group", "=", "optimizer", ".", "param_groups", "[", "i", "]", "\n", "if", "param_group", "[", "\"params\"", "]", ":", "\n", "                    ", "param_group", "[", "\"lr\"", "]", "=", "self", ".", "base_values", "[", "i", "]", "*", "decay_factor", "**", "exponent", "\n", "self", ".", "base_values", "[", "i", "]", "=", "param_group", "[", "\"lr\"", "]", "\n", "exponent", "+=", "1", "\n", "# set up for the first batch", "\n", "", "", "", "self", ".", "last_batch_num_total", "=", "-", "1", "\n", "self", ".", "step_batch", "(", "0", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.learning_rate_schedulers.slanted_triangular.SlantedTriangular.step": [[94, 123], ["len", "slanted_triangular.SlantedTriangular.batch_num_total_epoch_end.append", "slanted_triangular.SlantedTriangular.batch_num_total_epoch_end.append", "enumerate", "logger.info", "logger.info", "reversed", "len", "bool"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.dataset_readers.multiprocess_dataset_reader.logger.info", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.dataset_readers.multiprocess_dataset_reader.logger.info"], ["", "@", "overrides", "\n", "def", "step", "(", "self", ",", "metric", ":", "float", "=", "None", ",", "epoch", ":", "int", "=", "None", ")", "->", "None", ":", "\n", "        ", "if", "len", "(", "self", ".", "batch_num_total_epoch_end", ")", "==", "0", ":", "\n", "            ", "self", ".", "batch_num_total_epoch_end", ".", "append", "(", "0", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "batch_num_total_epoch_end", ".", "append", "(", "self", ".", "last_batch_num_total", ")", "\n", "\n", "", "if", "self", ".", "gradual_unfreezing", ":", "\n", "# the method is called once when initialising before the", "\n", "# first epoch (epoch -1) and then always at the end of each", "\n", "# epoch; so the first time, with epoch id -1, we want to set", "\n", "# up for epoch #1; the second time, with epoch id 0,", "\n", "# we want to set up for epoch #2, etc.", "\n", "            ", "if", "self", ".", "is_first_epoch", ":", "\n", "                ", "num_layers_to_unfreeze", "=", "1", "\n", "self", ".", "is_first_epoch", "=", "False", "\n", "", "else", ":", "\n", "                ", "num_layers_to_unfreeze", "=", "epoch", "+", "2", "\n", "", "if", "num_layers_to_unfreeze", ">=", "len", "(", "self", ".", "optimizer", ".", "param_groups", ")", "-", "1", ":", "\n", "                ", "logger", ".", "info", "(", "\"Gradual unfreezing finished. Training all layers.\"", ")", "\n", "self", ".", "freezing_current", "=", "False", "\n", "", "else", ":", "\n", "                ", "logger", ".", "info", "(", "\n", "f\"Gradual unfreezing. Training only the top {num_layers_to_unfreeze} layers.\"", "\n", ")", "\n", "", "for", "i", ",", "param_group", "in", "enumerate", "(", "reversed", "(", "self", ".", "optimizer", ".", "param_groups", ")", ")", ":", "\n", "                ", "for", "param", "in", "param_group", "[", "\"params\"", "]", ":", "\n", "# i = 0 is the default group; we care about i > 0", "\n", "                    ", "param", ".", "requires_grad", "=", "bool", "(", "i", "<=", "num_layers_to_unfreeze", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.learning_rate_schedulers.slanted_triangular.SlantedTriangular.step_batch": [[124, 132], ["zip", "slanted_triangular.SlantedTriangular.get_values"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.learning_rate_schedulers.slanted_triangular.SlantedTriangular.get_values"], ["", "", "", "", "def", "step_batch", "(", "self", ",", "batch_num_total", ":", "int", "=", "None", ")", ":", "\n", "        ", "if", "batch_num_total", "is", "None", ":", "\n", "            ", "batch_num_total", "=", "self", ".", "last_batch_num_total", "+", "1", "\n", "", "self", ".", "last_batch_num_total", "=", "batch_num_total", "\n", "for", "param_group", ",", "learning_rate", "in", "zip", "(", "\n", "self", ".", "optimizer", ".", "param_groups", ",", "self", ".", "get_values", "(", ")", "\n", ")", ":", "\n", "            ", "param_group", "[", "\"lr\"", "]", "=", "learning_rate", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.learning_rate_schedulers.slanted_triangular.SlantedTriangular.get_values": [[133, 166], ["int", "len", "int", "max", "min", "min", "len", "len"], "methods", ["None"], ["", "", "def", "get_values", "(", "self", ")", ":", "\n", "# get the actual number of batches per epoch seen in training", "\n", "        ", "if", "len", "(", "self", ".", "batch_num_total_epoch_end", ")", ">", "1", ":", "\n", "# have finished an epoch", "\n", "            ", "actual_num_steps_per_epoch", "=", "int", "(", "\n", "self", ".", "batch_num_total_epoch_end", "[", "-", "1", "]", "\n", "/", "(", "len", "(", "self", ".", "batch_num_total_epoch_end", ")", "-", "1", ")", "\n", ")", "\n", "", "else", ":", "\n", "            ", "actual_num_steps_per_epoch", "=", "max", "(", "\n", "self", ".", "num_steps_per_epoch", ",", "self", ".", "last_batch_num_total", "\n", ")", "\n", "\n", "", "if", "self", ".", "freezing_current", ":", "\n", "# if we still freeze, we restrict the schedule to the current epoch", "\n", "            ", "num_steps", "=", "actual_num_steps_per_epoch", "\n", "step", "=", "min", "(", "\n", "self", ".", "last_batch_num_total", "-", "self", ".", "batch_num_total_epoch_end", "[", "-", "1", "]", ",", "\n", "num_steps", ",", "\n", ")", "\n", "", "else", ":", "\n", "# otherwise we use the schedule for the rest of training", "\n", "            ", "if", "not", "self", ".", "gradual_unfreezing", ":", "\n", "                ", "frozen_steps", "=", "0", "\n", "", "else", ":", "\n", "                ", "num_frozen_epochs", "=", "len", "(", "self", ".", "optimizer", ".", "param_groups", ")", "-", "2", "\n", "frozen_steps", "=", "self", ".", "batch_num_total_epoch_end", "[", "num_frozen_epochs", "]", "\n", "", "num_steps", "=", "self", ".", "num_epochs", "*", "actual_num_steps_per_epoch", "-", "frozen_steps", "\n", "step", "=", "min", "(", "self", ".", "last_batch_num_total", "-", "frozen_steps", ",", "num_steps", ")", "\n", "", "cut", "=", "int", "(", "num_steps", "*", "self", ".", "cut_frac", ")", "\n", "prop", "=", "step", "/", "cut", "if", "step", "<", "cut", "else", "1", "-", "(", "step", "-", "cut", ")", "/", "(", "num_steps", "-", "cut", ")", "\n", "return", "[", "\n", "lr", "*", "(", "1", "+", "prop", "*", "(", "self", ".", "ratio", "-", "1", ")", ")", "/", "self", ".", "ratio", "for", "lr", "in", "self", ".", "base_values", "\n", "]", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.matrix_attention.dot_product_matrix_attention.DotProductMatrixAttention.forward": [[14, 17], ["matrix_1.bmm", "matrix_2.transpose"], "methods", ["None"], ["@", "overrides", "\n", "def", "forward", "(", "self", ",", "matrix_1", ":", "torch", ".", "Tensor", ",", "matrix_2", ":", "torch", ".", "Tensor", ")", "->", "torch", ".", "Tensor", ":", "\n", "        ", "return", "matrix_1", ".", "bmm", "(", "matrix_2", ".", "transpose", "(", "2", ",", "1", ")", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.matrix_attention.cosine_matrix_attention.CosineMatrixAttention.forward": [[14, 19], ["torch.bmm", "b_norm.transpose", "matrix_1.norm", "matrix_2.norm"], "methods", ["None"], ["@", "overrides", "\n", "def", "forward", "(", "self", ",", "matrix_1", ":", "torch", ".", "Tensor", ",", "matrix_2", ":", "torch", ".", "Tensor", ")", "->", "torch", ".", "Tensor", ":", "\n", "        ", "a_norm", "=", "matrix_1", "/", "(", "matrix_1", ".", "norm", "(", "p", "=", "2", ",", "dim", "=", "-", "1", ",", "keepdim", "=", "True", ")", "+", "1e-13", ")", "\n", "b_norm", "=", "matrix_2", "/", "(", "matrix_2", ".", "norm", "(", "p", "=", "2", ",", "dim", "=", "-", "1", ",", "keepdim", "=", "True", ")", "+", "1e-13", ")", "\n", "return", "torch", ".", "bmm", "(", "a_norm", ",", "b_norm", ".", "transpose", "(", "-", "1", ",", "-", "2", ")", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.matrix_attention.matrix_attention.MatrixAttention.forward": [[22, 24], ["None"], "methods", ["None"], ["def", "forward", "(", "self", ",", "matrix_1", ":", "torch", ".", "Tensor", ",", "matrix_2", ":", "torch", ".", "Tensor", ")", "->", "torch", ".", "Tensor", ":", "\n", "        ", "raise", "NotImplementedError", "\n", "", "", ""]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.matrix_attention.bilinear_matrix_attention.BilinearMatrixAttention.__init__": [[39, 63], ["allennlp.modules.matrix_attention.matrix_attention.MatrixAttention.__init__", "torch.nn.parameter.Parameter", "bilinear_matrix_attention.BilinearMatrixAttention.reset_parameters", "torch.nn.parameter.Parameter", "torch.nn.parameter.Parameter", "torch.Tensor", "torch.Tensor", "torch.Tensor", "allennlp.nn.Activation.by_name"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.graph_util.plot.GraphPlot.__init__", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.similarity_functions.linear.LinearSimilarity.reset_parameters", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.registrable.Registrable.by_name"], ["def", "__init__", "(", "\n", "self", ",", "\n", "matrix_1_dim", ":", "int", ",", "\n", "matrix_2_dim", ":", "int", ",", "\n", "activation", ":", "Activation", "=", "None", ",", "\n", "use_input_biases", ":", "bool", "=", "False", ",", "\n", "label_dim", ":", "int", "=", "1", ",", "\n", ")", "->", "None", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "if", "use_input_biases", ":", "\n", "            ", "matrix_1_dim", "+=", "1", "\n", "matrix_2_dim", "+=", "1", "\n", "\n", "", "if", "label_dim", "==", "1", ":", "\n", "            ", "self", ".", "_weight_matrix", "=", "Parameter", "(", "torch", ".", "Tensor", "(", "matrix_1_dim", ",", "matrix_2_dim", ")", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "_weight_matrix", "=", "Parameter", "(", "\n", "torch", ".", "Tensor", "(", "label_dim", ",", "matrix_1_dim", ",", "matrix_2_dim", ")", "\n", ")", "\n", "\n", "", "self", ".", "_bias", "=", "Parameter", "(", "torch", ".", "Tensor", "(", "1", ")", ")", "\n", "self", ".", "_activation", "=", "activation", "or", "Activation", ".", "by_name", "(", "\"linear\"", ")", "(", ")", "\n", "self", ".", "_use_input_biases", "=", "use_input_biases", "\n", "self", ".", "reset_parameters", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.matrix_attention.bilinear_matrix_attention.BilinearMatrixAttention.reset_parameters": [[64, 67], ["torch.nn.init.xavier_uniform_", "bilinear_matrix_attention.BilinearMatrixAttention._bias.data.fill_"], "methods", ["None"], ["", "def", "reset_parameters", "(", "self", ")", ":", "\n", "        ", "torch", ".", "nn", ".", "init", ".", "xavier_uniform_", "(", "self", ".", "_weight_matrix", ")", "\n", "self", ".", "_bias", ".", "data", ".", "fill_", "(", "0", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.matrix_attention.bilinear_matrix_attention.BilinearMatrixAttention.forward": [[68, 84], ["torch.matmul", "torch.matmul", "bilinear_matrix_attention.BilinearMatrixAttention._activation", "torch.cat.new_ones", "torch.cat.new_ones", "torch.cat", "torch.cat", "weight.unsqueeze.unsqueeze.dim", "weight.unsqueeze.unsqueeze.unsqueeze", "torch.cat.unsqueeze", "torch.cat.unsqueeze().transpose", "torch.matmul.squeeze", "torch.cat.unsqueeze", "torch.cat.size", "torch.cat.size"], "methods", ["None"], ["", "@", "overrides", "\n", "def", "forward", "(", "self", ",", "matrix_1", ":", "torch", ".", "Tensor", ",", "matrix_2", ":", "torch", ".", "Tensor", ")", "->", "torch", ".", "Tensor", ":", "\n", "\n", "        ", "if", "self", ".", "_use_input_biases", ":", "\n", "            ", "bias1", "=", "matrix_1", ".", "new_ones", "(", "matrix_1", ".", "size", "(", ")", "[", ":", "-", "1", "]", "+", "(", "1", ",", ")", ")", "\n", "bias2", "=", "matrix_2", ".", "new_ones", "(", "matrix_2", ".", "size", "(", ")", "[", ":", "-", "1", "]", "+", "(", "1", ",", ")", ")", "\n", "\n", "matrix_1", "=", "torch", ".", "cat", "(", "[", "matrix_1", ",", "bias1", "]", ",", "-", "1", ")", "\n", "matrix_2", "=", "torch", ".", "cat", "(", "[", "matrix_2", ",", "bias2", "]", ",", "-", "1", ")", "\n", "\n", "", "weight", "=", "self", ".", "_weight_matrix", "\n", "if", "weight", ".", "dim", "(", ")", "==", "2", ":", "\n", "            ", "weight", "=", "weight", ".", "unsqueeze", "(", "0", ")", "\n", "", "intermediate", "=", "torch", ".", "matmul", "(", "matrix_1", ".", "unsqueeze", "(", "1", ")", ",", "weight", ")", "\n", "final", "=", "torch", ".", "matmul", "(", "intermediate", ",", "matrix_2", ".", "unsqueeze", "(", "1", ")", ".", "transpose", "(", "2", ",", "3", ")", ")", "\n", "return", "self", ".", "_activation", "(", "final", ".", "squeeze", "(", "1", ")", "+", "self", ".", "_bias", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.matrix_attention.legacy_matrix_attention.LegacyMatrixAttention.__init__": [[23, 26], ["allennlp.modules.matrix_attention.matrix_attention.MatrixAttention.__init__", "allennlp.modules.similarity_functions.dot_product.DotProductSimilarity"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.graph_util.plot.GraphPlot.__init__"], ["def", "__init__", "(", "self", ",", "similarity_function", ":", "SimilarityFunction", "=", "None", ")", "->", "None", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "_similarity_function", "=", "similarity_function", "or", "DotProductSimilarity", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.matrix_attention.legacy_matrix_attention.LegacyMatrixAttention.forward": [[27, 42], ["matrix_1.unsqueeze().expand", "matrix_2.unsqueeze().expand", "legacy_matrix_attention.LegacyMatrixAttention._similarity_function", "matrix_1.unsqueeze", "matrix_1.size", "matrix_1.size", "matrix_2.size", "matrix_1.size", "matrix_2.unsqueeze", "matrix_2.size", "matrix_1.size", "matrix_2.size", "matrix_2.size"], "methods", ["None"], ["", "@", "overrides", "\n", "def", "forward", "(", "self", ",", "matrix_1", ":", "torch", ".", "Tensor", ",", "matrix_2", ":", "torch", ".", "Tensor", ")", "->", "torch", ".", "Tensor", ":", "\n", "        ", "tiled_matrix_1", "=", "matrix_1", ".", "unsqueeze", "(", "2", ")", ".", "expand", "(", "\n", "matrix_1", ".", "size", "(", ")", "[", "0", "]", ",", "\n", "matrix_1", ".", "size", "(", ")", "[", "1", "]", ",", "\n", "matrix_2", ".", "size", "(", ")", "[", "1", "]", ",", "\n", "matrix_1", ".", "size", "(", ")", "[", "2", "]", ",", "\n", ")", "\n", "tiled_matrix_2", "=", "matrix_2", ".", "unsqueeze", "(", "1", ")", ".", "expand", "(", "\n", "matrix_2", ".", "size", "(", ")", "[", "0", "]", ",", "\n", "matrix_1", ".", "size", "(", ")", "[", "1", "]", ",", "\n", "matrix_2", ".", "size", "(", ")", "[", "1", "]", ",", "\n", "matrix_2", ".", "size", "(", ")", "[", "2", "]", ",", "\n", ")", "\n", "return", "self", ".", "_similarity_function", "(", "tiled_matrix_1", ",", "tiled_matrix_2", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.matrix_attention.linear_matrix_attention.LinearMatrixAttention.__init__": [[48, 62], ["allennlp.modules.matrix_attention.matrix_attention.MatrixAttention.__init__", "allennlp.nn.util.get_combined_dim", "torch.nn.Parameter", "torch.nn.Parameter", "linear_matrix_attention.LinearMatrixAttention.reset_parameters", "torch.Tensor", "torch.Tensor", "allennlp.nn.activations.Activation.by_name"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.graph_util.plot.GraphPlot.__init__", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.nn.util.get_combined_dim", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.similarity_functions.linear.LinearSimilarity.reset_parameters", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.registrable.Registrable.by_name"], ["def", "__init__", "(", "\n", "self", ",", "\n", "tensor_1_dim", ":", "int", ",", "\n", "tensor_2_dim", ":", "int", ",", "\n", "combination", ":", "str", "=", "\"x,y\"", ",", "\n", "activation", ":", "Activation", "=", "None", ",", "\n", ")", "->", "None", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "_combination", "=", "combination", "\n", "combined_dim", "=", "util", ".", "get_combined_dim", "(", "combination", ",", "[", "tensor_1_dim", ",", "tensor_2_dim", "]", ")", "\n", "self", ".", "_weight_vector", "=", "Parameter", "(", "torch", ".", "Tensor", "(", "combined_dim", ")", ")", "\n", "self", ".", "_bias", "=", "Parameter", "(", "torch", ".", "Tensor", "(", "1", ")", ")", "\n", "self", ".", "_activation", "=", "activation", "or", "Activation", ".", "by_name", "(", "\"linear\"", ")", "(", ")", "\n", "self", ".", "reset_parameters", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.matrix_attention.linear_matrix_attention.LinearMatrixAttention.reset_parameters": [[63, 67], ["math.sqrt", "linear_matrix_attention.LinearMatrixAttention._weight_vector.data.uniform_", "linear_matrix_attention.LinearMatrixAttention._bias.data.fill_", "linear_matrix_attention.LinearMatrixAttention._weight_vector.size"], "methods", ["None"], ["", "def", "reset_parameters", "(", "self", ")", ":", "\n", "        ", "std", "=", "math", ".", "sqrt", "(", "6", "/", "(", "self", ".", "_weight_vector", ".", "size", "(", "0", ")", "+", "1", ")", ")", "\n", "self", ".", "_weight_vector", ".", "data", ".", "uniform_", "(", "-", "std", ",", "std", ")", "\n", "self", ".", "_bias", ".", "data", ".", "fill_", "(", "0", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.matrix_attention.linear_matrix_attention.LinearMatrixAttention.forward": [[68, 76], ["allennlp.nn.util.combine_tensors_and_multiply", "linear_matrix_attention.LinearMatrixAttention._activation", "matrix_1.unsqueeze", "matrix_2.unsqueeze"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.nn.util.combine_tensors_and_multiply"], ["", "@", "overrides", "\n", "def", "forward", "(", "self", ",", "matrix_1", ":", "torch", ".", "Tensor", ",", "matrix_2", ":", "torch", ".", "Tensor", ")", "->", "torch", ".", "Tensor", ":", "\n", "        ", "combined_tensors", "=", "util", ".", "combine_tensors_and_multiply", "(", "\n", "self", ".", "_combination", ",", "\n", "[", "matrix_1", ".", "unsqueeze", "(", "2", ")", ",", "matrix_2", ".", "unsqueeze", "(", "1", ")", "]", ",", "\n", "self", ".", "_weight_vector", ",", "\n", ")", "\n", "return", "self", ".", "_activation", "(", "combined_tensors", "+", "self", ".", "_bias", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.span_extractors.self_attentive_span_extractor.SelfAttentiveSpanExtractor.__init__": [[34, 38], ["allennlp.modules.span_extractors.span_extractor.SpanExtractor.__init__", "allennlp.modules.time_distributed.TimeDistributed", "torch.nn.Linear"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.graph_util.plot.GraphPlot.__init__"], ["def", "__init__", "(", "self", ",", "input_dim", ":", "int", ")", "->", "None", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "_input_dim", "=", "input_dim", "\n", "self", ".", "_global_attention", "=", "TimeDistributed", "(", "torch", ".", "nn", ".", "Linear", "(", "input_dim", ",", "1", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.span_extractors.self_attentive_span_extractor.SelfAttentiveSpanExtractor.get_input_dim": [[39, 41], ["None"], "methods", ["None"], ["", "def", "get_input_dim", "(", "self", ")", "->", "int", ":", "\n", "        ", "return", "self", ".", "_input_dim", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.span_extractors.self_attentive_span_extractor.SelfAttentiveSpanExtractor.get_output_dim": [[42, 44], ["None"], "methods", ["None"], ["", "def", "get_output_dim", "(", "self", ")", "->", "int", ":", "\n", "        ", "return", "self", ".", "_input_dim", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.span_extractors.self_attentive_span_extractor.SelfAttentiveSpanExtractor.forward": [[45, 82], ["self_attentive_span_extractor.SelfAttentiveSpanExtractor.SelfAttentiveSpanExtractor._global_attention", "torch.cat", "allennlp.nn.util.batched_span_select", "allennlp.nn.util.masked_softmax", "allennlp.nn.util.weighted_sum", "span_indices_mask.unsqueeze().float", "span_indices_mask.unsqueeze"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.nn.util.batched_span_select", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.nn.util.masked_softmax", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.nn.util.weighted_sum"], ["", "@", "overrides", "\n", "def", "forward", "(", "\n", "self", ",", "\n", "sequence_tensor", ":", "torch", ".", "FloatTensor", ",", "\n", "span_indices", ":", "torch", ".", "LongTensor", ",", "\n", "span_indices_mask", ":", "torch", ".", "LongTensor", "=", "None", ",", "\n", ")", "->", "torch", ".", "FloatTensor", ":", "\n", "# shape (batch_size, sequence_length, 1)", "\n", "        ", "global_attention_logits", "=", "self", ".", "_global_attention", "(", "sequence_tensor", ")", "\n", "\n", "# shape (batch_size, sequence_length, embedding_dim + 1)", "\n", "concat_tensor", "=", "torch", ".", "cat", "(", "[", "sequence_tensor", ",", "global_attention_logits", "]", ",", "-", "1", ")", "\n", "\n", "concat_output", ",", "span_mask", "=", "util", ".", "batched_span_select", "(", "concat_tensor", ",", "span_indices", ")", "\n", "\n", "# Shape: (batch_size, num_spans, max_batch_span_width, embedding_dim)", "\n", "span_embeddings", "=", "concat_output", "[", ":", ",", ":", ",", ":", ",", ":", "-", "1", "]", "\n", "# Shape: (batch_size, num_spans, max_batch_span_width)", "\n", "span_attention_logits", "=", "concat_output", "[", ":", ",", ":", ",", ":", ",", "-", "1", "]", "\n", "\n", "# Shape: (batch_size, num_spans, max_batch_span_width)", "\n", "span_attention_weights", "=", "util", ".", "masked_softmax", "(", "span_attention_logits", ",", "span_mask", ")", "\n", "\n", "# Do a weighted sum of the embedded spans with", "\n", "# respect to the normalised attention distributions.", "\n", "# Shape: (batch_size, num_spans, embedding_dim)", "\n", "attended_text_embeddings", "=", "util", ".", "weighted_sum", "(", "\n", "span_embeddings", ",", "span_attention_weights", "\n", ")", "\n", "\n", "if", "span_indices_mask", "is", "not", "None", ":", "\n", "# Above we were masking the widths of spans with respect to the max", "\n", "# span width in the batch. Here we are masking the spans which were", "\n", "# originally passed in as padding.", "\n", "            ", "return", "attended_text_embeddings", "*", "span_indices_mask", ".", "unsqueeze", "(", "-", "1", ")", ".", "float", "(", ")", "\n", "\n", "", "return", "attended_text_embeddings", "\n", "", "", ""]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.span_extractors.span_extractor.SpanExtractor.forward": [[19, 57], ["None"], "methods", ["None"], ["@", "overrides", "\n", "def", "forward", "(", "\n", "self", ",", "\n", "sequence_tensor", ":", "torch", ".", "FloatTensor", ",", "\n", "span_indices", ":", "torch", ".", "LongTensor", ",", "\n", "sequence_mask", ":", "torch", ".", "LongTensor", "=", "None", ",", "\n", "span_indices_mask", ":", "torch", ".", "LongTensor", "=", "None", ",", "\n", ")", ":", "\n", "        ", "\"\"\"\n        Given a sequence tensor, extract spans and return representations of\n        them. Span representation can be computed in many different ways,\n        such as concatenation of the start and end spans, attention over the\n        vectors contained inside the span, etc.\n\n        # Parameters\n\n        sequence_tensor : `torch.FloatTensor`, required.\n            A tensor of shape (batch_size, sequence_length, embedding_size)\n            representing an embedded sequence of words.\n        span_indices : `torch.LongTensor`, required.\n            A tensor of shape `(batch_size, num_spans, 2)`, where the last\n            dimension represents the inclusive start and end indices of the\n            span to be extracted from the `sequence_tensor`.\n        sequence_mask : `torch.LongTensor`, optional (default = `None`).\n            A tensor of shape (batch_size, sequence_length) representing padded\n            elements of the sequence.\n        span_indices_mask : `torch.LongTensor`, optional (default = `None`).\n            A tensor of shape (batch_size, num_spans) representing the valid\n            spans in the `indices` tensor. This mask is optional because\n            sometimes it's easier to worry about masking after calling this\n            function, rather than passing a mask directly.\n\n        # Returns\n\n        A tensor of shape `(batch_size, num_spans, embedded_span_size)`,\n        where `embedded_span_size` depends on the way spans are represented.\n        \"\"\"", "\n", "raise", "NotImplementedError", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.span_extractors.span_extractor.SpanExtractor.get_input_dim": [[58, 63], ["None"], "methods", ["None"], ["", "def", "get_input_dim", "(", "self", ")", "->", "int", ":", "\n", "        ", "\"\"\"\n        Returns the expected final dimension of the `sequence_tensor`.\n        \"\"\"", "\n", "raise", "NotImplementedError", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.span_extractors.span_extractor.SpanExtractor.get_output_dim": [[64, 69], ["None"], "methods", ["None"], ["", "def", "get_output_dim", "(", "self", ")", "->", "int", ":", "\n", "        ", "\"\"\"\n        Returns the expected final dimension of the returned span representation.\n        \"\"\"", "\n", "raise", "NotImplementedError", "\n", "", "", ""]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.span_extractors.endpoint_span_extractor.EndpointSpanExtractor.__init__": [[52, 82], ["allennlp.modules.span_extractors.span_extractor.SpanExtractor.__init__", "torch.nn.parameter.Parameter", "allennlp.modules.token_embedders.embedding.Embedding", "torch.randn", "all", "allennlp.common.checks.ConfigurationError", "int"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.graph_util.plot.GraphPlot.__init__"], ["def", "__init__", "(", "\n", "self", ",", "\n", "input_dim", ":", "int", ",", "\n", "combination", ":", "str", "=", "\"x,y\"", ",", "\n", "num_width_embeddings", ":", "int", "=", "None", ",", "\n", "span_width_embedding_dim", ":", "int", "=", "None", ",", "\n", "bucket_widths", ":", "bool", "=", "False", ",", "\n", "use_exclusive_start_indices", ":", "bool", "=", "False", ",", "\n", ")", "->", "None", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "_input_dim", "=", "input_dim", "\n", "self", ".", "_combination", "=", "combination", "\n", "self", ".", "_num_width_embeddings", "=", "num_width_embeddings", "\n", "self", ".", "_bucket_widths", "=", "bucket_widths", "\n", "\n", "self", ".", "_use_exclusive_start_indices", "=", "use_exclusive_start_indices", "\n", "if", "use_exclusive_start_indices", ":", "\n", "            ", "self", ".", "_start_sentinel", "=", "Parameter", "(", "torch", ".", "randn", "(", "[", "1", ",", "1", ",", "int", "(", "input_dim", ")", "]", ")", ")", "\n", "\n", "", "if", "num_width_embeddings", "is", "not", "None", "and", "span_width_embedding_dim", "is", "not", "None", ":", "\n", "            ", "self", ".", "_span_width_embedding", "=", "Embedding", "(", "\n", "num_width_embeddings", ",", "span_width_embedding_dim", "\n", ")", "\n", "", "elif", "not", "all", "(", "[", "num_width_embeddings", "is", "None", ",", "span_width_embedding_dim", "is", "None", "]", ")", ":", "\n", "            ", "raise", "ConfigurationError", "(", "\n", "\"To use a span width embedding representation, you must\"", "\n", "\"specify both num_width_buckets and span_width_embedding_dim.\"", "\n", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "_span_width_embedding", "=", "None", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.span_extractors.endpoint_span_extractor.EndpointSpanExtractor.get_input_dim": [[83, 85], ["None"], "methods", ["None"], ["", "", "def", "get_input_dim", "(", "self", ")", "->", "int", ":", "\n", "        ", "return", "self", ".", "_input_dim", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.span_extractors.endpoint_span_extractor.EndpointSpanExtractor.get_output_dim": [[86, 93], ["allennlp.nn.util.get_combined_dim", "endpoint_span_extractor.EndpointSpanExtractor._span_width_embedding.get_output_dim"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.nn.util.get_combined_dim", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.seq2seq_decoders.auto_regressive_seq_decoder.AutoRegressiveSeqDecoder.get_output_dim"], ["", "def", "get_output_dim", "(", "self", ")", "->", "int", ":", "\n", "        ", "combined_dim", "=", "util", ".", "get_combined_dim", "(", "\n", "self", ".", "_combination", ",", "[", "self", ".", "_input_dim", ",", "self", ".", "_input_dim", "]", "\n", ")", "\n", "if", "self", ".", "_span_width_embedding", "is", "not", "None", ":", "\n", "            ", "return", "combined_dim", "+", "self", ".", "_span_width_embedding", ".", "get_output_dim", "(", ")", "\n", "", "return", "combined_dim", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.span_extractors.endpoint_span_extractor.EndpointSpanExtractor.forward": [[94, 176], ["allennlp.nn.util.combine_tensors", "index.squeeze", "allennlp.nn.util.batched_index_select", "allennlp.nn.util.batched_index_select", "allennlp.nn.util.batched_index_select", "allennlp.nn.util.batched_index_select", "start_sentinel_mask.float", "endpoint_span_extractor.EndpointSpanExtractor._span_width_embedding", "torch.cat", "span_indices.split", "sequence_tensor.size", "ValueError", "ValueError", "allennlp.nn.util.bucket_values", "span_indices_mask.unsqueeze().float", "start_sentinel_mask.squeeze", "span_indices_mask.unsqueeze", "sequence_tensor.size"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.nn.util.combine_tensors", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.nn.util.batched_index_select", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.nn.util.batched_index_select", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.nn.util.batched_index_select", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.nn.util.batched_index_select", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.nn.util.bucket_values"], ["", "@", "overrides", "\n", "def", "forward", "(", "\n", "self", ",", "\n", "sequence_tensor", ":", "torch", ".", "FloatTensor", ",", "\n", "span_indices", ":", "torch", ".", "LongTensor", ",", "\n", "sequence_mask", ":", "torch", ".", "LongTensor", "=", "None", ",", "\n", "span_indices_mask", ":", "torch", ".", "LongTensor", "=", "None", ",", "\n", ")", "->", "None", ":", "\n", "# shape (batch_size, num_spans)", "\n", "        ", "span_starts", ",", "span_ends", "=", "[", "\n", "index", ".", "squeeze", "(", "-", "1", ")", "for", "index", "in", "span_indices", ".", "split", "(", "1", ",", "dim", "=", "-", "1", ")", "\n", "]", "\n", "\n", "if", "span_indices_mask", "is", "not", "None", ":", "\n", "# It's not strictly necessary to multiply the span indices by the mask here,", "\n", "# but it's possible that the span representation was padded with something other", "\n", "# than 0 (such as -1, which would be an invalid index), so we do so anyway to", "\n", "# be safe.", "\n", "            ", "span_starts", "=", "span_starts", "*", "span_indices_mask", "\n", "span_ends", "=", "span_ends", "*", "span_indices_mask", "\n", "\n", "", "if", "not", "self", ".", "_use_exclusive_start_indices", ":", "\n", "            ", "if", "sequence_tensor", ".", "size", "(", "-", "1", ")", "!=", "self", ".", "_input_dim", ":", "\n", "                ", "raise", "ValueError", "(", "\n", "f\"Dimension mismatch expected ({sequence_tensor.size(-1)}) \"", "\n", "f\"received ({self._input_dim}).\"", "\n", ")", "\n", "", "start_embeddings", "=", "util", ".", "batched_index_select", "(", "sequence_tensor", ",", "span_starts", ")", "\n", "end_embeddings", "=", "util", ".", "batched_index_select", "(", "sequence_tensor", ",", "span_ends", ")", "\n", "\n", "", "else", ":", "\n", "# We want `exclusive` span starts, so we remove 1 from the forward span starts", "\n", "# as the AllenNLP `SpanField` is inclusive.", "\n", "# shape (batch_size, num_spans)", "\n", "            ", "exclusive_span_starts", "=", "span_starts", "-", "1", "\n", "# shape (batch_size, num_spans, 1)", "\n", "start_sentinel_mask", "=", "(", "exclusive_span_starts", "==", "-", "1", ")", ".", "long", "(", ")", ".", "unsqueeze", "(", "-", "1", ")", "\n", "exclusive_span_starts", "=", "exclusive_span_starts", "*", "(", "\n", "1", "-", "start_sentinel_mask", ".", "squeeze", "(", "-", "1", ")", "\n", ")", "\n", "\n", "# We'll check the indices here at runtime, because it's difficult to debug", "\n", "# if this goes wrong and it's tricky to get right.", "\n", "if", "(", "exclusive_span_starts", "<", "0", ")", ".", "any", "(", ")", ":", "\n", "                ", "raise", "ValueError", "(", "\n", "f\"Adjusted span indices must lie inside the the sequence tensor, \"", "\n", "f\"but found: exclusive_span_starts: {exclusive_span_starts}.\"", "\n", ")", "\n", "\n", "", "start_embeddings", "=", "util", ".", "batched_index_select", "(", "\n", "sequence_tensor", ",", "exclusive_span_starts", "\n", ")", "\n", "end_embeddings", "=", "util", ".", "batched_index_select", "(", "sequence_tensor", ",", "span_ends", ")", "\n", "\n", "# We're using sentinels, so we need to replace all the elements which were", "\n", "# outside the dimensions of the sequence_tensor with the start sentinel.", "\n", "float_start_sentinel_mask", "=", "start_sentinel_mask", ".", "float", "(", ")", "\n", "start_embeddings", "=", "(", "\n", "start_embeddings", "*", "(", "1", "-", "float_start_sentinel_mask", ")", "\n", "+", "float_start_sentinel_mask", "*", "self", ".", "_start_sentinel", "\n", ")", "\n", "\n", "", "combined_tensors", "=", "util", ".", "combine_tensors", "(", "\n", "self", ".", "_combination", ",", "[", "start_embeddings", ",", "end_embeddings", "]", "\n", ")", "\n", "if", "self", ".", "_span_width_embedding", "is", "not", "None", ":", "\n", "# Embed the span widths and concatenate to the rest of the representations.", "\n", "            ", "if", "self", ".", "_bucket_widths", ":", "\n", "                ", "span_widths", "=", "util", ".", "bucket_values", "(", "\n", "span_ends", "-", "span_starts", ",", "\n", "num_total_buckets", "=", "self", ".", "_num_width_embeddings", ",", "\n", ")", "\n", "", "else", ":", "\n", "                ", "span_widths", "=", "span_ends", "-", "span_starts", "\n", "\n", "", "span_width_embeddings", "=", "self", ".", "_span_width_embedding", "(", "span_widths", ")", "\n", "combined_tensors", "=", "torch", ".", "cat", "(", "[", "combined_tensors", ",", "span_width_embeddings", "]", ",", "-", "1", ")", "\n", "\n", "", "if", "span_indices_mask", "is", "not", "None", ":", "\n", "            ", "return", "combined_tensors", "*", "span_indices_mask", ".", "unsqueeze", "(", "-", "1", ")", ".", "float", "(", ")", "\n", "\n", "", "return", "combined_tensors", "\n", "", "", ""]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.span_extractors.bidirectional_endpoint_span_extractor.BidirectionalEndpointSpanExtractor.__init__": [[68, 107], ["allennlp.modules.span_extractors.span_extractor.SpanExtractor.__init__", "allennlp.common.checks.ConfigurationError", "allennlp.modules.token_embedders.embedding.Embedding", "torch.nn.parameter.Parameter", "torch.nn.parameter.Parameter", "all", "allennlp.common.checks.ConfigurationError", "torch.randn", "torch.randn", "int", "int"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.graph_util.plot.GraphPlot.__init__"], ["def", "__init__", "(", "\n", "self", ",", "\n", "input_dim", ":", "int", ",", "\n", "forward_combination", ":", "str", "=", "\"y-x\"", ",", "\n", "backward_combination", ":", "str", "=", "\"x-y\"", ",", "\n", "num_width_embeddings", ":", "int", "=", "None", ",", "\n", "span_width_embedding_dim", ":", "int", "=", "None", ",", "\n", "bucket_widths", ":", "bool", "=", "False", ",", "\n", "use_sentinels", ":", "bool", "=", "True", ",", "\n", ")", "->", "None", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "_input_dim", "=", "input_dim", "\n", "self", ".", "_forward_combination", "=", "forward_combination", "\n", "self", ".", "_backward_combination", "=", "backward_combination", "\n", "self", ".", "_num_width_embeddings", "=", "num_width_embeddings", "\n", "self", ".", "_bucket_widths", "=", "bucket_widths", "\n", "\n", "if", "self", ".", "_input_dim", "%", "2", "!=", "0", ":", "\n", "            ", "raise", "ConfigurationError", "(", "\n", "\"The input dimension is not divisible by 2, but the \"", "\n", "\"BidirectionalEndpointSpanExtractor assumes the embedded representation \"", "\n", "\"is bidirectional (and hence divisible by 2).\"", "\n", ")", "\n", "", "if", "num_width_embeddings", "is", "not", "None", "and", "span_width_embedding_dim", "is", "not", "None", ":", "\n", "            ", "self", ".", "_span_width_embedding", "=", "Embedding", "(", "\n", "num_width_embeddings", ",", "span_width_embedding_dim", "\n", ")", "\n", "", "elif", "not", "all", "(", "[", "num_width_embeddings", "is", "None", ",", "span_width_embedding_dim", "is", "None", "]", ")", ":", "\n", "            ", "raise", "ConfigurationError", "(", "\n", "\"To use a span width embedding representation, you must\"", "\n", "\"specify both num_width_buckets and span_width_embedding_dim.\"", "\n", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "_span_width_embedding", "=", "None", "\n", "\n", "", "self", ".", "_use_sentinels", "=", "use_sentinels", "\n", "if", "use_sentinels", ":", "\n", "            ", "self", ".", "_start_sentinel", "=", "Parameter", "(", "torch", ".", "randn", "(", "[", "1", ",", "1", ",", "int", "(", "input_dim", "/", "2", ")", "]", ")", ")", "\n", "self", ".", "_end_sentinel", "=", "Parameter", "(", "torch", ".", "randn", "(", "[", "1", ",", "1", ",", "int", "(", "input_dim", "/", "2", ")", "]", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.span_extractors.bidirectional_endpoint_span_extractor.BidirectionalEndpointSpanExtractor.get_input_dim": [[108, 110], ["None"], "methods", ["None"], ["", "", "def", "get_input_dim", "(", "self", ")", "->", "int", ":", "\n", "        ", "return", "self", ".", "_input_dim", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.span_extractors.bidirectional_endpoint_span_extractor.BidirectionalEndpointSpanExtractor.get_output_dim": [[111, 126], ["int", "allennlp.nn.util.get_combined_dim", "allennlp.nn.util.get_combined_dim", "bidirectional_endpoint_span_extractor.BidirectionalEndpointSpanExtractor._span_width_embedding.get_output_dim"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.nn.util.get_combined_dim", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.nn.util.get_combined_dim", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.seq2seq_decoders.auto_regressive_seq_decoder.AutoRegressiveSeqDecoder.get_output_dim"], ["", "def", "get_output_dim", "(", "self", ")", "->", "int", ":", "\n", "        ", "unidirectional_dim", "=", "int", "(", "self", ".", "_input_dim", "/", "2", ")", "\n", "forward_combined_dim", "=", "util", ".", "get_combined_dim", "(", "\n", "self", ".", "_forward_combination", ",", "[", "unidirectional_dim", ",", "unidirectional_dim", "]", "\n", ")", "\n", "backward_combined_dim", "=", "util", ".", "get_combined_dim", "(", "\n", "self", ".", "_backward_combination", ",", "[", "unidirectional_dim", ",", "unidirectional_dim", "]", "\n", ")", "\n", "if", "self", ".", "_span_width_embedding", "is", "not", "None", ":", "\n", "            ", "return", "(", "\n", "forward_combined_dim", "\n", "+", "backward_combined_dim", "\n", "+", "self", ".", "_span_width_embedding", ".", "get_output_dim", "(", ")", "\n", ")", "\n", "", "return", "forward_combined_dim", "+", "backward_combined_dim", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.span_extractors.bidirectional_endpoint_span_extractor.BidirectionalEndpointSpanExtractor.forward": [[127, 265], ["sequence_tensor.split", "forward_sequence.contiguous.contiguous.contiguous", "backward_sequence.contiguous.contiguous.contiguous", "allennlp.nn.util.batched_index_select", "allennlp.nn.util.batched_index_select", "allennlp.nn.util.batched_index_select", "allennlp.nn.util.batched_index_select", "allennlp.nn.util.combine_tensors", "allennlp.nn.util.combine_tensors", "torch.cat", "int", "index.squeeze", "allennlp.nn.util.get_lengths_from_binary_sequence_mask", "ValueError", "end_sentinel_mask.float", "start_sentinel_mask.float", "bidirectional_endpoint_span_extractor.BidirectionalEndpointSpanExtractor._span_width_embedding", "torch.cat", "span_indices.split", "torch.ones_like", "sequence_tensor.size", "end_sentinel_mask.squeeze", "start_sentinel_mask.squeeze", "allennlp.nn.util.bucket_values", "span_indices_mask.float().unsqueeze", "allennlp.nn.util.get_lengths_from_binary_sequence_mask.unsqueeze", "span_indices_mask.float", "allennlp.nn.util.get_lengths_from_binary_sequence_mask.unsqueeze"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.nn.util.batched_index_select", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.nn.util.batched_index_select", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.nn.util.batched_index_select", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.nn.util.batched_index_select", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.nn.util.combine_tensors", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.nn.util.combine_tensors", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.nn.util.get_lengths_from_binary_sequence_mask", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.nn.util.bucket_values"], ["", "@", "overrides", "\n", "def", "forward", "(", "\n", "self", ",", "\n", "sequence_tensor", ":", "torch", ".", "FloatTensor", ",", "\n", "span_indices", ":", "torch", ".", "LongTensor", ",", "\n", "sequence_mask", ":", "torch", ".", "LongTensor", "=", "None", ",", "\n", "span_indices_mask", ":", "torch", ".", "LongTensor", "=", "None", ",", "\n", ")", "->", "torch", ".", "FloatTensor", ":", "\n", "\n", "# Both of shape (batch_size, sequence_length, embedding_size / 2)", "\n", "        ", "forward_sequence", ",", "backward_sequence", "=", "sequence_tensor", ".", "split", "(", "\n", "int", "(", "self", ".", "_input_dim", "/", "2", ")", ",", "dim", "=", "-", "1", "\n", ")", "\n", "forward_sequence", "=", "forward_sequence", ".", "contiguous", "(", ")", "\n", "backward_sequence", "=", "backward_sequence", ".", "contiguous", "(", ")", "\n", "\n", "# shape (batch_size, num_spans)", "\n", "span_starts", ",", "span_ends", "=", "[", "\n", "index", ".", "squeeze", "(", "-", "1", ")", "for", "index", "in", "span_indices", ".", "split", "(", "1", ",", "dim", "=", "-", "1", ")", "\n", "]", "\n", "\n", "if", "span_indices_mask", "is", "not", "None", ":", "\n", "            ", "span_starts", "=", "span_starts", "*", "span_indices_mask", "\n", "span_ends", "=", "span_ends", "*", "span_indices_mask", "\n", "# We want `exclusive` span starts, so we remove 1 from the forward span starts", "\n", "# as the AllenNLP `SpanField` is inclusive.", "\n", "# shape (batch_size, num_spans)", "\n", "", "exclusive_span_starts", "=", "span_starts", "-", "1", "\n", "# shape (batch_size, num_spans, 1)", "\n", "start_sentinel_mask", "=", "(", "exclusive_span_starts", "==", "-", "1", ")", ".", "long", "(", ")", ".", "unsqueeze", "(", "-", "1", ")", "\n", "\n", "# We want `exclusive` span ends for the backward direction", "\n", "# (so that the `start` of the span in that direction is exlusive), so", "\n", "# we add 1 to the span ends as the AllenNLP `SpanField` is inclusive.", "\n", "exclusive_span_ends", "=", "span_ends", "+", "1", "\n", "\n", "if", "sequence_mask", "is", "not", "None", ":", "\n", "# shape (batch_size)", "\n", "            ", "sequence_lengths", "=", "util", ".", "get_lengths_from_binary_sequence_mask", "(", "sequence_mask", ")", "\n", "", "else", ":", "\n", "# shape (batch_size), filled with the sequence length size of the sequence_tensor.", "\n", "            ", "sequence_lengths", "=", "torch", ".", "ones_like", "(", "\n", "sequence_tensor", "[", ":", ",", "0", ",", "0", "]", ",", "dtype", "=", "torch", ".", "long", "\n", ")", "*", "sequence_tensor", ".", "size", "(", "1", ")", "\n", "\n", "# shape (batch_size, num_spans, 1)", "\n", "", "end_sentinel_mask", "=", "(", "\n", "(", "exclusive_span_ends", ">=", "sequence_lengths", ".", "unsqueeze", "(", "-", "1", ")", ")", ".", "long", "(", ")", ".", "unsqueeze", "(", "-", "1", ")", "\n", ")", "\n", "\n", "# As we added 1 to the span_ends to make them exclusive, which might have caused indices", "\n", "# equal to the sequence_length to become out of bounds, we multiply by the inverse of the", "\n", "# end_sentinel mask to erase these indices (as we will replace them anyway in the block below).", "\n", "# The same argument follows for the exclusive span start indices.", "\n", "exclusive_span_ends", "=", "exclusive_span_ends", "*", "(", "1", "-", "end_sentinel_mask", ".", "squeeze", "(", "-", "1", ")", ")", "\n", "exclusive_span_starts", "=", "exclusive_span_starts", "*", "(", "\n", "1", "-", "start_sentinel_mask", ".", "squeeze", "(", "-", "1", ")", "\n", ")", "\n", "\n", "# We'll check the indices here at runtime, because it's difficult to debug", "\n", "# if this goes wrong and it's tricky to get right.", "\n", "if", "(", "exclusive_span_starts", "<", "0", ")", ".", "any", "(", ")", "or", "(", "\n", "exclusive_span_ends", ">", "sequence_lengths", ".", "unsqueeze", "(", "-", "1", ")", "\n", ")", ".", "any", "(", ")", ":", "\n", "            ", "raise", "ValueError", "(", "\n", "f\"Adjusted span indices must lie inside the length of the sequence tensor, \"", "\n", "f\"but found: exclusive_span_starts: {exclusive_span_starts}, \"", "\n", "f\"exclusive_span_ends: {exclusive_span_ends} for a sequence tensor with lengths \"", "\n", "f\"{sequence_lengths}.\"", "\n", ")", "\n", "\n", "# Forward Direction: start indices are exclusive. Shape (batch_size, num_spans, input_size / 2)", "\n", "", "forward_start_embeddings", "=", "util", ".", "batched_index_select", "(", "\n", "forward_sequence", ",", "exclusive_span_starts", "\n", ")", "\n", "# Forward Direction: end indices are inclusive, so we can just use span_ends.", "\n", "# Shape (batch_size, num_spans, input_size / 2)", "\n", "forward_end_embeddings", "=", "util", ".", "batched_index_select", "(", "forward_sequence", ",", "span_ends", ")", "\n", "\n", "# Backward Direction: The backward start embeddings use the `forward` end", "\n", "# indices, because we are going backwards.", "\n", "# Shape (batch_size, num_spans, input_size / 2)", "\n", "backward_start_embeddings", "=", "util", ".", "batched_index_select", "(", "\n", "backward_sequence", ",", "exclusive_span_ends", "\n", ")", "\n", "# Backward Direction: The backward end embeddings use the `forward` start", "\n", "# indices, because we are going backwards.", "\n", "# Shape (batch_size, num_spans, input_size / 2)", "\n", "backward_end_embeddings", "=", "util", ".", "batched_index_select", "(", "\n", "backward_sequence", ",", "span_starts", "\n", ")", "\n", "\n", "if", "self", ".", "_use_sentinels", ":", "\n", "# If we're using sentinels, we need to replace all the elements which were", "\n", "# outside the dimensions of the sequence_tensor with either the start sentinel,", "\n", "# or the end sentinel.", "\n", "            ", "float_end_sentinel_mask", "=", "end_sentinel_mask", ".", "float", "(", ")", "\n", "float_start_sentinel_mask", "=", "start_sentinel_mask", ".", "float", "(", ")", "\n", "forward_start_embeddings", "=", "(", "\n", "forward_start_embeddings", "*", "(", "1", "-", "float_start_sentinel_mask", ")", "\n", "+", "float_start_sentinel_mask", "*", "self", ".", "_start_sentinel", "\n", ")", "\n", "backward_start_embeddings", "=", "(", "\n", "backward_start_embeddings", "*", "(", "1", "-", "float_end_sentinel_mask", ")", "\n", "+", "float_end_sentinel_mask", "*", "self", ".", "_end_sentinel", "\n", ")", "\n", "\n", "# Now we combine the forward and backward spans in the manner specified by the", "\n", "# respective combinations and concatenate these representations.", "\n", "# Shape (batch_size, num_spans, forward_combination_dim)", "\n", "", "forward_spans", "=", "util", ".", "combine_tensors", "(", "\n", "self", ".", "_forward_combination", ",", "\n", "[", "forward_start_embeddings", ",", "forward_end_embeddings", "]", ",", "\n", ")", "\n", "# Shape (batch_size, num_spans, backward_combination_dim)", "\n", "backward_spans", "=", "util", ".", "combine_tensors", "(", "\n", "self", ".", "_backward_combination", ",", "\n", "[", "backward_start_embeddings", ",", "backward_end_embeddings", "]", ",", "\n", ")", "\n", "# Shape (batch_size, num_spans, forward_combination_dim + backward_combination_dim)", "\n", "span_embeddings", "=", "torch", ".", "cat", "(", "[", "forward_spans", ",", "backward_spans", "]", ",", "-", "1", ")", "\n", "\n", "if", "self", ".", "_span_width_embedding", "is", "not", "None", ":", "\n", "# Embed the span widths and concatenate to the rest of the representations.", "\n", "            ", "if", "self", ".", "_bucket_widths", ":", "\n", "                ", "span_widths", "=", "util", ".", "bucket_values", "(", "\n", "span_ends", "-", "span_starts", ",", "\n", "num_total_buckets", "=", "self", ".", "_num_width_embeddings", ",", "\n", ")", "\n", "", "else", ":", "\n", "                ", "span_widths", "=", "span_ends", "-", "span_starts", "\n", "\n", "", "span_width_embeddings", "=", "self", ".", "_span_width_embedding", "(", "span_widths", ")", "\n", "return", "torch", ".", "cat", "(", "[", "span_embeddings", ",", "span_width_embeddings", "]", ",", "-", "1", ")", "\n", "\n", "", "if", "span_indices_mask", "is", "not", "None", ":", "\n", "            ", "return", "span_embeddings", "*", "span_indices_mask", ".", "float", "(", ")", ".", "unsqueeze", "(", "-", "1", ")", "\n", "", "return", "span_embeddings", "\n", "", "", ""]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.language_model_heads.bert.BertLanguageModelHead.__init__": [[17, 28], ["allennlp.modules.language_model_heads.language_model_head.LanguageModelHead.__init__", "transformers.BertConfig.from_pretrained", "transformers.BertForMaskedLM.from_pretrained"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.graph_util.plot.GraphPlot.__init__"], ["def", "__init__", "(", "self", ",", "model_name", ":", "str", ")", "->", "None", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "config", "=", "BertConfig", ".", "from_pretrained", "(", "model_name", ")", "\n", "self", ".", "input_dim", "=", "config", ".", "hidden_size", "\n", "self", ".", "output_dim", "=", "config", ".", "vocab_size", "\n", "# TODO(mattg): It's possible that we could use some kind of cache like we have in", "\n", "# allennlp.modules.token_embedders.bert_token_embedder.PretrainedBertModel.  That way, we", "\n", "# would only load the BERT weights once.  Though, it's not clear how to do that here, as we", "\n", "# need to load `BertForMaskedLM`, not just `BertModel`...", "\n", "bert_model", "=", "BertForMaskedLM", ".", "from_pretrained", "(", "model_name", ")", "\n", "self", ".", "bert_lm_head", "=", "bert_model", ".", "cls", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.language_model_heads.bert.BertLanguageModelHead.get_input_dim": [[29, 32], ["None"], "methods", ["None"], ["", "@", "overrides", "\n", "def", "get_input_dim", "(", "self", ")", "->", "int", ":", "\n", "        ", "return", "self", ".", "input_dim", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.language_model_heads.bert.BertLanguageModelHead.get_output_dim": [[33, 36], ["None"], "methods", ["None"], ["", "@", "overrides", "\n", "def", "get_output_dim", "(", "self", ")", "->", "int", ":", "\n", "        ", "return", "self", ".", "output_dim", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.language_model_heads.bert.BertLanguageModelHead.forward": [[37, 39], ["bert.BertLanguageModelHead.bert_lm_head"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "hidden_states", ":", "torch", ".", "Tensor", ")", "->", "torch", ".", "Tensor", ":", "\n", "        ", "return", "self", ".", "bert_lm_head", "(", "hidden_states", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.language_model_heads.gpt2.Gpt2LanguageModelHead.__init__": [[17, 28], ["allennlp.modules.language_model_heads.language_model_head.LanguageModelHead.__init__", "transformers.GPT2Config.from_pretrained", "transformers.GPT2LMHeadModel.from_pretrained"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.graph_util.plot.GraphPlot.__init__"], ["def", "__init__", "(", "self", ",", "model_name", ":", "str", ")", "->", "None", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "config", "=", "GPT2Config", ".", "from_pretrained", "(", "model_name", ")", "\n", "self", ".", "input_dim", "=", "config", ".", "hidden_size", "\n", "self", ".", "output_dim", "=", "config", ".", "vocab_size", "\n", "# TODO(mattg): It's possible that we could use some kind of cache like we have in", "\n", "# allennlp.modules.token_embedders.bert_token_embedder.PretrainedBertModel.  That way, we", "\n", "# would only load the GPT2 weights once.  Though, it's not clear how to do that here, as we", "\n", "# need to load `GPT2LMHeadModel`, not just `GPT2Model`...", "\n", "gpt2_model", "=", "GPT2LMHeadModel", ".", "from_pretrained", "(", "model_name", ")", "\n", "self", ".", "gpt2_lm_head", "=", "gpt2_model", ".", "lm_head", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.language_model_heads.gpt2.Gpt2LanguageModelHead.get_input_dim": [[29, 32], ["None"], "methods", ["None"], ["", "@", "overrides", "\n", "def", "get_input_dim", "(", "self", ")", "->", "int", ":", "\n", "        ", "return", "self", ".", "input_dim", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.language_model_heads.gpt2.Gpt2LanguageModelHead.get_output_dim": [[33, 36], ["None"], "methods", ["None"], ["", "@", "overrides", "\n", "def", "get_output_dim", "(", "self", ")", "->", "int", ":", "\n", "        ", "return", "self", ".", "output_dim", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.language_model_heads.gpt2.Gpt2LanguageModelHead.forward": [[37, 39], ["gpt2.Gpt2LanguageModelHead.gpt2_lm_head"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "hidden_states", ":", "torch", ".", "Tensor", ")", "->", "torch", ".", "Tensor", ":", "\n", "        ", "return", "self", ".", "gpt2_lm_head", "(", "hidden_states", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.language_model_heads.language_model_head.LanguageModelHead.get_input_dim": [[12, 14], ["None"], "methods", ["None"], ["def", "get_input_dim", "(", "self", ")", "->", "int", ":", "\n", "        ", "raise", "NotImplementedError", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.language_model_heads.language_model_head.LanguageModelHead.get_output_dim": [[15, 17], ["None"], "methods", ["None"], ["", "def", "get_output_dim", "(", "self", ")", "->", "int", ":", "\n", "        ", "raise", "NotImplementedError", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.language_model_heads.language_model_head.LanguageModelHead.forward": [[18, 21], ["None"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "hidden_states", ":", "torch", ".", "Tensor", ")", "->", "torch", ".", "Tensor", ":", "# type: ignore", "\n", "\n", "        ", "raise", "NotImplementedError", "\n", "", "", ""]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.seq2vec_encoders.cnn_encoder.CnnEncoder.__init__": [[51, 83], ["allennlp.modules.seq2vec_encoders.seq2vec_encoder.Seq2VecEncoder.__init__", "enumerate", "torch.nn.Conv1d", "cnn_encoder.CnnEncoder.add_module", "len", "torch.nn.Linear", "allennlp.nn.Activation.by_name"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.graph_util.plot.GraphPlot.__init__", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.registrable.Registrable.by_name"], ["def", "__init__", "(", "\n", "self", ",", "\n", "embedding_dim", ":", "int", ",", "\n", "num_filters", ":", "int", ",", "\n", "ngram_filter_sizes", ":", "Tuple", "[", "int", ",", "...", "]", "=", "(", "2", ",", "3", ",", "4", ",", "5", ")", ",", "\n", "conv_layer_activation", ":", "Activation", "=", "None", ",", "\n", "output_dim", ":", "Optional", "[", "int", "]", "=", "None", ",", "\n", ")", "->", "None", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "_embedding_dim", "=", "embedding_dim", "\n", "self", ".", "_num_filters", "=", "num_filters", "\n", "self", ".", "_ngram_filter_sizes", "=", "ngram_filter_sizes", "\n", "self", ".", "_activation", "=", "conv_layer_activation", "or", "Activation", ".", "by_name", "(", "\"relu\"", ")", "(", ")", "\n", "self", ".", "_output_dim", "=", "output_dim", "\n", "\n", "self", ".", "_convolution_layers", "=", "[", "\n", "Conv1d", "(", "\n", "in_channels", "=", "self", ".", "_embedding_dim", ",", "\n", "out_channels", "=", "self", ".", "_num_filters", ",", "\n", "kernel_size", "=", "ngram_size", ",", "\n", ")", "\n", "for", "ngram_size", "in", "self", ".", "_ngram_filter_sizes", "\n", "]", "\n", "for", "i", ",", "conv_layer", "in", "enumerate", "(", "self", ".", "_convolution_layers", ")", ":", "\n", "            ", "self", ".", "add_module", "(", "\"conv_layer_%d\"", "%", "i", ",", "conv_layer", ")", "\n", "\n", "", "maxpool_output_dim", "=", "self", ".", "_num_filters", "*", "len", "(", "self", ".", "_ngram_filter_sizes", ")", "\n", "if", "self", ".", "_output_dim", ":", "\n", "            ", "self", ".", "projection_layer", "=", "Linear", "(", "maxpool_output_dim", ",", "self", ".", "_output_dim", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "projection_layer", "=", "None", "\n", "self", ".", "_output_dim", "=", "maxpool_output_dim", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.seq2vec_encoders.cnn_encoder.CnnEncoder.get_input_dim": [[84, 87], ["None"], "methods", ["None"], ["", "", "@", "overrides", "\n", "def", "get_input_dim", "(", "self", ")", "->", "int", ":", "\n", "        ", "return", "self", ".", "_embedding_dim", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.seq2vec_encoders.cnn_encoder.CnnEncoder.get_output_dim": [[88, 91], ["None"], "methods", ["None"], ["", "@", "overrides", "\n", "def", "get_output_dim", "(", "self", ")", "->", "int", ":", "\n", "        ", "return", "self", ".", "_output_dim", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.seq2vec_encoders.cnn_encoder.CnnEncoder.forward": [[92, 128], ["torch.transpose", "range", "len", "getattr", "filter_outputs.append", "torch.cat", "cnn_encoder.CnnEncoder.projection_layer", "mask.unsqueeze().float", "len", "cnn_encoder.CnnEncoder._activation().max", "mask.unsqueeze", "cnn_encoder.CnnEncoder._activation", "getattr."], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "tokens", ":", "torch", ".", "Tensor", ",", "mask", ":", "torch", ".", "Tensor", ")", ":", "\n", "        ", "if", "mask", "is", "not", "None", ":", "\n", "            ", "tokens", "=", "tokens", "*", "mask", ".", "unsqueeze", "(", "-", "1", ")", ".", "float", "(", ")", "\n", "\n", "# Our input is expected to have shape `(batch_size, num_tokens, embedding_dim)`.  The", "\n", "# convolution layers expect input of shape `(batch_size, in_channels, sequence_length)`,", "\n", "# where the conv layer `in_channels` is our `embedding_dim`.  We thus need to transpose the", "\n", "# tensor first.", "\n", "", "tokens", "=", "torch", ".", "transpose", "(", "tokens", ",", "1", ",", "2", ")", "\n", "# Each convolution layer returns output of size `(batch_size, num_filters, pool_length)`,", "\n", "# where `pool_length = num_tokens - ngram_size + 1`.  We then do an activation function,", "\n", "# then do max pooling over each filter for the whole input sequence.  Because our max", "\n", "# pooling is simple, we just use `torch.max`.  The resultant tensor of has shape", "\n", "# `(batch_size, num_conv_layers * num_filters)`, which then gets projected using the", "\n", "# projection layer, if requested.", "\n", "\n", "filter_outputs", "=", "[", "]", "\n", "for", "i", "in", "range", "(", "len", "(", "self", ".", "_convolution_layers", ")", ")", ":", "\n", "            ", "convolution_layer", "=", "getattr", "(", "self", ",", "\"conv_layer_{}\"", ".", "format", "(", "i", ")", ")", "\n", "filter_outputs", ".", "append", "(", "\n", "self", ".", "_activation", "(", "convolution_layer", "(", "tokens", ")", ")", ".", "max", "(", "dim", "=", "2", ")", "[", "0", "]", "\n", ")", "\n", "\n", "# Now we have a list of `num_conv_layers` tensors of shape `(batch_size, num_filters)`.", "\n", "# Concatenating them gives us a tensor of shape `(batch_size, num_filters * num_conv_layers)`.", "\n", "", "maxpool_output", "=", "(", "\n", "torch", ".", "cat", "(", "filter_outputs", ",", "dim", "=", "1", ")", "\n", "if", "len", "(", "filter_outputs", ")", ">", "1", "\n", "else", "filter_outputs", "[", "0", "]", "\n", ")", "\n", "\n", "if", "self", ".", "projection_layer", ":", "\n", "            ", "result", "=", "self", ".", "projection_layer", "(", "maxpool_output", ")", "\n", "", "else", ":", "\n", "            ", "result", "=", "maxpool_output", "\n", "", "return", "result", "\n", "", "", ""]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.seq2vec_encoders.bert_pooler.BertPooler.__init__": [[39, 58], ["allennlp.modules.seq2vec_encoders.seq2vec_encoder.Seq2VecEncoder.__init__", "isinstance", "torch.nn.Dropout", "torch.nn.Dropout", "torch.nn.Dropout", "torch.nn.Dropout", "bert_pooler.BertPooler.pooler.parameters", "allennlp.modules.token_embedders.bert_token_embedder.PretrainedBertModel.load"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.graph_util.plot.GraphPlot.__init__", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.token_embedders.bert_token_embedder.PretrainedBertModel.load"], ["def", "__init__", "(", "\n", "self", ",", "\n", "pretrained_model", ":", "Union", "[", "str", ",", "BertModel", "]", ",", "\n", "requires_grad", ":", "bool", "=", "True", ",", "\n", "dropout", ":", "float", "=", "0.0", ",", "\n", ")", "->", "None", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "\n", "if", "isinstance", "(", "pretrained_model", ",", "str", ")", ":", "\n", "            ", "model", "=", "PretrainedBertModel", ".", "load", "(", "pretrained_model", ")", "\n", "", "else", ":", "\n", "            ", "model", "=", "pretrained_model", "\n", "\n", "", "self", ".", "_dropout", "=", "torch", ".", "nn", ".", "Dropout", "(", "p", "=", "dropout", ")", "\n", "\n", "self", ".", "pooler", "=", "model", ".", "pooler", "\n", "for", "param", "in", "self", ".", "pooler", ".", "parameters", "(", ")", ":", "\n", "            ", "param", ".", "requires_grad", "=", "requires_grad", "\n", "", "self", ".", "_embedding_dim", "=", "model", ".", "config", ".", "hidden_size", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.seq2vec_encoders.bert_pooler.BertPooler.get_input_dim": [[59, 62], ["None"], "methods", ["None"], ["", "@", "overrides", "\n", "def", "get_input_dim", "(", "self", ")", "->", "int", ":", "\n", "        ", "return", "self", ".", "_embedding_dim", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.seq2vec_encoders.bert_pooler.BertPooler.get_output_dim": [[63, 66], ["None"], "methods", ["None"], ["", "@", "overrides", "\n", "def", "get_output_dim", "(", "self", ")", "->", "int", ":", "\n", "        ", "return", "self", ".", "_embedding_dim", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.seq2vec_encoders.bert_pooler.BertPooler.forward": [[67, 71], ["bert_pooler.BertPooler.pooler", "bert_pooler.BertPooler._dropout"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "tokens", ":", "torch", ".", "Tensor", ",", "mask", ":", "torch", ".", "Tensor", "=", "None", ")", ":", "\n", "        ", "pooled", "=", "self", ".", "pooler", "(", "tokens", ")", "\n", "pooled", "=", "self", ".", "_dropout", "(", "pooled", ")", "\n", "return", "pooled", "\n", "", "", ""]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.seq2vec_encoders.pytorch_seq2vec_wrapper.PytorchSeq2VecWrapper.__init__": [[39, 50], ["allennlp.modules.seq2vec_encoders.seq2vec_encoder.Seq2VecEncoder.__init__", "allennlp.common.checks.ConfigurationError"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.graph_util.plot.GraphPlot.__init__"], ["def", "__init__", "(", "self", ",", "module", ":", "torch", ".", "nn", ".", "modules", ".", "RNNBase", ")", "->", "None", ":", "\n", "# Seq2VecEncoders cannot be stateful.", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "stateful", "=", "False", ")", "\n", "self", ".", "_module", "=", "module", "\n", "try", ":", "\n", "            ", "if", "not", "self", ".", "_module", ".", "batch_first", ":", "\n", "                ", "raise", "ConfigurationError", "(", "\n", "\"Our encoder semantics assumes batch is always first!\"", "\n", ")", "\n", "", "", "except", "AttributeError", ":", "\n", "            ", "pass", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.seq2vec_encoders.pytorch_seq2vec_wrapper.PytorchSeq2VecWrapper.get_input_dim": [[51, 53], ["None"], "methods", ["None"], ["", "", "def", "get_input_dim", "(", "self", ")", "->", "int", ":", "\n", "        ", "return", "self", ".", "_module", ".", "input_size", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.seq2vec_encoders.pytorch_seq2vec_wrapper.PytorchSeq2VecWrapper.get_output_dim": [[54, 60], ["None"], "methods", ["None"], ["", "def", "get_output_dim", "(", "self", ")", "->", "int", ":", "\n", "        ", "try", ":", "\n", "            ", "is_bidirectional", "=", "self", ".", "_module", ".", "bidirectional", "\n", "", "except", "AttributeError", ":", "\n", "            ", "is_bidirectional", "=", "False", "\n", "", "return", "self", ".", "_module", ".", "hidden_size", "*", "(", "2", "if", "is_bidirectional", "else", "1", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.seq2vec_encoders.pytorch_seq2vec_wrapper.PytorchSeq2VecWrapper.forward": [[61, 116], ["mask.size", "pytorch_seq2vec_wrapper.PytorchSeq2VecWrapper.sort_and_run_forward", "isinstance", "torch.cat.size", "torch.cat.transpose().index_select", "last_layer_state.contiguous().view", "torch.cat.new_zeros", "torch.cat", "torch.cat.transpose", "last_layer_state.contiguous", "pytorch_seq2vec_wrapper.PytorchSeq2VecWrapper.get_output_dim", "pytorch_seq2vec_wrapper.PytorchSeq2VecWrapper._module"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.modules.encoder_base._EncoderBase.sort_and_run_forward", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.seq2seq_decoders.auto_regressive_seq_decoder.AutoRegressiveSeqDecoder.get_output_dim"], ["", "def", "forward", "(", "\n", "self", ",", "\n", "inputs", ":", "torch", ".", "Tensor", ",", "\n", "mask", ":", "torch", ".", "Tensor", ",", "\n", "hidden_state", ":", "torch", ".", "Tensor", "=", "None", ",", "\n", ")", "->", "torch", ".", "Tensor", ":", "\n", "\n", "        ", "if", "mask", "is", "None", ":", "\n", "# If a mask isn't passed, there is no padding in the batch of instances, so we can just", "\n", "# return the last sequence output as the state.  This doesn't work in the case of", "\n", "# variable length sequences, as the last state for each element of the batch won't be", "\n", "# at the end of the max sequence length, so we have to use the state of the RNN below.", "\n", "            ", "return", "self", ".", "_module", "(", "inputs", ",", "hidden_state", ")", "[", "0", "]", "[", ":", ",", "-", "1", ",", ":", "]", "\n", "\n", "", "batch_size", "=", "mask", ".", "size", "(", "0", ")", "\n", "\n", "_", ",", "state", ",", "restoration_indices", ",", "=", "self", ".", "sort_and_run_forward", "(", "\n", "self", ".", "_module", ",", "inputs", ",", "mask", ",", "hidden_state", "\n", ")", "\n", "\n", "# Deal with the fact the LSTM state is a tuple of (state, memory).", "\n", "if", "isinstance", "(", "state", ",", "tuple", ")", ":", "\n", "            ", "state", "=", "state", "[", "0", "]", "\n", "\n", "", "num_layers_times_directions", ",", "num_valid", ",", "encoding_dim", "=", "state", ".", "size", "(", ")", "\n", "# Add back invalid rows.", "\n", "if", "num_valid", "<", "batch_size", ":", "\n", "# batch size is the second dimension here, because pytorch", "\n", "# returns RNN state as a tensor of shape (num_layers * num_directions,", "\n", "# batch_size, hidden_size)", "\n", "            ", "zeros", "=", "state", ".", "new_zeros", "(", "\n", "num_layers_times_directions", ",", "batch_size", "-", "num_valid", ",", "encoding_dim", "\n", ")", "\n", "state", "=", "torch", ".", "cat", "(", "[", "state", ",", "zeros", "]", ",", "1", ")", "\n", "\n", "# Restore the original indices and return the final state of the", "\n", "# top layer. Pytorch's recurrent layers return state in the form", "\n", "# (num_layers * num_directions, batch_size, hidden_size) regardless", "\n", "# of the 'batch_first' flag, so we transpose, extract the relevant", "\n", "# layer state (both forward and backward if using bidirectional layers)", "\n", "# and return them as a single (batch_size, self.get_output_dim()) tensor.", "\n", "\n", "# now of shape: (batch_size, num_layers * num_directions, hidden_size).", "\n", "", "unsorted_state", "=", "state", ".", "transpose", "(", "0", ",", "1", ")", ".", "index_select", "(", "0", ",", "restoration_indices", ")", "\n", "\n", "# Extract the last hidden vector, including both forward and backward states", "\n", "# if the cell is bidirectional. Then reshape by concatenation (in the case", "\n", "# we have bidirectional states) or just squash the 1st dimension in the non-", "\n", "# bidirectional case. Return tensor has shape (batch_size, hidden_size * num_directions).", "\n", "try", ":", "\n", "            ", "last_state_index", "=", "2", "if", "self", ".", "_module", ".", "bidirectional", "else", "1", "\n", "", "except", "AttributeError", ":", "\n", "            ", "last_state_index", "=", "1", "\n", "", "last_layer_state", "=", "unsorted_state", "[", ":", ",", "-", "last_state_index", ":", ",", ":", "]", "\n", "return", "last_layer_state", ".", "contiguous", "(", ")", ".", "view", "(", "[", "-", "1", ",", "self", ".", "get_output_dim", "(", ")", "]", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.seq2vec_encoders.seq2vec_encoder.Seq2VecEncoder.get_input_dim": [[16, 23], ["None"], "methods", ["None"], ["def", "get_input_dim", "(", "self", ")", "->", "int", ":", "\n", "        ", "\"\"\"\n        Returns the dimension of the vector input for each element in the sequence input\n        to a `Seq2VecEncoder`. This is `not` the shape of the input tensor, but the\n        last element of that shape.\n        \"\"\"", "\n", "raise", "NotImplementedError", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.seq2vec_encoders.seq2vec_encoder.Seq2VecEncoder.get_output_dim": [[24, 30], ["None"], "methods", ["None"], ["", "def", "get_output_dim", "(", "self", ")", "->", "int", ":", "\n", "        ", "\"\"\"\n        Returns the dimension of the final vector output by this `Seq2VecEncoder`.  This is `not`\n        the shape of the returned tensor, but the last element of that shape.\n        \"\"\"", "\n", "raise", "NotImplementedError", "\n", "", "", ""]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.seq2vec_encoders.boe_encoder.BagOfEmbeddingsEncoder.__init__": [[26, 30], ["allennlp.modules.seq2vec_encoders.seq2vec_encoder.Seq2VecEncoder.__init__"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.graph_util.plot.GraphPlot.__init__"], ["def", "__init__", "(", "self", ",", "embedding_dim", ":", "int", ",", "averaged", ":", "bool", "=", "False", ")", "->", "None", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "_embedding_dim", "=", "embedding_dim", "\n", "self", ".", "_averaged", "=", "averaged", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.seq2vec_encoders.boe_encoder.BagOfEmbeddingsEncoder.get_input_dim": [[31, 34], ["None"], "methods", ["None"], ["", "@", "overrides", "\n", "def", "get_input_dim", "(", "self", ")", "->", "int", ":", "\n", "        ", "return", "self", ".", "_embedding_dim", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.seq2vec_encoders.boe_encoder.BagOfEmbeddingsEncoder.get_output_dim": [[35, 38], ["None"], "methods", ["None"], ["", "@", "overrides", "\n", "def", "get_output_dim", "(", "self", ")", "->", "int", ":", "\n", "        ", "return", "self", ".", "_embedding_dim", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.seq2vec_encoders.boe_encoder.BagOfEmbeddingsEncoder.forward": [[39, 64], ["tokens.sum", "mask.unsqueeze().float", "allennlp.nn.util.get_lengths_from_binary_sequence_mask", "torch.max", "tokens.new_full", "tokens.new_full.unsqueeze().float", "tokens.new_full.new_ones", "mask.unsqueeze", "tokens.size", "tokens.new_full.unsqueeze"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.nn.util.get_lengths_from_binary_sequence_mask"], ["", "def", "forward", "(", "self", ",", "tokens", ":", "torch", ".", "Tensor", ",", "mask", ":", "torch", ".", "Tensor", "=", "None", ")", ":", "\n", "        ", "if", "mask", "is", "not", "None", ":", "\n", "            ", "tokens", "=", "tokens", "*", "mask", ".", "unsqueeze", "(", "-", "1", ")", ".", "float", "(", ")", "\n", "\n", "# Our input has shape `(batch_size, num_tokens, embedding_dim)`, so we sum out the `num_tokens`", "\n", "# dimension.", "\n", "", "summed", "=", "tokens", ".", "sum", "(", "1", ")", "\n", "\n", "if", "self", ".", "_averaged", ":", "\n", "            ", "if", "mask", "is", "not", "None", ":", "\n", "                ", "lengths", "=", "get_lengths_from_binary_sequence_mask", "(", "mask", ")", "\n", "length_mask", "=", "lengths", ">", "0", "\n", "\n", "# Set any length 0 to 1, to avoid dividing by zero.", "\n", "lengths", "=", "torch", ".", "max", "(", "lengths", ",", "lengths", ".", "new_ones", "(", "1", ")", ")", "\n", "", "else", ":", "\n", "                ", "lengths", "=", "tokens", ".", "new_full", "(", "(", "1", ",", ")", ",", "fill_value", "=", "tokens", ".", "size", "(", "1", ")", ")", "\n", "length_mask", "=", "None", "\n", "\n", "", "summed", "=", "summed", "/", "lengths", ".", "unsqueeze", "(", "-", "1", ")", ".", "float", "(", ")", "\n", "\n", "if", "length_mask", "is", "not", "None", ":", "\n", "                ", "summed", "=", "summed", "*", "(", "length_mask", ">", "0", ")", ".", "float", "(", ")", ".", "unsqueeze", "(", "-", "1", ")", "\n", "\n", "", "", "return", "summed", "\n", "", "", ""]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.seq2vec_encoders.cnn_highway_encoder.CnnHighwayEncoder.__init__": [[38, 107], ["allennlp.modules.seq2vec_encoders.seq2vec_encoder.Seq2VecEncoder.__init__", "enumerate", "sum", "allennlp.modules.highway.Highway", "torch.nn.Linear", "cnn_highway_encoder.CnnHighwayEncoder._projection.weight.data.normal_", "cnn_highway_encoder.CnnHighwayEncoder._projection.bias.data.fill_", "allennlp.common.checks.ConfigurationError", "torch.nn.Conv1d", "torch.nn.Conv1d.weight.data.uniform_", "torch.nn.Conv1d.bias.data.fill_", "cnn_highway_encoder.CnnHighwayEncoder.add_module", "cnn_highway_encoder.CnnHighwayEncoder._convolutions.append", "highway_layer.weight.data.normal_", "highway_layer.bias[].data.fill_", "highway_layer.bias[].data.fill_", "allennlp.modules.layer_norm.LayerNorm", "allennlp.common.checks.ConfigurationError", "numpy.sqrt", "numpy.sqrt"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.graph_util.plot.GraphPlot.__init__"], ["def", "__init__", "(", "\n", "self", ",", "\n", "embedding_dim", ":", "int", ",", "\n", "filters", ":", "Sequence", "[", "Sequence", "[", "int", "]", "]", ",", "\n", "num_highway", ":", "int", ",", "\n", "projection_dim", ":", "int", ",", "\n", "activation", ":", "str", "=", "\"relu\"", ",", "\n", "projection_location", ":", "str", "=", "\"after_highway\"", ",", "\n", "do_layer_norm", ":", "bool", "=", "False", ",", "\n", ")", "->", "None", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "\n", "if", "projection_location", "not", "in", "_VALID_PROJECTION_LOCATIONS", ":", "\n", "            ", "raise", "ConfigurationError", "(", "\n", "f\"unknown projection location: {projection_location}\"", "\n", ")", "\n", "\n", "", "self", ".", "input_dim", "=", "embedding_dim", "\n", "self", ".", "output_dim", "=", "projection_dim", "\n", "self", ".", "_projection_location", "=", "projection_location", "\n", "\n", "if", "activation", "==", "\"tanh\"", ":", "\n", "            ", "self", ".", "_activation", "=", "torch", ".", "nn", ".", "functional", ".", "tanh", "\n", "", "elif", "activation", "==", "\"relu\"", ":", "\n", "            ", "self", ".", "_activation", "=", "torch", ".", "nn", ".", "functional", ".", "relu", "\n", "", "else", ":", "\n", "            ", "raise", "ConfigurationError", "(", "f\"unknown activation {activation}\"", ")", "\n", "\n", "# Create the convolutions", "\n", "", "self", ".", "_convolutions", ":", "List", "[", "torch", ".", "nn", ".", "Module", "]", "=", "[", "]", "\n", "for", "i", ",", "(", "width", ",", "num", ")", "in", "enumerate", "(", "filters", ")", ":", "\n", "            ", "conv", "=", "torch", ".", "nn", ".", "Conv1d", "(", "\n", "in_channels", "=", "embedding_dim", ",", "\n", "out_channels", "=", "num", ",", "\n", "kernel_size", "=", "width", ",", "\n", "bias", "=", "True", ",", "\n", ")", "\n", "conv", ".", "weight", ".", "data", ".", "uniform_", "(", "-", "0.05", ",", "0.05", ")", "\n", "conv", ".", "bias", ".", "data", ".", "fill_", "(", "0.0", ")", "\n", "self", ".", "add_module", "(", "f\"char_conv_{i}\"", ",", "conv", ")", "# needs to match the old ELMo name", "\n", "self", ".", "_convolutions", ".", "append", "(", "conv", ")", "\n", "\n", "# Create the highway layers", "\n", "", "num_filters", "=", "sum", "(", "num", "for", "_", ",", "num", "in", "filters", ")", "\n", "if", "projection_location", "==", "\"after_cnn\"", ":", "\n", "            ", "highway_dim", "=", "projection_dim", "\n", "", "else", ":", "\n", "# highway_dim is the number of cnn filters", "\n", "            ", "highway_dim", "=", "num_filters", "\n", "", "self", ".", "_highways", "=", "Highway", "(", "\n", "highway_dim", ",", "num_highway", ",", "activation", "=", "torch", ".", "nn", ".", "functional", ".", "relu", "\n", ")", "\n", "for", "highway_layer", "in", "self", ".", "_highways", ".", "_layers", ":", "\n", "# highway is a linear layer for each highway layer", "\n", "# with fused W and b weights", "\n", "            ", "highway_layer", ".", "weight", ".", "data", ".", "normal_", "(", "mean", "=", "0.0", ",", "std", "=", "np", ".", "sqrt", "(", "1.0", "/", "highway_dim", ")", ")", "\n", "highway_layer", ".", "bias", "[", ":", "highway_dim", "]", ".", "data", ".", "fill_", "(", "0.0", ")", "\n", "highway_layer", ".", "bias", "[", "highway_dim", ":", "]", ".", "data", ".", "fill_", "(", "2.0", ")", "\n", "\n", "# Projection layer: always num_filters -> projection_dim", "\n", "", "self", ".", "_projection", "=", "torch", ".", "nn", ".", "Linear", "(", "num_filters", ",", "projection_dim", ",", "bias", "=", "True", ")", "\n", "self", ".", "_projection", ".", "weight", ".", "data", ".", "normal_", "(", "mean", "=", "0.0", ",", "std", "=", "np", ".", "sqrt", "(", "1.0", "/", "num_filters", ")", ")", "\n", "self", ".", "_projection", ".", "bias", ".", "data", ".", "fill_", "(", "0.0", ")", "\n", "\n", "# And add a layer norm", "\n", "if", "do_layer_norm", ":", "\n", "            ", "self", ".", "_layer_norm", ":", "Callable", "=", "LayerNorm", "(", "self", ".", "output_dim", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "_layer_norm", "=", "lambda", "tensor", ":", "tensor", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.seq2vec_encoders.cnn_highway_encoder.CnnHighwayEncoder.forward": [[108, 158], ["inputs.transpose.transpose.transpose", "range", "torch.cat", "cnn_highway_encoder.CnnHighwayEncoder._highways", "cnn_highway_encoder.CnnHighwayEncoder._layer_norm", "len", "getattr", "getattr.", "torch.max", "cnn_highway_encoder.CnnHighwayEncoder._activation", "convolutions.append", "cnn_highway_encoder.CnnHighwayEncoder._projection", "cnn_highway_encoder.CnnHighwayEncoder._projection"], "methods", ["None"], ["", "", "def", "forward", "(", "\n", "self", ",", "inputs", ":", "torch", ".", "Tensor", ",", "mask", ":", "torch", ".", "Tensor", "\n", ")", "->", "Dict", "[", "str", ",", "torch", ".", "Tensor", "]", ":", "\n", "        ", "\"\"\"\n        Compute context insensitive token embeddings for ELMo representations.\n\n        # Parameters\n\n        inputs:\n            Shape `(batch_size, num_characters, embedding_dim)`\n            Character embeddings representing the current batch.\n        mask:\n            Shape `(batch_size, num_characters)`\n            Currently unused. The mask for characters is implicit. See TokenCharactersEncoder.forward.\n\n        # Returns\n\n        `encoding`:\n            Shape `(batch_size, projection_dim)` tensor with context-insensitive token representations.\n        \"\"\"", "\n", "# convolutions want (batch_size, embedding_dim, num_characters)", "\n", "inputs", "=", "inputs", ".", "transpose", "(", "1", ",", "2", ")", "\n", "\n", "convolutions", "=", "[", "]", "\n", "for", "i", "in", "range", "(", "len", "(", "self", ".", "_convolutions", ")", ")", ":", "\n", "            ", "char_conv_i", "=", "getattr", "(", "self", ",", "f\"char_conv_{i}\"", ")", "\n", "convolved", "=", "char_conv_i", "(", "inputs", ")", "\n", "\n", "# (batch_size, n_filters for this width)", "\n", "convolved", ",", "_", "=", "torch", ".", "max", "(", "convolved", ",", "dim", "=", "-", "1", ")", "\n", "convolved", "=", "self", ".", "_activation", "(", "convolved", ")", "\n", "convolutions", ".", "append", "(", "convolved", ")", "\n", "\n", "# (batch_size, n_filters)", "\n", "", "token_embedding", "=", "torch", ".", "cat", "(", "convolutions", ",", "dim", "=", "-", "1", ")", "\n", "\n", "if", "self", ".", "_projection_location", "==", "\"after_cnn\"", ":", "\n", "            ", "token_embedding", "=", "self", ".", "_projection", "(", "token_embedding", ")", "\n", "\n", "# apply the highway layers (batch_size, highway_dim)", "\n", "", "token_embedding", "=", "self", ".", "_highways", "(", "token_embedding", ")", "\n", "\n", "if", "self", ".", "_projection_location", "==", "\"after_highway\"", ":", "\n", "# final projection  (batch_size, projection_dim)", "\n", "            ", "token_embedding", "=", "self", ".", "_projection", "(", "token_embedding", ")", "\n", "\n", "# Apply layer norm if appropriate", "\n", "", "token_embedding", "=", "self", ".", "_layer_norm", "(", "token_embedding", ")", "\n", "\n", "return", "token_embedding", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.seq2vec_encoders.cnn_highway_encoder.CnnHighwayEncoder.get_input_dim": [[159, 161], ["None"], "methods", ["None"], ["", "def", "get_input_dim", "(", "self", ")", "->", "int", ":", "\n", "        ", "return", "self", ".", "input_dim", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.seq2vec_encoders.cnn_highway_encoder.CnnHighwayEncoder.get_output_dim": [[162, 164], ["None"], "methods", ["None"], ["", "def", "get_output_dim", "(", "self", ")", "->", "int", ":", "\n", "        ", "return", "self", ".", "output_dim", "\n", "", "", ""]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.seq2vec_encoders.__init__._Seq2VecWrapper.__init__": [[66, 68], ["None"], "methods", ["None"], []], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.seq2vec_encoders.__init__._Seq2VecWrapper.__call__": [[69, 71], ["__init__._Seq2VecWrapper.from_params", "allennlp.common.Params"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.tokenizers.tokenizer.Tokenizer.from_params"], []], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.seq2vec_encoders.__init__._Seq2VecWrapper.from_params": [[73, 82], ["__init__._Seq2VecWrapper._module_class", "allennlp.modules.seq2vec_encoders.pytorch_seq2vec_wrapper.PytorchSeq2VecWrapper", "params.pop", "allennlp.common.checks.ConfigurationError", "params.as_dict"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.params.Params.pop", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.params.Params.as_dict"], []], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.token_embedders.token_characters_encoder.TokenCharactersEncoder.__init__": [[21, 31], ["allennlp.modules.token_embedders.token_embedder.TokenEmbedder.__init__", "allennlp.modules.time_distributed.TimeDistributed", "allennlp.modules.time_distributed.TimeDistributed", "torch.nn.Dropout"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.graph_util.plot.GraphPlot.__init__"], ["def", "__init__", "(", "\n", "self", ",", "embedding", ":", "Embedding", ",", "encoder", ":", "Seq2VecEncoder", ",", "dropout", ":", "float", "=", "0.0", "\n", ")", "->", "None", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "_embedding", "=", "TimeDistributed", "(", "embedding", ")", "\n", "self", ".", "_encoder", "=", "TimeDistributed", "(", "encoder", ")", "\n", "if", "dropout", ">", "0", ":", "\n", "            ", "self", ".", "_dropout", "=", "torch", ".", "nn", ".", "Dropout", "(", "p", "=", "dropout", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "_dropout", "=", "lambda", "x", ":", "x", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.token_embedders.token_characters_encoder.TokenCharactersEncoder.get_output_dim": [[32, 34], ["token_characters_encoder.TokenCharactersEncoder._encoder._module.get_output_dim"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.seq2seq_decoders.auto_regressive_seq_decoder.AutoRegressiveSeqDecoder.get_output_dim"], ["", "", "def", "get_output_dim", "(", "self", ")", "->", "int", ":", "\n", "        ", "return", "self", ".", "_encoder", ".", "_module", ".", "get_output_dim", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.token_embedders.token_characters_encoder.TokenCharactersEncoder.forward": [[35, 38], ["token_characters_encoder.TokenCharactersEncoder._dropout", "token_characters_encoder.TokenCharactersEncoder._encoder", "token_characters_encoder.TokenCharactersEncoder._embedding"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "token_characters", ":", "torch", ".", "Tensor", ")", "->", "torch", ".", "Tensor", ":", "\n", "        ", "mask", "=", "(", "token_characters", "!=", "0", ")", ".", "long", "(", ")", "\n", "return", "self", ".", "_dropout", "(", "self", ".", "_encoder", "(", "self", ".", "_embedding", "(", "token_characters", ")", ",", "mask", ")", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.token_embedders.language_model_token_embedder.LanguageModelTokenEmbedder.__init__": [[55, 149], ["allennlp.modules.token_embedders.token_embedder.TokenEmbedder.__init__", "load_archive", "language_model_token_embedder.LanguageModelTokenEmbedder._lm.delete_softmax", "config.as_dict", "list", "language_model_token_embedder.LanguageModelTokenEmbedder._lm.num_layers", "allennlp.modules.scalar_mix.ScalarMix", "language_model_token_embedder.LanguageModelTokenEmbedder._lm._text_field_embedder.get_output_dim", "language_model_token_embedder.LanguageModelTokenEmbedder._lm._contextualizer.get_output_dim", "language_model_token_embedder.LanguageModelTokenEmbedder._lm.parameters", "text_field_embedder[].keys", "len", "allennlp.common.checks.ConfigurationError", "config.get", "dataset_reader_config.get.get.get().get", "allennlp.data.TokenIndexer.from_params", "torch.LongTensor", "torch.LongTensor", "torch.nn.Dropout", "allennlp.common.checks.ConfigurationError", "json.dumps", "dataset_reader_config.get.get.get", "dataset_reader_config.get.get.get", "allennlp.data.Token", "token_indexer.tokens_to_indices", "dataset_reader_config.get.get.get"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.graph_util.plot.GraphPlot.__init__", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.models.archival.load_archive", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.models.language_model.LanguageModel.delete_softmax", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.params.Params.as_dict", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.models.language_model.LanguageModel.num_layers", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.seq2seq_decoders.auto_regressive_seq_decoder.AutoRegressiveSeqDecoder.get_output_dim", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.seq2seq_decoders.auto_regressive_seq_decoder.AutoRegressiveSeqDecoder.get_output_dim", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.params.Params.get", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.params.Params.get", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.tokenizers.tokenizer.Tokenizer.from_params", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.params.Params.get", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.params.Params.get", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.token_indexers.token_characters_indexer.TokenCharactersIndexer.tokens_to_indices", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.params.Params.get"], ["def", "__init__", "(", "\n", "self", ",", "\n", "archive_file", ":", "str", ",", "\n", "dropout", ":", "float", "=", "None", ",", "\n", "bos_eos_tokens", ":", "Tuple", "[", "str", ",", "str", "]", "=", "(", "\"<S>\"", ",", "\"</S>\"", ")", ",", "\n", "remove_bos_eos", ":", "bool", "=", "True", ",", "\n", "requires_grad", ":", "bool", "=", "False", ",", "\n", ")", "->", "None", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "\n", "overrides", "=", "{", "\"model\"", ":", "{", "\"contextualizer\"", ":", "{", "\"return_all_layers\"", ":", "True", "}", "}", "}", "\n", "\n", "# Import here to avoid circular dependency.", "\n", "from", "allennlp", ".", "models", ".", "archival", "import", "load_archive", "\n", "\n", "# Load LM and the associated config.", "\n", "archive", "=", "load_archive", "(", "archive_file", ",", "overrides", "=", "json", ".", "dumps", "(", "overrides", ")", ")", "\n", "self", ".", "_lm", ":", "LanguageModel", "=", "archive", ".", "model", "\n", "self", ".", "_lm", ".", "delete_softmax", "(", ")", "\n", "config", "=", "archive", ".", "config", "\n", "dict_config", "=", "config", ".", "as_dict", "(", "quiet", "=", "True", ")", "\n", "\n", "# Extract the name of the tokens that the LM was trained on.", "\n", "text_field_embedder", "=", "dict_config", "[", "\"model\"", "]", "[", "\"text_field_embedder\"", "]", "\n", "token_names", "=", "list", "(", "text_field_embedder", "[", "\"token_embedders\"", "]", ".", "keys", "(", ")", ")", "\n", "if", "len", "(", "token_names", ")", "!=", "1", ":", "\n", "# We don't currently support embedding with language models trained with multiple", "\n", "# embedded indices.", "\n", "#", "\n", "# Note: We only care about embedded indices. This does not include \"tokens\" which", "\n", "# is just used to compute the loss in LanguageModel.", "\n", "            ", "raise", "ConfigurationError", "(", "\n", "f\"LM from {archive_file} trained with multiple embedders!\"", "\n", ")", "\n", "", "self", ".", "_token_name", "=", "token_names", "[", "0", "]", "\n", "\n", "# TODO(brendanr): Find a way to remove this hack. The issue fundamentally is that the", "\n", "# BasicTextFieldEmbedder concatenates multiple embedded representations. When a", "\n", "# downstream model uses both, tokens and token characters, say, and only adds bos/eos", "\n", "# tokens to the token characters, the dimensions don't match. See:", "\n", "# https://github.com/allenai/allennlp/blob/eff25a3085aa9976a7650d30d8961c3626ddc411/allennlp/modules/text_field_embedders/basic_text_field_embedder.py#L109", "\n", "#", "\n", "# For the equivalent hack in the ELMo embedder see:", "\n", "# https://github.com/allenai/allennlp/blob/eff25a3085aa9976a7650d30d8961c3626ddc411/allennlp/modules/elmo.py#L590", "\n", "if", "bos_eos_tokens", ":", "\n", "            ", "dataset_reader_config", "=", "config", ".", "get", "(", "\"dataset_reader\"", ")", "\n", "if", "dataset_reader_config", ".", "get", "(", "\"type\"", ")", "==", "\"multiprocess\"", ":", "\n", "                ", "dataset_reader_config", "=", "dataset_reader_config", ".", "get", "(", "\"base_reader\"", ")", "\n", "", "token_indexer_config", "=", "dataset_reader_config", ".", "get", "(", "\"token_indexers\"", ")", ".", "get", "(", "\n", "self", ".", "_token_name", "\n", ")", "\n", "token_indexer", ":", "TokenIndexer", "=", "TokenIndexer", ".", "from_params", "(", "token_indexer_config", ")", "\n", "token_list", "=", "[", "Token", "(", "token", ")", "for", "token", "in", "bos_eos_tokens", "]", "\n", "# TODO(brendanr): Obtain these indices from the vocab once the", "\n", "# ELMoTokenCharactersIndexer adds the mappings.", "\n", "bos_eos_indices", "=", "token_indexer", ".", "tokens_to_indices", "(", "\n", "token_list", ",", "self", ".", "_lm", ".", "vocab", "\n", ")", "[", "\"tokens\"", "]", "\n", "self", ".", "_bos_indices", "=", "torch", ".", "LongTensor", "(", "bos_eos_indices", "[", "0", "]", ")", "\n", "self", ".", "_eos_indices", "=", "torch", ".", "LongTensor", "(", "bos_eos_indices", "[", "1", "]", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "_bos_indices", "=", "None", "\n", "self", ".", "_eos_indices", "=", "None", "\n", "\n", "", "if", "dropout", ":", "\n", "            ", "self", ".", "_dropout", "=", "torch", ".", "nn", ".", "Dropout", "(", "dropout", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "_dropout", "=", "lambda", "x", ":", "x", "\n", "\n", "", "self", ".", "_remove_bos_eos", "=", "remove_bos_eos", "\n", "num_layers", "=", "self", ".", "_lm", ".", "num_layers", "(", ")", "\n", "# TODO(brendanr): Consider passing our LM as a custom module to `Elmo` instead.", "\n", "# See https://github.com/allenai/allennlp/blob/master/allennlp/modules/elmo.py#L76", "\n", "self", ".", "_scalar_mix", "=", "ScalarMix", "(", "\n", "mixture_size", "=", "num_layers", ",", "do_layer_norm", "=", "False", ",", "trainable", "=", "True", "\n", ")", "\n", "\n", "character_dim", "=", "self", ".", "_lm", ".", "_text_field_embedder", ".", "get_output_dim", "(", ")", "\n", "contextual_dim", "=", "self", ".", "_lm", ".", "_contextualizer", ".", "get_output_dim", "(", ")", "\n", "\n", "if", "contextual_dim", "%", "character_dim", "!=", "0", ":", "\n", "            ", "raise", "ConfigurationError", "(", "\n", "\"The output dimensions for the text_field_embedder \"", "\n", "+", "f\"({character_dim}) and the contextualizer ({contextual_dim})\"", "\n", "+", "f\" from the language model loaded from {archive_file} are \"", "\n", "+", "\"not compatible. Please check the config used to train that \"", "\n", "+", "\"model and ensure that the output dimension of the \"", "\n", "+", "\"text_field_embedder divides the output dimension of the \"", "\n", "+", "\"contextualizer.\"", "\n", ")", "\n", "", "self", ".", "_character_embedding_duplication_count", "=", "contextual_dim", "//", "character_dim", "\n", "\n", "for", "param", "in", "self", ".", "_lm", ".", "parameters", "(", ")", ":", "\n", "            ", "param", ".", "requires_grad", "=", "requires_grad", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.token_embedders.language_model_token_embedder.LanguageModelTokenEmbedder.get_output_dim": [[150, 152], ["language_model_token_embedder.LanguageModelTokenEmbedder._lm._contextualizer.get_output_dim"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.seq2seq_decoders.auto_regressive_seq_decoder.AutoRegressiveSeqDecoder.get_output_dim"], ["", "", "def", "get_output_dim", "(", "self", ")", "->", "int", ":", "\n", "        ", "return", "self", ".", "_lm", ".", "_contextualizer", ".", "get_output_dim", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.token_embedders.language_model_token_embedder.LanguageModelTokenEmbedder.forward": [[153, 206], ["language_model_token_embedder.LanguageModelTokenEmbedder._lm", "torch.cat", "language_model_token_embedder.LanguageModelTokenEmbedder._scalar_mix", "language_model_token_embedder.LanguageModelTokenEmbedder._dropout", "max", "allennlp.nn.util.get_text_field_mask", "allennlp.nn.util.add_sentence_boundary_token_ids", "allennlp.nn.util.remove_sentence_boundaries", "tokens.dim"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.nn.util.get_text_field_mask", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.nn.util.add_sentence_boundary_token_ids", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.nn.util.remove_sentence_boundaries"], ["", "def", "forward", "(", "\n", "self", ",", "# type: ignore", "\n", "tokens", ":", "torch", ".", "Tensor", ",", "\n", ")", "->", "Dict", "[", "str", ",", "torch", ".", "Tensor", "]", ":", "\n", "        ", "\"\"\"\n        # Parameters\n\n        tokens : `torch.Tensor`\n            Shape `(batch_size, timesteps, ...)` of token ids representing the current batch.\n            These must have been produced using the same indexer the LM was trained on.\n\n        # Returns\n\n        The bidirectional language model representations for the input sequence, shape\n        `(batch_size, timesteps, embedding_dim)`\n        \"\"\"", "\n", "\n", "if", "self", ".", "_bos_indices", "is", "not", "None", ":", "\n", "            ", "num_wrapping_dims", "=", "max", "(", "tokens", ".", "dim", "(", ")", "-", "2", ",", "0", ")", "\n", "mask", "=", "get_text_field_mask", "(", "\n", "{", "\"\"", ":", "{", "\"\"", ":", "tokens", "}", "}", ",", "num_wrapping_dims", "=", "num_wrapping_dims", "\n", ")", "\n", "tokens", ",", "mask", "=", "add_sentence_boundary_token_ids", "(", "\n", "tokens", ",", "mask", ",", "self", ".", "_bos_indices", ",", "self", ".", "_eos_indices", "\n", ")", "\n", "\n", "", "source", "=", "{", "self", ".", "_token_name", ":", "{", "\"token_characters\"", ":", "tokens", "}", "}", "\n", "result_dict", "=", "self", ".", "_lm", "(", "source", ")", "\n", "\n", "# shape (batch_size, timesteps, embedding_size)", "\n", "noncontextual_token_embeddings", "=", "result_dict", "[", "\"noncontextual_token_embeddings\"", "]", "\n", "contextual_embeddings", "=", "result_dict", "[", "\"lm_embeddings\"", "]", "\n", "\n", "# Typically the non-contextual embeddings are smaller than the contextualized embeddings.", "\n", "# Since we're averaging all the layers we need to make their dimensions match. Simply", "\n", "# repeating the non-contextual embeddings is a crude, but effective, way to do this.", "\n", "duplicated_character_embeddings", "=", "torch", ".", "cat", "(", "\n", "[", "noncontextual_token_embeddings", "]", "\n", "*", "self", ".", "_character_embedding_duplication_count", ",", "\n", "-", "1", ",", "\n", ")", "\n", "averaged_embeddings", "=", "self", ".", "_scalar_mix", "(", "\n", "[", "duplicated_character_embeddings", "]", "+", "contextual_embeddings", "\n", ")", "\n", "\n", "# Add dropout", "\n", "averaged_embeddings", "=", "self", ".", "_dropout", "(", "averaged_embeddings", ")", "\n", "if", "self", ".", "_remove_bos_eos", ":", "\n", "            ", "averaged_embeddings", ",", "_", "=", "remove_sentence_boundaries", "(", "\n", "averaged_embeddings", ",", "result_dict", "[", "\"mask\"", "]", "\n", ")", "\n", "\n", "", "return", "averaged_embeddings", "\n", "", "", ""]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.token_embedders.pretrained_transformer_mismatched_embedder.PretrainedTransformerMismatchedEmbedder.__init__": [[32, 38], ["allennlp.modules.token_embedders.TokenEmbedder.__init__", "allennlp.modules.token_embedders.PretrainedTransformerEmbedder"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.graph_util.plot.GraphPlot.__init__"], ["def", "__init__", "(", "\n", "self", ",", "model_name", ":", "Union", "[", "str", ",", "AutoModel", "]", ",", "max_length", ":", "int", "=", "None", "\n", ")", "->", "None", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "# The matched version v.s. mismatched", "\n", "self", ".", "_matched_embedder", "=", "PretrainedTransformerEmbedder", "(", "model_name", ",", "max_length", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.token_embedders.pretrained_transformer_mismatched_embedder.PretrainedTransformerMismatchedEmbedder.get_output_dim": [[39, 42], ["pretrained_transformer_mismatched_embedder.PretrainedTransformerMismatchedEmbedder._matched_embedder.get_output_dim"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.seq2seq_decoders.auto_regressive_seq_decoder.AutoRegressiveSeqDecoder.get_output_dim"], ["", "@", "overrides", "\n", "def", "get_output_dim", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "_matched_embedder", ".", "get_output_dim", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.token_embedders.pretrained_transformer_mismatched_embedder.PretrainedTransformerMismatchedEmbedder.forward": [[43, 100], ["pretrained_transformer_mismatched_embedder.PretrainedTransformerMismatchedEmbedder._matched_embedder", "allennlp.nn.util.batched_span_select", "span_mask.unsqueeze.unsqueeze.unsqueeze", "span_embeddings.sum", "span_mask.unsqueeze.unsqueeze.sum", "pretrained_transformer_mismatched_embedder.PretrainedTransformerMismatchedEmbedder.contiguous"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.nn.util.batched_span_select"], ["", "@", "overrides", "\n", "def", "forward", "(", "\n", "self", ",", "\n", "token_ids", ":", "torch", ".", "LongTensor", ",", "\n", "mask", ":", "torch", ".", "LongTensor", ",", "\n", "offsets", ":", "torch", ".", "LongTensor", ",", "\n", "wordpiece_mask", ":", "torch", ".", "LongTensor", ",", "\n", "type_ids", ":", "Optional", "[", "torch", ".", "LongTensor", "]", "=", "None", ",", "\n", "segment_concat_mask", ":", "Optional", "[", "torch", ".", "LongTensor", "]", "=", "None", ",", "\n", ")", "->", "torch", ".", "Tensor", ":", "# type: ignore", "\n", "        ", "\"\"\"\n        # Parameters\n\n        token_ids: torch.LongTensor\n            Shape: [batch_size, num_wordpieces] (for exception see `PretrainedTransformerEmbedder`).\n        mask: torch.LongTensor\n            Shape: [batch_size, num_orig_tokens].\n        offsets: torch.LongTensor\n            Shape: [batch_size, num_orig_tokens, 2].\n            Maps indices for the original tokens, i.e. those given as input to the indexer,\n            to a span in token_ids. `token_ids[i][offsets[i][j][0]:offsets[i][j][1] + 1]`\n            corresponds to the original j-th token from the i-th batch.\n        wordpiece_mask: torch.LongTensor\n            Shape: [batch_size, num_wordpieces].\n        type_ids: Optional[torch.LongTensor]\n            Shape: [batch_size, num_wordpieces].\n        segment_concat_mask: Optional[torch.LongTensor]\n            See `PretrainedTransformerEmbedder`.\n\n        # Returns:\n\n        Shape: [batch_size, num_orig_tokens, embedding_size].\n        \"\"\"", "\n", "# Shape: [batch_size, num_wordpieces, embedding_size].", "\n", "embeddings", "=", "self", ".", "_matched_embedder", "(", "\n", "token_ids", ",", "\n", "wordpiece_mask", ",", "\n", "type_ids", "=", "type_ids", ",", "\n", "segment_concat_mask", "=", "segment_concat_mask", ",", "\n", ")", "\n", "\n", "# span_embeddings: (batch_size, num_orig_tokens, max_span_length, embedding_size)", "\n", "# span_mask: (batch_size, num_orig_tokens, max_span_length)", "\n", "span_embeddings", ",", "span_mask", "=", "util", ".", "batched_span_select", "(", "\n", "embeddings", ".", "contiguous", "(", ")", ",", "offsets", "\n", ")", "\n", "span_mask", "=", "span_mask", ".", "unsqueeze", "(", "-", "1", ")", "\n", "span_embeddings", "*=", "span_mask", "# zero out paddings", "\n", "\n", "span_embeddings_sum", "=", "span_embeddings", ".", "sum", "(", "2", ")", "\n", "span_embeddings_len", "=", "span_mask", ".", "sum", "(", "2", ")", "\n", "# in some cases span_embeddings_len = 0 which causes NaN", "\n", "span_embeddings_len", "[", "span_embeddings_len", "==", "0", "]", "=", "1", "\n", "# Shape: (batch_size, num_orig_tokens, embedding_size)", "\n", "orig_embeddings", "=", "span_embeddings_sum", "/", "span_embeddings_len", "\n", "\n", "return", "orig_embeddings", "\n", "", "", ""]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.token_embedders.elmo_token_embedder_multilang.ElmoTokenEmbedderMultiLang.__init__": [[49, 114], ["allennlp.modules.token_embedders.token_embedder.TokenEmbedder.__init__", "weight_files.keys", "weight_files.keys", "options_files.keys", "weight_files.keys", "allennlp.common.checks.ConfigurationError", "allennlp.modules.elmo.Elmo", "elmo_token_embedder_multilang.ElmoTokenEmbedderMultiLang.add_module", "allennlp.modules.elmo.Elmo.get_output_dim", "torch.nn.Linear", "torch.eye", "torch.nn.Linear", "torch.nn.Parameter", "elmo_token_embedder_multilang.ElmoTokenEmbedderMultiLang.add_module", "allennlp.common.checks.check_dimensions_match", "allennlp.common.file_utils.cached_path", "torch.FloatTensor", "torch.load"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.graph_util.plot.GraphPlot.__init__", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.seq2seq_decoders.auto_regressive_seq_decoder.AutoRegressiveSeqDecoder.get_output_dim", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.checks.check_dimensions_match", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.file_utils.cached_path", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.token_embedders.bert_token_embedder.PretrainedBertModel.load"], ["def", "__init__", "(", "\n", "self", ",", "\n", "options_files", ":", "Dict", "[", "str", ",", "str", "]", ",", "\n", "weight_files", ":", "Dict", "[", "str", ",", "str", "]", ",", "\n", "do_layer_norm", ":", "bool", "=", "False", ",", "\n", "dropout", ":", "float", "=", "0.5", ",", "\n", "requires_grad", ":", "bool", "=", "False", ",", "\n", "projection_dim", ":", "int", "=", "None", ",", "\n", "vocab_to_cache", ":", "List", "[", "str", "]", "=", "None", ",", "\n", "scalar_mix_parameters", ":", "List", "[", "float", "]", "=", "None", ",", "\n", "aligning_files", ":", "Dict", "[", "str", ",", "str", "]", "=", "None", ",", "\n", ")", "->", "None", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "\n", "if", "options_files", ".", "keys", "(", ")", "!=", "weight_files", ".", "keys", "(", ")", ":", "\n", "            ", "raise", "ConfigurationError", "(", "\n", "\"Keys for Elmo's options files and weights files don't match\"", "\n", ")", "\n", "\n", "", "aligning_files", "=", "aligning_files", "or", "{", "}", "\n", "output_dim", "=", "None", "\n", "for", "lang", "in", "weight_files", ".", "keys", "(", ")", ":", "\n", "            ", "name", "=", "\"elmo_%s\"", "%", "lang", "\n", "elmo", "=", "Elmo", "(", "\n", "options_files", "[", "lang", "]", ",", "\n", "weight_files", "[", "lang", "]", ",", "\n", "num_output_representations", "=", "1", ",", "\n", "do_layer_norm", "=", "do_layer_norm", ",", "\n", "dropout", "=", "dropout", ",", "\n", "requires_grad", "=", "requires_grad", ",", "\n", "vocab_to_cache", "=", "vocab_to_cache", ",", "\n", "scalar_mix_parameters", "=", "scalar_mix_parameters", ",", "\n", ")", "\n", "self", ".", "add_module", "(", "name", ",", "elmo", ")", "\n", "\n", "output_dim_tmp", "=", "elmo", ".", "get_output_dim", "(", ")", "\n", "if", "output_dim", "is", "not", "None", ":", "\n", "# Verify that all ELMo embedders have the same output dimension.", "\n", "                ", "check_dimensions_match", "(", "\n", "output_dim_tmp", ",", "\n", "output_dim", ",", "\n", "\"%s output dim\"", "%", "name", ",", "\n", "\"elmo output dim\"", ",", "\n", ")", "\n", "\n", "", "output_dim", "=", "output_dim_tmp", "\n", "\n", "", "self", ".", "output_dim", "=", "output_dim", "\n", "\n", "if", "projection_dim", ":", "\n", "            ", "self", ".", "_projection", "=", "torch", ".", "nn", ".", "Linear", "(", "output_dim", ",", "projection_dim", ")", "\n", "self", ".", "output_dim", "=", "projection_dim", "\n", "", "else", ":", "\n", "            ", "self", ".", "_projection", "=", "None", "\n", "\n", "", "for", "lang", "in", "weight_files", ".", "keys", "(", ")", ":", "\n", "            ", "name", "=", "\"aligning_%s\"", "%", "lang", "\n", "aligning_matrix", "=", "torch", ".", "eye", "(", "output_dim", ")", "\n", "if", "lang", "in", "aligning_files", "and", "aligning_files", "[", "lang", "]", "!=", "\"\"", ":", "\n", "                ", "aligninig_path", "=", "cached_path", "(", "aligning_files", "[", "lang", "]", ")", "\n", "aligning_matrix", "=", "torch", ".", "FloatTensor", "(", "torch", ".", "load", "(", "aligninig_path", ")", ")", "\n", "\n", "", "aligning", "=", "torch", ".", "nn", ".", "Linear", "(", "output_dim", ",", "output_dim", ",", "bias", "=", "False", ")", "\n", "aligning", ".", "weight", "=", "torch", ".", "nn", ".", "Parameter", "(", "aligning_matrix", ",", "requires_grad", "=", "False", ")", "\n", "self", ".", "add_module", "(", "name", ",", "aligning", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.token_embedders.elmo_token_embedder_multilang.ElmoTokenEmbedderMultiLang.get_output_dim": [[115, 117], ["None"], "methods", ["None"], ["", "", "def", "get_output_dim", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "output_dim", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.token_embedders.elmo_token_embedder_multilang.ElmoTokenEmbedderMultiLang.forward": [[118, 148], ["getattr", "getattr.", "getattr", "getattr.", "range", "allennlp.modules.time_distributed.TimeDistributed.", "allennlp.modules.time_distributed.TimeDistributed", "allennlp.modules.time_distributed.TimeDistributed.dim"], "methods", ["None"], ["", "def", "forward", "(", "\n", "self", ",", "tokens", ":", "torch", ".", "Tensor", ",", "lang", ":", "str", ",", "word_inputs", ":", "torch", ".", "Tensor", "=", "None", "\n", ")", "->", "torch", ".", "Tensor", ":", "\n", "        ", "\"\"\"\n        # Parameters\n\n        tokens : `torch.Tensor`\n            Shape `(batch_size, timesteps, 50)` of character ids representing the current batch.\n        lang : `str`, , required.\n            The language of the ELMo embedder to use.\n        word_inputs : `torch.Tensor`, optional.\n            If you passed a cached vocab, you can in addition pass a tensor of shape\n            `(batch_size, timesteps)`, which represent word ids which have been pre-cached.\n\n        # Returns\n\n        The ELMo representations for the given language for the input sequence, shape\n        `(batch_size, timesteps, embedding_dim)`\n        \"\"\"", "\n", "elmo", "=", "getattr", "(", "self", ",", "\"elmo_{}\"", ".", "format", "(", "lang", ")", ")", "\n", "elmo_output", "=", "elmo", "(", "tokens", ",", "word_inputs", ")", "\n", "elmo_representations", "=", "elmo_output", "[", "\"elmo_representations\"", "]", "[", "0", "]", "\n", "aligning", "=", "getattr", "(", "self", ",", "\"aligning_{}\"", ".", "format", "(", "lang", ")", ")", "\n", "elmo_representations", "=", "aligning", "(", "elmo_representations", ")", "\n", "if", "self", ".", "_projection", ":", "\n", "            ", "projection", "=", "self", ".", "_projection", "\n", "for", "_", "in", "range", "(", "elmo_representations", ".", "dim", "(", ")", "-", "2", ")", ":", "\n", "                ", "projection", "=", "TimeDistributed", "(", "projection", ")", "\n", "", "elmo_representations", "=", "projection", "(", "elmo_representations", ")", "\n", "", "return", "elmo_representations", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.token_embedders.elmo_token_embedder_multilang.ElmoTokenEmbedderMultiLang.from_params": [[150, 181], ["params.pop", "params.pop", "params.pop", "params.pop_bool", "params.pop_float", "params.pop", "params.pop_int", "params.pop", "params.pop", "params.assert_empty", "cls", "list", "vocab.get_token_to_index_vocabulary().keys", "vocab.get_token_to_index_vocabulary"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.params.Params.pop", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.params.Params.pop", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.params.Params.pop", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.params.Params.pop_bool", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.params.Params.pop_float", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.params.Params.pop", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.params.Params.pop_int", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.params.Params.pop", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.params.Params.pop", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.params.Params.assert_empty", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.data.vocabulary.Vocabulary.get_token_to_index_vocabulary"], ["", "@", "classmethod", "\n", "def", "from_params", "(", "# type: ignore", "\n", "cls", ",", "vocab", ":", "Vocabulary", ",", "params", ":", "Params", ",", "**", "extras", "\n", ")", "->", "\"ElmoTokenEmbedderMultiLang\"", ":", "\n", "\n", "        ", "options_files", "=", "params", ".", "pop", "(", "\"options_files\"", ")", "\n", "weight_files", "=", "params", ".", "pop", "(", "\"weight_files\"", ")", "\n", "requires_grad", "=", "params", ".", "pop", "(", "\"requires_grad\"", ",", "False", ")", "\n", "do_layer_norm", "=", "params", ".", "pop_bool", "(", "\"do_layer_norm\"", ",", "False", ")", "\n", "dropout", "=", "params", ".", "pop_float", "(", "\"dropout\"", ",", "0.5", ")", "\n", "namespace_to_cache", "=", "params", ".", "pop", "(", "\"namespace_to_cache\"", ",", "None", ")", "\n", "if", "namespace_to_cache", "is", "not", "None", ":", "\n", "            ", "vocab_to_cache", "=", "list", "(", "\n", "vocab", ".", "get_token_to_index_vocabulary", "(", "namespace_to_cache", ")", ".", "keys", "(", ")", "\n", ")", "\n", "", "else", ":", "\n", "            ", "vocab_to_cache", "=", "None", "\n", "", "projection_dim", "=", "params", ".", "pop_int", "(", "\"projection_dim\"", ",", "None", ")", "\n", "scalar_mix_parameters", "=", "params", ".", "pop", "(", "\"scalar_mix_parameters\"", ",", "None", ")", "\n", "aligning_files", "=", "params", ".", "pop", "(", "\"aligning_files\"", ",", "{", "}", ")", "\n", "params", ".", "assert_empty", "(", "cls", ".", "__name__", ")", "\n", "return", "cls", "(", "\n", "options_files", "=", "options_files", ",", "\n", "weight_files", "=", "weight_files", ",", "\n", "do_layer_norm", "=", "do_layer_norm", ",", "\n", "dropout", "=", "dropout", ",", "\n", "requires_grad", "=", "requires_grad", ",", "\n", "projection_dim", "=", "projection_dim", ",", "\n", "vocab_to_cache", "=", "vocab_to_cache", ",", "\n", "scalar_mix_parameters", "=", "scalar_mix_parameters", ",", "\n", "aligning_files", "=", "aligning_files", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.token_embedders.bert_token_embedder.PretrainedBertModel.load": [[34, 44], ["transformers.modeling_bert.BertModel.from_pretrained"], "methods", ["None"], ["@", "classmethod", "\n", "def", "load", "(", "cls", ",", "model_name", ":", "str", ",", "cache_model", ":", "bool", "=", "True", ")", "->", "BertModel", ":", "\n", "        ", "if", "model_name", "in", "cls", ".", "_cache", ":", "\n", "            ", "return", "PretrainedBertModel", ".", "_cache", "[", "model_name", "]", "\n", "\n", "", "model", "=", "BertModel", ".", "from_pretrained", "(", "model_name", ")", "\n", "if", "cache_model", ":", "\n", "            ", "cls", ".", "_cache", "[", "model_name", "]", "=", "model", "\n", "\n", "", "return", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.token_embedders.bert_token_embedder.BertEmbedder.__init__": [[76, 103], ["allennlp.modules.token_embedders.token_embedder.TokenEmbedder.__init__", "allennlp.modules.scalar_mix.ScalarMix"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.graph_util.plot.GraphPlot.__init__"], ["def", "__init__", "(", "\n", "self", ",", "\n", "bert_model", ":", "BertModel", ",", "\n", "top_layer_only", ":", "bool", "=", "False", ",", "\n", "max_pieces", ":", "int", "=", "512", ",", "\n", "num_start_tokens", ":", "int", "=", "1", ",", "\n", "num_end_tokens", ":", "int", "=", "1", ",", "\n", "scalar_mix_parameters", ":", "List", "[", "float", "]", "=", "None", ",", "\n", ")", "->", "None", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "bert_model", "=", "bert_model", "\n", "self", ".", "output_dim", "=", "bert_model", ".", "config", ".", "hidden_size", "\n", "self", ".", "max_pieces", "=", "max_pieces", "\n", "self", ".", "num_start_tokens", "=", "num_start_tokens", "\n", "self", ".", "num_end_tokens", "=", "num_end_tokens", "\n", "\n", "if", "not", "top_layer_only", ":", "\n", "            ", "self", ".", "_scalar_mix", "=", "ScalarMix", "(", "\n", "bert_model", ".", "config", ".", "num_hidden_layers", "+", "1", ",", "\n", "do_layer_norm", "=", "False", ",", "\n", "initial_scalar_parameters", "=", "scalar_mix_parameters", ",", "\n", "trainable", "=", "scalar_mix_parameters", "is", "None", ",", "\n", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "_scalar_mix", "=", "None", "\n", "\n", "", "self", ".", "bert_model", ".", "encoder", ".", "output_hidden_states", "=", "True", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.token_embedders.bert_token_embedder.BertEmbedder.get_output_dim": [[104, 106], ["None"], "methods", ["None"], ["", "def", "get_output_dim", "(", "self", ")", "->", "int", ":", "\n", "        ", "return", "self", ".", "output_dim", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.token_embedders.bert_token_embedder.BertEmbedder.forward": [[107, 274], ["list", "bert_token_embedder.BertEmbedder.bert_model", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.cat.size", "torch.cat.size", "torch.cat.size", "torch.cat.size", "list", "split_input_ids[].size", "torch.pad", "torch.pad", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "torch.split", "torch.split", "torch.split", "torch.split", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "list", "list", "list.append", "bert_token_embedder.BertEmbedder._scalar_mix", "allennlp.nn.util.uncombine_initial_dims", "allennlp.nn.util.combine_initial_dims", "allennlp.nn.util.get_range_vector().unsqueeze", "allennlp.nn.util.uncombine_initial_dims", "torch.cat.split", "torch.cat.split", "list", "split_token_type_ids[].size", "torch.pad", "torch.pad", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "allennlp.nn.util.combine_initial_dims", "allennlp.nn.util.combine_initial_dims", "allennlp.nn.util.combine_initial_dims", "range", "range", "len", "torch.cat.size", "torch.cat.size", "offsets.size", "torch.cat.split", "torch.cat.split", "range", "allennlp.nn.util.get_range_vector", "allennlp.nn.util.combine_initial_dims.size", "allennlp.nn.util.get_device_of"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.nn.util.uncombine_initial_dims", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.nn.util.combine_initial_dims", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.nn.util.uncombine_initial_dims", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.nn.util.combine_initial_dims", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.nn.util.combine_initial_dims", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.nn.util.combine_initial_dims", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.nn.util.get_range_vector", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.nn.util.get_device_of"], ["", "def", "forward", "(", "\n", "self", ",", "\n", "input_ids", ":", "torch", ".", "LongTensor", ",", "\n", "offsets", ":", "torch", ".", "LongTensor", "=", "None", ",", "\n", "token_type_ids", ":", "torch", ".", "LongTensor", "=", "None", ",", "\n", "mask", ":", "torch", ".", "LongTensor", "=", "None", ",", "\n", ")", "->", "torch", ".", "Tensor", ":", "\n", "        ", "\"\"\"\n        # Parameters\n\n        input_ids : `torch.LongTensor`\n            The (batch_size, ..., max_sequence_length) tensor of wordpiece ids.\n        offsets : `torch.LongTensor`, optional\n            The BERT embeddings are one per wordpiece. However it's possible/likely\n            you might want one per original token. In that case, `offsets`\n            represents the indices of the desired wordpiece for each original token.\n            Depending on how your token indexer is configured, this could be the\n            position of the last wordpiece for each token, or it could be the position\n            of the first wordpiece for each token.\n\n            For example, if you had the sentence \"Definitely not\", and if the corresponding\n            wordpieces were [\"Def\", \"##in\", \"##ite\", \"##ly\", \"not\"], then the input_ids\n            would be 5 wordpiece ids, and the \"last wordpiece\" offsets would be [3, 4].\n            If offsets are provided, the returned tensor will contain only the wordpiece\n            embeddings at those positions, and (in particular) will contain one embedding\n            per token. If offsets are not provided, the entire tensor of wordpiece embeddings\n            will be returned.\n        token_type_ids : `torch.LongTensor`, optional\n            If an input consists of two sentences (as in the BERT paper),\n            tokens from the first sentence should have type 0 and tokens from\n            the second sentence should have type 1.  If you don't provide this\n            (the default BertIndexer doesn't) then it's assumed to be all 0s.\n        mask : `torch.LongTensor`, optional\n            This is not actually used here, but is needed for compatibility with other parts of the\n            code.  We pass all tensors created by a `TokenIndexer` by name to its corresponding\n            `TokenEmbedder`, but the `WordpieceIndexer` creates this `mask` that is intended for use\n            _outside_ of the `TokenEmbedder`, by `nn.util.get_text_field_mask`.\n        \"\"\"", "\n", "\n", "batch_size", ",", "full_seq_len", "=", "input_ids", ".", "size", "(", "0", ")", ",", "input_ids", ".", "size", "(", "-", "1", ")", "\n", "initial_dims", "=", "list", "(", "input_ids", ".", "shape", "[", ":", "-", "1", "]", ")", "\n", "\n", "# The embedder may receive an input tensor that has a sequence length longer than can", "\n", "# be fit. In that case, we should expect the wordpiece indexer to create padded windows", "\n", "# of length `self.max_pieces` for us, and have them concatenated into one long sequence.", "\n", "# E.g., \"[CLS] I went to the [SEP] [CLS] to the store to [SEP] ...\"", "\n", "# We can then split the sequence into sub-sequences of that length, and concatenate them", "\n", "# along the batch dimension so we effectively have one huge batch of partial sentences.", "\n", "# This can then be fed into BERT without any sentence length issues. Keep in mind", "\n", "# that the memory consumption can dramatically increase for large batches with extremely", "\n", "# long sentences.", "\n", "needs_split", "=", "full_seq_len", ">", "self", ".", "max_pieces", "\n", "last_window_size", "=", "0", "\n", "if", "needs_split", ":", "\n", "# Split the flattened list by the window size, `max_pieces`", "\n", "            ", "split_input_ids", "=", "list", "(", "input_ids", ".", "split", "(", "self", ".", "max_pieces", ",", "dim", "=", "-", "1", ")", ")", "\n", "\n", "# We want all sequences to be the same length, so pad the last sequence", "\n", "last_window_size", "=", "split_input_ids", "[", "-", "1", "]", ".", "size", "(", "-", "1", ")", "\n", "padding_amount", "=", "self", ".", "max_pieces", "-", "last_window_size", "\n", "split_input_ids", "[", "-", "1", "]", "=", "F", ".", "pad", "(", "\n", "split_input_ids", "[", "-", "1", "]", ",", "pad", "=", "[", "0", ",", "padding_amount", "]", ",", "value", "=", "0", "\n", ")", "\n", "\n", "# Now combine the sequences along the batch dimension", "\n", "input_ids", "=", "torch", ".", "cat", "(", "split_input_ids", ",", "dim", "=", "0", ")", "\n", "\n", "if", "token_type_ids", "is", "not", "None", ":", "\n", "# Same for token_type_ids", "\n", "                ", "split_token_type_ids", "=", "list", "(", "\n", "token_type_ids", ".", "split", "(", "self", ".", "max_pieces", ",", "dim", "=", "-", "1", ")", "\n", ")", "\n", "\n", "last_window_size", "=", "split_token_type_ids", "[", "-", "1", "]", ".", "size", "(", "-", "1", ")", "\n", "padding_amount", "=", "self", ".", "max_pieces", "-", "last_window_size", "\n", "split_token_type_ids", "[", "-", "1", "]", "=", "F", ".", "pad", "(", "\n", "split_token_type_ids", "[", "-", "1", "]", ",", "pad", "=", "[", "0", ",", "padding_amount", "]", ",", "value", "=", "0", "\n", ")", "\n", "\n", "token_type_ids", "=", "torch", ".", "cat", "(", "split_token_type_ids", ",", "dim", "=", "0", ")", "\n", "\n", "", "", "if", "token_type_ids", "is", "None", ":", "\n", "            ", "token_type_ids", "=", "torch", ".", "zeros_like", "(", "input_ids", ")", "\n", "\n", "", "input_mask", "=", "(", "input_ids", "!=", "0", ")", ".", "long", "(", ")", "\n", "\n", "# input_ids may have extra dimensions, so we reshape down to 2-d", "\n", "# before calling the BERT model and then reshape back at the end.", "\n", "_", ",", "_", ",", "all_encoder_layers", "=", "self", ".", "bert_model", "(", "\n", "input_ids", "=", "util", ".", "combine_initial_dims", "(", "input_ids", ")", ",", "\n", "token_type_ids", "=", "util", ".", "combine_initial_dims", "(", "token_type_ids", ")", ",", "\n", "attention_mask", "=", "util", ".", "combine_initial_dims", "(", "input_mask", ")", ",", "\n", ")", "\n", "all_encoder_layers", "=", "torch", ".", "stack", "(", "all_encoder_layers", ")", "\n", "\n", "if", "needs_split", ":", "\n", "# First, unpack the output embeddings into one long sequence again", "\n", "            ", "unpacked_embeddings", "=", "torch", ".", "split", "(", "all_encoder_layers", ",", "batch_size", ",", "dim", "=", "1", ")", "\n", "unpacked_embeddings", "=", "torch", ".", "cat", "(", "unpacked_embeddings", ",", "dim", "=", "2", ")", "\n", "\n", "# Next, select indices of the sequence such that it will result in embeddings representing the original", "\n", "# sentence. To capture maximal context, the indices will be the middle part of each embedded window", "\n", "# sub-sequence (plus any leftover start and final edge windows), e.g.,", "\n", "#  0     1 2    3  4   5    6    7     8     9   10   11   12    13 14  15", "\n", "# \"[CLS] I went to the very fine [SEP] [CLS] the very fine store to eat [SEP]\"", "\n", "# with max_pieces = 8 should produce max context indices [2, 3, 4, 10, 11, 12] with additional start", "\n", "# and final windows with indices [0, 1] and [14, 15] respectively.", "\n", "\n", "# Find the stride as half the max pieces, ignoring the special start and end tokens", "\n", "# Calculate an offset to extract the centermost embeddings of each window", "\n", "stride", "=", "(", "\n", "self", ".", "max_pieces", "-", "self", ".", "num_start_tokens", "-", "self", ".", "num_end_tokens", "\n", ")", "//", "2", "\n", "stride_offset", "=", "stride", "//", "2", "+", "self", ".", "num_start_tokens", "\n", "\n", "first_window", "=", "list", "(", "range", "(", "stride_offset", ")", ")", "\n", "\n", "max_context_windows", "=", "[", "\n", "i", "\n", "for", "i", "in", "range", "(", "full_seq_len", ")", "\n", "if", "stride_offset", "-", "1", "<", "i", "%", "self", ".", "max_pieces", "<", "stride_offset", "+", "stride", "\n", "]", "\n", "\n", "# Lookback what's left, unless it's the whole self.max_pieces window", "\n", "if", "full_seq_len", "%", "self", ".", "max_pieces", "==", "0", ":", "\n", "                ", "lookback", "=", "self", ".", "max_pieces", "\n", "", "else", ":", "\n", "                ", "lookback", "=", "full_seq_len", "%", "self", ".", "max_pieces", "\n", "\n", "", "final_window_start", "=", "full_seq_len", "-", "lookback", "+", "stride_offset", "+", "stride", "\n", "final_window", "=", "list", "(", "range", "(", "final_window_start", ",", "full_seq_len", ")", ")", "\n", "\n", "select_indices", "=", "first_window", "+", "max_context_windows", "+", "final_window", "\n", "\n", "initial_dims", ".", "append", "(", "len", "(", "select_indices", ")", ")", "\n", "\n", "recombined_embeddings", "=", "unpacked_embeddings", "[", ":", ",", ":", ",", "select_indices", "]", "\n", "", "else", ":", "\n", "            ", "recombined_embeddings", "=", "all_encoder_layers", "\n", "\n", "# Recombine the outputs of all layers", "\n", "# (layers, batch_size * d1 * ... * dn, sequence_length, embedding_dim)", "\n", "# recombined = torch.cat(combined, dim=2)", "\n", "", "input_mask", "=", "(", "recombined_embeddings", "!=", "0", ")", ".", "long", "(", ")", "\n", "\n", "if", "self", ".", "_scalar_mix", "is", "not", "None", ":", "\n", "            ", "mix", "=", "self", ".", "_scalar_mix", "(", "recombined_embeddings", ",", "input_mask", ")", "\n", "", "else", ":", "\n", "            ", "mix", "=", "recombined_embeddings", "[", "-", "1", "]", "\n", "\n", "# At this point, mix is (batch_size * d1 * ... * dn, sequence_length, embedding_dim)", "\n", "\n", "", "if", "offsets", "is", "None", ":", "\n", "# Resize to (batch_size, d1, ..., dn, sequence_length, embedding_dim)", "\n", "            ", "dims", "=", "initial_dims", "if", "needs_split", "else", "input_ids", ".", "size", "(", ")", "\n", "return", "util", ".", "uncombine_initial_dims", "(", "mix", ",", "dims", ")", "\n", "", "else", ":", "\n", "# offsets is (batch_size, d1, ..., dn, orig_sequence_length)", "\n", "            ", "offsets2d", "=", "util", ".", "combine_initial_dims", "(", "offsets", ")", "\n", "# now offsets is (batch_size * d1 * ... * dn, orig_sequence_length)", "\n", "range_vector", "=", "util", ".", "get_range_vector", "(", "\n", "offsets2d", ".", "size", "(", "0", ")", ",", "device", "=", "util", ".", "get_device_of", "(", "mix", ")", "\n", ")", ".", "unsqueeze", "(", "1", ")", "\n", "# selected embeddings is also (batch_size * d1 * ... * dn, orig_sequence_length)", "\n", "selected_embeddings", "=", "mix", "[", "range_vector", ",", "offsets2d", "]", "\n", "\n", "return", "util", ".", "uncombine_initial_dims", "(", "selected_embeddings", ",", "offsets", ".", "size", "(", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.token_embedders.bert_token_embedder.PretrainedBertEmbedder.__init__": [[299, 315], ["bert_token_embedder.PretrainedBertModel.load", "bert_token_embedder.PretrainedBertModel.load", "bert_token_embedder.BertEmbedder.__init__"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.token_embedders.bert_token_embedder.PretrainedBertModel.load", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.token_embedders.bert_token_embedder.PretrainedBertModel.load", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.graph_util.plot.GraphPlot.__init__"], ["def", "__init__", "(", "\n", "self", ",", "\n", "pretrained_model", ":", "str", ",", "\n", "requires_grad", ":", "bool", "=", "False", ",", "\n", "top_layer_only", ":", "bool", "=", "False", ",", "\n", "scalar_mix_parameters", ":", "List", "[", "float", "]", "=", "None", ",", "\n", ")", "->", "None", ":", "\n", "        ", "model", "=", "PretrainedBertModel", ".", "load", "(", "pretrained_model", ")", "\n", "\n", "for", "param", "in", "model", ".", "parameters", "(", ")", ":", "\n", "            ", "param", ".", "requires_grad", "=", "requires_grad", "\n", "\n", "", "super", "(", ")", ".", "__init__", "(", "\n", "bert_model", "=", "model", ",", "\n", "top_layer_only", "=", "top_layer_only", ",", "\n", "scalar_mix_parameters", "=", "scalar_mix_parameters", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.token_embedders.bidirectional_language_model_token_embedder.BidirectionalLanguageModelTokenEmbedder.__init__": [[42, 56], ["allennlp.modules.token_embedders.language_model_token_embedder.LanguageModelTokenEmbedder.__init__"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.graph_util.plot.GraphPlot.__init__"], ["def", "__init__", "(", "\n", "self", ",", "\n", "archive_file", ":", "str", ",", "\n", "dropout", ":", "float", "=", "None", ",", "\n", "bos_eos_tokens", ":", "Tuple", "[", "str", ",", "str", "]", "=", "(", "\"<S>\"", ",", "\"</S>\"", ")", ",", "\n", "remove_bos_eos", ":", "bool", "=", "True", ",", "\n", "requires_grad", ":", "bool", "=", "False", ",", "\n", ")", "->", "None", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "\n", "archive_file", "=", "archive_file", ",", "\n", "dropout", "=", "dropout", ",", "\n", "bos_eos_tokens", "=", "bos_eos_tokens", ",", "\n", "remove_bos_eos", "=", "remove_bos_eos", ",", "\n", "requires_grad", "=", "requires_grad", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.token_embedders.token_embedder.TokenEmbedder.get_output_dim": [[24, 30], ["None"], "methods", ["None"], ["def", "get_output_dim", "(", "self", ")", "->", "int", ":", "\n", "        ", "\"\"\"\n        Returns the final output dimension that this `TokenEmbedder` uses to represent each\n        token.  This is `not` the shape of the returned tensor, but the last element of that shape.\n        \"\"\"", "\n", "raise", "NotImplementedError", "\n", "", "", ""]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.token_embedders.pass_through_token_embedder.PassThroughTokenEmbedder.__init__": [[17, 20], ["allennlp.modules.token_embedders.token_embedder.TokenEmbedder.__init__"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.graph_util.plot.GraphPlot.__init__"], ["def", "__init__", "(", "self", ",", "hidden_dim", ":", "int", ")", "->", "None", ":", "\n", "        ", "self", ".", "hidden_dim", "=", "hidden_dim", "\n", "super", "(", ")", ".", "__init__", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.token_embedders.pass_through_token_embedder.PassThroughTokenEmbedder.get_output_dim": [[21, 23], ["None"], "methods", ["None"], ["", "def", "get_output_dim", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "hidden_dim", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.token_embedders.pass_through_token_embedder.PassThroughTokenEmbedder.forward": [[24, 26], ["None"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "tokens", ":", "torch", ".", "Tensor", ")", "->", "torch", ".", "Tensor", ":", "\n", "        ", "return", "tokens", "\n", "", "", ""]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.token_embedders.embedding.Embedding.__init__": [[92, 137], ["allennlp.modules.token_embedders.token_embedder.TokenEmbedder.__init__", "torch.FloatTensor", "torch.nn.Parameter", "torch.nn.init.xavier_uniform_", "torch.nn.Parameter", "torch.nn.functional.embedding.Embedding.weight.data[].fill_", "torch.nn.Linear", "torch.FloatTensor.size", "allennlp.common.checks.ConfigurationError"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.graph_util.plot.GraphPlot.__init__"], ["def", "__init__", "(", "\n", "self", ",", "\n", "num_embeddings", ":", "int", ",", "\n", "embedding_dim", ":", "int", ",", "\n", "projection_dim", ":", "int", "=", "None", ",", "\n", "weight", ":", "torch", ".", "FloatTensor", "=", "None", ",", "\n", "padding_index", ":", "int", "=", "None", ",", "\n", "trainable", ":", "bool", "=", "True", ",", "\n", "max_norm", ":", "float", "=", "None", ",", "\n", "norm_type", ":", "float", "=", "2.0", ",", "\n", "scale_grad_by_freq", ":", "bool", "=", "False", ",", "\n", "sparse", ":", "bool", "=", "False", ",", "\n", "vocab_namespace", ":", "str", "=", "None", ",", "\n", "pretrained_file", ":", "str", "=", "None", ",", "\n", ")", "->", "None", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "num_embeddings", "=", "num_embeddings", "\n", "self", ".", "padding_index", "=", "padding_index", "\n", "self", ".", "max_norm", "=", "max_norm", "\n", "self", ".", "norm_type", "=", "norm_type", "\n", "self", ".", "scale_grad_by_freq", "=", "scale_grad_by_freq", "\n", "self", ".", "sparse", "=", "sparse", "\n", "self", ".", "_vocab_namespace", "=", "vocab_namespace", "\n", "self", ".", "_pretrained_file", "=", "pretrained_file", "\n", "\n", "self", ".", "output_dim", "=", "projection_dim", "or", "embedding_dim", "\n", "\n", "if", "weight", "is", "None", ":", "\n", "            ", "weight", "=", "torch", ".", "FloatTensor", "(", "num_embeddings", ",", "embedding_dim", ")", "\n", "self", ".", "weight", "=", "torch", ".", "nn", ".", "Parameter", "(", "weight", ",", "requires_grad", "=", "trainable", ")", "\n", "torch", ".", "nn", ".", "init", ".", "xavier_uniform_", "(", "self", ".", "weight", ")", "\n", "", "else", ":", "\n", "            ", "if", "weight", ".", "size", "(", ")", "!=", "(", "num_embeddings", ",", "embedding_dim", ")", ":", "\n", "                ", "raise", "ConfigurationError", "(", "\n", "\"A weight matrix was passed with contradictory embedding shapes.\"", "\n", ")", "\n", "", "self", ".", "weight", "=", "torch", ".", "nn", ".", "Parameter", "(", "weight", ",", "requires_grad", "=", "trainable", ")", "\n", "\n", "", "if", "self", ".", "padding_index", "is", "not", "None", ":", "\n", "            ", "self", ".", "weight", ".", "data", "[", "self", ".", "padding_index", "]", ".", "fill_", "(", "0", ")", "\n", "\n", "", "if", "projection_dim", ":", "\n", "            ", "self", ".", "_projection", "=", "torch", ".", "nn", ".", "Linear", "(", "embedding_dim", ",", "projection_dim", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "_projection", "=", "None", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.token_embedders.embedding.Embedding.get_output_dim": [[138, 141], ["None"], "methods", ["None"], ["", "", "@", "overrides", "\n", "def", "get_output_dim", "(", "self", ")", "->", "int", ":", "\n", "        ", "return", "self", ".", "output_dim", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.token_embedders.embedding.Embedding.forward": [[142, 170], ["allennlp.nn.util.combine_initial_dims.size", "allennlp.nn.util.combine_initial_dims", "torch.nn.functional.embedding", "allennlp.nn.util.uncombine_initial_dims", "range", "allennlp.modules.time_distributed.TimeDistributed.", "allennlp.modules.time_distributed.TimeDistributed", "allennlp.modules.time_distributed.TimeDistributed.dim"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.nn.util.combine_initial_dims", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.nn.util.uncombine_initial_dims"], ["", "@", "overrides", "\n", "def", "forward", "(", "self", ",", "tokens", ":", "torch", ".", "Tensor", ")", "->", "torch", ".", "Tensor", ":", "\n", "# tokens may have extra dimensions (batch_size, d1, ..., dn, sequence_length),", "\n", "# but embedding expects (batch_size, sequence_length), so pass tokens to", "\n", "# util.combine_initial_dims (which is a no-op if there are no extra dimensions).", "\n", "# Remember the original size.", "\n", "        ", "original_size", "=", "tokens", ".", "size", "(", ")", "\n", "tokens", "=", "util", ".", "combine_initial_dims", "(", "tokens", ")", "\n", "\n", "embedded", "=", "embedding", "(", "\n", "tokens", ",", "\n", "self", ".", "weight", ",", "\n", "padding_idx", "=", "self", ".", "padding_index", ",", "\n", "max_norm", "=", "self", ".", "max_norm", ",", "\n", "norm_type", "=", "self", ".", "norm_type", ",", "\n", "scale_grad_by_freq", "=", "self", ".", "scale_grad_by_freq", ",", "\n", "sparse", "=", "self", ".", "sparse", ",", "\n", ")", "\n", "\n", "# Now (if necessary) add back in the extra dimensions.", "\n", "embedded", "=", "util", ".", "uncombine_initial_dims", "(", "embedded", ",", "original_size", ")", "\n", "\n", "if", "self", ".", "_projection", ":", "\n", "            ", "projection", "=", "self", ".", "_projection", "\n", "for", "_", "in", "range", "(", "embedded", ".", "dim", "(", ")", "-", "2", ")", ":", "\n", "                ", "projection", "=", "TimeDistributed", "(", "projection", ")", "\n", "", "embedded", "=", "projection", "(", "embedded", ")", "\n", "", "return", "embedded", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.token_embedders.embedding.Embedding.extend_vocab": [[171, 280], ["extended_vocab.get_vocab_size", "torch.cat", "torch.nn.Parameter", "logging.info", "allennlp.common.checks.ConfigurationError", "allennlp.common.file_utils.is_url_or_existing_file", "torch.FloatTensor", "torch.nn.init.xavier_uniform_", "embedding._read_pretrained_embeddings_file", "allennlp.common.checks.ConfigurationError", "allennlp.common.file_utils.is_url_or_existing_file", "torch.FloatTensor.to", "logging.warning"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.data.vocabulary.Vocabulary.get_vocab_size", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.dataset_readers.multiprocess_dataset_reader.logger.info", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.file_utils.is_url_or_existing_file", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.token_embedders.embedding._read_pretrained_embeddings_file", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.file_utils.is_url_or_existing_file"], ["", "def", "extend_vocab", "(", "\n", "self", ",", "\n", "extended_vocab", ":", "Vocabulary", ",", "\n", "vocab_namespace", ":", "str", "=", "None", ",", "\n", "extension_pretrained_file", ":", "str", "=", "None", ",", "\n", "model_path", ":", "str", "=", "None", ",", "\n", ")", ":", "\n", "        ", "\"\"\"\n        Extends the embedding matrix according to the extended vocabulary.\n        If extension_pretrained_file is available, it will be used for initializing the new words\n        embeddings in the extended vocabulary; otherwise we will check if _pretrained_file attribute\n        is already available. If none is available, they will be initialized with xavier uniform.\n\n        # Parameters\n\n        extended_vocab : `Vocabulary`\n            Vocabulary extended from original vocabulary used to construct\n            this `Embedding`.\n        vocab_namespace : `str`, (optional, default=None)\n            In case you know what vocab_namespace should be used for extension, you\n            can pass it. If not passed, it will check if vocab_namespace used at the\n            time of `Embedding` construction is available. If so, this namespace\n            will be used or else extend_vocab will be a no-op.\n        extension_pretrained_file : `str`, (optional, default=None)\n            A file containing pretrained embeddings can be specified here. It can be\n            the path to a local file or an URL of a (cached) remote file. Check format\n            details in `from_params` of `Embedding` class.\n        model_path : `str`, (optional, default=None)\n            Path traversing the model attributes upto this embedding module.\n            Eg. \"_text_field_embedder.token_embedder_tokens\". This is only useful\n            to give helpful error message when extend_vocab is implicitly called\n            by fine-tune or any other command.\n        \"\"\"", "\n", "# Caveat: For allennlp v0.8.1 and below, we weren't storing vocab_namespace as an attribute,", "\n", "# knowing which is necessary at time of embedding vocab extension. So old archive models are", "\n", "# currently unextendable.", "\n", "\n", "vocab_namespace", "=", "vocab_namespace", "or", "self", ".", "_vocab_namespace", "\n", "if", "not", "vocab_namespace", ":", "\n", "# It's not safe to default to \"tokens\" or any other namespace.", "\n", "            ", "logging", ".", "info", "(", "\n", "\"Loading a model trained before embedding extension was implemented; \"", "\n", "\"pass an explicit vocab namespace if you want to extend the vocabulary.\"", "\n", ")", "\n", "return", "\n", "\n", "", "extended_num_embeddings", "=", "extended_vocab", ".", "get_vocab_size", "(", "vocab_namespace", ")", "\n", "if", "extended_num_embeddings", "==", "self", ".", "num_embeddings", ":", "\n", "# It's already been extended. No need to initialize / read pretrained file in first place (no-op)", "\n", "            ", "return", "\n", "\n", "", "if", "extended_num_embeddings", "<", "self", ".", "num_embeddings", ":", "\n", "            ", "raise", "ConfigurationError", "(", "\n", "f\"Size of namespace, {vocab_namespace} for extended_vocab is smaller than \"", "\n", "f\"embedding. You likely passed incorrect vocab or namespace for extension.\"", "\n", ")", "\n", "\n", "# Case 1: user passed extension_pretrained_file and it's available.", "\n", "", "if", "extension_pretrained_file", "and", "is_url_or_existing_file", "(", "\n", "extension_pretrained_file", "\n", ")", ":", "\n", "# Don't have to do anything here, this is the happy case.", "\n", "            ", "pass", "\n", "# Case 2: user passed extension_pretrained_file and it's not available", "\n", "", "elif", "extension_pretrained_file", ":", "\n", "            ", "raise", "ConfigurationError", "(", "\n", "f\"You passed pretrained embedding file {extension_pretrained_file} \"", "\n", "f\"for model_path {model_path} but it's not available.\"", "\n", ")", "\n", "# Case 3: user didn't pass extension_pretrained_file, but pretrained_file attribute was", "\n", "# saved during training and is available.", "\n", "", "elif", "is_url_or_existing_file", "(", "self", ".", "_pretrained_file", ")", ":", "\n", "            ", "extension_pretrained_file", "=", "self", ".", "_pretrained_file", "\n", "# Case 4: no file is available, hope that pretrained embeddings weren't used in the first place and warn", "\n", "", "else", ":", "\n", "            ", "extra_info", "=", "(", "\n", "f\"Originally pretrained_file was at \"", "f\"{self._pretrained_file}. \"", "\n", "if", "self", ".", "_pretrained_file", "\n", "else", "\"\"", "\n", ")", "\n", "# It's better to warn here and not give error because there is no way to distinguish between", "\n", "# whether pretrained-file wasn't used during training or user forgot to pass / passed incorrect", "\n", "# mapping. Raising an error would prevent fine-tuning in the former case.", "\n", "logging", ".", "warning", "(", "\n", "f\"Embedding at model_path, {model_path} cannot locate the pretrained_file. \"", "\n", "f\"{extra_info} If you are fine-tuning and want to use using pretrained_file for \"", "\n", "f\"embedding extension, please pass the mapping by --embedding-sources argument.\"", "\n", ")", "\n", "\n", "", "embedding_dim", "=", "self", ".", "weight", ".", "data", ".", "shape", "[", "-", "1", "]", "\n", "if", "not", "extension_pretrained_file", ":", "\n", "            ", "extra_num_embeddings", "=", "extended_num_embeddings", "-", "self", ".", "num_embeddings", "\n", "extra_weight", "=", "torch", ".", "FloatTensor", "(", "extra_num_embeddings", ",", "embedding_dim", ")", "\n", "torch", ".", "nn", ".", "init", ".", "xavier_uniform_", "(", "extra_weight", ")", "\n", "", "else", ":", "\n", "# It's easiest to just reload the embeddings for the entire vocab,", "\n", "# then only keep the ones we need.", "\n", "            ", "whole_weight", "=", "_read_pretrained_embeddings_file", "(", "\n", "extension_pretrained_file", ",", "\n", "embedding_dim", ",", "\n", "extended_vocab", ",", "\n", "vocab_namespace", ",", "\n", ")", "\n", "extra_weight", "=", "whole_weight", "[", "self", ".", "num_embeddings", ":", ",", ":", "]", "\n", "\n", "", "device", "=", "self", ".", "weight", ".", "data", ".", "device", "\n", "extended_weight", "=", "torch", ".", "cat", "(", "[", "self", ".", "weight", ".", "data", ",", "extra_weight", ".", "to", "(", "device", ")", "]", ",", "dim", "=", "0", ")", "\n", "self", ".", "weight", "=", "torch", ".", "nn", ".", "Parameter", "(", "\n", "extended_weight", ",", "requires_grad", "=", "self", ".", "weight", ".", "requires_grad", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.token_embedders.embedding.Embedding.from_vocab_or_file": [[282, 361], ["cls", "vocab.get_vocab_size", "embedding._read_pretrained_embeddings_file"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.data.vocabulary.Vocabulary.get_vocab_size", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.token_embedders.embedding._read_pretrained_embeddings_file"], ["", "@", "classmethod", "\n", "def", "from_vocab_or_file", "(", "\n", "cls", ",", "\n", "vocab", ":", "Vocabulary", ",", "\n", "embedding_dim", ":", "int", ",", "\n", "num_embeddings", ":", "int", "=", "None", ",", "\n", "vocab_namespace", ":", "str", "=", "\"tokens\"", ",", "\n", "pretrained_file", ":", "str", "=", "None", ",", "\n", "projection_dim", ":", "int", "=", "None", ",", "\n", "trainable", ":", "bool", "=", "True", ",", "\n", "padding_index", ":", "int", "=", "None", ",", "\n", "max_norm", ":", "float", "=", "None", ",", "\n", "norm_type", ":", "float", "=", "2.0", ",", "\n", "scale_grad_by_freq", ":", "bool", "=", "False", ",", "\n", "sparse", ":", "bool", "=", "False", ",", "\n", ")", "->", "\"Embedding\"", ":", "\n", "        ", "\"\"\"\n        Similar to `__init__`, but does two functions on top of what's there: (1) if\n        `num_embeddings` is not given, it checks the vocabulary for how many embeddings to\n        construct; and (2) if a pretrained file is given, it loads weights from that file (while\n        looking at the given vocabulary) and passes those weights to `__init__`.\n\n        We need the vocabulary here to know how many items we need to embed, and we look for a\n        `vocab_namespace` key in the parameter dictionary to know which vocabulary to use.  If\n        you know beforehand exactly how many embeddings you need, or aren't using a vocabulary\n        mapping for the things getting embedded here, then you can pass in the `num_embeddings`\n        key directly, and the vocabulary will be ignored.\n\n        A file containing pretrained embeddings can be specified using the parameter\n        `\"pretrained_file\"`.\n        It can be the path to a local file or an URL of a (cached) remote file.\n        Two formats are supported:\n\n            * hdf5 file - containing an embedding matrix in the form of a torch.Tensor;\n\n            * text file - an utf-8 encoded text file with space separated fields::\n\n                    [word] [dim 1] [dim 2] ...\n\n              The text file can eventually be compressed with gzip, bz2, lzma or zip.\n              You can even select a single file inside an archive containing multiple files\n              using the URI::\n\n                    \"(archive_uri)#file_path_inside_the_archive\"\n\n              where `archive_uri` can be a file system path or a URL. For example::\n\n                    \"(https://nlp.stanford.edu/data/glove.twitter.27B.zip)#glove.twitter.27B.200d.txt\"\n        \"\"\"", "\n", "\n", "if", "num_embeddings", "is", "None", ":", "\n", "            ", "num_embeddings", "=", "vocab", ".", "get_vocab_size", "(", "vocab_namespace", ")", "\n", "", "else", ":", "\n", "# If num_embeddings is present, set default namespace to None so that extend_vocab", "\n", "# call doesn't misinterpret that some namespace was originally used.", "\n", "            ", "vocab_namespace", "=", "None", "\n", "\n", "", "if", "pretrained_file", ":", "\n", "# If we're loading a saved model, we don't want to actually read a pre-trained", "\n", "# embedding file - the embeddings will just be in our saved weights, and we might not", "\n", "# have the original embedding file anymore, anyway.", "\n", "            ", "weight", "=", "_read_pretrained_embeddings_file", "(", "\n", "pretrained_file", ",", "embedding_dim", ",", "vocab", ",", "vocab_namespace", "\n", ")", "\n", "", "else", ":", "\n", "            ", "weight", "=", "None", "\n", "\n", "", "return", "cls", "(", "\n", "num_embeddings", "=", "num_embeddings", ",", "\n", "embedding_dim", "=", "embedding_dim", ",", "\n", "projection_dim", "=", "projection_dim", ",", "\n", "weight", "=", "weight", ",", "\n", "padding_index", "=", "padding_index", ",", "\n", "trainable", "=", "trainable", ",", "\n", "max_norm", "=", "max_norm", ",", "\n", "norm_type", "=", "norm_type", ",", "\n", "scale_grad_by_freq", "=", "scale_grad_by_freq", ",", "\n", "sparse", "=", "sparse", ",", "\n", "vocab_namespace", "=", "vocab_namespace", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.token_embedders.embedding.EmbeddingsTextFile.__init__": [[569, 637], ["embedding.parse_embeddings_file_uri", "allennlp.common.file_utils.cached_path", "zipfile.is_zipfile", "next", "torch.nn.functional.embedding.EmbeddingsTextFile._get_num_tokens_from_first_line", "torch.nn.functional.embedding.EmbeddingsTextFile._open_inside_zip", "tarfile.is_tarfile", "itertools.chain", "torch.nn.functional.embedding.EmbeddingsTextFile._open_inside_tar", "allennlp.common.file_utils.get_file_extension", "package.open", "ValueError", "logger.warning"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.token_embedders.embedding.parse_embeddings_file_uri", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.file_utils.cached_path", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.token_embedders.embedding.EmbeddingsTextFile._get_num_tokens_from_first_line", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.token_embedders.embedding.EmbeddingsTextFile._open_inside_zip", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.token_embedders.embedding.EmbeddingsTextFile._open_inside_tar", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.file_utils.get_file_extension"], ["def", "__init__", "(", "\n", "self", ",", "file_uri", ":", "str", ",", "encoding", ":", "str", "=", "DEFAULT_ENCODING", ",", "cache_dir", ":", "str", "=", "None", "\n", ")", "->", "None", ":", "\n", "\n", "        ", "self", ".", "uri", "=", "file_uri", "\n", "self", ".", "_encoding", "=", "encoding", "\n", "self", ".", "_cache_dir", "=", "cache_dir", "\n", "self", ".", "_archive_handle", ":", "Any", "=", "None", "# only if the file is inside an archive", "\n", "\n", "main_file_uri", ",", "path_inside_archive", "=", "parse_embeddings_file_uri", "(", "file_uri", ")", "\n", "main_file_local_path", "=", "cached_path", "(", "main_file_uri", ",", "cache_dir", "=", "cache_dir", ")", "\n", "\n", "if", "zipfile", ".", "is_zipfile", "(", "main_file_local_path", ")", ":", "# ZIP archive", "\n", "            ", "self", ".", "_open_inside_zip", "(", "main_file_uri", ",", "path_inside_archive", ")", "\n", "\n", "", "elif", "tarfile", ".", "is_tarfile", "(", "main_file_local_path", ")", ":", "# TAR archive", "\n", "            ", "self", ".", "_open_inside_tar", "(", "main_file_uri", ",", "path_inside_archive", ")", "\n", "\n", "", "else", ":", "# all the other supported formats, including uncompressed files", "\n", "            ", "if", "path_inside_archive", ":", "\n", "                ", "raise", "ValueError", "(", "\"Unsupported archive format: %s\"", "+", "main_file_uri", ")", "\n", "\n", "# All the python packages for compressed files share the same interface of io.open", "\n", "", "extension", "=", "get_file_extension", "(", "main_file_uri", ")", "\n", "\n", "# Some systems don't have support for all of these libraries, so we import them only", "\n", "# when necessary.", "\n", "package", "=", "None", "\n", "if", "extension", "in", "[", "\".txt\"", ",", "\".vec\"", "]", ":", "\n", "                ", "package", "=", "io", "\n", "", "elif", "extension", "==", "\".gz\"", ":", "\n", "                ", "import", "gzip", "\n", "\n", "package", "=", "gzip", "\n", "", "elif", "extension", "==", "\".bz2\"", ":", "\n", "                ", "import", "bz2", "\n", "\n", "package", "=", "bz2", "\n", "", "elif", "extension", "==", "\".lzma\"", ":", "\n", "                ", "import", "lzma", "\n", "\n", "package", "=", "lzma", "\n", "\n", "", "if", "package", "is", "None", ":", "\n", "                ", "logger", ".", "warning", "(", "\n", "'The embeddings file has an unknown file extension \"%s\". '", "\n", "\"We will assume the file is an (uncompressed) text file\"", ",", "\n", "extension", ",", "\n", ")", "\n", "package", "=", "io", "\n", "\n", "", "self", ".", "_handle", "=", "package", ".", "open", "(", "# type: ignore", "\n", "main_file_local_path", ",", "\"rt\"", ",", "encoding", "=", "encoding", "\n", ")", "\n", "\n", "# To use this with tqdm we'd like to know the number of tokens. It's possible that the", "\n", "# first line of the embeddings file contains this: if it does, we want to start iteration", "\n", "# from the 2nd line, otherwise we want to start from the 1st.", "\n", "# Unfortunately, once we read the first line, we cannot move back the file iterator", "\n", "# because the underlying file may be \"not seekable\"; we use itertools.chain instead.", "\n", "", "first_line", "=", "next", "(", "self", ".", "_handle", ")", "# this moves the iterator forward", "\n", "self", ".", "num_tokens", "=", "EmbeddingsTextFile", ".", "_get_num_tokens_from_first_line", "(", "first_line", ")", "\n", "if", "self", ".", "num_tokens", ":", "\n", "# the first line is a header line: start iterating from the 2nd line", "\n", "            ", "self", ".", "_iterator", "=", "self", ".", "_handle", "\n", "", "else", ":", "\n", "# the first line is not a header line: start iterating from the 1st line", "\n", "            ", "self", ".", "_iterator", "=", "itertools", ".", "chain", "(", "[", "first_line", "]", ",", "self", ".", "_handle", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.token_embedders.embedding.EmbeddingsTextFile._open_inside_zip": [[638, 652], ["allennlp.common.file_utils.cached_path", "zipfile.ZipFile", "typing.cast", "zipfile.ZipFile.open", "io.TextIOWrapper", "zipfile.ZipFile.namelist", "torch.nn.functional.embedding.EmbeddingsTextFile._get_the_only_file_in_the_archive"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.file_utils.cached_path", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.token_embedders.embedding.EmbeddingsTextFile._get_the_only_file_in_the_archive"], ["", "", "def", "_open_inside_zip", "(", "\n", "self", ",", "archive_path", ":", "str", ",", "member_path", ":", "Optional", "[", "str", "]", "=", "None", "\n", ")", "->", "None", ":", "\n", "        ", "cached_archive_path", "=", "cached_path", "(", "archive_path", ",", "cache_dir", "=", "self", ".", "_cache_dir", ")", "\n", "archive", "=", "zipfile", ".", "ZipFile", "(", "cached_archive_path", ",", "\"r\"", ")", "\n", "if", "member_path", "is", "None", ":", "\n", "            ", "members_list", "=", "archive", ".", "namelist", "(", ")", "\n", "member_path", "=", "self", ".", "_get_the_only_file_in_the_archive", "(", "\n", "members_list", ",", "archive_path", "\n", ")", "\n", "", "member_path", "=", "cast", "(", "str", ",", "member_path", ")", "\n", "member_file", "=", "archive", ".", "open", "(", "member_path", ",", "\"r\"", ")", "\n", "self", ".", "_handle", "=", "io", ".", "TextIOWrapper", "(", "member_file", ",", "encoding", "=", "self", ".", "_encoding", ")", "\n", "self", ".", "_archive_handle", "=", "archive", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.token_embedders.embedding.EmbeddingsTextFile._open_inside_tar": [[653, 668], ["allennlp.common.file_utils.cached_path", "tarfile.open", "typing.cast", "tarfile.open.getmember", "typing.cast", "io.TextIOWrapper", "tarfile.open.getnames", "torch.nn.functional.embedding.EmbeddingsTextFile._get_the_only_file_in_the_archive", "tarfile.open.extractfile"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.file_utils.cached_path", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.token_embedders.embedding.EmbeddingsTextFile._get_the_only_file_in_the_archive"], ["", "def", "_open_inside_tar", "(", "\n", "self", ",", "archive_path", ":", "str", ",", "member_path", ":", "Optional", "[", "str", "]", "=", "None", "\n", ")", "->", "None", ":", "\n", "        ", "cached_archive_path", "=", "cached_path", "(", "archive_path", ",", "cache_dir", "=", "self", ".", "_cache_dir", ")", "\n", "archive", "=", "tarfile", ".", "open", "(", "cached_archive_path", ",", "\"r\"", ")", "\n", "if", "member_path", "is", "None", ":", "\n", "            ", "members_list", "=", "archive", ".", "getnames", "(", ")", "\n", "member_path", "=", "self", ".", "_get_the_only_file_in_the_archive", "(", "\n", "members_list", ",", "archive_path", "\n", ")", "\n", "", "member_path", "=", "cast", "(", "str", ",", "member_path", ")", "\n", "member", "=", "archive", ".", "getmember", "(", "member_path", ")", "# raises exception if not present", "\n", "member_file", "=", "cast", "(", "IO", "[", "bytes", "]", ",", "archive", ".", "extractfile", "(", "member", ")", ")", "\n", "self", ".", "_handle", "=", "io", ".", "TextIOWrapper", "(", "member_file", ",", "encoding", "=", "self", ".", "_encoding", ")", "\n", "self", ".", "_archive_handle", "=", "archive", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.token_embedders.embedding.EmbeddingsTextFile.read": [[669, 671], ["None"], "methods", ["None"], ["", "def", "read", "(", "self", ")", "->", "str", ":", "\n", "        ", "return", "\"\"", ".", "join", "(", "self", ".", "_iterator", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.token_embedders.embedding.EmbeddingsTextFile.readline": [[672, 674], ["next"], "methods", ["None"], ["", "def", "readline", "(", "self", ")", "->", "str", ":", "\n", "        ", "return", "next", "(", "self", ".", "_iterator", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.token_embedders.embedding.EmbeddingsTextFile.close": [[675, 679], ["torch.nn.functional.embedding.EmbeddingsTextFile._handle.close", "torch.nn.functional.embedding.EmbeddingsTextFile._archive_handle.close"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.token_embedders.embedding.EmbeddingsTextFile.close", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.token_embedders.embedding.EmbeddingsTextFile.close"], ["", "def", "close", "(", "self", ")", "->", "None", ":", "\n", "        ", "self", ".", "_handle", ".", "close", "(", ")", "\n", "if", "self", ".", "_archive_handle", ":", "\n", "            ", "self", ".", "_archive_handle", ".", "close", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.token_embedders.embedding.EmbeddingsTextFile.__enter__": [[680, 682], ["None"], "methods", ["None"], ["", "", "def", "__enter__", "(", "self", ")", "->", "\"EmbeddingsTextFile\"", ":", "\n", "        ", "return", "self", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.token_embedders.embedding.EmbeddingsTextFile.__exit__": [[683, 685], ["torch.nn.functional.embedding.EmbeddingsTextFile.close"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.token_embedders.embedding.EmbeddingsTextFile.close"], ["", "def", "__exit__", "(", "self", ",", "exc_type", ",", "exc_val", ",", "exc_tb", ")", "->", "None", ":", "\n", "        ", "self", ".", "close", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.token_embedders.embedding.EmbeddingsTextFile.__iter__": [[686, 688], ["None"], "methods", ["None"], ["", "def", "__iter__", "(", "self", ")", "->", "\"EmbeddingsTextFile\"", ":", "\n", "        ", "return", "self", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.token_embedders.embedding.EmbeddingsTextFile.__next__": [[689, 691], ["next"], "methods", ["None"], ["", "def", "__next__", "(", "self", ")", "->", "str", ":", "\n", "        ", "return", "next", "(", "self", ".", "_iterator", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.token_embedders.embedding.EmbeddingsTextFile.__len__": [[692, 698], ["AttributeError"], "methods", ["None"], ["", "def", "__len__", "(", "self", ")", "->", "Optional", "[", "int", "]", ":", "\n", "        ", "\"\"\" Hack for tqdm: no need for explicitly passing `total=file.num_tokens` \"\"\"", "\n", "if", "self", ".", "num_tokens", ":", "\n", "            ", "return", "self", ".", "num_tokens", "\n", "", "raise", "AttributeError", "(", "\n", "'an object of type EmbeddingsTextFile has \"len()\" only if the underlying '", "\n", "\"text file declares the number of tokens (i.e. the number of lines following)\"", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.token_embedders.embedding.EmbeddingsTextFile._get_the_only_file_in_the_archive": [[702, 718], ["len", "ValueError", "embedding.format_embeddings_file_uri"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.token_embedders.embedding.format_embeddings_file_uri"], ["", "@", "staticmethod", "\n", "def", "_get_the_only_file_in_the_archive", "(", "\n", "members_list", ":", "Sequence", "[", "str", "]", ",", "archive_path", ":", "str", "\n", ")", "->", "str", ":", "\n", "        ", "if", "len", "(", "members_list", ")", ">", "1", ":", "\n", "            ", "raise", "ValueError", "(", "\n", "\"The archive %s contains multiple files, so you must select \"", "\n", "\"one of the files inside providing a uri of the type: %s.\"", "\n", "%", "(", "\n", "archive_path", ",", "\n", "format_embeddings_file_uri", "(", "\n", "\"path_or_url_to_archive\"", ",", "\"path_inside_archive\"", "\n", ")", ",", "\n", ")", "\n", ")", "\n", "", "return", "members_list", "[", "0", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.token_embedders.embedding.EmbeddingsTextFile._get_num_tokens_from_first_line": [[719, 737], ["line.split", "len", "max", "logger.info", "int"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.dataset_readers.multiprocess_dataset_reader.logger.info"], ["", "@", "staticmethod", "\n", "def", "_get_num_tokens_from_first_line", "(", "line", ":", "str", ")", "->", "Optional", "[", "int", "]", ":", "\n", "        ", "\"\"\" This function takes in input a string and if it contains 1 or 2 integers, it assumes the\n        largest one it the number of tokens. Returns None if the line doesn't match that pattern. \"\"\"", "\n", "fields", "=", "line", ".", "split", "(", "\" \"", ")", "\n", "if", "1", "<=", "len", "(", "fields", ")", "<=", "2", ":", "\n", "            ", "try", ":", "\n", "                ", "int_fields", "=", "[", "int", "(", "x", ")", "for", "x", "in", "fields", "]", "\n", "", "except", "ValueError", ":", "\n", "                ", "return", "None", "\n", "", "else", ":", "\n", "                ", "num_tokens", "=", "max", "(", "int_fields", ")", "\n", "logger", ".", "info", "(", "\n", "\"Recognized a header line in the embedding file with number of tokens: %d\"", ",", "\n", "num_tokens", ",", "\n", ")", "\n", "return", "num_tokens", "\n", "", "", "return", "None", "\n", "", "", ""]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.token_embedders.embedding._read_pretrained_embeddings_file": [[367, 417], ["allennlp.common.file_utils.get_file_extension", "embedding._read_embeddings_from_text_file", "embedding._read_embeddings_from_hdf5"], "function", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.file_utils.get_file_extension", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.token_embedders.embedding._read_embeddings_from_text_file", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.token_embedders.embedding._read_embeddings_from_hdf5"], ["def", "_read_pretrained_embeddings_file", "(", "\n", "file_uri", ":", "str", ",", "embedding_dim", ":", "int", ",", "vocab", ":", "Vocabulary", ",", "namespace", ":", "str", "=", "\"tokens\"", "\n", ")", "->", "torch", ".", "FloatTensor", ":", "\n", "    ", "\"\"\"\n    Returns and embedding matrix for the given vocabulary using the pretrained embeddings\n    contained in the given file. Embeddings for tokens not found in the pretrained embedding file\n    are randomly initialized using a normal distribution with mean and standard deviation equal to\n    those of the pretrained embeddings.\n\n    We support two file formats:\n\n        * text format - utf-8 encoded text file with space separated fields: [word] [dim 1] [dim 2] ...\n          The text file can eventually be compressed, and even resides in an archive with multiple files.\n          If the file resides in an archive with other files, then `embeddings_filename` must\n          be a URI \"(archive_uri)#file_path_inside_the_archive\"\n\n        * hdf5 format - hdf5 file containing an embedding matrix in the form of a torch.Tensor.\n\n    If the filename ends with '.hdf5' or '.h5' then we load from hdf5, otherwise we assume\n    text format.\n\n    # Parameters\n\n    file_uri : `str`, required.\n        It can be:\n\n        * a file system path or a URL of an eventually compressed text file or a zip/tar archive\n          containing a single file.\n\n        * URI of the type `(archive_path_or_url)#file_path_inside_archive` if the text file\n          is contained in a multi-file archive.\n\n    vocab : `Vocabulary`, required.\n        A Vocabulary object.\n    namespace : `str`, (optional, default=tokens)\n        The namespace of the vocabulary to find pretrained embeddings for.\n    trainable : `bool`, (optional, default=True)\n        Whether or not the embedding parameters should be optimized.\n\n    # Returns\n\n    A weight matrix with embeddings initialized from the read file.  The matrix has shape\n    `(vocab.get_vocab_size(namespace), embedding_dim)`, where the indices of words appearing in\n    the pretrained embedding file are initialized to the pretrained embedding value.\n    \"\"\"", "\n", "file_ext", "=", "get_file_extension", "(", "file_uri", ")", "\n", "if", "file_ext", "in", "[", "\".h5\"", ",", "\".hdf5\"", "]", ":", "\n", "        ", "return", "_read_embeddings_from_hdf5", "(", "file_uri", ",", "embedding_dim", ",", "vocab", ",", "namespace", ")", "\n", "\n", "", "return", "_read_embeddings_from_text_file", "(", "file_uri", ",", "embedding_dim", ",", "vocab", ",", "namespace", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.token_embedders.embedding._read_embeddings_from_text_file": [[419, 501], ["set", "vocab.get_vocab_size", "logger.info", "numpy.asarray", "float", "float", "logger.info", "torch.FloatTensor().normal_", "vocab.get_index_to_token_vocabulary", "range", "logger.info", "vocab.get_index_to_token_vocabulary().values", "embedding.EmbeddingsTextFile", "allennlp.common.Tqdm.tqdm", "allennlp.common.checks.ConfigurationError", "list", "numpy.mean", "numpy.std", "embeddings.values", "torch.FloatTensor", "torch.FloatTensor", "logger.debug", "vocab.get_index_to_token_vocabulary", "line.split", "line.rstrip().split", "numpy.asarray", "logger.warning", "line.rstrip", "len", "len"], "function", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.data.vocabulary.Vocabulary.get_vocab_size", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.dataset_readers.multiprocess_dataset_reader.logger.info", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.dataset_readers.multiprocess_dataset_reader.logger.info", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.data.vocabulary.Vocabulary.get_index_to_token_vocabulary", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.dataset_readers.multiprocess_dataset_reader.logger.info", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.tqdm.Tqdm.tqdm", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.data.vocabulary.Vocabulary.get_index_to_token_vocabulary"], ["", "def", "_read_embeddings_from_text_file", "(", "\n", "file_uri", ":", "str", ",", "embedding_dim", ":", "int", ",", "vocab", ":", "Vocabulary", ",", "namespace", ":", "str", "=", "\"tokens\"", "\n", ")", "->", "torch", ".", "FloatTensor", ":", "\n", "    ", "\"\"\"\n    Read pre-trained word vectors from an eventually compressed text file, possibly contained\n    inside an archive with multiple files. The text file is assumed to be utf-8 encoded with\n    space-separated fields: [word] [dim 1] [dim 2] ...\n\n    Lines that contain more numerical tokens than `embedding_dim` raise a warning and are skipped.\n\n    The remainder of the docstring is identical to `_read_pretrained_embeddings_file`.\n    \"\"\"", "\n", "tokens_to_keep", "=", "set", "(", "vocab", ".", "get_index_to_token_vocabulary", "(", "namespace", ")", ".", "values", "(", ")", ")", "\n", "vocab_size", "=", "vocab", ".", "get_vocab_size", "(", "namespace", ")", "\n", "embeddings", "=", "{", "}", "\n", "\n", "# First we read the embeddings from the file, only keeping vectors for the words we need.", "\n", "logger", ".", "info", "(", "\"Reading pretrained embeddings from file\"", ")", "\n", "\n", "with", "EmbeddingsTextFile", "(", "file_uri", ")", "as", "embeddings_file", ":", "\n", "        ", "for", "line", "in", "Tqdm", ".", "tqdm", "(", "embeddings_file", ")", ":", "\n", "            ", "token", "=", "line", ".", "split", "(", "\" \"", ",", "1", ")", "[", "0", "]", "\n", "if", "token", "in", "tokens_to_keep", ":", "\n", "                ", "fields", "=", "line", ".", "rstrip", "(", ")", ".", "split", "(", "\" \"", ")", "\n", "if", "len", "(", "fields", ")", "-", "1", "!=", "embedding_dim", ":", "\n", "# Sometimes there are funny unicode parsing problems that lead to different", "\n", "# fields lengths (e.g., a word with a unicode space character that splits", "\n", "# into more than one column).  We skip those lines.  Note that if you have", "\n", "# some kind of long header, this could result in all of your lines getting", "\n", "# skipped.  It's hard to check for that here; you just have to look in the", "\n", "# embedding_misses_file and at the model summary to make sure things look", "\n", "# like they are supposed to.", "\n", "                    ", "logger", ".", "warning", "(", "\n", "\"Found line with wrong number of dimensions (expected: %d; actual: %d): %s\"", ",", "\n", "embedding_dim", ",", "\n", "len", "(", "fields", ")", "-", "1", ",", "\n", "line", ",", "\n", ")", "\n", "continue", "\n", "\n", "", "vector", "=", "numpy", ".", "asarray", "(", "fields", "[", "1", ":", "]", ",", "dtype", "=", "\"float32\"", ")", "\n", "embeddings", "[", "token", "]", "=", "vector", "\n", "\n", "", "", "", "if", "not", "embeddings", ":", "\n", "        ", "raise", "ConfigurationError", "(", "\n", "\"No embeddings of correct dimension found; you probably \"", "\n", "\"misspecified your embedding_dim parameter, or didn't \"", "\n", "\"pre-populate your Vocabulary\"", "\n", ")", "\n", "\n", "", "all_embeddings", "=", "numpy", ".", "asarray", "(", "list", "(", "embeddings", ".", "values", "(", ")", ")", ")", "\n", "embeddings_mean", "=", "float", "(", "numpy", ".", "mean", "(", "all_embeddings", ")", ")", "\n", "embeddings_std", "=", "float", "(", "numpy", ".", "std", "(", "all_embeddings", ")", ")", "\n", "# Now we initialize the weight matrix for an embedding layer, starting with random vectors,", "\n", "# then filling in the word vectors we just read.", "\n", "logger", ".", "info", "(", "\"Initializing pre-trained embedding layer\"", ")", "\n", "embedding_matrix", "=", "torch", ".", "FloatTensor", "(", "vocab_size", ",", "embedding_dim", ")", ".", "normal_", "(", "\n", "embeddings_mean", ",", "embeddings_std", "\n", ")", "\n", "num_tokens_found", "=", "0", "\n", "index_to_token", "=", "vocab", ".", "get_index_to_token_vocabulary", "(", "namespace", ")", "\n", "for", "i", "in", "range", "(", "vocab_size", ")", ":", "\n", "        ", "token", "=", "index_to_token", "[", "i", "]", "\n", "\n", "# If we don't have a pre-trained vector for this word, we'll just leave this row alone,", "\n", "# so the word has a random initialization.", "\n", "if", "token", "in", "embeddings", ":", "\n", "            ", "embedding_matrix", "[", "i", "]", "=", "torch", ".", "FloatTensor", "(", "embeddings", "[", "token", "]", ")", "\n", "num_tokens_found", "+=", "1", "\n", "", "else", ":", "\n", "            ", "logger", ".", "debug", "(", "\n", "\"Token %s was not found in the embedding file. Initialising randomly.\"", ",", "\n", "token", ",", "\n", ")", "\n", "\n", "", "", "logger", ".", "info", "(", "\n", "\"Pretrained embeddings were found for %d out of %d tokens\"", ",", "\n", "num_tokens_found", ",", "\n", "vocab_size", ",", "\n", ")", "\n", "\n", "return", "embedding_matrix", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.token_embedders.embedding._read_embeddings_from_hdf5": [[503, 524], ["torch.FloatTensor", "h5py.File", "list", "allennlp.common.checks.ConfigurationError", "vocab.get_vocab_size", "list", "vocab.get_vocab_size"], "function", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.data.vocabulary.Vocabulary.get_vocab_size", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.data.vocabulary.Vocabulary.get_vocab_size"], ["", "def", "_read_embeddings_from_hdf5", "(", "\n", "embeddings_filename", ":", "str", ",", "\n", "embedding_dim", ":", "int", ",", "\n", "vocab", ":", "Vocabulary", ",", "\n", "namespace", ":", "str", "=", "\"tokens\"", ",", "\n", ")", "->", "torch", ".", "FloatTensor", ":", "\n", "    ", "\"\"\"\n    Reads from a hdf5 formatted file. The embedding matrix is assumed to\n    be keyed by 'embedding' and of size `(num_tokens, embedding_dim)`.\n    \"\"\"", "\n", "with", "h5py", ".", "File", "(", "embeddings_filename", ",", "\"r\"", ")", "as", "fin", ":", "\n", "        ", "embeddings", "=", "fin", "[", "\"embedding\"", "]", "[", "...", "]", "\n", "\n", "", "if", "list", "(", "embeddings", ".", "shape", ")", "!=", "[", "vocab", ".", "get_vocab_size", "(", "namespace", ")", ",", "embedding_dim", "]", ":", "\n", "        ", "raise", "ConfigurationError", "(", "\n", "\"Read shape {0} embeddings from the file, but expected {1}\"", ".", "format", "(", "\n", "list", "(", "embeddings", ".", "shape", ")", ",", "[", "vocab", ".", "get_vocab_size", "(", "namespace", ")", ",", "embedding_dim", "]", "\n", ")", "\n", ")", "\n", "\n", "", "return", "torch", ".", "FloatTensor", "(", "embeddings", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.token_embedders.embedding.format_embeddings_file_uri": [[526, 532], ["None"], "function", ["None"], ["", "def", "format_embeddings_file_uri", "(", "\n", "main_file_path_or_url", ":", "str", ",", "path_inside_archive", ":", "Optional", "[", "str", "]", "=", "None", "\n", ")", "->", "str", ":", "\n", "    ", "if", "path_inside_archive", ":", "\n", "        ", "return", "\"({})#{}\"", ".", "format", "(", "main_file_path_or_url", ",", "path_inside_archive", ")", "\n", "", "return", "main_file_path_or_url", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.token_embedders.embedding.parse_embeddings_file_uri": [[539, 546], ["re.fullmatch", "typing.cast", "embedding.EmbeddingsFileURI", "embedding.EmbeddingsFileURI", "re.fullmatch.groups"], "function", ["None"], ["", "def", "parse_embeddings_file_uri", "(", "uri", ":", "str", ")", "->", "\"EmbeddingsFileURI\"", ":", "\n", "    ", "match", "=", "re", ".", "fullmatch", "(", "r\"\\((.*)\\)#(.*)\"", ",", "uri", ")", "\n", "if", "match", ":", "\n", "        ", "fields", "=", "cast", "(", "Tuple", "[", "str", ",", "str", "]", ",", "match", ".", "groups", "(", ")", ")", "\n", "return", "EmbeddingsFileURI", "(", "*", "fields", ")", "\n", "", "else", ":", "\n", "        ", "return", "EmbeddingsFileURI", "(", "uri", ",", "None", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.token_embedders.bag_of_word_counts_token_embedder.BagOfWordCountsTokenEmbedder.__init__": [[32, 58], ["allennlp.modules.token_embedders.token_embedder.TokenEmbedder.__init__", "vocab.get_vocab_size", "vocab.get_token_to_index_vocabulary().get", "torch.nn.Linear", "allennlp.common.checks.ConfigurationError", "vocab.get_token_to_index_vocabulary"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.graph_util.plot.GraphPlot.__init__", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.data.vocabulary.Vocabulary.get_vocab_size", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.params.Params.get", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.data.vocabulary.Vocabulary.get_token_to_index_vocabulary"], ["def", "__init__", "(", "\n", "self", ",", "\n", "vocab", ":", "Vocabulary", ",", "\n", "vocab_namespace", ":", "str", "=", "\"tokens\"", ",", "\n", "projection_dim", ":", "int", "=", "None", ",", "\n", "ignore_oov", ":", "bool", "=", "False", ",", "\n", ")", "->", "None", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "vocab", "=", "vocab", "\n", "self", ".", "vocab_size", "=", "vocab", ".", "get_vocab_size", "(", "vocab_namespace", ")", "\n", "if", "projection_dim", ":", "\n", "            ", "self", ".", "_projection", "=", "torch", ".", "nn", ".", "Linear", "(", "self", ".", "vocab_size", ",", "projection_dim", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "_projection", "=", "None", "\n", "", "self", ".", "_ignore_oov", "=", "ignore_oov", "\n", "oov_token", "=", "vocab", ".", "_oov_token", "\n", "self", ".", "_oov_idx", "=", "vocab", ".", "get_token_to_index_vocabulary", "(", "vocab_namespace", ")", ".", "get", "(", "\n", "oov_token", "\n", ")", "\n", "if", "self", ".", "_oov_idx", "is", "None", ":", "\n", "            ", "raise", "ConfigurationError", "(", "\n", "\"OOV token does not exist in vocabulary namespace {}\"", ".", "format", "(", "\n", "vocab_namespace", "\n", ")", "\n", ")", "\n", "", "self", ".", "output_dim", "=", "projection_dim", "or", "self", ".", "vocab_size", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.token_embedders.bag_of_word_counts_token_embedder.BagOfWordCountsTokenEmbedder.get_output_dim": [[59, 61], ["None"], "methods", ["None"], ["", "def", "get_output_dim", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "output_dim", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.token_embedders.bag_of_word_counts_token_embedder.BagOfWordCountsTokenEmbedder.forward": [[62, 92], ["allennlp.nn.util.get_text_field_mask", "zip", "torch.cat", "torch.masked_select", "torch.bincount().float", "vec.view.view.view", "bag_of_words_vectors.append", "projection", "doc_mask.to", "torch.bincount"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.nn.util.get_text_field_mask"], ["", "def", "forward", "(", "self", ",", "inputs", ":", "torch", ".", "Tensor", ")", "->", "torch", ".", "Tensor", ":", "\n", "        ", "\"\"\"\n        # Parameters\n\n        inputs : `torch.Tensor`\n            Shape `(batch_size, timesteps, sequence_length)` of word ids\n            representing the current batch.\n\n        # Returns\n\n        The bag-of-words representations for the input sequence, shape\n        `(batch_size, vocab_size)`\n        \"\"\"", "\n", "bag_of_words_vectors", "=", "[", "]", "\n", "\n", "mask", "=", "get_text_field_mask", "(", "{", "\"tokens\"", ":", "{", "\"tokens\"", ":", "inputs", "}", "}", ")", "\n", "if", "self", ".", "_ignore_oov", ":", "\n", "# also mask out positions corresponding to oov", "\n", "            ", "mask", "*=", "(", "inputs", "!=", "self", ".", "_oov_idx", ")", ".", "long", "(", ")", "\n", "", "for", "document", ",", "doc_mask", "in", "zip", "(", "inputs", ",", "mask", ")", ":", "\n", "            ", "document", "=", "torch", ".", "masked_select", "(", "document", ",", "doc_mask", ".", "to", "(", "dtype", "=", "torch", ".", "bool", ")", ")", "\n", "vec", "=", "torch", ".", "bincount", "(", "document", ",", "minlength", "=", "self", ".", "vocab_size", ")", ".", "float", "(", ")", "\n", "vec", "=", "vec", ".", "view", "(", "1", ",", "-", "1", ")", "\n", "bag_of_words_vectors", ".", "append", "(", "vec", ")", "\n", "", "bag_of_words_output", "=", "torch", ".", "cat", "(", "bag_of_words_vectors", ",", "0", ")", "\n", "\n", "if", "self", ".", "_projection", ":", "\n", "            ", "projection", "=", "self", ".", "_projection", "\n", "bag_of_words_output", "=", "projection", "(", "bag_of_words_output", ")", "\n", "", "return", "bag_of_words_output", "\n", "", "", ""]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.token_embedders.empty_embedder.EmptyEmbedder.__init__": [[17, 19], ["allennlp.modules.token_embedders.token_embedder.TokenEmbedder.__init__"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.graph_util.plot.GraphPlot.__init__"], ["def", "__init__", "(", "self", ")", "->", "None", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.token_embedders.empty_embedder.EmptyEmbedder.get_output_dim": [[20, 22], ["None"], "methods", ["None"], ["", "def", "get_output_dim", "(", "self", ")", ":", "\n", "        ", "return", "0", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.token_embedders.empty_embedder.EmptyEmbedder.forward": [[23, 25], ["None"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "*", "inputs", ",", "**", "kwargs", ")", "->", "torch", ".", "Tensor", ":", "\n", "        ", "return", "None", "\n", "", "", ""]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.token_embedders.elmo_token_embedder.ElmoTokenEmbedder.__init__": [[50, 81], ["allennlp.modules.token_embedders.token_embedder.TokenEmbedder.__init__", "allennlp.modules.elmo.Elmo", "torch.nn.Linear", "elmo_token_embedder.ElmoTokenEmbedder._elmo.get_output_dim", "elmo_token_embedder.ElmoTokenEmbedder._elmo.get_output_dim"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.graph_util.plot.GraphPlot.__init__", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.seq2seq_decoders.auto_regressive_seq_decoder.AutoRegressiveSeqDecoder.get_output_dim", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.seq2seq_decoders.auto_regressive_seq_decoder.AutoRegressiveSeqDecoder.get_output_dim"], ["def", "__init__", "(", "\n", "self", ",", "\n", "options_file", ":", "str", ",", "\n", "weight_file", ":", "str", ",", "\n", "do_layer_norm", ":", "bool", "=", "False", ",", "\n", "dropout", ":", "float", "=", "0.5", ",", "\n", "requires_grad", ":", "bool", "=", "False", ",", "\n", "projection_dim", ":", "int", "=", "None", ",", "\n", "vocab_to_cache", ":", "List", "[", "str", "]", "=", "None", ",", "\n", "scalar_mix_parameters", ":", "List", "[", "float", "]", "=", "None", ",", "\n", ")", "->", "None", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "\n", "self", ".", "_elmo", "=", "Elmo", "(", "\n", "options_file", ",", "\n", "weight_file", ",", "\n", "1", ",", "\n", "do_layer_norm", "=", "do_layer_norm", ",", "\n", "dropout", "=", "dropout", ",", "\n", "requires_grad", "=", "requires_grad", ",", "\n", "vocab_to_cache", "=", "vocab_to_cache", ",", "\n", "scalar_mix_parameters", "=", "scalar_mix_parameters", ",", "\n", ")", "\n", "if", "projection_dim", ":", "\n", "            ", "self", ".", "_projection", "=", "torch", ".", "nn", ".", "Linear", "(", "\n", "self", ".", "_elmo", ".", "get_output_dim", "(", ")", ",", "projection_dim", "\n", ")", "\n", "self", ".", "output_dim", "=", "projection_dim", "\n", "", "else", ":", "\n", "            ", "self", ".", "_projection", "=", "None", "\n", "self", ".", "output_dim", "=", "self", ".", "_elmo", ".", "get_output_dim", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.token_embedders.elmo_token_embedder.ElmoTokenEmbedder.get_output_dim": [[82, 84], ["None"], "methods", ["None"], ["", "", "def", "get_output_dim", "(", "self", ")", "->", "int", ":", "\n", "        ", "return", "self", ".", "output_dim", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.token_embedders.elmo_token_embedder.ElmoTokenEmbedder.forward": [[85, 110], ["elmo_token_embedder.ElmoTokenEmbedder._elmo", "range", "allennlp.modules.time_distributed.TimeDistributed.", "allennlp.modules.time_distributed.TimeDistributed", "allennlp.modules.time_distributed.TimeDistributed.dim"], "methods", ["None"], ["", "def", "forward", "(", "\n", "self", ",", "tokens", ":", "torch", ".", "Tensor", ",", "word_inputs", ":", "torch", ".", "Tensor", "=", "None", "\n", ")", "->", "torch", ".", "Tensor", ":", "\n", "        ", "\"\"\"\n        # Parameters\n\n        tokens : `torch.Tensor`\n            Shape `(batch_size, timesteps, 50)` of character ids representing the current batch.\n        word_inputs : `torch.Tensor`, optional.\n            If you passed a cached vocab, you can in addition pass a tensor of shape\n            `(batch_size, timesteps)`, which represent word ids which have been pre-cached.\n\n        # Returns\n\n        The ELMo representations for the input sequence, shape\n        `(batch_size, timesteps, embedding_dim)`\n        \"\"\"", "\n", "elmo_output", "=", "self", ".", "_elmo", "(", "tokens", ",", "word_inputs", ")", "\n", "elmo_representations", "=", "elmo_output", "[", "\"elmo_representations\"", "]", "[", "0", "]", "\n", "if", "self", ".", "_projection", ":", "\n", "            ", "projection", "=", "self", ".", "_projection", "\n", "for", "_", "in", "range", "(", "elmo_representations", ".", "dim", "(", ")", "-", "2", ")", ":", "\n", "                ", "projection", "=", "TimeDistributed", "(", "projection", ")", "\n", "", "elmo_representations", "=", "projection", "(", "elmo_representations", ")", "\n", "", "return", "elmo_representations", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.token_embedders.elmo_token_embedder.ElmoTokenEmbedder.from_params": [[112, 141], ["params.pop", "params.pop", "params.pop", "params.pop_bool", "params.pop_float", "params.pop", "params.pop_int", "params.pop", "params.assert_empty", "cls", "list", "vocab.get_token_to_index_vocabulary().keys", "vocab.get_token_to_index_vocabulary"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.params.Params.pop", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.params.Params.pop", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.params.Params.pop", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.params.Params.pop_bool", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.params.Params.pop_float", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.params.Params.pop", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.params.Params.pop_int", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.params.Params.pop", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.params.Params.assert_empty", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.data.vocabulary.Vocabulary.get_token_to_index_vocabulary"], ["", "@", "classmethod", "\n", "def", "from_params", "(", "# type: ignore", "\n", "cls", ",", "vocab", ":", "Vocabulary", ",", "params", ":", "Params", ",", "**", "extras", "\n", ")", "->", "\"ElmoTokenEmbedder\"", ":", "\n", "\n", "        ", "options_file", "=", "params", ".", "pop", "(", "\"options_file\"", ")", "\n", "weight_file", "=", "params", ".", "pop", "(", "\"weight_file\"", ")", "\n", "requires_grad", "=", "params", ".", "pop", "(", "\"requires_grad\"", ",", "False", ")", "\n", "do_layer_norm", "=", "params", ".", "pop_bool", "(", "\"do_layer_norm\"", ",", "False", ")", "\n", "dropout", "=", "params", ".", "pop_float", "(", "\"dropout\"", ",", "0.5", ")", "\n", "namespace_to_cache", "=", "params", ".", "pop", "(", "\"namespace_to_cache\"", ",", "None", ")", "\n", "if", "namespace_to_cache", "is", "not", "None", ":", "\n", "            ", "vocab_to_cache", "=", "list", "(", "\n", "vocab", ".", "get_token_to_index_vocabulary", "(", "namespace_to_cache", ")", ".", "keys", "(", ")", "\n", ")", "\n", "", "else", ":", "\n", "            ", "vocab_to_cache", "=", "None", "\n", "", "projection_dim", "=", "params", ".", "pop_int", "(", "\"projection_dim\"", ",", "None", ")", "\n", "scalar_mix_parameters", "=", "params", ".", "pop", "(", "\"scalar_mix_parameters\"", ",", "None", ")", "\n", "params", ".", "assert_empty", "(", "cls", ".", "__name__", ")", "\n", "return", "cls", "(", "\n", "options_file", "=", "options_file", ",", "\n", "weight_file", "=", "weight_file", ",", "\n", "do_layer_norm", "=", "do_layer_norm", ",", "\n", "dropout", "=", "dropout", ",", "\n", "requires_grad", "=", "requires_grad", ",", "\n", "projection_dim", "=", "projection_dim", ",", "\n", "vocab_to_cache", "=", "vocab_to_cache", ",", "\n", "scalar_mix_parameters", "=", "scalar_mix_parameters", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.token_embedders.pretrained_transformer_embedder.PretrainedTransformerEmbedder.__init__": [[32, 53], ["allennlp.modules.token_embedders.token_embedder.TokenEmbedder.__init__", "allennlp.data.tokenizers.PretrainedTransformerTokenizer", "isinstance", "transformers.modeling_auto.AutoModel.from_pretrained", "isinstance"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.graph_util.plot.GraphPlot.__init__"], ["def", "__init__", "(", "\n", "self", ",", "model_name", ":", "Union", "[", "str", ",", "AutoModel", "]", ",", "max_length", ":", "int", "=", "None", "\n", ")", "->", "None", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "transformer_model", "=", "(", "\n", "AutoModel", ".", "from_pretrained", "(", "model_name", ")", "\n", "if", "isinstance", "(", "model_name", ",", "str", ")", "\n", "else", "model_name", "\n", ")", "\n", "self", ".", "_max_length", "=", "max_length", "\n", "# I'm not sure if this works for all models; open an issue on github if you find a case", "\n", "# where it doesn't work.", "\n", "self", ".", "output_dim", "=", "self", ".", "transformer_model", ".", "config", ".", "hidden_size", "\n", "\n", "tokenizer", "=", "PretrainedTransformerTokenizer", "(", "\n", "model_name", ".", "_name", "if", "not", "isinstance", "(", "model_name", ",", "str", ")", "else", "model_name", "\n", ")", "\n", "self", ".", "_num_added_start_tokens", "=", "tokenizer", ".", "num_added_start_tokens", "\n", "self", ".", "_num_added_end_tokens", "=", "tokenizer", ".", "num_added_end_tokens", "\n", "self", ".", "_num_added_tokens", "=", "(", "\n", "self", ".", "_num_added_start_tokens", "+", "self", ".", "_num_added_end_tokens", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.token_embedders.pretrained_transformer_embedder.PretrainedTransformerEmbedder.get_output_dim": [[55, 58], ["None"], "methods", ["None"], ["", "@", "overrides", "\n", "def", "get_output_dim", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "output_dim", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.token_embedders.pretrained_transformer_embedder.PretrainedTransformerEmbedder._number_of_token_type_embeddings": [[59, 65], ["hasattr"], "methods", ["None"], ["", "def", "_number_of_token_type_embeddings", "(", "self", ")", ":", "\n", "        ", "config", "=", "self", ".", "transformer_model", ".", "config", "\n", "if", "hasattr", "(", "config", ",", "\"type_vocab_size\"", ")", ":", "\n", "            ", "return", "config", ".", "type_vocab_size", "\n", "", "else", ":", "\n", "            ", "return", "0", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.token_embedders.pretrained_transformer_embedder.PretrainedTransformerEmbedder.forward": [[66, 138], ["type_ids.max", "token_ids.size", "pretrained_transformer_embedder.PretrainedTransformerEmbedder._fold_long_sequences", "pretrained_transformer_embedder.PretrainedTransformerEmbedder.transformer_model", "pretrained_transformer_embedder.PretrainedTransformerEmbedder._unfold_long_sequences", "pretrained_transformer_embedder.PretrainedTransformerEmbedder._number_of_token_type_embeddings", "ValueError"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.token_embedders.pretrained_transformer_embedder.PretrainedTransformerEmbedder._fold_long_sequences", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.token_embedders.pretrained_transformer_embedder.PretrainedTransformerEmbedder._unfold_long_sequences", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.token_embedders.pretrained_transformer_embedder.PretrainedTransformerEmbedder._number_of_token_type_embeddings"], ["", "", "@", "overrides", "\n", "def", "forward", "(", "\n", "self", ",", "\n", "token_ids", ":", "torch", ".", "LongTensor", ",", "\n", "mask", ":", "torch", ".", "LongTensor", ",", "\n", "type_ids", ":", "Optional", "[", "torch", ".", "LongTensor", "]", "=", "None", ",", "\n", "segment_concat_mask", ":", "Optional", "[", "torch", ".", "LongTensor", "]", "=", "None", ",", "\n", ")", "->", "torch", ".", "Tensor", ":", "# type: ignore", "\n", "        ", "\"\"\"\n        # Parameters\n\n        token_ids: torch.LongTensor\n            Shape: [\n                batch_size, num_wordpieces if max_length is None else num_segment_concat_wordpieces\n            ].\n            num_segment_concat_wordpieces is num_wordpieces plus special tokens inserted in the\n            middle, e.g. the length of: \"[CLS] A B C [SEP] [CLS] D E F [SEP]\" (see indexer logic).\n        mask: torch.LongTensor\n            Shape: [batch_size, num_wordpieces].\n        type_ids: Optional[torch.LongTensor]\n            Shape: [\n                batch_size, num_wordpieces if max_length is None else num_segment_concat_wordpieces\n            ].\n        segment_concat_mask: Optional[torch.LongTensor]\n            Shape: [batch_size, num_segment_concat_wordpieces].\n\n        # Returns:\n\n        Shape: [batch_size, num_wordpieces, embedding_size].\n        \"\"\"", "\n", "\n", "# Some of the huggingface transformers don't support type ids at all and crash when you supply them. For", "\n", "# others, you can supply a tensor of zeros, and if you don't, they act as if you did. There is no practical", "\n", "# difference to the caller, so here we pretend that one case is the same as another case.", "\n", "if", "type_ids", "is", "not", "None", ":", "\n", "            ", "max_type_id", "=", "type_ids", ".", "max", "(", ")", "\n", "if", "max_type_id", "==", "0", ":", "\n", "                ", "type_ids", "=", "None", "\n", "", "else", ":", "\n", "                ", "if", "max_type_id", ">=", "self", ".", "_number_of_token_type_embeddings", "(", ")", ":", "\n", "                    ", "raise", "ValueError", "(", "\n", "\"Found type ids too large for the chosen transformer model.\"", "\n", ")", "\n", "", "assert", "token_ids", ".", "shape", "==", "type_ids", ".", "shape", "\n", "\n", "", "", "if", "self", ".", "_max_length", "is", "not", "None", ":", "\n", "            ", "batch_size", ",", "num_segment_concat_wordpieces", "=", "token_ids", ".", "size", "(", ")", "\n", "token_ids", ",", "segment_concat_mask", ",", "type_ids", "=", "self", ".", "_fold_long_sequences", "(", "\n", "token_ids", ",", "segment_concat_mask", ",", "type_ids", "\n", ")", "\n", "\n", "", "transformer_mask", "=", "segment_concat_mask", "if", "self", ".", "_max_length", "is", "not", "None", "else", "mask", "\n", "# Shape: [batch_size, num_wordpieces, embedding_size],", "\n", "# or if self._max_length is not None:", "\n", "# [batch_size * num_segments, self._max_length, embedding_size]", "\n", "\n", "# We call this with kwargs because some of the huggingface models don't have the token_type_ids parameter", "\n", "# and fail even when it's given as None.", "\n", "parameters", "=", "{", "\"input_ids\"", ":", "token_ids", ",", "\"attention_mask\"", ":", "transformer_mask", "}", "\n", "if", "type_ids", "is", "not", "None", ":", "\n", "            ", "parameters", "[", "\"token_type_ids\"", "]", "=", "type_ids", "\n", "", "embeddings", "=", "self", ".", "transformer_model", "(", "**", "parameters", ")", "[", "0", "]", "\n", "\n", "if", "self", ".", "_max_length", "is", "not", "None", ":", "\n", "            ", "embeddings", "=", "self", ".", "_unfold_long_sequences", "(", "\n", "embeddings", ",", "\n", "segment_concat_mask", ",", "\n", "batch_size", ",", "\n", "num_segment_concat_wordpieces", ",", "\n", ")", "\n", "\n", "", "return", "embeddings", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.token_embedders.pretrained_transformer_embedder.PretrainedTransformerEmbedder._fold_long_sequences": [[139, 188], ["token_ids.size", "math.ceil", "torch.pad", "torch.pad", "torch.pad.reshape", "pretrained_transformer_embedder.PretrainedTransformerEmbedder._fold_long_sequences.fold"], "methods", ["None"], ["", "def", "_fold_long_sequences", "(", "\n", "self", ",", "\n", "token_ids", ":", "torch", ".", "LongTensor", ",", "\n", "mask", ":", "torch", ".", "LongTensor", ",", "\n", "type_ids", ":", "Optional", "[", "torch", ".", "LongTensor", "]", "=", "None", ",", "\n", ")", "->", "Tuple", "[", "torch", ".", "LongTensor", ",", "torch", ".", "LongTensor", ",", "Optional", "[", "torch", ".", "LongTensor", "]", "]", ":", "\n", "        ", "\"\"\"\n        We fold 1D sequences (for each element in batch), returned by `PretrainedTransformerIndexer`\n        that are in reality multiple segments concatenated together, to 2D tensors, e.g.\n\n        [ [CLS] A B C [SEP] [CLS] D E [SEP] ]\n        -> [ [ [CLS] A B C [SEP] ], [ [CLS] D E [SEP] [PAD] ] ]\n        The [PAD] positions can be found in the returned `mask`.\n\n        # Parameters\n\n        token_ids: `torch.LongTensor`\n            Shape: [batch_size, num_segment_concat_wordpieces].\n            num_segment_concat_wordpieces is num_wordpieces plus special tokens inserted in the\n            middle, i.e. the length of: \"[CLS] A B C [SEP] [CLS] D E F [SEP]\" (see indexer logic).\n        mask: `torch.LongTensor`\n            Shape: [batch_size, num_segment_concat_wordpieces].\n            The mask for the concatenated segments of wordpieces. The same as `segment_concat_mask`\n            in `forward()`.\n        type_ids: Optional[torch.LongTensor]\n            Shape: [batch_size, num_segment_concat_wordpieces].\n\n        # Returns:\n\n        token_ids: `torch.LongTensor`\n            Shape: [batch_size * num_segments, self._max_length].\n        mask: `torch.LongTensor`\n            Shape: [batch_size * num_segments, self._max_length].\n        \"\"\"", "\n", "num_segment_concat_wordpieces", "=", "token_ids", ".", "size", "(", "1", ")", "\n", "num_segments", "=", "math", ".", "ceil", "(", "num_segment_concat_wordpieces", "/", "self", ".", "_max_length", ")", "\n", "padded_length", "=", "num_segments", "*", "self", ".", "_max_length", "\n", "length_to_pad", "=", "padded_length", "-", "num_segment_concat_wordpieces", "\n", "\n", "def", "fold", "(", "tensor", ")", ":", "# Shape: [batch_size, num_segment_concat_wordpieces]", "\n", "# Shape: [batch_size, num_segments * self._max_length]", "\n", "            ", "tensor", "=", "F", ".", "pad", "(", "tensor", ",", "[", "0", ",", "length_to_pad", "]", ",", "value", "=", "0", ")", "\n", "# Shape: [batch_size * num_segments, self._max_length]", "\n", "return", "tensor", ".", "reshape", "(", "-", "1", ",", "self", ".", "_max_length", ")", "\n", "\n", "", "return", "(", "\n", "fold", "(", "token_ids", ")", ",", "\n", "fold", "(", "mask", ")", ",", "\n", "fold", "(", "type_ids", ")", "if", "type_ids", "is", "not", "None", "else", "None", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.token_embedders.pretrained_transformer_embedder.PretrainedTransformerEmbedder._unfold_long_sequences": [[190, 302], ["int", "torch.cat.size", "torch.cat.size", "torch.cat.reshape", "torch.cat.reshape", "mask.reshape.reshape.reshape", "mask.reshape.reshape.sum", "allennlp.nn.util.batched_index_select", "torch.cat.reshape", "torch.cat.reshape", "torch.cat.reshape", "torch.cat.reshape", "num_removed_non_end_tokens.unsqueeze", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat.scatter_", "torch.cat.scatter_", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "ValueError", "end_token_indices.unsqueeze().expand_as", "torch.arange().expand", "torch.arange().expand", "torch.arange().expand", "torch.arange().expand", "lengths.unsqueeze", "torch.cat.size", "torch.cat.size", "mask.reshape.sum.unsqueeze", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "lengths.size", "end_token_indices.unsqueeze", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "pretrained_transformer_embedder.PretrainedTransformerEmbedder._unfold_long_sequences.lengths_to_mask"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.nn.util.batched_index_select"], ["", "def", "_unfold_long_sequences", "(", "\n", "self", ",", "\n", "embeddings", ":", "torch", ".", "FloatTensor", ",", "\n", "mask", ":", "torch", ".", "LongTensor", ",", "\n", "batch_size", ":", "int", ",", "\n", "num_segment_concat_wordpieces", ":", "int", ",", "\n", ")", "->", "torch", ".", "FloatTensor", ":", "\n", "        ", "\"\"\"\n        We take 2D segments of a long sequence and flatten them out to get the whole sequence\n        representation while remove unnecessary special tokens.\n\n        [ [ [CLS]_emb A_emb B_emb C_emb [SEP]_emb ], [ [CLS]_emb D_emb E_emb [SEP]_emb [PAD]_emb ] ]\n        -> [ [CLS]_emb A_emb B_emb C_emb D_emb E_emb [SEP]_emb ]\n\n        We truncate the start and end tokens for all segments, recombine the segments,\n        and manually add back the start and end tokens.\n\n        # Parameters\n\n        embeddings: `torch.FloatTensor`\n            Shape: [batch_size * num_segments, self._max_length, embedding_size].\n        mask: `torch.LongTensor`\n            Shape: [batch_size * num_segments, self._max_length].\n            The mask for the concatenated segments of wordpieces. The same as `segment_concat_mask`\n            in `forward()`.\n        batch_size: `int`\n        num_segment_concat_wordpieces: `int`\n            The length of the original \"[ [CLS] A B C [SEP] [CLS] D E F [SEP] ]\", i.e.\n            the original `token_ids.size(1)`.\n\n        # Returns:\n\n        embeddings: `torch.FloatTensor`\n            Shape: [batch_size, self._num_wordpieces, embedding_size].\n        \"\"\"", "\n", "\n", "def", "lengths_to_mask", "(", "lengths", ",", "max_len", ",", "device", ")", ":", "\n", "            ", "return", "torch", ".", "arange", "(", "max_len", ",", "device", "=", "device", ")", ".", "expand", "(", "\n", "lengths", ".", "size", "(", "0", ")", ",", "max_len", "\n", ")", "<", "lengths", ".", "unsqueeze", "(", "1", ")", "\n", "\n", "", "device", "=", "embeddings", ".", "device", "\n", "num_segments", "=", "int", "(", "embeddings", ".", "size", "(", "0", ")", "/", "batch_size", ")", "\n", "embedding_size", "=", "embeddings", ".", "size", "(", "2", ")", "\n", "\n", "# We want to remove all segment-level special tokens but maintain sequence-level ones", "\n", "num_wordpieces", "=", "(", "\n", "num_segment_concat_wordpieces", "-", "(", "num_segments", "-", "1", ")", "*", "self", ".", "_num_added_tokens", "\n", ")", "\n", "\n", "embeddings", "=", "embeddings", ".", "reshape", "(", "\n", "batch_size", ",", "num_segments", "*", "self", ".", "_max_length", ",", "embedding_size", "\n", ")", "\n", "mask", "=", "mask", ".", "reshape", "(", "batch_size", ",", "num_segments", "*", "self", ".", "_max_length", ")", "\n", "# We assume that all 1s in the mask preceed all 0s, and add an assert for that.", "\n", "# Open an issue on GitHub if this breaks for you.", "\n", "# Shape: (batch_size,)", "\n", "seq_lengths", "=", "mask", ".", "sum", "(", "-", "1", ")", "\n", "if", "not", "(", "\n", "lengths_to_mask", "(", "seq_lengths", ",", "mask", ".", "size", "(", "1", ")", ",", "device", ")", "==", "mask", ".", "bool", "(", ")", "\n", ")", ".", "all", "(", ")", ":", "\n", "            ", "raise", "ValueError", "(", "\n", "\"Long sequence splitting only supports masks with all 1s preceding all 0s.\"", "\n", ")", "\n", "# Shape: (batch_size, self._num_added_end_tokens); this is a broadcast op", "\n", "", "end_token_indices", "=", "(", "\n", "seq_lengths", ".", "unsqueeze", "(", "-", "1", ")", "\n", "-", "torch", ".", "arange", "(", "self", ".", "_num_added_end_tokens", ",", "device", "=", "device", ")", "\n", "-", "1", "\n", ")", "\n", "\n", "# Shape: (batch_size, self._num_added_start_tokens, embedding_size)", "\n", "start_token_embeddings", "=", "embeddings", "[", ":", ",", ":", "self", ".", "_num_added_start_tokens", ",", ":", "]", "\n", "# Shape: (batch_size, self._num_added_end_tokens, embedding_size)", "\n", "end_token_embeddings", "=", "batched_index_select", "(", "embeddings", ",", "end_token_indices", ")", "\n", "\n", "embeddings", "=", "embeddings", ".", "reshape", "(", "\n", "batch_size", ",", "num_segments", ",", "self", ".", "_max_length", ",", "embedding_size", "\n", ")", "\n", "embeddings", "=", "embeddings", "[", "\n", ":", ",", ":", ",", "self", ".", "_num_added_start_tokens", ":", "-", "self", ".", "_num_added_end_tokens", ",", ":", "\n", "]", "# truncate segment-level start/end tokens", "\n", "embeddings", "=", "embeddings", ".", "reshape", "(", "batch_size", ",", "-", "1", ",", "embedding_size", ")", "# flatten", "\n", "\n", "# Now try to put end token embeddings back which is a little tricky.", "\n", "\n", "# The number of segment each sequence spans, excluding padding. Mimicking ceiling operation.", "\n", "# Shape: (batch_size,)", "\n", "num_effective_segments", "=", "(", "seq_lengths", "+", "self", ".", "_max_length", "-", "1", ")", "/", "self", ".", "_max_length", "\n", "# The number of indices that end tokens should shift back.", "\n", "num_removed_non_end_tokens", "=", "(", "\n", "num_effective_segments", "*", "self", ".", "_num_added_tokens", "-", "self", ".", "_num_added_end_tokens", "\n", ")", "\n", "# Shape: (batch_size, self._num_added_end_tokens)", "\n", "end_token_indices", "-=", "num_removed_non_end_tokens", ".", "unsqueeze", "(", "-", "1", ")", "\n", "assert", "(", "end_token_indices", ">=", "self", ".", "_num_added_start_tokens", ")", ".", "all", "(", ")", "\n", "# Add space for end embeddings", "\n", "embeddings", "=", "torch", ".", "cat", "(", "[", "embeddings", ",", "torch", ".", "zeros_like", "(", "end_token_embeddings", ")", "]", ",", "1", ")", "\n", "# Add end token embeddings back", "\n", "embeddings", ".", "scatter_", "(", "\n", "1", ",", "\n", "end_token_indices", ".", "unsqueeze", "(", "-", "1", ")", ".", "expand_as", "(", "end_token_embeddings", ")", ",", "\n", "end_token_embeddings", ",", "\n", ")", "\n", "\n", "# Now put back start tokens. We can do this before putting back end tokens, but then", "\n", "# we need to change `num_removed_non_end_tokens` a little.", "\n", "embeddings", "=", "torch", ".", "cat", "(", "[", "start_token_embeddings", ",", "embeddings", "]", ",", "1", ")", "\n", "\n", "# Truncate to original length", "\n", "embeddings", "=", "embeddings", "[", ":", ",", ":", "num_wordpieces", ",", ":", "]", "\n", "return", "embeddings", "\n", "", "", ""]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.seq2seq_decoders.stacked_self_attention_decoder_net.StackedSelfAttentionDecoderNet.__init__": [[55, 97], ["allennlp.modules.seq2seq_decoders.decoder_net.DecoderNet.__init__", "allennlp.modules.seq2seq_encoders.bidirectional_language_model_transformer.MultiHeadedAttention", "allennlp.modules.seq2seq_encoders.bidirectional_language_model_transformer.PositionwiseFeedForward", "math.sqrt", "torch.nn.Dropout", "torch.nn.Dropout", "stacked_self_attention_decoder_net.StackedSelfAttentionDecoderNet.Decoder", "allennlp.modules.seq2seq_encoders.bidirectional_language_model_transformer.PositionalEncoding", "stacked_self_attention_decoder_net.StackedSelfAttentionDecoderNet.DecoderLayer", "copy.deepcopy", "copy.deepcopy"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.graph_util.plot.GraphPlot.__init__"], ["def", "__init__", "(", "\n", "self", ",", "\n", "decoding_dim", ":", "int", ",", "\n", "target_embedding_dim", ":", "int", ",", "\n", "feedforward_hidden_dim", ":", "int", ",", "\n", "num_layers", ":", "int", ",", "\n", "num_attention_heads", ":", "int", ",", "\n", "use_positional_encoding", ":", "bool", "=", "True", ",", "\n", "positional_encoding_max_steps", ":", "int", "=", "5000", ",", "\n", "dropout_prob", ":", "float", "=", "0.1", ",", "\n", "residual_dropout_prob", ":", "float", "=", "0.2", ",", "\n", "attention_dropout_prob", ":", "float", "=", "0.1", ",", "\n", ")", "->", "None", ":", "\n", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "\n", "decoding_dim", "=", "decoding_dim", ",", "\n", "target_embedding_dim", "=", "target_embedding_dim", ",", "\n", "decodes_parallel", "=", "True", ",", "\n", ")", "\n", "\n", "attn", "=", "MultiHeadedAttention", "(", "\n", "num_attention_heads", ",", "decoding_dim", ",", "attention_dropout_prob", "\n", ")", "\n", "feed_forward", "=", "PositionwiseFeedForward", "(", "\n", "decoding_dim", ",", "feedforward_hidden_dim", ",", "dropout_prob", "\n", ")", "\n", "self", ".", "_embed_scale", "=", "math", ".", "sqrt", "(", "decoding_dim", ")", "\n", "self", ".", "_positional_embedder", "=", "(", "\n", "PositionalEncoding", "(", "decoding_dim", ",", "positional_encoding_max_steps", ")", "\n", "if", "use_positional_encoding", "\n", "else", "None", "\n", ")", "\n", "self", ".", "_dropout", "=", "nn", ".", "Dropout", "(", "dropout_prob", ")", "\n", "self", ".", "_self_attention", "=", "Decoder", "(", "\n", "DecoderLayer", "(", "\n", "decoding_dim", ",", "\n", "deepcopy", "(", "attn", ")", ",", "\n", "deepcopy", "(", "attn", ")", ",", "\n", "feed_forward", ",", "\n", "residual_dropout_prob", ",", "\n", ")", ",", "\n", "num_layers", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.seq2seq_decoders.stacked_self_attention_decoder_net.StackedSelfAttentionDecoderNet.init_decoder_state": [[99, 104], ["None"], "methods", ["None"], ["", "@", "overrides", "\n", "def", "init_decoder_state", "(", "\n", "self", ",", "encoder_out", ":", "Dict", "[", "str", ",", "torch", ".", "LongTensor", "]", "\n", ")", "->", "Dict", "[", "str", ",", "torch", ".", "Tensor", "]", ":", "\n", "        ", "return", "{", "}", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.seq2seq_decoders.stacked_self_attention_decoder_net.StackedSelfAttentionDecoderNet.forward": [[105, 138], ["source_mask.unsqueeze.unsqueeze.unsqueeze", "torch.autograd.Variable", "torch.autograd.Variable", "stacked_self_attention_decoder_net.StackedSelfAttentionDecoderNet.StackedSelfAttentionDecoderNet._dropout", "stacked_self_attention_decoder_net.StackedSelfAttentionDecoderNet.StackedSelfAttentionDecoderNet._self_attention", "allennlp.modules.seq2seq_encoders.bidirectional_language_model_transformer.subsequent_mask().type_as", "stacked_self_attention_decoder_net.StackedSelfAttentionDecoderNet.StackedSelfAttentionDecoderNet._positional_embedder", "previous_steps_mask.unsqueeze", "allennlp.modules.seq2seq_encoders.bidirectional_language_model_transformer.subsequent_mask", "stacked_self_attention_decoder_net.StackedSelfAttentionDecoderNet.StackedSelfAttentionDecoderNet.size"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.seq2seq_encoders.bidirectional_language_model_transformer.subsequent_mask"], ["", "@", "overrides", "\n", "def", "forward", "(", "\n", "self", ",", "\n", "previous_state", ":", "Dict", "[", "str", ",", "torch", ".", "Tensor", "]", ",", "\n", "encoder_outputs", ":", "torch", ".", "Tensor", ",", "\n", "source_mask", ":", "torch", ".", "Tensor", ",", "\n", "previous_steps_predictions", ":", "torch", ".", "Tensor", ",", "\n", "previous_steps_mask", ":", "Optional", "[", "torch", ".", "Tensor", "]", "=", "None", ",", "\n", ")", "->", "Tuple", "[", "Dict", "[", "str", ",", "torch", ".", "Tensor", "]", ",", "torch", ".", "Tensor", "]", ":", "\n", "\n", "        ", "source_mask", "=", "source_mask", ".", "unsqueeze", "(", "-", "2", ")", "\n", "future_mask", "=", "Variable", "(", "\n", "subsequent_mask", "(", "\n", "previous_steps_predictions", ".", "size", "(", "-", "2", ")", ",", "device", "=", "source_mask", ".", "device", "\n", ")", ".", "type_as", "(", "source_mask", ".", "data", ")", "\n", ")", "\n", "if", "previous_steps_mask", "is", "None", ":", "\n", "            ", "previous_steps_mask", "=", "future_mask", "\n", "", "else", ":", "\n", "            ", "previous_steps_mask", "=", "previous_steps_mask", ".", "unsqueeze", "(", "-", "2", ")", "&", "future_mask", "\n", "", "previous_steps_predictions", "=", "previous_steps_predictions", "*", "self", ".", "_embed_scale", "\n", "if", "self", ".", "_positional_embedder", ":", "\n", "            ", "previous_steps_predictions", "=", "self", ".", "_positional_embedder", "(", "\n", "previous_steps_predictions", "\n", ")", "\n", "", "previous_steps_predictions", "=", "self", ".", "_dropout", "(", "previous_steps_predictions", ")", "\n", "decoded", "=", "self", ".", "_self_attention", "(", "\n", "previous_steps_predictions", ",", "\n", "encoder_outputs", ",", "\n", "source_mask", ",", "\n", "previous_steps_mask", ",", "\n", ")", "\n", "return", "{", "}", ",", "decoded", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.seq2seq_decoders.stacked_self_attention_decoder_net.Decoder.__init__": [[151, 155], ["torch.nn.Module.__init__", "stacked_self_attention_decoder_net._clones", "allennlp.modules.layer_norm.LayerNorm"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.graph_util.plot.GraphPlot.__init__", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.seq2seq_decoders.stacked_self_attention_decoder_net._clones"], ["def", "__init__", "(", "self", ",", "layer", ":", "nn", ".", "Module", ",", "num_layers", ":", "int", ")", "->", "None", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "layers", "=", "_clones", "(", "layer", ",", "num_layers", ")", "\n", "self", ".", "norm", "=", "LayerNorm", "(", "layer", ".", "size", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.seq2seq_decoders.stacked_self_attention_decoder_net.Decoder.forward": [[156, 168], ["stacked_self_attention_decoder_net.Decoder.Decoder.norm", "layer"], "methods", ["None"], ["", "@", "overrides", "\n", "def", "forward", "(", "\n", "self", ",", "\n", "x", ":", "torch", ".", "Tensor", ",", "\n", "memory", ":", "torch", ".", "Tensor", ",", "\n", "src_mask", ":", "torch", ".", "Tensor", ",", "\n", "tgt_mask", ":", "torch", ".", "Tensor", ",", "\n", ")", "->", "torch", ".", "Tensor", ":", "\n", "\n", "        ", "for", "layer", "in", "self", ".", "layers", ":", "\n", "            ", "x", "=", "layer", "(", "x", ",", "memory", ",", "src_mask", ",", "tgt_mask", ")", "\n", "", "return", "self", ".", "norm", "(", "x", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.seq2seq_decoders.stacked_self_attention_decoder_net.DecoderLayer.__init__": [[176, 190], ["torch.nn.Module.__init__", "stacked_self_attention_decoder_net._clones", "allennlp.modules.seq2seq_encoders.bidirectional_language_model_transformer.SublayerConnection"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.graph_util.plot.GraphPlot.__init__", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.seq2seq_decoders.stacked_self_attention_decoder_net._clones"], ["def", "__init__", "(", "\n", "self", ",", "\n", "size", ":", "int", ",", "\n", "self_attn", ":", "MultiHeadedAttention", ",", "\n", "src_attn", ":", "MultiHeadedAttention", ",", "\n", "feed_forward", ":", "F", ",", "\n", "dropout", ":", "float", ",", "\n", ")", "->", "None", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "size", "=", "size", "\n", "self", ".", "self_attn", "=", "self_attn", "\n", "self", ".", "src_attn", "=", "src_attn", "\n", "self", ".", "feed_forward", "=", "feed_forward", "\n", "self", ".", "sublayer", "=", "_clones", "(", "SublayerConnection", "(", "size", ",", "dropout", ")", ",", "3", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.seq2seq_decoders.stacked_self_attention_decoder_net.DecoderLayer.forward": [[191, 203], ["stacked_self_attention_decoder_net.DecoderLayer.DecoderLayer.self_attn", "stacked_self_attention_decoder_net.DecoderLayer.DecoderLayer.src_attn"], "methods", ["None"], ["", "def", "forward", "(", "\n", "self", ",", "\n", "x", ":", "torch", ".", "Tensor", ",", "\n", "memory", ":", "torch", ".", "Tensor", ",", "\n", "src_mask", ":", "torch", ".", "Tensor", ",", "\n", "tgt_mask", ":", "torch", ".", "Tensor", ",", "\n", ")", "->", "torch", ".", "Tensor", ":", "\n", "\n", "        ", "\"Follow Figure 1 (right) for connections.\"", "\n", "x", "=", "self", ".", "sublayer", "[", "0", "]", "(", "x", ",", "lambda", "x", ":", "self", ".", "self_attn", "(", "x", ",", "x", ",", "x", ",", "tgt_mask", ")", ")", "\n", "x", "=", "self", ".", "sublayer", "[", "1", "]", "(", "x", ",", "lambda", "x", ":", "self", ".", "src_attn", "(", "x", ",", "memory", ",", "memory", ",", "src_mask", ")", ")", "\n", "return", "self", ".", "sublayer", "[", "2", "]", "(", "x", ",", "self", ".", "feed_forward", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.seq2seq_decoders.stacked_self_attention_decoder_net._clones": [[140, 143], ["torch.nn.ModuleList", "copy.deepcopy", "range"], "function", ["None"], ["", "", "def", "_clones", "(", "module", ":", "nn", ".", "Module", ",", "num_layers", ":", "int", ")", ":", "\n", "    ", "\"Produce N identical layers.\"", "\n", "return", "nn", ".", "ModuleList", "(", "[", "copy", ".", "deepcopy", "(", "module", ")", "for", "_", "in", "range", "(", "num_layers", ")", "]", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.seq2seq_decoders.lstm_cell_decoder_net.LstmCellDecoderNet.__init__": [[30, 61], ["allennlp.modules.seq2seq_decoders.decoder_net.DecoderNet.__init__", "torch.nn.LSTMCell"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.graph_util.plot.GraphPlot.__init__"], ["def", "__init__", "(", "\n", "self", ",", "\n", "decoding_dim", ":", "int", ",", "\n", "target_embedding_dim", ":", "int", ",", "\n", "attention", ":", "Optional", "[", "Attention", "]", "=", "None", ",", "\n", "bidirectional_input", ":", "bool", "=", "False", ",", "\n", ")", "->", "None", ":", "\n", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "\n", "decoding_dim", "=", "decoding_dim", ",", "\n", "target_embedding_dim", "=", "target_embedding_dim", ",", "\n", "decodes_parallel", "=", "False", ",", "\n", ")", "\n", "\n", "# In this particular type of decoder output of previous step passes directly to the input of current step", "\n", "# We also assume that decoder output dimensionality is equal to the encoder output dimensionality", "\n", "decoder_input_dim", "=", "self", ".", "target_embedding_dim", "\n", "\n", "# Attention mechanism applied to the encoder output for each step.", "\n", "self", ".", "_attention", "=", "attention", "\n", "\n", "if", "self", ".", "_attention", ":", "\n", "# If using attention, a weighted average over encoder outputs will be concatenated", "\n", "# to the previous target embedding to form the input to the decoder at each", "\n", "# time step. encoder output dim will be same as decoding_dim", "\n", "            ", "decoder_input_dim", "+=", "decoding_dim", "\n", "\n", "# We'll use an LSTM cell as the recurrent cell that produces a hidden state", "\n", "# for the decoder at each time step.", "\n", "", "self", ".", "_decoder_cell", "=", "LSTMCell", "(", "decoder_input_dim", ",", "self", ".", "decoding_dim", ")", "\n", "self", ".", "_bidirectional_input", "=", "bidirectional_input", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.seq2seq_decoders.lstm_cell_decoder_net.LstmCellDecoderNet._prepare_attended_input": [[62, 83], ["encoder_outputs_mask.float.float.float", "lstm_cell_decoder_net.LstmCellDecoderNet._attention", "allennlp.nn.util.weighted_sum"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.nn.util.weighted_sum"], ["", "def", "_prepare_attended_input", "(", "\n", "self", ",", "\n", "decoder_hidden_state", ":", "torch", ".", "Tensor", "=", "None", ",", "\n", "encoder_outputs", ":", "torch", ".", "Tensor", "=", "None", ",", "\n", "encoder_outputs_mask", ":", "torch", ".", "Tensor", "=", "None", ",", "\n", ")", "->", "torch", ".", "Tensor", ":", "\n", "        ", "\"\"\"Apply attention over encoder outputs and decoder state.\"\"\"", "\n", "# Ensure mask is also a FloatTensor. Or else the multiplication within", "\n", "# attention will complain.", "\n", "# shape: (batch_size, max_input_sequence_length, encoder_output_dim)", "\n", "encoder_outputs_mask", "=", "encoder_outputs_mask", ".", "float", "(", ")", "\n", "\n", "# shape: (batch_size, max_input_sequence_length)", "\n", "input_weights", "=", "self", ".", "_attention", "(", "\n", "decoder_hidden_state", ",", "encoder_outputs", ",", "encoder_outputs_mask", "\n", ")", "\n", "\n", "# shape: (batch_size, encoder_output_dim)", "\n", "attended_input", "=", "util", ".", "weighted_sum", "(", "encoder_outputs", ",", "input_weights", ")", "\n", "\n", "return", "attended_input", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.seq2seq_decoders.lstm_cell_decoder_net.LstmCellDecoderNet.init_decoder_state": [[84, 103], ["encoder_out[].size", "allennlp.nn.util.get_final_encoder_states", "allennlp.nn.util.get_final_encoder_states.new_zeros"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.nn.util.get_final_encoder_states"], ["", "def", "init_decoder_state", "(", "\n", "self", ",", "encoder_out", ":", "Dict", "[", "str", ",", "torch", ".", "LongTensor", "]", "\n", ")", "->", "Dict", "[", "str", ",", "torch", ".", "Tensor", "]", ":", "\n", "\n", "        ", "batch_size", ",", "_", "=", "encoder_out", "[", "\"source_mask\"", "]", ".", "size", "(", ")", "\n", "\n", "# Initialize the decoder hidden state with the final output of the encoder,", "\n", "# and the decoder context with zeros.", "\n", "# shape: (batch_size, encoder_output_dim)", "\n", "final_encoder_output", "=", "util", ".", "get_final_encoder_states", "(", "\n", "encoder_out", "[", "\"encoder_outputs\"", "]", ",", "\n", "encoder_out", "[", "\"source_mask\"", "]", ",", "\n", "bidirectional", "=", "self", ".", "_bidirectional_input", ",", "\n", ")", "\n", "\n", "return", "{", "\n", "\"decoder_hidden\"", ":", "final_encoder_output", ",", "# shape: (batch_size, decoder_output_dim)", "\n", "\"decoder_context\"", ":", "final_encoder_output", ".", "new_zeros", "(", "\n", "batch_size", ",", "self", ".", "decoding_dim", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.seq2seq_decoders.lstm_cell_decoder_net.LstmCellDecoderNet.forward": [[107, 144], ["lstm_cell_decoder_net.LstmCellDecoderNet._decoder_cell", "lstm_cell_decoder_net.LstmCellDecoderNet._prepare_attended_input", "torch.cat"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.encoder_decoders.simple_seq2seq.SimpleSeq2Seq._prepare_attended_input"], ["", "@", "overrides", "\n", "def", "forward", "(", "\n", "self", ",", "\n", "previous_state", ":", "Dict", "[", "str", ",", "torch", ".", "Tensor", "]", ",", "\n", "encoder_outputs", ":", "torch", ".", "Tensor", ",", "\n", "source_mask", ":", "torch", ".", "Tensor", ",", "\n", "previous_steps_predictions", ":", "torch", ".", "Tensor", ",", "\n", "previous_steps_mask", ":", "Optional", "[", "torch", ".", "Tensor", "]", "=", "None", ",", "\n", ")", "->", "Tuple", "[", "Dict", "[", "str", ",", "torch", ".", "Tensor", "]", ",", "torch", ".", "Tensor", "]", ":", "\n", "\n", "        ", "decoder_hidden", "=", "previous_state", "[", "\"decoder_hidden\"", "]", "\n", "decoder_context", "=", "previous_state", "[", "\"decoder_context\"", "]", "\n", "\n", "# shape: (group_size, output_dim)", "\n", "last_predictions_embedding", "=", "previous_steps_predictions", "[", ":", ",", "-", "1", "]", "\n", "\n", "if", "self", ".", "_attention", ":", "\n", "# shape: (group_size, encoder_output_dim)", "\n", "            ", "attended_input", "=", "self", ".", "_prepare_attended_input", "(", "\n", "decoder_hidden", ",", "encoder_outputs", ",", "source_mask", "\n", ")", "\n", "\n", "# shape: (group_size, decoder_output_dim + target_embedding_dim)", "\n", "decoder_input", "=", "torch", ".", "cat", "(", "(", "attended_input", ",", "last_predictions_embedding", ")", ",", "-", "1", ")", "\n", "", "else", ":", "\n", "# shape: (group_size, target_embedding_dim)", "\n", "            ", "decoder_input", "=", "last_predictions_embedding", "\n", "\n", "# shape (decoder_hidden): (batch_size, decoder_output_dim)", "\n", "# shape (decoder_context): (batch_size, decoder_output_dim)", "\n", "", "decoder_hidden", ",", "decoder_context", "=", "self", ".", "_decoder_cell", "(", "\n", "decoder_input", ",", "(", "decoder_hidden", ",", "decoder_context", ")", "\n", ")", "\n", "\n", "return", "(", "\n", "{", "\"decoder_hidden\"", ":", "decoder_hidden", ",", "\"decoder_context\"", ":", "decoder_context", "}", ",", "\n", "decoder_hidden", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.seq2seq_decoders.decoder_net.DecoderNet.__init__": [[30, 37], ["super().__init__"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.graph_util.plot.GraphPlot.__init__"], ["def", "__init__", "(", "\n", "self", ",", "decoding_dim", ":", "int", ",", "target_embedding_dim", ":", "int", ",", "decodes_parallel", ":", "bool", "\n", ")", "->", "None", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "target_embedding_dim", "=", "target_embedding_dim", "\n", "self", ".", "decoding_dim", "=", "decoding_dim", "\n", "self", ".", "decodes_parallel", "=", "decodes_parallel", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.seq2seq_decoders.decoder_net.DecoderNet.get_output_dim": [[38, 44], ["None"], "methods", ["None"], ["", "def", "get_output_dim", "(", "self", ")", "->", "int", ":", "\n", "        ", "\"\"\"\n        Returns the dimension of each vector in the sequence output by this `DecoderNet`.\n        This is `not` the shape of the returned tensor, but the last element of that shape.\n        \"\"\"", "\n", "return", "self", ".", "decoding_dim", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.seq2seq_decoders.decoder_net.DecoderNet.init_decoder_state": [[45, 64], ["NotImplementedError"], "methods", ["None"], ["", "def", "init_decoder_state", "(", "\n", "self", ",", "encoder_out", ":", "Dict", "[", "str", ",", "torch", ".", "LongTensor", "]", "\n", ")", "->", "Dict", "[", "str", ",", "torch", ".", "Tensor", "]", ":", "\n", "        ", "\"\"\"\n        Initialize the encoded state to be passed to the first decoding time step.\n\n        # Parameters\n\n        batch_size : `int`\n            Size of batch\n        final_encoder_output : `torch.Tensor`\n            Last state of the Encoder\n\n        # Returns\n\n        `Dict[str, torch.Tensor]`\n        Initial state\n        \"\"\"", "\n", "raise", "NotImplementedError", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.seq2seq_decoders.decoder_net.DecoderNet.forward": [[65, 99], ["NotImplementedError"], "methods", ["None"], ["", "def", "forward", "(", "\n", "self", ",", "\n", "previous_state", ":", "Dict", "[", "str", ",", "torch", ".", "Tensor", "]", ",", "\n", "encoder_outputs", ":", "torch", ".", "Tensor", ",", "\n", "source_mask", ":", "torch", ".", "Tensor", ",", "\n", "previous_steps_predictions", ":", "torch", ".", "Tensor", ",", "\n", "previous_steps_mask", ":", "Optional", "[", "torch", ".", "Tensor", "]", "=", "None", ",", "\n", ")", "->", "Tuple", "[", "Dict", "[", "str", ",", "torch", ".", "Tensor", "]", ",", "torch", ".", "Tensor", "]", ":", "\n", "\n", "        ", "\"\"\"\n        Performs a decoding step, and returns dictionary with decoder hidden state or cache and the decoder output.\n        The decoder output is a 3d tensor (group_size, steps_count, decoder_output_dim)\n        if `self.decodes_parallel` is True, else it is a 2d tensor with (group_size, decoder_output_dim).\n\n        # Parameters\n\n        previous_steps_predictions : `torch.Tensor`, required\n            Embeddings of predictions on previous step.\n            Shape: (group_size, steps_count, decoder_output_dim)\n        encoder_outputs : `torch.Tensor`, required\n            Vectors of all encoder outputs.\n            Shape: (group_size, max_input_sequence_length, encoder_output_dim)\n        source_mask : `torch.Tensor`, required\n            This tensor contains mask for each input sequence.\n            Shape: (group_size, max_input_sequence_length)\n        previous_state : `Dict[str, torch.Tensor]`, required\n            previous state of decoder\n\n        # Returns\n\n        Tuple[Dict[str, torch.Tensor], torch.Tensor]\n        Tuple of new decoder state and decoder output. Output should be used to generate out sequence elements\n       \"\"\"", "\n", "raise", "NotImplementedError", "(", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.seq2seq_decoders.seq_decoder.SeqDecoder.__init__": [[32, 35], ["torch.nn.Module.__init__"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.graph_util.plot.GraphPlot.__init__"], ["def", "__init__", "(", "self", ",", "target_embedder", ":", "Embedding", ")", "->", "None", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "target_embedder", "=", "target_embedder", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.seq2seq_decoders.seq_decoder.SeqDecoder.get_output_dim": [[36, 42], ["NotImplementedError"], "methods", ["None"], ["", "def", "get_output_dim", "(", "self", ")", "->", "int", ":", "\n", "        ", "\"\"\"\n        The dimension of each timestep of the hidden state in the layer before final softmax.\n        Needed to check whether the model is compatible for embedding-final layer weight tying.\n        \"\"\"", "\n", "raise", "NotImplementedError", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.seq2seq_decoders.seq_decoder.SeqDecoder.get_metrics": [[43, 48], ["NotImplementedError"], "methods", ["None"], ["", "def", "get_metrics", "(", "self", ",", "reset", ":", "bool", "=", "False", ")", "->", "Dict", "[", "str", ",", "float", "]", ":", "\n", "        ", "\"\"\"\n        The decoder is responsible for computing metrics using the target tokens.\n        \"\"\"", "\n", "raise", "NotImplementedError", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.seq2seq_decoders.seq_decoder.SeqDecoder.forward": [[49, 69], ["NotImplementedError"], "methods", ["None"], ["", "def", "forward", "(", "\n", "self", ",", "\n", "encoder_out", ":", "Dict", "[", "str", ",", "torch", ".", "LongTensor", "]", ",", "\n", "target_tokens", ":", "Optional", "[", "Dict", "[", "str", ",", "torch", ".", "LongTensor", "]", "]", "=", "None", ",", "\n", ")", "->", "Dict", "[", "str", ",", "torch", ".", "Tensor", "]", ":", "\n", "        ", "\"\"\"\n        Decoding from encoded states to sequence of outputs\n        also computes loss if `target_tokens` are given.\n\n        # Parameters\n\n        encoder_out : `Dict[str, torch.LongTensor]`, required\n            Dictionary with encoded state, ideally containing the encoded vectors and the\n            source mask.\n        target_tokens : `Dict[str, torch.LongTensor]`, optional\n            The output of `TextField.as_array()` applied on the target `TextField`.\n\n       \"\"\"", "\n", "\n", "raise", "NotImplementedError", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.seq2seq_decoders.seq_decoder.SeqDecoder.post_process": [[70, 79], ["NotImplementedError"], "methods", ["None"], ["", "def", "post_process", "(", "\n", "self", ",", "output_dict", ":", "Dict", "[", "str", ",", "torch", ".", "Tensor", "]", "\n", ")", "->", "Dict", "[", "str", ",", "torch", ".", "Tensor", "]", ":", "\n", "        ", "\"\"\"\n            Post processing for converting raw outputs to prediction during inference.\n            The composing models such `allennlp.models.encoder_decoders.composed_seq2seq.ComposedSeq2Seq`\n            can call this method when `decode` is called.\n        \"\"\"", "\n", "raise", "NotImplementedError", "(", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.seq2seq_decoders.auto_regressive_seq_decoder.AutoRegressiveSeqDecoder.__init__": [[58, 126], ["allennlp.modules.seq2seq_decoders.seq_decoder.SeqDecoder.__init__", "auto_regressive_seq_decoder.AutoRegressiveSeqDecoder._vocab.get_token_index", "auto_regressive_seq_decoder.AutoRegressiveSeqDecoder._vocab.get_token_index", "allennlp.nn.beam_search.BeamSearch", "auto_regressive_seq_decoder.AutoRegressiveSeqDecoder._vocab.get_vocab_size", "torch.nn.Linear", "torch.nn.Linear", "auto_regressive_seq_decoder.AutoRegressiveSeqDecoder.target_embedder.get_output_dim", "allennlp.common.checks.ConfigurationError", "auto_regressive_seq_decoder.AutoRegressiveSeqDecoder._decoder_net.get_output_dim", "allennlp.common.checks.ConfigurationError"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.graph_util.plot.GraphPlot.__init__", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.data.vocabulary.Vocabulary.get_token_index", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.data.vocabulary.Vocabulary.get_token_index", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.data.vocabulary.Vocabulary.get_vocab_size", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.seq2seq_decoders.auto_regressive_seq_decoder.AutoRegressiveSeqDecoder.get_output_dim", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.seq2seq_decoders.auto_regressive_seq_decoder.AutoRegressiveSeqDecoder.get_output_dim"], ["def", "__init__", "(", "\n", "self", ",", "\n", "vocab", ":", "Vocabulary", ",", "\n", "decoder_net", ":", "DecoderNet", ",", "\n", "max_decoding_steps", ":", "int", ",", "\n", "target_embedder", ":", "Embedding", ",", "\n", "target_namespace", ":", "str", "=", "\"tokens\"", ",", "\n", "tie_output_embedding", ":", "bool", "=", "False", ",", "\n", "scheduled_sampling_ratio", ":", "float", "=", "0", ",", "\n", "label_smoothing_ratio", ":", "Optional", "[", "float", "]", "=", "None", ",", "\n", "beam_size", ":", "int", "=", "4", ",", "\n", "tensor_based_metric", ":", "Metric", "=", "None", ",", "\n", "token_based_metric", ":", "Metric", "=", "None", ",", "\n", ")", "->", "None", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "target_embedder", ")", "\n", "\n", "self", ".", "_vocab", "=", "vocab", "\n", "\n", "# Decodes the sequence of encoded hidden states into e new sequence of hidden states.", "\n", "self", ".", "_decoder_net", "=", "decoder_net", "\n", "self", ".", "_max_decoding_steps", "=", "max_decoding_steps", "\n", "self", ".", "_target_namespace", "=", "target_namespace", "\n", "self", ".", "_label_smoothing_ratio", "=", "label_smoothing_ratio", "\n", "\n", "# At prediction time, we use a beam search to find the most likely sequence of target tokens.", "\n", "# We need the start symbol to provide as the input at the first timestep of decoding, and", "\n", "# end symbol as a way to indicate the end of the decoded sequence.", "\n", "self", ".", "_start_index", "=", "self", ".", "_vocab", ".", "get_token_index", "(", "\n", "START_SYMBOL", ",", "self", ".", "_target_namespace", "\n", ")", "\n", "self", ".", "_end_index", "=", "self", ".", "_vocab", ".", "get_token_index", "(", "\n", "END_SYMBOL", ",", "self", ".", "_target_namespace", "\n", ")", "\n", "self", ".", "_beam_search", "=", "BeamSearch", "(", "\n", "self", ".", "_end_index", ",", "max_steps", "=", "max_decoding_steps", ",", "beam_size", "=", "beam_size", "\n", ")", "\n", "\n", "target_vocab_size", "=", "self", ".", "_vocab", ".", "get_vocab_size", "(", "self", ".", "_target_namespace", ")", "\n", "\n", "if", "(", "\n", "self", ".", "target_embedder", ".", "get_output_dim", "(", ")", "\n", "!=", "self", ".", "_decoder_net", ".", "target_embedding_dim", "\n", ")", ":", "\n", "            ", "raise", "ConfigurationError", "(", "\n", "\"Target Embedder output_dim doesn't match decoder module's input.\"", "\n", ")", "\n", "\n", "# We project the hidden state from the decoder into the output vocabulary space", "\n", "# in order to get log probabilities of each target token, at each time step.", "\n", "", "self", ".", "_output_projection_layer", "=", "Linear", "(", "\n", "self", ".", "_decoder_net", ".", "get_output_dim", "(", ")", ",", "target_vocab_size", "\n", ")", "\n", "\n", "if", "tie_output_embedding", ":", "\n", "            ", "if", "(", "\n", "self", ".", "_output_projection_layer", ".", "weight", ".", "shape", "\n", "!=", "self", ".", "target_embedder", ".", "weight", ".", "shape", "\n", ")", ":", "\n", "                ", "raise", "ConfigurationError", "(", "\n", "\"Can't tie embeddings with output linear layer, due to shape mismatch\"", "\n", ")", "\n", "", "self", ".", "_output_projection_layer", ".", "weight", "=", "self", ".", "target_embedder", ".", "weight", "\n", "\n", "# These metrics will be updated during training and validation", "\n", "", "self", ".", "_tensor_based_metric", "=", "tensor_based_metric", "\n", "self", ".", "_token_based_metric", "=", "token_based_metric", "\n", "\n", "self", ".", "_scheduled_sampling_ratio", "=", "scheduled_sampling_ratio", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.seq2seq_decoders.auto_regressive_seq_decoder.AutoRegressiveSeqDecoder._forward_beam_search": [[127, 149], ["state[].new_full", "auto_regressive_seq_decoder.AutoRegressiveSeqDecoder._beam_search.search", "state[].size"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.nn.beam_search.BeamSearch.search"], ["", "def", "_forward_beam_search", "(", "\n", "self", ",", "state", ":", "Dict", "[", "str", ",", "torch", ".", "Tensor", "]", "\n", ")", "->", "Dict", "[", "str", ",", "torch", ".", "Tensor", "]", ":", "\n", "        ", "\"\"\"\n        Prepare inputs for the beam search, does beam search and returns beam search results.\n        \"\"\"", "\n", "batch_size", "=", "state", "[", "\"source_mask\"", "]", ".", "size", "(", ")", "[", "0", "]", "\n", "start_predictions", "=", "state", "[", "\"source_mask\"", "]", ".", "new_full", "(", "\n", "(", "batch_size", ",", ")", ",", "fill_value", "=", "self", ".", "_start_index", "\n", ")", "\n", "\n", "# shape (all_top_k_predictions): (batch_size, beam_size, num_decoding_steps)", "\n", "# shape (log_probabilities): (batch_size, beam_size)", "\n", "all_top_k_predictions", ",", "log_probabilities", "=", "self", ".", "_beam_search", ".", "search", "(", "\n", "start_predictions", ",", "state", ",", "self", ".", "take_step", "\n", ")", "\n", "\n", "output_dict", "=", "{", "\n", "\"class_log_probabilities\"", ":", "log_probabilities", ",", "\n", "\"predictions\"", ":", "all_top_k_predictions", ",", "\n", "}", "\n", "return", "output_dict", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.seq2seq_decoders.auto_regressive_seq_decoder.AutoRegressiveSeqDecoder._forward_loss": [[150, 274], ["allennlp.nn.util.get_token_ids_from_text_field_tensors", "auto_regressive_seq_decoder.AutoRegressiveSeqDecoder.target_embedder", "allennlp.nn.util.get_text_field_mask", "allennlp.nn.util.get_text_field_mask", "auto_regressive_seq_decoder.AutoRegressiveSeqDecoder._get_loss", "auto_regressive_seq_decoder.AutoRegressiveSeqDecoder._decoder_net", "auto_regressive_seq_decoder.AutoRegressiveSeqDecoder._output_projection_layer", "allennlp.nn.util.get_token_ids_from_text_field_tensors.size", "source_mask.new_full", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "range", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "source_mask.size", "auto_regressive_seq_decoder.AutoRegressiveSeqDecoder._prepare_output_projections", "step_logits.append", "torch.max", "torch.max", "torch.max", "torch.max", "auto_regressive_seq_decoder.AutoRegressiveSeqDecoder.target_embedder().unsqueeze", "output_projections.unsqueeze", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.rand().item", "torch.rand().item", "torch.rand().item", "torch.rand().item", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "auto_regressive_seq_decoder.AutoRegressiveSeqDecoder.target_embedder", "torch.rand", "torch.rand", "torch.rand", "torch.rand"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.nn.util.get_token_ids_from_text_field_tensors", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.nn.util.get_text_field_mask", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.nn.util.get_text_field_mask", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.encoder_decoders.simple_seq2seq.SimpleSeq2Seq._get_loss", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.encoder_decoders.simple_seq2seq.SimpleSeq2Seq._prepare_output_projections"], ["", "def", "_forward_loss", "(", "\n", "self", ",", "state", ":", "Dict", "[", "str", ",", "torch", ".", "Tensor", "]", ",", "target_tokens", ":", "TextFieldTensors", "\n", ")", "->", "Dict", "[", "str", ",", "torch", ".", "Tensor", "]", ":", "\n", "        ", "\"\"\"\n        Make forward pass during training or do greedy search during prediction.\n\n        Notes\n        -----\n        We really only use the predictions from the method to test that beam search\n        with a beam size of 1 gives the same results.\n        \"\"\"", "\n", "# shape: (batch_size, max_input_sequence_length, encoder_output_dim)", "\n", "encoder_outputs", "=", "state", "[", "\"encoder_outputs\"", "]", "\n", "\n", "# shape: (batch_size, max_input_sequence_length)", "\n", "source_mask", "=", "state", "[", "\"source_mask\"", "]", "\n", "\n", "# shape: (batch_size, max_target_sequence_length)", "\n", "targets", "=", "util", ".", "get_token_ids_from_text_field_tensors", "(", "target_tokens", ")", "\n", "\n", "# Prepare embeddings for targets. They will be used as gold embeddings during decoder training", "\n", "# shape: (batch_size, max_target_sequence_length, embedding_dim)", "\n", "target_embedding", "=", "self", ".", "target_embedder", "(", "targets", ")", "\n", "\n", "# shape: (batch_size, max_target_batch_sequence_length)", "\n", "target_mask", "=", "util", ".", "get_text_field_mask", "(", "target_tokens", ")", "\n", "\n", "if", "self", ".", "_scheduled_sampling_ratio", "==", "0", "and", "self", ".", "_decoder_net", ".", "decodes_parallel", ":", "\n", "            ", "_", ",", "decoder_output", "=", "self", ".", "_decoder_net", "(", "\n", "previous_state", "=", "state", ",", "\n", "previous_steps_predictions", "=", "target_embedding", "[", ":", ",", ":", "-", "1", ",", ":", "]", ",", "\n", "encoder_outputs", "=", "encoder_outputs", ",", "\n", "source_mask", "=", "source_mask", ",", "\n", "previous_steps_mask", "=", "target_mask", "[", ":", ",", ":", "-", "1", "]", ",", "\n", ")", "\n", "\n", "# shape: (group_size, max_target_sequence_length, num_classes)", "\n", "logits", "=", "self", ".", "_output_projection_layer", "(", "decoder_output", ")", "\n", "", "else", ":", "\n", "            ", "batch_size", "=", "source_mask", ".", "size", "(", ")", "[", "0", "]", "\n", "_", ",", "target_sequence_length", "=", "targets", ".", "size", "(", ")", "\n", "\n", "# The last input from the target is either padding or the end symbol.", "\n", "# Either way, we don't have to process it.", "\n", "num_decoding_steps", "=", "target_sequence_length", "-", "1", "\n", "\n", "# Initialize target predictions with the start index.", "\n", "# shape: (batch_size,)", "\n", "last_predictions", "=", "source_mask", ".", "new_full", "(", "\n", "(", "batch_size", ",", ")", ",", "fill_value", "=", "self", ".", "_start_index", "\n", ")", "\n", "\n", "# shape: (steps, batch_size, target_embedding_dim)", "\n", "steps_embeddings", "=", "torch", ".", "Tensor", "(", "[", "]", ")", "\n", "\n", "step_logits", ":", "List", "[", "torch", ".", "Tensor", "]", "=", "[", "]", "\n", "\n", "for", "timestep", "in", "range", "(", "num_decoding_steps", ")", ":", "\n", "                ", "if", "(", "\n", "self", ".", "training", "\n", "and", "torch", ".", "rand", "(", "1", ")", ".", "item", "(", ")", "<", "self", ".", "_scheduled_sampling_ratio", "\n", ")", ":", "\n", "# Use gold tokens at test time and at a rate of 1 - _scheduled_sampling_ratio", "\n", "# during training.", "\n", "# shape: (batch_size, steps, target_embedding_dim)", "\n", "                    ", "state", "[", "\"previous_steps_predictions\"", "]", "=", "steps_embeddings", "\n", "\n", "# shape: (batch_size, )", "\n", "effective_last_prediction", "=", "last_predictions", "\n", "", "else", ":", "\n", "# shape: (batch_size, )", "\n", "                    ", "effective_last_prediction", "=", "targets", "[", ":", ",", "timestep", "]", "\n", "\n", "if", "timestep", "==", "0", ":", "\n", "                        ", "state", "[", "\"previous_steps_predictions\"", "]", "=", "torch", ".", "Tensor", "(", "[", "]", ")", "\n", "", "else", ":", "\n", "# shape: (batch_size, steps, target_embedding_dim)", "\n", "                        ", "state", "[", "\"previous_steps_predictions\"", "]", "=", "target_embedding", "[", "\n", ":", ",", ":", "timestep", "\n", "]", "\n", "\n", "# shape: (batch_size, num_classes)", "\n", "", "", "output_projections", ",", "state", "=", "self", ".", "_prepare_output_projections", "(", "\n", "effective_last_prediction", ",", "state", "\n", ")", "\n", "\n", "# list of tensors, shape: (batch_size, 1, num_classes)", "\n", "step_logits", ".", "append", "(", "output_projections", ".", "unsqueeze", "(", "1", ")", ")", "\n", "\n", "# shape (predicted_classes): (batch_size,)", "\n", "_", ",", "predicted_classes", "=", "torch", ".", "max", "(", "output_projections", ",", "1", ")", "\n", "\n", "# shape (predicted_classes): (batch_size,)", "\n", "last_predictions", "=", "predicted_classes", "\n", "\n", "# shape: (batch_size, 1, target_embedding_dim)", "\n", "last_predictions_embeddings", "=", "self", ".", "target_embedder", "(", "\n", "last_predictions", "\n", ")", ".", "unsqueeze", "(", "1", ")", "\n", "\n", "# This step is required, since we want to keep up two different prediction history: gold and real", "\n", "if", "steps_embeddings", ".", "shape", "[", "-", "1", "]", "==", "0", ":", "\n", "# There is no previous steps, except for start vectors in `last_predictions`", "\n", "# shape: (group_size, 1, target_embedding_dim)", "\n", "                    ", "steps_embeddings", "=", "last_predictions_embeddings", "\n", "", "else", ":", "\n", "# shape: (group_size, steps_count, target_embedding_dim)", "\n", "                    ", "steps_embeddings", "=", "torch", ".", "cat", "(", "\n", "[", "steps_embeddings", ",", "last_predictions_embeddings", "]", ",", "1", "\n", ")", "\n", "\n", "# shape: (batch_size, num_decoding_steps, num_classes)", "\n", "", "", "logits", "=", "torch", ".", "cat", "(", "step_logits", ",", "1", ")", "\n", "\n", "# Compute loss.", "\n", "", "target_mask", "=", "util", ".", "get_text_field_mask", "(", "target_tokens", ")", "\n", "loss", "=", "self", ".", "_get_loss", "(", "logits", ",", "targets", ",", "target_mask", ")", "\n", "\n", "# TODO: We will be using beam search to get predictions for validation, but if beam size in 1", "\n", "# we could consider taking the last_predictions here and building step_predictions", "\n", "# and use that instead of running beam search again, if performance in validation is taking a hit", "\n", "output_dict", "=", "{", "\"loss\"", ":", "loss", "}", "\n", "\n", "return", "output_dict", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.seq2seq_decoders.auto_regressive_seq_decoder.AutoRegressiveSeqDecoder._prepare_output_projections": [[275, 330], ["state.get", "auto_regressive_seq_decoder.AutoRegressiveSeqDecoder.target_embedder().unsqueeze", "auto_regressive_seq_decoder.AutoRegressiveSeqDecoder._decoder_net", "state.update", "auto_regressive_seq_decoder.AutoRegressiveSeqDecoder._output_projection_layer", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "auto_regressive_seq_decoder.AutoRegressiveSeqDecoder.target_embedder"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.params.Params.get", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.metrics.conll_coref_scores.Scorer.update"], ["", "def", "_prepare_output_projections", "(", "\n", "self", ",", "last_predictions", ":", "torch", ".", "Tensor", ",", "state", ":", "Dict", "[", "str", ",", "torch", ".", "Tensor", "]", "\n", ")", "->", "Tuple", "[", "torch", ".", "Tensor", ",", "Dict", "[", "str", ",", "torch", ".", "Tensor", "]", "]", ":", "\n", "        ", "\"\"\"\n        Decode current state and last prediction to produce produce projections\n        into the target space, which can then be used to get probabilities of\n        each target token for the next step.\n\n        Inputs are the same as for `take_step()`.\n        \"\"\"", "\n", "# shape: (group_size, max_input_sequence_length, encoder_output_dim)", "\n", "encoder_outputs", "=", "state", "[", "\"encoder_outputs\"", "]", "\n", "\n", "# shape: (group_size, max_input_sequence_length)", "\n", "source_mask", "=", "state", "[", "\"source_mask\"", "]", "\n", "\n", "# shape: (group_size, steps_count, decoder_output_dim)", "\n", "previous_steps_predictions", "=", "state", ".", "get", "(", "\"previous_steps_predictions\"", ")", "\n", "\n", "# shape: (batch_size, 1, target_embedding_dim)", "\n", "last_predictions_embeddings", "=", "self", ".", "target_embedder", "(", "last_predictions", ")", ".", "unsqueeze", "(", "\n", "1", "\n", ")", "\n", "\n", "if", "(", "\n", "previous_steps_predictions", "is", "None", "\n", "or", "previous_steps_predictions", ".", "shape", "[", "-", "1", "]", "==", "0", "\n", ")", ":", "\n", "# There is no previous steps, except for start vectors in `last_predictions`", "\n", "# shape: (group_size, 1, target_embedding_dim)", "\n", "            ", "previous_steps_predictions", "=", "last_predictions_embeddings", "\n", "", "else", ":", "\n", "# shape: (group_size, steps_count, target_embedding_dim)", "\n", "            ", "previous_steps_predictions", "=", "torch", ".", "cat", "(", "\n", "[", "previous_steps_predictions", ",", "last_predictions_embeddings", "]", ",", "1", "\n", ")", "\n", "\n", "", "decoder_state", ",", "decoder_output", "=", "self", ".", "_decoder_net", "(", "\n", "previous_state", "=", "state", ",", "\n", "encoder_outputs", "=", "encoder_outputs", ",", "\n", "source_mask", "=", "source_mask", ",", "\n", "previous_steps_predictions", "=", "previous_steps_predictions", ",", "\n", ")", "\n", "state", "[", "\"previous_steps_predictions\"", "]", "=", "previous_steps_predictions", "\n", "\n", "# Update state with new decoder state, override previous state", "\n", "state", ".", "update", "(", "decoder_state", ")", "\n", "\n", "if", "self", ".", "_decoder_net", ".", "decodes_parallel", ":", "\n", "            ", "decoder_output", "=", "decoder_output", "[", ":", ",", "-", "1", ",", ":", "]", "\n", "\n", "# shape: (group_size, num_classes)", "\n", "", "output_projections", "=", "self", ".", "_output_projection_layer", "(", "decoder_output", ")", "\n", "\n", "return", "output_projections", ",", "state", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.seq2seq_decoders.auto_regressive_seq_decoder.AutoRegressiveSeqDecoder._get_loss": [[331, 373], ["targets[].contiguous", "target_mask[].contiguous", "allennlp.nn.util.sequence_cross_entropy_with_logits"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.nn.util.sequence_cross_entropy_with_logits"], ["", "def", "_get_loss", "(", "\n", "self", ",", "\n", "logits", ":", "torch", ".", "LongTensor", ",", "\n", "targets", ":", "torch", ".", "LongTensor", ",", "\n", "target_mask", ":", "torch", ".", "LongTensor", ",", "\n", ")", "->", "torch", ".", "Tensor", ":", "\n", "        ", "\"\"\"\n        Compute loss.\n\n        Takes logits (unnormalized outputs from the decoder) of size (batch_size,\n        num_decoding_steps, num_classes), target indices of size (batch_size, num_decoding_steps+1)\n        and corresponding masks of size (batch_size, num_decoding_steps+1) steps and computes cross\n        entropy loss while taking the mask into account.\n\n        The length of `targets` is expected to be greater than that of `logits` because the\n        decoder does not need to compute the output corresponding to the last timestep of\n        `targets`. This method aligns the inputs appropriately to compute the loss.\n\n        During training, we want the logit corresponding to timestep i to be similar to the target\n        token from timestep i + 1. That is, the targets should be shifted by one timestep for\n        appropriate comparison.  Consider a single example where the target has 3 words, and\n        padding is to 7 tokens.\n           The complete sequence would correspond to <S> w1  w2  w3  <E> <P> <P>\n           and the mask would be                     1   1   1   1   1   0   0\n           and let the logits be                     l1  l2  l3  l4  l5  l6\n        We actually need to compare:\n           the sequence           w1  w2  w3  <E> <P> <P>\n           with masks             1   1   1   1   0   0\n           against                l1  l2  l3  l4  l5  l6\n           (where the input was)  <S> w1  w2  w3  <E> <P>\n        \"\"\"", "\n", "# shape: (batch_size, num_decoding_steps)", "\n", "relevant_targets", "=", "targets", "[", ":", ",", "1", ":", "]", ".", "contiguous", "(", ")", "\n", "\n", "# shape: (batch_size, num_decoding_steps)", "\n", "relevant_mask", "=", "target_mask", "[", ":", ",", "1", ":", "]", ".", "contiguous", "(", ")", "\n", "\n", "return", "util", ".", "sequence_cross_entropy_with_logits", "(", "\n", "logits", ",", "\n", "relevant_targets", ",", "\n", "relevant_mask", ",", "\n", "label_smoothing", "=", "self", ".", "_label_smoothing_ratio", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.seq2seq_decoders.auto_regressive_seq_decoder.AutoRegressiveSeqDecoder.get_output_dim": [[375, 377], ["auto_regressive_seq_decoder.AutoRegressiveSeqDecoder._decoder_net.get_output_dim"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.seq2seq_decoders.auto_regressive_seq_decoder.AutoRegressiveSeqDecoder.get_output_dim"], ["", "def", "get_output_dim", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "_decoder_net", ".", "get_output_dim", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.seq2seq_decoders.auto_regressive_seq_decoder.AutoRegressiveSeqDecoder.take_step": [[378, 420], ["auto_regressive_seq_decoder.AutoRegressiveSeqDecoder._prepare_output_projections", "torch.log_softmax", "torch.log_softmax"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.encoder_decoders.simple_seq2seq.SimpleSeq2Seq._prepare_output_projections"], ["", "def", "take_step", "(", "\n", "self", ",", "last_predictions", ":", "torch", ".", "Tensor", ",", "state", ":", "Dict", "[", "str", ",", "torch", ".", "Tensor", "]", "\n", ")", "->", "Tuple", "[", "torch", ".", "Tensor", ",", "Dict", "[", "str", ",", "torch", ".", "Tensor", "]", "]", ":", "\n", "        ", "\"\"\"\n        Take a decoding step. This is called by the beam search class.\n\n        # Parameters\n\n        last_predictions : `torch.Tensor`\n            A tensor of shape `(group_size,)`, which gives the indices of the predictions\n            during the last time step.\n        state : `Dict[str, torch.Tensor]`\n            A dictionary of tensors that contain the current state information\n            needed to predict the next step, which includes the encoder outputs,\n            the source mask, and the decoder hidden state and context. Each of these\n            tensors has shape `(group_size, *)`, where `*` can be any other number\n            of dimensions.\n\n        # Returns\n\n        Tuple[torch.Tensor, Dict[str, torch.Tensor]]\n            A tuple of `(log_probabilities, updated_state)`, where `log_probabilities`\n            is a tensor of shape `(group_size, num_classes)` containing the predicted\n            log probability of each class for the next step, for each item in the group,\n            while `updated_state` is a dictionary of tensors containing the encoder outputs,\n            source mask, and updated decoder hidden state and context.\n\n        Notes\n        -----\n            We treat the inputs as a batch, even though `group_size` is not necessarily\n            equal to `batch_size`, since the group may contain multiple states\n            for each source sentence in the batch.\n        \"\"\"", "\n", "# shape: (group_size, num_classes)", "\n", "output_projections", ",", "state", "=", "self", ".", "_prepare_output_projections", "(", "\n", "last_predictions", ",", "state", "\n", ")", "\n", "\n", "# shape: (group_size, num_classes)", "\n", "class_log_probabilities", "=", "F", ".", "log_softmax", "(", "output_projections", ",", "dim", "=", "-", "1", ")", "\n", "\n", "return", "class_log_probabilities", ",", "state", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.seq2seq_decoders.auto_regressive_seq_decoder.AutoRegressiveSeqDecoder.get_metrics": [[421, 432], ["all_metrics.update", "all_metrics.update", "auto_regressive_seq_decoder.AutoRegressiveSeqDecoder._tensor_based_metric.get_metric", "auto_regressive_seq_decoder.AutoRegressiveSeqDecoder._token_based_metric.get_metric"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.metrics.conll_coref_scores.Scorer.update", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.metrics.conll_coref_scores.Scorer.update", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.metrics.sequence_accuracy.SequenceAccuracy.get_metric", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.metrics.sequence_accuracy.SequenceAccuracy.get_metric"], ["", "@", "overrides", "\n", "def", "get_metrics", "(", "self", ",", "reset", ":", "bool", "=", "False", ")", "->", "Dict", "[", "str", ",", "float", "]", ":", "\n", "        ", "all_metrics", ":", "Dict", "[", "str", ",", "float", "]", "=", "{", "}", "\n", "if", "not", "self", ".", "training", ":", "\n", "            ", "if", "self", ".", "_tensor_based_metric", "is", "not", "None", ":", "\n", "                ", "all_metrics", ".", "update", "(", "\n", "self", ".", "_tensor_based_metric", ".", "get_metric", "(", "reset", "=", "reset", ")", "# type: ignore", "\n", ")", "\n", "", "if", "self", ".", "_token_based_metric", "is", "not", "None", ":", "\n", "                ", "all_metrics", ".", "update", "(", "self", ".", "_token_based_metric", ".", "get_metric", "(", "reset", "=", "reset", ")", ")", "# type: ignore", "\n", "", "", "return", "all_metrics", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.seq2seq_decoders.auto_regressive_seq_decoder.AutoRegressiveSeqDecoder.forward": [[433, 473], ["auto_regressive_seq_decoder.AutoRegressiveSeqDecoder._decoder_net.init_decoder_state", "state.update", "auto_regressive_seq_decoder.AutoRegressiveSeqDecoder._forward_loss", "auto_regressive_seq_decoder.AutoRegressiveSeqDecoder._forward_beam_search", "auto_regressive_seq_decoder.AutoRegressiveSeqDecoder.update", "allennlp.nn.util.get_token_ids_from_text_field_tensors", "auto_regressive_seq_decoder.AutoRegressiveSeqDecoder._tensor_based_metric", "auto_regressive_seq_decoder.AutoRegressiveSeqDecoder.post_process", "auto_regressive_seq_decoder.AutoRegressiveSeqDecoder._token_based_metric", "auto_regressive_seq_decoder.AutoRegressiveSeqDecoder.indices_to_tokens"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.seq2seq_decoders.decoder_net.DecoderNet.init_decoder_state", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.metrics.conll_coref_scores.Scorer.update", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.encoder_decoders.copynet_seq2seq.CopyNetSeq2Seq._forward_loss", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.encoder_decoders.simple_seq2seq.SimpleSeq2Seq._forward_beam_search", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.metrics.conll_coref_scores.Scorer.update", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.nn.util.get_token_ids_from_text_field_tensors", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.seq2seq_decoders.auto_regressive_seq_decoder.AutoRegressiveSeqDecoder.post_process", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.seq2seq_decoders.auto_regressive_seq_decoder.AutoRegressiveSeqDecoder.indices_to_tokens"], ["", "@", "overrides", "\n", "def", "forward", "(", "\n", "self", ",", "\n", "encoder_out", ":", "Dict", "[", "str", ",", "torch", ".", "LongTensor", "]", ",", "\n", "target_tokens", ":", "TextFieldTensors", "=", "None", ",", "\n", ")", "->", "Dict", "[", "str", ",", "torch", ".", "Tensor", "]", ":", "\n", "        ", "state", "=", "encoder_out", "\n", "decoder_init_state", "=", "self", ".", "_decoder_net", ".", "init_decoder_state", "(", "state", ")", "\n", "state", ".", "update", "(", "decoder_init_state", ")", "\n", "\n", "if", "target_tokens", ":", "\n", "            ", "output_dict", "=", "self", ".", "_forward_loss", "(", "state", ",", "target_tokens", ")", "\n", "", "else", ":", "\n", "            ", "output_dict", "=", "{", "}", "\n", "\n", "", "if", "not", "self", ".", "training", ":", "\n", "            ", "predictions", "=", "self", ".", "_forward_beam_search", "(", "state", ")", "\n", "output_dict", ".", "update", "(", "predictions", ")", "\n", "\n", "if", "target_tokens", ":", "\n", "                ", "targets", "=", "util", ".", "get_token_ids_from_text_field_tensors", "(", "target_tokens", ")", "\n", "if", "self", ".", "_tensor_based_metric", "is", "not", "None", ":", "\n", "# shape: (batch_size, beam_size, max_sequence_length)", "\n", "                    ", "top_k_predictions", "=", "output_dict", "[", "\"predictions\"", "]", "\n", "# shape: (batch_size, max_predicted_sequence_length)", "\n", "best_predictions", "=", "top_k_predictions", "[", ":", ",", "0", ",", ":", "]", "\n", "\n", "self", ".", "_tensor_based_metric", "(", "# type: ignore", "\n", "best_predictions", ",", "targets", "\n", ")", "\n", "\n", "", "if", "self", ".", "_token_based_metric", "is", "not", "None", ":", "\n", "                    ", "output_dict", "=", "self", ".", "post_process", "(", "output_dict", ")", "\n", "predicted_tokens", "=", "output_dict", "[", "\"predicted_tokens\"", "]", "\n", "\n", "self", ".", "_token_based_metric", "(", "# type: ignore", "\n", "predicted_tokens", ",", "self", ".", "indices_to_tokens", "(", "targets", "[", ":", ",", "1", ":", "]", ")", ",", "\n", ")", "\n", "\n", "", "", "", "return", "output_dict", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.seq2seq_decoders.auto_regressive_seq_decoder.AutoRegressiveSeqDecoder.post_process": [[474, 486], ["auto_regressive_seq_decoder.AutoRegressiveSeqDecoder.indices_to_tokens"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.seq2seq_decoders.auto_regressive_seq_decoder.AutoRegressiveSeqDecoder.indices_to_tokens"], ["", "@", "overrides", "\n", "def", "post_process", "(", "\n", "self", ",", "output_dict", ":", "Dict", "[", "str", ",", "torch", ".", "Tensor", "]", "\n", ")", "->", "Dict", "[", "str", ",", "torch", ".", "Tensor", "]", ":", "\n", "        ", "\"\"\"\n        This method trims the output predictions to the first end symbol, replaces indices with\n        corresponding tokens, and adds a field called `predicted_tokens` to the `output_dict`.\n        \"\"\"", "\n", "predicted_indices", "=", "output_dict", "[", "\"predictions\"", "]", "\n", "all_predicted_tokens", "=", "self", ".", "indices_to_tokens", "(", "predicted_indices", ")", "\n", "output_dict", "[", "\"predicted_tokens\"", "]", "=", "all_predicted_tokens", "\n", "return", "output_dict", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.seq2seq_decoders.auto_regressive_seq_decoder.AutoRegressiveSeqDecoder.indices_to_tokens": [[487, 509], ["isinstance", "batch_indeces.detach().cpu().numpy.detach().cpu().numpy.detach().cpu().numpy", "list", "all_tokens.append", "len", "auto_regressive_seq_decoder.AutoRegressiveSeqDecoder._vocab.get_token_from_index", "batch_indeces.detach().cpu().numpy.detach().cpu().numpy.detach().cpu", "list.index", "batch_indeces.detach().cpu().numpy.detach().cpu().numpy.detach"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.data.vocabulary.Vocabulary.get_token_from_index", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.fields.text_field.TextField.index"], ["", "def", "indices_to_tokens", "(", "self", ",", "batch_indeces", ":", "numpy", ".", "ndarray", ")", "->", "List", "[", "List", "[", "str", "]", "]", ":", "\n", "\n", "        ", "if", "not", "isinstance", "(", "batch_indeces", ",", "numpy", ".", "ndarray", ")", ":", "\n", "            ", "batch_indeces", "=", "batch_indeces", ".", "detach", "(", ")", ".", "cpu", "(", ")", ".", "numpy", "(", ")", "\n", "\n", "", "all_tokens", "=", "[", "]", "\n", "for", "indices", "in", "batch_indeces", ":", "\n", "# Beam search gives us the top k results for each source sentence in the batch", "\n", "# but we just want the single best.", "\n", "            ", "if", "len", "(", "indices", ".", "shape", ")", ">", "1", ":", "\n", "                ", "indices", "=", "indices", "[", "0", "]", "\n", "", "indices", "=", "list", "(", "indices", ")", "\n", "# Collect indices till the first end_symbol", "\n", "if", "self", ".", "_end_index", "in", "indices", ":", "\n", "                ", "indices", "=", "indices", "[", ":", "indices", ".", "index", "(", "self", ".", "_end_index", ")", "]", "\n", "", "tokens", "=", "[", "\n", "self", ".", "_vocab", ".", "get_token_from_index", "(", "x", ",", "namespace", "=", "self", ".", "_target_namespace", ")", "\n", "for", "x", "in", "indices", "\n", "]", "\n", "all_tokens", ".", "append", "(", "tokens", ")", "\n", "\n", "", "return", "all_tokens", "\n", "", "", ""]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.attention.legacy_attention.LegacyAttention.__init__": [[18, 23], ["allennlp.modules.attention.attention.Attention.__init__", "allennlp.modules.similarity_functions.DotProductSimilarity"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.graph_util.plot.GraphPlot.__init__"], ["def", "__init__", "(", "\n", "self", ",", "similarity_function", ":", "SimilarityFunction", "=", "None", ",", "normalize", ":", "bool", "=", "True", "\n", ")", "->", "None", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "normalize", ")", "\n", "self", ".", "_similarity_function", "=", "similarity_function", "or", "DotProductSimilarity", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.attention.legacy_attention.LegacyAttention._forward_internal": [[24, 32], ["vector.unsqueeze().expand", "legacy_attention.LegacyAttention._similarity_function", "vector.unsqueeze", "vector.size", "matrix.size", "vector.size"], "methods", ["None"], ["", "@", "overrides", "\n", "def", "_forward_internal", "(", "\n", "self", ",", "vector", ":", "torch", ".", "Tensor", ",", "matrix", ":", "torch", ".", "Tensor", "\n", ")", "->", "torch", ".", "Tensor", ":", "\n", "        ", "tiled_vector", "=", "vector", ".", "unsqueeze", "(", "1", ")", ".", "expand", "(", "\n", "vector", ".", "size", "(", ")", "[", "0", "]", ",", "matrix", ".", "size", "(", ")", "[", "1", "]", ",", "vector", ".", "size", "(", ")", "[", "1", "]", "\n", ")", "\n", "return", "self", ".", "_similarity_function", "(", "tiled_vector", ",", "matrix", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.attention.linear_attention.LinearAttention.__init__": [[47, 62], ["allennlp.modules.attention.legacy_attention.Attention.__init__", "allennlp.nn.util.get_combined_dim", "torch.nn.Parameter", "torch.nn.Parameter", "linear_attention.LinearAttention.reset_parameters", "torch.Tensor", "torch.Tensor", "allennlp.nn.activations.Activation.by_name"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.graph_util.plot.GraphPlot.__init__", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.nn.util.get_combined_dim", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.similarity_functions.linear.LinearSimilarity.reset_parameters", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.registrable.Registrable.by_name"], ["def", "__init__", "(", "\n", "self", ",", "\n", "tensor_1_dim", ":", "int", ",", "\n", "tensor_2_dim", ":", "int", ",", "\n", "combination", ":", "str", "=", "\"x,y\"", ",", "\n", "activation", ":", "Activation", "=", "None", ",", "\n", "normalize", ":", "bool", "=", "True", ",", "\n", ")", "->", "None", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "normalize", ")", "\n", "self", ".", "_combination", "=", "combination", "\n", "combined_dim", "=", "util", ".", "get_combined_dim", "(", "combination", ",", "[", "tensor_1_dim", ",", "tensor_2_dim", "]", ")", "\n", "self", ".", "_weight_vector", "=", "Parameter", "(", "torch", ".", "Tensor", "(", "combined_dim", ")", ")", "\n", "self", ".", "_bias", "=", "Parameter", "(", "torch", ".", "Tensor", "(", "1", ")", ")", "\n", "self", ".", "_activation", "=", "activation", "or", "Activation", ".", "by_name", "(", "\"linear\"", ")", "(", ")", "\n", "self", ".", "reset_parameters", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.attention.linear_attention.LinearAttention.reset_parameters": [[63, 67], ["math.sqrt", "linear_attention.LinearAttention._weight_vector.data.uniform_", "linear_attention.LinearAttention._bias.data.fill_", "linear_attention.LinearAttention._weight_vector.size"], "methods", ["None"], ["", "def", "reset_parameters", "(", "self", ")", ":", "\n", "        ", "std", "=", "math", ".", "sqrt", "(", "6", "/", "(", "self", ".", "_weight_vector", ".", "size", "(", "0", ")", "+", "1", ")", ")", "\n", "self", ".", "_weight_vector", ".", "data", ".", "uniform_", "(", "-", "std", ",", "std", ")", "\n", "self", ".", "_bias", ".", "data", ".", "fill_", "(", "0", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.attention.linear_attention.LinearAttention._forward_internal": [[68, 76], ["allennlp.nn.util.combine_tensors_and_multiply", "linear_attention.LinearAttention._activation", "vector.unsqueeze", "allennlp.nn.util.combine_tensors_and_multiply.squeeze"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.nn.util.combine_tensors_and_multiply"], ["", "@", "overrides", "\n", "def", "_forward_internal", "(", "\n", "self", ",", "vector", ":", "torch", ".", "Tensor", ",", "matrix", ":", "torch", ".", "Tensor", "\n", ")", "->", "torch", ".", "Tensor", ":", "\n", "        ", "combined_tensors", "=", "util", ".", "combine_tensors_and_multiply", "(", "\n", "self", ".", "_combination", ",", "[", "vector", ".", "unsqueeze", "(", "1", ")", ",", "matrix", "]", ",", "self", ".", "_weight_vector", "\n", ")", "\n", "return", "self", ".", "_activation", "(", "combined_tensors", ".", "squeeze", "(", "1", ")", "+", "self", ".", "_bias", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.attention.dot_product_attention.DotProductAttention._forward_internal": [[12, 17], ["matrix.bmm().squeeze", "matrix.bmm", "vector.unsqueeze"], "methods", ["None"], ["@", "overrides", "\n", "def", "_forward_internal", "(", "\n", "self", ",", "vector", ":", "torch", ".", "Tensor", ",", "matrix", ":", "torch", ".", "Tensor", "\n", ")", "->", "torch", ".", "Tensor", ":", "\n", "        ", "return", "matrix", ".", "bmm", "(", "vector", ".", "unsqueeze", "(", "-", "1", ")", ")", ".", "squeeze", "(", "-", "1", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.attention.additive_attention.AdditiveAttention.__init__": [[33, 41], ["allennlp.modules.attention.attention.Attention.__init__", "torch.nn.parameter.Parameter", "torch.nn.parameter.Parameter", "torch.nn.parameter.Parameter", "additive_attention.AdditiveAttention.reset_parameters", "torch.Tensor", "torch.Tensor", "torch.Tensor"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.graph_util.plot.GraphPlot.__init__", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.similarity_functions.linear.LinearSimilarity.reset_parameters"], ["def", "__init__", "(", "\n", "self", ",", "vector_dim", ":", "int", ",", "matrix_dim", ":", "int", ",", "normalize", ":", "bool", "=", "True", "\n", ")", "->", "None", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "normalize", ")", "\n", "self", ".", "_w_matrix", "=", "Parameter", "(", "torch", ".", "Tensor", "(", "vector_dim", ",", "vector_dim", ")", ")", "\n", "self", ".", "_u_matrix", "=", "Parameter", "(", "torch", ".", "Tensor", "(", "matrix_dim", ",", "vector_dim", ")", ")", "\n", "self", ".", "_v_vector", "=", "Parameter", "(", "torch", ".", "Tensor", "(", "vector_dim", ",", "1", ")", ")", "\n", "self", ".", "reset_parameters", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.attention.additive_attention.AdditiveAttention.reset_parameters": [[42, 46], ["torch.nn.init.xavier_uniform_", "torch.nn.init.xavier_uniform_", "torch.nn.init.xavier_uniform_"], "methods", ["None"], ["", "def", "reset_parameters", "(", "self", ")", ":", "\n", "        ", "torch", ".", "nn", ".", "init", ".", "xavier_uniform_", "(", "self", ".", "_w_matrix", ")", "\n", "torch", ".", "nn", ".", "init", ".", "xavier_uniform_", "(", "self", ".", "_u_matrix", ")", "\n", "torch", ".", "nn", ".", "init", ".", "xavier_uniform_", "(", "self", ".", "_v_vector", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.attention.additive_attention.AdditiveAttention._forward_internal": [[47, 56], ["torch.tanh", "torch.tanh.matmul().squeeze", "vector.matmul().unsqueeze", "matrix.matmul", "torch.tanh.matmul", "vector.matmul"], "methods", ["None"], ["", "@", "overrides", "\n", "def", "_forward_internal", "(", "\n", "self", ",", "vector", ":", "torch", ".", "Tensor", ",", "matrix", ":", "torch", ".", "Tensor", "\n", ")", "->", "torch", ".", "Tensor", ":", "\n", "        ", "intermediate", "=", "vector", ".", "matmul", "(", "self", ".", "_w_matrix", ")", ".", "unsqueeze", "(", "1", ")", "+", "matrix", ".", "matmul", "(", "\n", "self", ".", "_u_matrix", "\n", ")", "\n", "intermediate", "=", "torch", ".", "tanh", "(", "intermediate", ")", "\n", "return", "intermediate", ".", "matmul", "(", "self", ".", "_v_vector", ")", ".", "squeeze", "(", "2", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.attention.attention.Attention.__init__": [[37, 40], ["super().__init__"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.graph_util.plot.GraphPlot.__init__"], ["def", "__init__", "(", "self", ",", "normalize", ":", "bool", "=", "True", ")", "->", "None", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "_normalize", "=", "normalize", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.attention.attention.Attention.forward": [[41, 53], ["attention.Attention._forward_internal", "allennlp.nn.util.masked_softmax"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.attention.bilinear_attention.BilinearAttention._forward_internal", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.nn.util.masked_softmax"], ["", "@", "overrides", "\n", "def", "forward", "(", "\n", "self", ",", "\n", "vector", ":", "torch", ".", "Tensor", ",", "\n", "matrix", ":", "torch", ".", "Tensor", ",", "\n", "matrix_mask", ":", "torch", ".", "Tensor", "=", "None", ",", "\n", ")", "->", "torch", ".", "Tensor", ":", "\n", "        ", "similarities", "=", "self", ".", "_forward_internal", "(", "vector", ",", "matrix", ")", "\n", "if", "self", ".", "_normalize", ":", "\n", "            ", "return", "masked_softmax", "(", "similarities", ",", "matrix_mask", ")", "\n", "", "else", ":", "\n", "            ", "return", "similarities", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.attention.attention.Attention._forward_internal": [[54, 58], ["None"], "methods", ["None"], ["", "", "def", "_forward_internal", "(", "\n", "self", ",", "vector", ":", "torch", ".", "Tensor", ",", "matrix", ":", "torch", ".", "Tensor", "\n", ")", "->", "torch", ".", "Tensor", ":", "\n", "        ", "raise", "NotImplementedError", "\n", "", "", ""]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.attention.cosine_attention.CosineAttention._forward_internal": [[12, 19], ["torch.bmm().squeeze", "vector.norm", "matrix.norm", "torch.bmm", "a_norm.unsqueeze", "b_norm.transpose"], "methods", ["None"], ["@", "overrides", "\n", "def", "_forward_internal", "(", "\n", "self", ",", "vector", ":", "torch", ".", "Tensor", ",", "matrix", ":", "torch", ".", "Tensor", "\n", ")", "->", "torch", ".", "Tensor", ":", "\n", "        ", "a_norm", "=", "vector", "/", "(", "vector", ".", "norm", "(", "p", "=", "2", ",", "dim", "=", "-", "1", ",", "keepdim", "=", "True", ")", "+", "1e-13", ")", "\n", "b_norm", "=", "matrix", "/", "(", "matrix", ".", "norm", "(", "p", "=", "2", ",", "dim", "=", "-", "1", ",", "keepdim", "=", "True", ")", "+", "1e-13", ")", "\n", "return", "torch", ".", "bmm", "(", "a_norm", ".", "unsqueeze", "(", "dim", "=", "1", ")", ",", "b_norm", ".", "transpose", "(", "-", "1", ",", "-", "2", ")", ")", ".", "squeeze", "(", "1", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.attention.bilinear_attention.BilinearAttention.__init__": [[34, 46], ["allennlp.modules.attention.attention.Attention.__init__", "torch.nn.parameter.Parameter", "torch.nn.parameter.Parameter", "bilinear_attention.BilinearAttention.reset_parameters", "torch.Tensor", "torch.Tensor", "allennlp.nn.Activation.by_name"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.graph_util.plot.GraphPlot.__init__", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.similarity_functions.linear.LinearSimilarity.reset_parameters", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.registrable.Registrable.by_name"], ["def", "__init__", "(", "\n", "self", ",", "\n", "vector_dim", ":", "int", ",", "\n", "matrix_dim", ":", "int", ",", "\n", "activation", ":", "Activation", "=", "None", ",", "\n", "normalize", ":", "bool", "=", "True", ",", "\n", ")", "->", "None", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "normalize", ")", "\n", "self", ".", "_weight_matrix", "=", "Parameter", "(", "torch", ".", "Tensor", "(", "vector_dim", ",", "matrix_dim", ")", ")", "\n", "self", ".", "_bias", "=", "Parameter", "(", "torch", ".", "Tensor", "(", "1", ")", ")", "\n", "self", ".", "_activation", "=", "activation", "or", "Activation", ".", "by_name", "(", "\"linear\"", ")", "(", ")", "\n", "self", ".", "reset_parameters", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.attention.bilinear_attention.BilinearAttention.reset_parameters": [[47, 50], ["torch.nn.init.xavier_uniform_", "bilinear_attention.BilinearAttention._bias.data.fill_"], "methods", ["None"], ["", "def", "reset_parameters", "(", "self", ")", ":", "\n", "        ", "torch", ".", "nn", ".", "init", ".", "xavier_uniform_", "(", "self", ".", "_weight_matrix", ")", "\n", "self", ".", "_bias", ".", "data", ".", "fill_", "(", "0", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.attention.bilinear_attention.BilinearAttention._forward_internal": [[51, 58], ["vector.mm().unsqueeze", "bilinear_attention.BilinearAttention._activation", "vector.mm", "vector.mm().unsqueeze.bmm().squeeze", "vector.mm().unsqueeze.bmm", "matrix.transpose"], "methods", ["None"], ["", "@", "overrides", "\n", "def", "_forward_internal", "(", "\n", "self", ",", "vector", ":", "torch", ".", "Tensor", ",", "matrix", ":", "torch", ".", "Tensor", "\n", ")", "->", "torch", ".", "Tensor", ":", "\n", "        ", "intermediate", "=", "vector", ".", "mm", "(", "self", ".", "_weight_matrix", ")", ".", "unsqueeze", "(", "1", ")", "\n", "return", "self", ".", "_activation", "(", "\n", "intermediate", ".", "bmm", "(", "matrix", ".", "transpose", "(", "1", ",", "2", ")", ")", ".", "squeeze", "(", "1", ")", "+", "self", ".", "_bias", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.similarity_functions.similarity_function.SimilarityFunction.forward": [[26, 34], ["None"], "methods", ["None"], ["def", "forward", "(", "self", ",", "tensor_1", ":", "torch", ".", "Tensor", ",", "tensor_2", ":", "torch", ".", "Tensor", ")", "->", "torch", ".", "Tensor", ":", "\n", "\n", "        ", "\"\"\"\n        Takes two tensors of the same shape, such as `(batch_size, length_1, length_2,\n        embedding_dim)`.  Computes a (possibly parameterized) similarity on the final dimension\n        and returns a tensor with one less dimension, such as `(batch_size, length_1, length_2)`.\n        \"\"\"", "\n", "raise", "NotImplementedError", "\n", "", "", ""]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.similarity_functions.bilinear.BilinearSimilarity.__init__": [[31, 39], ["allennlp.modules.similarity_functions.similarity_function.SimilarityFunction.__init__", "torch.nn.parameter.Parameter", "torch.nn.parameter.Parameter", "bilinear.BilinearSimilarity.reset_parameters", "torch.Tensor", "torch.Tensor", "allennlp.nn.Activation.by_name"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.graph_util.plot.GraphPlot.__init__", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.similarity_functions.linear.LinearSimilarity.reset_parameters", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.registrable.Registrable.by_name"], ["def", "__init__", "(", "\n", "self", ",", "tensor_1_dim", ":", "int", ",", "tensor_2_dim", ":", "int", ",", "activation", ":", "Activation", "=", "None", "\n", ")", "->", "None", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "_weight_matrix", "=", "Parameter", "(", "torch", ".", "Tensor", "(", "tensor_1_dim", ",", "tensor_2_dim", ")", ")", "\n", "self", ".", "_bias", "=", "Parameter", "(", "torch", ".", "Tensor", "(", "1", ")", ")", "\n", "self", ".", "_activation", "=", "activation", "or", "Activation", ".", "by_name", "(", "\"linear\"", ")", "(", ")", "\n", "self", ".", "reset_parameters", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.similarity_functions.bilinear.BilinearSimilarity.reset_parameters": [[40, 43], ["torch.nn.init.xavier_uniform_", "bilinear.BilinearSimilarity._bias.data.fill_"], "methods", ["None"], ["", "def", "reset_parameters", "(", "self", ")", ":", "\n", "        ", "torch", ".", "nn", ".", "init", ".", "xavier_uniform_", "(", "self", ".", "_weight_matrix", ")", "\n", "self", ".", "_bias", ".", "data", ".", "fill_", "(", "0", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.similarity_functions.bilinear.BilinearSimilarity.forward": [[44, 49], ["torch.matmul", "bilinear.BilinearSimilarity._activation"], "methods", ["None"], ["", "@", "overrides", "\n", "def", "forward", "(", "self", ",", "tensor_1", ":", "torch", ".", "Tensor", ",", "tensor_2", ":", "torch", ".", "Tensor", ")", "->", "torch", ".", "Tensor", ":", "\n", "        ", "intermediate", "=", "torch", ".", "matmul", "(", "tensor_1", ",", "self", ".", "_weight_matrix", ")", "\n", "result", "=", "(", "intermediate", "*", "tensor_2", ")", ".", "sum", "(", "dim", "=", "-", "1", ")", "\n", "return", "self", ".", "_activation", "(", "result", "+", "self", ".", "_bias", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.similarity_functions.dot_product.DotProductSimilarity.__init__": [[22, 25], ["allennlp.modules.similarity_functions.similarity_function.SimilarityFunction.__init__"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.graph_util.plot.GraphPlot.__init__"], ["def", "__init__", "(", "self", ",", "scale_output", ":", "bool", "=", "False", ")", "->", "None", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "_scale_output", "=", "scale_output", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.similarity_functions.dot_product.DotProductSimilarity.forward": [[26, 32], ["math.sqrt", "tensor_1.size"], "methods", ["None"], ["", "@", "overrides", "\n", "def", "forward", "(", "self", ",", "tensor_1", ":", "torch", ".", "Tensor", ",", "tensor_2", ":", "torch", ".", "Tensor", ")", "->", "torch", ".", "Tensor", ":", "\n", "        ", "result", "=", "(", "tensor_1", "*", "tensor_2", ")", ".", "sum", "(", "dim", "=", "-", "1", ")", "\n", "if", "self", ".", "_scale_output", ":", "\n", "            ", "result", "*=", "math", ".", "sqrt", "(", "tensor_1", ".", "size", "(", "-", "1", ")", ")", "\n", "", "return", "result", "\n", "", "", ""]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.similarity_functions.cosine.CosineSimilarity.forward": [[14, 19], ["tensor_1.norm", "tensor_2.norm"], "methods", ["None"], ["@", "LearningRateScheduler", ".", "register", "(", "\"cosine\"", ")", "\n", "class", "CosineWithRestarts", "(", "LearningRateScheduler", ")", ":", "\n", "    "]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.similarity_functions.multiheaded.MultiHeadedSimilarity.__init__": [[49, 81], ["allennlp.modules.similarity_functions.dot_product.DotProductSimilarity", "allennlp.modules.similarity_functions.similarity_function.SimilarityFunction.__init__", "torch.nn.parameter.Parameter", "torch.nn.parameter.Parameter", "multiheaded.MultiHeadedSimilarity.reset_parameters", "allennlp.common.checks.ConfigurationError", "allennlp.common.checks.ConfigurationError", "torch.Tensor", "torch.Tensor"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.graph_util.plot.GraphPlot.__init__", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.similarity_functions.linear.LinearSimilarity.reset_parameters"], ["def", "__init__", "(", "\n", "self", ",", "\n", "num_heads", ":", "int", ",", "\n", "tensor_1_dim", ":", "int", ",", "\n", "tensor_1_projected_dim", ":", "int", "=", "None", ",", "\n", "tensor_2_dim", ":", "int", "=", "None", ",", "\n", "tensor_2_projected_dim", ":", "int", "=", "None", ",", "\n", "internal_similarity", ":", "SimilarityFunction", "=", "DotProductSimilarity", "(", ")", ",", "\n", ")", "->", "None", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "num_heads", "=", "num_heads", "\n", "self", ".", "_internal_similarity", "=", "internal_similarity", "\n", "tensor_1_projected_dim", "=", "tensor_1_projected_dim", "or", "tensor_1_dim", "\n", "tensor_2_dim", "=", "tensor_2_dim", "or", "tensor_1_dim", "\n", "tensor_2_projected_dim", "=", "tensor_2_projected_dim", "or", "tensor_2_dim", "\n", "if", "tensor_1_projected_dim", "%", "num_heads", "!=", "0", ":", "\n", "            ", "raise", "ConfigurationError", "(", "\n", "\"Projected dimension not divisible by number of heads: %d, %d\"", "\n", "%", "(", "tensor_1_projected_dim", ",", "num_heads", ")", "\n", ")", "\n", "", "if", "tensor_2_projected_dim", "%", "num_heads", "!=", "0", ":", "\n", "            ", "raise", "ConfigurationError", "(", "\n", "\"Projected dimension not divisible by number of heads: %d, %d\"", "\n", "%", "(", "tensor_2_projected_dim", ",", "num_heads", ")", "\n", ")", "\n", "", "self", ".", "_tensor_1_projection", "=", "Parameter", "(", "\n", "torch", ".", "Tensor", "(", "tensor_1_dim", ",", "tensor_1_projected_dim", ")", "\n", ")", "\n", "self", ".", "_tensor_2_projection", "=", "Parameter", "(", "\n", "torch", ".", "Tensor", "(", "tensor_2_dim", ",", "tensor_2_projected_dim", ")", "\n", ")", "\n", "self", ".", "reset_parameters", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.similarity_functions.multiheaded.MultiHeadedSimilarity.reset_parameters": [[82, 85], ["torch.nn.init.xavier_uniform_", "torch.nn.init.xavier_uniform_"], "methods", ["None"], ["", "def", "reset_parameters", "(", "self", ")", ":", "\n", "        ", "torch", ".", "nn", ".", "init", ".", "xavier_uniform_", "(", "self", ".", "_tensor_1_projection", ")", "\n", "torch", ".", "nn", ".", "init", ".", "xavier_uniform_", "(", "self", ".", "_tensor_2_projection", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.similarity_functions.multiheaded.MultiHeadedSimilarity.forward": [[86, 111], ["torch.matmul", "torch.matmul", "torch.matmul.view", "torch.matmul.view", "multiheaded.MultiHeadedSimilarity._internal_similarity", "torch.matmul.size", "torch.matmul.size", "list", "list", "torch.matmul.size", "torch.matmul.size"], "methods", ["None"], ["", "@", "overrides", "\n", "def", "forward", "(", "self", ",", "tensor_1", ":", "torch", ".", "Tensor", ",", "tensor_2", ":", "torch", ".", "Tensor", ")", "->", "torch", ".", "Tensor", ":", "\n", "        ", "projected_tensor_1", "=", "torch", ".", "matmul", "(", "tensor_1", ",", "self", ".", "_tensor_1_projection", ")", "\n", "projected_tensor_2", "=", "torch", ".", "matmul", "(", "tensor_2", ",", "self", ".", "_tensor_2_projection", ")", "\n", "\n", "# Here we split the last dimension of the tensors from (..., projected_dim) to", "\n", "# (..., num_heads, projected_dim / num_heads), using tensor.view().", "\n", "last_dim_size", "=", "projected_tensor_1", ".", "size", "(", "-", "1", ")", "//", "self", ".", "num_heads", "\n", "new_shape", "=", "list", "(", "projected_tensor_1", ".", "size", "(", ")", ")", "[", ":", "-", "1", "]", "+", "[", "\n", "self", ".", "num_heads", ",", "\n", "last_dim_size", ",", "\n", "]", "\n", "split_tensor_1", "=", "projected_tensor_1", ".", "view", "(", "*", "new_shape", ")", "\n", "last_dim_size", "=", "projected_tensor_2", ".", "size", "(", "-", "1", ")", "//", "self", ".", "num_heads", "\n", "new_shape", "=", "list", "(", "projected_tensor_2", ".", "size", "(", ")", ")", "[", ":", "-", "1", "]", "+", "[", "\n", "self", ".", "num_heads", ",", "\n", "last_dim_size", ",", "\n", "]", "\n", "split_tensor_2", "=", "projected_tensor_2", ".", "view", "(", "*", "new_shape", ")", "\n", "\n", "# And then we pass this off to our internal similarity function.  Because the similarity", "\n", "# functions don't care what dimension their input has, and only look at the last dimension,", "\n", "# we don't need to do anything special here.  It will just compute similarity on the", "\n", "# projection dimension for each head, returning a tensor of shape (..., num_heads).", "\n", "return", "self", ".", "_internal_similarity", "(", "split_tensor_1", ",", "split_tensor_2", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.similarity_functions.linear.LinearSimilarity.__init__": [[46, 60], ["allennlp.modules.similarity_functions.similarity_function.SimilarityFunction.__init__", "allennlp.nn.util.get_combined_dim", "torch.nn.parameter.Parameter", "torch.nn.parameter.Parameter", "linear.LinearSimilarity.reset_parameters", "torch.Tensor", "torch.Tensor", "allennlp.nn.Activation.by_name"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.graph_util.plot.GraphPlot.__init__", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.nn.util.get_combined_dim", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.similarity_functions.linear.LinearSimilarity.reset_parameters", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.registrable.Registrable.by_name"], ["def", "__init__", "(", "\n", "self", ",", "\n", "tensor_1_dim", ":", "int", ",", "\n", "tensor_2_dim", ":", "int", ",", "\n", "combination", ":", "str", "=", "\"x,y\"", ",", "\n", "activation", ":", "Activation", "=", "None", ",", "\n", ")", "->", "None", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "_combination", "=", "combination", "\n", "combined_dim", "=", "util", ".", "get_combined_dim", "(", "combination", ",", "[", "tensor_1_dim", ",", "tensor_2_dim", "]", ")", "\n", "self", ".", "_weight_vector", "=", "Parameter", "(", "torch", ".", "Tensor", "(", "combined_dim", ")", ")", "\n", "self", ".", "_bias", "=", "Parameter", "(", "torch", ".", "Tensor", "(", "1", ")", ")", "\n", "self", ".", "_activation", "=", "activation", "or", "Activation", ".", "by_name", "(", "\"linear\"", ")", "(", ")", "\n", "self", ".", "reset_parameters", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.similarity_functions.linear.LinearSimilarity.reset_parameters": [[61, 65], ["math.sqrt", "linear.LinearSimilarity._weight_vector.data.uniform_", "linear.LinearSimilarity._bias.data.fill_", "linear.LinearSimilarity._weight_vector.size"], "methods", ["None"], ["", "def", "reset_parameters", "(", "self", ")", ":", "\n", "        ", "std", "=", "math", ".", "sqrt", "(", "6", "/", "(", "self", ".", "_weight_vector", ".", "size", "(", "0", ")", "+", "1", ")", ")", "\n", "self", ".", "_weight_vector", ".", "data", ".", "uniform_", "(", "-", "std", ",", "std", ")", "\n", "self", ".", "_bias", ".", "data", ".", "fill_", "(", "0", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.similarity_functions.linear.LinearSimilarity.forward": [[66, 71], ["allennlp.nn.util.combine_tensors", "torch.matmul", "linear.LinearSimilarity._activation"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.nn.util.combine_tensors"], ["", "@", "overrides", "\n", "def", "forward", "(", "self", ",", "tensor_1", ":", "torch", ".", "Tensor", ",", "tensor_2", ":", "torch", ".", "Tensor", ")", "->", "torch", ".", "Tensor", ":", "\n", "        ", "combined_tensors", "=", "util", ".", "combine_tensors", "(", "self", ".", "_combination", ",", "[", "tensor_1", ",", "tensor_2", "]", ")", "\n", "dot_product", "=", "torch", ".", "matmul", "(", "combined_tensors", ",", "self", ".", "_weight_vector", ")", "\n", "return", "self", ".", "_activation", "(", "dot_product", "+", "self", ".", "_bias", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.saliency_interpreters.saliency_interpreter.SaliencyInterpreter.__init__": [[12, 14], ["None"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "predictor", ":", "Predictor", ")", "->", "None", ":", "\n", "        ", "self", ".", "predictor", "=", "predictor", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.saliency_interpreters.saliency_interpreter.SaliencyInterpreter.saliency_interpret_from_json": [[15, 34], ["NotImplementedError"], "methods", ["None"], ["", "def", "saliency_interpret_from_json", "(", "self", ",", "inputs", ":", "JsonDict", ")", "->", "JsonDict", ":", "\n", "        ", "\"\"\"\n        This function finds a modification to the input text that would change the model's\n        prediction in some desired manner (e.g., an adversarial attack).\n\n        # Parameters\n\n        inputs : `JsonDict`\n            The input you want to interpret (the same as the argument to a Predictor, e.g., predict_json()).\n\n        # Returns\n\n        interpretation : `JsonDict`\n            Contains the normalized saliency values for each input token. The dict has entries for\n            each instance in the inputs JsonDict, e.g., `{instance_1: ..., instance_2:, ... }`.\n            Each one of those entries has entries for the saliency of the inputs, e.g.,\n            `{grad_input_1: ..., grad_input_2: ... }`.\n        \"\"\"", "\n", "raise", "NotImplementedError", "(", "\"Implement this for saliency interpretations\"", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.saliency_interpreters.smooth_gradient.SmoothGradient.__init__": [[22, 27], ["allennlp.interpret.saliency_interpreters.saliency_interpreter.SaliencyInterpreter.__init__"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.graph_util.plot.GraphPlot.__init__"], ["def", "__init__", "(", "self", ",", "predictor", ":", "Predictor", ")", "->", "None", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "predictor", ")", "\n", "# Hyperparameters", "\n", "self", ".", "stdev", "=", "0.01", "\n", "self", ".", "num_samples", "=", "10", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.saliency_interpreters.smooth_gradient.SmoothGradient.saliency_interpret_from_json": [[28, 51], ["smooth_gradient.SmoothGradient.predictor.json_to_labeled_instances", "dict", "enumerate", "allennlp.common.util.sanitize", "smooth_gradient.SmoothGradient._smooth_grads", "smooth_gradient.SmoothGradient.items", "numpy.sum", "numpy.linalg.norm", "math.fabs", "str"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.predictors.predictor.Predictor.json_to_labeled_instances", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.util.sanitize", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.saliency_interpreters.smooth_gradient.SmoothGradient._smooth_grads"], ["", "def", "saliency_interpret_from_json", "(", "self", ",", "inputs", ":", "JsonDict", ")", "->", "JsonDict", ":", "\n", "# Convert inputs to labeled instances", "\n", "        ", "labeled_instances", "=", "self", ".", "predictor", ".", "json_to_labeled_instances", "(", "inputs", ")", "\n", "\n", "instances_with_grads", "=", "dict", "(", ")", "\n", "for", "idx", ",", "instance", "in", "enumerate", "(", "labeled_instances", ")", ":", "\n", "# Run smoothgrad", "\n", "            ", "grads", "=", "self", ".", "_smooth_grads", "(", "instance", ")", "\n", "\n", "# Normalize results", "\n", "for", "key", ",", "grad", "in", "grads", ".", "items", "(", ")", ":", "\n", "# TODO (@Eric-Wallace), SmoothGrad is not using times input normalization.", "\n", "# Fine for now, but should fix for consistency.", "\n", "\n", "# The [0] here is undo-ing the batching that happens in get_gradients.", "\n", "                ", "embedding_grad", "=", "numpy", ".", "sum", "(", "grad", "[", "0", "]", ",", "axis", "=", "1", ")", "\n", "norm", "=", "numpy", ".", "linalg", ".", "norm", "(", "embedding_grad", ",", "ord", "=", "1", ")", "\n", "normalized_grad", "=", "[", "math", ".", "fabs", "(", "e", ")", "/", "norm", "for", "e", "in", "embedding_grad", "]", "\n", "grads", "[", "key", "]", "=", "normalized_grad", "\n", "\n", "", "instances_with_grads", "[", "\"instance_\"", "+", "str", "(", "idx", "+", "1", ")", "]", "=", "grads", "\n", "\n", "", "return", "sanitize", "(", "instances_with_grads", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.saliency_interpreters.smooth_gradient.SmoothGradient._register_forward_hook": [[52, 70], ["allennlp.nn.util.find_embedding_layer", "allennlp.nn.util.find_embedding_layer.register_forward_hook", "output.add_", "output.detach().max", "output.detach().min", "torch.randn().to", "output.detach", "output.detach", "torch.randn"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.nn.util.find_embedding_layer"], ["", "def", "_register_forward_hook", "(", "self", ",", "stdev", ":", "float", ")", ":", "\n", "        ", "\"\"\"\n        Register a forward hook on the embedding layer which adds random noise to every embedding.\n        Used for one term in the SmoothGrad sum.\n        \"\"\"", "\n", "\n", "def", "forward_hook", "(", "module", ",", "inputs", ",", "output", ")", ":", "\n", "# Random noise = N(0, stdev * (max-min))", "\n", "            ", "scale", "=", "output", ".", "detach", "(", ")", ".", "max", "(", ")", "-", "output", ".", "detach", "(", ")", ".", "min", "(", ")", "\n", "noise", "=", "torch", ".", "randn", "(", "output", ".", "shape", ")", ".", "to", "(", "output", ".", "device", ")", "*", "stdev", "*", "scale", "\n", "\n", "# Add the random noise", "\n", "output", ".", "add_", "(", "noise", ")", "\n", "\n", "# Register the hook", "\n", "", "embedding_layer", "=", "util", ".", "find_embedding_layer", "(", "self", ".", "predictor", ".", "_model", ")", "\n", "handle", "=", "embedding_layer", ".", "register_forward_hook", "(", "forward_hook", ")", "\n", "return", "handle", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.saliency_interpreters.smooth_gradient.SmoothGradient._smooth_grads": [[71, 90], ["range", "total_gradients.keys", "smooth_gradient.SmoothGradient._register_forward_hook", "smooth_gradient.SmoothGradient.remove", "smooth_gradient.SmoothGradient.predictor.get_gradients", "grads.keys"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.saliency_interpreters.simple_gradient.SimpleGradient._register_forward_hook", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.predictors.predictor.Predictor.get_gradients"], ["", "def", "_smooth_grads", "(", "self", ",", "instance", ":", "Instance", ")", "->", "Dict", "[", "str", ",", "numpy", ".", "ndarray", "]", ":", "\n", "        ", "total_gradients", ":", "Dict", "[", "str", ",", "Any", "]", "=", "{", "}", "\n", "for", "_", "in", "range", "(", "self", ".", "num_samples", ")", ":", "\n", "            ", "handle", "=", "self", ".", "_register_forward_hook", "(", "self", ".", "stdev", ")", "\n", "grads", "=", "self", ".", "predictor", ".", "get_gradients", "(", "[", "instance", "]", ")", "[", "0", "]", "\n", "handle", ".", "remove", "(", ")", "\n", "\n", "# Sum gradients", "\n", "if", "total_gradients", "==", "{", "}", ":", "\n", "                ", "total_gradients", "=", "grads", "\n", "", "else", ":", "\n", "                ", "for", "key", "in", "grads", ".", "keys", "(", ")", ":", "\n", "                    ", "total_gradients", "[", "key", "]", "+=", "grads", "[", "key", "]", "\n", "\n", "# Average the gradients", "\n", "", "", "", "for", "key", "in", "total_gradients", ".", "keys", "(", ")", ":", "\n", "            ", "total_gradients", "[", "key", "]", "/=", "self", ".", "num_samples", "\n", "\n", "", "return", "total_gradients", "\n", "", "", ""]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.saliency_interpreters.integrated_gradient.IntegratedGradient.saliency_interpret_from_json": [[20, 40], ["integrated_gradient.IntegratedGradient.predictor.json_to_labeled_instances", "dict", "enumerate", "allennlp.common.util.sanitize", "integrated_gradient.IntegratedGradient._integrate_gradients", "integrated_gradient.IntegratedGradient.items", "numpy.sum", "numpy.linalg.norm", "math.fabs", "str"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.predictors.predictor.Predictor.json_to_labeled_instances", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.util.sanitize", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.saliency_interpreters.integrated_gradient.IntegratedGradient._integrate_gradients"], ["def", "saliency_interpret_from_json", "(", "self", ",", "inputs", ":", "JsonDict", ")", "->", "JsonDict", ":", "\n", "# Convert inputs to labeled instances", "\n", "        ", "labeled_instances", "=", "self", ".", "predictor", ".", "json_to_labeled_instances", "(", "inputs", ")", "\n", "\n", "instances_with_grads", "=", "dict", "(", ")", "\n", "for", "idx", ",", "instance", "in", "enumerate", "(", "labeled_instances", ")", ":", "\n", "# Run integrated gradients", "\n", "            ", "grads", "=", "self", ".", "_integrate_gradients", "(", "instance", ")", "\n", "\n", "# Normalize results", "\n", "for", "key", ",", "grad", "in", "grads", ".", "items", "(", ")", ":", "\n", "# The [0] here is undo-ing the batching that happens in get_gradients.", "\n", "                ", "embedding_grad", "=", "numpy", ".", "sum", "(", "grad", "[", "0", "]", ",", "axis", "=", "1", ")", "\n", "norm", "=", "numpy", ".", "linalg", ".", "norm", "(", "embedding_grad", ",", "ord", "=", "1", ")", "\n", "normalized_grad", "=", "[", "math", ".", "fabs", "(", "e", ")", "/", "norm", "for", "e", "in", "embedding_grad", "]", "\n", "grads", "[", "key", "]", "=", "normalized_grad", "\n", "\n", "", "instances_with_grads", "[", "\"instance_\"", "+", "str", "(", "idx", "+", "1", ")", "]", "=", "grads", "\n", "\n", "", "return", "sanitize", "(", "instances_with_grads", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.saliency_interpreters.integrated_gradient.IntegratedGradient._register_forward_hook": [[41, 62], ["allennlp.nn.util.find_embedding_layer", "allennlp.nn.util.find_embedding_layer.register_forward_hook", "output.mul_", "embeddings_list.append", "output.squeeze().clone().detach().numpy", "output.squeeze().clone().detach", "output.squeeze().clone", "output.squeeze"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.nn.util.find_embedding_layer", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.nn.util.clone"], ["", "def", "_register_forward_hook", "(", "self", ",", "alpha", ":", "int", ",", "embeddings_list", ":", "List", ")", ":", "\n", "        ", "\"\"\"\n        Register a forward hook on the embedding layer which scales the embeddings by alpha. Used\n        for one term in the Integrated Gradients sum.\n\n        We store the embedding output into the embeddings_list when alpha is zero.  This is used\n        later to element-wise multiply the input by the averaged gradients.\n        \"\"\"", "\n", "\n", "def", "forward_hook", "(", "module", ",", "inputs", ",", "output", ")", ":", "\n", "# Save the input for later use. Only do so on first call.", "\n", "            ", "if", "alpha", "==", "0", ":", "\n", "                ", "embeddings_list", ".", "append", "(", "output", ".", "squeeze", "(", "0", ")", ".", "clone", "(", ")", ".", "detach", "(", ")", ".", "numpy", "(", ")", ")", "\n", "\n", "# Scale the embedding by alpha", "\n", "", "output", ".", "mul_", "(", "alpha", ")", "\n", "\n", "# Register the hook", "\n", "", "embedding_layer", "=", "util", ".", "find_embedding_layer", "(", "self", ".", "predictor", ".", "_model", ")", "\n", "handle", "=", "embedding_layer", ".", "register_forward_hook", "(", "forward_hook", ")", "\n", "return", "handle", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.saliency_interpreters.integrated_gradient.IntegratedGradient._integrate_gradients": [[63, 103], ["numpy.linspace", "ig_grads.keys", "embeddings_list.reverse", "enumerate", "integrated_gradient.IntegratedGradient._register_forward_hook", "integrated_gradient.IntegratedGradient.remove", "integrated_gradient.IntegratedGradient.predictor.get_gradients", "grads.keys", "str"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.saliency_interpreters.simple_gradient.SimpleGradient._register_forward_hook", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.predictors.predictor.Predictor.get_gradients"], ["", "def", "_integrate_gradients", "(", "self", ",", "instance", ":", "Instance", ")", "->", "Dict", "[", "str", ",", "numpy", ".", "ndarray", "]", ":", "\n", "        ", "\"\"\"\n        Returns integrated gradients for the given [`Instance`](../../data/instance.md)\n        \"\"\"", "\n", "ig_grads", ":", "Dict", "[", "str", ",", "Any", "]", "=", "{", "}", "\n", "\n", "# List of Embedding inputs", "\n", "embeddings_list", ":", "List", "[", "numpy", ".", "ndarray", "]", "=", "[", "]", "\n", "\n", "# Use 10 terms in the summation approximation of the integral in integrated grad", "\n", "steps", "=", "10", "\n", "\n", "# Exclude the endpoint because we do a left point integral approximation", "\n", "for", "alpha", "in", "numpy", ".", "linspace", "(", "0", ",", "1.0", ",", "num", "=", "steps", ",", "endpoint", "=", "False", ")", ":", "\n", "# Hook for modifying embedding value", "\n", "            ", "handle", "=", "self", ".", "_register_forward_hook", "(", "alpha", ",", "embeddings_list", ")", "\n", "\n", "grads", "=", "self", ".", "predictor", ".", "get_gradients", "(", "[", "instance", "]", ")", "[", "0", "]", "\n", "handle", ".", "remove", "(", ")", "\n", "\n", "# Running sum of gradients", "\n", "if", "ig_grads", "==", "{", "}", ":", "\n", "                ", "ig_grads", "=", "grads", "\n", "", "else", ":", "\n", "                ", "for", "key", "in", "grads", ".", "keys", "(", ")", ":", "\n", "                    ", "ig_grads", "[", "key", "]", "+=", "grads", "[", "key", "]", "\n", "\n", "# Average of each gradient term", "\n", "", "", "", "for", "key", "in", "ig_grads", ".", "keys", "(", ")", ":", "\n", "            ", "ig_grads", "[", "key", "]", "/=", "steps", "\n", "\n", "# Gradients come back in the reverse order that they were sent into the network", "\n", "", "embeddings_list", ".", "reverse", "(", ")", "\n", "\n", "# Element-wise multiply average gradient by the input", "\n", "for", "idx", ",", "input_embedding", "in", "enumerate", "(", "embeddings_list", ")", ":", "\n", "            ", "key", "=", "\"grad_input_\"", "+", "str", "(", "idx", "+", "1", ")", "\n", "ig_grads", "[", "key", "]", "*=", "input_embedding", "\n", "\n", "", "return", "ig_grads", "\n", "", "", ""]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.saliency_interpreters.simple_gradient.SimpleGradient.saliency_interpret_from_json": [[15, 48], ["simple_gradient.SimpleGradient.predictor.json_to_labeled_instances", "dict", "enumerate", "allennlp.common.util.sanitize", "simple_gradient.SimpleGradient._register_forward_hook", "simple_gradient.SimpleGradient.remove", "embeddings_list.reverse", "grads.items", "simple_gradient.SimpleGradient.predictor.get_gradients", "numpy.sum", "numpy.linalg.norm", "int", "math.fabs", "str"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.predictors.predictor.Predictor.json_to_labeled_instances", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.util.sanitize", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.saliency_interpreters.simple_gradient.SimpleGradient._register_forward_hook", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.predictors.predictor.Predictor.get_gradients"], ["    ", "def", "saliency_interpret_from_json", "(", "self", ",", "inputs", ":", "JsonDict", ")", "->", "JsonDict", ":", "\n", "        ", "\"\"\"\n        Interprets the model's prediction for inputs.  Gets the gradients of the loss with respect\n        to the input and returns those gradients normalized and sanitized.\n        \"\"\"", "\n", "labeled_instances", "=", "self", ".", "predictor", ".", "json_to_labeled_instances", "(", "inputs", ")", "\n", "\n", "# List of embedding inputs, used for multiplying gradient by the input for normalization", "\n", "embeddings_list", ":", "List", "[", "numpy", ".", "ndarray", "]", "=", "[", "]", "\n", "\n", "instances_with_grads", "=", "dict", "(", ")", "\n", "for", "idx", ",", "instance", "in", "enumerate", "(", "labeled_instances", ")", ":", "\n", "# Hook used for saving embeddings", "\n", "            ", "handle", "=", "self", ".", "_register_forward_hook", "(", "embeddings_list", ")", "\n", "grads", "=", "self", ".", "predictor", ".", "get_gradients", "(", "[", "instance", "]", ")", "[", "0", "]", "\n", "handle", ".", "remove", "(", ")", "\n", "\n", "# Gradients come back in the reverse order that they were sent into the network", "\n", "embeddings_list", ".", "reverse", "(", ")", "\n", "for", "key", ",", "grad", "in", "grads", ".", "items", "(", ")", ":", "\n", "# Get number at the end of every gradient key (they look like grad_input_[int],", "\n", "# we're getting this [int] part and subtracting 1 for zero-based indexing).", "\n", "# This is then used as an index into the reversed input array to match up the", "\n", "# gradient and its respective embedding.", "\n", "                ", "input_idx", "=", "int", "(", "key", "[", "-", "1", "]", ")", "-", "1", "\n", "# The [0] here is undo-ing the batching that happens in get_gradients.", "\n", "emb_grad", "=", "numpy", ".", "sum", "(", "grad", "[", "0", "]", "*", "embeddings_list", "[", "input_idx", "]", ",", "axis", "=", "1", ")", "\n", "norm", "=", "numpy", ".", "linalg", ".", "norm", "(", "emb_grad", ",", "ord", "=", "1", ")", "\n", "normalized_grad", "=", "[", "math", ".", "fabs", "(", "e", ")", "/", "norm", "for", "e", "in", "emb_grad", "]", "\n", "grads", "[", "key", "]", "=", "normalized_grad", "\n", "\n", "", "instances_with_grads", "[", "\"instance_\"", "+", "str", "(", "idx", "+", "1", ")", "]", "=", "grads", "\n", "", "return", "sanitize", "(", "instances_with_grads", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.saliency_interpreters.simple_gradient.SimpleGradient._register_forward_hook": [[49, 63], ["allennlp.nn.util.find_embedding_layer", "allennlp.nn.util.find_embedding_layer.register_forward_hook", "embeddings_list.append", "output.squeeze().clone().detach().numpy", "output.squeeze().clone().detach", "output.squeeze().clone", "output.squeeze"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.nn.util.find_embedding_layer", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.nn.util.clone"], ["", "def", "_register_forward_hook", "(", "self", ",", "embeddings_list", ":", "List", ")", ":", "\n", "        ", "\"\"\"\n        Finds all of the TextFieldEmbedders, and registers a forward hook onto them. When forward()\n        is called, embeddings_list is filled with the embedding values. This is necessary because\n        our normalization scheme multiplies the gradient by the embedding value.\n        \"\"\"", "\n", "\n", "def", "forward_hook", "(", "module", ",", "inputs", ",", "output", ")", ":", "\n", "            ", "embeddings_list", ".", "append", "(", "output", ".", "squeeze", "(", "0", ")", ".", "clone", "(", ")", ".", "detach", "(", ")", ".", "numpy", "(", ")", ")", "\n", "\n", "", "embedding_layer", "=", "util", ".", "find_embedding_layer", "(", "self", ".", "predictor", ".", "_model", ")", "\n", "handle", "=", "embedding_layer", ".", "register_forward_hook", "(", "forward_hook", ")", "\n", "\n", "return", "handle", "\n", "", "", ""]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.attackers.attacker.Attacker.__init__": [[14, 16], ["None"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "predictor", ":", "Predictor", ")", "->", "None", ":", "\n", "        ", "self", ".", "predictor", "=", "predictor", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.attackers.attacker.Attacker.initialize": [[17, 23], ["None"], "methods", ["None"], ["", "def", "initialize", "(", "self", ")", ":", "\n", "        ", "\"\"\"\n        Initializes any components of the Attacker that are expensive to compute, so that they are\n        not created on __init__().  Default implementation is `pass`.\n        \"\"\"", "\n", "pass", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.attackers.attacker.Attacker.attack_from_json": [[24, 60], ["NotImplementedError"], "methods", ["None"], ["", "def", "attack_from_json", "(", "\n", "self", ",", "\n", "inputs", ":", "JsonDict", ",", "\n", "input_field_to_attack", ":", "str", ",", "\n", "grad_input_field", ":", "str", ",", "\n", "ignore_tokens", ":", "List", "[", "str", "]", ",", "\n", "target", ":", "JsonDict", ",", "\n", ")", "->", "JsonDict", ":", "\n", "        ", "\"\"\"\n        This function finds a modification to the input text that would change the model's\n        prediction in some desired manner (e.g., an adversarial attack).\n\n        # Parameters\n\n        inputs : `JsonDict`\n            The input you want to attack (the same as the argument to a Predictor, e.g.,\n            predict_json()).\n        input_field_to_attack : `str`\n            The key in the inputs JsonDict you want to attack, e.g., `tokens`.\n        grad_input_field : `str`\n            The field in the gradients dictionary that contains the input gradients.  For example,\n            `grad_input_1` will be the field for single input tasks. See get_gradients() in\n            `Predictor` for more information on field names.\n        target : `JsonDict`\n            If given, this is a `targeted` attack, trying to change the prediction to a particular\n            value, instead of just changing it from its original prediction.  Subclasses are not\n            required to accept this argument, as not all attacks make sense as targeted attacks.\n            Perhaps that means we should make the API more crisp, but adding another class is not\n            worth it.\n\n        # Returns\n\n        reduced_input : `JsonDict`\n            Contains the final, sanitized input after adversarial modification.\n        \"\"\"", "\n", "raise", "NotImplementedError", "(", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.attackers.input_reduction.InputReduction.__init__": [[28, 31], ["allennlp.interpret.attackers.attacker.Attacker.__init__"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.graph_util.plot.GraphPlot.__init__"], ["def", "__init__", "(", "self", ",", "predictor", ":", "Predictor", ",", "beam_size", ":", "int", "=", "3", ")", "->", "None", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "predictor", ")", "\n", "self", ".", "beam_size", "=", "beam_size", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.attackers.input_reduction.InputReduction.attack_from_json": [[32, 60], ["input_reduction.InputReduction.predictor.json_to_labeled_instances", "copy.deepcopy", "allennlp.common.util.sanitize", "ValueError", "final_tokens.append", "input_reduction.InputReduction._attack_instance"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.predictors.predictor.Predictor.json_to_labeled_instances", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.util.sanitize", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.attackers.input_reduction.InputReduction._attack_instance"], ["", "def", "attack_from_json", "(", "\n", "self", ",", "\n", "inputs", ":", "JsonDict", "=", "None", ",", "\n", "input_field_to_attack", ":", "str", "=", "\"tokens\"", ",", "\n", "grad_input_field", ":", "str", "=", "\"grad_input_1\"", ",", "\n", "ignore_tokens", ":", "List", "[", "str", "]", "=", "None", ",", "\n", "target", ":", "JsonDict", "=", "None", ",", "\n", ")", ":", "\n", "        ", "if", "target", "is", "not", "None", ":", "\n", "            ", "raise", "ValueError", "(", "\"Input reduction does not implement targeted attacks\"", ")", "\n", "", "ignore_tokens", "=", "[", "\"@@NULL@@\"", "]", "if", "ignore_tokens", "is", "None", "else", "ignore_tokens", "\n", "original_instances", "=", "self", ".", "predictor", ".", "json_to_labeled_instances", "(", "inputs", ")", "\n", "original_text_field", ":", "TextField", "=", "original_instances", "[", "0", "]", "[", "# type: ignore", "\n", "input_field_to_attack", "\n", "]", "\n", "original_tokens", "=", "deepcopy", "(", "original_text_field", ".", "tokens", ")", "\n", "final_tokens", "=", "[", "]", "\n", "for", "instance", "in", "original_instances", ":", "\n", "            ", "final_tokens", ".", "append", "(", "\n", "self", ".", "_attack_instance", "(", "\n", "inputs", ",", "\n", "instance", ",", "\n", "input_field_to_attack", ",", "\n", "grad_input_field", ",", "\n", "ignore_tokens", ",", "\n", ")", "\n", ")", "\n", "", "return", "sanitize", "(", "{", "\"final\"", ":", "final_tokens", ",", "\"original\"", ":", "original_tokens", "}", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.attackers.input_reduction.InputReduction._attack_instance": [[61, 150], ["allennlp.interpret.attackers.utils.get_fields_to_compare", "copy.deepcopy", "input_reduction._get_ner_tags_and_mask", "heapq.nsmallest", "copy.deepcopy", "len", "len", "copy.deepcopy", "input_reduction.InputReduction.predictor.get_gradients", "copy.deepcopy", "input_reduction._remove_one_token", "heapq.nsmallest.extend", "isinstance", "allennlp.interpret.attackers.utils.instance_has_changed", "input_reduction.InputReduction._attack_instance.get_length"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.attackers.utils.get_fields_to_compare", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.attackers.input_reduction._get_ner_tags_and_mask", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.predictors.predictor.Predictor.get_gradients", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.attackers.input_reduction._remove_one_token", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.attackers.utils.instance_has_changed"], ["", "def", "_attack_instance", "(", "\n", "self", ",", "\n", "inputs", ":", "JsonDict", ",", "\n", "instance", ":", "Instance", ",", "\n", "input_field_to_attack", ":", "str", ",", "\n", "grad_input_field", ":", "str", ",", "\n", "ignore_tokens", ":", "List", "[", "str", "]", ",", "\n", ")", ":", "\n", "# Save fields that must be checked for equality", "\n", "        ", "fields_to_compare", "=", "utils", ".", "get_fields_to_compare", "(", "\n", "inputs", ",", "instance", ",", "input_field_to_attack", "\n", ")", "\n", "\n", "# Set num_ignore_tokens, which tells input reduction when to stop", "\n", "# We keep at least one token for input reduction on classification/entailment/etc.", "\n", "if", "\"tags\"", "not", "in", "instance", ":", "\n", "            ", "num_ignore_tokens", "=", "1", "\n", "tag_mask", "=", "None", "\n", "\n", "# Set num_ignore_tokens for NER and build token mask", "\n", "", "else", ":", "\n", "            ", "num_ignore_tokens", ",", "tag_mask", ",", "original_tags", "=", "_get_ner_tags_and_mask", "(", "\n", "instance", ",", "input_field_to_attack", ",", "ignore_tokens", "\n", ")", "\n", "\n", "", "text_field", ":", "TextField", "=", "instance", "[", "input_field_to_attack", "]", "# type: ignore", "\n", "current_tokens", "=", "deepcopy", "(", "text_field", ".", "tokens", ")", "\n", "candidates", "=", "[", "(", "instance", ",", "-", "1", ",", "tag_mask", ")", "]", "\n", "# keep removing tokens until prediction is about to change", "\n", "while", "len", "(", "current_tokens", ")", ">", "num_ignore_tokens", "and", "candidates", ":", "\n", "# sort current candidates by smallest length (we want to remove as many tokens as possible)", "\n", "            ", "def", "get_length", "(", "input_instance", ":", "Instance", ")", ":", "\n", "                ", "input_text_field", ":", "TextField", "=", "input_instance", "[", "input_field_to_attack", "]", "# type: ignore", "\n", "return", "len", "(", "input_text_field", ".", "tokens", ")", "\n", "\n", "", "candidates", "=", "heapq", ".", "nsmallest", "(", "\n", "self", ".", "beam_size", ",", "candidates", ",", "key", "=", "lambda", "x", ":", "get_length", "(", "x", "[", "0", "]", ")", "\n", ")", "\n", "\n", "beam_candidates", "=", "deepcopy", "(", "candidates", ")", "\n", "candidates", "=", "[", "]", "\n", "for", "beam_instance", ",", "smallest_idx", ",", "tag_mask", "in", "beam_candidates", ":", "\n", "# get gradients and predictions", "\n", "                ", "beam_tag_mask", "=", "deepcopy", "(", "tag_mask", ")", "\n", "grads", ",", "outputs", "=", "self", ".", "predictor", ".", "get_gradients", "(", "[", "beam_instance", "]", ")", "\n", "\n", "for", "output", "in", "outputs", ":", "\n", "                    ", "if", "isinstance", "(", "outputs", "[", "output", "]", ",", "torch", ".", "Tensor", ")", ":", "\n", "                        ", "outputs", "[", "output", "]", "=", "(", "\n", "outputs", "[", "output", "]", ".", "detach", "(", ")", ".", "cpu", "(", ")", ".", "numpy", "(", ")", ".", "squeeze", "(", ")", ".", "squeeze", "(", ")", "\n", ")", "\n", "", "elif", "isinstance", "(", "outputs", "[", "output", "]", ",", "list", ")", ":", "\n", "                        ", "outputs", "[", "output", "]", "=", "outputs", "[", "output", "]", "[", "0", "]", "\n", "\n", "# Check if any fields have changed, if so, next beam", "\n", "", "", "if", "\"tags\"", "not", "in", "instance", ":", "\n", "# relabel beam_instance since last iteration removed an input token", "\n", "                    ", "beam_instance", "=", "self", ".", "predictor", ".", "predictions_to_labeled_instances", "(", "\n", "beam_instance", ",", "outputs", "\n", ")", "[", "0", "]", "\n", "if", "utils", ".", "instance_has_changed", "(", "beam_instance", ",", "fields_to_compare", ")", ":", "\n", "                        ", "continue", "\n", "\n", "# special case for sentence tagging (we have tested NER)", "\n", "", "", "else", ":", "\n", "# remove the mask where you remove the input token from.", "\n", "                    ", "if", "smallest_idx", "!=", "-", "1", ":", "# Don't delete on the very first iteration", "\n", "                        ", "del", "beam_tag_mask", "[", "smallest_idx", "]", "\n", "", "cur_tags", "=", "[", "\n", "outputs", "[", "\"tags\"", "]", "[", "x", "]", "\n", "for", "x", "in", "range", "(", "len", "(", "outputs", "[", "\"tags\"", "]", ")", ")", "\n", "if", "beam_tag_mask", "[", "x", "]", "\n", "]", "\n", "if", "cur_tags", "!=", "original_tags", ":", "\n", "                        ", "continue", "\n", "\n", "# remove a token from the input", "\n", "", "", "text_field", ":", "TextField", "=", "beam_instance", "[", "input_field_to_attack", "]", "# type: ignore", "\n", "current_tokens", "=", "deepcopy", "(", "text_field", ".", "tokens", ")", "\n", "reduced_instances_and_smallest", "=", "_remove_one_token", "(", "\n", "beam_instance", ",", "\n", "input_field_to_attack", ",", "\n", "grads", "[", "grad_input_field", "]", "[", "0", "]", ",", "\n", "ignore_tokens", ",", "\n", "self", ".", "beam_size", ",", "\n", "beam_tag_mask", ",", "\n", ")", "\n", "candidates", ".", "extend", "(", "reduced_instances_and_smallest", ")", "\n", "", "", "return", "current_tokens", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.attackers.input_reduction._remove_one_token": [[152, 207], ["enumerate", "range", "numpy.sqrt", "enumerate", "copy.deepcopy", "numpy.argmin", "float", "reduced_instances_and_smallest.append", "grad.dot", "float", "float", "float"], "function", ["None"], ["", "", "def", "_remove_one_token", "(", "\n", "instance", ":", "Instance", ",", "\n", "input_field_to_attack", ":", "str", ",", "\n", "grads", ":", "np", ".", "ndarray", ",", "\n", "ignore_tokens", ":", "List", "[", "str", "]", ",", "\n", "beam_size", ":", "int", ",", "\n", "tag_mask", ":", "List", "[", "int", "]", ",", "\n", ")", "->", "List", "[", "Tuple", "[", "Instance", ",", "int", ",", "List", "[", "int", "]", "]", "]", ":", "\n", "    ", "\"\"\"\n    Finds the token with the smallest gradient and removes it.\n    \"\"\"", "\n", "# Compute L2 norm of all grads.", "\n", "grads_mag", "=", "[", "np", ".", "sqrt", "(", "grad", ".", "dot", "(", "grad", ")", ")", "for", "grad", "in", "grads", "]", "\n", "\n", "# Skip all ignore_tokens by setting grad to infinity", "\n", "text_field", ":", "TextField", "=", "instance", "[", "input_field_to_attack", "]", "# type: ignore", "\n", "for", "token_idx", ",", "token", "in", "enumerate", "(", "text_field", ".", "tokens", ")", ":", "\n", "        ", "if", "token", "in", "ignore_tokens", ":", "\n", "            ", "grads_mag", "[", "token_idx", "]", "=", "float", "(", "\"inf\"", ")", "\n", "\n", "# For NER, skip all tokens that are not in outside", "\n", "", "", "if", "\"tags\"", "in", "instance", ":", "\n", "        ", "tag_field", ":", "SequenceLabelField", "=", "instance", "[", "\"tags\"", "]", "# type: ignore", "\n", "labels", ":", "List", "[", "str", "]", "=", "tag_field", ".", "labels", "# type: ignore", "\n", "for", "idx", ",", "label", "in", "enumerate", "(", "labels", ")", ":", "\n", "            ", "if", "label", "!=", "\"O\"", ":", "\n", "                ", "grads_mag", "[", "idx", "]", "=", "float", "(", "\"inf\"", ")", "\n", "", "", "", "reduced_instances_and_smallest", ":", "List", "[", "Tuple", "[", "Instance", ",", "int", ",", "List", "[", "int", "]", "]", "]", "=", "[", "]", "\n", "for", "_", "in", "range", "(", "beam_size", ")", ":", "\n", "# copy instance and edit later", "\n", "        ", "copied_instance", "=", "deepcopy", "(", "instance", ")", "\n", "copied_text_field", ":", "TextField", "=", "copied_instance", "[", "input_field_to_attack", "]", "# type: ignore", "\n", "\n", "# find smallest", "\n", "smallest", "=", "np", ".", "argmin", "(", "grads_mag", ")", "\n", "if", "grads_mag", "[", "smallest", "]", "==", "float", "(", "\"inf\"", ")", ":", "# if all are ignored tokens, return.", "\n", "            ", "break", "\n", "", "grads_mag", "[", "smallest", "]", "=", "float", "(", "\"inf\"", ")", "# so the other beams don't use this token", "\n", "\n", "# remove smallest", "\n", "inputs_before_smallest", "=", "copied_text_field", ".", "tokens", "[", "0", ":", "smallest", "]", "\n", "inputs_after_smallest", "=", "copied_text_field", ".", "tokens", "[", "smallest", "+", "1", ":", "]", "\n", "copied_text_field", ".", "tokens", "=", "inputs_before_smallest", "+", "inputs_after_smallest", "\n", "\n", "if", "\"tags\"", "in", "instance", ":", "\n", "            ", "tag_field", ":", "SequenceLabelField", "=", "copied_instance", "[", "\"tags\"", "]", "# type: ignore", "\n", "tag_field_before_smallest", "=", "tag_field", ".", "labels", "[", "0", ":", "smallest", "]", "\n", "tag_field_after_smallest", "=", "tag_field", ".", "labels", "[", "smallest", "+", "1", ":", "]", "\n", "tag_field", ".", "labels", "=", "tag_field_before_smallest", "+", "tag_field_after_smallest", "# type: ignore", "\n", "tag_field", ".", "sequence_field", "=", "copied_text_field", "\n", "\n", "", "copied_instance", ".", "indexed", "=", "False", "\n", "reduced_instances_and_smallest", ".", "append", "(", "(", "copied_instance", ",", "smallest", ",", "tag_mask", ")", ")", "\n", "\n", "", "return", "reduced_instances_and_smallest", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.attackers.input_reduction._get_ner_tags_and_mask": [[209, 235], ["str", "tag_mask.append", "original_tags.append", "tag_mask.append"], "function", ["None"], ["", "def", "_get_ner_tags_and_mask", "(", "\n", "instance", ":", "Instance", ",", "input_field_to_attack", ":", "str", ",", "ignore_tokens", ":", "List", "[", "str", "]", "\n", ")", ":", "\n", "    ", "\"\"\"\n    Used for the NER task. Sets the num_ignore tokens, saves the original predicted tag and a 0/1\n    mask in the position of the tags\n    \"\"\"", "\n", "# Set num_ignore_tokens", "\n", "num_ignore_tokens", "=", "0", "\n", "input_field", ":", "TextField", "=", "instance", "[", "input_field_to_attack", "]", "# type: ignore", "\n", "for", "token", "in", "input_field", ".", "tokens", ":", "\n", "        ", "if", "str", "(", "token", ")", "in", "ignore_tokens", ":", "\n", "            ", "num_ignore_tokens", "+=", "1", "\n", "\n", "# save the original tags and a 0/1 mask where the tags are", "\n", "", "", "tag_mask", "=", "[", "]", "\n", "original_tags", "=", "[", "]", "\n", "tag_field", ":", "SequenceLabelField", "=", "instance", "[", "\"tags\"", "]", "# type: ignore", "\n", "for", "label", "in", "tag_field", ".", "labels", ":", "\n", "        ", "if", "label", "!=", "\"O\"", ":", "\n", "            ", "tag_mask", ".", "append", "(", "1", ")", "\n", "original_tags", ".", "append", "(", "label", ")", "\n", "num_ignore_tokens", "+=", "1", "\n", "", "else", ":", "\n", "            ", "tag_mask", ".", "append", "(", "0", ")", "\n", "", "", "return", "num_ignore_tokens", ",", "tag_mask", ",", "original_tags", "\n", "", ""]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.attackers.utils.get_fields_to_compare": [[5, 38], ["None"], "function", ["None"], ["def", "get_fields_to_compare", "(", "\n", "inputs", ":", "JsonDict", ",", "instance", ":", "Instance", ",", "input_field_to_attack", ":", "str", "\n", ")", "->", "JsonDict", ":", "\n", "    ", "\"\"\"\n    Gets a list of the fields that should be checked for equality after an attack is performed.\n\n    # Parameters\n\n    inputs : `JsonDict`\n        The input you want to attack, similar to the argument to a Predictor, e.g., predict_json().\n    instance : `Instance`\n        A labeled instance that is output from json_to_labeled_instances().\n    input_field_to_attack : `str`\n        The key in the inputs JsonDict you want to attack, e.g., tokens.\n\n    # Returns\n\n    fields : `JsonDict`\n        The fields that must be compared for equality.\n    \"\"\"", "\n", "# TODO(mattg): this really should live on the Predictor.  We have some messy stuff for, e.g.,", "\n", "# reading comprehension models, and the interpret code can't really know about the internals of", "\n", "# that (or at least it shouldn't now, and once we split out the reading comprehension repo, it", "\n", "# really *can't*).", "\n", "fields_to_compare", "=", "{", "\n", "key", ":", "instance", "[", "key", "]", "\n", "for", "key", "in", "instance", ".", "fields", "\n", "if", "key", "not", "in", "inputs", "\n", "and", "key", "!=", "input_field_to_attack", "\n", "and", "key", "!=", "\"metadata\"", "\n", "and", "key", "!=", "\"output\"", "\n", "}", "\n", "return", "fields_to_compare", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.attackers.utils.instance_has_changed": [[40, 50], ["any", "set", "set", "tuple", "tuple"], "function", ["None"], ["", "def", "instance_has_changed", "(", "instance", ":", "Instance", ",", "fields_to_compare", ":", "JsonDict", ")", ":", "\n", "    ", "if", "\"clusters\"", "in", "fields_to_compare", ":", "\n", "# Coref needs a special case here, apparently.  I (mattg) am not sure why the check below", "\n", "# doesn't catch this case; TODO: look into this.", "\n", "        ", "original_clusters", "=", "set", "(", "tuple", "(", "l", ")", "for", "l", "in", "fields_to_compare", "[", "\"clusters\"", "]", ")", "\n", "new_clusters", "=", "set", "(", "tuple", "(", "l", ")", "for", "l", "in", "instance", "[", "\"clusters\"", "]", ")", "# type: ignore", "\n", "return", "original_clusters", "!=", "new_clusters", "\n", "", "if", "any", "(", "instance", "[", "field", "]", "!=", "fields_to_compare", "[", "field", "]", "for", "field", "in", "fields_to_compare", ")", ":", "\n", "        ", "return", "True", "\n", "", "return", "False", "\n", "", ""]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.attackers.hotflip.Hotflip.__init__": [[67, 86], ["allennlp.interpret.attackers.attacker.Attacker.__init__", "[].isalnum", "hotflip.Hotflip.invalid_replacement_indices.append"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.graph_util.plot.GraphPlot.__init__"], ["def", "__init__", "(", "\n", "self", ",", "\n", "predictor", ":", "Predictor", ",", "\n", "vocab_namespace", ":", "str", "=", "\"tokens\"", ",", "\n", "max_tokens", ":", "int", "=", "5000", ",", "\n", ")", "->", "None", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "predictor", ")", "\n", "self", ".", "vocab", "=", "self", ".", "predictor", ".", "_model", ".", "vocab", "\n", "self", ".", "namespace", "=", "vocab_namespace", "\n", "# Force new tokens to be alphanumeric", "\n", "self", ".", "max_tokens", "=", "max_tokens", "\n", "self", ".", "invalid_replacement_indices", ":", "List", "[", "int", "]", "=", "[", "]", "\n", "for", "i", "in", "self", ".", "vocab", ".", "_index_to_token", "[", "self", ".", "namespace", "]", ":", "\n", "            ", "if", "not", "self", ".", "vocab", ".", "_index_to_token", "[", "self", ".", "namespace", "]", "[", "i", "]", ".", "isalnum", "(", ")", ":", "\n", "                ", "self", ".", "invalid_replacement_indices", ".", "append", "(", "i", ")", "\n", "", "", "self", ".", "embedding_matrix", ":", "torch", ".", "Tensor", "=", "None", "\n", "self", ".", "embedding_layer", ":", "torch", ".", "nn", ".", "Module", "=", "None", "\n", "# get device number", "\n", "self", ".", "cuda_device", "=", "predictor", ".", "cuda_device", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.attackers.hotflip.Hotflip.initialize": [[87, 95], ["hotflip.Hotflip._construct_embedding_matrix"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.attackers.hotflip.Hotflip._construct_embedding_matrix"], ["", "def", "initialize", "(", "self", ")", ":", "\n", "        ", "\"\"\"\n        Call this function before running attack_from_json(). We put the call to\n        `_construct_embedding_matrix()` in this function to prevent a large amount of compute\n        being done when __init__() is called.\n        \"\"\"", "\n", "if", "self", ".", "embedding_matrix", "is", "None", ":", "\n", "            ", "self", ".", "embedding_matrix", "=", "self", ".", "_construct_embedding_matrix", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.attackers.hotflip.Hotflip._construct_embedding_matrix": [[96, 128], ["allennlp.nn.util.find_embedding_layer", "isinstance", "hotflip.Hotflip.vocab.get_token_index", "hotflip.Hotflip._make_embedder_input", "allennlp.nn.util.find_embedding_layer.squeeze", "list", "allennlp.nn.util.find_embedding_layer."], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.nn.util.find_embedding_layer", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.data.vocabulary.Vocabulary.get_token_index", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.attackers.hotflip.Hotflip._make_embedder_input"], ["", "", "def", "_construct_embedding_matrix", "(", "self", ")", "->", "Embedding", ":", "\n", "        ", "\"\"\"\n        For HotFlip, we need a word embedding matrix to search over. The below is necessary for\n        models such as ELMo, character-level models, or for models that use a projection layer\n        after their word embeddings.\n\n        We run all of the tokens from the vocabulary through the TextFieldEmbedder, and save the\n        final output embedding. We then group all of those output embeddings into an \"embedding\n        matrix\".\n        \"\"\"", "\n", "embedding_layer", "=", "util", ".", "find_embedding_layer", "(", "self", ".", "predictor", ".", "_model", ")", "\n", "self", ".", "embedding_layer", "=", "embedding_layer", "\n", "if", "isinstance", "(", "embedding_layer", ",", "(", "Embedding", ",", "torch", ".", "nn", ".", "modules", ".", "sparse", ".", "Embedding", ")", ")", ":", "\n", "# If we're using something that already has an only embedding matrix, we can just use", "\n", "# that and bypass this method.", "\n", "            ", "return", "embedding_layer", ".", "weight", "\n", "\n", "# We take the top `self.max_tokens` as candidates for hotflip.  Because we have to", "\n", "# construct a new vector for each of these, we can't always afford to use the whole vocab,", "\n", "# for both runtime and memory considerations.", "\n", "", "all_tokens", "=", "list", "(", "self", ".", "vocab", ".", "_token_to_index", "[", "self", ".", "namespace", "]", ")", "[", ":", "self", ".", "max_tokens", "]", "\n", "max_index", "=", "self", ".", "vocab", ".", "get_token_index", "(", "all_tokens", "[", "-", "1", "]", ",", "self", ".", "namespace", ")", "\n", "self", ".", "invalid_replacement_indices", "=", "[", "\n", "i", "for", "i", "in", "self", ".", "invalid_replacement_indices", "if", "i", "<", "max_index", "\n", "]", "\n", "\n", "inputs", "=", "self", ".", "_make_embedder_input", "(", "all_tokens", ")", "\n", "\n", "# pass all tokens through the fake matrix and create an embedding out of it.", "\n", "embedding_matrix", "=", "embedding_layer", "(", "inputs", ")", ".", "squeeze", "(", ")", "\n", "\n", "return", "embedding_matrix", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.attackers.hotflip.Hotflip._make_embedder_input": [[129, 173], ["indexers.items", "allennlp.nn.util.move_to_device", "isinstance", "isinstance", "torch.LongTensor().unsqueeze", "max", "max", "token_indexer.tokens_to_indices", "token_indexer.get_padding_lengths", "token_indexer.as_padded_tensor_dict", "isinstance", "allennlp.data.tokenizers.Token", "torch.LongTensor().unsqueeze", "RuntimeError", "torch.LongTensor", "len", "elmo_tokens.append", "torch.LongTensor().unsqueeze", "torch.LongTensor", "token_indexer.tokens_to_indices", "torch.LongTensor", "allennlp.data.tokenizers.Token"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.nn.util.move_to_device", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.token_indexers.token_characters_indexer.TokenCharactersIndexer.tokens_to_indices", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.fields.text_field.TextField.get_padding_lengths", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.token_indexers.token_characters_indexer.TokenCharactersIndexer.as_padded_tensor_dict", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.token_indexers.token_characters_indexer.TokenCharactersIndexer.tokens_to_indices"], ["", "def", "_make_embedder_input", "(", "self", ",", "all_tokens", ":", "List", "[", "str", "]", ")", "->", "Dict", "[", "str", ",", "torch", ".", "Tensor", "]", ":", "\n", "        ", "inputs", "=", "{", "}", "\n", "# A bit of a hack; this will only work with some dataset readers, but it'll do for now.", "\n", "indexers", "=", "self", ".", "predictor", ".", "_dataset_reader", ".", "_token_indexers", "# type: ignore", "\n", "for", "indexer_name", ",", "token_indexer", "in", "indexers", ".", "items", "(", ")", ":", "\n", "            ", "if", "isinstance", "(", "token_indexer", ",", "SingleIdTokenIndexer", ")", ":", "\n", "                ", "all_indices", "=", "[", "\n", "self", ".", "vocab", ".", "_token_to_index", "[", "self", ".", "namespace", "]", "[", "token", "]", "\n", "for", "token", "in", "all_tokens", "\n", "]", "\n", "inputs", "[", "indexer_name", "]", "=", "{", "\n", "\"tokens\"", ":", "torch", ".", "LongTensor", "(", "all_indices", ")", ".", "unsqueeze", "(", "0", ")", "\n", "}", "\n", "", "elif", "isinstance", "(", "token_indexer", ",", "TokenCharactersIndexer", ")", ":", "\n", "                ", "tokens", "=", "[", "Token", "(", "x", ")", "for", "x", "in", "all_tokens", "]", "\n", "max_token_length", "=", "max", "(", "len", "(", "x", ")", "for", "x", "in", "all_tokens", ")", "\n", "# sometime max_token_length is too short for cnn encoder", "\n", "max_token_length", "=", "max", "(", "\n", "max_token_length", ",", "token_indexer", ".", "_min_padding_length", "\n", ")", "\n", "indexed_tokens", "=", "token_indexer", ".", "tokens_to_indices", "(", "tokens", ",", "self", ".", "vocab", ")", "\n", "padding_lengths", "=", "token_indexer", ".", "get_padding_lengths", "(", "indexed_tokens", ")", "\n", "padded_tokens", "=", "token_indexer", ".", "as_padded_tensor_dict", "(", "\n", "indexed_tokens", ",", "padding_lengths", "\n", ")", "\n", "inputs", "[", "indexer_name", "]", "=", "{", "\n", "\"token_characters\"", ":", "torch", ".", "LongTensor", "(", "\n", "padded_tokens", "[", "\"token_characters\"", "]", "\n", ")", ".", "unsqueeze", "(", "0", ")", "\n", "}", "\n", "", "elif", "isinstance", "(", "token_indexer", ",", "ELMoTokenCharactersIndexer", ")", ":", "\n", "                ", "elmo_tokens", "=", "[", "]", "\n", "for", "token", "in", "all_tokens", ":", "\n", "                    ", "elmo_indexed_token", "=", "token_indexer", ".", "tokens_to_indices", "(", "\n", "[", "Token", "(", "text", "=", "token", ")", "]", ",", "self", ".", "vocab", ",", "\"sentence\"", "\n", ")", "[", "\"sentence\"", "]", "\n", "elmo_tokens", ".", "append", "(", "elmo_indexed_token", "[", "0", "]", ")", "\n", "", "inputs", "[", "indexer_name", "]", "=", "{", "\n", "\"tokens\"", ":", "torch", ".", "LongTensor", "(", "elmo_tokens", ")", ".", "unsqueeze", "(", "0", ")", "\n", "}", "\n", "", "else", ":", "\n", "                ", "raise", "RuntimeError", "(", "\"Unsupported token indexer:\"", ",", "token_indexer", ")", "\n", "\n", "", "", "return", "util", ".", "move_to_device", "(", "inputs", ",", "self", ".", "cuda_device", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.attackers.hotflip.Hotflip.attack_from_json": [[174, 351], ["hotflip.Hotflip.predictor._json_to_instance", "hotflip.Hotflip.predictor.predictions_to_labeled_instances", "copy.deepcopy", "allennlp.common.util.sanitize", "hotflip.Hotflip.initialize", "hotflip.Hotflip.predictor._model.forward_on_instance", "allennlp.interpret.attackers.utils.get_fields_to_compare", "hotflip.Hotflip.predictor.get_gradients", "enumerate", "final_tokens.append", "numpy.argmax", "flipped.append", "text_field.as_tensor", "allennlp.nn.util.get_token_ids_from_text_field_tensors", "hotflip.Hotflip._first_order_taylor", "allennlp.data.tokenizers.Token", "hotflip.Hotflip.predictor.get_gradients", "outputs.items", "allennlp.interpret.attackers.utils.instance_has_changed", "flipped.append", "g.dot", "text_field.get_padding_lengths", "isinstance", "hotflip.Hotflip.predictor.predictions_to_labeled_instances", "range", "output.detach().cpu().numpy().squeeze", "isinstance", "flipped.append", "output.detach().cpu().numpy", "output.detach().cpu", "output.detach"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.predictors.constituency_parser.ConstituencyParserPredictor._json_to_instance", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.predictors.masked_language_model.MaskedLanguageModelPredictor.predictions_to_labeled_instances", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.util.sanitize", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.attackers.hotflip.Hotflip.initialize", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.models.model.Model.forward_on_instance", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.attackers.utils.get_fields_to_compare", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.predictors.predictor.Predictor.get_gradients", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.fields.text_field.TextField.as_tensor", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.nn.util.get_token_ids_from_text_field_tensors", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.attackers.hotflip.Hotflip._first_order_taylor", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.predictors.predictor.Predictor.get_gradients", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.attackers.utils.instance_has_changed", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.fields.text_field.TextField.get_padding_lengths", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.predictors.masked_language_model.MaskedLanguageModelPredictor.predictions_to_labeled_instances"], ["", "def", "attack_from_json", "(", "\n", "self", ",", "\n", "inputs", ":", "JsonDict", ",", "\n", "input_field_to_attack", ":", "str", "=", "\"tokens\"", ",", "\n", "grad_input_field", ":", "str", "=", "\"grad_input_1\"", ",", "\n", "ignore_tokens", ":", "List", "[", "str", "]", "=", "None", ",", "\n", "target", ":", "JsonDict", "=", "None", ",", "\n", ")", "->", "JsonDict", ":", "\n", "        ", "\"\"\"\n        Replaces one token at a time from the input until the model's prediction changes.\n        `input_field_to_attack` is for example `tokens`, it says what the input field is\n        called.  `grad_input_field` is for example `grad_input_1`, which is a key into a grads\n        dictionary.\n\n        The method computes the gradient w.r.t. the tokens, finds the token with the maximum\n        gradient (by L2 norm), and replaces it with another token based on the first-order Taylor\n        approximation of the loss.  This process is iteratively repeated until the prediction\n        changes.  Once a token is replaced, it is not flipped again.\n\n        # Parameters\n\n        inputs : `JsonDict`\n            The model inputs, the same as what is passed to a `Predictor`.\n        input_field_to_attack : `str`, optional (default='tokens')\n            The field that has the tokens that we're going to be flipping.  This must be a\n            `TextField`.\n        grad_input_field : `str`, optional (default='grad_input_1')\n            If there is more than one field that gets embedded in your model (e.g., a question and\n            a passage, or a premise and a hypothesis), this tells us the key to use to get the\n            correct gradients.  This selects from the output of :func:`Predictor.get_gradients`.\n        ignore_tokens : `List[str]`, optional (default=DEFAULT_IGNORE_TOKENS)\n            These tokens will not be flipped.  The default list includes some simple punctuation,\n            OOV and padding tokens, and common control tokens for BERT, etc.\n        target : `JsonDict`, optional (default=None)\n            If given, this will be a `targeted` hotflip attack, where instead of just trying to\n            change a model's prediction from what it current is predicting, we try to change it to\n            a `specific` target value.  This is a `JsonDict` because it needs to specify the\n            field name and target value.  For example, for a masked LM, this would be something\n            like `{\"words\": [\"she\"]}`, because `\"words\"` is the field name, there is one mask\n            token (hence the list of length one), and we want to change the prediction from\n            whatever it was to `\"she\"`.\n        \"\"\"", "\n", "if", "self", ".", "embedding_matrix", "is", "None", ":", "\n", "            ", "self", ".", "initialize", "(", ")", "\n", "", "ignore_tokens", "=", "(", "\n", "DEFAULT_IGNORE_TOKENS", "if", "ignore_tokens", "is", "None", "else", "ignore_tokens", "\n", ")", "\n", "\n", "# If `target` is `None`, we move away from the current prediction, otherwise we move", "\n", "# _towards_ the target.", "\n", "sign", "=", "-", "1", "if", "target", "is", "None", "else", "1", "\n", "instance", "=", "self", ".", "predictor", ".", "_json_to_instance", "(", "inputs", ")", "\n", "if", "target", "is", "None", ":", "\n", "            ", "output_dict", "=", "self", ".", "predictor", ".", "_model", ".", "forward_on_instance", "(", "instance", ")", "\n", "", "else", ":", "\n", "            ", "output_dict", "=", "target", "\n", "\n", "# This now holds the predictions that we want to change (either away from or towards,", "\n", "# depending on whether `target` was passed).  We'll use this in the loop below to check for", "\n", "# when we've met our stopping criterion.", "\n", "", "original_instances", "=", "self", ".", "predictor", ".", "predictions_to_labeled_instances", "(", "\n", "instance", ",", "output_dict", "\n", ")", "\n", "\n", "# This is just for ease of access in the UI, so we know the original tokens.  It's not used", "\n", "# in the logic below.", "\n", "original_text_field", ":", "TextField", "=", "original_instances", "[", "0", "]", "[", "# type: ignore", "\n", "input_field_to_attack", "\n", "]", "\n", "original_tokens", "=", "deepcopy", "(", "original_text_field", ".", "tokens", ")", "\n", "\n", "final_tokens", "=", "[", "]", "\n", "# `original_instances` is a list because there might be several different predictions that", "\n", "# we're trying to attack (e.g., all of the NER tags for an input sentence).  We attack them", "\n", "# one at a time.", "\n", "for", "instance", "in", "original_instances", ":", "\n", "# Gets a list of the fields that we want to check to see if they change.", "\n", "            ", "fields_to_compare", "=", "utils", ".", "get_fields_to_compare", "(", "\n", "inputs", ",", "instance", ",", "input_field_to_attack", "\n", ")", "\n", "\n", "# We'll be modifying the tokens in this text field below, and grabbing the modified", "\n", "# list after the `while` loop.", "\n", "text_field", ":", "TextField", "=", "instance", "[", "input_field_to_attack", "]", "# type: ignore", "\n", "\n", "# Because we can save computation by getting grads and outputs at the same time, we do", "\n", "# them together at the end of the loop, even though we use grads at the beginning and", "\n", "# outputs at the end.  This is our initial gradient for the beginning of the loop.  The", "\n", "# output can be ignored here.", "\n", "grads", ",", "outputs", "=", "self", ".", "predictor", ".", "get_gradients", "(", "[", "instance", "]", ")", "\n", "\n", "# Ignore any token that is in the ignore_tokens list by setting the token to already", "\n", "# flipped.", "\n", "flipped", ":", "List", "[", "int", "]", "=", "[", "]", "\n", "for", "index", ",", "token", "in", "enumerate", "(", "text_field", ".", "tokens", ")", ":", "\n", "                ", "if", "token", ".", "text", "in", "ignore_tokens", ":", "\n", "                    ", "flipped", ".", "append", "(", "index", ")", "\n", "", "", "if", "\"clusters\"", "in", "outputs", ":", "\n", "# Coref unfortunately needs a special case here.  We don't want to flip words in", "\n", "# the same predicted coref cluster, but we can't really specify a list of tokens,", "\n", "# because, e.g., \"he\" could show up in several different clusters.", "\n", "# TODO(mattg): perhaps there's a way to get `predictions_to_labeled_instances` to", "\n", "# return the set of tokens that shouldn't be changed for each instance?  E.g., you", "\n", "# could imagine setting a field on the `Token` object, that we could then read", "\n", "# here...", "\n", "                ", "for", "cluster", "in", "outputs", "[", "\"clusters\"", "]", ":", "\n", "                    ", "for", "mention", "in", "cluster", ":", "\n", "                        ", "for", "index", "in", "range", "(", "mention", "[", "0", "]", ",", "mention", "[", "1", "]", "+", "1", ")", ":", "\n", "                            ", "flipped", ".", "append", "(", "index", ")", "\n", "\n", "", "", "", "", "while", "True", ":", "\n", "# Compute L2 norm of all grads.", "\n", "                ", "grad", "=", "grads", "[", "grad_input_field", "]", "[", "0", "]", "\n", "grads_magnitude", "=", "[", "g", ".", "dot", "(", "g", ")", "for", "g", "in", "grad", "]", "\n", "\n", "# only flip a token once", "\n", "for", "index", "in", "flipped", ":", "\n", "                    ", "grads_magnitude", "[", "index", "]", "=", "-", "1", "\n", "\n", "# We flip the token with highest gradient norm.", "\n", "", "index_of_token_to_flip", "=", "numpy", ".", "argmax", "(", "grads_magnitude", ")", "\n", "if", "grads_magnitude", "[", "index_of_token_to_flip", "]", "==", "-", "1", ":", "\n", "# If we've already flipped all of the tokens, we give up.", "\n", "                    ", "break", "\n", "", "flipped", ".", "append", "(", "index_of_token_to_flip", ")", "\n", "\n", "text_field_tensors", "=", "text_field", ".", "as_tensor", "(", "\n", "text_field", ".", "get_padding_lengths", "(", ")", "\n", ")", "\n", "input_tokens", "=", "util", ".", "get_token_ids_from_text_field_tensors", "(", "\n", "text_field_tensors", "\n", ")", "\n", "original_id_of_token_to_flip", "=", "input_tokens", "[", "index_of_token_to_flip", "]", "\n", "\n", "# Get new token using taylor approximation.", "\n", "new_id", "=", "self", ".", "_first_order_taylor", "(", "\n", "grad", "[", "index_of_token_to_flip", "]", ",", "original_id_of_token_to_flip", ",", "sign", "\n", ")", "\n", "\n", "# Flip token.  We need to tell the instance to re-index itself, so the text field", "\n", "# will actually update.", "\n", "new_token", "=", "Token", "(", "\n", "self", ".", "vocab", ".", "_index_to_token", "[", "self", ".", "namespace", "]", "[", "new_id", "]", "\n", ")", "# type: ignore", "\n", "text_field", ".", "tokens", "[", "index_of_token_to_flip", "]", "=", "new_token", "\n", "instance", ".", "indexed", "=", "False", "\n", "\n", "# Get model predictions on instance, and then label the instances", "\n", "grads", ",", "outputs", "=", "self", ".", "predictor", ".", "get_gradients", "(", "[", "instance", "]", ")", "# predictions", "\n", "for", "key", ",", "output", "in", "outputs", ".", "items", "(", ")", ":", "\n", "                    ", "if", "isinstance", "(", "output", ",", "torch", ".", "Tensor", ")", ":", "\n", "                        ", "outputs", "[", "key", "]", "=", "output", ".", "detach", "(", ")", ".", "cpu", "(", ")", ".", "numpy", "(", ")", ".", "squeeze", "(", ")", "\n", "", "elif", "isinstance", "(", "output", ",", "list", ")", ":", "\n", "                        ", "outputs", "[", "key", "]", "=", "output", "[", "0", "]", "\n", "\n", "# TODO(mattg): taking the first result here seems brittle, if we're in a case where", "\n", "# there are multiple predictions.", "\n", "", "", "labeled_instance", "=", "self", ".", "predictor", ".", "predictions_to_labeled_instances", "(", "\n", "instance", ",", "outputs", "\n", ")", "[", "0", "]", "\n", "\n", "# If we've met our stopping criterion, we stop.", "\n", "has_changed", "=", "utils", ".", "instance_has_changed", "(", "\n", "labeled_instance", ",", "fields_to_compare", "\n", ")", "\n", "if", "target", "is", "None", "and", "has_changed", ":", "\n", "# With no target, we just want to change the prediction.", "\n", "                    ", "break", "\n", "", "if", "target", "is", "not", "None", "and", "not", "has_changed", ":", "\n", "# With a given target, we want to *match* the target, which we check by", "\n", "# `not has_changed`.", "\n", "                    ", "break", "\n", "\n", "", "", "final_tokens", ".", "append", "(", "text_field", ".", "tokens", ")", "\n", "\n", "", "return", "sanitize", "(", "\n", "{", "\"final\"", ":", "final_tokens", ",", "\"original\"", ":", "original_tokens", ",", "\"outputs\"", ":", "outputs", "}", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.attackers.hotflip.Hotflip._first_order_taylor": [[353, 392], ["allennlp.nn.util.move_to_device", "torch.nn.functional.embedding.detach().unsqueeze", "grad.unsqueeze().unsqueeze.unsqueeze().unsqueeze.unsqueeze().unsqueeze", "torch.einsum", "torch.einsum().unsqueeze", "neg_dir_dot_grad.detach().cpu().numpy.detach().cpu().numpy.detach().cpu().numpy", "neg_dir_dot_grad.detach().cpu().numpy.detach().cpu().numpy.argmax", "torch.from_numpy", "hotflip.Hotflip.embedding_matrix.size", "hotflip.Hotflip._make_embedder_input", "torch.nn.functional.embedding", "hotflip.Hotflip.embedding_layer", "allennlp.nn.util.move_to_device", "torch.nn.functional.embedding.detach", "grad.unsqueeze().unsqueeze.unsqueeze().unsqueeze.unsqueeze", "torch.einsum", "neg_dir_dot_grad.detach().cpu().numpy.detach().cpu().numpy.detach().cpu", "hotflip.Hotflip.vocab.get_token_from_index", "torch.LongTensor", "neg_dir_dot_grad.detach().cpu().numpy.detach().cpu().numpy.detach"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.nn.util.move_to_device", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.attackers.hotflip.Hotflip._make_embedder_input", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.nn.util.move_to_device", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.data.vocabulary.Vocabulary.get_token_from_index"], ["", "def", "_first_order_taylor", "(", "\n", "self", ",", "grad", ":", "numpy", ".", "ndarray", ",", "token_idx", ":", "int", ",", "sign", ":", "int", "\n", ")", "->", "int", ":", "\n", "        ", "\"\"\"\n        The below code is based on\n        https://github.com/pmichel31415/translate/blob/paul/pytorch_translate/\n        research/adversarial/adversaries/brute_force_adversary.py\n\n        Replaces the current token_idx with another token_idx to increase the loss. In particular, this\n        function uses the grad, alongside the embedding_matrix to select the token that maximizes the\n        first-order taylor approximation of the loss.\n        \"\"\"", "\n", "grad", "=", "util", ".", "move_to_device", "(", "torch", ".", "from_numpy", "(", "grad", ")", ",", "self", ".", "cuda_device", ")", "\n", "if", "token_idx", ">=", "self", ".", "embedding_matrix", ".", "size", "(", "0", ")", ":", "\n", "# This happens when we've truncated our fake embedding matrix.  We need to do a dot", "\n", "# product with the word vector of the current token; if that token is out of", "\n", "# vocabulary for our truncated matrix, we need to run it through the embedding layer.", "\n", "            ", "inputs", "=", "self", ".", "_make_embedder_input", "(", "\n", "[", "self", ".", "vocab", ".", "get_token_from_index", "(", "token_idx", ")", "]", "\n", ")", "\n", "word_embedding", "=", "self", ".", "embedding_layer", "(", "inputs", ")", "[", "0", "]", "\n", "", "else", ":", "\n", "            ", "word_embedding", "=", "torch", ".", "nn", ".", "functional", ".", "embedding", "(", "\n", "util", ".", "move_to_device", "(", "torch", ".", "LongTensor", "(", "[", "token_idx", "]", ")", ",", "self", ".", "cuda_device", ")", ",", "\n", "self", ".", "embedding_matrix", ",", "\n", ")", "\n", "", "word_embedding", "=", "word_embedding", ".", "detach", "(", ")", ".", "unsqueeze", "(", "0", ")", "\n", "grad", "=", "grad", ".", "unsqueeze", "(", "0", ")", ".", "unsqueeze", "(", "0", ")", "\n", "# solves equation (3) here https://arxiv.org/abs/1903.06620", "\n", "new_embed_dot_grad", "=", "torch", ".", "einsum", "(", "\"bij,kj->bik\"", ",", "(", "grad", ",", "self", ".", "embedding_matrix", ")", ")", "\n", "prev_embed_dot_grad", "=", "torch", ".", "einsum", "(", "\n", "\"bij,bij->bi\"", ",", "(", "grad", ",", "word_embedding", ")", "\n", ")", ".", "unsqueeze", "(", "-", "1", ")", "\n", "neg_dir_dot_grad", "=", "sign", "*", "(", "prev_embed_dot_grad", "-", "new_embed_dot_grad", ")", "\n", "neg_dir_dot_grad", "=", "neg_dir_dot_grad", ".", "detach", "(", ")", ".", "cpu", "(", ")", ".", "numpy", "(", ")", "\n", "# Do not replace with non-alphanumeric tokens", "\n", "neg_dir_dot_grad", "[", ":", ",", ":", ",", "self", ".", "invalid_replacement_indices", "]", "=", "-", "numpy", ".", "inf", "\n", "best_at_each_step", "=", "neg_dir_dot_grad", ".", "argmax", "(", "2", ")", "\n", "return", "best_at_each_step", "[", "0", "]", ".", "data", "[", "0", "]", "\n", "", "", ""]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.encoder_decoders.copynet_seq2seq.CopyNetSeq2Seq.__init__": [[76, 171], ["allennlp.nn.InitializerApplicator", "allennlp.models.model.Model.__init__", "copynet_seq2seq.CopyNetSeq2Seq.vocab.get_token_index", "copynet_seq2seq.CopyNetSeq2Seq.vocab.get_token_index", "copynet_seq2seq.CopyNetSeq2Seq.vocab.get_token_index", "copynet_seq2seq.CopyNetSeq2Seq.vocab.get_token_index", "copynet_seq2seq.CopyNetSeq2Seq.vocab.get_token_index", "copynet_seq2seq.CopyNetSeq2Seq.vocab.get_token_index", "copynet_seq2seq.CopyNetSeq2Seq.vocab.add_token_to_namespace", "copynet_seq2seq.CopyNetSeq2Seq.vocab.get_vocab_size", "copynet_seq2seq.CopyNetSeq2Seq._encoder.get_output_dim", "copynet_seq2seq.CopyNetSeq2Seq.vocab.get_vocab_size", "allennlp.modules.token_embedders.Embedding", "torch.nn.modules.linear.Linear", "torch.nn.modules.rnn.LSTMCell", "torch.nn.modules.linear.Linear", "torch.nn.modules.linear.Linear", "allennlp.nn.beam_search.BeamSearch", "initializer", "allennlp.training.metrics.BLEU"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.graph_util.plot.GraphPlot.__init__", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.data.vocabulary.Vocabulary.get_token_index", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.data.vocabulary.Vocabulary.get_token_index", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.data.vocabulary.Vocabulary.get_token_index", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.data.vocabulary.Vocabulary.get_token_index", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.data.vocabulary.Vocabulary.get_token_index", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.data.vocabulary.Vocabulary.get_token_index", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.data.vocabulary.Vocabulary.add_token_to_namespace", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.data.vocabulary.Vocabulary.get_vocab_size", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.seq2seq_decoders.auto_regressive_seq_decoder.AutoRegressiveSeqDecoder.get_output_dim", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.data.vocabulary.Vocabulary.get_vocab_size"], ["def", "__init__", "(", "\n", "self", ",", "\n", "vocab", ":", "Vocabulary", ",", "\n", "source_embedder", ":", "TextFieldEmbedder", ",", "\n", "encoder", ":", "Seq2SeqEncoder", ",", "\n", "attention", ":", "Attention", ",", "\n", "beam_size", ":", "int", ",", "\n", "max_decoding_steps", ":", "int", ",", "\n", "target_embedding_dim", ":", "int", "=", "30", ",", "\n", "copy_token", ":", "str", "=", "\"@COPY@\"", ",", "\n", "source_namespace", ":", "str", "=", "\"source_tokens\"", ",", "\n", "target_namespace", ":", "str", "=", "\"target_tokens\"", ",", "\n", "tensor_based_metric", ":", "Metric", "=", "None", ",", "\n", "token_based_metric", ":", "Metric", "=", "None", ",", "\n", "initializer", ":", "InitializerApplicator", "=", "InitializerApplicator", "(", ")", ",", "\n", ")", "->", "None", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "vocab", ")", "\n", "self", ".", "_source_namespace", "=", "source_namespace", "\n", "self", ".", "_target_namespace", "=", "target_namespace", "\n", "self", ".", "_src_start_index", "=", "self", ".", "vocab", ".", "get_token_index", "(", "\n", "START_SYMBOL", ",", "self", ".", "_source_namespace", "\n", ")", "\n", "self", ".", "_src_end_index", "=", "self", ".", "vocab", ".", "get_token_index", "(", "\n", "END_SYMBOL", ",", "self", ".", "_source_namespace", "\n", ")", "\n", "self", ".", "_start_index", "=", "self", ".", "vocab", ".", "get_token_index", "(", "\n", "START_SYMBOL", ",", "self", ".", "_target_namespace", "\n", ")", "\n", "self", ".", "_end_index", "=", "self", ".", "vocab", ".", "get_token_index", "(", "END_SYMBOL", ",", "self", ".", "_target_namespace", ")", "\n", "self", ".", "_oov_index", "=", "self", ".", "vocab", ".", "get_token_index", "(", "\n", "self", ".", "vocab", ".", "_oov_token", ",", "self", ".", "_target_namespace", "\n", ")", "\n", "self", ".", "_pad_index", "=", "self", ".", "vocab", ".", "get_token_index", "(", "\n", "self", ".", "vocab", ".", "_padding_token", ",", "self", ".", "_target_namespace", "\n", ")", "\n", "self", ".", "_copy_index", "=", "self", ".", "vocab", ".", "add_token_to_namespace", "(", "\n", "copy_token", ",", "self", ".", "_target_namespace", "\n", ")", "\n", "\n", "self", ".", "_tensor_based_metric", "=", "tensor_based_metric", "or", "BLEU", "(", "\n", "exclude_indices", "=", "{", "self", ".", "_pad_index", ",", "self", ".", "_end_index", ",", "self", ".", "_start_index", "}", "\n", ")", "\n", "self", ".", "_token_based_metric", "=", "token_based_metric", "\n", "\n", "self", ".", "_target_vocab_size", "=", "self", ".", "vocab", ".", "get_vocab_size", "(", "self", ".", "_target_namespace", ")", "\n", "\n", "# Encoding modules.", "\n", "self", ".", "_source_embedder", "=", "source_embedder", "\n", "self", ".", "_encoder", "=", "encoder", "\n", "\n", "# Decoder output dim needs to be the same as the encoder output dim since we initialize the", "\n", "# hidden state of the decoder with the final hidden state of the encoder.", "\n", "# We arbitrarily set the decoder's input dimension to be the same as the output dimension.", "\n", "self", ".", "encoder_output_dim", "=", "self", ".", "_encoder", ".", "get_output_dim", "(", ")", "\n", "self", ".", "decoder_output_dim", "=", "self", ".", "encoder_output_dim", "\n", "self", ".", "decoder_input_dim", "=", "self", ".", "decoder_output_dim", "\n", "\n", "target_vocab_size", "=", "self", ".", "vocab", ".", "get_vocab_size", "(", "self", ".", "_target_namespace", ")", "\n", "\n", "# The decoder input will be a function of the embedding of the previous predicted token,", "\n", "# an attended encoder hidden state called the \"attentive read\", and another", "\n", "# weighted sum of the encoder hidden state called the \"selective read\".", "\n", "# While the weights for the attentive read are calculated by an `Attention` module,", "\n", "# the weights for the selective read are simply the predicted probabilities", "\n", "# corresponding to each token in the source sentence that matches the target", "\n", "# token from the previous timestep.", "\n", "self", ".", "_target_embedder", "=", "Embedding", "(", "target_vocab_size", ",", "target_embedding_dim", ")", "\n", "self", ".", "_attention", "=", "attention", "\n", "self", ".", "_input_projection_layer", "=", "Linear", "(", "\n", "target_embedding_dim", "+", "self", ".", "encoder_output_dim", "*", "2", ",", "self", ".", "decoder_input_dim", "\n", ")", "\n", "\n", "# We then run the projected decoder input through an LSTM cell to produce", "\n", "# the next hidden state.", "\n", "self", ".", "_decoder_cell", "=", "LSTMCell", "(", "self", ".", "decoder_input_dim", ",", "self", ".", "decoder_output_dim", ")", "\n", "\n", "# We create a \"generation\" score for each token in the target vocab", "\n", "# with a linear projection of the decoder hidden state.", "\n", "self", ".", "_output_generation_layer", "=", "Linear", "(", "\n", "self", ".", "decoder_output_dim", ",", "target_vocab_size", "\n", ")", "\n", "\n", "# We create a \"copying\" score for each source token by applying a non-linearity", "\n", "# (tanh) to a linear projection of the encoded hidden state for that token,", "\n", "# and then taking the dot product of the result with the decoder hidden state.", "\n", "self", ".", "_output_copying_layer", "=", "Linear", "(", "\n", "self", ".", "encoder_output_dim", ",", "self", ".", "decoder_output_dim", "\n", ")", "\n", "\n", "# At prediction time, we'll use a beam search to find the best target sequence.", "\n", "self", ".", "_beam_search", "=", "BeamSearch", "(", "\n", "self", ".", "_end_index", ",", "max_steps", "=", "max_decoding_steps", ",", "beam_size", "=", "beam_size", "\n", ")", "\n", "\n", "initializer", "(", "self", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.encoder_decoders.copynet_seq2seq.CopyNetSeq2Seq.forward": [[172, 251], ["copynet_seq2seq.CopyNetSeq2Seq._encode", "copynet_seq2seq.CopyNetSeq2Seq._init_decoder_state", "copynet_seq2seq.CopyNetSeq2Seq._forward_loss", "copynet_seq2seq.CopyNetSeq2Seq._init_decoder_state", "copynet_seq2seq.CopyNetSeq2Seq._forward_beam_search", "copynet_seq2seq.CopyNetSeq2Seq.update", "copynet_seq2seq.CopyNetSeq2Seq._gather_extended_gold_tokens", "copynet_seq2seq.CopyNetSeq2Seq._tensor_based_metric", "copynet_seq2seq.CopyNetSeq2Seq._get_predicted_tokens", "copynet_seq2seq.CopyNetSeq2Seq._token_based_metric"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.encoder_decoders.simple_seq2seq.SimpleSeq2Seq._encode", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.encoder_decoders.simple_seq2seq.SimpleSeq2Seq._init_decoder_state", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.encoder_decoders.copynet_seq2seq.CopyNetSeq2Seq._forward_loss", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.encoder_decoders.simple_seq2seq.SimpleSeq2Seq._init_decoder_state", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.encoder_decoders.simple_seq2seq.SimpleSeq2Seq._forward_beam_search", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.metrics.conll_coref_scores.Scorer.update", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.encoder_decoders.copynet_seq2seq.CopyNetSeq2Seq._gather_extended_gold_tokens", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.encoder_decoders.copynet_seq2seq.CopyNetSeq2Seq._get_predicted_tokens"], ["", "@", "overrides", "\n", "def", "forward", "(", "\n", "self", ",", "# type: ignore", "\n", "source_tokens", ":", "TextFieldTensors", ",", "\n", "source_token_ids", ":", "torch", ".", "Tensor", ",", "\n", "source_to_target", ":", "torch", ".", "Tensor", ",", "\n", "metadata", ":", "List", "[", "Dict", "[", "str", ",", "Any", "]", "]", ",", "\n", "target_tokens", ":", "TextFieldTensors", "=", "None", ",", "\n", "target_token_ids", ":", "torch", ".", "Tensor", "=", "None", ",", "\n", ")", "->", "Dict", "[", "str", ",", "torch", ".", "Tensor", "]", ":", "\n", "\n", "        ", "\"\"\"\n        Make foward pass with decoder logic for producing the entire target sequence.\n\n        # Parameters\n\n        source_tokens : `TextFieldTensors`, required\n            The output of `TextField.as_array()` applied on the source `TextField`. This will be\n            passed through a `TextFieldEmbedder` and then through an encoder.\n        source_token_ids : `torch.Tensor`, required\n            Tensor containing IDs that indicate which source tokens match each other.\n            Has shape: `(batch_size, trimmed_source_length)`.\n        source_to_target : `torch.Tensor`, required\n            Tensor containing vocab index of each source token with respect to the\n            target vocab namespace. Shape: `(batch_size, trimmed_source_length)`.\n        metadata : `List[Dict[str, Any]]`, required\n            Metadata field that contains the original source tokens with key 'source_tokens'\n            and any other meta fields. When 'target_tokens' is also passed, the metadata\n            should also contain the original target tokens with key 'target_tokens'.\n        target_tokens : `TextFieldTensors`, optional (default = None)\n            Output of `Textfield.as_array()` applied on target `TextField`. We assume that the\n            target tokens are also represented as a `TextField` which must contain a \"tokens\"\n            key that uses single ids.\n        target_token_ids : `torch.Tensor`, optional (default = None)\n            A tensor of shape `(batch_size, target_sequence_length)` which indicates which\n            tokens in the target sequence match tokens in the source sequence.\n\n        # Returns\n\n        Dict[str, torch.Tensor]\n        \"\"\"", "\n", "state", "=", "self", ".", "_encode", "(", "source_tokens", ")", "\n", "state", "[", "\"source_token_ids\"", "]", "=", "source_token_ids", "\n", "state", "[", "\"source_to_target\"", "]", "=", "source_to_target", "\n", "\n", "if", "target_tokens", ":", "\n", "            ", "state", "=", "self", ".", "_init_decoder_state", "(", "state", ")", "\n", "output_dict", "=", "self", ".", "_forward_loss", "(", "target_tokens", ",", "target_token_ids", ",", "state", ")", "\n", "", "else", ":", "\n", "            ", "output_dict", "=", "{", "}", "\n", "\n", "", "output_dict", "[", "\"metadata\"", "]", "=", "metadata", "\n", "\n", "if", "not", "self", ".", "training", ":", "\n", "            ", "state", "=", "self", ".", "_init_decoder_state", "(", "state", ")", "\n", "predictions", "=", "self", ".", "_forward_beam_search", "(", "state", ")", "\n", "output_dict", ".", "update", "(", "predictions", ")", "\n", "if", "target_tokens", ":", "\n", "                ", "if", "self", ".", "_tensor_based_metric", "is", "not", "None", ":", "\n", "# shape: (batch_size, beam_size, max_sequence_length)", "\n", "                    ", "top_k_predictions", "=", "output_dict", "[", "\"predictions\"", "]", "\n", "# shape: (batch_size, max_predicted_sequence_length)", "\n", "best_predictions", "=", "top_k_predictions", "[", ":", ",", "0", ",", ":", "]", "\n", "# shape: (batch_size, target_sequence_length)", "\n", "gold_tokens", "=", "self", ".", "_gather_extended_gold_tokens", "(", "\n", "target_tokens", "[", "\"tokens\"", "]", "[", "\"tokens\"", "]", ",", "\n", "source_token_ids", ",", "\n", "target_token_ids", ",", "\n", ")", "\n", "self", ".", "_tensor_based_metric", "(", "best_predictions", ",", "gold_tokens", ")", "# type: ignore", "\n", "", "if", "self", ".", "_token_based_metric", "is", "not", "None", ":", "\n", "                    ", "predicted_tokens", "=", "self", ".", "_get_predicted_tokens", "(", "\n", "output_dict", "[", "\"predictions\"", "]", ",", "metadata", ",", "n_best", "=", "1", "\n", ")", "\n", "self", ".", "_token_based_metric", "(", "# type: ignore", "\n", "predicted_tokens", ",", "[", "x", "[", "\"target_tokens\"", "]", "for", "x", "in", "metadata", "]", "\n", ")", "\n", "\n", "", "", "", "return", "output_dict", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.encoder_decoders.copynet_seq2seq.CopyNetSeq2Seq._gather_extended_gold_tokens": [[252, 307], ["target_tokens.size", "source_token_ids.size", "source_token_ids.unsqueeze().expand", "target_token_ids.unsqueeze().expand", "matches.sum", "source_token_ids.unsqueeze", "target_token_ids.unsqueeze", "first_match.long", "matches.cumsum"], "methods", ["None"], ["", "def", "_gather_extended_gold_tokens", "(", "\n", "self", ",", "\n", "target_tokens", ":", "torch", ".", "Tensor", ",", "\n", "source_token_ids", ":", "torch", ".", "Tensor", ",", "\n", "target_token_ids", ":", "torch", ".", "Tensor", ",", "\n", ")", "->", "torch", ".", "LongTensor", ":", "\n", "        ", "\"\"\"\n        Modify the gold target tokens relative to the extended vocabulary.\n\n        For gold targets that are OOV but were copied from the source, the OOV index\n        will be changed to the index of the first occurence in the source sentence,\n        offset by the size of the target vocabulary.\n\n        # Parameters\n\n        target_tokens : `torch.Tensor`\n            Shape: `(batch_size, target_sequence_length)`.\n        source_token_ids : `torch.Tensor`\n            Shape: `(batch_size, trimmed_source_length)`.\n        target_token_ids : `torch.Tensor`\n            Shape: `(batch_size, target_sequence_length)`.\n\n        # Returns\n\n        torch.Tensor\n            Modified `target_tokens` with OOV indices replaced by offset index\n            of first match in source sentence.\n        \"\"\"", "\n", "batch_size", ",", "target_sequence_length", "=", "target_tokens", ".", "size", "(", ")", "\n", "trimmed_source_length", "=", "source_token_ids", ".", "size", "(", "1", ")", "\n", "# Only change indices for tokens that were OOV in target vocab but copied from source.", "\n", "# shape: (batch_size, target_sequence_length)", "\n", "oov", "=", "target_tokens", "==", "self", ".", "_oov_index", "\n", "# shape: (batch_size, target_sequence_length, trimmed_source_length)", "\n", "expanded_source_token_ids", "=", "source_token_ids", ".", "unsqueeze", "(", "1", ")", ".", "expand", "(", "\n", "batch_size", ",", "target_sequence_length", ",", "trimmed_source_length", "\n", ")", "\n", "# shape: (batch_size, target_sequence_length, trimmed_source_length)", "\n", "expanded_target_token_ids", "=", "target_token_ids", ".", "unsqueeze", "(", "-", "1", ")", ".", "expand", "(", "\n", "batch_size", ",", "target_sequence_length", ",", "trimmed_source_length", "\n", ")", "\n", "# shape: (batch_size, target_sequence_length, trimmed_source_length)", "\n", "matches", "=", "expanded_source_token_ids", "==", "expanded_target_token_ids", "\n", "# shape: (batch_size, target_sequence_length)", "\n", "copied", "=", "matches", ".", "sum", "(", "-", "1", ")", ">", "0", "\n", "# shape: (batch_size, target_sequence_length)", "\n", "mask", "=", "(", "oov", "&", "copied", ")", ".", "long", "(", ")", "\n", "# shape: (batch_size, target_sequence_length)", "\n", "first_match", "=", "(", "(", "matches", ".", "cumsum", "(", "-", "1", ")", "==", "1", ")", "*", "matches", ")", ".", "to", "(", "torch", ".", "uint8", ")", ".", "argmax", "(", "-", "1", ")", "\n", "# shape: (batch_size, target_sequence_length)", "\n", "new_target_tokens", "=", "(", "\n", "target_tokens", "*", "(", "1", "-", "mask", ")", "\n", "+", "(", "first_match", ".", "long", "(", ")", "+", "self", ".", "_target_vocab_size", ")", "*", "mask", "\n", ")", "\n", "return", "new_target_tokens", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.encoder_decoders.copynet_seq2seq.CopyNetSeq2Seq._init_decoder_state": [[308, 332], ["state[].size", "allennlp.nn.util.get_final_encoder_states", "state[].new_zeros", "copynet_seq2seq.CopyNetSeq2Seq._encoder.is_bidirectional"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.nn.util.get_final_encoder_states", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.seq2seq_encoders.qanet_encoder.QaNetEncoderBlock.is_bidirectional"], ["", "def", "_init_decoder_state", "(", "\n", "self", ",", "state", ":", "Dict", "[", "str", ",", "torch", ".", "Tensor", "]", "\n", ")", "->", "Dict", "[", "str", ",", "torch", ".", "Tensor", "]", ":", "\n", "        ", "\"\"\"\n        Initialize the encoded state to be passed to the first decoding time step.\n        \"\"\"", "\n", "batch_size", ",", "_", "=", "state", "[", "\"source_mask\"", "]", ".", "size", "(", ")", "\n", "\n", "# Initialize the decoder hidden state with the final output of the encoder,", "\n", "# and the decoder context with zeros.", "\n", "# shape: (batch_size, encoder_output_dim)", "\n", "final_encoder_output", "=", "util", ".", "get_final_encoder_states", "(", "\n", "state", "[", "\"encoder_outputs\"", "]", ",", "\n", "state", "[", "\"source_mask\"", "]", ",", "\n", "self", ".", "_encoder", ".", "is_bidirectional", "(", ")", ",", "\n", ")", "\n", "# shape: (batch_size, decoder_output_dim)", "\n", "state", "[", "\"decoder_hidden\"", "]", "=", "final_encoder_output", "\n", "# shape: (batch_size, decoder_output_dim)", "\n", "state", "[", "\"decoder_context\"", "]", "=", "state", "[", "\"encoder_outputs\"", "]", ".", "new_zeros", "(", "\n", "batch_size", ",", "self", ".", "decoder_output_dim", "\n", ")", "\n", "\n", "return", "state", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.encoder_decoders.copynet_seq2seq.CopyNetSeq2Seq._encode": [[333, 346], ["copynet_seq2seq.CopyNetSeq2Seq._source_embedder", "allennlp.nn.util.get_text_field_mask", "copynet_seq2seq.CopyNetSeq2Seq._encoder"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.nn.util.get_text_field_mask"], ["", "def", "_encode", "(", "\n", "self", ",", "source_tokens", ":", "Dict", "[", "str", ",", "torch", ".", "Tensor", "]", "\n", ")", "->", "Dict", "[", "str", ",", "torch", ".", "Tensor", "]", ":", "\n", "        ", "\"\"\"\n        Encode source input sentences.\n        \"\"\"", "\n", "# shape: (batch_size, max_input_sequence_length, encoder_input_dim)", "\n", "embedded_input", "=", "self", ".", "_source_embedder", "(", "source_tokens", ")", "\n", "# shape: (batch_size, max_input_sequence_length)", "\n", "source_mask", "=", "util", ".", "get_text_field_mask", "(", "source_tokens", ")", "\n", "# shape: (batch_size, max_input_sequence_length, encoder_output_dim)", "\n", "encoder_outputs", "=", "self", ".", "_encoder", "(", "embedded_input", ",", "source_mask", ")", "\n", "return", "{", "\"source_mask\"", ":", "source_mask", ",", "\"encoder_outputs\"", ":", "encoder_outputs", "}", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.encoder_decoders.copynet_seq2seq.CopyNetSeq2Seq._decoder_step": [[347, 376], ["state[].float", "copynet_seq2seq.CopyNetSeq2Seq._target_embedder", "copynet_seq2seq.CopyNetSeq2Seq._attention", "allennlp.nn.util.weighted_sum", "allennlp.nn.util.weighted_sum", "torch.cat", "copynet_seq2seq.CopyNetSeq2Seq._input_projection_layer", "copynet_seq2seq.CopyNetSeq2Seq._decoder_cell"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.nn.util.weighted_sum", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.nn.util.weighted_sum"], ["", "def", "_decoder_step", "(", "\n", "self", ",", "\n", "last_predictions", ":", "torch", ".", "Tensor", ",", "\n", "selective_weights", ":", "torch", ".", "Tensor", ",", "\n", "state", ":", "Dict", "[", "str", ",", "torch", ".", "Tensor", "]", ",", "\n", ")", "->", "Dict", "[", "str", ",", "torch", ".", "Tensor", "]", ":", "\n", "# shape: (group_size, max_input_sequence_length, encoder_output_dim)", "\n", "        ", "encoder_outputs_mask", "=", "state", "[", "\"source_mask\"", "]", ".", "float", "(", ")", "\n", "# shape: (group_size, target_embedding_dim)", "\n", "embedded_input", "=", "self", ".", "_target_embedder", "(", "last_predictions", ")", "\n", "# shape: (group_size, max_input_sequence_length)", "\n", "attentive_weights", "=", "self", ".", "_attention", "(", "\n", "state", "[", "\"decoder_hidden\"", "]", ",", "state", "[", "\"encoder_outputs\"", "]", ",", "encoder_outputs_mask", "\n", ")", "\n", "# shape: (group_size, encoder_output_dim)", "\n", "attentive_read", "=", "util", ".", "weighted_sum", "(", "state", "[", "\"encoder_outputs\"", "]", ",", "attentive_weights", ")", "\n", "# shape: (group_size, encoder_output_dim)", "\n", "selective_read", "=", "util", ".", "weighted_sum", "(", "\n", "state", "[", "\"encoder_outputs\"", "]", "[", ":", ",", "1", ":", "-", "1", "]", ",", "selective_weights", "\n", ")", "\n", "# shape: (group_size, target_embedding_dim + encoder_output_dim * 2)", "\n", "decoder_input", "=", "torch", ".", "cat", "(", "(", "embedded_input", ",", "attentive_read", ",", "selective_read", ")", ",", "-", "1", ")", "\n", "# shape: (group_size, decoder_input_dim)", "\n", "projected_decoder_input", "=", "self", ".", "_input_projection_layer", "(", "decoder_input", ")", "\n", "\n", "state", "[", "\"decoder_hidden\"", "]", ",", "state", "[", "\"decoder_context\"", "]", "=", "self", ".", "_decoder_cell", "(", "\n", "projected_decoder_input", ",", "(", "state", "[", "\"decoder_hidden\"", "]", ",", "state", "[", "\"decoder_context\"", "]", ")", "\n", ")", "\n", "return", "state", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.encoder_decoders.copynet_seq2seq.CopyNetSeq2Seq._get_generation_scores": [[377, 379], ["copynet_seq2seq.CopyNetSeq2Seq._output_generation_layer"], "methods", ["None"], ["", "def", "_get_generation_scores", "(", "self", ",", "state", ":", "Dict", "[", "str", ",", "torch", ".", "Tensor", "]", ")", "->", "torch", ".", "Tensor", ":", "\n", "        ", "return", "self", ".", "_output_generation_layer", "(", "state", "[", "\"decoder_hidden\"", "]", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.encoder_decoders.copynet_seq2seq.CopyNetSeq2Seq._get_copy_scores": [[380, 392], ["copynet_seq2seq.CopyNetSeq2Seq._output_copying_layer", "torch.tanh", "torch.tanh.bmm().squeeze", "torch.tanh.bmm", "state[].unsqueeze"], "methods", ["None"], ["", "def", "_get_copy_scores", "(", "self", ",", "state", ":", "Dict", "[", "str", ",", "torch", ".", "Tensor", "]", ")", "->", "torch", ".", "Tensor", ":", "\n", "# shape: (batch_size, max_input_sequence_length - 2, encoder_output_dim)", "\n", "        ", "trimmed_encoder_outputs", "=", "state", "[", "\"encoder_outputs\"", "]", "[", ":", ",", "1", ":", "-", "1", "]", "\n", "# shape: (batch_size, max_input_sequence_length - 2, decoder_output_dim)", "\n", "copy_projection", "=", "self", ".", "_output_copying_layer", "(", "trimmed_encoder_outputs", ")", "\n", "# shape: (batch_size, max_input_sequence_length - 2, decoder_output_dim)", "\n", "copy_projection", "=", "torch", ".", "tanh", "(", "copy_projection", ")", "\n", "# shape: (batch_size, max_input_sequence_length - 2)", "\n", "copy_scores", "=", "copy_projection", ".", "bmm", "(", "\n", "state", "[", "\"decoder_hidden\"", "]", ".", "unsqueeze", "(", "-", "1", ")", "\n", ")", ".", "squeeze", "(", "-", "1", ")", "\n", "return", "copy_scores", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.encoder_decoders.copynet_seq2seq.CopyNetSeq2Seq._get_ll_contrib": [[393, 475], ["generation_scores.size", "torch.cat", "torch.cat", "allennlp.nn.util.masked_log_softmax", "allennlp.nn.util.masked_softmax", "torch.cat", "allennlp.nn.util.logsumexp", "allennlp.nn.util.masked_log_softmax.gather", "target_tokens.unsqueeze", "target_to_source.float", "target_to_source.sum"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.nn.util.masked_log_softmax", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.nn.util.masked_softmax", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.nn.util.logsumexp"], ["", "def", "_get_ll_contrib", "(", "\n", "self", ",", "\n", "generation_scores", ":", "torch", ".", "Tensor", ",", "\n", "generation_scores_mask", ":", "torch", ".", "Tensor", ",", "\n", "copy_scores", ":", "torch", ".", "Tensor", ",", "\n", "target_tokens", ":", "torch", ".", "Tensor", ",", "\n", "target_to_source", ":", "torch", ".", "Tensor", ",", "\n", "copy_mask", ":", "torch", ".", "Tensor", ",", "\n", ")", "->", "Tuple", "[", "torch", ".", "Tensor", ",", "torch", ".", "Tensor", "]", ":", "\n", "        ", "\"\"\"\n        Get the log-likelihood contribution from a single timestep.\n\n        # Parameters\n\n        generation_scores : `torch.Tensor`\n            Shape: `(batch_size, target_vocab_size)`\n        generation_scores_mask : `torch.Tensor`\n            Shape: `(batch_size, target_vocab_size)`. This is just a tensor of 1's.\n        copy_scores : `torch.Tensor`\n            Shape: `(batch_size, trimmed_source_length)`\n        target_tokens : `torch.Tensor`\n            Shape: `(batch_size,)`\n        target_to_source : `torch.Tensor`\n            Shape: `(batch_size, trimmed_source_length)`\n        copy_mask : `torch.Tensor`\n            Shape: `(batch_size, trimmed_source_length)`\n\n        # Returns\n\n        Tuple[torch.Tensor, torch.Tensor]\n            Shape: `(batch_size,), (batch_size, max_input_sequence_length)`\n        \"\"\"", "\n", "_", ",", "target_size", "=", "generation_scores", ".", "size", "(", ")", "\n", "\n", "# The point of this mask is to just mask out all source token scores", "\n", "# that just represent padding. We apply the mask to the concatenation", "\n", "# of the generation scores and the copy scores to normalize the scores", "\n", "# correctly during the softmax.", "\n", "# shape: (batch_size, target_vocab_size + trimmed_source_length)", "\n", "mask", "=", "torch", ".", "cat", "(", "(", "generation_scores_mask", ",", "copy_mask", ")", ",", "dim", "=", "-", "1", ")", "\n", "# shape: (batch_size, target_vocab_size + trimmed_source_length)", "\n", "all_scores", "=", "torch", ".", "cat", "(", "(", "generation_scores", ",", "copy_scores", ")", ",", "dim", "=", "-", "1", ")", "\n", "# Normalize generation and copy scores.", "\n", "# shape: (batch_size, target_vocab_size + trimmed_source_length)", "\n", "log_probs", "=", "util", ".", "masked_log_softmax", "(", "all_scores", ",", "mask", ")", "\n", "# Calculate the log probability (`copy_log_probs`) for each token in the source sentence", "\n", "# that matches the current target token. We use the sum of these copy probabilities", "\n", "# for matching tokens in the source sentence to get the total probability", "\n", "# for the target token. We also need to normalize the individual copy probabilities", "\n", "# to create `selective_weights`, which are used in the next timestep to create", "\n", "# a selective read state.", "\n", "# shape: (batch_size, trimmed_source_length)", "\n", "copy_log_probs", "=", "(", "\n", "log_probs", "[", ":", ",", "target_size", ":", "]", "+", "(", "target_to_source", ".", "float", "(", ")", "+", "1e-45", ")", ".", "log", "(", ")", "\n", ")", "\n", "# Since `log_probs[:, target_size]` gives us the raw copy log probabilities,", "\n", "# we use a non-log softmax to get the normalized non-log copy probabilities.", "\n", "selective_weights", "=", "util", ".", "masked_softmax", "(", "\n", "log_probs", "[", ":", ",", "target_size", ":", "]", ",", "target_to_source", "\n", ")", "\n", "# This mask ensures that item in the batch has a non-zero generation probabilities", "\n", "# for this timestep only when the gold target token is not OOV or there are no", "\n", "# matching tokens in the source sentence.", "\n", "# shape: (batch_size, 1)", "\n", "gen_mask", "=", "(", "\n", "(", "target_tokens", "!=", "self", ".", "_oov_index", ")", "|", "(", "target_to_source", ".", "sum", "(", "-", "1", ")", "==", "0", ")", "\n", ")", ".", "float", "(", ")", "\n", "log_gen_mask", "=", "(", "gen_mask", "+", "1e-45", ")", ".", "log", "(", ")", ".", "unsqueeze", "(", "-", "1", ")", "\n", "# Now we get the generation score for the gold target token.", "\n", "# shape: (batch_size, 1)", "\n", "generation_log_probs", "=", "(", "\n", "log_probs", ".", "gather", "(", "1", ",", "target_tokens", ".", "unsqueeze", "(", "1", ")", ")", "+", "log_gen_mask", "\n", ")", "\n", "# ... and add the copy score to get the step log likelihood.", "\n", "# shape: (batch_size, 1 + trimmed_source_length)", "\n", "combined_gen_and_copy", "=", "torch", ".", "cat", "(", "\n", "(", "generation_log_probs", ",", "copy_log_probs", ")", ",", "dim", "=", "-", "1", "\n", ")", "\n", "# shape: (batch_size,)", "\n", "step_log_likelihood", "=", "util", ".", "logsumexp", "(", "combined_gen_and_copy", ")", "\n", "\n", "return", "step_log_likelihood", ",", "selective_weights", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.encoder_decoders.copynet_seq2seq.CopyNetSeq2Seq._forward_loss": [[476, 575], ["[].size", "source_mask.new_full", "source_mask[].float", "state[].new_zeros", "state[].new_zeros", "state[].new_full", "range", "torch.cat", "allennlp.nn.util.get_text_field_mask", "target_mask[].float", "source_mask[].float.size", "source_mask[].float.size", "copynet_seq2seq.CopyNetSeq2Seq._decoder_step", "copynet_seq2seq.CopyNetSeq2Seq._get_generation_scores", "copynet_seq2seq.CopyNetSeq2Seq._get_copy_scores", "copynet_seq2seq.CopyNetSeq2Seq._get_ll_contrib", "step_log_likelihoods.append", "step_log_likelihood.unsqueeze", "log_likelihood.sum", "target_token_ids[].unsqueeze", "state[].new_zeros.sum"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.nn.util.get_text_field_mask", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.encoder_decoders.copynet_seq2seq.CopyNetSeq2Seq._decoder_step", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.encoder_decoders.copynet_seq2seq.CopyNetSeq2Seq._get_generation_scores", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.encoder_decoders.copynet_seq2seq.CopyNetSeq2Seq._get_copy_scores", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.encoder_decoders.copynet_seq2seq.CopyNetSeq2Seq._get_ll_contrib"], ["", "def", "_forward_loss", "(", "\n", "self", ",", "\n", "target_tokens", ":", "TextFieldTensors", ",", "\n", "target_token_ids", ":", "torch", ".", "Tensor", ",", "\n", "state", ":", "Dict", "[", "str", ",", "torch", ".", "Tensor", "]", ",", "\n", ")", "->", "Dict", "[", "str", ",", "torch", ".", "Tensor", "]", ":", "\n", "        ", "\"\"\"\n        Calculate the loss against gold targets.\n        \"\"\"", "\n", "batch_size", ",", "target_sequence_length", "=", "target_tokens", "[", "\"tokens\"", "]", "[", "\"tokens\"", "]", ".", "size", "(", ")", "\n", "\n", "# shape: (batch_size, max_input_sequence_length)", "\n", "source_mask", "=", "state", "[", "\"source_mask\"", "]", "\n", "\n", "# The last input from the target is either padding or the end symbol.", "\n", "# Either way, we don't have to process it.", "\n", "num_decoding_steps", "=", "target_sequence_length", "-", "1", "\n", "# We use this to fill in the copy index when the previous input was copied.", "\n", "# shape: (batch_size,)", "\n", "copy_input_choices", "=", "source_mask", ".", "new_full", "(", "\n", "(", "batch_size", ",", ")", ",", "fill_value", "=", "self", ".", "_copy_index", "\n", ")", "\n", "# shape: (batch_size, trimmed_source_length)", "\n", "copy_mask", "=", "source_mask", "[", ":", ",", "1", ":", "-", "1", "]", ".", "float", "(", ")", "\n", "# We need to keep track of the probabilities assigned to tokens in the source", "\n", "# sentence that were copied during the previous timestep, since we use", "\n", "# those probabilities as weights when calculating the \"selective read\".", "\n", "# shape: (batch_size, trimmed_source_length)", "\n", "selective_weights", "=", "state", "[", "\"decoder_hidden\"", "]", ".", "new_zeros", "(", "copy_mask", ".", "size", "(", ")", ")", "\n", "\n", "# Indicates which tokens in the source sentence match the current target token.", "\n", "# shape: (batch_size, trimmed_source_length)", "\n", "target_to_source", "=", "state", "[", "\"source_token_ids\"", "]", ".", "new_zeros", "(", "copy_mask", ".", "size", "(", ")", ")", "\n", "\n", "# This is just a tensor of ones which we use repeatedly in `self._get_ll_contrib`,", "\n", "# so we create it once here to avoid doing it over-and-over.", "\n", "generation_scores_mask", "=", "state", "[", "\"decoder_hidden\"", "]", ".", "new_full", "(", "\n", "(", "batch_size", ",", "self", ".", "_target_vocab_size", ")", ",", "fill_value", "=", "1.0", "\n", ")", "\n", "\n", "step_log_likelihoods", "=", "[", "]", "\n", "for", "timestep", "in", "range", "(", "num_decoding_steps", ")", ":", "\n", "# shape: (batch_size,)", "\n", "            ", "input_choices", "=", "target_tokens", "[", "\"tokens\"", "]", "[", "\"tokens\"", "]", "[", ":", ",", "timestep", "]", "\n", "# If the previous target token was copied, we use the special copy token.", "\n", "# But the end target token will always be THE end token, so we know", "\n", "# it was not copied.", "\n", "if", "timestep", "<", "num_decoding_steps", "-", "1", ":", "\n", "# Get mask tensor indicating which instances were copied.", "\n", "# shape: (batch_size,)", "\n", "                ", "copied", "=", "(", "\n", "(", "input_choices", "==", "self", ".", "_oov_index", ")", "&", "(", "target_to_source", ".", "sum", "(", "-", "1", ")", ">", "0", ")", "\n", ")", ".", "long", "(", ")", "\n", "# shape: (batch_size,)", "\n", "input_choices", "=", "(", "\n", "input_choices", "*", "(", "1", "-", "copied", ")", "+", "copy_input_choices", "*", "copied", "\n", ")", "\n", "# shape: (batch_size, trimmed_source_length)", "\n", "target_to_source", "=", "state", "[", "\"source_token_ids\"", "]", "==", "target_token_ids", "[", "\n", ":", ",", "timestep", "+", "1", "\n", "]", ".", "unsqueeze", "(", "-", "1", ")", "\n", "# Update the decoder state by taking a step through the RNN.", "\n", "", "state", "=", "self", ".", "_decoder_step", "(", "input_choices", ",", "selective_weights", ",", "state", ")", "\n", "# Get generation scores for each token in the target vocab.", "\n", "# shape: (batch_size, target_vocab_size)", "\n", "generation_scores", "=", "self", ".", "_get_generation_scores", "(", "state", ")", "\n", "# Get copy scores for each token in the source sentence, excluding the start", "\n", "# and end tokens.", "\n", "# shape: (batch_size, trimmed_source_length)", "\n", "copy_scores", "=", "self", ".", "_get_copy_scores", "(", "state", ")", "\n", "# shape: (batch_size,)", "\n", "step_target_tokens", "=", "target_tokens", "[", "\"tokens\"", "]", "[", "\"tokens\"", "]", "[", ":", ",", "timestep", "+", "1", "]", "\n", "step_log_likelihood", ",", "selective_weights", "=", "self", ".", "_get_ll_contrib", "(", "\n", "generation_scores", ",", "\n", "generation_scores_mask", ",", "\n", "copy_scores", ",", "\n", "step_target_tokens", ",", "\n", "target_to_source", ",", "\n", "copy_mask", ",", "\n", ")", "\n", "step_log_likelihoods", ".", "append", "(", "step_log_likelihood", ".", "unsqueeze", "(", "1", ")", ")", "\n", "\n", "# Gather step log-likelihoods.", "\n", "# shape: (batch_size, num_decoding_steps = target_sequence_length - 1)", "\n", "", "log_likelihoods", "=", "torch", ".", "cat", "(", "step_log_likelihoods", ",", "1", ")", "\n", "# Get target mask to exclude likelihood contributions from timesteps after", "\n", "# the END token.", "\n", "# shape: (batch_size, target_sequence_length)", "\n", "target_mask", "=", "util", ".", "get_text_field_mask", "(", "target_tokens", ")", "\n", "# The first timestep is just the START token, which is not included in the likelihoods.", "\n", "# shape: (batch_size, num_decoding_steps)", "\n", "target_mask", "=", "target_mask", "[", ":", ",", "1", ":", "]", ".", "float", "(", ")", "\n", "# Sum of step log-likelihoods.", "\n", "# shape: (batch_size,)", "\n", "log_likelihood", "=", "(", "log_likelihoods", "*", "target_mask", ")", ".", "sum", "(", "dim", "=", "-", "1", ")", "\n", "# The loss is the negative log-likelihood, averaged over the batch.", "\n", "loss", "=", "-", "log_likelihood", ".", "sum", "(", ")", "/", "batch_size", "\n", "\n", "return", "{", "\"loss\"", ":", "loss", "}", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.encoder_decoders.copynet_seq2seq.CopyNetSeq2Seq._forward_beam_search": [[576, 598], ["state[].size", "state[].new_full", "copynet_seq2seq.CopyNetSeq2Seq._beam_search.search", "state[].new_zeros"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.nn.beam_search.BeamSearch.search"], ["", "def", "_forward_beam_search", "(", "\n", "self", ",", "state", ":", "Dict", "[", "str", ",", "torch", ".", "Tensor", "]", "\n", ")", "->", "Dict", "[", "str", ",", "torch", ".", "Tensor", "]", ":", "\n", "        ", "batch_size", ",", "source_length", "=", "state", "[", "\"source_mask\"", "]", ".", "size", "(", ")", "\n", "trimmed_source_length", "=", "source_length", "-", "2", "\n", "# Initialize the copy scores to zero.", "\n", "state", "[", "\"copy_log_probs\"", "]", "=", "(", "\n", "state", "[", "\"decoder_hidden\"", "]", ".", "new_zeros", "(", "(", "batch_size", ",", "trimmed_source_length", ")", ")", "\n", "+", "1e-45", "\n", ")", ".", "log", "(", ")", "\n", "# shape: (batch_size,)", "\n", "start_predictions", "=", "state", "[", "\"source_mask\"", "]", ".", "new_full", "(", "\n", "(", "batch_size", ",", ")", ",", "fill_value", "=", "self", ".", "_start_index", "\n", ")", "\n", "# shape (all_top_k_predictions): (batch_size, beam_size, num_decoding_steps)", "\n", "# shape (log_probabilities): (batch_size, beam_size)", "\n", "all_top_k_predictions", ",", "log_probabilities", "=", "self", ".", "_beam_search", ".", "search", "(", "\n", "start_predictions", ",", "state", ",", "self", ".", "take_search_step", "\n", ")", "\n", "return", "{", "\n", "\"predicted_log_probs\"", ":", "log_probabilities", ",", "\n", "\"predictions\"", ":", "all_top_k_predictions", ",", "\n", "}", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.encoder_decoders.copynet_seq2seq.CopyNetSeq2Seq._get_input_and_selective_weights": [[600, 691], ["state[].size", "only_copied_mask.new_full", "last_predictions.unsqueeze().expand", "source_token_ids.gather", "allennlp.nn.util.masked_softmax", "adjusted_predictions.unsqueeze", "only_copied_mask.unsqueeze", "last_predictions.unsqueeze"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.nn.util.masked_softmax"], ["", "def", "_get_input_and_selective_weights", "(", "\n", "self", ",", "last_predictions", ":", "torch", ".", "LongTensor", ",", "state", ":", "Dict", "[", "str", ",", "torch", ".", "Tensor", "]", "\n", ")", "->", "Tuple", "[", "torch", ".", "LongTensor", ",", "torch", ".", "Tensor", "]", ":", "\n", "        ", "\"\"\"\n        Get input choices for the decoder and the selective copy weights.\n\n        The decoder input choices are simply the `last_predictions`, except for\n        target OOV predictions that were copied from source tokens, in which case\n        the prediction will be changed to the COPY symbol in the target namespace.\n\n        The selective weights are just the probabilities assigned to source\n        tokens that were copied, normalized to sum to 1. If no source tokens were copied,\n        there will be all zeros.\n\n        # Parameters\n\n        last_predictions : `torch.LongTensor`\n            Shape: `(group_size,)`\n        state : `Dict[str, torch.Tensor]`\n\n        # Returns\n\n        Tuple[torch.LongTensor, torch.Tensor]\n            `input_choices` (shape `(group_size,)`) and `selective_weights`\n            (shape `(group_size, trimmed_source_length)`).\n        \"\"\"", "\n", "group_size", ",", "trimmed_source_length", "=", "state", "[", "\"source_to_target\"", "]", ".", "size", "(", ")", "\n", "\n", "# This is a mask indicating which last predictions were copied from the", "\n", "# the source AND not in the target vocabulary (OOV).", "\n", "# (group_size,)", "\n", "only_copied_mask", "=", "(", "last_predictions", ">=", "self", ".", "_target_vocab_size", ")", ".", "long", "(", ")", "\n", "\n", "# If the last prediction was in the target vocab or OOV but not copied,", "\n", "# we use that as input, otherwise we use the COPY token.", "\n", "# shape: (group_size,)", "\n", "copy_input_choices", "=", "only_copied_mask", ".", "new_full", "(", "\n", "(", "group_size", ",", ")", ",", "fill_value", "=", "self", ".", "_copy_index", "\n", ")", "\n", "input_choices", "=", "(", "\n", "last_predictions", "*", "(", "1", "-", "only_copied_mask", ")", "\n", "+", "copy_input_choices", "*", "only_copied_mask", "\n", ")", "\n", "\n", "# In order to get the `selective_weights`, we need to find out which predictions", "\n", "# were copied or copied AND generated, which is the case when a prediction appears", "\n", "# in both the source sentence and the target vocab. But whenever a prediction", "\n", "# is in the target vocab (even if it also appeared in the source sentence),", "\n", "# its index will be the corresponding target vocab index, not its index in", "\n", "# the source sentence offset by the target vocab size. So we first", "\n", "# use `state[\"source_to_target\"]` to get an indicator of every source token", "\n", "# that matches the predicted target token.", "\n", "# shape: (group_size, trimmed_source_length)", "\n", "expanded_last_predictions", "=", "last_predictions", ".", "unsqueeze", "(", "-", "1", ")", ".", "expand", "(", "\n", "group_size", ",", "trimmed_source_length", "\n", ")", "\n", "# shape: (group_size, trimmed_source_length)", "\n", "source_copied_and_generated", "=", "(", "\n", "state", "[", "\"source_to_target\"", "]", "==", "expanded_last_predictions", "\n", ")", ".", "long", "(", ")", "\n", "\n", "# In order to get indicators for copied source tokens that are OOV with respect", "\n", "# to the target vocab, we'll make use of `state[\"source_token_ids\"]`.", "\n", "# First we adjust predictions relative to the start of the source tokens.", "\n", "# This makes sense because predictions for copied tokens are given by the index of the copied", "\n", "# token in the source sentence, offset by the size of the target vocabulary.", "\n", "# shape: (group_size,)", "\n", "adjusted_predictions", "=", "last_predictions", "-", "self", ".", "_target_vocab_size", "\n", "# The adjusted indices for items that were not copied will be negative numbers,", "\n", "# and therefore invalid. So we zero them out.", "\n", "adjusted_predictions", "=", "adjusted_predictions", "*", "only_copied_mask", "\n", "# shape: (group_size, trimmed_source_length)", "\n", "source_token_ids", "=", "state", "[", "\"source_token_ids\"", "]", "\n", "# shape: (group_size, trimmed_source_length)", "\n", "adjusted_prediction_ids", "=", "source_token_ids", ".", "gather", "(", "\n", "-", "1", ",", "adjusted_predictions", ".", "unsqueeze", "(", "-", "1", ")", "\n", ")", "\n", "# This mask will contain indicators for source tokens that were copied", "\n", "# during the last timestep.", "\n", "# shape: (group_size, trimmed_source_length)", "\n", "source_only_copied", "=", "(", "source_token_ids", "==", "adjusted_prediction_ids", ")", ".", "long", "(", ")", "\n", "# Since we zero'd-out indices for predictions that were not copied,", "\n", "# we need to zero out all entries of this mask corresponding to those predictions.", "\n", "source_only_copied", "=", "source_only_copied", "*", "only_copied_mask", ".", "unsqueeze", "(", "-", "1", ")", "\n", "\n", "# shape: (group_size, trimmed_source_length)", "\n", "mask", "=", "source_only_copied", "|", "source_copied_and_generated", "\n", "# shape: (group_size, trimmed_source_length)", "\n", "selective_weights", "=", "util", ".", "masked_softmax", "(", "state", "[", "\"copy_log_probs\"", "]", ",", "mask", ")", "\n", "\n", "return", "input_choices", ",", "selective_weights", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.encoder_decoders.copynet_seq2seq.CopyNetSeq2Seq._gather_final_log_probs": [[692, 796], ["state[].size", "range", "modified_log_probs_list.insert", "torch.cat", "copy_log_probs_to_add.unsqueeze.unsqueeze.unsqueeze", "generation_log_probs.scatter.scatter.gather", "allennlp.nn.util.logsumexp", "generation_log_probs.scatter.scatter.scatter", "modified_log_probs_list.append", "source_to_target_slice.unsqueeze", "torch.cat", "source_to_target_slice.unsqueeze", "allennlp.nn.util.logsumexp.unsqueeze", "torch.cat", "allennlp.nn.util.logsumexp", "left_over_copy_log_probs.unsqueeze", "source_token_ids[].unsqueeze", "allennlp.nn.util.logsumexp.unsqueeze", "source_token_ids[].unsqueeze", "source_previous_occurences.sum"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.nn.util.logsumexp", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.nn.util.logsumexp"], ["", "def", "_gather_final_log_probs", "(", "\n", "self", ",", "\n", "generation_log_probs", ":", "torch", ".", "Tensor", ",", "\n", "copy_log_probs", ":", "torch", ".", "Tensor", ",", "\n", "state", ":", "Dict", "[", "str", ",", "torch", ".", "Tensor", "]", ",", "\n", ")", "->", "torch", ".", "Tensor", ":", "\n", "        ", "\"\"\"\n        Combine copy probabilities with generation probabilities for matching tokens.\n\n        # Parameters\n\n        generation_log_probs : `torch.Tensor`\n            Shape: `(group_size, target_vocab_size)`\n        copy_log_probs : `torch.Tensor`\n            Shape: `(group_size, trimmed_source_length)`\n        state : `Dict[str, torch.Tensor]`\n\n        # Returns\n\n        torch.Tensor\n            Shape: `(group_size, target_vocab_size + trimmed_source_length)`.\n        \"\"\"", "\n", "_", ",", "trimmed_source_length", "=", "state", "[", "\"source_to_target\"", "]", ".", "size", "(", ")", "\n", "source_token_ids", "=", "state", "[", "\"source_token_ids\"", "]", "\n", "\n", "# shape: [(batch_size, *)]", "\n", "modified_log_probs_list", ":", "List", "[", "torch", ".", "Tensor", "]", "=", "[", "]", "\n", "for", "i", "in", "range", "(", "trimmed_source_length", ")", ":", "\n", "# shape: (group_size,)", "\n", "            ", "copy_log_probs_slice", "=", "copy_log_probs", "[", ":", ",", "i", "]", "\n", "# `source_to_target` is a matrix of shape (group_size, trimmed_source_length)", "\n", "# where element (i, j) is the vocab index of the target token that matches the jth", "\n", "# source token in the ith group, if there is one, or the index of the OOV symbol otherwise.", "\n", "# We'll use this to add copy scores to corresponding generation scores.", "\n", "# shape: (group_size,)", "\n", "source_to_target_slice", "=", "state", "[", "\"source_to_target\"", "]", "[", ":", ",", "i", "]", "\n", "# The OOV index in the source_to_target_slice indicates that the source", "\n", "# token is not in the target vocab, so we don't want to add that copy score", "\n", "# to the OOV token.", "\n", "copy_log_probs_to_add_mask", "=", "(", "\n", "source_to_target_slice", "!=", "self", ".", "_oov_index", "\n", ")", ".", "float", "(", ")", "\n", "copy_log_probs_to_add", "=", "(", "\n", "copy_log_probs_slice", "+", "(", "copy_log_probs_to_add_mask", "+", "1e-45", ")", ".", "log", "(", ")", "\n", ")", "\n", "# shape: (batch_size, 1)", "\n", "copy_log_probs_to_add", "=", "copy_log_probs_to_add", ".", "unsqueeze", "(", "-", "1", ")", "\n", "# shape: (batch_size, 1)", "\n", "selected_generation_log_probs", "=", "generation_log_probs", ".", "gather", "(", "\n", "1", ",", "source_to_target_slice", ".", "unsqueeze", "(", "-", "1", ")", "\n", ")", "\n", "combined_scores", "=", "util", ".", "logsumexp", "(", "\n", "torch", ".", "cat", "(", "(", "selected_generation_log_probs", ",", "copy_log_probs_to_add", ")", ",", "dim", "=", "1", ")", "\n", ")", "\n", "generation_log_probs", "=", "generation_log_probs", ".", "scatter", "(", "\n", "-", "1", ",", "source_to_target_slice", ".", "unsqueeze", "(", "-", "1", ")", ",", "combined_scores", ".", "unsqueeze", "(", "-", "1", ")", "\n", ")", "\n", "# We have to combine copy scores for duplicate source tokens so that", "\n", "# we can find the overall most likely source token. So, if this is the first", "\n", "# occurence of this particular source token, we add the log_probs from all other", "\n", "# occurences, otherwise we zero it out since it was already accounted for.", "\n", "if", "i", "<", "(", "trimmed_source_length", "-", "1", ")", ":", "\n", "# Sum copy scores from future occurences of source token.", "\n", "# shape: (group_size, trimmed_source_length - i)", "\n", "                ", "source_future_occurences", "=", "(", "\n", "source_token_ids", "[", ":", ",", "(", "i", "+", "1", ")", ":", "]", "\n", "==", "source_token_ids", "[", ":", ",", "i", "]", ".", "unsqueeze", "(", "-", "1", ")", "\n", ")", ".", "float", "(", ")", "# noqa", "\n", "# shape: (group_size, trimmed_source_length - i)", "\n", "future_copy_log_probs", "=", "(", "\n", "copy_log_probs", "[", ":", ",", "(", "i", "+", "1", ")", ":", "]", "\n", "+", "(", "source_future_occurences", "+", "1e-45", ")", ".", "log", "(", ")", "\n", ")", "\n", "# shape: (group_size, 1 + trimmed_source_length - i)", "\n", "combined", "=", "torch", ".", "cat", "(", "\n", "(", "copy_log_probs_slice", ".", "unsqueeze", "(", "-", "1", ")", ",", "future_copy_log_probs", ")", ",", "dim", "=", "-", "1", "\n", ")", "\n", "# shape: (group_size,)", "\n", "copy_log_probs_slice", "=", "util", ".", "logsumexp", "(", "combined", ")", "\n", "", "if", "i", ">", "0", ":", "\n", "# Remove copy log_probs that we have already accounted for.", "\n", "# shape: (group_size, i)", "\n", "                ", "source_previous_occurences", "=", "source_token_ids", "[", "\n", ":", ",", "0", ":", "i", "\n", "]", "==", "source_token_ids", "[", ":", ",", "i", "]", ".", "unsqueeze", "(", "-", "1", ")", "\n", "# shape: (group_size,)", "\n", "duplicate_mask", "=", "(", "source_previous_occurences", ".", "sum", "(", "dim", "=", "-", "1", ")", "==", "0", ")", ".", "float", "(", ")", "\n", "copy_log_probs_slice", "=", "(", "\n", "copy_log_probs_slice", "+", "(", "duplicate_mask", "+", "1e-45", ")", ".", "log", "(", ")", "\n", ")", "\n", "\n", "# Finally, we zero-out copy scores that we added to the generation scores", "\n", "# above so that we don't double-count them.", "\n", "# shape: (group_size,)", "\n", "", "left_over_copy_log_probs", "=", "(", "\n", "copy_log_probs_slice", "+", "(", "1.0", "-", "copy_log_probs_to_add_mask", "+", "1e-45", ")", ".", "log", "(", ")", "\n", ")", "\n", "modified_log_probs_list", ".", "append", "(", "left_over_copy_log_probs", ".", "unsqueeze", "(", "-", "1", ")", ")", "\n", "", "modified_log_probs_list", ".", "insert", "(", "0", ",", "generation_log_probs", ")", "\n", "\n", "# shape: (group_size, target_vocab_size + trimmed_source_length)", "\n", "modified_log_probs", "=", "torch", ".", "cat", "(", "modified_log_probs_list", ",", "dim", "=", "-", "1", ")", "\n", "\n", "return", "modified_log_probs", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.encoder_decoders.copynet_seq2seq.CopyNetSeq2Seq.take_search_step": [[797, 895], ["state[].size", "copynet_seq2seq.CopyNetSeq2Seq._get_input_and_selective_weights", "copynet_seq2seq.CopyNetSeq2Seq._decoder_step", "copynet_seq2seq.CopyNetSeq2Seq._get_generation_scores", "copynet_seq2seq.CopyNetSeq2Seq._get_copy_scores", "torch.cat", "[].float", "torch.cat", "allennlp.nn.util.masked_log_softmax", "allennlp.nn.util.masked_log_softmax.split", "copynet_seq2seq.CopyNetSeq2Seq._gather_final_log_probs", "copynet_seq2seq.CopyNetSeq2Seq.new_full", "copynet_seq2seq.CopyNetSeq2Seq.size"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.encoder_decoders.copynet_seq2seq.CopyNetSeq2Seq._get_input_and_selective_weights", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.encoder_decoders.copynet_seq2seq.CopyNetSeq2Seq._decoder_step", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.encoder_decoders.copynet_seq2seq.CopyNetSeq2Seq._get_generation_scores", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.encoder_decoders.copynet_seq2seq.CopyNetSeq2Seq._get_copy_scores", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.nn.util.masked_log_softmax", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.encoder_decoders.copynet_seq2seq.CopyNetSeq2Seq._gather_final_log_probs"], ["", "def", "take_search_step", "(", "\n", "self", ",", "last_predictions", ":", "torch", ".", "Tensor", ",", "state", ":", "Dict", "[", "str", ",", "torch", ".", "Tensor", "]", "\n", ")", "->", "Tuple", "[", "torch", ".", "Tensor", ",", "Dict", "[", "str", ",", "torch", ".", "Tensor", "]", "]", ":", "\n", "        ", "\"\"\"\n        Take step during beam search.\n\n        This function is what gets passed to the `BeamSearch.search` method. It takes\n        predictions from the last timestep and the current state and outputs\n        the log probabilities assigned to tokens for the next timestep, as well as the updated\n        state.\n\n        Since we are predicting tokens out of the extended vocab (target vocab + all unique\n        tokens from the source sentence), this is a little more complicated that just\n        making a forward pass through the model. The output log probs will have\n        shape `(group_size, target_vocab_size + trimmed_source_length)` so that each\n        token in the target vocab and source sentence are assigned a probability.\n\n        Note that copy scores are assigned to each source token based on their position, not unique value.\n        So if a token appears more than once in the source sentence, it will have more than one score.\n        Further, if a source token is also part of the target vocab, its final score\n        will be the sum of the generation and copy scores. Therefore, in order to\n        get the score for all tokens in the extended vocab at this step,\n        we have to combine copy scores for re-occuring source tokens and potentially\n        add them to the generation scores for the matching token in the target vocab, if\n        there is one.\n\n        So we can break down the final log probs output as the concatenation of two\n        matrices, A: `(group_size, target_vocab_size)`, and B: `(group_size, trimmed_source_length)`.\n        Matrix A contains the sum of the generation score and copy scores (possibly 0)\n        for each target token. Matrix B contains left-over copy scores for source tokens\n        that do NOT appear in the target vocab, with zeros everywhere else. But since\n        a source token may appear more than once in the source sentence, we also have to\n        sum the scores for each appearance of each unique source token. So matrix B\n        actually only has non-zero values at the first occurence of each source token\n        that is not in the target vocab.\n\n        # Parameters\n\n        last_predictions : `torch.Tensor`\n            Shape: `(group_size,)`\n\n        state : `Dict[str, torch.Tensor]`\n            Contains all state tensors necessary to produce generation and copy scores\n            for next step.\n\n        Notes\n        -----\n        `group_size` != `batch_size`. In fact, `group_size` = `batch_size * beam_size`.\n        \"\"\"", "\n", "_", ",", "trimmed_source_length", "=", "state", "[", "\"source_to_target\"", "]", ".", "size", "(", ")", "\n", "\n", "# Get input to the decoder RNN and the selective weights. `input_choices`", "\n", "# is the result of replacing target OOV tokens in `last_predictions` with the", "\n", "# copy symbol. `selective_weights` consist of the normalized copy probabilities", "\n", "# assigned to the source tokens that were copied. If no tokens were copied,", "\n", "# there will be all zeros.", "\n", "# shape: (group_size,), (group_size, trimmed_source_length)", "\n", "input_choices", ",", "selective_weights", "=", "self", ".", "_get_input_and_selective_weights", "(", "\n", "last_predictions", ",", "state", "\n", ")", "\n", "# Update the decoder state by taking a step through the RNN.", "\n", "state", "=", "self", ".", "_decoder_step", "(", "input_choices", ",", "selective_weights", ",", "state", ")", "\n", "# Get the un-normalized generation scores for each token in the target vocab.", "\n", "# shape: (group_size, target_vocab_size)", "\n", "generation_scores", "=", "self", ".", "_get_generation_scores", "(", "state", ")", "\n", "# Get the un-normalized copy scores for each token in the source sentence,", "\n", "# excluding the start and end tokens.", "\n", "# shape: (group_size, trimmed_source_length)", "\n", "copy_scores", "=", "self", ".", "_get_copy_scores", "(", "state", ")", "\n", "# Concat un-normalized generation and copy scores.", "\n", "# shape: (batch_size, target_vocab_size + trimmed_source_length)", "\n", "all_scores", "=", "torch", ".", "cat", "(", "(", "generation_scores", ",", "copy_scores", ")", ",", "dim", "=", "-", "1", ")", "\n", "# shape: (group_size, trimmed_source_length)", "\n", "copy_mask", "=", "state", "[", "\"source_mask\"", "]", "[", ":", ",", "1", ":", "-", "1", "]", ".", "float", "(", ")", "\n", "# shape: (batch_size, target_vocab_size + trimmed_source_length)", "\n", "mask", "=", "torch", ".", "cat", "(", "\n", "(", "generation_scores", ".", "new_full", "(", "generation_scores", ".", "size", "(", ")", ",", "1.0", ")", ",", "copy_mask", ")", ",", "\n", "dim", "=", "-", "1", ",", "\n", ")", "\n", "# Normalize generation and copy scores.", "\n", "# shape: (batch_size, target_vocab_size + trimmed_source_length)", "\n", "log_probs", "=", "util", ".", "masked_log_softmax", "(", "all_scores", ",", "mask", ")", "\n", "# shape: (group_size, target_vocab_size), (group_size, trimmed_source_length)", "\n", "generation_log_probs", ",", "copy_log_probs", "=", "log_probs", ".", "split", "(", "\n", "[", "self", ".", "_target_vocab_size", ",", "trimmed_source_length", "]", ",", "dim", "=", "-", "1", "\n", ")", "\n", "# Update copy_probs needed for getting the `selective_weights` at the next timestep.", "\n", "state", "[", "\"copy_log_probs\"", "]", "=", "copy_log_probs", "\n", "# We now have normalized generation and copy scores, but to produce the final", "\n", "# score for each token in the extended vocab, we have to go through and add", "\n", "# the copy scores to the generation scores of matching target tokens, and sum", "\n", "# the copy scores of duplicate source tokens.", "\n", "# shape: (group_size, target_vocab_size + trimmed_source_length)", "\n", "final_log_probs", "=", "self", ".", "_gather_final_log_probs", "(", "\n", "generation_log_probs", ",", "copy_log_probs", ",", "state", "\n", ")", "\n", "\n", "return", "final_log_probs", ",", "state", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.encoder_decoders.copynet_seq2seq.CopyNetSeq2Seq._get_predicted_tokens": [[896, 933], ["zip", "isinstance", "predicted_indices.detach().cpu().numpy.detach().cpu().numpy.detach().cpu().numpy", "list", "batch_predicted_tokens.append", "predicted_tokens.append", "predicted_tokens.append", "predicted_indices.detach().cpu().numpy.detach().cpu().numpy.detach().cpu", "tokens.append", "copynet_seq2seq.CopyNetSeq2Seq.vocab.get_token_from_index", "predicted_indices.detach().cpu().numpy.detach().cpu().numpy.detach", "list.index"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.data.vocabulary.Vocabulary.get_token_from_index", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.fields.text_field.TextField.index"], ["", "def", "_get_predicted_tokens", "(", "\n", "self", ",", "\n", "predicted_indices", ":", "Union", "[", "torch", ".", "Tensor", ",", "numpy", ".", "ndarray", "]", ",", "\n", "batch_metadata", ":", "List", "[", "Any", "]", ",", "\n", "n_best", ":", "int", "=", "None", ",", "\n", ")", "->", "List", "[", "Union", "[", "List", "[", "List", "[", "str", "]", "]", ",", "List", "[", "str", "]", "]", "]", ":", "\n", "        ", "\"\"\"\n        Convert predicted indices into tokens.\n\n        If `n_best = 1`, the result type will be `List[List[str]]`. Otherwise the result\n        type will be `List[List[List[str]]]`.\n        \"\"\"", "\n", "if", "not", "isinstance", "(", "predicted_indices", ",", "numpy", ".", "ndarray", ")", ":", "\n", "            ", "predicted_indices", "=", "predicted_indices", ".", "detach", "(", ")", ".", "cpu", "(", ")", ".", "numpy", "(", ")", "\n", "", "predicted_tokens", ":", "List", "[", "Union", "[", "List", "[", "List", "[", "str", "]", "]", ",", "List", "[", "str", "]", "]", "]", "=", "[", "]", "\n", "for", "top_k_predictions", ",", "metadata", "in", "zip", "(", "predicted_indices", ",", "batch_metadata", ")", ":", "\n", "            ", "batch_predicted_tokens", ":", "List", "[", "List", "[", "str", "]", "]", "=", "[", "]", "\n", "for", "indices", "in", "top_k_predictions", "[", ":", "n_best", "]", ":", "\n", "                ", "tokens", ":", "List", "[", "str", "]", "=", "[", "]", "\n", "indices", "=", "list", "(", "indices", ")", "\n", "if", "self", ".", "_end_index", "in", "indices", ":", "\n", "                    ", "indices", "=", "indices", "[", ":", "indices", ".", "index", "(", "self", ".", "_end_index", ")", "]", "\n", "", "for", "index", "in", "indices", ":", "\n", "                    ", "if", "index", ">=", "self", ".", "_target_vocab_size", ":", "\n", "                        ", "adjusted_index", "=", "index", "-", "self", ".", "_target_vocab_size", "\n", "token", "=", "metadata", "[", "\"source_tokens\"", "]", "[", "adjusted_index", "]", "\n", "", "else", ":", "\n", "                        ", "token", "=", "self", ".", "vocab", ".", "get_token_from_index", "(", "\n", "index", ",", "self", ".", "_target_namespace", "\n", ")", "\n", "", "tokens", ".", "append", "(", "token", ")", "\n", "", "batch_predicted_tokens", ".", "append", "(", "tokens", ")", "\n", "", "if", "n_best", "==", "1", ":", "\n", "                ", "predicted_tokens", ".", "append", "(", "batch_predicted_tokens", "[", "0", "]", ")", "\n", "", "else", ":", "\n", "                ", "predicted_tokens", ".", "append", "(", "batch_predicted_tokens", ")", "\n", "", "", "return", "predicted_tokens", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.encoder_decoders.copynet_seq2seq.CopyNetSeq2Seq.decode": [[934, 948], ["copynet_seq2seq.CopyNetSeq2Seq._get_predicted_tokens"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.encoder_decoders.copynet_seq2seq.CopyNetSeq2Seq._get_predicted_tokens"], ["", "@", "overrides", "\n", "def", "decode", "(", "self", ",", "output_dict", ":", "Dict", "[", "str", ",", "torch", ".", "Tensor", "]", ")", "->", "Dict", "[", "str", ",", "Any", "]", ":", "\n", "        ", "\"\"\"\n        Finalize predictions.\n\n        After a beam search, the predicted indices correspond to tokens in the target vocabulary\n        OR tokens in source sentence. Here we gather the actual tokens corresponding to\n        the indices.\n        \"\"\"", "\n", "predicted_tokens", "=", "self", ".", "_get_predicted_tokens", "(", "\n", "output_dict", "[", "\"predictions\"", "]", ",", "output_dict", "[", "\"metadata\"", "]", "\n", ")", "\n", "output_dict", "[", "\"predicted_tokens\"", "]", "=", "predicted_tokens", "\n", "return", "output_dict", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.encoder_decoders.copynet_seq2seq.CopyNetSeq2Seq.get_metrics": [[949, 960], ["all_metrics.update", "all_metrics.update", "copynet_seq2seq.CopyNetSeq2Seq._tensor_based_metric.get_metric", "copynet_seq2seq.CopyNetSeq2Seq._token_based_metric.get_metric"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.metrics.conll_coref_scores.Scorer.update", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.metrics.conll_coref_scores.Scorer.update", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.metrics.sequence_accuracy.SequenceAccuracy.get_metric", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.metrics.sequence_accuracy.SequenceAccuracy.get_metric"], ["", "@", "overrides", "\n", "def", "get_metrics", "(", "self", ",", "reset", ":", "bool", "=", "False", ")", "->", "Dict", "[", "str", ",", "float", "]", ":", "\n", "        ", "all_metrics", ":", "Dict", "[", "str", ",", "float", "]", "=", "{", "}", "\n", "if", "not", "self", ".", "training", ":", "\n", "            ", "if", "self", ".", "_tensor_based_metric", "is", "not", "None", ":", "\n", "                ", "all_metrics", ".", "update", "(", "\n", "self", ".", "_tensor_based_metric", ".", "get_metric", "(", "reset", "=", "reset", ")", "# type: ignore", "\n", ")", "\n", "", "if", "self", ".", "_token_based_metric", "is", "not", "None", ":", "\n", "                ", "all_metrics", ".", "update", "(", "self", ".", "_token_based_metric", ".", "get_metric", "(", "reset", "=", "reset", ")", ")", "# type: ignore", "\n", "", "", "return", "all_metrics", "\n", "", "", ""]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.encoder_decoders.composed_seq2seq.ComposedSeq2Seq.__init__": [[46, 99], ["allennlp.nn.InitializerApplicator", "allennlp.models.model.Model.__init__", "initializer", "composed_seq2seq.ComposedSeq2Seq._encoder.get_output_dim", "composed_seq2seq.ComposedSeq2Seq._decoder.get_output_dim", "allennlp.common.checks.ConfigurationError", "isinstance", "allennlp.common.checks.ConfigurationError", "isinstance", "allennlp.common.checks.ConfigurationError", "source_embedder.get_output_dim", "composed_seq2seq.ComposedSeq2Seq._decoder.target_embedder.get_output_dim", "allennlp.common.checks.ConfigurationError", "composed_seq2seq.ComposedSeq2Seq._encoder.get_output_dim", "composed_seq2seq.ComposedSeq2Seq._decoder.get_output_dim"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.graph_util.plot.GraphPlot.__init__", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.seq2seq_decoders.auto_regressive_seq_decoder.AutoRegressiveSeqDecoder.get_output_dim", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.seq2seq_decoders.auto_regressive_seq_decoder.AutoRegressiveSeqDecoder.get_output_dim", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.seq2seq_decoders.auto_regressive_seq_decoder.AutoRegressiveSeqDecoder.get_output_dim", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.seq2seq_decoders.auto_regressive_seq_decoder.AutoRegressiveSeqDecoder.get_output_dim", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.seq2seq_decoders.auto_regressive_seq_decoder.AutoRegressiveSeqDecoder.get_output_dim", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.seq2seq_decoders.auto_regressive_seq_decoder.AutoRegressiveSeqDecoder.get_output_dim"], ["def", "__init__", "(", "\n", "self", ",", "\n", "vocab", ":", "Vocabulary", ",", "\n", "source_text_embedder", ":", "TextFieldEmbedder", ",", "\n", "encoder", ":", "Seq2SeqEncoder", ",", "\n", "decoder", ":", "SeqDecoder", ",", "\n", "tied_source_embedder_key", ":", "Optional", "[", "str", "]", "=", "None", ",", "\n", "initializer", ":", "InitializerApplicator", "=", "InitializerApplicator", "(", ")", ",", "\n", "**", "kwargs", ",", "\n", ")", "->", "None", ":", "\n", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "vocab", ",", "**", "kwargs", ")", "\n", "\n", "self", ".", "_source_text_embedder", "=", "source_text_embedder", "\n", "self", ".", "_encoder", "=", "encoder", "\n", "self", ".", "_decoder", "=", "decoder", "\n", "\n", "if", "self", ".", "_encoder", ".", "get_output_dim", "(", ")", "!=", "self", ".", "_decoder", ".", "get_output_dim", "(", ")", ":", "\n", "            ", "raise", "ConfigurationError", "(", "\n", "f\"Encoder output dimension {self._encoder.get_output_dim()} should be\"", "\n", "f\" equal to decoder dimension {self._decoder.get_output_dim()}.\"", "\n", ")", "\n", "", "if", "tied_source_embedder_key", ":", "\n", "# A bit of a ugly hack to tie embeddings.", "\n", "# Works only for `BasicTextFieldEmbedder`, and since", "\n", "# it can have multiple embedders, and `SeqDecoder` contains only a single embedder, we need", "\n", "# the key to select the source embedder to replace it with the target embedder from the decoder.", "\n", "            ", "if", "not", "isinstance", "(", "self", ".", "_source_text_embedder", ",", "BasicTextFieldEmbedder", ")", ":", "\n", "                ", "raise", "ConfigurationError", "(", "\n", "\"Unable to tie embeddings,\"", "\n", "\"Source text embedder is not an instance of `BasicTextFieldEmbedder`.\"", "\n", ")", "\n", "\n", "", "source_embedder", "=", "self", ".", "_source_text_embedder", ".", "_token_embedders", "[", "\n", "tied_source_embedder_key", "\n", "]", "\n", "if", "not", "isinstance", "(", "source_embedder", ",", "Embedding", ")", ":", "\n", "                ", "raise", "ConfigurationError", "(", "\n", "\"Unable to tie embeddings,\"", "\n", "\"Selected source embedder is not an instance of `Embedding`.\"", "\n", ")", "\n", "", "if", "(", "\n", "source_embedder", ".", "get_output_dim", "(", ")", "\n", "!=", "self", ".", "_decoder", ".", "target_embedder", ".", "get_output_dim", "(", ")", "\n", ")", ":", "\n", "                ", "raise", "ConfigurationError", "(", "\n", "f\"Output Dimensions mismatch between\"", "\n", "f\"source embedder and target embedder.\"", "\n", ")", "\n", "", "self", ".", "_source_text_embedder", ".", "_token_embedders", "[", "\n", "tied_source_embedder_key", "\n", "]", "=", "self", ".", "_decoder", ".", "target_embedder", "\n", "", "initializer", "(", "self", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.encoder_decoders.composed_seq2seq.ComposedSeq2Seq.forward": [[100, 127], ["composed_seq2seq.ComposedSeq2Seq._encode", "composed_seq2seq.ComposedSeq2Seq._decoder"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.encoder_decoders.simple_seq2seq.SimpleSeq2Seq._encode"], ["", "@", "overrides", "\n", "def", "forward", "(", "\n", "self", ",", "# type: ignore", "\n", "source_tokens", ":", "TextFieldTensors", ",", "\n", "target_tokens", ":", "TextFieldTensors", "=", "None", ",", "\n", ")", "->", "Dict", "[", "str", ",", "torch", ".", "Tensor", "]", ":", "\n", "\n", "        ", "\"\"\"\n        Make foward pass on the encoder and decoder for producing the entire target sequence.\n\n        # Parameters\n\n        source_tokens : `TextFieldTensors`\n           The output of `TextField.as_array()` applied on the source `TextField`. This will be\n           passed through a `TextFieldEmbedder` and then through an encoder.\n        target_tokens : `TextFieldTensors`, optional (default = None)\n           Output of `Textfield.as_array()` applied on target `TextField`. We assume that the\n           target tokens are also represented as a `TextField`.\n\n        # Returns\n\n        Dict[str, torch.Tensor]\n            The output tensors from the decoder.\n        \"\"\"", "\n", "state", "=", "self", ".", "_encode", "(", "source_tokens", ")", "\n", "\n", "return", "self", ".", "_decoder", "(", "state", ",", "target_tokens", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.encoder_decoders.composed_seq2seq.ComposedSeq2Seq.decode": [[128, 134], ["composed_seq2seq.ComposedSeq2Seq._decoder.post_process"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.seq2seq_decoders.auto_regressive_seq_decoder.AutoRegressiveSeqDecoder.post_process"], ["", "@", "overrides", "\n", "def", "decode", "(", "self", ",", "output_dict", ":", "Dict", "[", "str", ",", "torch", ".", "Tensor", "]", ")", "->", "Dict", "[", "str", ",", "torch", ".", "Tensor", "]", ":", "\n", "        ", "\"\"\"\n        Finalize predictions.\n        \"\"\"", "\n", "return", "self", ".", "_decoder", ".", "post_process", "(", "output_dict", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.encoder_decoders.composed_seq2seq.ComposedSeq2Seq._encode": [[135, 162], ["composed_seq2seq.ComposedSeq2Seq._source_text_embedder", "allennlp.nn.util.get_text_field_mask", "composed_seq2seq.ComposedSeq2Seq._encoder"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.nn.util.get_text_field_mask"], ["", "def", "_encode", "(", "\n", "self", ",", "source_tokens", ":", "Dict", "[", "str", ",", "torch", ".", "Tensor", "]", "\n", ")", "->", "Dict", "[", "str", ",", "torch", ".", "Tensor", "]", ":", "\n", "        ", "\"\"\"\n        Make foward pass on the encoder.\n\n        # Parameters\n\n        source_tokens : `TextFieldTensors`\n           The output of `TextField.as_array()` applied on the source `TextField`. This will be\n           passed through a `TextFieldEmbedder` and then through an encoder.\n\n        # Returns\n\n        Dict[str, torch.Tensor]\n            Map consisting of the key `source_mask` with the mask over the\n            `source_tokens` text field,\n            and the key `encoder_outputs` with the output tensor from\n            forward pass on the encoder.\n        \"\"\"", "\n", "# shape: (batch_size, max_input_sequence_length, encoder_input_dim)", "\n", "embedded_input", "=", "self", ".", "_source_text_embedder", "(", "source_tokens", ")", "\n", "# shape: (batch_size, max_input_sequence_length)", "\n", "source_mask", "=", "util", ".", "get_text_field_mask", "(", "source_tokens", ")", "\n", "# shape: (batch_size, max_input_sequence_length, encoder_output_dim)", "\n", "encoder_outputs", "=", "self", ".", "_encoder", "(", "embedded_input", ",", "source_mask", ")", "\n", "return", "{", "\"source_mask\"", ":", "source_mask", ",", "\"encoder_outputs\"", ":", "encoder_outputs", "}", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.encoder_decoders.composed_seq2seq.ComposedSeq2Seq.get_metrics": [[163, 166], ["composed_seq2seq.ComposedSeq2Seq._decoder.get_metrics"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.coreference_resolution.coref.CoreferenceResolver.get_metrics"], ["", "@", "overrides", "\n", "def", "get_metrics", "(", "self", ",", "reset", ":", "bool", "=", "False", ")", "->", "Dict", "[", "str", ",", "float", "]", ":", "\n", "        ", "return", "self", ".", "_decoder", ".", "get_metrics", "(", "reset", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.encoder_decoders.simple_seq2seq.SimpleSeq2Seq.__init__": [[74, 165], ["allennlp.models.model.Model.__init__", "simple_seq2seq.SimpleSeq2Seq.vocab.get_token_index", "simple_seq2seq.SimpleSeq2Seq.vocab.get_token_index", "allennlp.nn.beam_search.BeamSearch", "simple_seq2seq.SimpleSeq2Seq.vocab.get_vocab_size", "allennlp.modules.token_embedders.Embedding", "simple_seq2seq.SimpleSeq2Seq._encoder.get_output_dim", "torch.nn.modules.rnn.LSTMCell", "torch.nn.modules.rnn.LSTMCell", "torch.nn.modules.linear.Linear", "torch.nn.modules.linear.Linear", "simple_seq2seq.SimpleSeq2Seq.vocab.get_token_index", "allennlp.training.metrics.BLEU", "source_embedder.get_output_dim", "allennlp.common.checks.ConfigurationError", "allennlp.modules.attention.LegacyAttention"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.graph_util.plot.GraphPlot.__init__", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.data.vocabulary.Vocabulary.get_token_index", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.data.vocabulary.Vocabulary.get_token_index", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.data.vocabulary.Vocabulary.get_vocab_size", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.seq2seq_decoders.auto_regressive_seq_decoder.AutoRegressiveSeqDecoder.get_output_dim", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.data.vocabulary.Vocabulary.get_token_index", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.seq2seq_decoders.auto_regressive_seq_decoder.AutoRegressiveSeqDecoder.get_output_dim"], []], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.encoder_decoders.simple_seq2seq.SimpleSeq2Seq.take_step": [[166, 208], ["simple_seq2seq.SimpleSeq2Seq._prepare_output_projections", "torch.log_softmax", "torch.log_softmax"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.encoder_decoders.simple_seq2seq.SimpleSeq2Seq._prepare_output_projections"], []], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.encoder_decoders.simple_seq2seq.SimpleSeq2Seq.forward": [[209, 254], ["simple_seq2seq.SimpleSeq2Seq._encode", "simple_seq2seq.SimpleSeq2Seq._init_decoder_state", "simple_seq2seq.SimpleSeq2Seq._forward_loop", "simple_seq2seq.SimpleSeq2Seq._init_decoder_state", "simple_seq2seq.SimpleSeq2Seq._forward_beam_search", "simple_seq2seq.SimpleSeq2Seq.update", "simple_seq2seq.SimpleSeq2Seq._bleu"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.encoder_decoders.simple_seq2seq.SimpleSeq2Seq._encode", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.encoder_decoders.simple_seq2seq.SimpleSeq2Seq._init_decoder_state", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.encoder_decoders.simple_seq2seq.SimpleSeq2Seq._forward_loop", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.encoder_decoders.simple_seq2seq.SimpleSeq2Seq._init_decoder_state", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.encoder_decoders.simple_seq2seq.SimpleSeq2Seq._forward_beam_search", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.metrics.conll_coref_scores.Scorer.update"], []], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.encoder_decoders.simple_seq2seq.SimpleSeq2Seq.decode": [[255, 287], ["isinstance", "predicted_indices.detach().cpu().numpy.detach().cpu().numpy.detach().cpu().numpy", "list", "all_predicted_tokens.append", "len", "simple_seq2seq.SimpleSeq2Seq.vocab.get_token_from_index", "predicted_indices.detach().cpu().numpy.detach().cpu().numpy.detach().cpu", "list.index", "predicted_indices.detach().cpu().numpy.detach().cpu().numpy.detach"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.data.vocabulary.Vocabulary.get_token_from_index", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.fields.text_field.TextField.index"], []], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.encoder_decoders.simple_seq2seq.SimpleSeq2Seq._encode": [[288, 298], ["simple_seq2seq.SimpleSeq2Seq._source_embedder", "allennlp.nn.util.get_text_field_mask", "simple_seq2seq.SimpleSeq2Seq._encoder"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.nn.util.get_text_field_mask"], []], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.encoder_decoders.simple_seq2seq.SimpleSeq2Seq._init_decoder_state": [[299, 317], ["state[].size", "allennlp.nn.util.get_final_encoder_states", "state[].new_zeros", "simple_seq2seq.SimpleSeq2Seq._encoder.is_bidirectional"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.nn.util.get_final_encoder_states", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.seq2seq_encoders.qanet_encoder.QaNetEncoderBlock.is_bidirectional"], []], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.encoder_decoders.simple_seq2seq.SimpleSeq2Seq._forward_loop": [[318, 401], ["source_mask.new_full", "range", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "source_mask.size", "targets.size", "simple_seq2seq.SimpleSeq2Seq._prepare_output_projections", "step_logits.append", "torch.softmax", "torch.softmax", "torch.max", "torch.max", "torch.max", "torch.max", "step_predictions.append", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "allennlp.nn.util.get_text_field_mask", "simple_seq2seq.SimpleSeq2Seq._get_loss", "output_projections.unsqueeze", "source_mask.new_full.unsqueeze", "torch.rand().item", "torch.rand().item", "torch.rand().item", "torch.rand().item", "torch.rand", "torch.rand", "torch.rand", "torch.rand"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.encoder_decoders.simple_seq2seq.SimpleSeq2Seq._prepare_output_projections", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.nn.util.get_text_field_mask", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.encoder_decoders.simple_seq2seq.SimpleSeq2Seq._get_loss"], []], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.encoder_decoders.simple_seq2seq.SimpleSeq2Seq._forward_beam_search": [[402, 422], ["state[].new_full", "simple_seq2seq.SimpleSeq2Seq._beam_search.search", "state[].size"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.nn.beam_search.BeamSearch.search"], []], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.encoder_decoders.simple_seq2seq.SimpleSeq2Seq._prepare_output_projections": [[423, 473], ["simple_seq2seq.SimpleSeq2Seq._target_embedder", "simple_seq2seq.SimpleSeq2Seq._decoder_cell", "simple_seq2seq.SimpleSeq2Seq._output_projection_layer", "simple_seq2seq.SimpleSeq2Seq._prepare_attended_input", "torch.cat", "torch.cat", "torch.cat", "torch.cat"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.encoder_decoders.simple_seq2seq.SimpleSeq2Seq._prepare_attended_input"], []], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.encoder_decoders.simple_seq2seq.SimpleSeq2Seq._prepare_attended_input": [[474, 495], ["encoder_outputs_mask.float.float.float", "simple_seq2seq.SimpleSeq2Seq._attention", "allennlp.nn.util.weighted_sum"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.nn.util.weighted_sum"], []], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.encoder_decoders.simple_seq2seq.SimpleSeq2Seq._get_loss": [[496, 535], ["targets[].contiguous", "target_mask[].contiguous", "allennlp.nn.util.sequence_cross_entropy_with_logits"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.nn.util.sequence_cross_entropy_with_logits"], []], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.encoder_decoders.simple_seq2seq.SimpleSeq2Seq.get_metrics": [[537, 543], ["all_metrics.update", "simple_seq2seq.SimpleSeq2Seq._bleu.get_metric"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.metrics.conll_coref_scores.Scorer.update", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.metrics.sequence_accuracy.SequenceAccuracy.get_metric"], []], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.coreference_resolution.coref.CoreferenceResolver.__init__": [[65, 120], ["allennlp.nn.InitializerApplicator", "allennlp.models.model.Model.__init__", "allennlp.modules.TimeDistributed", "torch.nn.Sequential", "torch.nn.Sequential", "torch.nn.Sequential", "torch.nn.Sequential", "allennlp.modules.Pruner", "allennlp.modules.TimeDistributed", "allennlp.modules.span_extractors.EndpointSpanExtractor", "allennlp.modules.span_extractors.SelfAttentiveSpanExtractor", "allennlp.modules.token_embedders.Embedding", "allennlp.training.metrics.MentionRecall", "allennlp.training.metrics.ConllCorefScores", "initializer", "allennlp.modules.TimeDistributed", "allennlp.modules.TimeDistributed", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "context_layer.get_output_dim", "torch.nn.Dropout", "torch.nn.Dropout", "torch.nn.Dropout", "torch.nn.Dropout", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "antecedent_feedforward.get_output_dim", "text_field_embedder.get_output_dim", "mention_feedforward.get_output_dim"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.graph_util.plot.GraphPlot.__init__", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.seq2seq_decoders.auto_regressive_seq_decoder.AutoRegressiveSeqDecoder.get_output_dim", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.seq2seq_decoders.auto_regressive_seq_decoder.AutoRegressiveSeqDecoder.get_output_dim", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.seq2seq_decoders.auto_regressive_seq_decoder.AutoRegressiveSeqDecoder.get_output_dim", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.seq2seq_decoders.auto_regressive_seq_decoder.AutoRegressiveSeqDecoder.get_output_dim"], ["\n", "return", "self", ".", "predict_json", "(", "{", "\"document\"", ":", "document", "}", ")", "\n", "\n", "", "def", "predict_tokenized", "(", "self", ",", "tokenized_document", ":", "List", "[", "str", "]", ")", "->", "JsonDict", ":", "\n", "        ", "\"\"\"\n        Predict the coreference clusters in the given document.\n\n        # Parameters\n\n        tokenized_document : `List[str]`\n            A list of words representation of a tokenized document.\n\n        # Returns\n\n        A dictionary representation of the predicted coreference clusters.\n        \"\"\"", "\n", "instance", "=", "self", ".", "_words_list_to_instance", "(", "tokenized_document", ")", "\n", "return", "self", ".", "predict_instance", "(", "instance", ")", "\n", "\n", "", "@", "overrides", "\n", "def", "predictions_to_labeled_instances", "(", "\n", "self", ",", "instance", ":", "Instance", ",", "outputs", ":", "Dict", "[", "str", ",", "numpy", ".", "ndarray", "]", "\n", ")", "->", "List", "[", "Instance", "]", ":", "\n", "        ", "\"\"\"\n        Takes each predicted cluster and makes it into a labeled `Instance` with only that\n        cluster labeled, so we can compute gradients of the loss `on the model's prediction of that\n        cluster`.  This lets us run interpretation methods using those gradients.  See superclass\n        docstring for more info.\n        \"\"\"", "\n", "# Digging into an Instance makes mypy go crazy, because we have all kinds of things where", "\n", "# the type has been lost.  So there are lots of `type: ignore`s here...", "\n", "predicted_clusters", "=", "outputs", "[", "\"clusters\"", "]", "\n", "span_field", ":", "ListField", "=", "instance", "[", "\"spans\"", "]", "# type: ignore", "\n", "instances", "=", "[", "]", "\n", "for", "cluster", "in", "predicted_clusters", ":", "\n", "            ", "new_instance", "=", "deepcopy", "(", "instance", ")", "\n", "span_labels", "=", "[", "\n", "0", "if", "(", "span", ".", "span_start", ",", "span", ".", "span_end", ")", "in", "cluster", "else", "-", "1", "# type: ignore", "\n", "for", "span", "in", "span_field", "\n", "]", "# type: ignore", "\n", "new_instance", ".", "add_field", "(", "\n", "\"span_labels\"", ",", "\n", "SequenceLabelField", "(", "span_labels", ",", "span_field", ")", ",", "\n", "self", ".", "_model", ".", "vocab", ",", "\n", ")", "\n", "new_instance", "[", "\"metadata\"", "]", ".", "metadata", "[", "\"clusters\"", "]", "=", "[", "cluster", "]", "# type: ignore", "\n", "instances", ".", "append", "(", "new_instance", ")", "\n", "", "if", "not", "instances", ":", "\n", "# No predicted clusters; we just give an empty coref prediction.", "\n", "            ", "new_instance", "=", "deepcopy", "(", "instance", ")", "\n", "span_labels", "=", "[", "-", "1", "]", "*", "len", "(", "span_field", ")", "# type: ignore", "\n", "new_instance", ".", "add_field", "(", "\n", "\"span_labels\"", ",", "\n", "SequenceLabelField", "(", "span_labels", ",", "span_field", ")", ",", "\n", "self", ".", "_model", ".", "vocab", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.coreference_resolution.coref.CoreferenceResolver.forward": [[121, 341], ["coref.CoreferenceResolver._lexical_dropout", "coref.CoreferenceResolver.size", "torch.relu().long.size", "allennlp.nn.util.get_text_field_mask().float", "torch.relu().long", "torch.relu().long", "coref.CoreferenceResolver._context_layer", "coref.CoreferenceResolver._endpoint_span_extractor", "coref.CoreferenceResolver._attentive_span_extractor", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "int", "coref.CoreferenceResolver._mention_pruner", "top_span_mask.unsqueeze.unsqueeze.unsqueeze", "allennlp.nn.util.flatten_and_batch_shift_indices", "allennlp.nn.util.batched_index_select", "min", "coref.CoreferenceResolver._generate_valid_antecedents", "allennlp.nn.util.flattened_index_select", "allennlp.nn.util.flattened_index_select().squeeze", "coref.CoreferenceResolver._compute_span_pair_embeddings", "coref.CoreferenceResolver._compute_coreference_scores", "coref.CoreferenceResolver.max", "coref.CoreferenceResolver._text_field_embedder", "math.floor", "allennlp.nn.util.get_device_of", "allennlp.nn.util.batched_index_select", "allennlp.nn.util.flattened_index_select().squeeze", "valid_antecedent_log_mask.long", "coref.CoreferenceResolver._compute_antecedent_gold_labels", "allennlp.nn.util.masked_log_softmax", "coref.CoreferenceResolver._mention_recall", "coref.CoreferenceResolver._conll_coref_scores", "allennlp.nn.util.get_text_field_mask", "torch.relu", "torch.relu", "allennlp.nn.util.flattened_index_select", "span_labels.unsqueeze", "coref.CoreferenceResolver.log", "allennlp.nn.util.logsumexp().sum", "torch.relu().long.float", "allennlp.nn.util.flattened_index_select", "allennlp.nn.util.logsumexp"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.nn.util.flatten_and_batch_shift_indices", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.nn.util.batched_index_select", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.coreference_resolution.coref.CoreferenceResolver._generate_valid_antecedents", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.nn.util.flattened_index_select", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.coreference_resolution.coref.CoreferenceResolver._compute_span_pair_embeddings", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.coreference_resolution.coref.CoreferenceResolver._compute_coreference_scores", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.nn.util.get_device_of", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.nn.util.batched_index_select", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.coreference_resolution.coref.CoreferenceResolver._compute_antecedent_gold_labels", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.nn.util.masked_log_softmax", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.nn.util.get_text_field_mask", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.nn.util.flattened_index_select", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.nn.util.flattened_index_select", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.nn.util.logsumexp"], ["new_instance", "[", "\"metadata\"", "]", ".", "metadata", "[", "\"clusters\"", "]", "=", "[", "]", "# type: ignore", "\n", "instances", ".", "append", "(", "new_instance", ")", "\n", "", "return", "instances", "\n", "\n", "", "@", "staticmethod", "\n", "def", "replace_corefs", "(", "document", ":", "Doc", ",", "clusters", ":", "List", "[", "List", "[", "List", "[", "int", "]", "]", "]", ")", "->", "str", ":", "\n", "        ", "\"\"\"\n        Uses a list of coreference clusters to convert a spacy document into a\n        string, where each coreference is replaced by its main mention.\n        \"\"\"", "\n", "# Original tokens with correct whitespace", "\n", "resolved", "=", "list", "(", "tok", ".", "text_with_ws", "for", "tok", "in", "document", ")", "\n", "\n", "for", "cluster", "in", "clusters", ":", "\n", "# The main mention is the first item in the cluster", "\n", "            ", "mention_start", ",", "mention_end", "=", "cluster", "[", "0", "]", "[", "0", "]", ",", "cluster", "[", "0", "]", "[", "1", "]", "+", "1", "\n", "mention_span", "=", "document", "[", "mention_start", ":", "mention_end", "]", "\n", "\n", "# The coreferences are all items following the first in the cluster", "\n", "for", "coref", "in", "cluster", "[", "1", ":", "]", ":", "\n", "                ", "final_token", "=", "document", "[", "coref", "[", "1", "]", "]", "\n", "# In both of the following cases, the first token in the coreference", "\n", "# is replaced with the main mention, while all subsequent tokens", "\n", "# are masked out with \"\", so that they can be elimated from", "\n", "# the returned document during \"\".join(resolved).", "\n", "\n", "# The first case attempts to correctly handle possessive coreferences", "\n", "# by inserting \"'s\" between the mention and the final whitespace", "\n", "# These include my, his, her, their, our, etc.", "\n", "\n", "# Disclaimer: Grammar errors can occur when the main mention is plural,", "\n", "# e.g. \"zebras\" becomes \"zebras's\" because this case isn't", "\n", "# being explictly checked and handled.", "\n", "\n", "if", "final_token", ".", "tag_", "in", "[", "\"PRP$\"", ",", "\"POS\"", "]", ":", "\n", "                    ", "resolved", "[", "coref", "[", "0", "]", "]", "=", "(", "\n", "mention_span", ".", "text", "+", "\"'s\"", "+", "final_token", ".", "whitespace_", "\n", ")", "\n", "", "else", ":", "\n", "# If not possessive, then replace first token with main mention directly", "\n", "                    ", "resolved", "[", "coref", "[", "0", "]", "]", "=", "mention_span", ".", "text", "+", "final_token", ".", "whitespace_", "\n", "# Mask out remaining tokens", "\n", "", "for", "i", "in", "range", "(", "coref", "[", "0", "]", "+", "1", ",", "coref", "[", "1", "]", "+", "1", ")", ":", "\n", "                    ", "resolved", "[", "i", "]", "=", "\"\"", "\n", "\n", "", "", "", "return", "\"\"", ".", "join", "(", "resolved", ")", "\n", "\n", "", "def", "coref_resolved", "(", "self", ",", "document", ":", "str", ")", "->", "str", ":", "\n", "        ", "\"\"\"\n        Produce a document where each coreference is replaced by the its main mention\n\n        # Parameters\n\n        document : `str`\n            A string representation of a document.\n\n        # Returns\n\n        A string with each coference replaced by its main mention\n        \"\"\"", "\n", "\n", "spacy_document", "=", "self", ".", "_spacy", "(", "document", ")", "\n", "clusters", "=", "self", ".", "predict", "(", "document", ")", ".", "get", "(", "\"clusters\"", ")", "\n", "\n", "# Passing a document with no coreferences returns its original form", "\n", "if", "not", "clusters", ":", "\n", "            ", "return", "document", "\n", "\n", "", "return", "self", ".", "replace_corefs", "(", "spacy_document", ",", "clusters", ")", "\n", "\n", "", "def", "_words_list_to_instance", "(", "self", ",", "words", ":", "List", "[", "str", "]", ")", "->", "Instance", ":", "\n", "        ", "\"\"\"\n        Create an instance from words list represent an already tokenized document,\n        for skipping tokenization when that information already exist for the user\n        \"\"\"", "\n", "spacy_document", "=", "Doc", "(", "self", ".", "_spacy", ".", "vocab", ",", "words", "=", "words", ")", "\n", "for", "pipe", "in", "filter", "(", "None", ",", "self", ".", "_spacy", ".", "pipeline", ")", ":", "\n", "            ", "pipe", "[", "1", "]", "(", "spacy_document", ")", "\n", "\n", "", "sentences", "=", "[", "\n", "[", "token", ".", "text", "for", "token", "in", "sentence", "]", "for", "sentence", "in", "spacy_document", ".", "sents", "\n", "]", "\n", "instance", "=", "self", ".", "_dataset_reader", ".", "text_to_instance", "(", "sentences", ")", "\n", "return", "instance", "\n", "\n", "", "@", "overrides", "\n", "def", "_json_to_instance", "(", "self", ",", "json_dict", ":", "JsonDict", ")", "->", "Instance", ":", "\n", "        ", "\"\"\"\n        Expects JSON that looks like `{\"document\": \"string of document text\"}`\n        \"\"\"", "\n", "document", "=", "json_dict", "[", "\"document\"", "]", "\n", "spacy_document", "=", "self", ".", "_spacy", "(", "document", ")", "\n", "sentences", "=", "[", "\n", "[", "token", ".", "text", "for", "token", "in", "sentence", "]", "for", "sentence", "in", "spacy_document", ".", "sents", "\n", "]", "\n", "instance", "=", "self", ".", "_dataset_reader", ".", "text_to_instance", "(", "sentences", ")", "\n", "return", "instance", "\n", "", "", ""]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.coreference_resolution.coref.CoreferenceResolver.decode": [[342, 428], ["output_dict[].detach().cpu", "output_dict[].detach().cpu", "output_dict[].detach().cpu", "zip", "enumerate", "batch_clusters.append", "output_dict[].detach", "output_dict[].detach", "output_dict[].detach", "zip", "clusters[].append", "top_spans[].item", "top_spans[].item", "len", "clusters.append", "span[].item", "span[].item"], "methods", ["None"], []], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.coreference_resolution.coref.CoreferenceResolver.get_metrics": [[429, 441], ["coref.CoreferenceResolver._mention_recall.get_metric", "coref.CoreferenceResolver._conll_coref_scores.get_metric"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.metrics.sequence_accuracy.SequenceAccuracy.get_metric", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.metrics.sequence_accuracy.SequenceAccuracy.get_metric"], []], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.coreference_resolution.coref.CoreferenceResolver._generate_valid_antecedents": [[443, 510], ["allennlp.nn.util.get_range_vector().unsqueeze", "torch.relu().long", "torch.relu().long", "allennlp.nn.util.get_range_vector", "torch.relu", "torch.relu", "allennlp.nn.util.get_range_vector", "raw_antecedent_indices.float"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.nn.util.get_range_vector", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.nn.util.get_range_vector"], []], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.coreference_resolution.coref.CoreferenceResolver._compute_span_pair_embeddings": [[512, 580], ["top_span_embeddings.unsqueeze().expand_as", "coref.CoreferenceResolver._distance_embedding", "antecedent_distance_embeddings.expand.expand.unsqueeze", "antecedent_distance_embeddings.expand.expand.expand", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "allennlp.nn.util.bucket_values", "antecedent_embeddings.size", "antecedent_embeddings.size", "antecedent_embeddings.size", "antecedent_distance_embeddings.expand.expand.size", "top_span_embeddings.unsqueeze"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.nn.util.bucket_values"], []], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.coreference_resolution.coref.CoreferenceResolver._compute_antecedent_gold_labels": [[581, 623], ["top_span_labels.expand_as", "torch.cat", "torch.cat", "torch.cat", "torch.cat"], "methods", ["None"], []], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.coreference_resolution.coref.CoreferenceResolver._compute_coreference_scores": [[624, 675], ["coref.CoreferenceResolver._antecedent_scorer().squeeze", "coref.CoreferenceResolver.new_zeros", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "coref.CoreferenceResolver.size", "coref.CoreferenceResolver.size", "coref.CoreferenceResolver._antecedent_scorer", "coref.CoreferenceResolver._antecedent_feedforward"], "methods", ["None"], []], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.coreference_resolution.winobias.WinobiasReader.__init__": [[59, 68], ["allennlp.data.dataset_readers.dataset_reader.DatasetReader.__init__", "allennlp.data.token_indexers.SingleIdTokenIndexer"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.graph_util.plot.GraphPlot.__init__"], ["def", "__init__", "(", "\n", "self", ",", "\n", "max_span_width", ":", "int", ",", "\n", "token_indexers", ":", "Dict", "[", "str", ",", "TokenIndexer", "]", "=", "None", ",", "\n", "**", "kwargs", ",", "\n", ")", "->", "None", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "**", "kwargs", ")", "\n", "self", ".", "_max_span_width", "=", "max_span_width", "\n", "self", ".", "_token_indexers", "=", "token_indexers", "or", "{", "\"tokens\"", ":", "SingleIdTokenIndexer", "(", ")", "}", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.coreference_resolution.winobias.WinobiasReader._read": [[69, 108], ["open", "allennlp.common.file_utils.cached_path", "sentence.strip().split", "collections.defaultdict", "enumerate", "token.endswith", "winobias.WinobiasReader.text_to_instance", "sentence.strip", "clusters[].append", "clusters[].append", "words.append", "words.append", "words.append", "clusters[].append", "clusters[].append", "token.strip", "token.strip", "allennlp.data.tokenizers.Token", "clusters.values"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.file_utils.cached_path", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.coreference_resolution.conll.ConllCorefReader.text_to_instance"], ["", "@", "overrides", "\n", "def", "_read", "(", "self", ",", "file_path", ":", "str", ")", ":", "\n", "\n", "        ", "for", "sentence", "in", "open", "(", "cached_path", "(", "file_path", ")", ",", "\"r\"", ")", ":", "\n", "            ", "tokens", "=", "sentence", ".", "strip", "(", ")", ".", "split", "(", "\" \"", ")", "\n", "clusters", ":", "DefaultDict", "[", "int", ",", "List", "[", "Tuple", "[", "int", ",", "int", "]", "]", "]", "=", "collections", ".", "defaultdict", "(", "\n", "list", "\n", ")", "\n", "words", "=", "[", "]", "\n", "for", "index", ",", "token", "in", "enumerate", "(", "tokens", ")", ":", "\n", "# Coreference is annotated using [square brackets]", "\n", "# or (round brackets) around coreferent phrases.", "\n", "                ", "if", "\"[\"", "in", "token", "and", "\"]\"", "in", "token", ":", "\n", "                    ", "clusters", "[", "0", "]", ".", "append", "(", "(", "index", ",", "index", ")", ")", "\n", "", "elif", "\"[\"", "in", "token", ":", "\n", "                    ", "clusters", "[", "0", "]", ".", "append", "(", "(", "index", ",", "index", ")", ")", "\n", "", "elif", "\"]\"", "in", "token", ":", "\n", "                    ", "old_span", "=", "clusters", "[", "0", "]", "[", "-", "1", "]", "\n", "clusters", "[", "0", "]", "[", "-", "1", "]", "=", "(", "old_span", "[", "0", "]", ",", "index", ")", "\n", "\n", "", "if", "\"(\"", "in", "token", "and", "\")\"", "in", "token", ":", "\n", "                    ", "clusters", "[", "1", "]", ".", "append", "(", "(", "index", ",", "index", ")", ")", "\n", "", "elif", "\"(\"", "in", "token", ":", "\n", "                    ", "clusters", "[", "1", "]", ".", "append", "(", "(", "index", ",", "index", ")", ")", "\n", "", "elif", "\")\"", "in", "token", ":", "\n", "                    ", "old_span", "=", "clusters", "[", "1", "]", "[", "-", "1", "]", "\n", "clusters", "[", "1", "]", "[", "-", "1", "]", "=", "(", "old_span", "[", "0", "]", ",", "index", ")", "\n", "\n", "", "if", "token", ".", "endswith", "(", "\".\"", ")", ":", "\n", "# Winobias is tokenised, but not for full stops.", "\n", "# We'll just special case them here.", "\n", "                    ", "token", "=", "token", "[", ":", "-", "1", "]", "\n", "words", ".", "append", "(", "token", ".", "strip", "(", "\"[]()\"", ")", ")", "\n", "words", ".", "append", "(", "\".\"", ")", "\n", "", "else", ":", "\n", "                    ", "words", ".", "append", "(", "token", ".", "strip", "(", "\"[]()\"", ")", ")", "\n", "\n", "", "", "yield", "self", ".", "text_to_instance", "(", "\n", "[", "Token", "(", "x", ")", "for", "x", "in", "words", "]", ",", "[", "x", "for", "x", "in", "clusters", ".", "values", "(", ")", "]", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.coreference_resolution.winobias.WinobiasReader.text_to_instance": [[110, 179], ["allennlp.data.fields.TextField", "allennlp.data.dataset_readers.dataset_utils.enumerate_spans", "allennlp.data.fields.ListField", "allennlp.data.fields.MetadataField", "allennlp.data.instance.Instance", "enumerate", "spans.append", "allennlp.data.fields.SequenceLabelField", "allennlp.data.fields.SpanField", "span_labels.append", "span_labels.append", "tuple"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.dataset_utils.span_utils.enumerate_spans"], ["", "", "@", "overrides", "\n", "def", "text_to_instance", "(", "\n", "self", ",", "# type: ignore", "\n", "sentence", ":", "List", "[", "Token", "]", ",", "\n", "gold_clusters", ":", "Optional", "[", "List", "[", "List", "[", "Tuple", "[", "int", ",", "int", "]", "]", "]", "]", "=", "None", ",", "\n", ")", "->", "Instance", ":", "\n", "\n", "        ", "\"\"\"\n        # Parameters\n\n        sentence : `List[Token]`, required.\n            The already tokenised sentence to analyse.\n        gold_clusters : `Optional[List[List[Tuple[int, int]]]]`, optional (default = None)\n            A list of all clusters in the sentence, represented as word spans. Each cluster\n            contains some number of spans, which can be nested and overlap, but will never\n            exactly match between clusters.\n\n        # Returns\n\n        An `Instance` containing the following `Fields`:\n            text : `TextField`\n                The text of the full sentence.\n            spans : `ListField[SpanField]`\n                A ListField containing the spans represented as `SpanFields`\n                with respect to the sentence text.\n            span_labels : `SequenceLabelField`, optional\n                The id of the cluster which each possible span belongs to, or -1 if it does\n                 not belong to a cluster. As these labels have variable length (it depends on\n                 how many spans we are considering), we represent this a as a `SequenceLabelField`\n                 with respect to the `spans `ListField`.\n        \"\"\"", "\n", "metadata", ":", "Dict", "[", "str", ",", "Any", "]", "=", "{", "\"original_text\"", ":", "sentence", "}", "\n", "if", "gold_clusters", "is", "not", "None", ":", "\n", "            ", "metadata", "[", "\"clusters\"", "]", "=", "gold_clusters", "\n", "\n", "", "text_field", "=", "TextField", "(", "sentence", ",", "self", ".", "_token_indexers", ")", "\n", "\n", "cluster_dict", "=", "{", "}", "\n", "if", "gold_clusters", "is", "not", "None", ":", "\n", "            ", "for", "cluster_id", ",", "cluster", "in", "enumerate", "(", "gold_clusters", ")", ":", "\n", "                ", "for", "mention", "in", "cluster", ":", "\n", "                    ", "cluster_dict", "[", "tuple", "(", "mention", ")", "]", "=", "cluster_id", "\n", "\n", "", "", "", "spans", ":", "List", "[", "Field", "]", "=", "[", "]", "\n", "span_labels", ":", "Optional", "[", "List", "[", "int", "]", "]", "=", "[", "]", "if", "gold_clusters", "is", "not", "None", "else", "None", "\n", "\n", "for", "start", ",", "end", "in", "enumerate_spans", "(", "\n", "sentence", ",", "max_span_width", "=", "self", ".", "_max_span_width", "\n", ")", ":", "\n", "            ", "if", "span_labels", "is", "not", "None", ":", "\n", "                ", "if", "(", "start", ",", "end", ")", "in", "cluster_dict", ":", "\n", "                    ", "span_labels", ".", "append", "(", "cluster_dict", "[", "(", "start", ",", "end", ")", "]", ")", "\n", "", "else", ":", "\n", "                    ", "span_labels", ".", "append", "(", "-", "1", ")", "\n", "\n", "", "", "spans", ".", "append", "(", "SpanField", "(", "start", ",", "end", ",", "text_field", ")", ")", "\n", "\n", "", "span_field", "=", "ListField", "(", "spans", ")", "\n", "metadata_field", "=", "MetadataField", "(", "metadata", ")", "\n", "\n", "fields", ":", "Dict", "[", "str", ",", "Field", "]", "=", "{", "\n", "\"text\"", ":", "text_field", ",", "\n", "\"spans\"", ":", "span_field", ",", "\n", "\"metadata\"", ":", "metadata_field", ",", "\n", "}", "\n", "if", "span_labels", "is", "not", "None", ":", "\n", "            ", "fields", "[", "\"span_labels\"", "]", "=", "SequenceLabelField", "(", "span_labels", ",", "span_field", ")", "\n", "\n", "", "return", "Instance", "(", "fields", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.coreference_resolution.conll.ConllCorefReader.__init__": [[90, 101], ["allennlp.data.dataset_readers.dataset_reader.DatasetReader.__init__", "allennlp.data.token_indexers.SingleIdTokenIndexer"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.graph_util.plot.GraphPlot.__init__"], ["def", "__init__", "(", "\n", "self", ",", "\n", "max_span_width", ":", "int", ",", "\n", "token_indexers", ":", "Dict", "[", "str", ",", "TokenIndexer", "]", "=", "None", ",", "\n", "wordpiece_modeling_tokenizer", ":", "Optional", "[", "PretrainedTransformerTokenizer", "]", "=", "None", ",", "\n", "**", "kwargs", ",", "\n", ")", "->", "None", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "**", "kwargs", ")", "\n", "self", ".", "_max_span_width", "=", "max_span_width", "\n", "self", ".", "_token_indexers", "=", "token_indexers", "or", "{", "\"tokens\"", ":", "SingleIdTokenIndexer", "(", ")", "}", "\n", "self", ".", "_wordpiece_modeling_tokenizer", "=", "wordpiece_modeling_tokenizer", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.coreference_resolution.conll.ConllCorefReader._read": [[102, 126], ["allennlp.common.file_utils.cached_path", "allennlp.data.dataset_readers.dataset_utils.Ontonotes", "allennlp.data.dataset_readers.dataset_utils.Ontonotes.dataset_document_iterator", "collections.defaultdict", "conll.canonicalize_clusters", "len", "conll.ConllCorefReader.text_to_instance", "clusters[].append"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.file_utils.cached_path", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.dataset_utils.ontonotes.Ontonotes.dataset_document_iterator", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.coreference_resolution.conll.canonicalize_clusters", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.coreference_resolution.conll.ConllCorefReader.text_to_instance"], ["", "@", "overrides", "\n", "def", "_read", "(", "self", ",", "file_path", ":", "str", ")", ":", "\n", "# if `file_path` is a URL, redirect to the cache", "\n", "        ", "file_path", "=", "cached_path", "(", "file_path", ")", "\n", "\n", "ontonotes_reader", "=", "Ontonotes", "(", ")", "\n", "for", "sentences", "in", "ontonotes_reader", ".", "dataset_document_iterator", "(", "file_path", ")", ":", "\n", "            ", "clusters", ":", "DefaultDict", "[", "int", ",", "List", "[", "Tuple", "[", "int", ",", "int", "]", "]", "]", "=", "collections", ".", "defaultdict", "(", "\n", "list", "\n", ")", "\n", "\n", "total_tokens", "=", "0", "\n", "for", "sentence", "in", "sentences", ":", "\n", "                ", "for", "typed_span", "in", "sentence", ".", "coref_spans", ":", "\n", "# Coref annotations are on a _per sentence_", "\n", "# basis, so we need to adjust them to be relative", "\n", "# to the length of the document.", "\n", "                    ", "span_id", ",", "(", "start", ",", "end", ")", "=", "typed_span", "\n", "clusters", "[", "span_id", "]", ".", "append", "(", "(", "start", "+", "total_tokens", ",", "end", "+", "total_tokens", ")", ")", "\n", "", "total_tokens", "+=", "len", "(", "sentence", ".", "words", ")", "\n", "\n", "", "canonical_clusters", "=", "canonicalize_clusters", "(", "clusters", ")", "\n", "yield", "self", ".", "text_to_instance", "(", "\n", "[", "s", ".", "words", "for", "s", "in", "sentences", "]", ",", "canonical_clusters", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.coreference_resolution.conll.ConllCorefReader.text_to_instance": [[128, 246], ["allennlp.data.fields.TextField", "allennlp.data.fields.ListField", "allennlp.data.fields.MetadataField", "allennlp.data.instance.Instance", "conll.ConllCorefReader._normalize_word", "conll.ConllCorefReader._wordpiece_modeling_tokenizer.intra_word_tokenize", "enumerate", "allennlp.data.dataset_readers.dataset_utils.enumerate_spans", "len", "allennlp.data.fields.SequenceLabelField", "allennlp.data.tokenizers.Token", "spans.append", "enumerate", "allennlp.data.fields.SpanField", "span_labels.append", "span_labels.append", "tuple", "len"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.coreference_resolution.conll.ConllCorefReader._normalize_word", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.tokenizers.pretrained_transformer_tokenizer.PretrainedTransformerTokenizer.intra_word_tokenize", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.dataset_utils.span_utils.enumerate_spans"], ["", "", "@", "overrides", "\n", "def", "text_to_instance", "(", "\n", "self", ",", "# type: ignore", "\n", "sentences", ":", "List", "[", "List", "[", "str", "]", "]", ",", "\n", "gold_clusters", ":", "Optional", "[", "List", "[", "List", "[", "Tuple", "[", "int", ",", "int", "]", "]", "]", "]", "=", "None", ",", "\n", ")", "->", "Instance", ":", "\n", "\n", "        ", "\"\"\"\n        # Parameters\n\n        sentences : `List[List[str]]`, required.\n            A list of lists representing the tokenised words and sentences in the document.\n        gold_clusters : `Optional[List[List[Tuple[int, int]]]]`, optional (default = None)\n            A list of all clusters in the document, represented as word spans. Each cluster\n            contains some number of spans, which can be nested and overlap, but will never\n            exactly match between clusters.\n\n        # Returns\n\n        An `Instance` containing the following `Fields`:\n            text : `TextField`\n                The text of the full document.\n            spans : `ListField[SpanField]`\n                A ListField containing the spans represented as `SpanFields`\n                with respect to the document text.\n            span_labels : `SequenceLabelField`, optional\n                The id of the cluster which each possible span belongs to, or -1 if it does\n                 not belong to a cluster. As these labels have variable length (it depends on\n                 how many spans we are considering), we represent this a as a `SequenceLabelField`\n                 with respect to the `spans `ListField`.\n        \"\"\"", "\n", "flattened_sentences", "=", "[", "\n", "self", ".", "_normalize_word", "(", "word", ")", "for", "sentence", "in", "sentences", "for", "word", "in", "sentence", "\n", "]", "\n", "\n", "if", "self", ".", "_wordpiece_modeling_tokenizer", "is", "not", "None", ":", "\n", "            ", "(", "\n", "flat_sentences_tokens", ",", "\n", "offsets", ",", "\n", ")", "=", "self", ".", "_wordpiece_modeling_tokenizer", ".", "intra_word_tokenize", "(", "\n", "flattened_sentences", "\n", ")", "\n", "flattened_sentences", "=", "[", "t", ".", "text", "for", "t", "in", "flat_sentences_tokens", "]", "\n", "", "else", ":", "\n", "            ", "flat_sentences_tokens", "=", "[", "Token", "(", "word", ")", "for", "word", "in", "flattened_sentences", "]", "\n", "\n", "", "text_field", "=", "TextField", "(", "flat_sentences_tokens", ",", "self", ".", "_token_indexers", ")", "\n", "\n", "cluster_dict", "=", "{", "}", "\n", "if", "gold_clusters", "is", "not", "None", ":", "\n", "            ", "if", "self", ".", "_wordpiece_modeling_tokenizer", "is", "not", "None", ":", "\n", "                ", "for", "cluster", "in", "gold_clusters", ":", "\n", "                    ", "for", "mention_id", ",", "mention", "in", "enumerate", "(", "cluster", ")", ":", "\n", "                        ", "start", "=", "offsets", "[", "mention", "[", "0", "]", "]", "[", "0", "]", "\n", "end", "=", "offsets", "[", "mention", "[", "1", "]", "]", "[", "1", "]", "\n", "cluster", "[", "mention_id", "]", "=", "(", "start", ",", "end", ")", "\n", "\n", "", "", "", "for", "cluster_id", ",", "cluster", "in", "enumerate", "(", "gold_clusters", ")", ":", "\n", "                ", "for", "mention", "in", "cluster", ":", "\n", "                    ", "cluster_dict", "[", "tuple", "(", "mention", ")", "]", "=", "cluster_id", "\n", "\n", "", "", "", "spans", ":", "List", "[", "Field", "]", "=", "[", "]", "\n", "span_labels", ":", "Optional", "[", "List", "[", "int", "]", "]", "=", "[", "]", "if", "gold_clusters", "is", "not", "None", "else", "None", "\n", "\n", "sentence_offset", "=", "0", "\n", "for", "sentence", "in", "sentences", ":", "\n", "            ", "for", "start", ",", "end", "in", "enumerate_spans", "(", "\n", "sentence", ",", "offset", "=", "sentence_offset", ",", "max_span_width", "=", "self", ".", "_max_span_width", "\n", ")", ":", "\n", "                ", "if", "self", ".", "_wordpiece_modeling_tokenizer", "is", "not", "None", ":", "\n", "                    ", "start", "=", "offsets", "[", "start", "]", "[", "0", "]", "\n", "end", "=", "offsets", "[", "end", "]", "[", "1", "]", "\n", "\n", "# `enumerate_spans` uses word-level width limit; here we apply it to wordpieces", "\n", "# We have to do this check here because we use a span width embedding that has", "\n", "# only `self._max_span_width` entries, and since we are doing wordpiece", "\n", "# modeling, the span width embedding operates on wordpiece lengths. So a check", "\n", "# here is necessary or else we wouldn't know how many entries there would be.", "\n", "if", "end", "-", "start", "+", "1", ">", "self", ".", "_max_span_width", ":", "\n", "                        ", "continue", "\n", "# We also don't generate spans that contain special tokens", "\n", "", "if", "(", "\n", "start", "\n", "<", "self", ".", "_wordpiece_modeling_tokenizer", ".", "num_added_start_tokens", "\n", ")", ":", "\n", "                        ", "continue", "\n", "", "if", "(", "\n", "end", "\n", ">=", "len", "(", "flat_sentences_tokens", ")", "\n", "-", "self", ".", "_wordpiece_modeling_tokenizer", ".", "num_added_end_tokens", "\n", ")", ":", "\n", "                        ", "continue", "\n", "\n", "", "", "if", "span_labels", "is", "not", "None", ":", "\n", "                    ", "if", "(", "start", ",", "end", ")", "in", "cluster_dict", ":", "\n", "                        ", "span_labels", ".", "append", "(", "cluster_dict", "[", "(", "start", ",", "end", ")", "]", ")", "\n", "", "else", ":", "\n", "                        ", "span_labels", ".", "append", "(", "-", "1", ")", "\n", "\n", "", "", "spans", ".", "append", "(", "SpanField", "(", "start", ",", "end", ",", "text_field", ")", ")", "\n", "", "sentence_offset", "+=", "len", "(", "sentence", ")", "\n", "\n", "", "span_field", "=", "ListField", "(", "spans", ")", "\n", "\n", "metadata", ":", "Dict", "[", "str", ",", "Any", "]", "=", "{", "\"original_text\"", ":", "flattened_sentences", "}", "\n", "if", "gold_clusters", "is", "not", "None", ":", "\n", "            ", "metadata", "[", "\"clusters\"", "]", "=", "gold_clusters", "\n", "", "metadata_field", "=", "MetadataField", "(", "metadata", ")", "\n", "\n", "fields", ":", "Dict", "[", "str", ",", "Field", "]", "=", "{", "\n", "\"text\"", ":", "text_field", ",", "\n", "\"spans\"", ":", "span_field", ",", "\n", "\"metadata\"", ":", "metadata_field", ",", "\n", "}", "\n", "if", "span_labels", "is", "not", "None", ":", "\n", "            ", "fields", "[", "\"span_labels\"", "]", "=", "SequenceLabelField", "(", "span_labels", ",", "span_field", ")", "\n", "\n", "", "return", "Instance", "(", "fields", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.coreference_resolution.conll.ConllCorefReader._normalize_word": [[247, 253], ["None"], "methods", ["None"], ["", "@", "staticmethod", "\n", "def", "_normalize_word", "(", "word", ")", ":", "\n", "        ", "if", "word", "in", "(", "\"/.\"", ",", "\"/?\"", ")", ":", "\n", "            ", "return", "word", "[", "1", ":", "]", "\n", "", "else", ":", "\n", "            ", "return", "word", "\n", "", "", "", ""]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.coreference_resolution.conll.canonicalize_clusters": [[25, 57], ["clusters.values", "list", "cluster_with_overlapping_mention.update", "merged_clusters.append", "set"], "function", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.metrics.conll_coref_scores.Scorer.update"], ["def", "canonicalize_clusters", "(", "\n", "clusters", ":", "DefaultDict", "[", "int", ",", "List", "[", "Tuple", "[", "int", ",", "int", "]", "]", "]", "\n", ")", "->", "List", "[", "List", "[", "Tuple", "[", "int", ",", "int", "]", "]", "]", ":", "\n", "    ", "\"\"\"\n    The CONLL 2012 data includes 2 annotated spans which are identical,\n    but have different ids. This checks all clusters for spans which are\n    identical, and if it finds any, merges the clusters containing the\n    identical spans.\n    \"\"\"", "\n", "merged_clusters", ":", "List", "[", "Set", "[", "Tuple", "[", "int", ",", "int", "]", "]", "]", "=", "[", "]", "\n", "for", "cluster", "in", "clusters", ".", "values", "(", ")", ":", "\n", "        ", "cluster_with_overlapping_mention", "=", "None", "\n", "for", "mention", "in", "cluster", ":", "\n", "# Look at clusters we have already processed to", "\n", "# see if they contain a mention in the current", "\n", "# cluster for comparison.", "\n", "            ", "for", "cluster2", "in", "merged_clusters", ":", "\n", "                ", "if", "mention", "in", "cluster2", ":", "\n", "# first cluster in merged clusters", "\n", "# which contains this mention.", "\n", "                    ", "cluster_with_overlapping_mention", "=", "cluster2", "\n", "break", "\n", "# Already encountered overlap - no need to keep looking.", "\n", "", "", "if", "cluster_with_overlapping_mention", "is", "not", "None", ":", "\n", "                ", "break", "\n", "", "", "if", "cluster_with_overlapping_mention", "is", "not", "None", ":", "\n", "# Merge cluster we are currently processing into", "\n", "# the cluster in the processed list.", "\n", "            ", "cluster_with_overlapping_mention", ".", "update", "(", "cluster", ")", "\n", "", "else", ":", "\n", "            ", "merged_clusters", ".", "append", "(", "set", "(", "cluster", ")", ")", "\n", "", "", "return", "[", "list", "(", "c", ")", "for", "c", "in", "merged_clusters", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.data.vocabulary._NamespaceDependentDefaultDict.__init__": [[76, 86], ["set", "collections.defaultdict.__init__"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.graph_util.plot.GraphPlot.__init__"], ["def", "__init__", "(", "\n", "self", ",", "\n", "non_padded_namespaces", ":", "Iterable", "[", "str", "]", ",", "\n", "padded_function", ":", "Callable", "[", "[", "]", ",", "Any", "]", ",", "\n", "non_padded_function", ":", "Callable", "[", "[", "]", ",", "Any", "]", ",", "\n", ")", "->", "None", ":", "\n", "        ", "self", ".", "_non_padded_namespaces", "=", "set", "(", "non_padded_namespaces", ")", "\n", "self", ".", "_padded_function", "=", "padded_function", "\n", "self", ".", "_non_padded_function", "=", "non_padded_function", "\n", "super", "(", ")", ".", "__init__", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.data.vocabulary._NamespaceDependentDefaultDict.__missing__": [[87, 96], ["any", "dict.__setitem__", "vocabulary._NamespaceDependentDefaultDict._non_padded_function", "vocabulary._NamespaceDependentDefaultDict._padded_function", "allennlp.common.util.namespace_match"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.params.Params.__setitem__", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.util.namespace_match"], ["", "def", "__missing__", "(", "self", ",", "key", ":", "str", ")", ":", "\n", "        ", "if", "any", "(", "\n", "namespace_match", "(", "pattern", ",", "key", ")", "for", "pattern", "in", "self", ".", "_non_padded_namespaces", "\n", ")", ":", "\n", "            ", "value", "=", "self", ".", "_non_padded_function", "(", ")", "\n", "", "else", ":", "\n", "            ", "value", "=", "self", ".", "_padded_function", "(", ")", "\n", "", "dict", ".", "__setitem__", "(", "self", ",", "key", ",", "value", ")", "\n", "return", "value", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.data.vocabulary._NamespaceDependentDefaultDict.add_non_padded_namespaces": [[97, 100], ["vocabulary._NamespaceDependentDefaultDict._non_padded_namespaces.update"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.metrics.conll_coref_scores.Scorer.update"], ["", "def", "add_non_padded_namespaces", "(", "self", ",", "non_padded_namespaces", ":", "Set", "[", "str", "]", ")", ":", "\n", "# add non_padded_namespaces which weren't already present", "\n", "        ", "self", ".", "_non_padded_namespaces", ".", "update", "(", "non_padded_namespaces", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.data.vocabulary._TokenToIndexDefaultDict.__init__": [[103, 108], ["vocabulary._NamespaceDependentDefaultDict.__init__"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.graph_util.plot.GraphPlot.__init__"], ["    ", "def", "__init__", "(", "\n", "self", ",", "non_padded_namespaces", ":", "Set", "[", "str", "]", ",", "padding_token", ":", "str", ",", "oov_token", ":", "str", "\n", ")", "->", "None", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "\n", "non_padded_namespaces", ",", "lambda", ":", "{", "padding_token", ":", "0", ",", "oov_token", ":", "1", "}", ",", "lambda", ":", "{", "}", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.data.vocabulary._IndexToTokenDefaultDict.__init__": [[112, 117], ["vocabulary._NamespaceDependentDefaultDict.__init__"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.graph_util.plot.GraphPlot.__init__"], ["    ", "def", "__init__", "(", "\n", "self", ",", "non_padded_namespaces", ":", "Set", "[", "str", "]", ",", "padding_token", ":", "str", ",", "oov_token", ":", "str", "\n", ")", "->", "None", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "\n", "non_padded_namespaces", ",", "lambda", ":", "{", "0", ":", "padding_token", ",", "1", ":", "oov_token", "}", ",", "lambda", ":", "{", "}", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.data.vocabulary.Vocabulary.__init__": [[215, 254], ["set", "vocabulary._TokenToIndexDefaultDict", "vocabulary._IndexToTokenDefaultDict", "vocabulary.Vocabulary._extend"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.token_indexers.wordpiece_indexer.WordpieceIndexer._extend"], ["def", "__init__", "(", "\n", "self", ",", "\n", "counter", ":", "Dict", "[", "str", ",", "Dict", "[", "str", ",", "int", "]", "]", "=", "None", ",", "\n", "min_count", ":", "Dict", "[", "str", ",", "int", "]", "=", "None", ",", "\n", "max_vocab_size", ":", "Union", "[", "int", ",", "Dict", "[", "str", ",", "int", "]", "]", "=", "None", ",", "\n", "non_padded_namespaces", ":", "Iterable", "[", "str", "]", "=", "DEFAULT_NON_PADDED_NAMESPACES", ",", "\n", "pretrained_files", ":", "Optional", "[", "Dict", "[", "str", ",", "str", "]", "]", "=", "None", ",", "\n", "only_include_pretrained_words", ":", "bool", "=", "False", ",", "\n", "tokens_to_add", ":", "Dict", "[", "str", ",", "List", "[", "str", "]", "]", "=", "None", ",", "\n", "min_pretrained_embeddings", ":", "Dict", "[", "str", ",", "int", "]", "=", "None", ",", "\n", "padding_token", ":", "Optional", "[", "str", "]", "=", "DEFAULT_PADDING_TOKEN", ",", "\n", "oov_token", ":", "Optional", "[", "str", "]", "=", "DEFAULT_OOV_TOKEN", ",", "\n", ")", "->", "None", ":", "\n", "        ", "self", ".", "_padding_token", "=", "(", "\n", "padding_token", "if", "padding_token", "is", "not", "None", "else", "DEFAULT_PADDING_TOKEN", "\n", ")", "\n", "self", ".", "_oov_token", "=", "oov_token", "if", "oov_token", "is", "not", "None", "else", "DEFAULT_OOV_TOKEN", "\n", "\n", "self", ".", "_non_padded_namespaces", "=", "set", "(", "non_padded_namespaces", ")", "\n", "\n", "self", ".", "_token_to_index", "=", "_TokenToIndexDefaultDict", "(", "\n", "self", ".", "_non_padded_namespaces", ",", "self", ".", "_padding_token", ",", "self", ".", "_oov_token", "\n", ")", "\n", "self", ".", "_index_to_token", "=", "_IndexToTokenDefaultDict", "(", "\n", "self", ".", "_non_padded_namespaces", ",", "self", ".", "_padding_token", ",", "self", ".", "_oov_token", "\n", ")", "\n", "\n", "self", ".", "_retained_counter", ":", "Optional", "[", "Dict", "[", "str", ",", "Dict", "[", "str", ",", "int", "]", "]", "]", "=", "None", "\n", "\n", "# Made an empty vocabulary, now extend it.", "\n", "self", ".", "_extend", "(", "\n", "counter", ",", "\n", "min_count", ",", "\n", "max_vocab_size", ",", "\n", "non_padded_namespaces", ",", "\n", "pretrained_files", ",", "\n", "only_include_pretrained_words", ",", "\n", "tokens_to_add", ",", "\n", "min_pretrained_embeddings", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.data.vocabulary.Vocabulary.from_instances": [[256, 298], ["logger.info", "collections.defaultdict", "allennlp.common.tqdm.Tqdm.tqdm", "cls", "instance.count_vocab_items", "collections.defaultdict"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.dataset_readers.multiprocess_dataset_reader.logger.info", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.tqdm.Tqdm.tqdm", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.fields.text_field.TextField.count_vocab_items"], ["", "@", "classmethod", "\n", "def", "from_instances", "(", "\n", "cls", ",", "\n", "instances", ":", "Iterable", "[", "\"adi.Instance\"", "]", ",", "\n", "min_count", ":", "Dict", "[", "str", ",", "int", "]", "=", "None", ",", "\n", "max_vocab_size", ":", "Union", "[", "int", ",", "Dict", "[", "str", ",", "int", "]", "]", "=", "None", ",", "\n", "non_padded_namespaces", ":", "Iterable", "[", "str", "]", "=", "DEFAULT_NON_PADDED_NAMESPACES", ",", "\n", "pretrained_files", ":", "Optional", "[", "Dict", "[", "str", ",", "str", "]", "]", "=", "None", ",", "\n", "only_include_pretrained_words", ":", "bool", "=", "False", ",", "\n", "tokens_to_add", ":", "Dict", "[", "str", ",", "List", "[", "str", "]", "]", "=", "None", ",", "\n", "min_pretrained_embeddings", ":", "Dict", "[", "str", ",", "int", "]", "=", "None", ",", "\n", "padding_token", ":", "Optional", "[", "str", "]", "=", "DEFAULT_PADDING_TOKEN", ",", "\n", "oov_token", ":", "Optional", "[", "str", "]", "=", "DEFAULT_OOV_TOKEN", ",", "\n", ")", "->", "\"Vocabulary\"", ":", "\n", "        ", "\"\"\"\n        Constructs a vocabulary given a collection of `Instances` and some parameters.\n        We count all of the vocabulary items in the instances, then pass those counts\n        and the other parameters, to :func:`__init__`.  See that method for a description\n        of what the other parameters do.\n        \"\"\"", "\n", "logger", ".", "info", "(", "\"Fitting token dictionary from dataset.\"", ")", "\n", "padding_token", "=", "(", "\n", "padding_token", "if", "padding_token", "is", "not", "None", "else", "DEFAULT_PADDING_TOKEN", "\n", ")", "\n", "oov_token", "=", "oov_token", "if", "oov_token", "is", "not", "None", "else", "DEFAULT_OOV_TOKEN", "\n", "namespace_token_counts", ":", "Dict", "[", "str", ",", "Dict", "[", "str", ",", "int", "]", "]", "=", "defaultdict", "(", "\n", "lambda", ":", "defaultdict", "(", "int", ")", "\n", ")", "\n", "for", "instance", "in", "Tqdm", ".", "tqdm", "(", "instances", ")", ":", "\n", "            ", "instance", ".", "count_vocab_items", "(", "namespace_token_counts", ")", "\n", "\n", "", "return", "cls", "(", "\n", "counter", "=", "namespace_token_counts", ",", "\n", "min_count", "=", "min_count", ",", "\n", "max_vocab_size", "=", "max_vocab_size", ",", "\n", "non_padded_namespaces", "=", "non_padded_namespaces", ",", "\n", "pretrained_files", "=", "pretrained_files", ",", "\n", "only_include_pretrained_words", "=", "only_include_pretrained_words", ",", "\n", "tokens_to_add", "=", "tokens_to_add", ",", "\n", "min_pretrained_embeddings", "=", "min_pretrained_embeddings", ",", "\n", "padding_token", "=", "padding_token", ",", "\n", "oov_token", "=", "oov_token", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.data.vocabulary.Vocabulary.from_files": [[300, 352], ["logger.info", "cls", "os.listdir", "codecs.open", "namespace_filename.startswith", "namespace_filename.replace", "any", "os.path.join", "cls.set_from_file", "os.path.join", "namespace_str.strip", "allennlp.common.util.namespace_match"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.dataset_readers.multiprocess_dataset_reader.logger.info", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.dataset_readers.multiprocess_dataset_reader.QIterable.join", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.data.vocabulary.Vocabulary.set_from_file", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.dataset_readers.multiprocess_dataset_reader.QIterable.join", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.util.namespace_match"], ["", "@", "classmethod", "\n", "def", "from_files", "(", "\n", "cls", ",", "\n", "directory", ":", "str", ",", "\n", "padding_token", ":", "Optional", "[", "str", "]", "=", "DEFAULT_PADDING_TOKEN", ",", "\n", "oov_token", ":", "Optional", "[", "str", "]", "=", "DEFAULT_OOV_TOKEN", ",", "\n", ")", "->", "\"Vocabulary\"", ":", "\n", "        ", "\"\"\"\n        Loads a `Vocabulary` that was serialized using `save_to_files`.\n\n        # Parameters\n\n        directory : `str`\n            The directory containing the serialized vocabulary.\n        \"\"\"", "\n", "logger", ".", "info", "(", "\"Loading token dictionary from %s.\"", ",", "directory", ")", "\n", "padding_token", "=", "(", "\n", "padding_token", "if", "padding_token", "is", "not", "None", "else", "DEFAULT_PADDING_TOKEN", "\n", ")", "\n", "oov_token", "=", "oov_token", "if", "oov_token", "is", "not", "None", "else", "DEFAULT_OOV_TOKEN", "\n", "with", "codecs", ".", "open", "(", "\n", "os", ".", "path", ".", "join", "(", "directory", ",", "NAMESPACE_PADDING_FILE", ")", ",", "\"r\"", ",", "\"utf-8\"", "\n", ")", "as", "namespace_file", ":", "\n", "            ", "non_padded_namespaces", "=", "[", "\n", "namespace_str", ".", "strip", "(", ")", "for", "namespace_str", "in", "namespace_file", "\n", "]", "\n", "\n", "", "vocab", "=", "cls", "(", "\n", "non_padded_namespaces", "=", "non_padded_namespaces", ",", "\n", "padding_token", "=", "padding_token", ",", "\n", "oov_token", "=", "oov_token", ",", "\n", ")", "\n", "\n", "# Check every file in the directory.", "\n", "for", "namespace_filename", "in", "os", ".", "listdir", "(", "directory", ")", ":", "\n", "            ", "if", "namespace_filename", "==", "NAMESPACE_PADDING_FILE", ":", "\n", "                ", "continue", "\n", "", "if", "namespace_filename", ".", "startswith", "(", "\".\"", ")", ":", "\n", "                ", "continue", "\n", "", "namespace", "=", "namespace_filename", ".", "replace", "(", "\".txt\"", ",", "\"\"", ")", "\n", "if", "any", "(", "\n", "namespace_match", "(", "pattern", ",", "namespace", ")", "for", "pattern", "in", "non_padded_namespaces", "\n", ")", ":", "\n", "                ", "is_padded", "=", "False", "\n", "", "else", ":", "\n", "                ", "is_padded", "=", "True", "\n", "", "filename", "=", "os", ".", "path", ".", "join", "(", "directory", ",", "namespace_filename", ")", "\n", "vocab", ".", "set_from_file", "(", "\n", "filename", ",", "is_padded", ",", "namespace", "=", "namespace", ",", "oov_token", "=", "oov_token", "\n", ")", "\n", "\n", "", "return", "vocab", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.data.vocabulary.Vocabulary.from_files_and_instances": [[353, 389], ["cls.from_files", "logger.info", "collections.defaultdict", "allennlp.common.tqdm.Tqdm.tqdm", "cls.from_files._extend", "instance.count_vocab_items", "collections.defaultdict"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.data.vocabulary.Vocabulary.from_files", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.dataset_readers.multiprocess_dataset_reader.logger.info", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.tqdm.Tqdm.tqdm", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.token_indexers.wordpiece_indexer.WordpieceIndexer._extend", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.fields.text_field.TextField.count_vocab_items"], ["", "@", "classmethod", "\n", "def", "from_files_and_instances", "(", "\n", "cls", ",", "\n", "instances", ":", "Iterable", "[", "\"adi.Instance\"", "]", ",", "\n", "directory", ":", "str", ",", "\n", "padding_token", ":", "Optional", "[", "str", "]", "=", "DEFAULT_PADDING_TOKEN", ",", "\n", "oov_token", ":", "Optional", "[", "str", "]", "=", "DEFAULT_OOV_TOKEN", ",", "\n", "min_count", ":", "Dict", "[", "str", ",", "int", "]", "=", "None", ",", "\n", "max_vocab_size", ":", "Union", "[", "int", ",", "Dict", "[", "str", ",", "int", "]", "]", "=", "None", ",", "\n", "non_padded_namespaces", ":", "Iterable", "[", "str", "]", "=", "DEFAULT_NON_PADDED_NAMESPACES", ",", "\n", "pretrained_files", ":", "Optional", "[", "Dict", "[", "str", ",", "str", "]", "]", "=", "None", ",", "\n", "only_include_pretrained_words", ":", "bool", "=", "False", ",", "\n", "tokens_to_add", ":", "Dict", "[", "str", ",", "List", "[", "str", "]", "]", "=", "None", ",", "\n", "min_pretrained_embeddings", ":", "Dict", "[", "str", ",", "int", "]", "=", "None", ",", "\n", ")", "->", "\"Vocabulary\"", ":", "\n", "        ", "\"\"\"\n        Extends an already generated vocabulary using a collection of instances.\n        \"\"\"", "\n", "vocab", "=", "cls", ".", "from_files", "(", "directory", ",", "padding_token", ",", "oov_token", ")", "\n", "logger", ".", "info", "(", "\"Fitting token dictionary from dataset.\"", ")", "\n", "namespace_token_counts", ":", "Dict", "[", "str", ",", "Dict", "[", "str", ",", "int", "]", "]", "=", "defaultdict", "(", "\n", "lambda", ":", "defaultdict", "(", "int", ")", "\n", ")", "\n", "for", "instance", "in", "Tqdm", ".", "tqdm", "(", "instances", ")", ":", "\n", "            ", "instance", ".", "count_vocab_items", "(", "namespace_token_counts", ")", "\n", "", "vocab", ".", "_extend", "(", "\n", "counter", "=", "namespace_token_counts", ",", "\n", "min_count", "=", "min_count", ",", "\n", "max_vocab_size", "=", "max_vocab_size", ",", "\n", "non_padded_namespaces", "=", "non_padded_namespaces", ",", "\n", "pretrained_files", "=", "pretrained_files", ",", "\n", "only_include_pretrained_words", "=", "only_include_pretrained_words", ",", "\n", "tokens_to_add", "=", "tokens_to_add", ",", "\n", "min_pretrained_embeddings", "=", "min_pretrained_embeddings", ",", "\n", ")", "\n", "return", "vocab", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.data.vocabulary.Vocabulary.set_from_file": [[390, 445], ["codecs.open", "input_file.read().split", "enumerate", "line.replace", "input_file.read"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.scripts.convert_openie_to_conll.read"], ["", "def", "set_from_file", "(", "\n", "self", ",", "\n", "filename", ":", "str", ",", "\n", "is_padded", ":", "bool", "=", "True", ",", "\n", "oov_token", ":", "str", "=", "DEFAULT_OOV_TOKEN", ",", "\n", "namespace", ":", "str", "=", "\"tokens\"", ",", "\n", ")", ":", "\n", "        ", "\"\"\"\n        If you already have a vocabulary file for a trained model somewhere, and you really want to\n        use that vocabulary file instead of just setting the vocabulary from a dataset, for\n        whatever reason, you can do that with this method.  You must specify the namespace to use,\n        and we assume that you want to use padding and OOV tokens for this.\n\n        # Parameters\n\n        filename : `str`\n            The file containing the vocabulary to load.  It should be formatted as one token per\n            line, with nothing else in the line.  The index we assign to the token is the line\n            number in the file (1-indexed if `is_padded`, 0-indexed otherwise).  Note that this\n            file should contain the OOV token string!\n        is_padded : `bool`, optional (default=True)\n            Is this vocabulary padded?  For token / word / character vocabularies, this should be\n            `True`; while for tag or label vocabularies, this should typically be `False`.  If\n            `True`, we add a padding token with index 0, and we enforce that the `oov_token` is\n            present in the file.\n        oov_token : `str`, optional (default=DEFAULT_OOV_TOKEN)\n            What token does this vocabulary use to represent out-of-vocabulary characters?  This\n            must show up as a line in the vocabulary file.  When we find it, we replace\n            `oov_token` with `self._oov_token`, because we only use one OOV token across\n            namespaces.\n        namespace : `str`, optional (default=\"tokens\")\n            What namespace should we overwrite with this vocab file?\n        \"\"\"", "\n", "if", "is_padded", ":", "\n", "            ", "self", ".", "_token_to_index", "[", "namespace", "]", "=", "{", "self", ".", "_padding_token", ":", "0", "}", "\n", "self", ".", "_index_to_token", "[", "namespace", "]", "=", "{", "0", ":", "self", ".", "_padding_token", "}", "\n", "", "else", ":", "\n", "            ", "self", ".", "_token_to_index", "[", "namespace", "]", "=", "{", "}", "\n", "self", ".", "_index_to_token", "[", "namespace", "]", "=", "{", "}", "\n", "", "with", "codecs", ".", "open", "(", "filename", ",", "\"r\"", ",", "\"utf-8\"", ")", "as", "input_file", ":", "\n", "            ", "lines", "=", "input_file", ".", "read", "(", ")", ".", "split", "(", "\"\\n\"", ")", "\n", "# Be flexible about having final newline or not", "\n", "if", "lines", "and", "lines", "[", "-", "1", "]", "==", "\"\"", ":", "\n", "                ", "lines", "=", "lines", "[", ":", "-", "1", "]", "\n", "", "for", "i", ",", "line", "in", "enumerate", "(", "lines", ")", ":", "\n", "                ", "index", "=", "i", "+", "1", "if", "is_padded", "else", "i", "\n", "token", "=", "line", ".", "replace", "(", "\"@@NEWLINE@@\"", ",", "\"\\n\"", ")", "\n", "if", "token", "==", "oov_token", ":", "\n", "                    ", "token", "=", "self", ".", "_oov_token", "\n", "", "self", ".", "_token_to_index", "[", "namespace", "]", "[", "token", "]", "=", "index", "\n", "self", ".", "_index_to_token", "[", "namespace", "]", "[", "index", "]", "=", "token", "\n", "", "", "if", "is_padded", ":", "\n", "            ", "assert", "(", "\n", "self", ".", "_oov_token", "in", "self", ".", "_token_to_index", "[", "namespace", "]", "\n", ")", ",", "\"OOV token not found!\"", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.data.vocabulary.Vocabulary.extend_from_instances": [[446, 454], ["logger.info", "collections.defaultdict", "allennlp.common.tqdm.Tqdm.tqdm", "vocabulary.Vocabulary._extend", "instance.count_vocab_items", "collections.defaultdict"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.dataset_readers.multiprocess_dataset_reader.logger.info", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.tqdm.Tqdm.tqdm", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.token_indexers.wordpiece_indexer.WordpieceIndexer._extend", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.fields.text_field.TextField.count_vocab_items"], ["", "", "def", "extend_from_instances", "(", "self", ",", "instances", ":", "Iterable", "[", "\"adi.Instance\"", "]", ")", "->", "None", ":", "\n", "        ", "logger", ".", "info", "(", "\"Fitting token dictionary from dataset.\"", ")", "\n", "namespace_token_counts", ":", "Dict", "[", "str", ",", "Dict", "[", "str", ",", "int", "]", "]", "=", "defaultdict", "(", "\n", "lambda", ":", "defaultdict", "(", "int", ")", "\n", ")", "\n", "for", "instance", "in", "Tqdm", ".", "tqdm", "(", "instances", ")", ":", "\n", "            ", "instance", ".", "count_vocab_items", "(", "namespace_token_counts", ")", "\n", "", "self", ".", "_extend", "(", "counter", "=", "namespace_token_counts", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.data.vocabulary.Vocabulary._extend": [[455, 545], ["set", "vocabulary.Vocabulary._token_to_index.add_non_padded_namespaces", "vocabulary.Vocabulary._index_to_token.add_non_padded_namespaces", "vocabulary.Vocabulary._non_padded_namespaces.update", "tokens_to_add.items", "isinstance", "collections.defaultdict", "list", "list.sort", "any", "any", "allennlp.common.checks.ConfigurationError", "vocabulary._read_pretrained_tokens", "min_pretrained_embeddings.get", "set", "counter[].items", "vocabulary.Vocabulary.add_token_to_namespace", "tokens_to_add.get", "allennlp.common.util.namespace_match", "allennlp.common.util.namespace_match", "min_count.get", "vocabulary.Vocabulary.add_token_to_namespace", "vocabulary.Vocabulary.add_token_to_namespace", "vocabulary.Vocabulary.add_token_to_namespace", "min_count.get", "min_count.get"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.data.vocabulary._NamespaceDependentDefaultDict.add_non_padded_namespaces", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.data.vocabulary._NamespaceDependentDefaultDict.add_non_padded_namespaces", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.metrics.conll_coref_scores.Scorer.update", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.data.vocabulary._read_pretrained_tokens", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.params.Params.get", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.data.vocabulary.Vocabulary.add_token_to_namespace", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.params.Params.get", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.util.namespace_match", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.util.namespace_match", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.params.Params.get", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.data.vocabulary.Vocabulary.add_token_to_namespace", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.data.vocabulary.Vocabulary.add_token_to_namespace", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.data.vocabulary.Vocabulary.add_token_to_namespace", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.params.Params.get", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.params.Params.get"], ["", "def", "_extend", "(", "\n", "self", ",", "\n", "counter", ":", "Dict", "[", "str", ",", "Dict", "[", "str", ",", "int", "]", "]", "=", "None", ",", "\n", "min_count", ":", "Dict", "[", "str", ",", "int", "]", "=", "None", ",", "\n", "max_vocab_size", ":", "Union", "[", "int", ",", "Dict", "[", "str", ",", "int", "]", "]", "=", "None", ",", "\n", "non_padded_namespaces", ":", "Iterable", "[", "str", "]", "=", "DEFAULT_NON_PADDED_NAMESPACES", ",", "\n", "pretrained_files", ":", "Optional", "[", "Dict", "[", "str", ",", "str", "]", "]", "=", "None", ",", "\n", "only_include_pretrained_words", ":", "bool", "=", "False", ",", "\n", "tokens_to_add", ":", "Dict", "[", "str", ",", "List", "[", "str", "]", "]", "=", "None", ",", "\n", "min_pretrained_embeddings", ":", "Dict", "[", "str", ",", "int", "]", "=", "None", ",", "\n", ")", "->", "None", ":", "\n", "        ", "\"\"\"\n        This method can be used for extending already generated vocabulary.  It takes same\n        parameters as Vocabulary initializer. The `_token_to_index` and `_index_to_token`\n        mappings of calling vocabulary will be retained.  It is an inplace operation so None will be\n        returned.\n        \"\"\"", "\n", "if", "not", "isinstance", "(", "max_vocab_size", ",", "dict", ")", ":", "\n", "            ", "int_max_vocab_size", "=", "max_vocab_size", "\n", "max_vocab_size", "=", "defaultdict", "(", "lambda", ":", "int_max_vocab_size", ")", "# type: ignore", "\n", "", "min_count", "=", "min_count", "or", "{", "}", "\n", "pretrained_files", "=", "pretrained_files", "or", "{", "}", "\n", "min_pretrained_embeddings", "=", "min_pretrained_embeddings", "or", "{", "}", "\n", "non_padded_namespaces", "=", "set", "(", "non_padded_namespaces", ")", "\n", "counter", "=", "counter", "or", "{", "}", "\n", "tokens_to_add", "=", "tokens_to_add", "or", "{", "}", "\n", "\n", "self", ".", "_retained_counter", "=", "counter", "\n", "# Make sure vocabulary extension is safe.", "\n", "current_namespaces", "=", "{", "*", "self", ".", "_token_to_index", "}", "\n", "extension_namespaces", "=", "{", "*", "counter", ",", "*", "tokens_to_add", "}", "\n", "\n", "for", "namespace", "in", "current_namespaces", "&", "extension_namespaces", ":", "\n", "# if new namespace was already present", "\n", "# Either both should be padded or none should be.", "\n", "            ", "original_padded", "=", "not", "any", "(", "\n", "namespace_match", "(", "pattern", ",", "namespace", ")", "\n", "for", "pattern", "in", "self", ".", "_non_padded_namespaces", "\n", ")", "\n", "extension_padded", "=", "not", "any", "(", "\n", "namespace_match", "(", "pattern", ",", "namespace", ")", "for", "pattern", "in", "non_padded_namespaces", "\n", ")", "\n", "if", "original_padded", "!=", "extension_padded", ":", "\n", "                ", "raise", "ConfigurationError", "(", "\n", "\"Common namespace {} has conflicting \"", ".", "format", "(", "namespace", ")", "\n", "+", "\"setting of padded = True/False. \"", "\n", "+", "\"Hence extension cannot be done.\"", "\n", ")", "\n", "\n", "# Add new non-padded namespaces for extension", "\n", "", "", "self", ".", "_token_to_index", ".", "add_non_padded_namespaces", "(", "non_padded_namespaces", ")", "\n", "self", ".", "_index_to_token", ".", "add_non_padded_namespaces", "(", "non_padded_namespaces", ")", "\n", "self", ".", "_non_padded_namespaces", ".", "update", "(", "non_padded_namespaces", ")", "\n", "\n", "for", "namespace", "in", "counter", ":", "\n", "            ", "if", "namespace", "in", "pretrained_files", ":", "\n", "                ", "pretrained_list", "=", "_read_pretrained_tokens", "(", "pretrained_files", "[", "namespace", "]", ")", "\n", "min_embeddings", "=", "min_pretrained_embeddings", ".", "get", "(", "namespace", ",", "0", ")", "\n", "if", "min_embeddings", ">", "0", ":", "\n", "                    ", "tokens_old", "=", "tokens_to_add", ".", "get", "(", "namespace", ",", "[", "]", ")", "\n", "tokens_new", "=", "pretrained_list", "[", ":", "min_embeddings", "]", "\n", "tokens_to_add", "[", "namespace", "]", "=", "tokens_old", "+", "tokens_new", "\n", "", "pretrained_set", "=", "set", "(", "pretrained_list", ")", "\n", "", "else", ":", "\n", "                ", "pretrained_set", "=", "None", "\n", "", "token_counts", "=", "list", "(", "counter", "[", "namespace", "]", ".", "items", "(", ")", ")", "\n", "token_counts", ".", "sort", "(", "key", "=", "lambda", "x", ":", "x", "[", "1", "]", ",", "reverse", "=", "True", ")", "\n", "try", ":", "\n", "                ", "max_vocab", "=", "max_vocab_size", "[", "namespace", "]", "\n", "", "except", "KeyError", ":", "\n", "                ", "max_vocab", "=", "None", "\n", "", "if", "max_vocab", ":", "\n", "                ", "token_counts", "=", "token_counts", "[", ":", "max_vocab", "]", "\n", "", "for", "token", ",", "count", "in", "token_counts", ":", "\n", "                ", "if", "pretrained_set", "is", "not", "None", ":", "\n", "                    ", "if", "only_include_pretrained_words", ":", "\n", "                        ", "if", "token", "in", "pretrained_set", "and", "count", ">=", "min_count", ".", "get", "(", "\n", "namespace", ",", "1", "\n", ")", ":", "\n", "                            ", "self", ".", "add_token_to_namespace", "(", "token", ",", "namespace", ")", "\n", "", "", "elif", "token", "in", "pretrained_set", "or", "count", ">=", "min_count", ".", "get", "(", "\n", "namespace", ",", "1", "\n", ")", ":", "\n", "                        ", "self", ".", "add_token_to_namespace", "(", "token", ",", "namespace", ")", "\n", "", "", "elif", "count", ">=", "min_count", ".", "get", "(", "namespace", ",", "1", ")", ":", "\n", "                    ", "self", ".", "add_token_to_namespace", "(", "token", ",", "namespace", ")", "\n", "\n", "", "", "", "for", "namespace", ",", "tokens", "in", "tokens_to_add", ".", "items", "(", ")", ":", "\n", "            ", "for", "token", "in", "tokens", ":", "\n", "                ", "self", ".", "add_token_to_namespace", "(", "token", ",", "namespace", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.data.vocabulary.Vocabulary.__getstate__": [[546, 561], ["copy.copy", "dict", "dict", "dict", "state[].items"], "methods", ["None"], ["", "", "", "def", "__getstate__", "(", "self", ")", ":", "\n", "        ", "\"\"\"\n        Need to sanitize defaultdict and defaultdict-like objects\n        by converting them to vanilla dicts when we pickle the vocabulary.\n        \"\"\"", "\n", "state", "=", "copy", ".", "copy", "(", "self", ".", "__dict__", ")", "\n", "state", "[", "\"_token_to_index\"", "]", "=", "dict", "(", "state", "[", "\"_token_to_index\"", "]", ")", "\n", "state", "[", "\"_index_to_token\"", "]", "=", "dict", "(", "state", "[", "\"_index_to_token\"", "]", ")", "\n", "\n", "if", "\"_retained_counter\"", "in", "state", ":", "\n", "            ", "state", "[", "\"_retained_counter\"", "]", "=", "{", "\n", "key", ":", "dict", "(", "value", ")", "for", "key", ",", "value", "in", "state", "[", "\"_retained_counter\"", "]", ".", "items", "(", ")", "\n", "}", "\n", "\n", "", "return", "state", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.data.vocabulary.Vocabulary.__setstate__": [[562, 577], ["copy.copy", "vocabulary._TokenToIndexDefaultDict", "vocabulary.Vocabulary._token_to_index.update", "vocabulary._IndexToTokenDefaultDict", "vocabulary.Vocabulary._index_to_token.update"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.metrics.conll_coref_scores.Scorer.update", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.metrics.conll_coref_scores.Scorer.update"], ["", "def", "__setstate__", "(", "self", ",", "state", ")", ":", "\n", "        ", "\"\"\"\n        Conversely, when we unpickle, we need to reload the plain dicts\n        into our special DefaultDict subclasses.\n        \"\"\"", "\n", "\n", "self", ".", "__dict__", "=", "copy", ".", "copy", "(", "state", ")", "\n", "self", ".", "_token_to_index", "=", "_TokenToIndexDefaultDict", "(", "\n", "self", ".", "_non_padded_namespaces", ",", "self", ".", "_padding_token", ",", "self", ".", "_oov_token", "\n", ")", "\n", "self", ".", "_token_to_index", ".", "update", "(", "state", "[", "\"_token_to_index\"", "]", ")", "\n", "self", ".", "_index_to_token", "=", "_IndexToTokenDefaultDict", "(", "\n", "self", ".", "_non_padded_namespaces", ",", "self", ".", "_padding_token", ",", "self", ".", "_oov_token", "\n", ")", "\n", "self", ".", "_index_to_token", ".", "update", "(", "state", "[", "\"_index_to_token\"", "]", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.data.vocabulary.Vocabulary.save_to_files": [[578, 609], ["os.makedirs", "os.listdir", "vocabulary.Vocabulary._index_to_token.items", "logging.warning", "codecs.open", "os.path.join", "print", "codecs.open", "len", "range", "os.path.join", "print", "mapping[].replace"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.dataset_readers.multiprocess_dataset_reader.QIterable.join", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.dataset_readers.multiprocess_dataset_reader.QIterable.join"], ["", "def", "save_to_files", "(", "self", ",", "directory", ":", "str", ")", "->", "None", ":", "\n", "        ", "\"\"\"\n        Persist this Vocabulary to files so it can be reloaded later.\n        Each namespace corresponds to one file.\n\n        # Parameters\n\n        directory : `str`\n            The directory where we save the serialized vocabulary.\n        \"\"\"", "\n", "os", ".", "makedirs", "(", "directory", ",", "exist_ok", "=", "True", ")", "\n", "if", "os", ".", "listdir", "(", "directory", ")", ":", "\n", "            ", "logging", ".", "warning", "(", "\n", "\"vocabulary serialization directory %s is not empty\"", ",", "directory", "\n", ")", "\n", "\n", "", "with", "codecs", ".", "open", "(", "\n", "os", ".", "path", ".", "join", "(", "directory", ",", "NAMESPACE_PADDING_FILE", ")", ",", "\"w\"", ",", "\"utf-8\"", "\n", ")", "as", "namespace_file", ":", "\n", "            ", "for", "namespace_str", "in", "self", ".", "_non_padded_namespaces", ":", "\n", "                ", "print", "(", "namespace_str", ",", "file", "=", "namespace_file", ")", "\n", "\n", "", "", "for", "namespace", ",", "mapping", "in", "self", ".", "_index_to_token", ".", "items", "(", ")", ":", "\n", "# Each namespace gets written to its own file, in index order.", "\n", "            ", "with", "codecs", ".", "open", "(", "\n", "os", ".", "path", ".", "join", "(", "directory", ",", "namespace", "+", "\".txt\"", ")", ",", "\"w\"", ",", "\"utf-8\"", "\n", ")", "as", "token_file", ":", "\n", "                ", "num_tokens", "=", "len", "(", "mapping", ")", "\n", "start_index", "=", "1", "if", "mapping", "[", "0", "]", "==", "self", ".", "_padding_token", "else", "0", "\n", "for", "i", "in", "range", "(", "start_index", ",", "num_tokens", ")", ":", "\n", "                    ", "print", "(", "mapping", "[", "i", "]", ".", "replace", "(", "\"\\n\"", ",", "\"@@NEWLINE@@\"", ")", ",", "file", "=", "token_file", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.data.vocabulary.Vocabulary.is_padded": [[610, 615], ["None"], "methods", ["None"], ["", "", "", "", "def", "is_padded", "(", "self", ",", "namespace", ":", "str", ")", "->", "bool", ":", "\n", "        ", "\"\"\"\n        Returns whether or not there are padding and OOV tokens added to the given namespace.\n        \"\"\"", "\n", "return", "self", ".", "_index_to_token", "[", "namespace", "]", "[", "0", "]", "==", "self", ".", "_padding_token", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.data.vocabulary.Vocabulary.add_token_to_namespace": [[616, 633], ["isinstance", "ValueError", "len", "repr", "type"], "methods", ["None"], ["", "def", "add_token_to_namespace", "(", "self", ",", "token", ":", "str", ",", "namespace", ":", "str", "=", "\"tokens\"", ")", "->", "int", ":", "\n", "        ", "\"\"\"\n        Adds `token` to the index, if it is not already present.  Either way, we return the index of\n        the token.\n        \"\"\"", "\n", "if", "not", "isinstance", "(", "token", ",", "str", ")", ":", "\n", "            ", "raise", "ValueError", "(", "\n", "\"Vocabulary tokens must be strings, or saving and loading will break.\"", "\n", "\"  Got %s (with type %s)\"", "%", "(", "repr", "(", "token", ")", ",", "type", "(", "token", ")", ")", "\n", ")", "\n", "", "if", "token", "not", "in", "self", ".", "_token_to_index", "[", "namespace", "]", ":", "\n", "            ", "index", "=", "len", "(", "self", ".", "_token_to_index", "[", "namespace", "]", ")", "\n", "self", ".", "_token_to_index", "[", "namespace", "]", "[", "token", "]", "=", "index", "\n", "self", ".", "_index_to_token", "[", "namespace", "]", "[", "index", "]", "=", "token", "\n", "return", "index", "\n", "", "else", ":", "\n", "            ", "return", "self", ".", "_token_to_index", "[", "namespace", "]", "[", "token", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.data.vocabulary.Vocabulary.add_tokens_to_namespace": [[634, 642], ["vocabulary.Vocabulary.add_token_to_namespace"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.data.vocabulary.Vocabulary.add_token_to_namespace"], ["", "", "def", "add_tokens_to_namespace", "(", "\n", "self", ",", "tokens", ":", "List", "[", "str", "]", ",", "namespace", ":", "str", "=", "\"tokens\"", "\n", ")", "->", "List", "[", "int", "]", ":", "\n", "        ", "\"\"\"\n        Adds `tokens` to the index, if they are not already present.  Either way, we return the\n        indices of the tokens in the order that they were given.\n        \"\"\"", "\n", "return", "[", "self", ".", "add_token_to_namespace", "(", "token", ",", "namespace", ")", "for", "token", "in", "tokens", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.data.vocabulary.Vocabulary.get_index_to_token_vocabulary": [[643, 647], ["None"], "methods", ["None"], ["", "def", "get_index_to_token_vocabulary", "(", "\n", "self", ",", "namespace", ":", "str", "=", "\"tokens\"", "\n", ")", "->", "Dict", "[", "int", ",", "str", "]", ":", "\n", "        ", "return", "self", ".", "_index_to_token", "[", "namespace", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.data.vocabulary.Vocabulary.get_token_to_index_vocabulary": [[648, 652], ["None"], "methods", ["None"], ["", "def", "get_token_to_index_vocabulary", "(", "\n", "self", ",", "namespace", ":", "str", "=", "\"tokens\"", "\n", ")", "->", "Dict", "[", "str", ",", "int", "]", ":", "\n", "        ", "return", "self", ".", "_token_to_index", "[", "namespace", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.data.vocabulary.Vocabulary.get_token_index": [[653, 663], ["logger.error", "logger.error"], "methods", ["None"], ["", "def", "get_token_index", "(", "self", ",", "token", ":", "str", ",", "namespace", ":", "str", "=", "\"tokens\"", ")", "->", "int", ":", "\n", "        ", "if", "token", "in", "self", ".", "_token_to_index", "[", "namespace", "]", ":", "\n", "            ", "return", "self", ".", "_token_to_index", "[", "namespace", "]", "[", "token", "]", "\n", "", "else", ":", "\n", "            ", "try", ":", "\n", "                ", "return", "self", ".", "_token_to_index", "[", "namespace", "]", "[", "self", ".", "_oov_token", "]", "\n", "", "except", "KeyError", ":", "\n", "                ", "logger", ".", "error", "(", "\"Namespace: %s\"", ",", "namespace", ")", "\n", "logger", ".", "error", "(", "\"Token: %s\"", ",", "token", ")", "\n", "raise", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.data.vocabulary.Vocabulary.get_token_from_index": [[664, 666], ["None"], "methods", ["None"], ["", "", "", "def", "get_token_from_index", "(", "self", ",", "index", ":", "int", ",", "namespace", ":", "str", "=", "\"tokens\"", ")", "->", "str", ":", "\n", "        ", "return", "self", ".", "_index_to_token", "[", "namespace", "]", "[", "index", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.data.vocabulary.Vocabulary.get_vocab_size": [[667, 669], ["len"], "methods", ["None"], ["", "def", "get_vocab_size", "(", "self", ",", "namespace", ":", "str", "=", "\"tokens\"", ")", "->", "int", ":", "\n", "        ", "return", "len", "(", "self", ".", "_token_to_index", "[", "namespace", "]", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.data.vocabulary.Vocabulary.__eq__": [[670, 674], ["isinstance"], "methods", ["None"], ["", "def", "__eq__", "(", "self", ",", "other", ")", ":", "\n", "        ", "if", "isinstance", "(", "self", ",", "other", ".", "__class__", ")", ":", "\n", "            ", "return", "self", ".", "__dict__", "==", "other", ".", "__dict__", "\n", "", "return", "False", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.data.vocabulary.Vocabulary.__str__": [[675, 685], ["vocabulary.Vocabulary.get_vocab_size"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.data.vocabulary.Vocabulary.get_vocab_size"], ["", "def", "__str__", "(", "self", ")", "->", "str", ":", "\n", "        ", "base_string", "=", "f\"Vocabulary with namespaces:\\n\"", "\n", "non_padded_namespaces", "=", "(", "\n", "f\"\\tNon Padded Namespaces: {self._non_padded_namespaces}\\n\"", "\n", ")", "\n", "namespaces", "=", "[", "\n", "f\"\\tNamespace: {name}, Size: {self.get_vocab_size(name)} \\n\"", "\n", "for", "name", "in", "self", ".", "_index_to_token", "\n", "]", "\n", "return", "\" \"", ".", "join", "(", "[", "base_string", ",", "non_padded_namespaces", "]", "+", "namespaces", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.data.vocabulary.Vocabulary.__repr__": [[686, 695], ["vocabulary.Vocabulary.get_vocab_size"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.data.vocabulary.Vocabulary.get_vocab_size"], ["", "def", "__repr__", "(", "self", ")", "->", "str", ":", "\n", "# This is essentially the same as __str__, but with no newlines", "\n", "        ", "base_string", "=", "f\"Vocabulary with namespaces: \"", "\n", "namespaces", "=", "[", "\n", "f\"{name}, Size: {self.get_vocab_size(name)} ||\"", "\n", "for", "name", "in", "self", ".", "_index_to_token", "\n", "]", "\n", "non_padded_namespaces", "=", "f\"Non Padded Namespaces: {self._non_padded_namespaces}\"", "\n", "return", "\" \"", ".", "join", "(", "[", "base_string", "]", "+", "namespaces", "+", "[", "non_padded_namespaces", "]", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.data.vocabulary.Vocabulary.print_statistics": [[696, 729], ["logger.info", "print", "logger.info", "list", "list.sort", "print", "list.sort", "print", "print", "reversed", "vocabulary.Vocabulary._retained_counter[].items", "print", "print", "print", "len", "len", "len"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.dataset_readers.multiprocess_dataset_reader.logger.info", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.dataset_readers.multiprocess_dataset_reader.logger.info"], ["", "def", "print_statistics", "(", "self", ")", "->", "None", ":", "\n", "        ", "if", "self", ".", "_retained_counter", ":", "\n", "            ", "logger", ".", "info", "(", "\n", "\"Printed vocabulary statistics are only for the part of the vocabulary generated \"", "\n", "\"from instances. If vocabulary is constructed by extending saved vocabulary with \"", "\n", "\"dataset instances, the directly loaded portion won't be considered here.\"", "\n", ")", "\n", "print", "(", "\"\\n\\n----Vocabulary Statistics----\\n\"", ")", "\n", "# Since we don't saved counter info, it is impossible to consider pre-saved portion.", "\n", "for", "namespace", "in", "self", ".", "_retained_counter", ":", "\n", "                ", "tokens_with_counts", "=", "list", "(", "self", ".", "_retained_counter", "[", "namespace", "]", ".", "items", "(", ")", ")", "\n", "tokens_with_counts", ".", "sort", "(", "key", "=", "lambda", "x", ":", "x", "[", "1", "]", ",", "reverse", "=", "True", ")", "\n", "print", "(", "f\"\\nTop 10 most frequent tokens in namespace '{namespace}':\"", ")", "\n", "for", "token", ",", "freq", "in", "tokens_with_counts", "[", ":", "10", "]", ":", "\n", "                    ", "print", "(", "f\"\\tToken: {token}\\t\\tFrequency: {freq}\"", ")", "\n", "# Now sort by token length, not frequency", "\n", "", "tokens_with_counts", ".", "sort", "(", "key", "=", "lambda", "x", ":", "len", "(", "x", "[", "0", "]", ")", ",", "reverse", "=", "True", ")", "\n", "\n", "print", "(", "f\"\\nTop 10 longest tokens in namespace '{namespace}':\"", ")", "\n", "for", "token", ",", "freq", "in", "tokens_with_counts", "[", ":", "10", "]", ":", "\n", "                    ", "print", "(", "\n", "f\"\\tToken: {token}\\t\\tlength: {len(token)}\\tFrequency: {freq}\"", "\n", ")", "\n", "\n", "", "print", "(", "f\"\\nTop 10 shortest tokens in namespace '{namespace}':\"", ")", "\n", "for", "token", ",", "freq", "in", "reversed", "(", "tokens_with_counts", "[", "-", "10", ":", "]", ")", ":", "\n", "                    ", "print", "(", "\n", "f\"\\tToken: {token}\\t\\tlength: {len(token)}\\tFrequency: {freq}\"", "\n", ")", "\n", "", "", "", "else", ":", "\n", "# _retained_counter would be set only if instances were used for vocabulary construction.", "\n", "            ", "logger", ".", "info", "(", "\n", "\"Vocabulary statistics cannot be printed since \"", "\n", "\"dataset instances were not used for its construction.\"", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.data.vocabulary._read_pretrained_tokens": [[120, 136], ["logger.info", "EmbeddingsTextFile", "enumerate", "allennlp.common.tqdm.Tqdm.tqdm", "line.find", "tokens.append", "logger.warning", "len"], "function", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.dataset_readers.multiprocess_dataset_reader.logger.info", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.tqdm.Tqdm.tqdm"], ["", "", "def", "_read_pretrained_tokens", "(", "embeddings_file_uri", ":", "str", ")", "->", "List", "[", "str", "]", ":", "\n", "# Moving this import to the top breaks everything (cycling import, I guess)", "\n", "    ", "from", "allennlp", ".", "modules", ".", "token_embedders", ".", "embedding", "import", "EmbeddingsTextFile", "\n", "\n", "logger", ".", "info", "(", "\"Reading pretrained tokens from: %s\"", ",", "embeddings_file_uri", ")", "\n", "tokens", ":", "List", "[", "str", "]", "=", "[", "]", "\n", "with", "EmbeddingsTextFile", "(", "embeddings_file_uri", ")", "as", "embeddings_file", ":", "\n", "        ", "for", "line_number", ",", "line", "in", "enumerate", "(", "Tqdm", ".", "tqdm", "(", "embeddings_file", ")", ",", "start", "=", "1", ")", ":", "\n", "            ", "token_end", "=", "line", ".", "find", "(", "\" \"", ")", "\n", "if", "token_end", ">=", "0", ":", "\n", "                ", "token", "=", "line", "[", ":", "token_end", "]", "\n", "tokens", ".", "append", "(", "token", ")", "\n", "", "else", ":", "\n", "                ", "line_begin", "=", "line", "[", ":", "20", "]", "+", "\"...\"", "if", "len", "(", "line", ")", ">", "20", "else", "line", "\n", "logger", ".", "warning", "(", "f\"Skipping line number %d: %s\"", ",", "line_number", ",", "line_begin", ")", "\n", "", "", "", "return", "tokens", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.data.batch.Batch.__init__": [[27, 36], ["typing.Iterable.__init__", "allennlp.common.util.ensure_list", "batch.Batch._check_types"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.graph_util.plot.GraphPlot.__init__", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.util.ensure_list", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.data.batch.Batch._check_types"], ["def", "__init__", "(", "self", ",", "instances", ":", "Iterable", "[", "Instance", "]", ")", "->", "None", ":", "\n", "        ", "\"\"\"\n        A Batch just takes an iterable of instances in its constructor and hangs onto them\n        in a list.\n        \"\"\"", "\n", "super", "(", ")", ".", "__init__", "(", ")", "\n", "\n", "self", ".", "instances", ":", "List", "[", "Instance", "]", "=", "ensure_list", "(", "instances", ")", "\n", "self", ".", "_check_types", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.data.batch.Batch._check_types": [[37, 54], ["all", "allennlp.common.checks.ConfigurationError", "x.fields.items"], "methods", ["None"], ["", "def", "_check_types", "(", "self", ")", "->", "None", ":", "\n", "        ", "\"\"\"\n        Check that all the instances have the same types.\n        \"\"\"", "\n", "all_instance_fields_and_types", ":", "List", "[", "Dict", "[", "str", ",", "str", "]", "]", "=", "[", "\n", "{", "k", ":", "v", ".", "__class__", ".", "__name__", "for", "k", ",", "v", "in", "x", ".", "fields", ".", "items", "(", ")", "}", "\n", "for", "x", "in", "self", ".", "instances", "\n", "]", "\n", "# Check all the field names and Field types are the same for every instance.", "\n", "if", "not", "all", "(", "\n", "[", "\n", "all_instance_fields_and_types", "[", "0", "]", "==", "x", "\n", "for", "x", "in", "all_instance_fields_and_types", "\n", "]", "\n", ")", ":", "\n", "            ", "raise", "ConfigurationError", "(", "\n", "\"You cannot construct a Batch with non-homogeneous Instances.\"", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.data.batch.Batch.get_padding_lengths": [[56, 81], ["collections.defaultdict", "collections.defaultdict", "all_field_lengths.items", "instance.get_padding_lengths", "instance_lengths.items", "field_lengths[].keys", "all_field_lengths[].append", "max", "x.get"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.fields.text_field.TextField.get_padding_lengths", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.params.Params.get"], ["", "", "def", "get_padding_lengths", "(", "self", ")", "->", "Dict", "[", "str", ",", "Dict", "[", "str", ",", "int", "]", "]", ":", "\n", "        ", "\"\"\"\n        Gets the maximum padding lengths from all `Instances` in this batch.  Each `Instance`\n        has multiple `Fields`, and each `Field` could have multiple things that need padding.\n        We look at all fields in all instances, and find the max values for each (field_name,\n        padding_key) pair, returning them in a dictionary.\n\n        This can then be used to convert this batch into arrays of consistent length, or to set\n        model parameters, etc.\n        \"\"\"", "\n", "padding_lengths", ":", "Dict", "[", "str", ",", "Dict", "[", "str", ",", "int", "]", "]", "=", "defaultdict", "(", "dict", ")", "\n", "all_instance_lengths", ":", "List", "[", "Dict", "[", "str", ",", "Dict", "[", "str", ",", "int", "]", "]", "]", "=", "[", "\n", "instance", ".", "get_padding_lengths", "(", ")", "for", "instance", "in", "self", ".", "instances", "\n", "]", "\n", "if", "not", "all_instance_lengths", ":", "\n", "            ", "return", "{", "**", "padding_lengths", "}", "\n", "", "all_field_lengths", ":", "Dict", "[", "str", ",", "List", "[", "Dict", "[", "str", ",", "int", "]", "]", "]", "=", "defaultdict", "(", "list", ")", "\n", "for", "instance_lengths", "in", "all_instance_lengths", ":", "\n", "            ", "for", "field_name", ",", "instance_field_lengths", "in", "instance_lengths", ".", "items", "(", ")", ":", "\n", "                ", "all_field_lengths", "[", "field_name", "]", ".", "append", "(", "instance_field_lengths", ")", "\n", "", "", "for", "field_name", ",", "field_lengths", "in", "all_field_lengths", ".", "items", "(", ")", ":", "\n", "            ", "for", "padding_key", "in", "field_lengths", "[", "0", "]", ".", "keys", "(", ")", ":", "\n", "                ", "max_value", "=", "max", "(", "x", ".", "get", "(", "padding_key", ",", "0", ")", "for", "x", "in", "field_lengths", ")", "\n", "padding_lengths", "[", "field_name", "]", "[", "padding_key", "]", "=", "max_value", "\n", "", "", "return", "{", "**", "padding_lengths", "}", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.data.batch.Batch.as_tensor_dict": [[82, 172], ["batch.Batch.get_padding_lengths", "collections.defaultdict", "batch.Batch.items", "collections.defaultdict", "field_tensors.items", "collections.defaultdict", "logger.info", "logger.info", "logger.info", "instance_field_lengths.keys", "logger.info", "instance.as_tensor_dict().items", "field_classes[].batch_tensors", "len", "str", "str", "str", "field_tensors[].append", "instance.as_tensor_dict"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.fields.text_field.TextField.get_padding_lengths", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.dataset_readers.multiprocess_dataset_reader.logger.info", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.dataset_readers.multiprocess_dataset_reader.logger.info", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.dataset_readers.multiprocess_dataset_reader.logger.info", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.dataset_readers.multiprocess_dataset_reader.logger.info", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.fields.text_field.TextField.batch_tensors", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.data.instance.Instance.as_tensor_dict"], ["", "def", "as_tensor_dict", "(", "\n", "self", ",", "padding_lengths", ":", "Dict", "[", "str", ",", "Dict", "[", "str", ",", "int", "]", "]", "=", "None", ",", "verbose", ":", "bool", "=", "False", "\n", ")", "->", "Dict", "[", "str", ",", "Union", "[", "torch", ".", "Tensor", ",", "Dict", "[", "str", ",", "torch", ".", "Tensor", "]", "]", "]", ":", "\n", "# This complex return type is actually predefined elsewhere as a DataArray,", "\n", "# but we can't use it because mypy doesn't like it.", "\n", "        ", "\"\"\"\n        This method converts this `Batch` into a set of pytorch Tensors that can be passed\n        through a model.  In order for the tensors to be valid tensors, all `Instances` in this\n        batch need to be padded to the same lengths wherever padding is necessary, so we do that\n        first, then we combine all of the tensors for each field in each instance into a set of\n        batched tensors for each field.\n\n        # Parameters\n\n        padding_lengths : `Dict[str, Dict[str, int]]`\n            If a key is present in this dictionary with a non-`None` value, we will pad to that\n            length instead of the length calculated from the data.  This lets you, e.g., set a\n            maximum value for sentence length if you want to throw out long sequences.\n\n            Entries in this dictionary are keyed first by field name (e.g., \"question\"), then by\n            padding key (e.g., \"num_tokens\").\n        verbose : `bool`, optional (default=`False`)\n            Should we output logging information when we're doing this padding?  If the batch is\n            large, this is nice to have, because padding a large batch could take a long time.\n            But if you're doing this inside of a data generator, having all of this output per\n            batch is a bit obnoxious (and really slow).\n\n        # Returns\n\n        tensors : `Dict[str, DataArray]`\n            A dictionary of tensors, keyed by field name, suitable for passing as input to a model.\n            This is a `batch` of instances, so, e.g., if the instances have a \"question\" field and\n            an \"answer\" field, the \"question\" fields for all of the instances will be grouped\n            together into a single tensor, and the \"answer\" fields for all instances will be\n            similarly grouped in a parallel set of tensors, for batched computation. Additionally,\n            for complex `Fields`, the value of the dictionary key is not necessarily a single\n            tensor.  For example, with the `TextField`, the output is a dictionary mapping\n            `TokenIndexer` keys to tensors. The number of elements in this sub-dictionary\n            therefore corresponds to the number of `TokenIndexers` used to index the\n            `TextField`.  Each `Field` class is responsible for batching its own output.\n        \"\"\"", "\n", "if", "padding_lengths", "is", "None", ":", "\n", "            ", "padding_lengths", "=", "defaultdict", "(", "dict", ")", "\n", "# First we need to decide _how much_ to pad.  To do that, we find the max length for all", "\n", "# relevant padding decisions from the instances themselves.  Then we check whether we were", "\n", "# given a max length for a particular field and padding key.  If we were, we use that", "\n", "# instead of the instance-based one.", "\n", "", "if", "verbose", ":", "\n", "            ", "logger", ".", "info", "(", "\n", "\"Padding batch of size %d to lengths %s\"", ",", "\n", "len", "(", "self", ".", "instances", ")", ",", "\n", "str", "(", "padding_lengths", ")", ",", "\n", ")", "\n", "logger", ".", "info", "(", "\"Getting max lengths from instances\"", ")", "\n", "", "instance_padding_lengths", "=", "self", ".", "get_padding_lengths", "(", ")", "\n", "if", "verbose", ":", "\n", "            ", "logger", ".", "info", "(", "\"Instance max lengths: %s\"", ",", "str", "(", "instance_padding_lengths", ")", ")", "\n", "", "lengths_to_use", ":", "Dict", "[", "str", ",", "Dict", "[", "str", ",", "int", "]", "]", "=", "defaultdict", "(", "dict", ")", "\n", "for", "field_name", ",", "instance_field_lengths", "in", "instance_padding_lengths", ".", "items", "(", ")", ":", "\n", "            ", "for", "padding_key", "in", "instance_field_lengths", ".", "keys", "(", ")", ":", "\n", "                ", "if", "padding_key", "in", "padding_lengths", "[", "field_name", "]", ":", "\n", "                    ", "lengths_to_use", "[", "field_name", "]", "[", "padding_key", "]", "=", "padding_lengths", "[", "\n", "field_name", "\n", "]", "[", "padding_key", "]", "\n", "", "else", ":", "\n", "                    ", "lengths_to_use", "[", "field_name", "]", "[", "padding_key", "]", "=", "instance_field_lengths", "[", "\n", "padding_key", "\n", "]", "\n", "\n", "# Now we actually pad the instances to tensors.", "\n", "", "", "", "field_tensors", ":", "Dict", "[", "str", ",", "list", "]", "=", "defaultdict", "(", "list", ")", "\n", "if", "verbose", ":", "\n", "            ", "logger", ".", "info", "(", "\n", "\"Now actually padding instances to length: %s\"", ",", "str", "(", "lengths_to_use", ")", "\n", ")", "\n", "", "for", "instance", "in", "self", ".", "instances", ":", "\n", "            ", "for", "field", ",", "tensors", "in", "instance", ".", "as_tensor_dict", "(", "lengths_to_use", ")", ".", "items", "(", ")", ":", "\n", "                ", "field_tensors", "[", "field", "]", ".", "append", "(", "tensors", ")", "\n", "\n", "# Finally, we combine the tensors that we got for each instance into one big tensor (or set", "\n", "# of tensors) per field.  The `Field` classes themselves have the logic for batching the", "\n", "# tensors together, so we grab a dictionary of field_name -> field class from the first", "\n", "# instance in the batch.", "\n", "", "", "field_classes", "=", "self", ".", "instances", "[", "0", "]", ".", "fields", "\n", "final_fields", "=", "{", "}", "\n", "for", "field_name", ",", "field_tensor_list", "in", "field_tensors", ".", "items", "(", ")", ":", "\n", "            ", "final_fields", "[", "field_name", "]", "=", "field_classes", "[", "field_name", "]", ".", "batch_tensors", "(", "\n", "field_tensor_list", "\n", ")", "\n", "", "return", "final_fields", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.data.batch.Batch.__iter__": [[173, 175], ["iter"], "methods", ["None"], ["", "def", "__iter__", "(", "self", ")", "->", "Iterator", "[", "Instance", "]", ":", "\n", "        ", "return", "iter", "(", "self", ".", "instances", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.data.batch.Batch.index_instances": [[176, 179], ["instance.index_fields"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.data.instance.Instance.index_fields"], ["", "def", "index_instances", "(", "self", ",", "vocab", ":", "Vocabulary", ")", "->", "None", ":", "\n", "        ", "for", "instance", "in", "self", ".", "instances", ":", "\n", "            ", "instance", ".", "index_fields", "(", "vocab", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.data.batch.Batch.print_statistics": [[180, 205], ["collections.defaultdict", "print", "sequence_field_lengths.items", "print", "list", "instance.get_padding_lengths().items", "print", "print", "numpy.random.randint", "print", "print", "allennlp.common.checks.ConfigurationError", "field_padding_lengths.items", "len", "instance.get_padding_lengths", "sequence_field_lengths[].append", "numpy.mean", "numpy.std", "numpy.max", "numpy.min"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.fields.text_field.TextField.get_padding_lengths"], ["", "", "def", "print_statistics", "(", "self", ")", "->", "None", ":", "\n", "# Make sure if has been indexed first", "\n", "        ", "sequence_field_lengths", ":", "Dict", "[", "str", ",", "List", "]", "=", "defaultdict", "(", "list", ")", "\n", "for", "instance", "in", "self", ".", "instances", ":", "\n", "            ", "if", "not", "instance", ".", "indexed", ":", "\n", "                ", "raise", "ConfigurationError", "(", "\n", "\"Instances must be indexed with vocabulary \"", "\n", "\"before asking to print dataset statistics.\"", "\n", ")", "\n", "", "for", "field", ",", "field_padding_lengths", "in", "instance", ".", "get_padding_lengths", "(", ")", ".", "items", "(", ")", ":", "\n", "                ", "for", "key", ",", "value", "in", "field_padding_lengths", ".", "items", "(", ")", ":", "\n", "                    ", "sequence_field_lengths", "[", "f\"{field}.{key}\"", "]", ".", "append", "(", "value", ")", "\n", "\n", "", "", "", "print", "(", "\"\\n\\n----Dataset Statistics----\\n\"", ")", "\n", "for", "name", ",", "lengths", "in", "sequence_field_lengths", ".", "items", "(", ")", ":", "\n", "            ", "print", "(", "f\"Statistics for {name}:\"", ")", "\n", "print", "(", "\n", "f\"\\tLengths: Mean: {numpy.mean(lengths)}, Standard Dev: {numpy.std(lengths)}, \"", "\n", "f\"Max: {numpy.max(lengths)}, Min: {numpy.min(lengths)}\"", "\n", ")", "\n", "\n", "", "print", "(", "\"\\n10 Random instances: \"", ")", "\n", "for", "i", "in", "list", "(", "numpy", ".", "random", ".", "randint", "(", "len", "(", "self", ".", "instances", ")", ",", "size", "=", "10", ")", ")", ":", "\n", "            ", "print", "(", "f\"Instance {i}:\"", ")", "\n", "print", "(", "f\"\\t{self.instances[i]}\"", ")", "\n", "", "", "", ""]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.data.instance.Instance.__init__": [[26, 29], ["None"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "fields", ":", "MutableMapping", "[", "str", ",", "Field", "]", ")", "->", "None", ":", "\n", "        ", "self", ".", "fields", "=", "fields", "\n", "self", ".", "indexed", "=", "False", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.data.instance.Instance.__getitem__": [[33, 35], ["None"], "methods", ["None"], ["", "def", "__getitem__", "(", "self", ",", "key", ":", "str", ")", "->", "Field", ":", "\n", "        ", "return", "self", ".", "fields", "[", "key", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.data.instance.Instance.__iter__": [[36, 38], ["iter"], "methods", ["None"], ["", "def", "__iter__", "(", "self", ")", ":", "\n", "        ", "return", "iter", "(", "self", ".", "fields", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.data.instance.Instance.__len__": [[39, 41], ["len"], "methods", ["None"], ["", "def", "__len__", "(", "self", ")", "->", "int", ":", "\n", "        ", "return", "len", "(", "self", ".", "fields", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.data.instance.Instance.add_field": [[42, 53], ["field.index"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.fields.text_field.TextField.index"], ["", "def", "add_field", "(", "\n", "self", ",", "field_name", ":", "str", ",", "field", ":", "Field", ",", "vocab", ":", "Vocabulary", "=", "None", "\n", ")", "->", "None", ":", "\n", "        ", "\"\"\"\n        Add the field to the existing fields mapping.\n        If we have already indexed the Instance, then we also index `field`, so\n        it is necessary to supply the vocab.\n        \"\"\"", "\n", "self", ".", "fields", "[", "field_name", "]", "=", "field", "\n", "if", "self", ".", "indexed", ":", "\n", "            ", "field", ".", "index", "(", "vocab", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.data.instance.Instance.count_vocab_items": [[54, 61], ["instance.Instance.fields.values", "field.count_vocab_items"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.fields.text_field.TextField.count_vocab_items"], ["", "", "def", "count_vocab_items", "(", "self", ",", "counter", ":", "Dict", "[", "str", ",", "Dict", "[", "str", ",", "int", "]", "]", ")", ":", "\n", "        ", "\"\"\"\n        Increments counts in the given `counter` for all of the vocabulary items in all of the\n        `Fields` in this `Instance`.\n        \"\"\"", "\n", "for", "field", "in", "self", ".", "fields", ".", "values", "(", ")", ":", "\n", "            ", "field", ".", "count_vocab_items", "(", "counter", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.data.instance.Instance.index_fields": [[62, 76], ["instance.Instance.fields.values", "field.index"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.fields.text_field.TextField.index"], ["", "", "def", "index_fields", "(", "self", ",", "vocab", ":", "Vocabulary", ")", "->", "None", ":", "\n", "        ", "\"\"\"\n        Indexes all fields in this `Instance` using the provided `Vocabulary`.\n        This `mutates` the current object, it does not return a new `Instance`.\n        A `DataIterator` will call this on each pass through a dataset; we use the `indexed`\n        flag to make sure that indexing only happens once.\n\n        This means that if for some reason you modify your vocabulary after you've\n        indexed your instances, you might get unexpected behavior.\n        \"\"\"", "\n", "if", "not", "self", ".", "indexed", ":", "\n", "            ", "self", ".", "indexed", "=", "True", "\n", "for", "field", "in", "self", ".", "fields", ".", "values", "(", ")", ":", "\n", "                ", "field", ".", "index", "(", "vocab", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.data.instance.Instance.get_padding_lengths": [[77, 86], ["instance.Instance.fields.items", "field.get_padding_lengths"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.fields.text_field.TextField.get_padding_lengths"], ["", "", "", "def", "get_padding_lengths", "(", "self", ")", "->", "Dict", "[", "str", ",", "Dict", "[", "str", ",", "int", "]", "]", ":", "\n", "        ", "\"\"\"\n        Returns a dictionary of padding lengths, keyed by field name.  Each `Field` returns a\n        mapping from padding keys to actual lengths, and we just key that dictionary by field name.\n        \"\"\"", "\n", "lengths", "=", "{", "}", "\n", "for", "field_name", ",", "field", "in", "self", ".", "fields", ".", "items", "(", ")", ":", "\n", "            ", "lengths", "[", "field_name", "]", "=", "field", ".", "get_padding_lengths", "(", ")", "\n", "", "return", "lengths", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.data.instance.Instance.as_tensor_dict": [[87, 103], ["instance.Instance.fields.items", "instance.Instance.get_padding_lengths", "field.as_tensor"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.fields.text_field.TextField.get_padding_lengths", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.fields.text_field.TextField.as_tensor"], ["", "def", "as_tensor_dict", "(", "\n", "self", ",", "padding_lengths", ":", "Dict", "[", "str", ",", "Dict", "[", "str", ",", "int", "]", "]", "=", "None", "\n", ")", "->", "Dict", "[", "str", ",", "DataArray", "]", ":", "\n", "        ", "\"\"\"\n        Pads each `Field` in this instance to the lengths given in `padding_lengths` (which is\n        keyed by field name, then by padding key, the same as the return value in\n        :func:`get_padding_lengths`), returning a list of torch tensors for each field.\n\n        If `padding_lengths` is omitted, we will call `self.get_padding_lengths()` to get the\n        sizes of the tensors to create.\n        \"\"\"", "\n", "padding_lengths", "=", "padding_lengths", "or", "self", ".", "get_padding_lengths", "(", ")", "\n", "tensors", "=", "{", "}", "\n", "for", "field_name", ",", "field", "in", "self", ".", "fields", ".", "items", "(", ")", ":", "\n", "            ", "tensors", "[", "field_name", "]", "=", "field", ".", "as_tensor", "(", "padding_lengths", "[", "field_name", "]", ")", "\n", "", "return", "tensors", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.data.instance.Instance.__str__": [[104, 109], ["instance.Instance.fields.items"], "methods", ["None"], ["", "def", "__str__", "(", "self", ")", "->", "str", ":", "\n", "        ", "base_string", "=", "f\"Instance with fields:\\n\"", "\n", "return", "\" \"", ".", "join", "(", "\n", "[", "base_string", "]", "\n", "+", "[", "f\"\\t {name}: {field} \\n\"", "for", "name", ",", "field", "in", "self", ".", "fields", ".", "items", "(", ")", "]", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.tokenizers.character_tokenizer.CharacterTokenizer.__init__": [[36, 51], ["character_tokenizer.CharacterTokenizer._start_tokens.reverse"], "methods", ["None"], ["def", "__init__", "(", "\n", "self", ",", "\n", "byte_encoding", ":", "str", "=", "None", ",", "\n", "lowercase_characters", ":", "bool", "=", "False", ",", "\n", "start_tokens", ":", "List", "[", "str", "]", "=", "None", ",", "\n", "end_tokens", ":", "List", "[", "str", "]", "=", "None", ",", "\n", ")", "->", "None", ":", "\n", "# TODO(brendanr): Add length truncation.", "\n", "        ", "self", ".", "_byte_encoding", "=", "byte_encoding", "\n", "self", ".", "_lowercase_characters", "=", "lowercase_characters", "\n", "self", ".", "_start_tokens", "=", "start_tokens", "or", "[", "]", "\n", "# We reverse the tokens here because we're going to insert them with `insert(0)` later;", "\n", "# this makes sure they show up in the right order.", "\n", "self", ".", "_start_tokens", ".", "reverse", "(", ")", "\n", "self", ".", "_end_tokens", "=", "end_tokens", "or", "[", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.tokenizers.character_tokenizer.CharacterTokenizer.tokenize": [[52, 75], ["text.lower.lower.lower", "isinstance", "tokens.insert", "isinstance", "tokens.append", "allennlp.data.tokenizers.token.Token", "allennlp.data.tokenizers.token.Token", "allennlp.data.tokenizers.token.Token", "allennlp.data.tokenizers.token.Token", "allennlp.data.tokenizers.token.Token", "allennlp.data.tokenizers.token.Token", "text.lower.lower.encode", "list"], "methods", ["None"], ["", "@", "overrides", "\n", "def", "tokenize", "(", "self", ",", "text", ":", "str", ")", "->", "List", "[", "Token", "]", ":", "\n", "        ", "if", "self", ".", "_lowercase_characters", ":", "\n", "            ", "text", "=", "text", ".", "lower", "(", ")", "\n", "", "if", "self", ".", "_byte_encoding", "is", "not", "None", ":", "\n", "# We add 1 here so that we can still use 0 for masking, no matter what bytes we get out", "\n", "# of this.", "\n", "            ", "tokens", "=", "[", "Token", "(", "text_id", "=", "c", "+", "1", ")", "for", "c", "in", "text", ".", "encode", "(", "self", ".", "_byte_encoding", ")", "]", "\n", "", "else", ":", "\n", "            ", "tokens", "=", "[", "Token", "(", "t", ")", "for", "t", "in", "list", "(", "text", ")", "]", "\n", "", "for", "start_token", "in", "self", ".", "_start_tokens", ":", "\n", "            ", "if", "isinstance", "(", "start_token", ",", "int", ")", ":", "\n", "                ", "token", "=", "Token", "(", "text_id", "=", "start_token", ",", "idx", "=", "0", ")", "\n", "", "else", ":", "\n", "                ", "token", "=", "Token", "(", "text", "=", "start_token", ",", "idx", "=", "0", ")", "\n", "", "tokens", ".", "insert", "(", "0", ",", "token", ")", "\n", "", "for", "end_token", "in", "self", ".", "_end_tokens", ":", "\n", "            ", "if", "isinstance", "(", "end_token", ",", "int", ")", ":", "\n", "                ", "token", "=", "Token", "(", "text_id", "=", "end_token", ",", "idx", "=", "0", ")", "\n", "", "else", ":", "\n", "                ", "token", "=", "Token", "(", "text", "=", "end_token", ",", "idx", "=", "0", ")", "\n", "", "tokens", ".", "append", "(", "token", ")", "\n", "", "return", "tokens", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.tokenizers.character_tokenizer.CharacterTokenizer.__eq__": [[76, 80], ["isinstance"], "methods", ["None"], ["", "def", "__eq__", "(", "self", ",", "other", ")", "->", "bool", ":", "\n", "        ", "if", "isinstance", "(", "self", ",", "other", ".", "__class__", ")", ":", "\n", "            ", "return", "self", ".", "__dict__", "==", "other", ".", "__dict__", "\n", "", "return", "NotImplemented", "\n", "", "", ""]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.tokenizers.tokenizer.Tokenizer.batch_tokenize": [[30, 39], ["tokenizer.Tokenizer.tokenize"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.tokenizers.whitespace_tokenizer.WhitespaceTokenizer.tokenize"], ["def", "batch_tokenize", "(", "self", ",", "texts", ":", "List", "[", "str", "]", ")", "->", "List", "[", "List", "[", "Token", "]", "]", ":", "\n", "        ", "\"\"\"\n        Batches together tokenization of several texts, in case that is faster for particular\n        tokenizers.\n\n        By default we just do this without batching.  Override this in your tokenizer if you have a\n        good way of doing batched computation.\n        \"\"\"", "\n", "return", "[", "self", ".", "tokenize", "(", "text", ")", "for", "text", "in", "texts", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.tokenizers.tokenizer.Tokenizer.tokenize": [[40, 49], ["None"], "methods", ["None"], ["", "def", "tokenize", "(", "self", ",", "text", ":", "str", ")", "->", "List", "[", "Token", "]", ":", "\n", "        ", "\"\"\"\n        Actually implements splitting words into tokens.\n\n        # Returns\n\n        tokens : `List[Token]`\n        \"\"\"", "\n", "raise", "NotImplementedError", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.tokenizers.tokenizer.Tokenizer.from_params": [[50, 85], ["params.get", "params.get", "super().from_params", "params.get", "params.get", "logger.warning", "allennlp.common.Params", "isinstance", "params.get", "params.get", "allennlp.common.checks.ConfigurationError", "str", "str", "allennlp.common.Params"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.params.Params.get", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.params.Params.get", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.tokenizers.tokenizer.Tokenizer.from_params", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.params.Params.get", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.params.Params.get", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.params.Params.get", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.params.Params.get"], ["", "@", "classmethod", "\n", "def", "from_params", "(", "cls", ",", "params", ":", "Params", ",", "**", "extras", ")", "->", "\"Tokenizer\"", ":", "# type: ignore", "\n", "\n", "# Backwards compatibility for legacy \"word\" Tokenizer", "\n", "# which provided arguments to intitalize current tokenizers", "\n", "# inside \"word_splitter\" key.", "\n", "        ", "tokenizer_type", "=", "params", ".", "get", "(", "\"type\"", ")", "\n", "splitter_params", "=", "params", ".", "get", "(", "\"word_splitter\"", ")", "\n", "if", "tokenizer_type", "==", "\"word\"", "or", "(", "tokenizer_type", "is", "None", "and", "splitter_params", ")", ":", "\n", "            ", "if", "not", "splitter_params", ":", "\n", "                ", "splitter_params", "=", "Params", "(", "{", "\"type\"", ":", "\"spacy\"", "}", ")", "\n", "", "elif", "isinstance", "(", "splitter_params", ",", "str", ")", ":", "\n", "                ", "splitter_params", "=", "Params", "(", "{", "\"type\"", ":", "splitter_params", "}", ")", "\n", "\n", "", "if", "params", ".", "get", "(", "\"word_filter\"", ")", "or", "params", ".", "get", "(", "\"word_stemmer\"", ")", ":", "\n", "                ", "raise", "ConfigurationError", "(", "\n", "\"Support for word_filter, word_stemmer is dropped in the current default tokenizer.\"", "\n", ")", "\n", "\n", "", "start_tokens", "=", "params", ".", "get", "(", "\"start_tokens\"", ")", "\n", "end_tokens", "=", "params", ".", "get", "(", "\"end_tokens\"", ")", "\n", "if", "start_tokens", ":", "\n", "                ", "splitter_params", "[", "\"start_tokens\"", "]", "=", "start_tokens", "\n", "", "if", "end_tokens", ":", "\n", "                ", "splitter_params", "[", "\"end_tokens\"", "]", "=", "end_tokens", "\n", "\n", "", "logger", ".", "warning", "(", "\n", "\"Converting old WordTokenizer params - %s \\n\"", "\"to new params %s.\"", ",", "\n", "str", "(", "params", ")", ",", "\n", "str", "(", "splitter_params", ")", ",", "\n", ")", "\n", "\n", "params", "=", "splitter_params", "\n", "\n", "", "return", "super", "(", ")", ".", "from_params", "(", "params", ",", "**", "extras", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.tokenizers.token.Token.__str__": [[49, 51], ["None"], "methods", ["None"], ["def", "__str__", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "text", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.tokenizers.token.Token.__repr__": [[52, 54], ["token.Token.__str__"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.ai2_internal.resume_daemon.Action.__str__"], ["", "def", "__repr__", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "__str__", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.tokenizers.token.show_token": [[56, 59], ["None"], "function", ["None"], ["", "", "def", "show_token", "(", "token", ":", "Token", ")", "->", "str", ":", "\n", "    ", "return", "(", "\n", "f\"{token.text} \"", "\n", "f\"(idx: {token.idx}) \"", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.tokenizers.spacy_tokenizer.SpacyTokenizer.__init__": [[50, 71], ["allennlp.common.util.get_spacy_model", "spacy_tokenizer.SpacyTokenizer._start_tokens.reverse", "spacy_tokenizer._WhitespaceSpacyTokenizer"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.util.get_spacy_model"], ["def", "__init__", "(", "\n", "self", ",", "\n", "language", ":", "str", "=", "\"en_core_web_sm\"", ",", "\n", "pos_tags", ":", "bool", "=", "False", ",", "\n", "parse", ":", "bool", "=", "False", ",", "\n", "ner", ":", "bool", "=", "False", ",", "\n", "keep_spacy_tokens", ":", "bool", "=", "False", ",", "\n", "split_on_spaces", ":", "bool", "=", "False", ",", "\n", "start_tokens", ":", "Optional", "[", "List", "[", "str", "]", "]", "=", "None", ",", "\n", "end_tokens", ":", "Optional", "[", "List", "[", "str", "]", "]", "=", "None", ",", "\n", ")", "->", "None", ":", "\n", "        ", "self", ".", "spacy", "=", "get_spacy_model", "(", "language", ",", "pos_tags", ",", "parse", ",", "ner", ")", "\n", "if", "split_on_spaces", ":", "\n", "            ", "self", ".", "spacy", ".", "tokenizer", "=", "_WhitespaceSpacyTokenizer", "(", "self", ".", "spacy", ".", "vocab", ")", "\n", "\n", "", "self", ".", "_keep_spacy_tokens", "=", "keep_spacy_tokens", "\n", "self", ".", "_start_tokens", "=", "start_tokens", "or", "[", "]", "\n", "# We reverse the tokens here because we're going to insert them with `insert(0)` later;", "\n", "# this makes sure they show up in the right order.", "\n", "self", ".", "_start_tokens", ".", "reverse", "(", ")", "\n", "self", ".", "_end_tokens", "=", "end_tokens", "or", "[", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.tokenizers.spacy_tokenizer.SpacyTokenizer._sanitize": [[72, 95], ["tokens.insert", "tokens.append", "allennlp.data.tokenizers.token.Token", "allennlp.data.tokenizers.token.Token", "allennlp.data.tokenizers.token.Token"], "methods", ["None"], ["", "def", "_sanitize", "(", "self", ",", "tokens", ":", "List", "[", "spacy", ".", "tokens", ".", "Token", "]", ")", "->", "List", "[", "Token", "]", ":", "\n", "        ", "\"\"\"\n        Converts spaCy tokens to allennlp tokens. Is a no-op if\n        keep_spacy_tokens is True\n        \"\"\"", "\n", "if", "not", "self", ".", "_keep_spacy_tokens", ":", "\n", "            ", "tokens", "=", "[", "\n", "Token", "(", "\n", "token", ".", "text", ",", "\n", "token", ".", "idx", ",", "\n", "token", ".", "lemma_", ",", "\n", "token", ".", "pos_", ",", "\n", "token", ".", "tag_", ",", "\n", "token", ".", "dep_", ",", "\n", "token", ".", "ent_type_", ",", "\n", ")", "\n", "for", "token", "in", "tokens", "\n", "]", "\n", "", "for", "start_token", "in", "self", ".", "_start_tokens", ":", "\n", "            ", "tokens", ".", "insert", "(", "0", ",", "Token", "(", "start_token", ",", "0", ")", ")", "\n", "", "for", "end_token", "in", "self", ".", "_end_tokens", ":", "\n", "            ", "tokens", ".", "append", "(", "Token", "(", "end_token", ",", "-", "1", ")", ")", "\n", "", "return", "tokens", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.tokenizers.spacy_tokenizer.SpacyTokenizer.batch_tokenize": [[96, 101], ["spacy_tokenizer.SpacyTokenizer._sanitize", "spacy_tokenizer._remove_spaces", "spacy_tokenizer.SpacyTokenizer.spacy.pipe"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.tokenizers.spacy_tokenizer.SpacyTokenizer._sanitize", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.tokenizers.pretrained_transformer_pre_tokenizer._remove_spaces"], ["", "@", "overrides", "\n", "def", "batch_tokenize", "(", "self", ",", "texts", ":", "List", "[", "str", "]", ")", "->", "List", "[", "List", "[", "Token", "]", "]", ":", "\n", "        ", "return", "[", "\n", "self", ".", "_sanitize", "(", "_remove_spaces", "(", "tokens", ")", ")", "\n", "for", "tokens", "in", "self", ".", "spacy", ".", "pipe", "(", "texts", ",", "n_threads", "=", "-", "1", ")", "\n", "]", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.tokenizers.spacy_tokenizer.SpacyTokenizer.tokenize": [[103, 107], ["spacy_tokenizer.SpacyTokenizer._sanitize", "spacy_tokenizer._remove_spaces", "spacy_tokenizer.SpacyTokenizer.spacy"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.tokenizers.spacy_tokenizer.SpacyTokenizer._sanitize", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.tokenizers.pretrained_transformer_pre_tokenizer._remove_spaces"], ["", "@", "overrides", "\n", "def", "tokenize", "(", "self", ",", "text", ":", "str", ")", "->", "List", "[", "Token", "]", ":", "\n", "# This works because our Token class matches spacy's.", "\n", "        ", "return", "self", ".", "_sanitize", "(", "_remove_spaces", "(", "self", ".", "spacy", "(", "text", ")", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.tokenizers.spacy_tokenizer._WhitespaceSpacyTokenizer.__init__": [[121, 123], ["None"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "vocab", ")", ":", "\n", "        ", "self", ".", "vocab", "=", "vocab", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.tokenizers.spacy_tokenizer._WhitespaceSpacyTokenizer.__call__": [[124, 128], ["text.split", "spacy.tokens.Doc", "len"], "methods", ["None"], ["", "def", "__call__", "(", "self", ",", "text", ")", ":", "\n", "        ", "words", "=", "text", ".", "split", "(", "\" \"", ")", "\n", "spaces", "=", "[", "True", "]", "*", "len", "(", "words", ")", "\n", "return", "Doc", "(", "self", ".", "vocab", ",", "words", "=", "words", ",", "spaces", "=", "spaces", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.tokenizers.spacy_tokenizer._remove_spaces": [[130, 132], ["None"], "function", ["None"], ["", "", "def", "_remove_spaces", "(", "tokens", ":", "List", "[", "spacy", ".", "tokens", ".", "Token", "]", ")", "->", "List", "[", "spacy", ".", "tokens", ".", "Token", "]", ":", "\n", "    ", "return", "[", "token", "for", "token", "in", "tokens", "if", "not", "token", ".", "is_space", "]", "\n", "", ""]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.tokenizers.pretrained_transformer_pre_tokenizer.BertPreTokenizer.__init__": [[24, 36], ["transformers.tokenization_bert.BasicTokenizer"], "methods", ["None"], ["def", "__init__", "(", "\n", "self", ",", "do_lower_case", ":", "bool", "=", "True", ",", "never_split", ":", "Optional", "[", "List", "[", "str", "]", "]", "=", "None", "\n", ")", "->", "None", ":", "\n", "\n", "        ", "if", "never_split", "is", "None", ":", "\n", "            ", "never_split", "=", "self", ".", "default_never_split", "\n", "", "else", ":", "\n", "            ", "never_split", "=", "never_split", "+", "self", ".", "default_never_split", "\n", "\n", "", "self", ".", "basic_tokenizer", "=", "BertTokenizer", "(", "do_lower_case", ",", "never_split", ")", "\n", "self", ".", "basic_tokenizer", ".", "_run_split_on_punc", "=", "self", ".", "_run_split_on_punc", "\n", "self", ".", "never_split", "=", "never_split", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.tokenizers.pretrained_transformer_pre_tokenizer.BertPreTokenizer.tokenize": [[37, 40], ["allennlp.data.tokenizers.token.Token", "pretrained_transformer_pre_tokenizer.BertPreTokenizer.basic_tokenizer.tokenize"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.tokenizers.whitespace_tokenizer.WhitespaceTokenizer.tokenize"], ["", "@", "overrides", "\n", "def", "tokenize", "(", "self", ",", "text", ":", "str", ")", "->", "List", "[", "Token", "]", ":", "\n", "        ", "return", "[", "Token", "(", "text", ")", "for", "text", "in", "self", ".", "basic_tokenizer", ".", "tokenize", "(", "text", ")", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.tokenizers.pretrained_transformer_pre_tokenizer.BertPreTokenizer._run_split_on_punc": [[44, 67], ["list", "len", "transformers.tokenization_bert._is_punctuation", "output.append", "output[].append", "output.append"], "methods", ["None"], ["", "def", "_run_split_on_punc", "(", "self", ",", "text", ",", "never_split", "=", "None", ")", ":", "\n", "        ", "\"\"\"Splits punctuation on a piece of text.\"\"\"", "\n", "if", "never_split", "is", "None", ":", "\n", "            ", "never_split", "=", "self", ".", "never_split", "\n", "", "if", "never_split", "is", "not", "None", "and", "text", "in", "never_split", ":", "\n", "            ", "return", "[", "text", "]", "\n", "", "chars", "=", "list", "(", "text", ")", "\n", "i", "=", "0", "\n", "start_new_word", "=", "True", "\n", "output", "=", "[", "]", "\n", "while", "i", "<", "len", "(", "chars", ")", ":", "\n", "            ", "char", "=", "chars", "[", "i", "]", "\n", "if", "_is_punctuation", "(", "char", ")", ":", "\n", "                ", "output", ".", "append", "(", "[", "char", "]", ")", "\n", "start_new_word", "=", "True", "\n", "", "else", ":", "\n", "                ", "if", "start_new_word", ":", "\n", "                    ", "output", ".", "append", "(", "[", "]", ")", "\n", "", "start_new_word", "=", "False", "\n", "output", "[", "-", "1", "]", ".", "append", "(", "char", ")", "\n", "", "i", "+=", "1", "\n", "\n", "", "return", "[", "\"\"", ".", "join", "(", "x", ")", "for", "x", "in", "output", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.tokenizers.pretrained_transformer_pre_tokenizer._remove_spaces": [[69, 71], ["None"], "function", ["None"], ["", "", "def", "_remove_spaces", "(", "tokens", ":", "List", "[", "spacy", ".", "tokens", ".", "Token", "]", ")", "->", "List", "[", "spacy", ".", "tokens", ".", "Token", "]", ":", "\n", "    ", "return", "[", "token", "for", "token", "in", "tokens", "if", "not", "token", ".", "is_space", "]", "\n", "", ""]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.tokenizers.letters_digits_tokenizer.LettersDigitsTokenizer.tokenize": [[17, 25], ["allennlp.data.tokenizers.token.Token", "m.group", "re.finditer", "m.start"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.dataset_readers.multiprocess_dataset_reader.QIterable.start"], ["@", "overrides", "\n", "def", "tokenize", "(", "self", ",", "text", ":", "str", ")", "->", "List", "[", "Token", "]", ":", "\n", "# We use the [^\\W\\d_] pattern as a trick to match unicode letters", "\n", "        ", "tokens", "=", "[", "\n", "Token", "(", "m", ".", "group", "(", ")", ",", "idx", "=", "m", ".", "start", "(", ")", ")", "\n", "for", "m", "in", "re", ".", "finditer", "(", "r\"[^\\W\\d_]+|\\d+|\\S\"", ",", "text", ")", "\n", "]", "\n", "return", "tokens", "\n", "", "", ""]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.tokenizers.sentence_splitter.SentenceSplitter.split_sentences": [[17, 22], ["None"], "methods", ["None"], ["def", "split_sentences", "(", "self", ",", "text", ":", "str", ")", "->", "List", "[", "str", "]", ":", "\n", "        ", "\"\"\"\n        Splits a `text` :class:`str` paragraph into a list of :class:`str`, where each is a sentence.\n        \"\"\"", "\n", "raise", "NotImplementedError", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.tokenizers.sentence_splitter.SentenceSplitter.batch_split_sentences": [[23, 28], ["sentence_splitter.SentenceSplitter.split_sentences"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.tokenizers.sentence_splitter.SpacySentenceSplitter.split_sentences"], ["", "def", "batch_split_sentences", "(", "self", ",", "texts", ":", "List", "[", "str", "]", ")", "->", "List", "[", "List", "[", "str", "]", "]", ":", "\n", "        ", "\"\"\"\n        Default implementation is to just iterate over the texts and call `split_sentences`.\n        \"\"\"", "\n", "return", "[", "self", ".", "split_sentences", "(", "text", ")", "for", "text", "in", "texts", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.tokenizers.sentence_splitter.SpacySentenceSplitter.__init__": [[44, 58], ["allennlp.common.util.get_spacy_model", "sentence_splitter.SpacySentenceSplitter.spacy.has_pipe", "sentence_splitter.SpacySentenceSplitter.spacy.create_pipe", "sentence_splitter.SpacySentenceSplitter.spacy.add_pipe"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.util.get_spacy_model"], ["def", "__init__", "(", "\n", "self", ",", "language", ":", "str", "=", "\"en_core_web_sm\"", ",", "rule_based", ":", "bool", "=", "False", "\n", ")", "->", "None", ":", "\n", "# we need spacy's dependency parser if we're not using rule-based sentence boundary detection.", "\n", "        ", "self", ".", "spacy", "=", "get_spacy_model", "(", "\n", "language", ",", "parse", "=", "not", "rule_based", ",", "ner", "=", "False", ",", "pos_tags", "=", "False", "\n", ")", "\n", "if", "rule_based", ":", "\n", "# we use `sentencizer`, a built-in spacy module for rule-based sentence boundary detection.", "\n", "# depending on the spacy version, it could be called 'sentencizer' or 'sbd'", "\n", "            ", "sbd_name", "=", "\"sbd\"", "if", "spacy", ".", "__version__", "<", "\"2.1\"", "else", "\"sentencizer\"", "\n", "if", "not", "self", ".", "spacy", ".", "has_pipe", "(", "sbd_name", ")", ":", "\n", "                ", "sbd", "=", "self", ".", "spacy", ".", "create_pipe", "(", "sbd_name", ")", "\n", "self", ".", "spacy", ".", "add_pipe", "(", "sbd", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.tokenizers.sentence_splitter.SpacySentenceSplitter.split_sentences": [[59, 62], ["sent.string.strip", "sentence_splitter.SpacySentenceSplitter.spacy"], "methods", ["None"], ["", "", "", "@", "overrides", "\n", "def", "split_sentences", "(", "self", ",", "text", ":", "str", ")", "->", "List", "[", "str", "]", ":", "\n", "        ", "return", "[", "sent", ".", "string", ".", "strip", "(", ")", "for", "sent", "in", "self", ".", "spacy", "(", "text", ")", ".", "sents", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.tokenizers.sentence_splitter.SpacySentenceSplitter.batch_split_sentences": [[63, 71], ["sentence.string.strip", "sentence_splitter.SpacySentenceSplitter.spacy.pipe"], "methods", ["None"], ["", "@", "overrides", "\n", "def", "batch_split_sentences", "(", "self", ",", "texts", ":", "List", "[", "str", "]", ")", "->", "List", "[", "List", "[", "str", "]", "]", ":", "\n", "        ", "\"\"\"\n        This method lets you take advantage of spacy's batch processing.\n        \"\"\"", "\n", "return", "[", "\n", "[", "sentence", ".", "string", ".", "strip", "(", ")", "for", "sentence", "in", "doc", ".", "sents", "]", "\n", "for", "doc", "in", "self", ".", "spacy", ".", "pipe", "(", "texts", ")", "\n", "]", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.tokenizers.pretrained_transformer_tokenizer.PretrainedTransformerTokenizer.__init__": [[60, 91], ["transformers.tokenization_auto.AutoTokenizer.from_pretrained", "pretrained_transformer_tokenizer.PretrainedTransformerTokenizer.tokenizer.tokenize", "pretrained_transformer_tokenizer.PretrainedTransformerTokenizer._determine_num_special_tokens_added"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.tokenizers.whitespace_tokenizer.WhitespaceTokenizer.tokenize", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.tokenizers.pretrained_transformer_tokenizer.PretrainedTransformerTokenizer._determine_num_special_tokens_added"], ["def", "__init__", "(", "\n", "self", ",", "\n", "model_name", ":", "str", ",", "\n", "add_special_tokens", ":", "bool", "=", "True", ",", "\n", "max_length", ":", "int", "=", "None", ",", "\n", "stride", ":", "int", "=", "0", ",", "\n", "truncation_strategy", ":", "str", "=", "\"longest_first\"", ",", "\n", "calculate_character_offsets", ":", "bool", "=", "False", ",", "\n", "tokenizer_kwargs", ":", "Dict", "[", "str", ",", "Any", "]", "=", "None", ",", "\n", ")", "->", "None", ":", "\n", "        ", "tokenizer_kwargs", "=", "tokenizer_kwargs", "or", "{", "}", "\n", "self", ".", "tokenizer", "=", "AutoTokenizer", ".", "from_pretrained", "(", "model_name", ",", "**", "tokenizer_kwargs", ")", "\n", "\n", "# Huggingface tokenizers have different ways of remembering whether they lowercase or not. Detecting it", "\n", "# this way seems like the least brittle way to do it.", "\n", "tokenized", "=", "self", ".", "tokenizer", ".", "tokenize", "(", "\n", "\"A\"", "\n", ")", "# Use a single character that won't be cut into word pieces.", "\n", "detokenized", "=", "\" \"", ".", "join", "(", "tokenized", ")", "\n", "self", ".", "_tokenizer_lowercases", "=", "\"a\"", "in", "detokenized", "\n", "\n", "self", ".", "_add_special_tokens", "=", "add_special_tokens", "\n", "self", ".", "_max_length", "=", "max_length", "\n", "self", ".", "_stride", "=", "stride", "\n", "self", ".", "_truncation_strategy", "=", "truncation_strategy", "\n", "self", ".", "_calculate_character_offsets", "=", "calculate_character_offsets", "\n", "\n", "(", "\n", "self", ".", "num_added_start_tokens", ",", "\n", "self", ".", "num_added_end_tokens", ",", "\n", ")", "=", "self", ".", "_determine_num_special_tokens_added", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.tokenizers.pretrained_transformer_tokenizer.PretrainedTransformerTokenizer._tokenize": [[92, 181], ["pretrained_transformer_tokenizer.PretrainedTransformerTokenizer.tokenizer.encode_plus", "zip", "pretrained_transformer_tokenizer.PretrainedTransformerTokenizer.tokenizer.convert_ids_to_tokens", "tokens.append", "allennlp.data.tokenizers.token.Token", "whole_text.lower.lower.lower", "allennlp.common.util.sanitize_wordpiece", "whole_text.lower.lower.find", "sum", "tokens[]._replace", "len", "len", "token_text.lower.lower.lower", "len", "c.isspace"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.util.sanitize_wordpiece"], ["", "def", "_tokenize", "(", "self", ",", "sentence_1", ":", "str", ",", "sentence_2", ":", "str", "=", "None", ")", ":", "\n", "        ", "\"\"\"\n        This method works on both sentence and sentence pair.\n        \"\"\"", "\n", "\n", "encoded_tokens", "=", "self", ".", "tokenizer", ".", "encode_plus", "(", "\n", "text", "=", "sentence_1", ",", "\n", "text_pair", "=", "sentence_2", ",", "\n", "add_special_tokens", "=", "self", ".", "_add_special_tokens", ",", "\n", "max_length", "=", "self", ".", "_max_length", ",", "\n", "stride", "=", "self", ".", "_stride", ",", "\n", "truncation_strategy", "=", "self", ".", "_truncation_strategy", ",", "\n", "return_tensors", "=", "None", ",", "\n", ")", "\n", "# token_ids contains a final list with ids for both regular and special tokens", "\n", "token_ids", ",", "token_type_ids", "=", "(", "\n", "encoded_tokens", "[", "\"input_ids\"", "]", ",", "\n", "encoded_tokens", "[", "\"token_type_ids\"", "]", ",", "\n", ")", "\n", "\n", "tokens", "=", "[", "]", "\n", "for", "token_id", ",", "token_type_id", "in", "zip", "(", "token_ids", ",", "token_type_ids", ")", ":", "\n", "            ", "token_str", "=", "self", ".", "tokenizer", ".", "convert_ids_to_tokens", "(", "\n", "token_id", ",", "skip_special_tokens", "=", "False", "\n", ")", "\n", "tokens", ".", "append", "(", "\n", "Token", "(", "text", "=", "token_str", ",", "text_id", "=", "token_id", ",", "type_id", "=", "token_type_id", ")", "\n", ")", "\n", "\n", "", "if", "self", ".", "_calculate_character_offsets", ":", "\n", "# The huggingface tokenizers produce tokens that may or may not be slices from the", "\n", "# original text.  Differences arise from lowercasing, Unicode normalization, and other", "\n", "# kinds of normalization, as well as special characters that are included to denote", "\n", "# various situations, such as \"##\" in BERT for word pieces from the middle of a word, or", "\n", "# \"\u0120\" in RoBERTa for the beginning of words not at the start of a sentence.", "\n", "\n", "# This code attempts to calculate character offsets while being tolerant to these", "\n", "# differences. It scans through the text and the tokens in parallel, trying to match up", "\n", "# positions in both. If it gets out of sync, it backs off to not adding any token", "\n", "# indices, and attempts to catch back up afterwards. This procedure is approximate.", "\n", "# Don't rely on precise results, especially in non-English languages that are far more", "\n", "# affected by Unicode normalization.", "\n", "\n", "            ", "whole_text", "=", "sentence_1", "\n", "if", "sentence_2", "is", "not", "None", ":", "\n", "                ", "whole_text", "+=", "sentence_2", "# Calculating character offsets with sentence pairs is sketchy at best.", "\n", "", "if", "self", ".", "_tokenizer_lowercases", ":", "\n", "                ", "whole_text", "=", "whole_text", ".", "lower", "(", ")", "\n", "\n", "", "min_allowed_skipped_whitespace", "=", "3", "\n", "allowed_skipped_whitespace", "=", "min_allowed_skipped_whitespace", "\n", "\n", "text_index", "=", "0", "\n", "token_index", "=", "0", "\n", "while", "text_index", "<", "len", "(", "whole_text", ")", "and", "token_index", "<", "len", "(", "tokens", ")", ":", "\n", "                ", "token_text", "=", "tokens", "[", "token_index", "]", ".", "text", "\n", "if", "self", ".", "_tokenizer_lowercases", ":", "\n", "                    ", "token_text", "=", "token_text", ".", "lower", "(", ")", "\n", "", "token_text", "=", "sanitize_wordpiece", "(", "token_text", ")", "\n", "token_start_index", "=", "whole_text", ".", "find", "(", "token_text", ",", "text_index", ")", "\n", "\n", "# Did we not find it at all?", "\n", "if", "token_start_index", "<", "0", ":", "\n", "                    ", "token_index", "+=", "1", "\n", "# When we skip a token, we increase our tolerance, so we have a chance of catching back up.", "\n", "allowed_skipped_whitespace", "+=", "1", "+", "min_allowed_skipped_whitespace", "\n", "continue", "\n", "\n", "# Did we jump too far?", "\n", "", "non_whitespace_chars_skipped", "=", "sum", "(", "\n", "1", "\n", "for", "c", "in", "whole_text", "[", "text_index", ":", "token_start_index", "]", "\n", "if", "not", "c", ".", "isspace", "(", ")", "\n", ")", "\n", "if", "non_whitespace_chars_skipped", ">", "allowed_skipped_whitespace", ":", "\n", "# Too many skipped characters. Something is wrong. Ignore this token.", "\n", "                    ", "token_index", "+=", "1", "\n", "# When we skip a token, we increase our tolerance, so we have a chance of catching back up.", "\n", "allowed_skipped_whitespace", "+=", "1", "+", "min_allowed_skipped_whitespace", "\n", "continue", "\n", "", "allowed_skipped_whitespace", "=", "min_allowed_skipped_whitespace", "\n", "\n", "tokens", "[", "token_index", "]", "=", "tokens", "[", "token_index", "]", ".", "_replace", "(", "\n", "idx", "=", "token_start_index", "\n", ")", "\n", "text_index", "=", "token_start_index", "+", "len", "(", "token_text", ")", "\n", "token_index", "+=", "1", "\n", "\n", "", "", "return", "tokens", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.tokenizers.pretrained_transformer_tokenizer.PretrainedTransformerTokenizer.tokenize_sentence_pair": [[182, 187], ["pretrained_transformer_tokenizer.PretrainedTransformerTokenizer._tokenize"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.tokenizers.pretrained_transformer_tokenizer.PretrainedTransformerTokenizer._tokenize"], ["", "def", "tokenize_sentence_pair", "(", "self", ",", "sentence_1", ":", "str", ",", "sentence_2", ":", "str", ")", "->", "List", "[", "Token", "]", ":", "\n", "        ", "\"\"\"\n        This methods properly handles a pair of sentences.\n        \"\"\"", "\n", "return", "self", ".", "_tokenize", "(", "sentence_1", ",", "sentence_2", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.tokenizers.pretrained_transformer_tokenizer.PretrainedTransformerTokenizer.tokenize": [[188, 195], ["pretrained_transformer_tokenizer.PretrainedTransformerTokenizer._tokenize"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.tokenizers.pretrained_transformer_tokenizer.PretrainedTransformerTokenizer._tokenize"], ["", "@", "overrides", "\n", "def", "tokenize", "(", "self", ",", "text", ":", "str", ")", "->", "List", "[", "Token", "]", ":", "\n", "        ", "\"\"\"\n        This method only handles a single sentence (or sequence) of text.\n        Refer to the `tokenize_sentence_pair` method if you have a sentence pair.\n        \"\"\"", "\n", "return", "self", ".", "_tokenize", "(", "text", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.tokenizers.pretrained_transformer_tokenizer.PretrainedTransformerTokenizer.intra_word_tokenize": [[196, 233], ["pretrained_transformer_tokenizer.PretrainedTransformerTokenizer.tokenizer.build_inputs_with_special_tokens", "pretrained_transformer_tokenizer.PretrainedTransformerTokenizer.tokenizer.convert_ids_to_tokens", "pretrained_transformer_tokenizer.PretrainedTransformerTokenizer.tokenizer.create_token_type_ids_from_sequences", "pretrained_transformer_tokenizer.PretrainedTransformerTokenizer.tokenizer.encode", "pretrained_transformer_tokenizer.PretrainedTransformerTokenizer.extend", "len", "offsets.append", "len", "allennlp.data.tokenizers.token.Token", "zip"], "methods", ["None"], ["", "def", "intra_word_tokenize", "(", "\n", "self", ",", "tokens", ":", "List", "[", "str", "]", "\n", ")", "->", "Tuple", "[", "List", "[", "Token", "]", ",", "List", "[", "Tuple", "[", "int", ",", "int", "]", "]", "]", ":", "\n", "        ", "\"\"\"\n        Tokenizes each word into wordpieces separately and returns the wordpiece IDs.\n        Also calculates offsets such that wordpices[offsets[i][0]:offsets[i][1] + 1]\n        corresponds to the original i-th token.\n\n        This function inserts special tokens.\n        \"\"\"", "\n", "wordpieces", ":", "List", "[", "int", "]", "=", "[", "]", "\n", "offsets", "=", "[", "]", "\n", "cumulative", "=", "self", ".", "num_added_start_tokens", "\n", "for", "token", "in", "tokens", ":", "\n", "            ", "subword_wordpieces", "=", "self", ".", "tokenizer", ".", "encode", "(", "token", ",", "add_special_tokens", "=", "False", ")", "\n", "wordpieces", ".", "extend", "(", "subword_wordpieces", ")", "\n", "\n", "start_offset", "=", "cumulative", "\n", "cumulative", "+=", "len", "(", "subword_wordpieces", ")", "\n", "end_offset", "=", "cumulative", "-", "1", "# inclusive", "\n", "offsets", ".", "append", "(", "(", "start_offset", ",", "end_offset", ")", ")", "\n", "\n", "", "wordpieces", "=", "self", ".", "tokenizer", ".", "build_inputs_with_special_tokens", "(", "wordpieces", ")", "\n", "assert", "cumulative", "+", "self", ".", "num_added_end_tokens", "==", "len", "(", "wordpieces", ")", "\n", "\n", "texts", "=", "self", ".", "tokenizer", ".", "convert_ids_to_tokens", "(", "wordpieces", ")", "\n", "# `create_token_type_ids_from_sequences()` inserts special tokens", "\n", "type_ids", "=", "self", ".", "tokenizer", ".", "create_token_type_ids_from_sequences", "(", "\n", "wordpieces", "[", "self", ".", "num_added_start_tokens", ":", "-", "self", ".", "num_added_end_tokens", "]", "\n", ")", "\n", "\n", "wp_tokens", "=", "[", "\n", "Token", "(", "text", ",", "text_id", "=", "text_id", ",", "type_id", "=", "type_id", ")", "\n", "for", "text", ",", "text_id", ",", "type_id", "in", "zip", "(", "texts", ",", "wordpieces", ",", "type_ids", ")", "\n", "]", "\n", "\n", "return", "wp_tokens", ",", "offsets", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.tokenizers.pretrained_transformer_tokenizer.PretrainedTransformerTokenizer._determine_num_special_tokens_added": [[234, 266], ["pretrained_transformer_tokenizer.PretrainedTransformerTokenizer.tokenizer.build_inputs_with_special_tokens", "pretrained_transformer_tokenizer.PretrainedTransformerTokenizer.tokenizer.num_added_tokens", "ValueError"], "methods", ["None"], ["", "def", "_determine_num_special_tokens_added", "(", "self", ")", "->", "Tuple", "[", "int", ",", "int", "]", ":", "\n", "        ", "\"\"\"\n        Determines the number of tokens `tokenizer` adds to a sequence (currently doesn't\n        consider sequence pairs) in the start & end.\n\n        # Returns\n\n        The number of tokens (`int`) that are inserted in the start & end of a sequence.\n        \"\"\"", "\n", "# Uses a slightly higher index to avoid tokenizer doing special things to lower-indexed", "\n", "# tokens which might be special.", "\n", "dummy", "=", "[", "1000", "]", "\n", "inserted", "=", "self", ".", "tokenizer", ".", "build_inputs_with_special_tokens", "(", "dummy", ")", "\n", "\n", "num_start", "=", "num_end", "=", "0", "\n", "seen_dummy", "=", "False", "\n", "for", "idx", "in", "inserted", ":", "\n", "            ", "if", "idx", "==", "dummy", "[", "0", "]", ":", "\n", "                ", "if", "seen_dummy", ":", "# seeing it twice", "\n", "                    ", "raise", "ValueError", "(", "\n", "\"Cannot auto-determine the number of special tokens added.\"", "\n", ")", "\n", "", "seen_dummy", "=", "True", "\n", "continue", "\n", "\n", "", "if", "not", "seen_dummy", ":", "\n", "                ", "num_start", "+=", "1", "\n", "", "else", ":", "\n", "                ", "num_end", "+=", "1", "\n", "\n", "", "", "assert", "num_start", "+", "num_end", "==", "self", ".", "tokenizer", ".", "num_added_tokens", "(", ")", "\n", "return", "num_start", ",", "num_end", "\n", "", "", ""]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.tokenizers.whitespace_tokenizer.WhitespaceTokenizer.tokenize": [[21, 24], ["allennlp.data.tokenizers.token.Token", "text.split"], "methods", ["None"], ["@", "overrides", "\n", "def", "tokenize", "(", "self", ",", "text", ":", "str", ")", "->", "List", "[", "Token", "]", ":", "\n", "        ", "return", "[", "Token", "(", "t", ")", "for", "t", "in", "text", ".", "split", "(", ")", "]", "\n", "", "", ""]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.token_indexers.dep_label_indexer.DepLabelIndexer.__init__": [[27, 33], ["allennlp.data.token_indexers.token_indexer.TokenIndexer.__init__", "set"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.graph_util.plot.GraphPlot.__init__"], ["def", "__init__", "(", "\n", "self", ",", "namespace", ":", "str", "=", "\"dep_labels\"", ",", "token_min_padding_length", ":", "int", "=", "0", "\n", ")", "->", "None", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "token_min_padding_length", ")", "\n", "self", ".", "namespace", "=", "namespace", "\n", "self", ".", "_logged_errors", ":", "Set", "[", "str", "]", "=", "set", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.token_indexers.dep_label_indexer.DepLabelIndexer.count_vocab_items": [[34, 43], ["logger.warning", "dep_label_indexer.DepLabelIndexer._logged_errors.add"], "methods", ["None"], ["", "@", "overrides", "\n", "def", "count_vocab_items", "(", "self", ",", "token", ":", "Token", ",", "counter", ":", "Dict", "[", "str", ",", "Dict", "[", "str", ",", "int", "]", "]", ")", ":", "\n", "        ", "dep_label", "=", "token", ".", "dep_", "\n", "if", "not", "dep_label", ":", "\n", "            ", "if", "token", ".", "text", "not", "in", "self", ".", "_logged_errors", ":", "\n", "                ", "logger", ".", "warning", "(", "\"Token had no dependency label: %s\"", ",", "token", ".", "text", ")", "\n", "self", ".", "_logged_errors", ".", "add", "(", "token", ".", "text", ")", "\n", "", "dep_label", "=", "\"NONE\"", "\n", "", "counter", "[", "self", ".", "namespace", "]", "[", "dep_label", "]", "+=", "1", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.token_indexers.dep_label_indexer.DepLabelIndexer.tokens_to_indices": [[44, 54], ["vocabulary.get_token_index"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.data.vocabulary.Vocabulary.get_token_index"], ["", "@", "overrides", "\n", "def", "tokens_to_indices", "(", "\n", "self", ",", "tokens", ":", "List", "[", "Token", "]", ",", "vocabulary", ":", "Vocabulary", "\n", ")", "->", "Dict", "[", "str", ",", "List", "[", "int", "]", "]", ":", "\n", "        ", "dep_labels", "=", "[", "token", ".", "dep_", "or", "\"NONE\"", "for", "token", "in", "tokens", "]", "\n", "\n", "return", "{", "\n", "\"tokens\"", ":", "[", "\n", "vocabulary", ".", "get_token_index", "(", "dep_label", ",", "self", ".", "namespace", ")", "\n", "for", "dep_label", "in", "dep_labels", "\n", "]", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.token_indexers.dep_label_indexer.DepLabelIndexer.get_empty_token_list": [[57, 60], ["None"], "methods", ["None"], ["", "@", "overrides", "\n", "def", "get_empty_token_list", "(", "self", ")", "->", "IndexedTokenList", ":", "\n", "        ", "return", "{", "\"tokens\"", ":", "[", "]", "}", "\n", "", "", ""]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.token_indexers.pos_tag_indexer.PosTagIndexer.__init__": [[30, 40], ["allennlp.data.token_indexers.token_indexer.TokenIndexer.__init__", "set"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.graph_util.plot.GraphPlot.__init__"], ["def", "__init__", "(", "\n", "self", ",", "\n", "namespace", ":", "str", "=", "\"pos_tokens\"", ",", "\n", "coarse_tags", ":", "bool", "=", "False", ",", "\n", "token_min_padding_length", ":", "int", "=", "0", ",", "\n", ")", "->", "None", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "token_min_padding_length", ")", "\n", "self", ".", "_namespace", "=", "namespace", "\n", "self", ".", "_coarse_tags", "=", "coarse_tags", "\n", "self", ".", "_logged_errors", ":", "Set", "[", "str", "]", "=", "set", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.token_indexers.pos_tag_indexer.PosTagIndexer.count_vocab_items": [[41, 53], ["logger.warning", "pos_tag_indexer.PosTagIndexer._logged_errors.add"], "methods", ["None"], ["", "@", "overrides", "\n", "def", "count_vocab_items", "(", "self", ",", "token", ":", "Token", ",", "counter", ":", "Dict", "[", "str", ",", "Dict", "[", "str", ",", "int", "]", "]", ")", ":", "\n", "        ", "if", "self", ".", "_coarse_tags", ":", "\n", "            ", "tag", "=", "token", ".", "pos_", "\n", "", "else", ":", "\n", "            ", "tag", "=", "token", ".", "tag_", "\n", "", "if", "not", "tag", ":", "\n", "            ", "if", "token", ".", "text", "not", "in", "self", ".", "_logged_errors", ":", "\n", "                ", "logger", ".", "warning", "(", "\"Token had no POS tag: %s\"", ",", "token", ".", "text", ")", "\n", "self", ".", "_logged_errors", ".", "add", "(", "token", ".", "text", ")", "\n", "", "tag", "=", "\"NONE\"", "\n", "", "counter", "[", "self", ".", "_namespace", "]", "[", "tag", "]", "+=", "1", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.token_indexers.pos_tag_indexer.PosTagIndexer.tokens_to_indices": [[54, 72], ["tags.append", "vocabulary.get_token_index"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.data.vocabulary.Vocabulary.get_token_index"], ["", "@", "overrides", "\n", "def", "tokens_to_indices", "(", "\n", "self", ",", "tokens", ":", "List", "[", "Token", "]", ",", "vocabulary", ":", "Vocabulary", "\n", ")", "->", "Dict", "[", "str", ",", "List", "[", "int", "]", "]", ":", "\n", "        ", "tags", ":", "List", "[", "str", "]", "=", "[", "]", "\n", "\n", "for", "token", "in", "tokens", ":", "\n", "            ", "if", "self", ".", "_coarse_tags", ":", "\n", "                ", "tag", "=", "token", ".", "pos_", "\n", "", "else", ":", "\n", "                ", "tag", "=", "token", ".", "tag_", "\n", "", "if", "not", "tag", ":", "\n", "                ", "tag", "=", "\"NONE\"", "\n", "\n", "", "tags", ".", "append", "(", "tag", ")", "\n", "\n", "", "return", "{", "\n", "\"tokens\"", ":", "[", "vocabulary", ".", "get_token_index", "(", "tag", ",", "self", ".", "_namespace", ")", "for", "tag", "in", "tags", "]", "\n", "}", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.token_indexers.pos_tag_indexer.PosTagIndexer.get_empty_token_list": [[74, 77], ["None"], "methods", ["None"], ["", "@", "overrides", "\n", "def", "get_empty_token_list", "(", "self", ")", "->", "IndexedTokenList", ":", "\n", "        ", "return", "{", "\"tokens\"", ":", "[", "]", "}", "\n", "", "", ""]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.token_indexers.token_indexer.TokenIndexer.__init__": [[42, 44], ["None"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "token_min_padding_length", ":", "int", "=", "0", ")", "->", "None", ":", "\n", "        ", "self", ".", "_token_min_padding_length", ":", "int", "=", "token_min_padding_length", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.token_indexers.token_indexer.TokenIndexer.count_vocab_items": [[45, 55], ["None"], "methods", ["None"], ["", "def", "count_vocab_items", "(", "self", ",", "token", ":", "Token", ",", "counter", ":", "Dict", "[", "str", ",", "Dict", "[", "str", ",", "int", "]", "]", ")", ":", "\n", "        ", "\"\"\"\n        The :class:`Vocabulary` needs to assign indices to whatever strings we see in the training\n        data (possibly doing some frequency filtering and using an OOV, or out of vocabulary,\n        token).  This method takes a token and a dictionary of counts and increments counts for\n        whatever vocabulary items are present in the token.  If this is a single token ID\n        representation, the vocabulary item is likely the token itself.  If this is a token\n        characters representation, the vocabulary items are all of the characters in the token.\n        \"\"\"", "\n", "raise", "NotImplementedError", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.token_indexers.token_indexer.TokenIndexer.tokens_to_indices": [[56, 68], ["None"], "methods", ["None"], ["", "def", "tokens_to_indices", "(", "\n", "self", ",", "tokens", ":", "List", "[", "Token", "]", ",", "vocabulary", ":", "Vocabulary", "\n", ")", "->", "IndexedTokenList", ":", "\n", "        ", "\"\"\"\n        Takes a list of tokens and converts them to an `IndexedTokenList`.\n        This could be just an ID for each token from the vocabulary.\n        Or it could split each token into characters and return one ID per character.\n        Or (for instance, in the case of byte-pair encoding) there might not be a clean\n        mapping from individual tokens to indices, and the `IndexedTokenList` could be a complex\n        data structure.\n        \"\"\"", "\n", "raise", "NotImplementedError", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.token_indexers.token_indexer.TokenIndexer.get_empty_token_list": [[69, 75], ["None"], "methods", ["None"], ["", "def", "get_empty_token_list", "(", "self", ")", "->", "IndexedTokenList", ":", "\n", "        ", "\"\"\"\n        Returns an `already indexed` version of an empty token list.  This is typically just an\n        empty list for whatever keys are used in the indexer.\n        \"\"\"", "\n", "raise", "NotImplementedError", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.token_indexers.token_indexer.TokenIndexer.get_padding_lengths": [[76, 88], ["indexed_tokens.items", "max", "len"], "methods", ["None"], ["", "def", "get_padding_lengths", "(", "self", ",", "indexed_tokens", ":", "IndexedTokenList", ")", "->", "Dict", "[", "str", ",", "int", "]", ":", "\n", "        ", "\"\"\"\n        This method returns a padding dictionary for the given `indexed_tokens` specifying all\n        lengths that need padding.  If all you have is a list of single ID tokens, this is just the\n        length of the list, and that's what the default implementation will give you.  If you have\n        something more complicated, like a list of character ids for token, you'll need to override\n        this.\n        \"\"\"", "\n", "padding_lengths", "=", "{", "}", "\n", "for", "key", ",", "token_list", "in", "indexed_tokens", ".", "items", "(", ")", ":", "\n", "            ", "padding_lengths", "[", "key", "]", "=", "max", "(", "len", "(", "token_list", ")", ",", "self", ".", "_token_min_padding_length", ")", "\n", "", "return", "padding_lengths", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.token_indexers.token_indexer.TokenIndexer.as_padded_tensor_dict": [[89, 110], ["tokens.items", "torch.LongTensor", "allennlp.common.util.pad_sequence_to_length"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.util.pad_sequence_to_length"], ["", "def", "as_padded_tensor_dict", "(", "\n", "self", ",", "tokens", ":", "IndexedTokenList", ",", "padding_lengths", ":", "Dict", "[", "str", ",", "int", "]", "\n", ")", "->", "Dict", "[", "str", ",", "torch", ".", "Tensor", "]", ":", "\n", "        ", "\"\"\"\n        This method pads a list of tokens given the input padding lengths (which could actually\n        truncate things, depending on settings) and returns that padded list of input tokens as a\n        `Dict[str, torch.Tensor]`.  This is a dictionary because there should be one key per\n        argument that the `TokenEmbedder` corresponding to this class expects in its `forward()`\n        method (where the argument name in the `TokenEmbedder` needs to make the key in this\n        dictionary).\n\n        The base class implements the case when all you want to do is create a padded `LongTensor`\n        for every list in the `tokens` dictionary.  If your `TokenIndexer` needs more complex\n        logic than that, you need to override this method.\n        \"\"\"", "\n", "tensor_dict", "=", "{", "}", "\n", "for", "key", ",", "val", "in", "tokens", ".", "items", "(", ")", ":", "\n", "            ", "tensor_dict", "[", "key", "]", "=", "torch", ".", "LongTensor", "(", "\n", "pad_sequence_to_length", "(", "val", ",", "padding_lengths", "[", "key", "]", ")", "\n", ")", "\n", "", "return", "tensor_dict", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.token_indexers.token_indexer.TokenIndexer.__eq__": [[111, 115], ["isinstance"], "methods", ["None"], ["", "def", "__eq__", "(", "self", ",", "other", ")", "->", "bool", ":", "\n", "        ", "if", "isinstance", "(", "self", ",", "other", ".", "__class__", ")", ":", "\n", "            ", "return", "self", ".", "__dict__", "==", "other", ".", "__dict__", "\n", "", "return", "NotImplemented", "\n", "", "", ""]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.token_indexers.pretrained_transformer_mismatched_indexer.PretrainedTransformerMismatchedIndexer.__init__": [[42, 54], ["allennlp.data.token_indexers.TokenIndexer.__init__", "allennlp.data.token_indexers.PretrainedTransformerIndexer"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.graph_util.plot.GraphPlot.__init__"], ["def", "__init__", "(", "\n", "self", ",", "model_name", ":", "str", ",", "namespace", ":", "str", "=", "\"tags\"", ",", "max_length", ":", "int", "=", "None", ",", "**", "kwargs", "\n", ")", "->", "None", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "**", "kwargs", ")", "\n", "# The matched version v.s. mismatched", "\n", "self", ".", "_matched_indexer", "=", "PretrainedTransformerIndexer", "(", "\n", "model_name", ",", "namespace", ",", "max_length", ",", "**", "kwargs", "\n", ")", "\n", "self", ".", "_allennlp_tokenizer", "=", "self", ".", "_matched_indexer", ".", "_allennlp_tokenizer", "\n", "self", ".", "_tokenizer", "=", "self", ".", "_matched_indexer", ".", "_tokenizer", "\n", "self", ".", "_num_added_start_tokens", "=", "self", ".", "_matched_indexer", ".", "_num_added_start_tokens", "\n", "self", ".", "_num_added_end_tokens", "=", "self", ".", "_matched_indexer", ".", "_num_added_end_tokens", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.token_indexers.pretrained_transformer_mismatched_indexer.PretrainedTransformerMismatchedIndexer.count_vocab_items": [[55, 58], ["pretrained_transformer_mismatched_indexer.PretrainedTransformerMismatchedIndexer._matched_indexer.count_vocab_items"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.fields.text_field.TextField.count_vocab_items"], ["", "@", "overrides", "\n", "def", "count_vocab_items", "(", "self", ",", "token", ":", "Token", ",", "counter", ":", "Dict", "[", "str", ",", "Dict", "[", "str", ",", "int", "]", "]", ")", ":", "\n", "        ", "return", "self", ".", "_matched_indexer", ".", "count_vocab_items", "(", "token", ",", "counter", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.token_indexers.pretrained_transformer_mismatched_indexer.PretrainedTransformerMismatchedIndexer.tokens_to_indices": [[59, 78], ["pretrained_transformer_mismatched_indexer.PretrainedTransformerMismatchedIndexer._matched_indexer._add_encoding_to_vocabulary_if_needed", "pretrained_transformer_mismatched_indexer.PretrainedTransformerMismatchedIndexer._allennlp_tokenizer.intra_word_tokenize", "pretrained_transformer_mismatched_indexer.PretrainedTransformerMismatchedIndexer._matched_indexer._postprocess_output", "len", "len"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.token_indexers.pretrained_transformer_indexer.PretrainedTransformerIndexer._add_encoding_to_vocabulary_if_needed", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.tokenizers.pretrained_transformer_tokenizer.PretrainedTransformerTokenizer.intra_word_tokenize", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.token_indexers.pretrained_transformer_indexer.PretrainedTransformerIndexer._postprocess_output"], ["", "@", "overrides", "\n", "def", "tokens_to_indices", "(", "\n", "self", ",", "tokens", ":", "List", "[", "Token", "]", ",", "vocabulary", ":", "Vocabulary", "\n", ")", "->", "IndexedTokenList", ":", "\n", "        ", "self", ".", "_matched_indexer", ".", "_add_encoding_to_vocabulary_if_needed", "(", "vocabulary", ")", "\n", "\n", "wordpieces", ",", "offsets", "=", "self", ".", "_allennlp_tokenizer", ".", "intra_word_tokenize", "(", "\n", "[", "t", ".", "text", "for", "t", "in", "tokens", "]", "\n", ")", "\n", "output", ":", "IndexedTokenList", "=", "{", "\n", "\"token_ids\"", ":", "[", "t", ".", "text_id", "for", "t", "in", "wordpieces", "]", ",", "\n", "\"mask\"", ":", "[", "1", "]", "*", "len", "(", "tokens", ")", ",", "# for original tokens (i.e. word-level)", "\n", "\"type_ids\"", ":", "[", "t", ".", "type_id", "for", "t", "in", "wordpieces", "]", ",", "\n", "\"offsets\"", ":", "offsets", ",", "\n", "\"wordpiece_mask\"", ":", "[", "1", "]", "\n", "*", "len", "(", "wordpieces", ")", ",", "# for wordpieces (i.e. subword-level)", "\n", "}", "\n", "\n", "return", "self", ".", "_matched_indexer", ".", "_postprocess_output", "(", "output", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.token_indexers.pretrained_transformer_mismatched_indexer.PretrainedTransformerMismatchedIndexer.get_empty_token_list": [[79, 85], ["pretrained_transformer_mismatched_indexer.PretrainedTransformerMismatchedIndexer._matched_indexer.get_empty_token_list"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.token_indexers.token_characters_indexer.TokenCharactersIndexer.get_empty_token_list"], ["", "@", "overrides", "\n", "def", "get_empty_token_list", "(", "self", ")", "->", "IndexedTokenList", ":", "\n", "        ", "output", "=", "self", ".", "_matched_indexer", ".", "get_empty_token_list", "(", ")", "\n", "output", "[", "\"offsets\"", "]", "=", "[", "]", "\n", "output", "[", "\"wordpiece_mask\"", "]", "=", "[", "]", "\n", "return", "output", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.token_indexers.pretrained_transformer_mismatched_indexer.PretrainedTransformerMismatchedIndexer.as_padded_tensor_dict": [[86, 105], ["tokens.copy.copy.copy", "padding_lengths.copy.copy.copy", "tokens.copy.copy.pop", "padding_lengths.copy.copy.pop", "pretrained_transformer_mismatched_indexer.PretrainedTransformerMismatchedIndexer._matched_indexer.as_padded_tensor_dict", "torch.LongTensor", "allennlp.common.util.pad_sequence_to_length"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.params.Params.pop", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.params.Params.pop", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.token_indexers.token_characters_indexer.TokenCharactersIndexer.as_padded_tensor_dict", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.util.pad_sequence_to_length"], ["", "@", "overrides", "\n", "def", "as_padded_tensor_dict", "(", "\n", "self", ",", "tokens", ":", "IndexedTokenList", ",", "padding_lengths", ":", "Dict", "[", "str", ",", "int", "]", "\n", ")", "->", "Dict", "[", "str", ",", "torch", ".", "Tensor", "]", ":", "\n", "        ", "tokens", "=", "tokens", ".", "copy", "(", ")", "\n", "padding_lengths", "=", "padding_lengths", ".", "copy", "(", ")", "\n", "\n", "offsets_tokens", "=", "tokens", ".", "pop", "(", "\"offsets\"", ")", "\n", "offsets_padding_lengths", "=", "padding_lengths", ".", "pop", "(", "\"offsets\"", ")", "\n", "\n", "tensor_dict", "=", "self", ".", "_matched_indexer", ".", "as_padded_tensor_dict", "(", "\n", "tokens", ",", "padding_lengths", "\n", ")", "\n", "tensor_dict", "[", "\"offsets\"", "]", "=", "torch", ".", "LongTensor", "(", "\n", "pad_sequence_to_length", "(", "\n", "offsets_tokens", ",", "offsets_padding_lengths", ",", "default_value", "=", "lambda", ":", "(", "0", ",", "0", ")", "\n", ")", "\n", ")", "\n", "return", "tensor_dict", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.token_indexers.pretrained_transformer_mismatched_indexer.PretrainedTransformerMismatchedIndexer.__eq__": [[106, 117], ["isinstance"], "methods", ["None"], ["", "def", "__eq__", "(", "self", ",", "other", ")", ":", "\n", "        ", "if", "isinstance", "(", "other", ",", "PretrainedTransformerMismatchedIndexer", ")", ":", "\n", "            ", "for", "key", "in", "self", ".", "__dict__", ":", "\n", "                ", "if", "key", "==", "\"_tokenizer\"", ":", "\n", "# This is a reference to a function in the huggingface code, which we can't", "\n", "# really modify to make this clean.  So we special-case it.", "\n", "                    ", "continue", "\n", "", "if", "self", ".", "__dict__", "[", "key", "]", "!=", "other", ".", "__dict__", "[", "key", "]", ":", "\n", "                    ", "return", "False", "\n", "", "", "return", "True", "\n", "", "return", "NotImplemented", "\n", "", "", ""]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.token_indexers.ner_tag_indexer.NerTagIndexer.__init__": [[27, 32], ["allennlp.data.token_indexers.token_indexer.TokenIndexer.__init__"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.graph_util.plot.GraphPlot.__init__"], ["def", "__init__", "(", "\n", "self", ",", "namespace", ":", "str", "=", "\"ner_tokens\"", ",", "token_min_padding_length", ":", "int", "=", "0", "\n", ")", "->", "None", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "token_min_padding_length", ")", "\n", "self", ".", "_namespace", "=", "namespace", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.token_indexers.ner_tag_indexer.NerTagIndexer.count_vocab_items": [[33, 39], ["None"], "methods", ["None"], ["", "@", "overrides", "\n", "def", "count_vocab_items", "(", "self", ",", "token", ":", "Token", ",", "counter", ":", "Dict", "[", "str", ",", "Dict", "[", "str", ",", "int", "]", "]", ")", ":", "\n", "        ", "tag", "=", "token", ".", "ent_type_", "\n", "if", "not", "tag", ":", "\n", "            ", "tag", "=", "\"NONE\"", "\n", "", "counter", "[", "self", ".", "_namespace", "]", "[", "tag", "]", "+=", "1", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.token_indexers.ner_tag_indexer.NerTagIndexer.tokens_to_indices": [[40, 48], ["vocabulary.get_token_index"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.data.vocabulary.Vocabulary.get_token_index"], ["", "@", "overrides", "\n", "def", "tokens_to_indices", "(", "\n", "self", ",", "tokens", ":", "List", "[", "Token", "]", ",", "vocabulary", ":", "Vocabulary", "\n", ")", "->", "Dict", "[", "str", ",", "List", "[", "int", "]", "]", ":", "\n", "        ", "tags", "=", "[", "\"NONE\"", "if", "not", "token", ".", "ent_type_", "else", "token", ".", "ent_type_", "for", "token", "in", "tokens", "]", "\n", "\n", "return", "{", "\n", "\"tokens\"", ":", "[", "vocabulary", ".", "get_token_index", "(", "tag", ",", "self", ".", "_namespace", ")", "for", "tag", "in", "tags", "]", "\n", "}", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.token_indexers.ner_tag_indexer.NerTagIndexer.get_empty_token_list": [[50, 53], ["None"], "methods", ["None"], ["", "@", "overrides", "\n", "def", "get_empty_token_list", "(", "self", ")", "->", "IndexedTokenList", ":", "\n", "        ", "return", "{", "\"tokens\"", ":", "[", "]", "}", "\n", "", "", ""]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.token_indexers.spacy_indexer.SpacyTokenIndexer.__init__": [[31, 34], ["allennlp.data.token_indexers.token_indexer.TokenIndexer.__init__"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.graph_util.plot.GraphPlot.__init__"], ["def", "__init__", "(", "self", ",", "hidden_dim", ":", "int", "=", "96", ",", "token_min_padding_length", ":", "int", "=", "0", ")", "->", "None", ":", "\n", "        ", "self", ".", "_hidden_dim", "=", "hidden_dim", "\n", "super", "(", ")", ".", "__init__", "(", "token_min_padding_length", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.token_indexers.spacy_indexer.SpacyTokenIndexer.count_vocab_items": [[35, 41], ["None"], "methods", ["None"], ["", "@", "overrides", "\n", "def", "count_vocab_items", "(", "self", ",", "token", ":", "Token", ",", "counter", ":", "Dict", "[", "str", ",", "Dict", "[", "str", ",", "int", "]", "]", ")", ":", "\n", "# We are using spacy to generate embeddings directly for our model,", "\n", "# so we don't need to capture the vocab - it is defined by the spacy", "\n", "# model we are using instead.", "\n", "        ", "pass", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.token_indexers.spacy_indexer.SpacyTokenIndexer.tokens_to_indices": [[42, 56], ["all", "ValueError", "indices.append", "isinstance"], "methods", ["None"], ["", "@", "overrides", "\n", "def", "tokens_to_indices", "(", "\n", "self", ",", "tokens", ":", "List", "[", "SpacyToken", "]", ",", "vocabulary", ":", "Vocabulary", "\n", ")", "->", "Dict", "[", "str", ",", "List", "[", "numpy", ".", "ndarray", "]", "]", ":", "\n", "\n", "        ", "if", "not", "all", "(", "[", "isinstance", "(", "x", ",", "SpacyToken", ")", "for", "x", "in", "tokens", "]", ")", ":", "\n", "            ", "raise", "ValueError", "(", "\n", "\"The spacy indexer requires you to use a Tokenizer which produces SpacyTokens.\"", "\n", ")", "\n", "", "indices", ":", "List", "[", "numpy", ".", "ndarray", "]", "=", "[", "]", "\n", "for", "token", "in", "tokens", ":", "\n", "            ", "indices", ".", "append", "(", "token", ".", "vector", ")", "\n", "\n", "", "return", "{", "\"tokens\"", ":", "indices", "}", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.token_indexers.spacy_indexer.SpacyTokenIndexer.as_padded_tensor_dict": [[57, 70], ["torch.FloatTensor", "numpy.zeros", "allennlp.common.util.pad_sequence_to_length"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.util.pad_sequence_to_length"], ["", "@", "overrides", "\n", "def", "as_padded_tensor_dict", "(", "\n", "self", ",", "tokens", ":", "IndexedTokenList", ",", "padding_lengths", ":", "Dict", "[", "str", ",", "int", "]", "\n", ")", "->", "Dict", "[", "str", ",", "torch", ".", "Tensor", "]", ":", "\n", "        ", "def", "padding_token", "(", ")", ":", "\n", "            ", "return", "numpy", ".", "zeros", "(", "self", ".", "_hidden_dim", ",", "dtype", "=", "numpy", ".", "float32", ")", "\n", "\n", "", "tensor", "=", "torch", ".", "FloatTensor", "(", "\n", "pad_sequence_to_length", "(", "\n", "tokens", "[", "\"tokens\"", "]", ",", "padding_lengths", "[", "\"tokens\"", "]", ",", "default_value", "=", "padding_token", "\n", ")", "\n", ")", "\n", "return", "{", "\"tokens\"", ":", "tensor", "}", "\n", "", "", ""]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.token_indexers.pretrained_transformer_indexer.PretrainedTransformerIndexer.__init__": [[41, 61], ["allennlp.data.token_indexers.token_indexer.TokenIndexer.__init__", "allennlp.data.tokenizers.PretrainedTransformerTokenizer", "pretrained_transformer_indexer.PretrainedTransformerIndexer._tokenizer.num_added_tokens", "ValueError"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.graph_util.plot.GraphPlot.__init__"], ["def", "__init__", "(", "\n", "self", ",", "model_name", ":", "str", ",", "namespace", ":", "str", "=", "\"tags\"", ",", "max_length", ":", "int", "=", "None", ",", "**", "kwargs", "\n", ")", "->", "None", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "**", "kwargs", ")", "\n", "self", ".", "_namespace", "=", "namespace", "\n", "self", ".", "_allennlp_tokenizer", "=", "PretrainedTransformerTokenizer", "(", "model_name", ")", "\n", "self", ".", "_tokenizer", "=", "self", ".", "_allennlp_tokenizer", ".", "tokenizer", "\n", "self", ".", "_added_to_vocabulary", "=", "False", "\n", "\n", "self", ".", "_num_added_start_tokens", "=", "self", ".", "_allennlp_tokenizer", ".", "num_added_start_tokens", "\n", "self", ".", "_num_added_end_tokens", "=", "self", ".", "_allennlp_tokenizer", ".", "num_added_end_tokens", "\n", "\n", "self", ".", "_max_length", "=", "max_length", "\n", "if", "self", ".", "_max_length", "is", "not", "None", ":", "\n", "            ", "self", ".", "_effective_max_length", "=", "(", "# we need to take into account special tokens", "\n", "self", ".", "_max_length", "-", "self", ".", "_tokenizer", ".", "num_added_tokens", "(", ")", "\n", ")", "\n", "if", "self", ".", "_effective_max_length", "<=", "0", ":", "\n", "                ", "raise", "ValueError", "(", "\n", "\"max_length needs to be greater than the number of special tokens inserted.\"", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.token_indexers.pretrained_transformer_indexer.PretrainedTransformerIndexer._add_encoding_to_vocabulary_if_needed": [[63, 89], ["hasattr", "hasattr", "getattr", "getattr.items", "logger.warning"], "methods", ["None"], ["", "", "", "def", "_add_encoding_to_vocabulary_if_needed", "(", "self", ",", "vocab", ":", "Vocabulary", ")", "->", "None", ":", "\n", "        ", "\"\"\"\n        Copies tokens from ```transformers``` model to the specified namespace.\n        Transformers vocab is taken from the <vocab>/<encoder> keys of the tokenizer object.\n        \"\"\"", "\n", "if", "self", ".", "_added_to_vocabulary", ":", "\n", "            ", "return", "\n", "\n", "", "vocab_field_name", "=", "None", "\n", "if", "hasattr", "(", "self", ".", "_tokenizer", ",", "\"vocab\"", ")", ":", "\n", "            ", "vocab_field_name", "=", "\"vocab\"", "\n", "", "elif", "hasattr", "(", "self", ".", "_tokenizer", ",", "\"encoder\"", ")", ":", "\n", "            ", "vocab_field_name", "=", "\"encoder\"", "\n", "", "else", ":", "\n", "            ", "logger", ".", "warning", "(", "\n", "\"\"\"Wasn't able to fetch vocabulary from pretrained transformers lib.\n                Neither <vocab> nor <encoder> are the valid fields for vocab.\n                Your tokens will still be correctly indexed, but vocabulary file will not be saved.\"\"\"", "\n", ")", "\n", "", "if", "vocab_field_name", "is", "not", "None", ":", "\n", "            ", "pretrained_vocab", "=", "getattr", "(", "self", ".", "_tokenizer", ",", "vocab_field_name", ")", "\n", "for", "word", ",", "idx", "in", "pretrained_vocab", ".", "items", "(", ")", ":", "\n", "                ", "vocab", ".", "_token_to_index", "[", "self", ".", "_namespace", "]", "[", "word", "]", "=", "idx", "\n", "vocab", ".", "_index_to_token", "[", "self", ".", "_namespace", "]", "[", "idx", "]", "=", "word", "\n", "\n", "", "", "self", ".", "_added_to_vocabulary", "=", "True", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.token_indexers.pretrained_transformer_indexer.PretrainedTransformerIndexer.count_vocab_items": [[90, 94], ["None"], "methods", ["None"], ["", "@", "overrides", "\n", "def", "count_vocab_items", "(", "self", ",", "token", ":", "Token", ",", "counter", ":", "Dict", "[", "str", ",", "Dict", "[", "str", ",", "int", "]", "]", ")", ":", "\n", "# If we only use pretrained models, we don't need to do anything here.", "\n", "        ", "pass", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.token_indexers.pretrained_transformer_indexer.PretrainedTransformerIndexer.tokens_to_indices": [[95, 108], ["pretrained_transformer_indexer.PretrainedTransformerIndexer._add_encoding_to_vocabulary_if_needed", "pretrained_transformer_indexer.PretrainedTransformerIndexer._extract_token_and_type_ids", "pretrained_transformer_indexer.PretrainedTransformerIndexer._postprocess_output", "len"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.token_indexers.pretrained_transformer_indexer.PretrainedTransformerIndexer._add_encoding_to_vocabulary_if_needed", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.token_indexers.pretrained_transformer_indexer.PretrainedTransformerIndexer._extract_token_and_type_ids", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.token_indexers.pretrained_transformer_indexer.PretrainedTransformerIndexer._postprocess_output"], ["", "@", "overrides", "\n", "def", "tokens_to_indices", "(", "\n", "self", ",", "tokens", ":", "List", "[", "Token", "]", ",", "vocabulary", ":", "Vocabulary", "\n", ")", "->", "IndexedTokenList", ":", "\n", "        ", "self", ".", "_add_encoding_to_vocabulary_if_needed", "(", "vocabulary", ")", "\n", "\n", "indices", ",", "type_ids", "=", "self", ".", "_extract_token_and_type_ids", "(", "tokens", ")", "\n", "# The mask has 1 for real tokens and 0 for padding tokens. Only real tokens are attended to.", "\n", "output", "=", "{", "\"token_ids\"", ":", "indices", ",", "\"mask\"", ":", "[", "1", "]", "*", "len", "(", "indices", ")", "}", "\n", "if", "type_ids", "is", "not", "None", ":", "\n", "            ", "output", "[", "\"type_ids\"", "]", "=", "type_ids", "\n", "\n", "", "return", "self", ".", "_postprocess_output", "(", "output", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.token_indexers.pretrained_transformer_indexer.PretrainedTransformerIndexer._extract_token_and_type_ids": [[109, 136], ["getattr", "indices.append", "KeyError", "type_ids.append", "getattr"], "methods", ["None"], ["", "def", "_extract_token_and_type_ids", "(", "\n", "self", ",", "tokens", ":", "List", "[", "Token", "]", "\n", ")", "->", "Tuple", "[", "List", "[", "int", "]", ",", "Optional", "[", "List", "[", "int", "]", "]", "]", ":", "\n", "        ", "\"\"\"\n        Roughly equivalent to `zip(*[(token.text_id, token.type_id) for token in tokens])`,\n        with some checks.\n        \"\"\"", "\n", "indices", ":", "List", "[", "int", "]", "=", "[", "]", "\n", "type_ids", ":", "List", "[", "int", "]", "=", "[", "]", "\n", "for", "token", "in", "tokens", ":", "\n", "            ", "if", "getattr", "(", "token", ",", "\"text_id\"", ",", "None", ")", "is", "not", "None", ":", "\n", "# `text_id` being set on the token means that we aren't using the vocab, we just use", "\n", "# this id instead. Id comes from the pretrained vocab.", "\n", "# It is computed in PretrainedTransformerTokenizer.", "\n", "                ", "indices", ".", "append", "(", "token", ".", "text_id", ")", "\n", "", "else", ":", "\n", "                ", "raise", "KeyError", "(", "\n", "\"Using PretrainedTransformerIndexer but field text_id is not set\"", "\n", "f\" for the following token: {token.text}\"", "\n", ")", "\n", "\n", "", "if", "type_ids", "is", "not", "None", "and", "getattr", "(", "token", ",", "\"type_id\"", ",", "None", ")", "is", "not", "None", ":", "\n", "                ", "type_ids", ".", "append", "(", "token", ".", "type_id", ")", "\n", "", "else", ":", "\n", "                ", "type_ids", "=", "None", "\n", "\n", "", "", "return", "indices", ",", "type_ids", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.token_indexers.pretrained_transformer_indexer.PretrainedTransformerIndexer._postprocess_output": [[137, 178], ["pretrained_transformer_indexer.PretrainedTransformerIndexer._tokenizer.create_token_type_ids_from_sequences", "pretrained_transformer_indexer.PretrainedTransformerIndexer._tokenizer.build_inputs_with_special_tokens", "len", "range", "len"], "methods", ["None"], ["", "def", "_postprocess_output", "(", "self", ",", "output", ":", "IndexedTokenList", ")", "->", "IndexedTokenList", ":", "\n", "        ", "\"\"\"\n        Takes an IndexedTokenList about to be returned by `tokens_to_indices()` and adds any\n        necessary postprocessing, e.g. long sequence splitting.\n\n        The input should have a `\"token_ids\"` key corresponding to the token indices. They should\n        have special tokens already inserted.\n        \"\"\"", "\n", "if", "self", ".", "_max_length", "is", "not", "None", ":", "\n", "# We prepare long indices by converting them to (assuming max_length == 5)", "\n", "# [CLS] A B C [SEP] [CLS] D E F [SEP] ...", "\n", "# Embedder is responsible for folding this 1-d sequence to 2-d and feed to the", "\n", "# transformer model.", "\n", "# TODO(zhaofengw): we aren't respecting word boundaries when segmenting wordpieces.", "\n", "\n", "            ", "indices", "=", "output", "[", "\"token_ids\"", "]", "\n", "# Strips original special tokens", "\n", "indices", "=", "indices", "[", "\n", "self", ".", "_num_added_start_tokens", ":", "-", "self", ".", "_num_added_end_tokens", "\n", "]", "\n", "# Folds indices", "\n", "folded_indices", "=", "[", "\n", "indices", "[", "i", ":", "i", "+", "self", ".", "_effective_max_length", "]", "\n", "for", "i", "in", "range", "(", "0", ",", "len", "(", "indices", ")", ",", "self", ".", "_effective_max_length", ")", "\n", "]", "\n", "# Adds special tokens to each segment", "\n", "folded_indices", "=", "[", "\n", "self", ".", "_tokenizer", ".", "build_inputs_with_special_tokens", "(", "segment", ")", "\n", "for", "segment", "in", "folded_indices", "\n", "]", "\n", "# Flattens", "\n", "indices", "=", "[", "i", "for", "segment", "in", "folded_indices", "for", "i", "in", "segment", "]", "\n", "\n", "output", "[", "\"token_ids\"", "]", "=", "indices", "\n", "# `create_token_type_ids_from_sequences()` inserts special tokens", "\n", "output", "[", "\"type_ids\"", "]", "=", "self", ".", "_tokenizer", ".", "create_token_type_ids_from_sequences", "(", "\n", "indices", "[", "self", ".", "_num_added_start_tokens", ":", "-", "self", ".", "_num_added_end_tokens", "]", "\n", ")", "\n", "output", "[", "\"segment_concat_mask\"", "]", "=", "[", "1", "]", "*", "len", "(", "indices", ")", "\n", "\n", "", "return", "output", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.token_indexers.pretrained_transformer_indexer.PretrainedTransformerIndexer.get_empty_token_list": [[179, 185], ["None"], "methods", ["None"], ["", "@", "overrides", "\n", "def", "get_empty_token_list", "(", "self", ")", "->", "IndexedTokenList", ":", "\n", "        ", "output", ":", "IndexedTokenList", "=", "{", "\"token_ids\"", ":", "[", "]", ",", "\"mask\"", ":", "[", "]", ",", "\"type_ids\"", ":", "[", "]", "}", "\n", "if", "self", ".", "_max_length", "is", "not", "None", ":", "\n", "            ", "output", "[", "\"segment_concat_mask\"", "]", "=", "[", "]", "\n", "", "return", "output", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.token_indexers.pretrained_transformer_indexer.PretrainedTransformerIndexer.as_padded_tensor_dict": [[186, 203], ["torch.LongTensor", "allennlp.common.util.pad_sequence_to_length", "tokens.items"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.util.pad_sequence_to_length"], ["", "@", "overrides", "\n", "def", "as_padded_tensor_dict", "(", "\n", "self", ",", "tokens", ":", "IndexedTokenList", ",", "padding_lengths", ":", "Dict", "[", "str", ",", "int", "]", ",", "\n", ")", "->", "Dict", "[", "str", ",", "torch", ".", "Tensor", "]", ":", "\n", "# Different transformers use different padding values for tokens, but for mask and type id, the padding", "\n", "# value is always 0.", "\n", "        ", "return", "{", "\n", "key", ":", "torch", ".", "LongTensor", "(", "\n", "pad_sequence_to_length", "(", "\n", "val", ",", "\n", "padding_lengths", "[", "key", "]", ",", "\n", "default_value", "=", "lambda", ":", "0", "\n", "if", "key", "in", "{", "\"mask\"", ",", "\"type_ids\"", "}", "\n", "else", "self", ".", "_tokenizer", ".", "pad_token_id", ",", "\n", ")", "\n", ")", "\n", "for", "key", ",", "val", "in", "tokens", ".", "items", "(", ")", "\n", "}", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.token_indexers.pretrained_transformer_indexer.PretrainedTransformerIndexer.__eq__": [[205, 216], ["isinstance"], "methods", ["None"], ["", "def", "__eq__", "(", "self", ",", "other", ")", ":", "\n", "        ", "if", "isinstance", "(", "other", ",", "PretrainedTransformerIndexer", ")", ":", "\n", "            ", "for", "key", "in", "self", ".", "__dict__", ":", "\n", "                ", "if", "key", "==", "\"_tokenizer\"", ":", "\n", "# This is a reference to a function in the huggingface code, which we can't", "\n", "# really modify to make this clean.  So we special-case it.", "\n", "                    ", "continue", "\n", "", "if", "self", ".", "__dict__", "[", "key", "]", "!=", "other", ".", "__dict__", "[", "key", "]", ":", "\n", "                    ", "return", "False", "\n", "", "", "return", "True", "\n", "", "return", "NotImplemented", "\n", "", "", ""]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.token_indexers.single_id_token_indexer.SingleIdTokenIndexer.__init__": [[31, 45], ["allennlp.data.token_indexers.token_indexer.TokenIndexer.__init__", "allennlp.data.tokenizers.token.Token", "allennlp.data.tokenizers.token.Token"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.graph_util.plot.GraphPlot.__init__"], ["def", "__init__", "(", "\n", "self", ",", "\n", "namespace", ":", "str", "=", "\"tokens\"", ",", "\n", "lowercase_tokens", ":", "bool", "=", "False", ",", "\n", "start_tokens", ":", "List", "[", "str", "]", "=", "None", ",", "\n", "end_tokens", ":", "List", "[", "str", "]", "=", "None", ",", "\n", "token_min_padding_length", ":", "int", "=", "0", ",", "\n", ")", "->", "None", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "token_min_padding_length", ")", "\n", "self", ".", "namespace", "=", "namespace", "\n", "self", ".", "lowercase_tokens", "=", "lowercase_tokens", "\n", "\n", "self", ".", "_start_tokens", "=", "[", "Token", "(", "st", ")", "for", "st", "in", "(", "start_tokens", "or", "[", "]", ")", "]", "\n", "self", ".", "_end_tokens", "=", "[", "Token", "(", "et", ")", "for", "et", "in", "(", "end_tokens", "or", "[", "]", ")", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.token_indexers.single_id_token_indexer.SingleIdTokenIndexer.count_vocab_items": [[46, 55], ["getattr", "text.lower.lower.lower"], "methods", ["None"], ["", "@", "overrides", "\n", "def", "count_vocab_items", "(", "self", ",", "token", ":", "Token", ",", "counter", ":", "Dict", "[", "str", ",", "Dict", "[", "str", ",", "int", "]", "]", ")", ":", "\n", "# If `text_id` is set on the token (e.g., if we're using some kind of hash-based word", "\n", "# encoding), we will not be using the vocab for this token.", "\n", "        ", "if", "getattr", "(", "token", ",", "\"text_id\"", ",", "None", ")", "is", "None", ":", "\n", "            ", "text", "=", "token", ".", "text", "\n", "if", "self", ".", "lowercase_tokens", ":", "\n", "                ", "text", "=", "text", ".", "lower", "(", ")", "\n", "", "counter", "[", "self", ".", "namespace", "]", "[", "text", "]", "+=", "1", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.token_indexers.single_id_token_indexer.SingleIdTokenIndexer.tokens_to_indices": [[56, 74], ["itertools.chain", "getattr", "indices.append", "indices.append", "text.lower.lower.lower", "vocabulary.get_token_index"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.data.vocabulary.Vocabulary.get_token_index"], ["", "", "@", "overrides", "\n", "def", "tokens_to_indices", "(", "\n", "self", ",", "tokens", ":", "List", "[", "Token", "]", ",", "vocabulary", ":", "Vocabulary", "\n", ")", "->", "Dict", "[", "str", ",", "List", "[", "int", "]", "]", ":", "\n", "        ", "indices", ":", "List", "[", "int", "]", "=", "[", "]", "\n", "\n", "for", "token", "in", "itertools", ".", "chain", "(", "self", ".", "_start_tokens", ",", "tokens", ",", "self", ".", "_end_tokens", ")", ":", "\n", "            ", "if", "getattr", "(", "token", ",", "\"text_id\"", ",", "None", ")", "is", "not", "None", ":", "\n", "# `text_id` being set on the token means that we aren't using the vocab, we just use", "\n", "# this id instead.", "\n", "                ", "indices", ".", "append", "(", "token", ".", "text_id", ")", "\n", "", "else", ":", "\n", "                ", "text", "=", "token", ".", "text", "\n", "if", "self", ".", "lowercase_tokens", ":", "\n", "                    ", "text", "=", "text", ".", "lower", "(", ")", "\n", "", "indices", ".", "append", "(", "vocabulary", ".", "get_token_index", "(", "text", ",", "self", ".", "namespace", ")", ")", "\n", "\n", "", "", "return", "{", "\"tokens\"", ":", "indices", "}", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.token_indexers.single_id_token_indexer.SingleIdTokenIndexer.get_empty_token_list": [[75, 78], ["None"], "methods", ["None"], ["", "@", "overrides", "\n", "def", "get_empty_token_list", "(", "self", ")", "->", "IndexedTokenList", ":", "\n", "        ", "return", "{", "\"tokens\"", ":", "[", "]", "}", "\n", "", "", ""]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.token_indexers.elmo_indexer.ELMoCharacterMapper.__init__": [[65, 67], ["None"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "tokens_to_add", ":", "Dict", "[", "str", ",", "int", "]", "=", "None", ")", "->", "None", ":", "\n", "        ", "self", ".", "tokens_to_add", "=", "tokens_to_add", "or", "{", "}", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.token_indexers.elmo_indexer.ELMoCharacterMapper.convert_word_to_char_ids": [[68, 94], ["enumerate", "word.encode", "len"], "methods", ["None"], ["", "def", "convert_word_to_char_ids", "(", "self", ",", "word", ":", "str", ")", "->", "List", "[", "int", "]", ":", "\n", "        ", "if", "word", "in", "self", ".", "tokens_to_add", ":", "\n", "            ", "char_ids", "=", "[", "\n", "ELMoCharacterMapper", ".", "padding_character", "\n", "]", "*", "ELMoCharacterMapper", ".", "max_word_length", "\n", "char_ids", "[", "0", "]", "=", "ELMoCharacterMapper", ".", "beginning_of_word_character", "\n", "char_ids", "[", "1", "]", "=", "self", ".", "tokens_to_add", "[", "word", "]", "\n", "char_ids", "[", "2", "]", "=", "ELMoCharacterMapper", ".", "end_of_word_character", "\n", "", "elif", "word", "==", "ELMoCharacterMapper", ".", "bos_token", ":", "\n", "            ", "char_ids", "=", "ELMoCharacterMapper", ".", "beginning_of_sentence_characters", "\n", "", "elif", "word", "==", "ELMoCharacterMapper", ".", "eos_token", ":", "\n", "            ", "char_ids", "=", "ELMoCharacterMapper", ".", "end_of_sentence_characters", "\n", "", "else", ":", "\n", "            ", "word_encoded", "=", "word", ".", "encode", "(", "\"utf-8\"", ",", "\"ignore\"", ")", "[", "\n", ":", "(", "ELMoCharacterMapper", ".", "max_word_length", "-", "2", ")", "\n", "]", "\n", "char_ids", "=", "[", "\n", "ELMoCharacterMapper", ".", "padding_character", "\n", "]", "*", "ELMoCharacterMapper", ".", "max_word_length", "\n", "char_ids", "[", "0", "]", "=", "ELMoCharacterMapper", ".", "beginning_of_word_character", "\n", "for", "k", ",", "chr_id", "in", "enumerate", "(", "word_encoded", ",", "start", "=", "1", ")", ":", "\n", "                ", "char_ids", "[", "k", "]", "=", "chr_id", "\n", "", "char_ids", "[", "len", "(", "word_encoded", ")", "+", "1", "]", "=", "ELMoCharacterMapper", ".", "end_of_word_character", "\n", "\n", "# +1 one for masking", "\n", "", "return", "[", "c", "+", "1", "for", "c", "in", "char_ids", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.token_indexers.elmo_indexer.ELMoCharacterMapper.__eq__": [[95, 99], ["isinstance"], "methods", ["None"], ["", "def", "__eq__", "(", "self", ",", "other", ")", "->", "bool", ":", "\n", "        ", "if", "isinstance", "(", "self", ",", "other", ".", "__class__", ")", ":", "\n", "            ", "return", "self", ".", "__dict__", "==", "other", ".", "__dict__", "\n", "", "return", "NotImplemented", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.token_indexers.elmo_indexer.ELMoTokenCharactersIndexer.__init__": [[117, 126], ["allennlp.data.token_indexers.token_indexer.TokenIndexer.__init__", "elmo_indexer.ELMoCharacterMapper"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.graph_util.plot.GraphPlot.__init__"], ["def", "__init__", "(", "\n", "self", ",", "\n", "namespace", ":", "str", "=", "\"elmo_characters\"", ",", "\n", "tokens_to_add", ":", "Dict", "[", "str", ",", "int", "]", "=", "None", ",", "\n", "token_min_padding_length", ":", "int", "=", "0", ",", "\n", ")", "->", "None", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "token_min_padding_length", ")", "\n", "self", ".", "_namespace", "=", "namespace", "\n", "self", ".", "_mapper", "=", "ELMoCharacterMapper", "(", "tokens_to_add", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.token_indexers.elmo_indexer.ELMoTokenCharactersIndexer.count_vocab_items": [[127, 130], ["None"], "methods", ["None"], ["", "@", "overrides", "\n", "def", "count_vocab_items", "(", "self", ",", "token", ":", "Token", ",", "counter", ":", "Dict", "[", "str", ",", "Dict", "[", "str", ",", "int", "]", "]", ")", ":", "\n", "        ", "pass", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.token_indexers.elmo_indexer.ELMoTokenCharactersIndexer.tokens_to_indices": [[131, 147], ["any", "allennlp.common.checks.ConfigurationError", "elmo_indexer.ELMoTokenCharactersIndexer._mapper.convert_word_to_char_ids"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.token_indexers.elmo_indexer.ELMoCharacterMapper.convert_word_to_char_ids"], ["", "@", "overrides", "\n", "def", "tokens_to_indices", "(", "\n", "self", ",", "tokens", ":", "List", "[", "Token", "]", ",", "vocabulary", ":", "Vocabulary", "\n", ")", "->", "Dict", "[", "str", ",", "List", "[", "List", "[", "int", "]", "]", "]", ":", "\n", "# TODO(brendanr): Retain the token to index mappings in the vocabulary and remove this", "\n", "\n", "# https://github.com/allenai/allennlp/blob/master/allennlp/data/token_indexers/wordpiece_indexer.py#L113", "\n", "\n", "        ", "texts", "=", "[", "token", ".", "text", "for", "token", "in", "tokens", "]", "\n", "\n", "if", "any", "(", "text", "is", "None", "for", "text", "in", "texts", ")", ":", "\n", "            ", "raise", "ConfigurationError", "(", "\n", "\"ELMoTokenCharactersIndexer needs a tokenizer that retains text\"", "\n", ")", "\n", "", "return", "{", "\n", "\"tokens\"", ":", "[", "self", ".", "_mapper", ".", "convert_word_to_char_ids", "(", "text", ")", "for", "text", "in", "texts", "]", "\n", "}", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.token_indexers.elmo_indexer.ELMoTokenCharactersIndexer.as_padded_tensor_dict": [[149, 165], ["torch.LongTensor", "allennlp.common.util.pad_sequence_to_length"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.util.pad_sequence_to_length"], ["", "@", "overrides", "\n", "def", "as_padded_tensor_dict", "(", "\n", "self", ",", "tokens", ":", "IndexedTokenList", ",", "padding_lengths", ":", "Dict", "[", "str", ",", "int", "]", "\n", ")", "->", "Dict", "[", "str", ",", "torch", ".", "Tensor", "]", ":", "\n", "# Overriding this method only because we need a different padding token than the default.", "\n", "        ", "tensor_dict", "=", "{", "}", "\n", "\n", "def", "padding_token", "(", ")", ":", "\n", "            ", "return", "[", "0", "]", "*", "ELMoCharacterMapper", ".", "max_word_length", "\n", "\n", "", "tensor_dict", "[", "\"tokens\"", "]", "=", "torch", ".", "LongTensor", "(", "\n", "pad_sequence_to_length", "(", "\n", "tokens", "[", "\"tokens\"", "]", ",", "padding_lengths", "[", "\"tokens\"", "]", ",", "default_value", "=", "padding_token", "\n", ")", "\n", ")", "\n", "return", "tensor_dict", "\n", "", "", ""]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.token_indexers.elmo_indexer._make_bos_eos": [[13, 25], ["None"], "function", ["None"], ["def", "_make_bos_eos", "(", "\n", "character", ":", "int", ",", "\n", "padding_character", ":", "int", ",", "\n", "beginning_of_word_character", ":", "int", ",", "\n", "end_of_word_character", ":", "int", ",", "\n", "max_word_length", ":", "int", ",", "\n", ")", ":", "\n", "    ", "char_ids", "=", "[", "padding_character", "]", "*", "max_word_length", "\n", "char_ids", "[", "0", "]", "=", "beginning_of_word_character", "\n", "char_ids", "[", "1", "]", "=", "character", "\n", "char_ids", "[", "2", "]", "=", "end_of_word_character", "\n", "return", "char_ids", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.token_indexers.wordpiece_indexer.WordpieceIndexer.__init__": [[65, 118], ["allennlp.data.token_indexers.token_indexer.TokenIndexer.__init__", "set", "set", "wordpiece_tokenizer", "wordpiece_tokenizer", "wordpiece_tokenizer"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.graph_util.plot.GraphPlot.__init__"], ["def", "__init__", "(", "\n", "self", ",", "\n", "vocab", ":", "Dict", "[", "str", ",", "int", "]", ",", "\n", "wordpiece_tokenizer", ":", "Callable", "[", "[", "str", "]", ",", "List", "[", "str", "]", "]", ",", "\n", "namespace", ":", "str", "=", "\"wordpiece\"", ",", "\n", "use_starting_offsets", ":", "bool", "=", "False", ",", "\n", "max_pieces", ":", "int", "=", "512", ",", "\n", "do_lowercase", ":", "bool", "=", "False", ",", "\n", "never_lowercase", ":", "List", "[", "str", "]", "=", "None", ",", "\n", "start_tokens", ":", "List", "[", "str", "]", "=", "None", ",", "\n", "end_tokens", ":", "List", "[", "str", "]", "=", "None", ",", "\n", "separator_token", ":", "str", "=", "\"[SEP]\"", ",", "\n", "truncate_long_sequences", ":", "bool", "=", "True", ",", "\n", "token_min_padding_length", ":", "int", "=", "0", ",", "\n", ")", "->", "None", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "token_min_padding_length", ")", "\n", "self", ".", "vocab", "=", "vocab", "\n", "\n", "# The BERT code itself does a two-step tokenization:", "\n", "#    sentence -> [words], and then word -> [wordpieces]", "\n", "# In AllenNLP, the first step is implemented as the `BertBasicWordSplitter`,", "\n", "# and this token indexer handles the second.", "\n", "self", ".", "wordpiece_tokenizer", "=", "wordpiece_tokenizer", "\n", "\n", "self", ".", "_namespace", "=", "namespace", "\n", "self", ".", "_added_to_vocabulary", "=", "False", "\n", "self", ".", "max_pieces", "=", "max_pieces", "\n", "self", ".", "use_starting_offsets", "=", "use_starting_offsets", "\n", "self", ".", "_do_lowercase", "=", "do_lowercase", "\n", "self", ".", "_truncate_long_sequences", "=", "truncate_long_sequences", "\n", "self", ".", "_warned_about_truncation", "=", "False", "\n", "\n", "if", "never_lowercase", "is", "None", ":", "\n", "# Use the defaults", "\n", "            ", "self", ".", "_never_lowercase", "=", "set", "(", "_NEVER_LOWERCASE", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "_never_lowercase", "=", "set", "(", "never_lowercase", ")", "\n", "\n", "# Convert the start_tokens and end_tokens to wordpiece_ids", "\n", "", "self", ".", "_start_piece_ids", "=", "[", "\n", "vocab", "[", "wordpiece", "]", "\n", "for", "token", "in", "(", "start_tokens", "or", "[", "]", ")", "\n", "for", "wordpiece", "in", "wordpiece_tokenizer", "(", "token", ")", "\n", "]", "\n", "self", ".", "_end_piece_ids", "=", "[", "\n", "vocab", "[", "wordpiece", "]", "\n", "for", "token", "in", "(", "end_tokens", "or", "[", "]", ")", "\n", "for", "wordpiece", "in", "wordpiece_tokenizer", "(", "token", ")", "\n", "]", "\n", "\n", "# Convert the separator_token to wordpiece_ids", "\n", "self", ".", "_separator_ids", "=", "[", "\n", "vocab", "[", "wordpiece", "]", "for", "wordpiece", "in", "wordpiece_tokenizer", "(", "separator_token", ")", "\n", "]", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.token_indexers.wordpiece_indexer.WordpieceIndexer.count_vocab_items": [[120, 124], ["None"], "methods", ["None"], ["", "@", "overrides", "\n", "def", "count_vocab_items", "(", "self", ",", "token", ":", "Token", ",", "counter", ":", "Dict", "[", "str", ",", "Dict", "[", "str", ",", "int", "]", "]", ")", ":", "\n", "# If we only use pretrained models, we don't need to do anything here.", "\n", "        ", "pass", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.token_indexers.wordpiece_indexer.WordpieceIndexer._add_encoding_to_vocabulary": [[125, 130], ["wordpiece_indexer.WordpieceIndexer.vocab.items"], "methods", ["None"], ["", "def", "_add_encoding_to_vocabulary", "(", "self", ",", "vocabulary", ":", "Vocabulary", ")", "->", "None", ":", "\n", "\n", "        ", "for", "word", ",", "idx", "in", "self", ".", "vocab", ".", "items", "(", ")", ":", "\n", "            ", "vocabulary", ".", "_token_to_index", "[", "self", ".", "_namespace", "]", "[", "word", "]", "=", "idx", "\n", "vocabulary", ".", "_index_to_token", "[", "self", ".", "_namespace", "]", "[", "idx", "]", "=", "word", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.token_indexers.wordpiece_indexer.WordpieceIndexer._warn_about_truncation": [[131, 140], ["logger.warning", "str"], "methods", ["None"], ["", "", "def", "_warn_about_truncation", "(", "self", ",", "tokens", ":", "List", "[", "Token", "]", ")", "->", "None", ":", "\n", "        ", "if", "not", "self", ".", "_warned_about_truncation", ":", "\n", "            ", "logger", ".", "warning", "(", "\n", "\"Too many wordpieces, truncating sequence. \"", "\n", "\"If you would like a sliding window, set `truncate_long_sequences` to False.\"", "\n", "f\"The offending input was: {str([token.text for token in tokens])}.\"", "\n", "\"To avoid polluting your logs we will not warn about this again.\"", "\n", ")", "\n", "self", ".", "_warned_about_truncation", "=", "True", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.token_indexers.wordpiece_indexer.WordpieceIndexer.tokens_to_indices": [[141, 291], ["wordpiece_indexer._get_token_type_ids", "wordpiece_indexer.WordpieceIndexer._add_encoding_to_vocabulary", "len", "len", "len", "len", "wordpiece_indexer.WordpieceIndexer._extend", "token.text.lower", "len", "len", "offsets.append", "len", "len", "offsets.append", "wordpiece_indexer.WordpieceIndexer._add_start_and_end", "wordpiece_indexer.WordpieceIndexer._warn_about_truncation", "wordpiece_indexer.WordpieceIndexer._extend", "wordpiece_indexer.WordpieceIndexer.wordpiece_tokenizer", "wordpiece_indexer.WordpieceIndexer._add_start_and_end", "wordpiece_indexer.WordpieceIndexer._add_start_and_end", "wordpiece_indexer.WordpieceIndexer._extend", "range", "range", "len", "len", "len", "len"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.token_indexers.wordpiece_indexer._get_token_type_ids", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.token_indexers.wordpiece_indexer.WordpieceIndexer._add_encoding_to_vocabulary", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.token_indexers.wordpiece_indexer.WordpieceIndexer._extend", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.token_indexers.wordpiece_indexer.WordpieceIndexer._add_start_and_end", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.token_indexers.wordpiece_indexer.WordpieceIndexer._warn_about_truncation", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.token_indexers.wordpiece_indexer.WordpieceIndexer._extend", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.token_indexers.wordpiece_indexer.WordpieceIndexer._add_start_and_end", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.token_indexers.wordpiece_indexer.WordpieceIndexer._add_start_and_end", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.token_indexers.wordpiece_indexer.WordpieceIndexer._extend"], ["", "", "@", "overrides", "\n", "def", "tokens_to_indices", "(", "\n", "self", ",", "tokens", ":", "List", "[", "Token", "]", ",", "vocabulary", ":", "Vocabulary", "\n", ")", "->", "Dict", "[", "str", ",", "List", "[", "int", "]", "]", ":", "\n", "        ", "if", "not", "self", ".", "_added_to_vocabulary", ":", "\n", "            ", "self", ".", "_add_encoding_to_vocabulary", "(", "vocabulary", ")", "\n", "self", ".", "_added_to_vocabulary", "=", "True", "\n", "\n", "# This lowercases tokens if necessary", "\n", "", "text", "=", "(", "\n", "token", ".", "text", ".", "lower", "(", ")", "\n", "if", "self", ".", "_do_lowercase", "and", "token", ".", "text", "not", "in", "self", ".", "_never_lowercase", "\n", "else", "token", ".", "text", "\n", "for", "token", "in", "tokens", "\n", ")", "\n", "\n", "# Obtain a nested sequence of wordpieces, each represented by a list of wordpiece ids", "\n", "token_wordpiece_ids", "=", "[", "\n", "[", "self", ".", "vocab", "[", "wordpiece", "]", "for", "wordpiece", "in", "self", ".", "wordpiece_tokenizer", "(", "token", ")", "]", "\n", "for", "token", "in", "text", "\n", "]", "\n", "\n", "# Flattened list of wordpieces. In the end, the output of the model (e.g., BERT) should", "\n", "# have a sequence length equal to the length of this list. However, it will first be split into", "\n", "# chunks of length `self.max_pieces` so that they can be fit through the model. After packing", "\n", "# and passing through the model, it should be unpacked to represent the wordpieces in this list.", "\n", "flat_wordpiece_ids", "=", "[", "\n", "wordpiece", "for", "token", "in", "token_wordpiece_ids", "for", "wordpiece", "in", "token", "\n", "]", "\n", "\n", "# Similarly, we want to compute the token_type_ids from the flattened wordpiece ids before", "\n", "# we do the windowing; otherwise [SEP] tokens would get counted multiple times.", "\n", "flat_token_type_ids", "=", "_get_token_type_ids", "(", "\n", "flat_wordpiece_ids", ",", "self", ".", "_separator_ids", "\n", ")", "\n", "\n", "# The code below will (possibly) pack the wordpiece sequence into multiple sub-sequences by using a sliding", "\n", "# window `window_length` that overlaps with previous windows according to the `stride`. Suppose we have", "\n", "# the following sentence: \"I went to the store to buy some milk\". Then a sliding window of length 4 and", "\n", "# stride of length 2 will split them up into:", "\n", "\n", "# \"[I went to the] [to the store to] [store to buy some] [buy some milk [PAD]]\".", "\n", "\n", "# This is to ensure that the model has context of as much of the sentence as possible to get accurate", "\n", "# embeddings. Finally, the sequences will be padded with any start/end piece ids, e.g.,", "\n", "\n", "# \"[CLS] I went to the [SEP] [CLS] to the store to [SEP] ...\".", "\n", "\n", "# The embedder should then be able to split this token sequence by the window length,", "\n", "# pass them through the model, and recombine them.", "\n", "\n", "# Specify the stride to be half of `self.max_pieces`, minus any additional start/end wordpieces", "\n", "window_length", "=", "(", "\n", "self", ".", "max_pieces", "-", "len", "(", "self", ".", "_start_piece_ids", ")", "-", "len", "(", "self", ".", "_end_piece_ids", ")", "\n", ")", "\n", "stride", "=", "window_length", "//", "2", "\n", "\n", "# offsets[i] will give us the index into wordpiece_ids", "\n", "# for the wordpiece \"corresponding to\" the i-th input token.", "\n", "offsets", "=", "[", "]", "\n", "\n", "# If we're using initial offsets, we want to start at offset = len(text_tokens)", "\n", "# so that the first offset is the index of the first wordpiece of tokens[0].", "\n", "# Otherwise, we want to start at len(text_tokens) - 1, so that the \"previous\"", "\n", "# offset is the last wordpiece of \"tokens[-1]\".", "\n", "offset", "=", "(", "\n", "len", "(", "self", ".", "_start_piece_ids", ")", "\n", "if", "self", ".", "use_starting_offsets", "\n", "else", "len", "(", "self", ".", "_start_piece_ids", ")", "-", "1", "\n", ")", "\n", "\n", "# Count amount of wordpieces accumulated", "\n", "pieces_accumulated", "=", "0", "\n", "for", "token", "in", "token_wordpiece_ids", ":", "\n", "# Truncate the sequence if specified, which depends on where the offsets are", "\n", "            ", "next_offset", "=", "1", "if", "self", ".", "use_starting_offsets", "else", "0", "\n", "if", "(", "\n", "self", ".", "_truncate_long_sequences", "\n", "and", "offset", "+", "len", "(", "token", ")", "-", "1", ">=", "window_length", "+", "next_offset", "\n", ")", ":", "\n", "                ", "break", "\n", "\n", "# For initial offsets, the current value of `offset` is the start of", "\n", "# the current wordpiece, so add it to `offsets` and then increment it.", "\n", "", "if", "self", ".", "use_starting_offsets", ":", "\n", "                ", "offsets", ".", "append", "(", "offset", ")", "\n", "offset", "+=", "len", "(", "token", ")", "\n", "# For final offsets, the current value of `offset` is the end of", "\n", "# the previous wordpiece, so increment it and then add it to `offsets`.", "\n", "", "else", ":", "\n", "                ", "offset", "+=", "len", "(", "token", ")", "\n", "offsets", ".", "append", "(", "offset", ")", "\n", "\n", "", "pieces_accumulated", "+=", "len", "(", "token", ")", "\n", "\n", "", "if", "len", "(", "flat_wordpiece_ids", ")", "<=", "window_length", ":", "\n", "# If all the wordpieces fit, then we don't need to do anything special", "\n", "            ", "wordpiece_windows", "=", "[", "self", ".", "_add_start_and_end", "(", "flat_wordpiece_ids", ")", "]", "\n", "token_type_ids", "=", "self", ".", "_extend", "(", "flat_token_type_ids", ")", "\n", "", "elif", "self", ".", "_truncate_long_sequences", ":", "\n", "            ", "self", ".", "_warn_about_truncation", "(", "tokens", ")", "\n", "wordpiece_windows", "=", "[", "\n", "self", ".", "_add_start_and_end", "(", "flat_wordpiece_ids", "[", ":", "pieces_accumulated", "]", ")", "\n", "]", "\n", "token_type_ids", "=", "self", ".", "_extend", "(", "flat_token_type_ids", "[", ":", "pieces_accumulated", "]", ")", "\n", "", "else", ":", "\n", "# Create a sliding window of wordpieces of length `max_pieces` that advances by `stride` steps and", "\n", "# add start/end wordpieces to each window", "\n", "# TODO: this currently does not respect word boundaries, so words may be cut in half between windows", "\n", "# However, this would increase complexity, as sequences would need to be padded/unpadded in the middle", "\n", "            ", "wordpiece_windows", "=", "[", "\n", "self", ".", "_add_start_and_end", "(", "flat_wordpiece_ids", "[", "i", ":", "i", "+", "window_length", "]", ")", "\n", "for", "i", "in", "range", "(", "0", ",", "len", "(", "flat_wordpiece_ids", ")", ",", "stride", ")", "\n", "]", "\n", "\n", "token_type_windows", "=", "[", "\n", "self", ".", "_extend", "(", "flat_token_type_ids", "[", "i", ":", "i", "+", "window_length", "]", ")", "\n", "for", "i", "in", "range", "(", "0", ",", "len", "(", "flat_token_type_ids", ")", ",", "stride", ")", "\n", "]", "\n", "\n", "# Check for overlap in the last window. Throw it away if it is redundant.", "\n", "last_window", "=", "wordpiece_windows", "[", "-", "1", "]", "[", "1", ":", "]", "\n", "penultimate_window", "=", "wordpiece_windows", "[", "-", "2", "]", "\n", "if", "last_window", "==", "penultimate_window", "[", "-", "len", "(", "last_window", ")", ":", "]", ":", "\n", "                ", "wordpiece_windows", "=", "wordpiece_windows", "[", ":", "-", "1", "]", "\n", "token_type_windows", "=", "token_type_windows", "[", ":", "-", "1", "]", "\n", "\n", "", "token_type_ids", "=", "[", "\n", "token_type", "for", "window", "in", "token_type_windows", "for", "token_type", "in", "window", "\n", "]", "\n", "\n", "# Flatten the wordpiece windows", "\n", "", "wordpiece_ids", "=", "[", "\n", "wordpiece", "for", "sequence", "in", "wordpiece_windows", "for", "wordpiece", "in", "sequence", "\n", "]", "\n", "\n", "# Our mask should correspond to the original tokens,", "\n", "# because calling util.get_text_field_mask on the", "\n", "# \"wordpiece_id\" tokens will produce the wrong shape.", "\n", "# However, because of the max_pieces constraint, we may", "\n", "# have truncated the wordpieces; accordingly, we want the mask", "\n", "# to correspond to the remaining tokens after truncation, which", "\n", "# is captured by the offsets.", "\n", "mask", "=", "[", "1", "for", "_", "in", "offsets", "]", "\n", "\n", "return", "{", "\n", "\"input_ids\"", ":", "wordpiece_ids", ",", "\n", "\"offsets\"", ":", "offsets", ",", "\n", "\"token_type_ids\"", ":", "token_type_ids", ",", "\n", "\"mask\"", ":", "mask", ",", "\n", "}", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.token_indexers.wordpiece_indexer.WordpieceIndexer.get_empty_token_list": [[293, 296], ["None"], "methods", ["None"], ["", "@", "overrides", "\n", "def", "get_empty_token_list", "(", "self", ")", "->", "IndexedTokenList", ":", "\n", "        ", "return", "{", "\"input_ids\"", ":", "[", "]", ",", "\"offsets\"", ":", "[", "]", ",", "\"token_type_ids\"", ":", "[", "]", ",", "\"mask\"", ":", "[", "]", "}", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.token_indexers.wordpiece_indexer.WordpieceIndexer._add_start_and_end": [[297, 299], ["None"], "methods", ["None"], ["", "def", "_add_start_and_end", "(", "self", ",", "wordpiece_ids", ":", "List", "[", "int", "]", ")", "->", "List", "[", "int", "]", ":", "\n", "        ", "return", "self", ".", "_start_piece_ids", "+", "wordpiece_ids", "+", "self", ".", "_end_piece_ids", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.token_indexers.wordpiece_indexer.WordpieceIndexer._extend": [[300, 311], ["None"], "methods", ["None"], ["", "def", "_extend", "(", "self", ",", "token_type_ids", ":", "List", "[", "int", "]", ")", "->", "List", "[", "int", "]", ":", "\n", "        ", "\"\"\"\n        Extend the token type ids by len(start_piece_ids) on the left\n        and len(end_piece_ids) on the right.\n        \"\"\"", "\n", "first", "=", "token_type_ids", "[", "0", "]", "if", "token_type_ids", "else", "0", "\n", "last", "=", "token_type_ids", "[", "-", "1", "]", "if", "token_type_ids", "else", "0", "\n", "return", "(", "\n", "[", "first", "for", "_", "in", "self", ".", "_start_piece_ids", "]", "\n", "+", "token_type_ids", "\n", "+", "[", "last", "for", "_", "in", "self", ".", "_end_piece_ids", "]", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.token_indexers.wordpiece_indexer.PretrainedBertIndexer.__init__": [[349, 383], ["transformers.tokenization_bert.BertTokenizer.from_pretrained", "wordpiece_indexer.WordpieceIndexer.__init__", "pretrained_model.endswith", "logger.warning", "pretrained_model.endswith", "logger.warning"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.graph_util.plot.GraphPlot.__init__"], ["def", "__init__", "(", "\n", "self", ",", "\n", "pretrained_model", ":", "str", ",", "\n", "use_starting_offsets", ":", "bool", "=", "False", ",", "\n", "do_lowercase", ":", "bool", "=", "True", ",", "\n", "never_lowercase", ":", "List", "[", "str", "]", "=", "None", ",", "\n", "max_pieces", ":", "int", "=", "512", ",", "\n", "truncate_long_sequences", ":", "bool", "=", "True", ",", "\n", ")", "->", "None", ":", "\n", "        ", "if", "pretrained_model", ".", "endswith", "(", "\"-cased\"", ")", "and", "do_lowercase", ":", "\n", "            ", "logger", ".", "warning", "(", "\n", "\"Your BERT model appears to be cased, but your indexer is lowercasing tokens.\"", "\n", ")", "\n", "", "elif", "pretrained_model", ".", "endswith", "(", "\"-uncased\"", ")", "and", "not", "do_lowercase", ":", "\n", "            ", "logger", ".", "warning", "(", "\n", "\"Your BERT model appears to be uncased, \"", "\n", "\"but your indexer is not lowercasing tokens.\"", "\n", ")", "\n", "\n", "", "bert_tokenizer", "=", "BertTokenizer", ".", "from_pretrained", "(", "\n", "pretrained_model", ",", "do_lower_case", "=", "do_lowercase", "\n", ")", "\n", "super", "(", ")", ".", "__init__", "(", "\n", "vocab", "=", "bert_tokenizer", ".", "vocab", ",", "\n", "wordpiece_tokenizer", "=", "bert_tokenizer", ".", "wordpiece_tokenizer", ".", "tokenize", ",", "\n", "namespace", "=", "\"bert\"", ",", "\n", "use_starting_offsets", "=", "use_starting_offsets", ",", "\n", "max_pieces", "=", "max_pieces", ",", "\n", "do_lowercase", "=", "do_lowercase", ",", "\n", "never_lowercase", "=", "never_lowercase", ",", "\n", "start_tokens", "=", "[", "\"[CLS]\"", "]", ",", "\n", "end_tokens", "=", "[", "\"[SEP]\"", "]", ",", "\n", "separator_token", "=", "\"[SEP]\"", ",", "\n", "truncate_long_sequences", "=", "truncate_long_sequences", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.token_indexers.wordpiece_indexer.PretrainedBertIndexer.__eq__": [[385, 396], ["isinstance"], "methods", ["None"], ["", "def", "__eq__", "(", "self", ",", "other", ")", ":", "\n", "        ", "if", "isinstance", "(", "other", ",", "PretrainedBertIndexer", ")", ":", "\n", "            ", "for", "key", "in", "self", ".", "__dict__", ":", "\n", "                ", "if", "key", "==", "\"wordpiece_tokenizer\"", ":", "\n", "# This is a reference to a function in the huggingface code, which we can't", "\n", "# really modify to make this clean.  So we special-case it.", "\n", "                    ", "continue", "\n", "", "if", "self", ".", "__dict__", "[", "key", "]", "!=", "other", ".", "__dict__", "[", "key", "]", ":", "\n", "                    ", "return", "False", "\n", "", "", "return", "True", "\n", "", "return", "NotImplemented", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.token_indexers.wordpiece_indexer._get_token_type_ids": [[398, 424], ["len", "len", "token_type_ids.extend", "all", "token_type_ids.extend", "len", "token_type_ids.append", "range", "enumerate"], "function", ["None"], ["", "", "def", "_get_token_type_ids", "(", "\n", "wordpiece_ids", ":", "List", "[", "int", "]", ",", "separator_ids", ":", "List", "[", "int", "]", "\n", ")", "->", "List", "[", "int", "]", ":", "\n", "    ", "num_wordpieces", "=", "len", "(", "wordpiece_ids", ")", "\n", "token_type_ids", ":", "List", "[", "int", "]", "=", "[", "]", "\n", "type_id", "=", "0", "\n", "cursor", "=", "0", "\n", "while", "cursor", "<", "num_wordpieces", ":", "\n", "# check length", "\n", "        ", "if", "num_wordpieces", "-", "cursor", "<", "len", "(", "separator_ids", ")", ":", "\n", "            ", "token_type_ids", ".", "extend", "(", "type_id", "for", "_", "in", "range", "(", "num_wordpieces", "-", "cursor", ")", ")", "\n", "cursor", "+=", "num_wordpieces", "-", "cursor", "\n", "# check content", "\n", "# when it is a separator", "\n", "", "elif", "all", "(", "\n", "wordpiece_ids", "[", "cursor", "+", "index", "]", "==", "separator_id", "\n", "for", "index", ",", "separator_id", "in", "enumerate", "(", "separator_ids", ")", "\n", ")", ":", "\n", "            ", "token_type_ids", ".", "extend", "(", "type_id", "for", "_", "in", "separator_ids", ")", "\n", "type_id", "+=", "1", "\n", "cursor", "+=", "len", "(", "separator_ids", ")", "\n", "# when it is not", "\n", "", "else", ":", "\n", "            ", "cursor", "+=", "1", "\n", "token_type_ids", ".", "append", "(", "type_id", ")", "\n", "", "", "return", "token_type_ids", "\n", "", ""]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.token_indexers.token_characters_indexer.TokenCharactersIndexer.__init__": [[42, 67], ["allennlp.data.tokenizers.character_tokenizer.CharacterTokenizer", "allennlp.data.token_indexers.token_indexer.TokenIndexer.__init__", "warnings.warn", "allennlp.data.tokenizers.token.Token", "allennlp.data.tokenizers.token.Token"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.graph_util.plot.GraphPlot.__init__"], ["def", "__init__", "(", "\n", "self", ",", "\n", "namespace", ":", "str", "=", "\"token_characters\"", ",", "\n", "character_tokenizer", ":", "CharacterTokenizer", "=", "CharacterTokenizer", "(", ")", ",", "\n", "start_tokens", ":", "List", "[", "str", "]", "=", "None", ",", "\n", "end_tokens", ":", "List", "[", "str", "]", "=", "None", ",", "\n", "min_padding_length", ":", "int", "=", "0", ",", "\n", "token_min_padding_length", ":", "int", "=", "0", ",", "\n", ")", "->", "None", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "token_min_padding_length", ")", "\n", "if", "min_padding_length", "==", "0", ":", "\n", "            ", "url", "=", "\"https://github.com/allenai/allennlp/issues/1954\"", "\n", "warnings", ".", "warn", "(", "\n", "\"You are using the default value (0) of `min_padding_length`, \"", "\n", "f\"which can cause some subtle bugs (more info see {url}). \"", "\n", "\"Strongly recommend to set a value, usually the maximum size \"", "\n", "\"of the convolutional layer size when using CnnEncoder.\"", ",", "\n", "UserWarning", ",", "\n", ")", "\n", "", "self", ".", "_min_padding_length", "=", "min_padding_length", "\n", "self", ".", "_namespace", "=", "namespace", "\n", "self", ".", "_character_tokenizer", "=", "character_tokenizer", "\n", "\n", "self", ".", "_start_tokens", "=", "[", "Token", "(", "st", ")", "for", "st", "in", "(", "start_tokens", "or", "[", "]", ")", "]", "\n", "self", ".", "_end_tokens", "=", "[", "Token", "(", "et", ")", "for", "et", "in", "(", "end_tokens", "or", "[", "]", ")", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.token_indexers.token_characters_indexer.TokenCharactersIndexer.count_vocab_items": [[68, 79], ["token_characters_indexer.TokenCharactersIndexer._character_tokenizer.tokenize", "allennlp.common.checks.ConfigurationError", "getattr"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.tokenizers.whitespace_tokenizer.WhitespaceTokenizer.tokenize"], ["", "@", "overrides", "\n", "def", "count_vocab_items", "(", "self", ",", "token", ":", "Token", ",", "counter", ":", "Dict", "[", "str", ",", "Dict", "[", "str", ",", "int", "]", "]", ")", ":", "\n", "        ", "if", "token", ".", "text", "is", "None", ":", "\n", "            ", "raise", "ConfigurationError", "(", "\n", "\"TokenCharactersIndexer needs a tokenizer that retains text\"", "\n", ")", "\n", "", "for", "character", "in", "self", ".", "_character_tokenizer", ".", "tokenize", "(", "token", ".", "text", ")", ":", "\n", "# If `text_id` is set on the character token (e.g., if we're using byte encoding), we", "\n", "# will not be using the vocab for this character.", "\n", "            ", "if", "getattr", "(", "character", ",", "\"text_id\"", ",", "None", ")", "is", "None", ":", "\n", "                ", "counter", "[", "self", ".", "_namespace", "]", "[", "character", ".", "text", "]", "+=", "1", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.token_indexers.token_characters_indexer.TokenCharactersIndexer.tokens_to_indices": [[80, 101], ["itertools.chain", "token_characters_indexer.TokenCharactersIndexer._character_tokenizer.tokenize", "indices.append", "allennlp.common.checks.ConfigurationError", "token_indices.append", "getattr", "vocabulary.get_token_index"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.tokenizers.whitespace_tokenizer.WhitespaceTokenizer.tokenize", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.data.vocabulary.Vocabulary.get_token_index"], ["", "", "", "@", "overrides", "\n", "def", "tokens_to_indices", "(", "\n", "self", ",", "tokens", ":", "List", "[", "Token", "]", ",", "vocabulary", ":", "Vocabulary", "\n", ")", "->", "Dict", "[", "str", ",", "List", "[", "List", "[", "int", "]", "]", "]", ":", "\n", "        ", "indices", ":", "List", "[", "List", "[", "int", "]", "]", "=", "[", "]", "\n", "for", "token", "in", "itertools", ".", "chain", "(", "self", ".", "_start_tokens", ",", "tokens", ",", "self", ".", "_end_tokens", ")", ":", "\n", "            ", "token_indices", ":", "List", "[", "int", "]", "=", "[", "]", "\n", "if", "token", ".", "text", "is", "None", ":", "\n", "                ", "raise", "ConfigurationError", "(", "\n", "\"TokenCharactersIndexer needs a tokenizer that retains text\"", "\n", ")", "\n", "", "for", "character", "in", "self", ".", "_character_tokenizer", ".", "tokenize", "(", "token", ".", "text", ")", ":", "\n", "                ", "if", "getattr", "(", "character", ",", "\"text_id\"", ",", "None", ")", "is", "not", "None", ":", "\n", "# `text_id` being set on the token means that we aren't using the vocab, we just", "\n", "# use this id instead.", "\n", "                    ", "index", "=", "character", ".", "text_id", "\n", "", "else", ":", "\n", "                    ", "index", "=", "vocabulary", ".", "get_token_index", "(", "character", ".", "text", ",", "self", ".", "_namespace", ")", "\n", "", "token_indices", ".", "append", "(", "index", ")", "\n", "", "indices", ".", "append", "(", "token_indices", ")", "\n", "", "return", "{", "\"token_characters\"", ":", "indices", "}", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.token_indexers.token_characters_indexer.TokenCharactersIndexer.get_padding_lengths": [[102, 113], ["max", "len", "max", "len"], "methods", ["None"], ["", "@", "overrides", "\n", "def", "get_padding_lengths", "(", "self", ",", "indexed_tokens", ":", "IndexedTokenList", ")", "->", "Dict", "[", "str", ",", "int", "]", ":", "\n", "        ", "padding_lengths", "=", "{", "}", "\n", "padding_lengths", "[", "\"token_characters\"", "]", "=", "max", "(", "\n", "len", "(", "indexed_tokens", "[", "\"token_characters\"", "]", ")", ",", "self", ".", "_token_min_padding_length", "\n", ")", "\n", "max_num_characters", "=", "self", ".", "_min_padding_length", "\n", "for", "token", "in", "indexed_tokens", "[", "\"token_characters\"", "]", ":", "\n", "            ", "max_num_characters", "=", "max", "(", "len", "(", "token", ")", ",", "max_num_characters", ")", "# type: ignore", "\n", "", "padding_lengths", "[", "\"num_token_characters\"", "]", "=", "max_num_characters", "\n", "return", "padding_lengths", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.token_indexers.token_characters_indexer.TokenCharactersIndexer.as_padded_tensor_dict": [[114, 144], ["allennlp.common.util.pad_sequence_to_length", "max", "list", "len", "list.append", "zip", "len", "list.pop", "torch.LongTensor", "itertools.zip_longest", "list"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.util.pad_sequence_to_length", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.params.Params.pop"], ["", "@", "overrides", "\n", "def", "as_padded_tensor_dict", "(", "\n", "self", ",", "tokens", ":", "IndexedTokenList", ",", "padding_lengths", ":", "Dict", "[", "str", ",", "int", "]", "\n", ")", "->", "Dict", "[", "str", ",", "torch", ".", "Tensor", "]", ":", "\n", "# Pad the tokens.", "\n", "        ", "padded_tokens", "=", "pad_sequence_to_length", "(", "\n", "tokens", "[", "\"token_characters\"", "]", ",", "\n", "padding_lengths", "[", "\"token_characters\"", "]", ",", "\n", "default_value", "=", "lambda", ":", "[", "]", ",", "\n", ")", "\n", "\n", "# Pad the characters within the tokens.", "\n", "desired_token_length", "=", "padding_lengths", "[", "\"num_token_characters\"", "]", "\n", "longest_token", ":", "List", "[", "int", "]", "=", "max", "(", "tokens", "[", "\"token_characters\"", "]", ",", "key", "=", "len", ",", "default", "=", "[", "]", ")", "# type: ignore", "\n", "padding_value", "=", "0", "\n", "if", "desired_token_length", ">", "len", "(", "longest_token", ")", ":", "\n", "# Since we want to pad to greater than the longest token, we add a", "\n", "# \"dummy token\" so we can take advantage of the fast implementation of itertools.zip_longest.", "\n", "            ", "padded_tokens", ".", "append", "(", "[", "padding_value", "]", "*", "desired_token_length", ")", "\n", "# pad the list of lists to the longest sublist, appending 0's", "\n", "", "padded_tokens", "=", "list", "(", "\n", "zip", "(", "*", "itertools", ".", "zip_longest", "(", "*", "padded_tokens", ",", "fillvalue", "=", "padding_value", ")", ")", "\n", ")", "\n", "if", "desired_token_length", ">", "len", "(", "longest_token", ")", ":", "\n", "# Removes the \"dummy token\".", "\n", "            ", "padded_tokens", ".", "pop", "(", ")", "\n", "# Truncates all the tokens to the desired length, and return the result.", "\n", "", "return", "{", "\n", "\"token_characters\"", ":", "torch", ".", "LongTensor", "(", "\n", "[", "list", "(", "token", "[", ":", "desired_token_length", "]", ")", "for", "token", "in", "padded_tokens", "]", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.token_indexers.token_characters_indexer.TokenCharactersIndexer.get_empty_token_list": [[147, 150], ["None"], "methods", ["None"], ["", "@", "overrides", "\n", "def", "get_empty_token_list", "(", "self", ")", "->", "IndexedTokenList", ":", "\n", "        ", "return", "{", "\"token_characters\"", ":", "[", "]", "}", "\n", "", "", ""]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.iterators.same_language_iterator.SameLanguageIterator._create_batches": [[34, 57], ["same_language_iterator.SameLanguageIterator._memory_sized_lists", "same_language_iterator.split_by_language", "random.shuffle", "iter", "collections.deque", "allennlp.common.util.lazy_groups_of", "same_language_iterator.SameLanguageIterator._ensure_batch_is_sufficiently_small", "allennlp.data.batch.Batch", "allennlp.data.batch.Batch"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.iterators.data_iterator.DataIterator._memory_sized_lists", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.iterators.same_language_iterator.split_by_language", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.util.lazy_groups_of", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.iterators.data_iterator.DataIterator._ensure_batch_is_sufficiently_small"], ["def", "_create_batches", "(", "\n", "self", ",", "instances", ":", "Iterable", "[", "Instance", "]", ",", "shuffle", ":", "bool", "\n", ")", "->", "Iterable", "[", "Batch", "]", ":", "\n", "# First break the dataset into memory-sized lists:", "\n", "        ", "for", "instance_list", "in", "self", ".", "_memory_sized_lists", "(", "instances", ")", ":", "\n", "            ", "if", "shuffle", ":", "\n", "                ", "random", ".", "shuffle", "(", "instance_list", ")", "\n", "", "instance_list", "=", "split_by_language", "(", "instance_list", ")", "\n", "for", "same_lang_batch", "in", "instance_list", ":", "\n", "                ", "iterator", "=", "iter", "(", "same_lang_batch", ")", "\n", "excess", ":", "Deque", "[", "Instance", "]", "=", "deque", "(", ")", "\n", "# Then break each memory-sized list into batches.", "\n", "for", "batch_instances", "in", "lazy_groups_of", "(", "iterator", ",", "self", ".", "_batch_size", ")", ":", "\n", "                    ", "for", "(", "\n", "poss_smaller_batches", "\n", ")", "in", "self", ".", "_ensure_batch_is_sufficiently_small", "(", "\n", "batch_instances", ",", "# type: ignore", "\n", "excess", ",", "\n", ")", ":", "\n", "                        ", "batch", "=", "Batch", "(", "poss_smaller_batches", ")", "\n", "yield", "batch", "\n", "", "", "if", "excess", ":", "\n", "                    ", "yield", "Batch", "(", "excess", ")", "\n", "", "", "", "", "", ""]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.iterators.same_language_iterator.split_by_language": [[15, 22], ["collections.defaultdict", "iter", "insts_by_lang[].append", "collections.defaultdict.values"], "function", ["None"], ["def", "split_by_language", "(", "instance_list", ")", ":", "\n", "    ", "insts_by_lang", "=", "defaultdict", "(", "lambda", ":", "[", "]", ")", "\n", "for", "inst", "in", "instance_list", ":", "\n", "        ", "inst_lang", "=", "inst", ".", "fields", "[", "\"metadata\"", "]", ".", "metadata", "[", "\"lang\"", "]", "\n", "insts_by_lang", "[", "inst_lang", "]", ".", "append", "(", "inst", ")", "\n", "\n", "", "return", "iter", "(", "insts_by_lang", ".", "values", "(", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.iterators.bucket_iterator.BucketIterator.__init__": [[66, 91], ["allennlp.data.iterators.data_iterator.DataIterator.__init__"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.graph_util.plot.GraphPlot.__init__"], ["def", "__init__", "(", "\n", "self", ",", "\n", "sorting_keys", ":", "List", "[", "Tuple", "[", "str", ",", "str", "]", "]", "=", "None", ",", "\n", "padding_noise", ":", "float", "=", "0.1", ",", "\n", "biggest_batch_first", ":", "bool", "=", "False", ",", "\n", "batch_size", ":", "int", "=", "32", ",", "\n", "instances_per_epoch", ":", "int", "=", "None", ",", "\n", "max_instances_in_memory", ":", "int", "=", "None", ",", "\n", "cache_instances", ":", "bool", "=", "False", ",", "\n", "track_epoch", ":", "bool", "=", "False", ",", "\n", "maximum_samples_per_batch", ":", "Tuple", "[", "str", ",", "int", "]", "=", "None", ",", "\n", "skip_smaller_batches", ":", "bool", "=", "False", ",", "\n", ")", "->", "None", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "\n", "cache_instances", "=", "cache_instances", ",", "\n", "track_epoch", "=", "track_epoch", ",", "\n", "batch_size", "=", "batch_size", ",", "\n", "instances_per_epoch", "=", "instances_per_epoch", ",", "\n", "max_instances_in_memory", "=", "max_instances_in_memory", ",", "\n", "maximum_samples_per_batch", "=", "maximum_samples_per_batch", ",", "\n", ")", "\n", "self", ".", "_sorting_keys", "=", "sorting_keys", "\n", "self", ".", "_padding_noise", "=", "padding_noise", "\n", "self", ".", "_biggest_batch_first", "=", "biggest_batch_first", "\n", "self", ".", "_skip_smaller_batches", "=", "skip_smaller_batches", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.iterators.bucket_iterator.BucketIterator._create_batches": [[92, 136], ["bucket_iterator.BucketIterator._memory_sized_lists", "bucket_iterator.BucketIterator._sort_by_padding", "collections.deque", "allennlp.common.util.lazy_groups_of", "iter", "bucket_iterator.BucketIterator._ensure_batch_is_sufficiently_small", "batches.append", "batches.pop", "batches.pop", "random.shuffle", "batches.insert", "batches.insert", "batches.append", "allennlp.data.batch.Batch", "len", "allennlp.data.batch.Batch", "len", "len"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.iterators.data_iterator.DataIterator._memory_sized_lists", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.iterators.bucket_iterator.BucketIterator._sort_by_padding", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.util.lazy_groups_of", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.iterators.data_iterator.DataIterator._ensure_batch_is_sufficiently_small", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.params.Params.pop", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.params.Params.pop"], ["", "@", "overrides", "\n", "def", "_create_batches", "(", "\n", "self", ",", "instances", ":", "Iterable", "[", "Instance", "]", ",", "shuffle", ":", "bool", "\n", ")", "->", "Iterable", "[", "Batch", "]", ":", "\n", "        ", "for", "instance_list", "in", "self", ".", "_memory_sized_lists", "(", "instances", ")", ":", "\n", "\n", "            ", "instance_list", "=", "self", ".", "_sort_by_padding", "(", "instance_list", ")", "\n", "\n", "batches", "=", "[", "]", "\n", "excess", ":", "Deque", "[", "Instance", "]", "=", "deque", "(", ")", "\n", "for", "batch_instances", "in", "lazy_groups_of", "(", "\n", "iter", "(", "instance_list", ")", ",", "self", ".", "_batch_size", "\n", ")", ":", "\n", "                ", "for", "(", "\n", "possibly_smaller_batches", "\n", ")", "in", "self", ".", "_ensure_batch_is_sufficiently_small", "(", "batch_instances", ",", "excess", ")", ":", "\n", "                    ", "if", "(", "\n", "self", ".", "_skip_smaller_batches", "\n", "and", "len", "(", "possibly_smaller_batches", ")", "<", "self", ".", "_batch_size", "\n", ")", ":", "\n", "                        ", "continue", "\n", "", "batches", ".", "append", "(", "Batch", "(", "possibly_smaller_batches", ")", ")", "\n", "", "", "if", "excess", "and", "(", "\n", "not", "self", ".", "_skip_smaller_batches", "or", "len", "(", "excess", ")", "==", "self", ".", "_batch_size", "\n", ")", ":", "\n", "                ", "batches", ".", "append", "(", "Batch", "(", "excess", ")", ")", "\n", "\n", "# TODO(brendanr): Add multi-GPU friendly grouping, i.e. group", "\n", "# num_gpu batches together, shuffle and then expand the groups.", "\n", "# This guards against imbalanced batches across GPUs.", "\n", "", "move_to_front", "=", "self", ".", "_biggest_batch_first", "and", "len", "(", "batches", ")", ">", "1", "\n", "if", "move_to_front", ":", "\n", "# We'll actually pop the last _two_ batches, because the last one might not be full.", "\n", "                ", "last_batch", "=", "batches", ".", "pop", "(", ")", "\n", "penultimate_batch", "=", "batches", ".", "pop", "(", ")", "\n", "", "if", "shuffle", ":", "\n", "# NOTE: if shuffle is false, the data will still be in a different order", "\n", "# because of the bucket sorting.", "\n", "                ", "random", ".", "shuffle", "(", "batches", ")", "\n", "", "if", "move_to_front", ":", "\n", "                ", "batches", ".", "insert", "(", "0", ",", "penultimate_batch", ")", "\n", "batches", ".", "insert", "(", "0", ",", "last_batch", ")", "\n", "\n", "", "yield", "from", "batches", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.iterators.bucket_iterator.BucketIterator._sort_by_padding": [[137, 173], ["instances_with_lengths.sort", "logger.info", "bucket_iterator.BucketIterator._guess_sorting_keys", "logger.info", "instance.index_fields", "typing.cast", "instances_with_lengths.append", "instance.get_padding_lengths", "typing.cast.items", "allennlp.common.util.add_noise_to_dict_values"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.dataset_readers.multiprocess_dataset_reader.logger.info", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.iterators.bucket_iterator.BucketIterator._guess_sorting_keys", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.dataset_readers.multiprocess_dataset_reader.logger.info", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.data.instance.Instance.index_fields", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.fields.text_field.TextField.get_padding_lengths", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.util.add_noise_to_dict_values"], ["", "", "def", "_sort_by_padding", "(", "self", ",", "instances", ":", "List", "[", "Instance", "]", ")", "->", "List", "[", "Instance", "]", ":", "\n", "        ", "\"\"\"\n        Sorts the instances by their padding lengths, using the keys in\n        `sorting_keys` (in the order in which they are provided).  `sorting_keys` is a list of\n        `(field_name, padding_key)` tuples.\n        \"\"\"", "\n", "if", "not", "self", ".", "_sorting_keys", ":", "\n", "            ", "logger", ".", "info", "(", "\"No sorting keys given; trying to guess a good one\"", ")", "\n", "self", ".", "_guess_sorting_keys", "(", "instances", ")", "\n", "logger", ".", "info", "(", "f\"Using {self._sorting_keys} as the sorting keys\"", ")", "\n", "", "instances_with_lengths", "=", "[", "]", "\n", "for", "instance", "in", "instances", ":", "\n", "# Make sure instance is indexed before calling .get_padding", "\n", "            ", "instance", ".", "index_fields", "(", "self", ".", "vocab", ")", "\n", "padding_lengths", "=", "cast", "(", "\n", "Dict", "[", "str", ",", "Dict", "[", "str", ",", "float", "]", "]", ",", "instance", ".", "get_padding_lengths", "(", ")", "\n", ")", "\n", "if", "self", ".", "_padding_noise", ">", "0.0", ":", "\n", "                ", "noisy_lengths", "=", "{", "}", "\n", "for", "field_name", ",", "field_lengths", "in", "padding_lengths", ".", "items", "(", ")", ":", "\n", "                    ", "noisy_lengths", "[", "field_name", "]", "=", "add_noise_to_dict_values", "(", "\n", "field_lengths", ",", "self", ".", "_padding_noise", "\n", ")", "\n", "", "padding_lengths", "=", "noisy_lengths", "\n", "", "instance_with_lengths", "=", "(", "\n", "[", "\n", "padding_lengths", "[", "field_name", "]", "[", "padding_key", "]", "\n", "for", "(", "field_name", ",", "padding_key", ")", "in", "self", ".", "_sorting_keys", "\n", "]", ",", "\n", "instance", ",", "\n", ")", "\n", "instances_with_lengths", ".", "append", "(", "instance_with_lengths", ")", "\n", "", "instances_with_lengths", ".", "sort", "(", "key", "=", "lambda", "x", ":", "x", "[", "0", "]", ")", "\n", "return", "[", "\n", "instance_with_lengths", "[", "-", "1", "]", "\n", "for", "instance_with_lengths", "in", "instances_with_lengths", "\n", "]", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.iterators.bucket_iterator.BucketIterator._guess_sorting_keys": [[175, 196], ["instance.index_fields", "typing.cast", "typing.cast.items", "AssertionError", "instance.get_padding_lengths", "field_padding.items"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.data.instance.Instance.index_fields", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.fields.text_field.TextField.get_padding_lengths"], ["", "def", "_guess_sorting_keys", "(", "self", ",", "instances", ":", "List", "[", "Instance", "]", ")", "->", "None", ":", "\n", "        ", "max_length", "=", "0.0", "\n", "longest_padding_key", ":", "Tuple", "[", "str", ",", "str", "]", "=", "None", "\n", "for", "instance", "in", "instances", ":", "\n", "            ", "instance", ".", "index_fields", "(", "self", ".", "vocab", ")", "\n", "padding_lengths", "=", "cast", "(", "\n", "Dict", "[", "str", ",", "Dict", "[", "str", ",", "float", "]", "]", ",", "instance", ".", "get_padding_lengths", "(", ")", "\n", ")", "\n", "for", "field_name", ",", "field_padding", "in", "padding_lengths", ".", "items", "(", ")", ":", "\n", "                ", "for", "padding_key", ",", "length", "in", "field_padding", ".", "items", "(", ")", ":", "\n", "                    ", "if", "length", ">", "max_length", ":", "\n", "                        ", "max_length", "=", "length", "\n", "longest_padding_key", "=", "(", "field_name", ",", "padding_key", ")", "\n", "", "", "", "", "if", "not", "longest_padding_key", ":", "\n", "# This shouldn't ever happen (you basically have to have an empty instance list), but", "\n", "# just in case...", "\n", "            ", "raise", "AssertionError", "(", "\n", "\"Found no field that needed padding; we are surprised you got this error, please \"", "\n", "\"open an issue on github\"", "\n", ")", "\n", "", "self", ".", "_sorting_keys", "=", "[", "longest_padding_key", "]", "\n", "", "", ""]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.iterators.pass_through_iterator.PassThroughIterator.__init__": [[27, 29], ["allennlp.data.iterators.data_iterator.DataIterator.__init__"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.graph_util.plot.GraphPlot.__init__"], ["def", "__init__", "(", "self", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "batch_size", "=", "1", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.iterators.pass_through_iterator.PassThroughIterator._create_batches": [[30, 35], ["RuntimeError"], "methods", ["None"], ["", "@", "overrides", "\n", "def", "_create_batches", "(", "\n", "self", ",", "instances", ":", "Iterable", "[", "Instance", "]", ",", "shuffle", ":", "bool", "\n", ")", "->", "Iterable", "[", "Batch", "]", ":", "\n", "        ", "raise", "RuntimeError", "(", "\"PassThroughIterator doesn't use create_batches\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.iterators.pass_through_iterator.PassThroughIterator.__call__": [[36, 59], ["logger.warning", "itertools.count", "range", "instance.index_fields", "instance.as_tensor_dict"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.data.instance.Instance.index_fields", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.data.instance.Instance.as_tensor_dict"], ["", "def", "__call__", "(", "\n", "self", ",", "\n", "instances", ":", "Iterable", "[", "Instance", "]", ",", "\n", "num_epochs", ":", "int", "=", "None", ",", "\n", "shuffle", ":", "bool", "=", "False", ",", "\n", ")", "->", "Iterator", "[", "TensorDict", "]", ":", "\n", "# Warn users that this iterator does not do anything for you.", "\n", "        ", "if", "shuffle", ":", "\n", "            ", "logger", ".", "warning", "(", "\n", "\"PassThroughIterator does not shuffle instances. If shuffling is \"", "\n", "\"required, please implement in your DatasetReader.\"", "\n", ")", "\n", "\n", "", "if", "num_epochs", "is", "None", ":", "\n", "            ", "epochs", ":", "Iterable", "[", "int", "]", "=", "itertools", ".", "count", "(", ")", "\n", "", "else", ":", "\n", "            ", "epochs", "=", "range", "(", "num_epochs", ")", "\n", "\n", "", "for", "_", "in", "epochs", ":", "\n", "            ", "for", "instance", "in", "instances", ":", "\n", "                ", "if", "self", ".", "vocab", "is", "not", "None", ":", "\n", "                    ", "instance", ".", "index_fields", "(", "self", ".", "vocab", ")", "\n", "", "yield", "instance", ".", "as_tensor_dict", "(", ")", "\n", "", "", "", "", ""]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.iterators.multiprocess_iterator.MultiprocessIterator.__init__": [[129, 156], ["allennlp.data.iterators.data_iterator.DataIterator.__init__", "allennlp.common.checks.ConfigurationError", "allennlp.common.checks.ConfigurationError"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.graph_util.plot.GraphPlot.__init__"], ["def", "__init__", "(", "\n", "self", ",", "\n", "base_iterator", ":", "DataIterator", ",", "\n", "num_workers", ":", "int", "=", "1", ",", "\n", "output_queue_size", ":", "int", "=", "1000", ",", "\n", ")", "->", "None", ":", "\n", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "num_workers", "=", "num_workers", "\n", "self", ".", "batch_size", "=", "base_iterator", ".", "_batch_size", "\n", "self", ".", "output_queue_size", "=", "output_queue_size", "\n", "\n", "# These two options make the iterator stateful, which means it can't be shared", "\n", "# across multiple processes.", "\n", "if", "base_iterator", ".", "_cache_instances", ":", "\n", "            ", "raise", "ConfigurationError", "(", "\n", "\"cannot use Multiprocess iterator with cache_instances\"", "\n", ")", "\n", "", "if", "base_iterator", ".", "_instances_per_epoch", ":", "\n", "            ", "raise", "ConfigurationError", "(", "\n", "\"cannot use instances_per_epoch with Multiprocess iterator\"", "\n", ")", "\n", "\n", "", "self", ".", "iterator", "=", "base_iterator", "\n", "\n", "self", ".", "processes", ":", "List", "[", "Process", "]", "=", "[", "]", "\n", "self", ".", "queuer", ":", "Optional", "[", "Process", "]", "=", "None", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.iterators.multiprocess_iterator.MultiprocessIterator._create_batches": [[157, 161], ["RuntimeError"], "methods", ["None"], ["", "def", "_create_batches", "(", "\n", "self", ",", "instances", ":", "Iterable", "[", "Instance", "]", ",", "shuffle", ":", "bool", "\n", ")", "->", "Iterable", "[", "Batch", "]", ":", "\n", "        ", "raise", "RuntimeError", "(", "\"MultiprocessIterator doesn't use create_batches\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.iterators.multiprocess_iterator.MultiprocessIterator.index_with": [[162, 164], ["multiprocess_iterator.MultiprocessIterator.iterator.index_with"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.iterators.data_iterator.DataIterator.index_with"], ["", "def", "index_with", "(", "self", ",", "vocab", ":", "Vocabulary", ")", ":", "\n", "        ", "self", ".", "iterator", ".", "index_with", "(", "vocab", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.iterators.multiprocess_iterator.MultiprocessIterator._call_with_instances": [[165, 205], ["torch.multiprocessing.JoinableQueue", "torch.multiprocessing.Queue", "torch.multiprocessing.Process", "multiprocess_iterator.MultiprocessIterator.queuer.start", "range", "multiprocess_iterator.MultiprocessIterator.processes.clear", "torch.multiprocessing.Process", "torch.multiprocessing.Process.start", "multiprocess_iterator.MultiprocessIterator.processes.append", "torch.multiprocessing.JoinableQueue.get", "torch.multiprocessing.JoinableQueue.task_done", "isinstance", "torch.multiprocessing.Process.join", "multiprocess_iterator.MultiprocessIterator.queuer.join", "logger.info"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.dataset_readers.multiprocess_dataset_reader.QIterable.start", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.training.metric_tracker.MetricTracker.clear", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.dataset_readers.multiprocess_dataset_reader.QIterable.start", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.params.Params.get", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.dataset_readers.multiprocess_dataset_reader.QIterable.join", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.dataset_readers.multiprocess_dataset_reader.QIterable.join", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.dataset_readers.multiprocess_dataset_reader.logger.info"], ["", "def", "_call_with_instances", "(", "\n", "self", ",", "instances", ":", "Iterable", "[", "Instance", "]", ",", "num_epochs", ":", "int", ",", "shuffle", ":", "bool", "\n", ")", "->", "Iterator", "[", "TensorDict", "]", ":", "\n", "# JoinableQueue needed here as sharing tensors across processes", "\n", "# requires that the creating process not exit prematurely.", "\n", "        ", "output_queue", "=", "JoinableQueue", "(", "self", ".", "output_queue_size", ")", "\n", "input_queue", "=", "Queue", "(", "self", ".", "output_queue_size", "*", "self", ".", "batch_size", ")", "\n", "\n", "# Start process that populates the queue.", "\n", "self", ".", "queuer", "=", "Process", "(", "\n", "target", "=", "_queuer", ",", "args", "=", "(", "instances", ",", "input_queue", ",", "self", ".", "num_workers", ",", "num_epochs", ")", "\n", ")", "\n", "self", ".", "queuer", ".", "start", "(", ")", "\n", "\n", "# Start the tensor-dict workers.", "\n", "for", "i", "in", "range", "(", "self", ".", "num_workers", ")", ":", "\n", "            ", "args", "=", "(", "input_queue", ",", "output_queue", ",", "self", ".", "iterator", ",", "shuffle", ",", "i", ")", "\n", "process", "=", "Process", "(", "target", "=", "_create_tensor_dicts_from_queue", ",", "args", "=", "args", ")", "\n", "process", ".", "start", "(", ")", "\n", "self", ".", "processes", ".", "append", "(", "process", ")", "\n", "\n", "", "num_finished", "=", "0", "\n", "while", "num_finished", "<", "self", ".", "num_workers", ":", "\n", "            ", "item", "=", "output_queue", ".", "get", "(", ")", "\n", "output_queue", ".", "task_done", "(", ")", "\n", "if", "isinstance", "(", "item", ",", "int", ")", ":", "\n", "                ", "num_finished", "+=", "1", "\n", "logger", ".", "info", "(", "\n", "f\"worker {item} finished ({num_finished} / {self.num_workers})\"", "\n", ")", "\n", "", "else", ":", "\n", "                ", "yield", "item", "\n", "\n", "", "", "for", "process", "in", "self", ".", "processes", ":", "\n", "            ", "process", ".", "join", "(", ")", "\n", "", "self", ".", "processes", ".", "clear", "(", ")", "\n", "\n", "if", "self", ".", "queuer", "is", "not", "None", ":", "\n", "            ", "self", ".", "queuer", ".", "join", "(", ")", "\n", "self", ".", "queuer", "=", "None", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.iterators.multiprocess_iterator.MultiprocessIterator._call_with_qiterable": [[206, 240], ["torch.multiprocessing.JoinableQueue", "range", "qiterable.start", "range", "multiprocess_iterator.MultiprocessIterator.processes.clear", "qiterable.join", "torch.multiprocessing.Process", "torch.multiprocessing.Process.start", "multiprocess_iterator.MultiprocessIterator.processes.append", "torch.multiprocessing.JoinableQueue.get", "torch.multiprocessing.JoinableQueue.task_done", "isinstance", "torch.multiprocessing.Process.join", "logger.info"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.dataset_readers.multiprocess_dataset_reader.QIterable.start", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.training.metric_tracker.MetricTracker.clear", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.dataset_readers.multiprocess_dataset_reader.QIterable.join", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.dataset_readers.multiprocess_dataset_reader.QIterable.start", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.params.Params.get", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.dataset_readers.multiprocess_dataset_reader.QIterable.join", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.dataset_readers.multiprocess_dataset_reader.logger.info"], ["", "", "def", "_call_with_qiterable", "(", "\n", "self", ",", "qiterable", ":", "QIterable", ",", "num_epochs", ":", "int", ",", "shuffle", ":", "bool", "\n", ")", "->", "Iterator", "[", "TensorDict", "]", ":", "\n", "# JoinableQueue needed here as sharing tensors across processes", "\n", "# requires that the creating tensor not exit prematurely.", "\n", "        ", "output_queue", "=", "JoinableQueue", "(", "self", ".", "output_queue_size", ")", "\n", "\n", "for", "_", "in", "range", "(", "num_epochs", ")", ":", "\n", "            ", "qiterable", ".", "start", "(", ")", "\n", "\n", "# Start the tensor-dict workers.", "\n", "for", "i", "in", "range", "(", "self", ".", "num_workers", ")", ":", "\n", "                ", "args", "=", "(", "qiterable", ",", "output_queue", ",", "self", ".", "iterator", ",", "shuffle", ",", "i", ")", "\n", "process", "=", "Process", "(", "target", "=", "_create_tensor_dicts_from_qiterable", ",", "args", "=", "args", ")", "\n", "process", ".", "start", "(", ")", "\n", "self", ".", "processes", ".", "append", "(", "process", ")", "\n", "\n", "", "num_finished", "=", "0", "\n", "while", "num_finished", "<", "self", ".", "num_workers", ":", "\n", "                ", "item", "=", "output_queue", ".", "get", "(", ")", "\n", "output_queue", ".", "task_done", "(", ")", "\n", "if", "isinstance", "(", "item", ",", "int", ")", ":", "\n", "                    ", "num_finished", "+=", "1", "\n", "logger", ".", "info", "(", "\n", "f\"worker {item} finished ({num_finished} / {self.num_workers})\"", "\n", ")", "\n", "", "else", ":", "\n", "                    ", "yield", "item", "\n", "\n", "", "", "for", "process", "in", "self", ".", "processes", ":", "\n", "                ", "process", ".", "join", "(", ")", "\n", "", "self", ".", "processes", ".", "clear", "(", ")", "\n", "\n", "qiterable", ".", "join", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.iterators.multiprocess_iterator.MultiprocessIterator.__call__": [[241, 259], ["isinstance", "allennlp.common.checks.ConfigurationError", "multiprocess_iterator.MultiprocessIterator._call_with_qiterable", "multiprocess_iterator.MultiprocessIterator._call_with_instances"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.iterators.multiprocess_iterator.MultiprocessIterator._call_with_qiterable", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.iterators.multiprocess_iterator.MultiprocessIterator._call_with_instances"], ["", "", "def", "__call__", "(", "\n", "self", ",", "\n", "instances", ":", "Iterable", "[", "Instance", "]", ",", "\n", "num_epochs", ":", "int", "=", "None", ",", "\n", "shuffle", ":", "bool", "=", "True", ",", "\n", ")", "->", "Iterator", "[", "TensorDict", "]", ":", "\n", "\n", "# If you run it forever, the multiprocesses won't shut down correctly.", "\n", "# TODO(joelgrus) find a solution for this", "\n", "        ", "if", "num_epochs", "is", "None", ":", "\n", "            ", "raise", "ConfigurationError", "(", "\n", "\"Multiprocess Iterator must be run for a fixed number of epochs\"", "\n", ")", "\n", "\n", "", "if", "isinstance", "(", "instances", ",", "QIterable", ")", ":", "\n", "            ", "return", "self", ".", "_call_with_qiterable", "(", "instances", ",", "num_epochs", ",", "shuffle", ")", "\n", "", "else", ":", "\n", "            ", "return", "self", ".", "_call_with_instances", "(", "instances", ",", "num_epochs", ",", "shuffle", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.iterators.multiprocess_iterator.MultiprocessIterator.__del__": [[260, 277], ["process.terminate", "multiprocess_iterator.MultiprocessIterator.queuer.terminate"], "methods", ["None"], ["", "", "def", "__del__", "(", "self", ")", "->", "None", ":", "\n", "        ", "\"\"\"\n        Terminate processes if the user hasn't joined implicitly by consuming\n        all the tensors. This is necessary as leaving stray processes running\n        can corrupt shared state. In brief, we've observed shared memory\n        counters being reused (when the memory was free from the perspective of\n        the parent process) while the stray workers still held a reference to\n        them.\n\n        For a discussion of using destructors in Python in this manner, see\n        https://eli.thegreenplace.net/2009/06/12/safely-using-destructors-in-python/.\n        \"\"\"", "\n", "for", "process", "in", "self", ".", "processes", ":", "\n", "            ", "process", ".", "terminate", "(", ")", "\n", "\n", "", "if", "self", ".", "queuer", "is", "not", "None", ":", "\n", "            ", "self", ".", "queuer", ".", "terminate", "(", ")", "\n", "", "", "", ""]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.iterators.multiprocess_iterator._create_tensor_dicts_from_queue": [[19, 51], ["logger.info", "iterator", "output_queue.put", "output_queue.join", "input_queue.get", "multiprocess_iterator._create_tensor_dicts_from_queue.instances"], "function", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.dataset_readers.multiprocess_dataset_reader.logger.info", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.dataset_readers.multiprocess_dataset_reader.QIterable.join", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.params.Params.get"], ["def", "_create_tensor_dicts_from_queue", "(", "\n", "input_queue", ":", "Queue", ",", "\n", "output_queue", ":", "Queue", ",", "\n", "iterator", ":", "DataIterator", ",", "\n", "shuffle", ":", "bool", ",", "\n", "index", ":", "int", ",", "\n", ")", "->", "None", ":", "\n", "    ", "\"\"\"\n    Pulls instances from `input_queue`, converts them into `TensorDict`s\n    using `iterator`, and puts them on the `output_queue`.\n    \"\"\"", "\n", "logger", ".", "info", "(", "f\"Iterator worker: {index} PID: {os.getpid()}\"", ")", "\n", "\n", "def", "instances", "(", ")", "->", "Iterator", "[", "Instance", "]", ":", "\n", "        ", "instance", "=", "input_queue", ".", "get", "(", ")", "\n", "while", "instance", "is", "not", "None", ":", "\n", "            ", "yield", "instance", "\n", "instance", "=", "input_queue", ".", "get", "(", ")", "\n", "\n", "", "", "for", "tensor_dict", "in", "iterator", "(", "instances", "(", ")", ",", "num_epochs", "=", "1", ",", "shuffle", "=", "shuffle", ")", ":", "\n", "        ", "output_queue", ".", "put", "(", "tensor_dict", ")", "\n", "\n", "", "output_queue", ".", "put", "(", "index", ")", "\n", "\n", "# We need to ensure we've gotten all the tensors out of this queue before", "\n", "# this process ends. Otherwise we'll crash. See", "\n", "# https://github.com/pytorch/pytorch/issues/7181. This appears to be an", "\n", "# issue specifically with tensors, perhaps due to the refcounting involved", "\n", "# in managing them in shared memory. If you're working on this code, be", "\n", "# aware that I've only been able to reproduce this issue on Linux.  Testing", "\n", "# on a Mac alone is not sufficient.", "\n", "output_queue", ".", "join", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.iterators.multiprocess_iterator._create_tensor_dicts_from_qiterable": [[53, 86], ["logger.info", "iterator", "output_queue.put", "output_queue.join", "multiprocess_iterator._create_tensor_dicts_from_queue.instances"], "function", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.dataset_readers.multiprocess_dataset_reader.logger.info", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.dataset_readers.multiprocess_dataset_reader.QIterable.join"], ["", "def", "_create_tensor_dicts_from_qiterable", "(", "\n", "qiterable", ":", "QIterable", ",", "\n", "output_queue", ":", "Queue", ",", "\n", "iterator", ":", "DataIterator", ",", "\n", "shuffle", ":", "bool", ",", "\n", "index", ":", "int", ",", "\n", ")", "->", "None", ":", "\n", "    ", "\"\"\"\n    Pulls instances from `qiterable.output_queue`, converts them into\n    `TensorDict`s using `iterator`, and puts them on the `output_queue`.\n    \"\"\"", "\n", "logger", ".", "info", "(", "f\"Iterator worker: {index} PID: {os.getpid()}\"", ")", "\n", "\n", "def", "instances", "(", ")", "->", "Iterator", "[", "Instance", "]", ":", "\n", "        ", "while", "(", "\n", "qiterable", ".", "num_active_workers", ".", "value", ">", "0", "\n", "or", "qiterable", ".", "num_inflight_items", ".", "value", ">", "0", "\n", ")", ":", "\n", "            ", "while", "True", ":", "\n", "                ", "try", ":", "\n", "                    ", "yield", "qiterable", ".", "output_queue", ".", "get", "(", "block", "=", "False", ",", "timeout", "=", "1.0", ")", "\n", "with", "qiterable", ".", "num_inflight_items", ".", "get_lock", "(", ")", ":", "\n", "                        ", "qiterable", ".", "num_inflight_items", ".", "value", "-=", "1", "\n", "", "", "except", "Empty", ":", "\n", "                    ", "break", "\n", "\n", "", "", "", "", "for", "tensor_dict", "in", "iterator", "(", "instances", "(", ")", ",", "num_epochs", "=", "1", ",", "shuffle", "=", "shuffle", ")", ":", "\n", "        ", "output_queue", ".", "put", "(", "tensor_dict", ")", "\n", "\n", "", "output_queue", ".", "put", "(", "index", ")", "\n", "\n", "# See the note above in _create_tensor_dicts_from_queue.", "\n", "output_queue", ".", "join", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.iterators.multiprocess_iterator._queuer": [[88, 109], ["logger.info", "range", "input_queue.put", "input_queue.put", "os.getpid"], "function", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.dataset_readers.multiprocess_dataset_reader.logger.info"], ["", "def", "_queuer", "(", "\n", "instances", ":", "Iterable", "[", "Instance", "]", ",", "\n", "input_queue", ":", "Queue", ",", "\n", "num_workers", ":", "int", ",", "\n", "num_epochs", ":", "Optional", "[", "int", "]", ",", "\n", ")", "->", "None", ":", "\n", "    ", "\"\"\"\n    Reads Instances from the iterable and puts them in the input_queue.\n    \"\"\"", "\n", "logger", ".", "info", "(", "f\"Iterator queuer. PID: {os.getpid()}\"", ")", "\n", "epoch", "=", "0", "\n", "\n", "while", "num_epochs", "is", "None", "or", "epoch", "<", "num_epochs", ":", "\n", "        ", "epoch", "+=", "1", "\n", "for", "instance", "in", "instances", ":", "\n", "            ", "input_queue", ".", "put", "(", "instance", ")", "\n", "\n", "# Now put a None for each worker, since each needs to receive one", "\n", "# to know that it's done.", "\n", "", "", "for", "_", "in", "range", "(", "num_workers", ")", ":", "\n", "        ", "input_queue", ".", "put", "(", "None", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.iterators.homogeneous_batch_iterator.HomogeneousBatchIterator.__init__": [[44, 63], ["allennlp.data.iterators.data_iterator.DataIterator.__init__"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.graph_util.plot.GraphPlot.__init__"], ["def", "__init__", "(", "\n", "self", ",", "\n", "batch_size", ":", "int", "=", "32", ",", "\n", "instances_per_epoch", ":", "int", "=", "None", ",", "\n", "max_instances_in_memory", ":", "int", "=", "None", ",", "\n", "cache_instances", ":", "bool", "=", "False", ",", "\n", "track_epoch", ":", "bool", "=", "False", ",", "\n", "partition_key", ":", "str", "=", "\"dataset\"", ",", "\n", "skip_smaller_batches", ":", "bool", "=", "False", ",", "\n", ")", "->", "None", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "\n", "batch_size", ",", "\n", "instances_per_epoch", ",", "\n", "max_instances_in_memory", ",", "\n", "cache_instances", ",", "\n", "track_epoch", ",", "\n", ")", "\n", "self", ".", "_partition_key", "=", "partition_key", "\n", "self", ".", "_skip_smaller_batches", "=", "skip_smaller_batches", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.iterators.homogeneous_batch_iterator.HomogeneousBatchIterator._create_batches": [[64, 99], ["homogeneous_batch_iterator.HomogeneousBatchIterator._memory_sized_lists", "collections.defaultdict", "set", "random.shuffle", "hoppers[].append", "allennlp.common.util.lazy_groups_of", "batches.items", "iter", "hoppers.items", "next", "set.remove", "len", "allennlp.data.batch.Batch"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.iterators.data_iterator.DataIterator._memory_sized_lists", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.util.lazy_groups_of"], ["", "def", "_create_batches", "(", "\n", "self", ",", "instances", ":", "Iterable", "[", "Instance", "]", ",", "shuffle", ":", "bool", "\n", ")", "->", "Iterable", "[", "Batch", "]", ":", "\n", "# First break the dataset into memory-sized lists:", "\n", "        ", "for", "instance_list", "in", "self", ".", "_memory_sized_lists", "(", "instances", ")", ":", "\n", "            ", "if", "shuffle", ":", "\n", "                ", "random", ".", "shuffle", "(", "instance_list", ")", "\n", "\n", "# Divvy up the instances based on their value of the \"partition_key\" field.", "\n", "", "hoppers", ":", "Dict", "[", "str", ",", "List", "[", "Instance", "]", "]", "=", "defaultdict", "(", "list", ")", "\n", "for", "instance", "in", "instance_list", ":", "\n", "                ", "partition", "=", "instance", ".", "fields", "[", "self", ".", "_partition_key", "]", ".", "metadata", "# type: ignore", "\n", "hoppers", "[", "partition", "]", ".", "append", "(", "instance", ")", "\n", "\n", "# Get a `lazy_groups_of` iterator over each set of homogeneous instances.", "\n", "", "batches", "=", "{", "\n", "key", ":", "lazy_groups_of", "(", "iter", "(", "hopper", ")", ",", "self", ".", "_batch_size", ")", "\n", "for", "key", ",", "hopper", "in", "hoppers", ".", "items", "(", ")", "\n", "}", "\n", "\n", "remaining", "=", "set", "(", "batches", ")", "\n", "\n", "# Yield batches in a round-robin fashion until none are left.", "\n", "while", "remaining", ":", "\n", "                ", "for", "key", ",", "lazy_batches", "in", "batches", ".", "items", "(", ")", ":", "\n", "                    ", "if", "key", "in", "remaining", ":", "\n", "                        ", "try", ":", "\n", "                            ", "batch", "=", "next", "(", "lazy_batches", ")", "\n", "if", "(", "\n", "not", "self", ".", "_skip_smaller_batches", "\n", "or", "len", "(", "batch", ")", "==", "self", ".", "_batch_size", "\n", ")", ":", "\n", "                                ", "yield", "Batch", "(", "batch", ")", "\n", "", "", "except", "StopIteration", ":", "\n", "                            ", "remaining", ".", "remove", "(", "key", ")", "\n", "", "", "", "", "", "", "", ""]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.iterators.basic_iterator.BasicIterator._create_batches": [[22, 40], ["basic_iterator.BasicIterator._memory_sized_lists", "iter", "collections.deque", "allennlp.common.util.lazy_groups_of", "random.shuffle", "basic_iterator.BasicIterator._ensure_batch_is_sufficiently_small", "allennlp.data.batch.Batch", "allennlp.data.batch.Batch"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.iterators.data_iterator.DataIterator._memory_sized_lists", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.util.lazy_groups_of", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.iterators.data_iterator.DataIterator._ensure_batch_is_sufficiently_small"], ["def", "_create_batches", "(", "\n", "self", ",", "instances", ":", "Iterable", "[", "Instance", "]", ",", "shuffle", ":", "bool", "\n", ")", "->", "Iterable", "[", "Batch", "]", ":", "\n", "# First break the dataset into memory-sized lists:", "\n", "        ", "for", "instance_list", "in", "self", ".", "_memory_sized_lists", "(", "instances", ")", ":", "\n", "            ", "if", "shuffle", ":", "\n", "                ", "random", ".", "shuffle", "(", "instance_list", ")", "\n", "", "iterator", "=", "iter", "(", "instance_list", ")", "\n", "excess", ":", "Deque", "[", "Instance", "]", "=", "deque", "(", ")", "\n", "# Then break each memory-sized list into batches.", "\n", "for", "batch_instances", "in", "lazy_groups_of", "(", "iterator", ",", "self", ".", "_batch_size", ")", ":", "\n", "                ", "for", "(", "\n", "possibly_smaller_batches", "\n", ")", "in", "self", ".", "_ensure_batch_is_sufficiently_small", "(", "batch_instances", ",", "excess", ")", ":", "\n", "                    ", "batch", "=", "Batch", "(", "possibly_smaller_batches", ")", "\n", "yield", "batch", "\n", "", "", "if", "excess", ":", "\n", "                ", "yield", "Batch", "(", "excess", ")", "\n", "", "", "", "", ""]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.iterators.data_iterator.DataIterator.__init__": [[61, 90], ["collections.defaultdict", "collections.defaultdict"], "methods", ["None"], ["def", "__init__", "(", "\n", "self", ",", "\n", "batch_size", ":", "int", "=", "32", ",", "\n", "instances_per_epoch", ":", "int", "=", "None", ",", "\n", "max_instances_in_memory", ":", "int", "=", "None", ",", "\n", "cache_instances", ":", "bool", "=", "False", ",", "\n", "track_epoch", ":", "bool", "=", "False", ",", "\n", "maximum_samples_per_batch", ":", "Tuple", "[", "str", ",", "int", "]", "=", "None", ",", "\n", ")", "->", "None", ":", "\n", "        ", "self", ".", "vocab", ":", "Vocabulary", "=", "None", "\n", "\n", "self", ".", "_batch_size", "=", "batch_size", "\n", "self", ".", "_max_instances_in_memory", "=", "max_instances_in_memory", "\n", "self", ".", "_instances_per_epoch", "=", "instances_per_epoch", "\n", "self", ".", "_maximum_samples_per_batch", "=", "maximum_samples_per_batch", "\n", "\n", "# We might want to cache the instances in memory.", "\n", "self", ".", "_cache_instances", "=", "cache_instances", "\n", "self", ".", "_cache", ":", "Dict", "[", "int", ",", "List", "[", "TensorDict", "]", "]", "=", "defaultdict", "(", "list", ")", "\n", "\n", "# We also might want to add the epoch number to each instance.", "\n", "self", ".", "_track_epoch", "=", "track_epoch", "\n", "self", ".", "_epochs", ":", "Dict", "[", "int", ",", "int", "]", "=", "defaultdict", "(", "int", ")", "\n", "\n", "# We also might want to keep track of cursors;", "\n", "# for example, if each epoch represents less than one pass through the dataset,", "\n", "# we want to remember where we left off. As `Iterator`s are not necessarily hashable,", "\n", "# we use their id() as the key.", "\n", "self", ".", "_cursors", ":", "Dict", "[", "int", ",", "Iterator", "[", "Instance", "]", "]", "=", "{", "}", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.iterators.data_iterator.DataIterator.__call__": [[91, 167], ["id", "itertools.count", "range", "data_iterator.DataIterator._create_batches", "random.shuffle", "batch.get_padding_lengths", "logger.debug", "logger.debug", "batch.as_tensor_dict", "data_iterator.add_epoch_number", "batch.index_instances", "str", "len", "data_iterator.DataIterator._cache[].append"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.iterators.data_iterator.DataIterator._create_batches", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.fields.text_field.TextField.get_padding_lengths", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.data.instance.Instance.as_tensor_dict", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.iterators.data_iterator.add_epoch_number", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.data.batch.Batch.index_instances"], ["", "def", "__call__", "(", "\n", "self", ",", "\n", "instances", ":", "Iterable", "[", "Instance", "]", ",", "\n", "num_epochs", ":", "int", "=", "None", ",", "\n", "shuffle", ":", "bool", "=", "True", ",", "\n", ")", "->", "Iterator", "[", "TensorDict", "]", ":", "\n", "        ", "\"\"\"\n        Returns a generator that yields batches over the given dataset\n        for the given number of epochs. If `num_epochs` is not specified,\n        it will yield batches forever.\n\n        # Parameters\n\n        instances : `Iterable[Instance]`\n            The instances in the dataset. IMPORTANT: this must be able to be\n            iterated over *multiple times*. That is, it must be either a List\n            or some other object whose `__iter__` method returns a fresh iterator\n            each time it's called.\n        num_epochs : `int`, optional (default=`None`)\n            How times should we iterate over this dataset?  If `None`, we will iterate over it\n            forever.\n        shuffle : `bool`, optional (default=`True`)\n            If `True`, we will shuffle the instances in `dataset` before constructing batches\n            and iterating over the data.\n        \"\"\"", "\n", "# Instances is likely to be a list, which cannot be used as a key,", "\n", "# so we take the object id instead.", "\n", "key", "=", "id", "(", "instances", ")", "\n", "starting_epoch", "=", "self", ".", "_epochs", "[", "key", "]", "\n", "\n", "if", "num_epochs", "is", "None", ":", "\n", "            ", "epochs", ":", "Iterable", "[", "int", "]", "=", "itertools", ".", "count", "(", "starting_epoch", ")", "\n", "", "else", ":", "\n", "            ", "epochs", "=", "range", "(", "starting_epoch", ",", "starting_epoch", "+", "num_epochs", ")", "\n", "\n", "", "for", "epoch", "in", "epochs", ":", "\n", "            ", "if", "self", ".", "_cache_instances", "and", "key", "in", "self", ".", "_cache", ":", "\n", "# Serve the results from the cache.", "\n", "                ", "tensor_dicts", "=", "self", ".", "_cache", "[", "key", "]", "\n", "\n", "if", "shuffle", ":", "\n", "# TODO(brendanr): How can we handle this shuffle in a way", "\n", "# that respects multi-GPU friendly grouping?", "\n", "                    ", "random", ".", "shuffle", "(", "tensor_dicts", ")", "\n", "", "for", "tensor_dict", "in", "tensor_dicts", ":", "\n", "                    ", "if", "self", ".", "_track_epoch", ":", "\n", "# The tensor_dict already has an \"epoch_num\" list,", "\n", "# so just fill it with the right value.", "\n", "                        ", "epoch_list", ":", "List", "[", "int", "]", "=", "tensor_dict", "[", "\"epoch_num\"", "]", "# type: ignore", "\n", "tensor_dict", "[", "\"epoch_num\"", "]", "=", "[", "epoch", "for", "_", "in", "epoch_list", "]", "\n", "", "yield", "tensor_dict", "\n", "", "", "else", ":", "\n", "                ", "batches", "=", "self", ".", "_create_batches", "(", "instances", ",", "shuffle", ")", "\n", "\n", "# Should we add the instances to the cache this epoch?", "\n", "add_to_cache", "=", "self", ".", "_cache_instances", "and", "key", "not", "in", "self", ".", "_cache", "\n", "\n", "for", "batch", "in", "batches", ":", "\n", "                    ", "if", "self", ".", "_track_epoch", ":", "\n", "                        ", "add_epoch_number", "(", "batch", ",", "epoch", ")", "\n", "\n", "", "if", "self", ".", "vocab", "is", "not", "None", ":", "\n", "                        ", "batch", ".", "index_instances", "(", "self", ".", "vocab", ")", "\n", "\n", "", "padding_lengths", "=", "batch", ".", "get_padding_lengths", "(", ")", "\n", "logger", ".", "debug", "(", "\"Batch padding lengths: %s\"", ",", "str", "(", "padding_lengths", ")", ")", "\n", "logger", ".", "debug", "(", "\"Batch size: %d\"", ",", "len", "(", "batch", ".", "instances", ")", ")", "\n", "tensor_dict", "=", "batch", ".", "as_tensor_dict", "(", "padding_lengths", ")", "\n", "\n", "if", "add_to_cache", ":", "\n", "                        ", "self", ".", "_cache", "[", "key", "]", ".", "append", "(", "tensor_dict", ")", "\n", "\n", "", "yield", "tensor_dict", "\n", "\n", "# Increment epoch tracker", "\n", "", "", "self", ".", "_epochs", "[", "key", "]", "=", "epoch", "+", "1", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.iterators.data_iterator.DataIterator._take_instances": [[168, 198], ["id", "data_iterator.DataIterator._cursors.get", "iter", "iter", "next", "iter"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.params.Params.get"], ["", "", "def", "_take_instances", "(", "\n", "self", ",", "instances", ":", "Iterable", "[", "Instance", "]", ",", "max_instances", ":", "Optional", "[", "int", "]", "=", "None", "\n", ")", "->", "Iterator", "[", "Instance", "]", ":", "\n", "        ", "\"\"\"\n        Take the next `max_instances` instances from the given dataset.\n        If `max_instances` is `None`, then just take all instances from the dataset.\n        If `max_instances` is not `None`, each call resumes where the previous one\n        left off, and when you get to the end of the dataset you start again from the beginning.\n        \"\"\"", "\n", "# If max_instances isn't specified, just iterate once over the whole dataset", "\n", "if", "max_instances", "is", "None", ":", "\n", "            ", "yield", "from", "iter", "(", "instances", ")", "\n", "", "else", ":", "\n", "# If we don't have a cursor for this dataset, create one. We use `id()`", "\n", "# for the key because `instances` could be a list, which can't be used as a key.", "\n", "            ", "key", "=", "id", "(", "instances", ")", "\n", "iterator", "=", "self", ".", "_cursors", ".", "get", "(", "key", ",", "iter", "(", "instances", ")", ")", "\n", "\n", "while", "max_instances", ">", "0", ":", "\n", "                ", "try", ":", "\n", "# If there are instances left on this iterator,", "\n", "# yield one and decrement max_instances.", "\n", "                    ", "yield", "next", "(", "iterator", ")", "\n", "max_instances", "-=", "1", "\n", "", "except", "StopIteration", ":", "\n", "# None left, so start over again at the beginning of the dataset.", "\n", "                    ", "iterator", "=", "iter", "(", "instances", ")", "\n", "\n", "# We may have a new iterator, so update the cursor.", "\n", "", "", "self", ".", "_cursors", "[", "key", "]", "=", "iterator", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.iterators.data_iterator.DataIterator._memory_sized_lists": [[199, 235], ["allennlp.common.util.is_lazy", "data_iterator.DataIterator._take_instances", "allennlp.common.util.lazy_groups_of", "allennlp.common.util.lazy_groups_of", "allennlp.common.util.ensure_list", "list"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.util.is_lazy", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.iterators.data_iterator.DataIterator._take_instances", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.util.lazy_groups_of", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.util.lazy_groups_of", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.util.ensure_list"], ["", "", "def", "_memory_sized_lists", "(", "\n", "self", ",", "instances", ":", "Iterable", "[", "Instance", "]", "\n", ")", "->", "Iterable", "[", "List", "[", "Instance", "]", "]", ":", "\n", "        ", "\"\"\"\n        Breaks the dataset into \"memory-sized\" lists of instances,\n        which it yields up one at a time until it gets through a full epoch.\n\n        For example, if the dataset is already an in-memory list, and each epoch\n        represents one pass through the dataset, it just yields back the dataset.\n        Whereas if the dataset is lazily read from disk and we've specified to\n        load 1000 instances at a time, then it yields lists of 1000 instances each.\n        \"\"\"", "\n", "lazy", "=", "is_lazy", "(", "instances", ")", "\n", "\n", "# Get an iterator over the next epoch worth of instances.", "\n", "iterator", "=", "self", ".", "_take_instances", "(", "instances", ",", "self", ".", "_instances_per_epoch", ")", "\n", "\n", "# We have four different cases to deal with:", "\n", "\n", "# With lazy instances and no guidance about how many to load into memory,", "\n", "# we just load `batch_size` instances at a time:", "\n", "if", "lazy", "and", "self", ".", "_max_instances_in_memory", "is", "None", ":", "\n", "            ", "yield", "from", "lazy_groups_of", "(", "iterator", ",", "self", ".", "_batch_size", ")", "\n", "# If we specified max instances in memory, lazy or not, we just", "\n", "# load `max_instances_in_memory` instances at a time:", "\n", "", "elif", "self", ".", "_max_instances_in_memory", "is", "not", "None", ":", "\n", "            ", "yield", "from", "lazy_groups_of", "(", "iterator", ",", "self", ".", "_max_instances_in_memory", ")", "\n", "# If we have non-lazy instances, and we want all instances each epoch,", "\n", "# then we just yield back the list of instances:", "\n", "", "elif", "self", ".", "_instances_per_epoch", "is", "None", ":", "\n", "            ", "yield", "ensure_list", "(", "instances", ")", "\n", "# In the final case we have non-lazy instances, we want a specific number", "\n", "# of instances each epoch, and we didn't specify how to many instances to load", "\n", "# into memory. So we convert the whole iterator to a list:", "\n", "", "else", ":", "\n", "            ", "yield", "list", "(", "iterator", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.iterators.data_iterator.DataIterator._ensure_batch_is_sufficiently_small": [[236, 310], ["excess.extend", "excess.extend", "excess.popleft", "excess.popleft.get_padding_lengths", "excess.popleft.get_padding_lengths.values", "list", "excess.popleft.index_fields", "len", "batches.append", "excess.appendleft", "batch.append", "max", "excess.popleft.get_padding_lengths.values"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.fields.text_field.TextField.get_padding_lengths", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.data.instance.Instance.index_fields"], ["", "", "def", "_ensure_batch_is_sufficiently_small", "(", "\n", "self", ",", "batch_instances", ":", "Iterable", "[", "Instance", "]", ",", "excess", ":", "Deque", "[", "Instance", "]", "\n", ")", "->", "List", "[", "List", "[", "Instance", "]", "]", ":", "\n", "        ", "\"\"\"\n        If self._maximum_samples_per_batch is specified, then split the batch\n        into smaller sub-batches if it exceeds the maximum size.\n\n        # Parameters\n\n        batch_instances : `Iterable[Instance]`\n            A candidate batch.\n        excess : `Deque[Instance]`\n            Instances that were not sufficient to form an entire batch\n            previously. They will be used as part of the first sub-batch. This\n            will be populated with instances from the end of batch_instances\n            that do not consist of more than self._maximum_samples_per_batch\n            samples or self._batch_size instances. It is the caller's\n            responsibility to place these in a batch too, which may, of course,\n            be done in part with subsequent calls to this method.\n\n            WARNING: Mutated in place!\n        \"\"\"", "\n", "if", "self", ".", "_maximum_samples_per_batch", "is", "None", ":", "\n", "            ", "assert", "not", "excess", "\n", "return", "[", "list", "(", "batch_instances", ")", "]", "\n", "\n", "", "key", ",", "limit", "=", "self", ".", "_maximum_samples_per_batch", "\n", "\n", "batches", ":", "List", "[", "List", "[", "Instance", "]", "]", "=", "[", "]", "\n", "batch", ":", "List", "[", "Instance", "]", "=", "[", "]", "\n", "padding_length", "=", "-", "1", "\n", "\n", "excess", ".", "extend", "(", "batch_instances", ")", "\n", "while", "excess", ":", "\n", "            ", "instance", "=", "excess", ".", "popleft", "(", ")", "\n", "\n", "if", "self", ".", "vocab", "is", "not", "None", ":", "\n", "# we index here to ensure that shape information is available,", "\n", "# as in some cases (with self._maximum_samples_per_batch)", "\n", "# we need access to shaping information before batches are constructed)", "\n", "                ", "instance", ".", "index_fields", "(", "self", ".", "vocab", ")", "\n", "", "field_lengths", "=", "instance", ".", "get_padding_lengths", "(", ")", "\n", "for", "lengths", "in", "field_lengths", ".", "values", "(", ")", ":", "\n", "                ", "try", ":", "\n", "                    ", "padding_length", "=", "max", "(", "padding_length", ",", "lengths", "[", "key", "]", ")", "\n", "", "except", "KeyError", ":", "\n", "                    ", "pass", "\n", "", "", "assert", "padding_length", ">", "-", "1", ",", "(", "\n", "f\"Padding key {key} from \"", "\n", "f\"maximum_samples_per_batch not found! Possible padding \"", "\n", "f\"keys: {[k for l in field_lengths.values() for k in l]}\"", "\n", ")", "\n", "\n", "proposed_batch_size", "=", "len", "(", "batch", ")", "+", "1", "\n", "\n", "# Adding the current instance would exceed the batch size or sample size.", "\n", "if", "(", "\n", "proposed_batch_size", ">=", "self", ".", "_batch_size", "\n", "or", "padding_length", "*", "proposed_batch_size", ">", "limit", "\n", ")", ":", "\n", "# Output the already existing batch", "\n", "                ", "batches", ".", "append", "(", "batch", ")", "\n", "\n", "# Put the current instance back, reset state.", "\n", "excess", ".", "appendleft", "(", "instance", ")", "\n", "batch", "=", "[", "]", "\n", "padding_length", "=", "-", "1", "\n", "", "else", ":", "\n", "                ", "batch", ".", "append", "(", "instance", ")", "\n", "\n", "# Keep the current batch as excess.", "\n", "", "", "excess", ".", "extend", "(", "batch", ")", "\n", "\n", "return", "batches", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.iterators.data_iterator.DataIterator.get_num_batches": [[311, 325], ["allennlp.common.util.is_lazy", "math.ceil", "math.ceil", "len", "allennlp.common.util.ensure_list"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.util.is_lazy", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.util.ensure_list"], ["", "def", "get_num_batches", "(", "self", ",", "instances", ":", "Iterable", "[", "Instance", "]", ")", "->", "int", ":", "\n", "        ", "\"\"\"\n        Returns the number of batches that `dataset` will be split into; if you want to track\n        progress through the batch with the generator produced by `__call__`, this could be\n        useful.\n        \"\"\"", "\n", "if", "is_lazy", "(", "instances", ")", "and", "self", ".", "_instances_per_epoch", "is", "None", ":", "\n", "# Unable to compute num batches, so just return 1.", "\n", "            ", "return", "1", "\n", "", "elif", "self", ".", "_instances_per_epoch", "is", "not", "None", ":", "\n", "            ", "return", "math", ".", "ceil", "(", "self", ".", "_instances_per_epoch", "/", "self", ".", "_batch_size", ")", "\n", "", "else", ":", "\n", "# Not lazy, so can compute the list length.", "\n", "            ", "return", "math", ".", "ceil", "(", "len", "(", "ensure_list", "(", "instances", ")", ")", "/", "self", ".", "_batch_size", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.iterators.data_iterator.DataIterator._create_batches": [[326, 333], ["None"], "methods", ["None"], ["", "", "def", "_create_batches", "(", "\n", "self", ",", "instances", ":", "Iterable", "[", "Instance", "]", ",", "shuffle", ":", "bool", "\n", ")", "->", "Iterable", "[", "Batch", "]", ":", "\n", "        ", "\"\"\"\n        This method should return one epoch worth of batches.\n        \"\"\"", "\n", "raise", "NotImplementedError", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.iterators.data_iterator.DataIterator.index_with": [[334, 336], ["None"], "methods", ["None"], ["", "def", "index_with", "(", "self", ",", "vocab", ":", "Vocabulary", ")", ":", "\n", "        ", "self", ".", "vocab", "=", "vocab", "\n", "", "", ""]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.iterators.data_iterator.add_epoch_number": [[22, 29], ["allennlp.data.fields.MetadataField"], "function", ["None"], ["def", "add_epoch_number", "(", "batch", ":", "Batch", ",", "epoch", ":", "int", ")", "->", "Batch", ":", "\n", "    ", "\"\"\"\n    Add the epoch number to the batch instances as a MetadataField.\n    \"\"\"", "\n", "for", "instance", "in", "batch", ".", "instances", ":", "\n", "        ", "instance", ".", "fields", "[", "\"epoch_num\"", "]", "=", "MetadataField", "(", "epoch", ")", "\n", "", "return", "batch", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.fields.adjacency_field.AdjacencyField.__init__": [[49, 84], ["adjacency_field.AdjacencyField._maybe_warn_for_namespace", "sequence_field.sequence_length", "len", "len", "allennlp.common.checks.ConfigurationError", "all", "allennlp.common.checks.ConfigurationError", "allennlp.common.checks.ConfigurationError", "set", "len", "len"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.fields.multilabel_field.MultiLabelField._maybe_warn_for_namespace", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.fields.text_field.TextField.sequence_length"], ["def", "__init__", "(", "\n", "self", ",", "\n", "indices", ":", "List", "[", "Tuple", "[", "int", ",", "int", "]", "]", ",", "\n", "sequence_field", ":", "SequenceField", ",", "\n", "labels", ":", "List", "[", "str", "]", "=", "None", ",", "\n", "label_namespace", ":", "str", "=", "\"labels\"", ",", "\n", "padding_value", ":", "int", "=", "-", "1", ",", "\n", ")", "->", "None", ":", "\n", "        ", "self", ".", "indices", "=", "indices", "\n", "self", ".", "labels", "=", "labels", "\n", "self", ".", "sequence_field", "=", "sequence_field", "\n", "self", ".", "_label_namespace", "=", "label_namespace", "\n", "self", ".", "_padding_value", "=", "padding_value", "\n", "self", ".", "_indexed_labels", ":", "List", "[", "int", "]", "=", "None", "\n", "\n", "self", ".", "_maybe_warn_for_namespace", "(", "label_namespace", ")", "\n", "field_length", "=", "sequence_field", ".", "sequence_length", "(", ")", "\n", "\n", "if", "len", "(", "set", "(", "indices", ")", ")", "!=", "len", "(", "indices", ")", ":", "\n", "            ", "raise", "ConfigurationError", "(", "f\"Indices must be unique, but found {indices}\"", ")", "\n", "\n", "", "if", "not", "all", "(", "\n", "[", "\n", "0", "<=", "index", "[", "1", "]", "<", "field_length", "and", "0", "<=", "index", "[", "0", "]", "<", "field_length", "\n", "for", "index", "in", "indices", "\n", "]", "\n", ")", ":", "\n", "            ", "raise", "ConfigurationError", "(", "\n", "f\"Label indices and sequence length \"", "\n", "f\"are incompatible: {indices} and {field_length}\"", "\n", ")", "\n", "\n", "", "if", "labels", "is", "not", "None", "and", "len", "(", "indices", ")", "!=", "len", "(", "labels", ")", ":", "\n", "            ", "raise", "ConfigurationError", "(", "\n", "f\"Labelled indices were passed, but their lengths do not match: \"", "\n", "f\" {labels}, {indices}\"", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.fields.adjacency_field.AdjacencyField._maybe_warn_for_namespace": [[87, 101], ["adjacency_field.AdjacencyField._label_namespace.endswith", "adjacency_field.AdjacencyField._label_namespace.endswith", "logger.warning", "adjacency_field.AdjacencyField._already_warned_namespaces.add"], "methods", ["None"], ["", "", "def", "_maybe_warn_for_namespace", "(", "self", ",", "label_namespace", ":", "str", ")", "->", "None", ":", "\n", "        ", "if", "not", "(", "\n", "self", ".", "_label_namespace", ".", "endswith", "(", "\"labels\"", ")", "\n", "or", "self", ".", "_label_namespace", ".", "endswith", "(", "\"tags\"", ")", "\n", ")", ":", "\n", "            ", "if", "label_namespace", "not", "in", "self", ".", "_already_warned_namespaces", ":", "\n", "                ", "logger", ".", "warning", "(", "\n", "\"Your label namespace was '%s'. We recommend you use a namespace \"", "\n", "\"ending with 'labels' or 'tags', so we don't add UNK and PAD tokens by \"", "\n", "\"default to your vocabulary.  See documentation for \"", "\n", "\"`non_padded_namespaces` parameter in Vocabulary.\"", ",", "\n", "self", ".", "_label_namespace", ",", "\n", ")", "\n", "self", ".", "_already_warned_namespaces", ".", "add", "(", "label_namespace", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.fields.adjacency_field.AdjacencyField.count_vocab_items": [[102, 107], ["None"], "methods", ["None"], ["", "", "", "@", "overrides", "\n", "def", "count_vocab_items", "(", "self", ",", "counter", ":", "Dict", "[", "str", ",", "Dict", "[", "str", ",", "int", "]", "]", ")", ":", "\n", "        ", "if", "self", ".", "_indexed_labels", "is", "None", "and", "self", ".", "labels", "is", "not", "None", ":", "\n", "            ", "for", "label", "in", "self", ".", "labels", ":", "\n", "                ", "counter", "[", "self", ".", "_label_namespace", "]", "[", "label", "]", "+=", "1", "# type: ignore", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.fields.adjacency_field.AdjacencyField.index": [[108, 114], ["vocab.get_token_index"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.data.vocabulary.Vocabulary.get_token_index"], ["", "", "", "@", "overrides", "\n", "def", "index", "(", "self", ",", "vocab", ":", "Vocabulary", ")", ":", "\n", "        ", "if", "self", ".", "labels", "is", "not", "None", ":", "\n", "            ", "self", ".", "_indexed_labels", "=", "[", "\n", "vocab", ".", "get_token_index", "(", "label", ",", "self", ".", "_label_namespace", ")", "\n", "for", "label", "in", "self", ".", "labels", "\n", "]", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.fields.adjacency_field.AdjacencyField.get_padding_lengths": [[116, 119], ["adjacency_field.AdjacencyField.sequence_field.sequence_length"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.fields.text_field.TextField.sequence_length"], ["", "", "@", "overrides", "\n", "def", "get_padding_lengths", "(", "self", ")", "->", "Dict", "[", "str", ",", "int", "]", ":", "\n", "        ", "return", "{", "\"num_tokens\"", ":", "self", ".", "sequence_field", ".", "sequence_length", "(", ")", "}", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.fields.adjacency_field.AdjacencyField.as_tensor": [[120, 131], ["zip", "torch.ones", "range", "len"], "methods", ["None"], ["", "@", "overrides", "\n", "def", "as_tensor", "(", "self", ",", "padding_lengths", ":", "Dict", "[", "str", ",", "int", "]", ")", "->", "torch", ".", "Tensor", ":", "\n", "        ", "desired_num_tokens", "=", "padding_lengths", "[", "\"num_tokens\"", "]", "\n", "tensor", "=", "(", "\n", "torch", ".", "ones", "(", "desired_num_tokens", ",", "desired_num_tokens", ")", "*", "self", ".", "_padding_value", "\n", ")", "\n", "labels", "=", "self", ".", "_indexed_labels", "or", "[", "1", "for", "_", "in", "range", "(", "len", "(", "self", ".", "indices", ")", ")", "]", "\n", "\n", "for", "index", ",", "label", "in", "zip", "(", "self", ".", "indices", ",", "labels", ")", ":", "\n", "            ", "tensor", "[", "index", "]", "=", "label", "\n", "", "return", "tensor", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.fields.adjacency_field.AdjacencyField.empty_field": [[132, 143], ["AdjacencyField.AdjacencyField", "AdjacencyField.AdjacencyField.sequence_field.empty_field"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.fields.text_field.TextField.empty_field"], ["", "@", "overrides", "\n", "def", "empty_field", "(", "self", ")", "->", "\"AdjacencyField\"", ":", "\n", "\n", "# The empty_list here is needed for mypy", "\n", "        ", "empty_list", ":", "List", "[", "Tuple", "[", "int", ",", "int", "]", "]", "=", "[", "]", "\n", "adjacency_field", "=", "AdjacencyField", "(", "\n", "empty_list", ",", "\n", "self", ".", "sequence_field", ".", "empty_field", "(", ")", ",", "\n", "padding_value", "=", "self", ".", "_padding_value", ",", "\n", ")", "\n", "return", "adjacency_field", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.fields.adjacency_field.AdjacencyField.__str__": [[144, 154], ["adjacency_field.AdjacencyField.sequence_field.sequence_length", "textwrap.wrap", "textwrap.wrap", "repr", "repr"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.fields.text_field.TextField.sequence_length"], ["", "def", "__str__", "(", "self", ")", "->", "str", ":", "\n", "        ", "length", "=", "self", ".", "sequence_field", ".", "sequence_length", "(", ")", "\n", "formatted_labels", "=", "\"\"", ".", "join", "(", "\n", "[", "\"\\t\\t\"", "+", "labels", "+", "\"\\n\"", "for", "labels", "in", "textwrap", ".", "wrap", "(", "repr", "(", "self", ".", "labels", ")", ",", "100", ")", "]", "\n", ")", "\n", "formatted_indices", "=", "\"\"", ".", "join", "(", "\n", "[", "\"\\t\\t\"", "+", "index", "+", "\"\\n\"", "for", "index", "in", "textwrap", ".", "wrap", "(", "repr", "(", "self", ".", "indices", ")", ",", "100", ")", "]", "\n", ")", "\n", "return", "(", "\n", "f\"AdjacencyField of length {length}\\n\"", "\n", "f\"\\t\\twith indices:\\n {formatted_indices}\\n\"", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.fields.sequence_label_field.SequenceLabelField.__init__": [[48, 76], ["sequence_label_field.SequenceLabelField._maybe_warn_for_namespace", "all", "len", "sequence_field.sequence_length", "allennlp.common.checks.ConfigurationError", "isinstance", "all", "allennlp.common.checks.ConfigurationError", "len", "sequence_field.sequence_length", "isinstance", "type"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.fields.multilabel_field.MultiLabelField._maybe_warn_for_namespace", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.fields.text_field.TextField.sequence_length", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.fields.text_field.TextField.sequence_length"], ["def", "__init__", "(", "\n", "self", ",", "\n", "labels", ":", "Union", "[", "List", "[", "str", "]", ",", "List", "[", "int", "]", "]", ",", "\n", "sequence_field", ":", "SequenceField", ",", "\n", "label_namespace", ":", "str", "=", "\"labels\"", ",", "\n", ")", "->", "None", ":", "\n", "        ", "self", ".", "labels", "=", "labels", "\n", "self", ".", "sequence_field", "=", "sequence_field", "\n", "self", ".", "_label_namespace", "=", "label_namespace", "\n", "self", ".", "_indexed_labels", "=", "None", "\n", "self", ".", "_maybe_warn_for_namespace", "(", "label_namespace", ")", "\n", "if", "len", "(", "labels", ")", "!=", "sequence_field", ".", "sequence_length", "(", ")", ":", "\n", "            ", "raise", "ConfigurationError", "(", "\n", "\"Label length and sequence length \"", "\n", "\"don't match: %d and %d\"", "\n", "%", "(", "len", "(", "labels", ")", ",", "sequence_field", ".", "sequence_length", "(", ")", ")", "\n", ")", "\n", "\n", "", "self", ".", "_skip_indexing", "=", "False", "\n", "if", "all", "(", "[", "isinstance", "(", "x", ",", "int", ")", "for", "x", "in", "labels", "]", ")", ":", "\n", "            ", "self", ".", "_indexed_labels", "=", "labels", "\n", "self", ".", "_skip_indexing", "=", "True", "\n", "\n", "", "elif", "not", "all", "(", "[", "isinstance", "(", "x", ",", "str", ")", "for", "x", "in", "labels", "]", ")", ":", "\n", "            ", "raise", "ConfigurationError", "(", "\n", "\"SequenceLabelFields must be passed either all \"", "\n", "\"strings or all ints. Found labels {} with \"", "\n", "\"types: {}.\"", ".", "format", "(", "labels", ",", "[", "type", "(", "x", ")", "for", "x", "in", "labels", "]", ")", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.fields.sequence_label_field.SequenceLabelField._maybe_warn_for_namespace": [[78, 92], ["sequence_label_field.SequenceLabelField._label_namespace.endswith", "sequence_label_field.SequenceLabelField._label_namespace.endswith", "logger.warning", "sequence_label_field.SequenceLabelField._already_warned_namespaces.add"], "methods", ["None"], ["", "", "def", "_maybe_warn_for_namespace", "(", "self", ",", "label_namespace", ":", "str", ")", "->", "None", ":", "\n", "        ", "if", "not", "(", "\n", "self", ".", "_label_namespace", ".", "endswith", "(", "\"labels\"", ")", "\n", "or", "self", ".", "_label_namespace", ".", "endswith", "(", "\"tags\"", ")", "\n", ")", ":", "\n", "            ", "if", "label_namespace", "not", "in", "self", ".", "_already_warned_namespaces", ":", "\n", "                ", "logger", ".", "warning", "(", "\n", "\"Your label namespace was '%s'. We recommend you use a namespace \"", "\n", "\"ending with 'labels' or 'tags', so we don't add UNK and PAD tokens by \"", "\n", "\"default to your vocabulary.  See documentation for \"", "\n", "\"`non_padded_namespaces` parameter in Vocabulary.\"", ",", "\n", "self", ".", "_label_namespace", ",", "\n", ")", "\n", "self", ".", "_already_warned_namespaces", ".", "add", "(", "label_namespace", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.fields.sequence_label_field.SequenceLabelField.__iter__": [[94, 96], ["iter"], "methods", ["None"], ["", "", "", "def", "__iter__", "(", "self", ")", "->", "Iterator", "[", "Union", "[", "str", ",", "int", "]", "]", ":", "\n", "        ", "return", "iter", "(", "self", ".", "labels", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.fields.sequence_label_field.SequenceLabelField.__getitem__": [[97, 99], ["None"], "methods", ["None"], ["", "def", "__getitem__", "(", "self", ",", "idx", ":", "int", ")", "->", "Union", "[", "str", ",", "int", "]", ":", "\n", "        ", "return", "self", ".", "labels", "[", "idx", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.fields.sequence_label_field.SequenceLabelField.__len__": [[100, 102], ["len"], "methods", ["None"], ["", "def", "__len__", "(", "self", ")", "->", "int", ":", "\n", "        ", "return", "len", "(", "self", ".", "labels", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.fields.sequence_label_field.SequenceLabelField.count_vocab_items": [[103, 108], ["None"], "methods", ["None"], ["", "@", "overrides", "\n", "def", "count_vocab_items", "(", "self", ",", "counter", ":", "Dict", "[", "str", ",", "Dict", "[", "str", ",", "int", "]", "]", ")", ":", "\n", "        ", "if", "self", ".", "_indexed_labels", "is", "None", ":", "\n", "            ", "for", "label", "in", "self", ".", "labels", ":", "\n", "                ", "counter", "[", "self", ".", "_label_namespace", "]", "[", "label", "]", "+=", "1", "# type: ignore", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.fields.sequence_label_field.SequenceLabelField.index": [[109, 115], ["vocab.get_token_index"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.data.vocabulary.Vocabulary.get_token_index"], ["", "", "", "@", "overrides", "\n", "def", "index", "(", "self", ",", "vocab", ":", "Vocabulary", ")", ":", "\n", "        ", "if", "not", "self", ".", "_skip_indexing", ":", "\n", "            ", "self", ".", "_indexed_labels", "=", "[", "\n", "vocab", ".", "get_token_index", "(", "label", ",", "self", ".", "_label_namespace", ")", "# type: ignore", "\n", "for", "label", "in", "self", ".", "labels", "\n", "]", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.fields.sequence_label_field.SequenceLabelField.get_padding_lengths": [[117, 120], ["sequence_label_field.SequenceLabelField.sequence_field.sequence_length"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.fields.text_field.TextField.sequence_length"], ["", "", "@", "overrides", "\n", "def", "get_padding_lengths", "(", "self", ")", "->", "Dict", "[", "str", ",", "int", "]", ":", "\n", "        ", "return", "{", "\"num_tokens\"", ":", "self", ".", "sequence_field", ".", "sequence_length", "(", ")", "}", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.fields.sequence_label_field.SequenceLabelField.as_tensor": [[121, 127], ["allennlp.common.util.pad_sequence_to_length", "torch.LongTensor"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.util.pad_sequence_to_length"], ["", "@", "overrides", "\n", "def", "as_tensor", "(", "self", ",", "padding_lengths", ":", "Dict", "[", "str", ",", "int", "]", ")", "->", "torch", ".", "Tensor", ":", "\n", "        ", "desired_num_tokens", "=", "padding_lengths", "[", "\"num_tokens\"", "]", "\n", "padded_tags", "=", "pad_sequence_to_length", "(", "self", ".", "_indexed_labels", ",", "desired_num_tokens", ")", "\n", "tensor", "=", "torch", ".", "LongTensor", "(", "padded_tags", ")", "\n", "return", "tensor", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.fields.sequence_label_field.SequenceLabelField.empty_field": [[128, 138], ["SequenceLabelField.SequenceLabelField", "SequenceLabelField.SequenceLabelField.sequence_field.empty_field"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.fields.text_field.TextField.empty_field"], ["", "@", "overrides", "\n", "def", "empty_field", "(", "self", ")", "->", "\"SequenceLabelField\"", ":", "\n", "\n", "# The empty_list here is needed for mypy", "\n", "        ", "empty_list", ":", "List", "[", "str", "]", "=", "[", "]", "\n", "sequence_label_field", "=", "SequenceLabelField", "(", "\n", "empty_list", ",", "self", ".", "sequence_field", ".", "empty_field", "(", ")", "\n", ")", "\n", "sequence_label_field", ".", "_indexed_labels", "=", "empty_list", "\n", "return", "sequence_label_field", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.fields.sequence_label_field.SequenceLabelField.__str__": [[139, 146], ["sequence_label_field.SequenceLabelField.sequence_field.sequence_length", "textwrap.wrap", "repr"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.fields.text_field.TextField.sequence_length"], ["", "def", "__str__", "(", "self", ")", "->", "str", ":", "\n", "        ", "length", "=", "self", ".", "sequence_field", ".", "sequence_length", "(", ")", "\n", "formatted_labels", "=", "\"\"", ".", "join", "(", "\n", "[", "\"\\t\\t\"", "+", "labels", "+", "\"\\n\"", "for", "labels", "in", "textwrap", ".", "wrap", "(", "repr", "(", "self", ".", "labels", ")", ",", "100", ")", "]", "\n", ")", "\n", "return", "(", "\n", "f\"SequenceLabelField of length {length} with \"", "\n", "f\"labels:\\n {formatted_labels} \\t\\tin namespace: '{self._label_namespace}'.\"", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.fields.namespace_swapping_field.NamespaceSwappingField.__init__": [[26, 30], ["None"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "source_tokens", ":", "List", "[", "Token", "]", ",", "target_namespace", ":", "str", ")", "->", "None", ":", "\n", "        ", "self", ".", "_source_tokens", "=", "source_tokens", "\n", "self", ".", "_target_namespace", "=", "target_namespace", "\n", "self", ".", "_mapping_array", ":", "List", "[", "int", "]", "=", "None", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.fields.namespace_swapping_field.NamespaceSwappingField.index": [[31, 36], ["vocab.get_token_index"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.data.vocabulary.Vocabulary.get_token_index"], ["", "@", "overrides", "\n", "def", "index", "(", "self", ",", "vocab", ":", "Vocabulary", ")", ":", "\n", "        ", "self", ".", "_mapping_array", "=", "[", "\n", "vocab", ".", "get_token_index", "(", "x", ".", "text", ",", "self", ".", "_target_namespace", ")", "\n", "for", "x", "in", "self", ".", "_source_tokens", "\n", "]", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.fields.namespace_swapping_field.NamespaceSwappingField.get_padding_lengths": [[38, 41], ["len"], "methods", ["None"], ["", "@", "overrides", "\n", "def", "get_padding_lengths", "(", "self", ")", "->", "Dict", "[", "str", ",", "int", "]", ":", "\n", "        ", "return", "{", "\"num_tokens\"", ":", "len", "(", "self", ".", "_source_tokens", ")", "}", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.fields.namespace_swapping_field.NamespaceSwappingField.as_tensor": [[42, 48], ["allennlp.common.util.pad_sequence_to_length", "torch.LongTensor"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.util.pad_sequence_to_length"], ["", "@", "overrides", "\n", "def", "as_tensor", "(", "self", ",", "padding_lengths", ":", "Dict", "[", "str", ",", "int", "]", ")", "->", "torch", ".", "Tensor", ":", "\n", "        ", "desired_length", "=", "padding_lengths", "[", "\"num_tokens\"", "]", "\n", "padded_tokens", "=", "pad_sequence_to_length", "(", "self", ".", "_mapping_array", ",", "desired_length", ")", "\n", "tensor", "=", "torch", ".", "LongTensor", "(", "padded_tokens", ")", "\n", "return", "tensor", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.fields.namespace_swapping_field.NamespaceSwappingField.empty_field": [[49, 52], ["namespace_swapping_field.NamespaceSwappingField"], "methods", ["None"], ["", "@", "overrides", "\n", "def", "empty_field", "(", "self", ")", "->", "\"NamespaceSwappingField\"", ":", "\n", "        ", "return", "NamespaceSwappingField", "(", "[", "]", ",", "self", ".", "_target_namespace", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.fields.sequence_field.SequenceField.sequence_length": [[12, 17], ["None"], "methods", ["None"], ["def", "sequence_length", "(", "self", ")", "->", "int", ":", "\n", "        ", "\"\"\"\n        How many elements are there in this sequence?\n        \"\"\"", "\n", "raise", "NotImplementedError", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.fields.sequence_field.SequenceField.empty_field": [[18, 20], ["None"], "methods", ["None"], ["", "def", "empty_field", "(", "self", ")", "->", "\"SequenceField\"", ":", "\n", "        ", "raise", "NotImplementedError", "\n", "", "", ""]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.fields.label_field.LabelField.__init__": [[46, 69], ["label_field.LabelField._maybe_warn_for_namespace", "isinstance", "allennlp.common.checks.ConfigurationError", "isinstance", "allennlp.common.checks.ConfigurationError", "type"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.fields.multilabel_field.MultiLabelField._maybe_warn_for_namespace"], ["def", "__init__", "(", "\n", "self", ",", "\n", "label", ":", "Union", "[", "str", ",", "int", "]", ",", "\n", "label_namespace", ":", "str", "=", "\"labels\"", ",", "\n", "skip_indexing", ":", "bool", "=", "False", ",", "\n", ")", "->", "None", ":", "\n", "        ", "self", ".", "label", "=", "label", "\n", "self", ".", "_label_namespace", "=", "label_namespace", "\n", "self", ".", "_label_id", "=", "None", "\n", "self", ".", "_maybe_warn_for_namespace", "(", "label_namespace", ")", "\n", "self", ".", "_skip_indexing", "=", "skip_indexing", "\n", "\n", "if", "skip_indexing", ":", "\n", "            ", "if", "not", "isinstance", "(", "label", ",", "int", ")", ":", "\n", "                ", "raise", "ConfigurationError", "(", "\n", "\"In order to skip indexing, your labels must be integers. \"", "\n", "\"Found label = {}\"", ".", "format", "(", "label", ")", "\n", ")", "\n", "", "self", ".", "_label_id", "=", "label", "\n", "", "elif", "not", "isinstance", "(", "label", ",", "str", ")", ":", "\n", "            ", "raise", "ConfigurationError", "(", "\n", "\"LabelFields must be passed a string label if skip_indexing=False. \"", "\n", "\"Found label: {} with type: {}.\"", ".", "format", "(", "label", ",", "type", "(", "label", ")", ")", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.fields.label_field.LabelField._maybe_warn_for_namespace": [[71, 85], ["label_field.LabelField._label_namespace.endswith", "label_field.LabelField._label_namespace.endswith", "logger.warning", "label_field.LabelField._already_warned_namespaces.add"], "methods", ["None"], ["", "", "def", "_maybe_warn_for_namespace", "(", "self", ",", "label_namespace", ":", "str", ")", "->", "None", ":", "\n", "        ", "if", "not", "(", "\n", "self", ".", "_label_namespace", ".", "endswith", "(", "\"labels\"", ")", "\n", "or", "self", ".", "_label_namespace", ".", "endswith", "(", "\"tags\"", ")", "\n", ")", ":", "\n", "            ", "if", "label_namespace", "not", "in", "self", ".", "_already_warned_namespaces", ":", "\n", "                ", "logger", ".", "warning", "(", "\n", "\"Your label namespace was '%s'. We recommend you use a namespace \"", "\n", "\"ending with 'labels' or 'tags', so we don't add UNK and PAD tokens by \"", "\n", "\"default to your vocabulary.  See documentation for \"", "\n", "\"`non_padded_namespaces` parameter in Vocabulary.\"", ",", "\n", "self", ".", "_label_namespace", ",", "\n", ")", "\n", "self", ".", "_already_warned_namespaces", ".", "add", "(", "label_namespace", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.fields.label_field.LabelField.count_vocab_items": [[86, 90], ["None"], "methods", ["None"], ["", "", "", "@", "overrides", "\n", "def", "count_vocab_items", "(", "self", ",", "counter", ":", "Dict", "[", "str", ",", "Dict", "[", "str", ",", "int", "]", "]", ")", ":", "\n", "        ", "if", "self", ".", "_label_id", "is", "None", ":", "\n", "            ", "counter", "[", "self", ".", "_label_namespace", "]", "[", "self", ".", "label", "]", "+=", "1", "# type: ignore", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.fields.label_field.LabelField.index": [[91, 96], ["vocab.get_token_index"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.data.vocabulary.Vocabulary.get_token_index"], ["", "", "@", "overrides", "\n", "def", "index", "(", "self", ",", "vocab", ":", "Vocabulary", ")", ":", "\n", "        ", "if", "not", "self", ".", "_skip_indexing", ":", "\n", "            ", "self", ".", "_label_id", "=", "vocab", ".", "get_token_index", "(", "\n", "self", ".", "label", ",", "self", ".", "_label_namespace", "# type: ignore", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.fields.label_field.LabelField.get_padding_lengths": [[98, 101], ["None"], "methods", ["None"], ["", "", "@", "overrides", "\n", "def", "get_padding_lengths", "(", "self", ")", "->", "Dict", "[", "str", ",", "int", "]", ":", "\n", "        ", "return", "{", "}", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.fields.label_field.LabelField.as_tensor": [[102, 107], ["torch.tensor"], "methods", ["None"], ["", "@", "overrides", "\n", "def", "as_tensor", "(", "self", ",", "padding_lengths", ":", "Dict", "[", "str", ",", "int", "]", ")", "->", "torch", ".", "Tensor", ":", "\n", "\n", "        ", "tensor", "=", "torch", ".", "tensor", "(", "self", ".", "_label_id", ",", "dtype", "=", "torch", ".", "long", ")", "\n", "return", "tensor", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.fields.label_field.LabelField.empty_field": [[108, 111], ["label_field.LabelField"], "methods", ["None"], ["", "@", "overrides", "\n", "def", "empty_field", "(", "self", ")", ":", "\n", "        ", "return", "LabelField", "(", "-", "1", ",", "self", ".", "_label_namespace", ",", "skip_indexing", "=", "True", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.fields.label_field.LabelField.__str__": [[112, 114], ["None"], "methods", ["None"], ["", "def", "__str__", "(", "self", ")", "->", "str", ":", "\n", "        ", "return", "f\"LabelField with label: {self.label} in namespace: '{self._label_namespace}'.'\"", "\n", "", "", ""]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.fields.array_field.ArrayField.__init__": [[17, 26], ["None"], "methods", ["None"], ["def", "__init__", "(", "\n", "self", ",", "\n", "array", ":", "numpy", ".", "ndarray", ",", "\n", "padding_value", ":", "int", "=", "0", ",", "\n", "dtype", ":", "numpy", ".", "dtype", "=", "numpy", ".", "float32", ",", "\n", ")", "->", "None", ":", "\n", "        ", "self", ".", "array", "=", "array", "\n", "self", ".", "padding_value", "=", "padding_value", "\n", "self", ".", "dtype", "=", "dtype", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.fields.array_field.ArrayField.get_padding_lengths": [[27, 31], ["str", "enumerate"], "methods", ["None"], ["", "@", "overrides", "\n", "def", "get_padding_lengths", "(", "self", ")", "->", "Dict", "[", "str", ",", "int", "]", ":", "\n", "        ", "return", "{", "\n", "\"dimension_\"", "+", "str", "(", "i", ")", ":", "shape", "for", "i", ",", "shape", "in", "enumerate", "(", "self", ".", "array", ".", "shape", ")", "\n", "}", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.fields.array_field.ArrayField.as_tensor": [[33, 59], ["numpy.asarray", "list", "tuple", "torch.from_numpy", "len", "len", "range", "numpy.ones", "slice", "len", "range", "len", "len"], "methods", ["None"], ["", "@", "overrides", "\n", "def", "as_tensor", "(", "self", ",", "padding_lengths", ":", "Dict", "[", "str", ",", "int", "]", ")", "->", "torch", ".", "Tensor", ":", "\n", "        ", "max_shape", "=", "[", "\n", "padding_lengths", "[", "\"dimension_{}\"", ".", "format", "(", "i", ")", "]", "\n", "for", "i", "in", "range", "(", "len", "(", "padding_lengths", ")", ")", "\n", "]", "\n", "\n", "# Convert explicitly to an ndarray just in case it's an scalar", "\n", "# (it'd end up not being an ndarray otherwise).", "\n", "# Also, the explicit dtype declaration for `asarray` is necessary for scalars.", "\n", "return_array", "=", "numpy", ".", "asarray", "(", "\n", "numpy", ".", "ones", "(", "max_shape", ",", "dtype", "=", "self", ".", "dtype", ")", "*", "self", ".", "padding_value", ",", "\n", "dtype", "=", "self", ".", "dtype", ",", "\n", ")", "\n", "\n", "# If the tensor has a different shape from the largest tensor, pad dimensions with zeros to", "\n", "# form the right shaped list of slices for insertion into the final tensor.", "\n", "slicing_shape", "=", "list", "(", "self", ".", "array", ".", "shape", ")", "\n", "if", "len", "(", "self", ".", "array", ".", "shape", ")", "<", "len", "(", "max_shape", ")", ":", "\n", "            ", "slicing_shape", "=", "slicing_shape", "+", "[", "\n", "0", "for", "_", "in", "range", "(", "len", "(", "max_shape", ")", "-", "len", "(", "self", ".", "array", ".", "shape", ")", ")", "\n", "]", "\n", "", "slices", "=", "tuple", "(", "[", "slice", "(", "0", ",", "x", ")", "for", "x", "in", "slicing_shape", "]", ")", "\n", "return_array", "[", "slices", "]", "=", "self", ".", "array", "\n", "tensor", "=", "torch", ".", "from_numpy", "(", "return_array", ")", "\n", "return", "tensor", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.fields.array_field.ArrayField.empty_field": [[60, 68], ["array_field.ArrayField", "numpy.array"], "methods", ["None"], ["", "@", "overrides", "\n", "def", "empty_field", "(", "self", ")", ":", "\n", "# Pass the padding_value, so that any outer field, e.g., `ListField[ArrayField]` uses the", "\n", "# same padding_value in the padded ArrayFields", "\n", "        ", "return", "ArrayField", "(", "\n", "numpy", ".", "array", "(", "[", "]", ",", "dtype", "=", "self", ".", "dtype", ")", ",", "\n", "padding_value", "=", "self", ".", "padding_value", ",", "\n", "dtype", "=", "self", ".", "dtype", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.fields.array_field.ArrayField.__str__": [[70, 72], ["None"], "methods", ["None"], ["", "def", "__str__", "(", "self", ")", "->", "str", ":", "\n", "        ", "return", "f\"ArrayField with shape: {self.array.shape} and dtype: {self.dtype}.\"", "\n", "", "", ""]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.fields.multilabel_field.MultiLabelField.__init__": [[53, 89], ["multilabel_field.MultiLabelField._maybe_warn_for_namespace", "all", "allennlp.common.checks.ConfigurationError", "allennlp.common.checks.ConfigurationError", "all", "allennlp.common.checks.ConfigurationError", "all", "allennlp.common.checks.ConfigurationError", "isinstance", "isinstance", "typing.cast"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.fields.multilabel_field.MultiLabelField._maybe_warn_for_namespace"], ["def", "__init__", "(", "\n", "self", ",", "\n", "labels", ":", "Sequence", "[", "Union", "[", "str", ",", "int", "]", "]", ",", "\n", "label_namespace", ":", "str", "=", "\"labels\"", ",", "\n", "skip_indexing", ":", "bool", "=", "False", ",", "\n", "num_labels", ":", "Optional", "[", "int", "]", "=", "None", ",", "\n", ")", "->", "None", ":", "\n", "        ", "self", ".", "labels", "=", "labels", "\n", "self", ".", "_label_namespace", "=", "label_namespace", "\n", "self", ".", "_label_ids", "=", "None", "\n", "self", ".", "_maybe_warn_for_namespace", "(", "label_namespace", ")", "\n", "self", ".", "_num_labels", "=", "num_labels", "\n", "\n", "if", "skip_indexing", "and", "self", ".", "labels", ":", "\n", "            ", "if", "not", "all", "(", "isinstance", "(", "label", ",", "int", ")", "for", "label", "in", "labels", ")", ":", "\n", "                ", "raise", "ConfigurationError", "(", "\n", "\"In order to skip indexing, your labels must be integers. \"", "\n", "\"Found labels = {}\"", ".", "format", "(", "labels", ")", "\n", ")", "\n", "", "if", "not", "num_labels", ":", "\n", "                ", "raise", "ConfigurationError", "(", "\n", "\"In order to skip indexing, num_labels can't be None.\"", "\n", ")", "\n", "\n", "", "if", "not", "all", "(", "cast", "(", "int", ",", "label", ")", "<", "num_labels", "for", "label", "in", "labels", ")", ":", "\n", "                ", "raise", "ConfigurationError", "(", "\n", "\"All labels should be < num_labels. \"", "\n", "\"Found num_labels = {} and labels = {} \"", ".", "format", "(", "num_labels", ",", "labels", ")", "\n", ")", "\n", "\n", "", "self", ".", "_label_ids", "=", "labels", "\n", "", "else", ":", "\n", "            ", "if", "not", "all", "(", "isinstance", "(", "label", ",", "str", ")", "for", "label", "in", "labels", ")", ":", "\n", "                ", "raise", "ConfigurationError", "(", "\n", "\"MultiLabelFields expects string labels if skip_indexing=False. \"", "\n", "\"Found labels: {}\"", ".", "format", "(", "labels", ")", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.fields.multilabel_field.MultiLabelField._maybe_warn_for_namespace": [[91, 102], ["label_namespace.endswith", "label_namespace.endswith", "logger.warning", "multilabel_field.MultiLabelField._already_warned_namespaces.add"], "methods", ["None"], ["", "", "", "def", "_maybe_warn_for_namespace", "(", "self", ",", "label_namespace", ":", "str", ")", "->", "None", ":", "\n", "        ", "if", "not", "(", "label_namespace", ".", "endswith", "(", "\"labels\"", ")", "or", "label_namespace", ".", "endswith", "(", "\"tags\"", ")", ")", ":", "\n", "            ", "if", "label_namespace", "not", "in", "self", ".", "_already_warned_namespaces", ":", "\n", "                ", "logger", ".", "warning", "(", "\n", "\"Your label namespace was '%s'. We recommend you use a namespace \"", "\n", "\"ending with 'labels' or 'tags', so we don't add UNK and PAD tokens by \"", "\n", "\"default to your vocabulary.  See documentation for \"", "\n", "\"`non_padded_namespaces` parameter in Vocabulary.\"", ",", "\n", "self", ".", "_label_namespace", ",", "\n", ")", "\n", "self", ".", "_already_warned_namespaces", ".", "add", "(", "label_namespace", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.fields.multilabel_field.MultiLabelField.count_vocab_items": [[103, 108], ["None"], "methods", ["None"], ["", "", "", "@", "overrides", "\n", "def", "count_vocab_items", "(", "self", ",", "counter", ":", "Dict", "[", "str", ",", "Dict", "[", "str", ",", "int", "]", "]", ")", ":", "\n", "        ", "if", "self", ".", "_label_ids", "is", "None", ":", "\n", "            ", "for", "label", "in", "self", ".", "labels", ":", "\n", "                ", "counter", "[", "self", ".", "_label_namespace", "]", "[", "label", "]", "+=", "1", "# type: ignore", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.fields.multilabel_field.MultiLabelField.index": [[109, 118], ["vocab.get_vocab_size", "vocab.get_token_index"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.data.vocabulary.Vocabulary.get_vocab_size", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.data.vocabulary.Vocabulary.get_token_index"], ["", "", "", "@", "overrides", "\n", "def", "index", "(", "self", ",", "vocab", ":", "Vocabulary", ")", ":", "\n", "        ", "if", "self", ".", "_label_ids", "is", "None", ":", "\n", "            ", "self", ".", "_label_ids", "=", "[", "\n", "vocab", ".", "get_token_index", "(", "label", ",", "self", ".", "_label_namespace", ")", "# type: ignore", "\n", "for", "label", "in", "self", ".", "labels", "\n", "]", "\n", "", "if", "not", "self", ".", "_num_labels", ":", "\n", "            ", "self", ".", "_num_labels", "=", "vocab", ".", "get_vocab_size", "(", "self", ".", "_label_namespace", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.fields.multilabel_field.MultiLabelField.get_padding_lengths": [[119, 122], ["None"], "methods", ["None"], ["", "", "@", "overrides", "\n", "def", "get_padding_lengths", "(", "self", ")", "->", "Dict", "[", "str", ",", "int", "]", ":", "\n", "        ", "return", "{", "}", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.fields.multilabel_field.MultiLabelField.as_tensor": [[123, 130], ["torch.zeros", "torch.zeros.scatter_", "torch.LongTensor"], "methods", ["None"], ["", "@", "overrides", "\n", "def", "as_tensor", "(", "self", ",", "padding_lengths", ":", "Dict", "[", "str", ",", "int", "]", ")", "->", "torch", ".", "Tensor", ":", "\n", "        ", "tensor", "=", "torch", ".", "zeros", "(", "self", ".", "_num_labels", ",", "dtype", "=", "torch", ".", "long", ")", "# vector of zeros", "\n", "if", "self", ".", "_label_ids", ":", "\n", "            ", "tensor", ".", "scatter_", "(", "0", ",", "torch", ".", "LongTensor", "(", "self", ".", "_label_ids", ")", ",", "1", ")", "\n", "\n", "", "return", "tensor", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.fields.multilabel_field.MultiLabelField.empty_field": [[131, 135], ["multilabel_field.MultiLabelField"], "methods", ["None"], ["", "@", "overrides", "\n", "def", "empty_field", "(", "self", ")", ":", "\n", "        ", "return", "MultiLabelField", "(", "\n", "[", "]", ",", "self", ".", "_label_namespace", ",", "skip_indexing", "=", "True", ",", "num_labels", "=", "self", ".", "_num_labels", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.fields.multilabel_field.MultiLabelField.__str__": [[137, 139], ["None"], "methods", ["None"], ["", "def", "__str__", "(", "self", ")", "->", "str", ":", "\n", "        ", "return", "f\"MultiLabelField with labels: {self.labels} in namespace: '{self._label_namespace}'.'\"", "\n", "", "", ""]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.fields.field.Field.count_vocab_items": [[31, 55], ["None"], "methods", ["None"], ["def", "count_vocab_items", "(", "self", ",", "counter", ":", "Dict", "[", "str", ",", "Dict", "[", "str", ",", "int", "]", "]", ")", ":", "\n", "        ", "\"\"\"\n        If there are strings in this field that need to be converted into integers through a\n        :class:`Vocabulary`, here is where we count them, to determine which tokens are in or out\n        of the vocabulary.\n\n        If your `Field` does not have any strings that need to be converted into indices, you do\n        not need to implement this method.\n\n        A note on this `counter`: because `Fields` can represent conceptually different things,\n        we separate the vocabulary items by `namespaces`.  This way, we can use a single shared\n        mechanism to handle all mappings from strings to integers in all fields, while keeping\n        words in a `TextField` from sharing the same ids with labels in a `LabelField` (e.g.,\n        \"entailment\" or \"contradiction\" are labels in an entailment task)\n\n        Additionally, a single `Field` might want to use multiple namespaces - `TextFields` can\n        be represented as a combination of word ids and character ids, and you don't want words and\n        characters to share the same vocabulary - \"a\" as a word should get a different id from \"a\"\n        as a character, and the vocabulary sizes of words and characters are very different.\n\n        Because of this, the first key in the `counter` object is a `namespace`, like \"tokens\",\n        \"token_characters\", \"tags\", or \"labels\", and the second key is the actual vocabulary item.\n        \"\"\"", "\n", "pass", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.fields.field.Field.index": [[56, 65], ["None"], "methods", ["None"], ["", "def", "index", "(", "self", ",", "vocab", ":", "Vocabulary", ")", ":", "\n", "        ", "\"\"\"\n        Given a :class:`Vocabulary`, converts all strings in this field into (typically) integers.\n        This `modifies` the `Field` object, it does not return anything.\n\n        If your `Field` does not have any strings that need to be converted into indices, you do\n        not need to implement this method.\n        \"\"\"", "\n", "pass", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.fields.field.Field.get_padding_lengths": [[66, 76], ["None"], "methods", ["None"], ["", "def", "get_padding_lengths", "(", "self", ")", "->", "Dict", "[", "str", ",", "int", "]", ":", "\n", "        ", "\"\"\"\n        If there are things in this field that need padding, note them here.  In order to pad a\n        batch of instance, we get all of the lengths from the batch, take the max, and pad\n        everything to that length (or use a pre-specified maximum length).  The return value is a\n        dictionary mapping keys to lengths, like {'num_tokens': 13}.\n\n        This is always called after :func:`index`.\n        \"\"\"", "\n", "raise", "NotImplementedError", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.fields.field.Field.as_tensor": [[77, 91], ["None"], "methods", ["None"], ["", "def", "as_tensor", "(", "self", ",", "padding_lengths", ":", "Dict", "[", "str", ",", "int", "]", ")", "->", "DataArray", ":", "\n", "        ", "\"\"\"\n        Given a set of specified padding lengths, actually pad the data in this field and return a\n        torch Tensor (or a more complex data structure) of the correct shape.  We also take a\n        couple of parameters that are important when constructing torch Tensors.\n\n        # Parameters\n\n        padding_lengths : `Dict[str, int]`\n            This dictionary will have the same keys that were produced in\n            :func:`get_padding_lengths`.  The values specify the lengths to use when padding each\n            relevant dimension, aggregated across all instances in a batch.\n        \"\"\"", "\n", "raise", "NotImplementedError", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.fields.field.Field.empty_field": [[92, 104], ["None"], "methods", ["None"], ["", "def", "empty_field", "(", "self", ")", "->", "\"Field\"", ":", "\n", "        ", "\"\"\"\n        So that `ListField` can pad the number of fields in a list (e.g., the number of answer\n        option `TextFields`), we need a representation of an empty field of each type.  This\n        returns that.  This will only ever be called when we're to the point of calling\n        :func:`as_tensor`, so you don't need to worry about `get_padding_lengths`,\n        `count_vocab_items`, etc., being called on this empty field.\n\n        We make this an instance method instead of a static method so that if there is any state\n        in the Field, we can copy it over (e.g., the token indexers in `TextField`).\n        \"\"\"", "\n", "raise", "NotImplementedError", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.fields.field.Field.batch_tensors": [[105, 118], ["torch.stack"], "methods", ["None"], ["", "def", "batch_tensors", "(", "self", ",", "tensor_list", ":", "List", "[", "DataArray", "]", ")", "->", "DataArray", ":", "# type: ignore", "\n", "        ", "\"\"\"\n        Takes the output of `Field.as_tensor()` from a list of `Instances` and merges it into\n        one batched tensor for this `Field`.  The default implementation here in the base class\n        handles cases where `as_tensor` returns a single torch tensor per instance.  If your\n        subclass returns something other than this, you need to override this method.\n\n        This operation does not modify `self`, but in some cases we need the information\n        contained in `self` in order to perform the batching, so this is an instance method, not\n        a class method.\n        \"\"\"", "\n", "\n", "return", "torch", ".", "stack", "(", "tensor_list", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.fields.field.Field.__eq__": [[119, 123], ["isinstance"], "methods", ["None"], ["", "def", "__eq__", "(", "self", ",", "other", ")", "->", "bool", ":", "\n", "        ", "if", "isinstance", "(", "self", ",", "other", ".", "__class__", ")", ":", "\n", "            ", "return", "self", ".", "__dict__", "==", "other", ".", "__dict__", "\n", "", "return", "NotImplemented", "\n", "", "", ""]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.fields.metadata_field.MetadataField.__init__": [[27, 29], ["None"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "metadata", ":", "Any", ")", "->", "None", ":", "\n", "        ", "self", ".", "metadata", "=", "metadata", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.fields.metadata_field.MetadataField.__getitem__": [[30, 35], ["TypeError"], "methods", ["None"], ["", "def", "__getitem__", "(", "self", ",", "key", ":", "str", ")", "->", "Any", ":", "\n", "        ", "try", ":", "\n", "            ", "return", "self", ".", "metadata", "[", "key", "]", "# type: ignore", "\n", "", "except", "TypeError", ":", "\n", "            ", "raise", "TypeError", "(", "\"your metadata is not a dict\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.fields.metadata_field.MetadataField.__iter__": [[36, 41], ["iter", "TypeError"], "methods", ["None"], ["", "", "def", "__iter__", "(", "self", ")", ":", "\n", "        ", "try", ":", "\n", "            ", "return", "iter", "(", "self", ".", "metadata", ")", "\n", "", "except", "TypeError", ":", "\n", "            ", "raise", "TypeError", "(", "\"your metadata is not iterable\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.fields.metadata_field.MetadataField.__len__": [[42, 47], ["len", "TypeError"], "methods", ["None"], ["", "", "def", "__len__", "(", "self", ")", ":", "\n", "        ", "try", ":", "\n", "            ", "return", "len", "(", "self", ".", "metadata", ")", "\n", "", "except", "TypeError", ":", "\n", "            ", "raise", "TypeError", "(", "\"your metadata has no length\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.fields.metadata_field.MetadataField.get_padding_lengths": [[48, 51], ["None"], "methods", ["None"], ["", "", "@", "overrides", "\n", "def", "get_padding_lengths", "(", "self", ")", "->", "Dict", "[", "str", ",", "int", "]", ":", "\n", "        ", "return", "{", "}", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.fields.metadata_field.MetadataField.as_tensor": [[52, 56], ["None"], "methods", ["None"], ["", "@", "overrides", "\n", "def", "as_tensor", "(", "self", ",", "padding_lengths", ":", "Dict", "[", "str", ",", "int", "]", ")", "->", "DataArray", ":", "\n", "\n", "        ", "return", "self", ".", "metadata", "# type: ignore", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.fields.metadata_field.MetadataField.empty_field": [[57, 60], ["metadata_field.MetadataField"], "methods", ["None"], ["", "@", "overrides", "\n", "def", "empty_field", "(", "self", ")", "->", "\"MetadataField\"", ":", "\n", "        ", "return", "MetadataField", "(", "None", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.fields.metadata_field.MetadataField.batch_tensors": [[61, 64], ["None"], "methods", ["None"], ["", "@", "overrides", "\n", "def", "batch_tensors", "(", "self", ",", "tensor_list", ":", "List", "[", "DataArray", "]", ")", "->", "List", "[", "DataArray", "]", ":", "# type: ignore", "\n", "        ", "return", "tensor_list", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.fields.metadata_field.MetadataField.__str__": [[65, 67], ["None"], "methods", ["None"], ["", "def", "__str__", "(", "self", ")", "->", "str", ":", "\n", "        ", "return", "f\"MetadataField (print field.metadata to see specific information).\"", "\n", "", "", ""]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.fields.list_field.ListField.__init__": [[27, 34], ["len", "str"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "field_list", ":", "List", "[", "Field", "]", ")", "->", "None", ":", "\n", "        ", "field_class_set", "=", "{", "field", ".", "__class__", "for", "field", "in", "field_list", "}", "\n", "assert", "(", "\n", "len", "(", "field_class_set", ")", "==", "1", "\n", ")", ",", "\"ListFields must contain a single field type, found \"", "+", "str", "(", "field_class_set", ")", "\n", "# Not sure why mypy has a hard time with this type...", "\n", "self", ".", "field_list", ":", "List", "[", "Field", "]", "=", "field_list", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.fields.list_field.ListField.__iter__": [[36, 38], ["iter"], "methods", ["None"], ["", "def", "__iter__", "(", "self", ")", "->", "Iterator", "[", "Field", "]", ":", "\n", "        ", "return", "iter", "(", "self", ".", "field_list", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.fields.list_field.ListField.__getitem__": [[39, 41], ["None"], "methods", ["None"], ["", "def", "__getitem__", "(", "self", ",", "idx", ":", "int", ")", "->", "Field", ":", "\n", "        ", "return", "self", ".", "field_list", "[", "idx", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.fields.list_field.ListField.__len__": [[42, 44], ["len"], "methods", ["None"], ["", "def", "__len__", "(", "self", ")", "->", "int", ":", "\n", "        ", "return", "len", "(", "self", ".", "field_list", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.fields.list_field.ListField.count_vocab_items": [[45, 49], ["field.count_vocab_items"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.fields.text_field.TextField.count_vocab_items"], ["", "@", "overrides", "\n", "def", "count_vocab_items", "(", "self", ",", "counter", ":", "Dict", "[", "str", ",", "Dict", "[", "str", ",", "int", "]", "]", ")", ":", "\n", "        ", "for", "field", "in", "self", ".", "field_list", ":", "\n", "            ", "field", ".", "count_vocab_items", "(", "counter", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.fields.list_field.ListField.index": [[50, 54], ["field.index"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.fields.text_field.TextField.index"], ["", "", "@", "overrides", "\n", "def", "index", "(", "self", ",", "vocab", ":", "Vocabulary", ")", ":", "\n", "        ", "for", "field", "in", "self", ".", "field_list", ":", "\n", "            ", "field", ".", "index", "(", "vocab", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.fields.list_field.ListField.get_padding_lengths": [[55, 81], ["set", "field.get_padding_lengths", "len", "max", "max", "list", "field_length.keys"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.fields.text_field.TextField.get_padding_lengths"], ["", "", "@", "overrides", "\n", "def", "get_padding_lengths", "(", "self", ")", "->", "Dict", "[", "str", ",", "int", "]", ":", "\n", "        ", "field_lengths", "=", "[", "field", ".", "get_padding_lengths", "(", ")", "for", "field", "in", "self", ".", "field_list", "]", "\n", "padding_lengths", "=", "{", "\"num_fields\"", ":", "len", "(", "self", ".", "field_list", ")", "}", "\n", "\n", "# We take the set of all possible padding keys for all fields, rather", "\n", "# than just a random key, because it is possible for fields to be empty", "\n", "# when we pad ListFields.", "\n", "possible_padding_keys", "=", "[", "\n", "key", "for", "field_length", "in", "field_lengths", "for", "key", "in", "list", "(", "field_length", ".", "keys", "(", ")", ")", "\n", "]", "\n", "\n", "for", "key", "in", "set", "(", "possible_padding_keys", ")", ":", "\n", "# In order to be able to nest ListFields, we need to scope the padding length keys", "\n", "# appropriately, so that nested ListFields don't all use the same \"num_fields\" key.  So", "\n", "# when we construct the dictionary from the list of fields, we add something to the", "\n", "# name, and we remove it when padding the list of fields.", "\n", "            ", "padding_lengths", "[", "\"list_\"", "+", "key", "]", "=", "max", "(", "\n", "x", "[", "key", "]", "if", "key", "in", "x", "else", "0", "for", "x", "in", "field_lengths", "\n", ")", "\n", "\n", "# Set minimum padding length to handle empty list fields.", "\n", "", "for", "padding_key", "in", "padding_lengths", ":", "\n", "            ", "padding_lengths", "[", "padding_key", "]", "=", "max", "(", "padding_lengths", "[", "padding_key", "]", ",", "1", ")", "\n", "\n", "", "return", "padding_lengths", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.fields.list_field.ListField.sequence_length": [[82, 85], ["len"], "methods", ["None"], ["", "@", "overrides", "\n", "def", "sequence_length", "(", "self", ")", "->", "int", ":", "\n", "        ", "return", "len", "(", "self", ".", "field_list", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.fields.list_field.ListField.as_tensor": [[86, 104], ["allennlp.common.util.pad_sequence_to_length", "list_field.ListField.field_list[].batch_tensors", "key.replace", "field.as_tensor", "padding_lengths.items", "key.startswith"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.util.pad_sequence_to_length", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.fields.text_field.TextField.batch_tensors", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.fields.text_field.TextField.as_tensor"], ["", "@", "overrides", "\n", "def", "as_tensor", "(", "self", ",", "padding_lengths", ":", "Dict", "[", "str", ",", "int", "]", ")", "->", "DataArray", ":", "\n", "        ", "padded_field_list", "=", "pad_sequence_to_length", "(", "\n", "self", ".", "field_list", ",", "\n", "padding_lengths", "[", "\"num_fields\"", "]", ",", "\n", "self", ".", "field_list", "[", "0", "]", ".", "empty_field", ",", "\n", ")", "\n", "# Here we're removing the scoping on the padding length keys that we added in", "\n", "# `get_padding_lengths`; see the note there for more detail.", "\n", "child_padding_lengths", "=", "{", "\n", "key", ".", "replace", "(", "\"list_\"", ",", "\"\"", ",", "1", ")", ":", "value", "\n", "for", "key", ",", "value", "in", "padding_lengths", ".", "items", "(", ")", "\n", "if", "key", ".", "startswith", "(", "\"list_\"", ")", "\n", "}", "\n", "padded_fields", "=", "[", "\n", "field", ".", "as_tensor", "(", "child_padding_lengths", ")", "for", "field", "in", "padded_field_list", "\n", "]", "\n", "return", "self", ".", "field_list", "[", "0", "]", ".", "batch_tensors", "(", "padded_fields", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.fields.list_field.ListField.empty_field": [[105, 115], ["list_field.ListField", "list_field.ListField.field_list[].empty_field"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.fields.text_field.TextField.empty_field"], ["", "@", "overrides", "\n", "def", "empty_field", "(", "self", ")", ":", "\n", "# Our \"empty\" list field will actually have a single field in the list, so that we can", "\n", "# correctly construct nested lists.  For example, if we have a type that is", "\n", "# `ListField[ListField[LabelField]]`, we need the top-level `ListField` to know to", "\n", "# construct a `ListField[LabelField]` when it's padding, and the nested `ListField` needs", "\n", "# to know that it's empty objects are `LabelFields`.  Having an \"empty\" list actually have", "\n", "# length one makes this all work out, and we'll always be padding to at least length 1,", "\n", "# anyway.", "\n", "        ", "return", "ListField", "(", "[", "self", ".", "field_list", "[", "0", "]", ".", "empty_field", "(", ")", "]", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.fields.list_field.ListField.batch_tensors": [[116, 120], ["list_field.ListField.field_list[].batch_tensors"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.fields.text_field.TextField.batch_tensors"], ["", "@", "overrides", "\n", "def", "batch_tensors", "(", "self", ",", "tensor_list", ":", "List", "[", "DataArray", "]", ")", "->", "DataArray", ":", "\n", "# We defer to the class we're wrapping in a list to handle the batching.", "\n", "        ", "return", "self", ".", "field_list", "[", "0", "]", ".", "batch_tensors", "(", "tensor_list", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.fields.list_field.ListField.__str__": [[121, 125], ["len"], "methods", ["None"], ["", "def", "__str__", "(", "self", ")", "->", "str", ":", "\n", "        ", "field_class", "=", "self", ".", "field_list", "[", "0", "]", ".", "__class__", ".", "__name__", "\n", "base_string", "=", "f\"ListField of {len(self.field_list)} {field_class}s : \\n\"", "\n", "return", "\" \"", ".", "join", "(", "[", "base_string", "]", "+", "[", "f\"\\t {field} \\n\"", "for", "field", "in", "self", ".", "field_list", "]", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.fields.index_field.IndexField.__init__": [[30, 38], ["isinstance", "allennlp.common.checks.ConfigurationError", "type"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "index", ":", "int", ",", "sequence_field", ":", "SequenceField", ")", "->", "None", ":", "\n", "        ", "self", ".", "sequence_index", "=", "index", "\n", "self", ".", "sequence_field", "=", "sequence_field", "\n", "\n", "if", "not", "isinstance", "(", "index", ",", "int", ")", ":", "\n", "            ", "raise", "ConfigurationError", "(", "\n", "\"IndexFields must be passed integer indices. \"", "\n", "\"Found index: {} with type: {}.\"", ".", "format", "(", "index", ",", "type", "(", "index", ")", ")", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.fields.index_field.IndexField.get_padding_lengths": [[40, 44], ["None"], "methods", ["None"], ["", "", "@", "overrides", "\n", "def", "get_padding_lengths", "(", "self", ")", "->", "Dict", "[", "str", ",", "int", "]", ":", "\n", "\n", "        ", "return", "{", "}", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.fields.index_field.IndexField.as_tensor": [[45, 50], ["torch.LongTensor"], "methods", ["None"], ["", "@", "overrides", "\n", "def", "as_tensor", "(", "self", ",", "padding_lengths", ":", "Dict", "[", "str", ",", "int", "]", ")", "->", "torch", ".", "Tensor", ":", "\n", "\n", "        ", "tensor", "=", "torch", ".", "LongTensor", "(", "[", "self", ".", "sequence_index", "]", ")", "\n", "return", "tensor", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.fields.index_field.IndexField.empty_field": [[51, 54], ["index_field.IndexField", "index_field.IndexField.sequence_field.empty_field"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.fields.text_field.TextField.empty_field"], ["", "@", "overrides", "\n", "def", "empty_field", "(", "self", ")", ":", "\n", "        ", "return", "IndexField", "(", "-", "1", ",", "self", ".", "sequence_field", ".", "empty_field", "(", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.fields.index_field.IndexField.__str__": [[55, 57], ["None"], "methods", ["None"], ["", "def", "__str__", "(", "self", ")", "->", "str", ":", "\n", "        ", "return", "f\"IndexField with index: {self.sequence_index}.\"", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.fields.index_field.IndexField.__eq__": [[58, 63], ["isinstance", "super().__eq__"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.fields.span_field.SpanField.__eq__"], ["", "def", "__eq__", "(", "self", ",", "other", ")", "->", "bool", ":", "\n", "# Allow equality checks to ints that are the sequence index", "\n", "        ", "if", "isinstance", "(", "other", ",", "int", ")", ":", "\n", "            ", "return", "self", ".", "sequence_index", "==", "other", "\n", "", "return", "super", "(", ")", ".", "__eq__", "(", "other", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.fields.span_field.SpanField.__init__": [[27, 49], ["TypeError", "ValueError", "ValueError", "isinstance", "isinstance", "span_field.SpanField.sequence_field.sequence_length", "type", "type", "span_field.SpanField.sequence_field.sequence_length"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.fields.text_field.TextField.sequence_length", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.fields.text_field.TextField.sequence_length"], ["def", "__init__", "(", "\n", "self", ",", "span_start", ":", "int", ",", "span_end", ":", "int", ",", "sequence_field", ":", "SequenceField", "\n", ")", "->", "None", ":", "\n", "        ", "self", ".", "span_start", "=", "span_start", "\n", "self", ".", "span_end", "=", "span_end", "\n", "self", ".", "sequence_field", "=", "sequence_field", "\n", "\n", "if", "not", "isinstance", "(", "span_start", ",", "int", ")", "or", "not", "isinstance", "(", "span_end", ",", "int", ")", ":", "\n", "            ", "raise", "TypeError", "(", "\n", "f\"SpanFields must be passed integer indices. Found span indices: \"", "\n", "f\"({span_start}, {span_end}) with types \"", "\n", "f\"({type(span_start)} {type(span_end)})\"", "\n", ")", "\n", "", "if", "span_start", ">", "span_end", ":", "\n", "            ", "raise", "ValueError", "(", "\n", "f\"span_start must be less than span_end, \"", "\n", "f\"but found ({span_start}, {span_end}).\"", "\n", ")", "\n", "\n", "", "if", "span_end", ">", "self", ".", "sequence_field", ".", "sequence_length", "(", ")", "-", "1", ":", "\n", "            ", "raise", "ValueError", "(", "\n", "f\"span_end must be <= len(sequence_length) - 1, but found \"", "\n", "f\"{span_end} and {self.sequence_field.sequence_length() - 1} respectively.\"", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.fields.span_field.SpanField.get_padding_lengths": [[52, 56], ["None"], "methods", ["None"], ["", "", "@", "overrides", "\n", "def", "get_padding_lengths", "(", "self", ")", "->", "Dict", "[", "str", ",", "int", "]", ":", "\n", "\n", "        ", "return", "{", "}", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.fields.span_field.SpanField.as_tensor": [[57, 62], ["torch.LongTensor"], "methods", ["None"], ["", "@", "overrides", "\n", "def", "as_tensor", "(", "self", ",", "padding_lengths", ":", "Dict", "[", "str", ",", "int", "]", ")", "->", "torch", ".", "Tensor", ":", "\n", "\n", "        ", "tensor", "=", "torch", ".", "LongTensor", "(", "[", "self", ".", "span_start", ",", "self", ".", "span_end", "]", ")", "\n", "return", "tensor", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.fields.span_field.SpanField.empty_field": [[63, 66], ["span_field.SpanField", "span_field.SpanField.sequence_field.empty_field"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.fields.text_field.TextField.empty_field"], ["", "@", "overrides", "\n", "def", "empty_field", "(", "self", ")", ":", "\n", "        ", "return", "SpanField", "(", "-", "1", ",", "-", "1", ",", "self", ".", "sequence_field", ".", "empty_field", "(", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.fields.span_field.SpanField.__str__": [[67, 69], ["None"], "methods", ["None"], ["", "def", "__str__", "(", "self", ")", "->", "str", ":", "\n", "        ", "return", "f\"SpanField with spans: ({self.span_start}, {self.span_end}).\"", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.fields.span_field.SpanField.__eq__": [[70, 74], ["super().__eq__", "isinstance", "len"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.fields.span_field.SpanField.__eq__"], ["", "def", "__eq__", "(", "self", ",", "other", ")", "->", "bool", ":", "\n", "        ", "if", "isinstance", "(", "other", ",", "tuple", ")", "and", "len", "(", "other", ")", "==", "2", ":", "\n", "            ", "return", "other", "==", "(", "self", ".", "span_start", ",", "self", ".", "span_end", ")", "\n", "", "return", "super", "(", ")", ".", "__eq__", "(", "other", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.fields.text_field.TextField.__init__": [[44, 55], ["all", "allennlp.common.checks.ConfigurationError", "isinstance", "type"], "methods", ["None"], ["def", "__init__", "(", "\n", "self", ",", "tokens", ":", "List", "[", "Token", "]", ",", "token_indexers", ":", "Dict", "[", "str", ",", "TokenIndexer", "]", "\n", ")", "->", "None", ":", "\n", "        ", "self", ".", "tokens", "=", "tokens", "\n", "self", ".", "_token_indexers", "=", "token_indexers", "\n", "self", ".", "_indexed_tokens", ":", "Optional", "[", "Dict", "[", "str", ",", "IndexedTokenList", "]", "]", "=", "None", "\n", "\n", "if", "not", "all", "(", "[", "isinstance", "(", "x", ",", "(", "Token", ",", "SpacyToken", ")", ")", "for", "x", "in", "tokens", "]", ")", ":", "\n", "            ", "raise", "ConfigurationError", "(", "\n", "\"TextFields must be passed Tokens. \"", "\n", "\"Found: {} with types {}.\"", ".", "format", "(", "tokens", ",", "[", "type", "(", "x", ")", "for", "x", "in", "tokens", "]", ")", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.fields.text_field.TextField.count_vocab_items": [[57, 62], ["text_field.TextField._token_indexers.values", "indexer.count_vocab_items"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.fields.text_field.TextField.count_vocab_items"], ["", "", "@", "overrides", "\n", "def", "count_vocab_items", "(", "self", ",", "counter", ":", "Dict", "[", "str", ",", "Dict", "[", "str", ",", "int", "]", "]", ")", ":", "\n", "        ", "for", "indexer", "in", "self", ".", "_token_indexers", ".", "values", "(", ")", ":", "\n", "            ", "for", "token", "in", "self", ".", "tokens", ":", "\n", "                ", "indexer", ".", "count_vocab_items", "(", "token", ",", "counter", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.fields.text_field.TextField.index": [[63, 69], ["text_field.TextField._token_indexers.items", "indexer.tokens_to_indices"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.token_indexers.token_characters_indexer.TokenCharactersIndexer.tokens_to_indices"], ["", "", "", "@", "overrides", "\n", "def", "index", "(", "self", ",", "vocab", ":", "Vocabulary", ")", ":", "\n", "        ", "self", ".", "_indexed_tokens", "=", "{", "}", "\n", "for", "indexer_name", ",", "indexer", "in", "self", ".", "_token_indexers", ".", "items", "(", ")", ":", "\n", "            ", "self", ".", "_indexed_tokens", "[", "indexer_name", "]", "=", "indexer", ".", "tokens_to_indices", "(", "\n", "self", ".", "tokens", ",", "vocab", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.fields.text_field.TextField.get_padding_lengths": [[71, 91], ["text_field.TextField._token_indexers.items", "allennlp.common.checks.ConfigurationError", "indexer.get_padding_lengths", "indexer.get_padding_lengths.items"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.fields.text_field.TextField.get_padding_lengths"], ["", "", "@", "overrides", "\n", "def", "get_padding_lengths", "(", "self", ")", "->", "Dict", "[", "str", ",", "int", "]", ":", "\n", "        ", "\"\"\"\n        The `TextField` has a list of `Tokens`, and each `Token` gets converted into arrays by\n        (potentially) several `TokenIndexers`.  This method gets the max length (over tokens)\n        associated with each of these arrays.\n        \"\"\"", "\n", "if", "self", ".", "_indexed_tokens", "is", "None", ":", "\n", "            ", "raise", "ConfigurationError", "(", "\n", "\"You must call .index(vocabulary) on a field before determining padding lengths.\"", "\n", ")", "\n", "\n", "", "padding_lengths", "=", "{", "}", "\n", "for", "indexer_name", ",", "indexer", "in", "self", ".", "_token_indexers", ".", "items", "(", ")", ":", "\n", "            ", "indexer_lengths", "=", "indexer", ".", "get_padding_lengths", "(", "\n", "self", ".", "_indexed_tokens", "[", "indexer_name", "]", "\n", ")", "\n", "for", "key", ",", "length", "in", "indexer_lengths", ".", "items", "(", ")", ":", "\n", "                ", "padding_lengths", "[", "f\"{indexer_name}___{key}\"", "]", "=", "length", "\n", "", "", "return", "padding_lengths", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.fields.text_field.TextField.sequence_length": [[92, 95], ["len"], "methods", ["None"], ["", "@", "overrides", "\n", "def", "sequence_length", "(", "self", ")", "->", "int", ":", "\n", "        ", "return", "len", "(", "self", ".", "tokens", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.fields.text_field.TextField.as_tensor": [[96, 112], ["collections.defaultdict", "padding_lengths.items", "text_field.TextField._token_indexers.items", "key.split", "indexer.as_padded_tensor_dict"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.token_indexers.token_characters_indexer.TokenCharactersIndexer.as_padded_tensor_dict"], ["", "@", "overrides", "\n", "def", "as_tensor", "(", "self", ",", "padding_lengths", ":", "Dict", "[", "str", ",", "int", "]", ")", "->", "Dict", "[", "str", ",", "torch", ".", "Tensor", "]", ":", "\n", "        ", "tensors", "=", "{", "}", "\n", "\n", "indexer_lengths", ":", "Dict", "[", "str", ",", "Dict", "[", "str", ",", "int", "]", "]", "=", "defaultdict", "(", "dict", ")", "\n", "for", "key", ",", "value", "in", "padding_lengths", ".", "items", "(", ")", ":", "\n", "# We want this to crash if the split fails. Should never happen, so I'm not", "\n", "# putting in a check, but if you fail on this line, open a github issue.", "\n", "            ", "indexer_name", ",", "padding_key", "=", "key", ".", "split", "(", "\"___\"", ")", "\n", "indexer_lengths", "[", "indexer_name", "]", "[", "padding_key", "]", "=", "value", "\n", "\n", "", "for", "indexer_name", ",", "indexer", "in", "self", ".", "_token_indexers", ".", "items", "(", ")", ":", "\n", "            ", "tensors", "[", "indexer_name", "]", "=", "indexer", ".", "as_padded_tensor_dict", "(", "\n", "self", ".", "_indexed_tokens", "[", "indexer_name", "]", ",", "indexer_lengths", "[", "indexer_name", "]", "\n", ")", "\n", "", "return", "tensors", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.fields.text_field.TextField.empty_field": [[113, 120], ["TextField.TextField", "TextField.TextField._token_indexers.items", "indexer.get_empty_token_list"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.token_indexers.token_characters_indexer.TokenCharactersIndexer.get_empty_token_list"], ["", "@", "overrides", "\n", "def", "empty_field", "(", "self", ")", ":", "\n", "        ", "text_field", "=", "TextField", "(", "[", "]", ",", "self", ".", "_token_indexers", ")", "\n", "text_field", ".", "_indexed_tokens", "=", "{", "}", "\n", "for", "indexer_name", ",", "indexer", "in", "self", ".", "_token_indexers", ".", "items", "(", ")", ":", "\n", "            ", "text_field", ".", "_indexed_tokens", "[", "indexer_name", "]", "=", "indexer", ".", "get_empty_token_list", "(", ")", "\n", "", "return", "text_field", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.fields.text_field.TextField.batch_tensors": [[121, 138], ["collections.defaultdict", "tensor_dict.items", "allennlp.nn.util.batch_tensor_dicts", "indexer_lists[].append", "indexer_lists.items"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.nn.util.batch_tensor_dicts"], ["", "@", "overrides", "\n", "def", "batch_tensors", "(", "self", ",", "tensor_list", ":", "List", "[", "TextFieldTensors", "]", ")", "->", "TextFieldTensors", ":", "\n", "# This is creating a dict of {token_indexer_name: {token_indexer_outputs: batched_tensor}}", "\n", "# for each token indexer used to index this field.", "\n", "        ", "indexer_lists", ":", "Dict", "[", "str", ",", "List", "[", "Dict", "[", "str", ",", "torch", ".", "Tensor", "]", "]", "]", "=", "defaultdict", "(", "list", ")", "\n", "for", "tensor_dict", "in", "tensor_list", ":", "\n", "            ", "for", "indexer_name", ",", "indexer_output", "in", "tensor_dict", ".", "items", "(", ")", ":", "\n", "                ", "indexer_lists", "[", "indexer_name", "]", ".", "append", "(", "indexer_output", ")", "\n", "", "", "batched_tensors", "=", "{", "\n", "# NOTE(mattg): if an indexer has its own nested structure, rather than one tensor per", "\n", "# argument, then this will break.  If that ever happens, we should move this to an", "\n", "# `indexer.batch_tensors` method, with this logic as the default implementation in the", "\n", "# base class.", "\n", "indexer_name", ":", "util", ".", "batch_tensor_dicts", "(", "indexer_outputs", ")", "\n", "for", "indexer_name", ",", "indexer_outputs", "in", "indexer_lists", ".", "items", "(", ")", "\n", "}", "\n", "return", "batched_tensors", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.fields.text_field.TextField.__str__": [[139, 151], ["text_field.TextField._token_indexers.items", "text_field.TextField.sequence_length", "textwrap.wrap", "repr"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.fields.text_field.TextField.sequence_length"], ["", "def", "__str__", "(", "self", ")", "->", "str", ":", "\n", "        ", "indexers", "=", "{", "\n", "name", ":", "indexer", ".", "__class__", ".", "__name__", "\n", "for", "name", ",", "indexer", "in", "self", ".", "_token_indexers", ".", "items", "(", ")", "\n", "}", "\n", "\n", "# Double tab to indent under the header.", "\n", "formatted_text", "=", "\"\"", ".", "join", "(", "\n", "[", "\"\\t\\t\"", "+", "text", "+", "\"\\n\"", "for", "text", "in", "textwrap", ".", "wrap", "(", "repr", "(", "self", ".", "tokens", ")", ",", "100", ")", "]", "\n", ")", "\n", "return", "(", "\n", "f\"TextField of length {self.sequence_length()} with \"", "\n", "f\"text: \\n {formatted_text} \\t\\tand TokenIndexers : {indexers}\"", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.fields.text_field.TextField.__iter__": [[155, 157], ["iter"], "methods", ["None"], ["", "def", "__iter__", "(", "self", ")", "->", "Iterator", "[", "Token", "]", ":", "\n", "        ", "return", "iter", "(", "self", ".", "tokens", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.fields.text_field.TextField.__getitem__": [[158, 160], ["None"], "methods", ["None"], ["", "def", "__getitem__", "(", "self", ",", "idx", ":", "int", ")", "->", "Token", ":", "\n", "        ", "return", "self", ".", "tokens", "[", "idx", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.fields.text_field.TextField.__len__": [[161, 163], ["len"], "methods", ["None"], ["", "def", "__len__", "(", "self", ")", "->", "int", ":", "\n", "        ", "return", "len", "(", "self", ".", "tokens", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.commands.find_learning_rate.FindLearningRate.add_subparser": [[69, 140], ["parser.add_parser", "parser.add_parser.add_argument", "parser.add_parser.add_argument", "parser.add_parser.add_argument", "parser.add_parser.add_argument", "parser.add_parser.add_argument", "parser.add_parser.add_argument", "parser.add_parser.add_argument", "parser.add_parser.add_argument", "parser.add_parser.add_argument", "parser.add_parser.set_defaults"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.commands.__init__.ArgumentParserWithDefaults.add_argument", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.commands.__init__.ArgumentParserWithDefaults.add_argument", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.commands.__init__.ArgumentParserWithDefaults.add_argument", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.commands.__init__.ArgumentParserWithDefaults.add_argument", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.commands.__init__.ArgumentParserWithDefaults.add_argument", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.commands.__init__.ArgumentParserWithDefaults.add_argument", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.commands.__init__.ArgumentParserWithDefaults.add_argument", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.commands.__init__.ArgumentParserWithDefaults.add_argument", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.commands.__init__.ArgumentParserWithDefaults.add_argument"], ["    ", "@", "overrides", "\n", "def", "add_subparser", "(", "\n", "self", ",", "parser", ":", "argparse", ".", "_SubParsersAction", "\n", ")", "->", "argparse", ".", "ArgumentParser", ":", "\n", "\n", "        ", "description", "=", "\"\"\"Find a learning rate range where loss decreases quickly\n                         for the specified model and dataset.\"\"\"", "\n", "subparser", "=", "parser", ".", "add_parser", "(", "\n", "self", ".", "name", ",", "description", "=", "description", ",", "help", "=", "\"Find a learning rate range.\"", "\n", ")", "\n", "\n", "subparser", ".", "add_argument", "(", "\n", "\"param_path\"", ",", "\n", "type", "=", "str", ",", "\n", "help", "=", "\"path to parameter file describing the model to be trained\"", ",", "\n", ")", "\n", "subparser", ".", "add_argument", "(", "\n", "\"-s\"", ",", "\n", "\"--serialization-dir\"", ",", "\n", "required", "=", "True", ",", "\n", "type", "=", "str", ",", "\n", "help", "=", "\"The directory in which to save results.\"", ",", "\n", ")", "\n", "subparser", ".", "add_argument", "(", "\n", "\"-o\"", ",", "\n", "\"--overrides\"", ",", "\n", "type", "=", "str", ",", "\n", "default", "=", "\"\"", ",", "\n", "help", "=", "\"a JSON structure used to override the experiment configuration\"", ",", "\n", ")", "\n", "subparser", ".", "add_argument", "(", "\n", "\"--start-lr\"", ",", "\n", "type", "=", "float", ",", "\n", "default", "=", "1e-5", ",", "\n", "help", "=", "\"learning rate to start the search\"", ",", "\n", ")", "\n", "subparser", ".", "add_argument", "(", "\n", "\"--end-lr\"", ",", "\n", "type", "=", "float", ",", "\n", "default", "=", "10", ",", "\n", "help", "=", "\"learning rate up to which search is done\"", ",", "\n", ")", "\n", "subparser", ".", "add_argument", "(", "\n", "\"--num-batches\"", ",", "\n", "type", "=", "int", ",", "\n", "default", "=", "100", ",", "\n", "help", "=", "\"number of mini-batches to run learning rate finder\"", ",", "\n", ")", "\n", "subparser", ".", "add_argument", "(", "\n", "\"--stopping-factor\"", ",", "\n", "type", "=", "float", ",", "\n", "default", "=", "None", ",", "\n", "help", "=", "\"stop the search when the current loss exceeds the best loss recorded by \"", "\n", "\"multiple of stopping factor\"", ",", "\n", ")", "\n", "subparser", ".", "add_argument", "(", "\n", "\"--linear\"", ",", "\n", "action", "=", "\"store_true\"", ",", "\n", "help", "=", "\"increase learning rate linearly instead of exponential increase\"", ",", "\n", ")", "\n", "subparser", ".", "add_argument", "(", "\n", "\"-f\"", ",", "\n", "\"--force\"", ",", "\n", "action", "=", "\"store_true\"", ",", "\n", "required", "=", "False", ",", "\n", "help", "=", "\"overwrite the output directory if it exists\"", ",", "\n", ")", "\n", "\n", "subparser", ".", "set_defaults", "(", "func", "=", "find_learning_rate_from_args", ")", "\n", "\n", "return", "subparser", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.commands.find_learning_rate.find_learning_rate_from_args": [[142, 156], ["allennlp.common.Params.from_file", "find_learning_rate.find_learning_rate_model"], "function", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.params.Params.from_file", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.commands.find_learning_rate.find_learning_rate_model"], ["", "", "def", "find_learning_rate_from_args", "(", "args", ":", "argparse", ".", "Namespace", ")", "->", "None", ":", "\n", "    ", "\"\"\"\n    Start learning rate finder for given args\n    \"\"\"", "\n", "params", "=", "Params", ".", "from_file", "(", "args", ".", "param_path", ",", "args", ".", "overrides", ")", "\n", "find_learning_rate_model", "(", "\n", "params", ",", "\n", "args", ".", "serialization_dir", ",", "\n", "start_lr", "=", "args", ".", "start_lr", ",", "\n", "end_lr", "=", "args", ".", "end_lr", ",", "\n", "num_batches", "=", "args", ".", "num_batches", ",", "\n", "linear_steps", "=", "args", ".", "linear", ",", "\n", "stopping_factor", "=", "args", ".", "stopping_factor", ",", "\n", "force", "=", "args", ".", "force", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.commands.find_learning_rate.find_learning_rate_model": [[159, 271], ["allennlp.training.util.create_serialization_dir", "allennlp.common.util.prepare_environment", "params.params.get().get", "allennlp.common.checks.check_for_gpu", "params.params.get", "allennlp.training.util.datasets_from_params", "set", "logger.info", "allennlp.data.Vocabulary.from_params", "allennlp.models.Model.from_params", "allennlp.data.DataIterator.from_params", "DataIterator.from_params.index_with", "params.pop", "params.pop.pop", "Model.from_params.named_parameters", "params.pop.pop", "allennlp.training.TrainerBase.from_params", "logger.info", "find_learning_rate.search_learning_rate", "logger.info", "find_learning_rate._smooth", "find_learning_rate._save_plot", "params.pop", "params.pop", "params.pop", "any", "allennlp.common.checks.ConfigurationError", "os.path.join", "params.params.get", "allennlp.common.checks.ConfigurationError", "params.pop", "parameter.requires_grad_", "re.search", "allennlp.training.util.datasets_from_params.items"], "function", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.training.util.create_serialization_dir", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.util.prepare_environment", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.params.Params.get", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.checks.check_for_gpu", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.params.Params.get", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.training.util.datasets_from_params", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.dataset_readers.multiprocess_dataset_reader.logger.info", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.tokenizers.tokenizer.Tokenizer.from_params", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.tokenizers.tokenizer.Tokenizer.from_params", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.tokenizers.tokenizer.Tokenizer.from_params", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.iterators.data_iterator.DataIterator.index_with", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.params.Params.pop", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.params.Params.pop", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.params.Params.pop", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.tokenizers.tokenizer.Tokenizer.from_params", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.dataset_readers.multiprocess_dataset_reader.logger.info", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.commands.find_learning_rate.search_learning_rate", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.dataset_readers.multiprocess_dataset_reader.logger.info", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.commands.find_learning_rate._smooth", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.commands.find_learning_rate._save_plot", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.params.Params.pop", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.params.Params.pop", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.params.Params.pop", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.dataset_readers.multiprocess_dataset_reader.QIterable.join", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.params.Params.get", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.params.Params.pop", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.nn.beam_search.BeamSearch.search"], ["", "def", "find_learning_rate_model", "(", "\n", "params", ":", "Params", ",", "\n", "serialization_dir", ":", "str", ",", "\n", "start_lr", ":", "float", "=", "1e-5", ",", "\n", "end_lr", ":", "float", "=", "10", ",", "\n", "num_batches", ":", "int", "=", "100", ",", "\n", "linear_steps", ":", "bool", "=", "False", ",", "\n", "stopping_factor", ":", "float", "=", "None", ",", "\n", "force", ":", "bool", "=", "False", ",", "\n", ")", "->", "None", ":", "\n", "    ", "\"\"\"\n    Runs learning rate search for given `num_batches` and saves the results in ``serialization_dir``\n\n    # Parameters\n\n    params : ``Params``\n        A parameter object specifying an AllenNLP Experiment.\n    serialization_dir : ``str``\n        The directory in which to save results.\n    start_lr : ``float``\n        Learning rate to start the search.\n    end_lr : ``float``\n        Learning rate upto which search is done.\n    num_batches : ``int``\n        Number of mini-batches to run Learning rate finder.\n    linear_steps : ``bool``\n        Increase learning rate linearly if False exponentially.\n    stopping_factor : ``float``\n        Stop the search when the current loss exceeds the best loss recorded by\n        multiple of stopping factor. If ``None`` search proceeds till the ``end_lr``\n    force : ``bool``\n        If True and the serialization directory already exists, everything in it will\n        be removed prior to finding the learning rate.\n    \"\"\"", "\n", "create_serialization_dir", "(", "params", ",", "serialization_dir", ",", "recover", "=", "False", ",", "force", "=", "force", ")", "\n", "\n", "prepare_environment", "(", "params", ")", "\n", "\n", "cuda_device", "=", "params", ".", "params", ".", "get", "(", "\"trainer\"", ")", ".", "get", "(", "\"cuda_device\"", ",", "-", "1", ")", "\n", "check_for_gpu", "(", "cuda_device", ")", "\n", "distributed_params", "=", "params", ".", "params", ".", "get", "(", "\"distributed\"", ",", "None", ")", "\n", "# See https://github.com/allenai/allennlp/issues/3658", "\n", "assert", "(", "\n", "not", "distributed_params", "\n", ")", ",", "\"find-lr is not compatible with DistributedDataParallel.\"", "\n", "\n", "all_datasets", "=", "datasets_from_params", "(", "params", ")", "\n", "datasets_for_vocab_creation", "=", "set", "(", "\n", "params", ".", "pop", "(", "\"datasets_for_vocab_creation\"", ",", "all_datasets", ")", "\n", ")", "\n", "\n", "for", "dataset", "in", "datasets_for_vocab_creation", ":", "\n", "        ", "if", "dataset", "not", "in", "all_datasets", ":", "\n", "            ", "raise", "ConfigurationError", "(", "f\"invalid 'dataset_for_vocab_creation' {dataset}\"", ")", "\n", "\n", "", "", "logger", ".", "info", "(", "\n", "\"From dataset instances, %s will be considered for vocabulary creation.\"", ",", "\n", "\", \"", ".", "join", "(", "datasets_for_vocab_creation", ")", ",", "\n", ")", "\n", "vocab", "=", "Vocabulary", ".", "from_params", "(", "\n", "params", ".", "pop", "(", "\"vocabulary\"", ",", "{", "}", ")", ",", "\n", "instances", "=", "(", "\n", "instance", "\n", "for", "key", ",", "dataset", "in", "all_datasets", ".", "items", "(", ")", "\n", "for", "instance", "in", "dataset", "\n", "if", "key", "in", "datasets_for_vocab_creation", "\n", ")", ",", "\n", ")", "\n", "\n", "model", "=", "Model", ".", "from_params", "(", "vocab", "=", "vocab", ",", "params", "=", "params", ".", "pop", "(", "\"model\"", ")", ")", "\n", "iterator", "=", "DataIterator", ".", "from_params", "(", "params", ".", "pop", "(", "\"iterator\"", ")", ")", "\n", "iterator", ".", "index_with", "(", "vocab", ")", "\n", "\n", "train_data", "=", "all_datasets", "[", "\"train\"", "]", "\n", "\n", "trainer_params", "=", "params", ".", "pop", "(", "\"trainer\"", ")", "\n", "\n", "no_grad_regexes", "=", "trainer_params", ".", "pop", "(", "\"no_grad\"", ",", "(", ")", ")", "\n", "for", "name", ",", "parameter", "in", "model", ".", "named_parameters", "(", ")", ":", "\n", "        ", "if", "any", "(", "re", ".", "search", "(", "regex", ",", "name", ")", "for", "regex", "in", "no_grad_regexes", ")", ":", "\n", "            ", "parameter", ".", "requires_grad_", "(", "False", ")", "\n", "\n", "", "", "trainer_choice", "=", "trainer_params", ".", "pop", "(", "\"type\"", ",", "\"default\"", ")", "\n", "if", "trainer_choice", "!=", "\"default\"", ":", "\n", "        ", "raise", "ConfigurationError", "(", "\n", "\"currently find-learning-rate only works with the default Trainer\"", "\n", ")", "\n", "", "trainer", ":", "Trainer", "=", "TrainerBase", ".", "from_params", "(", "# type: ignore", "\n", "model", "=", "model", ",", "\n", "serialization_dir", "=", "serialization_dir", ",", "\n", "iterator", "=", "iterator", ",", "\n", "train_data", "=", "train_data", ",", "\n", "validation_data", "=", "None", ",", "\n", "params", "=", "trainer_params", ",", "\n", "validation_iterator", "=", "None", ",", "\n", ")", "\n", "\n", "logger", ".", "info", "(", "\n", "f\"Starting learning rate search from {start_lr} to {end_lr} in {num_batches} iterations.\"", "\n", ")", "\n", "learning_rates", ",", "losses", "=", "search_learning_rate", "(", "\n", "trainer", ",", "\n", "start_lr", "=", "start_lr", ",", "\n", "end_lr", "=", "end_lr", ",", "\n", "num_batches", "=", "num_batches", ",", "\n", "linear_steps", "=", "linear_steps", ",", "\n", "stopping_factor", "=", "stopping_factor", ",", "\n", ")", "\n", "logger", ".", "info", "(", "f\"Finished learning rate search.\"", ")", "\n", "losses", "=", "_smooth", "(", "losses", ",", "0.98", ")", "\n", "\n", "_save_plot", "(", "learning_rates", ",", "losses", ",", "os", ".", "path", ".", "join", "(", "serialization_dir", ",", "\"lr-losses.png\"", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.commands.find_learning_rate.search_learning_rate": [[273, 358], ["trainer.model.train", "trainer.iterator", "allennlp.common.Tqdm.tqdm", "enumerate", "allennlp.common.checks.ConfigurationError", "trainer.optimizer.zero_grad", "trainer.batch_loss", "loss.detach().cpu().item.backward", "loss.detach().cpu().item.detach().cpu().item", "trainer.rescale_gradients", "trainer.optimizer.step", "learning_rates.append", "losses.append", "logger.info", "loss.detach().cpu().item.detach().cpu", "math.isnan", "loss.detach().cpu().item.detach"], "function", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.training.no_op_trainer.NoOpTrainer.train", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.tqdm.Tqdm.tqdm", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.training.trainer.Trainer.batch_loss", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.callbacks.gradient_norm_and_clip.GradientNormAndClip.rescale_gradients", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.learning_rate_schedulers.slanted_triangular.SlantedTriangular.step", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.dataset_readers.multiprocess_dataset_reader.logger.info"], ["", "def", "search_learning_rate", "(", "\n", "trainer", ":", "Trainer", ",", "\n", "start_lr", ":", "float", "=", "1e-5", ",", "\n", "end_lr", ":", "float", "=", "10", ",", "\n", "num_batches", ":", "int", "=", "100", ",", "\n", "linear_steps", ":", "bool", "=", "False", ",", "\n", "stopping_factor", ":", "float", "=", "None", ",", "\n", ")", "->", "Tuple", "[", "List", "[", "float", "]", ",", "List", "[", "float", "]", "]", ":", "\n", "    ", "\"\"\"\n    Runs training loop on the model using :class:`~allennlp.training.trainer.Trainer`\n    increasing learning rate from ``start_lr`` to ``end_lr`` recording the losses.\n    # Parameters\n\n    trainer: :class:`~allennlp.training.trainer.Trainer`\n    start_lr : ``float``\n        The learning rate to start the search.\n    end_lr : ``float``\n        The learning rate upto which search is done.\n    num_batches : ``int``\n        Number of batches to run the learning rate finder.\n    linear_steps : ``bool``\n        Increase learning rate linearly if False exponentially.\n    stopping_factor : ``float``\n        Stop the search when the current loss exceeds the best loss recorded by\n        multiple of stopping factor. If ``None`` search proceeds till the ``end_lr``\n    # Returns\n\n    (learning_rates, losses) : ``Tuple[List[float], List[float]]``\n        Returns list of learning rates and corresponding losses.\n        Note: The losses are recorded before applying the corresponding learning rate\n    \"\"\"", "\n", "if", "num_batches", "<=", "10", ":", "\n", "        ", "raise", "ConfigurationError", "(", "\n", "\"The number of iterations for learning rate finder should be greater than 10.\"", "\n", ")", "\n", "\n", "", "trainer", ".", "model", ".", "train", "(", ")", "\n", "\n", "train_generator", "=", "trainer", ".", "iterator", "(", "trainer", ".", "train_data", ",", "shuffle", "=", "trainer", ".", "shuffle", ")", "\n", "train_generator_tqdm", "=", "Tqdm", ".", "tqdm", "(", "train_generator", ",", "total", "=", "num_batches", ")", "\n", "\n", "learning_rates", "=", "[", "]", "\n", "losses", "=", "[", "]", "\n", "best", "=", "1e9", "\n", "if", "linear_steps", ":", "\n", "        ", "lr_update_factor", "=", "(", "end_lr", "-", "start_lr", ")", "/", "num_batches", "\n", "", "else", ":", "\n", "        ", "lr_update_factor", "=", "(", "end_lr", "/", "start_lr", ")", "**", "(", "1.0", "/", "num_batches", ")", "\n", "\n", "", "for", "i", ",", "batch", "in", "enumerate", "(", "train_generator_tqdm", ")", ":", "\n", "\n", "        ", "if", "linear_steps", ":", "\n", "            ", "current_lr", "=", "start_lr", "+", "(", "lr_update_factor", "*", "i", ")", "\n", "", "else", ":", "\n", "            ", "current_lr", "=", "start_lr", "*", "(", "lr_update_factor", "**", "i", ")", "\n", "\n", "", "for", "param_group", "in", "trainer", ".", "optimizer", ".", "param_groups", ":", "\n", "            ", "param_group", "[", "\"lr\"", "]", "=", "current_lr", "\n", "\n", "", "trainer", ".", "optimizer", ".", "zero_grad", "(", ")", "\n", "loss", "=", "trainer", ".", "batch_loss", "(", "batch", ",", "for_training", "=", "True", ")", "\n", "loss", ".", "backward", "(", ")", "\n", "loss", "=", "loss", ".", "detach", "(", ")", ".", "cpu", "(", ")", ".", "item", "(", ")", "\n", "\n", "if", "stopping_factor", "is", "not", "None", "and", "(", "\n", "math", ".", "isnan", "(", "loss", ")", "or", "loss", ">", "stopping_factor", "*", "best", "\n", ")", ":", "\n", "            ", "logger", ".", "info", "(", "\n", "f\"Loss ({loss}) exceeds stopping_factor * lowest recorded loss.\"", "\n", ")", "\n", "break", "\n", "\n", "", "trainer", ".", "rescale_gradients", "(", ")", "\n", "trainer", ".", "optimizer", ".", "step", "(", ")", "\n", "\n", "learning_rates", ".", "append", "(", "current_lr", ")", "\n", "losses", ".", "append", "(", "loss", ")", "\n", "\n", "if", "loss", "<", "best", "and", "i", ">", "10", ":", "\n", "            ", "best", "=", "loss", "\n", "\n", "", "if", "i", "==", "num_batches", ":", "\n", "            ", "break", "\n", "\n", "", "", "return", "learning_rates", ",", "losses", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.commands.find_learning_rate._smooth": [[360, 368], ["enumerate", "smoothed.append"], "function", ["None"], ["", "def", "_smooth", "(", "values", ":", "List", "[", "float", "]", ",", "beta", ":", "float", ")", "->", "List", "[", "float", "]", ":", "\n", "    ", "\"\"\" Exponential smoothing of values \"\"\"", "\n", "avg_value", "=", "0.0", "\n", "smoothed", "=", "[", "]", "\n", "for", "i", ",", "value", "in", "enumerate", "(", "values", ")", ":", "\n", "        ", "avg_value", "=", "beta", "*", "avg_value", "+", "(", "1", "-", "beta", ")", "*", "value", "\n", "smoothed", ".", "append", "(", "avg_value", "/", "(", "1", "-", "beta", "**", "(", "i", "+", "1", ")", ")", ")", "\n", "", "return", "smoothed", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.commands.find_learning_rate._save_plot": [[370, 391], ["plt.ylabel", "plt.xlabel", "plt.xscale", "plt.plot", "logger.info", "plt.savefig", "matplotlib.use", "logger.warn"], "function", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.graph_util.plot.GraphPlot.plot", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.dataset_readers.multiprocess_dataset_reader.logger.info"], ["", "def", "_save_plot", "(", "learning_rates", ":", "List", "[", "float", "]", ",", "losses", ":", "List", "[", "float", "]", ",", "save_path", ":", "str", ")", ":", "\n", "\n", "    ", "try", ":", "\n", "        ", "import", "matplotlib", "\n", "\n", "matplotlib", ".", "use", "(", "\"Agg\"", ")", "# noqa", "\n", "import", "matplotlib", ".", "pyplot", "as", "plt", "\n", "\n", "", "except", "ModuleNotFoundError", "as", "error", ":", "\n", "\n", "        ", "logger", ".", "warn", "(", "\n", "\"To use allennlp find-learning-rate, please install matplotlib: pip install matplotlib>=2.2.3 .\"", "\n", ")", "\n", "raise", "error", "\n", "\n", "", "plt", ".", "ylabel", "(", "\"loss\"", ")", "\n", "plt", ".", "xlabel", "(", "\"learning rate (log10 scale)\"", ")", "\n", "plt", ".", "xscale", "(", "\"log\"", ")", "\n", "plt", ".", "plot", "(", "learning_rates", ",", "losses", ")", "\n", "logger", ".", "info", "(", "f\"Saving learning_rate vs loss plot to {save_path}.\"", ")", "\n", "plt", ".", "savefig", "(", "save_path", ")", "\n", "", ""]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.commands.print_results.PrintResults.add_subparser": [[43, 83], ["parser.add_parser", "parser.add_parser.add_argument", "parser.add_parser.add_argument", "parser.add_parser.add_argument", "parser.add_parser.set_defaults"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.commands.__init__.ArgumentParserWithDefaults.add_argument", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.commands.__init__.ArgumentParserWithDefaults.add_argument", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.commands.__init__.ArgumentParserWithDefaults.add_argument"], ["    ", "@", "overrides", "\n", "def", "add_subparser", "(", "\n", "self", ",", "parser", ":", "argparse", ".", "_SubParsersAction", "\n", ")", "->", "argparse", ".", "ArgumentParser", ":", "\n", "\n", "        ", "description", "=", "(", "\n", "\"\"\"Print results from allennlp training runs in a helpful CSV format.\"\"\"", "\n", ")", "\n", "subparser", "=", "parser", ".", "add_parser", "(", "\n", "self", ".", "name", ",", "\n", "description", "=", "description", ",", "\n", "help", "=", "\"Print results from allennlp serialization directories to the console.\"", ",", "\n", ")", "\n", "subparser", ".", "add_argument", "(", "\n", "\"path\"", ",", "\n", "type", "=", "str", ",", "\n", "help", "=", "\"Path to recursively search for allennlp serialization directories.\"", ",", "\n", ")", "\n", "\n", "subparser", ".", "add_argument", "(", "\n", "\"-k\"", ",", "\n", "\"--keys\"", ",", "\n", "type", "=", "str", ",", "\n", "nargs", "=", "\"+\"", ",", "\n", "help", "=", "\"Keys to print from metrics.json.\"", "\n", "'Keys not present in all metrics.json will result in \"N/A\"'", ",", "\n", "default", "=", "None", ",", "\n", "required", "=", "False", ",", "\n", ")", "\n", "subparser", ".", "add_argument", "(", "\n", "\"-m\"", ",", "\n", "\"--metrics-filename\"", ",", "\n", "type", "=", "str", ",", "\n", "help", "=", "\"Name of the metrics file to inspect.\"", ",", "\n", "default", "=", "\"metrics.json\"", ",", "\n", "required", "=", "False", ",", "\n", ")", "\n", "\n", "subparser", ".", "set_defaults", "(", "func", "=", "print_results_from_args", ")", "\n", "return", "subparser", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.commands.print_results.print_results_from_args": [[85, 107], ["os.walk", "sorted", "print", "list", "print", "os.path.join", "results_dict.keys", "str", "open", "json.load", "results.get"], "function", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.dataset_readers.multiprocess_dataset_reader.QIterable.join", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.token_embedders.bert_token_embedder.PretrainedBertModel.load", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.params.Params.get"], ["", "", "def", "print_results_from_args", "(", "args", ":", "argparse", ".", "Namespace", ")", ":", "\n", "    ", "\"\"\"\n    Prints results from an ``argparse.Namespace`` object.\n    \"\"\"", "\n", "path", "=", "args", ".", "path", "\n", "metrics_name", "=", "args", ".", "metrics_filename", "\n", "keys", "=", "args", ".", "keys", "\n", "\n", "results_dict", "=", "{", "}", "\n", "for", "root", ",", "_", ",", "files", "in", "os", ".", "walk", "(", "path", ")", ":", "\n", "        ", "if", "metrics_name", "in", "files", ":", "\n", "            ", "full_name", "=", "os", ".", "path", ".", "join", "(", "root", ",", "metrics_name", ")", "\n", "with", "open", "(", "full_name", ")", "as", "file_", ":", "\n", "                ", "metrics", "=", "json", ".", "load", "(", "file_", ")", "\n", "", "results_dict", "[", "full_name", "]", "=", "metrics", "\n", "\n", "", "", "sorted_keys", "=", "sorted", "(", "list", "(", "results_dict", ".", "keys", "(", ")", ")", ")", "\n", "print", "(", "f\"model_run, {', '.join(keys)}\"", ")", "\n", "for", "name", "in", "sorted_keys", ":", "\n", "        ", "results", "=", "results_dict", "[", "name", "]", "\n", "keys_to_print", "=", "(", "str", "(", "results", ".", "get", "(", "key", ",", "\"N/A\"", ")", ")", "for", "key", "in", "keys", ")", "\n", "print", "(", "f\"{name}, {', '.join(keys_to_print)}\"", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.commands.predict.Predict.add_subparser": [[70, 148], ["parser.add_parser", "parser.add_parser.add_argument", "parser.add_parser.add_argument", "parser.add_parser.add_argument", "parser.add_parser.add_argument", "parser.add_parser.add_mutually_exclusive_group", "parser.add_parser.add_mutually_exclusive_group.add_argument", "parser.add_parser.add_argument", "parser.add_parser.add_mutually_exclusive_group", "parser.add_parser.add_mutually_exclusive_group.add_argument", "parser.add_parser.add_argument", "parser.add_parser.add_argument", "parser.add_parser.add_argument", "parser.add_parser.add_argument", "parser.add_parser.set_defaults"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.commands.__init__.ArgumentParserWithDefaults.add_argument", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.commands.__init__.ArgumentParserWithDefaults.add_argument", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.commands.__init__.ArgumentParserWithDefaults.add_argument", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.commands.__init__.ArgumentParserWithDefaults.add_argument", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.commands.__init__.ArgumentParserWithDefaults.add_argument", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.commands.__init__.ArgumentParserWithDefaults.add_argument", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.commands.__init__.ArgumentParserWithDefaults.add_argument", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.commands.__init__.ArgumentParserWithDefaults.add_argument", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.commands.__init__.ArgumentParserWithDefaults.add_argument", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.commands.__init__.ArgumentParserWithDefaults.add_argument", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.commands.__init__.ArgumentParserWithDefaults.add_argument"], ["    ", "@", "overrides", "\n", "def", "add_subparser", "(", "\n", "self", ",", "parser", ":", "argparse", ".", "_SubParsersAction", "\n", ")", "->", "argparse", ".", "ArgumentParser", ":", "\n", "\n", "        ", "description", "=", "\"\"\"Run the specified model against a JSON-lines input file.\"\"\"", "\n", "subparser", "=", "parser", ".", "add_parser", "(", "\n", "self", ".", "name", ",", "\n", "description", "=", "description", ",", "\n", "help", "=", "\"Use a trained model to make predictions.\"", ",", "\n", ")", "\n", "\n", "subparser", ".", "add_argument", "(", "\n", "\"archive_file\"", ",", "type", "=", "str", ",", "help", "=", "\"the archived model to make predictions with\"", "\n", ")", "\n", "subparser", ".", "add_argument", "(", "\n", "\"input_file\"", ",", "type", "=", "str", ",", "help", "=", "\"path to or url of the input file\"", "\n", ")", "\n", "\n", "subparser", ".", "add_argument", "(", "\"--output-file\"", ",", "type", "=", "str", ",", "help", "=", "\"path to output file\"", ")", "\n", "subparser", ".", "add_argument", "(", "\n", "\"--weights-file\"", ",", "\n", "type", "=", "str", ",", "\n", "help", "=", "\"a path that overrides which weights file to use\"", ",", "\n", ")", "\n", "\n", "batch_size", "=", "subparser", ".", "add_mutually_exclusive_group", "(", "required", "=", "False", ")", "\n", "batch_size", ".", "add_argument", "(", "\n", "\"--batch-size\"", ",", "\n", "type", "=", "int", ",", "\n", "default", "=", "1", ",", "\n", "help", "=", "\"The batch size to use for processing\"", ",", "\n", ")", "\n", "\n", "subparser", ".", "add_argument", "(", "\n", "\"--silent\"", ",", "action", "=", "\"store_true\"", ",", "help", "=", "\"do not print output to stdout\"", "\n", ")", "\n", "\n", "cuda_device", "=", "subparser", ".", "add_mutually_exclusive_group", "(", "required", "=", "False", ")", "\n", "cuda_device", ".", "add_argument", "(", "\n", "\"--cuda-device\"", ",", "type", "=", "int", ",", "default", "=", "-", "1", ",", "help", "=", "\"id of GPU to use (if any)\"", "\n", ")", "\n", "\n", "subparser", ".", "add_argument", "(", "\n", "\"--use-dataset-reader\"", ",", "\n", "action", "=", "\"store_true\"", ",", "\n", "help", "=", "\"Whether to use the dataset reader of the original model to load Instances. \"", "\n", "\"The validation dataset reader will be used if it exists, otherwise it will \"", "\n", "\"fall back to the train dataset reader. This behavior can be overridden \"", "\n", "\"with the --dataset-reader-choice flag.\"", ",", "\n", ")", "\n", "\n", "subparser", ".", "add_argument", "(", "\n", "\"--dataset-reader-choice\"", ",", "\n", "type", "=", "str", ",", "\n", "choices", "=", "[", "\"train\"", ",", "\"validation\"", "]", ",", "\n", "default", "=", "\"validation\"", ",", "\n", "help", "=", "\"Indicates which model dataset reader to use if the --use-dataset-reader \"", "\n", "\"flag is set.\"", ",", "\n", ")", "\n", "\n", "subparser", ".", "add_argument", "(", "\n", "\"-o\"", ",", "\n", "\"--overrides\"", ",", "\n", "type", "=", "str", ",", "\n", "default", "=", "\"\"", ",", "\n", "help", "=", "\"a JSON structure used to override the experiment configuration\"", ",", "\n", ")", "\n", "\n", "subparser", ".", "add_argument", "(", "\n", "\"--predictor\"", ",", "\n", "type", "=", "str", ",", "\n", "help", "=", "\"optionally specify a specific predictor to use\"", ",", "\n", ")", "\n", "\n", "subparser", ".", "set_defaults", "(", "func", "=", "_predict", ")", "\n", "\n", "return", "subparser", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.commands.predict._PredictManager.__init__": [[165, 187], ["open"], "methods", ["None"], ["    ", "def", "__init__", "(", "\n", "self", ",", "\n", "predictor", ":", "Predictor", ",", "\n", "input_file", ":", "str", ",", "\n", "output_file", ":", "Optional", "[", "str", "]", ",", "\n", "batch_size", ":", "int", ",", "\n", "print_to_console", ":", "bool", ",", "\n", "has_dataset_reader", ":", "bool", ",", "\n", ")", "->", "None", ":", "\n", "\n", "        ", "self", ".", "_predictor", "=", "predictor", "\n", "self", ".", "_input_file", "=", "input_file", "\n", "if", "output_file", "is", "not", "None", ":", "\n", "            ", "self", ".", "_output_file", "=", "open", "(", "output_file", ",", "\"w\"", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "_output_file", "=", "None", "\n", "", "self", ".", "_batch_size", "=", "batch_size", "\n", "self", ".", "_print_to_console", "=", "print_to_console", "\n", "if", "has_dataset_reader", ":", "\n", "            ", "self", ".", "_dataset_reader", "=", "predictor", ".", "_dataset_reader", "\n", "", "else", ":", "\n", "            ", "self", ".", "_dataset_reader", "=", "None", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.commands.predict._PredictManager._predict_json": [[188, 195], ["len", "predict._PredictManager._predictor.predict_batch_json", "predict._PredictManager._predictor.predict_json", "predict._PredictManager._predictor.dump_line"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.predictors.semantic_role_labeler.SemanticRoleLabelerPredictor.predict_batch_json", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.predictors.semantic_role_labeler.SemanticRoleLabelerPredictor.predict_json", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.predictors.predictor.Predictor.dump_line"], ["", "", "def", "_predict_json", "(", "self", ",", "batch_data", ":", "List", "[", "JsonDict", "]", ")", "->", "Iterator", "[", "str", "]", ":", "\n", "        ", "if", "len", "(", "batch_data", ")", "==", "1", ":", "\n", "            ", "results", "=", "[", "self", ".", "_predictor", ".", "predict_json", "(", "batch_data", "[", "0", "]", ")", "]", "\n", "", "else", ":", "\n", "            ", "results", "=", "self", ".", "_predictor", ".", "predict_batch_json", "(", "batch_data", ")", "\n", "", "for", "output", "in", "results", ":", "\n", "            ", "yield", "self", ".", "_predictor", ".", "dump_line", "(", "output", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.commands.predict._PredictManager._predict_instances": [[196, 203], ["len", "predict._PredictManager._predictor.predict_batch_instance", "predict._PredictManager._predictor.predict_instance", "predict._PredictManager._predictor.dump_line"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.predictors.constituency_parser.ConstituencyParserPredictor.predict_batch_instance", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.predictors.constituency_parser.ConstituencyParserPredictor.predict_instance", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.predictors.predictor.Predictor.dump_line"], ["", "", "def", "_predict_instances", "(", "self", ",", "batch_data", ":", "List", "[", "Instance", "]", ")", "->", "Iterator", "[", "str", "]", ":", "\n", "        ", "if", "len", "(", "batch_data", ")", "==", "1", ":", "\n", "            ", "results", "=", "[", "self", ".", "_predictor", ".", "predict_instance", "(", "batch_data", "[", "0", "]", ")", "]", "\n", "", "else", ":", "\n", "            ", "results", "=", "self", ".", "_predictor", ".", "predict_batch_instance", "(", "batch_data", ")", "\n", "", "for", "output", "in", "results", ":", "\n", "            ", "yield", "self", ".", "_predictor", ".", "dump_line", "(", "output", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.commands.predict._PredictManager._maybe_print_to_console_and_file": [[204, 213], ["print", "predict._PredictManager._output_file.write", "print"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.None.predictor.write"], ["", "", "def", "_maybe_print_to_console_and_file", "(", "\n", "self", ",", "index", ":", "int", ",", "prediction", ":", "str", ",", "model_input", ":", "str", "=", "None", "\n", ")", "->", "None", ":", "\n", "        ", "if", "self", ".", "_print_to_console", ":", "\n", "            ", "if", "model_input", "is", "not", "None", ":", "\n", "                ", "print", "(", "f\"input {index}: \"", ",", "model_input", ")", "\n", "", "print", "(", "\"prediction: \"", ",", "prediction", ")", "\n", "", "if", "self", ".", "_output_file", "is", "not", "None", ":", "\n", "            ", "self", ".", "_output_file", ".", "write", "(", "prediction", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.commands.predict._PredictManager._get_json_data": [[214, 225], ["allennlp.common.file_utils.cached_path", "open", "line.isspace", "predict._PredictManager._predictor.load_line", "line.isspace", "predict._PredictManager._predictor.load_line"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.file_utils.cached_path", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.predictors.predictor.Predictor.load_line", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.predictors.predictor.Predictor.load_line"], ["", "", "def", "_get_json_data", "(", "self", ")", "->", "Iterator", "[", "JsonDict", "]", ":", "\n", "        ", "if", "self", ".", "_input_file", "==", "\"-\"", ":", "\n", "            ", "for", "line", "in", "sys", ".", "stdin", ":", "\n", "                ", "if", "not", "line", ".", "isspace", "(", ")", ":", "\n", "                    ", "yield", "self", ".", "_predictor", ".", "load_line", "(", "line", ")", "\n", "", "", "", "else", ":", "\n", "            ", "input_file", "=", "cached_path", "(", "self", ".", "_input_file", ")", "\n", "with", "open", "(", "input_file", ",", "\"r\"", ")", "as", "file_input", ":", "\n", "                ", "for", "line", "in", "file_input", ":", "\n", "                    ", "if", "not", "line", ".", "isspace", "(", ")", ":", "\n", "                        ", "yield", "self", ".", "_predictor", ".", "load_line", "(", "line", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.commands.predict._PredictManager._get_instance_data": [[226, 237], ["allennlp.common.checks.ConfigurationError", "allennlp.common.checks.ConfigurationError", "predict._PredictManager._dataset_reader.read"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.scripts.convert_openie_to_conll.read"], ["", "", "", "", "", "def", "_get_instance_data", "(", "self", ")", "->", "Iterator", "[", "Instance", "]", ":", "\n", "        ", "if", "self", ".", "_input_file", "==", "\"-\"", ":", "\n", "            ", "raise", "ConfigurationError", "(", "\n", "\"stdin is not an option when using a DatasetReader.\"", "\n", ")", "\n", "", "elif", "self", ".", "_dataset_reader", "is", "None", ":", "\n", "            ", "raise", "ConfigurationError", "(", "\n", "\"To generate instances directly, pass a DatasetReader.\"", "\n", ")", "\n", "", "else", ":", "\n", "            ", "yield", "from", "self", ".", "_dataset_reader", ".", "read", "(", "self", ".", "_input_file", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.commands.predict._PredictManager.run": [[238, 262], ["allennlp.common.util.lazy_groups_of", "allennlp.common.util.lazy_groups_of", "predict._PredictManager._output_file.close", "predict._PredictManager._get_instance_data", "zip", "predict._PredictManager._get_json_data", "zip", "predict._PredictManager._predict_instances", "predict._PredictManager._maybe_print_to_console_and_file", "predict._PredictManager._predict_json", "predict._PredictManager._maybe_print_to_console_and_file", "str", "json.dumps"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.util.lazy_groups_of", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.util.lazy_groups_of", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.token_embedders.embedding.EmbeddingsTextFile.close", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.commands.predict._PredictManager._get_instance_data", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.commands.predict._PredictManager._get_json_data", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.commands.predict._PredictManager._predict_instances", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.commands.predict._PredictManager._maybe_print_to_console_and_file", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.commands.predict._PredictManager._predict_json", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.commands.predict._PredictManager._maybe_print_to_console_and_file"], ["", "", "def", "run", "(", "self", ")", "->", "None", ":", "\n", "        ", "has_reader", "=", "self", ".", "_dataset_reader", "is", "not", "None", "\n", "index", "=", "0", "\n", "if", "has_reader", ":", "\n", "            ", "for", "batch", "in", "lazy_groups_of", "(", "self", ".", "_get_instance_data", "(", ")", ",", "self", ".", "_batch_size", ")", ":", "\n", "                ", "for", "model_input_instance", ",", "result", "in", "zip", "(", "\n", "batch", ",", "self", ".", "_predict_instances", "(", "batch", ")", "\n", ")", ":", "\n", "                    ", "self", ".", "_maybe_print_to_console_and_file", "(", "\n", "index", ",", "result", ",", "str", "(", "model_input_instance", ")", "\n", ")", "\n", "index", "=", "index", "+", "1", "\n", "", "", "", "else", ":", "\n", "            ", "for", "batch_json", "in", "lazy_groups_of", "(", "self", ".", "_get_json_data", "(", ")", ",", "self", ".", "_batch_size", ")", ":", "\n", "                ", "for", "model_input_json", ",", "result", "in", "zip", "(", "\n", "batch_json", ",", "self", ".", "_predict_json", "(", "batch_json", ")", "\n", ")", ":", "\n", "                    ", "self", ".", "_maybe_print_to_console_and_file", "(", "\n", "index", ",", "result", ",", "json", ".", "dumps", "(", "model_input_json", ")", "\n", ")", "\n", "index", "=", "index", "+", "1", "\n", "\n", "", "", "", "if", "self", ".", "_output_file", "is", "not", "None", ":", "\n", "            ", "self", ".", "_output_file", ".", "close", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.commands.predict._get_predictor": [[150, 161], ["allennlp.common.checks.check_for_gpu", "allennlp.models.archival.load_archive", "allennlp.predictors.predictor.Predictor.from_archive"], "function", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.checks.check_for_gpu", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.models.archival.load_archive", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.predictors.predictor.Predictor.from_archive"], ["", "", "def", "_get_predictor", "(", "args", ":", "argparse", ".", "Namespace", ")", "->", "Predictor", ":", "\n", "    ", "check_for_gpu", "(", "args", ".", "cuda_device", ")", "\n", "archive", "=", "load_archive", "(", "\n", "args", ".", "archive_file", ",", "\n", "weights_file", "=", "args", ".", "weights_file", ",", "\n", "cuda_device", "=", "args", ".", "cuda_device", ",", "\n", "overrides", "=", "args", ".", "overrides", ",", "\n", ")", "\n", "\n", "return", "Predictor", ".", "from_archive", "(", "\n", "archive", ",", "args", ".", "predictor", ",", "dataset_reader_to_load", "=", "args", ".", "dataset_reader_choice", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.commands.predict._predict": [[264, 281], ["predict._get_predictor", "predict._PredictManager", "predict._PredictManager.run", "print", "print", "sys.exit"], "function", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.commands.predict._get_predictor", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.commands.train.TrainModel.run"], ["", "", "", "def", "_predict", "(", "args", ":", "argparse", ".", "Namespace", ")", "->", "None", ":", "\n", "    ", "predictor", "=", "_get_predictor", "(", "args", ")", "\n", "\n", "if", "args", ".", "silent", "and", "not", "args", ".", "output_file", ":", "\n", "        ", "print", "(", "\"--silent specified without --output-file.\"", ")", "\n", "print", "(", "\"Exiting early because no output will be created.\"", ")", "\n", "sys", ".", "exit", "(", "0", ")", "\n", "\n", "", "manager", "=", "_PredictManager", "(", "\n", "predictor", ",", "\n", "args", ".", "input_file", ",", "\n", "args", ".", "output_file", ",", "\n", "args", ".", "batch_size", ",", "\n", "not", "args", ".", "silent", ",", "\n", "args", ".", "use_dataset_reader", ",", "\n", ")", "\n", "manager", ".", "run", "(", ")", "\n", "", ""]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.commands.evaluate.Evaluate.add_subparser": [[74, 151], ["parser.add_parser", "parser.add_parser.add_argument", "parser.add_parser.add_argument", "parser.add_parser.add_argument", "parser.add_parser.add_argument", "parser.add_parser.add_mutually_exclusive_group", "parser.add_parser.add_mutually_exclusive_group.add_argument", "parser.add_parser.add_argument", "parser.add_parser.add_argument", "parser.add_parser.add_argument", "parser.add_parser.add_argument", "parser.add_parser.add_argument", "parser.add_parser.set_defaults"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.commands.__init__.ArgumentParserWithDefaults.add_argument", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.commands.__init__.ArgumentParserWithDefaults.add_argument", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.commands.__init__.ArgumentParserWithDefaults.add_argument", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.commands.__init__.ArgumentParserWithDefaults.add_argument", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.commands.__init__.ArgumentParserWithDefaults.add_argument", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.commands.__init__.ArgumentParserWithDefaults.add_argument", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.commands.__init__.ArgumentParserWithDefaults.add_argument", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.commands.__init__.ArgumentParserWithDefaults.add_argument", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.commands.__init__.ArgumentParserWithDefaults.add_argument", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.commands.__init__.ArgumentParserWithDefaults.add_argument"], ["\n", "if", "run_on_gpu", ":", "\n", "        ", "model", "=", "model", ".", "to", "(", "cuda_device", ")", "\n", "\n", "", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "        ", "iterator", "=", "data_iterator", "(", "instances", ",", "num_epochs", "=", "1", ",", "shuffle", "=", "False", ")", "\n", "logger", ".", "info", "(", "\"Iterating over dataset\"", ")", "\n", "generator_tqdm", "=", "tqdm", ".", "tqdm", "(", "\n", "iterator", ",", "total", "=", "data_iterator", ".", "get_num_batches", "(", "instances", ")", "\n", ")", "\n", "\n", "eval_loss", "=", "0", "\n", "nb_batches", "=", "0", "\n", "infered_dicts", "=", "[", "]", "\n", "for", "batch", "in", "generator_tqdm", ":", "\n", "            ", "if", "run_on_gpu", ":", "\n", "                ", "batch", "=", "util", ".", "move_to_device", "(", "batch", ",", "cuda_device", ")", "\n", "", "nb_batches", "+=", "1", "\n", "\n", "eval_output_dict", "=", "model", ".", "forward", "(", "\n", "task_name", "=", "task_name", ",", "tensor_batch", "=", "batch", ",", "sample", "=", "False", "\n", ")", "\n", "loss", "=", "eval_output_dict", "[", "\"loss\"", "]", "\n", "eval_loss", "+=", "loss", ".", "item", "(", ")", "\n", "metrics", "=", "model", ".", "get_metrics", "(", "task_name", "=", "task_name", ")", "\n", "metrics", "[", "\"loss\"", "]", "=", "float", "(", "eval_loss", "/", "nb_batches", ")", "\n", "\n", "if", "requires_inf_dict", ":", "\n", "                ", "infered_dicts", ".", "append", "(", "\n", "model", ".", "decode", "(", "eval_output_dict", ",", "task_name", "=", "task_name", ")", "\n", ")", "\n", "\n", "", "description", "=", "(", "\n", "\", \"", ".", "join", "(", "\n", "[", "\"%s: %.2f\"", "%", "(", "name", ",", "value", ")", "for", "name", ",", "value", "in", "metrics", ".", "items", "(", ")", "]", "\n", ")", "\n", "+", "\" ||\"", "\n", ")", "\n", "generator_tqdm", ".", "set_description", "(", "description", ",", "refresh", "=", "False", ")", "\n", "\n", "", "metrics", "=", "model", ".", "get_metrics", "(", "task_name", "=", "task_name", ",", "reset", "=", "True", ",", "full", "=", "True", ")", "\n", "metrics", "[", "\"loss\"", "]", "=", "float", "(", "eval_loss", "/", "nb_batches", ")", "\n", "return", "metrics", ",", "infered_dicts", "\n", "\n", "\n", "", "", "if", "__name__", "==", "\"__main__\"", ":", "\n", "### Evaluate from args ###", "\n", "\n", "# Parse arguments", "\n", "    ", "parser", "=", "argparse", ".", "ArgumentParser", "(", ")", "\n", "parser", ".", "add_argument", "(", "\n", "\"-s\"", ",", "\n", "\"--serialization_dir\"", ",", "\n", "required", "=", "True", ",", "\n", "help", "=", "\"Directory in which the model is serialized.\"", ",", "\n", "type", "=", "str", ",", "\n", ")", "\n", "parser", ".", "add_argument", "(", "\n", "\"-t\"", ",", "\n", "\"--task\"", ",", "\n", "required", "=", "True", ",", "\n", "help", "=", "\"Name of the task to evaluate.\"", ",", "\n", "choices", "=", "[", "\"coref\"", ",", "\"srl\"", "]", ",", "\n", "type", "=", "str", ",", "\n", ")", "\n", "parser", ".", "add_argument", "(", "\n", "\"-d\"", ",", "\n", "\"--data\"", ",", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.commands.evaluate.evaluate_from_args": [[153, 211], ["logging.getLogger().setLevel", "allennlp.models.archival.load_archive", "allennlp.common.util.prepare_environment", "model.eval", "config.pop", "logger.info", "DatasetReader.from_params.read", "config.pop", "allennlp.data.iterators.DataIterator.from_params", "DataIterator.from_params.index_with", "allennlp.training.util.evaluate", "logger.info", "allennlp.common.util.dump_metrics", "logging.getLogger", "logging.getLogger", "allennlp.data.dataset_readers.dataset_reader.DatasetReader.from_params", "allennlp.data.dataset_readers.dataset_reader.DatasetReader.from_params", "json.loads", "logger.info", "model.vocab.extend_from_instances", "model.extend_embedder_vocab", "config.pop", "logging.getLogger", "config.pop"], "function", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.models.archival.load_archive", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.util.prepare_environment", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.params.Params.pop", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.dataset_readers.multiprocess_dataset_reader.logger.info", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.scripts.convert_openie_to_conll.read", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.params.Params.pop", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.tokenizers.tokenizer.Tokenizer.from_params", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.iterators.data_iterator.DataIterator.index_with", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.training.util.evaluate", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.dataset_readers.multiprocess_dataset_reader.logger.info", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.util.dump_metrics", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.tokenizers.tokenizer.Tokenizer.from_params", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.tokenizers.tokenizer.Tokenizer.from_params", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.dataset_readers.multiprocess_dataset_reader.logger.info", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.data.vocabulary.Vocabulary.extend_from_instances", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.models.model.Model.extend_embedder_vocab", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.params.Params.pop", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.params.Params.pop"], ["help", "=", "\"Name of the dataset to load.\"", ",", "\n", "choices", "=", "[", "\n", "\"conll12\"", ",", "\n", "\"preco\"", ",", "\n", "\"phrase_detectives_g\"", ",", "\n", "\"phrase_detectives_w\"", ",", "\n", "\"wikicoref\"", ",", "\n", "\"winobias\"", ",", "\n", "\"conll05_w\"", ",", "\n", "\"conll05_b\"", ",", "\n", "\"ewt\"", ",", "\n", "]", ",", "\n", "type", "=", "str", ",", "\n", ")", "\n", "parser", ".", "add_argument", "(", "\n", "\"-m\"", ",", "\n", "\"--model_type\"", ",", "\n", "required", "=", "True", ",", "\n", "help", "=", "\"Model type to evaluate on (pre-trained or fine-tuned).\"", ",", "\n", "choices", "=", "[", "\"pt\"", ",", "\"ft\"", "]", ",", "\n", "type", "=", "str", ",", "\n", ")", "\n", "parser", ".", "add_argument", "(", "\n", "\"-g\"", ",", "\n", "\"--gold_mentions\"", ",", "\n", "action", "=", "\"store_true\"", ",", "\n", "required", "=", "False", ",", "\n", "default", "=", "False", ",", "\n", "help", "=", "\"Whether or not evaluate using gold mentions in coreference\"", ",", "\n", ")", "\n", "args", "=", "parser", ".", "parse_args", "(", ")", "\n", "\n", "params", "=", "Params", ".", "from_file", "(", "\n", "params_file", "=", "os", ".", "path", ".", "join", "(", "args", ".", "serialization_dir", ",", "\"config.json\"", ")", "\n", ")", "\n", "\n", "### Open a ft-config file and read the test data paths (doesn't matter which config) ###", "\n", "with", "open", "(", "\"./configs/ft/th_config_bert.json\"", ",", "\"r\"", ")", "as", "f", ":", "\n", "        ", "ft_config", "=", "json", ".", "load", "(", "f", ")", "\n", "\n", "### Instantiate tasks ###", "\n", "", "task_list", "=", "[", "]", "\n", "task_keys", "=", "[", "\"task_{}\"", ".", "format", "(", "args", ".", "task", ")", "]", "\n", "\n", "reader_classes", "=", "{", "\n", "\"coref\"", ":", "{", "\n", "\"conll12\"", ":", "\"coref_conll\"", ",", "\n", "\"preco\"", ":", "\"coref_preco\"", ",", "\n", "\"wikicoref\"", ":", "\"coref_wikicoref\"", ",", "\n", "\"winobias\"", ":", "\"coref_winobias\"", ",", "\n", "\"phrase_detectives_g\"", ":", "\"coref_conll\"", ",", "\n", "\"phrase_detectives_w\"", ":", "\"coref_conll\"", ",", "\n", "}", ",", "\n", "\"srl\"", ":", "{", "\n", "\"conll12\"", ":", "\"srl_conll\"", ",", "\n", "\"conll05_w\"", ":", "\"srl_conll05\"", ",", "\n", "\"conll05_b\"", ":", "\"srl_conll05\"", ",", "\n", "\"wikibank\"", ":", "\"srl_wikibank\"", ",", "\n", "\"ewt\"", ":", "\"srl_wikibank\"", ",", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.commands.train.Train.add_subparser": [[64, 128], ["parser.add_parser", "parser.add_parser.add_argument", "parser.add_parser.add_argument", "parser.add_parser.add_argument", "parser.add_parser.add_argument", "parser.add_parser.add_argument", "parser.add_parser.add_argument", "parser.add_parser.add_argument", "parser.add_parser.set_defaults"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.commands.__init__.ArgumentParserWithDefaults.add_argument", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.commands.__init__.ArgumentParserWithDefaults.add_argument", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.commands.__init__.ArgumentParserWithDefaults.add_argument", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.commands.__init__.ArgumentParserWithDefaults.add_argument", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.commands.__init__.ArgumentParserWithDefaults.add_argument", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.commands.__init__.ArgumentParserWithDefaults.add_argument", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.commands.__init__.ArgumentParserWithDefaults.add_argument"], ["\n", "### Instantiate the different tasks ###", "\n", "task_list", "=", "[", "]", "\n", "instances_for_vocab_creation", "=", "itertools", ".", "chain", "(", ")", "\n", "datasets_for_vocab_creation", "=", "{", "}", "\n", "task_keys", "=", "[", "key", "for", "key", "in", "params", ".", "keys", "(", ")", "if", "re", ".", "search", "(", "\"^task_\"", ",", "key", ")", "]", "\n", "\n", "for", "key", "in", "task_keys", ":", "\n", "        ", "logger", ".", "info", "(", "\"Creating %s\"", ",", "key", ")", "\n", "task_params", "=", "params", ".", "pop", "(", "key", ")", "\n", "task_description", "=", "task_params", ".", "pop", "(", "\"task_description\"", ")", "\n", "task_data_params", "=", "task_params", ".", "pop", "(", "\"data_params\"", ")", "\n", "\n", "task", "=", "Task", ".", "from_params", "(", "params", "=", "task_description", ")", "\n", "task_list", ".", "append", "(", "task", ")", "\n", "\n", "task_instances_for_vocab", ",", "task_datasets_for_vocab", "=", "task", ".", "load_data_from_params", "(", "\n", "params", "=", "task_data_params", "\n", ")", "\n", "instances_for_vocab_creation", "=", "itertools", ".", "chain", "(", "\n", "instances_for_vocab_creation", ",", "task_instances_for_vocab", "\n", ")", "\n", "datasets_for_vocab_creation", "[", "task", ".", "_name", "]", "=", "task_datasets_for_vocab", "\n", "\n", "### Create and save the vocabulary ###", "\n", "", "for", "task_name", ",", "task_dataset_list", "in", "datasets_for_vocab_creation", ".", "items", "(", ")", ":", "\n", "        ", "logger", ".", "info", "(", "\n", "\"Creating a vocabulary using %s data from %s.\"", ",", "\n", "\", \"", ".", "join", "(", "task_dataset_list", ")", ",", "\n", "task_name", ",", "\n", ")", "\n", "\n", "", "logger", ".", "info", "(", "\"Fitting vocabulary from dataset\"", ")", "\n", "vocab", "=", "Vocabulary", ".", "from_params", "(", "\n", "params", ".", "pop", "(", "\"vocabulary\"", ",", "{", "}", ")", ",", "instances", "=", "instances_for_vocab_creation", "\n", ")", "\n", "\n", "vocab", ".", "save_to_files", "(", "os", ".", "path", ".", "join", "(", "serialization_dir", ",", "\"vocabulary\"", ")", ")", "\n", "logger", ".", "info", "(", "\"Vocabulary saved to %s\"", ",", "os", ".", "path", ".", "join", "(", "serialization_dir", ",", "\"vocabulary\"", ")", ")", "\n", "\n", "return", "task_list", ",", "vocab", "\n", "\n", "\n", "", "def", "train_model", "(", "\n", "multi_task_trainer", ":", "MultiTaskTrainer", ",", "recover", ":", "bool", "=", "False", "\n", ")", "->", "Dict", "[", "str", ",", "Any", "]", ":", "\n", "    "]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.commands.train.TrainModel.__init__": [[495, 512], ["None"], "methods", ["None"], []], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.commands.train.TrainModel.run": [[513, 515], ["train.TrainModel.trainer.train"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.training.no_op_trainer.NoOpTrainer.train"], []], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.commands.train.TrainModel.finish": [[516, 536], ["allennlp.common.util.dump_metrics", "logger.info", "allennlp.training.util.evaluate", "allennlp.training.util.evaluate.items", "os.path.join", "logger.info"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.util.dump_metrics", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.dataset_readers.multiprocess_dataset_reader.logger.info", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.training.util.evaluate", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.dataset_readers.multiprocess_dataset_reader.QIterable.join", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.dataset_readers.multiprocess_dataset_reader.logger.info"], []], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.commands.train.TrainModel.from_partial_objects": [[538, 677], ["allennlp.training.util.read_all_datasets", "vocabulary.construct", "model.construct", "allennlp.common.util.is_master", "iterator.index_with", "validation_iterator.index_with", "trainer.construct", "cls", "allennlp.data.Vocabulary.from_instances", "os.path.join", "allennlp.data.Vocabulary.from_instances.save_to_files", "allennlp.training.util.read_all_datasets.items", "allennlp.training.util.read_all_datasets.get", "allennlp.training.util.read_all_datasets.get", "allennlp.common.checks.ConfigurationError"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.training.util.read_all_datasets", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.lazy.Lazy.construct", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.lazy.Lazy.construct", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.util.is_master", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.iterators.data_iterator.DataIterator.index_with", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.iterators.data_iterator.DataIterator.index_with", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.lazy.Lazy.construct", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.data.vocabulary.Vocabulary.from_instances", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.dataset_readers.multiprocess_dataset_reader.QIterable.join", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.data.vocabulary.Vocabulary.save_to_files", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.params.Params.get", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.params.Params.get"], []], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.commands.train.train_model_from_args": [[130, 143], ["train.train_model_from_file"], "function", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.commands.train.train_model_from_file"], ["\n", "### Train the multi-task model ###", "\n", "metrics", "=", "multi_task_trainer", ".", "train", "(", "recover", "=", "recover", ")", "\n", "\n", "task_list", "=", "multi_task_trainer", ".", "_task_list", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.commands.train.train_model_from_file": [[146, 192], ["allennlp.common.Params.from_file", "train.train_model"], "function", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.params.Params.from_file", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.commands.train.train_model"], ["\n", "### Evaluate the model on test data if necessary ###", "\n", "# This is a multi-task learning framework, the best validation metrics for one task are not necessarily", "\n", "# obtained from the same epoch for all the tasks, one epoch begin equal to N forward+backward passes,", "\n", "# where N is the total number of batches in all the training sets.", "\n", "# We evaluate each of the best model for each task (based on the validation metrics) for all the other tasks (which have a test set).", "\n", "for", "task", "in", "task_list", ":", "\n", "        ", "if", "not", "task", ".", "_evaluate_on_test", ":", "\n", "            ", "continue", "\n", "\n", "", "logger", ".", "info", "(", "\n", "\"Task %s will be evaluated using the best epoch weights.\"", ",", "task", ".", "_name", "\n", ")", "\n", "assert", "(", "\n", "task", ".", "_test_data", "is", "not", "None", "\n", ")", ",", "\"Task {} wants to be evaluated on test dataset but no there is no test data loaded.\"", ".", "format", "(", "\n", "task", ".", "_name", "\n", ")", "\n", "\n", "logger", ".", "info", "(", "\"Loading the best epoch weights for task %s\"", ",", "task", ".", "_name", ")", "\n", "best_model_state_path", "=", "os", ".", "path", ".", "join", "(", "\n", "serialization_dir", ",", "\"best_{}.th\"", ".", "format", "(", "task", ".", "_name", ")", "\n", ")", "\n", "best_model_state", "=", "torch", ".", "load", "(", "best_model_state_path", ")", "\n", "best_model", "=", "model", "\n", "best_model", ".", "load_state_dict", "(", "state_dict", "=", "best_model_state", ")", "\n", "\n", "test_metric_dict", "=", "{", "}", "\n", "\n", "for", "pair_task", "in", "task_list", ":", "\n", "            ", "if", "not", "pair_task", ".", "_evaluate_on_test", ":", "\n", "                ", "continue", "\n", "\n", "", "logger", ".", "info", "(", "\n", "\"Pair task %s is evaluated with the best model for %s\"", ",", "\n", "pair_task", ".", "_name", ",", "\n", "task", ".", "_name", ",", "\n", ")", "\n", "test_metric_dict", "[", "pair_task", ".", "_name", "]", "=", "{", "}", "\n", "test_metrics", ",", "_", "=", "evaluate", "(", "\n", "model", "=", "best_model", ",", "\n", "task_name", "=", "pair_task", ".", "_name", ",", "\n", "instances", "=", "pair_task", ".", "_test_data", ",", "\n", "data_iterator", "=", "pair_task", ".", "_data_iterator", ",", "\n", "cuda_device", "=", "multi_task_trainer", ".", "_cuda_device", ",", "\n", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.commands.train.train_model": [[195, 335], ["allennlp.training.util.create_serialization_dir", "params.to_file", "params.params.pop", "os.path.join", "train._train_worker", "allennlp.models.archival.archive_model", "params.params.pop.pop", "params.params.pop.pop", "allennlp.common.checks.check_for_gpu", "params.params.pop.pop", "params.params.pop.pop", "len", "logging.info", "torch.spawn", "allennlp.models.archival.archive_model", "allennlp.models.model.Model.load", "isinstance", "allennlp.common.checks.ConfigurationError", "params.get().get", "allennlp.training.util.make_vocab_from_params", "len", "params.duplicate", "os.path.join", "params.get", "params.duplicate", "allennlp.common.Params"], "function", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.training.util.create_serialization_dir", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.params.Params.to_file", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.params.Params.pop", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.dataset_readers.multiprocess_dataset_reader.QIterable.join", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.commands.train._train_worker", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.models.archival.archive_model", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.params.Params.pop", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.params.Params.pop", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.checks.check_for_gpu", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.params.Params.pop", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.params.Params.pop", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.dataset_readers.multiprocess_dataset_reader.logger.info", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.models.archival.archive_model", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.token_embedders.bert_token_embedder.PretrainedBertModel.load", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.params.Params.get", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.training.util.make_vocab_from_params", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.params.Params.duplicate", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.dataset_readers.multiprocess_dataset_reader.QIterable.join", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.params.Params.get", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.params.Params.duplicate"], ["\n", "", "", "metrics", "[", "task", ".", "_name", "]", "[", "\"test\"", "]", "=", "deepcopy", "(", "test_metric_dict", ")", "\n", "logger", ".", "info", "(", "\"Finished evaluation of task %s.\"", ",", "task", ".", "_name", ")", "\n", "\n", "### Dump validation and possibly test metrics ###", "\n", "", "metrics_json", "=", "json", ".", "dumps", "(", "metrics", ",", "indent", "=", "2", ")", "\n", "with", "open", "(", "os", ".", "path", ".", "join", "(", "serialization_dir", ",", "\"metrics.json\"", ")", ",", "\"w\"", ")", "as", "metrics_file", ":", "\n", "        ", "metrics_file", ".", "write", "(", "metrics_json", ")", "\n", "", "logger", ".", "info", "(", "\"Metrics: %s\"", ",", "metrics_json", ")", "\n", "\n", "return", "metrics", "\n", "\n", "\n", "", "if", "__name__", "==", "\"__main__\"", ":", "\n", "# Parse arguments", "\n", "    ", "parser", "=", "argparse", ".", "ArgumentParser", "(", ")", "\n", "parser", ".", "add_argument", "(", "\n", "\"-s\"", ",", "\n", "\"--serialization_dir\"", ",", "\n", "required", "=", "True", ",", "\n", "help", "=", "\"Directory in which to save the model and its logs.\"", ",", "\n", "type", "=", "str", ",", "\n", ")", "\n", "parser", ".", "add_argument", "(", "\n", "\"-c\"", ",", "\n", "\"--config_file_path\"", ",", "\n", "required", "=", "True", ",", "\n", "help", "=", "\"Path to parameter file describing the multi-tasked model to be trained.\"", ",", "\n", "type", "=", "str", ",", "\n", ")", "\n", "parser", ".", "add_argument", "(", "\n", "\"-r\"", ",", "\n", "\"--recover\"", ",", "\n", "action", "=", "\"store_true\"", ",", "\n", "default", "=", "False", ",", "\n", "help", "=", "\"Recover a previous training from the state in serialization_dir.\"", ",", "\n", ")", "\n", "args", "=", "parser", ".", "parse_args", "(", ")", "\n", "\n", "torch", ".", "set_default_dtype", "(", "torch", ".", "float32", ")", "\n", "\n", "params", "=", "Params", ".", "from_file", "(", "params_file", "=", "args", ".", "config_file_path", ")", "\n", "serialization_dir", "=", "args", ".", "serialization_dir", "\n", "create_serialization_dir", "(", "params", ",", "serialization_dir", ",", "args", ".", "recover", ",", "force", "=", "False", ")", "\n", "\n", "serialization_params", "=", "deepcopy", "(", "params", ")", ".", "as_dict", "(", "quiet", "=", "True", ")", "\n", "with", "open", "(", "os", ".", "path", ".", "join", "(", "serialization_dir", ",", "\"config.json\"", ")", ",", "\"w\"", ")", "as", "param_file", ":", "\n", "        ", "json", ".", "dump", "(", "serialization_params", ",", "param_file", ",", "indent", "=", "4", ")", "\n", "\n", "### Instantiate the different tasks from the param file, load datasets and create vocabulary ###", "\n", "", "tasks", ",", "vocab", "=", "tasks_and_vocab_from_params", "(", "\n", "params", "=", "params", ",", "serialization_dir", "=", "serialization_dir", "\n", ")", "\n", "\n", "### Load the data iterators for each task ###", "\n", "tasks", "=", "create_and_set_iterators", "(", "params", "=", "params", ",", "task_list", "=", "tasks", ",", "vocab", "=", "vocab", ")", "\n", "\n", "### Create model ###", "\n", "model_params", "=", "params", ".", "pop", "(", "\"model\"", ")", "\n", "model", "=", "Model", ".", "from_params", "(", "vocab", "=", "vocab", ",", "params", "=", "model_params", ",", "regularizer", "=", "None", ")", "\n", "\n", "### Create multi-task trainer ###", "\n", "multi_task_trainer_params", "=", "params", ".", "pop", "(", "\"multi_task_trainer\"", ")", "\n", "trainer", "=", "MultiTaskTrainer", ".", "from_params", "(", "\n", "model", "=", "model", ",", "\n", "task_list", "=", "tasks", ",", "\n", "serialization_dir", "=", "serialization_dir", ",", "\n", "params", "=", "multi_task_trainer_params", ",", "\n", ")", "\n", "\n", "### Launch training ###", "\n", "metrics", "=", "train_model", "(", "multi_task_trainer", "=", "trainer", ",", "recover", "=", "args", ".", "recover", ")", "\n", "if", "metrics", "is", "not", "None", ":", "\n", "        ", "logging", ".", "info", "(", "\"Training is finished ! Let's have a drink. It's on the house !\"", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.commands.train._train_worker": [[337, 471], ["allennlp.common.util.prepare_global_logging", "allennlp.common.util.prepare_environment", "TrainModel.from_params", "len", "str", "torch.cuda.set_device", "torch.cuda.set_device", "torch.cuda.set_device", "torch.init_process_group", "logging.info", "TrainModel.from_params.run", "TrainModel.from_params.finish", "int", "torch.barrier", "allennlp.common.util.import_submodules", "os.path.exists", "logging.info", "allennlp.models.archival.archive_model", "os.path.join"], "function", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.util.prepare_global_logging", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.util.prepare_environment", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.tokenizers.tokenizer.Tokenizer.from_params", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.None.build_dgl_graph.set_device", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.None.build_dgl_graph.set_device", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.None.build_dgl_graph.set_device", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.dataset_readers.multiprocess_dataset_reader.logger.info", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.commands.train.TrainModel.run", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.commands.train.TrainModel.finish", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.util.import_submodules", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.dataset_readers.multiprocess_dataset_reader.logger.info", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.models.archival.archive_model", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.dataset_readers.multiprocess_dataset_reader.QIterable.join"], []], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.commands.elmo.Elmo.add_subparser": [[108, 192], ["parser.add_parser", "parser.add_parser.add_argument", "parser.add_parser.add_argument", "parser.add_parser.add_mutually_exclusive_group", "parser.add_parser.add_mutually_exclusive_group.add_argument", "parser.add_parser.add_mutually_exclusive_group.add_argument", "parser.add_parser.add_mutually_exclusive_group.add_argument", "parser.add_parser.add_argument", "parser.add_parser.add_argument", "parser.add_parser.add_argument", "parser.add_parser.add_argument", "parser.add_parser.add_argument", "parser.add_parser.add_argument", "parser.add_parser.add_argument", "parser.add_parser.add_argument", "parser.add_parser.set_defaults", "argparse.FileType"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.commands.__init__.ArgumentParserWithDefaults.add_argument", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.commands.__init__.ArgumentParserWithDefaults.add_argument", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.commands.__init__.ArgumentParserWithDefaults.add_argument", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.commands.__init__.ArgumentParserWithDefaults.add_argument", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.commands.__init__.ArgumentParserWithDefaults.add_argument", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.commands.__init__.ArgumentParserWithDefaults.add_argument", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.commands.__init__.ArgumentParserWithDefaults.add_argument", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.commands.__init__.ArgumentParserWithDefaults.add_argument", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.commands.__init__.ArgumentParserWithDefaults.add_argument", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.commands.__init__.ArgumentParserWithDefaults.add_argument", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.commands.__init__.ArgumentParserWithDefaults.add_argument", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.commands.__init__.ArgumentParserWithDefaults.add_argument", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.commands.__init__.ArgumentParserWithDefaults.add_argument"], [")", "->", "None", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "\n", "logger", ".", "info", "(", "\"Initializing ELMo\"", ")", "\n", "if", "module", "is", "not", "None", ":", "\n", "            ", "if", "options_file", "is", "not", "None", "or", "weight_file", "is", "not", "None", ":", "\n", "                ", "raise", "ConfigurationError", "(", "\n", "\"Don't provide options_file or weight_file with module\"", "\n", ")", "\n", "", "self", ".", "_elmo_lstm", "=", "module", "\n", "", "else", ":", "\n", "            ", "self", ".", "_elmo_lstm", "=", "_ElmoBiLm", "(", "\n", "options_file", ",", "\n", "weight_file", ",", "\n", "requires_grad", "=", "requires_grad", ",", "\n", "vocab_to_cache", "=", "vocab_to_cache", ",", "\n", ")", "\n", "", "self", ".", "_has_cached_vocab", "=", "vocab_to_cache", "is", "not", "None", "\n", "self", ".", "_keep_sentence_boundaries", "=", "keep_sentence_boundaries", "\n", "self", ".", "_dropout", "=", "Dropout", "(", "p", "=", "dropout", ")", "\n", "self", ".", "_scalar_mixes", ":", "Any", "=", "[", "]", "\n", "for", "k", "in", "range", "(", "num_output_representations", ")", ":", "\n", "            ", "scalar_mix", "=", "ScalarMix", "(", "\n", "self", ".", "_elmo_lstm", ".", "num_layers", ",", "\n", "do_layer_norm", "=", "do_layer_norm", ",", "\n", "initial_scalar_parameters", "=", "scalar_mix_parameters", ",", "\n", "trainable", "=", "scalar_mix_parameters", "is", "None", ",", "\n", ")", "\n", "self", ".", "add_module", "(", "\"scalar_mix_{}\"", ".", "format", "(", "k", ")", ",", "scalar_mix", ")", "\n", "self", ".", "_scalar_mixes", ".", "append", "(", "scalar_mix", ")", "\n", "\n", "", "", "def", "get_output_dim", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "_elmo_lstm", ".", "get_output_dim", "(", ")", "\n", "\n", "", "def", "forward", "(", "\n", "self", ",", "inputs", ":", "torch", ".", "Tensor", ",", "word_inputs", ":", "torch", ".", "Tensor", "=", "None", "\n", ")", "->", "Dict", "[", "str", ",", "Union", "[", "torch", ".", "Tensor", ",", "List", "[", "torch", ".", "Tensor", "]", "]", "]", ":", "\n", "        ", "\"\"\"\n        # Parameters\n\n        inputs : `torch.Tensor`, required.\n        Shape `(batch_size, timesteps, 50)` of character ids representing the current batch.\n        word_inputs : `torch.Tensor`, required.\n            If you passed a cached vocab, you can in addition pass a tensor of shape\n            `(batch_size, timesteps)`, which represent word ids which have been pre-cached.\n\n        # Returns\n\n        Dict with keys:\n        `'elmo_representations'` : `List[torch.Tensor]`\n            A `num_output_representations` list of ELMo representations for the input sequence.\n            Each representation is shape `(batch_size, timesteps, embedding_dim)`\n        `'mask'`:  `torch.Tensor`\n            Shape `(batch_size, timesteps)` long tensor with sequence mask.\n        \"\"\"", "\n", "# reshape the input if needed", "\n", "original_shape", "=", "inputs", ".", "size", "(", ")", "\n", "if", "len", "(", "original_shape", ")", ">", "3", ":", "\n", "            ", "timesteps", ",", "num_characters", "=", "original_shape", "[", "-", "2", ":", "]", "\n", "reshaped_inputs", "=", "inputs", ".", "view", "(", "-", "1", ",", "timesteps", ",", "num_characters", ")", "\n", "", "else", ":", "\n", "            ", "reshaped_inputs", "=", "inputs", "\n", "\n", "", "if", "word_inputs", "is", "not", "None", ":", "\n", "            ", "original_word_size", "=", "word_inputs", ".", "size", "(", ")", "\n", "if", "self", ".", "_has_cached_vocab", "and", "len", "(", "original_word_size", ")", ">", "2", ":", "\n", "                ", "reshaped_word_inputs", "=", "word_inputs", ".", "view", "(", "-", "1", ",", "original_word_size", "[", "-", "1", "]", ")", "\n", "", "elif", "not", "self", ".", "_has_cached_vocab", ":", "\n", "                ", "logger", ".", "warning", "(", "\n", "\"Word inputs were passed to ELMo but it does not have a cached vocab.\"", "\n", ")", "\n", "reshaped_word_inputs", "=", "None", "\n", "", "else", ":", "\n", "                ", "reshaped_word_inputs", "=", "word_inputs", "\n", "", "", "else", ":", "\n", "            ", "reshaped_word_inputs", "=", "word_inputs", "\n", "\n", "# run the biLM", "\n", "", "bilm_output", "=", "self", ".", "_elmo_lstm", "(", "reshaped_inputs", ",", "reshaped_word_inputs", ")", "\n", "layer_activations", "=", "bilm_output", "[", "\"activations\"", "]", "\n", "mask_with_bos_eos", "=", "bilm_output", "[", "\"mask\"", "]", "\n", "\n", "# compute the elmo representations", "\n", "representations", "=", "[", "]", "\n", "for", "i", "in", "range", "(", "len", "(", "self", ".", "_scalar_mixes", ")", ")", ":", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.commands.elmo.ElmoEmbedder.__init__": [[199, 222], ["allennlp.data.token_indexers.elmo_indexer.ELMoTokenCharactersIndexer", "logger.info", "allennlp.modules.elmo._ElmoBiLm", "elmo.ElmoEmbedder.elmo_bilm.cuda"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.dataset_readers.multiprocess_dataset_reader.logger.info"], ["processed_mask", "=", "mask_with_bos_eos", "\n", "", "else", ":", "\n", "                ", "(", "\n", "representation_without_bos_eos", ",", "\n", "mask_without_bos_eos", ",", "\n", ")", "=", "remove_sentence_boundaries", "(", "\n", "representation_with_bos_eos", ",", "mask_with_bos_eos", "\n", ")", "\n", "processed_representation", "=", "representation_without_bos_eos", "\n", "processed_mask", "=", "mask_without_bos_eos", "\n", "", "representations", ".", "append", "(", "self", ".", "_dropout", "(", "processed_representation", ")", ")", "\n", "\n", "# reshape if necessary", "\n", "", "if", "word_inputs", "is", "not", "None", "and", "len", "(", "original_word_size", ")", ">", "2", ":", "\n", "            ", "mask", "=", "processed_mask", ".", "view", "(", "original_word_size", ")", "\n", "elmo_representations", "=", "[", "\n", "representation", ".", "view", "(", "original_word_size", "+", "(", "-", "1", ",", ")", ")", "\n", "for", "representation", "in", "representations", "\n", "]", "\n", "", "elif", "len", "(", "original_shape", ")", ">", "3", ":", "\n", "            ", "mask", "=", "processed_mask", ".", "view", "(", "original_shape", "[", ":", "-", "1", "]", ")", "\n", "elmo_representations", "=", "[", "\n", "representation", ".", "view", "(", "original_shape", "[", ":", "-", "1", "]", "+", "(", "-", "1", ",", ")", ")", "\n", "for", "representation", "in", "representations", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.commands.elmo.ElmoEmbedder.batch_to_embeddings": [[223, 258], ["allennlp.modules.elmo.batch_to_ids", "elmo.ElmoEmbedder.elmo_bilm", "torch.cat", "character_ids.cuda.cuda.cuda", "allennlp.nn.util.remove_sentence_boundaries", "ele[].unsqueeze"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.modules.elmo.batch_to_ids", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.nn.util.remove_sentence_boundaries"], ["]", "\n", "", "else", ":", "\n", "            ", "mask", "=", "processed_mask", "\n", "elmo_representations", "=", "representations", "\n", "\n", "", "return", "{", "\"elmo_representations\"", ":", "elmo_representations", ",", "\"mask\"", ":", "mask", "}", "\n", "\n", "\n", "", "", "def", "batch_to_ids", "(", "batch", ":", "List", "[", "List", "[", "str", "]", "]", ")", "->", "torch", ".", "Tensor", ":", "\n", "    ", "\"\"\"\n    Converts a batch of tokenized sentences to a tensor representing the sentences with encoded characters\n    (len(batch), max sentence length, max word length).\n\n    # Parameters\n\n    batch : `List[List[str]]`, required\n        A list of tokenized sentences.\n\n    # Returns\n\n        A tensor of padded character ids.\n    \"\"\"", "\n", "instances", "=", "[", "]", "\n", "indexer", "=", "ELMoTokenCharactersIndexer", "(", ")", "\n", "for", "sentence", "in", "batch", ":", "\n", "        ", "tokens", "=", "[", "Token", "(", "token", ")", "for", "token", "in", "sentence", "]", "\n", "field", "=", "TextField", "(", "tokens", ",", "{", "\"character_ids\"", ":", "indexer", "}", ")", "\n", "instance", "=", "Instance", "(", "{", "\"elmo\"", ":", "field", "}", ")", "\n", "instances", ".", "append", "(", "instance", ")", "\n", "\n", "", "dataset", "=", "Batch", "(", "instances", ")", "\n", "vocab", "=", "Vocabulary", "(", ")", "\n", "dataset", ".", "index_instances", "(", "vocab", ")", "\n", "return", "dataset", ".", "as_tensor_dict", "(", ")", "[", "\"elmo\"", "]", "[", "\"character_ids\"", "]", "[", "\"tokens\"", "]", "\n", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.commands.elmo.ElmoEmbedder.embed_sentence": [[259, 277], ["elmo.ElmoEmbedder.embed_batch"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.commands.elmo.ElmoEmbedder.embed_batch"], ["", "class", "_ElmoCharacterEncoder", "(", "torch", ".", "nn", ".", "Module", ")", ":", "\n", "    "]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.commands.elmo.ElmoEmbedder.embed_batch": [[278, 313], ["elmo_embeddings.append", "elmo.ElmoEmbedder.batch_to_embeddings", "range", "elmo.empty_embedding", "len", "int", "mask[].sum", "elmo_embeddings.append", "elmo_embeddings.append", "elmo.empty_embedding", "embeddings[].detach().cpu().numpy", "embeddings[].detach().cpu", "embeddings[].detach"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.commands.elmo.ElmoEmbedder.batch_to_embeddings", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.commands.elmo.empty_embedding", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.commands.elmo.empty_embedding"], ["\n", "\n", "def", "__init__", "(", "\n", "self", ",", "options_file", ":", "str", ",", "weight_file", ":", "str", ",", "requires_grad", ":", "bool", "=", "False", "\n", ")", "->", "None", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "\n", "with", "open", "(", "cached_path", "(", "options_file", ")", ",", "\"r\"", ")", "as", "fin", ":", "\n", "            ", "self", ".", "_options", "=", "json", ".", "load", "(", "fin", ")", "\n", "", "self", ".", "_weight_file", "=", "weight_file", "\n", "\n", "self", ".", "output_dim", "=", "self", ".", "_options", "[", "\"lstm\"", "]", "[", "\"projection_dim\"", "]", "\n", "self", ".", "requires_grad", "=", "requires_grad", "\n", "\n", "self", ".", "_load_weights", "(", ")", "\n", "\n", "# Cache the arrays for use in forward -- +1 due to masking.", "\n", "self", ".", "_beginning_of_sentence_characters", "=", "torch", ".", "from_numpy", "(", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.commands.elmo.ElmoEmbedder.embed_sentences": [[314, 336], ["allennlp.common.util.lazy_groups_of", "iter", "elmo.ElmoEmbedder.embed_batch"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.util.lazy_groups_of", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.commands.elmo.ElmoEmbedder.embed_batch"], ["numpy", ".", "array", "(", "ELMoCharacterMapper", ".", "beginning_of_sentence_characters", ")", "+", "1", "\n", ")", "\n", "self", ".", "_end_of_sentence_characters", "=", "torch", ".", "from_numpy", "(", "\n", "numpy", ".", "array", "(", "ELMoCharacterMapper", ".", "end_of_sentence_characters", ")", "+", "1", "\n", ")", "\n", "\n", "", "def", "get_output_dim", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "output_dim", "\n", "\n", "", "@", "overrides", "\n", "def", "forward", "(", "self", ",", "inputs", ":", "torch", ".", "Tensor", ")", "->", "Dict", "[", "str", ",", "torch", ".", "Tensor", "]", ":", "\n", "        "]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.commands.elmo.ElmoEmbedder.embed_file": [[337, 433], ["logger.info", "input_file.close", "line.strip", "allennlp.common.checks.ConfigurationError", "sentence.split", "logger.warning", "zip", "h5py.File", "allennlp.common.tqdm.Tqdm.tqdm", "enumerate", "elmo.ElmoEmbedder.embed_sentences", "fout.create_dataset", "fout.create_dataset", "json.dumps", "str", "enumerate", "allennlp.common.checks.ConfigurationError", "str", "elmo.ElmoEmbedder.embed_sentences", "fout.keys", "h5py.special_dtype", "int", "numpy.average"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.dataset_readers.multiprocess_dataset_reader.logger.info", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.token_embedders.embedding.EmbeddingsTextFile.close", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.tqdm.Tqdm.tqdm", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.commands.elmo.ElmoEmbedder.embed_sentences", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.commands.elmo.ElmoEmbedder.embed_sentences"], ["\n", "# Add BOS/EOS", "\n", "mask", "=", "(", "(", "inputs", ">", "0", ")", ".", "long", "(", ")", ".", "sum", "(", "dim", "=", "-", "1", ")", ">", "0", ")", ".", "long", "(", ")", "\n", "character_ids_with_bos_eos", ",", "mask_with_bos_eos", "=", "add_sentence_boundary_token_ids", "(", "\n", "inputs", ",", "\n", "mask", ",", "\n", "self", ".", "_beginning_of_sentence_characters", ",", "\n", "self", ".", "_end_of_sentence_characters", ",", "\n", ")", "\n", "\n", "# the character id embedding", "\n", "max_chars_per_token", "=", "self", ".", "_options", "[", "\"char_cnn\"", "]", "[", "\"max_characters_per_token\"", "]", "\n", "# (batch_size * sequence_length, max_chars_per_token, embed_dim)", "\n", "character_embedding", "=", "torch", ".", "nn", ".", "functional", ".", "embedding", "(", "\n", "character_ids_with_bos_eos", ".", "view", "(", "-", "1", ",", "max_chars_per_token", ")", ",", "\n", "self", ".", "_char_embedding_weights", ",", "\n", ")", "\n", "\n", "# run convolutions", "\n", "cnn_options", "=", "self", ".", "_options", "[", "\"char_cnn\"", "]", "\n", "if", "cnn_options", "[", "\"activation\"", "]", "==", "\"tanh\"", ":", "\n", "            ", "activation", "=", "torch", ".", "tanh", "\n", "", "elif", "cnn_options", "[", "\"activation\"", "]", "==", "\"relu\"", ":", "\n", "            ", "activation", "=", "torch", ".", "nn", ".", "functional", ".", "relu", "\n", "", "else", ":", "\n", "            ", "raise", "ConfigurationError", "(", "\"Unknown activation\"", ")", "\n", "\n", "# (batch_size * sequence_length, embed_dim, max_chars_per_token)", "\n", "", "character_embedding", "=", "torch", ".", "transpose", "(", "character_embedding", ",", "1", ",", "2", ")", "\n", "convs", "=", "[", "]", "\n", "for", "i", "in", "range", "(", "len", "(", "self", ".", "_convolutions", ")", ")", ":", "\n", "            ", "conv", "=", "getattr", "(", "self", ",", "\"char_conv_{}\"", ".", "format", "(", "i", ")", ")", "\n", "convolved", "=", "conv", "(", "character_embedding", ")", "\n", "# (batch_size * sequence_length, n_filters for this width)", "\n", "convolved", ",", "_", "=", "torch", ".", "max", "(", "convolved", ",", "dim", "=", "-", "1", ")", "\n", "convolved", "=", "activation", "(", "convolved", ")", "\n", "convs", ".", "append", "(", "convolved", ")", "\n", "\n", "# (batch_size * sequence_length, n_filters)", "\n", "", "token_embedding", "=", "torch", ".", "cat", "(", "convs", ",", "dim", "=", "-", "1", ")", "\n", "\n", "# apply the highway layers (batch_size * sequence_length, n_filters)", "\n", "token_embedding", "=", "self", ".", "_highways", "(", "token_embedding", ")", "\n", "\n", "# final projection  (batch_size * sequence_length, embedding_dim)", "\n", "token_embedding", "=", "self", ".", "_projection", "(", "token_embedding", ")", "\n", "\n", "# reshape to (batch_size, sequence_length, embedding_dim)", "\n", "batch_size", ",", "sequence_length", ",", "_", "=", "character_ids_with_bos_eos", ".", "size", "(", ")", "\n", "\n", "return", "{", "\n", "\"mask\"", ":", "mask_with_bos_eos", ",", "\n", "\"token_embedding\"", ":", "token_embedding", ".", "view", "(", "batch_size", ",", "sequence_length", ",", "-", "1", ")", ",", "\n", "}", "\n", "\n", "", "def", "_load_weights", "(", "self", ")", ":", "\n", "        ", "self", ".", "_load_char_embedding", "(", ")", "\n", "self", ".", "_load_cnn_weights", "(", ")", "\n", "self", ".", "_load_highway", "(", ")", "\n", "self", ".", "_load_projection", "(", ")", "\n", "\n", "", "def", "_load_char_embedding", "(", "self", ")", ":", "\n", "        ", "with", "h5py", ".", "File", "(", "cached_path", "(", "self", ".", "_weight_file", ")", ",", "\"r\"", ")", "as", "fin", ":", "\n", "            ", "char_embed_weights", "=", "fin", "[", "\"char_embed\"", "]", "[", "...", "]", "\n", "\n", "", "weights", "=", "numpy", ".", "zeros", "(", "\n", "(", "char_embed_weights", ".", "shape", "[", "0", "]", "+", "1", ",", "char_embed_weights", ".", "shape", "[", "1", "]", ")", ",", "\n", "dtype", "=", "\"float32\"", ",", "\n", ")", "\n", "weights", "[", "1", ":", ",", ":", "]", "=", "char_embed_weights", "\n", "\n", "self", ".", "_char_embedding_weights", "=", "torch", ".", "nn", ".", "Parameter", "(", "\n", "torch", ".", "FloatTensor", "(", "weights", ")", ",", "requires_grad", "=", "self", ".", "requires_grad", "\n", ")", "\n", "\n", "", "def", "_load_cnn_weights", "(", "self", ")", ":", "\n", "        ", "cnn_options", "=", "self", ".", "_options", "[", "\"char_cnn\"", "]", "\n", "filters", "=", "cnn_options", "[", "\"filters\"", "]", "\n", "char_embed_dim", "=", "cnn_options", "[", "\"embedding\"", "]", "[", "\"dim\"", "]", "\n", "\n", "convolutions", "=", "[", "]", "\n", "for", "i", ",", "(", "width", ",", "num", ")", "in", "enumerate", "(", "filters", ")", ":", "\n", "            ", "conv", "=", "torch", ".", "nn", ".", "Conv1d", "(", "\n", "in_channels", "=", "char_embed_dim", ",", "\n", "out_channels", "=", "num", ",", "\n", "kernel_size", "=", "width", ",", "\n", "bias", "=", "True", ",", "\n", ")", "\n", "# load the weights", "\n", "with", "h5py", ".", "File", "(", "cached_path", "(", "self", ".", "_weight_file", ")", ",", "\"r\"", ")", "as", "fin", ":", "\n", "                ", "weight", "=", "fin", "[", "\"CNN\"", "]", "[", "\"W_cnn_{}\"", ".", "format", "(", "i", ")", "]", "[", "...", "]", "\n", "bias", "=", "fin", "[", "\"CNN\"", "]", "[", "\"b_cnn_{}\"", ".", "format", "(", "i", ")", "]", "[", "...", "]", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.commands.elmo.empty_embedding": [[194, 196], ["numpy.zeros"], "function", ["None"], ["representation_with_bos_eos", "=", "scalar_mix", "(", "\n", "layer_activations", ",", "mask_with_bos_eos", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.commands.elmo.elmo_command": [[435, 457], ["elmo.ElmoEmbedder", "allennlp.common.util.prepare_global_logging", "os.path.realpath", "torch.no_grad", "elmo.ElmoEmbedder.embed_file", "os.path.dirname"], "function", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.util.prepare_global_logging", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.commands.elmo.ElmoEmbedder.embed_file"], ["", "w_reshaped", "=", "numpy", ".", "transpose", "(", "weight", ".", "squeeze", "(", "axis", "=", "0", ")", ",", "axes", "=", "(", "2", ",", "1", ",", "0", ")", ")", "\n", "if", "w_reshaped", ".", "shape", "!=", "tuple", "(", "conv", ".", "weight", ".", "data", ".", "shape", ")", ":", "\n", "                ", "raise", "ValueError", "(", "\"Invalid weight file\"", ")", "\n", "", "conv", ".", "weight", ".", "data", ".", "copy_", "(", "torch", ".", "FloatTensor", "(", "w_reshaped", ")", ")", "\n", "conv", ".", "bias", ".", "data", ".", "copy_", "(", "torch", ".", "FloatTensor", "(", "bias", ")", ")", "\n", "\n", "conv", ".", "weight", ".", "requires_grad", "=", "self", ".", "requires_grad", "\n", "conv", ".", "bias", ".", "requires_grad", "=", "self", ".", "requires_grad", "\n", "\n", "convolutions", ".", "append", "(", "conv", ")", "\n", "self", ".", "add_module", "(", "\"char_conv_{}\"", ".", "format", "(", "i", ")", ",", "conv", ")", "\n", "\n", "", "self", ".", "_convolutions", "=", "convolutions", "\n", "\n", "", "def", "_load_highway", "(", "self", ")", ":", "\n", "\n", "# the highway layers have same dimensionality as the number of cnn filters", "\n", "        ", "cnn_options", "=", "self", ".", "_options", "[", "\"char_cnn\"", "]", "\n", "filters", "=", "cnn_options", "[", "\"filters\"", "]", "\n", "n_filters", "=", "sum", "(", "f", "[", "1", "]", "for", "f", "in", "filters", ")", "\n", "n_highway", "=", "cnn_options", "[", "\"n_highway\"", "]", "\n", "\n", "# create the layers, and load the weights", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.commands.fine_tune.FineTune.add_subparser": [[73, 152], ["parser.add_parser", "parser.add_parser.add_argument", "parser.add_parser.add_argument", "parser.add_parser.add_argument", "parser.add_parser.add_argument", "parser.add_parser.add_argument", "parser.add_parser.add_argument", "parser.add_parser.add_argument", "parser.add_parser.add_argument", "parser.add_parser.set_defaults"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.commands.__init__.ArgumentParserWithDefaults.add_argument", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.commands.__init__.ArgumentParserWithDefaults.add_argument", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.commands.__init__.ArgumentParserWithDefaults.add_argument", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.commands.__init__.ArgumentParserWithDefaults.add_argument", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.commands.__init__.ArgumentParserWithDefaults.add_argument", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.commands.__init__.ArgumentParserWithDefaults.add_argument", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.commands.__init__.ArgumentParserWithDefaults.add_argument", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.commands.__init__.ArgumentParserWithDefaults.add_argument"], ["    ", "@", "overrides", "\n", "def", "add_subparser", "(", "\n", "self", ",", "parser", ":", "argparse", ".", "_SubParsersAction", "\n", ")", "->", "argparse", ".", "ArgumentParser", ":", "\n", "        ", "description", "=", "\"\"\"Continues training a saved model on a new dataset.\"\"\"", "\n", "subparser", "=", "parser", ".", "add_parser", "(", "\n", "self", ".", "name", ",", "\n", "description", "=", "description", ",", "\n", "help", "=", "\"Continue training a model on a new dataset.\"", ",", "\n", ")", "\n", "\n", "subparser", ".", "add_argument", "(", "\n", "\"-m\"", ",", "\n", "\"--model-archive\"", ",", "\n", "required", "=", "True", ",", "\n", "type", "=", "str", ",", "\n", "help", "=", "\"path to the saved model archive from training on the original data\"", ",", "\n", ")", "\n", "\n", "subparser", ".", "add_argument", "(", "\n", "\"-c\"", ",", "\n", "\"--config-file\"", ",", "\n", "required", "=", "True", ",", "\n", "type", "=", "str", ",", "\n", "help", "=", "\"configuration file to use for training. Format is the same as \"", "\n", "'for the \"train\" command, but the \"model\" section is ignored.'", ",", "\n", ")", "\n", "\n", "subparser", ".", "add_argument", "(", "\n", "\"-s\"", ",", "\n", "\"--serialization-dir\"", ",", "\n", "required", "=", "True", ",", "\n", "type", "=", "str", ",", "\n", "help", "=", "\"directory in which to save the fine-tuned model and its logs\"", ",", "\n", ")", "\n", "\n", "subparser", ".", "add_argument", "(", "\n", "\"-o\"", ",", "\n", "\"--overrides\"", ",", "\n", "type", "=", "str", ",", "\n", "default", "=", "\"\"", ",", "\n", "help", "=", "\"a JSON structure used to override the training configuration \"", "\n", "\"(only affects the config_file, _not_ the model_archive)\"", ",", "\n", ")", "\n", "\n", "subparser", ".", "add_argument", "(", "\n", "\"--extend-vocab\"", ",", "\n", "action", "=", "\"store_true\"", ",", "\n", "default", "=", "False", ",", "\n", "help", "=", "\"if specified, we will use the instances in your new dataset to \"", "\n", "\"extend your vocabulary. If pretrained-file was used to initialize \"", "\n", "\"embedding layers, you may also need to pass --embedding-sources-mapping.\"", ",", "\n", ")", "\n", "subparser", ".", "add_argument", "(", "\n", "\"--file-friendly-logging\"", ",", "\n", "action", "=", "\"store_true\"", ",", "\n", "default", "=", "False", ",", "\n", "help", "=", "\"outputs tqdm status on separate lines and slows tqdm refresh rate\"", ",", "\n", ")", "\n", "\n", "subparser", ".", "add_argument", "(", "\n", "\"--batch-weight-key\"", ",", "\n", "type", "=", "str", ",", "\n", "default", "=", "\"\"", ",", "\n", "help", "=", "\"If non-empty, name of metric used to weight the loss on a per-batch basis.\"", ",", "\n", ")", "\n", "\n", "subparser", ".", "add_argument", "(", "\n", "\"--embedding-sources-mapping\"", ",", "\n", "type", "=", "json", ".", "loads", ",", "\n", "default", "=", "\"{}\"", ",", "\n", "help", "=", "\"a JSON dict defining mapping from embedding module path to embedding \"", "\n", "\"pretrained-file used during training. If not passed, and embedding needs to be \"", "\n", "\"extended, we will try to use the original file paths used during training. If \"", "\n", "\"they are not available we will use random vectors for embedding extension.\"", ",", "\n", ")", "\n", "subparser", ".", "set_defaults", "(", "func", "=", "fine_tune_model_from_args", ")", "\n", "\n", "return", "subparser", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.commands.fine_tune.fine_tune_model_from_args": [[154, 167], ["fine_tune.fine_tune_model_from_file_paths"], "function", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.commands.fine_tune.fine_tune_model_from_file_paths"], ["", "", "def", "fine_tune_model_from_args", "(", "args", ":", "argparse", ".", "Namespace", ")", ":", "\n", "    ", "\"\"\"\n    Just converts from an ``argparse.Namespace`` object to string paths.\n    \"\"\"", "\n", "fine_tune_model_from_file_paths", "(", "\n", "model_archive_path", "=", "args", ".", "model_archive", ",", "\n", "config_file", "=", "args", ".", "config_file", ",", "\n", "serialization_dir", "=", "args", ".", "serialization_dir", ",", "\n", "overrides", "=", "args", ".", "overrides", ",", "\n", "extend_vocab", "=", "args", ".", "extend_vocab", ",", "\n", "file_friendly_logging", "=", "args", ".", "file_friendly_logging", ",", "\n", "batch_weight_key", "=", "args", ".", "batch_weight_key", ",", "\n", "embedding_sources_mapping", "=", "args", ".", "embedding_sources_mapping", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.commands.fine_tune.fine_tune_model_from_file_paths": [[170, 228], ["allennlp.models.load_archive", "allennlp.common.Params.from_file", "allennlp.commands.train.train_model"], "function", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.models.archival.load_archive", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.params.Params.from_file", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.commands.train.train_model"], ["", "def", "fine_tune_model_from_file_paths", "(", "\n", "model_archive_path", ":", "str", ",", "\n", "config_file", ":", "str", ",", "\n", "serialization_dir", ":", "str", ",", "\n", "overrides", ":", "str", "=", "\"\"", ",", "\n", "extend_vocab", ":", "bool", "=", "False", ",", "\n", "file_friendly_logging", ":", "bool", "=", "False", ",", "\n", "recover", ":", "bool", "=", "False", ",", "\n", "force", ":", "bool", "=", "False", ",", "\n", "batch_weight_key", ":", "str", "=", "\"\"", ",", "\n", "embedding_sources_mapping", ":", "Dict", "[", "str", ",", "str", "]", "=", "None", ",", "\n", ")", "->", "Model", ":", "\n", "    ", "\"\"\"\n    A wrapper around :func:`fine_tune_model` which loads the model archive from a file.\n\n    # Parameters\n\n    model_archive_path : ``str``\n        Path to a saved model archive that is the result of running the ``train`` command.\n    config_file : ``str``\n        A configuration file specifying how to continue training.  The format is identical to the\n        configuration file for the ``train`` command, but any contents in the ``model`` section is\n        ignored (as we are using the provided model archive instead).\n    serialization_dir : ``str``\n        The directory in which to save results and logs. We just pass this along to\n        :func:`fine_tune_model`.\n    overrides : ``str``\n        A JSON string that we will use to override values in the input parameter file.\n    extend_vocab : ``bool``, optional (default=False)\n        If ``True``, we use the new instances to extend your vocabulary.\n    file_friendly_logging : ``bool``, optional (default=False)\n        If ``True``, we make our output more friendly to saved model files.  We just pass this\n        along to :func:`fine_tune_model`.\n    recover : ``bool``, optional (default=False)\n        If ``True``, we will try to recover a training run from an existing serialization\n        directory.  This is only intended for use when something actually crashed during the middle\n        of a run.  For continuing training a model on new data, see the ``fine-tune`` command.\n    force : ``bool``, optional (default=False)\n        If ``True``, we will overwrite the serialization directory if it already exists.\n    batch_weight_key : ``str``, optional (default=\"\")\n        If non-empty, name of metric used to weight the loss on a per-batch basis.\n    embedding_sources_mapping : ``Dict[str, str]``, optional (default=None)\n        Mapping from model paths to the pretrained embedding filepaths.\n    \"\"\"", "\n", "# We don't need to pass in `cuda_device` here, because the trainer will call `model.cuda()` if", "\n", "# necessary.", "\n", "archive", "=", "load_archive", "(", "model_archive_path", ")", "\n", "params", "=", "Params", ".", "from_file", "(", "config_file", ",", "overrides", ")", "\n", "return", "train_model", "(", "\n", "model", "=", "archive", ".", "model", ",", "\n", "params", "=", "params", ",", "\n", "serialization_dir", "=", "serialization_dir", ",", "\n", "extend_vocab", "=", "extend_vocab", ",", "\n", "file_friendly_logging", "=", "file_friendly_logging", ",", "\n", "recover", "=", "recover", ",", "\n", "force", "=", "force", ",", "\n", "batch_weight_key", "=", "batch_weight_key", ",", "\n", "embedding_sources_mapping", "=", "embedding_sources_mapping", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.commands.test_install.TestInstall.add_subparser": [[40, 66], ["parser.add_parser", "parser.add_parser.add_argument", "parser.add_parser.add_argument", "parser.add_parser.set_defaults"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.commands.__init__.ArgumentParserWithDefaults.add_argument", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.commands.__init__.ArgumentParserWithDefaults.add_argument"], ["    ", "@", "overrides", "\n", "def", "add_subparser", "(", "\n", "self", ",", "parser", ":", "argparse", ".", "_SubParsersAction", "\n", ")", "->", "argparse", ".", "ArgumentParser", ":", "\n", "\n", "        ", "description", "=", "\"\"\"Test that installation works by running the unit tests.\"\"\"", "\n", "subparser", "=", "parser", ".", "add_parser", "(", "\n", "self", ".", "name", ",", "description", "=", "description", ",", "help", "=", "\"Run the unit tests.\"", "\n", ")", "\n", "\n", "subparser", ".", "add_argument", "(", "\n", "\"--run-all\"", ",", "\n", "action", "=", "\"store_true\"", ",", "\n", "help", "=", "\"By default, we skip tests that are slow \"", "\n", "\"or download large files. This flag will run all tests.\"", ",", "\n", ")", "\n", "subparser", ".", "add_argument", "(", "\n", "\"-k\"", ",", "\n", "type", "=", "str", ",", "\n", "default", "=", "None", ",", "\n", "help", "=", "\"Limit tests by setting pytest -k argument\"", ",", "\n", ")", "\n", "\n", "subparser", ".", "set_defaults", "(", "func", "=", "_run_test", ")", "\n", "\n", "return", "subparser", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.commands.test_install._get_module_root": [[68, 70], ["pathlib.Path"], "function", ["None"], ["", "", "def", "_get_module_root", "(", ")", ":", "\n", "    ", "return", "pathlib", ".", "Path", "(", "allennlp", ".", "__file__", ")", ".", "parent", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.commands.test_install._run_test": [[72, 93], ["logger.info", "test_install._get_module_root", "allennlp.common.util.pushd", "os.path.join", "logger.info", "pytest.main", "sys.exit", "logger.warning"], "function", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.dataset_readers.multiprocess_dataset_reader.logger.info", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.commands.test_install._get_module_root", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.util.pushd", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.dataset_readers.multiprocess_dataset_reader.QIterable.join", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.dataset_readers.multiprocess_dataset_reader.logger.info", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.ai2_internal.run_with_beaker.main"], ["", "def", "_run_test", "(", "args", ":", "argparse", ".", "Namespace", ")", ":", "\n", "    ", "module_parent", "=", "_get_module_root", "(", ")", ".", "parent", "\n", "logger", ".", "info", "(", "\"Changing directory to %s\"", ",", "module_parent", ")", "\n", "with", "pushd", "(", "module_parent", ")", ":", "\n", "        ", "test_dir", "=", "os", ".", "path", ".", "join", "(", "module_parent", ",", "\"allennlp\"", ")", "\n", "logger", ".", "info", "(", "\"Running tests at %s\"", ",", "test_dir", ")", "\n", "\n", "if", "args", ".", "k", ":", "\n", "            ", "pytest_k", "=", "[", "\"-k\"", ",", "args", ".", "k", "]", "\n", "pytest_m", "=", "[", "\"-m\"", ",", "\"not java\"", "]", "\n", "if", "args", ".", "run_all", ":", "\n", "                ", "logger", ".", "warning", "(", "\"the argument '-k' overwrites '--run-all'.\"", ")", "\n", "", "", "elif", "args", ".", "run_all", ":", "\n", "            ", "pytest_k", "=", "[", "]", "\n", "pytest_m", "=", "[", "]", "\n", "", "else", ":", "\n", "            ", "pytest_k", "=", "[", "\"-k\"", ",", "\"not sniff_test\"", "]", "\n", "pytest_m", "=", "[", "\"-m\"", ",", "\"not java\"", "]", "\n", "\n", "", "exit_code", "=", "pytest", ".", "main", "(", "[", "test_dir", ",", "\"--color=no\"", "]", "+", "pytest_k", "+", "pytest_m", ")", "\n", "sys", ".", "exit", "(", "exit_code", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.commands.dry_run.DryRun.add_subparser": [[53, 86], ["parser.add_parser", "parser.add_parser.add_argument", "parser.add_parser.add_argument", "parser.add_parser.add_argument", "parser.add_parser.set_defaults"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.commands.__init__.ArgumentParserWithDefaults.add_argument", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.commands.__init__.ArgumentParserWithDefaults.add_argument", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.commands.__init__.ArgumentParserWithDefaults.add_argument"], ["    ", "@", "overrides", "\n", "def", "add_subparser", "(", "\n", "self", ",", "parser", ":", "argparse", ".", "_SubParsersAction", "\n", ")", "->", "argparse", ".", "ArgumentParser", ":", "\n", "        ", "description", "=", "\"\"\"Create a vocabulary, compute dataset statistics and other training utilities.\"\"\"", "\n", "subparser", "=", "parser", ".", "add_parser", "(", "\n", "self", ".", "name", ",", "\n", "description", "=", "description", ",", "\n", "help", "=", "\"Create a vocabulary, compute dataset statistics and other training utilities.\"", ",", "\n", ")", "\n", "subparser", ".", "add_argument", "(", "\n", "\"param_path\"", ",", "\n", "type", "=", "str", ",", "\n", "help", "=", "\"path to parameter file describing the model and its inputs\"", ",", "\n", ")", "\n", "subparser", ".", "add_argument", "(", "\n", "\"-s\"", ",", "\n", "\"--serialization-dir\"", ",", "\n", "required", "=", "True", ",", "\n", "type", "=", "str", ",", "\n", "help", "=", "\"directory in which to save the output of the dry run.\"", ",", "\n", ")", "\n", "subparser", ".", "add_argument", "(", "\n", "\"-o\"", ",", "\n", "\"--overrides\"", ",", "\n", "type", "=", "str", ",", "\n", "default", "=", "\"\"", ",", "\n", "help", "=", "\"a JSON structure used to override the experiment configuration\"", ",", "\n", ")", "\n", "\n", "subparser", ".", "set_defaults", "(", "func", "=", "dry_run_from_args", ")", "\n", "\n", "return", "subparser", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.commands.dry_run.dry_run_from_args": [[88, 99], ["allennlp.common.params.Params.from_file", "dry_run.dry_run_from_params"], "function", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.params.Params.from_file", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.commands.dry_run.dry_run_from_params"], ["", "", "def", "dry_run_from_args", "(", "args", ":", "argparse", ".", "Namespace", ")", ":", "\n", "    ", "\"\"\"\n    Just converts from an ``argparse.Namespace`` object to params.\n    \"\"\"", "\n", "parameter_path", "=", "args", ".", "param_path", "\n", "serialization_dir", "=", "args", ".", "serialization_dir", "\n", "overrides_", "=", "args", ".", "overrides", "\n", "\n", "params", "=", "Params", ".", "from_file", "(", "parameter_path", ",", "overrides_", ")", "\n", "\n", "dry_run_from_params", "(", "params", ",", "serialization_dir", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.commands.dry_run.dry_run_from_params": [[101, 151], ["allennlp.common.util.prepare_environment", "params.pop", "os.makedirs", "os.path.join", "allennlp.training.util.datasets_from_params", "set", "logger.info", "allennlp.data.Vocabulary.from_params", "allennlp.data.batch.Batch", "allennlp.data.batch.Batch.index_instances", "allennlp.data.batch.Batch.print_statistics", "Vocabulary.from_params.print_statistics", "logger.info", "Vocabulary.from_params.save_to_files", "allennlp.models.Model.from_params", "params.pop", "params.pop.pop", "Model.from_params.named_parameters", "allennlp.common.util.log_frozen_and_tunable_parameter_names", "os.path.isdir", "allennlp.common.checks.ConfigurationError", "params.pop", "any", "os.listdir", "allennlp.common.checks.ConfigurationError", "allennlp.training.util.datasets_from_params.items", "params.pop", "parameter.requires_grad_", "re.search"], "function", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.util.prepare_environment", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.params.Params.pop", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.dataset_readers.multiprocess_dataset_reader.QIterable.join", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.training.util.datasets_from_params", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.dataset_readers.multiprocess_dataset_reader.logger.info", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.tokenizers.tokenizer.Tokenizer.from_params", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.data.batch.Batch.index_instances", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.data.batch.Batch.print_statistics", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.data.batch.Batch.print_statistics", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.dataset_readers.multiprocess_dataset_reader.logger.info", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.data.vocabulary.Vocabulary.save_to_files", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.tokenizers.tokenizer.Tokenizer.from_params", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.params.Params.pop", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.params.Params.pop", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.util.log_frozen_and_tunable_parameter_names", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.params.Params.pop", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.params.Params.pop", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.nn.beam_search.BeamSearch.search"], ["", "def", "dry_run_from_params", "(", "params", ":", "Params", ",", "serialization_dir", ":", "str", ")", "->", "None", ":", "\n", "    ", "prepare_environment", "(", "params", ")", "\n", "\n", "vocab_params", "=", "params", ".", "pop", "(", "\"vocabulary\"", ",", "{", "}", ")", "\n", "os", ".", "makedirs", "(", "serialization_dir", ",", "exist_ok", "=", "True", ")", "\n", "vocab_dir", "=", "os", ".", "path", ".", "join", "(", "serialization_dir", ",", "\"vocabulary\"", ")", "\n", "\n", "if", "os", ".", "path", ".", "isdir", "(", "vocab_dir", ")", "and", "os", ".", "listdir", "(", "vocab_dir", ")", "is", "not", "None", ":", "\n", "        ", "raise", "ConfigurationError", "(", "\n", "\"The 'vocabulary' directory in the provided serialization directory is non-empty\"", "\n", ")", "\n", "\n", "", "all_datasets", "=", "datasets_from_params", "(", "params", ")", "\n", "datasets_for_vocab_creation", "=", "set", "(", "\n", "params", ".", "pop", "(", "\"datasets_for_vocab_creation\"", ",", "all_datasets", ")", "\n", ")", "\n", "\n", "for", "dataset", "in", "datasets_for_vocab_creation", ":", "\n", "        ", "if", "dataset", "not", "in", "all_datasets", ":", "\n", "            ", "raise", "ConfigurationError", "(", "f\"invalid 'dataset_for_vocab_creation' {dataset}\"", ")", "\n", "\n", "", "", "logger", ".", "info", "(", "\n", "\"From dataset instances, %s will be considered for vocabulary creation.\"", ",", "\n", "\", \"", ".", "join", "(", "datasets_for_vocab_creation", ")", ",", "\n", ")", "\n", "\n", "instances", "=", "[", "\n", "instance", "\n", "for", "key", ",", "dataset", "in", "all_datasets", ".", "items", "(", ")", "\n", "for", "instance", "in", "dataset", "\n", "if", "key", "in", "datasets_for_vocab_creation", "\n", "]", "\n", "\n", "vocab", "=", "Vocabulary", ".", "from_params", "(", "vocab_params", ",", "instances", "=", "instances", ")", "\n", "dataset", "=", "Batch", "(", "instances", ")", "\n", "dataset", ".", "index_instances", "(", "vocab", ")", "\n", "dataset", ".", "print_statistics", "(", ")", "\n", "vocab", ".", "print_statistics", "(", ")", "\n", "\n", "logger", ".", "info", "(", "f\"writing the vocabulary to {vocab_dir}.\"", ")", "\n", "vocab", ".", "save_to_files", "(", "vocab_dir", ")", "\n", "\n", "model", "=", "Model", ".", "from_params", "(", "vocab", "=", "vocab", ",", "params", "=", "params", ".", "pop", "(", "\"model\"", ")", ")", "\n", "trainer_params", "=", "params", ".", "pop", "(", "\"trainer\"", ")", "\n", "no_grad_regexes", "=", "trainer_params", ".", "pop", "(", "\"no_grad\"", ",", "(", ")", ")", "\n", "for", "name", ",", "parameter", "in", "model", ".", "named_parameters", "(", ")", ":", "\n", "        ", "if", "any", "(", "re", ".", "search", "(", "regex", ",", "name", ")", "for", "regex", "in", "no_grad_regexes", ")", ":", "\n", "            ", "parameter", ".", "requires_grad_", "(", "False", ")", "\n", "\n", "", "", "log_frozen_and_tunable_parameter_names", "(", "model", ")", "\n", "", ""]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.commands.__init__.ArgumentParserWithDefaults._is_empty_default": [[32, 39], ["isinstance", "bool"], "methods", ["None"], []], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.commands.__init__.ArgumentParserWithDefaults.add_argument": [[40, 52], ["kwargs.get", "super().add_argument", "kwargs.get", "kwargs.get", "__init__.ArgumentParserWithDefaults._is_empty_default"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.params.Params.get", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.commands.__init__.ArgumentParserWithDefaults.add_argument", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.params.Params.get", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.params.Params.get", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.commands.__init__.ArgumentParserWithDefaults._is_empty_default"], []], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.commands.__init__.create_parser": [[54, 78], ["__init__.ArgumentParserWithDefaults", "__init__.ArgumentParserWithDefaults.add_argument", "ArgumentParserWithDefaults.add_subparsers", "sorted", "allennlp.commands.subcommand.Subcommand.list_available", "allennlp.commands.subcommand.Subcommand.by_name", "Subcommand.by_name.", "subcommand_class.add_subparser", "subcommand.add_subparser.add_argument"], "function", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.commands.__init__.ArgumentParserWithDefaults.add_argument", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.registrable.Registrable.list_available", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.registrable.Registrable.by_name", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.commands.subcommand.Subcommand.add_subparser", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.commands.__init__.ArgumentParserWithDefaults.add_argument"], []], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.commands.__init__.main": [[80, 102], ["allennlp.common.plugins.import_plugins", "__init__.create_parser", "create_parser.parse_args", "dir", "parser.parse_args.func", "create_parser.print_help", "allennlp.common.util.import_submodules"], "function", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.plugins.import_plugins", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.commands.__init__.create_parser", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.util.import_submodules"], []], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.commands.subcommand.Subcommand.add_subparser": [[28, 32], ["None"], "methods", ["None"], ["def", "add_subparser", "(", "\n", "self", ",", "parser", ":", "argparse", ".", "_SubParsersAction", "\n", ")", "->", "argparse", ".", "ArgumentParser", ":", "\n", "        ", "raise", "NotImplementedError", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.commands.subcommand.Subcommand.register": [[33, 53], ["super().register", "super().register."], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.commands.subcommand.Subcommand.register"], ["", "@", "classmethod", "\n", "@", "overrides", "\n", "def", "register", "(", "\n", "cls", ":", "Type", "[", "T", "]", ",", "\n", "name", ":", "str", ",", "\n", "constructor", ":", "Optional", "[", "str", "]", "=", "None", ",", "\n", "exist_ok", ":", "bool", "=", "False", ",", "\n", ")", "->", "Callable", "[", "[", "Type", "[", "T", "]", "]", ",", "Type", "[", "T", "]", "]", ":", "\n", "        ", "super_register_fn", "=", "super", "(", ")", ".", "register", "(", "\n", "name", ",", "constructor", "=", "constructor", ",", "exist_ok", "=", "exist_ok", "\n", ")", "\n", "\n", "def", "add_name_to_reverse_registry", "(", "subclass", ":", "Type", "[", "T", "]", ")", "->", "Type", "[", "T", "]", ":", "\n", "            ", "subclass", "=", "super_register_fn", "(", "subclass", ")", "\n", "# Don't need to check `exist_ok`, as it's done by super.", "\n", "# Also, don't need to delete previous entries if overridden, they can just stay there.", "\n", "cls", ".", "reverse_registry", "[", "subclass", "]", "=", "name", "\n", "return", "subclass", "\n", "\n", "", "return", "add_name_to_reverse_registry", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.commands.subcommand.Subcommand.name": [[54, 57], ["None"], "methods", ["None"], ["", "@", "property", "\n", "def", "name", "(", "self", ")", "->", "str", ":", "\n", "        ", "return", "self", ".", "reverse_registry", "[", "self", ".", "__class__", "]", "\n", "", "", ""]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.testing.test_case.AllenNlpTestCase.setUp": [[26, 44], ["logging.basicConfig", "logging.getLogger().setLevel", "allennlp.common.checks.log_pytorch_version_info", "pathlib.Path", "os.makedirs", "logging.getLogger", "logging.getLogger", "logging.getLogger", "logging.getLogger"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.checks.log_pytorch_version_info"], ["def", "setUp", "(", "self", ")", ":", "\n", "        ", "logging", ".", "basicConfig", "(", "\n", "format", "=", "\"%(asctime)s - %(levelname)s - %(name)s - %(message)s\"", ",", "\n", "level", "=", "logging", ".", "DEBUG", ",", "\n", ")", "\n", "# Disabling some of the more verbose logging statements that typically aren't very helpful", "\n", "# in tests.", "\n", "logging", ".", "getLogger", "(", "\"allennlp.common.params\"", ")", ".", "disabled", "=", "True", "\n", "logging", ".", "getLogger", "(", "\"allennlp.nn.initializers\"", ")", ".", "disabled", "=", "True", "\n", "logging", ".", "getLogger", "(", "\"allennlp.modules.token_embedders.embedding\"", ")", ".", "setLevel", "(", "\n", "logging", ".", "INFO", "\n", ")", "\n", "logging", ".", "getLogger", "(", "\"urllib3.connectionpool\"", ")", ".", "disabled", "=", "True", "\n", "log_pytorch_version_info", "(", ")", "\n", "\n", "self", ".", "TEST_DIR", "=", "pathlib", ".", "Path", "(", "TEST_DIR", ")", "\n", "\n", "os", ".", "makedirs", "(", "self", ".", "TEST_DIR", ",", "exist_ok", "=", "True", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.testing.test_case.AllenNlpTestCase.tearDown": [[45, 47], ["shutil.rmtree"], "methods", ["None"], ["", "def", "tearDown", "(", "self", ")", ":", "\n", "        ", "shutil", ".", "rmtree", "(", "self", ".", "TEST_DIR", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.testing.model_test_case.ModelTestCase.set_up_model": [[21, 44], ["allennlp.common.Params.from_file", "allennlp.data.DatasetReader.from_params", "list", "allennlp.models.Model.from_params", "allennlp.data.batch.Batch", "model_test_case.ModelTestCase.dataset.index_instances", "allennlp.data.DatasetReader.from_params.read", "allennlp.data.Vocabulary.from_params", "allennlp.data.Vocabulary.from_instances", "str"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.params.Params.from_file", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.tokenizers.tokenizer.Tokenizer.from_params", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.tokenizers.tokenizer.Tokenizer.from_params", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.data.batch.Batch.index_instances", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.scripts.convert_openie_to_conll.read", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.tokenizers.tokenizer.Tokenizer.from_params", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.data.vocabulary.Vocabulary.from_instances"], ["def", "set_up_model", "(", "self", ",", "param_file", ",", "dataset_file", ")", ":", "\n", "\n", "        ", "self", ".", "param_file", "=", "param_file", "\n", "params", "=", "Params", ".", "from_file", "(", "self", ".", "param_file", ")", "\n", "\n", "reader", "=", "DatasetReader", ".", "from_params", "(", "params", "[", "\"dataset_reader\"", "]", ")", "\n", "# The dataset reader might be lazy, but a lazy list here breaks some of our tests.", "\n", "instances", "=", "list", "(", "reader", ".", "read", "(", "str", "(", "dataset_file", ")", ")", ")", "\n", "# Use parameters for vocabulary if they are present in the config file, so that choices like", "\n", "# \"non_padded_namespaces\", \"min_count\" etc. can be set if needed.", "\n", "if", "\"vocabulary\"", "in", "params", ":", "\n", "            ", "vocab_params", "=", "params", "[", "\"vocabulary\"", "]", "\n", "vocab", "=", "Vocabulary", ".", "from_params", "(", "params", "=", "vocab_params", ",", "instances", "=", "instances", ")", "\n", "", "else", ":", "\n", "            ", "vocab", "=", "Vocabulary", ".", "from_instances", "(", "instances", ")", "\n", "", "self", ".", "vocab", "=", "vocab", "\n", "self", ".", "instances", "=", "instances", "\n", "self", ".", "model", "=", "Model", ".", "from_params", "(", "vocab", "=", "self", ".", "vocab", ",", "params", "=", "params", "[", "\"model\"", "]", ")", "\n", "\n", "# TODO(joelgrus) get rid of these", "\n", "# (a lot of the model tests use them, so they'll have to be changed)", "\n", "self", ".", "dataset", "=", "Batch", "(", "self", ".", "instances", ")", "\n", "self", ".", "dataset", ".", "index_instances", "(", "self", ".", "vocab", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.testing.model_test_case.ModelTestCase.ensure_model_can_train_save_and_load": [[45, 155], ["allennlp.commands.train.train_model_from_file", "allennlp.commands.train.train_model_from_file.state_dict().keys", "loaded_model.state_dict().keys", "allennlp.common.Params.from_file", "allennlp.data.DatasetReader.from_params", "allennlp.common.Params", "allennlp.data.DataIterator.from_params", "allennlp.data.DataIterator.from_params", "print", "allennlp.data.DatasetReader.from_params.read", "allennlp.data.DataIterator.from_params.index_with", "next", "print", "allennlp.data.DatasetReader.from_params.read", "allennlp.data.DataIterator.from_params.index_with", "next", "model_test_case.ModelTestCase.check_model_computes_gradients_correctly", "next.keys", "allennlp.commands.train.train_model_from_file.eval", "loaded_model.eval", "print", "allennlp.commands.train.train_model_from_file.", "print", "loaded_model", "loaded_model_loss.backward", "allennlp.commands.train.train_model_from_file.keys", "allennlp.models.load_archive", "numpy.testing.assert_allclose", "copy.deepcopy", "allennlp.data.DataIterator.from_params.", "allennlp.data.DataIterator.from_params.", "next.keys", "next.keys", "model_test_case.ModelTestCase.assert_fields_equal", "model_.modules", "model_test_case.ModelTestCase.assert_fields_equal", "allennlp.commands.train.train_model_from_file.state_dict", "loaded_model.state_dict", "[].cpu().numpy", "[].cpu().numpy", "iterator_params.as_dict", "hasattr", "module.reset_states", "[].cpu", "[].cpu", "allennlp.commands.train.train_model_from_file.state_dict", "loaded_model.state_dict"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.commands.train.train_model_from_file", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.params.Params.from_file", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.tokenizers.tokenizer.Tokenizer.from_params", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.tokenizers.tokenizer.Tokenizer.from_params", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.tokenizers.tokenizer.Tokenizer.from_params", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.scripts.convert_openie_to_conll.read", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.iterators.data_iterator.DataIterator.index_with", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.scripts.convert_openie_to_conll.read", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.iterators.data_iterator.DataIterator.index_with", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.testing.model_test_case.ModelTestCase.check_model_computes_gradients_correctly", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.models.archival.load_archive", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.testing.model_test_case.ModelTestCase.assert_fields_equal", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.testing.model_test_case.ModelTestCase.assert_fields_equal", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.learning_rate_schedulers.learning_rate_scheduler._PyTorchLearningRateSchedulerWrapper.state_dict", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.learning_rate_schedulers.learning_rate_scheduler._PyTorchLearningRateSchedulerWrapper.state_dict", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.params.Params.as_dict", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.modules.encoder_base._EncoderBase.reset_states", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.learning_rate_schedulers.learning_rate_scheduler._PyTorchLearningRateSchedulerWrapper.state_dict", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.learning_rate_schedulers.learning_rate_scheduler._PyTorchLearningRateSchedulerWrapper.state_dict"], ["", "def", "ensure_model_can_train_save_and_load", "(", "\n", "self", ",", "\n", "param_file", ":", "str", ",", "\n", "tolerance", ":", "float", "=", "1e-4", ",", "\n", "cuda_device", ":", "int", "=", "-", "1", ",", "\n", "gradients_to_ignore", ":", "Set", "[", "str", "]", "=", "None", ",", "\n", "overrides", ":", "str", "=", "\"\"", ",", "\n", "disable_dropout", ":", "bool", "=", "True", ",", "\n", ")", ":", "\n", "        ", "\"\"\"\n        # Parameters\n\n        param_file : `str`\n            Path to a training configuration file that we will use to train the model for this\n            test.\n        tolerance : `float`, optional (default=1e-4)\n            When comparing model predictions between the originally-trained model and the model\n            after saving and loading, we will use this tolerance value (passed as `rtol` to\n            `numpy.testing.assert_allclose`).\n        cuda_device : `int`, optional (default=-1)\n            The device to run the test on.\n        gradients_to_ignore : `Set[str]`, optional (default=None)\n            This test runs a gradient check to make sure that we're actually computing gradients\n            for all of the parameters in the model.  If you really want to ignore certain\n            parameters when doing that check, you can pass their names here.  This is not\n            recommended unless you're `really` sure you don't need to have non-zero gradients for\n            those parameters (e.g., some of the beam search / state machine models have\n            infrequently-used parameters that are hard to force the model to use in a small test).\n        overrides : `str`, optional (default = \"\")\n            A JSON string that we will use to override values in the input parameter file.\n        disable_dropout : `bool`, optional (default = True)\n            If True we will set all dropout to 0 before checking gradients. (Otherwise, with small\n            datasets, you may get zero gradients because of unlucky dropout.)\n        \"\"\"", "\n", "save_dir", "=", "self", ".", "TEST_DIR", "/", "\"save_and_load_test\"", "\n", "archive_file", "=", "save_dir", "/", "\"model.tar.gz\"", "\n", "model", "=", "train_model_from_file", "(", "param_file", ",", "save_dir", ",", "overrides", "=", "overrides", ")", "\n", "loaded_model", "=", "load_archive", "(", "archive_file", ",", "cuda_device", "=", "cuda_device", ")", ".", "model", "\n", "state_keys", "=", "model", ".", "state_dict", "(", ")", ".", "keys", "(", ")", "\n", "loaded_state_keys", "=", "loaded_model", ".", "state_dict", "(", ")", ".", "keys", "(", ")", "\n", "assert", "state_keys", "==", "loaded_state_keys", "\n", "# First we make sure that the state dict (the parameters) are the same for both models.", "\n", "for", "key", "in", "state_keys", ":", "\n", "            ", "assert_allclose", "(", "\n", "model", ".", "state_dict", "(", ")", "[", "key", "]", ".", "cpu", "(", ")", ".", "numpy", "(", ")", ",", "\n", "loaded_model", ".", "state_dict", "(", ")", "[", "key", "]", ".", "cpu", "(", ")", ".", "numpy", "(", ")", ",", "\n", "err_msg", "=", "key", ",", "\n", ")", "\n", "", "params", "=", "Params", ".", "from_file", "(", "param_file", ",", "params_overrides", "=", "overrides", ")", "\n", "reader", "=", "DatasetReader", ".", "from_params", "(", "params", "[", "\"dataset_reader\"", "]", ")", "\n", "\n", "# Need to duplicate params because Iterator.from_params will consume.", "\n", "iterator_params", "=", "params", "[", "\"iterator\"", "]", "\n", "iterator_params2", "=", "Params", "(", "copy", ".", "deepcopy", "(", "iterator_params", ".", "as_dict", "(", ")", ")", ")", "\n", "\n", "iterator", "=", "DataIterator", ".", "from_params", "(", "iterator_params", ")", "\n", "iterator2", "=", "DataIterator", ".", "from_params", "(", "iterator_params2", ")", "\n", "\n", "# We'll check that even if we index the dataset with each model separately, we still get", "\n", "# the same result out.", "\n", "print", "(", "\"Reading with original model\"", ")", "\n", "model_dataset", "=", "reader", ".", "read", "(", "params", "[", "\"validation_data_path\"", "]", ")", "\n", "iterator", ".", "index_with", "(", "model", ".", "vocab", ")", "\n", "model_batch", "=", "next", "(", "iterator", "(", "model_dataset", ",", "shuffle", "=", "False", ")", ")", "\n", "\n", "print", "(", "\"Reading with loaded model\"", ")", "\n", "loaded_dataset", "=", "reader", ".", "read", "(", "params", "[", "\"validation_data_path\"", "]", ")", "\n", "iterator2", ".", "index_with", "(", "loaded_model", ".", "vocab", ")", "\n", "loaded_batch", "=", "next", "(", "iterator2", "(", "loaded_dataset", ",", "shuffle", "=", "False", ")", ")", "\n", "\n", "# Check gradients are None for non-trainable parameters and check that", "\n", "# trainable parameters receive some gradient if they are trainable.", "\n", "self", ".", "check_model_computes_gradients_correctly", "(", "\n", "model", ",", "model_batch", ",", "gradients_to_ignore", ",", "disable_dropout", "\n", ")", "\n", "\n", "# The datasets themselves should be identical.", "\n", "assert", "model_batch", ".", "keys", "(", ")", "==", "loaded_batch", ".", "keys", "(", ")", "\n", "for", "key", "in", "model_batch", ".", "keys", "(", ")", ":", "\n", "            ", "self", ".", "assert_fields_equal", "(", "model_batch", "[", "key", "]", ",", "loaded_batch", "[", "key", "]", ",", "key", ",", "1e-6", ")", "\n", "\n", "# Set eval mode, to turn off things like dropout, then get predictions.", "\n", "", "model", ".", "eval", "(", ")", "\n", "loaded_model", ".", "eval", "(", ")", "\n", "# Models with stateful RNNs need their states reset to have consistent", "\n", "# behavior after loading.", "\n", "for", "model_", "in", "[", "model", ",", "loaded_model", "]", ":", "\n", "            ", "for", "module", "in", "model_", ".", "modules", "(", ")", ":", "\n", "                ", "if", "hasattr", "(", "module", ",", "\"stateful\"", ")", "and", "module", ".", "stateful", ":", "\n", "                    ", "module", ".", "reset_states", "(", ")", "\n", "", "", "", "print", "(", "\"Predicting with original model\"", ")", "\n", "model_predictions", "=", "model", "(", "**", "model_batch", ")", "\n", "print", "(", "\"Predicting with loaded model\"", ")", "\n", "loaded_model_predictions", "=", "loaded_model", "(", "**", "loaded_batch", ")", "\n", "\n", "# Check loaded model's loss exists and we can compute gradients, for continuing training.", "\n", "loaded_model_loss", "=", "loaded_model_predictions", "[", "\"loss\"", "]", "\n", "assert", "loaded_model_loss", "is", "not", "None", "\n", "loaded_model_loss", ".", "backward", "(", ")", "\n", "\n", "# Both outputs should have the same keys and the values for these keys should be close.", "\n", "for", "key", "in", "model_predictions", ".", "keys", "(", ")", ":", "\n", "            ", "self", ".", "assert_fields_equal", "(", "\n", "model_predictions", "[", "key", "]", ",", "\n", "loaded_model_predictions", "[", "key", "]", ",", "\n", "name", "=", "key", ",", "\n", "tolerance", "=", "tolerance", ",", "\n", ")", "\n", "\n", "", "return", "model", ",", "loaded_model", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.testing.model_test_case.ModelTestCase.assert_fields_equal": [[156, 188], ["isinstance", "numpy.testing.assert_allclose", "isinstance", "field1.detach().cpu().numpy", "field2.detach().cpu().numpy", "isinstance", "field1.keys", "field2.keys", "model_test_case.ModelTestCase.assert_fields_equal", "enumerate", "isinstance", "field1.detach().cpu", "field2.detach().cpu", "len", "len", "zip", "model_test_case.ModelTestCase.assert_fields_equal", "numpy.testing.assert_allclose", "field1.detach", "field2.detach", "str", "print", "type", "type", "getattr", "getattr"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.testing.model_test_case.ModelTestCase.assert_fields_equal", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.testing.model_test_case.ModelTestCase.assert_fields_equal"], ["", "def", "assert_fields_equal", "(", "\n", "self", ",", "field1", ",", "field2", ",", "name", ":", "str", ",", "tolerance", ":", "float", "=", "1e-6", "\n", ")", "->", "None", ":", "\n", "        ", "if", "isinstance", "(", "field1", ",", "torch", ".", "Tensor", ")", ":", "\n", "            ", "assert_allclose", "(", "\n", "field1", ".", "detach", "(", ")", ".", "cpu", "(", ")", ".", "numpy", "(", ")", ",", "\n", "field2", ".", "detach", "(", ")", ".", "cpu", "(", ")", ".", "numpy", "(", ")", ",", "\n", "rtol", "=", "tolerance", ",", "\n", "err_msg", "=", "name", ",", "\n", ")", "\n", "", "elif", "isinstance", "(", "field1", ",", "dict", ")", ":", "\n", "            ", "assert", "field1", ".", "keys", "(", ")", "==", "field2", ".", "keys", "(", ")", "\n", "for", "key", "in", "field1", ":", "\n", "                ", "self", ".", "assert_fields_equal", "(", "\n", "field1", "[", "key", "]", ",", "\n", "field2", "[", "key", "]", ",", "\n", "tolerance", "=", "tolerance", ",", "\n", "name", "=", "name", "+", "\".\"", "+", "str", "(", "key", ")", ",", "\n", ")", "\n", "", "", "elif", "isinstance", "(", "field1", ",", "(", "list", ",", "tuple", ")", ")", ":", "\n", "            ", "assert", "len", "(", "field1", ")", "==", "len", "(", "field2", ")", "\n", "for", "i", ",", "(", "subfield1", ",", "subfield2", ")", "in", "enumerate", "(", "zip", "(", "field1", ",", "field2", ")", ")", ":", "\n", "                ", "self", ".", "assert_fields_equal", "(", "\n", "subfield1", ",", "subfield2", ",", "tolerance", "=", "tolerance", ",", "name", "=", "name", "+", "f\"[{i}]\"", "\n", ")", "\n", "", "", "elif", "isinstance", "(", "field1", ",", "(", "float", ",", "int", ")", ")", ":", "\n", "            ", "assert_allclose", "(", "[", "field1", "]", ",", "[", "field2", "]", ",", "rtol", "=", "tolerance", ",", "err_msg", "=", "name", ")", "\n", "", "else", ":", "\n", "            ", "if", "field1", "!=", "field2", ":", "\n", "                ", "for", "key", "in", "field1", ".", "__dict__", ":", "\n", "                    ", "print", "(", "key", ",", "getattr", "(", "field1", ",", "key", ")", "==", "getattr", "(", "field2", ",", "key", ")", ")", "\n", "", "", "assert", "field1", "==", "field2", ",", "f\"{name}, {type(field1)}, {type(field2)}\"", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.testing.model_test_case.ModelTestCase.check_model_computes_gradients_correctly": [[189, 244], ["print", "model.zero_grad", "model", "result[].backward", "model.named_parameters", "model.named_modules", "torch.zeros", "has_zero_or_none_grads.items", "Exception", "model.named_modules", "isinstance", "parameter.size", "print", "getattr", "setattr", "setattr", "parameter.grad.cpu", "tuple", "parameter.grad.size"], "methods", ["None"], ["", "", "@", "staticmethod", "\n", "def", "check_model_computes_gradients_correctly", "(", "\n", "model", ":", "Model", ",", "\n", "model_batch", ":", "Dict", "[", "str", ",", "Union", "[", "Any", ",", "Dict", "[", "str", ",", "Any", "]", "]", "]", ",", "\n", "params_to_ignore", ":", "Set", "[", "str", "]", "=", "None", ",", "\n", "disable_dropout", ":", "bool", "=", "True", ",", "\n", ")", ":", "\n", "        ", "print", "(", "\"Checking gradients\"", ")", "\n", "model", ".", "zero_grad", "(", ")", "\n", "\n", "original_dropouts", ":", "Dict", "[", "str", ",", "float", "]", "=", "{", "}", "\n", "\n", "if", "disable_dropout", ":", "\n", "# Remember original dropouts so we can restore them.", "\n", "            ", "for", "name", ",", "module", "in", "model", ".", "named_modules", "(", ")", ":", "\n", "                ", "if", "isinstance", "(", "module", ",", "torch", ".", "nn", ".", "Dropout", ")", ":", "\n", "                    ", "original_dropouts", "[", "name", "]", "=", "getattr", "(", "module", ",", "\"p\"", ")", "\n", "setattr", "(", "module", ",", "\"p\"", ",", "0", ")", "\n", "\n", "", "", "", "result", "=", "model", "(", "**", "model_batch", ")", "\n", "result", "[", "\"loss\"", "]", ".", "backward", "(", ")", "\n", "has_zero_or_none_grads", "=", "{", "}", "\n", "for", "name", ",", "parameter", "in", "model", ".", "named_parameters", "(", ")", ":", "\n", "            ", "zeros", "=", "torch", ".", "zeros", "(", "parameter", ".", "size", "(", ")", ")", "\n", "if", "params_to_ignore", "and", "name", "in", "params_to_ignore", ":", "\n", "                ", "continue", "\n", "", "if", "parameter", ".", "requires_grad", ":", "\n", "\n", "                ", "if", "parameter", ".", "grad", "is", "None", ":", "\n", "                    ", "has_zero_or_none_grads", "[", "\n", "name", "\n", "]", "=", "\"No gradient computed (i.e parameter.grad is None)\"", "\n", "\n", "", "elif", "parameter", ".", "grad", ".", "is_sparse", "or", "parameter", ".", "grad", ".", "data", ".", "is_sparse", ":", "\n", "                    ", "pass", "\n", "\n", "# Some parameters will only be partially updated,", "\n", "# like embeddings, so we just check that any gradient is non-zero.", "\n", "", "elif", "(", "parameter", ".", "grad", ".", "cpu", "(", ")", "==", "zeros", ")", ".", "all", "(", ")", ":", "\n", "                    ", "has_zero_or_none_grads", "[", "\n", "name", "\n", "]", "=", "f\"zeros with shape ({tuple(parameter.grad.size())})\"", "\n", "", "", "else", ":", "\n", "                ", "assert", "parameter", ".", "grad", "is", "None", "\n", "\n", "", "", "if", "has_zero_or_none_grads", ":", "\n", "            ", "for", "name", ",", "grad", "in", "has_zero_or_none_grads", ".", "items", "(", ")", ":", "\n", "                ", "print", "(", "f\"Parameter: {name} had incorrect gradient: {grad}\"", ")", "\n", "", "raise", "Exception", "(", "\"Incorrect gradients found. See stdout for more info.\"", ")", "\n", "\n", "# Now restore dropouts if we disabled them.", "\n", "", "if", "disable_dropout", ":", "\n", "            ", "for", "name", ",", "module", "in", "model", ".", "named_modules", "(", ")", ":", "\n", "                ", "if", "name", "in", "original_dropouts", ":", "\n", "                    ", "setattr", "(", "module", ",", "\"p\"", ",", "original_dropouts", "[", "name", "]", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.testing.model_test_case.ModelTestCase.ensure_batch_predictions_are_consistent": [[245, 292], ["model_test_case.ModelTestCase.model.eval", "enumerate", "allennlp.data.batch.Batch", "allennlp.data.batch.Batch.as_tensor_dict", "model_test_case.ModelTestCase.model", "enumerate", "allennlp.data.batch.Batch", "allennlp.data.batch.Batch.as_tensor_dict", "model_test_case.ModelTestCase.model", "single_predictions.append", "allennlp.data.batch.Batch.get_padding_lengths", "instance_predictions.items", "allennlp.data.batch.Batch.get_padding_lengths", "isinstance", "numpy.testing.assert_allclose", "single_predicted.size", "batch_predicted.size", "tuple", "single_predicted.data.numpy", "batch_predicted.data.numpy", "slice", "single_predicted.size"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.data.instance.Instance.as_tensor_dict", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.data.instance.Instance.as_tensor_dict", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.fields.text_field.TextField.get_padding_lengths", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.fields.text_field.TextField.get_padding_lengths"], ["", "", "", "", "def", "ensure_batch_predictions_are_consistent", "(", "\n", "self", ",", "keys_to_ignore", ":", "Iterable", "[", "str", "]", "=", "(", ")", "\n", ")", ":", "\n", "        ", "\"\"\"\n        Ensures that the model performs the same on a batch of instances as on individual instances.\n        Ignores metrics matching the regexp .*loss.* and those specified explicitly.\n\n        # Parameters\n\n        keys_to_ignore : `Iterable[str]`, optional (default=())\n            Names of metrics that should not be taken into account, e.g. \"batch_weight\".\n        \"\"\"", "\n", "self", ".", "model", ".", "eval", "(", ")", "\n", "single_predictions", "=", "[", "]", "\n", "for", "i", ",", "instance", "in", "enumerate", "(", "self", ".", "instances", ")", ":", "\n", "            ", "dataset", "=", "Batch", "(", "[", "instance", "]", ")", "\n", "tensors", "=", "dataset", ".", "as_tensor_dict", "(", "dataset", ".", "get_padding_lengths", "(", ")", ")", "\n", "result", "=", "self", ".", "model", "(", "**", "tensors", ")", "\n", "single_predictions", ".", "append", "(", "result", ")", "\n", "", "full_dataset", "=", "Batch", "(", "self", ".", "instances", ")", "\n", "batch_tensors", "=", "full_dataset", ".", "as_tensor_dict", "(", "full_dataset", ".", "get_padding_lengths", "(", ")", ")", "\n", "batch_predictions", "=", "self", ".", "model", "(", "**", "batch_tensors", ")", "\n", "for", "i", ",", "instance_predictions", "in", "enumerate", "(", "single_predictions", ")", ":", "\n", "            ", "for", "key", ",", "single_predicted", "in", "instance_predictions", ".", "items", "(", ")", ":", "\n", "                ", "tolerance", "=", "1e-6", "\n", "if", "\"loss\"", "in", "key", ":", "\n", "# Loss is particularly unstable; we'll just be satisfied if everything else is", "\n", "# close.", "\n", "                    ", "continue", "\n", "", "if", "key", "in", "keys_to_ignore", ":", "\n", "                    ", "continue", "\n", "", "single_predicted", "=", "single_predicted", "[", "0", "]", "\n", "batch_predicted", "=", "batch_predictions", "[", "key", "]", "[", "i", "]", "\n", "if", "isinstance", "(", "single_predicted", ",", "torch", ".", "Tensor", ")", ":", "\n", "                    ", "if", "single_predicted", ".", "size", "(", ")", "!=", "batch_predicted", ".", "size", "(", ")", ":", "\n", "                        ", "slices", "=", "tuple", "(", "\n", "slice", "(", "0", ",", "size", ")", "for", "size", "in", "single_predicted", ".", "size", "(", ")", "\n", ")", "\n", "batch_predicted", "=", "batch_predicted", "[", "slices", "]", "\n", "", "assert_allclose", "(", "\n", "single_predicted", ".", "data", ".", "numpy", "(", ")", ",", "\n", "batch_predicted", ".", "data", ".", "numpy", "(", ")", ",", "\n", "atol", "=", "tolerance", ",", "\n", "err_msg", "=", "key", ",", "\n", ")", "\n", "", "else", ":", "\n", "                    ", "assert", "single_predicted", "==", "batch_predicted", ",", "key", "\n", "", "", "", "", "", ""]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.scripts.benchmark_iter.Action.__str__": [[120, 122], ["None"], "methods", ["None"], ["def", "__str__", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "name", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.scripts.benchmark_iter.run_periodically": [[32, 40], ["print", "time.sleep", "reader_output.qsize", "iterator_output.qsize"], "function", ["None"], ["def", "run_periodically", "(", "reader_output", ",", "iterator_output", ")", ":", "\n", "    ", "while", "True", ":", "\n", "        ", "message", "=", "(", "\n", "f\"read out q: {reader_output.qsize()} \"", "\n", "+", "f\"it out q: {iterator_output.qsize()}\"", "\n", ")", "\n", "print", "(", "message", ")", "\n", "time", ".", "sleep", "(", "LOGGING_INTERVAL_SECONDS", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.scripts.benchmark_iter.log_iterable": [[42, 85], ["time.perf_counter", "allennlp.training.util.get_batch_size", "multiprocessing.Process.terminate", "multiprocessing.Process", "multiprocessing.Process.start", "time.perf_counter", "print"], "function", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.training.util.get_batch_size", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.dataset_readers.multiprocess_dataset_reader.QIterable.start"], ["", "", "def", "log_iterable", "(", "iterable", ",", "assume_multiprocess_types", ")", ":", "\n", "    ", "start", "=", "time", ".", "perf_counter", "(", ")", "\n", "last", "=", "start", "\n", "periodic_logging_process", "=", "None", "\n", "have_started_periodic_process", "=", "False", "\n", "\n", "batch_count", "=", "0", "\n", "cumulative_batch_size", "=", "0", "\n", "for", "batch", "in", "iterable", ":", "\n", "        ", "batch_count", "+=", "1", "\n", "cumulative_batch_size", "+=", "get_batch_size", "(", "batch", ")", "\n", "\n", "if", "assume_multiprocess_types", "and", "not", "have_started_periodic_process", ":", "\n", "            ", "have_started_periodic_process", "=", "True", "\n", "periodic_logging_process", "=", "Process", "(", "\n", "target", "=", "run_periodically", ",", "\n", "# Pass the queues directly. Passing the iterable naively", "\n", "# won't work because the forked process (in contrast with", "\n", "# threads) has an entirely separate address space.", "\n", "# Presumably this could be worked around with", "\n", "# multiprocessing.managers or similar.", "\n", "args", "=", "(", "\n", "iterable", ".", "gi_frame", ".", "f_locals", "[", "\"qiterable\"", "]", ".", "output_queue", ",", "\n", "iterable", ".", "gi_frame", ".", "f_locals", "[", "\"output_queue\"", "]", ",", "\n", ")", ",", "\n", ")", "\n", "periodic_logging_process", ".", "start", "(", ")", "\n", "\n", "", "if", "batch_count", "%", "BATCH_INTERVAL", "==", "0", ":", "\n", "            ", "end", "=", "time", ".", "perf_counter", "(", ")", "\n", "\n", "msg", "=", "(", "\n", "f\"s/b total: {(end - start) / batch_count:.3f} \"", "\n", "+", "f\"s/b last: {(end - last) / BATCH_INTERVAL:.3f} \"", "\n", "+", "f\"batch count: {batch_count} \"", "\n", "+", "f\"batch size: {cumulative_batch_size / batch_count:.1f} \"", "\n", ")", "\n", "print", "(", "msg", ")", "\n", "\n", "last", "=", "end", "\n", "\n", "", "", "if", "periodic_logging_process", ":", "\n", "        ", "periodic_logging_process", ".", "terminate", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.scripts.benchmark_iter.time_iterable": [[87, 102], ["print", "time.perf_counter", "time.perf_counter", "print"], "function", ["None"], ["", "", "def", "time_iterable", "(", "iterable", ",", "batch_count", ")", ":", "\n", "    ", "assert", "batch_count", ">", "0", "\n", "\n", "print", "(", "\"Starting test\"", ")", "\n", "start", "=", "time", ".", "perf_counter", "(", ")", "\n", "\n", "i", "=", "batch_count", "\n", "for", "_", "in", "iterable", ":", "\n", "        ", "i", "-=", "1", "\n", "if", "i", "==", "0", ":", "\n", "            ", "break", "\n", "", "", "assert", "i", "==", "0", ",", "\"Not enough batches!\"", "\n", "\n", "end", "=", "time", ".", "perf_counter", "(", ")", "\n", "print", "(", "f\"{(end - start)/batch_count:.3f} s/b over {batch_count} batches\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.scripts.benchmark_iter.time_to_first": [[104, 113], ["print", "time.perf_counter", "time.perf_counter", "print"], "function", ["None"], ["", "def", "time_to_first", "(", "iterable", ")", ":", "\n", "    ", "print", "(", "\"Starting test\"", ")", "\n", "start", "=", "time", ".", "perf_counter", "(", ")", "\n", "\n", "for", "_", "in", "iterable", ":", "\n", "        ", "break", "\n", "\n", "", "end", "=", "time", ".", "perf_counter", "(", ")", "\n", "print", "(", "f\"{(end - start):.3f} s/b for first batch\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.scripts.convert_openie_to_conll.main": [[35, 52], ["open", "convert_openie_to_conll.read", "fout.write", "map", "convert_openie_to_conll.convert_sent_to_conll", "convert_openie_to_conll.pad_line_to_ontonotes"], "function", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.scripts.convert_openie_to_conll.read", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.None.predictor.write", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.scripts.convert_openie_to_conll.convert_sent_to_conll", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.scripts.convert_openie_to_conll.pad_line_to_ontonotes"], ["def", "main", "(", "inp_fn", ":", "str", ",", "domain", ":", "str", ",", "out_fn", ":", "str", ")", "->", "None", ":", "\n", "    ", "\"\"\"\n    inp_fn: str, required.\n       Path to file from which to read Open IE extractions in Open IE4's format.\n    domain: str, required.\n       Domain to be used when writing CoNLL format.\n    out_fn: str, required.\n       Path to file to which to write the CoNLL format Open IE extractions.\n    \"\"\"", "\n", "with", "open", "(", "out_fn", ",", "\"w\"", ")", "as", "fout", ":", "\n", "        ", "for", "sent_ls", "in", "read", "(", "inp_fn", ")", ":", "\n", "            ", "fout", ".", "write", "(", "\n", "\"{}\\n\\n\"", ".", "format", "(", "\n", "\"\\n\"", ".", "join", "(", "\n", "[", "\n", "\"\\t\"", ".", "join", "(", "map", "(", "str", ",", "pad_line_to_ontonotes", "(", "line", ",", "domain", ")", ")", ")", "\n", "for", "line", "in", "convert_sent_to_conll", "(", "sent_ls", ")", "\n", "]", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.scripts.convert_openie_to_conll.safe_zip": [[58, 64], ["zip", "len", "set", "map"], "function", ["None"], ["", "", "", "def", "safe_zip", "(", "*", "args", ")", ":", "\n", "    ", "\"\"\"\n    Zip which ensures all lists are of same size.\n    \"\"\"", "\n", "assert", "len", "(", "set", "(", "map", "(", "len", ",", "args", ")", ")", ")", "==", "1", "\n", "return", "zip", "(", "*", "args", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.scripts.convert_openie_to_conll.char_to_word_index": [[66, 72], ["sent[].count"], "function", ["None"], ["", "def", "char_to_word_index", "(", "char_ind", ":", "int", ",", "sent", ":", "str", ")", "->", "int", ":", "\n", "    ", "\"\"\"\n    Convert a character index to\n    word index in the given sentence.\n    \"\"\"", "\n", "return", "sent", "[", ":", "char_ind", "]", ".", "count", "(", "\" \"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.scripts.convert_openie_to_conll.element_from_span": [[74, 80], ["Element", "map", "len"], "function", ["None"], ["", "def", "element_from_span", "(", "span", ":", "List", "[", "int", "]", ",", "span_type", ":", "str", ")", "->", "Element", ":", "\n", "    ", "\"\"\"\n    Return an Element from span (list of spacy toks)\n    \"\"\"", "\n", "return", "Element", "(", "\n", "span_type", ",", "[", "span", "[", "0", "]", ".", "idx", ",", "span", "[", "-", "1", "]", ".", "idx", "+", "len", "(", "span", "[", "-", "1", "]", ")", "]", ",", "\" \"", ".", "join", "(", "map", "(", "str", ",", "span", ")", ")", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.scripts.convert_openie_to_conll.split_predicate": [[83, 116], ["Extraction", "convert_openie_to_conll.element_from_span", "rel_parts.append", "rel_parts.append", "convert_openie_to_conll.char_to_word_index", "enumerate", "tok.tag_.startswith", "len", "convert_openie_to_conll.element_from_span", "convert_openie_to_conll.element_from_span", "convert_openie_to_conll.char_to_word_index"], "function", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.scripts.convert_openie_to_conll.element_from_span", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.scripts.convert_openie_to_conll.char_to_word_index", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.scripts.convert_openie_to_conll.element_from_span", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.scripts.convert_openie_to_conll.element_from_span", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.scripts.convert_openie_to_conll.char_to_word_index"], ["", "def", "split_predicate", "(", "ex", ":", "Extraction", ")", "->", "Extraction", ":", "\n", "    ", "\"\"\"\n    Ensure single word predicate\n    by adding \"before-predicate\" and \"after-predicate\"\n    arguments.\n    \"\"\"", "\n", "rel_toks", "=", "ex", ".", "toks", "[", "\n", "char_to_word_index", "(", "ex", ".", "rel", ".", "span", "[", "0", "]", ",", "ex", ".", "sent", ")", ":", "char_to_word_index", "(", "\n", "ex", ".", "rel", ".", "span", "[", "1", "]", ",", "ex", ".", "sent", "\n", ")", "\n", "+", "1", "\n", "]", "\n", "if", "not", "rel_toks", ":", "\n", "        ", "return", "ex", "\n", "\n", "", "verb_inds", "=", "[", "\n", "tok_ind", "for", "(", "tok_ind", ",", "tok", ")", "in", "enumerate", "(", "rel_toks", ")", "if", "tok", ".", "tag_", ".", "startswith", "(", "\"VB\"", ")", "\n", "]", "\n", "\n", "last_verb_ind", "=", "verb_inds", "[", "-", "1", "]", "if", "verb_inds", "else", "(", "len", "(", "rel_toks", ")", "-", "1", ")", "\n", "\n", "rel_parts", "=", "[", "element_from_span", "(", "[", "rel_toks", "[", "last_verb_ind", "]", "]", ",", "\"V\"", ")", "]", "\n", "\n", "before_verb", "=", "rel_toks", "[", ":", "last_verb_ind", "]", "\n", "after_verb", "=", "rel_toks", "[", "last_verb_ind", "+", "1", ":", "]", "\n", "\n", "if", "before_verb", ":", "\n", "        ", "rel_parts", ".", "append", "(", "element_from_span", "(", "before_verb", ",", "\"BV\"", ")", ")", "\n", "\n", "", "if", "after_verb", ":", "\n", "        ", "rel_parts", ".", "append", "(", "element_from_span", "(", "after_verb", ",", "\"AV\"", ")", ")", "\n", "\n", "", "return", "Extraction", "(", "ex", ".", "sent", ",", "ex", ".", "toks", ",", "ex", ".", "arg1", ",", "rel_parts", ",", "ex", ".", "args2", ",", "ex", ".", "confidence", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.scripts.convert_openie_to_conll.extraction_to_conll": [[118, 137], ["convert_openie_to_conll.split_predicate", "split_predicate.sent.split", "len", "convert_openie_to_conll.char_to_word_index", "convert_openie_to_conll.char_to_word_index", "enumerate"], "function", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.scripts.convert_openie_to_conll.split_predicate", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.scripts.convert_openie_to_conll.char_to_word_index", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.scripts.convert_openie_to_conll.char_to_word_index"], ["", "def", "extraction_to_conll", "(", "ex", ":", "Extraction", ")", "->", "List", "[", "str", "]", ":", "\n", "    ", "\"\"\"\n    Return a conll representation of a given input Extraction.\n    \"\"\"", "\n", "ex", "=", "split_predicate", "(", "ex", ")", "\n", "toks", "=", "ex", ".", "sent", ".", "split", "(", "\" \"", ")", "\n", "ret", "=", "[", "\"*\"", "]", "*", "len", "(", "toks", ")", "\n", "args", "=", "[", "ex", ".", "arg1", "]", "+", "ex", ".", "args2", "\n", "rels_and_args", "=", "[", "\n", "(", "\"ARG{}\"", ".", "format", "(", "arg_ind", ")", ",", "arg", ")", "for", "arg_ind", ",", "arg", "in", "enumerate", "(", "args", ")", "\n", "]", "+", "[", "(", "rel_part", ".", "elem_type", ",", "rel_part", ")", "for", "rel_part", "in", "ex", ".", "rel", "]", "\n", "\n", "for", "rel", ",", "arg", "in", "rels_and_args", ":", "\n", "# Add brackets", "\n", "        ", "cur_start_ind", "=", "char_to_word_index", "(", "arg", ".", "span", "[", "0", "]", ",", "ex", ".", "sent", ")", "\n", "cur_end_ind", "=", "char_to_word_index", "(", "arg", ".", "span", "[", "1", "]", ",", "ex", ".", "sent", ")", "\n", "ret", "[", "cur_start_ind", "]", "=", "\"({}{}\"", ".", "format", "(", "rel", ",", "ret", "[", "cur_start_ind", "]", ")", "\n", "ret", "[", "cur_end_ind", "]", "+=", "\")\"", "\n", "", "return", "ret", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.scripts.convert_openie_to_conll.interpret_span": [[139, 179], ["regex.match", "regex.match.captures", "regex.match.captures", "list", "span.startswith", "int_spans.append", "ret.append", "map", "list.append", "len", "ret.append", "span[].split", "len"], "function", ["None"], ["", "def", "interpret_span", "(", "text_spans", ":", "str", ")", "->", "List", "[", "int", "]", ":", "\n", "    ", "\"\"\"\n    Return an integer tuple from\n    textual representation of closed / open spans.\n    \"\"\"", "\n", "m", "=", "regex", ".", "match", "(", "\n", "\"^(?:(?:([\\(\\[]\\d+, \\d+[\\)\\]])|({\\d+}))[,]?\\s*)+$\"", ",", "text_spans", "\n", ")", "# noqa", "\n", "\n", "spans", "=", "m", ".", "captures", "(", "1", ")", "+", "m", ".", "captures", "(", "2", ")", "\n", "\n", "int_spans", "=", "[", "]", "\n", "for", "span", "in", "spans", ":", "\n", "        ", "ints", "=", "list", "(", "map", "(", "int", ",", "span", "[", "1", ":", "-", "1", "]", ".", "split", "(", "\",\"", ")", ")", ")", "\n", "if", "span", "[", "0", "]", "==", "\"(\"", ":", "\n", "            ", "ints", "[", "0", "]", "+=", "1", "\n", "", "if", "span", "[", "-", "1", "]", "==", "\"]\"", ":", "\n", "            ", "ints", "[", "1", "]", "+=", "1", "\n", "", "if", "span", ".", "startswith", "(", "\"{\"", ")", ":", "\n", "            ", "assert", "len", "(", "ints", ")", "==", "1", "\n", "ints", ".", "append", "(", "ints", "[", "0", "]", "+", "1", ")", "\n", "\n", "", "assert", "len", "(", "ints", ")", "==", "2", "\n", "\n", "int_spans", ".", "append", "(", "ints", ")", "\n", "\n", "# Merge consecutive spans", "\n", "", "ret", "=", "[", "]", "\n", "cur_span", "=", "int_spans", "[", "0", "]", "\n", "for", "(", "start", ",", "end", ")", "in", "int_spans", "[", "1", ":", "]", ":", "\n", "        ", "if", "start", "-", "1", "==", "cur_span", "[", "-", "1", "]", ":", "\n", "            ", "cur_span", "=", "(", "cur_span", "[", "0", "]", ",", "end", ")", "\n", "", "else", ":", "\n", "            ", "ret", ".", "append", "(", "cur_span", ")", "\n", "cur_span", "=", "(", "start", ",", "end", ")", "\n", "\n", "", "", "if", "(", "not", "ret", ")", "or", "(", "cur_span", "!=", "ret", "[", "-", "1", "]", ")", ":", "\n", "        ", "ret", ".", "append", "(", "cur_span", ")", "\n", "\n", "", "return", "ret", "[", "0", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.scripts.convert_openie_to_conll.interpret_element": [[181, 187], ["Element", "convert_openie_to_conll.interpret_span"], "function", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.scripts.convert_openie_to_conll.interpret_span"], ["", "def", "interpret_element", "(", "element_type", ":", "str", ",", "text", ":", "str", ",", "span", ":", "str", ")", "->", "Element", ":", "\n", "    ", "\"\"\"\n    Construct an Element instance from regexp\n    groups.\n    \"\"\"", "\n", "return", "Element", "(", "element_type", ",", "interpret_span", "(", "span", ")", ",", "text", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.scripts.convert_openie_to_conll.parse_element": [[189, 200], ["regex.match", "convert_openie_to_conll.interpret_element", "elem.lstrip().rstrip", "raw_element.split", "elem.lstrip", "elem.groups"], "function", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.scripts.convert_openie_to_conll.interpret_element"], ["", "def", "parse_element", "(", "raw_element", ":", "str", ")", "->", "List", "[", "Element", "]", ":", "\n", "    ", "\"\"\"\n    Parse a raw element into text and indices (integers).\n    \"\"\"", "\n", "elements", "=", "[", "\n", "regex", ".", "match", "(", "\n", "\"^(([a-zA-Z]+)\\(([^;]+),List\\(([^;]*)\\)\\))$\"", ",", "elem", ".", "lstrip", "(", ")", ".", "rstrip", "(", ")", "\n", ")", "# noqa", "\n", "for", "elem", "in", "raw_element", ".", "split", "(", "\";\"", ")", "\n", "]", "\n", "return", "[", "interpret_element", "(", "*", "elem", ".", "groups", "(", ")", "[", "1", ":", "]", ")", "for", "elem", "in", "elements", "if", "elem", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.scripts.convert_openie_to_conll.read": [[202, 237], ["allennlp.data.tokenizers.spacy_tokenizer.SpacyTokenizer", "open", "tqdm.tqdm", "line.strip().split", "map", "all", "Extraction", "line.strip", "len", "len", "len", "prev_sent.append", "allennlp.data.tokenizers.spacy_tokenizer.SpacyTokenizer.tokenize"], "function", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.tqdm.Tqdm.tqdm", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.tokenizers.whitespace_tokenizer.WhitespaceTokenizer.tokenize"], ["", "def", "read", "(", "fn", ":", "str", ")", "->", "List", "[", "Extraction", "]", ":", "\n", "    ", "tokenizer", "=", "SpacyTokenizer", "(", "pos_tags", "=", "True", ")", "\n", "prev_sent", "=", "[", "]", "\n", "\n", "with", "open", "(", "fn", ")", "as", "fin", ":", "\n", "        ", "for", "line", "in", "tqdm", "(", "fin", ")", ":", "\n", "            ", "data", "=", "line", ".", "strip", "(", ")", ".", "split", "(", "\"\\t\"", ")", "\n", "confidence", "=", "data", "[", "0", "]", "\n", "if", "not", "all", "(", "data", "[", "2", ":", "5", "]", ")", ":", "\n", "# Make sure that all required elements are present", "\n", "                ", "continue", "\n", "", "arg1", ",", "rel", ",", "args2", "=", "map", "(", "parse_element", ",", "data", "[", "2", ":", "5", "]", ")", "\n", "\n", "# Exactly one subject and one relation", "\n", "# and at least one object", "\n", "if", "(", "len", "(", "rel", ")", "==", "1", ")", "and", "(", "len", "(", "arg1", ")", "==", "1", ")", "and", "(", "len", "(", "args2", ")", ">=", "1", ")", ":", "\n", "                ", "sent", "=", "data", "[", "5", "]", "\n", "cur_ex", "=", "Extraction", "(", "\n", "sent", "=", "sent", ",", "\n", "toks", "=", "tokenizer", ".", "tokenize", "(", "sent", ")", ",", "\n", "arg1", "=", "arg1", "[", "0", "]", ",", "\n", "rel", "=", "rel", "[", "0", "]", ",", "\n", "args2", "=", "args2", ",", "\n", "confidence", "=", "confidence", ",", "\n", ")", "\n", "\n", "# Decide whether to append or yield", "\n", "if", "(", "not", "prev_sent", ")", "or", "(", "prev_sent", "[", "0", "]", ".", "sent", "==", "sent", ")", ":", "\n", "                    ", "prev_sent", ".", "append", "(", "cur_ex", ")", "\n", "", "else", ":", "\n", "                    ", "yield", "prev_sent", "\n", "prev_sent", "=", "[", "cur_ex", "]", "\n", "", "", "", "", "if", "prev_sent", ":", "\n", "# Yield last element", "\n", "        ", "yield", "prev_sent", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.scripts.convert_openie_to_conll.convert_sent_to_conll": [[239, 250], ["sent_ls[].sent.split", "convert_openie_to_conll.safe_zip", "len", "range", "convert_openie_to_conll.extraction_to_conll", "len"], "function", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.scripts.convert_openie_to_conll.safe_zip", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.scripts.convert_openie_to_conll.extraction_to_conll"], ["", "", "def", "convert_sent_to_conll", "(", "sent_ls", ":", "List", "[", "Extraction", "]", ")", ":", "\n", "    ", "\"\"\"\n    Given a list of extractions for a single sentence -\n    convert it to conll representation.\n    \"\"\"", "\n", "# Sanity check - make sure all extractions are on the same sentence", "\n", "assert", "len", "(", "{", "ex", ".", "sent", "for", "ex", "in", "sent_ls", "}", ")", "==", "1", "\n", "toks", "=", "sent_ls", "[", "0", "]", ".", "sent", ".", "split", "(", "\" \"", ")", "\n", "\n", "return", "safe_zip", "(", "\n", "*", "[", "range", "(", "len", "(", "toks", ")", ")", ",", "toks", "]", "+", "[", "extraction_to_conll", "(", "ex", ")", "for", "ex", "in", "sent_ls", "]", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.scripts.convert_openie_to_conll.pad_line_to_ontonotes": [[253, 267], ["list"], "function", ["None"], ["", "def", "pad_line_to_ontonotes", "(", "line", ",", "domain", ")", "->", "List", "[", "str", "]", ":", "\n", "    ", "\"\"\"\n    Pad line to conform to ontonotes representation.\n    \"\"\"", "\n", "word_ind", ",", "word", "=", "line", "[", ":", "2", "]", "\n", "pos", "=", "\"XX\"", "\n", "oie_tags", "=", "line", "[", "2", ":", "]", "\n", "line_num", "=", "0", "\n", "parse", "=", "\"-\"", "\n", "lemma", "=", "\"-\"", "\n", "return", "(", "\n", "[", "domain", ",", "line_num", ",", "word_ind", ",", "word", ",", "pos", ",", "parse", ",", "lemma", ",", "\"-\"", ",", "\"-\"", ",", "\"-\"", ",", "\"*\"", "]", "\n", "+", "list", "(", "oie_tags", ")", "\n", "+", "[", "\"-\"", "]", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.scripts.convert_openie_to_conll.convert_sent_dict_to_conll": [[270, 284], ["sent_dic.iteritems", "map", "convert_openie_to_conll.convert_sent_to_conll", "convert_openie_to_conll.pad_line_to_ontonotes"], "function", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.scripts.convert_openie_to_conll.convert_sent_to_conll", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.scripts.convert_openie_to_conll.pad_line_to_ontonotes"], ["", "def", "convert_sent_dict_to_conll", "(", "sent_dic", ",", "domain", ")", "->", "str", ":", "\n", "    ", "\"\"\"\n    Given a dictionary from sentence -> extractions,\n    return a corresponding CoNLL representation.\n    \"\"\"", "\n", "return", "\"\\n\\n\"", ".", "join", "(", "\n", "[", "\n", "\"\\n\"", ".", "join", "(", "\n", "[", "\n", "\"\\t\"", ".", "join", "(", "map", "(", "str", ",", "pad_line_to_ontonotes", "(", "line", ",", "domain", ")", ")", ")", "\n", "for", "line", "in", "convert_sent_to_conll", "(", "sent_ls", ")", "\n", "]", "\n", ")", "\n", "for", "sent_ls", "in", "sent_dic", ".", "iteritems", "(", ")", "\n", "]", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.scripts.train_fixtures.train_fixture": [[22, 43], ["os.path.exists", "allennlp.commands.train.train_model_from_file", "shutil.rmtree", "glob.glob", "shutil.rmtree", "os.path.join", "os.path.join", "filename.endswith", "filename.endswith", "re.search", "os.remove"], "function", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.commands.train.train_model_from_file", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.dataset_readers.multiprocess_dataset_reader.QIterable.join", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.dataset_readers.multiprocess_dataset_reader.QIterable.join", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.nn.beam_search.BeamSearch.search"], ["def", "train_fixture", "(", "config_prefix", ":", "str", ")", "->", "None", ":", "\n", "    ", "config_file", "=", "config_prefix", "+", "\"experiment.json\"", "\n", "serialization_dir", "=", "config_prefix", "+", "\"serialization\"", "\n", "# Train model doesn't like it if we have incomplete serialization", "\n", "# directories, so remove them if they exist.", "\n", "if", "os", ".", "path", ".", "exists", "(", "serialization_dir", ")", ":", "\n", "        ", "shutil", ".", "rmtree", "(", "serialization_dir", ")", "\n", "\n", "# train the model", "\n", "", "train_model_from_file", "(", "config_file", ",", "serialization_dir", ")", "\n", "\n", "# remove unnecessary files", "\n", "shutil", ".", "rmtree", "(", "os", ".", "path", ".", "join", "(", "serialization_dir", ",", "\"log\"", ")", ")", "\n", "\n", "for", "filename", "in", "glob", ".", "glob", "(", "os", ".", "path", ".", "join", "(", "serialization_dir", ",", "\"*\"", ")", ")", ":", "\n", "        ", "if", "(", "\n", "filename", ".", "endswith", "(", "\".log\"", ")", "\n", "or", "filename", ".", "endswith", "(", "\".json\"", ")", "\n", "or", "re", ".", "search", "(", "r\"epoch_[0-9]+\\.th$\"", ",", "filename", ")", "\n", ")", ":", "\n", "            ", "os", ".", "remove", "(", "filename", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.scripts.train_fixtures.train_fixture_gpu": [[45, 62], ["allennlp.common.Params.from_file", "tempfile.gettempdir", "allennlp.commands.train.train_model", "shutil.copy", "shutil.copy", "os.path.join", "os.path.join", "os.path.join", "os.path.join"], "function", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.params.Params.from_file", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.commands.train.train_model", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.dataset_readers.multiprocess_dataset_reader.QIterable.join", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.dataset_readers.multiprocess_dataset_reader.QIterable.join", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.dataset_readers.multiprocess_dataset_reader.QIterable.join", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.dataset_readers.multiprocess_dataset_reader.QIterable.join"], ["", "", "", "def", "train_fixture_gpu", "(", "config_prefix", ":", "str", ")", "->", "None", ":", "\n", "    ", "config_file", "=", "config_prefix", "+", "\"experiment.json\"", "\n", "serialization_dir", "=", "config_prefix", "+", "\"serialization\"", "\n", "params", "=", "Params", ".", "from_file", "(", "config_file", ")", "\n", "params", "[", "\"trainer\"", "]", "[", "\"cuda_device\"", "]", "=", "0", "\n", "\n", "# train this one to a tempdir", "\n", "tempdir", "=", "tempfile", ".", "gettempdir", "(", ")", "\n", "train_model", "(", "params", ",", "tempdir", ")", "\n", "\n", "# now copy back the weights and and archived model", "\n", "shutil", ".", "copy", "(", "\n", "os", ".", "path", ".", "join", "(", "tempdir", ",", "\"best.th\"", ")", ",", "os", ".", "path", ".", "join", "(", "serialization_dir", ",", "\"best_gpu.th\"", ")", "\n", ")", "\n", "shutil", ".", "copy", "(", "\n", "os", ".", "path", ".", "join", "(", "tempdir", ",", "\"model.tar.gz\"", ")", ",", "\n", "os", ".", "path", ".", "join", "(", "serialization_dir", ",", "\"model_gpu.tar.gz\"", ")", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.scripts.verify.main": [[12, 62], ["print", "print", "subprocess.run", "print", "subprocess.run", "print", "print", "subprocess.run", "print", "print", "subprocess.run", "print", "print", "subprocess.run", "print", "print", "subprocess.run", "print", "sys.exit", "str"], "function", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.commands.train.TrainModel.run", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.commands.train.TrainModel.run", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.commands.train.TrainModel.run", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.commands.train.TrainModel.run", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.commands.train.TrainModel.run", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.commands.train.TrainModel.run"], ["def", "main", "(", "checks", ")", ":", "\n", "    ", "try", ":", "\n", "        ", "print", "(", "\"Verifying with \"", "+", "str", "(", "checks", ")", ")", "\n", "if", "\"pytest\"", "in", "checks", ":", "\n", "            ", "print", "(", "\"Tests (pytest):\"", ",", "flush", "=", "True", ")", "\n", "run", "(", "\"pytest -v --color=yes\"", ",", "shell", "=", "True", ",", "check", "=", "True", ")", "\n", "\n", "", "if", "\"flake8\"", "in", "checks", ":", "\n", "            ", "print", "(", "\"Linter (flake8)\"", ",", "flush", "=", "True", ")", "\n", "run", "(", "\"flake8 -v\"", ",", "shell", "=", "True", ",", "check", "=", "True", ")", "\n", "print", "(", "\"flake8 checks passed\"", ")", "\n", "\n", "", "if", "\"black\"", "in", "checks", ":", "\n", "            ", "print", "(", "\"Formatter (black)\"", ",", "flush", "=", "True", ")", "\n", "run", "(", "\"black -v --check .\"", ",", "shell", "=", "True", ",", "check", "=", "True", ")", "\n", "print", "(", "\"black checks passed\"", ")", "\n", "\n", "", "if", "\"mypy\"", "in", "checks", ":", "\n", "            ", "print", "(", "\"Typechecker (mypy):\"", ",", "flush", "=", "True", ")", "\n", "run", "(", "\n", "\"mypy allennlp\"", "\n", "# This is necessary because not all the imported libraries have type stubs.", "\n", "\" --ignore-missing-imports\"", "\n", "# This is necessary because PyTorch has some type stubs but they're incomplete,", "\n", "# and mypy will follow them and generate a lot of spurious errors.", "\n", "\" --no-site-packages \"", "\n", "# We are extremely lax about specifying Optional[] types, so we need this flag.", "\n", "# TODO: tighten up our type annotations and remove this", "\n", "\" --no-strict-optional\"", "\n", "# Some versions of mypy crash randomly when caching, probably because of our use of", "\n", "# NamedTuple (https://github.com/python/mypy/issues/7281).", "\n", "\" --cache-dir=/dev/null\"", ",", "\n", "shell", "=", "True", ",", "\n", "check", "=", "True", ",", "\n", ")", "\n", "print", "(", "\"mypy checks passed\"", ")", "\n", "\n", "", "if", "\"check-links\"", "in", "checks", ":", "\n", "            ", "print", "(", "\"Checking links in Markdown files:\"", ",", "flush", "=", "True", ")", "\n", "run", "(", "\"./scripts/check_links.py\"", ",", "shell", "=", "True", ",", "check", "=", "True", ")", "\n", "print", "(", "\"check links passed\"", ")", "\n", "\n", "", "if", "\"check-large-files\"", "in", "checks", ":", "\n", "            ", "print", "(", "\"Checking all added files have size <= 2MB\"", ",", "flush", "=", "True", ")", "\n", "run", "(", "\"./scripts/check_large_files.sh 2\"", ",", "shell", "=", "True", ",", "check", "=", "True", ")", "\n", "print", "(", "\"check large files passed\"", ")", "\n", "\n", "", "", "except", "CalledProcessError", ":", "\n", "# squelch the exception stacktrace", "\n", "        ", "sys", ".", "exit", "(", "1", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.scripts.build_docs.render_file": [[19, 42], ["relative_src_path.replace", "src_file.replace", "subprocess.check_output().decode", "print", "open", "f.write", "subprocess.check_output"], "function", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.coreference_resolution.coref.CoreferenceResolver.decode", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.None.predictor.write"], ["def", "render_file", "(", "\n", "relative_src_path", ":", "str", ",", "src_file", ":", "str", ",", "to_file", ":", "str", ",", "modifier", "=", "\"++\"", "\n", ")", "->", "None", ":", "\n", "    ", "\"\"\"\n    Shells out to pydocmd, which creates a .md file from the docstrings of python functions and classes in\n    the file we specify. The modifer specifies the depth at which to generate docs for classes and functions\n    in the file. More information here: https://pypi.org/project/pydoc-markdown/\n\n    \"\"\"", "\n", "relative_src_namespace", "=", "relative_src_path", ".", "replace", "(", "\"/\"", ",", "\".\"", ")", "\n", "src_base", "=", "src_file", ".", "replace", "(", "\".py\"", ",", "\"\"", ")", "\n", "\n", "if", "relative_src_namespace", "==", "\"\"", ":", "\n", "        ", "namespace", "=", "f\"allennlp.{src_base}{modifier}\"", "\n", "", "else", ":", "\n", "        ", "namespace", "=", "f\"allennlp.{relative_src_namespace}.{src_base}{modifier}\"", "\n", "\n", "", "args", "=", "[", "\"pydocmd\"", ",", "\"simple\"", ",", "namespace", "]", "\n", "call_result", "=", "check_output", "(", "args", ",", "env", "=", "os", ".", "environ", ")", ".", "decode", "(", "\"utf-8\"", ")", "\n", "with", "open", "(", "to_file", ",", "\"w\"", ")", "as", "f", ":", "\n", "        ", "f", ".", "write", "(", "call_result", ")", "\n", "\n", "", "print", "(", "f\"Built docs for {src_file}: {to_file}\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.scripts.build_docs.build_docs_for_file": [[44, 58], ["file_name.replace", "os.path.join", "os.path.join", "build_docs.render_file", "os.path.basename"], "function", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.dataset_readers.multiprocess_dataset_reader.QIterable.join", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.dataset_readers.multiprocess_dataset_reader.QIterable.join", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.scripts.build_docs.render_file"], ["", "def", "build_docs_for_file", "(", "\n", "relative_path", ":", "str", ",", "file_name", ":", "str", ",", "docs_dir", ":", "str", "\n", ")", "->", "Dict", "[", "str", ",", "str", "]", ":", "\n", "    ", "\"\"\"\n    Build docs for an individual python file.\n    \"\"\"", "\n", "clean_filename", "=", "file_name", ".", "replace", "(", "\".py\"", ",", "\"\"", ")", "\n", "markdown_filename", "=", "f\"{clean_filename}.md\"", "\n", "\n", "output_path", "=", "os", ".", "path", ".", "join", "(", "docs_dir", ",", "relative_path", ",", "markdown_filename", ")", "\n", "nav_path", "=", "os", ".", "path", ".", "join", "(", "\"api\"", ",", "relative_path", ",", "markdown_filename", ")", "\n", "render_file", "(", "relative_path", ",", "file_name", ",", "output_path", ")", "\n", "\n", "return", "{", "os", ".", "path", ".", "basename", "(", "clean_filename", ")", ":", "nav_path", "}", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.scripts.build_docs.build_docs": [[60, 94], ["os.listdir", "os.path.join", "str().replace", "os.path.join", "os.path.isdir", "os.path.exists", "os.mkdir", "build_docs.build_docs", "build_docs.sort", "nav_root.append", "build_docs.build_docs_for_file", "nav_root.append", "str", "child.endswith", "list"], "function", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.dataset_readers.multiprocess_dataset_reader.QIterable.join", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.dataset_readers.multiprocess_dataset_reader.QIterable.join", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.scripts.build_docs.build_docs", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.scripts.build_docs.build_docs_for_file"], ["", "def", "build_docs", "(", "root_path", ":", "str", ",", "docs_dir", ":", "str", ")", ":", "\n", "\n", "    ", "nav_root", "=", "[", "]", "\n", "\n", "for", "child", "in", "os", ".", "listdir", "(", "root_path", ")", ":", "\n", "        ", "relative_path", "=", "os", ".", "path", ".", "join", "(", "root_path", ",", "child", ")", "\n", "\n", "if", "(", "\n", "\"__pycache__\"", "in", "relative_path", "\n", "or", "\"tests\"", "in", "relative_path", "\n", "or", "\"mypy_cache\"", "in", "relative_path", "\n", ")", ":", "\n", "            ", "continue", "\n", "\n", "", "without_allennlp", "=", "str", "(", "root_path", ")", ".", "replace", "(", "\"allennlp/\"", ",", "\"\"", ")", "\n", "target_dir", "=", "os", ".", "path", ".", "join", "(", "docs_dir", ",", "without_allennlp", ")", "\n", "if", "not", "os", ".", "path", ".", "exists", "(", "target_dir", ")", ":", "\n", "            ", "os", ".", "mkdir", "(", "target_dir", ")", "\n", "\n", "", "if", "os", ".", "path", ".", "isdir", "(", "relative_path", ")", ":", "\n", "            ", "nav_subsection", "=", "build_docs", "(", "relative_path", ",", "docs_dir", ")", "\n", "if", "not", "nav_subsection", ":", "\n", "                ", "continue", "\n", "", "nav_subsection", ".", "sort", "(", "key", "=", "lambda", "x", ":", "list", "(", "x", ")", "[", "0", "]", ",", "reverse", "=", "False", ")", "\n", "nav_root", ".", "append", "(", "{", "child", ":", "nav_subsection", "}", ")", "\n", "\n", "", "else", ":", "\n", "            ", "if", "child", "in", "exclude_files", "or", "not", "child", ".", "endswith", "(", "\".py\"", ")", ":", "\n", "                ", "continue", "\n", "\n", "", "nav", "=", "build_docs_for_file", "(", "without_allennlp", ",", "child", ",", "docs_dir", ")", "\n", "nav_root", ".", "append", "(", "nav", ")", "\n", "\n", "", "", "return", "nav_root", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.scripts.check_links.url_ok": [[44, 54], ["http_session.head"], "function", ["None"], ["", "def", "url_ok", "(", "match_tuple", ":", "MatchTuple", ")", "->", "Tuple", "[", "bool", ",", "str", "]", ":", "\n", "    ", "\"\"\"Check if a URL is reachable.\"\"\"", "\n", "try", ":", "\n", "        ", "result", "=", "http_session", ".", "head", "(", "match_tuple", ".", "link", ",", "timeout", "=", "5", ",", "allow_redirects", "=", "True", ")", "\n", "return", "(", "\n", "result", ".", "ok", "or", "result", ".", "status_code", "in", "OK_STATUS_CODES", ",", "\n", "f\"status code = {result.status_code}\"", ",", "\n", ")", "\n", "", "except", "(", "requests", ".", "ConnectionError", ",", "requests", ".", "Timeout", ")", ":", "\n", "        ", "return", "False", ",", "\"connection error\"", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.scripts.check_links.path_ok": [[56, 61], ["os.path.join", "os.path.exists", "match_tuple.link.split", "os.path.dirname", "str"], "function", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.dataset_readers.multiprocess_dataset_reader.QIterable.join"], ["", "", "def", "path_ok", "(", "match_tuple", ":", "MatchTuple", ")", "->", "bool", ":", "\n", "    ", "\"\"\"Check if a file in this repository exists.\"\"\"", "\n", "relative_path", "=", "match_tuple", ".", "link", ".", "split", "(", "\"#\"", ")", "[", "0", "]", "\n", "full_path", "=", "os", ".", "path", ".", "join", "(", "os", ".", "path", ".", "dirname", "(", "str", "(", "match_tuple", ".", "source", ")", ")", ",", "relative_path", ")", "\n", "return", "os", ".", "path", ".", "exists", "(", "full_path", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.scripts.check_links.link_ok": [[63, 71], ["match_tuple.link.startswith", "print", "check_links.url_ok", "check_links.path_ok"], "function", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.scripts.check_links.url_ok", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.scripts.check_links.path_ok"], ["", "def", "link_ok", "(", "match_tuple", ":", "MatchTuple", ")", "->", "Tuple", "[", "MatchTuple", ",", "bool", ",", "Optional", "[", "str", "]", "]", ":", "\n", "    ", "reason", ":", "Optional", "[", "str", "]", "=", "None", "\n", "if", "match_tuple", ".", "link", ".", "startswith", "(", "\"http\"", ")", ":", "\n", "        ", "result_ok", ",", "reason", "=", "url_ok", "(", "match_tuple", ")", "\n", "", "else", ":", "\n", "        ", "result_ok", "=", "path_ok", "(", "match_tuple", ")", "\n", "", "print", "(", "f\"  {'\u2713' if result_ok else '\u2717'} {match_tuple.link}\"", ")", "\n", "return", "match_tuple", ",", "result_ok", ",", "reason", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.scripts.check_links.main": [[73, 110], ["print", "project_root.glob", "set", "re.compile", "print", "print", "print", "multiprocessing.dummy.Pool", "pool.map", "print", "sys.exit", "open", "handle.readlines", "print", "print", "print", "re.compile.findall", "len", "print", "pathlib.Path", "list", "len", "set.add", "check_links.MatchTuple", "str"], "function", ["None"], ["", "def", "main", "(", ")", ":", "\n", "    ", "print", "(", "\"Finding all markdown files in the current directory...\"", ")", "\n", "\n", "project_root", "=", "(", "pathlib", ".", "Path", "(", "__file__", ")", ".", "parent", "/", "\"..\"", ")", ".", "resolve", "(", ")", "\n", "markdown_files", "=", "project_root", ".", "glob", "(", "\"**/*.md\"", ")", "\n", "\n", "all_matches", "=", "set", "(", ")", "\n", "url_regex", "=", "re", ".", "compile", "(", "r\"\\[([^!][^\\]]+)\\]\\(([^)(]+)\\)\"", ")", "\n", "for", "markdown_file", "in", "markdown_files", ":", "\n", "        ", "with", "open", "(", "markdown_file", ")", "as", "handle", ":", "\n", "            ", "for", "line", "in", "handle", ".", "readlines", "(", ")", ":", "\n", "                ", "matches", "=", "url_regex", ".", "findall", "(", "line", ")", "\n", "for", "name", ",", "link", "in", "matches", ":", "\n", "                    ", "if", "\"localhost\"", "not", "in", "link", ":", "\n", "                        ", "all_matches", ".", "add", "(", "\n", "MatchTuple", "(", "source", "=", "str", "(", "markdown_file", ")", ",", "name", "=", "name", ",", "link", "=", "link", ")", "\n", ")", "\n", "\n", "", "", "", "", "", "print", "(", "f\"  {len(all_matches)} markdown files found\"", ")", "\n", "print", "(", "\"Checking to make sure we can retrieve each link...\"", ")", "\n", "\n", "with", "Pool", "(", "processes", "=", "THREADS", ")", "as", "pool", ":", "\n", "        ", "results", "=", "pool", ".", "map", "(", "link_ok", ",", "[", "match", "for", "match", "in", "list", "(", "all_matches", ")", "]", ")", "\n", "", "unreachable_results", "=", "[", "\n", "(", "match_tuple", ",", "reason", ")", "for", "match_tuple", ",", "success", ",", "reason", "in", "results", "if", "not", "success", "\n", "]", "\n", "\n", "if", "unreachable_results", ":", "\n", "        ", "print", "(", "f\"Unreachable links ({len(unreachable_results)}):\"", ")", "\n", "for", "match_tuple", ",", "reason", "in", "unreachable_results", ":", "\n", "            ", "print", "(", "\"  > Source: \"", "+", "match_tuple", ".", "source", ")", "\n", "print", "(", "\"    Name: \"", "+", "match_tuple", ".", "name", ")", "\n", "print", "(", "\"    Link: \"", "+", "match_tuple", ".", "link", ")", "\n", "if", "reason", "is", "not", "None", ":", "\n", "                ", "print", "(", "\"    Reason: \"", "+", "reason", ")", "\n", "", "", "sys", ".", "exit", "(", "1", ")", "\n", "", "print", "(", "\"No Unreachable link found.\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.scripts.regenerate_archived_models.generate_archive": [[14, 38], ["os.path.join", "os.path.exists", "logger.info", "tarfile.open", "archive.add", "archive.add", "archive.add", "logger.info", "logger.error", "sys.exit", "os.path.join", "os.path.join"], "function", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.dataset_readers.multiprocess_dataset_reader.QIterable.join", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.dataset_readers.multiprocess_dataset_reader.logger.info", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.dataset_readers.multiprocess_dataset_reader.logger.info", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.dataset_readers.multiprocess_dataset_reader.QIterable.join", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.dataset_readers.multiprocess_dataset_reader.QIterable.join"], ["def", "generate_archive", "(", "\n", "config_file", ":", "str", ",", "\n", "serialization_dir", ":", "str", ",", "\n", "weights_file", ":", "str", "=", "\"best.th\"", ",", "\n", "archive_name", ":", "str", "=", "\"model.tar.gz\"", ",", "\n", "exist_ok", ":", "bool", "=", "False", ",", "\n", ")", "->", "None", ":", "\n", "    ", "archive_file", "=", "os", ".", "path", ".", "join", "(", "serialization_dir", ",", "archive_name", ")", "\n", "\n", "if", "os", ".", "path", ".", "exists", "(", "archive_file", ")", ":", "\n", "        ", "if", "exist_ok", ":", "\n", "            ", "logger", ".", "info", "(", "\"removing archive file %s\"", ",", "archive_file", ")", "\n", "", "else", ":", "\n", "            ", "logger", ".", "error", "(", "\"archive file %s already exists\"", ",", "archive_file", ")", "\n", "sys", ".", "exit", "(", "-", "1", ")", "\n", "\n", "", "", "logger", ".", "info", "(", "\"creating new archive file %s\"", ",", "archive_file", ")", "\n", "\n", "with", "tarfile", ".", "open", "(", "archive_file", ",", "\"w:gz\"", ")", "as", "archive", ":", "\n", "        ", "archive", ".", "add", "(", "config_file", ",", "arcname", "=", "CONFIG_NAME", ")", "\n", "archive", ".", "add", "(", "\n", "os", ".", "path", ".", "join", "(", "serialization_dir", ",", "weights_file", ")", ",", "arcname", "=", "_WEIGHTS_NAME", "\n", ")", "\n", "archive", ".", "add", "(", "os", ".", "path", ".", "join", "(", "serialization_dir", ",", "\"vocabulary\"", ")", ",", "arcname", "=", "\"vocabulary\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.scripts.write_srl_predictions_to_conll_format.main": [[19, 94], ["allennlp.common.Params.from_file", "allennlp.data.DatasetReader.from_params", "allennlp.models.archival.load_archive", "model.eval", "os.path.join", "os.path.join", "open", "open", "print", "DatasetReader.from_params.read", "os.path.join", "config[].pop", "os.path.join", "torch.autograd.no_grad", "allennlp.data.iterators.BasicIterator", "allennlp.data.iterators.BasicIterator.index_with", "allennlp.data.iterators.BasicIterator.", "allennlp.common.tqdm.Tqdm.tqdm", "zip", "open.close", "open.close", "allennlp.nn.util.move_to_device", "model", "model.decode", "model_predictions.extend", "allennlp.models.semantic_role_labeler.write_to_conll_eval_file"], "function", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.params.Params.from_file", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.tokenizers.tokenizer.Tokenizer.from_params", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.models.archival.load_archive", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.dataset_readers.multiprocess_dataset_reader.QIterable.join", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.dataset_readers.multiprocess_dataset_reader.QIterable.join", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.scripts.convert_openie_to_conll.read", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.dataset_readers.multiprocess_dataset_reader.QIterable.join", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.params.Params.pop", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.dataset_readers.multiprocess_dataset_reader.QIterable.join", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.iterators.data_iterator.DataIterator.index_with", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.tqdm.Tqdm.tqdm", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.token_embedders.embedding.EmbeddingsTextFile.close", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.token_embedders.embedding.EmbeddingsTextFile.close", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.nn.util.move_to_device", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.coreference_resolution.coref.CoreferenceResolver.decode", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.models.semantic_role_labeler.write_to_conll_eval_file"], ["def", "main", "(", "\n", "serialization_directory", ":", "int", ",", "\n", "device", ":", "int", ",", "\n", "data", ":", "str", ",", "\n", "prefix", ":", "str", ",", "\n", "domain", ":", "str", "=", "None", ",", "\n", ")", ":", "\n", "    ", "\"\"\"\n    serialization_directory : str, required.\n        The directory containing the serialized weights.\n    device: int, default = -1\n        The device to run the evaluation on.\n    data: str, default = None\n        The data to evaluate on. By default, we use the validation data from\n        the original experiment.\n    prefix: str, default=\"\"\n        The prefix to prepend to the generated gold and prediction files, to distinguish\n        different models/data.\n    domain: str, optional (default = None)\n        If passed, filters the ontonotes evaluation/test dataset to only contain the\n        specified domain. This overwrites the domain in the config file from the model,\n        to allow evaluation on domains other than the one the model was trained on.\n    \"\"\"", "\n", "config", "=", "Params", ".", "from_file", "(", "os", ".", "path", ".", "join", "(", "serialization_directory", ",", "\"config.json\"", ")", ")", "\n", "\n", "if", "domain", "is", "not", "None", ":", "\n", "# Hack to allow evaluation on different domains than the", "\n", "# model was trained on.", "\n", "        ", "config", "[", "\"dataset_reader\"", "]", "[", "\"domain_identifier\"", "]", "=", "domain", "\n", "prefix", "=", "f\"{domain}_{prefix}\"", "\n", "", "else", ":", "\n", "        ", "config", "[", "\"dataset_reader\"", "]", ".", "pop", "(", "\"domain_identifier\"", ",", "None", ")", "\n", "\n", "", "dataset_reader", "=", "DatasetReader", ".", "from_params", "(", "config", "[", "\"dataset_reader\"", "]", ")", "\n", "evaluation_data_path", "=", "data", "if", "data", "else", "config", "[", "\"validation_data_path\"", "]", "\n", "\n", "archive", "=", "load_archive", "(", "\n", "os", ".", "path", ".", "join", "(", "serialization_directory", ",", "\"model.tar.gz\"", ")", ",", "cuda_device", "=", "device", "\n", ")", "\n", "model", "=", "archive", ".", "model", "\n", "model", ".", "eval", "(", ")", "\n", "\n", "prediction_file_path", "=", "os", ".", "path", ".", "join", "(", "\n", "serialization_directory", ",", "prefix", "+", "\"_predictions.txt\"", "\n", ")", "\n", "gold_file_path", "=", "os", ".", "path", ".", "join", "(", "serialization_directory", ",", "prefix", "+", "\"_gold.txt\"", ")", "\n", "prediction_file", "=", "open", "(", "prediction_file_path", ",", "\"w+\"", ")", "\n", "gold_file", "=", "open", "(", "gold_file_path", ",", "\"w+\"", ")", "\n", "\n", "# Load the evaluation data and index it.", "\n", "print", "(", "\"reading evaluation data from {}\"", ".", "format", "(", "evaluation_data_path", ")", ")", "\n", "instances", "=", "dataset_reader", ".", "read", "(", "evaluation_data_path", ")", "\n", "\n", "with", "torch", ".", "autograd", ".", "no_grad", "(", ")", ":", "\n", "        ", "iterator", "=", "BasicIterator", "(", "batch_size", "=", "32", ")", "\n", "iterator", ".", "index_with", "(", "model", ".", "vocab", ")", "\n", "\n", "model_predictions", "=", "[", "]", "\n", "batches", "=", "iterator", "(", "instances", ",", "num_epochs", "=", "1", ",", "shuffle", "=", "False", ")", "\n", "for", "batch", "in", "Tqdm", ".", "tqdm", "(", "batches", ")", ":", "\n", "            ", "batch", "=", "move_to_device", "(", "batch", ",", "device", ")", "\n", "result", "=", "model", "(", "**", "batch", ")", "\n", "predictions", "=", "model", ".", "decode", "(", "result", ")", "\n", "model_predictions", ".", "extend", "(", "predictions", "[", "\"tags\"", "]", ")", "\n", "\n", "", "for", "instance", ",", "prediction", "in", "zip", "(", "instances", ",", "model_predictions", ")", ":", "\n", "            ", "fields", "=", "instance", ".", "fields", "\n", "verb_index", "=", "fields", "[", "\"metadata\"", "]", "[", "\"verb_index\"", "]", "\n", "gold_tags", "=", "fields", "[", "\"metadata\"", "]", "[", "\"gold_tags\"", "]", "\n", "sentence", "=", "fields", "[", "\"metadata\"", "]", "[", "\"words\"", "]", "\n", "write_to_conll_eval_file", "(", "\n", "prediction_file", ",", "gold_file", ",", "verb_index", ",", "sentence", ",", "prediction", ",", "gold_tags", "\n", ")", "\n", "", "prediction_file", ".", "close", "(", ")", "\n", "gold_file", ".", "close", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.ai2_internal.resume_daemon_test.ResumeDaemonTest.setUp": [[21, 25], ["super().setUp", "sqlite3.connect", "scripts.ai2_internal.resume_daemon.create_table"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.ai2_internal.resume_daemon_test.ResumeDaemonTest.setUp", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.ai2_internal.resume_daemon.create_table"], ["    ", "def", "setUp", "(", "self", ")", ":", "\n", "        ", "super", "(", ")", ".", "setUp", "(", ")", "\n", "self", ".", "connection", "=", "sqlite3", ".", "connect", "(", "\":memory:\"", ")", "\n", "create_table", "(", "self", ".", "connection", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.ai2_internal.resume_daemon_test.ResumeDaemonTest.test_create_beaker_status_works": [[26, 29], ["scripts.ai2_internal.resume_daemon.BeakerStatus"], "methods", ["None"], ["", "def", "test_create_beaker_status_works", "(", "self", ")", ":", "\n", "        ", "status", "=", "BeakerStatus", "(", "\"stopped\"", ")", "\n", "assert", "status", ".", "name", "==", "\"stopped\"", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.ai2_internal.resume_daemon_test.ResumeDaemonTest.test_create_beaker_status_throws": [[30, 34], ["pytest.raises", "scripts.ai2_internal.resume_daemon.BeakerStatus"], "methods", ["None"], ["", "def", "test_create_beaker_status_throws", "(", "self", ")", ":", "\n", "        ", "with", "pytest", ".", "raises", "(", "ValueError", ")", ":", "\n", "            ", "status", "=", "BeakerStatus", "(", "\"garbage\"", ")", "\n", "assert", "status", ".", "name", "==", "\"garbage\"", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.ai2_internal.resume_daemon_test.ResumeDaemonTest.test_does_nothing_on_empty_db": [[35, 39], ["unittest.mock.Mock", "scripts.ai2_internal.resume_daemon.resume"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.ai2_internal.resume_daemon.resume"], ["", "", "def", "test_does_nothing_on_empty_db", "(", "self", ")", ":", "\n", "        ", "beaker", "=", "Mock", "(", ")", "\n", "resume", "(", "self", ".", "connection", ",", "beaker", ")", "\n", "assert", "not", "beaker", ".", "method_calls", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.ai2_internal.resume_daemon_test.ResumeDaemonTest.test_does_not_resume_a_running_experiment": [[40, 48], ["unittest.mock.Mock", "scripts.ai2_internal.resume_daemon.start_autoresume", "scripts.ai2_internal.resume_daemon.resume", "unittest.mock.Mock.get_status.assert_called", "len"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.ai2_internal.resume_daemon.start_autoresume", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.ai2_internal.resume_daemon.resume"], ["", "def", "test_does_not_resume_a_running_experiment", "(", "self", ")", ":", "\n", "        ", "beaker", "=", "Mock", "(", ")", "\n", "experiment_id", "=", "\"foo\"", "\n", "start_autoresume", "(", "self", ".", "connection", ",", "experiment_id", ",", "5", ")", "\n", "beaker", ".", "get_status", ".", "return_value", "=", "BeakerStatus", ".", "running", "\n", "resume", "(", "self", ".", "connection", ",", "beaker", ")", "\n", "beaker", ".", "get_status", ".", "assert_called", "(", ")", "\n", "assert", "len", "(", "beaker", ".", "method_calls", ")", "==", "1", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.ai2_internal.resume_daemon_test.ResumeDaemonTest.test_does_not_resume_a_finished_experiment": [[49, 57], ["unittest.mock.Mock", "scripts.ai2_internal.resume_daemon.start_autoresume", "scripts.ai2_internal.resume_daemon.resume", "unittest.mock.Mock.get_status.assert_called", "len"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.ai2_internal.resume_daemon.start_autoresume", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.ai2_internal.resume_daemon.resume"], ["", "def", "test_does_not_resume_a_finished_experiment", "(", "self", ")", ":", "\n", "        ", "beaker", "=", "Mock", "(", ")", "\n", "experiment_id", "=", "\"foo\"", "\n", "start_autoresume", "(", "self", ".", "connection", ",", "experiment_id", ",", "5", ")", "\n", "beaker", ".", "get_status", ".", "return_value", "=", "BeakerStatus", ".", "succeeded", "\n", "resume", "(", "self", ".", "connection", ",", "beaker", ")", "\n", "beaker", ".", "get_status", ".", "assert_called", "(", ")", "\n", "assert", "len", "(", "beaker", ".", "method_calls", ")", "==", "1", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.ai2_internal.resume_daemon_test.ResumeDaemonTest.test_does_resume_a_preempted_experiment": [[58, 68], ["unittest.mock.Mock", "scripts.ai2_internal.resume_daemon.start_autoresume", "scripts.ai2_internal.resume_daemon.resume", "unittest.mock.Mock.get_status.assert_called", "unittest.mock.Mock.resume.assert_called", "len"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.ai2_internal.resume_daemon.start_autoresume", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.ai2_internal.resume_daemon.resume"], ["", "def", "test_does_resume_a_preempted_experiment", "(", "self", ")", ":", "\n", "        ", "beaker", "=", "Mock", "(", ")", "\n", "experiment_id", "=", "\"foo\"", "\n", "start_autoresume", "(", "self", ".", "connection", ",", "experiment_id", ",", "5", ")", "\n", "beaker", ".", "get_status", ".", "return_value", "=", "BeakerStatus", ".", "preempted", "\n", "beaker", ".", "resume", ".", "return_value", "=", "\"foo2\"", "\n", "resume", "(", "self", ".", "connection", ",", "beaker", ")", "\n", "beaker", ".", "get_status", ".", "assert_called", "(", ")", "\n", "beaker", ".", "resume", ".", "assert_called", "(", ")", "\n", "assert", "len", "(", "beaker", ".", "method_calls", ")", "==", "2", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.ai2_internal.resume_daemon_test.ResumeDaemonTest.test_respects_upper_bound_on_resumes": [[69, 91], ["unittest.mock.Mock", "scripts.ai2_internal.resume_daemon.start_autoresume", "range", "unittest.mock.Mock.assert_has_calls", "scripts.ai2_internal.resume_daemon.resume", "unittest.mock.call.get_status", "unittest.mock.call.resume", "unittest.mock.call.get_status", "unittest.mock.call.resume", "unittest.mock.call.get_status", "unittest.mock.call.resume", "unittest.mock.call.get_status", "unittest.mock.call.resume", "unittest.mock.call.get_status", "unittest.mock.call.resume", "unittest.mock.call.get_status"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.ai2_internal.resume_daemon.start_autoresume", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.ai2_internal.resume_daemon.resume", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.ai2_internal.resume_daemon.BeakerWrapper.get_status", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.ai2_internal.resume_daemon.resume", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.ai2_internal.resume_daemon.BeakerWrapper.get_status", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.ai2_internal.resume_daemon.resume", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.ai2_internal.resume_daemon.BeakerWrapper.get_status", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.ai2_internal.resume_daemon.resume", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.ai2_internal.resume_daemon.BeakerWrapper.get_status", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.ai2_internal.resume_daemon.resume", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.ai2_internal.resume_daemon.BeakerWrapper.get_status", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.ai2_internal.resume_daemon.resume", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.ai2_internal.resume_daemon.BeakerWrapper.get_status"], ["", "def", "test_respects_upper_bound_on_resumes", "(", "self", ")", ":", "\n", "        ", "beaker", "=", "Mock", "(", ")", "\n", "experiment_id", "=", "\"foo\"", "\n", "start_autoresume", "(", "self", ".", "connection", ",", "experiment_id", ",", "5", ")", "\n", "beaker", ".", "get_status", ".", "return_value", "=", "BeakerStatus", ".", "preempted", "\n", "for", "i", "in", "range", "(", "10", ")", ":", "\n", "            ", "beaker", ".", "resume", ".", "return_value", "=", "f\"foo{i}\"", "\n", "resume", "(", "self", ".", "connection", ",", "beaker", ")", "\n", "", "calls", "=", "[", "\n", "call", ".", "get_status", "(", "\"foo\"", ")", ",", "\n", "call", ".", "resume", "(", "\"foo\"", ")", ",", "\n", "call", ".", "get_status", "(", "\"foo0\"", ")", ",", "\n", "call", ".", "resume", "(", "\"foo0\"", ")", ",", "\n", "call", ".", "get_status", "(", "\"foo1\"", ")", ",", "\n", "call", ".", "resume", "(", "\"foo1\"", ")", ",", "\n", "call", ".", "get_status", "(", "\"foo2\"", ")", ",", "\n", "call", ".", "resume", "(", "\"foo2\"", ")", ",", "\n", "call", ".", "get_status", "(", "\"foo3\"", ")", ",", "\n", "call", ".", "resume", "(", "\"foo3\"", ")", ",", "\n", "call", ".", "get_status", "(", "\"foo4\"", ")", ",", "\n", "]", "\n", "beaker", ".", "assert_has_calls", "(", "calls", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.ai2_internal.resume_daemon_test.ResumeDaemonTest.test_handles_a_realistic_scenario": [[92, 110], ["unittest.mock.Mock", "scripts.ai2_internal.resume_daemon.start_autoresume", "range", "unittest.mock.Mock.assert_has_calls", "scripts.ai2_internal.resume_daemon.resume", "unittest.mock.call.get_status", "unittest.mock.call.resume", "unittest.mock.call.get_status", "unittest.mock.call.resume", "unittest.mock.call.get_status"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.ai2_internal.resume_daemon.start_autoresume", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.ai2_internal.resume_daemon.resume", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.ai2_internal.resume_daemon.BeakerWrapper.get_status", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.ai2_internal.resume_daemon.resume", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.ai2_internal.resume_daemon.BeakerWrapper.get_status", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.ai2_internal.resume_daemon.resume", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.ai2_internal.resume_daemon.BeakerWrapper.get_status"], ["", "def", "test_handles_a_realistic_scenario", "(", "self", ")", ":", "\n", "        ", "beaker", "=", "Mock", "(", ")", "\n", "experiment_id", "=", "\"foo\"", "\n", "start_autoresume", "(", "self", ".", "connection", ",", "experiment_id", ",", "5", ")", "\n", "beaker", ".", "get_status", ".", "return_value", "=", "BeakerStatus", ".", "preempted", "\n", "for", "i", "in", "range", "(", "10", ")", ":", "\n", "            ", "beaker", ".", "resume", ".", "return_value", "=", "f\"foo{i}\"", "\n", "if", "i", "==", "2", ":", "\n", "                ", "beaker", ".", "get_status", ".", "return_value", "=", "BeakerStatus", ".", "succeeded", "\n", "", "resume", "(", "self", ".", "connection", ",", "beaker", ")", "\n", "", "calls", "=", "[", "\n", "call", ".", "get_status", "(", "\"foo\"", ")", ",", "\n", "call", ".", "resume", "(", "\"foo\"", ")", ",", "\n", "call", ".", "get_status", "(", "\"foo0\"", ")", ",", "\n", "call", ".", "resume", "(", "\"foo0\"", ")", ",", "\n", "call", ".", "get_status", "(", "\"foo1\"", ")", ",", "\n", "]", "\n", "beaker", ".", "assert_has_calls", "(", "calls", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.ai2_internal.resume_daemon.BeakerStatus.__str__": [[70, 72], ["None"], "methods", ["None"], ["def", "__str__", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "name", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.ai2_internal.resume_daemon.BeakerStatus.is_end_state": [[73, 86], ["None"], "methods", ["None"], ["", "def", "is_end_state", "(", "self", ")", ":", "\n", "        ", "if", "self", "is", "BeakerStatus", ".", "preempted", ":", "\n", "            ", "return", "True", "\n", "", "elif", "self", "is", "BeakerStatus", ".", "succeeded", ":", "\n", "            ", "return", "True", "\n", "", "elif", "self", "is", "BeakerStatus", ".", "skipped", ":", "\n", "            ", "return", "True", "\n", "", "elif", "self", "is", "BeakerStatus", ".", "stopped", ":", "\n", "            ", "return", "True", "\n", "", "elif", "self", "is", "BeakerStatus", ".", "failed", ":", "\n", "            ", "return", "True", "\n", "", "else", ":", "\n", "            ", "return", "False", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.ai2_internal.resume_daemon.BeakerWrapper.get_status": [[89, 147], ["subprocess.check_output", "json.loads", "resume_daemon.BeakerStatus", "time.sleep", "len", "len"], "methods", ["None"], ["    ", "def", "get_status", "(", "self", ",", "experiment_id", ":", "str", ")", "->", "BeakerStatus", ":", "\n", "        ", "command", "=", "[", "\"beaker\"", ",", "\"experiment\"", ",", "\"inspect\"", ",", "experiment_id", "]", "\n", "experiment_json", "=", "subprocess", ".", "check_output", "(", "command", ")", "\n", "\n", "# Example output from beaker.", "\n", "# brendanr.local$ beaker experiment inspect ex_g7knlblsjxxk", "\n", "# [", "\n", "#     {", "\n", "#         \"id\": \"ex_g7knlblsjxxk\",", "\n", "#         \"owner\": {", "\n", "#             \"id\": \"us_a4hw8yvr3xut\",", "\n", "#             \"name\": \"ai2\",", "\n", "#             \"displayName\": \"AI2\"", "\n", "#         },", "\n", "#         \"author\": {", "\n", "#             \"id\": \"us_hl8x796649u9\",", "\n", "#             \"name\": \"brendanr\",", "\n", "#             \"displayName\": \"Brendan Roof\"", "\n", "#         },", "\n", "#         \"workspace\": \"\",", "\n", "#         \"user\": {", "\n", "#             \"id\": \"\",", "\n", "#             \"name\": \"\",", "\n", "#             \"displayName\": \"\"", "\n", "#         },", "\n", "#         \"nodes\": [", "\n", "#             {", "\n", "#                 \"name\": \"training\",", "\n", "#                 \"task_id\": \"\",", "\n", "#                 \"taskId\": \"tk_64wm85lc3f0m\",", "\n", "#                 \"result_id\": \"\",", "\n", "#                 \"resultId\": \"ds_du02un92r57b\",", "\n", "#                 \"status\": \"initializing\",", "\n", "#                 \"child_task_ids\": null,", "\n", "#                 \"childTaskIds\": [],", "\n", "#                 \"parent_task_ids\": null,", "\n", "#                 \"parentTaskIds\": []", "\n", "#             }", "\n", "#         ],", "\n", "#         \"created\": \"2019-09-25T02:03:30.820437Z\",", "\n", "#         \"archived\": false", "\n", "#     }", "\n", "# ]", "\n", "\n", "experiment_data", "=", "json", ".", "loads", "(", "experiment_json", ")", "\n", "# Beaker lets there be multiple tasks in a single experiment. Here we", "\n", "# just try to handle the simple case of single task experiments like", "\n", "# those created by run_with_beaker.py.", "\n", "assert", "(", "\n", "len", "(", "experiment_data", ")", "==", "1", "\n", ")", ",", "\"Experiment not created with run_with_beaker.py\"", "\n", "assert", "(", "\n", "len", "(", "experiment_data", "[", "0", "]", "[", "\"nodes\"", "]", ")", "==", "1", "\n", ")", ",", "\"Experiment not created with run_with_beaker.py\"", "\n", "status", "=", "BeakerStatus", "(", "experiment_data", "[", "0", "]", "[", "\"nodes\"", "]", "[", "0", "]", "[", "\"status\"", "]", ")", "\n", "# Small delay to avoid thrashing Beaker.", "\n", "time", ".", "sleep", "(", "BEAKER_QUERY_INTERVAL_SECONDS", ")", "\n", "return", "status", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.ai2_internal.resume_daemon.BeakerWrapper.resume": [[148, 158], ["time.sleep", "subprocess.check_output().strip", "subprocess.check_output"], "methods", ["None"], ["", "def", "resume", "(", "self", ",", "experiment_id", ":", "str", ")", "->", "str", ":", "\n", "        ", "command", "=", "[", "\n", "\"beaker\"", ",", "\n", "\"experiment\"", ",", "\n", "\"resume\"", ",", "\n", "f\"--experiment-name={experiment_id}\"", ",", "\n", "]", "\n", "# Small delay to avoid thrashing Beaker.", "\n", "time", ".", "sleep", "(", "BEAKER_QUERY_INTERVAL_SECONDS", ")", "\n", "return", "subprocess", ".", "check_output", "(", "command", ",", "universal_newlines", "=", "True", ")", ".", "strip", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.ai2_internal.resume_daemon.Action.__str__": [[241, 243], ["None"], "methods", ["None"], ["def", "__str__", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "name", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.ai2_internal.resume_daemon.create_table": [[160, 168], ["connection.cursor", "connection.cursor.execute", "connection.commit"], "function", ["None"], ["", "", "def", "create_table", "(", "connection", ":", "Connection", ")", "->", "None", ":", "\n", "    ", "cursor", "=", "connection", ".", "cursor", "(", ")", "\n", "create_table_statement", "=", "\"\"\"\n    CREATE TABLE active_experiments\n    (experiment_id TEXT PRIMARY KEY, original_id TEXT, max_resumes INTEGER, current_resume INTEGER)\n    \"\"\"", "\n", "cursor", ".", "execute", "(", "create_table_statement", ")", "\n", "connection", ".", "commit", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.ai2_internal.resume_daemon.start_autoresume": [[170, 179], ["connection.cursor", "connection.cursor.execute", "connection.commit"], "function", ["None"], ["", "def", "start_autoresume", "(", "\n", "connection", ":", "Connection", ",", "experiment_id", ":", "str", ",", "max_resumes", ":", "int", "\n", ")", "->", "None", ":", "\n", "    ", "cursor", "=", "connection", ".", "cursor", "(", ")", "\n", "cursor", ".", "execute", "(", "\n", "\"INSERT INTO active_experiments VALUES (?, ?, ?, ?)\"", ",", "\n", "(", "experiment_id", ",", "experiment_id", ",", "max_resumes", ",", "0", ")", ",", "\n", ")", "\n", "connection", ".", "commit", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.ai2_internal.resume_daemon.stop_autoresume": [[181, 192], ["connection.cursor", "connection.cursor.execute", "connection.cursor.fetchall", "connection.cursor.execute", "connection.commit"], "function", ["None"], ["", "def", "stop_autoresume", "(", "connection", ":", "Connection", ",", "experiment_id", ":", "str", ")", "->", "None", ":", "\n", "    ", "cursor", "=", "connection", ".", "cursor", "(", ")", "\n", "cursor", ".", "execute", "(", "\n", "\"SELECT * FROM active_experiments WHERE experiment_id = ?\"", ",", "(", "experiment_id", ",", ")", "\n", ")", "\n", "result", "=", "cursor", ".", "fetchall", "(", ")", "\n", "assert", "result", ",", "f\"Experiment {experiment_id} not found!\"", "\n", "cursor", ".", "execute", "(", "\n", "\"DELETE FROM active_experiments WHERE experiment_id = ?\"", ",", "(", "experiment_id", ",", ")", "\n", ")", "\n", "connection", ".", "commit", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.ai2_internal.resume_daemon.resume": [[194, 232], ["logger.info", "connection.cursor", "connection.cursor.execute", "connection.cursor.fetchall", "beaker.get_status", "beaker.get_status.is_end_state", "resume_daemon.stop_autoresume", "logger.info", "logger.info", "beaker.resume", "logger.info", "connection.cursor.execute", "connection.commit"], "function", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.dataset_readers.multiprocess_dataset_reader.logger.info", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.ai2_internal.resume_daemon.BeakerWrapper.get_status", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.ai2_internal.resume_daemon.BeakerStatus.is_end_state", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.ai2_internal.resume_daemon.stop_autoresume", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.dataset_readers.multiprocess_dataset_reader.logger.info", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.dataset_readers.multiprocess_dataset_reader.logger.info", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.ai2_internal.resume_daemon.resume", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.dataset_readers.multiprocess_dataset_reader.logger.info"], ["", "def", "resume", "(", "connection", ":", "Connection", ",", "beaker", ":", "BeakerWrapper", ")", "->", "None", ":", "\n", "    ", "logger", ".", "info", "(", "\"Checking if resumes are needed.\"", ")", "\n", "\n", "cursor", "=", "connection", ".", "cursor", "(", ")", "\n", "cursor", ".", "execute", "(", "\"SELECT * FROM active_experiments\"", ")", "\n", "experiments", "=", "cursor", ".", "fetchall", "(", ")", "\n", "\n", "for", "experiment_row", "in", "experiments", ":", "\n", "        ", "experiment_id", ",", "original_id", ",", "max_resumes", ",", "current_resume", "=", "experiment_row", "\n", "status", "=", "beaker", ".", "get_status", "(", "experiment_id", ")", "\n", "if", "status", ".", "is_end_state", "(", ")", ":", "\n", "            ", "stop_autoresume", "(", "connection", ",", "experiment_id", ")", "\n", "if", "status", "is", "BeakerStatus", ".", "preempted", ":", "\n", "                ", "if", "current_resume", ">=", "max_resumes", ":", "\n", "                    ", "logger", ".", "info", "(", "\n", "f\"Experiment {experiment_id} preempted too many times \"", "\n", "f\"({max_resumes}). Original experiment: {original_id}\"", "\n", ")", "\n", "", "else", ":", "\n", "                    ", "new_experiment_id", "=", "beaker", ".", "resume", "(", "experiment_id", ")", "\n", "logger", ".", "info", "(", "\n", "f\"Experiment {experiment_id} preempted \"", "\n", "f\"({current_resume}/{max_resumes}). Resuming as: \"", "\n", "f\"{new_experiment_id} Original experiment: {original_id}\"", "\n", ")", "\n", "cursor", ".", "execute", "(", "\n", "\"INSERT INTO active_experiments VALUES (?, ?, ?, ?)\"", ",", "\n", "(", "\n", "new_experiment_id", ",", "\n", "original_id", ",", "\n", "max_resumes", ",", "\n", "current_resume", "+", "1", ",", "\n", ")", ",", "\n", ")", "\n", "connection", ".", "commit", "(", ")", "\n", "", "", "else", ":", "\n", "                ", "logger", ".", "info", "(", "\n", "f\"Experiment {experiment_id} completed with status: \"", "\n", "f\"{status}. Original experiment: {original_id}\"", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.ai2_internal.resume_daemon.main": [[245, 300], ["time.sleep", "sqlite3.connect", "sqlite3.connect.cursor", "connection.cursor.execute", "connection.cursor.fetchall", "subprocess.run", "os.path.abspath", "sqlite3.connect.close", "random.randint", "resume_daemon.create_table", "subprocess.run", "resume_daemon.start_autoresume", "resume_daemon.stop_autoresume", "resume_daemon.BeakerWrapper", "resume_daemon.resume", "Exception"], "function", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.commands.train.TrainModel.run", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.token_embedders.embedding.EmbeddingsTextFile.close", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.ai2_internal.resume_daemon.create_table", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.commands.train.TrainModel.run", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.ai2_internal.resume_daemon.start_autoresume", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.ai2_internal.resume_daemon.stop_autoresume", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.ai2_internal.resume_daemon.resume"], ["", "", "def", "main", "(", "args", ")", "->", "None", ":", "\n", "# Smooth load from potentially many daemons on different machines.", "\n", "    ", "time", ".", "sleep", "(", "random", ".", "randint", "(", "0", ",", "args", ".", "random_delay_seconds", ")", ")", "\n", "\n", "db_path", "=", "f\"{dot_allennlp_dir}/resume.db\"", "\n", "connection", "=", "sqlite3", ".", "connect", "(", "db_path", ")", "\n", "\n", "# Create the DB if needed.", "\n", "cursor", "=", "connection", ".", "cursor", "(", ")", "\n", "cursor", ".", "execute", "(", "\n", "\"SELECT name FROM sqlite_master WHERE type='table' AND name='active_experiments'\"", "\n", ")", "\n", "tables", "=", "cursor", ".", "fetchall", "(", ")", "\n", "if", "not", "tables", ":", "\n", "        ", "create_table", "(", "connection", ")", "\n", "\n", "# Modify the crontab if needed.", "\n", "", "crontab_l_result", "=", "subprocess", ".", "run", "(", "\n", "[", "\"crontab\"", ",", "\"-l\"", "]", ",", "universal_newlines", "=", "True", ",", "stdout", "=", "PIPE", ",", "stderr", "=", "PIPE", "\n", ")", "\n", "if", "crontab_l_result", ".", "returncode", "==", "0", ":", "\n", "        ", "current_crontab", "=", "crontab_l_result", ".", "stdout", "\n", "", "else", ":", "\n", "# `crontab -l` fails when a crontab hasn't been installed previously.", "\n", "# Sanity check the error message to guard against blowing away the", "\n", "# crontab in some obscure failure case.", "\n", "        ", "assert", "(", "\n", "\"no crontab\"", "in", "crontab_l_result", ".", "stderr", "\n", ")", ",", "f\"crontab failed: {crontab_l_result.stderr}\"", "\n", "current_crontab", "=", "\"\"", "\n", "\n", "", "full_path", "=", "os", ".", "path", ".", "abspath", "(", "__file__", ")", "\n", "if", "full_path", "not", "in", "current_crontab", ":", "\n", "# Execute this script every ten minutes. We set the PATH to that used", "\n", "# to run this install step to make sure that we have access to python3", "\n", "# and beaker.", "\n", "        ", "cron_line", "=", "(", "\n", "f\"*/10 * * * * bash -c 'export PATH={os.environ['PATH']};\"", "\n", "f\" python3 {full_path} --action=resume --random-delay-seconds=60'\\n\"", "\n", ")", "\n", "new_crontab", "=", "current_crontab", "+", "cron_line", "\n", "subprocess", ".", "run", "(", "[", "\"crontab\"", ",", "\"-\"", "]", ",", "input", "=", "new_crontab", ",", "encoding", "=", "\"utf-8\"", ")", "\n", "\n", "", "if", "args", ".", "action", "is", "Action", ".", "start", ":", "\n", "        ", "assert", "args", ".", "experiment_id", "\n", "start_autoresume", "(", "connection", ",", "args", ".", "experiment_id", ",", "args", ".", "max_resumes", ")", "\n", "", "elif", "args", ".", "action", "is", "Action", ".", "stop", ":", "\n", "        ", "assert", "args", ".", "experiment_id", "\n", "stop_autoresume", "(", "connection", ",", "args", ".", "experiment_id", ")", "\n", "", "elif", "args", ".", "action", "is", "Action", ".", "resume", ":", "\n", "        ", "beaker", "=", "BeakerWrapper", "(", ")", "\n", "resume", "(", "connection", ",", "beaker", ")", "\n", "", "else", ":", "\n", "        ", "raise", "Exception", "(", "f\"Unaccounted for action {args.action}\"", ")", "\n", "", "connection", ".", "close", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.ai2_internal.run_with_beaker.main": [[27, 194], ["subprocess.check_output().strip", "allennlp.common.params.Params.from_file", "tempfile.mkdtemp", "os.path.join", "Params.from_file.to_file", "print", "Params.from_file.as_flat_dict", "params.as_flat_dict.items", "subprocess.run", "subprocess.check_output().strip", "print", "var.split", "str().replace", "str", "print", "print", "subprocess.run", "print", "subprocess.check_output().strip", "print", "allennlp_suffix.append", "allennlp_suffix.append", "source.split", "dataset_mounts.append", "var.split", "float", "int", "open", "output.write", "experiment_command.append", "experiment_command.append", "os.path.join", "print", "print", "print", "print", "subprocess.check_output().strip", "print", "subprocess.check_output", "subprocess.check_output", "tempfile.mkstemp", "json.dumps", "args.name.replace", "os.path.dirname", "print", "print", "print", "print", "subprocess.run", "str", "subprocess.check_output", "subprocess.check_output", "run_with_beaker.main.resume_command"], "function", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.params.Params.from_file", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.dataset_readers.multiprocess_dataset_reader.QIterable.join", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.params.Params.to_file", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.params.Params.as_flat_dict", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.commands.train.TrainModel.run", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.commands.train.TrainModel.run", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.None.predictor.write", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.dataset_readers.multiprocess_dataset_reader.QIterable.join", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.commands.train.TrainModel.run"], ["def", "main", "(", "param_file", ":", "str", ",", "args", ":", "argparse", ".", "Namespace", ")", ":", "\n", "    ", "commit", "=", "subprocess", ".", "check_output", "(", "\n", "[", "\"git\"", ",", "\"rev-parse\"", ",", "\"HEAD\"", "]", ",", "universal_newlines", "=", "True", "\n", ")", ".", "strip", "(", ")", "\n", "docker_image", "=", "f\"allennlp/allennlp:{commit}\"", "\n", "overrides", "=", "args", ".", "overrides", "\n", "\n", "# Reads params and sets environment.", "\n", "ext_vars", "=", "{", "}", "\n", "\n", "for", "var", "in", "args", ".", "env", ":", "\n", "        ", "key", ",", "value", "=", "var", ".", "split", "(", "\"=\"", ")", "\n", "ext_vars", "[", "key", "]", "=", "value", "\n", "\n", "", "params", "=", "Params", ".", "from_file", "(", "param_file", ",", "overrides", ",", "ext_vars", ")", "\n", "\n", "# Write params as json. Otherwise Jsonnet's import feature breaks.", "\n", "params_dir", "=", "tempfile", ".", "mkdtemp", "(", "prefix", "=", "\"config\"", ")", "\n", "compiled_params_path", "=", "os", ".", "path", ".", "join", "(", "params_dir", ",", "\"config.json\"", ")", "\n", "params", ".", "to_file", "(", "compiled_params_path", ")", "\n", "print", "(", "f\"Compiled jsonnet config written to {compiled_params_path}.\"", ")", "\n", "\n", "flat_params", "=", "params", ".", "as_flat_dict", "(", ")", "\n", "env", "=", "{", "}", "\n", "for", "k", ",", "v", "in", "flat_params", ".", "items", "(", ")", ":", "\n", "        ", "k", "=", "str", "(", "k", ")", ".", "replace", "(", "\".\"", ",", "\"_\"", ")", "\n", "env", "[", "k", "]", "=", "str", "(", "v", ")", "\n", "\n", "# If the git repository is dirty, add a random hash.", "\n", "", "result", "=", "subprocess", ".", "run", "(", "\"git diff-index --quiet HEAD --\"", ",", "shell", "=", "True", ")", "\n", "if", "result", ".", "returncode", "!=", "0", ":", "\n", "        ", "dirty_hash", "=", "\"%x\"", "%", "random_int", "\n", "docker_image", "+=", "\"-\"", "+", "dirty_hash", "\n", "\n", "", "if", "args", ".", "image", ":", "\n", "        ", "image", "=", "args", ".", "image", "\n", "print", "(", "f\"Using the specified image: {image}\"", ")", "\n", "", "else", ":", "\n", "        ", "print", "(", "f\"Building the Docker image ({docker_image})...\"", ")", "\n", "subprocess", ".", "run", "(", "f\"docker build -t {docker_image} .\"", ",", "shell", "=", "True", ",", "check", "=", "True", ")", "\n", "\n", "print", "(", "f\"Create a Beaker image...\"", ")", "\n", "image", "=", "subprocess", ".", "check_output", "(", "\n", "f\"beaker image create --quiet {docker_image}\"", ",", "\n", "shell", "=", "True", ",", "\n", "universal_newlines", "=", "True", ",", "\n", ")", ".", "strip", "(", ")", "\n", "print", "(", "f\"  Image created: {docker_image}\"", ")", "\n", "\n", "", "config_dataset_id", "=", "subprocess", ".", "check_output", "(", "\n", "f\"beaker dataset create --quiet {params_dir}/*\"", ",", "\n", "shell", "=", "True", ",", "\n", "universal_newlines", "=", "True", ",", "\n", ")", ".", "strip", "(", ")", "\n", "\n", "# Arguments that differ between preemptible and regular machine execution.", "\n", "if", "args", ".", "preemptible", ":", "\n", "        ", "allennlp_prefix", "=", "[", "\n", "\"/stage/allennlp/resumable_train.sh\"", ",", "\n", "\"/output\"", ",", "\n", "\"/config/config.json\"", ",", "\n", "]", "\n", "", "else", ":", "\n", "        ", "allennlp_prefix", "=", "[", "\n", "\"python\"", ",", "\n", "\"-m\"", ",", "\n", "\"allennlp.run\"", ",", "\n", "\"train\"", ",", "\n", "\"/config/config.json\"", ",", "\n", "\"-s\"", ",", "\n", "\"/output\"", ",", "\n", "]", "\n", "\n", "# All other arguments", "\n", "", "allennlp_suffix", "=", "[", "\"--file-friendly-logging\"", "]", "\n", "for", "package_name", "in", "args", ".", "include_package", ":", "\n", "        ", "allennlp_suffix", ".", "append", "(", "\"--include-package\"", ")", "\n", "allennlp_suffix", ".", "append", "(", "package_name", ")", "\n", "\n", "", "allennlp_command", "=", "allennlp_prefix", "+", "allennlp_suffix", "\n", "\n", "dataset_mounts", "=", "[", "]", "\n", "for", "source", "in", "args", ".", "source", "+", "[", "f\"{config_dataset_id}:/config\"", "]", ":", "\n", "        ", "datasetId", ",", "containerPath", "=", "source", ".", "split", "(", "\":\"", ")", "\n", "dataset_mounts", ".", "append", "(", "{", "\"datasetId\"", ":", "datasetId", ",", "\"containerPath\"", ":", "containerPath", "}", ")", "\n", "\n", "", "for", "var", "in", "args", ".", "env", ":", "\n", "        ", "key", ",", "value", "=", "var", ".", "split", "(", "\"=\"", ")", "\n", "env", "[", "key", "]", "=", "value", "\n", "\n", "", "requirements", "=", "{", "}", "\n", "if", "args", ".", "cpu", ":", "\n", "        ", "requirements", "[", "\"cpu\"", "]", "=", "float", "(", "args", ".", "cpu", ")", "\n", "", "if", "args", ".", "memory", ":", "\n", "        ", "requirements", "[", "\"memory\"", "]", "=", "args", ".", "memory", "\n", "", "if", "args", ".", "gpu_count", ":", "\n", "        ", "requirements", "[", "\"gpuCount\"", "]", "=", "int", "(", "args", ".", "gpu_count", ")", "\n", "", "if", "args", ".", "preemptible", ":", "\n", "        ", "requirements", "[", "\"preemptible\"", "]", "=", "True", "\n", "", "config_spec", "=", "{", "\n", "\"description\"", ":", "args", ".", "desc", ",", "\n", "\"image\"", ":", "image", ",", "\n", "\"resultPath\"", ":", "\"/output\"", ",", "\n", "\"args\"", ":", "allennlp_command", ",", "\n", "\"datasetMounts\"", ":", "dataset_mounts", ",", "\n", "\"requirements\"", ":", "requirements", ",", "\n", "\"env\"", ":", "env", ",", "\n", "}", "\n", "config_task", "=", "{", "\"spec\"", ":", "config_spec", ",", "\"name\"", ":", "\"training\"", "}", "\n", "\n", "config", "=", "{", "\"tasks\"", ":", "[", "config_task", "]", "}", "\n", "\n", "output_path", "=", "(", "\n", "args", ".", "spec_output_path", "\n", "if", "args", ".", "spec_output_path", "\n", "else", "tempfile", ".", "mkstemp", "(", "\".yaml\"", ",", "\"beaker-config-\"", ")", "[", "1", "]", "\n", ")", "\n", "with", "open", "(", "output_path", ",", "\"w\"", ")", "as", "output", ":", "\n", "        ", "output", ".", "write", "(", "json", ".", "dumps", "(", "config", ",", "indent", "=", "4", ")", ")", "\n", "", "print", "(", "f\"Beaker spec written to {output_path}.\"", ")", "\n", "\n", "experiment_command", "=", "[", "\n", "\"beaker\"", ",", "\n", "\"experiment\"", ",", "\n", "\"create\"", ",", "\n", "\"--quiet\"", ",", "\n", "\"--file\"", ",", "\n", "output_path", ",", "\n", "]", "\n", "if", "args", ".", "name", ":", "\n", "        ", "experiment_command", ".", "append", "(", "\"--name\"", ")", "\n", "experiment_command", ".", "append", "(", "args", ".", "name", ".", "replace", "(", "\" \"", ",", "\"-\"", ")", ")", "\n", "\n", "", "def", "resume_command", "(", "experiment_id", ")", ":", "\n", "        ", "resume_daemon_path", "=", "os", ".", "path", ".", "join", "(", "os", ".", "path", ".", "dirname", "(", "__file__", ")", ",", "\"resume_daemon.py\"", ")", "\n", "return", "[", "\n", "# Run with python (instead of calling directly) in case the", "\n", "# executable bit wasn't preserved for some reason.", "\n", "\"python3\"", ",", "\n", "resume_daemon_path", ",", "\n", "\"--action=start\"", ",", "\n", "f\"--max-resumes={args.max_resumes}\"", ",", "\n", "f\"--experiment-id={experiment_id}\"", ",", "\n", "]", "\n", "\n", "", "if", "args", ".", "dry_run", ":", "\n", "        ", "print", "(", "\n", "\"This is a dry run (--dry-run).  Launch your job with the following command:\"", "\n", ")", "\n", "print", "(", "\"    \"", "+", "\" \"", ".", "join", "(", "experiment_command", ")", ")", "\n", "if", "args", ".", "max_resumes", ">", "0", ":", "\n", "            ", "print", "(", "\"Configure auto-resumes with the following command:\"", ")", "\n", "print", "(", "\"    \"", "+", "\" \"", ".", "join", "(", "resume_command", "(", "\"$YOUR_EXPERIMENT_ID\"", ")", ")", ")", "\n", "", "", "else", ":", "\n", "        ", "print", "(", "\"Running the experiment:\"", ")", "\n", "print", "(", "\"    \"", "+", "\" \"", ".", "join", "(", "experiment_command", ")", ")", "\n", "experiment_id", "=", "subprocess", ".", "check_output", "(", "\n", "experiment_command", ",", "universal_newlines", "=", "True", "\n", ")", ".", "strip", "(", ")", "\n", "print", "(", "\n", "f\"Experiment {experiment_id} submitted. \"", "\n", "f\"See progress at https://beaker.org/ex/{experiment_id}\"", "\n", ")", "\n", "if", "args", ".", "max_resumes", ">", "0", ":", "\n", "            ", "print", "(", "\"Configuring auto-resumes:\"", ")", "\n", "print", "(", "\"    \"", "+", "\" \"", ".", "join", "(", "resume_command", "(", "experiment_id", ")", ")", ")", "\n", "subprocess", ".", "run", "(", "resume_command", "(", "experiment_id", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.graph_util.graph_encoder_gcn.TAGCN.__init__": [[10, 27], ["torch.Module.__init__", "torch.ModuleList", "torch.ModuleList", "torch.ModuleList", "graph_encoder_gcn.TAGCN.layers.append", "range", "torch.Dropout", "torch.Dropout", "torch.Dropout", "dgl.nn.pytorch.conv.TAGConv", "graph_encoder_gcn.TAGCN.layers.append", "graph_aggregate.GraphAggregate", "dgl.nn.pytorch.conv.TAGConv"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.graph_util.plot.GraphPlot.__init__"], ["    ", "def", "__init__", "(", "\n", "self", ",", "in_dim", ",", "hidden_dim", ",", "num_layers", ",", "activation", ",", "dropout", ",", "aggregation", "=", "\"none\"", "\n", ")", ":", "\n", "        ", "super", "(", "TAGCN", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "layers", "=", "nn", ".", "ModuleList", "(", ")", "\n", "# input layer", "\n", "self", ".", "layers", ".", "append", "(", "TAGConv", "(", "in_dim", ",", "hidden_dim", ",", "activation", "=", "activation", ")", ")", "\n", "# hidden layers", "\n", "for", "i", "in", "range", "(", "num_layers", "-", "1", ")", ":", "\n", "            ", "self", ".", "layers", ".", "append", "(", "TAGConv", "(", "hidden_dim", ",", "hidden_dim", ",", "activation", "=", "activation", ")", ")", "\n", "# output layer", "\n", "# self.layers.append(TAGConv(hidden_dim, n_classes)) #activation=None", "\n", "", "self", ".", "dropout", "=", "nn", ".", "Dropout", "(", "p", "=", "dropout", ")", "\n", "\n", "self", ".", "aggregation", "=", "aggregation", "\n", "if", "aggregation", "==", "\"weighted\"", ":", "\n", "            ", "self", ".", "G_aggregator", "=", "GraphAggregate", "(", "hidden_dim", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.graph_util.graph_encoder_gcn.TAGCN.forward": [[28, 40], ["enumerate", "layer", "graph_encoder_gcn.TAGCN.G_aggregator", "graph_encoder_gcn.TAGCN.dropout"], "methods", ["None"], ["", "", "def", "forward", "(", "self", ",", "g", ",", "features", ")", ":", "\n", "        ", "h", "=", "features", "\n", "for", "i", ",", "layer", "in", "enumerate", "(", "self", ".", "layers", ")", ":", "\n", "            ", "if", "i", "!=", "0", ":", "\n", "                ", "h", "=", "self", ".", "dropout", "(", "h", ")", "\n", "", "h", "=", "layer", "(", "g", ",", "h", ")", "\n", "\n", "", "if", "self", ".", "aggregation", "==", "\"weighted\"", ":", "\n", "            ", "wh", "=", "self", ".", "G_aggregator", "(", "h", ")", "\n", "return", "wh", "\n", "", "else", ":", "\n", "            ", "return", "h", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.graph_util.graph_encoder_gcn.GCN.__init__": [[43, 63], ["torch.Module.__init__", "torch.ModuleList", "torch.ModuleList", "torch.ModuleList", "graph_encoder_gcn.GCN.layers.append", "range", "torch.Dropout", "torch.Dropout", "torch.Dropout", "dgl.nn.pytorch.GraphConv", "graph_encoder_gcn.GCN.layers.append", "graph_aggregate.GraphAggregate", "graph_aggregate.GraphAggregate", "dgl.nn.pytorch.GraphConv"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.graph_util.plot.GraphPlot.__init__"], ["    ", "def", "__init__", "(", "\n", "self", ",", "in_dim", ",", "hidden_dim", ",", "num_layers", ",", "activation", ",", "dropout", ",", "aggregation", "=", "\"none\"", "\n", ")", ":", "\n", "        ", "super", "(", "GCN", ",", "self", ")", ".", "__init__", "(", ")", "\n", "\n", "self", ".", "layers", "=", "nn", ".", "ModuleList", "(", ")", "\n", "# input layer", "\n", "self", ".", "layers", ".", "append", "(", "GraphConv", "(", "in_dim", ",", "hidden_dim", ",", "activation", "=", "activation", ")", ")", "\n", "# hidden layers", "\n", "for", "i", "in", "range", "(", "num_layers", "-", "1", ")", ":", "\n", "            ", "self", ".", "layers", ".", "append", "(", "GraphConv", "(", "hidden_dim", ",", "hidden_dim", ",", "activation", "=", "activation", ")", ")", "\n", "# output layer", "\n", "# self.layers.append(GraphConv(n_hidden, n_classes))", "\n", "", "self", ".", "dropout", "=", "nn", ".", "Dropout", "(", "p", "=", "dropout", ")", "\n", "\n", "self", ".", "aggregation", "=", "aggregation", "\n", "if", "aggregation", "==", "\"weighted\"", ":", "\n", "            ", "self", ".", "G_aggregator", "=", "GraphAggregate", "(", "hidden_dim", ",", "\"weighted\"", ")", "\n", "", "if", "aggregation", "==", "\"attention\"", ":", "\n", "            ", "self", ".", "G_aggregator", "=", "GraphAggregate", "(", "hidden_dim", ",", "\"attention\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.graph_util.graph_encoder_gcn.GCN.forward": [[64, 73], ["enumerate", "graph_encoder_gcn.GCN.G_aggregator", "layer", "graph_encoder_gcn.GCN.dropout"], "methods", ["None"], ["", "", "def", "forward", "(", "self", ",", "dgl_g", ",", "graph", ",", "features", ")", ":", "\n", "        ", "h", "=", "features", "\n", "for", "i", ",", "layer", "in", "enumerate", "(", "self", ".", "layers", ")", ":", "\n", "            ", "if", "i", "!=", "0", ":", "\n", "                ", "h", "=", "self", ".", "dropout", "(", "h", ")", "\n", "", "h", "=", "layer", "(", "dgl_g", ",", "h", ")", "\n", "\n", "", "wh", "=", "self", ".", "G_aggregator", "(", "graph", ",", "h", ")", "\n", "return", "wh", ",", "h", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.graph_util.graph_aggregate.GraphAggregate.__init__": [[10, 28], ["torch.nn.Module.__init__", "torch.nn.Sequential", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Tanh", "torch.nn.Softmax", "torch.nn.Linear", "torch.nn.Sigmoid"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.graph_util.plot.GraphPlot.__init__"], ["    ", "def", "__init__", "(", "self", ",", "node_hidden_size", ",", "aggregation_type", ")", ":", "\n", "        ", "super", "(", "GraphAggregate", ",", "self", ")", ".", "__init__", "(", ")", "\n", "\n", "# Setting from the paper", "\n", "self", ".", "graph_hidden_size", "=", "node_hidden_size", "\n", "\n", "# Embed graphs", "\n", "self", ".", "node_gating", "=", "nn", ".", "Sequential", "(", "nn", ".", "Linear", "(", "node_hidden_size", ",", "1", ")", ",", "nn", ".", "Sigmoid", "(", ")", ")", "\n", "self", ".", "node_to_graph", "=", "nn", ".", "Linear", "(", "node_hidden_size", ",", "self", ".", "graph_hidden_size", ")", "\n", "\n", "self", ".", "type", "=", "aggregation_type", "\n", "\n", "# additive attention weights", "\n", "self", ".", "W_verbs", "=", "nn", ".", "Linear", "(", "node_hidden_size", ",", "node_hidden_size", ")", "\n", "self", ".", "W_nodes", "=", "nn", ".", "Linear", "(", "node_hidden_size", ",", "node_hidden_size", ")", "\n", "self", ".", "W_out", "=", "nn", ".", "Linear", "(", "node_hidden_size", ",", "1", ")", "\n", "self", ".", "activation", "=", "nn", ".", "Tanh", "(", ")", "\n", "self", ".", "softmax", "=", "nn", ".", "Softmax", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.graph_util.graph_aggregate.GraphAggregate.forward": [[29, 62], ["graph_aggregate.GraphAggregate.node_gating", "graph_aggregate.GraphAggregate.node_to_graph", "verb_reps.mean", "graph_aggregate.GraphAggregate.W_verbs", "graph_aggregate.GraphAggregate.W_nodes", "graph_aggregate.GraphAggregate.softmax", "torch.sum", "graph_aggregate.GraphAggregate.W_out", "graph_aggregate.GraphAggregate.activation"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "graph", ",", "hvs", ")", ":", "\n", "        ", "if", "self", ".", "type", "==", "\"weighted\"", ":", "\n", "            ", "hvs", "=", "hvs", "\n", "whvs", "=", "self", ".", "node_gating", "(", "hvs", ")", "*", "self", ".", "node_to_graph", "(", "\n", "hvs", "\n", ")", "# .sum(0, keepdim=True)", "\n", "return", "whvs", "\n", "", "elif", "self", ".", "type", "==", "\"attention\"", ":", "\n", "            ", "nodes", "=", "graph", "[", "\"nodes\"", "]", "\n", "verb_indices", "=", "[", "node", "[", "\"node_id\"", "]", "for", "node", "in", "nodes", "if", "node", "[", "\"type\"", "]", "==", "\"V\"", "]", "\n", "verb_reps", "=", "hvs", "[", "verb_indices", "]", "\n", "# print(verb_reps.shape, 'verb_reps.shape')", "\n", "verb_reps_aggregated", "=", "verb_reps", ".", "mean", "(", "dim", "=", "0", ")", "\n", "# print(verb_reps_aggregated.shape, \" verb_reps_aggregated.shape\")", "\n", "\n", "transformed_verb_reps", "=", "self", ".", "W_verbs", "(", "verb_reps_aggregated", ")", "\n", "# print(transformed_verb_reps.shape, \"transformed_verb_reps\")", "\n", "transformed_node_reps", "=", "self", ".", "W_nodes", "(", "hvs", ")", "\n", "# print(transformed_node_reps.shape, \" transformed_node_reps\")", "\n", "node_weights", "=", "self", ".", "softmax", "(", "\n", "self", ".", "W_out", "(", "\n", "self", ".", "activation", "(", "transformed_verb_reps", "+", "transformed_node_reps", ")", "\n", ")", "\n", ")", "\n", "# print(node_weights.shape, \"node_weights.shape\")", "\n", "\n", "weighted_nodes", "=", "node_weights", "*", "hvs", "\n", "# print(weighted_nodes.shape, \" weighted_nodes.shape\")", "\n", "\n", "graph_rep", "=", "torch", ".", "sum", "(", "weighted_nodes", ",", "dim", "=", "0", ")", "\n", "# print(graph_rep.shape, \"graph_rep\")", "\n", "# print('\\n')", "\n", "return", "graph_rep", "\n", "", "", "", ""]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.graph_util.graph_encoder_gat.GATLayer.__init__": [[21, 27], ["torch.Module.__init__", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.graph_util.plot.GraphPlot.__init__"], ["    ", "def", "__init__", "(", "self", ",", "in_dim", ",", "out_dim", ")", ":", "\n", "        ", "super", "(", "GATLayer", ",", "self", ")", ".", "__init__", "(", ")", "\n", "# equation (1)", "\n", "self", ".", "fc", "=", "nn", ".", "Linear", "(", "in_dim", ",", "out_dim", ",", "bias", "=", "False", ")", "\n", "# equation (2)", "\n", "self", ".", "attn_fc", "=", "nn", ".", "Linear", "(", "2", "*", "out_dim", ",", "1", ",", "bias", "=", "False", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.graph_util.graph_encoder_gat.GATLayer.edge_attention": [[28, 33], ["torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "graph_encoder_gat.GATLayer.attn_fc", "torch.leaky_relu", "torch.leaky_relu", "torch.leaky_relu"], "methods", ["None"], ["", "def", "edge_attention", "(", "self", ",", "edges", ")", ":", "\n", "# edge UDF for equation (2)", "\n", "        ", "z2", "=", "torch", ".", "cat", "(", "[", "edges", ".", "src", "[", "\"z\"", "]", ",", "edges", ".", "dst", "[", "\"z\"", "]", "]", ",", "dim", "=", "1", ")", "\n", "a", "=", "self", ".", "attn_fc", "(", "z2", ")", "\n", "return", "{", "\"e\"", ":", "F", ".", "leaky_relu", "(", "a", ")", "}", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.graph_util.graph_encoder_gat.GATLayer.message_func": [[34, 37], ["None"], "methods", ["None"], ["", "def", "message_func", "(", "self", ",", "edges", ")", ":", "\n", "# message UDF for equation (3) & (4)", "\n", "        ", "return", "{", "\"z\"", ":", "edges", ".", "src", "[", "\"z\"", "]", ",", "\"e\"", ":", "edges", ".", "data", "[", "\"e\"", "]", "}", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.graph_util.graph_encoder_gat.GATLayer.reduce_func": [[38, 45], ["torch.softmax", "torch.softmax", "torch.softmax", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum"], "methods", ["None"], ["", "def", "reduce_func", "(", "self", ",", "nodes", ")", ":", "\n", "# reduce UDF for equation (3) & (4)", "\n", "# equation (3)", "\n", "        ", "alpha", "=", "F", ".", "softmax", "(", "nodes", ".", "mailbox", "[", "\"e\"", "]", ",", "dim", "=", "1", ")", "\n", "# equation (4)", "\n", "h", "=", "torch", ".", "sum", "(", "alpha", "*", "nodes", ".", "mailbox", "[", "\"z\"", "]", ",", "dim", "=", "1", ")", "\n", "return", "{", "\"h\"", ":", "h", "}", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.graph_util.graph_encoder_gat.GATLayer.forward": [[46, 55], ["graph_encoder_gat.GATLayer.fc", "g.apply_edges", "g.update_all", "g.ndata.pop"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.common.params.Params.pop"], ["", "def", "forward", "(", "self", ",", "g", ",", "h", ")", ":", "\n", "# equation (1)", "\n", "        ", "z", "=", "self", ".", "fc", "(", "h", ")", "\n", "g", ".", "ndata", "[", "\"z\"", "]", "=", "z", "\n", "# equation (2)", "\n", "g", ".", "apply_edges", "(", "self", ".", "edge_attention", ")", "\n", "# equation (3) & (4)", "\n", "g", ".", "update_all", "(", "self", ".", "message_func", ",", "self", ".", "reduce_func", ")", "\n", "return", "g", ".", "ndata", ".", "pop", "(", "\"h\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.graph_util.graph_encoder_gat.MultiHeadGATLayer.__init__": [[58, 64], ["torch.Module.__init__", "torch.ModuleList", "torch.ModuleList", "torch.ModuleList", "range", "graph_encoder_gat.MultiHeadGATLayer.heads.append", "graph_encoder_gat.GATLayer"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.graph_util.plot.GraphPlot.__init__"], ["    ", "def", "__init__", "(", "self", ",", "in_dim", ",", "out_dim", ",", "num_heads", ",", "merge", ")", ":", "\n", "        ", "super", "(", "MultiHeadGATLayer", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "heads", "=", "nn", ".", "ModuleList", "(", ")", "\n", "for", "_", "in", "range", "(", "num_heads", ")", ":", "\n", "            ", "self", ".", "heads", ".", "append", "(", "GATLayer", "(", "in_dim", ",", "out_dim", ")", ")", "\n", "", "self", ".", "merge", "=", "merge", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.graph_util.graph_encoder_gat.MultiHeadGATLayer.forward": [[65, 79], ["attn_head", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.max", "torch.max", "torch.max", "torch.max", "torch.max", "torch.max", "torch.max", "torch.max", "torch.max", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.mean", "torch.mean", "torch.mean", "torch.mean", "torch.mean", "torch.mean", "torch.mean", "torch.mean", "torch.mean", "ValueError", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "g", ",", "h", ")", ":", "\n", "        ", "head_outs", "=", "[", "attn_head", "(", "g", ",", "h", ")", "for", "attn_head", "in", "self", ".", "heads", "]", "\n", "if", "self", ".", "merge", "==", "\"cat\"", ":", "\n", "# concat on the output feature dimension (dim=1)", "\n", "            ", "return", "torch", ".", "cat", "(", "head_outs", ",", "dim", "=", "1", ")", "\n", "", "elif", "self", ".", "merge", "==", "\"max\"", ":", "\n", "# max pool", "\n", "            ", "max_pooled", ",", "_", "=", "torch", ".", "max", "(", "torch", ".", "stack", "(", "head_outs", ")", ",", "dim", "=", "0", ")", "\n", "return", "max_pooled", "\n", "", "elif", "self", ".", "merge", "==", "\"mean\"", ":", "\n", "# merge using average", "\n", "            ", "return", "torch", ".", "mean", "(", "torch", ".", "stack", "(", "head_outs", ")", ")", "\n", "", "else", ":", "\n", "            ", "raise", "ValueError", "(", "\"Invalid `merge` value.\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.graph_util.graph_encoder_gat.GAT.__init__": [[82, 98], ["torch.Module.__init__", "graph_encoder_gat.MultiHeadGATLayer", "graph_encoder_gat.MultiHeadGATLayer", "graph_encoder_gat.MultiHeadGATLayer", "graph_aggregate.GraphAggregate"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.graph_util.plot.GraphPlot.__init__"], ["    ", "def", "__init__", "(", "self", ",", "in_dim", ",", "hidden_dim", ",", "num_heads", ",", "merge", "=", "\"max\"", ",", "aggregation", "=", "\"none\"", ")", ":", "\n", "        ", "super", "(", "GAT", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "layer1", "=", "MultiHeadGATLayer", "(", "in_dim", ",", "hidden_dim", ",", "num_heads", ",", "merge", ")", "\n", "# Be aware that the input dimension is hidden_dim*num_heads since", "\n", "# multiple head outputs are concatenated together. Also, only", "\n", "# one attention head in the output layer.", "\n", "if", "merge", "==", "\"cat\"", ":", "\n", "            ", "self", ".", "layer2", "=", "MultiHeadGATLayer", "(", "\n", "hidden_dim", "*", "num_heads", ",", "hidden_dim", ",", "1", ",", "merge", "\n", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "layer2", "=", "MultiHeadGATLayer", "(", "hidden_dim", ",", "hidden_dim", ",", "1", ",", "merge", ")", "\n", "\n", "", "self", ".", "aggregation", "=", "aggregation", "\n", "if", "aggregation", "==", "\"weighted\"", ":", "\n", "            ", "self", ".", "G_aggregator", "=", "GraphAggregate", "(", "hidden_dim", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.graph_util.graph_encoder_gat.GAT.forward": [[99, 109], ["graph_encoder_gat.GAT.layer1", "torch.elu", "torch.elu", "torch.elu", "graph_encoder_gat.GAT.layer2", "graph_encoder_gat.GAT.G_aggregator"], "methods", ["None"], ["", "", "def", "forward", "(", "self", ",", "g", ",", "h", ")", ":", "\n", "        ", "h", "=", "self", ".", "layer1", "(", "g", ",", "h", ")", "\n", "h", "=", "F", ".", "elu", "(", "h", ")", "\n", "h", "=", "self", ".", "layer2", "(", "g", ",", "h", ")", "\n", "\n", "if", "self", ".", "aggregation", "==", "\"weighted\"", ":", "\n", "            ", "wh", "=", "self", ".", "G_aggregator", "(", "h", ")", "\n", "return", "wh", "\n", "", "else", ":", "\n", "            ", "return", "h", "\n", "", "", "", ""]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.graph_util.plot.GraphPlot.__init__": [[19, 22], ["super().__init__"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.graph_util.plot.GraphPlot.__init__"], ["    ", "def", "__init__", "(", "self", ",", "dir", ")", ":", "\n", "        ", "super", "(", "GraphPlot", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "dir", "=", "dir", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.graph_util.plot.GraphPlot.plot": [[23, 36], ["networkx.spring_layout", "networkx.draw", "matplotlib.pyplot.savefig", "matplotlib.pyplot.close", "networkx.from_dict_of_lists", "networkx.from_dict_of_lists", "numpy.sqrt", "len", "networkx.from_dict_of_lists"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.token_embedders.embedding.EmbeddingsTextFile.close"], ["", "def", "plot", "(", "self", ",", "adj_list", ",", "name", ")", ":", "\n", "# to space out nodes", "\n", "        ", "pos", "=", "nx", ".", "spring_layout", "(", "\n", "nx", ".", "from_dict_of_lists", "(", "adj_list", ")", ",", "\n", "k", "=", "0.8", "*", "1", "/", "np", ".", "sqrt", "(", "len", "(", "nx", ".", "from_dict_of_lists", "(", "adj_list", ")", ")", ")", ",", "\n", "iterations", "=", "20", ",", "\n", ")", "\n", "nx", ".", "draw", "(", "\n", "nx", ".", "from_dict_of_lists", "(", "adj_list", ")", ",", "node_size", "=", "500", ",", "with_labels", "=", "True", ",", "pos", "=", "pos", "\n", ")", "\n", "# plt.show()", "\n", "plt", ".", "savefig", "(", "self", ".", "dir", "+", "\"/{:d}\"", ".", "format", "(", "name", ")", ")", "\n", "plt", ".", "close", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.graph_util.plot.GraphPlot.plot_batch": [[37, 54], ["len", "matplotlib.pyplot.subplots", "range", "matplotlib.pyplot.savefig", "networkx.draw", "networkx.from_dict_of_lists"], "methods", ["None"], ["", "def", "plot_batch", "(", "self", ",", "adj_lists_to_plot", ")", ":", "\n", "        ", "plot_times", "=", "0", "\n", "\n", "if", "len", "(", "adj_lists_to_plot", ")", ">=", "4", ":", "\n", "            ", "plot_times", "+=", "1", "\n", "fig", ",", "(", "(", "ax0", ",", "ax1", ")", ",", "(", "ax2", ",", "ax3", ")", ")", "=", "plt", ".", "subplots", "(", "2", ",", "2", ")", "\n", "axes", "=", "{", "0", ":", "ax0", ",", "1", ":", "ax1", ",", "2", ":", "ax2", ",", "3", ":", "ax3", "}", "\n", "for", "i", "in", "range", "(", "1", ")", ":", "\n", "                ", "nx", ".", "draw", "(", "\n", "nx", ".", "from_dict_of_lists", "(", "adj_lists_to_plot", "[", "i", "]", ")", ",", "\n", "with_labels", "=", "True", ",", "\n", "ax", "=", "axes", "[", "i", "]", ",", "\n", ")", "\n", "# plt.show()", "\n", "", "plt", ".", "savefig", "(", "self", ".", "dir", "+", "\"/{:d}\"", ".", "format", "(", "plot_times", ")", ")", "\n", "# plt.close()", "\n", "adj_lists_to_plot", "=", "[", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.graph_util.plot.GraphPlot.rollout_and_examine": [[55, 92], ["range", "model", "isinstance", "print", "plot.dglGraph_to_adj_list", "adj_lists_to_plot.append", "print", "model.number_of_nodes", "len", "matplotlib.pyplot.subplots", "range", "matplotlib.pyplot.savefig", "networkx.draw", "networkx.from_dict_of_lists"], "methods", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.graph_util.plot.dglGraph_to_adj_list"], ["", "", "def", "rollout_and_examine", "(", "self", ",", "model", ",", "num_samples", ")", ":", "\n", "        ", "assert", "not", "model", ".", "training", ",", "\"You need to call model.eval().\"", "\n", "\n", "plot_times", "=", "0", "\n", "adj_lists_to_plot", "=", "[", "]", "\n", "\n", "for", "i", "in", "range", "(", "num_samples", ")", ":", "\n", "            ", "sampled_graph", "=", "model", "(", ")", "\n", "if", "isinstance", "(", "sampled_graph", ",", "list", ")", ":", "\n", "# When the model is a batched implementation, a list of", "\n", "# DGLGraph objects is returned. Note that with model(),", "\n", "# we generate a single graph as with the non-batched", "\n", "# implementation. We actually support batched generation", "\n", "# during the inference so feel free to modify the code.", "\n", "                ", "sampled_graph", "=", "sampled_graph", "[", "0", "]", "\n", "\n", "", "print", "(", "sampled_graph", ")", "\n", "sampled_adj_list", "=", "dglGraph_to_adj_list", "(", "sampled_graph", ")", "\n", "adj_lists_to_plot", ".", "append", "(", "sampled_adj_list", ")", "\n", "print", "(", "sampled_adj_list", ")", "\n", "graph_size", "=", "sampled_graph", ".", "number_of_nodes", "(", ")", "\n", "\n", "if", "len", "(", "adj_lists_to_plot", ")", ">=", "4", ":", "\n", "                ", "plot_times", "+=", "1", "\n", "fig", ",", "(", "(", "ax0", ",", "ax1", ")", ",", "(", "ax2", ",", "ax3", ")", ")", "=", "plt", ".", "subplots", "(", "2", ",", "2", ")", "\n", "axes", "=", "{", "0", ":", "ax0", ",", "1", ":", "ax1", ",", "2", ":", "ax2", ",", "3", ":", "ax3", "}", "\n", "for", "i", "in", "range", "(", "1", ")", ":", "\n", "                    ", "nx", ".", "draw", "(", "\n", "nx", ".", "from_dict_of_lists", "(", "adj_lists_to_plot", "[", "i", "]", ")", ",", "\n", "with_labels", "=", "True", ",", "\n", "ax", "=", "axes", "[", "i", "]", ",", "\n", ")", "\n", "# plt.show()", "\n", "", "plt", ".", "savefig", "(", "self", ".", "dir", "+", "\"/{:d}\"", ".", "format", "(", "plot_times", ")", ")", "\n", "# plt.close()", "\n", "\n", "adj_lists_to_plot", "=", "[", "]", "\n", "", "", "", "", ""]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.graph_util.plot.dglGraph_to_adj_list": [[9, 16], ["range", "g.number_of_nodes", "g.successors().tolist", "g.successors"], "function", ["None"], ["def", "dglGraph_to_adj_list", "(", "g", ")", ":", "\n", "    ", "adj_list", "=", "{", "}", "\n", "for", "node", "in", "range", "(", "g", ".", "number_of_nodes", "(", ")", ")", ":", "\n", "# For undirected graph. successors and", "\n", "# predecessors are equivalent.", "\n", "        ", "adj_list", "[", "node", "]", "=", "g", ".", "successors", "(", "node", ")", ".", "tolist", "(", ")", "\n", "", "return", "adj_list", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.graph_util.output_to_graph.get_end_index": [[18, 26], ["None"], "function", ["None"], ["def", "get_end_index", "(", "current_index", ",", "tags_list", ")", ":", "\n", "    ", "length", "=", "0", "\n", "for", "tag", "in", "tags_list", "[", "current_index", "+", "1", ":", "]", ":", "\n", "        ", "if", "tag", "[", "0", "]", "==", "\"I\"", ":", "\n", "            ", "length", "+=", "1", "\n", "", "else", ":", "\n", "            ", "break", "\n", "", "", "return", "length", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.graph_util.output_to_graph.get_node_type": [[28, 31], ["tag.lstrip().lstrip().lstrip", "tag.lstrip().lstrip", "tag.lstrip"], "function", ["None"], ["", "def", "get_node_type", "(", "tag", ")", ":", "\n", "    ", "type_tag", "=", "tag", ".", "lstrip", "(", "\"B-\"", ")", ".", "lstrip", "(", "\"R-\"", ")", ".", "lstrip", "(", "\"C-\"", ")", "\n", "return", "type_tag", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.graph_util.output_to_graph.get_doc_indices": [[33, 42], ["range", "len", "len", "len"], "function", ["None"], ["", "def", "get_doc_indices", "(", "doc", ",", "pattern", ")", ":", "\n", "    ", "\"\"\" This is required since the doc and sentence \n    indices are not always aligned. We return the first \n    matched index. While this does not hurt the model, \n    it is not the best way to do it.\n    \"\"\"", "\n", "for", "i", "in", "range", "(", "len", "(", "doc", ")", ")", ":", "\n", "        ", "if", "doc", "[", "i", "]", "==", "pattern", "[", "0", "]", "and", "doc", "[", "i", ":", "i", "+", "len", "(", "pattern", ")", "]", "==", "pattern", ":", "\n", "            ", "return", "(", "i", ",", "i", "+", "len", "(", "pattern", ")", "-", "1", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.graph_util.output_to_graph.extract_srl_nodes_and_edges": [[44, 104], ["zip", "enumerate", "edges.extend", "nodes.append", "edges.extend", "output_to_graph.get_end_index", "output_to_graph.get_node_type", "output_to_graph.get_doc_indices", "nodes.append", "vdict_node_ids.append", "tag.startswith", "verb_node_ids.append"], "function", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.graph_util.output_to_graph.get_end_index", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.graph_util.output_to_graph.get_node_type", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.graph_util.output_to_graph.get_doc_indices"], ["", "", "", "def", "extract_srl_nodes_and_edges", "(", "\n", "all_srl_tags", ",", "document_text", ",", "all_sentences", ",", "add_root_node", "=", "True", "\n", ")", ":", "\n", "    ", "\"\"\" Input is a list of srl tags for sentences\n    each of which has 1 or more dictionaries per\n    verb with the verb with the verb and tags.\n\n    Output is a list of nodes each of which is a dict\n    and a list of edges which are node id pairs.\n    \"\"\"", "\n", "nodes", ",", "edges", ",", "verb_node_ids", "=", "[", "]", ",", "[", "]", ",", "[", "]", "\n", "node_id", "=", "0", "\n", "for", "srl_tags_per_sentence", ",", "sentence", "in", "zip", "(", "all_srl_tags", ",", "all_sentences", ")", ":", "\n", "        ", "if", "not", "srl_tags_per_sentence", ":", "\n", "            ", "continue", "\n", "\n", "", "for", "verb_dict", "in", "srl_tags_per_sentence", ":", "\n", "            ", "tags", "=", "verb_dict", "[", "\"tags\"", "]", "\n", "vdict_node_ids", "=", "[", "]", "\n", "for", "tag_index", ",", "tag", "in", "enumerate", "(", "tags", ")", ":", "\n", "                ", "if", "not", "tag", ".", "startswith", "(", "\"B\"", ")", ":", "\n", "                    ", "continue", "\n", "\n", "", "constituent_len", "=", "get_end_index", "(", "tag_index", ",", "tags", ")", "\n", "type", "=", "get_node_type", "(", "tag", ")", "\n", "text", "=", "sentence", "[", "tag_index", ":", "tag_index", "+", "constituent_len", "+", "1", "]", "\n", "doc_indices", "=", "get_doc_indices", "(", "document_text", ",", "text", ")", "\n", "nodes", ".", "append", "(", "\n", "{", "\n", "\"node_id\"", ":", "node_id", ",", "\n", "\"type\"", ":", "type", ",", "\n", "\"doc_indices\"", ":", "doc_indices", ",", "\n", "\"text\"", ":", "text", ",", "\n", "}", "\n", ")", "\n", "vdict_node_ids", ".", "append", "(", "node_id", ")", "\n", "if", "type", "==", "\"V\"", ":", "\n", "                    ", "verb_id", "=", "node_id", "\n", "verb_node_ids", ".", "append", "(", "verb_id", ")", "\n", "", "node_id", "+=", "1", "\n", "\n", "", "vdict_edges", "=", "[", "\n", "{", "\"nids\"", ":", "[", "verb_id", ",", "n_id", "]", ",", "\"type\"", ":", "\"srl\"", "}", "\n", "for", "n_id", "in", "vdict_node_ids", "\n", "if", "n_id", "!=", "verb_id", "\n", "]", "\n", "edges", ".", "extend", "(", "vdict_edges", ")", "\n", "\n", "# add root node", "\n", "", "", "if", "add_root_node", ":", "\n", "        ", "if", "nodes", ":", "\n", "            ", "nodes", ".", "append", "(", "\n", "{", "\"node_id\"", ":", "node_id", ",", "\"type\"", ":", "\"ROOT\"", ",", "\"doc_indices\"", ":", "(", "-", "1", ",", "-", "1", ")", ",", "\"text\"", ":", "None", "}", "\n", ")", "\n", "vdict_edges", "=", "[", "\n", "{", "\"nids\"", ":", "[", "node_id", ",", "verb_id", "]", ",", "\"type\"", ":", "\"root\"", "}", "\n", "for", "verb_id", "in", "verb_node_ids", "\n", "]", "\n", "edges", ".", "extend", "(", "vdict_edges", ")", "\n", "", "", "return", "nodes", ",", "edges", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.graph_util.output_to_graph.get_overlapping_span": [[106, 113], ["None"], "function", ["None"], ["", "def", "get_overlapping_span", "(", "span", ",", "doc_indices", ")", ":", "\n", "    ", "best_left", ",", "best_right", ",", "best_span_len", "=", "10e9", ",", "-", "10e9", ",", "10e9", "\n", "s", ",", "e", "=", "span", "\n", "for", "(", "ds", ",", "de", ")", "in", "doc_indices", ":", "\n", "        ", "if", "s", "-", "ds", ">=", "0", "and", "e", "-", "de", "<=", "0", "and", "best_span_len", ">", "de", "-", "ds", ":", "\n", "            ", "best_left", ",", "best_right", ",", "best_span_len", "=", "ds", ",", "de", ",", "de", "-", "ds", "\n", "", "", "return", "(", "best_left", ",", "best_right", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.graph_util.output_to_graph.get_coref_edges": [[115, 136], ["set", "output_to_graph.get_overlapping_span", "len", "new_edges.extend", "doc_node_indices.index", "set.add", "itertools.combinations"], "function", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.graph_util.output_to_graph.get_overlapping_span", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.fields.text_field.TextField.index"], ["", "def", "get_coref_edges", "(", "srl_nodes", ",", "coref_clusters", ")", ":", "\n", "    ", "doc_node_indices", "=", "[", "node", "[", "\"doc_indices\"", "]", "for", "node", "in", "srl_nodes", "]", "\n", "doc_node_ids", "=", "[", "node", "[", "\"node_id\"", "]", "for", "node", "in", "srl_nodes", "]", "\n", "span_in_doc_counter", "=", "0", "\n", "new_edges", "=", "[", "]", "\n", "# logger.info(sorted(doc_node_indices, key=lambda x: x[0]))", "\n", "for", "cluster", "in", "coref_clusters", ":", "\n", "        ", "matching_span_node_ids", "=", "set", "(", ")", "\n", "for", "span", "in", "cluster", ":", "\n", "            ", "span_in_doc_counter", "+=", "1", "\n", "overlapping_span", "=", "get_overlapping_span", "(", "span", ",", "doc_node_indices", ")", "\n", "# logger.info(span, overlapping_span)", "\n", "if", "overlapping_span", "!=", "(", "10e9", ",", "-", "10e9", ")", ":", "\n", "                ", "span_index_in_doc_indices", "=", "doc_node_indices", ".", "index", "(", "overlapping_span", ")", "\n", "span_node_id", "=", "doc_node_ids", "[", "span_index_in_doc_indices", "]", "\n", "matching_span_node_ids", ".", "add", "(", "span_node_id", ")", "\n", "", "", "if", "len", "(", "matching_span_node_ids", ")", ">", "1", ":", "\n", "            ", "new_edges", ".", "extend", "(", "combinations", "(", "matching_span_node_ids", ",", "2", ")", ")", "\n", "", "", "if", "new_edges", "is", "not", "None", ":", "\n", "        ", "new_edges", "=", "[", "{", "\"nids\"", ":", "n", ",", "\"type\"", ":", "\"cor\"", "}", "for", "n", "in", "new_edges", "]", "\n", "", "return", "new_edges", ",", "span_in_doc_counter", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.graph_util.output_to_graph.json2graph": [[138, 152], ["dict", "output_to_graph.extract_srl_nodes_and_edges", "output_to_graph.get_coref_edges", "edges.extend", "len"], "function", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.graph_util.output_to_graph.extract_srl_nodes_and_edges", "home.repos.pwc.inspect_result.rahular_joint-coref-srl.graph_util.output_to_graph.get_coref_edges"], ["", "def", "json2graph", "(", "graph_json", ")", ":", "\n", "    ", "metadata", "=", "dict", "(", ")", "\n", "document_text", "=", "graph_json", "[", "\"document\"", "]", "\n", "all_srl_tags", "=", "graph_json", "[", "\"srl_tags\"", "]", "\n", "corref_clusters", "=", "graph_json", "[", "\"clusters\"", "]", "\n", "sentences", "=", "graph_json", "[", "\"sentences\"", "]", "\n", "nodes", ",", "edges", "=", "extract_srl_nodes_and_edges", "(", "all_srl_tags", ",", "document_text", ",", "sentences", ")", "\n", "coref_edges", ",", "_", "=", "get_coref_edges", "(", "nodes", ",", "corref_clusters", "[", "0", "]", ")", "\n", "edges", ".", "extend", "(", "coref_edges", ")", "\n", "metadata", "[", "\"empty_doc\"", "]", "=", "True", "if", "not", "nodes", "else", "False", "\n", "metadata", "[", "\"coref_edges\"", "]", "=", "True", "if", "coref_edges", "else", "False", "\n", "metadata", "[", "\"num_sent\"", "]", "=", "len", "(", "sentences", ")", "\n", "# logger.info(json.dumps({'nodes': nodes, 'edges': edges}, indent=2))", "\n", "return", "nodes", ",", "edges", ",", "metadata", "\n", "\n"]], "home.repos.pwc.inspect_result.rahular_joint-coref-srl.graph_util.output_to_graph.dump_graph": [[154, 158], ["ntpath.split", "open", "json.dump", "os.path.join"], "function", ["home.repos.pwc.inspect_result.rahular_joint-coref-srl.dataset_readers.multiprocess_dataset_reader.QIterable.join"], ["", "def", "dump_graph", "(", "graphs", ",", "input_path", ")", ":", "\n", "    ", "head", ",", "tail", "=", "ntpath", ".", "split", "(", "input_path", ")", "\n", "with", "open", "(", "os", ".", "path", ".", "join", "(", "head", ",", "\"graphs_{}\"", ".", "format", "(", "tail", ")", ")", ",", "\"w\"", ",", "encoding", "=", "\"utf-8\"", ")", "as", "f", ":", "\n", "        ", "json", ".", "dump", "(", "graphs", ",", "f", ")", "\n", "\n"]]}